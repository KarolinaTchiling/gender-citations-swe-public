FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Fan, JP
   Elmagarmid, AK
   Zhu, XQ
   Aref, WG
   Wu, LD
AF Fan, JP
   Elmagarmid, AK
   Zhu, XQ
   Aref, WG
   Wu, LD
TI <i>ClassView</i>:: Hierarchical video shot classification, indexing, and
   accessing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE video classification; video database indexing; video retrieval; visual
   summarization
ID IMAGE RETRIEVAL; VISUALIZATION; DATABASES; TREE
AB Recent advances in digital video compression and networks have made video more accessible than ever. However, the existing content-based video retrieval systems still suffer from the following problems. 1) Semantics-sensitive video classification problem because of the semantic gap between low-level visual features and high-level semantic visual concepts; 2) Integrated video access problem because of the lack of efficient video database indexing, automatic video annotation, and concept-oriented summary organization techniques. In this paper, we have proposed a novel framework, called ClassView, to make some advances toward more efficient video database indexing and access. 1) A hierarchical semantics-sensitive video classifier is proposed to shorten the semantic gap. The hierarchical tree structure of the semantics-sensitive video classifier is derived from the domain-dependent concept hierarchy of video contents in a database. Relevance analysis is used for selecting the discriminating visual features with suitable importances. The Expectation-Maximization (EM) algorithm is also used to determine the classification rule for each visual concept node in the classifier. 2) A hierarchical video database indexing and summary presentation technique is proposed to support more effective video access over a large-scale database. The hierarchical tree structure of our video database indexing scheme is determined by the domain-dependent concept hierarchy which is also used for video classification. The presentation of visual summary is also integrated with the inherent hierarchical video database indexing tree structure. Integrating video access with efficient database indexing tree structure has provided great opportunity for supporting more powerful video search engines.
C1 Univ N Carolina, Dept Comp Sci, Charlotte, NC 28223 USA.
   Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA.
   Fudan Univ, Dept Comp Sci, Shanghai 200433, Peoples R China.
C3 University of North Carolina; University of North Carolina Charlotte;
   Purdue University System; Purdue University; Fudan University
RP Univ N Carolina, Dept Comp Sci, Charlotte, NC 28223 USA.
EM jfan@uncc.edu
RI Aref, Walid/D-4403-2019; ZOU, Fengcai/ABE-4598-2021
OI Aref, Walid/0000-0001-8169-7775; ZOU, Fengcai/0000-0002-9613-3734; Zhu,
   Xingquan/0000-0003-4129-9611
CR [Anonymous], 1994, VLDB
   [Anonymous], VLDB
   BENITEZ AB, 2002, 2002 INT WORKSH MULT
   BENITEZ AB, 2001, P SPIE
   BERITEZ A, 2000, SIGNAL PROCESS-IMAGE, V16, P235
   Buntine W., 1992, Statistics and Computing, V2, P63, DOI 10.1007/BF01889584
   Chang SF, 1998, IEEE T CIRC SYST VID, V8, P602, DOI 10.1109/76.718507
   Chen JY, 1999, PROC SPIE, V3846, P148, DOI 10.1117/12.368470
   Deng YN, 1998, IEEE T CIRC SYST VID, V8, P616, DOI 10.1109/76.718508
   Duda R., 1973, Pattern Classification and Scene Analysis
   FAN J, 2000, J ELECT IMAGING, V9
   FLICKNER M, 1995, IEEE COMPUT, V38, P23
   GUPTA S, 1998, ACM SIGMOD
   Guttman A., 1984, ACM SIGMOD, P47, DOI DOI 10.1145/971697.602266
   HUANG J, 1998, ACM MULTIMEDIA BRIST
   HUANG Q, 1999, P SPIE
   HUMRAPUR A, 1997, P SPIE STOR RETR IM, V5, P188
   Ishikawa Y., 1998, Proceedings of the Twenty-Fourth International Conference on Very-Large Databases, P218
   Jain AK, 1999, MULTIMEDIA SYST, V7, P369, DOI 10.1007/s005300050139
   JORDAN MI, 1996, MACH LEARN
   Katayama N., 1997, ACM SIGMOD
   Lin K, 1994, VLDB J
   LOMET DB, 1990, ACM T DATABASE SYST, V15, P625, DOI 10.1145/99935.99949
   Manolopoulos Y., 2000, Advanced Database Indexing
   MICHALSKI RS, 1983, IEEE T PATTERN ANAL, V5, P396, DOI 10.1109/TPAMI.1983.4767409
   Miller G.A., 1990, Int. J. Lexicogr, V3, P235, DOI [DOI 10.1093/IJL/3.4.235, 10.1093/ijl/3.4.235]
   Minka TP, 1997, PATTERN RECOGN, V30, P565, DOI 10.1016/S0031-3203(96)00113-6
   Pentland A, 1996, INT J COMPUT VISION, V18, P233, DOI 10.1007/BF00123143
   Quinlan J. R., 1986, Machine Learning, V1, P81, DOI 10.1007/BF00116251
   RAUDYS SJ, 1991, IEEE T PATTERN ANAL, V13, P252, DOI 10.1109/34.75512
   Raymer ML, 2000, IEEE T EVOLUT COMPUT, V4, P164, DOI 10.1109/4235.850656
   Rui Y, 1998, IEEE T CIRC SYST VID, V8, P644, DOI 10.1109/76.718510
   Rui Y, 1999, MULTIMEDIA SYST, V7, P359, DOI 10.1007/s005300050138
   Rui Y., 1999, P 7 ACM INT C MULTIM, P67, DOI DOI 10.1145/319878.319896
   Salembier P, 2000, SIGNAL PROCESS-IMAGE, V16, P211, DOI 10.1016/S0923-5965(00)00026-6
   SATOH S, 1997, P COMP VIS PATT REC
   SHEIKHOLESLAMI G, 1998, ACM MULTIMEDIA BRIST
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Smith DL, 1999, STAND MAG, V1, P157
   THOMASIAN V, CIKM 98 BETH MD, P201
   Vailaya A, 1998, IEEE WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO LIBRARIES - PROCEEDINGS, P3, DOI 10.1109/IVL.1998.694464
   Wang JZ, 2001, IEEE T PATTERN ANAL, V23, P947, DOI 10.1109/34.955109
   White DA, 1996, PROC INT CONF DATA, P516, DOI 10.1109/ICDE.1996.492202
   Wu P, 2001, ACM MULTIMEDIA
   Yeo BL, 1997, P SOC PHOTO-OPT INS, V3312, P60, DOI 10.1117/12.298470
   Yeung MM, 1997, IEEE T CIRC SYST VID, V7, P771, DOI 10.1109/76.633496
   Yu HH, 1995, P SOC PHOTO-OPT INS, V2606, P363, DOI 10.1117/12.227258
   ZHANG C, 2002, ACTIVE LEARNING INFO
   Zhang HJ, 1997, PATTERN RECOGN, V30, P643, DOI 10.1016/S0031-3203(96)00109-4
   Zhang T., 1996, ACM SIGMOD
   Zhong D, 1996, PROC SPIE, V2670, P239, DOI 10.1117/12.234800
   ZHOU WS, 2000, ACM MULTIMEDIA
   Zhou XS, 2002, IEEE MULTIMEDIA, V9, P23, DOI 10.1109/93.998050
   ZHU X, 2002, MULTIMEDIA SYST
NR 54
TC 93
Z9 114
U1 1
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2004
VL 6
IS 1
BP 70
EP 86
DI 10.1109/TMM.2003.819583
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 765QP
UT WOS:000188295200006
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yu, YZ
   Cheng, I
   Basu, A
AF Yu, YZ
   Cheng, I
   Basu, A
TI Optimal adaptive bandwidth monitoring for QoS based retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE bandwidth monitoring; probabilitic model; QoS
AB Network aware multimedia delivery applications are a class of applications that provide certain level of quality of service (QoS) guarantees to end users while not assuming underlying network resource reservations. These applications guarantee QoS parameters like media object transmission time limit by actively monitoring the available bandwidth of the network and adapting the object to a target size that can be transmitted within a given time limit. A critical problem is how to obtain an accurate enough estimation of available bandwidth while not wasting too much time in bandwidth testing. In this paper, we present an algorithm to determine optimal amount of bandwidth testing given a probabilistic confidence level for network-aware multimedia object retrieval applications. The model treats the bandwidth testing as sampling from an actual bandwidth population. It uses statistical estimation method to quantify the benefit of each new bandwidth-testing sample, which is used to determine the optimal amount of bandwidth testing by balancing the benefit with the cost of each sample. Our implementation and experiments shows the algorithm determines the optimal amount of bandwidth testing effectively with minimum computation overhead.
C1 Univ Alberta, Dept Comp Sci, Edmonton, AB T6G 2E8, Canada.
C3 University of Alberta
RP Yu, YZ (corresponding author), Univ Alberta, Dept Comp Sci, Edmonton, AB T6G 2E8, Canada.
CR [Anonymous], 1996, CRC Standard Mathematical Tables and Formulae
   [Anonymous], 1997, 2205 RFC
   Bolliger J, 1998, IEEE T SOFTWARE ENG, V24, P376, DOI 10.1109/32.685260
   BOLLIGER J, 1999, P INF 99 MAR
   CHENG I, 2001, P EUROCON 2001 BRAT, P485
   Ferguson Paul., 1998, Quality of service : delivering QoS on the Internet and in corporate networks
   FERRARI D, 1997, ACM SPRINGER VERLAG, V5
   GOPALAKRISHNA G, 1994, EFFICIENT QUALITY SE
   Harnett D.L., 1982, STAT METHODS, VThird
   JAIN R, 1991, AIR COMPUTER SYSTEMS
   MILOUCHEVA I, 1995, 1995 ACM PAC WORKSH
   PACIFICI G, 1995, P IFIP IEEE INT S IN
   VOGEL A, 1995, IEEE MULTIMEDIA, V2
   Wang X, 1999, IEICE T COMMUN, VE82B, P806
   WILLIAM WH, 1993, NUMERICAL RECIPES C
   Zhang L., 1993, IEEE Network, V7, P8, DOI 10.1109/65.238150
NR 16
TC 9
Z9 10
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2003
VL 5
IS 3
BP 466
EP 472
DI 10.1109/TMM.2003.814725
PG 7
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 714BM
UT WOS:000184892500017
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Huang, JY
AF Huang, JY
TI An omnidirectional stroll-based virtual reality interface and its
   application on overhead crane training
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE gait analysis; locomotion; overhead crane; virtual reality
AB Locomotion is an virtual reality interface that enables the user to walk inside the virtual environment in any directions over a long distance without actually leaving the physical device. In order to enable the user to freely navigate the virtual world and get fully immersed into the virtual environment accordingly, a locomotion device must fulfill the following two distinct requirements. First, it should allow the user to navigate an infinite distance within a limited area. Secondly, the user should not need to wear any tracking devices to detect his motion. This paper presents a locomotion mechanism called Omni-directional Ball-bearing Disc Platform (OBDP), which allows the user to walk naturally on it and thus to navigate the virtual environment. The gait sensing algorithm that simulates the user's posture based upon his footstep data collected from the OBDP is then elaborated. Followed with an omnidirectional stroll-based virtual reality system to integrate the OBDP with the gait sensing algorithm. Significantly, instead of using the three-dimensional (3-D) tracker, the OBDP adopts arrays of ball-bearing sensors on a disc to detect the pace. No other sensor, except the head tracker to, detect the user's head rotation, is required on the user's body. Finally, a prototype of the overhead crane training simulator that fully explores the advantage of the OBDP is presented at the end of this paper along with the verification of the effectiveness of the presented gait sensing algorithm.
C1 Tamkang Univ, Dept Informat Engn, Taipei 251, Taiwan.
C3 Tamkang University
RP Tamkang Univ, Dept Informat Engn, Taipei 251, Taiwan.
EM jiungyao@ms45.hinet.net
RI Huang, Jiung-yao/D-2461-2010
CR [Anonymous], 1986, P 1986 WORKSH INT 3D
   CHAO EY, 1983, J BIOMECH, V16, P219, DOI 10.1016/0021-9290(83)90129-X
   CHRISTENSEN R, PRES TEL VIRT ENV
   Darken R. P., 1997, Proceedings of the ACM Symposium on User Interface Software and Technology. 10th Annual Symposium. UIST '97, P213, DOI 10.1145/263407.263550
   HUANG JY, 2000, P WSCG 2000, P290
   HUANG JY, 2001, P IEEE ICDCS 21 W, P402
   HUANG JY, 1997, P IEEE RTCSA, P160
   HUANG JY, 1996, P 1996 COMP GRAPH WO, P81
   INTILLE SS, 1996, P IEEE C COMP VIS PA, P607
   Iwata H, 1996, P IEEE VIRT REAL ANN, P60, DOI 10.1109/VRAIS.1996.490511
   Iwata H, 1999, P IEEE VIRT REAL ANN, P286, DOI 10.1109/VR.1999.756964
   KUHL J, 1995, COMPUTER, V28, P35, DOI 10.1109/2.391039
   Moore M., 1988, Computer Graphics, V22, P289, DOI 10.1145/378456.378528
   PRATT DR, 1994, P 1994 AI SIM PLANN, P34
   Slater M., 1993, First Eurographics Workshop on Virtual Environments, P71
   SRINIVASAN S, 1997, P 1997 SPRING SIW, P329
   Stytz MR, 1996, IEEE COMPUT GRAPH, V16, P19, DOI 10.1109/38.491182
   Tucker WV, 1998, T SOC COMPUT SIMUL I, V15, P3
   Usoh M, 1999, COMP GRAPH, P359, DOI 10.1145/311535.311589
   Ward Mark., 1992, Proceedings of the 1992 symposium on Interactive 3D graphics, P43, DOI [DOI 10.1145/147156.147162, 10.1145/147156.147162]
   YANO H, 2000, P ACM C COMP SUPP CO, P163
NR 21
TC 24
Z9 33
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2003
VL 5
IS 1
BP 39
EP 51
DI 10.1109/TMM.2003.808822
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 675HP
UT WOS:000182688200005
DA 2024-07-18
ER

PT J
AU Chen, D
   Zhuang, YT
   Shen, ZJ
   Yang, C
   Wang, GM
   Tang, SL
   Yang, Y
AF Chen, Dong
   Zhuang, Yueting
   Shen, Zijin
   Yang, Carl
   Wang, Guoming
   Tang, Siliang
   Yang, Yi
TI Cross-Modal Data Augmentation for Tasks of Different Modalities
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-modal; data augmentation.
AB Data augmentation has become one of the keys to alleviating the over-fitting of models on training data and improving the generalization capabilities on testing data. Most existing data augmentation methods only focus on one modality, which is incapable when facing multiple data modalities. Some prior works try to interpolate with random coefficients in the latent space to generate new samples, which can generically work for any data modality. However, these works ignore the extra information conveyed by multimodality data. In fact, the extra information in one modality can provide semantic directions to generate more meaningful samples in another modality. This paper proposes Cross-modal Data Augmentation (CMDA), a simple yet effective data augmentation method to alleviate the over-fitting issue and improve the generalization performance. We evaluate CMDA on unsupervised and supervised tasks of different modalities, on which CMDA consistently and significantly outperforms baselines. For instance, CMDA improves the unsupervised anomaly detection baseline in vision modality from the AUROC 76.46%, 73.07% and 64.36% to 83.25%, 76.22% and 70.57% on three different datasets, respectively. Besides, extensive experiments demonstrate that CMDA is applicable to various neural network architectures. Furthermore, prior methods that interpolate in the latent space need to work with downstream tasks to construct the latent space. In contrast, CMDA can work with or without downstream tasks, which makes the applicability of CMDA more extensive. The source code is publicly available for non-commercial or research use at https://github.com/Anfeather/CMDA
C1 [Chen, Dong; Zhuang, Yueting; Shen, Zijin; Wang, Guoming; Tang, Siliang; Yang, Yi] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310058, Peoples R China.
   [Yang, Carl] Emory Univ, Atlanta, GA 30322 USA.
C3 Zhejiang University; Emory University
RP Wang, GM (corresponding author), Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310058, Peoples R China.
EM chendongcs@zju.edu.cn; yzhuang@zju.edu.cn; zijinshen@zju.edu.cn;
   j.carlyang@emory.edu; nb21013@zju.edu.cn; siliang@zju.edu.cn;
   yangyics@zju.edu.cn
RI Chen, Dong/JID-3666-2023
OI Chen, Dong/0000-0002-4859-1757; Tang, Siliang/0000-0002-7356-9711
FU Zhejiang NSF [LR21F020004]; NSFC [62272411]; Alibaba-Zhejiang University
   Joint Research Institute of Frontier Technologies, Chinese Knowledge
   Center of Engineering Science and Technology (CKCEST)
FX This work was supported in part by the Zhejiang NSF under Grant
   LR21F020004, in part by NSFC under Grant 62272411, and in part by the
   Alibaba-Zhejiang University Joint Research Institute of Frontier
   Technologies, Chinese Knowledge Center of Engineering Science and
   Technology (CKCEST).
CR Amini M. R., 2009, NEURIPS, V22, P1, DOI DOI 10.5555/2984093.2984097
   Andrew G., 2013, ICML, P1247
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Bengio Y., 2009, P 26 ANN INT C MACH, V60, P6, DOI [DOI 10.1145/1553374.1553380, 10.1145/1553374.1553380]
   Cao Y, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1445, DOI 10.1145/2939672.2939812
   Chen Dave Zhenyu, 2020, COMPUTER VISION ECCV, P202
   Cheung T.-H., 2021, P INT C LEARN REPR
   DeVries T, 2017, Arxiv, DOI [arXiv:1708.04552, DOI 10.48550/ARXIV.1708.04552]
   Gong CY, 2021, PROC CVPR IEEE, P2474, DOI 10.1109/CVPR46437.2021.00250
   Guo XB, 2023, IEEE T MULTIMEDIA, V25, P2085, DOI 10.1109/TMM.2022.3142448
   Han JL, 2022, Arxiv, DOI arXiv:2201.12078
   Hu P, 2021, PROC CVPR IEEE, P5399, DOI 10.1109/CVPR46437.2021.00536
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang Yu, 2021, ADV NEUR IN
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Kumar V, 2019, PROCEEDINGS OF THE TENTH INTERNATIONAL CONFERENCE ON INFORMATION AND COMMUNICATION TECHNOLOGIES AND DEVELOPMENT (ICTD), DOI 10.1145/3287098.3287099
   LAU JH, 2016, P 1 WORKSH REPR LEAR, P78, DOI DOI 10.18653/V1/W16-1609
   Li SY, 2023, IEEE T MULTIMEDIA, V25, P805, DOI 10.1109/TMM.2021.3132166
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu XF, 2018, INT C PATT RECOG, P728, DOI 10.1109/ICPR.2018.8545506
   Moon TK, 1996, IEEE SIGNAL PROC MAG, V13, P47, DOI 10.1109/79.543975
   Peng Y, 2016, P INT JOINT C ART IN, P3846
   Peng YX, 2018, IEEE T CIRC SYST VID, V28, P2372, DOI 10.1109/TCSVT.2017.2705068
   Peng YX, 2018, IEEE T MULTIMEDIA, V20, P405, DOI 10.1109/TMM.2017.2742704
   Radford A, 2021, PR MACH LEARN RES, V139
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Reed S, 2016, PR MACH LEARN RES, V48
   Reynolds D., 2015, ENCY BIOMETRICS, P827, DOI 10.1007/978-1-4899-7488-4_196
   Sehwag V., 2020, P INT C LEARN REPR
   Shorten C, 2021, J BIG DATA-GER, V8, DOI 10.1186/s40537-021-00492-0
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song Y, 2019, PROC CVPR IEEE, P1979, DOI 10.1109/CVPR.2019.00208
   Stowell D, 2015, IEEE T MULTIMEDIA, V17, P1733, DOI 10.1109/TMM.2015.2428998
   Tan H, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5100
   Tripuraneni N., 2020, Advances in neural information processing systems, P7852, DOI 10.5555/3495724.3496382
   Upchurch P, 2017, PROC CVPR IEEE, P6090, DOI 10.1109/CVPR.2017.645
   Verma V, 2019, PR MACH LEARN RES, V97
   Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326
   Wang D, 2023, IEEE T MULTIMEDIA, V25, P1217, DOI 10.1109/TMM.2022.3140656
   Xiao H., 2018, bert-as-service
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yan C, 2022, IEEE T MULTIMEDIA, V24, P1665, DOI 10.1109/TMM.2021.3069562
   Ye F, 2022, IEEE T MULTIMEDIA, V24, P116, DOI 10.1109/TMM.2020.3046884
   Ye SQ, 2024, IEEE T VIS COMPUT GR, V30, P1772, DOI 10.1109/TVCG.2022.3225327
   Ye SQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6423, DOI 10.1109/ICCV48922.2021.00638
   Zhai DM, 2012, ACM T INTEL SYST TEC, V3, DOI 10.1145/2168752.2168767
   Zhai X., 2013, PROC 27 AAAI C ARTIF, P1198
   Zhang CY, 2021, COMMUN ACM, V64, P107, DOI 10.1145/3446776
   Zhang Hongyi, 2018, MIXUP EMPIRICAL RISK, DOI DOI 10.48550/ARXIV.1710.09412
   Zhen LL, 2019, PROC CVPR IEEE, P10386, DOI 10.1109/CVPR.2019.01064
   Zheng F, 2018, IEEE T PATTERN ANAL, V40, P1059, DOI 10.1109/TPAMI.2016.2645565
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
NR 53
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7814
EP 7824
DI 10.1109/TMM.2022.3228696
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AV4K5
UT WOS:001121212400016
DA 2024-07-18
ER

PT J
AU Ding, GC
   Yang, DQ
   Wang, T
   Wang, SH
   Zhang, YF
AF Ding, Guanchen
   Yang, Daiqin
   Wang, Tao
   Wang, Sihan
   Zhang, Yunfei
TI Crowd Counting via Unsupervised Cross-Domain Feature Adaptation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Unsupervised crowd counting; domain adaptation; density map estimation;
   adversarial learning
AB Given an image, crowd counting aims to estimate the amount of target objects in the image. With un-predictable installation situations of surveillance systems (or other equipments), crowd counting images from different data sets may exhibit severe discrepancies in viewing angle, scale, lighting condition, etc. As it is usually expensive and time-consuming to annotate each data set for model training, it has been an essential issue in crowd counting to transfer a well-trained model on a labeled data set (source domain) to a new data set (target domain). To tackle this problem, we propose a cross-domain learning network to learn the domain gaps in an unsupervised learning manner. The proposed network comprises of a Multi-granularity Feature-aware Discriminator (MFD) module, a Domain-invariant Feature Adaptation (DFA) module, and a Cross-domain Vanishing Bridge (CVB) module to remove domain-specific information from the extracted features and promote the mapping performances of the network. Unlike most existing methods that use only Global Feature Discriminator (GFD) to align features at image level, an additional Local Feature Discriminator (LFD) is inserted and together with GFD form the MFD module. As a complement to MFD, LFD refines features at pixel level and has the ability to align local features. The DFA module explicitly measures the distances between the source domain features and the target domain features and aligns the marginal distribution of their features with Maximum Mean Discrepancy (MMD). Finally, the CVB module provides an incremental capability of removing the impact of interfering part of the extracted features. Several well-known networks are adopted as the backbone of our algorithm to prove the effectiveness of the proposed adaptation structure. Comprehensive experiments demonstrate that our model achieves competitive performance to the state-of-the-art methods.
C1 [Ding, Guanchen; Yang, Daiqin] Wuhan Univ, Sch Remote Sensing & Informat Engn, Wuhan 430079, Peoples R China.
   [Wang, Tao; Wang, Sihan; Zhang, Yunfei] Tencent, Shenzhen 518000, Peoples R China.
C3 Wuhan University; Tencent
RP Yang, DQ (corresponding author), Wuhan Univ, Sch Remote Sensing & Informat Engn, Wuhan 430079, Peoples R China.
EM gcding@whu.edu.cn; dqyang@whu.edu.cn; tuckerwang@tencent.com;
   lovingwang@tencent.com; yanniszhang@tencent.com
OI Ding, Guanchen/0000-0002-9523-1850
FU National Natural Science Foundation of China [61771348]; Tencent
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61771348, and in part by Tencent.
CR Bai S, 2020, PROC CVPR IEEE, P4593, DOI 10.1109/CVPR42600.2020.00465
   Cao XK, 2018, LECT NOTES COMPUT SC, V11209, P757, DOI 10.1007/978-3-030-01228-1_45
   Chan AB, 2008, PROC CVPR IEEE, P1766, DOI 10.1109/cvpr.2008.4587569
   Chang-Dong Xu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11721, DOI 10.1109/CVPR42600.2020.01174
   Chen K, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.21
   Chen YC, 2019, PROC CVPR IEEE, P1791, DOI 10.1109/CVPR.2019.00189
   Cheng J, 2021, IEEE T IMAGE PROCESS, V30, P2862, DOI 10.1109/TIP.2021.3055631
   Elassal N, 2017, LECT NOTES COMPUT SC, V10115, P329, DOI 10.1007/978-3-319-54193-8_21
   Ganin Y, 2016, J MACH LEARN RES, V17
   Gao J., 2019, arXiv
   Gao JY, 2021, IEEE T CYBERNETICS, V51, P4822, DOI 10.1109/TCYB.2020.3034316
   Ge Weina, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2913, DOI 10.1109/CVPRW.2009.5206621
   Gretton A, 2006, Adv Neural Inf Process Syst, V19
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Guangrui Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P440, DOI 10.1007/978-3-030-58568-6_26
   Guo D, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1823, DOI 10.1145/3343031.3350881
   Han T, 2020, INT CONF ACOUST SPEE, P1848, DOI [10.1109/icassp40776.2020.9054768, 10.1109/ICASSP40776.2020.9054768]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He YH, 2021, AAAI CONF ARTIF INTE, V35, P1540
   Hoffman J, 2018, PR MACH LEARN RES, V80
   Hoffmann Johannes, 2016, 2016 Conference on Precision Electromagnetic Measurements (CPEM), P1, DOI 10.1109/CPEM.2016.7540615
   Hossain MA, 2020, 2020 17TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV 2020), P150, DOI 10.1109/CRV50864.2020.00028
   Hu LQ, 2020, PROC CVPR IEEE, P4042, DOI 10.1109/CVPR42600.2020.00410
   Idrees H, 2018, LECT NOTES COMPUT SC, V11206, P544, DOI 10.1007/978-3-030-01216-8_33
   Jiang XH, 2020, PROC CVPR IEEE, P4705, DOI 10.1109/CVPR42600.2020.00476
   Jiang XH, 2021, IEEE T MULTIMEDIA, V23, P443, DOI 10.1109/TMM.2020.2980945
   Kingma D. P., 2014, arXiv
   Lempitsky V., 2010, P ADV NEUR INF PROC, V23, P1, DOI DOI 10.5555/2997189.2997337
   Li S, 2022, IEEE T PATTERN ANAL, V44, P4093, DOI 10.1109/TPAMI.2021.3062644
   Li YH, 2018, PROC CVPR IEEE, P1091, DOI 10.1109/CVPR.2018.00120
   Li YS, 2019, PROC CVPR IEEE, P6929, DOI 10.1109/CVPR.2019.00710
   Lian Q, 2019, IEEE I CONF COMP VIS, P6757, DOI 10.1109/ICCV.2019.00686
   Liu L, 2021, IEEE T MULTIMEDIA, V23, P1060, DOI 10.1109/TMM.2020.2992979
   Liu N, 2019, PROC CVPR IEEE, P3220, DOI 10.1109/CVPR.2019.00334
   Liu XL, 2018, PROC CVPR IEEE, P7661, DOI 10.1109/CVPR.2018.00799
   Liu YT, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P129, DOI 10.1145/3394171.3413825
   Luo YW, 2020, ADV NEUR IN, V33
   Ma ZH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3185, DOI 10.1109/ICCV48922.2021.00319
   Ma ZH, 2019, IEEE I CONF COMP VIS, P6141, DOI 10.1109/ICCV.2019.00624
   Minghao Xu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12352, DOI 10.1109/CVPR42600.2020.01237
   Motiian Saeid, 2017, Advances in Neural Information Processing Systems, P6670
   Pei ZY, 2018, AAAI CONF ARTIF INTE, P3934
   Reddy MKK, 2022, IEEE T MULTIMEDIA, V24, P1008, DOI 10.1109/TMM.2021.3062481
   Reddy MKK, 2020, IEEE WINT CONF APPL, P2803, DOI [10.1109/WACV45572.2020.9093409, 10.1109/wacv45572.2020.9093409]
   Sam DB, 2019, AAAI CONF ARTIF INTE, P8868
   Sam DB, 2017, PROC CVPR IEEE, P4031, DOI 10.1109/CVPR.2017.429
   Shrivastava A, 2017, PROC CVPR IEEE, P2242, DOI 10.1109/CVPR.2017.241
   Shuhao Cui, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12452, DOI 10.1109/CVPR42600.2020.01247
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tsai YH, 2018, PROC CVPR IEEE, P7472, DOI 10.1109/CVPR.2018.00780
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   Volpi R, 2018, PROC CVPR IEEE, P5495, DOI 10.1109/CVPR.2018.00576
   Wan J, 2021, IEEE T IMAGE PROCESS, V30, P2114, DOI 10.1109/TIP.2021.3049938
   Wang C, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1299, DOI 10.1145/2733373.28063370-12345-67-8/90/01
   Wang L, 2019, IEEE INT CON MULTI, P193, DOI 10.1109/ICME.2019.00041
   Wang Q, 2022, IEEE T NEUR NET LEAR, V33, P3238, DOI 10.1109/TNNLS.2021.3051371
   Wang Q, 2021, IEEE T PATTERN ANAL, V43, P2141, DOI 10.1109/TPAMI.2020.3013269
   Wang Q, 2021, INT J COMPUT VISION, V129, DOI 10.1007/s11263-020-01365-4
   Wang Q, 2019, PROC CVPR IEEE, P8190, DOI 10.1109/CVPR.2019.00839
   Wu QQ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P658, DOI 10.1145/3474085.3475230
   Yan ZY, 2022, IEEE T MULTIMEDIA, V24, P2633, DOI 10.1109/TMM.2021.3086709
   Yoo D, 2016, LECT NOTES COMPUT SC, V9912, P517, DOI 10.1007/978-3-319-46484-8_31
   Yosinski J, 2014, ADV NEUR IN, V27
   Zhang AR, 2019, IEEE I CONF COMP VIS, P6787, DOI 10.1109/ICCV.2019.00689
   Zhang C, 2015, PROC CVPR IEEE, P833, DOI 10.1109/CVPR.2015.7298684
   Zhang C, 2016, IEEE T MULTIMEDIA, V18, P1048, DOI 10.1109/TMM.2016.2542585
   Zhang YY, 2016, PROC CVPR IEEE, P589, DOI 10.1109/CVPR.2016.70
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zou ZK, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2185, DOI 10.1145/3474085.3475377
NR 70
TC 7
Z9 7
U1 3
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4665
EP 4678
DI 10.1109/TMM.2022.3180222
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300005
DA 2024-07-18
ER

PT J
AU Dong, Y
   Jiang, XH
   Li, ZH
   Sun, TF
   Zhang, ZZ
AF Dong, Yi
   Jiang, Xinghao
   Li, Zhaohong
   Sun, Tanfeng
   Zhang, Zhenzhen
TI Multi-Channel HEVC Steganography by Minimizing IPM Steganographic
   Distortions
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE HEVC steganalysis; HEVC steganography; IPM
ID REPRESENTATIONS; ALGORITHM
AB Calibration is a common method for steganalysis, and Intra Prediction Mode (IPM) shift is a typical phenomenon used in calibration to detect video steganography. The current HEVC steganography lacks resistance to steganalysis based on this phenomenon because the new technology of HEVC introduces steganographic distortion in addition to providing more potential steganographic space. In this paper, an HEVC steganographic algorithm that resists IPM shift is proposed. First, we introduce the IPM shift in HEVC, and the previous H.264 steganalytic IPM shift feature is modeled and improved. By analyzing the HEVC encoding process, we found that modifying large-size blocks has a more significant impact on compression efficiency, while small ones are more sensitive to IPM optimality. Therefore, we perform the embedding channel division based on the block size and design the distortion function separately. In addition, we discover a unique IPM transition probability distribution in HEVC. According to our analysis, this unique distribution arises due to HEVC's MPM rules and the regularity of IPM direction. Modifying IPM in HEVC will change such distribution, thus, a mapping rule is designed based on this distribution to achieve a better embedding effect. Experimental results show that the channel division and proposed distortion function can effectively improve the overall performance. The proposed steganography outperforms the state-of-the-art steganography in resisting steganalysis, bitrate controlling, and visual quality.
C1 [Dong, Yi; Jiang, Xinghao; Sun, Tanfeng] Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Natl Engn Lab Informat Content Anal Tech, Shanghai 200240, Peoples R China.
   [Li, Zhaohong; Zhang, Zhenzhen] Beijing Jiaotong Univ, Beijing 100044, Peoples R China.
C3 Shanghai Jiao Tong University; Beijing Jiaotong University
RP Jiang, XH (corresponding author), Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Natl Engn Lab Informat Content Anal Tech, Shanghai 200240, Peoples R China.
EM aa44@sjtu.edu.cn; xhjiang@sjtu.edu.cn; zhhli2@bjtu.edu.cn;
   tfsun@sjtu.edu.cn; zhangzhenzhen@bjtu.edu.cn
RI Sun, Xinyu/JXX-2281-2024; feng, feng/KBR-1814-2024
FU National Natural Science Foundation of China [62002220]; Scientific
   Research Common Program of Beijing Municipal Commission of Education
   [KM202110015004]; BAIDU supports Ministry of Education's Education
   Cooperation Program [2012115PCK00690]
FX his work was supported in part by the National Natural Science
   Foundation of China under Grant 62002220, in part by the Scientific
   Research Common Program of Beijing Municipal Commission of Education
   under Grant KM202110015004, and in part by the BAIDU supports Ministry
   of Education's Education Cooperation Program under Grant
   2012115PCK00690.The Associate Editor coordinating the review of this
   manuscript and approving it for publication was Dr. Jiantao
   Zhou.(Corresponding author: Xinghao Jiang.).
CR Aayush M., 2019, P BRIT MACH VIS C, V9, P1
   Bouchama S, 2012, LECT NOTES ENG COMP, P655
   Cao MY, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20185242
   Chen Y, 2022, IEEE T DEPEND SECURE, V19, P2405, DOI 10.1109/TDSC.2021.3058134
   Dong Y., 2018, INT WORKSHOP DIGIT W, P233
   Filler T, 2011, IEEE T INF FOREN SEC, V6, P920, DOI 10.1109/TIFS.2011.2134094
   Fridrich J, 2012, IEEE T INF FOREN SEC, V7, P868, DOI 10.1109/TIFS.2012.2190402
   He PS, 2021, IEEE T MULTIMEDIA, V23, P3179, DOI 10.1109/TMM.2020.3021234
   He PS, 2020, IEEE T CIRC SYST VID, V30, P4034, DOI 10.1109/TCSVT.2019.2951630
   Kim IK, 2012, IEEE T CIRC SYST VID, V22, P1697, DOI 10.1109/TCSVT.2012.2223011
   [孔维国 Kong Weiguo], 2014, [四川大学学报. 自然科学版, Journal of Sichuan University. Natural Science Edition], V51, P1183
   Liu JD, 2022, IEEE T MULTIMEDIA, V24, P2084, DOI 10.1109/TMM.2021.3075858
   Liu P, 2020, IOP CONF SER-MAT SCI, V719, DOI 10.1088/1757-899X/719/1/012068
   Liu YX, 2019, MULTIMED TOOLS APPL, V78, P6459, DOI 10.1007/s11042-018-6320-y
   Nie QK, 2018, CMC-COMPUT MATER CON, V55, P59, DOI 10.3970/cmc.2018.055.059
   Tang WX, 2019, IEEE T INF FOREN SEC, V14, P2074, DOI 10.1109/TIFS.2019.2891237
   Wang J, 2019, IEEE ACCESS, V7, P119393, DOI 10.1109/ACCESS.2019.2936614
   Wang Y, 2018, PROCEEDINGS OF THE 6TH ACM WORKSHOP ON INFORMATION HIDING AND MULTIMEDIA SECURITY (IH&MMSEC'18), P97, DOI 10.1145/3206004.3206020
   Wu ST, 2020, IEEE T MULTIMEDIA, V22, P256, DOI 10.1109/TMM.2019.2920605
   Xu DW, 2012, J REAL-TIME IMAGE PR, V7, P205, DOI 10.1007/s11554-010-0175-4
   Yanbin Zhao, 2016, Digital Forensics and Watermarking. 14th International Workshop, IWDW 2015. Revised Selected Papers: LNCS 9569, P119, DOI 10.1007/978-3-319-31960-5_11
   Yang GB, 2011, AEU-INT J ELECTRON C, V65, P331, DOI 10.1016/j.aeue.2010.03.011
   Ye J, 2017, IEEE T INF FOREN SEC, V12, P2545, DOI 10.1109/TIFS.2017.2710946
   [尹秋来 Yin Qiulai], 2012, [光电子·激光, Journal of Optoelectronics·Laser], V23, P2194
   Zhai LM, 2020, IEEE T INF FOREN SEC, V15, P1762, DOI 10.1109/TIFS.2019.2949428
   Zhang LY, 2020, MULTIMED TOOLS APPL, V79, P12659, DOI 10.1007/s11042-019-08528-7
   Zhang ZJ, 2021, EURASIP J WIREL COMM, V2021, DOI 10.1186/s13638-021-01895-6
   Zhou H, 2019, IEEE T MULTIMEDIA, V21, P1384, DOI 10.1109/TMM.2018.2882088
NR 28
TC 16
Z9 16
U1 4
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2698
EP 2709
DI 10.1109/TMM.2022.3150180
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA M5LW1
UT WOS:001030640600019
DA 2024-07-18
ER

PT J
AU Du, C
   Graham, S
   Depp, C
   Nguyen, T
AF Du, Chen
   Graham, Sarah
   Depp, Colin
   Nguyen, Truong
TI View-Invariant Center-of-Pressure Metrics Estimation With Monocular RGB
   Camera
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE View-invariant action evaluation; center of pressure; computer vision;
   pose estimation; body landmarks; balance control; motion capture
ID BTRACKS BALANCE PLATE; ACTION RECOGNITION; NETWORKS; POSTURE; SWAY
AB Center of pressure (CoP) metrics, including CoP path length and sway area, have been used as gold standard measurements of postural and balance control in biomechanical studies. A recent study of computer-vision-based CoP metrics estimation from 3D body landmark sequences offers a more portable and comprehensive solution than conventional force plate methods to obtain these important metrics for real-time evaluation of balance control. However, obtaining accurate 3D body landmarks requires a calibrated motion capture system or on-body markers, which involves lengthy data collection and processing time and limits their implementation in home and clinical environments. Existing methods that instead use 2D body landmarks fail to adapt to different camera positions. To overcome these challenges, we propose a view-invariant deep learning framework for video-level CoP metrics estimation, including CoP path length and sway area, using pose dimension lifting and graph convolutional network (GCN). This work is the first step toward obtaining gold-standard CoP metrics with an accessible, monocular RGB camera. We propose to use a dimension lifting convolutional neural network (CNN) to obtain view-invariant 3D body landmark features from 2D body landmarks. We also propose a two-stream regression model using GCN and discrete cosine transform (DCT) for a robust CoP metrics estimation. To facilitate the line of research, we release a novel multi-view body landmark dataset containing 2D body landmarks of a wide variety of action patterns from four different camera views with synchronized CoP labels and corresponding 3D body landmarks, which enables cross-view evaluation with different camera angles. We subsequently validate the proposed method through a cross-dataset training by training the dimension lifting model on an existing balance dataset and evaluating the CoP metrics estimation on the multi-view body landmark dataset. The experiments validate that our framework achieves state-of-the-art accuracy for both CoP path length and CoP sway area using a monocular RGB camera input for unseen views.
C1 [Du, Chen; Nguyen, Truong] Univ Calif San Diego, Dept Elect & Comp Engn, San Diego, CA 92093 USA.
   [Graham, Sarah] Lark Hlth, Mountain View, CA 94040 USA.
   [Depp, Colin] Univ Calif San Diego, Dept Psychiat, San Diego, CA 92103 USA.
   [Depp, Colin] Univ Calif San Diego, Sam & Rose Stein Inst Res Aging, San Diego, CA USA.
   [Depp, Colin] VA San Diego Healthcare Syst, San Diego, CA 92161 USA.
C3 University of California System; University of California San Diego;
   University of California System; University of California San Diego;
   University of California System; University of California San Diego; US
   Department of Veterans Affairs; Veterans Health Administration (VHA); VA
   San Diego Healthcare System
RP Nguyen, T (corresponding author), Univ Calif San Diego, Dept Elect & Comp Engn, San Diego, CA 92093 USA.
EM c9du@eng.ucsd.edu; sarah.graham@lark.com; cdepp@health.ucsd.edu;
   tqn001@eng.ucsd.edu
RI Nguyen, Truong/JXN-9786-2024
OI Nguyen, Truong/0000-0002-5022-063X
FU IBM Research AI
FX No Statement Available
CR AHMED N, 1974, IEEE T COMPUT, VC 23, P90, DOI 10.1109/T-C.1974.223784
   Amori V, 2015, GAIT POSTURE, V41, P19, DOI 10.1016/j.gaitpost.2014.08.003
   Baptista R, 2019, INT CONF ACOUST SPEE, P2542, DOI 10.1109/ICASSP.2019.8682904
   Cai XY, 2016, IEEE T MULTIMEDIA, V18, P141, DOI 10.1109/TMM.2015.2505089
   Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   cdc, Keep on your feet-preventing older adult falls
   CHEW V, 1966, J AM STAT ASSOC, V61, P605, DOI 10.2307/2282774
   Cunha BP, 2012, NEUROSCI LETT, V513, P6, DOI 10.1016/j.neulet.2012.01.053
   Dong LJ, 2021, KNOWL-BASED SYST, V229, DOI 10.1016/j.knosys.2021.107388
   dos Santos DA, 2017, PEERJ, V5, DOI 10.7717/peerj.3626
   Du C, 2022, IEEE T MULTIMEDIA, V24, P2018, DOI 10.1109/TMM.2021.3075025
   Du C, 2021, IEEE ENG MED BIO, P281, DOI 10.1109/EMBC46164.2021.9629569
   Du C, 2020, INT CONF ACOUST SPEE, P2313, DOI [10.1109/ICASSP40776.2020.9053764, 10.1109/icassp40776.2020.9053764]
   Duarte M, 2010, BRAZ J PHYS THER, V14, P183, DOI 10.1590/S1413-35552010000300003
   Funk C, 2019, Arxiv, DOI arXiv:1811.12607
   Goble DJ, 2016, INT J SPORTS PHYS TH, V11, P149
   Graham SA, 2015, J AUTISM DEV DISORD, V45, P1419, DOI 10.1007/s10803-014-2303-7
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Gupta A, 2014, PROC CVPR IEEE, P2601, DOI 10.1109/CVPR.2014.333
   Haid T, 2018, ENTROPY-SWITZ, V20, DOI 10.3390/e20010030
   He ZY, 2009, IEEE SYS MAN CYBERN, P5041, DOI 10.1109/ICSMC.2009.5346042
   Hsu WL, 2007, J NEUROPHYSIOL, V97, P3024, DOI 10.1152/jn.01142.2006
   Hu GY, 2020, IEEE T MULTIMEDIA, V22, P2207, DOI 10.1109/TMM.2019.2953325
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Jibin Gao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P222, DOI 10.1007/978-3-030-58577-8_14
   Kanekar N, 2014, EXP BRAIN RES, V232, P1127, DOI 10.1007/s00221-014-3822-3
   Li M, 2016, IEEE T MULTIMEDIA, V18, P2293, DOI 10.1109/TMM.2016.2614228
   Li MP, 2019, IEEE T MULTIMEDIA, V21, P2653, DOI 10.1109/TMM.2019.2903455
   Liao YL, 2020, IEEE T NEUR SYS REH, V28, P468, DOI 10.1109/TNSRE.2020.2966249
   Lin CW, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20195588
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P2684, DOI 10.1109/TPAMI.2019.2916873
   Liu K, 2021, IEEE T MULTIMEDIA, V23, P64, DOI 10.1109/TMM.2020.2974323
   Mehta D, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073596
   Melzer I, 2004, AGE AGEING, V33, P602, DOI 10.1093/ageing/afh218
   Ning GH, 2018, IEEE T MULTIMEDIA, V20, P1246, DOI 10.1109/TMM.2017.2762010
   O'Connor SM, 2016, J BIOMECH, V49, P4142, DOI 10.1016/j.jbiomech.2016.10.020
   Pan JH, 2019, IEEE I CONF COMP VIS, P6340, DOI 10.1109/ICCV.2019.00643
   Parmar P, 2019, PROC CVPR IEEE, P304, DOI 10.1109/CVPR.2019.00039
   Parmar P, 2019, IEEE WINT CONF APPL, P1468, DOI 10.1109/WACV.2019.00161
   Pavllo D, 2019, PROC CVPR IEEE, P7745, DOI 10.1109/CVPR.2019.00794
   Pirsiavash H, 2014, LECT NOTES COMPUT SC, V8694, P556, DOI 10.1007/978-3-319-10599-4_36
   Prioli AC, 2006, HUM MOVEMENT SCI, V25, P435, DOI 10.1016/j.humov.2006.03.003
   Quinn L, 2021, PHYS THER, V101, DOI 10.1093/ptj/pzab154
   Rahmani H, 2018, IEEE T PATTERN ANAL, V40, P667, DOI 10.1109/TPAMI.2017.2691768
   Rahmani H, 2015, PROC CVPR IEEE, P2458, DOI 10.1109/CVPR.2015.7298860
   Rogind H, 2003, CLIN PHYSIOL FUNCT I, V23, P171, DOI 10.1046/j.1475-097X.2003.00492.x
   Ruhe A, 2011, EUR SPINE J, V20, P358, DOI 10.1007/s00586-010-1543-2
   Schmit JM, 2006, EXP BRAIN RES, V168, P357, DOI 10.1007/s00221-005-0094-y
   Schubert P, 2014, GAIT POSTURE, V39, P518, DOI 10.1016/j.gaitpost.2013.09.001
   Scott J, 2020, Arxiv, DOI arXiv:2001.00657
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Shi L, 2019, PROC CVPR IEEE, P12018, DOI 10.1109/CVPR.2019.01230
   Si CY, 2018, LECT NOTES COMPUT SC, V11205, P106, DOI 10.1007/978-3-030-01246-5_7
   Simonyan K., 2014, P 27 INT C NEUR INF, P568, DOI DOI 10.1002/14651858.CD001941.PUB3
   Tang YS, 2018, PROC CVPR IEEE, P5323, DOI 10.1109/CVPR.2018.00558
   Wang J, 2014, PROC CVPR IEEE, P2649, DOI 10.1109/CVPR.2014.339
   Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813
   Wang SL, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4902, DOI 10.1145/3474085.3475438
   Wei W., 2019, P IEEE INT C HEALTHC, P1
   Wei WC, 2020, IEEE ACCESS, V8, P99889, DOI 10.1109/ACCESS.2020.2997341
   Winter DA, 1996, J NEUROPHYSIOL, V75, P2334, DOI 10.1152/jn.1996.75.6.2334
   WINTER DA, 1995, GAIT POSTURE, V3, P193
   Wu Y., 2021, Detectron2
   Wu YP, 2020, IEEE T MULTIMEDIA, V22, P2177, DOI 10.1109/TMM.2019.2953380
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Zhang SY, 2018, IEEE T MULTIMEDIA, V20, P2330, DOI 10.1109/TMM.2018.2802648
NR 66
TC 1
Z9 1
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7388
EP 7401
DI 10.1109/TMM.2022.3222681
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP5R1
UT WOS:001133278300016
DA 2024-07-18
ER

PT J
AU Fan, B
   Yang, YZ
   Feng, WS
   Wu, FC
   Lu, JW
   Liu, HM
AF Fan, Bin
   Yang, Yuzhu
   Feng, Wensen
   Wu, Fuchao
   Lu, Jiwen
   Liu, Hongmin
TI Seeing Through Darkness: Visual Localization at Night via Weakly
   Supervised Learning of Domain Invariant Features
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Domain invariant local features; image matching; long-term visual
   localization; weakly supervised learning
ID OBJECT DETECTION; ACCURATE; DESCRIPTORS; SLAM
AB Long term visual localization has to conquer the problem of matching images with dramatic photometric changes caused by different seasons, natural and man-made illumination changes, etc. Visual localization at night plays a vital role in many applications like autonomous driving and augmented reality, for which extracting keypoints and descriptors with robustness to day-night illumination changes has became the bottleneck. This paper proposes an adversarial learning based solution to harvest from the weakly domain labels of day and night images, along with the point level correspondences among day time images, to achieve robust local feature extraction and description across day-night images. The key idea is to learn a discriminator to distinguish whether a feature map is generated from the day or night images, and simultaneously to adjust the parameters of feature extraction network so as to fool the discriminator. After adversarial training of the discriminator and feature extraction network, the feature extraction network finally reaches a stable status so that the extracted feature maps are robust to day-night photometric changes, based on which day-night domain invariant keypoints and descriptors can be extracted. Compared to existing local feature learning methods, it only requires an additional set of easily captured night images to improve the domain invariance of learned features. Experiments on two challenging benchmarks show the effectiveness of proposed method. In addition, this paper revisits the widely used image matching metrics on HPatches and finds that recall of different methods is highly related to their relative localization performance.
C1 [Fan, Bin; Yang, Yuzhu; Liu, Hongmin] Univ Sci & Technol Beijing, Sch Automat & Elect Engn, Beijing 100083, Peoples R China.
   [Feng, Wensen] Cent Media Technol Inst, Shenzhen 518129, Peoples R China.
   [Wu, Fuchao] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Lu, Jiwen] Tsinghua Univ, Dept Automat, Beijing 100084, Peoples R China.
C3 University of Science & Technology Beijing; Chinese Academy of Sciences;
   Institute of Automation, CAS; Tsinghua University
RP Liu, HM (corresponding author), Univ Sci & Technol Beijing, Sch Automat & Elect Engn, Beijing 100083, Peoples R China.
EM bin.fan@ieee.org; yyz15600992348@163.com; fengwensen@huawei.com;
   fcwu@nlpr.ia.ac.cn; lujiwen@tsinghua.edu.cn; hmliu_82@163.com
RI Li, YiXue/JRW-6306-2023; Lin, Xiaoqi/KFS-5750-2024; Fan,
   Bin/HKN-3438-2023; Zhang, Chi/JSK-0744-2023; ZHU, JIALI/JNE-3065-2023;
   Lu, Lu/JPE-5187-2023; Chen, Fang/JZE-4446-2024; cheng,
   chen/JHS-9462-2023; zhang, jt/JVE-1333-2024; Lu, Jiwen/C-5291-2009
OI Liu, Hongmin/0000-0001-9834-4087; Lu, Jiwen/0000-0002-6121-5529; fan,
   bin/0000-0002-1155-467X
FU National Key Research and Development Program of China [2020YFB1313002];
   National Natural Science Foundation of China [61876180, U2013202,
   62076026]; Beijing Natural Science Foundation [4202073]; Guangdong Basic
   and Applied Basic Research Foundation [2020B1515120050]; Fundamental
   Research Funds for the Central Universities [FRF-TP-20-08B]; CAAI-Huawei
   MindSpore Open Fund
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2020YFB1313002, in part by the
   National Natural Science Foundation of China under Grants 61876180,
   U2013202, and 62076026, in part by the Beijing Natural Science
   Foundation under Grant 4202073, in part by the Guangdong Basic and
   Applied Basic Research Foundation under Grant 2020B1515120050, in part
   by the Fundamental Research Funds for the Central Universities under
   Grant FRF-TP-20-08B, and in part by the CAAI-Huawei MindSpore Open Fund.
CR [Anonymous], LONG TERM VIS LOC BE
   Anoosheh A, 2019, IEEE INT CONF ROBOT, P5958, DOI [10.1109/ICRA.2019.8794387, 10.1109/icra.2019.8794387]
   Anoosheh A, 2018, IEEE COMPUT SOC CONF, P896, DOI 10.1109/CVPRW.2018.00122
   Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI [10.1109/TPAMI.2017.2711011, 10.1109/CVPR.2016.572]
   Baik S., 2020, PROC BRIT MACH VIS C
   Balntas V, 2017, PROC CVPR IEEE, P3852, DOI 10.1109/CVPR.2017.410
   Chaoyue Wang, 2020, IEEE Transactions on Artificial Intelligence, V1, P34, DOI 10.1109/TAI.2020.3031581
   Chen L, 2019, IEEE T MULTIMEDIA, V21, P2664, DOI 10.1109/TMM.2019.2907052
   Cheng G, 2020, IEEE T IMAGE PROCESS, V29, P5794, DOI 10.1109/TIP.2020.2987161
   DeTone D, 2018, IEEE COMPUT SOC CONF, P337, DOI 10.1109/CVPRW.2018.00060
   Duh PJ, 2021, IEEE T MULTIMEDIA, V23, P1567, DOI 10.1109/TMM.2020.3001500
   Dusmanu M, 2019, PROC CVPR IEEE, P8084, DOI 10.1109/CVPR.2019.00828
   Fan B, 2021, IEEE T MULTIMEDIA, V23, P2770, DOI 10.1109/TMM.2020.3016122
   Fan B, 2019, IEEE T IMAGE PROCESS, V28, P4774, DOI 10.1109/TIP.2019.2909640
   Fan B, 2012, IEEE T PATTERN ANAL, V34, P2031, DOI 10.1109/TPAMI.2011.277
   Gong XX, 2021, IEEE T MULTIMEDIA, V23, P2820, DOI 10.1109/TMM.2020.3017886
   Goodfellow I, 2014, ADV NEURAL INFORM PR, P2672
   Han JW, 2021, IEEE T PATTERN ANAL, V43, P1423, DOI 10.1109/TPAMI.2019.2949562
   Han JW, 2015, IEEE T GEOSCI REMOTE, V53, P3325, DOI 10.1109/TGRS.2014.2374218
   Hu HJ, 2019, IEEE INT C INT ROBOT, P3684, DOI [10.1109/iros40897.2019.8968047, 10.1109/IROS40897.2019.8968047]
   Kang C, 2019, IEEE T MULTIMEDIA, V21, P1563, DOI 10.1109/TMM.2018.2883868
   Kendall A, 2015, IEEE I CONF COMP VIS, P2938, DOI 10.1109/ICCV.2015.336
   Lepetit V, 2009, INT J COMPUT VISION, V81, P155, DOI 10.1007/s11263-008-0152-6
   Lin J., 2020, EUROPEAN C COMPUTER, P18
   Lin JX, 2021, IEEE T PATTERN ANAL, V43, P1254, DOI 10.1109/TPAMI.2019.2950198
   Liu HM, 2020, IEEE T CIRC SYST VID, V30, P3675, DOI 10.1109/TCSVT.2019.2943595
   Liu ZG, 2017, IEEE T MULTIMEDIA, V19, P874, DOI 10.1109/TMM.2016.2636750
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luo ZX, 2020, PROC CVPR IEEE, P6588, DOI 10.1109/CVPR42600.2020.00662
   Luo ZX, 2019, PROC CVPR IEEE, P2522, DOI 10.1109/CVPR.2019.00263
   Maddern W, 2017, INT J ROBOT RES, V36, P3, DOI 10.1177/0278364916679498
   Madeo S, 2017, IEEE T MULTIMEDIA, V19, P221, DOI 10.1109/TMM.2016.2615521
   Melekhov I, 2020, Arxiv, DOI arXiv:2008.06959
   Min WQ, 2020, IEEE T MULTIMEDIA, V22, P3128, DOI 10.1109/TMM.2020.2974326
   Mishchuk A., 2017, P ADV NEURAL INFORM, P4826
   Mishkin D, 2018, LECT NOTES COMPUT SC, V11213, P287, DOI 10.1007/978-3-030-01240-3_18
   Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671
   Piao JC, 2019, IEEE T MULTIMEDIA, V21, P2827, DOI 10.1109/TMM.2019.2913324
   Porav H, 2018, IEEE INT CONF ROBOT, P1011, DOI 10.1109/ICRA.2018.8462894
   Rana A, 2019, IEEE T MULTIMEDIA, V21, P256, DOI 10.1109/TMM.2018.2839885
   Revaud J, 2019, ADV NEUR IN, V32
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Salarian M, 2018, IEEE T MULTIMEDIA, V20, P3298, DOI 10.1109/TMM.2018.2839893
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Sarlin PE, 2020, PROC CVPR IEEE, P4937, DOI 10.1109/CVPR42600.2020.00499
   Sarlin PE, 2019, PROC CVPR IEEE, P12708, DOI 10.1109/CVPR.2019.01300
   Sattler T, 2018, PROC CVPR IEEE, P8601, DOI 10.1109/CVPR.2018.00897
   Schönberger JL, 2017, PROC CVPR IEEE, P6959, DOI 10.1109/CVPR.2017.736
   Schönberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shen TW, 2019, LECT NOTES COMPUT SC, V11361, P415, DOI 10.1007/978-3-030-20887-5_26
   Shermin T, 2021, IEEE T MULTIMEDIA, V23, P2732, DOI 10.1109/TMM.2020.3016126
   Song YF, 2016, IEEE T MULTIMEDIA, V18, P1542, DOI 10.1109/TMM.2016.2568743
   Tang L, 2020, IEEE INT CONF ROBOT, P1301, DOI [10.1109/ICRA40945.2020.9196518, 10.1109/icra40945.2020.9196518]
   Tian M, 2020, IEEE INT CONF ROBOT, P4211, DOI 10.1109/icra40945.2020.9196940
   Tian YR, 2019, PROC CVPR IEEE, P11008, DOI 10.1109/CVPR.2019.01127
   Tian YR, 2017, PROC CVPR IEEE, P6128, DOI 10.1109/CVPR.2017.649
   Toft C, 2022, IEEE T PATTERN ANAL, V44, P2074, DOI 10.1109/TPAMI.2020.3032010
   Wang ZH, 2016, IEEE T PATTERN ANAL, V38, P2198, DOI 10.1109/TPAMI.2015.2513396
   Wei G., 2021, P IEEE CVF C COMP VI, P16643
   Xiao B, 2019, IEEE T CIRC SYST VID, V29, P2796, DOI 10.1109/TCSVT.2018.2869841
   Yan HL, 2020, IEEE T MULTIMEDIA, V22, P2420, DOI 10.1109/TMM.2019.2953375
   Yan L, 2020, IEEE T GEOSCI REMOTE, V58, P3558, DOI 10.1109/TGRS.2019.2958123
   Yan L, 2018, IEEE IMAGE PROC, P1583, DOI 10.1109/ICIP.2018.8451010
   Yang F, 2021, IEEE T MULTIMEDIA, V23, P1681, DOI 10.1109/TMM.2020.3001522
   Yang S, 2023, IEEE T NEUR NET LEAR, V34, P1639, DOI 10.1109/TNNLS.2020.3010524
   Yu F, 2020, PROC CVPR IEEE, P2633, DOI 10.1109/CVPR42600.2020.00271
   Zhang DW, 2022, IEEE T PATTERN ANAL, V44, P3349, DOI 10.1109/TPAMI.2020.3046647
   Zhang DW, 2019, INT J COMPUT VISION, V127, P363, DOI 10.1007/s11263-018-1112-4
   Zhang DW, 2020, IEEE T PATTERN ANAL, V42, P475, DOI 10.1109/TPAMI.2018.2881114
NR 70
TC 17
Z9 17
U1 3
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1713
EP 1726
DI 10.1109/TMM.2022.3154165
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100009
DA 2024-07-18
ER

PT J
AU Guo, YL
   Wang, Y
   Wang, LG
   Wang, Z
   Cheng, C
AF Guo, Yulan
   Wang, Yun
   Wang, Longguang
   Wang, Zi
   Cheng, Chen
TI CVCNet: Learning Cost Volume Compression for Efficient Stereo Matching
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Computer vision; image matching; machine vision; robot vision systems;
   stereo vision
ID DEPTH; TIME
AB State-of-the-art deep learning based stereo matching algorithms usually rely on full-size cost volumes for highly accurate disparity estimation. The full-size cost volume processes all possible disparity candidates equally without considering their different matching uncertainties. Consequently, considerable redundant computation is involved on those candidates with very low matching uncertainties, making these methods difficult to be deployed in real-time applications. To tackle this problem, we propose CVCNet featuring an adaptive disparity range prediction module (ADR) and a disparity refinement module (DRM). The ADR adaptively predicts pixel-wise disparity range to discard the "unimportant" disparity candidates. It enables our network to obtain a compressed cost volume. Besides, the DRM improves disparity range prediction and refines the predicted disparity map. With the proposed modules, our CVCNet learns to build a compressed cost volume to achieve efficient disparity estimation. Experimental results on the KITTI and SceneFlow datasets show that our method achieves state-of-the-art performance, and runs at a significant order of magnitude faster speed than existing 3D CNN based methods. Particularly, our method ranks 1st on the KITTI 2012 and KITTI 2015 benchmarks among all published methods with running time shorter than 100 ms.
C1 [Guo, Yulan; Wang, Yun] Sun Yat Sen Univ, Sch Elect & Commun Engn, Shenzhen Campus, Shenzhen 510275, Peoples R China.
   [Wang, Longguang] Aviat Univ Air Force, Changchun 130012, Peoples R China.
   [Wang, Zi] Natl Univ Def Technol, Coll Aerosp Sci & Engn, Changsha 410073, Peoples R China.
   [Cheng, Chen] Xian Coll Technol, Xian 710049, Shaanxi, Peoples R China.
C3 Sun Yat Sen University; Aviation University Air Force; National
   University of Defense Technology - China
RP Guo, YL (corresponding author), Sun Yat Sen Univ, Sch Elect & Commun Engn, Shenzhen Campus, Shenzhen 510275, Peoples R China.
EM guoyulan@sysu.edu.cn; wangy978@mail2.sysu.edu.cn;
   wanglongguang15@nudt.edu.cn; wangzi16@nudt.edu.cn; cche943@sina.com
OI Wang, Yun/0000-0001-8384-6981; Wang, Zi/0000-0001-9081-7185
FU National Key Research and Development Program of China [2021YFB3100800];
   National Natural Science Foundation of China [U20A20185, 61972435];
   Guangdong Basic and Applied Basic Research Foundation [2022B1515020103];
   Shenzhen Science and Technology Program [RCYX20200714114641140]; Natural
   Science Foundation of Shaanxi Province [2019JQ-467]
FX This work was supported in part by the National Key Research and
   Development Program of China No. 2021YFB3100800, in part by the National
   Natural Science Foundation of China under Grants U20A20185 and 61972435,
   in part by the Guangdong Basic and Applied Basic Research Foundation
   under Grant 2022B1515020103, in part by the Shenzhen Science and
   Technology Program under Grant RCYX20200714114641140, and in part by the
   Natural Science Foundation of Shaanxi Province under Grant 2019JQ-467.
CR Alexiadis DS, 2013, IEEE T MULTIMEDIA, V15, P339, DOI 10.1109/TMM.2012.2229264
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Chang JR, 2018, PROC CVPR IEEE, P5410, DOI 10.1109/CVPR.2018.00567
   Chen BL, 2018, IEEE T MULTIMEDIA, V20, P2882, DOI 10.1109/TMM.2018.2825883
   Chen CR, 2019, IEEE I CONF COMP VIS, P8996, DOI 10.1109/ICCV.2019.00909
   Cheng S, 2020, PROC CVPR IEEE, P2521, DOI 10.1109/CVPR42600.2020.00260
   Cheng Xuelian, 2020, Advances in Neural Information Processing Systems, V33
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Duggal S, 2019, IEEE I CONF COMP VIS, P4383, DOI 10.1109/ICCV.2019.00448
   Farbiz F, 2005, IEEE T MULTIMEDIA, V7, P514, DOI 10.1109/TMM.2005.846787
   Forstmann S., 2004, PROC IEEE C COMPUT, P1
   Garg D., 2020, P ADV NEUR INF PROC, P22517
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Gu XD, 2020, PROC CVPR IEEE, P2492, DOI 10.1109/CVPR42600.2020.00257
   Guo XY, 2019, PROC CVPR IEEE, P3268, DOI 10.1109/CVPR.2019.00339
   Guo YL, 2021, IEEE T PATTERN ANAL, V43, P4338, DOI 10.1109/TPAMI.2020.3005434
   Hong L, 2004, PROC CVPR IEEE, P74
   Hosni A, 2009, IEEE IMAGE PROC, P2093, DOI 10.1109/ICIP.2009.5414478
   Jiao JB, 2014, IEEE MULTIMEDIA, V21, P16, DOI 10.1109/MMUL.2014.51
   Kendall A, 2017, IEEE I CONF COMP VIS, P66, DOI 10.1109/ICCV.2017.17
   Khamis S, 2018, LECT NOTES COMPUT SC, V11219, P596, DOI 10.1007/978-3-030-01267-0_35
   Kim H, 2021, IEEE T MULTIMEDIA, V23, P4117, DOI 10.1109/TMM.2020.3037537
   Kingma D. P., 2014, arXiv
   Li G, 2005, LECT NOTES COMPUT SC, V3757, P617
   Li J, 2019, IEEE T MULTIMEDIA, V21, P2531, DOI 10.1109/TMM.2019.2908350
   Liang ZF, 2018, PROC CVPR IEEE, P2811, DOI 10.1109/CVPR.2018.00297
   Luo WJ, 2016, PROC CVPR IEEE, P5695, DOI 10.1109/CVPR.2016.614
   Mao Y., 2021, PROC IEEE INT C COM, P6311
   Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438
   Menze M, 2015, PROC CVPR IEEE, P3061, DOI 10.1109/CVPR.2015.7298925
   Pang JH, 2017, IEEE INT CONF COMP V, P878, DOI 10.1109/ICCVW.2017.108
   Rao ZB, 2020, APSIPA TRANS SIGNAL, V9, DOI 10.1017/ATSIP.2020.16
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Roy S, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P492, DOI 10.1109/ICCV.1998.710763
   Scharstein D, 2001, IEEE WORKSHOP ON STEREO AND MULTI-BASELINE VISION, PROCEEDINGS, P131, DOI 10.1023/A:1014573219977
   Shen ZL, 2021, PROC CVPR IEEE, P13901, DOI 10.1109/CVPR46437.2021.01369
   Tankovich V, 2021, PROC CVPR IEEE, P14357, DOI 10.1109/CVPR46437.2021.01413
   Tosi F, 2021, PROC CVPR IEEE, P8938, DOI 10.1109/CVPR46437.2021.00883
   Wang FJ, 2021, PROC CVPR IEEE, P14189, DOI 10.1109/CVPR46437.2021.01397
   Wang LG, 2022, IEEE T PATTERN ANAL, V44, P2108, DOI 10.1109/TPAMI.2020.3026899
   Wang Q, 2020, IEEE INT CONF ROBOT, P101, DOI [10.1109/icra40945.2020.9197031, 10.1109/ICRA40945.2020.9197031]
   Wang Y, 2019, IEEE INT CONF ROBOT, P5893, DOI [10.1109/ICRA.2019.8794003, 10.1109/icra.2019.8794003]
   Wang Y, 2022, IEEE ROBOT AUTOM LET, V7, P6258, DOI 10.1109/LRA.2022.3164755
   Xu B, 2021, PROC CVPR IEEE, P12492, DOI 10.1109/CVPR46437.2021.01231
   Xu HF, 2020, PROC CVPR IEEE, P1956, DOI 10.1109/CVPR42600.2020.00203
   Yang GS, 2019, PROC CVPR IEEE, P5510, DOI 10.1109/CVPR.2019.00566
   Yedidia J.S., 2003, EXPLORING ARTIFICIAL, P239, DOI DOI 10.5555/779343.779352
   Yoon KJ, 2005, PROC CVPR IEEE, P924
   Zbontar J, 2016, J MACH LEARN RES, V17
   Zhang FH, 2019, PROC CVPR IEEE, P185, DOI 10.1109/CVPR.2019.00027
   Zhang S., 2021, PROC IEEECVF C COMP, P5433
   Zhang Y., 2008, PROC IEEE INT C PAT, P1
   Zhang YM, 2020, AAAI CONF ARTIF INTE, V34, P12926
NR 53
TC 0
Z9 0
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7786
EP 7799
DI 10.1109/TMM.2022.3228169
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AV4K5
UT WOS:001121212400014
DA 2024-07-18
ER

PT J
AU Li, Q
   Zhang, CQ
   Hu, QH
   Fu, HZ
   Zhu, PF
AF Li, Qing
   Zhang, Changqing
   Hu, Qinghua
   Fu, Huazhu
   Zhu, Pengfei
TI Confidence-Aware Fusion Using Dempster-Shafer Theory for Multispectral
   Pedestrian Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multimodal learning; pedestrian detection; fusion learning
AB Multispectral pedestrian detection is an important and valuable task in many applications, which could provide a more accurate and reliable pedestrian detection result by using the complementary visual information from color and thermal images. However, it faces two open and difficult challenges: 1) how to effectively and dynamically integrate multispectral information according to the confidence of different modalities, and 2) how to produce a reliable prediction result. In this paper, we propose a novel confidence-aware multispectral pedestrian detection (CMPD) method, which flexibly learns the multispectral representation while simultaneously producing a reliable result with confidence estimation. Specifically, a dense fusion strategy is first proposed to extract the multilevel multispectral representation at the feature level. Then, an additional confidence subnetwork is utilized to dynamically estimate the detection confidence for each modality. Finally, Dempster's combination rule is introduced to fuse the results of different branches according to the rectified confidence. Our proposed CMPD method not only effectively integrates multimodal information but also provides a reliable prediction. Extensive experimental results demonstrate the efficiency of our algorithm compared with state-of-the-art methods.
C1 [Li, Qing; Zhang, Changqing; Hu, Qinghua; Zhu, Pengfei] Tianjin Univ, Coll Intelligence & Comp, Tianjin 300072, Peoples R China.
   [Fu, Huazhu] Agcy Sci Technol & Res, Inst Infocomm Res, Singapore 138632, Singapore.
C3 Tianjin University; Agency for Science Technology & Research (A*STAR);
   A*STAR - Institute for Infocomm Research (I2R)
RP Zhang, CQ (corresponding author), Tianjin Univ, Coll Intelligence & Comp, Tianjin 300072, Peoples R China.
EM liqing0315@tju.edu.cn; zhangchangqing@tju.edu.cn; huqinghua@tju.edu.cn;
   huazhufu@gmail.com; zhupengfei@tju.edu.cn
RI Fu, Huazhu/A-1411-2014; Zhang, Chang/HTO-2939-2023; Hu,
   Qinghua/B-8857-2008; ren, jing/JXN-8411-2024
OI Fu, Huazhu/0000-0002-9702-5524; Hu, Qinghua/0000-0001-7765-8095; Li,
   Qing/0000-0002-5003-5944
FU National Key Research and Development Program of China [2019YFB2101900];
   National Natural Science Foundation of China [61976151, 61732011]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2019YFB2101900, and in part by
   the National Natural Science Foundation of China under Grants 61976151
   and 61732011.
CR Ahmed E, 2015, PROC CVPR IEEE, P3908, DOI 10.1109/CVPR.2015.7299016
   Benenson R, 2012, PROC CVPR IEEE, P2903, DOI 10.1109/CVPR.2012.6248017
   Bodla N, 2017, IEEE I CONF COMP VIS, P5562, DOI 10.1109/ICCV.2017.593
   Cheng YH, 2017, PROC CVPR IEEE, P1475, DOI 10.1109/CVPR.2017.161
   Choi J, 2019, IEEE I CONF COMP VIS, P502, DOI 10.1109/ICCV.2019.00059
   Cong RM, 2019, IEEE T CIRC SYST VID, V29, P2941, DOI 10.1109/TCSVT.2018.2870832
   Corbière C, 2019, ADV NEUR IN, V32
   DEMPSTER AP, 1968, J ROY STAT SOC B, V30, P205
   Dempster AP, 2008, STUD FUZZ SOFT COMP, V219, P57
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Feng D, 2019, IEEE INT VEH SYM, P1280, DOI [10.1109/IVS.2019.8814046, 10.1109/ivs.2019.8814046]
   Gal Y, 2016, PR MACH LEARN RES, V48
   Glorot X., 2010, INT C ARTIFICIAL INT, P249
   González A, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16060820
   Guan DY, 2019, INFORM FUSION, V50, P148, DOI 10.1016/j.inffus.2018.11.017
   Ha Q, 2017, IEEE INT C INT ROBOT, P5108, DOI 10.1109/IROS.2017.8206396
   Hao X, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16383, DOI 10.1109/ICCV48922.2021.01609
   Harakeh A, 2020, IEEE INT CONF ROBOT, P87, DOI [10.1109/icra40945.2020.9196544, 10.1109/ICRA40945.2020.9196544]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He YH, 2019, PROC CVPR IEEE, P2883, DOI 10.1109/CVPR.2019.00300
   Hendrycks D., 2017, 5 INT C LEARNING REP
   Huang X., 2020, P IEEE CVF C COMP VI, P10747, DOI DOI 10.1109/CVPR42600.2020.01076
   Hwang S, 2015, PROC CVPR IEEE, P1037, DOI 10.1109/CVPR.2015.7298706
   Jian LH, 2021, IEEE T MULTIMEDIA, V24, P3314, DOI 10.1109/TMM.2021.3096088
   Josang A., 2012, 2012 15th International Conference on Information Fusion (FUSION 2012), P1225
   Kailai Zhou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P787, DOI 10.1007/978-3-030-58523-5_46
   Kingma D. P., 2014, arXiv
   Koenig D, 2017, IEEE COMPUT SOC CONF, P243, DOI 10.1109/CVPRW.2017.36
   Lakshminarayanan B, 2017, ADV NEUR IN, V30
   Le MT, 2018, IEEE INT C INTELL TR, P3873, DOI 10.1109/ITSC.2018.8569637
   Lee YW, 2022, Arxiv, DOI arXiv:2006.15607
   Li C., 2018, PROC BRIT MACH VIS C, P225
   Li CY, 2019, PATTERN RECOGN, V85, P161, DOI 10.1016/j.patcog.2018.08.005
   Li JN, 2018, IEEE T MULTIMEDIA, V20, P985, DOI 10.1109/TMM.2017.2759508
   Lin M, 2014, 2014 INTERNATIONAL CONFERENCE ON MEDICAL BIOMETRICS (ICMB 2014), P1, DOI 10.1109/ICMB.2014.8
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu J., 2016, ARXIV161102644, P1, DOI DOI 10.5244/C.30.73
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Meyer GP, 2020, IEEE INT C INT ROBOT, P10521, DOI 10.1109/IROS45743.2020.9341623
   Meyer GP, 2019, PROC CVPR IEEE, P12669, DOI 10.1109/CVPR.2019.01296
   Neubeck A, 2006, INT C PATT RECOG, P850, DOI 10.1109/icpr.2006.479
   Ouyang WL, 2013, IEEE I CONF COMP VIS, P2056, DOI 10.1109/ICCV.2013.257
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sentz Kari, 2002, COMBINATION EVIDENCE, V4015, P2
   Shao ZF, 2022, IEEE T MULTIMEDIA, V24, P2069, DOI 10.1109/TMM.2021.3075566
   Shridhar K, 2019, A comprehensive guide to bayesian convolutional neural network with variational inference, DOI DOI 10.48550/ARXIV.1901.02731
   Solovyev R, 2021, IMAGE VISION COMPUT, V107, DOI 10.1016/j.imavis.2021.104117
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Wang J, 2022, IEEE T CIRC SYST VID, V32, P2949, DOI 10.1109/TCSVT.2021.3099120
   Wu DM, 2022, IEEE T INF FOREN SEC, V17, P115, DOI 10.1109/TIFS.2021.3075894
   Wu HD, 2002, IEEE IMTC P, P7
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P924, DOI 10.1109/TPAMI.2020.3013379
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Ye M, 2021, IEEE T INF FOREN SEC, V16, P728, DOI 10.1109/TIFS.2020.3001665
   Zhang H, 2020, IEEE IMAGE PROC, P276, DOI [10.1109/ICIP40778.2020.9191080, 10.1109/icip40778.2020.9191080]
   Zhang LL, 2016, LECT NOTES COMPUT SC, V9906, P443, DOI 10.1007/978-3-319-46475-6_28
   Zhang L, 2019, IEEE I CONF COMP VIS, P5126, DOI 10.1109/ICCV.2019.00523
   Zhang L, 2019, INFORM FUSION, V50, P20, DOI 10.1016/j.inffus.2018.09.015
   Zhang Q, 2021, PROC CVPR IEEE, P2633, DOI 10.1109/CVPR46437.2021.00266
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
NR 60
TC 15
Z9 15
U1 7
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3420
EP 3431
DI 10.1109/TMM.2022.3160589
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA O7RP6
UT WOS:001045742200037
DA 2024-07-18
ER

PT J
AU Liu, C
   Jiang, XD
   Ding, HH
AF Liu, Chang
   Jiang, Xudong
   Ding, Henghui
TI Instance-Specific Feature Propagation for Referring Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Image segmentation; Fuses; Visualization; Task
   analysis; Linguistics; Proposals; Feature propagation;
   instance-specific; referring segmentation
AB Referring segmentation aims to generate a segmentation mask for the target instance indicated by a natural language expression. There are typically two kinds of existing methods: one-stage methods that directly perform segmentation on the fused vision and language features; and two-stage methods that first utilize an instance segmentation model for instance proposal and then select one of these instances via matching them with language features. In this work, we propose a novel framework that simultaneously detects the target-of-interest via feature propagation and generates a fine-grained segmentation mask. In our framework, each instance is represented by an Instance-Specific Feature (ISF), and the target-of-referring is identified by exchanging information among all ISFs using our proposed Feature Propagation Module (FPM). Our instance-aware approach learns the relationship among all objects, which helps to better locate the target-of-interest than one-stage methods. Comparing to two-stage methods, our approach collaboratively and interactively utilizes both vision and language information for synchronous identification and segmentation. In the experimental tests, our method outperforms previous state-of-the-art methods on all three RefCOCO series datasets.
C1 [Liu, Chang; Jiang, Xudong; Ding, Henghui] Nanyang Technol Univ, Sch Elect & Elect Engn EEE, Singapore 639798, Singapore.
C3 Nanyang Technological University
RP Ding, HH (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn EEE, Singapore 639798, Singapore.
EM liuc0058@e.ntu.edu.sg; exdjiang@ntu.edu.sg; ding0093@ntu.edu.sg
RI Jiang, Xudong/B-1555-2008; Ding, Henghui/C-7486-2019
OI Jiang, Xudong/0000-0002-9104-2315; Ding, Henghui/0000-0003-4868-6526
CR [Anonymous], 2014, NEURIPS WORKSHOP
   Chen DJ, 2019, IEEE I CONF COMP VIS, P7453, DOI 10.1109/ICCV.2019.00755
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen Y., 2019, BRIT MACH VISION C
   Chenyun Wu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10213, DOI 10.1109/CVPR42600.2020.01023
   Ding HH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16301, DOI 10.1109/ICCV48922.2021.01601
   Ding HH, 2019, IEEE I CONF COMP VIS, P6818, DOI 10.1109/ICCV.2019.00692
   Ding HH, 2019, PROC CVPR IEEE, P8877, DOI 10.1109/CVPR.2019.00909
   Ding HH, 2020, IEEE T IMAGE PROCESS, V29, P3520, DOI 10.1109/TIP.2019.2962685
   Ding HH, 2018, PROC CVPR IEEE, P2393, DOI 10.1109/CVPR.2018.00254
   Gen Luo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10031, DOI 10.1109/CVPR42600.2020.01005
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   HU P, 2020, P NEURIPS, P21713
   Hu P, 2020, IEEE WINT CONF APPL, P1893, DOI 10.1109/WACV45572.2020.9093333
   Hu RH, 2016, PROC CVPR IEEE, P4555, DOI 10.1109/CVPR.2016.493
   Hu RH, 2016, LECT NOTES COMPUT SC, V9905, P108, DOI 10.1007/978-3-319-46448-0_7
   Hu ZW, 2020, PROC CVPR IEEE, P4423, DOI 10.1109/CVPR42600.2020.00448
   Huang S., 2020, 2020 IEEE CVF C COMP, P10485, DOI [DOI 10.1109/CVPR42600.2020.01050, 10.1109/CVPR42600.2020.01050]
   Hui T., 2020, COMPUTER VISION ECCV, P59
   Kuen J, 2019, IEEE I CONF COMP VIS, P6043, DOI 10.1109/ICCV.2019.00614
   Li RY, 2018, PROC CVPR IEEE, P5745, DOI 10.1109/CVPR.2018.00602
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Liu CX, 2017, IEEE I CONF COMP VIS, P1280, DOI 10.1109/ICCV.2017.143
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Luo G, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1274, DOI 10.1145/3394171.3414006
   Luo RT, 2017, PROC CVPR IEEE, P3125, DOI 10.1109/CVPR.2017.333
   Mao JH, 2016, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2016.9
   Margffoy-Tuay E, 2018, LECT NOTES COMPUT SC, V11215, P656, DOI 10.1007/978-3-030-01252-6_39
   Nagaraja VK, 2016, LECT NOTES COMPUT SC, V9908, P792, DOI 10.1007/978-3-319-46493-0_48
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Redmon J., 2016, PROC CVPR IEEE, DOI [10.1109/CVPR.2016.91, DOI 10.1109/CVPR.2016.91]
   Shi HC, 2018, LECT NOTES COMPUT SC, V11210, P38, DOI 10.1007/978-3-030-01231-1_3
   Wang P, 2019, PROC CVPR IEEE, P1960, DOI 10.1109/CVPR.2019.00206
   Xinlong Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P649, DOI 10.1007/978-3-030-58523-5_38
   Yang SB, 2019, IEEE I CONF COMP VIS, P4643, DOI 10.1109/ICCV.2019.00474
   Ye LW, 2020, IEEE T MULTIMEDIA, V22, P3224, DOI 10.1109/TMM.2020.2971171
   Ye LW, 2019, PROC CVPR IEEE, P10494, DOI 10.1109/CVPR.2019.01075
   Yu LC, 2018, PROC CVPR IEEE, P1307, DOI 10.1109/CVPR.2018.00142
   Yu LC, 2017, PROC CVPR IEEE, P3521, DOI 10.1109/CVPR.2017.375
   Yu LC, 2016, LECT NOTES COMPUT SC, V9906, P69, DOI 10.1007/978-3-319-46475-6_5
   Zhang H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6954, DOI 10.1109/ICCV48922.2021.00689
   Zhang YT, 2017, PROC CVPR IEEE, P1090, DOI 10.1109/CVPR.2017.122
NR 43
TC 11
Z9 11
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3657
EP 3667
DI 10.1109/TMM.2022.3163578
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FE2H4
UT WOS:001144015500011
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Liu, Q
   Su, HL
   Chen, TX
   Yuan, H
   Hamzaoui, R
AF Liu, Qi
   Su, Honglei
   Chen, Tianxin
   Yuan, Hui
   Hamzaoui, Raouf
TI No-Reference Bitstream-Layer Model for Perceptual Quality Assessment of
   V-PCC Encoded Point Clouds
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Point cloud compression; image quality assessment; bitstream-based
   quality model; V-PCC
ID NETWORKED VIDEO; SEGMENTATION; DATABASE
AB No-reference bitstream-layer models for point cloud quality assessment (PCQA) use the information extracted from a bitstream for real-time and nonintrusive quality monitoring. We propose a no-reference bitstream-layer model for the perceptual quality assessment of video-based point cloud compression (V-PCC) encoded point clouds. First, we study the relationship between the perceptual coding distortion and the texture quantization parameter (TQP) when geometry encoding is lossless. The results indicate that the perceptual coding distortion depends on the texture complexity (TC). Next, we estimate TC using TQP and the texture bitrate per pixel (TBPP), both of which are extracted from the compressed bitstream without resorting to complete decoding. This allows us to build a texture distortion model as a function of TQP and TBPP. By combining this texture distortion model with a geometry distortion model that depends on the geometry quantization parameter (GQP), we obtain an overall no-reference bitstream-layer PCQA model that we call bitstreamPCQ. Experimental results show that the proposed model markedly outperforms existing models in terms of widely used performance criteria, including the Pearson linear correlation coefficient (PLCC), the Spearman rank order correlation coefficient (SRCC) and the root mean square error (RMSE).
C1 [Liu, Qi; Su, Honglei; Chen, Tianxin] Qingdao Univ, Coll Elect Informat, Qingdao 266071, Peoples R China.
   [Yuan, Hui] Shandong Univ, Sch Control Sci & Engn, Jinan 250061, Peoples R China.
   [Hamzaoui, Raouf] Montfort Univ, Sch Engn & Sustainable Dev, Leicester LE1 9BH, England.
C3 Qingdao University; Shandong University; De Montfort University
RP Su, HL (corresponding author), Qingdao Univ, Coll Elect Informat, Qingdao 266071, Peoples R China.
EM sdqi.liu@gmail.com; suhonglei@qdu.edu.cn; chentx3854@gmail.com;
   yuanhui0325@gmail.com; rhamzaoui@dmu.ac.uk
RI Su, Honglei/KQV-0892-2024; zhao, wenqing/KEZ-9488-2024; Zhang,
   Zhipeng/KHY-2239-2024
OI Su, Honglei/0000-0001-6144-4930; , Qi/0000-0002-3958-9962; yuan,
   hui/0000-0001-5212-3393
FU Natural Sciences and Engineering Research Council of Canada; National
   Science Foundation of China [61772294, 62172259]; open project program
   of state key laboratory of virtual realitytechnology and systems,
   Beihang University [VRLAB2021A01]; Shandong Provincial Natural Science
   Foundation of China [ZR2018PF002, ZR2021MF025]; Joint funding for smart
   computing of Shandong Natural Science Foundation of China
   [ZR2019LZH002]; State Key Laboratory of High Performance Server and
   Storage Technology, Inspur Group, Jinan
FX This work was supported in part by the Natural Sciences and Engineering
   Research Council of Canada, in part by the National Science Foundation
   of China under Grants 61772294 and 62172259, in part by the open project
   program of state key laboratory of virtual realitytechnology and
   systems, Beihang University, under Grant VRLAB2021A01, in part by the
   Shandong Provincial Natural Science Foundation of China under Grants
   ZR2018PF002 and ZR2021MF025, in part by the Joint funding for smart
   computing of Shandong Natural Science Foundation of China under Grant
   ZR2019LZH002, and in part by the State Key Laboratory of High
   Performance Server and Storage Technology, Inspur Group, Jinan.
CR Alexiou E., 2019, P IEEE INT C QUAL MU, P1
   Alexiou E., 2019, APSIPA Trans. Signal Inf. Process, V8, P1
   Alexiou E, 2020, IEEE INT CONF MULTI
   Alexiou E, 2018, IEEE INT CON MULTI
   [Anonymous], 2012, Recommendation BT.500-13
   B. Series, 2012, Recommendation ITU-R BT, V500, P500
   Berger T., 2003, Encyclopedia of Telecommu-nications, DOI DOI 10.1002/0471219282.EOT142
   Biswas J, 2012, IEEE INT CONF ROBOT, P1697, DOI 10.1109/icra.2012.6224766
   Clarke R.J., 1985, TRANSFORM CODING IMA
   Diniz R., 2020, IEEE INT WORKSH MULT, P1, DOI DOI 10.1109/mmsp48831.2020.9287154
   Diniz R., 2021, Electronic Imaging, V2021
   Diniz R, 2021, IEEE SIGNAL PROC LET, V28, P1150, DOI 10.1109/LSP.2021.3088059
   Diniz R, 2020, IEEE IMAGE PROC, P3443, DOI 10.1109/ICIP40778.2020.9190956
   Diniz R, 2020, INT WORK QUAL MULTIM, DOI 10.1109/qomex48832.2020.9123076
   Duanmu ZF, 2021, ANNU REV VIS SCI, V7, P437, DOI 10.1146/annurev-vision-100419-120301
   GISH H, 1968, IEEE T INFORM THEORY, V14, P676, DOI 10.1109/TIT.1968.1054193
   Gu S, 2020, IEEE SIGNAL PROC LET, V27, P176, DOI 10.1109/LSP.2019.2963793
   Gu S, 2020, IEEE T IMAGE PROCESS, V29, P796, DOI 10.1109/TIP.2019.2936738
   Guede C., 2017, ISO/IEC JTC1/SC29/WG11 MPEG, n16902
   He ZY, 2021, IEEE IMAGE PROC, P1444, DOI 10.1109/ICIP42928.2021.9506762
   Hou JH, 2019, IEEE SIGNAL PROC LET, V26, P1847, DOI 10.1109/LSP.2019.2949724
   Hou JH, 2015, IEEE T CIRC SYST VID, V25, P51, DOI 10.1109/TCSVT.2014.2329376
   Hou JH, 2014, IEEE T CIRC SYST VID, V24, P1541, DOI 10.1109/TCSVT.2014.2313890
   Hua L, 2021, IEEE INT SYM BROADB, DOI 10.1109/BMSB53066.2021.9547070
   Hua L, 2022, IET IMAGE PROCESS, V16, P1083, DOI 10.1049/ipr2.12211
   Hua L, 2020, PROC SPIE, V11550, DOI 10.1117/12.2573686
   Javaheri A, 2020, IEEE IMAGE PROC, P3438, DOI 10.1109/ICIP40778.2020.9191233
   Javaheri A, 2020, INT WORK QUAL MULTIM, DOI 10.1109/qomex48832.2020.9123087
   Javaheri A, 2020, IEEE SIGNAL PROC LET, V27, P1350, DOI 10.1109/LSP.2020.3010128
   Jiang QP, 2018, IEEE T MULTIMEDIA, V20, P2035, DOI 10.1109/TMM.2017.2763321
   Lin WS, 2011, J VIS COMMUN IMAGE R, V22, P297, DOI 10.1016/j.jvcir.2011.01.005
   Liu Q, 2023, IEEE T VIS COMPUT GR, V29, P3642, DOI 10.1109/TVCG.2022.3167151
   Liu Q, 2021, IEEE T IMAGE PROCESS, V30, P6623, DOI 10.1109/TIP.2021.3096060
   Liu YT, 2019, IEEE T MULTIMEDIA, V21, P135, DOI 10.1109/TMM.2018.2849602
   Mekuria R., 2016, Standard ISO/IEC JTC1/SC29/WG11 MPEG N16332
   Mekuria R, 2017, IEEE T CIRC SYST VID, V27, P828, DOI 10.1109/TCSVT.2016.2543039
   Meynet G, 2020, INT WORK QUAL MULTIM, DOI 10.1109/qomex48832.2020.9123147
   Meynet G, 2019, INT WORK QUAL MULTIM
   Min XK, 2019, IEEE T MULTIMEDIA, V21, P2319, DOI 10.1109/TMM.2019.2902097
   Montgomery D.C., 2014, Applied Statistics and Probability for Engineers, Vsixth
   MPEG, 2020, ISO/IEC JTC1/SC29/WG7 MPEG, n00100
   Ou YF, 2011, IEEE T CIRC SYST VID, V21, P286, DOI 10.1109/TCSVT.2010.2087833
   Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009
   Pulido H. M. B., 2003, Geometry videos: A new representation for 3D animations
   Remondino F., 2003, International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, V34
   Richter R, 2013, INT J REMOTE SENS, V34, P8408, DOI 10.1080/01431161.2013.838710
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Sim K, 2022, IEEE T MULTIMEDIA, V24, P1389, DOI 10.1109/TMM.2021.3064240
   Su HL, 2019, IEEE IMAGE PROC, P3182, DOI [10.1109/ICIP.2019.8803298, 10.1109/icip.2019.8803298]
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tang PB, 2010, AUTOMAT CONSTR, V19, P829, DOI 10.1016/j.autcon.2010.06.007
   Tao WX, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5266, DOI 10.1145/3474085.3475645
   Tian D., 2017, M40522 ISOIEC JTC1SC
   Tian D, 2017, IEEE IMAGE PROC, P3460, DOI 10.1109/ICIP.2017.8296925
   Torlig EM, 2018, PROC SPIE, V10752, DOI 10.1117/12.2322741
   Viola I, 2020, IEEE SIGNAL PROC LET, V27, P1660, DOI 10.1109/LSP.2020.3024065
   Viola I, 2020, INT WORK QUAL MULTIM, DOI 10.1109/qomex48832.2020.9123089
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Woo H, 2002, INT J MACH TOOL MANU, V42, P167, DOI 10.1016/S0890-6955(01)00120-1
   Wu XJ, 2021, IEEE T CIRC SYST VID, V31, P4630, DOI 10.1109/TCSVT.2021.3101484
   Xu YL, 2022, IEEE T BROADCAST, V68, P33, DOI 10.1109/TBC.2021.3114510
   Yang FZ, 2012, IEEE COMMUN MAG, V50, P203, DOI 10.1109/MCOM.2012.6353702
   Yang FZ, 2010, IEEE T CIRC SYST VID, V20, P1544, DOI 10.1109/TCSVT.2010.2087433
   Yang JC, 2021, IEEE T MULTIMEDIA, V23, P797, DOI 10.1109/TMM.2020.2990075
   Yang Q, 2022, IEEE T PATTERN ANAL, V44, P3015, DOI 10.1109/TPAMI.2020.3047083
   Yang Q, 2021, IEEE T MULTIMEDIA, V23, P3877, DOI 10.1109/TMM.2020.3033117
   Zhai GT, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2757-1
   Zhang QJ, 2020, Arxiv, DOI arXiv:2012.03028
   Zhang YJ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1230, DOI 10.1145/3474085.3475294
NR 71
TC 5
Z9 5
U1 3
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4533
EP 4546
DI 10.1109/TMM.2022.3177926
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA W1OJ1
UT WOS:001089390200033
DA 2024-07-18
ER

PT J
AU Lu, Z
   Hu, Y
   Yu, C
   Jiang, YC
   Chen, Y
   Zeng, B
AF Lu, Zhi
   Hu, Yang
   Yu, Cong
   Jiang, Yunchao
   Chen, Yan
   Zeng, Bing
TI Personalized Fashion Recommendation With Discrete Content-Based Tensor
   Factorization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Fashion analysis; personalized recommendation; outfit recommendation;
   binary code; learn to hash
AB Fashion outfit recommendation has attracted lots of attention recently. The problem becomes even more interesting and challenging when considering users' personalized fashion preferences. Although existing works have successfully improved the recommendation accuracy, the efficiency issue of computation and storage is still under-investigated and often ignored. In this paper, we propose a discrete content-based tensor factorization model that maps items and user to binary codes for efficient fashion recommendation. We introduce a probabilistic perspective for learning to hash, where the binary codes are sampled from a set of underlying Bernoulli variables. To demonstrate the effectiveness of our model, we collect a large-scale outfit dataset together with user label information from a fashion-focused social website. Extensive experiments on our dataset show that the proposed model outperforms other state-of-the-art methods.
C1 [Lu, Zhi; Yu, Cong; Jiang, Yunchao; Zeng, Bing] Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 611731, Peoples R China.
   [Hu, Yang] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230026, Peoples R China.
   [Chen, Yan] Univ Sci & Technol China, Sch Cyber Sci & Technol, Hefei 230026, Peoples R China.
C3 University of Electronic Science & Technology of China; Chinese Academy
   of Sciences; University of Science & Technology of China, CAS; Chinese
   Academy of Sciences; University of Science & Technology of China, CAS
RP Hu, Y (corresponding author), Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230026, Peoples R China.
EM zhilu@std.uestc.edu.cn; eeyhu@ustc.edu.cn; congyu@std.uestc.edu.cn;
   jiangyunchao@std.uestc.edu.cn; eecyan@ustc.edu.cn; eezeng@uestc.edu.cn
RI Yu, Zhou/KBP-8384-2024; Lu, Zhi/KGM-0198-2024
OI Lu, Zhi/0000-0001-6941-981X
FU National Natural Science Foundation of China [62172381]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 62172381
CR [Anonymous], 2008, Proceedings of the 21st International Conference on Neural Information Processing Systems
   Ballard G, 2015, IEEE DATA MINING, P11, DOI 10.1109/ICDM.2015.46
   Cao Y, 2016, AAAI CONF ARTIF INTE, P3457
   Cao ZJ, 2017, IEEE I CONF COMP VIS, P5609, DOI 10.1109/ICCV.2017.598
   Chen YD, 2019, IEEE I CONF COMP VIS, P9795, DOI 10.1109/ICCV.2019.00989
   Cheng WH, 2021, ACM COMPUT SURV, V54, DOI [10.1145/3447239, 10.1145/3552468.3554360]
   Cucurull G, 2019, PROC CVPR IEEE, P12609, DOI 10.1109/CVPR.2019.01290
   Cui ZY, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P307, DOI 10.1145/3308558.3313444
   Dong H, 2019, IEEE I CONF COMP VIS, P9025, DOI 10.1109/ICCV.2019.00912
   Fan B, 2013, INT CONF ACOUST SPEE, P2395, DOI 10.1109/ICASSP.2013.6638084
   Han XT, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1078, DOI 10.1145/3123266.3123394
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hidayati SC, 2021, IEEE T MULTIMEDIA, V23, P365, DOI 10.1109/TMM.2020.2980195
   Hu Y, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P129, DOI 10.1145/2733373.2806239
   Huang Jianqiang, 2020, ARXIV200303369
   Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876
   Jiang QY, 2018, AAAI CONF ARTIF INTE, P3342
   Jing PG, 2022, IEEE T MULTIMEDIA, V24, P1277, DOI 10.1109/TMM.2021.3062736
   Kang WC, 2019, PROC CVPR IEEE, P10524, DOI 10.1109/CVPR.2019.01078
   Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li WJ, 2016, IJCAI, P1711
   Li XC, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P159, DOI 10.1145/3397271.3401080
   Li YC, 2017, IEEE T MULTIMEDIA, V19, P1946, DOI 10.1109/TMM.2017.2690144
   Lian DF, 2021, IEEE T KNOWL DATA EN, V33, P1919, DOI 10.1109/TKDE.2019.2951386
   Lian DF, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P325, DOI 10.1145/3097983.3098008
   Lin YL, 2020, PROC CVPR IEEE, P3308, DOI 10.1109/CVPR42600.2020.00337
   Lin YS, 2020, WEB CONFERENCE 2020: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2020), P77, DOI 10.1145/3366423.3380096
   Liu HM, 2020, INT J COMPUT VISION, V128, P2223, DOI 10.1007/s11263-020-01315-0
   Liu HM, 2019, INT J COMPUT VISION, V127, P1217, DOI 10.1007/s11263-019-01174-4
   Liu LY, 2018, LECT NOTES COMPUT SC, V10827, P116, DOI 10.1007/978-3-319-91452-7_8
   Liu W, 2011, SER INF MANAGE SCI, V10, P1
   Liu XL, 2014, PROC CVPR IEEE, P2147, DOI 10.1109/CVPR.2014.275
   Lu Z, 2022, 6TH INTERNATIONAL CONFERENCE ON INNOVATION IN ARTIFICIAL INTELLIGENCE, ICIAI2022, P166, DOI 10.1145/3529466.3529472
   Lu Z, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2400
   Lu Z, 2019, PROC CVPR IEEE, P10554, DOI 10.1109/CVPR.2019.01081
   Ma Z, 2020, AAAI CONF ARTIF INTE, V34, P11741
   McAuley J, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P43, DOI 10.1145/2766462.2767755
   Paszke A, 2019, ADV NEUR IN, V32
   Rendle S., 2009, P 25 C UNC ART INT, P452, DOI DOI 10.5555/1795114.1795167
   Rendle Steffen, 2010, P 3 ACM INT C WEB SE, P81, DOI DOI 10.1145/1718487.1718498
   Shen DH, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2041
   Song XM, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P320, DOI 10.1145/3343031.3350956
   Sutskever I, 2014, ADV NEUR IN, V27
   Tan RB, 2019, IEEE I CONF COMP VIS, P10372, DOI 10.1109/ICCV.2019.01047
   Tangseng P, 2017, IEEE INT CONF COMP V, P2275, DOI 10.1109/ICCVW.2017.267
   Do TT, 2020, IEEE T MULTIMEDIA, V22, P992, DOI 10.1109/TMM.2019.2935680
   Vasileva MI, 2018, LECT NOTES COMPUT SC, V11220, P405, DOI 10.1007/978-3-030-01270-0_24
   Veit A, 2017, PROC CVPR IEEE, P1781, DOI 10.1109/CVPR.2017.193
   Veit A, 2015, IEEE I CONF COMP VIS, P4642, DOI 10.1109/ICCV.2015.527
   Wang HY, 2019, AAAI CONF ARTIF INTE, P5248
   Wang JD, 2018, IEEE T PATTERN ANAL, V40, P769, DOI 10.1109/TPAMI.2017.2699960
   Wang QF, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P1185, DOI 10.1145/2505515.2507851
   Wang WG, 2019, IEEE I CONF COMP VIS, P5702, DOI 10.1109/ICCV.2019.00580
   Yang XW, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2636, DOI 10.1145/3394171.3413936
   Yang X, 2019, AAAI CONF ARTIF INTE, P403
   Yu C., 2019, P IEEE CVF INT C COM, P10
   Zhang HW, 2016, SIGIR'16: PROCEEDINGS OF THE 39TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P325, DOI 10.1145/2911451.2911502
   Zhang J, 2018, IEEE T MULTIMEDIA, V20, P2400, DOI 10.1109/TMM.2018.2804763
   Zhang L, 2013, PROC CVPR IEEE, P1586, DOI 10.1109/CVPR.2013.208
   Zhou H., 2012, P 18 ACM SIGKDD INT, P498
NR 61
TC 1
Z9 1
U1 5
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5053
EP 5064
DI 10.1109/TMM.2022.3186744
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300032
DA 2024-07-18
ER

PT J
AU Sun, MY
   Suo, W
   Wang, P
   Zhang, YN
   Wu, Q
AF Sun, Mengyang
   Suo, Wei
   Wang, Peng
   Zhang, Yanning
   Wu, Qi
TI A Proposal-Free One-Stage Framework for Referring Expression
   Comprehension and Generation via Dense Cross-Attention
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Referring expression comprehension; referring expression generation;
   one-stage method
AB Referring Expression Comprehension (REC) and Generation (REG) have become one of the most important tasks in visual reasoning, since it is an essential step for many vision-and-language tasks such as visual question answering or visual dialogue. However, it has not been widely used in many downstream tasks, mainly for the following reasons: 1) mainstream two-stage methods rely on additional annotations or off-the-shelf detectors to generate proposals. It would heavily degrade the generalization ability of models and lead to inevitable error accumulation. 2) Although one-stage strategies for REC have been proposed, these methods have to depend on lots of hyper-parameters (such as anchors) to generate bounding box. In this paper, we present a proposal-free one-stage (PFOS) framework that can directly regress the region-of-interest from the image or generate unambiguous descriptions in an end-to-end manner. Instead of using the dominant two-stage fashion, we take the dense-grid of images as input for a cross-attention transformer that learns multi-modal correspondences. The final bounding box or sentence is directly predicted from the image without the anchor selection or the computation of visual difference. Furthermore, we expand the traditional two-stage listener-speaker framework to jointly train by a one-stage learning paradigm. Our model achieves state-of-the-art performance on both accuracy and speed for comprehension and competitive results for generation.
C1 [Sun, Mengyang] Northwestern Polytech Univ, Sch Cybersecur, Xian 710060, Peoples R China.
   [Suo, Wei; Wang, Peng; Zhang, Yanning] Northwestern Polytech Univ, Sch Comp Sci, Xian 710060, Peoples R China.
   [Wu, Qi] Univ Adelaide, Adelaide, SA 5005, Australia.
C3 Northwestern Polytechnical University; Northwestern Polytechnical
   University; University of Adelaide
RP Wang, P; Zhang, YN (corresponding author), Northwestern Polytech Univ, Sch Comp Sci, Xian 710060, Peoples R China.
EM sunmenmian@mail.nwpu.edu.cn; suowei1994@mail.nwpu.edu.cn;
   peng.wang@nwpu.edu.cn; ynzhang@nwpu.edu.cn; qi.wu01@adelaide.edu.au
RI SUO, WEI/JEF-9390-2023; Sun, Mengyang/AAP-5033-2020; Wu,
   Qi/ABD-6304-2021
OI Sun, Mengyang/0000-0002-3998-8744; Wu, Qi/0000-0003-3631-256X; Sun,
   Mengyang/0000-0003-0638-3295; Suo, Wei/0000-0002-8283-8637
FU National Key R&D Program of China [2020AAA0106900]; National Natural
   Science Foundation of China [U19B2037, 61876152]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2020AAA0106900, and in part by the National Natural Science
   Foundation of China under Grants U19B2037 and 61876152.
CR Cai ZW, 2021, IEEE T PATTERN ANAL, V43, P1483, DOI 10.1109/TPAMI.2019.2956516
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen K, 2017, PROC CVPR IEEE, P6203, DOI 10.1109/CVPR.2017.657
   Chen XP, 2018, Arxiv, DOI arXiv:1812.03426
   Chen XP, 2018, PROC CVPR IEEE, P7995, DOI 10.1109/CVPR.2018.00834
   Das A, 2017, IEEE I CONF COMP VIS, P2970, DOI 10.1109/ICCV.2017.321
   Deng J., 2021, Proceedings of the IEEE/CVF International Conference on Computer Vision, P1769
   Gan C, 2017, IEEE I CONF COMP VIS, P1829, DOI 10.1109/ICCV.2017.201
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378
   Hu RH, 2017, PROC CVPR IEEE, P4418, DOI 10.1109/CVPR.2017.470
   Hu Ronghang, 2020, 2020 IEEE CVF C COMP, P9992, DOI DOI 10.1109/CVPR42600.2020.01001
   Huang BB, 2021, PROC CVPR IEEE, P16883, DOI 10.1109/CVPR46437.2021.01661
   Escalante HJ, 2010, COMPUT VIS IMAGE UND, V114, P419, DOI 10.1016/j.cviu.2009.03.008
   Kazemzadeh S., 2014, EMNLP, P787, DOI DOI 10.3115/V1/D14-1086
   Kim JJ, 2021, NEURAL NETWORKS, V139, P158, DOI 10.1016/j.neunet.2021.02.001
   Kingma D. P., 2014, arXiv
   Ku A, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P4392
   Lavie A., 2007, P 2 WORKSH STAT MACH, P228
   Li YK, 2018, PROC CVPR IEEE, P6116, DOI 10.1109/CVPR.2018.00640
   Liao Y., 2020, P IEEE CVF C COMP VI, P10877, DOI 10.24963/ijcai.2018/155
   Lin CY, 2004, ROUGE: A Package for Automatic Evaluation of Summaries, P74, DOI DOI 10.1253/JCJ.34.1213
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu JY, 2020, IEEE T IMAGE PROCESS, V29, P5244, DOI 10.1109/TIP.2020.2979010
   Liu JY, 2017, IEEE I CONF COMP VIS, P4866, DOI 10.1109/ICCV.2017.520
   Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345
   Luo YP, 2021, AAAI CONF ARTIF INTE, V35, P2286
   Mao JH, 2016, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2016.9
   Nagaraja VK, 2016, LECT NOTES COMPUT SC, V9908, P792, DOI 10.1007/978-3-319-46493-0_48
   Niu YL, 2021, IEEE T PATTERN ANAL, V43, P347, DOI 10.1109/TPAMI.2019.2926266
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Qin Y, 2019, PROC CVPR IEEE, P8359, DOI 10.1109/CVPR.2019.00856
   Ren S., 2015, ADV NEURAL INFORM PR, V28, P91
   Sun M., 2021, P IEEECVF C COMPUTER, P14060
   Suo W., 2021, P 30 INT JOINT C ART, P1032, DOI [10.24963/ijcai.2021/143, DOI 10.24963/IJCAI.2021/143]
   Tan H, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5100
   Tanaka M, 2019, IEEE I CONF COMP VIS, P5793, DOI 10.1109/ICCV.2019.00589
   Tsu-Jui Fu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P71, DOI 10.1007/978-3-030-58539-6_5
   Vaswani A, 2017, ADV NEUR IN, V30
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Wang P, 2019, PROC CVPR IEEE, P1960, DOI 10.1109/CVPR.2019.00206
   Yang SB, 2019, IEEE I CONF COMP VIS, P4643, DOI 10.1109/ICCV.2019.00474
   Yang ZY, 2019, IEEE I CONF COMP VIS, P4682, DOI 10.1109/ICCV.2019.00478
   Yu LC, 2018, PROC CVPR IEEE, P1307, DOI 10.1109/CVPR.2018.00142
   Yu LC, 2017, PROC CVPR IEEE, P3521, DOI 10.1109/CVPR.2017.375
   Yu LC, 2016, LECT NOTES COMPUT SC, V9906, P69, DOI 10.1007/978-3-319-46475-6_5
   Yuankai Qi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9979, DOI 10.1109/CVPR42600.2020.01000
   Zhang HW, 2018, PROC CVPR IEEE, P4158, DOI 10.1109/CVPR.2018.00437
   Zheng ZL, 2019, PROC CVPR IEEE, P3662, DOI 10.1109/CVPR.2019.00683
   Zhengyuan Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P387, DOI 10.1007/978-3-030-58568-6_23
   Zhou XY, 2019, Arxiv, DOI [arXiv:1904.07850, 10.48550/arXiv.1904.07850]
   Zhou YY, 2023, IEEE T NEUR NET LEAR, V34, P134, DOI 10.1109/TNNLS.2021.3090426
   Zhuang BH, 2018, PROC CVPR IEEE, P4252, DOI 10.1109/CVPR.2018.00447
NR 53
TC 13
Z9 13
U1 4
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2446
EP 2458
DI 10.1109/TMM.2022.3147385
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100065
DA 2024-07-18
ER

PT J
AU Tu, ZG
   Zhang, JX
   Li, HY
   Chen, YJ
   Yuan, JS
AF Tu, Zhigang
   Zhang, Jiaxu
   Li, Hongyan
   Chen, Yujin
   Yuan, Junsong
TI Joint-Bone Fusion Graph Convolutional Network for Semi-Supervised
   Skeleton Action Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Action recognition; graph convolutional network; semi-supervised
   learning; skeleton action
AB In recent years, graph convolutional networks (GCNs) play an increasingly critical role in skeleton-based human action recognition. However, most GCN-based methods still have two main limitations: 1) They only consider the motion information of the joints or process the joints and bones separately, which are unable to fully explore the latent functional correlation between joints and bones for action recognition. 2) Most of these works are performed in the supervised learning way, which heavily relies on massive labeled training data. To address these issues, we propose a semi-supervised skeleton-based action recognition method which has been rarely exploited before. We design a novel correlation-driven joint-bone fusion graph convolutional network (CD-JBF-GCN) as an encoder and use a pose prediction head as a decoder to achieve semi-supervised learning. Specifically, the correlation-driven joint-bone fusion graph convolution (CD-JBF-GC) can explore the motion transmission between the joint stream and the bone stream, so as to promote both streams to learn more discriminative feature representations. The pose prediction based auto-encoder in the self-supervised training fashion allows the network to learn motion representation from the unlabeled data, which is essential for action recognition. Extensive experiments on two popular datasets, i.e. NTU-RGB+D and Kinetics-Skeleton, demonstrate that our model achieves the state-of-the-art performance for semi-supervised skeleton-based action recognition and is also useful for fully-supervised methods.
C1 [Tu, Zhigang; Zhang, Jiaxu] Wuhan Univ, State Key Lab Informat Engn Surveying Mapping & Re, Wuhan 430079, Peoples R China.
   [Li, Hongyan] Hubei Univ Econ, Sch Informat Engn, Wuhan 430205, Peoples R China.
   [Chen, Yujin] Tech Univ Munich, Dept Informat, Garching 85748, Germany.
   [Yuan, Junsong] SUNY Buffalo, Comp Sci & Engn Dept, Buffalo, NY 14260 USA.
C3 Wuhan University; Hubei University of Economics; Technical University of
   Munich; State University of New York (SUNY) System; State University of
   New York (SUNY) Buffalo
RP Zhang, JX (corresponding author), Wuhan Univ, State Key Lab Informat Engn Surveying Mapping & Re, Wuhan 430079, Peoples R China.
EM tuzhigang@whu.edu.cn; zjiaxu@whu.edu.cn; hongyanli2000@126.com;
   terencecyj@gmail.com; jsyuan@buffalo.edu
RI Yuan, Junsong/A-5171-2011
OI Yuan, Junsong/0000-0002-7901-8793; Tu, Zhigang/0000-0001-5003-2260
FU National Natural Science Foundation of China [62106177]; Central
   University Basic Research Fund of China [2042020KF0016]; supercomputing
   system in the Supercomputing Center of Wuhan University
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62106177, in part by the Central
   University Basic Research Fund of China under Grant 2042020KF0016, and
   in part by the supercomputing system in the Supercomputing Center of
   Wuhan University.
CR Cao CQ, 2019, IEEE T CIRC SYST VID, V29, P3247, DOI 10.1109/TCSVT.2018.2879913
   Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
   Cheng K, 2020, PROC CVPR IEEE, P180, DOI 10.1109/CVPR42600.2020.00026
   Chenyang Si, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P35, DOI 10.1007/978-3-030-58571-6_3
   Defferrard M, 2016, ADV NEUR IN, V29
   Demisse GG, 2018, IEEE COMPUT SOC CONF, P301, DOI 10.1109/CVPRW.2018.00056
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Fan ZX, 2019, IEEE T MULTIMEDIA, V21, P363, DOI 10.1109/TMM.2018.2859620
   Gao X, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P601, DOI 10.1145/3343031.3351170
   Hu GY, 2020, IEEE T MULTIMEDIA, V22, P2207, DOI 10.1109/TMM.2019.2953325
   Ke Cheng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P536, DOI 10.1007/978-3-030-58586-0_32
   Ke Q, 2017, PROC CVPR IEEE, P4570, DOI 10.1109/CVPR.2017.486
   Kim TS, 2017, IEEE COMPUT SOC CONF, P1623, DOI 10.1109/CVPRW.2017.207
   Li B, 2017, IEEE INT C COMPUT, P187, DOI 10.1109/CSE-EUC.2017.217
   Li C, 2017, IEEE INT CONF MULTI
   Li MS, 2020, PROC CVPR IEEE, P211, DOI 10.1109/CVPR42600.2020.00029
   Li MS, 2019, PROC CVPR IEEE, P3590, DOI 10.1109/CVPR.2019.00371
   Lin LL, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2490, DOI 10.1145/3394171.3413548
   Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50
   Liu MY, 2017, PATTERN RECOGN, V68, P346, DOI 10.1016/j.patcog.2017.02.030
   Liu ZY, 2020, PROC CVPR IEEE, P140, DOI 10.1109/CVPR42600.2020.00022
   Miyato T, 2019, IEEE T PATTERN ANAL, V41, P1979, DOI 10.1109/TPAMI.2018.2858821
   Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576
   Niepert M, 2016, PR MACH LEARN RES, V48
   Paszke A, 2019, ADV NEUR IN, V32
   Peng GZ, 2019, AAAI CONF ARTIF INTE, P8827
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Shi L, 2019, PROC CVPR IEEE, P7904, DOI 10.1109/CVPR.2019.00810
   Shi L, 2019, PROC CVPR IEEE, P12018, DOI 10.1109/CVPR.2019.01230
   Si CY, 2018, LECT NOTES COMPUT SC, V11205, P106, DOI 10.1007/978-3-030-01246-5_7
   Singh A, 2021, PROC CVPR IEEE, P10384, DOI 10.1109/CVPR46437.2021.01025
   Song SJ, 2018, IEEE T IMAGE PROCESS, V27, P3459, DOI 10.1109/TIP.2018.2818328
   Song SJ, 2017, AAAI CONF ARTIF INTE, P4263
   Tang L, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P817
   Tu ZG, 2018, PATTERN RECOGN, V79, P32, DOI 10.1016/j.patcog.2018.01.020
   Vemulapalli R, 2016, PROC CVPR IEEE, P4471, DOI 10.1109/CVPR.2016.484
   Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82
   Wang HS, 2018, IEEE T IMAGE PROCESS, V27, P4382, DOI 10.1109/TIP.2018.2837386
   Wang S, 2014, IEEE T MULTIMEDIA, V16, P289, DOI 10.1109/TMM.2013.2293060
   Wen YH, 2019, AAAI CONF ARTIF INTE, P8989
   Xiao J, 2021, IEEE T MULTIMEDIA, V23, P3454, DOI 10.1109/TMM.2020.3025661
   Xu ZM, 2019, IEEE T NEUR NET LEAR, V30, P2951, DOI 10.1109/TNNLS.2018.2886008
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yang JY, 2021, IEEE T MULTIMEDIA, V23, P883, DOI 10.1109/TMM.2020.2990082
   Yunpeng Chang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P329, DOI 10.1007/978-3-030-58555-6_20
   Zhang DJ, 2020, PATTERN RECOGN, V103, DOI 10.1016/j.patcog.2020.107312
   Zhang JG, 2017, IEEE T CYBERNETICS, V47, P960, DOI 10.1109/TCYB.2016.2535122
   Zhang JX, 2022, CAAI T INTELL TECHNO, V7, P46, DOI 10.1049/cit2.12012
   Zhang PF, 2020, PROC CVPR IEEE, P1109, DOI 10.1109/CVPR42600.2020.00119
   Zhang PF, 2019, IEEE T PATTERN ANAL, V41, P1963, DOI 10.1109/TPAMI.2019.2896631
   Zhang SY, 2018, IEEE T MULTIMEDIA, V20, P2330, DOI 10.1109/TMM.2018.2802648
   Zhang X., 2020, P IEEE CVF C COMP VI, P14333, DOI DOI 10.1109/CVPR42600202001434
   Zhang XY, 2023, IEEE T NEUR NET LEAR, V34, P1852, DOI 10.1109/TNNLS.2019.2962815
   Zhang XK, 2020, IEEE T NEUR NET LEAR, V31, P3047, DOI 10.1109/TNNLS.2019.2935173
   Zhao R, 2019, IEEE I CONF COMP VIS, P6881, DOI 10.1109/ICCV.2019.00698
   Zheng NG, 2018, AAAI CONF ARTIF INTE, P2644
   Zheng W, 2019, IEEE INT CON MULTI, P826, DOI 10.1109/ICME.2019.00147
   Zhou ZY, 2020, AAAI CONF ARTIF INTE, V34, P1258
   Zhu KJ, 2020, IEEE T MULTIMEDIA, V22, P2977, DOI 10.1109/TMM.2019.2962304
   Zitnik M, 2017, BIOINFORMATICS, V33, pI190, DOI 10.1093/bioinformatics/btx252
NR 63
TC 26
Z9 27
U1 20
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1819
EP 1831
DI 10.1109/TMM.2022.3168137
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100018
OA Green Submitted
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Wang, DP
   Hu, ZZ
   Zhou, YE
   Hong, RC
   Wang, M
AF Wang, Depeng
   Hu, Zhenzhen
   Zhou, Yuanen
   Hong, Richang
   Wang, Meng
TI A Text-Guided Generation and Refinement Model for Image Captioning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Attention mechanism; generating and refining decoder; image captioning;
   text-guided
ID ATTENTION
AB A high-quality image description requires not only the logic and fluency of language but also the richness and accuracy ofcontent. However, due to the semantic gap between vision and language, most existing image captioning approaches thatdirectly learn the cross-modal mapping from vision to language are difficult to meet these two requirements simultaneously. Inspired by the progressive learning mechanism, we trace the "generating + refining" route and propose a novel Text-GuidedGeneration and Refinement (dubbed as TGGAR) model with assistance from the guide text to improve the quality of captions.The guide text is selected from the training set according to content similarity, then utilized to explore salient objects andextend candidate words. Specifically, we follow the encoderdecoder architecture, and design a Text-Guided Relation Encoder(TGRE) to learn the visual representation that is more consistent with human visual cognition. Besides, we divide the decoderpart into two sub-modules: a Generator for the primary sentence generation and a Refiner for the sentence refinement.Generator, consisting of a standard LSTM and a Gate on Attention (GOA) module, aims to generate the primary sentencelogically and fluently. Refiner contains a caption encoder module, an attentionbased LSTM and a GOA module, whichiteratively modifies the details in the primary caption to make captions rich and accurate. Extensive experiments on theMSCOCO captioning dataset demonstrate our framework with fewer parameters remains comparable to transformer-basedmethods, and achieves state-of-the-art performance compared with other relevant approaches.
C1 [Wang, Depeng; Hu, Zhenzhen; Zhou, Yuanen; Hong, Richang; Wang, Meng] Hefei Univ Technol, Sch Comp & Informat, Key Lab Knowledge Engn Big Data, Hefei 230009, Peoples R China.
C3 Hefei University of Technology
RP Hu, ZZ (corresponding author), Hefei Univ Technol, Sch Comp & Informat, Key Lab Knowledge Engn Big Data, Hefei 230009, Peoples R China.
EM wangdepeng@baidu.com; huzhen.ice@gmail.com; y.e.zhou.hb@gmail.com;
   hongrc.hfut@gmail.com; eric.mengwang@gmail.com
OI Zhou, Yuanen/0000-0002-4986-3611; Hu, Zhenzhen/0000-0003-1042-8361
FU National Natural Science Foundation of China [62172138, 61932009];
   Fundamental Research Funds for the Central Universities [JZ2021HGTB0082]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62172138 and 61932009, and in part by
   the Fundamental Research Funds for the Central Universities under Grant
   JZ2021HGTB0082. The Associate Editor coordinating the review of this
   manuscript and approving it for publication was Dr. Xinxiao Wu.& nbsp;
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24
   Banerjee S., 2005, P ACL WORKSH INTR EX, P65
   Bengio S, 2015, ADV NEUR IN, V28
   Chen XL, 2015, Arxiv, DOI arXiv:1504.00325
   Cho K., 2014, ARXIV14061078
   Cho K, 2015, IEEE T MULTIMEDIA, V17, P1875, DOI 10.1109/TMM.2015.2477044
   Choi Y, 2011, P 15 C COMPUTATIONAL, P220
   Eric M, 2017, ARXIV PREPRINT ARXIV
   Gu JT, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1631
   Gu JX, 2018, AAAI CONF ARTIF INTE, P6837
   Guo LT, 2020, IEEE T MULTIMEDIA, V22, P2149, DOI 10.1109/TMM.2019.2951226
   Hashimoto TB, 2018, ADV NEUR IN, V31
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Herdade S, 2019, ADV NEUR IN, V32
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Huang L, 2019, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2019.00473
   Ji JY, 2021, AAAI CONF ARTIF INTE, V35, P1655
   Jiang WH, 2018, LECT NOTES COMPUT SC, V11206, P510, DOI 10.1007/978-3-030-01216-8_31
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kingma D. P., 2014, arXiv
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Kulkarni G, 2013, IEEE T PATTERN ANAL, V35, P2891, DOI 10.1109/TPAMI.2012.162
   Kuznetsova Polina, 2012, Association for Computational Linguistics
   Lee S, 2019, IEEE I CONF COMP VIS, P4412, DOI 10.1109/ICCV.2019.00451
   Li LH, 2018, IEEE T MULTIMEDIA, V20, P726, DOI 10.1109/TMM.2017.2751140
   Lin CY, 2004, ROUGE: A Package for Automatic Evaluation of Summaries, P74, DOI DOI 10.1253/JCJ.34.1213
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu C, 2009, PROC CVPR IEEE, P1972, DOI 10.1109/CVPRW.2009.5206536
   Liu DQ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1416, DOI 10.1145/3240508.3240632
   Liu F., 2020, ADV NEURAL INFORM PR, V33, P1865
   Liu W., 2021, arXiv, DOI DOI 10.48550/ARXIV.2101.10804
   Lu JS, 2018, PROC CVPR IEEE, P7219, DOI 10.1109/CVPR.2018.00754
   Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345
   MacLeod H, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P5988, DOI 10.1145/3025453.3025814
   Mitchell M, 2012, P 13 C EUR CHAPT ASS, P747
   Mun J, 2017, AAAI CONF ARTIF INTE, P4233
   Ordonez V, 2016, INT J COMPUT VISION, V119, P46, DOI 10.1007/s11263-015-0840-y
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Ranzato M., 2015, PROC 4 INT C LEARN R
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Sammani F., 2020, COMPUT VIS PATTERN R, P1
   Sammani F, 2020, PROC CVPR IEEE, P4807, DOI 10.1109/CVPR42600.2020.00486
   Sharma P, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2556
   Stahlberg F, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P5147
   Sutskever I, 2014, ADV NEUR IN, V27
   Sutton RS, 2018, ADAPT COMPUT MACH LE, P1
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Vulchanova M, 2019, J CULT COGN SCI, V3, P103, DOI 10.1007/s41809-019-00047-z
   Wang D., 2020, P IEEE INT COMP MULT, P1
   Wang WG, 2016, IEEE T IMAGE PROCESS, V25, P5025, DOI 10.1109/TIP.2016.2601784
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yang X, 2019, PROC CVPR IEEE, P10677, DOI 10.1109/CVPR.2019.01094
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yao T, 2017, IEEE I CONF COMP VIS, P4904, DOI 10.1109/ICCV.2017.524
   Yingwei Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10968, DOI 10.1109/CVPR42600.2020.01098
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Zeng XR, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P506
   Zhang L., 2017, arXiv
   Zheng ZL, 2019, PROC CVPR IEEE, P3662, DOI 10.1109/CVPR.2019.00683
   Zhou Y., 2022, arXiv
   Zhou YE, 2021, IEEE INT CONF COMP V, P3132, DOI 10.1109/ICCVW54120.2021.00350
   Zhou YAN, 2020, PROC CVPR IEEE, P4776, DOI 10.1109/CVPR42600.2020.00483
NR 66
TC 3
Z9 3
U1 4
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2966
EP 2977
DI 10.1109/TMM.2022.3154149
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA O7RP6
UT WOS:001045742200002
DA 2024-07-18
ER

PT J
AU Xiao, SX
   Du, SD
   Chen, ZL
   Zhang, YH
   Wang, SP
AF Xiao, Shunxin
   Du, Shide
   Chen, Zhaoliang
   Zhang, Yunhe
   Wang, Shiping
TI Dual Fusion-Propagation Graph Neural Network for Multi-View Clustering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Graph neural networks; Task analysis; Deep learning; Convolution;
   Clustering algorithms; Representation learning; Message passing; graph
   neural network; unsuper- vised learning; multi-view clustering
ID CONVOLUTIONAL NETWORKS
AB Deep multi-view representation learning focuses on training a unified low-dimensional representation for data with multiple sources or modalities. With the rapidly growing attention of graph neural networks, more and more researchers have introduced various graph models into multi-view learning. Although considerable achievements have been made, most existing methods usually propagate information in a single view and fuse multiple information only from the perspective of attributes or relationships. To solve the aforementioned problems, we propose an efficient model termed Dual Fusion-Propagation Graph Neural Network (DFP-GNN) and apply it to deep multi-view clustering tasks. The proposed method is designed with three submodules and has the following merits: a) The proposed view-specific and cross-view propagation modules can capture the consistency and complementarity information among multiple views; b) The designed fusion module performs multi-view information fusion with the attributes of nodes and the relationships among them simultaneously. Experiments on popular databases show that DFP-GNN achieves significant results compared with several state-of-the-art algorithms.
C1 [Xiao, Shunxin; Du, Shide; Chen, Zhaoliang; Zhang, Yunhe; Wang, Shiping] Fuzhou Univ, Coll Comp & Data Sci, Fuzhou 350116, Peoples R China.
C3 Fuzhou University
RP Wang, SP (corresponding author), Fuzhou Univ, Coll Comp & Data Sci, Fuzhou 350116, Peoples R China.
EM xiaoshunxin.tj@gmail.com; dushidems@gmail.com; Chenchenzl23@outlook.com;
   zhangyhannie@163.com; shipingwangphd@163.com
RI Chen, Zhaoliang/KGL-0282-2024; Du, Shide/HLW-0768-2023
OI Chen, Zhaoliang/0000-0002-7832-908X; Du, Shide/0000-0002-6354-4705
FU National Natural Science Foundation of China
FX No Statement Available
CR Andrew G., 2013, ICML, P1247
   Bianchi FM, 2020, PR MACH LEARN RES, V119
   Cao XC, 2015, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2015.7298657
   Chen ZX, 2022, INFORM SCIENCES, V610, P114, DOI 10.1016/j.ins.2022.07.177
   Cheng JF, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2973
   Du SD, 2021, IEEE T SIGNAL PROCES, V69, P4623, DOI 10.1109/TSP.2021.3101979
   Fan SH, 2020, WEB CONFERENCE 2020: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2020), P3070, DOI 10.1145/3366423.3380079
   Fei Xue, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11372, DOI 10.1109/CVPR42600.2020.01139
   Gao HY, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P2743, DOI 10.1145/3308558.3313395
   Huang ZY, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2563
   Kanezaki A, 2021, IEEE T PATTERN ANAL, V43, P269, DOI 10.1109/TPAMI.2019.2922640
   Kang Z, 2020, AAAI CONF ARTIF INTE, V34, P4412
   Khan MR, 2019, AAAI CONF ARTIF INTE, P606
   Kipf T.N., 2017, P INT C LEARN REPR S
   Li QM, 2018, AAAI CONF ARTIF INTE, P3538
   Li S, 2020, AAAI CONF ARTIF INTE, V34, P4691
   Li XL, 2022, IEEE T PATTERN ANAL, V44, P330, DOI 10.1109/TPAMI.2020.3011148
   Lin YJ, 2023, IEEE T PATTERN ANAL, V45, P4447, DOI 10.1109/TPAMI.2022.3197238
   Liu K, 2021, IEEE T MULTIMEDIA, V23, P64, DOI 10.1109/TMM.2020.2974323
   Liu XW, 2019, IEEE T PATTERN ANAL, V41, P2410, DOI 10.1109/TPAMI.2018.2879108
   MacQueen J., 1967, P 5 BERK S MATH STAT, P281
   Mai SJ, 2020, AAAI CONF ARTIF INTE, V34, P164
   Nie F., 2016, IJCAI, P1881
   Nie FP, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2564
   Peng X, 2019, PR MACH LEARN RES, V97
   Runwu Zhou, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14607, DOI 10.1109/CVPR42600.2020.01463
   Tang C, 2020, AAAI CONF ARTIF INTE, V34, P5924
   Tang C, 2019, IEEE T MULTIMEDIA, V21, P1724, DOI 10.1109/TMM.2018.2889560
   Valsesia D, 2021, IEEE T MULTIMEDIA, V23, P402, DOI 10.1109/TMM.2020.2976627
   Velickovic Petar, 2018, INT C LEARN REPR
   Wang BY, 2021, IEEE T MULTIMEDIA, V23, P216, DOI 10.1109/TMM.2020.2975394
   Wang C, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3670
   Wang H, 2020, IEEE T KNOWL DATA EN, V32, P1116, DOI 10.1109/TKDE.2019.2903810
   Wang SP, 2022, IEEE T PATTERN ANAL, V44, P5042, DOI 10.1109/TPAMI.2021.3082632
   Wang XB, 2019, PATTERN RECOGN, V88, P50, DOI 10.1016/j.patcog.2018.09.009
   Wang ZD, 2019, PROC CVPR IEEE, P1117, DOI 10.1109/CVPR.2019.00121
   Xhonneux Louis-Pascal, 2020, INT C MACHINE LEARNI, P10432
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2149
   Xiao SX, 2023, IEEE T BIG DATA, V9, P254, DOI 10.1109/TBDATA.2022.3160477
   Xiao SX, 2022, MACH VISION APPL, V33, DOI 10.1007/s00138-021-01251-0
   Xie Y, 2021, IEEE T KNOWL DATA EN, V33, P3594, DOI 10.1109/TKDE.2020.2973981
   Xu KYL, 2018, PR MACH LEARN RES, V80
   Xu N, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3968
   Yang MX, 2023, IEEE T PATTERN ANAL, V45, P1055, DOI 10.1109/TPAMI.2022.3155499
   Yoon S, 2019, PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM '19), P2093, DOI 10.1145/3357384.3358148
   Zhan K, 2019, IEEE T IMAGE PROCESS, V28, P1261, DOI 10.1109/TIP.2018.2877335
   Zhan K, 2018, IEEE T CYBERNETICS, V48, P2887, DOI 10.1109/TCYB.2017.2751646
   Zhang CQ, 2017, PROC CVPR IEEE, P4333, DOI 10.1109/CVPR.2017.461
   Zhang C, 2019, PROC CVPR IEEE, P9444, DOI 10.1109/CVPR.2019.00968
   Zhang MY, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4431
   Zhang Z, 2019, IEEE T PATTERN ANAL, V41, P1774, DOI 10.1109/TPAMI.2018.2847335
   Zhao HD, 2017, AAAI CONF ARTIF INTE, P2921
   Zhou ZX, 2019, J MACH LEARN RES, V20
NR 53
TC 11
Z9 11
U1 13
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 9203
EP 9215
DI 10.1109/TMM.2023.3248173
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP7K5
UT WOS:001133324200027
DA 2024-07-18
ER

PT J
AU Xing, ZN
   Wu, YC
   Liu, S
   Di, SZ
   Ma, HM
AF Xing, Zhening
   Wu, Yuchen
   Liu, Si
   Di, Shangzhe
   Ma, Huimin
TI Virtual Try-On With Garment Self-Occlusion Conditions
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Index Terms-Virtual try-on; generative adversarial network;
   self-occlusion
AB Image-based virtual try-on focuses on changing the model's garment item to the target ones and preserving other visual features. To preserve the texture detail of the given in-shop garment, former methods use geometry-based methods (e.g., Thin-plate-spline interpolation) to realize garment warping. However, due to limited degree of freedom, geometry-based methods perform poorly when garment self-occlusion occurs, which is common in daily life. To address this challenge, we propose a novel occlusion-focused virtual try-on system. Compared to previous ones, our system contains three critical submodules, namely, Garment Part Modeling (GPM), a group of Garment Part Generators (GPGs), and Overlap Relation Estimator (ORE). GPM takes the pose landmarks as input, and progressively models the mask of body parts and garments. Based on these masks, GPGs are introduced to generate each garment part. Finally, ORE is proposed to model the overlap relationships between each garment part, and we bind the generated garments under the guidance of overlap relationships predicted by ORE. To make the most of extracted overlap relationships, we proposed an IoU-based hard example mining method for loss terms to handle the sparsity of the self-occlusion samples in the dataset. Furthermore, we introduce part affinity field as pose representation instead of landmark used widely by previous methods and achieve accuracy improvement on try-on layout estimation stage. We evaluate our model on the VITON dataset and found it can outperform previous approaches, especially on samples with garment self-occlusion.
C1 [Xing, Zhening; Wu, Yuchen; Ma, Huimin] Univ Sci & Technol Beijing, Sch Comp & Commuicat Engn, Beijing 100083, Peoples R China.
   [Liu, Si; Di, Shangzhe] Beihang Univ, Sch Comp Sci & Engn, Beijing 100083, Peoples R China.
C3 University of Science & Technology Beijing; Beihang University
RP Ma, HM (corresponding author), Univ Sci & Technol Beijing, Sch Comp & Commuicat Engn, Beijing 100083, Peoples R China.
EM leoxing1996@gmail.com; curiosity_wu@outlook.com; liusi@buaa.edu.cn;
   shangzhe.di@gmail.com; mhmpub@ustb.edu.cn
RI Wu, Yuchen/GWU-7608-2022
OI liu, si/0000-0002-9180-2935
FU National Natural Science Foundation of China [U20B2062]
FX This work was supported by the National Natural Science Foundation of
   China under Grant U20B2062. The Associate Editor coordinating the review
   of this manuscript and approving it for publication was Dr. Chang Xu.
CR Bhatnagar BL, 2019, IEEE I CONF COMP VIS, P5419, DOI 10.1109/ICCV.2019.00552
   BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792
   Brouet R, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185532
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong H, 2019, IEEE I CONF COMP VIS, P9025, DOI 10.1109/ICCV.2019.00912
   Ge YY, 2021, PROC CVPR IEEE, P8481, DOI 10.1109/CVPR46437.2021.00838
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guan P, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185531
   Güler RA, 2018, PROC CVPR IEEE, P7297, DOI 10.1109/CVPR.2018.00762
   Han XT, 2019, IEEE I CONF COMP VIS, P10470, DOI 10.1109/ICCV.2019.01057
   Han XT, 2018, PROC CVPR IEEE, P7543, DOI 10.1109/CVPR.2018.00787
   Han Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7847, DOI 10.1109/CVPR42600.2020.00787
   Hsieh CW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P275, DOI 10.1145/3343031.3351075
   Issenhuth T, 2019, Arxiv, DOI arXiv:1906.01347
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jetchev N, 2017, IEEE INT CONF COMP V, P2287, DOI 10.1109/ICCVW.2017.269
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kingma D. P., 2014, arXiv
   Lee HJ, 2019, IEEE INT CONF COMP V, P3129, DOI 10.1109/ICCVW.2019.00381
   Li YN, 2019, PROC CVPR IEEE, P3688, DOI 10.1109/CVPR.2019.00381
   Liu ZW, 2016, LECT NOTES COMPUT SC, V9906, P229, DOI 10.1007/978-3-319-46475-6_15
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Minar M. R., 2020, CPVTON: Clothing shape and texture preserving image-based virtual try-on
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Miyato T., 2018, 6 INT C LEARNING REP
   Neuberger A, 2020, PROC CVPR IEEE, P5183, DOI 10.1109/CVPR42600.2020.00523
   Pandey N., 2019, arXiv
   Park Taesung, 2020, EUR C COMP VIS, P319, DOI [DOI 10.1007/978-3-030-58545-719, DOI 10.1007/978-3-030-58545-7_19]
   Patel Chaitanya, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7363, DOI 10.1109/CVPR42600.2020.00739
   Pons-Moll G, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073711
   Raj A, 2018, LECT NOTES COMPUT SC, V11216, P679, DOI 10.1007/978-3-030-01258-8_41
   Rohmer D, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866183
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Roy D., 2020, arXiv
   Santesteban I, 2019, COMPUT GRAPH FORUM, V38, P355, DOI 10.1111/cgf.13643
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Wang BC, 2018, LECT NOTES COMPUT SC, V11217, P607, DOI 10.1007/978-3-030-01261-8_36
   Wang JH, 2020, Arxiv, DOI arXiv:1912.06324
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Yu RY, 2019, IEEE I CONF COMP VIS, P10510, DOI 10.1109/ICCV.2019.01061
   Yuan ML, 2013, IEEE T MULTIMEDIA, V15, P1958, DOI 10.1109/TMM.2013.2280560
   Zheng N, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P266, DOI 10.1145/3343031.3350946
   Zhenyu Xie, 2020, Pattern Recognition and Computer Vision. Third Chinese Conference, PRCV 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12307), P286, DOI 10.1007/978-3-030-60636-7_24
NR 47
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7323
EP 7336
DI 10.1109/TMM.2022.3221346
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000045
DA 2024-07-18
ER

PT J
AU Xu, CP
   Jia, WJ
   Wang, RM
   Luo, XA
   He, XJ
AF Xu, Chengpei
   Jia, Wenjing
   Wang, Ruomei
   Luo, Xiaonan
   He, Xiangjian
TI MorphText: Deep Morphology Regularized Accurate Arbitrary-Shape Scene
   Text Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Morphology; Couplings; Shape; Morphological operations; Image
   segmentation; Visualization; Feature extraction; Arbitrary-shape scene
   text detection; bottom-up methods; deep morphology; regularized text
   segments
AB Bottom-up text detection methods play an important role in arbitrary-shape scene text detection but there are two restrictions preventing them from achieving their great potential, i.e., 1) the accumulation of false text segment detections, which affects subsequent processing, and 2) the difficulty of building reliable connections between text segments. Targeting these two problems, we propose a novel approach, named "MorphText," to capture the regularity of texts by embedding deep morphology for arbitrary-shape text detection. Towards this end, two deep morphological modules are designed to regularize text segments and determine the linkage between them. First, a Deep Morphological Opening (DMOP) module is constructed to remove false text segment detections generated in the feature extraction process. Then, a Deep Morphological Closing (DMCL) module is proposed to allow text instances of various shapes to stretch their morphology along their most significant orientation while deriving their connections. Extensive experiments conducted on four challenging benchmark datasets (CTW1500, Total-Text, MSRA-TD500 and ICDAR2017) demonstrate that our proposed MorphText outperforms both top-down and bottom-up state-of-the-art arbitrary-shape scene text detection approaches.
C1 [Xu, Chengpei; Jia, Wenjing; He, Xiangjian] Univ Technol Sydney, Fac Engn & IT, Ultimo, NSW 2007, Australia.
   [He, Xiangjian] Univ Nottingham Ningbo China, Sch Comp Sci, Ningbo 315104, Zhejiang, Peoples R China.
   [Wang, Ruomei] Sun Yat sen Univ, Natl Engn Res Ctr Digital Life, Sch Data & Comp Sci, Guangzhou 510275, Guangdong, Peoples R China.
   [Luo, Xiaonan] Guilin Univ Elect Technol, Sch Comp Sci & Informat Secur, Guilin 541004, Peoples R China.
C3 University of Technology Sydney; University of Nottingham Ningbo China;
   Sun Yat Sen University; Guilin University of Electronic Technology
RP He, XJ (corresponding author), Univ Technol Sydney, Fac Engn & IT, Ultimo, NSW 2007, Australia.; He, XJ (corresponding author), Univ Nottingham Ningbo China, Sch Comp Sci, Ningbo 315104, Zhejiang, Peoples R China.
EM chengpei.xu@student.uts.edu.au; wenjing.jia@uts.edu.au;
   isswrm@mail.sysu.edu.cn; luoxn@guet.edu.cn; Sean.He@nottingham.edu.cn
OI He, Xiangjian/0000-0001-8962-540X; Jia, Wenjing/0000-0002-0940-3338
FU National Key R&D Program of China [2018AAA0100300]; UTSFEIT Research
   Scholarship; China Natural Science Foundation [61976037]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2018AAA0100300, in part by UTSFEIT Research Scholarship, and
   in part by China Natural Science Foundation under Grant 61976037.
CR Baek Y, 2019, PROC CVPR IEEE, P9357, DOI 10.1109/CVPR.2019.00959
   Ch'ng CK, 2017, PROC INT CONF DOC, P935, DOI 10.1109/ICDAR.2017.157
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Dai PW, 2021, PROC CVPR IEEE, P7389, DOI 10.1109/CVPR46437.2021.00731
   Dai PW, 2022, IEEE T MULTIMEDIA, V24, P1883, DOI 10.1109/TMM.2021.3073575
   Dai PW, 2020, IEEE T MULTIMEDIA, V22, P1969, DOI 10.1109/TMM.2019.2952978
   Feng W, 2019, IEEE I CONF COMP VIS, P9075, DOI 10.1109/ICCV.2019.00917
   Franchi G, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2020.107246
   Gupta A, 2016, PROC CVPR IEEE, P2315, DOI 10.1109/CVPR.2016.254
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Kipf T.N., 2017, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1609.02907
   Liao MH, 2020, AAAI CONF ARTIF INTE, V34, P11474
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu H., 2020, Puzzlenet: scene text detection by segment context graph learning
   Liu YL, 2019, PATTERN RECOGN, V90, P337, DOI 10.1016/j.patcog.2019.02.002
   Liu ZC, 2019, PROC CVPR IEEE, P7261, DOI 10.1109/CVPR.2019.00744
   Long SB, 2018, LECT NOTES COMPUT SC, V11206, P19, DOI 10.1007/978-3-030-01216-8_2
   Ma CX, 2021, PATTERN RECOGN, V111, DOI 10.1016/j.patcog.2020.107684
   Mondal R., 2020, Math. Morphol.-Theory Appl., V4, P87, DOI DOI 10.1515/MATHM-2020-0103
   Mondal R, 2020, Arxiv, DOI arXiv:1901.00109
   Mondal R, 2019, LECT NOTES COMPUT SC, V11414, P262, DOI 10.1007/978-3-030-14085-4_21
   Nayef N, 2017, PROC INT CONF DOC, P1454, DOI 10.1109/ICDAR.2017.237
   Nogueira K, 2021, IEEE ACCESS, V9, P114308, DOI 10.1109/ACCESS.2021.3104405
   Qiao L, 2020, AAAI CONF ARTIF INTE, V34, P11899
   Serra J., 1983, IMAGE ANAL MATH MORP
   Shanyu Xiao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P108, DOI 10.1007/978-3-030-58526-6_7
   Shi BG, 2017, PROC CVPR IEEE, P3482, DOI 10.1109/CVPR.2017.371
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Tian ZT, 2019, PROC CVPR IEEE, P4229, DOI 10.1109/CVPR.2019.00436
   VATTI BR, 1992, COMMUN ACM, V35, P56, DOI 10.1145/129902.129906
   Wang FF, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P111, DOI 10.1145/3394171.3413819
   Wang WH, 2019, PROC CVPR IEEE, P9328, DOI 10.1109/CVPR.2019.00956
   Wang XB, 2019, PROC CVPR IEEE, P6442, DOI 10.1109/CVPR.2019.00661
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang Yaxing, 2020, P IEEE CVF C COMP VI
   Xie EZ, 2019, AAAI CONF ARTIF INTE, P9038
   Xu YC, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2900589
   Yao C, 2012, PROC CVPR IEEE, P1083, DOI 10.1109/CVPR.2012.6247787
   Ye J, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P516
   Yu F, 2018, PROC CVPR IEEE, P2403, DOI 10.1109/CVPR.2018.00255
   Zamora E, 2017, NEUROCOMPUTING, V260, P420, DOI 10.1016/j.neucom.2017.04.044
   Zhang CQ, 2019, PROC CVPR IEEE, P10544, DOI 10.1109/CVPR.2019.01080
   Zhang S, 2021, IEEE T MULTIMEDIA, V23, P454, DOI 10.1109/TMM.2020.2978630
   Zhang Shi-Xue, 2020, IEEE C COMPUTER VISI, P9699
   Zhang WQ, 2021, LECT NOTES COMPUT SC, V12824, P79, DOI 10.1007/978-3-030-86337-1_6
   Zhu YQ, 2021, PROC CVPR IEEE, P3122, DOI 10.1109/CVPR46437.2021.00314
NR 47
TC 1
Z9 1
U1 11
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4199
EP 4212
DI 10.1109/TMM.2022.3172547
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA W1OJ1
UT WOS:001089390200010
DA 2024-07-18
ER

PT J
AU Ye, ZP
   Xia, MF
   Yi, R
   Zhang, JY
   Lai, YK
   Huang, XW
   Zhang, GX
   Liu, YJ
AF Ye, Zipeng
   Xia, Mengfei
   Yi, Ran
   Zhang, Juyong
   Lai, Yu-Kun
   Huang, Xuwei
   Zhang, Guoxin
   Liu, Yong-Jin
TI Audio-Driven Talking Face Video Generation With Dynamic Convolution
   Kernels
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dynamic kernel; convolutional neural network; multi-modal generation
   task; audio-driven talking-face generation
ID FUSION
AB In this paper, we present a dynamic convolution kernel (DCK) strategy for convolutional neural networks. Using a fully convolutional network with the proposed DCKs, high-quality talking-face video can be generated from multi-modal sources (i.e., unmatched audio and video) in real time, and our trained model is robust to different identities, head postures, and input audios. Our proposed DCKs are specially designed for audio-driven talking face video generation, leading to a simple yet effective end-to-end system. We also provide a theoretical analysis to interpret why DCKs work. Experimental results show that our method can generate high-quality talking-face video with background at 60 fps. Comparison and evaluation between our method and the state-of-the-art methods demonstrate the superiority of our method.
C1 [Ye, Zipeng; Xia, Mengfei; Liu, Yong-Jin] Tsinghua Univ, Dept Comp Sci & Technol, Key Lab Pervas Comp, MOE,BNRist, Beijing 100084, Peoples R China.
   [Yi, Ran] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Beijing 100084, Peoples R China.
   [Zhang, Juyong] Univ Sci & Technol China, Sch Math Sci, Hefei 230026, Peoples R China.
   [Lai, Yu-Kun] Cardiff Univ, Sch Comp Sci & Informat, Cardiff CF24 3AA, Wales.
   [Huang, Xuwei; Zhang, Guoxin] Kwai Inc, Beijing 10085, Peoples R China.
C3 Tsinghua University; Shanghai Jiao Tong University; Chinese Academy of
   Sciences; University of Science & Technology of China, CAS; Cardiff
   University
RP Liu, YJ (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, Key Lab Pervas Comp, MOE,BNRist, Beijing 100084, Peoples R China.; Yi, R (corresponding author), Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Beijing 100084, Peoples R China.
EM yezp17@mails.tsinghua.edu.cn; xiamf16@mails.tsinghua.edu.cn;
   ranyi@sjtu.edu.cn; juyong@ustc.edu.cn; laiy4@cardiff.ac.uk;
   huangxuwei@kuaishou.com; zhangguoxin@kuaishou.com;
   liuyongjin@tsinghua.edu.cn
RI Yi, Ran/AAU-6636-2021; Lai, Yu-Kun/D-2343-2010
OI Yi, Ran/0000-0003-1858-3358; Ye, Zipeng/0000-0002-4322-7550; Lai,
   Yukun/0000-0002-2094-5680
FU Natural Science Foundation of China [61725204]; Tsinghua University
   Initiative Scientific Research Program; Shanghai Municipal Science and
   Technology Major Project [2021SHZDZX0102]
FX This work was supported in part by the Natural Science Foundation of
   China under Grant 61725204, in part by the Tsinghua University
   Initiative Scientific Research Program, and in part by the Shanghai
   Municipal Science and Technology Major Project under Grant
   2021SHZDZX0102.
CR Atrey PK, 2010, MULTIMEDIA SYST, V16, P345, DOI 10.1007/s00530-010-0182-0
   Averbuch-Elor H, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130818
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Buitelaar P, 2018, IEEE T MULTIMEDIA, V20, P2454, DOI 10.1109/TMM.2018.2798287
   Cao C, 2014, IEEE T VIS COMPUT GR, V20, P413, DOI 10.1109/TVCG.2013.249
   Chen LL, 2019, PROC CVPR IEEE, P7824, DOI 10.1109/CVPR.2019.00802
   Chen LL, 2018, LECT NOTES COMPUT SC, V11211, P538, DOI 10.1007/978-3-030-01234-2_32
   Chung J. S., 2017, P BRIT MACH VIS C
   Chung JS, 2017, LECT NOTES COMPUT SC, V10112, P87, DOI 10.1007/978-3-319-54184-6_6
   Fan B, 2015, INT CONF ACOUST SPEE, P4884, DOI 10.1109/ICASSP.2015.7178899
   Fried O, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323028
   Ha D., 2017, ICLR
   Hall DL, 1997, P IEEE, V85, P6, DOI [10.1109/5.554205, 10.1109/ISCAS.1998.705329]
   Hori C, 2017, IEEE I CONF COMP VIS, P4203, DOI 10.1109/ICCV.2017.450
   Jiang YG, 2018, IEEE T MULTIMEDIA, V20, P3137, DOI 10.1109/TMM.2018.2823900
   Kalman R E., 1960, J BASIC ENG, V82, P35, DOI DOI 10.1115/1.3662552
   Kim H, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201283
   King DE, 2009, J MACH LEARN RES, V10, P1755
   Kuang Q, 2020, IEEE T MULTIMEDIA, V22, P2623, DOI 10.1109/TMM.2019.2960656
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Lin SS, 2021, IEEE T MULTIMEDIA, V23, P1581, DOI 10.1109/TMM.2020.3001497
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   NAZZARO JR, 1970, J EXP PSYCHOL, V84, P477, DOI 10.1037/h0020861
   Nie XC, 2019, IEEE I CONF COMP VIS, P6941, DOI 10.1109/ICCV.2019.00704
   Nojavanasghari B, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P284, DOI 10.1145/2993148.2993176
   Paszke Adam, 2017, NIPS W
   Prajwal KR, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P484, DOI 10.1145/3394171.3413532
   Pumarola A, 2018, LECT NOTES COMPUT SC, V11214, P835, DOI 10.1007/978-3-030-01249-6_50
   Ronneberger O, 2015, PROC INT C MED IMAGE, P1, DOI [10.1007/978-3-319-24574-4_28, DOI 10.1007/978-3-319-24574-4_28]
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song L., 2020, EVERYBODYS TALKIN LE, Vabs/2001.05201, P1
   Song Y, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P919
   Thies Justus, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P716, DOI 10.1007/978-3-030-58517-4_42
   Thies J, 2016, PROC CVPR IEEE, P2387, DOI 10.1109/CVPR.2016.262
   Vougioukas K., 2018, PROC BRIT MACH VIS C
   Vougioukas K, 2020, INT J COMPUT VISION, V128, P1398, DOI 10.1007/s11263-019-01251-8
   Wang SF, 2020, IEEE T MULTIMEDIA, V22, P1084, DOI 10.1109/TMM.2019.2934824
   Wen X, 2020, IEEE T VIS COMPUT GR, V26, P3457, DOI 10.1109/TVCG.2020.3023573
   Wiles O, 2018, LECT NOTES COMPUT SC, V11217, P690, DOI 10.1007/978-3-030-01261-8_41
   Xi Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12332, DOI 10.1109/CVPR42600.2020.01235
   Yi R., 2020, AUDIO DRIVEN TALKING, Vabs/2002.10137, P1
   Yinpeng Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11027, DOI 10.1109/CVPR42600.2020.01104
   Zakharov E., 2019, arXiv
   Zamora-Esquivel J, 2019, IEEE INT CONF COMP V, P1998, DOI 10.1109/ICCVW.2019.00249
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang Y. X., 2019, arXiv
   Zhou H, 2019, AAAI CONF ARTIF INTE, P9299
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 49
TC 10
Z9 11
U1 4
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2033
EP 2046
DI 10.1109/TMM.2022.3142387
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100035
OA Green Accepted, Green Submitted
DA 2024-07-18
ER

PT J
AU Yi, XK
   Wang, HL
   Kwong, S
   Kuo, CCJ
AF Yi, Xiaokai
   Wang, Hanli
   Kwong, Sam
   Jay Kuo, C. -C.
TI Task-Driven Video Compression for Humans and Machines: Framework Design
   and Optimization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video compression; Task analysis; Image coding; Machine vision; Feature
   extraction; Semantics; Neural networks; neural network; video coding for
   machine; multi-task optimization; action recognition
AB Learned video compression has developed rapidly and achieved impressive progress in recent years. Despite efficient compression performance, existing signal fidelity oriented or semantic fidelity oriented video compression methods limit the capability to meet the requirements of both machine and human vision. To address this problem, a task-driven video compression framework is proposed to flexibly support vision tasks for both human vision and machine vision. Specifically, to improve the compression performance, the backbone of the video compression framework is optimized by using three novel modules, including multi-scale motion estimation, multi-frame feature fusion, and reference based in-loop filters. Then, based on the proposed efficient compression backbone, a task-driven optimization approach is designed to achieve the trade-off between signal fidelity oriented compression and semantic fidelity oriented compression. Moreover, a post-filter module is employed for the framework to further improve the performance of the human vision branch. Finally, rate-distortion performance, rate-accuracy performance, and subjective quality are employed as the evaluation metrics, and experimental results show the superiority of the proposed framework for both human vision and machine vision.
C1 [Yi, Xiaokai; Wang, Hanli] Tongji Univ, Dept Comp Sci & Technol, Minist Educ, Key Lab Embedded Syst & Serv Comp, Shanghai 200092, Peoples R China.
   [Kwong, Sam] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
   [Jay Kuo, C. -C.] Univ Southern Calif, Signal & Image Proc Inst, Ming Hsieh Dept Elect Engn, Los Angeles, CA 90089 USA.
C3 Tongji University; City University of Hong Kong; University of Southern
   California
RP Wang, HL (corresponding author), Tongji Univ, Dept Comp Sci & Technol, Minist Educ, Key Lab Embedded Syst & Serv Comp, Shanghai 200092, Peoples R China.
EM xkyi@tongji.edu.cn; hanliwang@tongji.edu.cn; cssamk@cityu.edu.hk;
   cckuo@sipi.usc.edu
RI Wang, Hanli/G-5111-2014; Kwong, Sam/C-9319-2012; Kuo, C.-C.
   Jay/A-7110-2011
OI Wang, Hanli/0000-0002-9999-4871; Kwong, Sam/0000-0001-7484-7261; Kuo,
   C.-C. Jay/0000-0001-9474-5035
FU National Natural Science Foundation of China
FX No Statement Available
CR Bellard F., 2020, FFmpeg multimedia system
   Bellard Fabrice, 2018, Bpg image format
   Bjotegaard G., 2001, VCEGM33
   Bottou Leon, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P421, DOI 10.1007/978-3-642-35289-8_25
   Bross B, 2021, P IEEE, V109, P1463, DOI 10.1109/JPROC.2020.3043399
   Chan K. C., 2021, P IEEE CVF C COMP VI, P5972
   Chan KCK, 2022, Arxiv, DOI arXiv:2204.05308
   Chan KCK, 2021, AAAI CONF ARTIF INTE, V35, P973
   Chen Z, 2018, Arxiv, DOI arXiv:1809.06196
   Choi H, 2022, IEEE T IMAGE PROCESS, V31, P2739, DOI 10.1109/TIP.2022.3160602
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Duan LY, 2019, IEEE MULTIMEDIA, V26, P44, DOI 10.1109/MMUL.2018.2873844
   Duan LY, 2016, IEEE T IMAGE PROCESS, V25, P179, DOI 10.1109/TIP.2015.2500034
   Duan LY, 2020, IEEE T IMAGE PROCESS, V29, P8680, DOI 10.1109/TIP.2020.3016485
   Gao CS, 2023, IEEE T MULTIMEDIA, V25, P721, DOI 10.1109/TMM.2021.3130754
   Gao F, 2018, IEEE T MULTIMEDIA, V20, P2774, DOI 10.1109/TMM.2018.2818012
   Gao W., 2021, arXiv
   Guo Lu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P456, DOI 10.1007/978-3-030-58536-5_27
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu ZH, 2021, PROC CVPR IEEE, P1502, DOI 10.1109/CVPR46437.2021.00155
   Jinyoung Choi, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P309, DOI 10.1007/978-3-030-58565-5_19
   Kendall A, 2018, PROC CVPR IEEE, P7482, DOI 10.1109/CVPR.2018.00781
   Kingma D. P., 2015, P INT C LEARN REPR, P1, DOI DOI 10.1002/9781118900772.ETRDS0277
   Li S, 2019, PROC CVPR IEEE, P10514, DOI 10.1109/CVPR.2019.01077
   Li TY, 2021, IEEE T IMAGE PROCESS, V30, P5377, DOI 10.1109/TIP.2021.3083447
   Liebel L, 2018, Arxiv, DOI [arXiv:1805.06334, DOI 10.48550/ARXIV.1805.06334]
   Lin JP, 2019, IEEE T CIRC SYST VID, V29, P3701, DOI 10.1109/TCSVT.2018.2884203
   Liu HJ, 2021, IEEE T CIRC SYST VID, V31, P3182, DOI 10.1109/TCSVT.2020.3035680
   Liu K, 2021, INT J COMPUT VISION, V129, P2605, DOI 10.1007/s11263-021-01491-7
   Lu G, 2021, IEEE T PATTERN ANAL, V43, P3292, DOI 10.1109/TPAMI.2020.2988453
   Mercat A, 2020, MMSYS'20: PROCEEDINGS OF THE 2020 MULTIMEDIA SYSTEMS CONFERENCE, P297, DOI 10.1145/3339825.3394937
   Pan ZQ, 2020, IEEE T IMAGE PROCESS, V29, P5352, DOI 10.1109/TIP.2020.2982534
   Redondi A, 2013, IEEE INT WORKSH MULT, P278, DOI 10.1109/MMSP.2013.6659301
   Soomro K, 2012, Arxiv, DOI arXiv:1212.0402
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sun HM, 2020, IEEE T MULTIMEDIA, V22, P2764, DOI 10.1109/TMM.2019.2963620
   Tian YP, 2020, PROC CVPR IEEE, P3357, DOI 10.1109/CVPR42600.2020.00342
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   Wang HG, 2016, IEEE IMAGE PROC, P1509, DOI 10.1109/ICIP.2016.7532610
   Wang SR, 2020, Arxiv, DOI arXiv:2002.03627
   Wang SR, 2022, IEEE T MULTIMEDIA, V24, P3169, DOI 10.1109/TMM.2021.3094300
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Wu YW, 2019, IEEE T MULTIMEDIA, V21, P388, DOI 10.1109/TMM.2018.2856628
   Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2
   Yang FZ, 2020, PROC CVPR IEEE, P5790, DOI 10.1109/CVPR42600.2020.00583
   Yang S, 2021, IEEE T MULTIMEDIA, V23, P2957, DOI 10.1109/TMM.2021.3068580
   Yang WH, 2021, Arxiv, DOI arXiv:2110.09241
   Zhang Y, 2022, IEEE T KNOWL DATA EN, V34, P5586, DOI 10.1109/TKDE.2021.3070203
   Zhihao Hu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P193, DOI 10.1007/978-3-030-58536-5_12
   Zhu LW, 2020, IEEE T MULTIMEDIA, V22, P45, DOI 10.1109/TMM.2019.2924591
NR 51
TC 1
Z9 1
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8091
EP 8102
DI 10.1109/TMM.2022.3233245
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000042
DA 2024-07-18
ER

PT J
AU Zhu, YW
   Huang, YK
   Qiao, XQ
   Tan, ZJ
   Bai, BY
   Ma, HD
   Dustdar, S
AF Zhu, Yuanwei
   Huang, Yakun
   Qiao, Xiuquan
   Tan, Zhijie
   Bai, Boyuan
   Ma, Huadong
   Dustdar, Schahram
TI A Semantic-Aware Transmission With Adaptive Control Scheme for
   Volumetric Video Service
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Streaming media; Point cloud compression; Feature extraction; Semantics;
   Adaptation models; Real-time systems; Bandwidth; Point cloud video;
   reinforcement learning; adaptive transmission; semantic-aware
ID QUALITY PREDICTION; CHALLENGES; DISTANCE
AB Volumetric video provides a more immersive holographic virtual experience than conventional video services such as 360-degree and virtual reality (VR) videos. However, due to ultra-high bandwidth requirements, existing compression and transmission technology cannot handle the delivery of real-time volumetric video. Unlike traditional compression methods and the approaches that extend 360-degree video streaming, we propose AITransfer, an AI-powered compression and semantic-aware transmission method for point cloud video data (a popular volumetric data format). AITransfer targets the semantic-level communication beyond transmitting raw point cloud video or compressed video with two outstanding contributions: (1) designing an integrated end-to-end architecture with two fundamental contents of feature extraction and reconstruction to reduce the bandwidth consumption and alleviate the computational pressure; and (2) incorporating the dynamic network condition into end-to-end architecture design and employing a deep reinforcement learning-based adaptive control scheme to provide robust transmission. We conduct extensive experiments on the typical datasets and develop a case study to demonstrate the efficiency and effectiveness. The results show that AITransfer can provide extremely efficient point cloud transmission while maintaining considerable user experience with more than 30.72x compression ratio under the existing network environments.
C1 [Zhu, Yuanwei; Huang, Yakun; Qiao, Xiuquan; Tan, Zhijie; Bai, Boyuan] Beijing Univ Posts & Telecommun, State Key Lab Networking & Switching Technol, Beijing 100876, Peoples R China.
   [Ma, Huadong] Beijing Univ Posts & Telecommun, Beijing Key Lab Intell Telecomm Software & Multime, Beijing 100876, Peoples R China.
   [Dustdar, Schahram] Tech Univ Wien, Distributed Syst Grp, A-1040 Vienna, Austria.
C3 Beijing University of Posts & Telecommunications; Beijing University of
   Posts & Telecommunications; Technische Universitat Wien
RP Qiao, XQ (corresponding author), Beijing Univ Posts & Telecommun, State Key Lab Networking & Switching Technol, Beijing 100876, Peoples R China.
EM zhuyw@bupt.edu.cn; ykhuang@bupt.edu.cn; qiaoxq@bupt.edu.cn;
   besttangent@bupt.edu.cn; BXB911@student.bham.ac.uk; mhd@bupt.edu.cn;
   dustdar@dsg.tuwien.ac.at
RI Dustdar, Schahram/G-9877-2015
OI Dustdar, Schahram/0000-0001-6872-8821; Zhu, Yuanwei/0000-0003-2946-1890
FU Funds for International Cooperation and Exchange of NSFC [61720106007];
   National Natural Science Foundation of China [62202065]; China
   Postdoctoral Science Foundation [2022TQ0047, 2022M710465]; Fundamental
   Research Funds for the Central Universities [2019XD-A03-3]; BUPT
   Excellent Ph.D. Students Foundation [CX2022128]
FX This work was supported in part by the Funds for International
   Cooperation and Exchange of NSFC under Grant 61720106007, in part by the
   National Natural Science Foundation of China under Grant 62202065, in
   part by the Project funded by China Postdoctoral Science Foundation
   under Grants 2022TQ0047 and 2022M710465, in part by the Fundamental
   Research Funds for the Central Universities under Grant 2019XD-A03-3,
   and in part by the BUPT Excellent Ph.D. Students Foundation under Grant
   CX2022128.
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   [Anonymous], 2021, Microsoft HoloLens | Mixed Reality Technology for Business
   azure.microsoft, 2021, Azure kinect dk
   Berger M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2451236.2451246
   Butt MA, 1998, IEEE T IMAGE PROCESS, V7, P1477, DOI 10.1109/83.718487
   Chou PA, 2020, IEEE T IMAGE PROCESS, V29, P2203, DOI 10.1109/TIP.2019.2908095
   Clemm A, 2020, IEEE COMMUN MAG, V58, P93, DOI 10.1109/MCOM.001.1900272
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Du JB, 2020, IEEE INTERNET THINGS, V7, P9517, DOI 10.1109/JIOT.2020.3003449
   Du JB, 2019, IEEE T VEH TECHNOL, V68, P1079, DOI 10.1109/TVT.2018.2883156
   google.github, 2021, Draco 3D graphics compression
   Guarda AFR, 2020, IEEE IMAGE PROC, P3354, DOI [10.1109/ICIP40778.2020.9191021, 10.1109/icip40778.2020.9191021]
   Han B, 2020, MOBICOM '20: PROCEEDINGS OF THE 26TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING (MOBICOM 2020), P137, DOI 10.1145/3372224.3380888
   Hosseini M, 2018, PROCEEDINGS OF THE 23TH ACM WORKSHOP ON PACKET VIDEO (PV'18), P25, DOI 10.1145/3210424.3210429
   Huang TX, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P890, DOI 10.1145/3343031.3351061
   Huang YK, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3989, DOI 10.1145/3474085.3475624
   Hubo E, 2006, RT 06: IEEE SYMPOSIUM ON INTERACTIVE RAY TRACING 2006, PROCEEDINGS, P105
   Jang ES, 2019, IEEE SIGNAL PROC MAG, V36, P118, DOI 10.1109/MSP.2019.2900721
   Kan NW, 2019, INT CONF ACOUST SPEE, P4030, DOI 10.1109/ICASSP.2019.8683779
   Krivokuca M., 2018, ISO/IEC JTC1/SC29 WG11 (MPEG), Input Document M42914
   Krivokuca M, 2020, IEEE T IMAGE PROCESS, V29, P2217, DOI 10.1109/TIP.2019.2957853
   Lee K., 2020, PROC 26 ANN INT C MO, P1
   Li J, 2020, IEEE CONF COMPUT, P1326
   Li RH, 2019, IEEE I CONF COMP VIS, P7202, DOI 10.1109/ICCV.2019.00730
   Liu Z, 2021, IEEE NETWORK, V35, P202, DOI 10.1109/MNET.101.2000364
   Mao HZ, 2017, SIGCOMM '17: PROCEEDINGS OF THE 2017 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P197, DOI 10.1145/3098822.3098843
   Mnih V, 2016, PR MACH LEARN RES, V48
   nreal, 2021, Nreal-building mixed reality for everyone
   Pauly M, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P163, DOI 10.1109/VISUAL.2002.1183771
   Qi C., 2017, NeurIPS, P5105, DOI [10.1109/ CVPR.2017.16, DOI 10.1109/CVPR.2017.16]
   Qian F, 2019, HOTMOBILE '19 - PROCEEDINGS OF THE 20TH INTERNATIONAL WORKSHOP ON MOBILE COMPUTING SYSTEMS AND APPLICATIONS, P135, DOI 10.1145/3301293.3302358
   Qian Y, 2021, IEEE T IMAGE PROCESS, V30, P8354, DOI 10.1109/TIP.2021.3115385
   Quach M, 2019, IEEE IMAGE PROC, P4320, DOI [10.1109/ICIP.2019.8803413, 10.1109/icip.2019.8803413]
   Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054
   Rusu R. B., 2011, PROC IEEE INT C ROBO, P1
   Schnabel R., 2006, P S POINT BAS GRAPH, V6, P111, DOI DOI 10.2312/SPBG/SPBG06/111-120
   Sun XB, 2022, IEEE T INTELL TRANSP, V23, P2190, DOI 10.1109/TITS.2020.3034879
   van der Hooft J, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2405, DOI 10.1145/3343031.3350917
   van der Hooft J, 2016, IEEE COMMUN LETT, V20, P2177, DOI 10.1109/LCOMM.2016.2601087
   Wang LS, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P1930, DOI 10.1109/ICASSP39728.2021.9414121
   Winkler S, 2008, IEEE T BROADCAST, V54, P660, DOI 10.1109/TBC.2008.2000733
   Yu LQ, 2018, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2018.00295
   Zhang YW, 2021, IEEE T SERV COMPUT, V14, P1333, DOI 10.1109/TSC.2019.2891517
   Zhang YW, 2021, FUTURE GENER COMP SY, V114, P336, DOI 10.1016/j.future.2020.08.014
   Zhang YW, 2021, IEEE T SERV COMPUT, V14, P695, DOI 10.1109/TSC.2018.2830773
   Zhang YX, 2019, IEEE INFOCOM SER, P1252, DOI [10.1109/INFOCOM.2019.8737361, 10.1109/infocom.2019.8737361]
NR 46
TC 11
Z9 11
U1 8
U2 27
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7160
EP 7172
DI 10.1109/TMM.2022.3217928
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000032
DA 2024-07-18
ER

PT J
AU Zou, SH
   Zuo, XX
   Wang, S
   Qian, YM
   Guo, C
   Cheng, L
AF Zou, Shihao
   Zuo, Xinxin
   Wang, Sen
   Qian, Yiming
   Guo, Chuan
   Cheng, Li
TI Human Pose and Shape Estimation From Single Polarization Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Shape; Three-dimensional displays; Surface reconstruction; Image
   reconstruction; Cameras; Pose estimation; Deep learning; Human pose and
   shape estimation; human shape reconstruction; shape from polarization
AB This paper focuses on a new problem of estimating human pose and shape from single polarization images. Polarization camera is known to be able to capture the polarization of reflected lights that preserves rich geometric cues of an object surface. Inspired by the recent applications in surface normal reconstruction from polarization images, in this paper, we attempt to estimate human pose and shape from single polarization images by leveraging the polarization-induced geometric cues. A dedicated two-stage pipeline is proposed: given a single polarization image, stage one (Polar2Normal) focuses on the fine detailed human body surface normal estimation; stage two (Polar2Shape) then reconstructs clothed human shape from the polarization image and the estimated surface normal. To empirically validate our approach, a dedicated dataset (PHSPD) is constructed, consisting of over 500 K frames with accurate pose and parametric shape annotations. Empirical evaluations on this real-world dataset as well as a synthetic dataset, SURREAL, demonstrate the effectiveness of our approach. It suggests polarization camera as a promising alternative to the more conventional RGB camera for human pose and shape estimation.
C1 [Zou, Shihao; Zuo, Xinxin; Wang, Sen; Guo, Chuan; Cheng, Li] Univ Alberta, Dept Elect & Comp Engn, Edmonton, AB T6G 1H9, Canada.
   [Qian, Yiming] Univ Manitoba, Dept Comp Sci, Winnipeg, MB R3T 2N2, Canada.
C3 University of Alberta; University of Manitoba
RP Cheng, L (corresponding author), Univ Alberta, Dept Elect & Comp Engn, Edmonton, AB T6G 1H9, Canada.
EM szou2@ualberta.ca; xzuo@ualberta.ca; sen9@ualberta.ca;
   yiming.qian@umanitoba.ca; cguo2@ualberta.ca; lcheng5@ualberta.ca
OI Zuo, Xinxin/0000-0002-7116-9634; Wang, Sen/0000-0002-1808-5239; Zou,
   Shihao/0000-0002-9042-6069
FU Alberta Innovates Graduate Student Scholarship for Data-Enabled
   Innovation; University of Alberta Start-up grant; University of
   Alberta-Huawei Joint Innovation Collaboration (UAHJIC) grants; Natural
   Sciences and Engineering Research Council of Canada (NSERC) Discovery
   [RGPIN-2019-04575]
FX The work of Shihao Zou was supported by Alberta Innovates Graduate
   Student Scholarship for Data-Enabled Innovation. This work was supported
   in part by the University of Alberta Start-up grant, in part by the
   University of Alberta-Huawei Joint Innovation Collaboration (UAHJIC)
   grants, and in part by the Natural Sciences and Engineering Research
   Council of Canada (NSERC) Discovery under Grant RGPIN-2019-04575. The
   associate editor coordinating the review of this manuscript and
   approving it for publicationwas Prof. Yadong Mu.
CR Akhter I, 2015, PROC CVPR IEEE, P1446, DOI 10.1109/CVPR.2015.7298751
   Andriluka M, 2018, PROC CVPR IEEE, P5167, DOI 10.1109/CVPR.2018.00542
   Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207
   Atkinson GA, 2006, IEEE T IMAGE PROCESS, V15, P1653, DOI 10.1109/TIP.2006.871114
   Balan A. O., 2007, 2007 IEEE Conference on Computer Vision and Pattern Recognition, P1, DOI [10.1109/CVPR.2007.383340, DOI 10.1109/CVPR.2007.383340]
   Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34
   Bollapragada R, 2018, PR MACH LEARN RES, V80
   Cai YJ, 2019, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2019.00236
   Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   Chen LX, 2018, LECT NOTES COMPUT SC, V11220, P21, DOI 10.1007/978-3-030-01270-0_2
   Ci H, 2019, IEEE I CONF COMP VIS, P2262, DOI 10.1109/ICCV.2019.00235
   Cui ZP, 2017, PROC CVPR IEEE, P369, DOI 10.1109/CVPR.2017.47
   Daly IM, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms12140
   Dibra E, 2017, PROC CVPR IEEE, P5504, DOI 10.1109/CVPR.2017.584
   Dibra E, 2016, INT CONF 3D VISION, P108, DOI 10.1109/3DV.2016.19
   Fang HS, 2018, AAAI CONF ARTIF INTE, P6821
   Gao H, 2019, IEEE I CONF COMP VIS, P9005, DOI 10.1109/ICCV.2019.00910
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Jiang HY, 2019, IEEE I CONF COMP VIS, P5430, DOI 10.1109/ICCV.2019.00553
   Kadambi A, 2017, INT J COMPUT VISION, V125, P34, DOI 10.1007/s11263-017-1025-7
   Kamel A, 2021, IEEE T MULTIMEDIA, V23, P1330, DOI 10.1109/TMM.2020.2999181
   Kanazawa A, 2019, PROC CVPR IEEE, P5597, DOI 10.1109/CVPR.2019.00576
   Kanazawa A, 2018, PROC CVPR IEEE, P7122, DOI 10.1109/CVPR.2018.00744
   Kingma D. P., 2014, arXiv
   Kocabas M, 2020, PROC CVPR IEEE, P5252, DOI 10.1109/CVPR42600.2020.00530
   Kolotouros N, 2019, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2019.00234
   Lassner C., 2017, PROC CVPR IEEE, V2, P3, DOI DOI 10.1109/CVPR.2017.500
   Li SJ, 2015, IEEE I CONF COMP VIS, P2848, DOI 10.1109/ICCV.2015.326
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Lu Y, 2021, IEEE T MULTIMEDIA, V23, P3657, DOI 10.1109/TMM.2020.3029941
   Martinez Julieta, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2659, DOI 10.1109/ICCV.2017.288
   Mehta D, 2017, INT CONF 3D VISION, P506, DOI 10.1109/3DV.2017.00064
   MURRAY RM, 1994, A MATHEMATICAL INTRO
   Nehab D, 2005, ACM T GRAPHIC, V24, P536, DOI 10.1145/1073204.1073226
   Nie BX, 2017, IEEE I CONF COMP VIS, P3467, DOI 10.1109/ICCV.2017.373
   Ning GH, 2018, IEEE T MULTIMEDIA, V20, P1246, DOI 10.1109/TMM.2017.2762010
   Omran M, 2018, INT CONF 3D VISION, P484, DOI 10.1109/3DV.2018.00062
   Park S, 2016, LECT NOTES COMPUT SC, V9915, P156, DOI 10.1007/978-3-319-49409-8_15
   Pavlakos G, 2019, PROC CVPR IEEE, P10967, DOI 10.1109/CVPR.2019.01123
   Pavlakos G, 2018, PROC CVPR IEEE, P7307, DOI 10.1109/CVPR.2018.00763
   Pavlakos G, 2018, PROC CVPR IEEE, P459, DOI 10.1109/CVPR.2018.00055
   Saito S, 2020, PROC CVPR IEEE, P81, DOI 10.1109/CVPR42600.2020.00016
   Saito S, 2019, IEEE I CONF COMP VIS, P2304, DOI 10.1109/ICCV.2019.00239
   Shihao Zou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P351, DOI 10.1007/978-3-030-58568-6_21
   Smith WAP, 2019, IEEE T PATTERN ANAL, V41, P2875, DOI 10.1109/TPAMI.2018.2868065
   Tang SC, 2019, IEEE I CONF COMP VIS, P7749, DOI 10.1109/ICCV.2019.00784
   Tome D, 2017, PROC CVPR IEEE, P5689, DOI 10.1109/CVPR.2017.603
   Varol G, 2018, LECT NOTES COMPUT SC, V11211, P20, DOI 10.1007/978-3-030-01234-2_2
   Varol G, 2017, PROC CVPR IEEE, P4627, DOI 10.1109/CVPR.2017.492
   von Marcard T, 2018, LECT NOTES COMPUT SC, V11214, P614, DOI 10.1007/978-3-030-01249-6_37
   Wandt B, 2019, PROC CVPR IEEE, P7774, DOI 10.1109/CVPR.2019.00797
   Wang CY, 2014, PROC CVPR IEEE, P2369, DOI 10.1109/CVPR.2014.303
   Wang KZ, 2020, IEEE T PATTERN ANAL, V42, P1069, DOI 10.1109/TPAMI.2019.2892452
   Wang M, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P978
   Wehner R, 2006, P NATL ACAD SCI USA, V103, P12575, DOI 10.1073/pnas.0604430103
   Xu YL, 2019, IEEE I CONF COMP VIS, P7759, DOI 10.1109/ICCV.2019.00785
   Yang LW, 2018, PROC CVPR IEEE, P3857, DOI 10.1109/CVPR.2018.00406
   Yang W, 2018, PROC CVPR IEEE, P5255, DOI 10.1109/CVPR.2018.00551
   Yang Z, 2021, PROC CVPR IEEE, P13279, DOI 10.1109/CVPR46437.2021.01308
   Yunhao Ba, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P554, DOI 10.1007/978-3-030-58586-0_33
   ZHANG Q, IEEE T MULTIMEDIA
   Zhao RQ, 2018, IEEE T PATTERN ANAL, V40, P3059, DOI 10.1109/TPAMI.2017.2772922
   Zhao TH, 2019, IEEE T MULTIMEDIA, V21, P114, DOI 10.1109/TMM.2018.2844087
   ZHENG Z, IEEE T PATTERN ANAL
   Zheng ZR, 2019, IEEE I CONF COMP VIS, P7738, DOI 10.1109/ICCV.2019.00783
   Zhou XW, 2016, PROC CVPR IEEE, P4966, DOI 10.1109/CVPR.2016.537
   Zhou XY, 2017, IEEE I CONF COMP VIS, P398, DOI 10.1109/ICCV.2017.51
   ZHU H, IEEE T PATTERN ANAL
   Zuo XX, 2021, IEEE T MULTIMEDIA, V23, P1617, DOI 10.1109/TMM.2020.3001506
NR 71
TC 9
Z9 9
U1 12
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3560
EP 3572
DI 10.1109/TMM.2022.3162469
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FE2H4
UT WOS:001144015500006
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Hao, ZW
   Luo, Y
   Wang, Z
   Hu, H
   An, JP
AF Hao, Zhiwei
   Luo, Yong
   Wang, Zhi
   Hu, Han
   An, Jianping
TI CDFKD-MFS: Collaborative Data-Free Knowledge Distillation via
   Multi-Level Feature Sharing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Generators; Knowledge engineering; Computer architecture; Computational
   modeling; Aggregates; Predictive models; Collaboration; Model
   Compression; Knowledge Distillation; Data-free Distillation;
   Multi-teacher Distillation; Attention
AB Recently, the compression and deployment of powerful deep neural networks (DNNs) on resource-limited edge devices to provide intelligent services have become attractive tasks. Although knowledge distillation (KD) is a feasible solution for compression, its requirement on the original dataset raises privacy concerns. In addition, it is common to integrate multiple pretrained models to achieve satisfactory performance. How to compress multiplemodels into a tinymodel is challenging, especially when the original data are unavailable. To tackle this challenge, we propose a framework termed collaborative data-free knowledge distillation via multi-level feature sharing (CDFKD-MFS), which consists of a multi-header student module, an asymmetric adversarial data-free KD module, and an attention-based aggregation module. In this framework, the student model equipped with a multi-level feature-sharing structure learns from multiple teacher models and is trained together with a generator in an asymmetric adversarial manner. When some real samples are available, the attention module adaptively aggregates predictions of the student headers, which can further improve performance. We conduct extensive experiments on three popular computer visual datasets. In particular, compared with the most competitive alternative, the accuracy of the proposed framework is 1.18% higher on the CIFAR-100 dataset, 1.67% higher on the Caltech-101 dataset, and 2.99% higher on the mini-ImageNet dataset.
C1 [Hao, Zhiwei; Hu, Han; An, Jianping] Beijing Inst Technol, Sch Informat & Elect, Beijing 100081, Peoples R China.
   [Luo, Yong] Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China.
   [Luo, Yong] Hubei Luojia Lab, Wuhan 430072, Peoples R China.
   [Wang, Zhi] Tsinghua Univ, Shenzhen Int Grad Sch, Shenzhen 518055, Peoples R China.
C3 Beijing Institute of Technology; Wuhan University; Tsinghua University
RP Hu, H (corresponding author), Beijing Inst Technol, Sch Informat & Elect, Beijing 100081, Peoples R China.
EM haozhw@bit.edu.cn; luoy-ong@whu.edu.cn; wangzhi@sz.tsinghua.edu.cn;
   hhu@bit.edu.cn; an@bit.edu.cn
OI Hu, Han/0000-0001-7532-0496; Wang, Zhi/0000-0002-5462-6178
FU National Key Research and Development Program of China [2021YFC3300200];
   National Natural Science Foundation of China [61971457]; Special Fund of
   Hubei Luojia Laboratory [220100014]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2021YFC3300200, in part by the
   National Natural Science Foundation of China under Grant 61971457, and
   in part by the Special Fund of Hubei Luojia Laboratory under Grant
   220100014.
CR Rusu AA, 2016, Arxiv, DOI arXiv:1606.04671
   Allen-Zhu Z, 2021, Arxiv, DOI arXiv:2012.09816
   [Anonymous], 2017, P IEEE C COMP VIS PA
   Asif U, 2020, FRONT ARTIF INTEL AP, V325, P953, DOI 10.3233/FAIA200188
   Chen DF, 2021, AAAI CONF ARTIF INTE, V35, P7028
   Chen HT, 2021, PROC CVPR IEEE, P6424, DOI 10.1109/CVPR46437.2021.00636
   Chen HT, 2019, IEEE I CONF COMP VIS, P3513, DOI 10.1109/ICCV.2019.00361
   Chen T, 2020, PR MACH LEARN RES, V119
   Choi Y, 2020, IEEE COMPUT SOC CONF, P3047, DOI 10.1109/CVPRW50498.2020.00363
   Ding QG, 2019, Arxiv, DOI arXiv:1908.05474
   Fang G, 2021, arXiv
   Fang GF, 2020, Arxiv, DOI arXiv:1912.11006
   Lopes RG, 2017, Arxiv, DOI arXiv:1710.07535
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Han L, 2014, AAAI CONF ARTIF INTE, P1854
   Hao Z., 2021, ICME, P1
   Hao ZW, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1803, DOI 10.1145/3474085.3475329
   Haroush Matan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8491, DOI 10.1109/CVPR42600.2020.00852
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Hongxu Yin, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8712, DOI 10.1109/CVPR42600.2020.00874
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang ZH, 2017, Arxiv, DOI arXiv:1707.01219
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Kim S., 2017, PROC INT C LEARN REP
   Kingma D. P, 2015, International Conference on Learning Representations
   Komodakis N, 2017, P ICLR
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Kunran Xu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P664, DOI 10.1007/978-3-030-58595-2_40
   Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79
   Liu SK, 2019, PROC CVPR IEEE, P1871, DOI 10.1109/CVPR.2019.00197
   Liu YA, 2021, PROC CVPR IEEE, P1512, DOI 10.1109/CVPR46437.2021.00156
   Loshchilov I., 2019, arXiv
   Luo L., 2020, arXiv
   Micaelli P., 2019, P ADV NEUR INF PROC, P9547
   Misra I, 2016, PROC CVPR IEEE, P3994, DOI 10.1109/CVPR.2016.433
   Müller R, 2019, ADV NEUR IN, V32
   Nayak GK, 2019, PR MACH LEARN RES, V97
   Nayak GK, 2021, IEEE WINT CONF APPL, P1429, DOI 10.1109/WACV48630.2021.00147
   Park S, 2020, FRONT ARTIF INTEL AP, V325, P1411, DOI 10.3233/FAIA200246
   Passalis N, 2018, LECT NOTES COMPUT SC, V11215, P283, DOI 10.1007/978-3-030-01252-6_17
   Radford A., 2016, COMPUT SCI
   Romero A., 2014, ARXIV14126550
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shoukai Xu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P1, DOI 10.1007/978-3-030-58610-2_1
   Teerapittayanon S, 2017, Arxiv, DOI arXiv:1709.01686
   Tran L, 2021, Arxiv, DOI arXiv:2001.04694
   Truong J., 2021, PROC IEEE C COMPUT V, P4771
   Vaswani A, 2017, ADV NEUR IN, V30
   Velickovic Petar, 2018, PROC INT C LEARN REP
   Vinyals O, 2016, 30 C NEURAL INFORM P, V29
   Wang XW, 2021, PEDIATR NEPHROL, V36, P163, DOI 10.1007/s00467-020-04715-z
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Ye J., 2020, P IEEECVF C COMPUTER, P12516
   Yim J, 2017, PROC CVPR IEEE, P7130, DOI 10.1109/CVPR.2017.754
   Yoo J., 2019, Advances in Neural Information Processing Systems, P2701
   You S, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1285, DOI 10.1145/3097983.3098135
   Zagoruyko S, 2015, PROC CVPR IEEE, P4353, DOI 10.1109/CVPR.2015.7299064
   Zhang Hongyi, 2018, MIXUP EMPIRICAL RISK, DOI DOI 10.48550/ARXIV.1710.09412
   Zhao SC, 2022, IEEE T NEUR NET LEAR, V33, P473, DOI 10.1109/TNNLS.2020.3028503
NR 61
TC 7
Z9 7
U1 3
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 4262
EP 4274
DI 10.1109/TMM.2022.3192663
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 5C3SZ
UT WOS:000864185400002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Kong, YQ
   Wang, YH
   Li, AN
AF Kong, Yongqiang
   Wang, Yunhong
   Li, Annan
TI Spatiotemporal Saliency Representation Learning for Video Action
   Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Object detection; Three-dimensional displays; Spatiotemporal phenomena;
   Computer architecture; Task analysis; Solid modeling; Noise reduction;
   Action recognition; high-quality inputs; salient object detection;
   spatiotemporal CNNs
ID ATTENTION NETWORKS; FUSION
AB Deep convolutional neural networks (CNNs) have achieved great success in human action recognition, however they are still limited in understanding complex and noisy videos owing to the difficulties of exploiting appearance and motion information. Most existing works have been devoted to designing CNN architectures, which overlook the quality of network inputs that is of great importance. This paper provides an alternative solution of action recognition improvement by focusing on the quality of network inputs. A multi-task video salient object detection approach with object-of-interest segmentation scheme, which takes into account both human and action-relevant cues, is proposed to immunize the input video from background clutter. Further, a simple spatiotemporal residual network architecture is presented, which operates on multiple high-quality inputs for long-term action representation learning. Empirical evaluations on various challenging datasets demonstrate that the proposed framework can perform competitively against state-of-the-art. Besides better performance, learning representations of saliency can help prevent the action recognition model from overfitting and speed up the convergence of training.
C1 [Kong, Yongqiang; Wang, Yunhong; Li, Annan] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Sch Comp Sci & Engn, Beijing 100191, Peoples R China.
C3 Beihang University
RP Li, AN (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Sch Comp Sci & Engn, Beijing 100191, Peoples R China.
EM yqkong@buaa.edu.cn; yhwang@buaa.edu.cn; liannan@buaa.edu.cn
RI Li, Annan/KRP-2299-2024
OI Li, Annan/0000-0003-3497-5052
FU Key Program of National Natural Science Foundation of China [U20B2069];
   CCF-Tencent Rhino-Bird Research Fund
FX This work was supported in part by theKey Program of National Natural
   Science Foundation of China under Grant U20B2069 and CCF-Tencent
   Rhino-Bird Research Fund. The associate editor coordinating the review
   of this manuscript and approving it for publicationwas Prof. L. Ballan.
CR [Anonymous], 2018, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2017.2712608
   Ballan L, 2012, IEEE T MULTIMEDIA, V14, P1234, DOI 10.1109/TMM.2012.2191268
   Bilen H, 2018, IEEE T PATTERN ANAL, V40, P2799, DOI 10.1109/TPAMI.2017.2769085
   Bilen H, 2016, PROC CVPR IEEE, P3034, DOI 10.1109/CVPR.2016.331
   Brox T., 2006, PROC EUR C COMPUT VI, P25
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen CLZ, 2020, IEEE T IMAGE PROCESS, V29, P1090, DOI 10.1109/TIP.2019.2934350
   Chen ZY, 2020, AAAI CONF ARTIF INTE, V34, P10599
   Crasto N, 2019, PROC CVPR IEEE, P7874, DOI 10.1109/CVPR.2019.00807
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2019, PROC CVPR IEEE, P8546, DOI 10.1109/CVPR.2019.00875
   Fan QF, 2019, ADV NEUR IN, V32
   Fang YM, 2019, IEEE T IMAGE PROCESS, V28, P2305, DOI 10.1109/TIP.2018.2885229
   Feichtenhofer C., 2016, ADV NEURAL INFORM PR, P3468
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Feng MY, 2019, PROC CVPR IEEE, P1623, DOI 10.1109/CVPR.2019.00172
   Gkioxari G, 2015, IEEE I CONF COMP VIS, P1080, DOI 10.1109/ICCV.2015.129
   Gongyang Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P665, DOI 10.1007/978-3-030-58520-4_39
   Gorelick L, 2007, IEEE T PATTERN ANAL, V29, P2247, DOI 10.1109/TPAMI.2007.70711
   Goyal R, 2017, IEEE I CONF COMP VIS, P5843, DOI 10.1109/ICCV.2017.622
   Gu YC, 2020, AAAI CONF ARTIF INTE, V34, P10869
   Guo F, 2018, IEEE T CYBERNETICS, V48, P3159, DOI 10.1109/TCYB.2017.2761361
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang MK, 2019, NEUROCOMPUTING, V364, P310, DOI 10.1016/j.neucom.2019.07.054
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kay W., 2017, ARXIV170506950
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Lai QX, 2020, IEEE T IMAGE PROCESS, V29, P1113, DOI 10.1109/TIP.2019.2936112
   Le T, 2017, P BRIT MACH VIS C
   Li C, 2019, PROC CVPR IEEE, P7864, DOI 10.1109/CVPR.2019.00806
   Li D, 2019, IEEE T MULTIMEDIA, V21, P416, DOI 10.1109/TMM.2018.2862341
   Li GB, 2018, PROC CVPR IEEE, P3243, DOI 10.1109/CVPR.2018.00342
   Li HF, 2019, IEEE I CONF COMP VIS, P7273, DOI 10.1109/ICCV.2019.00737
   Li J, 2020, IEEE T MULTIMEDIA, V22, P2990, DOI 10.1109/TMM.2020.2965434
   Li YX, 2020, IEEE T MULTIMEDIA, V22, P1153, DOI 10.1109/TMM.2019.2940851
   Li ZY, 2018, COMPUT VIS IMAGE UND, V166, P41, DOI 10.1016/j.cviu.2017.10.011
   Lin J, 2022, IEEE T PATTERN ANAL, V44, P2760, DOI 10.1109/TPAMI.2020.3029799
   Liu Z, 2017, IEEE T CIRC SYST VID, V27, P2527, DOI 10.1109/TCSVT.2016.2595324
   Liu Z, 2014, IEEE T CIRC SYST VID, V24, P1522, DOI 10.1109/TCSVT.2014.2308642
   Lu XK, 2019, PROC CVPR IEEE, P3618, DOI 10.1109/CVPR.2019.00374
   Mathe S, 2015, IEEE T PATTERN ANAL, V37, P1408, DOI 10.1109/TPAMI.2014.2366154
   Monfort M, 2020, IEEE T PATTERN ANAL, V42, P502, DOI 10.1109/TPAMI.2019.2901464
   Nguyen TV, 2015, IEEE T CIRC SYST VID, V25, P77, DOI 10.1109/TCSVT.2014.2333151
   Paszke A, 2019, ADV NEUR IN, V32
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Rapantzikos K, 2009, PROC CVPR IEEE, P1454, DOI 10.1109/CVPRW.2009.5206525
   Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683
   Shi XJ, 2015, ADV NEUR IN, V28
   Shi YM, 2017, IEEE T MULTIMEDIA, V19, P1510, DOI 10.1109/TMM.2017.2666540
   Simonyan K, 2014, ADV NEUR IN, V27
   Song HM, 2018, LECT NOTES COMPUT SC, V11215, P744, DOI 10.1007/978-3-030-01252-6_44
   Soomro Khurram, 2012, UCF101 DATASET 101 H
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   Vig E, 2012, LECT NOTES COMPUT SC, V7578, P84, DOI 10.1007/978-3-642-33786-4_7
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang L., 2016, P ECCV
   Wang LM, 2019, IEEE T PATTERN ANAL, V41, P2740, DOI 10.1109/TPAMI.2018.2868668
   Wang WG, 2022, IEEE T PATTERN ANAL, V44, P3239, DOI 10.1109/TPAMI.2021.3051099
   Wang WG, 2020, IEEE T PATTERN ANAL, V42, P1913, DOI 10.1109/TPAMI.2019.2905607
   Wang WG, 2021, IEEE T PATTERN ANAL, V43, P220, DOI 10.1109/TPAMI.2019.2924417
   Wang WG, 2019, PROC CVPR IEEE, P3059, DOI 10.1109/CVPR.2019.00318
   Wang WG, 2021, IEEE T PATTERN ANAL, V43, P2413, DOI 10.1109/TPAMI.2020.2966453
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P2368, DOI 10.1109/TIP.2017.2787612
   Wang Wenguan, 2018, IEEE Trans Image Process, V27, P38, DOI 10.1109/TIP.2017.2754941
   Wang WG, 2018, IEEE T PATTERN ANAL, V40, P20, DOI 10.1109/TPAMI.2017.2662005
   Wang WG, 2015, IEEE T IMAGE PROCESS, V24, P4185, DOI 10.1109/TIP.2015.2460013
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang XH, 2017, IEEE SIGNAL PROC LET, V24, P510, DOI 10.1109/LSP.2016.2611485
   Wang XH, 2018, IEEE T MULTIMEDIA, V20, P634, DOI 10.1109/TMM.2017.2749159
   Wu RM, 2019, PROC CVPR IEEE, P8142, DOI 10.1109/CVPR.2019.00834
   Yang Z, 2019, IEEE I CONF COMP VIS, P931, DOI 10.1109/ICCV.2019.00102
   Yun K., 2012, 2012 IEEE COMP SOC C, P28, DOI DOI 10.1109/CVPRW.2012.6239234
   Zhao PS, 2021, IEEE T MULTIMEDIA, V23, P3441, DOI 10.1109/TMM.2020.3025665
   Zhao ZC, 2017, IEEE I CONF COMP VIS, P3411, DOI 10.1109/ICCV.2017.367
   Zhou BL, 2018, LECT NOTES COMPUT SC, V11205, P831, DOI 10.1007/978-3-030-01246-5_49
   Zhou TF, 2020, IEEE T IMAGE PROCESS, V29, P8326, DOI 10.1109/TIP.2020.3013162
   Zhou XF, 2018, IEEE T MULTIMEDIA, V20, P2993, DOI 10.1109/TMM.2018.2829605
   Zhou YZ, 2018, PROC CVPR IEEE, P449, DOI 10.1109/CVPR.2018.00054
NR 84
TC 8
Z9 8
U1 1
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1515
EP 1528
DI 10.1109/TMM.2021.3066775
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0D8GN
UT WOS:000776227200021
DA 2024-07-18
ER

PT J
AU Li, ZL
   Tang, C
   Liu, XW
   Zheng, X
   Zhang, W
   Zhu, E
AF Li, Zhenglai
   Tang, Chang
   Liu, Xinwang
   Zheng, Xiao
   Zhang, Wei
   Zhu, En
TI Consensus Graph Learning for Multi-View Clustering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Tensors; Clustering methods; Optimization; Streaming media; Feature
   extraction; Task analysis; Correlation; Multi-view clustering; consensus
   graph learning; weighted tensor nuclear norm
ID REPRESENTATION
AB Multi-view clustering, which exploits the multi-view information to partition data into their clusters, has attracted intense attention. However, most existing methods directly learn a similarity graph from original multi-view features, which inevitably contain noises and redundancy information. The learned similarity graph is inaccurate and is insufficient to depict the underlying cluster structure of multi-view data. To address this issue, we propose a novel multi-view clustering method that is able to construct an essential similarity graph in a spectral embedding space instead of the original feature space. Concretely, we first obtain multiple spectral embedding matrices from the view-specific similarity graphs, and reorganize the gram matrices constructed by the inner product of the normalized spectral embedding matrices into a tensor. Then, we impose a weighted tensor nuclear norm constraint on the tensor to capture high-order consistent information among multiple views. Furthermore, we unify the spectral embedding and low rank tensor learning into a unified optimization framework to determine the spectral embedding matrices and tensor representation jointly. Finally, we obtain the consensus similarity graph from the gram matrices via an adaptive neighbor manner. An efficient optimization algorithm is designed to solve the resultant optimization problem. Extensive experiments on six benchmark datasets are conducted to verify the efficacy of the proposed method. The code is implemented by using MATLAB R2018a and MindSpore library [1]: https://github.com/guanyuezhen/CGL.
C1 [Li, Zhenglai; Tang, Chang] China Univ Geosci, Sch Comp, Wuhan 430074, Peoples R China.
   [Liu, Xinwang; Zheng, Xiao; Zhu, En] Natl Univ Def Technol, Sch Comp, Changsha 410073, Peoples R China.
   [Zhang, Wei] Qilu Univ Technol, Shandong Acad Sci, Shandong Prov Key Lab Comp Networks, Shandong Comp Sci Ctr,Natl Supercomp Ctr Jinan, Jinan 250000, Peoples R China.
C3 China University of Geosciences; National University of Defense
   Technology - China; Qilu University of Technology
RP Tang, C (corresponding author), China Univ Geosci, Sch Comp, Wuhan 430074, Peoples R China.
EM yuezhenguan@cug.edu.cn; tangchang@cug.edu.cn; xinwangliu@nudt.edu.cn;
   zhengxiao@nudt.edu.cn; wzhang@qlu.edu.cn; enzhu@nudt.edu.cn
RI LIU, Xinwang/L-8089-2019; Tang, Chang/AAU-8995-2020
OI LIU, Xinwang/0000-0001-9066-1475; Tang, Chang/0000-0002-6515-7696; Li,
   Zhenglai/0000-0002-9151-3631; Zhang, Wei/0000-0002-8947-9067
FU National Key R&D Program of China [2020AAA0107100]; National Science
   Foundation (NSF) of China [62076228, 61976196]; Natural Natural Science
   Foundation of Hubei Province [2020CFB644]; Opening Fund of Key
   Laboratory of Geological Survey, and Evaluation of Ministry of Education
   [GLAB2020ZR18]; Fundamental Research Funds for the Central Universities;
   CAAI-Huawei MindSpore Open Fund
FX This work was supported in part by National Key R&D Program of China
   under Grant 2020AAA0107100; in part by the National Science Foundation
   (NSF) of China under Grant 62076228, and 61976196, in part by Natural
   Natural Science Foundation of Hubei Province under Grant 2020CFB644, in
   part by the Opening Fund of Key Laboratory of Geological Survey, and
   Evaluation ofMinistry of Education under Grant GLAB2020ZR18, and in part
   by the Fundamental Research Funds for the CentralUniversities. This work
   was sponsored by CAAI-Huawei MindSpore Open Fund. The associate editor
   coordinating the reviewof this manuscript and approving it for
   publication was Prof. Mohammed Daoudi.
CR [Anonymous], 2020, MindSpore
   Brbic M, 2018, PATTERN RECOGN, V73, P247, DOI 10.1016/j.patcog.2017.08.024
   Cao XC, 2015, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2015.7298657
   Chen YY, 2020, PATTERN RECOGN, V106, DOI 10.1016/j.patcog.2020.107441
   Chen YY, 2020, IEEE T MULTIMEDIA, V22, P1985, DOI 10.1109/TMM.2019.2952984
   Cheng MM, 2019, IEEE T IMAGE PROCESS, V28, P2399, DOI 10.1109/TIP.2018.2877937
   Du L, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3476
   Elhamifar Ehsan, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2790, DOI 10.1109/CVPRW.2009.5206547
   Fu YW, 2010, IEEE T MULTIMEDIA, V12, P717, DOI 10.1109/TMM.2010.2052025
   Gao HC, 2015, IEEE I CONF COMP VIS, P4238, DOI 10.1109/ICCV.2015.482
   Gretton A., 2008, Advances in neural information processing systems, P585
   Gu SH, 2017, INT J COMPUT VISION, V121, P183, DOI 10.1007/s11263-016-0930-5
   Huang SD, 2020, INFORM SCIENCES, V512, P18, DOI 10.1016/j.ins.2019.09.079
   Ji YX, 2019, IEEE DATA MINING, P1132, DOI 10.1109/ICDM.2019.00136
   Kang Z, 2020, AAAI CONF ARTIF INTE, V34, P4412
   Kang Z, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2312
   Kang Z, 2020, NEURAL NETWORKS, V122, P279, DOI 10.1016/j.neunet.2019.10.010
   Kang Z, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2701
   Liang YW, 2019, IEEE DATA MINING, P1204, DOI 10.1109/ICDM.2019.00148
   Liu BY, 2021, IEEE T CYBERNETICS, V51, P1571, DOI 10.1109/TCYB.2019.2955388
   Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88
   Lu CY, 2020, IEEE T PATTERN ANAL, V42, P925, DOI 10.1109/TPAMI.2019.2891760
   Nene S. A., 1996, Tech. Rep. CUCS-005-96
   Ng AY, 2002, ADV NEUR IN, V14, P849
   Nie F., 2016, IJCAI, P1881
   Nie FP, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2564
   Nie FP, 2017, AAAI CONF ARTIF INTE, P2408
   Nie FP, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P977, DOI 10.1145/2623330.2623726
   Peng X, 2019, PR MACH LEARN RES, V97
   Sun X., 2020, 2020 IEEE INT C MULT, P1
   Tan Junpeng, 2020, IEEE T MULTIMEDIA
   Tang C, 2022, IEEE T KNOWL DATA EN, V34, P4705, DOI 10.1109/TKDE.2020.3048678
   Tang C, 2020, AAAI CONF ARTIF INTE, V34, P5924
   Tang C, 2019, IEEE T MULTIMEDIA, V21, P1724, DOI 10.1109/TMM.2018.2889560
   Tang JH, 2019, IEEE T PATTERN ANAL, V41, P2027, DOI 10.1109/TPAMI.2019.2906603
   Tang JH, 2018, IEEE T NEUR NET LEAR, V29, P6154, DOI 10.1109/TNNLS.2018.2816743
   Wang CD, 2016, IEEE T KNOWL DATA EN, V28, P1007, DOI 10.1109/TKDE.2015.2503743
   Wang H, 2020, IEEE T KNOWL DATA EN, V32, P1116, DOI 10.1109/TKDE.2019.2903810
   Wang HB, 2021, IEEE T MULTIMEDIA, V23, P3828, DOI 10.1109/TMM.2020.3032023
   Wang Q, 2021, IEEE T MULTIMEDIA, V23, P429, DOI 10.1109/TMM.2020.2978633
   Wang Y., 2016, P 25 INT JOINT C ART, P2153
   Wang Y, 2015, IEEE T IMAGE PROCESS, V24, P3939, DOI 10.1109/TIP.2015.2457339
   Wen J, 2021, IEEE T MULTIMEDIA, V23, P2493, DOI 10.1109/TMM.2020.3013408
   Wu JL, 2019, IEEE T IMAGE PROCESS, V28, P5910, DOI 10.1109/TIP.2019.2916740
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2149
   Xiaobo Wang, 2017, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P1, DOI 10.1109/CVPR.2017.8
   Xie Y, 2021, IEEE T NEUR NET LEAR, V32, P868, DOI 10.1109/TNNLS.2020.2979685
   Xie Y, 2018, INT J COMPUT VISION, V126, P1157, DOI 10.1007/s11263-018-1086-2
   Xu N, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1794, DOI 10.1145/3240508.3240679
   Yang SJ, 2019, IEEE T MULTIMEDIA, V21, P2916, DOI 10.1109/TMM.2019.2912735
   Yang ZY, 2019, IEEE T IMAGE PROCESS, V28, P5147, DOI 10.1109/TIP.2019.2913096
   Zhan K, 2019, IEEE T KNOWL DATA EN, V31, P1984, DOI 10.1109/TKDE.2018.2872061
   Zhan K, 2019, IEEE T IMAGE PROCESS, V28, P1261, DOI 10.1109/TIP.2018.2877335
   Zhang CQ, 2020, INT J COMPUT VISION, V128, P2344, DOI 10.1007/s11263-020-01307-0
   Zhang CQ, 2020, IEEE T PATTERN ANAL, V42, P86, DOI 10.1109/TPAMI.2018.2877660
   Zhang CQ, 2015, IEEE I CONF COMP VIS, P1582, DOI 10.1109/ICCV.2015.185
   Zhu XF, 2019, IEEE T KNOWL DATA EN, V31, P2022, DOI 10.1109/TKDE.2018.2873378
NR 57
TC 121
Z9 123
U1 17
U2 38
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2461
EP 2472
DI 10.1109/TMM.2021.3081930
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 1D5KQ
UT WOS:000793839600018
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Lin, L
   Yan, PX
   Xu, XQ
   Yang, SB
   Zeng, K
   Li, GB
AF Lin, Liang
   Yan, Pengxiang
   Xu, Xiaoqian
   Yang, Sibei
   Zeng, Kun
   Li, Guanbin
TI Structured Attention Network for Referring Image Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Linguistics; Image segmentation; Cognition; Feature
   extraction; Semantics; Task analysis; Referring image segmentation;
   vision and language; cross-modal reasoning
AB Referring image segmentation aims at segmenting out the object or stuff referred to by a natural language expression. The challenge of this task lies in the requirement of understanding both vision and language. The linguistic structure of a referring expression can provide an intuitive and explainable layout for reasoning over visual and linguistic concepts. In this paper, we propose a structured attention network (SANet) to explore the multimodal reasoning over the dependency tree parsed from the referring expression. Specifically, SANet implements the multimodal reasoning using an attentional multimodal tree-structure recurrent module (AMTreeGRU) in a bottom-up manner. In addition, for spatial detail improvement, SANet further incorporates the semantics-guided low-level features into high-level ones using the proposed attentional skip connection module. Extensive experiments on four public benchmark datasets demonstrate the superiority of our proposed SANet with more explainable visualization examples.
C1 [Lin, Liang; Yan, Pengxiang; Xu, Xiaoqian; Zeng, Kun; Li, Guanbin] Sun Yat Sen Univ, Sch Engn & Comp Sci, Guangzhou 510006, Peoples R China.
   [Yang, Sibei] Hong Kong Polytech Univ, Dept Comp, Hong Kong, Peoples R China.
C3 Sun Yat Sen University; Hong Kong Polytechnic University
RP Li, GB (corresponding author), Sun Yat Sen Univ, Sch Engn & Comp Sci, Guangzhou 510006, Peoples R China.
EM linliang@ieee.org; yanpx@mail2.sysu.edu.cn; xuxq7@mail2.sysu.edu.cn;
   sibeiyang@comp.polyu.edu.hk; zengkun2@mail.sysu.edu.cn;
   liguanbin@mail.sysu.edu.cn
RI l, j/HNC-5728-2023; L, J/JEF-9564-2023; Li, Jiaxi/HTS-3430-2023; l,
   j/JVZ-8480-2024; Lin, L/HKO-8213-2023; zhang, cl/JDW-6549-2023; Li,
   Li/IAQ-0885-2023; LU, LU/JEZ-4760-2023; Lin, Liang/IQR-8601-2023
OI Li, Jiaxi/0000-0002-8197-8590; Lin, Liang/0000-0003-2248-3755; Yan,
   Pengxiang/0000-0002-3075-2903
FU National Key Research and Development Program of China [2018YFC0830103];
   National Natural Science Foundation of China [61976250, U1811463];
   Guangdong Basic and Applied Basic Research Foundation [2020B1515020048];
   Natural Science Foundations of Guangdong [2017A03031335]; Guangzhou
   Science and technology project [202102020633]; National High Level
   Talents Special Support Plan (Ten Thousand Talents Program); CCF-Tencent
   Open Research Fund
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2018YFC0830103, in part by the
   National Natural Science Foundation of China under Grants 61976250 and
   U1811463, in part by the Guangdong Basic and Applied Basic Research
   Foundation under Grant 2020B1515020048, in part by the Natural Science
   Foundations of Guangdong under Grant 2017A03031335, in part by the
   Guangzhou Science and technology project under Grant 202102020633, and
   in part by National High Level Talents Special Support Plan (Ten
   Thousand Talents Program). This work was also sponsored by CCF-Tencent
   Open Research Fund.
CR [Anonymous], 2011, ADV NEURAL INF PROCE
   [Anonymous], 2010, INT J COMPUT VISION, DOI [DOI 10.1007/s11263-009-0275-4, 10.1007/s11263-009-0275-4]
   [Anonymous], 2018, IEEE T MULTIMEDIA, DOI DOI 10.1109/TMM.2018.2811621
   Ben-younes H, 2017, IEEE I CONF COMP VIS, P2631, DOI 10.1109/ICCV.2017.285
   Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS)
   Cao QX, 2018, PROC CVPR IEEE, P7249, DOI 10.1109/CVPR.2018.00757
   Chen DD, 2014, J ANAL METHODS CHEM, V2014, DOI 10.1155/2014/575246
   Chen DJ, 2019, IEEE I CONF COMP VIS, P7453, DOI 10.1109/ICCV.2019.00755
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Cho K, 2015, IEEE T MULTIMEDIA, V17, P1875, DOI 10.1109/TMM.2015.2477044
   Cirik V, 2018, AAAI CONF ARTIF INTE, P6756
   Dong JF, 2018, IEEE T MULTIMEDIA, V20, P3377, DOI 10.1109/TMM.2018.2832602
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He YH, 2016, IEEE T MULTIMEDIA, V18, P1363, DOI 10.1109/TMM.2016.2558463
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hong RC, 2022, IEEE T PATTERN ANAL, V44, P684, DOI 10.1109/TPAMI.2019.2911066
   Hu RH, 2017, PROC CVPR IEEE, P4418, DOI 10.1109/CVPR.2017.470
   Hu RH, 2016, PROC CVPR IEEE, P4555, DOI 10.1109/CVPR.2016.493
   Hu RH, 2016, LECT NOTES COMPUT SC, V9905, P108, DOI 10.1007/978-3-319-46448-0_7
   Huang S., 2020, 2020 IEEE CVF C COMP, P10485, DOI [DOI 10.1109/CVPR42600.2020.01050, 10.1109/CVPR42600.2020.01050]
   Hui T., 2020, COMPUTER VISION ECCV, P59
   Escalante HJ, 2010, COMPUT VIS IMAGE UND, V114, P419, DOI 10.1016/j.cviu.2009.03.008
   Kazemzadeh S., 2014, EMNLP, P787, DOI DOI 10.3115/V1/D14-1086
   Ke GL, 2017, ADV NEUR IN, V30
   Kingma D. P., 2015, PROC INT C LEARN REP, P1
   Li RY, 2018, PROC CVPR IEEE, P5745, DOI 10.1109/CVPR.2018.00602
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu CX, 2017, IEEE I CONF COMP VIS, P1280, DOI 10.1109/ICCV.2017.143
   Liu DQ, 2019, IEEE I CONF COMP VIS, P4672, DOI 10.1109/ICCV.2019.00477
   Luo G, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1274, DOI 10.1145/3394171.3414006
   Luo Gen, 2020, CVPR, P10034
   Mao JH, 2016, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2016.9
   Margffoy-Tuay E, 2018, LECT NOTES COMPUT SC, V11215, P656, DOI 10.1007/978-3-030-01252-6_39
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683
   Shi HC, 2018, LECT NOTES COMPUT SC, V11210, P38, DOI 10.1007/978-3-030-01231-1_3
   Shi XJ, 2015, ADV NEUR IN, V28
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tai KS, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P1556
   Yang SB, 2021, IEEE T PATTERN ANAL, V43, P2765, DOI 10.1109/TPAMI.2020.2973983
   Yang SB, 2019, PROC CVPR IEEE, P4140, DOI 10.1109/CVPR.2019.00427
   Ye LW, 2019, PROC CVPR IEEE, P10494, DOI 10.1109/CVPR.2019.01075
   Yu LC, 2018, PROC CVPR IEEE, P1307, DOI 10.1109/CVPR.2018.00142
   Yu LC, 2016, LECT NOTES COMPUT SC, V9906, P69, DOI 10.1007/978-3-319-46475-6_5
NR 54
TC 17
Z9 17
U1 2
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1922
EP 1932
DI 10.1109/TMM.2021.3074008
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0H8DA
UT WOS:000778959200011
DA 2024-07-18
ER

PT J
AU Ng, W
   Zhang, MY
   Wang, T
AF Ng, Wing
   Zhang, Mingyang
   Wang, Ting
TI Multi-Localized Sensitive Autoencoder-Attention-LSTM For Skeleton-based
   Action Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Skeleton; Feature extraction; Joints; Hidden Markov models; Convolution;
   Task analysis; Bones; Skeleton-based action recognition (SAR); Localized
   Stochastic Sensitive Autoencoder (LiSSA)
ID REPRESENTATION; NETWORKS; CAMERAS
AB One of key challenges of skeleton-based action recognition (SAR) tasks is the complex nature of human motion patterns. Variations such as performers and viewpoints may impose negative effects to the action recognition accuracy. In this work, we propose the Multi-Localized Sensitive Autoencoder-Attention-LSTM (Multi-LiSAAL) for SAR. The Localized Stochastic Sensitive Autoencoder (LiSSA) encodes both spatial and temporal information, and extracts meaningful features from different parts (four limbs and a trunk) from the skeleton. The LiSSA is trained by minimizing the localized generalization error to enhance the robustness of autoencoders via reducing its sensitivity with respect to small variations in inputs. We apply an attention mechanism to assign different weights to different skeleton parts and focus more on informative sections. Then, a backbone classifier network takes weighted features as inputs to differentiates actions. Experimental results on five public benchmarking datasets show that the Multi-LiSAAL outperforms state-of-the-art methods.
C1 [Ng, Wing; Zhang, Mingyang; Wang, Ting] South China Univ Technol, Sch Comp Sci & Engn, Guangdong Prov Key Lab Computat Intelligence & Cy, Guangzhou 510006, Peoples R China.
C3 South China University of Technology
RP Wang, T (corresponding author), South China Univ Technol, Sch Comp Sci & Engn, Guangdong Prov Key Lab Computat Intelligence & Cy, Guangzhou 510006, Peoples R China.
EM wingng@ieee.org; cszhangmingyang@mail.scut.edu.cn; tingwang@ieee.org
RI Wang, Ting/KPB-6170-2024
OI Wang, Ting/0000-0001-5967-5940; Ng, Wing W. Y./0000-0003-0783-3585
FU National Natural Science Foundation of China [61876066]; Guangdong
   Province Science and Technology Plan Project (Collaborative Innovation
   and Platform Environment Construction) [2019A050510006]; 2020 R&D
   Program in Key Areas of Guangdong Province [2020B010166002]
FX This work was supported in part by the National Natural Science
   Foundation ofChina underGrant 61876066, in part by the Guangdong
   Province Science and Technology Plan Project (Collaborative Innovation
   and Platform Environment Construction) under Grant 2019A050510006, and
   in part by the 2020 R&D Program in Key Areas of Guangdong Province under
   Grant 2020B010166002.
CR Cai MJ, 2019, IEEE T CYBERNETICS, V49, P1616, DOI 10.1109/TCYB.2018.2806381
   Cao CQ, 2019, IEEE T CIRC SYST VID, V29, P3247, DOI 10.1109/TCSVT.2018.2879913
   Cao CQ, 2018, IEEE T CYBERNETICS, V48, P1095, DOI 10.1109/TCYB.2017.2756840
   Chen C, 2015, IEEE IMAGE PROC, P168, DOI 10.1109/ICIP.2015.7350781
   Chen MJ, 2019, IEEE INTERNET THINGS, V6, P1410, DOI 10.1109/JIOT.2018.2856241
   Chen X, 2018, IEEE T NEUR NET LEAR, V29, P3938, DOI 10.1109/TNNLS.2017.2740318
   Chen Y, 2019, IEEE T COMPUT SOC SY, V6, P1115, DOI 10.1109/TCSS.2019.2934639
   Cheng K, 2020, PROC CVPR IEEE, P180, DOI 10.1109/CVPR42600.2020.00026
   Han JH, 2016, IEEE T HUM-MACH SYST, V46, P708, DOI 10.1109/THMS.2016.2558539
   Hou YH, 2018, IEEE T CIRC SYST VID, V28, P807, DOI 10.1109/TCSVT.2016.2628339
   Huang LJ, 2020, AAAI CONF ARTIF INTE, V34, P11045
   Huynh-The T, 2020, IEEE T IND INFORM, V16, P3100, DOI 10.1109/TII.2019.2910876
   Iosifidis A, 2017, IEEE T CYBERNETICS, V47, P4485, DOI 10.1109/TCYB.2016.2612479
   Kamel A, 2019, IEEE T SYST MAN CY-S, V49, P1806, DOI 10.1109/TSMC.2018.2850149
   Ke Q, 2017, PROC CVPR IEEE, P4570, DOI 10.1109/CVPR.2017.486
   Li C, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P786
   Li CL, 2018, IEEE T IMAGE PROCESS, V27, P3657, DOI 10.1109/TIP.2018.2815744
   Li CK, 2019, IEEE T HUM-MACH SYST, V49, P95, DOI 10.1109/THMS.2018.2883001
   Li MS, 2019, PROC CVPR IEEE, P3590, DOI 10.1109/CVPR.2019.00371
   Liu J, 2018, IEEE T PATTERN ANAL, V40, P3007, DOI 10.1109/TPAMI.2017.2771306
   Liu ZY, 2020, PROC CVPR IEEE, P140, DOI 10.1109/CVPR42600.2020.00022
   Ma QL, 2021, IEEE T CYBERNETICS, V51, P1613, DOI 10.1109/TCYB.2019.2919648
   Ma QL, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2457
   Phyo CN, 2019, IEEE T CONSUM ELECTR, V65, P243, DOI 10.1109/TCE.2019.2908986
   Seidenari L, 2013, IEEE COMPUT SOC CONF, P479, DOI 10.1109/CVPRW.2013.77
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Shahroudy A, 2016, IEEE T PATTERN ANAL, V38, P2123, DOI 10.1109/TPAMI.2015.2505295
   Shao ZF, 2018, IEEE T BIG DATA, V4, P105, DOI 10.1109/TBDATA.2017.2715815
   Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316
   Song SJ, 2018, IEEE T IMAGE PROCESS, V27, P3459, DOI 10.1109/TIP.2018.2818328
   Talha SAW, 2018, IEEE T COGN DEV SYST, V10, P894, DOI 10.1109/TCDS.2018.2844279
   Wang HS, 2018, IEEE T IMAGE PROCESS, V27, P4382, DOI 10.1109/TIP.2018.2837386
   Wang L, 2020, IEEE T IMAGE PROCESS, V29, P15, DOI 10.1109/TIP.2019.2925285
   Wang PC, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P97, DOI 10.1145/2964284.2967191
   Wang T, 2021, IEEE T CYBERNETICS, V51, P2748, DOI 10.1109/TCYB.2019.2923756
   Wei P, 2019, IEEE T MULTIMEDIA, V21, P2195, DOI 10.1109/TMM.2019.2897902
   Wei WC, 2019, IEEE T NEUR SYS REH, V27, P1824, DOI 10.1109/TNSRE.2019.2934097
   Wu HM, 2018, IEEE T HUM-MACH SYST, V48, P304, DOI 10.1109/THMS.2017.2776211
   Xie CY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1639
   Xikun Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14321, DOI 10.1109/CVPR42600.2020.01434
   Yang YH, 2017, IEEE T MULTIMEDIA, V19, P519, DOI 10.1109/TMM.2016.2626959
   Yang YH, 2017, IEEE T CYBERNETICS, V47, P439, DOI 10.1109/TCYB.2016.2519448
   Yang ZY, 2019, IEEE T CIRC SYST VID, V29, P2405, DOI 10.1109/TCSVT.2018.2864148
   Yao YQ, 2018, IEEE T BIG DATA, V4, P530, DOI 10.1109/TBDATA.2018.2803838
   Yun K., 2012, 2012 IEEE COMP SOC C, P28, DOI DOI 10.1109/CVPRW.2012.6239234
   Zhang JG, 2017, IEEE T CYBERNETICS, V47, P960, DOI 10.1109/TCYB.2016.2535122
   Zhang PF, 2019, IEEE T PATTERN ANAL, V41, P1963, DOI 10.1109/TPAMI.2019.2896631
   Zhang SY, 2018, IEEE T MULTIMEDIA, V20, P2330, DOI 10.1109/TMM.2018.2802648
   Zhang YP, 2012, IEEE VTS VEH TECHNOL
   Zhu WT, 2016, AAAI CONF ARTIF INTE, P3697
NR 50
TC 19
Z9 20
U1 4
U2 37
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1678
EP 1690
DI 10.1109/TMM.2021.3070127
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0D8GN
UT WOS:000776227200033
DA 2024-07-18
ER

PT J
AU Tang, HY
   Zhu, JH
   Liu, M
   Gao, Z
   Cheng, ZY
AF Tang, Haoyu
   Zhu, Jihua
   Liu, Meng
   Gao, Zan
   Cheng, Zhiyong
TI Frame-Wise Cross-Modal Matching for Video Moment Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Predictive models; Location awareness; Feature extraction; Task
   analysis; Proposals; Semantics; Streaming media; Cross-modal retrieval;
   frame-wise matching; moment localization; video moment retrieval
ID LOCALIZATION; TEXT
AB Video moment retrieval targets at retrieving a golden moment in a video for a given natural language query. The main challenges of this task include 1) the requirement of accurately localizing (i.e., the start time and the end time of) the relevant moment in an untrimmed video stream, and 2) bridging the semantic gap between textual query and video contents. To tackle those problems, early approaches adopt the sliding window or uniform sampling to collect video clips first and then match each clip with the query to identify relevant clips. Obviously, these strategies are time-consuming and often lead to unsatisfied accuracy in localization due to the unpredictable length of the golden moment. To avoid the limitations, researchers recently attempt to directly predict the relevant moment boundaries without the requirement to generate video clips first. One mainstream approach is to generate a multimodal feature vector for the target query and video frames (e.g., concatenation) and then use a regression approach upon the multimodal feature vector for boundary detection. Although some progress has been achieved by this approach, we argue that those methods have not well captured the cross-modal interactions between the query and video frames. In this paper, we propose an Attentive Cross-modal Relevance Matching (ACRM) model which predicts the temporal boundaries based on an interaction modeling between two modalities. In addition, an attention module is introduced to automatically assign higher weights to query words with richer semantic cues, which are considered to be more important for finding relevant video contents. Another contribution is that we propose an additional predictor to utilize the internal frames in the model training to improve the localization accuracy. Extensive experiments on two public datasets TACoS and Charades-STA demonstrate the superiority of our method over several state-of-the-art methods. Ablation studies have been also conducted to examine the effectiveness of different modules in our ACRM model.
C1 [Tang, Haoyu; Zhu, Jihua] Xi An Jiao Tong Univ, Sch Software Engn, Xian 710049, Peoples R China.
   [Tang, Haoyu; Gao, Zan; Cheng, Zhiyong] Qilu Univ Technol, Shandong Artificial Intelligence Inst, Shandong Acad Sci, Jinan 250014, Peoples R China.
   [Liu, Meng] Shandong Jianzhu Univ, Sch Comp Sci & Technol, Jinan 250101, Peoples R China.
C3 Xi'an Jiaotong University; Qilu University of Technology; Shandong
   Jianzhu University
RP Cheng, ZY (corresponding author), Qilu Univ Technol, Shandong Artificial Intelligence Inst, Shandong Acad Sci, Jinan 250014, Peoples R China.
EM tanghao258@stu.xjtu.edu.cn; zhujh@xjtu.edu.cn; mengliu.sdu@gmail.com;
   zangaonsh4522@gmail.com; jason.zy.cheng@gmail.com
RI Tang, Haoyu/ABE-6093-2021
OI Tang, Haoyu/0000-0002-6488-3565; Zhu, Jihua/0000-0002-3081-8781; Liu,
   Meng/0000-0002-1582-5764; , zan/0000-0003-2182-5741
FU National Natural Science Foundation of China [61902223, 62006142];
   Innovation Teams in Colleges and Universities in Jinan [2018GXRC014];
   Young CreativeTeam in Universities of Shandong Province [2020KJN012]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61902223 and 62006142, in part by the
   Innovation Teams in Colleges and Universities in Jinan under Grant
   2018GXRC014, and in part by Young CreativeTeam in Universities of
   Shandong Province under Grant 2020KJN012. The Associate Editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Ting Yao.
CR [Anonymous], 2016, P C N AM CHAPT ASS C, DOI DOI 10.18653/V1/N16-1170
   [Anonymous], 2019, P 2019 INT C MULT RE, DOI DOI 10.1145/3323873.3325019
   Bojanowski P, 2015, IEEE I CONF COMP VIS, P4462, DOI 10.1109/ICCV.2015.507
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen JY, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P162
   Chen L, 2020, AAAI CONF ARTIF INTE, V34, P10551
   Chen SX, 2019, AAAI CONF ARTIF INTE, P8199
   Dong JF, 2019, PROC CVPR IEEE, P9338, DOI 10.1109/CVPR.2019.00957
   Dong JF, 2018, IEEE T MULTIMEDIA, V20, P3377, DOI 10.1109/TMM.2018.2832602
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Feng FL, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P455, DOI 10.1145/3077136.3080773
   Feng FL, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P1523, DOI 10.1145/3178876.3186064
   Gao JY, 2017, IEEE I CONF COMP VIS, P5277, DOI 10.1109/ICCV.2017.563
   Gao JY, 2017, IEEE I CONF COMP VIS, P3648, DOI 10.1109/ICCV.2017.392
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Ge RZ, 2019, IEEE WINT CONF APPL, P245, DOI 10.1109/WACV.2019.00032
   Ghosh S, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P1984
   Hahn Meera, 2019, ARXIV190409936
   Hendricks LA, 2017, IEEE I CONF COMP VIS, P5804, DOI 10.1109/ICCV.2017.618
   Hu RH, 2016, PROC CVPR IEEE, P4555, DOI 10.1109/CVPR.2016.493
   Kiros R, 2015, 29 ANN C NEURAL INFO, V28
   Lin DH, 2014, PROC CVPR IEEE, P2657, DOI 10.1109/CVPR.2014.340
   Lin TW, 2018, LECT NOTES COMPUT SC, V11208, P3, DOI 10.1007/978-3-030-01225-0_1
   Liu M, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P843, DOI 10.1145/3240508.3240549
   Liu M, 2018, ACM/SIGIR PROCEEDINGS 2018, P15, DOI 10.1145/3209978.3210003
   Liu W., 2019, Neural Inf. Process. Syst., P536
   Ma SG, 2016, PROC CVPR IEEE, P1942, DOI 10.1109/CVPR.2016.214
   Manning CD, 2014, PROCEEDINGS OF 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: SYSTEM DEMONSTRATIONS, P55, DOI 10.3115/v1/p14-5010
   Mithun NC, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P19, DOI 10.1145/3206025.3206064
   Paul S, 2018, LECT NOTES COMPUT SC, V11208, P588, DOI 10.1007/978-3-030-01225-0_35
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Rodriguez-Opazo C, 2020, IEEE WINT CONF APPL, P2453, DOI [10.1109/WACV45572.2020.9093328, 10.1109/wacv45572.2020.9093328]
   Rohrbach M, 2012, LECT NOTES COMPUT SC, V7572, P144, DOI 10.1007/978-3-642-33718-5_11
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Shou Z, 2018, LECT NOTES COMPUT SC, V11220, P162, DOI 10.1007/978-3-030-01270-0_10
   Sigurdsson GA, 2016, LECT NOTES COMPUT SC, V9905, P510, DOI 10.1007/978-3-319-46448-0_31
   Wang JW, 2020, AAAI CONF ARTIF INTE, V34, P12168
   Wang LM, 2017, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2017.678
   Wang WN, 2019, PROC CVPR IEEE, P334, DOI 10.1109/CVPR.2019.00042
   Wu Jie, 2020, ARXIV200106680
   Xu HJ, 2019, AAAI CONF ARTIF INTE, P9062
   Xu HJ, 2017, IEEE I CONF COMP VIS, P5794, DOI 10.1109/ICCV.2017.617
   Xu Huijuan., 2018, Text-to-clip video retrieval with early fusion and re-captioning
   Xu N, 2020, IEEE T MULTIMEDIA, V22, P1372, DOI 10.1109/TMM.2019.2941820
   Yan CG, 2020, IEEE T MULTIMEDIA, V22, P229, DOI 10.1109/TMM.2019.2924576
   Yuan YT, 2019, AAAI CONF ARTIF INTE, P9159
   Zeng R., 2020, CVPR
   Zhang D, 2019, PROC CVPR IEEE, P1247, DOI 10.1109/CVPR.2019.00134
   Zhang H., P 58 ANN M ASS COMP, P6543
   Zhang S., 2019, ARXIV COMPUTER VISIO
   Zhu WW, 2020, IEEE T MULTIMEDIA, V22, P1823, DOI 10.1109/TMM.2020.2969791
NR 51
TC 21
Z9 21
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1338
EP 1349
DI 10.1109/TMM.2021.3063631
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0D8GN
UT WOS:000776227200008
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Xie, P
   Zhao, MY
   Hu, XH
AF Xie, Pan
   Zhao, Mengyi
   Hu, Xiaohui
TI PiSLTRc: Position-Informed Sign Language Transformer With Content-Aware
   Convolution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Sign language recognition; sign language translation; content-aware
   neighborhood gathering; position-informed convolution; relative position
   encoding
ID RECOGNITION; FRAMEWORK; NETWORK
AB Since the superiority of Transformer in learning long-term dependency, the sign language Transformer model achieves remarkable progress in Sign Language Recognition (SLR) and Translation (SLT). However, there are several issues with the Transformer that prevent it from better sign language understanding. The first issue is that the self-attention mechanism learns sign video representation in a frame-wise manner, neglecting the temporal semantic structure of sign gestures. Secondly, the attention mechanism with absolute position encoding is direction and distance unaware, thus limiting its ability. To address these issues, we propose a new model architecture, namely PiSLTRc, with two distinctive characteristics: (i) content-aware and position-aware convolution layers. Specifically, we explicitly select relevant features using a novel content-aware neighborhood gathering method. Then we aggregate these features with position-informed temporal convolution layers, thus generating robust neighborhood-enhanced sign representation. (ii) injecting the relative position information to the attention mechanism in the encoder, decoder, and even encoder-decoder cross attention. Compared with the vanilla Transformer model, our model performs consistently better on three large-scale sign language benchmarks: PHOENIX-2014, PHOENIX-2014-T and CSL. Furthermore, extensive experiments demonstrate that the proposed method achieves state-of-the-art performance on translation quality with +1.6 BLEU improvements.
C1 [Xie, Pan; Zhao, Mengyi] Beihang Univ, Sch Automat Sci & Elect Engn, Beijing 100191, Peoples R China.
   [Hu, Xiaohui] Chinese Acad Sci, Inst Software, Sci & Technol Integrated Informat Syst Lab, Beijing 100191, Peoples R China.
C3 Beihang University; Chinese Academy of Sciences; Institute of Software,
   CAS
RP Xie, P (corresponding author), Beihang Univ, Sch Automat Sci & Elect Engn, Beijing 100191, Peoples R China.
EM panxie@buaa.edu.cn; zhaomengyi@buaa.edu.cn; hxh@iscas.ac.cn
FU key R&D project of the Ministry of Science and Technology of the
   People's Republic of China [2019YFB1405103]
FX This work was supported by the key R&D project of the Ministry of
   Science and Technology of the People's Republic of China, with award
   number: 2019YFB1405103.
CR [Anonymous], 2015, P 2015 C EMP METH NA, DOI [10.18653/v1/D15-1166, DOI 10.48550/ARXIV.1508.04025]
   Ba J. L., 2016, LAYER NORMALIZATION, DOI DOI 10.48550/ARXIV.1607.06450
   Camgoz Necati Cihan, 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12538), P301, DOI 10.1007/978-3-030-66823-5_18
   Camgoz Necati Cihan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10020, DOI 10.1109/CVPR42600.2020.01004
   Camgoz NC, 2017, IEEE I CONF COMP VIS, P3075, DOI 10.1109/ICCV.2017.332
   Camgoz NC, 2018, PROC CVPR IEEE, P7784, DOI 10.1109/CVPR.2018.00812
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Cui RP, 2019, IEEE T MULTIMEDIA, V21, P1880, DOI 10.1109/TMM.2018.2889563
   Cui RP, 2017, PROC CVPR IEEE, P1610, DOI 10.1109/CVPR.2017.175
   Dai ZH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P2978
   Devlin J., 2018, BERT PRE TRAINING DE
   Forster J, 2014, LREC 2014 - NINTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION, P1911
   Graves A., 2006, P 23 INT C MACHINE L, P369
   Guo D, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P744
   Guo D, 2018, AAAI CONF ARTIF INTE, P6845
   Guo DS, 2018, IEEE T MULTIMEDIA, V20, P3428, DOI 10.1109/TMM.2018.2839534
   He P., 2020, INT C LEARNING REPRE
   Huang C.-Z. A., 2019, ICLR
   Huang J, 2019, IEEE T CIRC SYST VID, V29, P2822, DOI 10.1109/TCSVT.2018.2870740
   Huang J, 2018, AAAI CONF ARTIF INTE, P2257
   Islam MA, 2020, PROC INT C LEARN REP
   Joze Hamid Vaezi, 2019, BMVC
   Ka Leong Cheng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P697, DOI 10.1007/978-3-030-58586-0_41
   Ke G., 2021, PROC INT C LEARN REP
   Koller O, 2020, IEEE T PATTERN ANAL, V42, P2306, DOI 10.1109/TPAMI.2019.2911077
   Koller O, 2017, PROC CVPR IEEE, P3416, DOI 10.1109/CVPR.2017.364
   Koller O, 2016, PROC CVPR IEEE, P3793, DOI 10.1109/CVPR.2016.412
   Koller Oscar., 2016, P BMVC, P1, DOI 10.5244/C.30.136
   Lei Ba J., 2016, arXiv
   Li D., 2020, ARXIV 201005468, p12 034
   Li DX, 2020, IEEE WINT CONF APPL, P1448, DOI [10.1109/WACV45572.2020.9093512, 10.1109/wacv45572.2020.9093512]
   Martinez B, 2020, INT CONF ACOUST SPEE, P6319, DOI [10.1109/ICASSP40776.2020.9053841, 10.1109/icassp40776.2020.9053841]
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Pfau R, 2018, GLOSSA-UK, V3, DOI 10.5334/gjgl.511
   Pu JF, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1497, DOI 10.1145/3394171.3413931
   Pu JF, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P885
   Pu JF, 2019, PROC CVPR IEEE, P4160, DOI 10.1109/CVPR.2019.00429
   Raffel C, 2020, J MACH LEARN RES, V21
   Shaw P., 2018, P 2018 NAACL, V2, P464, DOI [DOI 10.18653/V1/N18-2074, 10.18653/v1/N18-2074]
   Vaswani A, 2017, ADV NEUR IN, V30
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Wang HJ, 2019, IEEE T MULTIMEDIA, V21, P2806, DOI 10.1109/TMM.2019.2915032
   Wang S, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1483, DOI 10.1145/3240508.3240671
   Wei CC, 2021, IEEE T CIRC SYST VID, V31, P1138, DOI 10.1109/TCSVT.2020.2999384
   Wu YH, 2016, Arxiv, DOI [arXiv:1609.08144, DOI 10.48550/ARXIV.1609.08144]
   Xia ZQ, 2020, IEEE T MULTIMEDIA, V22, P626, DOI 10.1109/TMM.2019.2931351
   Yan H, 2019, Arxiv, DOI [arXiv:1911.04474, DOI 10.48550/ARXIV.1911.04474, 10.48550/arXiv.1911.04474]
   Yang ZY, 2019, Arxiv, DOI arXiv:1908.01341
   Yang ZL, 2019, ADV NEUR IN, V32
   Zhang SQ, 2018, IEEE T MULTIMEDIA, V20, P1576, DOI 10.1109/TMM.2017.2766843
   Zhou H, 2022, IEEE T MULTIMEDIA, V24, P768, DOI 10.1109/TMM.2021.3059098
   Zhou H, 2020, AAAI CONF ARTIF INTE, V34, P13009
NR 52
TC 12
Z9 13
U1 0
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 3908
EP 3919
DI 10.1109/TMM.2021.3109665
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 3R1UG
UT WOS:000838704400017
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yi, QS
   Li, JC
   Fang, FM
   Jiang, AW
   Zhang, GX
AF Yi, Qiaosi
   Li, Juncheng
   Fang, Faming
   Jiang, Aiwen
   Zhang, Guixu
TI Efficient and Accurate Multi-Scale Topological Network for Single Image
   Dehazing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Atmospheric modeling; Feature extraction; Task analysis; Scattering;
   Adaptation models; Semantics; Image color analysis; Adaptive feature
   selection; feature fusion; image dehazing; multi-scale topological
   network
ID QUALITY ASSESSMENT
AB Single image dehazing is a challenging ill-posed problem that has drawn significant attention in the last few years. Recently, convolutional neural networks have achieved great success in image dehazing. However, it is still difficult for these increasingly complex models to recover accurate details from the hazy image. In this paper, we pay attention to the feature extraction and utilization of the input image itself. To achieve this, we propose a Multi-scale Topological Network (MSTN) to fully explore the features at different scales. Meanwhile, we design a Multi-scale Feature Fusion Module (MFFM) and an Adaptive Feature Selection Module (AFSM) to achieve the selection and fusion of features at different scales, so as to achieve progressive image dehazing. This topological network provides a large number of search paths that enable the network to extract abundant image features as well as strong fault tolerance and robustness. In addition, ASFM and MFFM can adaptively select important features and ignore interference information when fusing different scale representations. Extensive experiments are conducted to demonstrate the superiority of our method compared with state-of-the-art methods.
C1 [Yi, Qiaosi; Li, Juncheng; Fang, Faming; Zhang, Guixu] East China Normal Univ, Sch Comp Sci & Technol, Shanghai 200062, Peoples R China.
   [Yi, Qiaosi; Li, Juncheng; Fang, Faming; Zhang, Guixu] East China Normal Univ, Key Lab Adv Theory & Applicat Stat & Data Sci MOE, Shanghai 200062, Peoples R China.
   [Jiang, Aiwen] Jiangxi Normal Univ, Sch Comp & Informat Engn, Nanchang 330022, Jiangxi, Peoples R China.
C3 East China Normal University; East China Normal University; Jiangxi
   Normal University
RP Fang, FM (corresponding author), East China Normal Univ, Sch Comp Sci & Technol, Shanghai 200062, Peoples R China.; Fang, FM (corresponding author), East China Normal Univ, Key Lab Adv Theory & Applicat Stat & Data Sci MOE, Shanghai 200062, Peoples R China.
EM qiaosiyijoyies@gmail.com; cvjunchengli@gmail.com; fmfang@cs.ecnu.edu.cn;
   jiangaiwen@jxnu.edu.cn; gxzhang@cs.ecnu.edu.cn
RI Li, Juncheng/AHA-3971-2022
OI Jiang, Aiwen/0000-0002-5979-7590; Li, Juncheng/0000-0001-7314-6754
FU Key Project of the National Natural Science Foundation of China
   [61731009, 61966018]; NSFC-RGC [61961160734]; National Natural Science
   Foundation of China [61966018]; Shanghai Rising-Star Program
   [21QA1402500]; Science Foundation of Shanghai [20ZR1416200]; Open
   Research Fund of KLATASDS-MOE, ECNU
FX This work was supported in part by the Key Project of the National
   Natural Science Foundation of China under Grant 61731009, in part by the
   NSFC-RGC under Grant 61961160734, in part by the National Natural
   Science Foundation of China under Grant 61871185, in part by the
   Shanghai Rising-Star Program under Grant 21QA1402500, in part by the
   Science Foundation of Shanghai under Grant 20ZR1416200, and in part by
   the Open Research Fund of KLATASDS-MOE, ECNU. The work of Aiwen Jiangwas
   supported by the National Natural Science Foundation of China under
   Grant 61966018.
CR Ancuti CO, 2020, IEEE COMPUT SOC CONF, P1798, DOI 10.1109/CVPRW50498.2020.00230
   Ancuti CO, 2020, IEEE COMPUT SOC CONF, P2029, DOI 10.1109/CVPRW50498.2020.00253
   Attar N, 2017, CHAOS, V27, DOI 10.1063/1.4997921
   Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Dong H, 2020, PROC CVPR IEEE, P2154, DOI 10.1109/CVPR42600.2020.00223
   Dong Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P323, DOI 10.1007/978-3-030-58604-1_20
   Duta I. Cosmin, 2020, arXiv preprint arXiv:200611538
   Fattal R, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2651362
   Fattal R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360671
   Fu XY, 2017, PROC CVPR IEEE, P1715, DOI 10.1109/CVPR.2017.186
   Fu XY, 2017, IEEE T IMAGE PROCESS, V26, P2944, DOI 10.1109/TIP.2017.2691802
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2009, PROC CVPR IEEE, P1956, DOI [10.1109/CVPRW.2009.5206515, 10.1109/CVPR.2009.5206515]
   He T, 2019, PROC CVPR IEEE, P558, DOI 10.1109/CVPR.2019.00065
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jiangxin Dong, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P188, DOI 10.1007/978-3-030-58577-8_12
   Kui Jiang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8343, DOI 10.1109/CVPR42600.2020.00837
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li CY, 2020, IEEE T MULTIMEDIA, V22, P704, DOI 10.1109/TMM.2019.2933334
   Li D, 2019, IEEE T MULTIMEDIA, V21, P416, DOI 10.1109/TMM.2018.2862341
   Li JQ, 2021, IEEE T MULTIMEDIA, V23, P3153, DOI 10.1109/TMM.2020.3021243
   Li JC, 2021, IEEE T CIRC SYST VID, V31, P2547, DOI 10.1109/TCSVT.2020.3027732
   Li JC, 2018, LECT NOTES COMPUT SC, V11212, P527, DOI 10.1007/978-3-030-01237-3_32
   Li JC, 2019, IEEE INT CONF COMP V, P3814, DOI 10.1109/ICCVW.2019.00474
   Li X, 2018, LECT NOTES COMPUT SC, V11211, P262, DOI 10.1007/978-3-030-01234-2_16
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Li Y, 2016, PROC CVPR IEEE, P2736, DOI 10.1109/CVPR.2016.299
   Li Y, 2015, IEEE I CONF COMP VIS, P226, DOI 10.1109/ICCV.2015.34
   Li Z, 2019, PROC ACM MULTIMEDIA, P1
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu FY, 2016, IEEE T PATTERN ANAL, V38, P2024, DOI 10.1109/TPAMI.2015.2505283
   Liu LX, 2014, SIGNAL PROCESS-IMAGE, V29, P856, DOI 10.1016/j.image.2014.06.006
   Liu XH, 2019, IEEE I CONF COMP VIS, P7313, DOI 10.1109/ICCV.2019.00741
   Luo Y, 2015, IEEE I CONF COMP VIS, P3397, DOI 10.1109/ICCV.2015.388
   Mei KF, 2019, LECT NOTES COMPUT SC, V11361, P203, DOI 10.1007/978-3-030-20887-5_13
   Pan JS, 2021, IEEE T PATTERN ANAL, V43, P2449, DOI 10.1109/TPAMI.2020.2969348
   Qili Deng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P722, DOI 10.1007/978-3-030-58539-6_43
   Qin JH, 2020, NEUROCOMPUTING, V379, P334, DOI 10.1016/j.neucom.2019.10.076
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Qu YY, 2019, PROC CVPR IEEE, P8152, DOI 10.1109/CVPR.2019.00835
   Ren DW, 2019, PROC CVPR IEEE, P3932, DOI 10.1109/CVPR.2019.00406
   Ren WQ, 2020, INT J COMPUT VISION, V128, P240, DOI 10.1007/s11263-019-01235-8
   Ren WQ, 2018, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2018.00343
   Ren WQ, 2019, IEEE T IMAGE PROCESS, V28, P1895, DOI 10.1109/TIP.2018.2876178
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Scharstein D, 2003, PROC CVPR IEEE, P195
   Scharstein D, 2014, LECT NOTES COMPUT SC, V8753, P31, DOI 10.1007/978-3-319-11752-2_3
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Sulami M, 2014, IEEE INT CONF COMPUT
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Wang WC, 2017, IEEE T MULTIMEDIA, V19, P1142, DOI 10.1109/TMM.2017.2652069
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Yang WH, 2017, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2017.183
   Yang XT, 2018, AAAI CONF ARTIF INTE, P7485
   Yi QS, 2021, IET IMAGE PROCESS, V15, P143, DOI 10.1049/ipr2.12013
   Yi QS, 2020, IEEE ACCESS, V8, P54514, DOI 10.1109/ACCESS.2020.2981491
   Zhang H, 2018, IEEE COMPUT SOC CONF, P1015, DOI 10.1109/CVPRW.2018.00135
   Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337
   Zhang H, 2018, PROC CVPR IEEE, P695, DOI 10.1109/CVPR.2018.00079
   Zhang ZJ, 2019, IEEE T MULTIMEDIA, V21, P1681, DOI 10.1109/TMM.2018.2888822
   Zhao D, 2021, IEEE T CIRC SYST VID, V31, P3037, DOI 10.1109/TCSVT.2020.3036992
   Zhu L, 2017, IEEE I CONF COMP VIS, P2545, DOI 10.1109/ICCV.2017.276
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 66
TC 19
Z9 20
U1 4
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 3114
EP 3128
DI 10.1109/TMM.2021.3093724
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2A3LX
UT WOS:000809408000034
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yun, XL
   Zhang, YM
   Yin, F
   Liu, CL
AF Yun, Xiao-Long
   Zhang, Yan-Ming
   Yin, Fei
   Liu, Cheng-Lin
TI Instance GNN: A Learning Framework for Joint Symbol Segmentation and
   Recognition in Online Handwritten Diagrams
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Handwriting recognition; Task analysis; Grammar; Semantics; Image
   segmentation; Trajectory; Text recognition; Online handwritten diagram
   recognition; symbol segmentation; symbol recognition; freehand sketch
   analysis; graph neural networks
ID CLASSIFICATION
AB Online handwritten diagram recognition (OHDR) has attracted considerable attention for its potential applications in many areas, but it is a challenging task due to the complex 2D structure, writing style variation, and lack of annotated data. Existing OHDR methods often have limitations in modeling and learning complex contextual relationships. To overcome these challenges, we propose an OHDR method based on graph neural networks (GNNs) in this paper. In particular, we formulate symbol segmentation and symbol recognition as node clustering and node classification problems on stroke graphs and solve the problems jointly under a unified learning framework with a GNN model. This GNN model is denoted as Instance GNN since it gives the symbol instance label as well as the semantic label. Extensive experiments on two flowchart datasets and a finite automata dataset show that our method consistently outperforms previous methods with large margins and achieves state-of-the-art performance. In addition, we release a large-scale annotated online handwritten flowchart dataset, CASIA-OHFC, and provide initial experimental results as a baseline.
C1 [Yun, Xiao-Long] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
   [Yun, Xiao-Long; Zhang, Yan-Ming; Yin, Fei; Liu, Cheng-Lin] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Liu, Cheng-Lin] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
   [Liu, Cheng-Lin] Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   Chinese Academy of Sciences
RP Liu, CL (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.; Liu, CL (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
EM xiaolong.yun@nlpr.ia.ac.cn; ymzhang@nlpr.ia.ac.cn; fyin@nlpr.ia.ac.cn;
   liucl@nlpr.ia.ac.cn
RI Liu, Cheng-Lin/JCO-6642-2023
OI Yun, Xiao-Long/0000-0001-9353-2900; yan, fei/0000-0002-6412-9140; Liu,
   Cheng-Lin/0000-0002-6743-4175
FU Major Project for New Generation of AI [2018AAA0100400]; National
   Natural Science Foundation of China (NSFC) [61773376, 61721004]
FX This work was supported in part by the Major Project for New Generation
   of AI under Grant 2018AAA0100400 and in part by the National Natural
   Science Foundation of China (NSFC) under Grants 61773376 and 61721004.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Houqiang Li.
CR Alvaro F, 2016, PATTERN RECOGN, V51, P135, DOI 10.1016/j.patcog.2015.09.013
   Alvaro F, 2014, PATTERN RECOGN LETT, V35, P58, DOI 10.1016/j.patrec.2012.09.023
   Le AD, 2017, PROC INT CONF DOC, P1056, DOI 10.1109/ICDAR.2017.175
   [Anonymous], 2013, P ICML
   Awal A.-M., 2011, P SPIE INT SOC OPT E
   Battaglia P.W., 2016, ARXIV161200222
   Bresler M, 2016, INT CONF FRONT HAND, P48, DOI [10.1109/ICFHR.2016.0022, 10.1109/ICFHR.2016.19]
   Bresler M, 2014, INT CONF FRONT HAND, P563, DOI 10.1109/ICFHR.2014.100
   Bresler M, 2016, INT J DOC ANAL RECOG, V19, P253, DOI 10.1007/s10032-016-0269-z
   Bresler M, 2015, IEEE WINT CONF APPL, P610, DOI 10.1109/WACV.2015.87
   Bresler M, 2013, PROC INT CONF DOC, P1215, DOI 10.1109/ICDAR.2013.246
   Carton C, 2013, PROC INT CONF DOC, P1210, DOI 10.1109/ICDAR.2013.245
   Costagliola G, 2014, J VISUAL LANG COMPUT, V25, P955, DOI 10.1016/j.jvlc.2014.10.021
   Coüasnon B, 2006, INT J DOC ANAL RECOG, V8, P111, DOI 10.1007/s10032-005-0148-5
   De Brabandere Bert, 2017, Semantic Instance Segmentation with a Discriminative Loss Function
   Delaye A, 2014, PATTERN RECOGN, V47, P959, DOI 10.1016/j.patcog.2013.04.017
   Deng YT, 2017, 34 INT C MACHINE LEA, V70
   Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304
   Fathi A, 2017, PROC IEEE C COMPUT V, P1
   Feng GH, 2009, PATTERN RECOGN, V42, P3215, DOI 10.1016/j.patcog.2009.01.031
   FUKUNAGA K, 1975, IEEE T INFORM THEORY, V21, P32, DOI 10.1109/TIT.1975.1055330
   Gennari L, 2005, COMPUT GRAPH-UK, V29, P547, DOI 10.1016/j.cag.2005.05.007
   Gervais P., 2020, ARXIV200209303V2, P1
   Glorot X., 2010, INT C ARTIFICIAL INT, P249
   Gong LY, 2019, PROC CVPR IEEE, P9203, DOI 10.1109/CVPR.2019.00943
   Ha David, 2018, INT C LEARN REPR
   Hamilton WL, 2017, ADV NEUR IN, V30
   Hamrick J. B, 2018, PROC ANN M COGN SCI
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang Z, 2014, ACM T GRAPHIC, V33, DOI [10.1145/2661229.2661280, 10.1145/2661228.2661280]
   Julca-Aguilar F, 2020, INT J DOC ANAL RECOG, V23, P143, DOI 10.1007/s10032-019-00349-6
   Julca-Aguilar FD, 2018, 2018 13TH IAPR INTERNATIONAL WORKSHOP ON DOCUMENT ANALYSIS SYSTEMS (DAS), P151, DOI 10.1109/DAS.2018.79
   Jun-Yu Ye, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P993, DOI 10.1109/ICDAR.2019.00163
   Kaiyrbekov K, 2020, IEEE COMPUT GRAPH, V40, P112, DOI 10.1109/MCG.2019.2943333
   Kingma D. P., 2014, arXiv
   Kipf TN, 2017, INT C LEARN REPR
   Lahoud J, 2019, IEEE I CONF COMP VIS, P9255, DOI 10.1109/ICCV.2019.00935
   Lemaitre A., 2011, GRAPHICS RECOGNITION, P89, DOI DOI 10.1007/978-3-642-36824-0
   MacLean S, 2013, INT J DOC ANAL RECOG, V16, P139, DOI 10.1007/s10032-012-0184-x
   Miyao H, 2012, INT CONF FRONT HAND, P83, DOI 10.1109/ICFHR.2012.250
   Paszke A, 2019, ADV NEUR IN, V32
   Payer C, 2018, LECT NOTES COMPUT SC, V11071, P3, DOI 10.1007/978-3-030-00934-2_1
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sarvadevabhatla RK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P10, DOI 10.1145/3123266.3123270
   Schäfer B, 2019, PROC INT CONF DOC, P7, DOI 10.1109/ICDARW.2019.00007
   Schneider RG, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2898351
   Van Phan T, 2016, PATTERN RECOGN, V51, P112, DOI 10.1016/j.patcog.2015.07.012
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Velickovic Petar, 2018, INT C LEARN REPR
   Wang CC, 2017, INT J DOC ANAL RECOG, V20, P123, DOI 10.1007/s10032-017-0284-8
   Wang CC, 2016, INT CONF FRONT HAND, P252, DOI [10.1109/ICFHR.2016.0056, 10.1109/ICFHR.2016.51]
   Wang F, 2019, IEEE INT CON MULTI, P1654, DOI 10.1109/ICME.2019.00285
   Wang QF, 2012, IEEE T PATTERN ANAL, V34, P1469, DOI 10.1109/TPAMI.2011.264
   Wu J, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P1200
   Wu JW, 2020, INT J COMPUT VISION, V128, P2386, DOI 10.1007/s11263-020-01291-5
   Xu DF, 2017, PROC CVPR IEEE, P3097, DOI 10.1109/CVPR.2017.330
   Xu PY, 2020, ANN HEMATOL, V99, P1205, DOI 10.1007/s00277-020-04019-0
   Yang L., 2021, ACM T GRAPHIC, V37, P1
   Yao L., 2019, Proceedings of the AAAI Conference on Artificial Intelligence, V33, P7370
   Ye JY, 2016, INT C PATT RECOG, P3264, DOI 10.1109/ICPR.2016.7900138
   Yuan ZM, 2008, LECT NOTES COMPUT SC, V5328, P55
   Yun XL, 2019, LECT NOTES COMPUT SC, V11901, P232, DOI 10.1007/978-3-030-34120-6_19
   Zanibbi R, 2012, INT J DOC ANAL RECOG, V15, P331, DOI 10.1007/s10032-011-0174-4
   Zhang JS, 2017, PROC INT CONF DOC, P902, DOI 10.1109/ICDAR.2017.152
   Zhang J, 2019, IEEE T MULTIMEDIA, V21, P221, DOI 10.1109/TMM.2018.2844689
   Zhang JS, 2017, PATTERN RECOGN, V71, P196, DOI 10.1016/j.patcog.2017.06.017
   Zhang XL, 2019, IMAGE VISION COMPUT, V89, P67, DOI 10.1016/j.imavis.2019.06.010
   Zhou J, 2020, AI OPEN, V1, P57, DOI 10.1016/j.aiopen.2021.01.001
   Zhu XY, 2020, MULTIMED TOOLS APPL, V79, P1585, DOI 10.1007/s11042-019-08158-z
NR 69
TC 12
Z9 13
U1 4
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2580
EP 2594
DI 10.1109/TMM.2021.3087000
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 1D5KQ
UT WOS:000793839600026
DA 2024-07-18
ER

PT J
AU Zhang, C
   Li, ZX
   Liu, JJ
   Peng, PX
   Ye, QX
   Lu, SJ
   Huang, TJ
   Tian, YH
AF Zhang, Chong
   Li, Zongxian
   Liu, Jingjing
   Peng, Peixi
   Ye, Qixiang
   Lu, Shijian
   Huang, Tiejun
   Tian, Yonghong
TI Self-Guided Adaptation: Progressive Representation Alignment for Domain
   Adaptive Object Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptation models; Object detection; Feature extraction; Detectors;
   Proposals; Kernel; Hilbert space; Self-Guided Adaptation; Progressive
   Representation Alignment; Domain Adaptive Object Detection
AB Unsupervised domain adaptation (UDA) has achieved unprecedented success in improving the cross-domain robustness of object detection models. However, existing UDA methods largely ignore the instantaneous data distribution and the sampling strategy during model learning, which could deteriorate the feature representation given large domain shift. In this work, we propose a Self-Guided Adaptation (SGA) model, targeting at aligning feature representation and transferring object detection models across domains while considering the instantaneous alignment difficulty. The core of SGA is to calculate "hardness" factors for sample pairs indicating domain distance in a kernel space. With the hardness factor, the proposed SGA adaptively indicates the importance of samples and assigns them different constrains. Indicated by these hardness factors, Self-Guided Progressive Sampling (SPS) is implemented in an "easy-to-hard" way during model adaptation. Using multi-stage convolutional features, SGA is further aggregated to fully align hierarchical representations of detection models. Extensive experiments on commonly-used benchmarks show that SGA improves the state-of-the-art methods with significant margins especially on large domain shift cases.
C1 [Zhang, Chong] Peking Univ, Shenzhen Grad Sch, Dept Elect & Comp Engn, Shenzhen 518055, Guangdong, Peoples R China.
   [Zhang, Chong; Li, Zongxian; Liu, Jingjing; Peng, Peixi; Ye, Qixiang; Lu, Shijian; Huang, Tiejun; Tian, Yonghong] Peng Cheng Lab, Shenzhen 518066, Peoples R China.
   [Li, Zongxian; Liu, Jingjing; Peng, Peixi; Huang, Tiejun; Tian, Yonghong] Peking Univ, Sch EE&CS, Natl Engn Lab Video Technol, Beijing 100871, Peoples R China.
   [Ye, Qixiang] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Lu, Shijian] Nanyang Technol Univ, Singapore 639798, Singapore.
C3 Peking University; Peng Cheng Laboratory; Peking University; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   Nanyang Technological University
RP Tian, YH (corresponding author), Peking Univ, Sch EE&CS, Natl Engn Lab Video Technol, Beijing 100871, Peoples R China.
EM ch.zh@pku.edu.cn; zongxian_lee@pku.edu.cn; jingjliu@aiit.org.cn;
   pxpeng@pku.edu.cn; qxye@ucas.ac.cn; Shijian.Lu@ntu.edu.sg;
   tjhuang@pku.edu.cn; yhtian@pku.edu.cn
RI Huang, Tiejun/D-6161-2011; Lu, Shijian/AAU-4831-2021
OI Lu, Shijian/0000-0002-6766-2506; ye, qi xiang/0000-0003-1215-6259;
   Zhang, Chong/0000-0001-6957-2996
FU Key-Area Research and Development Program of Guangdong Province
   [2019B010153002]; National Natural Science Foundation of China
   [61825101]
FX This work was supported by the Key-Area Research and Development Program
   of Guangdong Province under Grant 2019B010153002, and the National
   Natural Science Foundation of China under Grant 61825101. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Ferdous Sohel.
CR Borgwardt KM, 2006, BIOINFORMATICS, V22, pE49, DOI 10.1093/bioinformatics/btl242
   Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18
   Chang-Dong Xu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11721, DOI 10.1109/CVPR42600.2020.01174
   Chen CQ, 2019, PROC CVPR IEEE, P627, DOI 10.1109/CVPR.2019.00072
   Chen Chaoqi, 2020, P IEEE CVF C COMP VI, P8869, DOI DOI 10.1109/CVPR42600.2020.00889
   Chen CL, 2019, IEEE T MULTIMEDIA, V21, P3205, DOI 10.1109/TMM.2019.2916104
   Chen TH, 2017, IEEE I CONF COMP VIS, P521, DOI 10.1109/ICCV.2017.64
   Chen Y, 2018, PROC CVPR IEEE, P3339, DOI 10.1109/CVPR.2018.00352
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Ganin Y, 2016, J MACH LEARN RES, V17
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hoffman J, 2018, PR MACH LEARN RES, V80
   Hsu HK, 2020, IEEE WINT CONF APPL, P738, DOI [10.1109/WACV45572.2020.9093358, 10.1109/wacv45572.2020.9093358]
   Huang J., 2021, ARXIV PREPRINT ARXIV
   Inoue N, 2018, PROC CVPR IEEE, P5001, DOI 10.1109/CVPR.2018.00525
   Jiang L, 2015, AAAI CONF ARTIF INTE, P2694
   Kim S, 2019, IEEE I CONF COMP VIS, P6091, DOI 10.1109/ICCV.2019.00619
   Kim T, 2019, PROC CVPR IEEE, P12448, DOI 10.1109/CVPR.2019.01274
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li JA, 2018, IEEE T MULTIMEDIA, V20, P1645, DOI 10.1109/TMM.2017.2772796
   Li JN, 2018, IEEE T MULTIMEDIA, V20, P985, DOI 10.1109/TMM.2017.2759508
   Li JN, 2017, IEEE T MULTIMEDIA, V19, P944, DOI 10.1109/TMM.2016.2642789
   Li W, 2018, IEEE T PATTERN ANAL, V40, P1114, DOI 10.1109/TPAMI.2017.2704624
   Liu M.-Y., 2016, P ADV NEUR INF PROC, P469
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Long M, 2016, PROCEEDINGS OF SYMPOSIUM OF POLICING DIPLOMACY AND THE BELT & ROAD INITIATIVE, 2016, P136
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Ma XH, 2019, IEEE T MULTIMEDIA, V21, P2419, DOI 10.1109/TMM.2019.2902100
   Motiian S, 2017, IEEE I CONF COMP VIS, P5716, DOI 10.1109/ICCV.2017.609
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281
   Pawan Kumar M., 2010, NIPS
   Redmon J., 2018, P IEEE C COMP VIS PA
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Saito K, 2019, PROC CVPR IEEE, P6949, DOI 10.1109/CVPR.2019.00712
   Saito K, 2018, PROC CVPR IEEE, P3723, DOI 10.1109/CVPR.2018.00392
   Sakaridis C, 2018, INT J COMPUT VISION, V126, P973, DOI 10.1007/s11263-018-1072-8
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Shermin T, 2021, IEEE T MULTIMEDIA, V23, P2732, DOI 10.1109/TMM.2020.3016126
   Simonyan K., 2014, CORR
   Song L, 2013, IEEE SIGNAL PROC MAG, V30, P98, DOI 10.1109/MSP.2013.2252713
   Springenberg Jost Tobias, 2015, Striving for simplicity: The all convolutional net, DOI DOI 10.48550/ARXIV.1412.6806
   Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Tzeng E, 2015, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2015.463
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang XD, 2019, PROC CVPR IEEE, P7281, DOI 10.1109/CVPR.2019.00746
   Yan HL, 2020, IEEE T MULTIMEDIA, V22, P2420, DOI 10.1109/TMM.2019.2953375
   Yang F, 2021, IEEE T MULTIMEDIA, V23, P1681, DOI 10.1109/TMM.2020.3001522
   Yang M, 2019, IEEE T MULTIMEDIA, V21, P1047, DOI 10.1109/TMM.2018.2869276
   Zheng Y., 2020, P IEEE C COMP VIS PA, p13 763
   Zheng YH, 2019, IEEE T MULTIMEDIA, V21, P2292, DOI 10.1109/TMM.2019.2900166
   Zhong Z, 2019, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2019.00069
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
NR 59
TC 15
Z9 16
U1 5
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2246
EP 2258
DI 10.1109/TMM.2021.3078141
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 1D5KQ
UT WOS:000793839600003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Eskimez, SE
   Zhang, Y
   Duan, ZY
AF Eskimez, Sefik Emre
   Zhang, You
   Duan, Zhiyao
TI Speech Driven Talking Face Generation From a Single Image and an Emotion
   Condition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Faces; Visualization; Face recognition; Emotion recognition;
   Synchronization; Speech processing; Lips; Audiovisual; emotion;
   multimodal; talking face generation
ID RECOGNITION; VOICE; MODEL
AB Visual emotion expression plays an important role in audiovisual speech communication. In this work, we propose a novel approach to rendering visual emotion expression in speech-driven talking face generation. Specifically, we design an end-to-end talking face generation system that takes a speech utterance, a single face image, and a categorical emotion label as input to render a talking face video synchronized with the speech and expressing the conditioned emotion. Objective evaluation on image quality, audiovisual synchronization, and visual emotion expression shows that the proposed system outperforms a state-of-the-art baseline system. Subjective evaluation of visual emotion expression and video realness also demonstrates the superiority of the proposed system. Furthermore, we conduct a human emotion recognition pilot study using generated videos with mismatched emotions among the audio and visual modalities. Results show that humans respond to the visual modality more significantly than the audio modality on this task.
C1 [Eskimez, Sefik Emre] Dept Elect & Comp Engn, Rochester, NY 14623 USA.
   [Eskimez, Sefik Emre] Microsoft, Cognit Serv Res CSR Team, Bellevue, WA 98007 USA.
   [Zhang, You; Duan, Zhiyao] Univ Rochester, Dept Elect & Comp Engn, Rochester, NY 14627 USA.
C3 Microsoft; University of Rochester
RP Eskimez, SE (corresponding author), Dept Elect & Comp Engn, Rochester, NY 14623 USA.; Eskimez, SE (corresponding author), Microsoft, Cognit Serv Res CSR Team, Bellevue, WA 98007 USA.
EM seeskime@microsoft.com; yzh298@ur.rochester.edu;
   zhiyao.duan@rochester.edu
RI Zhang, You/AAS-4103-2021; Eskimez, Sefik Emre/AAB-3665-2022
OI Zhang, You/0000-0002-4649-278X; Eskimez, Sefik Emre/0000-0001-6259-5925;
   Duan, Zhiyao/0000-0002-8334-9974
FU National Science Foundation (NSF) [1741472]
FX This work was supported by the National Science Foundation (NSF) under
   Grant 1741472.
CR ALPERT M, 1963, ARCH GEN PSYCHIAT, V8, P362
   [Anonymous], 2013, EMOTION FACE GUIDELI
   [Anonymous], 2005, P INT SEP
   Bernstein JGW, 2009, J ACOUST SOC AM, V125, P3358, DOI 10.1121/1.3110132
   Binnie C. A., 1973, Journal of the Academy of Rehabilitative Audiology, V6, P43
   Bourgais M, 2018, JASSS-J ARTIF SOC S, V21, DOI 10.18564/jasss.3681
   Bulut M., 2004, P 6 INT C MULT INT, P205
   Buslaev A, 2020, INFORMATION, V11, DOI 10.3390/info11020125
   Cambria Erik, 2012, Cognitive Behavioural Systems (COST 2012). International Training School. Revised Selected Papers, P144, DOI 10.1007/978-3-642-34584-5_11
   Cambria E, 2017, SOCIO AFFECT COMPUT, V5, P1, DOI 10.1007/978-3-319-55394-8_1
   Cambria E, 2013, PROCEEDINGS OF THE 2013 IEEE SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE FOR HUMAN-LIKE INTELLIGENCE (CIHLI), P108, DOI 10.1109/CIHLI.2013.6613272
   Cao HW, 2014, IEEE T AFFECT COMPUT, V5, P377, DOI 10.1109/TAFFC.2014.2336244
   Chaturvedi I, 2019, PATTERN RECOGN LETT, V125, P264, DOI 10.1016/j.patrec.2019.04.024
   Chen LL, 2019, PROC CVPR IEEE, P7824, DOI 10.1109/CVPR.2019.00802
   Chen LL, 2018, LECT NOTES COMPUT SC, V11211, P538, DOI 10.1007/978-3-030-01234-2_32
   Chung J. S., 2017, PROC BRIT MACH VIS C
   Cosatto E, 2000, IEEE T MULTIMEDIA, V2, P152, DOI 10.1109/6046.865480
   Cowie R, 2009, PHILOS T R SOC B, V364, P3515, DOI 10.1098/rstb.2009.0139
   CURETON EE, 1967, J AM STAT ASSOC, V62, P1068, DOI 10.2307/2283694
   Dutu L. C., 2019, PROC INT C SPEECH TE, P1
   Eskimez SE, 2020, INT CONF ACOUST SPEE, P1948, DOI [10.1109/ICASSP40776.2020.9054103, 10.1109/icassp40776.2020.9054103]
   Eskimez SE, 2018, LECT NOTES COMPUT SC, V10891, P372, DOI 10.1007/978-3-319-93764-9_35
   Eskimez SE, 2016, INT CONF ACOUST SPEE, P2274, DOI 10.1109/ICASSP.2016.7472082
   Esposito A, 2009, COGN COMPUT, V1, P268, DOI 10.1007/s12559-009-9017-8
   Fang Z, 2022, VISUAL COMPUT, V38, P1151, DOI 10.1007/s00371-021-02074-w
   Gutierrez-Osuna R, 2005, IEEE T MULTIMEDIA, V7, P33, DOI 10.1109/TMM.2004.840611
   Helfer KS, 2005, J ACOUST SOC AM, V117, P842, DOI 10.1121/1.1836832
   Gulrajani I, 2017, ADV NEUR IN, V30
   Jamaludin A, 2019, INT J COMPUT VISION, V127, P1767, DOI 10.1007/s11263-019-01150-y
   Jessen S, 2013, FRONT HUM NEUROSCI, V7, DOI 10.3389/fnhum.2013.00369
   Karras T, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073658
   Kaur R., 2022, Research Anthology on Implementing Sentiment Analysis Across Multiple Disciplines, V10, P1846, DOI DOI 10.4018/IJSSMET.2019040103
   Kowalska M., 2017, HDB COGNITION EMOTIO, P1, DOI [https://doi.org/10.1007/978-3-319-28099-8495-1, DOI 10.1002/0470013494.CH3, 10.1002/0470013494.ch3]
   Lewis PA, 2007, CEREB CORTEX, V17, P742, DOI 10.1093/cercor/bhk024
   Ma YK, 2020, INFORM FUSION, V64, P50, DOI 10.1016/j.inffus.2020.06.011
   Maddox RK, 2015, ELIFE, V4, DOI 10.7554/eLife.04995
   Mariani G., 2018, Bagan: Data augmentation with balancing gan
   Mehrabian A., 1974, APPROACH ENV PSYCHOL, V12
   Nicolaou MA, 2011, IEEE T AFFECT COMPUT, V2, P92, DOI 10.1109/T-AFFC.2011.9
   Park J, 2008, IEEE T MULTIMEDIA, V10, P1299, DOI 10.1109/TMM.2008.2004908
   Piwek L, 2015, FRONT PSYCHOL, V6, DOI 10.3389/fpsyg.2015.00611
   Poria S, 2016, NEUROCOMPUTING, V174, P50, DOI 10.1016/j.neucom.2015.01.095
   PS S., 2017, Int. J. Control. Theory Appl, V10, P651
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   RUSSELL JA, 1980, J PERS SOC PSYCHOL, V39, P1161, DOI 10.1037/h0077714
   Sadoughi N, 2021, IEEE T AFFECT COMPUT, V12, P1031, DOI 10.1109/TAFFC.2019.2916031
   Scherer K., 2000, Neuropsychology of Emotion, V137, P137
   Schirmer A, 2017, TRENDS COGN SCI, V21, P216, DOI 10.1016/j.tics.2017.01.001
   Schreer O, 2008, IEEE T MULTIMEDIA, V10, P352, DOI 10.1109/TMM.2008.917336
   Simonyan K., 2014, CORR
   Soleymani M, 2017, IMAGE VISION COMPUT, V65, P3, DOI 10.1016/j.imavis.2017.08.003
   Song Y, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P919
   Stappen L, 2021, IEEE INTELL SYST, V36, P88, DOI 10.1109/MIS.2021.3062200
   Susanto Y, 2022, COGN COMPUT, V14, P5, DOI 10.1007/s12559-021-09824-x
   Susanto Y, 2020, IEEE INTELL SYST, V35, P96, DOI 10.1109/MIS.2020.2992799
   Suwajanakorn S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073640
   Tang H, 2008, IEEE T MULTIMEDIA, V10, P969, DOI 10.1109/TMM.2008.2001355
   Tsiourti C, 2019, INT J SOC ROBOT, V11, P555, DOI 10.1007/s12369-019-00524-z
   Tzirakis P, 2021, INFORM FUSION, V68, P46, DOI 10.1016/j.inffus.2020.10.011
   Verma A, 2004, IEEE T MULTIMEDIA, V6, P791, DOI 10.1109/TMM.2004.837256
   Vougioukas K, 2020, INT J COMPUT VISION, V128, P1398, DOI 10.1007/s11263-019-01251-8
   Vougioukas Konstantinos, 2018, PROC BRIT MACH VIS C
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xie L, 2007, IEEE T MULTIMEDIA, V9, P500, DOI 10.1109/TMM.2006.888009
   Yu LY, 2019, IEEE DATA MINING, P787, DOI 10.1109/ICDM.2019.00089
   Zadeh A, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2236
   Zakharov E, 2019, IEEE I CONF COMP VIS, P9458, DOI 10.1109/ICCV.2019.00955
   Zhou H, 2019, AAAI CONF ARTIF INTE, P9299
NR 68
TC 25
Z9 27
U1 6
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL 26
PY 2021
VL 24
BP 3480
EP 3490
DI 10.1109/TMM.2021.3099900
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2W7NF
UT WOS:000824706400004
OA Green Submitted, Bronze
DA 2024-07-18
ER

PT J
AU Wang, JY
   Bao, BK
   Xu, CS
AF Wang, Jianyu
   Bao, Bing-Kun
   Xu, Changsheng
TI DualVGR: A Dual-Visual Graph Reasoning Unit for Video Question Answering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cognition; Visualization; Task analysis; Knowledge discovery; Feature
   extraction; Fuses; Dogs; Video question answering; multi-step reasoning;
   graph neural network; multi-modal
AB Video question answering is a challenging task, which requires agents to be able to understand rich video contents and perform spatial-temporal reasoning. However, existing graph-based methods fail to perform multi-step reasoning well, neglecting two properties of VideoQA: (1) Even for the same video, different questions may require different amount of video clips or objects to infer the answer with relational reasoning; (2) During reasoning, appearance and motion features have complicated interdependence which are correlated and complementary to each other. Based on these observations, we propose a Dual-Visual Graph Reasoning Unit (DualVGR) which reasons over videos in an end-to-end fashion. The first contribution of our DualVGR is the design of an explainable Query Punishment Module, which can filter out irrelevant visual features through multiple cycles of reasoning. The second contribution is the proposed Video-based Multi-view Graph Attention Network, which captures the relations between appearance and motion features. Our DualVGR network achieves state-of-the-art performance on the benchmark MSVD-QA and SVQA datasets, and demonstrates competitive results on benchmark MSRVTT-QA datasets. Our code is available at https://github.com/MM-IR/DualVGR-VideoQA.
C1 [Wang, Jianyu; Bao, Bing-Kun] Nanjing Univ Posts & Telecommun, Coll Telecommun & Informat Engn, Nanjing 210003, Peoples R China.
   [Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Xu, Changsheng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
C3 Nanjing University of Posts & Telecommunications; Chinese Academy of
   Sciences; Institute of Automation, CAS; Chinese Academy of Sciences;
   University of Chinese Academy of Sciences, CAS
RP Bao, BK (corresponding author), Nanjing Univ Posts & Telecommun, Coll Telecommun & Informat Engn, Nanjing 210003, Peoples R China.
EM jianyuwang.work@outlook.com; bingkunbao@njupt.edu.cn; csxu@nlpr.ia.ac.cn
RI Wang, Jianyu/ACT-7563-2022; xu, cj/HJZ-3488-2023; Xu,
   Chang/GQP-7280-2022
OI Wang, Jianyu/0000-0002-8304-680X; 
FU National Key Research and Development Plan of China [2020AAA0106200];
   National Natural Science Foundation of China [61936005, 62036012,
   61872424]; Natural Science Foundation of Jiangsu Province [BK20200037]
FX Thisworkwas supported in part by theNationalKey Research andDevelopment
   Plan of China under Grant 2020AAA0106200, in part by the National
   Natural Science Foundation ofChina underGrants 61936005, 62036012, and
   61872424, and in part by the Natural Science Foundation of Jiangsu
   Province under Grant BK20200037. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr.
   Concetto Spampinato.
CR Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Chen D., 2011, P 49 ANN M ASS COMP, P190
   Chen Z., PROC INT C LEARN REP, P2021
   Nguyen DK, 2018, PROC CVPR IEEE, P6087, DOI 10.1109/CVPR.2018.00637
   Fan CY, 2019, PROC CVPR IEEE, P1999, DOI 10.1109/CVPR.2019.00210
   Fukui Akira, 2016, P C EMP METH NAT LAN
   Gan C, 2017, IEEE I CONF COMP VIS, P1829, DOI 10.1109/ICCV.2017.201
   Gao JY, 2018, PROC CVPR IEEE, P6576, DOI 10.1109/CVPR.2018.00688
   Gretton A, 2005, LECT NOTES ARTIF INT, V3734, P63
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu RH, 2019, IEEE I CONF COMP VIS, P10293, DOI 10.1109/ICCV.2019.01039
   Hu RH, 2017, IEEE I CONF COMP VIS, P804, DOI 10.1109/ICCV.2017.93
   Huang D, 2020, AAAI CONF ARTIF INTE, V34, P11021
   Jang Y, 2017, PROC CVPR IEEE, P1359, DOI 10.1109/CVPR.2017.149
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jiang P, 2020, AAAI CONF ARTIF INTE, V34, P11109
   Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215
   Kim J. H., 2017, PROC INT C LEARN REP, P1
   Li GH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P530, DOI 10.1145/3343031.3350922
   Li LJ, 2019, IEEE I CONF COMP VIS, P10312, DOI 10.1109/ICCV.2019.01041
   Li XP, 2019, AAAI CONF ARTIF INTE, P8658
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Mao J., 2018, PROC INT C LEARN REP
   Nie LQ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1098, DOI 10.1145/3343031.3350923
   Nie LQ, 2013, IEEE T MULTIMEDIA, V15, P426, DOI 10.1109/TMM.2012.2229971
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Shi JX, 2019, PROC CVPR IEEE, P8368, DOI 10.1109/CVPR.2019.00857
   Singh G., 2019, THESIS U BRIT COLUMB
   Song L., 2007, P 24 INT C MACH LEAR, P823, DOI DOI 10.1145/1273496.1273600
   Song XM, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P239, DOI 10.1145/3240508.3240563
   Tenenbaum JB, 2000, NEURAL COMPUT, V12, P1247, DOI 10.1162/089976600300015349
   Teney D, 2017, PROC CVPR IEEE, P3233, DOI 10.1109/CVPR.2017.344
   Thao Minh Le, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9969, DOI 10.1109/CVPR42600.2020.00999
   Velickovi  c P., 2018, P INT C LEARN REPR, P1
   Wang X, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1243, DOI 10.1145/3394486.3403177
   Wu ZH, 2021, IEEE T NEUR NET LEAR, V32, P4, DOI 10.1109/TNNLS.2020.2978386
   Xiong CM, 2016, PR MACH LEARN RES, V48
   Xu DJ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1645, DOI 10.1145/3123266.3123427
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Xue HY, 2017, IEEE T IMAGE PROCESS, V26, P5656, DOI 10.1109/TIP.2017.2746267
   Yang TH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1184, DOI 10.1145/3343031.3350969
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   Yi K, 2019, PROC INT C LEARN REP
   Yi KX, 2018, ADV NEUR IN, V31
   Yu Z, 2018, IEEE T NEUR NET LEAR, V29, P5947, DOI 10.1109/TNNLS.2018.2817340
   Yu Z, 2019, PROC CVPR IEEE, P6274, DOI 10.1109/CVPR.2019.00644
   Yu Z, 2017, IEEE I CONF COMP VIS, P1839, DOI 10.1109/ICCV.2017.202
   Zhao Z, 2020, IEEE T IMAGE PROCESS, V29, P3859, DOI 10.1109/TIP.2020.2963950
   Zhao Z, 2019, IEEE T IMAGE PROCESS, V28, P5939, DOI 10.1109/TIP.2019.2922062
   Zhao Z, 2019, IEEE T IMAGE PROCESS, V28, P3860, DOI 10.1109/TIP.2019.2902106
   Zhu WW, 2020, IEEE T MULTIMEDIA, V22, P1823, DOI 10.1109/TMM.2020.2969791
NR 53
TC 26
Z9 27
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL 16
PY 2021
VL 24
BP 3369
EP 3380
DI 10.1109/TMM.2021.3097171
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2W7NK
UT WOS:000824706900001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Amirpour, H
   Pinheiro, AMG
   Fonseca, E
   Ghanbari, M
   Pereira, M
AF Amirpour, Hadi
   Pinheiro, Antonio M. G.
   Fonseca, Elsa
   Ghanbari, Mohammad
   Pereira, Manuela
TI Quality Evaluation of Holographic Images Coded With Standard Codecs
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Holography; Quality assessment; Codecs;
   Transform coding; Optical distortion; Image reconstruction; Digital
   holography; perceived quality; MOS; codecs
ID LIGHT-FIELD; REPRESENTATION; INFORMATION
AB Recently, a larger interest in the different plenoptic formats, including digital holograms, has emerged. Aside from other challenges that several steps of the holographic pipeline, from digital acquisition to display, have to face, visual quality assessment of compressed holograms is particularly demanding due to the distinct nature of this 3D image modality when compared to regular 2D imaging. There are few studies on holographic data quality assessment, particularly with respect to perceptual effects of lossy compression. This work aims to study the quality evaluation of digital hologram reconstructions, presented on regular 2D displays, in the presence of compression distortions. As there is no established or generally agreed compression methodology for digital holograms compression on the hologram plane with available implementations, a set of state-of-the-art compression codecs, namely HEVC, AV1, and JPEG2000, were used for compression of the digital holograms on the object plane. Both computer generated and optically generated holograms were considered. Two subjective tests were conducted to evaluate distortions caused by compression. The first subjective test was conducted on the reconstructed amplitude images of central views, while the second test was conducted on pseudo-videos generated from the reconstructed amplitudes of different views. The subjective quality assessment was based on mean opinion scores. A selection of objective quality metrics was evaluated, and their correlations with mean opinion scores were computed. The VIFp metrics appeared to have the highest correlation.
C1 [Amirpour, Hadi; Pinheiro, Antonio M. G.; Pereira, Manuela] Univ Beira Interior, Inst Telecomunicacoes, P-6200001 Covilha, Portugal.
   [Fonseca, Elsa] Univ Beira Interior, Fiber Mat & Environm Technol FibEnTech, P-6200001 Covilha, Portugal.
   [Ghanbari, Mohammad] Univ Tehran, Sch Elect & Comp Engn, Coll Engn, Tehran, Iran.
   [Ghanbari, Mohammad] Univ Essex, Sch Comp Sci & Elect Engn, Colchester CO4 3SQ, Essex, England.
C3 Universidade da Beira Interior; Universidade da Beira Interior;
   University of Tehran; University of Essex
RP Pereira, M (corresponding author), Univ Beira Interior, Inst Telecomunicacoes, P-6200001 Covilha, Portugal.
EM hadi.amirpourazarian@ubi.pt; pinheiro@ubi.pt; efonseca@ubi.pt;
   ghan@essex.ac.uk; mpereira@di.ubi.pt
RI Pereira, Manuela/Q-3456-2019; Ghanbari, Mohammad/L-4053-2019; Pinheiro,
   Antonio/B-2723-2012
OI Pereira, Manuela/0000-0002-8648-6464; Ghanbari,
   Mohammad/0000-0002-5482-8378; Pinheiro, Antonio/0000-0002-5968-9901;
   Amirpour, Hadi/0000-0001-9853-1720; Fonseca, Elsa/0000-0002-1531-8789
FU Portuguese FCT-Fundacao para a Ciencia e Tecnologia
   [PTDC/EEI-PRO/2849/2014 - POCI-01-0145-FEDER016693, UIDB/EEA/50008/2020,
   UIDB/00195/2020, PLive X-0017-LX-20]; FEDERPT2020 Partnership Agreement
   [PTDC/EEI-PRO/2849/2014 - POCI-01-0145-FEDER016693, UIDB/EEA/50008/2020,
   UIDB/00195/2020, PLive X-0017-LX-20]; Centro de Competencias em Cloud
   Computing [Centro-01-0145-FEDER000019 - C4]; European Regional
   Development Fund (ERDF) through the Programa Operacional Regional do
   Centro (Centro 2020)
FX This work was supported in part by the Portuguese FCT-Fundacao para a
   Ciencia e Tecnologia, in part by FEDERPT2020 Partnership Agreement under
   Projects PTDC/EEI-PRO/2849/2014 - POCI-01-0145-FEDER016693,
   UIDB/EEA/50008/2020, UIDB/EEA/50008/2020, UIDB/00195/2020 and PLive
   X-0017-LX-20, and in part by Operation Centro-01-0145-FEDER000019 - C4 -
   Centro de Competencias em Cloud Computing, cofinanced by the European
   Regional Development Fund (ERDF) through the Programa Operacional
   Regional do Centro (Centro 2020), in the scope of the Sistema de Apoio
   Investigacao Cientifica e Tecnologica -Programas Integrados de IC&DT.
CR Ahar A, 2020, OPT EXPRESS, V28, P37069, DOI 10.1364/OE.405984
   Ahar A, 2015, PROC SPIE, V9599, DOI 10.1117/12.2189887
   Amirpour H, 2019, INT CONF ACOUST SPEE, P2402, DOI 10.1109/ICASSP.2019.8683215
   B. Series, 2012, Recommendation ITU-R BT, V500, P500
   Bernardo M., 2018, P SOC PHOTO-OPT INS, P6
   Bernardo MV, 2018, SIGNAL PROCESS-IMAGE, V68, P193, DOI 10.1016/j.image.2018.08.006
   Blinder D, 2019, SIGNAL PROCESS-IMAGE, V70, P114, DOI 10.1016/j.image.2018.09.014
   Blinder D, 2015, INT WORK QUAL MULTIM
   Bruckheimer E, 2016, EUR HEART J-CARD IMG, V17, P845, DOI 10.1093/ehjci/jew087
   Corda R, 2020, IEEE T BROADCAST, V66, P292, DOI 10.1109/TBC.2019.2954095
   da Silva Cruz Luis A., 2019, INT WORK QUAL MULTIM, P6
   Darakis E, 2010, PROC SPIE, V7529, DOI 10.1117/12.840234
   Demetri H. J, 2000, HOLOGRAPHIC DATA STO, V8, P1
   Ebrahimi T, 2016, IEEE MULTIMEDIA, V23, P14, DOI 10.1109/MMUL.2016.64
   El Rhammad A, 2018, APPL OPTICS, V57, P4930, DOI 10.1364/AO.57.004930
   Fonseca E, 2019, APPL OPTICS, V58, pG282, DOI 10.1364/AO.58.00G282
   GABOR D, 1948, NATURE, V161, P777, DOI 10.1038/161777a0
   Gombköto B, 2002, OPT COMMUN, V214, P115, DOI 10.1016/S0030-4018(02)02183-1
   Hanhart P, 2015, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-015-0091-4
   Javaheri A, 2017, IEEE INT WORKSH MULT
   Kim MK, 2011, SPRINGER SER OPT SCI, V162, P1, DOI 10.1007/978-1-4419-7793-9
   Muhamad RK, 2021, APPL OPTICS, V60, P641, DOI 10.1364/AO.404305
   Onural L, 2011, P IEEE, V99, P576, DOI 10.1109/JPROC.2010.2098430
   Peixeiro JP, 2018, IEEE T MULTIMEDIA, V20, P282, DOI 10.1109/TMM.2017.2742701
   Schelkens P., 2020, PROC IMAG APPL OPT C
   Schelkens P., 2018, P APPL DIGITAL IMAGE, V10752, P544
   Schnars U, 2002, MEAS SCI TECHNOL, V13, pR85, DOI 10.1088/0957-0233/13/9/201
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Shimobaba T, 2016, IEEE T IND INFORM, V12, P1611, DOI 10.1109/TII.2015.2509452
   Shiu MT, 2015, APPL OPTICS, V54, pA84, DOI 10.1364/AO.54.000A84
   Symeonidou A, 2016, PROC SPIE, V9896, DOI 10.1117/12.2225201
   Viola I, 2017, IEEE J-STSP, V11, P1092, DOI 10.1109/JSTSP.2017.2740167
   Viswanathan K, 2013, PROC SPIE, V8856, DOI 10.1117/12.2027199
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
NR 36
TC 10
Z9 10
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL 9
PY 2021
VL 24
BP 3256
EP 3264
DI 10.1109/TMM.2021.3096059
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2W7NT
UT WOS:000824707800002
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Cao, M
   Chen, C
   Dou, H
   Hu, XY
   Peng, SL
   Kuijper, A
AF Cao, Min
   Chen, Chen
   Dou, Hao
   Hu, Xiyuan
   Peng, Silong
   Kuijper, Arjan
TI Progressive Bilateral-Context Driven Model for Post-Processing Person
   Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Probes; Feature extraction; Computational complexity; Visualization;
   Manifolds; Context modeling; Training; Contextual information; person
   re-identification; post-processing
ID NETWORK
AB Most existing person re-identification methods compute pairwise similarity by extracting robust visual features and learning the discriminative metric. Owing to visual ambiguities, these content-based methods that determine the pairwise relationship only based on the similarity between them, inevitably produce a suboptimal ranking list. Instead, the pairwise similarity can be estimated more accurately along the geodesic path of the underlying data manifold by exploring the rich contextual information of the sample. In this paper, we propose a lightweight post-processing person re-identification method in which the pairwise measure is determined by the relationship between the sample and the counterpart's context in an unsupervised way. We translate the point-to-point comparison into the bilateral point-to-set comparison. The sample's context is composed of its neighbor samples with two different definition ways: the first order context and the second order context, which are used to compute the pairwise similarity in sequence, resulting in a progressive post-processing model. The experiments on four large-scale person re-identification benchmark datasets indicate that (1) the proposed method can consistently achieve higher accuracies by serving as a post-processing procedure after the content-based person re-identification methods, showing its state-of-the-art results, (2) the proposed lightweight method only needs about 6 milliseconds for optimizing the ranking results of one sample, showing its high-efficiency. Code is available at: https://github.com/123ci/PBCmodel.
C1 [Cao, Min] Soochow Univ, Sch Comp Sci & Technol, Suzhou 215006, Peoples R China.
   [Cao, Min; Chen, Chen; Dou, Hao; Peng, Silong] Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China.
   [Cao, Min; Kuijper, Arjan] Fraunhofer IGD, D-64283 Darmstadt, Germany.
   [Hu, Xiyuan] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.
   [Kuijper, Arjan] Tech Univ Darmstadt, Math & Appl Visual Comp, D-64283 Darmstadt, Germany.
C3 Soochow University - China; Chinese Academy of Sciences; Institute of
   Automation, CAS; Nanjing University of Science & Technology; Technical
   University of Darmstadt
RP Chen, C (corresponding author), Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China.
EM mcao@suda.edu.cn; chen.chen@ia.ac.cn; douhao2015@ia.ac.cn;
   xiyuan.hu@foxmail.com; silong.peng@ia.ac.cn;
   arjan.kuijper@igd.fraunhofer.de
RI Hu, Xiyuan/AAF-7773-2021; Silong, Peng/E-4704-2013
OI Hu, Xiyuan/0000-0002-7095-6986; Kuijper, Arjan/0000-0002-6413-0061
FU National Key R&D Program of China [25904]; National Natural Science
   Foundation of China [NSFC 61906194]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2017YFC0803505, in part by the National Natural Science
   Foundation of China under Grant NSFC 61906194, and in part by the
   National Key R&D Program of China under Grant 25904.
CR [Anonymous], 2017, P IEEE C COMP VIS PA
   Bai S, 2019, PROC CVPR IEEE, P740, DOI 10.1109/CVPR.2019.00083
   Bai S, 2016, IEEE T IMAGE PROCESS, V25, P1056, DOI 10.1109/TIP.2016.2514498
   Chen BH, 2019, IEEE I CONF COMP VIS, P371, DOI 10.1109/ICCV.2019.00046
   Chen L, 2018, 2018 11TH INTERNATIONAL WORKSHOP ON HUMAN FRIENDLY ROBOTICS (HFR), P54, DOI 10.1109/HFR.2018.8633487
   Dai ZZ, 2019, IEEE I CONF COMP VIS, P3690, DOI 10.1109/ICCV.2019.00379
   Donoser M, 2013, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2013.174
   García J, 2015, IEEE I CONF COMP VIS, P1305, DOI 10.1109/ICCV.2015.154
   Gray D, 2008, LECT NOTES COMPUT SC, V5302, P262, DOI 10.1007/978-3-540-88682-2_21
   Pedronette DCG, 2012, INFORM SCIENCES, V207, P19, DOI 10.1016/j.ins.2012.04.032
   Guo J., 2019, P IEEE INT C COMP VI
   Kalayeh MM, 2018, PROC CVPR IEEE, P1062, DOI 10.1109/CVPR.2018.00117
   Karanam S, 2019, IEEE T PATTERN ANAL, V41, P523, DOI 10.1109/TPAMI.2018.2807450
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Leng QM, 2015, MULTIMED TOOLS APPL, V74, P6989, DOI 10.1007/s11042-014-1949-7
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Lin WY, 2017, IEEE T IMAGE PROCESS, V26, P2438, DOI 10.1109/TIP.2017.2683063
   Liu CX, 2013, IEEE I CONF COMP VIS, P441, DOI 10.1109/ICCV.2013.62
   Liu JX, 2018, PROC CVPR IEEE, P4099, DOI 10.1109/CVPR.2018.00431
   Liu ZM, 2019, IEEE I CONF COMP VIS, P6121, DOI 10.1109/ICCV.2019.00622
   Luo CC, 2019, IEEE I CONF COMP VIS, P4975, DOI 10.1109/ICCV.2019.00508
   Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190
   Matsukawa T, 2016, PROC CVPR IEEE, P1363, DOI 10.1109/CVPR.2016.152
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Sarfraz MS, 2018, PROC CVPR IEEE, P420, DOI 10.1109/CVPR.2018.00051
   Shen YT, 2018, PROC CVPR IEEE, P2265, DOI 10.1109/CVPR.2018.00241
   Shen YT, 2018, LECT NOTES COMPUT SC, V11219, P508, DOI 10.1007/978-3-030-01267-0_30
   Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427
   Suh Y, 2018, LECT NOTES COMPUT SC, V11218, P418, DOI 10.1007/978-3-030-01264-9_25
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Sun YF, 2017, IEEE I CONF COMP VIS, P3820, DOI 10.1109/ICCV.2017.410
   Varior RR, 2016, LECT NOTES COMPUT SC, V9912, P791, DOI 10.1007/978-3-319-46484-8_48
   Wang HX, 2016, LECT NOTES COMPUT SC, V9908, P405, DOI 10.1007/978-3-319-46493-0_25
   Wang H, 2018, 2018 INTERNATIONAL CONFERENCE ON EDUCATION, PSYCHOLOGY, AND MANAGEMENT SCIENCE (ICEPMS 2018), P365, DOI 10.25236/icepms.2018.079
   Wang Z, 2019, IEEE T MULTIMEDIA, V21, P2376, DOI 10.1109/TMM.2019.2898753
   Wang ZH, 2014, METHOD ENZYMOL, V548, P1, DOI 10.1016/B978-0-12-397918-6.00001-X
   Yan YC, 2019, PROC CVPR IEEE, P2153, DOI 10.1109/CVPR.2019.00226
   Yang WJ, 2019, PROC CVPR IEEE, P1389, DOI 10.1109/CVPR.2019.00148
   Yang X, 2018, IEEE T IMAGE PROCESS, V27, P791, DOI 10.1109/TIP.2017.2765836
   Ye M, 2016, IEEE T MULTIMEDIA, V18, P2553, DOI 10.1109/TMM.2016.2605058
   Zhang X, 2016, CLIN MASS SPECTROM, V2, P1, DOI 10.1016/j.clinms.2016.11.002
   Zhao ZC, 2018, PATTERN RECOGN, V75, P90, DOI 10.1016/j.patcog.2017.03.023
   Zheng F, 2019, PROC CVPR IEEE, P8506, DOI 10.1109/CVPR.2019.00871
   Zheng F, 2018, IEEE T PATTERN ANAL, V40, P1059, DOI 10.1109/TPAMI.2016.2645565
   Zheng L, 2017, PROC CVPR IEEE, P3346, DOI 10.1109/CVPR.2017.357
   Zheng L, 2016, LECT NOTES COMPUT SC, V9910, P868, DOI 10.1007/978-3-319-46466-4_52
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z., ARXIV170804896
   Zhong Z., 2017, P IEEE CVF C COMP VI, P1318, DOI [10.1109/CVPR.2017.389, DOI 10.1109/CVPR.2017.389]
   Zhou SP, 2018, IEEE T MULTIMEDIA, V20, P593, DOI 10.1109/TMM.2017.2755983
NR 53
TC 8
Z9 8
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1239
EP 1251
DI 10.1109/TMM.2020.2994524
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA RU3SE
UT WOS:000645068200006
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Chen, XL
   Song, XM
   Cui, SW
   Gan, T
   Cheng, ZY
   Nie, LQ
AF Chen, Xiaolin
   Song, Xuemeng
   Cui, Siwei
   Gan, Tian
   Cheng, Zhiyong
   Nie, Liqiang
TI User Identity Linkage Across Social Media via Attentive Time-Aware User
   Modeling
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Couplings; Social networking (online); Correlation; Multimedia Web
   sites; Adaptation models; Task analysis; Blogs; Attention mechanism;
   user identity linkage; temporal correlation
ID NETWORK; IDENTIFICATION; CLASSIFICATION
AB In this paper, we work towards linking users' identities on different social media platforms by exploring the user-generated contents (UGCs). This task is non-trivial due to the following challenges. 1) As UGCs involve multiple modalities (e.g., text and image), how to accurately characterize the user account based on their heterogeneous multi-modal UGCs poses the main challenge. 2) As people tend to post similar UGCs on different social media platforms during the same period, how to effectively model the temporal post correlation is a crucial challenge. And 3) no public benchmark dataset is available to support our user identity linkage based on heterogeneous UGCs with timestamps. Towards this end, we present an attentive time-aware user identity linkage scheme, which seamlessly integrates the temporal post correlation modeling and attentive user similarity modeling. To facilitate the evaluation, we create a comprehensive large-scale user identity linkage dataset from two popular social media platforms: Instagram and Twitter. Extensive experiments have been conducted on our dataset and the results verify the effectiveness of the proposed scheme. As a residual product, we have released the dataset, codes, and parameters to facilitate other researchers.
C1 [Chen, Xiaolin; Song, Xuemeng; Gan, Tian; Nie, Liqiang] Shandong Univ, Sch Comp Sci & Technol, Qingdao 266237, Peoples R China.
   [Cui, Siwei] Texas A&M Univ, Dept Comp Sci & Engn, College Stn, TX 77843 USA.
   [Cheng, Zhiyong] Qilu Univ Technol, Shandong Acad Sci, Jinan 250014, Peoples R China.
C3 Shandong University; Texas A&M University System; Texas A&M University
   College Station; Qilu University of Technology
RP Song, XM (corresponding author), Shandong Univ, Sch Comp Sci & Technol, Qingdao 266237, Peoples R China.
EM cxlicd@gmail.com; sxmustc@gmail.com; jncuisiwei@gmail.com;
   ttgantian@gmail.com; jason.zy.cheng@gmail.com; nieliqiang@gmail.com
RI Cui, Siwei/KHE-0273-2024
OI Cui, Siwei/0000-0002-0302-1064
FU National Key Research and Development Project of New Generation
   Artificial Intelligence [2018AAA0102502]; NationalNatural Science
   Foundation of China [61772310, 61702300, U1936203]; Shandong Provincial
   Natural Science Foundation [ZR2019JQ23]; Shandong Provincial Key
   Research and Development Program [2019JZZY010118]; Innovation Teams in
   Colleges and Universities in Jinan [2018GXRC014]
FX tThis work was supported by the National Key Research and Development
   Project of New Generation Artificial Intelligence under Grant
   2018AAA0102502, in part by the NationalNatural Science Foundation of
   China under Grant 61772310, Grant 61702300, and Grant U1936203, in part
   by the Shandong Provincial Natural Science Foundation under Grant
   ZR2019JQ23, in part by the Shandong Provincial Key Research and
   Development Program under Grant 2019JZZY010118, and in part by the
   Innovation Teams in Colleges and Universities in Jinan under Grant
   2018GXRC014. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. M. Shamim Hossain.
   (Corresponding author: Xuemeng Song.)
CR Abdi H, 2010, WIRES COMPUT STAT, V2, P433, DOI 10.1002/wics.101
   Ahangama S, 2018, P INT C INF SYST ASS
   Alqaisi T, 2012, IEEE IMAGE PROC, P2393
   [Anonymous], 1962, PRINCIPLES NEURODYNA
   [Anonymous], 2011, P ICML
   [Anonymous], 2014, 2014 IEEE PES GEN M, DOI [10.1109/URSIGASS.2014.6929023, DOI 10.1109/URSIGASS.2014.6929023, DOI 10.1109/PESGM.2014.6939258]
   Bennacer N, 2014, LECT NOTES COMPUT SC, V8484, P424, DOI 10.1007/978-3-319-07881-6_29
   Bors AG, 2017, MULTIMED SYST APPL, P171, DOI 10.1007/978-3-319-57687-9_8
   Cao X.Z., 2016, P JOINT EUR C MACH L, P459
   Carvalho M, 2018, ACM/SIGIR PROCEEDINGS 2018, P35, DOI 10.1145/3209978.3210036
   Chen JY, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P335, DOI 10.1145/3077136.3080797
   Cho K, 2015, IEEE T MULTIMEDIA, V17, P1875, DOI 10.1109/TMM.2015.2477044
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Cui P, 2018, IEEE T MULTIMEDIA, V20, P198, DOI 10.1109/TMM.2017.2724843
   Faloutsos C., 1994, Journal of Intelligent Information Systems: Integrating Artificial Intelligence and Database Technologies, V3, P231, DOI 10.1007/BF00962238
   Fan ZX, 2019, IEEE T MULTIMEDIA, V21, P363, DOI 10.1109/TMM.2018.2859620
   FELLEGI IP, 1969, J AM STAT ASSOC, V64, P1183, DOI 10.2307/2286061
   Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451
   Georgiev K., 2013, PMLR, P1148
   Goga O., 2013, P 22 INT C WORLD WID, P447, DOI DOI 10.1145/2488388.2488428
   Graves A, 2005, NEURAL NETWORKS, V18, P602, DOI 10.1016/j.neunet.2005.06.042
   Guglielmo EJ, 1996, ACM T INFORM SYST, V14, P237, DOI 10.1145/230538.230539
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He YH, 2016, IEEE T MULTIMEDIA, V18, P1363, DOI 10.1109/TMM.2016.2558463
   Hsu CC, 2018, IEEE T MULTIMEDIA, V20, P421, DOI 10.1109/TMM.2017.2745702
   Jacobs C. E., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P277, DOI 10.1145/218380.218454
   Jiao YF, 2018, IEEE T MULTIMEDIA, V20, P2693, DOI 10.1109/TMM.2018.2815998
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kong XN, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P179, DOI 10.1145/2505515.2505531
   Krizhevsky A., 2012, ADV NEURAL INF PROCE, V25, P1097
   LACHENBRUCH PA, 1979, BIOMETRICS, V35, P69, DOI 10.2307/2529937
   Le Quoc V., 2014, P INT C MACH LEARN I
   LI CH, 1993, PATTERN RECOGN, V26, P617, DOI 10.1016/0031-3203(93)90115-D
   Li CZ, 2018, CIKM'18: PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P447, DOI 10.1145/3269206.3271675
   Lim BH, 2015, PROCEEDINGS OF THE 2015 IEEE/ACM INTERNATIONAL CONFERENCE ON ADVANCES IN SOCIAL NETWORKS ANALYSIS AND MINING (ASONAM 2015), P113, DOI 10.1145/2808797.2808820
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu AA, 2016, IEEE T IMAGE PROCESS, V25, P2103, DOI 10.1109/TIP.2016.2540802
   Liu M, 2018, ACM/SIGIR PROCEEDINGS 2018, P15, DOI 10.1145/3209978.3210003
   Liu P., 2016, P 25 INT JOINT C ART, P2873, DOI DOI 10.5555/3060832.3061023
   Liu SY, 2014, SIGMOD'14: PROCEEDINGS OF THE 2014 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P51, DOI 10.1145/2588555.2588559
   Loria S, 2014, Secondary TextBlob: Simplified Text Processing
   Man T., 2016, IJCAI, V16, P1823, DOI DOI 10.5555/3060832.3060876
   Mohamed AR, 2015, 2015 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P78, DOI 10.1109/ASRU.2015.7404777
   Mu X, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1775, DOI 10.1145/2939672.2939849
   Olszewska J. I., 2012, Proceedings of the 2012 IEEE 16th International Conference on Intelligent Engineering Systems (INES), P91, DOI 10.1109/INES.2012.6249809
   Olszewska JI, 2019, PROCEEDINGS OF THE 11TH INTERNATIONAL CONFERENCE ON AGENTS AND ARTIFICIAL INTELLIGENCE (ICAART), VOL 2, P850, DOI 10.5220/0007585208500856
   Pedregosa F, 2011, J. Mach. Learn. Res., V12, P2825
   Pennington J., 2014, P C EMP METH NAT LAN, P1532, DOI DOI 10.3115/V1/D14-1162
   Perito D, 2011, LECT NOTES COMPUT SC, V6794, P1, DOI 10.1007/978-3-642-22263-4_1
   Rajendran J., 2016, 2016 C N AM CHAPT AS, P171, DOI [DOI 10.18653/V1/N16-1021, 10.18653/v1/N16-1021]
   Ren YX, 2021, IEEE T KNOWL DATA EN, V33, P1848, DOI 10.1109/TKDE.2019.2947908
   Rendle S., 2009, P 25 C UNC ART INT, P452, DOI DOI 10.5555/1795114.1795167
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sang JT, 2018, IEEE T MULTIMEDIA, V20, P3439, DOI 10.1109/TMM.2018.2839530
   Sang JT, 2015, IEEE T MULTIMEDIA, V17, P2259, DOI 10.1109/TMM.2015.2486524
   Shu K., 2017, ACM SIGKDD Explor. Newslett, V18, P5, DOI 10.1145/3068777.3068781
   Song XM, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P753, DOI 10.1145/3123266.3123314
   Tan JH, 2019, IEEE T MULTIMEDIA, V21, P2686, DOI 10.1109/TMM.2019.2904878
   Victor SP, 2011, 2011 THIRD INTERNATIONAL CONFERENCE ON ADVANCED COMPUTING (ICOAC), P1, DOI 10.1109/ICoAC.2011.6165168
   Wang DX, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1225, DOI 10.1145/2939672.2939753
   Wang DX, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2291
   Wang Q., 2016, PROC 25 INT JOINT C, DOI DOI 10.5555/3060832.3061033
   Wang SP, 2017, IEEE T MULTIMEDIA, V19, P1454, DOI 10.1109/TMM.2017.2663324
   Wang WQ, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P545, DOI 10.1145/3331184.3331258
   Wu F, 2016, IEEE T IMAGE PROCESS, V25, P630, DOI 10.1109/TIP.2015.2507401
   Yan M, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P19, DOI 10.1145/2671188.2749344
   Yan M, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2957755
   Yan M, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P557, DOI 10.1145/2647868.2654920
   Yan M, 2015, IEEE T MULTIMEDIA, V17, P1248, DOI 10.1109/TMM.2015.2446949
   Yao YS, 2016, LECT NOTES COMPUT SC, V9950, P345, DOI 10.1007/978-3-319-46681-1_42
   Zafarani R., 2009, P 3 INT C WEBL SOC M, V9, P354
   Zhang JW, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2125
   Zhang YT, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1485, DOI 10.1145/2783258.2783268
   Zhang ZJ, 2019, IEEE T MULTIMEDIA, V21, P1681, DOI 10.1109/TMM.2018.2888822
   Zhao B, 2017, IEEE T MULTIMEDIA, V19, P1245, DOI 10.1109/TMM.2017.2648498
   Zheng R, 2006, J AM SOC INF SCI TEC, V57, P378, DOI 10.1002/asi.20316
   Zhou JY, 2019, IEEE INFOCOM SER, P2116, DOI [10.1109/INFOCOM.2019.8737542, 10.1109/infocom.2019.8737542]
   Zhou XP, 2016, IEEE T KNOWL DATA EN, V28, P411, DOI 10.1109/TKDE.2015.2485222
NR 78
TC 0
Z9 0
U1 19
U2 35
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3957
EP 3967
DI 10.1109/TMM.2020.3034540
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XA2YU
UT WOS:000720519900003
DA 2024-07-18
ER

PT J
AU Chen, XS
   Liu, D
   Xiong, ZW
   Zha, ZJ
AF Chen, Xusong
   Liu, Dong
   Xiong, Zhiwei
   Zha, Zheng-Jun
TI Learning and Fusing Multiple User Interest Representations for
   Micro-Video and Movie Recommendations
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Videos; Motion pictures; Collaboration; Fuses; Recommender systems;
   Machine learning; Computational modeling; Attention mechanism; deep
   learning; information fusion; personalized recommendation;
   representation learning; user interest
AB Deep learning is known to be effective at automating the generation of representations, which eliminates the need for handcrafted features. For the task of personalized recommendation, deep learning-based methods have achieved great success by learning efficient representations of multimedia items, especially images and videos. Previous works usually adopt simple, single-modality representations of user interest, such as user embeddings, which cannot fully characterize the diversity and volatility of user interest. To address this problem, in this paper we focus on learning and fusing multiple kinds of user interest representations by leveraging deep networks. Specifically, we consider efficient representations of four aspects of user interest: first, we use latent representation, i.e. user embedding, to profile the overall interest; second, we propose item-level representation, which is learned from and integrates the features of a user's historical items; third, we investigate neighbor-assisted representation, i.e. using neighboring users' information to characterize user interest collaboratively; fourth, we propose category-level representation, which is learned from the categorical attributes of a user's historical items. In order to integrate these multiple user interest representations, we study both early fusion and late fusion; where for early fusion, we study different fusion functions. We validate the proposed method on two real-world video recommendation datasets for micro-video and movie recommendations, respectively. Experimental results demonstrate that our method outperforms existing state-of-the-arts by a significant margin. Our code is publicly available.
C1 [Chen, Xusong; Liu, Dong; Xiong, Zhiwei; Zha, Zheng-Jun] Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230027, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Liu, D (corresponding author), Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230027, Peoples R China.
EM cxs2016@mail.ustc.edu.cn; dongeliu@ustc.edu.cn; zwxiong@ustc.edu.cn;
   zhazj@ustc.edu.cn
RI liu, dong/GRJ-9115-2022; Zha, Zheng-Jun/AAF-8667-2020
OI Liu, Dong/0000-0001-9100-2906; Chen, Xusong/0000-0003-3425-1791
FU National Key Research and Development Program of China [2018YFA0701603];
   Natural Science Foundation of China [61772483, 61622211, U19B2038,
   61620106009]; Fundamental Research Funds for the Central Universities
   [WK2100100030]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2018YFA0701603, in part by the
   Natural Science Foundation of China under Grants 61772483, 61622211,
   U19B2038, and 61620106009, and in part by the Fundamental Research Funds
   for the Central Universities under Grant WK2100100030.
CR [Anonymous], 2016, P 1 WORKSH DEEP LEAR, DOI [DOI 10.1145/2988450.2988454, 10.1145/2988450.2988454]
   Bahdanau D., 2015, 3 INT C LEARN REPR I
   Baluja S, 2008, WORLD WID WEB C, P895
   Chen JY, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P335, DOI 10.1145/3077136.3080797
   Chen JY, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P898, DOI 10.1145/2964284.2964314
   Chen XS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1146, DOI 10.1145/3240508.3240617
   Covington P, 2016, PROCEEDINGS OF THE 10TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'16), P191, DOI 10.1145/2959100.2959190
   Cui P, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P597, DOI 10.1145/2647868.2654946
   Cui Y., 2016, P INT C COMP LING, P1777
   Davidson James, 2010, P 4 ACM C REC SYST, P293, DOI [DOI 10.1145/1864708.1864770, 10.1145/1864708]
   Deldjoo Y, 2016, J DATA SEMANT, V5, P99, DOI 10.1007/s13740-016-0060-9
   Du XZ, 2020, IEEE T KNOWL DATA EN, V32, P492, DOI 10.1109/TKDE.2018.2885520
   Du YL, 2019, IEEE T MULTIMEDIA, V21, P555, DOI 10.1109/TMM.2018.2887018
   Ferracani A, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P351, DOI 10.1145/2911996.2912066
   Gao JY, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P127, DOI 10.1145/3123266.3123433
   Gomez-Rodriguez M, 2016, ACM T INFORM SYST, V34, DOI 10.1145/2824253
   Guo HF, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1725
   Hamilton WL, 2017, ADV NEUR IN, V30
   Han TT, 2017, IEEE T MULTIMEDIA, V19, P712, DOI 10.1109/TMM.2016.2631881
   Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872
   He XN, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P173, DOI 10.1145/3038912.3052569
   Huang YX, 2016, SIGMOD'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P35, DOI 10.1145/2882903.2903743
   Kingma D. P, 2015, International Conference on Learning Representations
   Koren Y., 2008, P 14 ACM SIGKDD INT, P426
   Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263
   Li JF, 2011, IEEE IMAGE PROC, P1525, DOI 10.1109/ICIP.2011.6115735
   Liu S, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P3020, DOI 10.1145/3308558.3313513
   Mei T, 2011, ACM T INFORM SYST, V29, DOI 10.1145/1961209.1961213
   Park J, 2011, IEEE MULTIMEDIA, V18, P78, DOI 10.1109/MMUL.2010.6
   Pazzani M. J., 2007, The Adaptive Web. Methods and Strategies of Web Personalization, P325
   Peng T, 2012, 2012 13TH INTERNATIONAL CONFERENCE ON ELECTRONIC PACKAGING TECHNOLOGY & HIGH DENSITY PACKAGING (ICEPT-HDP 2012), P1133, DOI 10.1109/ICEPT-HDP.2012.6474807
   Perozzi B., 2014, KDD, P701, DOI DOI 10.1145/2623330.2623732
   Rendle S., 2009, P 25 C UNCERTAINTY A, P452
   Sang JT, 2018, IEEE T MULTIMEDIA, V20, P3439, DOI 10.1109/TMM.2018.2839530
   Sun LF, 2017, IEEE T MULTIMEDIA, V19, P609, DOI 10.1109/TMM.2016.2635589
   Tao CY, 2019, PROCEEDINGS OF THE TWELFTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'19), P267, DOI 10.1145/3289600.3290985
   Vaswani A, 2017, ADV NEUR IN, V30
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yan M, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P19, DOI 10.1145/2671188.2749344
   Yang YJ, 2018, IEEE T MULTIMEDIA, V20, P1888, DOI 10.1109/TMM.2017.2779043
   Zhang J, 2019, IEEE T MULTIMEDIA, V21, P2762, DOI 10.1109/TMM.2019.2912124
   Zhao Z, 2018, IEEE T MULTIMEDIA, V20, P430, DOI 10.1109/TMM.2017.2740022
   Zhou C, 2018, AAAI CONF ARTIF INTE, P4564
   Zhou GR, 2019, AAAI CONF ARTIF INTE, P5941
   Zhou GR, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1059, DOI 10.1145/3219819.3219823
   Zhou XM, 2015, SIGMOD'15: PROCEEDINGS OF THE 2015 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P1645, DOI 10.1145/2723372.2749444
   Zhu QS, 2013, IEEE INT SYM MULTIM, P219, DOI 10.1109/ISM.2013.41
NR 47
TC 26
Z9 27
U1 2
U2 45
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 484
EP 496
DI 10.1109/TMM.2020.2978618
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA PJ6LW
UT WOS:000601877600038
DA 2024-07-18
ER

PT J
AU Cheng, JC
   Yuan, YH
   Li, YL
   Wang, JD
   Wang, SJ
AF Cheng, Jingchun
   Yuan, Yuhui
   Li, Yali
   Wang, Jingdong
   Wang, Shengjin
TI Learning to Segment Video Object With Accurate Boundaries
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Training; Object segmentation; Prediction algorithms;
   Image segmentation; Semantics; Proposals; Mask-boundAry-Consistent
   network; video object segmentation; convolutional neural networks; joint
   learning
ID CO-SEGMENTATION; MOVING-OBJECTS; SCENES
AB Video object segmentation has attracted considerable research interest these years. Top-performing video object segmentation methods mainly rely on fully convolutional neural networks which are specifically trained for predicting high-performance masks, resulting in a lack of preciseness in boundary details. This paper tackles the problem of predicting both mask-accurate and boundary-precise segmentation masks in videos. To solve this problem, we propose a simple and efficient network structure: the Mask-boundAry-Consistent Network (MAC-Net). The MAC-Net is an end-to-end fully convolutional network, where both mask and boundaries are jointly optimized during training, enabling it to predict masks along with accurate boundaries. An inner-net boundary-computing module is incorporated in the MAC-Net for producing spontaneously mask-consistent boundaries. We analyze the influence of parameter settings, network constructions of the MAC-Net, and compare with state-of-the-art algorithms on three widely-adopted datasets. Experimental results show that the MAC-Net achieves state-of-the-art performance, demonstrating the effectiveness of its mask-boundary-consistent network structure. We also propose that the boundary module in MAC-Net has high compatibility, and can be easily adapted to other segmentation-related techniques.
C1 [Cheng, Jingchun] Beihang Univ, Beijing 100083, Peoples R China.
   [Cheng, Jingchun; Li, Yali; Wang, Shengjin] Tsinghua Univ, Beijing 100084, Peoples R China.
   [Yuan, Yuhui; Wang, Jingdong] Microsoft Res Asia, Beijing 100080, Peoples R China.
C3 Beihang University; Tsinghua University; Microsoft; Microsoft Research
   Asia
RP Li, YL (corresponding author), Tsinghua Univ, Beijing 100084, Peoples R China.
EM chengjingchun14@163.com; Yuhui.Yuan@microsoft.com;
   liyali13@mail.tsinghua.edu.cn; jingdw@microsoft.com;
   wgsgj@tsinghua.edu.cn
RI Cheng, Jingchun/AEJ-7497-2022; Wang, Jingdong/E-9920-2017
OI Wang, Jingdong/0000-0002-4888-4445
FU National Natural Science Foundation of China [61701277]; Cross-Media
   Intelligent Technology Project of Beijing National Research Center for
   Information Science and Technology (BNRist) [BNR2019TD01022]; Institute
   for Guo Qiang, Tsinghua University [2019GQG0001]
FX This work was supported by the National Natural Science Foundation of
   China under Grant No. 61701277, Cross-Media Intelligent Technology
   Project of Beijing National Research Center for Information Science and
   Technology (BNRist) under Grant No. BNR2019TD01022 and the research fund
   under Grant No. 2019GQG0001 from the Institute for Guo Qiang, Tsinghua
   University.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Badrinarayanan V, 2013, IEEE T PATTERN ANAL, V35, P2751, DOI 10.1109/TPAMI.2013.54
   Bao LC, 2018, PROC CVPR IEEE, P5977, DOI 10.1109/CVPR.2018.00626
   Bonneel N, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661253
   Brand M, 2000, IEEE T PATTERN ANAL, V22, P844, DOI 10.1109/34.868685
   Caelles S, 2017, PROC CVPR IEEE, P5320, DOI 10.1109/CVPR.2017.565
   Chapelle O, 2010, INFORM RETRIEVAL, V13, P216, DOI 10.1007/s10791-009-9110-3
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen LC, 2016, PROC CVPR IEEE, P4545, DOI 10.1109/CVPR.2016.492
   Chen LC, 2016, PROC CVPR IEEE, P3640, DOI 10.1109/CVPR.2016.396
   Cheng J., 2018, P AS C COMP VIS, P223
   Cheng J., 2017, CVPR WORKSH
   Cheng JC, 2018, PROC CVPR IEEE, P7415, DOI 10.1109/CVPR.2018.00774
   Cheng JC, 2017, IEEE I CONF COMP VIS, P686, DOI 10.1109/ICCV.2017.81
   Cucchiara R, 2003, IEEE T PATTERN ANAL, V25, P1337, DOI 10.1109/TPAMI.2003.1233909
   Deshpande A, 2015, IEEE I CONF COMP VIS, P567, DOI 10.1109/ICCV.2015.72
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fu HZ, 2015, IEEE T IMAGE PROCESS, V24, P3415, DOI 10.1109/TIP.2015.2442915
   Guler A, 2016, LECT NOTES COMPUT SC, V9914, P415, DOI 10.1007/978-3-319-48881-3_29
   Han JW, 2018, IEEE T IMAGE PROCESS, V27, P1639, DOI 10.1109/TIP.2017.2781424
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Jain SD, 2017, PROC CVPR IEEE, P2117, DOI 10.1109/CVPR.2017.228
   Jain SD, 2014, LECT NOTES COMPUT SC, V8692, P656, DOI 10.1007/978-3-319-10593-2_43
   Jampani V, 2017, PROC CVPR IEEE, P3154, DOI 10.1109/CVPR.2017.336
   Jerripothula KR, 2016, IEEE T MULTIMEDIA, V18, P1896, DOI 10.1109/TMM.2016.2576283
   Jiang HQ, 2015, IEEE T MULTIMEDIA, V17, P3, DOI 10.1109/TMM.2014.2368273
   Koh YJ, 2017, PROC CVPR IEEE, P7417, DOI 10.1109/CVPR.2017.784
   Lao D, 2018, LECT NOTES COMPUT SC, V11214, P441, DOI 10.1007/978-3-030-01249-6_27
   Li RJ, 2007, IEEE T CONSUM ELECTR, V53, P1161, DOI 10.1109/TCE.2007.4341600
   Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu Y, 2017, PROC CVPR IEEE, P5872, DOI 10.1109/CVPR.2017.622
   Luiten J, 2019, LECT NOTES COMPUT SC, V11364, P565, DOI 10.1007/978-3-030-20870-7_35
   Luo B, 2017, IEEE T MULTIMEDIA, V19, P1482, DOI 10.1109/TMM.2017.2671447
   Märki N, 2016, PROC CVPR IEEE, P743, DOI 10.1109/CVPR.2016.87
   Maini R., 2009, Int J Image Process, V3, P1
   Maninis KK, 2019, IEEE T PATTERN ANAL, V41, P1515, DOI 10.1109/TPAMI.2018.2838670
   Maninis KK, 2018, IEEE T PATTERN ANAL, V40, P819, DOI 10.1109/TPAMI.2017.2700300
   Meng JJ, 2016, IEEE T MULTIMEDIA, V18, P116, DOI 10.1109/TMM.2015.2500734
   Mottaghi R, 2014, PROC CVPR IEEE, P891, DOI 10.1109/CVPR.2014.119
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178
   Ochs P, 2014, IEEE T PATTERN ANAL, V36, P1187, DOI 10.1109/TPAMI.2013.242
   Oh SW, 2018, PROC CVPR IEEE, P7376, DOI 10.1109/CVPR.2018.00770
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Perazzi F, 2017, PROC CVPR IEEE, P3491, DOI 10.1109/CVPR.2017.372
   Pont-Tuset J., 2017, ARXIV170400675
   Prest A, 2012, PROC CVPR IEEE, P3282, DOI 10.1109/CVPR.2012.6248065
   Qiu ZF, 2018, IEEE T MULTIMEDIA, V20, P939, DOI 10.1109/TMM.2017.2759504
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Revaud J, 2015, PROC CVPR IEEE, P1164, DOI 10.1109/CVPR.2015.7298720
   Ruan T, 2019, AAAI CONF ARTIF INTE, P4814
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683
   Sidiropoulos P, 2011, IEEE T CIRC SYST VID, V21, P1163, DOI 10.1109/TCSVT.2011.2138830
   Song HM, 2018, LECT NOTES COMPUT SC, V11215, P744, DOI 10.1007/978-3-030-01252-6_44
   Sun C, 2017, IEEE T CIRC SYST VID, V27, P1491, DOI 10.1109/TCSVT.2016.2543038
   Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931
   Tokmakov P, 2017, IEEE I CONF COMP VIS, P4491, DOI 10.1109/ICCV.2017.480
   Tsai YH, 2016, PROC CVPR IEEE, P3899, DOI 10.1109/CVPR.2016.423
   Voigtlaender P., 2017, BMVC, P1000
   Wang F, 2014, IEEE T MULTIMEDIA, V16, P1303, DOI 10.1109/TMM.2014.2315780
   Wang Wenguan, 2018, IEEE Trans Image Process, V27, P38, DOI 10.1109/TIP.2017.2754941
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xie SN, 2017, INT J COMPUT VISION, V125, P3, DOI 10.1007/s11263-017-1004-z
   Yang J, 2016, IEEE T CIRC SYST VID, V26, P1070, DOI 10.1109/TCSVT.2015.2433171
   Ye LW, 2017, IEEE T MULTIMEDIA, V19, P1742, DOI 10.1109/TMM.2017.2693022
   Yuan Y, 2018, OCNET OBJECT CONTEXT
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179
   Zhu HY, 2016, IEEE T MULTIMEDIA, V18, P1516, DOI 10.1109/TMM.2016.2571629
NR 74
TC 2
Z9 2
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3112
EP 3123
DI 10.1109/TMM.2020.3020698
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UU6IS
UT WOS:000698902000013
DA 2024-07-18
ER

PT J
AU Gong, XX
   Liu, YP
   Wu, QY
   Huang, JY
   Zong, H
   Wang, J
AF Gong, Xiaoxi
   Liu, Yuanpeng
   Wu, Qiaoyun
   Huang, Jiayi
   Zong, Hua
   Wang, Jun
TI An Accurate, Robust Visual Odometry and Detail-Preserving Reconstruction
   System
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image reconstruction; Estimation; Cameras; Tracking; Visual odometry;
   Simultaneous localization and mapping; Brightness; Deep convolutional
   neural networks; monocular vision simultaneous localization and mapping;
   visual odometry
ID MONOCULAR SLAM
AB Tracking and mapping functions in a monocular SLAM system remain active due to their challenging nature. In this paper, we propose a novel approach to perform the accurate and robust ego-motion estimation and provide the detail-preserving reconstruction in indoor environments. More specifically, we design a new algorithm called synchronous event measurement (SEM) to create event-based difference images (EDIs) so as to highlight frame-to-frame (F2F) difference. The observation indicates that F2F difference is highly correlated with the camera's motion change. We hereby feed EDIs into a deep convolutional neural network, in order to infer ego-motion of the camera. Subsequently, based on a monocular reconstruction framework (REMODE), we devise an algorithm named event region search or briefly ERS, to reduce possibility of mismatch on the depth estimation stage. Evaluations on a variety of datasets demonstrate the satisfactory performance of our proposed method: the ego-motion estimation is more accurate than some geometric based Visual Odometry (VO) and learning based approaches. The results are robust under extreme situations, such as brightness variation and motion blur. Meanwhile, our approach can provide more precise depth map with relatively rich textural information.
C1 [Gong, Xiaoxi; Liu, Yuanpeng; Wu, Qiaoyun; Wang, Jun] Nanjing Univ Aeronaut & Astronaut, Coll Mech & Elect Engn, Nanjing 210007, Peoples R China.
   [Huang, Jiayi] Shanghai Aerosp Control Technol Inst, Shanghai 201109, Peoples R China.
   [Zong, Hua] Natl Key Lab Sci & Technol, Aerosp Intelligent Control, Beijing 100854, Peoples R China.
C3 Nanjing University of Aeronautics & Astronautics
RP Wang, J (corresponding author), Nanjing Univ Aeronaut & Astronaut, Coll Mech & Elect Engn, Nanjing 210007, Peoples R China.
EM xiaoxigong.nuaa@gmail.com; nuaayp_liu@nuaa.edu.cn;
   wuqiaoyun@nuaa.edu.cn; j_huang_sacti@sina.com; zonghua3@sina.cn;
   junwang@outlook.com
RI Wang, Jun/AAM-6868-2021; Liu, Yuanpeng/HNB-6644-2023
OI Wang, Jun/0000-0001-9223-2615; Gong, Xiaoxi/0000-0002-2607-5081
FU National Natural Science Foundation of China [61772267]; Fundamental
   Research Funds for the Central Universities [NE2014402, NE2016004];
   Nanjing University of Aeronautics and Astronautics Fundamental Research
   Funds [NS2015053]
FX Manuscript received June 19, 2019; revised March 14, 2020 and July 7,
   2020; accepted August 8, 2020. Date of publication August 21, 2020; date
   of current version August 24, 2021. This work was supported in part by
   the National Natural Science Foundation of China under Grant 61772267,
   in part by the Fundamental Research Funds for the Central Universities
   under Grant NE2014402 and Grant NE2016004, and in part by the Nanjing
   University of Aeronautics and Astronautics Fundamental Research Funds
   under Grant NS2015053. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Sen-Ching
   Samson Cheung. (Corresponding author: Jun Wang.)
CR [Anonymous], 2001, COMP SCI W
   Bojarski Mariusz, 2016, arXiv
   Brown R. G., 1997, INTRO RANDOM SIGNALS, V3rd, P335
   Burri M, 2016, INT J ROBOT RES, V35, P1157, DOI 10.1177/0278364915620033
   Calonder M, 2010, LECT NOTES COMPUT SC, V6314, P778, DOI 10.1007/978-3-642-15561-1_56
   Chien SY, 2002, IEEE T CIRC SYST VID, V12, P577, DOI 10.1109/TCSVT.2002.800516
   Civera J, 2008, IEEE T ROBOT, V24, P932, DOI 10.1109/TRO.2008.2003276
   Costante G, 2016, IEEE ROBOT AUTOM LET, V1, P18, DOI 10.1109/LRA.2015.2505717
   Davison AJ, 2007, IEEE T PATTERN ANAL, V29, P1052, DOI 10.1109/TPAMI.2007.1049
   Desa SM, 2004, I C COMP GRAPH IM VI, P41, DOI 10.1109/CGIV.2004.1323958
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Eigen D, 2014, ADV NEUR IN, V27
   Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304
   Engel J, 2014, LECT NOTES COMPUT SC, V8690, P834, DOI 10.1007/978-3-319-10605-2_54
   Engel J, 2013, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2013.183
   Forster C, 2014, IEEE INT CONF ROBOT, P15, DOI 10.1109/ICRA.2014.6906584
   Gallego G, 2017, IEEE ROBOT AUTOM LET, V2, P632, DOI 10.1109/LRA.2016.2647639
   Gálvez-López D, 2012, IEEE T ROBOT, V28, P1188, DOI 10.1109/TRO.2012.2197158
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Handa A, 2014, IEEE INT CONF ROBOT, P1524, DOI 10.1109/ICRA.2014.6907054
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou Y, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON INFORMATION AND AUTOMATION, P2238, DOI 10.1109/ICInfA.2015.7279659
   Kendall A, 2017, PROC CVPR IEEE, P6555, DOI 10.1109/CVPR.2017.694
   Kendall A, 2016, IEEE INT CONF ROBOT, P4762, DOI 10.1109/ICRA.2016.7487679
   Kendall A, 2015, IEEE I CONF COMP VIS, P2938, DOI 10.1109/ICCV.2015.336
   Kim H., 2014, P BRIT MACHINE VISIO, DOI DOI 10.5244/C.28.26
   Klein George, 2007, P1
   KOENDERINK JJ, 1986, VISION RES, V26, P161, DOI 10.1016/0042-6989(86)90078-7
   Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32
   Li RH, 2018, IEEE INT CONF ROBOT, P7286, DOI 10.1109/ICRA.2018.8461251
   Liu HM, 2016, INT SYM MIX AUGMENT, P1, DOI 10.1109/ISMAR.2016.24
   Liu Y, 2019, INT J TOUR RES, V21, P855, DOI 10.1002/jtr.2310
   Lu GY, 2017, IEEE T MULTIMEDIA, V19, P2117, DOI 10.1109/TMM.2017.2731044
   Luo J., 2009, INT J IMAGE PROCESSI, V3, P143, DOI DOI 10.1007/S11270-006-2859-8
   MILFORD M., 2015, PROBLEM MOBILE SENSO
   Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671
   Newcombe RA, 2011, IEEE I CONF COMP VIS, P2320, DOI 10.1109/ICCV.2011.6126513
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178
   Piao JC, 2019, IEEE T MULTIMEDIA, V21, P2827, DOI 10.1109/TMM.2019.2913324
   Pizzoli M, 2014, IEEE INT CONF ROBOT, P2609, DOI 10.1109/ICRA.2014.6907233
   Rebecq H, 2017, IEEE ROBOT AUTOM LET, V2, P593, DOI 10.1109/LRA.2016.2645143
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Saxena A., 2005, ADV NEURAL INFORM PR, V18, P1
   Shen YH, 2011, FRONT COMPUT SCI CHI, V5, P227, DOI 10.1007/s11704-011-9190-2
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song YF, 2016, IEEE T MULTIMEDIA, V18, P1542, DOI 10.1109/TMM.2016.2568743
   Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773
   Tateno K, 2017, PROC CVPR IEEE, P6565, DOI 10.1109/CVPR.2017.695
   Tedaldi D, 2016, 2016 2ND INTERNATIONAL CONFERENCE ON EVENT-BASED CONTROL, COMMUNICATION, AND SIGNAL PROCESSING (EBCCSP), DOI 10.1109/EBCCSP.2016.7605086
   UMEYAMA S, 1991, IEEE T PATTERN ANAL, V13, P376, DOI 10.1109/34.88573
   Vogiatzis G, 2011, IMAGE VISION COMPUT, V29, P434, DOI 10.1016/j.imavis.2011.01.006
   Wang S., 2017, ICRA, DOI [10.1109/icra.2017.7989236, DOI 10.1109/ICRA.2017.7989236]
   Zhang WD, 2017, IEEE T MULTIMEDIA, V19, P935, DOI 10.1109/TMM.2016.2642780
   Zhang Y, 2017, IEEE DECIS CONTR P, DOI 10.1109/CDC.2017.8263985
   Zhang ZC, 2018, IEEE INT C INT ROBOT, P7244, DOI 10.1109/IROS.2018.8593941
   Zhou Huizhong, 2018, P EUR C COMP VIS ECC, P822
   Zhou L., 2018, UNSUPERVISED LEARNIN
NR 58
TC 7
Z9 7
U1 1
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2820
EP 2832
DI 10.1109/TMM.2020.3017886
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UE9PT
UT WOS:000688215600022
DA 2024-07-18
ER

PT J
AU Huang, QY
   Cai, ZC
   Lan, T
AF Huang, Qiuying
   Cai, Zhanchuan
   Lan, Ting
TI A New Approach for Character Recognition of Multi-Style Vehicle License
   Plates
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Licenses; Optical character recognition software; Character recognition;
   Feature extraction; Proposals; Task analysis; Bridges; Hong
   Kong-Zhuhai-Macao Bridge; license plate recognition; feature pyramid
   network; mask region-based convolutional neural network
AB The recognition of vehicle license plate is an important part of the modern intelligent traffic management system, which has been widely used in many fields. On the Hong Kong-Zhuhai-Macao Bridge, the vehicles may have multiple license plates (LPs) with three different styles, and the traditional contour-based vehicle license plate recognition methods cause a considerable miss rate for multi-style license plates. With such a background, this paper proposes a multi-style license plate recognition method based on feature pyramid network with instance segmentation, which translates the license plate recognition into object instance detection and gets rid of the steps of segmentation and optical character recognition of traditional methods. In the scheme, we design a novel license plate recognition network to precisely locate and classify characters and LP regions concurrently, wherein an assembly layer is added for combining the characters into license plates and outputting license plate strings. The experimental results show that the proposed method achieves 98.57% recognition rate of multi-style LPs on the real world applications. Moreover, we also select the standard license plate datasets, that only contain single style license plates, to test the proposed license plate recognition method, and the corresponding results show that the proposed method achieves competitive performance.
C1 [Huang, Qiuying; Cai, Zhanchuan; Lan, Ting] Macau Univ Sci & Technol, Fac Informat Technol, Macau 999078, Peoples R China.
   [Huang, Qiuying] Beijing Normal Univ, Sch Informat Technol, Zhuhai, Peoples R China.
C3 Macau University of Science & Technology; Beijing Normal University;
   Beijing Normal University Zhuhai
RP Cai, ZC (corresponding author), Macau Univ Sci & Technol, Fac Informat Technol, Macau 999078, Peoples R China.
EM jacksonmm@163.com; zccai@must.edu.mo; lantingleo@gmail.com
RI Lan, Ting/GWV-2392-2022
OI LAN, TING/0000-0002-3553-4323
FU Science and Technology Development Fund of Macao [0052/2020/AFJ,
   0038/2020/A, 0025/2019/AKP, 0012/2018/A1, 0069/2018/A2]; Open Project
   Program of the State Key Laboratory of Virtual Reality Technology and
   Systems, Beihang University [VRLAB2019C02]; Open Fund of the State Key
   Laboratory of Remote Sensing Science [OFSLRSS201901]
FX Manuscript received May 29, 2020; revised August 25, 2020; accepted
   October 9, 2020. Date of publication October 14, 2020; date of current
   version October 19, 2021. This work was supported in part by the Science
   and Technology Development Fund of Macao under Grants 0052/2020/AFJ,
   0038/2020/A, 0025/2019/AKP, 0012/2018/A1, and 0069/2018/A2, in part by
   the Open Project Program of the State Key Laboratory of Virtual Reality
   Technology and Systems, BeihangUniversity underGrant VRLAB2019C02, and
   in part by the Open Fund of the State Key Laboratory of Remote Sensing
   Science under Grant OFSLRSS201901. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr.
   Concetto Spampinato. (Corresponding author: Zhanchuan Cai.)
CR Al-Shemarry MS, 2018, EXPERT SYST APPL, V92, P216, DOI 10.1016/j.eswa.2017.09.036
   Gupta A, 2016, PROC CVPR IEEE, P2315, DOI 10.1109/CVPR.2016.254
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Kaiming, 2017, P IEEE INT C COMPUTE
   Henry C, 2020, IEEE ACCESS, V8, P35185, DOI 10.1109/ACCESS.2020.2974973
   Hsu GS, 2013, IEEE T VEH TECHNOL, V62, P552, DOI 10.1109/TVT.2012.2226218
   Huang C, 2016, IEEE T MULTIMEDIA, V18, P2372, DOI 10.1109/TMM.2016.2602060
   Khare V, 2019, EXPERT SYST APPL, V131, P219, DOI 10.1016/j.eswa.2019.04.030
   Laroca R., 2018, P INT JOINT C NEUR N, P1
   Li H., 2016, ARXIV160105610
   Li H, 2019, IEEE T INTELL TRANSP, V20, P1126, DOI 10.1109/TITS.2018.2847291
   Li H, 2018, IMAGE VISION COMPUT, V72, P14, DOI 10.1016/j.imavis.2018.02.002
   Li JA, 2018, IEEE T MULTIMEDIA, V20, P1645, DOI 10.1109/TMM.2017.2772796
   Liu W, 2020, IEEE T IND INFORM, V16, P3997, DOI 10.1109/TII.2019.2936507
   Liu XC, 2018, IEEE T MULTIMEDIA, V20, P645, DOI 10.1109/TMM.2017.2751966
   Liu YJ, 2018, SOFT COMPUT, V22, P2403, DOI 10.1007/s00500-017-2503-0
   Ma JY, 2020, IEEE T IND ELECTRON, V67, P8649, DOI 10.1109/TIE.2019.2950866
   Qiao YL, 2019, COMPUT ELECTRON AGR, V165, DOI 10.1016/j.compag.2019.104958
   Rafique MA, 2018, SOFT COMPUT, V22, P6429, DOI 10.1007/s00500-017-2696-2
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Ren S, 2015, FASTER R CNN REAL TI, P91
   Rodríguez P, 2020, IEEE T MULTIMEDIA, V22, P502, DOI 10.1109/TMM.2019.2928494
   Shi YM, 2017, IEEE T MULTIMEDIA, V19, P1510, DOI 10.1109/TMM.2017.2666540
   Silva SM, 2018, LECT NOTES COMPUT SC, V11216, P593, DOI 10.1007/978-3-030-01258-8_36
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Wang L, 2017, IEEE T MULTIMEDIA, V19, P646, DOI 10.1109/TMM.2016.2617079
   Wu H, 2020, IEEE T COMP PACK MAN, V10, P525, DOI 10.1109/TCPMT.2019.2952393
   Yuan YL, 2017, IEEE T IMAGE PROCESS, V26, P1102, DOI 10.1109/TIP.2016.2631901
   Zhang LJ, 2021, IEEE T INTELL TRANSP, V22, P6967, DOI 10.1109/TITS.2020.3000072
   Zhang M, 2018, J PHYS CONF SER, V1004, DOI 10.1088/1742-6596/1004/1/012022
   Zhang SQ, 2018, IEEE T MULTIMEDIA, V20, P1576, DOI 10.1109/TMM.2017.2766843
   Zhou WG, 2012, IEEE T IMAGE PROCESS, V21, P4269, DOI 10.1109/TIP.2012.2199506
   Zunino R, 2000, IEEE T IND ELECTRON, V47, P159, DOI 10.1109/41.824138
NR 34
TC 11
Z9 12
U1 22
U2 143
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3768
EP 3777
DI 10.1109/TMM.2020.3031074
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA WJ5OB
UT WOS:000709093100026
DA 2024-07-18
ER

PT J
AU Huang, YF
   Qiu, S
   Wang, CB
   Li, CH
AF Huang, Yifei
   Qiu, Sheng
   Wang, Changbo
   Li, Chenhui
TI Learning Representations for High-Dynamic-Range Image Color Transfer in
   a Self-Supervised Way
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image color analysis; Generators; Colored noise; Standards; Generative
   adversarial networks; Gallium nitride; Visualization; Self-supervised
   learning; color transfer; image manipulation; generative adversarial
   network
ID MODEL
AB Reference-based color transfer between images has been a fundamental function in image editing. However, existing approaches pay less attention to high-dynamic-range (HDR) images. It is worth noting that designing an appropriate representation for HDR images to achieve satisfying color transfer is challenging. In this paper, we propose an innovative high-dynamic-range image color transfer generative adversarial network (HDRCTGAN) to encode the original image into fine representations that allow transfer of the color of the reference image to the target image. We propose to learn fine representations through a generative adversarial network (GAN) in a self-supervised way. Particularly, the proposed method is self-supervised learning that requires only unlabeled HDR images instead of supervised learning that requires lots of ground truth pairs. HDRCTGAN consists of a generator to transfer the color of the reference image to the target image over the feature domain and a discriminator to suppress the artifacts caused by the generator. We also design a loss function to ensure that HDRCTGAN possesses two required properties: (a) high fidelity and (b) self-identity. The proposed approach yields a pleasing visual result. We have carried out HDR specific evaluations including both objective quantitative experiments with HDR metrics and subjective user studies operated on HDR display devices to demonstrate the effectiveness of our method. Furthermore, we have verified the applicability of the proposed approach to several applications, such as color transfer of HDR images captured by smartphones, color transfer of fabric images, and reference-based grayscale image colorization.
C1 [Huang, Yifei; Qiu, Sheng] East China Normal Univ, Inst Software Engn, Shanghai 200062, Peoples R China.
   [Wang, Changbo; Li, Chenhui] East China Normal Univ, Sch Comp Sci & Technol, Shanghai 200062, Peoples R China.
C3 East China Normal University; East China Normal University
RP Wang, CB; Li, CH (corresponding author), East China Normal Univ, Sch Comp Sci & Technol, Shanghai 200062, Peoples R China.
EM chli@cs.ecnu.edu.cn; 52164500012@stu.ecnu.edu.cn; cbwang@cs.ecnu.edu.cn;
   chli@sei.ecnu.edu.cn
RI Li, Chenhui/AAR-3682-2020; Huang, Yifei/AAH-5639-2020
OI Li, Chenhui/0000-0001-9835-2650; Wang, Changbo/0000-0001-8940-6418;
   Huang, Yifei/0000-0002-3077-0175
FU National Natural Science Foundation of China [61802128, 61672237,
   61532002]; Natural Science Foundation of Shanghai [19ZR1415800]; Open
   Project Program of State Key Laboratory of Virtual Reality Technology
   and Systems, Beihang University [VRLAB2018B12]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61802128, 61672237, and 61532002, in
   part by the Natural Science Foundation of Shanghai under Grant
   19ZR1415800, and in part by the Open Project Program of State Key
   Laboratory of Virtual Reality Technology and Systems, Beihang University
   (VRLAB2018B12).
CR Arbelot B., 2016, 5 JOINT S COMPUTATIO, P21
   Chang HW, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766978
   Chang Y., 2005, ACM Trans. Appl. Perception, V2, P322
   Chang Y, 2007, IEEE T IMAGE PROCESS, V16, P329, DOI 10.1109/TIP.2006.888347
   Chen C, 2018, PROC CVPR IEEE, P3291, DOI 10.1109/CVPR.2018.00347
   Cheng ZZ, 2015, IEEE I CONF COMP VIS, P415, DOI 10.1109/ICCV.2015.55
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Durand F, 2002, ACM T GRAPHIC, V21, P257, DOI 10.1145/566570.566574
   Eilertsen G, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130816
   Fairchild MD, 2011, PROC SPIE, V7867, DOI 10.1117/12.872075
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Finlayson G, 2019, IEEE T PATTERN ANAL, V41, P20, DOI 10.1109/TPAMI.2017.2760833
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Gong H, 2019, VISUAL COMPUT, V35, P323, DOI 10.1007/s00371-017-1462-x
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   HaCohen Y, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964965
   Han Y, 2017, IEEE T MULTIMEDIA, V19, P80, DOI 10.1109/TMM.2016.2608000
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He MM, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3292482
   He MM, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201365
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hristova H, 2018, PROCEEDINGS OF THE 13TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS, VOL 1: GRAPP, P112, DOI 10.5220/0006610801120121
   Hristova H, 2018, IEEE T VIS COMPUT GR, V24, P2813, DOI 10.1109/TVCG.2017.2769050
   Hristova H, 2017, IEEE IMAGE PROC, P1237, DOI 10.1109/ICIP.2017.8296479
   Hristova H, 2015, PROC SPIE, V9599, DOI 10.1117/12.2186774
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Huang YF, 2018, COMPUT GRAPH FORUM, V37, P421, DOI 10.1111/cgf.13579
   Hwang Y, 2014, PROC CVPR IEEE, P3342, DOI 10.1109/CVPR.2014.427
   Iizuka S, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925974
   Kim HR, 2016, COMPUT GRAPH FORUM, V35, P209, DOI 10.1111/cgf.13018
   Kingma D. P., 2014, arXiv
   Kundu D, 2017, IEEE T IMAGE PROCESS, V26, P2957, DOI 10.1109/TIP.2017.2685941
   Li YJ, 2018, LECT NOTES COMPUT SC, V11207, P468, DOI 10.1007/978-3-030-01219-9_28
   Li YJ, 2017, ADV NEUR IN, V30
   Liao J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073683
   Liu YM, 2014, COMPUT GRAPH FORUM, V33, P21, DOI 10.1111/cgf.12409
   Luan FJ, 2017, PROC CVPR IEEE, P6997, DOI 10.1109/CVPR.2017.740
   Mahendran A, 2015, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR.2015.7299155
   McTaggart G., 2006, P ACM SIGGRAPH COURS, P7
   Nguyen RMH, 2014, COMPUT GRAPH FORUM, V33, P319, DOI 10.1111/cgf.12500
   Pitié F, 2005, IEEE I CONF COMP VIS, P1434
   Pouli T, 2011, COMPUT GRAPH-UK, V35, P67, DOI 10.1016/j.cag.2010.11.003
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Rensink RA, 2000, VIS COGN, V7, P17, DOI 10.1080/135062800394667
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Su Z, 2018, NEUROCOMPUTING, V311, P305, DOI 10.1016/j.neucom.2018.05.082
   Su Z, 2014, IEEE T MULTIMEDIA, V16, P988, DOI 10.1109/TMM.2014.2305914
   Sun IY, 2018, PALG S RACE ETHN IND, P1, DOI 10.1007/978-1-349-95807-8
   Tai YW, 2007, IEEE T PATTERN ANAL, V29, P1520, DOI 10.1109/TPAMI.2007.1168
   Tan JC, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275054
   Wang BY, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964959
   Wang BY, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866172
   Wang D, 2017, COMPUT GRAPH FORUM, V36, P93, DOI 10.1111/cgf.13275
   Wu HK, 2018, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2018.00197
   Xiao XZ, 2009, COMPUT GRAPH FORUM, V28, P1879, DOI 10.1111/j.1467-8659.2009.01566.x
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang Q, 2017, IEEE T IMAGE PROCESS, V26, P1952, DOI 10.1109/TIP.2017.2671779
   Zhang R, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073703
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 61
TC 21
Z9 23
U1 1
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 176
EP 188
DI 10.1109/TMM.2020.2981994
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA PJ6LW
UT WOS:000601877600014
DA 2024-07-18
ER

PT J
AU Li, ZP
   Xu, QQ
   Jiang, YBY
   Ma, K
   Cao, XC
   Huang, QM
AF Li, Zhaopeng
   Xu, Qianqian
   Jiang, Yangbangyan
   Ma, Ke
   Cao, Xiaochun
   Huang, Qingming
TI Neural Collaborative Preference Learning With Pairwise Comparisons
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Recommender system; collaborative ranking; neural networks; preference
   ranking
ID RECOMMENDATION; NETWORK
AB Collaborative Ranking (CR), as an effective recommendation framework, has attracted increasing attention in recent years. Most CR methods simply adopt the inner product between user/item embeddings as the rating score function, with an assumption that the interacted items are preferred to non-interacted ones. However, such fixed score functions and assumptions might not be sufficient to capture the real preference ranking list from the complicated interactions in real-world data. To alleviate this issue, we develop a novel collaborative ranking framework that learns an arbitrary utility function for item ranking with user preference concerned. In the core of our framework, a neural network is employed to model the utility function for personalized ranking with the strength of its nonlinearity. On top of this, we further adopt a pairwise ranking loss for user-item pairs to preserve the preference order of items for users. Besides, such a utility function enables us to generate the final top-K preference list in a much easier way. Finally, extensive experiments on four real-world datasets show the validity of our proposed method.
C1 [Li, Zhaopeng] Chinese Acad Sci, Inst Informat Engn, State Key Lab Informat Secur, SKLOIS, Beijing, Peoples R China.
   [Li, Zhaopeng; Jiang, Yangbangyan] Chinese Acad Sci, Inst Informat Engn, State Key Lab Informat Secur SKLOIS, Beijing 100093, Peoples R China.
   [Li, Zhaopeng; Jiang, Yangbangyan; Cao, Xiaochun] Univ Chinese Acad Sci, Sch Cyber Secur, Beijing 100049, Peoples R China.
   [Xu, Qianqian; Huang, Qingming] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
   [Ma, Ke; Huang, Qingming] Univ Chinese Acad Sci, Sch Comp Sci & Technol, Beijing 100049, Peoples R China.
   [Ma, Ke; Cao, Xiaochun] Peng Cheng Lab, Artificial Intelligence Res Ctr, Shenzhen 518055, Peoples R China.
   [Cao, Xiaochun] Chinese Acad Sci, Inst Informat Engn, Key Lab Informat Secur, Beijing 100093, Peoples R China.
   [Huang, Qingming] Univ Chinese Acad Sci, Key Lab Big Data Min & Knowledge Management BDKM, Beijing 101408, Peoples R China.
   [Huang, Qingming] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
C3 Chinese Academy of Sciences; Chinese Academy of Sciences; Institute of
   Information Engineering, CAS; Chinese Academy of Sciences; University of
   Chinese Academy of Sciences, CAS; Chinese Academy of Sciences; Institute
   of Computing Technology, CAS; Chinese Academy of Sciences; University of
   Chinese Academy of Sciences, CAS; Peng Cheng Laboratory; Chinese Academy
   of Sciences; Institute of Information Engineering, CAS; Chinese Academy
   of Sciences; University of Chinese Academy of Sciences, CAS; Peng Cheng
   Laboratory
RP Huang, QM (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.; Huang, QM (corresponding author), Univ Chinese Acad Sci, Sch Comp Sci & Technol, Beijing 100049, Peoples R China.; Huang, QM (corresponding author), Univ Chinese Acad Sci, Key Lab Big Data Min & Knowledge Management BDKM, Beijing 101408, Peoples R China.; Huang, QM (corresponding author), Peng Cheng Lab, Shenzhen 518055, Peoples R China.
EM lizhaopeng@iie.ac.cn; xuqianqian@ict.ac.cn; jiangyangbangyan@iie.ac.cn;
   make@ucas.ac.cn; caoxiaochun@iie.ac.cn; qmhuang@ucas.ac.cn
OI Jiang, Yangbangyan/0000-0002-0148-8306
FU National Key R&D Program of China [2018AAA0102003]; National Natural
   Science Foundation of China [61620106009, U1636214, 61931008, 61836002,
   61971016, U1936208, 61672514, 61976202]; Key Research Program of
   Frontier Sciences, CAS [QYZDJ-SSW-SYS013]; Beijing Education Committee
   Cooperation Beijing Natural Science Foundation [KZ201910005007];
   Strategic Priority Research Program of Chinese Academy of Sciences
   [XDB28000000]; Beijing Natural Science Foundation [4182079]; Youth
   Innovation Promotion Association CAS
FX Thisworkwas supported in part by theNationalKeyR&DProgram of China
   underGrant 2018AAA0102003, in part by NationalNatural Science Foundation
   of China underGrants 61620106009, U1636214, 61931008, 61836002,
   61971016, U1936208, 61672514, and 61976202, in part by Key Research
   Program of Frontier Sciences, CAS, under Grant QYZDJ-SSW-SYS013, in part
   by Beijing Education Committee Cooperation Beijing Natural Science
   Foundation under Grant KZ201910005007, in part by the Strategic Priority
   Research Program of Chinese Academy of Sciences, under Grant
   XDB28000000, in part by Beijing Natural Science Foundation under Grant
   4182079, and in part by Youth Innovation Promotion Association CAS. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Wen-Huang Cheng.
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   [Anonymous], 2012, Advances in Neural Information Processing Systems
   Balakrishnan Suhrid, 2012, P 5 ACM INT C WEB SE, P143, DOI DOI 10.1145/2124295.2124314
   Bennett J., 2007, P KDD CUP WORKSHOP, P35
   Bobadilla J, 2009, KNOWL-BASED SYST, V22, P261, DOI 10.1016/j.knosys.2009.01.008
   Cantador I., 2011, P 5 ACM C REC SYST
   Glorot X., 2011, JMLR Proceedings, V15, P315, DOI DOI 10.1002/ECS2.1832
   Hacker S, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P1207
   HANLEY JA, 1982, RADIOLOGY, V143, P29, DOI 10.1148/radiology.143.1.7063747
   Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872
   He RN, 2017, PROCEEDINGS OF THE ELEVENTH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'17), P161, DOI 10.1145/3109859.3109882
   He XN, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2227
   He XN, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P173, DOI 10.1145/3038912.3052569
   Hofmann T, 2004, ACM T INFORM SYST, V22, P89, DOI 10.1145/963770.963774
   Hsieh CK, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P193, DOI 10.1145/3038912.3052639
   Hu J, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P1321, DOI 10.1145/3038912.3052685
   Hu YF, 2008, IEEE DATA MINING, P263, DOI 10.1109/ICDM.2008.22
   Huang SR, 2017, IEEE T MULTIMEDIA, V19, P1314, DOI 10.1109/TMM.2017.2652074
   Järvelin K, 2002, ACM T INFORM SYST, V20, P422, DOI 10.1145/582415.582418
   Keqiang Wang, 2016, Database Systems for Advanced Applications. 21st International Conference, DASFAA 2016. Proceedings: LNCS 9642, P381, DOI 10.1007/978-3-319-32025-0_24
   Khoshgoftaar T.M., 2009, ADV ARTIFICIAL INTEL, DOI [10.1155/2009/421425, DOI 10.1155/2009/421425]
   Kingma D. P., 2014, arXiv
   Konstan J., 2001, P 10 INT C WORLD WID, P285
   Koren Y., 2008, P 14 ACM SIGKDD INT, P426
   Le B. H., 2016, J INF PROCESS, V24, P314
   Lee J., 2013, INT C MACH LEARN, P82
   Lee J, 2014, WWW'14: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P85, DOI 10.1145/2566486.2567970
   Lee J, 2012, J MACH LEARN RES, V13, P2699
   Liang DW, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P689, DOI 10.1145/3178876.3186150
   Liu CY, 2018, AAAI CONF ARTIF INTE, P346
   Liu Y, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3463
   Nguyen TV, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P63, DOI 10.1145/2600428.2609623
   Niu W, 2018, WSDM'18: PROCEEDINGS OF THE ELEVENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P423, DOI 10.1145/3159652.3159728
   Park D, 2015, PR MACH LEARN RES, V37, P1907
   Paterek A., 2007, P KDD CUP WORKSH, P5, DOI DOI 10.1145/1557019.1557072
   Qiao Z, 2014, IEEE DATA MINING, P520, DOI 10.1109/ICDM.2014.43
   Rendle Steffen, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P995, DOI 10.1109/ICDM.2010.127
   Rendle S., 2009, P 25 C UNCERTAINTY A, P452
   Ricci F, 2011, RECOMMENDER SYSTEMS HANDBOOK, P1, DOI 10.1007/978-0-387-85820-3_1
   Sedhain S, 2015, WWW'15 COMPANION: PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P111, DOI 10.1145/2740908.2742726
   Shi Y., 2013, IJCAI 2013, Proceedings of the 23rd International Joint Conference on Artificial Intelligence, Beijing, China, August 3-9, 2013, P3077
   Song B, 2018, CIKM'18: PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P1353, DOI 10.1145/3269206.3271715
   Sun LF, 2017, IEEE T MULTIMEDIA, V19, P609, DOI 10.1109/TMM.2016.2635589
   Sun Z, 2019, ELECTRON COMMER R A, V37, DOI 10.1016/j.elerap.2019.100879
   Tay Y, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P729, DOI 10.1145/3178876.3186154
   Wang J, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2776
   Wang J, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P515, DOI 10.1145/3077136.3080786
   Wang X, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P165, DOI 10.1145/3331184.3331267
   Weimer M., 2008, Advances in neural information processing systems, P1593
   Wu LW, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P515, DOI 10.1145/3097983.3098071
   Xiao T, 2019, AAAI CONF ARTIF INTE, P5474
   Xu ZX, 2017, IEEE T MULTIMEDIA, V19, P1933, DOI 10.1109/TMM.2017.2688928
   Yang JM, 2009, KNOWL-BASED SYST, V22, P105, DOI 10.1016/j.knosys.2008.07.004
   Yang YJ, 2018, IEEE T MULTIMEDIA, V20, P1888, DOI 10.1109/TMM.2017.2779043
   Yi J., 2013, P C HUM COMP CROWDS
   Zhang S, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3669
   Zhao GS, 2019, IEEE T MULTIMEDIA, V21, P771, DOI 10.1109/TMM.2018.2863598
   Zhao Z, 2018, IEEE T MULTIMEDIA, V20, P430, DOI 10.1109/TMM.2017.2740022
   Zhou YP, 2018, IEEE T MULTIMEDIA, V20, P2153, DOI 10.1109/TMM.2017.2781364
   Zhu QN, 2019, AAAI CONF ARTIF INTE, P5973
NR 60
TC 1
Z9 1
U1 3
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1977
EP 1989
DI 10.1109/TMM.2020.3006373
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XG0UQ
UT WOS:000724477100005
DA 2024-07-18
ER

PT J
AU Li, Z
   Hu, HM
   Zhang, W
   Pu, SL
   Li, B
AF Li, Zhuo
   Hu, Hai-Miao
   Zhang, Wei
   Pu, Shiliang
   Li, Bo
TI Spectrum Characteristics Preserved Visible and Near-Infrared Image
   Fusion Algorithm
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image fusion; Indexes; Image Fusion; near infrared; spectrum
   characteristic; color distortion; image enhancement
ID ENHANCEMENT; PERFORMANCE; INFORMATION; NETWORK
AB The visible and near-infrared images fusion aims at utilizing their spectrum characteristics to enhance visibility. However, the current visible and near-infrared fusion algorithms cannot well preserve spectrum characteristics, which results in color distortion and halo artifacts. Therefore, this paper proposes a new visible and near infrared images fusion algorithm by fully considering their different reflection and scattering characteristics. According to image degradation model, the reflection weight model and the transmission weight model are established, respectively. The reflection weight model is established by calculating the difference between the visible (red, green, and blue) spectra and the near-infrared spectrum, while maintaining the correlation of the visible spectra. The proposed reflection weight model can preserve the original reflection characteristic of objects in natural scenes. On the other hand, the transmission weight model is explicitly proposed by calculating the gradient ratio of the visible spectra to the near-infrared spectrum. The proposed transmission weight model intends to make full use of the strong transmission performance of the near-infrared spectrum, which can complement the details loss of the visible spectra caused by light scattering. Moreover, the fused image based on two models is further enhanced according to the reflection characteristics of near-infrared spectrum in case of the non-uniform illumination. The experimental results demonstrate that the proposed algorithm can not only well preserve spectrum characteristics, but also avoid color distortion while maintaining the naturalness, which outperforms the state-of-the-art.
C1 [Li, Zhuo; Hu, Hai-Miao; Zhang, Wei; Li, Bo] Beihang Univ, Sch Comp Sci & Engn, Beijing, Peoples R China.
   [Pu, Shiliang] Hangzhou Hikvis Digital Technol Co Ltd, Hangzhou 310000, Peoples R China.
C3 Beihang University; Hangzhou Hikvision Digital Technology Co., Ltd.
RP Hu, HM (corresponding author), Beihang Univ, Sch Comp Sci & Engn, Beijing, Peoples R China.
EM 18811763083@163.com; frank0139@163.com; 1622993630@qq.com;
   pushiliang@hikvision.com; boli@buaa.edu.cn
RI Li, bo/IWL-9318-2023; Li, Ye/JBS-2949-2023; Li, Kun/JLL-6505-2023
OI Li, Kun/0000-0002-3638-2974
FU National Key Research and Development Program [2020AAA0130200]; National
   Natural Science Foundation of China [61772058, 61421003]; Fundamental
   Research Funds for the Central Universities
FX This work was supported in part by the National Key Research and
   Development Program under Grant 2020AAA0130200, in part by the National
   Natural Science Foundation of China (Nos. 61772058 and 61421003), and in
   part by the Fundamental Research Funds for the Central Universities. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Guo-Jun Qi.
CR Alparone L, 2008, PHOTOGRAMM ENG REM S, V74, P193, DOI 10.14358/PERS.74.2.193
   [Anonymous], 2014, IEEE International Conference on Computational Photography (ICCP)
   [Anonymous], 2011, P IEEE APPL IM PATT
   Bernal EA, 2018, IEEE T MULTIMEDIA, V20, P107, DOI 10.1109/TMM.2017.2726187
   Bhatnagar G, 2013, IEEE T MULTIMEDIA, V15, P1014, DOI 10.1109/TMM.2013.2244870
   Brown M, 2011, PROC CVPR IEEE, P177, DOI 10.1109/CVPR.2011.5995637
   Chang SY, 2014, IEEE DATA MINING, P60, DOI 10.1109/ICDM.2014.115
   Chang SY, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P119, DOI 10.1145/2783258.2783296
   Chen QD, 2019, IEEE ACCESS, V7, P24333, DOI 10.1109/ACCESS.2019.2897213
   Feng C, 2013, IEEE IMAGE PROC, P2363, DOI 10.1109/ICIP.2013.6738487
   Fredembach C., 2008, COL IM C, P176
   Gao BC, 1996, REMOTE SENS ENVIRON, V58, P257, DOI 10.1016/S0034-4257(96)00067-3
   Gao YY, 2018, IEEE T MULTIMEDIA, V20, P335, DOI 10.1109/TMM.2017.2740025
   Ge SM, 2020, IEEE T CIRC SYST VID, V30, P3387, DOI 10.1109/TCSVT.2020.2967754
   Ge SM, 2020, IEEE T IMAGE PROCESS, V29, P2610, DOI 10.1109/TIP.2019.2950508
   Ge SM, 2019, IEEE T IMAGE PROCESS, V28, P2051, DOI 10.1109/TIP.2018.2883743
   Gijsenij A, 2011, IEEE T IMAGE PROCESS, V20, P2475, DOI 10.1109/TIP.2011.2118224
   Han Y, 2013, INFORM FUSION, V14, P127, DOI 10.1016/j.inffus.2011.08.002
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Hu HM, 2017, IEEE T MULTIMEDIA, V19, P2706, DOI 10.1109/TMM.2017.2711422
   Jang DW, 2017, IET IMAGE PROCESS, V11, P587, DOI 10.1049/iet-ipr.2017.0192
   Jiang JH, 2019, IEEE ACCESS, V7, P20607, DOI 10.1109/ACCESS.2019.2896128
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P451, DOI 10.1109/83.557356
   Kriegler F. J., 1969, REMOTE SENS ENV, V6
   Kumar B.K.S., 2015, SIGNAL IMAGE VIDEO P, V9, P1193, DOI DOI 10.1007/S11760-013-0556-9
   LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001
   LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108
   Li ST, 2013, IEEE T IMAGE PROCESS, V22, P2864, DOI 10.1109/TIP.2013.2244222
   Liu ZC, 2001, COMP GRAPH, P271
   Petschnigg G, 2004, ACM T GRAPHIC, V23, P664, DOI 10.1145/1015706.1015777
   Qi G.-J., 2012, CLUSTERING HETEROGEN
   Qi GJ, 2017, IEEE T PATTERN ANAL, V39, P1360, DOI 10.1109/TPAMI.2016.2587643
   Qi GJ, 2013, PROC INT CONF DATA, P793, DOI 10.1109/ICDE.2013.6544875
   Qi GJ, 2012, IEEE T PATTERN ANAL, V34, P850, DOI 10.1109/TPAMI.2011.191
   Salamati N., 2010, P 18 COL IM C
   Salamati N, 2011, PROC SPIE, V7865, DOI 10.1117/12.876494
   Sappa AD, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16060861
   Schaul L., 2010, P IEEE INT C IM PROC
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Shu X., 2015, P 23 ACM INT C MULT, P25
   Son CH, 2018, IEEE T CIRC SYST VID, V28, P3111, DOI 10.1109/TCSVT.2017.2748150
   Son CH, 2017, IEEE IMAGE PROC, P530, DOI 10.1109/ICIP.2017.8296337
   Son CH, 2017, IEEE T IMAGE PROCESS, V26, P5381, DOI 10.1109/TIP.2017.2724241
   Tang J, 2016, J CHINA TOUR RES, V12, P5, DOI 10.1080/19388160.2016.1165780
   Tapia J, 2017, 2017 IEEE INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB), P805, DOI 10.1109/BTAS.2017.8272774
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Valada A., 2016, P INT S EXP ROB
   Vanmali A.V., 2015, NAT CONF COMPUT VIS, P1
   Vanmali AV, 2017, SADHANA-ACAD P ENG S, V42, P1063, DOI 10.1007/s12046-017-0673-1
   Wang JD, 2012, INFORM RETRIEVAL, V15, P278, DOI 10.1007/s10791-012-9193-0
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wang Z, 2002, IEEE SIGNAL PROC LET, V9, P81, DOI 10.1109/97.995823
   Wu L., 2019, FRONTIERS COMPUT SCI, V13
   Xydeas CS, 2000, ELECTRON LETT, V36, P308, DOI 10.1049/el:20000267
   Yan Q, 2013, IEEE I CONF COMP VIS, P1537, DOI 10.1109/ICCV.2013.194
   Zhang J., 2016, P IEEE INT C MULT EX
   Zhao WD, 2018, IEEE T MULTIMEDIA, V20, P866, DOI 10.1109/TMM.2017.2760100
   Zhou T, 2018, ESTIMATION AND CONTROL OF LARGE-SCALE NETWORKED SYSTEMS, P321, DOI 10.1016/B978-0-12-805311-9.00010-5
NR 59
TC 27
Z9 31
U1 9
U2 81
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 306
EP 319
DI 10.1109/TMM.2020.2978640
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA PJ6LW
UT WOS:000601877600024
DA 2024-07-18
ER

PT J
AU Ling, SY
   Li, J
   Che, ZH
   Zhou, W
   Wang, JL
   Le Callet, P
AF Ling, Suiyi
   Li, Jing
   Che, Zhaohui
   Zhou, Wei
   Wang, Junle
   Le Callet, Patrick
TI Re-Visiting Discriminator for Blind Free-Viewpoint Image Quality
   Assessment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Measurement; Rendering (computer graphics); Distortion; Feature
   extraction; Quality assessment; Generators; Two dimensional displays;
   Generative adversarial networks; no-reference quality assessment;
   depth-image-based-rendering; free-viewpoint navigation; non-uniform
   structure-related distortions
ID VIEW SYNTHESIS; NETWORKS; DIBR
AB Accurate measurement of perceptual quality is important for various immersive multimedia, which demand real-time quality control or quality-based bench-marking for relevant algorithms. For instance, virtual views rendering in Free-Viewpoint (FV) navigation scenarios is a typical case that introduces challenging distortions, particularly the ones around dis-occluded regions. Existing quality metrics, most of which are targeting for impairments caused by compression or network condition, fail to quantify such non-uniform structure-related distortions. Moreover, the lack of quality databases for such distortions makes it even more challenging to develop robust quality metrics. In this work, a Generative Adversarial Networks based No-Reference (NR) quality Metric, namely GANs-NRM, is proposed. We first present an approach to create masks mimicking dis-occlusions/textureless regions, which is applicable on large-scale 2D image databases publicly available in the computer vision domain. Using these synthetic data, we then train a GANs-based context renderer with the capability of rendering those masked regions. Since the naturalness of the rendered dis-occluded regions strongly relates to the perceptual quality, we assume that the discriminator of the trained GANs has an intrinsic ability for quality assessment. We thus use the features extracted from the discriminator to learn a Bag-of-Distortion-Word (BDW) codebook. We show that a quality predictor can be then well trained using only a small amount of subjective quality data for the FV views rendering. Moreover, in the proposed framework, the discriminator is also adapted as a distortion-detector to locate possible distorted regions. According to the experimental results, the proposed model outperforms significantly the state-of-the-art quality metrics. The corresponding context renderer also shows appealing visualized results over other rendering algorithms.
C1 [Ling, Suiyi; Le Callet, Patrick] Univ Nantes, Lab Sci Numer Nantes, Equipe Image Percept & Interact, Nantes, France.
   [Li, Jing] Alibaba Grp, Moku Lab, Beijing 100016, Peoples R China.
   [Che, Zhaohui] Shanghai Jiao Tong Univ, Dept Elect Engn, Shanghai 200240, Peoples R China.
   [Zhou, Wei] Tencent, Turing Lab, Shenzhen 518057, Peoples R China.
   [Wang, Junle] Univ Sci & Technol China, Dept EEIS, Hefei 230026, Peoples R China.
C3 Nantes Universite; Alibaba Group; Shanghai Jiao Tong University;
   Tencent; Chinese Academy of Sciences; University of Science & Technology
   of China, CAS
RP Li, J (corresponding author), Alibaba Grp, Moku Lab, Beijing 100016, Peoples R China.
EM suiyi.ling@univ-nantes.fr; jing.li.univ@gmail.com;
   chezhaohui@sjtu.edu.cn; weichou@mail.ustc.edu.cn; wangjunle@gmail.com;
   patrick.lecallet@univ-nantes.fr
OI Ling, suiyi/0000-0002-5306-9189; Zhou, Wei/0000-0003-3641-1429; Le
   callet, Patrick/0000-0002-2143-7063
CR Achanta R., 2010, EPFL Technical Report 149300, V6, P15
   Ahn I, 2013, IEEE T BROADCAST, V59, P614, DOI 10.1109/TBC.2013.2281658
   [Anonymous], 2016, ITU-T Recommendation D.261: Regulatory principles for market definition and identification of operators with significant market power - SMP
   [Anonymous], 2015, P IEEE TRUE VIS CAPT
   [Anonymous], 2019, ARXIV190312107
   [Anonymous], 2003, P IIASTED VIIP
   [Anonymous], 2013, International Scholarly Research Notices., DOI DOI 10.1155/2013/905685
   Battisti F, 2015, SIGNAL PROCESS-IMAGE, V30, P78, DOI 10.1016/j.image.2014.10.005
   Bianco S, 2018, SIGNAL IMAGE VIDEO P, V12, P355, DOI 10.1007/s11760-017-1166-8
   Bosc E, 2011, IEEE J-STSP, V5, P1332, DOI 10.1109/JSTSP.2011.2166245
   Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Bosse S, 2016, IEEE IMAGE PROC, P3773, DOI 10.1109/ICIP.2016.7533065
   Boutsidis C, 2015, IEEE T INFORM THEORY, V61, P1045, DOI 10.1109/TIT.2014.2375327
   Conze PH, 2012, PROC SPIE, V8288, DOI 10.1117/12.908762
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Fehn C, 2004, PROC SPIE, V5291, P93, DOI 10.1117/12.524762
   Gastaldo P, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-54
   Goodfellow I., 2014, PROC NEURIPS, P2672, DOI DOI 10.1145/3422622
   Gu K, 2018, IEEE T IMAGE PROCESS, V27, P394, DOI 10.1109/TIP.2017.2733164
   Hedman P, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275084
   Huynh-Thu Q, 2008, ELECTRON LETT, V44, P800, DOI 10.1049/el:20080522
   Jantet V, 2011, IEEE IMAGE PROC, P125, DOI 10.1109/ICIP.2011.6115662
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Jung YJ, 2011, OPT EXPRESS, V19, P7325, DOI 10.1364/OE.19.007325
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Kinga D., 2015, C TRACK P, P5
   Kiran A.V., 2017, P IEEE C COMP VIS PA, P58
   Köppel M, 2010, IEEE IMAGE PROC, P1809, DOI 10.1109/ICIP.2010.5652138
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Li DQ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P378, DOI 10.1145/3123266.3123322
   Li J., 2013, THESIS U NANTES
   Li LD, 2018, IEEE T MULTIMEDIA, V20, P914, DOI 10.1109/TMM.2017.2760062
   Lin KY, 2018, PROC CVPR IEEE, P732, DOI 10.1109/CVPR.2018.00083
   Ling SY, 2019, IEEE J EM SEL TOP C, V9, P204, DOI 10.1109/JETCAS.2019.2893484
   Ling SY, 2018, IEEE INT CON MULTI
   Ling SY, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1157, DOI 10.1145/3123266.3123329
   Ling SY, 2018, IEEE IMAGE PROC, P286, DOI 10.1109/ICIP.2018.8451151
   Ling SY, 2017, IEEE INT WORKSH MULT
   Ling SY, 2017, IEEE INT CON MULTI, P79, DOI 10.1109/ICME.2017.8019431
   Luo GB, 2016, PROC CVPR IEEE, P1781, DOI 10.1109/CVPR.2016.197
   Mori Y., 2008, JTC1SC29WG11 ISO IEC
   Mori Y, 2009, SIGNAL PROCESS-IMAGE, V24, P65, DOI 10.1016/j.image.2008.10.013
   Müller K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/438148
   Muja M, 2014, IEEE T PATTERN ANAL, V36, P2227, DOI 10.1109/TPAMI.2014.2321376
   Ndjiki-Nya P, 2011, IEEE T MULTIMEDIA, V13, P453, DOI 10.1109/TMM.2011.2128862
   Ninassi A, 2007, IEEE IMAGE PROC, P733
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Patney A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980246
   Radford A., 2015, ARXIV151106434
   Ruan L. Y., 2018, P 39 ANN EUROPEAN AS, P19
   Sandic-Stankovic D., 2017, EURASIP J IMAGE VIDE, V2016, P1211
   Sandic-Stankovic D., 2015, P IEEE 7 INT WORKSH, P1
   Sandic-Stankovic DD, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2919416
   Solh M, 2012, IEEE J-STSP, V6, P495, DOI 10.1109/JSTSP.2012.2204723
   Tanimoto M, 2011, IEEE SIGNAL PROC MAG, V28, P67, DOI 10.1109/MSP.2010.939077
   Telea A., 2004, Journal of Graphics Tools, V9, P23, DOI 10.1080/10867651.2004.10487596
   Tian SS, 2019, IEEE T MULTIMEDIA, V21, P1235, DOI 10.1109/TMM.2018.2875307
   Tian SS, 2018, IEEE T IMAGE PROCESS, V27, P1652, DOI 10.1109/TIP.2017.2781420
   Tian SS, 2017, INT CONF ACOUST SPEE, P1248, DOI 10.1109/ICASSP.2017.7952356
   Wang CY, 2018, IEEE T IMAGE PROCESS, V27, P4066, DOI 10.1109/TIP.2018.2836316
   Wang GC, 2020, IEEE T IMAGE PROCESS, V29, P1802, DOI 10.1109/TIP.2019.2945675
   Wang W., 2017, P IEEE INT C COMP VI, P2298, DOI DOI 10.48550/ARXIV.1711.06375
   Wolski K, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3196493
   Xu J., 2019, ARXIV190901738
   Yan ZY, 2018, LECT NOTES COMPUT SC, V11218, P3, DOI 10.1007/978-3-030-01264-9_1
   Yang B, 2019, IEEE T PATTERN ANAL, V41, P2820, DOI 10.1109/TPAMI.2018.2868195
   Yeh RA, 2017, PROC CVPR IEEE, P6882, DOI 10.1109/CVPR.2017.728
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Zhang H, 2019, IEEE T PATTERN ANAL, V41, P1947, DOI 10.1109/TPAMI.2018.2856256
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
   Zhou W, 2020, IEEE T IMAGE PROCESS, V29, P4070, DOI 10.1109/TIP.2020.2969777
   Zhou W, 2019, IEEE T IMAGE PROCESS, V28, P3946, DOI 10.1109/TIP.2019.2902831
   Zhu C, 2016, IEEE T BROADCAST, V62, P82, DOI 10.1109/TBC.2015.2475697
NR 75
TC 13
Z9 14
U1 2
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 4245
EP 4258
DI 10.1109/TMM.2020.3038305
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XA2YU
UT WOS:000720519900026
OA Green Published
DA 2024-07-18
ER

PT J
AU Liu, X
   Zhao, GY
AF Liu, Xin
   Zhao, Guoying
TI 3D Skeletal Gesture Recognition via Discriminative Coding on
   Time-Warping Invariant Riemannian Trajectories
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D skeleton representation; gesture recognition; Riemannian geometry;
   sparse coding; time-warping invariant feature
ID ACTIONLET ENSEMBLE; K-SVD; POSE
AB Learning 3D skeleton-based representation for gesture recognition has progressively stood out because of its invariance to the viewpoint and background dynamics of video. Typically, existing techniques use absolute coordinates to determine human motion features. The recognition of gestures, however, is irrespective of the position of the performer, and the extracted features should be invariant to body size. In addition, when comparing and classifying gestures, the problem of temporal dynamics can greatly distort the distance metric. In this paper, we represent a 3D skeleton as a point in the special orthogonal group SO(3) product space that expressly models the 3D geometric relationships between body parts. As such, a gesture skeletal sequence can be described by a trajectory on a Riemannian manifold. Following that, we propose to generalize the transported square-root vector field to obtain a time-warping invariant metric for comparing these trajectories (identifying these gestures). Moreover, by specifically considering the labeling information with encoding, a sparse coding scheme of skeletal trajectories is presented to enforce the discriminant validity of atoms in the dictionary. Experimental results indicate that the proposed approach has achieved state-of-the-art performance on many challenging gesture recognition benchmarks.
C1 [Liu, Xin] Tianjin Univ, Sch Elect & Informat Engn, Tianjin, Peoples R China.
   [Liu, Xin; Zhao, Guoying] Univ Oulu, Ctr Machine Vis & Signal Anal, FI-90014 Oulu, Finland.
   [Zhao, Guoying] Northwest Univ, Sch Informat & Technol, Xian, Peoples R China.
C3 Tianjin University; University of Oulu; Northwest University Xi'an
RP Zhao, GY (corresponding author), Univ Oulu, Ctr Machine Vis & Signal Anal, FI-90014 Oulu, Finland.; Zhao, GY (corresponding author), Northwest Univ, Sch Informat & Technol, Xian, Peoples R China.
EM linuxsino@gmail.com; guoying.zhao@oulu.fi
RI Liu, Xin/AAD-5166-2019; Zhao, Guoying/ABE-7716-2020
OI Liu, Xin/0000-0002-2242-6139; Zhao, Guoying/0000-0003-3694-206X
FU ICT 2023 Project [328115]; strategic Funds of the University of Oulu;
   Infotech Oulu, Finland; Project MiGA [316765]
FX Manuscript received July 16, 2019; revisedMarch 22, 2020 and June 2,
   2020; accepted June 7, 2020. Date of publication June 22, 2020; date of
   current version June 25, 2021. This work was supported in part by
   Project MiGA under Grant 316765, in part by ICT 2023 Project under Grant
   328115, and in part by the strategic Funds of the University of Oulu,
   the Infotech Oulu, Finland. The associate editor coordinating the review
   of this manuscript and approving it for publication was Prof. Andrew D.
   Bagdanov.
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   Anirudh R, 2015, PROC CVPR IEEE, P3147, DOI 10.1109/CVPR.2015.7298934
   [Anonymous], 1997, NEURAL COMPUT
   Bauer M, 2014, J MATH IMAGING VIS, V50, P60, DOI 10.1007/s10851-013-0490-z
   Ben Amor B, 2016, IEEE T PATTERN ANAL, V38, P1, DOI 10.1109/TPAMI.2015.2439257
   Caetano C, 2019, 2019 16TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), DOI 10.1109/avss.2019.8909840
   Cai XY, 2016, IEEE T MULTIMEDIA, V18, P141, DOI 10.1109/TMM.2015.2505089
   Devanne M, 2015, IEEE T CYBERNETICS, V45, P1340, DOI 10.1109/TCYB.2014.2350774
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Escalera S, 2015, LECT NOTES COMPUT SC, V8925, P459, DOI 10.1007/978-3-319-16178-5_32
   Fan ZX, 2019, IEEE T MULTIMEDIA, V21, P363, DOI 10.1109/TMM.2018.2859620
   Fletcher PT, 2004, IEEE T MED IMAGING, V23, P995, DOI 10.1109/TMI.2004.831793
   Guo Y, 2018, IEEE T CYBERNETICS, V48, P1513, DOI 10.1109/TCYB.2017.2705227
   Han F, 2017, COMPUT VIS IMAGE UND, V158, P85, DOI 10.1016/j.cviu.2017.01.011
   Harandi M, 2015, PROC CVPR IEEE, P3926, DOI 10.1109/CVPR.2015.7299018
   Harandi MT, 2014, LECT NOTES COMPUT SC, V8690, P17, DOI 10.1007/978-3-319-10605-2_2
   Huang ZW, 2017, PROC CVPR IEEE, P1243, DOI 10.1109/CVPR.2017.137
   Jiang ZL, 2013, IEEE T PATTERN ANAL, V35, P2651, DOI 10.1109/TPAMI.2013.88
   Joachims T., 2006, P 12 ACM SIGKDD INT, P217, DOI [10.1145/1150402.1150429, DOI 10.1145/1150402.1150429]
   Kacem A, 2020, IEEE T PATTERN ANAL, V42, P1, DOI 10.1109/TPAMI.2018.2872564
   KARCHER H, 1977, COMMUN PUR APPL MATH, V30, P509, DOI 10.1002/cpa.3160300502
   Ke Q, 2017, PROC CVPR IEEE, P4570, DOI 10.1109/CVPR.2017.486
   Kim TS, 2017, IEEE COMPUT SOC CONF, P1623, DOI 10.1109/CVPRW.2017.207
   Koppula HS, 2016, IEEE T PATTERN ANAL, V38, P14, DOI 10.1109/TPAMI.2015.2430335
   Li C, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P786
   Li CL, 2018, AAAI CONF ARTIF INTE, P3482
   Li WB, 2010, 2010 THE 3RD INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND INDUSTRIAL APPLICATION (PACIIA2010), VOL I, P9, DOI 10.1109/cvprw.2010.5543273
   Li YH, 2016, LECT NOTES COMPUT SC, V9911, P203, DOI 10.1007/978-3-319-46478-7_13
   Liu J, 2017, PROC CVPR IEEE, P3671, DOI 10.1109/CVPR.2017.391
   Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50
   Liu X, 2019, LECT NOTES COMPUT SC, V11295, P678, DOI 10.1007/978-3-030-05710-7_56
   Liu X, 2020, IEEE T IMAGE PROCESS, V29, P4583, DOI 10.1109/TIP.2020.2974061
   Lo Presti L, 2016, PATTERN RECOGN, V53, P130, DOI 10.1016/j.patcog.2015.11.019
   Lv F, 2006, LECT NOTES COMPUT SC, V3954, P359
   Mahasseni B, 2016, PROC CVPR IEEE, P3054, DOI 10.1109/CVPR.2016.333
   Murray R. M., 1994, MATH INTRO ROBOTIC M
   Neverova N, 2016, IEEE T PATTERN ANAL, V38, P1692, DOI 10.1109/TPAMI.2015.2461544
   Ofli F, 2014, J VIS COMMUN IMAGE R, V25, P24, DOI 10.1016/j.jvcir.2013.04.007
   Oreifej O, 2013, PROC CVPR IEEE, P716, DOI 10.1109/CVPR.2013.98
   Packer B, 2012, PROC CVPR IEEE, P1378, DOI 10.1109/CVPR.2012.6247824
   Papadopoulos K., 2019, ARXIV191209745
   Piyathilaka L, 2013, C IND ELECT APPL, P567
   Seidenari L, 2013, IEEE COMPUT SOC CONF, P479, DOI 10.1109/CVPRW.2013.77
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Shotton J, 2013, COMMUN ACM, V56, P116, DOI 10.1145/2398356.2398381
   Si CY, 2019, PROC CVPR IEEE, P1227, DOI 10.1109/CVPR.2019.00132
   Srivastava A, 2011, IEEE T PATTERN ANAL, V33, P1415, DOI 10.1109/TPAMI.2010.184
   Su JY, 2014, ANN APPL STAT, V8, P530, DOI 10.1214/13-AOAS701
   Sung JY, 2012, IEEE INT CONF ROBOT, P842, DOI 10.1109/ICRA.2012.6224591
   Szczapa B, 2019, IEEE INT CONF COMP V, P1241, DOI 10.1109/ICCVW.2019.00157
   Tanfous A. B., 2019, IEEE T PATTERN ANAL, V42, P2594
   Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82
   Wang J, 2014, IEEE T PATTERN ANAL, V36, P914, DOI 10.1109/TPAMI.2013.198
   Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813
   Wang L, 2020, IEEE T IMAGE PROCESS, V29, P15, DOI 10.1109/TIP.2019.2925285
   Weng JW, 2017, PROC CVPR IEEE, P445, DOI 10.1109/CVPR.2017.55
   Wu CX, 2015, PROC CVPR IEEE, P4362
   Wu D, 2016, IEEE T PATTERN ANAL, V38, P1583, DOI 10.1109/TPAMI.2016.2537340
   Wu D, 2014, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2014.98
   Xie Yuchen, 2013, JMLR Workshop Conf Proc, V28, P1480
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yang X., 2012, IEEE COMP SOC C COMP, V2012, P14, DOI [DOI 10.1109/CVPRW.2012.6239232, 10.1109/CVPRW.2012.6239232]
   Zanfir M, 2013, IEEE I CONF COMP VIS, P2752, DOI 10.1109/ICCV.2013.342
   Zhang SY, 2018, IEEE T MULTIMEDIA, V20, P2330, DOI 10.1109/TMM.2018.2802648
   Zhang YP, 2012, IEEE VTS VEH TECHNOL
   Zhu WH, 2016, PROC INT CONF ANTI, P1, DOI 10.1109/ICASID.2016.7873885
NR 66
TC 14
Z9 14
U1 2
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1841
EP 1854
DI 10.1109/TMM.2020.3003783
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TC8FT
UT WOS:000668875100003
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Qi, F
   Yang, XS
   Xu, CS
AF Qi, Fan
   Yang, Xiaoshan
   Xu, Changsheng
TI Emotion Knowledge Driven Video Highlight Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Training data; Predictive models; Training; Semantics;
   Emotion recognition; Computational modeling; Deep ranking; knowledge
   graph; video highlight detection
ID RETRIEVAL; SENTICNET
AB This paper addresses video highlight detection which aims to select a small subset of frames according to user's major or special interest. The performances of conventional methods highly depend on large-scale manually labeled training data which are time-consuming and labor-intensive to collect. To deal with this problem, we trace back to the original problem definition and find that whether a user is interested in a specific video segment heavily depends on human's subjective emotions. Leveraging this insight, we introduce an emotion knowledge driven video detection framework for modeling human's general emotion and inferencing highlight strength. Firstly, we obtain the concept-level representation of the video clip with a front-end network. The concepts are used as nodes to build an emotion-related knowledge graph, and their relationships in the graph are modeled via external public knowledge graphs. Then we adopt Siamese GCNs to model the dependencies between nodes in the graph and propagate messages along the edges. Finally, we compute the emotion-aware representation of the video clip based on the GCN layers and further use it to predict the highlight score. Our framework, including the front-end network, graph convolution layers and the highlight mapping network, can be trained in an end-to-end manner with the constraint of a ranking loss. Experiments on two benchmark datasets show that our proposed method performs favorably against the state-of-the-art methods.
C1 [Qi, Fan] HeFei Univ Technol, Comp Sci, Hefei 230009, Peoples R China.
   [Qi, Fan; Yang, Xiaoshan; Xu, Changsheng] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
   [Yang, Xiaoshan; Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Yang, Xiaoshan; Xu, Changsheng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100190, Peoples R China.
C3 Hefei University of Technology; Peng Cheng Laboratory; Chinese Academy
   of Sciences; Institute of Automation, CAS; Chinese Academy of Sciences;
   University of Chinese Academy of Sciences, CAS
RP Xu, CS (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.; Xu, CS (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100190, Peoples R China.
EM fanqi@mail.hfut.edu.cn; xiaoshan.yang@nlpr.ia.ac.cn; csxu@nlpr.ia.ac.cn
RI Xu, Chang/GQP-7280-2022; xu, cj/HJZ-3488-2023
OI xu, chang sheng/0000-0001-8343-9665
FU National Key Research and Development Program of China [2018AAA0100604];
   National Natural Science Foundation of China [61720106006, 62072455,
   61702511, 61751211, 61620106003, 61532009, U1836220, U1705262,
   61872424]; Key Research Program of Frontier Sciences of CAS
   [QYZDJSSWJSC039]; Research Program of National Laboratory of Pattern
   Recognition [Z-2018007]
FX This work was supported by the National Key Research and Development
   Program of China under Grant 2018AAA0100604, in part by the
   NationalNatural Science Foundation of China under Grants 61720106006,
   62072455, 61702511, 61751211, 61620106003, 61532009, U1836220, U1705262,
   and 61872424, in part by the Key Research Program of Frontier Sciences
   of CAS under Grant QYZDJSSWJSC039, and in part by the Research Program
   of National Laboratory of Pattern Recognition under Grant Z-2018007.
CR [Anonymous], 2015, ARXIV150705670
   [Anonymous], 2019, PR MACH LEARN RES
   [Anonymous], 2013, People's Web Meets NLP
   Auer S, 2007, LECT NOTES COMPUT SC, V4825, P722, DOI 10.1007/978-3-540-76298-0_52
   Bond F., 2013, P 51 ANN M ASS COMPU, P1352
   Bruna J., 2013, INT C LEARNING REPRE
   Cambria E, 2018, AAAI CONF ARTIF INTE, P1795
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen YT, 2019, IEEE T MULTIMEDIA, V21, P704, DOI 10.1109/TMM.2018.2865860
   Chu WS, 2015, PROC CVPR IEEE, P3584, DOI 10.1109/CVPR.2015.7298981
   Chung F. R., 1997, Spectral Graph Theory, V92, DOI DOI 10.1090/CBMS/092
   Dagtas S, 2004, MULTIMEDIA SYST, V9, P586, DOI 10.1007/s00530-003-0130-3
   Defferrard M, 2016, ADV NEUR IN, V29
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fang Y, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1661
   Gao JY, 2021, IEEE T MULTIMEDIA, V23, P3203, DOI 10.1109/TMM.2020.3021980
   Gao JY, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P690, DOI 10.1145/3240508.3240566
   Gao JY, 2019, AAAI CONF ARTIF INTE, P8303
   Gygli M, 2016, PROC CVPR IEEE, P1001, DOI 10.1109/CVPR.2016.114
   Gygli M, 2014, LECT NOTES COMPUT SC, V8695, P505, DOI 10.1007/978-3-319-10584-0_33
   Hanjalic A, 2005, IEEE T MULTIMEDIA, V7, P1114, DOI 10.1109/TMM.2005.858397
   Hanjalic A, 1999, IEEE T CIRC SYST VID, V9, P1280, DOI 10.1109/76.809162
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jiao YF, 2018, IEEE T MULTIMEDIA, V20, P2693, DOI 10.1109/TMM.2018.2815998
   Kipf TN, 2016, ARXIV
   Kossaifi J, 2021, IEEE T PATTERN ANAL, V43, P1022, DOI 10.1109/TPAMI.2019.2944808
   Liu W, 2015, PROC CVPR IEEE, P3707, DOI 10.1109/CVPR.2015.7298994
   Marino K, 2017, PROC CVPR IEEE, P20, DOI 10.1109/CVPR.2017.10
   Mettes P, 2017, IEEE I CONF COMP VIS, P4453, DOI 10.1109/ICCV.2017.476
   Mettes P, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P175, DOI 10.1145/2911996.2912036
   Nepal S., 2001, ACM Multimedia, P261
   Ortony A., 1988, COGNITIVE STRUCTURE
   Panda R, 2017, IEEE I CONF COMP VIS, P3677, DOI 10.1109/ICCV.2017.395
   Ping Q, 2017, P WORKSH NEW FRONT S, P1
   Poria S, 2013, IEEE INTELL SYST, V28, P31, DOI 10.1109/MIS.2013.4
   Potapov D, 2014, LECT NOTES COMPUT SC, V8694, P540, DOI 10.1007/978-3-319-10599-4_35
   Rui Y., 2000, Proceedings ACM Multimedia 2000, P105, DOI 10.1145/354384.354443
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Scherer KR, 2005, SOC SCI INFORM, V44, P695, DOI 10.1177/0539018405058216
   Soomro Khurram, 2012, UCF101 DATASET 101 H
   Speer R, 2017, AAAI CONF ARTIF INTE, P4444
   Sun M, 2014, LECT NOTES COMPUT SC, V8689, P787, DOI 10.1007/978-3-319-10590-1_51
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802
   Valstar M., 2014, P 4 INT WORKSH AUD V, P3, DOI 10.1145/2661806.2661807
   Wolf W, 1996, INT CONF ACOUST SPEE, P1228, DOI 10.1109/ICASSP.1996.543588
   Wu F, 2019, PR MACH LEARN RES, V97
   Xiong B, 2019, PROC CVPR IEEE, P1258, DOI 10.1109/CVPR.2019.00135
   Yang H, 2015, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2015.526
   Yang XS, 2016, IEEE T MULTIMEDIA, V18, P1832, DOI 10.1109/TMM.2016.2582379
   Yao T, 2016, PROC CVPR IEEE, P982, DOI 10.1109/CVPR.2016.112
   Yeh MC, 2016, MULTIMEDIA SYST, V22, P287, DOI 10.1007/s00530-015-0457-6
   Yuan F, 2017, ARXIV171101714
   Zhang HJ, 1997, PATTERN RECOGN, V30, P643, DOI 10.1016/S0031-3203(96)00109-4
   Zhang YY, 2020, AAAI CONF ARTIF INTE, V34, P12902
   Zhong PX, 2022, IEEE T AFFECT COMPUT, V13, P1290, DOI 10.1109/TAFFC.2020.2994159
   Zhou J, 2018, ARTIF CELL NANOMED B, V46, pS1016, DOI 10.1080/21691401.2018.1442841
NR 57
TC 10
Z9 10
U1 1
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3999
EP 4013
DI 10.1109/TMM.2020.3035285
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XA2YU
UT WOS:000720519900007
DA 2024-07-18
ER

PT J
AU Qiao, YY
   Deng, CR
   Wu, Q
AF Qiao, Yanyuan
   Deng, Chaorui
   Wu, Qi
TI Referring Expression Comprehension: A Survey of Methods and Datasets
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Referring expression; Vision and Language; Attention Mechanism; Survey
ID IMAGE
AB Referring expression comprehension (REC) aims to localize a target object in an image described by a referring expression phrased in natural language. Different from the object detection task that queried object labels have been pre-defined, the REC problem only can observe the queries during the test. It is more challenging than a conventional computer vision problem. This task has attracted a lot of attention from both computer vision and natural language processing community, and several lines of work have been proposed, from CNN-RNN model, modular network to complex graph-based model. In this survey, we first examine the state-of-the-art by comparing modern approaches to the problem. We classify methods by their mechanism to encode the visual and textual modalities. In particular, we examine the common approach of joint embedding images and expressions to a common feature space. We also discuss modular architectures and graph-based models that interface with structured graph representation. In the second part of this survey, we review the datasets available for training and evaluating REC approaches. We then group results according to the datasets, backbone models, settings so that they can be fairly compared. Finally, we discuss promising future directions for this field, in particular the compositional referring expression comprehension that requires more reasoning steps to address.
C1 [Qiao, Yanyuan; Deng, Chaorui; Wu, Qi] Univ Adelaide, Australian Inst Machine Learning, Sch Comp Sci, Adelaide, SA 5005, Australia.
C3 University of Adelaide
RP Wu, Q (corresponding author), Univ Adelaide, Australian Inst Machine Learning, Sch Comp Sci, Adelaide, SA 5005, Australia.
EM yanyuan.qiao@adelaide.edu.au; chaorui.deng@adelaide.edu.au;
   qi.wu01@adelaide.edu.au
RI Wu, Qi/ABD-6304-2021; Yanyuan, Qiao/KBY-0525-2024
OI Wu, Qi/0000-0003-3631-256X; Deng, Chaorui/0000-0002-8587-9047
FU ARC [DE190100539]; Australian Research Council [DE190100539] Funding
   Source: Australian Research Council
FX This work was supported by ARC DE190100539. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Ting Yao.
CR Andreas J, 2017, PR MACH LEARN RES, V70
   Andrews Stuart, 2002, PROC 25 ANN C NEURAL, P561
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Chen DD, 2014, J ANAL METHODS CHEM, V2014, DOI 10.1155/2014/575246
   Chen H, 2019, PROC CVPR IEEE, P12530, DOI 10.1109/CVPR.2019.01282
   Chen K, 2018, PROC CVPR IEEE, P4042, DOI 10.1109/CVPR.2018.00425
   Chen Zhenfang, 2020, P IEEE CVF C COMP VI, P10086
   Cirik V, 2018, AAAI CONF ARTIF INTE, P6756
   Cirik Volkan, 2018, P 2018 C N AM CHAPT, V2, P781, DOI DOI 10.18653/V1/N18-2123
   Cirik Volkan, 2020, P 58 ANN M ASS COMPU, P7189, DOI DOI 10.18653/V1/2020.ACL-MAIN.644
   de Vries H, 2017, PROC CVPR IEEE, P4466, DOI 10.1109/CVPR.2017.475
   Deng CR, 2018, PROC CVPR IEEE, P7746, DOI 10.1109/CVPR.2018.00808
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong JF, 2018, IEEE T MULTIMEDIA, V20, P3377, DOI 10.1109/TMM.2018.2832602
   Fukui A.., 2016, P C EMP METH NAT LAN, P4555
   Gen Luo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10031, DOI 10.1109/CVPR42600.2020.01005
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Greff K, 2017, IEEE T NEUR NET LEAR, V28, P2222, DOI 10.1109/TNNLS.2016.2582924
   Grubinger M, 2006, The IAPR TC-12 benchmark: a new evaluation resource for visual information systems
   Hu RH, 2017, IEEE I CONF COMP VIS, P804, DOI 10.1109/ICCV.2017.93
   Hu RH, 2017, PROC CVPR IEEE, P4418, DOI 10.1109/CVPR.2017.470
   Hu RH, 2016, PROC CVPR IEEE, P4555, DOI 10.1109/CVPR.2016.493
   Escalante HJ, 2010, COMPUT VIS IMAGE UND, V114, P419, DOI 10.1016/j.cviu.2009.03.008
   Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215
   Kazemzadeh S., 2014, EMNLP, P787, DOI DOI 10.3115/V1/D14-1086
   Kottur S, 2018, LECT NOTES COMPUT SC, V11219, P160, DOI 10.1007/978-3-030-01267-0_10
   Li JN, 2017, IEEE T MULTIMEDIA, V19, P944, DOI 10.1109/TMM.2016.2642789
   Li LH, 2018, IEEE T MULTIMEDIA, V20, P726, DOI 10.1109/TMM.2017.2751140
   Li XY, 2019, IEEE T MULTIMEDIA, V21, P2117, DOI 10.1109/TMM.2019.2896516
   Liao Y., 2020, P IEEE CVF C COMP VI, P10877, DOI 10.24963/ijcai.2018/155
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu DQ, 2019, IEEE I CONF COMP VIS, P4672, DOI 10.1109/ICCV.2019.00477
   Liu JY, 2017, IEEE I CONF COMP VIS, P4866, DOI 10.1109/ICCV.2017.520
   Liu RT, 2019, PROC CVPR IEEE, P4180, DOI 10.1109/CVPR.2019.00431
   Liu XH, 2019, PROC CVPR IEEE, P1950, DOI 10.1109/CVPR.2019.00205
   Liu XJ, 2019, IEEE I CONF COMP VIS, P2611, DOI 10.1109/ICCV.2019.00270
   Liu XJ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P539, DOI 10.1145/3343031.3351074
   Liu YF, 2020, AAAI CONF ARTIF INTE, V34, P11645
   Lu JS, 2019, ADV NEUR IN, V32
   Luo RT, 2017, PROC CVPR IEEE, P3125, DOI 10.1109/CVPR.2017.333
   Mao JH, 2016, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2016.9
   Mitchell Margaret, 2013, P 2013 C N AM CHAPT, P1174
   Nagaraja VK, 2016, LECT NOTES COMPUT SC, V9908, P792, DOI 10.1007/978-3-319-46493-0_48
   Plummer BA, 2015, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2015.303
   Qiu S, 2020, IEEE T MULTIMEDIA, V22, P1333, DOI 10.1109/TMM.2019.2942480
   Radford A., 2015, ARXIV
   Redmon J., 2018, P IEEE C COMP VIS PA
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Rohrbach A, 2016, LECT NOTES COMPUT SC, V9905, P817, DOI 10.1007/978-3-319-46448-0_49
   Sadhu A, 2019, IEEE I CONF COMP VIS, P4693, DOI 10.1109/ICCV.2019.00479
   Simonyan K., 2014, CORR
   Socher Richard, 2013, Long Papers, P455
   Su WJ, 2019, ANN NUTR METAB, V75, P31, DOI 10.1159/000501710
   Wang P, 2019, PROC CVPR IEEE, P1960, DOI 10.1109/CVPR.2019.00206
   Wu Q, 2017, COMPUT VIS IMAGE UND, V163, P21, DOI 10.1016/j.cviu.2017.05.001
   Xiao XY, 2019, IEEE T MULTIMEDIA, V21, P2942, DOI 10.1109/TMM.2019.2915033
   Xu N, 2020, IEEE T MULTIMEDIA, V22, P1372, DOI 10.1109/TMM.2019.2941820
   Yang SB, 2019, IEEE I CONF COMP VIS, P4643, DOI 10.1109/ICCV.2019.00474
   Yang SB, 2019, PROC CVPR IEEE, P4140, DOI 10.1109/CVPR.2019.00427
   Yang ZY, 2019, IEEE I CONF COMP VIS, P4682, DOI 10.1109/ICCV.2019.00478
   Yao T, 2019, IEEE I CONF COMP VIS, P2621, DOI 10.1109/ICCV.2019.00271
   Yingwei Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10968, DOI 10.1109/CVPR42600.2020.01098
   Young P., 2014, Transactions of the Association for Computational Linguistics, V2, P67
   Yu LC, 2018, PROC CVPR IEEE, P1307, DOI 10.1109/CVPR.2018.00142
   Yu LC, 2017, PROC CVPR IEEE, P3521, DOI 10.1109/CVPR.2017.375
   Yu LC, 2016, LECT NOTES COMPUT SC, V9906, P69, DOI 10.1007/978-3-319-46475-6_5
   Yu Z, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1114
   Yuankai Qi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9979, DOI 10.1109/CVPR42600.2020.01000
   Zhang HW, 2018, PROC CVPR IEEE, P4158, DOI 10.1109/CVPR.2018.00437
   Zhang WQ, 2020, IEEE T MULTIMEDIA, V22, P1032, DOI 10.1109/TMM.2019.2935678
   Zhao F, 2018, PROC CVPR IEEE, P5696, DOI 10.1109/CVPR.2018.00597
   Zheng ZL, 2019, PROC CVPR IEEE, P3662, DOI 10.1109/CVPR.2019.00683
   Zhuang BH, 2018, PROC CVPR IEEE, P4252, DOI 10.1109/CVPR.2018.00447
NR 73
TC 30
Z9 33
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 4426
EP 4440
DI 10.1109/TMM.2020.3042066
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XM6HD
UT WOS:000728924800003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Toffa, OK
   Mignotte, M
AF Toffa, Ohini Kafui
   Mignotte, Max
TI A Hierarchical Visual Feature-Based Approach For Image Sonification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Sonification; visually impaired; sound synthesis; auditory feedback;
   audio mapping
AB This paper presents a new image sonification system that strives to help visually impaired users access visual information via an audio (easily decodable) signal that is generated in real time when the users explore the image on a touch screen or with a pointer. The sonified signal, which is generated for each position within the image, tries to capture the most useful and discriminant local information about the image content at different levels of abstraction, ranging from low-level (at the pixel level) to high-level (segmentation) and combining low-level (color edges and texture), mid-level and high-level (gradient or color distribution for each region of the image) features. The proposed system mainly uses musical notes at several octaves, the notion of timbre, and loudness but also uses pitch, rhythm and the distortion effect in an intuitive way to sonify the image content both locally and globally. To this end, we use perceptually meaningful mappings, in which the properties of an image are directly reflected in the audio domain, in a very predictable way. The listener can then draw simple and reliable conclusions about the image by quickly decoding the sonified result.
C1 [Toffa, Ohini Kafui; Mignotte, Max] Univ Montreal, Fac Arts & Sci, Dept Informat & Rech Operat DIRO, Vis Lab, Montreal, PQ H3C 3J7, Canada.
C3 Universite de Montreal
RP Toffa, OK (corresponding author), Univ Montreal, Fac Arts & Sci, Dept Informat & Rech Operat DIRO, Vis Lab, Montreal, PQ H3C 3J7, Canada.
EM ohini.kafui.toffa@umontreal.ca; mignotte@iro.umontreal.ca
RI Mignotte, Max/F-7014-2015
OI Toffa, ohini kafui/0000-0002-6646-6001
CR Ahmad A, 2010, OPT EXPRESS, V18, P9934, DOI 10.1364/OE.18.009934
   [Anonymous], 2013, 2013 15 EUR C POW EL
   Balakrishnan G., 2008, INT J INF CONTROL CO, V2, P1
   Banf M., 2012, Proceedings of the 18th International Conference on Auditory Display, P121
   Banf Michael, 2016, J REHAB ASSIST TECHN, V3, P1, DOI [10.1177/ToBeAssigned, DOI 10.1177/TOBEASSIGNED]
   Banf Michael, 2013, Fourth International Conference on Augmented Human, P162, DOI [10.1145/2459236.2459264, DOI 10.1145/2459236.2459264]
   Capp M, 2000, ENG SCI EDUC J, V9, P137, DOI 10.1049/esej:20000306
   Cavaco S., 2013, Procedia Technology, V9, P1048, DOI DOI 10.1016/J.PROTCY.2013.12.117
   Chidester B., 2013, 2013 IEEE INT C MULT, P1, DOI DOI 10.1109/ICMEW.2013.6618381
   Degara N, 2015, IEEE MULTIMEDIA, V22, P20, DOI 10.1109/MMUL.2015.8
   Dubus G, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0082491
   Edwards ADN, 2008, CISP 2008: FIRST INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, VOL 2, PROCEEDINGS, P128, DOI 10.1109/CISP.2008.751
   Elbert T, 2002, J NEUROSCI, V22, P9941
   FAUSTI SA, 1981, J ACOUST SOC AM, V69, P1343, DOI 10.1121/1.385805
   Franklin KM, 2003, IEEE INFOR VIS, P4, DOI 10.1109/IV.2003.1217949
   Goudarzi V, 2015, IEEE MULTIMEDIA, V22, P41, DOI 10.1109/MMUL.2015.4
   Gougoux F, 2004, NATURE, V430, P309, DOI 10.1038/430309a
   Jovanov E, 1999, IEEE Trans Inf Technol Biomed, V3, P109, DOI 10.1109/4233.767086
   Kopecek I, 2008, THIRD 2008 INTERNATIONAL CONFERENCE ON CONVERGENCE AND HYBRID INFORMATION TECHNOLOGY, VOL 2, PROCEEDINGS, P722, DOI 10.1109/ICCIT.2008.152
   Loughlin C., 2012, SENSORS IND INSPECTI
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Martins ACG, 2001, J ELECTRON IMAGING, V10, P690, DOI 10.1117/1.1382811
   Matta S, 2004, ISCCSP : 2004 FIRST INTERNATIONAL SYMPOSIUM ON CONTROL, COMMUNICATIONS AND SIGNAL PROCESSING, P431
   MEIJER PBL, 1992, IEEE T BIO-MED ENG, V39, P112, DOI 10.1109/10.121642
   Mignotte M., 1998, THESIS BREST U BREST
   Mignotte M, 2014, INFORM FUSION, V20, P7, DOI 10.1016/j.inffus.2013.10.012
   Munshell AH, 1912, AM J PSYCHOL, V23, P236, DOI 10.2307/1412843
   Parseihian G, 2016, IEEE T MULTIMEDIA, V18, P674, DOI 10.1109/TMM.2016.2531978
   Radecki A, 2017, SIG P ALGO ARCH ARR, P239, DOI 10.23919/SPA.2017.8166871
   Schaffert N, 2015, IEEE MULTIMEDIA, V22, P58, DOI 10.1109/MMUL.2015.9
   Tajadura-Jiménez A, 2015, IEEE MULTIMEDIA, V22, P48, DOI 10.1109/MMUL.2015.14
   Wan CY, 2010, NEUROPSYCHOLOGIA, V48, P344, DOI 10.1016/j.neuropsychologia.2009.08.016
   Wikipedia, PIANO KEY FREQUENCIE
   Yeo WS, 2006, P INT C DIG AUD EFF, P309
   Yoshida Tsubasa., 2011, P 2 AUGMENTED HUMAN, P11, DOI DOI 10.1145/1959826.1959837
NR 35
TC 4
Z9 5
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 706
EP 715
DI 10.1109/TMM.2020.2987710
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA QA6NL
UT WOS:000613560200013
DA 2024-07-18
ER

PT J
AU Zhang, HW
   Qian, SS
   Fang, Q
   Xu, CS
AF Zhang, Huaiwen
   Qian, Shengsheng
   Fang, Quan
   Xu, Changsheng
TI Multimodal Disentangled Domain Adaption for Social Media Event Rumor
   Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Social networking (online); Feature extraction; Task analysis;
   Adaptation models; Writing; Visualization; Training; Disentanglement
   representation learning; domain adaptation; event rumor detection;
   social media
AB With the rapid development of social media and the increasing scale of social media data, the rumor detection on social media platforms has become vitally crucial. The key challenges for rumor detection on social media platforms are how to identify rumors deeply entangled with the specific content and how to detect rumors for the emerging social media events without labeled data. Unfortunately, most of the existing approaches can hardly handle these challenges since they tend to learn event-specific features and cannot transfer the learned features to newly emerged events. To tackle the above challenges, we propose a novel Multimodal Disentangled Domain Adaption (MDDA) method which can derive event-invariant features and thus benefit the detection of rumors on emerging social media events. The model consists of two components: the multimodal disentangled representation learning and the unsupervised domain adaptation. The multimodal disentangled representation learning is responsible for disentangling the multimedia posts into the content features and the rumor style features, and removing the content-specific features from post representation. The unsupervised domain adaptation aims to filter out the event-specific features and keep shared rumor style features among events. Based on the final event-invariant rumor style features, we train a robust social media rumor detector that can transfer knowledge from source events to the target events, which can perform well on the newly emerged events. Extensive experiments on two Twitter benchmark datasets demonstrate that our rumor detection model outperforms state-of-the-art methods.
C1 [Zhang, Huaiwen; Qian, Shengsheng; Fang, Quan; Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Zhang, Huaiwen; Qian, Shengsheng; Fang, Quan; Xu, Changsheng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
   [Xu, Changsheng] Peng Cheng Lab, Shenzhen 518066, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   Peng Cheng Laboratory
RP Xu, CS (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.; Xu, CS (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.; Xu, CS (corresponding author), Peng Cheng Lab, Shenzhen 518066, Peoples R China.
EM huaiwen.zhang@nlpr.ia.ac.cn; shengsheng.qian@nlpr.ia.ac.cn;
   qfang@nlpr.ia.ac.cn; csxu@nlpr.ia.ac.cn
RI xu, cj/HJZ-3488-2023; Xu, Chang/GQP-7280-2022
OI Zhang, Huaiwen/0000-0002-3183-9218; xu, chang sheng/0000-0001-8343-9665
FU National Natural Science Foundation of China [61721004, 61532009,
   61720106006, 61572503, 61802405, 61872424, 61702509, 61832002, 61936005,
   U1705262]; Key Research Program of Frontier Sciences, CAS
   [QYZDJ-SSW-JSC039]; K.C.Wong Education Foundation
FX This work was supported in part by the National Natural Science
   Foundation ofChina underGrants, 61721004, 61532009, 61720106006,
   61572503, 61802405, 61872424, 61702509, 61832002, 61936005, and
   U1705262, in part by the Key Research Program of Frontier Sciences, CAS,
   under Grant QYZDJ-SSW-JSC039, and in part by the K.C.Wong Education
   Foundation. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Pradeep K Atrey.
CR Allcott H, 2017, J ECON PERSPECT, V31, P211, DOI 10.1257/jep.31.2.211
   [Anonymous], 2015, arXiv
   [Anonymous], 2014, INT C MACH LEARN ICM
   [Anonymous], CONVOLUTIONAL NEURAL
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18
   Castillo C., 2011, P 20 INT C WORLD WID, P675, DOI DOI 10.1145/1963405.1963500
   Chen T, 2018, LECT NOTES ARTIF INT, V11154, P40, DOI 10.1007/978-3-030-04503-6_4
   Chen X, 2016, ADV NEUR IN, V29
   DiFonzo N., 2007, Rumor Psychology: Social and Organizational Approaches, DOI [10.1037/11503-000, DOI 10.1037/11503-000]
   Fang Q, 2015, IEEE T MULTIMEDIA, V17, P2281, DOI 10.1109/TMM.2015.2491019
   Ganin Y, 2016, J MACH LEARN RES, V17
   Gong B., 2013, P INT C MACH LEARN, P222
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Haeusser P, 2017, IEEE I CONF COMP VIS, P2784, DOI 10.1109/ICCV.2017.301
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton GE, 2011, LECT NOTES COMPUT SC, V6791, P44, DOI 10.1007/978-3-642-21735-7_6
   Hoffman J, 2017, P INT C MACH LEARN P, V80, P1994
   HUANG X, 2018, PROC EUR C COMPUT VI
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   JIN Z, 2016, P ASS ADV ART INT
   Jin ZW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P795, DOI 10.1145/3123266.3123454
   Jin ZW, 2017, IEEE T MULTIMEDIA, V19, P598, DOI 10.1109/TMM.2016.2617078
   Khattar D, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P2915, DOI 10.1145/3308558.3313552
   Kim D, 2015, MULTIMEDIA SYST, V21, P73, DOI 10.1007/s00530-013-0342-0
   Kingma D. P., 2015, PROC INT C LEARN REP, P1
   Kingma D. P., 2014, arXiv
   Kingma DP, 2014, ADV NEUR IN, V27
   Kochkina E., 2018, P 27 INT C COMP LING, P3402, DOI [10.48550/arXiv.1806.03713, DOI 10.48550/ARXIV.1806.03713]
   Kumar A, 2020, MICROBIAL ENDOPHYTES: FUNCTIONAL BIOLOGY AND APPLICATIONS, P1, DOI 10.1016/B978-0-12-819654-0.00001-6
   KWON S, 2017, PLOS ONE, V12
   Kwon S, 2013, IEEE DATA MINING, P1103, DOI 10.1109/ICDM.2013.61
   Li S., 2020, MATH PROBL ENG, DOI DOI 10.1155/2020/2930515
   Li S, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P729, DOI 10.1145/3343031.3351070
   LIU S, 2020, SIGIR 2020 VIRT EVEN, P1379
   Liu X., 2015, P 24 ACM INT C INF K, P1867, DOI [10.1145/2806416.2806651, DOI 10.1145/2806416.2806651]
   Long MS, 2017, PR MACH LEARN RES, V70
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Ma J., 2015, P 24 ACM INT C INF K, P1751, DOI DOI 10.1145/2806416.2806607
   Ma J., 2016, P 25 INT JOINT C ART, P3818
   Ma J, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P3049, DOI 10.1145/3308558.3313741
   Ma XH, 2019, IEEE T MULTIMEDIA, V21, P2419, DOI 10.1109/TMM.2019.2902100
   Magdy Amr, 2010, P 2 INT WORKSHOP SEA, P103, DOI DOI 10.1145/1871985.1872002
   Mathieu M., 2016, Neural Information Processing Symposium, pages, P5041
   Pennington J., 2014, P 2014 C EMP METH NA, P1532, DOI [DOI 10.3115/V1/D14-1162, 10.3115/v1/D14-1162]
   Potthast M, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P231
   Qian SS, 2018, IEEE T MULTIMEDIA, V20, P2086, DOI 10.1109/TMM.2017.2785227
   Qian SS, 2016, IEEE T MULTIMEDIA, V18, P233, DOI 10.1109/TMM.2015.2510329
   Qian Shengsheng, 2016, P 24 ACM INT C MULTI
   Reed S, 2014, PR MACH LEARN RES, V32, P1431
   Rifai S, 2012, LECT NOTES COMPUT SC, V7577, P808, DOI 10.1007/978-3-642-33783-3_58
   Russo P, 2018, PROC CVPR IEEE, P8099, DOI 10.1109/CVPR.2018.00845
   Saito K, 2017, PR MACH LEARN RES, V70
   Shu K, 2018, IEEE 1ST CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2018), P430, DOI 10.1109/MIPR.2018.00092
   Tacchini Eugenio, 2017, ARXIV170407506, P1, DOI [10.48550/arXiv.1704.07506, DOI 10.1257/JEP.31.2.211]
   Tang SY, 2011, IEEE T MULTIMEDIA, V13, P1163, DOI 10.1109/TMM.2011.2159706
   Tausczik YR, 2010, J LANG SOC PSYCHOL, V29, P24, DOI 10.1177/0261927X09351676
   Tenenbaum JB, 1997, ADV NEUR IN, V9, P662
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   Ulyanov D, 2017, PROC CVPR IEEE, P4105, DOI 10.1109/CVPR.2017.437
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vlachos A., 2014, P ACL 2014 WORKSH LA, P18, DOI [DOI 10.3115/V1/W14-2508, 10.3115/v1/W14-2508]
   Wang YQ, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P849, DOI 10.1145/3219819.3219903
   Wu K, 2015, PROC INT CONF DATA, P651, DOI 10.1109/ICDE.2015.7113322
   Wu M, 2020, WEB CONFERENCE 2020: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2020), P1457, DOI 10.1145/3366423.3380219
   Xie SA, 2018, PR MACH LEARN RES, V80
   Yang S, 2019, AAAI CONF ARTIF INTE, P5644
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P64, DOI 10.1109/TMM.2014.2375793
   Yang Y., 2018, Ti-cnn: Convolutional neural networks for fake news detection
   Yosinski J, 2014, ADV NEUR IN, V27
   You QZ, 2015, IEEE T MULTIMEDIA, V17, P2271, DOI 10.1109/TMM.2015.2487863
   Yu F, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3901
   Zhang HW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1942, DOI 10.1145/3343031.3350850
   Zhang YY, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1089, DOI 10.1145/3343031.3351033
   Zhou XY, 2020, LECT NOTES ARTIF INT, V12085, P354, DOI 10.1007/978-3-030-47436-2_27
   Zubiaga A., 2017, P INT C SOCIAL INFOR, P109, DOI [10.1007/978-3-319-67217-5_8SeriesTitle:LectureNotesinComputerScience, DOI 10.1007/978-3-319-67217-5_8SERIESTITLE:LECTURENOTESINCOMPUTERSCIENCE, 10.1007/978-3-319-67217-58]
NR 76
TC 23
Z9 23
U1 5
U2 47
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 4441
EP 4454
DI 10.1109/TMM.2020.3042055
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XM6HD
UT WOS:000728924800004
DA 2024-07-18
ER

PT J
AU Sun, YB
   Yang, Y
   Liu, QS
   Chen, JW
   Yuan, XT
   Guo, GD
AF Sun, Yubao
   Yang, Ying
   Liu, Qingshan
   Chen, Jiwei
   Yuan, Xiao-Tong
   Guo, Guodong
TI Learning Non-Locally Regularized Compressed Sensing Network With
   Half-Quadratic Splitting
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image reconstruction; Image sequences; Compressed sensing; Training;
   Machine learning; Electronics packaging; Noise measurement; Compressed
   sensing; deep network prior; non-local prior; half-quadratic splitting
ID RECONSTRUCTION; ALGORITHM; ROBUST; SIGNAL; RECOVERY; IMAGES
AB Deep learning-based Compressed Sensing (CS) reconstruction attracts much attention in recent years, due to its significant superiority of reconstruction quality. Its success is mainly attributed to the employment of a large dataset for pre-training the network to learn a reconstruction mapping. In this paper, we propose a non-locally regularized compressed sensing network for reconstructing image sequences, which can achieve high reconstruction quality without pre-training. Specifically, the proposed method attempts to learn a deep network prior for the reconstruction of an individual instance under the constraint that the network output can well match the given CS measurement. The non-local prior is designed to guide the network to capture the long-range dependencies by exploiting the self-similarities among images, and it can also make the network noise-aware. In order to deal with the compound of non-local prior and deep network prior, we construct a half-quadratic splitting based optimization method for network learning, in which the two priors are decoupled into two simple sub-problems by introducing an auxiliary variable and a quadratic fidelity constraint. Extensive experimental results demonstrate that our method is competitive to the popular methods, including sparsity prior based methods and deep learning based methods, even better than them in the cases of low measurement rates.
C1 [Sun, Yubao; Yang, Ying; Liu, Qingshan; Chen, Jiwei; Yuan, Xiao-Tong] Nanjing Univ Informat Sci & Technol, Jiangsu Collaborat Innovat Ctr Atmospher Environm, Jiangsu Key Lab Big Data Anal Technol, Nanjing 210044, Peoples R China.
   [Guo, Guodong] West Virginia Univ, Natl Engn Lab Deep Learning Technol & Applicat, Inst Deep Learning, Baidu Res, Morgantown, WV 26506 USA.
   [Guo, Guodong] West Virginia Univ, Dept Comp Sci & Elect Engn, Morgantown, WV 26506 USA.
C3 Nanjing University of Information Science & Technology; Baidu; West
   Virginia University; West Virginia University
RP Liu, QS (corresponding author), Nanjing Univ Informat Sci & Technol, Jiangsu Collaborat Innovat Ctr Atmospher Environm, Jiangsu Key Lab Big Data Anal Technol, Nanjing 210044, Peoples R China.
EM sunyb@nuist.edu.cn; yingyang@nuist.edu.cn; qsliu@nuist.edu.cn;
   jiweichen1994@nuist.edu.cn; xtyuan1980@gmail.com;
   guodong.guo@mail.wvu.edu
RI Liu, Qing/GWC-9222-2022; Chen, Rainie/ISS-6016-2023; chen,
   jiwei/Z-2204-2019; Liu, Qingqing/HMV-4816-2023; liu,
   qingqing/HHD-0360-2022; Guo, Guodong/M-5066-2015
OI chen, jiwei/0000-0002-7835-5704; Guo, Guodong/0000-0001-9583-0055
FU National Natural Science Foundation of China [61672292, 61825601,
   61532009]; National Major Project of China for New Generation of AI
   [2018AAA0100400]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61672292, 61825601, and 61532009 and in
   part by the National Major Project of China for New Generation of AI
   under Grant 2018AAA0100400.
CR Baraniuk RG, 2010, IEEE T INFORM THEORY, V56, P1982, DOI 10.1109/TIT.2010.2040894
   Blumensath T, 2009, APPL COMPUT HARMON A, V27, P265, DOI 10.1016/j.acha.2009.04.002
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Candès EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083
   Candès E, 2007, INVERSE PROBL, V23, P969, DOI 10.1088/0266-5611/23/3/008
   Candès EJ, 2008, CR MATH, V346, P589, DOI 10.1016/j.crma.2008.03.014
   Candés EJ, 2015, APPL COMPUT HARMON A, V39, P277, DOI 10.1016/j.acha.2014.09.004
   Carrillo RE, 2011, INT CONF ACOUST SPEE, P4028
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Dong WS, 2017, IEEE T MULTIMEDIA, V19, P293, DOI 10.1109/TMM.2016.2613824
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Duarte MF, 2008, IEEE SIGNAL PROC MAG, V25, P83, DOI 10.1109/MSP.2007.914730
   Fazel M, 2008, CONF REC ASILOMAR C, P1043, DOI 10.1109/ACSSC.2008.5074571
   Felix H., 2016, ACM Transactions on Graphics (TOG), V35, P1
   Fowler JE, 2010, FOUND TRENDS SIGNAL, V4, P297, DOI 10.1561/2000000033
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   King DB, 2015, ACS SYM SER, V1214, P1
   Kulkarni K, 2016, PROC CVPR IEEE, P449, DOI 10.1109/CVPR.2016.55
   Lalos AS, 2017, IEEE T MULTIMEDIA, V19, P41, DOI 10.1109/TMM.2016.2605927
   Li CB, 2013, COMPUT OPTIM APPL, V56, P507, DOI 10.1007/s10589-013-9576-1
   Liu JM, 2019, INT CONF ACOUST SPEE, P7715, DOI 10.1109/ICASSP.2019.8682856
   Liu YL, 2008, J COMPUT SCI TECH-CH, V23, P270, DOI 10.1007/s11390-008-9129-8
   Lustig M, 2008, IEEE SIGNAL PROC MAG, V25, P72, DOI 10.1109/MSP.2007.914728
   Ma SQ, 2008, PROC CVPR IEEE, P389
   Metzler CA, 2017, ADV NEUR IN, V30
   Metzler CA, 2016, IEEE T INFORM THEORY, V62, P5117, DOI 10.1109/TIT.2016.2556683
   Mousavi A, 2015, ANN ALLERTON CONF, P1336, DOI 10.1109/ALLERTON.2015.7447163
   Mun S, 2009, IEEE IMAGE PROC, P3021, DOI 10.1109/ICIP.2009.5414429
   Nikolova M, 2005, SIAM J SCI COMPUT, V27, P937, DOI 10.1137/030600862
   Radford A., 2016, 4 INT C LEARN REPR I
   Song XD, 2017, IEEE T MULTIMEDIA, V19, P1351, DOI 10.1109/TMM.2017.2654123
   Sun Y, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107051
   Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108
   Ulyanov D, 2018, PROC CVPR IEEE, P9446, DOI 10.1109/CVPR.2018.00984
   Veen D. V., 2018, ARXIV180606438
   Wang W, 2014, NUMER MATH-THEORY ME, V7, P334, DOI 10.4208/nmtma.2014.1326nm
   Xiao YH, 2012, INVERSE PROBL IMAG, V6, P547, DOI 10.3934/ipi.2012.6.547
   Yang Y, 2020, IEEE T PATTERN ANAL, V42, P521, DOI 10.1109/TPAMI.2018.2883941
   Yao HT, 2019, NEUROCOMPUTING, V359, P483, DOI 10.1016/j.neucom.2019.05.006
   Yuan XL, 2016, IEEE T MULTIMEDIA, V18, P2002, DOI 10.1109/TMM.2016.2602758
   Zeiler MD, 2010, PROC CVPR IEEE, P2528, DOI 10.1109/CVPR.2010.5539957
   Zhang J, 2018, PROC CVPR IEEE, P1828, DOI 10.1109/CVPR.2018.00196
   Zhang LY, 2016, IEEE T MULTIMEDIA, V18, P1720, DOI 10.1109/TMM.2016.2581593
NR 43
TC 28
Z9 28
U1 4
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2020
VL 22
IS 12
BP 3236
EP 3248
DI 10.1109/TMM.2020.2973862
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU9BG
UT WOS:000591817700017
DA 2024-07-18
ER

PT J
AU Han, YM
   Zhang, P
   Zhuo, T
   Huang, W
   Zha, YF
   Zhang, YN
AF Han, Yamin
   Zhang, Peng
   Zhuo, Tao
   Huang, Wei
   Zha, Yufei
   Zhang, Yanning
TI Ensemble Tracking Based on Diverse Collaborative Framework With
   Multi-Cue Dynamic Fusion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Target tracking; Correlation; Robustness; Adaptation models; Training;
   Tracking loops; Collaboration; Ensembling structure; collaborative
   tracking framework; multi-cue dynamic fusion; heuristic frequency
ID ROBUST VISUAL TRACKING
AB Tracking with deep neural networks has been verified to arrive at a new level accuracy in many challenging scenarios, but the tracking robustness has been still challenged by model singularity and self-learning loop mechanism. As a promising solution for the limitations, to ensemble diverse tracking strategies into a highly-interactive framework has shown a potential effectiveness in recent studies. In this work, a collaborative tracking framework is proposed by exploiting both discriminative correlation filters and deep classifiers into an ensembling framework. With a multi-cue dynamic fusion scheme performed on all the ensembled members' outputs, a robust long-term tracking can be achieved by calculating the optimal robustness scores based on a dynamic weighted sum of multi-cue metrics. Meanwhile, the obtained reliable and diverse training samples are also utilized to adaptively update the tracker in each branch with heuristic frequency, which is able to alleviate the training samples' contamination and model corruption. Experiments on the OTB-2015, Temple color 128, UAV123, VOT2016, and VOT2018 benchmark datasets have shown superior performance in comparison to other state-of-the-art tracking approaches.
C1 [Han, Yamin; Zhang, Peng; Zha, Yufei; Zhang, Yanning] Northwestern Polytech Univ, Sch Comp Sci, Xian 710072, Peoples R China.
   [Han, Yamin; Zhang, Yanning] Northwestern Polytech Univ, Natl Engn Lab Integrated Aerospace Ground Ocean B, Xian 710072, Peoples R China.
   [Zhuo, Tao] Natl Univ Singapore, Sch Comp, Singapore 119077, Singapore.
   [Huang, Wei] Nanchang Univ, Sch Informat Engn, Nanchang 330022, Jiangxi, Peoples R China.
C3 Northwestern Polytechnical University; Northwestern Polytechnical
   University; National University of Singapore; Nanchang University
RP Zhang, P (corresponding author), Northwestern Polytech Univ, Sch Comp Sci, Xian 710072, Peoples R China.; Huang, W (corresponding author), Nanchang Univ, Sch Informat Engn, Nanchang 330022, Jiangxi, Peoples R China.
EM yaminhan@mail.nwpu.edu.cn; zh0036ng@nwpu.edu.cn; zhuotao@nus.edu.sg;
   n060101@e.ntu.edu.sg; zhayufei@126.com; ynzhang@nwpu.edu.cn
RI yang, peng/JEZ-8452-2023; Li, Chun/KBC-9591-2024
OI Zha, yufei/0000-0001-5013-2501
FU National Natural Science Foundation of China [61571362, 61971352,
   61862043, 61773397]; Natural Science Foundation of Shaanxi Province
   [2018JM6015]; Natural Science Foundation of Jiangxi Province
   [20181ACB20006, 20171ACB21017]; Fundamental Research Funds for the
   Central Universities [3102019ZY1004, 3102019ZY1003]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61571362, 61971352, 61862043, and
   61773397, in part by the Natural Science Foundation of Shaanxi Province
   under Grant 2018JM6015, in part by the Natural Science Foundation of
   Jiangxi Province under Grants 20181ACB20006 and 20171ACB21017, and in
   part by the Fundamental Research Funds for the Central Universities
   under Grants 3102019ZY1004 and 3102019ZY1003.
CR [Anonymous], 2019, IEEE T MULTIMEDIA, DOI DOI 10.1109/TMM.2018.2859831
   [Anonymous], PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2018.00745, DOI 10.1109/TPAMI.2019.2913372]
   Bai YC, 2012, PROC CVPR IEEE, P1854, DOI 10.1109/CVPR.2012.6247884
   Bailer C, 2014, LECT NOTES COMPUT SC, V8695, P170, DOI 10.1007/978-3-319-10584-0_12
   Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bhat G, 2018, LECT NOTES COMPUT SC, V11206, P493, DOI 10.1007/978-3-030-01216-8_30
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Chen K, 2019, IEEE T MULTIMEDIA, V21, P86, DOI 10.1109/TMM.2018.2846405
   Choi J, 2018, PROC CVPR IEEE, P479, DOI 10.1109/CVPR.2018.00057
   Dai KN, 2019, PROC CVPR IEEE, P4665, DOI 10.1109/CVPR.2019.00480
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2016, PROC CVPR IEEE, P1430, DOI 10.1109/CVPR.2016.159
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Danelljan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P621, DOI 10.1109/ICCVW.2015.84
   Danelljan Martin, 2014, P BRIT MACH VIS C 20
   Dong XP, 2018, LECT NOTES COMPUT SC, V11217, P472, DOI 10.1007/978-3-030-01261-8_28
   Fan H, 2017, IEEE I CONF COMP VIS, P5487, DOI 10.1109/ICCV.2017.585
   Gao J., 2018, IEEE T PATTERN ANAL
   Grabner H., 2006, BMVC, P47
   Han B, 2017, PROC CVPR IEEE, P521, DOI 10.1109/CVPR.2017.63
   Hare S, 2016, IEEE T PATTERN ANAL, V38, P2096, DOI 10.1109/TPAMI.2015.2509974
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Hong ZB, 2015, PROC CVPR IEEE, P749, DOI 10.1109/CVPR.2015.7298675
   Goodfellow IJ, 2014, Arxiv, DOI [arXiv:1406.2661, DOI 10.1145/3422622]
   Jia X, 2012, PROC CVPR IEEE, P1822, DOI 10.1109/CVPR.2012.6247880
   Jung I., 2018, P ECCV, P83
   Kristan M., 2018, LECT NOTES COMPUT SC, P3, DOI DOI 10.1007/978-3-030-11009-3_1
   Kristan M, 2016, LECT NOTES COMPUT SC, V9914, P777, DOI 10.1007/978-3-319-48881-3_54
   Kuncheva LI, 2003, MACH LEARN, V51, P181, DOI 10.1023/A:1022859003006
   Leistner C, 2010, LECT NOTES COMPUT SC, V6316, P29, DOI 10.1007/978-3-642-15567-3_3
   Li HX, 2015, LECT NOTES COMPUT SC, V9007, P194, DOI 10.1007/978-3-319-16814-2_13
   Li Y, 2015, LECT NOTES COMPUT SC, V8926, P254, DOI 10.1007/978-3-319-16181-5_18
   Liang PP, 2015, IEEE T IMAGE PROCESS, V24, P5630, DOI 10.1109/TIP.2015.2482905
   Liu GG, 2018, IEEE T MULTIMEDIA, V20, P2949, DOI 10.1109/TMM.2018.2844685
   Lu XK, 2018, LECT NOTES COMPUT SC, V11218, P369, DOI 10.1007/978-3-030-01264-9_22
   Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352
   Meshgi K, 2018, PROC CVPR IEEE, P4814, DOI 10.1109/CVPR.2018.00506
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Nam H., 2016, CORR
   Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465
   Nickel K, 2008, LECT NOTES COMPUT SC, V5305, P514, DOI 10.1007/978-3-540-88693-8_38
   Pu S., 2018, ADV NEURAL INFORM PR, P1
   Qi YK, 2016, PROC CVPR IEEE, P4303, DOI 10.1109/CVPR.2016.466
   Ruan WJ, 2019, IEEE T MULTIMEDIA, V21, P1122, DOI 10.1109/TMM.2018.2872897
   Saffari A, 2010, LECT NOTES COMPUT SC, V6313, P776
   Simonyan K, 2015, IEEE INT C ICLR
   Song YB, 2018, PROC CVPR IEEE, P8990, DOI 10.1109/CVPR.2018.00937
   Sun C, 2018, PROC CVPR IEEE, P489, DOI 10.1109/CVPR.2018.00058
   Wang D, 2013, IEEE T IMAGE PROCESS, V22, P314, DOI 10.1109/TIP.2012.2202677
   Wang LJ, 2015, IEEE I CONF COMP VIS, P3119, DOI 10.1109/ICCV.2015.357
   Wang N., 2013, Advances in Neural Information Processing Systems, P809
   Wang NY, 2015, IEEE I CONF COMP VIS, P3101, DOI 10.1109/ICCV.2015.355
   Wang N, 2018, PROC CVPR IEEE, P4844, DOI 10.1109/CVPR.2018.00509
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Yu XY, 2019, IEEE T NEUR NET LEAR, V30, P2805, DOI 10.1109/TNNLS.2018.2886017
   Yun S, 2017, PROC CVPR IEEE, P1349, DOI 10.1109/CVPR.2017.148
   Zhang JM, 2014, LECT NOTES COMPUT SC, V8694, P188, DOI 10.1007/978-3-319-10599-4_13
   Zhang KH, 2013, PATTERN RECOGN, V46, P397, DOI 10.1016/j.patcog.2012.07.013
   Zhang TZ, 2017, PROC CVPR IEEE, P4819, DOI [10.1109/CVPR.2017.512, 10.1109/ICCV.2017.469]
   Zhang TZ, 2012, PROC CVPR IEEE, P2042, DOI 10.1109/CVPR.2012.6247908
   Zhu G, 2016, PROC CVPR IEEE, P943, DOI 10.1109/CVPR.2016.108
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 64
TC 5
Z9 5
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2020
VL 22
IS 10
BP 2698
EP 2710
DI 10.1109/TMM.2019.2958759
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NU6VI
UT WOS:000573779600003
DA 2024-07-18
ER

PT J
AU Kuang, Q
   Jin, X
   Zhao, QP
   Zhou, B
AF Kuang, Qi
   Jin, Xin
   Zhao, Qinping
   Zhou, Bin
TI Deep Multimodality Learning for UAV Video Aesthetic Quality Assessment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Aesthetic quality assessment; aerial video aesthetic; deep multimodality
   learning
ID OBSTACLE AVOIDANCE; CLASSIFICATION; CATEGORIZATION; GENERATION; PHOTO
AB Despite the growing number of unmanned aerial vehicles (UAVs) and aerial videos, there is a paucity of studies focusing on the aesthetics of aerial videos that can provide valuable information for improving the aesthetic quality of aerial photography. In this article, we present a method of deep multimodality learning for UAV video aesthetic quality assessment. More specifically, a multistream framework is designed to exploit aesthetic attributes from multiple modalities, including spatial appearance, drone camera motion, and scene structure. A novel specially designed motion stream network is proposed for this new multistream framework. We construct a dataset with 6,000 UAV video shots captured by drone cameras. Our model can judge whether a UAV video was shot by professional photographers or amateurs together with the scene type classification. The experimental results reveal that our method outperforms the video classification methods and traditional SVM-based methods for video aesthetics. In addition, we present three application examples of UAV video grading, professional segment detection and aesthetic-based UAV path planning using the proposed method.
C1 [Kuang, Qi; Zhao, Qinping; Zhou, Bin] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Sch Comp Sci & Engn, Beijing 100191, Peoples R China.
   [Jin, Xin] Beijing Elect Sci & Technol Inst, Beijing 100070, Peoples R China.
C3 Beihang University; Beijing Electronic Science & Technology Institute
RP Zhou, B (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Sch Comp Sci & Engn, Beijing 100191, Peoples R China.
EM kuangqi@buaa.edu.cn; jinxin@besti.edu.cn; zhaoqp@buaa.edu.cn;
   zhoubin@buaa.edu.cn
RI jin, xin/GQZ-5811-2022
FU National Natural Science Foundation of China [U1736217, 61932003];
   National Key R&D Program of China [2019YFF0302902]; Preresearch Project
   of theManned Space Flight [060601]; Open Project Program of the State
   Key Laboratory of Virtual Reality Technology and Systems, Beihang
   University [VRLAB2019C03]; National Defense Technology Innovation
   Program of China
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants U1736217 and 61932003, in part by the
   National Key R&D Program of China under Grant 2019YFF0302902, in part by
   the Preresearch Project of theManned Space Flight (060601), in part by
   the Open Project Program of the State Key Laboratory of Virtual Reality
   Technology and Systems, Beihang University, under Grant VRLAB2019C03,
   and in part by the National Defense Technology Innovation Program of
   China.
CR Apostolidis Evlampios, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P6583, DOI 10.1109/ICASSP.2014.6854873
   Bergmann P, 2018, IEEE ROBOT AUTOM LET, V3, P627, DOI 10.1109/LRA.2017.2777002
   Brezeale D, 2008, IEEE T SYST MAN CY C, V38, P416, DOI 10.1109/TSMCC.2008.919173
   Chen HD, 2013, IEEE T AERO ELEC SYS, V49, P840, DOI 10.1109/TAES.2013.6494384
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dhar S, 2011, PROC CVPR IEEE, P1657, DOI 10.1109/CVPR.2011.5995467
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Engel J, 2018, IEEE T PATTERN ANAL, V40, P611, DOI 10.1109/TPAMI.2017.2658577
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Iacono M, 2018, ROBOT AUTON SYST, V106, P38, DOI 10.1016/j.robot.2018.04.005
   Jiang YG, 2018, IEEE T PATTERN ANAL, V40, P352, DOI 10.1109/TPAMI.2017.2670560
   Jin X, 2018, AAAI CONF ARTIF INTE, P77
   Jin X, 2019, IET COMPUT VIS, V13, P206, DOI 10.1049/iet-cvi.2018.5249
   Junfeng Yao, 2010, Proceedings of the Seventh International Conference on Information Technology: New Generations (ITNG 2010), P1154, DOI 10.1109/ITNG.2010.53
   Kao YY, 2015, IEEE IMAGE PROC, P1583, DOI 10.1109/ICIP.2015.7351067
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Ke Y., 2006, P IEEE COMP SOC C CO, V1, P419, DOI DOI 10.1109/CVPR.2006.303
   Kong S, 2016, LECT NOTES COMPUT SC, V9905, P662, DOI 10.1007/978-3-319-46448-0_40
   Lan ZZ, 2017, IEEE COMPUT SOC CONF, P1219, DOI 10.1109/CVPRW.2017.161
   Lee HJ, 2017, IEEE T MULTIMEDIA, V19, P1921, DOI 10.1109/TMM.2017.2687759
   Lin W.-H., 2002, Proceedings of the Tenth ACM International Conference on Multimedia, P323, DOI DOI 10.1145/641043.641075
   Lu X, 2015, IEEE I CONF COMP VIS, P990, DOI 10.1109/ICCV.2015.119
   Lu X, 2015, IEEE T MULTIMEDIA, V17, P2021, DOI 10.1109/TMM.2015.2477040
   Lu X, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P457, DOI 10.1145/2647868.2654927
   Luo YW, 2008, LECT NOTES COMPUT SC, V5304, P386
   Ma S, 2017, PROC CVPR IEEE, P722, DOI 10.1109/CVPR.2017.84
   Mai L, 2016, PROC CVPR IEEE, P497, DOI 10.1109/CVPR.2016.60
   Marchesotti L, 2011, IEEE I CONF COMP VIS, P1784, DOI 10.1109/ICCV.2011.6126444
   Mellinger D, 2011, IEEE INT CONF ROBOT, P2520
   Moorthy AK, 2010, LECT NOTES COMPUT SC, V6315, P1, DOI 10.1007/978-3-642-15555-0_1
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Nguyen PDH, 2018, J INTELL ROBOT SYST, V89, P27, DOI 10.1007/s10846-017-0478-9
   Nishiyama M, 2011, PROC CVPR IEEE, P33, DOI 10.1109/CVPR.2011.5995539
   Niu YZ, 2012, IEEE T CIRC SYST VID, V22, P1037, DOI 10.1109/TCSVT.2012.2189689
   Sheng K., 2018, P SIGGRAPH AS TECH B, DOI DOI 10.1145/3283254.3283260
   Simonyan K, 2014, ADV NEUR IN, V27
   Strasdat H, 2011, ROBOTICS: SCIENCE AND SYSTEMS VI, P73
   Suresh V, 2004, LECT NOTES COMPUT SC, V3316, P726
   Tian XM, 2015, IEEE T MULTIMEDIA, V17, P2035, DOI 10.1109/TMM.2015.2479916
   Tong HG, 2004, LECT NOTES COMPUT SC, V3331, P198
   Tzelepis C, 2016, IEEE IMAGE PROC, P2410, DOI 10.1109/ICIP.2016.7532791
   Wang LM, 2018, PROC CVPR IEEE, P1430, DOI 10.1109/CVPR.2018.00155
   Wei Zu, 2018, 2018 IEEE International Conference on Mechatronics and Automation (ICMA), P1563, DOI 10.1109/ICMA.2018.8484400
   Wu ZX, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P791, DOI 10.1145/2964284.2964328
   Yang XD, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P978, DOI 10.1145/2964284.2964297
   Yang Y, 2013, IEEE T MULTIMEDIA, V15, P661, DOI 10.1109/TMM.2012.2237023
   Yeh HH, 2013, IEEE T MULTIMEDIA, V15, P1944, DOI 10.1109/TMM.2013.2280250
   Yuan X, 2006, IEEE IMAGE PROC, P2905, DOI 10.1109/ICIP.2006.313037
   Zhang FL, 2018, IEEE T MULTIMEDIA, V20, P1987, DOI 10.1109/TMM.2018.2790163
   Zhang J, 2019, IEEE T MULTIMEDIA, V21, P2518, DOI 10.1109/TMM.2019.2907453
   Zhang LM, 2014, IEEE T IMAGE PROCESS, V23, DOI 10.1109/TIP.2014.2303650
   Zhang XD, 2019, IEEE T MULTIMEDIA, V21, P2815, DOI 10.1109/TMM.2019.2911428
   Zhou YH, 2005, PROCEEDINGS OF 2005 INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND CYBERNETICS, VOLS 1-9, P5398
NR 56
TC 14
Z9 15
U1 2
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2020
VL 22
IS 10
BP 2623
EP 2634
DI 10.1109/TMM.2019.2960656
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NT0FL
UT WOS:000572628000009
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Xu, C
   Leng, B
   Chen, B
   Zhang, C
   Zhou, XC
AF Xu, Cheng
   Leng, Biao
   Chen, Bo
   Zhang, Cheng
   Zhou, Xiaochen
TI Learning Discriminative and Generative Shape Embeddings for
   Three-Dimensional Shape Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Shape; Solid modeling; Task analysis; Image
   sequences; Two dimensional displays; Computational modeling; 3D shape
   retrieval; feature aggregation; recurrent neural network
ID CONVOLUTIONAL NEURAL-NETWORKS; MODEL; RECOGNITION; COVARIANCE
AB As an important solution for 3D shape retrieval, a multi-view shape descriptor has achieved impressive performance. One crucial part of view-based shape descriptors is to interpret 3D structures through various 2D observations. Most existing methods like MVCNN believe that a strong classification model trained with deep learning, can often provide an efficient shape embedding for 3D shape retrieval. However, these methods pay much attention to discriminative models and none of them necessarily incorporate the underlying 3D properties of the objects from 2D images. In this paper, we present a novel encoder-decoder recurrent feature aggregation network (ERFA-Net) to address this problem. Aiming at emphasizing the 3D properties of 3D shapes in the fusion of multiple view features, 3D properties prediction tasks are introduced into the 3D shape retrieval. Specifically, an image sequence of the shape is recurrently aggregated into a discriminative shape embedding based on LSTM network, and then this latent shape embedding is trained to predict the original voxel grids and estimate images of unseen viewpoints. This generation task gives an effective supervision which makes the network exploit 3D properties of shapes through various 2D images. Our method achieves the state-of-the-art performance for 3D shape retrieval, on two large-scale 3D shape datasets, ModelNet and ShapeNetCore55. Extensive experiments show that the proposed 3D representation performs robust discrimination against view occlusion, and strong generation ability for various 3D shape tasks.
C1 [Xu, Cheng; Leng, Biao] Beihang Univ, Sch Comp Sci & Engn, Beijing 100191, Peoples R China.
   [Leng, Biao] Beihang Univ, Beijing Adv Innovat Ctr Big Data & Brain Comp, Beijing 100191, Peoples R China.
   [Chen, Bo] Beihang Univ, Sch Sino French Engn, Beijing 100191, Peoples R China.
   [Zhang, Cheng] Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.
   [Zhou, Xiaochen] Washington Univ, Sch Engn & Appl Sci, St Louis, MO 63130 USA.
C3 Beihang University; Beihang University; Beihang University; Carnegie
   Mellon University; Washington University (WUSTL)
RP Leng, B (corresponding author), Beihang Univ, Sch Comp Sci & Engn, Beijing 100191, Peoples R China.; Leng, B (corresponding author), Beihang Univ, Beijing Adv Innovat Ctr Big Data & Brain Comp, Beijing 100191, Peoples R China.
EM cxu@buaa.edu.cn; lengbiao@buaa.edu.cn; florentchen@buaa.edu.cn;
   chengz2@andrew.cmu.edu; zhouxiaochen@wustl.edu
RI Chen, Bo/B-3447-2018
OI Zhou, Xiaochen/0000-0003-2280-7799
FU National Natural Science Foundation of China [61972014]; Beijing
   Municipal Natural Science Foundation [L182014]; National Key R&D Program
   of China [2019YFB2102402]; State Key Laboratory of Software Development
   Environment [SKLSDE-2019ZX-19]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61972014, in part by the Beijing
   Municipal Natural Science Foundation underGrant L182014, in part by
   theNational Key R&D Program of China under Grant 2019YFB2102402, and in
   part by the project of the State Key Laboratory of Software Development
   Environment under Grant SKLSDE-2019ZX-19.
CR [Anonymous], 2016, INT CONF CLOUD COMPU
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], 2015, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2015.123
   Bai S, 2017, IEEE T MULTIMEDIA, V19, P1257, DOI 10.1109/TMM.2017.2652071
   Bai S, 2016, PROC CVPR IEEE, P5023, DOI 10.1109/CVPR.2016.543
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264
   Fang Y, 2015, PROC CVPR IEEE, P2319, DOI 10.1109/CVPR.2015.7298845
   Fei-Fei L, 2005, PROC CVPR IEEE, P524
   Feng YF, 2018, PROC CVPR IEEE, P264, DOI 10.1109/CVPR.2018.00035
   Fu MY, 2016, NEURAL PLAST, V2016, DOI 10.1155/2016/3512098
   Furuya T., 2016, P BRIT MACH VIS C
   Furuya T., 2009, P ACM INT C IM VID R
   Gao Y, 2012, IEEE T IMAGE PROCESS, V21, P4290, DOI 10.1109/TIP.2012.2199502
   Girdhar R, 2016, LECT NOTES COMPUT SC, V9910, P484, DOI 10.1007/978-3-319-46466-4_29
   Han ZZ, 2019, IEEE T CYBERNETICS, V49, P481, DOI 10.1109/TCYB.2017.2778764
   Han ZZ, 2016, IEEE T IMAGE PROCESS, V25, P5331, DOI 10.1109/TIP.2016.2605920
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Johns E, 2016, PROC CVPR IEEE, P3813, DOI 10.1109/CVPR.2016.414
   Kanezaki A, 2018, PROC CVPR IEEE, P5010, DOI 10.1109/CVPR.2018.00526
   Kazhdan M., 2003, Symposium on Geometry Processing, P156
   Kazhdan M., 2006, P 4 EUR S GEOM PROC, P61, DOI DOI 10.2312/SGP/SGP06/061-070
   Kazhdan M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487237
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kulkarni T. D, 2015, DEEP CONVOLUTIONAL I, V71, P2539
   Lalos AS, 2017, IEEE T MULTIMEDIA, V19, P41, DOI 10.1109/TMM.2016.2605927
   Leng BA, 2016, INFORM SCIENCES, V366, P188, DOI 10.1016/j.ins.2015.08.007
   Lin CH, 2018, AAAI CONF ARTIF INTE, P7114
   Niu C., 2018, STRUCTURE, V4096
   Papadakis P, 2007, PATTERN RECOGN, V40, P2437, DOI 10.1016/j.patcog.2006.12.026
   Papadakis P, 2010, INT J COMPUT VISION, V89, P177, DOI 10.1007/s11263-009-0281-6
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Savva M., 2016, P EUR WORKSH 3D OBJ, P89
   Sfikas K, 2018, COMPUT GRAPH-UK, V71, P208, DOI 10.1016/j.cag.2017.12.001
   Shi BG, 2015, IEEE SIGNAL PROC LET, V22, P2339, DOI 10.1109/LSP.2015.2480802
   Song Bai, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P774, DOI 10.1109/ICCV.2017.90
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Tabia H, 2015, IEEE T MULTIMEDIA, V17, P1591, DOI 10.1109/TMM.2015.2457676
   Tatsuma A, 2016, IEICE T INF SYST, VE99D, P1711, DOI 10.1587/transinf.2015EDL8212
   Wang NY, 2018, LECT NOTES COMPUT SC, V11215, P55, DOI 10.1007/978-3-030-01252-6_4
   Wu PF, 2017, IEEE T MULTIMEDIA, V19, P266, DOI 10.1109/TMM.2016.2612761
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xie J, 2017, IEEE T MULTIMEDIA, V19, P2463, DOI 10.1109/TMM.2017.2698200
   Xu C, 2018, AAAI CONF ARTIF INTE, P7428
   Xu K, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980224
   Yi L, 2017, PROC CVPR IEEE, P6584, DOI 10.1109/CVPR.2017.697
   Zhou B., 2014, ADV NEURAL INFORM PR, V27, P487
   Zhu ZT, 2014, 2014 INTERNATIONAL CONFERENCE ON SECURITY, PATTERN ANALYSIS, AND CYBERNETICS (SPAC), P279, DOI 10.1109/SPAC.2014.6982699
NR 51
TC 5
Z9 5
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEPT
PY 2020
VL 22
IS 9
BP 2234
EP 2245
DI 10.1109/TMM.2019.2957933
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NE0TL
UT WOS:000562310200004
DA 2024-07-18
ER

PT J
AU Yang, LX
   Wu, D
   Cai, YM
   Shi, X
   Wu, Y
AF Yang, Lianxin
   Wu, Dan
   Cai, Yueming
   Shi, Xin
   Wu, Yan
TI Learning-Based User Clustering and Link Allocation for Content
   Recommendation Based on D2D Multicast Communications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Clustering algorithms; Resource management; Device-to-device
   communication; Multicast communication; Interference; Multicast
   algorithms; Games; D2D multicast communications; user clustering; link
   allocation; K-means algorithm; stochastic learning algorithm
ID CELLULAR NETWORKS; RESOURCE-ALLOCATION; POWER-CONTROL
AB Content recommendation based on device-to-device (D2D) multicast communications is expected to become a promising approach to improve local area services. Importantly, two main challenges should be considered: i) user clustering-in order to be tailored to recommend contents, the members in the same cluster should have great similarity in multiple characteristics, and ii) link allocation-we should use as little resource consumption and information exchange as possible while keeping the recommendation accuracy. In this paper, we firstly quantify the degree of the similarity between two target users with regard to multiple characteristics. Guided by such similarity, we define a clustering validity index in terms of between-within proportion (BWP) to characterize the clustering performance. Then, the issue of user clustering is modeled as a sum BWP maximum problem, and a user clustering algorithm based on modified K-means algorithm is designed to solve it in a fast-operating and low-complexity way. After user clustering, we model the issue of link allocation as a weighted aggregate interference minimization problem, and then transform it to an exact potential game. As such, a link allocation algorithm based on stochastic learning algorithm is proposed which helps to obtain the result of this game in a distributed way without complete information. Also, we analyze its convergence and optimality performance. Simulation results demonstrate the effectiveness of our proposed algorithms.
C1 [Yang, Lianxin; Wu, Dan; Cai, Yueming; Shi, Xin; Wu, Yan] Army Engn Univ PLA, Nanjing 210007, Peoples R China.
C3 Army Engineering University of PLA
RP Wu, D (corresponding author), Army Engn Univ PLA, Nanjing 210007, Peoples R China.
EM yanglianxin1228@126.com; wujing1958725@126.com; caiym@vip.sina.com;
   shi1995@plcu.edu.cn; wuyan1281@163.com
RI Shi, Xin/AAE-5900-2019
OI Shi, Xin/0000-0002-0243-1539; Wu, Yan/0000-0002-6237-9090
FU Jiangsu Provincial Natural Science Fund for Outstanding Young Scholars
   [BK20180028]; Natural Science Foundations of China [61671474]; Jiangsu
   Provincial Natural Science Fund for Excellent Young Scholars
   [BK20170089]
FX This work was supported in part by the Jiangsu Provincial Natural
   Science Fund for Outstanding Young Scholars under Grant No. BK20180028,
   the Natural Science Foundations of China under Grant No. 61671474, and
   the Jiangsu Provincial Natural Science Fund for Excellent Young Scholars
   under Grant No. BK20170089.
CR Amer R, 2018, IEEE T WIREL COMMUN, V17, P6108, DOI 10.1109/TWC.2018.2854603
   [Anonymous], ELECT TECHNOL APPL
   Asadi A, 2017, IEEE T MOBILE COMPUT, V16, P2246, DOI 10.1109/TMC.2016.2621041
   Bhardwaj A, 2015, 2015 IEEE 26TH ANNUAL INTERNATIONAL SYMPOSIUM ON PERSONAL, INDOOR, AND MOBILE RADIO COMMUNICATIONS (PIMRC), P1498, DOI 10.1109/PIMRC.2015.7343535
   Bin Gao, 2019, IEEE INFOCOM 2019 - IEEE Conference on Computer Communications, P1459, DOI 10.1109/INFOCOM.2019.8737543
   Cao Y, 2016, IEEE T MOBILE COMPUT, V15, P1528, DOI 10.1109/TMC.2015.2461214
   Dominic S, 2018, IEEE COMMUN LETT, V22, P388, DOI 10.1109/LCOMM.2017.2771778
   Feng L, 2018, IEEE COMMUN MAG, V56, P112, DOI 10.1109/MCOM.2018.1700667
   Gong WR, 2015, WIRELESS PERS COMMUN, V85, P1261, DOI 10.1007/s11277-015-2839-7
   Huang YF, 2018, PROCEEDINGS OF 2018 TENTH INTERNATIONAL CONFERENCE ON ADVANCED COMPUTATIONAL INTELLIGENCE (ICACI), P193, DOI 10.1109/ICACI.2018.8377605
   Jia LL, 2019, IEEE T VEH TECHNOL, V68, P1646, DOI 10.1109/TVT.2018.2889336
   Jin YQ, 2017, INT CON DISTR COMP S, P1301, DOI 10.1109/ICDCS.2017.236
   [寇兰 Kou Lan], 2019, [计算机应用研究, Application Research of Computers], V36, P1849
   Kumar N, 2018, IEEE T NETW SERV MAN, V15, P718, DOI 10.1109/TNSM.2018.2807594
   Law L., 2009, P IEEE GLOBECOM, P1
   Li Y, 2016, 2016 16TH INTERNATIONAL SYMPOSIUM ON COMMUNICATIONS AND INFORMATION TECHNOLOGIES (ISCIT), P53, DOI 10.1109/ISCIT.2016.7751590
   Liu J., 2016, ACM T MULTIM COMPUT, V12, P1, DOI DOI 10.2147/IJN.S118938
   Liu JJ, 2016, IEEE J SEL AREA COMM, V34, P163, DOI 10.1109/JSAC.2015.2452492
   Malandrino F, 2014, IEEE COMMUN MAG, V52, P94, DOI 10.1109/MCOM.2014.6957148
   Meshgi H, 2017, IEEE T VEH TECHNOL, V66, P8357, DOI 10.1109/TVT.2017.2691470
   Moghaddam SS, 2018, PROCEEDINGS OF THE 2018 7TH INTERNATIONAL CONFERENCE ON COMPUTER AND COMMUNICATION ENGINEERING (ICCCE), P228, DOI 10.1109/ICCCE.2018.8539276
   Monderer D, 1996, GAME ECON BEHAV, V14, P124, DOI 10.1006/game.1996.0044
   Pan Zhao, 2016, 2016 19th International Symposium on Wireless Personal Multimedia Communications (WPMC), P518
   Peng B., 2013, PROC 78 IEEE VEH TEC, P1
   Peng B, 2013, 2013 IEEE 24TH INTERNATIONAL SYMPOSIUM ON PERSONAL, INDOOR, AND MOBILE RADIO COMMUNICATIONS (PIMRC), P2383, DOI 10.1109/PIMRC.2013.6666544
   Qiong Chen, 2019, 2019 IEEE 39th International Conference on Distributed Computing Systems (ICDCS). Proceedings, P1040, DOI 10.1109/ICDCS.2019.00107
   Ren FX, 2018, INT CONF MEAS, P434, DOI 10.1109/ICMTMA.2018.00112
   Shamaei S, 2019, IEEE T MOBILE COMPUT, V18, P2091, DOI 10.1109/TMC.2018.2871073
   Sun LF, 2018, IEEE T MULTIMEDIA, V20, P3414, DOI 10.1109/TMM.2018.2834861
   Wang L, 2016, IEEE ACCESS, V4, P1971, DOI 10.1109/ACCESS.2016.2546685
   Wu D, 2017, IEEE T MULTIMEDIA, V19, P2571, DOI 10.1109/TMM.2017.2700621
   Wu QH, 2013, IEEE T VEH TECHNOL, V62, P4524, DOI 10.1109/TVT.2013.2269152
   Wu XL, 2014, IET COMMUN, V8, P2805, DOI 10.1049/iet-com.2013.1041
   Xu R, 2005, IEEE T NEURAL NETWOR, V16, P645, DOI 10.1109/TNN.2005.845141
   Yan C., IEEE T MULTIMEDIA
   Yan CG, 2018, IEEE T MULTIMEDIA, V20, P3389, DOI 10.1109/TMM.2018.2838320
   Yang LX, 2018, IEEE ACCESS, V6, P36092, DOI 10.1109/ACCESS.2018.2849204
   Yang P, 2019, IEEE T MULTIMEDIA, V21, P915, DOI 10.1109/TMM.2018.2870521
   [杨善林 YANG Shanlin], 2006, [系统工程理论与实践, Systems Engineering-Theory & Practice], V26, P97
   Zhai Dong-hai, 2014, Application Research of Computers, V31, P713, DOI 10.3969/j.issn.1001-3695.2014.03.017
   Zhang GP, 2016, IEEE WIREL COMMUN, V23, P68, DOI 10.1109/MWC.2016.7553028
   Zhao XY, 2018, IEEE T VEH TECHNOL, V67, P9583, DOI 10.1109/TVT.2018.2858254
   Zhao Z, 2018, IEEE T MULTIMEDIA, V20, P430, DOI 10.1109/TMM.2017.2740022
   Zheng JC, 2019, IEEE T MOBILE COMPUT, V18, P771, DOI 10.1109/TMC.2018.2847337
   Zheng JC, 2017, IEEE T WIREL COMMUN, V16, P4139, DOI 10.1109/TWC.2016.2646346
   Zheng JC, 2015, IEEE COMMUN LETT, V19, P251, DOI 10.1109/LCOMM.2014.2377156
   Zhou Shi-bing, 2010, Journal of Computer Applications, V30, P1995, DOI 10.3724/SP.J.1087.2010.01995
NR 47
TC 14
Z9 14
U1 1
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2020
VL 22
IS 8
BP 2111
EP 2125
DI 10.1109/TMM.2019.2949434
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MR2MS
UT WOS:000553424500016
DA 2024-07-18
ER

PT J
AU Yang, S
   Lin, GS
   Jiang, QP
   Lin, WS
AF Yang, Sheng
   Lin, Guosheng
   Jiang, Qiuping
   Lin, Weisi
TI A Dilated Inception Network for Visual Saliency Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Computational modeling; Predictive models; Feature
   extraction; Spatial resolution; Computer architecture; Solid modeling;
   Visual attention; saliency detection; eye fixation prediction;
   convolutional neural networks; dilated convolution; inception module
ID ATTENTION; QUALITY
AB Recently, with the advent of deep convolutional neural networks (DCNN), the improvements in visual saliency prediction research are impressive. One possible direction to approach the next improvement is to fully characterize the multi-scale saliency-influential factors with a computationally-friendly module in DCNN architectures. In this work, we propose an end-to-end dilated inception network (DINet) for visual saliency prediction. It captures multi-scale contextual features effectively with very limited extra parameters. Instead of utilizing parallel standard convolutions with different kernel sizes as the existing inception module, our proposed dilated inception module (DIM) uses parallel dilated convolutions with different dilation rates which can significantly reduce the computation load while enriching the diversity of receptive fields in feature maps. Moreover, the performance of our saliency model is further improved by using a set of linear normalization-based probability distribution distance metrics as loss functions. As such, we can formulate saliency prediction as a global probability distribution prediction task for better saliency inference instead of a pixel-wise regression problem. Experimental results on several challenging saliency benchmark datasets demonstrate that our DINet with proposed loss functions can achieve state-of-the-art performance with shorter inference time.
C1 [Yang, Sheng; Lin, Guosheng; Lin, Weisi] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
   [Jiang, Qiuping] Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Peoples R China.
C3 Nanyang Technological University; Ningbo University
RP Lin, WS (corresponding author), Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
EM syang014@e.ntu.edu.sg; gslin@ntu.edu.sg; jiangqiuping@nbu.edu.cn;
   wslin@ntu.edu.sg
RI Qiuping, Jiang/AAO-2830-2021; Lin, Weisi/A-3696-2011; Lin,
   Weisi/A-8011-2012; Lin, Guosheng/Q-4024-2017
OI Qiuping, Jiang/0000-0002-6025-9343; Lin, Weisi/0000-0001-9866-1947; Lin,
   Guosheng/0000-0002-0329-7458; Yang, Sheng/0000-0003-4441-4857
FU Singapore Ministry of Education Tier-2 Fund [MOE2016-T2-2-057(S)];
   Natural Science Foundation of China [61901236]; NTU start-up grant; MOE
   Tier-1 Research Grant [RG126/17 (S)]
FX This work was supported in part by the Singapore Ministry of Education
   Tier-2 Fund MOE2016-T2-2-057(S) and in part by the Natural Science
   Foundation of China under Grant 61901236. The work of G. Lin was
   supported in part by an NTU start-up grant and in part by an MOE Tier-1
   Research Grant [RG126/17 (S)].
CR Abadi M, 2016, ACM SIGPLAN NOTICES, V51, P1, DOI [10.1145/3022670.2976746, 10.1145/2951913.2976746]
   [Anonymous], 2016, P INT C LEARN REPR
   [Anonymous], 2012, Technical Report
   Borji A, 2019, COMPUT VIS MEDIA, V5, P117, DOI 10.1007/s41095-019-0149-9
   Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89
   Bylinskii Z, 2019, IEEE T PATTERN ANAL, V41, P740, DOI 10.1109/TPAMI.2018.2815601
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen Y, 2017, 2017 4TH INTERNATIONAL CONFERENCE ON INFORMATION SCIENCE AND CONTROL ENGINEERING (ICISCE), P1467, DOI 10.1109/ICISCE.2017.306
   Chollet F, 2015, KERAS
   Cornia M, 2018, IEEE T IMAGE PROCESS, V27, P5142, DOI 10.1109/TIP.2018.2851672
   Cornia M, 2016, INT C PATT RECOG, P3488, DOI 10.1109/ICPR.2016.7900174
   Dodge SF, 2018, IEEE T IMAGE PROCESS, V27, P4080, DOI 10.1109/TIP.2018.2834826
   Evangelopoulos G, 2013, IEEE T MULTIMEDIA, V15, P1553, DOI 10.1109/TMM.2013.2267205
   Gao Y, 2015, IEEE T MULTIMEDIA, V17, P359, DOI 10.1109/TMM.2015.2389616
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P1098, DOI 10.1109/TMM.2016.2547343
   Hadizadeh H, 2014, IEEE T IMAGE PROCESS, V23, P19, DOI 10.1109/TIP.2013.2282897
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He S., 2018, WHAT CATCHES EYE VIS
   Huang X, 2015, IEEE I CONF COMP VIS, P262, DOI 10.1109/ICCV.2015.38
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jetley S, 2016, PROC CVPR IEEE, P5753, DOI 10.1109/CVPR.2016.620
   Jiang M, 2015, PROC CVPR IEEE, P1072, DOI 10.1109/CVPR.2015.7298710
   Jiang QP, 2018, IEEE T MULTIMEDIA, V20, P2035, DOI 10.1109/TMM.2017.2763321
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Kim H, 2015, IEEE T MULTIMEDIA, V17, P2198, DOI 10.1109/TMM.2015.2493367
   King DB, 2015, ACS SYM SER, V1214, P1
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kruthiventi SSS, 2017, IEEE T IMAGE PROCESS, V26, P4446, DOI 10.1109/TIP.2017.2710620
   Kruthiventi SSS, 2016, PROC CVPR IEEE, P5781, DOI 10.1109/CVPR.2016.623
   Kümmerer M, 2017, IEEE I CONF COMP VIS, P4799, DOI 10.1109/ICCV.2017.513
   Kummerer M., 2015, INT C LEARN REPR ICL
   Li SX, 2018, IEEE T MULTIMEDIA, V20, P155, DOI 10.1109/TMM.2017.2721544
   Lin SS, 2013, IEEE T MULTIMEDIA, V15, P359, DOI 10.1109/TMM.2012.2228475
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu N, 2018, IEEE T IMAGE PROCESS, V27, P3264, DOI 10.1109/TIP.2018.2817047
   Liu NA, 2018, IEEE T NEUR NET LEAR, V29, P392, DOI 10.1109/TNNLS.2016.2628878
   Liu N, 2015, PROC CVPR IEEE, P362, DOI 10.1109/CVPR.2015.7298633
   Liu ZC, 2018, PROC CVPR IEEE, P6936, DOI 10.1109/CVPR.2018.00725
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Odena A., 2016, DISTILL, V1, P3, DOI [10.23915/distill.00003., DOI 10.23915/DISTILL, 10.23915/distill.00003, DOI 10.23915/DISTILL.00003]
   Pan J., 2017, PROC IEEE C COMPUT V
   Pan JT, 2016, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2016.71
   Riche N, 2013, IEEE I CONF COMP VIS, P1153, DOI 10.1109/ICCV.2013.147
   Shi XJ, 2015, ADV NEUR IN, V28
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sitzmann V, 2018, IEEE T VIS COMPUT GR, V24, P1633, DOI 10.1109/TVCG.2018.2793599
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Vig E, 2014, PROC CVPR IEEE, P2798, DOI 10.1109/CVPR.2014.358
   Visin F., 2015, Renet: A recurrent neural network based alternative to convolutional networks
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P2368, DOI 10.1109/TIP.2017.2787612
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Ye LW, 2017, IEEE T MULTIMEDIA, V19, P1742, DOI 10.1109/TMM.2017.2693022
   Yu F, 2017, PROC CVPR IEEE, P636, DOI 10.1109/CVPR.2017.75
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang JM, 2013, IEEE I CONF COMP VIS, P153, DOI 10.1109/ICCV.2013.26
   Zhou BL, 2014, ADV NEUR IN, V27
NR 57
TC 93
Z9 97
U1 2
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2020
VL 22
IS 8
BP 2163
EP 2176
DI 10.1109/TMM.2019.2947352
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MR2MS
UT WOS:000553424500020
OA Green Submitted, Green Accepted
DA 2024-07-18
ER

PT J
AU Wang, YB
   Zhang, LQ
   Nie, FP
   Li, XG
   Chen, ZJ
   Wang, FQ
AF Wang, Yuebin
   Zhang, Liqiang
   Nie, Feiping
   Li, Xingang
   Chen, Zhijun
   Wang, Faqiang
TI WeGAN: Deep Image Hashing With Weighted Generative Adversarial Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Gallium nitride; Generative adversarial networks; Deep learning;
   Uncertainty; Semantics; Task analysis; Linear programming; Image
   hashing; generative adversarial networks; image set; uncertainties
   between images and tags
ID RETRIEVAL; QUANTIZATION
AB Image hashing has been widely used in image retrieval tasks. Many existing methods generate hashing codes based on image feature representations. They rarely consider the rich information such as image clustering information contained in the image set as well as uncertain relationships between images and tags simultaneously. In this paper, we develop a Weighted Generative Adversarial Networks (WeGAN) to transfer the clustering information of images to construct the hashing code. WeGAN consists three modules: 1) a hashing learning process for transferring knowledge of the image set to hashing codes of single images; 2) by means of hashing codes, a module to generate image content, tag representation, and their joint information which reflects the correlation between the image and the corresponding tags; 3) a discriminator to distinguish the generated data from the original source, and then formulating three loss functions. Different weights are assigned to these loss functions in order to deal with the uncertainties between images and tags. Through introducing the image set to process the image hashing with different tags, WeGAN can naturally provide the information of clustering results, which is useful for image hashing with multi-tags. The generated hashing code has the ability to dynamically process the uncertain relationships between images and tags. Experiments on three challenging datasets show that WeGAN outperforms the state-of-the-art methods.
C1 [Wang, Yuebin] China Univ Geosci, Sch Land Sci & Technol, Beijing 100083, Peoples R China.
   [Wang, Yuebin] Shanxi Prov Key Lab Resources Environm & Disaster, Jinzhong 030600, Peoples R China.
   [Zhang, Liqiang; Li, Xingang] Beijing Normal Univ, State Key Lab Remote Sensing Sci, Fac Geog Sci, Beijing 100875, Peoples R China.
   [Nie, Feiping] Northwestern Polytech Univ, Sch Comp Sci, Xian 710072, Peoples R China.
   [Nie, Feiping] Northwestern Polytech Univ, Ctr Opt Imagery Anal & Learning, Xian 710072, Peoples R China.
   [Chen, Zhijun; Wang, Faqiang] Beijing Normal Univ, Sch Math Sci, Beijing 100875, Peoples R China.
C3 China University of Geosciences; Beijing Normal University; Northwestern
   Polytechnical University; Northwestern Polytechnical University; Beijing
   Normal University
RP Zhang, LQ (corresponding author), Beijing Normal Univ, State Key Lab Remote Sensing Sci, Fac Geog Sci, Beijing 100875, Peoples R China.
EM xxgcdxwyb@163.com; zhanglq@bnu.edu.cn; feipingnie@gmail.com;
   lixg95@126.com; zhijun.chen@mail.bnu.edu.cn; fqwang@mail.bnu.edu.cn
RI Nie, Feiping/B-3039-2012
OI Zhang, Liqiang/0000-0002-4175-7590; Wang, Yuebin/0000-0002-6978-4558;
   Nie, Feiping/0000-0002-0871-6519; Xingang, Li/0000-0003-2665-8671
FU National Natural Science Foundation of China [41801241]; National Key
   Research and Development Program of China [2018YFC0213600]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 41801241 and in part by the National Key
   Research and Development Program of China under Grant 2018YFC0213600.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Marco Bertini.
CR [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], DEEP REINFORCEMENT L
   Cao Y, 2017, PROC CVPR IEEE, P916, DOI 10.1109/CVPR.2017.104
   Cao Y, 2016, AAAI CONF ARTIF INTE, P3457
   Cao ZJ, 2017, IEEE I CONF COMP VIS, P5609, DOI 10.1109/ICCV.2017.598
   Chen X, 2016, ADV NEUR IN, V29
   Chua T.-S., 2009, CIVR, P1, DOI 10.1145/1646396.1646452
   Dai SC, 2016, CHIN CONT DECIS CONF, P20, DOI 10.1109/CCDC.2016.7530948
   Ding GG, 2014, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2014.267
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Gomez L, 2017, PROC CVPR IEEE, P2017, DOI 10.1109/CVPR.2017.218
   Gonçalves GR, 2018, SIBGRAPI, P110, DOI 10.1109/SIBGRAPI.2018.00021
   Gong YC, 2011, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2011.5995432
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Harandi M, 2015, IEEE I CONF COMP VIS, P4112, DOI 10.1109/ICCV.2015.468
   Hu D, 2019, IEEE T MULTIMEDIA, V21, P973, DOI 10.1109/TMM.2018.2866771
   Hu MQ, 2018, IEEE T IMAGE PROCESS, V27, P545, DOI 10.1109/TIP.2017.2749147
   Irie G, 2014, PROC CVPR IEEE, P2123, DOI 10.1109/CVPR.2014.272
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Jing CC, 2019, IEEE T MULTIMEDIA, V21, P782, DOI 10.1109/TMM.2018.2866222
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Kumar Shaishav, 2011, P 22 INT JOINT C ART, P1360, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-230
   Lai HJ, 2019, IEEE T CIRC SYST VID, V29, P2844, DOI 10.1109/TCSVT.2018.2869921
   Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947
   LeCun Y., 1990, NeurIPS, P396
   Li JA, 2017, PROC CVPR IEEE, P1951, DOI 10.1109/CVPR.2017.211
   Li XL, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1398, DOI 10.1145/3123266.3123355
   Li Y, 2015, PROC CVPR IEEE, P4758, DOI 10.1109/CVPR.2015.7299108
   Lin GS, 2014, PROC CVPR IEEE, P1971, DOI 10.1109/CVPR.2014.253
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liong VE, 2015, PROC CVPR IEEE, P2475, DOI 10.1109/CVPR.2015.7298862
   Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Lu JW, 2015, PROC CVPR IEEE, P1137, DOI 10.1109/CVPR.2015.7298717
   Ma Q, 2019, PATTERN RECOGN, V92, P156, DOI 10.1016/j.patcog.2019.03.022
   Niu YL, 2019, IEEE T IMAGE PROCESS, V28, P1720, DOI 10.1109/TIP.2018.2881928
   Qin J, 2017, PROC CVPR IEEE, P6728, DOI 10.1109/CVPR.2017.712
   Qiu ZF, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P225, DOI 10.1145/3077136.3080842
   Radford A., 2015, ARXIV
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Shen YM, 2017, IEEE I CONF COMP VIS, P4117, DOI 10.1109/ICCV.2017.441
   Song JK, 2019, IEEE T NEUR NET LEAR, V30, P3047, DOI 10.1109/TNNLS.2018.2851077
   Song JK, 2018, IEEE T IMAGE PROCESS, V27, P3210, DOI 10.1109/TIP.2018.2814344
   Song JK, 2018, PATTERN RECOGN, V75, P175, DOI 10.1016/j.patcog.2017.03.021
   Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572
   Wang J, 2014, PROC CVPR IEEE, P1386, DOI 10.1109/CVPR.2014.180
   Wang J, 2012, IEEE T PATTERN ANAL, V34, P2393, DOI 10.1109/TPAMI.2012.48
   Wang QF, 2014, LECT NOTES COMPUT SC, V8691, P378, DOI 10.1007/978-3-319-10578-9_25
   Wang RP, 2012, PROC CVPR IEEE, P2496, DOI 10.1109/CVPR.2012.6247965
   Wang W, 2017, PROC CVPR IEEE, P5749, DOI 10.1109/CVPR.2017.609
   Wang XH, 2018, IEEE T MULTIMEDIA, V20, P634, DOI 10.1109/TMM.2017.2749159
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2156
   Yang D, 2013, IEEE INT WORKSH COMP, P13, DOI 10.1109/CAMAD.2013.6708080
   Ye ZD, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P852, DOI 10.1145/3240508.3240560
   Zhang DQ, 2014, AAAI CONF ARTIF INTE, P2177
   Zhang J., 2019, IEEE T CIRC SYST VID, V29, P212, DOI DOI 10.1109/TCSVT.2017.2771332
   Zhang J, 2020, IEEE T CYBERNETICS, V50, P489, DOI 10.1109/TCYB.2018.2868826
   Zhang J, 2018, IEEE T MULTIMEDIA, V20, P2400, DOI 10.1109/TMM.2018.2804763
   Zhao F, 2015, PROC CVPR IEEE, P1556, DOI 10.1109/CVPR.2015.7298763
   Zhu J.-Y., 2017, IEEE I CONF COMP VIS, P2223, DOI DOI 10.1109/ICCV.2017.244
NR 63
TC 15
Z9 15
U1 2
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2020
VL 22
IS 6
BP 1458
EP 1469
DI 10.1109/TMM.2019.2947197
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU8YA
UT WOS:000538033100007
DA 2024-07-18
ER

PT J
AU Li, YX
   Li, S
   Chen, CLZ
   Hao, AM
   Qin, H
AF Li, Yunxiao
   Li, Shuai
   Chen, Chenglizhao
   Hao, Aimin
   Qin, Hong
TI Accurate and Robust Video Saliency Detection via Self-Paced Diffusion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Saliency detection; Proposals; Video sequences; Spatial coherence;
   Computational modeling; Optical imaging; Optical sensors; Video saliency
   detection; long-term saliency revealing; key frame strategy; self-paced
   saliency diffusion
ID OBJECT DETECTION; SEGMENTATION; OPTIMIZATION; FUSION
AB Conventional video saliency detection methods frequently follow the common bottom-up thread to estimate video saliency within the short-term fashion. As a result, such methods can not avoid the obstinate accumulation of errors when the collected low-level clues are constantly ill-detected. Also, being noticed that a portion of video frames, which are not nearby the current video frame over the time axis, may potentially benefit the saliency detection in the current video frame. Thus, we propose to solve the aforementioned problem using our newly-designed key frame strategy (KFS), whose core rationale is to utilize both the spatial-temporal coherency of the salient foregrounds and the objectness prior (i.e., how likely it is for an object proposal to contain an object of any class) to reveal the valuable long-term information. We could utilize all this newly-revealed long-term information to guide our subsequent "self-paced" saliency diffusion, which enables each key frame itself to determine its diffusion range and diffusion strength to correct those ill-detected video frames. At the algorithmic level, we first divide a video sequence into short-term frame batches, and the object proposals are obtained in a frame-wise manner. Then, for each object proposal, we utilize a pre-trained deep saliency model to obtain high-dimensional features in order to represent the spatial contrast. Since the contrast computation within multiple neighbored video frames (i.e., the non-local manner) is relatively insensitive to the appearance variation, those object proposals with high-quality low-level saliency estimation frequently exhibit strong similarity over the temporal scale. Next, the long-term common consistency (e.g., appearance models/movement patterns) of the salient foregrounds could be explicitly revealed via similarity analysis accordingly. We further boost the detection accuracy via long-term information guided saliency diffusion in a self-paced manner. We have conducted extensive experiments to compare our method with 16 state-of-the-art methods over 4 largest public available benchmarks, and all results demonstrate the superiority of our method in terms of both accuracy and robustness.
C1 [Li, Yunxiao; Li, Shuai; Chen, Chenglizhao; Hao, Aimin] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Chen, Chenglizhao] Qingdao Univ, Qingdao 266071, Peoples R China.
   [Qin, Hong] SUNY Stony Brook, Stony Brook, NY 11794 USA.
C3 Beihang University; Qingdao University; State University of New York
   (SUNY) System; State University of New York (SUNY) Stony Brook
RP Chen, CLZ (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
EM yunxiao_li@hotmail.com; lishuai@buaa.edu.cn; cclz123@163.com;
   ham_buaa@163.com; qin@cs.stonybrook.edu
FU National Key RAMP;D Program of China [2017YFF0106407]; National Natural
   Science Foundation of China [61190120, 61190124, 61190125, 61300067,
   61672077, 6167214, 61602341, 61532002, 61772277, 61806106, 61802215];
   Applied Basic Research Program of Qingdao [161013xx]; Capital Health
   Research and Development of Special [2016-1-4011]; Natural Science
   Foundation of Shandong Province [ZR2019BF011, ZR2019QF009]; National
   Science Foundation of USA [IIS1715985, IIS1812606]
FX This work was supported in part by National Key R&DProgram of China
   (2017YFF0106407), in part byNational Natural Science Foundation of China
   (61190120, 61190124, 61190125, 61300067, 61672077, 6167214, 61602341,
   61532002, 61772277, 61806106, and 61802215), in part by Applied Basic
   Research Program of Qingdao (161013xx), in part by Capital Health
   Research and Development of Special 2016-1-4011, in part by Natural
   Science Foundation of Shandong Province (ZR2019BF011 and ZR2019QF009),
   and in part by National Science Foundation of USA (IIS1715985 and
   IIS1812606). The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Susanto Rahardja.
   (Yunxiao Li and Shuai Li contributed equally to this work.)
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   [Anonymous], 2010, ACM MULTIMEDIA 2010
   Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Chen CLZ, 2018, IEEE T MULTIMEDIA, V20, P3324, DOI 10.1109/TMM.2018.2839523
   Chen CLZ, 2018, IEEE SIGNAL PROC LET, V25, P154, DOI 10.1109/LSP.2017.2775212
   Chen CZ, 2017, IEEE T IMAGE PROCESS, V26, P3156, DOI 10.1109/TIP.2017.2670143
   Chen CLZ, 2016, PATTERN RECOGN, V52, P410, DOI 10.1016/j.patcog.2015.09.033
   Chen CLZ, 2015, PATTERN RECOGN, V48, P2885, DOI 10.1016/j.patcog.2015.01.025
   Chen CLZ, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2403232
   Chen T, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618470
   Chen YY, 2019, IEEE T CIRC SYST VID, V29, P2567, DOI 10.1109/TCSVT.2017.2770319
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Cong RM, 2016, IEEE SIGNAL PROC LET, V23, DOI 10.1109/LSP.2016.2557347
   Danelljan M, 2014, PROC CVPR IEEE, P1090, DOI 10.1109/CVPR.2014.143
   Ding M, 2010, VISUAL COMPUT, V26, P721, DOI 10.1007/s00371-010-0448-8
   Evangelopoulos G, 2013, IEEE T MULTIMEDIA, V15, P1553, DOI 10.1109/TMM.2013.2267205
   Fang YM, 2017, IEEE T IMAGE PROCESS, V26, P4684, DOI 10.1109/TIP.2017.2721112
   Fang YM, 2014, IEEE T IMAGE PROCESS, V23, P3910, DOI 10.1109/TIP.2014.2336549
   Fang YM, 2014, IEEE T CIRC SYST VID, V24, P27, DOI 10.1109/TCSVT.2013.2273613
   Fang YM, 2012, IEEE T IMAGE PROCESS, V21, P3888, DOI 10.1109/TIP.2012.2199126
   Fu HZ, 2013, IEEE T IMAGE PROCESS, V22, P3766, DOI 10.1109/TIP.2013.2260166
   Fukuchi K, 2009, IEEE INT CON MULTI, P638, DOI 10.1109/ICME.2009.5202577
   Gao Z, 2014, IEEE T PATTERN ANAL, V36, P1975, DOI 10.1109/TPAMI.2014.2314663
   Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563
   Hou QB, 2019, IEEE T PATTERN ANAL, V41, P815, DOI 10.1109/TPAMI.2018.2815688
   Huang CR, 2014, IEEE T CIRC SYST VID, V24, P1336, DOI 10.1109/TCSVT.2014.2308652
   Itti L, 2001, NAT REV NEUROSCI, V2, P194, DOI 10.1038/35058500
   Jain SD, 2017, PROC CVPR IEEE, P2117, DOI 10.1109/CVPR.2017.228
   Ji P, 2014, LECT NOTES COMPUT SC, V8694, P204, DOI 10.1007/978-3-319-10599-4_14
   Kim H, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2425544
   Li FX, 2013, IEEE I CONF COMP VIS, P2192, DOI 10.1109/ICCV.2013.273
   Li GB, 2018, PROC CVPR IEEE, P3243, DOI 10.1109/CVPR.2018.00342
   Li GB, 2016, IEEE T IMAGE PROCESS, V25, P5012, DOI 10.1109/TIP.2016.2602079
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li SX, 2018, IEEE T MULTIMEDIA, V20, P155, DOI 10.1109/TMM.2017.2721544
   Li YS, 2015, COMPUT VIS IMAGE UND, V135, P126, DOI 10.1016/j.cviu.2015.01.011
   Liu C., 2009, Exploring New Representations and Applications for Motion Anal- ysis
   Liu Z, 2014, IEEE T CIRC SYST VID, V24, P1522, DOI 10.1109/TCSVT.2014.2308642
   Marchesotti L, 2009, IEEE I CONF COMP VIS, P2232, DOI 10.1109/ICCV.2009.5459467
   Mathe S, 2012, LECT NOTES COMPUT SC, V7573, P842, DOI 10.1007/978-3-642-33709-3_60
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Rahtu E, 2010, LECT NOTES COMPUT SC, V6315, P366, DOI 10.1007/978-3-642-15555-0_27
   Reinagel P, 1999, NETWORK-COMP NEURAL, V10, P341, DOI 10.1088/0954-898X/10/4/304
   Song HM, 2018, LECT NOTES COMPUT SC, V11215, P744, DOI 10.1007/978-3-030-01252-6_44
   Le TN, 2018, IEEE T IMAGE PROCESS, V27, P5002, DOI 10.1109/TIP.2018.2849860
   Wang LZ, 2016, LECT NOTES COMPUT SC, V9908, P825, DOI 10.1007/978-3-319-46493-0_50
   Wang Wenguan, 2018, IEEE Trans Image Process, V27, P38, DOI 10.1109/TIP.2017.2754941
   Wang WG, 2015, IEEE T IMAGE PROCESS, V24, P4185, DOI 10.1109/TIP.2015.2460013
   Wenguan Wang, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3395, DOI 10.1109/CVPR.2015.7298961
   Wright J, 2009, ADV NEURAL INFORM PR, P2080, DOI DOI 10.1109/NNSP.2000.889420
   Wu HF, 2014, VISUAL COMPUT, V30, P229, DOI 10.1007/s00371-013-0823-3
   Wu YY, 2017, IEEE T CIRC SYST VID, V27, P236, DOI 10.1109/TCSVT.2015.2493499
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yang H, 2015, IEEE T IMAGE PROCESS, V24, P4408, DOI 10.1109/TIP.2015.2465145
   Yang LJ, 2011, IEEE T MULTIMEDIA, V13, P1295, DOI 10.1109/TMM.2011.2162399
   Zhang Y, 2017, 2017 IEEE 3RD INTERNATIONAL FUTURE ENERGY ELECTRONICS CONFERENCE AND ECCE ASIA (IFEEC 2017-ECCE ASIA), P1328, DOI 10.1109/IFEEC.2017.7992236
   Zhou F, 2014, PROC CVPR IEEE, P3358, DOI 10.1109/CVPR.2014.429
   Zhou XF, 2018, IEEE T MULTIMEDIA, V20, P2993, DOI 10.1109/TMM.2018.2829605
   Zhou XW, 2013, IEEE T PATTERN ANAL, V35, P597, DOI 10.1109/TPAMI.2012.132
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 61
TC 41
Z9 42
U1 2
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2020
VL 22
IS 5
BP 1153
EP 1167
DI 10.1109/TMM.2019.2940851
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LJ3VZ
UT WOS:000530097200004
DA 2024-07-18
ER

PT J
AU Jian, MW
   Dong, JY
   Gong, MG
   Yu, H
   Nie, LQ
   Yin, YL
   Lam, KM
AF Jian, Muwei
   Dong, Junyu
   Gong, Maoguo
   Yu, Hui
   Nie, Liqiang
   Yin, Yilong
   Lam, Kin-Man
TI Learning the Traditional Art of Chinese Calligraphy via
   Three-Dimensional Reconstruction and Assessment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Surface reconstruction; Solid modeling;
   Image reconstruction; Geometry; Art; Ink; Image quality assessment;
   Chinese calligraphy; 3D surface geometry; 3D surface heightmap;
   Photometric Stereo
ID QUALITY ASSESSMENT; STROKE EXTRACTION; REGISTRATION; ILLUMINATION;
   CHARACTERS; CAPTURE; SYSTEM
AB The traditional art of Chinese calligraphy, reflecting the wisdom of the grass-roots community, is the soul of Chinese culture. Just like many other types of craftsmanship, it is part of the historical heritage and is worth conserving, from generation to generation. Since the movements of an ink brush are in a 3D style when Chinese calligraphy is written, they embody "The Power of Beauty," comprising various reflectance properties and rough-surface geometry. To truly understand the powerful significance and beauty of the art of Chinese calligraphy, in this paper, a 3D calligraphy reconstruction method, based on Photometric Stereo, is designed to capture the detailed appearance of the calligraphy's 3D surface geometry. For assessment, an Iterative Closest Point (ICP) algorithm is applied for registration of 3D intrinsic shapes between the Chinese calligraphy and the calligraphy fans' handwriting. Through matching these two sets of calligraphy characters, the designed system can give a score to the handwriting of a user. Experiments have been performed on Chinese calligraphy from different historical dynasties to evaluate the effectiveness of the proposed scheme, and experimental results show that the developed system is useful and provides a convenient method of calligraphy appreciation and assessment.
C1 [Jian, Muwei] Shandong Univ Finance & Econ, Sch Comp Sci & Technol, Jinan 250100, Peoples R China.
   [Jian, Muwei; Yu, Hui] Univ Portsmouth, Sch Creat Technol, Portsmouth PO1 2UP, Hants, England.
   [Dong, Junyu] Ocean Univ China, Dept Comp Sci & Technol, Qingdao, Peoples R China.
   [Gong, Maoguo] Xidian Univ, Key Lab Intelligent Percept & Image Understanding, Xian 710071, Peoples R China.
   [Nie, Liqiang] Shandong Univ, Sch Comp Sci & Technol, Qingdao 266237, Peoples R China.
   [Yin, Yilong] Shandong Univ, Sch Software Engn, Jinan 250100, Peoples R China.
   [Lam, Kin-Man] Hong Kong Polytech Univ, Ctr Signal Proc, Dept Elect & Informat Engn, Hong Kong, Peoples R China.
C3 Shandong University of Finance & Economics; University of Portsmouth;
   Ocean University of China; Xidian University; Shandong University;
   Shandong University; Hong Kong Polytechnic University
RP Yu, H (corresponding author), Univ Portsmouth, Sch Creat Technol, Portsmouth PO1 2UP, Hants, England.
EM jianmuweihk@163.com; dongjuityu@ouc.edu.cn; gong@ieee.org;
   hui.yu@port.ac.uk; nieliqiang@sdu.edu.cn; ylyin@sdu.edu.cn;
   enkmlam@polyu.edu.hk
RI Yu, Hui/G-1115-2018; Jian, Muwei/Q-8319-2018
OI Yu, Hui/0000-0002-7655-9228; Dong, Junyu/0000-0001-7012-2087; Jian,
   Muwei/0000-0002-4249-2264
FU National Natural Science Foundation of China [61601427, 61976123]; Royal
   Society-K; C. Wong International Fellowship [NIF\R1\180909]; Mount Tai
   Youth Talents Plan
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61601427 and 61976123, in part by Royal
   Society-K. C. Wong International Fellowship (NIF\R1\180909), and in part
   by Mount Tai Youth Talents Plan.
CR Agrawal A, 2005, IEEE I CONF COMP VIS, P174, DOI 10.1109/ICCV.2005.31
   Agrawal A, 2006, LECT NOTES COMPUT SC, V3951, P578
   [Anonymous], [No title captured]
   [Anonymous], [No title captured]
   [Anonymous], 1986, P 13 ANN C COMP GRAP, DOI DOI 10.1145/15922.15911
   Bergevin R, 1996, IEEE T PATTERN ANAL, V18, P540, DOI 10.1109/34.494643
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Chao F, 2019, IEEE T HUM-MACH SYST, V49, P47, DOI 10.1109/THMS.2018.2882485
   Chen MY, 2016, IEEE T HUM-MACH SYST, V46, P436, DOI 10.1109/THMS.2015.2492599
   Chen MY, 2016, IEEE T HUM-MACH SYST, V46, P403, DOI 10.1109/THMS.2015.2492598
   Chetverikov D, 2002, INT C PATT RECOG, P545, DOI 10.1109/ICPR.2002.1047997
   Clark J. J., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P29, DOI 10.1109/CVPR.1992.223231
   Dong J., 2003, THESIS
   Dong J, 2008, IEEE INTELL SYST, V23, P56, DOI 10.1109/MIS.2008.110
   Dong JY, 2005, INT J COMPUT VISION, V62, P177, DOI 10.1007/s11263-005-4641-6
   Gu J, 2018, IEEE T MULTIMEDIA, V20, P1140, DOI 10.1109/TMM.2017.2761993
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P1098, DOI 10.1109/TMM.2016.2547343
   Han CC, 2008, MACH VISION APPL, V19, P43, DOI 10.1007/s00138-007-0076-0
   Jian MW, 2018, COMPUT IND, V99, P110, DOI 10.1016/j.compind.2018.03.034
   Jian MW, 2015, PERCEPTION, V44, P35
   Jian MW, 2014, INFORM SCIENCES, V269, P60, DOI 10.1016/j.ins.2014.01.019
   Jian MW, 2013, COMPUT IND, V64, P1229, DOI 10.1016/j.compind.2013.06.011
   Jian MW, 2011, MULTIMED TOOLS APPL, V53, P237, DOI 10.1007/s11042-010-0509-z
   Kane L, 2017, IEEE T HUM-MACH SYST, V47, P1077, DOI 10.1109/THMS.2017.2706695
   Leung H, 2008, IEEE SIGNAL PROC MAG, V25, P49, DOI 10.1109/MSP.2008.923087
   Liu CL, 2001, PATTERN RECOGN, V34, P2339, DOI 10.1016/S0031-3203(00)00165-5
   Ma L, 2016, IEEE T MULTIMEDIA, V18, P2228, DOI 10.1109/TMM.2016.2614187
   Ma Z, 2017, INT J SOC ROBOT, V9, P525, DOI 10.1007/s12369-017-0410-2
   Ma Z, 2017, IEEE T COGN DEV SYST, V9, P80, DOI 10.1109/TCDS.2016.2645598
   Magda S, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P391, DOI 10.1109/ICCV.2001.937652
   Min XK, 2018, IEEE T MULTIMEDIA, V20, P2049, DOI 10.1109/TMM.2017.2788206
   Mulligan J. B., 2004, P 2004 IEEE COMP SOC, P1063
   Nakamura T, 2017, SA'17: SIGGRAPH ASIA 2017 EMERGING TECHNOLOGIES, DOI 10.1145/3132818.3132825
   Robb M., 2003, P VIS VID GRAPH 2003, P79
   Rusinkiewicz S, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P145, DOI 10.1109/IM.2001.924423
   Su YM, 2003, PATTERN RECOGN, V36, P635, DOI 10.1016/S0031-3203(02)00086-9
   Sun YD, 2014, IEEE INT CONF ROBOT, P3207, DOI 10.1109/ICRA.2014.6907320
   Sun YJ, 2015, MULTIMED TOOLS APPL, V74, P3635, DOI 10.1007/s11042-013-1791-3
   Wong HTF, 2000, COMPUT GRAPH-UK, V24, P99, DOI 10.1016/S0097-8493(99)00141-7
   WOODHAM RJ, 1981, ARTIF INTELL, V17, P117, DOI 10.1016/0004-3702(81)90022-9
   Xu SH, 2005, IEEE INTELL SYST, V20, P32, DOI 10.1109/MIS.2005.41
   Xu SH, 2012, IEEE INTELL SYST, V27, P63, DOI 10.1109/MIS.2012.46
   Zafeiriou S, 2013, IEEE T INF FOREN SEC, V8, P121, DOI 10.1109/TIFS.2012.2224109
   [张婷 Zhang Ting], 2014, [计算机学报, Chinese Journal of Computers], V37, P2380
   Zhang X, 2019, VISUAL COMPUT, V35, P1193, DOI 10.1007/s00371-019-01675-w
   Zhang XY, 2017, IEEE T HUM-MACH SYST, V47, P285, DOI 10.1109/THMS.2016.2634921
   Zhang YMZ, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.83
   Zhang YW, 2015, SCI REP-UK, V5, DOI 10.1038/srep10909
   Zhang YW, 2018, COMPUT GRAPH-UK, V70, P300, DOI 10.1016/j.cag.2017.07.022
   Zhou WJ, 2016, IEEE T MULTIMEDIA, V18, P1077, DOI 10.1109/TMM.2016.2542580
   Zhuang YT, 2009, J VIS COMMUN IMAGE R, V20, P84, DOI 10.1016/j.jvcir.2008.11.007
NR 51
TC 28
Z9 31
U1 6
U2 41
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2020
VL 22
IS 4
BP 970
EP 979
DI 10.1109/TMM.2019.2937187
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KY2ZI
UT WOS:000522440400012
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yin, ZX
   Xiang, YZ
   Zhang, XP
AF Yin, Zhaoxia
   Xiang, Youzhi
   Zhang, Xinpeng
TI Reversible Data Hiding in Encrypted Images Based on Multi-MSB Prediction
   and Huffman Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Data mining; Encryption; Image restoration; Image coding; Receivers;
   Huffman coding; Reversible data hiding; encrypted images; privacy
   protection; Huffman coding
ID WATERMARKING; EXPANSION; FRAMEWORK; MAP
AB With the development of cloud storage and privacy protection, reversible data hiding in encrypted images (RDHEI) has attracted increasing attention as a technology that can: embed additional data in the image encryption domain, ensure that the embedded data can be extracted error-free, and the original image can be restored losslessly. In this paper, a high-capacity RDHEI algorithm based on multi-MSB (most significant bit) prediction and Huffman coding is proposed. At first, multi-MSB of each pixel was predicted adaptively and marked by Huffman coding in the original image. Then, the image was encrypted by a stream cipher method. At last, the vacated space can be used to embed additional data by multi-MSB substitution. Experimental results show that our method achieved higher embedding capacity while comparing with the state-of-the-art methods.
C1 [Yin, Zhaoxia; Xiang, Youzhi] Anhui Univ, Key Lab Intelligent Comp & Signal Proc, Minist Educ, Sch Comp Sci & Technol, Hefei 230601, Peoples R China.
   [Zhang, Xinpeng] Fudan Univ, Sch Comp Sci, Shanghai 200433, Peoples R China.
C3 Anhui University; Fudan University
RP Zhang, XP (corresponding author), Fudan Univ, Sch Comp Sci, Shanghai 200433, Peoples R China.
EM yinzhaoxia@ahu.edu.cn; 1875738190@qq.com; zhangxinpeng@fudan.edu.cn
RI Yin, Zhaoxia/HRD-7425-2023
OI Yin, Zhaoxia/0000-0003-0387-4806
FU National Natural Science Foundation of China [61872003, U1636206,
   61860206004]
FX This work was supported by National Natural Science Foundation of China
   under Grants 61872003, U1636206, and 61860206004.
CR [Anonymous], 2011, P 13 INF HID C PRAG
   Bas P., 2017, Image database of BOWS-2
   Celik MU, 2005, IEEE T IMAGE PROCESS, V14, P253, DOI 10.1109/TIP.2004.840686
   Celik MU, 2006, IEEE T IMAGE PROCESS, V15, P1042, DOI 10.1109/TIP.2005.863053
   Chen KM, 2019, J VIS COMMUN IMAGE R, V58, P334, DOI 10.1016/j.jvcir.2018.12.023
   Coatrieux G, 2009, IEEE T INF TECHNOL B, V13, P158, DOI 10.1109/TITB.2008.2007199
   Dragoi IC, 2014, IEEE T IMAGE PROCESS, V23, P1779, DOI 10.1109/TIP.2014.2307482
   Hong W, 2012, IEEE SIGNAL PROC LET, V19, P199, DOI 10.1109/LSP.2012.2187334
   Hua ZY, 2018, SIGNAL PROCESS, V149, P148, DOI 10.1016/j.sigpro.2018.03.010
   Hua ZY, 2015, INFORM SCIENCES, V297, P80, DOI 10.1016/j.ins.2014.11.018
   Huang FJ, 2016, IEEE T INF FOREN SEC, V11, P2777, DOI 10.1109/TIFS.2016.2598528
   Jiang RQ, 2018, IEEE T MULTIMEDIA, V20, P55, DOI 10.1109/TMM.2017.2723244
   Lee S, 2007, IEEE T INF FOREN SEC, V2, P321, DOI 10.1109/TIFS.2007.905146
   Li RYM, 2007, IEEE INT SYMP CIRC S, P1273, DOI 10.1109/ISCAS.2007.378403
   Li XL, 2015, IEEE T INF FOREN SEC, V10, P2016, DOI 10.1109/TIFS.2015.2444354
   Li XL, 2013, IEEE T IMAGE PROCESS, V22, P2181, DOI 10.1109/TIP.2013.2246179
   Ma KD, 2013, IEEE T INF FOREN SEC, V8, P553, DOI 10.1109/TIFS.2013.2248725
   Ni ZC, 2006, IEEE T CIRC SYST VID, V16, P354, DOI 10.1109/TCSVT.2006.869964
   Ou B, 2013, IEEE T IMAGE PROCESS, V22, P5010, DOI 10.1109/TIP.2013.2281422
   Puech W, 2008, PROC SPIE, V6819, DOI 10.1117/12.766754
   Puteaux P, 2018, IEEE INT WORKS INFOR
   Puteaux P, 2018, IEEE T INF FOREN SEC, V13, P1670, DOI 10.1109/TIFS.2018.2799381
   Puyang Y, 2018, IEEE INT WORKS INFOR
   Qian ZX, 2016, IEEE T CIRC SYST VID, V26, P636, DOI 10.1109/TCSVT.2015.2418611
   Qian ZX, 2014, IEEE T MULTIMEDIA, V16, P1486, DOI 10.1109/TMM.2014.2316154
   Schaefer G, 2004, PROC SPIE, V5307, P472, DOI 10.1117/12.525375
   Tian J, 2003, IEEE T CIRC SYST VID, V13, P890, DOI 10.1109/TCSVT.2003.815962
   Wang JX, 2017, IEEE T CYBERNETICS, V47, P315, DOI 10.1109/TCYB.2015.2514110
   Weinberger MJ, 2000, IEEE T IMAGE PROCESS, V9, P1309, DOI 10.1109/83.855427
   Wu XT, 2014, SIGNAL PROCESS, V104, P387, DOI 10.1016/j.sigpro.2014.04.032
   Xu DW, 2016, SIGNAL PROCESS, V123, P9, DOI 10.1016/j.sigpro.2015.12.012
   Yi S, 2019, IEEE T MULTIMEDIA, V21, P51, DOI 10.1109/TMM.2018.2844679
   Zhang WM, 2016, IEEE T MULTIMEDIA, V18, P1469, DOI 10.1109/TMM.2016.2569497
   Zhang WM, 2014, SIGNAL PROCESS, V94, P118, DOI 10.1016/j.sigpro.2013.06.023
   Zhang XP, 2016, IEEE T CIRC SYST VID, V26, P1622, DOI 10.1109/TCSVT.2015.2433194
   Zhang XP, 2014, IEEE T MULTIMEDIA, V16, P1327, DOI 10.1109/TMM.2014.2315974
   Zhang XP, 2012, IEEE T INF FOREN SEC, V7, P826, DOI 10.1109/TIFS.2011.2176120
   Zhang XP, 2011, IEEE SIGNAL PROC LET, V18, P255, DOI 10.1109/LSP.2011.2114651
   Zhou H, 2019, IEEE T MULTIMEDIA, V21, P1384, DOI 10.1109/TMM.2018.2882088
   Zhou YC, 2014, SIGNAL PROCESS, V97, P172, DOI 10.1016/j.sigpro.2013.10.034
NR 40
TC 122
Z9 140
U1 3
U2 95
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2020
VL 22
IS 4
BP 874
EP 884
DI 10.1109/TMM.2019.2936314
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KY2ZI
UT WOS:000522440400004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Chen, LX
   Song, LQ
   Chakareski, J
   Xu, J
AF Chen, Lixing
   Song, Linqi
   Chakareski, Jacob
   Xu, Jie
TI Collaborative Content Placement Among Wireless Edge Caching Stations
   With Time-to-Live Cache
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Content placement; coded caching; wireless network; online
   decision-making
ID POPULARITY; NETWORKS; DESIGN
AB Content caching at the Internet edge using a network of wireless edge caching stations (ECSs) is recently considered as a key solution to alleviating the backhaul traffic burden and improving the quality of experience in 5G networks. This paper studies wireless edge caching systems with the following features: first, content files can be partitioned into many coded packets, which then can be cached in multiple ECSs for collaborative content delivery; second, the service provider (SP) deploys time-to-live cache at ECSs and each cached content file has an occupancy time that needs to be guaranteed; third, the content-to-be-cached arrives at the caching system following a stochastic process as users request new content over time. Unlike existing works that determine which content to cache, this paper focuses on how to distribute the coded packets of content-to-be-cached among the network of ECSs in order to reduce the content downloading time. A novel content placement strategy, called stochastic collaborative content placement is proposed based on Lyapunov techniques. The proposed algorithm makes content placement decisions using only currently available information without foreseeing future content arrivals, takes advantage of the spatial content popularity variation with coded caching, and achieves the provable close-to-optimal long-term caching performance. Simulations are carried out on a real-world YouTube video request trace and the results demonstrate a tremendous caching performance improvement against a variety of benchmark schemes.
C1 [Chen, Lixing; Xu, Jie] Univ Miami, Dept Elect & Comp Engn, Coral Gables, FL 33146 USA.
   [Song, Linqi] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
   [Chakareski, Jacob] New Jersey Inst Technol, Ying Wu Coll Comp, Newark, NJ 07103 USA.
C3 University of Miami; City University of Hong Kong; New Jersey Institute
   of Technology
RP Chen, LX; Xu, J (corresponding author), Univ Miami, Dept Elect & Comp Engn, Coral Gables, FL 33146 USA.; Song, LQ (corresponding author), City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
EM lx.chen@miami.edu; linqi.song@cityu.edu.hk; jacob@ua.edu;
   jiexu@miami.edu
RI WANG, YONGJIA/KFQ-4823-2024; zhong, jing/KBP-7800-2024; Chen,
   Lixing/ABB-7931-2020
OI Chen, Lixing/0000-0002-1805-0183; Song, Linqi/0000-0003-2756-4984; Xu,
   Jie/0000-0002-0515-1647
FU U.S. Army Research Office [W911NF1810343]; City University of Hong Kong
   [7200594]; National Science Foundation (NSF) [CCF-1528030, ECCS-1711592,
   CNS-1836909, CNS-1821875]; Adobe Systems; U.S. Department of Defense
   (DOD) [W911NF1810343] Funding Source: U.S. Department of Defense (DOD)
FX The work of L. Chen and J. Xu was supported in part by the U.S. Army
   Research Office under Grant W911NF1810343. The work of L. Song was
   supported by the City University of Hong Kong under Grant 7200594. The
   work of J. Chakareskiwas supported in part by the National Science
   Foundation (NSF) under Award CCF-1528030, Award ECCS-1711592, Award
   CNS-1836909, and Award CNS-1821875, and in part by research gifts and an
   Adobe Data Science Award from Adobe Systems. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Christian Timmerer.
CR Abedini N, 2014, IEEE ACM T NETWORK, V22, P864, DOI 10.1109/TNET.2013.2261542
   Agyapong PK, 2012, IEEE COMMUN MAG, V50, P18, DOI 10.1109/MCOM.2012.6384447
   [Anonymous], 2016, Computer Communications, IEEE INFOCOM 2016-The 35th Annual IEEE International Conference on
   [Anonymous], 2017, ADV INTEL SYS RES
   Bastug E, 2014, IEEE COMMUN MAG, V52, P82, DOI 10.1109/MCOM.2014.6871674
   Bhushan N, 2014, IEEE COMMUN MAG, V52, P82, DOI 10.1109/MCOM.2014.6736747
   Bioglio V, 2015, IEEE GLOB COMM CONF, DOI 10.1109/GLOCOM.2015.7417697
   Blasco P, 2014, IEEE INT SYMP INFO, P51, DOI 10.1109/ISIT.2014.6874793
   Blaszczyszyn B, 2015, IEEE ICC, P3358, DOI 10.1109/ICC.2015.7248843
   Chen L., 2019, ONLINE APPENDIX
   Chen Z, 2017, IEEE T WIREL COMMUN, V16, P3401, DOI 10.1109/TWC.2017.2682240
   Gharaibeh A, 2016, IEEE T MOBILE COMPUT, V15, P1863, DOI 10.1109/TMC.2015.2474364
   Golrezaei N, 2013, IEEE COMMUN MAG, V51, P142, DOI 10.1109/MCOM.2013.6495773
   Hu Q, 2014, 2014 21ST INTERNATIONAL CONFERENCE ON TELECOMMUNICATIONS (ICT), P426, DOI 10.1109/ICT.2014.6845152
   Huang LB, 2013, IEEE ACM T NETWORK, V21, P1117, DOI 10.1109/TNET.2012.2230336
   Iacono A., 2000, NEXT GENER WIRELESS, V598, P3
   Khreishah A, 2016, IEEE J SEL AREA COMM, V34, P2275, DOI 10.1109/JSAC.2016.2577199
   Kocak F, 2013, 2013 ASE/IEEE INTERNATIONAL CONFERENCE ON SOCIAL COMPUTING (SOCIALCOM), P45, DOI 10.1109/SocialCom.2013.13
   Lakshminarayana S, 2014, IEEE J SEL AREA COMM, V32, P1386, DOI 10.1109/JSAC.2014.2332093
   Lee D, 2001, IEEE T COMPUT, V50, P1352, DOI 10.1109/tc.2001.970573
   Li CL, 2018, IEEE T MULTIMEDIA, V20, P965, DOI 10.1109/TMM.2017.2757761
   Li SH, 2016, IEEE T MULTIMEDIA, V18, P2503, DOI 10.1109/TMM.2016.2596042
   Ma R.T., 2015, Proceedings of the 11th ACM Conference on Emerging Networking Experiments and Technologies, P1
   Maddah-Ali MA, 2015, IEEE ACM T NETWORK, V23, P1029, DOI 10.1109/TNET.2014.2317316
   Mao YY, 2015, IEEE J SEL AREA COMM, V33, P2463, DOI 10.1109/JSAC.2015.2481209
   Neely, 2010, STOCHASTIC NETWORK O
   Ozfatura E, 2018, IEEE COMMUN LETT, V22, P288, DOI 10.1109/LCOMM.2017.2774799
   Rossi F, 2011, NANOSCI TECHNOL, P1, DOI 10.1007/978-3-642-10556-2
   Shanmugam K, 2013, IEEE T INFORM THEORY, V59, P8402, DOI 10.1109/TIT.2013.2281606
   Tadrous J, 2016, IEEE ACM T NETWORK, V24, P2747, DOI 10.1109/TNET.2015.2478476
   Wang TY, 2015, IEEE WCNC, P861, DOI 10.1109/WCNC.2015.7127582
   Wang XF, 2015, P ANN HICSS, P5404, DOI 10.1109/HICSS.2015.635
   Wildemeersch M, 2014, IEEE T COMMUN, V62, P4440, DOI 10.1109/TCOMM.2014.2370056
   Xu J, 2015, IEEE J-STSP, V9, P330, DOI 10.1109/JSTSP.2014.2370942
   Zhang S, 2017, IEEE COMMUN MAG, V55, P202, DOI 10.1109/MCOM.2017.1700129
   Zhou B, 2016, IEEE T WIREL COMMUN, V15, P6284, DOI 10.1109/TWC.2016.2582689
   Zhou S, 2015, IEEE COMMUN MAG, V53, P142, DOI 10.1109/MCOM.2015.7081087
   Zhou ZY, 2015, IEEE ACCESS, V3, P1849, DOI 10.1109/ACCESS.2015.2478863
   Zink M, 2009, COMPUT NETW, V53, P501, DOI 10.1016/j.comnet.2008.09.022
NR 39
TC 19
Z9 20
U1 1
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2020
VL 22
IS 2
BP 432
EP 444
DI 10.1109/TMM.2019.2929004
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KH5FZ
UT WOS:000510676300012
OA hybrid
DA 2024-07-18
ER

PT J
AU Piao, JC
   Kim, SD
AF Piao, Jin-Chun
   Kim, Shin-Dug
TI Real-Time Visual-Inertial SLAM Based on Adaptive Keyframe Selection for
   Mobile AR Applications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Simultaneous localization and mapping; Cameras; Feature extraction;
   Tracking; Mobile handsets; Real-time systems; Adaptive optics; Adaptive
   keyframe selection; augmented reality; mobile applications;
   optical-flow-based tracking; simultaneous localization and mapping;
   visual-inertial odometry
ID VERSATILE
AB Simultaneous localization and mapping (SLAM) technology is used in many applications, such as augmented reality (AR)/virtual reality, robots, drones, and self-driving vehicles. In AR applications, rapid camera motion estimation, actual size, and scale are important issues. In this research, we introduce a real-time visual-inertial SLAM based on an adaptive keyframe selection for mobile AR applications. Specifically, the SLAM system is designed based on the adaptive keyframe selection visual-inertial odometry method that includes the adaptive keyframe selection method and the lightweight visual-inertial odometry method. The inertial measurement unit data are used to predict the motion state of the current frame and it is judged whether or not the current frame is a keyframe by an adaptive selection method based on learning and automatic setting. Relatively unimportant frames (not a keyframe) are processed using a lightweight visual-inertial odometry method for efficiency and real-time performance. We simulate it in a PC environment and compare it with state-of-the-art methods. The experimental results demonstrate that the mean translation root-mean-square error of the keyframe trajectory is 0.067 m without the ground-truth scale matching, and the scale error is 0.58 with the EuRoC dataset. Moreover, the experimental results of the mobile device show that the performance is improved by 34.5-53.8 using the proposed method.
C1 [Piao, Jin-Chun] Yanbian Univ, Dept Comp Sci & Technol, Yanji 133002, Peoples R China.
   [Kim, Shin-Dug] Yonsei Univ, Dept Comp Sci, Seoul 03722, South Korea.
C3 Yanbian University; Yonsei University
RP Kim, SD (corresponding author), Yonsei Univ, Dept Comp Sci, Seoul 03722, South Korea.
EM jcpiao@ybu.edu.cn; sdkim@yonsei.ac.kr
OI Kim, Shin Dug/0000-0002-2642-6662; Piao, Jin-Chun/0000-0003-1151-6814
CR Apple, 2017, ARKit
   Burri M, 2016, INT J ROBOT RES, V35, P1157, DOI 10.1177/0278364915620033
   Chai T, 2014, GEOSCI MODEL DEV, V7, P1247, DOI 10.5194/gmd-7-1247-2014
   Durrant-Whyte H, 2006, IEEE ROBOT AUTOM MAG, V13, P99, DOI 10.1109/MRA.2006.1638022
   Forster C, 2017, IEEE T ROBOT, V33, P1, DOI 10.1109/TRO.2016.2597321
   Furgale P, 2013, IEEE INT C INT ROBOT, P1280, DOI 10.1109/IROS.2013.6696514
   GOLUB GH, 1970, NUMER MATH, V14, P403, DOI 10.1007/BF02163027
   Hartley R, 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2
   HORN BKP, 1987, J OPT SOC AM A, V4, P629, DOI 10.1364/JOSAA.4.000629
   Kasyanov A, 2017, IEEE INT C INT ROBOT, P6662, DOI 10.1109/IROS.2017.8206581
   Leutenegger S, 2015, INT J ROBOT RES, V34, P314, DOI 10.1177/0278364914554813
   Li M., 2014, THESIS
   Lucas B., 1981, P INT JOINT C ART IN, P674, DOI DOI 10.1364/J0SAA.19.002142
   Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671
   Piao JC, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17112567
   Pillai S, 2015, ARXIV150601732
   Qin T, 2018, IEEE T ROBOT, V34, P1004, DOI 10.1109/TRO.2018.2853729
   Rives P, 2000, 2000 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2000), VOLS 1-3, PROCEEDINGS, P602, DOI 10.1109/IROS.2000.894670
   SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794
   Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773
   Suau X, 2012, IEEE T MULTIMEDIA, V14, P575, DOI 10.1109/TMM.2012.2189853
   Tateno K, 2017, PROC CVPR IEEE, P6565, DOI 10.1109/CVPR.2017.695
   Weiss S, 2012, IEEE INT CONF ROBOT, P957, DOI 10.1109/ICRA.2012.6225147
   Weiss S, 2012, IEEE INT CONF ROBOT, P31, DOI 10.1109/ICRA.2012.6225002
   Wu PF, 2017, IEEE T MULTIMEDIA, V19, P266, DOI 10.1109/TMM.2016.2612761
   Yang ZF, 2017, IEEE T AUTOM SCI ENG, V14, P39, DOI 10.1109/TASE.2016.2550621
NR 26
TC 37
Z9 41
U1 1
U2 74
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2019
VL 21
IS 11
BP 2827
EP 2836
DI 10.1109/TMM.2019.2913324
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JJ7VX
UT WOS:000494363000011
DA 2024-07-18
ER

PT J
AU Yang, X
   Gao, Y
   Luo, HC
   Liao, CY
   Cheng, KT
AF Yang, Xin
   Gao, Yang
   Luo, Hongcheng
   Liao, Chunyuan
   Cheng, Kwang-Ting
TI Bayesian DeNet: Monocular Depth Prediction and Frame-Wise Fusion With
   Synchronized Uncertainty
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Uncertainty; Cameras; Bayes methods; Simultaneous localization and
   mapping; Training; Video sequences; Estimation; Depth estimation; deep
   learning; convolutional neural network
ID 3D; VISION; 2D
AB Using deep convolutional neural networks (CNN) to predict the depth from a single image has received considerable attention in recent years due to its impressive performance. However, existing methods process each single image independently without leveraging the multiview information of video sequences in practical scenarios. Properly taking into account multiview information in video sequences beyond individual frames could offer considerable benefits in terms of depth prediction accuracy and robustness. In addition, a meaningful measure of prediction uncertainty is essential for decision making, which is not provided in existing methods. This paper presents a novel video-based depth prediction system based on a monocular camera, named Bayesian DeNet. Specifically, Bayesian DeNet consists of a 59-layer CNN that can concurrently output a depth map and an uncertainty map for each video frame. Each pixel in an uncertainty map indicates the error variance of the corresponding depth estimate. Depth estimates and uncertainties of previous frames are propagated to the current frame based on the tracked camera pose, yielding multiple depth/uncertainty hypotheses for the current frame which are then fused in a Bayesian inference framework for greater accuracy and robustness. Extensive exper-iments on three public datasets demonstrate that our Bayesian DeNet outperforms the state-of-the-art methods for monocular depth prediction. A demo video and code are publicly available.(1)
C1 [Yang, Xin; Gao, Yang; Luo, Hongcheng] Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Hubei, Peoples R China.
   [Liao, Chunyuan] HiScene Informat Technol Co Ltd, Shanghai 201210, Peoples R China.
   [Cheng, Kwang-Ting] Hong Kong Univ Sci & Technol, Sch Engn, Kowloon, Hong Kong, Peoples R China.
C3 Huazhong University of Science & Technology; Hong Kong University of
   Science & Technology
RP Luo, HC (corresponding author), Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Hubei, Peoples R China.
EM xinyang2014@hust.edu.cn; tena_na_gao@hust.edu.cn; hongcheng@hust.edu.cn;
   liaocy@hiscene.com; timcheng@ust.hk
OI Cheng, Kwang-Ting Tim/0000-0002-3885-4912
FU National Natural Science Foundation of China [61872417, 61502188]; Wuhan
   Science and Technology Bureau [2017010201010111]; Program for HUST
   Acadamic Frontier Youth Team
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61872417 and 61502188, in part by the
   Wuhan Science and Technology Bureau under Award 2017010201010111, and in
   part by the Program for HUST Acadamic Frontier Youth Team. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Marco Grangetto.
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   [Anonymous], 2018, NEUROCOMPUTING
   Cadena C, 2016, IEEE T ROBOT, V32, P1309, DOI 10.1109/TRO.2016.2624754
   Chen YH, 2008, IEEE T MULTIMEDIA, V10, P585, DOI 10.1109/TMM.2008.921741
   Clifford A.A., 1973, Multivariate Error Analysis
   Dehais J, 2017, IEEE T MULTIMEDIA, V19, P1090, DOI 10.1109/TMM.2016.2642792
   Eigen D, 2014, ADV NEUR IN, V27
   Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304
   Engel J, 2014, LECT NOTES COMPUT SC, V8690, P834, DOI 10.1007/978-3-319-10605-2_54
   Engel J, 2013, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2013.183
   Forster C, 2014, IEEE INT CONF ROBOT, P15, DOI 10.1109/ICRA.2014.6906584
   Garg R, 2016, LECT NOTES COMPUT SC, V9912, P740, DOI 10.1007/978-3-319-46484-8_45
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699
   Guan T, 2009, IEEE T MULTIMEDIA, V11, P1393, DOI 10.1109/TMM.2009.2032684
   Guo YL, 2014, IEEE T MULTIMEDIA, V16, P1377, DOI 10.1109/TMM.2014.2316145
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32
   Li B, 2015, PROC CVPR IEEE, P1119, DOI 10.1109/CVPR.2015.7298715
   Liu FY, 2016, IEEE T PATTERN ANAL, V38, P2024, DOI 10.1109/TPAMI.2015.2505283
   Liu FY, 2015, PROC CVPR IEEE, P5162, DOI 10.1109/CVPR.2015.7299152
   Liu MM, 2014, PROC CVPR IEEE, P716, DOI 10.1109/CVPR.2014.97
   Luo HC, 2019, IEEE T MULTIMEDIA, V21, P470, DOI 10.1109/TMM.2018.2859034
   Michels J., 2005, P 22 INT C MACHINE L, P593, DOI [10.1145/1102351.1102426, DOI 10.1145/1102351.1102426]
   Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103
   Newcombe RA, 2011, IEEE I CONF COMP VIS, P2320, DOI 10.1109/ICCV.2011.6126513
   Park M, 2013, IEEE T MULTIMEDIA, V15, P1569, DOI 10.1109/TMM.2013.2264926
   Phan R, 2014, IEEE T MULTIMEDIA, V16, P122, DOI 10.1109/TMM.2013.2283451
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Saxena A, 2009, IEEE T PATTERN ANAL, V31, P824, DOI 10.1109/TPAMI.2008.132
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773
   Szeliski R, 2011, TEXTS COMPUT SCI, P303, DOI 10.1007/978-1-84882-935-0_7
   Tateno K, 2017, PROC CVPR IEEE, P6565, DOI 10.1109/CVPR.2017.695
   Wang P, 2015, PROC CVPR IEEE, P2800, DOI 10.1109/CVPR.2015.7298897
   Wu PF, 2017, IEEE T MULTIMEDIA, V19, P266, DOI 10.1109/TMM.2016.2612761
   Xiao X, 2012, IEEE T MULTIMEDIA, V14, P1246, DOI 10.1109/TMM.2012.2190384
   Yang X, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P896, DOI 10.1145/3240508.3240564
   Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700
NR 39
TC 27
Z9 29
U1 1
U2 32
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2019
VL 21
IS 11
BP 2701
EP 2713
DI 10.1109/TMM.2019.2912121
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JJ7VX
UT WOS:000494363000001
DA 2024-07-18
ER

PT J
AU Min, XK
   Zhai, GT
   Gu, K
   Zhu, YC
   Zhou, JT
   Guo, GD
   Yang, XK
   Guan, XP
   Zhang, WJ
AF Min, Xiongkuo
   Zhai, Guangtao
   Gu, Ke
   Zhu, Yucheng
   Zhou, Jiantao
   Guo, Guodong
   Yang, Xiaokang
   Guan, Xinping
   Zhang, Wenjun
TI Quality Evaluation of Image Dehazing Methods Using Synthetic Hazy Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image dehazing; dehazing algorithm evaluation; quality assessment;
   synthetic haze; regular/aerial image
ID NATURAL SCENE; VISUAL-ATTENTION; INFORMATION; PREDICTION; INDEX
AB To enhance the visibility and usability of images captured in hazy conditions, many image dehazing algorithms (DHAs) have been proposed. With so many image DHAs, there is a need to evaluate and compare these DHAs. Due to the lack of the reference haze-free images, DHAs are generally evaluated qualitatively using real hazy images. But it is possible to perform quantitative evaluation using synthetic hazy images since the reference haze-free images are available and full-reference (FR) image quality assessment (IQA) measures can be utilized. In this paper, we follow this strategy and study DHA evaluation using synthetic hazy images systematically. We first build a synthetic haze removing quality (SHRQ) database. It consists of two subsets: regular and aerial image subsets, which include 360 and 240 dehazed images created from 45 and 30 synthetic hazy images using 8 DHAs, respectively. Since aerial imaging is an important application area of dehazing, we create an aerial image subset specifically. We then carry out subjective quality evaluation study on these two subsets. We observe that taking DHA evaluation as an exact FR IQA process is questionable, and the state-of-the-art FR IQA measures are not effective for DHA evaluation. Thus, we propose a DHA quality evaluation method by integrating some dehazing-relevant features, including image structure recovering, color rendition, and over-enhancement of low-contrast areas. The proposed method works for both types of images, but we further improve it for aerial images by incorporating its specific characteristics. Experimental results on two subsets of the SHRQ database validate the effectiveness of the proposed measures.
C1 [Min, Xiongkuo; Zhai, Guangtao; Zhu, Yucheng; Yang, Xiaokang; Zhang, Wenjun] Shanghai Jiao Tong Univ, Inst Image Commun & Network Engn, Shanghai 200240, Peoples R China.
   [Min, Xiongkuo] Univ Macau, Dept Comp & Informat Sci, Macau 999078, Peoples R China.
   [Gu, Ke] Beijing Univ Technol, Fac Informat Technol, Beijing Key Lab Computat Intelligence & Intellige, Beijing 100124, Peoples R China.
   [Zhou, Jiantao] Univ Macau, Dept Comp & Informat Sci, Fac Sci & Technol, Macau 999078, Peoples R China.
   [Zhou, Jiantao] Univ Macau, State Key Lab Internet Things Smart City, Macau 999078, Peoples R China.
   [Guo, Guodong] Baidu Res, Inst Deep Learning, Beijing 100193, Peoples R China.
   [Guo, Guodong] Baidu Res, Natl Engn Lab Deep Learning Technol & Applicat, Beijing 100193, Peoples R China.
   [Guan, Xinping] Shanghai Jiao Tong Univ, Dept Automat, Shanghai 200240, Peoples R China.
C3 Shanghai Jiao Tong University; University of Macau; Beijing University
   of Technology; University of Macau; University of Macau; Baidu; Baidu;
   Shanghai Jiao Tong University
RP Zhai, GT (corresponding author), Shanghai Jiao Tong Univ, Inst Image Commun & Network Engn, Shanghai 200240, Peoples R China.
EM minxiongkuo@gmail.com; zhaiguangtao@sjtu.edu.cn; guke@bjut.edu.cn;
   zyc420@sjtu.edu.cn; jtzhou@umac.mo; guoguodong01@baidu.com;
   xkyang@sjtu.edu.cn; xpguan@sjtu.edu.cn; zhangwenjun@sjtu.edu.cn
RI Zhai, Guangtao/X-5949-2019; Gu, Ke/AAJ-9684-2021; Zhang,
   Wenjun/GNH-2095-2022; Yang, Xiaokang/C-6137-2009; Min,
   Xiongkuo/A-7097-2019
OI Zhai, Guangtao/0000-0001-8165-9322; Zhang, Wenjun/0000-0002-5282-3725;
   Yang, Xiaokang/0000-0003-4029-3322; Min, Xiongkuo/0000-0001-5693-0416
FU National Natural Science Foundation of China [61831015, 61521062,
   61527804]; China Postdoctoral Science Foundation [BX20180197]; Macau
   Science and Technology Development Fund [FDCT/022/2017/A1,
   FDCT/077/2018/A2]; Research Committee at the University of Macau
   [MYRG2016-00137-FST, MYRG2018-00029-FST]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61831015, 61521062, and 61527804, in
   part by the China Postdoctoral Science Foundation under Grant
   BX20180197, in part by the Macau Science and Technology Development Fund
   under Grants FDCT/022/2017/A1 and FDCT/077/2018/A2, and in part by the
   Research Committee at the University of Macau under Grants
   MYRG2016-00137-FST and MYRG2018-00029-FST. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Hantao Liu.
CR Ancuti C, 2016, IEEE IMAGE PROC, P2226, DOI 10.1109/ICIP.2016.7532754
   [Anonymous], 2012, Methodology for the subjective assessment of the quality of television pictures
   [Anonymous], IEEE T INTELL TRANSP
   [Anonymous], IEEE T MULTIMED
   Baker S, 2007, IEEE I CONF COMP VIS, P588, DOI 10.1109/cvpr.2007.383191
   Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chen ZY, 2014, PROC CVPR IEEE, P3003, DOI 10.1109/CVPR.2014.384
   Choi LK, 2015, IEEE T IMAGE PROCESS, V24, P3888, DOI 10.1109/TIP.2015.2456502
   Fang YM, 2015, IEEE SIGNAL PROC LET, V22, P838, DOI 10.1109/LSP.2014.2372333
   Fattal R, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2651362
   Fattal R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360671
   Gu K, 2018, IEEE T NEUR NET LEAR, V29, P1301, DOI 10.1109/TNNLS.2017.2649101
   Gu K, 2017, IEEE T IND ELECTRON, V64, P3903, DOI 10.1109/TIE.2017.2652339
   Gu K, 2017, IEEE T CYBERNETICS, V47, P4559, DOI 10.1109/TCYB.2016.2575544
   Gu K, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2439035
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Hautiere Nicolas, 2008, Image Analysis & Stereology, V27, P87, DOI 10.5566/ias.v27.p87-95
   He KM, 2009, PROC CVPR IEEE, P1956, DOI [10.1109/CVPRW.2009.5206515, 10.1109/CVPR.2009.5206515]
   Hirschmüller H, 2007, PROC CVPR IEEE, P2134
   Lai YH, 2015, IEEE T CIRC SYST VID, V25, P1, DOI 10.1109/TCSVT.2014.2329381
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li LD, 2018, IEEE T MULTIMEDIA, V20, P914, DOI 10.1109/TMM.2017.2760062
   Li Y, 2017, COMPUT VIS IMAGE UND, V165, P1, DOI 10.1016/j.cviu.2017.09.003
   Liu AM, 2012, IEEE T IMAGE PROCESS, V21, P1500, DOI 10.1109/TIP.2011.2175935
   Liu HT, 2011, IEEE T CIRC SYST VID, V21, P971, DOI 10.1109/TCSVT.2011.2133770
   Long J, 2014, IEEE GEOSCI REMOTE S, V11, P59, DOI 10.1109/LGRS.2013.2245857
   Ma KD, 2018, IEEE T IMAGE PROCESS, V27, P1202, DOI 10.1109/TIP.2017.2774045
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P3951, DOI 10.1109/TIP.2017.2708503
   Ma KD, 2015, IEEE IMAGE PROC, P3600, DOI 10.1109/ICIP.2015.7351475
   Meng GF, 2013, IEEE I CONF COMP VIS, P617, DOI 10.1109/ICCV.2013.82
   Min XK, 2018, IEEE T BROADCAST, V64, P508, DOI 10.1109/TBC.2018.2816783
   Min XK, 2018, SIGNAL PROCESS, V145, P127, DOI 10.1016/j.sigpro.2017.10.025
   Min XK, 2017, INFORM SCIENCES, V420, P417, DOI 10.1016/j.ins.2017.08.040
   Min XK, 2017, IEEE T IMAGE PROCESS, V26, P5462, DOI 10.1109/TIP.2017.2735192
   Min XK, 2016, IEEE INT CON MULTI
   Min XK, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/2996463
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Narasimhan SG, 2003, IEEE T PATTERN ANAL, V25, P713, DOI 10.1109/TPAMI.2003.1201821
   Pan XX, 2015, IEEE SIGNAL PROC LET, V22, P1806, DOI 10.1109/LSP.2015.2432466
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Sheikh HR, 2005, IEEE T IMAGE PROCESS, V14, P2117, DOI 10.1109/TIP.2005.859389
   Tang KT, 2014, PROC CVPR IEEE, P2995, DOI 10.1109/CVPR.2014.383
   Tarel JP, 2009, IEEE I CONF COMP VIS, P2201, DOI 10.1109/ICCV.2009.5459251
   Wang SQ, 2013, IEEE T IMAGE PROCESS, V22, P1418, DOI 10.1109/TIP.2012.2231090
   Wang SQ, 2012, IEEE T CIRC SYST VID, V22, P516, DOI 10.1109/TCSVT.2011.2168269
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435
   Xia GS, 2017, IEEE T GEOSCI REMOTE, V55, P3965, DOI 10.1109/TGRS.2017.2685945
   Xiao CX, 2012, VISUAL COMPUT, V28, P713, DOI 10.1007/s00371-012-0679-y
   Xu M, 2016, IEEE T GEOSCI REMOTE, V54, P2998, DOI 10.1109/TGRS.2015.2509860
   Yang XK, 2005, SIGNAL PROCESS-IMAGE, V20, P662, DOI 10.1016/j.image.2005.04.001
   Zhai GT, 2008, IEEE T CIRC SYST VID, V18, P122, DOI 10.1109/TCSVT.2007.906942
   Zhai GT, 2012, IEEE T IMAGE PROCESS, V21, P41, DOI 10.1109/TIP.2011.2161092
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang W, 2017, IEEE T IMAGE PROCESS, V26, P1275, DOI 10.1109/TIP.2017.2651410
   Zhang W, 2016, IEEE T NEUR NET LEAR, V27, P1266, DOI 10.1109/TNNLS.2015.2461603
   Zhou Y, 2018, IEEE T MULTIMEDIA, V20, P3019, DOI 10.1109/TMM.2018.2829607
   Zhu YC, 2018, SIGNAL PROCESS-IMAGE, V69, P15, DOI 10.1016/j.image.2018.05.010
NR 62
TC 134
Z9 140
U1 3
U2 47
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2019
VL 21
IS 9
BP 2319
EP 2333
DI 10.1109/TMM.2019.2902097
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IT6VX
UT WOS:000483015200013
DA 2024-07-18
ER

PT J
AU Khan, K
   Goodridge, W
AF Khan, Koffka
   Goodridge, Wayne
TI S-MDP: Streaming With Markov Decision Processes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bottleneck link; household; LAN; QoE; fairness; stability; Markov
   decision process; streaming; buffer occupancy; average bandwidth; S-MDP
ID VIDEO; QUALITY; QOE
AB In recent years, finding adaptive players competing for network resources at a single bottleneck link has become common. This competition generally occurs at routers in household local area networks. Such competition severely reduces viewers' quality of experience, such as fairness and stability. Researchers harness Markov decision process (MDP) models to optimize the adaptive video streaming process. Typically, players follow a policy based on numerous parameters, such as buffer occupancy or average bandwidth. In this study, we defied this traditional decentralized client-side MDP approach by allowing players to share a network state among themselves, which we called a streaming MDP (S-MDP). This state includes a discrete data rate measurement (DRM) value. The DRM value is a normalized value of a player's incoming bitrate and is an example of an interval measurement scale. Players use video bitrates to produce unique state transition matrices. The S-MDP reward matrix penalizes excessive switching along the DRM interval scale and thus encourages stability. At intervals during streaming, players create unique policies. The near-real-time update of policies enables players DRM values to converge. S-MDP shows relatively good performance in emulation experiments compared with four streaming methods, namely, k-chunk MDP, stochastic dynamic programming for adaptive streaming over hypertext transfer protocol (sdpDASH), MDP-basedDASH, and RTRA_S. In Internet experiments, we compare the performance of streaming methods with a Roku, Amazon Fire TV, and Apple TV. We also compared it against users who play online games, download files, and/or chat online, and S-MDP outperforms the other methods in terms of both objective and subjective visual quality, except in the presence of transmission control protocol long-lived flows, such as Skype.
C1 [Khan, Koffka] Univ West Indies, St Augustine, Trinidad Tobago.
   [Goodridge, Wayne] Univ West Indies, Dept Comp & Informat Technol, St Augustine, Trinidad Tobago.
C3 University West Indies Mona Jamaica; University West Indies Saint
   Augustine; University West Indies Mona Jamaica; University West Indies
   Saint Augustine
RP Khan, K (corresponding author), Univ West Indies, St Augustine, Trinidad Tobago.
EM koffka.khan@gmail.com; wayne.goodridge@sta.uwi.edu
RI Khan, Koffka/KJL-5190-2024
CR [Anonymous], P 8 INT C HUM COMP I
   [Anonymous], 2014, P 2014 WORKSHOP DESI
   Bentaleb A, 2018, IEEE T BROADCAST, V64, P575, DOI 10.1109/TBC.2018.2816789
   Bentaleb A, 2017, IEEE T MULTIMEDIA, V19, P2136, DOI 10.1109/TMM.2017.2733344
   Bokani A, 2013, 2013 20TH INTERNATIONAL PACKET VIDEO WORKSHOP (PV)
   Brunnstrom K., 2013, HAL00977812 QUALINET
   Cofano G, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3092836
   Coolican H, 2017, RES METHODS STAT PSY, DOI DOI 10.4324/9780203769836
   Gadaleta M, 2017, IEEE T COGN COMMUN, V3, P703, DOI 10.1109/TCCN.2017.2755007
   García S, 2015, IEEE J-STSP, V9, DOI 10.1109/JSTSP.2014.2331912
   Gelman A., 1995, BAYESIAN DATA ANAL, DOI DOI 10.1201/9780429258411
   Hands DS, 2004, IEEE T MULTIMEDIA, V6, P806, DOI 10.1109/TMM.2004.837233
   Hoeg W., 1997, EBU TECH REV, P40
   Jiang JC, 2014, IEEE ACM T NETWORK, V22, P326, DOI 10.1109/TNET.2013.2291681
   Jumisko-Pyykko S., 2005, 13th Annual ACM International Conference on Multimedia, P535, DOI 10.1145/1101149.1101270
   Jumisko-Pyykko S., 2010, P 6 NORDIC CONFEREMC, P266, DOI DOI 10.1145/1868914.1868947
   Jumisko-Pyykkö S, 2007, PROC SPIE, V6507, DOI 10.1117/12.699797
   Kreutz D, 2015, P IEEE, V103, P14, DOI 10.1109/JPROC.2014.2371999
   Li Z, 2014, IEEE J SEL AREA COMM, V32, P719, DOI 10.1109/JSAC.2014.140405
   Liu A, 2014, IEEE T SIGNAL PROCES, V62, P390, DOI 10.1109/TSP.2013.2291211
   Lu Z, 2018, IEEE T MULTIMEDIA, V20, P1848, DOI 10.1109/TMM.2017.2772802
   Mao HZ, 2017, SIGCOMM '17: PROCEEDINGS OF THE 2017 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P197, DOI 10.1145/3098822.3098843
   Myllymaki P., 2002, International Journal on Artificial Intelligence Tools (Architectures, Languages, Algorithms), V11, P369, DOI 10.1142/S0218213002000940
   STEVENS SS, 1946, SCIENCE, V103, P677, DOI 10.1126/science.103.2684.677
   Strauss E, 1998, CLIN ORTHOP RELAT R, P2
   Vega MT, 2018, IEEE T BROADCAST, V64, P432, DOI 10.1109/TBC.2018.2822869
   Villa B. J., 2014, THESIS
   Xing M, 2014, IEEE J SEL AREA COMM, V32, P795, DOI 10.1109/JSAC.2014.140411
   Yuan H, 2018, IEEE T MULTIMEDIA, V20, P183, DOI 10.1109/TMM.2017.2724850
   Zhou C, 2016, IEEE T MULTIMEDIA, V18, P738, DOI 10.1109/TMM.2016.2522650
NR 30
TC 9
Z9 9
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2019
VL 21
IS 8
BP 2012
EP 2025
DI 10.1109/TMM.2019.2892304
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IK7YM
UT WOS:000476809700010
DA 2024-07-18
ER

PT J
AU Li, DQ
   Jiang, TT
   Lin, WS
   Jiang, M
AF Li, Dingquan
   Jiang, Tingting
   Lin, Weisi
   Jiang, Ming
TI Which Has Better Visual Quality: The Clear Blue Sky or a Blurry Animal?
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep semantic features; image content variation; no-reference image
   quality assessment; realistic blur; statistical aggregation
ID IMAGE SHARPNESS ASSESSMENT; METRICS
AB Image content variation is a typical and challenging problem in no-reference image-quality assessment (NR-IQA). This work pays special attention to the impact of image content variation on NR-IQA methods. To better analyze this impact, we focus on blur-dominated distortions to exclude the impacts of distortion-type variations. We empirically show that current NR-IQA methods are inconsistent with human visual perception when predicting the relative quality of image pairs with different image contents. In view of deep semantic features of pretrained image classification neural networks always containing discriminative image content information, we put forward a new NR-IQA method based on semantic feature aggregation (SFA) to alleviate the impact of image content variation. Specifically, instead of resizing the image, we first crop multiple overlapping patches over the entire distorted image to avoid introducing geometric deformations. Then, according to an adaptive layer selection procedure, we extract deep semantic features by leveraging the power of a pretrained image classification model for its inherent content-aware property. After that, the local patch features are aggregated using several statistical structures. Finally, a linear regression model is trained for mapping the aggregated global features to image-quality scores. The proposed method, SFA, is compared with nine representative blur-specific NR-IQA methods, two general-purpose NR-IQA methods, and two extra full-reference IQA methods on Gaussian blur images (with and without Gaussian noise/JPEG compression) and realistic blur images from multiple databases, including LIVE, TID2008, TID2013, MLIVE1, MLIVE2, BID, and CLIVE. Experimental results show that SFA is superior to the state-of-the-art NR methods on all seven databases. It is also verified that deep semantic features play a crucial role in addressing image content variation, and this provides a new perspective for NR-IQA.
C1 [Li, Dingquan; Jiang, Ming] Peking Univ, Cooperat Medianet Innovat Ctr, Beijing Int Ctr Math Res, Key Lab Math & Its Applicat LMAM,Sch Math Sci, Beijing 100871, Peoples R China.
   [Jiang, Tingting] Peking Univ, Cooperat Medianet Innovat Ctr, Sch Elect Engn & Comp Sci, Natl Engn Lab Video Technol, Beijing 100871, Peoples R China.
   [Lin, Weisi] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
C3 Peking University; Peking University; Nanyang Technological University
RP Jiang, TT (corresponding author), Peking Univ, Cooperat Medianet Innovat Ctr, Sch Elect Engn & Comp Sci, Natl Engn Lab Video Technol, Beijing 100871, Peoples R China.
EM dingquanli@pku.edu.cn; ttjiang@pku.edu.cn; wslin@ntu.edu.sg;
   ming-jiang@pku.edu.cn
RI Jiang, Ming/A-9840-2008; Lin, Weisi/A-8011-2012; Li,
   Dingquan/V-9254-2019; LI, Dingquan/ADW-3576-2022; Lin, Weisi/A-3696-2011
OI Jiang, Ming/0000-0002-1661-0538; Lin, Weisi/0000-0001-9866-1947; Jiang,
   Tingting/0000-0002-5372-0656; Li, Dingquan/0000-0002-5549-9027
FU National Basic Research Program of China (973 Program) [2015CB351803];
   National Natural Science Foundation of China [61390514, 61527804,
   61572042, 61520106004]; Ministry of Education, Singapore
   [MOE2016-T2-2-057 (S)]; Sino-German Center [GZ 1025]
FX This work was supported in part by the National Basic Research Program
   of China (973 Program) under Grant 2015CB351803, in part by the National
   Natural Science Foundation of China under Grant 61390514, Grant
   61527804, Grant 61572042, and Grant 61520106004, in part by the Tier 2
   Fund of Ministry of Education, Singapore: MOE2016-T2-2-057 (S), and in
   part by Sino-German Center (GZ 1025). The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Hantao Liu.
CR [Anonymous], NIPS
   Bac S, 2007, COMPUT GRAPH FORUM, V26, P571, DOI 10.1111/j.1467-8659.2007.01080.x
   Bahrami K, 2016, IEEE T MULTIMEDIA, V18, P1568, DOI 10.1109/TMM.2016.2573139
   Bahrami K, 2014, IEEE SIGNAL PROC LET, V21, P751, DOI 10.1109/LSP.2014.2314487
   Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Ciancio A, 2011, IEEE T IMAGE PROCESS, V20, P64, DOI 10.1109/TIP.2010.2053549
   Ferzli R, 2009, IEEE T IMAGE PROCESS, V18, P717, DOI 10.1109/TIP.2008.2011760
   Ghadiyaram D, 2016, IEEE T IMAGE PROCESS, V25, P372, DOI 10.1109/TIP.2015.2500021
   Golestaneh S, 2016, IEEE T IMAGE PROCESS, V25, P5293, DOI 10.1109/TIP.2016.2601821
   Gu J, 2018, IEEE T MULTIMEDIA, V20, P1140, DOI 10.1109/TMM.2017.2761993
   Gu K, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2439035
   Guan JW, 2017, IEEE T MULTIMEDIA, V19, P2505, DOI 10.1109/TMM.2017.2703148
   Hadizadeh H, 2018, IEEE T MULTIMEDIA, V20, P392, DOI 10.1109/TMM.2017.2740023
   Hassen R, 2013, IEEE T IMAGE PROCESS, V22, P2798, DOI 10.1109/TIP.2013.2251643
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang GB, 2006, NEUROCOMPUTING, V70, P489, DOI 10.1016/j.neucom.2005.12.126
   Jayaraman D, 2012, CONF REC ASILOMAR C, P1693, DOI 10.1109/ACSSC.2012.6489321
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Kim J, 2017, IEEE J-STSP, V11, P206, DOI 10.1109/JSTSP.2016.2639328
   Kupyn O, 2018, PROC CVPR IEEE, P8183, DOI 10.1109/CVPR.2018.00854
   Leclaire A, 2015, J MATH IMAGING VIS, V52, P145, DOI 10.1007/s10851-015-0560-5
   Levin Anat, 2007, ADV NEURAL INFORM PR, P841, DOI DOI 10.5555/2976456.2976562
   Li DQ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P378, DOI 10.1145/3123266.3123322
   Li LD, 2016, IEEE T CYBERNETICS, V46, P39, DOI 10.1109/TCYB.2015.2392129
   Li LD, 2017, IEEE T MULTIMEDIA, V19, P1030, DOI 10.1109/TMM.2016.2640762
   Li LD, 2016, IEEE T MULTIMEDIA, V18, P1085, DOI 10.1109/TMM.2016.2545398
   Li LRH, 2018, PROC CVPR IEEE, P6616, DOI 10.1109/CVPR.2018.00692
   Li QH, 2016, IEEE T MULTIMEDIA, V18, P2457, DOI 10.1109/TMM.2016.2601028
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Liu HT, 2016, IEEE T IMAGE PROCESS, V25, P3087, DOI 10.1109/TIP.2016.2561406
   Liu YT, 2018, IEEE T MULTIMEDIA, V20, P379, DOI 10.1109/TMM.2017.2729020
   Lu QB, 2016, INFORM SCIENCES, V369, P334, DOI 10.1016/j.ins.2016.06.042
   Lu X, 2015, IEEE I CONF COMP VIS, P990, DOI 10.1109/ICCV.2015.119
   Ma KD, 2018, IEEE T IMAGE PROCESS, V27, P5155, DOI 10.1109/TIP.2018.2847421
   Ma KD, 2018, IEEE T IMAGE PROCESS, V27, P1202, DOI 10.1109/TIP.2017.2774045
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P3951, DOI 10.1109/TIP.2017.2708503
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P1004, DOI 10.1109/TIP.2016.2631888
   Marziliano P, 2004, SIGNAL PROCESS-IMAGE, V19, P163, DOI 10.1016/j.image.2003.08.003
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Mukundan R, 2001, IEEE T IMAGE PROCESS, V10, P1357, DOI 10.1109/83.941859
   Narvekar ND, 2011, IEEE T IMAGE PROCESS, V20, P2678, DOI 10.1109/TIP.2011.2131660
   Ponomarenko N., 2009, ADV MODERN RADIOELEC, V10, P30, DOI 10.1109/TIP.2015.2439035
   Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009
   Rosipal R, 2006, LECT NOTES COMPUT SC, V3940, P34, DOI 10.1007/11752790_2
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Shi JP, 2014, PROC CVPR IEEE, P2965, DOI 10.1109/CVPR.2014.379
   Tang HX, 2014, PROC CVPR IEEE, P2877, DOI 10.1109/CVPR.2014.368
   Tao X, 2018, PROC CVPR IEEE, P8174, DOI 10.1109/CVPR.2018.00853
   Video Quality Experts Group, 2000, Final report from the video quality experts group on the validation of objective models of video quality assessment march 2000
   Vu CT, 2012, IEEE T IMAGE PROCESS, V21, P934, DOI 10.1109/TIP.2011.2169974
   Vu PV, 2012, IEEE SIGNAL PROC LET, V19, P423, DOI 10.1109/LSP.2012.2199980
   Wang SG, 2016, NEUROCOMPUTING, V174, P310, DOI 10.1016/j.neucom.2014.12.117
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435
   Wu QB, 2017, IEEE T MULTIMEDIA, V19, P2490, DOI 10.1109/TMM.2017.2700206
   Xu JT, 2016, IEEE T IMAGE PROCESS, V25, P4444, DOI 10.1109/TIP.2016.2585880
   Xu Y, 2015, IEEE T IMAGE PROCESS, V24, P2098, DOI 10.1109/TIP.2015.2413298
   Yu SD, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0176632
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028
   Zhao WD, 2018, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR.2018.00325
NR 61
TC 80
Z9 90
U1 2
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2019
VL 21
IS 5
BP 1221
EP 1234
DI 10.1109/TMM.2018.2875354
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HV8HY
UT WOS:000466223600012
DA 2024-07-18
ER

PT J
AU Wei, LH
   Zhang, SL
   Yao, HT
   Gao, W
   Tian, Q
AF Wei, Longhui
   Zhang, Shiliang
   Yao, Hantao
   Gao, Wen
   Tian, Qi
TI GLAD: Global-Local-Alignment Descriptor for Scalable Person
   Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Person re-identification (Re-ID); global-local-alignment descriptor;
   retrieval framework
ID IDENTIFICATION; PERFORMANCE
AB The huge variance of human pose and the misalignment of detected human images significantly increase the difficulty of pedestrian image matching in person Re-Identification (Re-ID). Moreover, the massive visual data being produced by surveillance video cameras requires highly efficient person Re-ID systems. Targeting to solve the first problem, thiswork proposes a robust and discriminative pedestrian image descriptor, namely, the Global-Local-Alignment Descriptor (GLAD). For the second problem, this work treats person Re-ID as image retrieval and proposes an efficient indexing and retrieval framework. GLAD explicitly leverages the local and global cues in the human body to generate a discriminative and robust representation. It consists of part extraction and descriptor learning modules, where several part regions are first detected and then deep neural networks are designed for representation learning on both the local and global regions. A hierarchical indexing and retrieval framework is designed to perform offline relevance mining to eliminate the huge person ID redundancy in the gallery set, and accelerate the online Re-ID procedure. Extensive experimental results on widely used public benchmark datasets show GLAD achieves competitive accuracy compared to the state-of-the-art methods. On a large-scale person, with the Re-ID dataset containing more than 520 K images, our retrieval framework significantly accelerates the online Re-ID procedure while also improving Re-ID accuracy. Therefore, this work has the potential to work better on person Re-ID tasks in real scenarios.
C1 [Wei, Longhui; Zhang, Shiliang; Gao, Wen] Peking Univ, Sch Elect Engn & Comp Sci, Beijing 100871, Peoples R China.
   [Yao, Hantao] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Tian, Qi] Huawei, Noahs Ark Lab, Shenzhen 518129, Peoples R China.
   [Tian, Qi] Univ Texas San Antonio, Dept Comp Sci, San Antonio, TX 78249 USA.
C3 Peking University; Chinese Academy of Sciences; Institute of Automation,
   CAS; Huawei Technologies; University of Texas System; University of
   Texas at San Antonio (UTSA)
RP Zhang, SL (corresponding author), Peking Univ, Sch Elect Engn & Comp Sci, Beijing 100871, Peoples R China.
EM longhuiwei@pku.edu.cn; slzhang.jdl@pku.edu.cn; hantao.yao@nlpr.ia.ac.cn;
   wgao@pku.edu.cn; qitian@cs.utsa.edu
OI Yao, Hantao/0000-0001-8125-2864
FU NVIDIA NVAIL program
FX The authors would like to thank NVIDIA NVAIL program for their support.
CR Ahmed E, 2015, PROC CVPR IEEE, P3908, DOI 10.1109/CVPR.2015.7299016
   Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471
   [Anonymous], BRIT MACH VIS C LOND
   [Anonymous], 2017, ARXIV
   [Anonymous], 2017, ARXIV170700798
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2018, IEEE T CIRCUITS SYST
   [Anonymous], 2017, ARXIV170307220
   [Anonymous], 2016, LECT NOTES COMPUT SC, DOI DOI 10.1007/978-3-319-46484-8_29
   [Anonymous], INT C NEUR INT PROC
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], CORR
   [Anonymous], 2007, P IEEE INT WORKSH PE
   [Anonymous], 2014, P ADV NEUR INF PROC
   [Anonymous], ACM T MULTIM COMPUT
   [Anonymous], 2017, P BRIT MACH VIS C
   [Anonymous], 2015, PROC 28 INT C NEURAL
   Bai S, 2017, PROC CVPR IEEE, P3356, DOI 10.1109/CVPR.2017.358
   Bedagkar-Gala A, 2012, PATTERN RECOGN LETT, V33, P1908, DOI 10.1016/j.patrec.2011.09.005
   Chen DP, 2016, PROC CVPR IEEE, P1268, DOI 10.1109/CVPR.2016.142
   Chen YB, 2017, IEEE INT CONF COMP V, P2590, DOI 10.1109/ICCVW.2017.304
   Chen YC, 2018, IEEE T PATTERN ANAL, V40, P392, DOI 10.1109/TPAMI.2017.2666805
   Chen YC, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3402
   Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149
   Cheng DS, 2014, ADV COMPUT VIS PATT, P139, DOI 10.1007/978-1-4471-6296-4_7
   Farenzena M, 2010, PROC CVPR IEEE, P2360, DOI 10.1109/CVPR.2010.5539926
   Felzenszwalb P., 2008, PROC IEEE C COMPUT V, P1
   Fox NA, 2007, IEEE T MULTIMEDIA, V9, P701, DOI 10.1109/TMM.2007.893339
   Hartigan J. A., 1979, Applied Statistics, V28, P100, DOI 10.2307/2346830
   Insafutdinov E, 2016, LECT NOTES COMPUT SC, V9910, P34, DOI 10.1007/978-3-319-46466-4_3
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jose C, 2016, LECT NOTES COMPUT SC, V9909, P875, DOI 10.1007/978-3-319-46454-1_53
   Leibe B., 2017, ARXIV170307737CS
   Li DW, 2017, PROC CVPR IEEE, P7398, DOI 10.1109/CVPR.2017.782
   Li T, 2016, IMAGE VISION COMPUT, V55, P64, DOI 10.1016/j.imavis.2016.04.002
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Liang C, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P771, DOI 10.1145/2733373.2807399
   Liao SC, 2015, IEEE I CONF COMP VIS, P3685, DOI 10.1109/ICCV.2015.420
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Lin J, 2017, PROC CVPR IEEE, P3396, DOI 10.1109/CVPR.2017.362
   Liu H, 2017, IEEE T IMAGE PROCESS, V26, P3492, DOI 10.1109/TIP.2017.2700762
   Matsukawa T, 2016, PROC CVPR IEEE, P1363, DOI 10.1109/CVPR.2016.152
   O'Hare N, 2009, IEEE T MULTIMEDIA, V11, P220, DOI 10.1109/TMM.2008.2009679
   Pishchulin L, 2016, PROC CVPR IEEE, P4929, DOI 10.1109/CVPR.2016.533
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Schumann A, 2017, IEEE COMPUT SOC CONF, P1435, DOI 10.1109/CVPRW.2017.186
   Simonyan K., 2014, Very Deep Convolutional Networks for Large-Scale Image Recognition
   Su C, 2018, IEEE T PATTERN ANAL, V40, P1167, DOI 10.1109/TPAMI.2017.2679002
   Su C, 2018, PATTERN RECOGN, V75, P77, DOI 10.1016/j.patcog.2017.07.005
   Su C, 2017, PATTERN RECOGN, V66, P4, DOI 10.1016/j.patcog.2017.01.006
   Su C, 2016, LECT NOTES COMPUT SC, V9906, P475, DOI 10.1007/978-3-319-46475-6_30
   Sun YF, 2017, IEEE I CONF COMP VIS, P3820, DOI 10.1109/ICCV.2017.410
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214
   Varior Rahul Rama, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9911, P135, DOI 10.1007/978-3-319-46478-7_9
   Varior RR, 2016, LECT NOTES COMPUT SC, V9912, P791, DOI 10.1007/978-3-319-46484-8_48
   Wang FQ, 2016, PROC CVPR IEEE, P1288, DOI 10.1109/CVPR.2016.144
   Wang Z, 2016, IEEE T MULTIMEDIA, V18, P260, DOI 10.1109/TMM.2015.2505083
   Wei LH, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P420, DOI 10.1145/3123266.3123279
   Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511
   Wu SH, 2019, INT J PAVEMENT ENG, V20, P33, DOI 10.1080/10298436.2016.1248204
   Xiao T, 2016, PROC CVPR IEEE, P1249, DOI 10.1109/CVPR.2016.140
   Yao HT, 2017, IEEE INT CON MULTI, P1440, DOI 10.1109/ICME.2017.8019485
   Ye M, 2016, IEEE T MULTIMEDIA, V18, P2553, DOI 10.1109/TMM.2016.2605058
   Yi D, 2014, INT C PATT RECOG, P34, DOI 10.1109/ICPR.2014.16
   Zhang L, 2016, PROC CVPR IEEE, P1239, DOI 10.1109/CVPR.2016.139
   Zhang SL, 2015, IEEE T MULTIMEDIA, V17, P1969, DOI 10.1109/TMM.2015.2478055
   Zhang Y, 2018, PROC CVPR IEEE, P4320, DOI 10.1109/CVPR.2018.00454
   Zhao HY, 2017, PROC CVPR IEEE, P907, DOI 10.1109/CVPR.2017.103
   Zhao R, 2013, PROC CVPR IEEE, P3586, DOI 10.1109/CVPR.2013.460
   Zheng L., 2017, IEEE T IMAGE PROCESS
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhou SP, 2018, IEEE T MULTIMEDIA, V20, P593, DOI 10.1109/TMM.2017.2755983
   Zhou SP, 2017, PROC CVPR IEEE, P5028, DOI 10.1109/CVPR.2017.534
NR 76
TC 119
Z9 134
U1 0
U2 34
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2019
VL 21
IS 4
BP 986
EP 999
DI 10.1109/TMM.2018.2870522
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HQ4XD
UT WOS:000462413700014
DA 2024-07-18
ER

PT J
AU Cho, SI
   Kang, SJ
AF Cho, Sung In
   Kang, Suk-Ju
TI Gradient Prior-Aided CNN Denoiser With Separable Convolution-Based
   Optimization of Feature Dimension
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image denoising; convolutional neural network; image restoration;
   separable convolution; image noise
ID ANISOTROPIC DIFFUSION; IMAGE; ALGORITHM
AB We propose a novel image denoising method based on a convolutional neural network (CNN), which uses the separable convolution and the gradient prior to reduce the computational complexity while enhancing the denoising performance. The proposed method converts the existing convolution filter in the conventional CNN denoiser to cascaded vertical and horizontal separable convolutions and reduces the number of feature channels between these convolutions by analyzing the distribution of convolution weights. The proposed separable convolution with feature dimension shrinking can greatly reduce the number of multiplications for CNN while minimizing the degradation of denoising quality. In addition, gradients of a given image are used as input for the proposed CNN denoiser by exploiting the relation between an anisotropic diffusion-based denoiser and a residual CNN denoiser to improve the quality of the image denoising. The simulation results showed that the proposed method provided comparable denoising quality while reducing the number of multiplications to 41% compared to the existing state-of-the-art CNN denoiser.
C1 [Cho, Sung In] Daegu Univ, Dept Elect & Elect Engn, Gyongsan 38453, South Korea.
   [Kang, Suk-Ju] Sogang Univ, Dept Elect Engn, Seoul 121742, South Korea.
C3 Daegu University; Sogang University
RP Kang, SJ (corresponding author), Sogang Univ, Dept Elect Engn, Seoul 121742, South Korea.
EM csi2267@daegu.ac.kr; sjkang@sogang.ac.kr
FU Daegu University [20170313]
FX This work was supported by the Daegu University Research Grant 20170313.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Jingdong Wang.
CR Abadi Martin, 2016, TENSORFLOW LARGE SCA, V16, P265
   Agostinelli F., 2013, ADV NEURAL INFORM PR, V26, P1493
   [Anonymous], 2002, FOOTBALL SEQUENCES
   Benierbah S, 2006, IEE P-VIS IMAGE SIGN, V153, P237, DOI 10.1049/ip-vis:20050129
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Burger H., 2012, CVPR
   Cho SI, 2018, IEEE T MULTIMEDIA, V20, P1738, DOI 10.1109/TMM.2017.2781371
   Cho SI, 2016, DIGIT SIGNAL PROCESS, V48, P27, DOI 10.1016/j.dsp.2015.08.019
   Cho SI, 2014, PATTERN RECOGN LETT, V46, P36, DOI 10.1016/j.patrec.2014.05.003
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Dai JJ, 2013, IEEE T CIRC SYST VID, V23, P128, DOI 10.1109/TCSVT.2012.2203203
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Goffman-Vinopal L, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P353
   Gu SH, 2014, PROC CVPR IEEE, P2862, DOI 10.1109/CVPR.2014.366
   Han, 2015, LEARNING BOTH WEIGHT
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Jakhetiya V, 2017, IEEE T MULTIMEDIA, V19, P93, DOI 10.1109/TMM.2016.2609419
   Jones K., 2008, ENERGY STAR PROGRAM
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kim YH, 2009, IEEE T CIRC SYST VID, V19, P1051, DOI 10.1109/TCSVT.2009.2020251
   Kingma D. P., 2014, arXiv
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P1004, DOI 10.1109/TIP.2016.2631888
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   Rensselaer Polytechnic Institute, 2002, CIPR DAT
   Roth S, 2009, INT J COMPUT VISION, V82, P205, DOI 10.1007/s11263-008-0197-6
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xu J., 2017, P IEEE INT C COMP VI, P1096
   Xu J, 2015, IEEE I CONF COMP VIS, P244, DOI 10.1109/ICCV.2015.36
   Yu FP, 2016, PROCEEDINGS OF 2016 SYMPOSIUM ON PIEZOELECTRICITY, ACOUSTIC WAVES, AND DEVICE APPLICATIONS (SPAWDA), P1, DOI 10.1109/SPAWDA.2016.7829944
   Zhang K., 2017, PROC CVPR IEEE, P3929, DOI [DOI 10.1109/CVPR.2017.300, 10.1109/CVPR.2017.300]
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
NR 36
TC 28
Z9 34
U1 3
U2 27
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2019
VL 21
IS 2
BP 484
EP 493
DI 10.1109/TMM.2018.2859791
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HJ7GT
UT WOS:000457364400017
DA 2024-07-18
ER

PT J
AU Wang, SF
   Xin, YJ
   Kong, DH
   Yin, BC
AF Wang, Shaofan
   Xin, Yongjia
   Kong, Dehui
   Yin, Baocai
TI Unsupervised Learning of Human Pose Distance Metric via Sparsity
   Locality Preserving Projections
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Pose similarity; distance metric; unsupervised learning; sparse
   representation; locality preserving projection
ID MOTION; RETRIEVAL; SELECTION; MODEL
AB Human poses admit complicated articulations and multigranular similarity. Previous works on learning human pose metric utilize sparse models, which concentrate large weights on highly similar poses and fail to depict an overall structure of poses with multigranular similarity. Moreover, previous works require a large number of similar/dissimilar annotated pairwise poses, which is an tedious task and remains inaccurate due to different subjective judgments of experts. Motivated by graph-based neighbor assignment techniques, we propose an unsupervised model called sparsity locality preserving projection with adaptive neighbors (SLPPAN), for learning human pose distance metric. By using a property of the graph Laplacian, SLPPAN introduces a fixed-rank constraint to enforce an adaptive graph structure of poses and learns the neighbor assignment, the similarity measurement, and pose metric simultaneously. Experiments on pose retrieval of the CMU Mocap database demonstrate that SLPPAN outperforms traditional pose metric learning methods by capturing viewpoint variations of human poses. Experiments on keyframe extraction of the MSRAction3D database demonstrate that SLPPAN outperforms current methods by precisely detecting important frames of action sequences.
C1 [Wang, Shaofan] Beijing Univ Technol, Fac Informat Technol, Beijing Key Lab Multimedia & Intelligent Software, Beijing 100124, Peoples R China.
   [Xin, Yongjia] Tencent Technol Co Ltd, Nat Language Proc, Beijing 100080, Peoples R China.
   [Kong, Dehui] Beijing Univ Technol, Fac Informat Technol, Beijing Key Lab Multimedia & Intelligent Software, Beijing Adv Innovat Ctr Future Internet Technol, Beijing 100124, Peoples R China.
   [Yin, Baocai] Dalian Univ Technol, Fac Elect Informat & Elect Engn, Dalian 116024, Peoples R China.
C3 Beijing University of Technology; Tencent; Beijing University of
   Technology; Dalian University of Technology
RP Kong, DH (corresponding author), Beijing Univ Technol, Fac Informat Technol, Beijing Key Lab Multimedia & Intelligent Software, Beijing Adv Innovat Ctr Future Internet Technol, Beijing 100124, Peoples R China.
EM wangshaofan@bjut.edu.cn; xinyongjia@emails.bjut.edu.cn; kdh@bjut.edu.cn;
   ybc@dlut.edu.cn
OI WANG, SHAOFAN/0000-0002-3045-624X
FU Natural Science Foundation of China [61390510, 61632006, 61772049];
   Beijing Natural Science Foundation [4162009]; Beijing Municipal Science
   and Technology Project [Z171100000517003, Z171100000517004,
   Z171100004417023, Z161100001116072]
FX This work was supported in part by the Natural Science Foundation of
   China under Grant 61390510, Grant 61632006, and Grant 61772049, in part
   by the Beijing Natural Science Foundation under Grant 4162009, and in
   part by the Beijing Municipal Science and Technology Project under Grant
   Z171100000517003, Grant Z171100000517004, Grant Z171100004417023, and
   Grant Z161100001116072. The associate editor coordinating the review of
   this manuscript and approving it for publication was Prof. Dong Xu.
CR [Anonymous], P GRAPH THEOR COMB A
   [Anonymous], 2012, CMU GRAPHICS LAB MOT
   [Anonymous], 2011, COMPUTER ANIMATION A
   Arikan O, 2003, ACM T GRAPHIC, V22, P402, DOI 10.1145/882262.882284
   Belkin M, 2002, ADV NEUR IN, V14, P585
   Cai XY, 2016, IEEE T MULTIMEDIA, V18, P141, DOI 10.1109/TMM.2015.2505089
   Chen C, 2011, IEEE T VIS COMPUT GR, V17, P1676, DOI 10.1109/TVCG.2010.272
   Chen C, 2009, COMPUT ANIMAT VIRT W, V20, P267, DOI 10.1002/cav.297
   Cong Y, 2012, IEEE T MULTIMEDIA, V14, P66, DOI 10.1109/TMM.2011.2166951
   Guo XJ, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3547
   Harada T., 2004, P IEEE INT C HUM ROB, P569
   He X. F., 2005, P ADV NEUR INF PROC
   Ho ESL, 2009, IEEE T VIS COMPUT GR, V15, P481, DOI 10.1109/TVCG.2008.199
   Huang KS, 2005, VISUAL COMPUT, V21, P532, DOI 10.1007/s00371-005-0316-0
   Jin Y, 2012, LECT NOTES COMPUT SC, V7585, P667, DOI 10.1007/978-3-642-33885-4_78
   Jing Wang, 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P232
   Kovar L, 2002, ACM T GRAPHIC, V21, P473, DOI 10.1145/566570.566605
   Lei J, 2015, SIGNAL PROCESS, V108, P136, DOI 10.1016/j.sigpro.2014.08.030
   Li WB, 2010, 2010 THE 3RD INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND INDUSTRIAL APPLICATION (PACIIA2010), VOL I, P9, DOI 10.1109/cvprw.2010.5543273
   Li W, 2018, IEEE T PATTERN ANAL, V40, P1114, DOI 10.1109/TPAMI.2017.2704624
   Liu HP, 2014, IEEE T IND INFORM, V10, P1736, DOI 10.1109/TII.2014.2330798
   Liu XM, 2013, VISUAL COMPUT, V29, P85, DOI 10.1007/s00371-012-0676-1
   López-Méndez A, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.49
   Muller Meinard., 2006, P ACM SIGGRAPHEUROGR, P137
   Ma ZG, 2017, IEEE T MULTIMEDIA, V19, P1558, DOI 10.1109/TMM.2017.2659221
   Müller M, 2005, ACM T GRAPHIC, V24, P677, DOI 10.1145/1073204.1073247
   Nie FP, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P977, DOI 10.1145/2623330.2623726
   Raptis M, 2013, PROC CVPR IEEE, P2650, DOI 10.1109/CVPR.2013.342
   Ren RD, 2012, IEEE T MULTIMEDIA, V14, P1652, DOI 10.1109/TMM.2012.2199971
   Tang JKT, 2008, COMPUT ANIMAT VIRT W, V19, P211, DOI 10.1002/cav.260
   Wang LC, 2014, IEEE T MULTIMEDIA, V16, P751, DOI 10.1109/TMM.2014.2298382
   Wang Z, 2016, SIGNAL PROCESS, V120, P691, DOI 10.1016/j.sigpro.2014.11.015
   Xia GY, 2017, IEEE T IND ELECTRON, V64, P1589, DOI 10.1109/TIE.2016.2610946
   Xiao QK, 2015, SOFT COMPUT, V19, P133, DOI 10.1007/s00500-014-1237-5
   Xiao SJ, 2016, IEEE T NEUR NET LEAR, V27, P2268, DOI 10.1109/TNNLS.2015.2472284
   Xu D, 2012, IEEE T IMAGE PROCESS, V21, P316, DOI 10.1109/TIP.2011.2160956
   Yan Y, 2016, IEEE T MULTIMEDIA, V18, P2494, DOI 10.1109/TMM.2016.2602938
   Yang Y, 2009, PR ELECTROMAGN RES S, P311, DOI 10.1145/1631272.1631316
   Yang Y, 2015, INT J COMPUT VISION, V113, P113, DOI 10.1007/s11263-014-0781-x
   Yao BP, 2012, LECT NOTES COMPUT SC, V7575, P173, DOI 10.1007/978-3-642-33765-9_13
   Yu J, 2012, IEEE T IMAGE PROCESS, V21, P4636, DOI 10.1109/TIP.2012.2207395
   Zhou DY, 2004, ADV NEUR IN, V16, P321
NR 42
TC 6
Z9 7
U1 1
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2019
VL 21
IS 2
BP 314
EP 327
DI 10.1109/TMM.2018.2859029
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HJ7GT
UT WOS:000457364400004
DA 2024-07-18
ER

PT J
AU Sang, JT
   Yan, M
   Xu, CS
AF Sang, Jitao
   Yan, Ming
   Xu, Changsheng
TI Understanding Dynamic Cross-OSN Associations for Cold-Start
   Recommendation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-OSN association; dynamic user modeling; cold-start recommendation
AB Online social networks (OSNs) have become an essential part of people's daily life, and an increasing number of users are now using multiple OSNs for different social media services simultaneously. As a result, user's interests and preferences usually distribute in different OSNs. While most of the existing work mainly aggregates the distributed user behaviors or features directly, recently very few efforts have been focused on understanding the crass-OSN association from collective user behaviors. In this paper, we go one step further to consider the dynamic characteristic of user behaviors and propose a dynamic cross-OSN association mining framework. In this framework, dynamic user modeling is first conducted to capture the drift of user interest in each OSN. A session-based factorization method is then proposed to establish the cross-OSN association in a dynamic manner, by incrementally updating the derived association each time a new session of data arrives. Based on the derived dynamic association, we finally design a cold-start YouTube video recommendation application, by only utilizing users' behaviors in Twitter. Experiments are conducted using real-world user data from Twitter and YouTube. The results demonstrate the effectiveness of this proposed framework in capturing the underlying association between different OSNs and achieving superior cold-start recommendation performance.
C1 [Sang, Jitao] Beijing Jiaotong Univ, Sch Comp & Informat Technol, Beijing 100044, Peoples R China.
   [Sang, Jitao] Beijing Jiaotong Univ, Beijing Key Lab Traff Data Anal & Min, Beijing 100044, Peoples R China.
   [Yan, Ming] Alibaba, Hangzhou 311121, Zhejiang, Peoples R China.
   [Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
C3 Beijing Jiaotong University; Beijing Jiaotong University; Alibaba Group;
   Chinese Academy of Sciences; Institute of Automation, CAS
RP Sang, JT (corresponding author), Beijing Jiaotong Univ, Sch Comp & Informat Technol, Beijing 100044, Peoples R China.; Sang, JT (corresponding author), Beijing Jiaotong Univ, Beijing Key Lab Traff Data Anal & Min, Beijing 100044, Peoples R China.
EM jtsang@bjtu.edu.cn; ym646016681@gmail.com; csxu@nlpr.ia.ac.cn
RI xu, cj/HJZ-3488-2023
OI xu, chang sheng/0000-0001-8343-9665
FU National Natural Science Foundation of China [61432019, 61720106006,
   U1705262, 61632004, 61632115, 61672518, 61473030, 61332016]; Key
   Research Program of Frontier Sciences, CAS [QYZDJ-SSW-JSC039]; Beijing
   Municipal Science and Technology Commission [Z181100008918012]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61432019, Grant 61720106006, Grant
   U1705262, Grant 61632004, Grant 61632115, Grant 61672518, Grant
   61473030, and Grant 61332016; in part by the Key Research Program of
   Frontier Sciences, CAS under Grant QYZDJ-SSW-JSC039, and in part by the
   Beijing Municipal Science and Technology Commission (No.
   Z181100008918012). The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Benoit Huet.
   (Corresponding author: Jitao Sang.)
CR Abel F, 2011, LECT NOTES COMPUT SC, V6757, P28, DOI 10.1007/978-3-642-22233-7_3
   [Anonymous], 2005, Proceedingsofthe14th ACM International Conference on Information and Knowledge Management. CIKM '05, DOI DOI 10.1145/1099554.1099689
   [Anonymous], 2012, ACM MM'12'
   [Anonymous], 2015, ACM T MULTIM COMPUT
   [Anonymous], 2012, P 20 ACM INT C MULTI
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Blei DM, 2003, P 26 ANN INT ACM SIG, P127, DOI DOI 10.1145/860435.860460
   CAMILLI G, 1978, PSYCHOL BULL, V85, P163, DOI 10.1037/0033-2909.85.1.163
   Chen Terence., 2012, P 2012 ACM WORKSHOP, P67
   Farseev A, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P235, DOI 10.1145/2671188.2749381
   Gao SH, 2010, PROC CVPR IEEE, P3555, DOI 10.1109/CVPR.2010.5539943
   Herlocker JL, 2004, ACM T INFORM SYST, V22, P5, DOI 10.1145/963770.963772
   Hsieh CK, 2016, PROCEEDINGS OF THE 25TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'16), P51, DOI 10.1145/2872427.2883006
   Khoshgoftaar T.M., 2009, ADV ARTIFICIAL INTEL, DOI [10.1155/2009/421425, DOI 10.1155/2009/421425]
   Koren Y., 2008, P 14 ACM SIGKDD INT, P426
   Koren Y, 2010, COMMUN ACM, V53, P89, DOI 10.1145/1721654.1721677
   Lika B, 2014, EXPERT SYST APPL, V41, P2065, DOI 10.1016/j.eswa.2013.09.005
   Liu NathanNan., 2010, RecSys, P95
   Lops P, 2011, RECOMMENDER SYSTEMS HANDBOOK, P73, DOI 10.1007/978-0-387-85820-3_3
   Min WQ, 2015, IEEE T MULTIMEDIA, V17, P1787, DOI 10.1109/TMM.2015.2463226
   Mnih A., 2007, ADV NEURAL INFORM PR, V20
   Rendle S., 2009, P 25 C UNCERTAINTY A, P452
   Roy SD, 2013, IEEE T MULTIMEDIA, V15, P1255, DOI 10.1109/TMM.2013.2265079
   Sang JT, 2017, ACM T INTEL SYST TEC, V8, DOI 10.1145/3001594
   Sang JT, 2016, IEEE INT SYM MULTIM, P481, DOI 10.1109/ISM.2016.130
   Sang JT, 2012, IEEE T MULTIMEDIA, V14, P883, DOI 10.1109/TMM.2012.2188782
   Sang Jitao, 2018, IEEE T MULTIMEDIA
   Sarwar B., 2001, P 10 INT C WORLD WID, P285, DOI 10.1145/371920.372071
   Shi Y, 2014, ACM COMPUT SURV, V47, DOI 10.1145/2556270
   Shi Y, 2011, LECT NOTES COMPUT SC, V6787, P305, DOI 10.1007/978-3-642-22362-4_26
   Venkatadri G, 2016, PROCEEDINGS OF THE 25TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'16), P1249, DOI 10.1145/2872427.2883015
   Wang Z, 2013, IEEE T MULTIMEDIA, V15, P698, DOI 10.1109/TMM.2012.2237022
   Xiang L., 2010, P 16 ACM SIGKDD INT, P723, DOI 10.1145/1835804.1835896
   Xiong L, 2010, P SIAM INT C DAT MIN, P211, DOI DOI 10.1137/1.9781611972801.19
   Yan M, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P19, DOI 10.1145/2671188.2749344
   Yan M, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P557, DOI 10.1145/2647868.2654920
   Yang XT, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P963, DOI 10.1145/2733373.2806375
   Yin HZ, 2014, SIGMOD'14: PROCEEDINGS OF THE 2014 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P1543, DOI 10.1145/2588555.2593685
   Yu H., 2015, INL P INT AAAI C WEB, P533
   Zhao HY, 2013, PROCEEDINGS OF 2013 IEEE INTERNATIONAL CONFERENCE ON GREY SYSTEMS AND INTELLIGENT SERVICES (GSIS), P200, DOI 10.1109/GSIS.2013.6714773
   Zhao WNX, 2011, LECT NOTES COMPUT SC, V6611, P338, DOI 10.1007/978-3-642-20161-5_34
   Zhong CT, 2014, WWW'14: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P305, DOI 10.1145/2566486.2568031
NR 42
TC 7
Z9 8
U1 1
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2018
VL 20
IS 12
BP 3439
EP 3451
DI 10.1109/TMM.2018.2839530
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HA4EY
UT WOS:000450212600021
DA 2024-07-18
ER

PT J
AU Wang, SG
   Cheng, J
   Liu, HJ
   Wang, F
   Zhou, H
AF Wang, Shiguang
   Cheng, Jian
   Liu, Haijun
   Wang, Feng
   Zhou, Hui
TI Pedestrian Detection via Body Part Semantic and Contextual Information
   With DNN
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Pedestrian detection; deep learning; occlusion handling; adaptive
   context selection
ID OBJECT DETECTION; OCCLUSION; TRACKING
AB Pedestrian detection has achieved great improvements in recent years, while complex occlusion handling and high-accurate localization are still the most important problems. To take advantage of the body part semantic information and the contextual information for pedestrian detection, we propose the part and context network (PCN) in this paper. A PCN is composed of three branches: the basic branch; the part branch; and the context branch. It specially utilizes two branches to detect the pedestrians through the body part semantic information and the contextual information, respectively. In the part branch, the semantic information of body parts can communicate with each other via long short-term memory (LSTM). In the context branch, we adopt a local competition mechanism (maxout) for adaptive context scale selection. By combining the outputs of all branches, we develop a strong complementary pedestrian detector with a lower miss rate and higher localization accuracy, especially for the occlusion pedestrian. The combination of the body part semantic information and the contextual information in pedestrian detection is fully explored in this paper. Comprehensive evaluations on three challenging pedestrian detection datasets (i.e., Caltech, INRIA and KITTI) well demonstrate the effectiveness of our proposed PCN.
C1 [Wang, Shiguang; Cheng, Jian; Liu, Haijun; Wang, Feng; Zhou, Hui] Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 11731, Sichuan, Peoples R China.
C3 University of Electronic Science & Technology of China
RP Wang, SG (corresponding author), Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 11731, Sichuan, Peoples R China.
EM xiaohu_wyyx@163.com; chengjian@uestc.edu.cn; haijun_liu@126.com;
   feng.wff@gmail.com; smarthuizhou@gmail.com
RI Wang, Shiguang/A-5476-2018; Wang, Hui/HMU-9512-2023
OI Cheng, Jian/0000-0001-6966-0531; Wang, Feng/0000-0002-7669-2547
FU National Natural Science Foundation of China [61671125, 61201271,
   61301269]; State Key Laboratory of Synthetical Automation for Process
   Industries [PAL-N201401]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61671125, 61201271, and 61301269 and in
   part by the State Key Laboratory of Synthetical Automation for Process
   Industries under Grant PAL-N201401.
CR [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.141
   [Anonymous], IEEE T CIRCUITS SYST
   [Anonymous], 2014, ARXIV14117714
   [Anonymous], 2015, ARXIV151008160
   [Anonymous], 2017, IEEE T PATTERN ANAL
   [Anonymous], 2013, ICML
   [Anonymous], 2013, PMLR
   [Anonymous], 2015, COMPUTER SCI
   [Anonymous], 2014, PROC IEEE C COMPUTER, P580, DOI [10.1109/CVPR.2014.81, DOI 10.1109/CVPR.2014.81]
   [Anonymous], 2017, P IEEE C COMP VIS PA
   [Anonymous], 2010, LONG SHORT TERM MEMO
   Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314
   Bilal M, 2017, IEEE T CIRC SYST VID, V27, P2260, DOI 10.1109/TCSVT.2016.2581660
   Broggi A, 2016, SPRINGER HANDBOOK OF ROBOTICS, P1627
   Cai ZW, 2016, LECT NOTES COMPUT SC, V9908, P354, DOI 10.1007/978-3-319-46493-0_22
   Cai ZW, 2015, IEEE I CONF COMP VIS, P3361, DOI 10.1109/ICCV.2015.384
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145
   Chu CT, 2013, IEEE T MULTIMEDIA, V15, P1602, DOI 10.1109/TMM.2013.2266634
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dollár P, 2012, IEEE T PATTERN ANAL, V34, P743, DOI 10.1109/TPAMI.2011.155
   Du XZ, 2017, IEEE WINT CONF APPL, P953, DOI 10.1109/WACV.2017.111
   Enzweiler M, 2010, PROC CVPR IEEE, P990, DOI 10.1109/CVPR.2010.5540111
   Ess A., 2008, COMPUTER VISION PATT, V2008, P1, DOI [10.1109/CVPR.2008.4587581, DOI 10.1109/CVPR.2008.4587581]
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Gidaris S, 2016, PROC CVPR IEEE, P789, DOI 10.1109/CVPR.2016.92
   Gidaris S, 2015, IEEE I CONF COMP VIS, P1134, DOI 10.1109/ICCV.2015.135
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Guindel C, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON VEHICULAR ELECTRONICS AND SAFETY (ICVES), P145, DOI 10.1109/ICVES.2017.7991916
   Hosang J, 2015, PROC CVPR IEEE, P4073, DOI 10.1109/CVPR.2015.7299034
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Li JN, 2017, IEEE T MULTIMEDIA, V19, P944, DOI 10.1109/TMM.2016.2642789
   Lin L, 2015, IEEE T PATTERN ANAL, V37, P959, DOI 10.1109/TPAMI.2014.2359888
   Luo P, 2014, PROC CVPR IEEE, P899, DOI 10.1109/CVPR.2014.120
   Mathias M, 2013, IEEE I CONF COMP VIS, P1505, DOI 10.1109/ICCV.2013.190
   Mikolajczyk K, 2004, LECT NOTES COMPUT SC, V3021, P69
   Mohan A, 2001, IEEE T PATTERN ANAL, V23, P349, DOI 10.1109/34.917571
   Ouyang W., 2017, IEEE T PATTERN ANAL
   Ouyang W., 2017, ARXIV170207054
   Ouyang WL, 2016, IEEE T CIRC SYST VID, V26, P2123, DOI 10.1109/TCSVT.2015.2501940
   Ouyang WL, 2012, PROC CVPR IEEE, P3258, DOI 10.1109/CVPR.2012.6248062
   Ozer IB, 2002, IEEE T MULTIMEDIA, V4, P283, DOI 10.1109/TMM.2002.1017740
   Ren J, 2017, PROC CVPR IEEE, P752, DOI 10.1109/CVPR.2017.87
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1476, DOI 10.1109/TPAMI.2016.2601099
   Sermanet P, 2013, PROC CVPR IEEE, P3626, DOI 10.1109/CVPR.2013.465
   Tian YL, 2015, IEEE I CONF COMP VIS, P1904, DOI 10.1109/ICCV.2015.221
   Tian YL, 2015, PROC CVPR IEEE, P5079, DOI 10.1109/CVPR.2015.7299143
   Wang S.-H., 2017, P BRIT MACH VIS C
   Wojek C., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1993, DOI 10.1109/CVPR.2011.5995547
   Wojek C, 2009, PROC CVPR IEEE, P794, DOI 10.1109/CVPRW.2009.5206638
   Xie Q, 2018, IEEE T MULTIMEDIA, V20, P580, DOI 10.1109/TMM.2017.2751965
   Xu D, 2017, PROC CVPR IEEE, P4236, DOI 10.1109/CVPR.2017.451
   Ye QX, 2017, PROC CVPR IEEE, P2057, DOI 10.1109/CVPR.2017.222
   Zeng XY, 2013, IEEE I CONF COMP VIS, P121, DOI 10.1109/ICCV.2013.22
   Zhang LL, 2016, LECT NOTES COMPUT SC, V9906, P443, DOI 10.1007/978-3-319-46475-6_28
   Zhu L, 2010, PROC CVPR IEEE, P1062, DOI 10.1109/CVPR.2010.5540096
   Zhu Y, 2015, PROC CVPR IEEE, P4703, DOI 10.1109/CVPR.2015.7299102
NR 57
TC 32
Z9 35
U1 0
U2 32
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2018
VL 20
IS 11
BP 3148
EP 3159
DI 10.1109/TMM.2018.2829602
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GX5WL
UT WOS:000447824800022
DA 2024-07-18
ER

PT J
AU Zhou, Y
   Li, LD
   Wu, JJ
   Gu, K
   Dong, WS
   Shi, GM
AF Zhou, Yu
   Li, Leida
   Wu, Jinjian
   Gu, Ke
   Dong, Weisheng
   Shi, Guangming
TI Blind Quality Index for Multiply Distorted Images Using Biorder
   Structure Degradation and Nonlocal Statistics
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Quality evaluation; multiply distorted images; spatial contrast; spatial
   distribution; biorder structures; nonlocal statistics
ID SHARPNESS ASSESSMENT; BLOCKING ARTIFACTS; GRADIENT MAGNITUDE;
   REGULARITY; EFFICIENT
AB In the past decade, extensive image quality metrics have been proposed. The majority of them are tailored for the images that contain a specific type of distortion. However, in practice, the images are usually degraded by different types of distortions simultaneously. This poses great challenges to the existing quality metrics. Motivated by this, this paper proposes a no-reference quality index for the multiply distorted images using the biorder structure degradation and the nonlocal statistics. The design philosophy is inspired by the fact that the human visual system (HVS) is highly sensitive to the degradations of both the spatial contrast and the spatial distribution, which are prone to be changed by the joint effects of the multiple distortions. Specifically, the multiresolution representation of the image is first built by downsampling to simulate the hierarchical property of the HVS. Then, the structure degradation is calculated to measure the spatial contrast. Considering the fact that the human visual cortex has the separate mechanisms to perceive the first- and second-order structures, dubbed biorder structures, the degradations of biorder structures are calculated to account for the spatial contrast, producing the first group of the quality-aware features. Furthermore, the nonlocal self-similarity statistics is calculated to measure the spatial distribution, producing the second group of features. Finally, all the features are fed into the random forest regression model to learn the quality model for the multiply distorted images. Extensive experimental results conducted on the three public databases demonstrate the superiority of the proposed metric to the state-of-the-art metrics. Moreover, the proposed metric is also advantageous over the existing metrics in terms of the generalization ability.
C1 [Zhou, Yu; Li, Leida] China Univ Min & Technol, Sch Informat & Control Engn, Xuzhou 221116, Jiangsu, Peoples R China.
   [Wu, Jinjian; Dong, Weisheng; Shi, Guangming] Xidian Univ, Sch Artif Intelligence, Xian 710071, Shaanxi, Peoples R China.
   [Gu, Ke] Beijing Univ Technol, BJUT, Fac Informat Technol, Beijing 100124, Peoples R China.
C3 China University of Mining & Technology; Xidian University; Beijing
   University of Technology
RP Li, LD (corresponding author), China Univ Min & Technol, Sch Informat & Control Engn, Xuzhou 221116, Jiangsu, Peoples R China.
EM zhouy7476@cumt.edu.cn; lileida@cumt.edu.cn;
   jinjian.wu@mail.xidian.edu.cn; guke.doctor@gmail.com;
   wsdong@mail.xidian.edu.cn; ginshi@xidian.edu.cn
RI li, li/HII-4157-2022; Wu, Jinjian/GQH-0222-2022; Gu, Ke/AAJ-9684-2021;
   Li, Li/AEM-3636-2022
FU National Natural Science Foundation of China [61771473, 61379143]; Six
   Talent Peaks High-level Talents in Jiangsu Province [XYDXX-063]; Qing
   Lan Project
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61771473 and Grant 61379143, in part by
   the Six Talent Peaks High-level Talents in Jiangsu Province under Grant
   XYDXX-063, and in part by the Qing Lan Project.
CR Anegekuh L, 2015, IEEE T MULTIMEDIA, V17, P1323, DOI 10.1109/TMM.2015.2444098
   [Anonymous], MSRTR2011114
   Bahrami K, 2014, IEEE SIGNAL PROC LET, V21, P751, DOI 10.1109/LSP.2014.2314487
   Bellamine I, 2015, P INT SYST COMP VIS, DOI [10.1109/ISACV.2015.7105545, DOI 10.1109/ISACV.2015.7105545]
   Celik T, 2016, J MOD OPTIC, V63, P1600, DOI 10.1080/09500340.2016.1163427
   David M., 1982, VISION COMPUTATIONAL, V1
   Ding L, 2017, IEEE T IMAGE PROCESS, V26, P1799, DOI 10.1109/TIP.2017.2665972
   Gu K, 2016, IEEE T CYBERNETICS, V46, P284, DOI 10.1109/TCYB.2015.2401732
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Gu K, 2014, IEEE T BROADCAST, V60, P555, DOI 10.1109/TBC.2014.2344471
   Gu K, 2013, IEEE WORKSHOP SIG, P241, DOI 10.1109/SiPS.2013.6674512
   Hadizadeh H, 2016, IEEE SIGNAL PROC LET, V23, DOI 10.1109/LSP.2016.2617743
   Jayaraman D, 2012, CONF REC ASILOMAR C, P1693, DOI 10.1109/ACSSC.2012.6489321
   Jiang QP, 2018, IEEE T MULTIMEDIA, V20, P2035, DOI 10.1109/TMM.2017.2763321
   Larsson J, 2006, J NEUROPHYSIOL, V95, P862, DOI 10.1152/jn.00668.2005
   Li LD, 2016, IEEE T CYBERNETICS, V46, P39, DOI 10.1109/TCYB.2015.2392129
   Li LD, 2017, IEEE T MULTIMEDIA, V19, P1030, DOI 10.1109/TMM.2016.2640762
   Li LD, 2016, IEEE T MULTIMEDIA, V18, P1085, DOI 10.1109/TMM.2016.2545398
   Li LD, 2015, J VIS COMMUN IMAGE R, V30, P153, DOI 10.1016/j.jvcir.2015.04.001
   Li LD, 2014, IEEE SIGNAL PROC LET, V21, P918, DOI 10.1109/LSP.2014.2320743
   Li QH, 2017, NEUROCOMPUTING, V236, P93, DOI 10.1016/j.neucom.2016.09.105
   Li QH, 2016, IEEE SIGNAL PROC LET, V23, P541, DOI 10.1109/LSP.2016.2537321
   Lin WS, 2011, J VIS COMMUN IMAGE R, V22, P297, DOI 10.1016/j.jvcir.2011.01.005
   Liu AM, 2012, IEEE T IMAGE PROCESS, V21, P1500, DOI 10.1109/TIP.2011.2175935
   Lu YA, 2015, IEEE SIGNAL PROC LET, V22, P1811, DOI 10.1109/LSP.2015.2436908
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Moorthy AK, 2010, IEEE SIGNAL PROC LET, V17, P513, DOI 10.1109/LSP.2010.2043888
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Oszust M, 2017, IEEE SIGNAL PROC LET, V24, P1656, DOI 10.1109/LSP.2017.2754539
   Pan XX, 2016, IEEE GEOSCI REMOTE S, V13, P1855, DOI 10.1109/LGRS.2016.2614890
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Sang QB, 2014, J VIS COMMUN IMAGE R, V25, P1625, DOI 10.1016/j.jvcir.2014.08.002
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Sun W, 2017, PATTERN RECOGN, V61, P153, DOI 10.1016/j.patcog.2016.07.033
   Tang LJ, 2017, J VIS COMMUN IMAGE R, V49, P204, DOI 10.1016/j.jvcir.2017.09.010
   Temel D, 2015, IEEE IMAGE PROC, P1682, DOI 10.1109/ICIP.2015.7351087
   Wan ZL, 2017, IEEE INT CON MULTI, P73, DOI 10.1109/ICME.2017.8019337
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435
   Wu JJ, 2016, SIGNAL PROCESS-IMAGE, V47, P16, DOI 10.1016/j.image.2016.05.008
   Wu JJ, 2016, INFORM SCIENCES, V351, P18, DOI 10.1016/j.ins.2016.02.043
   Wu JJ, 2015, IEEE T IMAGE PROCESS, V24, P4602, DOI 10.1109/TIP.2015.2460467
   Wu JJ, 2014, IEEE SIGNAL PROC LET, V21, P437, DOI 10.1109/LSP.2014.2304714
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P4850, DOI 10.1109/TIP.2014.2355716
   Xue WF, 2013, PROC CVPR IEEE, P995, DOI 10.1109/CVPR.2013.133
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Yan YP, 2016, IET IMAGE PROCESS, V10, P1017, DOI 10.1049/iet-ipr.2016.0177
   Zhang J, 2014, IEEE T CIRC SYST VID, V24, P915, DOI 10.1109/TCSVT.2014.2302380
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang XF, 2017, IEEE SIGNAL PROC LET, V24, P1403, DOI 10.1109/LSP.2017.2732680
   Zhang XF, 2017, IEEE SIGNAL PROC LET, V24, P96, DOI 10.1109/LSP.2016.2641456
   Zhang XF, 2016, IEEE T IMAGE PROCESS, V25, P4158, DOI 10.1109/TIP.2016.2588326
   Zhang XF, 2015, IEEE T IMAGE PROCESS, V24, P6048, DOI 10.1109/TIP.2015.2485780
   Zhang Y., 2012, P IEEE C COMP VIS PA, P16
   Zhang YZ, 2015, IEEE INT SYMP CIRC S, P2796, DOI 10.1109/ISCAS.2015.7169267
   Zheng LP, 2009, ICCIT: 2009 FOURTH INTERNATIONAL CONFERENCE ON COMPUTER SCIENCES AND CONVERGENCE INFORMATION TECHNOLOGY, VOLS 1 AND 2, P1596, DOI 10.1109/ICCIT.2009.66
   Zhu SH, 2008, 2008 7TH WORLD CONGRESS ON INTELLIGENT CONTROL AND AUTOMATION, VOLS 1-23, P5056, DOI 10.1109/WCICA.2008.4593750
NR 59
TC 37
Z9 38
U1 1
U2 34
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2018
VL 20
IS 11
BP 3019
EP 3032
DI 10.1109/TMM.2018.2829607
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GX5WL
UT WOS:000447824800012
DA 2024-07-18
ER

PT J
AU He, JH
   Huang, SH
   Tang, SH
   Huang, JW
AF He, Junhui
   Huang, Shuhao
   Tang, Shaohua
   Huang, Jiwu
TI JPEG Image Encryption With Improved Format Compatibility and File Size
   Preservation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE JPEG image encryption; format compatibility; file size preservation;
   bitstream
AB Image encryption techniques can be used to ensure the security and privacy of valuable images. The related works in this field have focused more on raster images than on compressed images. Many existing JPEG image encryption schemes are not quite well compatible with the JPEG standard, or the file size of an encrypted JPEG image is apparently increased. In this paper, a novel bitstream-based JPEG image encryption method is presented. First, the groups of successive DC codes that encode the quantized DC coefficient differences with the same sign are permuted within each group. Second, the left half and the right half of a group, whose size will increase with the number of iterations, of consecutive DC codes may be swapped with each other, depending on whether an overflow of quantized DC coefficients occurs during decoding. Third, all AC codes are classified into 63 categories according to their zero-run lengths, then the AC codes within each category are, respectively, scrambled. Finally, all MCUs, except for DC codes, are randomly shuffled as a whole. Moreover, an image-content-related encryption key is employed to provide further security. The experimental results show that the file size of an encrypted JPEG image is almost the same as that of the corresponding plaintext image except for slight variations because of byte alignment. In addition, the quantized DC coefficients decoded from an encrypted JPEG image will not fall outside the valid range. Improved format compatibility is provided compared with other related methods. Moreover, it is unnecessary to perform entropy encoding again because all of the encryption operations are performed directly on the JPEG bitstream. The proposed method proves to be secure against brute-force attacks, differential cryptanalysis, known plaintext attacks, and outline attacks. Our proposed method can also be applied to color JPEG images.
C1 [He, Junhui; Huang, Shuhao; Tang, Shaohua] South China Univ Technol, Guangzhou Higher Educ Ctr, Sch Comp Sci & Engn, Guangzhou 510006, Guangdong, Peoples R China.
   [Huang, Jiwu] Shenzhen Univ, Coll Informat Engn, Guangdong Key Lab Intelligent Informat Proc, Shenzhen 518060, Peoples R China.
   [Huang, Jiwu] Shenzhen Univ, Coll Informat Engn, Shenzhen Key Lab Media Secur, Shenzhen 518060, Peoples R China.
C3 South China University of Technology; Shenzhen University; Shenzhen
   University
RP He, JH (corresponding author), South China Univ Technol, Guangzhou Higher Educ Ctr, Sch Comp Sci & Engn, Guangzhou 510006, Guangdong, Peoples R China.
EM hejh@scut.edu.cn; cshuangsh@mail.scut.edu.cn; shtang@ieee.org;
   jwhuang@szu.edu.cn
RI huang, jw/KVY-9917-2024
OI He, Junhui/0000-0002-1689-0509
FU National Natural Science Foundation of China [61632013, U1636202,
   61332012]; Shenzhen RD Program [JCYJ20160328144421330]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61632013, U1636202, and 61332012, and
   in part by Shenzhen R&D Program (JCYJ20160328144421330). The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Abdulmotaleb El Saddik.
CR [Anonymous], 2012, P MM SEC COV UK SEP
   Auer S, 2013, INT J DIGIT CRIME FO, V5, P1, DOI 10.4018/jdcf.2013070101
   Bhargava B, 2004, MULTIMED TOOLS APPL, V24, P57, DOI 10.1023/B:MTAP.0000033983.62130.00
   Chen GR, 2004, CHAOS SOLITON FRACT, V21, P749, DOI 10.1016/j.chaos.2003.12.022
   Cheng H, 2016, J VIS COMMUN IMAGE R, V40, P111, DOI 10.1016/j.jvcir.2016.06.016
   Cheng H, 2015, PROCEEDINGS 10TH INTERNATIONAL CONFERENCE ON AVAILABILITY, RELIABILITY AND SECURITY ARES 2015, P417, DOI 10.1109/ARES.2015.18
   Çiftçi S, 2018, IEEE T MULTIMEDIA, V20, P68, DOI 10.1109/TMM.2017.2728479
   Digital Compression and Coding of Continuous-tone Still Images Requirements and Guidelines, 1992, T81 CCITT I TEL UN S
   Dufaux F, 2006, PROC SPIE, V6312, DOI 10.1117/12.686963
   Ghadi Musab, 2015, Journal of Innovation in Digital Ecosystems, V2, P20, DOI 10.1016/j.jides.2015.10.003
   Granlund T., 2016, GNU MP GNU MULTIPLE
   Kishore B, 2015, J REAL-TIME IMAGE PR, V10, P551, DOI 10.1007/s11554-012-0282-5
   Lei Tang, 1996, Proceedings ACM Multimedia 96, P219, DOI 10.1145/244130.244209
   Li SS, 2016, KSII T INTERNET INF, V10, P1790, DOI 10.3837/tiis.2016.04.018
   Li WH, 2007, INT J COMPUT MATH, V84, P1367, DOI 10.1080/00207160701294376
   Lian SG, 2004, IEEE INFOR VIS, P217, DOI 10.1109/IV.2004.1320147
   Minemura K, 2012, IEEE IMAGE PROC, P261, DOI 10.1109/ICIP.2012.6466845
   Niu XA, 2008, 2008 FOURTH INTERNATIONAL CONFERENCE ON INTELLIGENT INFORMATION HIDING AND MULTIMEDIA SIGNAL PROCESSING, PROCEEDINGS, P308, DOI 10.1109/IIH-MSP.2008.207
   Ong S, 2013, IEEE IMAGE PROC, P4574, DOI 10.1109/ICIP.2013.6738942
   Ong SY, 2015, SIGNAL PROCESS-IMAGE, V31, P47, DOI 10.1016/j.image.2014.11.008
   Qian ZX, 2014, IEEE T MULTIMEDIA, V16, P1486, DOI 10.1109/TMM.2014.2316154
   Rodrigues JM, 2006, IEEE IMAGE PROC, P1981, DOI 10.1109/ICIP.2006.312886
   Schaefer G, 2004, PROC SPIE, V5307, P472, DOI 10.1117/12.525375
   Shreyamsha Kumar BK, 2010, SIGNAL IMAGE VIDEO P, V4, P419, DOI 10.1007/s11760-009-0131-6
   Yuan L, 2015, IEEE CONF COMPUT, P185, DOI 10.1109/INFCOMW.2015.7179382
   Zhang DH, 2014, OPTIK, V125, P717, DOI 10.1016/j.ijleo.2013.07.069
   Zhang WM, 2016, IEEE T MULTIMEDIA, V18, P1469, DOI 10.1109/TMM.2016.2569497
   Zhou YC, 2014, SIGNAL PROCESS, V97, P172, DOI 10.1016/j.sigpro.2013.10.034
NR 28
TC 38
Z9 40
U1 1
U2 34
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2018
VL 20
IS 10
SI SI
BP 2645
EP 2658
DI 10.1109/TMM.2018.2817065
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GT9YH
UT WOS:000444903000009
DA 2024-07-18
ER

PT J
AU Trinh, H
   Calyam, P
   Chemodanov, D
   Yao, SZ
   Lei, Q
   Gao, F
   Palaniappan, K
AF Huy Trinh
   Calyam, Prasad
   Chemodanov, Dmitrii
   Yao, Shizeng
   Lei, Qing
   Gao, Fan
   Palaniappan, Kannappan
TI Energy-Aware Mobile Edge Computing and Routing for Low-Latency Visual
   Data Processing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Mobile Edge Computing; Policy-based Cloud Management; Energy-aware Edge
   Routing; Low-latency Visual Data Processing
ID PROTOCOL
AB New paradigms such as Mobile Edge Computing (MEC) are becoming feasible for use in, e.g., real-time decision-making during disaster incident response to handle the data deluge occurring in the network edge. However, MEC deployments today lack flexible IoT device data handling such as handling user preferences for real-time versus energy-efficient processing. Moreover, MEC can also benefit from a policy-based edge routing to handle sustained performance levels with efficient energy consumption. In this paper, we study the potential of MEC to address application issues related to energy management on constrained IoT devices with limited power sources, while also providing low-latency processing of visual data being generated at high resolutions. Using a facial recognition application that is important in disaster incident response scenarios, we propose a novel "offload decision-making" algorithm that analyzes the tradeoffs in computing policies to offload visual data processing (i.e., to an edge cloud or a core cloud) at low-to-high workloads. This algorithm also analyzes the impact on energy consumption in the decision-making under different visual data consumption requirements (i.e., users with thick clients or thin clients). To address the processing-throughput versus energy-efficiency tradeoffs, we propose a "Sustainable Policy-based Intelligence-Driven Edge Routing" algorithm that uses machine learning within Mobile Ad hoc Networks. This algorithm is energy aware and improves the geographic routing baseline performance (i.e., minimizes impact of local minima) for throughput performance sustainability, while also enabling flexible policy specification. We evaluate our proposed algorithms by conducting experiments on a realistic edge and core cloud test-bed in the GENT Cloud infrastructure, and recreate disaster scenes of tornado damages within simulations. Our empirical results show how MEC can provide flexibility to users who desire energy conservation over low latency or vice versa in the visual data processing with a facial recognition application. In addition, our simulation results show that our routing approach outperforms existing solutions under diverse user preferences, node mobility, and severe node failure conditions.
C1 [Huy Trinh; Calyam, Prasad; Chemodanov, Dmitrii; Yao, Shizeng; Palaniappan, Kannappan] Univ Missouri, Dept Elect & Comp Sci, Columbia, MO 65211 USA.
   [Lei, Qing] Univ Missouri, Comp Graph & Image Understanding Lab, Columbia, MO USA.
   [Gao, Fan] Univ Missouri, Elect Engn & Comp Sci, Columbia, MO USA.
C3 University of Missouri System; University of Missouri Columbia;
   University of Missouri System; University of Missouri Columbia;
   University of Missouri System; University of Missouri Columbia
RP Calyam, P (corresponding author), Univ Missouri, Dept Elect & Comp Sci, Columbia, MO 65211 USA.
EM hntzq4@mail.missouri.edu; calyamp@missouri.edu;
   dycbt4@mail.missouri.edu; syyh4@mail.missouri.edu;
   qlzm3@mail.missouri.edu; fgyf8@mail.missouri.edu; pal@missouri.edu
OI Gao, Fan/0000-0003-4618-0855
FU National Science Foundation [CNS-1647182]
FX This work was supported by the National Science Foundation under Award
   No. CNS-1647182. Any opinions, findings, and conclusions or
   recommendations expressed in this publication are those of the author(s)
   and do not necessarily reflect the views of the NSF. The guest editor
   coordinating the review of this manuscript and approving it for
   publication was Xuan Song.
CR Ahmed ANR, 2016, IEEE IMTC P, P1327
   Alesheikh AAli., 2002, Symposium on Geospatial Theory, Processing and Applications, Symposium sur la theorie, les traitements et les applications des donnees Geospatiales, P1
   Andersson D, 2004, PROC IEEE-PES, P570, DOI 10.1109/TDC.2004.1432442
   [Anonymous], 2008, "SIGCOMM Demon-stration
   [Anonymous], 2003, ACM SIGMOBILE mobile computing and communications review, DOI DOI 10.1145/961268.961272
   Berman M, 2014, COMPUT NETW, V61, P5, DOI 10.1016/j.bjp.2013.12.037
   Bourdena A, 2014, FUTURE GENER COMP SY, V39, P16, DOI 10.1016/j.future.2014.02.013
   Bradski G., 2008, LEARNING OPENCV
   Chemodanov D., 2017, FUTURE GENER COMPUT
   Cvetkovski A, 2009, IEEE INFOCOM SER, P1647, DOI 10.1109/INFCOM.2009.5062083
   Dhumane A., 2016, Proceedings of the International Multiconference of Engineers and Computer Scientists, V1, P16
   Frey M, 2014, IEEE ICC, P190, DOI 10.1109/ICC.2014.6883317
   Gidaris S, 2015, IEEE I CONF COMP VIS, P1134, DOI 10.1109/ICCV.2015.135
   Gillis John, 2016, 2016 13th IEEE Annual Consumer Communications & Networking Conference (CCNC), P264, DOI 10.1109/CCNC.2016.7444772
   Gupta R., 2016, INT J SCI RES COMPUT, V1, P24
   Ha K, 2014, MOBISYS'14: PROCEEDINGS OF THE 12TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE SYSTEMS, APPLICATIONS, AND SERVICES, P68, DOI 10.1145/2594368.2594383
   Haoru Su, 2013, MAEB ROUTING PROTOCO
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hiertz GR, 2010, IEEE WIREL COMMUN, V17, P104, DOI 10.1109/MWC.2010.5416357
   Huang GQ, 2009, INT J COMPUT INTEG M, V22, P579, DOI 10.1080/09511920701724934
   Jan MA, 2018, FUTURE GENER COMP SY, V80, P613, DOI 10.1016/j.future.2016.05.034
   Jararweh Y, 2016, 2016 23RD INTERNATIONAL CONFERENCE ON TELECOMMUNICATIONS (ICT), DOI 10.1109/ICT.2016.7500486
   Kim H, 2014, 2014 2ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE, MODELLING AND SIMULATION, P166, DOI 10.1109/AIMS.2014.48
   King DE, 2009, J MACH LEARN RES, V10, P1755
   Klontz J C., 2013, Michigan State University, V119, P1
   Krol Michal., 2016, EWSN, P89
   Kuiper E, 2008, 2008 22ND INTERNATIONAL WORKSHOPS ON ADVANCED INFORMATION NETWORKING AND APPLICATIONS, VOLS 1-3, P1690, DOI 10.1109/WAINA.2008.132
   Lam S.S., 2011, PROCEEDING ACM SIGME, P257
   Li C, 2014, INT J COMMUN SYST, V27, P4126, DOI 10.1002/dac.2602
   Menon VG., 2013, Life Sci J, V10, P1633
   Openly accessible Source Code Repository, 2017, SPID SUST POL BAS IN
   Orsini G, 2015, 2015 8TH IFIP WIRELESS AND MOBILE NETWORKING CONFERENCE (WMNC), P112, DOI 10.1109/WMNC.2015.10
   Parkhi OM, 2015, DEEP FACE RECOGNITIO
   Perkins CE, 1999, WMCSA '99, SECOND IEEE WORKSHOP ON MOBILE COMPUTING SYSTEMS AND APPLICATIONS, PROCEEDINGS, P90, DOI 10.1109/MCSA.1999.749281
   Perumal N, 2003, NATIONAL POWER ENGINEERING CONFERENCE PECON 2003, PROCEEDINGS, P211
   Ragona C, 2015, IEEE GLOB COMM CONF, DOI 10.1109/GLOCOM.2015.7417039
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Rohal P., 2013, INT J ADV RES ENG TE, V1, P54
   Sahhaf S, 2015, COMPUT NETW, V82, P156, DOI 10.1016/j.comnet.2015.02.022
   Satyanarayanan M, 2009, IEEE PERVAS COMPUT, V8, P14, DOI 10.1109/MPRV.2009.82
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shi WS, 2016, IEEE INTERNET THINGS, V3, P637, DOI 10.1109/JIOT.2016.2579198
   Singh GT, 2014, INT WIREL COMMUN, P1124, DOI 10.1109/IWCMC.2014.6906512
   Sun X, 2016, IEEE COMMUN MAG, V54, P22, DOI 10.1109/MCOM.2016.1600492CM
   Sun XY, 2016, AER ADV ENG RES, V98, P1
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Tian YA, 2010, PROCEEDINGS OF THE 7TH CONFERENCE ON BIOLOGICAL DYNAMIC SYSTEM AND STABILITY OF DIFFERENTIAL EQUATION, VOLS I AND II, P1
   Tolia N, 2006, COMPUTER, V39, P46, DOI 10.1109/MC.2006.101
   Wang SR, 2015, ADV INTEL SYS RES, V119, P1
   Weston Jason, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P639, DOI 10.1007/978-3-642-35289-8_34
   Yang L, 2015, IEEE T COMPUT, V64, P2253, DOI 10.1109/TC.2014.2366735
   Yu Y., 2001, GEOGRAPHICAL ENERGY
   Zhang K, 2016, IEEE ACCESS, V4, P5896, DOI 10.1109/ACCESS.2016.2597169
   Zhang L, 2010, PROCEEDINGS OF 2010 INTERNATIONAL CONFERENCE ON PUBLIC ADMINISTRATION (6TH), VOL II, P105, DOI 10.1145/1878961.1878982
NR 54
TC 50
Z9 52
U1 1
U2 31
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2018
VL 20
IS 10
SI SI
BP 2562
EP 2577
DI 10.1109/TMM.2018.2865661
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GT9YH
UT WOS:000444903000003
OA hybrid
DA 2024-07-18
ER

PT J
AU Li, XY
   Jiang, SQ
AF Li, Xiangyang
   Jiang, Shuqiang
TI Bundled Object Context for Referring Expressions
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bundled object context; referring expression; LSTM; vision-language
AB Referring expressions are natural language descriptions of objects within a given scene. Context is of crucial importance for a referring expression, as the description not only depicts the properties of the object but also involves the relationships of the referred object with other ones. Most of previous work uses either the whole image or one particular contextual object as the context. However, the context of these approaches is holistic and insufficient, as a referring expression often describes relationships of multiple objects in an image. To leverage rich context information from all objects in an image, in this paper, we propose a novel scheme that is composed of a visual context long short-term memory (LSTM) module and a sentence LSTM module to model bundled object context for referring expressions. All contextual objects are arranged with their spatial locations and progressively fed into the visual context LSTM module to acquire and aggregate the context features. Then the concatenation of the learned context features and the features of the referred object are put into the sentence LSTM module to learn the probability of a referring expression. The feedback connections and internal gating mechanism of the LSTM cells enable our model to selectively propagate relevant contextual information through the whole network. Experiments on three benchmark datasets show that our methods can achieve promising results compared to state-of-the-art methods. Moreover, visualization of the internal states of the visual context LSTM cells also shows that our method can automatically select the pertinent context objects.
C1 [Li, Xiangyang; Jiang, Shuqiang] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
   [Li, Xiangyang; Jiang, Shuqiang] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS
RP Jiang, SQ (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.; Jiang, SQ (corresponding author), Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
EM xiangyang.li@vipl.ict.ac.cn; sqjiang@ict.ac.cn
RI Li, Xiangyang/AAF-1260-2019
OI Li, Xiangyang/0000-0002-3944-4704
FU National Natural Science Foundation of China [61532018]; Beijing
   Municipal Commission of Science and Technology [D161100001816001];
   Lenovo Outstanding Young Scientists Program; National Program for
   Special Support of Eminent Professionals; National Program for Support
   of Top-notch Young Professionals
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61532018, in part by the Beijing
   Municipal Commission of Science and Technology under Grant
   D161100001816001, in part by the Lenovo Outstanding Young Scientists
   Program, and in part by National Program for Special Support of Eminent
   Professionals and National Program for Support of Top-notch Young
   Professionals. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Marco Bertini.
CR [Anonymous], 2012, Proceedings of the Twenty Ninth International Conference on Machine Learning
   Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   BYEON W, 2015, PROC CVPR IEEE, P3547, DOI DOI 10.1109/CVPR.2015.7298977
   Chen XL, 2017, IEEE I CONF COMP VIS, P4106, DOI 10.1109/ICCV.2017.440
   Cho K, 2015, IEEE T MULTIMEDIA, V17, P1875, DOI 10.1109/TMM.2015.2477044
   Farhadi A, 2010, LECT NOTES COMPUT SC, V6314, P15, DOI 10.1007/978-3-642-15561-1_2
   Fernández S, 2007, LECT NOTES COMPUT SC, V4669, P220
   FitzGerald Nicholas., 2013, EMNLP, P1914
   Fu K, 2017, IEEE T PATTERN ANAL, V39, P2321, DOI 10.1109/TPAMI.2016.2642953
   Gkioxari G, 2016, LECT NOTES COMPUT SC, V9908, P728, DOI 10.1007/978-3-319-46493-0_44
   He YH, 2016, IEEE T MULTIMEDIA, V18, P1363, DOI 10.1109/TMM.2016.2558463
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hu RH, 2017, PROC CVPR IEEE, P4418, DOI 10.1109/CVPR.2017.470
   Hu RH, 2016, PROC CVPR IEEE, P4555, DOI 10.1109/CVPR.2016.493
   Hu RH, 2016, LECT NOTES COMPUT SC, V9905, P108, DOI 10.1007/978-3-319-46448-0_7
   Hua Y, 2016, IEEE T MULTIMEDIA, V18, P1201, DOI 10.1109/TMM.2016.2535864
   Jia X, 2015, IEEE I CONF COMP VIS, P2407, DOI 10.1109/ICCV.2015.277
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jiang XY, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P69, DOI 10.1145/2733373.2806240
   Johnson J, 2016, PROC CVPR IEEE, P4565, DOI 10.1109/CVPR.2016.494
   Kang CC, 2015, IEEE T MULTIMEDIA, V17, P370, DOI 10.1109/TMM.2015.2390499
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kazemzadeh S., 2014, EMNLP, P787, DOI DOI 10.3115/V1/D14-1086
   Krahmer E, 2012, COMPUT LINGUIST, V38, P173, DOI 10.1162/COLI_a_00088
   Krishnamurthy Jayant., 2013, Transactions of the Association for Computational Linguistics, V1, P193, DOI DOI 10.1162/TACLA00220
   Kulkarni G, 2013, IEEE T PATTERN ANAL, V35, P2891, DOI 10.1109/TPAMI.2012.162
   Lavie A., 2007, P 2 WORKSH STAT MACH, P228
   Lebret R, 2015, PR MACH LEARN RES, V37, P2085
   Li JN, 2017, IEEE T MULTIMEDIA, V19, P944, DOI 10.1109/TMM.2016.2642789
   Li X., 2016, Proceedings of ACM international conference on Multimedia, P1107
   Liang XD, 2016, LECT NOTES COMPUT SC, V9905, P125, DOI 10.1007/978-3-319-46448-0_8
   Lin Chin-Yew, 2004, Text summarization branches out, P74, DOI DOI 10.2307/3105454
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu C, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4033
   Liu CX, 2017, AAAI CONF ARTIF INTE, P4176
   Luo RT, 2017, PROC CVPR IEEE, P3125, DOI 10.1109/CVPR.2017.333
   Mao J, 2014, CELL DEATH DIS, V5, DOI 10.1038/cddis.2013.515
   Mao JH, 2016, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2016.9
   Mitchell Margaret, 2013, P 2013 C N AM CHAPT, P1174
   Nagaraja VK, 2016, LECT NOTES COMPUT SC, V9908, P792, DOI 10.1007/978-3-319-46493-0_48
   Palangi H, 2016, IEEE-ACM T AUDIO SPE, V24, P694, DOI 10.1109/TASLP.2016.2520371
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Plummer BA, 2017, IEEE I CONF COMP VIS, P1946, DOI 10.1109/ICCV.2017.213
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rohrbach A, 2016, LECT NOTES COMPUT SC, V9905, P817, DOI 10.1007/978-3-319-46448-0_49
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shuai B, 2016, PROC CVPR IEEE, P3620, DOI 10.1109/CVPR.2016.394
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Varior RR, 2016, LECT NOTES COMPUT SC, V9911, P135, DOI 10.1007/978-3-319-46478-7_9
   Vinyals Oriol, 2016, P 4 INT C LEARN REPR
   Wang C., 2016, P 2016 ACM MULT C, P988, DOI 10.1145/2964284.2964299
   Wang J, 2016, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2016.251
   Wang L, 2016, PROC CVPR IEEE, P5005, DOI 10.1109/CVPR.2016.541
   WINOGRAD T, 1972, COGNITIVE PSYCHOL, V3, P1, DOI 10.1016/0010-0285(72)90002-3
   Wu Q, 2016, PROC CVPR IEEE, P203, DOI 10.1109/CVPR.2016.29
   Xu HJ, 2016, LECT NOTES COMPUT SC, V9911, P451, DOI 10.1007/978-3-319-46478-7_28
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Yu LC, 2017, PROC CVPR IEEE, P3521, DOI 10.1109/CVPR.2017.375
   Yu LC, 2016, LECT NOTES COMPUT SC, V9906, P69, DOI 10.1007/978-3-319-46475-6_5
   Zuo Z, 2016, IEEE T IMAGE PROCESS, V25, P2983, DOI 10.1109/TIP.2016.2548241
NR 62
TC 9
Z9 10
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2018
VL 20
IS 10
SI SI
BP 2749
EP 2760
DI 10.1109/TMM.2018.2811621
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GT9YH
UT WOS:000444903000017
DA 2024-07-18
ER

PT J
AU Yang, XS
   Zhang, TZ
   Xu, CS
AF Yang, Xiaoshan
   Zhang, Tianzhu
   Xu, Changsheng
TI Text2Video: An End-to-end Learning Framework for Expressing Text With
   Videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multimedia storytelling; video analysis; deep learning
ID REPRESENTATION; ANNOTATION; MOVIE
AB Video creation is a challenging and highly professional task that generally involves substantial manual efforts. To ease this burden, a better approach is to automatically produce new videos based on clips from the massive amount of existing videos according to arbitrary text. In this paper, we formulate video creation as a problem of retrieving a sequence of videos for a sentence stream. To achieve this goal, we propose a novel multimodal recurrent architecture for automatic video production. Compared with existing methods, the proposed model has three major advantages. First, it is the first completely integrated end-to-end deep learning system for real-world production to the best of our knowledge. We are among the first to address the problem of retrieving a sequence of videos for a sentence stream. Second, it can effectively exploit the correspondence between sentences and video clips through semantic consistency modeling. Third, it can model the visual coherence well by requiring that the produced videos should be organized coherently in terms of visual appearance. We have conducted extensive experiments on two applications, including video retrieval and video composition. The qualitative and quantitative results obtained on two public datasets used in the Large Scale Movie Description Challenge 2016 both demonstrate the effectiveness of the proposed model compared with other state-of-the-art algorithms.
C1 [Yang, Xiaoshan; Zhang, Tianzhu; Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Yang, Xiaoshan; Zhang, Tianzhu; Xu, Changsheng] Univ Chinese Acad Sci, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS
RP Xu, CS (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM xiaoshan.yang@nlpr.ia.ac.cn; tzzhang@nlpr.ia.ac.cn; csxu@nlpr.ia.ac.cn
RI xu, cj/HJZ-3488-2023; ARSLAN, Okan/AAA-3232-2020; Zhang,
   Tianzhu/AGY-9389-2022
OI Zhang, Tianzhu/0000-0003-0764-6106; xu, chang sheng/0000-0001-8343-9665;
   zhang, tian zhu/0000-0003-1856-9564
FU National Natural Science Foundation of China [61432019, 61572498,
   61532009, 61702511, 61720106006, 61711530243]; Beijing Natural Science
   Foundation [4172062]; Key Research Program of Frontier Sciences, CAS
   [QYZDJ-SSW-JSC039]
FX This work was supported in part by National Natural Science Foundation
   of China under Grants 61432019, 61572498, 61532009, 61702511,
   61720106006, 61711530243, in part by Beijing Natural Science Foundation
   under Grant 4172062, and in part by Key Research Program of Frontier
   Sciences, CAS, under Grant QYZDJ-SSW-JSC039.
CR [Anonymous], 2016, P IEEE C COMPUTER VI
   [Anonymous], COMPUT RES REPOSITOR
   [Anonymous], P SPEC INT GROUP COM
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2006, P COMP VIS PATT REC
   [Anonymous], N AM CHAPTER ASS COM
   [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.348
   [Anonymous], P BRIT MACH VIS C
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], ACM T MULTIMEDIA COM
   [Anonymous], 2016, ARXIV160908124
   [Anonymous], 2010, P 18 ACM INT C MULTI
   [Anonymous], 2016, P ADV NEUR INF PROC
   [Anonymous], 2001, PROC 18 INT C MACH L
   Bao BK, 2013, IEEE T IMAGE PROCESS, V22, P860, DOI 10.1109/TIP.2012.2219543
   Bao BK, 2012, IEEE T IMAGE PROCESS, V21, P3794, DOI 10.1109/TIP.2012.2192742
   Barzilay Regina, 2005, P 43 ANN M ASS COMPU, P141, DOI 10.3115/1219840.1219858
   Bergstra J., 2010, Proc. Python Sci. Comput. Conf., V1, P3
   Bojanowski P, 2015, IEEE I CONF COMP VIS, P4462, DOI 10.1109/ICCV.2015.507
   Chasanis V., 2009, Proceedings of the ACM International Conference on Image and Video Retrieval, p35:1, DOI DOI 10.1145/1646396.1646439
   Chen LC, 2015, PR MACH LEARN RES, V37, P1785
   Cour T, 2009, PROC CVPR IEEE, P919, DOI 10.1109/CVPRW.2009.5206667
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Fitzgibbon A, 2002, LECT NOTES COMPUT SC, V2352, P304
   Fukui Akira, 2016, P C EMP METH NAT LAN
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guadarrama S, 2013, IEEE I CONF COMP VIS, P2712, DOI 10.1109/ICCV.2013.337
   Habibian A, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P17, DOI 10.1145/2647868.2654913
   Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Kaisheng Yao, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P4077, DOI 10.1109/ICASSP.2014.6854368
   Kiros R, 2014, Arxiv, DOI arXiv:1411.2539
   Liang C, 2013, IEEE T MULTIMEDIA, V15, P401, DOI 10.1109/TMM.2012.2229972
   Liao HS, 2016, IEEE T MULTIMEDIA, V18, P2196, DOI 10.1109/TMM.2016.2614227
   Liu FY, 2015, PROC CVPR IEEE, P5162, DOI 10.1109/CVPR.2015.7299152
   Lu WT, 2013, IEEE T MULTIMEDIA, V15, P1920, DOI 10.1109/TMM.2013.2280895
   Ngo CW, 2005, IEEE T CIRC SYST VID, V15, P296, DOI 10.1109/TCSVT.2004.841694
   Nitta N, 2011, MULTIMED TOOLS APPL, V51, P649, DOI 10.1007/s11042-010-0633-9
   Park C. C., 2015, Advances in neural information processing systems, P73
   Rasheed Z, 2005, IEEE T MULTIMEDIA, V7, P1097, DOI 10.1109/TMM.2005.858392
   Revaud J, 2013, PROC CVPR IEEE, P2459, DOI 10.1109/CVPR.2013.318
   Rohrbach A, 2017, INT J COMPUT VISION, V123, P94, DOI 10.1007/s11263-016-0987-1
   Rohrbach A, 2015, PROC CVPR IEEE, P3202, DOI 10.1109/CVPR.2015.7298940
   Shen EYT, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P809
   Shrestha P., 2010, Proceedings of the international conference on Multimedia (MM '10), P541, DOI DOI 10.1145/1873951.1874023
   Simonyan K., 2014, 14091556 ARXIV
   Sivic J, 2005, LECT NOTES COMPUT SC, V3568, P226
   Sutskever I, 2014, ADV NEUR IN, V27
   Tapaswi M, 2015, PROC CVPR IEEE, P1827, DOI 10.1109/CVPR.2015.7298792
   Torabi Atousa., 2015, ARXIV150301070
   Ursu MF, 2008, MULTIMEDIA SYST, V14, P115, DOI 10.1007/s00530-008-0119-z
   Ursu MF, 2008, ACM T MULTIM COMPUT, V4, DOI 10.1145/1412196.1412198
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Vondrick C., 2016, ADV NEURAL INFORM PR, P613, DOI DOI 10.13016/M26GIH-TNYZ
   Wang H, 2014, IEEE T MULTIMEDIA, V16, P1282, DOI 10.1109/TMM.2014.2312251
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Yeung M, 1998, COMPUT VIS IMAGE UND, V71, P94, DOI 10.1006/cviu.1997.0628
   Yi J, 2013, IEEE T MULTIMEDIA, V15, P1400, DOI 10.1109/TMM.2013.2250266
   Yu Y, 2017, PROC CVPR IEEE, P3261, DOI 10.1109/CVPR.2017.347
   Zhang TZ, 2012, IEEE T MULTIMEDIA, V14, P1206, DOI 10.1109/TMM.2012.2191944
   Zhang YF, 2009, IEEE T MULTIMEDIA, V11, P1276, DOI 10.1109/TMM.2009.2030629
   Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179
   Zhou BL, 2014, ADV NEUR IN, V27
   Zhou J, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P1127
NR 65
TC 15
Z9 17
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2018
VL 20
IS 9
BP 2360
EP 2370
DI 10.1109/TMM.2018.2807588
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GR1ZX
UT WOS:000442358200010
DA 2024-07-18
ER

PT J
AU Chen, SX
   Zhang, CJ
   Dong, M
AF Chen, Shixing
   Zhang, Caojin
   Dong, Ming
TI Deep Age Estimation: From Classification to Ranking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Age estimation; convolutional neural networks; ranking algorithms; error
   bound; convergence
ID DIMENSIONALITY REDUCTION; FACE; CONVERGENCE; NETWORKS; PATTERNS; IMAGE
AB Human age is considered an important biometric trait for human identification or search. Recent research shows that the aging features deeply learned from large-scale data lead to significant performance improvement on facial image-based age estimation. However, age-related ordinal information is totally ignored in these approaches. In this paper, we propose a novel convolutional neural network (CNN)-based framework, ranking-CNN, for age estimation. Ranking-CNN contains a set of basic CNNs, each of which is trained with ordinal age labels. Then, their binary outputs are aggregated for the final age prediction. From a theoretical perspective, we obtain an approximation for the final ranking error, show that it is controlled by the maximum error produced among subranking problems, and thus find a new error bound, which provides helpful guidance for the training and analysis of deep rankers. Based on the new error bound, we theoretically give an explicit formula for the learning of ranking-CNN and demonstrate its convergence using the stochastic approximation method. Moreover, we rigorously prove that ranking-CNN, by considering ordinal relation between ages, is more likely to get smaller estimation errors when compared with multiclass classification approaches. Through extensive experiments, we show that ranking-CNN outperforms other state-of-the-art feature extractors and age estimators on benchmark datasets.
C1 [Chen, Shixing; Dong, Ming] Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA.
   [Zhang, Caojin] Wayne State Univ, Dept Math, Detroit, MI 48202 USA.
C3 Wayne State University; Wayne State University
RP Dong, M (corresponding author), Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA.
EM schen@wayne.edu; czhang@wayne.edu; mdong@wayne.edu
OI Zhang, Caojin/0000-0003-4951-9172
FU U.S. National Science Foundation [CNS-1637312]; Ford Motor Company
   University Research Program [2015-9186R]
FX This work was supported in part by the U.S. National Science Foundation
   under Grant CNS-1637312 and in part by Ford Motor Company University
   Research Program under Grant 2015-9186R. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Winston Hsu.
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   [Anonymous], 2014, INT C LEARN REPR
   [Anonymous], 2014, 52 ANN M ASS COMP LI
   [Anonymous], 1984, Approximation and weak convergence methods for random processes with applications to stochastic systems theory
   [Anonymous], 2003, Journal of machine learning research
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2015, P IEEE INT C COMP VI
   [Anonymous], 1998, THEMNIST DATABASE HA
   [Anonymous], 2004, FG NET AGING DATABAS
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2015, PROC CVPR IEEE
   [Anonymous], IEEE TPAMI
   [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.532
   [Anonymous], 2006, P 14 ANN ACM INT C M
   [Anonymous], 2013, ICB, DOI DOI 10.1109/ICB.2013.6613022
   Bolz J, 2003, ACM T GRAPHIC, V22, P917, DOI 10.1145/882262.882364
   Burges C., 2005, ICML, P89
   Cai D, 2006, IEEE T IMAGE PROCESS, V15, P3608, DOI 10.1109/TIP.2006.881945
   Cam LL., 1960, Pacific Journal of Mathematics, V10, P1181, DOI [DOI 10.2140/PJM.1960.10.1181, 10.2140/pjm.1960.10.1181]
   Chang KY, 2015, IEEE T IMAGE PROCESS, V24, P785, DOI 10.1109/TIP.2014.2387379
   Chang KY, 2011, PROC CVPR IEEE, P585, DOI 10.1109/CVPR.2011.5995437
   Chen J, 2016, ADV MECH ENG, V8, DOI 10.1177/1687814016671445
   Chen K, 2013, PROC CVPR IEEE, P2467, DOI 10.1109/CVPR.2013.319
   Chen SX, 2017, PROC CVPR IEEE, P742, DOI 10.1109/CVPR.2017.86
   Collobert R, 2011, J MACH LEARN RES, V12, P2493
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding CX, 2015, IEEE T MULTIMEDIA, V17, P2049, DOI 10.1109/TMM.2015.2477042
   Dosovitskiy A, 2014, ADV NEUR IN, V27
   Duda R. O., 2012, PATTERN CLASSIFICATI, DOI DOI 10.1007/978-3-319-57027-3_4
   Eidinger E, 2014, IEEE T INF FOREN SEC, V9, P2170, DOI 10.1109/TIFS.2014.2359646
   Fu Y, 2008, IEEE T MULTIMEDIA, V10, P578, DOI 10.1109/TMM.2008.921847
   Geng X, 2007, IEEE T PATTERN ANAL, V29, P2234, DOI 10.1109/TPAMI.2007.70733
   Glorot X., 2010, P INT C ART INT STAT, P249
   Gunay A., 2008, PROC 23 INT S COMPUT, P1
   Guo G., 2010, IEEE COMP SOC C COMP, P71, DOI DOI 10.1109/CVPRW.2010.5543609
   Guo GD, 2008, IEEE T IMAGE PROCESS, V17, P1178, DOI 10.1109/TIP.2008.924280
   Guo GD, 2011, PROC CVPR IEEE, P657, DOI 10.1109/CVPR.2011.5995404
   Guo GD, 2009, PROC CVPR IEEE, P112, DOI 10.1109/CVPRW.2009.5206681
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Herbrich R, 2000, ADV NEUR IN, P115
   Hu ZZ, 2017, IEEE T IMAGE PROCESS, V26, P3087, DOI 10.1109/TIP.2016.2633868
   Ioffe S., 2015, P INT C MACH LEARN, VVolume 1, P448, DOI DOI 10.48550/ARXIV.1502.03167
   Iqbal S, 2015, INT CONF ELECTRO INF, P25, DOI 10.1109/EIT.2015.7293419
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Kim Y., 2014, P 2014 C EMPIRICAL M
   KUAN CM, 1991, IEEE T NEURAL NETWOR, V2, P484, DOI 10.1109/72.134285
   Kuang-Yu Chang, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P3396, DOI 10.1109/ICPR.2010.829
   Kushner H., 2003, STOCHASTIC APPROXIMA, V35, DOI DOI 10.1007/978-1-4471-4285-0_3
   KWON YH, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P762, DOI 10.1109/CVPR.1994.323894
   Lanitis A, 2004, IEEE T SYST MAN CY B, V34, P621, DOI 10.1109/TSMCB.2003.817091
   Lanitis A, 2002, IEEE T PATTERN ANAL, V24, P442, DOI 10.1109/34.993553
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Levi Gil, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P34, DOI 10.1109/CVPRW.2015.7301352
   Li CS, 2012, PROC CVPR IEEE, P2570, DOI 10.1109/CVPR.2012.6247975
   Nair Vinod, 2010, INT C INT C MACHINE, P807
   Ni B., 2009, Proceedings of the 17th ACM international conference on Multimedia, P85
   Ni BB, 2011, IEEE T MULTIMEDIA, V13, P1217, DOI 10.1109/TMM.2011.2167317
   Peng Yang, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P3404, DOI 10.1109/ICPR.2010.831
   PHANSALKAR VV, 1994, IEEE T NEURAL NETWOR, V5, P505, DOI 10.1109/72.286925
   Ricanek K, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P341
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Rothe R, 2018, INT J COMPUT VISION, V126, P144, DOI 10.1007/s11263-016-0940-3
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Shu XB, 2015, IEEE I CONF COMP VIS, P3970, DOI 10.1109/ICCV.2015.452
   Simonyan K., 2014, 14091556 ARXIV
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sun Y, 2014, ADV NEUR IN, V27
   Suo JL, 2009, IEEE I CONF COMP VIS, P622, DOI 10.1109/ICCV.2009.5459181
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wang AR, 2015, IEEE T MULTIMEDIA, V17, P1887, DOI 10.1109/TMM.2015.2476655
   Wang XL, 2015, IEEE WINT CONF APPL, P534, DOI 10.1109/WACV.2015.77
   Wu W, 2008, J COMPUT MATH, V26, P613
   Yang ZG, 2007, LECT NOTES COMPUT SC, V4642, P464
   Yi D, 2015, LECT NOTES COMPUT SC, V9005, P144, DOI 10.1007/978-3-319-16811-1_10
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54
   Zhang Y, 2010, PROC CVPR IEEE, P2622, DOI 10.1109/CVPR.2010.5539975
NR 79
TC 29
Z9 31
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2018
VL 20
IS 8
BP 2209
EP 2222
DI 10.1109/TMM.2017.2786869
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GN8BR
UT WOS:000439378600023
DA 2024-07-18
ER

PT J
AU Li, J
   Wang, ZM
   Lai, SM
   Zhai, YP
   Zhang, MJ
AF Li, Jing
   Wang, Zhengming
   Lai, Shiming
   Zhai, Yongping
   Zhang, Maojun
TI Parallax-Tolerant Image Stitching Based on Robust Elastic Warping
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image stitching; image alignment; elastic warping; feature refinement;
   computational efficiency; computer vision
ID PANORAMIC VIDEO; ALIGNMENT
AB Image stitching aims at generating high-quality panoramas with the lowest computational cost. In this paper, we propose a parallax-tolerant image stitching method based on robust elastic warping, which could achieve accurate alignment and efficient processing simultaneously. Given a group of point matches between images, an analytical warping function is constructed to eliminate the parallax errors. Then, the input images are warped according to the computed deformations over the meshed image plane. The seamless panorama is composed by directly reprojecting the warped images. As an important complement to the proposed method, a Bayesian model of feature refinement is proposed to adaptively remove the incorrect local matches. This ensures a more robust alignment than existing approaches. Moreover, our warp is highly compatible with different transformation types. A flexible strategy of combining it with the global similarity transformation is provided as an example. The performance of the proposed approach is demonstrated using several challenging cases.
C1 [Li, Jing; Lai, Shiming; Zhai, Yongping; Zhang, Maojun] Natl Univ Def Technol, Coll Informat Syst & Management, Changsha 410073, Hunan, Peoples R China.
   [Wang, Zhengming] Natl Univ Def Technol, Coll Sci, Changsha 410073, Hunan, Peoples R China.
C3 National University of Defense Technology - China; National University
   of Defense Technology - China
RP Li, J (corresponding author), Natl Univ Def Technol, Coll Informat Syst & Management, Changsha 410073, Hunan, Peoples R China.
EM jingli@nudt.edu.cn; zmw@nudt.edu.cn; shiming413@nudt.edu.cn;
   zhaiyongping08@nudt.edu.cn; mjzhang@nudt.edu.cn
RI li, xiaomin/KCX-9845-2024
FU National Natural Science Foundation of China [61402491, 61703415]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61402491 and 61703415.
CR Anderson R, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980257
   ARAD N, 1995, COMPUT GRAPH FORUM, V14, P35, DOI 10.1111/1467-8659.1410035
   BAJCSY R, 1989, COMPUT VISION GRAPH, V46, P1, DOI 10.1016/S0734-189X(89)80014-3
   Baker S, 2004, INT J COMPUT VISION, V56, P221, DOI 10.1023/B:VISI.0000011205.11775.fd
   BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792
   Brown M, 2007, INT J COMPUT VISION, V74, P59, DOI 10.1007/s11263-006-0002-3
   BURT PJ, 1983, ACM T GRAPHIC, V2, P217, DOI 10.1145/245.247
   Chang CH, 2014, PROC CVPR IEEE, P3254, DOI 10.1109/CVPR.2014.422
   Chang CH, 2012, PROC CVPR IEEE, P1075, DOI 10.1109/CVPR.2012.6247786
   Chen YS, 2016, LECT NOTES COMPUT SC, V9909, P186, DOI 10.1007/978-3-319-46454-1_12
   Diego F, 2013, IEEE T MULTIMEDIA, V15, P1377, DOI 10.1109/TMM.2013.2247390
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Gaddam VR, 2016, IEEE T MULTIMEDIA, V18, P1819, DOI 10.1109/TMM.2016.2586304
   Gao J., 2013, Eurographics (Short Papers), P45, DOI DOI 10.2312/CONF/EG2013/SHORT/045-048
   Gao JH, 2011, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2011.5995433
   He KM, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462004
   Heckbert Paul S., 1989, THESIS
   Jia JY, 2008, IEEE T PATTERN ANAL, V30, P617, DOI 10.1109/TPAMI.2007.70729
   Joo K, 2015, IEEE IMAGE PROC, P1175, DOI 10.1109/ICIP.2015.7350985
   Kasahara S, 2017, IEEE T VIS COMPUT GR, V23, P1222, DOI 10.1109/TVCG.2016.2642947
   Kopf J, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276497, 10.1145/1239451.1239547]
   Li J, 2015, IEEE T CYBERNETICS, V45, P2707, DOI 10.1109/TCYB.2014.2381774
   Li SW, 2015, IEEE I CONF COMP VIS, P4283, DOI 10.1109/ICCV.2015.487
   Lin CC, 2015, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2015.7298719
   Lin K., 2017, IEEE C COMP VIS PATT, P2405
   Lin KM, 2016, LECT NOTES COMPUT SC, V9907, P370, DOI 10.1007/978-3-319-46487-9_23
   Lin WY, 2011, PROC CVPR IEEE, P345, DOI 10.1109/CVPR.2011.5995314
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147
   Lou ZY, 2014, IEEE T MULTIMEDIA, V16, P2052, DOI 10.1109/TMM.2014.2346476
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   OSearcoid M., 2006, METRIC SPACES
   Perazzi F, 2015, COMPUT GRAPH FORUM, V34, P57, DOI 10.1111/cgf.12541
   Rohr K, 1996, LECT NOTES COMPUT SC, V1131, P297, DOI 10.1007/BFb0046967
   Sprengel R, 1997, P IEEE EMBS, V18, P1190, DOI 10.1109/IEMBS.1996.652767
   Szeliski R, 2006, FOUND TRENDS COMPUT, V2, P1, DOI 10.1561/0600000009
   Triggs B., 2000, Vision Algorithms: Theory and Practice. International Workshop on Vision Algorithms. Proceedings (Lecture Notes in Computer Science Vol. 1883), P298
   von Gioi RG, 2010, IEEE T PATTERN ANAL, V32, P722, DOI 10.1109/TPAMI.2008.300
   Wang J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1330511.1330512
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Zaragoza J, 2014, IEEE T PATTERN ANAL, V36, P1285, DOI 10.1109/TPAMI.2013.247
   Zhang F, 2014, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2014.423
   Zhang GF, 2016, IEEE T IMAGE PROCESS, V25, P3099, DOI 10.1109/TIP.2016.2535225
   Zhang ZY, 1997, IMAGE VISION COMPUT, V15, P59, DOI 10.1016/S0262-8856(96)01112-2
   Zhu ZG, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P345, DOI 10.1109/ICCV.2001.937539
NR 44
TC 136
Z9 146
U1 10
U2 94
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2018
VL 20
IS 7
BP 1672
EP 1687
DI 10.1109/TMM.2017.2777461
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GJ7MC
UT WOS:000435570100007
DA 2024-07-18
ER

PT J
AU Nguyen, LS
   Ruiz-Correa, S
   Mast, MS
   Gatica-Perez, D
AF Nguyen, Laurent Son
   Ruiz-Correa, Salvador
   Mast, Marianne Schmid
   Gatica-Perez, Daniel
TI Check Out This Place: Inferring Ambiance From Airbnb Photos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Ambiance prediction; first impressions; home environments; Airbnb;
   social media; image processing
ID CUE
AB Airbnb is changing the landscape of the hospitality industry, and to this day, little is known about the inferences that guests make about Airbnb listings. Our work constitutes a first attempt at understanding how potential Airbnb guests form first impressions from images, one of the main modalities featured on the platform. We contribute to the multimedia community by proposing the novel task of automatically predicting human impressions of ambiance from pictures of listings on Airbnb. We collected Airbnb images, focusing on the countries Switzerland and Mexico as case studies, and used crowdsourcing mechanisms to gather annotations on physical and ambiance attributes, finding that agreement among raters was high for most of the attributes. Our cluster analysis showed that both physical and psychological attributes could be grouped into three clusters. We then extracted state-of-the-art features from the images to automatically infer the annotated variables in a regression task. Results show the feasibility of predicting ambiance impressions of homes on Airbnb, with up to 42% of the variance explained by our model, and best results were obtained using activation layers of deep convolutional neural networks trained on the Places dataset, a collection of scene-centric images.
C1 [Nguyen, Laurent Son] Idiap Res Inst, Social Comp Grp, CH-1920 Martigny, Switzerland.
   [Ruiz-Correa, Salvador] Inst Potosino Invest Cient & Tecnol, Ctr Nacl Supercomp, San Luis Potosi 78216, Mexico.
   [Mast, Marianne Schmid] Univ Lausanne, Fac Hautes Etud Commerciales HEC Lausanne, CH-1015 Lausanne, Switzerland.
   [Gatica-Perez, Daniel] Ecole Polytech Fed Lausanne, Idiap Res Inst, CH-1920 Martigny, Switzerland.
C3 Instituto Potosino Investigacion Cientifica y Tecnologica; University of
   Lausanne; Swiss Federal Institutes of Technology Domain; Ecole
   Polytechnique Federale de Lausanne
RP Nguyen, LS (corresponding author), Idiap Res Inst, Social Comp Grp, CH-1920 Martigny, Switzerland.
EM lnguyen@idiap.ch; src@cmls.pw; Marianne.SchmidMast@unil.ch;
   gat-ica@idiap.ch
OI Ruiz-Correa, Salvador/0000-0002-2918-6780
FU EPFL-UNIL CROSS program through the Mi Casa es su Casa project; Swiss
   National Science Foundation through the UBImpressed Sinergia project
FX This work was supported in part by the EPFL-UNIL CROSS program through
   the Mi Casa es su Casa project, and in part by the Swiss National
   Science Foundation through the UBImpressed Sinergia project.
CR [Anonymous], TECH REP
   [Anonymous], 2014, DIGITAL DISCRIMINATI
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], DEF AMB ENGL
   [Anonymous], TECH REP
   [Anonymous], 2011, ADV NEURAL INF PROCE, DOI DOI 10.21236/ADA554133
   Babenko A, 2014, LECT NOTES COMPUT SC, V8689, P584, DOI 10.1007/978-3-319-10590-1_38
   Bakhshi S, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P965, DOI 10.1145/2556288.2557403
   Bosch A., 2007, P 6 ACM INT C IMAGE, P401, DOI DOI 10.1145/1282280.1282340
   Breiman L., 2001, Mach. Learn., V45, P5
   Carney DR, 2008, POLIT PSYCHOL, V29, P807, DOI 10.1111/j.1467-9221.2008.00668.x
   Donahue J., 2013, Decaf: A deep convolutional activation feature for generic visual recognition
   Gosling SD, 2005, ENVIRON BEHAV, V37, P683, DOI 10.1177/0013916504274011
   Gosling SD, 2002, J PERS SOC PSYCHOL, V82, P379, DOI 10.1037//0022-3514.82.3.379
   Gosling SD, 2003, J RES PERS, V37, P504, DOI 10.1016/S0092-6566(03)00046-1
   Gosling SD., 2013, Handbook of interior architecture and design, P278
   Graham L., 2011, PROC 5 INT C WEBLOGS, P145
   Graham LT, 2015, PERSPECT PSYCHOL SCI, V10, P346, DOI 10.1177/1745691615576761
   Guttentag D, 2015, CURR ISSUES TOUR, V18, P1192, DOI 10.1080/13683500.2013.827159
   Hays J, 2008, PROC CVPR IEEE, P3436
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jin B, 2016, IEEE IMAGE PROC, P2291, DOI 10.1109/ICIP.2016.7532767
   Kittur A, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P453
   Lampinen A, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P1669, DOI 10.1145/2858036.2858092
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Lee Donghun., 2015, P 8 ACM C COMP COMP, P219, DOI [10.1145/2685553.2699011, DOI 10.1145/2685553.2699011]
   Lee YJ, 2013, IEEE I CONF COMP VIS, P1857, DOI 10.1109/ICCV.2013.233
   Lu X, 2015, IEEE T MULTIMEDIA, V17, P2021, DOI 10.1109/TMM.2015.2477040
   Marchesotti L, 2011, IEEE I CONF COMP VIS, P1784, DOI 10.1109/ICCV.2011.6126444
   Nguyen LS, 2016, IEEE T MULTIMEDIA, V18, P1422, DOI 10.1109/TMM.2016.2557058
   NISBETT RE, 1977, J PERS SOC PSYCHOL, V35, P250, DOI 10.1037//0022-3514.35.4.250
   Ordonez V, 2014, LECT NOTES COMPUT SC, V8694, P494, DOI 10.1007/978-3-319-10599-4_32
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Porzi L, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P139, DOI 10.1145/2733373.2806273
   Quattrone G., 2016, International World Wide Web Conference. WWW, P11
   Rahimi S, 2016, PROCEEDINGS OF THE 2ND ACM SIGSPATIAL WORKSHOP ON SMART CITIES AND URBAN ANALYTICS (URBANGIS'16, DOI 10.1145/3007540.3007547
   Redi M., 2015, 9 INT AAAI C WEB SOC
   RHEINGOLD HL, 1975, CHILD DEV, V46, P459, DOI 10.2307/1128142
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Salesses P, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0068400
   Santani D, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P211, DOI 10.1145/2733373.2806277
   Santani D, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P451, DOI 10.1145/2964284.2967261
   Siahaan E, 2016, IEEE T MULTIMEDIA, V18, P1338, DOI 10.1109/TMM.2016.2559942
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tian XM, 2015, IEEE T MULTIMEDIA, V17, P2035, DOI 10.1109/TMM.2015.2479916
   Zhou BL, 2014, ADV NEUR IN, V27
NR 47
TC 9
Z9 11
U1 0
U2 44
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2018
VL 20
IS 6
BP 1499
EP 1511
DI 10.1109/TMM.2017.2769444
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA GG0WG
UT WOS:000432400400017
DA 2024-07-18
ER

PT J
AU Wu, YP
   Ye, YD
   Zhao, CY
   Shi, ZL
AF Wu, Yunpeng
   Ye, Yangdong
   Zhao, Chenyang
   Shi, Zenglin
TI Collective Density Clustering for Coherent Motion Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Collective density clustering; coherent motion detection; affine motion
   segmentation
ID CROWD BEHAVIORS; SCENES; FLOW
AB Coherent motion detection remains a challenging problem due to the inherent complexity and vast diversity found in crowded scenes. Inspired by divide-and-conquer strategy, we desire to detect coherent motion from both local and global level. In this study, a novel collective density clustering (CDC) method is proposed to detect local and global coherent motion. We creatively define a collective density to discover underlying ordered density estimation, and subsequently a novel collective clustering algorithm is introduced, which is able to identify collective subgroups rapidly. Considering the complex interaction among subgroups, we present a hierarchical Union-Find-based collective merging algorithm to recognize coherent motion by merging collective subgroups. Our method is very efficient and effective. Experimental results on several challenging video datasets demonstrate that the proposed CDC achieves better results than state-of-the-art works, and multiple times or even tens of times faster. The proposed framework shows potential to be further applied to other problems (e.g., affine motion segmentation), related to local and global clustering.
C1 [Wu, Yunpeng; Ye, Yangdong; Zhao, Chenyang; Shi, Zenglin] Zhengzhou Univ, Sch Informat Engn, Zhengzhou 450052, Peoples R China.
C3 Zhengzhou University
RP Wu, YP (corresponding author), Zhengzhou Univ, Sch Informat Engn, Zhengzhou 450052, Peoples R China.
EM ieypwu@zzu.edu.cn; ieydye@zzu.edu.cn; iecyzhao@gs.zzu.edu.cn;
   iezlshi@zzu.edu.cn
OI Wu, Yunpeng/0000-0002-0648-868X
FU National Natural Science Foundation of China [61170223, 61502432,
   61502434, 61772475]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 61170223, 61502432, 61502434, and 61772475.
CR Ali S, 2007, PROC CVPR IEEE, P65
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2017, P IEEE C COMP VIS PA
   Bhattacharya K, 2010, NEW J PHYS, V12, DOI 10.1088/1367-2630/12/9/093019
   Brox T, 2010, LECT NOTES COMPUT SC, V6315, P282, DOI 10.1007/978-3-642-15555-0_21
   Bruhn A, 2005, INT J COMPUT VISION, V61, P211, DOI 10.1023/B:VISI.0000045324.43199.43
   Cheng MM, 2017, J COMPUT SCI TECH-CH, V32, P110, DOI 10.1007/s11390-017-1681-7
   CHENG YZ, 1995, IEEE T PATTERN ANAL, V17, P790, DOI 10.1109/34.400568
   Choi W, 2014, IEEE T PATTERN ANAL, V36, P1242, DOI 10.1109/TPAMI.2013.220
   Cormen Thomas H., 2001, INTRO ALGORITHMS
   Couzin I, 2007, NATURE, V445, P715, DOI 10.1038/445715a
   Couzin ID, 2009, TRENDS COGN SCI, V13, P36, DOI 10.1016/j.tics.2008.10.002
   Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57
   Ester M., 1996, KDD-96 Proceedings. Second International Conference on Knowledge Discovery and Data Mining, P226
   FUKUNAGA K, 1975, IEEE T INFORM THEORY, V21, P32, DOI 10.1109/TIT.1975.1055330
   Ge WN, 2012, IEEE T PATTERN ANAL, V34, P1003, DOI 10.1109/TPAMI.2011.176
   Hu M, 2008, INT C PATT RECOG, P9
   Jiang HQ, 2015, IEEE T MULTIMEDIA, V17, P3, DOI 10.1109/TMM.2014.2368273
   Kratz L, 2012, LECT NOTES COMPUT SC, V7575, P558, DOI 10.1007/978-3-642-33765-9_40
   Lin WY, 2016, IEEE T IMAGE PROCESS, V25, P1674, DOI 10.1109/TIP.2016.2531281
   Mahadevan V, 2010, PROC CVPR IEEE, P1975, DOI 10.1109/CVPR.2010.5539872
   Méhes E, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0031711
   Mehran R, 2010, LECT NOTES COMPUT SC, V6313, P439
   Mehran R, 2009, PROC CVPR IEEE, P935, DOI 10.1109/CVPRW.2009.5206641
   Rodriguez A, 2014, SCIENCE, V344, P1492, DOI 10.1126/science.1242072
   Szabó B, 2006, PHYS REV E, V74, DOI 10.1103/PhysRevE.74.061908
   Tomasi C, 1991, CMUCS91132 MELL U
   Tron R., 2007, PROC IEEE C COMPUT V, P1
   VICSEK T, 1995, PHYS REV LETT, V75, P1226, DOI 10.1103/PhysRevLett.75.1226
   Vicsek T, 2012, PHYS REP, V517, P71, DOI 10.1016/j.physrep.2012.03.004
   Wang WY, 2014, LECT NOTES COMPUT SC, V8689, P756, DOI 10.1007/978-3-319-10590-1_49
   Wang Z, 2018, IEEE T CYBERNETICS, V48, P3006, DOI 10.1109/TCYB.2017.2755044
   Wang Z, 2016, IEEE T MULTIMEDIA, V18, P260, DOI 10.1109/TMM.2015.2505083
   Wu S, 2012, IEEE T SYST MAN CY B, V42, P1443, DOI 10.1109/TSMCB.2012.2192267
   Wu YP, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P361, DOI 10.1145/2733373.2806227
   Yang Y, 2010, IEEE T IMAGE PROCESS, V19, P2761, DOI 10.1109/TIP.2010.2049235
   Zhang C, 2016, IEEE T MULTIMEDIA, V18, P1048, DOI 10.1109/TMM.2016.2542585
   Zhang HP, 2010, P NATL ACAD SCI USA, V107, P13626, DOI 10.1073/pnas.1001651107
   Zheng L, 2014, IEEE T IMAGE PROCESS, V23, P3368, DOI 10.1109/TIP.2014.2330763
   Zhou BL, 2015, INT J COMPUT VISION, V111, P50, DOI 10.1007/s11263-014-0735-3
   Zhou BL, 2012, LECT NOTES COMPUT SC, V7573, P857, DOI 10.1007/978-3-642-33709-3_61
   Zhou BL, 2014, IEEE T PATTERN ANAL, V36, P1586, DOI 10.1109/TPAMI.2014.2300484
   Zhou BL, 2013, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR.2013.392
   Zhou BL, 2012, PROC CVPR IEEE, P2871, DOI 10.1109/CVPR.2012.6248013
   Zhu LG, 2019, IRONMAK STEELMAK, V46, P499, DOI 10.1080/03019233.2017.1405153
NR 46
TC 9
Z9 9
U1 0
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2018
VL 20
IS 6
BP 1418
EP 1431
DI 10.1109/TMM.2017.2771477
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GG0WG
UT WOS:000432400400011
DA 2024-07-18
ER

PT J
AU Xu, XL
   Zeng, Y
   Guan, YL
   Yuan, L
AF Xu, Xiaoli
   Zeng, Yong
   Guan, Yong Liang
   Yuan, Lei
TI Expanding-Window BATS Code for Scalable Video Multicasting Over Erasure
   Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE And-Or tree analysis; erasure networks; network coding; unequal error
   protection
ID UNEQUAL ERROR PROTECTION; FOUNTAIN CODES
AB In this paper, we consider scalable video multicasting over erasure networks with heterogeneous video quality requirements. With random linear network coding (RLNC) applied at the intermediate nodes, the information received by the destinations is determined by the associated channel rank distributions, based on which we obtain the optimal achievable code rate at the source node. We show that although a concatenation of priority encoded transmission (PET) with RLNC achieves the optimal code rate, it incurs prohibitive high coding complexity. On the other hand, batched sparse (BATS) code has been recently proposed for unicast networks, which has low coding complexity with near-optimal overhead. However, the existing BATS code design cannot be applied for multicast networks with heterogeneous channel rank distributions at different destinations. To this end, we propose a novel expanding window BATS (EW-BATS) code, where the input symbols are grouped into overlapped windows according to their importance levels. The more important symbols are encoded with lower rate and hence they can be decoded by more destinations, while the less important symbols are encoded with higher rate and are only decoded by the destinations with high throughput for video quality enhancement. Based on asymptotical performance analysis, we formulate the linear optimization problems to jointly optimize the degree distributions for each window and the window selection probabilities. Simulation results show that the proposed EW-BATS code satisfies the decoding requirements with much lower transmission overhead compared with separate BATS code, where the degree distributions are separately optimized for each destination.
C1 [Xu, Xiaoli; Guan, Yong Liang] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639801, Singapore.
   [Zeng, Yong] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117583, Singapore.
   [Yuan, Lei] Lanzhou Univ, Signal & Informat Proc Inst, Lanzhou 730000, Gansu, Peoples R China.
C3 Nanyang Technological University; National University of Singapore;
   Lanzhou University
RP Zeng, Y (corresponding author), Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117583, Singapore.
EM xuxiaoli@ntu.edu.sg; elezeng@nus.edu.sg; eylguan@ntu.edu.sg;
   yuanl@lzu.edu.cn
RI Yuan, Lei/AAL-6265-2020; Guan, Yong Liang/A-5090-2011
OI Yuan, Lei/0000-0002-2293-0626; Guan, Yong Liang/0000-0002-9757-630X
FU Advanced Communications Research Program from DSO National Laboratories,
   Singapore [DSOCL14095]; Fundamental Research Funds for the Central
   Universities [lzujbky-2017-188]
FX This work was supported by the Advanced Communications Research Program
   under Contract DSOCL14095 from DSO National Laboratories, Singapore. The
   work of L. Yuan was supported by the Fundamental Research Funds for the
   Central Universities lzujbky-2017-188. The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Shiwen Mao. (Corresponding author: Yong Zeng.)
CR Ahmad S, 2011, IEEE T MULTIMEDIA, V13, P92, DOI 10.1109/TMM.2010.2093511
   Albanese A, 1996, IEEE T INFORM THEORY, V42, P1737, DOI 10.1109/18.556670
   [Anonymous], 2010, 2010 P IEEE INFOCOM, DOI DOI 10.1109/GLOCOM.2010.5684107
   [Anonymous], 2003, P 41 ANN ALL C COMM
   Bin Tang, 2012, Proceedings of the 2012 IEEE International Symposium on Information Theory - ISIT, P2451, DOI 10.1109/ISIT.2012.6283956
   Dana ATF, 2006, IEEE T INFORM THEORY, V52, P789, DOI 10.1109/TIT.2005.864424
   Ho T, 2006, IEEE T INFORM THEORY, V52, P4413, DOI 10.1109/TIT.2006.881746
   Li Y, 2011, IEEE T INFORM THEORY, V57, P1111, DOI 10.1109/TIT.2010.2095111
   Luby M. G., 1998, PROC 9 ANN ACM SIAM, P364
   Mahdaviani K., 2012, 2012 International Symposium on Network Coding (NetCod 2012), P125, DOI 10.1109/NETCOD.2012.6261896
   Mahdaviani K., 2013, P INT S NETW COD CAL, P1
   Ng TouHwee., 2013, P 17 C COMPUTATIONAL, P1
   Nguyen K, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P396
   Rahnavard N, 2007, IEEE T INFORM THEORY, V53, P1521, DOI 10.1109/TIT.2007.892814
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Sejdinovic D, 2009, IEEE T COMMUN, V57, P2510, DOI 10.1109/TCOMM.2009.09.070616
   Shokrollahi A., 2011, Raptor codes
   Vukobratovic D, 2009, IEEE T MULTIMEDIA, V11, P1094, DOI 10.1109/TMM.2009.2026087
   Xu XL, 2017, IEEE T VEH TECHNOL, V66, P3497, DOI 10.1109/TVT.2016.2594051
   Xu XL, 2017, IEEE COMMUN LETT, V21, P620, DOI 10.1109/LCOMM.2016.2636818
   Xu XL, 2014, IEEE COMMUN LETT, V18, P1163, DOI 10.1109/LCOMM.2014.2327614
   Xu Xiaoxin., 2016, 2016 IEEE S VLSI TEC, P1, DOI DOI 10.1109/VLSIT.2016.7573388
   Yang SH, 2016, IEEE COMMUN LETT, V20, P37, DOI 10.1109/LCOMM.2015.2499192
   Yang SH, 2014, IEEE T INFORM THEORY, V60, P5322, DOI 10.1109/TIT.2014.2334315
   Yuan L, 2016, IEEE T MULTIMEDIA, V18, P1389, DOI 10.1109/TMM.2016.2557079
NR 25
TC 16
Z9 16
U1 1
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2018
VL 20
IS 2
BP 271
EP 281
DI 10.1109/TMM.2017.2742699
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FT2ER
UT WOS:000422953200002
DA 2024-07-18
ER

PT J
AU Zhao, Z
   Yang, QF
   Lu, HQ
   Weninger, T
   Cai, D
   He, XF
   Zhuang, YT
AF Zhao, Zhou
   Yang, Qifan
   Lu, Hanqing
   Weninger, Tim
   Cai, Deng
   He, Xiaofei
   Zhuang, Yueting
TI Social-Aware Movie Recommendation via Multimodal Network Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Network representation; ranking metric learning; social-aware movie
   recommendation (SMR)
AB With the rapid development of Internet movie industry, social-aware movie recommendation systems (SMRs) have become a popular online web service that provide relevant movie recommendations to users. In this effort, many existing movie recommendation approaches learn a user ranking model from user feedback with respect to the movie's content. Unfortunately, this approach suffers from the sparsity problem inherent in SMR data. In the present work, we address the sparsity problem by learning a multimodal network representation for ranking movie recommendations. We develop a heterogeneous SMR network for movie recommendation that exploits the textual description and movie-poster image of each movie, as well as user ratings and social relationships. With this multimodal data, we then present a heterogeneous information network learning framework called SMR-multimodal network representation learning (MNRL) for movie recommendation. To learn a ranking metric from the heterogeneous information network we also developed a multimodal neural network model. We evaluated this model on a large-scale dataset from a real world SMR Web site, and we find that SMR-MNRL achieves better performance than other state-of-the-art solutions to the problem.
C1 [Zhao, Zhou; Yang, Qifan; Lu, Hanqing; Zhuang, Yueting] Zhejiang Univ, Coll Comp Sci, Hangzhou 310058, Zhejiang, Peoples R China.
   [Weninger, Tim] Univ Notre Dame, Dept Comp Sci & Engn, Notre Dame, IN 46556 USA.
   [Cai, Deng; He, Xiaofei] Zhejiang Univ, Coll Comp Sci, State Key Lab CAD&CG, Hangzhou 310058, Zhejiang, Peoples R China.
C3 Zhejiang University; University of Notre Dame; Zhejiang University
RP Zhao, Z (corresponding author), Zhejiang Univ, Coll Comp Sci, Hangzhou 310058, Zhejiang, Peoples R China.
EM zhaozhou@zju.edu.cn; bazinga@zju.edu.cn; lhq110@zju.edu.cn;
   tweninge@nd.edu; dengcai@gmail.com; xiaofeihe@gmail.com;
   yzhuang@zju.edu.cn
RI zhao, zhao/JAC-1686-2023; Zhao, zhuo/JYO-7894-2024
FU National Basic Research Program of China (973 Program) [2013CB336500];
   National Natural Science Foundation of China [61602405, U1611461];
   Fundamental Research Funds for the Central Universities [2016QNA5015];
   China Knowledge Centre for Engineering Sciences and Technology; Key
   Laboratory of Advanced Information Science and Network Technology of
   Beijing [XDXX1603]
FX This work was supported in part by the National Basic Research Program
   of China (973 Program) under Grant 2013CB336500, in part by the National
   Natural Science Foundation of China under Grant 61602405 and Grant
   U1611461, in part by the Fundamental Research Funds for the Central
   Universities 2016QNA5015, in part by the China Knowledge Centre for
   Engineering Sciences and Technology, and in part by the Key Laboratory
   of Advanced Information Science and Network Technology of Beijing
   (XDXX1603). The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Benoit Huet.
   (Corresponding author: Zhou Zhao.)
CR Andrew G., 2013, ICML, P1247
   [Anonymous], 2007, C NEUR INF PROC SYST
   [Anonymous], 2011, INTRO RECOMMENDER SY
   [Anonymous], 2012, P 20 ACM INT C MULT
   Atrey PK, 2010, MULTIMEDIA SYST, V16, P345, DOI 10.1007/s00530-010-0182-0
   Benini S, 2011, IEEE T MULTIMEDIA, V13, P1356, DOI 10.1109/TMM.2011.2163058
   Canini L, 2013, IEEE T CIRC SYST VID, V23, P636, DOI 10.1109/TCSVT.2012.2211935
   Chang SY, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P119, DOI 10.1145/2783258.2783296
   Chen CC, 2016, AAAI CONF ARTIF INTE, P108
   Chen CC, 2014, AAAI CONF ARTIF INTE, P9
   Cui P, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P597, DOI 10.1145/2647868.2654946
   Dean J., 2012, ADV NEURAL INFORM PR, P1223
   Deng ZY, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2637285
   Elkahky A, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P278, DOI 10.1145/2736277.2741667
   Fang Q, 2014, IEEE T MULTIMEDIA, V16, P796, DOI 10.1109/TMM.2014.2298216
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Koenigstein N., 2013, 7th ACM Conf. on Rec. Systems, P129, DOI DOI 10.1145/2507157.2507168
   Konstan J., 2001, P 10 INT C WORLD WID, P285
   Le Quoc V., 2014, P INT C MACH LEARN I
   LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9, DOI 10.1007/3-540-49430-8_2
   Liu NN, 2013, ACM T INTEL SYST TEC, V4, DOI 10.1145/2414425.2414440
   Ma ZG, 2012, IEEE T MULTIMEDIA, V14, P1662, DOI 10.1109/TMM.2012.2199293
   Mei T, 2011, ACM T INFORM SYST, V29, DOI 10.1145/1961209.1961213
   Ngiam J., 2011, IEEE INT C MACH LEAR, P689, DOI DOI 10.5555/3104482.3104569
   Pan W., 2013, P 23 INT JOINT C ART, P2691, DOI DOI 10.5555/2540128.2540516
   Perozzi B, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P701, DOI 10.1145/2623330.2623732
   Roy SD, 2013, IEEE T MULTIMEDIA, V15, P1255, DOI 10.1109/TMM.2013.2265079
   Shi Y, 2013, ACM T INTEL SYST TEC, V4, DOI 10.1145/2414425.2414441
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Srivastava N., 2012, ADV NEURAL INFORM PR, P2222, DOI DOI 10.1109/CVPR.2013.49
   Tang J, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1165, DOI 10.1145/2783258.2783307
   Tang J, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P1067, DOI 10.1145/2736277.2741093
   Tang JH, 2013, IEEE T KNOWL DATA EN, V25, P1510, DOI 10.1109/TKDE.2012.87
   Van den Oord A., 2013, ADV NEURAL INFORM PR, P2643, DOI [DOI 10.1109/MMUL.2011.34.VAN, 10.5555/2999792.2999907]
   Wang WR, 2015, PR MACH LEARN RES, V37, P1083
   Wang Z, 2013, IEEE T MULTIMEDIA, V15, P698, DOI 10.1109/TMM.2012.2237022
   Wei SX, 2016, ELECTRON COMMER R A, V18, P83, DOI 10.1016/j.elerap.2016.01.003
   Wu F, 2016, IEEE T IMAGE PROCESS, V25, P630, DOI 10.1109/TIP.2015.2507401
   Yan M, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P19, DOI 10.1145/2671188.2749344
   Yang Bo, 2007, P 6 ACM INT C IMAGE, P73
   Yang C, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2111
   Yeung MM, 1997, IEEE T CIRC SYST VID, V7, P771, DOI 10.1109/76.633496
   Zhao Sicheng, 2011, P 19 ACM INT C MULTI, P1473, DOI [10.1145/2072298.2072043, DOI 10.1145/2072298.2072043]
   Zhao XJ, 2012, LECT NOTES COMPUT SC, V7131, P149
   Zhao Xiaojian, 2011, P 19 ACM INT C MULTI, P1521
   Zheng XL, 2016, INFORM SCIENCES, V372, P276, DOI 10.1016/j.ins.2016.08.042
NR 47
TC 75
Z9 80
U1 3
U2 57
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2018
VL 20
IS 2
BP 430
EP 440
DI 10.1109/TMM.2017.2740022
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FT2ER
UT WOS:000422953200014
DA 2024-07-18
ER

PT J
AU Zhang, T
   Chen, HM
   Sun, MT
   Zhao, DB
   Gao, W
AF Zhang, Tao
   Chen, Haoming
   Sun, Ming-Ting
   Zhao, Debin
   Gao, Wen
TI Signal Dependent Transform Based on SVD for HEVC Intracoding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Transform; HEVC; Intra coding; SVD; DCT
ID INTRA PREDICTION; STANDARD; SCREEN; ALGORITHM
AB Transform is used to compact the energy of the blocks into a small number of coefficients and is widely used in recent image/video coding standards. In the latest video coding standard high efficiency video coding (HEVC), a combination of discrete cosine transform (DCT) and discrete sine transform (DST) is adopted to transform the residuals from intra prediction. Since the DCT and DST are the fixed transforms that are derived from the Gauss-Markov model, some of residual blocks may not be compacted well by the DCT/DST. In this paper, we propose a signal dependent transform based on singular value decomposition (SVD) for HEVC intracoding. The proposed transform (SDT-SVD) is derived by performing SVD on the synthetic block and applied to the residual block considering the structural similarity between them. Furthermore, we extend SDT-SVD to template matching prediction (TMP) to further improve the intracoding performance. Experimental results show that the proposed transform on angular intra prediction (AIP) outperforms the latest HEVC reference software with a bit rate reduction of 1.0% on average and it can be up to 2.1%. When the proposed transform is extended to TMP-based intracoding, the overall bit rate reduction is 2.7% on average and can be up to 5.8%.
C1 [Zhang, Tao] Harbin Inst Technol, Dept Comp Sci & Technol, Harbin 150001, Heilongjiang, Peoples R China.
   [Zhang, Tao] Tencent, Beijing 518057, Peoples R China.
   [Chen, Haoming] Google Inc, Kirkland, WA 98033 USA.
   [Sun, Ming-Ting] Univ Washington, Dept Elect Engn, Seattle, WA 98105 USA.
   [Zhao, Debin] Harbin Inst Technol, Dept Comp Sci & Technol, Harbin 150001, Heilongjiang, Peoples R China.
   [Gao, Wen] Peking Univ, Dept Elect Engn & Comp Sci, Beijing 100871, Peoples R China.
C3 Harbin Institute of Technology; Tencent; Google Incorporated; University
   of Washington; University of Washington Seattle; Harbin Institute of
   Technology; Peking University
RP Zhao, DB (corresponding author), Harbin Inst Technol, Dept Comp Sci & Technol, Harbin 150001, Heilongjiang, Peoples R China.
EM taozhang.hit@hotmail.com; haom-inghoward@gmail.com; sun@uw.edu;
   dbzhao@hit.edu.cn; wgao@pku.edu.cn
RI Zhang, Tao/V-5392-2019; Zhao, Debin/JEP-0204-2023
FU Major State Basic Research Development Program of China (973 Program)
   [2015CB351804]
FX This work was supported in part by the Major State Basic Research
   Development Program of China (973 Program) under Grant 2015CB351804. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Balakrishnan Prabhakaran.
   (Corresponding author: Debin Zhao.)
CR Alshina Elena, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P3689, DOI 10.1109/ICIP.2011.6116520
   ANDREWS HC, 1976, IEEE T COMMUN, V24, P425, DOI 10.1109/TCOM.1976.1093309
   [Anonymous], [No title captured]
   Bjontegaard G., 2001, Document VCEG-M33
   Bossen F., 2010, JCTVCB300 ITUT ISOIE
   Budagavi M, 2013, IEEE J-STSP, V7, P1029, DOI 10.1109/JSTSP.2013.2270429
   Cao XR, 2014, IEEE IMAGE PROC, P4127, DOI 10.1109/ICIP.2014.7025838
   Chen HM, 2016, IEEE T IMAGE PROCESS, V25, P3671, DOI 10.1109/TIP.2016.2573585
   Chen HM, 2014, IEEE IMAGE PROC, P3151, DOI 10.1109/ICIP.2014.7025637
   Chen HM, 2015, IEEE INT SYMP CIRC S, P2772, DOI 10.1109/ISCAS.2015.7169261
   Chen HM, 2012, IEEE SIGNAL PROC LET, V19, P344, DOI 10.1109/LSP.2012.2195172
   Dapena A, 2002, IEEE T CIRC SYST VID, V12, P114, DOI 10.1109/76.988658
   Han JN, 2012, IEEE T IMAGE PROCESS, V21, P1874, DOI 10.1109/TIP.2011.2169976
   Haoming Chen, 2012, 2012 IEEE International Symposium on Circuits and Systems - ISCAS 2012, P2921, DOI 10.1109/ISCAS.2012.6271927
   Haoming Chen, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P3697, DOI 10.1109/ICIP.2011.6116524
   Lainema J, 2012, IEEE T CIRC SYST VID, V22, P1792, DOI 10.1109/TCSVT.2012.2221525
   Lan CL, 2011, IEEE J-STSP, V5, P1298, DOI 10.1109/JSTSP.2011.2165273
   Lu Y, 2011, IEEE MULTIMEDIA, V18, P4, DOI 10.1109/MMUL.2011.33
   Rao K.R, 2014, DISCRETE COSINE TRAN
   Saxena A, 2013, IEEE T IMAGE PROCESS, V22, P3974, DOI 10.1109/TIP.2013.2265882
   Suehring K., 2016, 2 M JOINT VID EXPL T
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tan TK, 2006, IEEE IMAGE PROC, P1693, DOI 10.1109/ICIP.2006.312685
   Tao Zhang, 2015, 2015 Visual Communications and Image Processing (VCIP), P1, DOI 10.1109/VCIP.2015.7457833
   Ugur K., 2012, JCTVC10582 ITUT ISOI
   WALDEMAR P, 1996, P NORSIG C, P83
   Wang MH, 2014, IEEE T MULTIMEDIA, V16, P933, DOI 10.1109/TMM.2014.2305579
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xu J., 2013, JCTVCO0232 ITUT ISOI
   Xu JZ, 2016, IEEE T CIRC SYST VID, V26, P50, DOI 10.1109/TCSVT.2015.2478706
   Ye Y, 2008, IEEE IMAGE PROC, P2116, DOI 10.1109/ICIP.2008.4712205
   Yeo CH, 2012, IEEE T CIRC SYST VID, V22, P545, DOI 10.1109/TCSVT.2011.2168291
   Yu S. -L., 2002, 3 M JOINT VID TEAM M
   Yue HJ, 2013, IEEE T MULTIMEDIA, V15, P845, DOI 10.1109/TMM.2013.2239629
   Zhang T, 2015, ASIAPAC SIGN INFO PR, P388, DOI 10.1109/APSIPA.2015.7415300
   Zhao LP, 2016, IEEE T MULTIMEDIA, V18, P339, DOI 10.1109/TMM.2015.2512539
   Zhao X, 2012, IEEE T CIRC SYST VID, V22, P138, DOI 10.1109/TCSVT.2011.2158363
   Zhu WJ, 2015, IEEE T MULTIMEDIA, V17, P935, DOI 10.1109/TMM.2015.2428171
   Zhu WJ, 2014, IEEE T MULTIMEDIA, V16, P1316, DOI 10.1109/TMM.2014.2315782
   Zou F, 2013, IEEE J-STSP, V7, P1072, DOI 10.1109/JSTSP.2013.2274173
NR 40
TC 8
Z9 9
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2017
VL 19
IS 11
BP 2404
EP 2414
DI 10.1109/TMM.2017.2703114
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FJ9CT
UT WOS:000413068200004
DA 2024-07-18
ER

PT J
AU De Praeter, J
   Van Wallendael, G
   Slowack, J
   Lambert, P
AF De Praeter, Johan
   Van Wallendael, Glenn
   Slowack, Juergen
   Lambert, Peter
TI Video Encoder Architecture for Low-Delay Live-Streaming Events
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Encoder architecture; high efficiency video coding (HEVC); low delay;
   low complexity; video compression
ID MODE DECISION ALGORITHM; INTRA PREDICTION; MOTION ESTIMATION; SIZE
   DECISION; HEVC; H.264/AVC; STANDARD; SELECTION
AB Video-streaming events such as virtual classrooms and video conferences require a low delay between sender and receiver. In order to achieve this requirement, and to make full use of the bandwidth capacity of each receiver, each client can be provided with a personalized bitstream of which the bit rate is continuously adapted to his current network bandwidth capacity. However, such an approach requires an excessive amount of computationally complex video encoders. Therefore, this paper proposes an architecture based on coding information calculation (CIC) modules and residual encoder (RE) modules. The CIC modules calculate coding information for the video at certain bit rates whereas the RE modules use this information to skip all encoding steps of a traditional encoder, except for the encoding of the residual. By reducing the amount of bits used to encode the residual, the RE modules can then provide bitstreams with personalized bit rates for several users at the same time. Each CIC module has approximately the same computational complexity as a traditional encoder, whereas an RE module has the approximate complexity of a decoder. The proposed architecture was evaluated for the high efficiency video coding standard, showing that the system achieves its goal of drastically reducing the computational complexity of low-delay live-streaming with many participants and suggesting that using less than six CIC modules results in the best tradeoff between compression efficiency and computational complexity.
C1 [De Praeter, Johan; Van Wallendael, Glenn; Lambert, Peter] Univ Ghent, Dept Elect & Informat Syst, IDLab, IMEC, B-9052 Ghent, Belgium.
   [Slowack, Juergen] Barco, B-8500 Kortrijk, Belgium.
C3 Ghent University; IMEC
RP De Praeter, J (corresponding author), Univ Ghent, Dept Elect & Informat Syst, IDLab, IMEC, B-9052 Ghent, Belgium.
EM johan.depraeter@ugent.be; glenn.vanwallendael@ugent.be;
   jurgen.slowack@barco.com; peter.lambert@ugent.be
RI Van Wallendael, Glenn/H-8315-2015; Lambert, Peter/D-7776-2016
OI Van Wallendael, Glenn/0000-0001-9530-3466; Lambert,
   Peter/0000-0001-5313-4158
FU IDLab (Ghent University - imec); Flanders Innovation & Entrepreneurship
   (VLAIO); Scientific Research Flanders (FWO Flanders); European Union
FX This work was supported in part by the IDLab (Ghent University - imec),
   in part by the Flanders Innovation & Entrepreneurship (VLAIO), in part
   by the Scientific Research Flanders (FWO Flanders), and the European
   Union. The guest editor coordinating the review of this manuscript and
   approving it for publication was Dr. Shiwen Mao. (Corresponding author:
   Johan De Praeter.)
CR Ahn S, 2015, IEEE T CIRC SYST VID, V25, P422, DOI 10.1109/TCSVT.2014.2360031
   [Anonymous], 2006, ALL NONPARAMETRIC ST
   [Anonymous], 1915, MEMOIRE SERVICE GEOL
   [Anonymous], 2001, ITU T VCEG M AUST TE
   Bharanitharan K, 2008, IEEE T MULTIMEDIA, V10, P1250, DOI 10.1109/TMM.2008.2004904
   Chiang CK, 2011, IEEE T CIRC SYST VID, V21, P1304, DOI 10.1109/TCSVT.2011.2147250
   Corey DM, 1998, J GEN PSYCHOL, V125, P245, DOI 10.1080/00221309809595548
   Correa G, 2015, IEEE T CIRC SYST VID, V25, P660, DOI 10.1109/TCSVT.2014.2363753
   De Praeter J, 2016, ELECTRON LETT, V52, P1116, DOI 10.1049/el.2016.0201
   Fini MR, 2016, MULTIMED TOOLS APPL, V75, P7541, DOI 10.1007/s11042-015-2675-5
   Huang XP, 2017, SIGNAL IMAGE VIDEO P, V11, P33, DOI 10.1007/s11760-016-0887-4
   Ismail Y, 2012, IEEE T CIRC SYST VID, V22, P28, DOI 10.1109/TCSVT.2011.2148450
   Kim BG, 2008, IEEE T CIRC SYST VID, V18, P127, DOI 10.1109/TCSVT.2007.913748
   Kim HS, 2016, IEEE T CIRC SYST VID, V26, P130, DOI 10.1109/TCSVT.2015.2444672
   Kim TJ, 2009, IEEE T CONSUM ELECTR, V55, P179, DOI 10.1109/TCE.2009.4814432
   Lee H, 2015, IEEE T BROADCAST, V61, P388, DOI 10.1109/TBC.2015.2419172
   Lee J, 2015, IEEE T CIRC SYST VID, V25, P411, DOI 10.1109/TCSVT.2014.2339612
   Lee JY, 2012, IEEE T CIRC SYST VID, V22, P393, DOI 10.1109/TCSVT.2011.2163460
   Liu ZY, 2008, IEEE T CIRC SYST VID, V18, P620, DOI 10.1109/TCSVT.2008.918844
   Van LP, 2016, IEEE T MULTIMEDIA, V18, P364, DOI 10.1109/TMM.2015.2512231
   Paul M, 2009, IEEE T MULTIMEDIA, V11, P581, DOI 10.1109/TMM.2009.2017610
   Quan D, 2010, IEEE T CONSUM ELECTR, V56, P1049, DOI 10.1109/TCE.2010.5506038
   Ri SH, 2009, IEEE T CIRC SYST VID, V19, P302, DOI 10.1109/TCSVT.2008.2009257
   Rosewarne C., 2015, JCTVCT1002 ITUT
   Ruiz D, 2016, SIGNAL PROCESS-IMAGE, V44, P12, DOI 10.1016/j.image.2016.03.002
   Rusert T, 2016, IEEE IMAGE PROC, P1489, DOI 10.1109/ICIP.2016.7532606
   Schroeder D., IEEE T CIRC IN PRESS
   Schroeder D, 2015, IEEE IMAGE PROC, P3972, DOI 10.1109/ICIP.2015.7351551
   Severance C, 1998, COMPUTER, V31, P94
   Shen LQ, 2015, SIGNAL PROCESS-IMAGE, V32, P121, DOI 10.1016/j.image.2015.01.008
   Shen LQ, 2014, IEEE T CIRC SYST VID, V24, P1709, DOI 10.1109/TCSVT.2014.2313892
   Shen LQ, 2014, IEEE T IMAGE PROCESS, V23, P4232, DOI 10.1109/TIP.2014.2341927
   Sodagar I, 2011, IEEE MULTIMEDIA, V18, P62, DOI 10.1109/MMUL.2011.71
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sung YH, 2012, IEEE T MULTIMEDIA, V14, P693, DOI 10.1109/TMM.2012.2186793
   Tan HL, 2016, IEEE T BROADCAST, V62, P128, DOI 10.1109/TBC.2015.2505406
   Van Wallendael G, 2012, IEEE IMAGE PROC, P733, DOI 10.1109/ICIP.2012.6466964
   Vermeir T, 2016, IEEE IMAGE PROC, P819, DOI 10.1109/ICIP.2016.7532471
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Wu CY, 2013, IEEE T MULTIMEDIA, V15, P1083, DOI 10.1109/TMM.2013.2247033
   Xiong J, 2014, IEEE T MULTIMEDIA, V16, P559, DOI 10.1109/TMM.2013.2291958
   Xu XZ, 2008, IEEE T CIRC SYST VID, V18, P285, DOI 10.1109/TCSVT.2008.918122
   Yang SH, 2015, ELECTRON LETT, V51, P2109, DOI 10.1049/el.2015.3094
   Zeng HQ, 2009, IEEE T CIRC SYST VID, V19, P491, DOI 10.1109/TCSVT.2009.2014014
   Zhang JL, 2016, IEEE T CIRC SYST VID, V26, P1502, DOI 10.1109/TCSVT.2015.2461991
   Zupancic I, 2016, IEEE T MULTIMEDIA, V18, P1677, DOI 10.1109/TMM.2016.2579505
NR 46
TC 4
Z9 4
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2017
VL 19
IS 10
BP 2252
EP 2266
DI 10.1109/TMM.2017.2734330
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5YG
UT WOS:000411247600010
OA Green Published
DA 2024-07-18
ER

PT J
AU Wu, TT
   Dou, WC
   Ni, Q
   Yu, S
   Chen, GH
AF Wu, Taotao
   Dou, Wanchun
   Ni, Qiang
   Yu, Shui
   Chen, Guihai
TI Mobile Live Video Streaming Optimization via Crowdsourcing Brokerage
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Crowdsourcing broker; mobile live video streaming; QoE; scheduling
   optimization
ID TRANSMISSION; HEVC
AB Nowadays, people can enjoy a rich real-time sensing cognition of what they are interested in anytime and anywhere by leveraging powerful mobile devices such as smartphones. As a key support for the propagation of these richer live media contents, cellular-based access technologies play a vital role to provide reliable and ubiquitous Internet access to mobile devices. However, these limited wireless network channel conditions vary and fluctuate depending on weather, building shields, congestion, etc., which degrade the quality of live video streaming dramatically. To address this challenge, we propose to use crowdsourcing brokerage in future networks which can improve each mobile user's bandwidth condition and reduce the fluctuation of network condition. Further, to serve mobile users better in this crowdsourcing style, we study the brokerage scheduling problem which aims at maximizing the user's quality of experience satisfaction degree cost effectively. Both offline and online algorithms are proposed to solve this problem. The results of extensive evaluations demonstrate that by leveraging crowdsourcing technique, our solution can cost-effectively guarantee a higher quality view experience.
C1 [Wu, Taotao; Dou, Wanchun; Chen, Guihai] Nanjing Univ, Dept Comp Sci & Technol, State Key Lab Novel Software Technol, Nanjing 210093, Jiangsu, Peoples R China.
   [Ni, Qiang] Univ Lancaster, InfoLab21, Sch Comp & Commun, Lancaster LA1 4YW, England.
   [Yu, Shui] Deakin Univ, Sch Informat Technol, Burwood, Vic 3125, Australia.
C3 Nanjing University; Lancaster University; Deakin University
RP Dou, WC (corresponding author), Nanjing Univ, Dept Comp Sci & Technol, State Key Lab Novel Software Technol, Nanjing 210093, Jiangsu, Peoples R China.
EM wutaotaoxpy@gmail.com; douwc@nju.edu.cn; q.ni@lancaster.ac.uk;
   syu@deakin.edu.au; gchen@nju.edu.cn
RI Yu, Shui/AFL-2699-2022; Ni, Qiang/B-4990-2010
OI Yu, Shui/0000-0003-4485-6743; Ni, Qiang/0000-0002-4593-1656; Chen,
   Guihai/0000-0002-6934-1685
FU EU FP7 CROWN Project [PIRSES-GA-2013-610524]; National Science
   Foundation of China [61672276]; Key Research and Development Project of
   Jiangsu Province [BE2015154, BE2016120]; Collaborative Innovation Center
   of Novel Software Technology and Industrialization, Nanjing University;
   EPSRC [EP/R00692X/1] Funding Source: UKRI
FX This work was supported in part by the EU FP7 CROWN Project under Grant
   PIRSES-GA-2013-610524, in part by the National Science Foundation of
   China under Grant 61672276, in part by the Key Research and Development
   Project of Jiangsu Province under Grant BE2015154 and Grant BE2016120,
   and in part by the Collaborative Innovation Center of Novel Software
   Technology and Industrialization, Nanjing University. The guest editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Mahbub Hassan. (Corresponding author: Wanchun Dou.)
CR Acharya T, 2005, IMAGE PROCESSING: PRINCIPLES AND APPLICATIONS, P1, DOI 10.1002/0471745790
   [Anonymous], EURASIP J WIRELESS C
   [Anonymous], 2016, P 35 IEEE INT C COMP
   [Anonymous], 2016, IEEE COMMUN SURV TUT, DOI DOI 10.1109/COMST.2015.2477041
   [Anonymous], BIG BUCK BUNNY MOVIE
   [Anonymous], 2013, P IEEE 20 INT PACK V
   [Anonymous], 2017, DIVERSITY INDEX
   [Anonymous], 2016, IEEE INFOCOM 2016-The 35th Annual IEEE International Conference on Computer Communications
   Baik Eilwoo, 2015, 2015 IEEE Conference on Computer Communications (INFOCOM). Proceedings, P1, DOI 10.1109/INFOCOM.2015.7218361
   Bao W., 2016, PROC IEEE INT C COMP, P1
   Cicalò S, 2014, IEEE T MULTIMEDIA, V16, P848, DOI 10.1109/TMM.2014.2300442
   Dai J, 2012, IEEE J SEL AREA COMM, V30, P458, DOI 10.1109/JSAC.2012.120226
   Dobrian F, 2011, ACM SIGCOMM COMP COM, V41, P362, DOI 10.1145/2043164.2018478
   Fei Chen, 2015, 2015 IEEE Conference on Computer Communications (INFOCOM). Proceedings, P2524, DOI 10.1109/INFOCOM.2015.7218642
   Fesehaye D, 2012, IEEE INT ENTERP DIST, P123, DOI 10.1109/EDOC.2012.23
   Kreutz D, 2015, P IEEE, V103, P14, DOI 10.1109/JPROC.2014.2371999
   Krishnan R. K., 2012, P INT MEAS C, P211
   Le A, 2016, IEEE ACM T NETWORK, V24, P2705, DOI 10.1109/TNET.2015.2501349
   Li BC, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2505805
   Li Z, 2014, IEEE J SEL AREA COMM, V32, P719, DOI 10.1109/JSAC.2014.140405
   Lim Y.-s., 2015, P 11 ACM C EMERGING, P30
   Liu FM, 2015, IEEE T COMPUT, V64, P3051, DOI 10.1109/TC.2015.2401032
   Liu YN, 2016, IEEE T MULTIMEDIA, V18, P865, DOI 10.1109/TMM.2016.2538718
   Lu Z., 2016, INFOCOM, P1
   Nam H., 2016, IEEE INFOCOM 2016-The 35th Annual IEEE International Conference on Computer Communications, P1
   Neely, 2010, STOCHASTIC NETWORK O
   Nicholson A. J., 2006, MobiSys2006. The Fourth International Conference on Mobile Systems, Applications and Services, P233, DOI 10.1145/1134680.1134705
   Nguyen-Vuong QT, 2013, IEEE T VEH TECHNOL, V62, P1785, DOI 10.1109/TVT.2012.2234772
   Ra M.-R., 2010, 8th International Conference on Mobile systems, Applications, and Services, P255
   Riiser Haakon, 2013, P 4 ACM MULT SYST C, P114, DOI DOI 10.1145/2483977.2483991
   Sitaraman R. K., 2013, 2013 Fifth International Conference on Communication Systems and Networks (COMSNETS), DOI 10.1109/COMSNETS.2013.6465563
   Spiteri K, 2016, IEEE INFOCOM SER, DOI 10.1109/infocom.2016.7524428
   van der Hooft J, 2016, IEEE COMMUN LETT, V20, P2177, DOI 10.1109/LCOMM.2016.2601087
   Wang N., 2016, PROC 35 ANN IEEE INT, P1
   Wu C, 2017, PSYCHOL MED, V47, P837, DOI 10.1017/S0033291716002816
   Xu ZC, 2016, IEEE T PARALL DISTR, V27, P2866, DOI 10.1109/TPDS.2015.2510638
   Ye Y, 2014, IEEE MULTIMEDIA, V21, P58, DOI 10.1109/MMUL.2014.47
   Zhao YH, 2014, IEEE INFOCOM SER, P298, DOI 10.1109/INFOCOM.2014.6847951
NR 38
TC 4
Z9 5
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2017
VL 19
IS 10
BP 2267
EP 2281
DI 10.1109/TMM.2017.2736963
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5YG
UT WOS:000411247600011
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Tang, S
   Li, Y
   Deng, LX
   Zhang, YD
AF Tang, Sheng
   Li, Yu
   Deng, Lixi
   Zhang, Yongdong
TI Object Localization Based on Proposal Fusion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dense proposal fusion; object localization; object detection; region
   proposal
ID CONVOLUTIONAL NETWORKS; SEGMENTATION; GRADIENTS
AB Traditional regression framework of object localization such as Overfeat often suffers from the problem of inaccurate scoring due to the separate scoring of classification network and regression network upon inconsistent regions. To tackle this problem, in this paper, we propose a novel object localization framework based on multiple complementary region proposal methods from the view of classification rather than regression. On top of our framework, we first combine multiple complementary region proposals during both training and testing as a means of data augmentation to generate more dense and reliable proposals for fusion, then achieve optimal compromise between complexity and efficiency through category clustering for bounding box sharing among similar categories, and finally propose a dense proposal fusion approach to merge dense region proposals near true object for fine-tuning of the final bounding box's coordinates and updating the confidence of fused proposals for final decision. Extensive experiments on the well-known large scale ILSVRC 2015 LOC dataset verify the effectiveness of our object localization framework.
C1 [Tang, Sheng; Li, Yu; Deng, Lixi; Zhang, Yongdong] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
   [Tang, Sheng; Li, Yu; Deng, Lixi; Zhang, Yongdong] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS
RP Zhang, YD (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.; Zhang, YD (corresponding author), Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
EM ts@ict.ac.cn; liyu@ict.ac.cn; denglixi@ict.ac.cn; zhyd@ict.ac.cn
FU National Natural Science Foundation of China [61525206, 61572472];
   National Key Research and Development Program of China [2016YFB0800403];
   Beijing Natural Science Foundation [4152050]; Beijing Advanced
   Innovation Center for Imaging Technology [BAICIT-2016009]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61525206 and Grant 61572472, in part by
   the National Key Research and Development Program of China
   (2016YFB0800403), in part by the Beijing Natural Science Foundation
   under Grant 4152050, and in part by Beijing Advanced Innovation Center
   for Imaging Technology under Grant BAICIT-2016009. The guest editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Qi Tian. (Corresponding author: Yongdong Zhang.)
CR Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28
   Alexe B, 2010, PROC CVPR IEEE, P73, DOI 10.1109/CVPR.2010.5540226
   [Anonymous], COMPUT VIS PATTERN R
   [Anonymous], 2013, CoRR
   Arbeláez P, 2014, PROC CVPR IEEE, P328, DOI 10.1109/CVPR.2014.49
   Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314
   Bourdev L, 2005, PROC CVPR IEEE, P236, DOI 10.1109/cvpr.2005.310
   Brubaker SC, 2008, INT J COMPUT VISION, V77, P65, DOI 10.1007/s11263-007-0060-1
   CARREIRA J, 2010, PROC CVPR IEEE, P3241, DOI DOI 10.1109/CVPR.2010.5540063
   Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414
   Dai Jifeng, 2016, Advances in Neural Information Processing Systems, DOI DOI 10.1016/J.JPOWSOUR.2007.02.075
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Erhan D, 2014, PROC CVPR IEEE, P2155, DOI 10.1109/CVPR.2014.276
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Gidaris S, 2015, IEEE I CONF COMP VIS, P1134, DOI 10.1109/ICCV.2015.135
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2016, IEEE T PATTERN ANAL, V38, P142, DOI 10.1109/TPAMI.2015.2437384
   Harzallah H, 2009, IEEE I CONF COMP VIS, P237, DOI 10.1109/ICCV.2009.5459257
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   He Kaiming, 2016, EUR C COMP VIS ECCV, DOI [DOI 10.1109/CVPR.2016.90, DOI 10.1007/978-3-319-46493-0_38]
   Heng-Yu Chi, 2014, MultiMedia Modeling. 20th Anniversary International Conference, MMM 2014. Proceedings: LNCS 8325, P207, DOI 10.1007/978-3-319-04114-8_18
   Hosang J, 2016, IEEE T PATTERN ANAL, V38, P814, DOI 10.1109/TPAMI.2015.2465908
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Karypis G, 1999, COMPUTER, V32, P68, DOI 10.1109/2.781637
   Kong T, 2016, PROC CVPR IEEE, P845, DOI 10.1109/CVPR.2016.98
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lazebnik S., 2006, P IEEE CVF C COMP VI, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/CVPR.2006.68]
   Li LH, 2017, AAAI CONF ARTIF INTE, P4133
   Li T, 2011, IEEE T CIRC SYST VID, V21, P381, DOI 10.1109/TCSVT.2010.2041828
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Moxley E, 2010, IEEE T MULTIMEDIA, V12, P184, DOI 10.1109/TMM.2010.2041101
   Papageorgiou C, 2000, INT J COMPUT VISION, V38, P15, DOI 10.1023/A:1008162616689
   Pont-Tuset J, 2016, IEEE T PATTERN ANAL, V38, DOI [10.1109/TPAMI.2015.2481406, 10.1109/TPAMI.2016.2537320]
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sanchez-Riera J, 2016, PATTERN RECOGN LETT, V73, P1, DOI 10.1016/j.patrec.2015.12.006
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Suykens JAK, 1999, NEURAL PROCESS LETT, V9, P293, DOI 10.1023/A:1018628609742
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Tang S., P TRECVID 2008 WORKH
   Tang S, 2015, NEUROCOMPUTING, V169, P124, DOI 10.1016/j.neucom.2014.09.100
   Tang S, 2012, IEEE T MULTIMEDIA, V14, P43, DOI 10.1109/TMM.2011.2168198
   Tsai TH, 2014, IEEE T IMAGE PROCESS, V23, P1047, DOI 10.1109/TIP.2014.2298982
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   van de Sande KEA, 2011, IEEE I CONF COMP VIS, P1879, DOI 10.1109/ICCV.2011.6126456
   Wang M, 2015, IEEE T KNOWL DATA EN, V27, P2564, DOI 10.1109/TKDE.2015.2415497
   Yoo D, 2015, IEEE I CONF COMP VIS, P2659, DOI 10.1109/ICCV.2015.305
   Zagoruyko S., 2016, ARXIV160507146, DOI DOI 10.5244/C.30.87
   Zhang R., 2017, 26 INT JOINT C ART I
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 54
TC 19
Z9 22
U1 1
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2017
VL 19
IS 9
BP 2105
EP 2116
DI 10.1109/TMM.2017.2729786
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5XC
UT WOS:000411244200013
DA 2024-07-18
ER

PT J
AU Lee, HJ
   Hong, KS
   Kang, H
   Lee, S
AF Lee, Hui-Jin
   Hong, Ki-Sang
   Kang, Henry
   Lee, Seungyong
TI Photo Aesthetics Analysis via DCNN Feature Encoding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Aesthetic attributes; deep convolutional neural network (DCNN); feature
   encoding; photo aesthetics; restricted Boltzmann machines
ID QUALITY
AB We propose an automatic framework for quality assessment of a photograph as well as analysis of its aesthetic attributes. In contrast to the previous methods that rely on manually designed features to account for photo aesthetics, our method automatically extracts such features using a pretrained deep convolutional neural network (DCNN). To make the DCNN-extracted features more suited to our target tasks of photo quality assessment and aesthetic attribute analysis, we propose a novel feature encoding scheme, which supports vector machines-driven sparse restricted Boltzmann machines, which enhances sparseness of features and discrimination between target classes. Experimental results show that our method outperforms the current state-of-the-art methods in automatic photo quality assessment, and gives aesthetic attribute ratings that can be used for photo editing. We demonstrate that our feature encoding scheme can also be applied to general object classification task to achieve performance gains.
C1 [Lee, Hui-Jin; Hong, Ki-Sang] Pohang Univ Sci & Technol, Dept Elect Engn, Pohang 37673, South Korea.
   [Kang, Henry] Univ Missouri, Dept Math & Comp Sci, St Louis, MO 63121 USA.
   [Lee, Seungyong] Pohang Univ Sci & Technol, Dept Comp Sci & Engn, Pohang 37673, South Korea.
C3 Pohang University of Science & Technology (POSTECH); University of
   Missouri System; University of Missouri Saint Louis; Pohang University
   of Science & Technology (POSTECH)
RP Lee, S (corresponding author), Pohang Univ Sci & Technol, Dept Comp Sci & Engn, Pohang 37673, South Korea.
EM huijin@postech.ac.kr; hongks@postech.ac.kr; kangh@umsl.edu;
   leesy@postech.ac.kr
FU Institute for Information and Communications Technology Promotion
   [R0126-16-1078]; National Research Foundation of Korea
   [NRF-2014R1A2A1A11052779]; Korea Government (MSIP)
FX This work was supported in part by the Institute for Information and
   Communications Technology Promotion under Grant R0126-16-1078, and in
   part by the National Research Foundation of Korea under Grant
   NRF-2014R1A2A1A11052779, both funded by the Korea Government (MSIP). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Jingdong Wang. (Corresponding
   author: Seungyong Lee.)
CR [Anonymous], 2005, AISTATS BRIDGETOWN B
   [Anonymous], P EUR C COMPUT VIS
   [Anonymous], CORR
   [Anonymous], P NIPS 2010 WORKSH D
   [Anonymous], 2010, ACM MULTIMEDIA 2010
   [Anonymous], 2013, CORR
   Aydin TO, 2015, IEEE T VIS COMPUT GR, V21, P31, DOI 10.1109/TVCG.2014.2325047
   Boureau Y.-l., 2008, ADV NEURAL INFORM PR, P1185
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong Z, 2014, IEEE SYS MAN CYBERN, P2859, DOI 10.1109/SMC.2014.6974363
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772
   Goh H, 2012, LECT NOTES COMPUT SC, V7576, P298, DOI 10.1007/978-3-642-33715-4_22
   Goh Hanlin., 2013, Advances in Neural Information Processing Systems, P1878
   Guo YM, 2012, PATTERN RECOGN, V45, P3834, DOI 10.1016/j.patcog.2012.04.003
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hoyer PO, 2004, J MACH LEARN RES, V5, P1457
   Ke Y., 2006, P IEEE COMP SOC C CO, V1, P419, DOI DOI 10.1109/CVPR.2006.303
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Larochelle H., 2008, P 25 INT C MACH LEAR, P536
   Liu LG, 2010, COMPUT GRAPH FORUM, V29, P469, DOI 10.1111/j.1467-8659.2009.01616.x
   Lo KY, 2012, INT C PATT RECOG, P2186
   Lu X, 2015, IEEE I CONF COMP VIS, P990, DOI 10.1109/ICCV.2015.119
   Lu X, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P457, DOI 10.1145/2647868.2654927
   Luo YW, 2008, LECT NOTES COMPUT SC, V5304, P386
   Marchesotti L, 2011, IEEE I CONF COMP VIS, P1784, DOI 10.1109/ICCV.2011.6126444
   McClelland J L, 1986, Parallel distributed processing
   Mittelman R, 2013, PROC CVPR IEEE, P476, DOI 10.1109/CVPR.2013.68
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Sawant N, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P576, DOI 10.1109/ICVGIP.2008.17
   Simonyan K., 2014, CORR
   Su HH, 2012, IEEE T MULTIMEDIA, V14, P833, DOI 10.1109/TMM.2012.2186123
   Tang XO, 2013, IEEE T MULTIMEDIA, V15, P1930, DOI 10.1109/TMM.2013.2269899
   Tian XM, 2015, IEEE T MULTIMEDIA, V17, P2035, DOI 10.1109/TMM.2015.2479916
   Tong HG, 2004, LECT NOTES COMPUT SC, V3331, P198
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Wang YJ, 2010, PRODUCTION GRIDS IN ASIA: APPLICATIONS, DEVELOPMENTS AND GLOBAL TIES, P155, DOI 10.1007/978-1-4419-0046-3_13
   Wong LK, 2009, IEEE IMAGE PROC, P997, DOI 10.1109/ICIP.2009.5413825
   Yu HJ, 2003, PROCEEDINGS OF THE 2003 IEEE BIOINFORMATICS CONFERENCE, P220
   Zhang FL, 2013, IEEE T MULTIMEDIA, V15, P1480, DOI 10.1109/TMM.2013.2268051
   Zhe Dong, 2015, MultiMedia Modeling. 21st International Conference, MMM 2015. Proceedings: LNCS 8936, P524, DOI 10.1007/978-3-319-14442-9_57
NR 44
TC 25
Z9 25
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2017
VL 19
IS 8
BP 1921
EP 1932
DI 10.1109/TMM.2017.2687759
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5VV
UT WOS:000411240400019
DA 2024-07-18
ER

PT J
AU Sun, WT
   Chao, TH
   Kuo, YH
   Hsu, WH
AF Sun, Wei-Tse
   Chao, Ting-Hsuan
   Kuo, Yin-Hsi
   Hsu, Winston H.
TI Photo Filter Recommendation by Category-Aware Aesthetic Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Aesthetic; convolutional neural network (CNN); filter recommendation;
   image quality; pairwise comparison
ID QUALITY ASSESSMENT; IMAGE; NETWORKS; RANK
AB Nowadays, social media has become a popular platform for the public to share photos. To make photos more visually appealing, users usually apply filters on their photos without domain knowledge. However, due to the growing number of filter types, it becomes a major issue for users to choose the best filter type. For this purpose, filter recommendation for photo aesthetics takes an important role in image quality ranking problems. In these years, several works have declared that convolutional neural networks (CNNs) outperform traditional methods in image aesthetic categorization, which classifies images into high or low quality. Most of them do not consider the effect on filtered images; hence, we propose a novel image aesthetic learning for filter recommendation. Instead of binarizing image quality, we adjust the state-of-the-art CNN architectures and design a pairwise loss function to learn the embedded aesthetic responses in hidden layers for filtered images. Based on our pilot study, we observe image categories (e.g., portrait, landscape, food) will affect user preference on filter selection. We further integrate category classification into our proposed aesthetic-oriented models. To the best of our knowledge, there is no public dataset for aesthetic judgment with filtered images. We create a new dataset called filter aesthetic comparison dataset (FACD). It is the first dataset containing 28 160 filtered images and 42 240 user preference labels. We conduct experiments on the collected FACD for filter recommendation, and the results show that our proposed category-aware aesthetic learning outperforms aesthetic classification methods (e.g., 12% relative improvement).
C1 [Sun, Wei-Tse; Chao, Ting-Hsuan; Hsu, Winston H.] Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 10617, Taiwan.
   [Kuo, Yin-Hsi; Hsu, Winston H.] Natl Taiwan Univ, Grad Inst Networking & Multimedia, Taipei 10617, Taiwan.
C3 National Taiwan University; National Taiwan University
RP Hsu, WH (corresponding author), Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 10617, Taiwan.; Hsu, WH (corresponding author), Natl Taiwan Univ, Grad Inst Networking & Multimedia, Taipei 10617, Taiwan.
EM r03922071@ntu.edu.tw; r02922047@ntu.edu.tw;
   kuonini@cmlab.csie.ntu.edu.tw; whsu@ntu.edu.tw
OI HSU, WINSTON/0000-0002-3330-0638
FU Ministry of Science and Technology, Taiwan [MOST 104-2622-8-002-002,
   MOST 105-2218-E-002-032]; MediaTek Inc.; NVIDIA; NVIDIA DGX-1 AI
   Supercomputer
FX This work was supported in part by the Ministry of Science and
   Technology, Taiwan, under Grant MOST 104-2622-8-002-002 and Grant MOST
   105-2218-E-002-032, in part by MediaTek Inc., and in part by grants from
   NVIDIA and the NVIDIA DGX-1 AI Supercomputer. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Marco Bertini. (Corresponding author: Winston H.
   Hsu.)
CR [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2021, P THEINTERNATIONAL A
   [Anonymous], 2013, CoRR
   Chen ZY, 2014, PROC CVPR IEEE, P3003, DOI 10.1109/CVPR.2014.384
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Guo ZY, 2013, IEEE I CONF COMP VIS, P2568, DOI 10.1109/ICCV.2013.319
   He KM, 2015, IEEE T PATTERN ANAL, V37, P1904, DOI 10.1109/TPAMI.2015.2389824
   Hoffer E, 2015, LECT NOTES COMPUT SC, V9370, P84, DOI 10.1007/978-3-319-24261-3_7
   Isola P, 2011, PROC CVPR IEEE, P145, DOI 10.1109/CVPR.2011.5995721
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Ke Y., 2006, P IEEE COMP SOC C CO, V1, P419, DOI DOI 10.1109/CVPR.2006.303
   Lu X, 2015, IEEE I CONF COMP VIS, P990, DOI 10.1109/ICCV.2015.119
   Lu X, 2015, IEEE T MULTIMEDIA, V17, P2021, DOI 10.1109/TMM.2015.2477040
   Lu X, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P457, DOI 10.1145/2647868.2654927
   Luo W, 2011, IEEE I CONF COMP VIS, P2206, DOI 10.1109/ICCV.2011.6126498
   Luo YW, 2008, LECT NOTES COMPUT SC, V5304, P386
   Marchesotti L, 2011, IEEE I CONF COMP VIS, P1784, DOI 10.1109/ICCV.2011.6126444
   Mason W, 2012, BEHAV RES METHODS, V44, P1, DOI 10.3758/s13428-011-0124-6
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Redi Judith., 2014, Proc. of the International ACM Workshop on Crowdsourcing for Multimedia, P25, DOI [10.1145/2660114.2660118, DOI 10.1145/2660114.2660118]
   Simonyan K., 2014, CORR
   Su HH, 2012, IEEE T MULTIMEDIA, V14, P833, DOI 10.1109/TMM.2012.2186123
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Wang J, 2014, PROC CVPR IEEE, P1386, DOI 10.1109/CVPR.2014.180
   Weston J, 2010, MACH LEARN, V81, P21, DOI 10.1007/s10994-010-5198-3
   Yan JZ, 2014, PROC CVPR IEEE, P2987, DOI 10.1109/CVPR.2014.382
   Zhang ZP, 2014, LECT NOTES COMPUT SC, V8694, P94, DOI 10.1007/978-3-319-10599-4_7
   Zhe Dong, 2015, MultiMedia Modeling. 21st International Conference, MMM 2015. Proceedings: LNCS 8936, P524, DOI 10.1007/978-3-319-14442-9_57
NR 33
TC 30
Z9 34
U1 1
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2017
VL 19
IS 8
BP 1870
EP 1880
DI 10.1109/TMM.2017.2688929
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5VV
UT WOS:000411240400015
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wu, GL
   Kang, WX
AF Wu, Guile
   Kang, Wenxiong
TI Vision-Based Fingertip Tracking Utilizing Curvature Points Clustering
   and Hash Model Representation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Curvature points clustering; fingertip tracking; fingertip detection;
   hash model representation; human-computer interaction (HCI)
ID RECOGNITION
AB Fingertip tracking plays an increasingly important role in augmented reality and virtual-reality applications. However, existing approaches (either continuous-detection-based or separated-detection-tracking-based methods) cannot effectively learn temporal-spatial information or even separate fingertip tracking into unrelated stages, which causes poor real-time performance and incomplete tracking continuity. Moreover, due to the need for high-cost devices, the high degrees of freedom of the hand, and subtle differences among fingers, fingertip tracking remains a challenging task. To address these problems, we propose a novel tracking-combined-with-detection approach for vision-based fingertip tracking. By adopting clustering and geometric constraint analysis, we develop a curvature points clustering method for fingertip detection. Then, by exploiting the identified fingertip points for motion estimation with bidirectional optical flows and temporal-spatial probability calculation, the tracking stage is effectively integrated with the detection stage. To accurately locate the fingertip, we represent the fingertip model with a perceptual hash sequence and locate the fingertip by searching for the best-matching region. Extensive experimental results show the superiority of the proposed algorithm to commonly used and state-of-the-art methods and demonstrate its effectiveness and practicability.
C1 [Wu, Guile] South China Univ Technol, Automat & Sci Technol Coll, Guangzhou 510640, Guangdong, Peoples R China.
   [Kang, Wenxiong] South China Univ Technol, Sch Automat Sci & Engn, Guangzhou 510641, Guangdong, Peoples R China.
C3 South China University of Technology; South China University of
   Technology
RP Kang, WX (corresponding author), South China Univ Technol, Sch Automat Sci & Engn, Guangzhou 510641, Guangdong, Peoples R China.
EM guile.wu.cn@ieee.org; auwxkang@scut.edu.cn
OI Kang, Wenxiong/0000-0001-9023-7252
CR [Anonymous], P IEEE INT S APPL CO
   [Anonymous], 2011, THESIS
   [Anonymous], P IAPR C MACH VIS AP
   [Anonymous], P 1 INT C ADV DAT IN
   Argyros AA, 2006, INT C PATT RECOG, P207
   Argyros AA, 2006, LECT NOTES COMPUT SC, V3979, P40
   Baker S, 2011, INT J COMPUT VISION, V92, P1, DOI 10.1007/s11263-010-0390-2
   Bianco S, 2015, IEEE T IMAGE PROCESS, V24, P4756, DOI 10.1109/TIP.2015.2467209
   Crowley J., 1995, International Workshop on Gesture and Face Recognition, P195
   Dominguez SM, 2006, IEEE T MULTIMEDIA, V8, P956, DOI 10.1109/TMM.2006.879872
   Dorfmüller-Ulhaas K, 2001, IEEE AND ACM INTERNATIONAL SYMPOSIUM ON AUGMENTED REALITY, PROCEEDINGS, P55, DOI 10.1109/ISAR.2001.970515
   Gurav RM, 2015, 2015 INTERNATIONAL CONFERENCE ON INDUSTRIAL INSTRUMENTATION AND CONTROL (ICIC), P974, DOI 10.1109/IIC.2015.7150886
   Jones MJ, 2002, INT J COMPUT VISION, V46, P81, DOI 10.1023/A:1013200319198
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Kawulok M, 2014, PATTERN RECOGN LETT, V41, P3, DOI 10.1016/j.patrec.2013.08.028
   Kulshreshth A, 2013, 2013 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P187, DOI 10.1109/3DUI.2013.6550241
   Lee D, 2011, ETRI J, V33, P415, DOI 10.4218/etrij.11.0110.0313
   Li KP, 2014, 2014 7TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING (CISP 2014), P457, DOI 10.1109/CISP.2014.7003824
   Malima A., 2006, PROC 14 IEEE C SIGNA, P1
   Minotto VP, 2015, IEEE T MULTIMEDIA, V17, P1694, DOI 10.1109/TMM.2015.2463722
   Mooser J., 2007, P IEEEACM INT S MIXE, P145
   Nakamura T, 2008, LECT NOTES COMPUT SC, V5068, P292
   Nguyen DD, 2009, 2008 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS, VOLS 1-4, P324
   Oikonomidis I., 2010, Asian Conference on Computer Vision, P744, DOI DOI 10.1007/978-3-642-19318-7_58
   Qian C, 2014, PROC CVPR IEEE, P1106, DOI 10.1109/CVPR.2014.145
   Rautaray SS, 2015, ARTIF INTELL REV, V43, P1, DOI 10.1007/s10462-012-9356-9
   Reale MJ, 2011, IEEE T MULTIMEDIA, V13, P474, DOI 10.1109/TMM.2011.2120600
   Ren Z, 2011, PROC FL STATE HORTIC, V124, P1
   Segen J, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 3, P188, DOI 10.1109/ICIP.1998.727164
   Sridhar S, 2015, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2015.7298941
   Sridhar S, 2013, IEEE I CONF COMP VIS, P2456, DOI 10.1109/ICCV.2013.305
   Tong-de Tan, 2011, 2011 IEEE International Symposium on VR Innovation (ISVRI), P311, DOI 10.1109/ISVRI.2011.5759657
   Wang C, 2015, IEEE T MULTIMEDIA, V17, P29, DOI 10.1109/TMM.2014.2374357
   Wang CY, 2014, LECT NOTES COMPUT SC, V8588, P619, DOI 10.1007/978-3-319-09333-8_67
   Wu GL, 2016, IEEE T MULTIMEDIA, V18, P978, DOI 10.1109/TMM.2016.2545401
   Ye Z., 2013, PROC IEEE INT C MULT, P1
   Zen G, 2016, IEEE T MULTIMEDIA, V18, P775, DOI 10.1109/TMM.2016.2523421
   Zhang KH, 2014, LECT NOTES COMPUT SC, V8693, P127, DOI 10.1007/978-3-319-10602-1_9
NR 38
TC 22
Z9 22
U1 1
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2017
VL 19
IS 8
BP 1730
EP 1741
DI 10.1109/TMM.2017.2691538
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5VV
UT WOS:000411240400004
DA 2024-07-18
ER

PT J
AU Rizzi, A
   Antonelli, M
   Luzi, M
AF Rizzi, Antonello
   Antonelli, Mario
   Luzi, Massimiliano
TI Instrument Learning and Sparse NMD for Automatic Polyphonic Music
   Transcription
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Automatic music transcription (AMT); non-negative matrix decomposition
   (NMD); non-monotone optimization; spectrogram factorization; sparse
   coding
ID PITCH ESTIMATION; PIANO MUSIC; SIGNALS; MODEL; RETRIEVAL; TRANSFORM
AB In this paper, an automatic music transcription (AMT) algorithm based on a supervised non-negative matrix decomposition (NMD) is discussed. In particular, a novel approach for enhancing the sparsity of the solution is proposed. It consists of a two-step processing in which the NMD is solved joining a l(2) regularization and a threshold filtering. In the first step, the NMD is performed with the l(2) regularization in order to get an overall selection of the notes most likely appearing in the monotimbral musical excerpt. In the second step, a threshold filtering followed by another l(2) regularized NMD are repeatedly performed in order to progressively reduce the dictionary matrix and to refine the notes transcription. Furthermore, a user-oriented instrument learning procedure has been conceived and proposed. The proposed AMT system has been tested upon the dataset collected by the LabROSA laboratories considering the transcription of three different pianos. Moreover, it has been validated through a comparison with a regularized NMD and with three open source AMT software. The results prove the effectiveness of the proposed two-step processing in enhancing the sparsity of the solution and in improving the transcription accuracy. Moreover, the proposed system shows promising performance in both multi-F0 and note tracking tasks, obtaining better transcription accuracy than the competing algorithms in most tests.
C1 [Rizzi, Antonello; Antonelli, Mario; Luzi, Massimiliano] Univ Roma La Sapienza, Dept Informat Engn Elect & Telecommun, I-00184 Rome, Italy.
C3 Sapienza University Rome
RP Rizzi, A (corresponding author), Univ Roma La Sapienza, Dept Informat Engn Elect & Telecommun, I-00184 Rome, Italy.
EM antonello.rizzi@uniroma1.it; mario.antonelli@gmail.com;
   massimiliano.luzi@uniroma1.it
RI Rizzi, Antonello/D-4961-2014
OI Rizzi, Antonello/0000-0001-8244-0015; Luzi,
   Massimiliano/0000-0003-4853-3001
CR Adalbjörnsson SI, 2015, SIGNAL PROCESS, V109, P236, DOI 10.1016/j.sigpro.2014.10.014
   [Anonymous], 2013, VIOLIN TEST SET
   [Anonymous], 2007, EURASIP J ADV SIG PR
   [Anonymous], 2004, P INT SOC MUS INF RE
   Antonelli Mario, 2007, 2007 IEEE Workshop on Machine Learning for Signal Processing, P372, DOI 10.1109/MLSP.2007.4414335
   Antonelli M., 2017, NONMONOTONE PROJECTE
   Bartesekas B., 1999, NONLINEAR PROGRAMMIN
   BELLO JP, 2002, P 112 CONV AUD ENG S
   Benetos E, 2013, J INTELL INF SYST, V41, P407, DOI 10.1007/s10844-013-0258-3
   Benetos E, 2013, J ACOUST SOC AM, V133, P1727, DOI 10.1121/1.4790351
   Benetos E, 2012, COMPUT MUSIC J, V36, P81, DOI 10.1162/COMJ_a_00146
   Benetos Emmanouil., 2015, Proc. International Computer Music Conference, P701
   Birgin EG, 2000, SIAM J OPTIMIZ, V10, P1196, DOI 10.1137/S1052623497330963
   Brown J. C., 1992, J ACOUST SOC AM, V92
   BROWN JC, 1991, J ACOUST SOC AM, V89, P425, DOI 10.1121/1.400476
   BROWN JC, 1992, J ACOUST SOC AM, V92, P2698, DOI 10.1121/1.404385
   BROWN JC, 1991, J ACOUST SOC AM, V89, P2346, DOI 10.1121/1.400923
   Clausen M, 2004, IEEE T MULTIMEDIA, V6, P717, DOI 10.1109/TMM.2004.834859
   Conn A.R., 2010, LANCELOT FORTRAN PAC
   Davy M, 2006, J ACOUST SOC AM, V119, P2498, DOI 10.1121/1.2168548
   De Mulder T, 2006, IEEE T MULTIMEDIA, V8, P728, DOI 10.1109/TMM.2006.876291
   Dessein A., 2010, 11 INT SOC MUS INF R, P489
   Dixon S., 2000, COMPUTER RECOGNITION, P31
   Genussov M, 2013, DIGIT SIGNAL PROCESS, V23, P390, DOI 10.1016/j.dsp.2012.08.012
   Grindlay G, 2011, IEEE J-STSP, V5, P1159, DOI 10.1109/JSTSP.2011.2162395
   Klapuri A, 2008, IEEE T AUDIO SPEECH, V16, P255, DOI 10.1109/TASL.2007.908129
   Klapuri AP, 2005, IEEE WORK APPL SIG, P291, DOI 10.1109/ASPAA.2005.1540227
   LabROSA, 2006, AUT PIAN TRANSCR
   Lee CT, 2012, IEEE T MULTIMEDIA, V14, P608, DOI 10.1109/TMM.2012.2191398
   Marolt M, 2004, IEEE T MULTIMEDIA, V6, P439, DOI 10.1109/TMM.2004.827507
   Moorer J.A., 1977, COMPUT MUSIC J, V1, P32
   O'Hanlon Ken, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P3112, DOI 10.1109/ICASSP.2014.6854173
   O'Hanlon K, 2016, IEEE-ACM T AUDIO SPE, V24, P530, DOI 10.1109/TASLP.2016.2515514
   Patrick Daniel, 1996, THESIS, P2
   Piszczalski M., 1977, Computer Music Journal, V1, P24
   Rossi L, 1997, ACUSTICA, V83, P1077
   Ryynänen MP, 2005, IEEE WORK APPL SIG, P319, DOI 10.1109/ASPAA.2005.1540233
   SLANEY M, 1990, INT CONF ACOUST SPEE, P357, DOI 10.1109/ICASSP.1990.115684
   Smaragdis P, 2003, 2003 IEEE WORKSHOP ON APPLICATIONS OF SIGNAL PROCESSING TO AUDIO AND ACOUSTICS PROCEEDINGS, P177, DOI 10.1109/ASPAA.2003.1285860
   Umapathy K, 2005, IEEE T MULTIMEDIA, V7, P308, DOI 10.1109/TMM.2005.843363
   Vincent E, 2010, IEEE T AUDIO SPEECH, V18, P528, DOI 10.1109/TASL.2009.2034186
   Yoshii K, 2012, IEEE T AUDIO SPEECH, V20, P717, DOI 10.1109/TASL.2011.2164530
   Youngberg J. E., 1978, Proceedings of the 1978 IEEE International Conference on Acoustics, Speech and Signal Processing, P375
   Yu HM, 2008, IEEE T MULTIMEDIA, V10, P1626, DOI 10.1109/TMM.2008.2007345
   Yu Y, 2013, IEEE T MULTIMEDIA, V15, P1969, DOI 10.1109/TMM.2013.2269313
NR 45
TC 9
Z9 11
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2017
VL 19
IS 7
BP 1405
EP 1415
DI 10.1109/TMM.2017.2674603
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5ZN
UT WOS:000404061800001
DA 2024-07-18
ER

PT J
AU Wang, RG
   Luo, JJ
   Jiang, XB
   Wang, ZY
   Wang, WM
   Li, G
   Gao, W
AF Wang, Ronggang
   Luo, Jiajia
   Jiang, Xiubao
   Wang, Zhenyu
   Wang, Wenmin
   Li, Ge
   Gao, Wen
TI Accelerating Image-Domain-Warping Virtual View Synthesis on GPGPU
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE General-purpose computing on a graphics processing unit (GPGPU);
   image-domain-warping (IDW); real-time; stereo 3D (S3D) to N-view;
   virtual view synthesis
ID DEPTH
AB The image-domain-warping (IDW) method can effectively create high-quality virtual views. However, the IDW algorithm is very complex, and the software implementation for this method is far from real-time. In this paper, we propose an IDW-based view synthesis acceleration method on general-purpose computing on a graphics processing unit (GPGPU). Our method makes two main contributions. First, at the algorithm level, we employ FAST for sparse disparity estimation and adopt the successive over-relaxation iterative method to calculate warps. Second, at the platform level, two computation-intensive modules (data extraction and view synthesis) in IDW are offloaded to GPU using efficient data-level parallelism strategies. Experimental results demonstrate that our proposed acceleration method can speed up the original IDW algorithm by more than 110x, and HD stereo three-dimensional video can be converted to 8-view 4 K video (each view has an approximate 720P resolution) in real-time on a hybrid CPU + GPU (NVIDIA GTX980) platform.
C1 [Wang, Ronggang; Luo, Jiajia; Jiang, Xiubao; Wang, Zhenyu; Wang, Wenmin; Li, Ge; Gao, Wen] Peking Univ, Shenzhen Grad Sch, Sch Elect & Comp Engn, Shenzhen 518055, Peoples R China.
C3 Peking University
RP Wang, RG (corresponding author), Peking Univ, Shenzhen Grad Sch, Sch Elect & Comp Engn, Shenzhen 518055, Peoples R China.
EM rgwang@pkusz.edu.cn; abbyluo@sz.pku.edu.cn; xbjiang@pku.edu.cn;
   wangzhenyu@pkusz.edu.cn; wangwm@pkusz.edu.cn; gli@pkusz.edu.cn;
   wgao@pku.edu.cn
RI Wang, Wenmin/W-3511-2019
OI Wang, Wenmin/0000-0003-2664-4413
FU National Natural Science Foundation of China [61672063, 61370115]; China
   863 project [2015AA015905]; Shenzhen Peacock Plan; Shenzhen Research
   Projects [JCYJ20150331100658943, JCYJ20160506172227337]; Guangdong
   Province [2014B010117007, 2014B090910001]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61672063 and Grant 61370115, in part by
   the China 863 project of 2015AA015905, in part by the Shenzhen Peacock
   Plan, Shenzhen Research Projects of JCYJ20150331100658943 and
   JCYJ20160506172227337, and in part by the Guangdong Province Projects of
   2014B010117007 and 2014B090910001. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Leonel Sousa. (Ronggang Wang and Jiajia Luo contributed equally to this
   work.)
CR [Anonymous], 2004, INT SOC OPTICS PHOTO, DOI DOI 10.1117/12.524762
   [Anonymous], 2015, J REAL-TIME IMAGE PR
   [Anonymous], 2010, ACM T GRAPH P SIGGRA
   Calonder M, 2012, IEEE T PATTERN ANAL, V34, P1281, DOI 10.1109/TPAMI.2011.222
   Chaurasia G, 2011, COMPUT GRAPH FORUM, V30, P1223, DOI 10.1111/j.1467-8659.2011.01981.x
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   Farre Miquel., 2011, Multimedia and Expo (ICME), 2011 IEEE International Conference on, P1
   Gelman A, 2012, IEEE T IMAGE PROCESS, V21, P4092, DOI 10.1109/TIP.2012.2201490
   Ilkoo Ahn, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P109, DOI 10.1109/ICME.2012.95
   Jianbin Fang, 2011, 2011 International Conference on Parallel Processing, P216, DOI 10.1109/ICPP.2011.45
   Kuo P.C., 2016, MULTIDIMENSIONAL SYS, V27, P1
   Liu F, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531350
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Momcilovic S, 2014, IEEE T MULTIMEDIA, V16, P108, DOI 10.1109/TMM.2013.2284892
   Rosten E, 2006, LECT NOTES COMPUT SC, V3951, P430, DOI 10.1007/11744023_34
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Scarpino Matthew., 2012, OPENCL ACTION ACCELE
   Schaffner Michael, 2013, 2013 Proceedings of the ESSCIRC. 39th European Solid State Circuits Conference (ESSCIRC), P61, DOI 10.1109/ESSCIRC.2013.6649072
   Schwalb M, 2009, IEEE T MULTIMEDIA, V11, P1, DOI 10.1109/TMM.2008.2008873
   Smolic A, 2011, P IEEE, V99, P607, DOI 10.1109/JPROC.2010.2098350
   Smolic A, 2008, IEEE IMAGE PROC, P2448, DOI 10.1109/ICIP.2008.4712288
   Stefanoski N, 2013, IEEE T IMAGE PROCESS, V22, P3329, DOI 10.1109/TIP.2013.2264817
   van de Sande KEA, 2011, IEEE T MULTIMEDIA, V13, P60, DOI 10.1109/TMM.2010.2091400
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
NR 24
TC 7
Z9 7
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2017
VL 19
IS 6
BP 1392
EP 1400
DI 10.1109/TMM.2017.2654120
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5YS
UT WOS:000404059400022
DA 2024-07-18
ER

PT J
AU Zhao, SC
   Yao, HX
   Gao, Y
   Ji, RR
   Ding, GG
AF Zhao, Sicheng
   Yao, Hongxun
   Gao, Yue
   Ji, Rongrong
   Ding, Guiguang
TI Continuous Probability Distribution Prediction of Image Emotions via
   Multitask Shared Sparse Regression
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Gaussian mixture model; image emotion; multitask learning; probability
   distribution; valence-arousal; shared sparse regression (SSR)
ID RECOGNITION; CLASSIFICATION; VALENCE; SYSTEM
AB Previous works on image emotion analysis mainly focused on predicting the dominant emotion category or the average dimension values of an image for affective image classification and regression. However, this is often insufficient in various real-world applications, as the emotions that are evoked in viewers by an image are highly subjective and different. In this paper, we propose to predict the continuous probability distribution of image emotions which are represented in dimensional valence-arousal space. We carried out large-scale statistical analysis on the constructed Image-Emotion-Social-Net dataset, on which we observed that the emotion distribution can be well-modeled by a Gaussian mixture model. This model is estimated by an expectation-maximization algorithm with specified initializations. Then, we extract commonly used emotion features at different levels for each image. Finally, we formalize the emotion distribution prediction task as a shared sparse regression (SSR) problem and extend it to multitask settings, named multitask shared sparse regression (MTSSR),to explore the latent information between different prediction tasks. SSR and MTSSR are optimized by iteratively reweighted least squares. Experiments are conducted on the Image-Emotion-Social-Net dataset with comparisons to three alternative baselines. The quantitative results demonstrate the superiority of the proposed method.
C1 [Zhao, Sicheng; Gao, Yue; Ding, Guiguang] Tsinghua Univ, Sch Software, Beijing 100084, Peoples R China.
   [Yao, Hongxun] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Peoples R China.
   [Ji, Rongrong] Xiamen Univ, Dept Cognit Sci, Sch Informat Sci & Engn, Xiamen 361005, Peoples R China.
C3 Tsinghua University; Harbin Institute of Technology; Xiamen University
RP Ding, GG (corresponding author), Tsinghua Univ, Sch Software, Beijing 100084, Peoples R China.
EM schzhao@gmail.com; h.yao@hit.edu.cn; gaoyue@tsinghua.edu.cn;
   rrji@xmu.edu.cn; dinggg@tsinghua.edu.cn
RI Ding, Guiguang/KIL-3528-2024; Gao, Yue/B-3376-2012
FU National Natural Science Foundation of China [61571269, 61271394,
   61472103, 61133003, 61671267, 61422210, 61373076]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61571269, Grant 61271394, Grant 61472103, Grant
   61133003, Grant 61671267, Grant 61422210, and Grant 61373076.
CR Ando RK, 2005, J MACH LEARN RES, V6, P1817
   [Anonymous], 2012, P 20 ACM INT C MULT
   [Anonymous], 2012, MALSAR: Multi-tAsk Learning via StructurAl Regularization
   [Anonymous], 2012, P 20 ACM INT C MULT
   [Anonymous], 2012, ACM T INTEL SYST TEC, DOI DOI 10.1145/2168752.2168754
   Argyriou A., 2007, NIPS, P41
   Benini S, 2011, IEEE T MULTIMEDIA, V13, P1356, DOI 10.1109/TMM.2011.2163058
   Bickel S., 2008, P 25 INT C MACH LEAR, P56
   Bo Pang, 2008, Foundations and Trends in Information Retrieval, V2, P1, DOI 10.1561/1500000001
   Borth D., 2013, P 21 ACM INT C MULTI, P223, DOI 10.1145/2502081.2502282
   Boureau YL, 2010, PROC CVPR IEEE, P2559, DOI 10.1109/CVPR.2010.5539963
   Carney M., 2005, International Conference on Machine Learning, P113
   Chartrand R, 2008, INT CONF ACOUST SPEE, P3869, DOI 10.1109/ICASSP.2008.4518498
   Chen C, 2014, PROC CVPR IEEE, P2713, DOI 10.1109/CVPR.2014.353
   Chen PC, 2013, IEEE T MULTIMEDIA, V15, P1469, DOI 10.1109/TMM.2013.2267206
   CHEN T, 2014, PROC ACM INT CONF, P367
   Ding GG, 2014, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2014.267
   Gong Pinghua, 2012, KDD, V2012, P895
   Guo YC, 2015, AAAI CONF ARTIF INTE, P3783
   Hanjalic A, 2006, IEEE SIGNAL PROC MAG, V23, P90, DOI 10.1109/MSP.2006.1621452
   Hanjalic A, 2005, IEEE T MULTIMEDIA, V7, P143, DOI 10.1109/TMM.2004.840618
   Happy SL, 2015, IEEE T AFFECT COMPUT, V6, P1, DOI 10.1109/TAFFC.2014.2386334
   Joshi D, 2011, IEEE SIGNAL PROC MAG, V28, P94, DOI 10.1109/MSP.2011.941851
   Kleinsmith A, 2013, IEEE T AFFECT COMPUT, V4, P15, DOI 10.1109/T-AFFC.2012.16
   Koolagudi SG, 2012, INT J SPEECH TECHNOL, V15, P99, DOI 10.1007/s10772-011-9125-1
   Liu HF, 2013, 2013 ASE/IEEE INTERNATIONAL CONFERENCE ON SOCIAL COMPUTING (SOCIALCOM), P385, DOI 10.1109/SocialCom.2013.60
   Lu Xin, 2012, Proc ACM Int Conf Multimed, V2012, P229, DOI 10.1145/2393347.2393384
   Lucey P., 2010, IEEE COMP SOC C COMP, P94, DOI 10.1109/CVPRW.2010.5543262
   Machajdik J., 2010, P 18 ACM INT C MULT, P83, DOI DOI 10.1145/1873951.1873965
   Mao QR, 2014, IEEE T MULTIMEDIA, V16, P2203, DOI 10.1109/TMM.2014.2360798
   Mikels JA, 2005, BEHAV RES METHODS, V37, P626, DOI 10.3758/BF03192732
   Patterson G, 2012, PROC CVPR IEEE, P2751, DOI 10.1109/CVPR.2012.6247998
   Peng KC, 2015, PROC CVPR IEEE, P860, DOI 10.1109/CVPR.2015.7298687
   Pipa G, 2013, NEURAL COMPUT, V25, P1123, DOI 10.1162/NECO_a_00432
   Sandbach G, 2012, IMAGE VISION COMPUT, V30, P683, DOI 10.1016/j.imavis.2012.06.005
   Scherer KR, 2005, SOC SCI INFORM, V44, P695, DOI 10.1177/0539018405058216
   SCHLOSBERG H, 1954, PSYCHOL REV, V61, P81, DOI 10.1037/h0054570
   Shepstone SE, 2014, IEEE T MULTIMEDIA, V16, P1999, DOI 10.1109/TMM.2014.2337845
   Siersdorfer S., 2010, ACM MM, P715
   Soleymani M, 2014, IEEE T MULTIMEDIA, V16, P1075, DOI 10.1109/TMM.2014.2305573
   Sun ZQ, 2014, PROCEEDINGS OF THE ASME 4TH INTERNATIONAL CONFERENCE ON MICRO/NANOSCALE HEAT AND MASS TRANSFER - 2013
   Tang J, 2012, IEEE T AFFECT COMPUT, V3, P132, DOI 10.1109/T-AFFC.2011.23
   Tkalcic M, 2013, IEEE T MULTIMEDIA, V15, P391, DOI 10.1109/TMM.2012.2229970
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wang H, 2011, IEEE I CONF COMP VIS, P557, DOI 10.1109/ICCV.2011.6126288
   Wang SF, 2015, IEEE T AFFECT COMPUT, V6, P410, DOI 10.1109/TAFFC.2015.2432791
   Warriner AB, 2013, BEHAV RES METHODS, V45, P1191, DOI 10.3758/s13428-012-0314-x
   Wei-Ning W, 2006, IEEE SYS MAN CYBERN, P3534, DOI 10.1109/ICSMC.2006.384667
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Wu CH, 2013, IEEE T MULTIMEDIA, V15, P1732, DOI 10.1109/TMM.2013.2272917
   Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970
   Yadati K, 2014, IEEE T MULTIMEDIA, V16, P15, DOI 10.1109/TMM.2013.2282128
   Yang P, 2010, PROC CVPR IEEE, P2638, DOI 10.1109/CVPR.2010.5539978
   Yang Y., 2013, P 21 ACM INT C MULT, P785, DOI DOI 10.1145/2502081.2502204
   Yang Y, 2014, AAAI CONF ARTIF INTE, P306
   Yang YH, 2013, IEEE T MULTIMEDIA, V15, P1304, DOI 10.1109/TMM.2013.2265078
   Yanulevskaya V, 2008, IEEE IMAGE PROC, P101, DOI 10.1109/ICIP.2008.4711701
   Yu FLX, 2013, PROC CVPR IEEE, P771, DOI 10.1109/CVPR.2013.105
   Yuan J., 2013, Proceedings of the Second International Workshop on Issues of Sentiment Discovery and Opinion Mining, page, P10
   Yuan XT, 2012, IEEE T IMAGE PROCESS, V21, P4349, DOI 10.1109/TIP.2012.2205006
   Yue Gao, 2015, MultiMedia Modeling. 21st International Conference, MMM 2015, January 5-7, 2015, Proceedings: LNCS 8935, P269, DOI 10.1007/978-3-319-14445-0_24
   Zhang HW, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P187, DOI 10.1145/2647868.2654915
   Zhao S., 2013, INT C MULT MOD MMM 2, P7732, DOI DOI 10.1007/978-3-642-35725-1_34
   Zhao SC, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P879, DOI 10.1145/2733373.2806354
   Zhao SC, 2015, IEEE IMAGE PROC, P2459, DOI 10.1109/ICIP.2015.7351244
   Zhao SC, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P47, DOI 10.1145/2647868.2654930
   Zhao SC, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1025, DOI 10.1145/2647868.2655035
   Zhao SC, 2013, NEUROCOMPUTING, V119, P101, DOI 10.1016/j.neucom.2012.04.042
   Zhao Sicheng, 2011, P 19 ACM INT C MULTI, P1473, DOI [10.1145/2072298.2072043, DOI 10.1145/2072298.2072043]
   Zhao Sicheng, 2016, P 24 ACM INT C MULT, P1385, DOI DOI 10.1145/2964284.2964289
   Zhao W, 2014, INT CONF PERVAS COMP, P1
   Zheng SF, 2010, COMPUT VIS IMAGE UND, V114, P1055, DOI 10.1016/j.cviu.2010.07.004
NR 72
TC 153
Z9 154
U1 2
U2 62
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2017
VL 19
IS 3
BP 632
EP 645
DI 10.1109/TMM.2016.2617741
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA EN2VW
UT WOS:000395869400017
DA 2024-07-18
ER

PT J
AU Wu, YR
   Lu, T
   Yuan, ZH
   Wang, H
AF Wu, Yirui
   Lu, Tong
   Yuan, Zehuan
   Wang, Hao
TI FreeScup: A Novel Platform for Assisting Sculpture Pose Design
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Artistic design; FreeScup; pose editing; sculpture; spectral
ID MESH; MODEL
AB Sculpture design is challenging due to its inherent difficulty in characterizing artworks quantitatively; thus, few works have been done to assist sculpture design in the past decades in the multimedia community. We have cooperated with several sculptors on analyzing styles of different artists consisting of Giacometti, Augeuste Rodin, Henry Moore, and Marino Marini from which we find pose editing plays an important role in sculpture design. Motivated by this, we present a novel platform that allows sculptors to edit virtual three-dimensional (3-D) sculptures by a free way. The proposed platform consists of three modules, namely, sculpture initialization, sculptor-sculpture mapping, and interactive pose editing. In sculpture initialization, a virtual 3-D sculpture is first incrementally reconstructed from multiview images. Then, we define Laplace operator and its corresponding spectrum to describe the geometry information of the reconstructed sculpture. During sculptor-sculpture mapping, we apply spectral analysis on the low-frequency parts of the spectrum to search for candidate editing points on the surface of the sculpture. Next, body actions of the sculptor are captured by Kinect and further mapped onto editing points as a predefined configuration set. Finally, during interactive pose editing, a real-time Kinect-driven sculpture pose editing scheme is presented, which not only preserves geometry features of the sculpture but also allows instant changes of sculpture poses. We demonstrate that our platform successfully assists sculptors on real-time pose editing by comparing its performance with those of the existing sculpture assisting methods.
C1 [Wu, Yirui; Lu, Tong; Yuan, Zehuan] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210046, Jiangsu, Peoples R China.
   [Wang, Hao] Inst Deep Learning, Beijing 100085, Peoples R China.
C3 Nanjing University
RP Wu, YR (corresponding author), Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210046, Jiangsu, Peoples R China.
EM wuyirui1989@163.com; lutong@nju.edu.cn; zhyuan001@gmail.com;
   wanghao0711@gmail.com
RI Wu, Yirui/HGD-2965-2022
OI Wu, Yirui/0000-0003-3022-3718
FU Natural Science Foundation of China [61672273, 61272218, 61321491];
   Science Foundation for Distinguished Young Scholars of Jiangsu
   [BK20160021]
FX This work was supported by the Natural Science Foundation of China under
   Grant 61672273, Grant 61272218, and Grant 61321491, and by the Science
   Foundation for Distinguished Young Scholars of Jiangsu under Grant
   BK20160021. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Sen-Ching Samson
   Cheung.
CR Akagi Y., 2013, P IEEE INT C MULT EX, V318, P1
   Alcoverro M, 2008, 3DTV CONF, P373, DOI 10.1109/3DTV.2008.4547891
   Alexiadis DS, 2013, IEEE T MULTIMEDIA, V15, P339, DOI 10.1109/TMM.2012.2229264
   [Anonymous], 2013, P 12 ACM SIGGRAPHEUR, DOI DOI 10.1145/2485895.2485903
   [Anonymous], ACM T GRAPH
   [Anonymous], 2007, COMPUTER AIDED DESIG
   Au OKC, 2006, IEEE T VIS COMPUT GR, V12, P386, DOI 10.1109/TVCG.2006.47
   Au OKC, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239534, 10.1145/1276377.1276481]
   Barmpoutis A, 2015, LECT NOTES COMPUT SC, V9178, P3, DOI 10.1007/978-3-319-20687-5_1
   Bickel B, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778800
   Chen D, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461994
   Curless B, 1999, COMPUT GRAPHICS-US, V33, P38, DOI 10.1145/345370.345399
   Fosh Lesley., 2013, PROC CHI 13, P149, DOI DOI 10.1145/2470654.2470675
   Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161
   Gal R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531339
   García FG, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487232
   Guo YL, 2014, IEEE T MULTIMEDIA, V16, P1377, DOI 10.1109/TMM.2014.2316145
   Han JG, 2013, IEEE T CYBERNETICS, V43, P1318, DOI 10.1109/TCYB.2013.2265378
   Held RT, 2012, UIST'12: PROCEEDINGS OF THE 25TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P423
   Huang J, 2006, ACM T GRAPHIC, V25, P1126, DOI 10.1145/1141911.1142003
   Izadi S, 2011, P UIST, P559, DOI DOI 10.1145/2047196.2047270
   Ji ZP, 2010, COMPUT GRAPH FORUM, V29, P2169, DOI 10.1111/j.1467-8659.2010.01805.x
   Karni Z, 2000, COMP GRAPH, P279, DOI 10.1145/344779.344924
   Kowdle A, 2012, LECT NOTES COMPUT SC, V7576, P789, DOI 10.1007/978-3-642-33715-4_57
   Nealen A., 2006, P 4 INT C COMP GRAPH, P381, DOI DOI 10.1145/1174429.1174494
   Phamduy P, 2015, IEEE T MULTIMEDIA, V17, P2328, DOI 10.1109/TMM.2015.2480226
   Philipp-Foliguet S, 2011, PATTERN RECOGN, V44, P588, DOI 10.1016/j.patcog.2010.09.016
   Remondino F, 2006, PHOTOGRAMM REC, V21, P269, DOI 10.1111/j.1477-9730.2006.00383.x
   Reuter M, 2009, COMPUT AIDED DESIGN, V41, P739, DOI 10.1016/j.cad.2009.02.007
   Rubio JC, 2012, PROC CVPR IEEE, P749, DOI 10.1109/CVPR.2012.6247745
   Sequin C. H., 1998, P BRIDGES 98 C WINF, P1
   Séquin CH, 2005, COMMUN ACM, V48, P66, DOI 10.1145/1064830.1064860
   Séquin CH, 2001, VSMM 2001: SEVENTH INTERNATIONAL CONFERENCE ON VIRTUAL SYSTEMS AND MULTIMEDIA, PROCEEDINGS, P832
   Shi XH, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239532
   Shiratori T, 2013, 2013 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P59, DOI 10.1109/3DUI.2013.6550198
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Sorkine O., 2007, As-rigid-as-possible surface modeling, P109, DOI 10.1145/1281991.1282006
   Vaezi M, 2013, INT J ADV MANUF TECH, V67, P1721, DOI 10.1007/s00170-012-4605-2
   Vanek J, 2014, COMPUT GRAPH FORUM, V33, P322, DOI 10.1111/cgf.12353
   von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z
   Wu YR, 2015, COMPUT AIDED GEOM D, V35-36, P56, DOI 10.1016/j.cagd.2015.03.001
   Yoshizaki W, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P637
   Yu YZ, 2004, ACM T GRAPHIC, V23, P644, DOI 10.1145/1015706.1015774
   Yuan ZH, 2013, APPL INTELL, V39, P761, DOI 10.1007/s10489-012-0410-8
NR 44
TC 5
Z9 6
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2017
VL 19
IS 1
BP 183
EP 195
DI 10.1109/TMM.2016.2609407
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EH0SX
UT WOS:000391475200015
DA 2024-07-18
ER

PT J
AU Wu, YC
   Cao, N
   Gotz, D
   Tan, YP
   Keim, DA
AF Wu, Yingcai
   Cao, Nan
   Gotz, David
   Tan, Yap-Peng
   Keim, Daniel A.
TI A Survey on Visual Analytics of Social Media Data
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Social media data; visual analytics; visualization
ID NETWORK ANALYSIS; VISUALIZATION; DIFFUSION; BEHAVIOR
AB The unprecedented availability of social media data offers substantial opportunities for data owners, system operators, solution providers, and end users to explore and understand social dynamics. However, the exponential growth in the volume, velocity, and variability of social media data prevents people from fully utilizing such data. Visual analytics, which is an emerging research direction, has received considerable attention in recent years. Many visual analytics methods have been proposed across disciplines to understand large-scale structured and unstructured social media data. This objective, however, also poses significant challenges for researchers to obtain a comprehensive picture of the area, understand research challenges, and develop new techniques. In this paper, we present a comprehensive survey to characterize this fast-growing area and summarize the state-of-the-art techniques for analyzing social media data. In particular, we classify existing techniques into two categories: gathering information and understanding user behaviors. We aim to provide a clear overview of the research area through the established taxonomy. We then explore the design space and identify the research trends. Finally, we discuss challenges and open questions for future studies.
C1 [Wu, Yingcai] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Zhejiang, Peoples R China.
   [Cao, Nan] NYU, Tandon Sch Engn, New York, NY 11201 USA.
   [Cao, Nan] New York Univ ShangHai, Pudong 200122, Peoples R China.
   [Gotz, David] Univ N Carolina, Chapel Hill, NC 27514 USA.
   [Tan, Yap-Peng] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore, Singapore.
   [Keim, Daniel A.] Univ Konstanz, D-78464 Constance, Germany.
C3 Zhejiang University; New York University; New York University Tandon
   School of Engineering; University of North Carolina; University of North
   Carolina Chapel Hill; Nanyang Technological University; University of
   Konstanz
RP Wu, YC (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Zhejiang, Peoples R China.
EM ycwu@zju.edu.cn; nan.cao@nyu.edu; gotz@unc.edu; eyptan@ntu.edu.sg;
   Daniel.Keim@uni-konstanz.de
RI Keim, Daniel/X-7749-2019; Cao, Nan/O-5397-2014; Tan,
   Yap-Peng/A-5158-2011
OI Keim, Daniel/0000-0001-7966-9740; 
FU National 973 Program of China [2015CB352503]; Fundamental Research Funds
   for Central Universities [2016QNA5014]; National Science Foundation of
   China [61502416, 61602306]; Ministry of Education of China
   [188170-170160502]; 100 Talents Program of Zhejiang University;
   Microsoft Research Asia; National Science Foundation [DMS-1557593]
FX The work was supported in part by the National 973 Program of China
   under Grant 2015CB352503 by the Fundamental Research Funds for Central
   Universities under Grant 2016QNA5014, the National Science Foundation of
   China under Grant 61502416 and Grant 61602306, by the research fund of
   the Ministry of Education of China under Grant 188170-170160502, by the
   100 Talents Program of Zhejiang University, and a grant from Microsoft
   Research Asia and is based in part upon work supported by the National
   Science Foundation under Grant DMS-1557593. The guest editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Tao Mei.
CR Abel F., 2012, Proceedings of the 21st international conference companion on World Wide Web, P305, DOI 10.1145/2187980.2188035
   Abello J, 2006, IEEE T VIS COMPUT GR, V12, P669, DOI 10.1109/TVCG.2006.120
   [Anonymous], 2004, P ACM C HUMAN FACTOR, DOI [DOI 10.1145/985692.985765, 10.1145/985692.985765]
   [Anonymous], 2010, ANAL SOCIAL MEDIA NE
   Archambault D., 2011, Proceedings of the Workshop on Search and Mining User-generated Contents, P1
   Bastian M., 2009, P INT AAAI C WEBL SO, V3, P361
   BERNSTEIN MICHAELS., 2010, Proceedings of the 23rd Annual ACM Symposium on User Interface Software and Technology, New York: ACM, P303
   Bian JW, 2015, IEEE T MULTIMEDIA, V17, P216, DOI 10.1109/TMM.2014.2384912
   Borgatti S.P., 2014, Encyclopedia of Social Network Analysis and Mining, P2261
   Bosch H, 2013, IEEE T VIS COMPUT GR, V19, P2022, DOI 10.1109/TVCG.2013.186
   Bostock M, 2011, IEEE T VIS COMPUT GR, V17, P2301, DOI 10.1109/TVCG.2011.185
   Cai HY, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P89, DOI 10.1145/2733373.2806236
   Cao N, 2016, IEEE COMPUT GRAPH, V36, P72, DOI 10.1109/MCG.2015.73
   Cao N, 2016, IEEE T VIS COMPUT GR, V22, P280, DOI 10.1109/TVCG.2015.2467196
   Cao N, 2015, J VISUAL-JAPAN, V18, P221, DOI 10.1007/s12650-014-0246-x
   Cao N, 2012, IEEE T VIS COMPUT GR, V18, P2649, DOI 10.1109/TVCG.2012.291
   Chae J, 2012, IEEE CONF VIS ANAL, P143, DOI 10.1109/VAST.2012.6400557
   Chen SM, 2016, IEEE T VIS COMPUT GR, V22, P270, DOI 10.1109/TVCG.2015.2467619
   CHEN T, 2014, PROC ACM INT CONF, P367
   Chi Y, 2009, IEEE T MULTIMEDIA, V11, P372, DOI 10.1109/TMM.2009.2012912
   Christakis Nicholas A, 2009, Nor Epidemiol, V19, P5
   Correa CD, 2011, SOCIAL NETWORK DATA ANALYTICS, P307
   Cui WW, 2008, IEEE T VIS COMPUT GR, V14, P1277, DOI 10.1109/TVCG.2008.135
   Diakopoulos N., 2010, 2010 Proceedings of IEEE Symposium on Visual Analytics Science and Technology (VAST 2010), P115, DOI 10.1109/VAST.2010.5652922
   Diakopoulos N., 2012, P SIGCHI C HUM FACT
   Dörk M, 2010, IEEE T VIS COMPUT GR, V16, P1129, DOI 10.1109/TVCG.2010.129
   Dou WW, 2012, IEEE CONF VIS ANAL, P93, DOI 10.1109/VAST.2012.6400485
   Dunne C., 2013, P SIGCHI C HUMAN FAC, P3247, DOI DOI 10.1145/2470654.2466444
   Elmqvist N, 2008, IEEE PACIFIC VISUALISATION SYMPOSIUM 2008, PROCEEDINGS, P215
   Fang Q, 2015, IEEE T MULTIMEDIA, V17, P1031, DOI 10.1109/TMM.2015.2430819
   Ghani S, 2013, IEEE T VIS COMPUT GR, V19, P2032, DOI 10.1109/TVCG.2013.223
   Gilbert E, 2009, IEEE T MULTIMEDIA, V11, P413, DOI 10.1109/TMM.2009.2012916
   Gloor Peter A., 2009, 2009 International Conference on Computational Science and Engineering (CSE), P215, DOI 10.1109/CSE.2009.186
   Heer J, 2005, INFOVIS 05: IEEE SYMPOSIUM ON INFORMATION VISUALIZATION, PROCEEDINGS, P33
   Henry N, 2007, LECT NOTES COMPUT SC, V4663, P288
   Henry N, 2007, IEEE T VIS COMPUT GR, V13, P1302, DOI 10.1109/TVCG.2007.70582
   Holten D, 2006, IEEE T VIS COMPUT GR, V12, P741, DOI 10.1109/TVCG.2006.147
   Hu Y., 2013, Proceedings of the 2013 ACM annual conference on Human factors in computing systems, P3481
   Huang SR, 2016, IEEE T MULTIMEDIA, V18, P287, DOI 10.1109/TMM.2015.2510333
   Jin L, 2013, IEEE COMMUN MAG, V51, P144, DOI 10.1109/MCOM.2013.6588663
   Jou B, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P159, DOI 10.1145/2733373.2806246
   Krueger R, 2016, IEEE PAC VIS SYMP, P176, DOI 10.1109/PACIFICVIS.2016.7465266
   Lei CY, 2016, IEEE T MULTIMEDIA, V18, P687, DOI 10.1109/TMM.2015.2477277
   Lin CY, 2009, PROC INT CONF DATA, P1483, DOI 10.1109/ICDE.2009.140
   Liu MC, 2016, IEEE T VIS COMPUT GR, V22, P250, DOI 10.1109/TVCG.2015.2467554
   Liu SX, 2016, IEEE T VIS COMPUT GR, V22, P2451, DOI 10.1109/TVCG.2015.2509990
   MacEachren A. M., 2011, 2011 IEEE Conference on Visual Analytics Science and Technology, P181, DOI 10.1109/VAST.2011.6102456
   Marcus A, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P227
   Mei T, 2014, ACM COMPUT SURV, V46, DOI 10.1145/2536798
   Negoescu RA, 2010, IEEE T MULTIMEDIA, V12, P399, DOI 10.1109/TMM.2010.2050649
   Niu GL, 2014, IEEE T MULTIMEDIA, V16, P2025, DOI 10.1109/TMM.2014.2340133
   Nooy Wouter., 2011, Exploratory social network analysis with Pajek, DOI [10.1017/CBO9780511996368, DOI 10.1017/CBO9780511996368]
   Pang JB, 2015, IEEE T MULTIMEDIA, V17, P843, DOI 10.1109/TMM.2015.2425143
   Perer A., 2011, 2011 IEEE Conference on Visual Analytics Science and Technology, P71, DOI 10.1109/VAST.2011.6102443
   Perer A, 2006, IEEE T VIS COMPUT GR, V12, P693, DOI 10.1109/TVCG.2006.122
   Qian SS, 2016, IEEE T MULTIMEDIA, V18, P233, DOI 10.1109/TMM.2015.2510329
   Roy SD, 2013, IEEE T MULTIMEDIA, V15, P1255, DOI 10.1109/TMM.2013.2265079
   Shi L, 2009, IEEE PAC VIS SYMP, P41, DOI 10.1109/PACIFICVIS.2009.4906836
   Shneiderman B, 2006, IEEE T VIS COMPUT GR, V12, P733, DOI 10.1109/TVCG.2006.166
   Sun GD, 2014, IEEE T VIS COMPUT GR, V20, P1753, DOI 10.1109/TVCG.2014.2346919
   Thomas J. J., 2005, Illuminating the Path: The Research and Development Agenda for Visual Analytics
   van Ham F, 2004, IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2004, PROCEEDINGS, P199, DOI 10.1109/INFVIS.2004.43
   Viegas F., 2013, P 22 INT C WORLD WID, P1389
   Wang X., IEEE T VIS IN PRESS
   Weng CY, 2009, IEEE T MULTIMEDIA, V11, P256, DOI 10.1109/TMM.2008.2009684
   Wu YC, 2014, IEEE T VIS COMPUT GR, V20, P1763, DOI 10.1109/TVCG.2014.2346920
   Xiong R., 1999, 99 UIST. Proceedings of the 12th Annual ACM Symposium on User Interface Software and Technology, P37, DOI 10.1145/320719.322581
   Xu PP, 2013, IEEE T VIS COMPUT GR, V19, P2012, DOI 10.1109/TVCG.2013.221
   Yang YH, 2013, IEEE T MULTIMEDIA, V15, P1304, DOI 10.1109/TMM.2013.2265078
   Zhao GS, 2016, IEEE T MULTIMEDIA, V18, P496, DOI 10.1109/TMM.2016.2515362
   Zhao J, 2014, IEEE T VIS COMPUT GR, V20, P1773, DOI 10.1109/TVCG.2014.2346922
   Zubiaga A., 2013, Proceedings of the 2013 International Conference on Intelligent User Interfaces, IUI '13, P213
NR 72
TC 84
Z9 89
U1 2
U2 63
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2016
VL 18
IS 11
BP 2135
EP 2148
DI 10.1109/TMM.2016.2614220
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EA9BX
UT WOS:000386936900002
OA Green Submitted, Green Published, Bronze
DA 2024-07-18
ER

PT J
AU Fiandrotti, A
   Gaeta, R
   Grangetto, M
AF Fiandrotti, Attilio
   Gaeta, Rossano
   Grangetto, Marco
TI Characterization of Band Codes for Pollution-Resilient Peer-to-Peer
   Video Streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Continuity index; measurements; network coding; peer to peer; pollution
   attack
AB We provide a comprehensive characterization of band codes (BC) as a resilient-by-design solution to pollution attacks in network coding (NC)-based peer-to-peer live video streaming. Consider one malicious node injecting bogus coded packets into the network: the recombinations at the nodes generate an avalanche of novel coded bogus packets. Therefore, the malicious node can cripple the communication by injecting into the network only a handful of polluted packets. Pollution attacks are typically addressed by identifying and isolating the malicious nodes from the network. Pollution detection is, however, not straightforward in NC as the nodes exchange coded packets. Similarly, malicious nodes identification is complicated by the ambiguity between malicious nodes and nodes that have involuntarily relayed polluted packets. This paper addresses pollution attacks through a radically different approach which relies on BCs. BCs are a family of rateless codes originally designed for controlling the NC decoding complexity in mobile applications. Here, we exploit BCs for the totally different purpose of recombining the packets at the nodes so to avoid that the pollution propagates by adaptively adjusting the coding parameters. Our streaming experiments show that BCs curb the propagation of the pollution and restore the quality of the distributed video stream.
C1 [Fiandrotti, Attilio] Sisvel Technol, R&D Dept, I-10060 None Torinese, Italy.
   [Gaeta, Rossano; Grangetto, Marco] Univ Turin, Dept Comp Sci, I-10149 Turin, Italy.
C3 University of Turin
RP Fiandrotti, A (corresponding author), Sisvel Technol, R&D Dept, I-10060 None Torinese, Italy.; Gaeta, R; Grangetto, M (corresponding author), Univ Turin, Dept Comp Sci, I-10149 Turin, Italy.
EM attilio.fiandrotti@gmail.com; rossano@di.unito.it;
   marco.grangetto@unito.it
RI Grangetto, Marco/D-1222-2010; GAETA, Rossano/C-6256-2011
OI Fiandrotti, Attilio/0000-0002-9991-6822; GAETA,
   Rossano/0000-0002-6521-403X
CR Bioglio V, 2009, IEEE COMMUN LETT, V13, P953, DOI 10.1109/LCOMM.2009.12.091824
   Dhungel P., 2007, P2P TV PROC WORKSHOP, P323
   Fiandrotti A, 2015, IEEE INT CON MULTI, DOI 10.1109/ICME.2015.7177408
   Fiandrotti A, 2015, IEEE T MULTIMEDIA, V17, P562, DOI 10.1109/TMM.2015.2402516
   Fiandrotti A, 2014, IEEE T MULTIMEDIA, V16, P521, DOI 10.1109/TMM.2013.2285518
   Fiandrotti A, 2012, EUR SIGNAL PR CONF, P1529
   Gaeta R, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2568223
   Gaeta R, 2013, IEEE T PARALL DISTR, V24, P1994, DOI 10.1109/TPDS.2012.342
   Gkantsidis C, 2006, IEEE INFOCOM SER, P1749, DOI 10.1109/infocom.2006.233
   Grangetto M, 2009, IEEE INT CON MULTI, P1500, DOI 10.1109/ICME.2009.5202788
   Ho T, 2008, IEEE T INFORM THEORY, V54, P2798, DOI 10.1109/TIT.2008.921894
   Huang G, 2007, ACM SIGCOMM WORKSH P
   Jaggi S, 2008, IEEE T INFORM THEORY, V54, P2596, DOI 10.1109/TIT.2008.921711
   Kamal D. C., 2006, 40 ANN C INF SCI SYS
   Kehdi E, 2009, IEEE INFOCOM SER, P1224, DOI 10.1109/INFCOM.2009.5062036
   Kötter R, 2008, IEEE T INFORM THEORY, V54, P3579, DOI 10.1109/TIT.2008.926449
   Krohn MN, 2004, P IEEE S SECUR PRIV, P226
   Li Q., 2006, P WORKSH MULT SEC, P158
   Li YK, 2010, PERFORM EVALUATION, V67, P1273, DOI 10.1016/j.peva.2010.08.005
   Wang M, 2007, IEEE T MULTIMEDIA, V9, P1554, DOI 10.1109/TMM.2007.907460
   Yu Z., 2008, IEEE 27 C COMP COMM
   Yu Z, 2009, IEEE INFOCOM SER, P406, DOI 10.1109/INFCOM.2009.5061945
   Zhang XY, 2005, IEEE INFOCOM SER, P2102
NR 23
TC 2
Z9 2
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2016
VL 18
IS 6
BP 1138
EP 1148
DI 10.1109/TMM.2016.2535781
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DM1LP
UT WOS:000376107100016
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Dong, L
   Liang, Y
   Kong, GP
   Zhang, QN
   Cao, XC
   Izquierdo, E
AF Dong, Le
   Liang, Yan
   Kong, Gaipeng
   Zhang, Qianni
   Cao, Xiaochun
   Izquierdo, Ebroul
TI Holons Visual Representation for Image Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Clustering; image retrieval; similarity distance regularization; visual
   representation
ID CLASSIFICATION; BLURRY
AB Along with the enlargement of image scale, convolutional local features, such as SIFT, are ineffective for representing or indexing and more compact visual representations are required. Due to the intrinsic mechanism, the state-of-the-art vector of locally aggregated descriptors (VLAD) has a few limits. Based on this, we propose a new descriptor named holons visual representation (HVR). The proposed HVR is a derivative mutational self-contained combination of global and local information. It exploits both global characteristics and the statistic information of local descriptors in the image dataset. It also takes advantages of local features of each image and computes their distribution with respect to the entire local descriptor space. Accordingly, the HVR is computed by a two-layer hierarchical scheme, which splits the local feature space and obtains raw partitions, as well as the corresponding refined partitions. Then, according to the distances from the centroids of partition spaces to local features and their spatial correlation, we assign the local features into their nearest raw partitions and refined partitions to obtain the global description of an image. Compared with VLAD, HVR holds critical structure information and enhances the discriminative power of individual representation with a small amount of computation cost, while using the same memory overhead. Extensive experiments on several benchmark datasets demonstrate that the proposed HVR outperforms conventional approaches in terms of scalability as well as retrieval accuracy for images with similar intra local information.
C1 [Dong, Le; Kong, Gaipeng] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
   [Liang, Yan] China Acad Engn Phys, Inst Comp Applicat, Mianyang 621900, Peoples R China.
   [Zhang, Qianni; Izquierdo, Ebroul] Queen Mary Univ London, Sch Elect Engn & Comp Sci, London E1 4NS, England.
   [Cao, Xiaochun] Chinese Acad Sci, State Key Lab Informat Secur, Inst Informat Engn, Beijing 100093, Peoples R China.
   [Cao, Xiaochun] Tianjin Univ, Sch Comp Sci & Technol, Tianjin 300072, Peoples R China.
C3 University of Electronic Science & Technology of China; Chinese Academy
   of Engineering Physics; University of London; Queen Mary University
   London; Chinese Academy of Sciences; Institute of Information
   Engineering, CAS; Tianjin University
RP Dong, L; Kong, GP (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.; Liang, Y (corresponding author), China Acad Engn Phys, Inst Comp Applicat, Mianyang 621900, Peoples R China.; Zhang, QN; Izquierdo, E (corresponding author), Queen Mary Univ London, Sch Elect Engn & Comp Sci, London E1 4NS, England.; Cao, XC (corresponding author), Chinese Acad Sci, State Key Lab Informat Secur, Inst Informat Engn, Beijing 100093, Peoples R China.; Cao, XC (corresponding author), Tianjin Univ, Sch Comp Sci & Technol, Tianjin 300072, Peoples R China.
EM ledong@uestc.edu.cn; xue-huaiyan@163.com; konggaipeng@163.com;
   qianni.zhang@qmul.ac.uk; caoxiaochun@iie.ac.cn;
   ebroul.izquierdo@qmul.ac.uk
OI Zhang, Qianni/0000-0001-7685-2187
FU National Natural Science Foundation of China [61370149, 61422213];
   Fundamental Research Funds for Central Universities [ZYGX2013J083];
   Scientific Research Foundation for Returned Overseas Chinese Scholars,
   State Education Ministry [LXHG42DL]; National Basic Research Program of
   China [2013CB329305]; Strategic Priority Research Program of Chinese
   Academy of Sciences
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61370149, in part by the Fundamental
   Research Funds for the Central Universities under Grant ZYGX2013J083, by
   the Scientific Research Foundation for the Returned Overseas Chinese
   Scholars, State Education Ministry under Grant LXHG42DL, by the National
   Basic Research Program of China under Grant 2013CB329305, by the
   National Natural Science Foundation of China under Grant 61422213, and
   by the Strategic Priority Research Program of the Chinese Academy of
   Sciences. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Changsheng Xu.
CR [Anonymous], 2007, CIVR '07
   [Anonymous], P IEEE INT C MULT EX
   Arandjelovic R, 2013, PROC CVPR IEEE, P1578, DOI 10.1109/CVPR.2013.207
   Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018
   Caicedo J. C., 2009, T MASS DATA ANAL IMA, V2, P68
   Cao Y, 2010, PROC CVPR IEEE, P3352, DOI 10.1109/CVPR.2010.5540021
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Delhumeau J., 2013, P 21 ACM INT C MULT, P653
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong L, 2007, IEEE T CIRC SYST VID, V17, P590, DOI 10.1109/TCSVT.2007.894035
   Dong L, 2012, IEEE T IMAGE PROCESS, V21, P2534, DOI 10.1109/TIP.2012.2187528
   Dong L, 2008, IEEE IMAGE PROC, P2168, DOI 10.1109/ICIP.2008.4712218
   Hoàng NV, 2010, PATTERN RECOGN, V43, P3013, DOI 10.1016/j.patcog.2010.03.024
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Jégou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Jégou H, 2009, PROC CVPR IEEE, P1169, DOI 10.1109/CVPRW.2009.5206609
   Jégou H, 2010, INT J COMPUT VISION, V87, P316, DOI 10.1007/s11263-009-0285-2
   Jegou H, 2010, IEEE T PATTERN ANAL, V32, P2, DOI 10.1109/TPAMI.2008.285
   Jia D, 2011, PROC CVPR IEEE, P785, DOI 10.1109/CVPR.2011.5995516
   Jiang YN, 2012, PROC CVPR IEEE, P3100, DOI 10.1109/CVPR.2012.6248042
   Li YX, 2012, IEEE T MULTIMEDIA, V14, P1618, DOI 10.1109/TMM.2012.2199292
   Li ZC, 2015, IEEE T IMAGE PROCESS, V24, P5343, DOI 10.1109/TIP.2015.2479560
   Li ZC, 2015, IEEE T PATTERN ANAL, V37, P2085, DOI 10.1109/TPAMI.2015.2400461
   Li ZC, 2014, IEEE T KNOWL DATA EN, V26, P2138, DOI 10.1109/TKDE.2013.65
   Lin Z, 2010, LECT NOTES COMPUT SC, V6316, P294, DOI 10.1007/978-3-642-15567-3_22
   Liu Z, 2015, IEEE T MULTIMEDIA, V17, P538, DOI 10.1109/TMM.2015.2399851
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Nister David, 2006, CVPR
   Perronnin F., 2007, P IEEE CVPR, P1
   PERRONNIN F, 2010, PROC CVPR IEEE, P3384, DOI DOI 10.1109/CVPR.2010.5540009
   Philbin J., 2008, P CVPR, P1
   Philbin J, 2010, LECT NOTES COMPUT SC, V6313, P677
   Pourian N, 2015, IEEE T MULTIMEDIA, V17, P616, DOI 10.1109/TMM.2015.2410734
   Ran Chang, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P2425, DOI 10.1109/ICIP.2011.6116133
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Su Y, 2011, IEEE I CONF COMP VIS, P311, DOI 10.1109/ICCV.2011.6126257
   Tolias G, 2011, IEEE I CONF COMP VIS, P1653, DOI 10.1109/ICCV.2011.6126427
   van Gemert JC, 2010, IEEE T PATTERN ANAL, V32, P1271, DOI 10.1109/TPAMI.2009.132
   Wang XY, 2011, IEEE I CONF COMP VIS, P209, DOI 10.1109/ICCV.2011.6126244
   Wu L, 2009, IEEE T MULTIMEDIA, V11, P286, DOI 10.1109/TMM.2008.2009692
   Xie HT, 2014, IEEE T MULTIMEDIA, V16, P1104, DOI 10.1109/TMM.2014.2305909
   Yang Y, 2014, IEEE T MULTIMEDIA, V16, P1700, DOI 10.1109/TMM.2014.2326836
   Yu FX, 2012, PROC CVPR IEEE, P2949, DOI 10.1109/CVPR.2012.6248023
   Zhang Q, 2006, LECT NOTES COMPUT SC, V4071, P310
   Zhang QN, 2013, IEEE J BIOMED HEALTH, V17, P240, DOI 10.1109/TITB.2012.2227270
   Zhang SG, 2010, PROCEEDINGS OF THE ASME JOINT RAIL CONFERENCE, VOL 2, P501, DOI 10.1145/1873951.1874018
   Zhang YM, 2011, PROC CVPR IEEE, P809, DOI 10.1109/CVPR.2011.5995528
NR 48
TC 12
Z9 13
U1 1
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2016
VL 18
IS 4
BP 714
EP 725
DI 10.1109/TMM.2016.2530399
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DH4XZ
UT WOS:000372790300013
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Deng, C
   Tang, X
   Yan, JC
   Liu, W
   Gao, XB
AF Deng, Cheng
   Tang, Xu
   Yan, Junchi
   Liu, Wei
   Gao, Xinbo
TI Discriminative Dictionary Learning With Common Label Alignment for
   Cross-Modal Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Common space; cross-modal retrieval; discriminative dictionary learning;
   label alignment
ID COUPLED DICTIONARY
AB Cross-modal retrieval has attracted much attention in recent years due to its widespread applications. In this area, how to capture and correlate heterogeneous features originating from different modalities remains a challenge. However, most existing methods dealing with cross-modal learning only focus on learning relevant features shared by two distinct feature spaces, therefore overlooking discriminative feature information of them. To remedy this issue and explicitly capture discriminative feature information, we propose a novel cross-modal retrieval approach based on discriminative dictionary learning that is augmented with common label alignment. Concretely, a discriminative dictionary is first learned to account for each modality, which boosts not only the discriminating capability of intra-modality data from different classes but also the relevance of inter-modality data in the same class. Subsequently, all the resulting sparse codes are simultaneously mapped to a common label space, where the cross-modal data samples are characterized and associated. Also in the label space, the discriminativeness and relevance of the considered cross-modal data can be further strengthened by enforcing a common label alignment. Finally, cross-modal retrieval is performed over the common label space. Experiments conducted on two public cross-modal datasets show that the proposed approach outperforms several state-of-the-art methods in term of retrieval accuracy.
C1 [Deng, Cheng; Tang, Xu; Liu, Wei; Gao, Xinbo] Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China.
   [Yan, Junchi] E China Normal Univ, Sch Comp Sci & Software Engn, Shanghai 200062, Peoples R China.
C3 Xidian University; East China Normal University
RP Liu, W (corresponding author), Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China.
EM chdeng.xd@gmail.com; tangxu@stu.xidian.edu.cn; jcyan@sei.ecnu.edu.cn;
   knato_lw@hotmail.com; xbgao@mail.xidian.edu.cn
RI Liu, Wei/L-1951-2019; Yan, Jun/IXD-7801-2023
OI Yan, Junchi/0000-0001-9639-7679; Deng, Cheng/0000-0003-2620-3247; Liu,
   Wei/0000-0002-3865-8145
FU National Natural Science Foundation of China [61572388, 61125204,
   61432014, 61303220]; National High Technology Research and Development
   Program of China [2013AA01A602]; Program for New Century Excellent
   Talents in University [NCET-12-0917]; Fundamental Research Funds for the
   Central Universities [K5051302019]; Doctoral Program of Higher Education
   of China [20120203120014]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61572388, Grant 61125204, Grant 61432014, and Grant
   61303220, by the National High Technology Research and Development
   Program of China under Grant 2013AA01A602, by the Program for New
   Century Excellent Talents in University under Grant NCET-12-0917, by the
   Fundamental Research Funds for the Central Universities under Grant
   K5051302019, and by the Doctoral Program of Higher Education of China
   under Grant 20120203120014. The associate editor coordinating the review
   of this manuscript and approving it for publication was Prof. Marco
   Bertini. (Corresponding author: Wei Liu.)
CR [Anonymous], 2010, PROC ACM SIGMM INT C
   [Anonymous], 2008, 2008 IEEE C COMP VIS, DOI DOI 10.1109/CVPR.2008.4587652
   Baktashmotlagh M, 2014, PROC CVPR IEEE, P2481, DOI 10.1109/CVPR.2014.318
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Blei DM, 2003, P 26 ANN INT ACM SIG, P127, DOI DOI 10.1145/860435.860460
   Chen YM, 2012, IEEE IMAGE PROC, P1949, DOI 10.1109/ICIP.2012.6467268
   Chua T.-S., 2009, CIVR, P1, DOI 10.1145/1646396.1646452
   Deng C, 2014, IEEE T MULTIMEDIA, V16, P785, DOI 10.1109/TMM.2014.2298841
   Dollár P, 2014, IEEE T PATTERN ANAL, V36, P1532, DOI 10.1109/TPAMI.2014.2300479
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   Huang DA, 2013, IEEE I CONF COMP VIS, P2496, DOI 10.1109/ICCV.2013.310
   Jia Y., 2010, Adv. Neural Inf. Process. Syst., P982
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mairal J., 2009, ADV NEURAL INFORM PR, P1033
   Mairal J., 2009, P 26 ANN INT C MACHI, P689, DOI 10.1145/1553374.1553463
   Min HS, 2012, IEEE T CIRC SYST VID, V22, P1174, DOI 10.1109/TCSVT.2012.2197080
   Monaci G, 2007, IEEE T IMAGE PROCESS, V16, P2272, DOI 10.1109/TIP.2007.901813
   Putthividhya D, 2010, PROC CVPR IEEE, P3408, DOI 10.1109/CVPR.2010.5540000
   Qadir O, 2011, IEEE C EVOL COMPUTAT, P208
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Rasiwasia N, 2007, IEEE T MULTIMEDIA, V9, P923, DOI 10.1109/TMM.2007.900138
   Sharma A, 2012, PROC CVPR IEEE, P2160, DOI 10.1109/CVPR.2012.6247923
   Sharma A, 2011, PROC CVPR IEEE, P593, DOI 10.1109/CVPR.2011.5995350
   Shekhar S, 2013, PROC CVPR IEEE, P361, DOI 10.1109/CVPR.2013.53
   Tenenbaum JB, 2000, NEURAL COMPUT, V12, P1247, DOI 10.1162/089976600300015349
   Tibshirani R, 1996, J ROY STAT SOC B, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x
   Virtanen S., 2012, CORR
   Wang KY, 2013, IEEE I CONF COMP VIS, P2088, DOI 10.1109/ICCV.2013.261
   Wang SL, 2012, PROC CVPR IEEE, P2216, DOI 10.1109/CVPR.2012.6247930
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Wu F, 2012, INT J MULTIMED INF R, V1, P3, DOI 10.1007/s13735-012-0001-9
   Wu F, 2014, IEEE T MULTIMEDIA, V16, P427, DOI 10.1109/TMM.2013.2291214
   Yang M, 2011, IEEE I CONF COMP VIS, P543, DOI 10.1109/ICCV.2011.6126286
   Yang M, 2010, IEEE IMAGE PROC, P1601, DOI 10.1109/ICIP.2010.5652363
   Zhuang Y., 2013, P 27 AAAI C ART INT, P1070
NR 36
TC 87
Z9 95
U1 1
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2016
VL 18
IS 2
BP 208
EP 218
DI 10.1109/TMM.2015.2508146
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DB3HX
UT WOS:000368402400006
DA 2024-07-18
ER

PT J
AU Stowell, D
   Giannoulis, D
   Benetos, E
   Lagrange, M
   Plumbley, MD
AF Stowell, Dan
   Giannoulis, Dimitrios
   Benetos, Emmanouil
   Lagrange, Mathieu
   Plumbley, Mark D.
TI Detection and Classification of Acoustic Scenes and Events
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio databases; event detection; machine intelligence; pattern
   recognition
ID MUSIC; RETRIEVAL; RECOGNITION
AB For intelligent systems to make best use of the audio modality, it is important that they can recognize not just speech and music, which have been researched as specific tasks, but also general sounds in everyday environments. To stimulate research in this field we conducted a public research challenge: the IEEE Audio and Acoustic Signal Processing Technical Committee challenge on Detection and Classification of Acoustic Scenes and Events (DCASE). In this paper, we report on the state of the art in automatically classifying audio scenes, and automatically detecting and classifying audio events. We survey prior work as well as the state of the art represented by the submissions to the challenge from various research groups. We also provide detail on the organization of the challenge, so that our experience as challenge hosts may be useful to those organizing challenges in similar domains. We created new audio datasets and baseline systems for the challenge; these, as well as some submitted systems, are publicly available under open licenses, to serve as benchmarks for further research in general-purpose machine listening.
C1 [Stowell, Dan; Giannoulis, Dimitrios; Plumbley, Mark D.] Queen Mary Univ London, Ctr Digital Mus, London E1 4NS, England.
   [Benetos, Emmanouil] City Univ London, Dept Comp Sci, London EC1V 0HB, England.
   [Lagrange, Mathieu] Ecole Cent Nantes, ADTSI, IRCCYN, F-44321 Nantes, France.
C3 University of London; Queen Mary University London; City University
   London; Nantes Universite; Ecole Centrale de Nantes
RP Stowell, D (corresponding author), Queen Mary Univ London, Ctr Digital Mus, London E1 4NS, England.
EM dan.stowell@qmul.ac.uk
RI Stowell, Dan/HZJ-9689-2023; Stowell, Dan/AIE-7524-2022; Benetos,
   Emmanouil/S-1932-2018; Plumbley, Mark D/A-7298-2008
OI Stowell, Dan/0000-0001-8068-3769; Stowell, Dan/0000-0001-8068-3769;
   Benetos, Emmanouil/0000-0002-6820-6764; Plumbley, Mark
   D/0000-0002-9708-1075
FU IEEE AASP Technical Committee; EPSRC [EP/L020505/1, EP/H043101/1,
   EP/G007144/1] Funding Source: UKRI
FX The authors would like to thank the IEEE AASP Technical Committee for
   endorsing and supporting this work, as well as all challenge
   participants-not only for their submissions but their community
   participation in shaping the challenges, and their presentations at IEEE
   WASPAA 2013. They would also like to thank the IEEE WASPAA 2013
   Conference Committee for their support in organizing the special
   session.
CR [Anonymous], 2006, P 8 ACM INT WORKSHOP
   [Anonymous], 2013, IEEE Workshop on WASPAA, DOI DOI 10.1109/WASPAA.2013.6701819
   [Anonymous], 2006, Computational auditory scene analysis: Principles, algorithms, and applications
   [Anonymous], AUTOMATIC EVENT CLAS
   [Anonymous], THESIS QUEEN MARY U
   [Anonymous], ACOUSTIC EVENT DETEC
   [Anonymous], 2003, ISMIR
   [Anonymous], SOUND EVENT DETECTIO
   [Anonymous], 2004, Journal of negative results in speech and audio sciences
   Aucouturier JJ, 2007, J ACOUST SOC AM, V122, P881, DOI 10.1121/1.2750160
   Barchiesi D, 2015, IEEE SIGNAL PROC MAG, V32, P16, DOI 10.1109/MSP.2014.2326181
   Barker J, 2013, COMPUT SPEECH LANG, V27, P621, DOI 10.1016/j.csl.2012.10.004
   Barker JP, 2005, SPEECH COMMUN, V45, P5, DOI 10.1016/j.specom.2004.05.002
   Benetos E, 2013, J ACOUST SOC AM, V133, P1727, DOI 10.1121/1.4790351
   Bregman AS., 1994, AUDITORY SCENE ANAL
   Briggs F, 2012, J ACOUST SOC AM, V131, P4640, DOI 10.1121/1.4707424
   Cauchi B., 2011, THESIS PARISTECH PAR
   Chauhan S., 2013, EVENT DETECTION CLAS
   Chu S, 2009, IEEE T AUDIO SPEECH, V17, P1142, DOI 10.1109/TASL.2009.2017438
   Chum M., 2013, IEEE AASP SCENE CLASSIFICATION CHALLENGE USING HIDDEN MARKOV MODELS AND FRAME BASED CLASSIFICATION
   Cotton CV, 2011, 2011 IEEE WORKSHOP ON APPLICATIONS OF SIGNAL PROCESSING TO AUDIO AND ACOUSTICS (WASPAA), P69, DOI 10.1109/ASPAA.2011.6082331
   Downie JS, 2010, STUD COMPUT INTELL, V274, P93
   Elizalde B., 2013, I VECTOR BASED APPRO
   Ellis K., 2011, Proceedings of the 12th International Conference on Music Information Retrieval (ISMIR'11), P723
   Eronen AJ, 2006, IEEE T AUDIO SPEECH, V14, P321, DOI 10.1109/TSA.2005.854103
   Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010
   Foote JT, 1997, P SOC PHOTO-OPT INS, V3229, P138, DOI 10.1117/12.290336
   Geiger J. T., 2013, RECOGNISING ACOUSTIC
   Gemmeke J.F., 2013, AN EXEMPLAR-BASED NMF APPROACH FOR AUDIO EVENT DETECTION
   Giannoulis D, 2013, INT CONF ACOUST SPEE, P8658, DOI 10.1109/ICASSP.2013.6639356
   Gibbons JD, 2010, STAT TXB MONOGRAPHS, V15th, DOI DOI 10.5005/JP/BOOKS/10313_14
   Heittola T., 2011, WORKSH MACH LIST MUL, P36
   Heittola T, 2013, EURASIP J AUDIO SPEE, DOI 10.1186/1687-4722-2013-1
   Kompass R, 2007, NEURAL COMPUT, V19, P780, DOI 10.1162/neco.2007.19.3.780
   Krijnders J. D., 2013, TONE FIT FEATURE REP
   Li D., 2013, IEEE AASP CHALL DET
   Mesaros A, 2011, EUR SIGNAL PR CONF, P1307
   Mesaros A, 2010, EUR SIGNAL PR CONF, P1267
   Myung Jong Kim, 2011, 2011 9th International Workshop on Content-Based Multimedia Indexing (CBMI), P205, DOI 10.1109/CBMI.2011.5972546
   Nam J., 2013, ACOUSTIC SCENE CLASS
   Niessen M. E., 2013, HIERARCHICAL SOUND E
   Nogueira W., 2013, SOUND SCENE IDENTIFI
   Ntalampiras S, 2009, EURASIP J AUDIO SPEE, DOI 10.1155/2009/594103
   Okuno HG, 2007, ICKS 2007: SECOND INTERNATIONAL CONFERENCE ON INFORMATICS RESEARCH FOR DEVELOPMENT OF KNOWLEDGE SOCIETY INFRASTRUCTURE, PROCEEDINGS, P69
   Olivetti E., 2013, THE WONDERS OF THE NORMALIZED COMPRESSION DISSIMILARITY REPRESENTATION
   Patil K., 2013, MULTIRESOLUTION AUDI
   Plumbley M. D., 2013, J MACH LEARN RES, V14, P1891
   Rabiner L.R., 1993, FUNDAMENTALS SPEECH, VVolume 14
   Rakotomamonjy A., 2013, HISTOGRAM GRADIENTS
   Ranft Richard, 2004, An. Acad. Bras. Ciênc., V76, P456, DOI 10.1590/S0001-37652004000200041
   Roma G., 2013, RECURRENCE QUANTIFICATION ANALYSIS FEATURES FOR AUDITORY SCENE CLASSIFICATION
   Scaringella N, 2006, IEEE SIGNAL PROC MAG, V23, P133, DOI 10.1109/MSP.2006.1598089
   Schorkhuber C., 2010, P 7 SOUND MUS COMP C, P3
   Sprent P., 2001, APPL NONPARAMETRIC S
   Stiefelhagen R, 2007, LECT NOTES COMPUT SC, V4122, P1
   Stowell D., 2014, Proceedings of AES 53rd International Conference, P1
   Sturm BL, 2014, IEEE T MULTIMEDIA, V16, P1636, DOI 10.1109/TMM.2014.2330697
   Temko A, 2007, LECT NOTES COMPUT SC, V4122, P311
   Tranter SE, 2006, IEEE T AUDIO SPEECH, V14, P1557, DOI 10.1109/TASL.2006.878256
   Valin JM, 2004, IEEE INT CONF ROBOT, P1033, DOI 10.1109/ROBOT.2004.1307286
   Vincent E, 2013, 2013 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P162, DOI 10.1109/ASRU.2013.6707723
   Vincent E, 2012, SIGNAL PROCESS, V92, P1928, DOI 10.1016/j.sigpro.2011.10.007
   Vuegen L., 2013, MFCC GMM APPROACH EV
   Witten I. H., 2005, DATA MINING PRACTICA
   WOLF JJ, 1972, J ACOUST SOC AM, V51, P2044, DOI 10.1121/1.1913065
NR 65
TC 349
Z9 372
U1 0
U2 59
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2015
VL 17
IS 10
BP 1733
EP 1746
DI 10.1109/TMM.2015.2428998
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CR9OD
UT WOS:000361685400005
OA hybrid, Green Published, Green Submitted, Green Accepted
DA 2024-07-18
ER

PT J
AU Tabia, H
   Laga, H
AF Tabia, Hedi
   Laga, Hamid
TI Covariance-Based Descriptors for Efficient 3D Shape Matching, Retrieval,
   and Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bag of covariance (BoC) matrices; kernels on manifolds; Riemannian
   manifold; symmetric positif definite manifolds
ID FACE RECOGNITION; OBJECTS; IMAGES
AB State-of-the-art 3D shape classification and retrieval algorithms, hereinafter referred to as shape analysis, are often based on comparing signatures or descriptors that capture the main geometric and topological properties of 3D objects. None of the existing descriptors, however, achieve best performance on all shape classes. In this article, we explore, for the first time, the usage of covariance matrices of descriptors, instead of the descriptors themselves, in 3D shape analysis. Unlike histogram-based techniques, covariance-based 3D shape analysis enables the fusion and encoding of different types of features and modalities into a compact representation. Covariance matrices, however, are elements of the non-linear manifold of symmetric positive definite (SPD) matrices and thus L-2 metrics are not suitable for their comparison and clustering. In this article, we study geodesic distances on the Riemannian manifold of SPD matrices and use them as metrics for 3D shape matching and recognition. We then: (1) introduce the concepts of bag of covariance (BoC) matrices and spatially-sensitive BoC as a generalization to the Riemannian manifold of SPD matrices of the traditional bag of features framework, and (2) generalize the standard kernel methods for supervised classification of 3D shapes to the space of covariance matrices. We evaluate the performance of the proposed BoC matrices framework and covariance-based kernel methods and demonstrate their superiority compared to their descriptor-based counterparts in various 3D shape matching, retrieval, and classification setups.
C1 [Tabia, Hedi] ETIS CNRS, ENSEA, F-95014 Cergy Pontoise, France.
   [Laga, Hamid] Univ S Australia, Phen & Bioinformat Res Ctr, Mawson Lakes, SA 5095, Australia.
C3 University of South Australia
RP Tabia, H (corresponding author), ETIS CNRS, ENSEA, F-95014 Cergy Pontoise, France.
EM hedi.tabia@ensea.fr; hamid.laga@unisa.edu.au
RI Laga, Hamid/B-5116-2012
CR Agathos A., 2009, 3DOR, P29
   [Anonymous], P IEEE SHAP MOD INT
   [Anonymous], EUR WORKSH 3D OBJ RE
   Assfalg J, 2007, IEEE T MULTIMEDIA, V9, P589, DOI 10.1109/TMM.2006.886271
   Behmo R., 2008, P IEEE C COMP VIS PA, P1
   Biasotti S, 2006, COMPUT AIDED DESIGN, V38, P1002, DOI 10.1016/j.cad.2006.07.003
   Boutros J. J., 2006, PROC INT ZURICH SEMI, P1
   Bronstein AM, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1899404.1899405
   Bronstein MM, 2010, PROC CVPR IEEE, P1704, DOI 10.1109/CVPR.2010.5539838
   Bu SH, 2014, IEEE T MULTIMEDIA, V16, P2154, DOI 10.1109/TMM.2014.2351788
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Cornea ND, 2005, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P366, DOI 10.1109/SMI.2005.1
   Drira H, 2013, IEEE T PATTERN ANAL, V35, P2270, DOI 10.1109/TPAMI.2013.48
   Faraki M, 2014, PATTERN RECOGN, V47, P2348, DOI 10.1016/j.patcog.2013.10.011
   Forstner W, 2003, Geodesy-the Challenge of the 3rd Millennium, P299, DOI [10.1007/978-3-662-05296-9_31, DOI 10.1007/978-3-662-05296-9_31]
   Frome A, 2004, LECT NOTES COMPUT SC, V3023, P224
   Gal R, 2007, IEEE T VIS COMPUT GR, V13, P261, DOI 10.1109/TVCG.2007.45
   Harandi MT, 2012, LECT NOTES COMPUT SC, V7573, P216, DOI 10.1007/978-3-642-33709-3_16
   Jayasumana S, 2013, PROC CVPR IEEE, P73, DOI 10.1109/CVPR.2013.17
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   JONKER R, 1987, COMPUTING, V38, P325, DOI 10.1007/BF02278710
   Kazhdan M., 2003, Symposium on Geometry Processing, P156
   Koenderink J. J., 1990, Solid shape
   Kokkinos I, 2012, PROC CVPR IEEE, P159, DOI 10.1109/CVPR.2012.6247671
   Kurtek S, 2013, COMPUT GRAPH FORUM, V32, P429, DOI 10.1111/cgf.12063
   Laga H., 2006, PROC IEEE INT C SHAP, P1
   Laga H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2516971.2516975
   Laga Hamid., 2008, J SOC ART SCI, V7, P124
   Lavoué G, 2012, VISUAL COMPUT, V28, P931, DOI 10.1007/s00371-012-0724-x
   Li B, 2015, COMPUT VIS IMAGE UND, V131, P1, DOI 10.1016/j.cviu.2014.10.006
   Li X., 2008, PROC IEEE C COMPUT V, P1
   Mademlis A, 2008, IEEE T MULTIMEDIA, V10, P819, DOI 10.1109/TMM.2008.922790
   Mahoor MH, 2009, PATTERN RECOGN, V42, P445, DOI 10.1016/j.patcog.2008.08.012
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Moakher M, 2005, SIAM J MATRIX ANAL A, V26, P735, DOI 10.1137/S0895479803436937
   Moreno AB, 2005, ISPA 2005: PROCEEDINGS OF THE 4TH INTERNATIONAL SYMPOSIUM ON IMAGE AND SIGNAL PROCESSING AND ANALYSIS, P185, DOI 10.1109/ISPA.2005.195407
   Novotni M., 2003, P 8 ACM S SOL MOD AP, P216, DOI DOI 10.1145/781606.781639
   Ohbuchi R, 2008, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2008, PROCEEDINGS, P93, DOI 10.1109/SMI.2008.4547955
   Osada R, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P154, DOI 10.1109/SMA.2001.923386
   Pauly M., 2005, Proceedings of Eurographics/ACM SIGGRAPH Symposium on Geometry Processing, P23
   Pennec X, 2006, INT J COMPUT VISION, V66, P41, DOI 10.1007/s11263-005-3222-z
   Porikli F., 2006, 2006 IEEE COMPUTER S, V1, P728, DOI [10.1109/CVPR.2006.94, DOI 10.1109/CVPR.2006.94]
   Shapira L, 2010, INT J COMPUT VISION, V89, P309, DOI 10.1007/s11263-009-0279-0
   Siddiqi K, 2008, MACH VISION APPL, V19, P261, DOI 10.1007/s00138-007-0097-8
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Székely GJ, 2009, ANN APPL STAT, V3, P1236, DOI 10.1214/09-AOAS312
   Tabia H., 2013, Proceedings of the Sixth Eurographics Workshop on 3D Object Retrieval, P17
   Tabia H., 2010, EurographicsWorkshop on 3D Object Retrieval, P3
   Tabia H, 2014, PROC CVPR IEEE, P4185, DOI 10.1109/CVPR.2014.533
   Tabia H, 2011, IEEE T PATTERN ANAL, V33, P852, DOI 10.1109/TPAMI.2010.202
   Tierny J, 2009, COMPUT GRAPH FORUM, V28, P41, DOI 10.1111/j.1467-8659.2008.01190.x
   Toldo R., 2009, Proceedings of the 2nd Eurographics Conference on 3D Object Retrieval, P21
   Tuzel O, 2008, IEEE T PATTERN ANAL, V30, P1713, DOI 10.1109/TPAMI.2008.75
   Tuzel O, 2006, LECT NOTES COMPUT SC, V3952, P589
   Wallraven C, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P257
   Xiaoxing Li, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2575, DOI 10.1109/CVPRW.2009.5206613
   Zaharescu A, 2009, PROC CVPR IEEE, P373, DOI 10.1109/CVPRW.2009.5206748
NR 57
TC 56
Z9 58
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2015
VL 17
IS 9
SI SI
BP 1591
EP 1603
DI 10.1109/TMM.2015.2457676
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CP0RN
UT WOS:000359583000018
DA 2024-07-18
ER

PT J
AU Chuah, SP
   Tan, YP
   Chen, ZZ
AF Chuah, Seong-Ping
   Tan, Yap-Peng
   Chen, Zhenzhong
TI Rate and Power Allocation for Joint Coding and Transmission in Wireless
   Video Chat Applications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Joint rate and power allocation; pricing mechanism; Stackelberg game;
   video coding; wireless video chat
ID EFFICIENT RESOURCE-ALLOCATION; DISTORTION OPTIMIZATION; SCALABLE VIDEO;
   COMMUNICATION
AB Wireless video chat is a power-consuming and bitrate-intensive application. Unlike video streaming, which is one-way traffic, video chat features distributed two-way traffic relayed via base stations, where resource allocation of a client affects the video quality seen by its communicating partner. In this paper, we study the mechanism design of this application via dynamic pricing, and seek efficiency and fairness of resource utilization. Specifically, we assume that the base station relays video bitstreams and charges a service price on the clients based on the transmission power consumption. Based on the price and a given power budget, the clients allocate bitrate and power for video coding and transmission such that the service price and the distortion seen by their partners are minimized. We study such network dynamics in Stackelberg game-theoretic framework. To solve the problem, we propose a complexity-scalable video encoding method and a power-rate-distortion (PRD) model for video chat. The model is more accurate in describing the PRD characteristics, yet of lower complexity in online updates of its coefficients. Based on the PRD model, we derive the distributed rate and power allocations for the clients. We show that a simple pricing update in the base stations is sufficient for optimal pricing. The proposed algorithms are optimal and converge to the Stackelberg equilibrium. Existing SNR- and power-based pricing schemes could not ensure fairness and efficiency simultaneously. We propose a hybrid pricing scheme that balances these conflicting criteria. Extensive simulations demonstrate superior performance of the proposed methods and solutions.
C1 [Chuah, Seong-Ping; Tan, Yap-Peng] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
   [Chen, Zhenzhong] Wuhan Univ, Sch Remote Sensing & Informat Engn, Wuhan 430079, Peoples R China.
C3 Nanyang Technological University; Wuhan University
RP Chuah, SP (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
EM seongping_chuah@sutd.edu.sg; eyptan@ntu.edu.sg; zzchen@ieee.org
RI Chen, Zhenzhong/C-2529-2015; Tan, Yap-Peng/A-5158-2011
FU Agency for Science, Technology and Research (A*STAR), Singapore, under
   the Science and Engineering Research Council
FX Manuscript received March 10, 2014; revised September 14, 2014 and
   January 30, 2015; accepted March 10, 2015. Date of publication March 13,
   2015; date of current version April 15, 2015. This work was supported in
   part by a research grant awarded by The Agency for Science, Technology
   and Research (A*STAR), Singapore, under the Science and Engineering
   Research Council Mobile Media Thematic Strategic Research Programme.
CR Baliga J, 2011, IEEE COMMUN MAG, V49, P70, DOI 10.1109/MCOM.2011.5783987
   Bogucka H, 2011, IEEE COMMUN MAG, V49, P38, DOI 10.1109/MCOM.2011.5783983
   Boyd S., 2004, CONVEX OPTIMIZATION
   Burd TD, 1996, J VLSI SIG PROC SYST, V13, P203, DOI 10.1007/BF01130406
   Chen ZF, 2012, IEEE T CIRC SYST VID, V22, P352, DOI 10.1109/TCSVT.2011.2162763
   Chuah SP, 2013, IEEE T CIRC SYST VID, V23, P467, DOI 10.1109/TCSVT.2012.2210655
   Chuah SP, 2012, IEEE T MULTIMEDIA, V14, P1324, DOI 10.1109/TMM.2012.2193560
   Cicalò S, 2014, IEEE T MULTIMEDIA, V16, P848, DOI 10.1109/TMM.2014.2300442
   Han CZ, 2011, IEEE COMMUN MAG, V49, P46, DOI 10.1109/MCOM.2011.5783984
   He ZH, 2008, IEEE T CIRC SYST VID, V18, P596, DOI 10.1109/TCSVT.2008.918802
   Hjrungnes A., 2011, Game Theory in Wireless and Commu- nication Networks: Theory, Models, and Applications
   Huang JW, 2010, IEEE T SIGNAL PROCES, V58, P3635, DOI 10.1109/TSP.2010.2046894
   Kang X, 2012, IEEE J SEL AREA COMM, V30, P538, DOI 10.1109/JSAC.2012.120404
   Kannangara CS, 2008, IEEE T CIRC SYST VID, V18, P1191, DOI 10.1109/TCSVT.2008.928881
   Kim Jong-In., 2011, 2011 IEEE Custom Integrated Circuits Conference (CICC), P1
   Kuo WH, 2011, IEEE T MULTIMEDIA, V13, P116, DOI 10.1109/TMM.2010.2082350
   Li MD, 2013, IEEE T MULTIMEDIA, V15, P1519, DOI 10.1109/TMM.2013.2267207
   Li X, 2011, IEEE T CIRC SYST VID, V21, P957, DOI 10.1109/TCSVT.2011.2133750
   Liang YF, 2009, IEEE T CIRC SYST VID, V19, P1436, DOI 10.1109/TCSVT.2009.2026810
   Liao J, 2013, IEEE T MULTIMEDIA, V15, P670, DOI 10.1109/TMM.2012.2235416
   Lin WS, 2012, IEEE T IMAGE PROCESS, V21, P2667, DOI 10.1109/TIP.2012.2190609
   Ma Z, 2011, IEEE T MULTIMEDIA, V13, P1240, DOI 10.1109/TMM.2011.2165056
   Meshkati F, 2007, IEEE SIGNAL PROC MAG, V24, P58, DOI 10.1109/MSP.2007.361602
   Pandremmenou K, 2013, IEEE T CIRC SYST VID, V23, P1388, DOI 10.1109/TCSVT.2013.2243646
   Ren SL, 2011, IEEE T SIGNAL PROCES, V59, P2913, DOI 10.1109/TSP.2011.2121064
   Sen S, 2013, ACM COMPUT SURV, V46, DOI 10.1145/2543581.2543582
   Su L, 2009, IEEE T CIRC SYST VID, V19, P477, DOI 10.1109/TCSVT.2009.2014017
   Tan YH, 2010, IEEE T CIRC SYST VID, V20, P1271, DOI 10.1109/TCSVT.2010.2058480
   Tiwari M, 2011, IEEE T IMAGE PROCESS, V20, P3219, DOI 10.1109/TIP.2011.2146262
   Wang BB, 2009, IEEE T MOBILE COMPUT, V8, P975, DOI 10.1109/TMC.2008.153
   Wu YA, 2011, IEEE T WIREL COMMUN, V10, P12, DOI 10.1109/TWC.2010.120310.091430
   Xiao Y, 2011, IEEE T WIREL COMMUN, V10, P1527, DOI 10.1109/TWC.2011.032411.100753
   Zhu K, 2012, IEEE T WIREL COMMUN, V11, P1136, DOI 10.1109/TWC.2012.010312.110732
NR 33
TC 10
Z9 11
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2015
VL 17
IS 5
BP 687
EP 699
DI 10.1109/TMM.2015.2413354
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CG3AH
UT WOS:000353148300010
DA 2024-07-18
ER

PT J
AU Zhang, G
   Liu, W
   Hei, XJ
   Cheng, WQ
AF Zhang, Ge
   Liu, Wei
   Hei, Xiaojun
   Cheng, Wenqing
TI Unreeling Xunlei Kankan: Understanding Hybrid CDN-P2P Video-on-Demand
   Streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Content distribution network (CDN); hybrid content distribution network
   peer to peer (CDN-P2P); peer-to-peer (P2P) network; video on demand
   (VoD)
AB The hybrid architecture of content distribution network (CDN) and peer to peer (P2P) is promising in providing online streaming media services. In this paper, we conducted a comprehensive measurement study on Kankan, one of the leading VoD streaming service providers in China that is based on a hybrid CDN-P2P architecture. Our measurements are multi-fold, as follows. 1) Kankan adopts a loosely-coupled hybrid architecture, in which the user requests are handled by its CDN and P2P network independently. 2) Kankan deploys a small-scale CDN densely in three geographic clusters in China. It adopts specific redirection servers to dispatch the nationwide requests. 3) Kankan adopts a dual-server mechanism to enhance start-up video streaming. It also provides the CDN acceleration in case of inefficient P2P streaming performance. 4) According to our studies on the peer cache lists, the video contents stored in Kankan peers update quite slowly. The average lifetime of cached videos is longer than one week. Our results show that, by utilizing the slow-varying contents cached in peers and deploying various CDN enhancement mechanisms, Kankan provides a large-scale VoD streaming service with a small-scale fixed infrastructure. Insights obtained in this study will be valuable for the development and deployment of future hybrid CDN-P2P VoD streaming systems.
C1 [Zhang, Ge; Liu, Wei; Hei, Xiaojun; Cheng, Wenqing] Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China.
C3 Huazhong University of Science & Technology
RP Liu, W (corresponding author), Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China.
EM zhangge@mail.hust.edu.cn; liuwei@mail.hust.edu.cn;
   heixj@mail.hust.edu.cn; chengwq@mail.hust.edu.cn
RI Zhang, Ge/AAF-1242-2019; Hei, Xiaojun/E-7970-2012
OI Zhang, Ge/0000-0003-4694-5221; Hei, Xiaojun/0000-0002-6766-4923; Liu,
   Wei/0000-0002-2187-8125
FU National Natural Science Foundation of China (NSFC) [61371080, 61301127,
   61370231]
FX This work was supported in part by the National Natural Science
   Foundation of China (NSFC) under Grant 61371080, Grant 61301127, and
   Grant 61370231. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Pal Halvorsen.
   (Corresponding author: Wei Liu.)
CR Abrahamsson H., 2012, P 2012 INT MEAS C IM, P199
   Adhikari V.K., 2010, Proc. 10th Internet Measurement Conference IMC, P431
   Adhikari VK, 2012, IEEE INFOCOM SER, P2521, DOI 10.1109/INFCOM.2012.6195644
   Adhikari VK, 2012, IEEE INFOCOM SER, P1620, DOI 10.1109/INFCOM.2012.6195531
   Adhikari VK, 2012, IEEE CONF COMPUT, P7, DOI 10.1109/INFCOMW.2012.6193524
   [Anonymous], 2013, P C INT MEAS C, DOI DOI 10.1145/2504730.2504752
   [Anonymous], STAT REP INT DEV CHI
   Arefin A, 2012, INT CON DISTR COMP S, P82, DOI 10.1109/ICDCS.2012.58
   Baccaglini E, 2012, SIGNAL PROCESS-IMAGE, V27, P430, DOI 10.1016/j.image.2012.02.006
   Balachandran A., 2013, Proceedings of the 2013 conference on Internet measurement conference, P43
   Chen YS, 2013, IEEE T MULTIMEDIA, V15, P2087, DOI 10.1109/TMM.2013.2280123
   Dhungel P., 2012, P PAM, P231
   Finamore A., 2011, ACM IMC, P345, DOI DOI 10.1145/2068816.2068849
   Hu C, 2012, PEER PEER NETW APPL, V5, P312, DOI 10.1007/s12083-012-0140-z
   Huang C., 2008, P 18 INT WORKSH NETW, P75, DOI [10.1145/1496046.1496064., DOI 10.1145/1496046.1496064]
   Huang C, 2008, IMC'08: PROCEEDINGS OF THE 2008 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P15
   Huang Y, 2008, ACM SIGCOMM COMP COM, V38, P375, DOI 10.1145/1402946.1403001
   iResearch Inc. Shanghai China, 2014, IUSERTRACKER RANK DO
   iResearch Inc. Shanghai China, 2012, IUSERTRACKER RANK VI
   iResearch Inc. Shanghai China, PPTV RANK 1 DAIL US
   Lei J, 2010, PEER PEER NETW APPL, V3, P351, DOI 10.1007/s12083-009-0063-5
   Liu X, 2012, ACM SIGCOMM COMP COM, V42, P359, DOI 10.1145/2377677.2377752
   Mansy A., 2011, 2011 19th IEEE International Conference on Network Protocols, P276, DOI 10.1109/ICNP.2011.6089062
   Netflix. Los Gatos CA USA, OP CONN CONT DEL NET
   Tian Y, 2013, IEEE T PARALL DISTR, V24, P1908, DOI 10.1109/TPDS.2012.271
   Xu DY, 2006, MULTIMEDIA SYST, V11, P383, DOI 10.1007/s00530-006-0015-3
   Yin H, 2010, ACM T MULTIM COMPUT, V6, DOI 10.1145/1823746.1823750
   Zhihui Lu, 2012, Journal of Communications, V7, P232, DOI 10.4304/jcm.7.3.232-245
NR 28
TC 62
Z9 66
U1 1
U2 36
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2015
VL 17
IS 2
BP 229
EP 242
DI 10.1109/TMM.2014.2383617
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AZ4RN
UT WOS:000348210500007
DA 2024-07-18
ER

PT J
AU Yu, RS
   Shu, HY
   Jiang, WY
AF Yu, Rongshan
   Shu, Haiyan
   Jiang, Wenyu
TI Low-Complexity Packet Scheduling Algorithms for Streaming Scalable Media
   Based on Time Utility Function
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Media streaming; packet scheduling; scalable media; time utility
   function; utility maximization
AB We propose a time-utility function (TUF)-based packet scheduling algorithm for streaming scalable media. In the proposed system, the scalable media is partitioned into data units of different quality layers, which are then prioritized and transmitted according to their TUFs that capture both their quality contributions to the decoded media and urgencies with respect to their playback schedule. For optimal streaming quality while maintaining a reasonable computational complexity, packet transmissions are scheduled using a low-complexity algorithm based on utility accrual maximization. The computational complexity can be further reduced by considering the look-ahead window and a modified utility function. Simulation results show that the proposed scheduling algorithms achieve near-optimal performance when compared with the operational rate distortion bound of the stream source at any given bandwidth budget.
C1 [Yu, Rongshan; Shu, Haiyan; Jiang, Wenyu] ASTAR, Inst Infocomm Res, Signal Proc Dept, Singapore, Singapore.
C3 Agency for Science Technology & Research (A*STAR); A*STAR - Institute
   for Infocomm Research (I2R)
RP Yu, RS (corresponding author), ASTAR, Inst Infocomm Res, Signal Proc Dept, Singapore, Singapore.
EM rongshanyu@ieee.org; hshu@i2r.astar.edu.sg; wjiang@i2r.a-star.edu.sg
RI YU, Rongshan/AAF-3353-2021
OI YU, Rongshan/0000-0003-2179-173X
CR [Anonymous], 2001, VID COD LOW BIT RAT
   [Anonymous], 2006, 1449632005AMD32006 I
   BLAKE S, 1998, 2475 RFC IETF NETW W
   Brosh E, 2010, IEEE ACM T NETWORK, V18, P1478, DOI 10.1109/TNET.2010.2050780
   Chakareski J, 2004, IEEE IMAGE PROC, P2055
   Chakareski J., 2005, P IEEE INT C MULT EX, P1066
   Chen K, 1996, REAL-TIME SYST, V10, P293, DOI 10.1007/BF00383389
   Chou PA, 2006, IEEE T MULTIMEDIA, V8, P390, DOI 10.1109/TMM.2005.864313
   CHOU PA, 1989, IEEE T INFORM THEORY, V35, P299, DOI 10.1109/18.32124
   Curescu C, 2005, IEEE T PARALL DISTR, V16, P624, DOI 10.1109/TPDS.2005.87
   De Vleeschouwer C, 2007, IEEE T MULTIMEDIA, V9, P348, DOI 10.1109/TMM.2006.886283
   ELEFTHERIADIS A, 1994, IEEE IMAGE PROC, P273, DOI 10.1109/ICIP.1994.413318
   Hua J., 2013, P IEEE INT C IND INF
   Jensen E. D., 1985, IEEE REAL TIM SYST S
   KRASIC C, 1999, CSE99011 OR GRAD I S
   Lee KD, 2009, IEEE T MOBILE COMPUT, V8, P81, DOI 10.1109/TMC.2008.74
   Li WP, 2001, IEEE T CIRC SYST VID, V11, P301, DOI 10.1109/76.911157
   Liao RRF, 2001, WIREL NETW, V7, P541, DOI 10.1023/A:1016730812076
   Lu N, 2009, ELECTR ENG APPL SIGN, P149
   Miao ZR, 2000, CONF REC ASILOMAR C, P1357, DOI 10.1109/ACSSC.2000.911213
   Nguyen HX, 2013, IEEE ACM T NETWORK, V21, P734, DOI 10.1109/TNET.2012.2207915
   Ohm JR, 2005, P IEEE, V93, P42, DOI 10.1109/JPROC.2004.839611
   Röder M, 2006, IEEE T MULTIMEDIA, V8, P170, DOI 10.1109/TMM.2005.861281
   Talluri R, 1998, IEEE COMMUN MAG, V36, P112, DOI 10.1109/35.685373
   Tian DH, 2004, IEEE WCNC, P1287, DOI 10.1109/WCNC.2004.1311374
   Wang B, 2008, ACM T MULTIM COMPUT, V4, DOI 10.1145/1352012.1352020
   Wang JG, 2004, IEEE T PARALL DISTR, V15, P119
   Yu R., 2013, P IEEE INT C MULT EX, P1
   YU R, 2003, JTC1SC29WG11 ISOIEC
   Zhang Q, 2004, IEEE T MULTIMEDIA, V6, P897, DOI 10.1109/TMM.2004.837249
NR 30
TC 1
Z9 2
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2014
VL 16
IS 8
BP 2270
EP 2280
DI 10.1109/TMM.2014.2359335
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AU4HL
UT WOS:000345571300016
DA 2024-07-18
ER

PT J
AU Gao, P
   Xiang, W
AF Gao, Pan
   Xiang, Wei
TI Rate-Distortion Optimized Mode Switching for Error-Resilient Multi-View
   Video Plus Depth Based 3-D Video Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Distortion estimation; error-resilient; mode switching; multi-view video
   depth
ID PRONE NETWORKS; TRANSMISSION; COMPRESSION; SELECTION
AB In this paper, a rate-distortion optimized coding mode switching scheme is proposed to improve error resilience for multi-view video plus depth (MVD) based 3-D video transmission over lossy networks. First, we derive a new end-to-end distortion model for MVD-based 3-D video transmission. As compared with the previous MVD-based video distortion models in which distortion is measured by only investigating the expected texture video errors and depth errors on the synthesized virtual view, the proposed scheme characterizes both the end-to-end distortions in the rendered virtual view and the coded texture video due to packet losses. Moreover, inter-view error propagation for the texture video and depth map is also considered. Based on the proposed distortion model, an optimal mode decision algorithm is then performed in the texture video and depth map coding process. Experimental results show that the proposed method provides significant improvements in terms of both objective and subjective evaluations.
C1 [Gao, Pan; Xiang, Wei] Univ So Queensland, Sch Mech & Elect Engn, Toowoomba, Qld 4350, Australia.
C3 University of Southern Queensland
RP Gao, P (corresponding author), Univ So Queensland, Sch Mech & Elect Engn, Toowoomba, Qld 4350, Australia.
EM Pan.Gao@usq.edu.au; Wei.Xiang@usq.edu.au
RI Xiang, Wei/C-6765-2009; Xiang, Weidong/AAA-2883-2020
OI Xiang, Wei/0000-0002-0608-065X
FU Queensland Government's Smart Futures Fellowship; China Scholarship
   Council; National Natural Science Foundation of China [61201179]
FX This work was supported in part by the Queensland Government's Smart
   Futures Fellowship, the Postgraduate Scholarship Program of China
   Scholarship Council (2013-2014), and the National Natural Science
   Foundation of China under Grant 61201179. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Ebroul Izquierdo.
CR [Anonymous], 2013, JCT3VF1003
   Bjonteggaard G., 2001, VCEGM33
   Chen ZF, 2012, IEEE T CIRC SYST VID, V22, P636, DOI 10.1109/TCSVT.2011.2171262
   Côté G, 2000, IEEE J SEL AREA COMM, V18, P952, DOI 10.1109/49.848249
   Fehn C, 2004, PROC SPIE, V5291, P93, DOI 10.1117/12.524762
   Guo Y, 2009, IEEE T CIRC SYST VID, V19, P781, DOI 10.1109/TCSVT.2009.2017311
   He ZH, 2002, IEEE T CIRC SYST VID, V12, P511, DOI 10.1109/TCSVT.2002.800313
   Kim W.-S., 2010, P SPIE VIS INF PROC
   Li Z., 2003, JVTH014
   Liu YW, 2009, SIGNAL PROCESS-IMAGE, V24, P666, DOI 10.1016/j.image.2009.06.002
   Liu YQ, 2010, IEEE T CIRC SYST VID, V20, P600, DOI 10.1109/TCSVT.2009.2035838
   Macchiavello B., 2013, ARXIV13055464
   Macchiavello B, 2012, IEEE IMAGE PROC, P1653, DOI 10.1109/ICIP.2012.6467194
   Müller K, 2011, P IEEE, V99, P643, DOI 10.1109/JPROC.2010.2091090
   Oh BT, 2011, IEEE J-STSP, V5, P1344, DOI 10.1109/JSTSP.2011.2164893
   Shao F, 2012, IEEE T MULTIMEDIA, V14, P157, DOI 10.1109/TMM.2011.2169045
   STOCKHAMMER T, 2002, 12 INT PACK VID WORK
   Sullivan GJ, 1998, IEEE SIGNAL PROC MAG, V15, P74, DOI 10.1109/79.733497
   Vetro A, 2011, P IEEE, V99, P626, DOI 10.1109/JPROC.2010.2098830
   Wang QF, 2012, IEEE T CIRC SYST VID, V22, P875, DOI 10.1109/TCSVT.2011.2181229
   Xiang X., 2009, P VIS COMM IM PROC
   Yang H, 2007, IEEE T CIRC SYST VID, V17, P845, DOI 10.1109/TCSVT.2007.897116
   Yuan H, 2011, IEEE T CIRC SYST VID, V21, P485, DOI 10.1109/TCSVT.2011.2125610
   Zhang R, 2000, IEEE J SEL AREA COMM, V18, P966, DOI 10.1109/49.848250
   Zhang Y, 2007, IEEE T MULTIMEDIA, V9, P445, DOI 10.1109/TMM.2006.887989
   Zhou Y, 2011, IEEE T CIRC SYST VID, V21, P1679, DOI 10.1109/TCSVT.2011.2133390
NR 26
TC 37
Z9 38
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2014
VL 16
IS 7
BP 1797
EP 1808
DI 10.1109/TMM.2014.2331013
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AT1UX
UT WOS:000344720300001
DA 2024-07-18
ER

PT J
AU Ren, DN
   Chan, SHG
   Cheung, G
   Frossard, P
AF Ren, Dongni
   Chan, S. -H. Gary
   Cheung, Gene
   Frossard, Pascal
TI Coding Structure and Replication Optimization for Interactive Multiview
   Video Streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multimedia computing; digital video broadcasting
ID FRAME
AB Multiview video refers to videos of the same dynamic 3-D scene captured simultaneously by multiple closely spaced cameras from different viewpoints. We study interactive streaming of pre-encoded multiview videos, where, at any time, a client can request any one of many captured views for playback. Moreover, the client can periodically freeze the video in time and switch to neighboring views for a compelling look-around visual effect. We consider distributed content servers to support large-scale interactive multiview video service. These servers collaboratively replicate and access video contents. We study two challenges in this setting: what is an efficient coding structure that supports interactive view switching and, given that, what to replicate in each server in order to minimize the cost incurred by interactive temporal and view switches? We first propose a redundant coding structure that facilitates interactive view-switching, trading off storage with transmission rate. Using the coding structure, we next propose a content replication strategy that takes advantage of indirect hit to lower view-switching cost: in the event that the exact requested view is not available locally, the local server can fetch a different but correlated view from the other servers, so that the remote repository only needs to supply the pre-encoded view differential. We formulate the video content replication problem to minimize the switching cost as an integer linear programming (ILP) problem and show that it is NP-hard. We first propose an LP relaxation and rounding algorithm (termed Minimum Eviction) with bounded approximation error. We then study a more scalable solution based on dynamic programming and Lagrangian optimization (DPLO) with little sacrifice in performance. Simulation results show that our replication algorithms achieve substantially lower switching cost compared to other content replication schemes.
C1 [Ren, Dongni] Hong Kong Univ Sci & Technol, Hong Kong, Hong Kong, Peoples R China.
   [Chan, S. -H. Gary] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Hong Kong, Peoples R China.
   [Cheung, Gene] Natl Inst Informat, Tokyo 1018430, Japan.
   [Frossard, Pascal] Ecole Polytech Fed Lausanne, CH-1015 Lausanne, Switzerland.
C3 Hong Kong University of Science & Technology; Hong Kong University of
   Science & Technology; Research Organization of Information & Systems
   (ROIS); National Institute of Informatics (NII) - Japan; Swiss Federal
   Institutes of Technology Domain; Ecole Polytechnique Federale de
   Lausanne
RP Ren, DN (corresponding author), Hong Kong Univ Sci & Technol, Hong Kong, Hong Kong, Peoples R China.
EM tonyren@cse.ust.hk; gchan@cse.ust.hk; cheung@nii.ac.jp;
   pascal.frossard@epfl.ch
RI Cheung, Gene/AAB-9284-2020; Frossard, Pascal/AAF-2268-2019
OI Cheung, Gene/0000-0002-5571-4137
FU Hong Kong Research Grant Council (RGC) General Research Fund [610713];
   HKUST [FSGRF13EG15]; Hong Kong Innovation and Technology Fund [UIM/246];
   Grants-in-Aid for Scientific Research [23700136] Funding Source: KAKEN
FX This work was supported in part by Hong Kong Research Grant Council
   (RGC) General Research Fund under Grant 610713, HKUST under Grant
   FSGRF13EG15, and the Hong Kong Innovation and Technology Fund under
   Grant UIM/246. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Ebroul Izquierdo.
CR [Anonymous], P MIN IEEE INFOCOM O
   [Anonymous], P SPIE VIS COMM IM P
   [Anonymous], ACM T MULTIMEDIA COM
   [Anonymous], ACM SIGMETRICS PERFO
   Ataee S, 2013, EUROMICRO WORKSHOP P, P68, DOI 10.1109/PDP.2013.19
   Borst S, 2010, IEEE INFOCOM SER, DOI 10.1109/infcom.2010.5461964
   Chan SHG, 2001, IEEE COMMUN LETT, V5, P384, DOI 10.1109/4234.951385
   Chen Y, 2010, CONSUM COMM NETWORK, P1
   Cheung G, 2011, IEEE T IMAGE PROCESS, V20, P744, DOI 10.1109/TIP.2010.2070074
   Cheung NM, 2009, PCS: 2009 PICTURE CODING SYMPOSIUM, P269
   Dai W, 2013, IEEE IMAGE PROC, P1787, DOI 10.1109/ICIP.2013.6738368
   Ding Y, 2011, IEEE INT CONF PEER, P182, DOI 10.1109/P2P.2011.6038734
   Fujii T, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P437, DOI 10.1109/ICME.2006.262566
   Huan Huang, 2012, 2012 Proceedings of the 19th International Packet Video Workshop (PV 2012), P95, DOI 10.1109/PV.2012.6229748
   Jaho E, 2012, COMPUT COMMUN, V35, P637, DOI 10.1016/j.comcom.2011.12.006
   Jian-Guang Lou, 2005, 13th Annual ACM International Conference on Multimedia, P161
   Karczewicz M, 2003, IEEE T CIRC SYST VID, V13, P637, DOI 10.1109/TCSVT.2003.814969
   Mao SW, 2007, IEEE T WIREL COMMUN, V6, P338, DOI 10.1109/TWC.2007.05236
   Maugey T, 2013, IEEE T IMAGE PROCESS, V22, P3459, DOI 10.1109/TIP.2013.2270183
   Merkle P, 2007, IEEE T CIRC SYST VID, V17, P1461, DOI 10.1109/TCSVT.2007.903665
   Papadimitriou C.H., 1998, COMBINATORIAL OPTIMI
   Savas SS, 2012, SIGNAL PROCESS-IMAGE, V27, P522, DOI 10.1016/j.image.2012.02.013
   Shimizu S, 2007, IEEE T CIRC SYST VID, V17, P1485, DOI 10.1109/TCSVT.2007.903773
   Tanimoto M, 2011, IEEE SIGNAL PROC MAG, V28, P67, DOI 10.1109/MSP.2010.939077
   Wang M, 2011, COMPUT NETW, V55, P4069, DOI 10.1016/j.comnet.2011.07.014
   Xiangyang Zhang, 2010, 2010 25th Biennial Symposium on Communications (QBSC), P88, DOI 10.1109/BSC.2010.5472998
   Xiu XY, 2012, IEEE T MULTIMEDIA, V14, P1109, DOI 10.1109/TMM.2012.2191267
   Zhou Y, 2013, J OPTIMIZ THEORY APP, V156, P1, DOI 10.1007/s10957-013-0271-2
NR 28
TC 17
Z9 18
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2014
VL 16
IS 7
BP 1874
EP 1887
DI 10.1109/TMM.2014.2332139
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AT1UX
UT WOS:000344720300007
OA Green Published
DA 2024-07-18
ER

PT J
AU Soleymani, M
   Larson, M
   Pun, T
   Hanjalic, A
AF Soleymani, Mohammad
   Larson, Martha
   Pun, Thierry
   Hanjalic, Alan
TI Corpus Development for Affective Video Indexing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Benchmarks; content analysis; emotional characterization; multimedia;
   videos
ID INFORMATION-RETRIEVAL; EMOTION; MUSIC; MODEL; MOOD
AB Affective video indexing is the area of research that develops techniques to automatically generate descriptions of video content that encode the emotional reactions which the video content evokes in viewers. This paper provides a set of corpus development guidelines based on state-of-the-art practice intended to support researchers in this field. Affective descriptions can be used for video search and browsing systems offering users affective perspectives. The paper is motivated by the observation that affective video indexing has yet to fully profit from the standard corpora (data sets) that have benefited conventional forms of video indexing. Affective video indexing faces unique challenges, since viewer-reported affective reactions are difficult to assess. Moreover affect assessment efforts must be carefully designed in order to both cover the types of affective responses that video content evokes in viewers and also capture the stable and consistent aspects of these responses. We first present background information on affect and multimedia and related work on affective multimedia indexing, including existing corpora. Three dimensions emerge as critical for affective video corpora, and form the basis for our proposed guidelines: the context of viewer response, personal variation among viewers, and the effectiveness and efficiency of corpus creation. Finally, we present examples of three recent corpora and discuss how these corpora make progressive steps towards fulfilling the guidelines.
C1 [Soleymani, Mohammad] Univ London Imperial Coll Sci Technol & Med, Intelligent Behav Understanding Grp IBUG, London SW7 2AZ, England.
   [Larson, Martha; Hanjalic, Alan] Delft Univ Technol, Multimedia Informat Retrieval Lab, NL-2628 CD Delft, Netherlands.
   [Pun, Thierry] Univ Geneva, Comp Vis & Multimedia Lab, CH-1227 Carouge, GE, Switzerland.
C3 Imperial College London; Delft University of Technology; University of
   Geneva
RP Soleymani, M (corresponding author), Univ London Imperial Coll Sci Technol & Med, Intelligent Behav Understanding Grp IBUG, London SW7 2AZ, England.
EM m.soleymani@imperial.ac.uk; m.a.larson@tudelft.nl; thierry.pun@unige.ch;
   a.hanjalic@tudelft.nl
RI Soleymani, Mohammad/AAS-2161-2020
OI Soleymani, Mohammad/0000-0003-2770-7236; Hanjalic,
   Alan/0000-0002-5771-2549
FU European Research Area under the FP7 Marie Curie Intra-European
   Fellowship: Emotional continuous tagging using spontaneous behavior
   (EmoTag); European Community [287704]
FX The work of M. Soleymani was supported by the European Research Area
   under the FP7 Marie Curie Intra-European Fellowship: Emotional
   continuous tagging using spontaneous behavior (EmoTag). The work of M.
   Larson and A. Hanjalic was supported in part by the European Community's
   Seventh Framework Program under grant agreement no. 287704 (CUbRIK). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Sen-Ching Cheung.
CR [Anonymous], 2006, P 8 ACM INT WORKSHOP
   [Anonymous], P AFF COMP INT INT
   [Anonymous], P WORKSH CROWDS SEAR
   [Anonymous], AFFECTIVE SCI
   [Anonymous], 2005, BASIC EMOTIONS, DOI DOI 10.1002/0470013494.CH3
   [Anonymous], MULTIMEDIA TOOLS APP
   [Anonymous], AFFECT BASED INDEXIN
   [Anonymous], P ACM INT C MULT
   [Anonymous], 2011, P 13 INT C MULT INT
   [Anonymous], 321 MED LAB PERC COM
   [Anonymous], 2000, ISCA TUTORIAL RES WO
   [Anonymous], P ACM CHI 11 WORKSH
   [Anonymous], THESIS U NICE SOPHIA
   [Anonymous], EMPATHY AFFECT BEARI
   [Anonymous], 2000, P ISCA TUT RES WORKS
   Arapakis I., 2008, Proceedings of the 31st International ACM SIGIR Conference on Research and Development in Information Retrieval, P395, DOI DOI 10.1145/1390334.1390403
   Benini S, 2011, IEEE T MULTIMEDIA, V13, P1356, DOI 10.1109/TMM.2011.2163058
   Biel JI, 2013, IEEE T MULTIMEDIA, V15, P41, DOI 10.1109/TMM.2012.2225032
   BRADLEY MM, 1994, J BEHAV THER EXP PSY, V25, P49, DOI 10.1016/0005-7916(94)90063-9
   Ching Hau Chan, 2005, 13th Annual ACM International Conference on Multimedia, P427, DOI 10.1145/1101149.1101243
   Demarty CH, 2012, LECT NOTES COMPUT SC, V7585, P416, DOI 10.1007/978-3-642-33885-4_42
   Desmet P., 2003, Funology, V3, P111, DOI [DOI 10.1007/1-4020-2967-5_12, 10.1007/1-4020-2967-5_12]
   EKMAN P, 1987, J PERS SOC PSYCHOL, V53, P712, DOI 10.1037/0022-3514.53.4.712
   Fontaine JRJ, 2007, PSYCHOL SCI, V18, P1050, DOI 10.1111/j.1467-9280.2007.02024.x
   Hanjalic A, 2006, IEEE SIGNAL PROC MAG, V23, P90, DOI 10.1109/MSP.2006.1621452
   Hanjalic A, 2005, IEEE T MULTIMEDIA, V7, P143, DOI 10.1109/TMM.2004.840618
   Hanjalic A, 2005, IEEE T MULTIMEDIA, V7, P1114, DOI 10.1109/TMM.2005.858397
   Hunter PG, 2008, COGNITION EMOTION, V22, P327, DOI 10.1080/02699930701438145
   Hunter PG, 2011, EMOTION, V11, P1068, DOI 10.1037/a0023749
   Irie G, 2010, IEEE T MULTIMEDIA, V12, P523, DOI 10.1109/TMM.2010.2051871
   Kang H.-B., 2003, Proceedings of the 11th ACM International Conference on Multimedia, P259
   Kazemzadeh A, 2013, IEEE COMPUT INTELL M, V8, P34, DOI 10.1109/MCI.2013.2247824
   Kittur A, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P453
   Laird J. D., 2007, FEELINGS PERCEPTION
   Lew MS, 2006, ACM T MULTIM COMPUT, V2, P1, DOI 10.1145/1126004.1126005
   Lopatovska I, 2011, INFORM PROCESS MANAG, V47, P575, DOI 10.1016/j.ipm.2010.09.001
   Marsella Stacy, 2010, A Blueprint for Affective Computing-A sourcebook and manual, V11, P21
   Nathanson AI, 2003, LEA COMMUN SER, P107
   Ortony A., 1988, COGNITIVE STRUCTURE
   Otsuka I, 2009, SIGNALS COMMUN TECHN, P389, DOI 10.1007/978-0-387-76569-3_14
   Paolacci G, 2010, JUDGM DECIS MAK, V5, P411
   Plutchik R., 1980, GEN PSYCHOEVOLUTIONA, P3
   Quirin M, 2009, J PERS SOC PSYCHOL, V97, P500, DOI 10.1037/a0016063
   Reips UD, 2001, BEHAV RES METH INS C, V33, P201, DOI 10.3758/BF03195366
   Rui Y., 2000, Proceedings ACM Multimedia 2000, P105, DOI 10.1145/354384.354443
   RUSSELL JA, 1977, J RES PERS, V11, P273, DOI 10.1016/0092-6566(77)90037-X
   RUSSELL JA, 1989, J PERS SOC PSYCHOL, V57, P493, DOI 10.1037/0022-3514.57.3.493
   RUSSELL JA, 1980, J PERS SOC PSYCHOL, V39, P1161, DOI 10.1037/h0077714
   RUSSELL JA, 1991, PSYCHOL BULL, V110, P426, DOI 10.1037/0033-2909.110.3.426
   Sander D, 2005, NEURAL NETWORKS, V18, P317, DOI 10.1016/j.neunet.2005.03.001
   Schaefer A, 2010, COGNITION EMOTION, V24, P1153, DOI 10.1080/02699930903274322
   Scherer KR, 2005, SOC SCI INFORM, V44, P695, DOI 10.1177/0539018405058216
   SCHERER KR, 1993, COGNITION EMOTION, V7, P325, DOI 10.1080/02699939308409192
   Smeaton AF, 2009, SIGNALS COMMUN TECHN, P151, DOI 10.1007/978-0-387-76569-3_6
   Snoek Cees G. M., 2008, Foundations and Trends in Information Retrieval, V2, P215, DOI 10.1561/1500000014
   Soleymani M., 2009, Affective Computing and Intelligent Interaction and Workshops, P1
   Soleymani M., 2013, P 2 ACM INT WORKSH C, P1, DOI DOI 10.1145/2506364.2506365
   Soleymani M, 2012, IEEE T AFFECT COMPUT, V3, P42, DOI 10.1109/T-AFFC.2011.25
   Soleymani M, 2009, INT J SEMANT COMPUT, V3, P235, DOI 10.1142/S1793351X09000744
   Thornley CV, 2011, J AM SOC INF SCI TEC, V62, P613, DOI 10.1002/asi.21494
   Wang HL, 2006, IEEE T CIRC SYST VID, V16, P689, DOI 10.1109/TCSVT.2006.873781
   Watson D., 1994, The PANASX: Manual for the positive and negative affect scheduleexpanded form, DOI DOI 10.17077/48VT-M4T2
   Winoto P, 2010, EXPERT SYST APPL, V37, P6086, DOI 10.1016/j.eswa.2010.02.117
   Wirth W., 2005, Communication Research Trends, V24, P3, DOI [DOI 10.5167/UZH-101679, DOI 10.5167/UZH]
   Wundt W., 1905, Grundzuge der Physiologischen Psychologie
   Xu M., 2008, Proceeding of the 16th ACM International Conference on Multimedia, P677, DOI DOI 10.1145/1459359.1459457
   Yang YH, 2009, INT CONF ACOUST SPEE, P1657, DOI 10.1109/ICASSP.2009.4959919
   Zeng ZH, 2009, IEEE T PATTERN ANAL, V31, P39, DOI 10.1109/TPAMI.2008.52
   Zillmann D, 1996, LEA COMMUN SER, P199
NR 69
TC 41
Z9 42
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2014
VL 16
IS 4
BP 1075
EP 1089
DI 10.1109/TMM.2014.2305573
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA AJ8LN
UT WOS:000337955800015
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Chen, T
   Yap, KH
   Zhang, DJ
AF Chen, Tao
   Yap, Kim-Hui
   Zhang, Dajiang
TI Discriminative Soft Bag-of-Visual Phrase for Mobile Landmark Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Location and direction; mobile landmark recognition; soft BoP
AB This paper proposes a new bag-of-visual phrase (BoP) approach for mobile landmark recognition based on discriminative learning of category-dependent visual phrases. Many previous landmark recognition works adopt a bag-of-words (BoW) method which ignores the co-occurrence relationship between neighboring visual words in an image. Although some works that focus on visual phrase learning have appeared, they mainly construct a generalized phrase dictionary from all categories for recognition, which lacks descriptive capability for a specific category. Another shortcoming of these works is the hard assignment of numerous feature sets to a limited number of phrases, which causes some useful feature sets to be discarded, and yields information loss. In view of this, this paper presents a discriminative soft BoP approach for mobile landmark recognition. The candidate phrases defined as adjacent pairwise codewords are first generated for each category. The important candidates are then selected through a proposed discriminative visual phrase (DVP) selection approach to form the BoP dictionary. Finally, a soft encoding method is developed to quantize each image into a BoP histogram. The context information such as location and direction captured by mobile devices is also integrated with the proposed BoP-based content analysis for landmark recognition. Experimental results on two datasets show that the proposed method is effective in mobile landmark recognition.
C1 [Chen, Tao] ASTAR, Inst Infocomm Res I2R, Singapore 138632, Singapore.
   [Yap, Kim-Hui; Zhang, Dajiang] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
C3 Agency for Science Technology & Research (A*STAR); A*STAR - Institute
   for Infocomm Research (I2R); Nanyang Technological University
RP Chen, T (corresponding author), ASTAR, Inst Infocomm Res I2R, Singapore 138632, Singapore.
EM chent@i2r.a-star.edu.sg; ekhyap@ntu.edu.sg; DZhang3@e.ntu.edu.sg
RI Lin, Kuan-Yu/JXM-6653-2024
OI Yap, Kim-Hui/0000-0003-1933-4986
CR [Anonymous], ELECT LETT
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], DATA MINING KNOWL DI
   [Anonymous], P ACM INT C MULT ACM
   [Anonymous], 2007, CVPR
   [Anonymous], P INT WORKSH STAT LE
   [Anonymous], P ACM INT C MULT
   [Anonymous], P IEEE COMP VIS PATT
   [Anonymous], RISE CAMERA PHONE IT
   [Anonymous], INT J MULTIMEDIA TOO
   [Anonymous], P EUR C COMP VIS
   Bosch A, 2008, IEEE T PATTERN ANAL, V30, P712, DOI 10.1109/TPAMI.2007.70716
   Chen T, 2012, INT CONF ACOUST SPEE, P893, DOI 10.1109/ICASSP.2012.6288028
   Chen T, 2011, IEEE T CIRC SYST VID, V21, P1476, DOI 10.1109/TCSVT.2011.2161413
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Garcia Vincent, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563100
   Hays J, 2008, PROC CVPR IEEE, P3436
   HINTZ KJ, 1991, IEEE T SYST MAN CYB, V21, P434, DOI 10.1109/21.87090
   Hofmann T, 2001, MACH LEARN, V42, P177, DOI 10.1023/A:1007617005950
   KULLBACK S, 1987, AM STAT, V41, P340
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Li T, 2011, IEEE T CIRC SYST VID, V21, P381, DOI 10.1109/TCSVT.2010.2041828
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Philbin J., 2007, The oxford buildings dataset
   Quack T, 2008, LECT NOTES COMPUT SC, V4952, P230
   Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054
   Schindler G., 2007, 2007 IEEE Conference on Computer Vision and Pattern Recognition, P1
   Schroth G, 2011, IEEE SIGNAL PROC MAG, V28, P77, DOI 10.1109/MSP.2011.940882
   Sirianunpiboon S, 2008, IEEE T INFORM THEORY, V54, P4755, DOI 10.1109/TIT.2008.929012
   Song Y, 2010, IEEE INT CON MULTI, P649, DOI 10.1109/ICME.2010.5583852
   Torralba A, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P273
   Tzortzis G, 2008, IEEE IJCNN, P1977, DOI 10.1109/IJCNN.2008.4634069
   van Gemert JC, 2010, IEEE T PATTERN ANAL, V32, P1271, DOI 10.1109/TPAMI.2009.132
   Yap KH, 2010, IEEE INTELL SYST, V25, P48, DOI 10.1109/MIS.2010.12
   Yeh JB, 2010, IEEE INT SYMP CIRC S, P3681, DOI 10.1109/ISCAS.2010.5537760
   Zhang SG, 2010, PROCEEDINGS OF THE ASME JOINT RAIL CONFERENCE, VOL 2, P501, DOI 10.1145/1873951.1874018
   [郑宇 ZHENG Yu], 2008, [火工品, Initiators & Pyrotechnics], P1
NR 37
TC 38
Z9 42
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2014
VL 16
IS 3
BP 612
EP 622
DI 10.1109/TMM.2014.2301978
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AD3BJ
UT WOS:000333111500004
DA 2024-07-18
ER

PT J
AU Zhang, XY
   Chan, KL
   Constable, M
AF Zhang, Xiaoyan
   Chan, Kap Luk
   Constable, Martin
TI Atmospheric Perspective Effect Enhancement of Landscape Photographs
   Through Depth-Aware Contrast Manipulation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Atmospheric perspective; depth-aware contrast; image manipulation;
   landscape photograph
ID IMAGE-ENHANCEMENT; COLOR TRANSFER; ADJUSTMENT; RANGE
AB The atmospheric perspective effect is a physical phenomenon relating to the effect that atmosphere has on distant objects, causing them to be lighter and less distinct. The exaggeration of this effect by artists in 2D images increases the illusion of depth thereby making the image more interesting. This paper addresses the enhancement of the atmospheric perspective effect in landscape photographs by themanipulation of depth-aware lightness and saturation contrast values. The form of this manipulation follows the organization of such contrasts in landscape paintings. The objective of this manipulation is based on a statistical study which has clearly shown that the saturation and lightness contrasts inter-and intra-depth planes in paintings are more purposefully organized than those in photographs. This contrast organization in paintings respects the existing contrast relationships within a natural scene guided by the atmospheric perspective effect and also exaggerates them sufficiently with a view to improving the visual appeal of the painting and the illusion of depth within it. The depth-aware lightness and saturation contrasts revealed in landscape paintings guide the mapping of such contrasts in photographs. This contrast mapping is formulated as an optimization problem that simultaneously considers the desired inter-contrast, intra-contrast, and specified gradient constraints. Experimental results demonstrate that by using this proposed approach, both the visual appeal and the illusion of depth in the photographs are effectively improved.
C1 [Zhang, Xiaoyan; Chan, Kap Luk] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
   [Constable, Martin] Nanyang Technol Univ, Sch Art Design & Media, Singapore 637458, Singapore.
C3 Nanyang Technological University; Nanyang Technological University
RP Zhang, XY (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
EM zhan0292@e.ntu.edu.sg; eklchan@ntu.edu.sg; MConstable@ntu.edu.sg
RI Li, Mengqi/AAG-6804-2021
FU Institute for Media Innovation, Nanyang Technological University,
   Singapore
FX The authors would like to acknowledge the Ph.D. grant from the Institute
   for Media Innovation, Nanyang Technological University, Singapore.
CR Agaian SS, 2007, IEEE T IMAGE PROCESS, V16, P741, DOI 10.1109/TIP.2006.888338
   An XB, 2010, COMPUT GRAPH FORUM, V29, P263, DOI 10.1111/j.1467-8659.2009.01595.x
   Bae SM, 2006, ACM T GRAPHIC, V25, P637, DOI 10.1145/1141911.1141935
   Beaudot WHA, 2003, VISUAL NEUROSCI, V20, P51, DOI 10.1017/S0952523803201061
   Bhat P, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1731047.1731048
   Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505
   Bychkovsky V, 2011, PROC CVPR IEEE, P97
   Caicedo JC, 2011, PROC CVPR IEEE, P249, DOI 10.1109/CVPR.2011.5995439
   Celik T, 2012, IEEE T IMAGE PROCESS, V21, P145, DOI 10.1109/TIP.2011.2162419
   Chen SH, 2010, EURASIP J IMAGE VIDE, DOI 10.1155/2010/175203
   Cohen-Or D, 2006, ACM T GRAPHIC, V25, P624, DOI 10.1145/1141911.1141933
   Constable M., 2010, 2010 Fourth Pacific-Rim Symposium on Image and Video Technology (PSIVT), P481, DOI 10.1109/PSIVT.2010.87
   Dale K, 2009, IEEE I CONF COMP VIS, P2217, DOI 10.1109/ICCV.2009.5459473
   Dodgson N. A., 2009, P COMP AESTH GRAPH V, P107
   Dunning WilliamV., 1991, CHANGING IMAGES PICT
   Fattal R, 2002, ACM T GRAPHIC, V21, P249
   Hasler D, 2003, P SOC PHOTO-OPT INS, V5007, P87, DOI 10.1117/12.477378
   Huang H, 2010, VISUAL COMPUT, V26, P731, DOI 10.1007/s00371-010-0504-4
   Hwang SJ, 2012, LECT NOTES COMPUT SC, V7572, P569, DOI 10.1007/978-3-642-33718-5_41
   Johnson MK, 2011, IEEE T VIS COMPUT GR, V17, P1273, DOI 10.1109/TVCG.2010.233
   Jung J. I., 2010, ECTI T ELECT ENG ELE, V8, P164
   Kang SB, 2010, PROC CVPR IEEE, P1799, DOI 10.1109/CVPR.2010.5539850
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Lischinski D, 2006, ACM T GRAPHIC, V25, P646, DOI 10.1145/1141911.1141936
   Luft T, 2006, ACM T GRAPHIC, V25, P1206, DOI 10.1145/1141911.1142016
   Majumder A, 2007, ACM T APPL PERCEPT, V4, DOI 10.1145/1278387.1278391
   Mattingly David, 2011, The Digital Matte Painting Handbook
   Mukherjee J, 2008, IEEE T IMAGE PROCESS, V17, P1783, DOI 10.1109/TIP.2008.2002826
   Narasimhan SG, 2002, INT J COMPUT VISION, V48, P233, DOI 10.1023/A:1016328200723
   OSHEA RP, 1994, VISION RES, V34, P1595, DOI 10.1016/0042-6989(94)90116-3
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Saxena A, 2009, IEEE T PATTERN ANAL, V31, P824, DOI 10.1109/TPAMI.2008.132
   Sen D, 2011, IEEE T IMAGE PROCESS, V20, P1211, DOI 10.1109/TIP.2010.2083676
   Sheppard R., 2012, LANDSCAPE PHOTOGRAPH
   Siddiqui H, 2008, IEEE T IMAGE PROCESS, V17, P2138, DOI 10.1109/TIP.2008.2003412
   Sievers AnnH., 2000, Master Drawings from Smith College Museum of Art
   Wang BY, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964959
   Wang BY, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866172
   Wikipedia, AER PERSP
   Xiao Y, 2013, IEEE T MULTIMEDIA, V15, P549, DOI 10.1109/TMM.2012.2233725
   Xiaoyan Zhang, 2011, Proceedings of the 2011 IEEE 5th International Conference on Cybernetics and Intelligent Systems (CIS), P105, DOI 10.1109/ICCIS.2011.6070310
   Yendrikhovskij SN, 1998, P SOC PHOTO-OPT INS, V3299, P274, DOI 10.1117/12.320117
   Zhang X., 2011, P IEEE INT C IM PROC, P1137
NR 43
TC 11
Z9 12
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2014
VL 16
IS 3
BP 653
EP 667
DI 10.1109/TMM.2014.2299511
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AD3BJ
UT WOS:000333111500007
DA 2024-07-18
ER

PT J
AU Yi, J
   Peng, YX
   Xiao, JG
AF Yi, Jian
   Peng, Yuxin
   Xiao, Jianguo
TI Exploiting Semantic and Visual Context for Effective Video Annotation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Context mining; semantic context; video annotation; visual context;
   video retrieval
ID CONCEPT FUSION; RETRIEVAL
AB We propose a new method to refine the result of video annotation by exploiting the semantic and visual context of video. On one hand, semantic context mining is performed in a supervised way, using the manual concept labels of the training set. It is very useful for boosting video annotation performance, because semantic context is learned from labels given by people, indicating human intention. In this paper, we model the spatial and temporal context in video by using conditional random fields with different structures. Comparing with existing methods, our method could more accurately capture concept relationship in video and could more effectively improve the video annotation performance. On the other hand, visual context mining is performed in a semi-supervised way based on the visual similarities among video shots. It indicates the natural visual property of video, and could be considered as the compensation to semantic context, which generally could not be perfectly modeled. In this paper, we construct a graph based on the visual similarities among shots. Then a semi-supervised learning approach is adopt based on the graph to propagate probabilities of the reliable shots to others having similar visual features with them. Extensive experimental results on the widely used TRECVID datasets exhibit the effectiveness of our method for improving video annotation accuracy.
C1 [Yi, Jian; Peng, Yuxin; Xiao, Jianguo] Peking Univ, Inst Comp Sci & Technol, Beijing 100871, Peoples R China.
C3 Peking University
RP Yi, J (corresponding author), Peking Univ, Inst Comp Sci & Technol, Beijing 100871, Peoples R China.
EM pengyuxin@pku.edu.cn
RI peng, yu/GXW-2071-2022; Peng, Yuxin/U-7376-2019; Xiao,
   Jian/GYU-4351-2022
FU National Natural Science Foundation of China [61073084, 61371128]; Ph.D.
   Programs Foundation of Ministry of Education of China [20120001110097];
   National Hi-Tech Research and Development Program (863 Program) of China
   [2012AA012503]; National Key Technology Research and Development Program
   of China [2012BAH07B01]
FX Manuscript received June 02, 2011; revised January 23, 2012 and June 21,
   2012; accepted November 12, 2012. Date of publication March 06, 2013;
   date of current version September 13, 2013. This work was supported by
   National Natural Science Foundation of China under Grants 61073084 and
   61371128, Ph.D. Programs Foundation of Ministry of Education of China
   under Grant 20120001110097, National Hi-Tech Research and Development
   Program (863 Program) of China under Grant 2012AA012503, and National
   Key Technology Research and Development Program of China under Grant
   2012BAH07B01. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Svetha Venkatesh.
CR [Anonymous], P NIPS
   [Anonymous], P INT C MULT
   [Anonymous], 2006, ACM INT C MULTIMEDIA, DOI [10.1145/1180639.1180727, DOI 10.1145/1180639.1180727]
   [Anonymous], 2008, IEEE C COMP VIS PATT, DOI [10.1109/CVPR.2008.4587799, DOI 10.1109/CVPR.2008.4587799]
   [Anonymous], P ACM MULT
   [Anonymous], ACM MULTIMEDIA
   [Anonymous], 2007, CIVR '07
   [Anonymous], 2010, P ACM INT C IMAGE VI
   [Anonymous], 2007, COLUMBIA U BASELINE
   [Anonymous], 2001, PROC 18 INT C MACH L
   Broilo M, 2010, IEEE T MULTIMEDIA, V12, P267, DOI 10.1109/TMM.2010.2046269
   Carbonetto P, 2004, LECT NOTES COMPUT SC, V3021, P350
   Divvala SK, 2009, PROC CVPR IEEE, P1271, DOI 10.1109/CVPRW.2009.5206532
   Jiang W, 2007, INT CONF ACOUST SPEE, P949
   Jiang W, 2006, IEEE IMAGE PROC, P2917, DOI 10.1109/ICIP.2006.313129
   Jiang YG, 2009, IEEE I CONF COMP VIS, P1420, DOI 10.1109/ICCV.2009.5459295
   Kennedy L.S., 2007, Proceedings of the 6th ACM international conference on Image and video retrieval, P333
   Lew MS, 2006, ACM T MULTIM COMPUT, V2, P1, DOI 10.1145/1126004.1126005
   Li YN, 2010, IEEE T MULTIMEDIA, V12, P814, DOI 10.1109/TMM.2010.2066960
   Liu Jingjing., 2007, MULTIMEDIA 07, P208
   Liu KH, 2008, IEEE T MULTIMEDIA, V10, P240, DOI 10.1109/TMM.2007.911826
   Liu Y, 2009, IEEE T CIRC SYST VID, V19, P1841, DOI 10.1109/TCSVT.2009.2026951
   Liu YA, 2009, PROCEEDINGS 32ND ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P500, DOI 10.1145/1571941.1572027
   Lscom Lexicon Definitions and Annotations, 2006, DTO CHALL WORKSH LAR
   Naphade MR, 2002, IEEE T CIRC SYST VID, V12, P40
   Ngo C., 2009, TREC VID RETR EV
   Peng Y., 2009, P TRECVID GAITH MD U
   Peng YX, 2006, IEEE T CIRC SYST VID, V16, P612, DOI 10.1109/TCSVT.2006.873157
   Qi GJ, 2008, ACM T MULTIM COMPUT, V5, DOI 10.1145/1404880.1404883
   Shotton J, 2009, INT J COMPUT VISION, V81, P2, DOI 10.1007/s11263-007-0109-1
   Smeaton A. F., 2006, P MIR
   Smith J., 2003, P ICME
   Sutton Charles., 2007, INTRO STAT RELATIONA
   Tian X., 2008, ACM INT C MULTIMEDIA, P131, DOI DOI 10.1145/1459359.1459378.ISBN
   Torralba A., 2003, INT J COMPUT VISION
   Weng M.F., 2008, PROC ACM INT C MULTI, P71, DOI DOI 10.1145/1459359.1459370
   WU X, 2007, P ACM MULT
   Yan R, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P301, DOI 10.1109/ICME.2006.262458
   Yang J., 2006, PROC ACM INT WORKSHO, P33
   Yilmaz Emine, 2006, Proceedings of the 2006 ACM CIKM International Conference on Information and Knowledge Management, Arlington, Virginia, USA, November 6-11, 2006, P102, DOI [10.1145/1183614.1183633 (cit. on p. 34, DOI 10.1145/1183614.1183633(CIT.ONP.34]
   Zhang L., 2011, PATTERN RECOGNIT
   Zhu Jun, 2005, INT C MACHINE LEARNI, P1044, DOI [DOI 10.1145/1102351.1102483, 10.1145/1102351.1102483]
NR 42
TC 19
Z9 21
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2013
VL 15
IS 6
BP 1400
EP 1414
DI 10.1109/TMM.2013.2250266
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 222XH
UT WOS:000324765400016
DA 2024-07-18
ER

PT J
AU Ren, Z
   Yuan, JS
   Meng, JJ
   Zhang, ZY
AF Ren, Zhou
   Yuan, Junsong
   Meng, Jingjing
   Zhang, Zhengyou
TI Robust Part-Based Hand Gesture Recognition Using Kinect Sensor
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Finger-Earth Mover's Distance; hand gesture recognition; human-computer
   interaction; Kinect system
ID EARTH-MOVERS-DISTANCE; CLASSIFICATION; MODELS
AB The recently developed depth sensors, e. g., the Kinect sensor, have provided new opportunities for human-computer interaction (HCI). Although great progress has been made by leveraging the Kinect sensor, e. g., in human body tracking, face recognition and human action recognition, robust hand gesture recognition remains an open problem. Compared to the entire human body, the hand is a smaller object with more complex articulations and more easily affected by segmentation errors. It is thus a very challenging problem to recognize hand gestures. This paper focuses on building a robust part-based hand gesture recognition system using Kinect sensor. To handle the noisy hand shapes obtained from the Kinect sensor, we propose a novel distance metric, Finger-EarthMover's Distance (FEMD), to measure the dissimilarity between hand shapes. As it only matches the finger parts while not the whole hand, it can better distinguish the hand gestures of slight differences. The extensive experiments demonstrate that our hand gesture recognition system is accurate (a 93.2% mean accuracy on a challenging 10-gesture dataset), efficient (average 0.0750 s per frame), robust to hand articulations, distortions and orientation or scale changes, and can work in uncontrolled environments (cluttered backgrounds and lighting conditions). The superiority of our system is further demonstrated in two real-life HCI applications.
C1 [Ren, Zhou] Nanyang Technol Univ, Media Technol Lab, Singapore 639798, Singapore.
   [Yuan, Junsong] Nanyang Technol Univ, Sch EEE, Infocomm Ctr Excellence INFINITUS, Singapore 639798, Singapore.
   [Meng, Jingjing] Nanyang Technol Univ, Singapore 639798, Singapore.
   [Zhang, Zhengyou] Microsoft Res, Redmond, WA 98052 USA.
C3 Nanyang Technological University; Nanyang Technological University;
   Nanyang Technological University; Microsoft
RP Ren, Z (corresponding author), Nanyang Technol Univ, Media Technol Lab, Singapore 639798, Singapore.
RI meng, jingjing/HDM-6615-2022; Yuan, Junsong/A-5171-2011; Zhang,
   Zhang/JAX-2097-2023; zhang, ZY/HJH-6535-2023; zhang,
   zheng/HCH-9684-2022; Yuan, Junsong/R-4352-2019
OI meng, jingjing/0000-0002-8515-6893; Yuan, Junsong/0000-0002-7901-8793
FU Nanyang Assistant Professorship SUG [M4080134]; Microsoft Research gift
   grant
FX This work was supported in part by the Nanyang Assistant Professorship
   SUG M4080134 and Microsoft Research gift grant. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Eckehard G. Steinbach.
CR Bai X, 2008, IEEE T PATTERN ANAL, V30, P1282, DOI 10.1109/TPAMI.2007.70769
   Bai X, 2007, IEEE T PATTERN ANAL, V29, P449, DOI 10.1109/TPAMI.2007.59
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Bray M, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P675, DOI 10.1109/AFGR.2004.1301612
   Cai Q, 2010, LECT NOTES COMPUT SC, V6313, P229
   Chua CS, 2002, IMAGE VISION COMPUT, V20, P191, DOI 10.1016/S0262-8856(01)00094-4
   Datta R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1348246.1348248
   Dewaele G, 2004, LECT NOTES COMPUT SC, V3021, P495
   Doucet A, 2001, STAT ENG IN, P3
   Erol A, 2007, COMPUT VIS IMAGE UND, V108, P52, DOI 10.1016/j.cviu.2006.10.012
   Fang YK, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P995
   Grauman K, 2004, PROC CVPR IEEE, P220
   Holden E., 1997, THESIS U W AUSTR CRA
   Keogh Eamonn, 2006, VLDB '06: Proceedings of the 32nd International Conference on Very Large Data Bases, P882, DOI DOI 10.5555/1182635.1164203
   KinectHacks, MICR KIN APPL DEM
   Kwok C, 2004, P IEEE, V92, P469, DOI 10.1109/JPROC.2003.823144
   Lee HK, 1999, IEEE T PATTERN ANAL, V21, P961, DOI 10.1109/34.799904
   Lin JY, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P693, DOI 10.1109/AFGR.2004.1301615
   Ling H, 2007, IEEE T PATTERN ANAL, V29, P840, DOI 10.1109/TPAMI.2007.1058
   Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41
   LOWE DG, 1991, IEEE T PATTERN ANAL, V13, P441, DOI 10.1109/34.134043
   Mitra S, 2007, IEEE T SYST MAN CY C, V37, P311, DOI 10.1109/TSMCC.2007.893280
   Murthy G.R.S., 2009, International Journal of Information Technology and Knowledge Management, V2, P405
   Reale MJ, 2011, IEEE T MULTIMEDIA, V13, P474, DOI 10.1109/TMM.2011.2120600
   Ren Z., 2011, P 19 ACM INT C MULT, P759
   Ren Z., 2011, P 19 ACM INT C MULTI, P1093
   Ren Z., IEEE T PATT IN PRESS
   Ren Z, 2011, IEEE I CONF COMP VIS, P303, DOI 10.1109/ICCV.2011.6126256
   Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054
   Segen J., 1998, Proceedings ACM Multimedia 98, P455, DOI 10.1145/290747.290822
   Shimada N, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P268, DOI 10.1109/AFGR.1998.670960
   Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316
   Siddiqi K, 2002, INT J COMPUT VISION, V48, P215, DOI 10.1023/A:1016376116653
   Starner T, 1998, IEEE T PATTERN ANAL, V20, P1371, DOI 10.1109/34.735811
   Su MC, 2000, IEEE T SYST MAN CY C, V30, P276, DOI 10.1109/5326.868448
   Wachs JP, 2011, COMMUN ACM, V54, P60, DOI 10.1145/1897816.1897838
   Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813
   Wilson AD, 1999, IEEE T PATTERN ANAL, V21, P884, DOI 10.1109/34.790429
   Yang MH, 2002, IEEE T PATTERN ANAL, V24, P1061, DOI 10.1109/TPAMI.2002.1023803
   Zhang J, 2007, INT J COMPUT VISION, V73, P213, DOI 10.1007/s11263-006-9794-4
NR 40
TC 494
Z9 546
U1 12
U2 255
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2013
VL 15
IS 5
BP 1110
EP 1120
DI 10.1109/TMM.2013.2246148
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 222WT
UT WOS:000324763600014
DA 2024-07-18
ER

PT J
AU Koloda, J
   Ostergaard, J
   Jensen, SH
   Sánchez, V
   Peinado, AM
AF Koloda, Jan
   Ostergaard, Jan
   Jensen, Soren H.
   Sanchez, Victoria
   Peinado, Antonio M.
TI Sequential Error Concealment for Video/Images by Sparse Linear
   Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Block-coded images/video; convex optimization; error concealment;
   missing data imputation; sparse representation
ID STRUCTURAL SIMILARITY
AB In this paper, we propose a novel sequential error concealment algorithm for video and images based on sparse linear prediction. Block-based coding schemes in packet loss environments are considered. Images are modelled by means of linear prediction, and missing macroblocks are sequentially reconstructed using the available groups of pixels. The optimal predictor coefficients are computed by applying a missing data regression imputation procedure with a sparsity constraint. Moreover, an efficient procedure for the computation of these coefficients based on an exponential approximation is also proposed. Both techniques provide high-quality reconstructions and outperform the state-of-the-art algorithms both in terms of PSNR and MS-SSIM.
C1 [Koloda, Jan; Sanchez, Victoria; Peinado, Antonio M.] Univ Granada, Dept Signal Theory Networking & Commun, Granada 18011, Spain.
   [Ostergaard, Jan; Jensen, Soren H.] Aalborg Univ, Dept Elect Syst, DK-9220 Aalborg, Denmark.
C3 University of Granada; Aalborg University
RP Koloda, J (corresponding author), Univ Granada, Dept Signal Theory Networking & Commun, Granada 18011, Spain.
EM janko@ugr.es; jo@es.aau.dk; shj@es.aau.dk; victoria@ugr.es; amp@ugr.es
RI Jensen, Søren H/A-3139-2011; Peinado Herreros, Antonio
   Miguel/C-2401-2012; Ostergaard, Jan/I-5693-2014
OI Peinado Herreros, Antonio Miguel/0000-0001-8214-6676; Sanchez Calle,
   Victoria Eugenia/0000-0003-1546-9728; Ostergaard,
   Jan/0000-0002-3724-6114
FU Spanish MEC/FEDER Project [TEC 2010-18009]
FX This work was supported by the Spanish MEC/FEDER Project TEC 2010-18009.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Eckehard Steinbach.
CR [Anonymous], 2010, The H.264 Advanced Video Compression Standard
   Brooks AC, 2008, IEEE T IMAGE PROCESS, V17, P1261, DOI 10.1109/TIP.2008.926161
   Chen Y, 2008, IEEE T MULTIMEDIA, V10, P2, DOI 10.1109/TMM.2007.911223
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   Donoho DL, 2008, IEEE T INFORM THEORY, V54, P4789, DOI 10.1109/TIT.2008.929958
   Nguyen D, 2011, INT CONF ACOUST SPEE, P1125
   Gharavi H, 2008, INT CONF ACOUST SPEE, P1153, DOI 10.1109/ICASSP.2008.4517819
   Giacobello D, 2012, IEEE T AUDIO SPEECH, V20, P1644, DOI 10.1109/TASL.2012.2186807
   Gómez AM, 2006, IEEE T MULTIMEDIA, V8, P1228, DOI 10.1109/TMM.2006.884611
   Harrison P., 2005, THESIS MONASH U VICT
   ITU-T, 2010, ITU T REC H 264
   Koloda J, 2012, IEEE DATA COMPR CONF, P159, DOI 10.1109/DCC.2012.24
   Kung WY, 2006, IEEE T CIRC SYST VID, V16, P789, DOI 10.1109/TCSVT.2006.877391
   Li X, 2002, IEEE T CIRC SYST VID, V12, P857, DOI 10.1109/TCSVT.2002.804882
   Little RJ, 1987, STAT ANAL MISSING DA
   Ma MY, 2010, IEEE T CIRC SYST VID, V20, P382, DOI 10.1109/TCSVT.2009.2035839
   Model J., JVT REFERENCE SOFTWA
   Ostergaard J, 2011, EURASIP J ADV SIG PR, DOI 10.1155/2011/857959
   Peinado A.M., 2008, P PIMRC, P1
   Persson D, 2008, IEEE T IMAGE PROCESS, V17, P145, DOI 10.1109/TIP.2007.914151
   Persson D, 2009, IEEE T IMAGE PROCESS, V18, P1048, DOI 10.1109/TIP.2009.2014261
   Robie DL, 2000, INT CONF ACOUST SPEE, P2131, DOI 10.1109/ICASSP.2000.859257
   Romberg J., 2008, IEEE SIGNAL PROCESS, V25
   Salama P., 1995, Proceedings. International Conference on Image Processing (Cat. No.95CB35819), P9, DOI 10.1109/ICIP.1995.529026
   Seiler J, 2008, INT CONF ACOUST SPEE, P781, DOI 10.1109/ICASSP.2008.4517726
   Shirani S, 2000, IEEE J SEL AREA COMM, V18, P1122, DOI 10.1109/49.848261
   Shirani S., 1999, P ICIP, V6, P3117
   Song K, 2007, IEEE INT SYMP CIRC S, P973, DOI 10.1109/ISCAS.2007.378089
   SUN HF, 1995, IEEE T IMAGE PROCESS, V4, P470, DOI 10.1109/83.370675
   Thaipanich T., 2007, P SPIE
   Vandenberghe L, 1996, SIAM REV, V38, P49, DOI 10.1137/1038003
   Varsa V., 2001, SG16 ITUT, V50
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Zhai GT, 2010, IEEE T CIRC SYST VID, V20, P1224, DOI 10.1109/TCSVT.2010.2057019
   Zhai GT, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P621, DOI 10.1109/ICME.2008.4607511
   Zhang RF, 2004, IEEE T CONSUM ELECTR, V50, P335, DOI 10.1109/TCE.2004.1277882
   Zhang X., 2009, P PCS
   Zhang YB, 2012, IEEE T CIRC SYST VID, V22, P12, DOI 10.1109/TCSVT.2011.2130450
   Zhao Y., 2005, P DICTA, P278
   Zhou J, 2011, IEEE T BROADCAST, V57, P75, DOI 10.1109/TBC.2010.2086771
NR 42
TC 36
Z9 42
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2013
VL 15
IS 4
BP 957
EP 969
DI 10.1109/TMM.2013.2238524
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 148FK
UT WOS:000319228500022
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Egilmez, HE
   Civanlar, S
   Tekalp, AM
AF Egilmez, Hilmi E.
   Civanlar, Seyhan
   Tekalp, A. Murat
TI An Optimization Framework for QoS-Enabled Adaptive Video Streaming Over
   OpenFlow Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dynamic quality of service; OpenFlow networks; optimization of routing;
   scalable video; video streaming
ID ALGORITHMS; QUALITY
AB OpenFlow is a programmable network protocol and associated hardware designed to effectively manage and direct traffic by decoupling control and forwarding layers of routing. This paper presents an analytical framework for optimization of forwarding decisions at the control layer to enable dynamic Quality of Service (QoS) over OpenFlow networks and discusses application of this framework to QoS-enabled streaming of scalable encoded videos with two QoS levels. We pose and solve optimization of dynamic QoS routing as a constrained shortest path problem, where we treat the base layer of scalable encoded video as a level-1 QoS flow, while the enhancement layers can be treated as level-2 QoS or best-effort flows. We provide experimental results which show that the proposed dynamic QoS framework achieves significant improvement in overall quality of streaming of scalable encoded videos under various coding configurations and network congestion scenarios.
C1 [Egilmez, Hilmi E.; Tekalp, A. Murat] Koc Univ, Coll Engn, TR-34450 Istanbul, Turkey.
   [Civanlar, Seyhan] Argela Technol, TR-34469 Istanbul, Turkey.
C3 Koc University; Argela
RP Egilmez, HE (corresponding author), Koc Univ, Coll Engn, TR-34450 Istanbul, Turkey.
EM hegilmez@ku.edu.tr; seyhan.civanlar@argela.com.tr
RI Tekalp, Murat/AAW-1060-2020
OI Tekalp, Ahmet Murat/0000-0003-1465-8121
FU European FP7 project SARACEN [FP7-248474]
FX This work was supported in part by the European FP7 project SARACEN
   under Grant FP7-248474. An early version of this work is presented at
   the 18th IEEE International Conference on Image Processing (ICIP 2011),
   Brussels, Belgium, September2011. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr.
   Jiangchuan Liu.
CR [Anonymous], 1998, An Architecture for Differentiated Services, RFC 2475 Informational
   Braden R., 1994, INTEGRATED SERVICES
   Calvert KL, 1997, IEEE COMMUN MAG, V35, P160, DOI 10.1109/35.587723
   Chen SG, 1998, IEEE NETWORK, V12, P64, DOI 10.1109/65.752646
   Chen SG, 2008, IEEE ACM T NETWORK, V16, P105, DOI 10.1109/TNET.2007.897965
   Civanlar S, 2010, IEEE GLOBE WORK, P351, DOI 10.1109/GLOCOMW.2010.5700340
   De Couto DSJ, 2005, WIREL NETW, V11, P419, DOI 10.1007/s11276-005-1766-z
   Dijkstra EW., 1959, NUMER MATH, V1, P269, DOI [10.1007/BF01386390, DOI 10.1007/BF01386390]
   Egilmez H. E., 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P2241, DOI 10.1109/ICIP.2011.6116083
   FROST VS, 1994, IEEE COMMUN MAG, V32, P70, DOI 10.1109/35.267444
   Jüttner A, 2001, IEEE INFOCOM SER, P859, DOI 10.1109/INFCOM.2001.916277
   Kim W., 2010, INM/WREN, V10, P1
   Kuipers F, 2002, IEEE COMMUN MAG, V40, P50, DOI 10.1109/MCOM.2002.1106159
   Masip-Bruin X, 2006, COMPUT COMMUN, V29, P563, DOI 10.1016/j.comcom.2005.06.008
   McKeown N, 2008, ACM SIGCOMM COMP COM, V38, P69, DOI 10.1145/1355734.1355746
   Misra S, 2008, IEEE COMMUN SURV TUT, V10, P18, DOI 10.1109/SURV.2008.080404
   Schulzrinne H., 2003, RTP TRANSPORT PROTOC
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Sun HF, 2007, WIREL COMMUN MOB COM, V7, P159, DOI 10.1002/wcm.471
   Wang Z, 1996, IEEE J SEL AREA COMM, V14, P1228, DOI 10.1109/49.536364
   Xiao Y, 2005, LECT NOTES COMPUT SC, V3827, P92, DOI 10.1007/11602613_11
   Xiao Y, 2005, AKCE INT J GRAPHS CO, V2, P63
   Xue GL, 2008, IEEE ACM T NETWORK, V16, P656, DOI 10.1109/TNET.2007.900712
   Yap KK, 2010, ACM SIGCOMM COMP COM, V40, P125, DOI 10.1145/1672308.1672331
   Zhang Q, 2008, P IEEE, V96, P64, DOI 10.1109/JPROC.2007.909930
   [No title captured]
NR 26
TC 159
Z9 178
U1 0
U2 35
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2013
VL 15
IS 3
BP 710
EP 715
DI 10.1109/TMM.2012.2232645
PG 6
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 109ZM
UT WOS:000316410900020
DA 2024-07-18
ER

PT J
AU Wang, WJ
   Chang, H
   Goodman, A
   Wucherer, E
   Jamin, S
AF Wang, Wenjie
   Chang, Hyunseok
   Goodman, Adam
   Wucherer, Eric
   Jamin, Sugih
TI Managing Digital Rights for P2P Live Broadcast and Recording on the
   Internet
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Digital rights management; live streaming; network digital video
   recorder; peer-to-peer network; privacy protection
AB Live broadcast over a peer-to-peer (P2P) network imposes a unique set of challenges to a digital rights management (DRM) system. Highly correlated service request arrivals at the start of a live event require peak-load provisioning if clients acquire licenses at playback time. Distributing the license management load across a P2P network requires the digital rights management system to ensure the integrity of both digital rights, the protection of client privacy and, at the same time, system scalability. In this paper we describe the requirements imposed on a digital rights management system in distributing live broadcast over a P2P network and present our design of such a system to meet the above challenges. We discuss the system's operation under a number of threat models and how to extend the system to further improve scalability and support network digital video recording (DVR). We close the paper after presenting some scalability results collected from a production P2P live broadcast network using our DRM design.
C1 [Wang, Wenjie] PPLive, Shanghai, Peoples R China.
   [Chang, Hyunseok] Alcatel Lucent Bell Labs, Holmdel, NJ 07733 USA.
   [Goodman, Adam; Wucherer, Eric; Jamin, Sugih] Univ Michigan, Dept Elect Engn & Comp Sci, Ann Arbor, MI 48109 USA.
C3 Alcatel-Lucent; University of Michigan System; University of Michigan
RP Wang, WJ (corresponding author), PPLive, Shanghai, Peoples R China.
EM wenjiewang@pplive.com; hyunseok@ieee.org; akgood@eecs.umich.edu;
   ewuch@eecs.umich.edu; jamin@eecs.umich.edu
CR auf der Maur R., 2006, ENTERTAINMENT LAW F
   Chang H., 2001, P SPIE ITCOM AUG
   Chang HS, 2011, IEEE ACM T NETWORK, V19, P55, DOI 10.1109/TNET.2010.2056382
   Guth S., 2003, P 3 ACM WORKSH DIG R
   MaxMind, GEOIP IP ADDR LOC TE
   NEUMAN BC, 1994, IEEE COMMUN MAG, V32, P33, DOI 10.1109/35.312841
   Perrig A., 2001, P NETW DISTR SYST SE, P35
   PERRIG A, 2001, P IEEE S SEC PRIV MA
   Recordon D., 2006, P 2 ACM WORKSH DIG I, P11, DOI DOI 10.1145/1179529.1179532
   *TRUST COMP GROUP, 2003, TRUST PLATF MOD MAIN
   Wang W., 2011, P 31 INT C DISTR COM
   Wong CK, 2000, IEEE ACM T NETWORK, V8, P16, DOI 10.1109/90.836475
NR 12
TC 0
Z9 0
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2012
VL 14
IS 6
BP 1538
EP 1545
DI 10.1109/TMM.2012.2186434
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 046YE
UT WOS:000311800400004
DA 2024-07-18
ER

PT J
AU Zhang, X
   Zhang, L
   Wang, XJ
   Shum, HY
AF Zhang, Xiao
   Zhang, Lei
   Wang, Xin-Jing
   Shum, Heung-Yeung
TI Finding Celebrities in Billions of Web Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Face recognition; image annotation; image database; image retrieval
ID LOCAL BINARY PATTERNS; RECOGNITION; SETS
AB In this paper, we present a face annotation system to automatically collect and label celebrity faces from the web. With the proposed system, we have constructed a large-scale dataset called "Celebrities on the Web," which contains 2.45 million distinct images of 421 436 celebrities and is orders of magnitude larger than previous datasets.
   Collecting and labeling such a large-scale dataset pose great challenges on current multimedia mining methods. In this work, a two-step face annotation approach is proposed to accomplish this task. In the first step, an image annotation system is proposed to label an input image with a list of celebrities. To utilize the noisy textual data, we construct a large-scale celebrity name vocabulary to identify candidate names from the surrounding text. Moreover, we expand the scope of analysis to the surrounding text of webpages hosting near-duplicates of the input image. In the second step, the celebrity names are assigned to the faces by label propagation on a facial similarity graph. To cope with the large variance in the facial appearances, a context likelihood is proposed to constrain the name assignment process. In an evaluation on 21 735 faces, both the image annotation system and name assignment algorithm significantly outperform previous techniques.
C1 [Zhang, Xiao] Tsinghua Univ, Inst Adv Study, Beijing 100084, Peoples R China.
   [Zhang, Lei; Wang, Xin-Jing] Microsoft Res Asia, Beijing 100080, Peoples R China.
   [Shum, Heung-Yeung] Microsoft Corp, Redmond, WA 98054 USA.
C3 Tsinghua University; Microsoft; Microsoft Research Asia; Microsoft
RP Zhang, X (corresponding author), Tsinghua Univ, Inst Adv Study, Beijing 100084, Peoples R China.
EM andypassion@gmail.com; leizhang@microsoft.com; xjwang@microsoft.com;
   hshum@microsoft.com
RI Wang, Xin-Jing/C-4505-2014
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   Angelova A, 2005, PROC CVPR IEEE, P494
   [Anonymous], NIPS VANC BRIT COL
   [Anonymous], INT J COMPUT VIS
   [Anonymous], ECCV PRAG CZECH REP
   [Anonymous], LREC MALT
   [Anonymous], 2007, 0749 U MASS
   [Anonymous], ACM MULT WORKSH WEB
   [Anonymous], NIPS VANC BRIT COL D
   [Anonymous], NIPS VANC BRIT COL
   Berg TL, 2004, PROC CVPR IEEE, P848
   Bhole A, 2007, INFORM-J COMPUT INFO, V31, P463
   Cao Y, 2010, PROC CVPR IEEE, P3352, DOI 10.1109/CVPR.2010.5540021
   Cao ZM, 2010, PROC CVPR IEEE, P2707, DOI 10.1109/CVPR.2010.5539992
   Chan CH, 2007, LECT NOTES COMPUT SC, V4642, P809
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Cour T, 2009, PROC CVPR IEEE, P919, DOI 10.1109/CVPRW.2009.5206667
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Gao W, 2008, IEEE T SYST MAN CY A, V38, P149, DOI 10.1109/TSMCA.2007.909557
   Guillaumin M., 2008, Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition, P1
   Guillaumin M, 2010, LECT NOTES COMPUT SC, V6311, P634, DOI 10.1007/978-3-642-15549-9_46
   Kemelmacher-Shlizerman I, 2010, LECT NOTES COMPUT SC, V6311, P341, DOI 10.1007/978-3-642-15549-9_25
   Liu C., 2008, MM'08: Proceeding of the 16th ACM international conference on Multimedia, P717
   Nie Zaiqing., 2007, World Wide Web Conference Series, P81
   Ozkan D., 2006, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, V2, P1477
   Ozkan D, 2006, LECT NOTES COMPUT SC, V4071, P173
   Pham PT, 2010, IEEE T MULTIMEDIA, V12, P13, DOI 10.1109/TMM.2009.2036232
   Phillips PJ, 2005, PROC CVPR IEEE, P947
   Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790
   Ramanan D., 2007, P ICCV, P1
   Sargin ME, 2009, INT CONF ACOUST SPEE, P1977, DOI 10.1109/ICASSP.2009.4959999
   Sim T, 2003, IEEE T PATTERN ANAL, V25, P1615, DOI 10.1109/TPAMI.2003.1251154
   Stone Z., 2008, PROC 1 IEEE WORKSHOP, P1
   Tan XY, 2007, LECT NOTES COMPUT SC, V4778, P235
   TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71
   Wang B, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P353, DOI 10.1109/ICME.2006.262509
   Wang XJ, 2010, PROC CVPR IEEE, P2987, DOI 10.1109/CVPR.2010.5540046
   Wu Z, 2010, PROC CVPR IEEE, P3469, DOI 10.1109/CVPR.2010.5539976
   Zhang GC, 2004, LECT NOTES COMPUT SC, V3338, P179
   Zhang X, 2009, IEEE I CONF COMP VIS, P1103, DOI 10.1109/ICCV.2009.5459354
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
   Zhao M., 2008, FG, P1
NR 42
TC 68
Z9 71
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2012
VL 14
IS 4
SI SI
BP 995
EP 1007
DI 10.1109/TMM.2012.2186121
PN 1
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 976QL
UT WOS:000306599300006
DA 2024-07-18
ER

PT J
AU Kim, K
   Seo, J
   Beack, S
   Kang, K
   Hahn, M
AF Kim, Kwangki
   Seo, Jeongil
   Beack, Seungkwon
   Kang, Kyeongok
   Hahn, Minsoo
TI Spatial Audio Object Coding With Two-Step Coding Structure for
   Interactive Audio Service
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio object; interactive audio service; residual coding; spatial audio
   object coding
AB An interactive audio service is a new conceptual audio service that provides the users with opportunities for a variety of experiences on the alternative and advanced audio services. In the interactive audio service, users can freely control various audio objects to make their own audio sounds. A spatial audio object coding (SAOC) is a useful technology that can support most parts of the interactive audio service with a relatively low bit-rate, but is very poor to perfect gain control of a certain audio object, i.e., the target audio object. In this paper, the SAOC with a two-step coding structure is proposed to efficiently handle the target audio object as well as the normal audio objects. A transform coded excitation (TCX) based residual coding scheme is presented in the context of the sound quality enhancement. From experimental results, it can be noted that the various audio objects can be successfully handled with respect to the bit-rate and the sound quality by using the proposed two-step coding structure SAOC.
C1 [Kim, Kwangki] Korea Adv Inst Sci & Technol, Dept Informat & Commun Engn, Taejon 305701, South Korea.
   [Seo, Jeongil; Beack, Seungkwon; Kang, Kyeongok] Elect & Telecommun Res Inst, Taejon 305606, South Korea.
   [Hahn, Minsoo] Korea Adv Inst Sci & Technol, Dept Elect Engn, Taejon 305701, South Korea.
C3 Korea Advanced Institute of Science & Technology (KAIST); Electronics &
   Telecommunications Research Institute - Korea (ETRI); Korea Advanced
   Institute of Science & Technology (KAIST)
RP Kim, K (corresponding author), Korea Adv Inst Sci & Technol, Dept Informat & Commun Engn, Taejon 305701, South Korea.
EM k2kim@kaist.ac.kr; seoji@etri.re.kr; skbeack@etri.re.kr;
   kokang@etri.re.kr; mshahn@ee.kaist.ac.kr
RI Hahn, Minsoo/C-1746-2011
CR [Anonymous], 2010, 20032 ISOIEC
   [Anonymous], 2001, 15431 ITUR BS
   [Anonymous], 2007, M14985 ISOIEC JTC1SC
   [Anonymous], 2007, N8853 ISOIEC JTC1SC2
   [Anonymous], 2003, 26109 3GPP TS
   [Anonymous], 2008, M15390 ISOIEC JTC1SC
   Bessette B., 1999, 1999 IEEE Workshop on Speech Coding Proceedings. Model, Coders, and Error Criteria (Cat. No.99EX351), P7, DOI 10.1109/SCFT.1999.781466
   Bessette B, 2005, INT CONF ACOUST SPEE, P301
   Breebaart J., 2008, P 124 AES CONV AMST
   CONWAY JH, 1983, IEEE T INFORM THEORY, V29, P820, DOI 10.1109/TIT.1983.1056761
   Faller C, 2003, IEEE T SPEECH AUDI P, V11, P520, DOI 10.1109/TSA.2003.818108
   Herre J., 2005, P 118 AES CONV BARC
   Herre J, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P1894
   *ISO IEC, 2006, 1449632005 ISOIEC
   *ISO IEC, 2007, 230031 ISOIEC
   Jang D., 2006, P 121 AES CONV SAN F
   Kim K, 2007, ETRI J, V29, P99, DOI 10.4218/etrij.07.0206.0149
   LEFEBVRE R, 1994, INT CONF ACOUST SPEE, P193
   Park J., 2011, P IEEE INT C CONS EL, P838
   Park J, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 3 AND 4, P2906
   Ragot S, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL I, PROCEEDINGS, P501
   Xie M., 1996, IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Atlanta, GA, USA, V1, P240
   Zwicker E., 1999, PSYCHOACOUSTICS, V2nd, DOI DOI 10.1007/978-3-662-09562-1
NR 23
TC 14
Z9 17
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2011
VL 13
IS 6
BP 1208
EP 1216
DI 10.1109/TMM.2011.2168197
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 852GD
UT WOS:000297343400003
DA 2024-07-18
ER

PT J
AU Ma, L
   Li, SN
   Zhang, F
   Ngan, KN
AF Ma, Lin
   Li, Songnan
   Zhang, Fan
   Ngan, King Ngi
TI Reduced-Reference Image Quality Assessment Using Reorganized DCT-Based
   Image Representation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE City-block distance; generalized Gaussian density (GGD); human visual
   system (HVS); image quality assessment (IQA); reduced-reference (RR)
AB In this paper, a novel reduced-reference (RR) image quality assessment (IQA) is proposed by statistical modeling of the discrete cosine transform (DCT) coefficient distributions. In order to reduce the RR data rates and further exploit the identical nature of the coefficient distributions between adjacent DCT subbands, the DCT coefficients are reorganized into a three-level coefficient tree. Subsequently, generalized Gaussian density (GGD) is employed to model the coefficient distribution of each reorganized DCT subband. The city-block distance is employed to measure the difference between the two images. Experimental results demonstrate that only a small number of RR features is sufficient for representing the image perceptual quality. The proposed method outperforms the RR WNISM and even the full-reference (FR) quality metric PSNR.
C1 [Ma, Lin; Li, Songnan; Zhang, Fan; Ngan, King Ngi] Chinese Univ Hong Kong, Dept Elect Engn, Shatin, Hong Kong, Peoples R China.
C3 Chinese University of Hong Kong
RP Ma, L (corresponding author), Chinese Univ Hong Kong, Dept Elect Engn, Shatin, Hong Kong, Peoples R China.
EM lma@ee.cuhk.edu.hk; snli@ee.cuhk.edu.hk; fzhang@ee.cuhk.edu.hk;
   knngan@ee.cuhk.edu.hk
RI Ngan, N/E-8240-2014
OI Ngan, N/0000-0003-1946-3235
FU Chinese University of Hong Kong [1903003]
FX This work was supported in part by a grant from the Chinese University
   of Hong Kong under the Focused Investment Scheme (Project 1903003). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Andrea Cavallaro.
CR Ahumada A. J.  Jr., 1992, Proceedings of the SPIE - The International Society for Optical Engineering, V1666, P365, DOI 10.1117/12.135982
   [Anonymous], 2006, MODERN IMAGE QUALITY
   Cover T.M., 1991, ELEMENT INFORM THEOR
   Ferzli R, 2009, IEEE T IMAGE PROCESS, V18, P717, DOI 10.1109/TIP.2008.2011760
   Gao XB, 2009, IEEE T IMAGE PROCESS, V18, P1409, DOI 10.1109/TIP.2009.2018014
   Lam EY, 2000, IEEE T IMAGE PROCESS, V9, P1661, DOI 10.1109/83.869177
   Le Callet P, 2006, IEEE T NEURAL NETWOR, V17, P1316, DOI 10.1109/TNN.2006.879766
   Lee S, 2002, IEEE T MULTIMEDIA, V4, P129, DOI 10.1109/6046.985561
   Li BW, 2003, J NEUROPHYSIOL, V90, P204, DOI 10.1152/jn.00954.2002
   Li L, 2010, IEEE T IMAGE PROCESS, V19, P1371, DOI 10.1109/TIP.2010.2041414
   Li Q, 2009, IEEE J-STSP, V3, P202, DOI 10.1109/JSTSP.2009.2014497
   PETERSON HA, 1993, P SOC PHOTO-OPT INS, V1913, P191, DOI 10.1117/12.152693
   Sheikh H.R., Live Image Quality Assessment Database
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Sheikh HR, 2005, IEEE T IMAGE PROCESS, V14, P1918, DOI 10.1109/TIP.2005.854492
   SIMONCELLI EP, 1992, IEEE T INFORM THEORY, V38, P587, DOI 10.1109/18.119725
   Simoncelli EP, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pC444
   Tao DC, 2009, IEEE T SYST MAN CY B, V39, P1623, DOI 10.1109/TSMCB.2009.2021951
   Video Quality Experts Group, 2000, Final report from the video quality experts group on the validation of objective models of video quality assessment march 2000
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   WANG Z, 2005, P SPIE HUM VIS EL IM
   Wang Z, 2006, IEEE T IMAGE PROCESS, V15, P1680, DOI 10.1109/TIP.2005.864165
   Xiong ZX, 1999, IEEE T CIRC SYST VID, V9, P692, DOI 10.1109/76.780358
   Yang KC, 2007, IEEE T MULTIMEDIA, V9, P1528, DOI 10.1109/TMM.2007.906576
   Yu ZH, 2002, P IEEE, V90, P154, DOI 10.1109/5.982412
   Zhai GT, 2008, IEEE T MULTIMEDIA, V10, P1316, DOI 10.1109/TMM.2008.2004910
   Zhao DB, 2002, IEEE T CIRC SYST VID, V12, P819, DOI 10.1109/TCSVT.2002.803218
NR 28
TC 127
Z9 142
U1 0
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2011
VL 13
IS 4
BP 824
EP 829
DI 10.1109/TMM.2011.2109701
PG 6
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 805EX
UT WOS:000293710300020
DA 2024-07-18
ER

PT J
AU Reale, MJ
   Canavan, S
   Yin, LJ
   Hu, KN
   Hung, T
AF Reale, Michael J.
   Canavan, Shaun
   Yin, Lijun
   Hu, Kaoning
   Hung, Terry
TI A Multi-Gesture Interaction System Using a 3-D Iris Disk Model for Gaze
   Estimation and an Active Appearance Model for 3-D Hand Pointing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Gaze estimation; hand tracking; human-computer interaction (HCI)
ID TRACKING
AB In this paper, we present a vision-based human-computer interaction system, which integrates control components using multiple gestures, including eye gaze, head pose, hand pointing, and mouth motions. To track head, eye, and mouth movements, we present a two-camera system that detects the face from a fixed, wide-angle camera, estimates a rough location for the eye region using an eye detector based on topographic features, and directs another active pan-tilt-zoom camera to focus in on this eye region. We also propose a novel eye gaze estimation approach for point-of-regard (POR) tracking on a viewing screen. To allow for greater head pose freedom, we developed a new calibration approach to find the 3-D eyeball location, eyeball radius, and fovea position. Moreover, in order to get the optical axis, we create a 3-D iris disk by mapping both the iris center and iris contour points to the eyeball sphere. We then rotate the fovea accordingly and compute the final, visual axis gaze direction. This part of the system permits natural, non-intrusive, pose-invariant POR estimation from a distance without resorting to infrared or complex hardware setups. We also propose and integrate a two-camera hand pointing estimation algorithm for hand gesture tracking in 3-D from a distance. The algorithms of gaze pointing and hand finger pointing are evaluated individually, and the feasibility of the entire system is validated through two interactive information visualization applications.
C1 [Reale, Michael J.; Canavan, Shaun; Yin, Lijun; Hu, Kaoning] SUNY Binghamton, Dept Comp Sci, Binghamton, NY 13902 USA.
   [Hung, Terry] Corning Corp, Taichung 40763, Taiwan.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Binghamton; Corning Inc
RP Reale, MJ (corresponding author), SUNY Binghamton, Dept Comp Sci, Binghamton, NY 13902 USA.
EM lijun@cs.binghamton.edu
OI Reale, Michael/0000-0003-2147-4443
FU Air Force Research Laboratory [FA8750-08-1-0096]; National Science
   Foundation [IIS-1051103]; New York State Science and Technology Office
   [C050040]; Div Of Information & Intelligent Systems; Direct For Computer
   & Info Scie & Enginr [1051103] Funding Source: National Science
   Foundation
FX This work was supported in part by the Air Force Research Laboratory
   (FA8750-08-1-0096), in part by the National Science Foundation
   (IIS-1051103), and in part by the New York State Science and Technology
   Office (C050040). The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Zhengyou Zhang.
CR [Anonymous], MACH VISION APPL
   [Anonymous], P BRAZ S COMP GRAPH
   [Anonymous], P INT C MULT INF RET
   [Anonymous], P ACM SIGCHI INT C A
   [Anonymous], P CHI 2006
   [Anonymous], P IEEE INT C AUT FAC
   [Anonymous], P NORD C HUM COMP IN
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], SEE MACH FAC
   [Anonymous], THESIS BINGHAMTON U
   [Anonymous], 2008, P 52 ANN M ISSS 2008
   [Anonymous], P ACM SIGGRAPH 2007
   Bernardin K, 2009, PERS UBIQUIT COMPUT, V13, P25, DOI 10.1007/s00779-007-0175-y
   Beymer D, 2003, PROC CVPR IEEE, P451
   Colombo C, 2003, IEEE T SYST MAN CY B, V33, P677, DOI 10.1109/TSMCB.2003.814281
   Colombo C, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1314303.1314305
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Hansen DW, 2010, IEEE T PATTERN ANAL, V32, P478, DOI 10.1109/TPAMI.2009.30
   Harrison C, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P507
   Jojic N., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P468, DOI 10.1109/AFGR.2000.840676
   Kölsch M, 2004, INT C PATT RECOG, P107, DOI 10.1109/ICPR.2004.1334480
   Kölsch M, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P614, DOI 10.1109/AFGR.2004.1301601
   Li D., 2005, IEEE C COMPUTER VISI, V3, P79
   Manders C., 2008, PROC IEEE INT C AUTO, P1
   Moeslund TB, 2001, COMPUT VIS IMAGE UND, V81, P231, DOI 10.1006/cviu.2000.0897
   Nguyen K., 2002, Proceedings ETRA 2002. Eye Tracking Research and Applications Symposium, P133, DOI 10.1145/507072.507099
   Nguyen Thuy Tuong., 2008, 2008 IEEE International Conference on Industrial Technology, P1, DOI [DOI 10.1109/ICIT.2008.4608701, 10.1109/ICIT.2008.4608701]
   Noureddin B, 2005, COMPUT VIS IMAGE UND, V98, P52, DOI 10.1016/j.cviu.2004.07.005
   Ohno T., 2004, Proc. ACM Symp. Eye Tracking Res. Appl, V22, P115, DOI [DOI 10.1145/968363.968387, 10.1145/968363.968387]
   Oka K, 2002, IEEE COMPUT GRAPH, V22, P64, DOI 10.1109/MCG.2002.1046630
   Pavlovic VI, 1997, IEEE T PATTERN ANAL, V19, P677, DOI 10.1109/34.598226
   Pogalin E, 2007, THIRD INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P57
   Press W. H., 2002, Numerical Recipes in C: the Art of Scientific Computing, V2nd ed., DOI DOI 10.1119/1.14981
   Schnieders D, 2010, PROC CVPR IEEE, P1442, DOI 10.1109/CVPR.2010.5539799
   Utsumi A, 2002, IEEE WORKSHOP ON KNOWLEDGE MEDIA NETWORKING, PROCEEDINGS, P31, DOI 10.1109/KMN.2002.1115159
   Vezhnevets V., 2003, P GRAPHICON, P81
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wachs JP, 2011, COMMUN ACM, V54, P60, DOI 10.1145/1897816.1897838
   Wang HG, 2001, IMAGE VISION COMPUT, V19, P891, DOI 10.1016/S0262-8856(01)00051-8
   Wang J, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1314303.1314306
   Wang R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531397
   Wu HY, 2007, LECT NOTES COMPUT SC, V4843, P688
   Wu Y, 1999, LECT NOTES ARTIF INT, V1739, P103
   Xia DS, 2007, SNPD 2007: EIGHTH ACIS INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING, ARTIFICIAL INTELLIGENCE, NETWORKING, AND PARALLEL/DISTRIBUTED COMPUTING, VOL 1, PROCEEDINGS, P220, DOI 10.1109/SNPD.2007.237
   Xie J, 2007, PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON IMAGE AND GRAPHICS, P672, DOI 10.1109/ICIG.2007.160
   Yamamoto Y, 2004, INT C PATT RECOG, P965, DOI 10.1109/ICPR.2004.1333934
   Yamazaki H., 2008, Proc.4th IET Int. Conf. Railway Condition Monit, P1
   Zeng ZH, 2009, IEEE T PATTERN ANAL, V31, P39, DOI 10.1109/TPAMI.2008.52
   Zhai Shumin., 1999, Proceedings of CHI, P246, DOI [10.1145/302979.303053 10.1145/302979.303053, DOI 10.1145/302979.3030532, DOI 10.1145/302979.303053]
NR 49
TC 57
Z9 68
U1 0
U2 27
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2011
VL 13
IS 3
BP 474
EP 486
DI 10.1109/TMM.2011.2120600
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 765UI
UT WOS:000290733700007
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Hua, S
   Guo, Y
   Liu, Y
   Liu, H
   Panwar, SS
AF Hua, Sha
   Guo, Yang
   Liu, Yong
   Liu, Hang
   Panwar, Shivendra S.
TI Scalable Video Multicast in Hybrid 3G/Ad-Hoc Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Ad-hoc video relay; resource allocation; scalable video coding
ID TRANSMISSION; BROADCAST
AB Mobile video broadcasting service, or mobile TV, is expected to become a popular application for 3G wireless network operators. Most existing solutions for video Broadcast Multicast Services (BCMCS) in 3G networks employ a single transmission rate to cover all viewers. The system-wide video quality of the cell is therefore throttled by a few viewers close to the boundary, and is far from reaching the social-optimum allowed by the radio resources available at the base station. In this paper, we propose a novel scalable video broadcast/multicast solution, SV-BCMCS, that efficiently integrates scalable video coding, 3G broadcast, and ad-hoc forwarding to balance the system-wide and worst-case video quality of all viewers at 3G cell. We solve the optimal resource allocation problem in SV-BCMCS and develop practical helper discovery and relay routing algorithms. Moreover, we analytically study the gain of using ad-hoc relay, in terms of users' effective distance to the base station. Through extensive real video sequence driven simulations, we show that SV-BCMCS significantly improves the system-wide perceived video quality. The users' average PSNR increases by as much as 1.70 dB with slight quality degradation for the few users close to the 3G cell boundary.
C1 [Hua, Sha; Liu, Yong; Panwar, Shivendra S.] NYU, Dept Elect & Comp Engn, Polytech Inst, Brooklyn, NY 11201 USA.
   [Guo, Yang] Bell Labs, Serv Infrastruct Res Dept, Holmdel, NJ 07733 USA.
C3 New York University; New York University Tandon School of Engineering;
   AT&T
RP Hua, S (corresponding author), NYU, Dept Elect & Comp Engn, Polytech Inst, Brooklyn, NY 11201 USA.
EM shua01@students.poly.edu; Yang.Guo@al-catel-lucent.com;
   yongliu@poly.edu; liu1999@ieee.org; panwar@catt.poly.edu
RI Panwar, Shivendra S/A-6884-2016; Panwar, Shivendra/K-6473-2019
OI Panwar, Shivendra S/0000-0002-9822-6838; 
FU New York State Center for Advanced Technology in Telecommunications
   (CATT); Wireless Internet Center for Advanced Technology (WICAT)
FX This work was supported in part by the New York State Center for
   Advanced Technology in Telecommunications (CATT) and in part by the
   Wireless Internet Center for Advanced Technology (WICAT). The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Beatrice Pesquet-Popescu.
CR *3GPP2, 2006, CS0054AV10060220 3GP
   Agashe P, 2004, IEEE COMMUN MAG, V42, P83, DOI 10.1109/MCOM.2003.1267104
   [Anonymous], 2006, DSTOTN0722
   [Anonymous], SVC test sequences
   Bender P, 2000, IEEE COMMUN MAG, V38, P70, DOI 10.1109/35.852034
   Bhatia R, 2006, IEEE T MOBILE COMPUT, V5, P1004, DOI 10.1109/TMC.2006.116
   Camp T, 2002, WIREL COMMUN MOB COM, V2, P483, DOI 10.1002/wcm.72
   Hsieh HY, 2004, IEEE T MOBILE COMPUT, V3, P57, DOI 10.1109/TMC.2004.1261817
   Hu DL, 2010, IEEE J SEL AREA COMM, V28, P434, DOI 10.1109/JSAC.2010.100414
   HUA S, 2009, P IEEE GLOB TEL C GL
   HUA S, 2009, SCALABLE VIDEO MULTI
   HWANG CS, 2002, P IEEE INT C COMM IC
   Kim J, 2008, IEEE T CONSUM ELECTR, V54, P1609, DOI 10.1109/TCE.2008.4711209
   Li P., 2009, P 17 IEEE INT C NETW
   Luo H., 2003, P ACM MOBICOM
   Mao SW, 2003, IEEE J SEL AREA COMM, V21, P1721, DOI 10.1109/JSAC.2003.815965
   Park JC, 2005, P IEEE WIR COMM NETW
   Schierl T, 2008, J VIS COMMUN IMAGE R, V19, P500, DOI 10.1016/j.jvcir.2008.06.004
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Shi J., 2006, P IEEE INT C COMM IC
   SINKAR K, 2008, P IEEE SECON
   Stuhlmüller K, 2000, IEEE J SEL AREA COMM, V18, P1012, DOI 10.1109/49.848253
   Tan WT, 2001, IEEE T CIRC SYST VID, V11, P373, DOI 10.1109/76.911162
   van der Schaar M, 2003, IEEE J SEL AREA COMM, V21, P1752, DOI 10.1109/JSAC.2003.815231
   Wang J, 2004, IEEE COMMUN MAG, V42, P76, DOI 10.1109/MCOM.2003.1267103
   Zhang H., 2009, P IEEE WIR COMM NETW
NR 26
TC 58
Z9 64
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2011
VL 13
IS 2
BP 402
EP 413
DI 10.1109/TMM.2010.2103929
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 738SG
UT WOS:000288661800020
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Lin, WS
   Zhao, HV
   Liu, KJR
AF Lin, W. Sabrina
   Zhao, H. Vicky
   Liu, K. J. Ray
TI Game-Theoretic Strategies and Equilibriums in Multimedia Fingerprinting
   Social Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE bargaining; collusion; game theory; multimedia fingerprinting;
   multimedia social network
ID BEHAVIOR FORENSICS; WATERMARKING; COLLUSION; ATTACKS
AB Multimedia social network is a network infrastructure in which the social network users share multimedia contents with all different purposes. Analyzing user behavior in multimedia social networks helps design more secured and efficient multimedia and networking systems. Multimedia fingerprinting protects multimedia from illegal alterations and multiuser collusion is a cost-effective attack. The colluder social network is naturally formed during multiuser collusion with which colluders gain reward by redistributing the colluded multimedia contents. Since the colluders have conflicting interest, the maximal-payoff collusion for one colluder may not be the maximal-payoff collusion for others. Hence, before a collusion being successful, the colluders must bargain with each other to reach agreements. We first model the bargaining behavior among colluders as a noncooperative game and study four different bargaining solutions of this game. Moreover, the market value of the redistributed multimedia content is often time-sensitive. The earlier the colluded copy being released, the more the people are willing to pay for it. Thus, the colluders have to reach agreements on how to distribute reward and risk among themselves as soon as possible. This paper further incorporates this time-sensitiveness of the colluders' reward and studies the time-sensitive bargaining equilibrium. The study in this paper reveals the strategies that are optimal for the colluders; thus, all the colluders have no inventive to disagree. Such understanding reduces the possible types of collusion into a small finite set.
C1 [Lin, W. Sabrina; Liu, K. J. Ray] Univ Maryland, Dept Elect & Comp Engn, College Pk, MD 20742 USA.
   [Zhao, H. Vicky] Univ Alberta, Dept Elect & Comp Engn, Edmonton, AB T6G 2V4, Canada.
C3 University System of Maryland; University of Maryland College Park;
   University of Alberta
RP Lin, WS (corresponding author), Univ Maryland, Dept Elect & Comp Engn, College Pk, MD 20742 USA.
EM wylin@umd.edu; vzhao@ece.ualberta.ca; kjrliu@umd.edu
RI Liu, K.J. Ray/C-2798-2009
CR [Anonymous], 1996, 96045 NEC RES I
   Cox IJ, 1998, IEEE J SEL AREA COMM, V16, P587, DOI 10.1109/49.668980
   Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120
   Dittmann J, 2000, J ELECTRON IMAGING, V9, P456, DOI 10.1117/1.1287729
   DOERR G, 2004, P 2004 ACM MULT SEC
   Ergun F, 1999, LECT NOTES COMPUT SC, V1592, P140
   Fudenberg D., 1991, GAME THEORY
   Hartung F, 1998, SIGNAL PROCESS, V66, P283, DOI 10.1016/S0165-1684(98)00011-5
   He S, 2006, IEEE T INF FOREN SEC, V1, P231, DOI 10.1109/TIFS.2006.873597
   Holliman M, 2000, IEEE T IMAGE PROCESS, V9, P432, DOI 10.1109/83.826780
   KIROVSKI D, 2005, P IEEE INT C AC SPEE, V2, P1037
   Lin WS, 2006, IEEE IMAGE PROC, P2293, DOI 10.1109/ICIP.2006.312833
   Lin WS, 2009, IEEE T INF FOREN SEC, V4, P911, DOI 10.1109/TIFS.2009.2033224
   LIN WS, 2008, P IEEE INT IN PRESS
   LIU KJR, 2005, EURASIP BOOK SERIES
   Osborne Martin J, 1994, COURSE GAME THEORY
   Owen G., 1995, GAME THEORY
   Podilchuk CI, 1998, IEEE J SEL AREA COMM, V16, P525, DOI 10.1109/49.668975
   Wang Y., 2001, VIDEO PROCESSING COM
   Wang Z. J., 2004, EURASIP J APPL SIG P, V14, P2142
   Wang ZJ, 2005, IEEE T IMAGE PROCESS, V14, P804, DOI 10.1109/TIP.2005.847284
   ZANE F, 1962, LECT NOTES COMPUTER, V1962, P21
   ZHAO H, IEEE SIGNAL IN PRESS
   Zhao HV, 2006, IEEE T INF FOREN SEC, V1, P311, DOI 10.1109/TIFS.2006.879279
   Zhao HV, 2006, IEEE T INF FOREN SEC, V1, P440, DOI 10.1109/TIFS.2006.885023
   Zhao HV, 2005, IEEE T IMAGE PROCESS, V14, P646, DOI 10.1109/TIP.2005.846035
NR 26
TC 14
Z9 15
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2011
VL 13
IS 2
BP 191
EP 205
DI 10.1109/TMM.2010.2102345
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 738SG
UT WOS:000288661800003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Stavropoulos, G
   Moschonas, P
   Moustakas, K
   Tzovaras, D
   Strintzis, MG
AF Stavropoulos, Georgios
   Moschonas, Panagiotis
   Moustakas, Konstantinos
   Tzovaras, Dimitrios
   Strintzis, Michael Gerassimos
TI 3-D Model Search and Retrieval From Range Images Using Salient Features
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3-D search; partial matching; range image; salient features
ID 3D; REPRESENTATION; RECOGNITION
AB This paper presents a novel framework for partial matching and retrieval of 3-D models based on a query-by-range-image approach. Initially, salient features are extracted for both the query range image and the 3-D target model. The concept behind the proposed algorithm is that, for a 3-D object and a corresponding query range image, there should be a virtual camera with such intrinsic and extrinsic parameters that would generate an optimum range image, in terms of minimizing an error function that takes into account the salient features of the objects, when compared to other parameter sets or other target 3-D models. In the context of the developed framework, a novel method is also proposed to hierarchically search in the parameter space for the optimum solution. Experimental results illustrate the efficiency of the proposed approach even in the presence of noise or occlusion.
C1 [Stavropoulos, Georgios; Moschonas, Panagiotis; Moustakas, Konstantinos; Tzovaras, Dimitrios; Strintzis, Michael Gerassimos] Ctr Res & Technol Hellas, Informat & Telemat Inst, Thessaloniki, Greece.
C3 Centre for Research & Technology Hellas
RP Stavropoulos, G (corresponding author), Ctr Res & Technol Hellas, Informat & Telemat Inst, Thessaloniki, Greece.
EM moustak@iti.gr
RI Tzovaras, Dimitrios/ABB-9576-2021
OI Tzovaras, Dimitrios/0000-0001-6915-6722; Moustakas,
   Konstantinos/0000-0001-7617-227X
CR [Anonymous], 2001, An Invitation to 3-D Vision
   [Anonymous], THESIS U LEIPZIG LEI
   Ansary TF, 2007, IEEE T MULTIMEDIA, V9, P78, DOI 10.1109/TMM.2006.886359
   Atmosukarto I, 2008, P 1 ACM INT C MULTIM, P208
   BESPALOV D, 2003, P DETC 03
   Bespalov D., 2003, Proceedings of the Eighth ACM Symposium on Solid Modeling and Applications, P208, DOI DOI 10.1145/781606.781638
   Biasotti S, 2003, LECT NOTES COMPUT SC, V2886, P194
   Bustos B, 2005, ACM COMPUT SURV, V37, P345, DOI 10.1145/1118890.1118893
   Caetano TS, 2006, IEEE T PATTERN ANAL, V28, P1646, DOI 10.1109/TPAMI.2006.207
   CHEN DY, 2002, P COMP GRAPH WORKSH
   CHEN DY, 2003, P COMP GRAPH FOR EG, V22
   Chua CS, 1997, INT J COMPUT VISION, V25, P63, DOI 10.1023/A:1007981719186
   Cicirello V, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P176, DOI 10.1109/SMA.2001.923388
   Daras P, 2006, IEEE T MULTIMEDIA, V8, P101, DOI 10.1109/TMM.2005.861287
   DARAS P, 2003, INT J COMPU IN PRESS
   El-Mehalawi M, 2003, COMPUT AIDED DESIGN, V35, P83, DOI 10.1016/S0010-4485(01)00177-4
   Elad M, 2002, SPRING EUROGRAP, P107
   Funkhouser T, 2003, ACM T GRAPHIC, V22, P83, DOI 10.1145/588272.588279
   Gal R, 2006, ACM T GRAPHIC, V25, P130, DOI 10.1145/1122501.1122507
   Germann M, 2007, 3DIM 2007: SIXTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P81
   Giorgi D., 2007, UUCS2007015
   Hilaga M, 2001, COMP GRAPH, P203, DOI 10.1145/383259.383282
   Hoffman DD, 1997, COGNITION, V63, P29, DOI 10.1016/S0010-0277(96)00791-3
   Ioannidis D, 2007, IEEE T INF FOREN SEC, V2, P623, DOI 10.1109/TIFS.2007.902040
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Kolonias I, 2005, IEEE T MULTIMEDIA, V7, P114, DOI 10.1109/TMM.2004.840605
   Kortgen M., 2003, CENTR EUR SEM COMP G
   LIN HS, 2004, P IEEE MMSP SIEN IT
   Lu XG, 2006, IEEE T PATTERN ANAL, V28, P31, DOI 10.1109/TPAMI.2006.15
   MACRINI D, 2002, P INT C PATT REC ICP
   Mademlis A, 2008, IEEE T MULTIMEDIA, V10, P819, DOI 10.1109/TMM.2008.922790
   MARINI S, 2006, P DAGST SEM 06171
   Matei B, 2006, IEEE T PATTERN ANAL, V28, P1111, DOI 10.1109/TPAMI.2006.148
   Mian AS, 2006, IEEE T PATTERN ANAL, V28, P1584, DOI 10.1109/TPAMI.2006.213
   Mian AS, 2006, INT J COMPUT VISION, V66, P19, DOI 10.1007/s11263-005-3221-0
   Moustakas K, 2007, IEEE T VIS COMPUT GR, V13, P80, DOI 10.1109/TVCG.2007.20
   Nair P, 2009, IEEE T MULTIMEDIA, V11, P611, DOI 10.1109/TMM.2009.2017629
   Ohbuchi R, 2008, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2008, PROCEEDINGS, P93, DOI 10.1109/SMI.2008.4547955
   Osada R, 2002, ACM T GRAPHIC, V21, P807, DOI 10.1145/571647.571648
   Shilane P., 2004, PRINCETON SHAPE BENC
   Shum HY, 1996, PROC CVPR IEEE, P526, DOI 10.1109/CVPR.1996.517122
   Tangelder JWH, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P145, DOI 10.1109/SMI.2004.1314502
   Xiaoqing Li, 2007, 2007 European Conference on Power Electronics and Applications, P1
   Zaharia T, 2001, PROC SPIE, V4304, P133, DOI 10.1117/12.424969
   ZARPALAS D, 2007, EURASIP J APPL SIG P, P207
NR 45
TC 27
Z9 28
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2010
VL 12
IS 7
BP 692
EP 704
DI 10.1109/TMM.2010.2053023
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 670SV
UT WOS:000283448500007
DA 2024-07-18
ER

PT J
AU Farrugia, RA
   Debono, CJ
AF Farrugia, Reuben A.
   Debono, Carl James
TI A Support Vector Machine Approach for Detection and Localization of
   Transmission Errors Within Standard H.263++ Decoders
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Error detection and concealment; error resilient coding; learning
   systems; multimedia communications; video coding; wireless network
ID VIDEO; CONCEALMENT; ALGORITHM
AB Wireless multimedia services are increasingly becoming popular boosting the need for better quality-of-experience (QoE) with minimal costs. The standard codecs employed by these systems remove spatio-temporal redundancies to minimize the bandwidth required. However, this increases the exposure of the system to transmission errors, thus presenting a significant degradation in perceptual quality of the reconstructed video sequences. A number of mechanisms were investigated in the past to make these codecs more robust against transmission errors. Nevertheless, these techniques achieved little success, forcing the transmission to be held at lower bit-error rates (BERs) to guarantee acceptable quality.
   This paper presents a novel solution to this problem based on the error detection capabilities of the transport protocols to identify potentially corrupted group-of-blocks (GOBs). The algorithm uses a support vector machine (SVM) at its core to localize visually impaired macroblocks (MBs) that require concealment within these GOBs. Hence, this method drastically reduces the region to be concealed compared to state-of-the-art error resilient strategies which assume a packet loss scenario. Testing on a standard H.263++ codec confirms that a significant gain in quality is achieved with error detection rates of 97.8% and peak signal-to-noise ratio (PSNR) gains of up to 5.33 dB. Moreover, most of the undetected errors provide minimal visual artifacts and are thus of little influence to the perceived quality of the reconstructed sequences.
C1 [Farrugia, Reuben A.; Debono, Carl James] Univ Malta, Dept Commun & Comp Engn, Msida Msd 2080, Malta.
C3 University of Malta
RP Farrugia, RA (corresponding author), Univ Malta, Dept Commun & Comp Engn, Msida Msd 2080, Malta.
EM rrfarr@eng.um.edu.mt; cjdebo@eng.um.edu.mt
RI Farrugia, Reuben/J-8170-2014
OI Farrugia, Reuben/0000-0001-8106-9891; Debono, Carl
   James/0000-0003-2659-8752
CR [Anonymous], 2005, H263 ITUT
   [Anonymous], 2003, Standard Codecs: Image Compression to Advanced Video Coding
   [Anonymous], 2000, INTRO SUPPORT VECTOR, DOI DOI 10.1017/CBO9780511801389
   Bhattacharyya K, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P1483, DOI 10.1109/ICME.2000.871048
   Budagavi M, 2001, IEEE T IMAGE PROCESS, V10, P252, DOI 10.1109/83.902290
   Chen M, 2006, EURASIP J APPL SIG P, DOI 10.1155/ASP/2006/13438
   Chen MH, 2005, IEEE T MULTIMEDIA, V7, P201, DOI 10.1109/TMM.2005.843367
   Chu WJ, 1998, IEEE T CIRC SYST VID, V8, P74, DOI 10.1109/76.660830
   Dimitrova N, 2002, IEEE MULTIMEDIA, V9, P42, DOI 10.1109/MMUL.2002.1022858
   DUDA RO, 2000, PATTERN CLASSIFICATI, P161
   Fan J, 2007, IEEE T MULTIMEDIA, V9, P939, DOI 10.1109/TMM.2007.900143
   FARRUGIA RA, 2007, P IEEE EUROCON 2007
   FARRUGIA RA, 2007, P IEEE INT C COMM GL
   FARRUGIA RA, 2007, P IEEE INT PICT COD
   FARRUGIA RA, 2007, P IEEE P EUROCON 200
   FARRUGIA RA, 2006, P OPNETWORK 2006 C W
   Farserotu J, 2000, IEEE COMMUN MAG, V38, P128, DOI 10.1109/35.846084
   Girod B, 1999, P IEEE, V87, P1707, DOI 10.1109/5.790632
   Gupta M. M., 2003, STATIC DYNAMIC NEURA, P105
   Hartung F, 2006, IEEE COMMUN MAG, V44, P82, DOI 10.1109/MCOM.2006.1678114
   Hsia SC, 2005, IEEE T MULTIMEDIA, V7, P860, DOI 10.1109/TMM.2005.854432
   Jahn A, 2003, IEEE COMMUN MAG, V41, P36, DOI 10.1109/MCOM.2003.1215637
   Keerthi SS, 2001, NEURAL COMPUT, V13, P637, DOI 10.1162/089976601300014493
   Khan E, 2004, IEEE T CIRC SYST VID, V14, P1294, DOI 10.1109/TCSVT.2004.837018
   Kim J, 2006, IEEE T CIRC SYST VID, V16, P974, DOI 10.1109/TCSVT.2006.877148
   Lehtoranta I, 2002, PROCEEDINGS OF THE 2002 IEEE WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P33
   Park W, 2002, PROC SPIE, V4671, P1, DOI 10.1117/12.453056
   Pickering MR, 1997, TENCON IEEE REGION, P773, DOI 10.1109/TENCON.1997.648538
   Recommendation ITU-T P.910, 1999, P910 ITUT
   Shyu HC, 1999, IEEE T CIRC SYST VID, V9, P937, DOI 10.1109/76.785732
   SONG J, 2001, IEEE T MULTIMEDIA, V3, P415
   Specht D.F., 1988, IEEE International Conference on Neural Networks, V1, P525, DOI DOI 10.1109/ICNN.1988.23820
   Suh JW, 2002, IEEE T BROADCAST, V48, P299, DOI 10.1109/TBC.2002.806797
   Tsekeridou S, 2000, IEEE T CIRC SYST VID, V10, P646, DOI 10.1109/76.845010
   van Dijk TA, 2006, DISCOURSE STUD, V8, P5, DOI 10.1177/1461445606059544
   Wang Hom-Lay, 2006, Implant Dent, V15, P8, DOI 10.1097/01.id.0000204762.39826.0f
   Wang Y, 1999, BIOFACTORS, V9, P3, DOI 10.1002/biof.5520090102
   Wang Y, 1998, P IEEE, V86, P974, DOI 10.1109/5.664283
   Wang Y, 2000, IEEE SIGNAL PROC MAG, V17, P61, DOI 10.1109/79.855913
   WEN J, 1997, P 31 AS C SIGN SYST, P973
   Ye SM, 2003, PROCEEDINGS OF THE 2003 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL II, P368
   Yilmaz A, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 2, PROCEEDINGS, P679
   Zhang Jianhai, 2007, Marine Science Bulletin (Beijing), V9, P3
NR 43
TC 8
Z9 10
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2009
VL 11
IS 7
BP 1323
EP 1330
DI 10.1109/TMM.2009.2030651
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 506GB
UT WOS:000270761300010
OA Green Published
DA 2024-07-18
ER

PT J
AU Park, H
   van der Schaar, M
AF Park, Hyunggon
   van der Schaar, Mihaela
TI Coalition-Based Resource Negotiation for Multimedia Applications in
   Informationally Decentralized Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article; Proceedings Paper
CT 15th IEEE International Conference on Image Processing (ICIP 2008)
CY OCT 12-15, 2008
CL San Diego, CA
SP IEEE Signal Proc Soc
DE Axiomatic bargaining solutions; coalition game; marginal contribution;
   multiuser multimedia resource management; network resource management;
   Shapley value; value of information
ID ALLOCATION
AB Designing efficient and fair solutions for dividing the network resources in a distributed manner among self-interested multimedia users is recently becoming an important research topic because heterogeneous and high bandwidth multimedia applications (users), having different quality-of-service requirements, are sharing the same network. Suitable resource negotiation solutions need to explicitly consider the amount of information exchanged among the users and the computational complexity incurred by the users. In this paper, we propose decentralized solutions for resource negotiation, where multiple autonomous users self-organize into a coalition which shares the same network resources and negotiate the division of these resources by exchanging information about their requirements. We then discuss various resource sharing strategies that the users can deploy based on their exchanged information. Several of these strategies are designed to explicitly consider the utility (i.e., video quality) impact of multimedia applications. In order to quantify the utility benefit derived by exchanging different information, we define a new metric, which we refer to as the value of information. We quantify through simulations the improvements that can be achieved when various information is exchanged between users, and discuss the required complexity at the user side involved in implementing the various resource negotiation strategies.
C1 [Park, Hyunggon; van der Schaar, Mihaela] Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90095 USA.
C3 University of California System; University of California Los Angeles
RP Park, H (corresponding author), Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90095 USA.
EM hgpark@ee.ucla.edu; mihaela@ee.ucla.edu
CR [Anonymous], 2007, MULTIMEDIA OVER IP W
   *AXIS COMM, 2004, DIG VID COMPR
   Barton L, 2007, LECT NOTES ARTIF INT, V4676, P285
   BILLARD EA, 1993, IEEE T SYST MAN CYB, V23, P1265, DOI 10.1109/21.260659
   Boyd S., 2004, CONVEX OPTIMIZATION
   CHALKIADAKIS G, 2004, P INT JOINT C AUT AG, P1088
   GARCIAMOLINA H, 1982, IEEE T COMPUT, V31, P48, DOI 10.1109/TC.1982.1675885
   Gaston M.E., 2005, P 4 INT JOINT C AUTO, P230
   HILL H, 2001, P IEEE GLOB TEL C 20, V4, P2549
   IEEE, 2005, 80211E2005 IEEE
   KALAI E, 1975, ECONOMETRICA, V43, P513, DOI 10.2307/1914280
   Kim JG, 2000, IEEE ACM T NETWORK, V8, P337, DOI 10.1109/90.851980
   Kraus S., 2004, Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems, V4, P588
   LEE TJ, 2003, P IEEE GLOB TEL C 19, V3, P1515
   Monderer D, 1996, GAME ECON BEHAV, V14, P124, DOI 10.1006/game.1996.0044
   Monderer D., 2002, Handbook of Game Theory with Economic Applications, V3, P2055, DOI [10.1016/S1574-0005(02)03017-5, DOI 10.1016/S1574-0005(02)03017-5]
   NOWAK AS, 1994, GAME ECON BEHAV, V6, P150, DOI 10.1006/game.1994.1008
   Osborne Martin J, 1994, COURSE GAME THEORY
   Park H, 2007, IEEE T SIGNAL PROCES, V55, P3496, DOI 10.1109/TSP.2007.893755
   Raman C, 2005, 2005 1ST IEEE INTERNATIONAL SYMPOSIUM ON NEW FRONTIERS IN DYNAMIC SPECTRUM ACCESS NETWORKS, CONFERENCE RECORD, P110
   Shapley Lloyd S, 1953, Contributions to the Theory of Games (AM-28), V2, P307, DOI DOI 10.1515/9781400881970-018
   Shehory O, 1998, ARTIF INTELL, V101, P165, DOI 10.1016/S0004-3702(98)00045-9
   Shehory O, 1999, COMPUT INTELL-US, V15, P218, DOI 10.1111/0824-7935.00092
   Sheu ST, 2001, IEEE J SEL AREA COMM, V19, P2065, DOI 10.1109/49.957320
   SINGH S, 1991, P INT C DISTR COMP S, P464
   Winter E., 2002, THE SHAPLEY VALUE, P2025, DOI DOI 10.1016/S1574-0005(02)03016-3
   Zhao J, 2005, 2005 1ST IEEE INTERNATIONAL SYMPOSIUM ON NEW FRONTIERS IN DYNAMIC SPECTRUM ACCESS NETWORKS, CONFERENCE RECORD, P259
   Zheng HT, 2005, 2005 1st IEEE International Symposium on New Frontiers in Dynamic Spectrum Access Networks, Conference Record, P56
NR 28
TC 10
Z9 11
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2009
VL 11
IS 4
BP 765
EP 779
DI 10.1109/TMM.2009.2017638
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA 449LA
UT WOS:000266330900017
DA 2024-07-18
ER

PT J
AU Wang, M
   Hua, XS
   Tang, JH
   Hong, RC
AF Wang, Meng
   Hua, Xian-Sheng
   Tang, Jinhui
   Hong, Richang
TI Beyond Distance Measurement: Constructing Neighborhood Similarity for
   Video Annotation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Neighborhood similarity; semi-supervised learning; video annotation
AB In the past few years, video annotation has benefited a lot from the progress of machine learning techniques. Recently, graph-based semi-supervised learning has gained much attention in this domain. However, as a crucial factor of these algorithms, the estimation of pairwise similarity has not been sufficiently studied. Generally, the similarity of two samples is estimated based on the Euclidean distance between them. But we will show that the similarity between two samples is not merely related to their distance but also related to the distribution of surrounding samples and labels. It is shown that the traditional distance-based similarity measure may lead to high classification error rates even on several simple datasets. To address this issue, we propose a novel neighborhood similarity measure, which explores the local sample and label distributions. We show that the neighborhood similarity between two samples simultaneously takes into account three characteristics: 1) their distance; 2) the distribution difference of the surrounding samples; and 3) the distribution difference of surrounding labels. Extensive experiments have demonstrated the superiority of neighborhood similarity over the existing distance-based similarity.
C1 [Wang, Meng; Hua, Xian-Sheng] Microsoft Res Asia, Beijing 100080, Peoples R China.
   [Tang, Jinhui; Hong, Richang] Univ Sci & Technol China, Hefei 230027, Peoples R China.
C3 Microsoft Research Asia; Microsoft; Chinese Academy of Sciences;
   University of Science & Technology of China, CAS
RP Wang, M (corresponding author), Microsoft Res Asia, Beijing 100080, Peoples R China.
EM mengwang@microsoft.com; xshua@microsoft.com; tangjh@comp.nus.edu.sg
RI Tang, Jinhui/KBR-0891-2024
CR Aggarwal C. C., 2001, P INT C DAT THEOR
   AMIR A, 2005, P TREC VID RETR EV
   [Anonymous], RC23612W0505104 IBM
   [Anonymous], 22220068 COL U ADVEN
   [Anonymous], 2004, P ADV NEUR INF PROC
   [Anonymous], 2005, SEMISUPERVISED LEARN
   BELKIN M, 2004, P COLT
   BENGIO Y, 2006, BOOK CHAPTER SEMISUP
   Chang C.C., LIBSVM: a library for support vector machines
   Chapelle O., 2006, SEMISUPERVISED LEARN
   Cohen I, 2004, IEEE T PATTERN ANAL, V26, P1553, DOI 10.1109/TPAMI.2004.127
   Friedman J., 2001, ELEMENTS STAT LEARNI, V1, DOI DOI 10.1007/978-0-387-84858-7
   Goldberger J., 2005, ADV NEURAL INFORM PR
   Hastie T, 1998, STAT SCI, V13, P54
   HAUPTMANN AG, 2005, P ACM INT C IM VID R
   Hauptmann A, 2007, IEEE T MULTIMEDIA, V9, P958, DOI 10.1109/TMM.2007.900150
   HE JR, 2004, P ACM MULT
   KRAAJ W, P TRECVID
   Kullback S., 1959, STAT INFORM THEORY
   LIN C., 2003, P INT C MULT EXP
   NAPHADE MR, 2004, P ACM MULT
   Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054
   Sebe N, 2000, IEEE T PATTERN ANAL, V22, P1132, DOI 10.1109/34.879793
   SHIN H, 2006, P EUR C MACH LEARN
   SMEATON AF, 2007, P ACM WORKSH MULT IN
   Snoek CGM, 2006, IEEE T PATTERN ANAL, V28, P1678, DOI 10.1109/TPAMI.2006.212
   Snoek CG, 2005, P ACM MULT
   Song Y., 2005, P ACM INT WORKSH MUL
   TANG J, 2007, P PAC AS C KERN DISC
   TRECVID, TREC VID RETR EV
   WANG M, 2006, P ACM MULT
   WANG M, 2007, P ACM MULT
   Weinberger K., 2006, P ADV NEUR INF PROC, P1
   YAN R, 2005, P INT C COMP VIS PAT
   Yang L., 2006, P AAAI C ART INT
   Yu J, 2008, IEEE T PATTERN ANAL, V30, P451, DOI 10.1109/TPAMI.2007.70714
   YUAN X, 2006, P ACM MULT
   ZAKAI M, 1964, IEEE T INFORM THEORY, V10, P94, DOI 10.1109/TIT.1964.1053633
   ZHU X, 1930, SEMISUPERVISED LEARN
   Zhu X.J, 2003, P 20 INT C MACH LEAR
   TREC 10 P APP COMM E
NR 41
TC 215
Z9 221
U1 0
U2 34
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2009
VL 11
IS 3
BP 465
EP 476
DI 10.1109/TMM.2009.2012919
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 425JH
UT WOS:000264632300012
DA 2024-07-18
ER

PT J
AU Gu, ZW
   Mei, T
   Hua, XS
   Tang, JH
   Wu, XQ
AF Gu, Zhiwei
   Mei, Tao
   Hua, Xian-Sheng
   Tang, Jinhui
   Wu, Xiuqing
TI Multi-Layer Multi-Instance Learning for Video Concept Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multi-layer multi-instance learning; kernel; video concept detection
AB This paper presents a novel learning-based method, called "multi-layer multi-instance (MLMI) learning" for video concept detection. Most of existing methods have treated video as a flat data sequence and have not investigated the intrinsic hierarchy structure of the video content deeply. However, video is essentially a kind of media with ML structure. For example, a video can be represented by a hierarchical structure including, from large to small, shot, frame, and region, where each pair of contiguous layers fits the typical MI setting. We call such a ML structure and the MI relations embedded in the structure as the MLMI setting. In this paper, we systematically study both ML structure and MI relations embedded in video content by formulating video concept detection as a MLMI learning problem. Specifically, we first construct a MLMI kernel to simultaneously model such ML structure and MI relations. To deal with the ambiguity, propagation problem which is introduced by weak labeling and MI, structure, we then propose a regularization framework which takes hyper-bag prediction error, sublayer prediction error, inter-layer inconsistency measure, and classifier complexity into consideration. We have applied the proposed MLMI learning method to concept detection task over TRECVid 2005 development corpus, and report better performance to vector-based and the state-of-the-art MI learning methods.
C1 [Gu, Zhiwei; Tang, Jinhui; Wu, Xiuqing] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230027, Peoples R China.
   [Mei, Tao; Hua, Xian-Sheng] Microsoft Res Asia, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Microsoft; Microsoft Research Asia
RP Gu, ZW (corresponding author), Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230027, Peoples R China.
EM guzhiwei@mail.ustc.edu.cn; tmei@microsoft.com; xshua@microsoft.com;
   jhtang@mail.ustc.edu.cn; wuxq@ustc.edu.cn
RI Mei, Tao/GQZ-0596-2022; Tang, Jinhui/KBR-0891-2024
OI Mei, Tao/0000-0002-5990-7307; 
CR Altun Yasemin., 2006, YAIR WEISS BERNHARD, V18, P33
   AMIR A, 2003, P TRECVID WORKSH GAI
   Andrews Stuart, 2002, PROC 25 ANN C NEURAL, P561
   [Anonymous], TRECVID: TREC Video Retrieval Evaluation
   [Anonymous], 2006, ACM INT C MULTIMEDIA, DOI [10.1145/1180639.1180727, DOI 10.1145/1180639.1180727]
   [Anonymous], 2006, P 8 ACM INT WORKSHOP
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], 1999, UCSC-CRL-99-10
   Chen YX, 2006, IEEE T PATTERN ANAL, V28, P1931, DOI 10.1109/TPAMI.2006.248
   Chen YX, 2004, J MACH LEARN RES, V5, P913
   CHEUNG PM, 2006, P 23 INT C MACH LEAR, P193
   Collins M., 2002, ADV NEURAL INFORM PR, V14
   Deng YN, 2001, IEEE T PATTERN ANAL, V23, P800, DOI 10.1109/34.946985
   DIETEL P, 1897, NATURL PFLANZ, V1, P2
   Gartner T., 2002, P 19 INT C MACH LEAR, V2, P179
   Gartner T., 2003, SIGKDD EXPLORATIONS
   GARTNER T, 2004, MACH LEARN
   GHOSHAL A, 2005, P ACM C RES DEV INF
   Gu Z., 2007, P ACM MULT AUGSB GER, P349
   GU Z, 2008, P 14 INT MULT MOD C, P24
   Hofmann T., 2006, A Review of Kernel Methods in Machine Learning
   JIANG YG, 2007, P CIVR
   Kashima H., 2003, P 20 INT C MACHINE L, P321
   Kwok JT, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P901
   MARON O, 1998, P 15 INT C MACH LEAR, P341
   NAPHADE M, 2005, P INT C AC SPEECH SI
   Naphade M, 2006, IEEE MULTIMEDIA, V13, P86, DOI 10.1109/MMUL.2006.63
   Naphade MR, 2001, IEEE T MULTIMEDIA, V3, P141, DOI 10.1109/6046.909601
   PEKER KA, 2003, J VIS COMMUN IMAGE R, V14
   SMOLA A, 2005, P INT WORKSH ART INT
   Snoek CGM, 2006, IEEE T PATTERN ANAL, V28, P1678, DOI 10.1109/TPAMI.2006.212
   Snoek CG, 2005, P ACM MULT
   TANG J, 2007, P ACM MULT AUGSB GER
   Vapnik V.N., 2000, The Nature of Statistical Learning Theory, DOI DOI 10.1007/978-1-4757-3264-1_1
   YAN R, 2005, P IEEE COMP SOC C CO
   YAN R, 2007, P ACM SIGKDD INT C K
   YUAN J, 2007, TRECVID WORKSH
NR 37
TC 12
Z9 13
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2008
VL 10
IS 8
BP 1605
EP 1616
DI 10.1109/TMM.2008.2007290
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 390KS
UT WOS:000262163600015
DA 2024-07-18
ER

PT J
AU Liu, DY
   Chen, SQ
   Shen, B
AF Liu, Dongyu
   Chen, Songqing
   Shen, Bo
TI Modeling and Optimization of Meta-Caching Assisted Transcoding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptation; CPU intensive computing; meta-caching; transcoding
ID PROXY
AB The increase of aggregate Internet bandwidth and the rapid development of 3G wireless networks demand efficient delivery of multimedia objects to all types of wireless devices. To handle requests from wireless devices at runtime, the transcoding-enabled caching proxy has been proposed to save transcoded versions to reduce the intensive computing demanded by online transcoding. Constrained by available CPU and storage, existing transcoding-enabled caching schemes always selectively cache certain transcoded versions, expecting that many future requests can be served from the cache. But such schemes treat the transcoder as a black box, leaving no room for flexible control of joint resource management between CPU and storage. In this paper, we first introduce the idea of meta-caching by looking into a transcoding procedure. Instead of caching certain selected transcoded versions in full, meta-caching identifies intermediate transcoding steps from which certain intermediate results (called metadata) can be cached so that a fully transcoded version can be easily produced from the metadata with a small amount of CPU cycles. Achieving big saving in caching space with possibly small sacrifice on CPU load, the proposed meta-caching scheme provides a unique method to balance the utilization of CPU and storage resources at the proxy. We further construct a model to analyze the meta-caching scheme. Based on the analysis, we propose AMTrac, Adaptive Meta-caching for Transcoding, which adaptively applies meta-caching based on the client request patterns and available resources. Experimental results show that AMTrac can significantly improve the system throughput over existing approaches.
C1 [Liu, Dongyu; Chen, Songqing] George Mason Univ, Dept Comp Sci, Fairfax, VA 22030 USA.
   [Shen, Bo] Vuclip Com, Vuclip Com Team, Milpitas, CA 95035 USA.
C3 George Mason University
RP Liu, DY (corresponding author), George Mason Univ, Dept Comp Sci, Fairfax, VA 22030 USA.
EM dliu1@cs.gmu.edu; sqchen@cs.gmu.edu; bo.shen@ieee.org
FU U.S. National Science Foundation [CNS-0509061, CNS-0621631,
   CNS-0746649]; Direct For Computer & Info Scie & Enginr; Division Of
   Computer and Network Systems [0746649] Funding Source: National Science
   Foundation
FX Manuscript received December 20, 2006; revised July 29, 2008. Current
   version published December 10, 2008. The work was supported in part by
   the U.S. National Science Foundation under Grants CNS-0509061,
   CNS-0621631, and CNS-0746649. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr. John
   R. Smith.
CR ACHARYA S, 2000, P ACM WORKSH NETW OP
   AMIR E, 1995, P ACM MULT SAN FRANC
   Brewer EA, 1998, IEEE PERS COMMUN, V5, P8, DOI 10.1109/98.729719
   CHERKASOVA L, 2002, P ACM WORKSH NETW OP
   CUETOS PD, 2001, P PACK VID WORKSH KY
   GUO L, 2005, P 25 INT C DISTR COM
   Han R, 1998, IEEE PERS COMMUN, V5, P8, DOI 10.1109/98.736473
   HARTANTO F, 2002, IEEE INT C MULT EXP
   HESS CK, 2000, P SPIE ACM MMCN SAN
   KIM T, 2001, P ACM WORKSH NETW OP
   MOHAN R, 1999, IEEE T MULTIMEDIA, V1
   REJAIE R, 2001, P ACM WORKSH NETW OP
   SARHAN N, 2004, IEEE T PARALLEL DIST
   Shen B, 2004, IEEE T MULTIMEDIA, V6, P375, DOI 10.1109/TMM.2003.822791
   SHEN B, 2003, P IEEE INT C MULT EX, V1
   TAMAI M, 2004, P ACM INT WORKSH NET
   TANG X, 2002, P 31 INT C PAR PROC
   TU Y, 2005, P INT C DAT EXP SYST
   Warabino T, 2000, IEEE COMMUN MAG, V38, P66, DOI 10.1109/35.874971
   Xin J, 2005, P IEEE, V93, P84, DOI 10.1109/JPROC.2004.839620
   YOUR PHONE MOBILE MI
   SUPPORT NATIONWIDE D
   [No title captured]
NR 23
TC 1
Z9 1
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2008
VL 10
IS 8
BP 1444
EP 1454
DI 10.1109/TMM.2008.2007312
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 390KS
UT WOS:000262163600002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Masala, E
   Quaglia, D
   De Martin, JC
AF Masala, Enrico
   Quaglia, Davide
   De Martin, Juan Carlos
TI Variable Time Scale Multimedia Streaming Over IP Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Rate adaptation; variable time-scale streaming; video communication
ID CONGESTION CONTROL; VIDEO; MPEG-4; TRANSMISSION; COMPRESSION;
   SCALABILITY; MECHANISMS
AB This paper presents a comprehensive analysis of a variable time-scale streaming technique, VTSS, according to which rate changes are obtained by varying the inter-packet transmission interval, rather than altering, as in most cases, the source coding rate. Instead of constraining the transmitter to operate in real-time, the time scale of the packet scheduler can vary between zero, when the network is congested, to as faster than real-time as the channel bandwidth allows, when the network is lightly loaded. Although this approach is reportedly used in commercial streaming products, so far the technique has not yet been analyzed in a rigorous fashion, nor it has been compared to other state-of-the-art streaming techniques. This work first presents a theoretical analysis of the performance achievable by the VTSS approach, and it shows that, for the same channel conditions, VTSS yields a total distortion which is lower or, in the worst case, equal than the distortion of the standard real-time source-rate adaptive approach. A lower bound on receiver buffer size is also derived. Network simulations then analyze the performance of a TCP-friendly test implementation of VTSS compared with an ideal real-time source rate-adaptive technique, whose performance, being ideal, represents the upper bound of any transmission scheme based on source rate adaptation. The simulation results, also based on actual network traces, show that the VTSS approach delivers higher perceptual quality (up to 1.2 dB PSNR in the considered scenarios) and reduced video quality fluctuations (1.6 dB standard deviation PSNR, instead of 4.9 dB) for a wide range of standard video sequences. Perceptual quality evaluation by means of PVQM confirms such results. The gains, as expected, are even more pronounced (7.6 dB PSNR on average) if compared to real-time constant bit-rate video transmission.
C1 [Masala, Enrico; De Martin, Juan Carlos] Politecn Torino, Dept Control & Comp Engn, Turin, Italy.
   [Quaglia, Davide] Univ Verona, Dept Comp Sci, I-37100 Verona, Italy.
C3 Polytechnic University of Turin; University of Verona
RP Masala, E (corresponding author), Politecn Torino, Dept Control & Comp Engn, Turin, Italy.
EM masala@polito.it; davide.quaglia@univr.it; demartin@polito.it
RI Masala, Enrico/B-6973-2008
OI Masala, Enrico/0000-0001-8906-354X; Quaglia, Davide/0000-0002-0775-939X
CR *ADB, 2008, ADB 3800W DAT SHEET
   ALDRIDGE R, 1995, P 5 IEE INT C IM PRO, P336
   ALLEN A, 2001, P ACM MULT, P79
   [Anonymous], 2006, RFC4340
   [Anonymous], RFC3448
   [Anonymous], 80216 IEEE
   [Anonymous], 2002, BT50011 ITUR
   Balk A, 2004, COMPUT NETW, V44, P415, DOI 10.1016/j.comnet.2003.12.002
   Bansal D, 2001, IEEE INFOCOM SER, P631, DOI 10.1109/INFCOM.2001.916251
   Barberis A, 2001, COMPUT COMMUN, V24, P757, DOI 10.1016/S0140-3664(00)00349-2
   BOLOT JC, 1994, IEEE INFOCOM SER, P1216, DOI 10.1109/INFCOM.1994.337568
   BOLOT JC, 1993, COMPUT COMMUN REV, V23, P289
   Busse I, 1996, COMPUT COMMUN, V19, P49, DOI 10.1016/0140-3664(95)01038-6
   Cao GH, 2003, COMPUT COMMUN, V26, P639, DOI 10.1016/S0140-3664(02)00197-4
   Chen CM, 2007, SIGNAL PROCESS-IMAGE, V22, P403, DOI 10.1016/j.image.2007.01.002
   DAVINI G, 2003, P IEEE INT C COMM AN, V1, P577
   DEMARCO G, 2004, P IEEE INT C ADV INF, V1, P531
   Fenger J, 1999, ENVIRONM POLLUT SER, V1, P3
   Floyd S, 2000, ACM SIGCOMM COMP COM, V30, P43, DOI 10.1145/347057.347397
   Floyd S, 1999, IEEE ACM T NETWORK, V7, P458, DOI 10.1109/90.793002
   Fung CW, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 2, P67, DOI 10.1109/MMCS.1999.778141
   *GCT, 2008, GCT K35 DAT SHEET GC
   Goyal VK, 2001, IEEE SIGNAL PROC MAG, V18, P74, DOI 10.1109/79.952806
   Hekstra AP, 2002, SIGNAL PROCESS-IMAGE, V17, P781, DOI 10.1016/S0923-5965(02)00056-5
   HEMY M, 1999, P PACK VID WORKSH MA
   Hsu CY, 1999, IEEE J SEL AREA COMM, V17, P756, DOI 10.1109/49.768193
   *ISO IEC, 2001, 144962 ISOIEC
   *ITUT ISO IEC, 2003, H264 ITUT ISOIEC
   Jacobs S, 1997, 1997 IEEE FIRST WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P558, DOI 10.1109/MMSP.1997.602693
   Jacobson V., 1988, Computer Communication Review, V18, P314, DOI 10.1145/52325.52356
   Jin SD, 2003, IEEE ACM T NETWORK, V11, P341, DOI 10.1109/TNET.2003.813046
   Kanakia H, 1995, IEEE ACM T NETWORK, V3, P671, DOI 10.1109/90.477713
   Karczewicz M, 2003, IEEE T CIRC SYST VID, V13, P637, DOI 10.1109/TCSVT.2003.814969
   KAZUNORI U, 2003, P IEEE S APPL INT
   Kozen D, 1998, IEEE DATA COMPR CONF, P229, DOI 10.1109/DCC.1998.672151
   LAUDERDALE J, 1995, P IEEE ATM WORKSH WA
   Li WP, 2001, IEEE T CIRC SYST VID, V11, P301, DOI 10.1109/76.911157
   Liu YX, 2002, IEEE IPCCC, P83, DOI 10.1109/IPCCC.2002.995139
   LU X, 2002, P PACK VID WORKSH PI
   Mahdavi J., 1997, TCP FRIENDLY UNICAST
   MASALA E, 2005, P EUR SIGN PROC C EU
   McCanne S, 1997, IEEE J SEL AREA COMM, V15, P983, DOI 10.1109/49.611154
   McManus JM, 1996, IEEE J SEL AREA COMM, V14, P1087, DOI 10.1109/49.508280
   MUKHERJEE B, 2000, P ICNP 2000 NOV
   *NOK, 2008, NOK 6120 CLASS TECHN
   ODLYZKO A, TELECOM DOGMAS SPECT
   Ortega A, 1998, IEEE SIGNAL PROC MAG, V15, P23, DOI 10.1109/79.733495
   PADHYE J, 1999, P IEEE NOSSDAV JUN
   Padhye Jitendra., 1998, P ACM SIGCOMM, P303
   PANCHA P, 1993, P INFOCOM 93, P902
   Puri R, 2001, SIGNAL PROCESS-IMAGE, V16, P745, DOI 10.1016/S0923-5965(01)00005-4
   Radha HM, 2001, IEEE T MULTIMEDIA, V3, P53, DOI 10.1109/6046.966110
   REJAIE R, 1999, P IEEE INFOCOM MAR
   Salehi JD, 1998, IEEE ACM T NETWORK, V6, P397, DOI [10.1109/90.720873, 10.1142/S0218213097000219]
   Schulzrinne H, 1997, PROCEEDINGS OF THE IEEE 7TH INTERNATIONAL WORKSHOP ON NETWORK AND OPERATING SYSTEM SUPPORT FOR DIGITAL AUDIO AND VIDEO, P65, DOI 10.1109/NOSDAV.1997.629366
   SISALEM D, 1998, P IEEE NOSSDAV CAMBR
   Tan WT, 1999, IEEE T MULTIMEDIA, V1, P172, DOI 10.1109/6046.766738
   *U NAPL COMP DEPT, 2008, NETW TOOLS TRAFF TRA
   *UCB LBNL VINT, 1997, UCB LBNL VINT NETW S
   Vadde KK, 2004, IEEE J SEL AREA COMM, V22, P1335, DOI 10.1109/JSAC.2004.829351
   van der Schaar M, 2001, IEEE T CIRC SYST VID, V11, P318, DOI 10.1109/76.911158
   Vickers BJ, 1997, IEEE J SEL AREA COMM, V15, P512, DOI 10.1109/49.564146
   Wang X, 1999, IEICE T COMMUN, VE82B, P806
   Wang Y, 1998, P IEEE, V86, P974, DOI 10.1109/5.664283
   Wang Y, 2000, IEEE SIGNAL PROC MAG, V17, P61, DOI 10.1109/79.855913
   WINKLER S, 2005, DIGITAL VIDEO QUALIT, P51
   Yang YR, 2000, 2000 INTERNATIONAL CONFERENCE ON NETWORK PROTOCOLS, PROCEEDINGS, P187, DOI 10.1109/ICNP.2000.896303
   Yeadon N, 1996, IEEE J SEL AREA COMM, V14, P1245, DOI 10.1109/49.536366
   YIN NY, 1991, IEEE J SEL AREA COMM, V9, P1003, DOI 10.1109/49.103548
   Zhang Q, 2001, IEEE T MULTIMEDIA, V3, P339, DOI 10.1109/6046.944477
   Zhang ZL, 1997, IEEE J SEL AREA COMM, V15, P1148, DOI 10.1109/49.611165
   Zhang ZL, 1999, IEEE INFOCOM SER, P472, DOI 10.1109/INFCOM.1999.751380
NR 72
TC 2
Z9 2
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2008
VL 10
IS 8
BP 1657
EP 1670
DI 10.1109/TMM.2008.2007284
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 390KS
UT WOS:000262163600020
DA 2024-07-18
ER

PT J
AU Yan, WQ
   Fu, WG
   Kankanhalli, MS
AF Yan, Wei-Qi
   Fu, Wei-Gang
   Kankanhalli, Mohan S.
TI Progressive Audio Scrambling in Compressed Domain
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio scrambling/descrambling; compressed domain; discrete wavelet
   transform; MP3; progressive scrambling
AB Audio scrambling can be employed to ensure confidentiality in audio distribution. We first describe scrambling for raw audio using the Discrete Wavelet Transform (DWT) first and then focus on MP3 audio scrambling. We perform scrambling based on a set of keys which allows for a set of audio outputs having different qualities. During descrambling, the number of keys provided and the number of rounds of descrambling performed will decide the audio output quality. We also perform scrambling by using multiple keys on the MP3 audio format. With a subset of keys, we can descramble to obtain a low quality audio. However, we can obtain the original quality audio by using all of the keys. Our experiments show that the proposed algorithms are effective, fast, simple to implement while providing flexible control over the progressive quality of the audio output. The security level provided by the scheme is sufficient for protecting MP3 music content.
C1 [Yan, Wei-Qi] Queens Univ Belfast, Dept Comp Sci, Belfast BT7 1NN, Antrim, North Ireland.
   [Kankanhalli, Mohan S.] Natl Univ Singapore, Sch Comp, Singapore 117543, Singapore.
C3 Queens University Belfast; National University of Singapore
RP Yan, WQ (corresponding author), Queens Univ Belfast, Dept Comp Sci, Belfast BT7 1NN, Antrim, North Ireland.
EM dc-syanwq@gmail.com; mohan@comp.nus.edu.sg
RI Kankanhalli, Mohan/Q-9284-2019
OI Kankanhalli, Mohan/0000-0002-4846-2015
CR BANSAL M, 2003, P IEEE PCM SING DEC
   Borujeni SE, 2000, ICECS 2000: 7TH IEEE INTERNATIONAL CONFERENCE ON ELECTRONICS, CIRCUITS & SYSTEMS, VOLS I AND II, P290, DOI 10.1109/ICECS.2000.911539
   Cochran W.G., 1977, Sampling techniques, V3rd ed.
   FARKASH S, 1991, P 17 CONV EL EL ENG, P365
   FU WG, 2005, P IEEEISCAS 05 KOB J, P5525
   HARN L, 1991, P S APPL COMP, P430
   Kadambe S., 1994, Proceedings of the IEEE-SP International Symposium on Time-Frequency and Time-Scale Analysis (Cat. No.94TH8007), P632, DOI 10.1109/TFSA.1994.467272
   Kankanhalli MS, 2002, IEEE T CONSUM ELECTR, V48, P356, DOI 10.1109/TCE.2002.1010142
   LAMPORT L, 1981, COMMUN ACM, V24, P770, DOI 10.1145/358790.358797
   Ma FL, 1996, ELECTRON LETT, V32, P719, DOI 10.1049/el:19960471
   Mallat S.G. A., 1989, IEEE T PATTERN ANAL, V11, P7
   MATSUNAGA A, 1989, IEEE J SEL AREA COMM, V7, P540, DOI 10.1109/49.17718
   MILOSEVIC V, 1997, P 13 INT C DIG SIGN, P361
   Qi DX, 2000, SCI CHINA SER E, V43, P304, DOI 10.1007/BF02916835
   Senk V, 1997, IEEE SIGNAL PROC LET, V4, P161, DOI 10.1109/97.586036
   Ten Daubechies I., 1992, lecture on wavelets
   WANG Y, 2004, P ACM MULT 04 NEW YO
   Wu Y, 2002, 2002 6TH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING PROCEEDINGS, VOLS I AND II, P1560, DOI 10.1109/ICOSP.2002.1180094
NR 18
TC 18
Z9 18
U1 1
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2008
VL 10
IS 6
BP 960
EP 968
DI 10.1109/TMM.2008.2001373
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 371WE
UT WOS:000260862600002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Ali, S
   Aarabi, P
AF Ali, Sarah
   Aarabi, Parham
TI A cyclic interface for the presentation of multiple music files
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE audio interface; audio search; interface; music information retrieval;
   world wide web
ID RETRIEVAL
AB This paper proposes a novel cyclic interface for browsing through a song database. The method, which sums multiple audio streams on a server and broadcasts only a single summed stream, allows the user to hear different parts of each audio stream by cycling through all available streams. Songs are summed into a single stream based on a combination of spectral entropy and local power of each song's waveform. Perceptual parameters of the system are determined based on experiments conducted on 20 users, for three, four, and five songs. Results illustrate that the proposed methodology requires less listening time as compared to traditional list-based interfaces when the desired audio clip is among one of the audio streams. Applications of this methodology include any search system which returns multiple audio search results, including music query by example. The proposed methodology can be used for real-time searching with an ordinary internet browser.
C1 [Ali, Sarah] Univ Toronto, Toronto, ON, Canada.
C3 University of Toronto
RP Ali, S (corresponding author), Univ Toronto, Toronto, ON, Canada.
RI Ali, Sarah/JFB-5205-2023
CR ALI S, 2006, P IEEE ICME 2006
   Ambikairajah E, 1997, ELECTRON COMMUN ENG, V9, P165, DOI 10.1049/ecej:19970403
   Bartsch MA, 2005, IEEE T MULTIMEDIA, V7, P96, DOI 10.1109/TMM.2004.840597
   BIRMINGHAM WP, 2006, P IEEE ICME
   Cohen HA, 1997, J VIS COMMUN IMAGE R, V8, P226, DOI 10.1006/jvci.1997.0350
   COLBATH S, 2000, P 33 HAW INT C SYST, P1
   Foote J, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P77, DOI 10.1145/319463.319472
   GHIAS A, 1998, P ACM MULT 1998, P235
   Hiraga R, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P501, DOI 10.1109/VISUAL.2002.1183815
   Hiraga R., 2004, P WORKING C ADV VISU, P103, DOI DOI 10.1145/989863.989878
   JANG JS, 2001, P ICME 2001 AUG 22 2, P405
   KOSUGI N, 2000, MULTIMEDIA 00, P333
   KURTH F, 2002, P AUD ENG SOC CONV, P1
   Likert R., 1932, ARCH PSYCHOL, V140
   Logan B, 2000, INT CONF ACOUST SPEE, P749
   MAVANDADI S, 2006, P IEEE ICME 2006
   Pampalk E., 2002, Proceedings of the tenth ACM international conference on Multimedia, P570, DOI DOI 10.1145/641007.641121
   PIKRAKIS A, 2005, EURASIP NEWSLETTER, V16, P9
   PONCELEON D, 1999, P 7 ACM INT C MULT O, P199
   Smith SM, 1997, VISUALIZATION '97 - PROCEEDINGS, P499, DOI 10.1109/VISUAL.1997.663931
   Sonoda T, 2002, ELECTRON COMM JPN 2, V85, P63, DOI 10.1002/ecjb.10073
   SRINIVASAN S, 2000, P IEEE INT C MULT EX, P388
   Tzanetakis G, 2002, THESIS PRINCETON U P
   UITDENBOGERD AL, 1995, P ACM MULT 1998, P231
   WOODRUFF A, 2001, P SIGCHI C HUM FACT, P198
   ZHU Y, 2003, SIGMOD 2003, P672
   ZWICKER E, 1991, J AUDIO ENG SOC, V39, P115
NR 27
TC 3
Z9 5
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2008
VL 10
IS 5
BP 780
EP 793
DI 10.1109/TMM.2008.922848
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 334LR
UT WOS:000258223800011
DA 2024-07-18
ER

PT J
AU Zhai, GT
   Zhang, WJ
   Yang, XK
   Lin, WS
AF Zhai, Guangtao
   Zhang, Wenjun
   Yang, Xiaokang
   Lin, Weisi
TI Efficient deblocking with coefficient regularization, shape-adaptive
   filtering, and quantization constraint
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
ID BLOCKING ARTIFACTS; IMAGES; REDUCTION; ALGORITHM
AB We propose an effective deblocking scheme with extremely low computational complexity. The algorithm involves three parts: local ac coefficient regularization (ACR) of shifted blocks in the discrete cosine transform (DCT) domain, block-wise shape adaptive filtering (BSAF) in the spatial domain, and quantization constraint (QC) in the DCT domain. The DCT domain ACR suppresses the grid noise (blockiness) in monotone areas. The spatial-domain BSAF alleviates the staircase noise along the edge, and the ringing near the edge and the corner outliers. The narrow quantization constraint set is imposed to prevent possible oversmoothing and improve PSNR performance. Extensive simulation results and comparative studies are provided to justify the effectiveness and efficiency of the proposed deblocking algorithm.
C1 [Zhai, Guangtao; Zhang, Wenjun; Yang, Xiaokang] Shanghai Jiao Tong Univ, Inst Image Communicat & Informat Proc, Shanghai 200240, Peoples R China.
   [Zhai, Guangtao; Lin, Weisi] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
C3 Shanghai Jiao Tong University; Nanyang Technological University
RP Zhai, GT (corresponding author), Shanghai Jiao Tong Univ, Inst Image Communicat & Informat Proc, Shanghai 200240, Peoples R China.
EM zhaiguangtao@sjtu.edu.cn; zhangwenjun@sjtu.edu.cn; xkyang@sjtu.edu.cn;
   xuyi@sjtu.edu.cn
RI Lin, Weisi/A-8011-2012; Liu, Anmin/A-4730-2012; Zhai,
   Guangtao/X-5949-2019; Zhang, Wenjun/GNH-2095-2022; Lin, Wei/D-3353-2012;
   Lin, Weisi/A-3696-2011; Yang, Xiaokang/C-6137-2009
OI Zhai, Guangtao/0000-0001-8165-9322; Zhang, Wenjun/0000-0002-5282-3725;
   Lin, Weisi/0000-0001-9866-1947; Yang, Xiaokang/0000-0003-4029-3322
CR Al-Fahoum AS, 2001, IEEE T IMAGE PROCESS, V10, P1288, DOI 10.1109/83.941853
   Apostolopoulos JG, 1999, IEEE T IMAGE PROCESS, V8, P1125, DOI 10.1109/83.777093
   CHANG SF, 1995, IEEE J SEL AREA COMM, V13, P1, DOI 10.1109/49.363151
   CHEN Q, 2001, CHIN J MED CHEM, V11, P5
   Choi H, 2000, IEEE T CIRC SYST VID, V10, P801, DOI 10.1109/76.856457
   Côté G, 1998, IEEE T CIRC SYST VID, V8, P849, DOI 10.1109/76.735381
   Hsung TC, 1998, IEEE T IMAGE PROCESS, V7, P1488, DOI 10.1109/83.718489
   Hsung TC, 1998, IEEE T CIRCUITS-II, V45, P640, DOI 10.1109/82.673648
   Kalva H, 2006, IEEE MULTIMEDIA, V13, P86, DOI 10.1109/MMUL.2006.93
   Kim NC, 1998, IEEE T CIRC SYST VID, V8, P253, DOI 10.1109/76.678618
   KUO CJ, 1995, IEEE T CIRC SYST VID, V5, P298, DOI 10.1109/76.465083
   Lee YL, 1998, IEEE T IMAGE PROCESS, V7, P229, DOI 10.1109/83.661000
   Liew AWC, 2004, IEEE T CIRC SYST VID, V14, P450, DOI 10.1109/TCSVT.2004.825555
   Liu SZ, 2002, IEEE T CIRC SYST VID, V12, P1139, DOI 10.1109/TCSVT.2002.806819
   Luo Y, 2003, IEEE T IMAGE PROCESS, V12, P838, DOI 10.1109/TIP.2003.814252
   Merhav N, 1997, IEEE T CIRC SYST VID, V7, P468, DOI 10.1109/76.585926
   MITCHEL JL, 1997, MPEG VIDEO COMPRESSI
   MONTAG ED, 2006, DIGITAL VIDEO IMAGE
   NGAN KN, 1989, IEEE T ACOUST SPEECH, V37, P1743, DOI 10.1109/29.46556
   ORourke TP, 1995, IEEE T CIRC SYST VID, V5, P490, DOI 10.1109/76.475891
   Park HW, 1999, IEEE T CIRC SYST VID, V9, P161, DOI 10.1109/76.744283
   Park SH, 1999, IEEE T IMAGE PROCESS, V8, P1361, DOI 10.1109/83.791962
   Pennebaker W. B., 1993, JPEG: Still image data compression standard
   RAMAMURTHI B, 1986, IEEE T ACOUST SPEECH, V34, P1258, DOI 10.1109/TASSP.1986.1164961
   REEVES HCI, 1984, OPT ENG, V23, P37
   Wandell B. A, 1995, Foundations of vision
   Wu SH, 2001, IEEE T CIRC SYST VID, V11, P1193, DOI 10.1109/76.964789
   Xiong ZX, 1997, IEEE T CIRC SYST VID, V7, P433, DOI 10.1109/76.564123
   Yang YY, 1993, IEEE T CIRC SYST VID, V3, P421, DOI 10.1109/76.260198
   Yang YY, 1997, IEEE T IMAGE PROCESS, V6, P1345, DOI 10.1109/83.624945
   Yuen M, 2006, SIGN PROC COMMUN SER, V28, P87
   Zakhor A, 1992, IEEE T CIRC SYST VID, V2, P91, DOI 10.1109/76.134377
   Zeng B, 1999, SIGNAL PROCESS, V79, P205, DOI 10.1016/S0165-1684(99)00094-8
   2006, IEEE T CIRCUITS SYST, V16, P1034
NR 34
TC 54
Z9 61
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2008
VL 10
IS 5
BP 735
EP 745
DI 10.1109/TMM.2008.922849
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 334LR
UT WOS:000258223800007
DA 2024-07-18
ER

PT J
AU Tang, JH
   Hua, XS
   Qi, GJ
   Song, Y
   Wu, XQ
AF Tang, Jinhui
   Hua, Xian-Sheng
   Qi, Guo-Jun
   Song, Yan
   Wu, Xiuqing
TI Video annotation based on kernel linear neighborhood propagation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE method; label propagation; semi-supervised learning; video annotation
AB The insufficiency of labeled training data for representing the distribution of the entire dataset is a major obstacle in automatic semantic annotation of large-scale video database. Semi-supervised learning algorithms, which attempt to learn from both labeled and unlabeled data, are promising to solve this problem. In this paper, a novel graph-based semi-supervised learning method named kernel linear neighborhood propagation (KLNP) is proposed and applied to video annotation. This approach combines the consistency assumption, which is the basic assumption in semi-supervised learning, and the local linear embedding (LLE) method in a nonlinear kernel-mapped space. KLNP improves a recently proposed method linear neighborhood propagation (LNP) by tackling the limitation of its local linear assumption on the distribution of semantics. Experiments conducted on the TRECVID data set demonstrate that this approach outperforms other popular graph-based semi-supervised learning methods for video semantic annotation.
C1 [Tang, Jinhui; Song, Yan; Wu, Xiuqing] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230027, Peoples R China.
   [Hua, Xian-Sheng] Microsoft Res Asia, Beijing 100080, Peoples R China.
   [Qi, Guo-Jun] Univ Sci & Technol China, Dept Automat, Hefei 230027, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Microsoft; Microsoft Research Asia; Chinese Academy of
   Sciences; University of Science & Technology of China, CAS
RP Tang, JH (corresponding author), Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230027, Peoples R China.
EM jhtang@mail.ustc.edu.cn; xshua@microsoft.com; qgj@mail.ustc.edu.cn;
   songy@ustc.edu.cn; wuxq@ustc.edu.cn
RI Qi, Guo-Jun/AAH-8294-2019; Tang, Jinhui/KBR-0891-2024
OI Qi, Guo-Jun/0000-0003-3508-1851
CR [Anonymous], GUIDELINES TRECVID 2
   [Anonymous], ACM INT WORKSH MULT
   [Anonymous], 2007, P 15 ACM INT C MULT, DOI DOI 10.1145/1291233.1291430
   Belkin M, 2002, ADV NEUR IN, V14, P585
   Chapelle O., 2006, SEMISUPERVISED LEARN
   Naphade M, 2006, IEEE MULTIMEDIA, V13, P86, DOI 10.1109/MMUL.2006.63
   Roweis SamT., 2003, J MACHINE LEARNING R, V4, P119
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467
   Schölkopf B, 2001, ADV NEUR IN, V13, P301
   TANG J, 2007, P 11 PAC AS C KNOWL, P793
   Tang J., 2007, ELECT LETT, V43
   Tao DC, 2007, IEEE T KNOWL DATA EN, V19, P568, DOI 10.1109/TKDE.2007.1003
   Tao DC, 2006, IEEE T MULTIMEDIA, V8, P716, DOI 10.1109/TMM.2005.861375
   Wang CB, 2006, LECT NOTES COMPUT SC, V4035, P647, DOI 10.1145/1180639.1180774
   WANG F, 2006, P IEEE CVPR, P160
   Wang Fei, 2006, P 23 INT C MACH LEAR, P985
   Yan R, 2005, PROC CVPR IEEE, P657
   YUAN X, 2006, P ACM MULT SANT BARB, P623, DOI DOI 10.1145/1180639.1180768
   Zhou DY, 2004, ADV NEUR IN, V16, P321
   Zhu X., 2003, P 20 INT C MACH LEAR, V3, P58, DOI DOI 10.1109/18.850663
   TREC 10 P APP COMM E
NR 22
TC 29
Z9 38
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2008
VL 10
IS 4
BP 620
EP 628
DI 10.1109/TMM.2008.921853
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 342EK
UT WOS:000258767200007
DA 2024-07-18
ER

PT J
AU Chen, DT
   Liu, Q
   Sun, MG
   Yang, J
AF Chen, Datong
   Liu, Qiang
   Sun, Mingui
   Yang, Jie
TI Mining appearance models directly from compressed video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE compressed video; DCT domain; object appearance modeling; video mining
AB In this paper, we propose an approach for learning appearance models of moving objects directly from compressed video. The appearance of a moving object changes dynamically in video due to varying object poses, lighting conditions, and partial occlusions. Efficiently mining the appearance models of objects is a crucial and challenging technology to support content-based video coding, clustering, indexing, and retrieval at the object level. The proposed approach learns the appearance models of moving objects in the spatial-temporal dimension of video data by taking advantage of the MPEG video compression format. It detects a moving object and recovers the trajectory of each macroblock covered by the object using the motion vector present in the compressed stream. The appearances are then reconstructed in the DCT domain along the object's trajectory, and modeled as a mixture of Gaussians (MoG) using DCT coefficients. We prove that, under certain assumptions, the MoG model learned from the DCT domain can achieve pixel-level accuracy when transformed back to the spatial domain, and has a better band-selectivity compared to the MoG model learned in the spatial domain. We finally cluster the MoG models to merge the appearance models of the same object together for object-level content analysis.
C1 [Chen, Datong] Carnegie Mellon Univ, Sch Comp Sci, Dept Comp Sci, Pittsburgh, PA 15213 USA.
   [Liu, Qiang; Sun, Mingui] Univ Pittsburgh, Dept Neurol Surg, Pittsburgh, PA 15213 USA.
   [Yang, Jie] Carnegie Mellon Univ, Sch Comp Sci, Human Comp Interface Inst, Pittsburgh, PA 15213 USA.
C3 Carnegie Mellon University; Pennsylvania Commonwealth System of Higher
   Education (PCSHE); University of Pittsburgh; Carnegie Mellon University
RP Chen, DT (corresponding author), Carnegie Mellon Univ, Sch Comp Sci, Dept Comp Sci, Pittsburgh, PA 15213 USA.
EM datong@cs.cmu.edu
CR [Anonymous], 2000, P EUR C COMP VIS, DOI DOI 10.1007/3-540-45053-X_48
   [Anonymous], P ACM INT WORKSH VID
   [Anonymous], **NON-TRADITIONAL**
   AYGUN AZR, 2001, P IEEE ICME, P701
   Babu RV, 2004, IEEE T CIRC SYST VID, V14, P462, DOI 10.1109/TCSVT.2004.825536
   CHANG SF, 1993, P ICASSP 1993
   CHAPARRO LF, 2003, NEW ALGORITHM DCT BA
   HAN B, 2004, P ACCV
   ISARD M, 1996, P EUR C COMP VIS CAM, V1, P343
   KIM C, 2000, P 8 ACM INT C MULT, P303
   Li L, 2003, ICCAD-2003: IEEE/ACM DIGEST OF TECHNICAL PAPERS, P2
   Lo BPL, 2001, PROCEEDINGS OF 2001 INTERNATIONAL SYMPOSIUM ON INTELLIGENT MULTIMEDIA, VIDEO AND SPEECH PROCESSING, P158, DOI 10.1109/ISIMP.2001.925356
   Ngo CW, 2001, PATTERN RECOGN, V34, P1841, DOI 10.1016/S0031-3203(00)00111-4
   Oliver NM, 2000, IEEE T PATTERN ANAL, V22, P831, DOI 10.1109/34.868684
   PHILLIPS J, 2002, P FAC GEST REC
   STAUFFER WEL, 1999, P COMPUT VISION PATT, V2, P246
   WU ZP, 2001, P 9 ACM C MULT OTT O, P552
   YU QTX, 2003, P INT C IM PROC, V2, P933
   ZENG DZW, 2003, P ISCAS, P524
NR 19
TC 4
Z9 5
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2008
VL 10
IS 2
BP 268
EP 276
DI 10.1109/TMM.2007.911835
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 254HC
UT WOS:000252576700011
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Akyol, E
   van der Schaar, M
AF Akyol, Emrah
   van der Schaar, Mihaela
TI Complexity model based Proactive dynamic voltage scaling for video
   decoding systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE complexity prediction; dynamic voltage scaling; video decoding
AB Significant power savings can be achieved on voltage/ frequency configurable platforms by dynamically adapting the frequency and voltage according to the workload (complexity). Video decoding is one of the most complex tasks performed on such systems due to its computationally demanding operations like inverse filtering, interpolation, motion compensation and entropy decoding. Dynamically adapting the frequency and voltage for video decoding is attractive due to the time-varying workload and because the utility of decoding a frame is dependent only on decoding the frame before the display deadline. Our contribution in this paper is twofold. First, we adopt a complexity model that explicitly considers the video compression and platform specifics to accurately predict execution times. Second, based on this complexity model, we propose a dynamic voltage scaling algorithm that changes effective deadlines of frame decoding jobs. We pose our problem as a buffer-constrained optimization and show that significant improvements can be achieved over the state-of-the-art dynamic voltage scaling techniques without any performance degradation.
C1 Univ Calif Los Angeles, Henry Samuel Sch Engn & Appl Sci, Dept Elect Engn, Los Angeles, CA 90095 USA.
C3 University of California System; University of California Los Angeles
RP Akyol, E (corresponding author), Univ Calif Los Angeles, Henry Samuel Sch Engn & Appl Sci, Dept Elect Engn, Los Angeles, CA 90095 USA.
EM eakyol@ee.ucla.edu; mihaela@ee.ucla.edu
RI Akyol, Emrah/W-9598-2019
OI Akyol, Emrah/0000-0002-0663-1677
CR *AMD INC, AMD POW TM TECHN PLA
   Andreopoulos Y, 2004, SIGNAL PROCESS-IMAGE, V19, P653, DOI 10.1016/j.image.2004.05.007
   ANDREOPOULOS Y, IN PRESS IEEE T SIGN
   ANDREOPOULOS Y, IN PRESS IEEE T CIRC
   Bavier A.C., 1998, P ACM SIGMETRICS 98, P131, DOI DOI 10.1145/277851.277892
   BERTSEKAS DP, 1995, DYNAMIC PROGRAMMING, V2
   CHOU PA, IN PRESS IEEE T MULT
   De Micheli G., 1997, DESIGN TECHNIQUES CA
   Dinda P. A., 1999, Proceedings. The Eighth International Symposium on High Performance Distributed Computing (Cat. No.99TH8469), P87, DOI 10.1109/HPDC.1999.805285
   Horowitz M, 2003, IEEE T CIRC SYST VID, V13, P704, DOI 10.1109/TCSVT.2003.814967
   Irani S, 2005, IEEE T VLSI SYST, V13, P1349, DOI 10.1109/TVLSI.2005.862725
   ISHIHARA T, 1998, P INT S LOW POW EL D
   Jensen JLWV, 1906, ACTA MATH-DJURSHOLM, V30, P175, DOI 10.1007/BF02418571
   LANDGE G, 2005, P IEEE INT C AC SPEE
   Lorch J. R., 2001, P ACM SIGMETRICS 200
   Ohm JR, 2004, SIGNAL PROCESS-IMAGE, V19, P877, DOI 10.1016/j.image.2004.06.004
   ORTEGA A, 1994, IEEE T IMAGE PROCESS, V3
   Raghunathan V, 2005, IEEE T VLSI SYST, V13, P211, DOI 10.1109/TVLSI.2004.840773
   Ravasi M, 2005, IEEE T CIRC SYST VID, V15, P673, DOI 10.1109/TCSVT.2005.846414
   Regunathan SL, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P289
   Sayed AH., 2003, FUNDAMENTALS ADAPTIV
   SHOHAM Y, 1988, IEEE T ACOUST SPEECH, V36, P1445, DOI 10.1109/29.90373
   Sinha A, 2001, DES AUT CON, P220, DOI 10.1109/DAC.2001.935508
   van der Schaar M, 2005, IEEE T MULTIMEDIA, V7, P471, DOI 10.1109/TMM.2005.846790
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   YUAN W, IN PRESS IEEE T MOBI
NR 26
TC 16
Z9 19
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2007
VL 9
IS 7
BP 1475
EP 1492
DI 10.1109/TMM.2007.906563
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 224LB
UT WOS:000250447400013
DA 2024-07-18
ER

PT J
AU Amin, T
   Zeytinoglu, M
   Guan, L
AF Amin, Tahir
   Zeytinoglu, Mehmet
   Guan, Ling
TI Application of laplacian mixture model to image and video retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE feature extraction; image indexing and retrieval; Laplacian mixture
   model; video indexing and retrieval
ID PATTERN-RECOGNITION
AB In this paper, we study the peaky nature of wavelet coefficient distributions. The study shows that the wavelet coefficients cannot be effectively modeled by a single distribution. We then propose a new modeling scheme based on a Laplacian mixture model and apply it to the indexing and retrieval of image and video databases. In this work, the parameters of the model are first used to represent texture information in image retrieval. Then we explore its application to video retrieval. Traditionally,, visual information is used for video indexing and retrieval. However, in some cases audio information is more helpful for finding clues to the video events. The proposed feature extraction scheme is based on the fundamental property of the wavelet transform. Therefore, it can also be adopted to analyze the audio contents of the video data. The experimental evaluation indicates the high discriminatory power of the proposed feature set. The dimension of the extracted feature vector is low, which is important for the retrieval efficiency of the system in terms of response time. User feedback is used to enhance the retrieval performance by modifying the system parameters according to the users' behavior. A nonlinear approach for defining the similarity between the two images is also explored in this work.
C1 Ryerson Univ, Dept Elect & Comp Engn, Toronto, ON M5B 2K3, Canada.
   Univ Illinois, Chicago, IL 60680 USA.
   McMaster Univ, Commun Res Lab, Hamilton, ON, Canada.
C3 Toronto Metropolitan University; University of Illinois System;
   University of Illinois Chicago; University of Illinois Chicago Hospital;
   McMaster University
RP Amin, T (corresponding author), Univ Toronto, Dept Elect & Comp Engn, Toronto, ON M5S 3G4, Canada.
EM tahir.amin@utoronto.ca; mzeytin@ee.ryerson.ca; lguan@ee.ryerson.ca
CR AMIN T, 2004, P INT S CIRC SYSTEMS, V2, P45
   AMIN T, 2004, P IEEE INT C AC SPEE, V3, P449
   [Anonymous], 1993, Multimedia Systems, DOI DOI 10.1007/BF01210504
   ARMAN F, 1994, P 2 ACM INT C MULT S, P97
   Ashwin TV, 2001, INT CONF ACOUST SPEE, P1637, DOI 10.1109/ICASSP.2001.941250
   Courtney JD, 1997, PATTERN RECOGN, V30, P607, DOI 10.1016/S0031-3203(96)00107-0
   Crouse MS, 1998, IEEE T SIGNAL PROCES, V46, P886, DOI 10.1109/78.668544
   Dagtas S, 2000, IEEE T IMAGE PROCESS, V9, P88, DOI 10.1109/83.817601
   DSOUZA AA, USING EM ESTIMATE PR
   ESMAILI S, 2004, P IEEE INT C AC SPEE, V5
   FABLET R, 2000, P 6 INT C CONT BAS M
   Figueiredo MAT, 2000, INT C PATT RECOG, P87, DOI 10.1109/ICPR.2000.906023
   Haering N, 2000, IEEE T CIRC SYST VID, V10, P857, DOI 10.1109/76.867923
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   HU M, 1962, IRE T INFORM THEOR, V8, P179, DOI 10.1109/tit.1962.1057692
   HUANG CB, 2004, P INT C COMM CIRC SY, V2, P927
   Huang J, 1997, ACM MULTIMEDIA 97, PROCEEDINGS, P325, DOI 10.1145/266180.266383
   HUANG XY, 2003, P JOINT C 4 INT C IN, V3, P1571
   Jain AK, 2000, IEEE T PATTERN ANAL, V22, P4, DOI 10.1109/34.824819
   Kim KM, 2006, IEEE T CONSUM ELECTR, V52, P200
   Kokare M, 2005, IEEE T SYST MAN CY B, V35, P1168, DOI 10.1109/TSMCB.2005.850176
   Laaksonen J, 2002, IEEE T NEURAL NETWOR, V13, P841, DOI 10.1109/TNN.2002.1021885
   Lei Z, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P721, DOI 10.1109/ICIP.2001.958595
   Lie WN, 2002, PROCEEDINGS OF THE 2002 IEEE WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P237
   Liu Z, 2004, Proceedings of the Fourth IEEE International Symposium on Signal Processing and Information Technology, P103
   LU ZM, 2005, ELECTRON LETT, V41, P29
   Marques O., 2002, CONTENT BASED IMAGE
   Muneesawang P, 2004, IEEE T MULTIMEDIA, V6, P703, DOI 10.1109/TMM.2004.834866
   Muneesawang P, 2001, INT CONF ACOUST SPEE, P1641, DOI 10.1109/ICASSP.2001.941251
   Naphade MR, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 3, P536, DOI 10.1109/ICIP.1998.999041
   Pass G., 1996, P 4 ACM INT C MULT, V96, P65, DOI DOI 10.1145/244130.244148
   PERSOON E, 1986, IEEE T PATTERN ANAL, V8, P388, DOI 10.1109/TPAMI.1986.4767799
   Rui Y, 1998, IEEE T CIRC SYST VID, V8, P644, DOI 10.1109/76.718510
   Rui Yong, 1996, P 1 INT WORKSH IM DA
   Saunders J, 1996, INT CONF ACOUST SPEE, P993, DOI 10.1109/ICASSP.1996.543290
   Sigitani T, 1999, IEEE T NEURAL NETWOR, V10, P381, DOI 10.1109/72.750567
   Smith J. R., 1996, Proceedings ACM Multimedia 96, P87, DOI 10.1145/244130.244151
   Smith JR, 1996, INT CONF ACOUST SPEE, P2239, DOI 10.1109/ICASSP.1996.545867
   STRICKER M, 1995, P SPIE STOR RETR IM, P31
   Swain M.J., 1991, P 3 INT C COMP VIS R, P11
   Usevitch BE, 2001, IEEE SIGNAL PROC MAG, V18, P22, DOI 10.1109/79.952803
   Wang R, 2001, 2001 IEEE FOURTH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P33, DOI 10.1109/MMSP.2001.962708
   Yuan H, 2003, PROC SPIE, V5150, P422, DOI 10.1117/12.503262
NR 43
TC 19
Z9 25
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2007
VL 9
IS 7
BP 1416
EP 1429
DI 10.1109/TMM.2007.906587
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 224LB
UT WOS:000250447400008
OA Green Published
DA 2024-07-18
ER

PT J
AU Zhang, W
   Zhon-Fang, X
   Yang, XK
   Wu, QMJ
AF Zhang, Wei
   Zhon-Fang, Xiang
   Yang, Xiaokang K.
   Wu, Q. M. Jonathan
TI Moving cast shadows detection using ratio edge
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article; Proceedings Paper
CT 18th International Conference on Pattern Recognition (ICPR 2006)
CY AUG 20-24, 2006
CL Hong Kong, PEOPLES R CHINA
SP IAPR, CAA, Hong Kong Baptist Univ
DE Chi-square distribution; moving objects detection; ratio edge; shadow
   detection
ID OBJECT DETECTION; SURVEILLANCE; ELIMINATION; TRACKING; VEHICLE
AB Moving objects segmentation plays a very important role in real-time image analysis. However, as one of the common parts in the natural scenes, shadows severely interfere with the accuracy of moving objects detection in video surveillance. In this paper, we present a novel method for moving cast shadows detection. Based on the analysis of the physical model of moving shadows. we prove that the ratio edge is illumination invariant. The distribution of the ratio edge is discussed and a significance test is performed to classify each moving pixel into foreground object or moving shadow. Intensity constraint and geometric heuristics are imposed to further improve the performance. Experiments on various typical scenes exhibit the robustness of the proposed method. Extensively quantitative evaluation and comparison demonstrate that the proposed method significantly outperforms state-of-the-art methods.
C1 Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Inst Image Commun & Informat Proc, Shanghai 200240, Peoples R China.
   Univ Windsor, Dept Elect & Comp Engn, Windsor, ON N9B 3P4, Canada.
C3 Shanghai Jiao Tong University; University of Windsor
RP Zhang, W (corresponding author), Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Inst Image Commun & Informat Proc, Shanghai 200240, Peoples R China.
EM weizhang1216@hotmail.com; xzfang@sjtu.edu.cn; xkyang@sjtu.edu.cn;
   jwu@uwindsor.ca
RI cheng, cheng/JBR-8359-2023; Wu, Q.M.Jonathan/O-3234-2017; Yang,
   Xiaokang/C-6137-2009
OI Yang, Xiaokang/0000-0003-4029-3322
CR [Anonymous], PATTERN RECOGN
   Cavallaro A, 2005, IEE P-VIS IMAGE SIGN, V152, P398, DOI 10.1049/ip-vis:20045108
   Chien SY, 2002, IEEE T CIRC SYST VID, V12, P577, DOI 10.1109/TCSVT.2002.800516
   Cucchiara R, 2003, IEEE T PATTERN ANAL, V25, P1337, DOI 10.1109/TPAMI.2003.1233909
   Cucchiara R, 2001, 2001 IEEE INTELLIGENT TRANSPORTATION SYSTEMS - PROCEEDINGS, P334, DOI 10.1109/ITSC.2001.948679
   Fung GSK, 2002, OPT ENG, V41, P1425, DOI 10.1117/1.1473638
   Gonzalez R.C., 1987, DIGITAL IMAGE PROCES, VSecond
   Haritaoglu I, 2000, IEEE T PATTERN ANAL, V22, P809, DOI 10.1109/34.868683
   Hsieh JW, 2006, IEEE T INTELL TRANSP, V7, P175, DOI 10.1109/TITS.2006.874722
   Hsieh JW, 2003, IMAGE VISION COMPUT, V21, P505, DOI 10.1016/S0262-8856(03)00030-1
   HSIEH JW, 2004, P IEEE INT C PATT RE, V4, P372
   Leone A, 2006, PATTERN RECOGN LETT, V27, P345, DOI 10.1016/j.patrec.2005.08.020
   Martel-Brisson N, 2005, PROC CVPR IEEE, P643
   McKenna SJ, 2000, COMPUT VIS IMAGE UND, V80, P42, DOI 10.1006/cviu.2000.0870
   Mikic I, 2000, INT C PATT RECOG, P321, DOI 10.1109/ICPR.2000.905341
   Nadimi S, 2004, IEEE T PATTERN ANAL, V26, P1079, DOI 10.1109/TPAMI.2004.51
   Payne A, 2005, PATTERN RECOGN, V38, P1533, DOI 10.1016/j.patcog.2004.12.014
   Prati A, 2003, IEEE T PATTERN ANAL, V25, P918, DOI 10.1109/TPAMI.2003.1206520
   Rank K, 1999, IEE P-VIS IMAGE SIGN, V146, P80, DOI 10.1049/ip-vis:19990238
   Salvador E, 2004, COMPUT VIS IMAGE UND, V95, P238, DOI 10.1016/j.cviu.2004.03.008
   Siala K, 2004, INT C PATT RECOG, P384, DOI 10.1109/ICPR.2004.1333783
   Stauder J, 1999, IEEE T MULTIMEDIA, V1, P65, DOI 10.1109/6046.748172
   Stauffer C, 2000, IEEE T PATTERN ANAL, V22, P747, DOI 10.1109/34.868677
   THONGKAMWITOON T, 2004, P IEEE INT C MULTIME, V2, P1459
   THorprasert D Harwood, 1999, IEEE ICCV, V99
   Toth D, 2004, INT C PATT RECOG, P260, DOI 10.1109/ICPR.2004.1333753
   Wang Y, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P937
   Yoneyama A, 2005, EURASIP J APPL SIG P, V2005, P2305, DOI 10.1155/ASP.2005.2305
   Yoneyama A, 2003, IEEE CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, PROCEEDINGS, P229, DOI 10.1109/AVSS.2003.1217926
   ZHANG W, 2006, P IEEE INT C PATT RE, V1, P626
   Zhang W, 2006, INT C PATT RECOG, P73
NR 31
TC 67
Z9 90
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2007
VL 9
IS 6
BP 1202
EP 1214
DI 10.1109/TMM.2007.902842
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA 215XJ
UT WOS:000249842000011
DA 2024-07-18
ER

PT J
AU Hauptmann, A
   Yan, R
   Lin, WH
   Christel, M
   Wactlar, H
AF Hauptmann, Alexander
   Yan, Rong
   Lin, Wei-Hao
   Christel, Michael
   Wactlar, Howard
TI Can high-level concepts fill the semantic gap in video retrieval? A case
   study with broadcast news
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE concept-based video retrieval; high-level semantic concepts; semantic
   gap
AB A number of researchers have been building high-level semantic concept detectors such as outdoors, face, building, to help with semantic video retrieval. Our goal is to examine how many concepts would be needed, and how they should be selected and used. Simulating performance of video retrieval under different assumptions of concept detection accuracy, we find that good retrieval can be achieved even when detection accuracy is low, if sufficiently many concepts are combined. We also derive suggestions regarding the types of concepts that would be most helpful for a large concept lexicon. Since our user study finds that people cannot predict which concepts will help their query, we also suggest ways to find the best concepts to use. Ultimately, this paper concludes that "concept-based" video retrieval with fewer than 5000 concepts, detected with a minimal accuracy of 10% mean average precision is likely to provide high accuracy results in broadcast news retrieval.
C1 Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.
C3 Carnegie Mellon University
RP Hauptmann, A (corresponding author), Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.
EM alex@cs.cmu.edu; yanrong@cs.cmu.edu; whlin@cs.cmu.edu; mac@cs.cmu.edu;
   hdw@cs.cmu.edu
OI Christel, Michael/0000-0002-2660-2515
CR AMIR A, 2003, P NIST TRECVID 2003
   [Anonymous], ACM MULTIMEDIA
   BARNARD K, 2002, J MACH LEARN RES, V3
   BEITZEL SM, 2005, P SIGIR, P583
   CHANG SF, P IEEE ICASSP 2005
   CHANG SF, 2005, P NIST TRECVID
   Cover T. M., 1991, ELEMENTS INFORM THEO
   Fellbaum C., 1998, WORDNET ELECT LEXICA, DOI DOI 10.7551/MITPRESS/7287.001.0001
   Jeon J., 2003, P 26 ANN INT ACM SIG
   LEW M, 2002, P INT C IM VID RETR
   Markkula M., 2000, Information Retrieval, V1, P259, DOI 10.1023/A:1009995816485
   Naphade M, 2006, IEEE MULTIMEDIA, V13, P86, DOI 10.1109/MMUL.2006.63
   NAPHADE MR, 2004, P 12 ACM INT C MULT
   NATSEV AP, 2005, P 13 ACM INT C MULT
   NEO SRY, 2006, P C IM VID RETR CIVR
   OVER P, P TRECVID 2005
   Qiu Y., P 16 ANN INT ACM SIG, P160, DOI DOI 10.1145/160688.160713
   RENDER JR, 2005, P C COMP VIS PATT RE, P1174
   RODDEN K, 2001, P SIGCHI C HUM FACT, P190, DOI DOI 10.1145/365024.365097
   SMEATON AF, 2003, P INT C IM VID RETR
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   SMITH JR, 2002, P INT WORKSH DIG COM
   Snoek CGM, 2006, IEEE T PATTERN ANAL, V28, P1678, DOI 10.1109/TPAMI.2006.212
   Volkmer T, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P765, DOI 10.1109/ICME.2006.262951
   Yan R., 2003, P 11 ACM INT C MULT, V3, P339, DOI DOI 10.1145/957013.957086
   Yan R., 2004, PROC ACM INT C MULTI, P548
   YAN R, 2006, THESIS CARN MELL U P
   YUAN J, 2005, NIST TRECVID 200 NOV
   Zhao W, 2003, ACM COMPUT SURV, V35, P399, DOI 10.1145/954339.954342
   Zipf G.K., 1972, Human behavior and the principle of least effort
   2006, OVERV TRECVID 2006
NR 31
TC 112
Z9 123
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2007
VL 9
IS 5
BP 958
EP 966
DI 10.1109/TMM.2007.900150
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 193ZX
UT WOS:000248314800005
DA 2024-07-18
ER

PT J
AU Huang, YC
   Lu, CS
   Wu, HK
AF Huang, Yu-Chen
   Lu, Chun-Shien
   Wu, Hsiao-Kuang
TI JitterPath: Probing noise resilient one-way delay jitter-based available
   bandwidth estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE available bandwidth; bottleneck; congestion; one-way-delay jitter;
   probing QoS
ID REAL-TIME VIDEO; INTERNET
AB Measurement of end-to-end available bandwidth has received considerable attention due to its potential use in improving QoS. Available bandwidth enables the sending rate to adapt to network conditions, so that packet loss, caused by congestion, can be significantly reduced before error control mechanisms are finally employed. To this end, we propose a probing noise resilient available bandwidth estimation scheme, called JitterPath, which is adaptive to both the fluid and bursty traffic models. Two key factors, one-way delay jitter and accumulated queuing delay, are both exploited to predict the type of queuing region for each packet pair. Then, the bottleneck utilization information included in the joint queuing regions is estimated and used to quantify the captured traffic ratio, which indicates the relationship between the probing rate and available bandwidth. The contributions of our method are as follows: 1) JitterPath can work without being restricted to fluid traffic models; 2) since JitterPath does not directly use the bottleneck link capacity to calculate the available bandwidth, it is feasible for use in a multihop environment with a single bottleneck; and 3) JitterPath inherently reduces the impact of probing noises under the bursty cross traffic model. Extensive simulations, Internet experiments, and comparisons with other methods were conducted to verify the effectiveness of our method under both single-hop and multihop environments.
C1 Acad Sinica, Inst Informat Sci, Taipei 115, Taiwan.
   Natl Cent Univ, Dept Comp Sci & Informat Engn, Chungli 32054, Taiwan.
C3 Academia Sinica - Taiwan; National Central University
RP Huang, YC (corresponding author), Acad Sinica, Inst Informat Sci, Taipei 115, Taiwan.
EM lcs@iis.sinica.edu.tw
RI Huang, Yuchen/HOH-2286-2023
CR [Anonymous], P ACM SIGCOMM
   Dovrolis C, 2001, IEEE INFOCOM SER, P905, DOI 10.1109/INFCOM.2001.916282
   DUTTA D, 2002, P IEEE INT C COMM
   HU N, 2004, P ACM SIGCOMM INT ME
   HU N, 2005, P 24 IEEE INF MAR, P41
   Hu NN, 2003, IEEE J SEL AREA COMM, V21, P879, DOI 10.1109/JSAC.2003.814505
   Hu NN, 2003, 11TH IEEE INTERNATIONAL CONFERENCE ON NETWORK PROTOCOLS, PROCEEDINGS, P107
   Huang Y. C., 2004, P IEEE GLOB TEL C GL
   HUANG YC, P IEEE WIR COMM NETW
   Jacobson V., 1988, Computer Communication Review, V18, P314, DOI 10.1145/52325.52356
   JAIN M, 2002, P ACM SIGCOMM AUG, P295
   JAIN M, 2002, P PASS ACT MEAS
   Jain M., 2004, P 4 ACM SIGCOMM C IN, P272
   Lai K, 2001, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD USENIX SYMPOSIUM ON INTERNET TECHNOLOGIES AND SYSTEMS, P123
   Li ZG, 2003, IEEE T CIRC SYST VID, V13, P472, DOI 10.1109/TCSVT.2003.813420
   Liu JC, 2003, IEEE MULTIMEDIA, V10, P22, DOI 10.1109/MMUL.2003.1167919
   LIU Q, 2003, P IEEE INT C MULT EX
   LIU X, 2004, P 4 ACM SIGCOMM C IN, P300
   Melander B, 2000, GLOB TELECOMM CONF, P415, DOI 10.1109/GLOCOM.2000.892039
   Navratil J., 2003, P 4 PASSIVE ACTIVE M
   Paxson V, 1997, IEEE ACM T NETWORK, V5, P601, DOI 10.1109/90.649563
   Puri R, 2001, IEEE T MULTIMEDIA, V3, P18, DOI 10.1109/6046.909591
   RIBEIRO V, 2000, P ITC SPEC SEM IP TR
   Ribeiro V.J., 2003, PASSIVE ACTIVE MEASU
   RIBEIRO VJ, 2004, P 2 INT WORKSH PROT
   ROBERTS JW, 2001, IEEE COMMUN MAG, V1
   Strauss J., 2003, P INT MEAS WORKSH, P27
   Wu DP, 2000, P IEEE, V88, P1855, DOI 10.1109/5.899055
   Yoma NB, 2004, IEEE T MULTIMEDIA, V6, P174, DOI 10.1109/TMM.2003.819582
   ZHANG Y, 2001, P ACM SIGCOMM INT ME
NR 30
TC 8
Z9 9
U1 1
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2007
VL 9
IS 4
BP 798
EP 812
DI 10.1109/TMM.2007.893343
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 170NW
UT WOS:000246671200011
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zeng, ZH
   Tu, JL
   Liu, M
   Huang, TS
   Pianfetti, B
   Roth, D
   Levinson, S
AF Zeng, Zhihong
   Tu, Jilin
   Liu, Ming
   Huang, Thomas S.
   Pianfetti, Brian
   Roth, Dan
   Levinson, Stephen
TI Audio-visual affect recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE affect recognition; affective computing; emotion recognition; multimodal
   human-computer interaction
AB The ability of a computer to detect and appropriately respond to changes in a user's affective state has significant implications to Human-Computer Interaction (HCI). In this paper, we present our efforts toward audio-visual affect recognition on 11 affective states customized for HCI application (four cognitive/motivational and seven basic affective states) of 20 nonactor subjects. A smoothing method is proposed to reduce the detrimental influence of speech on facial expression recognition. The feature selection analysis shows that subjects are prone to use brow movement in face, pitch and energy in prosody to express their affects while speaking. For person-dependent recognition, we apply the voting method to combine the frame-based classification results from both audio and visual channels. The result shows 7.5% improvement over the best unimodal performance. For person-independent test, we apply multistream HMM to combine the information from multiple component streams. This test shows 6.1% improvement over the best component performance.
C1 Univ Illinois, Beckman Inst Adv Sci & Technol, Urbana, IL 61801 USA.
C3 University of Illinois System; University of Illinois Urbana-Champaign
RP Zeng, ZH (corresponding author), Univ Illinois, Beckman Inst Adv Sci & Technol, Urbana, IL 61801 USA.
EM zhzeng@ifp.uiuc.edu; jilintu@ifp.uiuc.edu; mingliu1@ifp.uiuc.edu;
   huang@ifp.uiuc.edu; bpianfet@uiuc.edu; danr@cs.uiuc.edu;
   sel@ifp.uiuc.edu
RI yan, shuicheng/HCH-9860-2022; yan, shuicheng/A-8531-2014
OI yan, shuicheng/0000-0003-4527-1018; yan, shuicheng/0000-0001-8906-3777
CR CARLSON AJ, SNOW USER MANUAL
   CHEN L, 1998, INT C AUT FAC GEST R, P396
   Chen LS, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P423, DOI 10.1109/ICME.2000.869630
   CHEN LS, 2000, THESIS UIUC
   Cohen I, 2003, COMPUT VIS IMAGE UND, V91, P160, DOI 10.1016/S1077-3142(03)00081-X
   Cowie R, 2003, SPEECH COMMUN, V40, P5, DOI 10.1016/S0167-6393(02)00071-7
   Cowie R, 2001, IEEE SIGNAL PROC MAG, V18, P32, DOI 10.1109/79.911197
   Cowie R., 2000, PROC ISCA WORKSHOP S, P19
   De Silva L. C., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P332, DOI 10.1109/AFGR.2000.840655
   ESSA IA, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P360, DOI 10.1109/ICCV.1995.466916
   Fragopanagos N, 2005, NEURAL NETWORKS, V18, P389, DOI 10.1016/j.neunet.2005.03.006
   Hoch S, 2005, INT CONF ACOUST SPEE, P1085
   Huber R., 2000, Proceedings of ICSLP, P665
   KWON OW, 2003, P EUROSPEECH
   LITMAN DJ, 2004, P 42 ANN M AS COMP L
   MEHRABIAN A, 1968, PSYCHOL TODAY, V2, P53
   Pantic M, 2003, P IEEE, V91, P1370, DOI 10.1109/JPROC.2003.817122
   Picard R. W., 1997, AFFECTIVE COMPUTING
   POLZIN T, 1999, THESIS CARNEGIE MELL
   ROTH D, 1998, P NAT C ART INT AAAI
   Tao H., 1999, PROC IEEE COMPUT VIS, P611
   Tian YI, 2001, IEEE T PATTERN ANAL, V23, P97, DOI 10.1109/34.908962
   Tu Jilin., 2004, P COMPUTER VISION PA
   WANG Y, INT C AC SPEECH SIGN, V2, P1125
   Yoshitomi Y, 2000, IEEE RO-MAN 2000: 9TH IEEE INTERNATIONAL WORKSHOP ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION, PROCEEDINGS, P178, DOI 10.1109/ROMAN.2000.892491
NR 25
TC 82
Z9 91
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2007
VL 9
IS 2
BP 424
EP 428
DI 10.1109/TMM.2006.886310
PG 5
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 135CO
UT WOS:000244131900021
DA 2024-07-18
ER

PT J
AU Dielmann, A
   Renals, S
AF Dielmann, Alfred
   Renals, Steve
TI Automatic meeting segmentation using dynamic Bayesian networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE multimodal; multistream; meeting actions
ID SPEECH
AB Multiparty meetings are a ubiquitous feature of organizations, and there are considerable economic benefits that would arise from their automatic analysis and structuring. In this paper, we are concerned with the segmentation and structuring of meetings (recorded using multiple cameras and microphones) into sequences of group meeting actions such as monologue, discussion and presentation. We outline four families of multimodal features based on speaker turns, lexical transcription, prosody, and visual motion that are extracted from the raw audio and video recordings. We relate these low-level features to more complex group behaviors using a multistream modelling framework based on multistream dynamic Bayesian networks (DBNs). This results in an effective approach to the segmentation problem, resulting in an action error rate of 12.2%, compared with 43% using an approach based on hidden Markov models. Moreover, the multistream DBN developed here leaves scope for many further improvements and extensions.
C1 Univ Edinburgh, Ctr Speech Technol Res, Edinburgh EH8 9LW, Midlothian, Scotland.
C3 University of Edinburgh
RP Dielmann, A (corresponding author), Univ Edinburgh, Ctr Speech Technol Res, Edinburgh EH8 9LW, Midlothian, Scotland.
EM a.dielmann@ed.ac.uk; s.re-nals@ed.ac.uk
RI Renals, Steve/L-7175-2014
OI Renals, Steve/0000-0002-8790-3389
CR Al-Hames M, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P45, DOI 10.1109/ICME.2005.1521356
   ALHAMES M, 2006, P MULT INT REL MACH, P52
   [Anonymous], 2001, P SIGCHI C HUMAN FAC, DOI DOI 10.1145/365024.365119
   BASU S, 2001, P IEEE WORKSH CUES C
   BENGIO S, 2003, ADV NEURAL INFORM PR
   BILMES J, 2002, P IEEE ICASSP JUN
   Bilmes Jeff., 2003, MATH FDN SPEECH LANG
   Carletta J, 2005, LECT NOTES COMPUT SC, V3869, P28
   Dielmann A, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL V, PROCEEDINGS, P629
   Dielmann A, 2004, 2004 IEEE 6TH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P167
   DUPONT S, 2000, P IEEE T MULT, V2
   Fine S, 1998, MACH LEARN, V32, P41, DOI 10.1023/A:1007469218079
   Garg A, 2003, P IEEE, V91, P1355, DOI 10.1109/JPROC.2003.817119
   Hakeem A, 2004, PROCEEDING OF THE NINETEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND THE SIXTEENTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE, P263
   HOWARD A, 2004, UNCERTAINTY ARTI JUL
   JANIN A, 2003, P IEEE ICASSP APR
   Kazman R, 1996, IEEE MULTIMEDIA, V3, P63, DOI 10.1109/93.486705
   LEE D, 2002, ACM MULTIMEDIA   DEC
   McCowan I, 2005, IEEE T PATTERN ANAL, V27, P305, DOI 10.1109/TPAMI.2005.49
   MCCOWAN J, 2003, P IEEE ICASSP
   MCGRATH JE, 1991, SMALL GR RES, V22, P147, DOI 10.1177/1046496491222001
   McNeill D., 2000, LANGUAGE GESTURE
   Morgan N, 1998, INT CONF ACOUST SPEE, P729, DOI 10.1109/ICASSP.1998.675368
   Murphy Kevin Patrick, 2002, Dynamic bayesian networks: representation, inference and learning
   OLIVER N, 2003, ACM ICMI NOV
   PFAU T, 2001, P IEEE ASRU WORKSH D
   Potamianos G, 2003, P IEEE, V91, P1306, DOI 10.1109/JPROC.2003.817150
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   REITER S, 2004, P IEEE ICPR AUG
   Schultz T., 2001, P WORKSH HANDS FREE
   SHI H, 1994, P IEEE CVPR SEATTL W
   Smyth P, 1997, NEURAL COMPUT, V9, P227, DOI 10.1162/neco.1997.9.2.227
   SNOEK CG, 2004, P IEEE ICME TAIP TAI
   Sonmez K., 1998, P INT C SPOKEN LANGU, P3189
   WAIBEL A, 2001, P IEEE ICASSP MAY
   Wrigley SN, 2005, IEEE T SPEECH AUDI P, V13, P84, DOI 10.1109/TSA.2004.838531
   Yang J., 1998, Proc. ACCV'98, VII, P687
   ZHANG D, 2004, P IEEE CVPR WORKSH E
   ZHANG D, 2004, P ACM MULT WORKSH VI
   ZHANG Y, 2003, P IEEE ICASSP
NR 40
TC 34
Z9 34
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2007
VL 9
IS 1
BP 25
EP 36
DI 10.1109/TMM.2006.886337
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 119VE
UT WOS:000243041500004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Lim, TM
   Lee, BS
   Yeo, CK
AF Lim, Teck Meng
   Lee, Bu-Sung
   Yeo, Chai Kiat
TI Quantum-based earliest deadline first scheduling for multiservices
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE packet scheduling; quality-of-service (QoS)
ID NETWORKS; SERVERS
AB Latency-rate (LR) schedulers have shown their ability in providing fair and weighted sharing of bandwidth with an upper bound on delivery latency of packets while earliest departure first (EDF) schedulers have shown their ability in providing LR-decoupled service whereby the delivery latency of packets is not bounded by the reserved rate. However, EDF schedulers require traffic shapers to ensure flow protection. We propose quantum-based earliest deadline first scheduling (QEDF), a quantum-based scheduler that provides flow protection, throughput guarantee and delay bound guarantee for flows that require LR-coupled and LR-decoupled types of reservations. It classifies flows into time-critical (TC), jitter-sensitive (JS), and rate-based (RB) classes and uses a quality-of-service forwarding rule to determine the next packet to be serviced by the scheduler. It provides nonpreemptive priority service to TC queues. This allows LR-decoupled reservation for flows that have a low rate and intolerable delay. Packets from JS queues can be delayed by other packets if forwarding the latter will not result in the former missing its deadline. As a quantum-based scheduler, the QEDF scheduler provides throughput guarantees for RB queues. We present both analytical and simulation results of QEDF, whereby we evaluated QEDF in its deployment as a single-class as well as a multiservice scheduler.
C1 Nanyang Technol Univ, Ctr Multimedia & Network Technol, Sch Comp Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University
RP Lim, TM (corresponding author), Nanyang Technol Univ, Ctr Multimedia & Network Technol, Sch Comp Engn, Singapore 639798, Singapore.
EM limtm@ntu.edu.sg; ebslee@ntu.edu.sg; asckyeo@ntu.edu.sg
RI Lee, Francis BS/G-9323-2014; Yeo, Chai Kiat/A-3683-2011
OI Lee, Francis BS/0000-0001-7828-7900; Yeo, Chai Kiat/0000-0002-7618-1472
CR [Anonymous], J ACM
   CLAYPOOL M, 2003, P IEEE INT PERF COMP
   Farber J., 2002, P 1 WORKSHOP NETWORK, P53
   FERRARI D, 1990, IEEE J SEL AREA COMM, V8, P368, DOI 10.1109/49.53013
   FINEBERG MS, 1967, P AFIPS FALL JOINT C
   Lim TM, 2005, COMPUT COMMUN, V28, P1711, DOI 10.1016/j.comcom.2004.12.015
   LIM TM, 2004, WEIGHTED DEFICIT EAR
   McKenney P. E., 1991, Internetworking: Research and Experience, V2, P113
   Parekh AK, 1993, IEEE ACM T NETWORK, V1, P344, DOI 10.1109/90.234856
   Sariowan H, 1999, IEEE ACM T NETWORK, V7, P669, DOI 10.1109/90.803382
   SARIOWAN H, 1995, P IEEE INT C COMP CO
   SHELDON N, 2003, P ACM SIGCOMM NETGAM
   Shreedhar M, 1996, IEEE ACM T NETWORK, V4, P375, DOI 10.1109/90.502236
   Stiliadis D, 1998, IEEE ACM T NETWORK, V6, P611, DOI 10.1109/90.731196
   VERMA DC, 1991, PROCEEDINGS OF TRI COMM 91 - IEEE CONFERENCE ON COMMUNICATIONS SOFTWARE : COMMUNICATIONS FOR DISTRIBUTED APPLICATIONS AND SYSTEMS, P35, DOI 10.1109/TRICOM.1991.152873
   West R, 2004, IEEE T COMPUT, V53, P744, DOI 10.1109/TC.2004.10
   ZHENG Q, 1994, IEEE T COMMUN, V42, P1096, DOI 10.1109/TCOMM.1994.580218
NR 17
TC 12
Z9 14
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2007
VL 9
IS 1
BP 157
EP 168
DI 10.1109/TMM.2006.886380
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 119VE
UT WOS:000243041500015
DA 2024-07-18
ER

PT J
AU Dai, M
   Loguinov, D
   Radha, HA
AF Dai, Min
   Loguinov, Dmitri
   Radha, Hayder A.
TI Rate-distortion analysis and quality control in scalable Internet
   streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE MPEG-4 FGS; quality control; rate distortion; scalable streaming
ID CONGESTION CONTROL; BIT ALLOCATION; VIDEO; FRAMEWORK
AB Rate-distortion (R-D) modeling of video coders has always been an important issue in video streaming; however, few if the traditional R-D models and their performance have been closely examined in the context of scalable (FGS-like) video. To,overcome this shortcoming, the first half of the paper models rate-distortion of DCT-based fine-granular scalable coders and derives a simple operational R-D model for Internet streaming applications. Experimental results demonstrate that this R-D result, an extension of the classical R-D formula, is very accurate within the domain of scalable coding methods exemplified by MPEG-4 FGS and H.264 progressive FGS. In the second half of the paper, we examine congestion control and dynamic rate-scaling algorithms that achieve smooth visual quality during streaming using he proposed R-D model. In constant bitrate (CBR) channels, our R-D based quality-control algorithm dramatically reduces PSNR variation between adjacent frames (to less than 0.1 dB in sample sequences). Since the Internet is a changing environment shared by many sources, even R-D based quality control often cannot guarantee nonfluctuating PSNR in variable-bitrate (VBR) channels without the help from an appropriate congestion controller. Thus, we apply recent utility-based congestion control methods to our problem and show how a combination of this approach and our R-D model can benefit future streaming applications.
C1 Qualcomm Inc, MediaFLO Video Syst Team, San Diego, CA 92121 USA.
   Texas A&M Univ, Dept Comp Sci, College Stn, TX 77843 USA.
   Michigan State Univ, Dept Elect & Elect Engn, E Lansing, MI 48824 USA.
C3 Qualcomm; Texas A&M University System; Texas A&M University College
   Station; Michigan State University
RP Dai, M (corresponding author), Qualcomm Inc, MediaFLO Video Syst Team, San Diego, CA 92121 USA.
EM mdai@qualcomm.com; dmitri@cs.tamu.edu; radha@egr.msu.edu
CR [Anonymous], ICSITR97021 U CAL
   Bansal D, 2001, IEEE INFOCOM SER, P631, DOI 10.1109/INFCOM.2001.916251
   Chen JJ, 1997, IEEE J SEL AREA COMM, V15, P1002, DOI 10.1109/49.611155
   Chiang TH, 1997, IEEE T CIRC SYST VID, V7, P246, DOI 10.1109/76.554439
   Cover Thomas A., 1991, ELEM INF THEORY, DOI 10.1002/047174882X
   Dai M, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P301
   DEVORE RA, 1992, IEEE T INFORM THEORY, V38, P719, DOI 10.1109/18.119733
   Ding W, 1996, IEEE T CIRC SYST VID, V6, P12, DOI 10.1109/76.486416
   FLOYD S, 2000, ACM SIGCOMM      AUG, P45
   Gray R. M., 1989, Source coding theory, V83
   HANG HM, 1997, IEEE T CIRCUITS SYST, V7, P197
   He ZH, 2001, IEEE T CIRC SYST VID, V11, P1221, DOI 10.1109/76.974677
   Hsu CY, 1997, IEEE J SEL AREA COMM, V15, P1016, DOI 10.1109/49.611156
   *ISO IEC, 2001, JTC1SC29WG11N3908 IS
   *ISO IEC, 2000, JTCISC29WG2 ISOIEC
   Jayant N.C., 1984, Digital Coding of Waveforms: Principles and Applications to Speech and Video
   Johari R, 2001, IEEE ACM T NETWORK, V9, P818, DOI 10.1109/90.974534
   Kamaci N, 2005, IEEE T CIRC SYST VID, V15, P994, DOI 10.1109/TCSVT.2005.852400
   Kang SR, 2004, INT CON DISTR COMP S, P768, DOI 10.1109/ICDCS.2004.1281645
   Kelly FP, 1998, J OPER RES SOC, V49, P237, DOI 10.2307/3010473
   Kunniyur S., 2000, IEEE INFOCOM, P1323
   Leviatan D, 2002, J APPROX THEORY, V118, P20, DOI 10.1006/jath.2002.3695
   Li WP, 2001, IEEE T CIRC SYST VID, V11, P301, DOI 10.1109/76.911157
   Lin LJ, 1998, IEEE T CIRC SYST VID, V8, P446, DOI 10.1109/76.709411
   LING F, 1999, SPIE C VIS COMM IM P, P500
   Massoulié L, 2002, IEEE T AUTOMAT CONTR, V47, P895, DOI 10.1109/TAC.2002.1008356
   NETRAVALI AN, 1988, DIGITAL PICTURES PRE
   ORTEGA A, 1998, IEEE SIGNAL PROCESS, V15, P74
   Radha HM, 2001, IEEE T MULTIMEDIA, V3, P53, DOI 10.1109/6046.966110
   RAJPOOT NM, 2002, MIDDL E S SIM MOD SE
   Viterbi A. J., 1979, Principles of Digital Communication and Coding
   Wang Q, 2002, IEEE SIGNAL PROC LET, V9, P33, DOI 10.1109/97.991132
   WANG Q, 2001, VIDEO PROCESSING COM
   Wu DP, 2000, P IEEE, V88, P1855, DOI 10.1109/5.899055
   Wu F, 2001, IEEE T CIRC SYST VID, V11, P332, DOI 10.1109/76.911159
   Zhang YP, 2004, ACM SIGCOMM COMP COM, V34, P307, DOI 10.1145/1030194.1015501
   ZHAO L, 2002, SPIE VCIP        JAN, P230
   ZHAO X, 2002, PACK VID WORKSH APR
NR 38
TC 27
Z9 29
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2006
VL 8
IS 6
BP 1135
EP 1146
DI 10.1109/TMM.2006.884626
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 109MG
UT WOS:000242311700004
OA Green Published
DA 2024-07-18
ER

PT J
AU Mao, SW
   Hou, YT
   Cheng, XL
   Sherali, HD
   Midkiff, SF
   Zhang, YQ
AF Mao, Shiwen
   Hou, Y. Thomas
   Cheng, Xiaolin
   Sherali, Hanif D.
   Midkiff, Scott F.
   Zhang, Ya-Qin
TI On routing for multiple description video over wireless ad hoc networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE ad hoc networks; cross-layer design; metaheuristics; multipath routing;
   multiple description video; optimization
ID GENETIC ALGORITHM
AB We study the problem of multipath routing for double description (DD) video in wireless ad hoc networks. We follow an application-centric cross-layer approach and formulate an optimal routing problem that minimizes the application layer video distortion. We show that the optimization problem has a highly complex objective function and an exact analytic solution is not obtainable. However, we find that a meta-heuristic approach such as genetic algorithms (GAs) is eminently effective in addressing this type of complex cross-layer optimization problems. We provide a detailed solution procedure for the GA-based approach. Simulation results demonstrate the superior performance of the GA-based approach versus several other approaches. Our efforts in this work provide an important methodology for addressing complex cross-layer optimization problems, particularly those involved in the application and network layers.
C1 Virginia Polytech Inst & State Univ, Dept Elect & Comp Engn, Blacksburg, VA 24061 USA.
   Virginia Polytech Inst & State Univ, Dept Ind & Syst Engn, Blacksburg, VA 24061 USA.
   Microsoft Corp, Redmond, WA 98502 USA.
C3 Virginia Polytechnic Institute & State University; Virginia Polytechnic
   Institute & State University; Microsoft
RP Mao, SW (corresponding author), Virginia Polytech Inst & State Univ, Dept Elect & Comp Engn, Blacksburg, VA 24061 USA.
EM smao@ieee.org; thou@vt.edu; xicheng@vt.edu; hanifs@vt.edu;
   midkiff@vt.edu; yzhang@microsoft.com
RI Midkiff, Scott/U-9596-2019; Cheng, Xiaolin/I-4561-2013; Mao,
   Shiwen/AAY-4471-2020
OI Midkiff, Scott/0000-0003-4933-7360; 
CR Ahn CW, 2002, IEEE T EVOLUT COMPUT, V6, P566, DOI 10.1109/TEVC.2002.804323
   Akyildiz IF, 2005, IEEE COMMUN MAG, V43, pS23, DOI 10.1109/MCOM.2005.1509968
   Alasti M, 2001, IEEE T INFORM THEORY, V47, P891, DOI 10.1109/18.915641
   [Anonymous], 1989, Simulated annealing and Boltzmann machines: A stochastic approach to combinatorial optimization and neural computing
   Apostolopoulos J, 2002, IEEE INFOCOM SER, P1736, DOI 10.1109/INFCOM.2002.1019427
   Back T., 1997, Handbook of evolutionary computation, V1st
   Banerjee N, 2001, 2001 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS, VOLS 1-10, CONFERENCE RECORD, P2588, DOI 10.1109/ICC.2001.936617
   Begen AC, 2005, SIGNAL PROCESS-IMAGE, V20, P39, DOI 10.1016/j.image.2004.09.002
   Blum C, 2003, ACM COMPUT SURV, V35, P268, DOI 10.1145/937503.937505
   CHAKARESKI J, 2003, P 11 ACM INT C MULT, P422
   CLAUSEN T, 2003, 3626 IETFRFC
   Di Fatta G, 2003, IEEE T SYST MAN CY C, V33, P313, DOI 10.1109/TSMCC.2003.818946
   Elbaum R, 1996, IEEE ACM T NETWORK, V4, P766, DOI 10.1109/90.541324
   Eppstein D, 1998, SIAM J COMPUT, V28, P652, DOI 10.1137/S0097539795290477
   FRITCHMAN BD, 1967, IEEE T INFORM THEORY, V13, P221, DOI 10.1109/TIT.1967.1053975
   Ghosh SC, 2003, IEEE T VEH TECHNOL, V52, P860, DOI 10.1109/TVT.2003.808806
   Glover F., 1998, Tabu Search
   Gogate N, 2002, IEEE T CIRC SYST VID, V12, P777, DOI 10.1109/TCSVT.2002.803229
   Goyal VK, 2001, IEEE SIGNAL PROC MAG, V18, P74, DOI 10.1109/79.952806
   Gustafsson E, 1997, IEEE NETWORK, V11, P28, DOI 10.1109/65.580915
   HU YC, 2002, P IEEE MILCOM 2002, P7
   Johnson D.B., 2003, The Dynamic Source Routing Protocol for Mobile Ad Hoc Networks (DSR)
   Kapoor R, 2004, ACM SIGCOMM COMP COM, V34, P67, DOI 10.1145/1030194.1015476
   Malpani N, 2002, INFORM PROCESS LETT, V83, P175, DOI 10.1016/S0020-0190(01)00323-4
   MAO S, 2006, IEEE T VEH TECHNOL, V55
   Mao SW, 2003, IEEE J SEL AREA COMM, V21, P1721, DOI 10.1109/JSAC.2003.815965
   Murthy S, 1996, IEEE INFOCOM SER, P1028, DOI 10.1109/INFCOM.1996.493045
   Ngo CY, 2003, IEEE T COMMUN, V51, P1439, DOI 10.1109/TCOMM.2003.816950
   OGIER R, 2004, 3684 IETFFC
   OZAROW L, 1980, BELL SYST TECH J, V59, P1909, DOI 10.1002/j.1538-7305.1980.tb03344.x
   Papadimitratos P., 2002, SECURING MOBILE AD H, P1
   SETTON E, 2003, P IEEE ICME BALT MD
   Sherali HD, 1998, NETWORKS, V31, P259, DOI 10.1002/(SICI)1097-0037(199807)31:4<259::AID-NET6>3.0.CO;2-C
   Sinclair MC, 1999, IEE P-COMMUN, V146, P1, DOI 10.1049/ip-com:19990281
   Yener A, 1997, IEEE T VEH TECHNOL, V46, P72, DOI 10.1109/25.554739
NR 35
TC 31
Z9 38
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2006
VL 8
IS 5
BP 1063
EP 1074
DI 10.1109/TMM.2006.879845
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 091TY
UT WOS:000241052400017
DA 2024-07-18
ER

PT J
AU Zhang, F
   Macnicol, J
   Pickering, MR
   Frater, MR
   Arnold, JF
AF Zhang, Fei
   Macnicol, James
   Pickering, Mark R.
   Frater, Michael R.
   Arnold, John F.
TI Efficient streaming packet video over differentiated services networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE differentiated services networks; error resilience; video compression
AB We investigate streaming video over Differentiated Services (Diffserv) networks that can provide a number of aggregated traffic classes with increasing quality guarantee. We propose a method to measure the loss impact of a video packet on the quality of the decoded video images. We show how the optimal Quality-of-Service (QoS) mapping from the video packets into a set of traffic classes depends on the loss rates of the classes and the pricing model, and we develop an algorithm to accurately find the optimal QoS mapping. The performance of our algorithm is evaluated through computer simulations and compares favorably to an existing algorithm.
C1 Univ New S Wales, Australian Def Force Acad, Sch Informat Technol & Elect Engn, Canberra, ACT 2600, Australia.
C3 University of New South Wales Sydney; Australian Defense Force Academy
RP Zhang, F (corresponding author), Univ New S Wales, Australian Def Force Acad, Sch Informat Technol & Elect Engn, Canberra, ACT 2600, Australia.
EM f.zhang@adfa.edu.au; m.pickering@adfa.edu.au; m.frater@adfa.edu.au;
   j.arnold@adfa.edu.au
CR Ahmed T, 2001, IEEE SYMP COMP COMMU, P346, DOI 10.1109/ISCC.2001.935397
   [Anonymous], 1994, INTEGRATED SERVICES
   [Anonymous], 1998, 2475 RFC IETF
   Dovrolis C, 1999, IEEE NETWORK, V13, P26, DOI 10.1109/65.793688
   Heinanen J., 1999, Assured forwarding PHB group
   HOW YT, 2000, COMPUT NETW, P185
   ITU, 2000, BT50010 ITUR
   KIM JG, 2001, P SPIE VIS COMM IM P, P410
   Reibman AR, 2004, IEEE T MULTIMEDIA, V6, P327, DOI 10.1109/TMM.2003.822785
   Shao HR, 2001, SIGNAL PROCESS-IMAGE, V16, P763, DOI 10.1016/S0923-5965(01)00006-6
   Shin J, 2001, IEEE T MULTIMEDIA, V3, P219, DOI 10.1109/6046.923821
   Shin J, 2001, EUR T TELECOMMUN, V12, P217, DOI 10.1002/ett.4460120309
   YU H, 2001, P IEEE PAC RIM C COM, P369
   ZHANG F, 2002, P 1 WORKSH INT TEL S, P100
   2003, P ICASSP HONG KONG A, V5, P744
NR 15
TC 10
Z9 10
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2006
VL 8
IS 5
BP 1005
EP 1010
DI 10.1109/TMM.2006.879865
PG 6
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 091TY
UT WOS:000241052400012
DA 2024-07-18
ER

PT J
AU Sun, Y
   Ahmad, I
   Li, DD
   Zhang, YQ
AF Sun, Y
   Ahmad, I
   Li, DD
   Zhang, YQ
TI Region-based rate control and bit allocation for wireless video
   transmission
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE channel model; moving region detection; region-based rate control;
   wireless systems
ID RATE-CONTROL SCHEME; STATISTICS; ROBUST
AB In this paper, we propose a joint source-channel region-based rate control algorithm for real-time video transmissions over wireless systems. During the video transmission, the channel throughput available to the video encoder in the wireless systems is inherently variable, due to the retransmission of the error packets using the automatic repeat request (ARQ) error control. The variable data rate of the wireless system is characterized by the packet-level Gilbert two-state Markov Model, the parameters of which are extracted from the statistical properties of the channel information obtained from the wireless channel simulator. The proposed algorithm adopts a fast but effective block-based segmentation method to extract the regions of interest. Unlike traditional bit allocation methods used in the region/content-based rate control, the algorithm exploits the most effective criteria "coding qualities" as quantitative factors to directly control bit allocation among different regions so as to achieve better visual quality in the regions of interest. The computational complexity of the algorithm is low making it suitable for real-time applications. Compared with the MPEG-4 rate control algorithm, our algorithm can effectively enhance the perceptual quality for the regions of interest and significantly reduce the number of frame skipping; thereby, improve the smoothness of the video.
C1 Univ Cent Arkansas, Dept Comp Sci, Conway, AR 72035 USA.
   Univ Texas, Dept Comp Sci & Engn, Arlington, TX 76019 USA.
   Univ Texas, Dept Elect Engn, Arlington, TX 76019 USA.
   Microsoft Corp, Mobile & Embedded Devices Div, Redmond, WA 98052 USA.
C3 University of Central Arkansas; University of Texas System; University
   of Texas Arlington; University of Texas System; University of Texas
   Arlington; Microsoft
RP Univ Cent Arkansas, Dept Comp Sci, Conway, AR 72035 USA.
EM yusun@uca.edu; iahmad@cse.uta.edu; ldd@ieee.org; yzhang@microsoft.com
CR Aramvith S, 2002, INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY: CODING AND COMPUTING, PROCEEDINGS, P71, DOI 10.1109/ITCC.2002.1000362
   Aramvith S, 2001, IEEE T CIRC SYST VID, V11, P569, DOI 10.1109/76.920187
   Garcia L., 1994, PROBABILITY RANDOM P
   GILBERT EN, 1960, BELL SYST TECH J, V39, P1253, DOI 10.1002/j.1538-7305.1960.tb03959.x
   He ZH, 2002, IEEE T CIRC SYST VID, V12, P511, DOI 10.1109/TCSVT.2002.800313
   Hsu CY, 1999, IEEE J SEL AREA COMM, V17, P756, DOI 10.1109/49.768193
   Iskander CD, 2002, IEEE T COMMUN, V50, P1301, DOI 10.1109/TCOMM.2002.801465
   Jakes W. C, 1993, Microwave Mobile Communications
   Khansari M, 1996, IEEE T CIRC SYST VID, V6, P1, DOI 10.1109/76.486415
   Lee HJ, 2000, IEEE T CIRC SYST VID, V10, P878, DOI 10.1109/76.867926
   LI W, 2000, MPEG 4 VIDEO VERIFIC
   Lin CH, 1997, IEEE T CONSUM ELECTR, V43, P123, DOI 10.1109/30.585530
   Lin CW, 2003, IEICE T INF SYST, VE86D, P101
   Parsons J. D., 2000, The Mobile Radio Propagation Channel
   Rappaport TS., 1996, WIRELESS COMMUNICATI
   Ribas-Corbera J, 1999, IEEE T CIRC SYST VID, V9, P172, DOI 10.1109/76.744284
   Sadka AbdulH., 2002, COMPRESSED VIDEO COM
   Sikora T, 1997, IEEE T CIRC SYST VID, V7, P19, DOI 10.1109/76.554415
   Song HJ, 2004, IEEE T MULTIMEDIA, V6, P489, DOI 10.1109/TMM.2004.827488
   Sun Y, 2004, IEEE T CIRC SYST VID, V14, P1167, DOI 10.1109/TCSVT.2004.833164
   Sweeney P., 2002, ERROR CONTROL CODING
   Turin W, 1998, IEEE J SEL AREA COMM, V16, P1809, DOI 10.1109/49.737649
   WANG HS, P ICUPC 94, P160
   Yacoub MD, 1999, IEEE T VEH TECHNOL, V48, P790, DOI 10.1109/25.764995
   Yacoub MD, 2001, IEEE T VEH TECHNOL, V50, P1464, DOI 10.1109/25.966577
   Yokoyama Y, 1995, IEEE T CIRC SYST VID, V5, P500, DOI 10.1109/76.475892
   ZORZI M, ICUPC 95, P160
NR 27
TC 54
Z9 72
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2006
VL 8
IS 1
BP 1
EP 10
DI 10.1109/TMM.2005.861296
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 005UR
UT WOS:000234850000001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Pei, SC
   Chou, YZ
AF Pei, SC
   Chou, YZ
TI Scene-effect detection and insertion MPEG encoding scheme for video
   browsing and error concealment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE error concealment; MPEG; scene change; scene cut; video analysis; video
   transmission
ID HIGH-QUALITY VIDEO; TRANSMISSION; ALGORITHM; IMAGES
AB For an MPEG coding scheme, the encoder can do more than video compression. In this paper, a novel MPEG codec embedded with scene-effect information detection and insertion is proposed to provide more functionality at the decoder end. Based on the macroblock (MB) type of information that is generated simultaneously in the encoding process, a single-pass automatic scene-effect insertion MPEG coding scheme can be achieved. Using the USER-DATA of picture header, the video output bitstreams by our method still conform to the conventional MPEG decoding system. The proposed method provides a solution toward upgrading the existing MPEG codec with low complexity to accomplish at least two major advantages. Precise and effective video browsing resulting from the scene-effect extraction can significantly reduce the user's time to look up what they are interested in. For video transmission, the bitstreams containing scene-effect information can obtain better error concealment performance when scene changes are involved. Compared with the gain it achieves, the payout of our algorithm is very worthy with comparatively small efforts.
C1 Natl Taiwan Univ, Dept Elect Engn, Taipei 10764, Taiwan.
C3 National Taiwan University
RP Natl Taiwan Univ, Dept Elect Engn, Taipei 10764, Taiwan.
EM pei@cc.ee.ntu.edu.tw
CR Al-Mualla M, 1999, ELECTRON LETT, V35, P215, DOI 10.1049/el:19990174
   Alattar AM, 1998, IEEE T CONSUM ELECTR, V44, P43, DOI 10.1109/30.663729
   [Anonymous], 1993, Multimedia Systems, DOI DOI 10.1007/BF01210504
   ARMAN F, 1993, P SOC PHOTO-OPT INS, V1908, P2, DOI 10.1117/12.143638
   Arman F., 1993, Proceedings ACM Multimedia 93, P267, DOI 10.1145/166266.166297
   ARMAN F, 1994, P 2 ACM INT C MULT S, P97
   Arnold JF, 1999, SIGNAL PROCESS-IMAGE, V14, P607, DOI 10.1016/S0923-5965(98)00059-9
   Boyce JM, 1999, SIGNAL PROCESS-IMAGE, V15, P7, DOI 10.1016/S0923-5965(99)00021-1
   Bystrom M, 1999, IEEE T CIRC SYST VID, V9, P868, DOI 10.1109/76.785725
   CHANG SF, 1995, IEEE J SEL AREA COMM, V13, P1, DOI 10.1109/49.363151
   Chen MJ, 1997, IEEE T CIRC SYST VID, V7, P560, DOI 10.1109/76.585936
   Chung YJ, 1999, IEEE T CIRCUITS-II, V46, P951, DOI 10.1109/82.775393
   Feng J, 1997, IEEE T CONSUM ELECTR, V43, P183, DOI 10.1109/30.585539
   Fernando W. A. C., 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P294, DOI 10.1109/ICIP.1999.817120
   Gamaz N, 1998, 1998 IEEE SOUTHWEST SYMPOSIUM ON IMAGE ANALYSIS AND INTERPRETATION, P12, DOI 10.1109/IAI.1998.666852
   Gargi U, 2000, IEEE T CIRC SYST VID, V10, P1, DOI 10.1109/76.825852
   Golin SJ, 1998, P SOC PHOTO-OPT INS, V3653, P1464, DOI 10.1117/12.334656
   Hong MC, 1999, SIGNAL PROCESS-IMAGE, V14, P473, DOI 10.1016/S0923-5965(98)00061-7
   Kaiser S, 1999, SIGNAL PROCESS-IMAGE, V14, P655, DOI 10.1016/S0923-5965(98)00066-6
   Keck W, 1996, IEEE T CONSUM ELECTR, V42, P411, DOI 10.1109/30.536138
   KWOK W, 1993, IEEE T CONSUM ELECTR, V39, P455, DOI 10.1109/30.234620
   LAM WM, 1995, IEEE T IMAGE PROCESS, V4, P533, DOI 10.1109/83.382489
   Lee PJ, 1999, IEEE T CONSUM ELECTR, V45, P851, DOI 10.1109/30.793622
   LEE VHL, 1995, DRUG TARG D, V4, P3
   MENG J, 1995, IS T SPIE S P, V2419
   MENG JH, 1995, P SOC PHOTO-OPT INS, V2419, P14, DOI 10.1117/12.206359
   NAGASAKA A, 1991, VISUAL DATABASE SYST, V2, P119
   NAKAJIMA H, 1994, ELECTRON COMM JPN 3, V77, P12
   NGO CW, 1999, MULTIMEDIA COMPUTING, V1, P750
   Otsuji K., 1993, Proceedings ACM Multimedia 93, P251, DOI 10.1145/166266.166295
   Park JW, 1999, IEEE T CIRC SYST VID, V9, P1003, DOI 10.1109/76.795052
   Parthasarathy V, 1999, IEEE T IMAGE PROCESS, V8, P361, DOI 10.1109/83.748891
   Parthasarathy V, 1997, IEEE T CIRC SYST VID, V7, P358, DOI 10.1109/76.564113
   Patel NV, 1997, PATTERN RECOGN, V30, P583, DOI 10.1016/S0031-3203(96)00114-8
   Patel NV, 1996, IEE P-VIS IMAGE SIGN, V143, P315, DOI 10.1049/ip-vis:19960778
   Pei SC, 1999, IEEE T MULTIMEDIA, V1, P321, DOI 10.1109/6046.807952
   REDMILL DW, 1998, ELECT COMMUN ENG AUG
   ROWE LA, 1995, BERKELEY MPEG TOOLS
   SHIN T, P 1998 IEEE INT S CI, V4, P253
   Shyu HC, 1999, IEEE T CIRC SYST VID, V9, P937, DOI 10.1109/76.785732
   Sugano M, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 1, P888, DOI 10.1109/ICIP.1998.723663
   Suh JW, 1997, IEEE T CONSUM ELECTR, V43, P295, DOI 10.1109/30.628616
   SUN HF, 1995, IEEE T IMAGE PROCESS, V4, P470, DOI 10.1109/83.370675
   TSE K, 1995, P CAN C EL COMP ENG, V2, P827
   Wang Y, 1999, BIOFACTORS, V9, P3, DOI 10.1002/biof.5520090102
   Wang Z, 1998, IEEE T IMAGE PROCESS, V7, P1056, DOI 10.1109/83.701166
   Wei Q, 1997, P SOC PHOTO-OPT INS, V3022, P448, DOI 10.1117/12.263434
   Wu M, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 1, P893, DOI 10.1109/ICIP.1998.723664
   Yeo BL, 1995, IEEE T CIRC SYST VID, V5, P533, DOI 10.1109/76.475896
   YEO BL, 1996, P SPIE DIGITAL VIDEO, V2668, P47
   Yu GS, 1998, IEEE T CIRC SYST VID, V8, P422, DOI 10.1109/76.709409
   Zeng WJ, 1999, IEEE T CIRC SYST VID, V9, P648, DOI 10.1109/76.767129
   ZHANG FJ, 1989, ACTA MATH APPL SIN-E, V1, P1
   Zhu WW, 1998, IEEE T CIRC SYST VID, V8, P713, DOI 10.1109/76.728413
NR 54
TC 0
Z9 0
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2005
VL 7
IS 4
BP 606
EP 614
DI 10.1109/TMM.2005.850961
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 946XQ
UT WOS:000230607000002
DA 2024-07-18
ER

PT J
AU Kawaguchi, H
   Shin, Y
   Sakurai, T
AF Kawaguchi, H
   Shin, Y
   Sakurai, T
TI μITRON-LP:: Power-conscious real-time OS based on cooperative voltage
   scaling for multimedia applications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE application slicing; dynamic voltage scaling; embedded system; low
   power; MPEG-4; multimedia application; realtime OS
AB This paper presents a cooperative dynamic power management method and its implementation. The implementation consists of design of a real-time OS, applications including MPEG-4, and development of a supporting hardware platform with an off-the-shelf processor. We describe several factors that are important in the implementation and discuss its efficiency through experiment. The experimental results with the prototype system shows that 74% power saving is possible in multi-task multimedia environment.
C1 Univ Tokyo, Inst Ind Sci, Tokyo 1538505, Japan.
   IBM Corp, Thomas J Watson Res Ctr, Yorktown Hts, NY 10598 USA.
   Univ Tokyo, Ctr Collaborat Res, Tokyo 1538505, Japan.
C3 University of Tokyo; International Business Machines (IBM); University
   of Tokyo
RP Univ Tokyo, Inst Ind Sci, Tokyo 1538505, Japan.
EM kawapy@iis.u-tokyo.ac.jp; youngsoo@ee.kaist.ac.kr;
   tsakurai@iis.u-tokyo.ac.jp
RI Shin, Youngsoo/C-1621-2011; Kawaguchi, Hiroshi/S-6075-2016
OI Kawaguchi, Hiroshi/0000-0001-8677-4733
CR DITZEL DR, 2000, P INT S LOW POW HIGH, P1
   Hwang CH, 1997, 1997 IEEE/ACM INTERNATIONAL CONFERENCE ON COMPUTER-AIDED DESIGN - DIGEST OF TECHNICAL PAPERS, P28, DOI 10.1109/ICCAD.1997.643266
   Kawaguchi H, 2002, IEICE T ELECTRON, VE85C, P263
   Lee S, 2000, DES AUT CON, P806
   LEE S, 2000, P AS S PAC DES AUT C, P381
   LEE Y, 1999, P 6 INT C REAL TIM C, P272
   LIM SS, 1994, REAL TIM SYST SYMP P, P97, DOI 10.1109/REAL.1994.342726
   Okuma T, 2001, IEEE DES TEST COMPUT, V18, P31, DOI 10.1109/54.914613
   Shin Y, 2001, PROCEEDINGS OF THE IEEE 2001 CUSTOM INTEGRATED CIRCUITS CONFERENCE, P553, DOI 10.1109/CICC.2001.929841
   SHIN Y, 1999, P DES AUT C, P134
   Weiser M., 1994, Proceedings of the First USENIX Symposium on Operating Systems Design and Implementation (OSDI), P13
   Yao F, 1995, AN S FDN CO, P374, DOI 10.1109/SFCS.1995.492493
NR 12
TC 4
Z9 4
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2005
VL 7
IS 1
BP 67
EP 74
DI 10.1109/TMM.2004.840592
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 893CQ
UT WOS:000226697300007
DA 2024-07-18
ER

PT J
AU Su, K
   Kundur, D
   Hatzinakos, D
AF Su, K
   Kundur, D
   Hatzinakos, D
TI Statistical invisibility for collusion-resistant digital video
   watermarking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE linear collusion; robust imperceptible digital video watermarking;
   statistical invisibility; watermarking attack
AB In this paper, we present a theoretical framework for the linear collusion analysis of watermarked digital video sequences, and derive a new theorem equating a definition of statistical invisibility, collusion-resistance, and two practical watermark design rules. The proposed framework is simple and intuitive; the basic processing unit is the video frame and we consider secondorder statistical descriptions of their temporal inter-relationships. Within this analytical setup, we define the linear frame collusion attack, the analytic notion of a statistically invisible video watermark, and show that the latter is an effective counterattack against the former. Finally, to show how the theoretical results detailed in this paper can easily be applied to the construction of collusion-resistant video watermarks, we encapsulate the analysis into two practical video watermark design rules that play a key role in the subsequent development of a novel collusion-resistant video watermarking algorithm discussed in a companion paper.
C1 Univ Cambridge, Commun Engn Lab, Cambridge, England.
   Univ Toronto, Dept Elect & Comp Engn, Toronto, ON, Canada.
C3 University of Cambridge; University of Toronto
RP Univ Cambridge, Commun Engn Lab, Cambridge, England.
EM deepa@ee.tamu.edu
CR Barnett R, 1999, ELECTRON COMMUN ENG, V11, P173, DOI 10.1049/ecej:19990401
   Bloom JA, 1999, P IEEE, V87, P1267, DOI 10.1109/5.771077
   Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120
   Darmstaedter V, 1998, LECT NOTES COMPUT SC, V1425, P190
   Deguillaume F, 2000, PROC SPIE, V3971, P346, DOI 10.1117/12.384989
   Dittmann J, 1998, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS, P286, DOI 10.1109/MMCS.1998.693657
   Grimmett G., 1992, PROBABILITY RANDOM P
   GRIWODZ C, 1998, ACM INT C MULT, P21
   Hartung F, 1998, COMPUT GRAPH, V22, P425, DOI 10.1016/S0097-8493(98)00032-6
   HOLLIMAN M, 1999, P IEEE S CONT SEC DA
   Kalker T, 1999, PROC SPIE, V3657, P103, DOI 10.1117/12.344661
   Mobasseri BG, 1999, P SOC PHOTO-OPT INS, V3657, P96, DOI 10.1117/12.344660
   Pereira S, 2000, IEEE T IMAGE PROCESS, V9, P1123, DOI 10.1109/83.846253
   Su K, 2001, IEEE IMAGE PROC, P818, DOI 10.1109/ICIP.2001.959171
   SU K, 2005, IEEE T MULTIMEDIA, V7, P61
   Swanson MD, 1998, IEEE J SEL AREA COMM, V16, P540, DOI 10.1109/49.668976
   TANABE K, 1990, CRIT REV SURF CHEM, V90, P1
   TIRKEL AZ, 1993, P DIG IM COMP TECHN, P666
   TRAPPE W, 2002, P IEEE INT C AC SPEE, V4, P3309
   VANSCHYNDELL RG, 1994, IEEE IMAGE PROC, P86, DOI 10.1109/ICIP.1994.413536
   Voloshynovskiy S, 2000, LECT NOTES COMPUT SC, V1768, P211
NR 21
TC 63
Z9 73
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2005
VL 7
IS 1
BP 43
EP 51
DI 10.1109/TMM.2004.840617
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 893CQ
UT WOS:000226697300005
DA 2024-07-18
ER

PT J
AU Chang, CJ
   Yu, CH
   Chang, CS
   Lin, LF
AF Chang, CJ
   Yu, CH
   Chang, CS
   Lin, LF
TI Intelligent leaky bucket algorithms for sustainable-cell-rate usage
   parameter control in ATM networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE asynchronous transfer mode (ATM); fuzzy logic; leaky bucket algorithm;
   neural fuzzy; sustainable cell rate; usage parameter control
ID POLICING MECHANISMS; FUZZY
AB In this paper, we propose two intelligent leaky bucket algorithms for sustainable-cell-rate usage parameter control of multimedia transmission in asynchronous transfer mode networks. One is the fuzzy leaky bucket algorithm, in which a fuzzy increment controller (FIC) is incorporated with the conventional leaky bucket algorithm; the other is the neural fuzzy leaky bucket algorithm, where a neural fuzzy increment controller (NFIC) is added with the conventional leaky bucket algorithm. Both the FIC and the NFIC properly choose the long-term mean cell rate and the short-term mean cell rate as input variables to intelligently determine the increment value. Simulation results show that both intelligent leaky bucket algorithms have significantly outperformed the conventional leaky bucket algorithm, by responding about 160% faster when taking control actions against a nonconforming connection while reducing as much as 50% of the queueing delay experienced by a conforming connection. In addition, the neural fuzzy leaky bucket algorithm outperforms the fuzzy leaky bucket algorithm, in aspects of three performance measures such as selectivity, responsiveness, and queueing delay, especially when the traffic flow is bursty, dynamic, and nonstationary.
C1 Natl Chiao Tung Univ, Dept Commun Engn, Hsinchu 300, Taiwan.
C3 National Yang Ming Chiao Tung University
RP Chang, CJ (corresponding author), Natl Chiao Tung Univ, Dept Commun Engn, Hsinchu 300, Taiwan.
EM cjchang@cc.nctu.edu.tw; ckyu@zyxel.com.tw; dole@cht.com.tw;
   kawai.cm852@nctu.edu.tw
CR BONDE AR, 1994, IEEE ACM T NETWORK, V2, P337, DOI 10.1109/90.330414
   BUTTO M, 1991, IEEE J SEL AREA COMM, V9, P335, DOI 10.1109/49.76631
   Catania V, 1996, IEEE ACM T NETWORK, V4, P449, DOI 10.1109/90.502243
   Chen JX, 1999, COMPUT SCI ENG, V1, P12, DOI 10.1109/5992.743611
   Cheng RG, 1996, IEEE ACM T NETWORK, V4, P460, DOI 10.1109/90.502244
   DITTMANN L, 1991, IEEE J SEL AREA COMM, V9, P343, DOI 10.1109/49.76632
   GARROPPO RG, P ICC 98, P1119
   GUILLEMIN F, P INFOCOM 95, P1129
   LEE CC, 1990, IEEE T SYST MAN CYB, V20, P419, DOI 10.1109/21.52552
   Lin C.-.T., 1996, Neural Fuzzy Systems: A Neuro-Fuzzy Synergism to Intelligent Systems
   NDOUSSE TD, 1994, IEEE J SEL AREA COMM, V12, P1488, DOI 10.1109/49.339916
   RATHGEB EP, 1991, IEEE J SEL AREA COMM, V9, P325, DOI 10.1109/49.76630
   SHIODA S, P ICC 95, P965
   TARRAF AA, 1994, IEEE J SEL AREA COMM, V12, P1088, DOI 10.1109/49.310965
   ZIEMMERMANN HJ, 1991, FUZZY SET THEORY ITS, P11
NR 15
TC 5
Z9 7
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2004
VL 6
IS 5
BP 749
EP 759
DI 10.1109/TMM.2004.834860
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 854XI
UT WOS:000223936800007
DA 2024-07-18
ER

PT J
AU Bai, Z
   Liu, Z
   Li, GY
   Wang, Y
AF Bai, Zhen
   Liu, Zhi
   Li, Gongyang
   Wang, Yang
TI Adaptive Group-Wise Consistency Network for Co-Saliency Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Adaptation models; Decoding; Semantics; Global
   communication; Aggregates; Prediction algorithms; Co-saliency detection;
   content-adaptive layer; group consistency; intra-saliency priors;
   semantic information
ID OBJECT DETECTION; FUSION; SEGMENTATION
AB Co-saliency detection focuses on detecting common and salient objects among a group of images. With the application of deep learning in co-saliency detection, more accurate and more effective models are proposed in an end-to-end manner. However, two major drawbacks in these models hinder the further performance improvement of co-saliency detection: 1) the static manner-based inference, and 2) the constant quantity of input images. To address these limitations, we present a novel Adaptive Group-wise Consistency Network (AGCNet) with the ability of content-adaptive adjustment for a given image group with random quantity of images. In AGCNet, we first introduce intra-saliency priors generated from any off-the-shelf salient object detection model. Then, an Adaptive Group-wise Consistency (AGC) module is proposed to capture group consistency for each individual image, and is applied on three-scale features to capture the group consistency from different perspectives. This module is composed of two key components, where the content-adaptive group consistency block breaks the above limitations to adaptively capture the global group consistency with the assistance of intra-saliency priors and the ranking-based fusion block combines the consistency with individual attributes of each image feature to generate discriminative group consistency feature for each image. Following AGC modules, a specially designed Aggregated Decoder aggregates the three-scale group consistency features to adapt to co-salient objects with diverse scales for preliminary detection. Finally, we incorporate two normal decoders to progressively refine the preliminary detection and generate the final co-saliency maps. Extensive experiments on four benchmark datasets demonstrate that our AGCNet achieves competitive performance as compared with 19 state-of-the-art models, and the proposed modules experimentally show substantial practical merits.
C1 [Bai, Zhen; Liu, Zhi; Li, Gongyang] Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Shanghai 200444, Peoples R China.
   [Bai, Zhen; Liu, Zhi; Li, Gongyang] Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
   [Li, Gongyang] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
   [Wang, Yang] Univ Manitoba, Dept Compter Sci, Winnipeg, MB R3T 2N2, Canada.
C3 Shanghai University; Shanghai University; Nanyang Technological
   University; University of Manitoba
RP Liu, Z (corresponding author), Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Shanghai 200444, Peoples R China.; Liu, Z (corresponding author), Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
EM bz536476@163.com; liuzhisjtu@163.com; ligongyang@shu.edu.cn;
   ywang@cs.umanitoba.ca
RI Qi, Ling/KHE-3068-2024; Li, Gongyang/IXD-9078-2023; LIU, Zhi/D-4518-2012
OI LIU, Zhi/0000-0002-8428-1131; Bai, Zhen/0000-0001-9331-8525; , Gongyang
   Li/0000-0001-7324-1196
FU National Natural Science Foundation of China [62171269]; China
   Scholarship Council [202006890079]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62171269 and in part by the China
   Scholarship Council under Grant 202006890079.
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Batra D, 2010, PROC CVPR IEEE, P3169, DOI 10.1109/CVPR.2010.5540080
   Cao XC, 2014, IEEE T IMAGE PROCESS, V23, P4175, DOI 10.1109/TIP.2014.2332399
   Chen HT, 2010, IEEE IMAGE PROC, P1117, DOI 10.1109/ICIP.2010.5650014
   Cheng MM, 2021, INT J COMPUT VISION, V129, P2622, DOI [10.1007/s11263-021-01490-8, 10.1109/ICCV.2017.487]
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Cong RM, 2019, IEEE T MULTIMEDIA, V21, P1660, DOI 10.1109/TMM.2018.2884481
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fan DP, 2022, IEEE T PATTERN ANAL, V44, P4339, DOI 10.1109/TPAMI.2021.3060412
   Fan DP, 2020, PROC CVPR IEEE, P2916, DOI 10.1109/CVPR42600.2020.00299
   Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698
   Feng MY, 2019, PROC CVPR IEEE, P1623, DOI 10.1109/CVPR.2019.00172
   Fu HZ, 2013, IEEE T IMAGE PROCESS, V22, P3766, DOI 10.1109/TIP.2013.2260166
   Gao GS, 2021, IEEE T CIRC SYST VID, V31, P877, DOI 10.1109/TCSVT.2020.2992054
   Ge C., 2015, P IEEE INT C IM PROC, P1845
   Han JW, 2015, IEEE T CIRC SYST VID, V25, P1309, DOI 10.1109/TCSVT.2014.2381471
   Hsu KJ, 2019, PROC CVPR IEEE, P8838, DOI 10.1109/CVPR.2019.00905
   Hsu KJ, 2018, LECT NOTES COMPUT SC, V11209, P502, DOI 10.1007/978-3-030-01228-1_30
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu RY, 2021, AAAI CONF ARTIF INTE, V35, P7789
   Huang MK, 2019, NEUROCOMPUTING, V364, P310, DOI 10.1016/j.neucom.2019.07.054
   Jacobs D.E., 2010, ACM S USER INTERFACE, P219
   Jerripothula KR, 2018, IEEE T MULTIMEDIA, V20, P2466, DOI 10.1109/TMM.2018.2798294
   Jerripothula KR, 2016, IEEE T MULTIMEDIA, V18, P1896, DOI 10.1109/TMM.2016.2576283
   Jiang B, 2019, IEEE INT CON MULTI, P332, DOI 10.1109/ICME.2019.00065
   Jin W-D., 2020, Adv. Neural Inf. Process. Syst., P18749
   Kingma D. P., 2014, arXiv
   Kompella A, 2020, NEURAL COMPUT APPL, V32, P16571, DOI 10.1007/s00521-019-04265-y
   Li B, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P818
   Li CY, 2015, PROC CVPR IEEE, P2710, DOI 10.1109/CVPR.2015.7298887
   Li GY, 2021, IEEE T IMAGE PROCESS, V30, P3528, DOI 10.1109/TIP.2021.3062689
   Li GB, 2018, IEEE T NEUR NET LEAR, V29, P6038, DOI 10.1109/TNNLS.2018.2817540
   Li GB, 2016, IEEE T IMAGE PROCESS, V25, P5012, DOI 10.1109/TIP.2016.2602079
   Li HL, 2013, IEEE T MULTIMEDIA, V15, P1896, DOI 10.1109/TMM.2013.2271476
   Li HL, 2011, IEEE T IMAGE PROCESS, V20, P3365, DOI 10.1109/TIP.2011.2156803
   Li JL, 2015, IEEE IMAGE PROC, P2189, DOI 10.1109/ICIP.2015.7351189
   Li M., 2018, PROC BRIT MACH VIS C, P1
   Li XH, 2013, IEEE I CONF COMP VIS, P2976, DOI 10.1109/ICCV.2013.370
   Li YJ, 2015, IEEE SIGNAL PROC LET, V22, P588, DOI 10.1109/LSP.2014.2364896
   Lin HJ, 2019, IEEE I CONF COMP VIS, P3948, DOI 10.1109/ICCV.2019.00405
   Lin T.Y., 2014, P 13 EUR C COMP VIS, P740
   Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Liu Z., 2014, PROC IEEE INT C MULT, P1
   Liu Z, 2014, IEEE T IMAGE PROCESS, V23, P1937, DOI 10.1109/TIP.2014.2307434
   Liu Z, 2014, IEEE SIGNAL PROC LET, V21, P88, DOI 10.1109/LSP.2013.2292873
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Meng F., 2012, IEEE T MULTIMEDIA, V14, P1520
   Paszke A, 2019, ADV NEUR IN, V32
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Ren J., 2020, PROC IEEE INT C MULT, P1
   Ren JR, 2020, NEUROCOMPUTING, V371, P137, DOI 10.1016/j.neucom.2019.09.010
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tasi CC, 2019, IEEE T IMAGE PROCESS, V28, P56, DOI 10.1109/TIP.2018.2861217
   Tsai CC, 2020, IEEE T MULTIMEDIA, V22, P1016, DOI 10.1109/TMM.2019.2936803
   Tsai CC, 2017, INT CONF ACOUST SPEE, P1897, DOI 10.1109/ICASSP.2017.7952486
   Wang C, 2019, AAAI CONF ARTIF INTE, P8917
   Wang WG, 2019, PROC CVPR IEEE, P1448, DOI 10.1109/CVPR.2019.00154
   Wang ZQ, 2019, IEEE I CONF COMP VIS, P3977, DOI 10.1109/ICCV.2019.00408
   Wei LN, 2019, IEEE T IMAGE PROCESS, V28, P5052, DOI 10.1109/TIP.2019.2909649
   Wei YW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1437, DOI 10.1145/3343031.3351034
   Winn J, 2005, IEEE I CONF COMP VIS, P1800
   Wu Y, 2020, J VIS COMMUN IMAGE R, V67, DOI 10.1016/j.jvcir.2020.102761
   Wu Z, 2019, IEEE I CONF COMP VIS, P7263, DOI 10.1109/ICCV.2019.00736
   Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403
   Xue JR, 2013, PATTERN RECOGN, V46, P2874, DOI 10.1016/j.patcog.2013.03.028
   Yang B., 2019, P ADV INT C NEUR INF
   Yao XW, 2017, IEEE T IMAGE PROCESS, V26, P3196, DOI 10.1109/TIP.2017.2694222
   Ye LW, 2017, IEEE T MULTIMEDIA, V19, P1742, DOI 10.1109/TMM.2017.2693022
   Ye LW, 2015, IEEE SIGNAL PROC LET, V22, P2073, DOI 10.1109/LSP.2015.2458434
   Yinpeng Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11027, DOI 10.1109/CVPR42600.2020.01104
   Zha ZJ, 2020, IEEE T NEUR NET LEAR, V31, P2398, DOI 10.1109/TNNLS.2020.2967471
   Zhang DW, 2017, IEEE T PATTERN ANAL, V39, P865, DOI 10.1109/TPAMI.2016.2567393
   Zhang DW, 2016, IEEE T NEUR NET LEAR, V27, P1163, DOI 10.1109/TNNLS.2015.2495161
   Zhang DW, 2015, PROC CVPR IEEE, P2994, DOI 10.1109/CVPR.2015.7298918
   Zhang HS, 2022, IEEE T PATTERN ANAL, V44, P3577, DOI 10.1109/TPAMI.2021.3059783
   Zhang K., 2020, P IEEE CVF C COMP VI, P9050
   Zhang KH, 2019, PROC CVPR IEEE, P3090, DOI 10.1109/CVPR.2019.00321
   Zhang PP, 2017, IEEE I CONF COMP VIS, P202, DOI 10.1109/ICCV.2017.31
   Zhang Q., 2020, Adv. Neural Inf. Process. Syst.
   Zhang ZF, 2017, KSII T INTERNET INF, V11, P2576, DOI 10.3837/tiis.2017.05.015
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhao Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P455, DOI 10.1007/978-3-030-58610-2_27
   Zhou H., 2020, P IEEE C COMP VIS PA, P9141, DOI 10.1109/CVPR42600.2020.00916
   Zhou XF, 2016, IEEE SIGNAL PROC LET, V23, P517, DOI 10.1109/LSP.2016.2536743
NR 86
TC 7
Z9 7
U1 2
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 764
EP 776
DI 10.1109/TMM.2021.3138246
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA C4YC5
UT WOS:000961977900007
DA 2024-07-18
ER

PT J
AU Chen, C
   Wang, D
   Song, B
   Tan, H
AF Chen, Chen
   Wang, Dan
   Song, Bin
   Tan, Hao
TI Inter-Intra Modal Representation Augmentation With DCT-Transformer
   Adversarial Network for Image-Text Matching
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Transformers; Discrete cosine transforms; Background
   noise; Task analysis; Semantics; Visualization; Image-text matching;
   data augmentation; adversarial learning; transformer; DCT
AB Image-text matching has become a challenging task in the multimedia analysis field. Many advanced methods have been used to explore local and global cross-modal correspondence in matching. However, most methods ignore the importance of eliminating potential irrelevant features in the original features of each modality and cross-modal common feature. Moreover, the features extracted from regions in images and words in sentences contain cluttered background noise and different occlusion noise, which negatively affects alignment. Different from these methods, we propose a novel DCT-Transformer Adversarial Network (DTAN) for image-text matching in this paper. This work can obtain an effective metric based on two aspects: i) DCT-Transformer uses DCT (Discrete Cosine Transform) method based on a transformer mechanism to extract multi-domain common representations and eliminate irrelevant features from different modalities (inter-modal). Among them, DCT divides multi-modal content into chunks of different frequencies and quantifies them. ii) The adversarial network introduces an adversary idea by combining the original features of various single modalities and the multi-domain common representation, alleviating the background noise within each modality (intra-modal). The proposed adversarial feature augmentation method can easily obtain the common representation that is only useful for alignment. Extensive experiments are completed on the benchmark datasets Flickr30 K and MS-COCO, demonstrating the superiority of the DTAN model over the state-of-the-art methods.
C1 [Chen, Chen; Wang, Dan; Song, Bin] Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
   [Tan, Hao] Guangdong OPPO Mobile Telecommun Corp, Software Engn Div, Dongguan 523860, Peoples R China.
C3 Xidian University
RP Wang, D; Song, B (corresponding author), Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
EM chenchen_123@stu.xidian.edu.cn; danwang@xidian.edu.cn;
   bsong@mail.xidian.edu.cn; tanhao@oppo.com
OI Song, Bin/0000-0002-8096-3370
FU National Natural Science Foundation of China
FX No Statement Available
CR AHMED N, 1974, IEEE T COMPUT, VC 23, P90, DOI 10.1109/T-C.1974.223784
   Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Ben HX, 2022, IEEE T MULTIMEDIA, V24, P904, DOI 10.1109/TMM.2021.3060948
   Carvalho M, 2018, ACM/SIGIR PROCEEDINGS 2018, P35, DOI 10.1145/3209978.3210036
   Chen K., 2013, EFFICIENT ESTIMATION, P2
   Chen TL, 2020, AAAI CONF ARTIF INTE, V34, P10583
   Cui Y, 2018, PROC CVPR IEEE, P5804, DOI 10.1109/CVPR.2018.00608
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Diao HW, 2021, AAAI CONF ARTIF INTE, V35, P1218
   Faghri F, 2018, P BRIT MACH VIS C, P1
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Frid-Adar M, 2018, I S BIOMED IMAGING, P289, DOI 10.1109/ISBI.2018.8363576
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hui Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12652, DOI 10.1109/CVPR42600.2020.01267
   Ji Z., 2021, INT JOINT C ART INT, P765, DOI DOI 10.24963/IJCAI.2021/106
   Jin ZX, 2023, IEEE T MULTIMEDIA, V25, P1, DOI 10.1109/TMM.2021.3120194
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kingma D. P., 2014, arXiv
   Kipf T.N., 2017, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1609.02907
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Li KP, 2019, IEEE I CONF COMP VIS, P4653, DOI 10.1109/ICCV.2019.00475
   Li S, 2017, IEEE I CONF COMP VIS, P1908, DOI 10.1109/ICCV.2017.209
   Li X J., 2020, P EUR C COMP VIS, P121, DOI DOI 10.1007/978-3-030-58577-8_8
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu C., 2020, P IEEE CVF C COMP VI, P10921, DOI DOI 10.1109/CVPR42600.2020.01093
   Liu LC, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P227, DOI 10.1145/3474085.3475566
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Lu JS, 2019, ADV NEUR IN, V32
   Nam H, 2017, PROC CVPR IEEE, P2156, DOI 10.1109/CVPR.2017.232
   Qu LG, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1104, DOI 10.1145/3404835.3462829
   Qu LG, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1047, DOI 10.1145/3394171.3413961
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang YX, 2021, IEEE T MULTIMEDIA, V23, P3362, DOI 10.1109/TMM.2020.3024822
   Wang Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1793, DOI 10.1109/ICCV48922.2021.00183
   Wehrmann P, 2020, AAAI CONF ARTIF INTE, V34, P12313
   Wei KM, 2020, IEEE ACCESS, V8, P96237, DOI 10.1109/ACCESS.2020.2996407
   Wen KY, 2021, IEEE T CIRC SYST VID, V31, P2866, DOI 10.1109/TCSVT.2020.3030656
   Wu J, 2021, IEEE T MULTIMEDIA, V23, P2413, DOI 10.1109/TMM.2020.3011317
   Wu YL, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2088, DOI 10.1145/3343031.3350940
   Xi Wei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10938, DOI 10.1109/CVPR42600.2020.01095
   Yongzhi Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12783, DOI 10.1109/CVPR42600.2020.01280
   Young P., 2014, Transactions of the Association for Computational Linguistics, V2, P67
   Zhang Hongyi, 2018, MIXUP EMPIRICAL RISK, DOI DOI 10.48550/ARXIV.1710.09412
   Zhang K, 2023, IEEE T MULTIMEDIA, V25, P1320, DOI 10.1109/TMM.2022.3141603
   Zhang K, 2022, PROC CVPR IEEE, P15640, DOI 10.1109/CVPR52688.2022.01521
   Zhang Q, 2020, PROC CVPR IEEE, P3533, DOI 10.1109/CVPR42600.2020.00359
   Zheng ZD, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3383184
   Zhong HS, 2021, IEEE T MULTIMEDIA, V23, P1264, DOI 10.1109/TMM.2020.2995278
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
NR 52
TC 3
Z9 3
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8933
EP 8945
DI 10.1109/TMM.2023.3243665
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000056
DA 2024-07-18
ER

PT J
AU Chen, XY
   Mu, W
   Xu, CQ
   Yu, Z
   Yang, SJ
   Ke, J
   Qing, L
   Zhong, LJ
   Muntean, GM
AF Chen, Xingyan
   Mu, Wang
   Xu, Changqiao
   Yu, Zhao
   Yang, Shujie
   Ke, Jiang
   Qing, Li
   Zhong, Lujie
   Muntean, Gabriel-Miro
TI FedLive: A Federated Transmission Framework for Panoramic Livecast With
   Reinforced Variational Inference
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Streaming media; Training; Predictive models; Bandwidth; Collaboration;
   Behavioral sciences; Solid modeling; Federated learning; reinforcement
   learning; video streaming; virtual reality
ID VIDEO; DELIVERY; CHALLENGES; PREDICTION; VIEW
AB Providing premium panoramic livecast services to worldwide viewers considering their ultra-high data rate and delay-sensitivity is a significant challenge in the current network delivery environment. Therefore, it is important to design an efficient way of improving viewer quality of experience while conserving bandwidth resources. In this context, this paper introduces a novel cost-efficient federated transmission framework called FedLive and a set of algorithms to support it. First a gradient-based clustering method is proposed to group the geo-distributed viewers with similar viewing behavior into content delivery alliances by exploiting the geometric properties of the gradient loss. Next, a Reinforced Variational Inference (RVI) structure-based approach is proposed to assist with the collaborative training of the viewer field of view (FoV) prediction model while also accelerating the tile delivery process. A novel prediction-based asynchronous delivery algorithm is designed in which both the high accuracy FoV prediction and efficient live 360(degrees) video transmission are achieved in a decentralized manner. FedLive was implemented for testing and an open source code is made available. Finally, the proposed solution was evaluated against a benchmark and three alternative state-of-the-art solutions using a real-world dataset. The experimental results show that our approach provides the highest prediction accuracy, better service performance, and saves bandwidth when compared with the other solutions.
C1 [Chen, Xingyan; Yu, Zhao; Qing, Li] Southwestern Univ Finance & Econ, Financial Intelligence & Financial Engn Key Lab Si, Inst Digital Econ & Interdisciplinary Sci Innovat, Sch Comp & Artificial Intelligence, Chengdu 611130, Sichuan, Peoples R China.
   [Mu, Wang; Xu, Changqiao; Yang, Shujie; Ke, Jiang] Beijing Univ Posts & Telecommun, State Key Lab Networking & Switching Technol, Beijing 100876, Peoples R China.
   [Zhong, Lujie] Capital Normal Univ, Informat Engn Coll, Beijing 100876, Peoples R China.
   [Muntean, Gabriel-Miro] Dublin City Univ, Sch Elect Engn, Glasnevin, Dublin, Ireland.
C3 Southwestern University of Finance & Economics - China; Beijing
   University of Posts & Telecommunications; Capital Normal University;
   Dublin City University
RP Yu, Z (corresponding author), Southwestern Univ Finance & Econ, Financial Intelligence & Financial Engn Key Lab Si, Inst Digital Econ & Interdisciplinary Sci Innovat, Sch Comp & Artificial Intelligence, Chengdu 611130, Sichuan, Peoples R China.
EM xychen@swufe.edu.cn; muwang@tsinghua.edu.cn; cqxu@bupt.edu.cn;
   zhaoyu@swufe.edu.cn; sjyang@bupt.edu.cn; jke@bupt.edu.cn;
   Liq_t@swufe.edu.cn; zhonglj@cnu.edu.cn; gabriel.muntean@dcu.ie
RI Chen, Xingyan/AFL-4355-2022
OI Chen, Xingyan/0000-0003-0861-2100; Zhao, Yu/0000-0002-8454-0025;
   Muntean, Gabriel-Miro/0000-0002-9332-4770; Zhong,
   Lujie/0000-0002-2111-0896
FU Science Foundation Ireland Research Centres
FX No Statement Available
CR Nguyen A, 2019, PROCEEDINGS OF THE 10TH ACM MULTIMEDIA SYSTEMS CONFERENCE (ACM MMSYS'19), P279, DOI 10.1145/3304109.3325820
   Nguyen A, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1190, DOI 10.1145/3240508.3240669
   [Anonymous], 2017, P 27 NOSSDAV, DOI DOI 10.1145/3083165.3083180
   Ban YX, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102836
   Bao Y., 2017, P 14 ANN IEEE INT C, P1, DOI DOI 10.1109/SAHCN.2017.7964928
   Biatek T, 2020, IEEE T BROADCAST, V66, P322, DOI 10.1109/TBC.2019.2941073
   Chen J., 2021, P IEEE C COMP COMM, P1
   Chen JY, 2021, IEEE T MULTIMEDIA, V23, P3853, DOI 10.1109/TMM.2020.3033127
   Chen XY, 2021, IEEE INFOCOM SER, DOI 10.1109/INFOCOM42981.2021.9488868
   Chen XY, 2021, IEEE T CIRC SYST VID, V31, P4470, DOI 10.1109/TCSVT.2020.3047859
   Cheng Y., 2019, Synth. Lectures Artif. Intell. Mach. Learn., V13, P1, DOI 10.2200/S00960ED2V01Y201910AIM043
   Cisco, 2018, White Paper
   Corbillon X, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P199, DOI 10.1145/3083187.3083215
   Cui EF, 2022, T EMERG TELECOMMUN T, V33, DOI 10.1002/ett.4485
   Dasari M, 2020, IEEE INFOCOM SER, P1977, DOI [10.1109/infocom41043.2020.9155477, 10.1109/INFOCOM41043.2020.9155477]
   Duan HH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P153, DOI 10.1145/3474085.3479238
   Eltobgy O, 2020, IEEE T MULTIMEDIA, V22, P3139, DOI 10.1109/TMM.2020.2973855
   Feng XL, 2021, IEEE T VIS COMPUT GR, V27, P2736, DOI 10.1109/TVCG.2021.3067686
   Feng XL, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P800, DOI [10.1109/VR46266.2020.00005, 10.1109/VR46266.2020.1584727730619]
   Ghaznavi-Youvalari R, 2019, IEEE T CIRC SYST VID, V29, P3106, DOI 10.1109/TCSVT.2018.2874179
   Haarnoja T, 2018, PR MACH LEARN RES, V80
   He G, 2018, IEEE COMMUN LETT, V22, P25, DOI 10.1109/LCOMM.2017.2764021
   Hou XS, 2021, IEEE T MULTIMEDIA, V23, P716, DOI 10.1109/TMM.2020.2987693
   iLab H., 2017, Cloud vr solution white paper
   Jiang ZQ, 2021, IEEE T BROADCAST, V67, P409, DOI 10.1109/TBC.2020.3028286
   Kim HK, 2018, APPL ERGON, V69, P66, DOI 10.1016/j.apergo.2017.12.016
   Kim J, 2020, SIGCOMM '20: PROCEEDINGS OF THE 2020 ANNUAL CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION ON THE APPLICATIONS, TECHNOLOGIES, ARCHITECTURES, AND PROTOCOLS FOR COMPUTER COMMUNICATION, P107, DOI 10.1145/3387514.3405856
   Levine S, 2018, Arxiv, DOI [arXiv:1805.00909, 10.48550/arXiv.1805.00909]
   Maniotis P, 2020, IEEE T MULTIMEDIA, V22, P2382, DOI 10.1109/TMM.2019.2957993
   Mao YX, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3696, DOI 10.1145/3394171.3413751
   Mnih V, 2016, PR MACH LEARN RES, V48
   Nasrabadi AT, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1689, DOI 10.1145/3123266.3123414
   Pang HT, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1217, DOI 10.1145/3240508.3240642
   Pang HT, 2019, IEEE INFOCOM SER, P991, DOI 10.1109/INFOCOM.2019.8737395
   Qiao ML, 2021, IEEE T MULTIMEDIA, V23, P748, DOI 10.1109/TMM.2020.2987682
   Rondón MFR, 2020, MMSYS'20: PROCEEDINGS OF THE 2020 MULTIMEDIA SYSTEMS CONFERENCE, P279, DOI 10.1145/3339825.3394934
   Sattler F, 2021, IEEE T NEUR NET LEAR, V32, P3710, DOI 10.1109/TNNLS.2020.3015958
   Tan JH, 2022, INFORM SCIENCES, V618, P317, DOI 10.1016/j.ins.2022.11.013
   Verma R, 2022, T EMERG TELECOMMUN T, V33, DOI 10.1002/ett.4535
   Wang M, 2019, IEEE INTERNET THINGS, V6, P9475, DOI 10.1109/JIOT.2019.2929263
   Watkins C. J. C. H., 1989, Learning from Delayed Rewards
   Weber T., 2015, ADV NEURAL INFORM PR
   Wu CL, 2020, AAAI CONF ARTIF INTE, V34, P14003
   Wu CL, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P193, DOI 10.1145/3083187.3083210
   Xie L, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P564, DOI 10.1145/3240508.3240556
   Xie L, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P315, DOI 10.1145/3123266.3123291
   Yang SJ, 2022, T EMERG TELECOMMUN T, V33, DOI 10.1002/ett.4430
   Yaqoob A, 2021, IEEE T BROADCAST, V67, P746, DOI 10.1109/TBC.2021.3105022
   Yaqoob A, 2020, IEEE COMMUN SURV TUT, V22, P2801, DOI 10.1109/COMST.2020.3006999
   Zhang KQ, 2018, IEEE DECIS CONTR P, P2771, DOI 10.1109/CDC.2018.8619581
   Zhang YX, 2019, IEEE INFOCOM SER, P1252, DOI [10.1109/INFOCOM.2019.8737361, 10.1109/infocom.2019.8737361]
   Zhong LJ, 2023, IEEE T MOBILE COMPUT, V22, P4405, DOI 10.1109/TMC.2022.3162147
   Zink M, 2019, P IEEE, V107, P639, DOI 10.1109/JPROC.2019.2894817
NR 53
TC 2
Z9 2
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8471
EP 8486
DI 10.1109/TMM.2023.3237325
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000016
DA 2024-07-18
ER

PT J
AU Cheng, WL
   Tang, W
   Huang, Y
   Luo, YW
   Wang, L
AF Cheng, Wenlong
   Tang, Wei
   Huang, Yan
   Luo, Yiwen
   Wang, Liang
TI A Reconstruction-Based Visual-Acoustic-Semantic Embedding Method for
   Speech-Image Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Speech-image retrieval; tri-modal ranking loss; cycle-consistency loss;
   visual-acoustic-semantic embedding
ID NATURAL-LANGUAGE; NETWORK
AB Speech-image retrieval aims at learning the relevance between image and speech.Prior approaches are mainly based on bi-modal contrastive learning, which can not alleviate the cross-modal heterogeneous issue between visual and acoustic modalities well. To address this issue, we propose a visual-acoustic-semantic embedding (VASE) method. First, we propose a tri-modal ranking loss by taking advantage of semantic information corresponding to the acoustic data, which introduces the auxiliary alignment to enhance the alignment between image and speech. Second, we introduce a cycle-consistency loss based on feature reconstruction. It can further alleviate the heterogeneous issue between different data modalities (e.g., visual-acoustic, visual-textual and acoustic-textual). Extensive experiments have demonstrated the effectiveness of our proposed method. In addition, our VASE model achieves state-of-the-art performance on the speech-image retrieval task on the Flickr8K [Harwath and Glass, 2015]s and Places [Harwath et al., 2018] datasets.
C1 [Cheng, Wenlong; Tang, Wei; Huang, Yan; Wang, Liang] Chinese Acad Sci CASIA, Inst Automat, Ctr Res Intelligent Percept & Comp CRIPAC, Natl Lab Pattern Recognit NLPR, Beijing 100190, Peoples R China.
   [Cheng, Wenlong; Tang, Wei; Huang, Yan; Wang, Liang] Univ Chinese Acad Sci UCAS, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
   [Luo, Yiwen] Xian Jiaotong Univ XJTU, Inst Artificial Intelligence & Robot IAIR, Xian 710049, Peoples R China.
   [Wang, Liang] Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Xi'an
   Jiaotong University; Chinese Academy of Sciences
RP Wang, L (corresponding author), Chinese Acad Sci CASIA, Inst Automat, Ctr Res Intelligent Percept & Comp CRIPAC, Natl Lab Pattern Recognit NLPR, Beijing 100190, Peoples R China.; Wang, L (corresponding author), Univ Chinese Acad Sci UCAS, Sch Artificial Intelligence, Beijing 100049, Peoples R China.; Wang, L (corresponding author), Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Beijing 100190, Peoples R China.
EM wenlong.cheng@cripac.ia.ac.cn; tangweirichard@163.com;
   yhuang@nlpr.ia.ac.cn; luoyiwen@mail.nwpu.edu.cn; wangliang@nlpr.ia.ac.cn
RI zhang, zheng/KBQ-7815-2024; zhao, wenqing/KEZ-9488-2024
FU National Key Research and Development Program of China [2018AAA0100400];
   National Natural Science Foundation of China [61721004, U1803261,
   61976132]; Beijing Nova Program [Z201100006820079]; Key Research Program
   of Frontier Sciences CAS [ZDBS-LY-JSC032]; CAS-AIR
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2018AAA0100400, in part by the
   National Natural Science Foundation of China under Grants 61721004,
   U1803261, and 61976132, in part by the Beijing Nova Program under Grant
   Z201100006820079, in part by the Key Research Program of Frontier
   Sciences CAS under Grant ZDBS-LY-JSC032, and in part by CAS-AIR.
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Nguyen AT, 2012, IEEE INT CONF AUTOM, P70, DOI 10.1145/2351676.2351687
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Aytar Y, 2017, Arxiv, DOI arXiv:1706.00932
   Aytar Y, 2018, IEEE T PATTERN ANAL, V40, P2303, DOI 10.1109/TPAMI.2017.2753232
   Bengio S, 2014, INTERSPEECH, P1053
   Castrejon L, 2016, PROC CVPR IEEE, P2940, DOI 10.1109/CVPR.2016.321
   Chen YX, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12010084
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Dag JNO, 2004, 12TH IEEE INTERNATIONAL REQUIREMENTS ENGINEERING CONFERENCE, PROCEEDINGS, P283
   Dwibedi D, 2019, PROC CVPR IEEE, P1801, DOI 10.1109/CVPR.2019.00190
   Faghri F, 2018, P BRIT MACH VIS C, P1
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   Gao DH, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P2251, DOI 10.1145/3397271.3401430
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo M, 2019, IEEE J-STARS, V12, P4644, DOI 10.1109/JSTARS.2019.2949220
   Guo WY, 2020, AAAI CONF ARTIF INTE, V34, P91
   Haoran Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P18, DOI 10.1007/978-3-030-58586-0_2
   Harwath D, 2020, INT J COMPUT VISION, V128, P620, DOI 10.1007/s11263-019-01205-0
   Harwath D, 2018, LECT NOTES COMPUT SC, V11210, P659, DOI 10.1007/978-3-030-01231-1_40
   Harwath D, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P506, DOI 10.18653/v1/P17-1047
   Harwath D, 2015, 2015 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P237, DOI 10.1109/ASRU.2015.7404800
   Harwath David, 2016, Advances in Neural Information Processing Systems, P1858
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hodosh M, 2013, J ARTIF INTELL RES, V47, P853, DOI 10.1613/jair.3994
   Hossain MZ, 2019, ACM COMPUT SURV, V51, DOI 10.1145/3295748
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang Y, 2019, IEEE I CONF COMP VIS, P5773, DOI 10.1109/ICCV.2019.00587
   Huang Y, 2018, PROC CVPR IEEE, P6163, DOI 10.1109/CVPR.2018.00645
   Huang Y, 2015, IEEE T MULTIMEDIA, V17, P1923, DOI 10.1109/TMM.2015.2476658
   Hui Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12652, DOI 10.1109/CVPR42600.2020.01267
   Kaiser L., 2016, ADV NEURAL INFORM PR, P3781
   Karpathy A, 2014, ADV NEUR IN, V27
   Kingma D. P., 2014, arXiv
   Kiros R, 2014, Arxiv, DOI arXiv:1411.2539
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Li D., 2003, P 11 ACM INT C MULTI, P604
   Li KP, 2019, IEEE I CONF COMP VIS, P4653, DOI 10.1109/ICCV.2019.00475
   Liu C., 2020, P IEEE CVF C COMP VI, P10921, DOI DOI 10.1109/CVPR42600.2020.01093
   Liu Y, 2019, PATTERN RECOGN, V93, P365, DOI 10.1016/j.patcog.2019.05.008
   Mao G, 2018, IAPR WORKSHOP PATTER, P1, DOI [DOI 10.1109/PRRS.2018.8486338, 10.1109/PRRS.2018.8486338]
   Matsubara T, 2021, IEICE T INF SYST, VE104D, P24, DOI 10.1587/transinf.2020MUP0003
   Rashtchian C., 2010, P NAACL HLT 2010 WOR, V2010, P139, DOI DOI 10.5555/1866696.1866717
   Runeson P, 2007, PROC INT CONF SOFTW, P499
   Shen JL, 2021, INFORM SCIENCES, V569, P469, DOI 10.1016/j.ins.2020.11.026
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Socher R., 2014, Trans Assoc Comput Linguist, V2, P207, DOI [DOI 10.1162/TACLA00177, 10.1162/tacl_a_00177, DOI 10.1162/TACL_A_00177]
   Sun C, 2011, INT J CHEM ENG, V2011, DOI 10.1155/2011/545234
   Surfs D., 2018, P EUR C COMP VIS WOR
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Thung F, 2013, WORK CONF REVERSE EN, P182, DOI 10.1109/WCRE.2013.6671293
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang JB, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P63, DOI 10.1145/3240508.3240538
   Wang KY, 2016, IEEE T PATTERN ANAL, V38, P2010, DOI 10.1109/TPAMI.2015.2505311
   Wang W., 2020, Pattern Recognit., V102
   Wang XY, 2008, ICSE'08 PROCEEDINGS OF THE THIRTIETH INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING, P461, DOI 10.1145/1368088.1368151
   Wu Q, 2017, COMPUT VIS IMAGE UND, V163, P21, DOI 10.1016/j.cviu.2017.05.001
   Xu L, 2014, ADV NEUR IN, V27
   Yu J, 2020, IEEE T MULTIMEDIA, V22, P3196, DOI 10.1109/TMM.2020.2972830
   Zheng AH, 2022, IEEE T MULTIMEDIA, V24, P338, DOI 10.1109/TMM.2021.3050089
   Zheng ZD, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3383184
   Zhou BL, 2014, ADV NEUR IN, V27
NR 65
TC 0
Z9 0
U1 2
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4067
EP 4080
DI 10.1109/TMM.2022.3171090
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA W1OJ1
UT WOS:001089390200002
DA 2024-07-18
ER

PT J
AU Dai, LH
   Song, X
   Liu, XH
   Li, CQ
   Shi, ZH
   Chen, J
   Brooks, M
AF Dai, Linhui
   Song, Xiang
   Liu, Xiaohong
   Li, Chengqi
   Shi, Zhihao
   Chen, Jun
   Brooks, Martin
TI Enabling Trimap-Free Image Matting With a Frequency-Guided
   Saliency-Aware Network via Joint Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image matting; joint-task learning; frequency-guided attention
ID ATTENTION
AB This paper presents a strategic approach to tackling trimap-free natural image matting. Specifically, to address the false detection issue of existing trimap-free matting algorithms when the foreground object is not uniquely defined, we design a novel tangled structure (TangleNet) to handle foreground detection and matting prediction simultaneously. TangleNet enables information exchange between foreground segmentation and alpha prediction, producing high-quality alpha mattes for the most salient foreground object based on RGB inputs alone. TangleNet boosts network performance with a frequency-guided attention mechanism utilizing wavelet data. Additionally, we pretrain for salient object detection to aid in the foreground segmentation. Experimental results demonstrate that TangleNet is on par with the state-of-the-art matting methods requiring additional inputs, and outperforms all previous trimap-free algorithms in terms of both qualitative and quantitative results.
C1 [Dai, Linhui; Song, Xiang; Li, Chengqi; Shi, Zhihao; Chen, Jun] McMaster Univ, Dept Elect & Comp Engn, Hamilton, ON L8S 4K1, Canada.
   [Liu, Xiaohong] Shanghai Jiao Tong Univ, John Hopcroft Ctr, Shanghai 200240, Peoples R China.
   [Brooks, Martin] ShapeVis Inc, Ottawa, ON K2P 0A4, Canada.
C3 McMaster University; Shanghai Jiao Tong University
RP Liu, XH (corresponding author), Shanghai Jiao Tong Univ, John Hopcroft Ctr, Shanghai 200240, Peoples R China.
EM dail5@mcmaster.ca; songx5@mcmaster.ca; xiaohongliu@sjtu.edu.cn;
   lic222@mcmaster.ca; shiz31@mcmaster.ca; chenjun@mcmaster.ca;
   brooks@shapevision.biz
OI Liu, Xiaohong/0000-0001-6377-4730
FU Natural Sciences and Engineering Research Council (NSERC) of Canada
FX This work was supported in part by the Natural Sciences and Engineering
   Research Council (NSERC) of Canada through a Discovery Grant.
CR Aksoy Y, 2017, PROC CVPR IEEE, P228, DOI 10.1109/CVPR.2017.32
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Cai SF, 2019, IEEE I CONF COMP VIS, P8818, DOI 10.1109/iccv.2019.00891
   Cao Y, 2019, IEEE INT CONF COMP V, P1971, DOI 10.1109/ICCVW.2019.00246
   Chen QF, 2013, IEEE T PATTERN ANAL, V35, P2175, DOI 10.1109/TPAMI.2013.18
   Chen Q, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P618, DOI 10.1145/3240508.3240610
   Cho D, 2016, LECT NOTES COMPUT SC, V9906, P626, DOI 10.1007/978-3-319-46475-6_39
   Chuang YY, 2001, PROC CVPR IEEE, P264
   Dai YT, 2021, PROC CVPR IEEE, P6837, DOI 10.1109/CVPR46437.2021.00677
   De Boer PT, 2005, ANN OPER RES, V134, P19, DOI 10.1007/s10479-005-5724-z
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Feng XX, 2016, LECT NOTES COMPUT SC, V9906, P204, DOI 10.1007/978-3-319-46475-6_13
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Grady L, 2005, PROCEEDINGS OF THE FIFTH IASTED INTERNATIONAL CONFERENCE ON VISUALIZATION, IMAGING, AND IMAGE PROCESSING, P423
   Hou QQ, 2019, IEEE I CONF COMP VIS, P4129, DOI 10.1109/ICCV.2019.00423
   Hu J, 2018, ADV NEURAL INFORM PR, P9401, DOI DOI 10.5555/3327546.3327612
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jiang B, 2021, IEEE T MULTIMEDIA, V23, P1343, DOI 10.1109/TMM.2020.2997184
   Jinlin Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8560, DOI 10.1109/CVPR42600.2020.00859
   Kaiming He, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2049, DOI 10.1109/CVPR.2011.5995495
   Kendall A, 2018, PROC CVPR IEEE, P7482, DOI 10.1109/CVPR.2018.00781
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Li YY, 2020, AAAI CONF ARTIF INTE, V34, P11450
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Linhui Dai, 2020, Proceedings of the 16th European Conference on Computer Vision - ECCV 2020 Workshops. Lecture Notes in Computer Science (LNCS 12537), P185, DOI 10.1007/978-3-030-67070-2_11
   Liu PJ, 2018, IEEE COMPUT SOC CONF, P886, DOI 10.1109/CVPRW.2018.00121
   Liu XH, 2019, IEEE I CONF COMP VIS, P7313, DOI 10.1109/ICCV.2019.00741
   Liu YH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7535, DOI 10.1109/ICCV48922.2021.00746
   Liu YH, 2022, IEEE T MULTIMEDIA, V24, P2727, DOI 10.1109/TMM.2021.3087007
   Lu H, 2019, IEEE I CONF COMP VIS, P3265, DOI 10.1109/ICCV.2019.00336
   Luo XT, 2020, IEEE COMPUT SOC CONF, P1687, DOI 10.1109/CVPRW50498.2020.00218
   Lutz S., 2018, BMVC
   Máttyus G, 2017, IEEE I CONF COMP VIS, P3458, DOI 10.1109/ICCV.2017.372
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Rhemann C, 2009, PROC CVPR IEEE, P1826, DOI 10.1109/CVPRW.2009.5206503
   Sengupta S, 2020, PROC CVPR IEEE, P2288, DOI 10.1109/CVPR42600.2020.00236
   Shahrian E, 2012, PROC CVPR IEEE, P718, DOI 10.1109/CVPR.2012.6247741
   Smith A. R., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P259, DOI 10.1145/237170.237263
   Sun J, 2004, ACM T GRAPHIC, V23, P315, DOI 10.1145/1015706.1015721
   Tang JW, 2019, PROC CVPR IEEE, P3050, DOI 10.1109/CVPR.2019.00317
   Vandenhende Simon, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P527, DOI 10.1007/978-3-030-58548-8_31
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang J, 2005, IEEE I CONF COMP VIS, P936
   Wang Jue, 2007, P IEEE C COMP VIS PA, P1
   Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xu N, 2017, PROC CVPR IEEE, P311, DOI 10.1109/CVPR.2017.41
   Youwei Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9410, DOI 10.1109/CVPR42600.2020.00943
   Yu Qiao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13673, DOI 10.1109/CVPR42600.2020.01369
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhang YK, 2019, PROC CVPR IEEE, P7461, DOI 10.1109/CVPR.2019.00765
   Zhang ZY, 2018, LECT NOTES COMPUT SC, V11214, P238, DOI 10.1007/978-3-030-01249-6_15
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhou FF, 2021, IEEE T CIRC SYST VID, V31, P2192, DOI 10.1109/TCSVT.2020.3024213
NR 58
TC 1
Z9 1
U1 1
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4868
EP 4879
DI 10.1109/TMM.2022.3183403
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300019
DA 2024-07-18
ER

PT J
AU Deng, X
   Feng, SH
   Lyu, GY
   Wang, T
   Lang, CY
AF Deng, Xiang
   Feng, Songhe
   Lyu, Gengyu
   Wang, Tao
   Lang, Congyan
TI Beyond Word Embeddings: Heterogeneous Prior Knowledge Driven Multi-Label
   Image Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Correlation; Semantics; Prototypes; Visualization; Transformers;
   Knowledge engineering; Image classification; Multi-label image
   classification; heterogeneous prior knowledge; graph convolutional
   network; multi-head attention
AB Multi-Label Image Classification (MLIC) is a fundamental yet challenging task which aims to recognize multiple labels from given images. The key to solve MLIC lies in how to accurately model the correlation between labels. Recent studies often adopt Graph Convolutional Network (GCN) to model label dependencies with word embeddings as prior knowledge. However, classical word embeddings typically contain redundant information due to the imperfect distributional hypothesis it relies on, which may degrade model generalizability. To tackle this problem, we propose a novel deep learning framework termed Visual-Semantic based Graph Convolutional Network (VSGCN), which alleviates the negative impact of redundant information by utilizing heterogeneous sources of prior knowledge. Specifically, we construct both visual prototype and semantic prototype for each label as heterogeneous prior label representations, which are further mapped to multi-label classifiers via two Multi-Head GCNs separately. The Multi-Head GCN mechanism proposed in this paper aims to guide the information propagation between prototypes for each label, which constructs multiple correlation graphs to simultaneously model the label correlation in different subspaces. Notably, we alleviate the negative influence of needless information by decreasing the inconsistency of predictions that come from visual space and semantic space. Extensive experiments conducted on various multi-label image datasets demonstrate the superiority of our proposed method.
C1 [Deng, Xiang; Feng, Songhe; Lyu, Gengyu; Wang, Tao; Lang, Congyan] Beijing Jiaotong Univ, Beijing Key Lab Traff Data Anal & Min, Beijing 100044, Peoples R China.
   [Deng, Xiang; Feng, Songhe; Lyu, Gengyu; Wang, Tao; Lang, Congyan] Beijing Jiaotong Univ, Sch Comp & Informat Technol, Beijing 100044, Peoples R China.
C3 Beijing Jiaotong University; Beijing Jiaotong University
RP Feng, SH (corresponding author), Beijing Jiaotong Univ, Beijing Key Lab Traff Data Anal & Min, Beijing 100044, Peoples R China.
EM 20120346@bjtu.edu.cn; shfeng@bjtu.edu.cn; 18112030@bjtu.edu.cn;
   twang@bjtu.edu.cn; cylang@bjtu.edu.cn
FU Beijing Natural Science Foundation [4202058, 4202057, 4202060]; National
   Natural Science Foundation of China [61872032, 62072027, 62076021];
   National Key Research and Development Project [2018AAA0100300]
FX This work was supported in part by the Beijing Natural Science
   Foundation under Grants 4202058, 4202057, and 4202060, in part by the
   National Natural Science Foundation of China under Grants 61872032,
   62072027, and 62076021, and in part by the National Key Research and
   Development Project under Grant 2018AAA0100300.
CR [Anonymous], 2016, P 26 INT C COMP LING
   CHEN B, IEEE T SYST MAN CYBE
   Chen TS, 2022, IEEE T PATTERN ANAL, V44, P1371, DOI 10.1109/TPAMI.2020.3025814
   Chen TS, 2019, IEEE I CONF COMP VIS, P522, DOI 10.1109/ICCV.2019.00061
   Chen ZM, 2023, IEEE T PATTERN ANAL, V45, P6969, DOI 10.1109/TPAMI.2021.3063496
   Chen ZM, 2021, IEEE T MULTIMEDIA, V23, P1827, DOI 10.1109/TMM.2020.3003779
   Chen ZM, 2019, PROC CVPR IEEE, P5172, DOI 10.1109/CVPR.2019.00532
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Cole E, 2021, PROC CVPR IEEE, P933, DOI 10.1109/CVPR46437.2021.00099
   Dat Huynh, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8773, DOI 10.1109/CVPR42600.2020.00880
   Dat Huynh, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9420, DOI 10.1109/CVPR42600.2020.00944
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dutta Ayushi, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P191, DOI 10.1007/978-3-030-58526-6_12
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Guo H, 2021, PROC CVPR IEEE, P15084, DOI 10.1109/CVPR46437.2021.01484
   Guo QP, 2020, AAAI CONF ARTIF INTE, V34, P7847
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   HU Y, IEEE T MULTIMEDIA
   Jin Ye, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P649, DOI 10.1007/978-3-030-58589-1_39
   Jiu MY, 2022, NEUROCOMPUTING, V474, P154, DOI 10.1016/j.neucom.2021.12.006
   Johnson J, 2018, PROC CVPR IEEE, P1219, DOI 10.1109/CVPR.2018.00133
   Joulin A., 2016, ARXIV
   Kipf TN, 2017, INT C LEARN REPR
   Lanchantin J, 2021, PROC CVPR IEEE, P16473, DOI 10.1109/CVPR46437.2021.01621
   Lele Cheng, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P306, DOI 10.1007/978-3-030-58577-8_19
   Li J, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P2897
   Li YN, 2016, LECT NOTES COMPUT SC, V9910, P684, DOI 10.1007/978-3-319-46466-4_41
   LIN T, 2021, A SURVEY TRANSFORMER
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin ZH, 2020, PROC CVPR IEEE, P1797, DOI 10.1109/CVPR42600.2020.00187
   LIU W, IEEE T PATTERN ANAL
   Liu YC, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P700, DOI 10.1145/3240508.3240567
   Lu LH, 2020, IEEE T MULTIMEDIA, V22, P524, DOI 10.1109/TMM.2019.2930344
   MIKOLOV GCT, 2013, P INT C LEARN REPRES, P1
   MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748
   Nguyen HD, 2021, AAAI CONF ARTIF INTE, V35, P9092
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Qian SS, 2021, AAAI CONF ARTIF INTE, V35, P2440
   Ridnik T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P82, DOI 10.1109/ICCV48922.2021.00015
   Shen JL, 2021, INFORM SCIENCES, V569, P469, DOI 10.1016/j.ins.2020.11.026
   Speer R, 2017, AAAI CONF ARTIF INTE, P4444
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sukhbaatar S, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P331
   Tramer Florian, 2018, P 7 INT C LEARN REPR
   Vaswani Ashish, 2017, Advances in Neural Information Processing Systems (NeurIPS), V17, P6000, DOI DOI 10.48550/ARXIV.1706.03762
   Vu XS, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2299, DOI 10.1145/3394171.3414047
   Wang J, 2016, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2016.251
   Wang SF, 2015, IEEE T MULTIMEDIA, V17, P2185, DOI 10.1109/TMM.2015.2484966
   Wang SR, 2020, COMPUTING, V102, P717, DOI 10.1007/s00607-019-00768-7
   WANG Y, IEEE T MULTIMEDIA
   Wang Y, 2020, AAAI CONF ARTIF INTE, V34, P12265
   Wei YC, 2016, IEEE T PATTERN ANAL, V38, P1901, DOI 10.1109/TPAMI.2015.2491929
   Xu JH, 2021, IEEE T MULTIMEDIA, V23, P1696, DOI 10.1109/TMM.2020.3002185
   Yang YD, 2021, PROC CVPR IEEE, P8070, DOI 10.1109/CVPR46437.2021.00798
   Yazici Vacit Oguz, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13437, DOI 10.1109/CVPR42600.2020.01345
   YOU R, 2020, P AAAI C ARTIF INTEL, P12709
   Yun S, 2021, PROC CVPR IEEE, P2340, DOI 10.1109/CVPR46437.2021.00237
   Zhou FT, 2021, AAAI CONF ARTIF INTE, V35, P3572
   Zhu F, 2017, PROC CVPR IEEE, P2027, DOI 10.1109/CVPR.2017.219
NR 59
TC 4
Z9 4
U1 5
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4013
EP 4025
DI 10.1109/TMM.2022.3171095
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FE2H4
UT WOS:001144015500035
DA 2024-07-18
ER

PT J
AU Gou, JP
   Yuan, X
   Yu, BS
   Yu, JL
   Yi, Z
AF Gou, Jianping
   Yuan, Xia
   Yu, Baosheng
   Yu, Jiali
   Yi, Zhang
TI Intra- and Inter-Class Induced Discriminative Deep Dictionary Learning
   for Visual Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Machine learning; Dictionaries; Visualization; Training; Task analysis;
   Testing; Optimization; Deep dictionary learning; visual recognition
ID ROBUST FACE RECOGNITION; ALGORITHM
AB Deep dictionary learning (DDL) aims to learn dictionaries at different levels and the deepest level representations. However, existing DDL algorithms impose a l(1)-norm constraint on the deepest level representations, ignoring the constraints on different level representations. Meanwhile, they fail to discover effectively the essential discrimination information. Therefore, the obtained representations are less discriminative, which degrades model performance. To tackle those issues, we propose an intra- and inter-class induced discriminative deep dictionary learning (DDDL). Specifically, both intra-class compactness and inter-class separability of layer-wise data representations are newly devised as two discriminative constraints on deep dictionary learning. In a hierarchical structure, we obtain a more informative dictionary and the class-specific representations are thus more discriminative at each layer. Due to the l(2)-norm intra- and inter-class constraints of layer-wise data representation, we devise a layer-wise optimization strategy to efficiently learn the closed-form solution of the deepest representation for classification. Comprehensive experiments and analyses on several visual recognition tasks show that our DDDL model surpasses recent shallow and deep representation learning approaches.
C1 [Gou, Jianping] Southwest Univ, Coll Software, Coll Comp & Informat Sci, Chongqing 400715, Peoples R China.
   [Yuan, Xia] Jiangsu Univ, Sch Comp Sci & Commun Engn, Zhenjiang 212013, Jiangsu, Peoples R China.
   [Yu, Baosheng] Univ Sydney, Sch Comp Sci, Darlington, NSW 2008, Australia.
   [Yu, Jiali] Univ Elect Sci & Technol China, Sch Math Sci, Chengdu 611731, Peoples R China.
   [Yi, Zhang] Sichuan Univ, Intelligent Interdisciplinary Res Ctr, Chengdu 610065, Peoples R China.
   [Yi, Zhang] Sichuan Univ, Sch Comp Sci, Chengdu 610065, Peoples R China.
C3 Southwest University - China; Jiangsu University; University of Sydney;
   University of Electronic Science & Technology of China; Sichuan
   University; Sichuan University
RP Gou, JP (corresponding author), Southwest Univ, Coll Software, Coll Comp & Informat Sci, Chongqing 400715, Peoples R China.
EM goujianping@ujs.edu.cn; 2212008046@stmail.ujs.edu.cn;
   baosheng.yu@sydney.edu.au; yujiali@uestc.edu.cn; zhangyi@scu.edu.cn
RI Gou, Jianping/JQX-2453-2023; Zhang, Can/JUU-9511-2023; feng,
   feng/KBR-1814-2024; Liu, Yining/KHC-6217-2024
OI Gou, Jianping/0000-0003-1413-0693; Liu, Yining/0000-0002-2218-2349
FU National Natural Science Foundation of China [61976107, 61502208]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 61976107 and 61502208.
CR Abdi A, 2021, PATTERN RECOGN, V110, DOI 10.1016/j.patcog.2020.107634
   Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   [Anonymous], 2009, Scholarpedia
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Colak O, 2022, EXPERT SYST APPL, V190, DOI 10.1016/j.eswa.2021.116192
   Cun Y. L., 2015, NATURE, V521, P436, DOI DOI 10.1038/NATURE14539
   Du HS, 2020, KNOWL-BASED SYST, V187, DOI 10.1016/j.knosys.2019.06.031
   Fan JC, 2021, IEEE T SIGNAL PROCES, V69, P1755, DOI 10.1109/TSP.2021.3062988
   Fan ZZ, 2023, IEEE T NEUR NET LEAR, V34, P64, DOI 10.1109/TNNLS.2021.3089566
   Goel A., 2021, IEEE Geosci Remote Sens Lett, V19, P1, DOI 10.1109/LGRS.2021.3112603
   Gou JP, 2022, IEEE T INTELL TRANSP, V23, P25308, DOI 10.1109/TITS.2022.3177647
   Huang JJ, 2020, IEEE T SIGNAL PROCES, V68, P6633, DOI 10.1109/TSP.2020.3036902
   Huang Z, 2021, PATTERN RECOGN, V113, DOI 10.1016/j.patcog.2020.107757
   HULL JJ, 1994, IEEE T PATTERN ANAL, V16, P550, DOI 10.1109/34.291440
   Jiang ZL, 2011, PROC CVPR IEEE, P1697, DOI 10.1109/CVPR.2011.5995354
   Latecki LJ, 2000, PROC CVPR IEEE, P424, DOI 10.1109/CVPR.2000.855850
   Lazebnik S., 2006, P IEEE CVF C COMP VI, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/CVPR.2006.68]
   Lee KC, 2005, IEEE T PATTERN ANAL, V27, P684, DOI 10.1109/TPAMI.2005.92
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Li HF, 2021, NEUROCOMPUTING, V422, P62, DOI 10.1016/j.neucom.2020.09.024
   Li HF, 2020, NEUROCOMPUTING, V379, P356, DOI 10.1016/j.neucom.2019.11.001
   Li S, 2021, AAAI CONF ARTIF INTE, V35, P1949
   Li SY, 2023, IEEE T MULTIMEDIA, V25, P805, DOI 10.1109/TMM.2021.3132166
   Li XS, 2021, KNOWL-BASED SYST, V224, DOI 10.1016/j.knosys.2021.107087
   Li YX, 2021, PATTERN RECOGN, V112, DOI 10.1016/j.patcog.2020.107793
   Li ZM, 2020, IEEE T NEUR NET LEAR, V31, P786, DOI 10.1109/TNNLS.2019.2910146
   Liu HF, 2021, IEEE T COMPUT IMAG, V7, P638, DOI 10.1109/TCI.2021.3083135
   Liu X, 2021, IEEE T MULTIMEDIA, V23, P1841, DOI 10.1109/TMM.2020.3003783
   Liu YX, 2021, NEUROCOMPUTING, V435, P1, DOI 10.1016/j.neucom.2021.01.010
   Luo XL, 2019, PATTERN RECOGN, V93, P283, DOI 10.1016/j.patcog.2019.04.027
   Martinez A.M., 1998, AR FACE DATABASE CVC
   Ou WH, 2018, PATTERN RECOGN LETT, V107, P41, DOI 10.1016/j.patrec.2017.07.006
   Ou WH, 2014, PATTERN RECOGN, V47, P1559, DOI 10.1016/j.patcog.2013.10.017
   Rong Y, 2020, IEEE T IMAGE PROCESS, V29, P7707, DOI 10.1109/TIP.2020.3004246
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Sim T, 2003, IEEE T PATTERN ANAL, V25, P1615, DOI 10.1109/TPAMI.2003.1251154
   Singhal V, 2020, NEUROCOMPUTING, V408, P135, DOI 10.1016/j.neucom.2019.11.107
   Singhal V, 2020, PATTERN RECOGN, V100, DOI 10.1016/j.patcog.2019.107163
   Song JQ, 2019, PATTERN RECOGN, V91, P135, DOI 10.1016/j.patcog.2019.02.018
   Su HJ, 2018, IEEE T GEOSCI REMOTE, V56, P2467, DOI 10.1109/TGRS.2017.2781805
   Tang H, 2021, IEEE T NEUR NET LEAR, V32, P2129, DOI 10.1109/TNNLS.2020.2997289
   Tang W., 2019, PATTERN RECOGN, V28, P6025
   Tariyal S, 2016, IEEE ACCESS, V4, P10096, DOI 10.1109/ACCESS.2016.2611583
   Tharwat A, 2017, AI COMMUN, V30, P169, DOI 10.3233/AIC-170729
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang H, 2021, IEEE T GEOSCI REMOTE, V59, P7098, DOI 10.1109/TGRS.2020.3030740
   Wang HB, 2021, NEUROCOMPUTING, V438, P55, DOI 10.1016/j.neucom.2020.06.148
   Wang LC, 2021, IEEE T MULTIMEDIA, V23, P2857, DOI 10.1109/TMM.2020.3017916
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Wu Y, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P222, DOI 10.1145/3206025.3206045
   Xu Y, 2016, PATTERN RECOGN, V54, P68, DOI 10.1016/j.patcog.2015.12.017
   Yang BQ, 2021, PATTERN RECOGN, V112, DOI 10.1016/j.patcog.2020.107690
   YUAN X, 2022, P IEEE INT C MULT EX, P1
   Zeng YJ, 2020, NEURAL NETWORKS, V123, P331, DOI 10.1016/j.neunet.2019.11.015
   Zhang A, 2019, NEUROCOMPUTING, V338, P293, DOI 10.1016/j.neucom.2019.02.013
   Zhang GQ, 2021, INFORM SCIENCES, V547, P498, DOI 10.1016/j.ins.2020.08.066
   Zhang L, 2011, IEEE I CONF COMP VIS, P471, DOI 10.1109/ICCV.2011.6126277
   Zhang QA, 2010, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2010.5539989
   Zhang YC, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13061218
   Zhang Z, 2020, IEEE T CIRC SYST VID, V30, P2430, DOI 10.1109/TCSVT.2019.2923007
   Zhao CR, 2021, IEEE T IMAGE PROCESS, V30, P4212, DOI 10.1109/TIP.2021.3070182
NR 61
TC 2
Z9 2
U1 6
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1575
EP 1583
DI 10.1109/TMM.2023.3258141
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA G2GW2
UT WOS:000987415000017
DA 2024-07-18
ER

PT J
AU Huang, J
   Fu, XY
   Xiao, ZY
   Zhao, F
   Xiong, ZW
AF Huang, Jie
   Fu, Xueyang
   Xiao, Zeyu
   Zhao, Feng
   Xiong, Zhiwei
TI Low-Light Stereo Image Enhancement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Stereo images; low-light enhancement; deep learning; retinex
ID ADAPTIVE HISTOGRAM EQUALIZATION; PARALLAX ATTENTION; NETWORK
AB Stereo cameras are now commonly used in more and more devices. Nevertheless, visually unpleasant images captured under low-light conditions hinder their practical application. As an initial attempt at low-light stereo image enhancement, we propose a novel Dual-View Enhancement Network (DVENet) based on the Retinex theory, which consists of two stages. The first stage estimates an illumination map to obtain a coarse enhancement result, which boosts the correlation of two views, while the second stage recovers details by integrating the information from two views to achieve fine image quality improvement with the guidance of the illumination map. To fully utilize the dual-view correlation, we further design a wavelet-based view transfer module to efficiently carry out multi-scale detail recovery. Then, we design an illumination-aware attention fusion module to exploit the complementarity between the fused features from two views and the single-view features. Experiments on both synthetic and real-world stereo datasets demonstrate the superiority of our proposed method over existing solutions. The code and model are publicly available at: https://github.com/KevinJ-Huang/Stereo-Low-Light.
C1 [Huang, Jie; Xiao, Zeyu; Xiong, Zhiwei] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230052, Anhui, Peoples R China.
   [Fu, Xueyang; Zhao, Feng] Univ Sci & Technol China, Dept Automat, Hefei 230052, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS
RP Xiong, ZW (corresponding author), Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230052, Anhui, Peoples R China.
EM hj0117@mail.ustc.edu.cn; xyfu@ustc.edu.cn; zeyuxiao@mail.ustc.edu.cn;
   fzhao956@ustc.edu.cn; zwxiong@ustc.edu.cn
RI Liu, Jinyu/JYQ-6274-2024; cheng, chen/JHS-9462-2023; SUN,
   YANLING/JTT-9082-2023; Yang, Tian/JFB-1008-2023; Wang,
   Minghao/JMD-0670-2023; WANG, YANG/JFA-8821-2023; Jiang,
   Yu/JEZ-9814-2023; Zhao, Feng/C-8367-2009
OI Fu, Xueyang/0000-0001-8036-4071; Zhao, Feng/0000-0001-6767-8105; Huang,
   Jie/0000-0002-3518-3404
FU National Key Ramp;D Program of China [2017YFA0700800]; National Natural
   Science Foundation of China [62131003, 62021001]
FX This work was supported by the National Key R & D Program of China under
   Grant 2017YFA0700800, and in part by the National Natural Science
   Foundation of China under Grants 62131003 and 62021001. The as-sociate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Li Cheng.
CR Abdullah-Al-Wadud M, 2007, IEEE T CONSUM ELECTR, V53, P593, DOI 10.1109/TCE.2007.381734
   Bo Yan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13176, DOI 10.1109/CVPR42600.2020.01319
   Cai BL, 2017, IEEE I CONF COMP VIS, P4020, DOI 10.1109/ICCV.2017.431
   Cao Y, 2019, IEEE INT CONF COMP V, P1971, DOI 10.1109/ICCVW.2019.00246
   Chang JR, 2018, PROC CVPR IEEE, P5410, DOI 10.1109/CVPR.2018.00567
   Chen CQ, 2022, IEEE T MULTIMEDIA, V24, P202, DOI 10.1109/TMM.2021.3050092
   Chen DD, 2018, PROC CVPR IEEE, P6654, DOI 10.1109/CVPR.2018.00696
   Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   Guo S, 2019, PROC CVPR IEEE, P1712, DOI 10.1109/CVPR.2019.00181
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hua Y., 2020, PROC IEEE C COMPUT V, P1
   Ilg E, 2018, LECT NOTES COMPUT SC, V11216, P626, DOI 10.1007/978-3-030-01258-8_38
   Jeon DS, 2018, PROC CVPR IEEE, P1721, DOI 10.1109/CVPR.2018.00185
   Ju MY, 2021, IEEE T IMAGE PROCESS, V30, P2180, DOI 10.1109/TIP.2021.3050643
   LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108
   LEE YH, 1990, IEEE T CIRCUITS SYST, V37, P940, DOI 10.1109/31.55069
   Li CY, 2022, IEEE T PATTERN ANAL, V44, P4225, DOI 10.1109/TPAMI.2021.3063604
   Li JQ, 2021, IEEE T MULTIMEDIA, V23, P3153, DOI 10.1109/TMM.2020.3021243
   Li MD, 2018, IEEE T IMAGE PROCESS, V27, P2828, DOI 10.1109/TIP.2018.2810539
   Lim S, 2021, IEEE T MULTIMEDIA, V23, P4272, DOI 10.1109/TMM.2020.3039361
   Liu JY, 2021, INT J COMPUT VISION, V129, P1153, DOI 10.1007/s11263-020-01418-8
   Liu PJ, 2018, IEEE COMPUT SOC CONF, P886, DOI 10.1109/CVPRW.2018.00121
   Luo WJ, 2016, PROC CVPR IEEE, P5695, DOI 10.1109/CVPR.2016.614
   Lv F., 2018, P BMVC, V220, P4
   Lv FF, 2021, INT J COMPUT VISION, V129, P2175, DOI 10.1007/s11263-021-01466-8
   Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Pang JH, 2017, IEEE INT CONF COMP V, P878, DOI 10.1109/ICCVW.2017.108
   Pang YW, 2020, PROC CVPR IEEE, P5930, DOI 10.1109/CVPR42600.2020.00597
   PIZER SM, 1987, COMPUT VISION GRAPH, V39, P355, DOI 10.1016/S0734-189X(87)80186-X
   Ren WQ, 2019, IEEE T IMAGE PROCESS, V28, P4364, DOI 10.1109/TIP.2019.2910412
   Ren XT, 2020, IEEE T IMAGE PROCESS, V29, P5862, DOI 10.1109/TIP.2020.2984098
   Reza AM, 2004, J VLSI SIG PROC SYST, V38, P35, DOI 10.1023/B:VLSI.0000028532.53893.82
   Scharstein D, 2014, LECT NOTES COMPUT SC, V8753, P31, DOI 10.1007/978-3-319-11752-2_3
   Sharma A, 2020, INT CONF 3D VISION, P23, DOI 10.1109/3DV50981.2020.00012
   Tian QC, 2017, IEEE INT CONF COMP V, P3023, DOI 10.1109/ICCVW.2017.357
   Wang LG, 2019, PROC CVPR IEEE, P12242, DOI 10.1109/CVPR.2019.01253
   Wang RX, 2019, PROC CVPR IEEE, P6842, DOI 10.1109/CVPR.2019.00701
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wang WC, 2019, INFORM SCIENCES, V496, P25, DOI 10.1016/j.ins.2019.05.015
   Wang YQ, 2021, IEEE COMPUT SOC CONF, P766, DOI 10.1109/CVPRW53098.2021.00086
   Wei Cui, 2018, 2018 Photonics North (PN), DOI 10.1109/PN.2018.8438843
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xu K, 2020, PROC CVPR IEEE, P2278, DOI 10.1109/CVPR42600.2020.00235
   Yang WH, 2021, IEEE T IMAGE PROCESS, V30, P3461, DOI 10.1109/TIP.2021.3062184
   Yang WH, 2021, IEEE T IMAGE PROCESS, V30, P2072, DOI 10.1109/TIP.2021.3050850
   Yang WH, 2020, PROC CVPR IEEE, P3060, DOI 10.1109/CVPR42600.2020.00313
   Zamir Syed Waqas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P492, DOI 10.1007/978-3-030-58595-2_30
   Zhang Q, 2021, IEEE T MULTIMEDIA, V23, P189, DOI 10.1109/TMM.2020.2982045
   Zhang Q, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P582, DOI 10.1145/3240508.3240595
   Zhang Y., 2020, arXiv
   Zhang YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1632, DOI 10.1145/3343031.3350926
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhou SC, 2019, PROC CVPR IEEE, P10988, DOI 10.1109/CVPR.2019.01125
NR 57
TC 3
Z9 3
U1 8
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2978
EP 2992
DI 10.1109/TMM.2022.3154152
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA O7RP6
UT WOS:001045742200003
DA 2024-07-18
ER

PT J
AU Jin, ZX
   Wu, HR
   Yang, C
   Zhou, F
   Qin, JY
   Xiao, L
   Yin, XC
AF Jin, Zan-Xia
   Wu, Heran
   Yang, Chun
   Zhou, Fang
   Qin, Jingyan
   Xiao, Lei
   Yin, Xu-Cheng
TI RUArt: A Novel Text-Centered Solution for Text-Based Visual Question
   Answering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Attention mechanism; computer vision; machine reading comprehension;
   natural language processing; visual question answering
AB Text-based visual question answering (VQA) requires to read and understand text in an image to correctly answer a given question. However, most current methods simply add optical character recognition (OCR) tokens extracted from the image into the VQA model without considering contextual information of OCR tokens and mining the relationships between OCR tokens and scene objects. In this paper, we propose a novel text-centered method called RUArt (Reading, Understanding and Answering the Related Text) for text-based VQA. Taking an image and a question as input, RUArt first reads the image and obtains text and scene objects. Then, it understands the question, OCRed text and objects in the context of the scene, and further mines the relationships among them. Finally, it answers the related text for the given question through text semantic matching and reasoning. We evaluate our RUArt on two text-based VQA benchmarks (ST-VQA and TextVQA) and conduct extensive ablation studies for exploring the reasons behind RUArt's effectiveness. Experimental results demonstrate that our method can effectively explore the contextual information of the text and mine the stable relationships between the text and objects.
C1 [Jin, Zan-Xia; Wu, Heran; Yang, Chun; Zhou, Fang; Qin, Jingyan; Yin, Xu-Cheng] Univ Sci & Technol Beijing, Sch Comp & Commun Engn, Dept Comp Sci & Technol, Beijing 100083, Peoples R China.
   [Qin, Jingyan] Univ Sci & Technol Beijing, Sch Mech Engn, Dept Ind Design, Beijing 100083, Peoples R China.
   [Xiao, Lei] Tencent Technol Shenzhen Co Ltd, Shenzhen 518057, Peoples R China.
   [Yin, Xu-Cheng] Univ Sci & Technol Beijing, Inst Artificial Intelligence, Beijing 100083, Peoples R China.
   [Yin, Xu-Cheng] Univ Sci & Technol Beijing, USTB EEasy Tech Joint Lab Artificial Intelligence, Beijing 100083, Peoples R China.
C3 University of Science & Technology Beijing; University of Science &
   Technology Beijing; Tencent; University of Science & Technology Beijing;
   University of Science & Technology Beijing
RP Zhou, F; Qin, JY (corresponding author), Univ Sci & Technol Beijing, Sch Comp & Commun Engn, Dept Comp Sci & Technol, Beijing 100083, Peoples R China.; Qin, JY (corresponding author), Univ Sci & Technol Beijing, Sch Mech Engn, Dept Ind Design, Beijing 100083, Peoples R China.
EM jinzanxia_go@163.com; heranwu@yeah.net; chunyang@ustb.edu.cn;
   zhoufang@ies.ustb.edu.cn; qinjingyanking@foxmail.com;
   shawnxiao@tencent.com; xuchengyin@ustb.edu.cn
OI Yin, Xucheng/0000-0003-0023-0220
FU National Natural ScienceFoundation of China [62076024, 62006018];
   CheungKong Scholars Programme of China [FRF-TP-18-010C1]; National Major
   Special Project of China [2018YFB0704301]; Shunde Graduate Program of
   University of Science and Technology Beijing [BK19AE011]
FX This work was supported in part by the National Natural
   ScienceFoundation of China under Grants 62076024, and 62006018, in part
   by CheungKong Scholars Programme of China under Grant FRF-TP-18-010C1,
   in part bythe National Major Special Project of China under Grant
   2018YFB0704301,and in part by Shunde Graduate Program of University of
   Science and Technology Beijing under Grant BK19AE011. The Associate
   Editor coordinating thereview of this manuscript and approving it for
   publication was Dr. Elisa Ricci.(Corresponding authors: Fang Zhou;
   Jingyan Qin.
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   [Anonymous], 2012, Proceedings of the 21st International Conference on World Wide Web
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Baek Y, 2019, PROC CVPR IEEE, P9357, DOI 10.1109/CVPR.2019.00959
   Ben-younes H, 2017, IEEE I CONF COMP VIS, P2631, DOI 10.1109/ICCV.2017.285
   Biten Ali Furkan, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P1563, DOI 10.1109/ICDAR.2019.00251
   Biten AF, 2019, IEEE I CONF COMP VIS, P4290, DOI 10.1109/ICCV.2019.00439
   Che WB, 2020, IEEE T MULTIMEDIA, V22, P2307, DOI 10.1109/TMM.2019.2954750
   Chen DQ, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1870, DOI 10.18653/v1/P17-1171
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Fukui A., 2016, P C EMP METH NAT LAN, P457
   Gao Difei, 2020, CVPR, P12746
   Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670
   Gurari D, 2018, PROC CVPR IEEE, P3608, DOI 10.1109/CVPR.2018.00380
   Han CJ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P510, DOI 10.1145/3240508.3240611
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hou J., 2020, PROC 34 AAAI C ARTIF, p10 973
   Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378
   Hu J, 2021, IEEE T MULTIMEDIA, V23, P2321, DOI 10.1109/TMM.2020.3009491
   Hu Ronghang, 2020, 2020 IEEE CVF C COMP, P9992, DOI DOI 10.1109/CVPR42600.2020.01001
   Huang H, 2018, Fusionnet: Fusing via fully -aware attention with application to machine comprehension
   Hudson D. A., 2018, PROC 6 INT C LEARN R
   Jin ZX, 2020, INFORM SCIENCES, V538, P358, DOI 10.1016/j.ins.2020.05.110
   Jin ZX, 2019, BIOINFORMATICS, V35, P4129, DOI 10.1093/bioinformatics/btz195
   Joulin A., 2017, P 15 C EUR CHAPT ASS, P427, DOI DOI 10.18653/V1/E17-2068
   Kafle K, 2018, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2018.00592
   Karatzas D, 2015, PROC INT CONF DOC, P1156, DOI 10.1109/ICDAR.2015.7333942
   Karatzas D, 2013, PROC INT CONF DOC, P1484, DOI 10.1109/ICDAR.2013.221
   Kim J., 2017, PROC 5 INT C LEARN R
   Kingma D. P., 2014, arXiv
   Krasin I., 2017, Openimages: A public dataset for large-scale multi-label and multi-class image classification
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Li H, 2019, PROC CVPR IEEE, P6312, DOI 10.1109/CVPR.2019.00648
   Li LJ, 2019, IEEE I CONF COMP VIS, P10312, DOI 10.1109/ICCV.2019.01041
   Liang JW, 2018, PROC CVPR IEEE, P6135, DOI 10.1109/CVPR.2018.00642
   Liao MH, 2021, IEEE T PATTERN ANAL, V43, P532, DOI 10.1109/TPAMI.2019.2937086
   Liu F, 2021, IEEE T MULTIMEDIA, V23, P3518, DOI 10.1109/TMM.2020.3026892
   Liu JC, 2019, Arxiv, DOI arXiv:1903.11800
   Luo CJ, 2019, PATTERN RECOGN, V90, P109, DOI 10.1016/j.patcog.2019.01.020
   Mishra A, 2013, IEEE I CONF COMP VIS, P3040, DOI 10.1109/ICCV.2013.378
   Pennington J., 2014, P 2014 C EMP METH NA, P1532, DOI [DOI 10.3115/V1/D14-1162, 10.3115/v1/D14-1162]
   Redmon Joseph, 2013, Darknet: Open Source Neural Networks in C, DOI DOI 10.1109/CVPR.2016.91
   Ren S., 2015, ADV NEURAL INFORM PR, V28, P91
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shi BG, 2019, IEEE T PATTERN ANAL, V41, P2035, DOI 10.1109/TPAMI.2018.2848939
   Singh AK, 2019, IEEE I CONF COMP VIS, P4601, DOI 10.1109/ICCV.2019.00470
   Singh A, 2019, PROC CVPR IEEE, P8309, DOI 10.1109/CVPR.2019.00851
   Song XH, 2020, IEEE T IMAGE PROCESS, V29, P525, DOI 10.1109/TIP.2019.2933728
   Vaswani A, 2017, ADV NEUR IN, V30
   Veit A, 2016, Arxiv, DOI arXiv:1601.07140
   Wang X., 2020, P IEEE CVF C COMP VI, P10126
   Xu YM, 2020, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, EMNLP 2020, P367
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   Yu J, 2020, IEEE T MULTIMEDIA, V22, P3196, DOI 10.1109/TMM.2020.2972830
   Yu Z, 2018, IEEE T NEUR NET LEAR, V29, P5947, DOI 10.1109/TNNLS.2018.2817340
   Yu Z, 2019, PROC CVPR IEEE, P6274, DOI 10.1109/CVPR.2019.00644
   Yu Z, 2017, IEEE I CONF COMP VIS, P1839, DOI 10.1109/ICCV.2017.202
   Zhong HS, 2021, IEEE T MULTIMEDIA, V23, P1264, DOI 10.1109/TMM.2020.2995278
   Zhou L, 2018, Arxiv, DOI arXiv:1807.05857
   Zhu Chenguang, 2018, SDNet: Contextualized Attention-based Deep Network for Conversational Question Answering
NR 62
TC 13
Z9 13
U1 2
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1
EP 12
DI 10.1109/TMM.2021.3120194
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 9E8KM
UT WOS:000937028400001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Li, Y
   Meng, SB
   Zhang, XF
   Wang, M
   Wang, SQ
   Wang, Y
   Ma, SW
AF Li, Yang
   Meng, Shengbin
   Zhang, Xinfeng
   Wang, Meng
   Wang, Shiqi
   Wang, Yue
   Ma, Siwei
TI User-Generated Video Quality Assessment: A Subjective and Objective
   Study
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep neural network; user-generated content; video quality assessment
ID IMAGE; MODEL; INFORMATION; SIMILARITY; EFFICIENT; DATABASE
AB Recently, we have observed an exponential increase of user-generated content (UGC) videos. The distinguished characteristic of UGC videos originates from the video production and delivery chain, as they are usually acquired and processed by non-professional users before uploading to the hosting platforms for sharing. As such, these videos usually undergo multiple distortion stages that may affect visual quality before ultimately being viewed. Inspired by the increasing consensus that the optimization of the video coding and processing shall be fully driven by the perceptual quality, in this paper, we propose to study the quality of the UGC videos from both objective and subjective perspectives. We first construct a UGC video quality assessment (VQA) database, aiming to provide useful guidance for the UGC video coding and processing in the hosting platform. The database contains source UGC videos uploaded to the platform and their transcoded versions that are ultimately enjoyed by end-users, along with their subjective scores. Furthermore, we develop an objective quality assessment algorithm that automatically evaluates the quality of the transcoded videos based on the corrupted reference, which is in accordance with the application scenarios of UGC video sharing in the hosting platforms. The information from the corrupted reference is well leveraged and the quality is predicted based on the inferred quality maps with deep neural networks (DNN). Experimental results show that the proposed method yields superior performance. Both subjective and objective evaluations of the UGC videos also shed lights on the design of perceptual UGC video coding.
C1 [Li, Yang; Ma, Siwei] Peking Univ, Inst Digital Media, Beijing 100871, Peoples R China.
   [Zhang, Xinfeng] Univ Chinese Acad Sci, Sch Comp Sci & Technol, Beijing 101408, Peoples R China.
   [Wang, Meng; Wang, Shiqi] City Univ Hong Kong, Dept Comp Sci, Kwoloon, Hong Kong, Peoples R China.
   [Meng, Shengbin; Wang, Yue] Bytedance Inc, VideoArch Dept, Beijing 100871, Peoples R China.
C3 Peking University; Chinese Academy of Sciences; University of Chinese
   Academy of Sciences, CAS; City University of Hong Kong
RP Ma, SW (corresponding author), Peking Univ, Inst Digital Media, Beijing 100871, Peoples R China.
EM liyang.00@pku.edu.cn; mengshengbin@bytedance.com; xfzhang@ucas.ac.cn;
   mwang98-c@my.cityu.edu.hk; shiqwang@cityu.edu.hk;
   wangyue.v@bytedance.com; swma@pku.edu.cn
RI Zhang, Xinfeng/X-8148-2019; Wang, Shiqi/AAR-5013-2020
OI Wang, Shiqi/0000-0002-6338-1432; Li, Yang/0000-0002-1473-0296; WANG,
   Meng/0000-0002-5655-1464; Zhang, Xinfeng/0000-0002-7517-3868
FU National Natural Science Foundation of China [62025101, 61961130392,
   61931014]; National Key Research and Development [2019YFF0302703]; Hong
   Kong ITF UICP [9440203]; Fundamental Research Funds for the Central
   Universities; PKU-Baidu Fund [2019BD003]; High performance Computing
   Platform of Peking University
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62025101, 61961130392, and 61931014, in
   part by the National Key Research and Development Project under Grant
   2019YFF0302703, in part by Hong Kong ITF UICP under Grant 9440203, in
   part by Fundamental Research Funds for the Central Universities, and
   PKU-Baidu Fund under Grant 2019BD003, and in part by High performance
   Computing Platform of Peking University. The Associate Editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Manoranjan Paul. (Corresponding author: Prof.
   Siwei Ma.)
CR [Anonymous], 2012, REC ITU R BT 500 13
   Bampis CG, 2019, IEEE T CIRC SYST VID, V29, P2256, DOI 10.1109/TCSVT.2018.2868262
   Bampis CG, 2017, IEEE SIGNAL PROC LET, V24, P1333, DOI 10.1109/LSP.2017.2726542
   Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Fast Forward Moving Pictures Experts Group (FFMPEG), about us
   Freitas PG, 2018, SIGNAL PROCESS-IMAGE, V64, P1, DOI 10.1016/j.image.2018.02.010
   Ghadiyaram D, 2018, IEEE T CIRC SYST VID, V28, P2061, DOI 10.1109/TCSVT.2017.2707479
   Ghadiyaram D, 2017, IEEE IMAGE PROC, P3445, DOI 10.1109/ICIP.2017.8296922
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hosu Vlad, 2017, Int. Conf. Quality of Multimedia Experience, P1
   Ioffe S., 2015, P INT C MACH LEARN, VVolume 1, P448, DOI DOI 10.48550/ARXIV.1502.03167
   ITU-T, 2008, ITU T P910 SUBJECTIV
   ITU-T, 2009, SUBJ VID QUAL ASS ME
   Katsenou AV, 2021, IEEE T MULTIMEDIA, V23, P26, DOI 10.1109/TMM.2020.2976591
   Kim W, 2018, LECT NOTES COMPUT SC, V11205, P224, DOI 10.1007/978-3-030-01246-5_14
   Kingma D. P., 2015, INT C LEARNING REPRE
   Korhonen J, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3311, DOI 10.1145/3394171.3413845
   Korhonen J, 2019, IEEE T IMAGE PROCESS, V28, P5923, DOI 10.1109/TIP.2019.2923051
   Li DQ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2351, DOI 10.1145/3343031.3351028
   Li XL, 2016, IEEE T IMAGE PROCESS, V25, P3329, DOI 10.1109/TIP.2016.2568752
   Li Y, 2020, THIRD INTERNATIONAL CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2020), P35, DOI 10.1109/MIPR49039.2020.00015
   Li YM, 2016, IEEE T CIRC SYST VID, V26, P1044, DOI 10.1109/TCSVT.2015.2430711
   Lin JY, 2014, ASIAPAC SIGN INFO PR
   Liu WT, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P546, DOI 10.1145/3240508.3240643
   Lu W, 2019, INFORM SCIENCES, V478, P141, DOI 10.1016/j.ins.2018.11.003
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P1004, DOI 10.1109/TIP.2016.2631888
   Manasa K, 2016, IEEE T IMAGE PROCESS, V25, P2480, DOI 10.1109/TIP.2016.2548247
   Mittal A, 2016, IEEE T IMAGE PROCESS, V25, P289, DOI 10.1109/TIP.2015.2502725
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2012, IEEE J-STSP, V6, P652, DOI 10.1109/JSTSP.2012.2212417
   Nafchi HZ, 2016, IEEE ACCESS, V4, P5579, DOI 10.1109/ACCESS.2016.2604042
   Narvekar ND, 2011, IEEE T IMAGE PROCESS, V20, P2678, DOI 10.1109/TIP.2011.2131660
   Nuutinen M, 2016, IEEE T IMAGE PROCESS, V25, P3073, DOI 10.1109/TIP.2016.2562513
   Pan D, 2018, PROC CVPR IEEE, P6373, DOI 10.1109/CVPR.2018.00667
   Prashnani E, 2018, PROC CVPR IEEE, P1808, DOI 10.1109/CVPR.2018.00194
   Ren HY, 2018, AAAI CONF ARTIF INTE, P7308
   Saad MA, 2014, IEEE T IMAGE PROCESS, V23, P1352, DOI 10.1109/TIP.2014.2299154
   Seshadrinathan K, 2011, INT CONF ACOUST SPEE, P1153
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P1427, DOI 10.1109/TIP.2010.2042111
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P335, DOI 10.1109/TIP.2009.2034992
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Sinno Z, 2019, IEEE T IMAGE PROCESS, V28, P612, DOI 10.1109/TIP.2018.2869673
   Soundararajan R, 2013, IEEE T CIRC SYST VID, V23, P684, DOI 10.1109/TCSVT.2012.2214933
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Vonikakis V, 2016, IEEE IMAGE PROC, P3753, DOI 10.1109/ICIP.2016.7533061
   Vu PV, 2014, J ELECTRON IMAGING, V23, DOI 10.1117/1.JEI.23.1.013016
   Wang HG, 2016, IEEE IMAGE PROC, P1509, DOI 10.1109/ICIP.2016.7532610
   Wang YL, 2019, IEEE INT WORKSH MULT, DOI 10.1109/mmsp.2019.8901772
   Wang Y, 2012, IEEE T CIRC SYST VID, V22, P989, DOI 10.1109/TCSVT.2012.2186745
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, SIGNAL PROCESS-IMAGE, V19, P121, DOI 10.1016/S0923-5965(03)00076-6
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xu MN, 2020, INT CONF ACOUST SPEE, P4447, DOI [10.1109/ICASSP40776.2020.9053031, 10.1109/icassp40776.2020.9053031]
   Yang JC, 2020, IEEE T CIRC SYST VID, V30, P3608, DOI 10.1109/TCSVT.2019.2948383
   Yu F, 2017, PROC CVPR IEEE, P636, DOI 10.1109/CVPR.2017.75
   Yu XX, 2021, IEEE T IMAGE PROCESS, V30, P7511, DOI 10.1109/TIP.2021.3107213
   Yu XX, 2019, IEEE T IMAGE PROCESS, V28, P5757, DOI 10.1109/TIP.2019.2922850
   Zhang F, 2018, IEEE T MULTIMEDIA, V20, P2620, DOI 10.1109/TMM.2018.2817070
   Zhang F, 2013, IEEE T IMAGE PROCESS, V22, P1534, DOI 10.1109/TIP.2012.2233486
   Zhang Y, 2019, IEEE T CIRC SYST VID, V29, P2244, DOI 10.1109/TCSVT.2018.2868063
   Zhang Y, 2020, IEEE T NEUR NET LEAR, V31, P2716, DOI 10.1109/TNNLS.2018.2890310
   Zhao H, 2017, IEEE T COMPUT IMAG, V3, P47, DOI 10.1109/TCI.2016.2644865
   Zhu KF, 2015, IEEE T CIRC SYST VID, V25, P533, DOI 10.1109/TCSVT.2014.2363737
   Zhu Y, 2017, IEEE IMAGE PROC, P305, DOI 10.1109/ICIP.2017.8296292
NR 67
TC 11
Z9 12
U1 13
U2 36
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 154
EP 166
DI 10.1109/TMM.2021.3122347
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 9E8KM
UT WOS:000937028400011
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Li, Y
   Liu, Z
   Yao, LA
   Chang, XJ
AF Li, Yun
   Liu, Zhe
   Yao, Lina
   Chang, Xiaojun
TI Attribute-Modulated Generative Meta Learning for Zero-Shot Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Zero-shot learning; meta-learning; image retrieval
AB Zero-shot learning (ZSL) aims to transfer knowledge from seen classes to semantically related unseen classes, which are absent during training. The promising strategies for ZSL are to synthesize visual features of unseen classes conditioned on semantic side information and to incorporate meta-learning to eliminate the model's inherent bias towards seen classes. While existing meta generative approaches pursue a common model shared across task distributions, we aim to construct a generative network adaptive to task characteristics. To this end, we propose an Attribute-Modulated generAtive meta-model for Zero-shot learning (AMAZ). Our model consists of an attribute-aware modulation network, an attribute-augmented generative network, and an attribute-weighted classifier. Given unseen classes, the modulation network adaptively modulates the generator by applying task-specific transformations so that the generative network can adapt to highly diverse tasks. The weighted classifier utilizes the data quality to enhance the training procedure, further improving the model performance. Our empirical evaluations on four widely-used benchmarks show that AMAZ outperforms state-of-the-art methods by 3.8% and 3.1% in ZSL and generalized ZSL settings, respectively, demonstrating the superiority of our method. Our experiments on a zero-shot image retrieval task show AMAZ's ability to synthesize instances that portray real visual characteristics.
C1 [Li, Yun; Liu, Zhe; Yao, Lina] Univ New South Wales, Sch Comp Sci & Engn, Sydney, NSW 2052, Australia.
   [Chang, Xiaojun] Univ Technol Sydney, Fac Engn & Informat Technol, Sydney, NSW 2007, Australia.
C3 University of New South Wales Sydney; University of Technology Sydney
RP Liu, Z (corresponding author), Univ New South Wales, Sch Comp Sci & Engn, Sydney, NSW 2052, Australia.
EM yun.li5@unsw.edu.au; zheliu912@gmail.com; lina.yao@unsw.edu.au;
   cxj273@gmail.com
RI LIU, zhe/HGD-6875-2022; Chang, Xiaojun/A-2055-2015
OI Li, Yun/0000-0003-4442-3825; Chang, Xiaojun/0000-0002-7778-8807; Liu,
   Zhe/0000-0003-2692-2110
CR Akata Z, 2015, PROC CVPR IEEE, P2927, DOI 10.1109/CVPR.2015.7298911
   Bendre N, 2021, IEEE IMAGE PROC, P1284, DOI 10.1109/ICIP42928.2021.9506108
   Changpinyo S, 2016, PROC CVPR IEEE, P5327, DOI 10.1109/CVPR.2016.575
   Chen L, 2018, PROC CVPR IEEE, P1043, DOI 10.1109/CVPR.2018.00115
   Chen XY, 2022, IEEE T MULTIMEDIA, V24, P177, DOI 10.1109/TMM.2020.3047546
   David E, 2020, PLANT PHENOMICS, V2020, DOI 10.34133/2020/3521852
   de Vries H, 2017, ADV NEUR IN, V30
   Demertzis K, 2020, ALGORITHMS, V13, DOI 10.3390/a13030061
   Farhadi A, 2010, PROC CVPR IEEE, P2352, DOI 10.1109/CVPR.2010.5539924
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772
   Felix R, 2018, LECT NOTES COMPUT SC, V11210, P21, DOI 10.1007/978-3-030-01231-1_2
   Finn C, 2017, PR MACH LEARN RES, V70
   Fu YW, 2015, IEEE T PATTERN ANAL, V37, P2332, DOI 10.1109/TPAMI.2015.2408354
   Gao R, 2020, IEEE T IMAGE PROCESS, V29, P3665, DOI 10.1109/TIP.2020.2964429
   Gune O, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4262, DOI 10.1145/3394171.3413657
   Guo YC, 2017, IEEE T IMAGE PROCESS, V26, P3277, DOI 10.1109/TIP.2017.2696747
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu Y, 2021, Arxiv, DOI arXiv:2006.04648
   Imrattanatrai W, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P195, DOI 10.1145/3331184.3331220
   Kodirov E, 2017, PROC CVPR IEEE, P4447, DOI 10.1109/CVPR.2017.473
   Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140
   Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594
   Li J, 2019, PROC CVPR IEEE, P5458, DOI 10.1109/CVPR.2019.00561
   Li JJ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1348, DOI 10.1145/3394171.3413503
   Li ZH, 2019, PATTERN RECOGN, V88, P595, DOI 10.1016/j.patcog.2018.12.010
   Liu L, 2020, AAAI CONF ARTIF INTE, V34, P4868
   Liu SC, 2018, ADV NEUR IN, V31
   Liu Z, 2021, AAAI CONF ARTIF INTE, V35, P8723
   Ma PR, 2020, AAAI CONF ARTIF INTE, V34, P11733
   Mishra A, 2018, IEEE COMPUT SOC CONF, P2269, DOI 10.1109/CVPRW.2018.00294
   Nooralahzadeh F, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P4547
   Pal A, 2019, PROC CVPR IEEE, P2184, DOI 10.1109/CVPR.2019.00229
   Patterson G, 2012, PROC CVPR IEEE, P2751, DOI 10.1109/CVPR.2012.6247998
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Perez E, 2017, Arxiv, DOI arXiv:1709.07871
   Rahman S, 2019, IEEE I CONF COMP VIS, P6081, DOI 10.1109/ICCV.2019.00618
   Reed S, 2016, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2016.13
   Reuver M., 2021, P EACL HACKASHOP NEW, P45
   Romera-Paredes B, 2015, PR MACH LEARN RES, V37, P2152
   Schönfeld E, 2019, PROC CVPR IEEE, P8239, DOI 10.1109/CVPR.2019.00844
   Shigeto Y, 2015, LECT NOTES ARTIF INT, V9284, P135, DOI 10.1007/978-3-319-23528-8_9
   Sinha S, 2021, AAAI CONF ARTIF INTE, V35, P9666
   Soh JW, 2020, PROC CVPR IEEE, P3513, DOI 10.1109/CVPR42600.2020.00357
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Verma VK, 2020, AAAI CONF ARTIF INTE, V34, P6062
   Verma VK, 2017, LECT NOTES ARTIF INT, V10535, P792, DOI 10.1007/978-3-319-71246-8_48
   Vuorio Risto, 2019, Advances in Neural Information Processing Systems, P1
   Wan ZY, 2019, ADV NEUR IN, V32
   Wang JX, 2020, AAAI CONF ARTIF INTE, V34, P6186
   Wang WL, 2018, AAAI CONF ARTIF INTE, P4211
   Wang X, 2019, PROC CVPR IEEE, P1831, DOI 10.1109/CVPR.2019.00193
   Welinder P., 2010, California Inst. Technol., Tech. Rep. CNS-TR-2010-001
   Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581
   Xian YQ, 2019, IEEE T PATTERN ANAL, V41, P2251, DOI 10.1109/TPAMI.2018.2857768
   Xian YQ, 2016, PROC CVPR IEEE, P69, DOI 10.1109/CVPR.2016.15
   Xu X, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P1419, DOI 10.1145/3397271.3401149
   Yang G, 2018, ACM/SIGIR PROCEEDINGS 2018, P941, DOI 10.1145/3209978.3210096
   Yang SQ, 2021, Arxiv, DOI arXiv:2006.05938
   Ye M, 2019, PROC CVPR IEEE, P11720, DOI 10.1109/CVPR.2019.01200
   Yu YL, 2023, IEEE T NEUR NET LEAR, V34, P3183, DOI 10.1109/TNNLS.2021.3112229
   Yunlong Yu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14032, DOI 10.1109/CVPR42600.2020.01405
   Zhang H, 2019, 36 INT C MACHINE LEA, V97
   Zhang HF, 2019, IEEE T IMAGE PROCESS, V28, P506, DOI 10.1109/TIP.2018.2869696
   Zhang HG, 2018, PROC CVPR IEEE, P7670, DOI 10.1109/CVPR.2018.00800
   Zhang KH, 2020, AAAI CONF ARTIF INTE, V34, P12813
   Zhang L, 2017, PROC CVPR IEEE, P3010, DOI 10.1109/CVPR.2017.321
   Zhu PK, 2019, PROC CVPR IEEE, P2990, DOI 10.1109/CVPR.2019.00311
   Zhu YZ, 2018, PROC CVPR IEEE, P1004, DOI 10.1109/CVPR.2018.00111
   Zhu YZ, 2019, IEEE I CONF COMP VIS, P9843, DOI 10.1109/ICCV.2019.00994
   Zongyan Han, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12862, DOI 10.1109/CVPR42600.2020.01288
NR 70
TC 17
Z9 17
U1 5
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1600
EP 1610
DI 10.1109/TMM.2021.3139211
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100001
DA 2024-07-18
ER

PT J
AU Liu, HF
   Peng, P
   Chen, T
   Wang, Q
   Yao, YZ
   Hua, XS
AF Liu, Huafeng
   Peng, Pai
   Chen, Tao
   Wang, Qiong
   Yao, Yazhou
   Hua, Xian-Sheng
TI FECANet: Boosting Few-Shot Semantic Segmentation With Feature-Enhanced
   Context-Aware Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantic segmentation; few-shot learning; few-shot learning; few shot
   semantic segmentation; learning visual correspondence
AB Few-shot semantic segmentation is the task of learning to locate each pixel of the novel class in the query image with only a few annotated support images. The current correlation-based methods construct pair-wise feature correlations to establish the many-to-many matching because the typical prototype-based approaches cannot learn fine-grained correspondence relations. However, the existing methods still suffer from the noise contained in naive correlations and the lack of context semantic information in correlations. To alleviate these problems mentioned above, we propose a Feature-Enhanced Context-Aware Network (FECANet). Specifically, a feature enhancement module is proposed to suppress the matching noise caused by inter-class local similarity and enhance the intra-class relevance in the naive correlation. In addition, we propose a novel correlation reconstruction module that encodes extra correspondence relations between foreground and background and multi-scale context semantic features, significantly boosting the encoder to capture a reliable matching pattern. Experiments on PASCAL-5(i) and COCO-20(i) datasets demonstrate that our proposed FECANet leads to remarkable improvement compared to previous state-of-the-arts, demonstrating its effectiveness.
C1 [Liu, Huafeng; Peng, Pai; Chen, Tao; Wang, Qiong; Yao, Yazhou] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.
   [Hua, Xian-Sheng] Terminus Grp, Beijing 100027, Peoples R China.
C3 Nanjing University of Science & Technology
RP Chen, T; Yao, YZ (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.
EM liu.hua.feng@outlook.com; pakepeng@njust.edu.cn; taochen@njust.edu.cn;
   wangq@njust.edu.cn; yazhou.yao@njust.edu.cn;
   xiansheng.hxs@alibaba-inc.com
RI Chen, Tao/ABB-5983-2022
OI Chen, Tao/0000-0001-8239-1698; Yao, Yazhou/0000-0002-0337-9410; Liu,
   Huafeng/0000-0001-5396-3183; Hua, Xian-Sheng/0000-0002-8232-5049
FU National Natural Science Foundation of China
FX No Statement Available
CR Taghanaki SA, 2021, ARTIF INTELL REV, V54, P137, DOI 10.1007/s10462-020-09854-1
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Balntas V, 2017, PROC CVPR IEEE, P3852, DOI 10.1109/CVPR.2017.410
   Boudiaf M, 2021, PROC CVPR IEEE, P13974, DOI 10.1109/CVPR46437.2021.01376
   Boyu Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P763, DOI 10.1007/978-3-030-58598-3_45
   Chen LC, 2017, Arxiv, DOI [arXiv:1706.05587, DOI 10.48550/ARXIV.1706.05587]
   Chen T, 2023, IEEE T MULTIMEDIA, V25, P1727, DOI 10.1109/TMM.2022.3157481
   Chen T, 2022, IEEE T MULTIMEDIA, V24, P968, DOI 10.1109/TMM.2021.3061816
   Choy CB, 2016, ADV NEUR IN, V29
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Duchenne O, 2011, IEEE I CONF COMP VIS, P1792, DOI 10.1109/ICCV.2011.6126445
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Finn C, 2017, PR MACH LEARN RES, V70
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Ham B, 2018, IEEE T PATTERN ANAL, V40, P1711, DOI 10.1109/TPAMI.2017.2724510
   Haochen Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P730, DOI 10.1007/978-3-030-58601-0_43
   Hariharan B, 2014, LECT NOTES COMPUT SC, V8695, P297, DOI 10.1007/978-3-319-10584-0_20
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang SY, 2019, IEEE I CONF COMP VIS, P2010, DOI 10.1109/ICCV.2019.00210
   Kang B, 2018, IEEE T MULTIMEDIA, V20, P2478, DOI 10.1109/TMM.2018.2798282
   Kang D., 2022, CVPR, P9979
   Nguyen K, 2019, IEEE I CONF COMP VIS, P622, DOI 10.1109/ICCV.2019.00071
   Kim S., 2018, NeurIPS, P6126
   Kim S, 2017, PROC CVPR IEEE, P616, DOI 10.1109/CVPR.2017.73
   Lang CB, 2022, PROC CVPR IEEE, P8047, DOI 10.1109/CVPR52688.2022.00789
   Li S., 2020, P IEEE CVF C COMP VI, P10196
   Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu BH, 2021, PROC CVPR IEEE, P9742, DOI 10.1109/CVPR46437.2021.00962
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147
   Liu J, 2022, PROC CVPR IEEE, P11543, DOI 10.1109/CVPR52688.2022.01126
   Liu JL, 2020, Arxiv, DOI [arXiv:2002.03579, 10.48550/arXiv.2002.03579]
   Liu Y., 2020, P EUR C COMP VIS ECC, P142, DOI DOI 10.1007/978-3-030-58545-79
   Liu Y, 2022, P IEEECVF C COMPUTER, P11573
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu ZH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8721, DOI 10.1109/ICCV48922.2021.00862
   Min J, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6921, DOI 10.1109/ICCV48922.2021.00686
   Paszke A, 2019, ADV NEUR IN, V32
   Rakelly K., 2018, P ICLR WORKSH
   Rocco I, 2018, ADV NEUR IN, V31
   Rocco I, 2017, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2017.12
   Shaban A, 2017, ARXIV PREPRINT ARXIV, DOI 10.5244/C.31.167
   Siam M, 2019, IEEE I CONF COMP VIS, P5248, DOI 10.1109/ICCV.2019.00535
   Snell J, 2017, ADV NEUR IN, V30
   Sun ZR, 2022, PROC CVPR IEEE, P5301, DOI 10.1109/CVPR52688.2022.00524
   Sun ZR, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10582, DOI 10.1109/ICCV48922.2021.01043
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tian ZT, 2022, IEEE T PATTERN ANAL, V44, P1050, DOI 10.1109/TPAMI.2020.3013717
   Vinyals O, 2016, 30 C NEURAL INFORM P, V29
   Wang KX, 2019, IEEE I CONF COMP VIS, P9196, DOI 10.1109/ICCV.2019.00929
   Wang QR, 2019, IEEE T MULTIMEDIA, V21, P1839, DOI 10.1109/TMM.2018.2890360
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wong JM, 2017, IEEE INT C INT ROBOT, P5784, DOI 10.1109/IROS.2017.8206470
   Wu ZH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P497, DOI 10.1109/ICCV48922.2021.00056
   Xie GS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7273, DOI 10.1109/ICCV48922.2021.00720
   Xie GS, 2021, PROC CVPR IEEE, P5471, DOI 10.1109/CVPR46437.2021.00543
   Yao YZ, 2021, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR46437.2021.00515
   Yao YZ, 2021, PROC CVPR IEEE, P2623, DOI 10.1109/CVPR46437.2021.00265
   Yao YZ, 2017, IEEE T MULTIMEDIA, V19, P1771, DOI 10.1109/TMM.2017.2684626
   Zhan C, 2020, IEEE T MULTIMEDIA, V22, P795, DOI 10.1109/TMM.2019.2931441
   Zhang C, 2019, IEEE I CONF COMP VIS, P9586, DOI 10.1109/ICCV.2019.00968
   Zhang C, 2019, PROC CVPR IEEE, P5212, DOI 10.1109/CVPR.2019.00536
   Zhang JJ, 2018, IEEE T MULTIMEDIA, V20, P2801, DOI 10.1109/TMM.2018.2812605
   Zhang XL, 2020, IEEE T CYBERNETICS, V50, P3855, DOI 10.1109/TCYB.2020.2992433
NR 66
TC 11
Z9 11
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8580
EP 8592
DI 10.1109/TMM.2023.3238521
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP5R1
UT WOS:001133278300010
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Pang, HX
   Wei, SK
   Zhang, GJ
   Zhang, SY
   Qiu, S
   Zhao, Y
AF Pang, Huaxin
   Wei, Shikui
   Zhang, Gangjian
   Zhang, Shiyin
   Qiu, Shuang
   Zhao, Yao
TI Heterogeneous Feature Alignment and Fusion in Cross-Modal Augmented
   Space for Composed Image Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image retrieval; Semantics; Task analysis; Visualization; Transformers;
   Feature extraction; Fuses; Composed image retrieval; embedding fusion;
   multi-modal learning; image retrieval
ID REPRESENTATION; FRAMEWORK
AB Composed image retrieval (CIR) aims at fusing a reference image and text feedback to search for the desired images. Compared to general image retrieval, it canmodel the users' search intent more comprehensively and search the target images more accurately, which has significant impacts in various real-world applications, such as E-commerce and Internet search. However, because of the existing heterogeneous semantic gap, the synthetic understanding and fusion of both image and text are difficult to implement. In thiswork, to tackle this difficult problem, we propose an end-to-end framework MCR, which uses text and images as retrieval queries. The framework mainly includes four pivotal modules. Specifically, we introduce the Relative Caption-aware Consistency (RCC) constraint to align text pieces and images in the database, which can effectually bridge the heterogeneous gap. The Multi-modal Complementary Fusion (MCF) and Crossmodal Guided Pooling (CGP) are constructed to mine multiple interactions between image local features and text word features and learn the complementary representation of the composed query. Furthermore, we develop a plug-and-play Weak-text Semantic Augment (WSA) module for datasets with short or incomplete query texts, which can supplement the weak-text features and is conducive to modeling an augmented semantic space. Extensive experiments demonstrate the practical superior performance over the existing state-of-the-art empirical algorithms on several benchmarks.
C1 [Pang, Huaxin; Wei, Shikui; Zhang, Gangjian; Zhang, Shiyin; Qiu, Shuang; Zhao, Yao] Beijing Jiaotong Univ, Inst Informat Sci, Beijing Key Lab Adv Informat Sci & Network Techno, Beijing 100044, Peoples R China.
C3 Beijing Jiaotong University
RP Wei, SK (corresponding author), Beijing Jiaotong Univ, Inst Informat Sci, Beijing Key Lab Adv Informat Sci & Network Techno, Beijing 100044, Peoples R China.
EM 20112005@bjtu.edu.cn; shkwei@bjtu.edu.cn; 19120317@bjtu.edu.cn;
   zhangshiyin1850@163.com; 14120332@bjtu.edu.cn; yzhao@bjtu.edu.cn
OI Zhao, Yao/0000-0002-8581-9554; Pang, Huaxin/0000-0003-4998-4576
FU National Key Research and Development of China [2017YFC1703503];
   National Key R&D Program of China [2021ZD0112100]; National NSF of China
   [61972022, U1936212, 62120106009]
FX This work was supported in part by the National Key Research and
   Development of China under Grant 2017YFC1703503, in part by the National
   Key R&D Program of China under Grant 2021ZD0112100, and in part by the
   National N S Fof China under Grants 61972022, U1936212, and 62120106009.
CR Ak KE, 2018, PROC CVPR IEEE, P7708, DOI 10.1109/CVPR.2018.00804
   Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Anwaar MU, 2021, IEEE WINT CONF APPL, P1139, DOI 10.1109/WACV48630.2021.00118
   Berg TL, 2010, LECT NOTES COMPUT SC, V6311, P663, DOI 10.1007/978-3-642-15549-9_48
   Chen Y.-W., 2019, BMVC, P263
   Chen YB, 2020, PROC CVPR IEEE, P2998, DOI 10.1109/CVPR42600.2020.00307
   Dodds E, 2020, Arxiv, DOI arXiv:2007.00145
   Nguyen DK, 2018, PROC CVPR IEEE, P6087, DOI 10.1109/CVPR.2018.00637
   Eisenschtat A, 2017, PROC CVPR IEEE, P1855, DOI 10.1109/CVPR.2017.201
   Ercoli S, 2017, IEEE T MULTIMEDIA, V19, P2521, DOI 10.1109/TMM.2017.2697824
   Forbes M, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P708
   Fu X, 2020, IEEE T MULTIMEDIA, V22, P2354, DOI 10.1109/TMM.2019.2957948
   Fu Y, 2019, AAAI CONF ARTIF INTE, P8295
   Fukui Akira, 2016, P C EMP METH NAT LAN
   Gao P, 2019, PROC CVPR IEEE, P6632, DOI 10.1109/CVPR.2019.00680
   Gordo A, 2016, LECT NOTES COMPUT SC, V9910, P241, DOI 10.1007/978-3-319-46466-4_15
   Gu XL, 2019, IEEE T MULTIMEDIA, V21, P1524, DOI 10.1109/TMM.2018.2876822
   Han XT, 2017, IEEE I CONF COMP VIS, P1472, DOI 10.1109/ICCV.2017.163
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hosseinzadeh M, 2020, PROC CVPR IEEE, P3593, DOI 10.1109/CVPR42600.2020.00365
   Hu ZW, 2020, PROC CVPR IEEE, P4423, DOI 10.1109/CVPR42600.2020.00448
   Huang QB, 2022, IEEE T MULTIMEDIA, V24, P2004, DOI 10.1109/TMM.2021.3074803
   Isola P, 2015, PROC CVPR IEEE, P1383, DOI 10.1109/CVPR.2015.7298744
   Jhamtani H, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P4024
   Jun SH, 2017, ICEC'17: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON ELECTRONIC COMMERCE, DOI 10.1145/3154943.3154947
   Kim JH, 2016, ADV NEUR IN, V29
   Kiros R, 2014, Arxiv, DOI arXiv:1411.2539
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Li G, 2020, AAAI CONF ARTIF INTE, V34, P11336
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Lu JS, 2019, ADV NEUR IN, V32
   Lu JS, 2016, ADV NEUR IN, V29
   Ma J, 2020, IEEE WINT CONF APPL, P2492, DOI [10.1109/wacv45572.2020.9093427, 10.1109/WACV45572.2020.9093427]
   Mai L, 2017, PROC CVPR IEEE, P1121, DOI 10.1109/CVPR.2017.125
   Nam H, 2017, PROC CVPR IEEE, P2156, DOI 10.1109/CVPR.2017.232
   Noh H, 2016, PROC CVPR IEEE, P30, DOI 10.1109/CVPR.2016.11
   Ovalle J. E. A., 2017, Gated multimodal units for information fusion
   Parkhi OM, 2015, Proceedings of the British Machine Vision Conference, DOI DOI 10.5244/C.29.41
   Perez E, 2018, AAAI CONF ARTIF INTE, P3942
   Qiu S, 2020, IEEE T MULTIMEDIA, V22, P1333, DOI 10.1109/TMM.2019.2942480
   Radenovic F, 2018, LECT NOTES COMPUT SC, V11209, P774, DOI 10.1007/978-3-030-01228-1_46
   Rosani A, 2015, IEEE T MULTIMEDIA, V17, P1359, DOI 10.1109/TMM.2015.2441003
   Rui Y, 1998, IEEE T CIRC SYST VID, V8, P644, DOI 10.1109/76.718510
   Santoro A, 2017, ADV NEUR IN, V30
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Vaswani A, 2017, ADV NEUR IN, V30
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Vo N, 2019, PROC CVPR IEEE, P6432, DOI 10.1109/CVPR.2019.00660
   Wang L, 2016, PROC CVPR IEEE, P5005, DOI 10.1109/CVPR.2016.541
   Wang ZH, 2019, IEEE I CONF COMP VIS, P5763, DOI 10.1109/ICCV.2019.00586
   Wu H, 2020, Arxiv, DOI [arXiv:1905.12794, DOI 10.48550/ARXIV.1905.12794]
   Xu HJ, 2016, LECT NOTES COMPUT SC, V9911, P451, DOI 10.1007/978-3-319-46478-7_28
   Xu J, 2018, PROC CVPR IEEE, P2119, DOI 10.1109/CVPR.2018.00226
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yang XS, 2016, IEEE T MULTIMEDIA, V18, P1832, DOI 10.1109/TMM.2016.2582379
   Yang Y, 2021, FRONT INFORM TECH EL, V22, P1551, DOI 10.1631/FITEE.2100463
   Ye LW, 2019, PROC CVPR IEEE, P10494, DOI 10.1109/CVPR.2019.01075
   Yelamarthi SK, 2018, LECT NOTES COMPUT SC, V11208, P316, DOI 10.1007/978-3-030-01225-0_19
   Yen-Chun Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P104, DOI 10.1007/978-3-030-58577-8_7
   Yu Q, 2016, PROC CVPR IEEE, P799, DOI 10.1109/CVPR.2016.93
   Yu Z, 2017, IEEE I CONF COMP VIS, P1839, DOI 10.1109/ICCV.2017.202
   Zhang J, 2020, IEEE T MULTIMEDIA, V22, P174, DOI 10.1109/TMM.2019.2922128
   Zhang Q, 2020, PROC CVPR IEEE, P3533, DOI 10.1109/CVPR42600.2020.00359
   Zheng ZD, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3383184
   Zhu C, 2017, IEEE I CONF COMP VIS, P1300, DOI 10.1109/ICCV.2017.145
NR 70
TC 0
Z9 0
U1 6
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6446
EP 6457
DI 10.1109/TMM.2022.3208742
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500057
DA 2024-07-18
ER

PT J
AU Sharma, PK
   Abraham, A
   Rajendiran, VN
AF Sharma, Prasen Kumar
   Abraham, Arun
   Rajendiran, Vikram Nelvoy
TI A Generalized Zero-Shot Quantization of Deep Convolutional Neural
   Networks Via Learned Weights Statistics
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Quantization (signal); Training; Data models; Tensors; Calibration;
   Computational modeling; Convolutional neural networks; Data
   distillation; deep convolutional neural networks (CNNs); model
   compression; post-training quantization
AB Quantizating the floating-point weights and activations of deep convolutional neural networks to fixed-point representation yields reduced memory footprints and inference time. Recently, efforts have been afoot towards zero-shot quantization that does not require original unlabelled training samples of a given task. These best-published works heavily rely on the learned batch normalization (BN) parameters to infer the range of the activations for quantization. In particular, these methods are built upon either empirical estimation framework or the data distillation approach, for computing the range of the activations. However, the performance of such schemes severely degrades when presented with a network that does not accommodate BN layers. In this line of thought, we propose a generalized zero-shot quantization (GZSQ) framework that neither requires original data nor relies on BN layer statistics. We have utilized the data distillation approach and leveraged only the pre-trained weights of the model to estimate enriched data for range calibration of the activations. To the best of our knowledge, this is the first work that utilizes the distribution of the pre-trained weights to assist the process of zero-shot quantization. The proposed scheme has significantly outperformed the existing zero-shot works, e.g., an improvement of similar to 33% in classification accuracy for MobileNetV2 and several other models that are w & w/o BN layers, for a variety of tasks. We have also demonstrated the efficacy of the proposed work across multiple open-source quantization frameworks. Importantly, our work is the first attempt towards the post-training zero-shot quantization of futuristic unnormalized deep neural networks.
C1 [Sharma, Prasen Kumar; Abraham, Arun; Rajendiran, Vikram Nelvoy] Samsung R&D Inst India Bangalore, Bangalore 560037, India.
   [Sharma, Prasen Kumar] Indian Inst Technol Guwahati, Gauhati 781039, India.
   [Sharma, Prasen Kumar] TensorTour Inc, Newark, NJ 94560 USA.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Guwahati
RP Sharma, PK (corresponding author), Samsung R&D Inst India Bangalore, Bangalore 560037, India.; Sharma, PK (corresponding author), Indian Inst Technol Guwahati, Gauhati 781039, India.; Sharma, PK (corresponding author), TensorTour Inc, Newark, NJ 94560 USA.
EM kumar176101005@iitg.ac.in; arun.abraham@samsung.com;
   vikram.nr@samsung.com
CR Andersen KG, 2020, NAT MED, V26, P450, DOI 10.1038/s41591-020-0820-9
   Banner R, 2019, ADV NEUR IN, V32
   Baskin C., 2021, J. Mach. Learn. Res, V22, P1
   Bo Xu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14421, DOI 10.1109/CVPR42600.2020.01444
   Brattoli B, 2020, PROC CVPR IEEE, P4612, DOI 10.1109/CVPR42600.2020.00467
   Brock Andrew, 2021, ICLR
   Cao SJ, 2019, PROC CVPR IEEE, P11208, DOI 10.1109/CVPR.2019.01147
   Chen TQ, 2015, Arxiv, DOI arXiv:1512.01274
   Cheng Y, 2015, IEEE I CONF COMP VIS, P2857, DOI 10.1109/ICCV.2015.327
   Choi J, 2018, Arxiv, DOI [arXiv:1805.06085, DOI 10.48550/ARXIV.1805.06085, 10.48550/arXiv.1805.06085]
   Choukroun Y, 2019, IEEE INT CONF COMP V, P3009, DOI 10.1109/ICCVW.2019.00363
   Coates A, 2013, PROC INT C MACH LEAR, P1337
   Cun Y.L., 1990, Optimal Brain Damage
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Denton E, 2014, ADV NEUR IN, V27
   Federici M, 2017, Arxiv, DOI arXiv:1711.06494
   Franchi Gianni, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P105, DOI 10.1007/978-3-030-58520-4_7
   Howard AG, 2017, Arxiv, DOI arXiv:1704.04861
   Gao W., 2019, INT C MACHINE LEARNI, P2102
   Gholami A, 2018, IEEE COMPUT SOC CONF, P1719, DOI 10.1109/CVPRW.2018.00215
   Glorot X., 2010, 13 INT C ARTIFICIAL, V9, P249
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo JY, 2020, PROC CVPR IEEE, P1505, DOI 10.1109/CVPR42600.2020.00158
   Guo KY, 2019, ACM T RECONFIG TECHN, V12, DOI 10.1145/3289185
   Han  S., 2015, ARXIV151000149
   Han S, 2015, ADV NEUR IN, V28
   Haroush Matan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8491, DOI 10.1109/CVPR42600.2020.00852
   Hassibi B., 1993, P ADV NEUR INF PROC, P164
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Y, 2020, PROC CVPR IEEE, P2006, DOI 10.1109/CVPR42600.2020.00208
   He YH, 2018, LECT NOTES COMPUT SC, V11211, P815, DOI 10.1007/978-3-030-01234-2_48
   Hegde S, 2020, INT CONF ACOUST SPEE, P3247, DOI [10.1109/ICASSP40776.2020.9054157, 10.1109/icassp40776.2020.9054157]
   Hinton G., 2015, COMPUT SCI, V2
   Hongxu Yin, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8712, DOI 10.1109/CVPR42600.2020.00874
   HUFFMAN DA, 1952, P IRE, V40, P1098, DOI 10.1109/JRPROC.1952.273898
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Jacob B, 2018, PROC CVPR IEEE, P2704, DOI 10.1109/CVPR.2018.00286
   Jin Q, 2020, PROC CVPR IEEE, P2143, DOI 10.1109/CVPR42600.2020.00222
   Kaiming He, 2015, 2015 IEEE International Conference on Computer Vision (ICCV). Proceedings, P1026, DOI 10.1109/ICCV.2015.123
   Kermany DS, 2018, CELL, V172, P1122, DOI 10.1016/j.cell.2018.02.010
   Kingma D. P., 2014, arXiv
   Krishnamoorthi R, 2018, Arxiv, DOI arXiv:1806.08342
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Krizhevsky Alex, Cifar-10 canadian institute for advanced research
   Kwon K, 2018, DES AUT CON, DOI 10.1145/3195970.3199849
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lin MB, 2020, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR42600.2020.00160
   Lin T.Y., 2014, P 13 EUR C COMP VIS, P740
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Louizos C, 2017, ADV NEUR IN, V30
   Luo JH, 2020, PROC CVPR IEEE, P1455, DOI 10.1109/CVPR42600.2020.00153
   McRobbie D.W., 2006, MRI PICTURE PROTON
   Meller E., 2019, P 36 INT C MACHINE L, P4486
   Mordvintsev Alexander, 2015, GOOGLE RES BLOG, V2015, P3
   Iandola FN, 2016, Arxiv, DOI arXiv:1602.07360
   Nagel M, 2019, IEEE I CONF COMP VIS, P1325, DOI 10.1109/ICCV.2019.00141
   Odena A, 2017, PR MACH LEARN RES, V70
   Oktay O, 2018, IEEE T MED IMAGING, V37, P384, DOI 10.1109/TMI.2017.2743464
   Park E, 2018, LECT NOTES COMPUT SC, V11208, P608, DOI 10.1007/978-3-030-01225-0_36
   Paszke A, 2019, ADV NEUR IN, V32
   Qi H., 2020, INT C MACH LEARN, P7824
   Rajpurkar P, 2017, Arxiv, DOI arXiv:1711.05225
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Romero A., 2015, PROC 3 INT C LEARN R
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Shoukai Xu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P1, DOI 10.1007/978-3-030-58610-2_1
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh P, 2020, IEEE WINT CONF APPL, P824, DOI [10.13140/rg.2.2.28674.94402, 10.1109/WACV45572.2020.9093331]
   Sung WY, 2016, Arxiv, DOI arXiv:1511.06488
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Ullrich K., 2017, PROC INT C LEARN REP
   Wang K, 2019, PROC CVPR IEEE, P11899, DOI [10.1109/CVPR.2019.01218, 10.1109/CVPR.2019.00881]
   Wang X., 2020, P IEEE CVF C COMP VI, P10126
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xu YH, 2020, IEEE T MULTIMEDIA, V22, P1874, DOI 10.1109/TMM.2019.2949857
   Yang G, 2018, IEEE T MED IMAGING, V37, P1310, DOI 10.1109/TMI.2017.2785879
   Yaohui Cai, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13166, DOI 10.1109/CVPR42600.2020.01318
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang DQ, 2018, LECT NOTES COMPUT SC, V11212, P373, DOI 10.1007/978-3-030-01237-3_23
   Zhang H., 2019, PROC INT C LEARN REP
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhao R, 2019, PR MACH LEARN RES, V97
   Zhongnan Qu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7985, DOI 10.1109/CVPR42600.2020.00801
   Zhou A., 2017, PROC INT C LEARN REP, P1
   Zhou Shuchang, 2016, arXiv
   Zhou ZG, 2021, IEEE T MULTIMEDIA, V23, P871, DOI 10.1109/TMM.2020.2990087
   Zoph B, 2017, Arxiv, DOI [arXiv:1611.01578, DOI 10.48550/ARXIV.1611.01578]
NR 87
TC 4
Z9 4
U1 2
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 953
EP 965
DI 10.1109/TMM.2021.3134158
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA C4YC5
UT WOS:000961977900021
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Song, WF
   Hou, X
   Li, S
   Chen, CLZ
   Gao, DY
   Wang, XE
   Sun, YZ
   Hou, JX
   Hao, AM
AF Song, Wenfeng
   Hou, Xia
   Li, Shuai
   Chen, Chenglizhao
   Gao, Danyang
   Wang, Xian'e
   Sun, Yuzhe
   Hou, Jianxia
   Hao, Aimin
TI An Intelligent Virtual Standard Patient for Medical Students Training
   Based on Oral Knowledge Graph
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Intelligent Training; Oral Knowledge Graph; Virtual Standard Patient
AB Virtual standard patient (VSP) is in high demand for medical students' diagnosis ability training in an efficient manner. Different from the traditional conversation system in medical dialogue generation, VSP needs a novel conversation paradigm to act as the patient instead of the doctor. However, existing conversation techniques still have limited ability in terms of generation of symptoms exhibited by patients with the personalized and knowledge-centered expressions. To alleviate these problems, we propose to construct a novel oral knowledge graph, which sufficiently provides medical clues of the certain disease. Accordingly, the VSP could accurately interact with the dentists for their underlying intention and express the symptoms characters in a natural style. To efficiently retrieve the related disease clues, the symptoms descriptions of the oral diseases are encoded into the oral knowledge graph, which could well organize the disease-centered symptom entities and speaking styles. Moreover, to transfer the common sense knowledge from existing large scale of medical knowledge graph to the specific oral knowledge graph, a coupled pre-trained Bert models is further designed to learn the related medical knowledge from coarse-level to fine-level hierarchically. Finally, a series of well-designed personalized templates are proposed to generate plausible and realistic answers in condition of the certain disease. We also conduct extensive user studies to demonstrate that the VSP satisfies the medical students' diagnosis practice requirement in terms of naturalness, realism, and topic relevance.
C1 [Song, Wenfeng; Hou, Xia; Gao, Danyang] Beijing Informat Sci & Technol Univ, Comp Sch, Beijing 100101, Peoples R China.
   [Li, Shuai; Hao, Aimin] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Li, Shuai; Hao, Aimin] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
   [Chen, Chenglizhao] China Univ Petr East China, Coll Comp Sci & Technol, Qingdao 266580, Peoples R China.
   [Wang, Xian'e; Sun, Yuzhe; Hou, Jianxia] Peking Univ, Dept Periodontol, Sch & Hosp Stomatol, Beijing, Peoples R China.
C3 Beijing Information Science & Technology University; Beihang University;
   Peng Cheng Laboratory; China University of Petroleum; Peking University
RP Li, S (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.; Hou, JX (corresponding author), Peking Univ, Dept Periodontol, Sch & Hosp Stomatol, Beijing, Peoples R China.
EM songwenfenga@gmail.com; houxia@bistu.edu.cn; lishuai@buaa.edu.cn;
   cclz123@163.com; gdy561@bistu.edu.cn; wangxiane111@163.com;
   1710303126@pku.edu.cn; jxhou@163.com; ham@buaa.edu.cn
OI Hou, Jianxia/0000-0003-2414-0895
FU Beijing Natural Science Foundation [4222024]; National Natural Science
   Foundation of China [62102036]; R&D Program of Beijing Municipal
   Education Commission [KM202211232003]; Open Project Program of State Key
   Laboratory of Virtual Reality Technology and Systems, Beihang University
   [VRLAB2022A02]; Research Unit of Virtual Human and Virtual Surgery,
   Chinese Academy of Medical Sciences [2019RU004]; Beijing Advanced
   Innovation Center for Biomedical Engineering [ZF138G1714]
FX This work was supported in part by Beijing Natural Science Foundation
   under Grant 4222024, in part by the National Natural Science Foundation
   of China under Grant 62102036, in part by the R&D Program of Beijing
   Municipal Education Commission under Grant KM202211232003, in part by
   the Open Project Program of State Key Laboratory of Virtual Reality
   Technology and Systems, Beihang University under Grant VRLAB2022A02, in
   part by the Research Unit of Virtual Human and Virtual Surgery, Chinese
   Academy of Medical Sciences under Grant 2019RU004, and in part by the
   Beijing Advanced Innovation Center for Biomedical Engineering under
   Grant ZF138G1714.
CR Bond WF, 2019, SIMUL HEALTHC, V14, P241, DOI 10.1097/SIH.0000000000000373
   Bosselut A, 2021, AAAI CONF ARTIF INTE, V35, P4923
   Cao ZS, 2021, AAAI CONF ARTIF INTE, V35, P6894
   Chen JJ, 2021, AAAI CONF ARTIF INTE, V35, P6271
   Chen ZM, 2021, AAAI CONF ARTIF INTE, V35, P4019
   Clark K, 2016, Electra, V85
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Feng SX, 2021, AAAI CONF ARTIF INTE, V35, P12812
   Ghazvininejad M, 2018, AAAI CONF ARTIF INTE, P5110
   Guo QP, 2021, SCI CHINA INFORM SCI, V64, DOI 10.1007/s11432-019-2740-1
   Gurbuz O, 2020, IEEE INT C BIOINFORM, P1720, DOI 10.1109/BIBM49941.2020.9313179
   He Y, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P4604
   Hu Y, 2022, IEEE T MULTIMEDIA, V24, P2473, DOI 10.1109/TMM.2021.3082292
   Huang ZH, 2015, Arxiv, DOI arXiv:1508.01991
   Ji SX, 2022, IEEE T NEUR NET LEAR, V33, P494, DOI 10.1109/TNNLS.2021.3070843
   Joshi A, 2020, M ASS FOR COMPUTATIO
   Jung J, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P3484
   Kim B., 2020, 8 INT C LEARN REPR I
   Kim Y, 2014, ARXIV PREPRINT ARXIV, DOI 10.3115/v1/D14-1181
   Lee CG, 2020, IEEE T SYST MAN CY-S, V50, P1384, DOI 10.1109/TSMC.2017.2719037
   Li DF, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P1427
   Li ZG, 2022, IEEE J BIOMED HEALTH, V26, P1341, DOI 10.1109/JBHI.2021.3116769
   Li ZG, 2020, IEEE INT C BIOINFORM, P1051, DOI 10.1109/BIBM49941.2020.9313452
   Li ZJ, 2020, IEEE INT C BIOINFORM, P2409, DOI 10.1109/BIBM49941.2020.9313239
   Lian RZ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P5081
   Lin Q, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P1417
   Lin S, 2021, AAAI CONF ARTIF INTE, V35, P13362
   Liu B, 2021, AAAI CONF ARTIF INTE, V35, P15058
   Liu QL, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 2, P201
   Liu SM, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P1489
   Liu WG, 2021, NEUROCOMPUTING, V442, P260, DOI 10.1016/j.neucom.2021.02.021
   Liu ZB, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P1782
   Luo X, 2022, IEEE J BIOMED HEALTH, V26, P1737, DOI 10.1109/JBHI.2021.3123192
   Maas L, 2021, HEALTHINF: PROCEEDINGS OF THE 14TH INTERNATIONAL JOINT CONFERENCE ON BIOMEDICAL ENGINEERING SYSTEMS AND TECHNOLOGIES - VOL. 5: HEALTHINF, P509, DOI 10.5220/0010261605090514
   Nie LQ, 2013, IEEE T MULTIMEDIA, V15, P426, DOI 10.1109/TMM.2012.2229971
   Quan J, 2021, AAAI CONF ARTIF INTE, V35, P16097
   Sastre J, 2020, IEEE INT C BIOINFORM, P2513, DOI 10.1109/BIBM49941.2020.9313350
   Sharma S, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P6092
   Shi XM, 2022, IEEE J BIOMED HEALTH, V26, P2770, DOI 10.1109/JBHI.2021.3133667
   Sutton C, 2012, FOUND TRENDS MACH LE, V4, P267, DOI 10.1561/2200000013
   Tuan YL, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P1855
   Vales-Alonso J, 2015, IEEE T SYST MAN CY-S, V45, P1138, DOI 10.1109/TSMC.2015.2391258
   Wang JQ, 2020, IEEE INT C BIOINFORM, P1748, DOI 10.1109/BIBM49941.2020.9313258
   Wang RJ, 2019, LECT NOTES COMPUT SC, V11446, P659, DOI 10.1007/978-3-030-18576-3_39
   Wang TT, 2019, IEEE INT C BIOINFORM, P1256, DOI [10.1109/bibm47256.2019.8983062, 10.1109/BIBM47256.2019.8983062]
   Wu YX, 2023, IEEE T MULTIMEDIA, V25, P3113, DOI 10.1109/TMM.2022.3155900
   Xiong WH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P4258
   Xu J, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P1835
   Xu L, 2019, AAAI CONF ARTIF INTE, P7346
   Xu Liu, 2021, Proceedings. 2021 International Conference on Electronics, Circuits and Information Engineering (ECIE), P24, DOI 10.1109/ECIE52353.2021.00013
   Xu RJ, 2021, AAAI CONF ARTIF INTE, V35, P14158
   Yan JH, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P1490
   Yang SQ, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P1878
   Yuan J, 2021, IEEE T SYST MAN CY-S, V51, P354, DOI 10.1109/TSMC.2018.2871104
   Zhao XY, 2023, IEEE T NEUR NET LEAR, V34, P5212, DOI 10.1109/TNNLS.2021.3124640
   Zheng CJ, 2020, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, EMNLP 2020, P115
   Zheng W, 2018, J BIOMED INFORM, V83, P1, DOI 10.1016/j.jbi.2018.05.001
   Zhou H, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4623
   Zhu WY, 2017, Arxiv, DOI arXiv:1709.04264
NR 59
TC 2
Z9 2
U1 5
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6132
EP 6145
DI 10.1109/TMM.2022.3205456
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500035
DA 2024-07-18
ER

PT J
AU Tang, Z
   Yang, Y
   Li, W
   Lian, DF
   Duan, LX
AF Tang, Zhe
   Yang, Yi
   Li, Wen
   Lian, Defu
   Duan, Lixin
TI Deep Cross-Attention Network for Crowdfunding Success Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Crowdfunding success prediction; attention mechanism; multimodal
   learning
AB Crowdfunding creates opportunities for entrepre- neurs. It allows startup companies to reach a large audience for fundraising and bring their creative ideas to life. In this work, we are concerned with crowdfunding project success prediction problem, i.e., to predict whether a project will successfully reach its funding goal by using its project profiles. This is important for startup companies to refine their project profiles and achieve their goals. Crowdfunding project success prediction is a typical classification problem but with a few critical challenges. On the one hand, with only coarse-grained project status as weak supervision, it is hard for a deep learning network to learn the relationship between project profiles and explain why it makes this prediction. On the other hand, on the project homepage, there are various modalities of description, including metadata, textual description, images, and videos. Among those, videos play an important role in the success of a crowdfunding project, however, were ignored in previous works, due to the difficulty in extracting useful semantic and authentic information from videos, especially for the crowdfunding project where information in different modalities are unaligned. To this end, we propose a novel framework called Deep Cross-Attention Network to learn and fuse information from introduction videos and textual descriptions of project profiles. More specifically, we develop a cross-attention block to align and represent mismatched textual description and untrimmed introduction videos and fuse the information from these two modalities, which effectively remedies the lack of supervised information caused by project status as weak supervision. More importantly, with our cross-attention mechanism, the model is able to interpret how it makes such predictions and show which keywords and keyframes it depends on. We conduct extensive experiments on two crowdfunding datasets (collected from Kickstarter and Indiegogo) and show that our method achieves superior performance over existing state-of-the-art baselines.
C1 [Tang, Zhe; Li, Wen; Duan, Lixin] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610054, Peoples R China.
   [Yang, Yi] Hong Kong Univ Sci & Technol, Business Sch, Hong Kong 999077, Peoples R China.
   [Lian, Defu] Univ Sci & Technol China, Sch Comp Sci & Technol, Hefei 230026, Peoples R China.
   [Lian, Defu] Univ Sci & Technol China, Sch Data Sci, Hefei 230026, Peoples R China.
C3 University of Electronic Science & Technology of China; Hong Kong
   University of Science & Technology; Chinese Academy of Sciences;
   University of Science & Technology of China, CAS; Chinese Academy of
   Sciences; University of Science & Technology of China, CAS
RP Duan, LX (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610054, Peoples R China.
EM zhetang@std.uestc.edu.cn; imyiyang@ust.hk; liwen@uestc.edu.cn;
   dove.ustc@gmail.com; lxduan@uestc.edu.cn
OI Yang, Yi/0000-0001-8863-112X; Lian, Defu/0000-0002-3507-9607; Li,
   Wen/0000-0002-5559-8594
FU Major Project for New Generation of AI [2018AAA0100400]; National
   Natural Science Foundation of China [62176047]
FX This work was supported in part by the Major Project for New Generation
   of AI under Grant 2018AAA0100400 and in part by the National Natural
   Science Foundation of China under Grant 62176047.
CR Abu-El-Haija Sami, 2016, arXiv, DOI [DOI 10.48550/ARXIV.1609.08675, DOI 10.48550/-ARXIV.1609.08675]
   Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI [10.1109/TPAMI.2017.2711011, 10.1109/CVPR.2016.572]
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Bapna S, 2019, MANAGE SCI, V65, P933, DOI 10.1287/mnsc.2017.2833
   Cheng CR, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2158
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dey S, 2017, CSCW'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON COMPUTER SUPPORTED COOPERATIVE WORK AND SOCIAL COMPUTING, P755, DOI 10.1145/2998181.2998229
   Dong JF, 2018, IEEE T MULTIMEDIA, V20, P3377, DOI 10.1109/TMM.2018.2832602
   Etter V., 2013, P 1 ACM C ONL SOC NE, P177, DOI DOI 10.1145/2512938.2512957
   Evangelopoulos G, 2013, IEEE T MULTIMEDIA, V15, P1553, DOI 10.1109/TMM.2013.2267205
   Frermann Lea, 2018, Transactions of the Association for Computational Linguistics, V6, P1, DOI DOI 10.1162/TACLA00001
   Fukui Akira, 2016, P C EMP METH NAT LAN
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Gerber E., 2012, CROWDFUNDING WHY PEO, DOI DOI 10.1145/2530540
   Greenberg J., 2015, Academy of Management Proceedings
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jiang YG, 2018, IEEE T MULTIMEDIA, V20, P3137, DOI 10.1109/TMM.2018.2823900
   Kingma D. P., 2014, arXiv
   Kornblith S, 2019, PROC CVPR IEEE, P2656, DOI 10.1109/CVPR.2019.00277
   Lee J, 2019, LECT NOTES COMPUT SC, V11132, P193, DOI 10.1007/978-3-030-11018-5_18
   Li D, 2019, IEEE T MULTIMEDIA, V21, P416, DOI 10.1109/TMM.2018.2862341
   Lin RC, 2019, LECT NOTES COMPUT SC, V11132, P206, DOI 10.1007/978-3-030-11018-5_19
   Lokoc J, 2018, IEEE T MULTIMEDIA, V20, P3361, DOI 10.1109/TMM.2018.2830110
   Lu CT, 2014, WSDM'14: PROCEEDINGS OF THE 7TH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P573, DOI 10.1145/2556195.2556251
   Luong M.-T., 2015, P 2015 C EMPIRICAL M, DOI DOI 10.18653/V1/D15-1166
   Merler M, 2019, IEEE T MULTIMEDIA, V21, P1147, DOI 10.1109/TMM.2018.2876046
   Mitra T., 2014, P 17 ACM C COMP SUPP, P49, DOI [DOI 10.1145/2531602.2531656, 10.1145/2531602.2531656]
   Mollick E, 2014, J BUS VENTURING, V29, P1, DOI 10.1016/j.jbusvent.2013.06.005
   Nam H, 2017, PROC CVPR IEEE, P2156, DOI 10.1109/CVPR.2017.232
   Nauata N, 2020, IEEE T PATTERN ANAL, V42, P1257, DOI 10.1109/TPAMI.2019.2893215
   PARK CW, 1986, J MARKETING RES, V23, P11, DOI 10.2307/3151772
   Paszke A, 2019, ADV NEUR IN, V32
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Qin Y, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P390
   Wang Alex, 2018, ABS180407461 CORR, DOI DOI 10.18653/V1/W18-5446
   Wolf T, 2020, Arxiv, DOI arXiv:1910.03771
   Xu AB, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P591, DOI 10.1145/2556288.2557045
   Xupin Zhang, 2021, Journal of Social Computing, V2, P183, DOI 10.23919/JSC.2021.0010
   Yan CG, 2020, IEEE T MULTIMEDIA, V22, P229, DOI 10.1109/TMM.2019.2924576
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Younkin P, 2018, MANAGE SCI, V64, P3269, DOI 10.1287/mnsc.2017.2774
   Zadeh A, 2016, IEEE INTELL SYST, V31, P82, DOI 10.1109/MIS.2016.94
   Zhang HZ, 2020, IEEE T MULTIMEDIA, V22, P3210, DOI 10.1109/TMM.2020.2973828
   Zhang J, 2019, IEEE T MULTIMEDIA, V21, P2518, DOI 10.1109/TMM.2019.2907453
   Zhao HK, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P625, DOI 10.1145/3097983.3098030
   Zhou ZH, 2018, NATL SCI REV, V5, P44, DOI 10.1093/nsr/nwx106
   Zvilichovsky D., 2013, PROC INT C INF SYST
NR 49
TC 2
Z9 2
U1 9
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1306
EP 1319
DI 10.1109/TMM.2022.3141256
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA D7WI3
UT WOS:000970791100022
DA 2024-07-18
ER

PT J
AU Wang, BL
   Yang, K
   Zhao, YQ
   Long, T
   Li, XL
AF Wang, Binglu
   Yang, Kang
   Zhao, Yongqiang
   Long, Teng
   Li, Xuelong
TI Prototype-Based Intent Perception
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Prototypes; Semantics; Task analysis; Feature extraction; Training;
   Clustering algorithms; Animals; Intent perception; semantic ambiguity;
   prototype learning; cluster
ID MOTIVATION
AB Intent perception is a novel task that aims to understand the intention of images, regular classification methods usually perform unsatisfactorily on intent perception due to the semantic ambiguity problem, i.e. the intra-class variety problem in which images of the same intent class may contain objects of different semantic categories and the inter-class confusion problem in which images of different intent classes may contain objects of similar semantic categories. To address this problem, this paper introduces prototype learning into the intent perception and proposes a unified framework named PIP-Net to reduce the influence of semantic ambiguity. Specifically, for each intent class, we first filter semantic ambiguity samples which are far away from the cluster center. Then we use features of the filtered samples to generate prototypes via clustering algorithm. Besides, we enhance the diversity between prototypes of different classes to better handle the inter-class confusion problem. To update the prototypes in the training process, we introduce a global matching algorithm to holistically match each feature with class prototypes, and use the momentum update strategy to stably update prototypes. Experimental results on the Intentonomy dataset demonstrate that our method can consistently outperform the traditional classification paradigm in multiple baseline models, and verify the effectiveness of our proposed prototype learning paradigm in addressing the intent perception problem. Our proposed PIP-Net achieves a new state-of-the-art performance on Intentonomy, including Macro F1 score of 31.57% and averaging F1 score of 41.85%.
C1 [Wang, Binglu; Long, Teng] Beijing Inst Technol, Sch Informat & Elect, Beijing 100081, Peoples R China.
   [Yang, Kang] Xian Univ Architecture & Technol, Coll Informat & Control Engn, Xian 710055, Peoples R China.
   [Zhao, Yongqiang] Northwestern Polytech Univ, Sch Automat, Xian 710072, Peoples R China.
   [Li, Xuelong] Northwestern Polytech Univ, Sch Artificial Intelligence Opt & Elect iOPEN, Xian 710072, Peoples R China.
C3 Beijing Institute of Technology; Xi'an University of Architecture &
   Technology; Northwestern Polytechnical University; Northwestern
   Polytechnical University
RP Long, T (corresponding author), Beijing Inst Technol, Sch Informat & Elect, Beijing 100081, Peoples R China.
EM wbl921129@gmail.com; y1127238112@gmail.com; zhaoyq@nwpu.edu.cn;
   longteng@bit.edu.cn; li@nwpu.edu.cn
RI Zhao, Yongqiang/O-7342-2015; Li, Xue-long/AFU-6301-2022; Wang,
   Binglu/GLR-6556-2022
OI Li, Xue-long/0000-0003-2037-2525; Wang, Binglu/0000-0002-9266-4685
FU China Postdoctoral Science Foundation
FX No Statement Available
CR Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS)
   Caron M, 2019, IEEE I CONF COMP VIS, P2959, DOI 10.1109/ICCV.2019.00305
   Caron Mathilde, 2018, P EUR C COMP VIS ECC, P132, DOI [DOI 10.1007/978-3-030-01264-9_9, 10.48550/arXiv.1807.05520, DOI 10.48550/ARXIV.1807.05520]
   Chen JC, 2023, IEEE T MULTIMEDIA, V25, P4361, DOI 10.1109/TMM.2022.3174405
   Chen T, 2022, IEEE T MULTIMEDIA, V24, P968, DOI 10.1109/TMM.2021.3061816
   Choi MJ, 2012, PATTERN RECOGN LETT, V33, P853, DOI 10.1016/j.patrec.2011.12.004
   Cowie R, 2001, IEEE SIGNAL PROC MAG, V18, P32, DOI 10.1109/79.911197
   Dahmane M, 2014, IEEE T MULTIMEDIA, V16, P1574, DOI 10.1109/TMM.2014.2321113
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng JK, 2021, PROC CVPR IEEE, P11901, DOI 10.1109/CVPR46437.2021.01173
   Frénay B, 2014, IEEE T NEUR NET LEAR, V25, P845, DOI 10.1109/TNNLS.2013.2292894
   Fu Y., 2013, P 10 IEEE INT C WORK, P1
   Ghaisani AP, 2017, PROCEDIA COMPUT SCI, V124, P530, DOI 10.1016/j.procs.2017.12.186
   Gong S., 2020, COMPUTER VISION ECCV, P330
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Kaiming, 2020, C COMP VIS PATT REC, P2, DOI [DOI 10.1109/CVPR42600.2020.00975, 10.1109/CVPR42600.2020.00975]
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jia ML, 2021, PROC CVPR IEEE, P12981, DOI 10.1109/CVPR46437.2021.01279
   Jinlu Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P741, DOI 10.1007/978-3-030-58452-8_43
   Joo J, 2015, IEEE I CONF COMP VIS, P3712, DOI 10.1109/ICCV.2015.423
   Joo J, 2014, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2014.35
   Kan MN, 2013, IEEE T IMAGE PROCESS, V22, P3310, DOI 10.1109/TIP.2013.2256918
   Khosla P., 2020, C NEUR INF PROC SYST
   Kim H, 2021, PROC CVPR IEEE, P4863, DOI 10.1109/CVPR46437.2021.00483
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kruk J, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P4622
   Kulkarni G, 2013, IEEE T PATTERN ANAL, V35, P2891, DOI 10.1109/TPAMI.2012.162
   Lee KH, 2018, PROC CVPR IEEE, P5447, DOI 10.1109/CVPR.2018.00571
   Li G, 2021, PROC CVPR IEEE, P8330, DOI 10.1109/CVPR46437.2021.00823
   Li J., 2019, P INT C LEARN REPR
   Li J., 2020, P INT C LEARN REPR, P1
   Liberty Edo, 2016, 2016 P 18 WORKSH ALG, P81
   Liu BB, 2020, IEEE ROBOT AUTOM LET, V5, P3485, DOI 10.1109/LRA.2020.2976305
   Liu W, 2017, ADV SOC SCI EDUC HUM, V99, P212
   Liu WY, 2016, PR MACH LEARN RES, V48
   Lukasik M., 2020, INT C MACH LEARN, P6448, DOI DOI 10.48550/ARXIV.2003.02819
   Mottaghi R, 2014, PROC CVPR IEEE, P891, DOI 10.1109/CVPR.2014.119
   Pirsiavash H., 2014, Tech. Rep.
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Singh K.K., 2020, P IEEE CVF C COMP VI, P11070
   Snell J, 2017, ADV NEUR IN, V30
   Stavros C, 2014, SPORT MANAG REV, V17, P455, DOI 10.1016/j.smr.2013.11.004
   Torralba A, 2003, INT J COMPUT VISION, V53, P169, DOI 10.1023/A:1023052124951
   van den Oord A, 2019, Arxiv, DOI arXiv:1807.03748
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vondrick C, 2016, PROC CVPR IEEE, P2997, DOI 10.1109/CVPR.2016.327
   Wang B., 2022, INIEEE C COMPUT VIS, P19588
   Wang S., 2012, PROC AUST NZ MARKETI
   Wang XH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8148, DOI 10.1109/ICCV48922.2021.00806
   Wang XW, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P5635
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Xu W., 2020, NEURIPS, V33, P21969
   Yang HM, 2018, PROC CVPR IEEE, P3474, DOI 10.1109/CVPR.2018.00366
   Zhang Mengmi, 2020, Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit, V2020, P12982, DOI 10.1109/CVPR42600.2020.01300
   Zhang XX, 2020, IEEE T MULTIMEDIA, V22, P1692, DOI 10.1109/TMM.2019.2959433
   Zhou TF, 2022, PROC CVPR IEEE, P2572, DOI 10.1109/CVPR52688.2022.00261
   Zhu F, 2021, PROC CVPR IEEE, P5867, DOI 10.1109/CVPR46437.2021.00581
   Zhu K, 2021, PROC CVPR IEEE, P6797, DOI 10.1109/CVPR46437.2021.00673
   Zhu YK, 2014, IEEE T MULTIMEDIA, V16, P1585, DOI 10.1109/TMM.2014.2321534
   Zyner A, 2017, IEEE INT VEH SYM, P1484, DOI 10.1109/IVS.2017.7995919
NR 60
TC 3
Z9 3
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8308
EP 8319
DI 10.1109/TMM.2023.3234817
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000072
DA 2024-07-18
ER

PT J
AU Weng, SW
   Zhou, Y
   Zhang, TC
   Xiao, MY
   Zhao, Y
AF Weng, Shaowei
   Zhou, Ye
   Zhang, Tiancong
   Xiao, Mengyao
   Zhao, Yao
TI General Framework to Reversible Data Hiding for JPEG Images With
   Multiple Two-Dimensional Histograms
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptive 2D mapping generation; block smoothness estimator; band
   smoothness estimator; IDPSO; JPEG images; reversible data hiding
ID EXPANSION; BITSTREAM
AB In this paper, a general reversible data hiding (RDH) framework for joint photographic experts group (JPEG) images with multiple two dimensional histograms (2DHs) is proposed. Regardless of whether zero alternating current (AC) coefficients are included to join data embedding or only non-zero AC coefficients are applied, the performance in terms of visual quality and file size increment is improved by using the proposed framework. This framework is mainly composed of the following three parts: histogram generation, adaptive 2DH mapping selection, and improved discrete particle swarm optimization (IDPSO). Unlike existing 2DH-based JPEG RDH methods, in which a uniform threshold is utilized to construct multiple histograms, in histogram generation, thresholds for different histograms are adaptively assigned according to the local properties of histogram coefficients. As a result, as many coefficients in complex regions as possible are excluded from the construction of each histogram. We subtly design multiple 2DH mappings, and adaptively select 2DH mappings for different 2DHs based on their distribution characteristics. Through slight adjustments, each 2DH mapping can be employed in cases where either zero AC coefficients or only non-zero AC coefficients are used for data embedding. Adaptive threshold and 2DH mapping selection provide a better image quality at a given embedding capacity but inevitably cause considerable complexity cost. To significantly reduce the computational cost, we propose IDPSO by combining differential evolution. IDPSO has the advantages of rapid convergence speed as well as satisfactory qualities of the best solutions. With the help of differential evolution, IDPSO expands the diversity of particles and efficiently avoids local optimal trapping problems. The experimental results also demonstrate the effectiveness of the proposed method in terms of visual quality, file size increment and complexity cost.
C1 [Weng, Shaowei; Zhang, Tiancong] Fujian Univ Technol, Fujian Prov Key Lab Big Data Min & Applicat, Fuzhou 350108, Fujian, Peoples R China.
   [Weng, Shaowei; Zhang, Tiancong] Fujian Univ Technol, Sch Elect Elect Engn & Phys, Fuzhou 350108, Fujian, Peoples R China.
   [Zhou, Ye] Fujian Univ Technol, Sch Comp Sci & Math, Fuzhou 350108, Fujian, Peoples R China.
   [Xiao, Mengyao; Zhao, Yao] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
   [Xiao, Mengyao; Zhao, Yao] Beijing Key Lab Adv Informat Sci & Network Techno, Beijing 100044, Peoples R China.
C3 Fujian University of Technology; Fujian University of Technology; Fujian
   University of Technology; Beijing Jiaotong University; Beijing Jiaotong
   University
RP Zhang, TC (corresponding author), Fujian Univ Technol, Fujian Prov Key Lab Big Data Min & Applicat, Fuzhou 350108, Fujian, Peoples R China.; Zhang, TC (corresponding author), Fujian Univ Technol, Sch Elect Elect Engn & Phys, Fuzhou 350108, Fujian, Peoples R China.
EM wswweiwei@126.com; 1962270759@qq.com; kushentian@163.com;
   xiaomengyao@bjtu.edu.cn; yzhao@bjtu.edu.cn
OI Xiao, Mengyao/0009-0005-2877-7721; Zhao, Yao/0000-0002-8581-9554
FU National NSF of China [61872095, 61571139, 61872128]; International
   Scientific and Technological Cooperation of Guangdong Province
   [2019A050513012]; Open Project Program of Shenzhen Key Laboratory of
   Media Security [ML-2018-03]; Fujian Science Fund for Distinguished Young
   Scholars [2020J06043]
FX This work was supported in part by the National NSF of China under
   Grants 61872095, 61571139, and 61872128, in part International
   Scientific and Technological Cooperation of Guangdong Province under
   Grant 2019A050513012, in part by the Open Project Program of Shenzhen
   Key Laboratory of Media Security under Grant ML-2018-03, and in part by
   Fujian Science Fund for Distinguished Young Scholars under Grant
   2020J06043. The Associate Editor coordinating the review of this
   manuscript and approving it for publication was Prof. Siwei Lyu.
   (Corresponding author: Tiancong Zhang.)
CR Alattar AM, 2004, IEEE T IMAGE PROCESS, V13, P1147, DOI 10.1109/TIP.2004.828418
   [Anonymous], 1999, Kodak lossless true color image database
   [Anonymous], 2004, Uncompressed colour image database
   [Anonymous], 1977, USC SIPI IMAGE DATAB
   Chen KJ, 2021, IEEE T CIRC SYST VID, V31, P3942, DOI 10.1109/TCSVT.2020.3044186
   Di FQ, 2019, MULTIMED TOOLS APPL, V78, P34541, DOI 10.1007/s11042-019-08109-8
   Dong TF, 2020, ADV INTELL SYST, V895, P116, DOI 10.1007/978-3-030-16946-6_10
   Du Y, 2022, IEEE T DEPEND SECURE, V19, P1420, DOI 10.1109/TDSC.2020.3013326
   Fridrich J, 2002, P SOC PHOTO-OPT INS, V4675, P572, DOI 10.1117/12.465317
   He JH, 2020, SIGNAL PROCESS, V175, DOI 10.1016/j.sigpro.2020.107647
   He JH, 2020, IEEE T INF FOREN SEC, V15, P2121, DOI 10.1109/TIFS.2019.2958758
   Hou DD, 2018, SIGNAL PROCESS, V148, P41, DOI 10.1016/j.sigpro.2018.02.002
   Huang FJ, 2016, IEEE T CIRC SYST VID, V26, P1610, DOI 10.1109/TCSVT.2015.2473235
   Li FY, 2022, INFORM SCIENCES, V595, P142, DOI 10.1016/j.ins.2022.02.040
   Li N, 2020, SIGNAL PROCESS, V171, DOI 10.1016/j.sigpro.2020.107476
   Li XL, 2015, IEEE T INF FOREN SEC, V10, P2016, DOI 10.1109/TIFS.2015.2444354
   Mobasseri BG, 2010, IEEE T IMAGE PROCESS, V19, P958, DOI 10.1109/TIP.2009.2035227
   Ni ZC, 2006, IEEE T CIRC SYST VID, V16, P354, DOI 10.1109/TCSVT.2006.869964
   Ou B, 2013, IEEE T IMAGE PROCESS, V22, P5010, DOI 10.1109/TIP.2013.2281422
   Puteaux P, 2021, IEEE T MULTIMEDIA, V23, P636, DOI 10.1109/TMM.2020.2985537
   Qian ZX, 2019, IEEE T CIRC SYST VID, V29, P351, DOI 10.1109/TCSVT.2018.2797897
   Qian ZX, 2017, KSII T INTERNET INF, V11, P945, DOI 10.3837/tiis.2017.02.017
   Qian ZX, 2014, IEEE T MULTIMEDIA, V16, P1486, DOI 10.1109/TMM.2014.2316154
   Qin JQ, 2019, IEEE SIGNAL PROC LET, V26, P843, DOI 10.1109/LSP.2019.2909080
   Qiu YQ, 2021, IEEE T CIRC SYST VID, V31, P1380, DOI 10.1109/TCSVT.2020.3006494
   Qiu YQ, 2018, J VIS COMMUN IMAGE R, V52, P86, DOI 10.1016/j.jvcir.2018.02.005
   Thodi DM, 2007, IEEE T IMAGE PROCESS, V16, P721, DOI 10.1109/TIP.2006.891046
   Tian J, 2003, IEEE T CIRC SYST VID, V13, P890, DOI 10.1109/TCSVT.2003.815962
   Wang JX, 2017, IEEE T CYBERNETICS, V47, P315, DOI 10.1109/TCYB.2015.2514110
   Wang K, 2013, J SYST SOFTWARE, V86, P1965, DOI 10.1016/j.jss.2013.03.083
   Wedaj FT, 2017, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-017-0206-1
   Weng SW, 2008, IEEE SIGNAL PROC LET, V15, P721, DOI 10.1109/LSP.2008.2001984
   Xiao MY, 2021, IEEE SIGNAL PROC LET, V28, P1620, DOI 10.1109/LSP.2021.3101424
   Xiao MY, 2021, IEEE T CIRC SYST VID, V31, P2535, DOI 10.1109/TCSVT.2020.3027391
   Xuan GR, 2019, J INF SECUR APPL, V45, P1, DOI 10.1016/j.jisa.2018.12.007
   Yin ZX, 2020, IEEE T CIRC SYST VID, V30, P2343, DOI 10.1109/TCSVT.2020.2969463
   Zhang XP, 2013, IEEE T MULTIMEDIA, V15, P316, DOI 10.1109/TMM.2012.2229262
NR 37
TC 10
Z9 10
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5747
EP 5762
DI 10.1109/TMM.2022.3198877
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500008
DA 2024-07-18
ER

PT J
AU Xiao, J
   Jiang, XY
   Zheng, NX
   Yang, H
   Yang, YF
   Yang, YQ
   Li, DS
   Lam, KM
AF Xiao, Jun
   Jiang, Xinyang
   Zheng, Ningxin
   Yang, Huan
   Yang, Yifan
   Yang, Yuqing
   Li, Dongsheng
   Lam, Kin-Man
TI Online Video Super-Resolution With Convolutional Kernel Bypass Grafts
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Complexity theory; Streaming media; Kernel;
   Superresolution; Computational modeling; Video sequences; Video
   super-resolution; deep lightweight model; video restoration
AB Deep learning-based models have achieved remarkable performance in video super-resolution (VSR) in recent years, but most of these models are less applicable to online video applications. These methods solely consider the distortion quality and ignore crucial requirements for online applications, e.g., low latency and low model complexity. In this paper, we focus on online video transmission in which VSR algorithms are required to generate high-resolution video sequences frame by frame in real time. To address such challenges, we propose an extremely low-latency VSR algorithm based on a novel kernel knowledge transfer method, named the convolutional kernel bypass graft (CKBG). First, we design a lightweight network structure that does not require future frames as inputs and saves extra time for caching these frames. Then, our proposed CKBG method enhances this lightweight base model by bypassing the original network with "kernel grafts," which are extra convolutional kernels containing the prior knowledge of the external pretrained image SR models. During the testing phase, we further accelerate the grafted multibranch network by converting it into a simple single-path structure. The experimental results show that our proposed method can process online video sequences up to 110 FPS with very low model complexity and competitive SR performance.
C1 [Xiao, Jun; Lam, Kin-Man] Hong Kong Polytech Univ, Dept Elect & Informat Engn, Hong Kong, Peoples R China.
   [Jiang, Xinyang; Zheng, Ningxin; Yang, Huan; Yang, Yifan; Yang, Yuqing; Li, Dongsheng] Microsoft Res Asia, Shanghai 200232, Peoples R China.
C3 Hong Kong Polytechnic University; Microsoft Research Asia; Microsoft
RP Lam, KM (corresponding author), Hong Kong Polytech Univ, Dept Elect & Informat Engn, Hong Kong, Peoples R China.
EM jun-gwansiu.xiao@connect.polyu.hk; xinyangj@hotmail.com;
   Ningxin.Zheng@microsoft.com; huayan@microsoft.com;
   yifanyang@microsoft.com; yuqyang@microsoft.com;
   dongshengli@fudan.edu.cn; enkmlam@polyu.edu.hk
RI Jiang, Xinyang/AAI-8698-2020; Yuqing, Yang/ADJ-2720-2022; XIAO,
   JUN/GRS-1866-2022; Lam, Kin-Man/A-9352-2014
OI XIAO, JUN/0000-0002-4935-7866; Yang, Yifan/0000-0002-5481-2851; Lam,
   Kin-Man/0000-0002-0422-8454; Li, Dongsheng/0000-0003-3103-8442
FU Hong Kong Research Grants Council
FX No Statement Available
CR Ahn N, 2018, LECT NOTES COMPUT SC, V11214, P256, DOI 10.1007/978-3-030-01249-6_16
   Caballero J, 2017, PROC CVPR IEEE, P2848, DOI 10.1109/CVPR.2017.304
   Cai JR, 2019, IEEE I CONF COMP VIS, P3086, DOI 10.1109/ICCV.2019.00318
   Chan KCK, 2022, PROC CVPR IEEE, P5962, DOI 10.1109/CVPR52688.2022.00588
   Chan KCK, 2021, PROC CVPR IEEE, P4945, DOI 10.1109/CVPR46437.2021.00491
   Chen YX, 2022, IEEE T MULTIMEDIA, V24, P4128, DOI 10.1109/TMM.2022.3152941
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Ding XH, 2021, PROC CVPR IEEE, P10881, DOI 10.1109/CVPR46437.2021.01074
   Ding XH, 2021, PROC CVPR IEEE, P13728, DOI 10.1109/CVPR46437.2021.01352
   Fuoli D, 2019, IEEE INT CONF COMP V, P3476, DOI 10.1109/ICCVW.2019.00431
   Howard AG, 2017, Arxiv, DOI arXiv:1704.04861
   Gao QQ, 2019, LECT NOTES COMPUT SC, V11362, P527, DOI 10.1007/978-3-030-20890-5_34
   Haris M, 2019, PROC CVPR IEEE, P3892, DOI 10.1109/CVPR.2019.00402
   He ZB, 2020, IEEE IMAGE PROC, P518, DOI [10.1109/icip40778.2020.9190917, 10.1109/ICIP40778.2020.9190917]
   Huang Y, 2018, IEEE T PATTERN ANAL, V40, P1015, DOI 10.1109/TPAMI.2017.2701380
   Hui Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2024, DOI 10.1145/3343031.3351084
   Hui Z, 2018, PROC CVPR IEEE, P723, DOI 10.1109/CVPR.2018.00082
   Ignatov A, 2021, IEEE COMPUT SOC CONF, P2535, DOI 10.1109/CVPRW53098.2021.00287
   Isobe Takashi, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P645, DOI 10.1007/978-3-030-58610-2_38
   Isobe T., 2020, P BRIT MACH VIS C, P1
   Jo Y, 2018, PROC CVPR IEEE, P3224, DOI 10.1109/CVPR.2018.00340
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kingma D. P., 2014, arXiv
   Lau CP, 2017, IEEE SYST J, V11, P2737, DOI 10.1109/JSYST.2015.2493180
   Li JY, 2022, LECT NOTES COMPUT SC, V13678, P592, DOI 10.1007/978-3-031-19797-0_34
   Li Q., 2019, P BRIT MACH VIS C, P1
   Li Wenbo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P335, DOI 10.1007/978-3-030-58607-2_20
   Li Z, 2019, PROC CVPR IEEE, P3862, DOI 10.1109/CVPR.2019.00399
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Lin HW, 2019, IEEE T MULTIMEDIA, V21, P3010, DOI 10.1109/TMM.2019.2919433
   Lin J., 2021, arXiv
   Liu C, 2014, IEEE T PATTERN ANAL, V36, P346, DOI 10.1109/TPAMI.2013.127
   Lo S. Y., 2019, P ACM MULTIMEDIA ASI, P1, DOI DOI 10.1145/3338533.3366558
   Lu M, 2023, IEEE T MULTIMEDIA, V25, P2097, DOI 10.1109/TMM.2022.3142414
   Luo ZX, 2023, IEEE T MULTIMEDIA, V25, P2788, DOI 10.1109/TMM.2022.3151259
   Mi L., 2018, P EUROPEAN C COMPUTE, P322
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2011, CONF REC ASILOMAR C, P723, DOI 10.1109/ACSSC.2011.6190099
   Nan XM, 2017, IEEE T CIRC SYST VID, V27, P2687, DOI 10.1109/TCSVT.2016.2595330
   Pengxu Wei, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P101, DOI 10.1007/978-3-030-58598-3_7
   Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Son S, 2021, IEEE COMPUT SOC CONF, P166, DOI 10.1109/CVPRW53098.2021.00026
   Song DH, 2020, AAAI CONF ARTIF INTE, V34, P12007
   Sun HM, 2020, IEEE T MULTIMEDIA, V22, P2764, DOI 10.1109/TMM.2019.2963620
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5
   Wang H, 2023, IEEE T MULTIMEDIA, V25, P4742, DOI 10.1109/TMM.2022.3181458
   Wang XT, 2019, IEEE COMPUT SOC CONF, P1954, DOI 10.1109/CVPRW.2019.00247
   Wonkyung Lee, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P465, DOI 10.1007/978-3-030-58586-0_28
   Xiangxiang Chu, 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12538), P99, DOI 10.1007/978-3-030-66823-5_6
   Xiao J, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4408, DOI 10.1145/3474085.3475588
   Xiao ZY, 2021, PROC CVPR IEEE, P2113, DOI 10.1109/CVPR46437.2021.00215
   Xie LB, 2021, ADV NEUR IN, V34
   Xiong W, 2018, PROC CVPR IEEE, P2364, DOI 10.1109/CVPR.2018.00251
   Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2
   Yi P, 2019, IEEE I CONF COMP VIS, P3106, DOI 10.1109/ICCV.2019.00320
   Zhang XD, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4034, DOI 10.1145/3474085.3475291
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhao H, 2020, COMPUTER VISION ECCV, P56, DOI DOI 10.1007/978-3-030-67070-23
NR 60
TC 9
Z9 9
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8972
EP 8987
DI 10.1109/TMM.2023.3243615
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP5R1
UT WOS:001133278300009
DA 2024-07-18
ER

PT J
AU Xu, YH
   Bin, Y
   Wei, JW
   Yang, Y
   Wang, GQ
   Shen, HT
AF Xu, Yahui
   Bin, Yi
   Wei, Jiwei
   Yang, Yang
   Wang, Guoqing
   Shen, Heng Tao
TI Multi-Modal Transformer With Global-Local Alignment for Composed Query
   Image Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Transformers; Image retrieval; Visualization; Task analysis; Feature
   extraction; Bit error rate; Fuses; Transformer; composed query image
   retrieval; local alignment; spatial attention; multi-modal learning
ID NETWORK
AB In this paper, we study the composed query image retrieval, which aims at retrieving the target image similar to the composed query, i.e., a reference image and the desired modification text. Compared with conventional image retrieval, this task is more challenging as it not only requires precisely aligning the composed query and target image in a common embedding space, but also simultaneously extracting related information from the reference image and modification text. In order to properly extract related information from the composed query, existing methods usually embed vision-language inputs using different feature encoders, e.g., CNN for images and LSTM/BERT for text, and then employ a complicated manually-designed composition module for learning the joint image-text representation. However, the architecture discrepancy in feature encoders would restrict the vision-language plenitudinous interaction. Meanwhile, certain complicated composition designs might significantly hamper the generalization ability of the model. To tackle these problems, we propose a new framework termed ComqueryFormer, which effectively processes the composed query with the Transformer for this task. Specifically, to eliminate the architecture discrepancy, we leverage a unified transformer-based architecture to homogeneously encode the vision-language inputs. Meanwhile, instead of the complicated composition module, the neat yet effective cross-modal transformer is adopted to hierarchically fuse the composed query at various vision scales. On the other hand, we introduce an efficient global-local alignment module to narrow the distance between the composed query and the target image. It not only considers the divergence in the global joint embedding space but also forces the model to focus on the local detail differences. Extensive experiments on three real-world datasets demonstrate the superiority of our ComqueryFormer.
C1 [Xu, Yahui; Bin, Yi; Wei, Jiwei; Yang, Yang; Wang, Guoqing] Univ Elect Sci & Technol China, Ctr Future Media, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
   [Yang, Yang] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
   [Yang, Yang] Univ Elect Sci & Technol China, Inst Elect & Informat Engn, Chengdu 523808, Guangdong, Peoples R China.
   [Shen, Heng Tao] Univ Elect Sci & Technol China, Ctr Future Multimedia, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
   [Shen, Heng Tao] Peng Cheng Lab, Shenzhen 518066, Peoples R China.
C3 University of Electronic Science & Technology of China; University of
   Electronic Science & Technology of China; University of Electronic
   Science & Technology of China; University of Electronic Science &
   Technology of China; Peng Cheng Laboratory
RP Yang, Y (corresponding author), Univ Elect Sci & Technol China, Ctr Future Media, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.; Yang, Y (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.; Yang, Y (corresponding author), Univ Elect Sci & Technol China, Inst Elect & Informat Engn, Chengdu 523808, Guangdong, Peoples R China.
EM yahui2727@163.com; yi.bin@hotmail.com; mathematic6@gmail.com;
   dlyyang@gmail.com; gqwang0420@hotmail.com; shenhengtao@hotmail.com
RI Shen, Heng Tao/ABD-5331-2021; Xu, Yahui/KLC-2939-2024
OI Xu, Yahui/0000-0002-1123-6129; Wei, Jiwei/0000-0003-3912-1742
FU National Natural Science Foundation of China
FX No Statement Available
CR Anwaar MU, 2021, IEEE WINT CONF APPL, P1139, DOI 10.1109/WACV48630.2021.00118
   Bhunia Ayan Kumar, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9776, DOI 10.1109/CVPR42600.2020.00980
   Bin Y, 2022, IEEE T CIRC SYST VID, V32, P52, DOI 10.1109/TCSVT.2021.3063297
   Bin Y, 2019, IEEE T CYBERNETICS, V49, P2631, DOI 10.1109/TCYB.2018.2831447
   Chen YB, 2020, PROC CVPR IEEE, P2998, DOI 10.1109/CVPR42600.2020.00307
   Chun YD, 2008, IEEE T MULTIMEDIA, V10, P1073, DOI 10.1109/TMM.2008.2001357
   Delmas G., 2022, P INT C LEARN REPR, P1
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dodds E, 2020, Arxiv, DOI arXiv:2007.00145
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Gao X, 2017, IEEE INT CON MULTI, P127, DOI 10.1109/ICME.2017.8019306
   Goenka S, 2022, PROC CVPR IEEE, P14085, DOI 10.1109/CVPR52688.2022.01371
   Gu CB, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4600, DOI 10.1145/3474085.3475619
   Han XT, 2017, IEEE I CONF COMP VIS, P1472, DOI 10.1109/ICCV.2017.163
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He SY, 2020, IEEE T CYBERNETICS, V50, P4157, DOI 10.1109/TCYB.2019.2941284
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hosseinzadeh M, 2020, PROC CVPR IEEE, P3593, DOI 10.1109/CVPR42600.2020.00365
   Huang X, 2019, PROCEEDINGS OF THE TWELFTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'19), P105, DOI 10.1145/3289600.3290956
   Jandial S, 2022, IEEE WINT CONF APPL, P597, DOI 10.1109/WACV51458.2022.00067
   Jiang Z, 2020, IEEE T MULTIMEDIA, V22, P540, DOI 10.1109/TMM.2019.2929957
   Jin Z, 2020, IEEE T MULTIMEDIA, V22, P1055, DOI 10.1109/TMM.2019.2938340
   Jiwei Wei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13002, DOI 10.1109/CVPR42600.2020.01302
   Kasai J., 2020, PR MACH LEARN RES
   Kim J, 2021, AAAI CONF ARTIF INTE, V35, P1771
   Kim W, 2021, PR MACH LEARN RES, V139
   Kingma D, 2014, ICLR P, V2014, P1
   Lee S, 2021, PROC CVPR IEEE, P802, DOI 10.1109/CVPR46437.2021.00086
   Lei JJ, 2022, IEEE T IMAGE PROCESS, V31, P6707, DOI 10.1109/TIP.2022.3203213
   Li ZH, 2018, IEEE T NEUR NET LEAR, V29, P6323, DOI 10.1109/TNNLS.2018.2829867
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu ZY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2105, DOI 10.1109/ICCV48922.2021.00213
   Ma J, 2020, IEEE WINT CONF APPL, P2492, DOI [10.1109/wacv45572.2020.9093427, 10.1109/WACV45572.2020.9093427]
   Ma XH, 2020, IEEE T MULTIMEDIA, V22, P3101, DOI 10.1109/TMM.2020.2969792
   Ma ZY, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P1279, DOI 10.1145/3503161.3548165
   Ma Z, 2020, AAAI CONF ARTIF INTE, V34, P11741
   Qian SS, 2021, IEEE T MULTIMEDIA, V24, P3520, DOI 10.1109/TMM.2021.3101642
   Qian TW, 2023, IEEE T MULTIMEDIA, V25, P3950, DOI 10.1109/TMM.2022.3169065
   Qiao YY, 2021, IEEE T MULTIMEDIA, V23, P4426, DOI 10.1109/TMM.2020.3042066
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ryoo MS, 2021, ADV NEUR IN
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Shen FM, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1522, DOI 10.1145/3123266.3123345
   Shukor M, 2022, IEEE COMPUT SOC CONF, P4566, DOI 10.1109/CVPRW56347.2022.00503
   Suo W., 2021, P 30 INT JOINT C ART, P1032, DOI [10.24963/ijcai.2021/143, DOI 10.24963/IJCAI.2021/143]
   Tan H, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5100
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Vaswani A, 2017, ADV NEUR IN, V30
   Vo N, 2019, PROC CVPR IEEE, P6432, DOI 10.1109/CVPR.2019.00660
   Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326
   Wang JW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2548, DOI 10.1145/3343031.3356059
   Wang K, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, DOI 10.1145/3503161.3548382
   Wang YC, 2021, IEEE T MULTIMEDIA, V24, P3276, DOI 10.1109/TMM.2021.3096087
   Wu H, 2021, PROC CVPR IEEE, P11302, DOI 10.1109/CVPR46437.2021.01115
   Xu Y., 2021, P ACM MULT AS, P1
   Xu YH, 2017, IEEE INT CON MULTI, P133, DOI 10.1109/ICME.2017.8019425
   Xue H., 2021, Adv. Neural Inf. Process. Syst., V34, P4514
   Yang YC, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3303, DOI 10.1145/3474085.3475483
   Yao L, 2019, AAAI CONF ARTIF INTE, P7370
   Zhang FF, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3367, DOI 10.1145/3394171.3413917
   Zhang GJ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5353, DOI 10.1145/3474085.3475659
   Zhang MX, 2021, PROC CVPR IEEE, P12664, DOI 10.1109/CVPR46437.2021.01248
   Zheng ZQ, 2023, IEEE T MULTIMEDIA, V25, P2474, DOI 10.1109/TMM.2022.3147425
   Zhou RW, 2020, IEEE T NEUR NET LEAR, V31, P1592, DOI 10.1109/TNNLS.2019.2920905
   Zhou WA, 2017, Arxiv, DOI arXiv:1706.06064
NR 66
TC 7
Z9 7
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8346
EP 8357
DI 10.1109/TMM.2023.3235495
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000017
DA 2024-07-18
ER

PT J
AU Zhang, K
   Yuan, C
   Zhu, YM
   Jiang, Y
   Luo, LS
AF Zhang, Ke
   Yuan, Chun
   Zhu, Yiming
   Jiang, Yong
   Luo, Lishu
TI Weakly Supervised Instance Segmentation by Exploring Entire Object
   Regions
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image segmentation; Semantics; Task analysis; Streaming media; Location
   awareness; Saliency detection; Training; Weakly supervised learning;
   pseudo instance segmentation labels; two-stream network; center
   detection; integration module
AB Weakly supervised instance segmentation with image-level class supervision is a challenging task as it associates the highest-level instances to the lowest-level appearance. Previous approaches for the task utilize classification networks to obtain rough discriminative parts as seed regions and use distance as a metric to cluster pixels of the same instances. Unlike previous approaches, we provide a novel self-supervised joint learning framework as the basic network and consider the clustering problem as calculating the probability that pixels belong to each instance. To this end, we propose our self-supervised joint learning two-stream network (SJLT Net) to finish this task. In the first stream, we leverage a joint learning framework to implement image-level supervised semantic segmentation with self-supervised saliency detection. In the second stream, we propose a Center Detection Network to detect different instances' centers with the gaussian loss function to cluster instances pixels. Besides, an integration module is utilized to combine information of both streams and get precise pseudo instances labels. Our approach generates pseudo instance segmentation labels of training images, which are used to train a fully supervised model. Our model achieves excellent performance on the PASCAL VOC 2012 dataset, surpassing the best baseline trained with the same labels by 4.6$\%$ $AP<^>r_{50}$ on the train set and 2.6$\%$ $AP<^>r_{50}$ on the validation set.
C1 [Zhang, Ke; Luo, Lishu] Tsinghua Univ, Tsinghua Berkeley Shenzhen Instiute, Shenzhen 518055, Peoples R China.
   [Yuan, Chun; Jiang, Yong] Tsinghua Univ, Tsinghua Shenzhen Int Grad Sch, Shenzhen 518055, Peoples R China.
   [Zhu, Yiming] Tsinghua Univ, Master Engn Artificial Intelligence, Shenzhen 518055, Peoples R China.
C3 Tsinghua University; Tsinghua University; Tsinghua Shenzhen
   International Graduate School; Tsinghua University
RP Yuan, C (corresponding author), Tsinghua Univ, Tsinghua Shenzhen Int Grad Sch, Shenzhen 518055, Peoples R China.
EM ke-zhang19@mails.tsinghua.edu.cn; yuanc@sz.tsinghua.edu.cn;
   zym20@mails.tsinghua.edu.cn; jiangy@sz.tsinghua.edu.cn;
   lls18@mails.tsinghua.edu.cn
CR Ahn J, 2019, PROC CVPR IEEE, P2204, DOI 10.1109/CVPR.2019.00231
   Ahn J, 2018, PROC CVPR IEEE, P4981, DOI 10.1109/CVPR.2018.00523
   Araslanov N, 2020, PROC CVPR IEEE, P4252, DOI 10.1109/CVPR42600.2020.00431
   Arnab A, 2017, PROC CVPR IEEE, P879, DOI 10.1109/CVPR.2017.100
   Arun Aditya, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P254, DOI 10.1007/978-3-030-58604-1_16
   Borji A, 2015, IEEE T IMAGE PROCESS, V24, P5706, DOI 10.1109/TIP.2015.2487833
   Chen Hao, 2020, 2020 IEEE C COMP VIS, DOI [DOI 10.48550/ARXIV.2001.00309, DOI 10.1109/CVPR42600.2020.00860]
   Chen K, 2019, PROC CVPR IEEE, P4969, DOI 10.1109/CVPR.2019.00511
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen YY, 2020, IEEE T MULTIMEDIA, V22, P1985, DOI 10.1109/TMM.2019.2952984
   Choe J, 2020, PROC CVPR IEEE, P3130, DOI 10.1109/CVPR42600.2020.00320
   Choe J, 2019, PROC CVPR IEEE, P2214, DOI 10.1109/CVPR.2019.00232
   Dai JF, 2015, IEEE I CONF COMP VIS, P1635, DOI 10.1109/ICCV.2015.191
   Doersch C, 2017, IEEE I CONF COMP VIS, P2070, DOI 10.1109/ICCV.2017.226
   Enze Xie, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12190, DOI 10.1109/CVPR42600.2020.01221
   Fan JS, 2020, AAAI CONF ARTIF INTE, V34, P10762
   Gao NY, 2019, IEEE I CONF COMP VIS, P642, DOI 10.1109/ICCV.2019.00073
   Ge WF, 2019, IEEE I CONF COMP VIS, P3344, DOI 10.1109/ICCV.2019.00344
   Guolei Sun, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P347, DOI 10.1007/978-3-030-58536-5_21
   Hariharan B, 2014, LECT NOTES COMPUT SC, V8695, P297, DOI 10.1007/978-3-319-10584-0_20
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   Hong S, 2017, PROC CVPR IEEE, P2224, DOI 10.1109/CVPR.2017.239
   Hsu CC, 2019, ADV NEUR IN, V32
   Huang ZL, 2018, PROC CVPR IEEE, P7014, DOI 10.1109/CVPR.2018.00733
   Inoue N, 2018, PROC CVPR IEEE, P5001, DOI 10.1109/CVPR.2018.00525
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Khoreva A, 2017, PROC CVPR IEEE, P1665, DOI 10.1109/CVPR.2017.181
   Kolesnikov A, 2019, PROC CVPR IEEE, P1920, DOI 10.1109/CVPR.2019.00202
   Kolesnikov A, 2016, LECT NOTES COMPUT SC, V9908, P695, DOI 10.1007/978-3-319-46493-0_42
   KRAHENBUHL P, 2011, ADV NEURAL INFORM PR, P109, DOI DOI 10.5555/2986459.2986472
   Lee GY, 2016, PROC CVPR IEEE, P660, DOI 10.1109/CVPR.2016.78
   Li GB, 2018, Arxiv, DOI arXiv:1803.06503
   Li XY, 2021, AAAI CONF ARTIF INTE, V35, P1984
   Liao SS, 2019, INT CONF ACOUST SPEE, P1917, DOI 10.1109/ICASSP.2019.8682309
   Lin D, 2016, PROC CVPR IEEE, P3159, DOI 10.1109/CVPR.2016.344
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu YD, 2018, LECT NOTES COMPUT SC, V11207, P708, DOI 10.1007/978-3-030-01219-9_42
   Nawaz M, 2021, IEEE T MULTIMEDIA, V23, P2902, DOI 10.1109/TMM.2020.3019688
   Neven D, 2019, PROC CVPR IEEE, P8829, DOI 10.1109/CVPR.2019.00904
   Oh SJ, 2017, PROC CVPR IEEE, P5038, DOI 10.1109/CVPR.2017.535
   Oquab M, 2015, PROC CVPR IEEE, P685, DOI 10.1109/CVPR.2015.7298668
   Pinheiro PO, 2015, PROC CVPR IEEE, P1713, DOI 10.1109/CVPR.2015.7298780
   Ren MY, 2017, PROC CVPR IEEE, P293, DOI 10.1109/CVPR.2017.39
   Roy A, 2017, PROC CVPR IEEE, P7282, DOI 10.1109/CVPR.2017.770
   Saleh F, 2016, LECT NOTES COMPUT SC, V9912, P413, DOI 10.1007/978-3-319-46484-8_25
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shimoda W, 2019, IEEE I CONF COMP VIS, P5207, DOI 10.1109/ICCV.2019.00531
   Sun YQ, 2020, IEEE ACCESS, V8, P24135, DOI 10.1109/ACCESS.2020.2969480
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Tianyi Zhang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P663, DOI 10.1007/978-3-030-58542-6_40
   Wang X, 2020, INT J COMPUT VISION, V128, P1736, DOI 10.1007/s11263-020-01293-3
   Wang X, 2018, PROC CVPR IEEE, P1354, DOI 10.1109/CVPR.2018.00147
   Wei YC, 2017, PROC CVPR IEEE, P6488, DOI 10.1109/CVPR.2017.687
   Wei YC, 2018, PROC CVPR IEEE, P7268, DOI 10.1109/CVPR.2018.00759
   Wei YC, 2017, IEEE T PATTERN ANAL, V39, P2314, DOI 10.1109/TPAMI.2016.2636150
   Xu K, 2021, IEEE T MULTIMEDIA, V23, P3530, DOI 10.1109/TMM.2020.3026913
   Xue HL, 2019, IEEE I CONF COMP VIS, P6588, DOI 10.1109/ICCV.2019.00669
   Yan Y, 2020, IEEE T MULTIMEDIA, V22, P2792, DOI 10.1109/TMM.2019.2962317
   Ye M., 2020, ADV NEUR IN, V33
   Yude Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12272, DOI 10.1109/CVPR42600.2020.01229
   Zeng Y, 2019, IEEE I CONF COMP VIS, P7222, DOI 10.1109/ICCV.2019.00732
   Zhai XH, 2019, IEEE I CONF COMP VIS, P1476, DOI 10.1109/ICCV.2019.00156
   Zhang HM, 2021, IEEE T MULTIMEDIA, V23, P2033, DOI 10.1109/TMM.2020.3007352
   Zhang TY, 2019, IEEE T MULTIMEDIA, V21, P2930, DOI 10.1109/TMM.2019.2914870
   Zhang X, 2021, AUTOPHAGY, V17, P1519, DOI 10.1080/15548627.2020.1840796
   Zhang XL, 2018, PROC CVPR IEEE, P1325, DOI 10.1109/CVPR.2018.00144
   Zhang XL, 2018, LECT NOTES COMPUT SC, V11216, P610, DOI 10.1007/978-3-030-01258-8_37
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhou YZ, 2018, PROC CVPR IEEE, P3791, DOI 10.1109/CVPR.2018.00399
   Zhu Y, 2017, IEEE I CONF COMP VIS, P1859, DOI 10.1109/ICCV.2017.204
NR 70
TC 6
Z9 6
U1 5
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 352
EP 363
DI 10.1109/TMM.2021.3126430
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 9B8PL
UT WOS:000934993800002
DA 2024-07-18
ER

PT J
AU Zhao, DW
   Gao, QW
   Lu, YX
   Sun, D
AF Zhao, Dawei
   Gao, Qingwei
   Lu, Yixiang
   Sun, Dong
TI Non-Aligned Multi-View Multi-Label Classification via Learning
   View-Specific Labels
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Correlation; Face recognition; Feature extraction; Task analysis;
   Semantics; Predictive models; Optimization; Multi-view multi-label
   learning; Non-aligned view; View-specific labels learning; Low-rank
   label correlations; Manifold regularization learning; Kernel extension
ID FEATURE-SELECTION; MISSING LABELS; MODEL
AB In the multi-view multi-label (MVML) classification problem, multiple views are simultaneously associated with multiple semantic representations. Multi-view multi-label learning inevitably has the problems of consistency, diversity, and non-alignment among views and the correlation among labels. Most of the existing multi-view multi-label methods for non-aligned views assume that each view has a common or shared label set, but because a single view cannot contain the entire label information, they often learn suboptimal results. Based on this, this paper proposes a non-aligned multi-view multi-label classification method that learns view-specific labels (LVSL), aiming to explicitly mine the information of view-specific labels and low-rank label structures in non-aligned views in a unified model framework. Furthermore, to alleviate insufficient available label information, we thoroughly explored the global and local structural information among labels. Specifically, first, we assume that there is structural consistency between the view and the label space and then construct the view-specific label model in turn. Second, to enrich the original label space information, we mine the consistent information of multiple views and the low-rank correlation information hidden among multiple labels. Finally, the contribution weight of each view is combined with learning the complementary information among the views in the decision-making stage, and extend the model to handle nonlinear data. The results of the proposed method compared with existing state-of-the-art algorithms on several datasets validate its effectiveness.
C1 [Zhao, Dawei] Anhui Univ, Sch Elect Engn & Automat, Hefei 230601, Peoples R China.
   [Zhao, Dawei] Anhui Univ, Sch Comp & Technol, Hefei 230601, Peoples R China.
   [Gao, Qingwei; Lu, Yixiang; Sun, Dong] Anhui Univ, Sch Elect Engn & Automat, Minist Educ, Key Lab Intelligent Comp & Signal Proc, Hefei 230601, Peoples R China.
C3 Anhui University; Anhui University; Anhui University
RP Gao, QW (corresponding author), Anhui Univ, Sch Elect Engn & Automat, Minist Educ, Key Lab Intelligent Comp & Signal Proc, Hefei 230601, Peoples R China.
EM zhaodwahu@163.com; qingweigao@ahu.edu.cn; lyxahu@ahu.edu.cn;
   sundong@ahu.edu.cn
FU Nature Science Foundation of Anhui [2008085MF183, 2008085MF192];
   National Natural Science Foundation of China (NSFC) [62071001, 61502003]
FX This work was supported in part by the Nature Science Foundation of
   Anhui under Grants 2008085MF183 and 2008085MF192 and in part by the
   National Natural Science Foundation of China (NSFC) under Grants
   62071001 and 61502003. The Associate Editor coordinating the review of
   this manuscript and approving it for publication was Prof.Ngai-Man
   Cheung.
CR Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Cao XC, 2015, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2015.7298657
   Chen ZS, 2020, AAAI CONF ARTIF INTE, V34, P3553
   Da'u A, 2020, ARTIF INTELL REV, V53, P2709, DOI 10.1007/s10462-019-09744-1
   Demsar J, 2006, J MACH LEARN RES, V7, P1
   Fang Z, 2012, IEEE DATA MINING, P864, DOI 10.1109/ICDM.2012.88
   Feng L, 2022, IEEE T CYBERNETICS, V52, P3710, DOI 10.1109/TCYB.2020.3016897
   Gao HC, 2015, IEEE I CONF COMP VIS, P4238, DOI 10.1109/ICCV.2015.482
   Gibaja E, 2015, ACM COMPUT SURV, V47, DOI 10.1145/2716262
   Gretton A, 2005, LECT NOTES ARTIF INT, V3734, P63
   Guillaumin M, 2010, PROC CVPR IEEE, P902, DOI 10.1109/CVPR.2010.5540120
   Guo BL, 2018, INT C PATT RECOG, P417, DOI 10.1109/ICPR.2018.8545526
   He ZF, 2019, KNOWL-BASED SYST, V163, P145, DOI 10.1016/j.knosys.2018.08.018
   Huang J., 2015, PROC IEEE INT C MULT, P1
   Huang J, 2019, IEEE ACCESS, V7, P100979, DOI 10.1109/ACCESS.2019.2930468
   Huang J, 2019, INFORM SCIENCES, V492, P124, DOI 10.1016/j.ins.2019.04.021
   Huang S.-J., 2012, Association for the Advancement of Artificial Intelligence, P949, DOI DOI 10.5555/2900728.2900863
   Kim BK, 2016, IEEE COMPUT SOC CONF, P1499, DOI 10.1109/CVPRW.2016.187
   Li X, 2022, IEEE T PATTERN ANAL, V44, P5918, DOI 10.1109/TPAMI.2021.3086895
   Li YM, 2019, IEEE T KNOWL DATA EN, V31, P1863, DOI 10.1109/TKDE.2018.2872063
   Liang YW, 2019, IEEE DATA MINING, P1204, DOI 10.1109/ICDM.2019.00148
   Lin Z., 2011, ADV NEURAL INFORM PR, V24, P612, DOI DOI 10.1007/S11263-013-0611-6
   Liu M, 2015, AAAI CONF ARTIF INTE, P2778
   Liu XW, 2019, AAAI CONF ARTIF INTE, P4400
   Luo Y, 2015, IEEE T IMAGE PROCESS, V24, P2355, DOI 10.1109/TIP.2015.2421309
   Ma ZC, 2021, PATTERN RECOGN, V111, DOI 10.1016/j.patcog.2020.107675
   O'Mahony N, 2020, ADV INTELL SYST COMP, V943, P128, DOI 10.1007/978-3-030-17795-9_10
   Pizzuti C, 2009, PROC INT C TOOLS ART, P379
   Read J, 2011, MACH LEARN, V85, P333, DOI 10.1007/s10994-011-5256-5
   Ren WJ, 2017, LECT NOTES ARTIF INT, V10412, P543, DOI 10.1007/978-3-319-63558-3_46
   Shen JD, 2020, IEEE INT C BIOINFORM, P1124, DOI 10.1109/BIBM49941.2020.9313285
   Sun SL, 2021, IEEE T PATTERN ANAL, V43, P2682, DOI 10.1109/TPAMI.2020.2974203
   Tan QY, 2021, IEEE T CYBERNETICS, V51, P1716, DOI 10.1109/TCYB.2019.2950560
   Tan QY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2703
   Tsoumakas G., 2007, INT J DATA WAREHOUS, V3, P1, DOI DOI 10.4018/JDWM.2007070101
   Tsoumakas G, 2011, J MACH LEARN RES, V12, P2411
   Tsoumakas G, 2010, DATA MINING AND KNOWLEDGE DISCOVERY HANDBOOK, SECOND EDITION, P667, DOI 10.1007/978-0-387-09823-4_34
   Wang SP, 2015, PATTERN RECOGN, V48, P10, DOI 10.1016/j.patcog.2014.08.004
   Wen J, 2021, IEEE T MULTIMEDIA, V23, P2493, DOI 10.1109/TMM.2020.3013408
   Wu X, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3884
   Xu LL, 2014, IEEE DATA MINING, P1067, DOI 10.1109/ICDM.2014.125
   Yu ZB, 2022, IEEE T PATTERN ANAL, V44, P5199, DOI 10.1109/TPAMI.2021.3070215
   Zhang CQ, 2018, AAAI CONF ARTIF INTE, P4414
   Zhang FW, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2369
   Zhang J, 2019, APPL SOFT COMPUT, V76, P425, DOI 10.1016/j.asoc.2018.12.016
   Zhang JJ, 2018, IEEE T MULTIMEDIA, V20, P2801, DOI 10.1109/TMM.2018.2812605
   Zhang ML, 2007, PATTERN RECOGN, V40, P2038, DOI 10.1016/j.patcog.2006.12.019
   Zhang ML, 2015, IEEE T PATTERN ANAL, V37, P107, DOI 10.1109/TPAMI.2014.2339815
   Zhang ML, 2014, IEEE T KNOWL DATA EN, V26, P1819, DOI 10.1109/TKDE.2013.39
   Zhang TJ, 2023, IEEE T MULTIMEDIA, V25, P993, DOI 10.1109/TMM.2021.3136094
   Zhang XY, 2009, IEEE INT CON MULTI, P258, DOI 10.1109/ICME.2009.5202484
   Zhang YS, 2020, IEEE T MULTIMEDIA, V22, P2844, DOI 10.1109/TMM.2020.2966887
   Zhao DW, 2021, KNOWL-BASED SYST, V218, DOI 10.1016/j.knosys.2021.106841
   Zhao DW, 2021, APPL SOFT COMPUT, V102, DOI 10.1016/j.asoc.2021.107120
   Zhao J, 2017, INFORM FUSION, V38, P43, DOI 10.1016/j.inffus.2017.02.007
   Zhu CM, 2020, NEUROCOMPUTING, V371, P67, DOI 10.1016/j.neucom.2019.09.009
   Zhu PF, 2018, PATTERN RECOGN, V84, P126, DOI 10.1016/j.patcog.2018.07.009
   Zhu XF, 2016, IEEE T CYBERNETICS, V46, P450, DOI 10.1109/TCYB.2015.2403356
   Zhu Y, 2018, IEEE T KNOWL DATA EN, V30, P1081, DOI 10.1109/TKDE.2017.2785795
NR 59
TC 8
Z9 8
U1 9
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7235
EP 7247
DI 10.1109/TMM.2022.3219650
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000038
OA hybrid
DA 2024-07-18
ER

PT J
AU Zheng, LR
   Li, YS
   Zhang, KH
   Luo, WH
AF Zheng, Lirong
   Li, Yanshan
   Zhang, Kaihao
   Luo, Wenhan
TI T-Net: Deep Stacked Scale-Iteration Network for Image Dehazing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image dehazing; Multi-scale; Dual attention; Recurrent structure;
   Recursive strategy
ID FUSION NETWORK; REMOVAL; VISION
AB Haze reduces the visibility of image content and leads to failure in handling subsequent computer vision tasks. In this paper, we address the problem of single image dehazing by proposing a dehazing network named T-Net, which consists of a backbone network based on the U-Net architecture and a dual attention module. Multi-scale feature fusion can be achieved by using skip connections with a new fusion strategy. Furthermore, by repeatedly unfolding the plain T-Net, Stack T-Net is proposed to take advantage of the dependence of deep features across stages via a recursive strategy. To reduce network parameters, the intra-stage recursive computation of ResNet is adopted in our Stack T-Net. We take both the stage-wise result and the original hazy image as input to each T-Net and finally output the prediction of the clean image. Experimental results on both synthetic and real-world images demonstrate that our plain T-Net and the advanced Stack T-Net perform favorably against state-of-the-art dehazing algorithms and show that our Stack T-Net could further improve the dehazing effect, demonstrating the effectiveness of the recursive strategy.
C1 [Zheng, Lirong; Li, Yanshan] Shenzhen Univ, ATR Natl Key Lab Def Technol, Shenzhen 518060, Peoples R China.
   [Zheng, Lirong; Li, Yanshan] Shenzhen Univ, Guangdong Key Lab Intelligent Informat Proc, Shenzhen 518060, Peoples R China.
   [Zhang, Kaihao] Australian Natl Univ, Coll Engn & Comp Sci, Canberra, ACT 2601, Australia.
   [Luo, Wenhan] Sun Yat Sen Univ, Guangzhou 510275, Peoples R China.
C3 Shenzhen University; Shenzhen University; Australian National
   University; Sun Yat Sen University
RP Li, YS (corresponding author), Shenzhen Univ, ATR Natl Key Lab Def Technol, Shenzhen 518060, Peoples R China.; Li, YS (corresponding author), Shenzhen Univ, Guangdong Key Lab Intelligent Informat Proc, Shenzhen 518060, Peoples R China.
EM zhenglirong2021@email.szu.edu.cn; lys@szu.edu.cn;
   super.khzhang@gmail.com; whluo.china@gmail.com
RI Zheng, Lirong/JDN-3637-2023; Li, Yanshan/GVS-5245-2022; Zhang,
   Kaihao/HGC-0368-2022; Luo, Wenhan/GZL-0535-2022
OI Zheng, Lirong/0000-0002-8895-6351; Luo, Wenhan/0000-0002-5697-4168
FU National Natural Science Foundation of China [61771319, 61871154,
   2019A1515011307]; Natural Science Foundation of Guangdong Province
   [20200826154022001]; Shenzhen Science and Technology Project
   [JCYJ20180507182259896, 2020KCXTD004, WDZC20195500201];  [62076165]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61771319, 62076165, and 61871154, in
   part by the Natural Science Foundation of Guangdong Province under Grant
   2019A1515011307, in part by Shenzhen Science and Technology Project
   under Grants 20200826154022001 and JCYJ20180507182259896 and in part by
   other Project under Grants 2020KCXTD004 and WDZC20195500201.
CR Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Bui TM, 2018, IEEE T IMAGE PROCESS, V27, P999, DOI 10.1109/TIP.2017.2771158
   Busch C, 1998, IEEE INTELL SYST APP, V13, P66, DOI 10.1109/5254.736004
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chen WT, 2019, PROC CVPR IEEE, P11673, DOI 10.1109/CVPR.2019.01195
   Do T. D., 2020, P INT C MULT AN PATT, P1
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dong H, 2020, PROC CVPR IEEE, P2154, DOI 10.1109/CVPR42600.2020.00223
   Du YX, 2018, Arxiv, DOI arXiv:1805.01084
   Dudhane A, 2020, IEEE T IMAGE PROCESS, V29, P628, DOI 10.1109/TIP.2019.2934360
   Engin D, 2018, IEEE COMPUT SOC CONF, P938, DOI 10.1109/CVPRW.2018.00127
   Fang FM, 2020, IEEE T MULTIMEDIA, V22, P2537, DOI 10.1109/TMM.2019.2958755
   Fattal R, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2651362
   Fattal R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360671
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Golts A, 2020, IEEE T IMAGE PROCESS, V29, P2692, DOI 10.1109/TIP.2019.2952032
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Hu HM, 2020, IEEE T MULTIMEDIA, V22, P1485, DOI 10.1109/TMM.2019.2944260
   Huang YM, 2018, IEEE IMAGE PROC, P2840, DOI 10.1109/ICIP.2018.8451663
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P451, DOI 10.1109/83.557356
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kaihao Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P71, DOI 10.1007/978-3-030-58583-9_5
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kim TK, 1998, IEEE T CONSUM ELECTR, V44, P82, DOI 10.1109/30.663733
   Kingma D, 2014, ICLR P, V2014, P1
   Kupyn O, 2019, IEEE I CONF COMP VIS, P8877, DOI 10.1109/ICCV.2019.00897
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li CY, 2020, IEEE T MULTIMEDIA, V22, P704, DOI 10.1109/TMM.2019.2933334
   Li X, 2018, LECT NOTES COMPUT SC, V11211, P262, DOI 10.1007/978-3-030-01234-2_16
   Li Y, 2017, COMPUT VIS IMAGE UND, V165, P1, DOI 10.1016/j.cviu.2017.09.003
   Li YA, 2019, IEEE I CONF COMP VIS, P3275, DOI 10.1109/ICCV.2019.00337
   Li ZG, 2018, IEEE T IMAGE PROCESS, V27, P442, DOI 10.1109/TIP.2017.2750418
   Li ZG, 2015, IEEE T IMAGE PROCESS, V24, P5432, DOI 10.1109/TIP.2015.2482903
   Li ZG, 2015, IEEE T IMAGE PROCESS, V24, P120, DOI 10.1109/TIP.2014.2371234
   Liang CH, 2022, IEEE T MULTIMEDIA, V24, P61, DOI 10.1109/TMM.2020.3045303
   Liu XH, 2019, IEEE I CONF COMP VIS, P7313, DOI 10.1109/ICCV.2019.00741
   Liu YF, 2018, IEEE T IMAGE PROCESS, V27, P3064, DOI 10.1109/TIP.2018.2806202
   McCartney E.J., 1976, Optics of the atmosphere: Scattering by molecules and particles, P421
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35
   Narasimhan SG, 2002, INT J COMPUT VISION, V48, P233, DOI 10.1023/A:1016328200723
   Omer I., 2004, IEEE COMPUT SOC C CO, V2, pII
   Papyan V, 2016, IEEE T IMAGE PROCESS, V25, P249, DOI 10.1109/TIP.2015.2499698
   Paszke Adam, 2017, NIPS W
   Qu YY, 2019, PROC CVPR IEEE, P8152, DOI 10.1109/CVPR.2019.00835
   Que Y, 2021, IEEE T MULTIMEDIA, V23, P3059, DOI 10.1109/TMM.2020.3019680
   Raj NB, 2020, 2020 INTERNATIONAL CONFERENCE ON WIRELESS COMMUNICATIONS SIGNAL PROCESSING AND NETWORKING (WISPNET), P37, DOI [10.1109/WiSPNET48689.2020.9198400, 10.1109/wispnet48689.2020.9198400]
   Ren DW, 2019, PROC CVPR IEEE, P3932, DOI 10.1109/CVPR.2019.00406
   Ren WQ, 2018, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2018.00343
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Schechner YY, 2001, PROC CVPR IEEE, P325
   Schechner YY, 2003, APPL OPTICS, V42, P511, DOI 10.1364/AO.42.000511
   Shao YJ, 2020, PROC CVPR IEEE, P2805, DOI 10.1109/CVPR42600.2020.00288
   Shen LH, 2019, IEEE T MULTIMEDIA, V21, P1093, DOI 10.1109/TMM.2018.2871955
   Shin J, 2022, IEEE T MULTIMEDIA, V24, P245, DOI 10.1109/TMM.2021.3050053
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song YF, 2018, IEEE T MULTIMEDIA, V20, P1548, DOI 10.1109/TMM.2017.2771472
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Tan RT, 2008, PROC CVPR IEEE, P2347, DOI 10.1109/cvpr.2008.4587643
   Vaswani A, 2017, ADV NEUR IN, V30
   Venkatanath N, 2015, NATL CONF COMMUN
   Verma M, 2015, PROCEEDING OF THE THIRD INTERNATIONAL SYMPOSIUM ON WOMEN IN COMPUTING AND INFORMATICS (WCI-2015), P426, DOI 10.1145/2791405.2791513
   Wang WC, 2017, IEEE T MULTIMEDIA, V19, P1142, DOI 10.1109/TMM.2017.2652069
   Yang WH, 2017, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2017.183
   Yang X, 2019, IEEE T MULTIMEDIA, V21, P328, DOI 10.1109/TMM.2018.2863602
   Zhang H, 2018, IEEE COMPUT SOC CONF, P1015, DOI 10.1109/CVPRW.2018.00135
   Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337
   Zhang H, 2018, PROC CVPR IEEE, P695, DOI 10.1109/CVPR.2018.00079
   Zhang J, 2020, IEEE T IMAGE PROCESS, V29, P72, DOI 10.1109/TIP.2019.2922837
   Zhang K., 2022, Int. J. Comput. Vis., P1
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang KH, 2021, IEEE T IMAGE PROCESS, V30, P7419, DOI 10.1109/TIP.2021.3104166
   Zhang KH, 2020, PROC CVPR IEEE, P2734, DOI 10.1109/CVPR42600.2020.00281
   Zhang KH, 2019, IEEE T IMAGE PROCESS, V28, P291, DOI 10.1109/TIP.2018.2867733
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 83
TC 8
Z9 8
U1 6
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6794
EP 6807
DI 10.1109/TMM.2022.3214780
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000007
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhu, YC
   Li, YH
   Sun, W
   Min, XK
   Zhai, GT
   Yang, XK
AF Zhu, Yucheng
   Li, Yunhao
   Sun, Wei
   Min, Xiongkuo
   Zhai, Guangtao
   Yang, Xiaokang
TI Blind Image Quality Assessment via Cross-View Consistency
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image quality; Feature extraction; Distortion; Image color analysis;
   Data mining; Transformers; Task analysis; Image quality assessment;
   authentic distortion; self-supervised learning; transformer; deep
   learning
ID LEVEL
AB Image quality assessment (IQA) is very important for both end-users and service-providers since a high-quality image can significantly improve the user's quality of experience (QoE). Most existing blind image quality assessment (BIQA) models were developed for synthetically distorted images, however, they perform poorly on in-the-wild images, which are widely existed in various practical applications. In this paper, a BIQA model is proposed that consists of a desirable self-supervised feature learning approach to mitigate the data shortage problem and learn comprehensive feature representations, and a self-attention-based feature fusion module to introduce self-attention mechanism. We develop the image quality assessment model under the framework of contrastive learning with multi views. Since human visual system perceives signals through multiple channels, the most important visual information should exist among all views of the channels. So we design the cross-view consistent information mining (CVC-IM) module to extract compact mutual information between different views. Color information and pseudo-reference image (PRI) of different distortion types are employed to formulate rich feature embeddings and preserve the quality-aware fidelity of learned representations. We employ the Transformer as the self-attention-based architecture to integrate feature embeddings. Extensive experiments show that our model achieves remarkable image quality assessment results on in-the-wild IQA datasets.
C1 [Zhu, Yucheng; Li, Yunhao; Sun, Wei; Min, Xiongkuo; Zhai, Guangtao; Yang, Xiaokang] Shanghai Jiao Tong Univ, Inst Image Commun & Network Engn, Shanghai 200240, Peoples R China.
   [Zhu, Yucheng; Li, Yunhao; Sun, Wei; Min, Xiongkuo; Zhai, Guangtao; Yang, Xiaokang] Shanghai Jiao Tong Univ, Moe Key Lab Artificial Intelligence, Shanghai 200240, Peoples R China.
   [Zhu, Yucheng] USC SJTU Inst Cultural & Creat Ind, Shanghai 200240, Peoples R China.
C3 Shanghai Jiao Tong University; Shanghai Jiao Tong University
RP Zhai, GT (corresponding author), Shanghai Jiao Tong Univ, Inst Image Commun & Network Engn, Shanghai 200240, Peoples R China.; Zhai, GT (corresponding author), Shanghai Jiao Tong Univ, Moe Key Lab Artificial Intelligence, Shanghai 200240, Peoples R China.
EM zyc420@sjtu.edu.cn; lyhsjtu@sjtu.edu.cn; sunguwei@sjtu.edu.cn;
   minxiongkuo@sjtu.edu.cn; zhaiguangtao@sjtu.edu.cn; xkyang@sjtu.edu.cn
RI Zhai, Guangtao/X-5949-2019; Sun, Wei/HMV-5798-2023
OI Zhai, Guangtao/0000-0001-8165-9322; Sun, Wei/0000-0001-8162-1949; Zhu,
   Yucheng/0000-0002-3069-060X
FU National Natural Science Foundation of China [62101326, 62225112,
   61831015, 62271312]; National Key R&D Program of China [2021YFE0206700];
   China Postdoctoral Science Foundation [2022M712090]; Shanghai Municipal
   Science and Technology Major Project [2021SHZDZX0102]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62101326, 62225112, 61831015, and
   62271312, in part by the National Key R & D Program of China under Grant
   2021YFE0206700, in part by China Postdoctoral Science Foundation under
   Grant 2022M712090, and in part by Shanghai Municipal Science and
   Technology Major Project under Grant 2021SHZDZX0102. The associate
   editorcoordinating the review of this manuscript and approving it for
   publication wasDr. Marco Carli
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Ali M.A., 2012, VISION VERTEBRATES
   Antkowiak J., 2000, document COM9-80-E
   Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Celona L, 2022, J OPT SOC AM A, V39, pB1, DOI 10.1364/JOSAA.448144
   Chen Pengfei, 2021, P IEEECVF INT C COMP, P5178
   Cheon M, 2021, IEEE COMPUT SOC CONF, P433, DOI 10.1109/CVPRW53098.2021.00054
   den Ouden HEM, 2012, FRONT PSYCHOL, V3, DOI 10.3389/fpsyg.2012.00548
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Fang YM, 2020, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR42600.2020.00373
   Ou FZ, 2021, PROC CVPR IEEE, P7666, DOI 10.1109/CVPR46437.2021.00758
   Ghadiyaram D, 2016, IEEE T IMAGE PROCESS, V25, P372, DOI 10.1109/TIP.2015.2500021
   Girdhar R, 2019, PROC CVPR IEEE, P244, DOI 10.1109/CVPR.2019.00033
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Han Tengda, 2020, Adv. Neural Inf. Process. Syst., NIPS, V33, P5679
   Hancheng Zhu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14131, DOI 10.1109/CVPR42600.2020.01415
   Hassani A, 2022, Arxiv, DOI arXiv:2104.05704
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Kaiming, 2020, C COMP VIS PATT REC, P2, DOI [DOI 10.1109/CVPR42600.2020.00975, 10.1109/CVPR42600.2020.00975]
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Hosu V, 2020, IEEE T IMAGE PROCESS, V29, P4041, DOI 10.1109/TIP.2020.2967829
   Jain A. K., 1989, Fundamentals of Digital Image Processing
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Ke JJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5128, DOI 10.1109/ICCV48922.2021.00510
   Larsson G, 2017, PROC CVPR IEEE, P840, DOI 10.1109/CVPR.2017.96
   Larsson G, 2016, LECT NOTES COMPUT SC, V9908, P577, DOI 10.1007/978-3-319-46493-0_35
   Li DQ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P789, DOI 10.1145/3394171.3413804
   Li LD, 2022, IEEE T CIRC SYST VID, V32, P8512, DOI 10.1109/TCSVT.2021.3112197
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu YT, 2019, IEEE T MULTIMEDIA, V21, P135, DOI 10.1109/TMM.2018.2849602
   Ma KD, 2018, IEEE T IMAGE PROCESS, V27, P1202, DOI 10.1109/TIP.2017.2774045
   Madhusudana PC, 2022, IEEE WINT CONF APPL, P93, DOI 10.1109/WACVW54805.2022.00015
   Min XK, 2018, IEEE T MULTIMEDIA, V20, P2049, DOI 10.1109/TMM.2017.2788206
   Min XK, 2018, IEEE T BROADCAST, V64, P508, DOI 10.1109/TBC.2018.2816783
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Pan ZQ, 2022, IEEE T CIRC SYST VID, V32, P7518, DOI 10.1109/TCSVT.2022.3188991
   Prabhushankar M., 2017, Electronic Imaging, P30, DOI 10.2352/ISSN.2470-1173.2017.12.IQSP-223
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sohn K, 2016, ADV NEUR IN, V29
   Su SL, 2020, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR42600.2020.00372
   Sun W, 2020, IEEE J-STSP, V14, P64, DOI 10.1109/JSTSP.2019.2955024
   Tanaka J, 2001, TRENDS COGN SCI, V5, P211, DOI 10.1016/S1364-6613(00)01626-0
   Terhorst P, 2020, PROC CVPR IEEE, P5650, DOI 10.1109/CVPR42600.2020.00569
   Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149
   van den Oord A, 2019, Arxiv, DOI arXiv:1807.03748
   Virtanen T, 2015, IEEE T IMAGE PROCESS, V24, P390, DOI 10.1109/TIP.2014.2378061
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Xu JT, 2016, IEEE T IMAGE PROCESS, V25, P4444, DOI 10.1109/TIP.2016.2585880
   Yan B, 2019, IEEE T MULTIMEDIA, V21, P2603, DOI 10.1109/TMM.2019.2904879
   Ye P, 2012, PROC CVPR IEEE, P1098, DOI 10.1109/CVPR.2012.6247789
   Zhai GT, 2021, IEEE T MULTIMEDIA, V23, P3700, DOI 10.1109/TMM.2020.3029891
   Zhang JH, 2022, IEEE T INTELL TRANSP, V23, P3087, DOI 10.1109/TITS.2020.3030673
   Zhang R, 2017, PROC CVPR IEEE, P645, DOI 10.1109/CVPR.2017.76
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zhang WX, 2020, IEEE T CIRC SYST VID, V30, P36, DOI 10.1109/TCSVT.2018.2886771
   Zhu MM, 2021, IEEE INT CONF COMP V, P1953, DOI 10.1109/ICCVW54120.2021.00222
   Zhu YC, 2016, INT CONF ACOUST SPEE, P1085, DOI 10.1109/ICASSP.2016.7471843
NR 60
TC 13
Z9 13
U1 9
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7607
EP 7620
DI 10.1109/TMM.2022.3224319
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000063
DA 2024-07-18
ER

PT J
AU Deng, WX
   Zhao, LJ
   Liao, Q
   Guo, DK
   Kuang, GY
   Hu, DW
   Pietikainen, M
   Liu, L
AF Deng, Wanxia
   Zhao, Lingjun
   Liao, Qing
   Guo, Deke
   Kuang, Gangyao
   Hu, Dewen
   Pietikainen, Matti
   Liu, Li
TI Informative Feature Disentanglement for Unsupervised Domain Adaptation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Measurement; Feature extraction; Wheels; Image
   reconstruction; Image color analysis; Adaptation models; Domain
   Adaptation; deep learning; deep convolutional neural network;
   autoencoder; transfer learning; unsupervised learning
ID NETWORKS
AB Unsupervised Domain Adaptation (UDA) aims at learning a classifier for an unlabeled target domain by transferring knowledge from a labeled source domain with a related but different distribution. The strategy of aligning the two domains in latent feature space via metric discrepancy or adversarial learning has achieved considerable progress. However, these existing approaches mainly focus on adapting the entire image and ignore the bottleneck that occurs when forced adaptation of uninformative domain-specific variations undermines the effectiveness of learned features. To address this problem, we propose a novel component called Informative Feature Disentanglement (IFD), which is equipped with the adversarial network or the metric discrepancy model, respectively. Accordingly, the new network architectures, named IFDAN and IFDMN, enable informative feature refinement before the adaptation. The proposed IFD is designed to disentangle informative features from the uninformative domain-specific variations, which are produced by a Variational Autoencoder (VAE) with lateral connections from the encoder to the decoder. We cooperatively apply the IFD to conduct supervised disentanglement for the source domain and unsupervised disentanglement for the target domain. In this way, informative features are disentangled from the domain-specific details before the adaptation. Extensive experimental results on three gold-standard domain adaptation datasets, e.g., Office31, Office-Home and VisDA-C, demonstrate the effectiveness of the proposed IFDAN and IFDMN models for UDA.
C1 [Deng, Wanxia; Zhao, Lingjun; Kuang, Gangyao] Natl Univ Def Technol, State Key Lab Complex Electromagnet Environm Effe, Coll Elect Sci, Changsha 410073, Peoples R China.
   [Liao, Qing] Harbin Inst Technol, Dept Comp Sci & Technol, Shenzhen 518055, Peoples R China.
   [Guo, Deke; Liu, Li] Natl Univ Def Technol, Coll Syst Engn, Changsha 410073, Peoples R China.
   [Hu, Dewen] Natl Univ Def Technol, Coll Intelligent Sci, Changsha 410073, Peoples R China.
   [Pietikainen, Matti; Liu, Li] Univ Oulu, Ctr Machine Vis & Signal Anal, Oulu 90570, Finland.
C3 National University of Defense Technology - China; Harbin Institute of
   Technology; National University of Defense Technology - China; National
   University of Defense Technology - China; University of Oulu
RP Liu, L (corresponding author), Natl Univ Def Technol, Coll Syst Engn, Changsha 410073, Peoples R China.
EM wanxiadeng@163.com; nudtzlj@163.com; liaoqing@hit.edu.cn;
   guodeke@gmail.com; kuanggangyao@nudt.edu.cn; dwhu@nudt.edu.cn;
   matti.pietikainen@oulu.fi; dreamliu2010@gmail.com
RI Hu, Dewen/AAN-8511-2020; Zhao, Lingjun/HPE-8072-2023
OI Pietikainen, Matti/0000-0003-2263-6731; Liu, li/0000-0002-2011-2873;
   Guo, Deke/0000-0003-4894-5540
FU National Natural Science Foundation of China [62022091, 61872379,
   71701205, 61701508, 62036013]; Academy of Finland [331883]; Hunan
   Provincial Natural Science Foundation of China [2018JJ3613]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62022091, 61872379, 71701205, 61701508,
   and 62036013, in part by the Academy of Finland under Grant 331883, and
   in part by Hunan Provincial Natural Science Foundation of China under
   Grant 2018JJ3613. The associate editor coordinating the review of this
   manuscript and approving it for publicationwas Prof. Xin Geng.
CR Alemi A. A., 2017, ICLR, DOI DOI 10.48550/ARXIV.1612.00410
   [Anonymous], 2019, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2019.00309
   Baktashmotlagh M., 2019, ICLR
   Ben-David S., 2007, NIPS, P137
   Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4
   Bousmalis K, 2016, ADV NEUR IN, V29
   Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18
   Busto PP, 2017, IEEE I CONF COMP VIS, P754, DOI 10.1109/ICCV.2017.88
   Cao ZJ, 2018, LECT NOTES COMPUT SC, V11212, P139, DOI 10.1007/978-3-030-01237-3_9
   Cao ZJ, 2019, PROC CVPR IEEE, P2980, DOI 10.1109/CVPR.2019.00310
   Chang WG, 2019, PROC CVPR IEEE, P7346, DOI 10.1109/CVPR.2019.00753
   Chen QC, 2018, PROC CVPR IEEE, P7976, DOI 10.1109/CVPR.2018.00832
   Chen XY, 2019, PR MACH LEARN RES, V97
   Csurka G, 2017, ADV COMPUT VIS PATT, P1, DOI 10.1007/978-3-319-58347-1_1
   Donahue J, 2014, PR MACH LEARN RES, V32
   Ganin Y., 2015, ICML
   Ganin Y, 2016, J MACH LEARN RES, V17
   Garcia-Garcia A, 2018, APPL SOFT COMPUT, V70, P41, DOI 10.1016/j.asoc.2018.05.018
   Ghifary M, 2016, LECT NOTES COMPUT SC, V9908, P597, DOI 10.1007/978-3-319-46493-0_36
   Ghifary M, 2014, LECT NOTES ARTIF INT, V8862, P898, DOI 10.1007/978-3-319-13560-1_76
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hoffman J, 2018, PR MACH LEARN RES, V80
   Hui Tang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8722, DOI 10.1109/CVPR42600.2020.00875
   Kang GL, 2018, LECT NOTES COMPUT SC, V11215, P420, DOI 10.1007/978-3-030-01252-6_25
   Kang GL, 2019, PROC CVPR IEEE, P4888, DOI 10.1109/CVPR.2019.00503
   Kingma D. P., 2014, AUTOENCODING VARIATI, P3581
   Kouw WM, 2021, IEEE T PATTERN ANAL, V43, P766, DOI 10.1109/TPAMI.2019.2945942
   Lee C.-Y., 2019, PROC IEEE C COMPUT V, p10 285
   Lei Z, 2019, ARXIV190304687
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu H, 2019, 36 INT C MACHINE LEA, V97
   Liu L, 2020, INT J COMPUT VISION, V128, P261, DOI 10.1007/s11263-019-01247-4
   Liu M.-Y., 2016, P ADV NEUR INF PROC, P469
   Liu M.-Y., 2017, NIPS
   Liu YJ, 2019, PROC CVPR IEEE, P7186, DOI 10.1109/CVPR.2019.00736
   Locatello F, 2019, PR MACH LEARN RES, V97
   Long M., 2017, Proc Mach Learn Res, V70, P2208
   Long M, 2016, PROCEEDINGS OF SYMPOSIUM OF POLICING DIPLOMACY AND THE BELT & ROAD INITIATIVE, 2016, P136
   Long MS, 2018, ADV NEUR IN, V31
   Long MS, 2019, IEEE T PATTERN ANAL, V41, P3071, DOI 10.1109/TPAMI.2018.2868685
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Ma XH, 2019, IEEE T MULTIMEDIA, V21, P2419, DOI 10.1109/TMM.2019.2902100
   Maas AL., 2013, P ICML WORKSHOP DEEP, V28, P1
   Mansour Yishay, 2009, ARXIV09023430
   Minaee S, 2022, IEEE T PATTERN ANAL, V44, P3523, DOI 10.1109/TPAMI.2021.3059968
   Murez Z, 2018, PROC CVPR IEEE, P4500, DOI 10.1109/CVPR.2018.00473
   Paszke Adam, 2017, NIPS W
   Pei ZY, 2018, AAAI CONF ARTIF INTE, P3934
   Peng X., 2017, ARXIV 171006924
   Pinheiro PO, 2018, PROC CVPR IEEE, P8004, DOI 10.1109/CVPR.2018.00835
   Roy S, 2019, PROC CVPR IEEE, P9463, DOI 10.1109/CVPR.2019.00970
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Russo P, 2018, PROC CVPR IEEE, P8099, DOI 10.1109/CVPR.2018.00845
   Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16
   Saito K, 2018, PROC CVPR IEEE, P3723, DOI 10.1109/CVPR.2018.00392
   Saito K, 2018, LECT NOTES COMPUT SC, V11209, P156, DOI 10.1007/978-3-030-01228-1_10
   Sankaranarayanan S, 2018, PROC CVPR IEEE, P8503, DOI 10.1109/CVPR.2018.00887
   Shen J, 2018, AAAI CONF ARTIF INTE, P4058
   Shu R., 2018, P 6 INT C LEARN REPR
   Sun BC, 2016, AAAI CONF ARTIF INTE, P2058
   Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35
   Taigman Y., 2016, INT C LEARN REPR
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Tishby N., 2000, ARXIVPHYSICS0004057
   Tommasi T, 2015, LECT NOTES COMPUT SC, V9358, P504, DOI 10.1007/978-3-319-24947-6_42
   Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347
   Tzeng E., 2014, ARXIV14123474
   Tzeng E, 2015, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2015.463
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572
   Wang M, 2018, NEUROCOMPUTING, V312, P135, DOI 10.1016/j.neucom.2018.05.083
   Wang SS, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3181
   Wang XM, 2019, AAAI CONF ARTIF INTE, P5345
   Xie S., 2018, P 35 INT C MACHINE L, P5423
   Xu RJ, 2019, IEEE I CONF COMP VIS, P1426, DOI 10.1109/ICCV.2019.00151
   Yan HL, 2020, IEEE T MULTIMEDIA, V22, P2420, DOI 10.1109/TMM.2019.2953375
   Yan HL, 2017, PROC CVPR IEEE, P945, DOI 10.1109/CVPR.2017.107
   Yosinski J, 2014, ADV NEUR IN, V27
   You KC, 2019, PROC CVPR IEEE, P2715, DOI 10.1109/CVPR.2019.00283
   Zellinger Werner, 2017, ARXIV170208811
   Zhang J, 2018, PROC CVPR IEEE, P8156, DOI 10.1109/CVPR.2018.00851
   Zhang L, 2019, IEEE T NEUR NET LEAR, V30, P3759, DOI 10.1109/TNNLS.2019.2899037
   Zhang WC, 2018, PROC CVPR IEEE, P3801, DOI 10.1109/CVPR.2018.00400
   Zhang YB, 2019, PROC CVPR IEEE, P5026, DOI 10.1109/CVPR.2019.00517
   Zhao S., 2017, P INT C MACH LEARN I, P4091
NR 88
TC 22
Z9 22
U1 7
U2 51
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2407
EP 2421
DI 10.1109/TMM.2021.3080516
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 1D5KQ
UT WOS:000793839600014
OA Green Submitted, hybrid
DA 2024-07-18
ER

PT J
AU Guo, PF
   He, L
   Liu, SY
   Zeng, DL
   Liu, HT
AF Guo, Pengfei
   He, Lang
   Liu, Shuangyin
   Zeng, Delu
   Liu, Hantao
TI Underwater Image Quality Assessment: Subjective and Objective Methods
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image enhancement; Measurement; Histograms; Image color analysis; Image
   quality; Image restoration; Benchmark testing; Underwater image; image
   quality assessment; perception experiment; statistical analysis;
   objective metric
ID VISION; ENHANCEMENT; VISIBILITY; FRAMEWORK
AB Underwater image enhancement plays a critical role in marine industry. Various algorithms are applied to enhance underwater images, but their performance in terms of perceptual quality has been little studied. In this paper, we investigate five popular enhancement algorithms and their output image quality. To this end, we have created a benchmark, including images enhanced by different algorithms and ground truth image quality obtained by human perception experiments. We statistically analyse the impact of various enhancement algorithms on the perceived quality of underwater images. Also, the visual quality provided by these algorithms is evaluated objectively, aiming to inform the development of objective metrics for automatic assessment of the quality for underwater image enhancement. The image quality benchmark and its objective metric are made publicly available.
C1 [Guo, Pengfei] Zhongkai Univ Agr & Engn, Sch Computat Sci, Guangzhou 510225, Peoples R China.
   [Guo, Pengfei; Zeng, Delu] South China Univ Technol, Sch Math, Guangzhou 510641, Peoples R China.
   [He, Lang] Sun Yat Sen Univ, Sch Engn & Comp Sci, Guangzhou 510006, Peoples R China.
   [Liu, Shuangyin] Zhongkai Univ Agr & Engn, Coll Informat Sci & Technol, Guangzhou 510225, Peoples R China.
   [Liu, Hantao] Cardiff Univ, Sch Comp Sci & Informat, Cardiff CF24 3AA, Wales.
C3 Zhongkai University of Agriculture & Engineering; South China University
   of Technology; Sun Yat Sen University; Zhongkai University of
   Agriculture & Engineering; Cardiff University
RP Liu, HT (corresponding author), Cardiff Univ, Sch Comp Sci & Informat, Cardiff CF24 3AA, Wales.
EM Guopfzhku@163.com; helang5@mail2.sysu.edu.cn; 450799063@qq.com;
   dlzeng@scut.edu.cn; liuh35@cardiff.ac.uk
RI guo, peng/AAG-4052-2019; Guo, Peng/IZQ-0331-2023; Guo,
   Peng/GWC-0572-2022
FU Foundation for High-level Talents in Higher Education of Guangdong
   Province [2017KTSCX095, 2016KQNCX075]; General Program of Technology
   Project of Guangzhou [201707010221]; Lifting Project of Guangzhou Youth
   Talent; Guangdong basic and applied basic research foundation
   [2020A1515110958]
FX This work was supported in part by Foundation for High-level Talents in
   Higher Education of Guangdong Province under Grants 2017KTSCX095 and
   2016KQNCX075, in part by the General Program of Technology Project of
   Guangzhou under Grant 201707010221, and in part by the Lifting Project
   of Guangzhou Youth Talent, and Guangdong basic and applied basic
   research foundation under Grant 2020A1515110958.
CR Ancuti CO, 2013, IEEE T IMAGE PROCESS, V22, P3271, DOI 10.1109/TIP.2013.2262284
   Ancuti C, 2012, PROC CVPR IEEE, P81, DOI 10.1109/CVPR.2012.6247661
   [Anonymous], 2000, FINAL REPORT VIDEO Q
   [Anonymous], 2002, Int. Telecommun. Union
   BEDNAR JB, 1984, IEEE T ACOUST SPEECH, V32, P145, DOI 10.1109/TASSP.1984.1164279
   Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Chen WL, 2021, IEEE T MULTIMEDIA, V23, P1008, DOI 10.1109/TMM.2020.2991546
   DUNTLEY SQ, 1963, J OPT SOC AM, V53, P214, DOI 10.1364/JOSA.53.000214
   Fabbri C, 2018, IEEE INT CONF ROBOT, P7159
   Fattal R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360671
   Galdran A., 2017, P EUR C COMP METH AP, P844
   Ghani ASA, 2018, OCEAN ENG, V162, P224, DOI 10.1016/j.oceaneng.2018.05.027
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P432, DOI 10.1109/TMM.2016.2518868
   Guo PF, 2020, COMPUT ELECTRON AGR, V175, DOI 10.1016/j.compag.2020.105608
   Hadizadeh H, 2018, IEEE T MULTIMEDIA, V20, P392, DOI 10.1109/TMM.2017.2740023
   Hancheng Zhu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14131, DOI 10.1109/CVPR42600.2020.01415
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   HUMMEL R, 1977, COMPUT VISION GRAPH, V6, P184, DOI 10.1016/S0146-664X(77)80011-7
   Hung-Yu Yang, 2011, Proceedings of the 2011 2nd International Conference on Innovations in Bio-Inspired Computing and Applications (IBICA 2011), P17, DOI 10.1109/IBICA.2011.9
   Janowski L, 2015, IEEE T MULTIMEDIA, V17, P2210, DOI 10.1109/TMM.2015.2484963
   Lee DJ, 2003, IEEE IND ELEC, P1080
   Li CY, 2016, IEEE T IMAGE PROCESS, V26, P5664, DOI 10.1109/TIP.2016.2612882
   Li CY, 2020, IEEE T IMAGE PROCESS, V29, P4376, DOI 10.1109/TIP.2019.2955241
   Li CY, 2015, J ELECTRON IMAGING, V24, DOI 10.1117/1.JEI.24.3.033023
   Li DQ, 2019, IEEE T MULTIMEDIA, V21, P1221, DOI 10.1109/TMM.2018.2875354
   Li J, 2018, IEEE ROBOT AUTOM LET, V3, P387, DOI 10.1109/LRA.2017.2730363
   Li J, 2019, IEEE T MULTIMEDIA, V21, P2589, DOI 10.1109/TMM.2019.2903722
   Li LD, 2016, IEEE T CYBERNETICS, V46, P39, DOI 10.1109/TCYB.2015.2392129
   Li LD, 2020, IEEE T IMAGE PROCESS, V29, P3898, DOI 10.1109/TIP.2020.2968285
   Li LD, 2017, IEEE T MULTIMEDIA, V19, P1030, DOI 10.1109/TMM.2016.2640762
   Lin WS, 2011, J VIS COMMUN IMAGE R, V22, P297, DOI 10.1016/j.jvcir.2011.01.005
   Liu H, 2015, CHIN J OCEANOL LIMN, V33, P114, DOI 10.1007/s00343-015-4080-3
   Liu LX, 2014, SIGNAL PROCESS-IMAGE, V29, P856, DOI 10.1016/j.image.2014.06.006
   Liu YT, 2019, IEEE T MULTIMEDIA, V21, P135, DOI 10.1109/TMM.2018.2849602
   Ma L, 2011, IEEE T MULTIMEDIA, V13, P824, DOI 10.1109/TMM.2011.2109701
   Min XK, 2018, IEEE T MULTIMEDIA, V20, P2049, DOI 10.1109/TMM.2017.2788206
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Murino V, 2000, COMPUT VIS IMAGE UND, V79, P1, DOI 10.1006/cviu.2000.0852
   Narasimhan SG, 2002, INT J COMPUT VISION, V48, P233, DOI 10.1023/A:1016328200723
   Narasimhan SG, 2000, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2000.855874
   Panetta K, 2016, IEEE J OCEANIC ENG, V41, P541, DOI 10.1109/JOE.2015.2469915
   Panetta K, 2011, IEEE T SYST MAN CY B, V41, P460, DOI 10.1109/TSMCB.2010.2058847
   Peng YT, 2018, IEEE T IMAGE PROCESS, V27, P2856, DOI 10.1109/TIP.2018.2813092
   Peng YT, 2017, IEEE T IMAGE PROCESS, V26, P1579, DOI 10.1109/TIP.2017.2663846
   Pratt W.K., 1977, DIGITAL IMAGE PROCES
   RUDERMAN DL, 1994, PHYS REV LETT, V73, P814, DOI 10.1103/PhysRevLett.73.814
   Schechner YY, 2005, IEEE J OCEANIC ENG, V30, P570, DOI 10.1109/JOE.2005.850871
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Wan ZL, 2020, IEEE T MULTIMEDIA, V22, P2024, DOI 10.1109/TMM.2019.2950533
   Wang Y, 2017, IEEE IMAGE PROC, P1382, DOI 10.1109/ICIP.2017.8296508
   Wang Z, 2009, IEEE SIGNAL PROC MAG, V26, P98, DOI 10.1109/MSP.2008.930649
   Xi Qiao, 2017, Information Processing in Agriculture, V4, P206, DOI 10.1016/j.inpa.2017.06.001
   Xiang T, 2020, IEEE T MULTIMEDIA, V22, P1259, DOI 10.1109/TMM.2019.2938612
   Yang M, 2015, IEEE T IMAGE PROCESS, V24, P6062, DOI 10.1109/TIP.2015.2491020
   Ye XC, 2018, LECT NOTES COMPUT SC, V11166, P514, DOI 10.1007/978-3-030-00764-5_47
   Zhao MM, 2015, 2015 VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP), DOI 10.1109/VCIP.2015.7457900
   Zuiderveld K., 1994, Graphics Gems, P474, DOI 10.1016/B978-0-12-336156-1.50061-6
NR 59
TC 21
Z9 21
U1 5
U2 64
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1980
EP 1989
DI 10.1109/TMM.2021.3074825
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0H8DA
UT WOS:000778959200016
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Gutiérrez, J
   Pérez, P
   Orduna, M
   Singla, A
   Cortés, C
   Mazumdar, P
   Viola, I
   Brunnstrom, K
   Battisti, F
   Cieplinska, N
   Juszka, D
   Janowski, L
   Leszczuk, M
   Adeyemi-Ejeye, A
   Hu, YS
   Chen, ZZ
   Van Wallendael, G
   Lambert, P
   Díaz, C
   Hedlund, J
   Hamsis, O
   Fremerey, S
   Hofmeyer, F
   Raake, A
   César, P
   Carli, M
   García, N
AF Gutierrez, Jesus
   Perez, Pablo
   Orduna, Marta
   Singla, Ashutosh
   Cortes, Carlos
   Mazumdar, Pramit
   Viola, Irene
   Brunnstrom, Kjell
   Battisti, Federica
   Cieplinska, Natalia
   Juszka, Dawid
   Janowski, Lucjan
   Leszczuk, Mikolaj
   Adeyemi-Ejeye, Anthony
   Hu, Yaosi
   Chen, Zhenzhong
   Van Wallendael, Glenn
   Lambert, Peter
   Diaz, Cesar
   Hedlund, John
   Hamsis, Omar
   Fremerey, Stephan
   Hofmeyer, Frank
   Raake, Alexander
   Cesar, Pablo
   Carli, Marco
   Garcia, Narciso
TI Subjective Evaluation of Visual Quality and Simulator Sickness of Short
   360° Videos: ITU-T Rec. P.919
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Quality of experience; 360 degrees video; subjective test; methodology;
   simulator sickness; dataset
ID EXPERIENCE
AB Recently an impressive development in immersive technologies, such as Augmented Reality (AR), Virtual Reality (VR) and 360 degrees video, has been witnessed. However, methods for quality assessment have not been keeping up. This paper studies quality assessment of 360 degrees video from the cross-lab tests (involving ten laboratories and more than 300 participants) carried out by the Immersive Media Group (IMG) of the Video Quality Experts Group (VQEG). These tests were addressed to assess and validate subjective evaluation methodologies for 360 degrees video. Audiovisual quality, simulator sickness symptoms, and exploration behavior were evaluated with short (from 10 seconds to 30 seconds) 360 degrees sequences. The following factors' influences were also analyzed: assessment methodology, sequence duration, Head-Mounted Display (HMD) device, uniform and non-uniform coding degradations, and simulator sickness assessment methods. The obtained results have demonstrated the validity of Absolute Category Rating (ACR) and Degradation Category Rating (DCR) for subjective tests with 360 degrees videos, the possibility of using 10-second videos (with or without audio) when addressing quality evaluation of coding artifacts, as well as any commercial HMD (satisfying minimum requirements). Also, more efficient methods than the long Simulator Sickness Questionnaire (SSQ) have been proposed to evaluate related symptoms with 360 degrees videos. These results have been instrumental for the development of the ITU-T Recommendation P.919. Finally, the annotated dataset from the tests is made publicly available for the research community.
C1 [Gutierrez, Jesus; Orduna, Marta; Cortes, Carlos; Diaz, Cesar; Garcia, Narciso] Univ Politecn Madrid, Informat Proc & Telecommun Ctr, Grp Tratamiento Imagenes, Madrid 28040, Spain.
   [Gutierrez, Jesus; Orduna, Marta; Cortes, Carlos; Diaz, Cesar; Garcia, Narciso] Univ Politecn Madrid, Escuela Tecn Super Ingn Telecomunicac, Madrid 28040, Spain.
   [Perez, Pablo] Nokia Bell Labs, Madrid 28050, Spain.
   [Singla, Ashutosh; Fremerey, Stephan; Hofmeyer, Frank; Raake, Alexander] Tech Univ Ilmenau, Audiovisual Technol Grp, D-98693 Ilmenau, Germany.
   [Mazumdar, Pramit] Indian Inst Informat Technol Vadodara, Dept Comp Sci & Engn, Gandhinagar 382028, India.
   [Viola, Irene; Cesar, Pablo] Ctr Wiskunde Infomart, NL-1098 XG Amsterdam, Netherlands.
   [Brunnstrom, Kjell; Hedlund, John; Hamsis, Omar] RISE Res Inst Sweden AB, S-16440 Kista, Sweden.
   [Brunnstrom, Kjell] Mid Sweden Univ, S-85170 Sundsvall, Sweden.
   [Battisti, Federica] Univ Padua, Dept Informat Engn, I-35131 Padua, Italy.
   [Cieplinska, Natalia; Juszka, Dawid; Janowski, Lucjan; Leszczuk, Mikolaj] AGH Univ Sci & Technol, PL-30059 Krakow, Poland.
   [Adeyemi-Ejeye, Anthony] Univ Surrey, Dept Mus & Media, Innovat Media Lab, Guildford GU2 7XH, Surrey, England.
   [Hu, Yaosi; Chen, Zhenzhong] Wuhan Univ, Sch Remote Sensing & Informat Engn, Wuhan 430079, Peoples R China.
   [Van Wallendael, Glenn; Lambert, Peter] Univ Ghent, IMEC, Dept Elect & Informat Syst, B-9000 Ghent, Belgium.
   [Carli, Marco] Univ Roma Tre, Dept Engn, I-00154 Rome, Italy.
C3 Universidad Politecnica de Madrid; Centro de I+D+I en Procesado de la
   Informacion Telecomunicaciones (IPT); Universidad Politecnica de Madrid;
   Technische Universitat Ilmenau; RISE Research Institutes of Sweden;
   Mid-Sweden University; University of Padua; AGH University of Krakow;
   University of Surrey; Wuhan University; IMEC; Ghent University; Roma Tre
   University
RP Gutiérrez, J (corresponding author), Univ Politecn Madrid, Informat Proc & Telecommun Ctr, Grp Tratamiento Imagenes, Madrid 28040, Spain.; Gutiérrez, J (corresponding author), Univ Politecn Madrid, Escuela Tecn Super Ingn Telecomunicac, Madrid 28040, Spain.
EM jesus.gutierrez@upm.es; pablo.perez@nokia-bell-labs.com;
   moc@gti.ssr.upm.es; ashutosh.singla@tu-ilmenau.de; ccs@gti.ssr.upm.es;
   pramit.mazumdar@iiitvadodara.ac.in; irene.viola@cwi.nl;
   kjell.brunnstrom@ri.se; federica.battisti@unipd.it; natacalia@gmail.com;
   juszka@agh.edu.pl; janowski@kt.agh.edu.pl; leszczuk@agh.edu.pl;
   femi.ae@surrey.ac.uk; ys_hu@whu.edu.cn; zzchen@whu.edu.cn;
   glenn.vanwallendael@ugent.be; peter.lambert@ugent.be;
   cdm@gti.ssr.upm.es; john.hedlund@ri.se; omar.hamsis@ri.se;
   stephan.fremerey@tu-ilmenau.de; frank.hofmeyer@tu-ilmenau.de;
   alexander.raake@tu-ilmenau.de; P.S.Cesar@cwi.nl;
   marco.carli@uniroma3.it; narciso@gti.ssr.upm.es
RI Gutiérrez, Jesús/AAB-8858-2021; Chen, Zhenzhong/C-2529-2015; Lambert,
   Peter/D-7776-2016; García, Narciso/E-8603-2011; Leszczuk, Mikołaj
   I/C-4857-2011; Van Wallendael, Glenn/H-8315-2015; Díaz,
   César/Y-9989-2019; Hu, Yaosi/GRY-6324-2022; Janowski,
   Lucjan/B-2264-2013; Juszka, Dawid/IUP-1686-2023; Viola,
   Irene/AAT-9714-2020; Raake, Alexander/R-7050-2017
OI Gutiérrez, Jesús/0000-0001-7878-4712; García,
   Narciso/0000-0002-0397-894X; Leszczuk, Mikołaj I/0000-0001-9123-1039;
   Van Wallendael, Glenn/0000-0001-9530-3466; Díaz,
   César/0000-0003-2030-9390; Hu, Yaosi/0000-0003-2784-6738; Juszka,
   Dawid/0000-0002-7122-260X; Cieplinska, Natalia/0000-0001-6047-2833;
   Viola, Irene/0000-0001-8990-2665; Adeyemi-Ejeye,
   Anthony/0000-0002-8371-7829; Orduna, Marta/0000-0002-3475-4594;
   Janowski, Lucjan/0000-0002-3151-2944; Perez, Pablo/0000-0002-3502-6791;
   Battisti, Federica/0000-0002-0846-5879; Raake,
   Alexander/0000-0002-9357-1763; Mazumdar, Pramit/0000-0003-0999-3689
FU Ministerio de Ciencia, Innovacion y Universidades (AEI/FEDER) of the
   Spanish Government [TEC2016-75981]; Juan de la Cierva fellowship
   [IJC2018-037816]; Spanish Administration Agency CDTI [IDI-20200861];
   CYTEMEX project - Free State of Thuringia, Germany [FKZ: 2018-FGI-0019];
   European Commission [762111]; Vinnova (Sweden's Innovation Agency)
   [2018-00735]; Norwegian Financial Mechanism 2014-2021
   [2019/34/H/ST6/00599]; NVIDIA Corporation; National Natural Science
   Foundation of China [61771348]; research project imec [ICON ILLUMINATE
   HBC.2018.0201]; Vinnova [2018-00735] Funding Source: Vinnova
FX The work of Jesus Gutierrez, Marta Orduna, Carlos Cortes, Cesar Diaz,
   and Narciso Garcia was supported in part by the Ministerio de Ciencia,
   Innovacion y Universidades (AEI/FEDER) of the Spanish Government under
   Project TEC2016-75981 (IVME). The work of Jesus Gutierrez was also
   supported in part by Juan de la Cierva fellowship (IJC2018-037816). The
   work of Pablo Perez work was supported in part by the Spanish
   Administration Agency CDTI under Project IDI-20200861 (AMATISTA). The
   work of Ashutosh Singla, Stephan Fremerey, Frank Hofmeyer, and Alexander
   Raake was supported in part by the CYTEMEX project funded by the Free
   State of Thuringia, Germany (FKZ: 2018-FGI-0019). The work of Irene
   Viola and Pablo Cesar work was supported in part by the European
   Commission as part of the H2020 program, under Grant 762111,
   "VRTogether" (http://vrtogether.eu/).The work of Kjell Brunnstrom, John
   Hedlund, and Omar Hamsis was supported in part by Vinnova (Sweden's
   Innovation Agency) in the Celtic-Next project 5G Perfecta (2018-00735).
   The work of Lucjan Janowski and Mikolaj Leszczuk was supported in part
   by the Norwegian Financial Mechanism 2014-2021 (Project number:
   2019/34/H/ST6/00599). The work of Anthony Adeyemi-Ejeye was supported in
   part by the NVIDIA Corporation. The work of Yaosi Hu and Zhenzhong Chen
   was supported in part by the National Natural Science Foundation of
   China under Grant 61771348. The work of Glenn Van Wallendael and Peter
   Lambert was supported in part by the research project imec ICON
   ILLUMINATE HBC.2018.0201.
CR Abbas A., 2016, JVET D0026
   Adams WJ, 2000, J BROADCAST ELECTRON, V44, P78, DOI 10.1207/s15506878jobem4401_6
   Apostolopoulos JG, 2012, P IEEE, V100, P974, DOI 10.1109/JPROC.2011.2182069
   Ariely D, 2000, J EXP PSYCHOL GEN, V129, P508, DOI 10.1037//0096-3445.129.4.508
   Asbun E., 2016, JOINT VID EXPL TEAM, P15
   B. Series, 2012, Recommendation ITU-R BT, V500, P500
   Baayen R.Harald., 2012, The Oxford handbook of laboratory phonology, P668
   Beerends JG, 1999, J AUDIO ENG SOC, V47, P355
   BORST C, 1982, AM J PHYSIOL, V243, pH676, DOI 10.1152/ajpheart.1982.243.5.H676
   Brunnstrom Kjell, 2009, IEEE Signal Processing Magazine, V26, P96, DOI 10.1109/MSP.2009.932162
   Brunnström K, 2018, J ELECTRON IMAGING, V27, DOI 10.1117/1.JEI.27.5.053013
   Brunnstrom Kjell., 2017, Quality and User Experience, V2, P1, DOI DOI 10.1007/S41233-016-0003-0
   Chao F.-Y., 2020, IEEE INT CONF MULTI, P1, DOI DOI 10.1109/icmew46912.2020.9105956
   Chikkerur S, 2011, IEEE T BROADCAST, V57, P165, DOI 10.1109/TBC.2011.2104671
   Corbillon X, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P199, DOI 10.1145/3083187.3083215
   Cortés C, 2019, IEEE ICCE, P281, DOI [10.1109/icce-berlin47944.2019.8966170, 10.1109/ICCE-Berlin47944.2019.8966170]
   David EJ, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P432, DOI 10.1145/3204949.3208139
   De Moor K, 2014, PROC SPIE, V9014, DOI 10.1117/12.2042243
   De Simone F., 2019, PROC HUM VIS ELECT I
   Digi-Capital, 2017, AUGM VIRT REAL REP Q
   Dima Elijs., 2020, Quality and User Experience, V5, P2
   Duan H., 2017, P IEEE INT C SYST SI, P1
   Duan HY, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351786
   Duan HY, 2018, LECT NOTES COMPUT SC, V10735, P662, DOI 10.1007/978-3-319-77380-3_63
   Duzmanska N, 2018, FRONT PSYCHOL, V9, DOI 10.3389/fpsyg.2018.02132
   Elwardy M., 2019, PROC INT C SIGNAL PR, P1
   Faul F, 2007, BEHAV RES METHODS, V39, P175, DOI 10.3758/BF03193146
   Feltham J., 2018, USE PS4 SWITCH XBOX
   Fremerey S., 2019, SPIE ELECT IMAGING
   Fremerey S, 2020, IEEE INT SYM MULTIM, P65, DOI 10.1109/ISM.2020.00017
   Fremerey S, 2019, INT WORK QUAL MULTIM, DOI 10.1109/QoMEX.2019.8743307
   Fremerey S, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P403, DOI 10.1145/3204949.3208134
   Fremerey Stephan., 2020, 2020 Twelfth International Conference on Quality of Multimedia Experience (QoMEX), P1
   Fröhlich P, 2012, INT WORK QUAL MULTIM, P242, DOI 10.1109/QoMEX.2012.6263851
   Gutierrez J., 2020, ACM SIGMULTIMEDIA RE, V12
   Hakkinen J., 2006, P 8 C HUMAN COMPUTER, P227, DOI DOI 10.1145/1152215.1152263
   Hammer F., 2018, QUALITY USER EXPERIE, V3, P1, DOI DOI 10.1007/S41233-018-0022-0
   Hands DS, 2004, IEEE T MULTIMEDIA, V6, P806, DOI 10.1109/TMM.2004.837233
   Hofmeyer F., 2019, SPIE HUM VIS ELECT I, V7
   Huynh-Thu Q., 2015, P 1 INT C ADV IM, P1
   IEEE, 2017, IEEE SA 1918 1 1 HA
   IEEE, 2017, IEEE SA HFVE HUM FAC
   IEEE, 2017, IEEE SA VRAR VIRT RE
   ISO/IEC JTC1/SC29/WG1(JPEG) & WG11(MPEG), 2016, JTC1SC29WG11MPEG2016
   ITU-T, 2008, ITU T P910 SUBJECTIV
   ITU-T, 2020, INFL FACT QUAL EXP Q
   ITU-T, 2017, ITU T REC P 10 G 100
   ITU-T, 2016, METH SUBJ ASS VID QU
   ITU-T, 2016, SUBJ ASS METH 3D VID
   ITU-T, 2020, Technical Report P.919
   Jun HS, 2022, IEEE T AFFECT COMPUT, V13, P1416, DOI 10.1109/TAFFC.2020.3004617
   Keshavarz B, 2011, HUM FACTORS, V53, P415, DOI 10.1177/0018720811403736
   Kim HK, 2018, APPL ERGON, V69, P66, DOI 10.1016/j.apergo.2017.12.016
   Kim SY, 2019, MOB INF SYST, V2019, DOI 10.1155/2019/6971319
   Knaub J., 2007, Encyclopedia of Measurement and Statistics, P431, DOI [10.4135/9781412952644.n201, DOI 10.4135/9781412952644.N201]
   Knorr S., 2018, PROC 15 ACM SIGGRAPH, P1
   Le Callet P., 2013, PROC OUTPUT 5 QUALIN
   Li BJ, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.02116
   Lo WC, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P211, DOI 10.1145/3083187.3083219
   Lopes F, 2018, PROC SPIE, V10752, DOI 10.1117/12.2321679
   Min X., 2015, VIS COMMUN IMAGE PRO
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P6054, DOI 10.1109/TIP.2020.2988148
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P3805, DOI 10.1109/TIP.2020.2966082
   Muñoz L, 2020, DIGIT SIGNAL PROCESS, V100, DOI 10.1016/j.dsp.2020.102706
   Nasrabadi AT, 2019, PROCEEDINGS OF THE 10TH ACM MULTIMEDIA SYSTEMS CONFERENCE (ACM MMSYS'19), P273, DOI 10.1145/3304109.3325812
   Orduna M, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES WORKSHOPS (VRW 2020), P683, DOI [10.1109/VRW50115.2020.00-83, 10.1109/VRW50115.2020.00192]
   Orduna M, 2020, IEEE T CONSUM ELECTR, V66, P22, DOI 10.1109/TCE.2019.2957987
   Pan Gao, 2022, IEEE Transactions on Multimedia, V24, P1, DOI 10.1109/TMM.2020.3044458
   Perez P., 2019, 2019 11 INT C QUAL M, P1
   Perez Pablo, 2018, 2018 10 INT C QUAL M, P1, DOI [DOI 10.1109/QOMEX.2018.8463377, 10.1109/qomex.2018.8463377]
   Perkis Andrew, 2020, ARXIV200707032
   Pinson MH, 2015, IEEE SIGNAL PROC MAG, V32, P101, DOI 10.1109/MSP.2013.2292535
   P├rez J., 2020, PROC INT C QUAL MULT, P56
   Raake A, 2014, T-LAB SER TELECOMMUN, P11, DOI 10.1007/978-3-319-02681-7_2
   Reiter U., 2014, PROC QUAL EXPERIENCE, P45
   Rodriguez D. Zegarra, 2012, P 18 ACM BRAZ S MULT
   Rohaly AM, 2000, P SOC PHOTO-OPT INS, V4067, P742, DOI 10.1117/12.386632
   Singh Akshay, 2017, 2017 Conference on Lasers and Electro-Optics Europe & European Quantum Electronics Conference (CLEO/Europe-EQEC), DOI 10.1109/CLEOE-EQEC.2017.8087702
   Singh A, 2019, 2019 URSI ASIA-PACIFIC RADIO SCIENCE CONFERENCE (AP-RASC), DOI [10.23919/ursiap-rasc.2019.8738217, 10.23919/URSIAP-RASC.2019.8738217]
   Singla A., 2020, ELECT IMAG
   Singla A, 2019, PROCEEDINGS OF THE 10TH ACM MULTIMEDIA SYSTEMS CONFERENCE (ACM MMSYS'19), P232, DOI 10.1145/3304109.3306218
   Singla A, 2019, 2019 26TH IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR), P1163, DOI [10.1109/vr.2019.8798291, 10.1109/VR.2019.8798291]
   Singla A, 2017, PROCEEDINGS OF THE THEMATIC WORKSHOPS OF ACM MULTIMEDIA 2017 (THEMATIC WORKSHOPS'17), P511, DOI 10.1145/3126686.3126768
   Singla Ashutosh, 2018, ELECT IMAGING, V14, DOI 10.2352/ISSN.2470-1173. 2018.14.HVEI-525
   Song JR, 2020, IEEE T MULTIMEDIA, V22, P2366, DOI 10.1109/TMM.2019.2957976
   Staelens N, 2010, IEEE T BROADCAST, V56, P458, DOI 10.1109/TBC.2010.2067710
   Stone III WB, 2017, THESIS IOWA STATE U
   Sun Wei., 2019, Proceedings of the ACM on Human-Computer Interaction 3.CSCW, P1
   Tavakoli S, 2015, SIGNAL PROCESS-IMAGE, V39, P432, DOI 10.1016/j.image.2015.05.001
   Tran H.T., 2017, IEEE INT WORKSH MULT, P1
   Tse A., 2017, P 2017 CHI C HUM FAC, P2967, DOI DOI 10.1145/3027063.3053225
   Upenik E, 2016, PICT COD SYMP
   van der Hooft J, 2019, 2019 IFIP/IEEE SYMPOSIUM ON INTEGRATED NETWORK AND SERVICE MANAGEMENT (IM), P381
   Vishwanath B., 2016, PROC JVET D0072 4 M
   Vlad R., 2013, 2013 3DTV VIS DEPTH, P1, DOI DOI 10.1109/3DTV.2013.6676647
   Wang YB, 2020, IEEE I C VI COM I PR, P483, DOI 10.1109/vcip49819.2020.9301871
   Wertheim A. H., 2001, TNOTM01A066 HUM FACT
   Winter B., 2013, ARXIV 13085499
   Xiongkuo Min, 2014, 2014 Sixth International Workshop on Quality of Multimedia Experience (QoMEX), P153, DOI 10.1109/QoMEX.2014.6982312
   Xu M, 2020, IEEE J-STSP, V14, P5, DOI 10.1109/JSTSP.2020.2966864
   Yu M, 2015, 2015 IEEE International Symposium on Mixed and Augmented Reality, P31, DOI 10.1109/ISMAR.2015.12
   Yule S., 2016, MPEG Joint Video Explor. Team, V116
   Zakharchenko V, 2016, PROC SPIE, V9970, DOI 10.1117/12.2235885
   Zhai GT, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2757-1
   Zhang YX, 2018, IEEE T BROADCAST, V64, P461, DOI 10.1109/TBC.2018.2811627
NR 105
TC 18
Z9 18
U1 1
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 3087
EP 3100
DI 10.1109/TMM.2021.3093717
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2A3LX
UT WOS:000809408000032
OA Green Submitted, Green Published, Green Accepted, hybrid
DA 2024-07-18
ER

PT J
AU Jiang, XK
   Jin, LBA
   Rao, AY
   Xu, LN
   Lin, DH
AF Jiang, Xuekun
   Jin, Libiao
   Rao, Anyi
   Xu, Linning
   Lin, Dahua
TI Jointly Learning the Attributes and Composition of Shots for Boundary
   Detection in Videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Shot type; boundary detection; cinematic style
ID VISUAL FEATURES; CLASSIFICATION
AB In film making, shot has a profound influence on how the movie content is delivered and how the audiences are echoed, where different emotions and contents can be delivered through well-designed camera movements or shot editing. Therefore, in pursuit of high-level understanding of long videos, accurate shot detection from untrimmed videos should be considered as the first and the most fundamental step. Existing approaches address this problem based on the visual differences and content transitions between consecutive frames, while ignoring intrinsic shot attributes, viz., camera movements, scales, and viewing angles, which essentially reveal how each shot is created. In this work, we propose a new learning framework (SCTSNet) for shot boundary detection by jointly recognizing the attributes and composition of shots in videos. To facilitate the analysis of shots and the evaluation of shot detection models, we collect a large-scale shot boundary dataset MovieShots2, which contains 15K shots from 282 movie clips. It is richly annotated with the temporal boundary between consecutive shots and individual shot attributes, including camera movements, scales, and viewing angles, which are the three most distinct shot attributes. Our experiments show that the joint learning framework can significantly boost the boundary detection performance, surpassing the previous scores by a large margin. SCTSNet improves shot boundary detection AP from 0.65 to 0.77, pushing the performance to a new level.
C1 [Jiang, Xuekun; Jin, Libiao] Commun Univ China, State Key Lab Media Convergence & Commun, Beijing 100024, Peoples R China.
   [Rao, Anyi; Xu, Linning; Lin, Dahua] Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Peoples R China.
C3 Communication University of China; Chinese University of Hong Kong
RP Rao, AY (corresponding author), Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Peoples R China.
EM xkjiang@cuc.edu.cn; libiao@cuc.edu.cn; anyirao@ie.cuhk.edu.hk;
   linningxu@ie.cuhk.edu.hk; dhlin@ie.cuhk.edu.hk
RI Lin, Dahua/W-6576-2019; Rao, Anyi/IXE-1082-2023
OI Lin, Dahua/0000-0002-8865-7896; Rao, Anyi/0000-0003-1004-7753
FU SenseTime Collaborative Grant on Large-scale Multi-modality Analysis
   (CUHK) [TS1610626, TS1712093]; General Research Fund (GRF) of Hong Kong
   [14203518, 14205719]; Innovation and Technology Support Program (ITSP)
   Tier 2 [ITS/431/18F]; National Key R&D Program of China
   [2017YFB1402203-2]
FX This work was supported in part by the SenseTime Collaborative Grant on
   Large-scale Multi-modality Analysis (CUHK Agreement TS1610626 and No.
   TS1712093), in part by the General Research Fund (GRF) of Hong Kong
   under Grants 14203518 and 14205719, in part by the Innovation and
   Technology Support Program (ITSP) Tier 2, ITS/431/18F, and in part by
   the National Key R&D Program of China under Grant 2017YFB1402203-2.
CR [Anonymous], 1997, NEURAL COMPUT
   Baber J, 2013, INT J PATTERN RECOGN, V27, DOI 10.1142/S0218001413550070
   Beevi Y., 2009, INT J SIGNAL PROCESS, V2, P13
   Bhattacharya S, 2014, IEEE T MULTIMEDIA, V16, P686, DOI 10.1109/TMM.2014.2300833
   Boreczky JS, 1996, J ELECTRON IMAGING, V5, P122, DOI 10.1117/12.238675
   Canini L, 2013, MULTIMED TOOLS APPL, V62, P51, DOI 10.1007/s11042-011-0916-9
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Castellano B., 2018, Pyscenedetect: Intelligent scene cut detection and video splitting tool
   Chen JW, 2019, AAAI CONF ARTIF INTE, P8167
   Cherif I., 2007, 2007 9 INT S SIGN PR, P1
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Dong JF, 2018, IEEE T MULTIMEDIA, V20, P3377, DOI 10.1109/TMM.2018.2832602
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Duan LY, 2005, IEEE T MULTIMEDIA, V7, P1066, DOI 10.1109/TMM.2005.858395
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Ghadiyaram D, 2019, PROC CVPR IEEE, P12038, DOI 10.1109/CVPR.2019.01232
   Giannetti Louis D., 1999, Understanding Movies, V1
   Graves A, 2005, NEURAL NETWORKS, V18, P602, DOI 10.1016/j.neunet.2005.06.042
   Gygli M, 2016, PROC CVPR IEEE, P1001, DOI 10.1109/CVPR.2016.114
   Hanjalic A, 2002, IEEE T CIRC SYST VID, V12, P90, DOI 10.1109/76.988656
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou R, 2017, IEEE I CONF COMP VIS, P5823, DOI 10.1109/ICCV.2017.620
   Hui Jiang, 2011, 2011 Proceedings of IEEE International Conference on Computer Science and Automation Engineering (CSAE), P757, DOI 10.1109/CSAE.2011.5952612
   Janwe NJ, 2013, 2013 IEEE SECOND INTERNATIONAL CONFERENCE ON IMAGE INFORMATION PROCESSING (ICIIP), P476, DOI 10.1109/ICIIP.2013.6707637
   Jiangyue Xia, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P174, DOI 10.1007/978-3-030-58610-2_11
   Karakostas L, 2020, INFORM SCIENCES, V506, P273, DOI 10.1016/j.ins.2019.08.011
   Koprinska I, 2001, SIGNAL PROCESS-IMAGE, V16, P477, DOI 10.1016/S0923-5965(00)00011-4
   Li L, 2009, LECT NOTES COMPUT SC, V5879, P923, DOI 10.1007/978-3-642-10467-1_83
   Lin JC, 2018, IEEE T MULTIMEDIA, V20, P3123, DOI 10.1109/TMM.2018.2820904
   Lindeberg T., 2012, SCHOLARPEDIA, V7, P10491, DOI [10.4249/scholarpedia.10491, DOI 10.4249/SCHOLARPEDIA.10491]
   Lu Y., 2017, P IEEE INT C COMP VI, P2344
   Lu ZM, 2013, IEEE T IMAGE PROCESS, V22, P5136, DOI 10.1109/TIP.2013.2282081
   Luo B, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1187, DOI 10.1145/2733373.2806313
   NIST, TREC VID RETR EV HOM
   Priya GGL, 2012, PROC TECH, V1, P247, DOI 10.1016/j.protcy.2012.10.030
   Qingqiu Huang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P709, DOI 10.1007/978-3-030-58548-8_41
   Rao A., 2020, P IEEE CVF C COMP VI, P10146
   Rao A., 2020, PROC EUR C COMPUT VI, P1066
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Svanera M., 2015, 2015 13th International Workshop on Content-Based Multimedia Indexing (CBMI), P1, DOI DOI 10.1109/CBMI.2015.7153627
   Tippaya S, 2017, IEEE ACCESS, V5, P12563, DOI 10.1109/ACCESS.2017.2717998
   Tong W., 2015, 2015 IEEE INT S BROA, P1
   Tran D, 2019, IEEE I CONF COMP VIS, P5551, DOI 10.1109/ICCV.2019.00565
   Vaswani A, 2017, ADV NEUR IN, V30
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Wang HL, 2009, IEEE T CIRC SYST VID, V19, P1529, DOI 10.1109/TCSVT.2009.2022705
   Wang HL, 2003, J VIS COMMUN IMAGE R, V14, P150, DOI 10.1016/S1047-3203(03)00019-1
   Wang LM, 2019, IEEE T PATTERN ANAL, V41, P2740, DOI 10.1109/TPAMI.2018.2868668
   Wu LF, 2019, IEEE ACCESS, V7, P77268, DOI 10.1109/ACCESS.2019.2922038
   Xu JW, 2016, 2016 30TH ANNIVERSARY OF VISUAL COMMUNICATION AND IMAGE PROCESSING (VCIP)
   Xu M, 2011, IEEE IMAGE PROC, DOI 10.1109/ICIP.2011.6116510
   Yuan JH, 2007, IEEE T CIRC SYST VID, V17, P168, DOI 10.1109/TCSVT.2006.888023
   Zhang CY, 2016, INT C PATT RECOG, P2924, DOI 10.1109/ICPR.2016.7900081
   Zhao B, 2018, PROC CVPR IEEE, P7405, DOI 10.1109/CVPR.2018.00773
NR 55
TC 6
Z9 6
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 3049
EP 3059
DI 10.1109/TMM.2021.3092143
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2A3LX
UT WOS:000809408000029
DA 2024-07-18
ER

PT J
AU Liang, CH
   Chen, YA
   Liu, YC
   Hsu, WH
AF Liang, Chih-Hung
   Chen, Yu-An
   Liu, Yueh-Cheng
   Hsu, Winston H.
TI Raw Image Deblurring
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image restoration; Sensors; Kernel; Cameras; Image sensors; Image color
   analysis; Videos; Raw image deblurring; image deblurring; image quality
   enhancement
ID NETWORK
AB Deep learning-based blind image deblurring plays an essential role in solving image blur since all existing kernels are limited in modeling the real world blur. Thus far, researchers focus on powerful models to handle the deblurring problem and achieve decent results. For this work, in a new aspect, we discover the great opportunity for image enhancement (e.g., deblurring) directly from RAW images and investigate novel neural network structures benefiting RAW-based learning. However, to the best of our knowledge, there is no available RAW image deblurring dataset. Therefore, we built a new dataset containing both RAW images and processed sRGB images and design a new model to utilize the unique characteristics of RAW images. The proposed deblurring model, trained solely from RAW images, achieves the state-of-art performance and outweighs those trained on processed sRGB images. Furthermore, with fine-tuning, the proposed model, trained on our new dataset, can generalize to other sensors. Additionally, by a series of experiments, we demonstrate that existing deblurring models can also be improved by training on the RAW images in our new dataset. Ultimately, we show a new venue for further opportunities based on the devised novel raw-based deblurring method and the brand-new Deblur-RAW dataset.
C1 [Liang, Chih-Hung; Chen, Yu-An; Liu, Yueh-Cheng; Hsu, Winston H.] Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei, Taiwan.
C3 National Taiwan University
RP Hsu, WH (corresponding author), Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei, Taiwan.
EM r06922057@cmlab.csie.ntu.edu.tw; r07922076@cmlab.csie.ntu.edu.tw;
   ycliu0610@gmail.com; whsu@ntu.edu.tw
OI HSU, WINSTON/0000-0002-3330-0638
FU Ministry of Science and Technology, Taiwan [MOST 109-2634-F-002-032];
   Qualcomm Technologies, Inc.
FX This work was supported in part by the Ministry of Science and
   Technology, Taiwan, under Grant MOST 109-2634-F-002-032 and Qualcomm
   Technologies, Inc. The associate editor coordinating the review of this
   manuscript and approving it for publicationwas Prof. Junwei Han.
CR Caballero J, 2017, PROC CVPR IEEE, P2848, DOI 10.1109/CVPR.2017.304
   Cai XX, 2018, MULTIMEDIA SYST, V24, P597, DOI 10.1007/s00530-018-0585-x
   Chen C, 2018, PROC CVPR IEEE, P3291, DOI 10.1109/CVPR.2018.00347
   Chen J, 2018, PROC CVPR IEEE, P6286, DOI 10.1109/CVPR.2018.00658
   Gong D, 2017, PROC CVPR IEEE, P3806, DOI 10.1109/CVPR.2017.405
   Hasinoff SW, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980254
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Kim TH, 2013, IEEE I CONF COMP VIS, P3160, DOI 10.1109/ICCV.2013.392
   Kim TH, 2014, PROC CVPR IEEE, P2766, DOI 10.1109/CVPR.2014.348
   Kupyn O, 2018, PROC CVPR IEEE, P8183, DOI 10.1109/CVPR.2018.00854
   Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Liao X, 2017, 2017 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Lu BY, 2019, PROC CVPR IEEE, P10217, DOI 10.1109/CVPR.2019.01047
   Luan FJ, 2017, PROC CVPR IEEE, P6997, DOI 10.1109/CVPR.2017.740
   Michaeli T, 2014, LECT NOTES COMPUT SC, V8691, P783, DOI 10.1007/978-3-319-10578-9_51
   Nah S, 2019, IEEE COMPUT SOC CONF, P1996, DOI 10.1109/CVPRW.2019.00251
   Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35
   Plötz T, 2017, PROC CVPR IEEE, P2750, DOI 10.1109/CVPR.2017.294
   Schwartz E, 2019, IEEE T IMAGE PROCESS, V28, P912, DOI 10.1109/TIP.2018.2872858
   Su SC, 2017, PROC CVPR IEEE, P237, DOI 10.1109/CVPR.2017.33
   Sun J, 2015, PROC CVPR IEEE, P769, DOI 10.1109/CVPR.2015.7298677
   Sun LH, 2013, PROCEEDINGS OF THE 2013 INTERNATIONAL WORKSHOP ON COMPUTER SCIENCE IN SPORTS, P1
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Tao X, 2018, PROC CVPR IEEE, P8174, DOI 10.1109/CVPR.2018.00853
   Trimeche M, 2005, PROC SPIE, V5674, P169, DOI 10.1117/12.586598
   Xu Li, 2014, P ANN C NEUR INF PRO, P1790
   Xu XY, 2019, PROC CVPR IEEE, P1723, DOI 10.1109/CVPR.2019.00182
   Yang SD, 2019, IEEE INT CONF COMP V, P4521, DOI 10.1109/ICCVW.2019.00553
   Yang X, 2019, IEEE T MULTIMEDIA, V21, P328, DOI 10.1109/TMM.2018.2863602
   Yu X, 2014, IEEE T MULTIMEDIA, V16, P1510, DOI 10.1109/TMM.2014.2321734
   Zhang HG, 2019, PROC CVPR IEEE, P5971, DOI 10.1109/CVPR.2019.00613
   Zhang JW, 2018, PROC CVPR IEEE, P2521, DOI 10.1109/CVPR.2018.00267
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang XN, 2019, PROC CVPR IEEE, P3757, DOI 10.1109/CVPR.2019.00388
   Zhen R., 2013, THESIS U NOTRE DAME
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 38
TC 18
Z9 18
U1 7
U2 63
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 61
EP 72
DI 10.1109/TMM.2020.3045303
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YK9LD
UT WOS:000745524300005
DA 2024-07-18
ER

PT J
AU Lin, SH
   Wu, WH
   Wu, S
   Xu, Y
   Wong, HS
AF Lin, Sihao
   Wu, Wenhao
   Wu, Si
   Xu, Yong
   Wong, Hau-San
TI Unreliable-to-Reliable Instance Translation for Semi-Supervised
   Pedestrian Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Data models; Training; Reliability; Detectors; Task analysis; Semantics;
   Visualization; Generative adversarial network; image-to-image
   translation; pedestrian detection; semi-supervised learning
ID PERSON REIDENTIFICATION; SCENE
AB Generating realistic pedestrian instances in a semi-supervised setting is promising but challenging due to the limited labeled data. We propose an unreliable-to-reliable instance translation model (Un2Reliab) conditioned on unreliable instances which poorly align with pedestrians. Un2Reliab mainly consists of an encoder-decoder-like generative network and a discriminative network, which are jointly trained in a minimax game. We adopt regularization to ensure that the synthesized instances are semantically similar to the corresponding ground truth. Furthermore, to preserve the identities of persons, we propose another regularization to ensure that the synthesized instances associated with the same person should be consistent in appearance. As a result, Un2Reliab learns to restore the missing parts of the original instances. As a side benefit, the synthesized instances are brought into better alignment. Inclusion of the synthesized data improves both the diversity and quality of training data, which eventually leads to better generalization performance. Extensive experiments indicate that Un2Reliab is able to synthesize high-fidelity pedestrian instances and improve the previous state-of-the-art results on multiple semi-supervised pedestrian detection benchmarks.
C1 [Lin, Sihao; Wu, Wenhao; Wu, Si; Xu, Yong] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Guangdong, Peoples R China.
   [Wu, Si; Wong, Hau-San] City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
C3 South China University of Technology; City University of Hong Kong
RP Wu, S (corresponding author), South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Guangdong, Peoples R China.; Wong, HS (corresponding author), City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
EM linsihao6@gmail.com; wenhaowu.chn@gmail.com; ez.wusi@gmail.com;
   yxu@scut.edu.cn; cshswong@cityu.edu.hk
OI WONG, Hau-San/0000-0002-1530-7529
FU National Natural Science Foundation of China [62072189]; Research Grants
   Council of the Hong Kong Special Administration Region [CityU 11201220];
   Natural Science Foundation of Guangdong Province [2020A1515010484]
FX This work was supported in part by the National Natural Science
   Foundation of China under Project 62072189, in part by the Research
   Grants Council of the Hong Kong Special Administration Region under
   Project CityU 11201220, and in part by the Natural Science Foundation of
   Guangdong Province under Project 2020A1515010484. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Houqiang Li.
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   [Anonymous], 2016, PROC INT C DIGIT IMA
   [Anonymous], 2010, INT J COMPUT VISION, DOI [DOI 10.1007/s11263-009-0275-4, 10.1007/s11263-009-0275-4]
   Brazil G, 2019, PROC CVPR IEEE, P7224, DOI 10.1109/CVPR.2019.00740
   Brazil G, 2017, IEEE I CONF COMP VIS, P4960, DOI 10.1109/ICCV.2017.530
   Cai ZW, 2016, LECT NOTES COMPUT SC, V9908, P354, DOI 10.1007/978-3-319-46493-0_22
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dollár P, 2012, IEEE T PATTERN ANAL, V34, P743, DOI 10.1109/TPAMI.2011.155
   Dollár P, 2009, PROC CVPR IEEE, P304, DOI 10.1109/CVPRW.2009.5206631
   Du XZ, 2017, IEEE WINT CONF APPL, P953, DOI 10.1109/WACV.2017.111
   Fu C.-Y., 2017, DSSD: Deconvolutional Single Shot Detector
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Hattori H, 2015, PROC CVPR IEEE, P3819, DOI 10.1109/CVPR.2015.7299006
   Huang S., 2017, P IEEE C COMPUTER VI, P2243
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Joseph RK, 2016, CRIT POL ECON S ASIA, P1
   Kingma D. P., 2014, arXiv
   Li JN, 2018, IEEE T MULTIMEDIA, V20, P985, DOI 10.1109/TMM.2017.2759508
   Liu M.-Y., 2016, P ADV NEUR INF PROC, P469
   Liu W, 2018, LECT NOTES COMPUT SC, V11218, P643, DOI 10.1007/978-3-030-01264-9_38
   Liu W, 2019, PROC CVPR IEEE, P5182, DOI 10.1109/CVPR.2019.00533
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Mariani G., 2018, Bagan: Data augmentation with balancing gan
   Meng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3401, DOI 10.1109/CVPR.2011.5995698
   Miyato T, 2018, INT C LEARN REPR
   Murez Z, 2018, PROC CVPR IEEE, P4500, DOI 10.1109/CVPR.2018.00473
   Ouyang X, 2018, PEDESTRIAN SYNTHESIS
   Pang JB, 2011, IEEE T IMAGE PROCESS, V20, P1388, DOI 10.1109/TIP.2010.2103951
   Qian YQ, 2020, IEEE T MULTIMEDIA, V22, P421, DOI 10.1109/TMM.2019.2929949
   Radford A., 2015, ARXIV
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ros G, 2016, PROC CVPR IEEE, P3234, DOI 10.1109/CVPR.2016.352
   Rosenberg C, 2005, WACV 2005: SEVENTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P29
   Simonyan K, 2015, IEEE INT C ICLR
   Stalder S, 2010, LECT NOTES COMPUT SC, V6311, P369, DOI 10.1007/978-3-642-15549-9_27
   Tang YB, 2019, PR MACH LEARN RES, V102, P457
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang M, 2012, PROC CVPR IEEE, P3274, DOI 10.1109/CVPR.2012.6248064
   Wang SG, 2018, IEEE T MULTIMEDIA, V20, P3148, DOI 10.1109/TMM.2018.2829602
   Wang XG, 2014, IEEE T PATTERN ANAL, V36, P361, DOI 10.1109/TPAMI.2013.124
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wu S, 2020, IEEE T IMAGE PROCESS, V29, P1562, DOI 10.1109/TIP.2019.2944306
   Wu S, 2018, IEEE T CIRC SYST VID, V28, P1595, DOI 10.1109/TCSVT.2017.2672686
   Wu S, 2018, IEEE T IMAGE PROCESS, V27, P1418, DOI 10.1109/TIP.2017.2779271
   Wu Y, 2019, IEEE T IMAGE PROCESS, V28, P2872, DOI 10.1109/TIP.2019.2891895
   Wu Y, 2018, PROC CVPR IEEE, P5177, DOI 10.1109/CVPR.2018.00543
   Xiang Y, 2015, PROC CVPR IEEE, P1903, DOI 10.1109/CVPR.2015.7298800
   Zeng XY, 2014, LECT NOTES COMPUT SC, V8691, P472, DOI 10.1007/978-3-319-10578-9_31
   Zhang LL, 2016, LECT NOTES COMPUT SC, V9906, P443, DOI 10.1007/978-3-319-46475-6_28
   Zhang SS, 2018, PROC CVPR IEEE, P6995, DOI 10.1109/CVPR.2018.00731
   Zhang SS, 2018, IEEE T PATTERN ANAL, V40, P973, DOI 10.1109/TPAMI.2017.2700460
   Zhang SS, 2017, PROC CVPR IEEE, P4457, DOI 10.1109/CVPR.2017.474
   Zhang XW, 2018, IEEE T IMAGE PROCESS, V27, P3703, DOI 10.1109/TIP.2018.2818018
   Zhang X, 2015, NEUROCOMPUTING, V149, P800, DOI 10.1016/j.neucom.2014.07.054
   Zhou CL, 2018, LECT NOTES COMPUT SC, V11205, P138, DOI 10.1007/978-3-030-01246-5_9
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 59
TC 3
Z9 3
U1 1
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 728
EP 739
DI 10.1109/TMM.2021.3058546
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YW5XZ
UT WOS:000753488100017
DA 2024-07-18
ER

PT J
AU Lv, CL
   Lin, WS
   Zhao, BQ
AF Lv, Chenlei
   Lin, Weisi
   Zhao, Baoquan
TI Voxel Structure-Based Mesh Reconstruction From a 3D Point Cloud
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Optimization; Surface reconstruction; Image
   reconstruction; Noise reduction; Measurement; Reconstruction algorithms;
   Mesh reconstruction; intrinsic metric; voxel structure; isotropic
   property
ID SURFACE RECONSTRUCTION; REPAIR
AB Mesh reconstruction from a 3D point cloud is an important topic in the fields of computer graphic, computer vision, and multimedia analysis. In this paper, we propose a voxel structure-based mesh reconstruction framework. It provides the intrinsic metric to improve the accuracy of local region detection. Based on the detected local regions, an initial reconstructed mesh can be obtained. With the mesh optimization in our framework, the initial reconstructed mesh is optimized into an isotropic one with the important geometric features such as external and internal edges. The experimental results indicate that our framework shows great advantages over peer ones in terms of mesh quality, geometric feature keeping, and processing speed. The source code of the proposed method is publicly available(1).
C1 [Lv, Chenlei; Lin, Weisi; Zhao, Baoquan] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University
RP Lin, WS (corresponding author), Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
EM leilv@mail.bnu.edu.cn; wslin@ntu.edu.sg; bqzhao@ntu.edu.sg
RI Lin, Wei/D-3353-2012; Lin, Weisi/A-8011-2012; Lin, Weisi/A-3696-2011
OI Lin, Weisi/0000-0001-9866-1947; Chenlei, Lv/0000-0002-8203-3118
FU Ministry of Education, Singapore, under its Tier-2 Fund
   [MOE2016-T2-2-057(S)]
FX This work was supported by the Ministry of Education, Singapore, under
   its Tier-2 Fund MOE2016-T2-2-057(S). The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Zhu Liu.
CR Agudo A, 2019, IEEE T MULTIMEDIA, V21, P821, DOI 10.1109/TMM.2018.2870081
   Ahn JK, 2013, IEEE T MULTIMEDIA, V15, P485, DOI 10.1109/TMM.2012.2235417
   Alexa M, 2001, IEEE VISUAL, P21, DOI 10.1109/VISUAL.2001.964489
   ALEXA M., 2004, Proceedings of Eurographics Symposium on Point-Based Graphics 2004, P149, DOI DOI 10.2312/SPBG/SPBG04/149-155
   Amenta N., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P415, DOI 10.1145/280814.280947
   Amenta N, 2000, P 16 ANN S COMPUTATI, P213
   Amenta Nina, 2001, P 6 ACM S SOL MOD AP
   BLACKER TD, 1991, INT J NUMER METH ENG, V32, P811, DOI 10.1002/nme.1620320410
   Boltcheva D, 2017, COMPUT AIDED DESIGN, V90, P123, DOI 10.1016/j.cad.2017.05.011
   Botsch M., 2004, PROC EUROGRAPHICS AC, P185, DOI [10.1145/1057432.1057457, DOI 10.1145/1057432.1057457]
   Bouaziz Sofien., 2016, SIGGRAPH ASIA 2016 Courses, P1
   Chen ZG, 2018, COMPUT AIDED DESIGN, V102, P12, DOI 10.1016/j.cad.2018.04.010
   Cheng I, 2006, IEEE T MULTIMEDIA, V8, P550, DOI 10.1109/TMM.2006.870722
   Cohen-Steiner D, 2004, VISUAL COMPUT, V20, P4, DOI 10.1007/s00371-003-0217-z
   Corsini M, 2007, IEEE T MULTIMEDIA, V9, P247, DOI 10.1109/TMM.2006.886261
   De Floriani L, 2004, COMPUT AIDED DESIGN, V36, P141, DOI 10.1016/S0010-4485(03)00058-7
   Digne J, 2011, COMPUT GRAPH FORUM, V30, P1630, DOI 10.1111/j.1467-8659.2011.01848.x
   Du XY, 2018, COMPUT GRAPH FORUM, V37, P343, DOI 10.1111/cgf.13329
   Dunyach M., 2013, EUROGRAPHICS
   Fan HQ, 2010, IEEE T VIS COMPUT GR, V16, P312, DOI 10.1109/TVCG.2009.70
   Fei Y, 2014, COMPUT GRAPH-UK, V40, P1, DOI 10.1016/j.cag.2014.01.002
   Frey PJ, 1999, INT J NUMER METH ENG, V45, P101
   Fuhrmann S, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601163
   Grossmann R, 2002, IEEE T PATTERN ANAL, V24, P433, DOI 10.1109/34.993552
   Guan B., 2020, MULTIMEDIA TOOLS APP, V79, p20 561
   Hétroy F, 2011, COMPUT AIDED DESIGN, V43, P101, DOI 10.1016/j.cad.2010.09.012
   Jeong SW, 2017, IEEE T MULTIMEDIA, V19, P2692, DOI 10.1109/TMM.2017.2710802
   Jiang RQ, 2018, IEEE T MULTIMEDIA, V20, P55, DOI 10.1109/TMM.2017.2723244
   Ju T, 2004, ACM T GRAPHIC, V23, P888, DOI 10.1145/1015706.1015815
   Kazhdan M., 2006, P 4 EUR S GEOM PROC, P61, DOI DOI 10.2312/SGP/SGP06/061-070
   Kazhdan M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487237
   Kumar L., 2018, Int. J. Eng. Technol, V7, P220, DOI [10.14419/ijet.v7i2.13.11690, DOI 10.14419/IJET.V7I2.13.11690]
   Kuo CC, 2005, COMPUT AIDED DESIGN, V37, P825, DOI 10.1016/j.cad.2004.09.011
   Lafarge F, 2013, COMPUT GRAPH FORUM, V32, P225, DOI 10.1111/cgf.12042
   Lafarge F, 2012, INT J COMPUT VISION, V99, P69, DOI 10.1007/s11263-012-0517-8
   Lavoué G, 2010, IEEE T MULTIMEDIA, V12, P636, DOI 10.1109/TMM.2010.2060475
   Levin D, 2004, MATH VISUAL, P37
   Lévy B, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778856
   Liu SG, 2019, IEEE T MULTIMEDIA, V21, P690, DOI 10.1109/TMM.2018.2864576
   Liu Y, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1559755.1559758
   Luo CJ, 2018, COMPUT GRAPH FORUM, V37, P325, DOI 10.1111/cgf.13293
   Lv CL, 2019, PATTERN RECOGN, V88, P458, DOI 10.1016/j.patcog.2018.12.006
   Mérigot Q, 2011, IEEE T VIS COMPUT GR, V17, P743, DOI 10.1109/TVCG.2010.261
   Moenning C., 2003, Comput. Lab., Tech. Rep., V565, P1
   Nagai Y, 2009, COMPUT GRAPH FORUM, V28, P1339, DOI 10.1111/j.1467-8659.2009.01511.x
   Ohtake Y., 2005, P ACMEG S GEOMETRY P, P149
   Peethambaran J, 2015, COMPUT AIDED DESIGN, V58, P62, DOI 10.1016/j.cad.2014.08.021
   Ray N, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275092
   Sahillioglu Y, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2893477
   Samir A., 2017, ACM T GRAPHIC, V36
   Schnabel R., 2006, S POINT BAS GRAPH 20, P111
   Shewchuk JR, 2002, COMP GEOM-THEOR APPL, V22, P21, DOI 10.1016/S0925-7721(01)00047-5
   Surazhsky V, 2005, ACM T GRAPHIC, V24, P553, DOI 10.1145/1073204.1073228
   Wang JN, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P415
   Wang J, 2019, COMPUT AIDED DESIGN, V114, P133, DOI 10.1016/j.cad.2019.05.027
   Wang PS, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980232
   Wang W, 2019, COMPUT GRAPH-UK, V84, P144, DOI 10.1016/j.cag.2019.08.002
   Wang YQ, 2019, IEEE T VIS COMPUT GR, V25, P2430, DOI 10.1109/TVCG.2018.2837115
   Yadav SK, 2019, IEEE T VIS COMPUT GR, V25, P2304, DOI 10.1109/TVCG.2018.2828818
   Yan DM, 2009, COMPUT GRAPH FORUM, V28, P1445, DOI 10.1111/j.1467-8659.2009.01521.x
   Zhang SA, 2019, ACM COMPUT SURV, V52, DOI 10.1145/3285029
   Zhong SK, 2019, COMPUT AIDED GEOM D, V71, P43, DOI 10.1016/j.cagd.2019.04.011
   Zhou QN, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925901
NR 63
TC 25
Z9 26
U1 7
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1815
EP 1829
DI 10.1109/TMM.2021.3073265
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0I7PA
UT WOS:000779607000002
OA Green Submitted, Green Accepted
DA 2024-07-18
ER

PT J
AU Pan, ZQ
   Yuan, F
   Lei, JJ
   Li, WQ
   Ling, N
   Kwong, S
AF Pan, Zhaoqing
   Yuan, Feng
   Lei, Jianjun
   Li, Wanqing
   Ling, Nam
   Kwong, Sam
TI MIEGAN: Mobile Image Enhancement via a Multi-Module Cascade Neural
   Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image enhancement; Visualization; Image edge detection; Image color
   analysis; Dynamic range; Mobile handsets; Brightness; Mobile image
   enhancement; MIEGAN; multi-module cascade neural network; contrast loss;
   mixed loss
ID DYNAMIC HISTOGRAM EQUALIZATION
AB Visual quality of images captured by mobile devices is often inferior to that of images captured by a Digital Single Lens Reflex (DSLR) camera. This paper presents a novel generative adversarial network-based mobile image enhancement method, referred to as MIEGAN. It consists of a novel multi-module cascade generative network and a novel adaptive multi-scale discriminative network. The multi-module cascade generative network is built upon a two-stream encoder, a feature transformer, and a decoder. In the two-stream encoder, a luminance-regularizing stream is proposed to help the network focus on low-light areas. In the feature transformation module, two networks effectively capture both global and local information of an image. To further assist the generative network to generate the high visual quality images, a multi-scale discriminator is used instead of a regular single discriminator to distinguish whether an image is fake or real globally and locally. To balance the global and local discriminators, an adaptive weight allocation is proposed. In addition, a contrast loss is proposed, and a new mixed loss function is developed to improve the visual quality of the enhanced images. Extensive experiments on the popular DSLR photo enhancement dataset and MIT-FiveK dataset have verified the effectiveness of the proposed MIEGAN.
C1 [Pan, Zhaoqing; Lei, Jianjun] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Pan, Zhaoqing] Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
   [Pan, Zhaoqing; Yuan, Feng] Nanjing Univ Informat Sci & Technol, Sch Comp & Software, Nanjing 210044, Peoples R China.
   [Li, Wanqing] Univ Wollongong, Adv Multimedia Res Lab, Wollongong, NSW 2522, Australia.
   [Ling, Nam] Santa Clara Univ, Dept Comp Sci & Engn, Santa Clara, CA 95053 USA.
   [Kwong, Sam] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
C3 Tianjin University; Xidian University; Nanjing University of Information
   Science & Technology; University of Wollongong; Santa Clara University;
   City University of Hong Kong
RP Lei, JJ (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM zqpan3-c@my.cityu.edu.hk; yuanfeng@nuist.edu.cn; ijlei@tju.edu.cn;
   wanqing@uow.edu.au; nling@scu.edu; cssamk@cityu.edu.hk
RI Kwong, Sam/C-9319-2012
OI Kwong, Sam/0000-0001-7484-7261; Li, Wanqing/0000-0002-4427-2687
FU National Natural Science Foundation of China [61971232]; Natural Science
   Foundation of Jiangsu Province of China [BK20201391]; Natural Science
   Foundation of Tianjin [18ZXZNGX00110, 18JCJQJC45800]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61971232, in part by the Natural Science
   Foundation of Jiangsu Province of China under Grant BK20201391, and in
   part by the Natural Science Foundation of Tianjin under Grants
   18ZXZNGX00110 and 18JCJQJC45800.
CR Abdullah-Al-Wadud M, 2007, IEEE T CONSUM ELECTR, V53, P593, DOI 10.1109/TCE.2007.381734
   Al-Ameen, 2019, J SOFT COMPUT DECIS, V7, P7
   [Anonymous], 1995, BT601 ITUR
   Anwar S, 2019, IEEE T PATTERN ANAL, V41, P2112, DOI 10.1109/TPAMI.2018.2855177
   Bi S, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766946
   Bychkovsky V, 2011, PROC CVPR IEEE, P97
   Celik T, 2014, IEEE T IMAGE PROCESS, V23, P5298, DOI 10.1109/TIP.2014.2364537
   Celik T, 2012, IEEE T IMAGE PROCESS, V21, P145, DOI 10.1109/TIP.2011.2162419
   Celik T, 2011, IEEE T IMAGE PROCESS, V20, P3431, DOI 10.1109/TIP.2011.2157513
   Chen C, 2018, PROC CVPR IEEE, P3291, DOI 10.1109/CVPR.2018.00347
   Chen QF, 2017, IEEE I CONF COMP VIS, P2516, DOI 10.1109/ICCV.2017.273
   Chen SD, 2003, IEEE T CONSUM ELECTR, V49, P1301, DOI 10.1109/TCE.2003.1261233
   Chen YS, 2018, PROC CVPR IEEE, P6306, DOI 10.1109/CVPR.2018.00660
   Dale K, 2009, IEEE I CONF COMP VIS, P2217, DOI 10.1109/ICCV.2009.5459473
   Deng G, 2011, IEEE T IMAGE PROCESS, V20, P1249, DOI 10.1109/TIP.2010.2092441
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Fattal R, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239502, 10.1145/1276377.1276441]
   Gastal ESL, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964964
   Gharbi M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073592
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He ZW, 2020, IEEE T MULTIMEDIA, V22, P1042, DOI 10.1109/TMM.2019.2937688
   Hu B, 2019, IEEE T MULTIMEDIA, V21, P2042, DOI 10.1109/TMM.2019.2894958
   Huang GL, 2017, IEEE ICC
   Huang J, 2019, LECT NOTES COMPUT SC, V11133, P230, DOI 10.1007/978-3-030-11021-5_15
   Hwang SJ, 2012, LECT NOTES COMPUT SC, V7572, P569, DOI 10.1007/978-3-642-33718-5_41
   Ibrahim H, 2007, IEEE T CONSUM ELECTR, V53, P1752, DOI 10.1109/TCE.2007.4429280
   Ignatov A, 2018, IEEE COMPUT SOC CONF, P804, DOI 10.1109/CVPRW.2018.00112
   Ignatov A, 2017, IEEE I CONF COMP VIS, P3297, DOI 10.1109/ICCV.2017.355
   Iizuka S, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925974
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karacan L, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508403
   Kaur, 2020, MULTIMED TOOLS APPL, P1
   Kim M, 2008, IEEE T CONSUM ELECTR, V54, P1389, DOI 10.1109/TCE.2008.4637632
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lee JY, 2016, PROC CVPR IEEE, P2470, DOI 10.1109/CVPR.2016.271
   Li YJ, 2017, PROC CVPR IEEE, P5892, DOI 10.1109/CVPR.2017.624
   Liu H, 2019, INFORM SCIENCES, V473, P44, DOI 10.1016/j.ins.2018.09.018
   Liu H, 2018, NEUROCOMPUTING, V282, P52, DOI 10.1016/j.neucom.2017.12.014
   Liu SF, 2016, LECT NOTES COMPUT SC, V9908, P560, DOI 10.1007/978-3-319-46493-0_34
   Liu YF, 2017, IEEE T CIRC SYST VID, V27, P1171, DOI 10.1109/TCSVT.2016.2527338
   Liu YF, 2013, INT CONF ACOUST SPEE, P2444, DOI 10.1109/ICASSP.2013.6638094
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Pang YW, 2019, IEEE T INF FOREN SEC, V14, P3322, DOI 10.1109/TIFS.2019.2916592
   Paris S, 2015, COMMUN ACM, V58, P81, DOI 10.1145/2723694
   Peng F, 2020, IEEE T MULTIMEDIA, V22, P2511, DOI 10.1109/TMM.2019.2959443
   Polesel A, 2000, IEEE T IMAGE PROCESS, V9, P505, DOI 10.1109/83.826787
   Radford A., 2016, INT C LEARN REPR
   Ramponi G, 1996, J ELECTRON IMAGING, V5, P353, DOI 10.1117/12.242618
   Ramponi G, 1998, SIGNAL PROCESS, V67, P211, DOI 10.1016/S0165-1684(98)00038-3
   Ren WQ, 2019, IEEE T IMAGE PROCESS, V28, P4364, DOI 10.1109/TIP.2019.2910412
   Ulyanov D, 2017, PROC CVPR IEEE, P4105, DOI 10.1109/CVPR.2017.437
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yan ZC, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2790296
   Zhang B, 2008, IEEE T IMAGE PROCESS, V17, P664, DOI 10.1109/TIP.2008.919949
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 60
TC 24
Z9 24
U1 0
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 519
EP 533
DI 10.1109/TMM.2021.3054509
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YW5XZ
UT WOS:000753488100001
DA 2024-07-18
ER

PT J
AU Wu, HW
   Zhou, JT
   Li, YM
AF Wu, Haiwei
   Zhou, Jiantao
   Li, Yuanman
TI Deep Generative Model for Image Inpainting With Local Binary Pattern
   Learning and Spatial Attention
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Generators; Decoding; Task analysis; Semantics;
   Image edge detection; Correlation; Image inpainting; LBP; spatial
   attention; deep learning
ID TEXTURE SYNTHESIS; COMPLETION
AB Deep learning (DL) has demonstrated its powerful capabilities in the field of image inpainting. The DL-based image inpainting approaches can produce visually plausible results, but often generate various unpleasant artifacts, especially in the boundary and highly textured regions. To tackle this challenge, in this work, we propose a new end-to-end, two-stage (coarse-to-fine) generative model through combining a local binary pattern (LBP) learning network with an actual inpainting network. Specifically, the first LBP learning network using U-Net architecture is designed to accurately predict the structural information of the missing region, which subsequently guides the second image inpainting network for better filling the missing pixels. Furthermore, an improved spatial attention mechanism is integrated into the image inpainting network, by considering the consistency not only between the known region with the generated one, but also within the generated region itself. Extensive experiments on public datasets including CelebA-HQ, Places and Paris StreetView demonstrate that our model generates better inpainting results than the state-of-the-art competing algorithms, both quantitatively and qualitatively. The source code and trained models are available at https://github.com/HighwayWu/ImageInpainting.
C1 [Wu, Haiwei; Zhou, Jiantao] Univ Macau, Fac Sci & Technol, State Key Lab Internet Things Smart City, Macau, Peoples R China.
   [Wu, Haiwei; Zhou, Jiantao] Univ Macau, Fac Sci & Technol, Dept Comp & Informat Sci, Macau, Peoples R China.
   [Li, Yuanman] Shenzhen Univ, Coll Elect & Informat Engn, Shenzhen 518052, Guangdong, Peoples R China.
C3 University of Macau; University of Macau; Shenzhen University
RP Zhou, JT (corresponding author), Univ Macau, Fac Sci & Technol, State Key Lab Internet Things Smart City, Macau, Peoples R China.; Zhou, JT (corresponding author), Univ Macau, Fac Sci & Technol, Dept Comp & Informat Sci, Macau, Peoples R China.
EM yc07912@umac.mo; jtzhou@umac.mo; yuanmanx.li@gmail.com
OI Wu, Haiwei/0000-0001-8807-0254
FU Macau Science and Technology Development Fund [SKL-IOTSC-2021-2023,
   077/2018/A2, 0060/2019/A1]; Research Committee at University of Macau
   [MYRG2018-00029-FST, MYRG2019-00023-FST, 62001304, 61971476]; Guangdong
   Basic and Applied Basic Research Foundation [2019A1515110410]
FX This work was supported in part by Macau Science and Technology
   Development Fund under Grants SKL-IOTSC-2021-2023, 077/2018/A2, and
   0060/2019/A1, in part by the Research Committee at University of Macau
   under Grants MYRG2018-00029-FST and MYRG2019-00023-FST, in part by
   theNatural Science Foundation of China underGrants 62001304 and
   61971476, and in part by Guangdong Basic and Applied Basic Research
   Foundation under Grant 2019A1515110410. The Associate Editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Michael Tourapis.
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Bertalmio M, 2003, IEEE T IMAGE PROCESS, V12, P882, DOI 10.1109/TIP.2003.815261
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Cheung CH, 2018, IEEE T MULTIMEDIA, V20, P1376, DOI 10.1109/TMM.2017.2772442
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   Doersch C, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185597
   Dolhansky B, 2018, PROC CVPR IEEE, P7902, DOI 10.1109/CVPR.2018.00824
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Hays J, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239455
   Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karras T, 2018, Arxiv, DOI [arXiv:1710.10196, DOI 10.48550/ARXIV.1710.10196]
   Kingma D. P., 2014, arXiv
   Ko S., 2019, PROC IEEE C COMPUT V, p11 360
   Li A, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P811
   Lie WN, 2018, IEEE T MULTIMEDIA, V20, P1075, DOI 10.1109/TMM.2017.2763319
   Ling CH, 2011, IEEE T MULTIMEDIA, V13, P292, DOI 10.1109/TMM.2010.2095000
   Liu GL, 2018, LECT NOTES COMPUT SC, V11215, P89, DOI 10.1007/978-3-030-01252-6_6
   Liu HY, 2019, IEEE I CONF COMP VIS, P4169, DOI 10.1109/ICCV.2019.00427
   Liu JY, 2018, IEEE T MULTIMEDIA, V20, P3252, DOI 10.1109/TMM.2018.2831636
   Ma YQ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3123
   Nair Vinod, 2010, INT C INT C MACHINE, P807
   Nazeri K., 2019, ARXIV190100212
   Ndjiki-Nya P, 2011, IEEE T MULTIMEDIA, V13, P453, DOI 10.1109/TMM.2011.2128862
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Ojala T, 1999, PATTERN RECOGN, V32, P477, DOI 10.1016/S0031-3203(98)00038-7
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Ren YR, 2019, IEEE I CONF COMP VIS, P181, DOI 10.1109/ICCV.2019.00027
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sajjadi MSM, 2017, IEEE I CONF COMP VIS, P4501, DOI 10.1109/ICCV.2017.481
   Simakov D., 2008, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2008.4587842
   Song Y., 2018, BRIT MACH VIS C, V97, P1
   Song YH, 2018, LECT NOTES COMPUT SC, V11206, P3, DOI [10.1007/978-3-030-01216-8_1, 10.1109/APCAP.2017.8420330]
   Sun J, 2005, ACM T GRAPHIC, V24, P861, DOI 10.1145/1073204.1073274
   Nguyen TD, 2019, IEEE T MULTIMEDIA, V21, P1345, DOI 10.1109/TMM.2018.2880954
   Ulyanov D, 2017, Arxiv, DOI arXiv:1607.08022
   Waller BM, 2013, 2013 INTERNATIONAL CONFERENCE ON SIGNAL-IMAGE TECHNOLOGY & INTERNET-BASED SYSTEMS (SITIS), P118, DOI 10.1109/SITIS.2013.30
   Wang N, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3748
   Wang Y, 2018, ADV NEUR IN, V31
   Xiao J, 2019, AAAI CONF ARTIF INTE, P354
   Xie CH, 2019, IEEE I CONF COMP VIS, P8857, DOI 10.1109/ICCV.2019.00895
   Xiong W, 2019, PROC CVPR IEEE, P5833, DOI 10.1109/CVPR.2019.00599
   Xu B, 2015, Arxiv, DOI [arXiv:1505.00853, DOI 10.48550/ARXIV.1505.00853]
   Xu L, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366158
   Yan ZY, 2018, LECT NOTES COMPUT SC, V11218, P3, DOI 10.1007/978-3-030-01264-9_1
   Yeh RA, 2017, PROC CVPR IEEE, P6882, DOI 10.1109/CVPR.2017.728
   Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Zeng YH, 2019, PROC CVPR IEEE, P1486, DOI 10.1109/CVPR.2019.00158
   Zhang BC, 2010, IEEE T IMAGE PROCESS, V19, P533, DOI 10.1109/TIP.2009.2035882
   Zhao D, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9183856
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
NR 58
TC 23
Z9 27
U1 6
U2 43
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 4016
EP 4027
DI 10.1109/TMM.2021.3111491
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 3R1UG
UT WOS:000838704400024
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yang, K
   He, ZY
   Pei, WJ
   Zhou, ZK
   Li, X
   Yuan, D
   Zhang, HJ
AF Yang, Kai
   He, Zhenyu
   Pei, Wenjie
   Zhou, Zikun
   Li, Xin
   Yuan, Di
   Zhang, Haijun
TI SiamCorners: Siamese Corner Networks for Visual Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Target tracking; Visualization; Training; Task analysis; Object
   detection; Feature extraction; Proposals; Visual tracking; Siamese
   network; top-left corner; bottom-right corner
ID CORRELATION FILTER TRACKER
AB The current Siamese network based on region proposal network (RPN) has attracted great attention in visual tracking due to its excellent accuracy and high efficiency. However, the design of the RPN involves the selection of the number, scale, and aspect ratios of anchor boxes, which will affect the applicability and convenience of the model. Furthermore, these anchor boxes require complicated calculations, such as calculating their intersection-over-union (IoU) with ground truth bounding boxes. Due to the problems related to anchor boxes, we propose a simple yet effective anchor-free tracker (named Siamese corner networks, SiamCorners), which is end-to-end trained offline on large-scale image pairs. Specifically, we introduce a modified corner pooling layer to convert the bounding box estimate of the target into a pair of corner predictions (the bottom-right and the top-left corners). By tracking a target as a pair of corners, we avoid the need to design the anchor boxes. This will make the entire tracking algorithm more flexible and simple than anchor-based trackers. In our network design, we further introduce a layer-wise feature aggregation strategy that enables the corner pooling module to predict multiple corners for a tracking target in deep networks. We then introduce a new penalty term that is used to select an optimal tracking box in these candidate corners. Finally, SiamCorners achieves experimental results that are comparable to the state-of-art tracker while maintaining a high running speed. In particular, SiamCorners achieves a 53.7% AUC on NFS30 and a 61.4% AUC on UAV123, while still running at 42 frames per second (FPS).
C1 [Yang, Kai; He, Zhenyu; Pei, Wenjie; Zhou, Zikun; Yuan, Di; Zhang, Haijun] Harbin Inst Technol, Sch Comp Sci & Technol, Shenzhen 518055, Peoples R China.
   [He, Zhenyu] Shenzhen Inst Artificial Intelligence & Robot Soc, Shenzhen 518055, Peoples R China.
   [Li, Xin] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
C3 Harbin Institute of Technology; Shenzhen Institute of Artificial
   Intelligence & Robotics for Society; Peng Cheng Laboratory
RP He, ZY; Pei, WJ (corresponding author), Harbin Inst Technol, Sch Comp Sci & Technol, Shenzhen 518055, Peoples R China.
EM yangkaik88@163.com; zhenyuhe@hit.edu.cn; wenjiecoder@outlook.com;
   18b951038@stu.hit.edu.cn; xinlihitsz@gmail.com; dyuanhit@gmail.com;
   hjzhang@hit.edu.cn
RI Pei, Wenjie/IQT-7671-2023; Yang, Kai/HOA-7144-2023; Zhang,
   Haijun/N-8470-2015; Yuan, Di/Q-6521-2019
OI Pei, Wenjie/0000-0001-8117-2696; Yang, Kai/0000-0003-0140-3111; Yuan,
   Di/0000-0001-9403-1112; Zhou, Zikun/0000-0002-2687-7762; Li,
   Xin/0000-0002-1670-1368
FU Special Research Project on COVID-19 Prevention and Control of Guangdong
   Province [2020KZDZDX1227]; Shenzhen Research Council
   [JCYJ20170413104556946, JCYJ20170413105929681]; Natural Science
   Foundation of China [62006060, U2013210, 62002241, 61972112]; Guangdong
   Basic and Applied Basic Research Foundation [2021B1515020088]; Shenzhen
   Institute of Artificial Intelligence and Robotics for Society (AIRS)
   [2019-INT021]
FX This work was supported in part by the Special Research Project on
   COVID-19 Prevention and Control of Guangdong Province under Grant
   2020KZDZDX1227, in part by Shenzhen Research Council under Grants
   JCYJ20170413104556946 and JCYJ20170413105929681, in part by the Natural
   Science Foundation of China under Grants 62006060, U2013210, 62002241,
   and 61972112, in part by Guangdong Basic and Applied Basic Research
   Foundation under Grant 2021B1515020088, and in part by Shenzhen
   Institute of Artificial Intelligence and Robotics for Society (AIRS)
   under Grant 2019-INT021.
CR Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bhat G, 2019, IEEE I CONF COMP VIS, P6181, DOI 10.1109/ICCV.2019.00628
   Bhat G, 2018, LECT NOTES COMPUT SC, V11206, P493, DOI 10.1007/978-3-030-01216-8_30
   Chen ZD, 2020, PROC CVPR IEEE, P6667, DOI 10.1109/CVPR42600.2020.00670
   Dai KN, 2019, PROC CVPR IEEE, P4665, DOI 10.1109/CVPR.2019.00480
   Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Danelljan Martin, 2014, P BRIT MACH VIS C 20
   Dong Z., 2020, P IEEE C COMP VIS PA, P10519, DOI DOI 10.1109/CVPR42600.2020.01053
   Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667
   Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1134, DOI 10.1109/ICCV.2017.128
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1144, DOI [10.1109/ICCV.2017.128, 10.1109/ICCV.2017.129]
   Gao JY, 2019, PROC CVPR IEEE, P4644, DOI 10.1109/CVPR.2019.00478
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Guo Q, 2017, IEEE I CONF COMP VIS, P1781, DOI 10.1109/ICCV.2017.196
   Han G, 2019, IEEE ACCESS, V7, P123934, DOI 10.1109/ACCESS.2019.2937998
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He ZY, 2017, IEEE T CYBERNETICS, V47, P354, DOI 10.1109/TCYB.2016.2514714
   He ZY, 2016, IEEE T IMAGE PROCESS, V25, P3698, DOI 10.1109/TIP.2016.2570553
   Huang LH, 2021, IEEE T PATTERN ANAL, V43, P1562, DOI 10.1109/TPAMI.2019.2957464
   Jia X, 2012, PROC CVPR IEEE, P1822, DOI 10.1109/CVPR.2012.6247880
   Jung I, 2018, LECT NOTES COMPUT SC, V11208, P89, DOI 10.1007/978-3-030-01225-0_6
   Kang Kim, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P355, DOI 10.1007/978-3-030-58595-2_22
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Law H., 2020, P BRIT MACH VIS C
   Law H, 2020, INT J COMPUT VISION, V128, P642, DOI 10.1007/s11263-019-01204-1
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li X, 2019, PROC CVPR IEEE, P1369, DOI 10.1109/CVPR.2019.00146
   Li Y, 2015, LECT NOTES COMPUT SC, V8926, P254, DOI 10.1007/978-3-319-16181-5_18
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu C, 2018, IEEE T MULTIMEDIA, V20, P889, DOI 10.1109/TMM.2017.2760633
   Liu Q, 2020, AAAI CONF ARTIF INTE, V34, P11604
   Lukezic A, 2018, INT J COMPUT VISION, V126, P671, DOI 10.1007/s11263-017-1061-3
   Müller M, 2018, LECT NOTES COMPUT SC, V11205, P310, DOI 10.1007/978-3-030-01246-5_19
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Qi YK, 2019, AAAI CONF ARTIF INTE, P8835
   Qi YK, 2016, PROC CVPR IEEE, P4303, DOI 10.1109/CVPR.2016.466
   Rashwan A., 2020, ARXIV200103194
   Real E, 2017, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR.2017.789
   Ren S, 2015, FASTER R CNN REAL TI, P91
   Ruan WJ, 2019, IEEE T MULTIMEDIA, V21, P1122, DOI 10.1109/TMM.2018.2872897
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Song YB, 2018, PROC CVPR IEEE, P8990, DOI 10.1109/CVPR.2018.00937
   Tang M, 2018, PROC CVPR IEEE, P4874, DOI 10.1109/CVPR.2018.00512
   Tao R, 2016, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2016.158
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531
   Wang LJ, 2015, IEEE I CONF COMP VIS, P3119, DOI 10.1109/ICCV.2015.357
   Wang Q, 2019, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2019.00142
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Xu TY, 2019, IEEE I CONF COMP VIS, P7949, DOI 10.1109/ICCV.2019.00804
   Yunhua Zhang, 2018, Computer Vision - ECCV 2018. 15th European Conference. Proceedings: Lecture Notes in Computer Science (LNCS 11213), P355, DOI 10.1007/978-3-030-01240-3_22
   Zhang JM, 2014, LECT NOTES COMPUT SC, V8694, P188, DOI 10.1007/978-3-319-10599-4_13
   Zhang LC, 2019, IEEE I CONF COMP VIS, P4009, DOI 10.1109/ICCV.2019.00411
   Zhang SJ, 2020, I S MOD ANAL SIM COM, P1, DOI 10.1109/mascots50786.2020.9285955
   Zhang Z., 2020, EUR C COMP VIS AUG
   Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472
   Zhou K, 2016, DESTECH TRANS COMP
   Zhu Z, 2018, LECT NOTES COMPUT SC, V11213, P103, DOI 10.1007/978-3-030-01240-3_7
NR 65
TC 72
Z9 73
U1 17
U2 59
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1956
EP 1967
DI 10.1109/TMM.2021.3074239
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0H8DA
UT WOS:000778959200014
OA Green Submitted
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Yi, RM
   Huang, YP
AF Yi, Rumeng
   Huang, Yaping
TI TC-Net: Detecting Noisy Labels Via Transform Consistency
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Noise measurement; Training; Visualization; Transforms; Predictive
   models; Perturbation methods; Airplanes; Noisy labels; transform
   consistency; small-loss selection; category consistency; visual
   attention consistency
AB It is crucial to distinguish mislabeled samples for dealing with noisy labels. Previous methods such as "Co-teaching" and "JoCoR" introduce two different networks to select clean samples out of the noisy ones and only use these clean samples to train the deep models. Different from these methods which require to train two networks simultaneously, we propose a simple and effective method to identify clean samples only using one single network. We discover that the clean samples prefer to reach consistent predictions for the original images and the transformed images while noisy samples usually suffer from inconsistent predictions. Motivated by this observation, we propose a noisy label detection approach, named Transform Consistency Network (TC-Net), which constrains the transform consistency (i.e., category consistency and visual attention consistency) between the original images and the transformed images for network training. Then we can select small-loss samples to update the parameters of the network. Furthermore, in order to mitigate the negative influence of noisy labels, we design a classification loss by using the off-line hard labels and on-line soft labels to provide more reliable supervisions for training a robust model. We conduct comprehensive experiments on CIFAR-10, CIFAR-100 and Clothing1M datasets. Compared with the clean sample selection baselines, we achieve the state-of-the-art performance. Especially, in most cases, our proposed method outperforms the baselines by a large margin.
C1 [Yi, Rumeng; Huang, Yaping] Beijing Jiaotong Univ, Beijing Key Lab Traff Data Anal & Min, Beijing 100044, Peoples R China.
C3 Beijing Jiaotong University
RP Huang, YP (corresponding author), Beijing Jiaotong Univ, Beijing Key Lab Traff Data Anal & Min, Beijing 100044, Peoples R China.
EM 19112038@bjtu.edu.cn; yphuang@bjtu.edu.cn
FU Fundamental Research Funds for the Central Universities [2019JBZ104];
   Major Research and Development Plan of China State Railway Group Company
   Ltd. [K2020G024]
FX This work was supported in part by Fundamental Research Funds for the
   Central Universities under Grant 2019JBZ104 and in part by Major
   Research and Development Plan of China State Railway Group Company Ltd.
   under Grant K2020G024.
CR Arazo E, 2019, PR MACH LEARN RES, V97
   Berthelot D, 2019, ADV NEUR IN, V32
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Gidaris S., 2018, P 6 INT C LEARNING R
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Guo H, 2019, PROC CVPR IEEE, P729, DOI 10.1109/CVPR.2019.00082
   Guo S, 2018, LECT NOTES COMPUT SC, V11214, P139, DOI 10.1007/978-3-030-01249-6_9
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   Huang JC, 2019, IEEE I CONF COMP VIS, P3325, DOI 10.1109/ICCV.2019.00342
   Jiang Lu, 2018, PR MACH LEARN RES
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Lee H, 2020, PR MACH LEARN RES, V119
   Li JN, 2019, PROC CVPR IEEE, P5046, DOI 10.1109/CVPR.2019.00519
   Li Junnan, 2020, ARXIV200207394
   Liu TL, 2016, IEEE T PATTERN ANAL, V38, P447, DOI 10.1109/TPAMI.2015.2456899
   Malach E, 2017, ADV NEUR IN, V30
   Min SB, 2021, IEEE T MULTIMEDIA, V23, P899, DOI 10.1109/TMM.2020.2990063
   Nguyen Duc Tam, 2020, INT C LEARN REPR
   Samuli L., 2017, ICLR, P1
   Tanaka D, 2018, PROC CVPR IEEE, P5552, DOI 10.1109/CVPR.2018.00582
   Tarvainen A, 2017, ADV NEUR IN, V30
   Wei Hongxin, 2020, P IEEE C COMP VIS PA
   Xiao T, 2015, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2015.7298885
   Yi K, 2019, PROC CVPR IEEE, P7010, DOI 10.1109/CVPR.2019.00718
   Yu Xingrui, 2019, PROC INT C MACH LEAR, P7164
   Yu XY, 2018, PROC CVPR IEEE, P4480, DOI 10.1109/CVPR.2018.00471
   Zhang CX, 2018, PROTEINS, V86, P136, DOI 10.1002/prot.25414
   Zhang FF, 2022, IEEE T MULTIMEDIA, V24, P1800, DOI 10.1109/TMM.2021.3072786
   Zhang WH, 2019, PROC CVPR IEEE, P7365, DOI 10.1109/CVPR.2019.00755
   Zhang YS, 2020, IEEE T MULTIMEDIA, V22, P2844, DOI 10.1109/TMM.2020.2966887
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
NR 31
TC 3
Z9 3
U1 2
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 4328
EP 4341
DI 10.1109/TMM.2021.3115635
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 5C3SZ
UT WOS:000864185400005
DA 2024-07-18
ER

PT J
AU Jin, X
   Lan, CL
   Zeng, WJ
   Chen, ZB
AF Jin, Xin
   Lan, Cuiling
   Zeng, Wenjun
   Chen, Zhibo
TI Style Normalization and Restitution for Domain Generalization and
   Adaptation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Signal to noise ratio; Training; Feature extraction;
   Entropy; Testing; Lighting; Discriminative and generalizable feature
   representations; feature disentanglement; domain generalization;
   unsupervised domain adaptation
ID PERSON REIDENTIFICATION
AB For many computer vision applications, the learned models usually have high performance on the training datasets but suffer from significant performance degradation when deployed in new environments, where there are usually style differences between the training images and the testing images. For high-level vision tasks, an effective domain generalizable model is expected to be able to learn feature representations that are both generalizable and discriminative. In this paper, we design a novel Style Normalization and Restitution module (SNR) to simultaneously ensure high generalization and discrimination capability of the networks. In SNR, particularly, we filter out the style variations (e.g., illumination, color contrast) by performing Instance Normalization (IN) to obtain style normalized features, where the discrepancy among different samples/domains is reduced. However, such a process is task-ignorant and inevitably removes some task-relevant discriminative information, which may hurt the performance. To remedy this, we propose to distill task-relevant discriminative features from the residual (i.e., the difference between the original feature and the style normalized feature) and add them back to the network to ensure high discrimination. Moreover, for better disentanglement, we enforce a dual restitution loss constraint to encourage the better separation of task-relevant and task-irrelevant features. We validate the effectiveness of our SNR on different vision tasks, including classification, semantic segmentation, and object detection. Experiments demonstrate that our SNR is capable of improving the performance of networks for domain generalization (DG) and unsupervised domain adaptation (UDA).
C1 [Jin, Xin; Chen, Zhibo] Univ Sci & Technol China, Hefei 230026, Anhui, Peoples R China.
   [Lan, Cuiling; Zeng, Wenjun] Microsoft Res Asia, Beijing 100080, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Microsoft Research Asia; Microsoft
RP Chen, ZB (corresponding author), Univ Sci & Technol China, Hefei 230026, Anhui, Peoples R China.; Lan, CL (corresponding author), Microsoft Res Asia, Beijing 100080, Peoples R China.
EM jinxustc@mail.ustc.edu.cn; culan@microsoft.com; wezeng@microsoft.com;
   chenzhibo@ustc.edu.cn
RI jin, xin/GQZ-5811-2022; Lan, Cuiling/KCK-5597-2024
FU NSFC [U1908209, 61632001, 62021001]; National Key Research and
   Development Program of China [2018AAA0101400]
FX This work was supported in part by NSFC under Grants U1908209, 61632001,
   and 62021001, and in part by the National Key Research and Development
   Program of China under Grant 2018AAA0101400.
CR Bilen Hakan, 2017, ARXIV170107275
   Carlucci FM, 2019, PROC CVPR IEEE, P2224, DOI 10.1109/CVPR.2019.00233
   Carlucci FM, 2017, IEEE I CONF COMP VIS, P5077, DOI 10.1109/ICCV.2017.542
   Chang WG, 2019, PROC CVPR IEEE, P7346, DOI 10.1109/CVPR.2019.00753
   Chen MH, 2019, IEEE I CONF COMP VIS, P2090, DOI 10.1109/ICCV.2019.00218
   Chen Y, 2018, PROC CVPR IEEE, P3339, DOI 10.1109/CVPR.2018.00352
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dumoulin V., 2017, PROC INT C LEARN REP
   Fan Q., 2019, PROC CVPR IEEE
   Ganin Y, 2016, J MACH LEARN RES, V17
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Ghifary M, 2017, IEEE T PATTERN ANAL, V39, P1414, DOI 10.1109/TPAMI.2016.2599532
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hoffman J, 2018, PR MACH LEARN RES, V80
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   HULL JJ, 1994, IEEE T PATTERN ANAL, V16, P550, DOI 10.1109/34.291440
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Jia J., 2019, PROC BRIT MACH VIS C
   Kaiyang Zhou, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P561, DOI 10.1007/978-3-030-58517-4_33
   Khodabandeh M, 2019, IEEE I CONF COMP VIS, P480, DOI 10.1109/ICCV.2019.00057
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Li D, 2019, IEEE I CONF COMP VIS, P1446, DOI 10.1109/ICCV.2019.00153
   Li D, 2018, AAAI CONF ARTIF INTE, P3490
   Li D, 2017, IEEE I CONF COMP VIS, P5543, DOI 10.1109/ICCV.2017.591
   Li HL, 2018, PROC CVPR IEEE, P5400, DOI 10.1109/CVPR.2018.00566
   Li YH, 2018, PATTERN RECOGN, V80, P109, DOI 10.1016/j.patcog.2018.03.005
   Liu AH, 2018, ADV NEUR IN, V31
   Liu H, 2019, PR MACH LEARN RES, V97
   Long M., 2016, Advances in neural information processing systems, V29
   Long MS, 2017, PR MACH LEARN RES, V70
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Ma XH, 2019, IEEE T MULTIMEDIA, V21, P2419, DOI 10.1109/TMM.2019.2902100
   Motiian S, 2017, IEEE I CONF COMP VIS, P5716, DOI 10.1109/ICCV.2017.609
   Muandet Krikamol, 2013, INT C MACH LEARN, P10
   Nam H, 2018, ADV NEUR IN, V31
   Netzer Y., 2011, NIPS WORKSH DEEP LEA
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Pan XG, 2018, LECT NOTES COMPUT SC, V11208, P484, DOI 10.1007/978-3-030-01225-0_29
   Peng X., 2020, P INT C LEARN REPR, P1
   Peng XC, 2019, IEEE I CONF COMP VIS, P1406, DOI 10.1109/ICCV.2019.00149
   Peng XC, 2018, IEEE WINT CONF APPL, P1982, DOI 10.1109/WACV.2018.00219
   Ren S., 2015, NEURAL INFORM PROCES, V28, P91, DOI DOI 10.1109/TPAMI.2016.2577031
   Richter SR, 2016, LECT NOTES COMPUT SC, V9906, P102, DOI 10.1007/978-3-319-46475-6_7
   Ros G, 2016, PROC CVPR IEEE, P3234, DOI 10.1109/CVPR.2016.352
   Saito K, 2019, IEEE I CONF COMP VIS, P8049, DOI 10.1109/ICCV.2019.00814
   Saito K, 2018, PROC CVPR IEEE, P3723, DOI 10.1109/CVPR.2018.00392
   Sakaridis C, 2018, INT J COMPUT VISION, V126, P973, DOI 10.1007/s11263-018-1072-8
   Seonguk Seo, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P68, DOI 10.1007/978-3-030-58542-6_5
   Shankar S., 2018, PROC INT C LEARN REP
   Song JF, 2019, PROC CVPR IEEE, P719, DOI 10.1109/CVPR.2019.00081
   Sun BC, 2016, AAAI CONF ARTIF INTE, P2058
   Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35
   Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347
   Tsai YH, 2018, PROC CVPR IEEE, P7472, DOI 10.1109/CVPR.2018.00780
   Vu TH, 2019, PROC CVPR IEEE, P2512, DOI 10.1109/CVPR.2019.00262
   Tzeng E., 2014, ARXIV14123474
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   Ulyanov D, 2017, PROC CVPR IEEE, P4105, DOI 10.1109/CVPR.2017.437
   Ulyanov Dmitry, 2016, arXiv
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572
   Volpi R, 2018, ADV NEUR IN, V31
   Wang JD, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P402, DOI 10.1145/3240508.3240512
   Wang Ximei, 2019, PROC NEURIPS, V32, P1951
   Xu RJ, 2019, IEEE I CONF COMP VIS, P1426, DOI 10.1109/ICCV.2019.00151
   Xu RJ, 2018, PROC CVPR IEEE, P3964, DOI 10.1109/CVPR.2018.00417
   Yan HL, 2020, IEEE T MULTIMEDIA, V22, P2420, DOI 10.1109/TMM.2019.2953375
   Zellinger Werner, 2017, 5 INT C LEARN REPR I, P2
   Zhang WC, 2018, PROC CVPR IEEE, P3801, DOI 10.1109/CVPR.2018.00400
   Zhao H, 2018, 32 C NEURAL INFORM P, V31
   Zheng WS, 2011, PROC CVPR IEEE, P649, DOI 10.1109/CVPR.2011.5995598
   Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380
   Zhou Kaiyang, 2020, ARXIV200307325
NR 79
TC 28
Z9 30
U1 3
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG 13
PY 2021
VL 24
BP 3636
EP 3651
DI 10.1109/TMM.2021.3104379
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2W7NP
UT WOS:000824707400002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhao, TS
   Lin, YT
   Xu, YW
   Chen, WL
   Wang, Z
AF Zhao, Tiesong
   Lin, Yuting
   Xu, Yiwen
   Chen, Weiling
   Wang, Zhou
TI Learning-Based Quality Assessment for Image Super-Resolution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Databases; Measurement; Labeling; Deep learning; Image quality;
   Convolutional neural networks; Visualization; Image quality assessment;
   image super-resolution; reduced-reference
ID SHARPNESS ASSESSMENT; INTERPOLATION
AB Image Super-Resolution (SR) techniques improve visual quality by enhancing the spatial resolution of images. Quality evaluation metrics play a critical role in comparing and optimizing SR algorithms, but current metrics achieve only limited success, largely due to the lack of large-scale quality databases, which are essential for learning accurate and robust SR quality metrics. In this work, we first build a large-scale SR image database using a novel semi-automatic labeling approach, which allows us to label a large number of images with manageable human workload. The resulting SR Image quality database with Semi-Automatic Ratings (SISAR), so far the largest of SR-IQA database, contains 12 600 images of 100 natural scenes. We train an end-to-end Deep Image SR Quality (DISQ) model by employing two-stream Deep Neural Networks (DNNs) for feature extraction, followed by a feature fusion network for quality prediction. Experimental results demonstrate that the proposed method outperforms state-of-the-art metrics and achieves promising generalization performance in cross-database tests. The SISAR database and DISQ model will be made publicly available to facilitate reproducible research.
C1 [Zhao, Tiesong; Lin, Yuting; Xu, Yiwen; Chen, Weiling] Fuzhou Univ, Fujian Key Lab Intelligent Proc & Wireless Transm, Fuzhou 350116, Peoples R China.
   [Zhao, Tiesong] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
   [Wang, Zhou] Univ Waterloo, Dept Elect & Comp Engn, Waterloo, ON N2L 3G1, Canada.
C3 Fuzhou University; Peng Cheng Laboratory; University of Waterloo
RP Chen, WL (corresponding author), Fuzhou Univ, Fujian Key Lab Intelligent Proc & Wireless Transm, Fuzhou 350116, Peoples R China.
EM t.zhao@fzu.edu.cn; n181120063@fzu.edu.cn; xu_yiwen@fzu.edu.cn;
   weiling.chen@fzu.edu.cn; zhou.wang@uwaterloo.ca
RI lin, yt/IQT-6771-2023; Lin, Yuting/HPE-4176-2023; Weiling,
   Chen/JAA-9972-2023
OI Wang, Zhou/0000-0003-4413-4441
FU National Natural Science Foundation of China [62171134, 61901119]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 62171134 and 61901119.
CR [Anonymous], 2012, BT50013 ITUR
   Bahrami K, 2014, IEEE SIGNAL PROC LET, V21, P751, DOI 10.1109/LSP.2014.2314487
   Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Chen LJ, 2018, 2018 3RD INTERNATIONAL CONFERENCE ON SYSTEM RELIABILITY AND SAFETY (ICSRS), P1, DOI [10.1109/ICSRS.2018.8688869, 10.1109/ICSRS.2018.00009]
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Fang YM, 2018, MULTIMED TOOLS APPL, V77, P29829, DOI 10.1007/s11042-018-5805-z
   Fang YM, 2016, IEEE IMAGE PROC, P2057, DOI 10.1109/ICIP.2016.7532720
   Gu Jinjin, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P633, DOI 10.1007/978-3-030-58621-8_37
   Gu K, 2018, IEEE T NEUR NET LEAR, V29, P1301, DOI 10.1109/TNNLS.2017.2649101
   Hassen R, 2013, IEEE T IMAGE PROCESS, V22, P2798, DOI 10.1109/TIP.2013.2251643
   Hosseini MS, 2020, IEEE T MED IMAGING, V39, P62, DOI 10.1109/TMI.2019.2919722
   Hosseini MS, 2019, IEEE T IMAGE PROCESS, V28, P4510, DOI 10.1109/TIP.2019.2906582
   Hosseini MS, 2018, IEEE IMAGE PROC, P296, DOI 10.1109/ICIP.2018.8451488
   HOU HS, 1978, IEEE T ACOUST SPEECH, V26, P508
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kingma D.P., 2014, ARXIV14126980
   Leclaire A, 2015, J MATH IMAGING VIS, V52, P145, DOI 10.1007/s10851-015-0560-5
   Li LD, 2016, IEEE T MULTIMEDIA, V18, P1085, DOI 10.1109/TMM.2016.2545398
   Ma C, 2017, COMPUT VIS IMAGE UND, V158, P1, DOI 10.1016/j.cviu.2016.12.009
   Ma KD, 2018, IEEE T IMAGE PROCESS, V27, P1202, DOI 10.1109/TIP.2017.2774045
   Ma KD, 2015, IEEE T IMAGE PROCESS, V24, P3086, DOI [10.1109/TIP.2015.2436340, 10.1109/TIP.2015.2456638]
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Papyan V, 2016, IEEE T IMAGE PROCESS, V25, P249, DOI 10.1109/TIP.2015.2499698
   Prashnani E, 2018, PROC CVPR IEEE, P1808, DOI 10.1109/CVPR.2018.00194
   Reibman AR, 2006, IEEE IMAGE PROC, P2017, DOI 10.1109/ICIP.2006.312895
   Ren C, 2017, IEEE T IMAGE PROCESS, V26, P90, DOI 10.1109/TIP.2016.2619265
   Romano Y, 2014, IEEE T IMAGE PROCESS, V23, P3085, DOI 10.1109/TIP.2014.2325774
   Sajjadi MSM, 2017, IEEE I CONF COMP VIS, P4501, DOI 10.1109/ICCV.2017.481
   Shao F, 2016, IEEE T CYBERNETICS, V46, P730, DOI 10.1109/TCYB.2015.2414479
   Simonyan K, 2015, IEEE INT C ICLR
   Tang LJ, 2019, SIGNAL PROCESS-IMAGE, V79, P32, DOI 10.1016/j.image.2019.08.004
   Thapa D, 2016, COMPUT ELECTR ENG, V54, P313, DOI 10.1016/j.compeleceng.2015.09.011
   Tian CW, 2021, IEEE T MULTIMEDIA, V23, P1489, DOI 10.1109/TMM.2020.2999182
   Timofte R, 2015, LECT NOTES COMPUT SC, V9006, P111, DOI 10.1007/978-3-319-16817-3_8
   Wang GC, 2017, IEEE IMAGE PROC, P3145, DOI 10.1109/ICIP.2017.8296862
   Wang XT, 2018, PROC CVPR IEEE, P606, DOI 10.1109/CVPR.2018.00070
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZJ, 2011, IEEE SIGNAL PROC MAG, V28, P2, DOI 10.1109/MSP.2011.940297
   Wang ZH, 2021, IEEE T PATTERN ANAL, V43, P3365, DOI 10.1109/TPAMI.2020.2982166
   Xianming Liu, 2010, 2010 28th Picture Coding Symposium (PCS 2010), P118, DOI 10.1109/PCS.2010.5702437
   Xu JT, 2016, IEEE T IMAGE PROCESS, V25, P4444, DOI 10.1109/TIP.2016.2585880
   Yan B, 2019, IEEE T MULTIMEDIA, V21, P2603, DOI 10.1109/TMM.2019.2904879
   Yang CY, 2014, LECT NOTES COMPUT SC, V8692, P372, DOI 10.1007/978-3-319-10593-2_25
   Yang S, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1383, DOI 10.1145/3343031.3350990
   Yang WM, 2019, IEEE T MULTIMEDIA, V21, P3106, DOI 10.1109/TMM.2019.2919431
   Zhang KB, 2012, IEEE T IMAGE PROCESS, V21, P4544, DOI 10.1109/TIP.2012.2208977
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang W, 2017, IEEE T IMAGE PROCESS, V26, P2424, DOI 10.1109/TIP.2017.2681424
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang YF, 2020, IEEE T MULTIMEDIA, V22, P1407, DOI 10.1109/TMM.2019.2943750
   Zhongling Wang, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12265), P403, DOI 10.1007/978-3-030-59722-1_39
   Zhou F, 2019, IEEE T IMAGE PROCESS, V28, P3528, DOI 10.1109/TIP.2019.2898638
NR 55
TC 10
Z9 10
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG 12
PY 2021
VL 24
BP 3570
EP 3581
DI 10.1109/TMM.2021.3102401
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2W7NR
UT WOS:000824707600001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Ding, F
   Zhu, GP
   Li, YC
   Zhang, XP
   Atrey, PK
   Lyu, SW
AF Ding, Feng
   Zhu, Guopu
   Li, Yingcan
   Zhang, Xinpeng
   Atrey, Pradeep K.
   Lyu, Siwei
TI Anti-Forensics for Face Swapping Videos via Adversarial Training
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Videos; Information integrity; Faces; Forensics; Detectors;
   Visualization; Tools; Digital forensics; anti-forensics; DeepFake;
   generative adversarial network
AB Generating falsified faces by artificial intelligence, widely known as DeepFake, has attracted attention worldwide since 2017. Given the potential threat brought by this novel technique, forensics researchers dedicated themselves to detect the video forgery. Except for exposing falsified faces, there could be extended research directions for DeepFake such as anti-forensics. It can disclose the vulnerability of current DeepFake forensics methods. Besides, it could also enable DeepFake videos as tactical weapons if the falsified faces are more subtle to be detected. In this paper, we propose a GAN model to behave as an anti-forensics tool. It features a novel architecture with additional supervising modules for enhancing image visual quality. Besides, a loss function is designed to improve the efficiency of the proposed model. After experimental evaluations, we show that the DeepFake forensics detectors are susceptible to attacks launched by the proposed method. Besides, the proposed method can efficiently produce anti-forensics videos in satisfying visual quality without noticeable artifacts. Compared with the other anti-forensics approaches, this is tremendous progress achieved for DeepFake anti-forensics. The attack launched by our proposed method can be truly regarded as DeepFake anti-forensics as it can fool detecting algorithms and human eyes simultaneously.
C1 [Ding, Feng] Nanchang Univ, Sch Management, Nanchang 330031, Jiangxi, Peoples R China.
   [Zhu, Guopu] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Peoples R China.
   [Zhu, Guopu; Li, Yingcan] Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen 518055, Peoples R China.
   [Zhang, Xinpeng] Fudan Univ, Sch Comp Sci, Shanghai 200433, Peoples R China.
   [Atrey, Pradeep K.] SUNY Albany, Albany, NY 12222 USA.
   [Lyu, Siwei] SUNY Buffalo, Buffalo, NY 14222 USA.
C3 Nanchang University; Harbin Institute of Technology; Chinese Academy of
   Sciences; Shenzhen Institute of Advanced Technology, CAS; Fudan
   University; State University of New York (SUNY) System; State University
   of New York (SUNY) Albany; State University of New York (SUNY) System;
   State University of New York (SUNY) Buffalo
RP Zhu, GP (corresponding author), Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Peoples R China.; Zhu, GP (corresponding author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen 518055, Peoples R China.
EM fding@albany.edu; guopu.zhu@gmail.com; yc.il@siat.ac.cn;
   zhangxinpeng@fudan.edu.cn; patrey@albany.edu; slyu@albany.edu
OI Atrey, Pradeep/0000-0002-9577-0969; Zhu, Guopu/0000-0001-7956-5343
FU National Natural Science Foundation of China [61872350, U1936214,
   61572489]; Jiangxi Double Thousand Plan [JXSQ201901075]; Tip-top
   Scientific andTechnical Innovative Youth Talents of Guangdong Special
   Support Program [2019TQ05X696]; Basic Research Program of Shenzhen
   [JCYJ20170818163403748]; National Science Foundation [2103450]; Div Of
   Information & Intelligent Systems; Direct For Computer & Info Scie &
   Enginr [2103450] Funding Source: National Science Foundation
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61872350, U1936214, and 61572489, in
   part by Jiangxi Double Thousand Plan under Grant JXSQ201901075, in part
   by the Tip-top Scientific andTechnical InnovativeYouthTalents of
   Guangdong Special Support Program under Grant 2019TQ05X696, in part by
   the Basic Research Program of Shenzhen under Grant
   JCYJ20170818163403748, and in part by the National Science Foundation
   underGrant 2103450.
CR Afchar D, 2018, IEEE INT WORKS INFOR
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Cao G, 2011, IEEE SIGNAL PROC LET, V18, P603, DOI 10.1109/LSP.2011.2164791
   Chen BJ, 2021, IEEE T MULTIMEDIA, V23, P3506, DOI 10.1109/TMM.2020.3026868
   Chen C., 2019, IEEE Trans. Inf. Forensics Secur., DOI DOI 10.1109/TIFS.2019.2945198
   Chesney B, 2019, CALIF LAW REV, V107, P1753, DOI 10.15779/Z38RV0D15J
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Dai B, 2017, IEEE I CONF COMP VIS, P2989, DOI 10.1109/ICCV.2017.323
   Ding F, 2020, IEEE T CIRC SYST VID, V30, P4715, DOI 10.1109/TCSVT.2019.2963715
   Ding F, 2018, J VIS COMMUN IMAGE R, V50, P93, DOI 10.1016/j.jvcir.2017.11.009
   Ding F, 2015, IEEE SIGNAL PROC LET, V22, P327, DOI 10.1109/LSP.2014.2359033
   Dolhansky Brian, 2019, ARXIV191008854
   Dou LY, 2020, J ELECTRON IMAGING, V29, DOI 10.1117/1.JEI.29.4.043026
   Farid H, 2008, SCI AM, V298, P66, DOI 10.1038/scientificamerican0608-66
   Fridrich J, 2009, IEEE SIGNAL PROC MAG, V26, P26, DOI 10.1109/MSP.2008.931078
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gu SY, 2019, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2019.00355
   Güera D, 2018, 2018 15TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), P127
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Kim D, 2018, IEEE SIGNAL PROC LET, V25, DOI 10.1109/LSP.2017.2782363
   Korshunov P., 2018, arXiv
   Kupyn O, 2019, IEEE I CONF COMP VIS, P8877, DOI 10.1109/ICCV.2019.00897
   Li D., 2021, ARXIV210103272
   Li YZ, 2018, IEEE INT WORKS INFOR
   Li Yuezun, 2019, ARXIV190609288
   Li Yuezun, 2019, P CVPRW
   Luo YM, 2018, EUR SIGNAL PR CONF, P952, DOI 10.23919/EUSIPCO.2018.8553259
   Lyu S., 2020, IEEE INT CONF MULTI, DOI DOI 10.1109/icmew46912.2020.9105991
   Ma Liqian, 2017, ARXIV170509368
   Matern F, 2019, IEEE WINT CONF APPL, P83, DOI 10.1109/WACVW.2019.00020
   Nguyen Huy H, 2019, Use of a capsule network to detect fake images and videos
   Nowroozi E, 2021, COMPUT SECUR, V100, DOI 10.1016/j.cose.2020.102092
   Peng F, 2020, IEEE T MULTIMEDIA, V22, P2511, DOI 10.1109/TMM.2019.2959443
   Qiao T, 2019, IEEE T MULTIMEDIA, V21, P1077, DOI 10.1109/TMM.2018.2872863
   Rao Y, 2016, IEEE INT WORKS INFOR
   Rocha A, 2011, ACM COMPUT SURV, V43, DOI 10.1145/1978802.1978805
   Rössler A, 2019, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2019.00009
   Sabir E., 2019, INTERFACES GUI, V3, P80
   Shamsabadi AS, 2020, INT CONF ACOUST SPEE, P1898, DOI [10.1109/ICASSP40776.2020.9054368, 10.1109/icassp40776.2020.9054368]
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Stamm MC, 2010, IEEE IMAGE PROC, P2109, DOI 10.1109/ICIP.2010.5652553
   Tang HS, 2018, J VIS COMMUN IMAGE R, V51, P162, DOI 10.1016/j.jvcir.2018.01.011
   Thies J, 2016, PROC CVPR IEEE, P2387, DOI 10.1109/CVPR.2016.262
   Yang X, 2019, INT CONF ACOUST SPEE, P8261, DOI 10.1109/ICASSP.2019.8683164
   Yuezun Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P3204, DOI 10.1109/CVPR42600.2020.00327
   Zhang H, 2020, IEEE T CIRC SYST VID, V30, P3943, DOI 10.1109/TCSVT.2019.2920407
   Zhou P, 2017, IEEE COMPUT SOC CONF, P1831, DOI 10.1109/CVPRW.2017.229
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 50
TC 65
Z9 68
U1 7
U2 36
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL 26
PY 2021
VL 24
BP 3429
EP 3441
DI 10.1109/TMM.2021.3098422
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2W7NF
UT WOS:000824706400001
OA Bronze
DA 2024-07-18
ER

PT J
AU Fei, LK
   Zhang, B
   Zhang, L
   Jia, W
   Wen, J
   Wu, JG
AF Fei, Lunke
   Zhang, Bob
   Zhang, Lin
   Jia, Wei
   Wen, Jie
   Wu, Jigang
TI Learning Compact Multifeature Codes for Palmprint Recognition From a
   Single Training Image per Palm
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Training; Palmprint recognition; Learning systems;
   Binary codes; Machine learning; Palmprint recognition; single training
   image per palm; multifeature learning; compact binary code
ID FACE RECOGNITION; SAMPLE; LINE; EXTRACTION; DESCRIPTOR; PCANET
AB In this article, we propose a multifeature learning method to jointly learn compact multifeature codes (LCMFCs) for palmprint recognition with a single training sample per palm. Unlike most existing hand-crafted methods that extract single-type features from raw pixels, we first form the multi-type data vectors such as the direction-data, and texture-data to completely sample the multiple information of a palmprint image. Then, we learn the discriminative multifeatures from multi-type data vectors by maximizing the inter-palm distance, and minimizing the energy loss between the learned codes, and the original data. Moreover, our LCMFC method adaptively learns the optimal weights of multi-type features to jointly learn the compact multifeature codes. Finally, we cluster the nonoverlapping blockwise histograms of the compact multifeature codes into a feature vector for palmprint representation. Extensive experimental results on six benchmark palmprint databases are presented to show the effectiveness of the proposed method.
C1 [Fei, Lunke; Wu, Jigang] Guangdong Univ Technol, Sch Comp Sci & Technol, Guangzhou 510006, Peoples R China.
   [Zhang, Bob] Univ Macau, Dept Comp & Informat Sci, Taipa 999078, Macao, Peoples R China.
   [Zhang, Lin] Tongjing Univ, Sch Software Engn, Shanghai 201804, Peoples R China.
   [Jia, Wei] Hefei Univ Technol, Sch Comp & Informat, Hefei 230009, Peoples R China.
   [Wen, Jie] Harbin Inst Technol Shenzhen, Biocomp Res Ctr, Shenzhen 518055, Peoples R China.
C3 Guangdong University of Technology; University of Macau; Hefei
   University of Technology; Harbin Institute of Technology
RP Zhang, B (corresponding author), Univ Macau, Dept Comp & Informat Sci, Taipa 999078, Macao, Peoples R China.
EM flksxm@126.com; bobzhang@um.edu.mo; cslinzhang@tongji.edu.cn;
   china.jiawei@139.com; jiewen_pr@126.com; asjgwucn@outlook.com
RI Zhang, Bob/ABD-5926-2021; Wen, Jie/AAH-8083-2020; wu, ji/IAR-8520-2023;
   Zhang, Bob/HIR-3656-2022; Wen, Jie/G-7235-2015
OI Zhang, Bob/0000-0003-2497-9519; Zhang, Bob/0000-0001-6512-0474; Zhang,
   Lin/0000-0002-4360-5523; Fei, Lunke/0000-0001-6072-7875; Wen,
   Jie/0000-0001-9554-2379
FU National Key R&D Program of China [2018YFB1003201]; National Natural
   Science Foundation of China [61702110, 61673175]; Natural Science
   Foundation of Guangdong Province [2019A1515011811]; Guangdong Basic, and
   Applied Basic Research Foundation [2019A1515110582]; Guangzhou Science,
   and technology plan project [202002030110]; University of Macau
   [MYRG2019-00006-FST]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2018YFB1003201, in part by the National Natural Science
   Foundation of China under Grants 61702110, and 61673175, in part by the
   Natural Science Foundation of Guangdong Province under Grant
   2019A1515011811, in part by the Guangdong Basic, and Applied Basic
   Research Foundation under Grant 2019A1515110582, in part by the
   Guangzhou Science, and technology plan project under Grant 202002030110,
   and in part by the University of Macau under Grant MYRG2019-00006-FST.
CR [Anonymous], 2008, 19 INT C PATT REC
   Bai XF, 2019, IEEE T INSTRUM MEAS, V68, P3287, DOI 10.1109/TIM.2018.2877226
   Cao K, 2019, IEEE T PATTERN ANAL, V41, P788, DOI 10.1109/TPAMI.2018.2818162
   Chan TH, 2015, IEEE T IMAGE PROCESS, V24, P5017, DOI 10.1109/TIP.2015.2475625
   Choi O, 2019, IEEE T MULTIMEDIA, V21, P1487, DOI 10.1109/TMM.2018.2880608
   Dai JF, 2011, IEEE T PATTERN ANAL, V33, P945, DOI 10.1109/TPAMI.2010.164
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding GD, 2019, IEEE T MULTIMEDIA, V21, P2891, DOI 10.1109/TMM.2019.2916456
   Fei LK, 2020, IEEE T CIRC SYST VID, V30, P468, DOI 10.1109/TCSVT.2019.2890835
   Fei LK, 2019, IEEE T IMAGE PROCESS, V28, P3808, DOI 10.1109/TIP.2019.2903307
   Fei LK, 2018, IEEE T INSTRUM MEAS, V67, P2761, DOI 10.1109/TIM.2018.2830858
   Fei LK, 2019, IEEE T SYST MAN CY-S, V49, P346, DOI 10.1109/TSMC.2018.2795609
   Fei LK, 2019, INFORM SCIENCES, V473, P59, DOI 10.1016/j.ins.2018.09.032
   Fei LK, 2016, PATTERN RECOGN, V49, P89, DOI 10.1016/j.patcog.2015.08.001
   Genovese A, 2019, IEEE T INF FOREN SEC, V14, P3160, DOI 10.1109/TIFS.2019.2911165
   Guo ZH, 2012, IEEE T INF FOREN SEC, V7, P1094, DOI 10.1109/TIFS.2012.2189206
   Guo ZH, 2010, IEEE IMAGE PROC, P4521, DOI 10.1109/ICIP.2010.5653119
   Guo ZH, 2010, IEEE T IMAGE PROCESS, V19, P1657, DOI 10.1109/TIP.2010.2044957
   Guo ZH, 2009, PATTERN RECOGN LETT, V30, P1219, DOI 10.1016/j.patrec.2009.05.010
   Huang DS, 2008, PATTERN RECOGN, V41, P1316, DOI 10.1016/j.patcog.2007.08.016
   Jia W, 2008, PATTERN RECOGN, V41, P1504, DOI 10.1016/j.patcog.2007.10.011
   Jia W, 2017, IEEE T IMAGE PROCESS, V26, P4483, DOI 10.1109/TIP.2017.2705424
   Jia W, 2014, IEEE T SYST MAN CY-S, V44, P385, DOI 10.1109/TSMC.2013.2258010
   Jing XY, 2004, IEEE T SYST MAN CY B, V34, P2405, DOI 10.1109/TSMCB.2004.837586
   Kong AWK, 2004, INT C PATT RECOG, P520, DOI 10.1109/ICPR.2004.1334184
   Li G, 2017, PATTERN RECOGN, V61, P29, DOI 10.1016/j.patcog.2016.06.025
   Liao SC, 2013, IEEE T PATTERN ANAL, V35, P1193, DOI 10.1109/TPAMI.2012.191
   Liu YH, 2015, PATTERN RECOGN LETT, V53, P9, DOI 10.1016/j.patrec.2014.10.014
   Lu J, 2009, ELECTRON LETT, V45, P880, DOI 10.1049/el.2009.0871
   Lu JW, 2015, IEEE T PATTERN ANAL, V37, P2041, DOI 10.1109/TPAMI.2015.2408359
   Lu JW, 2013, IEEE T PATTERN ANAL, V35, P39, DOI 10.1109/TPAMI.2012.70
   Luo YT, 2016, PATTERN RECOGN, V50, P26, DOI 10.1016/j.patcog.2015.08.025
   Meraoumia A, 2017, SIGNAL PROC SEC TEC, P51, DOI 10.1007/978-3-319-47301-7_3
   Nie FP, 2017, SCI CHINA INFORM SCI, V60, DOI 10.1007/s11432-016-9021-9
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Ribaric S, 2005, IEEE T PATTERN ANAL, V27, P1698, DOI 10.1109/TPAMI.2005.209
   Rida I, 2019, PATTERN RECOGN LETT, V126, P21, DOI 10.1016/j.patrec.2018.04.033
   Satorra A, 2001, PSYCHOMETRIKA, V66, P507, DOI 10.1007/BF02296192
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun YL, 2018, IEEE T PATTERN ANAL, V40, P332, DOI 10.1109/TPAMI.2017.2669035
   Sun ZN, 2005, PROC CVPR IEEE, P279
   Svoboda J, 2016, INT C PATT RECOG, P4232, DOI 10.1109/ICPR.2016.7900298
   Wang JD, 2018, IEEE T PATTERN ANAL, V40, P769, DOI 10.1109/TPAMI.2017.2699960
   Wang J, 2010, PROC CVPR IEEE, P3424, DOI 10.1109/CVPR.2010.5539994
   Wang X, 2012, KNOWL-BASED SYST, V27, P451, DOI 10.1016/j.knosys.2011.10.008
   Wu XQ, 2006, IEEE T SYST MAN CY A, V36, P978, DOI 10.1109/TSMCA.2006.871797
   Wu XQ, 2014, PATTERN RECOGN, V47, P3314, DOI 10.1016/j.patcog.2014.04.008
   Wu XQ, 2003, PATTERN RECOGN LETT, V24, P2829, DOI 10.1016/S0167-8655(03)00141-7
   Xiao Q., 2019, IEEE ACCESS, V7, p74 327
   Xie SY, 2019, IEEE T MULTIMEDIA, V21, P211, DOI 10.1109/TMM.2018.2844085
   Xu Y, 2018, IEEE T SYST MAN CY-S, V48, P232, DOI 10.1109/TSMC.2016.2597291
   Yang M, 2013, IEEE I CONF COMP VIS, P689, DOI 10.1109/ICCV.2013.91
   Yao YF, 2007, NEUROCOMPUTING, V70, P1582, DOI 10.1016/j.neucom.2006.08.009
   Zhang, 2020, PATTERN RECOGN, V98, P1
   Zhang D, 2003, IEEE T PATTERN ANAL, V25, P1041, DOI 10.1109/TPAMI.2003.1227981
   Zhang D., 2009, P ADV INF COMMUN TEC, P254
   Zhang D, 2012, ACM COMPUT SURV, V44, DOI 10.1145/2071389.2071391
   Zhang D, 2009, IEEE T SYST MAN CY C, V39, P505, DOI 10.1109/TSMCC.2009.2020790
   Zhang L, 2006, IEEE T PATTERN ANAL, V28, P351, DOI 10.1109/TPAMI.2006.53
   Zhang L, 2017, PATTERN RECOGN, V69, P199, DOI 10.1016/j.patcog.2017.04.016
   Zhang L, 2012, IMAGE VISION COMPUT, V30, P1043, DOI 10.1016/j.imavis.2012.09.003
   Zhang Z, 2019, IEEE T PATTERN ANAL, V41, P1774, DOI 10.1109/TPAMI.2018.2847335
   Zhao Dandan, 2015, 6 INT C WIR MOB MULT, P214
   Zhao SP, 2019, INFORM SCIENCES, V489, P167, DOI 10.1016/j.ins.2019.03.027
   Zheng Q, 2016, IEEE T PATTERN ANAL, V38, P1272, DOI 10.1109/TPAMI.2015.2509968
   Zheng Q, 2016, IEEE T INF FOREN SEC, V11, P633, DOI 10.1109/TIFS.2015.2503265
   Zhong DX, 2019, NEUROCOMPUTING, V328, P16, DOI 10.1016/j.neucom.2018.03.081
   Zhong FJ, 2013, NEUROCOMPUTING, V119, P375, DOI 10.1016/j.neucom.2013.03.020
NR 68
TC 35
Z9 36
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2930
EP 2942
DI 10.1109/TMM.2020.3019701
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UE9PT
UT WOS:000688215600031
DA 2024-07-18
ER

PT J
AU Hou, XS
   Dey, S
   Zhang, JZ
   Budagavi, M
AF Hou, Xueshi
   Dey, Sujit
   Zhang, Jianzhong
   Budagavi, Madhukar
TI Predictive Adaptive Streaming to Enable Mobile 360-Degree and VR
   Experiences
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Virtual reality; video streaming; 360-degree video
ID QUANTIZATION
AB As 360-degree videos and virtual reality (VR) applications become popular for consumer and enterprise use cases, the desire to enable truly mobile experiences also increases. Delivering 360-degree videos and cloud/edge-based VR applications require ultra-high bandwidth and ultra-low latency [1], challenging to achieve with mobile networks. A common approach to reduce bandwidth is streaming only the field of view (FOV). However, extracting and transmitting the FOV in response to user head motion can add high latency, adversely affecting user experience. In this paper, we propose a predictive adaptive streaming approach, where the predicted view with high predictive probability is adaptively encoded in relatively high quality according to bandwidth conditions and transmitted in advance, leading to a simultaneous reduction in bandwidth and latency. The predictive adaptive streaming method is based on a deep-learning-based viewpoint prediction model we develop, which uses past head motions to predict where a user will be looking in the 360-degree view. Using a very large dataset consisting of head motion traces from over 36,000 viewers for nineteen 360-degree/VR videos, we validate the ability of our predictive adaptive streaming method to offer high-quality view while simultaneously significantly reducing bandwidth.
C1 [Hou, Xueshi; Dey, Sujit] Univ Calif San Diego, Dept Elect & Comp Engn, Mobile Syst Design Lab, La Jolla, CA 92093 USA.
   [Zhang, Jianzhong; Budagavi, Madhukar] Samsung Res Amer, Stand & Mobil Innovat Lab, Plano, TX 75023 USA.
C3 University of California System; University of California San Diego;
   Samsung
RP Hou, XS (corresponding author), Univ Calif San Diego, Dept Elect & Comp Engn, Mobile Syst Design Lab, La Jolla, CA 92093 USA.
EM x7hou@ucsd.edu; dey@ece.ucsd.edu; jianzhong.z@samsung.com;
   m.budagavi@samsung.com
RI zhan, y/ISA-2807-2023; zhang, jian/HPD-1712-2023; jin, li/IWU-4648-2023
OI Hou, Xueshi/0000-0003-3083-7656
FU Center for Wireless Communications at University of California, San
   Diego
FX This work was supported in part by the Center for Wireless
   Communications at University of California, San Diego.
CR Adobe, 2016, CAP VIEW HUNG VIRT A
   Alahi A, 2016, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2016.110
   Almowuena S, 2016, IEEE T MULTIMEDIA, V18, P102, DOI 10.1109/TMM.2015.2502067
   Amit Y, 1997, NEURAL COMPUT, V9, P1545, DOI 10.1162/neco.1997.9.7.1545
   [Anonymous], 2017, P 27 NOSSDAV, DOI DOI 10.1145/3083165.3083180
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], 2005, Digital Video Quality: Vision Models and Metrics
   [Anonymous], 2016, P 1 WORKSH DEEP LEAR, DOI [DOI 10.1145/2988450.2988454, 10.1145/2988450.2988454]
   Ban Y., 2017, Joint urban remote sensing event (JURSE), 6-8 march 2017 2017, P1, DOI DOI 10.1109/JURSE.2017.7924550
   Bao YN, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA), P1161, DOI 10.1109/BigData.2016.7840720
   Bellini H., 2016, REAL DEAL VIRTUAL AU
   Bengio S, 2015, ADV NEUR IN, V28
   Bengio Y., 2007, Advances in neural information processing systems, P153, DOI DOI 10.7551/MITPRESS/7503.003.0024
   Breiman L., 2001, Mach. Learn., V45, P5
   Bros W., 2018, KONG VR
   Bütepage J, 2017, PROC CVPR IEEE, P1591, DOI 10.1109/CVPR.2017.173
   Chakareski J., 2018, 2018 IEEE INT C COMM, P1, DOI DOI 10.1109/ICC.2018.8422859
   Chollet Keras F., Keras
   Chung Junyoung, 2014, ARXIV14123555
   Corbillon X, 2017, IEEE ICC, DOI 10.1109/ICC.2017.7996611
   COST S, 1993, MACH LEARN, V10, P57, DOI 10.1007/BF00993481
   Ding W, 1996, IEEE T CIRC SYST VID, V6, P12, DOI 10.1109/76.486416
   Gaddam VR, 2016, IEEE T MULTIMEDIA, V18, P1819, DOI 10.1109/TMM.2016.2586304
   Gao GY, 2018, IEEE T MULTIMEDIA, V20, P3399, DOI 10.1109/TMM.2018.2838330
   Ge XH, 2017, IEEE T MULTIMEDIA, V19, P2345, DOI 10.1109/TMM.2017.2733461
   Guestrin C., 2016, KDD16 P 22 ACM, P785, DOI DOI 10.1145/2939672.2939785
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hou X., 2017, Computer Communication and Networks (ICCCN), 2017 26th International Conference on, P1
   Hou X., 2018, ACM T MULTIMEDIA COM, V14, P1
   Hou Xueshi, 2018, P 2018 MORNING WORKS, P20
   HTC, 2018, HTC VIVE
   Ju R, 2017, VR/AR NETWORK '17: PROCEEDINGS OF THE 2017 WORKSHOP ON VIRTUAL REALITY AND AUGMENTED REALITY NETWORK, P19, DOI 10.1145/3097895.3097899
   Kim T, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P577, DOI 10.1145/2783258.2783356
   Längkvist M, 2014, PATTERN RECOGN LETT, V42, P11, DOI 10.1016/j.patrec.2014.01.008
   LaValle SM, 2014, IEEE INT CONF ROBOT, P187, DOI 10.1109/ICRA.2014.6906608
   Li CL, 2018, IEEE T MULTIMEDIA, V20, P965, DOI 10.1109/TMM.2017.2757761
   Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50
   Liu X, 2017, HOTNETS-XVI: PROCEEDINGS OF THE 16TH ACM WORKSHOP ON HOT TOPICS IN NETWORKS, P50, DOI 10.1145/3152434.3152443
   Liu YW, 2019, IEEE T MULTIMEDIA, V21, P1302, DOI 10.1109/TMM.2018.2876044
   Ma Z, 2012, IEEE T CIRC SYST VID, V22, P671, DOI 10.1109/TCSVT.2011.2177143
   Mangiante S, 2017, VR/AR NETWORK '17: PROCEEDINGS OF THE 2017 WORKSHOP ON VIRTUAL REALITY AND AUGMENTED REALITY NETWORK, P30, DOI 10.1145/3097895.3097901
   Mohamed AR, 2012, IEEE T AUDIO SPEECH, V20, P14, DOI 10.1109/TASL.2011.2109382
   Qian F., 2016, Proceedings of the 5th Workshop on All Things Cellular: Operations, Applications and Challenges, P1, DOI DOI 10.1145/2980055.2980056
   Qualcomm, 2016, WHIT MAK IMM VIRT RE
   Rift O., 2018, OCULUS
   Samsung, 2018, SAMSVR
   Sony, 2018, PLAYST VR
   Wang Z, 2015, IEEE T MULTIMEDIA, V17, P867, DOI 10.1109/TMM.2015.2425216
   Wikipedia, 2018, EQ MAP
   Wiltz C, 2017, 5 MAJOR CHALLENGES V
   Xu YY, 2018, PROC CVPR IEEE, P5333, DOI 10.1109/CVPR.2018.00559
   Yan CG, 2014, IEEE SIGNAL PROC LET, V21, P573, DOI 10.1109/LSP.2014.2310494
   Yang J, 2018, IEEE T MULTIMEDIA, V20, P1260, DOI 10.1109/TMM.2017.2760630
NR 53
TC 37
Z9 42
U1 7
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 716
EP 731
DI 10.1109/TMM.2020.2987693
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA QA6NL
UT WOS:000613560200014
DA 2024-07-18
ER

PT J
AU Lee, I
   Kim, D
   Lee, S
AF Lee, Inwoong
   Kim, Doyoung
   Lee, Sanghoon
TI 3-D Human Behavior Understanding Using Generalized TS-LSTM Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Skeleton; Hidden Markov models; Feature extraction; Three-dimensional
   displays; Dynamics; Machine learning; Solid modeling; Human action
   recognition; fusion of deep learning; long short-term memory; temporal
   sequence analysis
ID ACTIONLET ENSEMBLE; RECOGNITION; JOINTS; POSE
AB This paper addresses the problems of skeleton feature representation and the modeling of temporal dynamics to recognize human actions consisting of poses. In contrast to traditional methods which generally used relative coordinate systems dependent on some joints, or modeled only the long-term dependency, we attempt to understand 3D human behavior with observation by taking temporally different windows. Instead of taking raw skeletons as the input, we transform the skeletons into another coordinate system to obtain the robustness to scale, rotation and translation, and extract motion features between adjacent skeletons, which finally constructs an efficient hybrid-stream combining both pose and motion streams. We propose novel generalized Temporal Sliding Long Short-term Memory (TS-LSTM) networks. The proposed networks are composed of multiple TS-LSTM networks with various hyper-parameters, which can capture various temporal dynamics of actions. We also propose a novel hyper-parameter searching method, which finds decent hyper-parameters of generalized TS-LSTM to handle temporal dynamics of actions. In the experiment, we evaluate the proposed networks to verify the effectiveness of the proposed methods, and compare them with the other methods on three challenging datasets. Additionally, we analyze a relation between the recognized actions and the hyper-parameters, and visualize the layers of the proposed models.
C1 [Lee, Inwoong; Kim, Doyoung; Lee, Sanghoon] Yonsei Univ, Dept Elect & Elect Engn, Seoul 03722, South Korea.
C3 Yonsei University
RP Lee, S (corresponding author), Yonsei Univ, Dept Elect & Elect Engn, Seoul 03722, South Korea.
EM mayddb100@yonsei.ac.kr; tnyffx@yonsei.ac.kr; slee@yonsei.ac.kr
RI Cataldi, Antonio/AAM-7411-2021
OI Lee, Inwoong/0000-0003-4356-7616; Kim, Doyoung/0000-0002-8156-9738; Lee,
   Sanghoon/0000-0001-9895-5347
FU National Research Foundation of Korea (NRF); Korea government (MSIT)
   [2020R1A2C3011697]
FX This work was supported in part by the National Research Foundation of
   Korea (NRF) grant funded in part by the Korea government (MSIT) no.
   2020R1A2C3011697.
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   Aggarwal JK, 1997, IEEE NONRIGID AND ARTICULATED MOTION WORKSHOP, PROCEEDINGS, P90, DOI 10.1109/NAMW.1997.609859
   Nguyen AD, 2019, IEEE I CONF COMP VIS, P8627, DOI 10.1109/ICCV.2019.00872
   Devanne M, 2015, IEEE T CYBERNETICS, V45, P1340, DOI 10.1109/TCYB.2014.2350774
   Du WB, 2017, IEEE I CONF COMP VIS, P3745, DOI 10.1109/ICCV.2017.402
   Du Y, 2016, IEEE T IMAGE PROCESS, V25, P3010, DOI 10.1109/TIP.2016.2552404
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Feichtenhofer C., 2016, P NEUR INF PROC SYST, P3476
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Guo F, 2016, ONCOGENE, V35, P816, DOI 10.1038/onc.2015.139
   Han L, 2010, IMAGE VISION COMPUT, V28, P836, DOI 10.1016/j.imavis.2009.08.003
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hsieh JW, 2008, IEEE T MULTIMEDIA, V10, P372, DOI 10.1109/TMM.2008.917403
   Hussein, 2013, INT JOINT C ART INT
   Ke QH, 2018, IEEE T IMAGE PROCESS, V27, P2842, DOI 10.1109/TIP.2018.2812099
   Ke Q, 2017, PROC CVPR IEEE, P4570, DOI 10.1109/CVPR.2017.486
   Kim H, 2014, IEEE T IMAGE PROCESS, V23, P1476, DOI 10.1109/TIP.2014.2303640
   Kim J., 2018, CANCERS BASEL, V10, P1
   Kim J, 2019, PROC CVPR IEEE, P10583, DOI [10.1109/CVP8.2019.01084, 10.1109/CVPR.2019.01084]
   Kim Woojae, 2018, P EUR C COMP VIS ECC, P219
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kwon B, 2017, IEEE ACCESS, V5, P12496, DOI 10.1109/ACCESS.2017.2723039
   Lee I, 2017, IEEE I CONF COMP VIS, P1012, DOI 10.1109/ICCV.2017.115
   Lee K, 2018, LECT NOTES COMPUT SC, V11211, P123, DOI 10.1007/978-3-030-01234-2_8
   Li B, 2017, IEEE INT C COMPUT, P187, DOI 10.1109/CSE-EUC.2017.217
   Li M, 2016, IEEE T MULTIMEDIA, V18, P2293, DOI 10.1109/TMM.2016.2614228
   Li W, 2017, IEEE I CONF COMP VIS, P1453, DOI 10.1109/ICCV.2017.161
   Liu H., 2017, 2 STREAM 3D CONVOLUT
   Liu J, 2018, IEEE T IMAGE PROCESS, V27, P1586, DOI 10.1109/TIP.2017.2785279
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P1453, DOI 10.1109/TPAMI.2019.2898954
   Liu J, 2017, PROC CVPR IEEE, P3671, DOI 10.1109/CVPR.2017.391
   Liu MY, 2017, PATTERN RECOGN, V68, P346, DOI 10.1016/j.patcog.2017.02.030
   Luo J., 2013, P INT C COMP VIS, P5833
   Martinez J, 2017, IEEE I CONF COMP VIS, P2659, DOI 10.1109/ICCV.2017.288
   Newell A., 2017, P NIPS, P2171
   Núñez JC, 2018, PATTERN RECOGN, V76, P80, DOI 10.1016/j.patcog.2017.10.033
   Oh H, 2017, IEEE T IMAGE PROCESS, V26, P3789, DOI 10.1109/TIP.2017.2702383
   Pavlakos G, 2017, PROC CVPR IEEE, P1253, DOI 10.1109/CVPR.2017.138
   Rahmani H, 2017, IEEE I CONF COMP VIS, P5833, DOI 10.1109/ICCV.2017.621
   Rahmani H, 2016, IEEE T PATTERN ANAL, V38, P2430, DOI 10.1109/TPAMI.2016.2533389
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Shi L, 2019, PROC CVPR IEEE, P7904, DOI 10.1109/CVPR.2019.00810
   Shi L, 2019, PROC CVPR IEEE, P12018, DOI 10.1109/CVPR.2019.01230
   Shotton J, 2013, COMMUN ACM, V56, P116, DOI 10.1145/2398356.2398381
   Simonyan K, 2014, ADV NEUR IN, V27
   Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82
   Wang J, 2014, PROC CVPR IEEE, P2649, DOI 10.1109/CVPR.2014.339
   Wang J, 2014, IEEE T PATTERN ANAL, V36, P914, DOI 10.1109/TPAMI.2013.198
   Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813
   Wang L, 2017, IEEE T MULTIMEDIA, V19, P646, DOI 10.1109/TMM.2016.2617079
   Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059
   Wu D, 2014, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2014.98
   Xia L., 2012, CVPR 2012 HAU3D Workshop, P20
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yang YH, 2017, IEEE T MULTIMEDIA, V19, P519, DOI 10.1109/TMM.2016.2626959
   Ye M, 2014, PROC CVPR IEEE, P2353, DOI 10.1109/CVPR.2014.301
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang SY, 2017, IEEE WINT CONF APPL, P148, DOI 10.1109/WACV.2017.24
   Zhang Y, 2017, IEEE I CONF COMP VIS, P2116, DOI 10.1109/ICCV.2017.231
   Zhu GY, 2007, IEEE T MULTIMEDIA, V9, P1167, DOI 10.1109/TMM.2007.902847
   Zhu GY, 2009, IEEE T MULTIMEDIA, V11, P49, DOI 10.1109/TMM.2008.2008918
   Zhu WT, 2016, AAAI CONF ARTIF INTE, P3697
   Zhu Y, 2013, IEEE COMPUT SOC CONF, P486, DOI 10.1109/CVPRW.2013.78
NR 63
TC 22
Z9 22
U1 2
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 415
EP 428
DI 10.1109/TMM.2020.2978637
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA PJ6LW
UT WOS:000601877600033
DA 2024-07-18
ER

PT J
AU Liu, D
   Zhang, K
   Chen, ZZ
AF Liu, Di
   Zhang, Kao
   Chen, Zhenzhong
TI Attentive Cross-Modal Fusion Network for RGB-D Saliency Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Object detection; Saliency detection; Feature extraction; Fuses;
   Visualization; Computational modeling; Semantics; Cross-modal attention;
   residual attention; fusion refinement network; RGB-D salient object
   detection
ID OBJECT DETECTION; MODEL; DISPARITY; FIXATION
AB In this paper, an attentive cross-modal fusion (ACMF) network is proposed for RGB-D salient object detection. The proposed method selectively fuses features in a cross-modal manner and uses a fusion refinement module to fuse output features from different resolutions. Our attentive cross-modal fusion network is built based on residual attention. In each level of ResNet output, both the RGB and depth features are turned into an identity map and a weighted attention map. The identity map is reweighted by the attention map of the paired modality. Moreover, the lower level features with higher resolution are adopted to refine the boundary of detected targets. The entire architecture can be trained end-to-end. The proposed ACMF is compared with state-of-the-art methods on eight recent datasets. The results demonstrate that our model can achieve advanced performance on RGB-D salient object detection.
C1 [Liu, Di; Zhang, Kao; Chen, Zhenzhong] Wuhan Univ, Sch Remote Sensing & Informat Engn, Wuhan 430079, Peoples R China.
C3 Wuhan University
RP Chen, ZZ (corresponding author), Wuhan Univ, Sch Remote Sensing & Informat Engn, Wuhan 430079, Peoples R China.
EM dliu@whu.edu.cn; zhangkao@whu.edu.cn; zzchen@ieee.org
RI Chen, Zhenzhong/C-2529-2015
OI zhang, kao/0000-0001-9111-0499
FU National Key R&D Program of China [2017YFB1002202]; National Natural
   Science Foundation of China [61771348]; Wuhan Morning Light Plan of
   Youth Science and Technology [2017050304010302]
FX This work was supported in part by the National Key R&D Program of China
   under Contract 2017YFB1002202, in part by the National Natural Science
   Foundation of China under Contract 61771348, and in part by Wuhan
   Morning Light Plan of Youth Science and Technology under Contract
   2017050304010302.
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89
   Borji A, 2012, PROC CVPR IEEE, P438, DOI 10.1109/CVPR.2012.6247706
   Chen H, 2018, PROC CVPR IEEE, P3051, DOI 10.1109/CVPR.2018.00322
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Cheng Y., 2014, ICIMCS, P23
   Cong RM, 2016, IEEE SIGNAL PROC LET, V23, DOI 10.1109/LSP.2016.2557347
   Cong RM, 2018, IEEE T IMAGE PROCESS, V27, P568, DOI 10.1109/TIP.2017.2763819
   Ding XY, 2019, IEEE T IMAGE PROCESS, V28, P5379, DOI 10.1109/TIP.2019.2918735
   Ding XY, 2019, IEEE T MULTIMEDIA, V21, P124, DOI 10.1109/TMM.2018.2851389
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2019, RETHINKING RGB D SAL
   Feng D, 2016, PROC CVPR IEEE, P2343, DOI 10.1109/CVPR.2016.257
   Fu HZ, 2017, IEEE T IMAGE PROCESS, V26, P1418, DOI 10.1109/TIP.2017.2651369
   Fu KR, 2019, IEEE T MULTIMEDIA, V21, P457, DOI 10.1109/TMM.2018.2859746
   Guo CL, 2010, IEEE T IMAGE PROCESS, V19, P185, DOI 10.1109/TIP.2009.2030969
   Han JW, 2018, IEEE T CYBERNETICS, V48, P3171, DOI 10.1109/TCYB.2017.2761775
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hori C, 2017, IEEE I CONF COMP VIS, P4203, DOI 10.1109/ICCV.2017.450
   Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563
   Hou XD, 2007, PROC CVPR IEEE, P2280
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jansen L, 2009, J VISION, V9, DOI 10.1167/9.1.29
   Ju R, 2014, IEEE IMAGE PROC, P1115, DOI 10.1109/ICIP.2014.7025222
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Lee GY, 2016, PROC CVPR IEEE, P660, DOI 10.1109/CVPR.2016.78
   Lei JJ, 2016, IEEE T MULTIMEDIA, V18, P1783, DOI 10.1109/TMM.2016.2592325
   Li GB, 2016, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2016.58
   Li J, 2015, IEEE T PATTERN ANAL, V37, P2428, DOI 10.1109/TPAMI.2015.2424870
   Li NY, 2014, PROC CVPR IEEE, P2806, DOI 10.1109/CVPR.2014.359
   Li X, 2016, IEEE T IMAGE PROCESS, V25, P3919, DOI 10.1109/TIP.2016.2579306
   Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549
   Liu D, 2018, J VIS COMMUN IMAGE R, V57, P218, DOI 10.1016/j.jvcir.2018.10.002
   Liu D, 2018, J VIS COMMUN IMAGE R, V56, P73, DOI 10.1016/j.jvcir.2018.07.015
   Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Ma CY, 2015, J VISION, V15, DOI 10.1167/15.6.19
   Nager W, 2006, BMC NEUROSCI, V7, DOI 10.1186/1471-2202-7-31
   Niu YZ, 2012, PROC CVPR IEEE, P454, DOI 10.1109/CVPR.2012.6247708
   Park SJ, 2017, IEEE I CONF COMP VIS, P4990, DOI 10.1109/ICCV.2017.533
   Peng HW, 2014, LECT NOTES COMPUT SC, V8691, P92, DOI 10.1007/978-3-319-10578-9_7
   Peng YX, 2018, IEEE T IMAGE PROCESS, V27, P5585, DOI 10.1109/TIP.2018.2852503
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Piao YR, 2019, IEEE I CONF COMP VIS, P7253, DOI 10.1109/ICCV.2019.00735
   Pingping Zhang, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P202, DOI 10.1109/ICCV.2017.31
   Qu LQ, 2017, IEEE T IMAGE PROCESS, V26, P2274, DOI 10.1109/TIP.2017.2682981
   Ren JQ, 2015, IEEE COMPUT SOC CONF, DOI 10.1109/CVPRW.2015.7301391
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shigematsu R, 2017, IEEE INT CONF COMP V, P2749, DOI 10.1109/ICCVW.2017.323
   TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang LJ, 2015, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR.2015.7298938
   Wang TT, 2017, IEEE I CONF COMP VIS, P4039, DOI 10.1109/ICCV.2017.433
   Wang ZL, 2017, IEEE T MULTIMEDIA, V19, P750, DOI 10.1109/TMM.2016.2636739
   Xiao HX, 2018, IEEE T MULTIMEDIA, V20, P3239, DOI 10.1109/TMM.2018.2830098
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   Ye LW, 2017, IEEE T MULTIMEDIA, V19, P1742, DOI 10.1109/TMM.2017.2693022
   Yoo JW, 2013, IEEE SIGNAL PROC LET, V20, P519, DOI 10.1109/LSP.2013.2252165
   Zhang JM, 2016, IEEE T PATTERN ANAL, V38, P889, DOI 10.1109/TPAMI.2015.2473844
   Zhang K, 2019, IEEE T CIRC SYST VID, V29, P3544, DOI 10.1109/TCSVT.2018.2883305
   Zhang LY, 2008, J VISION, V8, DOI 10.1167/8.7.32
   Zhang PP, 2017, IEEE I CONF COMP VIS, P212, DOI 10.1109/ICCV.2017.32
   Zhang XN, 2018, PROC CVPR IEEE, P714, DOI 10.1109/CVPR.2018.00081
   Zhao J, 2020, IEEE T FUZZY SYST, V28, P2287, DOI 10.1109/TFUZZ.2019.2930492
   Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731
   Zhu CB, 2019, IEEE INT CON MULTI, P199, DOI 10.1109/ICME.2019.00042
   Zhu CB, 2017, IEEE INT CONF COMP V, P3008, DOI 10.1109/ICCVW.2017.355
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
NR 70
TC 21
Z9 21
U1 0
U2 35
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 967
EP 981
DI 10.1109/TMM.2020.2991523
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA QO8XK
UT WOS:000623420300011
DA 2024-07-18
ER

PT J
AU Liu, HJ
   Tan, XH
   Zhou, XC
AF Liu, Haijun
   Tan, Xiaoheng
   Zhou, Xichuan
TI Parameter Sharing Exploration and Hetero-Center Triplet Loss for
   Visible-Thermal Person Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Cameras; Training; Task analysis; Measurement;
   Generative adversarial networks; Loss measurement; Cross-modality
   discrepancy; hetero-center triplet loss; parameters sharing;
   visible-thermal person re-identification
AB This paper focuses on the visible-thermal cross-modality person re-identification (VT Re-ID) task, whose goal is to match person images between the daytime visible modality and the nighttime thermal modality. The two-stream network is usually adopted to address the cross-modality discrepancy, the most challenging problem for VT Re-ID, by learning the multi-modality person features. In this paper, we explore how many parameters a two-stream network should share, which is still not well investigated in the existing literature. By splitting the ResNet50 model to construct the modality-specific feature extraction network and modality-sharing feature embedding network, we experimentally demonstrate the effect of parameter sharing of two-stream network for VT Re-ID. Moreover, in the framework of part-level person feature learning, we propose the hetero-center triplet loss to relax the strict constraint of traditional triplet loss by replacing the comparison of the anchor to all the other samples by the anchor center to all the other centers. With extremely simple means, the proposed method can significantly improve the VT Re-ID performance. The experimental results on two datasets show that our proposed method distinctly outperforms the state-of-the-art methods by large margins, especially on the RegDB dataset achieving superior performance, rank1/mAP/mINP 91.05%/83.28%/68.84%. It can be a new baseline for VT Re-ID, with a simple but effective strategy.
C1 [Liu, Haijun; Tan, Xiaoheng; Zhou, Xichuan] Chongqing Univ, Sch Microelectron & Communt Engn, Chongqing 400044, Peoples R China.
C3 Chongqing University
RP Liu, HJ (corresponding author), Chongqing Univ, Sch Microelectron & Communt Engn, Chongqing 400044, Peoples R China.
EM haijun_liu@126.com; txh@cqu.edu.cn; zxc@cqu.edu.cn
RI Zhou, Xichuan/E-1208-2016
FU National Natural Science Foundation of China [62 001 063, U20A20157,
   61971072]; China Postdoctoral Science Foundation [2020M673135]
FX This work was supported in part by the National Natural Science
   Foundation of China (62 001 063, U20A20157, and 61971072), and in part
   by the China Postdoctoral Science Foundation (2020M673135). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Ling-Yu Duan.
CR Dai PY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P677
   Feng ZX, 2020, IEEE T IMAGE PROCESS, V29, P579, DOI 10.1109/TIP.2019.2928126
   HAO Y, 2019, P 27 ACM INT C MULT
   Hao Y, 2019, AAAI CONF ARTIF INTE, P8385
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hermans Alexander, 2017, ARXIV170307737
   Jiang W, 2020, ARXIV200300213
   Kang JK, 2019, IEEE ACCESS, V7, P57972, DOI 10.1109/ACCESS.2019.2914670
   Kim T., 2020, P C COMP VIS PATT, p10 257
   Kniaz VV, 2019, LECT NOTES COMPUT SC, V11134, P606, DOI 10.1007/978-3-030-11024-6_46
   Li DG, 2020, AAAI CONF ARTIF INTE, V34, P4610
   Li X, 2020, ARXIV200600878
   Liu HJ, 2020, NEUROCOMPUTING, V398, P11, DOI 10.1016/j.neucom.2020.01.089
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Nguyen DT, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17030605
   Radenovic F, 2019, IEEE T PATTERN ANAL, V41, P1655, DOI 10.1109/TPAMI.2018.2846566
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Wan CQ, 2020, IEEE T MULTIMEDIA, V22, P1605, DOI 10.1109/TMM.2019.2946486
   Wang GA, 2020, AAAI CONF ARTIF INTE, V34, P12144
   Wang GA, 2019, IEEE I CONF COMP VIS, P3622, DOI 10.1109/ICCV.2019.00372
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   WANG P, IN PRESS, DOI DOI 10.1109/TMM.2020.2999180
   Wang ZX, 2019, PROC CVPR IEEE, P618, DOI 10.1109/CVPR.2019.00071
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Wu AC, 2020, INT J COMPUT VISION, V128, P1765, DOI 10.1007/s11263-019-01290-1
   Wu AC, 2017, IEEE I CONF COMP VIS, P5390, DOI 10.1109/ICCV.2017.575
   Yan Lu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13376, DOI 10.1109/CVPR42600.2020.01339
   Yang F, 2021, IEEE J EM SEL TOP P, V9, P4026, DOI 10.1109/JESTPE.2020.2970335
   Ye M, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1092
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Ye M, 2021, IEEE T INF FOREN SEC, V16, P728, DOI 10.1109/TIFS.2020.3001665
   Ye M, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P347, DOI 10.1145/3343031.3351043
   Ye M, 2020, IEEE T INF FOREN SEC, V15, P407, DOI 10.1109/TIFS.2019.2921454
   Ye M, 2018, AAAI CONF ARTIF INTE, P7501
   Zhu YX, 2020, NEUROCOMPUTING, V386, P97, DOI 10.1016/j.neucom.2019.12.100
   Zhang SK, 2020, J ELECTRON IMAGING, V29, DOI 10.1117/1.JEI.29.3.033017
   Zhang SZ, 2021, IEEE T IMAGE PROCESS, V30, P8861, DOI 10.1109/TIP.2021.3120881
   Zhao YB, 2019, IET IMAGE PROCESS, V13, P2897, DOI 10.1049/iet-ipr.2019.0699
NR 39
TC 93
Z9 102
U1 3
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 4414
EP 4425
DI 10.1109/TMM.2020.3042080
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XM6HD
UT WOS:000728924800002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Liu, Q
   Li, X
   He, ZY
   Fan, NN
   Yuan, D
   Wang, HP
AF Liu, Qiao
   Li, Xin
   He, Zhenyu
   Fan, Nana
   Yuan, Di
   Wang, Hongpeng
TI Learning Deep Multi-Level Similarity for Thermal Infrared Object
   Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Object tracking; Semantics; Training; Task analysis; Adaptation models;
   Correlation; Feature extraction; TIR object tracking; Multi-level
   similarity; Siamese network; Thermal infrared dataset
ID TARGET TRACKING; SIAMESE NETWORK
AB Existing deep Thermal InfraRed (TIR) trackers only use semantic features to represent the TIR object, which lack the sufficient discriminative capacity for handling distractors. This becomes worse when the feature extraction network is only trained on RGB images. To address this issue, we propose a multi-level similarity model under a Siamese framework for robust TIR object tracking. Specifically, we compute different pattern similarities using the proposed multi-level similarity network. One of them focuses on the global semantic similarity and the other computes the local structural similarity of the TIR object. These two similarities complement each other and hence enhance the discriminative capacity of the network for handling distractors. In addition, we design a simple while effective relative entropy based ensemble subnetwork to integrate the semantic and structural similarities. This subnetwork can adaptive learn the weights of the semantic and structural similarities at the training stage. To further enhance the discriminative capacity of the tracker, we propose a large-scale TIR video sequence dataset for training the proposed model. To the best of our knowledge, this is the first and the largest TIR object tracking training dataset to date. The proposed TIR dataset not only benefits the training for TIR object tracking but also can be applied to numerous TIR visual tasks. Extensive experimental results on three benchmarks demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods.
C1 [Liu, Qiao; Li, Xin; He, Zhenyu; Fan, Nana; Yuan, Di; Wang, Hongpeng] Harbin Inst Technol, Sch Comp Sci & Technol, Shenzhen 518000, Peoples R China.
C3 Harbin Institute of Technology
RP He, ZY; Wang, HP (corresponding author), Harbin Inst Technol, Sch Comp Sci & Technol, Shenzhen 518000, Peoples R China.
EM liuqiao.hit@gmail.com; xinlihitsz@gmail.com; zhenyuhe@hit.edu.cn;
   nanafanhit@gmail.com; 1107449172@qq.com; wanghp@hit.edu.cn
RI Yuan, Di/Q-6521-2019
OI Yuan, Di/0000-0001-9403-1112
FU National Natural Science Foundation of China [61672183]; Shenzhen
   Research Council [JCYJ20170413104556946, JCYJ20170815113552036]
FX This research was supported in part by the National Natural Science
   Foundation of China under Grant 61672183 and in part by the Shenzhen
   Research Council under Grants JCYJ20170413104556946 and
   JCYJ20170815113552036.
CR [Anonymous], 2016, ARXIV PREPRINT ARXIV
   [Anonymous], 2015, PROC 12 IEEE INT C A
   [Anonymous], 2015, PROC IEEE INT C COMP
   Asha CS, 2017, INFRARED PHYS TECHN, V85, P114, DOI 10.1016/j.infrared.2017.05.022
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bhat G, 2019, IEEE I CONF COMP VIS, P6181, DOI 10.1109/ICCV.2019.00628
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Chen K, 2019, IEEE T MULTIMEDIA, V21, P86, DOI 10.1109/TMM.2018.2846405
   Chockalingam A., 2018, P IEEE 87 VEH TECHN, P1
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Davis JW, 2007, COMPUT VIS IMAGE UND, V106, P162, DOI 10.1016/j.cviu.2006.06.010
   Dong XP, 2021, IEEE T PATTERN ANAL, V43, P1515, DOI 10.1109/TPAMI.2019.2956703
   Dong XP, 2019, IEEE T IMAGE PROCESS, V28, P3516, DOI 10.1109/TIP.2019.2898567
   Dong XP, 2018, LECT NOTES COMPUT SC, V11217, P472, DOI 10.1007/978-3-030-01261-8_28
   Dong XP, 2017, IEEE T MULTIMEDIA, V19, P763, DOI 10.1109/TMM.2016.2631884
   Felsberg M, 2016, LECT NOTES COMPUT SC, V9914, P824, DOI 10.1007/978-3-319-48881-3_55
   Gade R, 2014, MACH VISION APPL, V25, P245, DOI 10.1007/s00138-013-0570-5
   Gao P, 2018, INT C PATT RECOG, P2380, DOI 10.1109/ICPR.2018.8545716
   Gao SJ, 2016, 2016 RESEARCH IN ADAPTIVE AND CONVERGENT SYSTEMS, P40, DOI 10.1145/2987386.2987392
   Gkioxari G, 2015, PROC CVPR IEEE, P759, DOI 10.1109/CVPR.2015.7298676
   Gundogdu Erhan, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P1, DOI 10.1109/CVPRW.2015.7301290
   Gundogdu E., 2016, P IEEE C COMP VIS PA, P24
   Guo Q, 2017, IEEE I CONF COMP VIS, P1781, DOI 10.1109/ICCV.2017.196
   Han B, 2017, PROC CVPR IEEE, P521, DOI 10.1109/CVPR.2017.63
   Hare S, 2016, IEEE T PATTERN ANAL, V38, P2096, DOI 10.1109/TPAMI.2015.2509974
   He AF, 2018, PROC CVPR IEEE, P4834, DOI 10.1109/CVPR.2018.00508
   He YJ, 2015, INFRARED PHYS TECHN, V73, P103, DOI 10.1016/j.infrared.2015.09.010
   He YJ, 2016, IEEE GEOSCI REMOTE S, V13, P232, DOI 10.1109/LGRS.2015.2506758
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Hu HW, 2019, IEEE T MULTIMEDIA, V21, P510, DOI 10.1109/TMM.2018.2859831
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Kristan M, 2015, P IEEE INT C COMPUTE, P1
   Kristan M, 2017, IEEE INT CONF COMP V, P1949, DOI 10.1109/ICCVW.2017.230
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li CL, 2019, PATTERN RECOGN, V96, DOI 10.1016/j.patcog.2019.106977
   Li F, 2018, PROC CVPR IEEE, P4904, DOI 10.1109/CVPR.2018.00515
   Li X, 2019, PROC CVPR IEEE, P1369, DOI 10.1109/CVPR.2019.00146
   Li X, 2019, KNOWL-BASED SYST, V166, P71, DOI 10.1016/j.knosys.2018.12.011
   Li Y, 2014, APPL OPTICS, V53, P6518, DOI 10.1364/AO.53.006518
   Liang ZY, 2020, IEEE T IMAGE PROCESS, V29, P3351, DOI 10.1109/TIP.2019.2959256
   Liu Q, 2020, IEEE T MULTIMEDIA, V22, P666, DOI 10.1109/TMM.2019.2932615
   Liu Q, 2017, KNOWL-BASED SYST, V134, P189, DOI 10.1016/j.knosys.2017.07.032
   Liu RM, 2012, INFRARED PHYS TECHN, V55, P505, DOI 10.1016/j.infrared.2012.08.003
   Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352
   Miezianko R., IEEE OTCBVS WS Series Bench
   Portmann J, 2014, IEEE INT CONF ROBOT, P1794, DOI 10.1109/ICRA.2014.6907094
   Qi YK, 2016, PROC CVPR IEEE, P4303, DOI 10.1109/CVPR.2016.466
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shen JB, 2020, IEEE T CYBERNETICS, V50, P3068, DOI 10.1109/TCYB.2019.2936503
   Shen JB, 2018, IEEE T INTELL TRANSP, V19, P162, DOI 10.1109/TITS.2017.2750082
   Shi X., 2013, P SENS SYST SPAC APP, V8739
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song YB, 2018, PROC CVPR IEEE, P8990, DOI 10.1109/CVPR.2018.00937
   Song YB, 2017, IEEE I CONF COMP VIS, P2574, DOI 10.1109/ICCV.2017.279
   Tang M, 2015, IEEE I CONF COMP VIS, P3038, DOI 10.1109/ICCV.2015.348
   Tian CW, 2020, NEURAL NETWORKS, V124, P117, DOI 10.1016/j.neunet.2019.12.024
   Tian CW, 2020, NEURAL NETWORKS, V121, P461, DOI 10.1016/j.neunet.2019.08.022
   Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Wang LJ, 2016, PROC CVPR IEEE, P1373, DOI 10.1109/CVPR.2016.153
   Wang LJ, 2015, IEEE I CONF COMP VIS, P3119, DOI 10.1109/ICCV.2015.357
   Wang N, 2019, PROC CVPR IEEE, P1308, DOI 10.1109/CVPR.2019.00140
   Wang Q, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P985
   Wang Q, 2018, PROC CVPR IEEE, P4854, DOI 10.1109/CVPR.2018.00510
   Wang QR, 2019, IEEE T MULTIMEDIA, V21, P930, DOI 10.1109/TMM.2018.2869277
   Wang WG, 2019, IEEE I CONF COMP VIS, P9235, DOI 10.1109/ICCV.2019.00933
   Wang WG, 2019, PROC CVPR IEEE, P1448, DOI 10.1109/CVPR.2019.00154
   Wen J, 2019, IEEE T CIRC SYST VID, V29, P390, DOI 10.1109/TCSVT.2018.2799214
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Wu Z, 2014, IEEE COMPUT SOC CONF, P201, DOI 10.1109/CVPRW.2014.39
   Yan B, 2019, IEEE I CONF COMP VIS, P2385, DOI 10.1109/ICCV.2019.00247
   Yu XG, 2017, PATTERN RECOGN LETT, V100, P152, DOI 10.1016/j.patrec.2017.10.026
   Zhang LC, 2019, IEEE T IMAGE PROCESS, V28, P1837, DOI 10.1109/TIP.2018.2879249
   Zhang YH, 2018, LECT NOTES COMPUT SC, V11213, P355, DOI 10.1007/978-3-030-01240-3_22
   Zheng F, 2018, IEEE T INTELL TRANSP, V19, P3387, DOI 10.1109/TITS.2017.2749981
   Zhu Z, 2018, PROC CVPR IEEE, P548, DOI 10.1109/CVPR.2018.00064
   Zhu Z, 2018, LECT NOTES COMPUT SC, V11213, P103, DOI 10.1007/978-3-030-01240-3_7
NR 82
TC 90
Z9 90
U1 2
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2114
EP 2126
DI 10.1109/TMM.2020.3008028
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TC8FT
UT WOS:000668875100021
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wang, XJ
   Jiang, QP
   Shao, F
   Gu, K
   Zhai, GT
   Yang, XK
AF Wang, Xuejin
   Jiang, Qiuping
   Shao, Feng
   Gu, Ke
   Zhai, Guangtao
   Yang, Xiaokang
TI Exploiting Local Degradation Characteristics and Global Statistical
   Properties for Blind Quality Assessment of Tone-Mapped HDR Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Distortion; Degradation; Dynamic range; Image
   quality; Standards; Quality assessment; tone-mapped image; high dynamic
   range; multi-scale statistics; multi-resolution statistics; no reference
AB Tone mapping operators (TMOs) are developed to convert a high dynamic range (HDR) image into a low dynamic range (LDR) one for display with the goal of preserving as much visual information as possible. However, image quality degradation is inevitable due to the dynamic range compression during the tone-mapping process. This accordingly raises an urgent demand for effective quality evaluation methods to select a high-quality tone-mapped image (TMI) from a set of candidates generated by distinct TMOs or the same TMO with different parameter settings. A key element to the success of TMI quality evaluation is to extract effective features that are highly consistent with human perception. Towards this end, this paper proposes a novel blind TMI quality metric by exploiting both local degradation characteristics and global statistical properties for feature extraction. Several image attributes including texture, structure, colorfulness and naturalness are considered either locally or globally. The extracted local and global features are aggregated into an overall quality via regression. Experimental results on two benchmark databases demonstrate the superiority of the proposed metric over both the state-of-the-art blind quality models designed for synthetically distorted images (SDIs) and the blind quality models specifically developed for TMIs.
C1 [Wang, Xuejin; Jiang, Qiuping; Shao, Feng] Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Peoples R China.
   [Gu, Ke] Beijing Univ Technol, Beijing Adv Innovat Ctr Future Internet Technol, Fac Informat Technol, Beijing Key Lab Computat Intelligence & Intellige, Beijing 100124, Peoples R China.
   [Zhai, Guangtao; Yang, Xiaokang] Shanghai Jiao Tong Univ, Inst Image Commun & Network Engn, Shanghai 200240, Peoples R China.
C3 Ningbo University; Beijing University of Technology; Shanghai Jiao Tong
   University
RP Shao, F (corresponding author), Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Peoples R China.
EM 1801082022@nbu.edu.cn; jiangqiuping@nbu.edu.cn; shaofeng@nbu.edu.cn;
   guke.doctor@gmail.com; zhaiguangtao@sjtu.edu.cn; xkyang@sjtu.edu.cn
RI Gu, Ke/AAJ-9684-2021; Zhai, Guangtao/X-5949-2019; Qiuping,
   Jiang/AAO-2830-2021; Yang, Xiaokang/C-6137-2009
OI Zhai, Guangtao/0000-0001-8165-9322; Qiuping, Jiang/0000-0002-6025-9343;
   Yang, Xiaokang/0000-0003-4029-3322; Wang, Xuejin/0000-0001-5772-2774
FU Natural Science Foundation of China [61901236, 61622109]; Zhejiang
   Natural Science Foundation of China [R18F010008]; Natural Science
   Foundation of Ningbo [2019A610097, 2017A610112]; K.C. Wong Magna Fund in
   Ningbo University
FX Thisworkwas supported in part by theNatural Science Foundation of China
   under Grants 61901236 and 61622109, in part by the Zhejiang Natural
   Science Foundation of China under Grant R18F010008, in part by the
   Natural Science Foundation of Ningbo under Grants 2019A610097,
   2017A610112, and in part by K.C. Wong Magna Fund in Ningbo University.
CR [Anonymous], 2011, ACM T INTEL SYST TEC, DOI DOI 10.1145/1961189.1961199
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chen PF, 2019, PATTERN RECOGN, V89, P108, DOI 10.1016/j.patcog.2019.01.010
   Durand F, 2002, ACM T GRAPHIC, V21, P257, DOI 10.1145/566570.566574
   Fattal R, 2002, ACM T GRAPHIC, V21, P249
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P432, DOI 10.1109/TMM.2016.2518868
   Gu K, 2016, IEEE T CYBERNETICS, V46, P284, DOI 10.1109/TCYB.2015.2401732
   Gu K, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2439035
   Gu K, 2014, IEEE INT SYMP CIRC S, P518, DOI 10.1109/ISCAS.2014.6865186
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Heidrich Wolfgang, ERIK REINHARD
   Jiang QP, 2019, IEEE T IMAGE PROCESS, V28, P1866, DOI 10.1109/TIP.2018.2881828
   Jiang QP, 2019, IEEE T CIRC SYST VID, V29, P323, DOI 10.1109/TCSVT.2017.2783938
   Jiang QP, 2018, IEEE T MULTIMEDIA, V20, P2035, DOI 10.1109/TMM.2017.2763321
   Jiang QP, 2015, J VIS COMMUN IMAGE R, V33, P123, DOI 10.1016/j.jvcir.2015.09.009
   Kundu D, 2017, IEEE T IMAGE PROCESS, V26, P4725, DOI 10.1109/TIP.2017.2713945
   Kundu D, 2017, IEEE T IMAGE PROCESS, V26, P2957, DOI 10.1109/TIP.2017.2685941
   Kundu D, 2016, IEEE IMAGE PROC, P96, DOI 10.1109/ICIP.2016.7532326
   Larson GW, 1997, IEEE T VIS COMPUT GR, V3, P291, DOI 10.1109/2945.646233
   Li LD, 2017, IEEE T MULTIMEDIA, V19, P1030, DOI 10.1109/TMM.2016.2640762
   Li LD, 2014, IEEE SIGNAL PROC LET, V21, P122, DOI 10.1109/LSP.2013.2294333
   Lin WS, 2011, J VIS COMMUN IMAGE R, V22, P297, DOI 10.1016/j.jvcir.2011.01.005
   Liu HT, 2010, IEEE T CIRC SYST VID, V20, P529, DOI 10.1109/TCSVT.2009.2035848
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P1004, DOI 10.1109/TIP.2016.2631888
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Mohammadi P., 2018, P IEEE IFIP INT C NE, P1
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Nafchi HZ, 2015, IEEE SIGNAL PROC LET, V22, P1026, DOI 10.1109/LSP.2014.2381458
   Nasrinpour HR, 2015, IEEE IMAGE PROC, P4947, DOI 10.1109/ICIP.2015.7351748
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Rehman A, 2012, IEEE T IMAGE PROCESS, V21, P3378, DOI 10.1109/TIP.2012.2197011
   Reinhard E, 2005, IEEE T VIS COMPUT GR, V11, P13, DOI 10.1109/TVCG.2005.9
   Reinhard E, 2002, ACM T GRAPHIC, V21, P267, DOI 10.1145/566570.566575
   Romeny B. M. H, 2003, FRONTEND VISION MULT
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Saremi S, 2013, P NATL ACAD SCI USA, V110, P3071, DOI 10.1073/pnas.1222618110
   Shao F, 2015, IEEE T IMAGE PROCESS, V24, P2971, DOI 10.1109/TIP.2015.2436332
   Tong HHY, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 3, P428, DOI 10.1109/ICIP.1998.999032
   Wang Q, 2016, NEUROCOMPUTING, V173, P1798, DOI 10.1016/j.neucom.2015.09.057
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xu JT, 2016, IEEE T IMAGE PROCESS, V25, P4444, DOI 10.1109/TIP.2016.2585880
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P4850, DOI 10.1109/TIP.2014.2355716
   Yang XK, 2005, SIGNAL PROCESS-IMAGE, V20, P662, DOI 10.1016/j.image.2005.04.001
   Yang XK, 2005, IEEE T CIRC SYST VID, V15, P742, DOI 10.1109/TCSVT.2005.848313
   Yeganeh H, 2013, IEEE T IMAGE PROCESS, V22, P657, DOI 10.1109/TIP.2012.2221725
   Yue GH, 2020, IEEE T IND INFORM, V16, P1764, DOI 10.1109/TII.2019.2927527
   Yue GH, 2019, IEEE T MULTIMEDIA, V21, P2184, DOI 10.1109/TMM.2019.2913315
   Yue GH, 2019, IEEE T IND ELECTRON, V66, P3784, DOI 10.1109/TIE.2018.2851984
   Yue GH, 2019, IEEE T INSTRUM MEAS, V68, P2733, DOI 10.1109/TIM.2018.2868555
   Yue GH, 2018, IEEE T IND ELECTRON, V65, P2525, DOI 10.1109/TIE.2017.2739708
   Zhai GT, 2008, IEEE T CIRC SYST VID, V18, P122, DOI 10.1109/TCSVT.2007.906942
   Zhai GT, 2012, IEEE T IMAGE PROCESS, V21, P41, DOI 10.1109/TIP.2011.2161092
   Zhai GT, 2008, IEEE T MULTIMEDIA, V10, P1316, DOI 10.1109/TMM.2008.2004910
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang Y, 2014, SIGNAL PROCESS-IMAGE, V29, P725, DOI 10.1016/j.image.2014.05.004
   Zhou WJ, 2021, IEEE T SYST MAN CY-S, V51, P3641, DOI 10.1109/TSMC.2019.2957386
NR 58
TC 21
Z9 23
U1 1
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 692
EP 705
DI 10.1109/TMM.2020.2986583
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA QI9PL
UT WOS:000619321200002
DA 2024-07-18
ER

PT J
AU Wang, YL
   Gong, D
   Yang, J
   Shi, QF
   van den Hengel, A
   Xie, DH
   Zeng, B
AF Wang, Yinglong
   Gong, Dong
   Yang, Jie
   Shi, Qinfeng
   van den Hengel, Anton
   Xie, Dehua
   Zeng, Bing
TI Deep Single Image Deraining via Modeling Haze-Like Effect
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Rain; Atmospheric modeling; Machine learning; Task analysis; Neural
   networks; Visualization; Image color analysis; Rain removing; haze-like
   effect; rain model; rain detection
ID RAIN STREAKS REMOVAL
AB Removing rain from images is of a great importance to various applications such as autonomous driving, drone piloting, and photo editing. Conventional methods rely on some heuristics to handcraft various priors to remove or separate rain from images. Recently, deep learning models are proposed to learn various end-to-end methods to complete this task. However, these methods might fail in obtaining satisfactory results in some real-world scenarios, especially when the captured images suffer from heavy rain that brings not only rain streaks but also a haze-like effect (caused by the accumulation of tiny raindrops). Different from most of the existing deep learning deraining methods that focus on handling rain streaks, we add a new variable to model the haze-like effect in a general model for rain, based on which a deep neural network is designed accordingly. Specifically, in our method, two branches are designed to handle rain streaks and the haze-like effect, respectively. The output of such branch structure is fed to an additional module to further enhance the performance. Three modules are trained jointly, leading to an end-to-end network, which supports a adjustment to the strength of removing the haze-like effect. Extensive experiments on several datasets show that our method outperforms several state-of-the-art methods in both objective assessment and visual quality.
C1 [Wang, Yinglong; Xie, Dehua; Zeng, Bing] Univ Elect Sci & Technol China, Inst Image Proc, Chengdu 611731, Peoples R China.
   [Gong, Dong; Yang, Jie; Shi, Qinfeng; van den Hengel, Anton] Univ Adelaide, Sch Comp Sci, Adelaide, SA 5005, Australia.
C3 University of Electronic Science & Technology of China; University of
   Adelaide
RP Zeng, B (corresponding author), Univ Elect Sci & Technol China, Inst Image Proc, Chengdu 611731, Peoples R China.
EM ylwanguestc@gmail.com; dong.gong@adelaide.edu.au; yangjie07@gmail.com;
   javen.shi@adelaide.edu.au; anton.vandenhengel@adelaide.edu.au;
   xiedehua2014@gmail.com; eezeng@uestc.edu.cn
RI Wang, Ying/HJI-2509-2023; wang, yinglong/HTN-9856-2023
OI van den Hengel, Anton/0000-0003-3027-8364; Gong,
   Dong/0000-0002-2668-9630; Shi, Javen Qinfeng/0000-0002-9126-2107
FU National Natural Science Foundation of China [61872067, 61502079,
   61672134, 61720106004]; 111 Project [B17008]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61872067, 61502079, 61672134, and
   61720106004, and in part by the 111 Project under Grant B17008.
CR Chen DY, 2014, IEEE T CIRC SYST VID, V24, P1430, DOI 10.1109/TCSVT.2014.2308627
   Fu XY, 2020, IEEE T NEUR NET LEAR, V31, P1794, DOI 10.1109/TNNLS.2019.2926481
   Fu XY, 2017, PROC CVPR IEEE, P1715, DOI 10.1109/CVPR.2017.186
   Fu XY, 2017, IEEE T IMAGE PROCESS, V26, P2944, DOI 10.1109/TIP.2017.2691802
   Garg K, 2004, PROC CVPR IEEE, P528
   Golts A, 2020, IEEE T IMAGE PROCESS, V29, P2692, DOI 10.1109/TIP.2019.2952032
   Gong D, 2020, IEEE T NEUR NET LEAR, V31, P5468, DOI 10.1109/TNNLS.2020.2968289
   Gong D, 2017, PROC CVPR IEEE, P3806, DOI 10.1109/CVPR.2017.405
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Huang DA, 2014, IEEE T MULTIMEDIA, V16, P83, DOI 10.1109/TMM.2013.2284759
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Kang LW, 2012, IEEE T IMAGE PROCESS, V21, P1742, DOI 10.1109/TIP.2011.2179057
   Kingma D. P., 2014, arXiv
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li, 2017, ARXIV171206830
   Li G, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1056, DOI 10.1145/3240508.3240636
   Li RT, 2019, PROC CVPR IEEE, P1633, DOI 10.1109/CVPR.2019.00173
   Li SY, 2019, COMPUT VIS IMAGE UND, V186, P48, DOI 10.1016/j.cviu.2019.05.003
   Li X, 2018, LECT NOTES COMPUT SC, V11211, P262, DOI 10.1007/978-3-030-01234-2_16
   Ma DQ, 2019, IEEE I CONF COMP VIS, P2444, DOI 10.1109/ICCV.2019.00253
   Mairal J, 2010, J MACH LEARN RES, V11, P19
   Narasimhan SG, 2002, INT J COMPUT VISION, V48, P233, DOI 10.1023/A:1016328200723
   Pan JS, 2021, IEEE T PATTERN ANAL, V43, P2449, DOI 10.1109/TPAMI.2020.2969348
   Ren DW, 2019, PROC CVPR IEEE, P3932, DOI 10.1109/CVPR.2019.00406
   Sifre L, THESIS ECOLE NORMALE
   Wan RJ, 2020, IEEE T PATTERN ANAL, V42, P2969, DOI 10.1109/TPAMI.2019.2921574
   Wang C, 2019, IEEE INT CON MULTI, P1276, DOI 10.1109/ICME.2019.00222
   Wang TY, 2019, PROC CVPR IEEE, P12262, DOI 10.1109/CVPR.2019.01255
   Wang WC, 2017, IEEE T MULTIMEDIA, V19, P1142, DOI 10.1109/TMM.2017.2652069
   Wang YL, 2017, IEEE T IMAGE PROCESS, V26, P3936, DOI 10.1109/TIP.2017.2708502
   Wang YL, 2016, IEEE IMAGE PROC, P4087, DOI 10.1109/ICIP.2016.7533128
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yan QS, 2019, PROC CVPR IEEE, P1751, DOI 10.1109/CVPR.2019.00185
   Yang J, 2018, LECT NOTES COMPUT SC, V11207, P675, DOI 10.1007/978-3-030-01219-9_40
   Yang WH, 2020, AAAI CONF ARTIF INTE, V34, P12629
   Yang WH, 2020, IEEE T IMAGE PROCESS, V29, P5737, DOI 10.1109/TIP.2020.2981922
   Yang WH, 2020, IEEE T PATTERN ANAL, V42, P1377, DOI 10.1109/TPAMI.2019.2895793
   Yang WH, 2019, IEEE T IMAGE PROCESS, V28, P2948, DOI 10.1109/TIP.2019.2892685
   Yang WH, 2017, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2017.183
   Yasarla R, 2019, PROC CVPR IEEE, P8397, DOI 10.1109/CVPR.2019.00860
   Zhang H, 2020, IEEE T CIRC SYST VID, V30, P3943, DOI 10.1109/TCSVT.2019.2920407
   Zhang H, 2018, PROC CVPR IEEE, P695, DOI 10.1109/CVPR.2018.00079
NR 44
TC 11
Z9 11
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2481
EP 2492
DI 10.1109/TMM.2020.3013383
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TS3DH
UT WOS:000679533800025
DA 2024-07-18
ER

PT J
AU Yang, LY
   Wang, HL
   Tang, PJ
   Li, QY
AF Yang, Longyu
   Wang, Hanli
   Tang, Pengjie
   Li, Qinyu
TI CaptionNet: A Tailor-made Recurrent Neural Network for Generating Image
   Descriptions
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Feature extraction; Semantics; Task analysis; Predictive
   models; Computer science; Computational modeling; Image captioning;
   memory initialization; recurrent neural network; visual attention;
   reinforcement learning
AB Image captioning is a challenging task of visual understanding and has drawn more attention of researchers. In general, two inputs are required at each time step by the Long Short-Term Memory (LSTM) network used in popular attention based image captioning frameworks, including image features and previous generated words. However, error will be accumulated if the previous words are not accurate and the related semantic is not efficient enough. Facing these challenges, a novel model named CaptionNet is proposed in this work as an improved LSTM specially designed for image captioning. Concretely, only attended image features are allowed to be fed into the memory of CaptionNet through input gates. In this way, the dependency on the previous predicted words can be reduced, forcing model to focus on more visual clues of images at the current time step. Moreover, a memory initialization method called image feature encoding is designed to capture richer semantics of the target image. The evaluation on the benchmark MSCOCO and Flickr30K datasets demonstrates the effectiveness of the proposed CaptionNet model, and extensive ablation studies are performed to verify each of the proposed methods. The project page can be found in https://mic.tongji.edu.cn/3f/9c/c9778a147356/page.htm.
C1 [Yang, Longyu; Tang, Pengjie] Tongji Univ, Dept Comp Sci & Technol, Shanghai 200092, Peoples R China.
   [Wang, Hanli] Tongji Univ, Dept Comp Sci & Technol, Minist Educ, Key Lab Embedded Syst & Serv Comp, Shanghai 200092, Peoples R China.
   [Wang, Hanli] Tongji Univ, Shanghai Inst Intelligent Sci & Technol, Shanghai 200092, Peoples R China.
   [Tang, Pengjie] Jinggangshan Univ, Coll Math & Phys, Jian 343009, Jiangxi, Peoples R China.
   [Li, Qinyu] Lanzhou City Univ, Dept Comp Sci, Lanzhou 730070, Peoples R China.
C3 Tongji University; Tongji University; Tongji University; Jinggangshan
   University; Lanzhou City University
RP Wang, HL (corresponding author), Tongji Univ, Dept Comp Sci & Technol, Minist Educ, Key Lab Embedded Syst & Serv Comp, Shanghai 200092, Peoples R China.; Wang, HL (corresponding author), Tongji Univ, Shanghai Inst Intelligent Sci & Technol, Shanghai 200092, Peoples R China.
EM 1651792@tongji.edu.cn; hanliwang@tongji.edu.cn;
   5tangpengjie@tongji.edu.cn; qinyu.li@tongji.edu.cn
RI Wang, Hanli/G-5111-2014; yuan, lin/JDW-7387-2023
OI Wang, Hanli/0000-0002-9999-4871; 
FU National Natural Science Foundation of China [61976159]; Key Project of
   Science and Technology Innovation 2030 - Ministry of Science and
   Technology of China [2018AAA0101303]; Shanghai Engineering Research
   Center of Industrial Vision Perception & Intelligent Computing
   [17DZ2251600]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61976159, Key Project of Science and
   Technology Innovation 2030 supported by the Ministry of Science and
   Technology of China under Grant 2018AAA0101303, and Shanghai Engineering
   Research Center of Industrial Vision Perception & Intelligent Computing
   (17DZ2251600).
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24
   [Anonymous], 2015, ICLR 2015
   Bahdanau D., 2015, P ICLR 15 MAY
   Cho K, 2015, IEEE T MULTIMEDIA, V17, P1875, DOI 10.1109/TMM.2015.2477044
   Cho Kyunghyun, 2014, SYNTAX SEMANTICS STR, P5, DOI [10.3115/v1/w14-4012, 10.3115 /v1/D14-1179, DOI 10.3115/V1/D14-1179]
   Denkowski Michael, 2014, P 9 WORKSH STAT MACH, DOI [10.3115/v1/W14-3348, DOI 10.3115/V1/W14-3348]
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Gu JX, 2018, AAAI CONF ARTIF INTE, P6837
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Jia X, 2015, IEEE I CONF COMP VIS, P2407, DOI 10.1109/ICCV.2015.277
   Jiang WH, 2018, LECT NOTES COMPUT SC, V11206, P510, DOI 10.1007/978-3-030-01216-8_31
   Karen S., 2015, P ICLR 15 MAY
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kingma D. P., 2015, P ICLR 15 MAY
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li LH, 2018, IEEE T MULTIMEDIA, V20, P726, DOI 10.1109/TMM.2017.2751140
   Li XY, 2019, IEEE T MULTIMEDIA, V21, P2117, DOI 10.1109/TMM.2019.2896516
   Lin Chin-Yew, 2004, P 42 ANN M AOC COMP, P605, DOI [DOI 10.3115/.1218955.1219032, DOI 10.3115/1218955.1219032]
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu DQ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1416, DOI 10.1145/3240508.3240632
   Liu SQ, 2017, IEEE I CONF COMP VIS, P873, DOI 10.1109/ICCV.2017.100
   Lu JS, 2018, PROC CVPR IEEE, P7219, DOI 10.1109/CVPR.2018.00754
   Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Parikh, 2015, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2015.7299087
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sutton RS, 2018, ADAPT COMPUT MACH LE, P1
   Sutton RS, 2000, ADV NEUR IN, V12, P1057
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Vinyals O, 2017, IEEE T PATTERN ANAL, V39, P652, DOI 10.1109/TPAMI.2016.2587640
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Xu DF, 2017, PROC CVPR IEEE, P3097, DOI 10.1109/CVPR.2017.330
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yang X, 2019, PROC CVPR IEEE, P10677, DOI 10.1109/CVPR.2019.01094
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yao T, 2017, IEEE I CONF COMP VIS, P4904, DOI 10.1109/ICCV.2017.524
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Young P., 2014, Transactions of the Association for Computational Linguistics, V2, P67
   Zaremba W., 2016, P ICLR 16 MAY
   Zhang ZJ, 2019, IEEE T MULTIMEDIA, V21, P1681, DOI 10.1109/TMM.2018.2888822
NR 45
TC 27
Z9 29
U1 0
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 835
EP 845
DI 10.1109/TMM.2020.2990074
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA QO8XK
UT WOS:000623420300001
DA 2024-07-18
ER

PT J
AU Yang, Q
   Chen, H
   Ma, Z
   Xu, YL
   Tang, RJ
   Sun, J
AF Yang, Qi
   Chen, Hao
   Ma, Zhan
   Xu, Yiling
   Tang, Rongjun
   Sun, Jun
TI Predicting the Perceptual Quality of Point Cloud: A 3D-to-2D
   Projection-Based Exploration
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Distortion; Image color analysis; Indexes;
   Distortion measurement; Image edge detection; 3D-to-2D projection; point
   cloud; image features; quality assessment
ID FREE-ENERGY PRINCIPLE; COLOR; ERROR; EDGE
AB Point cloud is emerged as a promising media format to represent realistic 3D objects or scenes in applications, such as virtual reality, teleportation, etc. How to accurately quantify the subjective point cloud quality for application-driven optimization, however, is still a challenging and open problem. In this paper, we attempt to tackle this problem in a systematic means. First, we produce a fairly large point cloud dataset where ten popular point clouds are augmented with seven types of impairments (e.g., compression, photometry/color noise, geometry noise, scaling) at six different distortion levels, and organize a formal subjective assessment with tens of subjects to collect mean opinion scores (MOS) for all 420 processed point cloud samples (PPCS). We then try to develop an objective metric that can accurately estimate the subjective quality. Towards this goal, we choose to project the 3D point cloud onto six perpendicular image planes of a cube for the color texture image and corresponding depth image, and aggregate image-based global (e.g., Jensen-Shannon (JS) divergence) and local features (e.g., edge, depth, pixel-wise similarity, complexity) among all projected planes for a final objective index. Model parameters are fixed constants after performing the regression using a small and independent dataset previously published. The proposed metric has demonstrated the state-of-the-art performance for predicting the subjective point cloud quality compared with multiple full-reference and no-reference models, e.g., the weighted peak signal-to-noise ratio (PSNR), structural similarity (SSIM), feature similarity (FSIM) and natural image quality evaluator (NIQE). The dataset is made publicly accessible at http://smt.sjtu.edu.cn or http://vision.nju.edu.cn for all interested audiences.
C1 [Yang, Qi; Xu, Yiling; Tang, Rongjun; Sun, Jun] Shanghai Jiao Tong Univ, Cooperat Medianet Innovat Ctr, Shanghai 200240, Peoples R China.
   [Chen, Hao; Ma, Zhan] Nanjing Univ, Elect Engn, Nanjing 210093, Jiangsu, Peoples R China.
C3 Shanghai Jiao Tong University; Nanjing University
RP Xu, YL (corresponding author), Shanghai Jiao Tong Univ, Cooperat Medianet Innovat Ctr, Shanghai 200240, Peoples R China.; Ma, Z (corresponding author), Nanjing Univ, Elect Engn, Nanjing 210093, Jiangsu, Peoples R China.
EM yang_littleqi@sjtu.edu.cn; chenhao1210@nju.edu.cn; mazhan@nju.edu.cn;
   yl.xu@sjtu.edu.cn; thekey@sjtu.edu.cn; junsun@sjtu.edu.cn
RI Chen, Hao/AGI-0052-2022; Ma, Zhan/HKW-2859-2023; Tang,
   Rongjun/HHZ-4270-2022
OI Chen, Hao/0000-0002-1179-8199; Ma, Zhan/0000-0003-3686-4057; Yang,
   Qi/0000-0002-4274-3457; Tang, Rongjun/0000-0002-2458-7980
FU National Key Research and Development Project of China Science, and
   Technology Exchange Center [2018YFE0206700, 2018YFB1802201]; National
   Natural Science Foundation of China [61971282]; Scientific Research Plan
   of the Science and Technology Commission of Shanghai Municipality
   [18511105400]
FX This work was supported in part by the National Key Research and
   Development Project of China Science, and Technology Exchange Center
   (2018YFE0206700, 2018YFB1802201), in part by the National Natural
   Science Foundation of China (61971282), and in part by the Scientific
   Research Plan of the Science and Technology Commission of Shanghai
   Municipality (18511105400). The associate editor coordinating the review
   of this manuscript and approving it for publication was Dr. Lu Fang.
CR Alexander E, 2021, PUBLIC HEALTH NUTR, V24, P1542, DOI 10.1017/S1368980020003961
   Alexiou E., 2019, INT WORK QUAL MULTIM, P1, DOI [DOI 10.1109/qomex.2019.8743277, 10.1109/QoMEX.2019.8743277]
   Alexiou E., 2017, P 9 INT C QUAL MULT, P1
   Alexiou E, 2018, IEEE INT CON MULTI
   Alexiou E, 2018, PROC SPIE, V10752, DOI 10.1117/12.2321518
   Alexiou E, 2017, PROC SPIE, V10396, DOI 10.1117/12.2275142
   [Anonymous], 2017, MPEG PEOPLE DATASETS
   [Anonymous], 2017, MPEG STATIC OBJECT S
   [Anonymous], 2009, Final report from the video quality experts group on the validation of objective models of video quality assessment II
   [Anonymous], 2017, MPEG INANIMATE DATAS
   [Anonymous], 2017, M74008 ISOIEC JTC
   Chalupa Leo M, 2014, NEW VISUAL NEUROSCIE
   Chen MX, 2020, IEEE J-STSP, V14, P89, DOI 10.1109/JSTSP.2019.2956408
   Chou P, 2016, document ISO/IEC MPEG
   Cignoni P, 1998, COMPUT GRAPH FORUM, V17, P167, DOI 10.1111/1467-8659.00236
   da Silva Costa Leonardo, 2019, 2019 Latin American Robotics Symposium (LARS), 2019 Brazilian Symposium on Robotics (SBR) and 2019 Workshop on Robotics in Education (WRE), P1, DOI 10.1109/LARS-SBR-WRE48964.2019.00009
   Friston KJ, 2006, J PHYSIOL-PARIS, V100, P70, DOI 10.1016/j.jphysparis.2006.10.001
   Friston KJ, 2010, NAT REV NEUROSCI, V11, P127, DOI 10.1038/nrn2787
   Geusebroek JM, 2001, IEEE T PATTERN ANAL, V23, P1338, DOI 10.1109/34.977559
   Geusebroek JM, 2000, LECT NOTES COMPUT SC, V1842, P331
   Gu K, 2018, IEEE T VIS COMPUT GR, V24, P2689, DOI 10.1109/TVCG.2017.2771284
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P1098, DOI 10.1109/TMM.2016.2547343
   He LY, 2018, LECT NOTES COMPUT SC, V11164, P50, DOI 10.1007/978-3-030-00776-8_5
   He LY, 2017, ASIA-PAC CONF COMMUN, P345
   Hershey JR, 2007, INT CONF ACOUST SPEE, P317, DOI 10.1109/icassp.2007.366913
   Int. Telecommun. Union, 2000, BT50011 ITUR
   Javaheri A, 2020, INT WORK QUAL MULTIM, DOI 10.1109/qomex48832.2020.9123087
   Javaheri A, 2017, IEEE INT WORKSH MULT
   KUNDU MK, 1986, PATTERN RECOGN LETT, V4, P433, DOI 10.1016/0167-8655(86)90041-3
   Liu AM, 2012, IEEE T IMAGE PROCESS, V21, P1500, DOI 10.1109/TIP.2011.2175935
   Liu AM, 2010, IEEE T CIRC SYST VID, V20, P1648, DOI 10.1109/TCSVT.2010.2087432
   Liu LX, 2016, SIGNAL PROCESS-IMAGE, V40, P1, DOI 10.1016/j.image.2015.10.005
   Ma Z, 2012, IEEE T CIRC SYST VID, V22, P671, DOI 10.1109/TCSVT.2011.2177143
   Marr D., 1982, Vision. A computational investigation into the human representation and processing of visual information
   Meynet G., 2019, P 11 INT C QUAL MULT, P1
   Meynet G, 2020, INT WORK QUAL MULTIM, DOI 10.1109/qomex48832.2020.9123147
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Murray N, 2014, PROC CVPR IEEE, P2473, DOI 10.1109/CVPR.2014.317
   Perry S., 2019, ELECT IMAGING, V31, P312
   Polani D., 2013, Encyclopedia of Systems Biology, P1087
   Ponomarenko N, 2005, P 3 INT WORKSH VID P, V4
   Schwarz S, 2018, PICT COD SYMP, P61, DOI 10.1109/PCS.2018.8456265
   Schwarz S, 2019, IEEE J EM SEL TOP C, V9, P133, DOI 10.1109/JETCAS.2018.2885981
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Sherman S., 1960, B AM MATH SOC, V66, P472
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tian D, 2017, IEEE IMAGE PROC, P3460, DOI 10.1109/ICIP.2017.8296925
   Torlig EM, 2018, PROC SPIE, V10752, DOI 10.1117/12.2322741
   Viola I, 2020, INT WORK QUAL MULTIM, DOI 10.1109/qomex48832.2020.9123089
   Wan S, OPT ENG, V44
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435
   Wu JJ, 2019, IEEE T MULTIMEDIA, V21, P2738, DOI 10.1109/TMM.2019.2908377
   Xie SW, 2020, IEEE T CIRC SYST VID, V30, P3029, DOI 10.1109/TCSVT.2019.2934136
   Xu L, 2016, IEEE T MULTIMEDIA, V18, P590, DOI 10.1109/TMM.2016.2525004
   Yang Q., 2020, IEEE TPAMI
   Yang Q, 2020, IEEE T BROADCAST, V66, P310, DOI 10.1109/TBC.2019.2954063
   Zhai GT, 2008, IEEE T MULTIMEDIA, V10, P1316, DOI 10.1109/TMM.2008.2004910
   Zhang J, 2014, 2014 INTERNATIONAL CONFERENCE ON AUDIO, LANGUAGE AND IMAGE PROCESSING (ICALIP), VOLS 1-2, P827, DOI 10.1109/ICALIP.2014.7009910
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhou WJ, 2016, IEEE T MULTIMEDIA, V18, P1077, DOI 10.1109/TMM.2016.2542580
   Zhu WW, 2022, IEEE T SYST MAN CY-S, V52, P893, DOI 10.1109/TSMC.2020.3010518
NR 66
TC 59
Z9 63
U1 7
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3877
EP 3891
DI 10.1109/TMM.2020.3033117
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA WJ5OB
UT WOS:000709093100035
DA 2024-07-18
ER

PT J
AU Zhang, XD
   Gao, XB
   Lu, W
   He, LH
   Li, J
AF Zhang, Xiaodan
   Gao, Xinbo
   Lu, Wen
   He, Lihuo
   Li, Jie
TI Beyond Vision: A Multimodal Recurrent Attention Convolutional Neural
   Network for Unified Image Aesthetic Prediction Tasks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image quality assessment; visual aesthetic quality assessment; long
   short-term memory (LSTM); deep learning
AB Over the past few years, image aesthetic prediction has attracted increasing attention because of its wide applications, such as image retrieval, photo album management and aesthetic-driven image enhancement. However, previous studies in this area only achieve limited success because 1) they primarily depend on visual features and ignore textual information. 2) they tend to focus equally on to each part of images and ignore the selective attention mechanism. This paper overcomes these limitations by proposing a novel multimodal recurrent attention convolutional neural network (MRACNN). More specifically, the MRACNN consists of two streams: the vision stream and the language stream. The former employs the recurrent attention network to tune out irrelevant information and focuses on some key regions to extract visual features. The latter utilizes the Text-CNN to capture the high-level semantics of user comments. Finally, a multimodal factorized bilinear (MFB) pooling approach is used to achieve effective fusion of textual and visual features. Extensive experiments demonstrate that the proposed MRACNN significantly outperforms state-of-the-art methods for unified aesthetic prediction tasks: (i) aesthetic quality classification; (ii) aesthetic score regression; and (iii) aesthetic score distribution prediction.
C1 [Zhang, Xiaodan] Northwest Univ, Sch Informat Sci & Technol, Xian 710127, Peoples R China.
   [Gao, Xinbo; Lu, Wen; He, Lihuo; Li, Jie] Xidian Univ, Sch Elect Engn, Video & Image Proc Syst Lab, Xian 710071, Peoples R China.
C3 Northwest University Xi'an; Xidian University
RP Gao, XB (corresponding author), Xidian Univ, Sch Elect Engn, Video & Image Proc Syst Lab, Xian 710071, Peoples R China.
EM xdanzhang.opt@gmail.com; xbgao@mail.xidian.edu.cn;
   luwen@mail.xidian.edu.cn; lihuo.he@gmail.com; leejie@mail.xidian.edu.cn
RI Li, jie/GXG-4583-2022; Zhang, Xiaodan/GQR-0608-2022
OI Zhang, Xiaodan/0000-0001-8192-0666; He, Lihuo/0000-0002-0555-3574
FU National Natural Science Foundation of China [61432014, 61772402,
   U1605252, 61671339]; National Key Research and Development Program of
   China [2016QY01W0200]; National High-Level Talents Special Support
   Program of China [CS31117200001]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61432014, 61772402, U1605252, and
   61671339, in part by the National Key Research and Development Program
   of China under Grant 2016QY01W0200, and in part by the National
   High-Level Talents Special Support Program of China under Grant
   CS31117200001. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Abdulmotaleb El
   Saddik.
CR [Anonymous], 2014, Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), DOI [DOI 10.3115/V1/D14-1181, 10.3115/v1/D14-1181]
   [Anonymous], 2011, ACM International Conference on Multimedia MM
   [Anonymous], 2009, P 17 ACM INT C MULT
   Benois-Pineau Benois-Pineau J. J., P INT C IM PROC THEO, P1
   Bhattacharya S., 2010, P 18 ACM INT C MULTI, P271
   Chen YX, 2018, IEEE T CYBERNETICS, V48, P3092, DOI 10.1109/TCYB.2017.2758350
   Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23
   Dhar S, 2011, PROC CVPR IEEE, P1657, DOI 10.1109/CVPR.2011.5995467
   Gao F, 2018, PATTERN RECOGN, V81, P432, DOI 10.1016/j.patcog.2018.04.016
   Guo LH, 2014, NEUROCOMPUTING, V143, P14, DOI 10.1016/j.neucom.2014.06.029
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   He XT, 2017, PROC CVPR IEEE, P7332, DOI 10.1109/CVPR.2017.775
   Hii YL, 2017, IEEE IMAGE PROC, P1722, DOI 10.1109/ICIP.2017.8296576
   Hosu V, 2019, PROC CVPR IEEE, P9367, DOI 10.1109/CVPR.2019.00960
   Hu A, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P350, DOI 10.1145/3219819.3219853
   Jin B, 2016, IEEE IMAGE PROC, P2291, DOI 10.1109/ICIP.2016.7532767
   Jin X, 2018, AAAI CONF ARTIF INTE, P77
   Kao YY, 2017, IEEE T IMAGE PROCESS, V26, P1482, DOI 10.1109/TIP.2017.2651399
   Kong S, 2016, LECT NOTES COMPUT SC, V9905, P662, DOI 10.1007/978-3-319-46448-0_40
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kucer M, 2018, IEEE T IMAGE PROCESS, V27, P5100, DOI 10.1109/TIP.2018.2845100
   Liu YT, 2019, IEEE T MULTIMEDIA, V21, P135, DOI 10.1109/TMM.2018.2849602
   Lu X, 2015, IEEE I CONF COMP VIS, P990, DOI 10.1109/ICCV.2015.119
   Lu X, 2015, IEEE T MULTIMEDIA, V17, P2021, DOI 10.1109/TMM.2015.2477040
   Luo YW, 2008, LECT NOTES COMPUT SC, V5304, P386
   Ma S, 2017, PROC CVPR IEEE, P722, DOI 10.1109/CVPR.2017.84
   Mai L, 2016, PROC CVPR IEEE, P497, DOI 10.1109/CVPR.2016.60
   Marchesotti L, 2011, IEEE I CONF COMP VIS, P1784, DOI 10.1109/ICCV.2011.6126444
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P3111, DOI DOI 10.5555/2999792.2999959
   Mnih V, 2014, ADV NEUR IN, V27
   Murray N., 2017, CoRR
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Nishiyama M, 2011, PROC CVPR IEEE, P33, DOI 10.1109/CVPR.2011.5995539
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Samii A, 2015, COMPUT GRAPH FORUM, V34, P141, DOI 10.1111/cgf.12465
   Sheng KK, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P879, DOI 10.1145/3240508.3240554
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Talebi H, 2018, IEEE T IMAGE PROCESS, V27, P3998, DOI 10.1109/TIP.2018.2831899
   Tang XO, 2013, IEEE T MULTIMEDIA, V15, P1930, DOI 10.1109/TMM.2013.2269899
   Tian XM, 2015, IEEE T MULTIMEDIA, V17, P2035, DOI 10.1109/TMM.2015.2479916
   Wang GL, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P957
   Xu Y., 2018, IEEE ACCESS, V6
   Yu Z, 2018, IEEE T NEUR NET LEAR, V29, P5947, DOI 10.1109/TNNLS.2018.2817340
   Zhang FL, 2013, IEEE T MULTIMEDIA, V15, P1480, DOI 10.1109/TMM.2013.2268051
   Zhang XD, 2019, IEEE T MULTIMEDIA, V21, P2815, DOI 10.1109/TMM.2019.2911428
   Zhou Y, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P262, DOI 10.1145/2964284.2967223
NR 48
TC 28
Z9 28
U1 2
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 611
EP 623
DI 10.1109/TMM.2020.2985526
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA QA6NL
UT WOS:000613560200007
DA 2024-07-18
ER

PT J
AU Zhou, WJ
   Wu, JW
   Lei, JS
   Hwang, JN
   Yu, L
AF Zhou, Wujie
   Wu, Junwei
   Lei, Jingsheng
   Hwang, Jenq-Neng
   Yu, Lu
TI Salient Object Detection in Stereoscopic 3D Images Using a Deep
   Convolutional Residual Autoencoder
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Object detection; Feature extraction; Saliency detection;
   Three-dimensional displays; Computer architecture; Two dimensional
   displays; Decoding; Stereoscopic 3D (S3D) images; saliency predic-tion;
   deep learning; convolutional residual neural networks
ID NETWORK; FUSION; SEGMENTATION; RECOGNITION; ATTENTION
AB In recent years, the detection of distinctive objects in stereoscopic 3D images has drawn increasing attention. Unlike 2D salient object detection, salient object detection in stereoscopic 3D images is highly challenging. Hence, we propose a novel Deep Convolutional Residual Autoencoder (DCRA) for end-to-end salient object detection in stereoscopic 3D images. The core trainable architecture of the salient object detection model employs raw stereoscopic 3D images as the inputs and their corresponding ground truth saliency masks as the labels. A convolutional residual module is applied to both the encoder and the decoder as a basic building block in the DCRA, and long-range skip connections are employed to bypass the equal-sized feature maps between the encoder and the decoder. To explore the complex relationships and exploit the complementarity between RGB (photometric) and depth (geometric) information, multiple feature map fusion modules are constructed. These modules integrate texture and structure information between the RGB and depth branches of the encoder and fuse their features over several multiscale layers. Finally, to efficiently optimize DCRA parameters, a supervision pyramid based on boundary loss and background prior loss is adopted, which employs supervised learning over the multiscale layers in the decoder to prevent vanishing gradients and accelerate the training at the fusion stage. We compare the proposed DCRA with state-of-the-art methods on two challenging benchmark datasets. The results of these experiments demonstrate that our proposed DCRA performs favorably against the comparison models.
C1 [Zhou, Wujie; Wu, Junwei; Lei, Jingsheng] Zhejiang Univ Sci & Technol, Sch Informat & Elect Engn, Hangzhou 310023, Peoples R China.
   [Zhou, Wujie; Yu, Lu] Zhejiang Univ, Inst Informat & Commun Engn, Hangzhou 310027, Peoples R China.
   [Hwang, Jenq-Neng] Univ Washington, Dept Elect Engn, Seattle, WA 98105 USA.
C3 Zhejiang University of Science & Technology; Zhejiang University;
   University of Washington; University of Washington Seattle
RP Zhou, WJ (corresponding author), Zhejiang Univ Sci & Technol, Sch Informat & Elect Engn, Hangzhou 310023, Peoples R China.
EM wujiezhou@163.com; hwang@uw.edu; yul@zju.edu.cn
RI Wu, Junwei/IUM-6922-2023; wu, junwei/GMW-9046-2022
OI wu, junwei/0000-0001-6155-4259; zhou, wujie/0000-0002-3055-2493; Hwang,
   Jenq-Neng/0000-0002-8877-2421
FU National Natural Science Foundation of China [61502429, 61672337,
   61972357]; Zhejiang Provincial Natural Science Foundation of China
   [LY18F020012]
FX Manuscript received April 23, 2019; revised September 23, 2019, December
   26, 2019, and May 23, 2020; accepted September 12, 2020. Date of
   publication September 21, 2020; date of current version September 24,
   2021. This work was supported in part by the National Natural Science
   Foundation of China under Grants 61502429, 61672337, and 61972357, and
   in part by the Zhejiang Provincial Natural Science Foundation of China
   under Grant LY18F020012 (Corresponding author: Wujie Zhou.)
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   [Anonymous], 1999, Handbook of Computer Vision and Applications
   Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30
   Chen H, 2019, IEEE T IMAGE PROCESS, V28, P2825, DOI 10.1109/TIP.2019.2891104
   Chen H, 2018, PROC CVPR IEEE, P3051, DOI 10.1109/CVPR.2018.00322
   Chen H, 2019, PATTERN RECOGN, V86, P376, DOI 10.1016/j.patcog.2018.08.007
   Chen H, 2017, LECT NOTES COMPUT SC, V10528, P459, DOI 10.1007/978-3-319-68345-4_41
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Cheng Y, 2014, IEEE INT CON MULTI
   Cong RM, 2016, IEEE SIGNAL PROC LET, V23, DOI 10.1109/LSP.2016.2557347
   Desingh K, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.98
   Ding Y, 2019, J VIS COMMUN IMAGE R, V61, P1, DOI 10.1016/j.jvcir.2019.03.019
   Du H, 2016, IEEE ACCESS, V4, P8987, DOI 10.1109/ACCESS.2016.2632724
   Fang YM, 2014, IEEE T IMAGE PROCESS, V23, P2625, DOI 10.1109/TIP.2014.2305100
   Fu KR, 2019, IEEE T MULTIMEDIA, V21, P457, DOI 10.1109/TMM.2018.2859746
   Goferman S, 2012, IEEE T PATTERN ANAL, V34, P1915, DOI 10.1109/TPAMI.2011.272
   Han JG, 2013, IEEE T CYBERNETICS, V43, P1318, DOI 10.1109/TCYB.2013.2265378
   Han JW, 2018, IEEE T CYBERNETICS, V48, P3171, DOI 10.1109/TCYB.2017.2761775
   Han JW, 2015, IEEE T CIRC SYST VID, V25, P1309, DOI 10.1109/TCSVT.2014.2381471
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou QB, 2019, IEEE T PATTERN ANAL, V41, P815, DOI 10.1109/TPAMI.2018.2815688
   Hou XD, 2007, PROC CVPR IEEE, P2280
   Hu P, 2017, PROC CVPR IEEE, P540, DOI 10.1109/CVPR.2017.65
   Huang R, 2019, IEEE SIGNAL PROC LET, V26, P552, DOI 10.1109/LSP.2019.2898508
   Huo SW, 2018, IEEE T MULTIMEDIA, V20, P1350, DOI 10.1109/TMM.2017.2769801
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jiang, 2018, ARXIV PREPRINT ARXIV, DOI DOI 10.5194/ACP-2018-920
   Ju R, 2014, IEEE IMAGE PROC, P1115, DOI 10.1109/ICIP.2014.7025222
   Ju R, 2015, SIGNAL PROCESS-IMAGE, V38, P115, DOI 10.1016/j.image.2015.07.002
   Junyao Guo, 2015, 2015 IEEE Power & Energy Society Innovative Smart Grid Technologies Conference (ISGT). Proceedings, P1, DOI 10.1109/ISGT.2015.7131832
   Kingma D. P., 2014, arXiv
   Li GB, 2018, IEEE T NEUR NET LEAR, V29, P6038, DOI 10.1109/TNNLS.2018.2817540
   Li GB, 2016, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2016.58
   Li HY, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2440174
   Li SX, 2018, IEEE T MULTIMEDIA, V20, P155, DOI 10.1109/TMM.2017.2721544
   Li X, 2016, IEEE T IMAGE PROCESS, V25, P3919, DOI 10.1109/TIP.2016.2579306
   Li YA, 2018, IEEE T CIRC SYST VID, V28, P2956, DOI 10.1109/TCSVT.2017.2749509
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Liu N, 2018, IEEE T IMAGE PROCESS, V27, P3264, DOI 10.1109/TIP.2018.2817047
   Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Luo ZM, 2017, PROC CVPR IEEE, P6593, DOI 10.1109/CVPR.2017.698
   Niu YZ, 2012, PROC CVPR IEEE, P454, DOI 10.1109/CVPR.2012.6247708
   Park JH, 2020, IEEE T MULTIMEDIA, V22, P2262, DOI 10.1109/TMM.2017.2757759
   Peng HW, 2014, LECT NOTES COMPUT SC, V8691, P92, DOI 10.1007/978-3-319-10578-9_7
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Qu LQ, 2017, IEEE T IMAGE PROCESS, V26, P2274, DOI 10.1109/TIP.2017.2682981
   Quan R, 2018, IEEE T MULTIMEDIA, V20, P1101, DOI 10.1109/TMM.2017.2763780
   Ren JQ, 2015, IEEE COMPUT SOC CONF, DOI 10.1109/CVPRW.2015.7301391
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Song HK, 2017, IEEE T IMAGE PROCESS, V26, P4204, DOI 10.1109/TIP.2017.2711277
   Tan WM, 2018, IEEE T CIRC SYST VID, V28, P3154, DOI 10.1109/TCSVT.2017.2742243
   Tang YB, 2019, IEEE T MULTIMEDIA, V21, P2237, DOI 10.1109/TMM.2019.2900908
   Thomas SS, 2019, IEEE T CIRC SYST VID, V29, P3132, DOI 10.1109/TCSVT.2018.2873185
   Wang AZ, 2017, IEEE SIGNAL PROC LET, V24, P663, DOI 10.1109/LSP.2017.2688136
   Wang ZL, 2017, IEEE T MULTIMEDIA, V19, P750, DOI 10.1109/TMM.2016.2636739
   Wu JW, 2021, SIGNAL PROCESS, V178, DOI 10.1016/j.sigpro.2020.107766
   Xiao HX, 2018, IEEE T MULTIMEDIA, V20, P3239, DOI 10.1109/TMM.2018.2830098
   Zhang DW, 2017, IEEE T PATTERN ANAL, V39, P865, DOI 10.1109/TPAMI.2016.2567393
   Zhou WJ, 2020, IEEE T COMPUT IMAG, V6, P883, DOI 10.1109/TCI.2020.2993640
   Zhou WJ, 2021, IEEE T SYST MAN CY-S, V51, P3641, DOI 10.1109/TSMC.2019.2957386
   Zhou WJ, 2017, INFORM SCIENCES, V397, P1, DOI 10.1016/j.ins.2017.02.049
   Zhou WJ, 2018, IEEE T IMAGE PROCESS, V27, P2086, DOI 10.1109/TIP.2018.2794207
   Zhou Y, 2019, IEEE T MULTIMEDIA, V21, P74, DOI 10.1109/TMM.2018.2845667
   Zhu, 2018, MULTILAYER BACKPROPA
   Zhu C., 2018, PDNET PRIOR MODEL GU, P199
   Zhu C., 2018, EXPLOITING VALUE CTR
NR 67
TC 59
Z9 59
U1 3
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3388
EP 3399
DI 10.1109/TMM.2020.3025166
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UU6IS
UT WOS:000698902000034
DA 2024-07-18
ER

PT J
AU Zhu, K
   Cao, Y
   Zhai, W
   Zha, ZJ
AF Zhu, Kai
   Cao, Yang
   Zhai, Wei
   Zha, Zheng-Jun
TI One-Shot Texture Retrieval Using Global Grouping Metric
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image segmentation; Task analysis; Measurement; Adaptation models;
   Feature extraction; Semantics; Computer vision; Attention; computer
   vision; one-shot learning; segmentation; texture
AB Texture retrieval is widely used in the fields of fashion and e-commerce. This paper presents the problem of one-shot texture retrieval: given an example of a new reference texture, we aim to detect and segment all pixels of the same texture category within an arbitrary image. To address this problem, an OS-TR network is proposed to encode both reference and query images into a texture representation space, and a better comparison is made based on the global grouping information. Because the learned texture representation should be invariant to the spatial layout while preserving the rough semantic concepts, we introduce an adaptive directionality-aware module to finely discriminate the orderless texture details. To make full use of the global context information given only a few examples, we incorporate a grouping-attention mechanism into the relation network, resulting in the per-channel modulation of the local relation features. Extensive experiments on two benchmark datasets (i.e., the DTD and ADE20K dataset) and real scenarios demonstrate that our proposed method can achieve above-par segmentation performance and robust generalization across domains.
C1 [Zhu, Kai; Cao, Yang; Zhai, Wei; Zha, Zheng-Jun] Univ Sci & Technol China, Dept Informat Sci & Technol, Hefei 230000, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Cao, Y (corresponding author), Univ Sci & Technol China, Dept Informat Sci & Technol, Hefei 230000, Peoples R China.
EM zkzy@mail.ustc.edu.cn; forrest@ustc.edu.cn; wzhai056@mail.ustc.edu.cn;
   zhazj@ustc.edu.cn
RI Cao, Yang/HGD-6463-2022; Zha, Zheng-Jun/AAF-8667-2020
FU National Key R&D Program of China [2017YFB130092]; National Natural
   Science Foundation of China (NSFC) [61872327]; Fundamental Research
   Funds for the Central Universities [WK2380000001]
FX Manuscript received October 15, 2019; revised June 29, 2020 and
   September 8, 2020; accepted October 1, 2020. Date of publication October
   14, 2020; date of current version October 19, 2021. This work was
   supported in part by the National Key R&D Program of China under Grant
   2017YFB130092, in part by the National Natural Science Foundation of
   China (NSFC) under Grant 61872327, and in part by the Fundamental
   Research Funds for the Central Universities under Grant WK2380000001.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Marco Carli. (Corresponding author:
   Yang Cao.)
CR Bell S, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462002
   Bertinetto Luca, 2018, P INT C LEARN REPR
   Chanda O., 2019, IEEE INT WORKSH MULT
   Chen Q, 2013, IEEE T MULTIMEDIA, V15, P521, DOI 10.1109/TMM.2012.2236306
   Chun YD, 2008, IEEE T MULTIMEDIA, V10, P1073, DOI 10.1109/TMM.2008.2001357
   Cimpoi M, 2015, PROC CVPR IEEE, P3828, DOI 10.1109/CVPR.2015.7299007
   Cimpoi M, 2014, PROC CVPR IEEE, P3606, DOI 10.1109/CVPR.2014.461
   Do MN, 2002, IEEE T MULTIMEDIA, V4, P517, DOI 10.1109/TMM.2002.802019
   Dong N., 2018, BMVC, V4, P4
   Fanet Z., 2020, P COMP VIS PATT REC
   Finn C, 2017, PR MACH LEARN RES, V70
   Guo ZY, 2013, IEEE T MULTIMEDIA, V15, P621, DOI 10.1109/TMM.2012.2234729
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jhang S.-J., 2019, 2019 16 IEEE INT C, P1
   Koch G., 2015, ICML DEEP LEARNING W, V2
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Kumar N, 2011, IEEE T PATTERN ANAL, V33, P1962, DOI 10.1109/TPAMI.2011.48
   Lee K, 2019, PROC CVPR IEEE, P10649, DOI 10.1109/CVPR.2019.01091
   Liu L, 2019, INT J COMPUT VISION, V127, P74, DOI 10.1007/s11263-018-1125-z
   Michaelis C., 2018, ARXIV180702654
   Munkhdalai T, 2017, PR MACH LEARN RES, V70
   Oreshkin Boris, 2018, Advances in Neural Information Processing Systems (NeurIPS)
   Qiao TT, 2019, PROC CVPR IEEE, P1505, DOI 10.1109/CVPR.2019.00160
   Rakelly K., 2018, P INT C LEARN REPR
   Ravi S., 2017, C TRACK P, P1
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rusu A. A., 2019, INT C LEARN REPR
   Santoro A, 2016, PR MACH LEARN RES, V48
   Satorras V.G., 2018, ICLR
   Shaban A, 2017, ARXIV PREPRINT ARXIV, DOI 10.5244/C.31.167
   Siam M, 2019, IEEE I CONF COMP VIS, P5248, DOI 10.1109/ICCV.2019.00535
   Snell J, 2017, ADV NEUR IN, V30
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vinyals O, 2016, 30 C NEURAL INFORM P, V29
   Wu Z., 2018, P INT JOINT C ART IN
   Yang G., 2017, P INT C LEARN REPR
   Yang GC, 2020, INFORM SCIENCES, V518, P225, DOI 10.1016/j.ins.2020.01.016
   Zhang C, 2019, PROC CVPR IEEE, P5212, DOI 10.1109/CVPR.2019.00536
   Zhang H, 2017, PROC CVPR IEEE, P2896, DOI 10.1109/CVPR.2017.309
   Zhang XL, 2020, IEEE T CYBERNETICS, V50, P3855, DOI 10.1109/TCYB.2020.2992433
   Zhu K., 2019, IEEE T MULTIMED
NR 43
TC 4
Z9 4
U1 1
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3726
EP 3737
DI 10.1109/TMM.2020.3031062
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA WJ5OB
UT WOS:000709093100023
DA 2024-07-18
ER

PT J
AU Tang, PP
   Dong, YN
   Jin, J
   Mao, SW
AF Tang, Pingping
   Dong, Yuning
   Jin, Jiong
   Mao, Shiwen
TI Fine-Grained Classification of Internet Video Traffic From QoS
   Perspective Using Fractal Spectrum
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Streaming media; Fractals; Feature extraction; Quality of service;
   Payloads; Inspection; Telecommunications; Fine-grained classification;
   fractal charact-eristics; quality of service (QoS); spectrum; video
   traffic
ID MULTIMEDIA; FRAMEWORK
AB Internet video traffic exhibits considerable variation as new video services continue to emerge. Some videos require strict real-time performance, while others may aim for a minimal packet loss rate or sufficient bandwidth. Therefore, it is important to develop fine-grained classification mechanisms to realize effective resource management and quality of service (QoS) provisioning. However, the existing methods for classifying video traffic always suffer from two problems: payload inspection and feature selection. In this paper, we propose a novel method that uses fractal characteristics to achieve traffic classification at a fine-grained level. This method requires neither payload signatures nor statistical features. Through rigorous analysis, we prove the feasibility of employing fractal characteristics for video traffic classification and further develop a theoretical framework for the proposed scheme. For the specific scenario of video flow classification, we improve the theory of fractals in terms of estimated spectrum, core domain, segmentation, and threshold setting. The results of an extensive experimental study on several real-world video traffic datasets show that the classification accuracy of the proposed scheme is higher than that of existing methods.
C1 [Tang, Pingping; Dong, Yuning] Nanjing Univ Posts & Telecommun, Coll Telecommun & Informat Engn, Nanjing 210003, Peoples R China.
   [Tang, Pingping] Anhui Normal Univ, Coll Phys & Elect Informat, Wuhu 241000, Peoples R China.
   [Jin, Jiong] Swinburne Univ Technol, Sch Software & Elect Engn, Melbourne, Vic 3122, Australia.
   [Mao, Shiwen] Auburn Univ, Dept Elect & Comp Engn, Auburn, AL 36849 USA.
C3 Nanjing University of Posts & Telecommunications; Anhui Normal
   University; Swinburne University of Technology; Auburn University
   System; Auburn University
RP Dong, YN (corresponding author), Nanjing Univ Posts & Telecommun, Coll Telecommun & Informat Engn, Nanjing 210003, Peoples R China.
EM tpping@ahnu.edu.cn; dongyn@njupt.edu.cn; jiongjin@swin.edu.au;
   smao@ieee.org
RI Mao, Shiwen/AAY-4471-2020; Jin, Jiong/R-3266-2019; Dong,
   Yuning/AAW-6970-2020
OI Jin, Jiong/0000-0002-0306-2691; Dong, Yuning/0000-0003-4898-331X; Mao,
   Shiwen/0000-0002-7052-0007
FU National Natural Science Foundation of China (NSFC) [61271233, 61401004,
   KJ2019A0491]; U.S. National Science Foundation [ECCS-1923717]
FX This work was supported in part by the National Natural Science
   Foundation of China (NSFC) under Grants 61271233 and 61401004, in part
   by AHUNSR under Grant KJ2019A0491, and in part by the U.S. National
   Science Foundation under Grant ECCS-1923717.
CR Abbessi W, 2019, MULTIMEDIA SYST, V25, P177, DOI 10.1007/s00530-018-0595-8
   Akar E, 2017, BIOMED SIGNAL PROCES, V31, P63, DOI 10.1016/j.bspc.2016.07.005
   Al-Abbasi AO, 2019, IEEE ACM T NETWORK, V27, P835, DOI 10.1109/TNET.2019.2900434
   Al-Saidi NMG, 2018, 2018 INTERNATIONAL CONFERENCE ON ADVANCE IN SUSTAINABLE ENGINEERING AND ITS APPLICATION (ICASEA), P99, DOI 10.1109/ICASEA.2018.8370964
   Allwright A, 2018, EUR PHYS J PLUS, V133, DOI 10.1140/epjp/i2018-11885-3
   [Anonymous], 2016, **NON-TRADITIONAL**
   [Anonymous], 2009, UNIBS 2009 PAYLOAD T
   [Anonymous], 2010, PROC IEEE INT C TELE
   Barakat C, 2003, IEEE T SIGNAL PROCES, V51, P2111, DOI 10.1109/TSP.2003.814521
   Canovas A, 2018, IEEE NETWORK, V32, P100, DOI 10.1109/MNET.2018.1800121
   Carlsson N, 2017, IEEE T MULTIMEDIA, V19, P1637, DOI 10.1109/TMM.2017.2673412
   Chen PN, 2000, IEEE T INFORM THEORY, V46, P2752, DOI 10.1109/18.887893
   Cheng X, 2013, IEEE T MULTIMEDIA, V15, P1184, DOI 10.1109/TMM.2013.2265531
   Dainotti A, 2012, IEEE NETWORK, V26, P35, DOI 10.1109/MNET.2012.6135854
   Dong JF, 2018, IEEE T MULTIMEDIA, V20, P3377, DOI 10.1109/TMM.2018.2832602
   Fu YJ, 2016, IEEE T MOBILE COMPUT, V15, P2851, DOI 10.1109/TMC.2016.2516020
   Gao GY, 2018, IEEE T MULTIMEDIA, V20, P3399, DOI 10.1109/TMM.2018.2838330
   Garcia J. O., 2018, P 5 EUR C ANT PROP A, P1, DOI DOI 10.1109/NTMS.2018.8328690
   Garcia J, 2018, INT CON ADV INFO NET, P358, DOI 10.1109/AINA.2018.00061
   [Гетьман А.И. Ge'tman A.I.], 2017, [Труды Института системного программирования РАН, Trudy Instituta sistemnogo programmirovaniya RAN], V29, P117, DOI 10.15514/ISPRAS-2017-29(3)-8
   Ghofrani F, 2015, IRAN CONF ELECTR ENG, P235, DOI 10.1109/IranianCEE.2015.7146216
   Hao SN, 2015, INT CONF CONTR AUTO, P102, DOI 10.1109/ICCAIS.2015.7338641
   Hernandez-Carrasco, 2018, IEEE T GEOSCI REMOTE, V56, P2243
   Jia XL, 2015, ENERG ECON, V49, P588, DOI 10.1016/j.eneco.2015.03.008
   Kim J, 2016, MOB INF SYST, V2016, DOI 10.1155/2016/6180527
   Kimura T, 2017, IEEE J-STSP, V11, P138, DOI 10.1109/JSTSP.2016.2632060
   Ko A, 2016, IEEE T GEOSCI REMOTE, V54, P3128, DOI 10.1109/TGRS.2015.2511628
   Kornycky J, 2017, IEEE ACM T NETWORK, V25, P56, DOI 10.1109/TNET.2016.2562259
   Kuzuoka S, 2015, IEEE T INFORM THEORY, V61, P5028, DOI 10.1109/TIT.2015.2458871
   LELAND WE, 1994, IEEE ACM T NETWORK, V2, P1, DOI 10.1109/90.282603
   Li MF, 2019, IEEE T MOBILE COMPUT, V18, P334, DOI 10.1109/TMC.2018.2836421
   Liu Q, 2015, IEEE T CIRC SYST VID, V25, P1815, DOI 10.1109/TCSVT.2015.2400751
   Liu YN, 2016, IEEE T MULTIMEDIA, V18, P865, DOI 10.1109/TMM.2016.2538718
   Livi L, 2016, IEEE T BIO-MED ENG, V63, P2243, DOI 10.1109/TBME.2016.2515760
   Long CC, 2018, IEEE T MULTIMEDIA, V20, P1126, DOI 10.1109/TMM.2017.2764330
   Lima FRM, 2012, IEEE T VEH TECHNOL, V61, P1318, DOI 10.1109/TVT.2012.2183905
   Medeiros FG, 2017, 2017 6TH BRAZILIAN CONFERENCE ON INTELLIGENT SYSTEMS (BRACIS), P258, DOI 10.1109/BRACIS.2017.48
   Nair L. M., 2015, P IEEE INT C COMP IN, P119
   Nossenson R, 2015, 2015 IEEE 14th International Symposium on Network Computing and Applications (NCA), P251, DOI 10.1109/NCA.2015.51
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Petrangeli S, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3165266
   Pratiher S, 2016, PROCEEDINGS OF THE 2016 2ND INTERNATIONAL CONFERENCE ON CONTEMPORARY COMPUTING AND INFORMATICS (IC3I), P831, DOI 10.1109/IC3I.2016.7918797
   Qin T, 2015, KNOWL-BASED SYST, V82, P152, DOI 10.1016/j.knosys.2015.03.002
   Riedi RH, 1999, IEEE T INFORM THEORY, V45, P992, DOI 10.1109/18.761337
   Shim KS, 2017, INT J NETW MANAG, V27, DOI 10.1002/nem.1981
   Tangworakitthaworn P., 2017, 2017 21 INT COMP SCI, P1, DOI DOI 10.1109/ICSEC.2017.8443924
   Tejero-de-Pablos A, 2018, IEEE T MULTIMEDIA, V20, P2000, DOI 10.1109/TMM.2018.2794265
   Thay, 2016, P IEEE INT C COMP SC, P23
   Wang ZJ, 2018, CHINA COMMUN, V15, P155, DOI 10.1109/CC.2018.8290814
   Wu Z, 2018, INT CONF ADV CLOUD B, P322, DOI 10.1109/CBD.2018.00064
   Wu ZJ, 2016, IEEE T DEPEND SECURE, V13, P559, DOI 10.1109/TDSC.2015.2443807
   Yang LY, 2018, WIRELESS PERS COMMUN, V103, P1481, DOI 10.1007/s11277-018-5864-5
   Yun XC, 2016, IEEE ACM T NETWORK, V24, P583, DOI 10.1109/TNET.2014.2381230
   Zhang J, 2013, IEEE T INF FOREN SEC, V8, P5, DOI 10.1109/TIFS.2012.2223675
   Zhang J, 2013, IEEE T PARALL DISTR, V24, P104, DOI 10.1109/TPDS.2012.98
NR 55
TC 8
Z9 8
U1 1
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2020
VL 22
IS 10
BP 2579
EP 2596
DI 10.1109/TMM.2019.2958764
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NU6VI
UT WOS:000573779600002
OA hybrid
DA 2024-07-18
ER

PT J
AU Zhang, SW
   Song, L
   Gao, CX
   Sang, N
AF Zhang, Shiwei
   Song, Lin
   Gao, Changxin
   Sang, Nong
TI GLNet: Global Local Network for Weakly Supervised Action Localization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Spatio-temporal action localization; global local network; weakly
   supervised
AB In this paper, we address the challenging problem of weakly supervised spatio-temporal action localization for which only video-level action labels are available during training. To solve this problem, we propose an end-to-end Global Local Network (GLNet) to predict the probability distribution simultaneously in both spatial and temporal space. The proposed GLNet model includes two key components: a local spatial module and a global temporal module. The local spatial module aims to predict the frame-level spatial distribution by encoding short-term temporal information. In particular, we propose a Region Actionness Network (RAN) to select the target region boxes from the precomputed exhaustive proposals. The global temporal module can predict temporal distribution by a long-term temporal structuremodelling. Specifically, we design a temporal fusion-and-excitation architecture on the top of several clips, and trained by a sparse loss function. Therefore, the proposed GLNet model can perform spatio-temporal action localization in an end-to-end manner. We evaluate the performance of GLNet on the J-HMDB and UCF101-24 datasets. The experimental results demonstrate GLNet achieves a significant margin against other state-of-the-art weakly supervised methods and even some fully supervised methods in terms of frame mean Average Precision (mAP) and the video mAP (called frame-mAP and video-mAP, respectively).
C1 [Zhang, Shiwei; Gao, Changxin; Sang, Nong] Huazhong Univ Sci & Technol, Sch Artificial Intelligence & Automat, Key Lab Minist Educ Image Proc & Intelligent Cont, Wuhan 430074, Peoples R China.
   [Song, Lin] Xi An Jiao Tong Univ, Sch Artificial Intelligence & Automat, Xian 710049, Peoples R China.
C3 Huazhong University of Science & Technology; Xi'an Jiaotong University
RP Sang, N (corresponding author), Huazhong Univ Sci & Technol, Sch Artificial Intelligence & Automat, Key Lab Minist Educ Image Proc & Intelligent Cont, Wuhan 430074, Peoples R China.
EM swZhang@hust.edu.cn; stevengrove@xtu.xjtu.edu.cn; cgao@hust.edu.cn;
   nsang@hust.edu.cn
RI Zhang, Shiwei/JIY-4344-2023; Gao, Changxin/L-4841-2016; Yang,
   Shu/JUU-4592-2023
OI Gao, Changxin/0000-0003-2736-3920; sang, nong/0000-0002-9167-1496
FU National Natural Science Foundation of China [61871435]; Fundamental
   Research Funds for the Central Universities [2019kfyXKJC024]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61871435 and in part by the Fundamental
   Research Funds for the Central Universities under Grant 2019kfyXKJC024.
CR Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28
   [Anonymous], 2016, P BRIT MACH VIS C
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Cinbis RG, 2014, PROC CVPR IEEE, P2409, DOI 10.1109/CVPR.2014.309
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Girshick R., 2014, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2014.81, 10.1109/CVPR.2014.81]
   Gkioxari G, 2015, PROC CVPR IEEE, P759, DOI 10.1109/CVPR.2015.7298676
   Hou R, 2017, IEEE I CONF COMP VIS, P5823, DOI 10.1109/ICCV.2017.620
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Ibrahim NE, 2019, ESC HEART FAIL, V6, P1085, DOI 10.1002/ehf2.12480
   Jhuang HH, 2013, IEEE I CONF COMP VIS, P3192, DOI 10.1109/ICCV.2013.396
   Kalogeiton V, 2017, IEEE I CONF COMP VIS, P4415, DOI 10.1109/ICCV.2017.472
   Kay W., 2017, ARXIV170506950
   Kohavi R, 1997, ARTIF INTELL, V97, P273, DOI 10.1016/S0004-3702(97)00043-X
   Li ZY, 2018, COMPUT VIS IMAGE UND, V166, P41, DOI 10.1016/j.cviu.2017.10.011
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Loffe R, 2015, International Workshop on OpenCL 2015, DOI 10.1145/2791321.2791324
   Mettes P., 2017, P BRIT MACH VIS C
   Mettes P, 2017, IEEE I CONF COMP VIS, P4453, DOI 10.1109/ICCV.2017.476
   Mettes P, 2016, LECT NOTES COMPUT SC, V9909, P437, DOI 10.1007/978-3-319-46454-1_27
   Hanh NQ, 2018, IEEE T MULTIMEDIA, V20, P1448, DOI 10.1109/TMM.2017.2772441
   Peng XJ, 2016, LECT NOTES COMPUT SC, V9908, P744, DOI 10.1007/978-3-319-46493-0_45
   Nguyen P, 2018, PROC CVPR IEEE, P6752, DOI 10.1109/CVPR.2018.00706
   Puscas MM, 2015, IEEE I CONF COMP VIS, P1653, DOI 10.1109/ICCV.2015.193
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Saha S, 2017, IEEE I CONF COMP VIS, P4424, DOI 10.1109/ICCV.2017.473
   Schreer O, 2008, IEEE T MULTIMEDIA, V10, P352, DOI 10.1109/TMM.2008.917336
   Simonyan K., 2014, P 27 INT C NEUR INF, P568, DOI DOI 10.1002/14651858.CD001941.PUB3
   Singh G, 2017, IEEE I CONF COMP VIS, P3657, DOI 10.1109/ICCV.2017.393
   Siva P, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.65
   Soomro K, 2017, IEEE I CONF COMP VIS, P696, DOI 10.1109/ICCV.2017.82
   Soomro Khurram, 2012, UCF101 DATASET 101 H
   Do TT, 2018, IEEE T MULTIMEDIA, V20, P2849, DOI 10.1109/TMM.2018.2814346
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Weinzaepfel P, 2015, IEEE I CONF COMP VIS, P3164, DOI 10.1109/ICCV.2015.362
   Yu G, 2015, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2015.7298735
   Zhang SW, 2018, IEEE T MULTIMEDIA, V20, P769, DOI 10.1109/TMM.2017.2758524
   Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317
   Zhaowei Cai, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P6154, DOI 10.1109/CVPR.2018.00644
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 43
TC 17
Z9 17
U1 1
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2020
VL 22
IS 10
BP 2610
EP 2622
DI 10.1109/TMM.2019.2959425
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NT0FL
UT WOS:000572628000008
DA 2024-07-18
ER

PT J
AU Wan, ZL
   Gu, K
   Zhao, DB
AF Wan, Zhaolin
   Gu, Ke
   Zhao, Debin
TI Reduced Reference Stereoscopic Image Quality Assessment Using Sparse
   Representation and Natural Scene Statistics
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Stereo image processing; Measurement; Visual perception;
   Distortion; Image quality; Three-dimensional displays; Stereoscopic
   image quality assessment; sparse representation; natural scene
   statistics; visual information; visual primitives
ID FREE-ENERGY PRINCIPLE; VISUAL-PERCEPTION; PREDICTION; DISTRIBUTIONS;
   PRECEDENCE; EVOLUTION; MODELS
AB An ideal quality assessment model should simulate the properties of the visual brain to be consistent with human evaluation. The visual brain appears to have both evolved to seek an efficient, decorrelated representation of image information and to "match" the statistics of the natural image. On one hand, the theoretical studies suggest that sparse representation resembles the strategy in the primary visual cortex of brain for representing natural images. On the other hand, the natural scene statistics have driven the evolution of human visual system and have also inspired the understanding and simulating of visual perception. Inspired by these observations, in this paper, we propose a novel reduced-reference stereoscopic image quality assessment metric using sparse representation and natural scene statistics to simulate the visual perception of the brain. Specifically, the distribution statistics of the classified visual primitives extracted by sparse representation are used to measure the visual information, which is closely related to the hierarchical progressive process of human visual perception. Particularly, the mutual information of classified primitives between two view images is derived as a binocular cue to simulate the binocular fusion process. The maximum mechanism that is applied to select the visual information is a pooling mechanism with which complex cells use the maximal stimuli from a group of simple cells during the transfer process in the primary visual cortex. The natural scene statistics of locally normalized luminance coefficients are used to evaluate the natural losses due to the presence of distortions. The differences of the visual information and the natural scene statistics between the original and distorted images are used to compute the quality score by a prediction function which is trained using support vector regression. Experimental results show that the proposed metric outperforms the state-of-the-art stereoscopic image quality assessment metrics on LIVE 3D IQA database and NBU-MDSID Phase-II database, and delivers competitive performance on Waterloo IVC 3D database.
C1 [Wan, Zhaolin; Zhao, Debin] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Peoples R China.
   [Wan, Zhaolin; Zhao, Debin] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
   [Gu, Ke] Beijing Univ Technol, Beijing Key Lab Computat Intelligence & Intellige, Fac Informat Technol, Beijing 100124, Peoples R China.
C3 Harbin Institute of Technology; Peng Cheng Laboratory; Beijing
   University of Technology
RP Zhao, DB (corresponding author), Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Peoples R China.
EM wanzhaolin@hit.edu.cn; guke.doctor@gmail.com; dbzhao@hit.edu.cn
RI Gu, Ke/AAJ-9684-2021; Zhao, Debin/JEP-0204-2023
FU National Science Foundation of China [61872116]; Major State Basic
   Research Development Program of China under 973 Program [2015CB351804]
FX This work was supported in part by the National Science Foundation of
   China underGrant No. 61872116 and in part the Major State Basic Research
   Development Program of China under 973 Program 2015CB351804. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Xiaoqing Zhu.
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   [Anonymous], 2012, P33331 IEEE
   [Anonymous], 2014, INT S BROADB MULT SY
   [Anonymous], 2000, Mutual information as a stereo correspondence measure
   Barlow HB., 1961, SENS COMMUN, V1, P217, DOI 10.7551/mitpress/9780262518420.003.0013
   Benoit A, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/659024
   BLAKE R, 1985, PERCEPT PSYCHOPHYS, V37, P114, DOI 10.3758/BF03202845
   Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chen MJ, 2013, SIGNAL PROCESS-IMAGE, V28, P1143, DOI 10.1016/j.image.2013.05.006
   Chen MJ, 2013, IEEE T IMAGE PROCESS, V22, P3379, DOI 10.1109/TIP.2013.2267393
   DAUGMAN JG, 1985, J OPT SOC AM A, V2, P1160, DOI 10.1364/JOSAA.2.001160
   Fookes C, 2002, INT C PATT RECOG, P937, DOI 10.1109/ICPR.2002.1048459
   Geisler WS, 2008, ANNU REV PSYCHOL, V59, P167, DOI 10.1146/annurev.psych.58.110405.085632
   Geisler WS, 2003, COGNITIVE SCI, V27, P379, DOI 10.1016/S0364-0213(03)00009-0
   Geisler WS, 2002, PHILOS T R SOC B, V357, P419, DOI 10.1098/rstb.2001.1055
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   He LH, 2012, PROC CVPR IEEE, P1146, DOI 10.1109/CVPR.2012.6247795
   Henriksen S, 2016, CURR BIOL, V26, pR500, DOI 10.1016/j.cub.2016.04.049
   Hewage CTER, 2011, IEEE T CONSUM ELECTR, V57, P1185, DOI 10.1109/TCE.2011.6018873
   Hochstein S, 2002, NEURON, V36, P791, DOI 10.1016/S0896-6273(02)01091-7
   Jiang QP, 2019, IEEE T IMAGE PROCESS, V28, P1866, DOI 10.1109/TIP.2018.2881828
   Jiang QP, 2018, IEEE T MULTIMEDIA, V20, P2035, DOI 10.1109/TMM.2017.2763321
   JONES JP, 1987, J NEUROPHYSIOL, V58, P1233, DOI 10.1152/jn.1987.58.6.1233
   Kandel E.R., 2000, Principles of neural science, V4
   Khan S, 2016, CONF REC ASILOMAR C, P1858, DOI 10.1109/ACSSC.2016.7869706
   Kim J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1033, DOI 10.1109/ICCV.2003.1238463
   Kim J, 2017, IEEE SIGNAL PROC MAG, V34, P130, DOI 10.1109/MSP.2017.2736018
   Lam EY, 2000, IEEE T IMAGE PROCESS, V9, P1661, DOI 10.1109/83.869177
   Liu YT, 2018, IEEE T MULTIMEDIA, V20, P379, DOI 10.1109/TMM.2017.2729020
   Ma SW, 2017, IEEE T CIRC SYST VID, V27, P249, DOI 10.1109/TCSVT.2015.2511838
   Maalouf A, 2011, INT CONF ACOUST SPEE, P1161
   MALLAT SG, 1989, IEEE T PATTERN ANAL, V11, P674, DOI 10.1109/34.192463
   MARCELJA S, 1980, J OPT SOC AM, V70, P1297, DOI 10.1364/JOSA.70.001297
   Md SK, 2015, IEEE SIGNAL PROC LET, V22, P1985, DOI 10.1109/LSP.2015.2449878
   Min XK, 2019, IEEE T INTELL TRANSP, V20, P2879, DOI 10.1109/TITS.2018.2868771
   Min XK, 2018, IEEE T MULTIMEDIA, V20, P2049, DOI 10.1109/TMM.2017.2788206
   Min XK, 2019, IEEE T MULTIMEDIA, V21, P2319, DOI 10.1109/TMM.2019.2902097
   Min XK, 2018, IEEE T BROADCAST, V64, P508, DOI 10.1109/TBC.2018.2816783
   Min XK, 2017, IEEE T IMAGE PROCESS, V26, P5462, DOI 10.1109/TIP.2017.2735192
   Ming-Jun Chen, 2011, 2011 IEEE 10th IVMSP Workshop: Perception and Visual Signal Analysis, P24, DOI 10.1109/IVMSPW.2011.5970349
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Mittal A, 2011, 2011 IEEE DIGITAL SIGNAL PROCESSING WORKSHOP AND IEEE SIGNAL PROCESSING EDUCATION WORKSHOP (DSP/SPE), P338, DOI 10.1109/DSP-SPE.2011.5739236
   Moorthy AK, 2013, SIGNAL PROCESS-IMAGE, V28, P870, DOI 10.1016/j.image.2012.08.004
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   NAVON D, 1977, COGNITIVE PSYCHOL, V9, P353, DOI 10.1016/0010-0285(77)90012-3
   Oh H, 2017, IEEE T IMAGE PROCESS, V26, P4923, DOI 10.1109/TIP.2017.2725584
   Ohzawa I, 1998, CURR OPIN NEUROBIOL, V8, P509, DOI 10.1016/S0959-4388(98)80039-1
   Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Olshausen BA, 1996, NETWORK-COMP NEURAL, V7, P333, DOI 10.1088/0954-898X/7/2/014
   PETTIGREW JD, 1972, SCI AM, V227, P84, DOI 10.1038/scientificamerican0872-84
   Qi F, 2015, IEEE T MULTIMEDIA, V17, P2338, DOI 10.1109/TMM.2015.2493781
   Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019, DOI 10.1038/14819
   RUDERMAN DL, 1994, NETWORK-COMP NEURAL, V5, P517, DOI 10.1088/0954-898X/5/4/006
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Schölkopf B, 2000, NEURAL COMPUT, V12, P1207, DOI 10.1162/089976600300015565
   Serre T, 2007, PROG BRAIN RES, V165, P33, DOI 10.1016/S0079-6123(06)65004-8
   Shao F, 2018, IEEE T MULTIMEDIA, V20, P2605, DOI 10.1109/TMM.2018.2817072
   Shao F, 2018, IEEE T CIRC SYST VID, V28, P573, DOI 10.1109/TCSVT.2016.2628082
   Shao F, 2016, IEEE T MULTIMEDIA, V18, P2104, DOI 10.1109/TMM.2016.2594142
   Shao F, 2015, IEEE T IMAGE PROCESS, V24, P2971, DOI 10.1109/TIP.2015.2436332
   SHARIFI K, 1995, IEEE T CIRC SYST VID, V5, P52, DOI 10.1109/76.350779
   Sheikh HR, 2005, LIVE IMAGE QUALITY A, DOI DOI 10.1109/CVPR.2015.7298594
   Simoncelli EP, 2001, ANNU REV NEUROSCI, V24, P1193, DOI 10.1146/annurev.neuro.24.1.1193
   Song Wang, 2013, 2013 ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM), P193, DOI 10.1109/ESEM.2013.24
   Srivastava A, 2003, J MATH IMAGING VIS, V18, P17, DOI 10.1023/A:1021889010444
   Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108
   Vinje WE, 2000, SCIENCE, V287, P1273, DOI 10.1126/science.287.5456.1273
   Wainwright MJ, 2000, ADV NEUR IN, V12, P855
   Wan ZL, 2017, IEEE INT CON MULTI, P73, DOI 10.1109/ICME.2017.8019337
   Wan ZL, 2017, IEEE DATA COMPR CONF, P231, DOI 10.1109/DCC.2017.27
   Wang JJ, 2014, TRANS MOBIL SER, P1
   Wang JH, 2015, IEEE T IMAGE PROCESS, V24, P3400, DOI 10.1109/TIP.2015.2446942
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZJ, 2011, IEEE SIGNAL PROC MAG, V28, P2, DOI 10.1109/MSP.2011.940297
   WOLFE JM, 1986, PSYCHOL REV, V93, P269, DOI 10.1037/0033-295X.93.3.269
   You J, 2010, IEEE CONF WIREL MOB, P1, DOI 10.1109/WIMOB.2010.5644989
   Zhang JH, 2012, APPL MECH MATER, V204-208, P674, DOI 10.4028/www.scientific.net/AMM.204-208.674
   Zhang X, 2015, IEEE SYS MAN CYBERN, P1561, DOI 10.1109/SMC.2015.276
   Zhang Z, 2015, IEEE ACCESS, V3, P490, DOI 10.1109/ACCESS.2015.2430359
   Zhou W, 2019, IEEE T IMAGE PROCESS, V28, P3946, DOI 10.1109/TIP.2019.2902831
NR 82
TC 29
Z9 29
U1 0
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2020
VL 22
IS 8
BP 2024
EP 2037
DI 10.1109/TMM.2019.2950533
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA MR2MS
UT WOS:000553424500009
DA 2024-07-18
ER

PT J
AU Jin, Z
   Iqbal, MZ
   Bobkov, D
   Zou, WB
   Li, X
   Steinbach, E
AF Jin, Zhi
   Iqbal, Muhammad Zafar
   Bobkov, Dmytro
   Zou, Wenbin
   Li, Xia
   Steinbach, Eckehard
TI A Flexible Deep CNN Framework for Image Restoration
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Training; Image restoration; Image coding; Automobiles; Task analysis;
   Transform coding; Image denoising; Image restoration; flexible CNN
   framework; image decomposition; recursive learning; residual learning
ID SUPERRESOLUTION; DEBLOCKING; MINIMIZATION; ARTIFACTS; NETWORKS
AB Image restoration is a long-standing problem in image processing and low-level computer vision. Recently, discriminative convolutional neural network (CNN)-based approaches have attracted considerable attention due to their superior performance. However, most of these frameworks are designed for one specific image restoration task; hence, they seldom show high performance on other image restoration tasks. To address this issue, we propose a flexible deep CNN framework that exploits the frequency characteristics of different types of artifacts. Hence, the same approach can be employed for a variety of image restoration tasks by adjusting the architecture. For reducing the artifacts with similar frequency characteristics, a quality enhancement network that adopts residual and recursive learning is proposed. Residual learning is utilized to speed up the training process and boost the performance; recursive learning is adopted to significantly reduce the number of training parameters as well as boost the performance. Moreover, lateral connections transmit the extracted features between different frequency streams via multiple paths. One aggregation network combines the outputs of these streams to further enhance the restored images. We demonstrate the capabilities of the proposed framework with three representative applications: image compression artifacts reduction (CAR), image denoising, and single image super-resolution (SISR). Extensive experiments confirm that the proposed framework outperforms the state-of-the-art approaches on benchmark datasets for these applications.
C1 [Jin, Zhi; Zou, Wenbin; Li, Xia] Shenzhen Univ, Coll Elect & Informat Engn, Guangdong Key Lab Intelligent Informat Proc, Shenzhen 518060, Peoples R China.
   [Iqbal, Muhammad Zafar; Bobkov, Dmytro; Steinbach, Eckehard] Tech Univ Munich, Chair Media Technol, D-80333 Munich, Germany.
   [Zou, Wenbin] Shenzhen Key Lab Adv Machine Learning & Applicat, Shenzhen 518060, Peoples R China.
C3 Shenzhen University; Technical University of Munich
RP Zou, WB (corresponding author), Shenzhen Univ, Coll Elect & Informat Engn, Guangdong Key Lab Intelligent Informat Proc, Shenzhen 518060, Peoples R China.
EM jinzhi_126@163.com; mzafar.iqbal@tum.de; dmytro.bobkov@tum.de;
   wzou@szu.edu.cn; lixia@szu.edu.cn; eckehard.steinbach@tum.de
RI Jin, Zhi/AAB-2440-2022; Iqbal, Muhammad Zafar/V-7644-2019
OI Jin, Zhi/0000-0001-9670-7366; Iqbal, Muhammad Zafar/0000-0002-5605-8143;
   Bobkov, Dmytro/0000-0002-5096-8891; Steinbach,
   Eckehard/0000-0001-8853-2703
FU National Natural Science Foundation of China [61701313, 61771321,
   61871273, 61872429]; China Postdoctoral Science Foundation
   [2017M622778]; Guangdong Key Research Platform of Universities
   [2018WCXTD015]; Science and Technology Program of Shenzhen
   [JCYJ20170818091621856]; Interdisciplinary Innovation Team of Shenzhen
   University
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61701313, 61771321, 61871273, and
   61872429; in part by China Postdoctoral Science Foundation under Grants
   2017M622778; in part by the Guangdong Key Research Platform of
   Universities under Grant 2018WCXTD015; in part by the Science and
   Technology Program of Shenzhen under Grant JCYJ20170818091621856; and in
   part by the Interdisciplinary Innovation Team of Shenzhen University.
CR [Anonymous], 2012, COMPUT SCI
   [Anonymous], PROC CVPR IEEE
   [Anonymous], ARXIV181204352
   [Anonymous], 2018, P IEEE CVF C COMP VI
   Beck A, 2009, IEEE T IMAGE PROCESS, V18, P2419, DOI 10.1109/TIP.2009.2028250
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Burger HC, 2012, PROC CVPR IEEE, P2392, DOI 10.1109/CVPR.2012.6247952
   Cavigelli L, 2017, IEEE IJCNN, P752, DOI 10.1109/IJCNN.2017.7965927
   Chang HB, 2014, IEEE T SIGNAL PROCES, V62, P718, DOI 10.1109/TSP.2013.2290508
   Chen YJ, 2017, IEEE T PATTERN ANAL, V39, P1256, DOI 10.1109/TPAMI.2016.2596743
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Dong C, 2015, IEEE I CONF COMP VIS, P576, DOI 10.1109/ICCV.2015.73
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Foi A, 2007, IEEE T IMAGE PROCESS, V16, P1395, DOI 10.1109/TIP.2007.891788
   Fu XY, 2017, IEEE T IMAGE PROCESS, V26, P2944, DOI 10.1109/TIP.2017.2691802
   Gu SH, 2014, PROC CVPR IEEE, P2862, DOI 10.1109/CVPR.2014.366
   Guo J, 2016, LECT NOTES COMPUT SC, V9905, P628, DOI 10.1007/978-3-319-46448-0_38
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   Huang DA, 2014, IEEE T MULTIMEDIA, V16, P83, DOI 10.1109/TMM.2013.2284759
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Hui Z, 2018, PROC CVPR IEEE, P723, DOI 10.1109/CVPR.2018.00082
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jin KH, 2017, IEEE T IMAGE PROCESS, V26, P4509, DOI 10.1109/TIP.2017.2713099
   Kang LW, 2015, IEEE T MULTIMEDIA, V17, P921, DOI 10.1109/TMM.2015.2434216
   Kang LW, 2012, IEEE T IMAGE PROCESS, V21, P1742, DOI 10.1109/TIP.2011.2179057
   Kao HT, 2018, 2018 5TH INTERNATIONAL CONFERENCE ON BEHAVIORAL, ECONOMIC, AND SOCIO-CULTURAL COMPUTING (BESC), P156, DOI [10.1109/BESC.2018.8697325, 10.1109/BESC.2018.00040]
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Korus P, 2015, IEEE T MULTIMEDIA, V17, P157, DOI 10.1109/TMM.2014.2368696
   Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618
   Levin A, 2012, LECT NOTES COMPUT SC, V7576, P73, DOI 10.1007/978-3-642-33715-4_6
   Li T, 2018, IEEE T MULTIMEDIA, V20, P1305, DOI 10.1109/TMM.2017.2766889
   Li Y, 2014, LECT NOTES COMPUT SC, V8690, P174, DOI 10.1007/978-3-319-10605-2_12
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu HF, 2015, PROC CVPR IEEE, P484, DOI 10.1109/CVPR.2015.7298646
   Liu JY, 2018, IEEE T MULTIMEDIA, V20, P3252, DOI 10.1109/TMM.2018.2831636
   Mao XJ, 2016, ADV NEUR IN, V29
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Ren C, 2019, IEEE T MULTIMEDIA, V21, P731, DOI 10.1109/TMM.2018.2866362
   Roth S, 2005, PROC CVPR IEEE, P860
   Rothe R, 2015, IEEE IMAGE PROC, P1543, DOI 10.1109/ICIP.2015.7351059
   Schmidt U, 2014, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR.2014.349
   Shi Y, 2017, IEEE T MULTIMEDIA, V19, P2804, DOI 10.1109/TMM.2017.2711263
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Timofte R., 2018, P IEEE C COMP VIS PA, P852
   Wang HJ, 2017, IEEE T IMAGE PROCESS, V26, P3556, DOI 10.1109/TIP.2017.2700725
   Wang WJ, 2018, IEEE ACCESS, V6, P23767, DOI 10.1109/ACCESS.2018.2829908
   Wang YL, 2008, SIAM J IMAGING SCI, V1, P248, DOI 10.1137/080724265
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xie J., 2012, ADV NEURAL INFORM PR, P341
   Xu L, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366158
   Xu M, 2014, IEEE T CIRC SYST VID, V24, P1743, DOI 10.1109/TCSVT.2014.2317886
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yin JL, 2018, IEEE T MULTIMEDIA, V20, P3045, DOI 10.1109/TMM.2018.2820910
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang W, 2018, IEEE CONF COMPUT
   Zhang XF, 2013, IEEE T IMAGE PROCESS, V22, P4613, DOI 10.1109/TIP.2013.2274386
   Zhang YB, 2018, IEEE T IMAGE PROCESS, V27, P3150, DOI 10.1109/TIP.2018.2812081
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zoran D, 2011, IEEE I CONF COMP VIS, P479, DOI 10.1109/ICCV.2011.6126278
NR 61
TC 61
Z9 65
U1 2
U2 42
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2020
VL 22
IS 4
BP 1055
EP 1068
DI 10.1109/TMM.2019.2938340
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KY2ZI
UT WOS:000522440400019
DA 2024-07-18
ER

PT J
AU Hu, YY
   Yang, WH
   Li, MD
   Liu, JY
AF Hu, Yueyu
   Yang, Wenhan
   Li, Mading
   Liu, Jiaying
TI Progressive Spatial Recurrent Neural Network for Intra Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video Coding; intra prediction; deep learning; spatial RNN; SATD loss;
   HEVC
ID IMAGE; OPTIMIZATION; HADAMARD
AB Intra prediction is an important component of modern video codecs, which is able to efficiently squeeze out the spatial redundancy in video frames. With preceding pixels as the context, traditional intra prediction schemes generate linear predictions based on several predefined directions (i.e., modes) for blocks to be encoded. However, these modes are relatively simple and their predictions may fail when facing blocks with complex textures, which leads to additional bits encoding the residue. In this paper, we design a progressive spatial recurrent neural network (PS-RNN) that learns to conduct intra prediction. Specifically, our PS-RNN consists of three spatial recurrent units and progressively generates predictions by passing information along from preceding contents to blocks to be encoded. To make our network generate predictions considering both distortion and bit rate, we propose using sum of absolute transformed difference (SATD) as the loss function to train PS-RNN since SATD is able to measure rate-distortion cost of encoding a residue block. Moreover, our method supports variable-block-size for intra prediction, which is more practical in real coding conditions. The proposed intra prediction scheme achieves on average 2.5% bit-rate reduction on variable-block-size settings under the same reconstruction quality compared with HEVC.
C1 [Hu, Yueyu; Yang, Wenhan; Li, Mading; Liu, Jiaying] Peking Univ, Inst Comp Sci & Technol, Beijing 100080, Peoples R China.
C3 Peking University
RP Liu, JY (corresponding author), Peking Univ, Inst Comp Sci & Technol, Beijing 100080, Peoples R China.
EM huyy@pku.edu.cn; yangwenhan@pku.edu.cn; martinli0822@pku.cdu.cn;
   liujiaying@pku.edu.cn
RI Liu, JY/GYJ-0138-2022
OI Liu, Jiaying/0000-0002-0468-9576
FU National Natural Science Foundation of China [61772043]; Beijing Natural
   Science Foundation [L182002, 4192025]
FX This work was supported in part by National Natural Science Foundation
   of China under contract No. 61772043 and in part by Beijing Natural
   Science Foundation under contract No. L182002 and No. 4192025.
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   Agustsson E, 2017, ADV NEUR IN, V30
   [Anonymous], 2001, VCEGM33
   [Anonymous], 12 M GEN SWITZ JAN
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], 2018, JVET K1001
   [Anonymous], P ANN C COMP GRAPH I
   [Anonymous], ARXIV180706244
   Baig MH, 2017, ADV NEUR IN, V30
   Balle J, 2017, 5 INT C LEARN REPR I
   Ballester C, 2001, IEEE T IMAGE PROCESS, V10, P1200, DOI 10.1109/83.935036
   Birman R, 2018, PROC SPIE, V10752, DOI 10.1117/12.2320551
   Chen HM, 2016, IEEE T IMAGE PROCESS, V25, P3671, DOI 10.1109/TIP.2016.2573585
   Chen Y, 2018, PICT COD SYMP, P41, DOI 10.1109/PCS.2018.8456249
   Chung Junyoung, 2014, ARXIV14123555
   Cui WX, 2017, IEEE DATA COMPR CONF, P436, DOI 10.1109/DCC.2017.53
   Gregor K, 2016, ADV NEUR IN, V29
   Jouppi NP, 2017, 44TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA 2017), P1, DOI 10.1145/3079856.3080246
   Kingma D. P., 2014, arXiv
   KUNZ HO, 1979, IEEE T COMPUT, V28, P267, DOI 10.1109/TC.1979.1675334
   Kwatra V, 2005, ACM T GRAPHIC, V24, P795, DOI 10.1145/1073204.1073263
   Lainema J, 2012, IEEE T CIRC SYST VID, V22, P1792, DOI 10.1109/TCSVT.2012.2221525
   Li JH, 2018, IEEE T IMAGE PROCESS, V27, P3236, DOI 10.1109/TIP.2018.2817044
   Li JH, 2018, IEEE T CIRC SYST VID, V28, P947, DOI 10.1109/TCSVT.2016.2633377
   Li JH, 2017, IEEE IMAGE PROC, P1, DOI 10.1109/ICIP.2017.8296231
   Li JH, 2017, IEEE DATA COMPR CONF, P221, DOI 10.1109/DCC.2017.59
   Liu D, 2007, IEEE T CIRC SYST VID, V17, P1273, DOI 10.1109/TCSVT.2007.903663
   Liu JY, 2018, IEEE T MULTIMEDIA, V20, P3252, DOI 10.1109/TMM.2018.2831636
   Liu S., 2017, P ADV NEUR INF PROC, V30, P1519
   Liu SF, 2016, LECT NOTES COMPUT SC, V9908, P560, DOI 10.1007/978-3-319-46493-0_34
   Liu ZY, 2016, IEEE INT SYMP CIRC S, P2270, DOI 10.1109/ISCAS.2016.7539036
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   PRATT WK, 1969, P IEEE, V57, P58, DOI 10.1109/PROC.1969.6869
   Qi XL, 2012, INT CONF ACOUST SPEE, P1217, DOI 10.1109/ICASSP.2012.6288107
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sullivan GJ, 1998, IEEE SIGNAL PROC MAG, V15, P74, DOI 10.1109/79.733497
   Tao Zhang, 2015, 2015 Visual Communications and Image Processing (VCIP), P1, DOI 10.1109/VCIP.2015.7457833
   Theis L., 2017, ICLR
   Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149
   Toderici G, 2017, PROC CVPR IEEE, P5435, DOI 10.1109/CVPR.2017.577
   Nguyen T, 2015, IEEE T CIRC SYST VID, V25, P790, DOI 10.1109/TCSVT.2014.2358000
   van den Oord A, 2016, PR MACH LEARN RES, V48
   Wei L, 2016, BOUND VALUE PROBL, DOI 10.1186/s13661-015-0477-3
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xia SF, 2018, IEEE DATA COMPR CONF, P127, DOI 10.1109/DCC.2018.00021
   Yang C, 2017, PROC CVPR IEEE, P4076, DOI 10.1109/CVPR.2017.434
   Yeh CH, 2015, IEEE T MULTIMEDIA, V17, P1508, DOI 10.1109/TMM.2015.2449659
   Zhang XS, 2018, IEEE IMAGE PROC, P390, DOI 10.1109/ICIP.2018.8451694
NR 48
TC 45
Z9 49
U1 1
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2019
VL 21
IS 12
BP 3024
EP 3037
DI 10.1109/TMM.2019.2920603
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KJ8ZS
UT WOS:000512345200005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Peng, QM
   Cheung, YM
AF Peng, Qinmu
   Cheung, Yiu-Ming
TI Automatic Video Object Segmentation Based on Visual and Motion Saliency
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Object segmentation; visual saliency; manifold ranking; graph model
ID IMAGE; OPTIMIZATION; TRACKING
AB We present an approach to extract the salient object automatically in videos. Given an unannotated video sequence, the proposed method first computes the visual saliency to identify object-like regions in each frame based on the proposed weighted multiple manifold ranking algorithm. We then compute motion cues to estimate the motion saliency and localization prior. Finally, adopting a new energy function, we estimate a superpixel-level object labeling across all frames, where 1) the data term depends on the visual saliency and localization prior, and 2) the smoothness term depends on the constraints in time and space. Compared to the existing counterparts, the proposed approach automatically segments the persistent foreground object meanwhile preserving the potential shape. Experiments show its promising results on the challenging benchmark videos in comparison with the existing counterparts.
C1 [Peng, Qinmu] Huazhong Univ Sci & Tecluiol, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China.
   [Peng, Qinmu] Hong Kong Baptist Univ, Dept Comp Sci, Hong Kong, Peoples R China.
   [Peng, Qinmu] Huazhong Univ Sci & Technol, Shenzhen Res Inst, Shenzhen 518055, Peoples R China.
   [Cheung, Yiu-Ming] Hong Kong Baptist Univ, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
   [Cheung, Yiu-Ming] HKBU Inst Res & Continuing Educ, Shenzhen 518057, Peoples R China.
C3 Hong Kong Baptist University; Huazhong University of Science &
   Technology; Hong Kong Baptist University; HKBU Institute for Research &
   Continuing Education
RP Cheung, YM (corresponding author), Hong Kong Baptist Univ, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
EM pengqinmu@hust.edu.cn; ymc@comp.hkbu.edu.hk
RI Cheung, Yiu-ming/E-2050-2015
OI Cheung, Yiu-ming/0000-0001-7629-4648
FU National Natural Science Foundation of China [61672444, 61272366];
   Faculty Research Grant of Hong Kong Baptist University (HKBU)
   [FRG2/17-18/082]; SZSTI [JCYJ20160531194006833, JCYJ20180305180637611];
   KTO Grant of HKBU [MPCF-0042017/18]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61672444 and 61272366, in part by the
   Faculty Research Grant of Hong Kong Baptist University (HKBU) under
   Project FRG2/17-18/082, in part by the KTO Grant of HKBU under Project
   MPCF-0042017/18, and in part by the SZSTI under Grants:
   JCYJ20160531194006833 and JCYJ20180305180637611. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Xilin Chen.
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   [Anonymous], 2011, P BMVC
   Bai X, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531376
   Bai X, 2009, INT J COMPUT VISION, V82, P113, DOI 10.1007/s11263-008-0191-z
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Blank M, 2005, IEEE I CONF COMP VIS, P1395
   Bo Peng, 2009, Computer Vision - ACCV 2009. 9th Asian Conference on Computer Vision. Revised Selected Papers, P677
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Brox T, 2010, LECT NOTES COMPUT SC, V6315, P282, DOI 10.1007/978-3-642-15555-0_21
   Chen L, 2015, IEEE T MULTIMEDIA, V17, P2225, DOI 10.1109/TMM.2015.2481711
   Cheng FC, 2010, IEEE INT CON MULTI, P754, DOI 10.1109/ICME.2010.5582598
   Dalal N., 2005, PROC IEEE COMPUT SOC, V1, P886
   Evangelopoulos G, 2013, IEEE T MULTIMEDIA, V15, P1553, DOI 10.1109/TMM.2013.2267205
   Faktor Alon, 2014, BMVC
   FAN Q., 2015, ACM T GRAPHICS, V34, P6
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   FOLEY SFJ, 1990, COMPUTER GRAPHICS PR
   Fragkiadaki K, 2015, PROC CVPR IEEE, P4083, DOI 10.1109/CVPR.2015.7299035
   Goferman S, 2010, PROC CVPR IEEE, P2376, DOI 10.1109/CVPR.2010.5539929
   Gopalakrishnan V, 2009, IEEE T MULTIMEDIA, V11, P892, DOI 10.1109/TMM.2009.2021726
   Guo C., 2008, P IEEE INT C COMP VI
   He J., P 12 ANN ACM INT C M, P9, DOI 10.1145/1027527.1027531
   Jerripothula KR, 2016, IEEE T MULTIMEDIA, V18, P1896, DOI 10.1109/TMM.2016.2576283
   Lee YJ, 2011, IEEE I CONF COMP VIS, P1995, DOI 10.1109/ICCV.2011.6126471
   Lei JJ, 2016, IEEE T MULTIMEDIA, V18, P1783, DOI 10.1109/TMM.2016.2592325
   Levinshtein A, 2009, IEEE T PATTERN ANAL, V31, P2290, DOI 10.1109/TPAMI.2009.96
   Li FX, 2013, IEEE I CONF COMP VIS, P2192, DOI 10.1109/ICCV.2013.273
   Li WT, 2013, IEEE T IMAGE PROCESS, V22, P2600, DOI 10.1109/TIP.2013.2253483
   Ma TY, 2012, PROC CVPR IEEE, P670, DOI 10.1109/CVPR.2012.6247735
   Ngo CW, 2005, IEEE T CIRC SYST VID, V15, P296, DOI 10.1109/TCSVT.2004.841694
   Oikonomopoulos A, 2006, IEEE T SYST MAN CY B, V36, P710, DOI 10.1109/TSMCB.2005.861864
   Papazoglou A, 2013, IEEE I CONF COMP VIS, P1777, DOI 10.1109/ICCV.2013.223
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Price BL, 2009, IEEE I CONF COMP VIS, P779, DOI 10.1109/ICCV.2009.5459293
   Ramakanth SA, 2014, PROC CVPR IEEE, P376, DOI 10.1109/CVPR.2014.55
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Sundaram N, 2010, LECT NOTES COMPUT SC, V6311, P438, DOI 10.1007/978-3-642-15549-9_32
   Tsai D, 2012, INT J COMPUT VISION, V100, P190, DOI 10.1007/s11263-011-0512-5
   Tsai YH, 2016, PROC CVPR IEEE, P3899, DOI 10.1109/CVPR.2016.423
   Vikram TN, 2012, PATTERN RECOGN, V45, P3114, DOI 10.1016/j.patcog.2012.02.009
   Wang J, 2005, ACM T GRAPHIC, V24, P585, DOI 10.1145/1073204.1073233
   Wang TH, 2012, IEEE T MULTIMEDIA, V14, P389, DOI 10.1109/TMM.2011.2177078
   Wang WG, 2017, IEEE I CONF COMP VIS, P1680, DOI 10.1109/ICCV.2017.185
   Wang Wenguan, 2018, IEEE Trans Image Process, V27, P38, DOI 10.1109/TIP.2017.2754941
   Wang WG, 2017, IEEE T IMAGE PROCESS, V26, P5645, DOI 10.1109/TIP.2017.2745098
   Wang WG, 2018, IEEE T PATTERN ANAL, V40, P20, DOI 10.1109/TPAMI.2017.2662005
   Wang WG, 2017, IEEE T VIS COMPUT GR, V23, P2014, DOI 10.1109/TVCG.2016.2600594
   Wang WG, 2015, IEEE T IMAGE PROCESS, V24, P4185, DOI 10.1109/TIP.2015.2460013
   Wang Wenguan, 2015, IEEE Trans Image Process, V24, P3137, DOI 10.1109/TIP.2015.2438550
   Wei YC, 2012, LECT NOTES COMPUT SC, V7574, P29, DOI 10.1007/978-3-642-33712-3_3
   Wenguan Wang, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3395, DOI 10.1109/CVPR.2015.7298961
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yang J, 2016, IEEE T IMAGE PROCESS, V25, P503, DOI 10.1109/TIP.2015.2500820
   Yu YL, 2010, IEEE T SYST MAN CY B, V40, P1398, DOI 10.1109/TSMCB.2009.2038895
   Zhang D, 2013, PROC CVPR IEEE, P628, DOI 10.1109/CVPR.2013.87
   Zhang JM, 2015, IEEE I CONF COMP VIS, P1404, DOI 10.1109/ICCV.2015.165
   Zhang Y, 2015, PROC CVPR IEEE, P3641, DOI 10.1109/CVPR.2015.7298987
   Zhong J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P44, DOI 10.1109/ICCV.2003.1238312
   Zhou DY, 2004, ADV NEUR IN, V16, P169
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
NR 60
TC 14
Z9 17
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2019
VL 21
IS 12
BP 3083
EP 3094
DI 10.1109/TMM.2019.2918730
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KJ8ZS
UT WOS:000512345200009
DA 2024-07-18
ER

PT J
AU Zhang, ZZ
   Xie, Y
   Zhang, WS
   Tian, Q
AF Zhang, Zhizhong
   Xie, Yuan
   Zhang, Wensheng
   Tian, Qi
TI Effective Image Retrieval via Multilinear Multi-Index Fusion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Image representation; Optimization; Buildings; Indexing;
   Image retrieval; multi-index fusion; tensor multi-rank; person
   re-identification
ID SCALE; FEATURES
AB Multi-index fusion has demonstrated impressive performances in the retrieval task by integrating different visual representations in a unified framework. However, previous works mainly consider propagating similarities via a neighbor structure, ignoring the high-order information among different visual representations. In this paper, we propose a new multi-index fusion scheme for image retrieval. By formulating this procedure as a multilinear-based optimization problem, the complementary information hidden in different indexes can be explored more thoroughly. Specifically, we first build our multiple indexes from various visual representations. Then, a so-called index-specific functional matrix, which aims to propagate similarities, is introduced to update the original index. The functional matrices are then optimized in a unified tensor space to achieve a refinement, such that the relevant images can be pushed closer. The optimization problem can be efficiently solved by the augmented Lagrangian method with a theoretical convergence guarantee. Unlike the traditional multi-index fusion scheme, our approach embeds the multi-index subspace structure into the new indexes with sparse constraint and, thus, it has little additional memory consumption in the online query stage. Experimental evaluation on three benchmark datasets reveals that the proposed approach achieves state-of-the-art performance, that is, N-score 3.94 on UKBench, mAP 94.1 on Holiday, and 62.39 on Market-1501.
C1 [Zhang, Zhizhong; Zhang, Wensheng] Chinese Acad Sci, Inst Automat, Res Ctr Precis Sensing & Control, Beijing 100190, Peoples R China.
   [Zhang, Zhizhong; Zhang, Wensheng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Shenzhen 518000, Peoples R China.
   [Xie, Yuan] East China Normal Univ, Sch Comp Sci & Software Engn, Shanghai 200241, Peoples R China.
   [Tian, Qi] Huawei Noahs Ark Lab, Comp Vis, Shenzhen 518000, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   East China Normal University; Huawei Technologies
RP Zhang, WS (corresponding author), Chinese Acad Sci, Inst Automat, Res Ctr Precis Sensing & Control, Beijing 100190, Peoples R China.; Tian, Q (corresponding author), Huawei Noahs Ark Lab, Comp Vis, Shenzhen 518000, Peoples R China.
EM zhangzhizhong2014@ia.ac.cn; yxie@sei.ecnu.edu.cn;
   zhangwenshengia@hotmail.com; tian.qi1@huawei.com
OI xie, yuan/0000-0001-6945-7437
FU National Key R&D Program of China [2017YFC0803700]; National Natural
   Science Foundation of China [U1636220, 61432008, 61472423, 61772524];
   Beijing Municipal Natural Science Foundation [4182067]
FX The work was supported in part by the National Key R&D Program of China
   (2017YFC0803700), in part by the National Natural Science Foundation of
   China (U1636220, 61432008, 61472423, and 61772524), and in part by the
   Beijing Municipal Natural Science Foundation (4182067). The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Benoit Huet. (Zhizhong Zhang and Yuan Xie
   contributed equally to this work.)
CR [Anonymous], 2007, P IEEE C COMP VIS PA
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], 2014, CVPR
   [Anonymous], ARXIV161007126
   ARANDJELOVIC R, 2012, PROC CVPR IEEE, P2911, DOI DOI 10.1109/CVPR.2012.6248018
   Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018
   Babenko A., 2014, P EUR C COMPUT VIS
   Babenko A, 2015, IEEE T PATTERN ANAL, V37, P1247, DOI 10.1109/TPAMI.2014.2361319
   Barbosa IB, 2018, COMPUT VIS IMAGE UND, V167, P50, DOI 10.1016/j.cviu.2017.12.002
   Chen DP, 2016, PROC CVPR IEEE, P1268, DOI 10.1109/CVPR.2016.142
   Chen T, 2013, IEEE T CIRC SYST VID, V23, P1611, DOI 10.1109/TCSVT.2013.2254978
   Chen X., 2017, P IEEE INT C COMM SY
   Chen ZX, 2017, IEEE T MULTIMEDIA, V19, P1996, DOI 10.1109/TMM.2017.2705918
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Gao ZN, 2016, IEEE T MULTIMEDIA, V18, P1661, DOI 10.1109/TMM.2016.2568748
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Hao YB, 2017, IEEE T MULTIMEDIA, V19, P1, DOI 10.1109/TMM.2016.2610324
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Jégou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235
   Jégou H, 2012, LECT NOTES COMPUT SC, V7573, P774, DOI 10.1007/978-3-642-33709-3_55
   Jia Yangqing, 2014, ARXIV14085093, DOI [10.1145/2647868.2654889, DOI 10.1145/2647868.2654889]
   Kilmer ME, 2013, SIAM J MATRIX ANAL A, V34, P148, DOI 10.1137/110837711
   Kilmer ME, 2011, LINEAR ALGEBRA APPL, V435, P641, DOI 10.1016/j.laa.2010.09.020
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li Z, 2015, IEEE IPCCC
   Lin Z., 2009, Technical Report (No. UILU-ENG-09-2215
   Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88
   Liu Z, 2015, IEEE T MULTIMEDIA, V17, P538, DOI 10.1109/TMM.2015.2399851
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Mohedano E, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P327, DOI 10.1145/2911996.2912061
   Neyshabur B, 2015, PR MACH LEARN RES, V37, P1926
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Nister David, 2006, CVPR
   Philbin J, 2008, PROC CVPR IEEE, P2285
   Radenovic F, 2016, LECT NOTES COMPUT SC, V9905, P3, DOI 10.1007/978-3-319-46448-0_1
   Semerci O, 2014, IEEE T IMAGE PROCESS, V23, P1678, DOI 10.1109/TIP.2014.2305840
   Shen FM, 2015, IEEE I CONF COMP VIS, P4148, DOI 10.1109/ICCV.2015.472
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sivic J., 2003, P IEEE C COMP VIS PA
   Su C, 2016, LECT NOTES COMPUT SC, V9906, P475, DOI 10.1007/978-3-319-46475-6_30
   Varior RR, 2016, LECT NOTES COMPUT SC, V9911, P135, DOI 10.1007/978-3-319-46478-7_9
   Varior RR, 2016, LECT NOTES COMPUT SC, V9912, P791, DOI 10.1007/978-3-319-46484-8_48
   Wang J, 2016, P IEEE, V104, P34, DOI 10.1109/JPROC.2015.2487976
   Weiss Y., 2009, ADV NEURAL INF PROCE
   Yandex Artem Babenko, 2015, 2015 IEEE International Conference on Computer Vision (ICCV). Proceedings, P1269, DOI 10.1109/ICCV.2015.150
   Yang Y, 2013, IEEE T MULTIMEDIA, V15, P572, DOI 10.1109/TMM.2012.2234731
   Zhang L, 2016, PROC CVPR IEEE, P1239, DOI 10.1109/CVPR.2016.139
   Zhang ST, 2015, IEEE T PATTERN ANAL, V37, P803, DOI 10.1109/TPAMI.2014.2346201
   Zhang SL, 2015, IEEE T PATTERN ANAL, V37, P2573, DOI 10.1109/TPAMI.2015.2417573
   Zhang Z., 2014, P IEEE C COMP VIS PA
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng L, 2015, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2015.7298783
   Zheng Liang, 2016, arXiv preprint arXiv
   Zhong Z., 2017, P IEEE CVF C COMP VI, P1318, DOI [10.1109/CVPR.2017.389, DOI 10.1109/CVPR.2017.389]
   Zhou W., 2010, P 18 ACM INT C MULTI, P511
   Zhou WG, 2018, IEEE T PATTERN ANAL, V40, P1154, DOI 10.1109/TPAMI.2017.2676779
NR 57
TC 12
Z9 13
U1 0
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2019
VL 21
IS 11
BP 2878
EP 2890
DI 10.1109/TMM.2019.2915036
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JJ7VX
UT WOS:000494363000015
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Jiang, SQ
   Min, WQ
   Mei, SH
AF Jiang, Shuqiang
   Min, Weiqing
   Mei, Shuhuan
TI Hierarchy-Dependent Cross-Platform Multi-View Feature Learning for Venue
   Category Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; knowledge transfer; supervised learning; video
   signal processing; Web 2.0
AB In this paper, we focus on visual venue category prediction, which can facilitate various applications for location-based service and personalization. Considering the complementarity of different media platforms, it is reasonable to leverage venue-relevant media data from different platforms to boost the prediction performance. Intuitively, recognizing one venue category involves multiple semantic cues, especially objects and scenes and, thus, they should contribute together to venue category prediction. In addition, these venues can be organized in a natural hierarchical structure, which provides prior knowledge to guide venue category estimation. Taking these aspects into account, we propose a Hierarchy-dependent Cross-platform Multi-view Feature Learning (HCM-FL) framework for venue category prediction from videos by leveraging images from other platforms. HCM-FL includes two major components, namely Cross-Platform Transfer Deep Learning (CPTDL) and Multi-View Feature Learning with the Hierarchical Venue Structure (MVFL-HVS). CPTDL is capable of reinforcing the learned deep network from videos using images from other platforms. Specifically, CPTDL first trained a deep network using videos. These images from other platforms are filtered by the learnt network and these selected images are then fed into this learnt network to enhance it. Two kinds of pre-trained networks on the ImageNet and Places dataset are employed. Therefore, we can harness both object-oriented and scene-oriented deep features through these enhanced deep networks. MVFL-HVS is then developed to enable multi-view feature fusion. It is capable of embedding the hierarchical structure ontology to support more discriminative joint feature learning. We conduct the experiment on videos from Vine and images from Foursquare. These experimental results demonstrate the advantage of our proposed framework in jointly utilizing multi-platform data, multi-view deep features, and hierarchical venue structure knowledge.
C1 [Jiang, Shuqiang; Min, Weiqing; Mei, Shuhuan] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
   [Jiang, Shuqiang] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Min, Weiqing] Chinese Acad Sci, Shenyang Inst Automat, State key Lab Robot, Shenyang 110016, Liaoning, Peoples R China.
   [Mei, Shuhuan] Shandong Univ Sci & Technol, Qingdao 266590, Shandong, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Chinese Academy of Sciences; Shenyang Institute of Automation, CAS;
   Shandong University of Science & Technology
RP Jiang, SQ (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
EM sqjiang@ict.ac.cn; minweiqing@ict.ac.cn; shuhuan.mei@vipl.ict.ac.cn
FU Beijing Natural Science Foundation [4174106]; National Natural Science
   Foundation of China [61532018, 61602437]; Lenovo Outstanding Young
   Scientists Program; National Program for Special Support of Eminent
   Professionals; National Program for Support of Top-notch Young
   Professionals; China Postdoctoral Science Foundation [2017T100110];
   State Key Laboratory of Robotics
FX This work was supported in part by the Beijing Natural Science
   Foundation under Grant 4174106, in part by the National Natural Science
   Foundation of China under Grants 61532018 and 61602437, in part by the
   Lenovo Outstanding Young Scientists Program, in part by the National
   Program for Special Support of Eminent Professionals and National
   Program for Support of Top-notch Young Professionals, in part by the
   China Postdoctoral Science Foundation under Grant 2017T100110, and in
   part by the State Key Laboratory of Robotics. The associate editor
   coordinating the reviewof this manuscript and approving it for
   publicationwas Dr. Marco Bertini.
CR [Anonymous], P EUR C COMPUT VIS
   [Anonymous], 2016, P 13 AAAI C ART INT
   [Anonymous], MULTIMODAL LOCATION
   [Anonymous], 2013, INT C MULTIMEDIA RET, DOI DOI 10.1145/2461466.2461468
   [Anonymous], THESIS
   Bao BK, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2730889
   Dredze M., 2016, HLT NAACL, P1064
   Farseev A, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P195, DOI 10.1145/3077136.3080774
   Fu J, 2015, 2015 INTERNATIONAL CONFERENCE ON MANAGEMENT SCIENCE AND ENGINEERING, MSE 2015, P349
   Gan C, 2016, PROC CVPR IEEE, P923, DOI 10.1109/CVPR.2016.106
   Gopal S., 2012, Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume, V2, P2411
   Gopal S, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P257
   Guan T, 2015, ACM T INTEL SYST TEC, V7, DOI 10.1145/2795234
   Han B, 2014, J ARTIF INTELL RES, V49, P451, DOI 10.1613/jair.4200
   Han XT, 2017, IEEE T MULTIMEDIA, V19, P1583, DOI 10.1109/TMM.2017.2671414
   Hays J, 2008, PROC CVPR IEEE, P3436
   Herranz L, 2016, PROC CVPR IEEE, P571, DOI 10.1109/CVPR.2016.68
   Hulden M, 2015, AAAI CONF ARTIF INTE, P145
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Liu M, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P970, DOI 10.1145/3123266.3123341
   Min WQ, 2015, IEEE T MULTIMEDIA, V17, P1787, DOI 10.1109/TMM.2015.2463226
   Min WQ, 2014, IEEE MULTIMEDIA, V21, P20, DOI 10.1109/MMUL.2014.1
   Min WQ, 2014, IEEE T MULTIMEDIA, V16, P623, DOI 10.1109/TMM.2014.2302744
   Vo N, 2017, IEEE I CONF COMP VIS, P2640, DOI 10.1109/ICCV.2017.286
   Nie LQ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1192, DOI 10.1145/3123266.3123313
   Roy SD, 2013, IEEE T MULTIMEDIA, V15, P1255, DOI 10.1109/TMM.2013.2265079
   Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16
   Sang JT, 2017, ACM T INTEL SYST TEC, V8, DOI 10.1145/3001594
   Simonyan K., 2014, 14091556 ARXIV
   Song YC, 2012, IEEE T MULTIMEDIA, V14, P456, DOI 10.1109/TMM.2011.2172937
   Srivastava N., 2013, NIPS, P2094
   Vandat A, 2017, ADV NEUR IN, V30
   Wang J, 2016, PROCEEDINGS OF THE 2016 12TH WORLD CONGRESS ON INTELLIGENT CONTROL AND AUTOMATION (WCICA), P3361, DOI 10.1109/WCICA.2016.7578462
   Wang S, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2700292
   Weyand T, 2016, LECT NOTES COMPUT SC, V9912, P37, DOI 10.1007/978-3-319-46484-8_3
   Xiang He, 2016, 2016 13th IEEE Annual Consumer Communications & Networking Conference (CCNC), P1125, DOI 10.1109/CCNC.2016.7444947
   Xiao T, 2015, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2015.7298885
   Yan M, 2015, IEEE T MULTIMEDIA, V17, P1248, DOI 10.1109/TMM.2015.2446949
   Zahálka J, 2015, IEEE T MULTIMEDIA, V17, P2235, DOI 10.1109/TMM.2015.2480007
   Zhang XM, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2401
   Zhang XM, 2016, IEEE T MULTIMEDIA, V18, P1855, DOI 10.1109/TMM.2016.2574122
   Zheng YT, 2009, PROC CVPR IEEE, P1085, DOI 10.1109/CVPRW.2009.5206749
   Zhou BL, 2014, ADV NEUR IN, V27
   Zhu W, 2016, IEEE INT C BIOINFORM, P1415, DOI 10.1109/BIBM.2016.7822730
   Zubiaga A, 2017, IEEE T KNOWL DATA EN, V29, P2053, DOI 10.1109/TKDE.2017.2698463
NR 46
TC 9
Z9 9
U1 1
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2019
VL 21
IS 6
BP 1609
EP 1619
DI 10.1109/TMM.2018.2876830
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IA1QQ
UT WOS:000469337400021
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhang, ZX
   Han, J
   Coutinho, E
   Schuller, B
AF Zhang, Zixing
   Han, Jing
   Coutinho, Eduardo
   Schuller, Bjorn
TI Dynamic Difficulty Awareness Training for Continuous Emotion Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Emotion prediction; difficulty awareness learning; dynamic learning
ID RECOGNITION; ATTENTION; NETWORK
AB Time-continuous emotion prediction has become an increasingly compelling task in machine learning. Considerable efforts have been made to advance the performance of these systems. Nonetheless, the main focus has been the development of more sophisticated models and the incorporation of different expressive modalities (e.g., speech, face, and physiology). In this paper, motivated by the benefit of difficulty awareness in a human learning procedure, we propose a novel machine learning framework, namely, dynamic difficulty awareness training (DDAT), which sheds fresh light on the research-directly exploiting the difficulties in learning to boost the machine learning process. The DDAT framework consists of two stages: information retrieval and information exploitation. In the first stage, we make use of the reconstruction error of input features or the annotation uncertainty to estimate the difficulty of learning specific information. The obtained difficulty level is then used in tandem with original features to update the model input in a second learning stage with the expectation that the model can learn to focus on high difficulty regions of the learning process. We perform extensive experiments on a benchmark database REmote COLlaborative and affective to evaluate the effectiveness of the proposed framework. The experimental results show that our approach outperforms related baselines as well as other well-established time-continuous emotion prediction systems, which suggests that dynamically integrating the difficulty information for neural networks can help enhance the learning process.
C1 [Zhang, Zixing; Schuller, Bjorn] Imperial Coll London, Grp Language Audio & Mus, London SW7 2AZ, England.
   [Han, Jing; Coutinho, Eduardo; Schuller, Bjorn] Univ Augsburg, ZD B Chair Embedded Intelligence Hlth Care & Well, D-86159 Augsburg, Germany.
   [Coutinho, Eduardo] Univ Liverpool, Dept Mus, Liverpool L69 3BX1, Merseyside, England.
C3 Imperial College London; University of Augsburg; University of Liverpool
RP Han, J (corresponding author), Univ Augsburg, ZD B Chair Embedded Intelligence Hlth Care & Well, D-86159 Augsburg, Germany.
EM zixing.zhang@imperial.ac.uk; jing.han@informatik.uni-augsburg.de;
   e.coutinho@liverpool.ac.uk; bjoern.schuller@imperial.ac.uk
RI Han, Jing/AAB-3944-2020; Coutinho, Eduardo/K-1391-2019; Schuller, Björn
   Wolfgang/D-3241-2011
OI Han, Jing/0000-0001-5776-6849; Coutinho, Eduardo/0000-0001-5234-1497;
   Schuller, Björn Wolfgang/0000-0002-6478-8699
FU UK's Economic and Social Research Council [HJ-253479]; European Union
   [645094, 645378]; TransAtlantic Platform "Digging into Data"
   collaboration Grant (ACLEW: Analysing Child Language Experiences Around
   TheWorld); ESRC [ES/R00398X/1] Funding Source: UKRI
FX This work was supported in part by the TransAtlantic Platform "Digging
   into Data" collaboration Grant (ACLEW: Analysing Child Language
   Experiences Around TheWorld), in part by the UK's Economic and Social
   Research Council through the research Grant HJ-253479 (ACLEW), and in
   part by the European Union's Horizon 2020 Programme through Research and
   Innovation Action 645094 (SEWA) and 645378 (ARIA-VALUSPA). The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Chuan Wu.
CR [Anonymous], 2014, PROCEDIA IEEE COMPUT, DOI DOI 10.1109/CVPR.2014.233
   [Anonymous], 2015, P 5 INT WORKSH AUD V, DOI DOI 10.1145/2808196.2811642
   [Anonymous], 2014, IEEE Sympos Comput Intell Ensemble Learn, DOI DOI 10.1109/CIEL.2014.7015739
   [Anonymous], 2013, 2013 10 IEEE INT C W, DOI [10.1109/FG.2013.6553805, DOI 10.1109/FG.2013.6553805]
   [Anonymous], 2012, SPEECH COMMUNICATION
   [Anonymous], 2014, P INTERSPEECH
   Bengio Y., 2009, P 26 ANN INT C MACH, V60, P6, DOI [DOI 10.1145/1553374.1553380, 10.1145/1553374.1553380]
   Brady K, 2016, PROCEEDINGS OF THE 6TH INTERNATIONAL WORKSHOP ON AUDIO/VISUAL EMOTION CHALLENGE (AVEC'16), P97, DOI 10.1145/2988257.2988264
   Braun S, 2017, EUR SIGNAL PR CONF, P548, DOI 10.23919/EUSIPCO.2017.8081267
   Chao L, 2015, P 5 INT WORKSH AUD V, P65, DOI [DOI 10.1145/2808196.2811634, 10.1145/2808196.2811634]
   Cho Kyunghyun, 2014, SYNTAX SEMANTICS STR, P5, DOI [10.3115/v1/w14-4012, 10.3115 /v1/D14-1179, DOI 10.3115/V1/D14-1179]
   Cohen J, 2003, APPL MULTIPLE REGRES, DOI DOI 10.4324/9780203774441
   Coutinho E, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0191754
   Dang T., 2017, P 7 ANN WORKSHOP AUD, P27
   Dang T, 2017, INTERSPEECH, P1248, DOI 10.21437/Interspeech.2017-512
   Deng J, 2018, IEEE-ACM T AUDIO SPE, V26, P31, DOI 10.1109/TASLP.2017.2759338
   Devillers L, 2005, NEURAL NETWORKS, V18, P407, DOI 10.1016/j.neunet.2005.03.007
   Eyben F, 2016, IEEE T AFFECT COMPUT, V7, P190, DOI 10.1109/TAFFC.2015.2457417
   Graves A, 2012, STUD COMPUT INTELL, V385, P37
   Gui LK, 2017, IEEE INT CONF AUTOMA, P505, DOI 10.1109/FG.2017.68
   Gunes Hatice, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P827, DOI 10.1109/FG.2011.5771357
   Gunes Hatice, 2010, International Journal of Strategic Synthetic Emotions, V1, P68, DOI 10.4018/jse.2010101605
   Han J, 2017, IMAGE VISION COMPUT, V65, P76, DOI 10.1016/j.imavis.2016.11.020
   Han J, 2017, INT CONF ACOUST SPEE, P2367, DOI 10.1109/ICASSP.2017.7952580
   He Lang, 2015, P 5 INT WORKSH AUD V
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Huang S, 2015, HIST PHILOS TECHNOSC, V5, P41
   Jozefowicz R, 2015, PR MACH LEARN RES, V37, P2342
   Kingma D. P., 2014, arXiv
   Lo Presti L, 2017, COMPUT VIS IMAGE UND, V156, P19, DOI 10.1016/j.cviu.2016.10.007
   Lotfian R., 2018, ARXIV180510339
   Ma XC, 2016, PROCEEDINGS OF THE 6TH INTERNATIONAL WORKSHOP ON AUDIO/VISUAL EMOTION CHALLENGE (AVEC'16), P35, DOI 10.1145/2988257.2988267
   Marchi E, 2015, INT CONF ACOUST SPEE, P1996, DOI 10.1109/ICASSP.2015.7178320
   Mariooryad S, 2015, IEEE T AFFECT COMPUT, V6, P97, DOI 10.1109/TAFFC.2014.2334294
   Mauss I, 2009, COGNITION EMOTION, V23, P209, DOI 10.1080/02699930802204677
   Meng HY, 2016, IEEE T CYBERNETICS, V46, P916, DOI 10.1109/TCYB.2015.2418092
   Nicolaou MA, 2012, IMAGE VISION COMPUT, V30, P186, DOI 10.1016/j.imavis.2011.12.005
   Parthasarathy S, 2017, INTERSPEECH, P1103, DOI 10.21437/Interspeech.2017-1494
   Petridis S, 2016, IEEE T AFFECT COMPUT, V7, P45, DOI 10.1109/TAFFC.2015.2446462
   Picard R. W., 1997, AFFECTIVE COMPUTING
   POSNER MI, 1990, ANNU REV NEUROSCI, V13, P25, DOI 10.1146/annurev.ne.13.030190.000325
   Ringeval F, 2015, PATTERN RECOGN LETT, V66, P22, DOI 10.1016/j.patrec.2014.11.007
   Ringeval Fabien, 2017, P 7 ANN WORKSHOP AUD, P3, DOI DOI 10.1145/3133944.3133953
   Schuller B., 2013, Computational Paralinguistics: Emotion, Affect and Personality in Speech and Language Processing
   Schuller B, 2011, LECT NOTES COMPUT SC, V6975, P415, DOI 10.1007/978-3-642-24571-8_53
   Seltzer ML, 2013, INT CONF ACOUST SPEE, P7398, DOI 10.1109/ICASSP.2013.6639100
   Tzirakis P, 2017, IEEE J-STSP, V11, P1301, DOI 10.1109/JSTSP.2017.2764438
   Valstar M, 2016, PROCEEDINGS OF THE 6TH INTERNATIONAL WORKSHOP ON AUDIO/VISUAL EMOTION CHALLENGE (AVEC'16), P3, DOI 10.1145/2988257.2988258
   Vincent Pascal, 2008, P 25 INT C MACHINE L, DOI DOI 10.1145/1390156.1390294
   Washburn DA, 2001, LEARN MOTIV, V32, P36, DOI 10.1006/lmot.2000.1065
   Wei J., 2014, Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2014 Asia-Pacific, P1
   Wöllmer M, 2008, INTERSPEECH 2008: 9TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2008, VOLS 1-5, P597
   WU Q, 2017, PROC ACM INT CONF MU, P890, DOI DOI 10.1109/ECTC.2017.99
   Xia R, 2017, IEEE T AFFECT COMPUT, V8, P3, DOI 10.1109/TAFFC.2015.2512598
   Xia Y, 2015, IEEE I CONF COMP VIS, P1511, DOI 10.1109/ICCV.2015.177
   Yang H, 2015, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2015.526
   Yang YH, 2013, IEEE T MULTIMEDIA, V15, P1304, DOI 10.1109/TMM.2013.2265078
   Zeng ZH, 2009, IEEE T PATTERN ANAL, V31, P39, DOI 10.1109/TPAMI.2008.52
   Zhang SQ, 2018, IEEE T MULTIMEDIA, V20, P1576, DOI 10.1109/TMM.2017.2766843
   Zhang Z., 2014, Proc. 5th Int. Workshop Emotion Social Signals, Sentiment, Linked Open Data, Satellite of LREC 2014, Reykjavik, P21
   Zhang ZX, 2016, INTERSPEECH, P3593, DOI 10.21437/Interspeech.2016-998
   Zhang ZX, 2017, IEEE SIGNAL PROC MAG, V34, P107, DOI 10.1109/MSP.2017.2699358
   Zhao SC, 2017, IEEE T MULTIMEDIA, V19, P632, DOI 10.1109/TMM.2016.2617741
   2017, IEEE T MULTIMEDIA, V19, P2816, DOI DOI 10.1109/TMM.2017.2713408
   2011, IEEE T AFFECT COMPUT, V2, P92, DOI DOI 10.1109/T-AFFC.2011.9
   2014, IEEE T MULTIMEDIA, V16, P2203, DOI DOI 10.1109/TMM.2014.2360798
   2015, IEEE-ACM T AUDIO SPE, V23, P115, DOI DOI 10.1109/TASLP.2014.2375558
   2018, IEEE ACCESS, V6, P2219, DOI DOI 10.1109/ACCESS.2018.2821192
NR 68
TC 23
Z9 24
U1 1
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2019
VL 21
IS 5
BP 1289
EP 1301
DI 10.1109/TMM.2018.2871949
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HV8HY
UT WOS:000466223600017
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Li, YK
AF Li, Yuke
TI A Deep Spatiotemporal Perspective for Understanding Crowd Behavior
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Computer vision; crowd behavior understanding; spatiotemporal
   dependencies; deep neural networks
ID REPRESENTATIONS; MODEL
AB Understanding crowd behavior is a pivotal step toward urban scene analysis. This is considered a very challenging task and has rarely been addressed to date due to the complexity of motion dynamics co-occurring across a given scene, which involves spatial and temporal dependencies. Unlike the mainstream research, which usually treats the temporal and spatial dependencies in a crowd separately, this paper presents a deep end-to-end approach that jointly considers the spatiotemporal information, leading to a rich understanding of crowd behavior. We first extract the displacement information describing crowd motion patterns from tracklets/trajectories. This information is subsequently fed into a convolutional layer to learn the underlying motion patterns and create high-level representations. The derived representations are used as inputs to a long short-term memory-based architecture to learn the underlying spatiotemporal cues in a single operation for the entire crowd in a given scene. We evaluate our approach on a widely used, large-scale benchmark datasets for three critical applications: pedestrian path forecasting, destination estimation, and holistic crowd behavior classification. The results show a drastic improvement compared to recently trending works.
C1 [Li, Yuke] Ist Italiano Tecnol, Dept Pattern Anal & Comp Vis, I-16163 Genoa, Italy.
   [Li, Yuke] Wuhan Univ, State Key Lab Informat Engn Surveying Mapping & R, Wuhan 430070, Hubei, Peoples R China.
C3 Istituto Italiano di Tecnologia - IIT; Wuhan University
RP Li, YK (corresponding author), Ist Italiano Tecnol, Dept Pattern Anal & Comp Vis, I-16163 Genoa, Italy.; Li, YK (corresponding author), Wuhan Univ, State Key Lab Informat Engn Surveying Mapping & R, Wuhan 430070, Hubei, Peoples R China.
EM sunfreshing@whu.edu.cn
CR Abdulnabi AH, 2015, IEEE T MULTIMEDIA, V17, P1949, DOI 10.1109/TMM.2015.2477680
   Alahi A, 2016, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2016.110
   [Anonymous], P BIGLEARN ADV NEUR
   [Anonymous], 2015, NIPS 15 P 28 INT C N
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], P EUR C COMPUT VIS
   [Anonymous], 2015, ARXIV151107889
   [Anonymous], P ACM MULT C
   Bearman A, 2016, LECT NOTES COMPUT SC, V9911, P549, DOI 10.1007/978-3-319-46478-7_34
   Deng ZW, 2016, PROC CVPR IEEE, P4772, DOI 10.1109/CVPR.2016.516
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Graves A., 2013, Generating sequences with recurrent neural networks
   Graves A, 2014, PR MACH LEARN RES, V32, P1764
   He CK, 2018, MULTIMED TOOLS APPL, V77, P29573, DOI 10.1007/s11042-017-5255-z
   Jain A, 2016, PROC CVPR IEEE, P5308, DOI 10.1109/CVPR.2016.573
   Lerner A, 2007, COMPUT GRAPH FORUM, V26, P655, DOI 10.1111/j.1467-8659.2007.01089.x
   Li JN, 2018, IEEE T MULTIMEDIA, V20, P985, DOI 10.1109/TMM.2017.2759508
   Li YK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P235, DOI 10.1145/3123266.3123287
   Liu WX, 2016, IEEE T MULTIMEDIA, V18, P2398, DOI 10.1109/TMM.2016.2598091
   Pellegrini S, 2009, IEEE I CONF COMP VIS, P261, DOI 10.1109/ICCV.2009.5459260
   Shao J, 2016, PROC CVPR IEEE, P5620, DOI 10.1109/CVPR.2016.606
   Shao J, 2015, PROC CVPR IEEE, P4657, DOI 10.1109/CVPR.2015.7299097
   Shao J, 2014, PROC CVPR IEEE, P2227, DOI 10.1109/CVPR.2014.285
   Shi BG, 2016, PROC CVPR IEEE, P4168, DOI 10.1109/CVPR.2016.452
   Srivastava N, 2015, PR MACH LEARN RES, V37, P843
   Su H., 2016, LICAI 16 P 25 INT JO, P3469
   Sun JY, 2019, MULTIMED TOOLS APPL, V78, P3633, DOI 10.1007/s11042-017-5244-2
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Xu K., 2015, COMPUTER SCI, P2048
   Xu WR, 2017, IEEE T MULTIMEDIA, V19, P1494, DOI 10.1109/TMM.2017.2674622
   Yi S, 2016, LECT NOTES COMPUT SC, V9905, P263, DOI 10.1007/978-3-319-46448-0_16
   Yi S, 2015, IEEE I CONF COMP VIS, P3137, DOI 10.1109/ICCV.2015.359
   Yi S, 2015, PROC CVPR IEEE, P3488, DOI 10.1109/CVPR.2015.7298971
   Zhang C, 2016, IEEE T MULTIMEDIA, V18, P1048, DOI 10.1109/TMM.2016.2542585
   Zhou BL, 2015, INT J COMPUT VISION, V111, P50, DOI 10.1007/s11263-014-0735-3
   Zhou BL, 2012, LECT NOTES COMPUT SC, V7573, P857, DOI 10.1007/978-3-642-33709-3_61
   Zhou BL, 2014, IEEE T PATTERN ANAL, V36, P1586, DOI 10.1109/TPAMI.2014.2300484
NR 41
TC 42
Z9 45
U1 0
U2 32
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2018
VL 20
IS 12
BP 3289
EP 3297
DI 10.1109/TMM.2018.2834873
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HA4EY
UT WOS:000450212600009
DA 2024-07-18
ER

PT J
AU Liang, NX
   Wu, GL
   Kang, WX
   Wang, ZY
   Feng, DD
AF Liang, Ningxin
   Wu, Guile
   Kang, Wenxiong
   Wang, Zhiyong
   Feng, David Dagan
TI Real-Time Long-Term Tracking With Prediction-Detection-Correction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visual tracking; correlation filter; long-term tracking; superpixel
   optical flow; dual SVMs
ID VISUAL TRACKING; OBJECT TRACKING
AB Real-time long-term visual tracking is one of the most challenging problems in computer vision due to various factors such as occlusion and motion ambiguity. To achieve robust long-term tracking, most state-of-the-art methods typically construct an online detector in each frame. However, they fail to achieve real-time performance due to high computational complexity. In this paper, we propose a novel real-time long-term tracking algorithm by exploiting a joint Prediction-Detection-Correction Tracking framework (PDCT). We utilize a superpixel optical flow to construct a predictor to estimate the target motion and internal scale variation. To locate the target at a finer level, we develop an improved kernelized correlation detector with an adaptive online learning rate and translation-scale parameters from the predictor. To refine the tracking result and redetect the target in the case of a tracking failure, we devise a corrector utilizing dual online SVMs with dense sampling and reliable history samples. The SVMs are trained with passive-aggressive learning and online retraining strategies. In addition, we employ a selection mechanism for the correlation responses to maintain reliable samples effectively. As a result, our proposed tracker is able to refine tracking results via the corrector and detector and maintains reliable tracking results for subsequent tracking. Extensive experiments on the widely used object tracking benchmark show that the proposed tracker is superior to state-of-the-art trackers in terms of both effectiveness and efficiency, and the integration of each component is effective under the PDCT framework.
C1 [Liang, Ningxin; Wu, Guile; Kang, Wenxiong] South China Univ Technol, Sch Automat Sci & Engn, Guangzhou 510641, Guangdong, Peoples R China.
   [Wang, Zhiyong; Feng, David Dagan] Univ Sydney, Sch Informat Technol, Sydney, NSW 2006, Australia.
C3 South China University of Technology; University of Sydney
RP Kang, WX (corresponding author), South China Univ Technol, Sch Automat Sci & Engn, Guangzhou 510641, Guangdong, Peoples R China.
EM aunxliang@mail.scut.edu.cn; guile.wu.cn@ieee.org; auwxkang@scut.edu.cn;
   zhiyong.wang@sydney.edu.au; dagan.feng@sydney.edu.au
RI liu, miao/KGL-7043-2024
OI Kang, Wenxiong/0000-0001-9023-7252; Feng, Dagan/0000-0002-3381-214X;
   Wang, Zhiyong/0000-0002-8043-0312
FU National Natural Science Foundation of China [61573151]; Guangdong
   Natural Science Foundation [2016A030313468]; Science and Technology
   Planning Project of Guangdong Province [2017A010101026]
FX This work was supported in part by the National Natural Science
   Foundation of China (No. 61573151), in part by the Guangdong Natural
   Science Foundation (No. 2016A030313468), and in part by the Science and
   Technology Planning Project of Guangdong Province (No. 2017A010101026).
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Avidan S, 2007, IEEE T PATTERN ANAL, V29, P261, DOI 10.1109/TPAMI.2007.35
   Babenko B, 2011, IEEE T PATTERN ANAL, V33, P1619, DOI 10.1109/TPAMI.2010.226
   Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414
   Danelljan M, 2017, IEEE T PATTERN ANAL, V39, P1561, DOI 10.1109/TPAMI.2016.2609928
   Danelljan M, 2014, PROC CVPR IEEE, P1090, DOI 10.1109/CVPR.2014.143
   Dong XP, 2017, IEEE T MULTIMEDIA, V19, P763, DOI 10.1109/TMM.2016.2631884
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Gao J, 2014, LECT NOTES COMPUT SC, V8691, P188, DOI 10.1007/978-3-319-10578-9_13
   Hare S, 2016, IEEE T PATTERN ANAL, V38, P2096, DOI 10.1109/TPAMI.2015.2509974
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Kwon J, 2010, PROC CVPR IEEE, P1269, DOI 10.1109/CVPR.2010.5539821
   Ma B, 2015, IEEE T MULTIMEDIA, V17, P1818, DOI 10.1109/TMM.2015.2463221
   Ma C, 2015, PROC CVPR IEEE, P5388, DOI 10.1109/CVPR.2015.7299177
   Nebehay G, 2015, PROC CVPR IEEE, P2784, DOI 10.1109/CVPR.2015.7298895
   Oron S., 2015, INT J COMPUT VISION, V111, P1940
   Santner J, 2010, PROC CVPR IEEE, P723, DOI 10.1109/CVPR.2010.5540145
   Smeulders AWM, 2014, IEEE T PATTERN ANAL, V36, P1442, DOI 10.1109/TPAMI.2013.230
   Supancic JS, 2013, PROC CVPR IEEE, P2379, DOI 10.1109/CVPR.2013.308
   Wang N, 2013, P ADV NEURAL INFORM
   Wu GL, 2017, PATTERN RECOGN, V68, P175, DOI 10.1016/j.patcog.2017.03.015
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Yang F, 2014, IEEE T IMAGE PROCESS, V23, P1639, DOI 10.1109/TIP.2014.2300823
   Yang HB, 2014, PROCEEDINGS OF THE 2014 INTERNATIONAL CONFERENCE ON MANAGEMENT SCIENCE AND MANAGEMENT INNOVATION, P172
   Yao R, 2017, IEEE T MULTIMEDIA, V19, P772, DOI 10.1109/TMM.2016.2631727
   Yuan Y, 2015, IEEE T MULTIMEDIA, V17, P1125, DOI 10.1109/TMM.2015.2440996
   Zhang KH, 2016, IEEE T IMAGE PROCESS, V25, P1779, DOI 10.1109/TIP.2016.2531283
   Zhang KH, 2014, LECT NOTES COMPUT SC, V8693, P127, DOI 10.1007/978-3-319-10602-1_9
   Zhong W, 2012, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2012.6247882
   Zhou Y, 2017, IEEE T MULTIMEDIA, V19, P1798, DOI 10.1109/TMM.2017.2689918
   Zhu G, 2016, PROC CVPR IEEE, P943, DOI 10.1109/CVPR.2016.108
NR 35
TC 33
Z9 33
U1 0
U2 37
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2018
VL 20
IS 9
BP 2289
EP 2302
DI 10.1109/TMM.2018.2803518
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GR1ZX
UT WOS:000442358200005
DA 2024-07-18
ER

PT J
AU Lin, YC
   Lin, YH
AF Lin, Yu-Chen
   Lin, Yuan-Hsiang
TI Step Count and Pulse Rate Detection Based on the Contactless Image
   Measurement Method
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Contactless measurement; motion analysis; step count; pulse rate
ID HEART-RATE; SENSOR; PPG
AB Contactless exercise monitoring is a new trend that makes people feel more comfortable and unconstrained. However, the decrease in accuracy of the pulse rate measurement caused by large motion artifacts is an urgent problem to be solved. In this paper, we proposed a novel approach to monitor step count and improve the accuracy of remote pulse rate measurement based on the image detection method. We designed a chrominance-based adaptive filter and normalization (CADN) method and a domain selection scheme (DSS) to enhance the accuracy of contactless pulse rate measurement during exercise. Various exercises such as biking, stepping, and treadmill running were conducted to evaluate motion robustness of the proposed CADN + DSS and the accuracy of step counts. The results reveal that the detection rates of the proposed step count method are 99.52% and 99.77% for stepping and treadmill exercise, respectively. The pulse rate accuracy is compared with two state-of-the-art algorithms-chrominance and chrominance-based adaptive filter. The results show the proposed CADN+DSS method provides a lower discrepancy between the detected pulse rate and a ground-truth device (Polar H7) for all activities. We expand the scope of contactless measurement for physical activity detection and develop an unfettered step count and pulse rate measurement method in exercise. Therefore, step count and pulse rate can be measured synchronously without relying on any contact sensors.
C1 [Lin, Yu-Chen; Lin, Yuan-Hsiang] Natl Taiwan Univ Sci & Technol Taiwan Tech, Dept Elect & Comp Engn, Taipei 106, Taiwan.
C3 National Taiwan University of Science & Technology
RP Lin, YH (corresponding author), Natl Taiwan Univ Sci & Technol Taiwan Tech, Dept Elect & Comp Engn, Taipei 106, Taiwan.
EM D10302014@mail.ntust.edu.tw; linyh@mail.ntust.edu.tw
FU Ministry of Science and Technology, Taiwan [MOST 105-2221-E-011-150,
   MOST 106-3011-E-011-001]
FX This work was supported by the Ministry of Science and Technology,
   Taiwan for academic research under Grant MOST 105-2221-E-011-150 and
   Grant MOST 106-3011-E-011-001. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr. Jun
   Wu.
CR Cennini G, 2010, OPT EXPRESS, V18, P4867, DOI 10.1364/OE.18.004867
   CROWE JA, 1992, P IEEE EMBS, V14, P2423
   de Haan G, 2014, PHYSIOL MEAS, V35, P1913, DOI 10.1088/0967-3334/35/9/1913
   de Haan G, 2013, IEEE T BIO-MED ENG, V60, P2878, DOI 10.1109/TBME.2013.2266196
   Feng LT, 2015, IEEE T CIRC SYST VID, V25, P879, DOI 10.1109/TCSVT.2014.2364415
   Glasgow RE, 2001, AM J PREV MED, V21, P189, DOI 10.1016/S0749-3797(01)00350-6
   Huang RX, 2015, CLIM DYNAM, V45, P3563, DOI 10.1007/s00382-015-2557-6
   Jang D.-G., 2014, International Journal of Electronics and Electrical Engineering, P45, DOI [10.12720/ijeee.2.1.45-49, DOI 10.12720/IJEEE.2.1.45-49]
   Lewandowska M., 2011, 2011 Federated Conference on Computer Science and Information Systems (FedCSIS), P405
   Libby R., 2008, A simple method for reliable footstep detection in embedded sensor platforms
   Lin YH, 2011, COMM COM INF SC, V223, P326
   Lucas B., 1981, P INT JOINT C ART IN, P674, DOI DOI 10.1364/J0SAA.19.002142
   Mao QR, 2017, IEEE T MULTIMEDIA, V19, P861, DOI 10.1109/TMM.2016.2629282
   Matsuda T, 2008, IEEE ENG MED BIO, P1315, DOI 10.1109/IEMBS.2008.4649406
   Ozcan K., 2015, Proceedings of the 9th International Conference on Distributed Smart Cameras, P164, DOI [DOI 10.1145/2789116, 10.1145/2789116.2789120, DOI 10.1145/2789116.2789120]
   Poh MZ, 2010, OPT EXPRESS, V18, P10762, DOI 10.1364/OE.18.010762
   Ryu UkJae., 2013, INFORM SCI APPL ICIS, P1
   SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794
   Takano C, 2007, MED ENG PHYS, V29, P853, DOI 10.1016/j.medengphy.2006.09.006
   Wang WJ, 2017, IEEE T BIO-MED ENG, V64, P1479, DOI 10.1109/TBME.2016.2609282
   Wang WJ, 2015, IEEE T BIO-MED ENG, V62, P415, DOI 10.1109/TBME.2014.2356291
   Wieringa FP, 2005, ANN BIOMED ENG, V33, P1034, DOI 10.1007/s10439-005-5763-2
   Xu WR, 2017, IEEE T MULTIMEDIA, V19, P1494, DOI 10.1109/TMM.2017.2674622
   Yang C, 2017, IEEE T MULTIMEDIA, V19, P1625, DOI 10.1109/TMM.2017.2672198
   Ying-Dar Lin, 2015, 2015 49th Annual International Carnahan Conference on Security Technology (ICCST), P1, DOI 10.1109/CCST.2015.7389677
   Yu G, 2011, IEEE T MULTIMEDIA, V13, P507, DOI 10.1109/TMM.2011.2128301
   Zhi L, 2009, 2009 WRI INTERNATIONAL CONFERENCE ON COMMUNICATIONS AND MOBILE COMPUTING: CMC 2009, VOL 3, P132, DOI 10.1109/CMC.2009.68
   Zhou Z, 2015, IEEE T MULTIMEDIA, V17, P512, DOI 10.1109/TMM.2015.2404779
NR 28
TC 9
Z9 10
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2018
VL 20
IS 8
BP 2223
EP 2231
DI 10.1109/TMM.2018.2790172
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GN8BR
UT WOS:000439378600024
DA 2024-07-18
ER

PT J
AU Qiu, ZF
   Yao, T
   Mei, T
AF Qiu, Zhaofan
   Yao, Ting
   Mei, Tao
TI Learning Deep Spatio-Temporal Dependence for Semantic Video Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantic segmentation; fully convolutional networks; long-short term
   memory; video segmentation
ID RECOGNITION; SCENES
AB Semantically labeling every pixel in a video is a very challenging task as video is an information-intensive media with complex spatio-temporal dependence. We present in this paper a novel deep convolutional network architecture, called deep spatio-temporal fully convolutional networks (DST-FCN), which leverages both spatial and temporal dependencies among pixels and voxels by training them in an end-to-end manner. Specifically, we introduce a two-stream network by learning the deep spatio-temporal dependence, in which a 2D FCN followed by the convolutional long short-term memory (ConvLSTM) is employed on the pixel level and a 3-D FCN is exploited on the voxel level. Our model differs from conventional FCN in that it not only extends FCN by adding ConvLSTM on the pixel level for exploring long-term dependence, but also proposes 3-D FCN to enable voxel level prediction. On two benchmarks of A2D and CamVid, our DST-FCN achieves superior results to state-of-the-art techniques. More remarkably, we obtain to-date the best reported results: 45.0% per-label accuracy on A2D and 68.8% mean IoU on CamVid.
C1 [Qiu, Zhaofan; Mei, Tao] Univ Sci & Technol China, Hefei 230000, Anhui, Peoples R China.
   [Yao, Ting; Mei, Tao] Microsoft Res Asia, Beijing 100080, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Microsoft Research Asia; Microsoft
RP Mei, T (corresponding author), Univ Sci & Technol China, Hefei 230000, Anhui, Peoples R China.
EM zhaofanqiu@gmail.com; tiyao@microsoft.com; tmei@microsoft.com
RI Mei, Tao/GQZ-0596-2022
OI Mei, Tao/0000-0002-5990-7307
CR [Anonymous], 2015, ARXIV151100561
   [Anonymous], 2016, PROC INT C LEARN RE
   [Anonymous], P INT C LEARN REPR W
   [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.348
   [Anonymous], 2014, P INT C LEARN REPR
   [Anonymous], 2017, P IEEE C COMP VIS PA
   [Anonymous], 2016, CORR
   Brostow GJ, 2008, LECT NOTES COMPUT SC, V5302, P44, DOI 10.1007/978-3-540-88682-2_5
   Chen A.Y.C., 2011, IEEE Workshop on Applications of Computer Vision, P614
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Geng J, 2015, IEEE T MULTIMEDIA, V17, P498, DOI 10.1109/TMM.2015.2398195
   Ghiasi G, 2016, LECT NOTES COMPUT SC, V9907, P519, DOI 10.1007/978-3-319-46487-9_32
   Grundmann M, 2010, PROC CVPR IEEE, P2141, DOI 10.1109/CVPR.2010.5539893
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jiang HQ, 2015, IEEE T MULTIMEDIA, V17, P3, DOI 10.1109/TMM.2014.2368273
   Jiang YG, 2015, IEEE T MULTIMEDIA, V17, P1174, DOI 10.1109/TMM.2015.2436813
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kundu A, 2016, PROC CVPR IEEE, P3168, DOI 10.1109/CVPR.2016.345
   Li Q, 2017, INT J MULTIMED INF R, V6, P85, DOI 10.1007/s13735-016-0117-4
   Li Q, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P159, DOI 10.1145/2911996.2912001
   Liang-Chieh Chen, 2015, INT C LEARN REPR
   Liu BY, 2016, LECT NOTES COMPUT SC, V9910, P650, DOI 10.1007/978-3-319-46466-4_39
   Liu BY, 2015, PROC CVPR IEEE, P4286, DOI 10.1109/CVPR.2015.7299057
   Liu S, 2015, IEEE T MULTIMEDIA, V17, P1347, DOI 10.1109/TMM.2015.2443559
   Liu ZW, 2015, IEEE I CONF COMP VIS, P1377, DOI 10.1109/ICCV.2015.162
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178
   Pinheiro PO, 2014, PR MACH LEARN RES, V32
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Raza SH, 2013, PROC CVPR IEEE, P3081, DOI 10.1109/CVPR.2013.396
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shi X., 2015, ADV NEURAL INFORM PR, DOI DOI 10.48550/ARXIV.1506.04214
   Song JK, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P831, DOI 10.1145/2964284.2964295
   Sturgess P., 2009, P BRIT MACH VIS C
   Tighe J, 2013, INT J COMPUT VISION, V101, P329, DOI 10.1007/s11263-012-0574-z
   Wang F, 2014, IEEE T MULTIMEDIA, V16, P1303, DOI 10.1109/TMM.2014.2315780
   Xu CL, 2016, PROC CVPR IEEE, P3083, DOI 10.1109/CVPR.2016.336
   Xu CL, 2015, PROC CVPR IEEE, P2264, DOI 10.1109/CVPR.2015.7298839
   Xu CL, 2012, LECT NOTES COMPUT SC, V7577, P626, DOI 10.1007/978-3-642-33783-3_45
   Zhang CX, 2010, LECT NOTES COMPUT SC, V6314, P708, DOI 10.1007/978-3-642-15561-1_51
   Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179
NR 42
TC 55
Z9 59
U1 1
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2018
VL 20
IS 4
BP 939
EP 949
DI 10.1109/TMM.2017.2759504
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FZ5HM
UT WOS:000427623000014
DA 2024-07-18
ER

PT J
AU Somandepalli, K
   Kumar, N
   Guha, T
   Narayanan, SS
AF Somandepalli, Krishna
   Kumar, Naveen
   Guha, Tanaya
   Narayanan, Shrikanth S.
TI Unsupervised Discovery of Character Dictionaries in Animation Movies
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Animation movies; deep neural networks; object tracking; saliency;
   unsupervised clustering; video diarization
AB Automatic content analysis of animation movies can enable an objective understanding of character (actor) representations and their portrayals. It can also help illuminate potential markers of unconscious biases and their impact. However, multimedia analysis of movie content has predominantly focused on live-action features. A dearth of multimedia research in this field is because of the complexity and heterogeneity in the design of animated characters-an extremely challenging problem to be generalized by a single method or model. In this paper, we address the problem of automatically discovering characters in animation movies as a first step toward automatic character labeling in these media. Movie-specific character dictionaries can act as a powerful first step for subsequent content analysis at scale. We propose an unsupervised approach which requires no prior information about the characters in a movie. We first use a deep neural network-based object detector that is trained on natural images to identify a set of initial character candidates. These candidates are further pruned using saliency constraints and visual object tracking. A character dictionary per movie is then generated from exemplars obtained by clustering these candidates. We are able to identify both anthropomorphic and nonanthropomorphic characters in a dataset of 46 animation movies with varying composition and character design. Our results indicate high precision and recall of the automatically detected characters compared to human-annotated ground truth, demonstrating the generalizability of our approach.
C1 [Somandepalli, Krishna; Narayanan, Shrikanth S.] Univ Southern Calif, Dept Elect Engn, Signal & Image Proc Inst, Los Angeles, CA 90089 USA.
   [Kumar, Naveen] Sony Comp Entertainment Amer, San Mateo, CA 94404 USA.
   [Guha, Tanaya] Indian Inst Technol, Dept Elect Engn, Kanpur, Uttar Pradesh, India.
C3 University of Southern California; Indian Institute of Technology System
   (IIT System); Indian Institute of Technology (IIT) - Kanpur
RP Somandepalli, K (corresponding author), Univ Southern Calif, Dept Elect Engn, Signal & Image Proc Inst, Los Angeles, CA 90089 USA.
EM somandep@usc.edu; knaveen87@gmail.com; tanayaguha@gmail.com;
   shri@sipi.usc.edu
RI Kumar, Praveen/KHC-7586-2024; Somandepalli, Krishna/AAB-6789-2020;
   Narayanan, Shrikanth S/D-5676-2012; Kumar, Naveen/GZK-9881-2022
OI Somandepalli, Krishna/0000-0002-2845-1079; Kumar,
   Naveen/0000-0003-2529-8344; Guha, Tanaya/0000-0003-2167-4891
FU National Science Foundation
FX This work is based upon work supported by the National Science
   Foundation.
CR Aghbari Z, 2003, IEEE T MULTIMEDIA, V5, P516, DOI 10.1109/TMM.2003.819092
   [Anonymous], J IMAGE VIDEO PROCES
   [Anonymous], COMPREHENSIVE ANNENB
   [Anonymous], P IEEE INT C AC SPEE
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2012, P BRIT MACH VIS C, DOI DOI 10.5244/C.26.4
   [Anonymous], BOX OFFICE HIST DIGI
   [Anonymous], ILLUSION LIFE DISNEY
   [Anonymous], 2006, P 14 ACM INT C MULT
   [Anonymous], P 5 PAC RIM C MULT A
   Chen BW, 2009, IEEE T MULTIMEDIA, V11, P295, DOI 10.1109/TMM.2008.2009703
   Christopher D.M., 2008, An Introduction To Information Retrieval, V151, P177
   Dueck D, 2007, IEEE I CONF COMP VIS, P198
   Erhan D, 2014, PROC CVPR IEEE, P2155, DOI 10.1109/CVPR.2014.276
   Everingham M, 2009, IMAGE VISION COMPUT, V27, P545, DOI 10.1016/j.imavis.2008.04.018
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Fitzgibbon A, 2002, LECT NOTES COMPUT SC, V2352, P304
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Guha T, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P31, DOI 10.1145/2818346.2820778
   Jaccard P., 1912, New Phytologist, V11, P37, DOI [10.1111/j.1469-8137.1912.tb05611.x, DOI 10.1111/J.1469-8137.1912.TB05611.X]
   Jain AK, 1999, ACM COMPUT SURV, V31, P264, DOI 10.1145/331499.331504
   Krippendorff K, 2004, HUM COMMUN RES, V30, P411, DOI 10.1093/hcr/30.3.411
   Kristan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P564, DOI 10.1109/ICCVW.2015.79
   Kurzhals K, 2016, IEEE T MULTIMEDIA, V18, P2149, DOI 10.1109/TMM.2016.2614184
   Lee KH, 2015, IEEE T MULTIMEDIA, V17, P1429, DOI 10.1109/TMM.2015.2455418
   Leutenegger S, 2011, IEEE I CONF COMP VIS, P2548, DOI 10.1109/ICCV.2011.6126542
   Li Y, 2006, IEEE SIGNAL PROC MAG, V23, P79
   Liu CL, 2013, IEEE T MULTIMEDIA, V15, P884, DOI 10.1109/TMM.2013.2238522
   Mahadevan Vijay., 2008, Computer Vision and Pattern Recognition, IEEE Computer Society Conference on, P1, DOI 10.1109/CVPR.2008.4587576
   Merler M, 2012, IEEE T MULTIMEDIA, V14, P88, DOI 10.1109/TMM.2011.2168948
   Mohanta PP, 2012, IEEE T MULTIMEDIA, V14, P223, DOI 10.1109/TMM.2011.2170963
   Nebehay G, 2015, PROC CVPR IEEE, P2784, DOI 10.1109/CVPR.2015.7298895
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Ott L, 2007, 14TH INTERNATIONAL CONFERENCE ON IMAGE ANALYSIS AND PROCESSING WORKSHOPS, PROCEEDINGS, P206, DOI 10.1109/ICIAPW.2007.12
   Rahtu Esa, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P1137, DOI 10.1109/ICCVW.2009.5457577
   Rahtu E, 2010, LECT NOTES COMPUT SC, V6315, P366, DOI 10.1007/978-3-642-15555-0_27
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sivic J, 2009, PROC CVPR IEEE, P1145, DOI 10.1109/CVPRW.2009.5206513
   Smith S.L., 2016, Media, Diversity, Social Change Initiative
   Szegedy C., 2015, Scalable, High-Quality Object Detection, March
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Takayama K., 2012, P IM EL VIS COMP WOR, P48
   Tang YX, 2017, IEEE T MULTIMEDIA, V19, P393, DOI 10.1109/TMM.2016.2614862
   Vallet F, 2013, IEEE T MULTIMEDIA, V15, P509, DOI 10.1109/TMM.2012.2233724
   Wang S, 2015, IEEE T MULTIMEDIA, V17, P1045, DOI 10.1109/TMM.2015.2431492
   Weng CY, 2009, IEEE T MULTIMEDIA, V11, P256, DOI 10.1109/TMM.2008.2009684
NR 48
TC 14
Z9 17
U1 2
U2 33
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2018
VL 20
IS 3
BP 539
EP 551
DI 10.1109/TMM.2017.2745712
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FW5ZH
UT WOS:000425397500003
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Zhao, XC
   Lin, YP
   Heikkila, J
AF Zhao, Xiaochao
   Lin, Yaping
   Heikkila, Janne
TI Dynamic Texture Recognition Using Volume Local Binary Count Patterns
   With an Application to 2D Face Spoofing Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dynamic texture; spatio-temporal descriptor; volume local binary count;
   2D face spoofing detection
ID IMAGE; VIDEO; CLASSIFICATION; REPRESENTATION; SCALE
AB In this paper, a local spatiotemporal descriptor, namely, the volume local binary count (VLBC), is proposed for the representation and recognition of dynamic texture. This descriptor, which is similar in spirit to the volume local binary pattern (VLBP), extracts histograms of thresholded local spatiotemporal volumes using both appearance and motion features to describe dynamic texture. Unlike VLBP using binary encoding, VLBC does not exploit the local structure information and only counts the number of is in the thresholded codes. Thus, VLBC can include more neighboring pixels without exponentially increasing the feature dimension as VLBP does. Furthermore, a completed version of VLBC (CVLBC) is also proposed to enhance the performance of dynamic texture recognition with additional information about local contrast and central pixel intensities. The proposed method is not only efficient to compute but also effective for dynamic texture representation. In experiments with three dynamic texture databases, namely, UCLA, DynTex, and DynTex++, the proposed method produces classification rates that are comparable to those produced by the state-of-the-art approaches. In addition to dynamic texture recognition, we propose utilizing CVLBC for 2-D face spoofing detection. As an effective spatiotemporal descriptor, CVLBC can well describe the differences between facial videos of valid users and impostors, thus achieving good performance for face spoofing detection. For comparison with other methods, the proposed method is evaluated on three face antispoofing databases: Print-Attack, Replay-Attack, and CAS Face Antispoofing. The experimental results demonstrate the effectiveness of CVLBC for 2-D face spoofing detection.
C1 [Zhao, Xiaochao; Lin, Yaping] Hunan Univ, Coll Comp Sci & Elect Engn, Changsha 410082, Hunan, Peoples R China.
   [Zhao, Xiaochao; Heikkila, Janne] Univ Oulu, Ctr Machine Vis & Signal Anal, Oulu 90014, Finland.
C3 Hunan University; University of Oulu
RP Lin, YP (corresponding author), Hunan Univ, Coll Comp Sci & Elect Engn, Changsha 410082, Hunan, Peoples R China.
EM s12103017@hnu.edu.cn; yplin@hnu.edu.cn; jth@ee.oulu.fi
FU Research Foundation of Chinese Ministry of Education [MCM20122061];
   National Natural Science Foundation of China [61472125]; China Mobile
   Communications Corporation [MCM20122061]; Academy of Finland [297732];
   China Scholarship Council; Academy of Finland (AKA) [297732] Funding
   Source: Academy of Finland (AKA)
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61472125, in part by the Research
   Foundation of Chinese Ministry of Education and China Mobile
   Communications Corporation under Grant MCM20122061, in part by the
   Academy of Finland under Grant 297732, and in part by the scholarship
   from China Scholarship Council.
CR Ali W, 2008, IEEE INT VEH SYM, P1144
   Anjos Andre, 2011, P INT JOINT C BIOM I, P1, DOI DOI 10.1109/IJCB.2011.6117503
   [Anonymous], PROC 9 EUR CONF
   [Anonymous], 2011, 2011 INT JOINT C BIO
   [Anonymous], 1995, Studies in Optics
   [Anonymous], 2012, ARXIV12013612
   [Anonymous], PROC 11 EUR CONF
   [Anonymous], 2013, 2013 INT C BIOMETRIC, DOI DOI 10.1109/ICB.2013.6612968
   [Anonymous], P 6 ACM SIGMM INT WO
   [Anonymous], 2005, WACV MOTION
   [Anonymous], 2008, IEEE C COMP VIS PATT
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI DOI 10.1145/1961189.1961199
   Arashloo SR, 2017, J VIS COMMUN IMAGE R, V43, P89, DOI 10.1016/j.jvcir.2016.12.015
   Arashloo SR, 2015, IEEE T INF FOREN SEC, V10, P2396, DOI 10.1109/TIFS.2015.2458700
   Arashloo SR, 2014, IEEE T MULTIMEDIA, V16, P2099, DOI 10.1109/TMM.2014.2362855
   Baktashmotlagh M, 2014, IEEE T PATTERN ANAL, V36, P2353, DOI 10.1109/TPAMI.2014.2339851
   Bar-Joseph Z, 2001, IEEE T VIS COMPUT GR, V7, P120, DOI 10.1109/2945.928165
   Bharadwaj S, 2013, IEEE COMPUT SOC CONF, P105, DOI 10.1109/CVPRW.2013.23
   Boulkenafet Z, 2016, IEEE T INF FOREN SEC, V11, P1818, DOI 10.1109/TIFS.2016.2555286
   Chan A. B., 2007, PROC IEEE C COMPUT V, P1
   Chan AB, 2005, 2005 IEEE INTELLIGENT VEHICLES SYMPOSIUM PROCEEDINGS, P771
   Chetverikov D, 2005, ADV SOFT COMP, P17
   Chingovska I, 2013, INT CONF BIOMETR
   Chingovska I, 2015, IEEE T INF FOREN SEC, V10, P787, DOI 10.1109/TIFS.2015.2400392
   Chingovska Ivana, 2012, BIOSIG
   Cimpoi M, 2015, PROC CVPR IEEE, P3828, DOI 10.1109/CVPR.2015.7299007
   Cimpoi M, 2014, PROC CVPR IEEE, P3606, DOI 10.1109/CVPR.2014.461
   de Freitas Pereira Tiago, 2013, Computer Vision - ACCV 2012 Workshops. ACCV 2012 International Workshops. Revised Selected Papers, P121, DOI 10.1007/978-3-642-37410-4_11
   Derpanis KG, 2012, IEEE T PATTERN ANAL, V34, P1193, DOI 10.1109/TPAMI.2011.221
   Derpanis KG, 2010, PROC CVPR IEEE, P191, DOI 10.1109/CVPR.2010.5540213
   Dollar P., 2005, Proceedings. 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS) (IEEE Cat. No. 05EX1178), P65
   Doretto G, 2003, INT J COMPUT VISION, V51, P91, DOI 10.1023/A:1021669406132
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Fitzgibbon AW, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P662, DOI 10.1109/ICCV.2001.937584
   Galbally J, 2014, INT C PATT RECOG, P1173, DOI 10.1109/ICPR.2014.211
   Gatys L., 2015, NIPS
   Ghanem B., 2010, LECT NOTES COMPUT SC, P223
   Hossain S, 2013, PATTERN RECOGN LETT, V34, P2007, DOI 10.1016/j.patrec.2013.02.009
   Ji H, 2013, IEEE T IMAGE PROCESS, V22, P286, DOI 10.1109/TIP.2012.2214040
   Jing Huang, 2010, 2010 Proceedings of 3rd International Congress on Image and Signal Processing (CISP 2010), P1887, DOI 10.1109/CISP.2010.5647609
   Li JW, 2004, P SOC PHOTO-OPT INS, V5404, P296, DOI 10.1117/12.541955
   Liao SC, 2010, PROC CVPR IEEE, P1301, DOI 10.1109/CVPR.2010.5539817
   Maatta J., 2011, 2011 INT JOINT C BIO, P1
   Menotti David, 2015, IEEE Transactions on Information Forensics and Security, V10, P864, DOI 10.1109/TIFS.2015.2398817
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Pan G, 2007, IEEE I CONF COMP VIS, P1879, DOI 10.1109/ICCV.2007.4409068
   Pan G, 2011, TELECOMMUN SYST, V47, P215, DOI 10.1007/s11235-010-9313-3
   Pereira TD, 2014, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2014-2
   Péteri R, 2005, LECT NOTES COMPUT SC, V3523, P223
   Qi XB, 2016, NEUROCOMPUTING, V171, P1230, DOI 10.1016/j.neucom.2015.07.071
   Quan YH, 2016, PROC CVPR IEEE, P308, DOI 10.1109/CVPR.2016.40
   Quan YH, 2015, IEEE I CONF COMP VIS, P73, DOI 10.1109/ICCV.2015.17
   Phan QT, 2016, IEEE IMAGE PROC, P404, DOI 10.1109/ICIP.2016.7532388
   Rivera AR, 2015, IEEE T PATTERN ANAL, V37, P2146, DOI 10.1109/TPAMI.2015.2392774
   Ravichandran A, 2009, PROC CVPR IEEE, P1651, DOI 10.1109/CVPRW.2009.5206847
   Ren JF, 2014, IEEE SIGNAL PROC LET, V21, P1346, DOI 10.1109/LSP.2014.2336252
   Ren JF, 2013, INT CONF ACOUST SPEE, P2400, DOI 10.1109/ICASSP.2013.6638085
   Saisan P, 2001, PROC CVPR IEEE, P58
   Sizintsev M, 2009, PROC CVPR IEEE, P493, DOI 10.1109/CVPRW.2009.5206728
   Soatto S, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P439, DOI 10.1109/ICCV.2001.937658
   Sungeun Hong, 2018, Multidimensional Systems and Signal Processing, V29, P279, DOI 10.1007/s11045-016-0463-7
   Szummer M, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL III, P823, DOI 10.1109/ICIP.1996.560871
   Tan XY, 2010, LECT NOTES COMPUT SC, V6316, P504, DOI 10.1007/978-3-642-15567-3_37
   Taylor GW, 2010, LECT NOTES COMPUT SC, V6316, P140, DOI 10.1007/978-3-642-15567-3_11
   Tirunagari S, 2015, IEEE T INF FOREN SEC, V10, P762, DOI 10.1109/TIFS.2015.2406533
   Tivive F. H. C., 2006, PROC IEEE REGION 10, P1
   Tiwari D, 2016, COMPUT VIS IMAGE UND, V150, P58, DOI 10.1016/j.cviu.2016.04.010
   Tiwari D, 2016, MULTIDIM SYST SIGN P, V27, P563, DOI 10.1007/s11045-015-0319-6
   Töreyin BU, 2006, PATTERN RECOGN LETT, V27, P49, DOI 10.1016/j.patrec.2005.06.015
   Ulyanov D., 2016, P 33 INT C INT C MAC, V48, P1349
   Wang LC, 2014, IEEE T MULTIMEDIA, V16, P751, DOI 10.1109/TMM.2014.2298382
   Wang XG, 2009, PROC CVPR IEEE, P142, DOI 10.1109/CVPRW.2009.5206736
   Wang Y, 2015, NEUROCOMPUTING, V154, P217, DOI 10.1016/j.neucom.2014.12.001
   Wang YZ, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P213, DOI 10.1109/ICCV.2003.1238343
   Wen D, 2015, IEEE T INF FOREN SEC, V10, P746, DOI 10.1109/TIFS.2015.2400395
   Wu PP, 2016, IEEE T MULTIMEDIA, V18, P326, DOI 10.1109/TMM.2016.2520091
   Xu Y, 2012, COMPUT VIS IMAGE UND, V116, P999, DOI 10.1016/j.cviu.2012.05.003
   Xu Y, 2011, IEEE I CONF COMP VIS, P1219, DOI 10.1109/ICCV.2011.6126372
   Yan X, 2014, LECT NOTES COMPUT SC, V8692, P215, DOI 10.1007/978-3-319-10593-2_15
   Yan Y, 2016, IEEE T MULTIMEDIA, V18, P2494, DOI 10.1109/TMM.2016.2602938
   Yang JW, 2015, IEEE T INF FOREN SEC, V10, P797, DOI 10.1109/TIFS.2015.2403306
   Yang Y, 2008, IEEE T MULTIMEDIA, V10, P437, DOI 10.1109/TMM.2008.917359
   Yang Y, 2010, IEEE T IMAGE PROCESS, V19, P2761, DOI 10.1109/TIP.2010.2049235
   Ye W, 2015, FIRE SAFETY J, V73, P91, DOI 10.1016/j.firesaf.2015.03.001
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
   Zhao GY, 2009, IEEE T MULTIMEDIA, V11, P1254, DOI 10.1109/TMM.2009.2030637
   Zhao Y, 2012, IEEE T IMAGE PROCESS, V21, P4492, DOI 10.1109/TIP.2012.2204271
   Zhiwei Zhang, 2012, 2012 5th IAPR International Conference on Biometrics (ICB), P26, DOI 10.1109/ICB.2012.6199754
NR 88
TC 75
Z9 78
U1 0
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2018
VL 20
IS 3
BP 552
EP 566
DI 10.1109/TMM.2017.2750415
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FW5ZH
UT WOS:000425397500004
DA 2024-07-18
ER

PT J
AU Ribeiro, FML
   de Oliveira, JFL
   Ciancio, AG
   da Silva, EAB
   Estrada, CRD
   Tavares, LGC
   Gois, JN
   Said, A
   Martelotte, MC
AF Ribeiro, Felipe M. L.
   de Oliveira, Jose F. L.
   Ciancio, Alexandre G.
   da Silva, Eduardo A. B.
   Estrada, Cassius R. D.
   Tavares, Luiz G. C.
   Gois, Jonathan N.
   Said, Amir
   Martelotte, Marcela C.
TI Quality of Experience in a Stereoscopic Multiview Environment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multiview; motion parallax; quality of experience (QoE); stereoscopic
   image; subjective evaluation; visual perception
ID MOTION-PARALLAX; DISPLAYS; SMOOTHNESS; IMAGES
AB In this paper, we investigate how visualization factors, such as disparity, mobility, angular resolution, and viewpoint interpolation, influence the quality of experience (QoE) in a stereoscopic multiview environment. In order to do so, we set up a dedicated testing room and conducted subjective experiments. We also developed a framework that emulates a supermultiview environment. This framework can be used to investigate and assess the effects of angular resolution and viewpoint interpolation on the QoE produced by multiview systems, and provide relevant cues as to how the baselines of cameras and interpolation strategies in such systems affect user experience. Aspects such as visual comfort, model fluidity, sense of immersion, and the three-dimensional (3D) experience as a whole have been assessed for several test cases. Obtained results suggest that user experience in an motion parallax environment is not as critically influenced by configuration parameters such as disparity as initially thought. In addition, extensive subjective tests have indicated that while users are very sensitive to angular resolution in multiview 3D systems, this sensitivity tends not to be as critical when a user is performing a task that involves a great amount of interaction with the multiview content. These tests have also indicated that interpolating intermediate viewpoints can be effective in reducing the required view density without degrading the perceived QoE.
C1 [Ribeiro, Felipe M. L.; de Oliveira, Jose F. L.; Ciancio, Alexandre G.; da Silva, Eduardo A. B.; Estrada, Cassius R. D.; Tavares, Luiz G. C.; Gois, Jonathan N.; Martelotte, Marcela C.] Univ Fed Rio de Janeiro, Program Elect Engn, BR-21941972 Rio De Janeiro, Brazil.
   [Said, Amir] Qualcomm Technol Inc, San Diego, CA 92121 USA.
C3 Universidade Federal do Rio de Janeiro; Qualcomm
RP Ribeiro, FML (corresponding author), Univ Fed Rio de Janeiro, Program Elect Engn, BR-21941972 Rio De Janeiro, Brazil.
EM felipe.ribeiro@smt.ufrj.br; jose.oliveira@smt.ufrj.br;
   alexandre.ciancio@smt.ufrj.br; eduardo@smt.ufrj.br;
   cassius.estrada@smt.ufrj.br; luiz.tavares@smt.ufrj.br;
   jonathan.gois@smt.ufrj.br; said@ieee.org; marcela.cohen@smt.ufrj.br
RI SILVA, EDUARDO/IQS-1403-2023; da Silva, Eduardo A B/M-8012-2018;
   Oliveira, José Fernando Leite de/K-9330-2014; Nogueira Gois,
   Jonathan/ABB-2030-2021
OI da Silva, Eduardo A B/0000-0001-7755-6988; Nogueira Gois,
   Jonathan/0000-0002-2859-7668; Cohen Martelotte,
   Marcela/0000-0002-1580-8948
CR Alioscopy, 2013, AL AUT 3D SCREENS
   [Anonymous], 2016, MPEG 115 M GEN SWITZ
   [Anonymous], 2016, 116 MPEG M CHENGD CH
   [Anonymous], 2010, P INT WORKSH VID PRO
   Boev A., 2009, P SOC PHOTO-OPT INS, V7237
   Bosc E, 2011, IEEE J-STSP, V5, P1332, DOI 10.1109/JSTSP.2011.2166245
   CAUDEK C, 1993, J EXP PSYCHOL HUMAN, V19, P32, DOI 10.1037/0096-1523.19.1.32
   Chen Wei, 2012, 26 AAAI C ART INT, P592
   Ciancio A, 2013, IEEE INT SYMP CIRC S, P9, DOI 10.1109/ISCAS.2013.6571769
   Daly SJ, 2011, IEEE T BROADCAST, V57, P347, DOI 10.1109/TBC.2011.2127630
   De Silva V, 2011, IEEE T MULTIMEDIA, V13, P498, DOI 10.1109/TMM.2011.2129500
   Ebrahimi T, 2016, IEEE MULTIMEDIA, V23, P14, DOI 10.1109/MMUL.2016.64
   Everitt B.S., 2002, CAMBRIDGE DICT STAT
   Experimental Framework for FTV ISO/IEC SC29WG11 Output doc, 2014, 110 MPEG M STRASB FR
   Farid MS, 2015, IEEE IMAGE PROC, P3720, DOI 10.1109/ICIP.2015.7351499
   Fehn C., P SPIE, V5291, P93
   Gutierrez Jesus, 2015, Journal of Display Technology, V11, P967, DOI 10.1109/JDT.2015.2448758
   HARDY LH, 1945, J OPT SOC AM, V35, P268, DOI 10.1364/JOSA.35.000268
   Janowski L, 2015, IEEE T MULTIMEDIA, V17, P2210, DOI 10.1109/TMM.2015.2484963
   JPEG Pleno Call for Proposals on Light Field Coding, 2016, 73 M CHENGD CHIN OCT
   Kang J, 2015, WIRELESS PERS COMMUN, V84, P1133, DOI 10.1007/s11277-015-2680-z
   Kim T, 2014, IEEE T MULTIMEDIA, V16, P387, DOI 10.1109/TMM.2013.2292592
   Montgomery D.C., 1991, DESIGN ANAL EXPT
   Moorthy A. K., 2013, P SOC PHOTO-OPT INS, V8651
   OpenSceneGraph, 2013, OP SOURC HIGH PERF 3
   Pastoor S., 1989, Proceedings of the S.I.D., V30, P217
   Pereira F., PLENOPTIC I IN PRESS
   Pharr M., 2004, Physically Based Rendering: From Theory to Implementation
   Qi F, 2015, IEEE T MULTIMEDIA, V17, P2338, DOI 10.1109/TMM.2015.2493781
   Reichelt S, 2010, PROC SPIE, V7690, DOI 10.1117/12.850094
   Ribeiro F. M. L., 2014, P VID PROC QUAL METR
   Runde D, 2000, IEEE T CIRC SYST VID, V10, P376, DOI 10.1109/76.836282
   Speranza F, 2005, P SOC PHOTO-OPT INS, V5664, P72, DOI 10.1117/12.587170
   Takaki Y., 2014, ITE Transactions on Media Technology and Applications, V2, P8
   Takaki Y, 2012, OPT EXPRESS, V20, P27180, DOI 10.1364/OE.20.027180
   Tanimoto M., 2010, FREE VIEWPOINT TELEV, P53
   Tourancheau S, 2012, PROC SPIE, V8288, DOI 10.1117/12.907962
   Wang J. L., 2016, THESIS
   Winkler S, 2013, SIGNAL PROCESS-IMAGE, V28, P1358, DOI 10.1016/j.image.2013.07.008
   Xing LY, 2012, IEEE T MULTIMEDIA, V14, P326, DOI 10.1109/TMM.2011.2172402
   Yano S, 2002, DISPLAYS, V23, P191, DOI 10.1016/S0141-9382(02)00038-0
   Zhang C., 2009, Proc. IEEE International Workshop on Multimedia Signal Processing, P1
   Zhang L, 2005, IEEE T BROADCAST, V51, P191, DOI 10.1109/TBC.2005.846190
   Zhou WJ, 2016, IEEE T MULTIMEDIA, V18, P1077, DOI 10.1109/TMM.2016.2542580
   Zwicker Matthias., 2006, P 17 EUROGRAPHICS C, P73
NR 45
TC 10
Z9 10
U1 1
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2018
VL 20
IS 1
BP 1
EP 14
DI 10.1109/TMM.2017.2714425
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FQ3WS
UT WOS:000418289700001
DA 2024-07-18
ER

PT J
AU Zhang, C
   He, QY
   Liu, JC
   Wang, Z
AF Zhang, Cong
   He, Qiyun
   Liu, Jiangchuan
   Wang, Zhi
TI Exploring Viewer Gazing Patterns for Touch-Based Mobile Gamecasting
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Gazing pattern prediction; mobile gamecasting; touch interaction
ID ALLOCATION
AB Recent years have witnessed an explosion of gamecasting applications, in which game players (or gamers in short) broadcast game playthroughs by their personal devices in real time. Such pioneer platforms, such as YouTube Gaming, Twitch, and Mobcrush, have attracted a massive number of online broadcasters, and each of them can have hundreds or thousands of fellow viewers. The growing number, however, has created significant challenges to the network and end-devices, particularly considering that bandwidth-and battery-limited smartphones or tablets are becoming dominating for both gamers and viewers. Yet the unique touch operations of the mobile interface offer opportunities, too. In this paper, our measurements based on the real traces from gamers and viewers reveal that strong associations exist between the gamers' touch interactions and the viewers' gazing patterns. Motivated by this, we present a novel interaction-aware optimization framework to improve the energy utilization and stream quality for mobile gamecasting. Our framework incorporates a touch-assisted prediction module to extract association rules for gazing pattern prediction and a tile-based optimization module to utilize energy on mobile devices efficiently. Trace-driven simulations illustrate the effectiveness of our framework in terms of energy consumption and stream quality. Our user study experiments also demonstrate much improved (3%-13%) quality satisfaction over the state-of-the-art solution with similar network resources.
C1 [Zhang, Cong; He, Qiyun; Liu, Jiangchuan] Simon Fraser Univ, Sch Comp Sci, Burnaby, BC V5A 1S6, Canada.
   [Wang, Zhi] Tsinghua Univ, Grad Sch Shenzhen, Shenzhen 518055, Peoples R China.
C3 Simon Fraser University; Tsinghua University; Tsinghua Shenzhen
   International Graduate School
RP Liu, JC (corresponding author), Simon Fraser Univ, Sch Comp Sci, Burnaby, BC V5A 1S6, Canada.
EM congz@cs.sfu.ca; qiyunh@cs.sfu.ca; jcliu@cs.sfu.ca;
   wangzhi@sz.tsinghua.edu.cn
FU National Priorities Research Program under Qatar National Research Fund
   (a member of Qatar Foundation) [8-519-1-108]; Natural Sciences and
   Engineering Research Natural Sciences and Engineering Research Council
   of Canada; Shenzhen University; Science and Technology Innovation
   [CXZZ20150323151850088]
FX This work was supported by the National Priorities Research Program
   under Grant #[8-519-1-108] from the Qatar National Research Fund (a
   member of Qatar Foundation). The work of J. Liu was supported by the
   Natural Sciences and Engineering Research Natural Sciences and
   Engineering Research Council of Canada. The work of Z. Wang was
   supported by Shenzhen University and Science and Technology Innovation
   under Grant CXZZ20150323151850088. This paper was presented in part at
   IEEE INFOCOM, Atlanta, GA, May 2017. The guest editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Hermann Hellwagner. (Corresponding author: Jiangchuan Liu.)
CR Ahmadi H, 2014, MULTIMEDIA SYST, V20, P485, DOI 10.1007/s00530-014-0381-1
   Ankerst M, 1999, SIGMOD RECORD, VOL 28, NO 2 - JUNE 1999, P49
   [Anonymous], 2007, EYE TRACKING METHODO
   Aparicio-Pardo Ramon., 2015, Proceedings of the 6th ACM Multimedia Systems Conference, MMSys'15, P49
   Bray J.H., 1985, Multivariate analysis of variance, V54
   Bylinskii Zoya, 2016, CORR
   Chen YM, 2015, IEEE INFOCOM SER, DOI 10.1109/INFOCOM.2015.7218660
   He QY, 2016, IEEE T MULTIMEDIA, V18, P916, DOI 10.1109/TMM.2016.2544698
   Hu WJ, 2014, IEEE INFOCOM SER, P916, DOI 10.1109/INFOCOM.2014.6848020
   Kellerer H., 2004, Introduction to NP-Completeness of Knapsack Problems, DOI 10.1007/978-3-540-24777-7
   Le Feuvre J., 2016, P 7 INT C MULT SYST
   Li ZC, 2011, IMAGE VISION COMPUT, V29, P1, DOI 10.1016/j.imavis.2010.07.001
   Mukherjee SS, 2015, IEEE T MULTIMEDIA, V17, P2094, DOI 10.1109/TMM.2015.2482819
   Niamut OA, 2016, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON MULTIMEDIA SYSTEMS (MMSYS'16), P46, DOI 10.1145/2910017.2910606
   RAJARAMAN S.V., 2013, Proceedings of the eighth ACM International Workshop on Mobility in the Evolving Internet Architecture Proc., Miami, 2013, P35
   Ryoo J., 2016, P 7 INT C MULT SYST
   Sun Y, 2006, IEEE T MULTIMEDIA, V8, P1, DOI 10.1109/TMM.2005.861296
   Wilk S., 2016, P 8 INT WORKSH MOB V
   Wu XD, 2008, KNOWL INF SYST, V14, P1, DOI 10.1007/s10115-007-0114-2
   Zhang C., 2017, P IEEE INFOCOM, P1027
   Zhang L, 2016, Proceedings of the Eleventh European Conference on Computer Systems, P1, DOI DOI 10.1109/BMSB.2016.7521908
NR 21
TC 10
Z9 10
U1 0
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2017
VL 19
IS 10
BP 2333
EP 2344
DI 10.1109/TMM.2017.2743987
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5YG
UT WOS:000411247600016
DA 2024-07-18
ER

PT J
AU Shi, YM
   Tian, YH
   Wang, YW
   Huang, TJ
AF Shi, Yemin
   Tian, Yonghong
   Wang, Yaowei
   Huang, Tiejun
TI Sequential Deep Trajectory Descriptor for Action Recognition With
   Three-Stream CNN
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Action recognition; sequential deep trajectory descriptor (sDTD);
   three-stream framework; long-term motion
AB Learning the spatial-temporal representation of motion information is crucial to human action recognition. Nevertheless, most of the existing features or descriptors cannot capture motion information effectively, especially for long-term motion. To address this problem, this paper proposes a long-term motion descriptor called sequential deep trajectory descriptor (sDTD). Specifically, we project dense trajectories into two-dimensional planes, and subsequently a CNN-RNN network is employed to learn an effective representation for long-term motion. Unlike the popular two-stream ConvNets, the sDTD stream is introduced into a three-stream framework so as to identify actions from a video sequence. Consequently, this three-stream framework can simultaneously capture static spatial features, short-term motion, and long-term motion in the video. Extensive experiments were conducted on three challenging datasets: KTH, HMDB51, and UCF101. Experimental results show that our method achieves state-of-the-art performance on the KTH and UCF101 datasets, and is comparable to the state-of-the-art methods on the HMDB51 dataset.
C1 [Shi, Yemin; Tian, Yonghong; Huang, Tiejun] Peking Univ, Sch Elect Engn & Comp Sci, Cooperat Medianet Innovat Ctr, Natl Engn Lab Video Technol, Beijing 100871, Peoples R China.
   [Wang, Yaowei] Beijing Inst Technol, Sch Informat & Elect, Beijing 100081, Peoples R China.
C3 Peking University; Beijing Institute of Technology
RP Tian, YH (corresponding author), Peking Univ, Sch Elect Engn & Comp Sci, Cooperat Medianet Innovat Ctr, Natl Engn Lab Video Technol, Beijing 100871, Peoples R China.
EM yhtian@pku.edu.cn; yaoweiwang@bit.edu.cn
RI Huang, Tiejun/D-6161-2011
FU National Basic Research Program of China [2015CB351806]; National
   Natural Science Foundation of China [61390515, U1611461, 61425025,
   61471042]; Beijing Municipal Commission of Science and Technology
   [Z151100000915070]; Shenzhen Peacock Plan
FX This work was supported in part by the National Basic Research Program
   of China under Grant 2015CB351806, in part by the National Natural
   Science Foundation of China under Contract 61390515, Contract U1611461,
   Contract 61425025, and Contract 61471042, in part by Beijing Municipal
   Commission of Science and Technology under Contract Z151100000915070,
   and in part by Shenzhen Peacock Plan. The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Sen-Ching Samson Cheung. (Corresponding author: Yonghong Tian.)
CR [Anonymous], 2014, ADV NEURAL INFORM PR
   [Anonymous], 2013, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2012.59
   [Anonymous], 2014, ARXIV14105401
   [Anonymous], 2012, UCF101 DATASET 101 H
   [Anonymous], 2015, CORR
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], 2012, IMPROVING NEURAL NET
   [Anonymous], 2015, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2015.123
   [Anonymous], P BRIT MACH VIS C
   [Anonymous], 2014, ICLR 15
   [Anonymous], CORR
   [Anonymous], 2014, CORR
   [Anonymous], 2015, 2015 IEEE International Conference on Multimedia and Expo (ICME)
   [Anonymous], 2015, CORR
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Bilinski P., 2013, P 2013 10 IEEE INT C, P1
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Cai ZW, 2014, PROC CVPR IEEE, P596, DOI 10.1109/CVPR.2014.83
   Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dalal N, 2006, LECT NOTES COMPUT SC, V3952, P428, DOI 10.1007/11744047_33
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Gilbert A, 2011, IEEE T PATTERN ANAL, V33, P883, DOI 10.1109/TPAMI.2010.144
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Hasan M, 2015, IEEE T MULTIMEDIA, V17, P1909, DOI 10.1109/TMM.2015.2477242
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kovashka A, 2010, PROC CVPR IEEE, P2046, DOI 10.1109/CVPR.2010.5539881
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756
   Le Q. V., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3361, DOI 10.1109/CVPR.2011.5995496
   Liu L, 2013, PATTERN RECOGN, V46, P1810, DOI 10.1016/j.patcog.2012.10.004
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Pang L, 2015, IEEE T MULTIMEDIA, V17, P2008, DOI 10.1109/TMM.2015.2482228
   Peng XJ, 2016, COMPUT VIS IMAGE UND, V150, P109, DOI 10.1016/j.cviu.2016.03.013
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Poppe R, 2010, IMAGE VISION COMPUT, V28, P976, DOI 10.1016/j.imavis.2009.11.014
   Reddy KK, 2013, MACH VISION APPL, V24, P971, DOI 10.1007/s00138-012-0450-4
   Schüldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Scovanner P., 2007, P 15 ACM INT C MULT, P357, DOI DOI 10.1145/1291233.1291311
   Sun L, 2015, IEEE I CONF COMP VIS, P4597, DOI 10.1109/ICCV.2015.522
   Sutskever I, 2014, ADV NEUR IN, V27
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Wang H., 2013, ICCV workshop on action recognition with a large number of classes, P1
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8
   Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059
   Wang LM, 2013, PROC CVPR IEEE, P2674, DOI 10.1109/CVPR.2013.345
   Wu ZX, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P461, DOI 10.1145/2733373.2806222
   Wu ZX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P167, DOI 10.1145/2647868.2654931
   Yuan CF, 2013, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2013.99
   Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22
   Zhou Z, 2015, IEEE T MULTIMEDIA, V17, P512, DOI 10.1109/TMM.2015.2404779
NR 58
TC 148
Z9 154
U1 0
U2 48
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2017
VL 19
IS 7
BP 1510
EP 1520
DI 10.1109/TMM.2017.2666540
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5ZN
UT WOS:000404061800009
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Servajean, M
   Joly, A
   Shasha, D
   Champ, J
   Pacitti, E
AF Servajean, Maximilien
   Joly, Alexis
   Shasha, Dennis
   Champ, Julien
   Pacitti, Esther
TI Crowdsourcing Thousands of Specialized Labels: A Bayesian Active
   Training Approach
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Crowdsourcing; Bayes methods; parameter estimation; Taylor series
AB Large-scale annotated corpora have yielded impressive performance improvements in computer vision and multimedia content analysis. However, such datasets depend on an enormous amount of human labeling effort. When the labels correspond to well-known concepts, it is straightforward to train the annotators by giving a few examples with known answers. It is also straightforward to judge the quality of their labels. Neither is true when there are thousands of complex domain-specific labels. Training on all labels is infeasible and the quality of an annotator's judgements may be vastly different for some subsets of labels than for others. This paper proposes a set of data-driven algorithms to 1) train image annotators on how to disambiguate among automatically generated candidate labels, 2) evaluate the quality of annotators' label suggestions, and 3) weigh predictions. The algorithms adapt to the skills of each annotator both in the questions asked and the weights given to their answers. The underlying judgements are Bayesian, based on adaptive priors. We measure the benefits of these algorithms on a live user experiment related to image-based plant identification involving around 1000 people. The proposed methods are shown to enable huge gains in annotation accuracy. A standard user can correctly label around 2% of our data. This goes up to 80% with machine learning assisted training and assignment and up to almost 90% when doing a weighted combination of several annotators' labels.
C1 [Servajean, Maximilien; Joly, Alexis; Champ, Julien; Pacitti, Esther] INRIA, LIRMM, Zenith Team, F-34095 Montpellier, France.
   [Shasha, Dennis] NYU, Dept Comp Sci, 550 1St Ave, New York, NY 10003 USA.
   [Shasha, Dennis] INRIA, F-34095 Montpellier, France.
C3 Inria; Centre National de la Recherche Scientifique (CNRS); Universite
   Paul-Valery; Universite Perpignan Via Domitia; Universite de
   Montpellier; New York University; Inria
RP Servajean, M (corresponding author), INRIA, LIRMM, Zenith Team, F-34095 Montpellier, France.
EM servajean@lirmm.fr; alexis.joly@inria.fr; shasha@courant.nyu.edu;
   champ@lirmm.fr; pacitti@lirmm.fr
RI Servajean, Maximilien/IQW-9683-2023; joly, alexis/AAV-3101-2021; Champ,
   Julien/AAG-1092-2020
OI Servajean, Maximilien/0000-0002-9426-2583; joly,
   alexis/0000-0002-2161-9940; Champ, Julien/0000-0002-2042-0411
FU INRIA International Chair
FX The work of D. Shasha was supported by the INRIA International Chair.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Dong Xu.
CR [Anonymous], HCIL201009 U MAR
   [Anonymous], 2015, Decision Making: Uncertainty, Imperfection, Deliberation and Scalability
   [Anonymous], P C LABS EV FOR
   [Anonymous], CORR
   [Anonymous], 2015, CoRR
   [Anonymous], 2013, DECISION MAKING IMPE
   [Anonymous], 2010, 2010 IEEE COMPUTER S, DOI DOI 10.1109/CVPRW.2010.5543189
   Basu S., 2013, Proc. AAAI, V27, P109
   Biel JI, 2013, IEEE T MULTIMEDIA, V15, P41, DOI 10.1109/TMM.2012.2225032
   Borne K., 2011, AGU Fall Meeting Abstracts, V1, page, P0650
   Bragg J, 2016, AAMAS'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P966
   Champ J, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P721, DOI 10.1145/2647868.2654875
   DANTZIG GB, 1957, OPER RES, V5, P266, DOI 10.1287/opre.5.2.266
   Dawid A. P., 1979, J ROY STAT SOC C, P28
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fang M, 2012, IEEE DATA MINING, P858, DOI 10.1109/ICDM.2012.64
   Garcia-Perez A., 2019, Designing and tracking knowledge management metrics, P163
   Gottlieb L, 2014, IEEE T MULTIMEDIA, V16, P2075, DOI 10.1109/TMM.2014.2347268
   Ipeirotis Panos., 2011, P 8 INT WORKSHOP INF, P1
   Joly A, 2016, MULTIMEDIA SYST, V22, P751, DOI 10.1007/s00530-015-0462-9
   Joly A, 2015, LECT NOTES COMPUT SC, V9283, P462, DOI 10.1007/978-3-319-24027-5_46
   Joly A, 2014, ECOL INFORM, V23, P22, DOI 10.1016/j.ecoinf.2013.07.006
   Kamar Ece, 2012, P 11 INT C AUT AG MU, P467
   Kim H.-C., 2012, INT C ARTIFICIAL INT, P619
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Liu D, 2011, IEEE T MULTIMEDIA, V13, P702, DOI 10.1109/TMM.2011.2134078
   Long T.-T., 2013, P 2013 INT C AUT AG, P901, DOI DOI 10.5555/2484920.2485063
   Parameswaran A.G., 2012, SIGMOD C, P361, DOI 10.1145/2213836.2213878
   Parameswaran A, 2011, PROC VLDB ENDOW, V4, P267, DOI 10.14778/1952376.1952377
   Raykar VC, 2010, J MACH LEARN RES, V11, P1297
   Roy SB, 2015, VLDB J, V24, P467, DOI 10.1007/s00778-015-0385-2
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   SAHNI S, 1976, J ACM, V23, P555, DOI 10.1145/321958.321975
   Sang JT, 2012, IEEE T MULTIMEDIA, V14, P883, DOI 10.1109/TMM.2012.2188782
   Sheng V. S., 2008, P 14 ACM SIGKDD INT, P614, DOI DOI 10.1145/1401890.1401965
   Tulyakov S, 2008, STUD COMPUT INTELL, V90, P361
   Venanzi M, 2014, WWW'14: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P155, DOI 10.1145/2566486.2567989
   Wang C, 2013, J MACH LEARN RES, V14, P1005
NR 38
TC 5
Z9 7
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2017
VL 19
IS 6
BP 1376
EP 1391
DI 10.1109/TMM.2017.2653763
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5YS
UT WOS:000404059400021
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Kalbkhani, H
   Shayesteh, MG
   Haghighat, N
AF Kalbkhani, Hashem
   Shayesteh, Mahrokh G.
   Haghighat, Nasser
TI Adaptive LSTAR Model for Long-Range Variable Bit Rate Video Traffic
   Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Kernel least mean square (KLMS); long range; logistic smooth transition
   autoregressive (LSTAR) model; normalized least mean square (NLMS);
   prediction; variable bit rate (VBR); video traffic
ID TIME VBR VIDEO; TRANSITION; ALLOCATION; SCHEME; MPEG-4
AB Static bandwidth allocation for variable bit rate (VBR) video traffic forfeits the available bandwidth. Prediction of the next frame size is thus useful in dynamic bandwidth allocation. It has been shown that VBR video traces are long-range dependent, which makes one-frame-ahead prediction insufficient for dynamic bandwidth allocation. Several studies have been conducted based on the linear autoregressive (AR) model to address VBR traffic prediction. In this paper, we propose the use of a nonlinear model from the AR family called logistic smooth transition autoregressive (LSTAR) to predict VBR video traffic. Furthermore, we introduce adaptive algorithms, including least mean square (LMS), normalized LMS (NLMS), kernel LMS (KLMS), and normalized KLMS (NKLMS), to obtain the parameters of the LSTAR model used in long-range VBR traffic prediction. In the proposed model, we do not separate traffic of different frame types and use only one predictor, which results in lower computational complexity. The performance of the proposed predictor for different prediction stepswas evaluated and compared with recently introduced predictors. The results indicate that the proposed nonlinear LSTAR-based predictor yields better results than the optimum linear AR predictor, i.e., Wiener-Hopf and others.
C1 [Kalbkhani, Hashem; Shayesteh, Mahrokh G.; Haghighat, Nasser] Urmia Univ, Dept Elect Engn, Orumiyeh 5756151818, Iran.
   [Shayesteh, Mahrokh G.] Sharif Univ Technol, Elect Engn Dept, Adv Commun Res Inst, Wireless Res Lab, Tehran 11365, Iran.
C3 Urmia University; Sharif University of Technology
RP Kalbkhani, H (corresponding author), Urmia Univ, Dept Elect Engn, Orumiyeh 5756151818, Iran.
EM h.kalbkhani@urmia.ac.ir; m.shayesteh@urmia.ac.ir;
   st_n.haghighat@urmia.ac.ir
RI Kalbkhani, Hashem/AFU-4862-2022
OI Kalbkhani, Hashem/0000-0003-2431-4920
CR Abdennour A., 2005, International Journal of Network Management, V15, P377, DOI 10.1002/nem.578
   Abe Shigeo., 2005, Support vector machines for pattern classification, V53
   Adas AM, 1998, IEEE ACM T NETWORK, V6, P635, DOI 10.1109/90.731200
   [Anonymous], 2011, KERNEL ADAPTIVE FILT
   [Anonymous], 1977, J MARKETING RES
   [Anonymous], 2008, ADAPTIVE FILTER THEO
   [Anonymous], 1978, THRESHOLD MODEL
   BACON DW, 1971, BIOMETRIKA, V58, P525, DOI 10.2307/2334387
   BERAN J, 1995, IEEE T COMMUN, V43, P1566, DOI 10.1109/26.380206
   Beran J., 2013, Long-memory processes probabilistic properties and statistical methods
   Bhattacharya A, 2003, IEEE T SIGNAL PROCES, V51, P2177, DOI 10.1109/TSP.2003.814470
   Dai M, 2009, IEEE T MULTIMEDIA, V11, P1010, DOI 10.1109/TMM.2009.2021802
   Doulamis AD, 2003, IEEE T NEURAL NETWOR, V14, P150, DOI 10.1109/TNN.2002.806645
   Du J, 2016, IEEE T MULTIMEDIA, V18, P820, DOI 10.1109/TMM.2016.2537781
   Gallardo JR, 2001, IEEE T MULTIMEDIA, V3, P177, DOI 10.1109/6046.923817
   Granero MAS, 2008, PHYSICA A, V387, P5543, DOI 10.1016/j.physa.2008.05.053
   Haghighat N, 2015, IET IMAGE PROCESS, V9, P777, DOI 10.1049/iet-ipr.2014.1035
   Hassan M, 2014, UKSIM INT CONF COMP, P542, DOI 10.1109/UKSim.2014.121
   Huang T.-M., 2006, STUD COMP INTELL, V17
   Kang S, 2010, IEEE T SIGNAL PROCES, V58, P1219, DOI 10.1109/TSP.2009.2035983
   Lanfranchi LI, 2008, IEEE T BROADCAST, V54, P741, DOI 10.1109/TBC.2008.2001244
   LaViola J. J.  Jr., 2003, IPT/EGVE 2003. Seventh Immersive Projection Technology Workshop. Ninth Eurographics Workshop on Virtual Environments, P199, DOI 10.1145/769953.769976
   Lee B, 2014, IEEE T CIRC SYST VID, V24, P465, DOI 10.1109/TCSVT.2013.2276880
   LELAND WE, 1994, IEEE ACM T NETWORK, V2, P1, DOI 10.1109/90.282603
   Li Y, 2009, IEEE T MULTIMEDIA, V11, P1182, DOI 10.1109/TMM.2009.2026102
   Liang Y, 2004, IEEE T SYST MAN CY C, V34, P32, DOI 10.1109/TSMCC.2003.818492
   Lim J.S., 1987, ADV TOPICS SIGNAL PR
   LUUKKONEN R, 1988, BIOMETRIKA, V75, P491, DOI 10.1093/biomet/75.3.491
   Lygizou A, 2014, WIRELESS PERS COMMUN, V74, P1, DOI 10.1007/s11277-012-0733-0
   Ou YF, 2014, IEEE T IMAGE PROCESS, V23, P2473, DOI 10.1109/TIP.2014.2303636
   PAXSON V, 1995, IEEE ACM T NETWORK, V3, P226, DOI 10.1109/90.392383
   Peng MG, 2016, IEEE T MULTIMEDIA, V18, P879, DOI 10.1109/TMM.2016.2535722
   Pokharel PP, 2007, INT CONF ACOUST SPEE, P1421
   Rossi L, 2015, IEEE ACM T NETWORK, V23, P547, DOI 10.1109/TNET.2014.2303162
   Sadek N, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS, VOLS 1-7, P2148, DOI 10.1109/ICC.2004.1312898
   Sadek N, 2003, IEEE IC COMP COM NET, P359, DOI 10.1109/ICCCN.2003.1284194
   Samorodnitsky G., 2007, TRENDS STOCH SYST, V1, P163, DOI [DOI 10.1561/0900000004, 10.1561/0900000004]
   Sarkar UK, 2003, IEEE ACM T NETWORK, V11, P638, DOI 10.1109/TNET.2003.815292
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Seeling P, 2014, SCI WORLD J, DOI 10.1155/2014/189481
   Seeling P, 2012, IEEE COMMUN SURV TUT, V14, P1142, DOI 10.1109/SURV.2011.082911.00067
   Shawe-Taylor J., 2004, KERNEL METHODS PATTE
   Tanwir S., 2016, P INT C COMP NETW CO, P1, DOI [10.1109/ICCNC.2016.7440638, DOI 10.1109/ICCNC.2016.7440638]
   Tanwir S, 2013, IEEE COMMUN SURV TUT, V15, P1778, DOI 10.1109/SURV.2013.010413.00071
   TERASVIRTA T., 1994, Handbook of econometrics, V2, P2917
   TONG H, 1980, J ROY STAT SOC B MET, V42, P245
   Trlin G., 2012, P 20 INT C SOFTW TEL, P1
   Van der Auwera G, 2008, IEEE T BROADCAST, V54, P698, DOI 10.1109/TBC.2008.2000422
   Wang CH, 2008, IEEE T CIRC SYST VID, V18, P1771, DOI 10.1109/TCSVT.2008.2004926
   Wang MH, 2015, IEEE SIGNAL PROC LET, V22, P896, DOI 10.1109/LSP.2014.2377032
   Wen Y., 2013, FRONTIERS INTERNET T, P1
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Wu M, 2001, IEEE T MULTIMEDIA, V3, P186, DOI 10.1109/6046.923818
   Yoo SJ, 2002, IEEE T BROADCAST, V48, P10, DOI 10.1109/11.992849
   Zhang S, 2010, IET COMMUN, V4, P1277, DOI 10.1049/iet-com.2009.0405
   Zhou ML, 2016, J VIS COMMUN IMAGE R, V34, P204, DOI 10.1016/j.jvcir.2015.11.011
NR 56
TC 15
Z9 16
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2017
VL 19
IS 5
BP 999
EP 1014
DI 10.1109/TMM.2016.2639379
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5XS
UT WOS:000404056000009
DA 2024-07-18
ER

PT J
AU Liu, WX
   Lau, RWH
   Wang, XG
   Manocha, D
AF Liu, Wenxi
   Lau, Rynson W. H.
   Wang, Xiaogang
   Manocha, Dinesh
TI Exemplar-AMMs: Recognizing Crowd Movements From Pedestrian Trajectories
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Crowd behavior modeling; crowd simulation; pattern recognition; video
   surveillance
ID TRACKING; VIDEO; BEHAVIOR; MODEL
AB In this paper, we present a novel method to recognize the types of crowd movement from crowd trajectories using agent-based motion models (AMMs). Our idea is to apply a number of AMMs, referred to as exemplar-AMMs, to describe the crowd movement. Specifically, we propose an optimization framework that filters out the unknown noise in the crowd trajectories and measures their similarity to the exemplar-AMMs to produce a crowd motion feature. We then address our real-world crowd movement recognition problem as a multilabel classification problem. Our experiments show that the proposed feature outperforms the state-of-the-art methods in recognizing both simulated and real-world crowd movements from their trajectories. Finally, we have created a synthetic dataset, SynCrowd, which contains two-dimensional (2D) crowd trajectories in various scenarios, generated by various crowd simulators. This dataset can serve as a training set or benchmark for crowd analysis work.
C1 [Liu, Wenxi] Fuzhou Univ, Dept Comp Sci, Fuzhou 350001, Peoples R China.
   [Lau, Rynson W. H.] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
   [Wang, Xiaogang] Chinese Univ Hong Kong, Dept Elect Engn, Hong Kong, Hong Kong, Peoples R China.
   [Manocha, Dinesh] Univ N Carolina, Dept Comp Sci, Chapel Hill, NC 27514 USA.
C3 Fuzhou University; City University of Hong Kong; Chinese University of
   Hong Kong; University of North Carolina; University of North Carolina
   Chapel Hill
RP Liu, WX (corresponding author), Fuzhou Univ, Dept Comp Sci, Fuzhou 350001, Peoples R China.
EM wenxi.liu@hotmail.com; rynson.lau@cityu.edu.hk; xgwang@ee.cuhk.edu.hk;
   dm@cs.unc.edu
RI Lin, Fan/JZT-1441-2024; zhang, weijie/JQX-1450-2023; yang,
   qing/JBR-8440-2023; Wang, Xiaogang/L-4369-2014; Wang,
   Xiaogang/B-2439-2013
OI Lin, Fan/0000-0002-7330-3833; Wang, Xiaogang/0000-0002-7929-5889
FU Natural Science Foundation of Fujian Province [2016J05155]
FX This work was supported by the Natural Science Foundation of Fujian
   Province under Grant 2016J05155. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr.
   Wolfgang Hurst.
CR Ali S, 2008, LECT NOTES COMPUT SC, V5303, P1, DOI 10.1007/978-3-540-88688-4_1
   Ali S, 2013, IEEE I CONF COMP VIS, P1097, DOI 10.1109/ICCV.2013.140
   Ali S, 2007, PROC CVPR IEEE, P65
   [Anonymous], 1999, P GAM DEV C
   [Anonymous], 2010, VRIPHYS 2010 7 WORK, DOI DOI 10.2312/PE/VRIPHYS/VRIPHYS10/125-134
   Antonini G, 2006, INT J COMPUT VISION, V69, P159, DOI 10.1007/s11263-005-4797-0
   Bera A, 2015, P 41 GRAPH INT C, P65
   Chan AB, 2008, IEEE T PATTERN ANAL, V30, P909, DOI 10.1109/TPAMI.2007.70738
   Charalambous P, 2014, COMPUT GRAPH FORUM, V33, P95, DOI 10.1111/cgf.12403
   Charalambous P, 2014, COMPUT GRAPH FORUM, V33, P41, DOI 10.1111/cgf.12472
   Choi W, 2012, LECT NOTES COMPUT SC, V7575, P215, DOI 10.1007/978-3-642-33765-9_16
   Evensen G., 2003, OCEAN DYNAM, V53, P343, DOI 10.1007/s10236-003-0036-9
   Feris RS, 2012, IEEE T MULTIMEDIA, V14, P28, DOI 10.1109/TMM.2011.2170666
   Ge WN, 2012, IEEE T PATTERN ANAL, V34, P1003, DOI 10.1109/TPAMI.2011.176
   Guy S.J., 2011, P 2011 ACM SIGGRAPH, P43, DOI [10.1145/2019406.2019413, DOI 10.1145/2019406.2019413]
   Guy SJ, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366209
   Hare S, 2011, IEEE I CONF COMP VIS, P263, DOI 10.1109/ICCV.2011.6126251
   HELBING D, 1995, PHYS REV E, V51, P4282, DOI 10.1103/PhysRevE.51.4282
   Jain A, 1997, IEEE T PATTERN ANAL, V19, P153, DOI 10.1109/34.574797
   Kuettel D, 2010, PROC CVPR IEEE, P1951, DOI 10.1109/CVPR.2010.5539869
   Lee KH, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P109
   Li RN, 2013, INT J COMPUT VISION, V101, P305, DOI 10.1007/s11263-012-0573-0
   Liu WX, 2015, IEEE T CIRC SYST VID, V25, P399, DOI 10.1109/TCSVT.2014.2344511
   Lo Presti L, 2012, IEEE T MULTIMEDIA, V14, P346, DOI 10.1109/TMM.2011.2173323
   Mahadevan V, 2010, PROC CVPR IEEE, P1975, DOI 10.1109/CVPR.2010.5539872
   Malisiewicz T, 2011, IEEE I CONF COMP VIS, P89, DOI 10.1109/ICCV.2011.6126229
   Mehran R, 2009, PROC CVPR IEEE, P935, DOI 10.1109/CVPRW.2009.5206641
   Moussaïd M, 2011, P NATL ACAD SCI USA, V108, P6884, DOI 10.1073/pnas.1016507108
   Ondrej J, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778860
   Pellegrini S, 2009, IEEE I CONF COMP VIS, P261, DOI 10.1109/ICCV.2009.5459260
   Pettre J., 2009, Proceedings of the 2009 ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA '09, P189, DOI DOI 10.1145/1599470.1599495
   Reynolds CW., 1987, SIGGRAPH Comput. Graph., V21, P25, DOI [10.1145/37402.37406, DOI 10.1145/37402.37406]
   Shao J, 2014, PROC CVPR IEEE, P2227, DOI 10.1109/CVPR.2014.285
   Uijlings J, 2015, INT J MULTIMED INF R, V4, P33, DOI 10.1007/s13735-014-0069-5
   van den Berg J, 2011, SPRINGER TRAC ADV RO, V70, P3
   VICSEK T, 1995, PHYS REV LETT, V75, P1226, DOI 10.1103/PhysRevLett.75.1226
   Wang XG, 2011, INT J COMPUT VISION, V95, P287, DOI 10.1007/s11263-011-0459-6
   Wang XG, 2009, IEEE T PATTERN ANAL, V31, P539, DOI 10.1109/TPAMI.2008.87
   Wolinski D, 2014, COMPUT GRAPH FORUM, V33, P303, DOI 10.1111/cgf.12328
   Wu JZ, 2014, IEEE T MULTIMEDIA, V16, P147, DOI 10.1109/TMM.2013.2283846
   Yamaguchi K, 2011, PROC CVPR IEEE, P1345, DOI 10.1109/CVPR.2011.5995468
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P346, DOI 10.1109/TMM.2015.2393635
   Yuan Y, 2015, IEEE T MULTIMEDIA, V17, P1125, DOI 10.1109/TMM.2015.2440996
   Zhang ML, 2007, PATTERN RECOGN, V40, P2038, DOI 10.1016/j.patcog.2006.12.019
   Zhou BL, 2013, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR.2013.392
   Zhou BL, 2012, PROC CVPR IEEE, P2871, DOI 10.1109/CVPR.2012.6248013
NR 46
TC 19
Z9 19
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2016
VL 18
IS 12
BP 2398
EP 2406
DI 10.1109/TMM.2016.2598091
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ED5VK
UT WOS:000388920200007
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Chen, W
   Lao, TY
   Xia, J
   Huang, XX
   Zhu, B
   Hu, WQ
   Guan, HH
AF Chen, Wei
   Lao, Tianyi
   Xia, Jing
   Huang, Xinxin
   Zhu, Biao
   Hu, Wanqi
   Guan, Huihua
TI GameFlow: Narrative Visualization of NBA Basketball Games
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Basketball games; game narration; visualization; video
AB Although basketball games have received broad attention, the forms of game reports and webcast are purely content-based cross-media: texts, videos, snapshots, and performance figures. Analytical narrations of games that seek to compose a complete game from heterogeneous datasets are challenging for general media producers because such a composition is time-consuming and heavily depends on domain experts. In particular, an appropriate analytical commentary of basketball games requires two factors, namely, rich context and domain knowledge, which includes game events, player locations, player profiles, and team profiles, among others. This type of analytical commentary elicits a timely and effective basketball game data visualization made up of different sources of media. Existing visualizations of basketball games mainly profile a particular aspect of the game. Therefore, this paper presents an expressive visualization scheme that comprehensively illustrates NBA games with three levels of details: a season level, a game level, and a session level. We reorganize a basketball game as a sequence of sessions to depict the game states and heated confrontations. We design and implement a live system that integrates multimedia NBA datasets: play-by-play text data, box score data, game video data, and action area data. We demonstrate the effectiveness of this scheme with case studies and user feedbacks.
C1 [Chen, Wei; Lao, Tianyi; Xia, Jing; Huang, Xinxin; Zhu, Biao; Hu, Wanqi; Guan, Huihua] Zhejiang Univ, State Key Lab Comp Aided Design & Comp Graph, Hangzhou 310027, Zhejiang, Peoples R China.
C3 Zhejiang University
RP Chen, W (corresponding author), Zhejiang Univ, State Key Lab Comp Aided Design & Comp Graph, Hangzhou 310027, Zhejiang, Peoples R China.
EM chenwei@cad.zju.edu.cn; laotianyi@gmail.com; xiajing_cad@zju.edu.cnl;
   huangxinxin07@gmail.com; arthurbzhu@gmail.com; huwanqi1234@gmail.com;
   higtonic@gmail.com
RI Chen, Wei/AAR-9817-2020
FU National 973 Program of China [2015CB352503]; Major Program of National
   Natural Science Foundation of China [61232012]; National Natural Science
   Foundation of China [61422211, 61303134]; Zhejiang Provincial Natural
   Science Foundation of China [LR13F020001]; Fundamental Research Funds
   for the Central Universities
FX This work was supported in part by the National 973 Program of China
   under Grant 2015CB352503, in part by the Major Program of National
   Natural Science Foundation of China under Grant 61232012, in part by the
   National Natural Science Foundation of China under Grant 61422211, in
   part by the National Natural Science Foundation of China under Grant
   61303134, in part by the Zhejiang Provincial Natural Science Foundation
   of China under Grant LR13F020001, and in part by the Fundamental
   Research Funds for the Central Universities. The guest editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Nan Cao.
CR [Anonymous], P MIT SLOAN SPORTS A
   Bashuk M., 2012, P MIT SLOAN SPORTS A
   Cava R., 2013, P SPORTVIS WORKSH SP
   Dietrich C, 2014, IEEE CONF VIS ANAL, P23, DOI 10.1109/VAST.2014.7042478
   Hamilton GR, 1997, J SPORT SCI, V15, P491, DOI 10.1080/026404197367137
   Holten D, 2006, IEEE T VIS COMPUT GR, V12, P741, DOI 10.1109/TVCG.2006.147
   Igor J., 2012, PHYS CULTURE J SPORT, V66, P15
   Janetzko H, 2014, IEEE CONF VIS ANAL, P13, DOI 10.1109/VAST.2014.7042477
   Kvam P, 2006, NAV RES LOG, V53, P788, DOI 10.1002/nav.20170
   Oliver D., 2004, BASKETBALL PAPER RUL
   Parry ML, 2011, IEEE T VIS COMPUT GR, V17, P1747, DOI 10.1109/TVCG.2011.208
   Perin C, 2013, IEEE T VIS COMPUT GR, V19, P2506, DOI 10.1109/TVCG.2013.192
   Perse M, 2009, COMPUT VIS IMAGE UND, V113, P612, DOI 10.1016/j.cviu.2008.03.001
   Pileggi H, 2012, IEEE T VIS COMPUT GR, V18, P2819, DOI 10.1109/TVCG.2012.263
   Polk T, 2014, IEEE T VIS COMPUT GR, V20, P2339, DOI 10.1109/TVCG.2014.2346445
   Sisneros R., 2013, P SPORTVIS WORKSH SP
   Wongsuphasawat K., 2013, P SPORTVIS WORKSH SP
NR 17
TC 39
Z9 46
U1 6
U2 56
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2016
VL 18
IS 11
BP 2247
EP 2256
DI 10.1109/TMM.2016.2614221
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EA9BX
UT WOS:000386936900012
DA 2024-07-18
ER

PT J
AU Ma, L
   Xu, L
   Zhang, YC
   Yan, YH
   Ngan, KN
AF Ma, Lin
   Xu, Long
   Zhang, Yichi
   Yan, Yihua
   Ngan, King Ngi
TI No-Reference Retargeted Image Quality Assessment Based on Pairwise Rank
   Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image quality assessment; no-reference; rank learning; retargeted image
ID DOMAIN; SCENE
AB In this paper, we propose a novel no-reference image quality assessment method for the retargeted image based on the pairwise rank learning approach. Each retargeted image needs to be first represented as a feature vector, which not only captures the image characteristics but also is sensitive to distortions during the retargeting process. As such, we investigate and examine different image representations for their abilities depicting the perceptual quality of retargeted image. Based on the image representations, we resort to the pairwise rank learning approach to discriminate the perceptual quality between the retargeted image pairs. Experimental results demonstrate that the proposed method can effectively depict the perceptual quality of the retargeted image, which can even perform comparably with the full-reference quality assessment methods.
C1 [Ma, Lin] Tencent AI Lab, Shenzhen 518057, Peoples R China.
   [Xu, Long; Yan, Yihua] Chinese Acad Sci, Natl Astron Observ, Key Lab Solar Act, Beijing 100049, Peoples R China.
   [Zhang, Yichi; Ngan, King Ngi] Chinese Univ Hong Kong, Dept Elect Engn, Hong Kong, Hong Kong, Peoples R China.
C3 Tencent; Chinese Academy of Sciences; National Astronomical Observatory,
   CAS; Chinese University of Hong Kong
RP Xu, L (corresponding author), Chinese Acad Sci, Natl Astron Observ, Key Lab Solar Act, Beijing 100049, Peoples R China.
EM forest.linma@gmail.com; lxu@nao.cas.cn; yczhang@ee.cuhk.edu.hk;
   yyh@nao.cas.cn; knngan@ee.cuhk.edu.hk
RI Xu, Long/AAH-9908-2019; Ngan, N/E-8240-2014; Yan, Yihua/AGY-9819-2022
OI Xu, Long/0000-0002-9286-2876; Ngan, N/0000-0003-1946-3235; Yan,
   Yihua/0000-0002-7106-6029
FU Research Grants Council of the Hong Kong SAR, China [CUHK 415913];
   National Natural Science Foundation of China [61572461, 11433006]; CAS
   100-Talents
FX This work was supported in part by a grant from the Research Grants
   Council of the Hong Kong SAR, China (Project CUHK 415913), and by the
   National Natural Science Foundation of China under Grant 61572461 and
   Grant 11433006. The work of L. Xu was supported by CAS 100-Talents. The
   guest editor coordinating the review of this manuscript and approving it
   for publication was Prof. Daniel Keim. (Corresponding author: Long Xu.)
CR [Anonymous], ITUTP800
   [Anonymous], MULT EXP WORKSH ICME
   [Anonymous], BT 500 11 METHODOLOG
   [Anonymous], PROC ACM INT CONF
   [Anonymous], P SIGGRAPH 07
   [Anonymous], 2008, P 25 INT C MACH LEAR, DOI [DOI 10.1145/1390156.1390306, 10.1145/1390156.1390306]
   [Anonymous], P SIGGRAPH AS 2011 C
   [Anonymous], IEEE T NEURAL NETW L, DOI [10.1109/TNNLS.2016.2527796, DOI 10.1109/TNNLS.2016.2527796]
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Bosch A, 2007, IEEE I CONF COMP VIS, P1863
   Brandao T, 2008, SIGNAL PROCESS, V88, P822, DOI 10.1016/j.sigpro.2007.09.017
   Castillo S., 2011, ACM SIGGRAPH S APPLP, P7
   Chen KT, 2009, MATH COMPUT SCI ENG, P491, DOI 10.1145/1631272.1631339
   Dong WM, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618471
   Fang YM, 2014, IEEE J EM SEL TOP C, V4, P95, DOI 10.1109/JETCAS.2014.2298919
   Fang YM, 2012, IEEE T IMAGE PROCESS, V21, P3888, DOI 10.1109/TIP.2012.2199126
   Gu B, 2017, IEEE T NEUR NET LEAR, V28, P1646, DOI 10.1109/TNNLS.2016.2544779
   Gu B, 2015, IEEE T NEUR NET LEAR, V26, P1403, DOI 10.1109/TNNLS.2014.2342533
   Gu B, 2015, NEURAL NETWORKS, V67, P140, DOI 10.1016/j.neunet.2015.03.013
   Kendall MG, 1938, BIOMETRIKA, V30, P81, DOI 10.2307/2332226
   Li B, 2014, IEEE T IMAGE PROCESS, V23, P1615, DOI 10.1109/TIP.2014.2305843
   Li SN, 2011, IEEE T MULTIMEDIA, V13, P935, DOI 10.1109/TMM.2011.2152382
   Liang Y, 2017, IEEE T VIS COMPUT GR, V23, P1099, DOI 10.1109/TVCG.2016.2517641
   Lin WS, 2011, J VIS COMMUN IMAGE R, V22, P297, DOI 10.1016/j.jvcir.2011.01.005
   Liu C, 2008, LECT NOTES COMPUT SC, V5304, P28, DOI 10.1007/978-3-540-88690-7_3
   Liu YJ, 2011, COMPUT GRAPH FORUM, V30, P583, DOI 10.1111/j.1467-8659.2011.01881.x
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Ma L, 2012, IEEE INT SYMP CIRC S, P2677, DOI 10.1109/ISCAS.2012.6271858
   Ma L, 2012, IEEE J-STSP, V6, P626, DOI 10.1109/JSTSP.2012.2211996
   Ma L, 2010, IEEE SIGNAL PROC LET, V17, P627, DOI 10.1109/LSP.2010.2048726
   Manjunath BS, 2001, IEEE T CIRC SYST VID, V11, P703, DOI 10.1109/76.927424
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Pele O, 2009, IEEE I CONF COMP VIS, P460, DOI 10.1109/ICCV.2009.5459199
   Pritch Y, 2009, IEEE I CONF COMP VIS, P151, DOI 10.1109/ICCV.2009.5459159
   Rubinstein M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866186
   Rubinstein M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531329
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P335, DOI 10.1109/TIP.2009.2034992
   Shamir Ariel, 2009, ACM SIG-GRAPH ASIA 2009 Courses, P11
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Sheikh HR, 2005, IEEE T IMAGE PROCESS, V14, P1918, DOI 10.1109/TIP.2005.854492
   Simakov D, 2008, PROC CVPR IEEE, P3887
   Simonyan K., 2014, CORR
   Tang HX, 2014, PROC CVPR IEEE, P2877, DOI 10.1109/CVPR.2014.368
   Tie-Yan Liu, 2009, Foundations and Trends in Information Retrieval, V3, P225, DOI 10.1561/1500000016
   Wang YS, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409071
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wen XZ, 2015, INFORM SCIENCES, V295, P395, DOI 10.1016/j.ins.2014.10.040
   Xu L, 2017, IEEE T CIRC SYST VID, V27, P1833, DOI 10.1109/TCSVT.2016.2543099
   Xu L, 2016, DISPLAYS, V44, P21, DOI 10.1016/j.displa.2016.06.002
   Xu Q, 2012, IEEE T MULTIMEDIA, V14, P844, DOI 10.1109/TMM.2012.2190924
   Xue WF, 2013, PROC CVPR IEEE, P995, DOI 10.1109/CVPR.2013.133
   Ye P, 2012, IEEE T IMAGE PROCESS, V21, P3129, DOI 10.1109/TIP.2012.2190086
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang YC, 2015, IEEE IMAGE PROC, P1757, DOI 10.1109/ICIP.2015.7351102
NR 55
TC 37
Z9 37
U1 1
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2016
VL 18
IS 11
BP 2228
EP 2237
DI 10.1109/TMM.2016.2614187
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EA9BX
UT WOS:000386936900010
DA 2024-07-18
ER

PT J
AU Qi, X
   Yang, Q
   Nguyen, DT
   Peng, G
   Zhou, G
   Dai, B
   Zhang, DQ
   Li, YT
AF Qi, Xin
   Yang, Qing
   Nguyen, David T.
   Peng, Ge
   Zhou, Gang
   Dai, Bo
   Zhang, Daqing
   Li, Yantao
TI A Context-Aware Framework for Reducing Bandwidth Usage of Mobile Video
   Chats
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bandwidth reduction; context awareness; frame interpolation; frame rate
   adaption; mobile video chats
ID QUALITY ADAPTATION; ALLOCATION
AB Mobile video chat apps offer users an approachable way to communicate with others. As high-speed 4G networks are being deployed worldwide, the number of mobile video chat app users increases. However, video chatting on mobile devices brings users financial concerns, since streaming video demands high bandwidth and can use up a large amount of data in dozens of minutes. Lowering the bandwidth usage of mobile video chats is challenging since video quality may be compromised. In this paper, we attempt to tame this challenge. Technically, we propose a context-aware frame rate adaption framework, named low-bandwidth video chat (LBVC). It follows a sender-receiver cooperative principle that smartly handles the tradeoff between lowering bandwidth usage and maintaining video quality. We implement LBVC by modifying an open-source app-Linphone-and evaluate it with both objective experiments and subjective studies.
C1 [Qi, Xin] VMware Inc, NSX Grp, Palo Alto, CA 94304 USA.
   [Yang, Qing; Nguyen, David T.; Peng, Ge; Zhou, Gang] Coll William & Mary, Dept Comp Sci, Williamsburg, VA 23187 USA.
   [Dai, Bo] Georgia Inst Technol, Sch Computat Sci & Engn, Coll Comp, Atlanta, GA 30332 USA.
   [Zhang, Daqing] Peking Univ, Key Lab High Confidence Software Technol, Beijing 100871, Peoples R China.
   [Li, Yantao] Southwest Univ, Coll Comp & Informat Sci, Chongqing 400715, Peoples R China.
C3 VMware, Inc.; William & Mary; University System of Georgia; Georgia
   Institute of Technology; Peking University; Southwest University - China
RP Zhang, DQ (corresponding author), Peking Univ, Key Lab High Confidence Software Technol, Beijing 100871, Peoples R China.
EM qix@vmware.com; qyang@cs.wm.edu; dnguyen@cs.wm.edu; gpeng@cs.wm.edu;
   gzhou@cs.wm.edu; bodai@gatech.edu; dqzhang@sei.pku.edu.cn;
   yantaoli@foxmail.com
RI Zhou, Gang/T-7901-2017
OI Zhou, Gang/0000-0002-4425-9837
FU U.S. National Science Foundation [CNS-1253506, CNS-1250180]; National
   Natural Science Foundation of China [61572048, 61402380]; Fundamental
   Research Funds for the Central Universities [XDJK2013C116]; Division Of
   Computer and Network Systems; Direct For Computer & Info Scie & Enginr
   [1253506] Funding Source: National Science Foundation
FX This work was supported in part by the U.S. National Science Foundation
   under Grant CNS-1253506 (CAREER) and Grant CNS-1250180, in part by the
   National Natural Science Foundation of China under Grant 61572048 and
   Grant 61402380, and in part by the Fundamental Research Funds for the
   Central Universities under Grant XDJK2013C116. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Liang Zhu. (Corresponding author: Daqing Zhang.)
CR Advanced Video Coding for Generic Audiovisual Service, 2013, ADV VID COD GEN AUD
   Almowuena S, 2016, IEEE T MULTIMEDIA, V18, P102, DOI 10.1109/TMM.2015.2502067
   [Anonymous], 2015, ELECT J DIFFER EQ
   [Anonymous], 2015, MOB VID CHAT US POP
   [Anonymous], 2012, WHIT PAP LIV ON DEM
   Argyriou A, 2015, IEEE T MULTIMEDIA, V17, P736, DOI 10.1109/TMM.2015.2408254
   Baker S., 2011, OPTICAL FLOW EXECUTI
   Bankoski J, 2011, RFC6386
   Bao X., 2011, P 12 WORKSH MOB COMP, P77
   Berthouzoz F, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185563
   Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3
   Chan A, 2012, MOBICOM 12: PROCEEDINGS OF THE 18TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P221
   Chang SF, 2005, P IEEE, V93, P148, DOI 10.1109/JPROC.2004.839600
   Chen J., 2013, Proceedings of the 19th Annual International Conference on Mobile Computing Networking, P389
   Chen Jing, 2012, Proceedings of the 2012 IEEE International Conference on Computer Science and Automation Engineering (CSAE), P121
   Chen L, 2015, IEEE T MULTIMEDIA, V17, P485, DOI 10.1109/TMM.2015.2405343
   Chen X., 2009, P ACM MULT C, P381
   Chuah SP, 2015, IEEE T MULTIMEDIA, V17, P687, DOI 10.1109/TMM.2015.2413354
   Egilmez HE, 2013, IEEE T MULTIMEDIA, V15, P710, DOI 10.1109/TMM.2012.2232645
   Falaki Hossein., 2010, Diversity in smartphone usage, P179, DOI DOI 10.1145/1814433.1814453
   Ha S, 2012, ACM SIGCOMM COMP COM, V42, P247, DOI 10.1145/2377677.2377723
   Handley M., 1998, ISI LBNL         APR
   Hong G., 2008, P ACM MULT, P749
   Keller Lorenzo., 2012, ACM MOBISYS, P57
   Kemelmacher-Shlizerman I, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964956
   Lee S, 2008, J VIS COMMUN IMAGE R, V19, P508, DOI 10.1016/j.jvcir.2008.08.004
   Li Y, 2009, IEEE T MULTIMEDIA, V11, P1182, DOI 10.1109/TMM.2009.2026102
   Liu Y., 2011, Proceedings of the 19th ACM International Conference on Multimedia, P473
   MARR D, 1980, PROC R SOC SER B-BIO, V207, P187, DOI 10.1098/rspb.1980.0020
   Meng J., 2010, THESIS
   Rahmati A, 2007, MOBISYS '07: PROCEEDINGS OF THE FIFTH INTERNATIONAL CONFERENCE ON MOBILE SYSTEMS, APPLICATIONS, AND SERVICES, P165
   Rainie L., 2010, US VIDEO CALL STAT
   Rakêt LL, 2012, LECT NOTES COMPUT SC, V7431, P447, DOI 10.1007/978-3-642-33179-4_43
   Recommendation ITU-R BT.500, 2002, Recommendation ITU-R BT. 500-11. ITU Telecom. Standardization Sector of ITU 7, P500
   Shi Shu., 2011, Proceedings of the 19th ACM International Conference on Multimedia, P103, DOI [10.1145/2072298.2072313, DOI 10.1145/2072298.2072313]
   Song Wei., 2011, Proceedings_of_the_19th_ACM international_conference_on_Multimedia, P403
   Su J., 1996, P 30 AS C SIGN SYST, P100
   Szeliski R., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P251, DOI 10.1145/258734.258861
   Wang XF, 2013, IEEE T MULTIMEDIA, V15, P811, DOI 10.1109/TMM.2013.2239630
   Wang Y., 2001, VIDEO PROCESSING COM
   Xu CQ, 2015, IEEE COMMUN MAG, V53, P150, DOI 10.1109/MCOM.2015.7295477
   Xu CQ, 2015, IEEE T VEH TECHNOL, V64, P1201, DOI 10.1109/TVT.2014.2329696
   Xu CQ, 2013, IEEE T VEH TECHNOL, V62, P2273, DOI 10.1109/TVT.2012.2228682
   Zhang L, 2010, PROCEEDINGS OF 2010 INTERNATIONAL CONFERENCE ON PUBLIC ADMINISTRATION (6TH), VOL II, P105, DOI 10.1145/1878961.1878982
   Zhu TX, 2012, PROCEEDINGS OF THE 4TH (2012) INTERNATIONAL CONFERENCE ON FINANCIAL RISK AND CORPORATE FINANCE MANAGEMENT, VOLS I AND II, P279
NR 45
TC 9
Z9 10
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2016
VL 18
IS 8
BP 1640
EP 1649
DI 10.1109/TMM.2016.2572001
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DR5YJ
UT WOS:000379978000016
OA hybrid
DA 2024-07-18
ER

PT J
AU Zhao, B
   Wu, X
   Peng, Q
   Yan, SC
AF Zhao, Bo
   Wu, Xiao
   Peng, Qiang
   Yan, Shuicheng
TI Clothing Cosegmentation for Shopping Images With Cluttered Background
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Clothing extraction; cluttered background; cosegmentation; segmentation;
   shopping images
ID SEGMENTATION
AB In this paper, we address an important and practical problem of clothing cosegmentation (CCS): given multiple fashion model photos with natural backgrounds on e-commerce websites, to automatically and simultaneously segment all images and extract the clothing regions. However, cluttered backgrounds, variations in colors and styles, and inconsistent human poses all make it a challenging task. In this paper, a novel CCS algorithm is proposed to improve the accuracy of clothing extraction by exploiting the properties of multiple clothing images with the same apparel. First, the co-salient objects are computed by detecting the upper bodies of fashion models and transferring their locations within multiple images. Based on the coarse clothing regions determined by the upper body localization and co-salient object detection, the foreground (clothing) and background Gaussian mixture models are estimated, respectively. Finally, the clothing region in each image is extracted through energy minimization based on graph cuts iteratively. The proposed cosegmentation algorithm is mainly designed for multiple clothing images. As a byproduct, it can also be applied to single image segmentation without any modification. The experiments demonstrate that the proposed approach outperforms the state-of-the-art cosegmentation methods as well as traditional single image segmentation solution for shopping images.
C1 [Zhao, Bo; Wu, Xiao; Peng, Qiang] Southwest Jiaotong Univ, Sch Informat Sci & Technol, Chengdu 610031, Peoples R China.
   [Yan, Shuicheng] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117583, Singapore.
C3 Southwest Jiaotong University; National University of Singapore
RP Wu, X (corresponding author), Southwest Jiaotong Univ, Sch Informat Sci & Technol, Chengdu 610031, Peoples R China.
EM zhaobo@my.swjtu.edu.cn; wuxiaohk@home.swjtu.edu.cn;
   qpeng@home.swjtu.edu.cn; eleyans@nus.edu.sg
RI Yan, Shuicheng/HCI-1431-2022
OI Wu, Xiao/0000-0002-8322-8558
FU National Natural Science Foundation of China [61373121, 61036008,
   61328205]; Program for Sichuan Provincial Science Fund for Distinguished
   Young Scholars [13QNJJ0149]; Fundamental Research Funds for the Central
   Universities; Sichuan Science and Technology Innovation Seedling Fund
   [2014-62]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61373121, Grant 61036008, and Grant
   61328205, in part by the Program for Sichuan Provincial Science Fund for
   Distinguished Young Scholars under Grant 13QNJJ0149, in part by the
   Fundamental Research Funds for the Central Universities, and in part by
   the Sichuan Science and Technology Innovation Seedling Fund (2014-62).
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Marco Bertini. (Coresponding
   author: Xiao Wu.)
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   [Anonymous], P 4 INT C INT MULT C
   [Anonymous], 2012, P ACM INT C MULT
   [Anonymous], 2013, P 3 ACM C INT C MULT
   [Anonymous], 2006, P CVPR, DOI 10.1109/CVPR.2006.81
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Bourdev L, 2011, IEEE I CONF COMP VIS, P1543, DOI 10.1109/ICCV.2011.6126413
   Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505
   Chai YN, 2011, IEEE I CONF COMP VIS, P2579, DOI 10.1109/ICCV.2011.6126546
   Chen HZ, 2012, LECT NOTES COMPUT SC, V7574, P609, DOI 10.1007/978-3-642-33712-3_44
   Cour T, 2005, PROC CVPR IEEE, P1124
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Endres I, 2010, LECT NOTES COMPUT SC, V6315, P575, DOI 10.1007/978-3-642-15555-0_42
   Faktor A, 2013, IEEE I CONF COMP VIS, P1297, DOI 10.1109/ICCV.2013.164
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   GALLAGHER AC, 2008, COMP VIS PATT REC 20, P1, DOI DOI 10.1109/CVPR.2008.4587481
   Hochbaum DS, 2009, IEEE I CONF COMP VIS, P269, DOI 10.1109/ICCV.2009.5459261
   Hu ZL, 2008, PATTERN RECOGN, V41, P1581, DOI 10.1016/j.patcog.2007.10.005
   Joulin A, 2012, PROC CVPR IEEE, P542, DOI 10.1109/CVPR.2012.6247719
   Joulin A, 2010, PROC CVPR IEEE, P1943, DOI 10.1109/CVPR.2010.5539868
   Kim E, 2012, PROC CVPR IEEE, P686, DOI 10.1109/CVPR.2012.6247737
   Kim G, 2012, PROC CVPR IEEE, P837, DOI 10.1109/CVPR.2012.6247756
   Kim G, 2011, IEEE I CONF COMP VIS, P169, DOI 10.1109/ICCV.2011.6126239
   Kim TH, 2010, PROC CVPR IEEE, P3201, DOI 10.1109/CVPR.2010.5540078
   Kohli Pushmeet., 2008, CVPR, DOI DOI 10.1109/CVPR.2008.4587417
   Kolmogorov V., 2006, P IEEE CVPR, V1, P993, DOI DOI 10.1109/CVPR.2006.91
   Li HL, 2014, IEEE T CIRC SYST VID, V24, P789, DOI 10.1109/TCSVT.2013.2280851
   Li HL, 2013, IEEE T MULTIMEDIA, V15, P1896, DOI 10.1109/TMM.2013.2271476
   Li SF, 2012, PATTERN ANAL APPL, V15, P399, DOI 10.1007/s10044-011-0220-3
   Liu S, 2012, PROC CVPR IEEE, P3330, DOI 10.1109/CVPR.2012.6248071
   Meng FM, 2012, IEEE T MULTIMEDIA, V14, P1429, DOI 10.1109/TMM.2012.2197741
   Ming Yang, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P2937, DOI 10.1109/ICIP.2011.6116276
   Mukherjee Lopamudra, 2011, Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit, P1881, DOI 10.1109/CVPR.2011.5995420
   Mukherjee L, 2009, PROC CVPR IEEE, P2028, DOI 10.1109/CVPRW.2009.5206652
   O'Hare N, 2009, IEEE T MULTIMEDIA, V11, P220, DOI 10.1109/TMM.2008.2009679
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Shen XH, 2012, LECT NOTES COMPUT SC, V7575, P114, DOI 10.1007/978-3-642-33765-9_9
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   SONG Z, 2011, P INT C COMP VIS NOV, P1084
   Vicente S., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2217, DOI 10.1109/CVPR.2011.5995530
   Vicente S, 2010, LECT NOTES COMPUT SC, V6312, P465, DOI 10.1007/978-3-642-15552-9_34
   Wu Xiao, 2013, ADV MULTIMEDIA MODEL, P316
   Yamaguchi K, 2013, IEEE I CONF COMP VIS, P3519, DOI 10.1109/ICCV.2013.437
   Yamaguchi K, 2012, PROC CVPR IEEE, P3570, DOI 10.1109/CVPR.2012.6248101
   Zhang W, 2010, IEEE IMAGE PROC, P4593, DOI 10.1109/ICIP.2010.5651704
NR 45
TC 25
Z9 27
U1 1
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2016
VL 18
IS 6
BP 1111
EP 1123
DI 10.1109/TMM.2016.2537783
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DM1LP
UT WOS:000376107100014
DA 2024-07-18
ER

PT J
AU Wu, J
   Wu, J
   Cui, H
   Luo, C
   Sun, XY
   Wu, F
AF Wu, Jun
   Wu, Jian
   Cui, Hao
   Luo, Chong
   Sun, Xiaoyan
   Wu, Feng
TI DAC-Mobi: Data-Assisted Communications of Mobile Images with Cloud
   Computing Support
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Correlated image; joint source channel coding; pseudo-analog
   transmission
AB This research proposes a novel data assisted image transmission scheme, which utilizes a large amount of correlated images stored in the cloud to improve the spectrum efficiency and visual quality. First, a two-layer Coset coding is proposed for the DCT coefficients transmission. The most significant bits (MSB) of the coefficients are generated by the first layer Coset and together with a few low frequency coefficients are transmitted through the most reliable channel coding and digital modulation. The middle bits generated by the second layer Coset are discarded by the sender and the residual bits are transmitted through amplitude modulation. Based on the MSB and the residual bits, an approximation of the original image is reconstructed. With this approximation, a lot of correlated images can be retrieved from the cloud, which are used to recover the discarded middle bits. The two layer Coset coding can significantly decrease the data energy so as to improve the transmission power efficiency. Hence, the end to end distortion of amplitude modulation can be reduced. Second, the image quality can be further improved by joint internal and external denoising with the retrieved images. Simulations show that the proposed scheme outperforms conventional digital schemes about 4 dB in peak signal to noise power ratio (PSNR) and achieves 2 dB gain over the state-of-the-art uncoded transmission. At low signal to noise power ratio (SNR), an additional 2-3 dB gain is achieved. The visual quality comparison also validates the objective image assessment result.
C1 [Wu, Jun; Wu, Jian; Cui, Hao] Tongji Univ, Coll Elect & Informat Engn, Key Lab, Minist Educ Embedded Syst & Serv Comp, Shanghai 201804, Peoples R China.
   [Luo, Chong; Sun, Xiaoyan] Microsoft Res Asia, Beijing 100080, Peoples R China.
   [Wu, Feng] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230027, Peoples R China.
C3 Tongji University; Microsoft Research Asia; Microsoft; Chinese Academy
   of Sciences; University of Science & Technology of China, CAS
RP Wu, J; Wu, J; Cui, H (corresponding author), Tongji Univ, Coll Elect & Informat Engn, Key Lab, Minist Educ Embedded Syst & Serv Comp, Shanghai 201804, Peoples R China.; Luo, C; Sun, XY (corresponding author), Microsoft Res Asia, Beijing 100080, Peoples R China.; Wu, F (corresponding author), Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230027, Peoples R China.
EM wujun@tongji.edu.cn; wujiantongji@126.com; hao.cui@live.com;
   cluo@microsoft.com; xysun@microsoft.com; fengwu@ustc.edu.cn
RI Wu, Feng/KCY-3017-2024
FU National Science Foundation China [61390513, 61425026, 61502341]
FX This work was supported in part by the National Science Foundation China
   under Grant 61390513, Grant 61425026, and Grant 61502341. The guest
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Honggang Wang.
CR [Anonymous], P 20 ACM INT C MULT
   Chen TY, 2009, BMC BIOINFORMATICS, V10, DOI 10.1186/1471-2105-10-24
   Cui Hao, 2013, P ACM INT C MOD AN S, P273
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Dai LC, 2013, IEEE IMAGE PROC, P2582, DOI 10.1109/ICIP.2013.6738532
   Eitz M, 2011, IEEE COMPUT GRAPH, V31, P56, DOI 10.1109/MCG.2011.67
   Fan XP, 2012, IEEE DATA COMPR CONF, P199, DOI 10.1109/DCC.2012.27
   Hays J, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239455
   Jakubczak S., 2011, PROC MOBICOM, P289
   Johnson MK, 2011, IEEE T VIS COMPUT GR, V17, P1273, DOI 10.1109/TVCG.2010.233
   LEE KH, 1976, IEEE T COMMUN, V24, P1283
   Liu XM, 2008, CATAL COMMUN, V9, P1, DOI 10.1016/j.catcom.2007.05.020
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Sivic J., 2009, BRIT MACH VIS C LOND
   Song XD, 2014, IEEE IMAGE PROC, P4802, DOI 10.1109/ICIP.2014.7025973
   Wu F, 2014, IEEE T IMAGE PROCESS, V23, P1015, DOI 10.1109/TIP.2014.2298972
   Yue HJ, 2014, PROC CVPR IEEE, P2933, DOI 10.1109/CVPR.2014.375
   Yue HJ, 2015, IEEE T IMAGE PROCESS, V24, P1967, DOI 10.1109/TIP.2015.2412373
   Yue HJ, 2013, IEEE T MULTIMEDIA, V15, P845, DOI 10.1109/TMM.2013.2239629
NR 19
TC 18
Z9 18
U1 0
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2016
VL 18
IS 5
SI SI
BP 893
EP 904
DI 10.1109/TMM.2016.2535727
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DK5YD
UT WOS:000374996200009
DA 2024-07-18
ER

PT J
AU Kim, H
   Rhee, CE
   Lee, HJ
AF Kim, Hyun
   Rhee, Chae Eun
   Lee, Hyuk-Jae
TI A Low-Power Video Recording System With Multiple Operation Modes for
   H.264 and Light-Weight Compression
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Down-sampling; H.264/AVC compression; light-weight compression (LWC);
   low power implementation; multi operation modes; multi video codec;
   video recording system (VRS)
ID PARALLEL SPIHT
AB An increasing demand for mobile video recording systems makes it important to reduce power consumption and to increase battery lifetime. The H.264/AVC compression is widely used for many video recording systems because of its high compression efficiency; however, the complex coding structure of H.264/AVC compression requires large power consumption. A light-weight video compression (LWC), based on discrete wavelet transform and set partitioning in hierarchical trees, consumes less power than H.264/AVC compression thanks to its relatively simple coding structure, although its compression efficiency is lower than that of H.264/AVC compression. This paper proposes a low-power video recording system that combines both the H.264/AVC encoder with high compression efficiency and LWC with low power consumption. The LWC is used to compress video data for temporal storage while the H.264/AVC encoder is used for permanent storage of data when some events are detected. For further power reduction, a down-sampling operation is utilized for permanent data storage. For an effective use of the two compressions with the down-sampling operation, an appropriate scheme is selected according to the proportion of long-term to short-term storage and the target bitrate. The proposed system reduces power consumption by up to 72.5% compared to that in a conventional video recording system.
C1 [Kim, Hyun; Lee, Hyuk-Jae] Seoul Natl Univ, Dept Elect Engn, Interuniv Semicond Res Ctr, Seoul 151742, South Korea.
   [Rhee, Chae Eun] Inha Univ, Sch Informat & Commun Engn, Inchon 402751, South Korea.
C3 Seoul National University (SNU); Inha University
RP Kim, H; Lee, HJ (corresponding author), Seoul Natl Univ, Dept Elect Engn, Interuniv Semicond Res Ctr, Seoul 151742, South Korea.; Rhee, CE (corresponding author), Inha Univ, Sch Informat & Commun Engn, Inchon 402751, South Korea.
EM snusbkh0@capp.snu.ac.kr; chae.rhee@inha.ac.kr;
   hyuk_jae_lee@capp.snu.ac.kr
FU Basic Science Research Program through National Research Foundation of
   Korea (NRF) - Ministry of Science, ICT, & Future Planning
   [NRF-2015R1C1A1A02037625]; Center for Integrated Smart Sensors -
   Ministry of Education, Science and Technology as part of the Global
   Frontier Project [CISS-2012M3A6A60542 02]; Samsung Electronics
   Semiconductor Business
FX This work was supported by Basic Science Research Program through the
   National Research Foundation of Korea (NRF) funded by the Ministry of
   Science, ICT, & Future Planning (NRF-2015R1C1A1A02037625), by the Center
   for Integrated Smart Sensors funded by the Ministry of Education,
   Science and Technology as part of the Global Frontier Project
   (CISS-2012M3A6A60542 02), and by the Samsung Electronics Semiconductor
   Business.
CR [Anonymous], 2000, JPEG 2000 IMAGE COMP
   [Anonymous], JOINT SCALABLE VIDEO
   Bjontegaard G., 2001, VCEG M33 AUST TX US
   Bruckstein AM, 2003, IEEE T IMAGE PROCESS, V12, P1132, DOI 10.1109/TIP.2003.816023
   Caulfield A. M., 2009, P 42 ANN IEEE ACM IN, P24
   Chang XT, 2007, IEEE INT SYMP CIRC S, P2120, DOI 10.1109/ISCAS.2007.378591
   Changwoo Ha, 2011, Proceedings of the Seventh International Conference on Signal-Image Technology & Internet-Based Systems (SITIS 2011), P209, DOI 10.1109/SITIS.2011.23
   Chen PY, 2004, IEEE T COMPUT, V53, P386, DOI 10.1109/TC.2004.1268396
   Gualdi G, 2008, IEEE T MULTIMEDIA, V10, P1142, DOI 10.1109/TMM.2008.2001378
   Jin X, 2011, SIGNAL PROCESS-IMAGE, V26, P130, DOI 10.1016/j.image.2011.01.002
   Jin Y, 2012, IEEE T CIRC SYST VID, V22, P1064, DOI 10.1109/TCSVT.2012.2189793
   Jin Y, 2009, IEEE INT SOC CONF, P432, DOI 10.1109/SOCCON.2009.5398002
   Jubran MK, 2008, IEEE T MULTIMEDIA, V10, P1698, DOI 10.1109/TMM.2008.2007317
   Kambhatla KKR, 2012, IEEE T MULTIMEDIA, V14, P1480, DOI 10.1109/TMM.2012.2196508
   Kim H, 2015, IEEE T VLSI SYST, V23, P2685, DOI 10.1109/TVLSI.2014.2369520
   Meng L., 2010, P IEEE C MULT TECHN, P1
   Nomura Shuou, 2008, 2008 IEEE International Solid-State Circuits Conference - Digest of Technical Papers, DOI 10.1109/ISSCC.2008.4523157
   Peng Q, 2004, CCNC 2004: 1ST IEEE CONSUMER COMMUNICATIONS AND NETWORKING CONFERENCE, PROCEEDINGS, P501, DOI 10.1109/CCNC.2004.1286912
   Ren ZK, 2009, WISM: 2009 INTERNATIONAL CONFERENCE ON WEB INFORMATION SYSTEMS AND MINING, PROCEEDINGS, P270, DOI 10.1109/WISM.2009.63
   Rhee CE, 2014, IEEE T MULTIMEDIA, V16, P947, DOI 10.1109/TMM.2014.2306396
   Rhee CE, 2010, IEEE T CIRC SYST VID, V20, P1848, DOI 10.1109/TCSVT.2010.2087834
   Sabirin H, 2012, IEEE T MULTIMEDIA, V14, P657, DOI 10.1109/TMM.2012.2187777
   Said A, 1996, IEEE T CIRC SYST VID, V6, P243, DOI 10.1109/76.499834
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Wang HL, 2007, IEEE T MULTIMEDIA, V9, P882, DOI 10.1109/TMM.2007.893345
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Wu Q, 2008, 2008 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND INFORMATION TECHNOLOGY, PROCEEDINGS, P833, DOI 10.1109/MMIT.2008.214
   Wu XL, 2009, IEEE T IMAGE PROCESS, V18, P552, DOI 10.1109/TIP.2008.2010638
   Zhang Mengmeng, 2010, 2010 International Conference on Measuring Technology and Mechatronics Automation (ICMTMA 2010), P1027, DOI 10.1109/ICMTMA.2010.224
   Zhang XG, 2013, IEEE T MULTIMEDIA, V15, P1769, DOI 10.1109/TMM.2013.2280117
NR 30
TC 10
Z9 12
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2016
VL 18
IS 4
BP 603
EP 613
DI 10.1109/TMM.2016.2525861
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DH4XZ
UT WOS:000372790300005
DA 2024-07-18
ER

PT J
AU Huang, SR
   Zhang, J
   Wang, L
   Hua, XS
AF Huang, Shangrong
   Zhang, Jian
   Wang, Lei
   Hua, Xian-Sheng
TI Social Friend Recommendation Based on Multiple Network Correlation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Social network alignment; friend recommendation; feature selection
AB Friend recommendation is an important recommender application in social media. Major social websites such as Twitter and Facebook are all capable of recommending friends to individuals. However, most of these websites use simple friend recommendation algorithms such as similarity, popularity, or "friend's friends are friends," which are intuitive but consider few of the characteristics of the social network. In this paper we investigate the structure of social networks and develop an algorithm for network correlation-based social friend recommendation (NC-based SFR). To accomplish this goal, we correlate different "social role" networks, find their relationships and make friend recommendations. NC-based SFR is characterized by two key components: 1) related networks are aligned by selecting important features from each network, and 2) the network structure should be maximally preserved before and after network alignment. After important feature selection has been made, we recommend friends based on these features. We conduct experiments on the Flickr network, which contains more than ten thousand nodes and over 30 thousand tags covering half a million photos, to show that the proposed algorithm recommends friends more precisely than reference methods.
C1 [Huang, Shangrong; Zhang, Jian] Univ Technol Sydney, Adv Analyt Inst, Sch Software, Sydney, NSW 2007, Australia.
   [Wang, Lei] Univ Wollongong, Sch Comp & Informat Technol, Wollongong, NSW 2522, Australia.
   [Hua, Xian-Sheng] Alibaba Grp, Hangzhou 311121, Zhejiang, Peoples R China.
C3 University of Technology Sydney; University of Wollongong; Alibaba Group
RP Huang, SR (corresponding author), Univ Technol Sydney, Adv Analyt Inst, Sch Software, Sydney, NSW 2007, Australia.
RI Wang, Lei/AAL-9684-2020; Wang, Lei/D-9079-2013
OI Zhang, Jian/0000-0002-7240-3541; Wang, Lei/0000-0002-0961-0441
CR [Anonymous], 2012, ACM MM'12'
   Barnett E, 2001, AM J PUBLIC HEALTH, V91, P465, DOI 10.2105/AJPH.91.3.465a
   BELLMAN R, 1966, SCIENCE, V153, P34, DOI 10.1126/science.153.3731.34
   Brym R., 2009, Sociology: Your Compass for a New World
   Cai D., 2010, KDD, P333
   Chen JL, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P201, DOI 10.1145/1518701.1518735
   Chen YY, 2013, IEEE T MULTIMEDIA, V15, P1283, DOI 10.1109/TMM.2013.2265077
   Cilibrasi RL, 2007, IEEE T KNOWL DATA EN, V19, P370, DOI 10.1109/TKDE.2007.48
   Conte D, 2004, INT J PATTERN RECOGN, V18, P265, DOI 10.1142/S0218001404003228
   Fernandez-Tobias Ignacio., 2011, Proceedings of the Second International Workshop on Information Heterogeneity and Fusion in Recommender Systems as the Fifth ACM Conference on Recommender Systems, HetRec'11, P25
   Goodfellow M. H., 2010, P EDBT ICDT WORKSH M
   Gori M, 2005, IEEE T PATTERN ANAL, V27, P1100, DOI 10.1109/TPAMI.2005.138
   Hannon John, 2010, ACM RecSys'10', P199, DOI [10.1145/1864708.1864746, DOI 10.1145/1864708.1864746]
   He XF, 2004, ADV NEUR IN, V16, P153
   Jian Guo, 2006, Proceedings of the 2006 IEEE Signal Processing Society Workshop, P391
   Jiang M, 2014, IEEE T KNOWL DATA EN, V26, P2789, DOI 10.1109/TKDE.2014.2300487
   Jiang Meng., 2012, PROC 21 ACM INT C IN, P1422
   Jolliffe I.T., 1986, Principal component analysis, DOI DOI 10.1016/0169-7439(87)80084-9
   Klau G., 2009, BMC BIOINFORMATICS, V10
   Knerr S., 1990, Neurocomputing, Algorithms, Architectures and Applications. Proceedings of the NATO Advanced Research Workshop, P41
   Kosir P, 1995, PROC NAECON IEEE NAT, P94, DOI 10.1109/NAECON.1995.521918
   KREIBICH C, 2006, P INT MEAS C IMC MEL, P307
   Li B, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P2052
   Li N, 2009, P MOB 2009 MOB 09, P1
   Liu D., 2012, ACM MM'12', P659
   Liu H, 2008, CH CRC DATA MIN KNOW, P3
   Liu XW, 2014, IEEE T NEUR NET LEAR, V25, P1083, DOI 10.1109/TNNLS.2013.2287275
   Ma H., 2008, P 17 ACM C INF KNOWL, P931, DOI [DOI 10.1145/1458082.1458205, 10.1145/1458082.1458205]
   Nithiya P., 2010, Proceedings of the 2010 Second International Conference on Computer and Network Technology (ICCNT 2010), P320, DOI 10.1109/ICCNT.2010.108
   Pan W., 2011, AAAI
   Pan WK, 2010, AAAI CONF ARTIF INTE, P230
   Rendle S, 2008, RECSYS'08: PROCEEDINGS OF THE 2008 ACM CONFERENCE ON RECOMMENDER SYSTEMS, P251, DOI 10.1145/1454008.1454047
   Rummel R., 1991, CONFLICT HELIX PRINC
   Wan SX, 2013, SIGIR'13: THE PROCEEDINGS OF THE 36TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH & DEVELOPMENT IN INFORMATION RETRIEVAL, P1045
   Weber M., 1991, The Nature of Social Action
   Wu HC, 2008, ACM T INFORM SYST, V26, DOI 10.1145/1361684.1361686
   Xie X., 2010, P IEEE ACM CPSCOM DE, P831
   Yang Y., 2013, ACM MM'13', P537
   Yang Y, 2013, IEEE T MULTIMEDIA, V15, P661, DOI 10.1109/TMM.2012.2237023
   Yohan Jin, 2005, 13th Annual ACM International Conference on Multimedia, P706
   You G.-W., 2011, P 14 INT C EXT DAT T, P515
   Zhao YC, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P695
   Zhao Z, 2013, IEEE T KNOWL DATA EN, V25, P619, DOI 10.1109/TKDE.2011.222
   Zhong E., 2014, ACM T KNOWL DISCOV D, V8
   Zhuang YT, 2008, IEEE T MULTIMEDIA, V10, P221, DOI 10.1109/TMM.2007.911822
NR 45
TC 72
Z9 74
U1 2
U2 55
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2016
VL 18
IS 2
BP 287
EP 299
DI 10.1109/TMM.2015.2510333
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DB3HX
UT WOS:000368402400012
OA Green Published
DA 2024-07-18
ER

PT J
AU Li, ZC
   Tang, JH
AF Li, Zechao
   Tang, Jinhui
TI Weakly Supervised Deep Metric Learning for Community-Contributed Image
   Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep; image retrieval; metric learning; weakly supervised
ID DIMENSIONALITY REDUCTION
AB Recent years have witnessed the explosive growth of community-contributed images with rich context information, which is beneficial to the task of image retrieval. It can help us to learn a suitable metric to alleviate the semantic gap. In this paper, we propose a new distance metric learning algorithm, namely weakly-supervised deep metric learning (WDML), under the deep learning framework. It utilizes a progressive learning manner to discover knowledge by jointly exploiting the heterogeneous data structures from visual contents and user-provided tags of social images. The semantic structure in the textual space is expected to be well preserved while the problem of the noisy, incomplete or subjective tags is addressed by leveraging the visual structure in the original visual space. Besides, a sparse model with the mixed norm is imposed on the transformation matrix of the first layer in the deep architecture to compress the noisy or redundant visual features. The proposed problem is formulated as an optimization problem with a well-defined objective function and a simple yet efficient iterative algorithm is proposed to solve it. Extensive experiments on real-world social image datasets are conducted to verify the effectiveness of the proposed method for image retrieval. Encouraging experimental results are achieved compared with several representative metric learning methods.
C1 [Li, Zechao; Tang, Jinhui] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Jiangsu, Peoples R China.
C3 Nanjing University of Science & Technology
RP Tang, JH (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Jiangsu, Peoples R China.
EM zechao.li@njust.edu.cn; jinhuitang@njust.edu.cn
RI Tang, Jinhui/KBR-0891-2024
OI Tang, Jinhui/0000-0001-9008-222X
FU 973 Program [2014CB347600]; National Natural Science Foundation of China
   [61522203, 61402228]
FX This work was supported in part by the 973 Program under Project
   2014CB347600 and by the National Natural Science Foundation of China
   under Grant 61522203 and Grant 61402228. The guest editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Guo-Jun Qi. (Corresponding author: Jinhui Tang.)
CR [Anonymous], 2012, P 20 ACM INT C MULT
   [Anonymous], 2006, P 2006 IEEE COMPUTER, DOI DOI 10.1109/CVPR.2006.167
   [Anonymous], 2009, P ACM INT C IM VID R
   [Anonymous], 2009, P 17 ACM INT C MULT
   Baghshah MS, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P1217
   Bar-Hillel AB, 2005, J MACH LEARN RES, V6, P937
   Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   Davis J. V., 2007, ICML, P209
   Duda R., 2001, Pattern Recognition, V2nd
   Feng ZY, 2013, IEEE I CONF COMP VIS, P1609, DOI 10.1109/ICCV.2013.203
   Goldberger J., 2004, P INT C NEUR INF PRO, V17, P513
   Guoqiang Zhong, 2011, Proceedings of the 2011 IEEE 11th International Conference on Data Mining (ICDM 2011), P1266, DOI 10.1109/ICDM.2011.95
   Hastie T, 1996, IEEE T PATTERN ANAL, V18, P607, DOI 10.1109/34.506411
   He XF, 2004, ADV NEUR IN, V16, P153
   Hoi S.C., 2008, CVPR, P1, DOI DOI 10.1109/CVPR.2008.4587351
   Hu JL, 2014, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2014.242
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Jeffries A., 2013, THE VERGE        MAR
   Kumar N, 2008, IEEE T KNOWL DATA EN, V20, P496, DOI 10.1109/TKDE.2007.190715
   Li ZC, 2015, IEEE T PATTERN ANAL, V37, P2085, DOI 10.1109/TPAMI.2015.2400461
   Li ZC, 2013, PATTERN RECOGN, V46, P2700, DOI 10.1016/j.patcog.2013.03.016
   Liu SW, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P617, DOI 10.1145/2647868.2654905
   Ng A. Y., 2002, Advances in Neural Information Processing Systems, P1473
   Parfeni L., 2011, SOFTPEDIA        AUG
   Qi GJ, 2012, P IEEE, V100, P2688, DOI 10.1109/JPROC.2012.2201909
   Qi GJ, 2012, IEEE T PATTERN ANAL, V34, P850, DOI 10.1109/TPAMI.2011.191
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Wan J, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P157, DOI 10.1145/2647868.2654948
   Wang J, 2014, PROC CVPR IEEE, P1386, DOI 10.1109/CVPR.2014.180
   Wang Jun., 2011, ADV NEURAL INFORM PR, P1170
   Wu L, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1899412.1899417
   Wu P., 2013, P 21 ACM INT C MULT, P153, DOI DOI 10.1145/2502081.2502112
   Wu Pengcheng., 2011, Proceedings of the Fourth 140 ACM International Conference on Web Search and Data Mining, WSDM, P197, DOI DOI 10.1145/1935826.1935865
   Xia H, 2014, IEEE T PATTERN ANAL, V36, P536, DOI 10.1109/TPAMI.2013.149
   Ying YM, 2012, J MACH LEARN RES, V13, P1
   Zha ZJ, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P1327
   Zhang Z., 2003, Proceedings of the 18th international joint conference on Artificial intelligence, V18, P1450
NR 39
TC 169
Z9 172
U1 0
U2 40
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2015
VL 17
IS 11
SI SI
BP 1989
EP 1999
DI 10.1109/TMM.2015.2477035
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CV2QV
UT WOS:000364102400012
DA 2024-07-18
ER

PT J
AU He, DL
   Luo, C
   Lan, CL
   Wu, F
   Zeng, WJ
AF He, Dongliang
   Luo, Chong
   Lan, Cuiling
   Wu, Feng
   Zeng, Wenjun
TI Structure-Preserving Hybrid Digital-Analog Video Delivery in Wireless
   Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Hybrid digital-analog (HDA); joint source channel coding; structural
   similarity; wireless video delivery
AB Hybrid digital-analog (HDA) transmission has gained increasing attention recently in the context of wireless video delivery, for its ability to simultaneously achieve high transmission efficiency and smooth quality adaptation. However, previous systems are optimized solely based on the mean squared error criterion without taking the perceptual video quality into consideration. In this work, we propose a structure-preserving HDA video delivery system, named SharpCast, to improve both the objective and subjective visual quality. SharpCast decomposes a video into a content part and structure part. The latter is important to the human perception and therefore is protected with a robust digital transmission scheme. Then, the energy-intensive part in the content information is extracted and transmitted in digital for energy efficiency while the residual is transmitted in analog to achieve the desired smooth adaptation. We formulate the resource (power and bandwidth) allocation problem in SharpCast and solve the problem with a greedy strategy. Evaluations over nine standard 720p video sequences show that the proposed SharpCast system outperforms the state-of-the-art digital, analog, and HDA schemes by a notable margin in both peak signal-to-noise ratio (PSNR) and structural similarity (SSIM).
C1 [He, Dongliang; Wu, Feng] Univ Sci & Technol China, Hefei 230026, Peoples R China.
   [Luo, Chong; Lan, Cuiling; Zeng, Wenjun] Microsoft Res Asia, Beijing 100080, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Microsoft Research Asia; Microsoft
RP Luo, C (corresponding author), Univ Sci & Technol China, Hefei 230026, Peoples R China.
EM hedl@mail.ustc.edu.cn; cluo@microsoft.com; culan@microsoft.com;
   fengwu@ustc.edu.cn; wezeng@microsoft.com
RI Lan, Cuiling/KCK-5597-2024; Wu, Feng/KCY-3017-2024
CR [Anonymous], 2005, 8802112005E ISOIEC
   [Anonymous], 2014, HEVC REFERENCE IMPLE
   Cover Thomas M, 1999, Elements of information theory
   Cui Hao, 2013, P ACM INT C MOD AN S, P273
   Fan XP, 2012, IEEE DATA COMPR CONF, P199, DOI 10.1109/DCC.2012.27
   Gonzalez R C, 2008, DIGITAL IMAGE PROCES
   Jakubczak S., 2011, PROC MOBICOM, P289
   Kamaci N, 2005, IEEE T CIRC SYST VID, V15, P994, DOI 10.1109/TCSVT.2005.852400
   Katto J., 1995, Proceedings. International Conference on Image Processing (Cat. No.95CB35819), P555, DOI 10.1109/ICIP.1995.537539
   Kratochvil T., 2008, P 2008 INT S INF THE, P1
   LANGDON GG, 1984, IBM J RES DEV, V28, P135, DOI 10.1147/rd.282.0135
   Li WP, 2001, IEEE T CIRC SYST VID, V11, P301, DOI 10.1109/76.911157
   Liu HF, 2014, 2014 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING CONFERENCE, P141, DOI 10.1109/VCIP.2014.7051524
   Liu Hongda., 2014, The optimal capacity and economic analysis of micro-grid on island, P1, DOI [DOI 10.1109/PECI.2014.6804573, 10.1109/APSIPA.2014.7041709]
   Liu XL, 2014, IEEE T MULTIMEDIA, V16, P2038, DOI 10.1109/TMM.2014.2331616
   Liu XL, 2012, MOBICOM 12: PROCEEDINGS OF THE 18TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P233
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xiong RG, 2014, IEEE DATA COMPR CONF, P133, DOI 10.1109/DCC.2014.55
   Yu L, 2014, IEEE T CIRC SYST VID, V24, P331, DOI 10.1109/TCSVT.2013.2273675
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
NR 25
TC 34
Z9 34
U1 1
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2015
VL 17
IS 9
SI SI
BP 1658
EP 1670
DI 10.1109/TMM.2015.2451956
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CP0RN
UT WOS:000359583000023
DA 2024-07-18
ER

PT J
AU Xie, J
   Feris, RS
   Yu, SS
   Sun, MT
AF Xie, Jun
   Feris, Rogerio Schmidt
   Yu, Shiaw-Shian
   Sun, Ming-Ting
TI Joint Super Resolution and Denoising From a Single Depth Image
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Coupled dictionary learning; depth image; L0 smoothing; shock filter;
   super resolution
ID SUPERRESOLUTION; ENHANCEMENT
AB This paper describes a new algorithm for depth image super resolution and denoising using a single depth image as input. A robust coupled dictionary learning method with locality coordinate constraints is introduced to reconstruct the corresponding high resolution depth map. The local constraints effectively reduce the prediction uncertainty and prevent the dictionary from over-fitting. We also incorporate an adaptively regularized shock filter to simultaneously reduce the jagged noise and sharpen the edges. Furthermore, a joint reconstruction and smoothing framework is proposed with an L0 gradient smooth constraint, making the reconstruction more robust to noise. Experimental results demonstrate the effectiveness of our proposed algorithm compared to previously reported methods.
C1 [Xie, Jun; Sun, Ming-Ting] Univ Washington, Dept Elect Engn, Seattle, WA 98195 USA.
   [Feris, Rogerio Schmidt] IBM TJ Watson Res Ctr, Yorktown Hts, NY 10532 USA.
   [Yu, Shiaw-Shian] Ind Technol Res Inst, Computat Intelligence Technol Ctr, Hsinchu 31040, Taiwan.
C3 University of Washington; University of Washington Seattle;
   International Business Machines (IBM); Industrial Technology Research
   Institute - Taiwan
RP Xie, J (corresponding author), Univ Washington, Dept Elect Engn, Seattle, WA 98195 USA.
EM junx@u.washington.edu; rsferis@us.ibm.com; ssyu@itri.org.tw;
   mts@u.washington.edu
CR [Anonymous], P SPIE
   [Anonymous], 2014, IEEE International Conference on Multimedia and Expo
   [Anonymous], 2009, P ADV NEUR INF PROC
   [Anonymous], OPTICAL ENG
   Aubry Mathieu, 2014, ACM T GRAPH, V33
   Belhedi A, 2012, LECT NOTES COMPUT SC, V7585, P476, DOI 10.1007/978-3-642-33885-4_48
   Chakrabarti A, 2007, IEEE T MULTIMEDIA, V9, P888, DOI 10.1109/TMM.2007.893346
   Chang H, 2004, PROC CVPR IEEE, P275, DOI 10.1109/cvpr.2004.1315043
   Cho S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618491
   Criminisi A, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1857907.1857910
   Cui Y, 2013, IEEE T PATTERN ANAL, V35, P1039, DOI 10.1109/TPAMI.2012.190
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Farbman Z, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866171
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Freeman WT, 2002, IEEE COMPUT GRAPH, V22, P56, DOI 10.1109/38.988747
   Fu JJ, 2012, IEEE INT SYMP CIRC S, P512, DOI 10.1109/ISCAS.2012.6272078
   Gao XB, 2012, IEEE T IMAGE PROCESS, V21, P469, DOI 10.1109/TIP.2011.2161482
   Gilboa G, 2004, IEEE T PATTERN ANAL, V26, P1020, DOI 10.1109/TPAMI.2004.47
   Glasner D, 2009, IEEE I CONF COMP VIS, P349, DOI 10.1109/ICCV.2009.5459271
   HaCohen Y., 2010, ICCP, P1
   Hornácek M, 2013, PROC CVPR IEEE, P1123, DOI 10.1109/CVPR.2013.149
   Huhle Benjamin, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563158
   Izadi S, 2011, P UIST, P559, DOI DOI 10.1145/2047196.2047270
   Jain AK, 2011, INT CONF ACOUST SPEE, P889
   Kim KI, 2010, IEEE T PATTERN ANAL, V32, P1127, DOI 10.1109/TPAMI.2010.25
   Kopf J, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239453
   Lang M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185530
   Levin A, 2008, LECT NOTES COMPUT SC, V5305, P88, DOI 10.1007/978-3-540-88693-8_7
   Li J, 2014, PROC CVPR IEEE, P3374, DOI 10.1109/CVPR.2014.431
   Liu MY, 2013, PROC CVPR IEEE, P169, DOI 10.1109/CVPR.2013.29
   Lu JB, 2011, INT CONF ACOUST SPEE, P985
   Mac Aodha O, 2012, LECT NOTES COMPUT SC, V7574, P71, DOI 10.1007/978-3-642-33712-3_6
   Mahmoudi M, 2012, IEEE T IMAGE PROCESS, V21, P2909, DOI 10.1109/TIP.2012.2185940
   OSHER S, 1990, SIAM J NUMER ANAL, V27, P919, DOI 10.1137/0727053
   Paris S, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964963
   Park J, 2011, IEEE I CONF COMP VIS, P1623, DOI 10.1109/ICCV.2011.6126423
   Rajagopalan AN, 2008, LECT NOTES COMPUT SC, V5096, P304, DOI 10.1007/978-3-540-69321-5_31
   Reynolds M, 2011, PROC CVPR IEEE, P945, DOI 10.1109/CVPR.2011.5995550
   Salem F, 2013, IEEE T MULTIMEDIA, V15, P27, DOI 10.1109/TMM.2012.2225037
   Scharstein D, 2001, IEEE WORKSHOP ON STEREO AND MULTI-BASELINE VISION, PROCEEDINGS, P131, DOI 10.1023/A:1014573219977
   Schuon S, 2009, PROC CVPR IEEE, P343, DOI 10.1109/CVPRW.2009.5206804
   Stefanoski N, 2013, IEEE IMAGE PROC, P1247, DOI 10.1109/ICIP.2013.6738257
   Sun JA, 2010, PROC CVPR IEEE, P231, DOI 10.1109/CVPR.2010.5540206
   Takeda H, 2008, IEEE T IMAGE PROCESS, V17, P550, DOI 10.1109/TIP.2007.918028
   Tang Y, 2011, INT J MACH LEARN CYB, V2, P15, DOI 10.1007/s13042-011-0011-6
   Tosic I, 2014, IEEE T IMAGE PROCESS, V23, P2122, DOI 10.1109/TIP.2014.2312645
   Tosic I, 2011, IEEE J-STSP, V5, P941, DOI 10.1109/JSTSP.2011.2158063
   Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108
   Tsai Cheng-Wang, 2012, Proceedings of the 2012 International Conference on Computer Science and Information Processing (CSIP), P1, DOI 10.1109/CSIP.2012.6308779
   Vacavant A, 2012, INT C PATT RECOG, P182
   Wang SL, 2012, PROC CVPR IEEE, P2216, DOI 10.1109/CVPR.2012.6247930
   Xie J, 2014, IEEE IMAGE PROC, P3773, DOI 10.1109/ICIP.2014.7025766
   Xiong ZW, 2013, IEEE T MULTIMEDIA, V15, P1458, DOI 10.1109/TMM.2013.2264654
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Yang CY, 2013, IEEE I CONF COMP VIS, P561, DOI 10.1109/ICCV.2013.75
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yang MC, 2013, IEEE T MULTIMEDIA, V15, P498, DOI 10.1109/TMM.2012.2232646
   Yang Q., 2007, PROC IEEE C COMPUT V, P1
   Yanjie Li, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P152, DOI 10.1109/ICME.2012.30
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   ZHANG Z, 2011, ADV NEURAL INFORM PR, V24, P1611
NR 61
TC 83
Z9 92
U1 0
U2 37
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2015
VL 17
IS 9
SI SI
BP 1525
EP 1537
DI 10.1109/TMM.2015.2457678
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CP0RN
UT WOS:000359583000013
DA 2024-07-18
ER

PT J
AU Vliegendhart, R
   Larson, M
   Loni, B
   Hanjalic, A
AF Vliegendhart, Raynor
   Larson, Martha
   Loni, Babak
   Hanjalic, Alan
TI Exploiting the Deep-Link Commentsphere to Support Non-Linear Video
   Access
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Crowdsourcing; deep links; non-linear video access; relevance criteria;
   video retrieval
AB In this paper, we investigate the usefulness of deep links for improving video search results. Deep links are time-coded comments with which viewers express their reactions to the content at specific time-points of a video that they find noteworthy. The rationale underlying our work is that deep links can open up an interesting new perspective on the relevance of a video, namely focusing on individual video segments, in addition to the existing ones that typically concern a video as a whole. In this perspective, deep-link comments provide non-linear access to videos via their time-codes, which can match alternate dimensions of user needs that extend beyond topical and affective relevance. We explore the different types of deep-link comments and develop a viewer expressive reaction variety (VERV) typology that captures how viewers deep-link on YouTube. We validate this typology through a user study on Amazon Mechanical Turk to show that it is a typology human annotators can agree upon. We then demonstrate, through experiments, that deep-link comments can automatically be classified into VERV categories and show the potential of our proposed usage of deep-link comments for video search through a user study.
C1 [Vliegendhart, Raynor; Larson, Martha; Loni, Babak; Hanjalic, Alan] Delft Univ Technol, Intelligent Syst INSY Dept, NL-2628 CD Delft, Netherlands.
C3 Delft University of Technology
RP Vliegendhart, R (corresponding author), Delft Univ Technol, Intelligent Syst INSY Dept, NL-2628 CD Delft, Netherlands.
EM R.Vliegendhart@tudelft.nl; M.A.Larson@tudelft.nl; B.Loni@tudelft.nl;
   a.hanjalic@tudelft.nl
OI Hanjalic, Alan/0000-0002-5771-2549
FU European Commission [287704]
FX This work was supported by the European Commission's 7th Framework
   Programme under Grant 287704 (CUbRIK). The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Jiebo Luo.
CR [Anonymous], 2005, P JOENS LEARN INSTR
   [Anonymous], 2008, INTRO INFORM RETRIEV, DOI DOI 10.1017/CBO9780511809071
   Chelaru S., 2013, WORLD WIDE WEB, V17, P1
   Chiao-Fang Hsu, 2009, 2009 International Conference on Computational Science and Engineering (CSE), P90, DOI 10.1109/CSE.2009.109
   Chorianopoulos K., 2011, P 9 INT INT C INT TE, P25
   Guimaraes R., 2012, P 18 BRAZ S MULT WEB, P253, DOI DOI 10.1145/2382636.2382690
   Hall M., 2009, ACM SIGKDD Explor. Newsl, V11, P18, DOI DOI 10.1145/1656274.1656278
   Hanjalic A, 2005, IEEE T MULTIMEDIA, V7, P143, DOI 10.1109/TMM.2004.840618
   Hanjalic A, 1999, IEEE T CIRC SYST VID, V9, P580, DOI 10.1109/76.767124
   Hanjalic Alan., 2012, P 20 ACM INT C MULT, P1239
   Kordumova S., 2014, P INT C MULT RETR NE
   Li H, 2009, INT CONF DAT MIN WOR, P164, DOI 10.1109/ICDMW.2009.46
   Lui M., 2012, 50 ANN M ASS COMP LI
   Madden A., 2013, J DOCUMENTA IN PRESS
   Potthast M, 2012, ACM T INTEL SYST TEC, V3, DOI 10.1145/2337542.2337553
   Rugg G, 1997, EXPERT SYST, V14, P80, DOI 10.1111/1468-0394.00045
   Shamma DA., 2007, P INT WORKSHOP WORKS, P275, DOI [10.1145/1290082.1290120, DOI 10.1145/1290082.1290120]
   Siersdorfer S, 2014, ACM T WEB, V8, DOI 10.1145/2628441
   Snoek Cees G. M., 2008, Foundations and Trends in Information Retrieval, V2, P215, DOI 10.1561/1500000014
   Vliegendhart R., 2012, P 20 ACM INT C MULT, P1271
   Vliegendhart R., 2013, P 21 ACM INT C MULT, P517
   Vliegendhart R., 2012, CROWDSEARCH 2012 1 I, P54
   Vliegendhart R., 2011, P WSDM 11 WORKSH CRO
   Wakamiya S, 2011, MULTIMED TOOLS APPL, V54, P7, DOI 10.1007/s11042-010-0531-1
   Witten I. H., 2005, DATA MINING PRACTICA
   Yang M, 2004, P ASIST ANNU, V41, P229, DOI 10.1002/meet.1450410127
NR 26
TC 1
Z9 1
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2015
VL 17
IS 8
BP 1372
EP 1384
DI 10.1109/TMM.2015.2449086
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN2EI
UT WOS:000358233000021
DA 2024-07-18
ER

PT J
AU Zhou, YP
   Chen, L
   Yang, CF
   Chiu, DM
AF Zhou, Yipeng
   Chen, Liang
   Yang, Chunfeng
   Chiu, Dah Ming
TI Video Popularity Dynamics and Its Implication for Replication
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dynamic video popularity; lifetime; video cache; video replication
ID STATE
AB Popular online video-on-demand (VoD) services all maintain a large catalog of videos for their users to access. The knowledge of video popularity is very important for system operation, such as video caching on content distribution network (CDN) servers. The video popularity distribution at a given time is quite well understood. We study how the video popularity changes with time, for different types of videos, and apply the results to design video caching strategies. Our study is based on analyzing the video access levels over time, based on data provided by a large video service provider. Our main finding is, while there are variations, the glory days of a video's popularity typically pass by quickly and the probability of replaying a video by the same user is low. The reason appears to be due to fairly regular number of users and view time per day for each user, and continuous arrival of new videos. All these facts will affect how video popularity changes, hence also affect the optimal video caching strategy. Based on the observation from our measurement study, we propose a mixed replication strategy (of LFU and FIFO) that can handle different kinds of videos. Offline strategy assuming tomorrow's video popularity is known in advance is used as a performance benchmark. Through trace-driven simulation, we show that the caching performance achieved by the mixed strategy is very close to the performance achieved by the offline strategy.
C1 [Zhou, Yipeng] Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518000, Peoples R China.
   [Chen, Liang] Shenzhen Univ, Coll Informat Engn, Shenzhen 518000, Peoples R China.
   [Yang, Chunfeng; Chiu, Dah Ming] Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong 999077, Hong Kong, Peoples R China.
C3 Shenzhen University; Shenzhen University; Chinese University of Hong
   Kong
RP Chen, L (corresponding author), Shenzhen Univ, Coll Informat Engn, Shenzhen 518000, Peoples R China.
EM ypzhou@szu.edu.cn; lchen@szu.edu.cn; yc012@ie.cuhk.edu.hk;
   dmchiu@ie.cuhk.edu.hk
RI Chiu, Dah Ming/F-1885-2011; Liu, Yiming/ISU-3780-2023
OI Zhou, Yipeng/0000-0003-1533-0865
FU Foundation of Shenzhen City [KQCX20140519103756206]; Hong Kong RGC
   [14201814]; Natural Science Foundation of China [61402297]
FX This work was supported in part by the Foundation of Shenzhen City under
   Grant KQCX20140519103756206, in part by the Hong Kong RGC under Grant
   14201814, and in part by the Natural Science Foundation of China under
   Grant 61402297. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Yiannis
   Andreopoulos. (Corresponding author: Liang Chen.)
CR Allocca Kevin, 2011, TED TALK
   Avramova Z, 2009, 2009 FIRST INTERNATIONAL CONFERENCE ON EVOLVING INTERNET (INTERNET 2009), P95, DOI 10.1109/INTERNET.2009.22
   Benevenuto F, 2009, ACM T MULTIM COMPUT, V5, DOI 10.1145/1596990.1596994
   Brodersen Anders, 2012, P 21 INT C WORLD WID, P241, DOI DOI 10.1145/2187836.2187870
   Cha M, 2007, IMC'07: PROCEEDINGS OF THE 2007 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P1
   Cha M, 2009, IEEE ACM T NETWORK, V17, P1357, DOI 10.1109/TNET.2008.2011358
   Chen L, 2014, IEEE IJCNN, P1, DOI 10.1109/IJCNN.2014.6889506
   Chen YS, 2013, IEEE T MULTIMEDIA, V15, P2087, DOI 10.1109/TMM.2013.2280123
   Finamore A., 2011, ACM IMC, P345, DOI DOI 10.1145/2068816.2068849
   Hofmann Markus., 2005, Content Networking: Architecture, Protocols, and Practice
   Hongliang Yu, 2006, Operating Systems Review, V40, P333, DOI 10.1145/1218063.1217968
   Li ZG, 2012, PLANT SCI, V185, P185, DOI 10.1016/j.plantsci.2011.10.006
   Lin CL, 2013, IEEE NANOTECHNOL MAT, P1, DOI [10.1109/NMDC.2013.6707460, 10.1109/PLASMA.2013.6633193]
   Martina V., 2013, CORR
   Pathan M, 2008, LECT NOTES ELECTR EN, V9, P3
   Ratkiewicz J, 2010, PHYS REV LETT, V105, DOI 10.1103/PhysRevLett.105.158701
   Rosensweig EJ, 2013, IEEE INFOCOM SER, P863
   Scellato S., 2011, Proceedings of the 20th International Conference on World Wide Web, P457
   Shamma David A, 2011, P ICWSM
   Szabo G, 2010, COMMUN ACM, V53, P80, DOI 10.1145/1787234.1787254
   Traverso S, 2013, ACM SIGCOMM COMP COM, V43, P6
   Wu D, 2009, IEEE INFOCOM SER, P73, DOI 10.1109/INFCOM.2009.5061908
   Yang J., 2011, P 4 ACM INT C WEB SE, P177, DOI DOI 10.1145/1935826.1935863
   Zhou Renjie, 2010, P 10 ACM SIGCOMM C I, P404, DOI DOI 10.1145/1879141.1879193
NR 24
TC 79
Z9 86
U1 1
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2015
VL 17
IS 8
BP 1273
EP 1285
DI 10.1109/TMM.2015.2447277
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN2EI
UT WOS:000358233000013
DA 2024-07-18
ER

PT J
AU Liao, S
   Li, XR
   Shen, HT
   Yang, Y
   Du, XY
AF Liao, Shuai
   Li, Xirong
   Shen, Heng Tao
   Yang, Yang
   Du, Xiaoyong
TI Tag Features for Geo-Aware Image Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Geo tags; geo-aware image classification; tag features
ID RECOMMENDATION; PHOTOS; EVENT
AB The use of geo tags in recording the location at which a picture was taken is becoming part of image metadata. Therefore, studying approaches to image classification that can favorably exploit both geo tags and the underlying geo context has become an emerging topic. This paper contributes to geo-aware image classification by studying how to encode geo information into image representation. Given a geo-tagged image, we propose to extract geo-aware tag features by tag propagation from the geo and visual neighbors of the given image. Depending on what neighbors are used and how they are weighted, we present and compare eight variants of geo-aware tag features. Using millions of Flickr images as source data for tag feature extraction, experiments on a popular benchmark set justify the effectiveness and robustness of the proposed tag features for geo-aware image classification.
C1 [Liao, Shuai; Li, Xirong; Du, Xiaoyong] Renmin Univ China, Key Lab Data Engn & Knowledge Engn, Beijing 100872, Peoples R China.
   [Li, Xirong] Shanghai Key Lab Intelligent Informat Proc, Shanghai 200443, Peoples R China.
   [Shen, Heng Tao] Univ Queensland, Sch Informat Technol & Elect Engn, Brisbane, Qld 4072, Australia.
   [Yang, Yang] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
C3 Renmin University of China; University of Queensland; University of
   Electronic Science & Technology of China
RP Liao, S (corresponding author), Renmin Univ China, Key Lab Data Engn & Knowledge Engn, Beijing 100872, Peoples R China.
EM xirong.li@gmail.com
RI Lang, Ming/HIK-0758-2022; yang, yang/HGT-7999-2022; yang,
   yang/GVT-5210-2022; Li, Xirong/AAD-3347-2019; Shen, Heng
   Tao/ABD-5331-2021
OI Li, Xirong/0000-0002-0220-8310; 
FU NSFC [61303184]; Fundamental Research Funds for the Central
   Universities; Research Funds of Renmin University of China [14XNLQ01];
   Specialized Research Fund for the Doctoral Program of Higher Education
   [20130004120006]; Scientific Research Foundation for the Returned
   Overseas Chinese Scholars, State Education Ministry; Shanghai Key
   Laboratory of Intelligent Information Processing, China [IIPL-2014-002]
FX This work was supported in part by the NSFC under Grant 61303184, the
   Fundamental Research Funds for the Central Universities and the Research
   Funds of Renmin University of China under Grant 14XNLQ01, the
   Specialized Research Fund for the Doctoral Program of Higher
   Educationunder Grant 20130004120006, the Scientific Research Foundation
   for the Returned Overseas Chinese Scholars, State Education Ministry,
   and the Shanghai Key Laboratory of Intelligent Information Processing,
   China under Grant IIPL-2014-002. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof. K.
   Selcuk Candan.
CR [Anonymous], P IEEE INT C MULT EX
   [Anonymous], 2008, PROC AMERICASCONF IN
   [Anonymous], 2013, PROC THE 21 ACM INT
   [Anonymous], CLEF WORKING NOTES
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], MEDIAEVAL WORKING NO
   [Anonymous], 2009, P ACM INT C IM VID R
   Cao LL, 2009, IEEE T MULTIMEDIA, V11, P208, DOI 10.1109/TMM.2008.2009693
   Chen TY, 2013, IEEE INT CONF COMMUN, P11, DOI 10.1109/ICCChinaW.2013.6670558
   Chua T.-S., 2009, CIVR, P1, DOI 10.1145/1646396.1646452
   Datta R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1348246.1348248
   Guillaumin M, 2010, PROC CVPR IEEE, P902, DOI 10.1109/CVPR.2010.5540120
   Hays J, 2008, PROC CVPR IEEE, P3436
   Joshi D., 2008, P INT C CONTENT BASE, P37
   Kennedy Lyndon., 2007, Proceedings of the 15th International Conference on Multimedia, P631, DOI DOI 10.1145/1291233.1291384
   Li J, 2013, IEEE T MULTIMEDIA, V15, P2058, DOI 10.1109/TMM.2013.2280127
   Li X., 2015, CORR
   Li XR, 2013, IEEE T MULTIMEDIA, V15, P933, DOI 10.1109/TMM.2013.2238523
   Liu J, 2014, IEEE T MULTIMEDIA, V16, P588, DOI 10.1109/TMM.2014.2302732
   Maji S., 2008, CVPR
   Merler M, 2012, IEEE T MULTIMEDIA, V14, P88, DOI 10.1109/TMM.2011.2168948
   Moxley Emily., 2008, Proceedings of the 1st ACM International Conference on Multimedia Information Retrieval, P24
   Qian XM, 2014, IEEE T KNOWL DATA EN, V26, P1763, DOI 10.1109/TKDE.2013.168
   Qian XM, 2013, NEUROCOMPUTING, V111, P144, DOI 10.1016/j.neucom.2012.12.021
   Rakotomamonjy A, 2008, J MACH LEARN RES, V9, P2491
   Rasiwasia N, 2007, IEEE T MULTIMEDIA, V9, P923, DOI 10.1109/TMM.2007.900138
   Serdyukov P, 2009, PROCEEDINGS 32ND ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P484, DOI 10.1145/1571941.1572025
   Sizov S., 2010, P 3 ACM INT C WEB SE, P281, DOI DOI 10.1145/1718487.1718522
   Smeulders A. W. M., 2012, PACIFIC ASIA C KNOWL
   Wang G, 2009, PROC CVPR IEEE, P1367, DOI 10.1109/CVPRW.2009.5206816
   Wang M, 2012, IEEE T IMAGE PROCESS, V21, P4649, DOI 10.1109/TIP.2012.2207397
   Wang XJ, 2010, PROC CVPR IEEE, P2987, DOI 10.1109/CVPR.2010.5540046
   Yaegashi Keita, 2010, Computer Vision - ACCV 2010. 10th Asian Conference on Computer Vision. Revised Selected Papers, P360, DOI 10.1007/978-3-642-19309-5_28
   Yang Y, 2014, IEEE T MULTIMEDIA, V16, P1677, DOI 10.1109/TMM.2014.2323014
   Yang Y, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2457450.2457456
   Zhang Haipeng., 2012, Proceedings of the fifth ACM International Conference on Web Search and Data Mining, P33
   Zielstra D, 2013, J SPAT SCI, V58, P251, DOI 10.1080/14498596.2013.801331
NR 37
TC 19
Z9 19
U1 2
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2015
VL 17
IS 7
BP 1058
EP 1067
DI 10.1109/TMM.2015.2436057
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CK8XC
UT WOS:000356522300012
DA 2024-07-18
ER

PT J
AU Saki, H
   Shikh-Bahaei, M
AF Saki, Hadi
   Shikh-Bahaei, Mohammad
TI Cross-Layer Resource Allocation for Video Streaming Over OFDMA Cognitive
   Radio Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cognitive radio; cross-layer; H.264/AVC; imperfect channel state
   information (CSI); MQAM/OFDM; probabilistic interference; quality-aware
   (QA); resource allocation; scalable video codec (SVC)
ID POWER ALLOCATION; SERVICES; LIMITS; SUM
AB In this paper, a cross-layer resource allocation algorithm is proposed in the context of orthogonal frequency division multiple access (OFDMA)-based cognitive radio (CR) video application systems. User video quality and channel awareness are incorporated in the design, towards an optimal subcarrier and power allocation scheme, subject to minimum secondary receiver (SRx) video quality and primary receiver (PRx) interference threshold constraints. First, the relationship between PRx interference margin and power limits at the secondary transmitter (STx) is analytically derived. Then, to provide PRx with satisfactory quality of service (QoS), we propose a new probabilistic approach to mitigate the total imposed interference of the STx on PRx, considering imperfect STx to PRx channel state information (CSI). The effect of PRx interference limit violation probability, interference threshold, and error variance of imperfect CSI are evaluated through mathematical analysis and computer simulations. Results indicate that significant improvement in SRx total video quality is achieved through our proposed quality-aware (QA) algorithm over other state-of-the-art non-quality-aware (NQA) designs in the literature. The enhanced performance was obtained whilst guaranteeing SRx minimum quality and PRx prescribed QoS constraints.
C1 [Saki, Hadi; Shikh-Bahaei, Mohammad] Kings Coll London, Ctr Telecommun Res CTR, London WC2R 2LS, England.
   [Saki, Hadi] Univ Kingston, Fac Sci Engn & Comp, Kingston Upon Thames KT1 2EE, Surrey, England.
C3 University of London; King's College London; Kingston University
RP Saki, H (corresponding author), Kings Coll London, Ctr Telecommun Res CTR, London WC2R 2LS, England.
EM hadi.saki@kcl.ac.uk; m.sbahaei@kcl.ac.uk
OI Shikh-Bahaei, Mohammad/0000-0001-7450-7574
CR Ahmed MH, 2003, IEEE VTS VEH TECHNOL, P1554, DOI 10.1109/VETECF.2003.1285285
   Akyildiz IF, 2006, COMPUT NETW, V50, P2127, DOI 10.1016/j.comnet.2006.05.001
   Almalfouh SM, 2011, IEEE T VEH TECHNOL, V60, P1699, DOI 10.1109/TVT.2011.2126613
   Alzer H, 1997, MATH COMPUT, V66, P771, DOI 10.1090/S0025-5718-97-00814-4
   [Anonymous], 2014, JSVM SOFTW MAN JSVM
   Atta-ur-Rahman Q.I., 2012, WORLD APPL SCI J, V18, P836, DOI DOI 10.5829/idosi.wasj.2012.18.06.906
   Ba1nsal G., 2011, THESIS U BRIT COLUMB
   Bansal G, 2008, IEEE T WIREL COMMUN, V7, P4710, DOI 10.1109/T-WC.2008.07091
   Bobarshad H, 2010, IEEE T MULTIMEDIA, V12, P427, DOI 10.1109/TMM.2010.2050734
   Bocus MZ, 2011, EURASIP J WIREL COMM, DOI 10.1155/2011/245673
   Castaño-Martínez A, 2005, TEST-SPAIN, V14, P397, DOI 10.1007/BF02595410
   COX DR, 1987, CAN J STAT, V15, P105, DOI 10.2307/3315199
   Dai M, 2006, IEEE T MULTIMEDIA, V8, P1135, DOI 10.1109/TMM.2006.884626
   Dashti M, 2012, IET COMMUN, V6, P2543, DOI 10.1049/iet-com.2011.0822
   Gabler S., 1987, Statistical Papers, V28, P317, DOI DOI 10.1007/BF02932611
   Ghasemi A, 2007, IEEE T WIREL COMMUN, V6, P649, DOI 10.1109/TWC.2007.05447
   Gupta R, 2012, IEEE T BROADCAST, V58, P428, DOI 10.1109/TBC.2012.2191702
   Hanzo L., 2007, VOICE COMPRESSION CO
   He YY, 2012, IEEE T VEH TECHNOL, V61, P1287, DOI 10.1109/TVT.2012.2186597
   Huang JW, 2009, IEEE T WIREL COMMUN, V8, P288, DOI 10.1109/T-WC.2009.071266
   Li ZC, 2009, IEEE T CIRC SYST VID, V19, P917, DOI 10.1109/TCSVT.2009.2022806
   Mitola Joseph., 2000, SCI AM, V294, P66
   Mokari N, 2011, IEEE T WIREL COMMUN, V10, P3482, DOI 10.1109/TWC.2011.072511.110020
   Nehra K, 2011, IEEE T VEH TECHNOL, V60, P1240, DOI 10.1109/TVT.2010.2101091
   Ngo DT, 2010, IEEE T VEH TECHNOL, V59, P1668, DOI 10.1109/TVT.2010.2042827
   REICHEL J, 2006, JOINT SCALABLE VIDEO
   Rezki Z, 2012, IEEE T VEH TECHNOL, V61, P2108, DOI 10.1109/TVT.2012.2195042
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Shojaeifard A, 2011, IEEE T WIREL COMMUN, V10, P3278, DOI 10.1109/TWC.2011.080311.101301
   SOLOMON H, 1977, J AM STAT ASSOC, V72, P881, DOI 10.2307/2286480
   Srinivasa S, 2008, IEEE T WIREL COMMUN, V7, P4010, DOI 10.1109/T-WC.2008.070647
   Sun J, 2009, IEEE T CIRC SYST VID, V19, P323, DOI 10.1109/TCSVT.2009.2013494
   Suraweera HA, 2010, IEEE T VEH TECHNOL, V59, P1811, DOI 10.1109/TVT.2010.2043454
   Taha A.-E.M., 2011, LTE, LTE-Advanced and WiMAX: Towards IMT-Advanced Networks, V1st
   Tan CK, 2011, IET COMMUN, V5, P1607, DOI 10.1049/iet-com.2010.1021
   Wang R, 2009, IEEE T WIREL COMMUN, V8, P2410, DOI 10.1109/TWC.2009.071147
   Wang SW, 2010, IEEE COMMUN LETT, V14, P725, DOI 10.1109/LCOMM.2010.08.100559
   Wang Y.-K., 2011, IEEE COMMUN LETT
   WANG Yubing., 2006, SURVEY OBJECTIVE VID
   Yang Z, 2013, IEEE T CIRC SYST VID, V23, P212, DOI 10.1109/TCSVT.2012.2203216
   Zhang YH, 2009, IEEE T VEH TECHNOL, V58, P4605, DOI 10.1109/TVT.2009.2020801
   Zhang YH, 2009, IEEE COMMUN LETT, V13, P16, DOI 10.1109/LCOMM.2009.081471
   Zhou XW, 2010, IEEE T WIREL COMMUN, V9, P2870, DOI 10.1109/TWC.2010.070610.091511
NR 43
TC 30
Z9 33
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2015
VL 17
IS 3
BP 333
EP 345
DI 10.1109/TMM.2015.2389032
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CE1QB
UT WOS:000351585700006
DA 2024-07-18
ER

PT J
AU Wang, L
   Jung, C
AF Wang, Lei
   Jung, Cheolkon
TI Example-Based Video Stereolization With Foreground Segmentation and
   Depth Propagation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3DTV; depth generation; depth propagation; depth-image-based-rendering;
   learning-based; stereoscopic views; video stereolization
ID IMAGE; MOTION
AB With advances in 3DTV technology, video stereolization has attracted much attention in recent years. Although video stereolization can enrich stereoscopic 3D contents, it is hard to create good depth maps from monocular 2D videos. In this paper, we propose an automatic example-based video stereolization method with foreground segmentation and depth propagation, called EBVS. To consider both performance and computational complexity, we separately estimate depth maps according to the key and non-key frames. In the key frames, we first estimate an initial depth map based on examples from the RGB-D training data set, then refine it to preserve boundaries of foreground objects. In the non-key frames, we propagate the depth map of the key frame using motion compensation, and generate depth maps. Finally, we employ depth-image-based-rendering (DIBR) to generate stereoscopic views from 2D videos and their depth maps. Extensive experiments verify that the proposed EBVS produces visually pleasing and realistic stereoscopic 3D views from 2D videos.
C1 [Wang, Lei; Jung, Cheolkon] Xidian Univ, Minist Educ China, Key Lab Intelligent Percept & Image Understanding, Xian 710071, Peoples R China.
C3 Xidian University
RP Wang, L (corresponding author), Xidian Univ, Minist Educ China, Key Lab Intelligent Percept & Image Understanding, Xian 710071, Peoples R China.
EM lwang@stu.xidian.edu.cn; zhengzk@xidian.edu.cn
FU National Natural Science Foundation of China [61271298]; International
   S&T Cooperation Program of China [2014DFG12780]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61271298 and the International S&T Cooperation Program
   of China under Grant 2014DFG12780. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Jing-Ming Guo.
CR [Anonymous], P IEEE INT C IM PROC
   Barnich O, 2011, IEEE T IMAGE PROCESS, V20, P1709, DOI 10.1109/TIP.2010.2101613
   Cheng CC, 2010, IEEE T CONSUM ELECTR, V56, P1739, DOI 10.1109/TCE.2010.5606320
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Diplaris S, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, pA233
   Economou George, 2004, 2004 12th European Signal Processing Conference (EUSIPCO), P941
   Gokturk S. B., 2004, 2004 C COMPUTER VISI, V2004, P35
   Guttmann M, 2009, IEEE I CONF COMP VIS, P136, DOI 10.1109/ICCV.2009.5459158
   Hoiem D, 2005, IEEE I CONF COMP VIS, P654
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   Karsch K, 2012, LECT NOTES COMPUT SC, V7576, P775, DOI 10.1007/978-3-642-33715-4_56
   Konrad J, 2012, PROC SPIE, V8288, DOI 10.1117/12.910601
   Konrad J., 2012, Proc. 3D Cinematography Workshop (3DCINE'12) at IEEE CVPR, P16, DOI DOI 10.1109/CVPRW.2012.6238903
   Liao M, 2012, IEEE T VIS COMPUT GR, V18, P1079, DOI 10.1109/TVCG.2011.114
   Lie WN, 2011, ELECTRON LETT, V47, P319, DOI 10.1049/el.2010.2912
   Lin G. S., 2012, P PAC RIM C ADV IM V, P381
   MANNOS JL, 1974, IEEE T INFORM THEORY, V20, P525, DOI 10.1109/TIT.1974.1055250
   Moustakas K, 2005, IEEE T CIRC SYST VID, V15, P1065, DOI 10.1109/TCSVT.2005.852401
   Nicola B., 2011, Power Electronics and Motor Drives, ed, P1, DOI DOI 10.1145/2063384.2063429
   Pan Ji, 2012, 2012 International Conference on Audio, Language and Image Processing (ICALIP 2012). Proceedings, P546, DOI 10.1109/ICALIP.2012.6376677
   Paris S, 2009, INT J COMPUT VISION, V81, P24, DOI 10.1007/s11263-007-0110-8
   Petschnigg G, 2004, ACM T GRAPHIC, V23, P664, DOI 10.1145/1015706.1015777
   Radke RJ, 2005, IEEE T IMAGE PROCESS, V14, P294, DOI 10.1109/TIP.2004.838698
   Rotem E, 2005, PROC SPIE, V5664, P198, DOI 10.1117/12.586599
   Saxena A., 2005, ADV NEURAL INFORM PR, V18, P1
   Saxena A, 2007, IEEE I CONF COMP VIS, P1
   Shams R, 2010, IEEE SIGNAL PROC MAG, V27, P50, DOI 10.1109/MSP.2009.935387
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   SUBBARAO M, 1994, INT J COMPUT VISION, V13, P271, DOI 10.1007/BF02028349
   Tam WJ, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P1869, DOI 10.1109/ICME.2006.262919
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yamamoto K, 2009, INT C ELECTR MACH SY, P103, DOI 10.1109/ICEMS.2009.5382975
   Yi-Min Tsai, 2006, 2006 International Symposium on Intelligent Signal Processing and Communications, P586
   Zhang GF, 2009, IEEE T VIS COMPUT GR, V15, P828, DOI 10.1109/TVCG.2009.47
   Zhang GF, 2009, IEEE T PATTERN ANAL, V31, P974, DOI 10.1109/TPAMI.2009.52
   Zhang L, 2004, IEEE IMAGE PROC, P2993
NR 39
TC 12
Z9 12
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2014
VL 16
IS 7
BP 1905
EP 1914
DI 10.1109/TMM.2014.2341599
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AT1UX
UT WOS:000344720300009
DA 2024-07-18
ER

PT J
AU Su, YC
   Chiu, TH
   Kuo, YH
   Yeh, CY
   Hsu, WH
AF Su, Yu-Chuan
   Chiu, Tzu-Hsuan
   Kuo, Yin-Hsi
   Yeh, Chun-Yen
   Hsu, Winston H.
TI Scalable Mobile Visual Classification by Kernel Preserving Projection
   Over High-Dimensional Features
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dimension reduction; distance metric learning; manifold learning; mobile
   image classification
ID IMAGES
AB Scalable mobile visual classification-classifying images/videos in a large semantic space on mobile devices in real-time-is an emerging problem as observing the paradigm shift towards mobile platforms and the explosive growth of visual data. Though seeing the advances in detecting thousands of concepts in the servers, the scalability is handicapped in mobile devices due to the severe resource constraints within. However, certain emerging applications require such scalable visual classification with prompt response for detecting local contexts (e.g., Google Glass) or ensuring user satisfaction. In this work, we point out the ignored challenges for scalable mobile visual classification and provide a feasible solution. To overcome the limitations of mobile visual classification, we propose an unsupervised linear dimension reduction algorithm, kernel preserving projection (KPP), which approximates the kernel matrix of high dimensional features with low dimensional linear embedding. We further introduce sparsity to the projection matrix to ensure its compliance with mobile computing (with merely 12% non-zero entries). By inspecting the similarity of linear dimension reduction with low-rank linear distance metric and Taylor expansion of RBF kernel, we justified the feasibility for the proposed KPP method over high-dimensional features. Experimental results on three public datasets confirm that the proposed method outperforms existing dimension reduction methods. What is even more, we can greatly reduce the storage consumption and efficiently compute the classification results on the mobile devices.
C1 [Su, Yu-Chuan; Chiu, Tzu-Hsuan; Yeh, Chun-Yen] Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 106, Taiwan.
   [Kuo, Yin-Hsi; Hsu, Winston H.] Natl Taiwan Univ, Grad Inst Networking & Multimedia, Taipei 10617, Taiwan.
   [Hsu, Winston H.] Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 10617, Taiwan.
C3 National Taiwan University; National Taiwan University; National Taiwan
   University
RP Su, YC (corresponding author), Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 106, Taiwan.
EM ycsu@cmlab.csie.ntu.edu.tw; d98944011@ntu.edu.tw;
   kuonini@cmlab.csie.ntu.edu.tw; chunyen0702@gmail.com;
   winston@csie.ntu.edu.tw
RI Su, Yu-Chuan/N-7445-2015
FU National Science Council of Taiwan [NSC 101-2628-E-002-027-MY2];
   Excellent Research Projects of National Taiwan University [102R7762];
   MediaTek Inc.
FX This work was supported in part by the National Science Council of
   Taiwan under Contract NSC 101-2628-E-002-027-MY2, the Excellent Research
   Projects of National Taiwan University under Grant 102R7762, and
   MediaTek Inc. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Alan Hanjalic.
CR [Anonymous], 2010, Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Washington, DC, USA, July 25-28, 2010, DOI DOI 10.1145/1835804.183594
   Berg A., Large scale visual recognition challenge
   Bingham E., 2001, KDD-2001. Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P245, DOI 10.1145/502512.502546
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng J, 2010, LECT NOTES COMPUT SC, V6315, P71, DOI 10.1007/978-3-642-15555-0_6
   Everingham M., 2007, The PASCAL Visual Object Classes Chal- lenge (VOC) Results, DOI 10.1007/s11263-009-0275-4
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Gavves E, 2012, PROC CVPR IEEE, P3610, DOI 10.1109/CVPR.2012.6248106
   Girod B, 2011, IEEE SIGNAL PROC MAG, V28, P61, DOI 10.1109/MSP.2011.940881
   Goeau H, 2013, P 21 ACM INT C MULT
   Gong YC, 2011, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2011.5995432
   Griffin G., 2007, CALTECHAUTHORSCNSTR2
   Heo JP, 2012, PROC CVPR IEEE, P2957, DOI 10.1109/CVPR.2012.6248024
   Hong Y, 2011, IEEE I CONF COMP VIS, P906, DOI 10.1109/ICCV.2011.6126332
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Joly A, 2011, PROC CVPR IEEE, P873, DOI 10.1109/CVPR.2011.5995709
   Kandola J., 2003, ADV NEURAL INFORM PR, V15, P657
   Lazebnik S., 2006, P IEEE CVF C COMP VI, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/CVPR.2006.68]
   Litayem S, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.86
   Ma ZG, 2012, IEEE T MULTIMEDIA, V14, P1021, DOI 10.1109/TMM.2012.2187179
   Maji S, 2009, IEEE I CONF COMP VIS, P40, DOI 10.1109/ICCV.2009.5459203
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Qi Guo-Jun., 2009, ICML, P106
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Sánchez J, 2011, PROC CVPR IEEE, P1665, DOI 10.1109/CVPR.2011.5995504
   Schmidt M., 2010, PhD Thesis
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Uijlings JRR, 2010, IEEE T MULTIMEDIA, V12, P665, DOI 10.1109/TMM.2010.2052027
   Wang JJ, 2012, IEEE T MULTIMEDIA, V14, P986, DOI 10.1109/TMM.2012.2186120
   Wang Jun., 2010, ICML, P1127
   Wang M, 2009, IEEE T CIRC SYST VID, V19, P733, DOI 10.1109/TCSVT.2009.2017400
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   Wu GL, 2013, IEEE MULTIMEDIA, V20, P47, DOI 10.1109/MMUL.2013.13
   Xu MQ, 2010, IEEE IMAGE PROC, P1837, DOI 10.1109/ICIP.2010.5653825
   Yuan GX, 2012, P IEEE, V100, P2584, DOI 10.1109/JPROC.2012.2188013
   Zhu SA, 2012, IEEE T MULTIMEDIA, V14, P1068, DOI 10.1109/TMM.2012.2190387
NR 38
TC 10
Z9 10
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2014
VL 16
IS 6
BP 1645
EP 1653
DI 10.1109/TMM.2014.2322337
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AT1UW
UT WOS:000344720200013
DA 2024-07-18
ER

PT J
AU Yu, X
   Xu, F
   Zhang, SL
   Zhang, L
AF Yu, Xin
   Xu, Feng
   Zhang, Shunli
   Zhang, Li
TI Efficient Patch-Wise Non-Uniform Deblurring for a Single Image
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Blind deconvolution; non-uniform deblurring; total variation (TV)
   regularization
ID KERNEL ESTIMATION; MOTION
AB In this paper, we address the problem of estimating a latent sharp image from a single spatially variant blurred image. Non-uniform deblurring methods based on projective motion path models formulate the blur as a linear combination of homographic projections of a clear image. But they are computationally expensive and require large memory due to the calculation and storage of a large number of the projections. Patch-wise non-uniform deblurring algorithms have been proposed to estimate each kernel locally by a uniform deblurring algorithm, which does not require to calculate and store the projections. The key issues of these methods are the accuracy of kernel estimation and the identification of erroneous kernels. To perform accurate kernel estimation, we employ the total variation (TV) regularization to recover a latent image, in which the edges are better enhanced and the ringing artifacts are reduced, rather than Tikhonov regularization that previous algorithms adopt. Thus blur kernels can be estimated more accurately from the latent image and estimated in a closed form while previous methods cannot estimate kernels in closed forms. To identify the erroneous kernels, we develop a novel metric, which is able to measure the similarity between the neighboring kernels. After replacing the erroneous kernels with the well-estimated ones, a clear image is obtained. The experiments show that our approach can achieve better results on the real-world blurry images while using less computation and memory.
C1 [Yu, Xin; Zhang, Shunli; Zhang, Li] Tsinghua Univ, Dept Elect Engn, Beijing 100084, Peoples R China.
   [Xu, Feng] Microsoft Res, Beijing 100084, Peoples R China.
C3 Tsinghua University; Microsoft
RP Yu, X (corresponding author), Tsinghua Univ, Dept Elect Engn, Beijing 100084, Peoples R China.
EM xin-yu09@mails.tsinghua.edu.cn; fengxu@microsoft.com; zslsdu@163.com;
   chinazhangli@tsinghua.edu.cn
RI jing, wang/KCZ-2144-2024
OI Yu, Xin/0000-0002-0269-5649
FU National Natural Science Foundation of China [61132007]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61132007. The associate editor coordinating the review
   of this manuscript and approving it for publication was Dr. Cees G. M.
   Snoek.
CR [Anonymous], P CVPR, DOI DOI 10.1109/CVPR.2008.4587507
   [Anonymous], 2010, NIPS
   [Anonymous], 1966, P APR 26 28 1966 SPR
   [Anonymous], US NEWS WORLD REP
   [Anonymous], P 3 INT S AIRCR AIRW
   [Anonymous], 2010, ACM T GRAPHICS TOG
   Bando Y., 2013, ACM SIGGRAPH, V32, P1
   Ben-Ezra M, 2004, IEEE T PATTERN ANAL, V26, P689, DOI 10.1109/TPAMI.2004.1
   BYRD RH, 1995, SIAM J SCI COMPUT, V16, P1190, DOI 10.1137/0916069
   Cai JF, 2012, IEEE T IMAGE PROCESS, V21, P562, DOI 10.1109/TIP.2011.2164413
   Cai JF, 2009, PROC CVPR IEEE, P104, DOI 10.1109/CVPRW.2009.5206743
   Chan TF, 1998, IEEE T IMAGE PROCESS, V7, P370, DOI 10.1109/83.661187
   Cho S, 2012, COMPUT GRAPH FORUM, V31, P2183, DOI 10.1111/j.1467-8659.2012.03211.x
   Cho S, 2011, IEEE I CONF COMP VIS, P495, DOI 10.1109/ICCV.2011.6126280
   Cho S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618491
   Cho TS, 2012, IEEE T PATTERN ANAL, V34, P683, DOI 10.1109/TPAMI.2011.166
   Cho TS, 2011, PROC CVPR IEEE, P241, DOI 10.1109/CVPR.2011.5995479
   Fergus R, 2006, ACM T GRAPHIC, V25, P787, DOI 10.1145/1141911.1141956
   Getreuer P, 2012, IMAGE PROCESS ON LIN, V2, P74, DOI 10.5201/ipol.2012.g-tvd
   Goldstein A, 2012, LECT NOTES COMPUT SC, V7576, P622, DOI 10.1007/978-3-642-33715-4_45
   Goldstein T, 2009, SIAM J IMAGING SCI, V2, P323, DOI 10.1137/080725891
   Gupta A, 2010, LECT NOTES COMPUT SC, V6311, P171, DOI 10.1007/978-3-642-15549-9_13
   Hirsch M, 2011, IEEE I CONF COMP VIS, P463, DOI 10.1109/ICCV.2011.6126276
   Hirsch M, 2010, PROC CVPR IEEE, P607, DOI 10.1109/CVPR.2010.5540158
   Hu W, 2012, IEEE T IMAGE PROCESS, V21, P386, DOI 10.1109/TIP.2011.2160073
   Hu Z, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.136
   Ji H, 2012, PROC CVPR IEEE, P73, DOI 10.1109/CVPR.2012.6247660
   Jia J, 2013, P IEEE VTC JUN DRESD, P1
   Kim S, 2012, PROC CVPR IEEE, P25, DOI [10.1109/MMBIA.2012.6164736, 10.1109/CVPR.2012.6247654]
   Krishnan D., 2010, P NIPS
   Krishnan D, 2011, PROC CVPR IEEE, P233, DOI 10.1109/CVPR.2011.5995521
   Kundur D, 1996, IEEE SIGNAL PROC MAG, V13, P43, DOI 10.1109/79.489268
   Levin A., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2657, DOI 10.1109/CVPR.2011.5995308
   Levin A, 2009, PROC CVPR IEEE, P1964, DOI 10.1109/CVPRW.2009.5206815
   Li X, 2012, IEEE C COMP INTEL FI, P1, DOI 10.1109/CIFEr.2012.6327833
   Liu YF, 2013, STAT INTERFACE, V6, P1
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Schmid U., 2013, P SMART SENS ACT, P1
   Shan Q, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360672
   Sorel M, 2009, IEEE IMAGE PROC, P157, DOI 10.1109/ICIP.2009.5414145
   Tai YW, 2012, PROC CVPR IEEE, P17, DOI 10.1109/CVPR.2012.6247653
   Tai YW, 2011, IEEE T PATTERN ANAL, V33, P1603, DOI 10.1109/TPAMI.2010.222
   Tai YW, 2010, IEEE T PATTERN ANAL, V32, P1012, DOI 10.1109/TPAMI.2009.97
   Tikhonov A. N., 1977, SOLUTION ILL POSED P, V2
   Whyte O, 2012, INT J COMPUT VISION, V98, P168, DOI 10.1007/s11263-011-0502-7
   Xu L, 2010, LECT NOTES COMPUT SC, V6311, P157
   You YL, 1996, IEEE T IMAGE PROCESS, V5, P416, DOI 10.1109/83.491316
   Yuan L, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360673
NR 48
TC 39
Z9 43
U1 1
U2 29
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2014
VL 16
IS 6
BP 1510
EP 1524
DI 10.1109/TMM.2014.2321734
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AT1UW
UT WOS:000344720200002
DA 2024-07-18
ER

PT J
AU Fujihashi, T
   Pan, ZY
   Watanabe, T
AF Fujihashi, Takuya
   Pan, Ziyuan
   Watanabe, Takashi
TI UMSM: A Traffic Reduction Method on Multi-View Video Streaming for
   Multiple Users
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multi-view video; multiple users; realtime streaming; traffic reduction
AB Multi-view video consists of multiple video sequences captured simultaneously by multiple closely spaced cameras and enables users to freely change their viewpoints by playing different video sequences. Therefore, the transmission bitrate of multi-view video requires more bandwidth than conventional multimedia. In order to reduce the bandwidth requirement, User Dependent Multi-view Video Transmission (UDMVT), which is based on Multi-view Video Coding (MVC), has been proposed for single users. In UDMVT, the same frames are encoded into different versions for each user, which increases the transmission bitrate for multiple users because of the redundant transmission of overlapping frames. In order to address this problem, we proposed User dependent Multi-view video Streaming for Multi-users (UMSM). UMSM possesses two characteristics: 1) overlapping frames required by multiple users are transmitted only once by multicast while un-overlapping frames required by each user are transmitted by unicast and 2) offset of video requests between multiple users is aligned so as to maximize the area of overlapping frames. UMSM achieves a low transmission bitrate of multi-view video for multiple users by means of combining these characteristics. To investigate the effect of user's view switching on traffic reduction, we design three types of view-switching models, namely, scanning model, random model, and watching model. Simulation results obtained using benchmark test sequences provided by MERL reveal that UMSM decreases the transmission bitrate by 80.6% on average for five users, as compared to MVC, when users gaze a viewpoint for a while after switching views in order to search an attractive viewpoint.
C1 [Fujihashi, Takuya; Watanabe, Takashi] Osaka Univ, Grad Sch Informat Sci & Technol, Suita, Osaka 5650871, Japan.
   [Pan, Ziyuan] Shizuoka Univ, Grad Sch Sci & Technol, Hamamatsu, Shizuoka 4328011, Japan.
C3 Osaka University; Shizuoka University
RP Fujihashi, T (corresponding author), Osaka Univ, Grad Sch Informat Sci & Technol, 2-2 Yamadaoka, Suita, Osaka 5650871, Japan.
EM fujihashi.takuya@ist.osaka-u.ac.jp; pan@aurum.cs.inf.shizuoka.ac.jp;
   watanabe@ist.osaka-u.ac.jp
RI Fujihashi, Takuya/Y-1527-2019
OI Fujihashi, Takuya/0000-0002-6960-0122; WATANABE,
   TAKASHI/0000-0002-3227-9048
FU Suzuki Foundation; Telecommunications Advancement Foundation
FX This work was supported in part by research grants from Suzuki
   Foundation and The Telecommunications Advancement Foundation. This paper
   is an extended version of the original paper which appeared in the
   Proceedings of IEEE ICME 2012 Conference and was among the top-rated 4%
   of ICME'12 submissions. The associate editor coordinating the review of
   this manuscript and approving it for publication was Prof. Enrico Magli.
CR [Anonymous], 2005, MULT VID TEST SEQ ME
   [Anonymous], 2006, 7 WORKSH DIG BROADC
   [Anonymous], BAD ROMANCE
   [Anonymous], 2008, TEXT ISO IEC 14496 1
   Cheung G, 2011, IEEE T IMAGE PROCESS, V20, P744, DOI 10.1109/TIP.2010.2070074
   Cheung NM, 2009, PCS: 2009 PICTURE CODING SYMPOSIUM, P269
   FRIEDL R, 2002, HEART SURG FORUM, V5, P17
   Fujihashi T., 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P182, DOI 10.1109/ICME.2012.185
   Huang H, 2012, IEEE INFOCOM SER, P2791, DOI 10.1109/INFCOM.2012.6195701
   KARCZEWICZ M, 2001, PROPOSAL SP FRAMES
   Kimata H., 2007, Systems and Computers in Japan, V38, P14, DOI 10.1002/scj.20683
   Kurutepe E, 2007, IEEE T CIRC SYST VID, V17, P1558, DOI 10.1109/TCSVT.2007.903664
   Liu Z, 2012, IEEE ICC, P2048, DOI 10.1109/ICC.2012.6363933
   Merkle P., 2005, P ICOB 2005 BERL GER, P27
   Muller K., 2006, P IEEE PCS APR, P385
   Okamoto M, 2000, OCEANS 2000 MTS/IEEE - WHERE MARINE SCIENCE AND TECHNOLOGY MEET, VOLS 1-3, CONFERENCE PROCEEDINGS, P1389, DOI 10.1109/OCEANS.2000.881797
   Pan  Z., 2011, P IEEE ICC, P1
   Pan ZY, 2012, 2012 PICTURE CODING SYMPOSIUM (PCS), P89, DOI 10.1109/PCS.2012.6213293
   Pan ZY, 2011, INT CON ADV INFO NET, P732, DOI 10.1109/AINA.2011.31
   Shiau YH, 2012, INT J ADV COMPUT SC, V3, P45
   Tanimoto M, 2006, SIGNAL PROCESS-IMAGE, V21, P454, DOI 10.1016/j.image.2006.03.009
   Utada H., 2012, SAKURANAGASHI
   Vetro A., 2008, JOINT DRAFT 8 0 MULT
   Watanabe T., 2013, J COMPUTATIONAL INFO, V9, P1439
   WIEGAND T, 2002, STUDY FINAL COMMITTE
   Xiu XY, 2011, IEEE IMAGE PROC, P593, DOI 10.1109/ICIP.2011.6116619
   Zhang BX, 2003, GLOB TELECOMM CONF, P2840
NR 27
TC 31
Z9 31
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2014
VL 16
IS 1
BP 228
EP 241
DI 10.1109/TMM.2013.2281588
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 279GJ
UT WOS:000328948100019
DA 2024-07-18
ER

PT J
AU Chu, LY
   Jiang, SQ
   Wang, SH
   Zhang, YY
   Huang, QM
AF Chu, Lingyang
   Jiang, Shuqiang
   Wang, Shuhui
   Zhang, Yanyan
   Huang, Qingming
TI Robust Spatial Consistency Graph Model for Partial Duplicate Image
   Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Combined orientation position; graph model; image retrieval; partial
   duplicate; rotation invariant.
ID FEATURES
AB Partial duplicate images often have large non-duplicate regions and small duplicate regions with random rotation, which lead to the following problems: 1) large number of noisy features from the non-duplicate regions; 2) small number of representative features from the duplicate regions; 3) randomly rotated or deformed duplicate regions. These problems challenge many content based image retrieval (CBIR) approaches, since most of them cannot distinguish the representative features from a large proportion of noisy features in a rotation invariant way. In this paper, we propose a rotation invariant partial duplicate image retrieval (PDIR) approach, which effectively and efficiently retrieves the partial duplicate images by accurately matching the representative SIFT features. Our method is based on the Combined-Orientation-Position (COP) consistency graph model, which consists of the following two parts: 1) The COP consistency, which is a rotation invariant measurement of the relative spatial consistency among the candidate matches of SIFT features; it uses a coarse-to-fine family of evenly sectored polar coordinate systems to softly quantize and combine the orientations and positions of the SIFT features. 2) The consistency graph model, which robustly rejects the spatially inconsistent noisy features by effectively detecting the group of candidate feature matches with the largest average COP consistency. Extensive experiments on five large scale image data sets show promising retrieval performances.
C1 [Chu, Lingyang; Jiang, Shuqiang; Wang, Shuhui; Zhang, Yanyan; Huang, Qingming] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
   [Zhang, Yanyan; Huang, Qingming] Univ Chinese Acad Sci, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS
RP Chu, LY (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
EM lychu@jdl.ac.cn; sqjiang@jdl.ac.cn; shwang@jdl.ac.cn; yyzhang@jdl.ac.cn;
   qmhuang@jdl.ac.cn
RI Huang, Qingming/GLR-3473-2022; Zhang, Yanyan/AFT-0833-2022
OI Huang, Qingming/0000-0002-3025-7099; 
FU National Basic Research Program of China (973 Program) [2012CB316400];
   National Natural Science Foundation of China [61025011, 61070108,
   61035001]
FX This work was supported in part by National Basic Research Program of
   China (973 Program): 2012CB316400, in part by National Natural Science
   Foundation of China: 61025011, 61070108, and 61035001.
CR [Anonymous], 2007, CVPR
   Bomze IM, 2002, J GLOBAL OPTIM, V22, P17, DOI 10.1023/A:1013886408463
   Cao Y, 2010, PROC CVPR IEEE, P3352, DOI 10.1109/CVPR.2010.5540021
   Chum O., 2009, P IEEE COMP SOC C CO
   Chum O., 2008, P BMVC
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Hartley RI, 1997, IEEE T PATTERN ANAL, V19, P580, DOI 10.1109/34.601246
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Liu HR, 2010, PROC CVPR IEEE, P1609, DOI 10.1109/CVPR.2010.5539780
   Liu Hairong, 2010, P 27 INT C MACH LEAR
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Matas J, 2004, IMAGE VISION COMPUT, V22, P761, DOI 10.1016/j.imavis.2004.02.006
   Mu YD, 2010, LECT NOTES COMPUT SC, V6313, P748
   Nister David, 2006, CVPR
   Pavan M, 2007, IEEE T PATTERN ANAL, V29, P167, DOI 10.1109/TPAMI.2007.250608
   Philbin J., 2008, P CVPR, P1
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   van Gemert JC, 2010, IEEE T PATTERN ANAL, V32, P1271, DOI 10.1109/TPAMI.2009.132
   Wang X., 2011, P IEEE 13 INT C COMP
   Weibull J.W., 1997, Evolutionary Game Theory
   Wu Z, 2009, PROC CVPR IEEE, P25, DOI 10.1109/CVPRW.2009.5206566
   Xie HT, 2011, IEEE T MULTIMEDIA, V13, P1319, DOI 10.1109/TMM.2011.2167224
   Xu D., 2008, P IEEE COMP SOC C CO
   Zhang Shiliang., 2009, MM 09 P 17 ACM INT C, P75, DOI DOI 10.1145/1631272.1631285
   Zhang YM, 2011, PROC CVPR IEEE, P809, DOI 10.1109/CVPR.2011.5995528
   Zhang YM, 2009, PROC CVPR IEEE, P1762, DOI 10.1109/CVPRW.2009.5206791
   Zhao WL, 2010, IEEE T MULTIMEDIA, V12, P448, DOI 10.1109/TMM.2010.2050651
   Zhipeng Wu, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P842, DOI 10.1109/ICPR.2010.212
   Zhou W., 2011, Proceedings of ACM Multimedia, P1349, DOI DOI 10.1145/2072298.2072012
   Zhou W., 2010, P 18 ACM INT C MULTI, P511
   Zhou W., 2012, ACM T MULTIMEDIA COM
NR 31
TC 35
Z9 38
U1 0
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2013
VL 15
IS 8
BP 1982
EP 1996
DI 10.1109/TMM.2013.2270455
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 257OI
UT WOS:000327393900021
DA 2024-07-18
ER

PT J
AU Hadizadeh, H
   Bajic, IV
   Cheung, G
AF Hadizadeh, Hadi
   Bajic, Ivan V.
   Cheung, Gene
TI Video Error Concealment Using a Computation-Efficient Low Saliency Prior
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visual communication; Video signal processing; Streaming media; Inverse
   problems
ID VISUAL-ATTENTION; MODEL; QUALITY
AB Error concealment in packet-loss-corrupted streaming video is inherently an under-determined problem, as there are insufficient number of well-defined criteria to recover the missing blocks perfectly. When a Region-of-Interest (ROI) based unequal error protection (UEP) scheme is deployed during video streaming-i.e., more visually salient regions are strongly protected-a lost block is likely to be of low saliency in the original frame. In this paper, we propose to add a low-saliency prior to the error concealment problem as a regularization term. It serves two purposes. First, in ROI-based UEP video streaming, low-saliency prior provides the correct side information for the client to identify the correct replacement blocks for concealment. Second, in the event that a perfectly matched block cannot be unambiguously identified, the low-saliency prior reduces viewer's visual attention on the loss-stricken region, resulting in higher overall subjective quality. We study the effectiveness of a low-saliency prior in the context of a previously proposed RECAP error concealment system. RECAP transmits a low-resolution (LR) version of an image alongside the original high-resolution (HR) version, so that if blocks in the HR version are lost, the correctly-received LR version can serve as a template for matching of suitable replacement blocks from a previously correctly-decoded HR frame. We add a low-saliency prior to the block identification process, so that only replacement candidate blocks with good match and low saliency can be selected. Further, we develop a low-complexity convex approximation to the well known Itti-Koch-Niebur saliency model, which enables the low-saliency error concealment problem to be solved efficiently. Experimental results show that: i) PSNR of the error-concealed frames can be increased dramatically (up to 3.6 dB over the original RECAP), showing the effectiveness of a low-saliency prior in the under-determined error concealment problem; and ii) subjective quality of the repaired video using our proposal, as confirmed by an extensive user study, is better than the original RECAP.
C1 [Hadizadeh, Hadi; Bajic, Ivan V.] Simon Fraser Univ, Sch Engn Sci, Burnaby, BC V5A 1S6, Canada.
   [Cheung, Gene] Natl Inst Informat, Chiyoda Ku, Tokyo 1018430, Japan.
C3 Simon Fraser University; Research Organization of Information & Systems
   (ROIS); National Institute of Informatics (NII) - Japan
RP Hadizadeh, H (corresponding author), Simon Fraser Univ, Sch Engn Sci, Burnaby, BC V5A 1S6, Canada.
EM hadi.sfu@gmail.com; ibajic@ensc.sfu.ca; cheung@nii.ac.jp
RI Cheung, Gene/AAB-9284-2020; Bajic, Ivan/I-1241-2013
OI Bajic, Ivan/0000-0003-3154-5743; Hadizadeh, Hadi/0000-0003-2018-0523;
   Cheung, Gene/0000-0002-5571-4137
FU Grants-in-Aid for Scientific Research [23700136] Funding Source: KAKEN
CR [Anonymous], 2009, P IEEE INT C COMP VI
   [Anonymous], 2000, HDB PARAMETRIC NONPA
   [Anonymous], 2009, P IEEE C COMP VIS PA
   [Anonymous], 2007, P IEEE C COMP VIS PA
   Boulos F., 2009, P 17 INT PACK VID WO
   Boyd S., 2004, CONVEX OPTIMIZATION
   Bruce N., 2009, P IEEE INT C IM PROC
   Bruce N., 2006, P ADV NEUR INF PROC, P155
   Cerf M., 2009, VISION RES, V9, P1
   Chen Y, 2008, IEEE T MULTIMEDIA, V10, P2, DOI 10.1109/TMM.2007.911223
   Chen ZZ, 2010, IEEE T CIRC SYST VID, V20, P806, DOI 10.1109/TCSVT.2010.2045912
   Chikkerur S, 2011, IEEE T BROADCAST, V57, P165, DOI 10.1109/TBC.2011.2104671
   CISCO, 2010, CISC VIS NETW IND FO
   Cover T. M., 1991, ELEMENTS INFORM THEO
   GILBERT EN, 1960, BELL SYST TECH J, V39, P1253, DOI 10.1002/j.1538-7305.1960.tb03959.x
   Hadizadeh H., 2012, P IEEE ICME 12 MELB, p73 
   Hadizadeh H., 2012, COMPLEXITY SALIENCY
   Itti L, 2004, IEEE T IMAGE PROCESS, V13, P1304, DOI 10.1109/TIP.2004.834657
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   ITU-R, 1998, REC BT 500 8 METH SU
   Kubota A, 2007, IEEE SIGNAL PROC MAG, V24, P10, DOI 10.1109/MSP.2007.905873
   Lim J.S., 1989, 2 DIMENSIONAL SIGNAL
   Magnus J.R., 1999, MATRIX DIFFERENTIAL
   Mateescu VA, 2012, IEEE GLOBE WORK, P1304, DOI 10.1109/GLOCOMW.2012.6477770
   Muntean GM, 2008, IEEE T BROADCAST, V54, P296, DOI 10.1109/TBC.2008.919012
   Nesterov Y., 1994, Interior-Point Polynomial Algorithms in Convex Programming
   Oxford Dictionary, DEF CONC
   Pinson MH, 2004, IEEE T BROADCAST, V50, P312, DOI 10.1109/TBC.2004.834028
   Stark H., 2012, Probability, Statistics, and Random processes for engineers, V4th
   Tatler BW, 2005, VISION RES, V45, P643, DOI 10.1016/j.visres.2004.09.017
   TAYLOR MM, 1967, J ACOUST SOC AM, V41, P782, DOI 10.1121/1.1910407
   Torralba A, 2003, NETWORK-COMP NEURAL, V14, P391, DOI 10.1088/0954-898X/14/3/302
   Vaidyanathan P. P., 1993, MULTIRATE SYSTEMS FI
   Vuduc R. W., 2003, PhD diss, DOI 10.1.1.1.4701
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Woods J.W., 2012, MULTIDIMENSIONAL SIG
   Yeo C., 2009, P IEEE INT C AC SPEE
NR 37
TC 12
Z9 15
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2013
VL 15
IS 8
BP 2099
EP 2113
DI 10.1109/TMM.2013.2281024
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 257OI
UT WOS:000327393900030
DA 2024-07-18
ER

PT J
AU Jing, YS
   Covell, M
   Tsai, D
   Rehg, JM
AF Jing, Yushi
   Covell, Michele
   Tsai, David
   Rehg, James M.
TI Learning Query-Specific Distance Functions for Large-Scale Web Image
   Search
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image search; image processing; content based retrieval; search engine;
   distance learning.
ID RETRIEVAL-SYSTEM
AB Current Google image search adopt a hybrid search approach in which a text-based query (e. g., "Paris landmarks") is used to retrieve a set of relevant images, which are then refined by the user (e. g., by re-ranking the retrieved images based on similarity to a selected example). We conjecture that given such hybrid image search engines, learning per-query distance functions over image features can improve the estimation of image similarity. We propose scalable solutions to learning query-specific distance functions by 1) adopting a simple large-margin learning framework, 2) using the query-logs of text-based image search engine to train distance functions used in content-based systems. We evaluate the feasibility and efficacy of our proposed system through comprehensive human evaluation, and compare the results with the state-of-the-art image distance function used by Google image search.
C1 [Jing, Yushi; Covell, Michele] Google Res, Mountain View, CA 94043 USA.
   [Tsai, David; Rehg, James M.] Georgia Inst Technol, Atlanta, GA 30332 USA.
C3 Google Incorporated; University System of Georgia; Georgia Institute of
   Technology
RP Jing, YS (corresponding author), Google Res, Mountain View, CA 94043 USA.
RI Rehg, James/AAM-6888-2020
OI Rehg, James/0000-0003-1793-5462
CR [Anonymous], 2006, DISTANCE METRIC LEAR
   [Anonymous], P INT WORLD WID WEB
   [Anonymous], 2009, P CVPR09
   [Anonymous], 2006, P 2006 IEEE COMPUTER, DOI DOI 10.1109/CVPR.2006.167
   Borlund P, 1997, J DOC, V53, P225, DOI 10.1108/EUM0000000007198
   Carterette B., P ADV NEUR INF PROC
   Combs T. T. A., 1999, Digital 99 Libraries. Fourth ACM Conference on Digital Libraries, P130, DOI 10.1145/313238.313286
   Conniss L. R., 2000, 95 I IM DAT RES LIB
   Cox IJ, 2000, IEEE T IMAGE PROCESS, V9, P20, DOI 10.1109/83.817596
   Cox IJ, 1996, PROCEEDINGS OF THE THIRD FORUM ON RESEARCH AND TECHNOLOGY ADVANCES IN DIGITAL LIBRARIES (ADL '96), P66, DOI 10.1109/ADL.1996.502517
   Craswell N., 2008, P 2008 INT C WEB SEA, P87, DOI [DOI 10.1145/1341531.1341545, 10.1145/1341531]
   Datta R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1348246.1348248
   Dean J, 2010, COMMUN ACM, V53, P72, DOI 10.1145/1629175.1629198
   Frome A., 2007, P NEUR INF PROC SYST
   Gang Wu, 2005, 13th Annual ACM International Conference on Multimedia, P725
   Grauman K, 2007, J MACH LEARN RES, V8, P725
   Griffin G., 2007, CALTECH 256 OBJECT C
   Herlocker JL, 1999, SIGIR'99: PROCEEDINGS OF 22ND INTERNATIONAL CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P230, DOI 10.1145/312624.312682
   Joachims T, 2002, P 8 ACM SIGKDD INT C, P133, DOI [DOI 10.1145/775047.775067, 10.1145/775047.775067]
   Joachims T., ACM T INF SCI TOIS 2
   Manning Christopher D., 2008, INTRO INFORM RETRIEV
   Miller G.A., 1990, Int. J. Lexicogr, V3, P235, DOI [DOI 10.1093/IJL/3.4.235, 10.1093/ijl/3.4.235]
   Müller H, 2001, PATTERN RECOGN LETT, V22, P593, DOI 10.1016/S0167-8655(00)00118-5
   Radlinski F., 2008, Proceedings of the 17th ACM Conference on Information and Knowledge Management. CIKM'08, P43, DOI [DOI 10.1145/1458082.1458092, DOI 10.1145/1458082.1458092.21K, 10.1145/1458082.1458092]
   Robertson S., 2009, P 27 INT C EXTENDED, P3937
   Rodden K, 1999, SIGIR'99: PROCEEDINGS OF 22ND INTERNATIONAL CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P275, DOI 10.1145/312624.312693
   Rodden K., 2003, P SIGCHI C HUMAN FAC, P409, DOI DOI 10.1145/642611.642682
   ROSCH E, 1975, COGNITIVE PSYCHOL, V7, P573, DOI 10.1016/0010-0285(75)90024-9
   Rubner Y., 2000, INT J COMPUT VISION
   SALTON G, 1992, INFORM PROCESS MANAG, V28, P441, DOI 10.1016/0306-4573(92)90002-H
   Saraiva P. C., 2001, SIGIR Forum, P51
   Schultz M., P 16 C ADV NEUR INF
   Shalev-Schwartz S., 2007, P INT C MACH LEARN
   Shirahatti NV, 2005, PROC CVPR IEEE, P955
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Tsai D., 2011, P INT C COMP VIS
   Uchihashi S., 2005, P INT C IM VID RETR
   Weinberger K., 2006, [No title captured], P1437
   Xing E., 2002, P 15 C ADV NEUR INF, P450
   Zhou X., 2004, MULTIMEDIA SYST
NR 40
TC 10
Z9 11
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2013
VL 15
IS 8
BP 2022
EP 2034
DI 10.1109/TMM.2013.2279663
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA 257OI
UT WOS:000327393900024
OA Bronze
DA 2024-07-18
ER

PT J
AU Wu, CH
   Wei, WL
   Lin, JC
   Lee, WY
AF Wu, Chung-Hsien
   Wei, Wen-Li
   Lin, Jen-Chun
   Lee, Wei-Yu
TI Speaking Effect Removal on Emotion Recognition From Facial Expressions
   Based on Eigenface Conversion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Arousal-valence emotion plane; articulatory attribute; conversion
   function; emotion recognition; facial expression.
ID SPEECH; REPRESENTATION; PCA
AB Speaking effect is a crucial issue that may dramatically degrade performance in emotion recognition from facial expressions. To manage this problem, an eigenface conversion-based approach is proposed to remove speaking effect on facial expressions for improving accuracy of emotion recognition. In the proposed approach, a context-dependent linear conversion function modeled by a statistical Gaussian Mixture Model (GMM) is constructed with parallel data from speaking and non-speaking facial expressions with emotions. To model the speaking effect in more detail, the conversion functions are categorized using a decision tree considering the visual temporal context of the Articulatory Attribute (AA) classes of the corresponding input speech segments. For verification of the identified quadrant of emotional expression on the Arousal-Valence (A-V) emotion plane, which is commonly used to dimensionally define the emotion classes, from the reconstructed facial feature points, an expression template is constructed to represent the feature points of the non-speaking facial expressions for each quadrant. With the verified quadrant, a regression scheme is further employed to estimate the A-V values of the facial expression as a precise point in the A-V emotion plane. Experimental results show that the proposed method outperforms current approaches and demonstrates that removing the speaking effect on facial expression is useful for improving the performance of emotion recognition.
C1 [Wu, Chung-Hsien; Wei, Wen-Li; Lin, Jen-Chun; Lee, Wei-Yu] Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, Tainan 70101, Taiwan.
C3 National Cheng Kung University
RP Wu, CH (corresponding author), Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, Tainan 70101, Taiwan.
EM chunghsienwu@gmail.com; lilijinjin@gmail.com; jenchunlin@gmail.com;
   to_ro_tin@yahoo.com.tw
RI Wu, Chung-Hsien/E-7970-2013; Lin, Jen-Chun/AAQ-3701-2021; Wei,
   Wen-Li/AAQ-3848-2021
OI Wu, Chung-Hsien/0000-0002-3947-2123; Lin, Jen-Chun/0000-0002-9237-4119;
   Wei, Wen-Li/0000-0002-6753-2824
FU National Science Council [NSC101-2221-E-006-252-MY3]; Headquarters of
   University Advancement at the National Cheng Kung University; Ministry
   of Education, Taiwan, ROC
FX This work was supported in part by the National Science Council, under
   Contract NSC101-2221-E-006-252-MY3 and the Headquarters of University
   Advancement at the National Cheng Kung University, which is sponsored by
   the Ministry of Education, Taiwan, ROC.
CR [Anonymous], 1980, TECHNOLOGY MENTAL HL
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], 1990, Regression analysis: theory, methods and applications
   [Anonymous], EUROMEDIA
   [Anonymous], 1999, Handbook of the International Phonetic Association: A Guide to the International Phonetic Alphabet
   Chang Y, 2006, IMAGE VISION COMPUT, V24, P605, DOI 10.1016/j.imavis.2005.08.006
   Choi HL, 2006, 2006 SICE-ICASE INTERNATIONAL JOINT CONFERENCE, VOLS 1-13, P4592
   Cohen I, 2003, COMPUT VIS IMAGE UND, V91, P160, DOI 10.1016/S1077-3142(03)00081-X
   COOTES TF, 1992, IMAGE VISION COMPUT, V10, P289, DOI 10.1016/0262-8856(92)90044-4
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Dubuisson S, 2002, SIGNAL PROCESS-IMAGE, V17, P657, DOI 10.1016/S0923-5965(02)00076-0
   Gunes Hatice, 2010, International Journal of Strategic Synthetic Emotions, V1, P68, DOI 10.4018/jse.2010101605
   Jolliffe I.T., 1986, Principal component analysis, DOI DOI 10.1016/0169-7439(87)80084-9
   Kain A, 1998, INT CONF ACOUST SPEE, P285, DOI 10.1109/ICASSP.1998.674423
   Kharat G. U., 2008, WSEAS Transactions on Computers, V7, P650
   Kotsia I, 2008, IMAGE VISION COMPUT, V26, P1052, DOI 10.1016/j.imavis.2007.11.004
   LANITIS A, 1994, ELECTRON LETT, V30, P1587, DOI 10.1049/el:19941110
   Lee CH, 2010, INT CONF ACOUST SPEE, P4826, DOI 10.1109/ICASSP.2010.5495140
   Li J, 2005, INT CONF ACOUST SPEE, P837
   Ligang Zhang, 2011, Proceedings of the 2011 International Conference on Digital Image Computing: Techniques and Applications (DICTA 2011), P620, DOI 10.1109/DICTA.2011.110
   Lima A, 2004, IEICE T INF SYST, VE87D, P2802
   Lin JC, 2012, IEEE T MULTIMEDIA, V14, P142, DOI 10.1109/TMM.2011.2171334
   Nicolaou M.A., 2011, Proceedings of the 19th ACM international conference on Multimedia, P933
   Park S, 2009, PATTERN RECOGN LETT, V30, P708, DOI 10.1016/j.patrec.2009.02.005
   Picard R. W., 1997, AFFECTIVE COMPUTING
   RUSSELL JA, 1980, J PERS SOC PSYCHOL, V39, P1161, DOI 10.1037/h0077714
   Siniscalchi SM, 2008, INT CONF ACOUST SPEE, P4261, DOI 10.1109/ICASSP.2008.4518596
   Smola AJ, 2004, STAT COMPUT, V14, P199, DOI 10.1023/B:STCO.0000035301.49549.88
   Stylianou Y, 1998, IEEE T SPEECH AUDI P, V6, P131, DOI 10.1109/89.661472
   Takiguchi Tetsuya, 2007, Journal of Multimedia, V2, P13, DOI 10.4304/jmm.2.5.13-18
   Tang FQ, 2007, ICNC 2007: THIRD INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION, VOL 2, PROCEEDINGS, P632
   Tao JH, 2006, IEEE T AUDIO SPEECH, V14, P1145, DOI 10.1109/TASL.2006.876113
   Thayer Robert E., 1990, The Biopsychology of Mood and Arousal
   Truong KP, 2009, INTERSPEECH 2009: 10TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2009, VOLS 1-5, P1995
   TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wu C.-H., 2010, P INT COMP S
   Wu DR, 2010, IEEE INT CON MULTI, P737, DOI 10.1109/ICME.2010.5583101
   Yang J, 2004, IEEE T PATTERN ANAL, V26, P131, DOI 10.1109/TPAMI.2004.1261097
   Yang YH, 2008, IEEE T AUDIO SPEECH, V16, P448, DOI 10.1109/TASL.2007.911513
   Yeasin M, 2006, IEEE T MULTIMEDIA, V8, P500, DOI 10.1109/TMM.2006.870737
   Zeng Z., 2004, P 6 INT C MULT INT, P137
   Zeng ZH, 2007, IEEE T MULTIMEDIA, V9, P424, DOI 10.1109/TMM.2006.886310
   Zhao XM, 2012, EURASIP J ADV SIG PR, P1, DOI 10.1186/1687-6180-2012-20
   Zhu Y, 2009, 2009 THIRD INTERNATIONAL SYMPOSIUM ON INTELLIGENT INFORMATION TECHNOLOGY APPLICATION, VOL 1, PROCEEDINGS, P513, DOI 10.1109/IITA.2009.317
NR 45
TC 32
Z9 34
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2013
VL 15
IS 8
BP 1732
EP 1744
DI 10.1109/TMM.2013.2272917
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 257OI
UT WOS:000327393900002
DA 2024-07-18
ER

PT J
AU Magli, E
   Wang, M
   Frossard, P
   Markopoulou, A
AF Magli, Enrico
   Wang, Mea
   Frossard, Pascal
   Markopoulou, Athina
TI Network Coding Meets Multimedia: A Review
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Review
DE Distributed media systems; error resilience; multimedia communications;
   network coding; peer-to-peer; video streaming; wireless media
ID ERROR-CORRECTION; VIDEO; MULTICAST
AB While every network node only relays messages in a traditional communication system, the recent network coding (NC) paradigm proposes to implement simple in-network processing with packet combinations in the nodes. NC extends the concept of "encoding" a message beyond source coding (for compression) and channel coding (for protection against errors and losses). It has been shown to increase network throughput compared to traditional networks implementation, to reduce delay and to provide robustness to transmission errors and network dynamics. These features are so appealing for multimedia applications that they have spurred a large research effort towards the development of multimedia-specific NC techniques. This paper reviews the recent work in NC for multimedia applications and focuses on the techniques that fill the gap between NC theory and practical applications. It outlines the benefits of NC and presents the open challenges in this area. The paper initially focuses on multimedia-specific aspects of network coding, in particular delay, in-network error control, and media-specific error control. These aspects permit to handle varying network conditions as well as client heterogeneity, which are critical to the design and deployment of multimedia systems. After introducing these general concepts, the paper reviews in detail two applications that lend themselves naturally to NC via the cooperation and broadcast models, namely peer-to-peer multimedia streaming and wireless networking.
C1 [Magli, Enrico] Politecn Torino DET, I-10129 Turin, Italy.
   [Wang, Mea] Univ Calgary, Dept Comp Sci, Calgary, AB T2N 1N4, Canada.
   [Frossard, Pascal] EPFL Signal Proc Lab LTS4 EPFL FSTI IEL LTS4 Stn, CH-1015 Lausanne, Switzerland.
   [Markopoulou, Athina] Univ Calif Irvine, Elect Engn & Comp Sci Dept, Irvine, CA 92697 USA.
C3 Polytechnic University of Turin; University of Calgary; University of
   California System; University of California Irvine
RP Magli, E (corresponding author), Politecn Torino DET, I-10129 Turin, Italy.
EM enrico.magli@polito.it; meawang@ucalgary.ca; pascal.frossard@epfl.ch;
   athina@uci.edu
RI Frossard, Pascal/AAF-2268-2019
OI Markopoulou, Athina/0000-0003-1803-8675
FU Division Of Computer and Network Systems; Direct For Computer & Info
   Scie & Enginr [0747110] Funding Source: National Science Foundation
CR Ahlswede R, 2000, IEEE T INFORM THEORY, V46, P1204, DOI 10.1109/18.850663
   Albanese A, 1996, IEEE T INFORM THEORY, V42, P1737, DOI 10.1109/18.556670
   Annapureddy S., 2006, P INT PROT TELEVISIO
   Annapureddy S., 2007, P 16 INT WORLD WID W
   [Anonymous], P 26 C IEEE COMM SOC
   [Anonymous], CISCO VISUAL NETWORK
   [Anonymous], P 5 INT WORKSH PEER
   [Anonymous], 2008, Network Coding: An Introduction
   [Anonymous], P 11 IEEE INT C NETW
   [Anonymous], P 19 ACM S OP SYST P
   Avestimehr AS, 2011, IEEE T INFORM THEORY, V57, P1872, DOI 10.1109/TIT.2011.2110110
   Boldrini C., 2008, P ACM INT C MOD AN S
   Cai N, 2006, COMMUN INF SYST, V6, P37
   Castro M., 2003, Operating Systems Review, V37, P298, DOI 10.1145/1165389.945474
   Chou P. A., 2003, P 41 ALL C
   Chou PA, 2006, IEEE T MULTIMEDIA, V8, P390, DOI 10.1109/TMM.2005.864313
   Cleju N, 2011, IEEE T MULTIMEDIA, V13, P1103, DOI 10.1109/TMM.2011.2161448
   Ebrahimi J., 2011, ACM T ALGORITHMS
   Effros M., 2006, P IEEE INF THEOR WOR
   Feng C., 2008, P ACM MULT OCT
   Feng Y., 2011, P IEEE INFOCOM
   Fiandrotti A., 2012, P IEEE INT C MULT EX
   Fiandrotti A, 2011, IEEE INT WORKSH MULT
   Fragouli C, 2006, ACM SIGCOMM COMP COM, V36, P63, DOI 10.1145/1111322.1111337
   Fragouli C, 2011, P IEEE, V99, P461, DOI 10.1109/JPROC.2010.2093470
   Gabidulin E., 1985, PROBL PEREDACHI INF, V21, P3
   Gallager R. G., 1963, RES MONOGRAPH SERIES
   Gheorghiu S., 2011, P INT S NETW COD NET
   Gkantsidis C., 2005, P 24 C IEEE COMM SOC
   Halloush M, 2011, IEEE T WIREL COMMUN, V10, P466, DOI 10.1109/TWC.2011.120810.090280
   Ho T, 2006, IEEE T INFORM THEORY, V52, P4413, DOI 10.1109/TIT.2006.881746
   Hsu Y.-P., 2011, P IEEE INT S INF THE
   Hsu Y.-P., 2012, P NETCOD BOST MA US
   Hua S, 2011, IEEE T MULTIMEDIA, V13, P402, DOI 10.1109/TMM.2010.2103929
   Huang T., 2008, P 5 ANN IEEE COMM SO
   Hui P, 2011, IEEE T MOBILE COMPUT, V10, P1576, DOI 10.1109/TMC.2010.246
   Jaggi S, 2005, IEEE T INFORM THEORY, V51, P1973, DOI 10.1109/TIT.2005.847712
   Jaho E., 2009, P IEEE IFIP C WIR ON
   Katti S., 2006, P ACM SIGCOMM PIS IT
   Keller L., 2012, P ACM MOBISYS LOW WO
   Kiraly Z., 2011, P INT S NETW COD NET
   Koetter R, 2003, IEEE ACM T NETWORK, V11, P782, DOI 10.1109/TNET.2003.818197
   Le A., 2012, P SIGCCOMM MOBIGAMES
   Le A., 2012, THESIS U CALIFORNIA
   LI J, 2005, P 1 ACM SIGCOMM AS W
   Li S., 2007, P IEEE INT C MULT EX
   Li SYR, 2011, P IEEE, V99, P372, DOI 10.1109/JPROC.2010.2093851
   Li SYR, 2003, IEEE T INFORM THEORY, V49, P371, DOI 10.1109/TIT.2002.807285
   Li X., 2011, P ALL C ALL PARK MON
   Li XH, 2011, IEEE J SEL AREA COMM, V29, P1094, DOI 10.1109/JSAC.2011.110519
   Liu X., 2008, P IEEE WORKSH MOB VI
   Liu Z., 2010, Proceedings of IEEE INFOCOM
   Luby M, 2002, ANN IEEE SYMP FOUND, P271, DOI 10.1109/SFCS.2002.1181950
   LUN D, 2004, P 42 ANN ALL C COMM
   LUN DS, 2006, THESIS MIT CAMBRIDGE
   Magharei N., 2009, P IEEE MULT COMM TEC
   Magli E., 2009, P IEEE INT C MULT EX
   Markopoulou A., 2009, IEEE MMTC E LETT FEB
   Mashhadi A. J., 2009, P IEEE INT S WORLD W
   Mehta T., 2011, P NUICONE
   Mirshokraie S., 2010, P 1 ACM MULT SYST MM
   Montpetit M.-J., 2010, P IEEE CONS COMM NET
   Nazer B, 2006, 2006 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY, VOLS 1-6, PROCEEDINGS, P1354, DOI 10.1109/ISIT.2006.262047
   Nguyen A.T., 2010, P IEEE INFOCOM
   Nguyen D., 2007, P PACK VID LAUS SWIT
   Nguyen K., 2007, P IEEE INT C MULT EX
   Omiwade S., 2008, P NETCOD HONG KONG J
   PADMANABHAN V, 2002, P 12 INT WORKSH NETW
   Pakzad P., 2005, P IEEE INT S INF THE
   ParandehGheibi A., 2011, P ANN INT TIT ASS IT
   Park Y., 2010, P IEEE VEH TECHN C T
   Pedersen M. V., 2011, P NETW COD APPL PROT
   Pedersen M. W., 2009, P 15 EUR WIR C EW AA
   Popovski P, 2006, IEEE ICC, P3885, DOI 10.1109/ICC.2006.255688
   Puri R., 1999, P 33 AS C SIGN SYST
   Ramasubramoniana A. K., 2009, P VIS COMM IM PROC V
   Reddy V, 2010, IEEE INFOCOM SER
   ROTH RM, 1991, IEEE T INFORM THEORY, V37, P328, DOI 10.1109/18.75248
   Saleh J. B., 2011, J SELECT AREAS T JAN
   Scellato S., 2010, P WORKSH ONL SOC NET
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Seferoglu H., 2009, P INT C MULT EXP ICM
   Seferoglu H., 2011, P ALL C ALL PARK MON
   Seferoglu H., 2009, P PACK VID SEATTL WA
   Seferoglu H., 2011, P IEEE INFOCOM
   Seferoglu H, 2009, IEEE J SEL AREA COMM, V27, P713, DOI 10.1109/JSAC.2009.090612
   Shojania H., 2010, P ACM MULT FIR IT OC
   Shojania H., 2009, P ACM NETW OP SYST S
   Shokrollahi A, 2006, IEEE T INFORM THEORY, V52, P2551, DOI 10.1109/TIT.2006.874390
   Silva D, 2008, IEEE T INFORM THEORY, V54, P3951, DOI 10.1109/TIT.2008.928291
   Sundaram N., 2003, P 43 ANN ALL C COMM
   Sundararajan JK, 2011, P IEEE, V99, P490, DOI 10.1109/JPROC.2010.2093850
   THOMOS N, 2007, P 1 ACM INT WORKSH M
   Thomos N., 2011, P INT S INF THEOR IS
   Thomos N, 2011, IEEE T MULTIMEDIA, V13, P776, DOI 10.1109/TMM.2011.2111364
   Thomos N, 2010, IEEE T CIRC SYST VID, V20, P1834, DOI 10.1109/TCSVT.2010.2087830
   Traskov D, 2006, 2006 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY, VOLS 1-6, PROCEEDINGS, P1758, DOI 10.1109/ISIT.2006.261656
   Venkataraman V., 2006, P 14 IEEE INT C NETW
   Vingelmann P., 2010, P ACM MULT 2010 WORK
   Vingelmann P., 2011, P IEEE CONS COMM NET
   Vingelmann P., 2011, P GLOB HOUST TX US D
   Vukobratovic D., 2012, P IEEE INT C MULT EX
   Vukobtatovic D., 2010, P IEEE INT WORKSH MU
   WALSH J, 2008, P 4 WORKSH NETW COD
   Wang H, 2010, J VIS COMMUN IMAGE R, V21, P77, DOI 10.1016/j.jvcir.2009.06.008
   Wang M., 2007, P 26 C IEEE COMM SOC
   Wang M, 2007, IEEE J SEL AREA COMM, V25, P1655, DOI 10.1109/JSAC.2007.071205
   Wang M, 2007, IEEE T MULTIMEDIA, V9, P1554, DOI 10.1109/TMM.2007.907460
   Wang Q., 2010, P IEEE INFOCOM MIN
   Wu MQ, 2005, SIGNAL PROCESS-IMAGE, V20, P728, DOI 10.1016/j.image.2005.05.002
   WU Y, 2005, P IEEE C INF SCI SYS
   Yang SH, 2011, IEEE INT SYMP INFO, P2647, DOI 10.1109/ISIT.2011.6034050
   Yang X., 2009, P 8 INT WORKSH PEER
   Yeung Raymond W., 2010, Frontiers of Electrical and Electronic Engineering in China, V5, P363, DOI 10.1007/s11460-010-0103-1
   Yeung R. W., 2008, INFORM TECHNOLOGY TR
   Yeung RW, 2006, COMMUN INF SYST, V6, P19
   Yoneki E., 2007, P ACM INT C MOD AN S
   Yu LJ, 2009, IEEE T CONSUM ELECTR, V55, P576, DOI 10.1109/TCE.2009.5174425
   Zhang G., 2010, ELSEVIER J COMPUT NE, V55, P1242
   Zhang M., 2005, P ACM MULT NOV
   Zhang SL, 2006, MOBICOM 2006, P358, DOI 10.1145/1161089.1161129
   Zhang X., 2005, P 24 C IEEE COMM SOC
NR 122
TC 74
Z9 84
U1 0
U2 51
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2013
VL 15
IS 5
BP 1195
EP 1212
DI 10.1109/TMM.2013.2241415
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 222WT
UT WOS:000324763600021
OA Green Submitted, Green Published
DA 2024-07-18
ER

PT J
AU Richter, F
   Ries, CX
   Cebron, N
   Lienhart, R
AF Richter, Fabian
   Ries, Christian X.
   Cebron, Nicolas
   Lienhart, Rainer
TI Learning to Reassemble Shredded Documents
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Annotated dataset; document assembly; graph algorithm; supervised
   learning
ID JIGSAW PUZZLES; RECONSTRUCTION
AB In this paper, we address the problem of automatically assembling shredded documents. We propose a two-step algorithmic framework. First, we digitize each fragment of a given document and extract shape- and content-based local features. Based on these multimodal features, we identify pairs of corresponding points on all pairs of fragments using an SVM classifier. Each pair is considered a point of attachment for aligning the respective fragments. In order to restore the layout of the document, we create a document graph in which nodes represent fragments and edges correspond to alignments. We assign weights to the edges by evaluating the alignments using a set of inter-fragment constraints which take into account shape- and content-based information. Finally, we use an iterative algorithm that chooses the edge having the highest weight during each iteration. However, since selecting edges corresponds to combining groups of fragments and thus provides new evidence, we reevaluate the edge weights after each iteration. We quantitatively evaluate the effectiveness of our approach by conducting experiments on a novel dataset. It comprises a total of 120 pages taken from two magazines which have been shredded and annotated manually. We thus provide the means for a quantitative evaluation of assembly algorithms which, to the best of our knowledge, has not been done before.
C1 [Richter, Fabian; Ries, Christian X.; Cebron, Nicolas; Lienhart, Rainer] Univ Augsburg, Multimedia Comp & Comp Vis Lab, D-86159 Augsburg, Germany.
C3 University of Augsburg
RP Richter, F (corresponding author), Univ Augsburg, Multimedia Comp & Comp Vis Lab, D-86159 Augsburg, Germany.
EM richter@informatik.uni-augsburg.de; ries@informatik.uni-augsburg.de;
   cebron@informatik.uni-augsburg.de; lienhart@informatik.uni-augsburg.de
OI Lienhart, Rainer/0000-0003-4007-6889
CR [Anonymous], P IEEE INT C MULT EX
   [Anonymous], 2011, ACM T INTEL SYST TEC
   Belongie S, 2000, IEEE WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO LIBRARIES, PROCEEDINGS, P20, DOI 10.1109/IVL.2000.853834
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Cao SJ, 2010, IEEE INT CON MULTI, P358, DOI 10.1109/ICME.2010.5582544
   Cho TS, 2010, PROC CVPR IEEE, P183, DOI 10.1109/CVPR.2010.5540212
   Chung MG, 1998, ICSP '98: 1998 FOURTH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING, PROCEEDINGS, VOLS I AND II, P877, DOI 10.1109/ICOSP.1998.770751
   Demaine ED, 2007, GRAPH COMBINATOR, V23, P195, DOI 10.1007/s00373-007-0713-4
   Douglas D.H., 1973, Cartographica: The International Journal for Geographic Information and Geovisualization, V10, P112, DOI [https://doi.org/10.3138/FM57-6770-U75U-7727, DOI 10.1002/9780470669488.CH2]
   FREEMAN H, 1964, IEEE T COMPUT, VEC13, P118, DOI 10.1109/PGEC.1964.263781
   Goldberg D, 2004, COMP GEOM-THEOR APPL, V28, P165, DOI 10.1016/j.comgeo.2004.03.007
   Kleber Florian, 2009, 2009 10th International Conference on Document Analysis and Recognition (ICDAR), P1061, DOI 10.1109/ICDAR.2009.154
   Kruskal J. B., 1956, Proceedings of the American Mathematical Society, V7, P48
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Papaodysseus C, 2002, IEEE T SIGNAL PROCES, V50, P1277, DOI 10.1109/TSP.2002.1003053
   RADACK GM, 1982, COMPUT VISION GRAPH, V19, P1, DOI 10.1016/0146-664X(82)90111-3
   Sagiroglu MS, 2006, INT C PATT RECOG, P1036
   SUZUKI S, 1985, COMPUT VISION GRAPH, V30, P32, DOI 10.1016/0734-189X(85)90016-7
   Zhu LJ, 2008, IEEE T PATTERN ANAL, V30, P1, DOI 10.1109/TPAMI.2007.1163
NR 19
TC 25
Z9 27
U1 1
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2013
VL 15
IS 3
BP 582
EP 593
DI 10.1109/TMM.2012.2235415
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 109ZM
UT WOS:000316410900010
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Li, YX
   Geng, B
   Tao, DC
   Zha, ZJ
   Yang, LJ
   Xu, C
AF Li, Yangxi
   Geng, Bo
   Tao, Dacheng
   Zha, Zheng-Jun
   Yang, Linjun
   Xu, Chao
TI Difficulty Guided Image Retrieval Using Linear Multiple Feature
   Embedding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Content based image retrieval; query difficulty estimation; spectral
   embedding
ID EIGENMAPS; SEARCH
AB Existing image retrieval systems suffer from a performance variance for different queries. Severe performance variance may greatly degrade the effectiveness of the subsequent query-dependent ranking optimization algorithms, especially those that utilize the information mined from the initial search results. In this paper, we tackle this problem by proposing a query difficulty guided image retrieval system, which can predict the queries' ranking performance in terms of their difficulties and adaptively apply ranking optimization approaches. We estimate the query difficulty by comprehensively exploring the information residing in the query image, the retrieval results, and the target database. To handle the high-dimensional and multi-model image features in the large-scale image retrieval setting, we propose a linear multiple feature embedding algorithm which learns a linear transformation from a small set of data by integrating a joint subspace in which the neighborhood information is preserved. The transformation can be effectively and efficiently used to infer the subspace features of the newly observed data in the online setting. We prove the significance of query difficulty to image retrieval by applying it to guide the conduction of three retrieval refinement applications, i.e., reranking, federated search, and query suggestion. Thorough empirical studies on three datasets suggest the effectiveness and scalability of the proposed image query difficulty estimation algorithm, as well as the promising of the image difficulty guided retrieval system.
C1 [Li, Yangxi; Geng, Bo; Xu, Chao] Peking Univ, Minist Educ, Key Lab Machine Percept, Beijing 100871, Peoples R China.
   [Xu, Chao] Peking Univ, Sch Elect Engn & Comp Sci, Beijing 100871, Peoples R China.
   [Tao, Dacheng] Univ Technol Sydney, Fac Engn & Informat Technol, Ctr Quantum Computat & Intelligent Syst, Sydney, NSW 2007, Australia.
   [Zha, Zheng-Jun] Natl Univ Singapore, Sch Comp, Singapore 117548, Singapore.
   [Yang, Linjun] Microsoft Res Asia, Media Comp Grp, Beijing, Peoples R China.
C3 Peking University; Peking University; University of Technology Sydney;
   National University of Singapore; Microsoft Research Asia; Microsoft
RP Li, YX (corresponding author), Peking Univ, Minist Educ, Key Lab Machine Percept, Beijing 100871, Peoples R China.
EM liyangxi@gmail.com; bogeng1985@gmail.com; dacheng.tao@uts.edu.au;
   zhazj@comp.nus.edu.sg; lin-juny@microsoft.com; xuchao@cis.pku.edu.cn
RI cheng, cheng/JBR-8359-2023; Zha, Zheng-Jun/AAE-8408-2020; Tao,
   Dacheng/A-5449-2012; Zha, Zheng-Jun/AAF-8667-2020
OI Zha, Zheng-Jun/0000-0003-2510-8993; Tao, Dacheng/0000-0001-7225-5449; 
FU NBRPC [2011CB302400]; NSFC [60975014, 61121002]; NSFB [4102024]
FX This work was supported in part by NBRPC 2011CB302400, NSFC 60975014,
   61121002, and NSFB 4102024. The associate editor coordinating the review
   of this manuscript and approving it for publication was Dr. Shin'ichi
   Satoh.
CR [Anonymous], 2008, Proceedings of the 17th ACM Conference on Information and Knowledge Management (CIKM '08)
   [Anonymous], 2008, P 2008 INT C CONTENT, DOI DOI 10.1145/1386352.1386373
   [Anonymous], 2009, P 17 ACM INT C MULTI
   [Anonymous], 2008, COMP VIS PATT REC 20
   Belkin M, 2002, ADV NEUR IN, V14, P585
   Bengio Y, 2004, ADV NEUR IN, V16, P177
   Bo Geng, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2396, DOI 10.1109/CVPRW.2009.5206695
   Callan J. P., 1995, SIGIR Forum, P21
   Carmel D., 2006, Proceedings of the Twenty-Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P390, DOI 10.1145/1148170.1148238
   Carpineto C, 2001, ACM T INFORM SYST, V19, P1, DOI 10.1145/366836.366860
   Chua T.-S., 2009, CIVR, P1, DOI 10.1145/1646396.1646452
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Cronen-Townsend S., 2002, Proceedings of SIGIR 2002. Twenty-Fifth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P299
   Deng J., 2009, PROC IEEE INT CONF I
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Geng B, 2010, PROC CVPR IEEE, P3400, DOI 10.1109/CVPR.2010.5540003
   Geng Bo., 2009, P ACM C INFORM KNOWL, P197
   Gibbons J.D., 2003, Nonparametric Statistical Inference, V168
   Guan NY, 2012, IEEE T NEUR NET LEAR, V23, P1087, DOI 10.1109/TNNLS.2012.2197827
   Guan NY, 2012, IEEE T SIGNAL PROCES, V60, P2882, DOI 10.1109/TSP.2012.2190406
   He X., 2004, PROC NIPS ADV NEURAL, V103
   He X, 2008, IEEE T KNOWL DATA EN, V20, P189, DOI 10.1109/TKDE.2007.190692
   Hotelling H, 1933, J EDUC PSYCHOL, V24, P417, DOI 10.1037/h0071325
   Hsu WinstonH., 2007, ACM MM
   Jinxi Xu, 1996, SIGIR Forum, P4
   Joachims T, 2002, P 8 ACM SIGKDD INT C, P133, DOI [DOI 10.1145/775047.775067, 10.1145/775047.775067]
   Kennedy L, 2008, P IEEE, V96, P567, DOI 10.1109/JPROC.2008.916345
   Long B., PROC 8TH SIAM INT CO, P822
   Si L, 2003, ACM T INFORM SYST, V21, P457, DOI 10.1145/944012.944017
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Song DJ, 2010, IEEE T IMAGE PROCESS, V19, P174, DOI 10.1109/TIP.2009.2032939
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Thomas P, 2009, PROCEEDINGS 32ND ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P419, DOI 10.1145/1571941.1572014
   Tian XM, 2011, IEEE T MULTIMEDIA, V13, P639, DOI 10.1109/TMM.2011.2111363
   Xia TA, 2010, IEEE T SYST MAN CY B, V40, P1438, DOI 10.1109/TSMCB.2009.2039566
   Yan R, 2003, LECT NOTES COMPUT SC, V2728, P238
   Yom-Tov E., 2005, SIGIR 2005. Proceedings of the Twenty-Eighth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P512, DOI 10.1145/1076034.1076121
   Zha Z.-J., 2008, 2008 IEEE C COMPUTER, DOI DOI 10.1109/CVPR.2008.4587384
   Zhao Y, 2008, LECT NOTES COMPUT SC, V4956, P52
   Zhou Y., 2006, CIKM, P567
NR 41
TC 23
Z9 26
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2012
VL 14
IS 6
BP 1618
EP 1630
DI 10.1109/TMM.2012.2199292
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 046YE
UT WOS:000311800400011
DA 2024-07-18
ER

PT J
AU Ng, KT
   Zhu, ZY
   Wang, C
   Chan, SC
   Shum, HY
AF Ng, King-To
   Zhu, Zhen-Yu
   Wang, Chong
   Chan, Shing-Chow
   Shum, Heung-Yeung
TI A Multi-Camera Approach to Image-Based Rendering and 3-D/Multiview
   Display of Ancient Chinese Artifacts
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE IBR object coding; image-based rendering; object-based coding; plenoptic
   videos
ID LIGHT-FIELD; DATA-COMPRESSION; VIDEO; 3-D
AB This paper proposes an image-based approach for the capturing, rendering and display of ancient Chinese artifacts for cultural heritage preservation. A multiple-camera circular array is proposed to record images of the artifacts, which forms a simplified circular light field (SCLF). A systematic image-based approach and associate algorithms such as segmentation, depth estimation and shape morphing are developed for rendering new views of the Chinese artifacts. An object-based compression scheme is also proposed to reduce the data size for storage and transmission of the texture, depth maps and alpha maps associated with the object-based circular light field. Spatial redundancies among the various images are exploited to improve the coding performance, while avoiding excessive complexity in selective decoding of the light field to support fast rendering speed. To allow the Chinese artifacts to be viewed over the internet, scalable prioritized transmission and rendering schemes of the SCLF with low latency were also developed. The multiple views so synthesized enable the ancient artifacts to be displayed in 3-D/multi-view displays. Several collections from the University Museum and Art Gallery at The University of Hong Kong were captured and excellent rendering results are obtained.
C1 [Ng, King-To; Zhu, Zhen-Yu; Wang, Chong; Chan, Shing-Chow] Univ Hong Kong, Dept Elect & Elect Engn, Hong Kong, Hong Kong, Peoples R China.
   [Shum, Heung-Yeung] Microsoft Corp, Search Prod Dev, Redmond, WA 98052 USA.
C3 University of Hong Kong; Microsoft
RP Ng, KT (corresponding author), Univ Hong Kong, Dept Elect & Elect Engn, Hong Kong, Hong Kong, Peoples R China.
EM ktng@eee.hku.hk; zyzhu@eee.hku.hk; cwang@eee.hku.hk; scchan@eee.hku.hk;
   hshum@microsoft.com
RI Wang, Chong/IRZ-7328-2023
OI Wang, Chong/0000-0001-6016-6545
FU Hong Kong Research Grant Council (RGC); Innovation and Technology fund
   (ITF)
FX This work was supported in part by Hong Kong Research Grant Council
   (RGC) and the Innovation and Technology fund (ITF). The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Monica Aguilar.
CR Adelson E.H., 1991, Computational Models of Visual Processing, P3
   [Anonymous], P INT C IM PROC SEP
   [Anonymous], 2004, 1544412004 ISOIEC
   [Anonymous], 2003, Geometric Level Set Methods in Imaging, Vision, and Graphics
   Attardi G., 1999, P ANN C COMP GRAPH S, P223
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   BYRD RH, 1995, SIAM J SCI COMPUT, V16, P1190, DOI 10.1137/0916069
   Carranza J, 2003, ACM T GRAPHIC, V22, P569, DOI 10.1145/882262.882309
   Cha Zhang, 2000, Proceedings DCC 2000. Data Compression Conference, P253, DOI 10.1109/DCC.2000.838165
   Chan SC, 2007, IEEE SIGNAL PROC MAG, V24, P22, DOI 10.1109/MSP.2007.905702
   Chan SC, 2005, IEEE T CIRC SYST VID, V15, P1650, DOI 10.1109/TCSVT.2005.858616
   Chan SC, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL 3, PROCEEDINGS, P905
   Chan SC, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P653
   Chan SC, 2009, IEEE T CIRC SYST VID, V19, P821, DOI 10.1109/TCSVT.2009.2017302
   Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291
   Chen SP, 1996, PROCEEDINGS OF THE IEEE SOUTHEASTCON '96, P29, DOI 10.1109/SECON.1996.510021
   Criminisi A, 2003, PROC CVPR IEEE, P721
   Flierl M, 2007, IEEE SIGNAL PROC MAG, V24, P66, DOI 10.1109/MSP.2007.905699
   Fujii T, 1996, PICT COD S 96, V2, P447
   Gan ZF, 2005, AVSS 2005: ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, PROCEEDINGS, P81
   *GB T, 2006, 2009022006 GBT
   Goldlücke B, 2002, VISION MODELING, AND VISUALIZATION 2002, PROCEEDINGS, P455
   Gortler S. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P43, DOI 10.1145/237170.237200
   Huang XL, 2006, IEEE T PATTERN ANAL, V28, P1303, DOI 10.1109/TPAMI.2006.171
   Ikeuchi K, 2003, SECOND IEEE AND ACM INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, PROCEEDINGS, P7, DOI 10.1109/ISMAR.2003.1240683
   Ikeuchi K., ENCY COMPUTER VISION
   ISO/IEC, 2010, 14496102010 ISOIEC
   ISO/IEC JTC1/SC29/WG11, 2005, N6909 ISOIEC JTC1SC2
   KIMATA H, 2004, P PICT COD S, P499
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Levoy M., 2002, P ANN C COMP GRAPH S, P131
   LI J, 2001, INT J IMAGE GRAPHICS, V1, P45
   Li Y, 2004, ACM T GRAPHIC, V23, P303, DOI 10.1145/1015706.1015719
   Lu JB, 2006, IEEE IMAGE PROC, P1089, DOI 10.1109/ICIP.2006.312745
   Lukacs M. E., 1986, ICASSP 86 Proceedings. IEEE-IECEJ-ASJ International Conference on Acoustics, Speech and Signal Processing (Cat. No.86CH2243-4), P521
   Magnor M, 2000, IEEE T CIRC SYST VID, V10, P338, DOI 10.1109/76.836278
   McMillan L., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P39, DOI 10.1145/218380.218398
   Merkle P, 2007, IEEE T CIRC SYST VID, V17, P1461, DOI 10.1109/TCSVT.2007.903665
   Müueller K, 2007, IEEE SIGNAL PROC MAG, V24, P58, DOI 10.1109/MSP2007.905697
   Naemura T, 2002, IEEE COMPUT GRAPH, V22, P66, DOI 10.1109/38.988748
   Naemura T, 1999, IEICE T INF SYST, VE82D, P558
   Ng KT, 2010, IEEE T CIRC SYST VID, V20, P548, DOI 10.1109/TCSVT.2010.2041820
   Ng KT, 2005, IEEE T CIRC SYST VID, V15, P82, DOI 10.1109/TCSVT.2004.839989
   OHM JR, 1999, IEEE SIGNAL PROCESSI, V16, P47
   OHM JR, 1999, P EL IM SAN DIEG CA, P242
   Puri A, 1997, SIGNAL PROCESS-IMAGE, V10, P201, DOI 10.1016/S0923-5965(97)00025-8
   Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977
   Sethian J., 1999, LEVEL SET METHODS FA
   Shum H., 2007, IMAGE BASED RENDERIN
   Shum HY, 2004, ACM T GRAPHIC, V23, P143, DOI 10.1145/990002.990005
   Shum HY, 2003, IEEE T CIRC SYST VID, V13, P1020, DOI 10.1109/TCSVT.2003.817360
   Shum HY, 1999, COMP GRAPH, P299, DOI 10.1145/311535.311573
   Smolic A, 2007, IEEE T CIRC SYST VID, V17, P1606, DOI 10.1109/TCSVT.2007.909972
   Strintzis MG, 1999, IEEE SIGNAL PROC MAG, V16, P14, DOI 10.1109/79.768570
   Sun J, 2005, PROC CVPR IEEE, P399
   Tong X, 2000, INT CONF ACOUST SPEE, P1879, DOI 10.1109/ICASSP.2000.859194
   van de Weijer J, 2006, IEEE T PATTERN ANAL, V28, P150, DOI 10.1109/TPAMI.2006.3
   Wang Y., 2002, VIDEO PROCESSING COM
   Wilburn B, 2002, PROC SPIE, V4674, P29
   Yang J. C., 2002, Rendering Techniques 2002. Eurographics Workshop Proceedings, P77
   Yang WX, 2006, IEEE T CIRC SYST VID, V16, P1385, DOI 10.1109/TCSVT.2006.884571
   Yao XZ, 2010, PROCEEDINGS OF THE 2010 IEEE ASIA PACIFIC CONFERENCE ON CIRCUIT AND SYSTEM (APCCAS), P340, DOI 10.1109/APCCAS.2010.5774987
   Yoon KJ, 2006, IEEE T PATTERN ANAL, V28, P650, DOI 10.1109/TPAMI.2006.70
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
   Zhu ZY, 2010, IEEE INT SYMP CIRC S, P3252, DOI 10.1109/ISCAS.2010.5537925
NR 65
TC 8
Z9 9
U1 0
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2012
VL 14
IS 6
BP 1631
EP 1641
DI 10.1109/TMM.2012.2199291
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 046YE
UT WOS:000311800400012
DA 2024-07-18
ER

PT J
AU Yeh, LY
   Tsaur, WJ
AF Yeh, Lo-Yao
   Tsaur, Woei-Jiunn
TI A Secure and Efficient Authentication Scheme for Access Control in
   Mobile Pay-TV Systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Authentication; conditional access systems; pay-TV services
ID KEY DISTRIBUTION
AB Recently, Sun and Leu proposed an efficient authentication scheme for access control in mobile pay-TV systems based on pairing and elliptic curve cryptography. In Sun and Leu's scheme, there exist two fatal security flaws, including 1) failure in subscriber authentication and 2) vulnerable to unauthorized access, which contradict to the security claims of Sun and Leu's scheme. To meet the claimed security requirements, we strengthen Sun and Leu's scheme with only lightweight modifications.
C1 [Yeh, Lo-Yao] Natl Ctr High Performance Comp NCHC, Network & Informat Secur Div, Tainan, Taiwan.
   [Yeh, Lo-Yao] Natl Chi Nan Univ, Dept Informat Management, Nantou, Taiwan.
   [Tsaur, Woei-Jiunn] Da Yeh Univ, Dept Informat Management, Changhua, Taiwan.
C3 National Chi Nan University; Da Yeh University
RP Yeh, LY (corresponding author), Natl Ctr High Performance Comp NCHC, Network & Informat Secur Div, Tainan, Taiwan.
EM lyyeh@nchc.narl.org.tw; wjtsaur@yahoo.com.tw
FU National Science Council, Taiwan, R.O.C. [NSC 101-2219-E-212-001]
FX This work was supported in part by the National Science Council, Taiwan,
   R.O.C., under Grant NSC 101-2219-E-212-001. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Yiannis Andreopoulos.
CR [Anonymous], 2004, GUIDE ELLIPTIC CURVE, DOI DOI 10.1007/B97644
   [Anonymous], 2007, 102474 ETSI TS
   Díaz-Sánchez D, 2009, IEEE T CONSUM ELECTR, V55, P88, DOI 10.1109/TCE.2009.4814419
   Huang JL, 2011, IEEE T VEH TECHNOL, V60, P248, DOI 10.1109/TVT.2010.2089544
   Huang YL, 2004, IEEE T MULTIMEDIA, V6, P760, DOI 10.1109/TMM.2004.834861
   Kim JY, 2010, IEEE T MULTIMEDIA, V12, P337, DOI 10.1109/TMM.2010.2046362
   Roh H, 2011, IEEE INTERNATIONAL CONFERENCE ON CONSUMER ELECTRONICS (ICCE 2011), P717, DOI 10.1109/ICCE.2011.5722826
   Shirazi H, 2010, IEEE T BROADCAST, V56, P44, DOI 10.1109/TBC.2009.2036956
   Sun HM, 2008, IEEE T MULTIMEDIA, V10, P1109, DOI 10.1109/TMM.2008.2001381
   Sun HM, 2009, IEEE T MULTIMEDIA, V11, P947, DOI 10.1109/TMM.2009.2021790
   Wang SY, 2008, IEEE T MULTIMEDIA, V10, P480, DOI 10.1109/TMM.2008.917417
   Yeung SF, 2005, IEEE T MULTIMEDIA, V7, P330, DOI 10.1109/TMM.2005.843361
   Zhu WT, 2008, IEEE T MULTIMEDIA, V10, P1214, DOI 10.1109/TMM.2008.2001376
NR 13
TC 14
Z9 14
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2012
VL 14
IS 6
BP 1690
EP 1693
DI 10.1109/TMM.2012.2199290
PG 4
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 046YE
UT WOS:000311800400018
DA 2024-07-18
ER

PT J
AU Ikizler-Cinbis, N
   Sclaroff, S
AF Ikizler-Cinbis, Nazli
   Sclaroff, Stan
TI Web-Based Classifiers for Human Action Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Computer vision; image retrieval; statistical learning
ID IMAGE; OBJECT
AB Action recognition in uncontrolled videos is a challenging task, where it is relatively hard to find the large amount of required training videos to model all the variations of the domain. This paper addresses this challenge and proposes a generic method for action recognition. The idea is to use images collected from the Web to learn representations of actions and leverage this knowledge to automatically annotate actions in videos. For this purpose, we first use an incremental image retrieval procedure to collect and clean up the necessary training set for building the human pose classifiers. Our approach is unsupervised in the sense that it requires no human intervention other than the text querying to an internet search engine. Its benefits are two-fold: 1) we can improve retrieval of action images, and 2) we can collect a large generic database of action poses, which can then be used in tagging videos. We present experimental evidence that using action images collected from the Web, annotating actions in the videos is possible. Additionally, we explore how the Web-based pose classifiers can be utilized in conjunction with limited labelled videos. We propose to use "ordered pose pairs" (OPP) for encoding the temporal ordering of poses in our action model, and show that considering the temporal ordering of pose pairs can increase the action recognition accuracy. We also show that by selecting the keyposes with the help of Web-based classifiers, the classification time can be reduced. Our experiments demonstrate that, with or without available video data, the pose models learned from the Web can improve the performance of the action recognition systems.
C1 [Ikizler-Cinbis, Nazli] Hacettepe Univ, Dept Comp Engn, TR-06532 Ankara, Turkey.
   [Sclaroff, Stan] Boston Univ, Dept Comp Sci, Boston, MA 02215 USA.
C3 Hacettepe University; Boston University
RP Ikizler-Cinbis, N (corresponding author), Hacettepe Univ, Dept Comp Engn, TR-06532 Ankara, Turkey.
EM nazli@cs.hacettepe.edu.tr; sclaroff@cs.bu.edu
RI Ikizler-Cinbis, Nazli/E-8961-2013
FU Google Research Award
FX This work has been supported in part by a Google Research Award. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Qi Tian.
CR [Anonymous], P 17 ACM INT C MULT
   [Anonymous], P 11 EUR C COMP VI 1
   [Anonymous], 2006, Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition
   Berg TamaraL., 2006, CVPR, DOI DOI 10.1109/CVPR.2006.57
   BERG TL, 2009, 2 INT VIS WORKSH IEE
   Bian W, 2012, IEEE T SYST MAN CY B, V42, P298, DOI 10.1109/TSMCB.2011.2166761
   Bissacco A., 2006, P ADV NEUR INF PROC, P169
   Blank M, 2005, IEEE I CONF COMP VIS, P1395
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dollár P, 2008, LECT NOTES COMPUT SC, V5303, P211, DOI 10.1007/978-3-540-88688-4_16
   Duan L., 2010, CVPR SAN FRANC CA
   Duchenne O, 2009, IEEE I CONF COMP VIS, P1491, DOI 10.1109/ICCV.2009.5459279
   Felzenszwalb P., 2008, CVPR ANCH AK
   Fergus R, 2005, IEEE I CONF COMP VIS, P1816
   Guan NY, 2011, IEEE T NEURAL NETWOR, V22, P1218, DOI 10.1109/TNN.2011.2157359
   Hamid R, 2005, PROC CVPR IEEE, P1031
   Ikizler N, 2008, INT J COMPUT VISION, V80, P337, DOI 10.1007/s11263-008-0142-8
   Ikizler-Cinbis N, 2009, IEEE I CONF COMP VIS, P995, DOI 10.1109/ICCV.2009.5459368
   Jhuang H., 2007, ICCV RIO DE JAN BRAZ
   Ke Y., 2007, INT C COMP VIS RIO D
   KIM TK, 2007, CVPR MINN MN
   Laptev I., 2008, CVPR ANCH AK
   Lee DD, 2001, ADV NEUR IN, V13, P556
   Li L.-J., 2007, CVPR MINN MN
   Li X., 2006, P ACM MM
   Li XR, 2009, IEEE T MULTIMEDIA, V11, P1310, DOI 10.1109/TMM.2009.2030598
   Liu JG, 2009, PROC CVPR IEEE, P1996
   Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918
   Mikolajczyk K., 2008, CVPR ANCH AK
   Ni BB, 2011, IEEE T MULTIMEDIA, V13, P1217, DOI 10.1109/TMM.2011.2167317
   Niebles JC, 2008, LECT NOTES COMPUT SC, V5305, P527, DOI 10.1007/978-3-540-88693-8_39
   Niebles JuanCarlos., 2006, British Machine Vision Conference, V3, P1249
   Okada R, 2008, LECT NOTES COMPUT SC, V5303, P434, DOI 10.1007/978-3-540-88688-4_32
   Ponce J, 2006, LECT NOTES COMPUT SC, V4170, P29
   Poppe R, 2010, IMAGE VISION COMPUT, V28, P976, DOI 10.1016/j.imavis.2009.11.014
   Schindler K., 2008, CVPR ANCH AK
   Schroff F., 2007, ICCV RIO DE JAN BRAZ
   Schüldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794
   Thurau C., 2008, CVPR ANCH AK
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Tran D, 2008, LECT NOTES COMPUT SC, V5302, P548, DOI 10.1007/978-3-540-88682-2_42
   Ulges A, 2011, IEEE T MULTIMEDIA, V13, P330, DOI 10.1109/TMM.2010.2101051
   Vedaldi A, 2010, PROC CVPR IEEE, P3539, DOI 10.1109/CVPR.2010.5539949
   Vijayanarasimhan S., 2008, CVPR ANCH AK
   Wang G., 2008, CVPR ANCH AK
   Wang G, 2009, IEEE I CONF COMP VIS, P428, DOI 10.1109/ICCV.2009.5459167
   Wang XJ, 2008, IEEE T PATTERN ANAL, V30, P1919, DOI 10.1109/TPAMI.2008.127
   Weinland D., 2008, CVPR ANCH AK
   Weinland D, 2011, COMPUT VIS IMAGE UND, V115, P224, DOI 10.1016/j.cviu.2010.10.002
   Wnuk K., 2008, IEEE C COMP VIS PATT
   Yao B., 2010, 23 IEEE C COMP VIS P
   Zanetti S., 2008, 1 IEEE WORKSH INT VI
   Zhang Z, 2012, IEEE T PATTERN ANAL, V34, P436, DOI 10.1109/TPAMI.2011.157
   Zheng VW, 2009, UBICOMP'09: PROCEEDINGS OF THE 11TH ACM INTERNATIONAL CONFERENCE ON UBIQUITOUS COMPUTING, P61
NR 56
TC 22
Z9 27
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2012
VL 14
IS 4
SI SI
BP 1031
EP 1045
DI 10.1109/TMM.2012.2187180
PN 1
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 976QL
UT WOS:000306599300009
DA 2024-07-18
ER

PT J
AU Zhao, HV
   Lin, WS
   Liu, KJR
AF Zhao, H. Vicky
   Lin, W. Sabrina
   Liu, K. J. Ray
TI Cooperation and Coalition in Multimedia Fingerprinting Colluder Social
   Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Coalition formation; game theory; multiuser collusion; multimedia
   fingerprinting
ID COLLUSION; FORENSICS
AB Users in multimedia social networks actively interact with each other. It is crucial to study the complex user dynamics and analyze its impact on the performance of multimedia social networks. This paper uses multimedia fingerprinting as an example and studies user dynamics in colluder social networks. During collusion, a group of attackers collectively attack multimedia fingerprinting system and use multimedia content illegally. This paper analyzes the incentives of cooperation among attackers and investigates how colluders form their coalitions to maximize their payoffs. We present a game-theoretic framework to model the complex dynamics among colluders, analyze when attackers cooperate with each other, and investigate how a colluder selects his/her fellow attackers to maximize his/her own payoff. We analyze multiuser collusion in two scenarios: when all attackers receive fingerprinted copies of the same resolution, and when they have copies of different resolutions. The proposed framework considers both the colluders' risk of being detected by the digital rights enforcer and the reward received from illegal usage of multimedia content. Our analysis shows that in both scenarios, colluding with more attackers does not always increase an attacker's utility, and attackers may not always want to cooperate with each other. We first examine the necessary conditions for attackers to collude together, and study how they select the collusion parameters such that cooperation benefits all colluders. We then study how the number of colluders affects each attacker's utility, and investigate the optimum strategy that an attacker should use to select fellow attackers and to form a coalition in order to maximize his or her own payoff.
C1 [Zhao, H. Vicky] Univ Alberta, Dept Elect & Comp Engn, Edmonton, AB T6G 2V4, Canada.
   [Lin, W. Sabrina] Intel Corp, Hillsboro, OR 97124 USA.
   [Liu, K. J. Ray] Univ Maryland, Dept Elect & Comp Engn, College Pk, MD 20742 USA.
C3 University of Alberta; Intel Corporation; University System of Maryland;
   University of Maryland College Park
RP Zhao, HV (corresponding author), Univ Alberta, Dept Elect & Comp Engn, Edmonton, AB T6G 2V4, Canada.
EM vzhao@ece.ualberta.ca; wylin1981@gmail.com; kjrliu@umd.edu
RI Liu, K.J. Ray/C-2798-2009
CR [Anonymous], 1996, 96045 NEC RES I
   [Anonymous], 1988, SOCIAL STRUCTURES NE
   Cheng X, 2008, INT WORKSH QUAL SERV, P249
   Dittmann J, 2000, J ELECTRON IMAGING, V9, P456, DOI 10.1117/1.1287729
   Ergun F., 2001, Advances in Cryptology: Lecture Notes in Computer Science, V1592, P140
   Fortunato S, 2010, PHYS REP, V486, P75, DOI 10.1016/j.physrep.2009.11.002
   Gummadi KrishnaP., 2003, Proceedings of the Nineteenth ACM Symposium on Operating Systems Principles, SOSP '03, P314
   Hei XJ, 2008, IEEE COMMUN MAG, V46, P86, DOI 10.1109/MCOM.2008.4473088
   Kirovski D, 2005, INT CONF ACOUST SPEE, P1037
   Knoke D., 2008, QUANTITATIVE APPL SO, V154
   Lee CS, 2007, IEEE I CONF COMP VIS, P1570
   Lin WS, 2011, IEEE T MULTIMEDIA, V13, P191, DOI 10.1109/TMM.2010.2102345
   Lin WS, 2009, IEEE T INF FOREN SEC, V4, P911, DOI 10.1109/TIFS.2009.2033224
   LIU KJR, 2005, EURASIP BOOK SERIES
   Lua EK, 2005, IEEE COMMUN SURV TUT, V7, P72, DOI 10.1109/COMST.2005.1610546
   Podilchuk CI, 1998, IEEE J SEL AREA COMM, V16, P525, DOI 10.1109/49.668975
   Saad Walid, 2008, ICC Workshops 2008 - IEEE International Conference on Communications Workshops, P311, DOI 10.1109/ICCW.2008.65
   Saad W, 2009, IEEE INFOCOM SER, P2114, DOI 10.1109/INFCOM.2009.5062135
   Skoric B, 2008, DESIGN CODE CRYPTOGR, V46, P137, DOI 10.1007/s10623-007-9142-x
   Su K, 2005, IEEE T MULTIMEDIA, V7, P43, DOI 10.1109/TMM.2004.840617
   Tardos G., 2003, Proceedings of the 35th Annual ACM Symposium on Theory of Computing, P116, DOI DOI 10.1145/780542.780561
   Wang Y., 2001, VIDEO PROCESSING COM
   Wang ZJ, 2005, IEEE T IMAGE PROCESS, V14, P804, DOI 10.1109/TIP.2005.847284
   Zane F, 2001, LECT NOTES COMPUT SC, V1962, P21
   Zhao HV, 2006, IEEE T INF FOREN SEC, V1, P311, DOI 10.1109/TIFS.2006.879279
NR 25
TC 5
Z9 9
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2012
VL 14
IS 3
BP 717
EP 733
DI 10.1109/TMM.2012.2191394
PN 2
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 943ZK
UT WOS:000304166700006
DA 2024-07-18
ER

PT J
AU Bao, XN
   Zhou, DJ
   Liu, PL
   Goto, S
AF Bao, Xuena
   Zhou, Dajiang
   Liu, Peilin
   Goto, Satoshi
TI An Advanced Hierarchical Motion Estimation Scheme With Lossless Frame
   Recompression and Early-Level Termination for Beyond High-Definition
   Video Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Beyond high definition (BHD); early-level termination; hierarchical
   motion estimation; lossless frame recompression; video coding
ID ENCODER CHIP; H.264/AVC; ALGORITHM; DESIGN
AB In this paper, we present a hardware-efficient fast algorithm with a lossless frame recompression scheme and early-level termination strategy for large search range (SR) motion estimation (ME) utilized in beyond high-definition video encoder. To achieve high ME quality for hierarchical motion search, we propose an advanced hierarchical ME scheme which processes the multiresolution motion search with an efficient refining stage. This enables high data and hardware reuse for much lower bandwidth and memory cost, while achieving higher ME quality than previous works. In addition, a lossless frame recompression scheme based on this ME algorithm is presented to further reduce bandwidth. A hierarchical memory organization as well as a leveling two-step data fetching strategy is applied to meet constraint of random access for hierarchical motion search structure. Also, the leveling compression strategy by allowing a lower level to refer to a higher one for compression is proposed to efficiently reduce the bandwidth. Furthermore, an early-level termination method suitable for hierarchical ME structure is also applied. This method terminates high-level redundant motion searches by establishing thresholds based on current block mode and motion search level; it also applies the early refinement termination in order to avoid unnecessary refinement for high levels. Experimental results show that the total scheme has a much lower bit rate increasing compared with previous works especially for high motion sequences, while achieving a considerable saving of memory and bandwidth cost for large SR of [-128, 127]..
C1 [Bao, Xuena; Zhou, Dajiang; Goto, Satoshi] Waseda Univ, Grad Sch Informat Prod & Syst, Kitakyushu, Fukuoka 8080135, Japan.
   [Liu, Peilin] Shanghai Jiao Tong Univ, Dept Elect Engn, Shanghai 200240, Peoples R China.
C3 Waseda University; Shanghai Jiao Tong University
RP Bao, XN (corresponding author), Shanghai Jiao Tong Univ, Dept Elect Engn, Shanghai 200240, Peoples R China.
EM baoxuena@sjtu.edu.cn; zhou@fuji.waseda.jp; liupeilin@sjtu.edu.cn;
   goto@waseda.jp
FU Knowledge Cluster Initiative of MEXT, Japan; CREST of Japan Science and
   Technology Agency; Waseda University Ambient SoC Global COE of MEXT,
   Japan
FX Manuscript received March 05, 2011; revised May 24, 2011; accepted
   September 26, 2011. Date of publication October 13, 2011; date of
   current version March 21, 2012. This work was supported by Waseda
   University Ambient SoC Global COE Program of MEXT, Japan, by Knowledge
   Cluster Initiative (2nd Stage) of MEXT, Japan, and by CREST of Japan
   Science and Technology Agency. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr.
   Ming-Ting Sun.
CR [Anonymous], P INT C DIG TEL
   Bao XN, 2010, IEEE INT SYMP CIRC S, P677, DOI 10.1109/ISCAS.2010.5537495
   Bao XN, 2010, IEEE INT CON MULTI, P820, DOI 10.1109/ICME.2010.5583073
   Bjontegaard G., 2001, P VCEG M33 APR
   Budagavi M, 2008, INT CONF ACOUST SPEE, P1165, DOI 10.1109/ICASSP.2008.4517822
   Chen CY, 2006, IEEE T MULTIMEDIA, V8, P698, DOI 10.1109/TMM.2006.876296
   Choi JA, 2010, IEEE IMAGE PROC, P1217, DOI 10.1109/ICIP.2010.5652438
   Deng CW, 2010, IEEE IMAGE PROC, P2037, DOI 10.1109/ICIP.2010.5651661
   Ding LF, 2010, IEEE J SOLID-ST CIRC, V45, P46, DOI 10.1109/JSSC.2009.2031787
   Günyel B, 2010, IEEE IMAGE PROC, P2793, DOI 10.1109/ICIP.2010.5651076
   Lee TY, 2003, IEEE T CIRC SYST VID, V13, P529, DOI 10.1109/TCSVT.2003.813425
   Lee YJ, 2007, IEEE INT SYMP CIRC S, P1621, DOI 10.1109/ISCAS.2007.378829
   LIN CC, 2007, P ICASSP APR, V2, P385
   Lin W., 2009, P ISCAS MAY, P625
   LIN YK, 2008, IEEE ISSCC, P314
   Liu ZY, 2009, IEEE J SOLID-ST CIRC, V44, P594, DOI 10.1109/JSSC.2008.2010797
   Ndili O., 2010, P IEEE ICIP SEP, P749
   Sarwer MG, 2009, IEEE T CIRC SYST VID, V19, P1196, DOI 10.1109/TCSVT.2009.2020322
   Sekiguchi S., 2010, P PCS DEC, P322
   Song JH, 2010, IEEE IMAGE PROC, P3745, DOI 10.1109/ICIP.2010.5653435
   Tsung PK, 2009, INT CONF ACOUST SPEE, P2013, DOI 10.1109/ICASSP.2009.4960008
   Wu WT, 2010, IEEE IMAGE PROC, P1245, DOI 10.1109/ICIP.2010.5652352
   XU J, 2003, P INT C INF COMM SIG, V1, P218
   Yi X., 2005, P JVT P021 JUL
   Zhou DJ, 2010, SYMP VLSI CIRCUITS, P171, DOI 10.1109/VLSIC.2010.5560311
NR 25
TC 25
Z9 30
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2012
VL 14
IS 2
BP 237
EP 249
DI 10.1109/TMM.2011.2171677
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 924OZ
UT WOS:000302702500001
DA 2024-07-18
ER

PT J
AU Khan, A
   Sun, LF
   Ifeachor, E
AF Khan, Asiya
   Sun, Lingfen
   Ifeachor, Emmanuel
TI QoE Prediction Model and its Application in Video Quality Adaptation
   Over UMTS Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Content types; MOS; non-intrusive model; NS2; QoE; UMTS; video quality
   prediction and adaptation
ID BIT-RATE
AB The primary aim of this paper is to present a new content-based, non-intrusive quality of experience (QoE) prediction model for low bitrate and resolution (QCIF) H.264 encoded videos and to illustrate its application in video quality adaptation over Universal Mobile Telecommunication Systems (UMTS) networks. The success of video applications over UMTS networks very much depends on meeting the QoE requirements of users. Thus, it is highly desirable to be able to predict and, if appropriate, to control video quality to meet such QoE requirements. Video quality is affected by distortions caused both by the encoder and theUMTS access network. The impact of these distortions is content dependent, but this feature is not widely used in non-intrusive video quality prediction models. In the new model, we chose four key parameters that can impact video quality and hence the QoE-content type, sender bitrate, block error rate and mean burst length. The video quality was predicted in terms of the mean opinion score (MOS). Subjective quality tests were carried out to develop and evaluate the model. The performance of the model was evaluated with unseen dataset with good prediction accuracy. The model also performed well with the LIVE database which was recently made available to the research community. We illustrate the application of the new model in a novel QoE-driven adaptation scheme at the pre-encoding stage in a UMTS network. Simulation results in NS2 demonstrate the effectiveness of the proposed adaptation scheme, especially at the UMTS access network which is a bottleneck. An advantage of the model is that it is light weight (and so it can be implemented for real-time monitoring), and it provides a measure of user-perceived quality, but without requiring time-consuming subjective tests. The model has potential applications in several other areas, including QoE control and optimization in network planning and content provisioning for network/service providers.
C1 [Khan, Asiya; Sun, Lingfen; Ifeachor, Emmanuel] Univ Plymouth, Sch Comp & Math, Ctr Signal Proc & Multimedia Commun, Plymouth PL4 8AA, Devon, England.
C3 University of Plymouth
RP Khan, A (corresponding author), Univ Plymouth, Sch Comp & Math, Ctr Signal Proc & Multimedia Commun, Plymouth PL4 8AA, Devon, England.
EM asiya.khan@plymouth.ac.uk; l.sun@plymouth.ac.uk;
   e.ifeachor@plymouth.ac.uk
OI Sun, Lingfen/0000-0002-9921-2817; Khan, Asiya/0000-0003-3620-3048
FU EU [214751]
FX Manuscript received January 05, 2011; revised May 31, 2011 and October
   29, 2011; accepted November 01, 2011. Date of publication November 16,
   2011; date of current version March 21, 2012. This work was supported in
   part by the EU FP7 ADAMANTIUM project (contract No. 214751). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. James E. Fowler.
CR [Anonymous], P 1 ERCIM WORKSH EMO
   [Anonymous], BT50011 INT TEL UN
   [Anonymous], 9 ITUT SG
   [Anonymous], ENHANCED UMTS RADIO
   [Anonymous], 2007, P 3 INT C NETW SERV
   [Anonymous], P IEEE GLOB SAN FRAN
   [Anonymous], RFC3448
   [Anonymous], IEEE TECHNOL UPDATES
   [Anonymous], DEGRADED VIDEO CLIPS
   [Anonymous], JM H 264 SOFTWARE
   [Anonymous], LIVE WIRELESS VIDEO
   [Anonymous], 25322 3GPP TS
   [Anonymous], FIN REP VID QUAL EXP
   [Anonymous], OPNET RES
   [Anonymous], P 1 INT WORKSH QUAL
   [Anonymous], RTP CONTROL PROTOCOL
   Calyam P, 2007, J COMMUN NETW-S KOR, V9, P446, DOI 10.1109/JCN.2007.6182880
   Ciubotaru B, 2009, IEEE T BROADCAST, V55, P202, DOI 10.1109/TBC.2009.2020448
   Eden A, 2008, IEEE T BROADCAST, V54, P691, DOI 10.1109/TBC.2008.2001718
   Feghali R, 2007, IEEE T BROADCAST, V53, P441, DOI 10.1109/TBC.2007.891700
   GILBERT EN, 1960, BELL SYST TECH J, V39, P1253, DOI 10.1002/j.1538-7305.1960.tb03959.x
   Hands D, 2005, BT TECHNOL J, V23, P208, DOI 10.1007/s10550-005-0017-2
   Huynh-Thu Q, 2008, IEEE T BROADCAST, V54, P641, DOI 10.1109/TBC.2008.2001246
   International Telecommunications Union, 2008, SUBJ VID QUAL ASS ME
   Jammeh E, 2008, IEEE T CIRC SYST VID, V18, P387, DOI 10.1109/TCSVT.2008.918459
   Jumisko SH, 2005, PROC SPIE, V5684, P243, DOI 10.1117/12.596503
   Kamer W, 2007, ETRI J, V29, P569, DOI 10.4218/etrij.07.0107.0102
   Kanumuri S, 2006, IEEE IMAGE PROC, P2245, DOI 10.1109/ICIP.2006.312809
   Khan A., 2009, P IEEE ICC, P1
   Khan A, 2010, IEEE ICC
   Khan A, 2010, INT J DIGIT MULTIMED, V2010, DOI 10.1155/2010/608138
   Lie A, 2008, MULTIMEDIA SYST, V14, P33, DOI 10.1007/s00530-007-0110-0
   Mohamed S, 2002, IEEE T CIRC SYST VID, V12, P1071, DOI 10.1109/TCSVT.2002.806808
   Moorthy AK, 2010, IEEE T CIRC SYST VID, V20, P587, DOI 10.1109/TCSVT.2010.2041829
   Muntean GM, 2004, IEEE T BROADCAST, V50, P1, DOI 10.1109/TBC.2004.824745
   Reibman AR, 2004, IEEE T MULTIMEDIA, V6, P327, DOI 10.1109/TMM.2003.822785
   Rezaei M, 2008, IEEE T CIRC SYST VID, V18, P633, DOI 10.1109/TCSVT.2008.919108
   Ries Michal, 2008, Journal of Communications, V3, P41, DOI 10.4304/jcm.3.1.41-50
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P1427, DOI 10.1109/TIP.2010.2042111
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P335, DOI 10.1109/TIP.2009.2034992
   Seungho Jeong, 2010, 2010 International Conference on Information and Communication Technology Convergence (ICTC), P99, DOI 10.1109/ICTC.2010.5674715
   Snedecor GW, 1983, Statistical Methods, V6th
   Tao S, 2008, IEEE ACM T NETWORK, V16, P1052, DOI 10.1109/TNET.2007.910617
   Winkler S, 2008, IEEE T BROADCAST, V54, P660, DOI 10.1109/TBC.2008.2000733
   Yamawaki K, 2009, PROG THEOR PHYS SUPP, P1, DOI 10.1143/PTPS.180.1
   Yang KC, 2007, IEEE T MULTIMEDIA, V9, P1528, DOI 10.1109/TMM.2007.906576
   Zhai GT, 2008, IEEE T BROADCAST, V54, P719, DOI 10.1109/TBC.2008.2001720
   Zhai GT, 2008, IEEE T MULTIMEDIA, V10, P1316, DOI 10.1109/TMM.2008.2004910
NR 48
TC 134
Z9 152
U1 0
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2012
VL 14
IS 2
BP 431
EP 442
DI 10.1109/TMM.2011.2176324
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 924OZ
UT WOS:000302702500016
DA 2024-07-18
ER

PT J
AU Cong, Y
   Yuan, JS
   Luo, JB
AF Cong, Yang
   Yuan, Junsong
   Luo, Jiebo
TI Towards Scalable Summarization of Consumer Videos Via Sparse Dictionary
   Selection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Group sparse; key frame; Lasso; scene analysis; video analysis; video
   skim; video summarization
ID EXTRACTION; REPRESENTATION; FRAMEWORK
AB The rapid growth of consumer videos requires an effective and efficient content summarization method to provide a user-friendly way to manage and browse the huge amount of video data. Compared with most previous methods that focus on sports and news videos, the summarization of personal videos is more challenging because of its unconstrained content and the lack of any pre-imposed video structures. We formulate video summarization as a novel dictionary selection problem using sparsity consistency, where a dictionary of key frames is selected such that the original video can be best reconstructed from this representative dictionary. An efficient global optimization algorithm is introduced to solve the dictionary selection model with the convergence rates as O(1/root K-2) (where K is the iteration counter), in contrast to traditional sub-gradient descent methods of O(1/root K). Our method provides a scalable solution for both key frame extraction and video skim generation, because one can select an arbitrary number of key frames to represent the original videos. Experiments on a human labeled benchmark dataset and comparisons to the state-of-the-art methods demonstrate the advantages of our algorithm.
C1 [Cong, Yang; Yuan, Junsong] Nanyang Technol Univ, Dept Elect & Elect Engn, Singapore 639798, Singapore.
   [Cong, Yang] Chinese Acad Sci, Shenyang Inst Automat, State Key Lab Robot, Beijing 100864, Peoples R China.
   [Luo, Jiebo] Univ Rochester, Dept Comp Sci, Rochester, NY 14627 USA.
C3 Nanyang Technological University; Chinese Academy of Sciences; Shenyang
   Institute of Automation, CAS; University of Rochester
RP Cong, Y (corresponding author), Nanyang Technol Univ, Dept Elect & Elect Engn, Singapore 639798, Singapore.
EM congyang81@gmail.com; jsyuan@ntu.edu.sg; jiebo.luo@gmail.com
RI Yuan, Junsong/R-4352-2019; Luo, Jiebo/AAI-7549-2020; Yuan,
   Junsong/A-5171-2011
OI Luo, Jiebo/0000-0002-4516-9729; Yuan, Junsong/0000-0002-7901-8793
FU Nanyang Assistant Professorship [SUG M58040015]; NSFC [61105013]
FX This work was done when C. Yang was a research fellow at Nanyang
   Technological University and was supported in part by the Nanyang
   Assistant Professorship (SUG M58040015) to Dr. J. Yuan and NSFC
   (61105013). The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Changsheng Xu.
CR Aner-Wolf A, 2004, COMPUT VIS IMAGE UND, V95, P201, DOI 10.1016/j.cviu.2004.03.005
   [Anonymous], 2007, GRADIENT METHODS MIN
   [Anonymous], 1995, STORAGE RETRIEVAL IM, DOI DOI 10.1117/12.205308
   Cheng CC, 2006, IEEE T MULTIMEDIA, V8, P585, DOI 10.1109/TMM.2006.870726
   Cong Y, 2011, PROC CVPR IEEE, P1807, DOI 10.1109/CVPR.2011.5995434
   Fu YW, 2010, IEEE T MULTIMEDIA, V12, P717, DOI 10.1109/TMM.2010.2052025
   Gresle P, 1997, P 2 INT C VIS INF SY, P279
   Huang K., 2007, P 19 INT C NEUR INF, P609, DOI DOI 10.7551/MITPRESS/7503.003.0081
   Ji S., 2009, An accelerated gradient method for trace norm minimization, P457
   Junee R., 2009, YOUTUBE BLOG, V20
   KENDER J, 2000, P ACCV
   Li B., 2003, P ICIP, V1
   Li Z, 2005, IEEE T CIRC SYST VID, V15, P1245, DOI 10.1109/TCSVT.2005.854230
   Li Z, 2005, IEEE T IMAGE PROCESS, V14, P1550, DOI 10.1109/TIP.2005.854477
   Li Z, 2009, IEEE T IMAGE PROCESS, V18, P2572, DOI 10.1109/TIP.2009.2026677
   Liu J, 2009, IEEE I CONF COMP VIS, P2114
   Liu TM, 2003, IEEE T CIRC SYST VID, V13, P1006, DOI 10.1109/TCSVT.2003.816521
   Loui Alexander., 2007, MIR 07, P245
   Luo JB, 2009, IEEE T CIRC SYST VID, V19, P289, DOI 10.1109/TCSVT.2008.2009241
   Ma Y., 2002, P 10 ACM INT C MULT, P542
   Money AG, 2008, J VIS COMMUN IMAGE R, V19, P121, DOI 10.1016/j.jvcir.2007.04.002
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Peng YX, 2006, IEEE T CIRC SYST VID, V16, P612, DOI 10.1109/TCSVT.2006.873157
   Rasheed Z, 2005, IEEE T MULTIMEDIA, V7, P1097, DOI 10.1109/TMM.2005.858392
   Rui Y., 2000, P 8 ACM INT C MULT, P115
   Truong BT, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1198302.1198305
   Wu J., 2009, P IROS
   Wu JX, 2011, IEEE T PATTERN ANAL, V33, P1489, DOI 10.1109/TPAMI.2010.224
   Xu CS, 2005, IEEE T SPEECH AUDI P, V13, P441, DOI 10.1109/TSA.2004.840939
   You JY, 2007, IEEE T CIRC SYST VID, V17, P273, DOI 10.1109/TCSVT.2007.890857
   Yu B., 2003, Proceedings of the eleventh ACM international conference on Multimedia, P382
   Yuan M., 2006, Journal of the Royal Statistical Society, Series B, V70, P53
   Zhang T, 2004, PROC SPIE, V5601, P25, DOI 10.1117/12.572474
   Zhuang YT, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 1, P866, DOI 10.1109/ICIP.1998.723655
   Ziyou Xiong, 2004, 2004 IEEE International Conference on Multimedia and Expo (ICME) (IEEE Cat. No.04TH8763), P1947, DOI 10.1109/ICME.2004.1394642
   2008, GUIDELINES TRECVID 2
NR 36
TC 215
Z9 234
U1 0
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2012
VL 14
IS 1
BP 66
EP 75
DI 10.1109/TMM.2011.2166951
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 924OL
UT WOS:000302701100007
DA 2024-07-18
ER

PT J
AU Tian, XM
   Yang, LJ
   Wang, JD
   Wu, XQ
   Hua, XS
AF Tian, Xinmie
   Yang, Linjun
   Wang, Jingdong
   Wu, Xiuqing
   Hua, Xian-Sheng
TI Bayesian Visual Reranking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image search; ranking distance; video search; visual consistency; visual
   reranking
ID IMAGE RETRIEVAL
AB Visual reranking has been proven effective to refine text-based video and image search results. It utilizes visual information to recover "true" ranking list from the noisy one generated by text-based search, by incorporating both textual and visual information. In this paper, we model the textual and visual information from the probabilistic perspective and formulate visual reranking as an optimization problem in the Bayesian framework, termed Bayesian visual reranking. In this method, the textual information is modeled as a likelihood, to reflect the disagreement between reranked results and text-based search results which is called ranking distance. The visual information is modeled as a conditional prior, to indicate the ranking score consistency among visually similar samples which is called visual consistency. Bayesian visual reranking derives the best reranking results by maximizing visual consistency while minimizing ranking distance. To model the ranking distance more precisely, we propose a novel pair-wise method which measure the ranking distance based on the disagreement in terms of pair-wise orders. For visual consistency, we study three different regularizers to mine the best way for its modeling. We conduct extensive experiments on both video and image search datasets. Experimental results demonstrate the effectiveness of our proposed Bayesian visual reranking.
C1 [Tian, Xinmie; Wu, Xiuqing] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230027, Peoples R China.
   [Yang, Linjun; Wang, Jingdong; Hua, Xian-Sheng] Microsoft Res Asia, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Microsoft; Microsoft Research Asia
RP Tian, XM (corresponding author), Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230027, Peoples R China.
EM xinmei@mail.ustc.edu.cn; linjuny@microsoft.com; jingdw@microsoft.com;
   wuxq@ustc.edu.cn; xshua@microsoft.com
RI Wang, Jingdong/E-9920-2017
OI Wang, Jingdong/0000-0002-4888-4445
CR [Anonymous], 2006, P 8 ACM INT WORKSHOP
   [Anonymous], P INT C ART INT STAT
   [Anonymous], RANK CORRELATION MET
   Cao Z., 2007, P 24 INT C MACHINE L, P129, DOI DOI 10.1145/1273496.1273513
   Carbonell JG, 1997, INT JOINT CONF ARTIF, P708
   Chang HS, 1999, IEEE T CIRC SYST VID, V9, P1269, DOI 10.1109/76.809161
   Cox IJ, 2000, IEEE T IMAGE PROCESS, V9, P20, DOI 10.1109/83.817596
   Cristianini N., 2000, INTRO SUPPORT VECTOR
   Fergus R, 2004, LECT NOTES COMPUT SC, V3021, P242
   Herbrich R, 2000, ADV NEUR IN, P115
   Hsu W. H., 2006, MULTIMEDIA '06, P35
   Hsu WinstonH., 2007, ACM MM
   Jarvelin Kalervo, 2000, P 23 ANN INT ACM SIG, P41, DOI DOI 10.1145/345508.345545
   Jing Y, 2008, IEEE T PATTERN ANAL, V30, P1877, DOI 10.1109/TPAMI.2008.121
   Kennedy L.S., 2007, Proceedings of the 6th ACM international conference on Image and video retrieval, P333
   Liu Jingjing., 2007, MULTIMEDIA 07, P208
   Liu Y, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P297, DOI 10.1109/ICME.2008.4607430
   Ma WY, 1998, CONF REC ASILOMAR C, P253, DOI 10.1109/ACSSC.1998.750865
   MEI T, 2007, TREV VID RETR EV ONL
   Meng J., 2010, P INT C ACM MULT, P1147
   Natsev Apostol., 2007, MULTIMEDIA 07, P991
   Robertson S.-E., 1997, TR356 CAMBR U COMP L
   SLONIM N, 1999, P ADV NEUR INF PROC, P250
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Song K., 2006, ACM Multimedia, P707
   Tian X., 2008, ACM INT C MULTIMEDIA, P131, DOI DOI 10.1145/1459359.1459378.ISBN
   *TRECVID, TREV VID RETR EV
   Van Leuken Reinier H, 2009, P 18 INT C WORLD WID, P341, DOI 10.1145/1526709.1526756
   van Zwol Roelof., 2008, P 1 ACM INT C MULTIM, P67
   Yan R, 2003, LECT NOTES COMPUT SC, V2728, P238
   YAN R, 2006, P ACM INT C CONT BAS, P60
   YANG L, 2010, P ACM INT C MULT, P183
   Yang Y.H., 2008, ACM International Conference on Multimedia (MM), P199
   Zhang SG, 2010, PROCEEDINGS OF THE ASME JOINT RAIL CONFERENCE, VOL 2, P501, DOI 10.1145/1873951.1874018
   Zhou DY, 2004, ADV NEUR IN, V16, P321
   Zhu X., 2003, P 20 INT C MACH LEAR, V3, P58, DOI DOI 10.1109/18.850663
NR 36
TC 26
Z9 30
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2011
VL 13
IS 4
BP 639
EP 652
DI 10.1109/TMM.2011.2111363
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 805EX
UT WOS:000293710300005
DA 2024-07-18
ER

PT J
AU Yang, KY
   Hua, XS
   Wang, M
   Zhang, HJ
AF Yang, Kuiyuan
   Hua, Xian-Sheng
   Wang, Meng
   Zhang, Hong-Jiang
TI Tag Tagging: Towards More Descriptive Keywords of Image Content
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image retrieval; property tags; tag tagging
ID SEARCH; WEB
AB Tags have been demonstrated to be effective and efficient for organizing and searching social image content. However, these human-provided keywords are far from a comprehensive description of the image content, which limits their effectiveness in tag-based image search. In this paper, we propose an automatic scheme called tag tagging to supplement semantic image descriptions by associating a group of property tags with each existing tag. For example, an initial tag "tiger" may be further tagged with "white", "stripes", and "bottom-right" along three tag properties: color, texture, and location, respectively. In this way, the descriptive ability of the existing tags can be greatly enhanced. In the proposed scheme, a lazy learning approach is first applied to estimate the corresponding image regions of each initial tag, and then a set of property tags that correspond to six properties, including location, color, texture, size, shape, and dominance, are derived for each initial tag. These tag properties enable much more precise image search especially when certain tag properties are included in the query. The results of the empirical evaluation show that tag properties remarkably boost the performance of social image retrieval.
C1 [Yang, Kuiyuan] Univ Sci & Technol China, Dept Automat, Hefei 230027, Peoples R China.
   [Hua, Xian-Sheng] Microsoft Res Asia, Media Comp Grp, Beijing 100080, Peoples R China.
   [Wang, Meng] AKiiRA Media Syst Inc, Palo Alto, CA 94301 USA.
   [Zhang, Hong-Jiang] Microsoft Adv Technol Ctr, Beijing 100080, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Microsoft Research Asia; Microsoft; Microsoft
RP Yang, KY (corresponding author), Univ Sci & Technol China, Dept Automat, Hefei 230027, Peoples R China.
EM yky@ustc.edu; xshua@microsoft.com; eric.mengwang@gmail.com;
   hjzhang@microsoft.com
CR AMES M, 2007, P SIGCHI
   ANDONI A, 2006, P FDN COMP SCI
   [Anonymous], P CVPR
   [Anonymous], 2009, P CVPR
   [Anonymous], P ICCV
   [Anonymous], P ACM INT C MULT
   [Anonymous], 2009, P CVPR
   CHRISTEL M, 2005, P CIVR
   CHUA TS, 2009, P CIVR SANT GREEC
   FELZENSZWALB P, 2004, P IJCV
   FERGUS R, 2003, P ICCV
   FERRARI V, 2008, P NIPS
   Goodrum A, 2001, INFORM PROCESS MANAG, V37, P295, DOI 10.1016/S0306-4573(00)00033-9
   Gupta A, 2008, LECT NOTES COMPUT SC, V5302, P16, DOI 10.1007/978-3-540-88682-2_3
   Hansen T, 2006, NAT NEUROSCI, V9, P1367, DOI 10.1038/nn1794
   HUA XS, 2008, P ACM INT C MULT
   IVANOV I, 2010, P INT C MULT INF RET
   Jansen BJ, 2006, INFORM PROCESS MANAG, V42, P248, DOI 10.1016/j.ipm.2004.10.007
   Li X, 2009, P IEEE INT C AC SPEE
   LIU X, 2009, P ACM INT C MULT
   Maron O, 1998, ADV NEUR IN, V10, P570
   Ojala T., 2000, P ECCV
   Peters I, 2009, KNOWL INFO-STUD INFO, P1, DOI 10.1515/9783598441851
   ROBERTSON S, 1996, P 4 TEXT RETR C
   Rorissa A, 2008, INFORM PROCESS MANAG, V44, P1741, DOI 10.1016/j.ipm.2008.03.004
   ROSCH E, 1976, COGNITIVE PSYCHOL, V8, P382, DOI 10.1016/0010-0285(76)90013-X
   Sigurbjornsson Borkur, 2008, P WWW
   Smith JR, 1996, INT CONF ACOUST SPEE, P2239, DOI 10.1109/ICASSP.1996.545867
   SMITH JR, 1996, P ACM INT C MULT
   SOROKIN A, 2008, P CVPR WORKSH INT VI
   Thompson Evan., 1995, Colour Vision : A Study in Cognitive Science and the Philosophy of Perception
   VANDEWEIJER J, 2007, P CVPR
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   von Ahn Luis., 2004, CHI, DOI DOI 10.1145/985692.985733
   WANG J, 2009, COLOR STRUCTURED IMA
   Wang M, 2010, IEEE T MULTIMEDIA, V12, P829, DOI 10.1109/TMM.2010.2055045
   Wang M, 2009, IEEE T MULTIMEDIA, V11, P465, DOI 10.1109/TMM.2009.2012919
   Wang M, 2009, IEEE T CIRC SYST VID, V19, P733, DOI 10.1109/TCSVT.2009.2017400
   Weber M., 2000, P ECCV
   WEINBERGER K, 2008, P ACM INT C MULT
   WHITE D, 1996, P SPIE STOR RETR IM
   YANG K, 2010, P ACM INT C MULT
   Yang KY, 2010, LECT NOTES COMPUT SC, V5916, P174, DOI 10.1007/978-3-642-11301-7_20
   ZHANG Q, 2002, P NIPS
NR 44
TC 30
Z9 34
U1 0
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2011
VL 13
IS 4
BP 662
EP 673
DI 10.1109/TMM.2011.2147777
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA 805EX
UT WOS:000293710300007
DA 2024-07-18
ER

PT J
AU Fu, ZY
   Lu, GJ
   Ting, KM
   Zhang, DS
AF Fu, Zhouyu
   Lu, Guojun
   Ting, Kai Ming
   Zhang, Dengsheng
TI A Survey of Audio-Based Music Classification and Annotation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Acoustic signal processing; classification algorithms; feature
   extraction; music information retrieval
ID INFORMATION-RETRIEVAL; GENRE CLASSIFICATION; INSTRUMENT RECOGNITION;
   ACOUSTIC FEATURES; SOCIAL TAGS; SIMILARITY; MATRIX; QUERY
AB Music information retrieval (MIR) is an emerging research area that receives growing attention from both the research community and music industry. It addresses the problem of querying and retrieving certain types of music from large music data set. Classification is a fundamental problem in MIR. Many tasks in MIR can be naturally cast in a classification setting, such as genre classification, mood classification, artist recognition, instrument recognition, etc. Music annotation, a new research area in MIR that has attracted much attention in recent years, is also a classification problem in the general sense. Due to the importance of music classification in MIR research, rapid development of new methods, and lack of review papers on recent progress of the field, we provide a comprehensive review on audio-based classification in this paper and systematically summarize the state-of-the-art techniques for music classification. Specifically, we have stressed the difference in the features and the types of classifiers used for different classification tasks. This survey emphasizes on recent development of the techniques and discusses several open issues for future research.
C1 [Fu, Zhouyu; Lu, Guojun; Ting, Kai Ming; Zhang, Dengsheng] Monash Univ, Fac Informat Technol, Gippsland Sch Informat Technol, Churchill, Vic 3842, Australia.
C3 Federation University Australia; Monash University
RP Fu, ZY (corresponding author), Monash Univ, Fac Informat Technol, Gippsland Sch Informat Technol, Gippsland Campus, Churchill, Vic 3842, Australia.
EM zhouyu.fu@monash.edu; guojun.lu@monash.edu; kaiming.ting@monash.edu;
   dengsheng.zhang@monash.edu
RI Zhang, Dengsheng/W-8467-2019
OI Zhang, Dengsheng/0000-0001-8728-1746; Ting, Kai
   Ming/0000-0001-7892-6194; Lu, Guojun/0000-0003-2523-7576
CR Agostini G, 2003, EURASIP J APPL SIG P, V2003, P5, DOI 10.1155/S1110865703210118
   ALLAMANCHE E, 2001, P INT C MUS INF RETR
   [Anonymous], P INT C MUS INF RETR
   [Anonymous], P INT C MUS INF RETR
   [Anonymous], P INT C MUS INF RETR
   [Anonymous], P INT C MUS INF RETR
   [Anonymous], 2004, Journal of negative results in speech and audio sciences
   [Anonymous], 2008, P INT C DIG AUD EFF
   [Anonymous], P INT C MUS INF RETR
   Aucouturier J.J., 2002, 3 INT SOC MUS INF RE, P13
   BELLO JP, 2007, P INT C MUS INF RETR
   BENETOS E, 2006, P INT C AC SPEECH SI
   Berenzweig  A., 2003, P INT C MUS INF RETR
   BERENZWEIG A, 2001, P IEEE WORKSH APPL S
   Berenzweig A., 2002, P INT C VIRT SYNTH E
   Bergstra J, 2006, MACH LEARN, V65, P473, DOI 10.1007/s10994-006-9019-7
   Bertin-Mahieux T, 2008, J NEW MUSIC RES, V37, P115, DOI 10.1080/09298210802479250
   Boser B. E., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P144, DOI 10.1145/130385.130401
   Boutell MR, 2004, PATTERN RECOGN, V37, P1757, DOI 10.1016/j.patcog.2004.03.009
   BROWN JC, 1991, J ACOUST SOC AM, V89, P425, DOI 10.1121/1.400476
   Brown JC, 1999, J ACOUST SOC AM, V105, P1933, DOI 10.1121/1.426728
   Casey MA, 2008, P IEEE, V96, P668, DOI 10.1109/JPROC.2008.916370
   CHECHIK G, 2008, P ACM MULT INT RETR
   Chen ZS, 2008, P AUD ENG SOC
   CHENG HT, 2008, P INT C MULT EXP
   Cheung W-L, 2008, P INT WORKSH MULT SI
   CHO Y, 2009, P INT C MACH LEARN
   CHUA BY, 2007, THESIS MONASH U CHUR
   COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964
   Duda R., 1973, Pattern Classification and Scene Analysis
   ELISSEEFF A, 2000, P NEUR INF PROC SYST
   Ellis D. P.W., 2007, P INT C AC SPEECH SI
   Essid S, 2006, IEEE T AUDIO SPEECH, V14, P68, DOI 10.1109/TSA.2005.860351
   Essid S, 2006, IEEE T AUDIO SPEECH, V14, P1401, DOI 10.1109/TSA.2005.860842
   Feng Y, 2003, P INT C WEB INT
   FOOTE J, 2002, P INT C MUS INF RETR
   Foote JT, 1997, P SOC PHOTO-OPT INS, V3229, P138, DOI 10.1117/12.290336
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   FU Z, 2010, P INT WORKSH STAT PA
   Fuhrmann F, 2009, P INT C MUS INF RETR
   Fujishima T., 1999, P INT COMP MUS C, P464
   Gjerdingen R, 2008, J NEW MUSIC RES, V37, P93, DOI 10.1080/09298210802479268
   GOMEZ E, 2004, P INF C MUS INF RETR
   GOMEZ E, 2006, P INT C MUS INF RETR
   Grangier D, 2008, IEEE T PATTERN ANAL, V30, P1371, DOI 10.1109/TPAMI.2007.70791
   GROSSE R, 2007, P UNC ART INT
   Gutierrez E.G., 2006, Tonal description of music audio signals
   Hamel P, 2009, P INT C MUS INF RETR
   Heittola T, 2009, P INT C MUS INF RETR
   Hevner K, 1936, AM J PSYCHOL, V48, P246, DOI 10.2307/1415746
   Hoffman M, 2009, P INT C MUS INF RETR
   HOFFMAN M, 2008, P INT C MUS INF RETR
   Holzapfel A, 2008, IEEE T AUDIO SPEECH, V16, P424, DOI 10.1109/TASL.2007.909434
   HOMBURG H, 2005, P INT C MUS INF RETR
   Hu X., 2008, P INT C MUS INF RETR
   Huang TS, 2008, P IEEE, V96, P648, DOI 10.1109/JPROC.2008.916364
   Jang JSR, 2008, IEEE T AUDIO SPEECH, V16, P350, DOI 10.1109/TASL.2007.913035
   Jensen JH, 2009, IEEE T AUDIO SPEECH, V17, P693, DOI 10.1109/TASL.2008.2012314
   JIANG DN, 2002, P INT C MULT EXP
   Kim HG, 2004, IEEE T CIRC SYST VID, V14, P716, DOI 10.1109/TCSVT.2004.826766
   Kim JH, 2009, P INT C MUS INF RETR
   Kim Y. E., 2002, P INT C MUS INF RETR
   Kitahara T, 2007, EURASIP J ADV SIG PR, DOI 10.1155/2007/51979
   Klapuri A, 2006, SIGNAL PROCESSING ME
   KLAPURI A, 2000, IEEE T SPEECH AUDIO
   KNEES P, 2009, P INT C MUS INF RETR
   Korhonen MD, 2006, IEEE T SYST MAN CY B, V36, P588, DOI 10.1109/TSMCB.2005.862491
   Lanckriet GRG, 2004, J MACH LEARN RES, V5, P27
   Laurier C., 2009, P INT C MUS INF RETR
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee CH, 2009, IEEE T MULTIMEDIA, V11, P670, DOI 10.1109/TMM.2009.2017635
   Lee H., 2009, P ADV NEUR INF PROC, P1
   Lee K., 2006, P INT COMP MUS C
   Levy M., 2007, P INT C MUS INF RETR
   Levy M, 2009, IEEE T MULTIMEDIA, V11, P383, DOI 10.1109/TMM.2009.2012913
   Li T., 2003, P INT C MUS INF RETR
   Li T, 2003, P SIGIR
   Li T, 2006, IEEE T MULTIMEDIA, V8, P564, DOI 10.1109/TMM.2006.870730
   LIDY T, 2005, P INT C MUS INF RETR
   Lin CC, 2005, IEEE T SPEECH AUDI P, V13, P644, DOI 10.1109/TSA.2005.851880
   LIPPENS S, 2004, P INT C AC SPEECH SI
   Little D, 2008, P INT C MUS INF RETR
   LOGAN B, 2001, P INT C MULT EXP
   Lu L, 2006, IEEE T AUDIO SPEECH, V14, P5, DOI 10.1109/TSA.2005.860344
   [Macy L. Oxford University Press Oxford University Press], GROVE MUSIC ONLINE
   Mandel M., 2005, P INT C MUS INF RETR
   Mandel MI, 2006, MULTIMEDIA SYST, V12, P3, DOI 10.1007/s00530-006-0032-2
   Marques Janet., 1999, A Study of Musical Instrument Classification Using Gaussian Mixture Models and Support Vector Machines
   Meng A, 2005, P INT C MUS INF RETR
   Meng A, 2007, IEEE T AUDIO SPEECH, V15, P1654, DOI 10.1109/TASL.2007.899293
   Mierswa I, 2005, MACH LEARN, V58, P127, DOI 10.1007/s10994-005-5824-7
   Mion L, 2008, IEEE T AUDIO SPEECH, V16, P458, DOI 10.1109/TASL.2007.913743
   Mochen F, 2006, P ACM SIGKDD
   MOH Y, 2009, P INT C AC SPEECH SI
   Mörchen F, 2006, IEEE T AUDIO SPEECH, V14, P81, DOI 10.1109/TSA.2005.860352
   MOROLT M, 2006, P INT C MUS INF RETR
   NESS SR, 2009, P ACM MULT
   Nwe TL, 2007, IEEE T AUDIO SPEECH, V15, P519, DOI 10.1109/TASL.2006.876756
   Pachet F, 2009, IEEE T AUDIO SPEECH, V17, P335, DOI 10.1109/TASL.2008.2008734
   PAMPALK E, 2003, P INT C MUS INF RETR
   Pampalk E., 2002, P ACM MULT
   Pampalk E., 2005, P INT C INT SOC MUS
   Panagakis I, 2009, P INT C MUS INF RETR
   Panagakis I., 2008, P INT C MUS INF RETR
   Poliner GE, 2007, IEEE T AUDIO SPEECH, V15, P1247, DOI 10.1109/TASL.2006.889797
   RAVURI S, 2010, P INT C AC SPEECH SI
   REED J, 2009, P INT C AC SPEECH SI
   REED J, 2006, P INT C MUS INF RETR
   Scaringella N, 2006, IEEE SIGNAL PROC MAG, V23, P133, DOI 10.1109/MSP.2006.1598089
   Scaringella N, 2005, P INT C MUS INF RETR
   Serrà J, 2008, IEEE T AUDIO SPEECH, V16, P1138, DOI 10.1109/TASL.2008.924595
   Shalev-Shwartz S., 2007, PhD thesis
   Shen JL, 2006, IEEE T MULTIMEDIA, V8, P1179, DOI 10.1109/TMM.2006.884618
   Shen JL, 2009, ACM T INFORM SYST, V27, DOI 10.1145/1508850.1508856
   Slaney M, 2007, P INT C AC SPEECH SI
   SLANEY M, 2008, P INT C MUS INF RETR
   Song YQ, 2008, IEEE T MULTIMEDIA, V10, P145, DOI 10.1109/TMM.2007.911305
   Tellegen A, 1999, PSYCHOL SCI, V10, P297, DOI 10.1111/1467-9280.00157
   Thayer Robert E., 1990, The Biopsychology of Mood and Arousal
   Thomas Lidy, 2007, P INT C MUS INF RETR
   Tolonen T, 2000, IEEE T SPEECH AUDI P, V8, P708, DOI 10.1109/89.876309
   Tomasik B, 2009, P INT C MUS INF RETR
   Tsai WH, 2006, IEEE T AUDIO SPEECH, V14, P330, DOI 10.1109/TSA.2005.854091
   TSAI WH, 2005, P INT C MUS INF RETR
   TSUNOO E, 2009, P INT C MUS INF RETR
   Turnbull D, 2005, IEEE T KNOWL DATA EN, V17, P580, DOI 10.1109/TKDE.2005.62
   Turnbull D, 2009, P ACM INF RETR
   Turnbull D, 2007, P ACM INF RETR
   Turnbull D, 2008, IEEE T AUDIO SPEECH, V16, P467, DOI 10.1109/TASL.2007.913750
   Tzanetakis G, 2002, IEEE T SPEECH AUDI P, V10, P293, DOI 10.1109/TSA.2002.800560
   TZANETAKIS G, 2002, P INT C MUS INF RETR
   TZANETAKIS G, P INT MOB INF SYST
   TZANETAKIS G, 2008, P INT C MUS INF RETR
   Tzanetakis G, 2010, J AUDIO ENG SOC, V58, P409
   Unal E, 2008, IEEE T AUDIO SPEECH, V16, P359, DOI 10.1109/TASL.2007.912373
   Wang F, 2009, P INT C MUS INF RETR
   WANG KS, 1995, IEEE T SPEECH AUDI P, V3, P382, DOI 10.1109/89.466657
   Weihs C, 2007, ADV DATA ANAL CLASSI, V1, P255, DOI 10.1007/s11634-007-0016-x
   West K., 2005, P INT C MUS INF RETR
   West K, 2008, THESIS U E ANGLIA NO
   WHITMAN B, 2001, P IEEE WORKSH NEUR N
   WIGGINS G, 2007, P INT C MUS INF RETR
   WOLPERT DH, 1992, NEURAL NETWORKS, V5, P241, DOI 10.1016/S0893-6080(05)80023-1
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Yang D., 2003, P INT C MUS INF RETR
   Yang Y-H, 2006, P ACM MULT
   Yang YH, 2008, IEEE T AUDIO SPEECH, V16, P448, DOI 10.1109/TASL.2007.911513
   ZHU J, SEMISUPERVISED LEARN
NR 148
TC 222
Z9 240
U1 5
U2 67
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2011
VL 13
IS 2
BP 303
EP 319
DI 10.1109/TMM.2010.2098858
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 738SG
UT WOS:000288661800012
DA 2024-07-18
ER

PT J
AU Zhang, W
   Wu, QMJ
   Wang, GH
   Yin, HB
AF Zhang, Wei
   Wu, Q. M. Jonathan
   Wang, Guanghui
   Yin, Haibing
TI An Adaptive Computational Model for Salient Object Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bayesian framework; bottom-up; observation behavior; salient object
   detection; top-down
ID VISUAL-ATTENTION REGIONS; COLOR; EXTRACTION; IMAGES
AB Salient object detection is a basic technique for many computer vision applications. In this paper, we propose an adaptive computational model to detect the salient object in color images. Firstly, three human observation behaviors and scalable subtractive clustering techniques are used to construct attention Gaussian mixture model (AGMM) and background Gaussian mixture model (BGMM). Secondly, the Bayesian framework is employed to classify each pixel into salient object or background object. Thirdly, expectation-maximization (EM) algorithm is utilized to update the parameters of AGMM, BGMM, and Bayesian framework based on the detection results. Finally, the classification and update procedures are repeated until the detection results evolve to a steady state. Experiments on a variety of images demonstrate the robustness of the proposed method. Extensive quantitative evaluations and comparisons demonstrate that the proposed method significantly outperforms state-of-the-art methods.
C1 [Zhang, Wei; Wu, Q. M. Jonathan; Wang, Guanghui] Univ Windsor, CVSSL, Dept Elect & Comp Engn, Windsor, ON N9B 3P4, Canada.
   [Yin, Haibing] China Jiliang Univ, Dept Informat Engn, Hangzhou, Zhejiang, Peoples R China.
   [Yin, Haibing] Peking Univ, Inst Digital Media, Beijing 100871, Peoples R China.
C3 University of Windsor; China Jiliang University; Peking University
RP Zhang, W (corresponding author), Univ Windsor, CVSSL, Dept Elect & Comp Engn, Windsor, ON N9B 3P4, Canada.
EM weizhang@uwindsor.ca; jwu@uwindsor.ca; ghwang@uwindsor.ca;
   hbyin@jdl.ac.cn
RI Wang, Guanghui/AAV-4605-2021; Wu, Q.M.Jonathan/O-3234-2017
OI Wang, Guanghui/0000-0001-6213-0693; 
FU Natural Sciences and Engineering Research Council of Canada; NSFC
   [60802025]
FX The work was supported in part by the Natural Sciences and Engineering
   Research Council of Canada and in part by NSFC 60802025. This paper was
   published in part at the International Conference on Pattern
   Recognition, Tampa, FL, in 2008. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr. Ajay
   Divakaran.
CR [Anonymous], NEURAL BASIS VISUAL
   [Anonymous], 2003, P 11 ACM INT C MULTI, DOI DOI 10.1145/957013.957094
   [Anonymous], P 14 ANN ACM INT C M
   [Anonymous], 2006, P IEEE CS C COMP VIS
   [Anonymous], P IEEE CVPR
   Aziz MZ, 2008, IEEE T IMAGE PROCESS, V17, P633, DOI 10.1109/TIP.2008.919365
   AZIZ MZ, 2006, P WORKSH FARBB ILM G, P74
   BORBA GB, 2009, P SPIE IS T SPIE EL, V7255
   Bradley AP, 2003, J VIS COMMUN IMAGE R, V14, P232, DOI 10.1016/S1047-3203(03)00037-3
   Chen LQ, 2003, MULTIMEDIA SYST, V9, P353, DOI 10.1007/s00530-003-0105-4
   Chiu S., 1994, J INTELL FUZZY SYST, V2
   Dave RN, 1997, IEEE T FUZZY SYST, V5, P270, DOI 10.1109/91.580801
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x
   DENGEL A, 2004, LNCS, V2956
   Fu H, 2006, PATTERN RECOGN, V39, P1604, DOI 10.1016/j.patcog.2005.12.015
   Gasparini F, 2007, OPT ENG, V46, DOI 10.1117/1.2721764
   Han JW, 2006, IEEE T CIRC SYST VID, V16, P141, DOI 10.1109/TCSVT.2005.859028
   Hu YQ, 2008, J VIS COMMUN IMAGE R, V19, P199, DOI 10.1016/j.jvcir.2007.11.001
   Itti L, 2004, IEEE T IMAGE PROCESS, V13, P1304, DOI 10.1109/TIP.2004.834657
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Itti L, 2001, OPT ENG, V40, P1784, DOI 10.1117/1.1389063
   Itti L, 2001, J ELECTRON IMAGING, V10, P161, DOI 10.1117/1.1333677
   Jost T, 2005, COMPUT VIS IMAGE UND, V100, P107, DOI 10.1016/j.cviu.2004.10.009
   Le Meur O, 2007, VISION RES, V47, P2483, DOI 10.1016/j.visres.2007.06.015
   Levin A, 2006, LECT NOTES COMPUT SC, V3954, P581
   LI J, 2007, P INT C ENT COMP, P345
   Liu T., 2007, Proc. International Conference on Advanced Intelligent Mechatronics, P1
   López MT, 2007, IMAGE VISION COMPUT, V25, P597, DOI 10.1016/j.imavis.2006.05.004
   López MT, 2006, PATTERN RECOGN, V39, P2194, DOI 10.1016/j.patcog.2006.04.018
   Lu ZK, 2005, IEEE T IMAGE PROCESS, V14, P1928, DOI 10.1109/TIP.2005.854478
   Marques O, 2007, EURASIP J ADV SIG PR, DOI 10.1155/2007/43450
   MATLIN MW, 1991, SENSATION PERCEPTION
   Meur O L, 2006, IEEE T PATTERN ANAL, V28, P802
   Ninassi A., 2007, Proceedings 2007 IEEE International Conference on Image Processing, ICIP 2007, P732
   Oliva A, 2003, IEEE IMAGE PROC, P253, DOI 10.1109/icip.2003.1246946
   Paschos G, 2001, IEEE T IMAGE PROCESS, V10, P932, DOI 10.1109/83.923289
   Peters R.J., 2007, P IEEE C COMPUTER VI, P1
   Rapantzikos K, 2007, IET IMAGE PROCESS, V1, P237, DOI 10.1049/iet-ipr:20060040
   Shic F, 2007, INT J COMPUT VISION, V73, P159, DOI 10.1007/s11263-006-9784-6
   Simon S., 1999, Proceedings 1999 IEEE International Symposium on Computational Intelligence in Robotics and Automation. CIRA'99 (Cat. No.99EX375), P244, DOI 10.1109/CIRA.1999.810056
   Sun YR, 2003, ARTIF INTELL, V146, P77, DOI 10.1016/S0004-3702(02)00399-5
   Williams C.K.I., PASCAL VISUAL OBJECT
   Yang M, 2007, PROC CVPR IEEE, P1590, DOI 10.1109/CVPR.2007.383178
   Yu ZW, 2007, IEEE T MULTIMEDIA, V9, P766, DOI 10.1109/TMM.2007.893351
   ZHANG S, 2007, P ICIP SAN ANT US SE, P513
   Zhang T, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON VEHICULAR ELECTRONICS AND SAFETY, P1, DOI 10.1109/ISEE.2008.4562845
NR 46
TC 39
Z9 43
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2010
VL 12
IS 4
BP 300
EP 316
DI 10.1109/TMM.2010.2047607
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 596ER
UT WOS:000277668100007
DA 2024-07-18
ER

PT J
AU Lu, ZW
   Ip, HHS
AF Lu, Zhiwu
   Ip, Horace H. S.
TI Combining Context, Consistency, and Diversity Cues for Interactive Image
   Categorization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Active learning; image categorization; kernel methods; Markov models;
   semi-supervised learning
ID HIDDEN MARKOV-MODELS; NEIGHBORHOOD PROPAGATION; RETRIEVAL; ANNOTATION;
   FRAMEWORK; SEMANTICS
AB This paper presents a novel graph-based framework which can combine context, consistency, and diversity cues for interactive image categorization. The image representation is first formed with visual keywords by dividing images into blocks and then performing clustering on these blocks. The context across visual keywords within an image is further captured by proposing a 2-D spatial Markov chain model. To develop a graph-based approach to image categorization, we incorporate intra-image context into a new class of kernel called spatial Markov kernel which can be used to define the affinity matrix for a graph. After graph construction with this kernel, the large unlabeled data can be exploited by graph-based semi-supervised learning through label propagation with inter-image consistency. For interactive image categorization, we further combine this semi-supervised learning with active learning by defining a new diversity-based data selection criterion using spectral embedding. Experiments then demonstrate that the proposed framework can achieve promising results.
C1 [Lu, Zhiwu; Ip, Horace H. S.] City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
C3 City University of Hong Kong
RP Lu, ZW (corresponding author), City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
EM lzhiwu2@student.cityu.edu.hk; cship@cityu.edu.hk
OI IP, Ho Shing Horace/0000-0002-1509-9002; Lu, Zhiwu/0000-0003-0280-7724;
   Lu, Zhiwu/0000-0001-6429-7956
CR [Anonymous], P 20 INT C MACH LEAR
   [Anonymous], P CVPR
   [Anonymous], P IEEE C MULT EXP IE
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Bosch A., 2007, P 6 ACM INT C IMAGE, P401, DOI DOI 10.1145/1282280.1282340
   Boutell MR, 2007, IEEE T MULTIMEDIA, V9, P136, DOI 10.1109/TMM.2006.886372
   Brinker K., 2003, P 20 INT C MACHINE L, P59
   Cai D., 2007, Proceedings of the 15th international conference on Multimedia, P403, DOI [DOI 10.1145/1291233.1291329, 10.1145/1291233.1291329]
   CHANG E, 2008, P ACM INT C IM VID R, P569
   DEVIJVER PA, 1986, P 8 INT C PATT REC P, P259
   Fan JP, 2008, IEEE T MULTIMEDIA, V10, P167, DOI 10.1109/TMM.2007.911775
   Goh K., 2004, PROC ACM INT C MULTI, P564
   Hauptmann A, 2007, IEEE T MULTIMEDIA, V9, P958, DOI 10.1109/TMM.2007.900150
   Hofmann T, 2001, MACH LEARN, V42, P177, DOI 10.1023/A:1007617005950
   Hoi SCH, 2005, PROC CVPR IEEE, P302
   Jaakkola T, 2000, J COMPUT BIOL, V7, P95, DOI 10.1089/10665270050081405
   KUO SS, 1994, IEEE T PATTERN ANAL, V16, P842, DOI 10.1109/34.308482
   Lazebnik S., 2006, P IEEE CVF C COMP VI, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/CVPR.2006.68]
   Li F, 2008, IEEE T MULTIMEDIA, V10, P1592, DOI 10.1109/TMM.2008.2004914
   Li J, 2003, IEEE T PATTERN ANAL, V25, P1075, DOI 10.1109/TPAMI.2003.1227984
   LI J, 2008, P CVPR
   LU Z, 2009, P CVPR, P2719
   Lu Z.G., 2009, Optical Fiber Communication - incudes post deadline papers, P1
   Lu ZW, 2009, PROC CVPR IEEE, P397, DOI 10.1109/CVPRW.2009.5206861
   Lu ZW, 2010, PATTERN RECOGN LETT, V31, P36, DOI 10.1016/j.patrec.2009.09.003
   Ng AY, 2002, ADV NEUR IN, V14, P849
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Shah-Hosseini A., 2006, P 14 ACM INT C MULT, P703
   Tang JH, 2008, IEEE T MULTIMEDIA, V10, P620, DOI 10.1109/TMM.2008.921853
   Tong S, 2001, P 9 ACM INT C MULT, P107, DOI DOI 10.1145/500141.500159
   Wang F, 2008, IEEE T KNOWL DATA EN, V20, P55, DOI 10.1109/TKDE.2007.190672
   Yang Y, 2008, IEEE T MULTIMEDIA, V10, P437, DOI 10.1109/TMM.2008.917359
   Yu FY, 2008, COMPUT BIOL MED, V38, P635, DOI 10.1016/j.compbiomed.2008.02.004
   Zhou DY, 2004, ADV NEUR IN, V16, P321
NR 34
TC 14
Z9 17
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2010
VL 12
IS 3
BP 194
EP 203
DI 10.1109/TMM.2010.2041100
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 570IN
UT WOS:000275666900004
DA 2024-07-18
ER

PT J
AU Barzelay, Z
   Schechner, YY
AF Barzelay, Zohar
   Schechner, Yoav Y.
TI Onsets Coincidence for Cross-Modal Analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Correlators; cross-sensor fusion; machine vision; multimodal analysis;
   multisensor fusion
ID FREQUENCY; SEPARATION; TRACKING
AB Cross-modal analysis offers information beyond that extracted from individual modalities. Consider a nontrivial scene, that includes several moving visual objects, some of which emit sounds. The scene is sensed by a camcorder having a single microphone. A task for audio-visual analysis is to assess the number of independent audio-associated visual objects (AVOs), pinpoint the AVOs' spatial locations in the video and isolate each corresponding audio component. We describe an approach that helps handle this challenge. The approach does not inspect the low-level data. Rather, it acknowledges the importance of mid-level features in each modality, which are based on significant temporal changes in each modality. A probabilistic formalism identifies temporal coincidences between these features, yielding cross-modal association and visual localization. This association is further utilized in order to isolate sounds that correspond to each of the localized visual features. This is of particular benefit in harmonic sounds, as it enables subsequent isolation of each audio source. We demonstrate this approach in challenging experiments. In these experiments, multiple objects move simultaneously, creating motion distractions for one another, and produce simultaneous sounds which mix.
C1 [Barzelay, Zohar; Schechner, Yoav Y.] Technion Israel Inst Technol, Dept Elect Engn, IL-32000 Haifa, Israel.
C3 Technion Israel Institute of Technology
RP Barzelay, Z (corresponding author), Technion Israel Inst Technol, Dept Elect Engn, IL-32000 Haifa, Israel.
EM zohar.barzelay@gmail.com; yoav@ee.technion.ac.il
FU Taub Foundation; Israeli Science Foundation [1031/08]; BMBF
FX The work of Y. Schechner (Landau Fellow) is supported by the Taub
   Foundation. This work was supported by the Israeli Science Foundation
   (grant 1031/08), and conducted at the Ollendorff Center in the
   Electrical Engineering Department at the Technion. Minerva is funded
   through the BMBF. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Svetha Venkatesh.
CR [Anonymous], 2002, Discrete-Time Speech Signal Processing: Principles and Practice
   BACH FR, 2004, P NIPS
   Barzelay Z., 2007, P IEEE CVPR
   BARZELAY Z, EXPT DATA ONLINE
   Bello JP, 2005, IEEE T SPEECH AUDI P, V13, P1035, DOI 10.1109/TSA.2005.851998
   Benaroya L, 2006, IEEE T AUDIO SPEECH, V14, P191, DOI 10.1109/TSA.2005.854110
   BIRCHFIELD S, IMPLEMENTATION KANAD
   Bregman A. S., 1990, Auditory Scene Analysis: The Perceptual Organization of Sound, DOI [10.7551/mitpress/1486.001.0001, DOI 10.7551/MITPRESS/1486.001.0001]
   BROWN LG, 1992, COMPUT SURV, V24, P325, DOI 10.1145/146370.146374
   Brox T., 2004, P ECCV
   CHAZAN D, 1993, P IEEE INT C AC SPEE, V2, P728
   CHEN J, 2002, INFORM FUSION, P213
   Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010
   Choudhury T, 2002, INT C PATT RECOG, P789, DOI 10.1109/ICPR.2002.1048137
   Crochiere R. E., 1983, Multirate Digital Signal Processing
   CUADRA P, 2001, P ICMI
   DARRELL T, 2000, P ICMI 2000, P1611
   Eronen A, 2000, INT CONF ACOUST SPEE, P753
   Forsyth D. A., 2002, Computer vision: a modern approach, DOI DOI 10.5555/580035
   Fujisaki W, 2005, EXP BRAIN RES, V166, P455, DOI 10.1007/s00221-005-2385-8
   Gutfreund Y, 2002, SCIENCE, V297, P1556, DOI 10.1126/science.1073712
   HERSHEY J, 1999, P NIPS, P813
   HERSHEY J, 2001, P NIPS, P1173
   Hu GN, 2003, INT CONF ACOUST SPEE, P749
   Irani M, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P959, DOI 10.1109/ICCV.1998.710832
   Ke Y, 2005, PROC CVPR IEEE, P597
   Kidron E, 2007, IEEE T SIGNAL PROCES, V55, P1390, DOI 10.1109/TSP.2006.888095
   Klapuri A, 1999, INT CONF ACOUST SPEE, P3089, DOI 10.1109/ICASSP.1999.757494
   Klapuri AP, 2003, IEEE T SPEECH AUDI P, V11, P804, DOI 10.1109/TSA.2003.815516
   Lowe D. G., 1985, Perceptual Organization and Visual Recognition
   MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082
   Mesgarani N, 2008, J ACOUST SOC AM, V123, P899, DOI 10.1121/1.2816572
   MONACI G, 2006, P IEEE WORK PERC ORG
   Moore B. C. J., 1997, INTRO PSYCHOL HEARIN
   NAKADAI K, 2002, P IEEE C ROB AUT, V1, P1043
   ODONOVAN A, 2007, P IEEE CVPR
   Pérez P, 2004, P IEEE, V92, P495, DOI 10.1109/JPROC.2003.823147
   Rabiner Lawrence, 2010, Digital Processing of Speech Signals
   Radfar MH, 2007, IEEE T AUDIO SPEECH, V15, P2299, DOI 10.1109/TASL.2007.904233
   Rajaram S, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL V, PROCEEDINGS, P657
   RAO C, 2003, P ACM MULT
   RAVULAPALLI S, 2006, P IEEE ICPR, P1216
   Reddy AM, 2007, IEEE T AUDIO SPEECH, V15, P1766, DOI 10.1109/TASL.2007.901310
   Roweis ST, 2001, ADV NEUR IN, V13, P793
   RUI Y, 2000, P IEEE CVPR, V1, P13
   Sarel B, 2005, IEEE I CONF COMP VIS, P26
   SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794
   Smaragdis P., 2003, 4 INT S INDPENDENT C, P709
   SYEDAMAHMOOD T, 2002, P ICPR, V4
   Tabrikian J, 2004, IEEE T SPEECH AUDI P, V12, P76, DOI 10.1109/TSA.2003.819950
   Vincent E, 2007, SIGNAL PROCESS, V87, P1933, DOI 10.1016/j.sigpro.2007.01.016
   Wixson L, 2000, IEEE T PATTERN ANAL, V22, P774, DOI 10.1109/34.868680
   Yilmaz Ö, 2004, IEEE T SIGNAL PROCES, V52, P1830, DOI 10.1109/TSP.2004.828896
NR 53
TC 10
Z9 11
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2010
VL 12
IS 2
BP 108
EP 120
DI 10.1109/TMM.2009.2037387
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 573OA
UT WOS:000275922000002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Li, XR
   Snoek, CGM
   Worring, M
AF Li, Xirong
   Snoek, Cees G. M.
   Worring, Marcel
TI Learning Social Tag Relevance by Neighbor Voting
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multimedia indexing and retrieval; neighbor voting; social tagging; tag
   relevance learning
ID IMAGE RETRIEVAL; ANNOTATION; WORDS; TIME; GAP
AB Social image analysis and retrieval is important for helping people organize and access the increasing amount of user-tagged multimedia. Since user tagging is known to be uncontrolled, ambiguous, and overly personalized, a fundamental problem is how to interpret the relevance of a user-contributed tag with respect to the visual content the tag is describing. Intuitively, if different persons label visually similar images using the same tags, these tags are likely to reflect objective aspects of the visual content. Starting from this intuition, we propose in this paper a neighbor voting algorithm which accurately and efficiently learns tag relevance by accumulating votes from visual neighbors. Under a set of well-defined and realistic assumptions, we prove that our algorithm is a good tag relevance measurement for both image ranking and tag ranking. Three experiments on 3.5 million Flickr photos demonstrate the general applicability of our algorithm in both social image retrieval and image tag suggestion. Our tag relevance learning algorithm substantially improves upon baselines for all the experiments. The results suggest that the proposed algorithm is promising for real-world applications.
C1 [Li, Xirong; Snoek, Cees G. M.; Worring, Marcel] Univ Amsterdam, Intelligent Syst Lab Amsterdam, Inst Informat, NL-1098 XG Amsterdam, Netherlands.
C3 University of Amsterdam
RP Li, XR (corresponding author), Univ Amsterdam, Intelligent Syst Lab Amsterdam, Inst Informat, NL-1098 XG Amsterdam, Netherlands.
EM x.li@uva.nl; cgmsnoek@uva.nl; m.worring@uva.nl
RI Li, Xirong/AAD-3347-2019; Worring, Marcel/JRW-7059-2023
OI Li, Xirong/0000-0002-0220-8310; Snoek, Cees/0000-0001-9092-1556;
   Worring, Marcel/0000-0003-4097-4136
FU VIDI-Video [EC-FP6]; STW SEARCHER
FX Manuscript received January 05, 2009; April 13, 2009. First published
   August 18, 2009; current version published October 16, 2009. This work
   was supported in part by the EC-FP6 VIDI-Video project and in part by
   the STW SEARCHER project. The associate editor coordinating the review
   of this manuscript and approving it for publication was Dr. Zhu Liu.
CR [Anonymous], P ACM MULT
   [Anonymous], 2008, P 17 INT C WORLD WID
   AUCHARD E, 2007, REUTERS          NOV
   Barnard K, 2003, J MACH LEARN RES, V3, P1107, DOI 10.1162/153244303322533214
   BEGELMAN G, 2006, P WWW COLL WEB TAGG
   BILLERBECK B, 2004, P 15 AUSTR DAT C ADC, P69
   Carneiro G, 2007, IEEE T PATTERN ANAL, V29, P394, DOI 10.1109/TPAMI.2007.61
   Cha M, 2007, IMC'07: PROCEEDINGS OF THE 2007 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P1
   Chang E, 2003, IEEE T CIRC SYST VID, V13, P26, DOI 10.1109/TCSVT.2002.808079
   CHUA TS, 2004, P TRECVID WORKSH
   Cusano C, 2004, PROC SPIE, V5304, P330
   Datta AK, 2008, OPT LASER TECHNOL, V40, P1, DOI 10.1016/j.optlastec.2007.04.006
   Datta R., 2007, Proc. ACM Multimedia, P393
   Datta R, 2007, IEEE MULTIMEDIA, V14, P24, DOI 10.1109/MMUL.2007.67
   Fergus R, 2004, LECT NOTES COMPUT SC, V3021, P242
   Golder SA, 2006, J INF SCI, V32, P198, DOI 10.1177/0165551506062337
   Grangier D, 2008, IEEE T PATTERN ANAL, V30, P1371, DOI 10.1109/TPAMI.2007.70791
   Hauptmann A, 2007, IEEE T MULTIMEDIA, V9, P958, DOI 10.1109/TMM.2007.900150
   Horster E., 2007, P 6 ACM INT C IM VID, P17
   Hsu W. H., 2006, MULTIMEDIA '06, P35
   Huang J, 1997, PROC CVPR IEEE, P762, DOI 10.1109/CVPR.1997.609412
   Jin Y., 2005, P 13 ANN ACM INT C M, P706
   Jing Y, 2008, IEEE T PATTERN ANAL, V30, P1877, DOI 10.1109/TPAMI.2008.121
   JONES KS, 2000, J INF PROCESS MANAGE, V36, P809
   Kennedy Lyndon., 2007, P 15 INT C MULTIMEDI, P631
   Li J, 2008, IEEE T PATTERN ANAL, V30, P985, DOI 10.1109/TPAMI.2007.70847
   LI X, 2006, P 14 ANN ACM INT C M, P607
   LI X, 2009, P ICASSP, P3717
   Li Xirong., 2008, Proceedings of the 1st ACM International Conference on Multimedia Information Retrieval, MIR '08, P180
   Lin WH, 2003, IEEE/WIC INTERNATIONAL CONFERENCE ON WEB INTELLIGENCE, PROCEEDINGS, P242
   Matusiak K. K., 2006, OCLC Systems & Services, V22, P283, DOI 10.1108/10650750610706998
   Nah FFH, 2004, BEHAV INFORM TECHNOL, V23, P153, DOI 10.1080/01449290410001669914
   Natsev Apostol., 2007, MULTIMEDIA 07, P991
   Park G., 2003, P CIVR, P499
   Quelhas P, 2007, IEEE T PATTERN ANAL, V29, P1575, DOI 10.1109/TPAMI.2007.1155
   Scott D.W., 2015, Multivariate Density Estimation: Theory, Practice and Visualization, DOI 10.1002/9780470316849
   Shamma DA., 2007, P INT WORKSHOP WORKS, P275, DOI [10.1145/1290082.1290120, DOI 10.1145/1290082.1290120]
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Snoek CGM, 2007, IEEE T MULTIMEDIA, V9, P975, DOI 10.1109/TMM.2007.900156
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Wang CH, 2008, MULTIMEDIA SYST, V14, P205, DOI 10.1007/s00530-008-0128-y
   Wang XJ, 2008, IEEE T PATTERN ANAL, V30, P1919, DOI 10.1109/TPAMI.2008.127
   Wu L., 2008, P ACM INT C MULTIMED, P31, DOI DOI 10.1145/1459359.1459364
   YAN R, 2003, P CIVR, P649
   Yu H, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P929, DOI 10.1109/ICIP.2002.1039125
   Zhu M., 2004, Work. Paper 2004-09
NR 46
TC 225
Z9 242
U1 0
U2 33
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2009
VL 11
IS 7
BP 1310
EP 1322
DI 10.1109/TMM.2009.2030598
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 506GB
UT WOS:000270761300009
DA 2024-07-18
ER

PT J
AU Zhang, YF
   Xu, CS
   Lu, HQ
   Huang, YM
AF Zhang, Yi-Fan
   Xu, Changsheng
   Lu, Hanqing
   Huang, Yeh-Min
TI Character Identification in Feature-Length Films Using Global Face-Name
   Matching
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Face identification; movie analysis; social network analysis; video
   browsing
ID RECOGNITION
AB Identification of characters in films, although very intuitive to humans, still poses a significant challenge to computer methods. In this paper, we investigate the problem of identifying characters in feature-length films using video and film script. Different from the state-of-the-art methods on naming faces in the videos, most of which used the local matching between a visible face and one of the names extracted from the temporally local video transcript, we attempt to do a global matching between names and clustered face tracks under the circumstances that there are not enough local name cues that can be found. The contributions of our work include: 1) A graph matching method is utilized to build face-name association between a face affinity network and a name affinity network which are, respectively, derived from their own domains (video and script). 2) An effective measure of face track distance is presented for face track clustering. 3) As an application, the relationship between characters is mined using social network analysis. The proposed framework is able to create a new experience on character-centered film browsing. Experiments are conducted on ten feature-length films and give encouraging results.
C1 [Zhang, Yi-Fan; Xu, Changsheng; Lu, Hanqing] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Zhang, Yi-Fan; Xu, Changsheng; Lu, Hanqing] China Singapore Inst Digital Media, Singapore 119615, Singapore.
   [Huang, Yeh-Min] Natl Cheng Kung Univ, Tainan 70101, Taiwan.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; National
   Cheng Kung University
RP Zhang, YF (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM yfzhang@nlpr.ia.ac.cn; csxu@nlpr.ia.ac.cn; luhq@nlpr.ia.ac.cn;
   huang@mail.ncku.edu.tw
RI zhang, yifan/ABB-5853-2021; xu, cj/HJZ-3488-2023
FU National Natural Science Foundation of China [60833006]; Beijing
   Municipal Laboratory of Multimedia and Intelligent Software Technology
FX Manuscript received August 04, 2008; revised April 12, 2009. First
   published August 18, 2009; current version published October 16, 2009.
   This work was supported in part by the National Natural Science
   Foundation of China ( Grant No. 60833006) and in part by the Beijing
   Municipal Laboratory of Multimedia and Intelligent Software Technology.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Jie Yang.
CR [Anonymous], 2006, P COMP VIS PATT REC
   [Anonymous], P 13 ANN ACM INT C M
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], P 6 ACM INT C IM VID
   Arandjelovic O, 2005, PROC CVPR IEEE, P860
   Berg TL, 2004, PROC CVPR IEEE, P848
   Cour T, 2008, LECT NOTES COMPUT SC, V5305, P158, DOI 10.1007/978-3-540-88693-8_12
   Duda R., 1973, Pattern Classification and Scene Analysis
   Everingham M., 2006, BMVC, V2, P6
   Fitzgibbon A, 2002, LECT NOTES COMPUT SC, V2352, P304
   GUILLAUMIN M, 2008, P IEEE INT C COMP VI
   Houghton R, 1999, IEEE INTELL SYST APP, V14, P45, DOI 10.1109/5254.796089
   Jain V, 2007, P IEEE INT C COMP VI
   Leordeanu M, 2005, IEEE I CONF COMP VIS, P1482
   Li Y, 2004, IEEE T CIRC SYST VID, V14, P1073, DOI 10.1109/TCSVT.2004.831968
   Li Y, 2006, LECT NOTES COMPUT SC, V3979, P29
   Liu RR, 2007, METAB ENG, V9, P1, DOI 10.1016/j.ymben.2006.08.003
   Mei Tao., 2007, Proceedings of the 15th International Conference on Multimedia, P1075
   MYERS CS, 1981, AT&T TECH J, V60, P1389, DOI 10.1002/j.1538-7305.1981.tb00272.x
   Ng AY, 2002, ADV NEUR IN, V14, P849
   OZKAN D, 2006, P INT C IM VID RETR, P173
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Rubner Y, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P59, DOI 10.1109/ICCV.1998.710701
   Satoh S, 1997, PROC CVPR IEEE, P368, DOI 10.1109/CVPR.1997.609351
   SCOTT J., 2017, Social Network Analysis, V4th
   Sivic J, 2005, LECT NOTES COMPUT SC, V3568, P226
   Weng C, 2007, P INT WORKSH MULT IN, P51, DOI DOI 10.1145/1290082.1290092
   WU Y, 2007, P PAC RIM C MULT, P665
   Yang J, 2005, Proceedings of the 2005 International Conference on Active Media Technology (AMT 2005), P28
   Yang J, 2004, LECT NOTES COMPUT SC, V3115, P270
   Yang J., 2004, P 12 ANN ACM INT C M, P580
   Zhao W, 2003, ACM COMPUT SURV, V35, P399, DOI 10.1145/954339.954342
NR 32
TC 104
Z9 115
U1 1
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2009
VL 11
IS 7
BP 1276
EP 1288
DI 10.1109/TMM.2009.2030629
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 506GB
UT WOS:000270761300006
DA 2024-07-18
ER

PT J
AU Zeng, WJ
   Zhu, YN
   Lu, HB
   Zhuang, XH
AF Zeng, Wenjun
   Zhu, Yingnan
   Lu, Haibin
   Zhuang, Xinhua
TI Path-Diversity P2P Overlay Retransmission for Reliable IP-Multicast
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE IP-multicast; overlay; path-diversity; reliability; retransmission
ID ERROR RECOVERY; FEC
AB IP-multicast is a bandwidth efficient transmission mechanism for group communications. Reliability in IP-multicast, however, poses a set of significant challenges. To address the reliability and scalability issues in IP-multicast, this paper proposes a novel, highly distributed, and lightweight overlay peer-to-peer retransmission architecture that exploits path-diversity by taking advantages of both IP-multicast and an overlay network. An approach that leverages both disjoint-path-finding and periodic selective probing to take into account peer's recent packet loss probability, retransmission delay and recent retransmission success rate is proposed to effectively construct an efficient and dynamic overlay retransmission network. We show that the proposed path diversity overlay retransmission architecture has the potential to significantly reduce the retransmission delay, improve the reliability, playback quality, and scalability of IP-multicast based multimedia applications. Given a deployed IP-multicast network, the proposed overlay retransmission architecture is practical, scalable, and easy to deploy, requiring no change to the existing network infrastructure.
C1 [Zeng, Wenjun; Zhu, Yingnan; Lu, Haibin; Zhuang, Xinhua] Univ Missouri, Dept Comp Sci, Columbia, MO 65211 USA.
C3 University of Missouri System; University of Missouri Columbia
RP Zeng, WJ (corresponding author), Univ Missouri, Dept Comp Sci, Columbia, MO 65211 USA.
FU Microsoft Research; NSF [CNS-0423386]
FX This work was supported in part by a grant from Microsoft Research and
   in part by the NSF under Grant CNS-0423386.
CR ADAMSON B, 2007, NACK ORIENT IN PRESS
   ALMEROTH K, 2007, KEYN SPEECH WORKSH P
   ANDERSEN D, 2001, P ACM S OP SYST PRIN
   [Anonymous], NETWORK SIMULATOR NS
   [Anonymous], 2001, Pastry: Scalable, decentralized object location, and routing for large-scale peer-to-peer systems, DOI DOI 10.1007/3-540-45518-3_18
   [Anonymous], P 2002 C APPL TECHN
   BANERJEE S, 2003, P ACM SIGMETRICS SAN
   BEAVERS J, 2004, MSRTR200442
   Bernstein D. S., 2003, P 2 INT WORKSH PEER
   Calderon M, 1998, IEEE NETWORK, V12, P46, DOI 10.1109/65.690961
   CHU Y, 2000, P ACM SIGMETRICS C
   Fei T., 2006, P IEEE INFOCOM
   Gemmell J, 2003, IEEE NETWORK, V17, P16, DOI 10.1109/MNET.2003.1174173
   Hefeeda M., 2003, P ACM MULT
   Holbrook H. W., 1995, P ACM SIGCOMM
   Kasera SK, 2000, IEEE NETWORK, V14, P48, DOI 10.1109/65.819171
   Kim E, 2002, IEEE COMMUN LETT, V6, P464, DOI 10.1109/LCOMM.2002.803474
   Lacher MS, 2000, IEEE ACM T NETWORK, V8, P224, DOI 10.1109/90.842144
   LEIBOWITZ N, 2003, P 3 IEEE WORKSH INT
   Lestayo T, 2001, ELECTRON LETT, V37, P1333, DOI 10.1049/el:20010913
   LEVINE BN, 1996, P INT C NETW PROT OC
   LI J, 2004, MSRTR2004101
   Lin JC, 1996, IEEE INFOCOM SER, P1414, DOI 10.1109/INFCOM.1996.493090
   Liu CG, 1998, IEEE ACM T NETWORK, V6, P686, DOI 10.1109/90.748082
   MANKIN A, 1998, RFC2357
   Nonnenmacher J, 1998, IEEE ACM T NETWORK, V6, P349, DOI 10.1109/90.720869
   Obraczka K, 1998, IEEE COMMUN MAG, V36, P94, DOI 10.1109/35.649333
   Ott J., 2006, 4585 IETF RFC
   PADMANABHAN VN, 2002, P ACM NOSSDAV MIAM B
   PAPADOPOULOS C, 1998, P IEEE INFOCOM MAR
   PINGALI S, 1994, P ACM SIGMETRICS
   RATNASAMY S, 2006, P ACM SIGC PIS IT SE
   Rey J., 2006, 4588 IETF RFC
   ROWSTRON A, 2001, P IFIP ACM MIDDL 200
   STOICA I, 2001, P ACM SIGCOMM
   Tan WT, 2001, IEEE T CIRC SYST VID, V11, P373, DOI 10.1109/76.911162
   THALER D, 2006, AUTOMATIC I IN PRESS
   Tran DA, 2004, IEEE J SEL AREA COMM, V22, P121, DOI 10.1109/JSAC.2003.818803
   WU JC, 2006, P IPTV WORKSH CONJ 1
   Xiang Z, 2004, IEEE T MULTIMEDIA, V6, P343, DOI 10.1109/TMM.2003.822819
   YAVATKAR R, 1995, P ACM MULT
   Yiu WPK, 2006, IEEE T MULTIMEDIA, V8, P219, DOI 10.1109/TMM.2005.864268
   Zegura EllenW., 1996, Gt-itm: Georgia tech internetwork topology models
   Zeng W., 2006, P IEEE INT C MULT EX
   ZHANG X, 2005, P IEEE MULT SIGN PRO
   Zhang XY, 2005, IEEE INFOCOM SER, P2102
   Zhu Y., 2007, P IEEE INT C MULT EX
NR 47
TC 9
Z9 9
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2009
VL 11
IS 5
BP 960
EP 971
DI 10.1109/TMM.2009.2021712
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 474KN
UT WOS:000268282300014
DA 2024-07-18
ER

PT J
AU van Gemert, JC
   Veenman, CJ
   Geusebrock, JM
AF van Gemert, Jan C.
   Veenman, Cor J.
   Geusebrock, Jan-Mark
TI Episode-Constrained Cross-Validation in Video Concept Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
ID PERFORMANCE; FRAMEWORK
AB Whereas video tells a narrative by a composition of shots, current video retrieval methods focus mainly on single shots. In retrieval performance estimation, similar shots in a narrative may result in performance overestimation. We propose an episode-based version of cross-validation leading up to 14% classification improvement over shot-based cross-validation.
C1 [van Gemert, Jan C.; Veenman, Cor J.; Geusebrock, Jan-Mark] Univ Amsterdam, Inst Informat, Intelligent Syst Lab Amsterdam, NL-1098 SJ Amsterdam, Netherlands.
C3 University of Amsterdam
RP van Gemert, JC (corresponding author), Univ Amsterdam, Inst Informat, Intelligent Syst Lab Amsterdam, Kruislaan 403, NL-1098 SJ Amsterdam, Netherlands.
EM jvgemert@uva.nl; c.j.veenman@uva.nl; mark@science.uva.nl
CR [Anonymous], P NIPS
   ASLAM JA, 2005, P SIGIR
   Davis J., 2006, P ICML
   Duan LY, 2005, IEEE T MULTIMEDIA, V7, P1066, DOI 10.1109/TMM.2005.858395
   Duda R., 1973, Pattern Classification and Scene Analysis
   Gao S, 2007, IEEE T MULTIMEDIA, V9, P1430, DOI 10.1109/TMM.2007.906597
   Hanjalic A, 1999, IEEE T CIRC SYST VID, V9, P580, DOI 10.1109/76.767124
   HUURNINK B, 2007, P ACM MULT MIR
   LU L, 2001, P ACM MULT
   Müller H, 2001, PATTERN RECOGN LETT, V22, P593, DOI 10.1016/S0167-8655(00)00118-5
   Naphade MR, 2001, IEEE T MULTIMEDIA, V3, P141, DOI 10.1109/6046.909601
   QI Y, 2003, P ICME JUL
   RAGHAVAN VV, 1989, ACM T INFORM SYST, V7, P205, DOI 10.1145/65943.65945
   RAUTIAINEN M, 2006, P ACM MULT
   SNOEK CGM, 2006, P ACM MULT, P101
   SNOEK CGM, 2006, TPAMI, V28, P1678
   VANGEMERT JC, 2006, P ACM MULT
   VANGEMERT JC, 2008, P EUR C COMP VIS OCT
   YANG J, 2004, P CIVR
   YANG J, 2006, P ACM MULT MIR
   Yilmaz Emine, 2006, P CIKM
NR 21
TC 6
Z9 8
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2009
VL 11
IS 4
BP 780
EP 786
DI 10.1109/TMM.2009.2017619
PG 7
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 449LA
UT WOS:000266330900018
DA 2024-07-18
ER

PT J
AU Nakashima, Y
   Tachibana, R
   Babaguchi, N
AF Nakashima, Yuta
   Tachibana, Ryuki
   Babaguchi, Noboru
TI Watermarked Movie Soundtrack Finds the Position of the Camcorder in a
   Theater
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio watermarking; movie soundtrack; prevention of movie piracy;
   recording position estimation
ID AUDIO; ROBUST
AB In recent years, the problem of camcorder piracy in theaters has become more serious due to technical advances in camcorders. In this paper, as a new deterrent to camcorder piracy, we propose a system for estimating the recording position from which a camcorder recording is made. The system is based on spread-spectrum audio watermarking for the multichannel movie soundtrack. It utilizes a stochastic model of the detection strength, which is calculated in the watermark detection process. Our experimental results show that the system estimates recording positions in an actual theater with a mean estimation error of 0.44 in. The results of our MUSHRA subjective listening tests show the method does not significantly spoil the subjective acoustic quality of the soundtrack. These results indicate that the proposed system is applicable for practical uses.
C1 [Nakashima, Yuta; Babaguchi, Noboru] Osaka Univ, Grad Sch Engn, Osaka, Japan.
   [Tachibana, Ryuki] IBM Japan Ltd, Tokyo Res Lab, Kanagawa, Japan.
C3 Osaka University; International Business Machines (IBM)
RP Nakashima, Y (corresponding author), Osaka Univ, Grad Sch Engn, Osaka, Japan.
EM nakashima@nanase.comm.eng.osaka-u.ac.jp; ryuki@jp.ibm.com;
   babaguchi@comm.eng.osaka-u.ac.jp
RI Nakashima, Yuta/Y-6218-2019; Nakashima, Yuta/O-6299-2014
OI Nakashima, Yuta/0000-0001-8000-3567; Nakashima, Yuta/0000-0001-8000-3567
FU Okawa Foundation
FX Manuscript received July 14, 2008; revised October 22, 2008. First
   published February 10, 2009; current version published March 18, 2009.
   This work was supported in part by the research grant of the Okawa
   Foundation. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Z. Jane Wang
CR BLOOM JA, 2004, P 38 AS C SIGN SYST, V1, P363
   Byers S, 2004, TELECOMMUN POLICY, V28, P619, DOI 10.1016/j.telpol.2004.05.007
   Gohshi S, 2005, LECT NOTES ARTIF INT, V3682, P1099
   GRUHL D, 1996, P 1 INT WORKSH INF H, V1174, P293
   Haitsma J, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P487, DOI 10.1109/ICIP.2001.958534
   *ISO IEC, 1993, 1117211993 ISOIEC
   *ITU BS, 2003, 1534 ITU BS
   Kirovski D, 2003, IEEE T SIGNAL PROCES, V51, P1020, DOI 10.1109/TSP.2003.809384
   Ko BS, 2005, IEEE T MULTIMEDIA, V7, P212, DOI 10.1109/TMM.2005.843366
   Lazic N, 2006, IEEE T MULTIMEDIA, V8, P918, DOI 10.1109/TMM.2006.879880
   Lubin J, 2003, P SOC PHOTO-OPT INS, V5020, P536, DOI 10.1117/12.477336
   *MOT PICT ASS, ANT FACT SHEET AS PA
   *MOT PICT ASS AM, 2005 US PIR FACT SHE
   Nakashima Y, 2007, INT CONF ACOUST SPEE, P253
   Nguyen P, 2003, PROC SPIE, V5020, P553, DOI 10.1117/12.479735
   SUZUKI Y, 1995, J ACOUST SOC AM, V97, P1119, DOI 10.1121/1.412224
   Swanson MD, 1998, SIGNAL PROCESS, V66, P337, DOI 10.1016/S0165-1684(98)00014-0
   Tachibana R, 2004, EURASIP J APPL SIG P, V2004, P1955, DOI 10.1155/S1110865704403138
   Tachibana R, 2002, SIGNAL PROCESS, V82, P1455, DOI 10.1016/S0165-1684(02)00284-0
   Tachibana R., 2006, P ACM MULT SEC WORKS, P108
   Tachibana R., 2007, P 3 INT C INT INF HI
   Zwicker E., 1999, PSYCHOACOUSTICS FACT, V2nd, DOI DOI 10.1007/978-3-662-09562-1
NR 22
TC 28
Z9 34
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2009
VL 11
IS 3
BP 443
EP 454
DI 10.1109/TMM.2009.2012938
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Arts &amp; Humanities Citation Index (A&amp;HCI)
SC Computer Science; Telecommunications
GA 425JH
UT WOS:000264632300010
DA 2024-07-18
ER

PT J
AU Wang, YB
   Claypool, M
   Kinicki, R
AF Wang, Yubing
   Claypool, Mark
   Kinicki, Robert
TI Modeling RPS and Evaluating Video Repair With VQM
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE H.264; PSNR; reference distance; repair; RPS; VQM
AB Reference Picture Selection (RPS) is a promising repair technique in lossy networks for delay-sensitive video, whereby the video encoder uses one of several previous frames as a reference frame for predictive encoding. RPS can operate in two different modes: an optimistic policy that uses negative acknowledgements (NACKs) and a more conservative policy that relies upon positive acknowledgements (ACKs). This paper compares RPS NACK and RPS ACK under various network conditions and video contents using two analytical models. The two models characterize RPS NACK and RPS ACK by incorporating the impact of reference distance on video quality, prediction dependency among video frames and Group of Pictures (GOP) length. Given packet-loss rate, round-trip time and capacity constraints, the models predict average video quality for videos with RPS ACK and RPS NACK using the Video Quality Metric (VQM). Using these two models, a series of experiments are conducted to investigate RPS performance under various conditions. The insights derived from our models can help determine appropriate choices for RPS NACK and RPS ACK under various network conditions and video contents.
C1 [Wang, Yubing] EMC Corp, Southborough, MA 01729 USA.
   [Claypool, Mark; Kinicki, Robert] Worcester Polytech Inst, Dept Comp Sci, Worcester, MA 01609 USA.
C3 Dell Incorporated; Dell EMC; Worcester Polytechnic Institute
RP Wang, YB (corresponding author), EMC Corp, Southborough, MA 01729 USA.
EM wang_yubing@emc.com; clay-pool@cs.wpi.edu; rek@cs.wpi.edu
RI Claypool, Mark/ABC-5316-2020
CR [Anonymous], P IASTED INT C INT M
   FUKUNAGA S, 1996, P IEEE GLOB TEL C GL, V3, P1503
   Girod B, 1999, P IEEE, V87, P1707, DOI 10.1109/5.790632
   *JVT, 2003, JVTG050RL ITUT ISOIE
   MAYERPATEL K, 2002, P ACM MULT JUAN LES
   Pinson MH, 2004, IEEE T BROADCAST, V50, P312, DOI 10.1109/TBC.2004.834028
   RHEE I, 1998, P ACM SIGCOM VANC BC
   Richardson Iain E, 2004, H. 264 and MPEG-4 Video Compression: Video Coding for Next-Generation Multimedia
   TOMITA Y, 1997, P INT PICT COD S PCS, P743
   Wang Y, 2000, IEEE SIGNAL PROC MAG, V17, P61, DOI 10.1109/79.855913
   WANG Y, 2008, P ACM SPIE MULT COMP
   Wang Y., 2007, P ACM SPIE MULT COMP
   WANG Y, 1998, P IEEE SPECIAL ISSUE, V86
   WU H, 2003, P WORKSH NETW OP SYS
NR 14
TC 4
Z9 4
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2009
VL 11
IS 1
BP 128
EP 137
DI 10.1109/TMM.2008.2008928
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 398EF
UT WOS:000262714700011
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Jubran, MK
   Bansal, M
   Kondi, LP
AF Jubran, Mohammad K.
   Bansal, Manu
   Kondi, Lisimachos P.
TI Low-Delay Low-Complexity Bandwidth-Constrained Wireless Video
   Transmission Using SVC Over MIMO Systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Distortion estimation; optimal bandwidth allocation; scalable H264; SVC;
   wireless MIMO systems
ID FINE-GRANULAR-SCALABILITY; H.264/AVC; CODES
AB We propose an efficient strategy for the transmission of scalable video over multiple-input multiple-output (MIMO) wireless systems. In this paper, we use the latest scalable H.264 codec (SVC), which provides combined temporal, quality and spatial scalability. At the transmitter, we estimate the decoded video distortion for given channel conditions taking into account the effects of quantization, packet loss and error concealment. The proposed scalable decoder distortion algorithm offers low delay and low complexity. The performance of this method is validated using experimental results. In our proposed system, we use a MIMO system with orthogonal space-time block codes (O-STBC) that provides spatial diversity and guarantees independent transmission of different symbols within the block code. The bandwidth constrained allocation problem considered here is simplified and solved for one O-STBC symbol at a time. Furthermore, we take the advantage of the hierarchical structure of SVC to attain the optimal solution for each group of pictures (GOP) of the video sequence. We incorporate the estimated decoder distortion to optimally select the application layer parameter, i.e., quantization parameter (QP), and physical layer parameters, i.e., channel coding rate and modulation type for wireless video transmission.
C1 [Jubran, Mohammad K.] Birzeit Univ, Dept Elect Engn, Birzeit, Israel.
   [Bansal, Manu] Goodwin Procter LLP, Intellectual Property Grp, Boston, MA 02109 USA.
   [Kondi, Lisimachos P.] Univ Ioannina, Dept Comp Sci, GR-45110 Ioannina, Greece.
C3 Birzeit University; University of Ioannina
RP Jubran, MK (corresponding author), Birzeit Univ, Dept Elect Engn, Birzeit, Israel.
EM mjubran@birzeit.edu; mbansal@goodwinprocter.com; lkon@cs.uoi.gr
OI JUBRAN, MOHAMMAD/0000-0002-7415-9044
CR Alamouti SM, 1998, IEEE J SEL AREA COMM, V16, P1451, DOI 10.1109/49.730453
   Chan YS, 2003, IEEE J SEL AREA COMM, V21, P1516, DOI 10.1109/JSAC.2003.815228
   Eisenberg Y, 2006, IEEE T IMAGE PROCESS, V15, P289, DOI 10.1109/TIP.2005.860600
   HAGENAUER J, 1988, IEEE T COMMUN, V36, P389, DOI 10.1109/26.2763
   JUBRAN MK, 2006, P IEEE MIL COMM C 20
   JUBRAN MK, 2008, IEEE T IMAG IN PRESS, V17
   Jubran MK, 2007, INT CONF ACOUST SPEE, P849
   Kondi LP, 2005, IEEE T CIRC SYST VID, V15, P1629, DOI 10.1109/TCSVT.2005.856922
   Kondi LP, 2002, IEEE T IMAGE PROCESS, V11, P1043, DOI 10.1109/TIP.2002.802507
   Kuo CH, 2002, IEEE WCNC, P931, DOI 10.1109/WCNC.2002.993396
   Radha HM, 2001, IEEE T MULTIMEDIA, V3, P53, DOI 10.1109/6046.966110
   Regunathan S, 2001, SIGNAL PROCESS-IMAGE, V16, P725, DOI 10.1016/S0923-5965(01)00003-0
   REICHEL J, 2005, JVTN020
   Schierl T, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P885
   Schwarz H, 2005, 2005 IEEE International Conference on Multimedia and Expo (ICME), Vols 1 and 2, P446, DOI 10.1109/ICME.2005.1521456
   SCHWARZ H, 2004, P PCS 2004 SAN FRANC
   SCHWARZ H, JTC1SC29WG11 ISOIEC
   Shen YS, 2006, IEEE T IMAGE PROCESS, V15, P273, DOI 10.1109/TIP.2005.860598
   Srinivasan R, 2004, AMYLOID, V11, P10, DOI 10.1080/13506120410001667872
   Stockhammer T, 2003, IEEE T CIRC SYST VID, V13, P657, DOI 10.1109/TCSVT.2003.815167
   Tarokh V, 1999, IEEE T INFORM THEORY, V45, P1456, DOI 10.1109/18.771146
   Tarokh V, 1998, IEEE T INFORM THEORY, V44, P744, DOI 10.1109/18.661517
   van der Schaar M, 2002, IEEE T CIRC SYST VID, V12, P360, DOI 10.1109/TCSVT.2002.800319
   van der Schaar M, 2001, IEEE T CIRC SYST VID, V11, P318, DOI 10.1109/76.911158
   Wenger S, 2003, IEEE T CIRC SYST VID, V13, P645, DOI 10.1109/TCSVT.2003.814966
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   WU J, 2004, P VIS COMM IM PROC S
   Zhai F, 2006, IEEE T IMAGE PROCESS, V15, P40, DOI 10.1109/TIP.2005.860353
   Zhang R, 2000, IEEE J SEL AREA COMM, V18, P966, DOI 10.1109/49.848250
   Zhao SJ, 2006, IEEE T MOBILE COMPUT, V5, P303, DOI 10.1109/TMC.2006.1599401
NR 30
TC 21
Z9 23
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2008
VL 10
IS 8
BP 1698
EP 1707
DI 10.1109/TMM.2008.2007317
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 390KS
UT WOS:000262163600023
DA 2024-07-18
ER

PT J
AU Bharanitharan, K
   Liu, BD
   Yang, JF
   Tsai, WC
AF Bharanitharan, K.
   Liu, Bin-Da
   Yang, Jar-Ferr
   Tsai, Wen-Chih
TI A Low Complexity Detection of Discrete Cross Differences for Fast
   H.264/AVC Intra Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Advanced video coding; fast mode decision; gradient algorithm; H.264
   intra prediction
ID MODE DECISION ALGORITHM; VIDEO; ARCHITECTURE
AB A low complexity fast mode decision algorithm for H.264/AVC intra prediction that uses discrete cross differences (DCD) to reduce the unlikely candidate modes in the RDO calculation is proposed. By using horizontal and vertical differences in different locations, the directions of the edges can be precisely detected. Experimental results show that the proposed fast mode decision algorithm reduces the encoding time by about 56%, with negligible loss of video quality. To realize the proposed algorithm, a VLSI design, which is comprised of a cross difference unit and direction detection unit, is implemented for the mode pre-selection stage of intra prediction. The design is synthesized using UMC 0.18 mu m CMOS technology and simulated with Verilog-XL. The operating frequency of the synthesized core can exceed 50 MHz.
C1 [Bharanitharan, K.; Liu, Bin-Da; Yang, Jar-Ferr; Tsai, Wen-Chih] Natl Cheng Kung Univ, Dept Elect Engn, Tainan 701, Taiwan.
C3 National Cheng Kung University
RP Bharanitharan, K (corresponding author), Natl Cheng Kung Univ, Dept Elect Engn, Tainan 701, Taiwan.
EM dharan@spic.ee.ncku.edu.tw; bdliu@mail.ncku.eud.tw;
   jfyang@ee.ncku.edu.tw; Researchh.264@gmail.com
OI Brown, Darren K/0000-0003-1246-1974
FU National Science Council of Taiwan [NSC95-2220-E-006-007, NSC
   96-2220-E-006-002]
FX Manuscript received December 14. 2007 revised June 08, 2008. First
   published November 17, 2008. This work was Supported in part by the
   National Science Council of Taiwan under Grants NSC95-2220-E-006-007 and
   NSC 96-2220-E-006-002. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Susanto
   Rahardja.
CR [Anonymous], H 264 AVC REFERENCE
   Assuncao PAA, 1997, INT CONF ACOUST SPEE, P2633, DOI 10.1109/ICASSP.1997.595329
   BJONTEGAARD G, 2001, 13 VCEG M33 M UNPUB
   Chang HS, 2005, IEEE T IMAGE PROCESS, V14, P145, DOI 10.1109/TIP.2004.840706
   Huang YW, 2005, IEEE T CIRC SYST VID, V15, P378, DOI 10.1109/TCSVT.2004.842620
   Hwang G, 2007, 9TH INTERNATIONAL CONFERENCE ON ADVANCED COMMUNICATION TECHNOLOGY: TOWARD NETWORK INNOVATION BEYOND EVOLUTION, VOLS 1-3, P399, DOI 10.1109/ICACT.2007.358380
   *ITUT, 1449610 ITUT H264ISO
   Jain AK, 1996, PATTERN RECOGN, V29, P1233, DOI 10.1016/0031-3203(95)00160-3
   Kim C, 2004, IEEE IMAGE PROC, P769
   Ku CW, 2006, IEEE T CIRC SYST VID, V16, P917, DOI 10.1109/TCSVT.2006.879992
   Lee SW, 2000, IEEE T MULTIMEDIA, V2, P240, DOI 10.1109/6046.890059
   Li H., 2006, PROCEEDING INT C COM, P67
   Meng B., 2003, P IEEE INT C AC SPEE, V3, P389
   Pan F, 2005, IEEE T CIRC SYST VID, V15, P813, DOI 10.1109/TCSVT.2005.848356
   Shanableh T, 2003, SIGNAL PROCESS-IMAGE, V18, P601, DOI 10.1016/S0923-5965(03)00055-9
   SULLIVAN G, 2003, 9 JVT M SAN DI UNPUB
   Tseng CH, 2006, IEEE T CIRC SYST VID, V16, P1027, DOI 10.1109/TCSVT.2006.878146
   Wang HL, 2007, IEEE T MULTIMEDIA, V9, P882, DOI 10.1109/TMM.2007.893345
   Wang JC, 2007, IEEE T CIRC SYST VID, V17, P1414, DOI 10.1109/TCSVT.2007.903786
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Yang CL, 2004, IEEE IMAGE PROC, P461
NR 21
TC 24
Z9 25
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2008
VL 10
IS 7
BP 1250
EP 1260
DI 10.1109/TMM.2008.2004904
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 378HB
UT WOS:000261310700003
DA 2024-07-18
ER

PT J
AU He, ZH
   Wu, DO
AF He, Zhihai
   Wu, Dapeng Oliver
TI Linear Rate Control and Optimum Statistical Multiplexing for H.264 Video
   Broadcast
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE H.264 video coding; rate control; statistical multiplexing; optimum rate
   allocation; TV broadcast
ID JOINT RATE CONTROL; STANDARD; MPEG
AB The H.264 video coding standard achieves significantly improved video compression efficiency and finds important applications in digital video broadcast. To enable H.264 video encoding for digital TV broadcast and maximize its broadcast efficiency, there are two important issues that need to be adequately addressed. First, we need to understand the complex coding mechanism of an H.264 video encoder and develop a model to analyze and control its rate-distortion (R-D) behavior in an accurate and robust manner. Second, the R-D behaviors of individual channels in the broadcast system should be jointly controlled and optimized under bandwidth and buffer constraints so as to maximize the overall broadcast quality. In this paper, we develop a linear rate model and a linear rate control scheme for H.264 video coding. We develop an optimum statistical multiplexing system to allocate bits across video programs (each being encoded by an H.264 encoder) and video frames so that the overall video broadcast quality is maximized. We study the bandwidth and buffer constraints in video broadcast and formulate the optimum statistical multiplexing into a constrained mathematical optimization problem. Realizing that it is impossible to find a close-form solution for global optima, we propose a simple yet efficient algorithm to find a near-optimum solution for joint rate allocation under buffer constraints. Our extensive simulation results demonstrate that the proposed statistical multiplexing system achieves about 40-50% saving in bandwidth, provides a smooth video quality change across programs and frames, and maintains robust decoder buffer control.
C1 [He, Zhihai] Univ Missouri, Dept Elect & Comp Engn, Columbia, MO 65211 USA.
   [Wu, Dapeng Oliver] Univ Florida, Dept Elect & Comp Engn, Gainesville, FL 32611 USA.
C3 University of Missouri System; University of Missouri Columbia; State
   University System of Florida; University of Florida
RP He, ZH (corresponding author), Univ Missouri, Dept Elect & Comp Engn, Columbia, MO 65211 USA.
EM HeZhi@missouri.edu; wu@ece.ufl.edu
RI He, Zhihai/A-5885-2019
OI Wu, Dapeng/0000-0003-1755-0183
CR BALAKRISHAN M, 1997, P ICIP 97 SANT BARB, V1, P377
   BERGER T, 1984, RATE DISTORTION THEO
   Böröczky L, 1999, IBM J RES DEV, V43, P511, DOI 10.1147/rd.434.0511
   DERIDDER H, 1992, P SPIE HUMAN VISION, V3, P1626
   Ding W, 1996, IEEE T CIRC SYST VID, V6, P12, DOI 10.1109/76.486416
   Eleftheriadis A, 2006, IEEE T MULTIMEDIA, V8, P297, DOI 10.1109/TMM.2005.864346
   He ZH, 2002, IEEE T CIRC SYST VID, V12, P970, DOI 10.1109/TCSVT.2002.805511
   HUS CY, 1997, IEEE J SEL AREA COMM, V15, P1016
   *ISO IEC, 2000, 1381802 ISOIEC
   *ITU T, 1998, H263 ITUT
   KIM S, 2004, P IEEE INT C MULT EX
   KWON DK, 2006, IEEE INT C MULT EXP
   LEGALL D, 1991, COMMUN ACM, V34, P46, DOI 10.1145/103085.103090
   LI ZG, 2004, P IEEE INT C IM PROC
   Lin L-J, 1990, IEEE T CIRCUITS SYST, V38, P82
   LUBIN J, 1997, 97612 ANSI TI TIA15
   Masry MA, 2004, SIGNAL PROCESS-IMAGE, V19, P133, DOI 10.1016/j.image.2003.08.001
   PERKINS M, 1995, SMPTE J, V104, P596, DOI 10.5594/J17232
   Ribas-Corbera J, 1999, IEEE T CIRC SYST VID, V9, P172, DOI 10.1109/76.744284
   SHOHAM Y, 1988, IEEE T ACOUST SPEECH, V36, P1445, DOI 10.1109/29.90373
   Sikora T, 1997, IEEE T CIRC SYST VID, V7, P19, DOI 10.1109/76.554415
   Wang LM, 1996, IEEE T CONSUM ELECTR, V42, P300, DOI 10.1109/30.536124
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xin J, 2005, P IEEE, V93, P84, DOI 10.1109/JPROC.2004.839620
   Yang J, 2005, IEEE T CONSUM ELECTR, V51, P617, DOI 10.1109/TCE.2005.1468009
   YIN P, 2004, P INT C IM PROC OCT, V1, P449
   YU H, 2005, P ISCAS 2005 MAY, V1, P312
   JVT H 264 JM SOFTWAR
NR 28
TC 43
Z9 50
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2008
VL 10
IS 7
BP 1237
EP 1249
DI 10.1109/TMM.2008.2004903
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 378HB
UT WOS:000261310700002
DA 2024-07-18
ER

PT J
AU Huang, CR
   Lee, HP
   Chen, CS
AF Huang, Chun-Rong
   Lee, Huai-Ping
   Chen, Chu-Song
TI Shot Change Detection via Local Keypoint Matching
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Invariant local feature; matching; recognition; shot change detection
ID OBJECT RECOGNITION; VIDEO; SEGMENTATION; HISTOGRAM
AB Shot change detection is an essential step in video content analysis. However, automatic shot change detection often suffers from high false detection rates due to camera or object movements. To solve this problem, we propose an approach based on local keypoint matching of video frames. This approach aims to detect both abrupt and gradual transitions between shots without modeling different kinds of transitions. Our experiment results show that the proposed algorithm is effective for most kinds of shot changes.
C1 [Huang, Chun-Rong; Chen, Chu-Song] Acad Sinica, Inst Informat Sci, Taipei, Taiwan.
   [Lee, Huai-Ping] Univ N Carolina, Dept Comp Sci, Chapel Hill, NC 27514 USA.
C3 Academia Sinica - Taiwan; University of North Carolina; University of
   North Carolina Chapel Hill
RP Huang, CR (corresponding author), Acad Sinica, Inst Informat Sci, Taipei, Taiwan.
EM nckuos@iis.sinica.edu.tw; lhp@cs.unc.edu; song@iis.sinica.edu.tw
OI Huang, Chun-Rong/0000-0003-2372-5429
CR [Anonymous], 1995, P ACM MULT, DOI DOI 10.1145/217279.215266
   [Anonymous], 2001, International Journal of Image and Graphics (IJIG), Vol, DOI DOI 10.1142/S021946780100027X
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Bescós J, 2004, IEEE T CIRC SYST VID, V14, P475, DOI 10.1109/TCSVT.2004.825546
   Boccignone G, 2005, IEEE T CIRC SYST VID, V15, P365, DOI 10.1109/TCSVT.2004.842603
   Bouthemy P, 1999, IEEE T CIRC SYST VID, V9, P1030, DOI 10.1109/76.795057
   Cerneková Z, 2006, IEEE T CIRC SYST VID, V16, P82, DOI 10.1109/TCSVT.2005.856896
   Comaniciu D, 2000, PROC CVPR IEEE, P142, DOI 10.1109/CVPR.2000.854761
   Fernando W. A. C., 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P299, DOI 10.1109/ICIP.1999.817121
   Fernando WAC, 2004, J ELECTRON IMAGING, V13, P362, DOI 10.1117/1.1666874
   FERNANDO WAC, 1999, P IEEE INT C IM PROC, V3, P294
   Gargi U, 2000, IEEE T CIRC SYST VID, V10, P1, DOI 10.1109/76.825852
   Harris C., 1988, P 4 ALV VIS C, V15, P10
   Huang CR, 2008, PATTERN RECOGN, V41, P3071, DOI 10.1016/j.patcog.2008.03.013
   Huang CR, 2006, INT C PATT RECOG, P53
   Joyce RA, 2006, IEEE T MULTIMEDIA, V8, P130, DOI 10.1109/TMM.2005.861285
   Ke Y, 2004, PROC CVPR IEEE, P506
   Lienhart R, 2001, PROC SPIE, V4315, P219, DOI 10.1117/12.410931
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Mikolajczyk K, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P525, DOI 10.1109/ICCV.2001.937561
   Nam J, 2005, IEEE T MULTIMEDIA, V7, P667, DOI 10.1109/TMM.2005.843362
   Ngo CW, 2003, IEEE T IMAGE PROCESS, V12, P341, DOI 10.1109/TIP.2003.809020
   Park M.-H., 2006, P SPIE VISUAL COMMUN, P569
   Pei SC, 2002, IEEE T MULTIMEDIA, V4, P309, DOI 10.1109/TMM.2002.802841
   Pei SC, 1999, IEEE T MULTIMEDIA, V1, P321, DOI 10.1109/6046.807952
   Schmid C, 1997, IEEE T PATTERN ANAL, V19, P530, DOI 10.1109/34.589215
   SHAHRARAY B, 1995, P SOC PHOTO-OPT INS, V2419, P2, DOI 10.1117/12.206348
   Shen K, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pB252
   Su CW, 2005, IEEE T MULTIMEDIA, V7, P1106, DOI 10.1109/TMM.2005.858394
   SWAIN MJ, 1991, INT J COMPUT VISION, V7, P11, DOI 10.1007/BF00130487
   Wu M, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 1, P893, DOI 10.1109/ICIP.1998.723664
   Yeo BL, 1995, IEEE T CIRC SYST VID, V5, P533, DOI 10.1109/76.475896
   ZHANG ZY, 1995, ARTIF INTELL, V78, P87, DOI 10.1016/0004-3702(95)00022-4
NR 34
TC 33
Z9 41
U1 1
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2008
VL 10
IS 6
BP 1097
EP 1108
DI 10.1109/TMM.2008.2001374
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 371WE
UT WOS:000260862600013
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Kudithipudi, D
   Petko, S
   John, EB
AF Kudithipudi, Dhireesha
   Petko, Stefan
   John, Eugene B.
TI Caches for Multimedia Workloads: Power and Energy Tradeoffs
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cache; leakage power; low power; multimedia workload characterization
ID ACCESS
AB One of the significant workloads in current generation desktop processors and mobile devices is multimedia processing. Large on-chip caches are common in modern processors, but large caches will result in increased power consumption and increased access delays. Regular data access patterns in streaming multimedia applications and video processing applications can provide high hit-rates, but due to issues associated with access time, power and energy, caches cannot be made very large. Characterizing and optimizing the memory system is conducive for designing power and performance efficient multimedia application processors. Performance tradeoffs for multimedia applications have been studied in the past, however, power and energy tradeoffs for caches for multimedia processing have not been adequately studied in the past. In this paper, we characterize multimedia applications for I-cache and D-cache power and energy using a multilevel cache hierarchy. Both dynamic and static power increase with increasing cache sizes, however, the increase in dynamic power is small. The increase in static power is significant, and becomes increasingly relevant for smaller feature sizes. There is significant static power dissipation, similar to 45%, in L1 & L2 caches at 70 ram technology sizes, emphasizing the fact that future multimedia systems must be designed by taking leakage power reduction techniques into account. The energy consumption of on-chip L2 caches is seen to be very sensitive to cache size variations. Sizes larger than 16 k for I-caches and 32 k for D-caches will not be efficient choices to maintain power and performance balance. Since multimedia applications spend significant amounts of time in integer operations, to improve the performance, we propose implementing low power full adders and hybrid multipliers in the data path, which results in 9% to 21% savings in the overall power consumption.
C1 [Kudithipudi, Dhireesha] Rochester Inst Technol, Dept Comp Engn, Rochester, NY 14623 USA.
   [John, Eugene B.] Univ Texas San Antonio, Dept Elect & Comp Engn, San Antonio, TX 78249 USA.
   [Petko, Stefan] Inst Syst Level Integrat, Livingston EH54 7EG, Scotland.
C3 Rochester Institute of Technology; University of Texas System;
   University of Texas at San Antonio (UTSA); Institute for System Level
   Integration - UK
RP Kudithipudi, D (corresponding author), Rochester Inst Technol, Dept Comp Engn, Rochester, NY 14623 USA.
EM dxkeec@rit.edu; s_petko@yahoo.com; Eugene.john@utsa.edu
RI Kudithipudi, Dhireesha/KOC-8348-2024
OI John, Eugene/0000-0001-9494-4894
CR AALMOES R, 1963, ROALTS H 263 PAGE
   Austin T, 2002, COMPUTER, V35, P59, DOI 10.1109/2.982917
   Brooks D, 2000, PROCEEDING OF THE 27TH INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE, P83, DOI [10.1145/342001.339657, 10.1109/ISCA.2000.854380]
   Cantin JasonF., Cache performance for SPEC CPU2000 benchmarks
   Cao Y, 2000, PROCEEDINGS OF THE IEEE 2000 CUSTOM INTEGRATED CIRCUITS CONFERENCE, P201, DOI 10.1109/CICC.2000.852648
   CMELIK RF, 1993, 9312 SMLI TR
   COLAGIOVANNI L, 1990, ELECTRON LETT, V26, P509, DOI 10.1049/el:19900331
   CUCCHIARA R, IEEE INT C PERF COMP, P311
   Diefendorff K, 1997, COMPUTER, V30, P43, DOI 10.1109/2.612247
   FRITTS J, 2000, P SIPS 2000 IEEE
   FRITTS J, 2000, THESIS PRINCETON U P
   Guthaus MR, 2001, WWC-4: IEEE INTERNATIONAL WORKSHOP ON WORKLOAD CHARACTERIZATION, P3, DOI 10.1109/WWC.2001.990739
   Hennessey J., 1998, COMPUTER ORG DESIGN, Vsecond
   HENNING JL, 2000, CPU PERFORMANCE NEW
   Huang M, 2000, INT SYMP MICROARCH, P202, DOI 10.1109/MICRO.2000.898071
   JOHN E, 2002, P 45 IEEE INT MIDW S
   KAIN RY, ADV COMPUTER ARCHITE
   KleinOswoski A., 2002, Computer Architecture Letters, P10
   Kudithipudi D, 2005, J LOW POWER ELECTRON, V1, P286, DOI 10.1166/jolpe.2005.046
   LEE BK, 2002, P ASAP C JUL
   LEE C, P 35 DES AUT C SAN F
   Lee CH, 1997, INT SYMP MICROARCH, P330, DOI 10.1109/MICRO.1997.645830
   LEE D, 2003, DES AUT C
   Patterson D., 2006, COMPUTER ARCHITECTUR
   PETKO S, 2002, P 3 INT WORKSH DIG C
   PETKO S, 2003, P INT SIGN PROC C DA
   Ranganathan P, 1999, CONF PROC INT SYMP C, P124, DOI [10.1145/307338.300990, 10.1109/ISCA.1999.765945]
   Reinman G., 1999, INTEGRATED CACHE TIM
   ROBINSON JA, BINARY TREE PREDICTI
   Shalem R, 1999, PR GR LAK SYMP VLSI, P380, DOI 10.1109/GLSV.1999.757461
   SHIVAKUMAR P, 20012 DEC W RES LAB
   *SIMPLESCALAR, SIMPLESCALAR LLC
   Slingerland NathanT., 2001, P 15 INT C SUPERCOMP, P204, DOI [10.1145/377792.377833, DOI 10.1145/377792.377833]
   SLINGERLAND T, 2001, P INT C SUP ICS, P204
   Stone HS., 1990, HIGH PERFORMANCE COM
   TALLA D, 2001, THESIS U TEXAS AUSTI
   WADA T, 1992, IEEE J SOLID-ST CIRC, V27, P1147, DOI 10.1109/4.148323
   Wilton SJE, 1996, IEEE J SOLID-ST CIRC, V31, P677, DOI 10.1109/4.509850
   Xu ZY, 2004, IEEE T COMPUT, V53, P20, DOI 10.1109/TC.2004.1255788
   Ye W, 2000, DES AUT CON, P340, DOI 10.1145/337292.337436
   YI JJ, HOMEPAGE CONFIGURATI
   ZHANG Y, 2003, CS200305 ARCH U
   SIMPLESCALAR ARM MOD
   FREEWARE SPARC SOLAR, V8
NR 44
TC 4
Z9 4
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2008
VL 10
IS 6
BP 1013
EP 1021
DI 10.1109/TMM.2008.2001385
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 371WE
UT WOS:000260862600006
DA 2024-07-18
ER

PT J
AU Ur Réhman, S
   Sun, J
   Liu, L
   Li, HB
AF ur Rehman, Shafiq
   Sun, Jiong
   Liu, Li
   Li, Haibo
TI Turn Your Mobile Into the Ball: Rendering Live Football Game Using
   Vibration
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dynamic content analysis; football game; mobile communication; tactile
   rendering; usability; vibrotactile coding; video coding
AB Vibration offers many potential benefits for the use of mobile phones. In this paper, we propose a new method of rendering live football game on mobile phones using vibration. A mobile phone is "synchronized" with the ball in the real field. By holding the phone, users are able to experience dynamic movements of the ball, to know attacking directions and which team is leading the attack. The usability test of our system shows that vibrotactile display is suitable for rendering live football information on mobile phones by adopting designed coding schemes with a right training process.
C1 [ur Rehman, Shafiq; Liu, Li; Li, Haibo] Umea Univ, Dept Appl Phys & Elect TFE, Digital Media Lab, S-90187 Umea, Sweden.
   [Sun, Jiong] Ericsson Res, Multimedia Technol, Stockholm, Sweden.
C3 Umea University; Ericsson
RP Ur Réhman, S (corresponding author), Umea Univ, Dept Appl Phys & Elect TFE, Digital Media Lab, S-90187 Umea, Sweden.
EM shafiq.urrehman@tfe.umu.se; jiong.sun@ericsson.com; li.liu@tfe.umu.se;
   haibo.li@tfe.umu.se
CR [Anonymous], USABILITY ENG
   Assfalg J, 2002, IEEE MULTIMEDIA, V9, P52, DOI 10.1109/93.998060
   ASSFALG J, 2002, P IEEE INT C MULT EX, V1, P26
   ASSFALG J, 2003, COMPUT VIS IMAGE UND, V92
   Chang A, 2002, P 4 C DES INT SYST P, P312, DOI DOI 10.1145/778712.778755
   CHOI S, 1997, P INT C IM AN PROC
   Coren S., 2004, Sensation and Perception
   CRAIG JC, 1972, PERCEPT PSYCHOPHYS, V11, P150, DOI 10.3758/BF03210362
   Devore J.L., 1999, PROBABILITY STAT ENG, V5th
   DOREN CV, 1990, J ACOUST SOC AM, P2655
   DUAN LY, 2003, P 11 ACM INT C MULT
   Ekin A, 2003, IEEE T IMAGE PROCESS, V12, P796, DOI 10.1109/TIP.2003.812758
   Faria G, 2006, P IEEE, V94, P194, DOI 10.1109/JPROC.2005.861011
   GELDARD F, 1977, J INVEST DERMATOL, P83
   GOFF D, 1968, J EXP PSYCHOL, P294
   Jones CE, 2001, WIREL NETW, V7, P343, DOI 10.1023/A:1016627727877
   Kaaresoja T, 2005, World Haptics Conference: First Joint Eurohaptics Conference and Symposium on Haptic Interfaces for Virutual Environment and Teleoperator Systems, Proceedings, P471
   KACZMAREK KA, 1991, IEEE T BIO-MED ENG, V38, P1, DOI 10.1109/10.68204
   KARLSSON J, 2005, P 14 INT C COMP COMM
   Luk J., 2006, P SIGCHI C HUM FACT, P171, DOI DOI 10.1145/1124772.1124800
   OAKLEY SO, 2003, P EUR C INT TEL VIEW, P41
   Sherrick C.E., 1986, HDB PERCEPTION HUMAN
   TOVINKERE V, 2001, P IEEE INT C MULT EX, P833
   VERRILLO RT, 1965, J ACOUST SOC AM, V37, P843, DOI 10.1121/1.1909458
   VERRILLO RT, 1962, J ACOUST SOC AM, V34, P1768, DOI 10.1121/1.1909124
   VONERP J, 2002, P EUR 2002, P18
   YAMAUCHI K, 2006, P 6 IEEE INT C COMP
   YOW D, 1995, P AS C COMP VIS
NR 28
TC 18
Z9 22
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2008
VL 10
IS 6
BP 1022
EP 1033
DI 10.1109/TMM.2008.2001352
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 371WE
UT WOS:000260862600007
DA 2024-07-18
ER

PT J
AU Chakareski, J
   Frossard, P
AF Chakareski, Jacob
   Frossard, Pascal
TI Distributed collaboration for enhanced sender-driven video streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article; Proceedings Paper
CT IEEE International Conference on Multimedia and Expo (ICME)
CY JUL 06-08, 2005
CL Toronto, CANADA
SP IEEE
DE channel coding; distributed collaboration; Internet; packet scheduling;
   source prunning; optimal control; packet partitioning; protocols; rate
   control; rate-distortion optimization; sender-driven transmission; video
   coding; video streaming
ID BIT ALLOCATION
AB We propose a sender-driven system for adaptive streaming from multiple servers to a single receiver over separate network paths. The servers employ information in receiver feedbacks to estimate the available bandwidth on the paths and then compute appropriate transmission schedules for streaming media packets to the receiver based on the bandwidth estimates. An optimization framework is proposed that enables the senders to compute their transmission schedules in a distributed way, and yet to dynamically coordinate them over time such that the resulting video quality at the receiver is maximized. To reduce the computational complexity of the optimization framework an alternative technique based on packet classification is proposed. The substantial reduction in online complexity due to the resulting packet partitioning makes the technique suitable for practical implementations of adaptive and efficient distributed streaming systems. Simulations with internet network traces demonstrate that the proposed solution adapts effectively to bandwidth variations and packet loss. They show that the proposed streaming framework provides superior performance over a conventional distortion-agnostic scheme that performs proportional packet scheduling on the network paths according to their respective bandwidth values.
C1 [Frossard, Pascal] Ecole Polytech Fed Lausanne, Signal Proc Inst, CH-1015 Lausanne, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne
EM pascal.frossard@epfl.ch
RI Frossard, Pascal/AAF-2268-2019
CR [Anonymous], MSRTR200135
   [Anonymous], 1992, Data networks
   [Anonymous], 2001, Probability, Random Variables and Stochastic Processes
   Apostolopoulos J, 2002, IEEE INFOCOM SER, P1736, DOI 10.1109/INFCOM.2002.1019427
   APOSTOLOPOULOS J, 2002, P INT C IM PROC ROCH, V2, P189
   Byers JW, 1999, IEEE INFOCOM SER, P275, DOI 10.1109/INFCOM.1999.749293
   Chakareski J, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P645
   Chakareski J, 2003, IEEE DATA COMPR CONF, P203
   CHAKARESKI J, 2005, P WORKSH MULT SIGN P
   De Vleeschouwer C, 2007, IEEE T MULTIMEDIA, V9, P348, DOI 10.1109/TMM.2006.886283
   Floyd S, 2000, ACM SIGCOMM COMP COM, V30, P43, DOI 10.1145/347057.347397
   KIM J, 2003, P IEEE INT C MULT EX, V2, P653
   Majumdar A, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P177, DOI 10.1109/ICIP.2002.1038934
   Markopoulou AP, 2003, IEEE ACM T NETWORK, V11, P747, DOI 10.1109/TNET.2003.818179
   Nguyen T., 2002, P INT PACK VID WORKS
   Nguyen TP, 2002, PROC SPIE, V4673, P186
   Padhye J., 1999, Performance Evaluation Review, V27, P220, DOI 10.1145/301464.301581
   Ramchandran K, 1993, IEEE T IMAGE PROCESS, V2, P160, DOI 10.1109/83.217221
   Ribeiro V., 2003, P 4 PASS ACT MEAS WO
   Savage S, 1999, COMP COMM R, V29, P289, DOI 10.1145/316194.316233
   Schuster GM, 1999, IEEE T MULTIMEDIA, V1, P3, DOI 10.1109/6046.748167
   SHOHAM Y, 1988, IEEE T ACOUST SPEECH, V36, P1445, DOI 10.1109/29.90373
NR 22
TC 26
Z9 28
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2008
VL 10
IS 5
BP 858
EP 870
DI 10.1109/TMM.2008.921846
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA 334LR
UT WOS:000258223800017
OA Green Published, Green Submitted
DA 2024-07-18
ER

PT J
AU Cai, R
   Lu, L
   Hanjalic, A
AF Cai, Rui
   Lu, Lie
   Hanjalic, Alan
TI Co-clustering for auditory scene categorization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE audio content analysis; auditory scene categorization; co-clustering;
   local feature grouping trends
ID CONTEXT; ALGORITHMS
AB Auditory scenes are temporal audio segments with coherent semantic content. Automatically classifying and grouping auditory scenes with similar semantics into categories is beneficial for many multimedia applications, such as semantic event detection and indexing. For such semantic categorization, auditory scenes are first characterized with either low-level acoustic features or some mid-level representations like audio effects, and then supervised classifiers or unsupervised clustering algorithms are employed to group scene segments into various semantic categories. In this paper, we focus on the problem of automatically categorizing audio scenes in unsupervised manner. To achieve more reasonable clustering results, we introduce the co-clustering scheme to exploit potential grouping trends among different dimensions of feature spaces (either low-level or mid-level feature spaces), and provide more accurate similarity measure for comparing auditory scenes. Moreover, we also extend the co-clustering scheme with a strategy based on the Bayesian information criterion (BIC) to automatically estimate the numbers of clusters. Evaluation performed on 272 auditory scenes extracted from 12-h audio data shows very encouraging categorization results. Co-clustering achieved a better performance compared to some traditional one-way clustering algorithms, both based on the low-level acoustic features and on the mid-level audio effect representations. Finally, we present our vision regarding the applicability of this approach on general multimedia data, and also show some preliminary results on content-based image clustering.
C1 [Cai, Rui; Lu, Lie] Microsoft Res Asia, Beijing 100080, Peoples R China.
   [Hanjalic, Alan] Delft Univ Technol, Dept Mediamat, NL-2628 CD Delft, Netherlands.
C3 Microsoft; Microsoft Research Asia; Delft University of Technology
RP Cai, R (corresponding author), Microsoft Res Asia, Beijing 100080, Peoples R China.
EM ruicai@microsoft.com; llu@microsoft.com; a.hanjalic@microsoft.com
RI Cai, Rui/GRO-1869-2022
OI Hanjalic, Alan/0000-0002-5771-2549
CR [Anonymous], 1996, COLUMBIA OBJECT IMAG
   Cai R, 2006, IEEE T AUDIO SPEECH, V14, P1026, DOI 10.1109/TSA.2005.857575
   Cai R, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL III, PROCEEDINGS, P37
   Cheng W.-H., 2003, P 5 ACM SIGMM INT WO, P109, DOI DOI 10.1145/973264.973282
   Chu WT, 2005, 11TH INTERNATIONAL MULTIMEDIA MODELLING CONFERENCE, PROCEEDINGS, P38
   Clarkson B, 1999, INT CONF ACOUST SPEE, P3037, DOI 10.1109/ICASSP.1999.757481
   Dhillon IS, 2003, THIRD IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P517, DOI 10.1109/ICDM.2003.1250966
   Dhillon IS, 2001, P 7 ACM SIGKDD INT C, P269, DOI DOI 10.1145/502512.502550
   Dhillon IS, 2003, P 9 ACM SIGKDD INT C, P89, DOI DOI 10.1145/956750.956764
   El-Yaniv R., 2001, ECML, P121
   Ellis D. R., 2004, Proceedings of a Workshop on Equine Recurrent Laryngeal Neuropathy, Stratford-upon-Avon, UK, 7-10 September, 2003, P39, DOI 10.1145/1026653.1026659
   Eronen A, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL V, PROCEEDINGS, P529
   Eronen AJ, 2006, IEEE T AUDIO SPEECH, V14, P321, DOI 10.1109/TSA.2005.854103
   Friedman N., 1998, PROC 14 C UNCERTAINT, P129
   Hanisch Daniel, 2002, Bioinformatics, V18 Suppl 1, pS145
   HANJALIC A, 2005, P IEE EWIMT 05 LOND
   HARTIGAN JA, 1972, J AM STAT ASSOC, V67, P123, DOI 10.2307/2284710
   Hyvärinen A, 2000, NEURAL NETWORKS, V13, P411, DOI 10.1016/S0893-6080(00)00026-5
   Jain A. K., 1988, Algorithms for Clustering Data, P446
   KASS RE, 1995, J AM STAT ASSOC, V90, P773, DOI 10.1080/01621459.1995.10476572
   Kern N, 2007, PERS UBIQUIT COMPUT, V11, P251, DOI 10.1007/s00779-006-0086-3
   Ma Ling., 2003, PROC EUROSPEECH 2003, P2237
   Madeira SC, 2004, IEEE ACM T COMPUT BI, V1, P24, DOI 10.1109/TCBB.2004.2
   Ng AY, 2002, ADV NEUR IN, V14, P849
   Pelleg D., 2000, P 17 INT C MACH LEAR, DOI DOI 10.1007/3-540-44491-2_3
   Peltonen V, 2002, INT CONF ACOUST SPEE, P1941
   Roy DK, 2002, COMPUT SPEECH LANG, V16, P353, DOI 10.1016/S0885-2308(02)00024-4
   Shlens J., 2005, ARXIV
   Sundaram H, 2000, INT CONF ACOUST SPEE, P2441, DOI 10.1109/ICASSP.2000.859335
   Sural S., 2002, P IEEE INT C IM PROC, DOI DOI 10.1109/ICIP.2002.1040019
   Xu M, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL II, PROCEEDINGS, P281
NR 31
TC 20
Z9 21
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2008
VL 10
IS 4
BP 596
EP 606
DI 10.1109/TMM.2008.921739
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 342EK
UT WOS:000258767200005
DA 2024-07-18
ER

PT J
AU Hsu, CH
   Hefeeda, M
AF Hsu, Cheng-Hsin
   Hefeeda, Mohamed
TI Partitioning of multiple fine-grained scalable video sequences
   concurrently streamed to heterogeneous clients
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE fine-grained scalable coding; multimedia communication; quality
   optimization; video streaming
AB Fine-grained scalable (FGS) coding of video streams has been proposed in the literature to accommodate client heterogeneity. FGS streams are composed of two layers: a base layer, which provides basic quality, and a single enhancement layer that adds incremental quality refinements proportional to number of bits received. The base layer uses nonscalable coding which is more efficient in terms of compression ratio than scalable coding used in the enhancement layer. Thus for coding efficiency larger base layers are desired. Larger base layers, however, disqualify more clients from getting the stream. In this paper, we experimentally analyze this coding efficiency gap using diverse video sequences. For FGS sequences, we show that this gap is a non-increasing function of the base layer rate. We then formulate an optimization problem to determine the base layer rate of a single sequence to maximize the average quality for a given client bandwidth distribution. We design an optimal and efficient algorithm (called FGSOPT) to solve this problem. We extend our formulation to the multiple-sequence case, in which a bandwidth-limited server concurrently streams multiple FGS sequences to diverse sets of clients. We prove that this problem is NP-Complete. We design a branch-and-bound algorithm (called MFGSOPT) to compute the optimal solution. MFGSOPT runs fast for many typical cases because it intelligently cuts the search space. In the worst case, however, it has exponential time complexity. We also propose a heuristic algorithm (called MFGS) to solve the multiple-sequence problem. We experimentally show that MFGS produces near-optimal results and it scales to large problems: it terminates in less than 0.5 s for problems with more than 30 sequences. Therefore, MFGS can be used in dynamic systems, where the server periodically adjusts the structure of FGS streams to suit current client distributions.
C1 [Hsu, Cheng-Hsin; Hefeeda, Mohamed] Simon Fraser Univ, Sch Comp Sci, Network Syst Lab, Surrey, BC V3T 0A3, Canada.
C3 Simon Fraser University
RP Hsu, CH (corresponding author), Simon Fraser Univ, Sch Comp Sci, Network Syst Lab, Surrey, BC V3T 0A3, Canada.
EM cha16@cs.sfu.ca; mhefeeda@cs.sfu.ca
FU Natural Sciences and Engineering Research Council (NSERC) of Canada
   [313083]; RTI [344619]
FX This work was supported in part by the Natural Sciences and Engineering
   Research Council (NSERC) of Canada under Discovery Grant #313083 and RTI
   Grant #344619. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Madjid Merabti.
CR Dai M, 2006, IEEE T MULTIMEDIA, V8, P1135, DOI 10.1109/TMM.2006.884626
   DECUETOS P, 2001, P INT PACK VID WORKS
   HSU C, 2007, P ACM INT WORKSH NET, P63
   HSU C, ACM T MULTI IN PRESS
   HSU C, 2007, PARTITIONING MULTIPL
   Hsu CH, 2007, INT WORKSH QUAL SERV, P182, DOI 10.1109/IWQOS.2007.376565
   *JOINT VID TEAM, 2007, JOINT SCAL VID MOD R
   *JOINT VID TEAM, 2005, 1449610 AVC ITU T
   Kellerer H., 2004, Knapsack Problems. Springer Nature Book Archives Millennium, P317
   Kim T, 2005, IEEE T MULTIMEDIA, V7, P1123, DOI 10.1109/TMM.2005.858376
   Kim T., 2001, P 11 INT WORKSHOP NE, P63
   Li WP, 2001, IEEE T CIRC SYST VID, V11, P301, DOI 10.1109/76.911157
   Liu JC, 2003, IEEE INFOCOM SER, P630
   Radha H, 2004, EURASIP J APPL SIG P, V2004, P265, DOI 10.1155/S111086570430805X
   Radha HM, 2001, IEEE T MULTIMEDIA, V3, P53, DOI 10.1109/6046.966110
   Radulovic I, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P1915, DOI 10.1109/ICME.2004.1394634
   REICHER J, 2006, JVTU202
   SCHWARZ H, 2006, P EUR S MOB MED DEL
   VANDERSCHAAR M, 2002, IEEE T CIRCUITS SYST, V12, P32
   WIEGAND T, 2006, CHINA TECH REP OCT, P8
   Wu F, 2001, IEEE T CIRC SYST VID, V11, P332, DOI 10.1109/76.911159
   Yang YR, 2000, 2000 INTERNATIONAL CONFERENCE ON NETWORK PROTOCOLS, PROCEEDINGS, P129, DOI 10.1109/ICNP.2000.896298
NR 22
TC 6
Z9 8
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2008
VL 10
IS 3
BP 457
EP 469
DI 10.1109/TMM.2008.917365
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 342EJ
UT WOS:000258767100014
OA Green Submitted
DA 2024-07-18
ER

PT J
AU van der Schaar, M
   Turaga, DS
   Sood, R
AF van der Schaar, Mihaela
   Turaga, Deepak S.
   Sood, Ritesh
TI Stochastic optimization for content sharing in P2P systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE content sharing; optimal upload policies; resource contributions in P2P
   networks
AB Available resources in Peer-to-Peer (P2P) systems depend strongly on resource contributions made by individual peers. Empirical data shows that in the absence of incentives, a majority of the participating peers do not contribute resources. Modeling interactions between individual peers is often difficult as the number of peers in the system can be very large, and the relationships among them can be very complex. In this paper, we propose a new solution for P2P systems, where peers upload and download content to and from the contributing peers based on agreed-upon/determined sharing rates. We propose a P2P solution that deters free-riders by imposing constraints on participating peers; specifically, a peer is allowed access to new content only as long as its own content contribution exceeds an adaptively set threshold. The constraints are enforced either by a central authority (e.g., a tracker) or by a decentralized coalition of peers in a swarm, social network, etc. We derive optimal upload policies for the peers given their estimated future download requirements and their previous contribution (credit) to the other peers. Our results show considerable improvement in the cost-benefit tradeoff for peers that deploy such an optimal policy as compared to heuristic upload policies. We also propose mechanisms based on which the coalition of peers can provide incentives or penalties to participating peers to adjust their policies such that the availability of content and/or number of peers contributing content is maximized.
C1 [van der Schaar, Mihaela] Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90024 USA.
   [Turaga, Deepak S.] IBM Corp, TJ Watson Res Ctr, Hawthorne, NY 10532 USA.
   [Sood, Ritesh] Univ Calif Davis, Davis, CA 95616 USA.
C3 University of California System; University of California Los Angeles;
   International Business Machines (IBM); University of California System;
   University of California Davis
RP van der Schaar, M (corresponding author), Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90024 USA.
EM mihaela@ee.ucla.edu
CR Adar E., 2000, 1 MONDAY, V5
   ALI A, 2006, WORKSH REC ADV PEER
   Androutsellis-Theotokis S, 2004, ACM COMPUT SURV, V36, P335, DOI 10.1145/1041680.1041681
   [Anonymous], 2007, 4 S NETWORKED SYSTEM
   [Anonymous], 1976, Dynamic Programming and Stochastic Optimal Control
   [Anonymous], P SIGCOMM
   Buragohain C, 2003, THIRD INTERNATIONAL CONFERENCE ON PEER-TO-PEER COMPUTING (P2P2003), PROCEEDINGS, P48, DOI 10.1109/PTP.2003.1231503
   Byers J., 2002, P SIGCOMM
   Byers J.W., 1998, P SIGCOMM
   CASTRO M, 2003, P SOSP OCT
   COHEN B, 2003, P P2P EC WORKSH BERK
   DAMIANI E, 2002, P ACM C COMP COMM SE
   FRY CP, 2006, REALLY TRULY TRACKER
   GKANTSIDIS C, 2005, P IEEE INFOCOM 2005
   GOLLE P, 2001, P 2 INT WORKSH EL CO
   Hardin G., 1968, SCIENCE
   Heyman D.P., 2004, STOCHASTIC MODELS OP, V2
   KUBIATOWICZ J, 2000, P ASPLOS 2000
   Lai K., 2003, P WORKSH EC PEER TO
   LEGOUT A, 2007, ACM SIGMETRICS   JUN
   LI J, 2004, MSRTR2004100
   LOVEJOY WS, 1990, MANAGE SCI, V36, P724, DOI 10.1287/mnsc.36.6.724
   PADMANABHAN VN, 2002, MSRTR200237
   QUI D, 2004, P C APPL TECHN ARCH
   SOOD R, 2004, P SPIE
   Sripanidkulchai K., 2004, P ACM SIGCOMM
   Wang Y., 2004, P WEB INT
   XU D, 2002, P DISTR COMP SYST
NR 28
TC 11
Z9 13
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2008
VL 10
IS 1
BP 132
EP 144
DI 10.1109/TMM.2007.911288
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 245QZ
UT WOS:000251952200013
DA 2024-07-18
ER

PT J
AU Hei, XJ
   Liang, C
   Liang, J
   Liu, Y
   Ross, KW
AF Hei, Xiaojun
   Liang, Chao
   Liang, Jian
   Liu, Yong
   Ross, Keith W.
TI A measurement study of a large-scale P2P IPTV system
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE IPTV; measurement; peer-to-peer streaming
AB An emerging Internet application, IPTV, has the potential to flood Internet access and backbone ISPs with massive amounts of new traffic. Although many architectures are possible for IPTV video distribution, several mesh-pull P2P architectures have been successfully deployed on the Internet. In order to gain insights into mesh-pull P2P IPTV systems and the traffic loads they place on ISPs, we have undertaken an in-depth measurement study of one of the most popular IPTV systems, namely, PPLive. We have developed a dedicated PPLive crawler, which enables us to study the global characteristics of the mesh-pull PPLive system. We have also collected extensive packet traces for various different measurement scenarios, including both campus access networks and residential access networks. The measurement results obtained through these platforms bring important insights into P2P IPTV systems. Specifically, our results show the following. 1) P2P IPTV users have the similar viewing behaviors as regular TV users. 2) During its session, a peer exchanges video data dynamically with a large number of peers. 3) A small set of super peers act as video proxy and contribute significantly to video data uploading. 4) Users in the measured P2P IPTV system still suffer from long start-up delays and playback lags, ranging from several seconds to a couple of minutes. Insights obtained in this study will be valuable for the development and deployment of future P2P IPTV systems.
C1 Polytech Univ, Dept Comp & Informat Sci, Brooklyn, NY 11201 USA.
   Polytech Univ, Dept Elect & Comp Engn, Brooklyn, NY 11201 USA.
C3 New York University; New York University
RP Hei, XJ (corresponding author), Polytech Univ, Dept Comp & Informat Sci, 6 Metrotech Ctr, Brooklyn, NY 11201 USA.
EM heixj@poly.edu; jliang@cis.poly.edu; cliang@photon.poly.edu;
   yongliu@poly.edu; ross@poly.edu
RI Liang, Chao/B-9709-2012; Hei, Xiaojun/E-7970-2012
OI Hei, Xiaojun/0000-0002-6766-4923
CR ALI S, 2006, P 1 WORKSH REC ADV P
   [Anonymous], P IEEE INFOCOM APR
   Chu YH, 2000, PERF E R SI, V28, P1, DOI 10.1145/345063.339337
   COHEN B, 2003, P P2P EC JUN
   Dana C., 2005, P IEEE MMSP OCT
   DEERING SE, 1990, ACM T COMPUT SYST, V8, P85, DOI 10.1145/78952.78953
   GUHA S, 2006, P IPTPS FEB
   GUMMADI KP, 2003, P 19 ACM S OP SYST P, P314
   Hei X., 2006, P IPTV WORKSH CONJ W
   HEI X, 2006, MEASUREMENT STUDY LA
   Izal M, 2004, LECT NOTES COMPUT SC, V3015, P1
   Kontothanassis L, 2004, P IEEE, V92, P1408, DOI 10.1109/JPROC.2004.832956
   KUMAR R, 2006, P IEEE INFOCOM MAY
   Liang J, 2006, COMPUT NETW, V50, P842, DOI 10.1016/j.comnet.2005.07.014
   LIANG J, 2006, P IEEE INFOCOM APR
   LIAO X, 2006, P IEEE INFOCOM APR
   LIU J, 2007, P IEEE
   MAGHAREI N, 2006, P NOSSDAV MAY
   PAI V, 2005, P IPTPS FEB
   PIANESE F, 2006, P IEEE GLOB INT
   POUWELSE J, 2005, P IPTPS FEB
   Saroiu S, 2003, MULTIMEDIA SYST, V9, P170, DOI 10.1007/s00530-003-0088-1
   SILVERSTON T, 2006, P NOSSDAV JUN
   Sripanidkulchai K., 2004, Proc. ACM Internet Measurement Conference, P41
   STUTZBACH D, 2005, P ACM IMC OCT
   TEWARI S, 2006, P IEEE NIME WORKSH J
   VLAVIANOS A, 2006, P IEEE GLOB INT APR
   VU L, 2006, P QSHINE AUG
   Zhang M., 2005, Proc. ACM Workshop on Advances in Peer-to-Peer Multimedia Streaming (P2PMMS), P21
   ZHANG X, 2005, P IEEE MMSP OCT
   Zhang XY, 2005, IEEE INFOCOM SER, P2102
NR 31
TC 455
Z9 541
U1 0
U2 58
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2007
VL 9
IS 8
BP 1672
EP 1687
DI 10.1109/TMM.2007.907451
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 233SF
UT WOS:000251109900012
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yi, XQ
   Ling, N
AF Yi, Xiaoquan
   Ling, Nam
TI Improved normalized partial distortion search with dual-halfway-stop for
   rapid block motion estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article; Proceedings Paper
CT 30th IEEE International Conference on Acoustics, Speech, and Signal
   Processing
CY MAR 19-23, 2005
CL Philadelphia, PA
SP IEEE
DE block matching; dynamic thresholding; halfway stop; motion estimation;
   partial distortion search; video coding; visual communications
ID ADAPTIVE PIXEL-DECIMATION; MATCHING ALGORITHMS; PATTERN
AB Motion estimation is a critical yet computationally intensive task for video encoding. In this paper, we present an enhancement over a normalized partial distortion search (NPDS) algorithm to further reduce block matching motion estimation complexity while retaining video fidelity. The novelty of our algorithm is that, in addition to the halfway-stop technique in NPDS, a dual-halfway-stop (DHS) method, which is based on a dynamic threshold, is proposed, so that block matching is not performed against all matching candidates. An adaptive search range (ASR) mechanism based on inter block distortion further constrains the searching process. Simulation results show that the proposed algorithm has a remarkable computational speedup when compared to that of full search and NPDS algorithms. Particularly, it requires less computation by 92-99% and encounters an average of only 0.08 dB PSNR video degradation when compared to that of full search. The speedup is also very significant when compared to that of fast motion estimation algorithms. This paper describes our work that led to our Joint Video Team (JVT) adopted contribution (included in software JM 10.1 onwards) as well as later enhancements, collectively known as Simplified and Unified Multi-Hexagon Search (SUMH), a simplified fast motion estimation.
C1 Santa Clara Univ, Dept Comp Engn, Santa Clara, CA 95053 USA.
C3 Santa Clara University
RP Yi, XQ (corresponding author), Santa Clara Univ, Dept Comp Engn, Santa Clara, CA 95053 USA.
EM xyi@scu.edu; nling@scu.edu
CR [Anonymous], 2010, JM REFERENCE SOFTWAR
   [Anonymous], 1981, P NTC81 NEW ORL LA U
   BJONTEGAARD G, 2002, JVT C028 JOINT VID T
   Chalidabhongse J, 1997, IEEE T CIRC SYST VID, V7, P477, DOI 10.1109/76.585927
   Chan YL, 1996, IEEE T CIRC SYST VID, V6, P113, DOI 10.1109/76.486426
   CHEN Z, 2002, JVT F017R JOINT VIDE
   Cheng FH, 1999, IEEE T CIRC SYST VID, V9, P977, DOI 10.1109/76.795049
   Cheung CH, 2002, IEEE T CIRC SYST VID, V12, P1168, DOI 10.1109/TCSVT.2002.806815
   Cheung CH, 2003, IEEE T CIRC SYST VID, V13, P100, DOI 10.1109/TCSVT.2002.808091
   Cheung CK, 2000, IEEE T CIRC SYST VID, V10, P417, DOI 10.1109/76.836286
   Choi KT, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL II, P497
   ECKART S, 1995, P SOC PHOTO-OPT INS, V2419, P100, DOI 10.1117/12.206349
   HSIEH CH, 1990, ELECTRON LETT, V26, P276, DOI 10.1049/el:19900183
   JAIN JR, 1981, IEEE T COMMUN, V29, P1799, DOI 10.1109/TCOM.1981.1094950
   Kim DW, 1998, SIGNAL PROCESS-IMAGE, V13, P161, DOI 10.1016/S0923-5965(98)80013-1
   Lam CW, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL 3, PROCEEDINGS, P729
   LEE JK, 1993, J MICROBIOL BIOTECHN, V3, P1
   Lengwehasatit K, 2001, IEEE T CIRC SYST VID, V11, P139, DOI 10.1109/76.905981
   LI RX, 1994, IEEE T CIRC SYST VID, V4, P438, DOI 10.1109/76.313138
   Liu B, 1993, IEEE T CIRC SYST VID, V3, P148, DOI 10.1109/76.212720
   Liu LK, 1996, IEEE T CIRC SYST VID, V6, P419, DOI 10.1109/76.510936
   Lu JH, 1997, IEEE T CIRC SYST VID, V7, P429, DOI 10.1109/76.564122
   Luo LJ, 1997, IEEE T CONSUM ELECTR, V43, P56, DOI 10.1109/30.580385
   Montrucchio B, 2005, IEEE T CIRC SYST VID, V15, P210, DOI 10.1109/TCSVT.2004.841689
   Ng ACK, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 3, P605, DOI 10.1109/ICIP.1998.727336
   Nie Y, 2002, IEEE T IMAGE PROCESS, V11, P1442, DOI 10.1109/TIP.2002.806251
   Po LM, 1996, IEEE T CIRC SYST VID, V6, P313, DOI 10.1109/76.499840
   Tham JY, 1998, IEEE T CIRC SYST VID, V8, P369, DOI 10.1109/76.709403
   Tourapis AM, 2002, IEEE T CIRC SYST VID, V12, P934, DOI 10.1109/TCSVT.2002.804894
   Tsai JC, 1998, SIGNAL PROCESS-IMAGE, V13, P119, DOI 10.1016/S0923-5965(97)00052-0
   Wang YK, 2000, IEEE T CIRC SYST VID, V10, P1006, DOI 10.1109/76.867940
   Xu JB, 1999, IEEE T CIRC SYST VID, V9, P1025, DOI 10.1109/76.795056
   Yi X, 2005, PROC SPIE, V5685, P995, DOI 10.1117/12.588128
   YI X, 2005, JVT P021 JOINT VID T
   ZHANG J, 2006, JVT T046 JOINT VID T
   Zhu C, 2002, IEEE T CIRC SYST VID, V12, P349, DOI 10.1109/TCSVT.2002.1003474
   Zhu C, 2004, IEEE T CIRC SYST VID, V14, P1210, DOI 10.1109/TCSVT.2004.833166
   Zhu S, 2000, IEEE T IMAGE PROCESS, V9, P287, DOI 10.1109/83.821744
   MOTION ESTIMATION DE
NR 39
TC 22
Z9 25
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2007
VL 9
IS 5
BP 995
EP 1003
DI 10.1109/TMM.2007.898930
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA 193ZX
UT WOS:000248314800009
DA 2024-07-18
ER

PT J
AU Fox, NA
   Gross, R
   Cohn, JF
   Reilly, RB
AF Fox, Niall A.
   Gross, Ralph
   Cohn, Jeffrey F.
   Reilly, Richard B.
TI Robust biometric person identification using automatic classifier fusion
   of speech, mouth, and face experts
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE biometric fusion; expert reliability; hidden Markov models; image
   information loss; mouth features; multi-modal; person recognition;
   robustness; tri-expert
ID RECOGNITION; INTEGRATION; MODELS
AB Information about person identity is multimodal. Yet, most person-recognition systems limit themselves to only a single modality, such as facial appearance. With a view to exploiting the complementary nature of different modes of information and increasing pattern recognition robustness to test signal degradation, we developed a multiple expert biometric person identification system that combines information from three experts: audio, visual speech, and face. The system uses multimodal fusion in an automatic unsupervised manner, adapting to the local performance (at the transaction level) and output reliability of each of the three experts. The expert weightings are chosen automatically such that the reliability measure of the combined scores is maximized. To test system robustness to train/test mismatch, we used a broad range of acoustic babble noise and JPEG compression to degrade the audio and visual signals, respectively. Identification experiments were carried out on a 248-subject subset of the XM2VTS database. The multimodal expert system outperformed each of the single experts in all comparisons. At severe audio and visual mismatch levels tested, the audio, mouth, face, and tri-expert fusion accuracies were 16.1%, 48%, 75%, and 89.9%, respectively, representing a relative improvement of 19.9% over the best performing expert.
C1 Univ Coll Dublin, Sch Elect Elect & Mech Engn, MMSP Lab, Dublin 4, Ireland.
   Carnegie Mellon Univ, Data Privacy Lab SCS, Inst Robot, Pittsburgh, PA 15213 USA.
   Univ Pittsburgh, Pittsburgh, PA 15260 USA.
C3 University College Dublin; Carnegie Mellon University; Pennsylvania
   Commonwealth System of Higher Education (PCSHE); University of
   Pittsburgh
RP Fox, NA (corresponding author), Univ Coll Dublin, Sch Elect Elect & Mech Engn, MMSP Lab, Dublin 4, Ireland.
EM niall.fox@ee.ucd.ie; rgross@cs.cmu.edu; jeffcohn@pitt.edu;
   richard.reilly@ucd.ie
RI Reilly, Richard/N-1080-2019; Reilly, Richard B/F-7034-2011
OI Reilly, Richard/0000-0001-8578-1245; Reilly, Richard
   B/0000-0001-8578-1245
CR [Anonymous], 2001, 3 WORK EMPIRE EVAL M
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Ben-Yacoub S, 1999, IEEE T NEURAL NETWOR, V10, P1065, DOI 10.1109/72.788647
   Benabdelkader C, 2005, IMAGE VISION COMPUT, V23, P339, DOI 10.1016/j.imavis.2004.09.004
   Blackburn D.M., 2001, Facial Recognition Vendor Test 2000, DOI DOI 10.21236/ADA415962
   BRUNELLI R, 1995, IEEE T PATTERN ANAL, V17, P955, DOI 10.1109/34.464560
   Cardinaux F, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P825, DOI 10.1109/AFGR.2004.1301636
   Chibelushi CC, 2002, IEEE T MULTIMEDIA, V4, P23, DOI 10.1109/6046.985551
   Chibelushi CC, 1999, IEEE T SYST MAN CY B, V29, P902, DOI 10.1109/3477.809043
   Dieckmann U, 1997, PATTERN RECOGN LETT, V18, P827, DOI 10.1016/S0167-8655(97)00063-9
   Duda R. O., 1973, PATTERN CLASSIFICATI, V3
   Dupont S, 2000, IEEE T MULTIMEDIA, V2, P141, DOI 10.1109/6046.865479
   Fierrez-Aguilar J, 2006, LECT NOTES COMPUT SC, V3832, P213
   Fox N, 2003, LECT NOTES COMPUT SC, V2688, P743
   Fox N., 2003, ACM Special Interest Group on Multimedia (SIGMM) Multimedia Biometrics Methods and Applications, P25
   Fox NA, 2004, IEEE SYS MAN CYBERN, P580
   Frischholz RW, 2000, COMPUTER, V33, P64, DOI 10.1109/2.820041
   Heckmann M, 2002, EURASIP J APPL SIG P, V2002, P1260, DOI 10.1155/S1110865702206150
   JAIN A, 2005, PATTERN RECOGNIT
   Jain AK, 2002, IEEE IMAGE PROC, P57
   Kanak A, 2003, INT CONF ACOUST SPEE, P377
   Kittler J, 1998, IEEE T PATTERN ANAL, V20, P226, DOI 10.1109/34.667881
   Lanitis A, 1997, IEEE T PATTERN ANAL, V19, P743, DOI 10.1109/34.598231
   Lawrence BS, 1997, ORGAN SCI, V8, P1, DOI 10.1287/orsc.8.1.1
   LI Y, 2000, P IEEE INT C AUT FAC, P300
   Lucey S, 2005, IEEE T MULTIMEDIA, V7, P495, DOI 10.1109/TMM.2005.846777
   Matthews I, 2002, IEEE T PATTERN ANAL, V24, P198, DOI 10.1109/34.982900
   Matthews I., 2001, P IEEE INT C MULT EX, P825, DOI [DOI 10.1109/ICME.2001.1237849, 10.1109/ICME.2001.1237849]
   Messer K., 1999, 2 INT C AUD VID BAS, V964, P965
   Nefian AV, 2003, LECT NOTES COMPUT SC, V2688, P761
   Netravali A.N., 1988, DIGITAL PICTURES
   Penev PS, 1996, NETWORK-COMP NEURAL, V7, P477, DOI 10.1088/0954-898X/7/3/002
   PHILLIPS PJ, 2003, P IEEE INT WORKSH AN, P44
   Potamianos G, 2003, P IEEE, V91, P1306, DOI 10.1109/JPROC.2003.817150
   Potamianos G, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 3, P173, DOI 10.1109/ICIP.1998.999008
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   REYNOLDS DA, 1995, IEEE T SPEECH AUDI P, V3, P72, DOI 10.1109/89.365379
   Sanderson C, 2006, PATTERN RECOGN, V39, P288, DOI 10.1016/j.patcog.2005.07.001
   Sandhu S, 2004, SIGHT SOUND, V14, P5
   SIROVICH L, 1987, J OPT SOC AM A, V4, P519, DOI 10.1364/JOSAA.4.000519
   Tamura S, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL I, PROCEEDINGS, P857
   TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71
   Vapnik V., 1999, NATURE STAT LEARNING
   Varga A.P., 1992, NOISEX 92 STUDY EFFE
   Wark T, 2001, DIGIT SIGNAL PROCESS, V11, P169, DOI 10.1006/dspr.2001.0397
   Wark T, 1998, INT C PATT RECOG, P123, DOI 10.1109/ICPR.1998.711095
   Wark TJ, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 1, P812, DOI 10.1109/MMCS.1999.779305
   Wiskott L, 1997, IEEE T PATTERN ANAL, V19, P775, DOI 10.1109/34.598235
   Yemez Y, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P5
   YOUNG SJ, 2001, HTK BOOK HTK VERSION
   YUILLE AL, 1991, J COGNITIVE NEUROSCI, V3, P59, DOI 10.1162/jocn.1991.3.1.59
   Zhao W, 2003, ACM COMPUT SURV, V35, P399, DOI 10.1145/954339.954342
NR 52
TC 45
Z9 50
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2007
VL 9
IS 4
BP 701
EP 714
DI 10.1109/TMM.2007.893339
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 170NW
UT WOS:000246671200003
OA Green Published
DA 2024-07-18
ER

PT J
AU Pasquero, J
   Luk, J
   Lévesque, V
   Wang, Q
   Hayward, V
   MacLean, KE
AF Pasquero, Jerome
   Luk, Joseph
   Levesque, Vincent
   Wang, Qi
   Hayward, Vincent
   MacLean, Karon E.
TI Haptically enabled handheld information display with distributed tactile
   transducer
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE handheld information devices; haptic interfaces; mobile devices;
   multimodal interaction; tactile displays; tactile transducers
ID SKIN
AB This paper describes the design, construction, and initial evaluation of a handheld information device that supports combined tactile and graphical interaction. The design comprises a liquid crystal graphic display co-located with a miniature, low-power, distributed tactile transducer. This transducer can create electronically-controlled lateral skin deformation patterns which give the sensation of sliding over small shapes. It is integrated within a slider mechanism to control scrolling. It also functions as a detent when pushing on it. Tactile feedback and the combination of visual and tactile feedback in a mobile context enable the development of new functions, such as multimodal navigation within large graphic spaces.
C1 McGill Univ, Ctr Intelligent Machines, Hapt Lab, Montreal, PQ H3A 2A7, Canada.
   Univ British Columbia, Dept Comp Sci, Sensory Percept Interact Grp, Vancouver, BC V6T 1Z4, Canada.
C3 McGill University; University of British Columbia
RP Pasquero, J (corresponding author), McGill Univ, Ctr Intelligent Machines, Hapt Lab, Montreal, PQ H3A 2A7, Canada.
EM jay@cim.mcgill.ca; luk@cs.ubc.ca; vleves@cim.mcgill.ca;
   qiwang@cim.mcgill.ca; hayward@cim.mcgill.ca; maclean@cs.ubc.ca
RI Hayward, Vincent/A-4646-2010; Wang, Qi/B-6596-2008; Wang,
   Qi/D-6997-2012; Levesque, Vincent/H-6454-2019
OI Hayward, Vincent/0000-0002-2102-1965; Levesque,
   Vincent/0000-0003-3511-545X; MacLean, Karon/0000-0003-2969-4627
CR [Anonymous], 2005, P SIGCHI C HUMAN FAC, DOI DOI 10.1145/1054972.1055101
   Benali-Khoudja M, 2004, IEEE INT CONF ROBOT, P721, DOI 10.1109/ROBOT.2004.1307234
   BLISS JC, 1970, IEEE T MAN MACHINE, VMM11, P58, DOI 10.1109/TMMS.1970.299963
   Brewster S., 2004, P 5 C AUSTRALASIAN U, P15
   Chan A, 2005, World Haptics Conference: First Joint Eurohaptics Conference and Symposium on Haptic Interfaces for Virutual Environment and Teleoperator Systems, Proceedings, P432
   Hayward V., 2000, P 8 S HAPTIC INTERFA, P1309
   JOHANSSON RS, 1983, TRENDS NEUROSCI, V6, P27, DOI 10.1016/0166-2236(83)90011-5
   Kajimoto H, 2003, 11TH SYMPOSIUM ON HAPTIC INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS - HAPTICS 2003, PROCEEDINGS, P40, DOI 10.1109/HAPTIC.2003.1191225
   Konyo M., 2000, Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065), P3416, DOI 10.1109/ROBOT.2000.845250
   Lavesque V., 2005, ACM Trans. Appl. Percept, V2, P132, DOI DOI 10.1145/1060581.1060587
   Levesque V., 2003, PROC EUROHAPTICS 200, P261
   Luk J., 2006, P SIGCHI C HUM FACT, P171, DOI DOI 10.1145/1124772.1124800
   MacLean K.E., 2003, P EUROHAPTICS, P351
   Moy G., 2000, Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065), P3409, DOI 10.1109/ROBOT.2000.845247
   Oviatt S, 1999, COMMUN ACM, V42, P74, DOI 10.1145/319382.319398
   Pasquero J., 2003, P EUROHAPTICS, P94
   PASQUERO J, 2006, 0604 TR CIM MCG U
   Sarakoglou I, 2005, World Haptics Conference: First Joint Eurohaptics Conference and Symposium on Haptic Interfaces for Virutual Environment and Teleoperator Systems, Proceedings, P547
   Smith C., 1991, Reviews in Fish Biology and Fisheries, V1, P41, DOI 10.1007/BF00042661
   TACHI S, 1985, IEEE T BIO-MED ENG, V32, P461, DOI 10.1109/TBME.1985.325561
   Takasaki M, 2005, 2005 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-4, P1115, DOI 10.1109/IROS.2005.1545129
   Tretiakoff O., 1977, U.S. Patent, Patent No. [4,044,350, 4044350]
   VANDOREN CL, 1987, J ACOUST SOC AM, V81, P1906, DOI 10.1121/1.394755
   Wang Q., 2006, P CHI 2006, P271
   Wang Q, 2007, J BIOMECH, V40, P851, DOI 10.1016/j.jbiomech.2006.03.004
   Wang Q, 2006, SYMPOSIUM ON HAPTICS INTERFACES FOR VIRTUAL ENVIRONMENT AND TELEOPERATOR SYSTEMS 2006, PROCEEDINGS, P67
   Wilson F.R., 1998, The hand: how its use shapes the brain, language, and human culture
NR 27
TC 24
Z9 30
U1 3
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2007
VL 9
IS 4
BP 746
EP 753
DI 10.1109/TMM.2007.895672
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 170NW
UT WOS:000246671200007
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Cristani, M
   Bicego, M
   Murino, V
AF Cristani, Marco
   Bicego, Manuele
   Murino, Vittorio
TI Audio-visual event recognition in surveillance video sequences
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE audio-visual analysis; automated surveillance; event classification and
   clustering; multimodal background modelling and foreground detection;
   multimodality; scene analysis
AB In the context of the automated surveillance field, automatic scene analysis and understanding systems typically consider only visual information, whereas other modalities, such as audio, are typically disregarded. This paper presents a new method able to integrate audio and visual information for scene analysis in a typical surveillance scenario, using only one camera and one monaural microphone. Visual information is analyzed by a standard visual background/foreground (BG/FG) modelling module, enhanced with a novelty detection stage and coupled with an audio BG/FG modelling scheme. These processes permit one to detect separate audio and visual patterns representing unusual unimodal events in a scene. The integration of audio and visual data is subsequently performed by exploiting the concept of synchrony between such events. The audio-visual (AV) association is carried out on-line and without need for training sequences, and is actually based on the computation of a characteristic feature called audio-video concurrence matrix, allowing one to detect and segment AV events, as well as to discriminate between them. Experimental tests involving classification and clustering of events show all the potentialities of the proposed approach, also in comparison with the results obtained by employing the single modalities and without considering the synchrony issue.
C1 Univ Verona, Dipartimento Informat, I-37134 Verona, Italy.
   Univ Sassari, DEIR, I-07100 Sassari, Italy.
C3 University of Verona; University of Sassari
RP Cristani, M (corresponding author), Univ Verona, Dipartimento Informat, I-37134 Verona, Italy.
EM cristanm@sci.univr.it; bicego@uniss.it; vittorio.murino@univr.it
RI Cristani, Marco/I-5275-2012; Murino, Vittorio/A-5570-2011
OI Murino, Vittorio/0000-0002-8645-2328
CR [Anonymous], 1999, Auditory Scene Analysis: The Perceptual Organization of Sound, DOI DOI 10.7551/MITPRESS/1486.001.0001
   [Anonymous], 2001, THESIS TAMPERE U TEC
   [Anonymous], P INT WORKSH IND COM
   Bach FR, 2004, IEEE T SIGNAL PROCES, V52, P2189, DOI 10.1109/TSP.2004.831032
   BARNARD M, 2003, P IEEE WORKSH NEUR N, P73
   Beal MJ, 2003, IEEE T PATTERN ANAL, V25, P828, DOI 10.1109/TPAMI.2003.1206512
   Broersen PMT, 2002, IEEE T INSTRUM MEAS, V51, P211, DOI 10.1109/19.997814
   Checka N., 2002, PERSON TRACKING USIN
   Cowling M, 2003, PATTERN RECOGN LETT, V24, P2895, DOI 10.1016/S0167-8655(03)00147-8
   Cristani M, 2004, INT C PATT RECOG, P399, DOI 10.1109/ICPR.2004.1334232
   CRISTANI M, 2004, P EUR C COMP VIS ECC, P202
   CUADRA MB, 2005, SIGNAL PROCESS, V85, P875
   DARRELL T, 2002, GEOMETRIC STAT APPRO
   Duda R. O., 2000, PATTERN CLASSIFICATI
   Fisher J.W., 2000, Neural Information Processing Systems (NIPS), V13, P772
   Fisher JW, 2004, IEEE T MULTIMEDIA, V6, P406, DOI 10.1109/TMM.2004.827503
   Hardoon D. R., 2003, CSDTR0302
   Jain A. K., 1988, Algorithms for Clustering Data, P446
   KANADE RCT, 2000, IEEE T PATTERN ANAL, V22
   KAY J, 1992, P INT JOINT C NEUR N, V4, P79
   Marple SL., 1987, Digital spectral analysis with applications
   Mason M, 2001, 30TH APPLIED IMAGERY PATTERN RECOGNITION WORKSHOP, PROCEEDINGS, P154, DOI 10.1109/AIPR.2001.991219
   MCCOWAN I, IN PRESS IEEE T PATT
   Niebur E, 2002, CURR OPIN NEUROBIOL, V12, P190, DOI 10.1016/S0959-4388(02)00310-0
   PETKOVIC M, 2003, CONTENT BASED VIDEO
   Pfeiffer S, 2001, MULTIMED TOOLS APPL, V15, P59, DOI 10.1023/A:1011315803415
   Roweis ST, 2001, ADV NEUR IN, V13, P793
   SLANEY M, 2000, P NEUR INF PROC NIPS
   Stauffer C., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P246, DOI 10.1109/CVPR.1999.784637
   Stein Barry E., 1993, The Merging of the Senses. The Merging of the Senses. Cognitive Neuroscience
   Toyama K., 1999, The Proceedings of the Seventh IEEE International Conference on Computer Vision, V1, P255
   WILSON K, 2001, P WORKSH PERC US INT
   Zhang D, 2005, PROC CVPR IEEE, P611
   Zhang T, 2001, IEEE T SPEECH AUDI P, V9, P441, DOI 10.1109/89.917689
   Zotkin DN, 2002, EURASIP J APPL SIG P, V2002, P1154, DOI 10.1155/S1110865702206058
   ZOU X, 2005, P 2005 C COMP VIS PA, P88
   ZOU X, 2005, P 2005 C COMP VIS PA
NR 37
TC 104
Z9 121
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2007
VL 9
IS 2
BP 257
EP 267
DI 10.1109/TMM.2006.886263
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 135CO
UT WOS:000244131900006
DA 2024-07-18
ER

PT J
AU Xie, X
   Liu, H
   Ma, WY
   Zhang, HJ
AF Xie, Xing
   Liu, Hao
   Ma, Wei-Ying
   Zhang, Hong-Jiang
TI Browsing large pictures under limited display sizes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE adaptive content delivery; attention model; image adaptation;
   information foraging
ID VISUAL-ATTENTION
AB Pictures have become increasingly common and popular in mobile communications. However, due to the limitation of mobile devices, there is a need to develop new technologies to facilitate the browsing of large pictures on the small screen. In this paper, we propose a set of novel approaches which are able to aid or automate common image browsing tasks on mobile devices. All of these approaches are based on an image attention model which is employed to illustrate the information structure within an image. An efficient algorithm to generate the optimal browsing path based on the information foraging theory is presented. Experimental evaluations of the proposed mechanism indicate that our approach is an effective way for viewing large images on small displays.
C1 Microsoft Res Asia, Ctr Adv Technol, Beijing 100080, Peoples R China.
   Stanford Univ, Dept Comp Sci, Stanford, CA 94035 USA.
C3 Microsoft Research Asia; Microsoft; Stanford University
RP Xie, X (corresponding author), Microsoft Res Asia, Ctr Adv Technol, Beijing 100080, Peoples R China.
EM xingx@rnicrosoft.com; hliu.ac@gmail.com; wyma@microsoft.com;
   hjzhang@microsoft.com
RI Liu, Hao/M-1010-2019
CR [Anonymous], 2003, P 11 ACM INT C MULTI, DOI DOI 10.1145/957013.957094
   Chang EC, 2000, APPL COMPUT HARMON A, V9, P312, DOI 10.1006/acha.2000.0324
   Chen LQ, 2003, MULTIMEDIA SYST, V9, P353, DOI 10.1007/s00530-003-0105-4
   Chen XR, 2001, LECT NOTES COMPUT SC, V2195, P222
   Chernyak DA, 2001, IEEE T SYST MAN CY B, V31, P514, DOI 10.1109/3477.938257
   Chi E., 2001, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, P490, DOI [10.1145/365024.365325, DOI 10.1145/365024.365325]
   Christopoulos C, 2000, IEEE T CONSUM ELECTR, V46, P1103, DOI 10.1109/30.920468
   de Bruijn O., 2000, Proceedings of the Working Conference on Advanced Visual Interfaces (AVI '00), P189, DOI DOI 10.1145/345513.345309
   Fan X, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I, PROCEEDINGS, P53
   Fan Xin., 2003, P ACM MULTIMEDIA, P247
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Itti L, 2001, NAT REV NEUROSCI, V2, P194, DOI 10.1038/35058500
   Itti L, 1999, P SOC PHOTO-OPT INS, V3644, P473, DOI 10.1117/12.348467
   Li SZ, 2002, LECT NOTES COMPUT SC, V2353, P67
   Luo JB, 2002, IEEE IMAGE PROC, P13
   Ma WY, 2000, P SOC PHOTO-OPT INS, V3969, P86
   MILANESE R, 1995, OPT ENG, V34, P2428, DOI 10.1117/12.205668
   Mohan R, 1999, IEEE T MULTIMEDIA, V1, P104, DOI 10.1109/6046.748175
   Pirolli P, 1999, PSYCHOL REV, V106, P643, DOI 10.1037/0033-295X.106.4.643
   Rayner K, 1998, PSYCHOL BULL, V124, P372, DOI 10.1037/0033-2909.124.3.372
   Smith JR, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 3, P7, DOI 10.1109/ICIP.1998.998987
   Suh Bongwon, 2003, P ACM S US INT SOFTW, P95, DOI DOI 10.1145/964696.964707
   SUN FC, 1995, P IEEE INT C NEUR NE, V5, P2309
   Vitu F, 2001, VISION RES, V41, P3513, DOI 10.1016/S0042-6989(01)00166-3
NR 24
TC 22
Z9 30
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2006
VL 8
IS 4
BP 707
EP 715
DI 10.1109/TMM.2006.876294
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 069BS
UT WOS:000239420300006
DA 2024-07-18
ER

PT J
AU del Valle, ACA
   Dugelay, JL
AF del Valle, ACA
   Dugelay, JL
TI Efficient ocular expression analysis for synthetic reproduction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE animation; communications systems; image motion analysis; image
   processing; Kalman filtering; machine vision; real-time systems;
   teleconferencing
ID FACE; ANIMATION; TRACKING
AB This paper presents an original framework that analyzes the complete ocular (eye + eyebrow) expression on video sequences to reproduce it later on synthetic three-dimensional (3-D) models in real-time. We propose a two step process to develop robust techniques for facial feature analysis aimed at working without special illumination conditions or physical constraints on the user (markers, fixed frontal pose, etc.). First, simple and efficient image-processing methods based on motion models are designed over a frontal point of view of the face. Natural and realistic intra-feature and inter-feature constraints are applied to improve the analysis results. Then, the usability of these algorithms is extended to enable the analysis regardless of the person's pose in front of the camera. This is achieved by redefining the motion models involved over the speaker's highly realistic synthetic representation (clone), by using a suitable observation model, and by predicting the head pose in 3-D, frame by frame.
C1 Eurecom Inst, Multimedia Commun Dept, F-06604 Sophia Antipolis, France.
C3 IMT - Institut Mines-Telecom; EURECOM
RP del Valle, ACA (corresponding author), Accenture Technol Labs, BP 99, F-06604 Sophia Antipolis, France.
EM ana.c.andresdelvalle@accenture.com; jld@eurecom.fr
RI DUGELAY, Jean-Luc/ABE-7096-2021
OI DUGELAY, jean-luc/0000-0003-3151-4330
CR Bradski GR, 1998, FOURTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION - WACV'98, PROCEEDINGS, P214, DOI 10.1109/ACV.1998.732882
   CHANG YJ, 2000, P IEEE INT C MULT EX
   CORDEA MD, 2001, P IEEE INSTR MEAS TE
   del Valle ACA, 2002, LECT NOTES COMPUT SC, V2492, P213
   DELVALLE ACA, 2001, RR01058 EUR RES
   DELVALLE ACA, 2001, P IEEE INT C IM PROC
   DELVALLE ACA, 2003, THESIS I EURECOM PAR
   DELVALLE ACA, 2002, TECHNICAL DEMO ACM M
   DELVALLE ACA, 2002, P IEEE INT C MULT EX
   DELVALLE ACA, 2003, P INT C HUM COMP INT
   DUGELAY JL, 1999, P IEEE PICT COD S PO
   Eisert P, 2000, IEEE T CIRC SYST VID, V10, P344, DOI 10.1109/76.836279
   GARAU M, 2003, P SIG CHI C HUM FACT, V1, P525
   Goto T, 2001, IEEE SIGNAL PROC MAG, V18, P17, DOI 10.1109/79.924885
   HARO A, 2000, P IEEE C COMP VIS PA
   Kampmann M, 2002, IEEE T CIRC SYST VID, V12, P172, DOI 10.1109/76.993438
   Pighin F, 2002, INT J COMPUT VISION, V50, P143, DOI 10.1023/A:1020393915769
   Redfern S., 2002, Journal of Information Technology Education, V1
   Ström J, 2002, EURASIP J APPL SIG P, V2002, P1039, DOI 10.1155/S1110865702206034
   Tian YI, 2001, IEEE T PATTERN ANAL, V23, P97, DOI 10.1109/34.908962
   Valente S, 2001, SIGNAL PROCESS-IMAGE, V16, P585, DOI 10.1016/S0923-5965(00)00038-2
   Valente S, 2000, IEEE MULTIMEDIA, V7, P34, DOI 10.1109/93.839309
   YIN L, 1993, J CHINA I COMMUN, V14, P3
NR 23
TC 0
Z9 0
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2006
VL 8
IS 1
BP 70
EP 80
DI 10.1109/TMM.2005.861290
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 005UR
UT WOS:000234850000007
DA 2024-07-18
ER

PT J
AU Joyce, RA
   Liu, BD
AF Joyce, RA
   Liu, BD
TI Temporal segmentation of video using frame and histogram space
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE compressed video processing; dissolve detection; gradual transition
   detection; MPEG video; scene change detection; shot detection; wipe
   detection
ID SCENE CHANGE DETECTION
AB Two algorithms are presented for the detection of gradual transitions in video sequences. The first is a dissolve detection algorithm utilizing certain properties of a dissolve's trajectory in image-space. It is implemented both as a simple threshold-based detector, and as a parametric detector by modeling the error properties of the extracted statistics. The second is an algorithm to detect a wide variety of wipes based on image histogram characteristics during such transitions. Both algorithms operate in the compressed domain, requiring only partial decoding of the compressed video stream. Experiments show the algorithms perform as well as-and in some cases, better than-full-frame methods, on a wide variety of gradual transitions, and can operate significantly faster than real-time.
C1 Princeton Univ, Dept Elect Engn, Princeton, NJ 08544 USA.
C3 Princeton University
RP ATC NY, Ithaca, NY 14850 USA.
EM rob@atc-nycorp.com; liu@princeton.edu
CR Alattar AM, 1998, IEEE T CONSUM ELECTR, V44, P43, DOI 10.1109/30.663729
   [Anonymous], 1993, Multimedia Systems, DOI DOI 10.1007/BF01210504
   Drew MS, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P929, DOI 10.1109/ICIP.2000.899609
   Fernando W. A. C., 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P299, DOI 10.1109/ICIP.1999.817121
   FERNANDO WAC, 1999, P IEEE INT C IM PROC, V3, P294
   JOYCE RA, 2002, THESIS PRINCETON U P
   KIM H, 1999, P SOC PHOTO-OPT INS, V3656, P280
   LIU HCH, 1995, P SOC PHOTO-OPT INS, V2419, P26, DOI 10.1117/12.206370
   MENG JH, 1995, P SOC PHOTO-OPT INS, V2419, P14, DOI 10.1117/12.206359
   Nakajima Y, 1997, P SOC PHOTO-OPT INS, V3024, P992, DOI 10.1117/12.263179
   Ngo CW, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 1, P750, DOI 10.1109/MMCS.1999.779293
   Pei SC, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P953, DOI 10.1109/ICIP.2000.899615
   Poor H. Vincent, 1994, An introduction to signal detection and estimation
   Shen K, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pB252
   Sugano M, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 1, P888, DOI 10.1109/ICIP.1998.723663
   Wu M, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 1, P893, DOI 10.1109/ICIP.1998.723664
   Yeo BL, 1995, IEEE T CIRC SYST VID, V5, P533, DOI 10.1109/76.475896
   YU H, 1998, P IEEE INT C AC SPEE, V5, P2965
   Zabih R, 1999, MULTIMEDIA SYST, V7, P119, DOI 10.1007/s005300050115
NR 19
TC 45
Z9 46
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2006
VL 8
IS 1
BP 130
EP 140
DI 10.1109/TMM.2005.861285
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 005UR
UT WOS:000234850000012
DA 2024-07-18
ER

PT J
AU Duan, LY
   Xu, M
   Tian, Q
   Xu, CS
   Jin, JS
AF Duan, LY
   Xu, M
   Tian, Q
   Xu, CS
   Jin, JS
TI A unified framework for semantic shot classification in sports video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE semantic gap; shot representation; shot similarity; video
   classification; video databases indexing
ID MOTION
AB The extensive amount of multimedia information available necessitates content-based video indexing and retrieval methods. Since humans tend to use high-level semantic concepts when querying and browsing multimedia databases, there is an increasing need for semantic video indexing and analysis. For this purpose, we present a unified framework for semantic shot classification in sports video, which has been widely studied due to tremendous commercial potentials. Unlike most existing approaches, which focus on clustering by aggregating shots or key-frames with similar low-level features, the proposed scheme employs supervised learning to perform a top-down video shot classification. Moreover, the supervised learning procedure is constructed on the basis of effective mid-level representations instead of exhaustive low-level features. This framework consists of three main steps: 1) identify video shot classes for each sport, 2) develop a common set of motion, color, shot length-related mid-level representations; and 3) supervised learning of the given sports video shots. It is observed that for each sport we can predefine a small number of semantic shot classes, about 5-10, which covers 90%-95% of broadcast sports video. We employ nonparametric feature space analysis to map low-level features to mid-level semantic video shot attributes such as dominant object (a player) motion, camera motion patterns, and court shape, etc. Based on the fusion of those mid-level shot attributes, we classify video shots into the predefined shot classes, each of which has clear semantic meanings. With this framework we have achieved good classification accuracy of 85%-95% on the game videos of five typical ball type sports (i.e., tennis, basketball, volleyball, soccer, and table tennis) with over 5500 shots of about 8 h. With correctly classified sports video shots, further structural and temporal analysis, such as event detection, highlight extraction, video skimming, and table of content, will be greatly facilitated.
C1 Inst Infocomm Res, Singapore 119613, Singapore.
   Univ Newcastle, Sch Design Commun & Informat Technol, Newcastle, NSW 2308, Australia.
C3 Agency for Science Technology & Research (A*STAR); A*STAR - Institute
   for Infocomm Research (I2R); University of Newcastle
RP Inst Infocomm Res, Singapore 119613, Singapore.
EM lingyu@i2r.a-star.edu.sg; MXu@ntu.edu.sg; tian@i2r.a-star.edu.sg;
   xucs@i2r.a-star.edu.sg; Jesse.Jin@newcastle.edu.au
RI xu, cj/HJZ-3488-2023; Tian, Qi/B-4182-2011; chen, yue/JXW-9556-2024
OI Tian, Qi/0000-0003-2269-6187; Xu, Min/0000-0001-9581-8849
CR Adams B., 2002, INT J IMAGE GRAPH, V2, P215
   ALANTAN AA, 2001, MULTIMED TOOLS APPL, V4, P137
   [Anonymous], IEEE COMPUT
   [Anonymous], 2002, P 10 ACM INT C MULT
   [Anonymous], 1995, P IEEE INT C MULT CO
   [Anonymous], P ACM C MULT
   ARONS B, 1994, P ICSLP 94, P1931
   Assfalg J, 2002, IEEE MULTIMEDIA, V9, P52, DOI 10.1109/93.998060
   Babaguchi N, 2002, IEEE T MULTIMEDIA, V4, P68, DOI 10.1109/6046.985555
   Bishop C. M., 1995, NEURAL NETWORKS PATT
   BLACK MJ, 1995, COMPUTER VISION IMAG, V6, P348
   CAELLI T, 1997, MACHINE LEARNING IMA, P189
   CANTONI V, 1997, ARTIFICIAL VISION, P1
   CHANG SF, 1995, IEEE J SEL AREA COMM, V13, P1, DOI 10.1109/49.363151
   Chang YL, 1996, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, P306, DOI 10.1109/MMCS.1996.534992
   CHENG YZ, 1995, IEEE T PATTERN ANAL, V17, P790, DOI 10.1109/34.400568
   Cherkassky V, 1997, IEEE Trans Neural Netw, V8, P1564, DOI 10.1109/TNN.1997.641482
   CHESHIRE D, 1990, COMPLETE BOOK VIDEO
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   DORAI C, 2002, P 10 ACM INT C MULT, P580
   Duan L.Y., 2003, Proc. ACM Int. Conf. Multimedia, P33
   Duan LY, 2004, IEEE IMAGE PROC, P1597
   Duan LY, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL V, PROCEEDINGS, P709
   Duan LY, 2003, PROC SPIE, V5021, P300, DOI 10.1117/12.476259
   Han M., 2002, Proc. ACM Multimedia, P347, DOI [DOI 10.1145/641007.641081, 10.1145/641007.641081]
   *I INF RES, 2003, 12R TECH REP
   Jain AK, 1999, MULTIMEDIA SYST, V7, P369, DOI 10.1007/s005300050139
   Kressel UHG, 1999, ADVANCES IN KERNEL METHODS, P255
   LANGLEY P, 1992, AAAI-92 PROCEEDINGS : TENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE, P223
   Li HP, 2000, IEEE T IMAGE PROCESS, V9, P147, DOI 10.1109/83.817607
   Lienhart R, 2000, MULTIMEDIA SYST, V8, P69, DOI 10.1007/s005300050006
   Mittal A, 2001, PROC CVPR IEEE, P110
   Naphade MR, 2001, IEEE T MULTIMEDIA, V3, P141, DOI 10.1109/6046.909601
   Nepal S., 2001, ACM Multimedia, P261
   Ngo C.W., 2001, Proc. ACM Multimedia, P51, DOI DOI 10.1145/500141.500151
   ODOBEZ JM, 1995, J VIS COMMUN IMAGE R, V6, P348, DOI 10.1006/jvci.1995.1029
   Peker KA, 2002, P SOC PHOTO-OPT INS, V4676, P318
   Platt JC, 2000, ADV NEUR IN, V12, P547
   Rubner Y, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P59, DOI 10.1109/ICCV.1998.710701
   Rui Y., 2000, Proceedings ACM Multimedia 2000, P105, DOI 10.1145/354384.354443
   Silverman, 2018, DENSITY ESTIMATION S, DOI 10.1201/9781315140919
   Smith E, 1997, ENVIRON TOXICOL PHAR, V4, P3, DOI 10.1016/S1382-6689(97)10035-7
   SNOEK GM, 2001, ISIS TECH REP SERIES
   Sudhir G, 1998, 1998 IEEE INTERNATIONAL WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO DATABASE, PROCEEDINGS, P81, DOI 10.1109/CAIVD.1998.646036
   SWAIN MJ, 1991, INT J COMPUT VISION, V7, P11, DOI 10.1007/BF00130487
   Tan YP, 2000, IEEE T CIRC SYST VID, V10, P133, DOI 10.1109/76.825867
   VINCENT L, 1991, IEEE T PATTERN ANAL, V13, P583, DOI 10.1109/34.87344
   Wang R, 2000, ISCAS 2000: IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS - PROCEEDINGS, VOL V, P21, DOI 10.1109/ISCAS.2000.857353
   Xie LX, 2002, INT CONF ACOUST SPEE, P4096
   Xiong ZY, 2003, IEEE IMAGE PROC, P5
   Xu M, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P189
   Xu M, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL II, PROCEEDINGS, P281
   XU P, 2001, P INT C MULT EXP, P184
   Yeo BL, 1999, MULTIMEDIA SYST, V7, P269, DOI 10.1007/s005300050129
   Zhang D., 2002, ACM Multimedia, P315
   ZHANG FJ, 1989, ACTA MATH APPL SIN-E, V1, P1
   ZHONG D, 2001, P IEEE ICME 01, P920
   Zhong Y, 2000, IEEE T PATTERN ANAL, V22, P385, DOI 10.1109/34.845381
   Zhuang YT, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 1, P866, DOI 10.1109/ICIP.1998.723655
   ROXIO
NR 60
TC 140
Z9 153
U1 1
U2 29
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2005
VL 7
IS 6
BP 1066
EP 1083
DI 10.1109/TMM.2005.858395
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 986SY
UT WOS:000233471200008
DA 2024-07-18
ER

PT J
AU Liao, WJ
   Chang, JC
   Li, VOK
AF Liao, WJ
   Chang, JC
   Li, VOK
TI Application-layer conference trees for multimedia multipoint conferences
   using Megaco/H.248
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE conference tree; MCU; Megaco/H.248; multipoint conference
AB In this paper, we propose a new approach to establishing application layer conference trees for multimedia multipoint conferences on the Internet using the Megaco/H.248 protocol, a Voice over IP (VoIP) media gateway control protocol. In existing VoIP protocols (and also legacy telephone networks), a multipoint conference takes place through an MCU, and forms a star topology centered at the MCU. This paper suggests to establishing shared, cost effective conference trees for VoIP conferences. Each tree is rooted at the conference initiator, who initiates the conference, and spans over all the conference members. Tree branches grow or are trimmed dynamically and adaptively, in a way to avoid the growth of a skewed tree. We develop a simplified analytical model and conduct simulations to evaluate the performance of the proposed approach. The results show that our approach enjoys the advantage of lower join latency and better bandwidth efficiency compared to the traditional MCU approach, and is cost effective compared to a near optimal Steiner tree.
C1 Natl Taiwan Univ, Dept Elect Engn, Taipei 10764, Taiwan.
   Natl Taiwan Univ, Grad Inst Commun Engn, Taipei 10764, Taiwan.
   Univ Hong Kong, Dept Elect & Elect Engn, Hong Kong, Hong Kong, Peoples R China.
C3 National Taiwan University; National Taiwan University; University of
   Hong Kong
RP Natl Taiwan Univ, Dept Elect Engn, Taipei 10764, Taiwan.
EM wjliao@ntu.edu.tw; vli@eee.hku.hk
RI Li, Victor On Kwok/C-1858-2009
OI Liao, Wanjiun/0000-0001-5396-8849
CR [Anonymous], 2543 IETF RFC
   Calvert K., GT INTERNETWORK TOPO
   CUERVO F, 2000, 0152TUT IETF
   Handley M, 1998, 2327 IETF RFC
   KADIRIRE J, 1995, IEEE INFOCOM SER, P212, DOI 10.1109/INFCOM.1995.515879
   KOU L, 1981, ACTA INFORM, V15, P141, DOI 10.1007/BF00288961
   Leslie I., 1993, P IEEE INFOCOM SAN F, P82
   MING Y, 1999, P APCC OECC 99 4 OPT, V2, P1130
   Schulzrinne H, 1999, IEEE NETWORK, V13, P18, DOI 10.1109/65.767133
   TAYLOR T, 2000, IEEE COMMUN MAG  OCT
   Thom GA, 1996, IEEE COMMUN MAG, V34, P52, DOI 10.1109/35.556487
   WAXMAN BM, 1988, IEEE J SEL AREA COMM, V6, P1617, DOI 10.1109/49.12889
   2000, IEEE COMMUN MAG  APR, V38
   1999, IEEE NETWORK     MAY, V13
NR 14
TC 1
Z9 1
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2005
VL 7
IS 5
BP 951
EP 959
DI 10.1109/TMM.2005.854387
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 967IM
UT WOS:000232084900016
OA Green Published
DA 2024-07-18
ER

PT J
AU Mehra, P
   De Vleeschouwer, C
   Zakhor, A
AF Mehra, P
   De Vleeschouwer, C
   Zakhor, A
TI Receiver-driven bandwidth sharing for TCP and its application to video
   streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE bandwith allocation; multimedia streaming; TCP
AB Applications using Transmission Control Protocol (TCP), such as web-browsers, ftp, and various peer-to-peer (P2P) programs, dominate most of the Internet traffic today. In many cases, users have bandwidth-limited last mile connections to the Internet which act as network bottlenecks. Users generally run multiple concurrent networking applications that compete for the scarce bandwidth resource. Standard TCP shares bottleneck link capacity according to connection round-trip time (RTT), and consequently may result in a bandwidth partition which does not necessarily coincide with the user's desires. In this work, we present a receiver-based bandwidth sharing system (BWSS) for allocating the capacity of last-hop access links according to user preferences. Our system does not require modifications to the TCP protocol, network infrastructure or sending hosts, making it easy to deploy. By breaking fairness between flows on the access link, the BWSS can limit the throughput fluctuations of high-priority applications. We utilize the BWSS to perform efficient video streaming over TCP to receivers with bandwidth-limited last mile connections. We demonstrate the effectiveness of our proposed system through Internet experiments.
C1 Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
   Univ Catholique Louvain, Telecommun Lab, Louvain, Belgium.
C3 University of California System; University of California Berkeley;
   Universite Catholique Louvain
RP Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
EM pmehra@eecs.berkeley.edu; devlees@tele.ucl.ac.be; avz@eecs.berkeley.edu
RI Zakhor, Avideh/GYA-1602-2022
OI Zakhor, Avideh/0000-0003-4770-6353
CR [Anonymous], 1992, 1323 RFC
   BALAKRISHNAN H, 1999, P ACM SIGCOMM 1999 S
   BENNETT JCR, 1996, P IEEE INFOCOM 96 SA
   Crowcroft J., 1998, Computer Communication Review, V28, P53, DOI 10.1145/293927.293930
   DEMERS A, 1989, P ACM SIGC 89
   DONG Y, 2002, PACKET VIDEO
   Floyd S., 2000, P ACM SIGCOMM 2000
   Floyd S, 1993, IEEE ACM T NETWORK, V1, P397, DOI 10.1109/90.251892
   HENDERSON TR, 1998, P GLOB 98 SYDN AUSTR
   Hsiao P., 2001, P ACM NOSSDAV
   HSIAO P, 2001, P IEEE GLOB 01 SEP
   Krasic C., 2001, 8 INT WORKSH INT DIS
   LIANG S, 2002, P IEEE INCP 02
   MEHRA P, 2003, P IEEE INFOCOM 2003
   MUKHERJEE B, 2000, P IEEE ICNP 00 NOV
   Postel J., 1981, Rfc0793: Transmission control protocol
   SPRING NT, 2000, P INFOCOM 2000
   STOICA I, 1998, P ACM SIGCOMM 98
   Tan WT, 1999, IEEE T MULTIMEDIA, V1, P172, DOI 10.1109/6046.766738
NR 19
TC 13
Z9 20
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2005
VL 7
IS 4
BP 740
EP 752
DI 10.1109/TMM.2005.846783
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 946XQ
UT WOS:000230607000014
DA 2024-07-18
ER

PT J
AU Robert, A
   Picard, J
AF Robert, A
   Picard, J
TI On the use of masking models for image and audio watermarking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE attacks; mask attack; masking models; robustness; watermarking; Wiener
   attack
ID DIGITAL IMAGES
AB In most watermarking systems, masking models, inherited from data compression algorithms, are used to preserve fidelity by controlling the perceived distortion resulting from adding the watermark to the original signal. So far, little attention has been paid to the consequences of using such models on a key design parameter: the robustness of the watermark to intentional attacks. The goal of this paper is to demonstrate that by considering fidelity alone, key information on the location and strength of the watermark may become available to an attacker; the latter can exploit such knowledge to build an effective mask attack. First, defining a theoretical framework in which analytical expressions for masking and watermarking are laid, a relation between the decrease of the detection statistic and the introduced perceptual distortion is found for the mask attack. The latter is compared to the Wiener filter attack. Then, considering masking models widely used in watermarking, experiments on both simulated and real data (audio and images) demonstrate how knowledge on the mask enables to greatly reduce the detection statistic, even for small perceptual distortion costs. The critical tradeoff between robustness and distortion is further discussed, and conclusions on the use of masking models in watermarking drawn.
C1 Thomson MediaSec, D-45127 Essen, Germany.
   EPFL, Swiss Fed Inst Technol, LCAV, CH-1015 Lausanne, Switzerland.
   EPFL, Swiss Fed Inst Technol, LANOS, CH-1015 Lausanne, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne; Swiss Federal Institutes of Technology Domain;
   Ecole Polytechnique Federale de Lausanne
RP Robert, A (corresponding author), Thomson MediaSec, D-45127 Essen, Germany.
EM arnaud.robert@thomson.net; jpicard@mediasec.com
CR [Anonymous], 1998, FUNDAMENTALS STAT SI
   BENDER W, 1996, IBM SYST J, V35
   Bosi M, 1997, J AUDIO ENG SOC, V45, P789
   BOSI M, 1996, AUD ENG SOC 101 CONV
   CHEN B, 1999, SPIE SECURITY WATERM
   Hernández JR, 2000, IEEE T IMAGE PROCESS, V9, P55, DOI 10.1109/83.817598
   KUTTER M, 1998, P SPIE SECURITY WATE
   KUTTER M, 2000, P SPIE SEC WAT MULT
   LANGELAAR GC, 1998, P EUR SIGN PROC C EU
   PETITCOLAS F, 1998, P 2 WORKSH INF HID O
   PODILCHUK CI, 1996, P EL IM SAN JOS CA
   ROBERT A, 2000, P SPIE SEC WAT MULT
   SU J, 2000, P SPIE SECURITY WATE
   Swanson MD, 1998, SIGNAL PROCESS, V66, P337, DOI 10.1016/S0165-1684(98)00014-0
   VOLOSHYNOVSKI S, 2000, P SPIE SECURITY WATE
   Wolfgang RB, 1999, P IEEE, V87, P1108, DOI 10.1109/5.771067
   Zeng WJ, 1999, IEEE T IMAGE PROCESS, V8, P1534, DOI 10.1109/83.799882
NR 17
TC 7
Z9 7
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2005
VL 7
IS 4
BP 727
EP 739
DI 10.1109/TMM.2005.846781
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 946XQ
UT WOS:000230607000013
OA Green Published, Green Submitted
DA 2024-07-18
ER

PT J
AU Wu, YD
AF Wu, YD
TI On the security of an SVD-based ownership watermarking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE one-way function; rightful ownership; singular value decomposition (SVD)
ID PROTECTING RIGHTFUL OWNERSHIP; SCHEMES; ATTACKS
AB This paper proposes a counterfeiting attack on an SVD-based ownership watermarking scheme. In the proposed attack, the adversary can claim the rightful ownership of any image by fabricating a bogus "original" image and meaningful logo. To defend against this attack, this paper proposes to generate the watermark from the original image and owner's logo with a one-way function.
C1 Inst Infocomm Res, Singapore 119613, Singapore.
C3 Agency for Science Technology & Research (A*STAR); A*STAR - Institute
   for Infocomm Research (I2R)
RP Wu, YD (corresponding author), Inst Infocomm Res, Singapore 119613, Singapore.
EM wydong@i2r.a-star.edu.sg
OI Wu, Yongdong/0000-0002-0850-724X
CR Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120
   Craver S, 1998, IEEE J SEL AREA COMM, V16, P573, DOI 10.1109/49.668979
   Katzenbeisser S, 2002, PROC SPIE, V4675, P260, DOI 10.1117/12.465283
   Liu RZ, 2002, IEEE T MULTIMEDIA, V4, P121, DOI 10.1109/6046.985560
   PIVA A, 1998, ADV INFORM TECHNOLOG, P636
   Qiao LT, 1998, J VIS COMMUN IMAGE R, V9, P194, DOI 10.1006/jvci.1998.0391
   RAMKUMAR M, ROBUST PROTOCOLS PRO
   WANG YW, WAVELET BASED WATERM
   Wong PW, 2001, IEEE T IMAGE PROCESS, V10, P1593, DOI 10.1109/83.951543
   1995, FIPS PUBL
NR 10
TC 61
Z9 66
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2005
VL 7
IS 4
BP 624
EP 627
DI 10.1109/TMM.2005.846774
PG 4
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 946XQ
UT WOS:000230607000004
DA 2024-07-18
ER

PT J
AU Mukherjee, D
   Delfosse, E
   Kim, JG
   Wang, Y
AF Mukherjee, D
   Delfosse, E
   Kim, JG
   Wang, Y
TI Optimal adaptation decision-taking for terminal and network
   quality-of-service
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE adaptation; decision-taking; digital item adaptation; MPEG-21; rate
   shaping; requantization; scalable bit-streams; terminal and network
   constraints; transcoding
ID BITSTREAM SYNTAX DESCRIPTION; SEARCH
AB In order to cater to the diversity of terminals and networks, efficient, and flexible adaptation of multimedia content in the delivery path to end consumers is required. To this end, it is necessary to associate the content with metadata that provides the relationship between feasible adaptation choices and various media characteristics obtained as a function of these choices. Furthermore, adaptation is driven by specification of terminal, network, user preference or rights based constraints on media characteristics that are to be satisfied by the adaptation process. Using the metadata and the constraint specification, an adaptation engine can take an appropriate decision for adaptation, efficiently and flexibly. MPEG-21 Part 7 entitled Digital Item Adaptation standardizes among other things the metadata and constraint specifications that act as interfaces to the decision-taking component of an adaptation engine. This paper presents the concepts behind these tools in the standard, shows universal methods based on pattern search to process the information in the tools to make decisions, and presents some adaptation use cases where these tools can be used.
C1 Hewlett Packard Labs, Palo Alto, CA 94304 USA.
   Interuniv Microelect Ctr IMEC, B-3001 Heverlee, Belgium.
   ETRI, Broadcasting Media Res Grp, Taejon 305350, South Korea.
   Columbia Univ, Dept Elect Engn, New York, NY 10027 USA.
C3 Hewlett-Packard; IMEC; Electronics & Telecommunications Research
   Institute - Korea (ETRI); Columbia University
RP Hewlett Packard Labs, Palo Alto, CA 94304 USA.
EM debargha@hpl.hp.com; delfosse@imec.be; jgkim@etri.re.kr;
   ywang@ee.columbia.edu
OI Mukherjee, Debargha/0000-0002-9380-7377
CR ABRAMSON MA, 2002, THESIS DEPT COMPUT A
   [Anonymous], 2002, JPEG2000: Image Compression Fundamentals, Standards, and Practice
   ASSUNCAO P, 1996, P IEEE INT C AC SPEE, V4, P1998
   AUDER C, 2004, UNPUB SIAM J OPTIMIZ
   Audet C, 2003, SIAM J OPTIMIZ, V13, P889, DOI 10.1137/S1052623400378742
   Audet C, 2001, SIAM J OPTIMIZ, V11, P573, DOI 10.1137/S1052623499352024
   Bauschke HH, 2003, IEEE T IMAGE PROCESS, V12, P843, DOI 10.1109/TIP.2003.812375
   Bormans J, 2003, IEEE SIGNAL PROC MAG, V20, P53, DOI 10.1109/MSP.2003.1184339
   Chakareski J., 2004, P IEEE INT C IM PROC
   Devillers S, 2005, IEEE T MULTIMEDIA, V7, P463, DOI 10.1109/TMM.2005.846794
   Eleftheriadis A., 1995, Proceedings. International Conference on Image Processing (Cat. No.95CB35819), P396, DOI 10.1109/ICIP.1995.537655
   Gill P. E., 2003, PRACTICAL OPTIMIZATI
   Hsiang ST, 2001, SIGNAL PROCESS-IMAGE, V16, P705, DOI 10.1016/S0923-5965(01)00002-9
   KIM J, 2003, P IEEE INT C MULT EX
   Kolda TG, 2003, SIAM REV, V45, P385, DOI [10.1137/S003614450242889, 10.1137/S0036144502428893]
   Lerouge S, 2003, LECT NOTES COMPUT SC, V2849, P122
   Mukherjee D, 2003, PROC SPIE, V5018, P148, DOI 10.1117/12.478426
   MUKHERJEE D, 2003, P SPIE VIS COMM IM P, V5150
   OHM JR, 2004, SIGNAL PROCESSING IM
   Panis G, 2003, SIGNAL PROCESS-IMAGE, V18, P721, DOI 10.1016/S0923-5965(03)00061-4
   Steuer R.E., 1996, Multiple Criteria Optimization Theory, Computation and Application
   Torczon V, 1997, SIAM J OPTIMIZ, V7, P1, DOI 10.1137/S1052623493250780
   van der Schaar M, 2005, IEEE T MULTIMEDIA, V7, P471, DOI 10.1109/TMM.2005.846790
   Vetro A, 2005, IEEE T MULTIMEDIA, V7, P418, DOI 10.1109/TMM.2005.846795
   Vetro A, 2001, IEEE T CIRC SYST VID, V11, P387, DOI 10.1109/76.911163
   WANG Y, 2003, P IEEE INT C IM PROC
   Werner O, 1999, IEEE T IMAGE PROCESS, V8, P179, DOI 10.1109/83.743853
   Yin P, 2002, IEEE T CIRC SYST VID, V12, P1009, DOI 10.1109/TCSVT.2002.805509
   2002, INFORMATION TECHNOLO
   2004, SIGNAL PROCESS IMAGE, V19
NR 30
TC 59
Z9 66
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2005
VL 7
IS 3
BP 454
EP 462
DI 10.1109/TMM.2005.846798
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 949OO
UT WOS:000230798200008
DA 2024-07-18
ER

PT J
AU Fu, SL
   Gutierrez-Osuna, R
   Esposito, A
   Kakumanu, PK
   Garcia, ON
AF Fu, SL
   Gutierrez-Osuna, R
   Esposito, A
   Kakumanu, PK
   Garcia, ON
TI Audio/visual mapping with cross-modal hidden Markov models
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3-D audio/video processing; joint media and multimodal processing;
   speech reading and lip synchroization
ID TO-VISUAL CONVERSION; FACIAL ANIMATION; SPEECH
AB The audio/visual mapping problem of speech-driven facial animation has intrigued researchers for years. Recent research efforts have demonstrated that hidden Markov model (HMM) techniques, which have been applied successfully to the problem of speech recognition, could achieve a similar level of success in audio/visual mapping problems. A number of HMM-based methods have been proposed and shown to be effective by the respective designers, but it is yet unclear how these techniques compare to each other on a common test bed. In this paper, we quantitatively compare three recently proposed cross-modal HMM methods, namely the remapping HMM (R-HMM), the least-mean-squared HMM (LMS-HMM), and HMM inversion (HMMI). The objective of our comparison is not only to highlight the merits and demerits of different mapping designs, but also to study the optimality of the acoustic representation and HMM structure for the purpose of speech-driven facial animation. This paper presents a brief overview of these models, followed by an analysis of their mapping capabilities on a synthetic dataset. An empirical comparison on an experimental audio-visual dataset consisting of 75 TIMIT sentences is finally presented. Our results show that HMMI provides the best performance, both on synthetic and experimental audio-visual data.
C1 Univ Delaware, Dept Elect & Comp Engn, Newark, DE 19716 USA.
   Texas A&M Univ, Dept Comp Sci, College Stn, TX 77843 USA.
   Univ Naples 2, Dept Psychol, Naples, Italy.
   Wright State Univ, Dept Comp Sci & Engn, Dayton, OH 45435 USA.
   Univ N Texas, Coll Engn, Denton, TX 76203 USA.
C3 University of Delaware; Texas A&M University System; Texas A&M
   University College Station; Universita della Campania Vanvitelli;
   University System of Ohio; Wright State University Dayton; University of
   North Texas System; University of North Texas Denton
RP Univ Delaware, Dept Elect & Comp Engn, Newark, DE 19716 USA.
EM fu@ee.udel.edu; rgutier@cs.tamu.edu; iiass.anna@tin.it;
   kpraveen@cs.wright.edu; garcia@unt.edu
RI Esposito, Anna/GWC-6719-2022; ESPOSITO, Anna/P-4018-2015
OI ESPOSITO, Anna/0000-0002-7268-1795; Gutierrez-Osuna,
   Ricardo/0000-0003-2817-2085
CR [Anonymous], 1999, AUDITORYVISUAL SPEEC
   Aversano G, 2001, PROCEEDINGS OF THE 44TH IEEE 2001 MIDWEST SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOLS 1 AND 2, P516, DOI 10.1109/MWSCAS.2001.986241
   BAUM LE, 1968, PAC J MATH, V27, P211, DOI 10.2140/pjm.1968.27.211
   Brand M, 1999, NEURAL COMPUT, V11, P1155, DOI 10.1162/089976699300016395
   Brand M., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1237, DOI 10.1109/ICCV.1999.790422
   BRAND M, 1909, P SIGGRAPH 99 LOS AN, P21
   Bregler C., 1997, P 24 ANN C COMP GRAP, V31, P353, DOI DOI 10.1145/258734.258880
   Chen TH, 2001, IEEE SIGNAL PROC MAG, V18, P9
   Choi KH, 2001, J VLSI SIG PROC SYST, V29, P51, DOI 10.1023/A:1011171430700
   COHEN MM, 1998, P AUD VIS SPEECH PER, P201
   Datta B.N., 1995, Numerical Linear Algebra and Applications
   FU S, 2002, THESIS WRIGHT STATE
   Fukunaga K., 1990, Introduction to StatisticalPattern Recognition
   Garofolo J., 1988, Getting started with the DARPA TIMIT CD-ROM: An acoustic phonetic continuous speech database
   GOLDSCHEN AJ, 1993, THESIS G WASHINGTON
   Gutierrez-Osuna R, 2005, IEEE T MULTIMEDIA, V7, P33, DOI 10.1109/TMM.2004.840611
   Hong PY, 2002, IEEE T NEURAL NETWOR, V13, P916, DOI 10.1109/TNN.2002.1021892
   KAKUMANU P, 2002, THESIS WRIGHT STATE
   Koenen Rob., 1999, Overview of the MPEG-4 Standard
   Kyoung Ho Choi, 1999, 1999 IEEE Third Workshop on Multimedia Signal Processing (Cat. No.99TH8451), P175, DOI 10.1109/MMSP.1999.793816
   LAVAGETTO F, 1995, IEEE T REHABIL ENG, V3, P1
   LI Y, 2002, P 5 AS C COMP VIS ME
   Moon S, 1997, IEEE T NEURAL NETWOR, V8, P194, DOI 10.1109/72.557656
   MORISHIMA S, 1991, IEEE J SEL AREA COMM, V9, P594, DOI 10.1109/49.81953
   MORISHIMA S, 1998, P AVSP 98, P195
   O'Shaughnessy D., 2000, SPEECH COMMUN
   Pandzic IS, 1999, VISUAL COMPUT, V15, P330, DOI 10.1007/s003710050182
   Parke F., 1996, COMPUTER FACIAL ANIM
   Rabiner L.R., 1993, FUNDAMENTALS SPEECH, VVolume 14
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Rao RR, 1998, IEEE T IND ELECTRON, V45, P15, DOI 10.1109/41.661300
   Yamamoto E, 1998, SPEECH COMMUN, V26, P105, DOI 10.1016/S0167-6393(98)00054-5
NR 32
TC 41
Z9 46
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2005
VL 7
IS 2
BP 243
EP 252
DI 10.1109/TMM.2005.843341
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 909OO
UT WOS:000227869400006
DA 2024-07-18
ER

PT J
AU Song, HJ
   Kuo, CCJ
AF Song, HJ
   Kuo, CCJ
TI A region-based H.263+codec and its rate control for low VBR video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
ID BIT-RATE VIDEO; COMPRESSION; IMAGE; QUANTIZATION; ALLOCATION;
   PERCEPTION; SELECTION
AB This paper presents a region-based video codee, which is compatible with the H.263+ standard, and its associated rate control algorithm for low variable-bit-rate (VBR) video. The proposed region-based coding scheme is a hybrid method that incorporates traditional block DCT coding as well as object-based coding. To achieve this, we adopt H.263+ as the platform, and develop a fast macroblock-based segmentation method to implement the new region-based codec. The associated rate control solution includes rate control in three levels: encoding frame selection, frame-layer rate control and macroblock-layer rate control. The goal is to improve human visual perceptual quality at low bit rates. The efficiency of the proposed rate control algorithm applied to the region-based H.263+ codec is demonstrated via several typical test sequences.
C1 Hongik Univ, Sch Elect Engn, Seoul, South Korea.
   Univ So Calif, Integrated Multimedia Syst Ctr, Los Angeles, CA 90089 USA.
   Univ So Calif, Dept Elect Engn Syst, Los Angeles, CA 90089 USA.
C3 Hongik University; University of Southern California; University of
   Southern California
RP Hongik Univ, Sch Elect Engn, Seoul, South Korea.
EM hwangjun@wow.hongik.ac.kr; cckuo@sipi.usc.edu
RI Kuo, C.-C. Jay/A-7110-2011
OI Kuo, C.-C. Jay/0000-0001-9474-5035
CR [Anonymous], 1998, Video coding for low bitrate communication
   Balasubramanian R., 1994, Journal of Electronic Imaging, V3, P45, DOI 10.1117/12.165065
   Benois J., 1994, Proceedings of the SPIE - The International Society for Optical Engineering, V2308, P1074, DOI 10.1117/12.185867
   Chen HT, 1996, IEEE T CONSUM ELECTR, V42, P781, DOI 10.1109/30.536185
   Chiang TH, 1997, IEEE T CIRC SYST VID, V7, P246, DOI 10.1109/76.554439
   CHOI JH, 1994, IEEE T IMAGE PROCESS, V3, P546, DOI 10.1109/83.334986
   COVER TM, 1992, ELEMENTS INFORMATION
   Ding W, 1996, IEEE T CIRC SYST VID, V6, P12, DOI 10.1109/76.486416
   Doulamis N, 1998, IEEE T CIRC SYST VID, V8, P928, DOI 10.1109/76.736718
   Fukuhara T, 1997, IEEE T CIRC SYST VID, V7, P212, DOI 10.1109/76.554432
   GU C, 1994, P ICIP 94, V2, P418
   Hartung J, 1998, IEEE J SEL AREA COMM, V16, P42, DOI 10.1109/49.650919
   HWANG J, 1998, IEEE 2 WORKSH MULT S
   Jain A. K., 1989, Fundamentals of Digital Image Processing
   JAYANT N, 1993, P IEEE, V81, P1385, DOI 10.1109/5.241504
   KUO T, 1998, P SPIE INT S OPT SCI
   LEE JW, 1994, IEEE T IMAGE PROCESS, V3, P513, DOI 10.1109/83.334989
   LIN DW, 1997, P IEEE INT C IM PROC, V2, P29
   Lin LJ, 1996, P SOC PHOTO-OPT INS, V2727, P111, DOI 10.1243/PIME_PROC_1996_210_178_02
   MUKHERJEE D, 1997, P IEEE INT C IM PROC, V2, P37
   MUSMANN HG, 1989, SIGNAL PROCESS-IMAGE, P117
   NG KT, 1996, P IEEE INT C IM PROC
   OEHLER K, 1997, P IEEE INT C IM PROC, V1, P365
   ORCHARD MT, 1994, IEEE T IMAGE PROCESS, V3, P693, DOI 10.1109/83.334974
   Ortega A, 1998, IEEE SIGNAL PROC MAG, V15, P23, DOI 10.1109/79.733495
   ORTEGA A, 1994, IEEE T IMAGE PROCESS, V3, P26, DOI 10.1109/83.265978
   PRATT W.K., 1991, DIGITAL IMAGE PROCES, V2
   RAMCHANDRAN K, 1994, IEEE T IMAGE PROCESS, V3, P533, DOI 10.1109/83.334987
   Ribas-Corbera J, 1999, IEEE T CIRC SYST VID, V9, P172, DOI 10.1109/76.744284
   SCHUSTER G, 1996, P SPIE VIS COMM IM P
   SHOHAM Y, 1988, IEEE T ACOUST SPEECH, V36, P1445, DOI 10.1109/29.90373
   Song H, 1999, SIGNAL PROCESS-IMAGE, V15, P127, DOI 10.1016/S0923-5965(99)00027-2
   Song HJ, 2001, IEEE T CIRC SYST VID, V11, P512, DOI 10.1109/76.915357
   SU JK, 2000, P 34 AS C SIGN SYST
   Tekalp A.M., 1995, DIGITAL VIDEO PROCES
   Wiegand T, 1996, IEEE T CIRC SYST VID, V6, P182, DOI 10.1109/76.488825
   WILLEMIN P, 1991, IEEE T COMMUN, V39, P1845, DOI 10.1109/26.120170
   Wong KW, 2001, IEEE T CIRC SYST VID, V11, P1128, DOI 10.1109/76.954499
   YANG KH, 1997, P INT C IM PROC SANT, V2, P41
   1997, VIDEO CODEC TEST MOD
NR 40
TC 37
Z9 42
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2004
VL 6
IS 3
BP 489
EP 500
DI 10.1109/TMM.2004.827488
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 819XS
UT WOS:000221350200012
DA 2024-07-18
ER

PT J
AU Taskiran, C
   Chen, JY
   Albiol, A
   Torres, L
   Bouman, CA
   Delp, EJ
AF Taskiran, C
   Chen, JY
   Albiol, A
   Torres, L
   Bouman, CA
   Delp, EJ
TI <i>ViBE</i>:: A compressed video database structured for active browsing
   and search
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE face detection; semantic gap; shot boundary detection; shot
   representation; shot similarity; video browsing; video databases; video
   search
ID SCENE CHANGE DETECTION; FACE DETECTION; SEGMENTATION; IMAGE
AB In this paper, we describe a unique new paradigm for video database management known as ViBE (video indexing and browsing environment). ViBE is a browseable/searchable paradigm for organizing video data containing a large number of sequences. The system first segments video sequences into shots by using a new feature vector known as the Generalized Trace obtained from the DC-sequence of the compressed data. Each video shot is then represented by a hierarchical structure known as the shot tree. The shots are then classified into pseudo-semantic classes that describe the shot content. Finally, the results are presented to the user in an active browsing environment using a similarity pyramid data structure. The similarity pyramid allows the user to view the video database at various levels of detail. The user can also define semantic classes and reorganize the browsing environment based on relevance feedback. We describe how ViBE performs on a database of MPEG sequences.
C1 Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47907 USA.
   Epson Palo Alto Lab, Palo Alto, CA 94304 USA.
   Univ Politecn Valencia, Dept Commun, Valencia, Spain.
   Univ Politecn Cataluna, ES-08034 Barcelona, Spain.
C3 Purdue University System; Purdue University; Universitat Politecnica de
   Valencia; Universitat Politecnica de Catalunya
RP Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47907 USA.
EM taskiran@ecn.purdue.edu; jauyuen@erd.epson.com; alabiol@dcom.upv.es;
   luis@gps.tsc.upc.es; bouman@ecn.purdue.edu; ace@ecn.purdue.edu
RI Albiol, Antonio/ABD-3393-2020; Albiol, Alberto/AAW-2231-2020; Chen,
   Jen-Yeu/K-5770-2012; torres urgell, luis/C-9713-2014; Delp, Edward
   J/C-3616-2013
OI Albiol, Antonio/0000-0002-0679-912X; Albiol,
   Alberto/0000-0002-1970-3289; torres urgell, luis/0000-0001-9141-9875;
   Delp, Edward/0000-0002-2909-7323
CR AIGRAIN P, 1994, COMPUT GRAPH, V18, P93, DOI 10.1016/0097-8493(94)90120-1
   ALBIOL A, 1999, P IEEE INT C IM PROC
   [Anonymous], P SPIE STORAGE RET 6
   Ardebilian M, 1999, P SOC PHOTO-OPT INS, V3846, P46, DOI 10.1117/12.360454
   Ardizzone E, 1996, P SOC PHOTO-OPT INS, V2916, P265, DOI 10.1117/12.257296
   Atkins CB, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P864, DOI 10.1109/ICIP.2001.958257
   Beucher S., 2018, Mathematical Morphology in Image Processing, P433
   Boreczky JS, 1996, P SOC PHOTO-OPT INS, V2670, P170, DOI 10.1117/12.234794
   Boreczky JS, 1998, INT CONF ACOUST SPEE, P3741, DOI 10.1109/ICASSP.1998.679697
   Breiman L, 1998, BIOMETRICS, DOI [10.1201/9781315139470, DOI 10.2307/2530946]
   Chang SF, 1998, IEEE T CIRC SYST VID, V8, P602, DOI 10.1109/76.718507
   Chen JY, 1999, PROC SPIE, V3846, P148, DOI 10.1117/12.368470
   Chen JY, 2000, IEEE T IMAGE PROCESS, V9, P442, DOI 10.1109/83.826781
   CHEN JY, 1998, P SPIE C HUM VIS EL, V3299
   Cheng H, 2001, IEEE T IMAGE PROCESS, V10, P511, DOI 10.1109/83.913586
   COMENAREZ AJ, 1997, P IEEE COMP SOC C CO, P782
   COX IJ, 1996, P INT C PATT REC VIE, V3, P361
   DAILIANAS A, 1995, P SPIE PHOT E 95 INT, P1
   Davies B, 1999, PROC SPIE, V3846, P22, DOI 10.1117/12.360452
   Drew M. S., 2000, Proceedings ACM Multimedia 2000, P365, DOI 10.1145/354384.354534
   Ferman AM, 1998, J VIS COMMUN IMAGE R, V9, P336, DOI 10.1006/jvci.1998.0402
   Garcia C, 1999, IEEE T MULTIMEDIA, V1, P264, DOI 10.1109/6046.784465
   Gargi U, 2000, IEEE T CIRC SYST VID, V10, P1, DOI 10.1109/76.825852
   GELFAND SB, 1991, IEEE T PATTERN ANAL, V13, P163, DOI 10.1109/34.67645
   Haering N, 2000, IEEE T CIRC SYST VID, V10, P857, DOI 10.1109/76.867923
   HAMPAPUR A, P SPIE C STOR RETR I, V3022, P188
   Hanjalic A, 2001, PROC SPIE, V4315, P301, DOI 10.1117/12.410940
   HUANG J, 2000, IEEE INT C MULT EXP
   Huitao Luo, 2000, Proceedings ACM Multimedia 2000, P285
   JAIMES A, 2000, P SPIE C INT IM SAN, V3694
   Jain AK, 1999, MULTIMEDIA SYST, V7, P369, DOI 10.1007/s005300050139
   KUPEEV KY, 2001, P SPIE C STOR RETR M, V4315, P24
   LANCE GN, 1967, COMPUT J, V9, P373, DOI 10.1093/comjnl/9.4.373
   LEE H, 2000, P RIAO 2000 CONT BAS, P1390
   Lee SW, 2000, IEEE T MULTIMEDIA, V2, P240, DOI 10.1109/6046.890059
   Lienhart R, 1998, PROC SPIE, V3656, P290, DOI 10.1117/12.333848
   Nagasaka A., 1995, Visual Database Systems II, P113
   NANG J, 1999, 7 ACM INT MULT C ORL
   Naphade MR, 2000, PROC SPIE, V3972, P168
   NAPHADE MR, 2000, IEEE INT C MULT EXP
   NAPHADE MR, 2000, P SPIE C STOR RETR M, V3972, P426
   OTOOLE C, 1999, 2 UK C IM RETR NEWC
   Patel NV, 1997, PATTERN RECOGN, V30, P583, DOI 10.1016/S0031-3203(96)00114-8
   Pei SC, 1999, IEEE T MULTIMEDIA, V1, P321, DOI 10.1109/6046.807952
   Rowley HA, 1996, ADV NEUR IN, V8, P875
   Salembier P, 2000, SIGNAL PROCESS-IMAGE, V16, P211, DOI 10.1016/S0923-5965(00)00026-6
   Santini S, 1998, 1998 IEEE SECOND WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P3, DOI 10.1109/MMSP.1998.738904
   SHAHRARAY B, 1995, P SOC PHOTO-OPT INS, V2419, P2, DOI 10.1117/12.206348
   Shen K, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pB252
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Srinivasan S, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 1, P388, DOI 10.1109/MMCS.1999.779235
   Sung KK, 1998, IEEE T PATTERN ANAL, V20, P39, DOI 10.1109/34.655648
   Szummer M, 1998, 1998 IEEE INTERNATIONAL WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO DATABASE, PROCEEDINGS, P42, DOI 10.1109/CAIVD.1998.646032
   TASKIRAN C, 2000, P SPIE IS 3T C STOR, P197
   TASKIRAN C, 1998, IEEE INT C AC SPEECH
   TAYCHER L, 1997, INT C VIS INF SAN DI
   Vailaya A., 1999, P 1999 SPIE C STORAG, P415
   VASCONCELOS N, 1997, IEEE COMP SOC C COMP
   WACTLAR HD, 2000, P IMA GINA C MON JAN, P2000
   Wang HL, 1997, IEEE T CIRC SYST VID, V7, P615, DOI 10.1109/76.611173
   WARD JH, 1963, J AM STAT ASSOC, V58, P236, DOI 10.2307/2282967
   Yang MH, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 1, P127, DOI 10.1109/ICIP.1998.723442
   Yeo BL, 1995, IEEE T CIRC SYST VID, V5, P533, DOI 10.1109/76.475896
   Yeung M, 1996, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, P296, DOI 10.1109/MMCS.1996.534991
   Yow KC, 1997, IMAGE VISION COMPUT, V15, P713, DOI 10.1016/S0262-8856(97)00003-6
   Zhao W, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 2, P752, DOI 10.1109/MMCS.1999.778579
   Zhong D, 1997, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL I, P21, DOI 10.1109/ICIP.1997.647374
   Zhuang YT, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 1, P866, DOI 10.1109/ICIP.1998.723655
   [No title captured]
   [No title captured]
NR 70
TC 43
Z9 46
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2004
VL 6
IS 1
BP 103
EP 118
DI 10.1109/TMM.2003.819783
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 765QP
UT WOS:000188295200008
DA 2024-07-18
ER

PT J
AU Liang, YJ
   Färber, N
   Girod, B
AF Liang, YJ
   Färber, N
   Girod, B
TI Adaptive playout scheduling and loss concealment for voice communication
   over IP networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE jitter absorption; loss concealment; playout scheduling; time-scale
   modification; voice over IP
ID WAVE-FORM SUBSTITUTION; PACKET
AB The quality of service limitation of today's Internet is a major challenge for real-time voice communications. Excessive delay, packet loss, and high delay jitter all impair the communication quality. A new receiver-based playout scheduling scheme is proposed to improve the tradeoff between buffering delay and late loss for real-time voice communication over IT networks. In this scheme the network delay is estimated from past statistics and the playout time of the voice packets is adaptively adjusted. In contrast to previous work, the adjustment is not only performed between talkspurts, but also within talkspurts in a highly dynamic way. Proper reconstruction of continuous playout speech is achieved by scaling individual voice packets using a time-scale modification technique based on the Waveform Similarity Overlap-Add (WSOLA) algorithm. Results of subjective listening tests show that this operation does not impair audio quality, since the adaptation process requires infrequent scaling of the voice packets and low playout jitter is perceptually tolerable. The same time-scale modification technique is also used to conceal packet loss at very low delay, i.e., one packet time. Simulation results based on Internet measurements show that the tradeoff between buffering delay and late loss can be improved significantly. The overall audio quality is investigated based on subjective listening tests, showing typical gains of 1 on a 5-point scale of the Mean Opinion Score.
C1 Stanford Univ, Dept Elect Engn, Informat Syst Lab, Stanford, CA 94305 USA.
C3 Stanford University
RP Stanford Univ, Dept Elect Engn, Informat Syst Lab, Stanford, CA 94305 USA.
EM yiliang@stanfordalumni.org
CR Bolot JC, 1999, IEEE INFOCOM SER, P1453, DOI 10.1109/INFCOM.1999.752166
   DeLeon P, 1999, INT CONF ACOUST SPEE, P3097, DOI 10.1109/ICASSP.1999.757496
   Gibbon JF, 1996, IEEE J SEL AREA COMM, V14, P1376, DOI 10.1109/49.536486
   GOODMAN DJ, 1986, IEEE T ACOUST SPEECH, V34, P1440, DOI 10.1109/TASSP.1986.1164984
   Hogg R. V., 1997, Probability and statistical inference
   Liang YJ, 2001, INT CONF ACOUST SPEE, P1445, DOI 10.1109/ICASSP.2001.941202
   Liao WT, 2001, IEEE INFOCOM SER, P815, DOI 10.1109/INFCOM.2001.916272
   MILLS D, 1989, RFC1129
   Moon SB, 1998, MULTIMEDIA SYST, V6, P17, DOI 10.1007/s005300050073
   Perkins C, 1998, IEEE NETWORK, V12, P40, DOI 10.1109/65.730750
   Pinto J., 1999, Proceedings 24th Conference on Local Computer Networks. LCN'99, P224, DOI 10.1109/LCN.1999.802033
   RAMJEE R, 1994, IEEE INFOCOM SER, P680, DOI 10.1109/INFCOM.1994.337672
   Rosenberg J., 2000, Proceedings IEEE INFOCOM 2000. Conference on Computer Communications. Nineteenth Annual Joint Conference of the IEEE Computer and Communications Societies (Cat. No.00CH37064), P1705, DOI 10.1109/INFCOM.2000.832570
   Sanneck H, 1996, IEEE GLOBECOM 1996 - GLOBAL INTERNET'96, CONFERENCE RECORD, P48, DOI 10.1109/GLOCOM.1996.586117
   Sreenan CJ, 2000, IEEE T MULTIMEDIA, V2, P88, DOI 10.1109/6046.845013
   STENGER A, 1996, P EUR SIGN PROC C, V3, P1965
   Verhelst W., 1993, PROC IEEE INT C ACOU, V2, P554
   Wah BW, 1999, IEEE T MULTIMEDIA, V1, P342, DOI 10.1109/6046.807954
   WASEM OJ, 1988, IEEE T ACOUST SPEECH, V36, P342, DOI 10.1109/29.1530
   Xie Y, 1999, MULTIMEDIA SYST, V7, P326, DOI 10.1007/s005300050134
NR 20
TC 80
Z9 108
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2003
VL 5
IS 4
BP 532
EP 543
DI 10.1109/TMM.2003.819095
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 742VA
UT WOS:000186537700004
DA 2024-07-18
ER

PT J
AU Frost, VS
AF Frost, VS
TI Quantifying the temporal characteristics of network congestion events
   for multimedia services
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE network congestion; prediction of quality-of-service; (QoS); QoS
ID SPEECH
AB Effective quality-of-service (QoS) metrics must relate to end-user experience. For multimedia services these metrics should focus on phenomena that are observable by the end user. Once a congestion event occurs in the network it tends to persist, resulting in long bursts of consecutive packet loss. Such an event is observable to the network customer. There is a need to increase our understanding of the temporal characteristics of congestion. It has become increasingly apparent that the temporal characteristics of congestion events have the dominant effect on user-perceived QoS. A rigorous definition of the time between congestion events is given here, as well as an associated prediction methodology. The inter-congestion event time or the rate of congestion events per unit time provides a network quality metric that is easily understandable to network users and is conveniently predicted and measured. The contribution of this paper is the definition of a metric to characterize congestion events and development of an analytic methodology to predict the expected number of congestion events per unit time. The proposed methodology is evaluated for a variety of traffic models.
C1 Univ Kansas, Dept Elect Engn & Comp Sci, Informat & Telecommun Technol Ctr, Lawrence, KS 66045 USA.
C3 University of Kansas
RP Univ Kansas, Dept Elect Engn & Comp Sci, Informat & Telecommun Technol Ctr, Lawrence, KS 66045 USA.
EM frost@eecs.ku.edu
CR Adas A, 1997, IEEE COMMUN MAG, V35, P82, DOI 10.1109/35.601746
   [Anonymous], 1989, Probability Approximations via the Poisson Clumping Heuristic
   Barakat C, 2001, IEEE NETWORK, V15, P38, DOI 10.1109/65.923939
   BOLELLA M, 1998, P INT C PAR PROC AUG, P3
   DASILVA LA, 1989, IEEE T ACOUST SPEECH, V37, P1597, DOI 10.1109/29.35400
   DUPUIS A, 1996, 3010 INRIA
   FIORINI PM, 1998, THESIS U CONNECTICUT
   GUILLEMIN F, 1995, ADV APPL PROBAB, V27, P862, DOI 10.2307/1428137
   GUILLEMIN F, 1994, INRIA PUBL, V874
   HICHIEL H, 1997, P IEEE, V85, P2007
   Jagerman D.L., 1994, PROC C INFO SCI SYST, V1, P24
   JAGERMAN DL, 1994, COMMUN STAT STOCH MO, V10, P831
   JAGERMAN DL, STOCH MODELS, V8, P193
   JIANG W, 2000, P NOSSDAV CHAP HILL
   Kleinrock L., 1975, Queueing Systems-Volume 1: Theory, V1
   KOODLI R, 2000, INTERNET ENG     NOV
   KOSTAS T, 1998, IEEE NETWORK     JAN
   Larson HaroldJ., 1979, PROBABILISTIC MODELS, V2
   LAZAROU G, 2000, THESIS U KANSAS LAWR
   MARKOPOULOU AP, 2002, P INFOCOM 2002 NEW Y
   Markovski V., 2000, Proceedings of the 2000 Symposium on Performance Evaluation of Computer and Telecommunication Systems, P278
   Nelson R., 1995, PROBABILITY STOCHAST
   PETR DW, 1989, IEEE J SEL AREA COMM, V7, P644, DOI 10.1109/49.32328
   Roberts J., 1996, BROADBAND NETWORK TE
   SEVEIK P, 2002, PAN INT QOS TECHN ST
   SOMONIAN A, 1992, P IEEE GLOB 92
   Taylor HM, 1998, An Introduction to Stochastic Modeling, Vthird
   Tierney BL, 2001, 10TH IEEE INTERNATIONAL SYMPOSIUM ON HIGH PERFORMANCE DISTRIBUTED COMPUTING, PROCEEDINGS, P281, DOI 10.1109/HPDC.2001.945196
   WIJATA YI, 2000, IEEE COMMUN MAG  SEP
   Yajnik M, 1999, IEEE INFOCOM SER, P345, DOI 10.1109/INFCOM.1999.749301
   1995, MULTIMEDIA COMMUNICA
NR 31
TC 5
Z9 5
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2003
VL 5
IS 3
BP 458
EP 465
DI 10.1109/TMM.2003.813276
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 714BM
UT WOS:000184892500016
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Hori, C
   Furui, S
AF Hori, C
   Furui, S
TI A new approach to automatic speech summarization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE dynamic programming; objective evaluation; speech summarization;
   summarization scores
AB This paper proposes a new automatic speech summarization method. In this method, a set of words maximizing a summarization score is extracted from automatically transcribed speech. This extraction is performed according to a target compression ratio using a dynamic programming (DP) technique. The extracted set of words is then connected to build a summarization sentence. The summarization score consists of a word significance measure, a confidence measure, linguistic likelihood, and a word concatenation probability. The word concatenation score is determined by a dependency structure in the original speech given by stochastic dependency context free grammar (SDCFG,). Japanese broadcast news speech transcribed using a large-vocabulary continuous-speech recognition (LVCSR) system is summarized using our proposed method and compared with manual summarization by human subjects. The manual summarization results are combined to build a word network. This word network is used to calculate the word accuracy of each automatic summarization result using the most similar word string in the network. Experimental results show that the proposed method effectively extracts relatively important information by removing redundant and irrelevant information.
C1 Tokyo Inst Technol, Grad Sch Informat Sci & Engn, Dept Comp Sci, Tokyo 1528552, Japan.
   NTT Corp, Commun Sci Labs, Kyoto 6190237, Japan.
C3 Tokyo Institute of Technology; Nippon Telegraph & Telephone Corporation
RP Tokyo Inst Technol, Grad Sch Informat Sci & Engn, Dept Comp Sci, Tokyo 1528552, Japan.
EM chiori@furui.cs.titech.ac.jp; furui@furui.cs.titech.ac.jp
CR [Anonymous], 2000, FDN STAT NATURAL LAN
   FURUI S, 2000, P ICSLP2000 6 INT C
   Hori C, 2000, INT CONF ACOUST SPEE, P1579, DOI 10.1109/ICASSP.2000.861983
   HORI C, 2000, P ICSLP2000 BEIJ CHI
   IMAI T, 2000, P ICSLP2000 6 INT C
   ITO A, 2000, P ICSLP 2000 BEIJ CH
   Kemp T., 1997, Proc. of Eurospeech, P827
   Lari K., 1990, Computer Speech and Language, V4, P35, DOI 10.1016/0885-2308(90)90022-X
   MANU I, 1999, ADV AUTOMATIC TEXT S
   VALENZA R, 2000, P ESCA WORKSH ACC IN, P111
   Valtchev V, 1997, SPEECH COMMUN, V22, P303, DOI 10.1016/S0167-6393(97)00029-0
NR 11
TC 36
Z9 42
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2003
VL 5
IS 3
BP 368
EP 378
DI 10.1109/TMM.2003.813274
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 714BM
UT WOS:000184892500009
OA Green Published
DA 2024-07-18
ER

PT J
AU Martinian, E
   Sundberg, CEW
AF Martinian, E
   Sundberg, CEW
TI Decreasing distortion using low delay codes for bursty packet loss
   channels
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE convolutional codes; erasure channels; low delay; channel coding;
   multidescriptive coding; packet loss; VoIP
ID MULTICAST
AB The strict delay constraints of real-time communication applications in packet networks limit the use of Automatic Repeat Request (ARQ) (retransmission systems) and error correction codes with extensive interleaving and decoding over long intervals. Since packet losses can introduce significant impairments, we study the effectiveness of low delay channel coding techniques to increase transmission quality across links with bursty losses.
   Specifically, we consider the benefits of the newly discovered class of low delay convolutional codes known as maximally short codes. By analyzing a Gaussian source transmitted over a Gilbert-Elliott channel, we demonstrate that these codes can achieve significant gains in comparison to uncoded transmission schemes or traditional coded schemes employing Reed-Solomon block codes. To complement and validate the theoretical analysis we also present results from informal listening tests with a Voice over IP application.
C1 MIT, Cambridge, MA 02139 USA.
   iBiquity Digital Corp, Warren, NJ 07059 USA.
C3 Massachusetts Institute of Technology (MIT)
RP MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
EM emin@allegro.mit.edu; sundberg@ibiquity.com
CR Albanese A, 1996, IEEE T INFORM THEORY, V42, P1737, DOI 10.1109/18.556670
   Altman E, 1998, IEEE J SEL AREA COMM, V16, P778, DOI 10.1109/49.700912
   ALTMAN E, 2001, INFOCOM 2001, V2, P796
   Arai M., 1999, Proceedings 1999 Pacific Rim International Symposium on Dependable Computing, P260, DOI 10.1109/PRDC.1999.816237
   Bolot J., 1993, Proc. ACM SIGCOMM 1993, P289
   BOLOT J, 1999, P INF 99 MAR
   Borella MS, 1998, PROCEEDINGS OF THE 1998 ICPP WORKSHOPS ON ARCHITECTURAL AND OS SUPPORT FOR MULTIMEDIA APPLICATIONS - FLEXIBLE COMMUNICATION SYSTEMS - WIRELESS NETWORKS AND MOBILE COMPUTING, P3, DOI 10.1109/ICPPW.1998.721868
   Byers J.W., 1998, P ACM SIGCOMM 98 C A, P56
   Chou PA, 2001, IEEE T MULTIMEDIA, V3, P108, DOI 10.1109/6046.909598
   Cover Thomas A., 1991, ELEM INF THEORY, DOI 10.1002/047174882X
   FORNEY GD, 1971, IEEE T COMMUN TECHN, VCO19, P772, DOI 10.1109/TCOM.1971.1090719
   Garcia A.V., FREEPHONE AUDIO TOOL
   GOODMAN DJ, 1983, AT&T TECH J, V62, P2017, DOI 10.1002/j.1538-7305.1983.tb03528.x
   HINDELANG T, 2000, IEEE 51 VEH TECHN C, V2, P1210
   JIANG W, 1999, QOS MEASUREMENT INTE
   Johannesson R., 1999, FUNDAMENTALS CONVOLU
   KONRAD A, 2001, P 4 ACM INT WORKSH M
   Lee CC, 2000, 2000 IEEE WORKSHOP ON SPEECH CODING, PROCEEDINGS, P69, DOI 10.1109/SCFT.2000.878399
   Lin S., 1983, Error Control Coding: Fundamentals and Applications
   MARTINIAN E, UNPUB IEEE T INFORM
   MARTINIAN E, 2002, P INT C COMM NEW YOR
   Paxson V, 1999, IEEE ACM T NETWORK, V7, P277, DOI 10.1109/90.779192
   Sanneck H. A., 2000, P SPIE ACM SIGMM MUL
   SCHUSTER GM, 1997, Patent No. 5870412
   Tan WT, 2001, IEEE T CIRC SYST VID, V11, P373, DOI 10.1109/76.911162
   *U COLL LOND, RAT ROB AUD TOLL MUL
   Yajnik M, 1999, IEEE INFOCOM SER, P345, DOI 10.1109/INFCOM.1999.749301
NR 27
TC 8
Z9 9
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2003
VL 5
IS 3
BP 285
EP 292
DI 10.1109/TMM.2003.814794
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 714BM
UT WOS:000184892500001
DA 2024-07-18
ER

PT J
AU Zoia, G
   Alberti, C
AF Zoia, G
   Alberti, C
TI A virtual DSP architecture for audio applications from a complexity
   analysis of MPEG-4 structured audio
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE audio coding; computational complexity; computer music; digital signal
   processors; standardization
AB The MPEG-4 audio standard provides several toolsets for natural and synthetic sound coding. Among them, the most innovative in terms of multimedia applications is Structured Audio (SA), which implements a high level, structured description of sound instead of the usual compression techniques based on psychoacoustics and subband analysis. SA permits to encode synthesis and processing algorithms by its Structured Audio Orchestra Language (SAOL), and it can theoretically be used to specify any other audio decoder. This great flexibility introduces a challenging implementation problem, which in a normative framework has to be solved by a systematic approach.
   In the first part of the paper, it is described how the SA decoding process can be analyzed in a platform independent way in order to determine the fundamental figures of this coding technique; it is then shown how the proposed method is being used for the MPEG-4 SA conformance test. In the second part the design of a virtual digital signal processor (DSP) architecture is presented, based on the results of the complexity analysis. This architecture is able to exploit the intrinsic data level parallelism of many Audio algorithms and to consistently reduce the implementation cost. Experimental results prove the effectiveness of the approach and its suitability for implementations on modern superscalar DSPs and multimedia processors.
C1 Swiss Fed Inst Technol, Signal Proc Inst, CH-1015 Lausanne, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne
RP Zoia, G (corresponding author), Swiss Fed Inst Technol, Signal Proc Inst, CH-1015 Lausanne, Switzerland.
EM Giorgio.Zoia@epfl.ch; Claudio.Alberti@epfl.ch
CR Begault D. R., 1994, 3 D SOUND VIRTUAL RE, P117
   Dannenberg RB, 1997, COMPUT MUSIC J, V21, P83, DOI 10.2307/3681016
   ESPASA R, 1997, IEEE MICRO       SEP, P20
   FLYNN MJ, 1997, COMPUTER ARCHITECTUR, P425
   *ISO IEC, 144961 ISOIEC
   *ISO IEC, 1449643 ISOIEC
   *ISO IEC, 1449635 ISOIEC
   LAPSLEY P, 1997, DSP PROCESSOR FUNDAM, P29
   Lindholm Tim., 1999, JAVA VIRTUAL MACHINE, V2nd
   *MIT, MED LAB MPEG 4 STRUC
   PIMENTEL A, 1998, P 24 ACM IEEE INT S
   POPE ST, 1993, COMPUT MUSIC J, V17, P23, DOI 10.2307/3680868
   ROADS C, 1996, COMPUTER MUSIC TUT 2
   SANDER M, OPTIVEC LIB VECTOR M
   Scheirer ED, 1999, COMPUT MUSIC J, V23, P31, DOI 10.1162/014892699559742
   SCHEIRER ED, 1999, 17 INT C HIGH QUAL A
   Scheirer ED, 1999, IEEE T MULTIMEDIA, V1, P237, DOI 10.1109/6046.784463
   Vercoe B., 1993, CSOUND MANUAL AUDIO
   Zoia G, 2001, PROCEEDINGS OF THE AES 19TH INTERNATIONAL CONFERENCE SURROUND SOUND: TECHNIQUES, TECHNOLOGY AND PERCEPTION, P155
   ZOIA G, 2001, THESIS ECOLE POLYTEC
NR 20
TC 0
Z9 0
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2003
VL 5
IS 3
BP 317
EP 328
DI 10.1109/TMM.2003.813277
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 714BM
UT WOS:000184892500004
DA 2024-07-18
ER

PT J
AU Jung, J
   Antonini, M
   Barlaud, M
AF Jung, J
   Antonini, M
   Barlaud, M
TI Optimal decoder for block-transform based video coders
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE blocking effect; cell loss; dropout; M-JPEG; MPEG; object-based
   decoding; optimization; variational approach
ID MISSING DATA; CONCEALMENT; RECONSTRUCTION; PROJECTIONS; ARTIFACTS;
   IMAGES
AB In this paper, we introduce a new decoding algorithm for DCT-based video encoders, such as Motion JPEG (M-JPEG), H26x, or MPEG. This algorithm considers not only the compression artifacts but also the ones due to transmission, acquisition or storage of the video. The novelty of our approach is to jointly tackle these two problems, using a variational approach. The resulting decoder is object-based, allowing independent and adaptive processing of objects and backgrounds, and considers available information provided by the bitstream, such as quantization steps, and motion vectors. Several experiments demonstrate the efficiency of the proposed method. Objective and subjective quality assessment methods are used to evaluate the improvement upon standard algorithms, such as the deblocking and deringing filters included in MPEG-4 postprocessing.
C1 Philips Res France, F-92156 Suresnes, France.
   Univ Nice, CNRS, UMR 6070, Lab 13S, F-06903 Sophia Antipolis, France.
C3 Philips; Philips Research; Universite Cote d'Azur; Centre National de la
   Recherche Scientifique (CNRS)
RP Jung, J (corresponding author), Philips Res France, F-92156 Suresnes, France.
EM joel.jung@philips.com; am@i3s.unce.fr; barlaud@i3s.unice.fr
CR Antonini M, 1992, IEEE T IMAGE PROCESS, V1, P205, DOI 10.1109/83.136597
   Bolot J.-C., 1993, Journal of High Speed Networks, V2, P305
   BRAILEAN JC, 1995, IEEE T IMAGE PROCESS, V4, P1236, DOI 10.1109/83.413168
   Caselles V., 1995, INT C COMPUTER VISIO
   CAVIEDES JE, 2001, 5 WORLD MULT SYST CY
   CAVIEDES JE, 2001, P PCS 2001 SEOUL APR, P89
   Charbonnier P, 1997, IEEE T IMAGE PROCESS, V6, P298, DOI 10.1109/83.551699
   DERVIAUX C, 1996, INT C IMAGE PROCESSI, V1, P5
   EBBECKE M, 1997, IEEE INT C IM PROC, V2, P402
   Gersho A., 1990, VECTOR QUANTIZATION
   Ghanbari M, 1993, IEEE T CIRC SYST VID, V3, P238, DOI 10.1109/76.224234
   GREEN PJ, 1990, IEEE T MED IMAGING, V1, P194
   GU C, 1995, MULTIMEDIA COMMUNICA
   HEMAMI SS, 1995, IEEE T IMAGE PROCESS, V4, P1023, DOI 10.1109/83.392344
   Hoshi T, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 2, P478, DOI 10.1109/ICIP.1998.723433
   HSUNG T, 1996, INT C IMAGE PROCESSI, V2, P561
   *INT ORG STAND, 1999, COD MOV PICT AUD
   JEHANBESSON S, 2001, P SPIE EL IM SAN JOS
   JEHANBESSON S, 2000, ANN TELECOMMUN, V55, P101
   JEHANBESSON S, 1999, Patent No. 9907443
   JEHANBESSON S, 2001, INT C IM PROC THESSL
   JEHANBESSON S, 2001, INT C IM PROC THESS
   JEHANBESSON S, 2002, EUR C COMP VIS COP D
   JEHANBESSON S, 2000, IST SPIE 12 INT S SA
   Jung J, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 1, P410, DOI 10.1109/ICIP.1998.723513
   KOKARAM AC, 1995, IEEE T IMAGE PROCESS, V4, P1496, DOI 10.1109/83.469931
   KOKARAM AC, 1995, IEEE T IMAGE PROCESS, V4, P1509, DOI 10.1109/83.469932
   Kornprobst P., 1998, Computer Vision - ECCV'98. 5th European Conference on Computer Vision. Proceedings, P548, DOI 10.1007/BFb0054764
   Lee YL, 1998, IEEE T IMAGE PROCESS, V7, P229, DOI 10.1109/83.661000
   MINAMI S, 1995, IEEE T CIRC SYST VID, V5, P74, DOI 10.1109/76.388056
   Nadenau M. J., 1996, 5 INT WORKSH TIM VAR
   PAEK H, 1995, INT C IMAGE PROCESSI, V3, P208
   Park HW, 1999, IEEE T CIRC SYST VID, V9, P161, DOI 10.1109/76.744283
   RAMAMURTHI B, 1986, IEEE T ACOUST SPEECH, V34, P1258, DOI 10.1109/TASSP.1986.1164961
   Salama P, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL II, P49, DOI 10.1109/ICIP.1996.560598
   SUN HF, 1995, IEEE T IMAGE PROCESS, V4, P470, DOI 10.1109/83.370675
   Tramini S, 1998, INT J IMAG SYST TECH, V9, P369, DOI 10.1002/(SICI)1098-1098(1998)9:5<369::AID-IMA7>3.0.CO;2-8
   TRAMINI S, 1998, INT C IMAGE PROCESSI, V1, P381
   VASS J, 1998, INT C IMAGE PROCESSI, V1, P958
   Wang Y, 1998, P IEEE, V86, P974, DOI 10.1109/5.664283
   WEBB JLH, 1997, INT C IMAGE PROCESSI, V2, P9
   Wu H.R., 1996, Proc. of Picture Coding Symposium, V1, P23
   YANG SF, 1997, VIS COMMUN IMAGE FEB
   Yang YY, 1993, IEEE T CIRC SYST VID, V3, P421, DOI 10.1109/76.260198
   Yang YY, 1997, IEEE T IMAGE PROCESS, V6, P1345, DOI 10.1109/83.624945
   1999, Patent No. 09406673
   1999, FINAL REPORT VIDEO Q
NR 47
TC 1
Z9 1
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2003
VL 5
IS 2
BP 145
EP 160
DI 10.1109/TMM.2003.811616
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 695HB
UT WOS:000183824100001
DA 2024-07-18
ER

PT J
AU Chen, L
   Liu, J
   Chen, WD
   Du, B
AF Chen, Liang
   Liu, Jun
   Chen, Weidong
   Du, Bo
TI A GLRT-Based Multi-Pixel Target Detector in Hyperspectral Imagery
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Target detection; hyperspectral imagery; generalized likelihood ratio
   criterion; multi-pixel target
ID ADAPTIVE RADAR DETECTION; SPARSE REPRESENTATION; DETECTION ALGORITHMS;
   UNIFYING FRAMEWORK; ANOMALIES
AB In hyperspectral imagery, target detection algorithms are usually based on the spectral signature information. Due to the advance of the spatial resolution of hyperspectral sensors, the ground sample distance may be much smaller than the size of targets. As a result, targets often occupy multiple consecutive pixels, which are referred to as multi-pixel targets. In this paper, we investigate the target detection problem for multi-pixel targets in hyperspectral imagery, when the target spectral signature is known. Jointly exploiting the pixels occupied by a target of interest, we propose a multi-pixel target detector resorting to the generalized likelihood ratio test criterion. Closed-form expressions for the probabilities of the false alarm and detection are derived, which are verified using Monte Carlo simulations. Experimental results on four real hyperspectral datasets show that the proposed detector outperforms its counterparts.
C1 [Chen, Liang; Liu, Jun; Chen, Weidong] Univ Sci & Tech nol China, Dept Elect Engn & Informat Sci, Hefei 230027, Peoples R China.
   [Du, Bo] Wuhan Univ, Sch Comp Sci, Wuhan 430079, Peoples R China.
C3 Wuhan University
RP Liu, J (corresponding author), Univ Sci & Tech nol China, Dept Elect Engn & Informat Sci, Hefei 230027, Peoples R China.
EM lagchen@mail.ustc.edu.cn; junliu@ustc.edu.cn; wdchen@ustc.edu.cn;
   gunspace@163.com
OI Liu, Jun/0000-0002-7193-0622
FU National Natural Science Foundation of China [61971392]; Youth
   Innovation Promotion Association CAS [CX2100060053]; Science and
   Technology Major Project of Hubei Province (Next-Generation AI
   Technologies) [2019AEA170]; USTC Tang Scholar
FX This work was supported in part by National Natural Science Foundation
   of China under Grant 61971392, in part by Youth Innovation Promotion
   Association CAS under Grant CX2100060053, in part by the Science and
   Technology Major Project of Hubei Province (Next-Generation AI
   Technologies) under Grant 2019AEA170, and in part by USTC Tang Scholar.
CR Acito N, 2013, IEEE T GEOSCI REMOTE, V51, P3475, DOI 10.1109/TGRS.2012.2221128
   Anderson T., 2003, INTRO MULTIVARIATE S
   Bioucas-Dias JM, 2013, IEEE GEOSC REM SEN M, V1, P6, DOI 10.1109/MGRS.2013.2244672
   Bitar AW, 2019, IEEE T GEOSCI REMOTE, V57, P5239, DOI 10.1109/TGRS.2019.2897635
   Broadwater J, 2007, IEEE T PATTERN ANAL, V29, P1891, DOI 10.1109/TPAMI.2007.1104
   Carlotto MJ, 2005, IEEE T GEOSCI REMOTE, V43, P374, DOI 10.1109/TGRS.2004.841481
   Chang CI, 2000, IEEE T INFORM THEORY, V46, P1927, DOI 10.1109/18.857802
   Chang CI, 2005, IEEE T GEOSCI REMOTE, V43, P502, DOI 10.1109/TGRS.2004.839543
   Chen Y, 2011, IEEE J-STSP, V5, P629, DOI 10.1109/JSTSP.2011.2113170
   Chen Y, 2011, IEEE GEOSCI REMOTE S, V8, P676, DOI 10.1109/LGRS.2010.2099640
   Cheng G, 2018, IEEE T GEOSCI REMOTE, V56, P6712, DOI 10.1109/TGRS.2018.2841823
   Cheng G, 2018, IEEE T GEOSCI REMOTE, V56, P2811, DOI 10.1109/TGRS.2017.2783902
   Ciuonzo D, 2016, IEEE T SIGNAL PROCES, V64, P2894, DOI 10.1109/TSP.2016.2519003
   Ciuonzo D, 2016, IEEE T SIGNAL PROCES, V64, P2907, DOI 10.1109/TSP.2016.2519005
   Conte E, 2001, IEEE T SIGNAL PROCES, V49, P1336, DOI 10.1109/78.928688
   Du B, 2017, IEEE T MULTIMEDIA, V19, P67, DOI 10.1109/TMM.2016.2608780
   Du B, 2016, IEEE T IMAGE PROCESS, V25, P5345, DOI 10.1109/TIP.2016.2601268
   DYKSTRA RL, 1970, ANN MATH STAT, V41, P2153, DOI 10.1214/aoms/1177696719
   Gu YF, 2015, NEUROCOMPUTING, V169, P5, DOI 10.1016/j.neucom.2014.09.101
   Guo T, 2020, IEEE GEOSCI REMOTE S, V17, P716, DOI 10.1109/LGRS.2019.2927256
   Hao XH, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12040697
   Harsanyi J.C., 1993, DETECTION CLASSIFICA
   Johnson S, 2002, IEEE T GEOSCI REMOTE, V40, P1326, DOI 10.1109/TGRS.2002.800434
   Kang XD, 2017, IEEE T GEOSCI REMOTE, V55, P5600, DOI 10.1109/TGRS.2017.2710145
   Kelly E. J., 1989, 848 MIT LINC LAB
   KELLY EJ, 1986, IEEE T AERO ELEC SYS, V22, P115, DOI 10.1109/TAES.1986.310745
   Liu YJ, 2017, IEEE T GEOSCI REMOTE, V55, P1967, DOI 10.1109/TGRS.2016.2632863
   Manolakis D., 2003, Lincoln Laboratory Journal, V14, P79
   Manolakis D, 2002, IEEE SIGNAL PROC MAG, V19, P29, DOI 10.1109/79.974724
   Manolakis D, 2001, P SOC PHOTO-OPT INS, V4381, P18, DOI 10.1117/12.437006
   Matteoli S, 2014, IEEE J-STARS, V7, P2317, DOI 10.1109/JSTARS.2014.2315772
   Matteoli S, 2014, IEEE GEOSCI REMOTE S, V11, P163, DOI 10.1109/LGRS.2013.2250907
   Matteoli S, 2013, IEEE T GEOSCI REMOTE, V51, P2837, DOI 10.1109/TGRS.2012.2214392
   Matteoli S, 2010, IEEE AERO EL SYS MAG, V25, P5, DOI 10.1109/MAES.2010.5546306
   Nasrabadi NM, 2014, IEEE SIGNAL PROC MAG, V31, P34, DOI 10.1109/MSP.2013.2278992
   Raghavan RS, 2013, IEEE T SIGNAL PROCES, V61, P3607, DOI 10.1109/TSP.2013.2260332
   REED IS, 1990, IEEE T ACOUST SPEECH, V38, P1760, DOI 10.1109/29.60107
   SCHARF LL, 1994, IEEE T SIGNAL PROCES, V42, P2146, DOI 10.1109/78.301849
   Schaum A., 2009, PROC IEEE APPL IMAGE, P1
   Schweizer SM, 2000, IEEE T INFORM THEORY, V46, P1855, DOI 10.1109/18.857796
   Schweizer SM, 2001, IEEE T IMAGE PROCESS, V10, P584, DOI 10.1109/83.913593
   Shi C, 2020, IEEE T MULTIMEDIA, V22, P487, DOI 10.1109/TMM.2019.2928491
   Theiler J, 2018, INT GEOSCI REMOTE SE, P2773, DOI 10.1109/IGARSS.2018.8517776
   Veracini T, 2011, IEEE GEOSCI REMOTE S, V8, P666, DOI 10.1109/LGRS.2010.2099103
   Vincent F, 2020, IEEE T GEOSCI REMOTE, V58, P4479, DOI 10.1109/TGRS.2020.2965212
   Wang T, 2013, IEEE GEOSCI REMOTE S, V10, P1577, DOI 10.1109/LGRS.2013.2262133
   Wang ZY, 2018, NEUROCOMPUTING, V272, P226, DOI 10.1016/j.neucom.2017.06.068
   Willett RM, 2014, IEEE SIGNAL PROC MAG, V31, P116, DOI 10.1109/MSP.2013.2279507
   Wu X, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11020150
   Xie WY, 2020, IEEE T GEOSCI REMOTE, V58, P1463, DOI 10.1109/TGRS.2019.2947033
   Yang S, 2016, IEEE T IMAGE PROCESS, V25, P2249, DOI 10.1109/TIP.2016.2545248
   Yang XL, 2019, IEEE J-STARS, V12, P2184, DOI 10.1109/JSTARS.2019.2912826
   Yu XL, 1997, IEEE T IMAGE PROCESS, V6, P143, DOI 10.1109/83.552103
   YU XL, 1993, IEEE T SIGNAL PROCES, V41, P2639, DOI 10.1109/78.229895
   Zhang LF, 2014, IEEE T GEOSCI REMOTE, V52, P1030, DOI 10.1109/TGRS.2013.2246837
   Zhang YM, 2017, IEEE GEOSCI REMOTE S, V14, P1923, DOI 10.1109/LGRS.2017.2732454
   Zhang YX, 2015, IEEE T GEOSCI REMOTE, V53, P1346, DOI 10.1109/TGRS.2014.2337883
   Zhou PC, 2019, IEEE T GEOSCI REMOTE, V57, P4823, DOI 10.1109/TGRS.2019.2893180
   Zou ZX, 2016, IEEE T GEOSCI REMOTE, V54, P330, DOI 10.1109/TGRS.2015.2456957
NR 59
TC 3
Z9 3
U1 18
U2 37
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2710
EP 2722
DI 10.1109/TMM.2022.3150185
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA M5LW1
UT WOS:001030640600020
DA 2024-07-18
ER

PT J
AU Cheng, XL
   Zheng, X
   Pei, JL
   Tang, H
   Lyu, ZH
   Chen, CB
AF Cheng, Xiaolong
   Zheng, Xuan
   Pei, Jialun
   Tang, He
   Lyu, Zehua
   Chen, Chuanbo
TI Depth-Induced Gap-Reducing Network for RGB-D Salient Object Detection:
   An Interaction, Guidance and Refinement Approach
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-modality interaction block; interference degree; mutually guided
   cross-level fusion module; RGB-D salient object detection
ID FUSION
AB Depth provides complementary information for salient object detection (SOD). However, the performance of RGB-D SOD methods is usually hindered by low quality depth map, semantic gap cross-modality and intrinsic gap between multi-level features. Although recent RGB-D SOD methods have been embedded into depth quality assessment, these methods do not consider the inconsistency of the depth format across datasets. In this paper, we propose an interpretable and effective mechanism called interference degree (ID) to assess depth quality and reweight the contribution of single-modality features without extra annotation. Then, a cross-modality interaction block (CMIB) is designed to reduce the semantic gap between RGB and depth features with the help of ID mechanism, and a mutually guided cross-level fusion (MGCF) module is designed to reduce the intrinsic gap among multi-level features. Finally, a refinement branch is proposed to enhance the salient regions and suppress the non-salient regions of fused features. Extensive experiments on six benchmark datasets show that the proposed depth-induced gap-reducing network (DIGR-Net) outperforms 20 recent state-of-the-art methods.
C1 [Cheng, Xiaolong; Zheng, Xuan; Tang, He; Lyu, Zehua; Chen, Chuanbo] Huazhong Univ Sci & Technol, Sch Software Engn, Wuhan 430074, Peoples R China.
   [Pei, Jialun] Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan 430074, Peoples R China.
C3 Huazhong University of Science & Technology; Huazhong University of
   Science & Technology
RP Tang, H (corresponding author), Huazhong Univ Sci & Technol, Sch Software Engn, Wuhan 430074, Peoples R China.
EM xiaolongcheng@hust.edu.cn; zxuan@hust.edu.cn; peijl@hust.edu.cn;
   hetang@hust.edu.cn; lvzehua@hust.edu.cn; chuanboc@163.com
RI Pei, jialun/HGA-9920-2022
OI Pei, jialun/0000-0002-2630-2838; Tang, He/0000-0002-8454-1407
FU National Natural Science Foundation of China [61902139]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61902139.
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Bolya D, 2019, IEEE I CONF COMP VIS, P9156, DOI 10.1109/ICCV.2019.00925
   Borji A, 2015, IEEE T IMAGE PROCESS, V24, P5706, DOI 10.1109/TIP.2015.2487833
   Chen CLZ, 2021, IEEE T IMAGE PROCESS, V30, P2350, DOI 10.1109/TIP.2021.3052069
   Chen CLZ, 2020, IEEE T IMAGE PROCESS, V29, P4296, DOI 10.1109/TIP.2020.2968250
   Chen H, 2018, PROC CVPR IEEE, P3051, DOI 10.1109/CVPR.2018.00322
   Chen H, 2019, PATTERN RECOGN, V86, P376, DOI 10.1016/j.patcog.2018.08.007
   Chen K, 2019, PROC CVPR IEEE, P4969, DOI 10.1109/CVPR.2019.00511
   Chen SH, 2018, LECT NOTES COMPUT SC, V11213, P236, DOI 10.1007/978-3-030-01240-3_15
   Chen ZY, 2021, IEEE T IMAGE PROCESS, V30, P7012, DOI 10.1109/TIP.2020.3028289
   Chen ZY, 2020, AAAI CONF ARTIF INTE, V34, P10599
   Cheng Y, 2014, IEEE INT CON MULTI
   Cheng-Feng Sun, 2020, Learning and Collaboration Technologies. Human and Technology Ecosystems. 7th International Conference, LCT 2020. Held as Part of the 22nd HCI International Conference, HCII 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12206), P520, DOI 10.1007/978-3-030-50506-6_35
   Deng ZJ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P684
   Deng-Ping Fan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P275, DOI 10.1007/978-3-030-58610-2_17
   Fan DP, 2018, Arxiv, DOI arXiv:1805.10421
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2021, IEEE T NEUR NET LEAR, V32, P2075, DOI 10.1109/TNNLS.2020.2996406
   Fu KR, 2020, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR42600.2020.00312
   Gongyang Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P665, DOI 10.1007/978-3-030-58520-4_39
   Grauman K, 2005, IEEE I CONF COMP VIS, P1458
   Gu ZW, 2019, IEEE T MED IMAGING, V38, P2281, DOI 10.1109/TMI.2019.2903562
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hongyi Su, 2015, Intelligent Computing Theories and Methodologies. 11th International Conference, ICIC 2015. Proceedings: LNCS 9225, P1, DOI 10.1007/978-3-319-22180-9_1
   Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563
   Huang NAC, 2022, IEEE T MULTIMEDIA, V24, P1651, DOI 10.1109/TMM.2021.3069297
   Jiang B, 2021, IEEE T MULTIMEDIA, V23, P1343, DOI 10.1109/TMM.2020.2997184
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Ju R, 2014, IEEE IMAGE PROC, P1115, DOI 10.1109/ICIP.2014.7025222
   Jun Wei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13022, DOI 10.1109/CVPR42600.2020.01304
   Kingma D. P., 2014, arXiv
   Li GY, 2020, IEEE T IMAGE PROCESS, V29, P4873, DOI 10.1109/TIP.2020.2976689
   Li GB, 2019, AAAI CONF ARTIF INTE, P8594
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li NY, 2014, PROC CVPR IEEE, P2806, DOI 10.1109/CVPR.2014.359
   Liang FF, 2018, NEUROCOMPUTING, V275, P2227, DOI 10.1016/j.neucom.2017.10.052
   Liao GB, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2436, DOI 10.1145/3394171.3413523
   Liu D, 2021, IEEE T MULTIMEDIA, V23, P967, DOI 10.1109/TMM.2020.2991523
   Liu GH, 2014, PROCEEDINGS OF 2013 INTERNATIONAL CONFERENCE ON INFORMATION SCIENCE AND CLOUD COMPUTING COMPANION (ISCC-C), P728, DOI 10.1109/ISCC-C.2013.21
   Liu N, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4702, DOI 10.1109/ICCV48922.2021.00468
   Miao Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P374, DOI 10.1007/978-3-030-58604-1_23
   Niu YZ, 2012, PROC CVPR IEEE, P454, DOI 10.1109/CVPR.2012.6247708
   Pang Y., 2020, P EUR C COMP VIS, P253
   Paszke A, 2019, ADV NEUR IN, V32
   Peng HW, 2014, LECT NOTES COMPUT SC, V8691, P92, DOI 10.1007/978-3-319-10578-9_7
   Piao YR, 2019, IEEE I CONF COMP VIS, P7253, DOI 10.1109/ICCV.2019.00735
   Qiao LM, 2019, IEEE I CONF COMP VIS, P3602, DOI 10.1109/ICCV.2019.00370
   Qu LQ, 2017, IEEE T IMAGE PROCESS, V26, P2274, DOI 10.1109/TIP.2017.2682981
   Rossi L, 2021, INT C PATT RECOG, P2203
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song HK, 2017, IEEE T IMAGE PROCESS, V26, P4204, DOI 10.1109/TIP.2017.2711277
   Tang YL, 2016, VISUAL COMPUT, V32, P111, DOI 10.1007/s00371-014-1059-6
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang B, 2020, AAAI CONF ARTIF INTE, V34, P12128
   Wang GT, 2021, PROC CVPR IEEE, P15114, DOI 10.1109/CVPR46437.2021.01487
   Wang NN, 2019, IEEE ACCESS, V7, P55277, DOI 10.1109/ACCESS.2019.2913107
   Wang XH, 2020, Arxiv, DOI arXiv:2008.04157
   Wang XH, 2021, IEEE T IMAGE PROCESS, V30, P458, DOI 10.1109/TIP.2020.3037470
   Wang Yue, 2020, P ASIAN C COMPUTER V, P336
   Wei Ji, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P52, DOI 10.1007/978-3-030-58523-5_4
   Wei J, 2020, AAAI CONF ARTIF INTE, V34, P12321
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403
   Wu ZY, 2022, IEEE T MULTIMEDIA, V24, P73, DOI 10.1109/TMM.2020.3046871
   Xiaoqi Zhao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P646, DOI 10.1007/978-3-030-58542-6_39
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yongri Piao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9057, DOI 10.1109/CVPR42600.2020.00908
   Youwei Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9410, DOI 10.1109/CVPR42600.2020.00943
   Zeng Y, 2019, IEEE I CONF COMP VIS, P7222, DOI 10.1109/ICCV.2019.00732
   Zhang H, 2022, IEEE COMPUT SOC CONF, P2735, DOI 10.1109/CVPRW56347.2022.00309
   Zhang J., 2020, P IEEE CVF C COMP VI, P8579, DOI DOI 10.1109/CVPR42600.2020.00861
   Zhang M, 2020, PROC CVPR IEEE, P3469, DOI 10.1109/CVPR42600.2020.00353
   Zhang Z, 2021, IEEE T IMAGE PROCESS, V30, P1949, DOI 10.1109/TIP.2021.3049959
   Zhao JX, 2019, PROC CVPR IEEE, P3922, DOI 10.1109/CVPR.2019.00405
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhao K, 2022, IEEE T PATTERN ANAL, V44, P4793, DOI 10.1109/TPAMI.2021.3077129
   Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731
   Zhao T, 2019, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR.2019.00320
   Zhou H., 2020, P IEEE C COMP VIS PA, P9141, DOI 10.1109/CVPR42600.2020.00916
   Zhou WJ, 2022, IEEE T MULTIMEDIA, V24, P2192, DOI 10.1109/TMM.2021.3077767
   Zhou Y, 2019, IEEE T MULTIMEDIA, V21, P74, DOI 10.1109/TMM.2018.2845667
   Zhu CB, 2019, IEEE INT CON MULTI, P199, DOI 10.1109/ICME.2019.00042
NR 83
TC 10
Z9 11
U1 6
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4253
EP 4266
DI 10.1109/TMM.2022.3172852
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA W3PP1
UT WOS:001090784100002
DA 2024-07-18
ER

PT J
AU Cong, RM
   Zhang, KP
   Zhang, C
   Zheng, F
   Zhao, Y
   Huang, QM
   Kwong, S
AF Cong, Runmin
   Zhang, Kepu
   Zhang, Chen
   Zheng, Feng
   Zhao, Yao
   Huang, Qingming
   Kwong, Sam
TI Does Thermal Really Always Matter for RGB-T Salient Object Detection?
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Decoding; Semantics; Object detection; Location
   awareness; Lighting; Feature extraction; RGB-T images; salient object
   detection; global illumination estimation; semantic constraint provider;
   localization and complementation
ID FUSION NETWORK
AB In recent years, RGB-T salient object detection (SOD) has attracted continuous attention, which makes it possible to identify salient objects in environments such as low light by introducing thermal image. However, most of the existing RGB-T SOD models focus on how to perform cross-modality feature fusion, ignoring whether thermal image is really always matter in SOD task. Starting from the definition and nature of this task, this paper rethinks the connotation of thermal modality, and proposes a network named TNet to solve the RGB-T SOD task. In this paper, we introduce a global illumination estimation module to predict the global illuminance score of the image, so as to regulate the role played by the two modalities. In addition, considering the role of thermal modality, we set up different cross-modality interaction mechanisms in the encoding phase and the decoding phase. On the one hand, we introduce a semantic constraint provider to enrich the semantics of thermal images in the encoding phase, which makes thermal modality more suitable for the SOD task. On the other hand, we introduce a two-stage localization and complementation module in the decoding phase to transfer object localization cue and internal integrity cue in thermal features to the RGB modality. Extensive experiments on three datasets show that the proposed TNet achieves competitive performance compared with 20 state-of-the-art methods.
C1 [Cong, Runmin; Zhang, Kepu; Zhang, Chen; Zhao, Yao] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
   [Cong, Runmin; Zhang, Kepu; Zhang, Chen; Zhao, Yao] Network Technol, Beijing Key Lab Adv Informat Sci, Beijing 100044, Peoples R China.
   [Cong, Runmin; Kwong, Sam] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
   [Zheng, Feng] Southern Univ Sci & Technol, Dept Comp Sci & Technol, Shenzhen 518055, Peoples R China.
   [Zheng, Feng] Res Inst Trustworthy Autonomous Syst, Shenzhen 518055, Peoples R China.
   [Huang, Qingming] Univ Chinese Acad Sci, Sch Comp Sci & Technol, Beijing 101408, Peoples R China.
   [Huang, Qingming] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
   [Huang, Qingming] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
   [Kwong, Sam] City Univ Hong Kong, Shenzhen Res Inst, Shenzhen 51800, Peoples R China.
C3 Beijing Jiaotong University; City University of Hong Kong; Southern
   University of Science & Technology; Chinese Academy of Sciences;
   University of Chinese Academy of Sciences, CAS; Chinese Academy of
   Sciences; Institute of Computing Technology, CAS; Peng Cheng Laboratory;
   Shenzhen Research Institute, City University of Hong Kong; City
   University of Hong Kong
RP Zheng, F (corresponding author), Southern Univ Sci & Technol, Dept Comp Sci & Technol, Shenzhen 518055, Peoples R China.
EM rmcong@bjtu.edu.cn; kpzhang@bjtu.edu.cn; chen.zhang@bjtu.edu.cn;
   f.zheng@ieee.org; yzhao@bjtu.edu.cn; qmhuang@ucas.ac.cn;
   cssamk@cityu.edu.hk
RI Zheng, Feng/AAH-5643-2019; Kwong, Sam/C-9319-2012
OI Zheng, Feng/0000-0002-1701-9141; Kwong, Sam/0000-0001-7484-7261; zhang,
   kepu/0000-0002-8720-1045; Zhao, Yao/0000-0002-8581-9554
FU National Key R&D Program of China [2021ZD0112100]; Beijing Nova Program
   [Z201100006820016]; National Natural Science Foundation of China
   [62002014, U1936212, 62120106009, 62236008, U21B2038, 61972188,
   62122035]; Beijing Natural Science Foundation [4222013]; Hong Kong
   Innovation and Technology Commission (InnoHK Project CIMDA); Hong Kong
   GRF-RGC General Research Fund [11209819 (CityU 9042816), 11203820 (CityU
   9042598)]; Young Elite Scientist Sponsorship Program by the China
   Association for Science and Technology [2020QNRC001]; CAAI-Huawei
   MindSpore Open Fund; Dr Cong's Project; Fundamental Research Funds for
   the Central Universities [2022JBMC002]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2021ZD0112100, in part by the Beijing Nova Program under
   Grant Z201100006820016, in part by the National Natural Science
   Foundation of China under Grants 62002014, U1936212, 62120106009,
   62236008, U21B2038, 61972188, and 62122035, in part by the Beijing
   Natural Science Foundation under Grant 4222013, in part by the Hong Kong
   Innovation and Technology Commission (InnoHK Project CIMDA), in part by
   the Hong Kong GRF-RGC General Research Fund under Grants 11209819 (CityU
   9042816) and 11203820 (CityU 9042598), in part by Young Elite Scientist
   Sponsorship Program by the China Association for Science and Technology
   under Grant 2020QNRC001, in part by CAAI-Huawei MindSpore Open Fund, in
   part by Dr Cong's Project and in part by the Fundamental Research Funds
   for the Central Universities under Grant 2022JBMC002.
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Chen H, 2019, PATTERN RECOGN, V86, P376, DOI 10.1016/j.patcog.2018.08.007
   Chen Q, 2021, AAAI CONF ARTIF INTE, V35, P1063
   Chen ZY, 2021, IEEE T IMAGE PROCESS, V30, P7012, DOI 10.1109/TIP.2020.3028289
   Chen ZY, 2020, AAAI CONF ARTIF INTE, V34, P10599
   Chongyi Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P225, DOI 10.1007/978-3-030-58598-3_14
   Cong R., 2022, IEEE Trans. Emerg. Topics Comput. Intell.
   Cong RM, 2023, IEEE T CIRC SYST VID, V33, P534, DOI 10.1109/TCSVT.2022.3205182
   Cong RM, 2022, IEEE T IMAGE PROCESS, V31, P6800, DOI 10.1109/TIP.2022.3216198
   Cong RM, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3196430
   Cong RM, 2023, IEEE T CYBERNETICS, V53, P1920, DOI 10.1109/TCYB.2022.3169431
   Cong RM, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3123984
   Cong RM, 2020, IEEE T CYBERNETICS, V50, P3627, DOI 10.1109/TCYB.2019.2932005
   Cong RM, 2019, IEEE T MULTIMEDIA, V21, P1660, DOI 10.1109/TMM.2018.2884481
   Cong RM, 2019, IEEE T IMAGE PROCESS, V28, P4819, DOI 10.1109/TIP.2019.2910377
   Cong RM, 2019, IEEE T CIRC SYST VID, V29, P2941, DOI 10.1109/TCSVT.2018.2870832
   Cong RM, 2019, IEEE T CYBERNETICS, V49, P233, DOI 10.1109/TCYB.2017.2771488
   Cong RM, 2018, IEEE T IMAGE PROCESS, V27, P568, DOI 10.1109/TIP.2017.2763819
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698
   Fan DP, 2021, IEEE T NEUR NET LEAR, V32, P2075, DOI 10.1109/TNNLS.2020.2996406
   Fu KR, 2022, IEEE T PATTERN ANAL, V44, P5541, DOI 10.1109/TPAMI.2021.3073689
   Fu KR, 2020, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR42600.2020.00312
   Gao W, 2022, IEEE T CIRC SYST VID, V32, P2091, DOI 10.1109/TCSVT.2021.3082939
   Han JW, 2015, IEEE T CIRC SYST VID, V25, P1309, DOI 10.1109/TCSVT.2014.2381471
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou QB, 2019, IEEE T PATTERN ANAL, V41, P815, DOI 10.1109/TPAMI.2018.2815688
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang NAC, 2022, IEEE T MULTIMEDIA, V24, P1651, DOI 10.1109/TMM.2021.3069297
   Huo FS, 2022, IEEE T CIRC SYST VID, V32, P3111, DOI 10.1109/TCSVT.2021.3102268
   Jiang B, 2021, IEEE T MULTIMEDIA, V23, P1343, DOI 10.1109/TMM.2020.2997184
   Kingma D. P., 2014, arXiv
   Li CY, 2021, IEEE T CYBERNETICS, V51, P88, DOI 10.1109/TCYB.2020.2969255
   Li CY, 2020, NEUROCOMPUTING, V415, P411, DOI 10.1016/j.neucom.2020.05.108
   Li CY, 2019, IEEE T GEOSCI REMOTE, V57, P9156, DOI 10.1109/TGRS.2019.2925070
   Li GB, 2016, IEEE T IMAGE PROCESS, V25, P5012, DOI 10.1109/TIP.2016.2602079
   Liang YH, 2022, NEUROCOMPUTING, V490, P132, DOI 10.1016/j.neucom.2022.03.029
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Ma GX, 2020, IEEE T MULTIMEDIA, V22, P324, DOI 10.1109/TMM.2019.2929943
   Mao YD, 2022, IEEE T MULTIMEDIA, V24, P2435, DOI 10.1109/TMM.2021.3081260
   Máttyus G, 2017, IEEE I CONF COMP VIS, P3458, DOI 10.1109/ICCV.2017.372
   Nian Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13753, DOI 10.1109/CVPR42600.2020.01377
   Ottonelli S, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1134, DOI 10.1109/ICIT.2013.6505832
   Piao YR, 2019, IEEE I CONF COMP VIS, P7253, DOI 10.1109/ICCV.2019.00735
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Tu ZZ, 2023, IEEE T MULTIMEDIA, V25, P4163, DOI 10.1109/TMM.2022.3171688
   Tu ZZ, 2021, IEEE T IMAGE PROCESS, V30, P5678, DOI 10.1109/TIP.2021.3087412
   Tu ZZ, 2020, IEEE T MULTIMEDIA, V22, P160, DOI 10.1109/TMM.2019.2924578
   Tu ZZ, 2019, 2019 2ND IEEE CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2019), P141, DOI 10.1109/MIPR.2019.00032
   Wang G., 2018, CHINESE C IMAGE GRAP, P359
   Wang J, 2022, IEEE T CIRC SYST VID, V32, P2949, DOI 10.1109/TCSVT.2021.3099120
   Wang KY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3611, DOI 10.1109/ICCV48922.2021.00361
   Wang NN, 2019, IEEE ACCESS, V7, P55277, DOI 10.1109/ACCESS.2019.2913107
   Wen HF, 2021, IEEE T IMAGE PROCESS, V30, P9179, DOI 10.1109/TIP.2021.3123548
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403
   Wu ZY, 2022, IEEE T MULTIMEDIA, V24, P73, DOI 10.1109/TMM.2020.3046871
   Youwei Pang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P235, DOI 10.1007/978-3-030-58595-2_15
   Zhang C, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2094, DOI 10.1145/3474085.3475364
   Zhang DW, 2020, IEEE T PATTERN ANAL, V42, P1755, DOI 10.1109/TPAMI.2019.2900649
   Zhang L, 2022, IEEE T PATTERN ANAL, V44, P456, DOI 10.1109/TPAMI.2020.3009758
   Zhang Q., 2020, Adv. Neural Inf. Process. Syst.
   Zhang QJ, 2021, IEEE T IMAGE PROCESS, V30, P1305, DOI 10.1109/TIP.2020.3042084
   Zhang WB, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P731, DOI 10.1145/3474085.3475240
   Zhang YH, 2021, INT J COMPUT VISION, V129, P1013, DOI 10.1007/s11263-020-01407-x
   Zhang Z, 2021, IEEE T IMAGE PROCESS, V30, P1949, DOI 10.1109/TIP.2021.3049959
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhou T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4661, DOI 10.1109/ICCV48922.2021.00464
   Zhou T, 2021, COMPUT VIS MEDIA, V7, P37, DOI 10.1007/s41095-020-0199-z
   Zhou WJ, 2022, IEEE T EM TOP COMP I, V6, P957, DOI 10.1109/TETCI.2021.3118043
   Zhou WJ, 2022, IEEE T CIRC SYST VID, V32, P1224, DOI 10.1109/TCSVT.2021.3077058
   Zhou WJ, 2022, IEEE T MULTIMEDIA, V24, P2192, DOI 10.1109/TMM.2021.3077767
   Zhou XF, 2023, IEEE T CYBERNETICS, V53, P539, DOI 10.1109/TCYB.2022.3163152
   Zhu CB, 2019, IEEE INT CON MULTI, P199, DOI 10.1109/ICME.2019.00042
NR 75
TC 23
Z9 23
U1 1
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6971
EP 6982
DI 10.1109/TMM.2022.3216476
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000019
OA Green Submitted
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Hu, XJ
   Pan, YX
   Wang, YM
   Zhang, L
   Shirmohammadi, S
AF Hu, Xinjue
   Pan, Yuxuan
   Wang, Yumei
   Zhang, Lin
   Shirmohammadi, Shervin
TI Multiple Description Coding for Best-Effort Delivery of Light Field
   Video Using GNN-Based Compression
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Streaming media; Encoding; Image coding; Decoding; Packet loss;
   Three-dimensional displays; Spatial resolution; Light field video;
   multiple description coding; best-effort video delivery; graph neural
   network
AB In recent years, Light Field (LF) video has grabbed much attention as an emerging form of immersive media. LF collects, through a lens matrix, light information emanating in every direction, and obtains rich information about the scene, providing users with an immersive 6 Degrees of Freedom (DoF) experience. The visual content between different viewpoints is highly homogenized, suggesting the possibility of good compression and encoding. However, most fixed-structure LF coding schemes are difficult to adapt to the real-time requirements of different LF applications and best-effort network conditions causing packet loss. In this paper, we propose a dynamic adaptive LF video transmission scheme that can achieve high compression and yet provide near-distortion-free LF video when the network condition is stable. Additionally, for unstable network conditions a description scheduling algorithm is proposed, which can decode the LF video with the highest possible quality even if partial data cannot be received completely and/or timely. We achieve this by designing a Multiple Description Coding (MDC) based solution to transport the LF video compressed by a Graph Neural Network (GNN) model. Experimental results show that the scheduling algorithm can improve the quality of the decoding results by 3% to 15%. Compared with other similar schemes, our system greatly improves the reliability of the video streaming system against packet loss/error and supports heterogeneous receivers.
C1 [Hu, Xinjue; Pan, Yuxuan; Wang, Yumei; Zhang, Lin] Beijing Univ Posts & Telecommun, Beijing 100876, Peoples R China.
   [Hu, Xinjue] Univ Ottawa, Ottawa, ON K1N 6N5, Canada.
   [Shirmohammadi, Shervin] Univ Ottawa, Ottawa, ON K1N 6N, Canada.
C3 Beijing University of Posts & Telecommunications; University of Ottawa;
   University of Ottawa
RP Hu, XJ (corresponding author), Beijing Univ Posts & Telecommun, Beijing 100876, Peoples R China.; Hu, XJ (corresponding author), Univ Ottawa, Ottawa, ON K1N 6N5, Canada.
EM huxinjue@bupt.edu.cn; panyx@bupt.edu.cn; ymwang@bupt.edu.cn;
   zhanglin@bupt.edu.cn; shervin@eecs.uottawa.ca
RI Shirmohammadi, Shervin/E-6945-2012
OI Shirmohammadi, Shervin/0000-0002-3973-4445; Pan,
   Yuxuan/0000-0002-7762-0713; Hu, Xinjue/0000-0001-8304-9720
FU National Key Research and Development Program of China [2018YFB1800501]
FX Manuscript received 30 January 2021; revised 31 August 2021 and 20
   October 2021; accepted 15 November 2021. Date of publication 23 November
   2021; date of current version 9 March 2023. This work was supported in
   part by the National Key Research and Development Program of China under
   Grant 2018YFB1800501. The code will be publicly available at
   https://github.com/xinjuehu/VideoLightField-MDC.git. The Associate
   Editor coordinating the review of this manuscript and approving it for
   publication was Dr. Ali Begen. (Corresponding author: Xinjuue Hu.)
CR Alain M, 2019, IEEE IMAGE PROC, P3761, DOI [10.1109/icip.2019.8803558, 10.1109/ICIP.2019.8803558]
   [Anonymous], 2017, COMMUN FRONTIER IEEE
   Bai HH, 2007, IEEE T CIRC SYST VID, V17, P912, DOI 10.1109/TCSVT.2007.898646
   Chen J, 2019, J IMAGING SCI TECHN, V63, DOI 10.2352/J.ImagingSci.Technol.2019.63.5.050401
   Cheng SC, 2020, LECT NOTES COMPUT SC, V11961, P690, DOI 10.1007/978-3-030-37731-1_56
   Conti C, 2016, SIGNAL PROCESS-IMAGE, V42, P59, DOI 10.1016/j.image.2016.01.008
   Duong DT, 2019, PROC INT CONF ADV, P137, DOI [10.1109/atc.2019.8924564, 10.1109/ATC.2019.8924564]
   Dumitrescu S, 2005, ALGORITHMICA, V41, P269, DOI 10.1007/s00453-004-1126-x
   ELLIOTT EO, 1963, AT&T TECH J, V42, P1977, DOI 10.1002/j.1538-7305.1963.tb00955.x
   GILBERT EN, 1960, BELL SYST TECH J, V39, P1253, DOI 10.1002/j.1538-7305.1960.tb03959.x
   Gotsch D, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174096
   Guillo L., 2018, JTC1SC29WG1 ISOIEC
   Hu X., 2021, IET IMAGE PROCESS, V2021, P1
   Hu XJ, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3395620
   Hu XJ, 2019, PROCEEDINGS OF THE 10TH ACM MULTIMEDIA SYSTEMS CONFERENCE (ACM MMSYS'19), P74, DOI 10.1145/3304109.3306228
   Kara PA, 2018, IEEE T BROADCAST, V64, P407, DOI 10.1109/TBC.2018.2834736
   Kazemi M, 2021, MULTIMED TOOLS APPL, V80, P12685, DOI 10.1007/s11042-020-10333-6
   Kazemi M, 2018, IEEE T MULTIMEDIA, V20, P781, DOI 10.1109/TMM.2017.2758578
   Kazemi M, 2017, IEEE T MULTIMEDIA, V19, P54, DOI 10.1109/TMM.2016.2607342
   Kazemi M, 2014, MULTIMEDIA SYST, V20, P283, DOI 10.1007/s00530-013-0319-z
   Kovacs PT, 2017, J IMAGING SCI TECHN, V61, DOI 10.2352/J.ImagingSci.Technol.2017.61.1.010403
   Lin CY, 2018, IEEE T MULTIMEDIA, V20, P1209, DOI 10.1109/TMM.2017.2766043
   Lin X, 2015, BIOMED OPT EXPRESS, V6, P3179, DOI 10.1364/BOE.6.003179
   Liu Z, 2015, IEEE J-STSP, V9, DOI 10.1109/JSTSP.2014.2330332
   Meng LL, 2014, IEEE T IMAGE PROCESS, V23, P582, DOI 10.1109/TIP.2013.2288928
   Niedermeier F, 2012, ADV COMPUT, V87, P55, DOI 10.1016/B978-0-12-396528-8.00003-1
   Overbeck RS, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275031
   Perra C, 2016, INT CONF IMAG PROC, DOI 10.1109/ICMEW.2016.7574671
   Raca D, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P460, DOI 10.1145/3204949.3208123
   Rassool R, 2017, IEEE INT SYM BROADB, P351
   Ravi Netravali, 2015, 2015 USENIX ANN TECH, P417
   Tillo T, 2004, IEEE SIGNAL PROC LET, V11, P908, DOI 10.1109/LSP.2004.836949
   van der Hooft J, 2016, IEEE COMMUN LETT, V20, P2177, DOI 10.1109/LCOMM.2016.2601087
   Venkataraman K, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508390
   Vetro A, 2011, P IEEE, V99, P626, DOI 10.1109/JPROC.2010.2098830
   Wang B, 2019, IEEE ACCESS, V7, P41183, DOI 10.1109/ACCESS.2019.2907572
   Wang G, 2016, IEEE T IMAGE PROCESS, V25, P5104, DOI 10.1109/TIP.2016.2603602
   Wang TY, 2017, ACTA OCEANOL SIN, V36, P1, DOI 10.1007/s13131-017-0987-1
   Wijnants M, 2018, 24TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2018), DOI 10.1145/3281505.3281539
   Wu GC, 2017, IEEE J-STSP, V11, P926, DOI 10.1109/JSTSP.2017.2747126
   Xiang W, 2016, IEEE NETWORK, V30, P30, DOI 10.1109/MNET.2016.7474341
   Yuan Y, 2017, IEEE IMAGE PROC, P2219, DOI 10.1109/ICIP.2017.8296676
   Zhao LJ, 2019, IEEE T CIRC SYST VID, V29, P2494, DOI 10.1109/TCSVT.2018.2867067
NR 43
TC 9
Z9 9
U1 4
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 690
EP 705
DI 10.1109/TMM.2021.3129918
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA C4YC5
UT WOS:000961977900002
DA 2024-07-18
ER

PT J
AU Kong, J
   Tao, XF
   Jiang, M
   Liu, TS
AF Kong, Jun
   Tao, Xuefeng
   Jiang, Min
   Liu, Tianshan
TI Weakly Supervised Distribution Discrepancy Minimization Learning With
   State Information for Person Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cameras; Target tracking; Task analysis; Training; Supervised learning;
   Scalability; Reliability; Weakly supervised; person re-identification;
   Index Terms; unsupervised cross domain; camera-distribution-based loss;
   ranking-confidence-based loss
AB Weakly supervised person re-identification (Re-ID) is appealing to handle real-world tasks by using state information that is available without manual annotation. At present, most methods perform unsupervised cross domain (UCD) learning by transferring the knowledge from the labeled source domain to the unlabeled target domain, which results in poor performance due to the severe shift. To address this problem, in this paper, we utilize the tracklet and camera information as weak supervision to propose a distribution discrepancy minimization learning (DDML) model for UCD person Re-ID. In addition to aligning data distributions from the perspective of domain adaptation learning, two losses are developed from the view of neighborhood invariance exploration to optimize matching results. Specifically, to bridge the gap between domains, we propose a camera-distribution-based (CDB) loss to align pair-wise distance distributions. Furthermore, to alleviate the biased search within the target domain, we propose a ranking-confidence-based (RCB) loss to perform the mined neighborhood for intra-camera and inter-camera separately to explore a high degree of confidence neighbor relations. Extensive experiments on three challenging datasets demonstrate that applying our method to unlabeled target domain outperforms current weakly supervised methods for person Re-ID.
C1 [Kong, Jun] Jiangnan Univ, Key Lab Adv Proc Control Light Ind, Minist Educ, Wuxi 214122, Peoples R China.
   [Tao, Xuefeng; Jiang, Min] Jiangnan Univ, Jiangsu Prov Engn Lab Pattern Recognit & Computat, Wuxi 214122, Peoples R China.
   [Liu, Tianshan] Hong Kong Polytech Univ, Dept Elect & Informat Engn, Hong Kong 999077, Peoples R China.
C3 Jiangnan University; Jiangnan University; Hong Kong Polytechnic
   University
RP Kong, J (corresponding author), Jiangnan Univ, Key Lab Adv Proc Control Light Ind, Minist Educ, Wuxi 214122, Peoples R China.
EM kongjun@jiangnan.edu.cn; 6191914049@stu.jiangnan.edu.cn;
   minjiang@jiangnan.edu.cn; tianshan.liu@connect.polyu.hk
RI JIANG, MIN/KSM-4856-2024
OI Kong, Jun/0000-0003-2551-4748; Tao, Xuefeng/0000-0003-1142-619X
FU Fundamental Research Funds for the Central Universities [JUSRP41908];
   National Natural Science Foundation of China [61362030, 61201429]; China
   Postdoctoral Science Foundation [2015M581720, 2016M600360]; 111 Projects
   [B12018]
FX This work was supported in part by the Fundamental Research Funds for
   the Central Universities under Grant JUSRP41908, in part the National
   Natural Science Foundation of China under Grants 61362030 and 61201429,
   in part China Postdoctoral Science Foundation under Grants 2015M581720
   and 2016M600360, and in part 111 Projects under Grant B12018.
CR Busto PP, 2017, IEEE I CONF COMP VIS, P754, DOI 10.1109/ICCV.2017.88
   Chen H, 2021, PROC CVPR IEEE, P2004, DOI 10.1109/CVPR46437.2021.00204
   Chen W., 2020, International Conference on Machine Learning, P1746
   Chen Y, 2022, PATTERN RECOGN LETT, V157, P90, DOI 10.1016/j.patrec.2022.03.020
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Chuanchen Luo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P224, DOI 10.1007/978-3-030-58555-6_14
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding YH, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3369393
   Dongkai Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10978, DOI 10.1109/CVPR42600.2020.01099
   Elezi I., 2022, IEEE T PATTERN ANAL, P1
   Felzenszwalb P, 2008, PROC CVPR IEEE, P1984
   Feng H, 2021, IEEE T IMAGE PROCESS, V30, P2898, DOI 10.1109/TIP.2021.3056212
   Fu JH, 2019, IEEE IMAGE PROC, P2506, DOI [10.1109/icip.2019.8803287, 10.1109/ICIP.2019.8803287]
   Fu Y, 2019, IEEE I CONF COMP VIS, P6111, DOI 10.1109/ICCV.2019.00621
   Ge Y., 2020, P NIPS, V33, P11309
   Ge YX, 2020, Arxiv, DOI [arXiv:2001.01526, 10.48550/arXiv.2001.01526]
   Ghorbel M, 2022, EXPERT SYST APPL, V197, DOI 10.1016/j.eswa.2022.116636
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Jayavarthini C., 2022, Evolutionary Computing and Mobile Sustainable Networks: Proceedings of ICECMSN 2021. Lecture Notes on Data Engineering and Communications Technologies (116), P941, DOI 10.1007/978-981-16-9605-3_66
   Jia C., 2022, arXiv
   Jianing Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P483, DOI 10.1007/978-3-030-58586-0_29
   Jin X, 2020, PROC CVPR IEEE, P3140, DOI 10.1109/CVPR42600.2020.00321
   Kalayeh MM, 2018, PROC CVPR IEEE, P1062, DOI 10.1109/CVPR.2018.00117
   Lee CY, 2019, PROC CVPR IEEE, P10277, DOI 10.1109/CVPR.2019.01053
   Li YJ, 2019, IEEE I CONF COMP VIS, P7918, DOI 10.1109/ICCV.2019.00801
   Li Z, 2022, MULTIMED TOOLS APPL, V81, P24493, DOI 10.1007/s11042-022-12022-y
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2905, DOI 10.1109/TMM.2020.2965491
   Mekhazni Djebril, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P159, DOI 10.1007/978-3-030-58583-9_10
   Qi L, 2019, IEEE I CONF COMP VIS, P8079, DOI 10.1109/ICCV.2019.00817
   Saito K, 2018, LECT NOTES COMPUT SC, V11209, P156, DOI 10.1007/978-3-030-01228-1_10
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tao XF, 2022, IEEE T CIRC SYST VID, V32, P4404, DOI 10.1109/TCSVT.2021.3135274
   Wan CQ, 2020, IEEE T MULTIMEDIA, V22, P1605, DOI 10.1109/TMM.2019.2946486
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang PY, 2021, IEEE T MULTIMEDIA, V23, P1474, DOI 10.1109/TMM.2020.2999180
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Xie QK, 2021, IEEE T MULTIMEDIA, V23, P597, DOI 10.1109/TMM.2020.2985525
   Xuan SY, 2021, PROC CVPR IEEE, P11921, DOI 10.1109/CVPR46437.2021.01175
   Yang F, 2021, IEEE T MULTIMEDIA, V23, P1681, DOI 10.1109/TMM.2020.3001522
   Yang FX, 2020, AAAI CONF ARTIF INTE, V34, P12597
   Yang QZ, 2019, PROC CVPR IEEE, P3628, DOI 10.1109/CVPR.2019.00375
   Yang Zou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P87, DOI 10.1007/978-3-030-58536-5_6
   Yiqi Jiang, 2021, MM '21: Proceedings of the 29th ACM International Conference on Multimedia, P4146, DOI 10.1145/3474085.3475547
   Zhu YX, 2020, NEUROCOMPUTING, V386, P97, DOI 10.1016/j.neucom.2019.12.100
   Zhang BH, 2022, MULTIMED TOOLS APPL, V81, P24081, DOI 10.1007/s11042-022-12728-z
   Zhang L, 2022, NEURAL COMPUT APPL, V34, P11817, DOI 10.1007/s00521-022-07071-1
   Zhang XY, 2019, IEEE I CONF COMP VIS, P8221, DOI 10.1109/ICCV.2019.00831
   Zhao CR, 2020, IEEE T MULTIMEDIA, V22, P3180, DOI 10.1109/TMM.2020.2972125
   Zhao HY, 2017, PROC CVPR IEEE, P907, DOI 10.1109/CVPR.2017.103
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z., 2017, P IEEE CVF C COMP VI, P1318, DOI [10.1109/CVPR.2017.389, DOI 10.1109/CVPR.2017.389]
   Zhong Z, 2021, IEEE T PATTERN ANAL, V43, P2723, DOI 10.1109/TPAMI.2020.2976933
   Zhong Z, 2019, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2019.00069
   Zhong Z, 2018, PROC CVPR IEEE, pCP99, DOI 10.1109/CVPR.2018.00541
NR 60
TC 3
Z9 3
U1 3
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1903
EP 1915
DI 10.1109/TMM.2022.3220115
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100025
DA 2024-07-18
ER

PT J
AU Lan, Y
   Hu, RM
   Xu, X
   Li, DS
   Wang, C
   Wang, XC
AF Lan, Yun
   Hu, Ruimin
   Xu, Xin
   Li, Dengshi
   Wang, Chao
   Wang, Xiaochen
TI From Collective Attribute Association of Groups to Precise Attribute
   Association of Individuals
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Trajectory; Hidden Markov models; Cameras; Visualization; Surveillance;
   Measurement; Data models; Precise element matching; trajectory
   association; person re-identification; pre-processing; unsupervised
ID PERSON REIDENTIFICATION
AB Obscured person re-identification (Re-ID) aims to match an obscured image with a complete image of the same person captured by other cameras. As a major challenge in person identification, occlusion severely affects the effectiveness of most traditional person Re-ID methods. To solve this problem, this study proposes a trajectory association method, which, as a pre-processing technique for person Re-ID, can narrow the search range and reduce the problem of degradation caused by mixing. We investigate the method of converting the fuzzy association between sets into the precise association between elements for M video objects and N phone objects (trajectory information) with fuzzy group association relationships at the crime scene. First, we decompose the M-N precise association problem and analyze the similarity of the video objects in the source point and on the trajectories. Then, we define high-similarity points, study their distribution characteristics in different trajectories, and find that there is a significant difference between the distribution of high-similarity points in correct and incorrect matching trajectories. We simplify the full-path association problem into a partial-path high-similarity point distribution difference problem, which effectively reduces the difficulty in accurate association relationship construction. The association experiments in simple and mixed scenarios as well as Re-ID experiments on the PRPW and Market1501 demonstrate the effectiveness of our method.
C1 [Lan, Yun; Hu, Ruimin; Wang, Chao; Wang, Xiaochen] Wuhan Univ, Natl Engn Res Ctr Multimedia Software, Sch Comp Sci, Wuhan 430072, Peoples R China.
   [Lan, Yun; Hu, Ruimin; Wang, Chao; Wang, Xiaochen] Wuhan Univ, Hubei Key Lab Multimedia, Network Commun Engn, Wuhan 430072, Peoples R China.
   [Xu, Xin] Wuhan Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan 430065, Peoples R China.
   [Li, Dengshi] Jianghan Univ, Sch Math & Comp, Wuhan 430014, Peoples R China.
C3 Wuhan University; Wuhan University; Wuhan University of Science &
   Technology; Jianghan University
RP Hu, RM; Wang, XC (corresponding author), Wuhan Univ, Natl Engn Res Ctr Multimedia Software, Sch Comp Sci, Wuhan 430072, Peoples R China.
EM 978369104@qq.com; hrm@whu.edu.cn; xuxin@wust.edu.cn; reallds@126.com;
   snipercwang@whu.edu.cn; clowang@163.com
RI Wang, Chao/JHT-4563-2023; Xu, Xin/JRW-5800-2023
OI Hu, Ruimin/0000-0002-0290-5757; Xu, Xin/0000-0003-0748-3669
FU NSFC [U22A2035, U1803262, U1736206, 19ZDA113]
FX This work was supported in part by NSFC under Grants U22A2035, U1803262,
   and U1736206 and in part by NSFC under Grant 19ZDA113.
CR Bai S, 2021, IEEE T PATTERN ANAL, V43, P2119, DOI 10.1109/TPAMI.2020.3031625
   Bai S, 2019, PROC CVPR IEEE, P740, DOI 10.1109/CVPR.2019.00083
   Bai S, 2017, PROC CVPR IEEE, P3356, DOI 10.1109/CVPR.2017.358
   Cheng DS, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.68
   de Montjoye YA, 2013, SCI REP-UK, V3, DOI 10.1038/srep01376
   Gray D, 2008, LECT NOTES COMPUT SC, V5302, P262, DOI 10.1007/978-3-540-88682-2_21
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He T, 2022, AAAI CONF ARTIF INTE, P879
   Huang WYC, 2021, P NATL ACAD SCI USA, V118, DOI 10.1073/pnas.2103598118
   Jiang N, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1457, DOI 10.1145/3240508.3240663
   Ksibi S, 2019, MULTIMED TOOLS APPL, V78, P1583, DOI 10.1007/s11042-018-6200-5
   Li DY, 2021, NEURAL PROCESS LETT, V53, P3267, DOI 10.1007/s11063-021-10540-8
   Li DY, 2020, LECT NOTES COMPUT SC, V11961, P813, DOI 10.1007/978-3-030-37731-1_66
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Lin YT, 2019, PATTERN RECOGN, V95, P151, DOI 10.1016/j.patcog.2019.06.006
   Lv JM, 2018, PROC CVPR IEEE, P7948, DOI 10.1109/CVPR.2018.00829
   Lv JM, 2014, LECT NOTES COMPUT SC, V8422, P16, DOI 10.1007/978-3-319-05813-9_2
   Matsukawa T, 2016, INT C PATT RECOG, P2428, DOI 10.1109/ICPR.2016.7900000
   Miao JX, 2019, IEEE I CONF COMP VIS, P542, DOI 10.1109/ICCV.2019.00063
   Sarfraz MS, 2018, PROC CVPR IEEE, P420, DOI 10.1109/CVPR.2018.00051
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Wang Z, 2019, IEEE T MULTIMEDIA, V21, P2376, DOI 10.1109/TMM.2019.2898753
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wen LY, 2017, INT J COMPUT VISION, V122, P313, DOI 10.1007/s11263-016-0943-0
   Wenxin Huang, 2016, MultiMedia Modeling. 22nd International Conference, MMM 2016. Proceedings, P174, DOI 10.1007/978-3-319-27671-7_15
   Yang FX, 2021, PROC CVPR IEEE, P4853, DOI 10.1109/CVPR46437.2021.00482
   Yu HX, 2019, PROC CVPR IEEE, P2143, DOI 10.1109/CVPR.2019.00225
   Zhang MY, 2021, AAAI CONF ARTIF INTE, V35, P3360
   Zheng KC, 2021, PROC CVPR IEEE, P5306, DOI 10.1109/CVPR46437.2021.00527
   Zheng KC, 2021, AAAI CONF ARTIF INTE, V35, P3538
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
NR 32
TC 0
Z9 0
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1547
EP 1554
DI 10.1109/TMM.2023.3251097
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA G2GW2
UT WOS:000987415000014
DA 2024-07-18
ER

PT J
AU Li, J
   Xiang, Y
   Wu, H
   Yao, SW
   Xu, D
AF Li, Jie
   Xiang, Yong
   Wu, Hao
   Yao, Shaowen
   Xu, Dan
TI Optimal Transport-Based Patch Matching for Image Style Transfer
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Style transfer; neural style transfer; optimal transport; patch matching
AB State-of-the-art image style transfer methods have achieved impressive results by using neural networks. However, neural style transfer (NST) methods either ignore the local details of the style image by using the global statistics for style modeling or cannot fully use shallow features of neural networks, leading to the synthesized image having fewer details. In this study, we proposed a new patch-based style transfer method that directly operates in the image pixel domain without using any neural networks, achieving fascinating style transfer results with rich image details. The proposed method was derived from classic texture synthesis methods. Most previous methods rely on nearest neighbor search (NNS) for patch matching. However, this greedy strategy cannot guarantee the similarity of patch distributions between the synthesized image and the style image, which limits the expressiveness of textures. We solved this problem by proposing an optimal patch matching algorithm formed on the Optimal Transport (OT) theory, which theoretically guarantees the similarity of the patch distributions and gives a flexible style modeling method. Various qualitative and quantitative experiments demonstrated that the proposed method achieves better synthesized results than state-of-the-art style transfer methods, including NST and classic methods based on texture synthesis.
C1 [Li, Jie] Yunnan Normal Univ, Sch Informat Sci & Technol, Kunming 650500, Yunnan, Peoples R China.
   [Xiang, Yong] Deakin Univ, Sch Informat Technol, Burwood, Vic 3125, Australia.
   [Wu, Hao; Xu, Dan] Yunnan Univ, Sch Informat Sci & Engn, Kunming 650091, Yunnan, Peoples R China.
   [Yao, Shaowen] Yunnan Univ, Sch Software, Kunming 650091, Yunnan, Peoples R China.
C3 Yunnan Normal University; Deakin University; Yunnan University; Yunnan
   University
RP Li, J (corresponding author), Yunnan Normal Univ, Sch Informat Sci & Technol, Kunming 650500, Yunnan, Peoples R China.; Xu, D (corresponding author), Yunnan Univ, Sch Informat Sci & Engn, Kunming 650091, Yunnan, Peoples R China.
EM lightstr@outlook.com; yxiang@deakin.edu.au; haowu1982@vip.163.com;
   yaosw@ynu.edu.cn; danxu@ynu.edu.cn
RI Xu, Dan/KPA-7396-2024
OI Xu, Dan/0000-0003-4602-3550; Xiang, Yong/0000-0003-3545-7863
FU National Natural Science Foundation of China [62162068, 62061049,
   61863036]; Yunnan Ten Thousand Talents Program, Yunling Scholars Special
   Project [YNWR-YLXZ-2018-022]; Yunnan Fundamental Research Projects
   [2018FB100]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62162068, 62061049, and 61863036, in
   part by Yunnan Ten Thousand Talents Program, Yunling Scholars Special
   Project under Grant YNWR-YLXZ-2018-022, and in part by Yunnan
   Fundamental Research Projects under Grant 2018FB100.
CR Ashikhmin M., 2001, P 2001 S INT 3D GRAP, P217, DOI DOI 10.1145/364338.364405
   Chan SH, 2017, IEEE T COMPUT IMAG, V3, P84, DOI 10.1109/TCI.2016.2629286
   Chen DD, 2017, PROC CVPR IEEE, P2770, DOI 10.1109/CVPR.2017.296
   Chizat L, 2018, J FUNCT ANAL, V274, P3090, DOI 10.1016/j.jfa.2018.03.008
   Cuturi M., 2013, ADV NEURAL INFORM PR, P2292, DOI DOI 10.48550/ARXIV.1306.0895
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Drori I, 2003, PROC CVPR IEEE, P143
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Elad M, 2017, IEEE T IMAGE PROCESS, V26, P2338, DOI 10.1109/TIP.2017.2678168
   Frigo O, 2016, PROC CVPR IEEE, P553, DOI 10.1109/CVPR.2016.66
   Gatys L., 2015, NIPS
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Gutierrez J, 2017, LECT NOTES COMPUT SC, V10302, P172, DOI 10.1007/978-3-319-58771-4_14
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Jiang K, 2020, PATTERN RECOGN, V107, DOI 10.1016/j.patcog.2020.107475
   Jing YC, 2020, IEEE T VIS COMPUT GR, V26, P3365, DOI 10.1109/TVCG.2019.2921336
   Kalischek N, 2021, PROC CVPR IEEE, P9377, DOI 10.1109/CVPR46437.2021.00926
   Kaspar A, 2015, COMPUT GRAPH FORUM, V34, P349, DOI 10.1111/cgf.12565
   Kolkin N, 2019, PROC CVPR IEEE, P10043, DOI 10.1109/CVPR.2019.01029
   Kotovenko D, 2019, PROC CVPR IEEE, P10024, DOI 10.1109/CVPR.2019.01027
   Kwatra V, 2005, ACM T GRAPHIC, V24, P795, DOI 10.1145/1073204.1073263
   Lee H., 2010, Proceedings of the 8th International Symposium on Non-Photorealistic Animation and Rendering, P43
   Li C, 2016, PROC CVPR IEEE, P2479, DOI 10.1109/CVPR.2016.272
   Li YJ, 2017, ADV NEUR IN, V30
   Liu SH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6629, DOI 10.1109/ICCV48922.2021.00658
   Lu M, 2019, IEEE I CONF COMP VIS, P5951, DOI 10.1109/ICCV.2019.00605
   Mroueh Y, 2020, PR MACH LEARN RES, V108, P842
   Phillips F, 2011, ISS ACCOUNT EDUC, V26, P593, DOI 10.2308/iace-50038
   Rabin J, 2015, LECT NOTES COMPUT SC, V9389, P87, DOI 10.1007/978-3-319-25040-3_10
   Sheng L, 2018, PROC CVPR IEEE, P8242, DOI 10.1109/CVPR.2018.00860
   Simonyan K., 2014, CORR
   Song ZC, 2017, IEEE T MULTIMEDIA, V19, P702, DOI 10.1109/TMM.2016.2631123
   Takatsu A, 2011, OSAKA J MATH, V48, P1005
   Tian M. S, 2016, P ADV NEUR INF PROC, P1
   van Noord N, 2017, IEEE INT CONF COMP V, P2907, DOI 10.1109/ICCVW.2017.343
   Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5
   Webster R, 2018, Arxiv, DOI arXiv:1801.04619
   Yang S, 2019, IEEE T IMAGE PROCESS, V28, P952, DOI 10.1109/TIP.2018.2873064
   Zhang H, 2018, P EUR C COMP VIS, P1
   Zhang YX, 2020, IEEE T IMAGE PROCESS, V29, P4085, DOI 10.1109/TIP.2020.2969081
   Zhang YX, 2018, PROC CVPR IEEE, P8447, DOI 10.1109/CVPR.2018.00881
   Zoran D, 2011, IEEE I CONF COMP VIS, P479, DOI 10.1109/ICCV.2011.6126278
NR 44
TC 0
Z9 0
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5927
EP 5940
DI 10.1109/TMM.2022.3201387
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500021
DA 2024-07-18
ER

PT J
AU Li, M
   Fu, B
   Zhang, ZF
   Qiao, Y
AF Li, Ming
   Fu, Bin
   Zhang, Zhengfu
   Qiao, Yu
TI Character-Aware Sampling and Rectification for Scene Text Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Scene text recognition; scene optical character recognition; deep
   learning
ID NETWORK
AB Curved scene text recognition is a challenging task in multimedia society due to large shape and texture variance. Previous methods address this challenge by extracting and rectifying text line with equidistantly sampling, which ignore character level information and lead to distorted characters. To address this issue, this paper proposes a Character-Aware Sampling and Rectification (CASR) module, which rectifies irregular text instance according to the location and orientation information of each individual character. Specifically, CASR regards each character as a basic unit and predicts the character-level attributes for sampling and rectification. Our module not only exploits detailed character information to obtain better rectification of text line, but also employs character-level supervision in training process. In addition, CASR provides a plug-and-play module which can be easily incorporated to existing text recognition pipeline. Extensive experiments on several benchmarks demonstrate that our method obtains more accurate rectified text instances and achieves promising performance. We will release our code and models in the future.
C1 [Li, Ming; Fu, Bin; Zhang, Zhengfu; Qiao, Yu] Chinese Acad Sci, Shenzhen Inst Adv Technol, ShenZhen Key Lab Comp Vis & Pattern Recognit, Shenzhen 518055, Peoples R China.
   [Qiao, Yu] Shanghai AI Lab, Shanghai 200030, Peoples R China.
C3 Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology,
   CAS; Shanghai Artificial Intelligence Laboratory
RP Qiao, Y (corresponding author), Chinese Acad Sci, Shenzhen Inst Adv Technol, ShenZhen Key Lab Comp Vis & Pattern Recognit, Shenzhen 518055, Peoples R China.; Qiao, Y (corresponding author), Shanghai AI Lab, Shanghai 200030, Peoples R China.
EM ming.li3@siat.ac.cn; bin.fu@siat.ac.cn; Zhang.zf.zhang@siat.ac.cn;
   yu.qiao@siat.ac.cn
OI Fu, Bin/0000-0002-1526-8654
FU National Natural Science Foundation of China [6163302]; Shenzhen
   Research Program [JSGG20191129141212311]; Shanghai Committee of Science,
   and Technology, China [20DZ1100800]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 6163302, in part by Shenzhen Research
   Program (JSGG20191129141212311), and in part by the Shanghai Committee
   of Science, and Technology, China under Grant 20DZ1100800. The Associate
   Editor coordinating the review of this manuscript and approving it for
   publication was Dr. Palaiahnakote Shivakumara. (Ming Li and Bin Fu
   contributed equally to this work)
CR Almazán J, 2014, IEEE T PATTERN ANAL, V36, P2552, DOI 10.1109/TPAMI.2014.2339814
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Bai F, 2018, PROC CVPR IEEE, P1508, DOI 10.1109/CVPR.2018.00163
   Bartz C, 2018, AAAI CONF ARTIF INTE, P6674
   BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792
   Chen JY, 2019, SCI CHINA INFORM SCI, V62, DOI 10.1007/s11432-018-9704-7
   Cheng ZZ, 2018, PROC CVPR IEEE, P5571, DOI 10.1109/CVPR.2018.00584
   Cheng ZZ, 2017, IEEE I CONF COMP VIS, P5086, DOI 10.1109/ICCV.2017.543
   Chorowski J, 2015, ADV NEUR IN, V28
   Dai PW, 2020, IEEE T MULTIMEDIA, V22, P1969, DOI 10.1109/TMM.2019.2952978
   Deli Yu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12110, DOI 10.1109/CVPR42600.2020.01213
   Gao YZ, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2710-7
   Gordo A, 2015, PROC CVPR IEEE, P2956, DOI 10.1109/CVPR.2015.7298914
   Graves A., 2006, P 23 INT C MACHINE L, P369
   Gupta A, 2016, PROC CVPR IEEE, P2315, DOI 10.1109/CVPR.2016.254
   He P, 2016, AAAI CONF ARTIF INTE, P3501
   Hui Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P751, DOI 10.1007/978-3-030-58586-0_44
   Jaderberg M., 2014, WORKSHOP DEEP LEARN
   Jaderberg M., 2015, PROC INT C LEARN REP
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jaderberg M, 2016, INT J COMPUT VISION, V116, P1, DOI 10.1007/s11263-015-0823-z
   Jaderberg M, 2014, LECT NOTES COMPUT SC, V8692, P512, DOI 10.1007/978-3-319-10593-2_34
   Karatzas D, 2015, PROC INT CONF DOC, P1156, DOI 10.1109/ICDAR.2015.7333942
   Karatzas D, 2013, PROC INT CONF DOC, P1484, DOI 10.1109/ICDAR.2013.221
   Lee CY, 2016, PROC CVPR IEEE, P2231, DOI 10.1109/CVPR.2016.245
   Li H, 2019, AAAI CONF ARTIF INTE, P8610
   Liao MH, 2019, AAAI CONF ARTIF INTE, P8714
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Litman Ron, 2020, CVPR, P11959, DOI 10.1109/CVPR42600.2020.01198
   Liu W, 2018, AAAI CONF ARTIF INTE, P7154
   Liu Y, 2018, LECT NOTES COMPUT SC, V11209, P449, DOI 10.1007/978-3-030-01228-1_27
   Long SB, 2021, INT J COMPUT VISION, V129, DOI 10.1007/s11263-020-01369-0
   Lucas S. M., 2005, International Journal on Document Analysis and Recognition, V7, P105, DOI 10.1007/s10032-004-0134-3
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Mishra A, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.127
   Mishra A, 2012, PROC CVPR IEEE, P2687, DOI 10.1109/CVPR.2012.6247990
   Novikova T, 2012, LECT NOTES COMPUT SC, V7577, P752, DOI 10.1007/978-3-642-33783-3_54
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Risnumawan A, 2014, EXPERT SYST APPL, V41, P8027, DOI 10.1016/j.eswa.2014.07.008
   Seok JH, 2015, PATTERN RECOGN, V48, P3584, DOI 10.1016/j.patcog.2015.05.004
   Shi BG, 2019, IEEE T PATTERN ANAL, V41, P2035, DOI 10.1109/TPAMI.2018.2848939
   Shi BG, 2017, IEEE T PATTERN ANAL, V39, P2298, DOI 10.1109/TPAMI.2016.2646371
   Shi BG, 2016, PROC CVPR IEEE, P4168, DOI 10.1109/CVPR.2016.452
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Tang YB, 2018, IEEE T MULTIMEDIA, V20, P2276, DOI 10.1109/TMM.2018.2802644
   Phan TQ, 2013, IEEE I CONF COMP VIS, P569, DOI 10.1109/ICCV.2013.76
   Wang K, 2011, IEEE I CONF COMP VIS, P1457, DOI 10.1109/ICCV.2011.6126402
   Wang T, 2012, INT C PATT RECOG, P3304
   Wang TW, 2020, AAAI CONF ARTIF INTE, V34, P12216
   Wang WH, 2019, PROC CVPR IEEE, P9328, DOI 10.1109/CVPR.2019.00956
   Wu XP, 2021, IEEE T MULTIMEDIA, V23, P3427, DOI 10.1109/TMM.2020.3025696
   Xue ML, 2021, IEEE T MULTIMEDIA, V23, P2706, DOI 10.1109/TMM.2020.3015037
   Yan MK, 2019, IEEE I CONF COMP VIS, P9146, DOI 10.1109/ICCV.2019.00924
   Yang X, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3280
   Yao C, 2014, IEEE T IMAGE PROCESS, V23, P4737, DOI 10.1109/TIP.2014.2353813
   Yao C, 2014, PROC CVPR IEEE, P4042, DOI 10.1109/CVPR.2014.515
   Yue X., 2020, ECCV, VVolume 12364, P135, DOI [DOI 10.1007/978-3-030-58529-7_9, 10.1007/978-3-030-58529-7_9]
   Zhan FN, 2019, PROC CVPR IEEE, P2054, DOI 10.1109/CVPR.2019.00216
   Zhang S, 2021, IEEE T MULTIMEDIA, V23, P454, DOI 10.1109/TMM.2020.2978630
   Zhi Qiao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13525, DOI 10.1109/CVPR42600.2020.01354
NR 61
TC 4
Z9 4
U1 4
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 649
EP 661
DI 10.1109/TMM.2021.3129651
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 9B8PL
UT WOS:000934993800024
DA 2024-07-18
ER

PT J
AU Liu, YX
   Wu, JL
   Qu, LG
   Gan, T
   Yin, JH
   Nie, LQ
AF Liu, Yaxin
   Wu, Jianlong
   Qu, Leigang
   Gan, Tian
   Yin, Jianhua
   Nie, Liqiang
TI Self-Supervised Correlation Learning for Cross-Modal Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-modal retrieval; self-supervised contrastive learning; mutual
   information estimation
AB Cross-modal retrieval aims to retrieve relevant data from another modality when given a query of one modality. Although most existing methods that rely on the label information of multimedia data have achieved promising results, the performance benefiting from labeled data comes at a high cost since labeling data often requires enormous labor resources, especially on large-scale multimedia datasets. Therefore, unsupervised cross-modal learning is of crucial importance in real-world applications. In this paper, we propose a novel unsupervised cross-modal retrieval method, named Self-supervised Correlation Learning (SCL), which takes full advantage of large amounts of unlabeled data to learn discriminative and modality-invariant representations. Since unsupervised learning lacks the supervision of category labels, we incorporate the knowledge from the input as a supervisory signal by maximizing the mutual information between the input and the output of different modality-specific projectors. Besides, for the purpose of learning discriminative representations, we exploit unsupervised contrastive learning to model the relationship among intra- and inter-modality instances, which makes similar samples closer and pushes dissimilar samples apart. Moreover, to further eliminate the modality gap, we use a weight-sharing scheme and minimize the modality-invariant loss in the joint representation space. Beyond that, we also extend the proposed method to the semi-supervised setting. Extensive experiments conducted on three widely-used benchmark datasets demonstrate that our method achieves competitive results compared with current state-of-the-art cross-modal retrieval approaches.
C1 [Liu, Yaxin; Wu, Jianlong; Qu, Leigang; Gan, Tian; Yin, Jianhua; Nie, Liqiang] Shandong Univ, Sch Comp Sci & Technol, Qingdao 266237, Peoples R China.
C3 Shandong University
RP Wu, JL (corresponding author), Shandong Univ, Sch Comp Sci & Technol, Qingdao 266237, Peoples R China.
EM liuyaxin0429@gmail.com; jlwu1992@sdu.edu.cn; leigangqu@gmail.com;
   gantian@sdu.edu.cn; jhyin@sdu.edu.cn; nieliqiang@gmail.com
RI Yin, Jianhua/HMD-6684-2023; Liu, Yaxin/AAA-1689-2022
OI Yin, Jianhua/0000-0002-4611-2986; Liu, Yaxin/0000-0003-2399-811X
FU National Natural Science Foundation of China [62006140, 62172261,
   62176137, U1936203]; Shandong Provincial Natural Science Foundation
   [ZR2019JQ23, ZR2019QF001, ZR2020QF106]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62006140, 62172261, 62176137, and
   U1936203, and in part by the Shandong Provincial Natural Science
   Foundation under Grants ZR2019JQ23, ZR2019QF001, and ZR2020QF106.
CR [Anonymous], 2013, P 30 INT C MACH LEAR
   [Anonymous], 2006, PROC IEEE COMPUT SOC
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Belghazi MI, 2018, PR MACH LEARN RES, V80
   Chen T, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P767, DOI 10.1145/3097983.3098202
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Chen W, 2021, PATTERN RECOGN, V117, DOI 10.1016/j.patcog.2021.107983
   Cheng SY, 2021, PROC CVPR IEEE, P4419, DOI 10.1109/CVPR46437.2021.00440
   Chua T.-S., 2009, CIVR, P1, DOI 10.1145/1646396.1646452
   Chun S, 2021, PROC CVPR IEEE, P8411, DOI 10.1109/CVPR46437.2021.00831
   Deng C, 2020, IEEE T IMAGE PROCESS, V29, P8892, DOI 10.1109/TIP.2020.3020383
   Deng C, 2018, IEEE T IMAGE PROCESS, V27, P3893, DOI 10.1109/TIP.2018.2821921
   Feng FX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P7, DOI 10.1145/2647868.2654902
   Gutmann Michael, 2010, P MACHINE LEARNING R, P297, DOI DOI 10.1145/3292500.3330651
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   He XT, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1740, DOI 10.1145/3343031.3350974
   Hjelm R. D., 2019, PROC INT C LEARN REP, P1, DOI [DOI 10.48550/ARXIV.1808.06670, 10.48550/arXiv.1808.06670]
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Hu P, 2020, AAAI CONF ARTIF INTE, V34, P99
   Hu P, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P635, DOI 10.1145/3331184.3331213
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Kang PP, 2019, ICMR'19: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P226, DOI 10.1145/3323873.3325029
   Kingma D. P., 2014, arXiv
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Mandal D, 2020, IEEE T MULTIMEDIA, V22, P2345, DOI 10.1109/TMM.2019.2954741
   Nair Vinod, 2010, INT C INT C MACHINE, P807
   Peng Y, 2016, P INT JOINT C ART IN, P3846
   Peng YX, 2018, IEEE T CIRC SYST VID, V28, P2372, DOI 10.1109/TCSVT.2017.2705068
   Peng YX, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3284750
   Peng YX, 2018, IEEE T MULTIMEDIA, V20, P405, DOI 10.1109/TMM.2017.2742704
   Peng YX, 2016, IEEE T CIRC SYST VID, V26, P583, DOI 10.1109/TCSVT.2015.2400779
   Qian SS, 2021, AAAI CONF ARTIF INTE, V35, P2440
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Rosipal R, 2006, LECT NOTES COMPUT SC, V3940, P34, DOI 10.1007/11752790_2
   Rupnik J., 2010, P C DATA MINING DATA, P1
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sharma A, 2011, PROC CVPR IEEE, P593, DOI 10.1109/CVPR.2011.5995350
   Sohn K, 2016, ADV NEUR IN, V29
   van den Oord A, 2019, Arxiv, DOI arXiv:1807.03748
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326
   Wang WR, 2015, PR MACH LEARN RES, V37, P1083
   Wang X, 2021, INFORM SCIENCES, V546, P298, DOI 10.1016/j.ins.2020.08.009
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Xie D, 2020, IEEE T IMAGE PROCESS, V29, P3626, DOI 10.1109/TIP.2020.2963957
   Xu X, 2019, WORLD WIDE WEB, V22, P657, DOI 10.1007/s11280-018-0541-x
   Yan F, 2015, PROC CVPR IEEE, P3441, DOI 10.1109/CVPR.2015.7298966
   Zhai XH, 2014, IEEE T CIRC SYST VID, V24, P965, DOI 10.1109/TCSVT.2013.2276704
   Zhang L, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3406
   Zhang L, 2018, IEEE T MULTIMEDIA, V20, P128, DOI 10.1109/TMM.2017.2723841
   Zhen LL, 2019, PROC CVPR IEEE, P10386, DOI 10.1109/CVPR.2019.01064
   Zheng ZD, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3383184
   Zou FH, 2019, WORLD WIDE WEB, V22, P825, DOI 10.1007/s11280-018-0581-2
NR 54
TC 15
Z9 15
U1 6
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2851
EP 2863
DI 10.1109/TMM.2022.3152086
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA M5LW1
UT WOS:001030640600031
DA 2024-07-18
ER

PT J
AU Ma, YW
   Ji, JY
   Sun, XS
   Zhou, YY
   Wu, YJ
   Huang, FY
   Ji, RR
AF Ma, Yiwei
   Ji, Jiayi
   Sun, Xiaoshuai
   Zhou, Yiyi
   Wu, Yongjian
   Huang, Feiyue
   Ji, Rongrong
TI Knowing What it is: Semantic-Enhanced Dual Attention Transformer
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Transformers; Task analysis; Semantics; Visualization; Integrated
   circuit modeling; Standards; Head; Image captioning; visual question
   answering; attention mechanism; transformer
AB Attention has become an indispensable component of the models of various multimedia tasks like Image Captioning (IC) and Visual Question Answering (VQA). However, most existing attention modules are designed for capturing the spatial dependency, and are still insufficient in semantic understanding, e.g., the categories of objects and their attributes, which is also critical for image captioning. To compensate for this defect, we propose a novel attention module termed Channel-wise Attention Block (CAB) to model channel-wise dependency for both visual modality and linguistic modality, thereby improving semantic learning and multi-modal reasoning simultaneously. Specifically, CAB has two novel designs to tackle with the high overhead of channel-wise attention, which are the reduction-reconstruction block structure and the gating-based attention prediction. Based on CAB, we further propose a novel Semantic-enhanced Dual Attention Transformer (termedSDATR), which combines the merits of spatial and channel-wise attentions. To validate SDATR, we conduct extensive experiments on the MS COCO dataset and yield new state-of-the-art performance of 134.5 CIDEr score on COCO Karpathy test split and 136.0 CIDEr score on the official online testing server. To examine the generalization of SDATR, we also apply it to the task of visual question answering, where superior performance gains are also witnessed. The code and models are publicly available at https:// github.com/ xmu-xiaoma666/SDATR.
C1 [Ma, Yiwei; Ji, Jiayi; Sun, Xiaoshuai; Zhou, Yiyi; Ji, Rongrong] Xiamen Univ, Sch Informat, Dept Artificial Intelligence, Media Analyt & Comp Lab, Xiamen 361005, Peoples R China.
   [Sun, Xiaoshuai; Ji, Rongrong] Xiamen Univ, Inst Artificial Intelligence, Xiamen 361005, Peoples R China.
   [Sun, Xiaoshuai] Xiamen Univ, Fujian Engn Res Ctr Trusted Artificial Intelligen, Xiamen 361005, Peoples R China.
   [Wu, Yongjian; Huang, Feiyue] Tencent, Youtu Lab, Shanghai 200233, Peoples R China.
C3 Xiamen University; Xiamen University; Xiamen University; Tencent
RP Sun, XS (corresponding author), Xiamen Univ, Sch Informat, Dept Artificial Intelligence, Media Analyt & Comp Lab, Xiamen 361005, Peoples R China.
EM yiweima@stu.xmu.edu.cn; jjyxmu@gmail.com; xssun@xmu.edu.cn;
   zhouyiyi@xmu.edu.cn; littlekenwu@tencent.com; garyhuang@tencent.com;
   rrji@xmu.edu.cn
RI Ma, Yazhou/GZM-0088-2022; Wang, Mengmeng/ABG-8454-2021
OI Ma, Yiwei/0000-0002-8744-3423; Ji, Jiayi/0000-0002-9956-6308
FU National Science Fund for Distinguished Young Scholars [62025603];
   National Natural Science Foundation of China [U21B2037, 62176222,
   62176223, 62176226, 62072386, 62072387, 62072389, 62002305]; Guangdong
   Basic and Applied Basic Research Foundation [2019B1515120049]; Natural
   Science Foundation of Fujian Province of China [2021J01002]
FX This work was supported in part by the National Science Fund for
   Distinguished Young Scholars under Grant 62025603, in part by the
   National Natural Science Foundation of China under Grants U21B2037,
   62176222, 62176223, 62176226, 62072386, 62072387, 62072389, and
   62002305, in part by Guangdong Basic and Applied Basic Research
   Foundation under Grant 2019B1515120049, and in part by the Natural
   Science Foundation of Fujian Province of China under Grant 2021J01002.
   The Associate Editor coordinating the reviewof this manuscript and
   approving it for publicationwas Prof. Catherine Zhao.
CR Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387
   Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24
   Ba J. L., 2016, LAYER NORMALIZATION, DOI DOI 10.48550/ARXIV.1607.06450
   Banerjee S., 2005, P WORKSHOP INTRINSIC, P65
   Carion N., 2020, EUR C COMP VIS, P213, DOI DOI 10.1007/978-3-030-58452-8_13
   Chen L, 2021, PROC CVPR IEEE, P16841, DOI 10.1109/CVPR46437.2021.01657
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Cheng B., 2021, Advances in Neural Information Processing Systems, V34, P34
   Chhabra A, 2020, INT CONF SOFT COMP, P121, DOI [10.1109/iscmi51676.2020.9311591, 10.1109/ISCMI51676.2020.9311591]
   Corbetta M, 2002, NAT REV NEUROSCI, V3, P201, DOI 10.1038/nrn755
   Cornia Marcella, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10575, DOI 10.1109/CVPR42600.2020.01059
   Cornia M, 2019, PROC CVPR IEEE, P8299, DOI 10.1109/CVPR.2019.00850
   Dai Zihang, 2021, ADV NEURAL INF PROCE, V34, P3, DOI DOI 10.48550/ARXIV.2106.04803
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   DOSOVITSKIY A, 2021, P INT C LEARN REPRES
   Fei ZC, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3182, DOI 10.1145/3394171.3413901
   Feng Y, 2019, PROC CVPR IEEE, P4120, DOI 10.1109/CVPR.2019.00425
   Fenglin Liu, 2020, MM '20: Proceedings of the 28th ACM International Conference on Multimedia, P4153, DOI 10.1145/3394171.3414004
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Biten AF, 2019, PROC CVPR IEEE, P12458, DOI 10.1109/CVPR.2019.01275
   Gan C, 2017, PROC CVPR IEEE, P955, DOI 10.1109/CVPR.2017.108
   Gan Z, 2017, PROC CVPR IEEE, P1141, DOI 10.1109/CVPR.2017.127
   Gomez R, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3164, DOI 10.1145/3394171.3413785
   Guo LT, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P765, DOI 10.1145/3343031.3350943
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Herdade S, 2019, ADV NEUR IN, V32
   Hodosh M, 2013, J ARTIF INTELL RES, V47, P853, DOI 10.1613/jair.3994
   Hong Yining, 2021, Advances in Neural Information Processing Systems, V34, P17427
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang L, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3136, DOI 10.1145/3394171.3413775
   Huang L, 2019, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2019.00473
   Ji JY, 2021, AAAI CONF ARTIF INTE, V35, P1655
   Jia X, 2015, IEEE I CONF COMP VIS, P2407, DOI 10.1109/ICCV.2015.277
   Jiang H., 2020, IEEE CVF C COMP VIS, DOI 10.1109/CVPR42600.2020.01028
   Jiang WH, 2018, LECT NOTES COMPUT SC, V11206, P510, DOI 10.1007/978-3-030-01216-8_31
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kim JH, 2018, ADV NEUR IN, V31
   Li G, 2019, IEEE I CONF COMP VIS, P8927, DOI 10.1109/ICCV.2019.00902
   Li GH, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1227, DOI 10.1145/3394171.3413943
   Li XP, 2019, AAAI CONF ARTIF INTE, P8658
   Li XY, 2019, IEEE T MULTIMEDIA, V21, P2117, DOI 10.1109/TMM.2019.2896516
   Lin Chin-Yew, 2004, Text summarization branches out, P74, DOI DOI 10.2307/3105454
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu F, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1175, DOI 10.1145/3343031.3350993
   Liu F, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4060, DOI 10.1145/3394171.3413924
   Liu W., 2021, CPTR FULL TRANSFORME
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lu JS, 2019, ADV NEUR IN, V32
   Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345
   Luo YP, 2021, AAAI CONF ARTIF INTE, V35, P2286
   MAO J, 2019, THE NEURO SYMBOLIC C
   MAO J, 2014, DEEP CAPTIONING WITH
   Meng LX, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3146, DOI 10.1145/3394171.3413499
   Nair V., 2010, P 27 INT C MACHINE L, P807
   Pan LM, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1132, DOI 10.1145/3394171.3413765
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Peng L, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1202, DOI 10.1145/3343031.3350925
   Plummer BA, 2015, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2015.303
   Radford A, 2021, PR MACH LEARN RES, V139
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Rensink RA, 2000, VIS COGN, V7, P17, DOI 10.1080/135062800394667
   Shaw P., 2018, P 2018 NAACL, V2, P464, DOI [DOI 10.18653/V1/N18-2074, 10.18653/v1/N18-2074]
   Shuster K, 2019, PROC CVPR IEEE, P12508, DOI 10.1109/CVPR.2019.01280
   Song YQ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P784, DOI 10.1145/3343031.3350996
   Stergiou A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10337, DOI 10.1109/ICCV48922.2021.01019
   Tan JH, 2019, IEEE T MULTIMEDIA, V21, P2686, DOI 10.1109/TMM.2019.2904878
   Teney D, 2018, PROC CVPR IEEE, P4223, DOI 10.1109/CVPR.2018.00444
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Vaswani Ashish, 2017, Advances in Neural Information Processing Systems (NeurIPS), V17, P6000, DOI DOI 10.48550/ARXIV.1706.03762
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang C, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3115432
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/isqed48828.2020.9137057, 10.1109/ISQED48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang WT, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2400, DOI 10.1145/3394171.3413507
   Woo S., 2018, P EUR C COMP VIS ECC, DOI DOI 10.1007/978-3-030-01234-2_1
   Wu J, 2021, IEEE T MULTIMEDIA, V23, P2413, DOI 10.1109/TMM.2020.3011317
   Xiao XY, 2019, IEEE T MULTIMEDIA, V21, P2942, DOI 10.1109/TMM.2019.2915033
   Xie EZ, 2021, ADV NEUR IN, V34
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yang M, 2019, IEEE T MULTIMEDIA, V21, P1047, DOI 10.1109/TMM.2018.2869276
   Yang X, 2019, PROC CVPR IEEE, P10677, DOI 10.1109/CVPR.2019.01094
   Yao T, 2019, IEEE I CONF COMP VIS, P2621, DOI 10.1109/ICCV.2019.00271
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yao T, 2017, IEEE I CONF COMP VIS, P4904, DOI 10.1109/ICCV.2017.524
   Yen-Chun Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P104, DOI 10.1007/978-3-030-58577-8_7
   YI K, 2020, P INT C LEARN REPRES
   Yi KX, 2018, ADV NEUR IN, V31
   Yingwei Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10968, DOI 10.1109/CVPR42600.2020.01098
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Young P., 2014, Transactions of the Association for Computational Linguistics, V2, P67
   Yu Z, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3743, DOI 10.1145/3394171.3413977
   Yu Z, 2018, IEEE T NEUR NET LEAR, V29, P5947, DOI 10.1109/TNNLS.2018.2817340
   Yu Z, 2019, PROC CVPR IEEE, P6274, DOI 10.1109/CVPR.2019.00644
   Yu Z, 2017, IEEE I CONF COMP VIS, P1839, DOI 10.1109/ICCV.2017.202
   Zhan LM, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2345, DOI 10.1145/3394171.3413761
   ZHANG BC, 2020, CAPTIONING PROC 28TH, P1112
   Zhang XY, 2021, PROC CVPR IEEE, P15460, DOI 10.1109/CVPR46437.2021.01521
   Zhao L, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2336, DOI 10.1145/3394171.3413760
   Zhou HY, 2021, AAAI CONF ARTIF INTE, V35, P11106
   Zhou YY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1245, DOI 10.1145/3394171.3413998
   ZHU X, 2021, P INT C LEARN REPRES
   Zi BJ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2382, DOI 10.1145/3394171.3413769
NR 105
TC 9
Z9 9
U1 5
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3723
EP 3736
DI 10.1109/TMM.2022.3164787
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FE2H4
UT WOS:001144015500015
PM 35791675
DA 2024-07-18
ER

PT J
AU Meng, DC
   Li, L
   Liu, XJ
   Gao, L
   Huang, QM
AF Meng, Dechao
   Li, Liang
   Liu, Xuejing
   Gao, Lin
   Huang, Qingming
TI Viewpoint Alignment and Discriminative Parts Enhancement in 3D Space for
   Vehicle ReID
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D reconstruction; feature enhancement; vehicle ReID; viewpoint
   alignment
AB Vehicle Re-Identification is to find the same vehicle from images captured in different views under cross-camera scenarios. Traditional methods focus on depicting the holistic appearance of a vehicle, but they suffer from the hard samples with the same vehicle type and color. Recent works leverage the discriminative visual cues to solve this problem, where three challenges exist as follows. First, vehicle features are misaligned and distorted because of the viewpoint variance. Second, the discriminative visual cues are usually subtle, which is easy to be diluted by the large area of non-discriminative regions in subsequent average pooling modules. Third, these discriminative visual cues are dynamic for the same image when it compares with different vehicle images. To tackle the above problems, we project the vehicle images from 2D to 3D space and rotate them to the same view, and leverage the viewpoint aligned features to enhance the discriminative parts for vehicle ReID. In detail, our method consists of three sub-modules, 1) The 3D viewpoint alignment module restores the 3D information of the vehicle from a single vehicle image, and then rotates and re-renders it under fixed viewpoints. It enables fine-grained viewpoint alignment and relieves the distortion of the vehicle caused by the viewpoint variation. 2) The discriminative parts enhancement module performs feature enhancement guided by the prior distribution of distinctive parts. 3) The adaptive duplicated parts suppression module guides the network to focus on the most discriminative parts, which not only prevents the dilution of the high responses but also provides explainable evidence. The experimental results reveal our method achieves new state-of-the-art on large scale vehicle ReID dataset.
C1 [Meng, Dechao; Li, Liang; Liu, Xuejing] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
   [Huang, Qingming] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Huang, Qingming] Univ Chinese Acad Sci, Inst Comp Technol, Beijing 100190, Peoples R China.
   [Gao, Lin] Univ Chinese Acad Sci, Inst Comp Technol, Beijing Key Lab Mobile Comp & Pervas Device, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Chinese Academy of Sciences; University of Chinese Academy of
   Sciences, CAS; Chinese Academy of Sciences; University of Chinese
   Academy of Sciences, CAS
RP Li, L (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
EM dechao.meng@vipl.ict.ac.cn; liang.li@ict.ac.cn;
   xuejing.liu@vipl.ict.ac.cn; gaolin@ict.ac.cn; qmhuang@ucas.ac.cn
RI Gao, Lin/JNF-0375-2023
OI Liu, Xuejing/0000-0001-9612-3707
FU National Key Ramp;D Program of China [2018AAA0102000]; National Natural
   Science Foundation of China [61732007]; Youth Innovation Promotion
   Association of Chinese Academy of Sciences [2020108]; CCF-Baidu Open
   Fund [2021PP15002000]; CAAI-Huawei MindSpore Open Fund
FX This work was supported in part by the National Key R & D Program of
   China under Grant 2018AAA0102000, in part by the National Natural
   Science Foundation of China under Grant 61732007, in part by the Youth
   Innovation Promotion Association of Chinese Academy of Sciences under
   Grant 2020108, in part by CCF-Baidu Open Fund under Grant
   2021PP15002000, and in part by CAAI-Huawei MindSpore Open Fund. The
   Associate Editor coordinating the review of this manuscript and
   approving it for publication was Prof. Ichiro Ide.& nbsp;
CR Bai Y, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P474
   Bai Y, 2018, IEEE T MULTIMEDIA, V20, P2385, DOI 10.1109/TMM.2018.2796240
   Chu RH, 2019, IEEE I CONF COMP VIS, P8281, DOI 10.1109/ICCV.2019.00837
   Goodfellow I, 2014, ADV NEURAL INFORM PR, P2672
   Guo HY, 2018, AAAI CONF ARTIF INTE, P6853
   He B, 2019, PROC CVPR IEEE, P3992, DOI 10.1109/CVPR.2019.00412
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He LX, 2020, Arxiv, DOI arXiv:2006.02631
   Khorramshahi Pirazh, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P369, DOI 10.1007/978-3-030-58568-6_22
   Khorramshahi P, 2019, IEEE I CONF COMP VIS, P6131, DOI 10.1109/ICCV.2019.00623
   Li HC, 2022, IEEE TETCI, V6, P1211, DOI 10.1109/TETCI.2021.3127906
   Li L, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3875, DOI 10.1145/3394171.3413830
   Li L, 2016, NEUROCOMPUTING, V174, P384, DOI 10.1016/j.neucom.2015.04.108
   Liu HY, 2016, PROC CVPR IEEE, P2167, DOI 10.1109/CVPR.2016.238
   Liu W, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1618, DOI 10.1145/3123266.3123422
   Liu XB, 2018, IEEE INT CON MULTI
   Liu XC, 2016, IEEE INT CON MULTI
   Liu XC, 2018, IEEE T MULTIMEDIA, V20, P645, DOI 10.1109/TMM.2017.2751966
   Lou YH, 2019, PROC CVPR IEEE, P3230, DOI 10.1109/CVPR.2019.00335
   Lou YH, 2019, IEEE T IMAGE PROCESS, V28, P3794, DOI 10.1109/TIP.2019.2902112
   Meng DC, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P619, DOI 10.1145/3394171.3413573
   Meng DC, 2020, PROC CVPR IEEE, P7101, DOI 10.1109/CVPR42600.2020.00713
   Shen F., IN PRESS
   Shen YT, 2017, IEEE I CONF COMP VIS, P1918, DOI 10.1109/ICCV.2017.210
   Song XB, 2019, PROC CVPR IEEE, P5447, DOI 10.1109/CVPR.2019.00560
   Suprem A, 2020, Arxiv, DOI arXiv:2002.02256
   Tang Z, 2019, IEEE I CONF COMP VIS, P211, DOI 10.1109/ICCV.2019.00030
   Tsai-Shien Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P330, DOI 10.1007/978-3-030-58536-5_20
   Wang ZD, 2017, IEEE I CONF COMP VIS, P379, DOI 10.1109/ICCV.2017.49
   Xue Z, 2019, INFORM SCIENCES, V482, P210, DOI 10.1016/j.ins.2019.01.018
   Zhang X., 2020, IEEE Transactions on Intelligent Transportation Systems
   Zhao JJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P205, DOI 10.1109/ICCV48922.2021.00027
   Zheng ZD, 2021, IEEE T MULTIMEDIA, V23, P2683, DOI 10.1109/TMM.2020.3014488
   Zhou Y, 2018, PROC CVPR IEEE, pCP99, DOI 10.1109/CVPR.2018.00679
   Zhu YC, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P646, DOI 10.1145/3394171.3413607
NR 35
TC 4
Z9 4
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2954
EP 2965
DI 10.1109/TMM.2022.3154102
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA O7RP6
UT WOS:001045742200001
DA 2024-07-18
ER

PT J
AU Nami, S
   Pakdaman, F
   Hashemi, MR
   Shirmohammadi, S
AF Nami, Sanaz
   Pakdaman, Farhad
   Hashemi, Mahmoud Reza
   Shirmohammadi, Shervin
TI BL-JUNIPER: A CNN-Assisted Framework for Perceptual Video Coding
   Leveraging Block-Level JND
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Convolutional neural network (cnn); just noticeable distortion (jnd);
   perceptual video coding (pvc); quantization control algorithm; visual
   attention
ID NOTICEABLE DISTORTION PROFILE; ATTENTION MODEL; QUALITY; SALIENCY;
   IMAGE; VISIBILITY
AB Just Noticeable Distortion (JND) finds the minimum distortion level perceivable by humans. This can be a natural solution for setting the compression for each video region in perceptual video coding. However, existing JND-based solutions estimate JND levels for each video frame and ignore the fact that different video regions have different perceptual importance. To address this issue, we propose a Block-Level Just Noticeable Distortion-based Perceptual (BL-JUNIPER) framework for video coding. The proposed four-stage framework combines different perceptual information to further improve the prediction accuracy. The JND mapping in the first stage derives block-level JNDs from frame-level information without the need to collect a new bock-level JND dataset. In the second stage, an efficient CNN-based model is proposed to predict JND levels for each block according to spatial and temporal characteristics. Unlike existing methods, BL-JUNIPER works on raw video frames and avoids re-encoding each frame several times, making it computationally practical. Third, the visual importance of each block is measured using a visual attention model. Finally, a proposed quantization control algorithm uses both JND levels and visual importance to adjust the Quantization Parameter (QP) for each block. The specific algorithm for each stage of the proposed framework can be changed, as long as the input and output formats of each block are followed, without the need to change other stages, based on any current or future methods, providing a flexible and robust solution. Extensive experimental results demonstrate that BL-JUNIPER achieves a mean bitrate reduction of 27.75% with a Delta Mean Opinion Score (DMOS) close to zero and BD-Rate gains of 25.44% based on MOS, compared to the baseline encoding, and also gains a better performance compared to competing methods.
C1 [Nami, Sanaz; Hashemi, Mahmoud Reza] Univ Tehran, Sch Elect & Comp Engn, Tehran 1417935840, Iran.
   [Pakdaman, Farhad] Univ Mazandaran, Fac Engn & Technol, Babolsar 4741613534, Iran.
   [Shirmohammadi, Shervin] Univ Ottawa, Sch Elect Engn & Comp Sci, DISCOVER Lab, Ottawa, ON K1N 6N5, Canada.
C3 University of Tehran; University of Mazandaran; University of Ottawa
RP Hashemi, MR (corresponding author), Univ Tehran, Sch Elect & Comp Engn, Tehran 1417935840, Iran.
EM snami@ut.ac.ir; farhad.pakdaman@umz.ac.ir; rhashemi@ut.ac.ir;
   shervin@eecs.uottawa.ca
RI Pakdaman, Farhad/L-1457-2019; Shirmohammadi, Shervin/E-6945-2012;
   Hashemi, Mahmoud Reza/H-2172-2011
OI Pakdaman, Farhad/0000-0001-6526-3811; Shirmohammadi,
   Shervin/0000-0002-3973-4445; Nami, Sanaz/0000-0002-4826-1168; Hashemi,
   Mahmoud Reza/0000-0002-3518-9195
CR Achanta R., 2010, EPFL Technical Report 149300, V6, P15
   Ahmadi H, 2021, IEEE ACCESS, V9, P12332, DOI 10.1109/ACCESS.2021.3050489
   Ahmadi H, 2014, MULTIMEDIA SYST, V20, P485, DOI 10.1007/s00530-014-0381-1
   Altamimi S, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3397227
   Altamimi S, 2019, PROCEEDINGS OF THE 29TH ACM WORKSHOP ON NETWORK AND OPERATING SYSTEMS SUPPORT FOR DIGITAL AUDIO AND VIDEO (NOSSDAV'19), P1, DOI 10.1145/3304112.3325604
   [Anonymous], 2021, ITU-T Rec. P.910
   [Anonymous], 2002, METHODOLOGY SUBJECTI
   [Anonymous], 2019, The Paper
   Bae SH, 2017, IEEE T CIRC SYST VID, V27, P1196, DOI 10.1109/TCSVT.2016.2539862
   Bae SH, 2016, IEEE T IMAGE PROCESS, V25, P3343, DOI 10.1109/TIP.2016.2568459
   Ballé J, 2021, IEEE J-STSP, V15, P339, DOI 10.1109/JSTSP.2020.3034501
   Bampis CG, 2018, IEEE T IMAGE PROCESS, V27, P3316, DOI 10.1109/TIP.2018.2815842
   Bjontegaard G, 2001, P 13 VID COD EXP GR, P1
   Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Bossen F., 2012, JCTVCH1100
   Doumanoglou A, 2018, IEEE T BROADCAST, V64, P379, DOI 10.1109/TBC.2018.2823909
   Fan CL, 2020, IEEE T MULTIMEDIA, V22, P744, DOI 10.1109/TMM.2019.2931807
   Jiang XT, 2019, ENTROPY-SWITZ, V21, DOI 10.3390/e21020165
   Jin L., 2016, P INT S EL IM SCI TE, P1
   Ki S, 2018, IEEE T IMAGE PROCESS, V27, P3178, DOI 10.1109/TIP.2018.2818439
   Kingma D. P., 2014, arXiv
   Lam YH, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P358, DOI 10.1145/3394171.3413536
   Li XH, 2013, IEEE I CONF COMP VIS, P2976, DOI 10.1109/ICCV.2013.370
   Liu HH, 2020, IEEE T IMAGE PROCESS, V29, P641, DOI 10.1109/TIP.2019.2933743
   Nair V., 2010, P 27 INT C MACHINE L, P807
   Nami S, 2020, IEEE INT CONF MULTI
   Pakdaman F, 2020, IEEE IMAGE PROC, P3134, DOI 10.1109/ICIP40778.2020.9190983
   Parastar P., 2020, P PACK VID WORKSH JU, P21
   Rosewarne C., 2016, JCTVCW1002
   Shen XL, 2021, IEEE T IMAGE PROCESS, V30, P26, DOI 10.1109/TIP.2020.3029428
   Shen XL, 2020, INT CONF ACOUST SPEE, P2058, DOI [10.1109/ICASSP40776.2020.9053580, 10.1109/icassp40776.2020.9053580]
   Sun XB, 2020, NEUROCOMPUTING, V411, P393, DOI 10.1016/j.neucom.2020.06.003
   Sun XB, 2019, IEEE ACCESS, V7, P56308, DOI 10.1109/ACCESS.2019.2910245
   Takeuchi M, 2018, PICT COD SYMP, P179, DOI 10.1109/PCS.2018.8456297
   Tian T, 2020, IEEE T BROADCAST, V66, P690, DOI 10.1109/TBC.2020.2977542
   Wang HG, 2016, IEEE IMAGE PROC, P1509, DOI 10.1109/ICIP.2016.7532610
   Wang HQ, 2017, J VIS COMMUN IMAGE R, V46, P292, DOI 10.1016/j.jvcir.2017.04.009
   Wang HQ, 2014, INT SYMP WIREL, P106, DOI 10.1109/WPMC.2014.7014800
   Wang SQ, 2016, IEEE T IMAGE PROCESS, V25, P3838, DOI 10.1109/TIP.2016.2573597
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei ZY, 2009, IEEE T CIRC SYST VID, V19, P337, DOI 10.1109/TCSVT.2009.2013518
   Wu JJ, 2017, IEEE T IMAGE PROCESS, V26, P2682, DOI 10.1109/TIP.2017.2685682
   Wu JJ, 2013, IEEE T MULTIMEDIA, V15, P1705, DOI 10.1109/TMM.2013.2268053
   Wu JJ, 2013, IEEE T IMAGE PROCESS, V22, P4892, DOI 10.1109/TIP.2013.2279934
   Wu JJ, 2012, J VIS COMMUN IMAGE R, V23, P845, DOI 10.1016/j.jvcir.2012.04.010
   Yang AS, 2017, MULTIDIM SYST SIGN P, V28, P1249, DOI 10.1007/s11045-016-0395-2
   Yuan D, 2019, IEEE ACCESS, V7, P29014, DOI 10.1109/ACCESS.2019.2901342
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang XF, 2017, IEEE SIGNAL PROC LET, V24, P96, DOI 10.1109/LSP.2016.2641456
   Zhang XY, 2019, IEEE IMAGE PROC, P4140, DOI [10.1109/icip.2019.8803454, 10.1109/ICIP.2019.8803454]
   Zhu SP, 2018, NEUROCOMPUTING, V275, P511, DOI 10.1016/j.neucom.2017.08.054
NR 51
TC 5
Z9 5
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5077
EP 5092
DI 10.1109/TMM.2022.3187259
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300034
DA 2024-07-18
ER

PT J
AU Peng, KY
   Roitberg, A
   Yang, KL
   Zhang, JM
   Stiefelhagen, R
AF Peng, Kunyu
   Roitberg, Alina
   Yang, Kailun
   Zhang, Jiaming
   Stiefelhagen, Rainer
TI Delving Deep Into One-Shot Skeleton-Based Action Recognition With
   Diverse Occlusions
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Transformers; Three-dimensional displays; Task analysis; Benchmark
   testing; Joints; Prototypes; Image recognition; Computer vision; human
   activity recognition; representation learning
AB Occlusions areuniversal disruptions constantly present in the real world. Especially for sparse representations, such as human skeletons, a few occluded points might destroy the geometrical and temporal continuity critically affecting the results. Yet, the research of data-scarce recognition from skeleton sequences, such as one-shot action recognition, does not explicitly consider occlusions despite their everyday pervasiveness. In this work, we explicitly tackle body occlusions for Skeleton-based One-shot Action Recognition (SOAR). We mainly consider two occlusion variants: 1) random occlusions and 2) more realistic occlusions caused by diverse everyday objects, which we generate by projecting the existing IKEA 3D furniture models into the camera coordinate system of the 3D skeletons with different geometric parameters, (e.g., rotation and displacement). We leverage the proposed pipeline to blend out portions of skeleton sequences of the three popular action recognition datasets (NTU-120, NTU-60 and Toyota Smart Home) and formalize the first benchmark for SOAR from partially occluded body poses. This is the first benchmark which considers occlusions for data-scarce action recognition. Another key property of our benchmark are the more realistic occlusions generated by everyday objects, as even in standard recognition from 3D skeletons, only randomly missing joints were considered. We re-evaluate existing state-of-the-art frameworks for SOAR in the light of this new task and further introduce Trans4SOAR - a new transformer-based model which leverages three data streams and mixed attention fusion mechanism to alleviate the adverse effects caused by occlusions. While our experiments demonstrate a clear decline in accuracy with missing skeleton portions, this effect is smaller with Trans4SOAR, which outperforms other architectures on all datasets. Although we specifically focus on occlusions, Trans4SOAR additionally yields state-of-the-art in the standard SOAR without occlusion, surpassing the best published approach by 2.85% on NTU-120.
C1 [Peng, Kunyu; Roitberg, Alina; Yang, Kailun; Zhang, Jiaming; Stiefelhagen, Rainer] Karlsruhe Inst Technol, Inst Anthropomat & Robot, D-76131 Karlsruhe, Germany.
C3 Helmholtz Association; Karlsruhe Institute of Technology
RP Yang, KL (corresponding author), Karlsruhe Inst Technol, Inst Anthropomat & Robot, D-76131 Karlsruhe, Germany.
EM kunyu.peng@kit.edu; alina.roitberg@kit.edu; kailun.yang@kit.edu;
   jiaming.zhang@kit.edu; rainer.stiefelhagen@kit.edu
RI Roitberg, Alina/JWA-5408-2024; Yang, Kailun/U-2491-2019
OI Yang, Kailun/0000-0002-1090-667X; Peng, Kunyu/0000-0002-5419-9292;
   Roitberg, Alina/0000-0003-4724-9164; Zhang, Jiaming/0000-0003-3471-328X
CR Angelini F, 2020, IEEE T MULTIMEDIA, V22, P1433, DOI 10.1109/TMM.2019.2944745
   [Anonymous], 2016, ARXIV160207360
   Arnab A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6816, DOI 10.1109/ICCV48922.2021.00676
   Bai R, 2022, 2022 IEEE INT C MULT, P1, DOI 10.1109/ICME52920.2022.9859781
   Bendou Y, 2022, J IMAGING, V8, DOI 10.3390/jimaging8070179
   Cao CQ, 2021, COMPUT VIS IMAGE UND, V210, DOI 10.1016/j.cviu.2021.103250
   Chen YX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13339, DOI 10.1109/ICCV48922.2021.01311
   Cheng YB, 2021, P 2 ACM INT C MULT A, P1
   Chia-Wen Kuo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P479, DOI 10.1007/978-3-030-58523-5_28
   Chu XX, 2021, ADV NEUR IN
   Cui YT, 2022, PROC CVPR IEEE, P13598, DOI 10.1109/CVPR52688.2022.01324
   Das S, 2019, IEEE I CONF COMP VIS, P833, DOI 10.1109/ICCV.2019.00092
   Dosovitskiy Alexey, 2021, 2021 INT C LEARN REP
   Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256
   Gao Z, 2021, IEEE T IMAGE PROCESS, V30, P767, DOI 10.1109/TIP.2020.3038372
   Ghafoor M, 2023, IEEE T MULTIMEDIA, V25, P3311, DOI 10.1109/TMM.2022.3158068
   Graham B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12239, DOI 10.1109/ICCV48922.2021.01204
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hong J, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9234, DOI 10.1109/ICCV48922.2021.00912
   Hu S. X., 2022, CVPR, P9068
   Hu YQ, 2022, ALGORITHMS, V15, DOI 10.3390/a15050147
   Krizhevsky Alex, 2014, ARXIV14045997
   Li D, 2021, LECT NOTES COMPUT SC, V13002, P178, DOI 10.1007/978-3-030-89029-2_14
   Li JF, 2019, PROC CVPR IEEE, P10855, DOI 10.1109/CVPR.2019.01112
   Li SC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13648, DOI 10.1109/ICCV48922.2021.01341
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P2684, DOI 10.1109/TPAMI.2019.2916873
   Liu J, 2018, IEEE T PATTERN ANAL, V40, P3007, DOI 10.1109/TPAMI.2017.2771306
   Liu J, 2017, PROC CVPR IEEE, P3671, DOI 10.1109/CVPR.2017.391
   Liu MY, 2018, IEEE T MULTIMEDIA, V20, P1932, DOI 10.1109/TMM.2017.2786868
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Loshchilov I., 2019, arXiv
   Mazzia V, 2022, PATTERN RECOGN, V124, DOI 10.1016/j.patcog.2021.108487
   Memmesheimer R, 2022, IEEE WINT CONF APPL, P837, DOI 10.1109/WACV51458.2022.00091
   Memmesheimer R, 2021, INT C PATT RECOG, P4573, DOI 10.1109/ICPR48806.2021.9413336
   Patravali J, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8464, DOI 10.1109/ICCV48922.2021.00837
   Peng KY, 2022, IEEE INT C INT ROBOT, P278, DOI 10.1109/IROS47612.2022.9981445
   Perrett T, 2021, PROC CVPR IEEE, P475, DOI 10.1109/CVPR46437.2021.00054
   Plizzari Chiara, 2021, Pattern Recognition. ICPR International Workshops and Challenges. Proceedings. Lecture Notes in Computer Science (LNCS 12663), P694, DOI 10.1007/978-3-030-68796-0_50
   Plizzari C, 2021, COMPUT VIS IMAGE UND, V208, DOI 10.1016/j.cviu.2021.103219
   Sabater A, 2021, IEEE COMPUT SOC CONF, P2771, DOI 10.1109/CVPRW53098.2021.00312
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Shi F, 2021, Arxiv, DOI [arXiv:2107.07089, DOI 10.48550/ARXIV.2107.07089]
   Song YF, 2023, IEEE T PATTERN ANAL, V45, P1474, DOI 10.1109/TPAMI.2022.3157033
   Song YF, 2021, IEEE T CIRC SYST VID, V31, P1915, DOI 10.1109/TCSVT.2020.3015051
   Song YF, 2019, IEEE IMAGE PROC, P1, DOI [10.1109/icip.2019.8802917, 10.1109/ICIP.2019.8802917, 10.1109/TFUZZ.2019.2910714]
   Sun XY, 2018, PROC CVPR IEEE, P2974, DOI 10.1109/CVPR.2018.00314
   Touvron H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P32, DOI 10.1109/ICCV48922.2021.00010
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Tsutsui S., 2019, NeurIPS, P3057
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang X, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P816, DOI 10.1145/3474085.3475253
   Wu Y, 2020, IEEE INT CONF BIG DA, P3355, DOI 10.1109/BigData50022.2020.9378458
   Xiaolu Ding, 2020, ICCPR 2020: Proceedings of the 2020 9th International Conference on Computing and Pattern Recognition, P43, DOI 10.1145/3436369.3436470
   Xiu Y., 2018, BMVC
   Xue WQ, 2020, AAAI CONF ARTIF INTE, V34, P6558
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Zhang JM, 2021, IEEE INT CONF COMP V, P1760, DOI 10.1109/ICCVW54120.2021.00202
   Zhang QL, 2021, ADV NEUR IN, V34
   Zhang YY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13557, DOI [10.1109/iccv48922.2021.01332, 10.1109/ICCV48922.2021.01332]
   Zhang YH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3229, DOI 10.1145/3474085.3475473
   Zou Y., 2018, IEEE INT C MULTIMEDI, P1, DOI 10.1109/ICME.2018.8486447
   Zou YX, 2020, IEEE T MULTIMEDIA, V22, P3166, DOI 10.1109/TMM.2020.2972128
NR 63
TC 7
Z9 7
U1 6
U2 27
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1489
EP 1504
DI 10.1109/TMM.2023.3235300
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA G2GW2
UT WOS:000987415000008
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Peng, YX
   Jiao, JL
   Feng, XT
   Zheng, WS
AF Peng, Yi-Xing
   Jiao, Jile
   Feng, Xuetao
   Zheng, Wei-Shi
TI Consistent Discrepancy Learning for Intra-Camera Supervised Person
   Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Intra-camera supervised; likelihood prototypes; person re-identification
AB Since annotating pedestrians across different views is extremely costly, intra-camera supervised person re-identification (ReID) aims to learn a ReID model from the intra-view labeled data. Under this setting, the most challenge lies in learning a view-invariant feature embedding in the absence of the cross-view annotations. Previous works focus on assigning a pseudo identity label for each image based on the feature similarity and learn view-invariant features by classification loss. However, because of the cross-view variations in lighting, background, etc., the pseudo labels are often noisy, and therefore not reliable for classification. In this paper, we explore learning a consistent discrepancy for pairwise images. Our main idea is that the discrepancy between pedestrian images should be consistent across different views regardless of view change so that it mainly depicts the identity difference. Due to the lack of cross-view annotations, we project images into different views and obtain likelihood prototypes for cross-view learning. These likelihood prototypes are used to measure the discrepancies between pairwise images under different views. And then, we propose an intra-view discrepancy preservation module to enforce the discrepancy to be view-consistent so as to encourage the model to distinguish the images based on the identities regardless of view change. Extensive experiments on multiple datasets show that our method outperforms existing related methods by clear margins and our method is comparable to supervised counterparts. Code will be made publicly available.
C1 [Peng, Yi-Xing] Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 510275, Peoples R China.
   [Jiao, Jile; Feng, Xuetao] Alibaba Grp, Beijing 100102, Peoples R China.
   [Zheng, Wei-Shi] Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 510275, Peoples R China.
C3 Sun Yat Sen University; Alibaba Group; Sun Yat Sen University
RP Zheng, WS (corresponding author), Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 510275, Peoples R China.
EM pengyx23@mail2.sysu.edu.cn; jile.jjl@alibaba-inc.com;
   xuetao.fxt@alibaba-inc.com; wszheng@ieee.org
FU NSFC [U21A20471, U1911401, U1811461]; Guangdong NSF Project
   [2020B1515120085, 2018B030312002]; Guangzhou Research Project
   [201902010037]; Research Projects of Zhejiang Lab [2019KD0AB03];
   Key-Area Research and Development Program of Guangzhou [202007030004]
FX This work was supported in part by the NSFC under Grants U21A20471,
   U1911401, and U1811461, in part by Guangdong NSF Project under Grants
   2020B1515120085 and 2018B030312002, in part by Guangzhou Research
   Project under Grant 201902010037, in part by the Research Projects of
   Zhejiang Lab under Grant 2019KD0AB03, and in part by the Key-Area
   Research and Development Program of Guangzhou under Grant 202007030004.
CR Chang XB, 2018, PROC CVPR IEEE, P2109, DOI 10.1109/CVPR.2018.00225
   Chen H, 2021, PROC CVPR IEEE, P2004, DOI 10.1109/CVPR46437.2021.00204
   Chen TL, 2019, IEEE I CONF COMP VIS, P8350, DOI 10.1109/ICCV.2019.00844
   Chuanchen Luo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P224, DOI 10.1007/978-3-030-58555-6_14
   Deng WJ, 2018, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.2018.00110
   Dongkai Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10978, DOI 10.1109/CVPR42600.2020.01099
   Fang Zhao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P526, DOI 10.1007/978-3-030-58621-8_31
   Fu Y, 2019, IEEE I CONF COMP VIS, P6111, DOI 10.1109/ICCV.2019.00621
   Ge Y., 2020, ADV NEURAL INFORM PR, P11309
   Ge Yixiao, 2020, ARXIV200101526
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Guangyi Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P643, DOI 10.1007/978-3-030-58598-3_38
   He R, 2019, IEEE T PATTERN ANAL, V41, P1761, DOI 10.1109/TPAMI.2018.2842770
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Huang Y, 2019, IEEE T IMAGE PROCESS, V28, P1391, DOI 10.1109/TIP.2018.2874715
   Jianing Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P483, DOI 10.1007/978-3-030-58586-0_29
   Jin X, 2020, PROC CVPR IEEE, P3140, DOI 10.1109/CVPR42600.2020.00321
   Kaiwei Zeng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13654, DOI 10.1109/CVPR42600.2020.01367
   Li M., 2018, PROC EUR C COMPUT, P737
   Li MX, 2020, IEEE T PATTERN ANAL, V42, P1770, DOI 10.1109/TPAMI.2019.2903058
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Lin YT, 2020, PROC CVPR IEEE, P3387, DOI 10.1109/CVPR42600.2020.00345
   Lin YT, 2019, AAAI CONF ARTIF INTE, P8738
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Meng JK, 2019, PROC CVPR IEEE, P760, DOI 10.1109/CVPR.2019.00085
   Qi L, 2020, IEEE T CIRC SYST VID, V30, P2815, DOI 10.1109/TCSVT.2020.2983600
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Wan CQ, 2020, IEEE T MULTIMEDIA, V22, P1605, DOI 10.1109/TMM.2019.2946486
   Wang F, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1041, DOI 10.1145/3123266.3123359
   Wang GC, 2019, AAAI CONF ARTIF INTE, P8933
   Wang JY, 2018, PROC CVPR IEEE, P2275, DOI 10.1109/CVPR.2018.00242
   Wang ML, 2021, AAAI CONF ARTIF INTE, V35, P2764
   Wang ML, 2021, IEEE WINT CONF APPL, P3228, DOI 10.1109/WACV48630.2021.00327
   Wang Z, 2019, IEEE T MULTIMEDIA, V21, P2376, DOI 10.1109/TMM.2019.2898753
   Wang Z, 2016, IEEE T MULTIMEDIA, V18, P260, DOI 10.1109/TMM.2015.2505083
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wu AC, 2019, IEEE I CONF COMP VIS, P6921, DOI 10.1109/ICCV.2019.00702
   Wu GL, 2020, AAAI CONF ARTIF INTE, V34, P12362
   Wu JL, 2019, IEEE I CONF COMP VIS, P8320, DOI 10.1109/ICCV.2019.00841
   Xie QK, 2021, IEEE T MULTIMEDIA, V23, P597, DOI 10.1109/TMM.2020.2985525
   Xuan SY, 2021, PROC CVPR IEEE, P11921, DOI 10.1109/CVPR46437.2021.01175
   Yang F, 2021, IEEE T MULTIMEDIA, V23, P2347, DOI 10.1109/TMM.2020.3009476
   Yang QZ, 2019, PROC CVPR IEEE, P3628, DOI 10.1109/CVPR.2019.00375
   Yang Zou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P87, DOI 10.1007/978-3-030-58536-5_6
   Ye M, 2016, IEEE T MULTIMEDIA, V18, P2553, DOI 10.1109/TMM.2016.2605058
   Yu HX, 2020, PROC CVPR IEEE, P5527, DOI 10.1109/CVPR42600.2020.00557
   Yu HX, 2019, PROC CVPR IEEE, P2143, DOI 10.1109/CVPR.2019.00225
   Yu HX, 2020, IEEE T PATTERN ANAL, V42, P956, DOI 10.1109/TPAMI.2018.2886878
   Yu HX, 2017, IEEE I CONF COMP VIS, P994, DOI 10.1109/ICCV.2017.113
   Zeng ZL, 2020, IEEE T MULTIMEDIA, V22, P3064, DOI 10.1109/TMM.2020.2969782
   Zhai Y, 2020, COMPUTER VISION ECCV, P594, DOI DOI 10.1007/978-3-030-58571-6_35
   Zhang X, 2021, PROC CVPR IEEE, P3435, DOI 10.1109/CVPR46437.2021.00344
   Zhang ZZ, 2020, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR42600.2020.00325
   Zhao CR, 2020, IEEE T MULTIMEDIA, V22, P3180, DOI 10.1109/TMM.2020.2972125
   Zheng K., 2021, P IEEE CVF C COMP VI, P5310
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng WS, 2016, IEEE T PATTERN ANAL, V38, P591, DOI 10.1109/TPAMI.2015.2453984
   Zhong Z, 2019, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2019.00069
   Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380
   Zhu XP, 2021, INT J COMPUT VISION, V129, P1580, DOI 10.1007/s11263-021-01440-4
   Zhu XP, 2019, IEEE INT CONF COMP V, P1079, DOI 10.1109/ICCVW.2019.00138
NR 61
TC 4
Z9 4
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2393
EP 2403
DI 10.1109/TMM.2022.3146775
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100061
DA 2024-07-18
ER

PT J
AU Sun, JJ
   Zhao, Y
   Wang, SG
   Wei, J
AF Sun, Jianjun
   Zhao, Yan
   Wang, Shigang
   Wei, Jian
TI 3D Holoscopic Image Compression Based on Gaussian Mixture Model
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Image coding; Gaussian distribution;
   Covariance matrices; Correlation; Prediction algorithms; High efficiency
   video coding; Feature-based dictionary; Gaussian mixture model; motion
   vector; three-dimensional distribution-rotation based decomposition
ID HEVC; SCHEME
AB We introduce a Gaussian Mixture Model (GMM) framework for 3D holoscopic image compression in this paper. The elemental-images of the 3D holoscopic image are predicted using GMM and the parameters of GMM are estimated using the common Expectation-Maximization (EM) algorithm. GMM Model Optimization (GMO) is used in this framework to select the optimal number of distributions and avoid local optimum of EM at the same time. A three-dimensional distribution-rotation based decomposition is proposed to change covariance parameters to meaningful features and improve the coding efficiency. The features and the remaining parameters of the GMM are encoded using fixed-length bits. A feature-based dictionary is proposed in this framework to match the similar Gaussian distributions utilizing the similar GMM features. And the offsets of the matched distributions are recorded as motion vectors to replace the similar areas in the elemental-images of the 3D holoscopic image. The residual between the original image and the prediction is encoded using Screen Content Coding Extension of High Efficiency Video Coding (HEVC-SCC). Experimental results show that our method performs better than HEVC-SCC, two coding methods based on pseudo-sequences and a state-of-the-art content-based compression method with Gaussian process regression.
C1 [Sun, Jianjun; Zhao, Yan; Wang, Shigang; Wei, Jian] Jilin Univ, Coll Commun Engn, Changchun 130012, Jilin, Peoples R China.
C3 Jilin University
RP Zhao, Y (corresponding author), Jilin Univ, Coll Commun Engn, Changchun 130012, Jilin, Peoples R China.
EM sunjj17@mails.jlu.edu.cn; zhao_y@jlu.edu.cn; wangshigang@vip.sina.com;
   weijian@jlu.edu.cn
RI Zhao, Yan/AAF-8988-2019
OI Sun, Jianjun/0000-0001-5113-8585
FU National Natural Science Foundation of China [61631009, 61771220];
   Fundamental Research Funds for the Central Universities [2017TD-19];
   National Key R&D Program of China [2017YFB1002900, 2017YFB0404800]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61631009 and 61771220,in part by the
   Fundamental Research Funds for the Central Universities under Grant
   2017TD-19, and in part by the National Key R & D Program of China under
   Grants 2017YFB1002900 and 2017YFB0404800.
CR Aggoun A, 2013, IEEE MULTIMEDIA, V20, P28, DOI 10.1109/MMUL.2012.42
   Ahmad W, 2017, IEEE IMAGE PROC, P4557, DOI 10.1109/ICIP.2017.8297145
   Akpinar U., 2017, PROC 3DTV C TRUE VIS, P1
   Ban ZH, 2018, IEEE T IMAGE PROCESS, V27, P4105, DOI 10.1109/TIP.2018.2836306
   Belghith F, 2016, SIGNAL IMAGE VIDEO P, V10, P811, DOI 10.1007/s11760-015-0820-2
   Bjontegaard G., 2001, CALCULATION AVERAGE
   Chandrakala S, 2020, IEEE T MULTIMEDIA, V22, P3, DOI 10.1109/TMM.2019.2925956
   Chen ML, 2018, IEEE T PATTERN ANAL, V40, P1518, DOI 10.1109/TPAMI.2017.2717828
   Cherifa S., 2013, PROC INT C COMPUT AP, P1
   Conti C, 2016, SIGNAL PROCESS-IMAGE, V42, P59, DOI 10.1016/j.image.2016.01.008
   CROSIGNANI B, 1985, OPT ACTA, V32, P1251, DOI 10.1080/713821844
   Georgiev T., 2013, LIGHT FIELD CAPTURE
   Jiang YC, 2018, INT CONF DIGIT SIG
   Jin XL, 2019, IEEE ACCESS, V7, P119018, DOI 10.1109/ACCESS.2019.2937129
   Lee CH, 2013, IEEE T MULTIMEDIA, V15, P454, DOI 10.1109/TMM.2012.2229969
   Liu DY, 2020, IEEE T MULTIMEDIA, V22, P846, DOI 10.1109/TMM.2019.2934426
   Liu DY, 2016, SIGNAL PROCESS-IMAGE, V47, P438, DOI 10.1016/j.image.2016.08.004
   Liu YY, 2018, ELECTRON LETT, V54, P500, DOI 10.1049/el.2017.4560
   Ma KD, 2018, IEEE T COMPUT IMAG, V4, P60, DOI 10.1109/TCI.2017.2786138
   Magnor M, 2000, IEEE T CIRC SYST VID, V10, P338, DOI 10.1109/76.836278
   Mardia K. V., 1979, MULTIVARIATE ANAL, P457
   Monteiro RJS, 2017, IEEE J-STSP, V11, P1120, DOI 10.1109/JSTSP.2017.2721358
   Ohm J., HM 14 0 REXT 7 0 SCM
   Panda DK, 2018, IET IMAGE PROCESS, V12, P1832, DOI 10.1049/iet-ipr.2017.0595
   Shin DH, 2005, ETRI J, V27, P708, DOI 10.4218/etrij.05.0104.0194
   Sun J., 2020, VIRTUAL LIGHT FIELD
   Sun JJ, 2019, INT CONF ACOUST SPEE, P1797, DOI 10.1109/ICASSP.2019.8683784
   Sun JF, 2016, PR IEEE I C PROGR IN, P369, DOI 10.1109/PIC.2016.7949528
   Verhack R, 2020, IEEE T MULTIMEDIA, V22, P579, DOI 10.1109/TMM.2019.2932614
   Verhack R, 2017, INT CONF ACOUST SPEE, P1288, DOI 10.1109/ICASSP.2017.7952364
   Verhack R, 2016, IEEE IMAGE PROC, P2142, DOI 10.1109/ICIP.2016.7532737
   Wu GC, 2017, IEEE J-STSP, V11, P926, DOI 10.1109/JSTSP.2017.2747126
   Xu M, 2018, IEEE T MULTIMEDIA, V20, P1335, DOI 10.1109/TMM.2017.2767784
   Yoon SH, 2003, IEEE DATA COMPR CONF, P457
   Zhao B, 2016, IEEE T MULTIMEDIA, V18, P1111, DOI 10.1109/TMM.2016.2537783
NR 35
TC 1
Z9 1
U1 6
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1374
EP 1389
DI 10.1109/TMM.2022.3141606
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA D7WI3
UT WOS:000970791100027
DA 2024-07-18
ER

PT J
AU Tan, WM
   Ru, GH
   Jiang, YM
   Li, JC
   Yan, B
AF Tan, Weimin
   Ru, Ganghui
   Jiang, Yueming
   Li, Jichun
   Yan, Bo
TI Rethinking and Improving Few-Shot Segmentation From a Contour-Aware
   Perspective
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Contour-aware; few-shot; semantic segmentation
ID NETWORK
AB Existing few-shot segmentation approaches basically adopt the idea of comparing the semantic prototype vector of the query image and support images, and then obtaining the segmentation result. However, recent studies have shown that a single feature vector in feature map cannot accurately represent pixel-level categories, thus leading to poor segmentation of object boundary and semantic ambiguity. To address this common problem, we propose a novel contour-aware network (CTANet) for few-shot segmentation in this paper. Unlike the usual practice of classifying each pixel separately, CTANet regards all pixels within the same contour as a whole, which can take advantage of the internal consistency of objects to obtain a more accurate representation of category information. To obtain the accurate object contour, our network consists of a contour generation module and a contour refinement module, where the former exploits multiple levels of features to generate a primary contour map and the latter learns to refine the primary contour map. Furthermore, a novel contour-aware mixed loss is proposed to fuse the common BCE loss and our contour-aware loss to supervise the training process on two levels, pixel-level and contour-level. Extensive experiments demonstrate that our CTANet achieves a new state-of-the-art performance on PASCAL-5(i) and COCO-20(i) . Hopefully, our new perspective could provide more clues for future research on few-shot segmentation. Our code is freely available at: https://github.com/hardtogetA/CTANet
C1 [Tan, Weimin; Ru, Ganghui; Jiang, Yueming; Li, Jichun; Yan, Bo] Fudan Univ, Sch Comp Sci, Shanghai Key Lab Intelligent Informat Proc, Shanghai 200437, Peoples R China.
C3 Fudan University
RP Yan, B (corresponding author), Fudan Univ, Sch Comp Sci, Shanghai Key Lab Intelligent Informat Proc, Shanghai 200437, Peoples R China.
EM wmtan@fudan.edu.cn; 19210240196@fudan.edu.can; 619371739@qq.com;
   19110240054@fudan.edu.cn; byan@fudan.edu.cn
RI Yan, Bo/AFQ-7025-2022; Jiang, Yueming/AAJ-5538-2020
OI Yan, Bo/0000-0002-7775-1270; Jiang, Yueming/0000-0002-4648-9572; Li,
   Jichun/0000-0003-4906-8244
FU NSFC [U2001209, 61902076]; Natural Science Foundation of Shanghai
   [21ZR1406600]
FX This work was supported in part by NSFC under Grants U2001209 and
   61902076, and in part by the Natural Science Foundation of Shanghai
   under Grant 21ZR1406600.
CR Akbarimoghaddam P, 2022, SIGNAL IMAGE VIDEO P, V16, P1773, DOI 10.1007/s11760-022-02134-1
   Bertinetto L., 2016, Advances in neural information processing systems, P523
   Boudiaf M, 2021, PROC CVPR IEEE, P13974, DOI 10.1109/CVPR46437.2021.01376
   Boyu Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P763, DOI 10.1007/978-3-030-58598-3_45
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   De Boer PT, 2005, ANN OPER RES, V134, P19, DOI 10.1007/s10479-005-5724-z
   Dollár P, 2015, IEEE T PATTERN ANAL, V37, P1558, DOI 10.1109/TPAMI.2014.2377715
   Dong N., 2018, P BRIT MACH VIS C
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Finn C, 2017, PR MACH LEARN RES, V70
   Gur S, 2019, IEEE I CONF COMP VIS, P10721, DOI 10.1109/ICCV.2019.01082
   Haochen Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P730, DOI 10.1007/978-3-030-58601-0_43
   Hariharan B, 2014, LECT NOTES COMPUT SC, V8695, P297, DOI 10.1007/978-3-319-10584-0_20
   Hayder Z, 2017, PROC CVPR IEEE, P587, DOI 10.1109/CVPR.2017.70
   He JZ, 2019, PROC CVPR IEEE, P3823, DOI 10.1109/CVPR.2019.00395
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu T, 2019, AAAI CONF ARTIF INTE, P8441
   Hu Y, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P782
   Huang HX, 2021, IEEE T MULTIMEDIA, V23, P1666, DOI 10.1109/TMM.2020.3001510
   KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570
   Nguyen K, 2019, IEEE I CONF COMP VIS, P622, DOI 10.1109/ICCV.2019.00071
   Kingma D. P., 2014, arXiv
   Koch G., 2015, ICML DEEP LEARNING W, V2
   Li G, 2021, PROC CVPR IEEE, P8330, DOI 10.1109/CVPR46437.2021.00823
   Li X, 2020, PROC CVPR IEEE, P2866, DOI 10.1109/CVPR42600.2020.00294
   Li X, 2018, LECT NOTES COMPUT SC, V11219, P370, DOI 10.1007/978-3-030-01267-0_22
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu BH, 2021, PROC CVPR IEEE, P9742, DOI 10.1109/CVPR46437.2021.00962
   Liu WD, 2020, PROC CVPR IEEE, P4164, DOI 10.1109/CVPR42600.2020.00422
   Liu Y., 2020, P EUR C COMP VIS ECC, P142, DOI DOI 10.1007/978-3-030-58545-79
   Liu Y, 2022, INT J COMPUT VISION, V130, P179, DOI 10.1007/s11263-021-01539-8
   Liu Y, 2017, PROC CVPR IEEE, P5872, DOI 10.1109/CVPR.2017.622
   Marcos D, 2018, PROC CVPR IEEE, P8877, DOI 10.1109/CVPR.2018.00925
   Munkhdalai T, 2017, PR MACH LEARN RES, V70
   Peng C, 2017, PROC CVPR IEEE, P1743, DOI 10.1109/CVPR.2017.189
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Rakelly K., 2018, P ICLR WORKSH
   Rakelly K, 2018, Arxiv, DOI arXiv:1806.07373
   Santoro A, 2016, PR MACH LEARN RES, V48
   Schwartz E, 2018, ADV NEUR IN, V31
   Shaban A, 2017, ARXIV PREPRINT ARXIV, DOI 10.5244/C.31.167
   Siam M, 2019, IEEE I CONF COMP VIS, P5248, DOI 10.1109/ICCV.2019.00535
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Snell J, 2017, ADV NEUR IN, V30
   Srinivas A, 2021, PROC CVPR IEEE, P16514, DOI 10.1109/CVPR46437.2021.01625
   Su Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5097, DOI 10.1109/ICCV48922.2021.00507
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Takikawa T, 2019, IEEE I CONF COMP VIS, P5228, DOI 10.1109/ICCV.2019.00533
   Topal Cihan, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2424, DOI 10.1109/ICPR.2010.593
   Vinyals O, 2016, 30 C NEURAL INFORM P, V29
   Wang KX, 2019, IEEE I CONF COMP VIS, P9196, DOI 10.1109/ICCV.2019.00929
   Wang YX, 2018, PROC CVPR IEEE, P7278, DOI 10.1109/CVPR.2018.00760
   Wang YX, 2016, LECT NOTES COMPUT SC, V9910, P616, DOI 10.1007/978-3-319-46466-4_37
   Xie GS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7273, DOI 10.1109/ICCV48922.2021.00720
   Xie GS, 2021, PROC CVPR IEEE, P5471, DOI 10.1109/CVPR46437.2021.00543
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Yosinski J., 2015, ARXIV150606579, V2015, P12
   Yu F., 2015, ARXIV
   Yu ZD, 2017, PROC CVPR IEEE, P1761, DOI 10.1109/CVPR.2017.191
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang BF, 2021, PROC CVPR IEEE, P8308, DOI 10.1109/CVPR46437.2021.00821
   Zhang C, 2019, IEEE I CONF COMP VIS, P9586, DOI 10.1109/ICCV.2019.00968
   Zhang C, 2019, PROC CVPR IEEE, P5212, DOI 10.1109/CVPR.2019.00536
   Zhang XL, 2020, IEEE T CYBERNETICS, V50, P3855, DOI 10.1109/TCYB.2020.2992433
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhu YH, 2021, IEEE T MULTIMEDIA, V23, P1200, DOI 10.1109/TMM.2020.2993952
NR 68
TC 0
Z9 0
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6917
EP 6929
DI 10.1109/TMM.2022.3215896
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000016
DA 2024-07-18
ER

PT J
AU Wang, H
   Yang, WM
   Liao, QM
   Zhou, J
AF Wang, Hai
   Yang, Wenming
   Liao, Qingmin
   Zhou, Jie
TI Bi-RSTU: Bidirectional Recurrent Upsampling Network for Space-Time Video
   Super-Resolution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Space-time video super-resolution; bidirectional recurrent neural
   network; feature interpolation; feature reconstruction
ID IMAGE SUPERRESOLUTION; QUALITY ASSESSMENT; FUSION NETWORK
AB One-stage space-time video super-resolution (STVSR) aims to directly reconstruct high-resolution (HR) and high frame rate (HFR) video from its low-resolution (LR) and low frame rate (LFR) counterpart. Due to the wide application, one-stage STVSR has drawn much attention recently. However, existing one-stage methods suffer from ineffective exploration of the auxiliary information from adjacent time steps that may be useful to STVSR at the current time step. To address this issue, we propose a novel Bidirectional Recurrent Space-Time Upsampling network called Bi-RSTU for one-stage STVSR to utilize auxiliary information at various time steps. Specifically, an efficient channel attention feature interpolation (ECAFI) module is devised to synthesize the intermediate frame's LR feature by exploiting its two neighboring LR video frame features. Subsequently, we fuse the information from the previous time step into these intermediate and neighboring features. Finally, second-order attention spindle (SOAS) blocks are stacked to form the feature reconstruction module that learns a mapping from LR fused feature space to HR feature space. Experimental results on public datasets demonstrate that our Bi-RSTU shows competitive performance compared with current two-stage and one-stage state-of-the-art STVSR methods.
C1 [Wang, Hai; Yang, Wenming; Liao, Qingmin] Tsinghua Univ, Shenzhen Int Grad Sch, Dept Elect Engn, Shenzhen 518055, Peoples R China.
   [Zhou, Jie] Tsinghua Univ, Dept Automat, Beijing 100084, Peoples R China.
C3 Tsinghua University; Tsinghua University
RP Yang, WM (corresponding author), Tsinghua Univ, Shenzhen Int Grad Sch, Dept Elect Engn, Shenzhen 518055, Peoples R China.
EM wanghai19@mails.tsinghua.edu.cn; yangelwm@163.com;
   liaoqm@sz.tsinghua.edu.cn; jzhou@tsinghua.edu.cn
RI Wang, Hai/GZH-1300-2022
OI Wang, Hai/0000-0002-5202-208X; Yang, Wenming/0000-0002-2506-1286
FU National Natural Science Foundation of China [62171251]; Natural Science
   Foundation of Guangdong Province [2020A1515010711]; Special Foundations
   for the Development of Strategic Emerging Industries of Shenzhen
   [JCYJ20200109143010272, JCYJ20200109143035495, CJGJZD20210408092804011,
   JSGG20211108092812020]; Oversea Cooperation Foundation of Tsinghua
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62171251, in part by the Natural Science
   Foundation of Guangdong Province under Grant 2020A1515010711, in part by
   the Special Foundations for the Development of Strategic Emerging
   Industries of Shenzhen under Grants JCYJ20200109143010272,
   JCYJ20200109143035495, CJGJZD20210408092804011 and
   JSGG20211108092812020, and in part by the Oversea Cooperation Foundation
   of Tsinghua.
CR Bao WB, 2019, PROC CVPR IEEE, P3698, DOI 10.1109/CVPR.2019.00382
   Bao WB, 2021, IEEE T PATTERN ANAL, V43, P933, DOI 10.1109/TPAMI.2019.2941941
   Chen Y., 2020, P IEEE INT C MULT EX, P1
   Choi M, 2020, AAAI CONF ARTIF INTE, V34, P10663
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Ding CX, 2015, IEEE T MULTIMEDIA, V17, P2049, DOI 10.1109/TMM.2015.2477042
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Guo Y, 2020, PROC CVPR IEEE, P5406, DOI 10.1109/CVPR42600.2020.00545
   Haris M, 2020, PROC CVPR IEEE, P2856, DOI 10.1109/CVPR42600.2020.00293
   Haris M, 2019, PROC CVPR IEEE, P3892, DOI 10.1109/CVPR.2019.00402
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang Y, 2018, IEEE T PATTERN ANAL, V40, P1015, DOI 10.1109/TPAMI.2017.2701380
   Isobe T., 2020, P 31 BRIT MACH VIS C
   Jiang HZ, 2018, PROC CVPR IEEE, P9000, DOI 10.1109/CVPR.2018.00938
   Jo Y, 2018, PROC CVPR IEEE, P3224, DOI 10.1109/CVPR.2018.00340
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kingma D. P., 2014, arXiv
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li JN, 2017, IEEE T MULTIMEDIA, V19, P944, DOI 10.1109/TMM.2016.2642789
   Li T, 2015, SIGNAL PROCESS-IMAGE, V30, P147, DOI 10.1016/j.image.2014.10.007
   Li Z, 2016, SCI REP-UK, V6, DOI 10.1038/srep30338
   Liu C, 2014, IEEE T PATTERN ANAL, V36, P346, DOI 10.1109/TPAMI.2013.127
   Niklaus S, 2018, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2018.00183
   Niklaus S, 2017, IEEE I CONF COMP VIS, P261, DOI 10.1109/ICCV.2017.37
   Niklaus S, 2017, PROC CVPR IEEE, P2270, DOI 10.1109/CVPR.2017.244
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P335, DOI 10.1109/TIP.2009.2034992
   Shahar O., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3353, DOI 10.1109/CVPR.2011.5995360
   Shechtman E, 2002, LECT NOTES COMPUT SC, V2350, P753
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Tao X, 2017, IEEE I CONF COMP VIS, P4482, DOI 10.1109/ICCV.2017.479
   Tian YP, 2020, PROC CVPR IEEE, P3357, DOI 10.1109/CVPR42600.2020.00342
   Timofte R, 2016, PROC CVPR IEEE, P1865, DOI 10.1109/CVPR.2016.206
   Tong T, 2017, IEEE I CONF COMP VIS, P4809, DOI 10.1109/ICCV.2017.514
   Wang LG, 2019, LECT NOTES COMPUT SC, V11361, P514, DOI 10.1007/978-3-030-20887-5_32
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/isqed48828.2020.9137057, 10.1109/ISQED48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang XT, 2019, IEEE COMPUT SOC CONF, P1954, DOI 10.1109/CVPRW.2019.00247
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xia Zhuofan, 2022, P IEEECVF C COMPUTER, P4794, DOI DOI 10.48550/ARXIV.2201.00520
   Xiang XY, 2020, PROC CVPR IEEE, P3367, DOI 10.1109/CVPR42600.2020.00343
   Xu G, 2021, PROC CVPR IEEE, P6384, DOI 10.1109/CVPR46437.2021.00632
   Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2
   Yang WM, 2019, IEEE SIGNAL PROC LET, V26, P538, DOI 10.1109/LSP.2018.2890770
   Yang X, 2019, IEEE T MULTIMEDIA, V21, P328, DOI 10.1109/TMM.2018.2863602
   Yi P, 2019, IEEE I CONF COMP VIS, P3106, DOI 10.1109/ICCV.2019.00320
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
NR 50
TC 2
Z9 2
U1 3
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4742
EP 4751
DI 10.1109/TMM.2022.3181458
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300011
DA 2024-07-18
ER

PT J
AU Wang, Y
   Su, TT
   Li, YS
   Cao, JW
   Wang, G
   Liu, XG
AF Wang, Yan
   Su, Tongtong
   Li, Yusen
   Cao, Jiuwen
   Wang, Gang
   Liu, Xiaoguang
TI DDistill-SR: Reparameterized Dynamic Distillation Network for
   Lightweight Image Super-Resolution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image super-resolution; convolutional neural networks; dynamic
   convolution; reparameter; deep learning
ID INTERPOLATION
AB Recent research on deep convolutional neural networks (CNNs) has provided a significant performance boost on efficient super-resolution (SR) tasks by trading off the performance and applicability. However, most existing methods focus on subtracting feature processing consumption to reduce the parameters and calculations without refining the immediate features, which leads to inadequate information in the restoration. In this paper, we propose a lightweight network termed DDistill-SR, which significantly improves the SR quality by capturing and reusing more helpful information in a static-dynamic feature distillation manner. Specifically, we propose a plug-in reparameterized dynamic unit (RDU) to promote the performance and inference cost trade-off. During the training phase, the RDU learns to linearly combine multiple reparameterizable blocks by analyzing varied input statistics to enhance layer-level representation. In the inference phase, the RDU is equally converted to simple dynamic convolutions that explicitly capture robust dynamic and static feature maps. Then, the information distillation block is constructed by several RDUs to enforce hierarchical refinement and selective fusion of spatial context information. Furthermore, we propose a dynamic distillation fusion (DDF) module to enable dynamic signals aggregation and communication between hierarchical modules to further improve performance. Empirical results show that our DDistill-SR outperforms the baselines and achieves state-of-the-art results on most super-resolution domains with much fewer parameters and less computational overhead.
C1 [Wang, Yan; Su, Tongtong; Li, Yusen; Wang, Gang; Liu, Xiaoguang] Nankai Univ, Sch Comp Sci, Nankai Baidu Joint Lab, Tianjin 300350, Peoples R China.
   [Wang, Yan; Su, Tongtong; Li, Yusen; Wang, Gang; Liu, Xiaoguang] Tianjin Key Lab Network & Data Secur Technol, Tianjin 300071, Peoples R China.
   [Cao, Jiuwen] Hangzhou Dianzi Univ, Artificial Intelligence Inst, Machine Learning & I Hlth Int Cooperat Base Zhejia, Hangzhou 310018, Peoples R China.
C3 Nankai University; Hangzhou Dianzi University
RP Li, YS (corresponding author), Nankai Univ, Sch Comp Sci, Nankai Baidu Joint Lab, Tianjin 300350, Peoples R China.
EM wangy@nbjl.nankai.edu.cn; sutt@nbjl.nankai.edu.cn;
   liyusen@nbjl.nankai.edu.cn; jwcao@hdu.edu.cn; wgzwp@nbjl.nankai.edu.cn;
   liuxg@nbjl.nankai.edu.cn
RI cao, jiuwen/C-9547-2009; Su, Tongtong/HLV-8240-2023; wang,
   gang/ITT-0670-2023
OI Li, Yusen/0000-0001-6623-350X; Wang, Gang/0000-0003-0387-2501; Wang,
   Yan/0000-0002-5989-2408; liu, xiaoguang/0000-0002-9010-3278
FU Key-Area Research and Development Program of Guangdong Province
   [2021B0101310002]; National Science Foundation of China [62272252,
   62272253, 62141412]; NSF of Tianjin [17JCYBJC15300]
FX This work was supported in part by Key-Area Research and Development
   Program of Guangdong Province underGrant 2021B0101310002, in part by the
   National Science Foundation of China under Grants 62272252, 62272253,
   and 62141412, and in part by the NSF of Tianjin under Grant
   17JCYBJC15300.
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Ahn N, 2018, LECT NOTES COMPUT SC, V11214, P256, DOI 10.1007/978-3-030-01249-6_16
   Ben Niu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P191, DOI 10.1007/978-3-030-58610-2_12
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Chen H, 2021, IEEE T MULTIMEDIA, V23, P584, DOI 10.1109/TMM.2020.2985538
   Chen J, 2021, PROC CVPR IEEE, P8060, DOI 10.1109/CVPR46437.2021.00797
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Ding XH, 2021, PROC CVPR IEEE, P10881, DOI 10.1109/CVPR46437.2021.01074
   Ding XH, 2021, PROC CVPR IEEE, P13728, DOI 10.1109/CVPR46437.2021.01352
   Ding XH, 2019, IEEE I CONF COMP VIS, P1911, DOI 10.1109/ICCV.2019.00200
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Glasner D, 2009, IEEE I CONF COMP VIS, P349, DOI 10.1109/ICCV.2009.5459271
   He H, 2011, PROC CVPR IEEE, P449, DOI 10.1109/CVPR.2011.5995713
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Hui Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2024, DOI 10.1145/3343031.3351084
   Hui Z, 2018, PROC CVPR IEEE, P723, DOI 10.1109/CVPR.2018.00082
   Kai Zhang, 2020, Proceedings of the 16th European Conference on Computer Vision - ECCV 2020 Workshops. Lecture Notes in Computer Science (LNCS 12537), P5, DOI 10.1007/978-3-030-67070-2_1
   KEYS RG, 1981, IEEE T ACOUST SPEECH, V29, P1153, DOI 10.1109/TASSP.1981.1163711
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kingma D. P., 2014, arXiv
   Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li B, 2020, IEEE T IMAGE PROCESS, V29, P8368, DOI 10.1109/TIP.2020.3014953
   Li Y., 2021, P 9 INT C LEARN REPR
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu J, 2020, PROC CVPR IEEE, P2356, DOI 10.1109/CVPR42600.2020.00243
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Matsui Y, 2017, MULTIMED TOOLS APPL, V76, P21811, DOI 10.1007/s11042-016-4020-z
   Muqeet A, 2020, Arxiv, DOI arXiv:2008.12912
   Romano Y, 2017, IEEE T COMPUT IMAG, V3, P110, DOI 10.1109/TCI.2016.2629284
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Sun J, 2008, PROC CVPR IEEE, P2471, DOI 10.1109/CVPR.2008.4587659
   Sun X, 2022, IEEE T CIRC SYST VID, V32, P2937, DOI 10.1109/TCSVT.2021.3096814
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Tai YW, 2010, PROC CVPR IEEE, P2400, DOI 10.1109/CVPR.2010.5539933
   Tian CW, 2022, IEEE T SYST MAN CY-S, V52, P3718, DOI 10.1109/TSMC.2021.3069265
   Tian CW, 2020, KNOWL-BASED SYST, V205, DOI 10.1016/j.knosys.2020.106235
   Tian CW, 2021, IEEE T MULTIMEDIA, V23, P1489, DOI 10.1109/TMM.2020.2999182
   Tong T, 2017, IEEE I CONF COMP VIS, P4809, DOI 10.1109/ICCV.2017.514
   Wang LG, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4781, DOI 10.1109/ICCV48922.2021.00476
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZH, 2021, IEEE T PATTERN ANAL, V43, P3365, DOI 10.1109/TPAMI.2020.2982166
   Yang B, 2019, ADV NEUR IN, V32
   Yang WM, 2019, IEEE T MULTIMEDIA, V21, P3106, DOI 10.1109/TMM.2019.2919431
   Yinpeng Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11027, DOI 10.1109/CVPR42600.2020.01104
   Yu F., 2015, ARXIV
   Yu-Syuan Xu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12493, DOI 10.1109/CVPR42600.2020.01251
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang K, 2019, IEEE INT CONF COMP V, P3565, DOI 10.1109/ICCVW.2019.00441
   Zhang K, 2017, PROC CVPR IEEE, P2808, DOI 10.1109/CVPR.2017.300
   Zhang L, 2006, IEEE T IMAGE PROCESS, V15, P2226, DOI 10.1109/TIP.2006.877407
   Zhang XD, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4034, DOI 10.1145/3474085.3475291
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhao H, 2020, COMPUTER VISION ECCV, P56, DOI DOI 10.1007/978-3-030-67070-23
   Zhu J, 2019, I S BIOMED IMAGING, P1669, DOI [10.1109/ISBI.2019.8759517, 10.17863/cam.40373]
   Zou WWW, 2012, IEEE T IMAGE PROCESS, V21, P327, DOI 10.1109/TIP.2011.2162423
NR 59
TC 7
Z9 7
U1 5
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7222
EP 7234
DI 10.1109/TMM.2022.3219646
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000037
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wei, D
   Shen, XB
   Sun, QS
   Gao, XZ
   Ren, ZW
AF Wei, Dong
   Shen, Xiaobo
   Sun, Quansen
   Gao, Xizhan
   Ren, Zhenwen
TI Sparse Representation Classifier Guided Grassmann Reconstruction Metric
   Learning With Applications to Image Set Analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Grassmann manifold; image set classification; multiple kernel learning;
   sparse representation-based classifier
ID KERNEL; MANIFOLDS; RECOGNITION; SUBSPACE
AB Modeling a sequence of video frames as a linear subspace on Grassmann manifold has recently become increasingly attractive in multiple computer vision applications. The success of such algorithms largely depends on a good distance measure, and learning an appropriate metric on Grassmann manifold remains a key challenge. Existing works address this by learning a discriminative mapping from the original Grassmann manifold to Hilbert space or a lower-dimensional, more discriminative Grassmann manifold. However, these approaches always highly rely on nearest neighbor matching of samples on the projected space, which is sensitive to noises and errors. Different from them, this paper proposes a Grassmann Reconstruction Metric Learning (GRML) algorithm guided by sparse representation-based classifier (SRC) for image set classification. SRC selects the coefficients associated with each class to reconstruct training samples, and then we employ it as a criterion to direct the design of a discriminant metric on Grassmann manifold. Specifically, GRML attempts to jointly maximize the inter-class reconstruction residual and minimize the intra-class reconstruction residual in the lower but more discriminative Grassmann manifold. To further explore the intrinsic geometry distance, we present a Grassmann Reconstruction Multiple Kernel Metric Learning (GRMKML) algorithm, which aims to jointly learn a metric and the corresponding kernel from a family of kernels for Grassmann manifold. Extensive experiments on eight benchmark datasets demonstrate that the proposed algorithms perform favorably against the state-of-the-art methods.
C1 [Wei, Dong; Shen, Xiaobo; Sun, Quansen] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.
   [Gao, Xizhan] Univ Jinan, Sch Informat Sci & Engn, Jinan 250022, Peoples R China.
   [Ren, Zhenwen] Southwest Univ Sci & Technol, Sch Natl Def Sci & Technol, Mianyang 621010, Peoples R China.
C3 Nanjing University of Science & Technology; University of Jinan;
   Southwest University of Science & Technology - China
RP Shen, XB; Sun, QS (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.
EM dongweinjust@163.com; njust.shenxiaobo@gmail.com;
   sunquansen@njust.edu.cn; gaoxizhan123@126.com; rzw@njust.edu.cn
RI gao, xizhan/U-2435-2018; WEi, Dong/KFR-9151-2024
OI Wei, Dong/0000-0002-7299-7693; Gao, Xizhan/0000-0001-5575-6786; REN,
   ZHEN WEN/0000-0003-3791-9750; Shen, Xiaobo/0000-0001-8494-4532
FU National Natural Science Foundation of China [62176126, 62101213,
   61906091]; Natural Science Foundation of Jiangsu Province, China
   [BK20190440]; Shandong Provincial Natural Science Foundation
   [ZR2020QF107]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62176126, 62101213, and 61906091, in
   part by the Natural Science Foundation of Jiangsu Province, China (Youth
   Fund Project)under Grant BK20190440, and in part by Shandong Provincial
   Natural Science Foundation under Grant ZR2020QF107.
CR Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1
   [Anonymous], 1999, Introduction to pattern recognition: Statistical, structural, neural, and fuzzy logic approaches
   Cevikalp H, 2010, PROC CVPR IEEE, P2567, DOI 10.1109/CVPR.2010.5539965
   Chan AB, 2005, PROC CVPR IEEE, P846
   Dhall A., 2014, PROC ICMI, P461, DOI DOI 10.1145/2663204.2666275
   Fathy MohammedE., 2016, P INT JOINT C ART IN, P3359
   Fukui K, 2015, IEEE T PATTERN ANAL, V37, P2164, DOI 10.1109/TPAMI.2015.2408358
   Gönen M, 2011, J MACH LEARN RES, V12, P2211
   Hamm Jihun, 2008, P 25 INT C MACH LEAR, P376, DOI DOI 10.1145/1390156.1390204
   Harandi M. T., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2705, DOI 10.1109/CVPR.2011.5995564
   Harandi M, 2018, IEEE T PATTERN ANAL, V40, P48, DOI 10.1109/TPAMI.2017.2655048
   Harandi M, 2015, INT J COMPUT VISION, V114, P113, DOI 10.1007/s11263-015-0833-x
   Harandi MT, 2014, LECT NOTES COMPUT SC, V8695, P408, DOI 10.1007/978-3-319-10584-0_27
   Huang B, 2020, IEEE T MULTIMEDIA, V22, P2820, DOI 10.1109/TMM.2020.2965482
   Huang J, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3569
   Huang ZW, 2018, AAAI CONF ARTIF INTE, P3279
   Huang ZW, 2018, IEEE T PATTERN ANAL, V40, P2827, DOI 10.1109/TPAMI.2017.2776154
   Huang ZW, 2015, PR MACH LEARN RES, V37, P720
   Huang ZW, 2015, PROC CVPR IEEE, P140, DOI 10.1109/CVPR.2015.7298609
   Jayasumana S, 2015, IEEE T PATTERN ANAL, V37, P2464, DOI 10.1109/TPAMI.2015.2414422
   Kim M, 2008, GLOB TELECOMM CONF, DOI 10.1109/GLOCOM.2008.ECP.1093
   Kim TK, 2007, IEEE T PATTERN ANAL, V29, P1005, DOI 10.1109/TPAMI.2007.1037
   Kim TK, 2009, IEEE T PATTERN ANAL, V31, P1415, DOI 10.1109/TPAMI.2008.167
   Lai K, 2011, IEEE INT CONF ROBOT, P1817
   Leibe B, 2003, PROC CVPR IEEE, P409
   Lu JW, 2016, IEEE T CIRC SYST VID, V26, P529, DOI 10.1109/TCSVT.2015.2412831
   Lu SY, 2014, IEEE T MULTIMEDIA, V16, P1497, DOI 10.1109/TMM.2014.2319778
   Luo L, 2019, AAAI CONF ARTIF INTE, P4480
   Piao XL, 2019, PROC CVPR IEEE, P12067, DOI 10.1109/CVPR.2019.01235
   Qiao LS, 2010, PATTERN RECOGN, V43, P331, DOI 10.1016/j.patcog.2009.05.005
   Sharma K, 2021, IEEE T NEUR NET LEAR, V32, P1082, DOI 10.1109/TNNLS.2020.2980059
   Shrivastava A, 2014, IEEE T IMAGE PROCESS, V23, P3013, DOI 10.1109/TIP.2014.2324290
   Shroff N, 2010, PROC CVPR IEEE, P1911, DOI 10.1109/CVPR.2010.5539864
   Sun MJ, 2022, IEEE T MULTIMEDIA, V24, P2567, DOI 10.1109/TMM.2021.3086727
   Tang PP, 2020, IEEE T MULTIMEDIA, V22, P2579, DOI 10.1109/TMM.2019.2958764
   Wang BY, 2021, IEEE T NEUR NET LEAR, V32, P3484, DOI 10.1109/TNNLS.2020.3011717
   Wang BY, 2021, IEEE T MULTIMEDIA, V23, P216, DOI 10.1109/TMM.2020.2975394
   Wang BY, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2893
   Wang R, 2022, IEEE T COGN DEV SYST, V14, P957, DOI 10.1109/TCDS.2021.3086814
   Wang R, 2021, IEEE T MULTIMEDIA, V23, P228, DOI 10.1109/TMM.2020.2981189
   Wang R, 2022, IEEE T BIG DATA, V8, P753, DOI 10.1109/TBDATA.2020.2982146
   Wei D., 2021, Pattern Recognit., V122
   Wolf L, 2011, PROC CVPR IEEE, P529, DOI 10.1109/CVPR.2011.5995566
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Yan H, 2015, PATTERN RECOGN, V48, P1827, DOI 10.1016/j.patcog.2014.10.021
   Yang CL, 2013, PATTERN RECOGN, V46, P948, DOI 10.1016/j.patcog.2012.07.011
   Yang J, 2013, IEEE T NEUR NET LEAR, V24, P1023, DOI 10.1109/TNNLS.2013.2249088
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Yin M, 2016, PROC CVPR IEEE, P5157, DOI 10.1109/CVPR.2016.557
   Zadeh PH, 2016, PR MACH LEARN RES, V48
   Zhang J, 2019, IEEE T MULTIMEDIA, V21, P2518, DOI 10.1109/TMM.2019.2907453
   Zhou SP, 2018, IEEE T MULTIMEDIA, V20, P593, DOI 10.1109/TMM.2017.2755983
   Zhu PF, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3235
   Zhu XJ, 2017, COMPUT OPTIM APPL, V67, P73, DOI 10.1007/s10589-016-9883-4
NR 54
TC 4
Z9 4
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4307
EP 4322
DI 10.1109/TMM.2022.3173535
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA W1OJ1
UT WOS:001089390200017
DA 2024-07-18
ER

PT J
AU Xu, WJ
   Xu, YF
   Sang, GN
   Li, L
   Wang, AC
   Wei, PP
   Zhu, L
AF Xu, Wujiang
   Xu, Yifei
   Sang, Genan
   Li, Li
   Wang, Aichen
   Wei, Pingping
   Zhu, Li
TI Recursive Multi-Relational Graph Convolutional Network for Automatic
   Photo Selection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Visualization; Logic gates; Feature extraction;
   Aggregates; Training; Topology; Automatic photo selection; graph
   convolutional network; recursive learning
AB Automatic Photo Selection (APS) is a fundamental and important task for further photo cropping and photo enhancement. As the images in a photo series normally have subtle differences, it remains challenging to surface the best photos among highly similar photos. In this work, we propose a Recursive Multi-Relational Graph Convolutional Network (RMGCN) for APS. Specifically, we explore and devise inner-relation and inter-relation graphs to learn informative representations in hierarchical manner. 1) Patch-aware Intra Graph Module (PIGM) captures visual and spatial relations between different patches to characterize the representations in an image. 2) Context-aware Inter Graph Module (CIGM) explicitly exploits mutual comparative relation between different images in a photo series. These two graphs are recursively refined each other by reasoning the graph representations. Then, our model aggregates the output of CIGM with multi-scale local features via the proposed Cross-domain Fusing Gate (CFG) to boost the discriminative ability. Besides, we formulate four companion objectives as soft constraints to improve convergence rate during training. Extensive experiments are conducted on photo-triage dataset, and superior results are reported on different metrics when comparing to the state-of-the-art methods. We also perform rigorous ablations and analysis to validate our approach.
C1 [Xu, Wujiang; Xu, Yifei; Zhu, Li] Xi An Jiao Tong Univ, Sch Software, Xian 710054, Shaanxi, Peoples R China.
   [Wei, Pingping] Xi An Jiao Tong Univ, State Key Lab Mfg Syst Engn, Xian 710054, Peoples R China.
   [Sang, Genan; Li, Li] Alltuu Inc, Hangzhou 310000, Peoples R China.
   [Wang, Aichen] Jiangsu Univ, Key Lab Modern Agr Equipment & Technol, Zhenjiang 212013, Jiangsu, Peoples R China.
C3 Xi'an Jiaotong University; Xi'an Jiaotong University; Jiangsu University
RP Xu, YF (corresponding author), Xi An Jiao Tong Univ, Sch Software, Xian 710054, Shaanxi, Peoples R China.
EM xjtuwujiangxu@stu.xjtu.edu.cn; belonxu_1@xjtu.edu.cn;
   sanggenan@alltuu.com; lili@alltuu.com; acwang@ujs.edu.cn;
   erin1989@xjtu.edu.cn; zhuli@xjtu.edu.cn
RI Xu, Wujiang/KTI-1908-2024
OI Xu, Wujiang/0000-0002-3500-1068
FU National Key Research and Development Program of China Project
   [2019YFB2102500, 2019YFB2103005]; Young Scientists Fund of the National
   Natural Science Foundation of China [61802300]; China Postdoctoral
   Science Foundation Funded Project [2018m643666]; Xi'an Jiaotong
   University through the basic research foundation for Young Teachers
   [xjh012019043]
FX This work was supported in part by the National Key Research and
   Development Program of China Project under Grants 2019YFB2102500 and
   2019YFB2103005, in part by the Young Scientists Fund of the National
   Natural Science Foundation of China under Grant 61802300, in part by
   China Postdoctoral Science Foundation Funded Project under Grant
   2018m643666, and in part by Xi'an Jiaotong University through the basic
   research foundation for Young Teachers under Grant xjh012019043.
CR Apostolidis E, 2020, LECT NOTES COMPUT SC, V11961, P492, DOI 10.1007/978-3-030-37731-1_40
   Bai ZC, 2021, PROC CVPR IEEE, P12909, DOI 10.1109/CVPR46437.2021.01272
   BRADLEY RA, 1952, BIOMETRIKA, V39, P324, DOI 10.1093/biomet/39.3-4.324
   Cai JM, 2021, IEEE WINT CONF APPL, P2734, DOI 10.1109/WACV48630.2021.00278
   CERONI A, 2017, PROCEEDINGS OF ICMR, P452
   CERONI A, 2015, P 5TH ACM INT C MULT, P187
   Chang HW, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925908
   Chen HX, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1503, DOI 10.1145/3394486.3403201
   Chen Y, 2021, EXPERT SYST APPL, V170, DOI 10.1016/j.eswa.2020.114505
   Chen YL, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P37, DOI 10.1145/3123266.3123274
   Chen YX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13339, DOI 10.1109/ICCV48922.2021.01311
   Chen ZM, 2019, PROC CVPR IEEE, P5172, DOI 10.1109/CVPR.2019.00532
   CHENG K, IEEE T NEURAL NETW L
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fajtl J, 2019, LECT NOTES COMPUT SC, V11367, P39, DOI 10.1007/978-3-030-21074-8_4
   Fan HN, 2022, IEEE T MULTIMEDIA, V24, P49, DOI 10.1109/TMM.2020.3045286
   Gao JY, 2021, IEEE T MULTIMEDIA, V23, P3203, DOI 10.1109/TMM.2020.3021980
   GATYS LA, 2015, A NEURAL ALGORITHM O
   Gori M, 2005, IEEE IJCNN, P729
   Guo ZJ, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P241
   Hamilton W., 2017, PROC NIPS
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He XN, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P639, DOI 10.1145/3397271.3401063
   Hosu V, 2019, PROC CVPR IEEE, P9367, DOI 10.1109/CVPR.2019.00960
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Ji Z, 2021, IEEE T NEUR NET LEAR, V32, P1765, DOI 10.1109/TNNLS.2020.2991083
   Ji Z, 2020, IEEE T CIRC SYST VID, V30, P1709, DOI 10.1109/TCSVT.2019.2904996
   Jia XW, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2735, DOI 10.1145/3394486.3403324
   JIANG B, IEEE T MULTIMEDIA
   Jungin Park, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P647, DOI 10.1007/978-3-030-58595-2_39
   KIM Y, 2017, P INT C LEARN REPRES
   Kipf TN, 2017, INT C LEARN REPR
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li LD, 2020, IEEE T IMAGE PROCESS, V29, P3898, DOI 10.1109/TIP.2020.2968285
   Li QZ, 2019, AAAI CONF ARTIF INTE, P8634
   Li QM, 2018, AAAI CONF ARTIF INTE, P3538
   Liu D, 2020, IEEE WINT CONF APPL, P3558, DOI [10.1109/WACV45572.2020.9093412, 10.1109/wacv45572.2020.9093412]
   Ma Ningning, 2018, P EUR C COMP VIS ECC, DOI [10.1007/978-3-030-01264-9_8, DOI 10.1007/978-3-030-01264-9_8]
   Ma Y., 2019, P 2019 SIAM INT C DA, P657, DOI DOI 10.1137/1.9781611975673.74
   Maas AL., 2013, P ICML WORKSHOP DEEP, V28, P1
   Nie WZ, 2021, IEEE T MULTIMEDIA, V23, P1962, DOI 10.1109/TMM.2020.3006371
   QIAN S, IEEE T MULTIMEDIA
   RONG Y, 2020, P INT C LEARN REPRES
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Schwarz K, 2018, IEEE WINT CONF APPL, P2048, DOI 10.1109/WACV.2018.00226
   SIMONYAN K, 2015, P 3RD INT C LEARN RE
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Talebi H, 2018, IEEE T IMAGE PROCESS, V27, P3998, DOI 10.1109/TIP.2018.2831899
   Tan ZC, 2020, AAAI CONF ARTIF INTE, V34, P12055
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Vaswani Ashish, 2017, Advances in Neural Information Processing Systems (NeurIPS), V17, P6000, DOI DOI 10.48550/ARXIV.1706.03762
   Velickovic Petar, 2018, INT C LEARN REPR
   Walber T, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P2065, DOI 10.1145/2556288.2557025
   Wan S, 2020, IEEE T GEOSCI REMOTE, V58, P3162, DOI 10.1109/TGRS.2019.2949180
   Wang BY, 2020, IEEE T IMAGE PROCESS, V29, P3065, DOI 10.1109/TIP.2019.2955563
   WANG M, IEEE T MULTIMEDIA
   Wang Y, 2020, AAAI CONF ARTIF INTE, V34, P12265
   Wei ZJ, 2018, PROC CVPR IEEE, P5437, DOI 10.1109/CVPR.2018.00570
   XU D, 2017, P ADV NEURAL INF PRO, V30, P3961
   Xu D, 2017, PROC CVPR IEEE, P161, DOI 10.1109/CVPR.2017.25
   XU Y, IEEE ACCESS
   Yang Chaoqi, 2020, Revisiting over-smoothing in deep gcns
   You RC, 2020, AAAI CONF ARTIF INTE, V34, P12709
   Zeng H, 2022, IEEE T PATTERN ANAL, V44, P1304, DOI 10.1109/TPAMI.2020.3024207
   Zhang XD, 2021, IEEE T MULTIMEDIA, V23, P611, DOI 10.1109/TMM.2020.2985526
   Zhang ZL, 2020, AAAI CONF ARTIF INTE, V34, P12943
   Zhang Z, 2021, PROC CVPR IEEE, P12131, DOI 10.1109/CVPR46437.2021.01196
   Zhao B, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P863, DOI 10.1145/3123266.3123328
   Zhao XM, 2020, PATTERN RECOGN, V104, DOI 10.1016/j.patcog.2020.107333
   Zhong Z, 2021, IEEE T PATTERN ANAL, V43, P2723, DOI 10.1109/TPAMI.2020.2976933
   Zhu WC, 2021, IEEE T IMAGE PROCESS, V30, P948, DOI 10.1109/TIP.2020.3039886
NR 73
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3825
EP 3840
DI 10.1109/TMM.2022.3167309
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FE2H4
UT WOS:001144015500022
DA 2024-07-18
ER

PT J
AU Yang, XF
   Liu, FY
   Lin, GS
AF Yang, Xiaofeng
   Liu, Fayao
   Lin, Guosheng
TI Effective End-to-End Vision Language Pretraining With Semantic Visual
   Loss
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Feature extraction; Training; Transformers; Predictive
   models; Convergence; Computational modeling; Computer vision; natural
   language processing
AB Current vision language pretraining models are dominated by methods using region visual features extracted from object detectors. Given their good performance, the extract-then-process pipeline significantly restricts the inference speed and therefore limits their real-world use cases. However, training vision language models from raw image pixels is difficult, as the raw image pixels give much less prior knowledge than region features. In this paper, we systematically study how to leverage auxiliary visual pretraining tasks to help training end-to-end vision language models. We introduce three types of visual losses that enable much faster convergence and better finetuning accuracy. Compared with region feature models, our end-to-end models could achieve similar or better performance on down-stream tasks and run more than 10 times faster during inference. Compared with other end-to-end models, our proposed method could achieve similar or better performance when pretrained for only 10% of the pretraining GPU hours.
C1 [Yang, Xiaofeng; Lin, Guosheng] Nanyang Technol Univ NTU, Sch Comp Sci & Engn, Singapore 639798, Singapore.
   [Liu, Fayao] ASTAR, Singapore 138632, Singapore.
C3 Nanyang Technological University; Agency for Science Technology &
   Research (A*STAR)
RP Lin, GS (corresponding author), Nanyang Technol Univ NTU, Sch Comp Sci & Engn, Singapore 639798, Singapore.
EM XIAOFENG001@e.ntu.edu.sg; fayaoliu@gmail.com; gslin@ntu.edu.sg
OI Yang, Xiaofeng/0000-0003-1882-9733; LIU, Fayao/0000-0001-6649-7660
FU National Research Foundation Singapore
FX No Statement Available
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Caesar H, 2018, PROC CVPR IEEE, P1209, DOI 10.1109/CVPR.2018.00132
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen X, 2015, Microsoft coco captions: Data collection and evaluation server
   Cubuk ED, 2020, IEEE COMPUT SOC CONF, P3008, DOI 10.1109/CVPRW50498.2020.00359
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670
   Gunel B., 2020, INT C LEARN REPR
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton G., 2015, COMPUT SCI, V2
   Hu J, 2021, IEEE T MULTIMEDIA, V23, P2321, DOI 10.1109/TMM.2020.3009491
   Huang C, 2008, LECT NOTES COMPUT SC, V5303, P788, DOI 10.1007/978-3-540-88688-4_58
   Huang Zhicheng, 2020, Pixel-bert: Aligning image pixels with text by deep multi-modal transformers
   Hudson Drew A, 2019, P IEEE CVF C COMP VI, P6700, DOI DOI 10.1109/CVPR.2019.00686
   Jiao XQ, 2020, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, EMNLP 2020, P4163
   Jiasen Lu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10434, DOI 10.1109/CVPR42600.2020.01045
   Kim W, 2021, PR MACH LEARN RES, V139
   Kitaev Nikita, 2020, INT C LEARN REPR
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Li G, 2020, AAAI CONF ARTIF INTE, V34, P11336
   Liu YH, 2019, INFORM SYST RES, DOI 10.48550/arXiv.1907.11692
   Lu JS, 2019, ADV NEUR IN, V32
   Luo RT, 2018, PROC CVPR IEEE, P6964, DOI 10.1109/CVPR.2018.00728
   Plummer BA, 2017, INT J COMPUT VISION, V123, P74, DOI 10.1007/s11263-016-0965-7
   Qi D, 2020, CoRR
   Sahbani B, 2016, INT CONF SYST ENG, P109, DOI 10.1109/ICSEngT.2016.7849633
   Sanh, 2019, P 5 WORKSH EN EFF MA
   Sharma P, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2556
   Shiwei Wang, 2020, 2020 IEEE 5th International Conference on Image, Vision and Computing (ICIVC), P29, DOI 10.1109/ICIVC50857.2020.9177456
   Suhr A, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 2, P217, DOI 10.18653/v1/P17-2034
   Sun SQ, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P4323
   Tan H, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5100
   Vaswani A, 2017, ADV NEUR IN, V30
   Yan CG, 2020, IEEE T MULTIMEDIA, V22, P229, DOI 10.1109/TMM.2019.2924576
   Yen-Chun Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P104, DOI 10.1007/978-3-030-58577-8_7
   Yu J, 2020, IEEE T MULTIMEDIA, V22, P3196, DOI 10.1109/TMM.2020.2972830
   Yuan ZQ, 2021, IEEE T MULTIMEDIA, V23, P1744, DOI 10.1109/TMM.2020.3002667
   Zellers R, 2019, PROC CVPR IEEE, P6713, DOI 10.1109/CVPR.2019.00688
   Zhang ZJ, 2019, IEEE T MULTIMEDIA, V21, P1681, DOI 10.1109/TMM.2018.2888822
   Zoph B., 2020, ARXIV200606882, V33, P3833
NR 41
TC 1
Z9 1
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8408
EP 8417
DI 10.1109/TMM.2023.3237166
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000065
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yu, QQ
   Fan, KQ
   Zheng, YH
AF Yu, Qianqian
   Fan, Keqi
   Zheng, Yuhui
TI Domain Adaptive Transformer Tracking Under Occlusions
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Object tracking; Training; Task analysis; Current
   transformers; Target tracking; Object detection; Domain adaptation;
   occlusion; transformer; visual object tracking
AB Due to their excellent performance on aggregating global features, Transformer structures are being widely employed in deep learning-based visual object tracking algorithms, recently. Nevertheless, existing Transformer-based trackers still fail to handle occlusion problems due to drift in feature distributions. To address this issue, we introduce domain adaptation techniques into a novel object tracking framework, DATransT, including feature extraction, domain adaptive Transformer module and prediction head. The domain adaptive Transformer module consists of three weight-sharing branches with self and cross attention mechanisms: the source, the target and the source-target branches. Specifically, the source-target branch employs cross-attention to effectively align the feature distributions of the source and target branches. Meanwhile, we present a pseudo-labeling strategy to generate high-quality training samples. Extensive experiments show that DATransT obtains promising results on several popular datasets, containing LaSOT, TrackingNet, GOT-10k, NfS, OTB2015 and UAV123. Moreover, our method outperforms existing state-of-the-art trackers under full occlusions and partial occlusions.
C1 [Yu, Qianqian; Fan, Keqi; Zheng, Yuhui] Nanjing Univ Informat Sci & Technol, Engn Res Ctr Digital Forens, Minist Educ, Nanjing 210044, Peoples R China.
C3 Nanjing University of Information Science & Technology
RP Zheng, YH (corresponding author), Nanjing Univ Informat Sci & Technol, Engn Res Ctr Digital Forens, Minist Educ, Nanjing 210044, Peoples R China.
EM 20201220051@nuist.edu.cn; 20201220013@nuist.edu.cn; zhengyh@vip.126.com
RI Fan, Keqi/GSM-9918-2022
OI Fan, Keqi/0000-0001-9881-2629
FU National Natural Science Foundation of China [U20B2065, 61972206,
   62011540407]; Natural Science Foundation of Jiangsu Province
   [BK20211539]; 15th Six Talent Peaks Project in Jiangsu Province
   [RJFW-015]; Qing Lan Project; PAPD Fund
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants U20B2065, 61972206, and 62011540407, in
   part by the Natural Science Foundation of Jiangsu Province under Grant
   BK20211539, in part by the 15th Six Talent Peaks Project in Jiangsu
   Province under Grant RJFW-015, in part by Qing Lan Project, and in part
   by PAPD Fund. The guest editor coordinating the review of this
   manuscript and approving it for publication was Dr. Shanshan Zhang.
CR Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bhat G, 2019, IEEE I CONF COMP VIS, P6181, DOI 10.1109/ICCV.2019.00628
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Carion N., 2020, EUR C COMP VIS, P213, DOI DOI 10.1007/978-3-030-58452-8_13
   Chen BY, 2019, PATTERN RECOGN, V87, P80, DOI 10.1016/j.patcog.2018.10.005
   Chen C, 2019, AAAI CONF ARTIF INTE, P3296
   Chen X, 2021, PROC CVPR IEEE, P8122, DOI 10.1109/CVPR46437.2021.00803
   Chen Y, 2018, PROC CVPR IEEE, P3339, DOI 10.1109/CVPR.2018.00352
   Comaniciu D, 2000, PROC CVPR IEEE, P142, DOI 10.1109/CVPR.2000.854761
   Danelljan M, 2020, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR42600.2020.00721
   Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Danelljan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P621, DOI 10.1109/ICCVW.2015.84
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1134, DOI 10.1109/ICCV.2017.128
   Guo J, 2018, IEEE IMAGE PROC, P226, DOI 10.1109/ICIP.2018.8451440
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Huang LH, 2021, IEEE T PATTERN ANAL, V43, P1562, DOI 10.1109/TPAMI.2019.2957464
   Inoue N, 2018, PROC CVPR IEEE, P5001, DOI 10.1109/CVPR.2018.00525
   Jung I, 2018, LECT NOTES COMPUT SC, V11208, P89, DOI 10.1007/978-3-030-01225-0_6
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li YS, 2019, PROC CVPR IEEE, P6929, DOI 10.1109/CVPR.2019.00710
   Lin L., 2022, PROC 36 INT C NEURAL
   Loshchilov I., 2019, arXiv
   Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352
   Ma D., 2018, 29 BRIT MACH VIS C
   Marvasti-Zadeh SM, 2022, IEEE T INTELL TRANSP, V23, P3943, DOI 10.1109/TITS.2020.3046478
   Müller M, 2018, LECT NOTES COMPUT SC, V11205, P310, DOI 10.1007/978-3-030-01246-5_19
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Nam H, 2016, Arxiv, DOI arXiv:1608.07242
   Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465
   Song WF, 2020, IEEE WINT CONF APPL, P1696, DOI 10.1109/WACV45572.2020.9093382
   Song YB, 2018, PROC CVPR IEEE, P8990, DOI 10.1109/CVPR.2018.00937
   Sun ZQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3591, DOI 10.1109/ICCV48922.2021.00359
   Truong TD, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8528, DOI 10.1109/ICCV48922.2021.00843
   Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531
   Vaswani A, 2017, ADV NEUR IN, V30
   Voigtlaender P, 2020, PROC CVPR IEEE, P6577, DOI 10.1109/CVPR42600.2020.00661
   Wang HY, 2021, PROC CVPR IEEE, P5459, DOI 10.1109/CVPR46437.2021.00542
   Wang N, 2021, PROC CVPR IEEE, P1571, DOI 10.1109/CVPR46437.2021.00162
   Wenzel P., 2018, PROC C ROBOT LEARN, P253
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Xu T., 2022, 10 INT C LEARN REPRE
   Xu YD, 2020, AAAI CONF ARTIF INTE, V34, P12549
   Yan B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10428, DOI 10.1109/ICCV48922.2021.01028
   Yang TY, 2017, IEEE INT CONF COMP V, P2010, DOI 10.1109/ICCVW.2017.235
   Yun S., 2019, PATTERN RECOGN, V87, P80
   Zhang HL, 2019, Arxiv, DOI arXiv:1907.01144
   Zhang YP, 2019, PROC CVPR IEEE, P2735, DOI 10.1109/CVPR.2019.00285
   Zhao F, 2019, IEEE T CIRC SYST VID, V29, P1998, DOI 10.1109/TCSVT.2018.2856540
   Zhipeng Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P771, DOI 10.1007/978-3-030-58589-1_46
   Zhu X., 2021, 9 INT C LEARN REPRES
   Zhu Z., 2018, 2018 IEEE International Magnetics Conference (INTERMAG), DOI 10.1109/INTMAG.2018.8508710
NR 58
TC 6
Z9 6
U1 8
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1452
EP 1461
DI 10.1109/TMM.2023.3234372
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA G2GW2
UT WOS:000987415000005
DA 2024-07-18
ER

PT J
AU Yu, Y
   Zhao, XH
   Ni, RR
   Yang, SY
   Zhao, Y
   Kot, AC
AF Yu, Yang
   Zhao, Xiaohui
   Ni, Rongrong
   Yang, Siyuan
   Zhao, Yao
   Kot, Alex C.
TI Augmented Multi-Scale Spatiotemporal Inconsistency Magnifier for
   Generalized DeepFake Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deepfakes; Spatiotemporal phenomena; Faces; Forgery; Heating systems;
   Detectors; Convolution; Adversarial data augmentation; generalized
   DeepFake detection; global guidance; multi-scale spatiotemporal
   inconsistency
ID FORGERY DETECTION; VIDEO
AB Recently, realistic DeepFake videos have raised severe security concerns in society. Existing video-based detection methods observe local spatial regions with the coarse temporal view, thus it is difficult to obtain subtle spatiotemporal information, resulting in limited generalization ability. In this paper, we propose a novel Augmented Multi-scale Spatiotemporal Inconsistency Magnifier (AMSIM) with a Global Inconsistency View (GIV) and a more meticulous Multi-timescale Local Inconsistency View (MLIV), focusing on mining comprehensive and more subtle spatiotemporal cues. Firstly, the GIV that includs the global spatial and long-term temporal views is established to ensure comprehensive spatiotemporal clues are captured. Then, the MLIV with the critical local spatial and multi-timescale local temporal views is designed for magnifying the indetectable spatiotemporal abnormality. Subsequently, GIV is utilized to guide MLIV to dynamically find local spatiotemporal anomalies that are highly relevant to the overall video. Finally, to further obtain a generalized framework, the adversarial data augmentation is specially designed to expand source domains and simulate unseen forgery domains. Extensive experiments on six large-scale datasets show that our AMSIM outperforms state-of-the-art detection methods and remains effective when applied to unseen forgery techniques and datasets.
C1 [Yu, Yang; Zhao, Xiaohui; Ni, Rongrong; Zhao, Yao] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
   [Yu, Yang; Zhao, Xiaohui; Ni, Rongrong] Beijing Jiaotong Univ, Beijing Key Lab Adv Informat Sci & Network Technol, Beijing 100044, Peoples R China.
   [Yang, Siyuan] Nanyang Technol Univ, Interdisciplinary Grad Programme, Rapid Rich Object Search Lab, Singapore 639798, Singapore.
   [Kot, Alex C.] Nanyang Technol Univ, Sch Elect & Elect Engn, Nanyang 639798, Singapore.
C3 Beijing Jiaotong University; Beijing Jiaotong University; Nanyang
   Technological University; Nanyang Technological University
RP Ni, RR (corresponding author), Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
EM 18112012@bjtu.edu.cn; 20120328@bjtu.edu.cn; rrni@bjtu.edu.cn;
   siyuan005@e.ntu.edu.sg; yzhao@bjtu.edu.cn; eackot@ntu.edu.sg
RI 于, 洋/IUN-7956-2023; yang, sy/AAO-8145-2020
OI yang, sy/0000-0003-1084-6598; Kot, Alex/0000-0001-6262-8125; Zhao,
   Yao/0000-0002-8581-9554; Yu, Yang/0000-0002-5204-2929
FU National Key Ramp;D Program of China
FX No Statement Available
CR Afchar D, 2018, IEEE INT WORKS INFOR
   Amerini I, 2019, IEEE INT CONF COMP V, P1205, DOI 10.1109/ICCVW.2019.00152
   Chai Lucy, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P103, DOI 10.1007/978-3-030-58574-7_7
   Chen BJ, 2021, IEEE T MULTIMEDIA, V23, P3506, DOI 10.1109/TMM.2020.3026868
   Chen CLZ, 2021, IEEE T IMAGE PROCESS, V30, P3995, DOI 10.1109/TIP.2021.3068644
   Chen CLZ, 2020, IEEE T IMAGE PROCESS, V29, P1090, DOI 10.1109/TIP.2019.2934350
   Chen S, 2021, AAAI CONF ARTIF INTE, V35, P1081
   Chen ZH, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P1985, DOI 10.1109/ICASSP39728.2021.9414225
   Chintha A, 2020, IEEE J-STSP, V14, P1024, DOI 10.1109/JSTSP.2020.2999185
   Choi DH, 2020, IEEE IMAGE PROC, P823, DOI [10.1109/icip40778.2020.9190655, 10.1109/ICIP40778.2020.9190655]
   Chugh K, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P439, DOI 10.1145/3394171.3413700
   Ciftci U. A., 2020, IEEE Trans. Pattern Anal. Mach. Intell., DOI [10.1109/TPAMI2020.3009287, DOI 10.1109/TPAMI2020.3009287]
   Coccomini DA, 2022, LECT NOTES COMPUT SC, V13233, P219, DOI 10.1007/978-3-031-06433-3_19
   Dolhansky B, 2020, Arxiv, DOI arXiv:2006.07397
   Dufour N., 2019, CONTRIBUTING DATA DE
   Fernandes S, 2019, IEEE INT CONF COMP V, P1721, DOI 10.1109/ICCVW.2019.00213
   Frank Joel, 2020, INT C MACH LEARN, P3247
   Ganiyusufoglu Ipek, 2020, arXiv
   Ghimire S, 2020, IEEE T MULTIMEDIA, V22, P108, DOI 10.1109/TMM.2019.2925961
   github, FACESWAP
   github, DEEPFAKES
   Gu QQ, 2022, AAAI CONF ARTIF INTE, P735
   Gu ZH, 2022, AAAI CONF ARTIF INTE, P744
   Gu ZH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3473, DOI 10.1145/3474085.3475508
   Güera D, 2018, 2018 15TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), P127
   Haliassos A, 2021, PROC CVPR IEEE, P5037, DOI 10.1109/CVPR46437.2021.00500
   He PS, 2019, IEEE IMAGE PROC, P2299, DOI [10.1109/icip.2019.8803740, 10.1109/ICIP.2019.8803740]
   Hu Z., 2021, IJCAI, P736
   Jiang LM, 2020, PROC CVPR IEEE, P2886, DOI 10.1109/CVPR42600.2020.00296
   Kakar P, 2011, IEEE T MULTIMEDIA, V13, P443, DOI 10.1109/TMM.2011.2121056
   Li J., 2021, IEEE Trans. Knowl. Data Eng.,, DOI [10.1109/TKDE2021.3117003, DOI 10.1109/TKDE2021.3117003]
   Li LZ, 2020, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR42600.2020.00505
   Li XD, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1864, DOI 10.1145/3394171.3414034
   [李艳歌 Li Yange], 2018, [高分子通报, Polymer Bulletin], P46
   Li YZ, 2018, IEEE INT WORKS INFOR
   Li YZ, 2020, PROC CVPR IEEE, P3204, DOI 10.1109/CVPR42600.2020.00327
   Liu HG, 2021, PROC CVPR IEEE, P772, DOI 10.1109/CVPR46437.2021.00083
   Masi Iacopo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P667, DOI 10.1007/978-3-030-58571-6_39
   Matern F, 2019, IEEE WINT CONF APPL, P83, DOI 10.1109/WACVW.2019.00020
   Mittal T, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2823, DOI 10.1145/3394171.3413570
   Nguyen HH, 2019, INT CONF ACOUST SPEE, P2307, DOI 10.1109/ICASSP.2019.8682602
   Qi H, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4318, DOI 10.1145/3394171.3413707
   Qiao T, 2019, IEEE T MULTIMEDIA, V21, P1077, DOI 10.1109/TMM.2018.2872863
   Rössler A, 2019, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2019.00009
   Sabir E., 2019, INTERFACES GUI, V3, P80
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shang ZH, 2021, PATTERN RECOGN, V116, DOI 10.1016/j.patcog.2021.107950
   Su LC, 2018, IEEE T MULTIMEDIA, V20, P825, DOI 10.1109/TMM.2017.2760098
   Sun K, 2021, AAAI CONF ARTIF INTE, V35, P2638
   Thies J, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323035
   Thies J, 2016, PROC CVPR IEEE, P2387, DOI 10.1109/CVPR.2016.262
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang O., 2020, PROC IEEECVF C COMPU, V7, P8695
   Yang X, 2019, INT CONF ACOUST SPEE, P8261, DOI 10.1109/ICASSP.2019.8683164
   Yu N, 2019, IEEE I CONF COMP VIS, P7555, DOI 10.1109/ICCV.2019.00765
   Yu Y, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3499026
   Yuyang Qian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P86, DOI 10.1007/978-3-030-58610-2_6
   Zhang D., 2021, PRCOEEDINGS 30 INT J, P1288
   Zhao HQ, 2021, PROC CVPR IEEE, P2185, DOI 10.1109/CVPR46437.2021.00222
   Zhao TC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15003, DOI 10.1109/ICCV48922.2021.01475
   Zhao XH, 2022, INT CONF ACOUST SPEE, P2884, DOI 10.1109/ICASSP43922.2022.9746061
   Zheng YL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15024, DOI 10.1109/ICCV48922.2021.01477
   Zhou TF, 2021, PROC CVPR IEEE, P5774, DOI 10.1109/CVPR46437.2021.00572
   Zhu Y, 2020, IEEE T IND INFORM, V16, P6714, DOI 10.1109/TII.2020.2982705
   Zi BJ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2382, DOI 10.1145/3394171.3413769
NR 66
TC 9
Z9 9
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8487
EP 8498
DI 10.1109/TMM.2023.3237322
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000044
DA 2024-07-18
ER

PT J
AU Zhang, HW
   Yang, Y
   Qi, F
   Qian, SS
   Xu, CS
AF Zhang, Huaiwen
   Yang, Yang
   Qi, Fan
   Qian, Shengsheng
   Xu, Changsheng
TI Robust Video-Text Retrieval Via Noisy Pair Calibration
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Noise calibration; uncertainty; video text retrieval
AB Video-text retrieval is a fundamental task in managing the emerging massive amounts of video data. The main challenge focuses on learning a common representation space for videos and queries where the similarity measurement can reflect the semantic closeness. However, existing video-text retrieval models may suffer from the following noise in the common space learning procedure: First, the video-text correspondences in positive pairs may not be exact matches. The crowdsourcing annotation for existing datasets leads to inevitable tagging noise for non-expert annotators. Second, the learning of video-text representation is based on the negative samples randomly sampled. Instances that are semantically similar to the query may be incorrectly categorized as negative samples. To alleviate the adverse impact of these noisy pairs, we propose a novel robust video-text retrieval method that protects the model from noisy positive and negative pairs by identifying and calibrating noisy pairs with their uncertainty score. In particular, we propose a noisy pair identifier, which divides the training dataset into noisy and clean subsets based on the estimated uncertainty of each pair. Then, with the help of uncertainties, we calibrate the two types of noisy pairs with an adaptive margin triplet loss and a weighted triplet loss function, respectively. To verify the effectiveness of our methods, we conduct extensive experiments on three widely used datasets. Experimental results show that the proposed robust video-text retrieval methods successfully identify and calibrate the noisy pairs and improve retrieval performance.
C1 [Zhang, Huaiwen; Yang, Yang] Inner Mongolia Univ, Coll Comp Sci, Mongolia 010031, Peoples R China.
   [Zhang, Huaiwen; Yang, Yang] Natl & Local Joint Engn Res Ctr Intelligent Inform, Mongolia 010031, Peoples R China.
   [Zhang, Huaiwen; Yang, Yang] Inner Mongolia Key Lab Mongolian Informat Proc Tec, Hohhot 010021, Peoples R China.
   [Qi, Fan] Tianjin Univ Technol, Sch Comp Sci & Engn, Tianjin 300384, Peoples R China.
   [Qian, Shengsheng; Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Qian, Shengsheng; Xu, Changsheng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
C3 Inner Mongolia University; Tianjin University of Technology; Chinese
   Academy of Sciences; Institute of Automation, CAS; Chinese Academy of
   Sciences; University of Chinese Academy of Sciences, CAS
RP Qian, SS (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.; Qian, SS (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
EM huaiwen.zhang@outlook.com; yangyang@mail.imu.edu.cn;
   cvaveryqf@gmail.com; shengsheng.qian@nlpr.ia.ac.cn; csxu@nlpr.ia.ac.cn
RI xu, cj/HJZ-3488-2023
OI xu, chang sheng/0000-0001-8343-9665; Zhang, Huaiwen/0000-0002-3183-9218
FU National Natural Science Foundation of China
FX No Statement Available
CR Anguelov D., 2015, P INT C LEARN REPR W
   Arazo E, 2019, PR MACH LEARN RES, V97
   Arpit D, 2017, PR MACH LEARN RES, V70
   Cao SQ, 2022, AAAI CONF ARTIF INTE, P167
   Chang HS, 2017, ADV NEUR IN, V30
   Chen D., 2011, P 49 ANN M ASS COMP, P190
   Chen FY, 2021, IEEE T MULTIMEDIA, V23, P3073, DOI 10.1109/TMM.2020.3019710
   Chen XL, 2015, IEEE I CONF COMP VIS, P1431, DOI 10.1109/ICCV.2015.168
   Chun S, 2021, PROC CVPR IEEE, P8411, DOI 10.1109/CVPR46437.2021.00831
   Croitoru I, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11563, DOI 10.1109/ICCV48922.2021.01138
   Dong JF, 2022, IEEE T CIRC SYST VID, V32, P5680, DOI 10.1109/TCSVT.2022.3150959
   Dong JF, 2019, PROC CVPR IEEE, P9338, DOI 10.1109/CVPR.2019.00957
   Dong JF, 2018, IEEE T MULTIMEDIA, V20, P3377, DOI 10.1109/TMM.2018.2832602
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Fang H., 2021, arXiv
   Gabeur Valentin, 2020, P EUR C COMP VIS, V12349, P214, DOI [10.48550/arXiv.2007.10639, DOI 10.48550/ARXIV.2007.10639]
   Goldberger J., 2017, INT C LEARNING REPRE
   Han B, 2018, ADV NEUR IN, V31
   Hendricks LA, 2017, IEEE I CONF COMP VIS, P5804, DOI 10.1109/ICCV.2017.618
   Hendrycks D, 2018, ADV NEUR IN, V31
   Huang G., 2021, Adv. Neural Inf. Process. Syst., V34, P29406
   Kingma D, 2014, ICLR P, V2014, P1
   Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83
   Li XR, 2021, IEEE T MULTIMEDIA, V23, P4351, DOI 10.1109/TMM.2020.3042067
   Liu Y., 2019, P BRIT MACH VIS C, P279
   Loshchilov I, 2017, P 5 INT C LEARN REPR
   Luo HS, 2022, NEUROCOMPUTING, V508, P293, DOI 10.1016/j.neucom.2022.07.028
   Oh S. J., 2019, P 7 INT C LEURN REPR, P1
   Patrick M., 2021, P 9 INT C LEARN REPR, P1
   Patrini G, 2017, PROC CVPR IEEE, P2233, DOI 10.1109/CVPR.2017.240
   Pereyra G., 2017, ICLR WORKSH, P313
   Portillo-Quintero JA, 2021, LECT NOTES COMPUT SC, V12725, P3, DOI 10.1007/978-3-030-77004-4_1
   Qin Y, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P4948, DOI 10.1145/3503161.3547922
   Radford A, 2021, PR MACH LEARN RES, V139
   Rohrbach A, 2015, PROC CVPR IEEE, P3202, DOI 10.1109/CVPR.2015.7298940
   Shizhe Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10635, DOI 10.1109/CVPR42600.2020.01065
   Song H, 2019, PR MACH LEARN RES, V97
   Song X, 2022, IEEE T MULTIMEDIA, V24, P2914, DOI 10.1109/TMM.2021.3090595
   Vaswani A, 2017, ADV NEUR IN, V30
   Villani C., 2003, TOPICS OPTIMAL TRANS, V58
   Wang W, 2023, IEEE T MULTIMEDIA, V25, P2661, DOI 10.1109/TMM.2022.3149716
   Wang W, 2021, IEEE T MULTIMEDIA, V23, P2386, DOI 10.1109/TMM.2020.3011288
   Wang XH, 2021, PROC CVPR IEEE, P5075, DOI 10.1109/CVPR46437.2021.00504
   Xiao T, 2015, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2015.7298885
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Yu Y, 2018, LECT NOTES COMPUT SC, V11211, P487, DOI 10.1007/978-3-030-01234-2_29
   Zhang BW, 2018, LECT NOTES COMPUT SC, V11217, P385, DOI 10.1007/978-3-030-01261-8_23
   Zhang Hongyi, 2018, MIXUP EMPIRICAL RISK, DOI DOI 10.48550/ARXIV.1710.09412
   Zheng Songzhu, 2020, PMLR, P11447
NR 49
TC 1
Z9 1
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8632
EP 8645
DI 10.1109/TMM.2023.3239183
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000070
DA 2024-07-18
ER

PT J
AU Zhang, TJ
   Liu, XW
   Gong, L
   Wang, SW
   Niu, X
   Shen, L
AF Zhang, Tiejian
   Liu, Xinwang
   Gong, Lei
   Wang, Siwei
   Niu, Xin
   Shen, Li
TI Late Fusion Multiple Kernel Clustering With Local Kernel Alignment
   Maximization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Kernel; Clustering algorithms; Robustness; Partitioning algorithms;
   Optimization; Manifolds; Benchmark testing; Multiple kernel clustering;
   neighbor; local kernel; local base partition; block diagonal structure
ID ALGORITHMS
AB Multi-view clustering, which appropriately integrates information from multiple sources to reveal data's inherent structure, is gaining traction in clustering. Though existing procedures have yielded satisfactory results, we observe that they have neglected the inherent local structure in the base kernels. This may cause adverse effects on clustering. To solve the problem, we introduce LF-MKC-LKA, a simple yet effective late fusion multiple kernel clustering with local kernel alignment maximisation approach. In particular, we first determine the nearest $k$ neighbours in the average kernel space for each sample and record the information in the nearest neighbor indicator matrix. Then, the nearest neighbor indicator matrix can be used to generate local structure matrix of each sample. The local kernels of each view may then be generated using the local structure matrix, retaining just the highly confident local similarities for learning the intrinsic global manifold of data. They can also be utilised to keep the block diagonal structure and improve the robustness of the underlying kernels against noise.We input the local kernels of each view into the kernel $k$-means (KKM) algorithm and get the local base partitions. Finally, we use a three-step iterative optimization approach to maximize the alignment of the consensus partition using base partitions and a regularisation term. As demonstrated, a significant number of trials on 11 multi-kernel benchmark datasets have shown that the proposed LF-MKC-LKA is effective and efficient. A number of experiments are also designed to demonstrate the fast convergence, excellent performance, robustness and low parameter sensitivity of the algorithm. Our code can be find at https://github.com/TiejianZhang/TMM21-LF-MKC-LKA.
C1 [Zhang, Tiejian; Liu, Xinwang; Gong, Lei; Wang, Siwei; Niu, Xin; Shen, Li] Natl Univ Def Technol, Sch Comp, Changsha 410073, Peoples R China.
C3 National University of Defense Technology - China
RP Liu, XW (corresponding author), Natl Univ Def Technol, Sch Comp, Changsha 410073, Peoples R China.
EM fegeneral007@gmail.com; xinwangliu@nudt.edu.cn; glnudt@163.com;
   wangsiwei13@nudt.edu.cn; niuxin@nudt.edu.cn; lishen@nudt.edu.cn
RI LIU, Xinwang/L-8089-2019; Wang, Siwei/ABD-1733-2021
OI LIU, Xinwang/0000-0001-9066-1475; Wang, Siwei/0000-0001-9517-262X
FU National Key R&D Program of China [2020AAA0107100]; National Natural
   Science Foundation of China [61922088, 61906020, 61872371, 62006237]
FX This work was supported in part by the National Key R&D Program of China
   under Project 2020AAA0107100 and in part by the National Natural Science
   Foundation of China under Projects 61922088, 61906020, 61872371 and
   62006237. The Associate Editor coordinating the review of this
   manuscript and approving it for publication was Prof. Xin Geng.
   (Corresponding author: Xinwang Liu.)
CR [Anonymous], 2011, INT C NEURAL INF PRO
   Bezdek J. C., 2003, Neural, Parallel & Scientific Computations, V11, P351
   Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962
   Boutsidis C, 2015, IEEE T INFORM THEORY, V61, P1045, DOI 10.1109/TIT.2014.2375327
   Chen JH, 2007, KDD-2007 PROCEEDINGS OF THE THIRTEENTH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P123
   Chen MS, 2020, AAAI CONF ARTIF INTE, V34, P3513
   Cortes C, 2012, J MACH LEARN RES, V13, P795
   Ding C, 2007, KDD-2007 PROCEEDINGS OF THE THIRTEENTH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P260
   Dou H, 2017, IEEE T MULTIMEDIA, V19, P1718, DOI 10.1109/TMM.2017.2689327
   Huang HC, 2012, IEEE T FUZZY SYST, V20, P120, DOI 10.1109/TFUZZ.2011.2170175
   Jegelka S., 2009, GEN CLUSTERING VIA K
   Kim DW, 2005, PATTERN RECOGN, V38, P607, DOI 10.1016/j.patcog.2004.09.006
   Kumar A., 2011, P 28 INT C MACHINE L, P393
   Li M., 2016, P 25 INT JOINT C ART, P1704
   Li XL, 2019, IEEE T NEUR NET LEAR, V30, P1587, DOI 10.1109/TNNLS.2018.2868847
   Li ZL, 2022, IEEE T MULTIMEDIA, V24, P2461, DOI 10.1109/TMM.2021.3081930
   Liu X., 2020, IEEE T KNOWL DATA EN, DOI [10.1109/TKDE.2020.3014104c/, DOI 10.1109/TKDE.2020.3014104C]
   Liu XW, 2021, IEEE T PATTERN ANAL, V43, P2634, DOI 10.1109/TPAMI.2020.2974828
   Liu XW, 2020, IEEE T PATTERN ANAL, V42, P1191, DOI 10.1109/TPAMI.2019.2892416
   Liu XW, 2019, IEEE T PATTERN ANAL, V41, P2410, DOI 10.1109/TPAMI.2018.2879108
   Liu XW, 2016, AAAI CONF ARTIF INTE, P1888
   Liu XW, 2014, IEEE T NEUR NET LEAR, V25, P1083, DOI 10.1109/TNNLS.2013.2287275
   Liu XW, 2013, IEEE T CYBERNETICS, V43, P557, DOI 10.1109/TSMCB.2012.2212243
   MARTIN CR, 1993, T ASAE, V36, P1399, DOI 10.13031/2013.28477
   Micchelli CA, 2005, J MACH LEARN RES, V6, P1099
   Nie FP, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2564
   Ren ZW, 2021, IEEE T NEUR NET LEAR, V32, P1839, DOI 10.1109/TNNLS.2020.2991366
   Scholkopf B, 1998, ADV NEUR IN, V10, P640
   Sun MJ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3528, DOI 10.1145/3474085.3475516
   Sun MJ, 2022, IEEE T MULTIMEDIA, V24, P2567, DOI 10.1109/TMM.2021.3086727
   Tang C, 2019, IEEE T MULTIMEDIA, V21, P1724, DOI 10.1109/TMM.2018.2889560
   Tao ZQ, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2843
   Wang H, 2020, IEEE T KNOWL DATA EN, V32, P1116, DOI 10.1109/TKDE.2019.2903810
   Wang Q, 2019, IEEE T NEUR NET LEAR, V30, P1265, DOI 10.1109/TNNLS.2018.2861209
   Wang QQ, 2021, IEEE T MULTIMEDIA, V23, P3483, DOI 10.1109/TMM.2020.3025666
   Wang SW, 2023, IEEE T NEUR NET LEAR, V34, P4359, DOI 10.1109/TNNLS.2021.3117403
   Wang SW, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3778
   Wei PF, 2019, IEEE T NEUR NET LEAR, V30, P1321, DOI 10.1109/TNNLS.2018.2868709
   Xiao XL, 2021, IEEE T MULTIMEDIA, V23, P4555, DOI 10.1109/TMM.2020.3045259
   Yao YQ, 2021, IEEE T NEUR NET LEAR, V32, P4983, DOI 10.1109/TNNLS.2020.3026532
   Yu S, 2012, IEEE T PATTERN ANAL, V34, P1031, DOI 10.1109/TPAMI.2011.255
   Zhou SH, 2023, IEEE T NEUR NET LEAR, V34, P252, DOI 10.1109/TNNLS.2021.3093426
   Zhou SH, 2020, IEEE T NEUR NET LEAR, V31, P1351, DOI 10.1109/TNNLS.2019.2919900
NR 43
TC 13
Z9 13
U1 2
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 993
EP 1007
DI 10.1109/TMM.2021.3136094
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA C4YC5
UT WOS:000961977900024
DA 2024-07-18
ER

PT J
AU Zhang, YS
   Zhou, WT
   Zhao, RY
   Zhang, XP
   Cao, XC
AF Zhang, Yushu
   Zhou, Wentao
   Zhao, Ruoyu
   Zhang, Xinpeng
   Cao, Xiaochun
TI F-TPE: Flexible Thumbnail-Preserving Encryption Based on Multi-Pixel
   Sum-Preserving Encryption
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image encryption; multi-pixel sum-preserving encryption; privacy
   preservation; thumbnail-preserving encryption
ID IMAGE ENCRYPTION; PRIVACY
AB People are accustomed to utilizing mobile phones to capture images and uploading them to the cloud due to various incomparable advantages such as saving the storage space on the device. In this context, privacy concerns are raised since images may contain some sensitive information. Encrypting images by traditional schemes can alleviate privacy leak, but the usability is often deprived since it is tough to browse them on the cloud. Recently, Tajik et al. proposed an ideal thumbnail-preserving encryption (TPE) scheme to achieve the balance of usability and privacy by leveraging a two-pixel substitution encryption method while the Markov chain is utilized to prove the security. However, the connectivity of Markov chain in this scheme is weak since the length of pixel groups in the encryption process is only two, leading to a long mixing time of achieving the stationary distribution of Markov chain. To this end, we firstly propose a method of multi-pixel sum-preserving encryption (MP-SPE) that realizes the encipherment of vectors of arbitrary length. Then, with the help of MP-SPE, a novel flexible ideal TPE scheme (F-TPE) is designed and the connection of Markov chain is improved. The experiments have demonstrated that the proposed scheme can effectively attain the balance between usability and privacy. In addition, F-TPE takes much less time in encrypting compared with the existing work.
C1 [Zhang, Yushu; Zhou, Wentao; Zhao, Ruoyu] Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 211106, Peoples R China.
   [Zhang, Yushu] Guangxi Normal Univ, Guangxi Key Lab Multisource Informat Min & Secur, Guilin 541004, Peoples R China.
   [Zhang, Xinpeng] Fudan Univ, Sch Comp Sci, Shanghai 200433, Peoples R China.
   [Cao, Xiaochun] Sun Yat Sen Univ, Sch Cyber Sci & Technol, Shenzhen Campus, Shenzhen 518107, Peoples R China.
C3 Nanjing University of Aeronautics & Astronautics; Guangxi Normal
   University; Fudan University; Sun Yat Sen University
RP Zhao, RY (corresponding author), Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 211106, Peoples R China.
EM yushu@nuaa.edu.cn; wentaozhou@nuaa.edu.cn; zhaoruoyu@nuaa.edu.cn;
   zhangxinpeng@fudan.edu.cn; caoxiaochun@mail.sysu.edu.cn
RI zhou, wentao/IQS-7359-2023
OI zhou, wentao/0000-0002-0472-989X; zhang, yushu/0000-0001-8183-8435;
   Zhao, Ruoyu/0000-0003-3631-1890
FU National Key R&D Program of China [2020YFB1406704]; National Natural
   Science Foundation of China [62072237, U1936214]; Research Fund of
   GuangxiKey Lab of Multi-source InformationMining and Security
   [MIMS20-02]; Basic Research Program of Jiangsu Province [BK20201290]
FX This work was supported in part by the National Key R&D Program of China
   underGrant 2020YFB1406704, in part by the NationalNatural Science
   Foundation of China under Grants 62072237 and U1936214, in part by the
   Research Fund of GuangxiKey Lab of Multi-source InformationMining and
   Security under Grant MIMS20-02, and in part by the Basic Research
   Program of Jiangsu Province under Grant BK20201290.
CR [Anonymous], 2016, Hackers publish nude pictures on Leslie Jones's website
   Atrey PK, 2006, MULTIMEDIA SYST, V12, P239, DOI 10.1007/s00530-006-0063-8
   Baltrusaitis T, 2016, IEEE WINT CONF APPL
   Bellarc M, 2009, LECT NOTES COMPUT SC, V5867, P295, DOI 10.1007/978-3-642-05445-7_19
   Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   Beugnon S, 2019, IEEE IMAGE PROC, P679, DOI [10.1109/icip.2019.8803836, 10.1109/ICIP.2019.8803836]
   Chai XL, 2022, INFORM SCIENCES, V604, P115, DOI 10.1016/j.ins.2022.05.008
   Chen JX, 2021, IEEE T MULTIMEDIA, V23, P2372, DOI 10.1109/TMM.2020.3011315
   Chuman T, 2019, IEEE T INF FOREN SEC, V14, P1515, DOI 10.1109/TIFS.2018.2881677
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Denning T, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P2615
   Driessen B, 2013, LECT NOTES COMPUT SC, V8099, P18
   Gafni O, 2019, IEEE I CONF COMP VIS, P9377, DOI 10.1109/ICCV.2019.00947
   Gregory RL, 1997, PHILOS T ROY SOC B, V352, P1121, DOI 10.1098/rstb.1997.0095
   Gross R, 2006, LECT NOTES COMPUT SC, V3856, P227
   Hassan ET, 2017, IEEE COMPUT SOC CONF, P1333, DOI 10.1109/CVPRW.2017.175
   Jianping He, 2016, 2016 46th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN). Proceedings, P359, DOI 10.1109/DSN.2016.40
   Jolfaei A, 2016, IEEE T INF FOREN SEC, V11, P235, DOI 10.1109/TIFS.2015.2489178
   Jourabloo A, 2015, INT CONF BIOMETR, P278, DOI 10.1109/ICB.2015.7139096
   Kang XJ, 2019, IEEE T CIRC SYST VID, V29, P1919, DOI 10.1109/TCSVT.2018.2859253
   Langheinrich Marc, 2001, Ubicomp 2001: Ubiquitous Computing, P273
   Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49
   Levin D. A., 2017, MARKOV CHAINS MIXING, V107, DOI DOI 10.1090/MBK/107
   Li A, 2018, 2018 IEEE SYMPOSIUM ON PRIVACY-AWARE COMPUTING (PAC), P10, DOI 10.1109/PAC.2018.00008
   Markatopoulou F, 2019, IEEE T CIRC SYST VID, V29, P1631, DOI 10.1109/TCSVT.2018.2848458
   Marohn B, 2017, PROCEEDINGS OF THE 2017 WORKSHOP ON MULTIMEDIA PRIVACY AND SECURITY (MPS'17), P33, DOI 10.1145/3137616.3137621
   Maximov M, 2020, PROC CVPR IEEE, P5446, DOI 10.1109/CVPR42600.2020.00549
   Menon S, 2020, PROC CVPR IEEE, P2434, DOI 10.1109/CVPR42600.2020.00251
   Newton EM, 2005, IEEE T KNOWL DATA EN, V17, P232, DOI 10.1109/TKDE.2005.32
   Ng HW, 2014, IEEE IMAGE PROC, P343, DOI 10.1109/ICIP.2014.7025068
   Parkhi OM, 2015, DEEP FACE RECOGNITIO
   Peng F, 2013, IEEE T INF FOREN SEC, V8, P1688, DOI 10.1109/TIFS.2013.2259819
   Qiao YG, 2019, IEEE T MULTIMEDIA, V21, P1, DOI 10.1109/TMM.2018.2845699
   Saini M. K., 2013, INT J TRUST MANAGEME, V1, P23
   Saini M, 2014, MULTIMED TOOLS APPL, V68, P135, DOI 10.1007/s11042-012-1207-9
   Saini M, 2010, IEEE INT CON MULTI, P60, DOI 10.1109/ICME.2010.5583334
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shi HC, 2021, IEEE T MULTIMEDIA, V23, P995, DOI 10.1109/TMM.2020.2991504
   Sun EYN, 2021, IEEE T CIRC SYST VID, V31, P112, DOI 10.1109/TCSVT.2020.2976050
   Taghavi S, 2020, 2020 IEEE/ACM SYMPOSIUM ON EDGE COMPUTING (SEC 2020), P382, DOI 10.1109/SEC50012.2020.00056
   Tajik K, 2019, 26TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2019), DOI 10.14722/ndss.2019.23432
   Uittenbogaard R, 2019, PROC CVPR IEEE, P10573, DOI 10.1109/CVPR.2019.01083
   Wright C.V., 2015, P 3 ACM WORKSH INF H, P141, DOI 10.1145/2756601.2756618
   Xia ZH, 2021, IEEE T NETW SCI ENG, V8, P318, DOI 10.1109/TNSE.2020.3038218
   Yifan Tian, 2017, 2017 IEEE Conference on Communications and Network Security (CNS), P1, DOI 10.1109/CNS.2017.8228627
   Yu J, 2018, IEEE T INF FOREN SEC, V13, P1317, DOI 10.1109/TIFS.2017.2787986
   Yu J, 2017, IEEE T INF FOREN SEC, V12, P1005, DOI 10.1109/TIFS.2016.2636090
   Zhang YS, 2022, IEEE T CIRC SYST VID, V32, P947, DOI 10.1109/TCSVT.2021.3070348
   Zhao RY, 2021, SIGNAL PROCESS, V183, DOI 10.1016/j.sigpro.2021.108019
NR 49
TC 21
Z9 21
U1 18
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5877
EP 5891
DI 10.1109/TMM.2022.3200310
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500017
DA 2024-07-18
ER

PT J
AU Zhao, TS
   Fang, Y
   Wang, K
   Liu, Q
   Niu, YZ
AF Zhao, Tiesong
   Fang, Ying
   Wang, Kai
   Liu, Qian
   Niu, Yuzhen
TI High Efficiency Vibrotactile Codec Based on Gate Recurrent Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Haptics; tactile; vibrotactile; codec
AB The multimedia has achieved dominant positions in both local storage and internet bandwidth, which inevitably promotes the compression of audio, image and video information. Nowadays, the emerging haptic technology, which enhances the immersion in virtual reality and remote control, has also brought new challenges in its codec design. It is thus imperative to develop haptic codecs, including kinesthetic and vibrotactile codecs, with high efficiency and low delay. In this paper, we exploit statistical features of vibrotactile data to develop a Recurrent-Network-based Vibrotactile Codec (RNVC) with high compression efficiency and low coding delay. The proposed encoder consists of vibrotactile estimation by Gate Recurrent Unit (GRU), non-uniform quantization/compensation of residuals and an entropy encoder. In particular, the GRU-based recurrent network is utilized for its high efficiency to predict signals and low complexity to converge. The decoder consists of all counterparts of encoder. Experimental results show the proposed RNVC significantly reduces of original bitrates with negligible encoding delay, which achieves the state-of-the-art coding performance of vibrotactile signal.
C1 [Zhao, Tiesong; Fang, Ying; Wang, Kai] Fuzhou Univ, Fujian Key Lab Intelligent Proc & Wireless Transmi, Fuzhou 350108, Fujian, Peoples R China.
   [Zhao, Tiesong] Peng Cheng Lab, Shenzhen 518000, Guangdong, Peoples R China.
   [Liu, Qian] Dalian Univ Technol, Dept Comp Sci & Technol, Dalian 116024, Peoples R China.
   [Niu, Yuzhen] Fuzhou Univ Fujian, Fujian Key Lab Network Comp & Intelligent Informat, Fuzhou 350108, Fujian, Peoples R China.
C3 Fuzhou University; Peng Cheng Laboratory; Dalian University of
   Technology; Fuzhou University
RP Niu, YZ (corresponding author), Fuzhou Univ Fujian, Fujian Key Lab Network Comp & Intelligent Informat, Fuzhou 350108, Fujian, Peoples R China.
EM t.zhao@fzu.edu.cn; fangying@fzu.edu.cn; n191120013@fzu.edu.cn;
   qianliu@dlut.edu.cn; YuzhenNiu@gmail.com
RI Wang, Kai/AAG-3471-2020
OI Wang, Kai/0000-0002-1271-2094
FU National_Science_Foundation of China [62171134, 62071083, U1908214];
   Fundamental Research Funds for the Central Universities [DUT21GJ208]
FX This work was supported in part by the National_Science_Foundation of
   China under Grants 62171134, 62071083,and U1908214, and in part by the
   Fundamental Research Funds for the Central Universities under Grant
   DUT21GJ208.
CR Al Jaafreh M., 2018, PROC IEEE INT S HAPT, P1
   Basdogan C, 2020, IEEE T HAPTICS, V13, P450, DOI 10.1109/TOH.2020.2990712
   Bi T, 2021, IEEE T MULTIMEDIA, V23, P3494, DOI 10.1109/TMM.2020.3025669
   Chaudhari R., 2012, P ACM MULT NAR JAP O, P409, DOI [10.1145/2393347.2393407, DOI 10.1145/2393347.2393407]
   Chaudhariu R, 2015, IEEE J-STSP, V9, P462, DOI 10.1109/JSTSP.2014.2374574
   Cho K., 2014, ARXIV14061078
   Chung Junyoung, 2014, ARXIV14123555
   Duan T, 2019, 2019 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND VIRTUAL REALITY (AIVR), P167, DOI 10.1109/AIVR46125.2019.00034
   Grigorii RV, 2017, 2017 IEEE WORLD HAPTICS CONFERENCE (WHC), P629, DOI 10.1109/WHC.2017.7989974
   Hashizume T, 2019, PROC IEEE INT SYMP, P1186, DOI 10.1109/ISIE.2019.8781236
   Hassen R, 2021, IEEE T MULTIMEDIA, V23, P4455, DOI 10.1109/TMM.2020.3042674
   Hassen R, 2020, IEEE T HAPTICS, V13, P25, DOI 10.1109/TOH.2019.2962446
   Hassen R, 2019, 2019 IEEE WORLD HAPTICS CONFERENCE (WHC), P301, DOI [10.1109/WHC.2019.8816110, 10.1109/whc.2019.8816110]
   Hinterseer P, 2005, INT CONF ACOUST SPEE, P1097
   Hsinfu Huang, 2019, 2019 IEEE 2nd International Conference on Knowledge Innovation and Invention (ICKII), P134, DOI 10.1109/ICKII46306.2019.9042684
   Jianqiong Xiao, 2020, 2020 Proceedings of IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA), P1285, DOI 10.1109/ICAICA50127.2020.9182390
   Jones LA, 2008, HUM FACTORS, V50, P90, DOI 10.1518/001872008X250638
   Kirsch J., 2018, P IEEE INT S HAPT AU, P1
   Klambauer G., 2017, Self-normalizing neural networks, P30, DOI 10.5555/3294771.3294864
   Kuchenbecker KJ, 2011, SPRINGER TRAC ADV RO, V70, P245
   Landin N, 2010, LECT NOTES COMPUT SC, V6192, P79, DOI 10.1007/978-3-642-14075-4_12
   Liu X, 2019, IEEE WRK SIG PRO SYS, P341, DOI [10.1109/sips47522.2019.9020534, 10.1109/SiPS47522.2019.9020534]
   Manfredi LR, 2014, J NEUROPHYSIOL, V111, P1792, DOI 10.1152/jn.00680.2013
   Mihelj M., 2012, HAPTICS VIRTUAL REAL
   Motaharifar M, 2019, IRAN CONF ELECTR ENG, P974, DOI [10.1109/IranianCEE.2019.8786510, 10.1109/iraniancee.2019.8786510]
   Noll A, 2021, 2021 IEEE WORLD HAPTICS CONFERENCE (WHC), P427, DOI 10.1109/WHC49131.2021.9517217
   Noll A, 2020, IEEE HAPTICS SYM, P854, DOI 10.1109/HAPTICS45997.2020.ras.HAP20.6.422bbc6e
   Okamoto S., 2010, Proceedings of the 2010 IEEE/SICE International Symposium on System Integration (SII 2010), P384, DOI 10.1109/SII.2010.5708356
   Okamoto S, 2008, IEEE INT CONF ROBOT, P220, DOI 10.1109/ROBOT.2008.4543212
   Okamoto S, 2009, IEEE T HAPTICS, V2, P73, DOI 10.1109/ToH.2009.17
   Sharma SK, 2020, IEEE ACCESS, V8, P56948, DOI 10.1109/ACCESS.2020.2980369
   Silva B, 2019, PROCEEDINGS OF 18TH INTERNATIONAL CONFERENCE ON SMART TECHNOLOGIES (IEEE EUROCON 2019), DOI 10.1109/eurocon.2019.8861847
   Steinbach E, 2019, P IEEE, V107, P447, DOI 10.1109/JPROC.2018.2867835
   Sun MM, 2019, ADJUNCT PROCEEDINGS OF THE 2019 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR-ADJUNCT 2019), P42, DOI 10.1109/ISMAR-Adjunct.2019.00026
   Tanaka H., 2009, P IEEE INT C MECH, P1
   Xue H, 2019, 2019 IEEE FIFTH INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM 2019), P111, DOI [10.1109/BigMM.2019.00026, 10.1109/BigMM.2019.00-36]
   Zeng CY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3642, DOI 10.1145/3394171.3413728
NR 37
TC 0
Z9 0
U1 1
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5043
EP 5052
DI 10.1109/TMM.2022.3186440
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300031
DA 2024-07-18
ER

PT J
AU Zheng, ZT
   Liu, JY
   Zheng, NN
AF Zheng, Zhentan
   Liu, Jianyi
   Zheng, Nanning
TI P<SUP>2</SUP>-GAN: Efficient Stroke Style Transfer Using Single Style
   Image
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Style transfer; generative adversarial network; stroke style; patch
   permutation
AB Style transfer is a useful image synthesis technique that can re-render given image into another artistic style while preserving its content information. Generative Adversarial Network (GAN) is a widely adopted framework toward this task for its better representation ability on local style patterns than the traditional Gram-matrix based methods. However, most previous methods rely on sufficient amount of pre-collected style images to train the model. In this paper, a novel Patch Permutation GAN (P-2-GAN) network that can efficiently learn the stroke style from a single style image is proposed. We use patch permutation to generate multiple training samples from the given style image. A patch discriminator that can simultaneously process patch-wise images and natural images seamlessly is designed. We also propose a local texture descriptor based criterion to quantitatively evaluate the style transfer quality. Experimental results showed that our method can produce finer quality re-renderings from single style image with improved computational efficiency compared with many state-of-the-arts methods.
C1 [Zheng, Zhentan; Liu, Jianyi; Zheng, Nanning] Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Shaanxi, Peoples R China.
C3 Xi'an Jiaotong University
RP Liu, JY (corresponding author), Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Shaanxi, Peoples R China.
EM 814998509@qq.com; jyliu@xjtu.edu.cn; nnzheng@mail.xjtu.edu.cn
OI Zheng, Zhentan/0000-0002-3336-1780
FU National Key Research and Development Program of China [2018AAA0102504];
   Provincial Key Laboratory Program of Shaanxi [2013SZS12-K01]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2018AAA0102504 and in part the
   Provincial Key Laboratory Program of Shaanxi under Grant 2013SZS12-K01.
CR [Anonymous], 2012, COURSERA NEURAL NETW
   Ashikhmin M, 2003, IEEE COMPUT GRAPH, V23, P38, DOI 10.1109/MCG.2003.1210863
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Chen DD, 2017, PROC CVPR IEEE, P2770, DOI 10.1109/CVPR.2017.296
   Chen XY, 2019, IEEE T IMAGE PROCESS, V28, P546, DOI 10.1109/TIP.2018.2869695
   Chen Y, 2018, PROC CVPR IEEE, P9465, DOI 10.1109/CVPR.2018.00986
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Howard AG, 2017, Arxiv, DOI arXiv:1704.04861
   Galteri L, 2019, IEEE T MULTIMEDIA, V21, P2131, DOI 10.1109/TMM.2019.2895280
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Gu SY, 2018, PROC CVPR IEEE, P8222, DOI 10.1109/CVPR.2018.00858
   Guo XP, 2019, IEEE T MULTIMEDIA, V21, P1982, DOI 10.1109/TMM.2019.2895292
   Guo Y, 2019, IEEE T MULTIMEDIA, V21, P2726, DOI 10.1109/TMM.2019.2908352
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jing YC, 2020, IEEE T VIS COMPUT GR, V26, P3365, DOI 10.1109/TVCG.2019.2921336
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kim BK, 2020, IEEE T MULTIMEDIA, V22, P298, DOI 10.1109/TMM.2019.2929000
   Li C, 2016, PROC CVPR IEEE, P2479, DOI 10.1109/CVPR.2016.272
   Li C, 2016, LECT NOTES COMPUT SC, V9907, P702, DOI 10.1007/978-3-319-46487-9_43
   Li XT, 2019, PROC CVPR IEEE, P3804, DOI 10.1109/CVPR.2019.00393
   Li ZY, 2021, IEEE T MULTIMEDIA, V23, P2694, DOI 10.1109/TMM.2020.3015015
   Liao J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073683
   Liu JY, 2018, IEEE T MULTIMEDIA, V20, P1724, DOI 10.1109/TMM.2017.2780761
   Liu Y, 2019, IEEE T MULTIMEDIA, V21, P2209, DOI 10.1109/TMM.2019.2897897
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Odena A., 2016, DISTILL, V1, pe3, DOI 10.23915/distill.00003.-URL
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Risser E, 2017, Arxiv, DOI arXiv:1701.08893
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sanakoyeu A, 2018, LECT NOTES COMPUT SC, V11212, P715, DOI 10.1007/978-3-030-01237-3_43
   Ulyanov D, 2016, PR MACH LEARN RES, V48
   Ulyanov D, 2017, PROC CVPR IEEE, P4105, DOI 10.1109/CVPR.2017.437
   Wang Q, 2021, INT J COMPUT VISION, V129, DOI 10.1007/s11263-020-01365-4
   Wang Q, 2019, IEEE T IMAGE PROCESS, V28, P4376, DOI 10.1109/TIP.2019.2910667
   Yao Y, 2019, PROC CVPR IEEE, P1467, DOI 10.1109/CVPR.2019.00156
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 41
TC 1
Z9 1
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6000
EP 6012
DI 10.1109/TMM.2022.3203220
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500026
DA 2024-07-18
ER

PT J
AU Du, C
   Graham, S
   Depp, C
   Nguyen, T
AF Du, Chen
   Graham, Sarah
   Depp, Colin
   Nguyen, Truong
TI Multi-Task Center-of-Pressure Metrics Estimation With Graph
   Convolutional Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Measurement; Estimation; Three-dimensional displays; Task analysis;
   Adaptation models; Computational modeling; Predictive models; Multi-task
   learning; center of pressure; computer vision; 3D body landmarks;
   balance control
ID BTRACKS BALANCE PLATE; UPPER-BODY BALANCE; POSTURAL CONTROL;
   NEURAL-NETWORKS; MODEL; MASS; GRAVITY; STANCE; SWAY; AGE
AB Center of pressure (CoP) metrics, including its path length, sway area, and position, are important measurements of postural and balance control in biomechanical studies. A computer-vision-based CoP metrics estimation system offers a portable solution to obtain these gold-standard metrics with 3D multi-joint coordination underlying body movements for real-time evaluation of balance control. In this paper, we propose an end-to-end framework for video-level estimation of CoP path length and sway area, as well as the frame-level estimation of CoP position, utilizing the spatial-temporal features and adaptive graph structure learned by graph convolution network. This work is the first step toward demonstrating that these gold-standard metrics can be obtained with a more comprehensive tool than current force plate technologies. We propose two single-task models for video-level and frame-level estimation, respectively, and a multi-task learning approach that jointly learns the two-temporal-level features. To facilitate this line of research, we release a novel computer-vision-based 3D body landmark dataset containing a wide variety of action patterns with synchronized CoP labels using pose estimation. We also adapt our framework on an existing kinematic dataset collected by wearable markers. The experiments on both datasets validate that our framework achieves state-of-the-art accuracies for all metric estimations, while the proposed multi-task approach yields the most accurate and robust performance on video-level estimation.(1)
C1 [Du, Chen; Nguyen, Truong] Univ Calif San Diego, Dept Elect & Comp Engn, La Jolla, CA 92093 USA.
   [Graham, Sarah; Depp, Colin] Univ Calif San Diego, Dept Psychiat, La Jolla, CA 92093 USA.
   [Graham, Sarah; Depp, Colin] Univ Calif San Diego, Sam & Rose Stein Inst Res Aging, La Jolla, CA 92093 USA.
   [Depp, Colin] VA San Diego Healthcare Syst, San Diego, CA 92161 USA.
C3 University of California System; University of California San Diego;
   University of California System; University of California San Diego;
   University of California System; University of California San Diego; US
   Department of Veterans Affairs; Veterans Health Administration (VHA); VA
   San Diego Healthcare System
RP Nguyen, T (corresponding author), Univ Calif San Diego, Dept Elect & Comp Engn, La Jolla, CA 92093 USA.
EM c9du@eng.ucsd.edu; sagraham@health.ucsd.edu; cdepp@health.ucsd.edu;
   tqn001@eng.ucsd.edu
RI Nguyen, Truong/JXN-9786-2024
OI Du, Chen/0000-0002-1023-5829; Nguyen, Truong/0000-0002-5022-063X
FU IBM Research AI through the AI Horizons Network
FX This work was supported by the IBM Research AI through the AI Horizons
   Network.
CR Amori V, 2015, GAIT POSTURE, V41, P19, DOI 10.1016/j.gaitpost.2014.08.003
   [Anonymous], 1997, NEURAL COMPUT
   Benda B. J., 1994, IEEE Transactions on Rehabilitation Engineering, V2, P3, DOI 10.1109/86.296348
   Cai XY, 2016, IEEE T MULTIMEDIA, V18, P141, DOI 10.1109/TMM.2015.2505089
   Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   CHEW V, 1966, J AM STAT ASSOC, V61, P605, DOI 10.2307/2282774
   Clauser C. E., 1969, Tech. Rep. TR-69-70 (AD 710 622), DOI DOI 10.21236/AD0710622
   Corriveau H, 2000, ARCH PHYS MED REHAB, V81, P45, DOI 10.1053/apmr.2000.0810045
   Cunha BP, 2012, NEUROSCI LETT, V513, P6, DOI 10.1016/j.neulet.2012.01.053
   dos Santos DA, 2017, PEERJ, V5, DOI 10.7717/peerj.3626
   Du C, 2020, INT CONF ACOUST SPEE, P2313, DOI [10.1109/ICASSP40776.2020.9053764, 10.1109/icassp40776.2020.9053764]
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Duarte M, 2010, BRAZ J PHYS THER, V14, P183, DOI 10.1590/S1413-35552010000300003
   Fujimoto M, 2015, J BIOMECH, V48, P963, DOI 10.1016/j.jbiomech.2015.02.012
   Funk C, 2018, LEARNING DYNAMICS KI
   Garrido-Jurado S, 2014, PATTERN RECOGN, V47, P2280, DOI 10.1016/j.patcog.2014.01.005
   Goble DJ, 2016, INT J SPORTS PHYS TH, V11, P149
   González A, 2014, SENSORS-BASEL, V14, P16955, DOI 10.3390/s140916955
   Graham SA, 2015, J AUTISM DEV DISORD, V45, P1419, DOI 10.1007/s10803-014-2303-7
   Gülcü A, 2020, IEEE ACCESS, V8, P52528, DOI 10.1109/ACCESS.2020.2981141
   Haid T, 2018, ENTROPY-SWITZ, V20, DOI 10.3390/e20010030
   Hartley R., 2003, MULTIPLE VIEW GEOMET
   Hasan S.S., 1996, GAIT POSTURE, V4, P1, DOI 10.1016/0966-6362(95)01030-0
   Hsu WL, 2007, J NEUROPHYSIOL, V97, P3024, DOI 10.1152/jn.01142.2006
   Hsu WL, 2013, AGE, V35, P1299, DOI 10.1007/s11357-012-9422-x
   Hu GY, 2020, IEEE T MULTIMEDIA, V22, P2207, DOI 10.1109/TMM.2019.2953325
   Jibin Gao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P222, DOI 10.1007/978-3-030-58577-8_14
   JOHANSSON G, 1973, PERCEPT PSYCHOPHYS, V14, P201, DOI 10.3758/BF03212378
   Kanekar N, 2014, EXP BRAIN RES, V232, P1127, DOI 10.1007/s00221-014-3822-3
   Kaselimi M, 2019, INT CONF ACOUST SPEE, P2747, DOI 10.1109/ICASSP.2019.8683110
   Lafond D, 2004, J BIOMECH, V37, P1421, DOI 10.1016/S0021-9290(03)00251-3
   Li M, 2016, IEEE T MULTIMEDIA, V18, P2293, DOI 10.1109/TMM.2016.2614228
   Li MP, 2019, IEEE T MULTIMEDIA, V21, P2653, DOI 10.1109/TMM.2019.2903455
   Liao YL, 2020, IEEE T NEUR SYS REH, V28, P468, DOI 10.1109/TNSRE.2020.2966249
   Lin CW, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20195588
   Liu K, 2021, IEEE T MULTIMEDIA, V23, P64, DOI 10.1109/TMM.2020.2974323
   Melzer I, 2004, AGE AGEING, V33, P602, DOI 10.1093/ageing/afh218
   Mettler A, 2015, J ATHL TRAINING, V50, P343, DOI 10.4085/1062-6050-49.3.94
   Ning GH, 2018, IEEE T MULTIMEDIA, V20, P1246, DOI 10.1109/TMM.2017.2762010
   O'Connor SM, 2016, J BIOMECH, V49, P4142, DOI 10.1016/j.jbiomech.2016.10.020
   Pan JH, 2019, IEEE I CONF COMP VIS, P6340, DOI 10.1109/ICCV.2019.00643
   Parmar P, 2019, PROC CVPR IEEE, P304, DOI 10.1109/CVPR.2019.00039
   Parmar P, 2019, IEEE WINT CONF APPL, P1468, DOI 10.1109/WACV.2019.00161
   Parmar P, 2017, IEEE COMPUT SOC CONF, P76, DOI 10.1109/CVPRW.2017.16
   Pirsiavash H, 2014, LECT NOTES COMPUT SC, V8694, P556, DOI 10.1007/978-3-319-10599-4_36
   PRINCE F, 1994, GAIT POSTURE, V2, P19, DOI 10.1016/0966-6362(94)90013-2
   Prioli AC, 2006, HUM MOVEMENT SCI, V25, P435, DOI 10.1016/j.humov.2006.03.003
   Rogind H, 2003, CLIN PHYSIOL FUNCT I, V23, P171, DOI 10.1046/j.1475-097X.2003.00492.x
   Ruhe A, 2011, EUR SPINE J, V20, P358, DOI 10.1007/s00586-010-1543-2
   Sainath TN, 2013, INT CONF ACOUST SPEE, P8614, DOI 10.1109/ICASSP.2013.6639347
   Schmit JM, 2006, EXP BRAIN RES, V168, P357, DOI 10.1007/s00221-005-0094-y
   Schubert P, 2014, GAIT POSTURE, V39, P518, DOI 10.1016/j.gaitpost.2013.09.001
   Scott J., 2020, KINEMATICS DYNAMICS
   Shi L, 2019, PROC CVPR IEEE, P12018, DOI 10.1109/CVPR.2019.01230
   Si CY, 2018, LECT NOTES COMPUT SC, V11205, P106, DOI 10.1007/978-3-030-01246-5_7
   Tang YS, 2018, PROC CVPR IEEE, P5323, DOI 10.1109/CVPR.2018.00558
   Terrier P, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10030774
   Wei WC, 2020, IEEE ACCESS, V8, P99889, DOI 10.1109/ACCESS.2020.2997341
   Wei Wenchuan., 2019, 2019 IEEE INT C HEAL, P1, DOI 10.1145/3358331.3358368
   Winter DA, 1996, J NEUROPHYSIOL, V75, P2334, DOI 10.1152/jn.1996.75.6.2334
   Wu YP, 2020, IEEE T MULTIMEDIA, V22, P2177, DOI 10.1109/TMM.2019.2953380
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Zhang SY, 2018, IEEE T MULTIMEDIA, V20, P2330, DOI 10.1109/TMM.2018.2802648
   Zhang ZY, 2012, IEEE MULTIMEDIA, V19, P4, DOI 10.1109/MMUL.2012.24
   Zoph B., 2016, INT C LEARN REPR
NR 65
TC 3
Z9 3
U1 1
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2018
EP 2033
DI 10.1109/TMM.2021.3075025
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0H8DA
UT WOS:000778959200019
DA 2024-07-18
ER

PT J
AU Gu, Z
   Dong, CQ
   Huo, J
   Li, WB
   Gao, Y
AF Gu, Zheng
   Dong, Chuanqi
   Huo, Jing
   Li, Wenbin
   Gao, Yang
TI CariMe: Unpaired Caricature Generation With Multiple Exaggerations
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Strain; Decoding; Faces; Deformable models; Visualization; Task
   analysis; Transforms; Caricature Generation; Image-to-image Translation;
   Image Warping; Style Transfer
AB Caricature generation aims to translate real photos into caricatures with artistic styles and shape exaggerations while maintaining the identity of the subject. Different from generic image-to-image translation, drawing caricatures automatically is a more challenging task due to the existence of various spatial deformations. Previous caricature generation methods are obsessed with predicting definite image warping from a given photo while ignoring the intrinsic representation and distribution of geometric exaggerations in caricatures. This limits their ability on diverse exaggeration generation. In this paper, we generalize the caricature generation problem from instance-level warping prediction to distribution-level deformation modeling. Based on this assumption, we present the first exploration for unpaired CARIcature generation with Multiple Exaggerations (CariMe). Technically, we propose a Multi-exaggeration Warper network to learn the distribution-level mapping from photos to facial exaggerations. This makes it possible to generate diverse and reasonable exaggerations from randomly sampled warp codes given one input photo. To better represent the facial exaggeration and produce fine-grained warping, a deformation-field-based warping method is also proposed, which captures more detailed exaggerations than previous point-based warping methods. Experiments and two perceptual studies prove the superiority of our method comparing with other state-of-the-art methods, showing the improvement of our work on caricature generation. The source code is available at https://github.com/edward3862/CariMe-pytorch.
C1 [Gu, Zheng; Dong, Chuanqi; Huo, Jing; Li, Wenbin; Gao, Yang] Nanjing Univ, State Key Lab Novel Software Technol, Nanjing 210023, Jiangsu, Peoples R China.
C3 Nanjing University
RP Huo, J (corresponding author), Nanjing Univ, State Key Lab Novel Software Technol, Nanjing 210023, Jiangsu, Peoples R China.
EM guzheng@smail.nju.edu.cn; dongchuanqi@smail.nju.edu.cn;
   huojing@nju.edu.cn; liwenbin@nju.edu.cn; gaoy@nju.edu.cn
RI GAO, Yang/HMO-8142-2023
OI Gu, Zheng/0000-0001-9914-3922; Gao, Yang/0000-0002-2488-1813
FU Science and Technology Innovation 2030 New Generation Artificial
   Intelligence Major [2018AAA0100905]; National Natural Science Foundation
   of China [61806092]; Natural Science Foundation of Jiangsu Province
   [BK20180326]; Fundamental Research Funds for the Central Universities
   [02021438008]; Collaborative Innovation Center of Novel Software
   Technology and Industrialization
FX This work was supported in part by Science and Technology Innovation
   2030 New Generation Artificial Intelligence Major under Project
   2018AAA0100905, in part by the National Natural Science Foundation of
   China 61806092, in part by the Natural Science Foundation of Jiangsu
   Province BK20180326, in part by the the Fundamental Research Funds for
   the Central Universities 02021438008, and in part by the Collaborative
   Innovation Center of Novel Software Technology and Industrialization.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Jianfei Cai.
CR Cao K., 2018, ACM Trans. on Graph., V37, P1, DOI [10.1145/3272127.3275046, DOI 10.1145/3272127.3275046]
   Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020
   Chen Y, 2018, PROC CVPR IEEE, P9465, DOI 10.1109/CVPR.2018.00986
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Chu W, 2020, ARXIV200805090
   Cole F, 2017, PROC CVPR IEEE, P3386, DOI 10.1109/CVPR.2017.361
   Fan XT, 2020, IEEE T MULTIMEDIA, V22, P655, DOI 10.1109/TMM.2019.2932573
   Ganin Y, 2016, LECT NOTES COMPUT SC, V9906, P311, DOI 10.1007/978-3-319-46475-6_20
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Gong J, 2020, PROC IEEE WINTER C A, P360
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gu SY, 2019, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2019.00355
   Han XG, 2020, IEEE T VIS COMPUT GR, V26, P2349, DOI 10.1109/TVCG.2018.2886007
   Han XG, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073629
   Hensel M, 2017, ADV NEUR IN, V30
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Huo Jing, 2018, BMVC, P223
   Jaderberg Max, 2015, ADV NEURAL INFORM PR, P8
   Jin J, 2016, IEEE T MULTIMEDIA, V18, P953, DOI 10.1109/TMM.2016.2539825
   Jing YC, 2020, IEEE T VIS COMPUT GR, V26, P3365, DOI 10.1109/TVCG.2019.2921336
   Kim Sunnie S. Y., 2020, P EUROPEAN C COMPUTE
   Kingma D. P., 2014, arXiv
   Li WB, 2020, NEURAL NETWORKS, V132, P66, DOI 10.1016/j.neunet.2020.08.011
   Li YJ, 2017, ADV NEUR IN, V30
   Liang L, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P386, DOI 10.1109/PCCGA.2002.1167882
   Lin CH, 2018, PROC CVPR IEEE, P9455, DOI 10.1109/CVPR.2018.00985
   Lin SS, 2013, IEEE T MULTIMEDIA, V15, P359, DOI 10.1109/TMM.2012.2228475
   Liu MY, 2017, ADV NEUR IN, V30
   Liu Y, 2019, IEEE T MULTIMEDIA, V21, P2209, DOI 10.1109/TMM.2019.2897897
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Paszke A, 2019, ADV NEUR IN, V32
   Ran Yi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8214, DOI 10.1109/CVPR42600.2020.00824
   Sadimon SB, 2010, 2010 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW 2010), P383, DOI 10.1109/CW.2010.33
   Shi YC, 2019, PROC CVPR IEEE, P6512, DOI 10.1109/CVPR.2019.00668
   Shin HC, 2007, IEEE T MULTIMEDIA, V9, P1125, DOI 10.1109/TMM.2007.898933
   Simonyan K., 2015, P 3 INT C LEARN REPR
   Wu QY, 2018, PROC CVPR IEEE, P7336, DOI 10.1109/CVPR.2018.00766
   Wu WY, 2019, PROC CVPR IEEE, P8004, DOI 10.1109/CVPR.2019.00820
   Xiao TH, 2018, LECT NOTES COMPUT SC, V11214, P172, DOI 10.1007/978-3-030-01249-6_11
   Xing XL, 2019, PROC CVPR IEEE, P10346, DOI 10.1109/CVPR.2019.01060
   Yaniv J, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322984
   Yu Xiaoming, 2019, P ADV NEUR INF PROC, P2990
   Yunjey Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8185, DOI 10.1109/CVPR42600.2020.00821
   Zheng ZQ, 2019, NEUROCOMPUTING, V355, P71, DOI 10.1016/j.neucom.2019.04.032
   Zhou YF, 2019, IEEE T MULTIMEDIA, V21, P3136, DOI 10.1109/TMM.2019.2920613
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 48
TC 4
Z9 4
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2673
EP 2686
DI 10.1109/TMM.2021.3086722
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 1D5KQ
UT WOS:000793839600031
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Hu, Y
   Wen, GH
   Chapman, A
   Yang, P
   Luo, MN
   Xu, YX
   Dai, D
   Hall, W
AF Hu, Yang
   Wen, Guihua
   Chapman, Adriane
   Yang, Pei
   Luo, Mingnan
   Xu, Yingxue
   Dai, Dan
   Hall, Wendy
TI Graph-Based Visual-Semantic Entanglement Network for Zero-Shot Image
   Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Semantics; Pipelines; Knowledge engineering; Load
   modeling; Task analysis; Couplings; Zero-shot learning; visual-semantic
   modeling; graph convolutional network; semantic knowledge graph;
   attribute word embedding
AB Zero-shot learning uses semantic attributes to connect the search space of unseen objects. In recent years, although the deep convolutional network brings powerful visual modeling capabilities to the ZSL task, its visual features have severe pattern inertia and lack of representation of semantic relationships, which leads to severe bias and ambiguity. In response to this, we propose the Graph-based Visual-Semantic Entanglement Network to conduct graph modeling of visual features, which is mapped to semantic attributes by using a knowledge graph, it contains several novel designs: 1. it establishes a multi-path entangled network with the convolutional neural network (CNN) and the graph convolutional network (GCN), which input the visual features from CNN to GCN to model the implicit semantic relations, then GCN feedback the graph modeled information to CNN features; 2. it uses attribute word vectors as the target for the graph semantic modeling of GCN, which forms a self-consistent regression for graph modeling and supervise GCN to learn more personalized attribute relations; 3. it fuses and supplements the hierarchical visual-semantic features refined by graph modeling into visual embedding. Our method outperforms state-of-the-art approaches on multiple representative ZSL datasets: AwA2, CUB, and SUN by promoting the semantic linkage modelling of visual features.
C1 [Hu, Yang; Wen, Guihua; Yang, Pei; Luo, Mingnan; Xu, Yingxue; Dai, Dan] South China Univ Technol, Guangzhou 510006, Peoples R China.
   [Hu, Yang; Chapman, Adriane; Hall, Wendy] Univ Southampton, Southampton SO17 1BJ, Hants, England.
   [Dai, Dan] Univ Lincoln, Lincoln LN6 7TS, England.
C3 South China University of Technology; University of Southampton;
   University of Lincoln
RP Hu, Y (corresponding author), South China Univ Technol, Guangzhou 510006, Peoples R China.
EM superhy199148@hotmail.com; crghwen@scut.edu.cn;
   Adriane.Chapman@soton.ac.uk; yangpei@scut.edu.cn; 770160905@qq.com;
   845947406@qq.com; daidanjune@hotmail.com; wh@ecs.soton.ac.uk
OI Hu, Yang/0000-0002-4856-5014; Hall, Wendy/0000-0003-4327-7811; Xu,
   Yingxue/0000-0002-9657-3107; Wen, Guihua/0000-0002-9709-1126; Dai,
   Dan/0000-0002-1287-7569; Chapman, Adriane/0000-0002-3814-2587
FU National Science Foundation of China [60973083, 61273363]; Science and
   Technology Planning Project of Guangdong Province [2014A010103009,
   2015A020217002]; Guangzhou Science and Technology Planning Project
   [201604020179, 201803010088]; Guangdong Province Key Area RD Plan
   [2020B1111120001]
FX This article was supported by National Science Foundation of China under
   Grants 60973083 and 61273363, Science and Technology Planning Project of
   Guangdong Province under Grants 2014A010103009 and 2015A020217002), and
   Guangzhou Science and Technology Planning Project under Grants
   201604020179 and 201803010088, and Guangdong Province Key Area R&D Plan
   under Project 2020B1111120001.
CR Akata Z, 2013, PROC CVPR IEEE, P819, DOI 10.1109/CVPR.2013.111
   Bojanowski P., 2017, Transactions of the Association for Computational Linguistics, V5, P135, DOI DOI 10.1162/TACLA00051
   Bouma G., 2009, P GSCL POTSD GERM, P31
   Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418
   Chao WL, 2016, LECT NOTES COMPUT SC, V9906, P52, DOI 10.1007/978-3-319-46475-6_4
   Chen JJ, 2020, AAAI CONF ARTIF INTE, V34, P10542
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Chen R, 2020, P 34 AAAI C ART INT
   Chen TS, 2019, IEEE I CONF COMP VIS, P522, DOI 10.1109/ICCV.2019.00061
   Chen TS, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P627
   Cui P, 2018, IEEE T MULTIMEDIA, V20, P198, DOI 10.1109/TMM.2017.2724843
   Cuturi M., 2013, ADV NEURAL INFORM PR, P2292, DOI DOI 10.48550/ARXIV.1306.0895
   Huynh D, 2020, PROC CVPR IEEE, P4482, DOI 10.1109/CVPR42600.2020.00454
   Dat Huynh, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8773, DOI 10.1109/CVPR42600.2020.00880
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Devlin J., 2018, BERT PRE TRAINING DE
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   Fu YW, 2015, IEEE T PATTERN ANAL, V37, P2332, DOI 10.1109/TPAMI.2015.2408354
   Gao JY, 2020, IEEE T MULTIMEDIA, V22, P3088, DOI 10.1109/TMM.2020.2969787
   Gao R, 2020, IEEE T IMAGE PROCESS, V29, P3665, DOI 10.1109/TIP.2020.2964429
   Guo JC, 2021, IEEE T MULTIMEDIA, V23, P524, DOI 10.1109/TMM.2020.2984091
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jiang HJ, 2018, LECT NOTES COMPUT SC, V11214, P121, DOI 10.1007/978-3-030-01249-6_8
   Jiang HJ, 2019, IEEE I CONF COMP VIS, P9764, DOI 10.1109/ICCV.2019.00986
   Kampffmeyer M., 2019, CVPR, P11487
   Kingma D. P., 2014, arXiv
   Kipf TN, 2016, ARXIV
   Kodirov E, 2017, PROC CVPR IEEE, P4447, DOI 10.1109/CVPR.2017.473
   Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594
   Li GH, 2019, IEEE I CONF COMP VIS, P9266, DOI 10.1109/ICCV.2019.00936
   Li KP, 2019, IEEE I CONF COMP VIS, P4653, DOI 10.1109/ICCV.2019.00475
   Li XY, 2019, IEEE T MULTIMEDIA, V21, P2117, DOI 10.1109/TMM.2019.2896516
   Li Y, 2018, PROC CVPR IEEE, P7463, DOI 10.1109/CVPR.2018.00779
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu K, 2021, IEEE T MULTIMEDIA, V23, P64, DOI 10.1109/TMM.2020.2974323
   Liu L, 2020, AAAI CONF ARTIF INTE, V34, P4868
   Liu Y, 2019, IEEE I CONF COMP VIS, P6697, DOI 10.1109/ICCV.2019.00680
   Lu LH, 2020, IEEE T MULTIMEDIA, V22, P524, DOI 10.1109/TMM.2019.2930344
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P3111, DOI DOI 10.5555/2999792.2999959
   Morgado P, 2017, PROC CVPR IEEE, P2037, DOI 10.1109/CVPR.2017.220
   Ni J, 2019, ADV NEUR IN, V32
   Niu L, 2019, IEEE T IMAGE PROCESS, V28, P965, DOI 10.1109/TIP.2018.2872916
   Patterson G, 2012, PROC CVPR IEEE, P2751, DOI 10.1109/CVPR.2012.6247998
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Rohrbach M., 2013, Advances in neural information processing systems, P46
   Romera-Paredes Bernardino, 2015, ICML
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Socher R., 2013, ADV NEURAL INFORM PR, V26, P935, DOI DOI 10.1007/978-3-319-46478-7
   Song J, 2018, PROC CVPR IEEE, P1024, DOI 10.1109/CVPR.2018.00113
   Speer R., 2017, PROC AAAI C ARTIF IN, V31
   Sun C, 2019, PROC CVPR IEEE, P273, DOI 10.1109/CVPR.2019.00036
   Tipping ME, 1999, J R STAT SOC B, V61, P611, DOI 10.1111/1467-9868.00196
   Verma VK, 2018, PROC CVPR IEEE, P4281, DOI 10.1109/CVPR.2018.00450
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wan ZY, 2019, ADV NEUR IN, V32
   Wang JB, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107075
   Wang XL, 2018, PROC CVPR IEEE, P6857, DOI 10.1109/CVPR.2018.00717
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xian YQ, 2019, PROC CVPR IEEE, P8248, DOI 10.1109/CVPR.2019.00845
   Xie GS, 2019, PROC CVPR IEEE, P9376, DOI 10.1109/CVPR.2019.00961
   Xu XY, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3996
   Ye M, 2019, PROC CVPR IEEE, P11720, DOI 10.1109/CVPR.2019.01200
   Yunlong Yu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14032, DOI 10.1109/CVPR42600.2020.01405
   Zhang L, 2019, P 30 BRIT MACH VIS C
   Zhang XX, 2020, IEEE T MULTIMEDIA, V22, P1692, DOI 10.1109/TMM.2019.2959433
   Zhang ZM, 2015, IEEE I CONF COMP VIS, P4166, DOI 10.1109/ICCV.2015.474
   Zhu Y., 2019, P 33 INT C NEURAL IN, p14 917
NR 73
TC 8
Z9 8
U1 3
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2473
EP 2487
DI 10.1109/TMM.2021.3082292
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 1D5KQ
UT WOS:000793839600019
OA Green Accepted, Green Submitted
DA 2024-07-18
ER

PT J
AU Kim, M
   Hong, J
   Park, SJ
   Ro, YM
AF Kim, Minsu
   Hong, Joanna
   Park, Se Jin
   Ro, Yong Man
TI CroMM-VSR: Cross-Modal Memory Augmented Visual Speech Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Speech recognition; Lips; Three-dimensional displays;
   Training; Face recognition; Feature extraction; Lip-reading; visual
   speech recognition; cross-modal; visual-audio memory
AB Visual Speech Recognition (VSR) is a task that recognizes speech from external appearances of the face ( i.e., lips) into text. Since the information from the visual lip movements is not sufficient to fully represent the speech, VSR is considered as one of the challenging problems. One possible way to resolve this problem is additionally utilizing audio which contains rich information for speech recognition. However, the audio information could not be always available such as in crowded situations. Thus, it is necessary to find a way that successfully provides enough information for speech recognition with visual inputs only. In this paper, we alleviate the information insufficiency of visual lip movement by proposing a cross-modal memory augmented VSR with Visual-Audio Memory (VAM). The proposed framework tries to utilize the complementary information of audio even when the audio inputs are not provided at the inference time. Concretely, the proposed VAM learns to imprint audio features of short clip-level into a memory network using the corresponding visual features. To this end, the VAM contains two memories, lip-video key and audio value. We guide the audio value memory to imprint the audio feature and the lip-video key memory to memorize the location of the imprinted audio. By doing this, the VAM can exploit rich audio information by accessing the memory using visual inputs only. Experimental results show that the proposed method achieves state-of-the-art performance on both word- and sentence-level VSR. In addition, we verify the learned representations inside the VAM contain meaningful information for VSR.
C1 [Kim, Minsu; Hong, Joanna; Park, Se Jin; Ro, Yong Man] Korea Adv Inst Sci & Technol, Image & Video Syst Lab, Sch Elect Engn, Daejeon 34141, South Korea.
C3 Korea Advanced Institute of Science & Technology (KAIST)
RP Ro, YM (corresponding author), Korea Adv Inst Sci & Technol, Image & Video Syst Lab, Sch Elect Engn, Daejeon 34141, South Korea.
EM ms.k@kaist.ac.kr; joanna2587@kaist.ac.kr; jinny960812@kaist.ac.kr;
   ymro@kaist.ac.kr
RI ; Ro, Yong Man/ABF-6817-2020
OI Hong, Joanna/0000-0003-4182-1000; Park, Se Jin/0000-0001-8467-3576; Ro,
   Yong Man/0000-0001-5306-6853; Kim, Minsu/0000-0002-6514-0018
CR Afouras T, 2022, IEEE T PATTERN ANAL, V44, P8717, DOI 10.1109/TPAMI.2018.2889052
   Afouras T, 2018, Arxiv, DOI arXiv:1809.00496
   Afouras T, 2020, INT CONF ACOUST SPEE, P2143, DOI [10.1109/ICASSP40776.2020.9054253, 10.1109/icassp40776.2020.9054253]
   [Anonymous], 2000, AUDIO-VISUAL SPEECH RECOGNITION
   Bo Xu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14421, DOI 10.1109/CVPR42600.2020.01444
   Cai Q, 2018, PROC CVPR IEEE, P4080, DOI 10.1109/CVPR.2018.00429
   Chen WC, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1985, DOI 10.1145/3394171.3413623
   Cho Kyunghyun, 2014, SYNTAX SEMANTICS STR, P5, DOI [10.3115/v1/w14-4012, 10.3115 /v1/D14-1179, DOI 10.3115/V1/D14-1179]
   Chung JS, 2017, LECT NOTES COMPUT SC, V10117, P251, DOI 10.1007/978-3-319-54427-4_19
   Chung JS, 2017, PROC CVPR IEEE, P3444, DOI 10.1109/CVPR.2017.367
   Chung JS, 2017, LECT NOTES COMPUT SC, V10112, P87, DOI 10.1007/978-3-319-54184-6_6
   Chung SW, 2019, INT CONF ACOUST SPEE, P3965, DOI 10.1109/ICASSP.2019.8682524
   Cooke M, 2006, J ACOUST SOC AM, V120, P2421, DOI 10.1121/1.2229005
   Dupont S, 2000, IEEE T MULTIMEDIA, V2, P141, DOI 10.1109/6046.865479
   Furlanello T, 2018, PR MACH LEARN RES, V80
   Gehring J, 2017, PR MACH LEARN RES, V70
   Graves A., 2006, P 23 INT C MACHINE L, P369
   Harte N, 2015, IEEE T MULTIMEDIA, V17, P603, DOI 10.1109/TMM.2015.2407694
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Huang X, 2018, PROC CVPR IEEE, P8837, DOI 10.1109/CVPR.2018.00921
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Kaiser L, 2017, Arxiv, DOI arXiv:1703.03129
   Kim J. U., 2021, IEEE CVF INT C COMP, P3050
   Kim M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P296, DOI 10.1109/ICCV48922.2021.00036
   Kingma D. P., 2014, arXiv
   KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694
   Lan Y., 2012, P C ACOUSTICS SPEECH, P4825
   Lee JS, 2008, IEEE T MULTIMEDIA, V10, P767, DOI 10.1109/TMM.2008.922789
   Lee S, 2021, PROC CVPR IEEE, P3053, DOI 10.1109/CVPR46437.2021.00307
   Lin KY, 2020, AAAI CONF ARTIF INTE, V34, P11515
   Luo M., 2020, Proceedings of the ACM on Human-Computer Interaction, V4, P1, DOI DOI 10.1145/3392853
   Luo MS, 2020, IEEE INT CONF AUTOMA, P273, DOI 10.1109/FG47880.2020.00010
   Assael YM, 2016, Arxiv, DOI arXiv:1611.01599
   Ma PC, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P7608, DOI 10.1109/ICASSP39728.2021.9415063
   Martinez B, 2020, INT CONF ACOUST SPEE, P6319, DOI [10.1109/ICASSP40776.2020.9053841, 10.1109/icassp40776.2020.9053841]
   Mesbah A, 2019, IMAGE VISION COMPUT, V88, P76, DOI 10.1016/j.imavis.2019.04.010
   Miller A., 2016, P 2016 C EMP METH NA, P1400, DOI DOI 10.18653/V1/D16-1147
   Nefian AV, 2002, INT CONF ACOUST SPEE, P2013
   Noda K, 2015, APPL INTELL, V42, P722, DOI 10.1007/s10489-014-0629-7
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Panayotov V, 2015, INT CONF ACOUST SPEE, P5206, DOI 10.1109/ICASSP.2015.7178964
   Peng YX, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3284750
   Petridis S, 2018, IEEE W SP LANG TECH, P513, DOI 10.1109/SLT.2018.8639643
   Petridis S, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P6548, DOI 10.1109/ICASSP.2018.8461326
   Rothkrantz L., 2017, 2017 SMART CITY S PR, P1, DOI [10.1109/SCSP.2017.7973348, DOI 10.1109/SCSP.2017.7973348]
   SATALOFF RT, 1992, SCI AM, V267, P108, DOI 10.1038/scientificamerican1292-108
   Stafylakis T, 2017, INTERSPEECH, P3652, DOI 10.21437/Interspeech.2017-85
   Sukhbaatar S, 2015, ADV NEUR IN, V28
   Tao F, 2021, IEEE T MULTIMEDIA, V23, P1, DOI 10.1109/TMM.2020.2975922
   Theobald B. J., 2006, P OPT PHOT COUNT CRI, V6402
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang C., 2019, arXiv preprint arXiv:1908.11618, P1
   Watanabe S, 2017, IEEE J-STSP, V11, P1240, DOI 10.1109/JSTSP.2017.2763455
   WENG X, 2019, ARXIV PREPRINT ARXIV, DOI DOI 10.48550/ARXIV.1905.02540
   Weston J, 2015, Arxiv, DOI arXiv:1410.3916
   Xiao JY, 2020, IEEE INT CONF AUTOMA, P364, DOI 10.1109/FG47880.2020.00132
   Xu K, 2018, IEEE INT CONF AUTOMA, P548, DOI 10.1109/FG.2018.00088
   Yang S, 2019, IEEE INT CONF AUTOMA, P222, DOI 10.1109/fg.2019.8756582
   Yu JW, 2020, INT CONF ACOUST SPEE, P6984, DOI [10.1109/icassp40776.2020.9054127, 10.1109/ICASSP40776.2020.9054127]
   Zhang XX, 2019, IEEE I CONF COMP VIS, P713, DOI 10.1109/ICCV.2019.00080
   Zhang YH, 2020, IEEE INT CONF AUTOMA, P356, DOI 10.1109/FG47880.2020.00134
   Zhao X, 2020, IEEE INT CONF AUTOMA, P420, DOI 10.1109/FG47880.2020.00133
   Zhao Y, 2020, AAAI CONF ARTIF INTE, V34, P6917
   Zhu MF, 2019, PROC CVPR IEEE, P5795, DOI 10.1109/CVPR.2019.00595
   Zisserman A., 2017, Proceedings of the British Machine Vision Conference, P155, DOI 10.5244/C.31.155
NR 66
TC 9
Z9 9
U1 1
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 4342
EP 4355
DI 10.1109/TMM.2021.3115626
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J3WX2
UT WOS:001008961300002
DA 2024-07-18
ER

PT J
AU Li, YY
   Yao, HT
   Xu, CS
AF Li, Yaoyu
   Yao, Hantao
   Xu, Changsheng
TI Intra-Domain Consistency Enhancement for Unsupervised Person
   Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Ice; Collaboration; Adaptation models; Training; Cameras; Pattern
   recognition; Noise measurement; Person re-identification; representation
   learning; unsupervised domain adaptation
ID ADAPTATION
AB Recently, unsupervised domain adaptation in person re-identification (ReID) has been widely studied to improve the generalization ability of the ReID model. Some existing methods focus on handling the intra-domain image variations caused by different camera configurations, pose, illumination, and background in target domain. However, they fail to fully mine the underlying consistency constraints contained in unlabeled target dataset. To comprehensively investigate the underlying constraints for unsupervised representation learning, we introduce two consistency constraints to deal with the intra-domain variations, namely instance-ensembling consistency and cross-granularity consistency. Specifically, the instance-ensembling consistency constraint aims to encourage similar features for a given instance and its positive samples. The cross-granularity consistency constraint is designed to enhance the collaboration of global clues and local clues in multi-granularity feature learning, which can overcome the negative effects caused by the noisy pseudo labels. By combining the advantages of the two constraints, we propose an iterative Intra-domain Consistency Enhancement (ICE) approach based on the Mean Teacher framework to fully mine the two underlying consistency constraints on multi-granularity features. The proposed ICE approach achieves significant improvement compared with the state-of-the-art, which demonstrates the superiority of the two consistency constraints.
C1 [Li, Yaoyu; Yao, Hantao; Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Li, Yaoyu] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
   [Xu, Changsheng] PengCheng Lab, Shenzhen 518066, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS
RP Xu, CS (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.; Xu, CS (corresponding author), PengCheng Lab, Shenzhen 518066, Peoples R China.
EM yaoyu.li@nlpr.ia.ac.cn; hantao.yao@nlpr.ia.ac.cn; csxu@nlpr.ia.ac.cn
RI Xu, Chang/GQP-7280-2022; xu, cj/HJZ-3488-2023
OI Yao, Hantao/0000-0001-8125-2864; xu, chang sheng/0000-0001-8343-9665
FU National Key Research and Development Program of China [2018AAA0102205];
   National Natural Science Foundation of China [61902399, 61721004,
   U1836220, U1705262, 61832002, 61720106006]; Beijing Natural Science
   Foundation [L201001]; Key Research Program of Frontier Sciences, CAS
   [QYZDJ-SSW-JSC039]
FX This work was supported in part by National Key Research and Development
   Program of China under Grant 2018AAA0102205, in part by National Natural
   Science Foundation ofChina 61902399, 61721004, U1836220, U1705262,
   61832002, and 61720106006, in part by Beijing Natural Science Foundation
   (L201001), and in part by the Key Research Program of Frontier Sciences,
   CAS(QYZDJ-SSW-JSC039). The associate editor coordinating the review of
   this manuscript and approving it for publication was Prof. L. Ballan.
   (Corresponding author: Changsheng Xu.)
CR [Anonymous], 2020, P C ASS ADV ART INT
   Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18
   Chapelle O., 2006, SEMISUPERVISED LEARN, P1, DOI 10.7551/mitpress/9780262033589.001.0001
   Chen M., 2011, ADV NEURAL INFORM PR, P2456
   Deng WJ, 2018, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.2018.00110
   Dongkai Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10978, DOI 10.1109/CVPR42600.2020.01099
   Ester Martin, 1996, kdd
   Fan HH, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3243316
   French G., 2018, INT C LEARN REPR ICL
   Fu Y, 2019, IEEE I CONF COMP VIS, P6111, DOI 10.1109/ICCV.2019.00621
   Ganin Y., 2015, ICML
   Ge Yixiao, 2020, ARXIV200101526
   Gretton A, 2006, Adv Neural Inf Process Syst, V19
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hermans Alexander, 2017, ARXIV170307737
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Laine Samuli, 2017, 5 INT C LEARNING REP, DOI DOI 10.48550/ARXIV.1610.02242
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Li YY, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P163, DOI 10.1145/3240508.3240573
   Li Y, 2018, LECT NOTES COMPUT SC, V10912, P172, DOI 10.1007/978-3-319-92252-2_13
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Lin Shan, 2018, 2018 IEEE 36 VLSI TE
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190
   Motiian S, 2017, IEEE I CONF COMP VIS, P5716, DOI 10.1109/ICCV.2017.609
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Song LC, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2019.107173
   Sun Y, 2018, INT CONF CLOUD COMPU, P1048, DOI 10.1109/CCIS.2018.8691342
   Tang HT, 2019, IEEE COMPUT SOC CONF, P1536, DOI 10.1109/CVPRW.2019.00195
   Tarvainen A, 2017, ADV NEUR IN, V30
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang JY, 2018, PROC CVPR IEEE, P2275, DOI 10.1109/CVPR.2018.00242
   Wang Z, 2016, IEEE T MULTIMEDIA, V18, P260, DOI 10.1109/TMM.2015.2505083
   Wei LH, 2019, IEEE T MULTIMEDIA, V21, P986, DOI 10.1109/TMM.2018.2870522
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wu L, 2019, IEEE T MULTIMEDIA, V21, P1412, DOI 10.1109/TMM.2018.2877886
   Ye M, 2016, IEEE T MULTIMEDIA, V18, P2553, DOI 10.1109/TMM.2016.2605058
   Yu HX, 2019, PROC CVPR IEEE, P2143, DOI 10.1109/CVPR.2019.00225
   Yunpeng Zhai, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9018, DOI 10.1109/CVPR42600.2020.00904
   Zheng HT, 2016, IEEE T MULTIMEDIA, V18, P2407, DOI 10.1109/TMM.2016.2598140
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng Liang, 2016, ARXIV
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z., 2017, P IEEE CVF C COMP VI, P1318, DOI [10.1109/CVPR.2017.389, DOI 10.1109/CVPR.2017.389]
   Zhong Z, 2018, LECT NOTES COMPUT SC, V11217, P176, DOI 10.1007/978-3-030-01261-8_11
   Zhong Z, 2021, IEEE T PATTERN ANAL, V43, P2723, DOI 10.1109/TPAMI.2020.2976933
   Zhong Z, 2019, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2019.00069
   Zhong Z, 2018, PROC CVPR IEEE, pCP99, DOI 10.1109/CVPR.2018.00541
   Zhou SP, 2018, IEEE T MULTIMEDIA, V20, P593, DOI 10.1109/TMM.2017.2755983
   Zou Y, 2018, LECT NOTES COMPUT SC, V11207, P297, DOI [10.1007/978-3-030-01219-9_, 10.1007/978-3-030-01219-9_18]
NR 52
TC 17
Z9 18
U1 3
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 415
EP 425
DI 10.1109/TMM.2021.3052354
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YK9LD
UT WOS:000745524300032
DA 2024-07-18
ER

PT J
AU Mikhailiuk, A
   Pérez-Ortiz, M
   Yue, DC
   Suen, W
   Mantiuk, RK
AF Mikhailiuk, Aliaksei
   Perez-Ortiz, Maria
   Yue, Dingcheng
   Suen, Wilson
   Mantiuk, Rafal K.
TI Consolidated Dataset and Metrics for High-Dynamic-Range Image Quality
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Measurement; Image quality; Training; Distortion; Brightness; Image
   coding; Observers; High Dynamic Range; Image Quality Dataset; Image
   Quality Metric
AB Increasing popularity of high-dynamic-range (HDR) image and video content brings the need for metrics that could predict the severity of image impairments as seen on displays of different brightness levels and dynamic range. Such metrics should be trained and validated on a sufficiently large subjective image quality dataset to ensure robust performance. As the existing HDR quality datasets are limited in size, we created a Unified Photometric Image Quality dataset (UPIQ) with over 4000 images by realigning and merging existing HDR and standard-dynamic-range (SDR) datasets. The realigned quality scores share the same unified quality scale across all datasets. Such realignment was achieved by collecting additional cross-dataset quality comparisons and re-scaling data with a psychometric scaling method. Images in the proposed dataset are represented in absolute photometric and colorimetric units, corresponding to light emitted from a display. We use the new dataset to retrain existing HDR metrics and show that the dataset is sufficiently large for training deep architectures. We show the utility of the dataset on brightness aware image compression.
C1 [Mikhailiuk, Aliaksei; Yue, Dingcheng; Suen, Wilson; Mantiuk, Rafal K.] Univ Cambridge, Dept Comp Sci & Technol, Cambridge CB3 0FD, England.
   [Perez-Ortiz, Maria] UCL, Dept Comp Sci, London WC1E 6BT, England.
C3 University of Cambridge; University of London; University College London
RP Mikhailiuk, A (corresponding author), Univ Cambridge, Dept Comp Sci & Technol, Cambridge CB3 0FD, England.
EM aliaksei.mikhailiuk@gmail.com; maria.perez@ucl.ac.uk; dy276@cam.ac.uk;
   wss28@cam.ac.uk; rafal.mantiuk@cl.cam.ac.uk
RI Perez-Ortiz, Maria/F-3910-2016; Mantiuk, Rafal K./I-4209-2016
OI Perez-Ortiz, Maria/0000-0003-1302-6093; Mantiuk, Rafal
   K./0000-0003-2353-0349
FU EPSRC from the European Research Council (ERC) under the European Union
   [EP/P007902/1, EP/R013616/1, 725253]; Marie Sklodowska-Curie grant
   agreement [765911]; EPSRC [EP/P007902/1] Funding Source: UKRI; Marie
   Curie Actions (MSCA) [765911] Funding Source: Marie Curie Actions
   (MSCA); European Research Council (ERC) [725253] Funding Source:
   European Research Council (ERC)
FX This work was supported by EPSRC research Grants EP/P007902/1 and
   EP/R013616/1, from the European Research Council (ERC) under the
   European Union's Horizon 2020 Research and Innovation Programme (grant
   agreementN degrees 725253 (EyeCode), and in part by the Marie
   Sklodowska-Curie grant agreement N degrees 765911 (RealVision).
CR Aldahdooh A, 2019, IEEE T MULTIMEDIA, V21, P2026, DOI 10.1109/TMM.2018.2882091
   Amirshahi SA, 2016, J IMAGING SCI TECHN, V60, DOI 10.2352/J.ImagingSci.Technol.2016.60.6.060410
   [Anonymous], 2015, Recommendation ITU
   [Anonymous], 2017, Quality and User Experience, DOI DOI 10.1007/S41233-017-0007-4
   Aydin TO, 2008, PROC SPIE, V6806, DOI 10.1117/12.765095
   Azimi-Sadjadi M. R., 2018, PROC INT JOINT C NEU, P1
   Bianco S, 2018, SIGNAL IMAGE VIDEO P, V12, P355, DOI 10.1007/s11760-017-1166-8
   Bovik Alan C, 2010, Handbook of image and video processing
   BRADLEY RA, 1952, BIOMETRIKA, V39, P324, DOI 10.1093/biomet/39.3-4.324
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Deng YB, 2017, IEEE SIGNAL PROC MAG, V34, P80, DOI 10.1109/MSP.2017.2696576
   Gao F, 2017, NEUROCOMPUTING, V257, P104, DOI 10.1016/j.neucom.2017.01.054
   Ghadiyaram D, 2016, IEEE T IMAGE PROCESS, V25, P372, DOI 10.1109/TIP.2015.2500021
   Hadizadeh H, 2018, IEEE T MULTIMEDIA, V20, P392, DOI 10.1109/TMM.2017.2740023
   Hainich R.R., 2016, PERCEPTUAL DISPLAY C
   Hanhart P, 2015, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-015-0091-4
   Herbrich Ralf., 2006, P ADV NEURAL INFORM
   Hosu V, 2020, IEEE T IMAGE PROCESS, V29, P4041, DOI 10.1109/TIP.2020.2967829
   Jayaraman D, 2012, CONF REC ASILOMAR C, P1693, DOI 10.1109/ACSSC.2012.6489321
   Jia S, 2017, IEEE IMAGE PROC, P765, DOI 10.1109/ICIP.2017.8296384
   Jin KH, 2017, IEEE T IMAGE PROCESS, V26, P4509, DOI 10.1109/TIP.2017.2713099
   Kim J, 2017, PROC CVPR IEEE, P1969, DOI 10.1109/CVPR.2017.213
   Kim J, 2017, IEEE SIGNAL PROC MAG, V34, P130, DOI 10.1109/MSP.2017.2736018
   Korshunov P, 2015, INT WORK QUAL MULTIM
   Krasula L, 2020, IEEE T MULTIMEDIA, V22, P2038, DOI 10.1109/TMM.2019.2952256
   Laparra V, 2017, J OPT SOC AM A, V34, P1511, DOI 10.1364/JOSAA.34.001511
   Li YM, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING (DSP), P685, DOI 10.1109/ICDSP.2016.7868646
   Li Z, 2016, SCI REP-UK, V6, DOI 10.1038/srep30338
   Lin H., 2020, ARXIV200108113
   Lin HH, 2019, INT WORK QUAL MULTIM
   Liu XL, 2019, IEEE T PATTERN ANAL, V41, P1862, DOI 10.1109/TPAMI.2019.2899857
   Ma KD, 2020, IEEE T PATTERN ANAL, V42, P851, DOI 10.1109/TPAMI.2018.2889948
   Ma KD, 2018, IEEE T IMAGE PROCESS, V27, P1202, DOI 10.1109/TIP.2017.2774045
   Mantiuk R, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964935
   Mantiuk RK, 2016, IEEE IMAGE PROC, P904, DOI 10.1109/ICIP.2016.7532488
   Mikhailiuk A., 2020, P INT C PATT REC APR, P1
   Mikhailiuk A., 2021, HUM VIS ELECT IMAG
   Mikhailiuk A., 2018, P INT C QUIAL MULT E, P1
   Min XK, 2018, IEEE T MULTIMEDIA, V20, P2049, DOI 10.1109/TMM.2017.2788206
   Min XK, 2019, IEEE T MULTIMEDIA, V21, P2319, DOI 10.1109/TMM.2019.2902097
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Narwaria M, 2015, SIGNAL PROCESS-IMAGE, V35, P46, DOI 10.1016/j.image.2015.04.009
   Narwaria M, 2015, J ELECTRON IMAGING, V24, DOI 10.1117/1.JEI.24.1.010501
   Narwaria M, 2013, OPT ENG, V52, DOI 10.1117/1.OE.52.10.102008
   Perez-Ortiz M., 2017, COMPUT RES REPOSITOR, P1
   Perez-Ortiz M, 2020, IEEE T IMAGE PROCESS, V29, P1139, DOI 10.1109/TIP.2019.2936103
   Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009
   Prashnani E, 2018, PROC CVPR IEEE, P1808, DOI 10.1109/CVPR.2018.00194
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Talebi H, 2018, IEEE T IMAGE PROCESS, V27, P3998, DOI 10.1109/TIP.2018.2831899
   Thurstone LL, 1927, PSYCHOL REV, V34, P273, DOI 10.1037/h0070288
   Valenzise G, 2014, PROC SPIE, V9217, DOI 10.1117/12.2063032
   Venkatanath N, 2015, NATL CONF COMMUN
   Voran S., 2002, ITERATED NESTED LEAS, P1
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2008, J VISION, V8, DOI 10.1167/8.12.8
   Wolski K, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3196493
   Wuerger SM, 2020, J VISION, V20, DOI 10.1167/jov.20.4.23
   Yan B, 2019, IEEE T MULTIMEDIA, V21, P2603, DOI 10.1109/TMM.2019.2904879
   Ye N., 2019, P IEEE CVF C COMP VI, P5434
   Ye P, 2014, PROC CVPR IEEE, P4249, DOI 10.1109/CVPR.2014.541
   Yue GH, 2018, IEEE T MULTIMEDIA, V20, P2722, DOI 10.1109/TMM.2018.2807589
   Zerman E., 2018, P HUM VIS EL IM, V2018, P1
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhao H, 2017, IEEE T COMPUT IMAG, V3, P47, DOI 10.1109/TCI.2016.2644865
NR 67
TC 4
Z9 4
U1 2
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2125
EP 2138
DI 10.1109/TMM.2021.3076298
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0H8DA
UT WOS:000778959200027
OA Green Published, Green Submitted
DA 2024-07-18
ER

PT J
AU Ning, HL
   Zheng, XT
   Lu, XQ
   Yuan, Y
AF Ning, Hailong
   Zheng, Xiangtao
   Lu, Xiaoqiang
   Yuan, Yuan
TI Disentangled Representation Learning for Cross-Modal Biometric Matching
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-modal biometric matching; disentangled representation learning;
   latent identity factors; modality-dependent factors
ID IMAGE; RECOGNITION; VOICES
AB Cross-modal biometric matching (CMBM) aims to determine the corresponding voice from a face, or identify the corresponding face from a voice. Recently, many CMBM methods have been proposed by forcing the distance between two modal features to be narrowed. However, these methods ignore the alignability between the two modal features. Because the feature is extracted under the supervision of identity information from single modal data, it can only reflect the identity information of single modal data. In order to address this problem, a disentangled representation learning method is proposed to disentangle the alignable latent identity factors and nonalignable the modality-dependent factors for CMBM. The proposed method consists of two main steps: 1) feature extraction and 2) disentangled representation learning. Firstly, an image feature extraction network is adopted to obtain face features, and a voice feature extraction network is applied to learn voice features. Secondly, a disentangled latent variable is explored to disentangle the latent identity factors that are shared across the modalities from the modality-dependent factors. The modality-dependent factors are filtered out, while the latent identity factors from the two modalities are enforced to be narrowed to align the same identity information. Then, the disentangled latent identity factors are considered as pure identity information to bridge the two modalities for cross-modal verification, 1:N matching, and retrieval. Note that the proposed method learns the identity information from the input face images and voice segments with only identity label as supervised information. Extensive experiments on the challenging VoxCeleb dataset demonstrate the proposed method outperforms the state-of-the-art methods.
C1 [Ning, Hailong; Zheng, Xiangtao; Lu, Xiaoqiang] Chinese Acad Sci, Xian Inst Opt & Precis Mech, Key Lab Spectral Imaging Technol CAS, Xian 710119, Peoples R China.
   [Ning, Hailong] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Yuan, Yuan] Northwestern Polytech Univ, Sch Artificial Intelligence Opt & Elect iOPEN, Xian 710072, Peoples R China.
C3 Chinese Academy of Sciences; Xi'an Institute of Optics & Precision
   Mechanics, CAS; Chinese Academy of Sciences; University of Chinese
   Academy of Sciences, CAS; Northwestern Polytechnical University
RP Zheng, XT (corresponding author), Chinese Acad Sci, Xian Inst Opt & Precis Mech, Key Lab Spectral Imaging Technol CAS, Xian 710119, Peoples R China.
EM ninghailong93@gmail.com; xiangtaoz@gmail.com; luxq666666@gmail.com;
   y.yuan1.ieee@gmail.com
OI Lu, Xiaoqiang/0000-0002-7037-5188; Ning, Hailong/0000-0001-8375-1181;
   Zheng, Xiangtao/0000-0002-8398-6324
FU Innovation Capability Support Program of Shaanxi [2020KJXX-091,
   2020TD-015]; National Key R&D Program of China [2020YFB2103902];
   National Science Fund for Distinguished Young Scholars [61825603,
   61925112]; Key Program of National Natural Science Foundation of China
   [61632018]
FX This work was supported in part by the Innovation Capability Support
   Program of Shaanxi under Grants 2020KJXX-091 and 2020TD-015, in part by
   the National Key R&D Program of China under Grant 2020YFB2103902, in
   part by the National Science Fund for Distinguished Young Scholars under
   Grants 61825603 and 61925112, and in part by the Key Program of National
   Natural Science Foundation of China under Grant 61632018.
CR Ainsworth SK, 2018, PR MACH LEARN RES, V80
   [Anonymous], 2017, PROC IEEE INT C COMP
   Arandjelovic R, 2017, IEEE I CONF COMP VIS, P609, DOI 10.1109/ICCV.2017.73
   Aytar Y, 2016, ADV NEUR IN, V29
   Brookes H, 2001, INFANT CHILD DEV, V10, P75, DOI 10.1002/icd.249
   Chen LL, 2017, PROCEEDINGS OF THE THEMATIC WORKSHOPS OF ACM MULTIMEDIA 2017 (THEMATIC WORKSHOPS'17), P349, DOI 10.1145/3126686.3126723
   Chowdhury A, 2018, INT C PATT RECOG, P3567, DOI 10.1109/ICPR.2018.8545260
   Chung JS, 2018, INTERSPEECH, P1086
   Chung JS, 2017, LECT NOTES COMPUT SC, V10117, P251, DOI 10.1007/978-3-319-54427-4_19
   Deng C, 2020, IEEE T IMAGE PROCESS, V29, P8892, DOI 10.1109/TIP.2020.3020383
   Glass J, 2018, Disentangling by Partitioning: A Representation Learning Framework for Multimodal Sensory Data
   Gong DH, 2017, IEEE T IMAGE PROCESS, V26, P2079, DOI 10.1109/TIP.2017.2651380
   Han CR, 2018, LECT NOTES COMPUT SC, V11213, P120, DOI 10.1007/978-3-030-01240-3_8
   Hershey S, 2017, INT CONF ACOUST SPEE, P131, DOI 10.1109/ICASSP.2017.7952132
   Higgins I., 2017, 5 INT C LEARN REPR T
   Hinz T, 2018, P INT JOINT C NEUR N, P1
   Horiguchi S, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1011, DOI 10.1145/3240508.3240601
   Hu D, 2019, PROC CVPR IEEE, P9240, DOI 10.1109/CVPR.2019.00947
   Hu WP, 2020, IEEE T MULTIMEDIA, V22, P1234, DOI 10.1109/TMM.2019.2938685
   Joassin F, 2011, CORTEX, V47, P367, DOI 10.1016/j.cortex.2010.03.003
   Kim Changil, 2018, P AS C COMP VIS ACCV, P276
   Kim H, 2018, PR MACH LEARN RES, V80
   Kingma D. P., 2014, arXiv
   Kümmerer M, 2017, IEEE I CONF COMP VIS, P4799, DOI 10.1109/ICCV.2017.513
   Leidal K, 2017, 2017 IEEE AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING WORKSHOP (ASRU), P424, DOI 10.1109/ASRU.2017.8268967
   Li J, 2020, IEEE T MULTIMEDIA, V22, P2990, DOI 10.1109/TMM.2020.2965434
   Liong VE, 2017, IEEE T MULTIMEDIA, V19, P1234, DOI 10.1109/TMM.2016.2646180
   Locatello F, 2019, PR MACH LEARN RES, V97
   Lorenz D, 2019, PROC CVPR IEEE, P10947, DOI 10.1109/CVPR.2019.01121
   Lu BY, 2019, PROC CVPR IEEE, P10217, DOI 10.1109/CVPR.2019.01047
   Lu XQ, 2020, IEEE T GEOSCI REMOTE, V58, P1985, DOI 10.1109/TGRS.2019.2951636
   Mavica LW, 2013, J EXP PSYCHOL HUMAN, V39, P307, DOI 10.1037/a0030945
   Mittal G, 2020, IEEE WINT CONF APPL, P3279, DOI [10.1109/WACV45572.2020.9093527, 10.1109/wacv45572.2020.9093527]
   Nagrani A, 2018, LECT NOTES COMPUT SC, V11217, P73, DOI 10.1007/978-3-030-01261-8_5
   Nagrani A, 2018, PROC CVPR IEEE, P8427, DOI 10.1109/CVPR.2018.00879
   Nagrani A, 2017, INTERSPEECH, P2616, DOI 10.21437/Interspeech.2017-950
   Nawaz S, 2019, 2019 DIGITAL IMAGE COMPUTING: TECHNIQUES AND APPLICATIONS (DICTA), P83, DOI 10.1109/dicta47822.2019.8945863
   Ning HL, 2021, NEUROCOMPUTING, V423, P124, DOI 10.1016/j.neucom.2020.10.053
   Owens A, 2016, LECT NOTES COMPUT SC, V9905, P801, DOI 10.1007/978-3-319-46448-0_48
   Parkhi OM, 2015, DEEP FACE RECOGNITIO
   Ponsot E, 2018, P NATL ACAD SCI USA, V115, P3972, DOI 10.1073/pnas.1716090115
   Senocak A, 2018, PROC CVPR IEEE, P4358, DOI 10.1109/CVPR.2018.00458
   Smith HMJ, 2016, EVOL PSYCHOL-US, V14, DOI 10.1177/1474704916630317
   Tian YP, 2018, LECT NOTES COMPUT SC, V11206, P252, DOI 10.1007/978-3-030-01216-8_16
   Wan CH, 2019, INT CONF ACOUST SPEE, P496, DOI [10.1109/ICASSP.2019.8682383, 10.1109/icassp.2019.8682383]
   Warren DE, 2020, FRONT HUM NEUROSCI, V13, DOI 10.3389/fnhum.2019.00448
   Wen Y, 2019, P 7 INT C LEARN REPR
   Wu Y, 2019, IEEE I CONF COMP VIS, P6301, DOI 10.1109/ICCV.2019.00639
   Xie D, 2020, IEEE T IMAGE PROCESS, V29, P3626, DOI 10.1109/TIP.2020.2963957
   Yan Y, 2020, IEEE T MULTIMEDIA, V22, P2792, DOI 10.1109/TMM.2019.2962317
   Yang E, 2018, IEEE T NEUR NET LEAR, V29, P5292, DOI 10.1109/TNNLS.2018.2793863
   Zhan HJ, 2021, IEEE T MULTIMEDIA, V23, P133, DOI 10.1109/TMM.2020.2978669
   Zhang JF, 2019, AAAI CONF ARTIF INTE, P9195
   Zhao Hang, 2018, P EUR C COMP VIS ECC, P570, DOI DOI 10.1109/CVPR.2018.00374
   Zhen LL, 2019, PROC CVPR IEEE, P10386, DOI 10.1109/CVPR.2019.01064
   Zheng XT, 2021, IEEE T MULTIMEDIA, V23, P1187, DOI 10.1109/TMM.2020.2993960
   Zheng XT, 2020, NEUROCOMPUTING, V403, P224, DOI 10.1016/j.neucom.2020.04.037
   Zheng XT, 2019, IEEE T GEOSCI REMOTE, V57, P4799, DOI 10.1109/TGRS.2019.2893115
   Zhou YP, 2018, PROC CVPR IEEE, P3550, DOI 10.1109/CVPR.2018.00374
NR 59
TC 20
Z9 20
U1 5
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1763
EP 1774
DI 10.1109/TMM.2021.3071243
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0H8DA
UT WOS:000778959200002
DA 2024-07-18
ER

PT J
AU Pramono, RRA
   Chen, YT
   Fang, WH
AF Pramono, Rizard Renanda Adhi
   Chen, Yie-Tarng
   Fang, Wen-Hsien
TI Spatial-Temporal Action Localization With Hierarchical Self-Attention
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Location awareness; Electron tubes; Videos; Proposals; Object detection;
   Detectors; Three-dimensional displays; Action localization;
   self-attention; object detection; convolutional neural networks (CNN);
   data association
ID ACTION RECOGNITION; NETWORK
AB This paper proposes a novel architecture for spatial-temporal action localization in videos. The new architecture first employs a two-stream 3D convolutional neural network (3D-CNN) to provide initial action detection. Next, a new Hierarchical Self-Attention Network (HiSAN), the core of this architecture, is developed to learn the spatial-temporal relationships of key actors. Spatial Gaussian priors (SGP) are also imbued to the bidirectional self-attention to enhance HiSAN in modelling the relationships of neighboring actors. Such a combination of 3D-CNN and SGP augmented HiSAN allows us to effectively extract both of the spatial context information and the long-term temporal dependency to improve action localization accuracy. Afterwards, a new fusion strategy is employed, which first re-scores the bounding boxes to settle the inconsistent detection scores caused by background clutter or occlusion, and then aggregates the motion and appearance information from the two-stream network with the motion saliency to alleviate the impact of camera movement. Finally, a tube association network based on the self-similarity of the actors' appearance and spatial information across frames is addressed to efficaciously construct the action tubes. Simulations on four widespread datasets reveal the efficacy of the new approach.
C1 [Pramono, Rizard Renanda Adhi; Chen, Yie-Tarng; Fang, Wen-Hsien] Natl Taiwan Univ Sci & Technol, Elect & Comp Engn, Taipei 106, Taiwan.
C3 National Taiwan University of Science & Technology
RP Fang, WH (corresponding author), Natl Taiwan Univ Sci & Technol, Elect & Comp Engn, Taipei 106, Taiwan.
EM d10702801@mail.ntust.edu.tw; ytchen@mail.ntust.edu.tw;
   whf@mail.ntust.edu.tw
OI Fag, Wen-Hsien/0000-0001-6402-2688; Pramono, Rizard Renanda
   Adhi/0000-0002-6668-5167
FU Ministry of Science and Technology, R.O.C. [MOST 109-2221-E-011-116,
   MOST 109-2221-E-011-131]
FX This work was supported by the Ministry of Science and Technology,
   R.O.C. under Contracts MOST 109-2221-E-011-116 and MOST
   109-2221-E-011-131.
CR Alwando EHP, 2020, IEEE T CIRC SYST VID, V30, P104, DOI 10.1109/TCSVT.2018.2887283
   [Anonymous], 2014, ADV COMPUT VIS PATTE, DOI 10.1007/978-3-319-09396-3_9
   Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Choutas V, 2018, PROC CVPR IEEE, P7024, DOI 10.1109/CVPR.2018.00734
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Devlin J., 2018, BERT PRE TRAINING DE
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Du WB, 2018, IEEE T IMAGE PROCESS, V27, P1347, DOI 10.1109/TIP.2017.2778563
   Duarte K, 2018, ADV NEUR IN, V31
   Fan ZX, 2017, LECT NOTES COMPUT SC, V10666, P262, DOI 10.1007/978-3-319-71607-7_23
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Girdhar R, 2019, PROC CVPR IEEE, P244, DOI 10.1109/CVPR.2019.00033
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Gkioxari G, 2015, PROC CVPR IEEE, P759, DOI 10.1109/CVPR.2015.7298676
   Gu CH, 2018, PROC CVPR IEEE, P6047, DOI 10.1109/CVPR.2018.00633
   Guo MS, 2019, AAAI CONF ARTIF INTE, P6489
   Han JW, 2018, IEEE SIGNAL PROC MAG, V35, P84, DOI 10.1109/MSP.2017.2749125
   Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685
   He JW, 2018, IEEE WINT CONF APPL, P343, DOI 10.1109/WACV.2018.00044
   Hou R, 2017, IEEE I CONF COMP VIS, P5823, DOI 10.1109/ICCV.2017.620
   Jhuang HH, 2013, IEEE I CONF COMP VIS, P3192, DOI 10.1109/ICCV.2013.396
   Jiaxi Wu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P456, DOI 10.1007/978-3-030-58517-4_27
   Kalogeiton V, 2017, IEEE I CONF COMP VIS, P4415, DOI 10.1109/ICCV.2017.472
   Kim IS, 2010, INT J CONTROL AUTOM, V8, P926, DOI 10.1007/s12555-010-0501-4
   Li D, 2018, LECT NOTES COMPUT SC, V11210, P306, DOI 10.1007/978-3-030-01231-1_19
   Li D, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P629, DOI 10.1145/3343031.3350978
   Li J, 2020, IEEE T MULTIMEDIA, V22, P2990, DOI 10.1109/TMM.2020.2965434
   Li YX, 2020, AAAI CONF ARTIF INTE, V34, P11466
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Lu LH, 2020, IEEE T MULTIMEDIA, V22, P524, DOI 10.1109/TMM.2019.2930344
   McGuire W., 1982, Matrix structural analysis
   Mettes P, 2017, IEEE I CONF COMP VIS, P4453, DOI 10.1109/ICCV.2017.476
   Peng XJ, 2016, LECT NOTES COMPUT SC, V9908, P744, DOI 10.1007/978-3-319-46493-0_45
   Pramono Rizard Renanda Adhi, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P71, DOI 10.1007/978-3-030-58452-8_5
   Pramono RRA, 2019, IEEE I CONF COMP VIS, P61, DOI 10.1109/ICCV.2019.00015
   Purwanto D, 2019, IEEE T MULTIMEDIA, V21, P3122, DOI 10.1109/TMM.2019.2919434
   Qiu ZF, 2019, PROC CVPR IEEE, P12048, DOI 10.1109/CVPR.2019.01233
   Redmon J., 2017, P IEEE C COMP VIS PA, P7263
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sadeghian A, 2017, IEEE I CONF COMP VIS, P300, DOI 10.1109/ICCV.2017.41
   Saha S., 2016, BMVC
   Shi YG, 2018, LECT NOTES COMPUT SC, V11214, P305, DOI 10.1007/978-3-030-01249-6_19
   Sidibé D, 2016, INT C PATT RECOG, P1876, DOI 10.1109/ICPR.2016.7899910
   Simonyan K, 2014, ADV NEUR IN, V27
   Singh B, 2016, PROC CVPR IEEE, P1961, DOI 10.1109/CVPR.2016.216
   Singh G, 2017, IEEE I CONF COMP VIS, P3657, DOI 10.1109/ICCV.2017.393
   Song L, 2019, PROC CVPR IEEE, P11979, DOI 10.1109/CVPR.2019.01226
   Soomro K., 2012, ARXIV12120402CS
   Sun C, 2018, LECT NOTES COMPUT SC, V11215, P335, DOI 10.1007/978-3-030-01252-6_20
   Sun SJ, 2021, IEEE T PATTERN ANAL, V43, P104, DOI 10.1109/TPAMI.2019.2929520
   Vannucci F., 2018, P IEEE RAS 18 INT C, P1, DOI [DOI 10.1109/HUMANOIDS.2018.8625004, 10.1109/HUMANOIDS.2018.8625004]
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang LM, 2016, PROC CVPR IEEE, P2708, DOI 10.1109/CVPR.2016.296
   Wang XL, 2018, LECT NOTES COMPUT SC, V11209, P413, DOI 10.1007/978-3-030-01228-1_25
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Weinzaepfel P, 2015, IEEE I CONF COMP VIS, P3164, DOI 10.1109/ICCV.2015.362
   Wu HB, 2020, IEEE T MULTIMEDIA, V22, P2293, DOI 10.1109/TMM.2019.2953814
   Xiang Y, 2015, IEEE I CONF COMP VIS, P4705, DOI 10.1109/ICCV.2015.534
   Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19
   Yang B., 2018, P C EMP METH NAT LAN, P6489
   Yang XT, 2019, PROC CVPR IEEE, P264, DOI 10.1109/CVPR.2019.00035
   Ye YC, 2019, J VIS COMMUN IMAGE R, V58, P515, DOI 10.1016/j.jvcir.2018.12.019
   Yixuan Li, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P68, DOI 10.1007/978-3-030-58517-4_5
   Yuxi Li, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P510, DOI 10.1007/978-3-030-58517-4_30
   Zha YF, 2020, IEEE T MULTIMEDIA, V22, P96, DOI 10.1109/TMM.2019.2922125
   Zhang DW, 2020, IEEE T PATTERN ANAL, V42, P1755, DOI 10.1109/TPAMI.2019.2900649
   Zhang Y, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P1, DOI 10.1145/3025453.3025842
   Zhang YB, 2019, PROC CVPR IEEE, P9967, DOI 10.1109/CVPR.2019.01021
   Zhao JJ, 2019, PROC CVPR IEEE, P9927, DOI 10.1109/CVPR.2019.01017
   Zhenzhi Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P34, DOI 10.1007/978-3-030-58595-2_3
   Zhu CC, 2019, PROC CVPR IEEE, P840, DOI 10.1109/CVPR.2019.00093
   Zolfaghari M, 2017, IEEE I CONF COMP VIS, P2923, DOI 10.1109/ICCV.2017.316
NR 76
TC 11
Z9 12
U1 2
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 625
EP 639
DI 10.1109/TMM.2021.3056892
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YW5XZ
UT WOS:000753488100009
DA 2024-07-18
ER

PT J
AU Wang, JW
   Zhao, JJ
   Yin, QL
   Luo, XY
   Zheng, YH
   Shi, YQ
   Jha, SK
AF Wang, Jinwei
   Zhao, Junjie
   Yin, Qilin
   Luo, Xiangyang
   Zheng, Yuhui
   Shi, Yun-Qing
   Jha, Sunil Kr
TI SmsNet: A New Deep Convolutional Neural Network Model for Adversarial
   Example Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Training; Manuals; Perturbation methods; Information
   science; Principal component analysis; Neural networks; Adversarial
   example detection; dynamic pruning strategy; feature statistical layer;
   SmsConnection
ID RECOGNITION; FRAMEWORK
AB The emergence of adversarial examples has had a significant impact on the development and application of deep learning. In this paper, a novel convolutional neural network model, the stochastic multifilter statistical network (SmsNet), is proposed for the detection of adversarial examples. A feature statistical layer is constructed to collect statistical data of feature map output from each convolutional layer in SmsNet by combining manual features with a neural network. The entire model is an end-to-end detection model, so the feature statistical layer is not independent of the network, and its output is directly transmitted to the fully connected layer by a short-cut connection called the SmsConnection. Additionally, a dynamic pruning strategy is introduced to simplify the model structure for better performance. The experiments demonstrate the effectiveness of the network structure and pruning strategy, and the proposed model achieves high detection rates against state-of-the-art adversarial attacks.
C1 [Wang, Jinwei; Jha, Sunil Kr] Nanjing Univ Informat Sci & Technol, Dept Comp & Software, Nanjing 210044, Jiangsu, Peoples R China.
   [Wang, Jinwei; Luo, Xiangyang] State Key Lab Math Engn & Adv Comp, Zhengzhou 450001, Henan, Peoples R China.
   [Wang, Jinwei] Xidian Univ, Shanxi Key Lab Network Syst Secur, Xian 710071, Shaanxi, Peoples R China.
   [Wang, Jinwei; Zhao, Junjie; Yin, Qilin; Zheng, Yuhui] Minist Educ, Engn Res Ctr Digital Forens, Nanjing 210044, Jiangsu, Peoples R China.
   [Zhao, Junjie; Yin, Qilin; Zheng, Yuhui] Nanjing Univ Informat Sci & Technol, Nanjing, Peoples R China.
   [Shi, Yun-Qing] New Jersey Inst Technol, Dept Elect & Comp Engn, Newark, NJ 07102 USA.
C3 Nanjing University of Information Science & Technology; PLA Information
   Engineering University; Xidian University; Nanjing University of
   Information Science & Technology; New Jersey Institute of Technology
RP Luo, XY (corresponding author), State Key Lab Math Engn & Adv Comp, Zhengzhou 450001, Henan, Peoples R China.; Zheng, YH (corresponding author), Minist Educ, Engn Res Ctr Digital Forens, Nanjing 210044, Jiangsu, Peoples R China.; Zheng, YH (corresponding author), Nanjing Univ Informat Sci & Technol, Nanjing, Peoples R China.
EM wjwei_2004@163.com; ginogogo@outlook.com; qilinyin1995@163.com;
   xiangyangluo@126.com; zhengyh@vip.126.com; shi@njit.edu;
   002891@nuist.edu.cn
OI Zhao, Junjie/0000-0002-1083-8216; Yin, Qilin/0000-0001-7571-046X
FU National Natural Science Foundation of China [62072250, 61772281,
   61702235, U1804263, U20B2065, U1636117, U1636219, 61872203, 61802212];
   National Key R&D Program of China [2016QY01W0105]; plan for Scientific
   Talent of Henan Province [2018JR0018]; Postgraduate Research & Practice
   Innvoation Program of Jiangsu Province [KYCX20_0974]; Priority Academic
   Program Development of Jiangsu Higher Education Institutions (PAPD) fund
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62072250, 61772281, 61702235, U1804263,
   U20B2065, U1636117, U1636219, 61872203, and 61802212, in part by the
   National Key R&D Program of China under Grant 2016QY01W0105, in part by
   the plan for ScientificTalent ofHenan Province underGrant 2018JR0018, in
   part by Postgraduate Research & Practice Innvoation Program of Jiangsu
   Province under Grant KYCX20_0974, and in part by the Priority Academic
   Program Development of Jiangsu Higher Education Institutions (PAPD)
   fund.
CR Amirian M, 2018, LECT NOTES ARTIF INT, V11081, P346, DOI 10.1007/978-3-319-99978-4_27
   Bengio S, 2016, ARXIV
   Caldelli R, 2019, IEEE IMAGE PROC, P2289, DOI [10.1109/icip.2019.8803776, 10.1109/ICIP.2019.8803776]
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Carlini Nicholas, 2017, ACM WORKSH ART INT S, P3
   Carrara F, 2017, PROCEEDINGS OF THE 15TH INTERNATIONAL WORKSHOP ON CONTENT-BASED MULTIMEDIA INDEXING (CBMI), DOI 10.1145/3095713.3095753
   Carrara Fabio, 2018, P EUR C COMP VIS ECC
   Cui RP, 2019, IEEE T MULTIMEDIA, V21, P1880, DOI 10.1109/TMM.2018.2889563
   Edraki M, 2018, LECT NOTES COMPUT SC, V11209, P90, DOI 10.1007/978-3-030-01228-1_6
   Fawzi Alhussein, 2018, Adv. Neural Inform. Process. Syst, DOI DOI 10.48550/ARXIV.1802.08686
   Feinman R., 2017, Detecting adversarial samples from artifacts
   Fridrich J, 2012, IEEE T INF FOREN SEC, V7, P868, DOI 10.1109/TIFS.2012.2190402
   Gal Y., 2016, THESIS
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Goodfellow I.J., 2015, PROC 3 INT C LEARN R
   Grosse K., 2017, ARXIV
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Li X, 2017, IEEE I CONF COMP VIS, P5775, DOI 10.1109/ICCV.2017.615
   Li ZC, 2020, INT J COMPUT VISION, V128, P2265, DOI 10.1007/s11263-020-01331-0
   Li ZC, 2015, IEEE T IMAGE PROCESS, V24, P5343, DOI 10.1109/TIP.2015.2479560
   Liu J., 2018, ARXIV181100189
   Liu JY, 2019, PROC CVPR IEEE, P4820, DOI 10.1109/CVPR.2019.00496
   Ma YY, 2019, IEEE T CIRC SYST VID, V29, P336, DOI 10.1109/TCSVT.2018.2799243
   Meng DY, 2017, CCS'17: PROCEEDINGS OF THE 2017 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P135, DOI 10.1145/3133956.3134057
   Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282
   Osadchy M, 2017, IEEE T INF FOREN SEC, V12, P2640, DOI 10.1109/TIFS.2017.2718479
   Papernot N., 2016, ARXIV161103814
   Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41
   Pevny T, 2010, IEEE T INF FOREN SEC, V5, P215, DOI 10.1109/TIFS.2010.2045842
   Qi GJ, 2020, INT J COMPUT VISION, V128, P1118, DOI 10.1007/s11263-019-01265-2
   Qi GJ, 2018, PROC CVPR IEEE, P1517, DOI 10.1109/CVPR.2018.00164
   Rockafellar R. T., 2015, CONVEX ANAL, DOI DOI 10.1515/9781400873173
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wang JW, 2020, IEEE T CIRC SYST VID, V30, P2736, DOI 10.1109/TCSVT.2019.2922309
   Wang JW, 2019, IEEE T CIRC SYST VID, V29, P2775, DOI 10.1109/TCSVT.2018.2867786
   Xu WL, 2018, 25TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2018), DOI 10.14722/ndss.2018.23198
   Yin QL, 2019, IEEE ACCESS, V7, P20293, DOI 10.1109/ACCESS.2019.2897000
   Yu RC, 2018, PROC CVPR IEEE, P9194, DOI 10.1109/CVPR.2018.00958
   Zhang J, 2019, IEEE T MULTIMEDIA, V21, P221, DOI 10.1109/TMM.2018.2844689
   Zhang LH, 2020, PROC CVPR IEEE, P3911, DOI 10.1109/CVPR42600.2020.00397
   Zhang QS, 2018, PROC CVPR IEEE, P8827, DOI 10.1109/CVPR.2018.00920
   Zhang SQ, 2018, IEEE T MULTIMEDIA, V20, P1576, DOI 10.1109/TMM.2017.2766843
   Zhao YR, 2018, LECT NOTES COMPUT SC, V11213, P508, DOI 10.1007/978-3-030-01240-3_31
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zou JH, 2018, COMPUT ELECTR ENG, V70, P950, DOI 10.1016/j.compeleceng.2018.01.036
NR 48
TC 17
Z9 18
U1 1
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 230
EP 244
DI 10.1109/TMM.2021.3050057
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YK9LD
UT WOS:000745524300018
DA 2024-07-18
ER

PT J
AU Wang, XJ
   Shao, F
   Jiang, QP
   Chai, XL
   Chao, MX
   Ho, YS
AF Wang, Xuejin
   Shao, Feng
   Jiang, Qiuping
   Chai, Xiongli
   Chao, Mengxiang
   Ho, Yo-Sung
TI List-Wise Rank Learning for Stereoscopic Image Retargeting Quality
   Assessment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Stereo image processing; Three-dimensional displays; Measurement;
   Visualization; Distortion; Two dimensional displays; Shape; Quality
   assessment; stereoscopic image retargeting; list-wise ranking
AB Stereoscopic imageretargeting (SIR) techniques attempt to display stereoscopic images on stereoscopic devices of various resolutions and aspect ratios to provide the users with better viewing experience. However, new quality perceptual problems emerge in the retargeted stereoscopic images generated by current SIR operators are quite different from those in the retargeted 2D images. In this paper, we dedicate to exploring the perceptual quality-related factors (e.g., shape preservation, object preservation and visual comfort.) of retargeted stereoscopic images, and propose a novel quality evaluation metric for SIR to achieve a more consistent evaluation with 3D perception and image degradation mechanism in the SIR process. Moreover, image quality features and 3D perceptual features are integrated into one representation for an overall perceptual quality prediction using a list-wise ranking approach, which gives priority to the ranking among the SIR results generated from the same stereoscopic source. Experimental results demonstrate that the proposed method outperforms most quality models developed for retargeted 2D/stereoscopic images.
C1 [Wang, Xuejin; Shao, Feng; Jiang, Qiuping; Chai, Xiongli; Chao, Mengxiang] Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Peoples R China.
   [Ho, Yo-Sung] Gwangju Inst Sci & Technol GIST, Sch Informat & Commun, Gwangju 500712, South Korea.
C3 Ningbo University; Gwangju Institute of Science & Technology (GIST)
RP Shao, F (corresponding author), Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Peoples R China.
EM 1020468620@qq.com; shaofeng@nbu.edu.cn; jiangqiuping@nbu.edu.cn;
   747866472@qq.com; mengxiangchao@nbu.edu.cn; hoyo@gist.ac.kr
RI Jiang, Qiuping/AAL-8273-2020
OI Chai, Xiongli/0000-0002-4245-5391; HO, YO-SUNG/0000-0002-7220-1034;
   Qiuping, Jiang/0000-0002-6025-9343
FU Natural Science Foundation of China [R18F010008]; Ningbo University
FX This work was supported in part by the Natural Science Foundation of
   China under Grants 62071261, 61901236, 41801252, and 61622109 and in
   part by Natural Science Foundation of China under Grant R18F010008. The
   work of K. C. Wong Magna was supported by Ningbo University. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. M. Carli.
CR Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   Boiman O, 2008, PROC CVPR IEEE, P1992, DOI 10.1109/CVPR.2008.4587598
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chai XL, 2020, IEEE T MULTIMEDIA, V22, P1208, DOI 10.1109/TMM.2019.2939707
   Chang CH, 2011, IEEE T MULTIMEDIA, V13, P589, DOI 10.1109/TMM.2011.2116775
   Chen Y, 2017, PROC CVPR IEEE, P4743, DOI 10.1109/CVPR.2017.504
   Chen YX, 2015, NEUROCOMPUTING, V151, P645, DOI 10.1016/j.neucom.2014.05.089
   Dekel T, 2013, IEEE T PATTERN ANAL, V35, P2513, DOI 10.1109/TPAMI.2013.46
   Dong WM, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618471
   Engelke U, 2015, IEEE SIGNAL PROC LET, V22, P705, DOI 10.1109/LSP.2014.2368136
   Fang YM, 2017, IEEE T SYST MAN CY-S, V47, P2956, DOI 10.1109/TSMC.2016.2557225
   Fang YM, 2014, IEEE J EM SEL TOP C, V4, P95, DOI 10.1109/JETCAS.2014.2298919
   Fu ZQ, 2021, IEEE T MULTIMEDIA, V23, P2100, DOI 10.1109/TMM.2020.3008054
   Fu ZQ, 2018, IEEE ACCESS, V6, P12008, DOI 10.1109/ACCESS.2018.2808322
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Hsu CC, 2014, IEEE J-STSP, V8, P377, DOI 10.1109/JSTSP.2014.2311884
   Hu B, 2019, IEEE T MULTIMEDIA, V21, P2042, DOI 10.1109/TMM.2019.2894958
   Karimi M, 2017, J VIS COMMUN IMAGE R, V43, P108, DOI 10.1016/j.jvcir.2016.12.011
   Karni Z, 2009, COMPUT GRAPH FORUM, V28, P1257, DOI 10.1111/j.1467-8659.2009.01503.x
   Khan S, 2018, IEEE T IMAGE PROCESS, V27, P5892, DOI 10.1109/TIP.2018.2860279
   Krähenbühl P, 2009, ACM T GRAPHIC, V28, DOI [10.1145/1616452.1618472, 10.1145/1618452.1618472]
   Lee KY, 2012, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2012.6247657
   Liang Y, 2017, IEEE T VIS COMPUT GR, V23, P1099, DOI 10.1109/TVCG.2016.2517641
   Lin JX, 2015, INT WORK QUAL MULTIM
   Lin SS, 2014, IEEE T CIRC SYST VID, V24, P759, DOI 10.1109/TCSVT.2013.2291282
   Lin SS, 2013, IEEE T VIS COMPUT GR, V19, P1677, DOI 10.1109/TVCG.2013.75
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147
   Liu Y., 2016, P IEEE INT C MULT EX, P1
   Liu YJ, 2020, IEEE T PATTERN ANAL, V42, P1798, DOI 10.1109/TPAMI.2019.2923998
   Liu YJ, 2011, COMPUT GRAPH FORUM, V30, P583, DOI 10.1111/j.1467-8659.2011.01881.x
   Ma L, 2016, IEEE T MULTIMEDIA, V18, P2228, DOI 10.1109/TMM.2016.2614187
   Ma L, 2012, IEEE J-STSP, V6, P626, DOI 10.1109/JSTSP.2012.2211996
   Mohammed SA, 2019, IEEE INT CONF MULTI, P342, DOI 10.1109/ICMEW.2019.00065
   Oliveira SAF, 2018, COMPUT VIS IMAGE UND, V168, P172, DOI 10.1016/j.cviu.2017.11.011
   Park J, 2015, IEEE T IMAGE PROCESS, V24, P1101, DOI 10.1109/TIP.2014.2383327
   Park J, 2014, IEEE J-STSP, V8, P415, DOI 10.1109/JSTSP.2014.2311885
   Pele O, 2009, IEEE I CONF COMP VIS, P460, DOI 10.1109/ICCV.2009.5459199
   Pritch Y, 2009, IEEE I CONF COMP VIS, P151, DOI 10.1109/ICCV.2009.5459159
   Rubinstein M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866186
   Rubinstein M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531329
   Rubinstein M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360615
   Shao F, 2021, IEEE T SYST MAN CY-S, V51, P3053, DOI 10.1109/TSMC.2019.2917496
   Shao F, 2017, IEEE T IMAGE PROCESS, V26, P4790, DOI 10.1109/TIP.2017.2721546
   Shao F, 2016, J DISP TECHNOL, V12, P22, DOI 10.1109/JDT.2015.2446973
   Shao F, 2013, IEEE T IMAGE PROCESS, V22, P1940, DOI 10.1109/TIP.2013.2240003
   Tie-Yan Liu, 2009, Foundations and Trends in Information Retrieval, V3, P225, DOI 10.1561/1500000016
   Wang WG, 2017, IEEE T VIS COMPUT GR, V23, P2014, DOI 10.1109/TVCG.2016.2600594
   Wang YS, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409071
   Wolf L, 2007, IEEE I CONF COMP VIS, P1418
   Ya M.L., 2014, 2014 S DESIGN TEST I, P1
   Zhang YB, 2016, IEEE T IMAGE PROCESS, V25, P4286, DOI 10.1109/TIP.2016.2585884
   Zhang YC, 2017, IEEE T IMAGE PROCESS, V26, P5980, DOI 10.1109/TIP.2017.2746260
   Zhou Y, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351198
   Zhou Y, 2021, SIGNAL PROCESS-IMAGE, V95, DOI 10.1016/j.image.2021.116236
   Zhu L., 2017, CHINA MULTIMEDIA
NR 55
TC 1
Z9 2
U1 1
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1595
EP 1608
DI 10.1109/TMM.2021.3068814
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0D8GN
UT WOS:000776227200027
DA 2024-07-18
ER

PT J
AU Xu, HY
   Jiang, GY
   Yu, M
   Zhu, ZJ
   Bai, YQ
   Song, Y
   Sun, HF
AF Xu, Haiyong
   Jiang, Gangyi
   Yu, Mei
   Zhu, Zhongjie
   Bai, Yongqiang
   Song, Yang
   Sun, Huifang
TI Tensor Product and Tensor-Singular Value Decomposition Based
   Multi-Exposure Fusion of Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Tensors; Feature extraction; Multi-exposure image fusion; tensor
   product; tensor-singular value decomposition; visual saliency
ID TRANSFORM; FRAMEWORK
AB Considering multidimensional structure of the multi-exposure images, a new Tensor product and Tensor-singular value decomposition based Multi-Exposure image Fusion (TT-MEF) method is proposed. The main innovation of this work is to explore a new feature representation of multi-exposure images in the new tensor domain and design the fusion strategy on this basis. Specifically, the luminance and the chrominance channels are fused separately to maintain color consistency. For the luminance fusion, the luminance channel of multi-exposure images is divided into two parts, that is, de-mean term and mean term. The de-mean term is represented as a tensor to extract the feature. Then, the tensor product and tensor-singular value decomposition (T-SVD) are used to design a tensor feature extractor. Furthermore, a fusion strategy of the de-mean term is presented according to the visual saliency model, and a fusion strategy of the mean term is defined by the local and the global visual weights to control counterpoise between the local and global luminance. For the chrominance fusion, a new fusion strategy is also designed by the tensor product and T-SVD, similar to the luminance fusion. Finally, the fused image is obtained by combining the luminance and chrominance fusion. Experimental results show that the proposed TT-MEF method generally outperforms the existing state-of-the-art in terms of subjective visual quality and objective evaluation.
C1 [Xu, Haiyong] Ningbo Univ, Sch Math & Stat, Ningbo 315211, Peoples R China.
   [Jiang, Gangyi; Yu, Mei; Song, Yang] Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Peoples R China.
   [Zhu, Zhongjie; Bai, Yongqiang] Zhejiang Wanli Univ, Coll Informat & Intelligence Engn, Ningbo 315100, Peoples R China.
   [Sun, Huifang] Mitsubishi Elect Res Labs, Cambridge, MA 02139 USA.
C3 Ningbo University; Ningbo University; Zhejiang Wanli University
RP Jiang, GY (corresponding author), Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Peoples R China.
EM haiyong_xu@163.com; jianggangyi@nbu.edu.cn; yumei@nbu.edu.cn;
   zhongjiezhu@hotmail.com; byq-163@163.com; songyang@nbu.edu.cn;
   hsun@merl.com
RI jiang, gang/KII-8233-2024
OI Yu, Mei/0000-0003-3583-1587; Bai, Yongqiang/0000-0002-4432-6593
FU Natural Science Foundation of China [62171243, 61871247, 62071266,
   61931022, 61671412]; Zhejiang Natural Science Foundation of China
   [LY19F020009, LY21F010003, LY19F010002, LQ20F010002, LY21F010014];
   K.C.Wong Magna Fund in Ningbo University
FX This work was supported in part by the Natural Science Foundation of
   China under Grants 62171243, 61871247, 62071266, 61931022, and 61671412,
   in part by the Zhejiang Natural Science Foundation of China
   underGrantsLY19F020009, LY21F010003, LY19F010002, LQ20F010002, and
   LY21F010014, and in part by K.C.Wong Magna Fund in Ningbo University.
CR Bhateja V, 2015, IEEE SENS J, V15, P6783, DOI 10.1109/JSEN.2015.2465935
   Chen G, 2019, OPT EXPRESS, V27, P10564, DOI 10.1364/OE.27.010564
   Chen YY, 2020, IEEE T COMPUT IMAG, V6, P1044, DOI 10.1109/TCI.2020.3001398
   De Lathauwer L, 2000, SIAM J MATRIX ANAL A, V21, P1253, DOI 10.1137/S0895479896305696
   Fang YM, 2020, IEEE T IMAGE PROCESS, V29, P1127, DOI 10.1109/TIP.2019.2940678
   Fotiadou K, 2020, IEEE T MULTIMEDIA, V22, P688, DOI 10.1109/TMM.2019.2933333
   Goshtasby AA, 2005, IMAGE VISION COMPUT, V23, P611, DOI 10.1016/j.imavis.2005.02.004
   Gu B, 2012, J VIS COMMUN IMAGE R, V23, P604, DOI 10.1016/j.jvcir.2012.02.009
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P432, DOI 10.1109/TMM.2016.2518868
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Hasinoff SW, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980254
   Huang YF, 2021, IEEE T MULTIMEDIA, V23, P176, DOI 10.1109/TMM.2020.2981994
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jung H, 2020, IEEE T IMAGE PROCESS, V29, P3845, DOI 10.1109/TIP.2020.2966075
   Kilmer ME, 2011, LINEAR ALGEBRA APPL, V435, P641, DOI 10.1016/j.laa.2010.09.020
   Kim YD, 2007, PROC CVPR IEEE, P3104
   Kinoshita Y, 2019, IEEE T IMAGE PROCESS, V28, P4101, DOI 10.1109/TIP.2019.2906501
   Kou F, 2018, IEEE T MULTIMEDIA, V20, P484, DOI 10.1109/TMM.2017.2743988
   Lee SH, 2018, IEEE IMAGE PROC, P1737, DOI 10.1109/ICIP.2018.8451153
   Li H, 2021, IEEE T CIRC SYST VID, V31, P4293, DOI 10.1109/TCSVT.2021.3053405
   Li H, 2020, IEEE T IMAGE PROCESS, V29, P5805, DOI 10.1109/TIP.2020.2987133
   Li H, 2018, IEEE IMAGE PROC, P1723, DOI 10.1109/ICIP.2018.8451689
   Li ST, 2013, IEEE T IMAGE PROCESS, V22, P2864, DOI 10.1109/TIP.2013.2244222
   Li ST, 2012, IEEE T CONSUM ELECTR, V58, P626, DOI 10.1109/TCE.2012.6227469
   Li ZG, 2012, IEEE T IMAGE PROCESS, V21, P4672, DOI 10.1109/TIP.2012.2207396
   Li ZG, 2017, IEEE T IMAGE PROCESS, V26, P1243, DOI 10.1109/TIP.2017.2651366
   Liang JL, 2012, IEEE T IMAGE PROCESS, V21, P2898, DOI 10.1109/TIP.2012.2183140
   Liu Y, 2015, INFORM FUSION, V24, P147, DOI 10.1016/j.inffus.2014.09.004
   Luo XQ, 2017, J VIS COMMUN IMAGE R, V45, P46, DOI 10.1016/j.jvcir.2017.02.006
   Ma KD, 2018, IEEE T COMPUT IMAG, V4, P60, DOI 10.1109/TCI.2017.2786138
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P2519, DOI 10.1109/TIP.2017.2671921
   Ma K, 2015, IEEE T IMAGE PROCESS, V24, P3345, DOI 10.1109/TIP.2015.2442920
   Martin CD, 2013, SIAM J SCI COMPUT, V35, pA474, DOI 10.1137/110841229
   Mertens T, 2009, COMPUT GRAPH FORUM, V28, P161, DOI 10.1111/j.1467-8659.2008.01171.x
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Paul S, 2016, J CIRCUIT SYST COMP, V25, DOI 10.1142/S0218126616501231
   Phan AH, 2013, IEEE T SIGNAL PROCES, V61, P4847, DOI 10.1109/TSP.2013.2269046
   Prabhakar KR, 2017, IEEE I CONF COMP VIS, P4724, DOI 10.1109/ICCV.2017.505
   Raman S., 2009, P 26 INT C MACHINE L, P1
   Rana A, 2019, IEEE T MULTIMEDIA, V21, P256, DOI 10.1109/TMM.2018.2839885
   Shao H, 2021, J VIS COMMUN IMAGE R, V75, DOI 10.1016/j.jvcir.2020.103017
   Shen JB, 2014, IEEE T CYBERNETICS, V44, P1579, DOI 10.1109/TCYB.2013.2290435
   Shen R, 2013, IEEE T IMAGE PROCESS, V22, P2469, DOI 10.1109/TIP.2012.2236346
   Shen R, 2011, IEEE T IMAGE PROCESS, V20, P3634, DOI 10.1109/TIP.2011.2150235
   Song ML, 2012, IEEE T IMAGE PROCESS, V21, P341, DOI 10.1109/TIP.2011.2157514
   Vonikakis V., 2011, P IASTED SIPA, P135
   Wang JH, 2014, NEUROCOMPUTING, V135, P145, DOI 10.1016/j.neucom.2013.12.042
   Wang QT, 2020, IEEE T CIRC SYST VID, V30, P2418, DOI 10.1109/TCSVT.2019.2919310
   Wang TH, 2015, IEEE T MULTIMEDIA, V17, P470, DOI 10.1109/TMM.2015.2403612
   Wu SC, 2020, MULTIMED TOOLS APPL, V79, P23957, DOI 10.1007/s11042-020-09131-x
   Xydeas CS, 2000, ELECTRON LETT, V36, P308, DOI 10.1049/el:20000267
   Yang B, 2010, IEEE T INSTRUM MEAS, V59, P884, DOI 10.1109/TIM.2009.2026612
   Yang KF, 2019, IEEE T CIRC SYST VID, V29, P640, DOI 10.1109/TCSVT.2018.2810212
   Zhai Y., 2006, P 14 ACM INT C MULT, P815, DOI [DOI 10.1145/1180639.1180824, 10.1145/1180639.1180824]
   Zhang Q, 2015, INFORM FUSION, V24, P54, DOI 10.1016/j.inffus.2014.09.008
   Zhang W, 2012, IEEE T IMAGE PROCESS, V21, P2318, DOI 10.1109/TIP.2011.2170079
NR 56
TC 6
Z9 7
U1 2
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 3738
EP 3753
DI 10.1109/TMM.2021.3106789
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 3R1UG
UT WOS:000838704400006
DA 2024-07-18
ER

PT J
AU Yao, HT
   Min, SB
   Zhang, YD
   Xu, CS
AF Yao, Hantao
   Min, Shaobo
   Zhang, Yongdong
   Xu, Changsheng
TI Attribute-Induced Bias Eliminating for Transductive Zero-Shot Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Visualization; Bridges; Training; Knowledge transfer; Image
   recognition; Topology; Transductive Zero-Shot Learning; Graph Attribute
   Embedding; Attribute-Induced Bias Eliminating; Semantic-Visual Alignment
AB Transductive zero-shot learning is designed to recognize unseen categories by aligning both visual and semantic information in a joint embedding space. Four types of domain biases exist in Transductive ZSL, i.e., visual bias and semantic bias in two domains, and two visual-semantic biases exist in the seen and unseen domains. However, the existing work has only focused on specific components of these topics, leading to severe semantic ambiguity during knowledge transfer. To solve this problem, we propose a novel attribute-induced bias eliminating (AIBE) module for Transductive ZSL. Specifically, for the visual bias between the two domains, the mean-teacher module is first used to bridge the visual representation discrepancy between the two domains using unsupervised learning and unlabeled images. Then, an attentional graph attribute embedding process is proposed to reduce the semantic bias between seen and unseen categories using a graph operation to describe the semantic relationship between categories. To reduce semantic-visual bias in the seen domain, we align the visual center of each category with the corresponding semantic attributes instead of with the individual visual data point, which preserves the semantic relationship in the embedding space. Finally, for the semantic-visual bias in the unseen domain, an unseen semantic alignment constraint is designed to align visual and semantic space using an unsupervised process. The evaluations on several benchmarks demonstrate the effectiveness of the proposed method, e.g., 82.8%/75.5%, 97.1%/82.5%, and 73.2%/52.1% for Conventional/Generalized ZSL settings for CUB, AwA2, and SUN datasets, respectively.
C1 [Yao, Hantao; Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Min, Shaobo; Zhang, Yongdong] Univ Sci & Technol China, Natl Engn Lab Brain Inspired Intelligence Technol, Hefei 230026, Peoples R China.
   [Xu, Changsheng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Science & Technology of China, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS
RP Yao, HT (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM hantao.yao@nlpr.ia.ac.cn; mbobo@mail.ustc.edu.cn; zhyd73@ustc.edu.cn;
   csxu@nlpr.ia.ac.cn
RI xu, cj/HJZ-3488-2023; Xu, Chang/GQP-7280-2022
OI xu, chang sheng/0000-0001-8343-9665; Yao, Hantao/0000-0001-8125-2864
FU National Key Research and Development Program of China [2018AAA0102200];
   National Natural Science Foundation of China [61902399, 61721004,
   U1836220, U1705262, 61832002, 61720106006, 62002355]; Beijing Natural
   Science Foundation [L201001]; Key Research Program of Frontier Sciences,
   CAS [QYZDJ-SSW-JSC039]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2018AAA0102200, in part by the
   National Natural Science Foundation of China under Grants 61902399,
   61721004, U1836220, U1705262, 61832002, 61720106006, and 62002355, in
   part by Beijing Natural Science Foundation under Grant L201001, and in
   part by the Key Research Program of Frontier Sciences, CAS under Grant
   QYZDJ-SSW-JSC039.
CR Annadani Y, 2018, PROC CVPR IEEE, P7603, DOI 10.1109/CVPR.2018.00793
   Changpinyo S, 2020, INT J COMPUT VISION, V128, P166, DOI 10.1007/s11263-019-01193-1
   French G., 2018, INT C LEARN REPR ICL
   Fu YW, 2015, IEEE T PATTERN ANAL, V37, P2332, DOI 10.1109/TPAMI.2015.2408354
   Guo JC, 2021, IEEE T MULTIMEDIA, V23, P524, DOI 10.1109/TMM.2020.2984091
   Guo YC, 2017, AAAI CONF ARTIF INTE, P4061
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jia Z, 2020, IEEE T IMAGE PROCESS, V29, P1958, DOI 10.1109/TIP.2019.2947780
   Kampffmeyer M., 2019, CVPR, P11487
   Kipf TN, 2017, INT C LEARN REPR
   Kodirov E, 2015, IEEE I CONF COMP VIS, P2452, DOI 10.1109/ICCV.2015.282
   Laine Samuli, 2017, 5 INT C LEARNING REP, DOI DOI 10.48550/ARXIV.1610.02242
   Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594
   Li K, 2019, IEEE I CONF COMP VIS, P3582, DOI 10.1109/ICCV.2019.00368
   Li Y, 2018, PROC CVPR IEEE, P7463, DOI 10.1109/CVPR.2018.00779
   Liu Y, 2019, IEEE I CONF COMP VIS, P6697, DOI 10.1109/ICCV.2019.00680
   Ma XH, 2019, IEEE T MULTIMEDIA, V21, P2419, DOI 10.1109/TMM.2019.2902100
   Norouzi M, 2013, ARXIV13125650
   Patterson G, 2012, PROC CVPR IEEE, P2751, DOI 10.1109/CVPR.2012.6247998
   Paul A, 2019, PROC CVPR IEEE, P7049, DOI 10.1109/CVPR.2019.00722
   Radovanovic M, 2010, J MACH LEARN RES, V11, P2487
   Rohrbach M., 2013, Advances in neural information processing systems, P46
   Romera-Paredes B, 2015, PR MACH LEARN RES, V37, P2152
   Sariyildiz MB, 2019, PROC CVPR IEEE, P2163, DOI 10.1109/CVPR.2019.00227
   Snell J, 2017, ADV NEUR IN, V30
   Song J, 2018, PROC CVPR IEEE, P1024, DOI 10.1109/CVPR.2018.00113
   Song J, 2018, LECT NOTES COMPUT SC, V11213, P474, DOI 10.1007/978-3-030-01240-3_29
   Tarvainen Antti, 2017, ADV NEURAL INFORM PR, P2, DOI DOI 10.1137/0330046
   Tomasev N, 2014, IEEE T KNOWL DATA EN, V26, P739, DOI 10.1109/TKDE.2013.25
   Tsai YHH, 2017, IEEE I CONF COMP VIS, P3591, DOI 10.1109/ICCV.2017.386
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Verma VK, 2018, PROC CVPR IEEE, P4281, DOI 10.1109/CVPR.2018.00450
   Verma VK, 2017, LECT NOTES ARTIF INT, V10535, P792, DOI 10.1007/978-3-319-71246-8_48
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wan ZY, 2019, ADV NEUR IN, V32
   Wang W, 2019, ACM T INTEL SYST TEC, V10, DOI 10.1145/3293318
   Xian YQ, 2019, IEEE T PATTERN ANAL, V41, P2251, DOI 10.1109/TPAMI.2018.2857768
   Yan HL, 2020, IEEE T MULTIMEDIA, V22, P2420, DOI 10.1109/TMM.2019.2953375
   Ye M, 2019, PROC CVPR IEEE, P11720, DOI 10.1109/CVPR.2019.01200
   Zhang L, 2020, IEEE T CIRC SYST VID, V30, P2843, DOI 10.1109/TCSVT.2020.2984666
   Zhang L, 2017, PROC CVPR IEEE, P3010, DOI 10.1109/CVPR.2017.321
   Zhang XX, 2020, IEEE T MULTIMEDIA, V22, P1692, DOI 10.1109/TMM.2019.2959433
   Zhu PK, 2019, PROC CVPR IEEE, P2990, DOI 10.1109/CVPR.2019.00311
NR 43
TC 3
Z9 3
U1 5
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1933
EP 1942
DI 10.1109/TMM.2021.3074252
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0H8DA
UT WOS:000778959200012
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhang, Y
   Liu, MY
   He, JW
   Pan, F
   Guo, YW
AF Zhang, Yang
   Liu, Moyun
   He, Jingwu
   Pan, Fei
   Guo, Yanwen
TI Affinity Fusion Graph-Based Framework for Natural Image Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image segmentation; Kernel; Image color analysis; Tensors; Task
   analysis; Sparse matrices; Linearity; Affinity fusion graph; kernel
   spectral clustering; natural image segmentation; sparse subspace
   clustering; subspace pursuit
ID MEAN-SHIFT; REGION
AB This paper proposes an affinity fusion graph framework to effectively connect different graphs with highly discriminating power and nonlinearity for natural image segmentation. The proposed framework combines adjacency-graphs and kernel spectral clustering based graphs (KSC-graphs) according to a new definition named affinity nodes of multi-scale superpixels. These affinity nodes are selected based on a better affiliation of superpixels, namely subspace-preserving representation which is generated by sparse subspace clustering based on subspace pursuit. Then a KSC-graph is built via a novel kernel spectral clustering to explore the nonlinear relationships among these affinity nodes. Moreover, an adjacency-graph at each scale is constructed, which is further used to update the proposed KSC-graph at affinity nodes. The fusion graph is built across different scales, and it is partitioned to obtain final segmentation result. Experimental results on the Berkeley segmentation dataset and Microsoft Research Cambridge dataset show the superiority of our framework in comparison with the state-of-the-art methods. The code is available at https://github.com/Yangzhangcst/AF-graph.
C1 [Zhang, Yang; He, Jingwu; Pan, Fei; Guo, Yanwen] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Peoples R China.
   [Liu, Moyun] Huazhong Univ Sci & Technol, Sch Mech Sci & Engn, Wuhan 430074, Peoples R China.
C3 Nanjing University; Huazhong University of Science & Technology
RP Guo, YW (corresponding author), Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Peoples R China.
EM yzhangcst@smail.nju.edu.cn; lmomoy8@gmail.com; hejw005@gmail.com;
   felix.panf@outlook.com; ywguo@nju.edu.cn
RI Zhang, Yang/KIG-5384-2024
OI Zhang, Yang/0000-0002-4170-4798; He, Jingwu/0000-0002-2651-2317; moyun,
   liu/0000-0002-4530-2606
FU National Natural Science Foundation of China [62032011, 61772257,
   61672279]; Fundamental Research Funds for the Central Universities
   [020214380058]; program B for Outstanding Ph.D. candidate of Nanjing
   University [202001B054]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62032011, 61772257, and 61672279, in
   part by the Fundamental Research Funds for the Central Universities
   (020214380058), and in part by the program B for Outstanding Ph.D.
   candidate of Nanjing University (202001B054). The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Xavier Giro-i-Nieto.
CR Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Bai X, 2010, IEEE T PATTERN ANAL, V32, P861, DOI 10.1109/TPAMI.2009.85
   Cho H, 2017, IEEE T CIRC SYST VID, V27, P2132, DOI 10.1109/TCSVT.2016.2576918
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Cour T, 2005, PROC CVPR IEEE, P1124
   Dai W, 2009, IEEE T INFORM THEORY, V55, P2230, DOI 10.1109/TIT.2009.2016006
   Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57
   Fang CW, 2019, IEEE T PATTERN ANAL, V41, P1470, DOI 10.1109/TPAMI.2018.2839733
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Freixenet J, 2002, LECT NOTES COMPUT SC, V2352, P408, DOI 10.1007/3-540-47977-5_27
   Fu X, 2015, IEEE I CONF COMP VIS, P1618, DOI 10.1109/ICCV.2015.189
   Hettiarachchi R, 2017, PATTERN RECOGN, V65, P119, DOI 10.1016/j.patcog.2016.12.011
   Kang Z, 2017, AAAI CONF ARTIF INTE, P2080
   Kim S, 2014, IEEE T PATTERN ANAL, V36, P1761, DOI 10.1109/TPAMI.2014.2303095
   Kim S, 2013, IEEE T IMAGE PROCESS, V22, P488, DOI 10.1109/TIP.2012.2218822
   Kim TH, 2013, IEEE T PATTERN ANAL, V35, P1690, DOI 10.1109/TPAMI.2012.237
   Lei T, 2020, IEEE T FUZZY SYST, V28, P2078, DOI 10.1109/TFUZZ.2019.2930030
   Lei T, 2019, IEEE T IMAGE PROCESS, V28, P5510, DOI 10.1109/TIP.2019.2920514
   Lei T, 2019, IEEE T FUZZY SYST, V27, P1753, DOI 10.1109/TFUZZ.2018.2889018
   Li KQ, 2018, PATTERN RECOGN, V76, P69, DOI 10.1016/j.patcog.2017.10.023
   Li X, 2016, MULTIMED TOOLS APPL, V75, P2969, DOI 10.1007/s11042-014-2416-1
   Li ZG, 2012, PROC CVPR IEEE, P789, DOI [10.1109/CVPR.2012.6247750, 10.1109/ISRA.2012.6219309]
   Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Meila M, 2007, J MULTIVARIATE ANAL, V98, P873, DOI 10.1016/j.jmva.2006.11.013
   Peng X, 2017, IEEE T CYBERNETICS, V47, P1053, DOI 10.1109/TCYB.2016.2536752
   Pereyra M, 2017, IEEE T IMAGE PROCESS, V26, P2577, DOI 10.1109/TIP.2017.2675165
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Shotton J, 2006, LECT NOTES COMPUT SC, V3951, P1
   Tighe J, 2010, LECT NOTES COMPUT SC, V6315, P352, DOI 10.1007/978-3-642-15555-0_26
   Unnikrishnan R, 2007, IEEE T PATTERN ANAL, V29, P929, DOI 10.1109/TPAMI.2007.1046
   Wang T, 2019, IEEE T IMAGE PROCESS, V28, P330, DOI 10.1109/TIP.2018.2867941
   Wang XF, 2013, IEEE IMAGE PROC, P4019
   Wang XF, 2015, IEEE T IMAGE PROCESS, V24, P1399, DOI 10.1109/TIP.2015.2397313
   Wu JY, 2014, 2014 19TH IEEE-NPSS REAL TIME CONFERENCE (RT), DOI 10.1109/RTC.2014.7097534
   Yang XW, 2013, IEEE T PATTERN ANAL, V35, P28, DOI 10.1109/TPAMI.2012.60
   Yin SB, 2017, PATTERN RECOGN, V68, P245, DOI 10.1016/j.patcog.2017.03.012
   You C, 2016, PROC CVPR IEEE, P3918, DOI 10.1109/CVPR.2016.425
   Zhang CS, 2010, NEUROCOMPUTING, V73, P959, DOI 10.1016/j.neucom.2009.08.014
   Zhang Y, 2019, IEEE INT CON MULTI, P802, DOI 10.1109/ICME.2019.00143
NR 40
TC 13
Z9 13
U1 2
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 440
EP 450
DI 10.1109/TMM.2021.3053393
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YK9LD
UT WOS:000745524300034
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhu, T
   Zhang, DX
   Hu, Y
   Wang, TR
   Jiang, XL
   Zhu, JK
   Li, JW
AF Zhu, Tun
   Zhang, Daoxin
   Hu, Yao
   Wang, Tianran
   Jiang, Xiaolong
   Zhu, Jianke
   Li, Jiawei
TI Horizontal-to-Vertical Video Conversion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Object detection; Task analysis; Faces; Visualization; Streaming media;
   Feature extraction; TV; Video conversion; subject selection; primary
   subject dataset
ID ATTENTION
AB At this blooming age of social media and mobile platform, mass consumers are migrating from horizontal video to vertical contents delivered on hand-held devices. Accordingly, revitalizing the exposure of horizontal video becomes vital and urgent, which is hereby tackled by our automated horizontal-to-vertical (abbreviated as H2V) video conversion framework. Essentially, the H2V framework performs subject-preserving video cropping instantiated in the proposed Rank-SS module. Rank-SS incorporates object detection to discover the candidate subjects, from which we select the primary subject-to-preserve leveraging location, appearance, and salient cues in a convolutional neural network. In addition to converting horizontal videos vertically by cropping around the selected subject, automatic shot detection and multi-object tracking are integrated into the H2V framework to accommodate long and complex videos. To develop H2V systems, we collect an H2V-142 K dataset containing 125 videos (132 K frames) and 9500 cover images annotated with primary subject bounding boxes. On H2V-142 K and public object detection datasets, our method demonstrates promising results on the subject selection comparing to the related solutions. Furthermore, our H2V framework is industrially deployed hosting millions of daily active users and exhibits favorable H2V conversion performance. By making this dataset as well as our approach publicly available, we wish to pave the way for more horizontal-to-vertical video conversion research. Our collected H2V-142 K dataset is available at https://tianchi.aliyun.com/dataset/dataDetail?dataId=93339.
C1 [Zhu, Tun; Zhu, Jianke] Zhejiang Univ, Coll Comp Sci, Hangzhou 310027, Peoples R China.
   [Zhang, Daoxin; Hu, Yao; Wang, Tianran; Jiang, Xiaolong; Li, Jiawei] Youku Cognit & Intelligent Lab Co Alibaba, Hangzhou 311121, Peoples R China.
   [Zhu, Jianke] Alibaba Zhejiang Univ, Joint Res Inst Frontier Technol, Hangzhou, Peoples R China.
   [Zhu, Jianke] Zhejiang Univ, Zhejiang Prov Key Lab Serv Robot, Coll Comp Sci, Hangzhou 310027, Peoples R China.
C3 Zhejiang University; Zhejiang University
RP Zhu, JK (corresponding author), Zhejiang Univ, Coll Comp Sci, Hangzhou 310027, Peoples R China.; Zhu, JK (corresponding author), Alibaba Zhejiang Univ, Joint Res Inst Frontier Technol, Hangzhou, Peoples R China.; Zhu, JK (corresponding author), Zhejiang Univ, Zhejiang Prov Key Lab Serv Robot, Coll Comp Sci, Hangzhou 310027, Peoples R China.
EM ianzhu@zju.edu.cn; daoxin.zdx@alibaba-inc.com; yaoohu@alibaba-inc.com;
   steve.wtr@alibaba-inc.com; xainglu.jxl@alibaba-inc.com;
   jkzhu@zju.edu.cn; mingong.ljw@alibaba-inc.com
RI jiang, xiaolong/KJM-3457-2024; Hu, Yao/KEH-3649-2024; Li,
   Jiawei/GXM-4151-2022; li, jiawei/HOA-5023-2023; Li, Jiaxi/HTS-3430-2023
OI Li, Jiaxi/0000-0002-8197-8590; Wang, Tianran/0000-0002-7950-9699; Zhu,
   Jianke/0000-0003-1831-0106
FU National Natural Science Foundation of China [61831015]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61831015.
CR Achanta R, 2008, LECT NOTES COMPUT SC, V5008, P66
   [Anonymous], 1970, ACCOMMODATION COMPUT
   [Anonymous], IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2010.70
   [Anonymous], 2001, FUNDAMENTALS KALMAN
   Borji A, 2021, IEEE T PATTERN ANAL, V43, P679, DOI 10.1109/TPAMI.2019.2935715
   Burges C., 2005, ICML, P89
   Chen JS, 2016, PROC CVPR IEEE, P507, DOI 10.1109/CVPR.2016.61
   Cheng JC, 2017, IEEE I CONF COMP VIS, P686, DOI 10.1109/ICCV.2017.81
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Deselaers T, 2008, PROC CVPR IEEE, P3017
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Fang C, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1105, DOI 10.1145/2647868.2654979
   Frey N., 2020, AUTOFLIP OPEN SOURCE
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gorji S, 2018, PROC CVPR IEEE, P7501, DOI 10.1109/CVPR.2018.00783
   Grundmann M, 2010, PROC CVPR IEEE, P2141, DOI 10.1109/CVPR.2010.5539893
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Kaiming, 2017, P IEEE INT C COMPUTE
   He S., 2019, P IEEE CVF C COMP VI, P10206, DOI [10.1109/cvpr.2019.01045, DOI 10.1109/CVPR.2019.01045]
   Hosu V, 2019, PROC CVPR IEEE, P9367, DOI 10.1109/CVPR.2019.00960
   Islam MA, 2018, PROC CVPR IEEE, P7142, DOI 10.1109/CVPR.2018.00746
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jain SD, 2017, PROC CVPR IEEE, P2117, DOI 10.1109/CVPR.2017.228
   Jiang B, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1375, DOI 10.1145/3343031.3350860
   Jiang L, 2018, LECT NOTES COMPUT SC, V11218, P625, DOI 10.1007/978-3-030-01264-9_37
   Jiang M, 2015, PROC CVPR IEEE, P1072, DOI 10.1109/CVPR.2015.7298710
   Koh YJ, 2017, PROC CVPR IEEE, P7417, DOI 10.1109/CVPR.2017.784
   Kong S, 2016, LECT NOTES COMPUT SC, V9905, P662, DOI 10.1007/978-3-319-46448-0_40
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Leifman G, 2017, IEEE I CONF COMP VIS, P1707, DOI 10.1109/ICCV.2017.188
   Li B, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1392, DOI 10.1145/3343031.3351016
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li GB, 2018, PROC CVPR IEEE, P3243, DOI 10.1109/CVPR.2018.00342
   Li GB, 2017, PROC CVPR IEEE, P247, DOI 10.1109/CVPR.2017.34
   Li J, 2019, PROC CVPR IEEE, P5055, DOI 10.1109/CVPR.2019.00520
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Lin M, 2014, 2014 INTERNATIONAL CONFERENCE ON MEDICAL BIOMETRICS (ICMB 2014), P1, DOI 10.1109/ICMB.2014.8
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu D, 2016, MULTIMED TOOLS APPL, V75, P12465, DOI 10.1007/s11042-014-2304-8
   Liu K, 2018, AAAI CONF ARTIF INTE, P7138
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Liu Y, 2019, IEEE I CONF COMP VIS, P1232, DOI 10.1109/ICCV.2019.00132
   Lokoc J, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1777, DOI 10.1145/3343031.3351046
   Lu P, 2019, SIGNAL PROCESS-IMAGE, V77, P1, DOI 10.1016/j.image.2019.05.010
   Ma Y.F., 2003, P 11 ACM INT C MULT, P374, DOI DOI 10.1145/957013.957094
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Ochs P, 2011, IEEE I CONF COMP VIS, P1583, DOI 10.1109/ICCV.2011.6126418
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Qi Q, 2019, IEEE INT CON MULTI, P1762, DOI 10.1109/ICME.2019.00303
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rubinstein M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866186
   Schlag J. F., 1983, Tech. Rep. CMU-RI-TR-83-14
   Siris A., 2020, P IEEE CVF C COMP VI, P12133
   Song HM, 2018, LECT NOTES COMPUT SC, V11215, P744, DOI 10.1007/978-3-030-01252-6_44
   Sun J, 2013, INT J COMPUT VISION, V104, P135, DOI 10.1007/s11263-013-0618-z
   Tokmakov P, 2017, IEEE I CONF COMP VIS, P4491, DOI 10.1109/ICCV.2017.480
   Tu Y, 2020, AAAI CONF ARTIF INTE, V34, P12104
   Vaquero D, 2010, PROC SPIE, V7798, DOI 10.1117/12.862419
   Wang Q, 2019, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2019.00142
   Wang WG, 2022, IEEE T PATTERN ANAL, V44, P3239, DOI 10.1109/TPAMI.2021.3051099
   Wang WG, 2021, IEEE T PATTERN ANAL, V43, P220, DOI 10.1109/TPAMI.2019.2924417
   Wang WG, 2019, IEEE T PATTERN ANAL, V41, P1531, DOI 10.1109/TPAMI.2018.2840724
   Wang WG, 2017, IEEE T VIS COMPUT GR, V23, P2014, DOI 10.1109/TVCG.2016.2600594
   Wang Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P873, DOI 10.1145/3343031.3350882
   Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403
   Xiao FY, 2016, PROC CVPR IEEE, P933, DOI 10.1109/CVPR.2016.107
   Yan JZ, 2013, PROC CVPR IEEE, P971, DOI 10.1109/CVPR.2013.130
   Yang Z, 2019, IEEE I CONF COMP VIS, P931, DOI 10.1109/ICCV.2019.00102
   Zarchan Paul., 2005, Fundamentals of Kalman Filtering: A Practical Approach, V2nd
   Zeng H, 2022, IEEE T PATTERN ANAL, V44, P1304, DOI 10.1109/TPAMI.2020.3024207
   Zeng H, 2019, PROC CVPR IEEE, P5942, DOI 10.1109/CVPR.2019.00610
   Zhang JM, 2016, PROC CVPR IEEE, P5733, DOI 10.1109/CVPR.2016.618
   Zhang P, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P512, DOI 10.1145/3343031.3351089
   Zhang XS, 2019, ADV NEUR IN, V32
   Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731
NR 75
TC 1
Z9 1
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 3036
EP 3048
DI 10.1109/TMM.2021.3092202
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2A3LX
UT WOS:000809408000028
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Huang, W
   Zhang, SY
   Zhang, P
   Zha, YF
   Fang, YM
   Zhang, YN
AF Huang, Wei
   Zhang, Siyuan
   Zhang, Peng
   Zha, Yufei
   Fang, Yuming
   Zhang, Yanning
TI Identity-Aware Facial Expression Recognition Via Deep Metric Learning
   Based on Synthesized Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Measurement; Generative adversarial networks; Face
   recognition; Feature extraction; Image synthesis; Image recognition;
   Deep learning; facial expression recognition; image synthesis;
   person-dependent; metric learning
ID PATTERN; FACE
AB Person-dependent facial expression recognition has received considerable research attention in recent years. Unfortunately, different identities can adversely influence recognition accuracy, and the recognition task becomes challenging. Other adverse factors, including limited training data and improper measures of facial expressions, can further contribute to the above dilemma. To solve these problems, a novel identity-aware method is proposed in this study. Furthermore, this study also represents the first attempt to fulfill the challenging person-dependent facial expression recognition task based on deep metric learning and facial image synthesis techniques. Technically, a StarGAN is incorporated to synthesize facial images depicting different but complete basic emotions for each identity to augment the training data. Then, a deep-convolutional-neural-network-based network is employed to automatically extract latent features from both real facial images and all synthesized facial images. Next, a Mahalanobis metric network trained based on extracted latent features outputs a learned metric that measures facial expression differences between images, and the recognition task can thus be realized. Extensive experiments based on several well-known publicly available datasets are carried out in this study for performance evaluations. Person-dependent datasets, including CK+, Oulu (all 6 subdatasets), MMI, ISAFE, ISED, etc., are all incorporated. After comparing the new method with several popular or state-of-the-art facial expression recognition methods, its superiority in person-dependent facial expression recognition can be proposed from a statistical point of view.
C1 [Huang, Wei; Zhang, Siyuan] Nanchang Univ, China Mobile NCU AI&IOT Jointed Lab, Informatizat Off, Nanchang 330022, Jiangxi, Peoples R China.
   [Huang, Wei; Zhang, Siyuan] Nanchang Univ, Dept Comp Sci, Sch Informat Engn, Nanchang 330022, Jiangxi, Peoples R China.
   [Zhang, Peng; Zha, Yufei; Zhang, Yanning] Northwestern Polytech Univ, Sch Comp Sci, Xian 710072, Peoples R China.
   [Fang, Yuming] Jiangxi Univ Finance & Econ, Sch Informat Technol, Nanchang 330013, Jiangxi, Peoples R China.
C3 Nanchang University; Nanchang University; Northwestern Polytechnical
   University; Jiangxi University of Finance & Economics
RP Zhang, P (corresponding author), Northwestern Polytech Univ, Sch Comp Sci, Xian 710072, Peoples R China.; Fang, YM (corresponding author), Jiangxi Univ Finance & Econ, Sch Informat Technol, Nanchang 330013, Jiangxi, Peoples R China.
EM n060101@e.ntu.edu.sg; 61224170@qq.com; zh0036ng@nwpu.edu.cn;
   yufeizha@nwpu.edu.cn; leo.fangyuming@foxmail.com; ynzhang@nwpu.edu.cn
OI Huang, Wei/0000-0002-0541-8612
FU National Natural Science Foundation of China [61862043, 61971352];
   Natural Science Foundation of Jiangxi Province [20204BCJ22011]; Natural
   Science Foundation of Shaanxi Province [2018JM6015]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61862043 and 61971352, in part by the
   Natural Science Foundation of Jiangxi Province under Grant
   20204BCJ22011, and in part by the Natural Science Foundation of Shaanxi
   Province under Grant 2018JM6015.
CR Corneanu CA, 2016, IEEE T PATTERN ANAL, V38, P1548, DOI 10.1109/TPAMI.2016.2515606
   Agarwal S, 2019, IEEE T MULTIMEDIA, V21, P902, DOI 10.1109/TMM.2018.2871417
   [Anonymous], FER 2013 DATASET
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Ben Tanfous A, 2020, IEEE T PATTERN ANAL, V42, P2594, DOI 10.1109/TPAMI.2019.2932979
   Benitez-Quiroz CF, 2016, PROC CVPR IEEE, P5562, DOI 10.1109/CVPR.2016.600
   Bin Iqbal MT, 2020, IEEE T AFFECT COMPUT, V11, P125, DOI 10.1109/TAFFC.2018.2829707
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Dapogny A, 2018, INT J COMPUT VISION, V126, P255, DOI 10.1007/s11263-017-1010-1
   Dapogny A, 2015, IEEE I CONF COMP VIS, P3783, DOI 10.1109/ICCV.2015.431
   Derkach D, 2017, IEEE INT CONF AUTOMA, P41, DOI 10.1109/FG.2017.143
   Ding H, 2017, IEEE INT CONF AUTOMA, P118, DOI 10.1109/FG.2017.23
   EKMAN P, 1971, J PERS SOC PSYCHOL, V17, P124, DOI 10.1037/h0030377
   Fan YR, 2022, IEEE T AFFECT COMPUT, V13, P1057, DOI 10.1109/TAFFC.2020.2988264
   Fu YJ, 2020, IEEE T IMAGE PROCESS, V29, P6535, DOI 10.1109/TIP.2020.2991510
   Goodfellow Ian J., 2013, Neural Information Processing. 20th International Conference, ICONIP 2013. Proceedings: LNCS 8228, P117, DOI 10.1007/978-3-642-42051-1_16
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Happy SL, 2017, IEEE T AFFECT COMPUT, V8, P131, DOI 10.1109/TAFFC.2015.2498174
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu GS, 2018, LECT NOTES COMPUT SC, V11216, P106, DOI 10.1007/978-3-030-01258-8_7
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Gulrajani I, 2017, ADV NEUR IN, V30
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jabid T, 2010, ETRI J, V32, P784, DOI 10.4218/etrij.10.1510.0132
   Jung H, 2015, IEEE I CONF COMP VIS, P2983, DOI 10.1109/ICCV.2015.341
   Kacem A, 2017, IEEE I CONF COMP VIS, P3199, DOI 10.1109/ICCV.2017.345
   Kanade T., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P46, DOI 10.1109/AFGR.2000.840611
   Kollias Dimitrios, 2018, ARXIV181107770
   Kumawat S., 2019, ARXIV190407647
   Lee SH, 2016, IEEE T AFFECT COMPUT, V7, P389, DOI 10.1109/TAFFC.2015.2496320
   Lucey P., 2010, IEEE COMP SOC C COMP, P94, DOI 10.1109/CVPRW.2010.5543262
   Mandal M, 2019, IET IMAGE PROCESS, V13, P850, DOI 10.1049/iet-ipr.2018.5683
   Meng ZB, 2017, IEEE INT CONF AUTOMA, P558, DOI 10.1109/FG.2017.140
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Odena Augustus., 2016, Semi-supervised learning with generative adversarial networks
   Pantic M, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P317, DOI 10.1109/ICME.2005.1521424
   Pantic M, 2000, IEEE T PATTERN ANAL, V22, P1424, DOI 10.1109/34.895976
   Peng GZ, 2018, PROC CVPR IEEE, P2188, DOI 10.1109/CVPR.2018.00233
   Rivera AR, 2013, IEEE T IMAGE PROCESS, V22, P1740, DOI 10.1109/TIP.2012.2235848
   Rivera AR, 2015, PATTERN RECOGN LETT, V51, P94, DOI 10.1016/j.patrec.2014.08.012
   Ryu B, 2017, IEEE T IMAGE PROCESS, V26, P6006, DOI 10.1109/TIP.2017.2726010
   Sariyanidi E, 2017, IEEE T IMAGE PROCESS, V26, P1708, DOI 10.1109/TIP.2016.2639448
   Sariyanidi E, 2015, IEEE T PATTERN ANAL, V37, P1113, DOI 10.1109/TPAMI.2014.2366127
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shan CF, 2009, IMAGE VISION COMPUT, V27, P803, DOI 10.1016/j.imavis.2008.08.005
   Shojaeilangari S, 2015, IEEE T IMAGE PROCESS, V24, P2140, DOI 10.1109/TIP.2015.2416634
   Singh S., 2020, Advances in Signal Processing and Intelligent Recognition Systems, P150
   Soleymani M, 2016, IEEE T AFFECT COMPUT, V7, P17, DOI 10.1109/TAFFC.2015.2436926
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tang Y., 2020, IEEE T IMAGE PROCESS, V30
   Verma Monu, 2019, IEEE Letters of the Computer Society, V2, P36, DOI 10.1109/LOCS.2019.2927959
   Wang J, 2014, PROC CVPR IEEE, P1386, DOI 10.1109/CVPR.2014.180
   Xie SY, 2019, IEEE T MULTIMEDIA, V21, P211, DOI 10.1109/TMM.2018.2844085
   Xie WC, 2017, IEEE T MULTIMEDIA, V19, P279, DOI 10.1109/TMM.2016.2614429
   Yan Y, 2020, IEEE T MULTIMEDIA, V22, P2792, DOI 10.1109/TMM.2019.2962317
   Yang HY, 2018, PROC CVPR IEEE, P2168, DOI 10.1109/CVPR.2018.00231
   Yang HY, 2018, IEEE INT CONF AUTOMA, P294, DOI 10.1109/FG.2018.00050
   Zhang FF, 2018, PROC CVPR IEEE, P3359, DOI 10.1109/CVPR.2018.00354
   Zhang T, 2016, IEEE T MULTIMEDIA, V18, P2528, DOI 10.1109/TMM.2016.2598092
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360
   Zhang ZP, 2018, INT J COMPUT VISION, V126, P550, DOI 10.1007/s11263-017-1055-1
   Zhao GY, 2011, IMAGE VISION COMPUT, V29, P607, DOI 10.1016/j.imavis.2011.07.002
   Zhao XY, 2016, LECT NOTES COMPUT SC, V9906, P425, DOI 10.1007/978-3-319-46475-6_27
   Zhu J.-Y., 2018, ARXIV170310593
NR 64
TC 15
Z9 16
U1 4
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL 9
PY 2021
VL 24
BP 3327
EP 3339
DI 10.1109/TMM.2021.3096068
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2W7NT
UT WOS:000824707800005
DA 2024-07-18
ER

PT J
AU Akbari, M
   Liang, J
   Han, JN
   Tu, CJ
AF Akbari, Mohammad
   Liang, Jie
   Han, Jingning
   Tu, Chengjie
TI Learned Multi-Resolution Variable-Rate Image Compression With
   Octave-Based Residual Blocks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image coding; Decoding; Convolutional codes; Transforms; Codecs; Image
   reconstruction; Linear programming; Deep learning; generalized octave
   convolutions; image compression; residual coding; variable-rate
AB Recently deep learning-based image compression has shown the potential to outperform traditional codecs. However, most existing methods train multiple networks for multiple bit rates, which increase the implementation complexity. In this paper, we propose a new variable-rate image compression framework, which employs generalized octave convolutions (GoConv) and generalized octave transposed-convolutions (GoTConv) with built-in generalized divisive normalization (GDN) and inverse GDN (IGDN) layers. Novel GoConv- and GoTConv-based residual blocks are also developed in the encoder and decoder networks. Our scheme also uses a stochastic rounding-based scalar quantization. To further improve the performance, we encode the residual between the input and the reconstructed image from the decoder network as an enhancement layer. To enable a single model to operate with different bit rates and to learn multi-rate image features, a new objective function is introduced. Experimental results show that the proposed framework trained with variable-rate objective function outperforms the standard codecs such as H.265/HEVC-based BPG and state-of-the-art learning-based variable-rate methods.
C1 [Akbari, Mohammad; Liang, Jie] Simon Fraser Univ, Engn Sci, Burnaby, BC V5A 1S6, Canada.
   [Han, Jingning] Google Inc, WebM Codec Team, Mountain View, CA 94043 USA.
   [Tu, Chengjie] Tencent Technol, Shenzhen 518054, Guangdong, Peoples R China.
C3 Simon Fraser University; Google Incorporated; Tencent
RP Akbari, M (corresponding author), Simon Fraser Univ, Engn Sci, Burnaby, BC V5A 1S6, Canada.
EM ak-bari@sfu.ca; jiel@sfu.ca; jingning@google.com; chengjietu@tencent.com
RI akbari, mohammad/ADF-5801-2022
OI Liang, Jie/0000-0003-3003-4343; Han, Jingning/0000-0001-7168-2254
FU Natural Sciences and Engineering Research Council (NSERC) of Canada
   [RGPIN-2015-06522]
FX This work was supported by the Natural Sciences and Engineering Research
   Council (NSERC) of Canada under Grant RGPIN-2015-06522. The Guest Editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. M. Mrak.
CR Agustsson E, 2017, ADV NEUR IN, V30
   Akbari M., 2020, ASS ADV ARTIF INT
   Akbari M, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102877
   Akbari M, 2019, INT CONF ACOUST SPEE, P2042, DOI [10.1109/icassp.2019.8683541, 10.1109/ICASSP.2019.8683541]
   Balle J, 2017, 5 INT C LEARN REPR I
   Balle J., 2020, IEEE J SEL TOPICS SI
   Balle J., 2018, INT C LEARN REPR ICL, P1
   Ballé J, 2016, PICT COD SYMP, DOI 10.1109/pcs.2016.7906310
   Ballé J, 2018, PICT COD SYMP, P248, DOI 10.1109/PCS.2018.8456272
   Bellard F., 2015, BPG image format
   Bengio Yoshua, 2013, CoRR abs/1308.3432
   Cai CL, 2019, IEEE T CIRC SYST VID, V29, P3687, DOI 10.1109/TCSVT.2018.2880492
   Christopoulos C, 2000, IEEE T CONSUM ELECTR, V46, P1103, DOI 10.1109/30.920468
   Fraunhofer H., 2020, VVC OFFICIAL TEST MO
   Google Inc, 2016, WEBP
   GRAY RM, 1993, IEEE T INFORM THEORY, V39, P805, DOI 10.1109/18.256489
   Gupta S, 2015, PR MACH LEARN RES, V37, P1737
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Johnston N, 2018, PROC CVPR IEEE, P4385, DOI 10.1109/CVPR.2018.00461
   Krishnamoorthi Raghuraman, 2018, ARXIV180608342V1
   Lee J., 2019, P 7 INT C LEARN REP
   Li BL, 2020, IEEE DATA COMPR CONF, P13, DOI 10.1109/DCC47342.2020.00009
   Minnen D, 2018, ADV NEUR IN, V31
   Raiko Tapani, 2015, INT C LEARNING REPRE
   Sneyers J, 2016, IEEE IMAGE PROC, P66, DOI 10.1109/ICIP.2016.7532320
   Theis L., 2017, ICLR
   Toderici G., 2016, P INT C LEARN REPR I
   Toderici G, 2017, PROC CVPR IEEE, P5435, DOI 10.1109/CVPR.2017.577
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Zhang L., 2020, IEEE T PATTERN ANAL
   Zhang ZZ, 2019, IEEE INT CON MULTI, P1438, DOI 10.1109/ICME.2019.00249
   Zhou BL, 2017, PROC CVPR IEEE, P5122, DOI 10.1109/CVPR.2017.544
NR 33
TC 12
Z9 12
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3013
EP 3021
DI 10.1109/TMM.2021.3068523
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UU6IS
UT WOS:000698902000005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Liu, S
   Wang, S
   Liu, XY
   Gandomi, AH
   Daneshmand, M
   Muhammad, K
   De Albuquerque, VHC
AF Liu, Shuai
   Wang, Shuai
   Liu, Xinyu
   Gandomi, Amir H.
   Daneshmand, Mahmoud
   Muhammad, Khan
   De Albuquerque, Victor Hugo C.
TI Human Memory Update Strategy: A Multi-Layer Template Update Mechanism
   for Remote Visual Monitoring
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Monitoring; Visualization; Real-time systems; Artificial intelligence;
   Filtering algorithms; Correlation; Feature extraction; Multimedia
   environment; visual monitoring; filtering algorithm; human memory;
   template update
ID TARGET TRACKING
AB In the era of rapid development of artificial intelligence, the integration of multimedia and human-artificial intelligence has become an important research hotspot. Especially in the multimedia environment, effective remote visual monitoring has become the exploration direction of many scholars. The use of traditional correlation filtering (CF) algorithm for real-time monitoring in the context of multimedia is a practical strategy. However, most existing filtering-based visual monitoring algorithms still have the problem of insufficient robustness and effectiveness. Therefore, by considering the strategy of updating human memory, this paper proposes a multi-layer template update mechanism to achieve effective monitoring in a multimedia environment. In this strategy, the weighted template of the high-confidence matching memory is used as the confidence memory, and the unweighted template of the low-confidence matching memory is used as the cognitive memory. Through the alternate use of confidence memory, matching memory, and cognitive memory, it is ensured that the target will not be lost during the monitoring process. Experimental results show that this strategy does not affect the speed (still real-time) and improves the robustness in the multimedia background.
C1 [Liu, Shuai; Wang, Shuai; Liu, Xinyu] Hunan Normal Univ, Hunan Prov Key Lab Intelligent Comp & Language In, Coll Informat Sci & Engn, Hunan Xiangjiang Artificial Intelligence Acad, Changsha 410000, Peoples R China.
   [Gandomi, Amir H.] Univ Technol Sydney, Fac Engn & Informat Technol, Ultimo, NSW 2007, Australia.
   [Daneshmand, Mahmoud] Stevens Inst Oftechnol, Hoboken, NJ 07030 USA.
   [Muhammad, Khan] Sejong Univ, Dept Software, Seoul 143747, South Korea.
   [Muhammad, Khan] Sungkyunkwan Univ, Sch Convergence, Coll Comp & Informat, Visual Analyt Knowledge Lab VIS2KNOW Lab, Seoul 03063, South Korea.
   [De Albuquerque, Victor Hugo C.] Univ Fed Ceara, Grad Program Teleinformat Engn, Fortaleza, Ceara, Brazil.
   [De Albuquerque, Victor Hugo C.] Fed Inst Educ Sci & Technol Ceara, Grad Program Telecommun Engn, Fortaleza, Ceara, Brazil.
C3 Hunan Normal University; University of Technology Sydney; Sejong
   University; Sungkyunkwan University (SKKU); Universidade Federal do
   Ceara; Instituto Federal do Ceara (IFCE)
RP Muhammad, K (corresponding author), Sungkyunkwan Univ, Sch Convergence, Coll Comp & Informat, Visual Analyt Knowledge Lab VIS2KNOW Lab, Seoul 03063, South Korea.
EM liushuai@hunnu.edu.cn; askshuai@gmail.com; askliu1995@gmail.com;
   gandomi@uts.edu.au; mdaneshm@stevens.edu; khan.muhammad@ieee.org;
   victor.albuquerque@ieee.org
RI Gandomi, Amir H/J-7595-2013; Khan, Muhammad/IXN-8470-2023; Liu,
   Shuai/P-3939-2017; Liu, Shuai/AAX-1239-2021; Muhammad, Khan/L-9059-2016;
   de Albuquerque, Victor Hugo C./C-3677-2016
OI Gandomi, Amir H/0000-0002-2798-0104; Liu, Shuai/0000-0001-9909-0664;
   Liu, Shuai/0000-0001-9909-0664; Muhammad, Khan/0000-0003-4055-7412; de
   Albuquerque, Victor Hugo C./0000-0003-3886-4309; Muhammad,
   Khan/0000-0002-5302-1150
FU Natural Science Foundation of Hunan Province [2020JJ4434]; Key
   Scientific Research Projects of Department of Education of Hunan
   Province [19A312]; Hunan Provincial Science AMP; Technology Project
   Foundation [2018TP1018, 2018RS3065]; Innovation and Entrepreneurship
   Training Program of Hunan Xiangjiang Artificial Intelligence Academy;
   Educational Reform Project of Hunan Xiangjiang Artificial Intelligence
   Academy; Open Project Program of the State Key Laboratory of CADAMP;CG,
   Zhejiang University [A1926]
FX This work was supported by the Natural Science Foundation of Hunan
   Province with No. 2020JJ4434, Key Scientific Research Projects of
   Department of Education of Hunan Province with No. 19A312; Hunan
   Provincial Science & Technology Project Foundation 2018TP1018,
   2018RS3065, Innovation and Entrepreneurship Training Program of Hunan
   Xiangjiang Artificial Intelligence Academy, Educational Reform Project
   of Hunan Xiangjiang Artificial Intelligence Academy, Open Project
   Program of the State Key Laboratory of CAD&CG under Grant A1926,
   Zhejiang University.
CR Ahmad S, 2009, IEEE T MULTIMEDIA, V11, P1381, DOI 10.1109/TMM.2009.2030546
   Asad M, 2020, DEF TECHNOL, V16, P1142, DOI 10.1016/j.dt.2019.12.008
   Bal A, 2005, IEEE T INSTRUM MEAS, V54, P1846, DOI 10.1109/TIM.2005.855090
   Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Cheng X, 2012, IEEE T MULTIMEDIA, V14, P1558, DOI 10.1109/TMM.2012.2217735
   Ciaparrone G, 2020, NEUROCOMPUTING, V381, P61, DOI 10.1016/j.neucom.2019.11.023
   Danelljan M, 2017, IEEE T PATTERN ANAL, V39, P1561, DOI 10.1109/TPAMI.2016.2609928
   Danelljan Martin, 2014, P BRIT MACH VIS C 20
   Deng L, 2018, IEEE SIGNAL PROC MAG, V35, P180, DOI 10.1109/MSP.2017.2762725
   Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552
   Frintrop S, 2009, IEEE INT CONF ROBOT, P758
   Guo W, 2010, LECT NOTES COMPUT SC, V6298, P462, DOI 10.1007/978-3-642-15696-0_43
   Han JW, 2018, PROC CVPR IEEE, P9080, DOI 10.1109/CVPR.2018.00946
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Hernandez J., 2009, P INT WORK C INT NAT, V5602, P125
   Hu HW, 2018, IEEE T NEUR NET LEAR, V29, P1786, DOI 10.1109/TNNLS.2017.2688448
   Huang Z., 2017, P IEEE C COMP VIS PA, P4021, DOI DOI 10.1109/CVPR.2017.510
   Jia Z., 2005, PROC IEEE INT C IMAG, pII
   Kang HB, 2004, LECT NOTES COMPUT SC, V3212, P597
   Khattak S, 2013, IEEE SIGNAL PROC LET, V20, P1126, DOI 10.1109/LSP.2013.2281607
   Kristan M, 2017, IEEE INT CONF COMP V, P1949, DOI 10.1109/ICCVW.2017.230
   Lamberti F, 2011, IEEE T AERO ELEC SYS, V47, P1467, DOI 10.1109/TAES.2011.5751271
   Li DQ, 2016, LECT NOTES ELECTR EN, V386, P531, DOI 10.1007/978-3-662-49831-6_53
   Li Y, 2019, AAAI CONF ARTIF INTE, P8666
   Liu H, 2009, IEEE INTL CONF CONTR, P1176, DOI 10.1109/CCA.2009.5281116
   Liu QY, 2021, PATTERN RECOGN, V112, DOI 10.1016/j.patcog.2020.107766
   Liu S, 2021, IEEE T FUZZY SYST, V29, P90, DOI 10.1109/TFUZZ.2020.3006520
   Lukezic A, 2017, PROC CVPR IEEE, P4847, DOI 10.1109/CVPR.2017.515
   Ma LL, 2010, LECT NOTES COMPUT SC, V6298, P483, DOI 10.1007/978-3-642-15696-0_45
   Roeder M, 2007, IEEE T MULTIMEDIA, V9, P1259, DOI 10.1109/TMM.2007.902872
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Shetty Dasharathraj K., 2019, 2019 International Conference on Automation, Computational and Technology Management (ICACTM). Proceedings, P297, DOI 10.1109/ICACTM.2019.8776763
   Shojaei K, 2017, OCEAN ENG, V133, P244, DOI 10.1016/j.oceaneng.2017.02.007
   Wahlström N, 2015, IEEE T SIGNAL PROCES, V63, P4165, DOI 10.1109/TSP.2015.2424194
   Weinstein BG, 2018, J ANIM ECOL, V87, P533, DOI 10.1111/1365-2656.12780
   Williamson T, 2018, INT J COMPUT ASS RAD, V13, P1605, DOI 10.1007/s11548-018-1780-0
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Xiaoxiao Xu, 2010, Proceedings 2010 Second International Conference on Multimedia Information Networking and Security (MINES 2010), P370, DOI 10.1109/MINES.2010.83
   Yang L, 2018, IEEE T IMAGE PROCESS, V27, P4025, DOI 10.1109/TIP.2018.2834221
   Yang YX, 2017, IEEE T SYST MAN CY-S, V47, P950, DOI 10.1109/TSMC.2016.2523907
NR 41
TC 119
Z9 119
U1 7
U2 60
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2188
EP 2198
DI 10.1109/TMM.2021.3065580
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TS3DH
UT WOS:000679533800002
OA Green Published
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Puteaux, P
   Puech, W
AF Puteaux, Pauline
   Puech, William
TI A Recursive Reversible Data Hiding in Encrypted Images Method With a
   Very High Payload
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image security; image encryption; reversible data hiding; recursive
   process; bit-plane prediction; signal processing in the encrypted domain
AB Reversible data hiding in encrypted images (RDHEI) can be used as an effective technique to embed additional data directly in the encrypted domain and therefore, without any invasion to privacy. In this way, RDHEI is especially useful for labeling encrypted images in cloud storage. In this paper, we propose a new method of data hiding in encrypted images, which is fully reversible and has a very high payload. All the bit-planes of an image are processed recursively, from the most significant one to the least significant by combining error prediction, reversible adaptation, encryption and embedding. For pixel prediction, the Median Edge Detector, also called LOCO-I and known to be efficient in JPEG-LS compression standard, is used for each bit-plane. Moreover, conversely to current state-of-the-art methods, in our proposed method, there is no pre-processing step to correct incorrectly predicted pixels and no flags to highlight them. Indeed, a reversible adaptation of the bit-planes is performed in order to make it possible to detect and correct all incorrectly predicted pixels during the decoding step. Thanks to the high correlation between pixels in the clear domain, a large part of the bits of an image can be substituted by bits of a secret message. Our experiments show that we can generally embed bits of the secret message until the fourth most-significant bit-plane of an image, this allows us to have an average payload value of 2.4586 bpp.
C1 [Puteaux, Pauline; Puech, William] Univ Montpellier, CNRS, LIRMM, F-34392 Montpellier, France.
C3 Centre National de la Recherche Scientifique (CNRS); Universite
   Paul-Valery; Universite Perpignan Via Domitia; Universite de Montpellier
RP Puech, W (corresponding author), Univ Montpellier, CNRS, LIRMM, F-34392 Montpellier, France.
EM pauline.puteaux@lirmm.fr; william.puech@lirmm.fr
OI Puteaux, Pauline/0000-0002-3236-4664; Puech, William/0000-0001-9383-2401
CR Alattar AM, 2004, IEEE T IMAGE PROCESS, V13, P1147, DOI 10.1109/TIP.2004.828418
   Bas P., Image database of BOWS-2.
   Cao XC, 2016, IEEE T CYBERNETICS, V46, P1132, DOI 10.1109/TCYB.2015.2423678
   Chen KM, 2019, J VIS COMMUN IMAGE R, V58, P334, DOI 10.1016/j.jvcir.2018.12.023
   Erkin Z., 2007, EURASIP Journal on Information Security, V7, P1
   Fridrich J, 2001, P SOC PHOTO-OPT INS, V4314, P197, DOI 10.1117/12.435400
   Gao XB, 2011, IEEE T CIRC SYST VID, V21, P1061, DOI 10.1109/TCSVT.2011.2130410
   Hong W, 2012, IEEE SIGNAL PROC LET, V19, P199, DOI 10.1109/LSP.2012.2187334
   Hu YJ, 2008, IEEE T MULTIMEDIA, V10, P1500, DOI 10.1109/TMM.2008.2007341
   Ma KD, 2013, IEEE T INF FOREN SEC, V8, P553, DOI 10.1109/TIFS.2013.2248725
   Memon N, 1997, P SOC PHOTO-OPT INS, V3024, P47, DOI 10.1117/12.263270
   Ni ZC, 2006, IEEE T CIRC SYST VID, V16, P354, DOI 10.1109/TCSVT.2006.869964
   Ou B, 2013, IEEE T IMAGE PROCESS, V22, P5010, DOI 10.1109/TIP.2013.2281422
   Peng F, 2012, SIGNAL PROCESS, V92, P54, DOI 10.1016/j.sigpro.2011.06.006
   Puech, 2018, 2018 IEEE INT WORKSH, P1
   Puech W, 2008, PROC SPIE, V6819, DOI 10.1117/12.766754
   Puteaux P, 2018, IEEE T INF FOREN SEC, V13, P1670, DOI 10.1109/TIFS.2018.2799381
   Puyang Y, 2018, IEEE INT WORKS INFOR
   Qian ZX, 2019, IEEE T CIRC SYST VID, V29, P351, DOI 10.1109/TCSVT.2018.2797897
   Qian ZX, 2014, IEEE T MULTIMEDIA, V16, P1486, DOI 10.1109/TMM.2014.2316154
   Sachnev V, 2009, IEEE T CIRC SYST VID, V19, P989, DOI 10.1109/TCSVT.2009.2020257
   Tai WL, 2009, IEEE T CIRC SYST VID, V19, P904, DOI 10.1109/TCSVT.2009.2017409
   Tian J, 2003, IEEE T CIRC SYST VID, V13, P890, DOI 10.1109/TCSVT.2003.815962
   Tian J., 2002, P WORKSH MULT SEC, V19
   Trappe W., 2006, INTRO CRYPTOGRAPHYWI
   Yi S, 2019, IEEE T MULTIMEDIA, V21, P51, DOI 10.1109/TMM.2018.2844679
   Zhang WM, 2016, IEEE T MULTIMEDIA, V18, P1469, DOI 10.1109/TMM.2016.2569497
   Zhang XP, 2016, IEEE T CIRC SYST VID, V26, P1622, DOI 10.1109/TCSVT.2015.2433194
   Zhang XP, 2013, IEEE T MULTIMEDIA, V15, P316, DOI 10.1109/TMM.2012.2229262
   Zhang XP, 2012, IEEE T INF FOREN SEC, V7, P826, DOI 10.1109/TIFS.2011.2176120
   Zhang XP, 2011, IEEE SIGNAL PROC LET, V18, P255, DOI 10.1109/LSP.2011.2114651
NR 31
TC 64
Z9 67
U1 3
U2 33
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 636
EP 650
DI 10.1109/TMM.2020.2985537
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA QA6NL
UT WOS:000613560200009
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Shi, HC
   Li, HL
   Wu, QB
   Ngan, KN
AF Shi, Hengcan
   Li, Hongliang
   Wu, Qingbo
   Ngan, King Ngi
TI Query Reconstruction Network for Referring Expression Image Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image segmentation; referring expression image segmentation; vision plus
   language; reconstruction
ID SCENE
AB Referring expression image segmentation aims at segmenting out the object described by a natural language query. Due to the diversity of visual content and language descriptions, it is very challenging to accurately model the correspondence between the vision and language, which inevitably produces some undesired segmentation objects from the queries. In this paper, we propose a query reconstruction network (QRN) to build more consistent corresponding relations between the language queries and object segmentation results. QRN not only generates segmentations from the queries and images but also reversely reconstructs the queries from the segmentations and the images. Through query reconstruction, QRN can confirm the vision-language consistency between the segmentations and queries. In the inference stage, for inconsistent segmentations and queries, we propose an iterative segmentation correction (ISC) method to correct them. ISC takes the difference between the reconstructed and input queries as a loss to optimize the proposed QRN. Then, the proposed QRN can generate new segmentations and queries. By iterative optimization, the segmentations can be gradually corrected. Extensive experiments on four referring expression image segmentation databases demonstrate the effectiveness of the proposed method.
C1 [Shi, Hengcan; Li, Hongliang; Wu, Qingbo; Ngan, King Ngi] Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 611731, Peoples R China.
   [Ngan, King Ngi] Chinese Univ Hong Kong, Dept Elect Engn, Hong Kong, Peoples R China.
C3 University of Electronic Science & Technology of China; Chinese
   University of Hong Kong
RP Li, HL (corresponding author), Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 611731, Peoples R China.
EM shihc@std.uestc.edu.cn; hlli@uestc.edu.cn; qbwu@uestc.edu.cn;
   knngan@ee.cuhk.edu.hk
RI Ngan, N/E-8240-2014; Wu, Qingbo/AAF-6872-2019
OI Ngan, N/0000-0003-1946-3235; Wu, Qingbo/0000-0003-2936-6340; Li,
   Hongliang/0000-0002-7481-095X
FU National Natural Science Foundation of China [61831005, 61525102,
   61971095, 61871078]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61831005, 61525102, 61971095, and
   61871078.
CR Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen LC, 2016, PROC CVPR IEEE, P3640, DOI 10.1109/CVPR.2016.396
   Chen Y.-W., 2019, BMVC, P263
   Ding HH, 2018, PROC CVPR IEEE, P2393, DOI 10.1109/CVPR.2018.00254
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Gavrilyuk K, 2018, PROC CVPR IEEE, P5958, DOI 10.1109/CVPR.2018.00624
   He D, 2016, ADV NEUR IN, V29
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hu R., 2016, Utilizing large scale vision and text datasets for image segmentation from referring expressions
   Hu RH, 2016, PROC CVPR IEEE, P4555, DOI 10.1109/CVPR.2016.493
   Hu RH, 2016, LECT NOTES COMPUT SC, V9905, P108, DOI 10.1007/978-3-319-46448-0_7
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Kalkan S., 2019, P BRIT MACHINE VISIO, P272
   Kazemzadeh S., 2014, EMNLP, P787, DOI DOI 10.3115/V1/D14-1086
   Khoreva Anna, 2018, LECT NOTES COMPUT 4, P123, DOI DOI 10.1007/978-3-030-20870-78
   Li LH, 2018, IEEE T MULTIMEDIA, V20, P726, DOI 10.1109/TMM.2017.2751140
   Li RY, 2018, PROC CVPR IEEE, P5745, DOI 10.1109/CVPR.2018.00602
   Li XY, 2019, IEEE T MULTIMEDIA, V21, P2117, DOI 10.1109/TMM.2019.2896516
   Liang XD, 2016, PROC CVPR IEEE, P3185, DOI 10.1109/CVPR.2016.347
   Liang XD, 2016, LECT NOTES COMPUT SC, V9905, P125, DOI 10.1007/978-3-319-46448-0_8
   Lin D, 2017, IEEE I CONF COMP VIS, P1320, DOI 10.1109/ICCV.2017.147
   Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu CX, 2017, IEEE I CONF COMP VIS, P1280, DOI 10.1109/ICCV.2017.143
   Liu JY, 2017, IEEE I CONF COMP VIS, P4866, DOI 10.1109/ICCV.2017.520
   Luo B, 2017, IEEE T MULTIMEDIA, V19, P1482, DOI 10.1109/TMM.2017.2671447
   Luo RT, 2017, PROC CVPR IEEE, P3125, DOI 10.1109/CVPR.2017.333
   Ma L, 2017, IEEE T MULTIMEDIA, V19, P2545, DOI 10.1109/TMM.2017.2703089
   Mao JH, 2016, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2016.9
   Margffoy-Tuay E, 2018, LECT NOTES COMPUT SC, V11215, P656, DOI 10.1007/978-3-030-01252-6_39
   Meng FM, 2018, IEEE T MULTIMEDIA, V20, P310, DOI 10.1109/TMM.2017.2739919
   Nagaraja VK, 2016, LECT NOTES COMPUT SC, V9908, P792, DOI 10.1007/978-3-319-46493-0_48
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178
   Park SJ, 2017, IEEE I CONF COMP VIS, P4990, DOI 10.1109/ICCV.2017.533
   Pedersoli M, 2017, IEEE I CONF COMP VIS, P1251, DOI 10.1109/ICCV.2017.140
   Qiu S, 2020, IEEE T MULTIMEDIA, V22, P1333, DOI 10.1109/TMM.2019.2942480
   Rohrbach A, 2016, LECT NOTES COMPUT SC, V9905, P817, DOI 10.1007/978-3-319-46448-0_49
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683
   Shi HC, 2018, LECT NOTES COMPUT SC, V11210, P38, DOI 10.1007/978-3-030-01231-1_3
   Shi HC, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P492, DOI 10.1145/3240508.3240657
   Shi HC, 2018, IEEE T MULTIMEDIA, V20, P2670, DOI 10.1109/TMM.2018.2812600
   Wang Y, 2019, P IEEE C COMP VIS PA
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yu LC, 2018, PROC CVPR IEEE, P1307, DOI 10.1109/CVPR.2018.00142
   Yu LC, 2017, PROC CVPR IEEE, P3521, DOI 10.1109/CVPR.2017.375
   Yu LC, 2016, LECT NOTES COMPUT SC, V9906, P69, DOI 10.1007/978-3-319-46475-6_5
   Zhang H, 2018, PROC CVPR IEEE, P7151, DOI 10.1109/CVPR.2018.00747
   Zhang HW, 2018, PROC CVPR IEEE, P4158, DOI 10.1109/CVPR.2018.00437
   Zhang R, 2017, IEEE I CONF COMP VIS, P2050, DOI 10.1109/ICCV.2017.224
   Zhang YT, 2017, PROC CVPR IEEE, P1090, DOI 10.1109/CVPR.2017.122
   Zhang ZJ, 2019, IEEE T MULTIMEDIA, V21, P1681, DOI 10.1109/TMM.2018.2888822
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 55
TC 16
Z9 16
U1 2
U2 44
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 995
EP 1007
DI 10.1109/TMM.2020.2991504
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA QO8XK
UT WOS:000623420300013
DA 2024-07-18
ER

PT J
AU Sun, C
   Jia, YD
   Song, H
   Wu, YW
AF Sun, Che
   Jia, Yunde
   Song, Hao
   Wu, Yuwei
TI Adversarial 3D Convolutional Auto-Encoder for Abnormal Event Detection
   in Videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Videos; Event detection; Noise reduction;
   Correlation; Decoding; Generators; Adversarial 3D convolutional
   auto-encoder; normal patterns; adversarial learning; abnormal event
   detection
ID ANOMALY DETECTION; NETWORK
AB Abnormal event detection aims to identify the events that deviate from expected normal patterns. Existing methods usually extract normal spatio-temporal patterns of appearance and motion in a separate manner, which ignores low-level correlations between appearance and motion patterns and may fall short of capturing fine-grained spatio-temporal patterns. In this paper, we propose to simultaneously learn appearance and motion to obtain fine-grained spatio-temporal patterns. To this end, we present an adversarial 3D convolutional auto-encoder to learn the normal spatio-temporal patterns and then identify abnormal events by diverging them from the learned normal patterns in videos. The encoder captures the low-level correlations between spatial and temporal dimensions of videos, and generates distinctive features representing visual spatio-temporal information. The decoder reconstrucccts the original video from the encoded features representing by 3D de-convolutions and learns the normal spatio-temporal patterns in an unsupervised manner. We introduce the denoising reconstruction error and adversarial learning strategy to train the 3D convolutional auto-encoder to implicitly learn accurate data distributions that are considered normal patterns, which benefits enhancing the reconstruction ability of the auto-encoder to discriminate abnormal events. Both the theoretical analysis and the extensive experiments on four publicly available datasets demonstrate the effectiveness of our method.
C1 [Sun, Che; Jia, Yunde; Song, Hao; Wu, Yuwei] Beijing Inst Technol, Sch Comp Sci, Beijing Lab Intelligent Informat Technol, Beijing 10081, Peoples R China.
C3 Beijing Institute of Technology
RP Wu, YW (corresponding author), Beijing Inst Technol, Sch Comp Sci, Beijing Lab Intelligent Informat Technol, Beijing 10081, Peoples R China.
EM sunche@bit.edu.cn; jiayunde@bit.edu.cn; songhao@bit.edu.cn;
   wuyuwei@bit.edu.cn
OI Sun, Che/0000-0002-7555-9146
FU Natural Science Foundation of China [61702037, 61773062]
FX Manuscript received January 6, 2020; revised May 30, 2020 and August 3,
   2020; accepted August 30, 2020. Date of publication September 10, 2020;
   date of current version September 24, 2021. This work was supported by
   the Natural Science Foundation of China under Grants 61702037 and
   61773062. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Jianfei Cai.
   (Corresponding author: Yuwei Wu.)
CR Adam A, 2008, IEEE T PATTERN ANAL, V30, P555, DOI 10.1109/TPAMI.2007.70825
   Alain G, 2014, J MACH LEARN RES, V15, P3563
   Bengio Y., 2007, Advances in neural information processing systems, P153, DOI DOI 10.7551/MITPRESS/7503.003.0024
   Bin Zhao, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3313, DOI 10.1109/CVPR.2011.5995524
   Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS)
   Bouvrie J., 2006, NOTES CONVOLUTIONAL
   Chong YS, 2017, LECT NOTES COMPUT SC, V10262, P189, DOI 10.1007/978-3-319-59081-3_23
   Chu WQ, 2019, IEEE T MULTIMEDIA, V21, P246, DOI 10.1109/TMM.2018.2846411
   Cong Y, 2011, PROC CVPR IEEE, P1807, DOI 10.1109/CVPR.2011.5995434
   Dacorogna B., 2014, Introduction to the Calculus of Variations
   Dutta JK, 2015, AAAI CONF ARTIF INTE, P3755
   Feng YC, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P591, DOI 10.1145/2964284.2967290
   Fuglede B, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY, PROCEEDINGS, P31
   Gong D, 2019, IEEE I CONF COMP VIS, P1705, DOI 10.1109/ICCV.2019.00179
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo DS, 2018, IEEE T MULTIMEDIA, V20, P3428, DOI 10.1109/TMM.2018.2839534
   Hasan M, 2016, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2016.86
   Hou DD, 2019, NEUROCOMPUTING, V330, P369, DOI 10.1016/j.neucom.2018.09.080
   Hou JY, 2018, IEEE T MULTIMEDIA, V20, P1537, DOI 10.1109/TMM.2017.2771462
   Hyunjong Park, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14360, DOI 10.1109/CVPR42600.2020.01438
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Ionescu RT, 2019, PROC CVPR IEEE, P7834, DOI 10.1109/CVPR.2019.00803
   Ionescu RT, 2019, IEEE WINT CONF APPL, P1951, DOI 10.1109/WACV.2019.00212
   Ionescu RT, 2017, IEEE I CONF COMP VIS, P2914, DOI 10.1109/ICCV.2017.315
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiang ZL, 2017, IEEE COMPUT SOC CONF, P309, DOI 10.1109/CVPRW.2017.44
   Kim J, 2009, PROC CVPR IEEE, P2913
   Kozlov Y., 2015, PERSISTENCE1D EXTRAC
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li YJ, 2018, LECT NOTES COMPUT SC, V11213, P609, DOI 10.1007/978-3-030-01240-3_37
   Liu W, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3023
   Liu W, 2018, PROC CVPR IEEE, P6536, DOI 10.1109/CVPR.2018.00684
   Lu CW, 2013, IEEE I CONF COMP VIS, P2720, DOI 10.1109/ICCV.2013.338
   Luo WX, 2017, IEEE I CONF COMP VIS, P341, DOI 10.1109/ICCV.2017.45
   Mahadevan V, 2010, PROC CVPR IEEE, P1975, DOI 10.1109/CVPR.2010.5539872
   Medel J. R., 2016, Anomaly detection in video using predictive convolutional long short-term memory networks
   Mehran R, 2009, PROC CVPR IEEE, P935, DOI 10.1109/CVPRW.2009.5206641
   Morais R, 2019, PROC CVPR IEEE, P11988, DOI 10.1109/CVPR.2019.01227
   Prasad NR, 2009, CMC-COMPUT MATER CON, V14, P1, DOI 10.1145/1541880.1541882
   Ravanbakhsh M, 2017, IEEE IMAGE PROC, P1577, DOI 10.1109/ICIP.2017.8296547
   Sabokrou M, 2019, LECT NOTES COMPUT SC, V11366, P488, DOI 10.1007/978-3-030-20876-9_31
   Sabokrou M, 2018, PROC CVPR IEEE, P3379, DOI 10.1109/CVPR.2018.00356
   Schlegl T, 2017, LECT NOTES COMPUT SC, V10265, P146, DOI 10.1007/978-3-319-59050-9_12
   Song H, 2020, IEEE T MULTIMEDIA, V22, P2138, DOI 10.1109/TMM.2019.2950530
   Srivastava N, 2015, PR MACH LEARN RES, V37, P843
   Sultani W, 2018, PROC CVPR IEEE, P6479, DOI 10.1109/CVPR.2018.00678
   Tang Y, 2020, PATTERN RECOGN LETT, V129, P123, DOI 10.1016/j.patrec.2019.11.024
   Nguyen TN, 2019, IEEE I CONF COMP VIS, P1273, DOI 10.1109/ICCV.2019.00136
   Vincent P., 2008, INT C MACH LEARN ICM, P1096, DOI [10.1145/1390156.1390294, DOI 10.1145/1390156.1390294]
   Wang C, 2017, MULTIMED TOOLS APPL, V76, P6263, DOI 10.1007/s11042-015-3199-8
   Wang SQ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P636, DOI 10.1145/3240508.3240615
   Wang T, 2012, 2012 IEEE NINTH INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL-BASED SURVEILLANCE (AVSS), P13, DOI 10.1109/AVSS.2012.39
   Xu D, 2017, COMPUT VIS IMAGE UND, V156, P117, DOI 10.1016/j.cviu.2016.10.010
   Xu K, 2020, IEEE T MULTIMEDIA, V22, P394, DOI 10.1109/TMM.2019.2929931
   Xu K, 2018, IEEE T MULTIMEDIA, V20, P1062, DOI 10.1109/TMM.2018.2818942
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang D, 2005, PROC CVPR IEEE, P611
   Zhou SF, 2016, SIGNAL PROCESS-IMAGE, V47, P358, DOI 10.1016/j.image.2016.06.007
   Zhu LC, 2017, PROC CVPR IEEE, P1339, DOI 10.1109/CVPR.2017.147
NR 59
TC 16
Z9 16
U1 2
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3292
EP 3305
DI 10.1109/TMM.2020.3023303
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UU6IS
UT WOS:000698902000027
DA 2024-07-18
ER

PT J
AU Wang, PY
   Zhao, ZC
   Su, F
   Zhao, YY
   Wang, HY
   Yang, L
   Li, Y
AF Wang, Pingyu
   Zhao, Zhicheng
   Su, Fei
   Zhao, Yanyun
   Wang, Haiying
   Yang, Lei
   Li, Yang
TI Deep Multi-Patch Matching Network for Visible Thermal Person
   Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Cameras; Feature extraction; Training; Correlation;
   Semantics; Visualization; Multi-Patch Matching; Person
   Re-Identification; Modality Alignment; Correlation Distillation;
   Priority Attention
ID RECOGNITION
AB Visible Thermal Person Re-Identification (VTReID) is a cross-modality retrieval problem in computer vision. Accurate VTReID is very challenging due to large modality discrepancies. In this work, we design a novel Multi-Patch Matching Network (MPMN) framework to simultaneously mitigate the heterogeneity of coarse-grained and fine-grained visual semantics. In view of cross-modality matching, we verify that aligning modality distributions of the original features is likely to suffer from the selective alignment behavior, i.e., only focuses on easiest dimensions or subspaces. Inspired by adversarial learning, we propose a new Multi-Patch Modality Alignment (MPMA) loss to jointly balance and reduce the modality discrepancies of multi-patch features by mining hard subspaces and abandoning easy subspaces. Since multi-patch features are potentially complementary to each other, the semantic correlations between different patches should be exploited during training. Motivated by knowledge distillation, we put forward a new Cross-Patch Correlation Distillation (CPCD) loss to transfer the semantic knowledges across different patches. To balance multi-patch tasks, an effective Patch-Aware Priority Attention (PAPA) method is further introduced to dynamically prioritize hard patch tasks during training. This paper experimentally demonstrates the effectiveness of the proposed methods, achieving superior performance over the state-of-the-art methods on RegDB and SYSU-MM01 datasets.
C1 [Wang, Pingyu; Zhao, Zhicheng; Su, Fei; Zhao, Yanyun; Wang, Haiying] Beijing Univ Posts & Telecommun, Sch Informat & Commun Engn, Beijing Key Lab Network Syst & Network Culture, Beijing 100876, Peoples R China.
   [Yang, Lei; Li, Yang] China Mobile Res Inst, Beijing 100053, Peoples R China.
C3 Beijing University of Posts & Telecommunications; China Mobile
RP Su, F (corresponding author), Beijing Univ Posts & Telecommun, Sch Informat & Commun Engn, Beijing Key Lab Network Syst & Network Culture, Beijing 100876, Peoples R China.
EM applewangpingyu@gmail.com; zhaozc@bupt.edu.cn; sufei@bupt.edu.cn;
   zyy@bupt.edu.cn; why@bupt.edu.cn; yangleiyj@chinamobile.com;
   liyangyjy@chinamobile.com
RI chen, qy/JXM-3217-2024
OI Zhao, Zhicheng/0000-0001-6506-7298; Wang, Pingyu/0000-0001-9769-8035
FU Chinese National Natural Science Foundation [61532018, U1931202];
   MoE-CMCC "Artifical Intelligence" Project [MCM20190701]
FX This work was supported in part by the Chinese National Natural Science
   Foundation under Grants 61532018 and U1931202 and in part by MoE-CMCC
   "Artifical Intelligence" Project No. MCM20190701. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Lei Zhang.
CR Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145
   Chen Z, 2018, PR MACH LEARN RES, V80
   Dai PY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P677
   Dean J., 2015, NIPS DEEP LEARNING R
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding GG, 2016, IEEE T IMAGE PROCESS, V25, P5427, DOI 10.1109/TIP.2016.2607421
   Feng ZX, 2020, IEEE T IMAGE PROCESS, V29, P579, DOI 10.1109/TIP.2019.2928126
   Fu Y, 2019, AAAI CONF ARTIF INTE, P8287
   Fu Y, 2019, AAAI CONF ARTIF INTE, P8295
   Ganin Y., 2015, ICML
   Gong DH, 2017, IEEE T IMAGE PROCESS, V26, P2079, DOI 10.1109/TIP.2017.2651380
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gu SQ, 2017, IEEE INT SYMP ELEC
   Hao Y, 2019, AAAI CONF ARTIF INTE, P8385
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He R, 2019, IEEE T PATTERN ANAL, V41, P1761, DOI 10.1109/TPAMI.2018.2842770
   Hermans Alexander, 2017, ARXIV170307737
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Kang JK, 2019, IEEE ACCESS, V7, P57972, DOI 10.1109/ACCESS.2019.2914670
   Kendall A, 2018, PROC CVPR IEEE, P7482, DOI 10.1109/CVPR.2018.00781
   Li KP, 2019, IEEE I CONF COMP VIS, P4653, DOI 10.1109/ICCV.2019.00475
   Luo H, 2019, PATTERN RECOGN, V94, P53, DOI 10.1016/j.patcog.2019.05.028
   Murrugarra-Llerena N, 2019, PROC CVPR IEEE, P6422, DOI 10.1109/CVPR.2019.00659
   Nguyen DT, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17030605
   Paszke A, 2019, ADV NEUR IN, V32
   Sarafianos N, 2019, IEEE I CONF COMP VIS, P5813, DOI 10.1109/ICCV.2019.00591
   Suh Y, 2018, LECT NOTES COMPUT SC, V11218, P418, DOI 10.1007/978-3-030-01264-9_25
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Tay CP, 2019, PROC CVPR IEEE, P7127, DOI 10.1109/CVPR.2019.00730
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vemulapalli R, 2015, IEEE I CONF COMP VIS, P630, DOI 10.1109/ICCV.2015.79
   Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326
   Wang C, 2018, LECT NOTES COMPUT SC, V11208, P384, DOI 10.1007/978-3-030-01225-0_23
   Wang GA, 2019, IEEE I CONF COMP VIS, P3622, DOI 10.1109/ICCV.2019.00372
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang PY, 2020, PATTERN RECOGN LETT, V133, P195, DOI 10.1016/j.patrec.2020.03.012
   Wang PY, 2019, NEUROCOMPUTING, V363, P35, DOI 10.1016/j.neucom.2019.04.085
   Wang ZX, 2019, PROC CVPR IEEE, P618, DOI 10.1109/CVPR.2019.00071
   Wang ZH, 2019, IEEE I CONF COMP VIS, P5763, DOI 10.1109/ICCV.2019.00586
   Wei LH, 2019, IEEE T MULTIMEDIA, V21, P986, DOI 10.1109/TMM.2018.2870522
   Wu AC, 2017, IEEE I CONF COMP VIS, P5390, DOI 10.1109/ICCV.2017.575
   Wu L, 2019, IEEE T MULTIMEDIA, V21, P1412, DOI 10.1109/TMM.2018.2877886
   Wu X, 2018, AAAI CONF ARTIF INTE, P1679
   Ye M, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1092
   Ye M, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P347, DOI 10.1145/3343031.3351043
   Ye M, 2018, AAAI CONF ARTIF INTE, P7501
   Yu R, 2018, LECT NOTES COMPUT SC, V11220, P196, DOI 10.1007/978-3-030-01270-0_12
   Zhang Y, 2018, LECT NOTES COMPUT SC, V11205, P707, DOI 10.1007/978-3-030-01246-5_42
   Zhao Z, 2017, P IEEE VIS COMM IM P, P1
   Zheng F, 2019, PROC CVPR IEEE, P8506, DOI 10.1109/CVPR.2019.00871
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhou SP, 2018, IEEE T MULTIMEDIA, V20, P593, DOI 10.1109/TMM.2017.2755983
NR 52
TC 25
Z9 26
U1 2
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1474
EP 1488
DI 10.1109/TMM.2020.2999180
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA SJ9EZ
UT WOS:000655830300002
DA 2024-07-18
ER

PT J
AU Wang, YX
   Yang, H
   Bai, XX
   Qian, XM
   Ma, L
   Lu, J
   Li, BA
   Fan, X
AF Wang, Yaxiong
   Yang, Hao
   Bai, Xiuxiu
   Qian, Xueming
   Ma, Lin
   Lu, Jing
   Li, Biao
   Fan, Xin
TI PFAN plus plus : Bi-Directional Image-Text Retrieval With Position
   Focused Attention Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Semantics; Task analysis; Reliability; Postal services;
   Feature extraction; Fans; Image-text matching; attention mechanism;
   cross-domain; position embedding learning
AB Bi-directional image-text retrieval and matching attract much attention recently. This cross-domain task demands a fine understanding of both modalities for learning a measure of different modality data. In this paper, we propose a novel position focused attention network to investigate the relation between the visual and the textual views. This work integrates the prior object position to enhance the visual-text joint-embedding learning. The image is first split into blocks, which are treated as the basic position cells, and the position of an image region is inferred. Then, we propose a position attention to model the relations between the image region and position cells. Finally, we generate a valuable position feature to further enhance the region expression and model a more reliable relationship between the visual image and the textual sentence. Experiments on the popular datasets Flickr30K and MS-COCO show the effectiveness of the proposed method. Besides the public datasets, we also conduct experiments on our collected practical large-scale news dataset (Tencent-News) to validate the practical application value of the proposed method. As far as we know, this is the first attempt to test the performance on the practical application. Our method achieves the competitive performance on all of these three datasets.
C1 [Wang, Yaxiong; Bai, Xiuxiu] Xi An Jiao Tong Univ, Sch Software Engn, Xian 10698, Peoples R China.
   [Wang, Yaxiong; Yang, Hao; Lu, Jing; Li, Biao; Fan, Xin] Tencent, Dept PCG, Shenzhen 518057, Peoples R China.
   [Qian, Xueming] Xi An Jiao Tong Univ, Minist Educ, Key Lab Intelligent Networks & Network Secur, Sch Informat & Commun Engn, Xian 10698, Peoples R China.
   [Qian, Xueming] Xi An Jiao Tong Univ, SMILES LAB, Xian 10698, Peoples R China.
   [Ma, Lin] Meituan Dianping Grp, Beijing 100000, Peoples R China.
C3 Xi'an Jiaotong University; Tencent; Xi'an Jiaotong University; Xi'an
   Jiaotong University
RP Bai, XX (corresponding author), Xi An Jiao Tong Univ, Sch Software Engn, Xian 10698, Peoples R China.; Qian, XM (corresponding author), Xi An Jiao Tong Univ, Minist Educ, Key Lab Intelligent Networks & Network Secur, Sch Informat & Commun Engn, Xian 10698, Peoples R China.; Qian, XM (corresponding author), Xi An Jiao Tong Univ, SMILES LAB, Xian 10698, Peoples R China.
EM wangyx15@stu.xjtu.edu.cn; applehyang@tencent.com; xiubai@xjtu.edu.cn;
   qianxm@mail.xjtu.edu.cn; forest.linma@gmail.com; luckielu@tencent.com;
   biotli@tencent.com; hsinfan@tencent.com
FU NSFC [61772407, 61732008]; National Key Research and Development
   [2019YFB2102500]
FX Manuscript received August 14, 2019; revised June 7, 2020 and September
   6, 2020; accepted September 10, 2020. Date of publication September 18,
   2020; date of current version September 24, 2021. This work was
   supported in part by the NSFC underGrant 61772407 and 61732008,
   andNationalKey Research and Development Project 2019YFB2102500. The
   conference version of this work has been accepted by IJCAI 2019. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Xavier Giro-i-Nieto. (YaxiongWang
   and Hao Yang contributed equally to this work.) (Corresponding authors:
   Xiuxiu Bai; Xueming Qian.)
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Andreas J., 2016, P 2016 C N AM CHAPT, P1545, DOI DOI 10.18653/V1/N16-1181
   Andrew G., 2013, P ICML, P1247
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Chang XB, 2018, PROC CVPR IEEE, P1488, DOI 10.1109/CVPR.2018.00161
   Devlin J., 2018, BERT PRE TRAINING DE
   Eisenschtat A, 2017, PROC CVPR IEEE, P1855, DOI 10.1109/CVPR.2017.201
   Faghri F, 2018, P BRIT MACH VIS C, P1
   Gong YC, 2014, LECT NOTES COMPUT SC, V8692, P529, DOI 10.1007/978-3-319-10593-2_35
   Gu JX, 2018, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR.2018.00750
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He YH, 2016, IEEE T MULTIMEDIA, V18, P1363, DOI 10.1109/TMM.2016.2558463
   Hu RH, 2016, PROC CVPR IEEE, P4555, DOI 10.1109/CVPR.2016.493
   Hu YT, 2018, IEEE T MULTIMEDIA, V20, P927, DOI 10.1109/TMM.2017.2760101
   Huang Y, 2019, IEEE I CONF COMP VIS, P5773, DOI 10.1109/ICCV.2019.00587
   Huang Y, 2019, AAAI CONF ARTIF INTE, P8489
   Huang Y, 2018, PROC CVPR IEEE, P6163, DOI 10.1109/CVPR.2018.00645
   Huang Y, 2017, PROC CVPR IEEE, P7254, DOI 10.1109/CVPR.2017.767
   Jba V. M., 2015, P INT C LEARN REPR
   Kang CC, 2015, IEEE T MULTIMEDIA, V17, P370, DOI 10.1109/TMM.2015.2390499
   Karaoglu S, 2017, IEEE T MULTIMEDIA, V19, P1063, DOI 10.1109/TMM.2016.2638622
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kingma D. P., 2014, arXiv
   Kiros Ryan., 2014, ABS14112539 CORR
   Klein Benjamin, 2014, ARXIV14117399
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Li G, 2020, AAAI CONF ARTIF INTE, V34, P11336
   Li KP, 2019, IEEE I CONF COMP VIS, P4653, DOI 10.1109/ICCV.2019.00475
   Li XR, 2019, IEEE T MULTIMEDIA, V21, P2347, DOI 10.1109/TMM.2019.2896494
   Lin X, 2016, LECT NOTES COMPUT SC, V9906, P261, DOI 10.1007/978-3-319-46475-6_17
   Ma L, 2020, IEEE T CIRC SYST VID, V30, P2250, DOI 10.1109/TCSVT.2019.2916167
   Ma L, 2019, NEUROCOMPUTING, V345, P36, DOI 10.1016/j.neucom.2018.11.089
   Ma L, 2015, IEEE I CONF COMP VIS, P2623, DOI 10.1109/ICCV.2015.301
   Mihalcea R., 2004, P 2004 C EMPIRICAL M, P404, DOI DOI 10.3115/1219044.1219064
   Nam H, 2017, PROC CVPR IEEE, P2156, DOI 10.1109/CVPR.2017.232
   Niu ZX, 2017, IEEE I CONF COMP VIS, P1899, DOI 10.1109/ICCV.2017.208
   Plummer BA, 2018, LECT NOTES COMPUT SC, V11216, P258, DOI 10.1007/978-3-030-01258-8_16
   Qian XM, 2017, IEEE T IMAGE PROCESS, V26, P3734, DOI 10.1109/TIP.2017.2699623
   Reed S, 2016, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2016.13
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shih KJ, 2016, PROC CVPR IEEE, P4613, DOI 10.1109/CVPR.2016.499
   Tian Z, 2016, LECT NOTES COMPUT SC, V9912, P56, DOI 10.1007/978-3-319-46484-8_4
   Vinyals O, 2017, IEEE T PATTERN ANAL, V39, P652, DOI 10.1109/TPAMI.2016.2587640
   Wang L., IEEE IEEE T PATT AN
   Wang WR, 2015, PR MACH LEARN RES, V37, P1083
   Wang WR, 2015, ANN ALLERTON CONF, P688, DOI 10.1109/ALLERTON.2015.7447071
   Wang YX, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3792
   Wang YX, 2018, IEEE T IMAGE PROCESS, V27, P4437, DOI 10.1109/TIP.2018.2837219
   Xi Wei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10938, DOI 10.1109/CVPR42600.2020.01095
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yan F, 2015, PROC CVPR IEEE, P3441, DOI 10.1109/CVPR.2015.7298966
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   Yen-Chun Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P104, DOI 10.1007/978-3-030-58577-8_7
   Young P., 2014, T ASSOC COMPUT LING, V2, P67, DOI [DOI 10.1162/TACL_A_00166, 10.1162/tacl_a_00166]
   Yu E, 2019, IEEE T MULTIMEDIA, V21, P1276, DOI 10.1109/TMM.2018.2877127
   Zhang L, 2018, IEEE T MULTIMEDIA, V20, P128, DOI 10.1109/TMM.2017.2723841
   Zhang L, 2017, IEEE T MULTIMEDIA, V19, P1220, DOI 10.1109/TMM.2016.2646219
   Zhang Y, 2018, LECT NOTES COMPUT SC, V11205, P707, DOI 10.1007/978-3-030-01246-5_42
   Zheng ZD, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3383184
   Zhu LC, 2017, PROC CVPR IEEE, P1339, DOI 10.1109/CVPR.2017.147
NR 61
TC 18
Z9 18
U1 2
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3362
EP 3376
DI 10.1109/TMM.2020.3024822
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UU6IS
UT WOS:000698902000032
DA 2024-07-18
ER

PT J
AU Min, WQ
   Mei, SH
   Li, Z
   Jiang, SQ
AF Min, Weiqing
   Mei, Shuhuan
   Li, Zhuo
   Jiang, Shuqiang
TI A Two-Stage Triplet Network Training Framework for Image Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image retrieval; Feature extraction; Convolution; Training; Image
   representation; Task analysis; Measurement
ID OBJECT RETRIEVAL; MODEL; CONSISTENCY
AB In this paper, we propose a novel framework for instance-level image retrieval. Recent methods focus on fine-tuning the Convolutional Neural Network (CNN) via a Siamese architecture to improve off-the-shelf CNN features. They generally use the ranking loss to train such networks, and do not take full use of supervised information for better network training, especially with more complex neural architectures. To solve this, we propose a two-stage triplet network training framework, which mainly consists of two stages. First, we propose a Double-Loss Regularized Triplet Network (DLRTN), which extends basic triplet network by attaching the classification sub-network, and is trained via simultaneously optimizing two different types of loss functions. Double-loss functions of DLRTN aim at specific retrieval task and can jointly boost the discriminative capability of DLRTN from different aspects via supervised learning. Second, considering feature maps of the last convolution layer extracted from DLRTN and regions detected from the region proposal network as the input, we then introduce the Regional Generalized-Mean Pooling (RGMP) layer for the triplet network, and re-train this network to learn pooling parameters. Through RGMP, we pool feature maps for each region and aggregate features of different regions from each image to Regional Generalized Activations of Convolutions (R-GAC) as final image representation. R-GAC is capable of generalizing existing Regional Maximum Activations of Convolutions (R-MAC) and is thus more robust to scale and translation. We conduct the experiment on six image retrieval datasets including standard benchmarks and recently introduced INSTRE dataset. Extensive experimental results demonstrate the effectiveness of the proposed framework.
C1 [Min, Weiqing; Li, Zhuo; Jiang, Shuqiang] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
   [Min, Weiqing; Li, Zhuo; Jiang, Shuqiang] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Mei, Shuhuan] Nanjing New Generat Artificial Intelligence Res I, Nanjing 210046, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS
RP Jiang, SQ (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
EM minweiqing@ict.ac.cn; meishuhuan@ngai.ac.cn; zhuo.li@vipl.ict.ac.cn;
   sqjiang@ict.ac.cn
FU National Natural Science Foundation of China [61532018, 61972378];
   Beijing Natural Science Foundation [L182054]; National Program for
   Special Support of Eminent Professionals; National Program for Support
   of Top-notch Young Professionals
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61532018 and 61972378, in part by
   Beijing Natural Science Foundation under Grant L182054, and in part by
   the National Program for Special Support of Eminent Professionals and
   National Program for Support of Top-notch Young Professionals. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Y. L. Tian.
CR [Anonymous], 2017, ARXIV170200338
   Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI [10.1109/TPAMI.2017.2711011, 10.1109/CVPR.2016.572]
   Azizpour Hossein, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P36, DOI 10.1109/CVPRW.2015.7301270
   Babenko A, 2015, IEEE I CONF COMP VIS, P1269, DOI 10.1109/ICCV.2015.150
   Babenko A, 2014, LECT NOTES COMPUT SC, V8689, P584, DOI 10.1007/978-3-319-10590-1_38
   Bergamo A, 2013, PROC CVPR IEEE, P763, DOI 10.1109/CVPR.2013.104
   Chadha A, 2017, IEEE T MULTIMEDIA, V19, P1596, DOI 10.1109/TMM.2017.2673415
   Chebaro MR, 2010, PROCEEDINGS OF THE ASME INTERNATIONAL PIPELINE CONFERENCE 2010, VOL 1, P511
   Chen ZF, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1703, DOI 10.1145/3343031.3351005
   Chu LY, 2013, IEEE T MULTIMEDIA, V15, P1982, DOI 10.1109/TMM.2013.2270455
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Chum O, 2011, PROC CVPR IEEE, P889, DOI 10.1109/CVPR.2011.5995601
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Gao ZN, 2016, IEEE T MULTIMEDIA, V18, P1661, DOI 10.1109/TMM.2016.2568748
   Gao ZM, 2019, IEEE T IMAGE PROCESS, V28, P1191, DOI 10.1109/TIP.2018.2872831
   Gong YC, 2014, LECT NOTES COMPUT SC, V8695, P392, DOI 10.1007/978-3-319-10584-0_26
   Gordo A, 2017, INT J COMPUT VISION, V124, P237, DOI 10.1007/s11263-017-1016-8
   Gordo A, 2016, LECT NOTES COMPUT SC, V9910, P241, DOI 10.1007/978-3-319-46466-4_15
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hoffer E, 2015, LECT NOTES COMPUT SC, V9370, P84, DOI 10.1007/978-3-319-24261-3_7
   Iscen A, 2017, PROC CVPR IEEE, P926, DOI 10.1109/CVPR.2017.105
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Jenicek T, 2019, IEEE I CONF COMP VIS, P9695, DOI 10.1109/ICCV.2019.00979
   Ji X, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1654, DOI 10.1145/3123266.3123429
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Joe Yue-Hei Ng, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P53, DOI 10.1109/CVPRW.2015.7301272
   Kalantidis Yannis, 2016, Computer Vision - ECCV 2016. 14th European Conference: Workshops. Proceedings: LNCS 9913, P685, DOI 10.1007/978-3-319-46604-0_48
   Ke Y, 2004, PROC CVPR IEEE, P506
   King DB, 2015, ACS SYM SER, V1214, P1
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li Ying., 2016, Proceedings of the 2016 ACM on Multimedia Conference, P132
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Mei SH, 2019, MULTIMED TOOLS APPL, V78, P13247, DOI 10.1007/s11042-018-6427-1
   Mohedano E, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P327, DOI 10.1145/2911996.2912061
   Pang SM, 2019, IEEE T MULTIMEDIA, V21, P760, DOI 10.1109/TMM.2018.2866230
   Paulin M, 2015, IEEE I CONF COMP VIS, P91, DOI 10.1109/ICCV.2015.19
   Perronnin F, 2015, PROC CVPR IEEE, P3743, DOI 10.1109/CVPR.2015.7298998
   Philbin J, 2008, PROC CVPR IEEE, P2285
   Radenovic F, 2019, IEEE T PATTERN ANAL, V41, P1655, DOI 10.1109/TPAMI.2018.2846566
   Razavian A. S., 2016, ITE Trans. Media Technol. Appl., V4, P251, DOI [DOI 10.3169/MTA.4.251, 10.3169/mta.4.251]
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Ren S., 2017, IEEETransactionsonPatternAnalysisandMachineIntelligence, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Revaud J, 2019, IEEE I CONF COMP VIS, P5106, DOI 10.1109/ICCV.2019.00521
   Schönberger JL, 2015, PROC CVPR IEEE, P5126, DOI 10.1109/CVPR.2015.7299148
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shen XH, 2014, IEEE T PATTERN ANAL, V36, P1229, DOI 10.1109/TPAMI.2013.237
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434
   Song YF, 2016, IEEE T MULTIMEDIA, V18, P1542, DOI 10.1109/TMM.2016.2568743
   Do TT, 2017, PROC CVPR IEEE, P4217, DOI 10.1109/CVPR.2017.449
   Tolias G., 2016, Conference Track Proceedings,
   Hoang T, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1600, DOI 10.1145/3123266.3123417
   Wang S, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2700292
   Xu CY, 2016, IEEE T CIRC SYST VID, V26, P2273, DOI 10.1109/TCSVT.2015.2477937
   Xu J, 2019, IEEE T IMAGE PROCESS, V28, P601, DOI 10.1109/TIP.2018.2867104
   Yan K, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P407, DOI 10.1145/2964284.2967252
   Yang F, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1513, DOI 10.1145/3123266.3123396
   Zheng L, 2018, IEEE T PATTERN ANAL, V40, P1224, DOI 10.1109/TPAMI.2017.2709749
   Zhong ZY, 2015, IEEE T MULTIMEDIA, V17, P1391, DOI 10.1109/TMM.2015.2446201
   Zhou JH, 2020, IEEE T PATTERN ANAL, V42, P2858, DOI 10.1109/TPAMI.2019.2918208
   Zhou WG, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2422956.2422960
NR 62
TC 27
Z9 29
U1 1
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2020
VL 22
IS 12
BP 3128
EP 3138
DI 10.1109/TMM.2020.2974326
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU9BG
UT WOS:000591817700009
DA 2024-07-18
ER

PT J
AU Kazemi, M
   Ghanbari, M
   Shirmohammadi, S
AF Kazemi, Mohammad
   Ghanbari, Mohammad
   Shirmohammadi, Shervin
TI Intra Coding Strategy for Video Error Resiliency: Behavioral Analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Encoding; Distortion; Resilience; Streaming media; Bit rate; Linear
   programming; Delays; Error resilient video coding; video error
   concealment; intra coding
ID RATE-DISTORTION OPTIMIZATION; CONCEALMENT; REFRESH
AB One challenge in video transmission is to deal with packet loss. Since the compressed video streams are sensitive to data loss, the error resiliency of the encoded video becomes important. When video data is lost and retransmission is not possible, the missed data should be concealed. But loss concealment causes distortion in the lossy frame which also propagates into the next frames even if their data are received correctly. One promising solution to mitigate this error propagation is intra coding. There are three approaches for intra coding: intra coding of a number of blocks selected randomly or regularly, intra coding of some specific blocks selected by an appropriate cost function, or intra coding of a whole frame. But Intra coding reduces the compression ratio; therefore, there exists a trade-off between bitrate and error resiliency achieved by intra coding. In this paper, we study and show the best strategy for getting the best rate-distortion performance. Considering the error propagation, an objective function is formulated, and with some approximations, this objective function is simplified and solved. The solution demonstrates that periodical I-frame coding is preferred over coding only a number of blocks as intra mode in P-frames. Through examination of various test sequences, it is shown that the best intra frame period depends on the coding bitrate as well as the packet loss rate. We then propose a scheme to estimate this period from curve fitting of the experimental results, and show that our proposed scheme outperforms other methods of intra coding especially for higher loss rates and coding bitrates.
C1 [Kazemi, Mohammad] Univ Isfahan, Dept Elect Engn, Esfahan 8174673441, Iran.
   [Ghanbari, Mohammad] Univ Tehran, Sch Elect & Comp Engn, Coll Engn, Tehran 1417466191, Iran.
   [Ghanbari, Mohammad] Univ Essex, Sch Comp Sci & Elect Engn, Colchester CO4 3SQ, Essex, England.
   [Shirmohammadi, Shervin] Univ Ottawa, Distributed & Collaborat Virtual Environm Res Lab, Sch Elect Engn & Comp Sci, Ottawa, ON K1N 6N5, Canada.
C3 University of Isfahan; University of Tehran; University of Essex;
   University of Ottawa
RP Kazemi, M (corresponding author), Univ Isfahan, Dept Elect Engn, Esfahan 8174673441, Iran.
EM m.kazemi@eng.ui.ac.ir; ghan@ut.ac.ir; shervin@eecs.uottawa.ca
RI Kazemi, Mohammad/G-7733-2017; Ghanbari, Mohammad/L-4053-2019;
   Shirmohammadi, Shervin/E-6945-2012
OI Ghanbari, Mohammad/0000-0002-5482-8378; Shirmohammadi,
   Shervin/0000-0002-3973-4445
CR Ali IA, 2013, J VIS COMMUN IMAGE R, V24, P486, DOI 10.1016/j.jvcir.2013.03.005
   Ali I, 2012, 2012 IEEE INTERNATIONAL CONFERENCE ON CONSUMER ELECTRONICS (ICCE), P118, DOI 10.1109/ICCE.2012.6161768
   [Anonymous], 2014, CONF REC ASILOMAR C
   [Anonymous], 2017, IEEE IMAGE PROC
   [Anonymous], 2014, J ENZYM INHIB MED CH, DOI DOI 10.3109/14756366.2013.805756
   [Anonymous], 2018, IEEE T MULTIMEDIA, DOI DOI 10.1109/TMM.2017.2758578
   [Anonymous], 2016, INT CONF CONTR AUTO
   [Anonymous], 2014, RECENT PAT SIGNAL PR
   [Anonymous], 2014, 2014 INT C OPT NETW
   Basso S., 2012, P ASS COMP MACH SPEC, P7
   Carreira JFM, 2018, IEEE T CIRC SYST VID, V28, P1960, DOI 10.1109/TCSVT.2017.2691471
   Chen HM, 2015, J VIS COMMUN IMAGE R, V31, P294, DOI 10.1016/j.jvcir.2015.06.018
   Cruz R. D. C., 2014, P AS PAC SIGN INF PR
   dela Cruz AR, 2014, INT CONF DIGIT SIG, P509, DOI 10.1109/ICDSP.2014.6900718
   Fernando A., 2016, P IEEE INT C CONS EL, P85
   Fleury M., 2012, MULTIMEDIA NETW CODI, P175
   Fleury M., 2011, RECENT PATENTS SIGNA, V1, P124
   Ghanbari M., 2011, APPENDIX E, V3rd
   Hadizadeh H, 2011, IEEE T CIRC SYST VID, V21, P1139, DOI 10.1109/TCSVT.2011.2138770
   He ZH, 2002, IEEE T CIRC SYST VID, V12, P511, DOI 10.1109/TCSVT.2002.800313
   Kazemi M, 2017, IEEE T MULTIMEDIA, V19, P54, DOI 10.1109/TMM.2016.2607342
   Khan E, 2004, IEEE T CIRC SYST VID, V14, P1294, DOI 10.1109/TCSVT.2004.837018
   Kulupana G, 2019, IEEE T CIRC SYST VID, V29, P3367, DOI 10.1109/TCSVT.2018.2879956
   Liao Y., 2009, P INT PACK VID WORKS, P1
   Lie WN, 2014, IEEE T MULTIMEDIA, V16, P216, DOI 10.1109/TMM.2013.2281587
   Lin T., 2013
   Moiron S, 2011, ELECTRON LETT, V47, P103, DOI 10.1049/el.2010.3018
   Moiron S., 2011, P IEEE INT C IM PROC, P3229
   Ortega A, 1998, IEEE SIGNAL PROC MAG, V15, P23, DOI 10.1109/79.733495
   Oztas B, 2012, IEEE I C ELECT CIRC, P785, DOI 10.1109/ICECS.2012.6463542
   Pinson MH, 2004, IEEE T BROADCAST, V50, P312, DOI 10.1109/TBC.2004.834028
   Qian XM, 2009, IEEE T MULTIMEDIA, V11, P683, DOI 10.1109/TMM.2009.2017609
   Shu HY, 2008, IEEE T MULTIMEDIA, V10, P97, DOI 10.1109/TMM.2007.911300
   Sullivan GJ, 1998, IEEE SIGNAL PROC MAG, V15, P74, DOI 10.1109/79.733497
   Tu W, 2009, IEEE T CIRC SYST VID, V19, P151, DOI 10.1109/TCSVT.2008.2009240
   Wan F., 2005, THESIS
   Wan S, 2007, IEEE T IMAGE PROCESS, V16, P1327, DOI 10.1109/TIP.2007.894230
   Wang Y, 1998, P IEEE, V86, P974, DOI 10.1109/5.664283
   Wang Y, 2000, IEEE SIGNAL PROC MAG, V17, P61, DOI 10.1109/79.855913
   Wang Y, 2006, IEEE T CIRC SYST VID, V16, P716, DOI 10.1109/TCSVT.2006.875203
   Xiao JM, 2011, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2011-12
   Xu J, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P693, DOI 10.1109/ICME.2006.262540
   Yang H, 2007, IEEE T CIRC SYST VID, V17, P845, DOI 10.1109/TCSVT.2007.897116
   Yang H, 2010, IEEE T IMAGE PROCESS, V19, P108, DOI 10.1109/TIP.2009.2032895
   Zhang R, 2000, IEEE J SEL AREA COMM, V18, P966, DOI 10.1109/49.848250
   Zhang Y., 2011, P INT C IM PROC BRUS, P3217
   Zhang Y, 2007, IEEE T MULTIMEDIA, V9, P445, DOI 10.1109/TMM.2006.887989
NR 47
TC 7
Z9 7
U1 1
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEPT
PY 2020
VL 22
IS 9
BP 2193
EP 2206
DI 10.1109/TMM.2019.2957991
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NE0TL
UT WOS:000562310200001
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Guo, LT
   Liu, J
   Lu, SC
   Lu, HQ
AF Guo, Longteng
   Liu, Jing
   Lu, Shichen
   Lu, Hanqing
TI Show, Tell, and Polish: Ruminant Decoding for Image Captioning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Decoding; Visualization; Planning; Training; Semantics; Reinforcement
   learning; Task analysis; Image captioning; multi-pass decoding;
   rumination
AB The encoder-decoder framework has been the base of popular image captioning models, which typically predicts the target sentence based on the encoded source image one word at a time in sequence. However, such a single-pass decoding framework encounters two problems. First, mistakes in the predicted words cannot be corrected and may propagate to the entire sentence. Second, because the single-pass decoder cannot access the following un-generated words, it can only perform local planning to choose every single word according to the preceding words, while lacks the global planning ability as for maintaining the semantic consistency and fluency of the whole sentence. In order to address the above two problems, in this work, we design a ruminant captioning framework which contains an image encoder, a base decoder, and a ruminant decoder. Specifically, the outputs of the former/base decoder are utilized as the global information to guide the words prediction of the latter/ruminant decoder, in an attempt to mimic human polishing process. We enable jointly training of the whole framework and overcome the non-differential problem of discrete words by designing a novel reinforcement learning based optimization algorithm. Experiments on two datasets (MS COCO and Flickr30 k) demonstrate that our ruminant decoding method can bring significant improvements over traditional single-pass decoding based models and achieves state-of-the-art performance.
C1 [Guo, Longteng; Liu, Jing; Lu, Hanqing] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Guo, Longteng] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Lu, Shichen] Wuhan Univ, Sch Comp, Wuhan 430072, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   Wuhan University
RP Liu, J (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM longteng.guo@nlpr.ia.ac.cn; jliu@nlpr.ia.ac.cn; sclu@whu.edu.cn;
   luhq@nlpr.ia.ac.cn
RI Liu, Jing/A-7644-2016
OI Guo, Longteng/0000-0002-4340-4000; liu, jing/0000-0003-0903-9131
FU National Natural Science Foundation of China [61922086, 61872366];
   Beijing Natural Science Foundation [4192059]
FX Thisworkwas supported in part by theNational Natural Science Foundation
   of China underGrants 61922086 and 61872366 and in part by the Beijing
   Natural Science Foundation under Grant 4192059.
CR Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387
   Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24
   Aneja J, 2018, PROC CVPR IEEE, P5561, DOI 10.1109/CVPR.2018.00583
   [Anonymous], 2018, P 3 C MACHINE TRANSL
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Barrault Loic, 2018, P 3 C MACH TRANSL WM, P304
   Bengio S., 2015, NIPS, DOI DOI 10.5555/2969239.2969370
   Cai DL, 2018, LEC NO MULTI IND ENG, P499, DOI 10.1007/978-3-319-59280-0_40
   Chen SJ, 2018, AER ADV ENG RES, V176, P68
   Cho K., 2014, ARXIV14061078
   Cho K, 2015, IEEE T MULTIMEDIA, V17, P1875, DOI 10.1109/TMM.2015.2477044
   Denkowski Michael, 2014, P 9 WORKSH STAT MACH, DOI [10.3115/v1/W14-3348, DOI 10.3115/V1/W14-3348]
   Graves A, 2005, NEURAL NETWORKS, V18, P602, DOI 10.1016/j.neunet.2005.06.042
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Gu JX, 2018, AAAI CONF ARTIF INTE, P6837
   Gu JX, 2017, IEEE I CONF COMP VIS, P1231, DOI 10.1109/ICCV.2017.138
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Jingna Mao, 2015, 2015 IEEE Biomedical Circuits and Systems Conference (BioCAS), P1, DOI 10.1109/BioCAS.2015.7348279
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kilickaya M, 2017, 15TH CONFERENCE OF THE EUROPEAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (EACL 2017), VOL 1: LONG PAPERS, P199
   Kingma D. P., 2014, arXiv
   Kiros R, 2014, PR MACH LEARN RES, V32, P595
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kusner MJ, 2015, PR MACH LEARN RES, V37, P957
   Li LH, 2018, IEEE T MULTIMEDIA, V20, P726, DOI 10.1109/TMM.2017.2751140
   Lin Chin-Yew, 2004, Text summarization branches out, P74, DOI DOI 10.2307/3105454
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu DQ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1416, DOI 10.1145/3240508.3240632
   Liu XH, 2018, LECT NOTES COMPUT SC, V11219, P353, DOI 10.1007/978-3-030-01267-0_21
   Lu JS, 2018, PROC CVPR IEEE, P7219, DOI 10.1109/CVPR.2018.00754
   Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345
   Luo RT, 2018, PROC CVPR IEEE, P6964, DOI 10.1109/CVPR.2018.00728
   Luong T., 2015, P 2015 C EMP METH NA, DOI [DOI 10.18653/V1/D15-1166, 10.18653/v1/D15-1166]
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Parikh, 2015, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2015.7299087
   Ranzato L, 2016, RIV PSICOL EMERG ASS, P4
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Sutskever I., 2014, ADV NEURAL INFORM PR, V4, P3104, DOI DOI 10.5555/2969033.2969173
   Sutton RS, 2018, ADAPT COMPUT MACH LE, P1
   Vaswani Ashish, 2017, Advances in Neural Information Processing Systems (NeurIPS), V17, P6000, DOI DOI 10.48550/ARXIV.1706.03762
   Vinyals O, 2017, IEEE T PATTERN ANAL, V39, P652, DOI 10.1109/TPAMI.2016.2587640
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696
   Xu X, 2015, IEEE ICC, P2048, DOI 10.1109/ICC.2015.7248627
   Yang M, 2019, IEEE T MULTIMEDIA, V21, P1047, DOI 10.1109/TMM.2018.2869276
   Yang Z, 2016, INT CONF ACOUST SPEE, P3236, DOI 10.1109/ICASSP.2016.7472275
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yao T, 2017, IEEE I CONF COMP VIS, P4904, DOI 10.1109/ICCV.2017.524
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Young P., 2014, Transactions of the Association for Computational Linguistics, V2, P67
   Zhang L., 2017, P NIPS WORKSH VIS GR
   Zhang ZJ, 2019, IEEE T MULTIMEDIA, V21, P1681, DOI 10.1109/TMM.2018.2888822
   Zhou LW, 2017, PROCEEDINGS OF THE THEMATIC WORKSHOPS OF ACM MULTIMEDIA 2017 (THEMATIC WORKSHOPS'17), P305, DOI 10.1145/3126686.3126717
NR 55
TC 33
Z9 35
U1 1
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2020
VL 22
IS 8
BP 2149
EP 2162
DI 10.1109/TMM.2019.2951226
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MR2MS
UT WOS:000553424500019
DA 2024-07-18
ER

PT J
AU Ma, HC
   Liu, D
   Xiong, RQ
   Wu, F
AF Ma, Haichuan
   Liu, Dong
   Xiong, Ruiqin
   Wu, Feng
TI iWave: CNN-Based Wavelet-Like Transform for Image Compression
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Wavelet transforms; Image coding; Wavelet analysis; Task analysis;
   Kernel; Quantization (signal); Convolutional neural network (CNN); image
   compression; lifting scheme; wavelet transform
ID LIFTING SCHEME; CONSTRUCTION
AB Wavelet transform is a powerful tool for multiresolution time-frequency analysis. It has been widely adopted in many image processing tasks, such as denoising, enhancement, fusion, and especially compression. Wavelets lead to the successful image coding standard JPEG-2000. Traditionally, wavelets were designed from the signal processing theory with certain assumption on the signal, but natural images are not as ideal as assumed by the theory. How to design content-adaptive wavelets for natural images remains a difficulty. Inspired by the recent progress of convolutional neural network (CNN), we propose iWave as a framework for deriving wavelet-like transform that is more suitable for natural image compression. iWave adopts an update-first lifting scheme, where the prediction filter is a trained CNN, to achieve wavelet-like transform. The CNN can be embedded into a deep network that is analogous to an auto-encoder, which is trained end-to-end. The trained wavelet-like transform still possesses the lifting structure, which ensures perfect reconstruction, supports multiresolution analysis, and is more interpretable than the deep networks trained as "black boxes." We perform experiments to verify the generality as well as the speciality of iWave in comparison with JPEG-2000. When trained with a generic set of natural images and tested on the Kodak dataset, iWave achieves on average 4.4% and up to 14% BD-rate reductions. When trained and tested with a specific kind of textures, iWave provides as high as 27% BD-rate reduction.
C1 [Ma, Haichuan; Liu, Dong; Wu, Feng] Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230027, Peoples R China.
   [Xiong, Ruiqin] Peking Univ, Sch Elect Engn & Comp Sci, Inst Digital Media, Beijing 100871, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Peking University
RP Liu, D (corresponding author), Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230027, Peoples R China.
EM hcma@mail.ustc.edu.cn; dongeliu@ustc.edu.cn; rqxiong@pku.edu.cn;
   fengwu@ustc.edu.cn
RI Wu, Feng/KCY-3017-2024; ma, haichuan/HTR-7868-2023; liu,
   dong/GRJ-9115-2022
OI Liu, Dong/0000-0001-9100-2906; Xiong, Ruiqin/0000-0001-9796-0478
FU National Key Research and Development Plan [2017YFB1002401]; Natural
   Science Foundation of China [61772483, 61425026, 61931014]
FX This work was supported in part by the National Key Research and
   Development Plan under Grant 2017YFB1002401, and in part by the Natural
   Science Foundation of China under Grants 61772483, 61425026, and
   61931014.
CR Agustsson E., 2017, ADV NEURAL INFORM PR, P1141
   Agustsson Eirikur, 2017, IEEE CVF C COMP VIS, P126, DOI DOI 10.1109/CVPRW.2017.150
   [Anonymous], 1999, TECH REP
   Balle J, 2018, ICLR
   Balle J., 2016, 5 INT C LEARNING REP
   Claypoole RL, 2003, IEEE T IMAGE PROCESS, V12, P1449, DOI 10.1109/TIP.2003.817237
   Daubechies I, 1998, J FOURIER ANAL APPL, V4, P247, DOI 10.1007/BF02476026
   Ding WP, 2007, IEEE T IMAGE PROCESS, V16, P416, DOI 10.1109/TIP.2006.888341
   Do MN, 2005, IEEE T IMAGE PROCESS, V14, P2091, DOI 10.1109/TIP.2005.859376
   Do MN, 2003, IEEE T IMAGE PROCESS, V12, P16, DOI 10.1109/TIP.2002.806252
   Gomez A. N., 2017, Advances in Neural Information Processing Systems, P2214
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   Jacobsen J.-H., 2018, ARXIV180207088
   Johnston N, 2018, PROC CVPR IEEE, P4385, DOI 10.1109/CVPR.2018.00461
   Kingma D. P., 2014, arXiv
   Liu Y, 2008, IEEE T IMAGE PROCESS, V17, P500, DOI 10.1109/TIP.2008.917104
   Minnen David, 2018, ADV NEURAL INFORM PR, P10771, DOI DOI 10.48550/ARXIV.1809.02736
   Pennebaker W. B., 1992, JPEG STILL IMAGE DAT
   Rabbani M, 2002, SIGNAL PROCESS-IMAGE, V17, P3, DOI 10.1016/S0923-5965(01)00024-8
   Rippel O., 2017, P 34 INT C MACH LEAR, P2922
   Starck JL, 2002, IEEE T IMAGE PROCESS, V11, P670, DOI [10.1109/TIP.2002.1014998, 10.1117/12.408568]
   Sweldens W, 1998, SIAM J MATH ANAL, V29, P511, DOI 10.1137/S0036141095289051
   Sweldens W, 1996, APPL COMPUT HARMON A, V3, P186, DOI 10.1006/acha.1996.0015
   Taubman David., 2012, JPEG2000 Image Compression Fundamentals, Standards and Practice
   Theis L., 2017, ICLR
   Toderici G., 2015, ARXIV PREPRINT ARXIV
   Todeschini G, 2017, INVENTIONS-BASEL, V2, DOI 10.3390/inventions2030014
NR 27
TC 45
Z9 47
U1 2
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2020
VL 22
IS 7
BP 1667
EP 1679
DI 10.1109/TMM.2019.2957990
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MG4GJ
UT WOS:000545990500002
DA 2024-07-18
ER

PT J
AU Wang, J
   Xu, W
   Cai, JF
   Zhu, Q
   Shi, YH
   Yin, BC
AF Wang, Jin
   Xu, Wei
   Cai, Jian-Feng
   Zhu, Qing
   Shi, Yunhui
   Yin, Baocai
TI Multi-Direction Dictionary Learning Based Depth Map Super-Resolution
   With Autoregressive Modeling
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dictionaries; Adaptation models; Image edge detection; Color; Machine
   learning; Cameras; Geometry; Depth map; super-resolution (SR);
   dictionary learning; autoregressive (AR) model; sparse representation
ID IMAGE SUPERRESOLUTION; SPARSE REPRESENTATION; SUPER RESOLUTION;
   RECONSTRUCTION; APPROXIMATION; COMPRESSION; ALGORITHM; TIME
AB 3D depth cameras have become more and more popular in recent years. However, depth maps captured by these cameras can hardly be used in 3D reconstruction directly because they often suffer from low resolution and blurring depth discontinuities. Super resolution of depth maps is necessary. In depth maps, the edge areas play more important role and demonstrate distinct geometry directions compared with natural images. However, most existing super-resolution methods ignore this fact, and they can not handle depth edges properly. Motivated by this, we propose a compound method that combines multi-direction dictionary sparse representation and autoregressive (AR) models, so that the depth edges are presented precisely at different levels. In the patch level, the depth edge patches with geometry directions are well represented by the pre-trained multi-directional dictionaries. Compared with a universal dictionary, multiple dictionaries trained from different directional patches can represent the directional depth patch much better. In the finer pixel level, we utilize an adaptive AR model to represent the local correlation patterns in small areas. Extensive experimental results on both synthetic and real datasets demonstrate that, the proposed model outperforms state-of-the-art depth map super-resolution methods in terms of both quantitative metrics and subjective visual quality.
C1 [Wang, Jin] Beijing Univ Technol, Fac Informat Technol, Beijing Adv Innovat Ctr Future Internet Technol, Beijing Key Lab Multimedia & Intelligent Software, Beijing 100124, Peoples R China.
   [Wang, Jin] Beijing Key Lab Internet Culture & Digital Dissem, Beijing 100101, Peoples R China.
   [Xu, Wei; Zhu, Qing; Shi, Yunhui] Beijing Univ Technol, Fac Informat Technol, Beijing 100124, Peoples R China.
   [Cai, Jian-Feng] Hong Kong Univ Sci & Technol, Dept Math, Hong Kong, Peoples R China.
   [Yin, Baocai] Dalian Univ Technol, Dalian 116024, Peoples R China.
C3 Beijing University of Technology; Beijing University of Technology; Hong
   Kong University of Science & Technology; Dalian University of Technology
RP Wang, J (corresponding author), Beijing Univ Technol, Fac Informat Technol, Beijing Adv Innovat Ctr Future Internet Technol, Beijing Key Lab Multimedia & Intelligent Software, Beijing 100124, Peoples R China.
EM ijinwang@bjut.edu.cn; weixu8532@163.com; jfcai@ust.hk;
   ccgszq@bjut.edu.cn; syhzm@bjut.edu.cn; ybc@bjut.edu.cn
RI ; Cai, Jianfeng/L-1808-2016
OI Wang, Jin/0000-0001-5437-3150; Cai, Jianfeng/0000-0003-2571-570X
FU National Natural Science Foundation of China [61906008, 61632006,
   61672066, 61976011, 61671070, 61906009, U1811463]; Hong Kong Research
   Grant Council GRF [16306317]; BeijingMunicipal Science and Technology
   Project [Z171100004417023]; Opening Project of Beijing Key Laboratory of
   Internet Culture and Digital Dissemination Research
FX This work was supported by in part by the National Natural Science
   Foundation of China under Grant 61906008, Grant 61632006, Grant
   61672066, Grant 61976011, Grant 61671070, Grant 61906009, and Grant
   U1811463; in part by the Hong Kong Research Grant Council GRF underGrant
   16306317; in part by theBeijingMunicipal Science and Technology Project
   (Z171100004417023); and in part by the Opening Project of Beijing Key
   Laboratory of Internet Culture and Digital Dissemination Research. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Professor Zixiang Xiong.
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   Alexiadis DS, 2013, IEEE T MULTIMEDIA, V15, P339, DOI 10.1109/TMM.2012.2229264
   [Anonymous], 2007, P 3DTV C MAY
   [Anonymous], 2017, FEATURE SQUEEZING DE
   [Anonymous], 2013, MIDDLEBURY DATASETS
   Bertsekas D. P., 1997, Journal of the Operational Research Society, V48, P334, DOI 10.1057/palgrave.jors.2600425
   Butler DJ, 2012, LECT NOTES COMPUT SC, V7577, P611, DOI 10.1007/978-3-642-33783-3_44
   Candes E. J., 2006, PROC INT C MATH, V17, P1433, DOI DOI 10.4171/022-3/69
   Candès EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083
   Candès EJ, 2008, J FOURIER ANAL APPL, V14, P877, DOI 10.1007/s00041-008-9045-x
   Chang H, 2004, PROC CVPR IEEE, P275, DOI 10.1109/cvpr.2004.1315043
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dong WS, 2017, IEEE T MULTIMEDIA, V19, P293, DOI 10.1109/TMM.2016.2613824
   Dong WS, 2011, IEEE T IMAGE PROCESS, V20, P1838, DOI 10.1109/TIP.2011.2108306
   Ekmekcioglu E, 2009, IEEE IMAGE PROC, P733, DOI 10.1109/ICIP.2009.5414296
   Elad M, 2010, P IEEE, V98, P972, DOI 10.1109/JPROC.2009.2037655
   Elad M, 2009, IEEE T INFORM THEORY, V55, P4701, DOI 10.1109/TIT.2009.2027565
   Engan K, 2000, SIGNAL PROCESS, V80, P2121, DOI 10.1016/S0165-1684(00)00072-4
   Ferstl D, 2013, IEEE I CONF COMP VIS, P993, DOI 10.1109/ICCV.2013.127
   Freeman WT, 2002, IEEE COMPUT GRAPH, V22, P56, DOI 10.1109/38.988747
   Guo CL, 2019, IEEE T IMAGE PROCESS, V28, P2545, DOI 10.1109/TIP.2018.2887029
   Guo YL, 2014, IEEE T MULTIMEDIA, V16, P1377, DOI 10.1109/TMM.2014.2316145
   Hannemann W., 2008, International Journal of Intelligent Systems Technologies and Applications, V5, P393, DOI 10.1504/IJISTA.2008.021302
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Hui TW, 2016, LECT NOTES COMPUT SC, V9907, P353, DOI 10.1007/978-3-319-46487-9_22
   Jiang JJ, 2017, IEEE T MULTIMEDIA, V19, P15, DOI 10.1109/TMM.2016.2599145
   Jiang ZY, 2018, IEEE T IMAGE PROCESS, V27, P2587, DOI 10.1109/TIP.2018.2806089
   Jung SW, 2013, IEEE T CIRC SYST VID, V23, P269, DOI 10.1109/TCSVT.2012.2203734
   Kiechle M, 2013, IEEE I CONF COMP VIS, P1545, DOI 10.1109/ICCV.2013.195
   Kolb A, 2010, COMPUT GRAPH FORUM, V29, P141, DOI 10.1111/j.1467-8659.2009.01583.x
   Kopf J, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276497, 10.1145/1239451.1239547]
   Kwon H, 2015, PROC CVPR IEEE, P159, DOI 10.1109/CVPR.2015.7298611
   Le Pennec E, 2005, MULTISCALE MODEL SIM, V4, P992, DOI 10.1137/040619454
   Liang H, 2014, IEEE T MULTIMEDIA, V16, P1241, DOI 10.1109/TMM.2014.2306177
   Lin Z., 2009, AUGMENTED LAGRANGE M
   Liu HT, 2017, INT CONF SEMANT, P1, DOI [10.1109/SKG.2017.00009, 10.1007/s13042-017-0646-z]
   Liu MY, 2013, PROC CVPR IEEE, P169, DOI 10.1109/CVPR.2013.29
   Liu W, 2017, IEEE T IMAGE PROCESS, V26, DOI 10.1109/TIP.2016.2612826
   Liu XM, 2019, IEEE T IMAGE PROCESS, V28, P1636, DOI 10.1109/TIP.2018.2875506
   Lu JB, 2012, PROC CVPR IEEE, P430, DOI 10.1109/CVPR.2012.6247705
   Lu JB, 2011, INT CONF ACOUST SPEE, P985
   Lu S, 2014, PROC CVPR IEEE, P3390, DOI 10.1109/CVPR.2014.433
   Lu T, 2017, IEEE ACCESS, V5, P13103, DOI 10.1109/ACCESS.2017.2717963
   Mairal J, 2008, IEEE T IMAGE PROCESS, V17, P53, DOI 10.1109/TIP.2007.911828
   Min DB, 2014, IEEE T IMAGE PROCESS, V23, P5638, DOI 10.1109/TIP.2014.2366600
   Ndjiki-Nya P, 2011, IEEE T MULTIMEDIA, V13, P453, DOI 10.1109/TMM.2011.2128862
   Ni M, 2017, IEEE ACCESS, V5, P26666, DOI 10.1109/ACCESS.2017.2773141
   Park J, 2011, IEEE I CONF COMP VIS, P1623, DOI 10.1109/ICCV.2011.6126423
   Protter M, 2010, IEEE T SIGNAL PROCES, V58, P3471, DOI 10.1109/TSP.2010.2046596
   Qu XB, 2012, MAGN RESON IMAGING, V30, P964, DOI 10.1016/j.mri.2012.02.019
   Ren Z, 2013, IEEE T MULTIMEDIA, V15, P1110, DOI 10.1109/TMM.2013.2246148
   Riegler G, 2016, LECT NOTES COMPUT SC, V9907, P268, DOI 10.1007/978-3-319-46487-9_17
   Rubinstein R, 2010, P IEEE, V98, P1045, DOI 10.1109/JPROC.2010.2040551
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Sun J, 2003, PROC CVPR IEEE, P729
   Tan X, 2014, PROC CVPR IEEE, P2941, DOI 10.1109/CVPR.2014.376
   Timofte R, 2015, LECT NOTES COMPUT SC, V9006, P111, DOI 10.1007/978-3-319-16817-3_8
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Tosic I, 2014, IEEE T IMAGE PROCESS, V23, P2122, DOI 10.1109/TIP.2014.2312645
   Tropp JA, 2010, P IEEE, V98, P948, DOI 10.1109/JPROC.2010.2044010
   Wang ZY, 2019, IEEE T IMAGE PROCESS, V28, P2530, DOI 10.1109/TIP.2018.2887017
   Wen Y, 2019, IEEE T IMAGE PROCESS, V28, P994, DOI 10.1109/TIP.2018.2874285
   Wu HY, 2007, IEEE I CONF COMP VIS, P628, DOI 10.1109/cvpr.2007.383211
   Xie J, 2015, IEEE T MULTIMEDIA, V17, P1525, DOI 10.1109/TMM.2015.2457678
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yang JY, 2012, LECT NOTES COMPUT SC, V7576, P158, DOI 10.1007/978-3-642-33715-4_12
   Yang JY, 2014, IEEE T IMAGE PROCESS, V23, P3443, DOI 10.1109/TIP.2014.2329776
   Yanjie Li, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P152, DOI 10.1109/ICME.2012.30
   Zhang XJ, 2008, IEEE T IMAGE PROCESS, V17, P887, DOI 10.1109/TIP.2008.924279
   Zhou LG, 2019, IEEE T NEUR NET LEAR, V30, P3275, DOI 10.1109/TNNLS.2018.2890550
   Zhu JJ, 2010, IEEE T PATTERN ANAL, V32, P899, DOI 10.1109/TPAMI.2009.68
NR 72
TC 20
Z9 21
U1 6
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2020
VL 22
IS 6
BP 1470
EP 1484
DI 10.1109/TMM.2019.2946075
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU8YA
UT WOS:000538033100008
DA 2024-07-18
ER

PT J
AU Chen, CL
   Ling, Q
AF Chen, Chunlin
   Ling, Qiang
TI Adaptive Convolution for Object Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE object detection; adaptive convolution; real time detector; deep
   learning
AB It is quite challenging to detect objects, especially, small objects, in complex scenes. To solve this problem, we propose a novel module named as adaptive convolution block (ACB), which adaptively adjusts the parameters of convolutional filters according to the current feature maps, and then, filter these feature maps with the obtained adaptive convolutional filters to generate enhanced features. Due to such adaptive convolution, the enhanced features can pay more attention to the concerned objects, suppress the interference information caused by irrelevant surroundings, and efficiently improve the detection accuracy. The proposed ACB is light weight and fast. By directly embedding the ACB into the single shot detection framework, we construct a novel real-time adaptive convolutional detector (ACD). Experiments on PASCAL VOC and MS COCO benchmarks confirm that our ACD outperforms the existing state-of-the-art single-stage detection models, and achieves a better tradeoff between accuracy and speed.
C1 [Chen, Chunlin; Ling, Qiang] Univ Sci & Technol China, Hefei 230027, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Ling, Q (corresponding author), Univ Sci & Technol China, Hefei 230027, Peoples R China.
EM johnchen@mail.ustc.edu.cn; qling@ustc.edu.cn
OI Ling, Qiang/0000-0001-5688-4130
FU Intelligent Networked Electric Vehicle Key System Integration
   Development and Industrialization Project; National Key Research and
   Development Program of China [2016YFC0201003]; Fundamental Research
   Funds for the Central Universities
FX This work was supported in part by the Intelligent Networked Electric
   Vehicle Key System Integration Development and Industrialization
   Project; in part by the National Key Research and Development Program of
   China (No. 2016YFC0201003); and in part by the Fundamental Research
   Funds for the Central Universities. The associate editor coordinating
   the review of this manuscript and approving it for publication was Prof.
   Manzur Murshed.
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   [Anonymous], IEEE I CONF COMP VIS
   [Anonymous], 2017, COMPUT RES REPOS
   [Anonymous], 2017, 31 C NEUR INF PROC S
   [Anonymous], 2017, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2017.322
   [Anonymous], 2015, NATURE, DOI [DOI 10.1038/NATURE14539, 10.1038/nature14539]
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2015, PROC CVPR IEEE
   [Anonymous], INT C NEUR INT PROC
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2010, INT J COMPUT VISION, DOI [DOI 10.1007/s11263-009-0275-4, 10.1007/s11263-009-0275-4]
   [Anonymous], 2015, PROC 28 INT C NEURAL
   Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen T., 2015, P NEUR INF PROC SYST
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036
   De Brabandere B, 2016, ADV NEUR IN, V29
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Glorot X., 2010, P INT C ART INT STAT, P249
   Goyal Priya, 2017, abs/1706.02677
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Hoiem D, 2012, LECT NOTES COMPUT SC, V7574, P340, DOI 10.1007/978-3-642-33712-3_25
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Kang D., 2017, Advances in Neural Information Processing Systems, P3867
   Klein B, 2015, PROC CVPR IEEE, P4840, DOI 10.1109/CVPR.2015.7299117
   Kong T, 2017, PROC CVPR IEEE, P5244, DOI 10.1109/CVPR.2017.557
   Li JA, 2018, IEEE T MULTIMEDIA, V20, P1645, DOI 10.1109/TMM.2017.2772796
   Li JN, 2018, IEEE T MULTIMEDIA, V20, P985, DOI 10.1109/TMM.2017.2759508
   Li JN, 2017, IEEE T MULTIMEDIA, V19, P944, DOI 10.1109/TMM.2016.2642789
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Shen ZQ, 2017, IEEE I CONF COMP VIS, P1937, DOI 10.1109/ICCV.2017.212
   Simonyan K., 2014, 14091556 ARXIV
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Zeiler MD, 2010, PROC CVPR IEEE, P2528, DOI 10.1109/CVPR.2010.5539957
   Zhou P, 2018, PROC CVPR IEEE, P528, DOI 10.1109/CVPR.2018.00062
NR 42
TC 29
Z9 29
U1 1
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2019
VL 21
IS 12
BP 3205
EP 3217
DI 10.1109/TMM.2019.2916104
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KJ8ZS
UT WOS:000512345200018
DA 2024-07-18
ER

PT J
AU Guo, YD
   Zou, BJ
   Ren, J
   Liu, QQ
   Zhang, DY
   Zhang, YX
AF Guo, Yundi
   Zou, Beiji
   Ren, Ju
   Liu, Qingqing
   Zhang, Deyu
   Zhang, Yaoxue
TI Distributed and Efficient Object Detection via Interactions Among
   Devices, Edge, and Cloud
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Object detection; Image edge detection; Image coding; Servers; Cloud
   computing; Edge computing; Real-time systems; Object detection; edge
   computing; image compression; RoI; wireless surveillance applications
AB With the rapid development of Internet-of-Things and communication techniques, media transmission in surveillance applications is gradually relying on wireless networks. Meanwhile, the emergence of edge computing has pushed the media data analysis from the cloud to the edge of the network to achieve fast response for delay-sensitive media processing tasks. Object detection is a representative delay-sensitive image processing task in surveillance applications, but faces significant challenges in this context. For example, how to compress images for transmission in wireless environment without compromising the detection accuracy, and how to integrate and update local inference models online in an edge computing-based object detection system. In this paper, we propose an object detection architecture based on edge computing to achieve distributed and efficient object detection for surveillance applications. Under this architecture, we develop an adaptive Region-of-Interest-based image compression scheme for end devices to efficiently compress their captured images for wireless transmission but not to sacrifice the object detection accuracy of edge servers. Furthermore, we carefully design distributed and communication-efficient interactions among end devices, edge servers, and the cloud to dynamically optimize the object detection accuracy online. Extensive simulation results demonstrate that our proposed architecture not only achieves a competitive detection accuracy to traditional cloud-based objective detection solution with reduced response delay but also significantly improves the image transmission efficiency with adaptive image compression ratio.
C1 [Guo, Yundi; Zou, Beiji; Ren, Ju; Liu, Qingqing; Zhang, Deyu; Zhang, Yaoxue] Cent South Univ, Sch Comp Sci & Engn, Changsha 410083, Hunan, Peoples R China.
   [Guo, Yundi; Zou, Beiji] Cent South Univ, Hunan Prov Engn Technol Res Ctr Comp Vis & Intell, Changsha 410083, Hunan, Peoples R China.
C3 Central South University; Central South University
RP Ren, J (corresponding author), Cent South Univ, Sch Comp Sci & Engn, Changsha 410083, Hunan, Peoples R China.
EM csgrandeur@csu.edu.cn; bjzou@csu.edu.cn; renju@csu.edu.cn;
   liuqingqing@csu.edu.cn; zdy876@csu.edu.cn; zyx@csu.edu.cn
RI Chen, Rainie/ISS-6016-2023; Ren, Ju/ABD-5213-2021; liu,
   qingqing/HHD-0360-2022; Liu, Qingqing/HMV-4816-2023; Ren,
   Ju/ABD-5251-2021
OI Ren, Ju/0000-0003-2782-183X
FU Natural Science Foundation of China [61702562, 61702561, 61573380]; 111
   Project [B18059]; International Science and and Technology Cooperation
   Program of China [2013DFB10070]; China Hunan Provincial Science and
   Technology Program [2012GK4106]
FX This work was supported in part by the Natural Science Foundation of
   China under Grants 61702562, 61702561 and 61573380, in part by the 111
   Project under Grant B18059, in part by the International Science and and
   Technology Cooperation Program of China under Grant 2013DFB10070, and in
   part by the China Hunan Provincial Science and Technology Program under
   Grant 2012GK4106. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Shiwen Mao.
CR Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28
   Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89
   Chen XZ, 2016, PROC CVPR IEEE, P2147, DOI 10.1109/CVPR.2016.236
   Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414
   CHO M, 2015, PROC CVPR IEEE, P1201, DOI DOI 10.1109/CVPR.2015.7298724
   Chorowski Jan K, 2015, ADV NEURAL INFORM PR, DOI [DOI 10.1016/0167-739X(94)90007-8, 10.1016/j.asr.2015.02.035, DOI 10.1016/J.ASR.2015.02.035]
   Courbariaux Y., 2015, ADV NEURAL INFORM PR, P3123, DOI DOI 10.5555/2969442.2969588
   Endres I, 2014, IEEE T PATTERN ANAL, V36, P222, DOI 10.1109/TPAMI.2013.122
   Everingham M., 2007, The PASCAL Visual Object Classes Chal- lenge (VOC) Results, DOI 10.1007/s11263-009-0275-4
   Fadlullah ZM, 2017, IEEE COMMUN SURV TUT, V19, P2432, DOI 10.1109/COMST.2017.2707140
   Fang ZW, 2016, IEEE T IMAGE PROCESS, V25, P4116, DOI 10.1109/TIP.2016.2579311
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Hare S, 2012, PROC CVPR IEEE, P1894, DOI 10.1109/CVPR.2012.6247889
   He K., 2017, IEEE INT C COMPUT VI
   Jie ZQ, 2016, IEEE T IMAGE PROCESS, V25, P4525, DOI 10.1109/TIP.2016.2593342
   Joseph RK, 2016, CRIT POL ECON S ASIA, P1
   Krähenbühl P, 2014, LECT NOTES COMPUT SC, V8693, P725, DOI 10.1007/978-3-319-10602-1_47
   Liu L, 2020, INT J COMPUT VISION, V128, P261, DOI 10.1007/s11263-019-01247-4
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Lu CW, 2015, IEEE I CONF COMP VIS, P2021, DOI 10.1109/ICCV.2015.234
   Nothdurft Hans-Christoph, 2005, P233, DOI 10.1016/B978-012375731-9/50042-2
   Pont-Tuset J, 2016, IEEE T PATTERN ANAL, V38, DOI [10.1109/TPAMI.2015.2481406, 10.1109/TPAMI.2016.2537320]
   Rabbani M, 2002, J ELECTRON IMAGING, V11, P286, DOI 10.1117/1.1469618.00000
   Redmon J., 2016, PROC CVPR IEEE, DOI [10.1109/CVPR.2016.91, DOI 10.1109/CVPR.2016.91]
   Ren J, 2018, IEEE NETWORK, V32, P137, DOI 10.1109/MNET.2018.1700415
   Ren J, 2017, IEEE NETWORK, V31, P96, DOI 10.1109/MNET.2017.1700030
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rodrigues TG, 2017, IEEE T COMPUT, V66, P810, DOI 10.1109/TC.2016.2620469
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Sun LF, 2018, IEEE T MULTIMEDIA, V20, P3414, DOI 10.1109/TMM.2018.2834861
   Tahoces PG, 2008, COMPUT VIS IMAGE UND, V109, P139, DOI 10.1016/j.cviu.2007.07.001
   Teerapittayanon S, 2017, INT CON DISTR COMP S, P328, DOI 10.1109/ICDCS.2017.226
   Tsymbal A, 2004, Computer Science Department, Trinity College Dublin
   Van den Bergh M, 2013, IEEE I CONF COMP VIS, P377, DOI 10.1109/ICCV.2013.54
   WALLACE GK, 1991, COMMUN ACM, V34, P30, DOI 10.1145/103085.103089
   Yang B, 2016, PROC CVPR IEEE, P6043, DOI 10.1109/CVPR.2016.650
   Yang P, 2019, IEEE T MULTIMEDIA, V21, P915, DOI 10.1109/TMM.2018.2870521
   Yi SH, 2015, 2015 THIRD IEEE WORKSHOP ON HOT TOPICS IN WEB SYSTEMS AND TECHNOLOGIES (HOTWEB), P73, DOI 10.1109/HotWeb.2015.22
   Zhang YX, 2017, CHINESE J ELECTRON, V26, P1, DOI 10.1049/cje.2016.11.016
   Zhu M., 2004, Recall, precision and average precision, V2, P6
NR 40
TC 26
Z9 26
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2019
VL 21
IS 11
BP 2903
EP 2915
DI 10.1109/TMM.2019.2912703
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JJ7VX
UT WOS:000494363000017
DA 2024-07-18
ER

PT J
AU Luo, CW
   Zhang, JY
   Yu, J
   Chen, CW
   Wang, SJ
AF Luo, Changwei
   Zhang, Juyong
   Yu, Jun
   Chen, Chang Wen
   Wang, Shengjin
TI Real-Time Head Pose Estimation and Face Modeling From a Depth Image
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Head pose estimation; random forest; depth image; face modeling
ID 3D; ROBUST; RECOGNITION
AB We address the issues of 3-D head pose estimation and face modeling from a depth image. Given a depth image, random forests are effective for estimating the location and orientation of a person's head. However, the accuracy of the estimation is not high enough. We propose using corrected regression votes. The corrected votes are obtained by considering the cooperation of all trees, leading to significant improvement of head pose estimation accuracy. Based on the head pose estimator, we present a face modeling system. In our system, the face model is generated by aligning a deformable face model to a depth image using an iterative closest point (ICP) algorithm. The novelty of our approach is that an optimal weight for each vertex is incorporated into the ICP algorithm with point to plane constraints. Experiments show that our system can automatically estimate the head pose and generate a realistic face model from a single depth image. We also provide a detailed evaluation that shows the benefits of our approach.
C1 [Luo, Changwei; Yu, Jun] Univ Sci & Technol China, Dept Automat, Hefei 230027, Anhui, Peoples R China.
   [Luo, Changwei; Wang, Shengjin] Tsinghua Univ, Dept Elect Engn, Beijing 100084, Peoples R China.
   [Luo, Changwei] Acad Mil Sci, Beijing, Peoples R China.
   [Zhang, Juyong] Univ Sci & Technol China, Sch Math Sci, Hefei 230027, Anhui, Peoples R China.
   [Chen, Chang Wen] SUNY Buffalo, Dept Comp Sci & Engn, Buffalo, NY 14260 USA.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Tsinghua University; Chinese Academy of Sciences; University
   of Science & Technology of China, CAS; State University of New York
   (SUNY) System; State University of New York (SUNY) Buffalo
RP Yu, J (corresponding author), Univ Sci & Technol China, Dept Automat, Hefei 230027, Anhui, Peoples R China.
EM luocw@mail.ustc.edu.cn; juyong@ustc.edu.cn; harryjun@ustc.edu.cn;
   chencw@buffalo.edu; wgsgj@tsinghua.edu.cn
OI Chen, Chang Wen/0000-0002-6720-234X
FU National Natural Science Foundation of China [U1736123, 61572450]; Anhui
   Provincial Natural Science Foundation [1708085QF138]; Beijing Dilusense
   Technology Corporation
FX This work was supported in part by the National Natural Science
   Foundation of China (U1736123 and 61572450), in part by the Anhui
   Provincial Natural Science Foundation (1708085QF138), and in part by the
   Beijing Dilusense Technology Corporation.
CR [Anonymous], P BRIT MACH VIS C
   [Anonymous], P IEEE INT C AUT FAC
   [Anonymous], 2013, TOG
   [Anonymous], P SIGGRAPH
   Balasubramanian V., 2007, P IEEE C COMP VIS PA
   BenAbdelkader C, 2010, LECT NOTES COMPUT SC, V6316, P518, DOI 10.1007/978-3-642-15567-3_38
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Booth J, 2018, INT J COMPUT VISION, V126, P233, DOI 10.1007/s11263-017-1009-7
   Cao C, 2014, IEEE T VIS COMPUT GR, V20, P413, DOI 10.1109/TVCG.2013.249
   Chen Huang, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P934, DOI 10.1109/ICPR.2010.234
   Fanelli Gabriele, 2011, Pattern Recognition. Proceedings 33rd DAGM Symposium, P101, DOI 10.1007/978-3-642-23123-0_11
   Fanelli G, 2013, INT J COMPUT VISION, V101, P437, DOI 10.1007/s11263-012-0549-0
   Ferrera C, 2017, INT J CARDIOL, V249, P410, DOI 10.1016/j.ijcard.2017.09.170
   Geng X, 2005, IEEE T SYST MAN CY B, V35, P1098, DOI 10.1109/TSMCB.2005.850151
   Hernandez M, 2015, IMAGE VISION COMPUT, V36, P61, DOI 10.1016/j.imavis.2014.12.004
   Hsieh PL, 2015, PROC CVPR IEEE, P1675, DOI 10.1109/CVPR.2015.7298776
   Kazemi Vahid, 2014, 2014 2nd International Conference on 3D Vision (3DV). Proceedings, P369, DOI 10.1109/3DV.2014.93
   Kemelmacher-Shlizerman I, 2011, IEEE T PATTERN ANAL, V33, P394, DOI 10.1109/TPAMI.2010.63
   Liang J, 2016, INT CONF ADV COMMUN, P264, DOI 10.1109/ICACT.2016.7423354
   Lindner C, 2015, IEEE T PATTERN ANAL, V37, P1862, DOI 10.1109/TPAMI.2014.2382106
   Liu MY, 2018, IEEE T MULTIMEDIA, V20, P1932, DOI 10.1109/TMM.2017.2786868
   Luo C., 2014, P IEEE INT C MULT EX
   Luo CW, 2015, IEEE SIGNAL PROC LET, V22, P2324, DOI 10.1109/LSP.2015.2480758
   Meyer GP, 2015, IEEE I CONF COMP VIS, P3649, DOI 10.1109/ICCV.2015.416
   Mukherjee SS, 2015, IEEE T MULTIMEDIA, V17, P2094, DOI 10.1109/TMM.2015.2482819
   Papazov C, 2015, PROC CVPR IEEE, P4722, DOI 10.1109/CVPR.2015.7299104
   Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58
   Peng Liu, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P794, DOI 10.1109/ICME.2012.61
   Ren SQ, 2014, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2014.218
   Ren Z, 2013, IEEE T MULTIMEDIA, V15, P1110, DOI 10.1109/TMM.2013.2246148
   Romdhani S, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P59
   Schulter S, 2013, IEEE I CONF COMP VIS, P417, DOI 10.1109/ICCV.2013.59
   Tan DJ, 2015, 2015 INTERNATIONAL CONFERENCE ON 3D VISION, P500, DOI 10.1109/3DV.2015.62
   Venturelli M., 2017, P INT WORKSH UND HUM, P74
   Zhang GY, 2017, IEEE SIGNAL PROC LET, V24, P1666, DOI 10.1109/LSP.2017.2731952
   Zhou QY, 2016, LECT NOTES COMPUT SC, V9906, P766, DOI 10.1007/978-3-319-46475-6_47
   Zollhöfer M, 2011, COMPUT ANIMAT VIRT W, V22, P195, DOI 10.1002/cav.405
NR 37
TC 22
Z9 24
U1 0
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2019
VL 21
IS 10
BP 2473
EP 2481
DI 10.1109/TMM.2019.2903724
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JD1IM
UT WOS:000489728400004
DA 2024-07-18
ER

PT J
AU Zheng, S
   Zhang, XP
   Chen, J
   Kuo, YH
AF Zheng, Shuai
   Zhang, Xiao-Ping
   Chen, Jian
   Kuo, Yonghong
TI A High-Efficiency Compressed Sensing-Based Terminal-to-Cloud Video
   Transmission System
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Compressed sensing; terminal-to-cloud upload network; multi-reference
   frames; local secondary reconstruction based cross recovery
ID COST-EFFICIENT; MEDIA CLOUD; SELECTION
AB With the rapid popularization of mobile intelligent terminals, mobile video and cloud services applications are widely used in people's lives. However, the resource-constrained characteristic of the terminals and the enormous amount of video information make the efficient terminal-to-cloud data upload a challenge. To solve the problem, this paper proposes an efficient compressed sensing-based high-efficiency video upload system for the terminal-to-cloud upload network. The system contains two main new components. First, to effectively remove the inter-frame redundant information, an encoder sampling scheme with high efficiency is developed by applying the skip block-based residual compressed sensing sampling technology. For the time-varying channel state, the encoder can adaptively allocate the sampling rate for different video frames by the proposed adaptive sampling scheme. Second, a local secondary reconstruction-based multi-reference frames cross-recovery algorithm is developed at the decoder. It further improves the reconstruction quality and reduces the quality fluctuation of the recovered video frames to improve the user experience. Compared with the state-of-the-art reference systems reported in the literature, the proposed system achieves the high-efficiency and high-quality terminal-to-cloud transmission.
C1 [Zheng, Shuai; Chen, Jian; Kuo, Yonghong] Xidian Univ, Sch Telecommun Engn, Xian 710071, Shaanxi, Peoples R China.
   [Zhang, Xiao-Ping] Ryerson Univ, Dept Elect & Comp Engn, Toronto, ON M5B 2K3, Canada.
C3 Xidian University; Toronto Metropolitan University
RP Chen, J (corresponding author), Xidian Univ, Sch Telecommun Engn, Xian 710071, Shaanxi, Peoples R China.
EM zhs_xd@163.com; xzhang@ee.ryerson.ca; jianchen@mail.xidian.edu.cn;
   yhkuo@mail.xidian.edu.cn
RI Zhang, Xiao-Ping (Steven)/B-1436-2016; Zheng, Shuai/AAH-5647-2020; KUO,
   Yong-Hong/M-9078-2015
OI Zhang, Xiao-Ping (Steven)/0000-0001-5241-0069; Zheng,
   Shuai/0000-0002-2570-0105
FU National Natural Science Foundation of China [61771366]; "111" project
   [B08038]; Natural Sciences and Engineering Research Council of Canada
   [RGPIN239031]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61771366, in part by the "111" project
   under Grant B08038, and in part by the Natural Sciences and Engineering
   Research Council of Canada under Grant RGPIN239031. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Sanjeev Mehrotra. (Corresponding author: Jian Chen.)
CR [Anonymous], 2014, J CHONGQING U, DOI DOI 10.11835/J.ISSN.1000-582X.2014.05.014
   Azghani M, 2016, IEEE T CIRC SYST VID, V26, P627, DOI 10.1109/TCSVT.2015.2418586
   Candes E. J., 2006, PROC INT C MATH, V17, P1433, DOI DOI 10.4171/022-3/69
   Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979
   Chen J, 2018, MULTIMED TOOLS APPL, V77, P14873, DOI 10.1007/s11042-017-5071-5
   Chen J, 2017, MULTIMED TOOLS APPL, V76, P15735, DOI 10.1007/s11042-016-3866-4
   Chen J, 2017, CIRC SYST SIGNAL PR, V36, P1621, DOI 10.1007/s00034-016-0432-2
   Chen J, 2015, MULTIMED TOOLS APPL, V74, P2085, DOI 10.1007/s11042-013-1743-y
   Chen Jian, 2014, [自动化学报, Acta Automatica Sinica], V40, P2316
   Dai XL, 2017, IEEE T CIRC SYST VID, V27, P73, DOI 10.1109/TCSVT.2016.2565918
   Do TT, 2009, IEEE IMAGE PROC, P1393, DOI 10.1109/ICIP.2009.5414631
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Fan N., 2013, IEEE VEH TECHN C, P1
   Fowler JE, 2010, FOUND TRENDS SIGNAL, V4, P297, DOI 10.1561/2000000033
   Gao GY, 2015, IEEE T MULTIMEDIA, V17, P1286, DOI 10.1109/TMM.2015.2438713
   Guo J, 2016, IEEE T MULTIMEDIA, V18, P1297, DOI 10.1109/TMM.2016.2564100
   Hong S, 2014, PR IEEE COMP DESIGN, P83, DOI 10.1109/ICCD.2014.6974666
   Hosseini MS, 2014, IEEE T IMAGE PROCESS, V23, P3869, DOI 10.1109/TIP.2014.2332755
   Jin YC, 2015, IEEE T CIRC SYST VID, V25, P1914, DOI 10.1109/TCSVT.2015.2402892
   Kang LW, 2009, INT CONF ACOUST SPEE, P1169, DOI 10.1109/ICASSP.2009.4959797
   Kuo YH, 2017, MULTIDIM SYST SIGN P, V28, P129, DOI 10.1007/s11045-015-0337-4
   Kuo YH, 2013, ELECTRON LETT, V49, P991, DOI 10.1049/el.2013.0345
   Li CB, 2013, IEEE T BROADCAST, V59, P197, DOI 10.1109/TBC.2012.2226509
   Li Ran, 2015, INT J DISTRIB SENS N, V2015, P1, DOI DOI 10.1155/2015/843802
   Li XB, 2018, IEEE T PARALL DISTR, V29, P556, DOI 10.1109/TPDS.2017.2766069
   Li XB, 2016, PROCEEDINGS OF 2016 IEEE INTERNATIONAL CONFERENCES ON BIG DATA AND CLOUD COMPUTING (BDCLOUD 2016) SOCIAL COMPUTING AND NETWORKING (SOCIALCOM 2016) SUSTAINABLE COMPUTING AND COMMUNICATIONS (SUSTAINCOM 2016) (BDCLOUD-SOCIALCOM-SUSTAINCOM 2016), P595, DOI 10.1109/BDCloud-SocialCom-SustainCom.2016.93
   Liu You-peng, 2012, J ENV ENG TECHNOLOGY, V1, P8
   Liu ZR, 2011, IEEE T CIRC SYST VID, V21, P1704, DOI 10.1109/TCSVT.2011.2133890
   Mun S, 2009, IEEE IMAGE PROC, P3021, DOI 10.1109/ICIP.2009.5414429
   Mun S, 2012, EUR SIGNAL PR CONF, P1424
   Mun S, 2011, IEEE DATA COMPR CONF, P183, DOI 10.1109/DCC.2011.25
   Pudlewski S, 2012, IEEE T MOBILE COMPUT, V11, P1060, DOI 10.1109/TMC.2011.175
   Pudlewski Scott., 2010, 2010 IEEE International Conference on Communications (ICC), P1
   Rehman AU, 2016, INT WIREL COMMUN, P170, DOI 10.1109/IWCMC.2016.7577052
   Roohi S., 2014, P C MACH VIS IM PROC, P53
   Sarwer Mohammed Golam, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P3465, DOI 10.1109/ICIP.2011.6116459
   Sarwer M, 2013, SIGNAL IMAGE VIDEO P, V7, P777, DOI 10.1007/s11760-011-0267-z
   Song XD, 2017, IEEE T MULTIMEDIA, V19, P1351, DOI 10.1109/TMM.2017.2654123
   Song XD, 2015, IEEE T CIRC SYST VID, V25, P1926, DOI 10.1109/TCSVT.2015.2416562
   Song Y, 2017, MULTIMED TOOLS APPL, V76, P10083, DOI 10.1007/s11042-016-3599-4
   Tramel EW, 2011, IEEE DATA COMPR CONF, P193, DOI 10.1109/DCC.2011.26
   Yu Shuang, 2014, 2014 International Conference on Communications and Signal Processing (ICCSP), P995, DOI 10.1109/ICCSP.2014.6949995
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang SW, 2014, IEEE INT CONF MOB, P222, DOI 10.1109/MASS.2014.37
   Zhu J., 2013, INT J DISTRIB SENS N, V9, P718
   Zhu JX, 2014, 2014 9TH INTERNATIONAL CONFERENCE ON COMMUNICATIONS AND NETWORKING IN CHINA (CHINACOM), P625, DOI 10.1109/CHINACOM.2014.7054371
   Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x
NR 47
TC 23
Z9 26
U1 0
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2019
VL 21
IS 8
BP 1905
EP 1920
DI 10.1109/TMM.2019.2891415
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IK7YM
UT WOS:000476809700002
DA 2024-07-18
ER

PT J
AU Cabrera-Quiros, L
   Hung, H
AF Cabrera-Quiros, Laura
   Hung, Hayley
TI A Hierarchical Approach for Associating Body-Worn Sensors to Video
   Regions in Crowded Mingling Scenarios
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Mingling; wearable sensor; acceleration; computer vision; association
ID ACTIVITY RECOGNITION; SPEAKER DIARIZATION; DOMINANCE; ROLES
AB We address the complex problem of associating several wearable devices with the spatio-temporal region of their wearers in video during crowded mingling events using only acceleration and proximity. This is a particularly important first step for multisensor behavior analysis using video and wearable technologies, where the privacy of the participants must be maintained. Most state-of-the-art works using these two modalities perform their association manually, which becomes practically unfeasible as the number of people in the scene increases. We proposed an automatic association method based on a hierarchical linear assignment optimization, which exploits the spatial context of the scene. Moreover, we present extensive experiments on matching from 2 to more than 69 acceleration and video streams, showing significant improvements over a random baseline in a real-world crowded mingling scenario. We also show the effectiveness of our method for incomplete or missing streams (up to a certain limit) and analyze the tradeoff between length of the streams and number of participants. Finally, we provide an analysis of failure cases, showing that deep understanding of the social actions within the context of the event is necessary to further improve performance on this intriguing task.
C1 [Cabrera-Quiros, Laura; Hung, Hayley] Delft Univ Technol, Dept Intelligent Syst, NL-2628 CD Delft, Netherlands.
   [Cabrera-Quiros, Laura] Inst Tecnol, Escuela Ingn Elect, Cartago 30101, Costa Rica.
C3 Delft University of Technology; Instituto Tecnologico de Costa Rica
RP Cabrera-Quiros, L (corresponding author), Delft Univ Technol, Dept Intelligent Syst, NL-2628 CD Delft, Netherlands.
EM l.c.cabreraquiros@tudelft.nl; h.hung@tudelft.nl
RI Hung, Hayley/AAS-2215-2021
OI Hung, Hayley/0000-0003-0719-8948
FU Instituto Tecnologico de Costa Rica
FX This work was partially supported by the Instituto Tecnologico de Costa
   Rica.
CR Alameda-Pineda X, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P5, DOI 10.1145/2733373.2806238
   Miro XA, 2012, IEEE T AUDIO SPEECH, V20, P356, DOI 10.1109/TASL.2011.2125954
   [Anonymous], 2013, P ACMMM, DOI DOI 10.1145/2502081.2502096
   [Anonymous], P AS C COMP VIS
   [Anonymous], 2016, Proceedings of the 2016 ACM on Multimedia Conference
   Bahle G, 2013, INT CONF PERVAS COMP, P409
   Burkard B., 2009, ASSIGNMENT PROBLEMS
   Cabrera-Quiros L, 2021, IEEE T AFFECT COMPUT, V12, P113, DOI 10.1109/TAFFC.2018.2848914
   Cristani M, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.23
   Delaherche E, 2012, IEEE T AFFECT COMPUT, V3, P349, DOI 10.1109/T-AFFC.2012.12
   Dong W, 2013, IEEE T MULTIMEDIA, V15, P83, DOI 10.1109/TMM.2012.2225039
   Gedik E, 2017, PERS UBIQUIT COMPUT, V21, P723, DOI 10.1007/s00779-017-1006-4
   Hung H., 2014, P ACM INT C MULT INT
   Hung H, 2011, IEEE T AUDIO SPEECH, V19, P847, DOI 10.1109/TASL.2010.2066267
   Hung H, 2010, IEEE T MULTIMEDIA, V12, P563, DOI 10.1109/TMM.2010.2055233
   Hung Hayley, 2011, P 13 INT C MULT INT, P231
   Huynh T., 2005, P 2005 JOINT C SMART, P159
   Jayagopi DB, 2009, IEEE T AUDIO SPEECH, V17, P501, DOI 10.1109/TASL.2008.2008238
   Lara OD, 2013, IEEE COMMUN SURV TUT, V15, P1192, DOI 10.1109/SURV.2012.110112.00192
   Martella C, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P201, DOI 10.1145/2733373.2806276
   Martella C, 2014, INT CONF PERVAS COMP, P78, DOI 10.1109/PerCom.2014.6813947
   Nguyen LT, 2014, UBICOMP'14: PROCEEDINGS OF THE 2014 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING, P529, DOI 10.1145/2632048.2636072
   Pentico DW, 2007, EUR J OPER RES, V176, P774, DOI 10.1016/j.ejor.2005.09.014
   Plötz T, 2012, IEEE INT SYM WRBL CO, P100, DOI 10.1109/ISWC.2012.15
   Rofouei M., 2012, P SIGCHI C HUM FACT, P1915
   Sanchez-Cortes D, 2012, IEEE T MULTIMEDIA, V14, P816, DOI 10.1109/TMM.2011.2181941
   Sapru A, 2015, IEEE T MULTIMEDIA, V17, P746, DOI 10.1109/TMM.2015.2408437
   Shigeta O, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P3872, DOI 10.1109/IROS.2008.4651201
   Stein S, 2013, UBICOMP'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING, P729, DOI 10.1145/2493432.2493482
   Teixeira T, 2010, UBICOMP 2010: PROCEEDINGS OF THE 2010 ACM CONFERENCE ON UBIQUITOUS COMPUTING, P213
   Vallet F, 2013, IEEE T MULTIMEDIA, V15, P509, DOI 10.1109/TMM.2012.2233724
   Vondrick Carl., 2012, Int J of Comput Vision, P1
   Wilson AndrewD., 2014, Proceedings of the 16th International Conference on Multimodal Interaction, P216, DOI DOI 10.1145/2663204.2663270
   Zhang L., 2013, P IEEE CONP VIS PATT, P184
NR 34
TC 3
Z9 3
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2019
VL 21
IS 7
BP 1867
EP 1879
DI 10.1109/TMM.2018.2888798
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IF6IF
UT WOS:000473183700020
OA Green Published
DA 2024-07-18
ER

PT J
AU Fujihashi, T
   Koike-Akino, T
   Watanabe, T
   Orlik, PV
AF Fujihashi, Takuya
   Koike-Akino, Toshiaki
   Watanabe, Takashi
   Orlik, Philip V.
TI FreeCast: Graceful Free-Viewpoint Video Delivery
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Free viewpoint video; graceful video delivery
ID MULTIVIEW VIDEO; TRANSMISSION; SYSTEM; DIBR
AB Wireless multi-view plus depth (MVD) video streaming enables free viewpoint video playback on wireless devices, where a viewer can freely synthesize any preferred virtual viewpoint from the received MVD frames. Existing schemes of wireless MVD streaming use digital-based compression to achieve better coding efficiency. However, the digital-based schemes have an issue called the cliff effect, where the video quality is a step function in terms of wireless channel quality. In addition, parameter optimization to assign quantization levels and transmission power across MVD frames are cumbersome. To realize high-quality wireless MVD video streaming, we propose a novel graceful video delivery scheme, called FreeCast. FreeCast directly transmits linear-transformed signals based on 5-D discrete cosine transform, without digital quantization and entropy coding operations. In addition, we exploit a fitting function based on a multidimensional Gaussian Markov random field model for overhead reduction to mitigate rate and power loss due to large overhead. The proposed FreeCast achieves graceful video quality with the improvement of wireless channel quality under a low overhead requirement. In addition, the parameter optimization to achieve highest video quality can be simplified by only controlling a transmission power assignment. Performance results with several test MVD video sequences show that FreeCast yields better video quality in band-limited environments by significantly decreasing the amount of overhead. For instance, structural similarity (SSIM) performance of FreeCast is approximately 0.127 higher than the existing graceful video delivery schemes across wireless channel quality, i.e., signal-to-noise ratio, of 0-25 dB at a transmission symbol rate of 37.5 Msymbols/s.
C1 [Fujihashi, Takuya] Ehime Univ, Grad Sch Sci & Engn, Matsuyama, Ehime 7908577, Japan.
   [Koike-Akino, Toshiaki; Orlik, Philip V.] Mitsubishi Elect Res Labs, Cambridge, MA 02139 USA.
   [Watanabe, Takashi] Osaka Univ, Grad Sch Informat Sci & Technol, Suita, Osaka 5650871, Japan.
C3 Ehime University; Osaka University
RP Fujihashi, T (corresponding author), Ehime Univ, Grad Sch Sci & Engn, Matsuyama, Ehime 7908577, Japan.
EM fujihashi@cs.ehime-u.ac.jp; koike@merl.com; watanabe@ist.osaka-u.ac.jp;
   porlik@merl.com
RI Fujihashi, Takuya/Y-1527-2019; Koike-Akino, Toshiaki/T-2062-2019
OI Fujihashi, Takuya/0000-0002-6960-0122; Koike-Akino,
   Toshiaki/0000-0002-2578-5372; WATANABE, TAKASHI/0000-0002-3227-9048
FU JSPS KAKENHI [17K12672]; Grants-in-Aid for Scientific Research
   [17K12672] Funding Source: KAKEN
FX The work of T. Fujihashi was partly supported by JSPS KAKENHI under
   Grant 17K12672. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Jun Wu.
   (Corresponding author: Takuya Fujihashi.)
CR [Anonymous], IEEE ICC
   [Anonymous], 2018, HTM130
   Candès EJ, 2008, IEEE SIGNAL PROC MAG, V25, P21, DOI 10.1109/MSP.2007.914731
   Chen Y, 2014, J VIS COMMUN IMAGE R, V25, P679, DOI 10.1016/j.jvcir.2013.03.013
   Chen Ying, 2013, Document JCT3VD1002.doc. JCT-3V
   Chen Z, 2017, IEEE T MULTIMEDIA, V19, P2788, DOI 10.1109/TMM.2017.2713414
   Cui H, 2016, IEEE T CIRC SYST VID, V26, P992, DOI 10.1109/TCSVT.2015.2430651
   Cui H, 2015, IEEE J-STSP, V9, DOI 10.1109/JSTSP.2014.2338279
   De Abreu A, 2015, IEEE J-STSP, V9, P487, DOI 10.1109/JSTSP.2015.2407320
   Ekmekcioglu E, 2017, IEEE T CIRC SYST VID, V27, P1313, DOI 10.1109/TCSVT.2016.2527318
   Fan XP, 2015, IEEE T CIRC SYST VID, V25, P1801, DOI 10.1109/TCSVT.2015.2402831
   Fehn C, 2004, PROC SPIE, V5291, P93, DOI 10.1117/12.524762
   Fujihashi T., 2015, IEEE GLOBAL COMMUNIC, P1
   Fujihashi T, 2018, IEEE T MULTIMEDIA, V20, P473, DOI 10.1109/TMM.2017.2743984
   He DL, 2017, IEEE T MULTIMEDIA, V19, P1894, DOI 10.1109/TMM.2017.2686703
   He DL, 2015, ACM S MODEL ANAL SIM, P327, DOI 10.1145/2811587.2811601
   Jakubczak S., 2011, PROC MOBICOM, P289
   Kim WS, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2447737
   Lei  J., 2016, IEEE T CIRCUITS SYST, V28, P706
   Li S, 2018, IEEE T MULTIMEDIA, V20, P1948, DOI 10.1109/TMM.2018.2791810
   Liu XL, 2014, IEEE T MULTIMEDIA, V16, P2038, DOI 10.1109/TMM.2014.2331616
   Liu XL, 2014, SIGNAL PROCESS-IMAGE, V29, P361, DOI 10.1016/j.image.2014.01.005
   Müller K, 2013, IEEE T IMAGE PROCESS, V22, P3366, DOI 10.1109/TIP.2013.2264820
   Shao F, 2016, IEEE T BROADCAST, V62, P94, DOI 10.1109/TBC.2015.2496818
   Song XD, 2017, IEEE T MULTIMEDIA, V19, P1351, DOI 10.1109/TMM.2017.2654123
   Stankiewicz O, 2018, IEEE T MULTIMEDIA, V20, P2182, DOI 10.1109/TMM.2018.2790162
   Suenaga R, 2015, PROC SPIE, V9393, DOI 10.1117/12.2077524
   Tan B, 2017, IEEE T MULTIMEDIA, V19, P2293, DOI 10.1109/TMM.2017.2733303
   Vetro  A., 2013, JCT3VF1011
   Wang AH, 2014, SIGNAL PROCESS-IMAGE, V29, P599, DOI 10.1016/j.image.2014.03.002
   Wang GH, 2014, IEEE INFOCOM SER, P2454, DOI 10.1109/INFOCOM.2014.6848191
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xiong RQ, 2016, IEEE T IMAGE PROCESS, V25, P1820, DOI 10.1109/TIP.2016.2535288
   Yu L, 2014, IEEE T CIRC SYST VID, V24, P331, DOI 10.1109/TCSVT.2013.2273675
NR 34
TC 14
Z9 14
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2019
VL 21
IS 4
BP 1000
EP 1010
DI 10.1109/TMM.2018.2870074
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HQ4XD
UT WOS:000462413700015
DA 2024-07-18
ER

PT J
AU Fu, KR
   Zhao, QJ
   Gu, IYH
AF Fu, Keren
   Zhao, Qijun
   Gu, Irene Yu-Hua
TI Refinet: A Deep Segmentation Assisted Refinement Network for Salient
   Object Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Salient object detection; refinement; convolutional network; image
   segmentation
ID REGION DETECTION; IMAGE; ATTENTION; CONTRAST; MODEL; COLOR
AB Compared to conventional saliency detection by handcrafted features, deep convolutional neural networks (CNNs) recently have been successfully applied to saliency detection field with superior performance on locating salient objects. However, due to repeated sub-sampling operations inside CNNs such as pooling and convolution, many CNN-based saliency models fail to maintain fine-grained spatial details and boundary structures of objects. To remedy this issue, this paper proposes a novel end-to-end deep learning-based refinement model named Refinet, which is based on fully convolutional network augmented with segmentation hypotheses. Intermediate saliency maps that are edge-aware are computed from segmentation-based pooling and then feed to a two-tier fully convolutional network for effective fusion and refinement, leading to more precise object details and boundaries. In addition, the resolution of feature maps in the proposed Refinet is carefully designed to guarantee sufficient boundary clarity of the refined saliency output. Compared to widely employed dense conditional random field, Refinet is able to enhance coarse saliency maps generated by existing models with more accurate spatial details, and its effectiveness is demonstrated by experimental results on seven benchmark datasets.
C1 [Fu, Keren; Zhao, Qijun] Sichuan Univ, Coll Comp Sci, Chengdu 610065, Sichuan, Peoples R China.
   [Gu, Irene Yu-Hua] Chalmers Univ Technol, Dept Elect Engn, S-41296 Gothenburg, Sweden.
C3 Sichuan University; Chalmers University of Technology
RP Zhao, QJ (corresponding author), Sichuan Univ, Coll Comp Sci, Chengdu 610065, Sichuan, Peoples R China.
EM fkrsuper@scu.edu.cn; qjzhao@scu.edu.cn; irenegu@chalmers.se
RI Fu, Keren/HPG-4742-2023; Zhao, QiJun/KIH-9623-2024; Gu, Irene
   Yu-Hua/D-4044-2018
OI Fu, Keren/0000-0002-3195-2077; Gu, Irene Yu-Hua/0000-0003-4759-7038
FU National Science Foundation, China [61703077, 61773270]; Fundamental
   Research Funds for the Central Universities [YJ201755]; National Key
   Research and Development Program of China [2017YFB0802300,
   2016YFC0801100]; National Key Scientific Instrument and Equipment
   Development Projects of China [2013YQ49087904]
FX This research is supported in part by the National Science Foundation,
   China, under Grants 61703077 and 61773270, in part by the Fundamental
   Research Funds for the Central Universities under Grant YJ201755, in
   part by the National Key Research and Development Program of China under
   Grants 2017YFB0802300 and 2016YFC0801100, and in part by the National
   Key Scientific Instrument and Equipment Development Projects of China
   under Grant 2013YQ49087904. The associate editor coordinating the review
   of this manuscript and approving it for publication was Prof. Chang-Su
   Kim.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Alpert S., 2007, PROC IEEE INT C COMP, P1
   [Anonymous], 2006, ACM T GRAPH
   [Anonymous], 2017, IEEE C COMP VIS PATT
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2011, ADV NEURAL INF PROCE
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2015, IEEE T IMAGE PROCESS, DOI DOI 10.1109/TIP.2015.2487833
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2017, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2017.120
   [Anonymous], VERY DEEP CONVOLUTIO
   [Anonymous], PROC CVPR IEEE
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89
   Chen XW, 2017, IEEE I CONF COMP VIS, P1050, DOI 10.1109/ICCV.2017.119
   Cheng MM, 2016, LECT NOTES COMPUT SC, V9907, P867, DOI 10.1007/978-3-319-46487-9_53
   Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401
   Cheng MM, 2013, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2013.193
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Ding YY, 2011, PROC CVPR IEEE, P89, DOI 10.1109/CVPR.2011.5995445
   Dollár P, 2013, IEEE I CONF COMP VIS, P1841, DOI 10.1109/ICCV.2013.231
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Fu KR, 2017, IEEE T MULTIMEDIA, V19, P1531, DOI 10.1109/TMM.2017.2679898
   Fu KR, 2015, IEEE T IMAGE PROCESS, V24, P5671, DOI 10.1109/TIP.2015.2485782
   Fu KR, 2013, SIGNAL PROCESS-IMAGE, V28, P1448, DOI 10.1016/j.image.2013.07.005
   Gao Y, 2015, IEEE T MULTIMEDIA, V17, P359, DOI 10.1109/TMM.2015.2389616
   Goferman S, 2012, IEEE T PATTERN ANAL, V34, P1915, DOI 10.1109/TPAMI.2011.272
   Gopalakrishnan V, 2009, IEEE T MULTIMEDIA, V11, P892, DOI 10.1109/TMM.2009.2021726
   Guo CL, 2010, IEEE T IMAGE PROCESS, V19, P185, DOI 10.1109/TIP.2009.2030969
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Hu P, 2017, PROC CVPR IEEE, P540, DOI 10.1109/CVPR.2017.65
   Hu P, 2016, IEEE T IMAGE PROCESS, V25, P4653, DOI 10.1109/TIP.2016.2594489
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jerripothula KR, 2016, IEEE T MULTIMEDIA, V18, P1896, DOI 10.1109/TMM.2016.2576283
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jia YQ, 2013, IEEE I CONF COMP VIS, P1761, DOI 10.1109/ICCV.2013.221
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Jiang P, 2013, IEEE I CONF COMP VIS, P1976, DOI 10.1109/ICCV.2013.248
   Kopf J, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239453
   Kruthiventi SSS, 2016, PROC CVPR IEEE, P5781, DOI 10.1109/CVPR.2016.623
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee GY, 2016, PROC CVPR IEEE, P660, DOI 10.1109/CVPR.2016.78
   Li GB, 2016, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2016.58
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li X, 2016, IEEE T IMAGE PROCESS, V25, P3919, DOI 10.1109/TIP.2016.2579306
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Liang M, 2015, PROC CVPR IEEE, P3367, DOI 10.1109/CVPR.2015.7298958
   Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Liu Z, 2012, IEEE T MULTIMEDIA, V14, P1275, DOI 10.1109/TMM.2012.2190385
   Luo ZM, 2017, PROC CVPR IEEE, P6593, DOI 10.1109/CVPR.2017.698
   Ma YF, 2005, IEEE T MULTIMEDIA, V7, P907, DOI 10.1109/TMM.2005.854410
   Marchesotti L, 2009, IEEE I CONF COMP VIS, P2232, DOI 10.1109/ICCV.2009.5459467
   Nair Vinod, 2010, INT C INT C MACHINE, P807
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Quan R, 2018, IEEE T MULTIMEDIA, V20, P1101, DOI 10.1109/TMM.2017.2763780
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Rutishauser U, 2004, PROC CVPR IEEE, P37
   Scharfenberger C, 2015, IEEE T IMAGE PROCESS, V24, P457, DOI 10.1109/TIP.2014.2380351
   Shen XH, 2012, PROC CVPR IEEE, P853, DOI 10.1109/CVPR.2012.6247758
   Shi JP, 2016, IEEE T PATTERN ANAL, V38, P717, DOI 10.1109/TPAMI.2015.2465960
   Stentiford F., 2007, Proceedings of the International Conference on Computer Vision Systems, P1
   Wang HS, 2017, PROC CVPR IEEE, P3633, DOI 10.1109/CVPR.2017.387
   Wang KZ, 2015, IEEE T IMAGE PROCESS, V24, P3019, DOI 10.1109/TIP.2015.2432712
   Wang LJ, 2015, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR.2015.7298938
   Wang LZ, 2016, LECT NOTES COMPUT SC, V9908, P825, DOI 10.1007/978-3-319-46493-0_50
   Wang TT, 2017, IEEE I CONF COMP VIS, P4039, DOI 10.1109/ICCV.2017.433
   Wei YC, 2012, LECT NOTES COMPUT SC, V7574, P29, DOI 10.1007/978-3-642-33712-3_3
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Ye LW, 2017, IEEE T MULTIMEDIA, V19, P1742, DOI 10.1109/TMM.2017.2693022
   Zhai Y., 2006, P 14 ACM INT C MULT, P815, DOI [DOI 10.1145/1180639.1180824, 10.1145/1180639.1180824]
   Zhang DW, 2017, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2017.436
   Zhang PP, 2017, IEEE I CONF COMP VIS, P202, DOI 10.1109/ICCV.2017.31
   Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731
NR 75
TC 51
Z9 53
U1 1
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2019
VL 21
IS 2
BP 457
EP 469
DI 10.1109/TMM.2018.2859746
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HJ7GT
UT WOS:000457364400015
OA Bronze
DA 2024-07-18
ER

PT J
AU Aguilar, E
   Remeseiro, B
   Bolaños, M
   Radeva, P
AF Aguilar, Eduardo
   Remeseiro, Bealriz
   Bolanos, Marc
   Radeva, Petia
TI Grab, Pay, and Eat: Semantic Food Detection for Smart Restaurants
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Food tray analysis; food recognition; semantic segmentation;
   convolutional neural networks
ID RECOGNITION; CONTEXT
AB The increase in awareness of people toward their nutritional habits has drawn considerable attention to the field of automatic food analysis. Focusing on self-service restaurants environment, automatic food analysis is not only useful for extracting nutritional information from foods selected by customers, it is also of high interest to speed up the service solving the bottleneck produced at the cashiers in times of high demand. In this paper, we address the problem of automatic food tray analysis in canteens and restaurants environment, which consists in predicting multiple foods placed on a tray image. We propose a new approach for food analysis based on convolutional neural networks, we name Semantic Food Detection, which integrates in the same framework food localization, recognition and segmentation. We demonstrate that our method improves the state-of-art food detection by a considerable margin on the public dataset UNIMIB2016, achieving about 90% in terms of F-measure, and thus provides a significant technological advance toward the automatic billing in restaurant environments.
C1 [Aguilar, Eduardo] Univ Catolica Norte, Dept Ingn Sistemas & Comp, Antofagasta 0610, Chile.
   [Remeseiro, Bealriz; Bolanos, Marc; Radeva, Petia] Univ Barcelona, Dept Matemat & Informat, E-08007 Barcelona, Spain.
   [Remeseiro, Bealriz] Univ Oviedo, Dept Comp Sci, Gijon 33203, Spain.
   [Bolanos, Marc; Radeva, Petia] Comp Vis Ctr, Cerdanyola Del Valles 08007, Barcelona, Spain.
C3 Universidad Catolica del Norte; University of Barcelona; University of
   Oviedo; Centre de Visio per Computador (CVC)
RP Remeseiro, B (corresponding author), Univ Barcelona, Dept Matemat & Informat, E-08007 Barcelona, Spain.
EM eaguilar02@ucn.cl; breme-seiro@uniovi.es; marc.bolanos@ub.edu;
   petia.ivanova@ub.edu
RI Remeseiro, Beatriz/N-3791-2014; Aguilar Torres, Eduardo/JDD-8273-2023
OI Remeseiro, Beatriz/0000-0001-9265-253X; Aguilar,
   Eduardo/0000-0002-2463-0301; Bolanos, Marc/0000-0001-9838-1435
FU CERCA Programme/Generalitat de Catalunya; NVIDIA Corporation; CONICYT
   Becas Chile; Spanish Ministry of Economy and Competitiveness under Juan
   de la Cierva Program [FJCI-2014-21194]; FPU fellowship [FPU15/01347];
   ICREA Academia 2014;  [TIN2015-66951-C2-1-R];  [SGR 1219]
FX This work was supported in part by TIN2015-66951-C2-1-R, in part by SGR
   1219, in part by CERCA Programme/Generalitat de Catalunya, and in part
   by the NVIDIA Corporation with the donation of the Titan Xp GPU. Thework
   of E. Aguilar was supported by CONICYT Becas Chile. The work of B.
   Remeseiro was supported by the Spanish Ministry of Economy and
   Competitiveness under Juan de la Cierva Program (ref. FJCI-2014-21194).
   The work of M. Bolanos was supported by an FPU fellowship (ref.
   FPU15/01347). Thework of P. Radeva was supported by ICREA Academia 2014.
CR Aguilar E, 2018, LECT NOTES COMPUT SC, V10672, P339, DOI 10.1007/978-3-319-74727-9_40
   Aguilar E, 2017, LECT NOTES COMPUT SC, V10485, P213, DOI 10.1007/978-3-319-68548-9_20
   Aizawa K, 2013, IEEE T MULTIMEDIA, V15, P2176, DOI 10.1109/TMM.2013.2271474
   [Anonymous], 2017, P IEEE C COMP VIS PA
   [Anonymous], 2015, P IEEE INT C MULT EX
   [Anonymous], 2009, P ACM MULTIMEDIA 200
   [Anonymous], 2016, DARKNET OPEN SOURCE
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Beijbom O, 2015, IEEE WINT CONF APPL, P844, DOI 10.1109/WACV.2015.117
   Bettadapura V, 2015, IEEE WINT CONF APPL, P580, DOI 10.1109/WACV.2015.83
   Bolaños M, 2017, LECT NOTES COMPUT SC, V10590, P394, DOI 10.1007/978-3-319-70742-6_37
   Bolaños M, 2016, INT C PATT RECOG, P3140, DOI 10.1109/ICPR.2016.7900117
   Bossard L, 2014, LECT NOTES COMPUT SC, V8694, P446, DOI 10.1007/978-3-319-10599-4_29
   Chen JJ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1771, DOI 10.1145/3123266.3123428
   Chen JJ, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P32, DOI 10.1145/2964284.2964315
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Ciocca G, 2017, IEEE J BIOMED HEALTH, V21, P588, DOI 10.1109/JBHI.2016.2636441
   Ciocca G, 2015, LECT NOTES COMPUT SC, V9281, P334, DOI 10.1007/978-3-319-23222-5_41
   Dehais J, 2017, IEEE T MULTIMEDIA, V19, P1090, DOI 10.1109/TMM.2016.2642792
   Del Fatto V, 2017, ADV INTELL SYST, V617, P36, DOI 10.1007/978-3-319-60819-8_5
   Deng YN, 2001, IEEE T PATTERN ANAL, V23, P800, DOI 10.1109/34.946985
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Gonzalez RC., 2011, DIGITAL IMAGE PROCES
   Hassannejad H, 2016, MADIMA'16: PROCEEDINGS OF THE 2ND INTERNATIONAL WORKSHOP ON MULTIMEDIA ASSISTED DIETARY MANAGEMENT, P41, DOI 10.1145/2986035.2986042
   Herranz L, 2017, IEEE T MULTIMEDIA, V19, P430, DOI 10.1109/TMM.2016.2614861
   Hoiem D, 2012, LECT NOTES COMPUT SC, V7574, P340, DOI 10.1007/978-3-642-33712-3_25
   Jégou S, 2017, IEEE COMPUT SOC CONF, P1175, DOI 10.1109/CVPRW.2017.156
   Kagaya H, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1085, DOI 10.1145/2647868.2654970
   Kagaya H, 2015, LECT NOTES COMPUT SC, V9281, P350, DOI 10.1007/978-3-319-23222-5_43
   Kawano Y, 2015, LECT NOTES COMPUT SC, V8927, P3, DOI 10.1007/978-3-319-16199-0_1
   Liu C, 2016, LECT NOTES COMPUT SC, V9677, P37, DOI 10.1007/978-3-319-39601-9_4
   Martinel N., 2016, ARXIV161206543
   Mezgec S, 2017, NUTRIENTS, V9, DOI 10.3390/nu9070657
   Min WQ, 2018, IEEE T MULTIMEDIA, V20, P950, DOI 10.1109/TMM.2017.2759499
   Min WQ, 2017, IEEE T MULTIMEDIA, V19, P1100, DOI 10.1109/TMM.2016.2639382
   Ragusa F, 2016, MADIMA'16: PROCEEDINGS OF THE 2ND INTERNATIONAL WORKSHOP ON MULTIMEDIA ASSISTED DIETARY MANAGEMENT, P77, DOI 10.1145/2986035.2986041
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683
   Shimoda W, 2016, MADIMA'16: PROCEEDINGS OF THE 2ND INTERNATIONAL WORKSHOP ON MULTIMEDIA ASSISTED DIETARY MANAGEMENT, P13, DOI 10.1145/2986035.2986043
   Shimoda W, 2015, LECT NOTES COMPUT SC, V9281, P449, DOI 10.1007/978-3-319-23222-5_55
   Singla A, 2016, MADIMA'16: PROCEEDINGS OF THE 2ND INTERNATIONAL WORKSHOP ON MULTIMEDIA ASSISTED DIETARY MANAGEMENT, P3, DOI 10.1145/2986035.2986039
   Waltner G., 2017, P INT WORKSH MULT AS
   Xu RH, 2015, IEEE T MULTIMEDIA, V17, P1187, DOI 10.1109/TMM.2015.2438717
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
NR 48
TC 57
Z9 62
U1 4
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2018
VL 20
IS 12
BP 3266
EP 3275
DI 10.1109/TMM.2018.2831627
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HA4EY
UT WOS:000450212600007
OA Green Submitted, Green Accepted
DA 2024-07-18
ER

PT J
AU Zhang, X
   Peng, F
   Long, M
AF Zhang, Xiang
   Peng, Fei
   Long, Min
TI Robust Coverless Image Steganography Based on DCT and LDA Topic
   Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Coverless image steganography; bag-of-features; latent dirichlet
   allocation; discrete cosine transform
ID WATERMARKING; SCHEME; MODULATION; TRANSFORM; SIFT
AB In order to improve the robustness and capability of resisting image steganalysis, a novel coverless image steganography algorithm based on discrete cosine transform and latent dirichlet allocation (LDA) topic classification is proposed. First, latent dirichlet allocation topic model is utilized for classifying the image database. Second, the images belonging to one topic are selected, and 8 x 8 block discrete cosine transform is performed to these images. Then robust feature sequence is generated through the relation between direct current coefficients in the adjacent blocks. Finally, an inverted index which contains the feature sequence, dc, location coordinates, and image path is created. For the purpose of achieving image steganography, the secret information is converted into a binary sequence and partitioned into segments, and the image whose feature sequence equals to the secret information segments is chosen as the cover image according to the index. After that, all cover images are sent to the receiver. In the whole process, no modification is done to the original images. Experimental results and analysis show that the proposed algorithm can resist the detection of existing steganalysis algorithms, and has better robustness against common image processing and better ability to resist steganalysis compared with the existing coverless image steganography algorithms. Meanwhile, it is resistant to geometric attacks to some extent. It has great potential application in secure communication of big data environment.
C1 [Zhang, Xiang; Peng, Fei] Hunan Univ, Coll Comp Sci & Elect Engn, Changsha 410082, Hunan, Peoples R China.
   [Long, Min] Hunan Univ, Changsha Univ Sci & Technol, Coll Comp & Commun Engn, Changsha 410114, Hunan, Peoples R China.
C3 Hunan University; Changsha University of Science & Technology; Hunan
   University
RP Peng, F (corresponding author), Hunan Univ, Coll Comp Sci & Elect Engn, Changsha 410082, Hunan, Peoples R China.
EM martin_zx@hnu.edu.cn; eepengf@gmail.com; caslongm@aliyun.cn
RI Long, Min/AGW-6059-2022; Peng, Fei/H-6951-2017
OI Zhang, Xiang/0000-0002-0827-9443; Peng, Fei/0000-0001-8053-4587
FU National Natural Science Foundation of China [61572182, 61370225]; Hunan
   Provincial Natural Science Foundation of China [15JJ2007]
FX This work was supported in part the National Natural Science Foundation
   of China under Grants 61572182 and 61370225, in part by Hunan Provincial
   Natural Science Foundation of China under Grant 15JJ2007.
CR Andalibi M, 2015, IEEE T IMAGE PROCESS, V24, P5060, DOI 10.1109/TIP.2015.2476961
   [Anonymous], 2012, IEEE T INF FORENSICS, DOI DOI 10.1109/TIFS.2011.2175919
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Borges PVK, 2008, IEEE T MULTIMEDIA, V10, P1479, DOI 10.1109/TMM.2008.2007294
   Chen B, 2001, IEEE T INFORM THEORY, V47, P1423, DOI 10.1109/18.923725
   Chen WY, 2007, APPL MATH COMPUT, V185, P432, DOI 10.1016/j.amc.2006.07.041
   Chen XY, 2017, J INTERNET TECHNOL, V18, P313, DOI 10.6138/JIT.2017.18.2.20160815
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Erfani Y, 2017, IEEE T INF FOREN SEC, V12, P840, DOI 10.1109/TIFS.2016.2636094
   Fridrich J, 2007, MM&SEC'07: PROCEEDINGS OF THE MULTIMEDIA & SECURITY WORKSHOP 2007, P3
   Holub V, 2014, EURASIP J INF SECUR, DOI 10.1186/1687-417X-2014-1
   Holub V, 2012, IEEE INT WORKS INFOR, P234, DOI 10.1109/WIFS.2012.6412655
   Hsieh MS, 2001, IEEE T IND ELECTRON, V48, P875, DOI 10.1109/41.954550
   Huang FJ, 2012, IEEE T INF FOREN SEC, V7, P1181, DOI 10.1109/TIFS.2012.2198213
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Jianjun Zhang, 2017, International Journal of Network Security, V19, P1016, DOI 10.6633/IJNS.201711.19(6).18
   Kang ZW, 2007, J SYST ENG ELECTRON, V18, P628, DOI 10.1016/S1004-4132(07)60139-X
   Ke Y, 2004, PROC CVPR IEEE, P506
   Ker AD, 2012, P MULT SEC, P1
   Khare P, 2011, J ENG RES STUD, V2, P101
   Kodovsky J., 2012, P SOC PHOTO-OPT INS, V8303
   Li Z, 2009, 2009 INTERNATIONAL CONFERENCE ON COMPUTER ENGINEERING AND TECHNOLOGY, VOL I, PROCEEDINGS, P588, DOI 10.1109/ICCET.2009.40
   Lin GS, 2010, IEEE T MULTIMEDIA, V12, P345, DOI 10.1109/TMM.2010.2051243
   Lin WH, 2008, IEEE T MULTIMEDIA, V10, P746, DOI 10.1109/TMM.2008.922795
   Liu SC, 2012, IEEE T INF FOREN SEC, V7, P1448, DOI 10.1109/TIFS.2012.2204250
   Lou DC, 2004, IEEE T MULTIMEDIA, V6, P501, DOI 10.1109/TMM.2004.827493
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu SF, 2012, IEEE COMPUT GRAPH, V32, P26, DOI 10.1109/MCG.2011.51
   Luo WQ, 2010, IEEE T INF FOREN SEC, V5, P201, DOI 10.1109/TIFS.2010.2041812
   Luo Y, 2015, ISPRS INTERNATIONAL WORKSHOP ON SPATIOTEMPORAL COMPUTING, P19, DOI 10.5194/isprsannals-II-4-W2-19-2015
   Mckeon R.T., 2007, P IEEE COMP SOC 2007, P178
   Otori H, 2007, LECT NOTES COMPUT SC, V4569, P146
   Otori H, 2009, IEEE COMPUT GRAPH, V29, P74, DOI 10.1109/MCG.2009.127
   Stütz T, 2014, IEEE T MULTIMEDIA, V16, P1337, DOI 10.1109/TMM.2014.2310595
   Su QT, 2018, SOFT COMPUT, V22, P91, DOI 10.1007/s00500-017-2489-7
   Sun HY, 2017, J INTERNET TECHNOL, V18, P443
   Wang XY, 2016, NEUROCOMPUTING, V174, P627, DOI 10.1016/j.neucom.2015.09.082
   Wu DC, 2003, PATTERN RECOGN LETT, V24, P1613, DOI 10.1016/S0167-8655(02)00402-6
   Wu HC, 2005, IEE P-VIS IMAGE SIGN, V152, P611, DOI 10.1049/ip-vis:20059022
   Wu KC, 2015, IEEE T IMAGE PROCESS, V24, P130, DOI 10.1109/TIP.2014.2371246
   Xianyi Chen, 2015, Cloud Computing and Security. First International Conference, ICCCS 2015. Revised Selected Papers: LNCS 9483, P133, DOI 10.1007/978-3-319-27051-7_12
   Xu JY, 2015, VISUAL COMPUT, V31, P1653, DOI 10.1007/s00371-014-1045-z
   Yuan CS, 2017, J INTERNET TECHNOL, V18, P435, DOI 10.6138/JIT.2017.18.2.20160624c
   Zhang JJ, 2016, LECT NOTES COMPUT SC, V10039, P145, DOI 10.1007/978-3-319-48671-0_14
   Zhang WM, 2016, IEEE T MULTIMEDIA, V18, P1469, DOI 10.1109/TMM.2016.2569497
   Zhang XP, 2013, IEEE T MULTIMEDIA, V15, P316, DOI 10.1109/TMM.2012.2229262
   Zhang XP, 2005, IEEE SIGNAL PROC LET, V12, P67, DOI 10.1109/LSP.2004.838214
   Zheng S., 2017, P INT C INT COMP LIV, P1536
   Zhili Zhou, 2015, Cloud Computing and Security. First International Conference, ICCCS 2015. Revised Selected Papers: LNCS 9483, P123, DOI 10.1007/978-3-319-27051-7_11
   Zhou ZL, 2016, INT J SECUR APPL, V10, P309, DOI 10.14257/ijsia.2016.10.9.30
NR 51
TC 114
Z9 123
U1 1
U2 46
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2018
VL 20
IS 12
BP 3223
EP 3238
DI 10.1109/TMM.2018.2838334
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HA4EY
UT WOS:000450212600004
DA 2024-07-18
ER

PT J
AU Zhou, YP
   Wu, JY
   Chan, TH
   Ho, SW
   Chiu, DM
   Wu, D
AF Zhou, Yipeng
   Wu, Jiyiang
   Chan, Terence H.
   Ho, Siu-Wai
   Chiu, Dah-Ming
   Wu, Di
TI Interpreting Video Recommendation Mechanisms by Mining View Count Traces
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video information diffusion; word-of-mouth recommendation; direct
   recommendation
ID WORD-OF-MOUTH; POPULARITY
AB All large-scale online video systems, for example, Netflix and Youku, make a significant investment on video recommendations that can dramatically affect video information diffusion processes among users. However, there is a lack of efficient methodology to interpret how various recommendation mechanisms affect information diffusion processes resulting in the difficulty to evaluate video recommendation efficiency. In this paper, we propose to quantify and explain video recommendation mechanisms by using epidemic models to mine video view count traces. It is well known that an epidemic model is an efficient approach to model information diffusion processes; while view count traces can be viewed as the results of video information diffusion driven by video recommendations. Thus, we propose a framework based on extended epidemic models to quantify and interpret two recommendation mechanisms, that is, direct and word-of-mouth (WOM) recommendations, by fitting video view count traces collected from Tencent Video, a large-scale online video system in China. Our approach is a novel methodology to evaluate video recommendation mechanisms, and a new perspective to interpret how recommendation mechanisms drive view count evolution.
C1 [Zhou, Yipeng; Wu, Di] Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou 510006, Guangdong, Peoples R China.
   [Zhou, Yipeng; Chan, Terence H.; Ho, Siu-Wai] Univ South Australia, Inst Telecommun Res, Adelaide, SA 5095, Australia.
   [Wu, Jiyiang] TCL Corp Res Hong Kong Sci Pk, Hong Kong 999077, Hong Kong, Peoples R China.
   [Chiu, Dah-Ming] Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong 999077, Hong Kong, Peoples R China.
   [Wu, Di] Guangdong Prov Key Lab Big Data Anal & Proc, Guangzhou 510006, Guangdong, Peoples R China.
C3 Sun Yat Sen University; University of South Australia; Chinese
   University of Hong Kong
RP Wu, D (corresponding author), Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou 510006, Guangdong, Peoples R China.
EM yipeng.job@gmail.com; jqwu@tcl.com; hlchan6@gmail.com;
   siuwai.ho@unisa.edu.au; dmchiu@ie.cuhk.edu.hk; wudi27@mail.sysu.edu.cn
RI wu, di/IYS-9217-2023; Wu, Di/HNP-3772-2023; Chan, Terence/C-8787-2013;
   Chiu, Dah Ming/F-1885-2011
OI Chan, Terence/0000-0002-6550-7203; Zhou, Yipeng/0000-0003-1533-0865; Ho,
   Siu-Wai/0000-0002-8630-494X
FU National Key R&D Program of China [2016YFB0201900]; National Natural
   Science Foundation of China [61572538]; Fundamental Research Funds for
   the Central Universities [17LGJC23]; Australian Research Council
   [DE180100950, DP150103658]; Australian Research Council [DE180100950]
   Funding Source: Australian Research Council
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2016YFB0201900, in part by the National Natural Science
   Foundation of China under Grant 61572538, in part by the Fundamental
   Research Funds for the Central Universities under Grant 17LGJC23, and in
   part by the Australian Research Council under DE180100950 and
   DP150103658. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Xiaoqing Zhu.
CR Ahmed Mohamed, 2013, P 6 ACM INT C WEB SE, P607, DOI [DOI 10.1145/2433396.2433473, 10.1145/2433396.2433473]
   [Anonymous], 2017, TENCENT VIDEO
   [Anonymous], 2013, P 6 ACM INT C WEB SE, DOI DOI 10.1145/2433396.2433443
   Avramova Z, 2009, 2009 FIRST INTERNATIONAL CONFERENCE ON EVOLVING INTERNET (INTERNET 2009), P95, DOI 10.1109/INTERNET.2009.22
   Bauckhage C., 2013, P 7 INT AAAI C WEBLO
   Borghol Y., 2012, ACM SIGKDD International Knowledge Discovery and Data Mining, P1186, DOI [DOI 10.1145/2339530.2339717, 10.1145/2339530.2339717]
   Borghol Y, 2011, PERFORM EVALUATION, V68, P1037, DOI 10.1016/j.peva.2011.07.008
   Buttle F.A., 1998, J. Strateg. Mark., V6, P241, DOI DOI 10.1080/096525498346658
   Cha M, 2009, IEEE ACM T NETWORK, V17, P1357, DOI 10.1109/TNET.2008.2011358
   Davidson James, 2010, P 4 ACM C REC SYST, P293, DOI [DOI 10.1145/1864708.1864770, 10.1145/1864708]
   De Bruyn A, 2008, INT J RES MARK, V25, P151, DOI 10.1016/j.ijresmar.2008.03.004
   Dellarocas C, 2003, MANAGE SCI, V49, P1407, DOI 10.1287/mnsc.49.10.1407.17308
   Figueiredo Flavio, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8724, P386, DOI 10.1007/978-3-662-44848-9_25
   Figueiredo Flavio, 2011, P 4 ACM INT C WEB SE, P745, DOI DOI 10.1145/1935826.1935925
   Fujimoto T, 2011, INT CON ADV INFO NET, P748, DOI 10.1109/AINA.2011.103
   Golrezaei N, 2014, IEEE T WIREL COMMUN, V13, P3665, DOI 10.1109/TWC.2014.2316817
   Hennig-Thurau T, 2004, J INTERACT MARK, V18, P38, DOI 10.1002/dir.10073
   Hu H., 2014, proceeding of The IEEE International Conference on Multimedia and Expo (ICME), P1, DOI 10.1109/ICME.2014.6890134
   Li Haitao., 2012, P 22 INT WORKSHOP NE, P83
   Lobzhanidze A, 2013, P IEEE ICME JUL, P1
   Matsubara Y., 2012, 18 ACM SIGKDD INT C, P6, DOI DOI 10.1145/2339530.2339537
   Mo YJ, 2014, IEEE SYST J, V8, P184, DOI 10.1109/JSYST.2013.2279732
   Richier C, 2016, ANN ALLERTON CONF, P31, DOI 10.1109/ALLERTON.2016.7852207
   Sarwar B., 2001, P 10 INT C WORLD WID, P285, DOI 10.1145/371920.372071
   Szabo G, 2010, COMMUN ACM, V53, P80, DOI 10.1145/1787234.1787254
   Tan B, 2013, IEEE ACM T NETWORK, V21, P566, DOI 10.1109/TNET.2012.2208199
   Trusov M, 2009, J MARKETING, V73, P90, DOI 10.1509/jmkg.73.5.90
   Wu JQ, 2016, IEEE T MULTIMEDIA, V18, P1882, DOI 10.1109/TMM.2016.2579600
   Wu JQ, 2015, 2015 IEEE 23RD INTERNATIONAL SYMPOSIUM ON QUALITY OF SERVICE (IWQOS), P141, DOI 10.1109/IWQoS.2015.7404724
   Wu WJ, 2012, IEEE T PARALL DISTR, V23, P1492, DOI 10.1109/TPDS.2011.295
   Wu X, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P369, DOI 10.1109/ICME.2008.4607448
   Yang J., 2011, P 4 ACM INT C WEB SE, P177, DOI DOI 10.1145/1935826.1935863
   Zhou Renjie, 2010, P 10 ACM SIGCOMM C I, P404, DOI DOI 10.1145/1879141.1879193
   Zhou YP, 2015, IEEE T MULTIMEDIA, V17, P1273, DOI 10.1109/TMM.2015.2447277
   Zhou YP, 2013, IEEE ACM T NETWORK, V21, P233, DOI 10.1109/TNET.2012.2196444
NR 35
TC 17
Z9 19
U1 2
U2 32
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2018
VL 20
IS 8
BP 2153
EP 2165
DI 10.1109/TMM.2017.2781364
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GN8BR
UT WOS:000439378600019
DA 2024-07-18
ER

PT J
AU Tang, J
   Tang, XY
   Yuan, JS
AF Tang, Jing
   Tang, Xueyan
   Yuan, Junsong
TI Traffic-optimized Data Placement for Social Media
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Social media; distributed storage; graph partitioning; data replication
AB Social media users are generating data on an unprecedented scale. Distributed storage systems are often used to cope with explosive data growth. Data partitioning and replication are two interrelated data placement issues affecting the interserver traffic caused by user-initiated read and write operations in distributed storage systems. This paper investigates how to minimize the interserver traffic among a cluster of social media servers through joint data partitioning and replication optimization. We formally define the problem and study its hardness. We then propose a traffic-optimized partitioning and replication (TOPR) method to continuously adapt data placement according to various dynamics. Evaluations with real Twitter and LiveJournal social graphs show that TOPR not only reduces the interserver traffic significantly but also saves much storage cost of replication compared to state-of-the-art methods. We also benchmark TOPR against the offline optimum by a binary linear program.
C1 [Tang, Jing] Nanyang Technol Univ, Interdisciplinary Grad Sch, Singapore 639798, Singapore.
   [Tang, Xueyan] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
   [Yuan, Junsong] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University; Nanyang Technological University;
   Nanyang Technological University
RP Tang, J (corresponding author), Nanyang Technol Univ, Interdisciplinary Grad Sch, Singapore 639798, Singapore.
EM tang0311@ntu.edu.sg; asxytang@ntu.edu.sg; jsyuan@ntu.edu.sg
RI Yuan, Junsong/R-4352-2019; Tang, Jing/K-2744-2016; Tang,
   Xueyan/A-3703-2011
OI Tang, Jing/0000-0002-0785-707X; Yuan, Junsong/0000-0002-7901-8793
FU National Research Foundation, Prime Minister's Office, Singapore, under
   its IDM Futures Funding Initiative; Singapore Ministry of Education
   Academic Research Fund [2013-T1-002-123, MOE2015-T2-2-114]
FX This work was supported in part by the National Research Foundation,
   Prime Minister's Office, Singapore, under its IDM Futures Funding
   Initiative, and in part by Singapore Ministry of Education Academic
   Research Fund Tier 1 under Grant 2013-T1-002-123 and Tier 2 under Grant
   MOE2015-T2-2-114. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Shiwen Mao.
CR [Anonymous], INFORM WEEK
   [Anonymous], 2010, IMC 2010 P
   [Anonymous], P NAT ACAD SCI US
   [Anonymous], 2015, Gurobi optimizer reference manual
   [Anonymous], 2011, C INN DAT SYST RES C
   [Anonymous], I I CPLEX V12 1 US M
   [Anonymous], J STAT MECH
   [Anonymous], P 21 IEEE INT C NETW
   [Anonymous], STAT MED SOC MED REP
   [Anonymous], 2011, 2011 USENIX C USENIX
   [Anonymous], CASSANDRA COMES HOME
   [Anonymous], 2004, P 16 ANN ACM S PAR A, DOI DOI 10.1145/1007912.1007931
   [Anonymous], HOURS VID UPL YOUTUB
   [Anonymous], SPRINGERBRIEF OPTIMI
   [Anonymous], P 5 INT WORKSH NETW
   Arora S, 2009, J ACM, V56, DOI 10.1145/1502793.1502794
   Barabási AL, 1999, SCIENCE, V286, P509, DOI 10.1126/science.286.5439.509
   Benevenuto F, 2009, IMC'09: PROCEEDINGS OF THE 2009 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P49
   Bian JW, 2015, IEEE T MULTIMEDIA, V17, P216, DOI 10.1109/TMM.2014.2384912
   Chang F, 2008, ACM T COMPUT SYST, V26, DOI 10.1145/1365815.1365816
   DeCandia Giuseppe, 2007, Operating Systems Review, V41, P205, DOI 10.1145/1323293.1294281
   Duong-Ba T, 2014, IEEE T EMERG TOP COM, V2, P422, DOI 10.1109/TETC.2014.2358801
   Faloutsos M, 1999, COMP COMM R, V29, P251, DOI 10.1145/316194.316229
   Hoque I, 2012, IEEE INTERNET COMPUT, V16, P24, DOI 10.1109/MIC.2012.40
   Jiao L., 2013, P FUT NETW MOB SUMM, P1
   Jiao L, 2016, IEEE ACM T NETWORK, V24, P99, DOI 10.1109/TNET.2014.2359365
   Jiao L, 2014, IEEE INFOCOM SER, P28, DOI 10.1109/INFOCOM.2014.6847921
   Karypis G, 1998, SIAM J SCI COMPUT, V20, P359, DOI 10.1137/S1064827595287997
   Lakshman Avinash, 2010, Operating Systems Review, V44, P35, DOI 10.1145/1773912.1773922
   Lei CY, 2016, IEEE T MULTIMEDIA, V18, P687, DOI 10.1109/TMM.2015.2477277
   Leskovec J., 2014, SNAP Datasets: Stanford large network dataset collection
   Mislove A, 2007, IMC'07: PROCEEDINGS OF THE 2007 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P29
   Mondal M., 2012, Proceeding of the 8th International Conference on emerging Networking EX-periments and Technologies, CoNext'12, P325
   Nasir MAU, 2014, 2014 PROCEEDINGS OF THE IEEE/ACM INTERNATIONAL CONFERENCE ON ADVANCES IN SOCIAL NETWORKS ANALYSIS AND MINING (ASONAM 2014), P33, DOI 10.1109/ASONAM.2014.6921557
   Nishida H, 2013, IEEE T PARALL DISTR, V24, P565, DOI 10.1109/TPDS.2012.169
   Nishimura J, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P1106
   Pujol JM, 2010, ACM SIGCOMM COMP COM, V40, P375, DOI 10.1145/1851275.1851227
   Roberts SW, 2000, TECHNOMETRICS, V42, P97, DOI 10.2307/1271439
   Schneider F, 2009, IMC'09: PROCEEDINGS OF THE 2009 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P35
   Song YC, 2012, IEEE T MULTIMEDIA, V14, P456, DOI 10.1109/TMM.2011.2172937
   Sovran Y, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P385
   Spearman C, 1904, AM J PSYCHOL, V15, P72, DOI 10.2307/1412159
   Stanton Isabelle, 2012, P INT C KNOWL DISC D, P1222, DOI [10.1145/2339530.2339722, DOI 10.1145/2339530.2339722]
   Tang J, 2015, INT CON DISTR COMP S, P215, DOI 10.1109/ICDCS.2015.30
   Tran DA, 2012, COMPUT NETW, V56, P2001, DOI 10.1016/j.comnet.2012.02.010
   Tsourakakis CE, 2014, WSDM'14: PROCEEDINGS OF THE 7TH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P333, DOI 10.1145/2556195.2556213
   Wilson C, 2009, EUROSYS'09: PROCEEDINGS OF THE FOURTH EUROSYS CONFERENCE, P205
   Wu YC, 2016, IEEE T MULTIMEDIA, V18, P2135, DOI 10.1109/TMM.2016.2614220
   Xu Z, 2014, IEEE T MULTIMEDIA, V16, P1986, DOI 10.1109/TMM.2014.2342658
   Yang Q, 2012, IEEE SYS MAN CYBERN, P1, DOI 10.1109/ICSMC.2012.6377667
NR 50
TC 10
Z9 10
U1 0
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2018
VL 20
IS 4
BP 1008
EP 1023
DI 10.1109/TMM.2017.2760627
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FZ5HM
UT WOS:000427623000019
DA 2024-07-18
ER

PT J
AU Fan, X
   Liu, RS
   Luo, ZX
   Li, YT
   Feng, YY
AF Fan, Xin
   Liu, Risheng
   Luo, Zhongxuan
   Li, Yuntao
   Feng, Yuyao
TI Explicit Shape Regression With Characteristic Number for Facial Landmark
   Localization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Facial landmark localization; projective invariant; characteristic
   number; regression learning
ID FACE ALIGNMENT; INVARIANT; VIEWPOINT; AFFINE
AB Robustly localizing facial landmarks plays a very important role in many multimedia and vision applications. Most recently proposed regression-based methods prevailing in the community lack explicit shape constraints for faces and require a large number of facial images to cover great appearance variations. To address these limitations, this paper introduces a novel projective invariant called characteristic number (CN) to explicitly characterize the intrinsic geometries of facial points shared by human faces. It can be verified that the shape priors from CN are inherently invariant to pose changes. By further developing a shape-to-gradient regression framework, we provide a robust and efficient landmark detector for facial images in the wild. The computation of our model can be successfully addressed by learning the descent directions using point-CN pairs without the need for large collections for appearance training. As a nontrivial byproduct, this paper also builds a face dataset, where each face has 15 well-defined viewpoints (poses) to quantitatively analyze the effects of different poses on localization methods. Extensive experiments on challenging benchmarks and our newly built dataset demonstrate the effectiveness of our proposed detector against other state-of-the-art approaches.
C1 [Fan, Xin; Liu, Risheng; Luo, Zhongxuan] Dalian Univ Technol, DUT RU Int Sch Informat & Software Engn, Dalian 116023, Peoples R China.
   [Fan, Xin; Liu, Risheng; Luo, Zhongxuan] Key Lab Ubiquitous Network & Serv Software, Dalian 116620, Peoples R China.
   [Li, Yuntao; Feng, Yuyao] Dalian Univ Technol, Sch Software, Dalian 116023, Peoples R China.
C3 Dalian University of Technology; Dalian University of Technology
RP Fan, X (corresponding author), Dalian Univ Technol, DUT RU Int Sch Informat & Software Engn, Dalian 116023, Peoples R China.
EM xin.fan@ieee.org; rsliu@dlut.edu.cn; zxluo@dlut.edu.cn;
   liyuntao@mail.dlut.edu.cn; yyaofeng@gmail.com
RI Zhang, Yi/KCY-9118-2024
OI Liu, Risheng/0000-0002-9554-0565
FU Natural Science Foundation of China [61272371, 61432003, 61572096,
   61672125]
FX This work was supported by the Natural Science Foundation of China under
   Grants 61272371, 61432003, 61572096, and 61672125.
CR [Anonymous], 2014, INT J DISTRIB SENS N
   [Anonymous], P 5 ACCV
   [Anonymous], 07 UMASS TR
   [Anonymous], P ECCV
   Belhumeur PN, 2013, IEEE T PATTERN ANAL, V35, P2930, DOI 10.1109/TPAMI.2013.23
   Bryner D, 2014, IEEE T PATTERN ANAL, V36, P998, DOI 10.1109/TPAMI.2013.199
   Burgos-Artizzu XP, 2013, IEEE I CONF COMP VIS, P1513, DOI 10.1109/ICCV.2013.191
   Cao C, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601204
   Cao XD, 2014, INT J COMPUT VISION, V107, P177, DOI 10.1007/s11263-013-0667-3
   Connor CE, 2010, SCIENCE, V330, P764, DOI 10.1126/science.1198348
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Cristinacce D., 2006, BRIT MACH VIS C, V1, P3
   Dantone M, 2012, PROC CVPR IEEE, P2578, DOI 10.1109/CVPR.2012.6247976
   Dibeklioglu H, 2012, IEEE T IMAGE PROCESS, V21, P844, DOI 10.1109/TIP.2011.2163162
   Dollár P, 2010, PROC CVPR IEEE, P1078, DOI 10.1109/CVPR.2010.5540094
   Efraty B., 2011, IJCB, P1
   Fan B, 2012, PATTERN RECOGN, V45, P794, DOI 10.1016/j.patcog.2011.08.004
   Fan X, 2005, IEEE I CONF COMP VIS, P302
   Fan X, 2015, IEEE T IMAGE PROCESS, V24, P1164, DOI 10.1109/TIP.2015.2390976
   Freiwald WA, 2010, SCIENCE, V330, P845, DOI 10.1126/science.1194908
   Goodall CR, 1999, J COMPUT GRAPH STAT, V8, P143, DOI 10.2307/1390631
   Hsu GS, 2015, IEEE I CONF COMP VIS, P3855, DOI 10.1109/ICCV.2015.439
   Jia Q, 2016, PATTERN RECOGN, V52, P358, DOI 10.1016/j.patcog.2015.11.003
   Kang XG, 2011, IEEE T MULTIMEDIA, V13, P181, DOI 10.1109/TMM.2010.2098850
   Kirwan F., 1994, GEOMETRIC INVARIANT, V34
   Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49
   Liang L., 2006, CVPR, P1313, DOI [DOI 10.1109/CVPR.2006.45, 10.1109/CVPR.2006.45]
   Lin W., 2006, THESIS
   Liu L, 2014, P AS C COMP VIS WORK, P71
   Liu QS, 2016, IEEE T IMAGE PROCESS, V25, P700, DOI 10.1109/TIP.2015.2502485
   Luo ZX, 2014, SCI CHINA MATH, V57, P2273, DOI 10.1007/s11425-014-4877-0
   Martinez B, 2013, IEEE T PATTERN ANAL, V35, P1149, DOI 10.1109/TPAMI.2012.205
   Milborrow S, 2008, LECT NOTES COMPUT SC, V5305, P504, DOI 10.1007/978-3-540-88693-8_37
   Özuysal M, 2010, IEEE T PATTERN ANAL, V32, P448, DOI 10.1109/TPAMI.2009.23
   Raviv D, 2014, J MATH IMAGING VIS, V50, P144, DOI 10.1007/s10851-013-0467-y
   Ren SQ, 2015, PROC CVPR IEEE, P723, DOI 10.1109/CVPR.2015.7298672
   Ren SQ, 2014, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2014.218
   Riccio D, 2007, PATTERN RECOGN LETT, V28, P1907, DOI 10.1016/j.patrec.2006.12.017
   Sagonas C, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P397, DOI 10.1109/ICCVW.2013.59
   Suk T, 2000, PATTERN RECOGN, V33, P251, DOI 10.1016/S0031-3203(99)00049-7
   Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446
   Tekin C, 2015, IEEE T MULTIMEDIA, V17, P549, DOI 10.1109/TMM.2015.2403234
   Thaddeus M, 1996, J AM MATH SOC, V9, P691, DOI 10.1090/S0894-0347-96-00204-4
   Tzimiropoulos G, 2015, PROC CVPR IEEE, P3659, DOI 10.1109/CVPR.2015.7298989
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Vukadinovic D, 2005, IEEE SYS MAN CYBERN, P1692
   Wang X, 2014, IEEE T MULTIMEDIA, V16, P2130, DOI 10.1109/TMM.2014.2355134
   Wang ZZ, 2011, IMAGE VISION COMPUT, V29, P681, DOI 10.1016/j.imavis.2011.07.005
   Xiong XH, 2015, PROC CVPR IEEE, P2664, DOI 10.1109/CVPR.2015.7298882
   Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75
   Yammine G, 2013, IEEE IMAGE PROC, P3017, DOI 10.1109/ICIP.2013.6738621
   Zhao XW, 2013, IEEE I CONF COMP VIS, P1033, DOI 10.1109/ICCV.2013.132
NR 53
TC 8
Z9 8
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2018
VL 20
IS 3
BP 567
EP 579
DI 10.1109/TMM.2017.2751143
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FW5ZH
UT WOS:000425397500005
DA 2024-07-18
ER

PT J
AU Wang, XH
   Gao, LL
   Wang, P
   Sun, XS
   Liu, XL
AF Wang, Xuanhan
   Gao, Lianli
   Wang, Peng
   Sun, Xiaoshuai
   Liu, Xianglong
TI Two-Stream 3-D convNet Fusion for Action Recognition in Videos With
   Arbitrary Size and Length
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Action recognition; 3D convolution neural networks
ID MOTION
AB 3-D convolutional neural networks (3-D-convNets) have been very recently proposed for action recognition in videos, and promising results are achieved. However, existing 3-D-convNets has two "artificial" requirements that may reduce the quality of video analysis: 1) It requires a fixed-sized (e.g., 112 x 112) input video; and 2) most of the 3-D-convNets require a fixed-length input (i.e., video shots with fixed number of frames). To tackle these issues, we propose an end-to-end pipeline named Two-stream 3-D-convNet Fusion, which can recognize human actions in videos of arbitrary size and length using multiple features. Specifically, we decompose a video into spatial and temporal shots. By taking a sequence of shots as input, each stream is implemented using a spatial temporal pyramid pooling (STPP) convNet with a long short-term memory (LSTM) or CNN-E model, softmax scores of which are combined by a late fusion. We devise the STPP convNet to extract equal-dimensional descriptions for each variable-size shot, and we adopt the LSTM/CNN-E model to learn a global description for the input video using these time-varying descriptions. With these advantages, our method should improve all 3-D CNN-based video analysis methods. We empirically evaluate our method for action recognition in videos and the experimental results show that our method outperforms the state-of-the-art methods (both 2-D and 3-D based) on three standard benchmark datasets (UCF101, HMDB51 and ACT datasets).
C1 [Wang, Xuanhan; Gao, Lianli] Univ Elect Sci & Technol China, Sch Comp Sci, Chengdu 610051, Sichuan, Peoples R China.
   [Wang, Peng] Univ Adelaide, Sch Comp Sci, Adelaide, SA 5005, Australia.
   [Sun, Xiaoshuai] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Heilongjiang, Peoples R China.
   [Liu, Xianglong] Beihang Univ, Sch Comp Sci & Engn, Beijing 100191, Peoples R China.
C3 University of Electronic Science & Technology of China; University of
   Adelaide; Harbin Institute of Technology; Beihang University
RP Gao, LL (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci, Chengdu 610051, Sichuan, Peoples R China.
EM wangxuauhan@uestc.edu.cn; lianli.gao@uestc.edu.cn;
   peng.wang@adelaide.edu.au; xiaoshuaisun.hit@gmail.com;
   xlliu@nlsde.buaa.edu.cn
RI wang, peng/AAH-2781-2020
OI Wang, Peng/0000-0002-5397-9115; Liu, Xianglong/0000-0002-7618-3275
FU Fundamental Research Funds for the Central Universities [ZYGX2014J063,
   ZYGX2014Z007]; National Natural Science Foundation of China [61502080,
   61632007]
FX This work was supported in part by the Fundamental Research Funds for
   the Central Universities under Grants ZYGX2014J063 and ZYGX2014Z007, and
   in part by the National Natural Science Foundation of China under Grants
   61502080 and 61632007.
CR [Anonymous], 2015, CORR
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], PATTERN REC IN PRESS
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Gan C, 2016, AAAI CONF ARTIF INTE, P3487
   Gan C, 2016, PROC CVPR IEEE, P923, DOI 10.1109/CVPR.2016.106
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jingen Liu, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3337, DOI 10.1109/CVPR.2011.5995353
   Kalchbrenner N, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P655, DOI 10.3115/v1/p14-1062
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Lan ZZ, 2015, PROC CVPR IEEE, P204, DOI 10.1109/CVPR.2015.7298616
   Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756
   Leordeanu M, 2016, AAAI CONF ARTIF INTE, P3530
   Lin Cheng, 2017, 2017 IEEE Conference on Energy Internet and Energy System Integration (EI2). Proceedings, DOI 10.1109/EI2.2017.8245533
   Liu XL, 2017, IEEE T IMAGE PROCESS, V26, P5324, DOI 10.1109/TIP.2017.2729896
   Liu XL, 2016, IEEE T IMAGE PROCESS, V25, P907, DOI 10.1109/TIP.2015.2505180
   Lu JS, 2016, ADV NEUR IN, V29
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Pan ZQ, 2016, J VIS COMMUN IMAGE R, V40, P516, DOI 10.1016/j.jvcir.2016.07.018
   Pan ZQ, 2015, IEEE T BROADCAST, V61, P166, DOI 10.1109/TBC.2015.2419824
   Peng XJ, 2016, COMPUT VIS IMAGE UND, V150, P109, DOI 10.1016/j.cviu.2016.03.013
   Scovanner P., 2007, P 15 ACM INT C MULT, P357, DOI DOI 10.1145/1291233.1291311
   Shen FM, 2017, IEEE T MULTIMEDIA, V19, P2022, DOI 10.1109/TMM.2017.2699863
   Simonyan K., 2014, P 27 INT C NEUR INF, P568, DOI DOI 10.1002/14651858.CD001941.PUB3
   Song JK, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2737
   Song JK, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P831, DOI 10.1145/2964284.2964295
   Song J, 2016, IEEE T IMAGE PROCESS, V25, P4999, DOI 10.1109/TIP.2016.2601260
   Song JK, 2013, IEEE T MULTIMEDIA, V15, P1997, DOI 10.1109/TMM.2013.2271746
   Sun L, 2015, IEEE I CONF COMP VIS, P4597, DOI 10.1109/ICCV.2015.522
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang XL, 2016, PROC CVPR IEEE, P2658, DOI 10.1109/CVPR.2016.291
   Wu ZX, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P461, DOI 10.1145/2733373.2806222
   Xu R, 2015, AAAI CONF ARTIF INTE, P2346
   Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22
   Zhang HW, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P781, DOI 10.1145/2964284.2964308
   Zhu XF, 2013, IEEE T MULTIMEDIA, V15, P633, DOI 10.1109/TMM.2012.2233723
NR 41
TC 192
Z9 201
U1 7
U2 142
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2018
VL 20
IS 3
BP 634
EP 644
DI 10.1109/TMM.2017.2749159
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FW5ZH
UT WOS:000425397500010
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Bachhuber, C
   Steinbach, E
   Freundl, M
   Reisslein, M
AF Bachhuber, Christoph
   Steinbach, Eckehard
   Freundl, Martin
   Reisslein, Martin
TI On the Minimization of Glass-to-Glass and Glass-to-Algorithm Delay in
   Video Communication
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Delay analysis; delay measurement; frame skipping; preemption; prototype
   system
ID PERCEPTUAL QUALITY; FRAME RATE; STABILITY; IMPACT
AB Video cameras are increasingly used to provide real-time feedback in automatic control systems, such as autonomous driving and robotics systems. For such highly dynamic applications, the glass-to-glass (G2G) and glass-to-algorithm (G2A) latencies are critical. In this paper, we analyze the latencies in a point-to-point video transmission system and propose novel frame skipping and preemption approaches to reduce the G2G and G2A delays. We implement the proposed approaches in a prototype that shows significantly reduced G2G and G2A latencies as well as reduced transmission bitrate requirements compared with traditional video transmission schemes. In our low-delay video communication prototype, a VGA resolution video is transmitted with average G2G and G2A delays of 21.2 and 11.5 ms, respectively, with off-the-shelf hardware.
C1 [Bachhuber, Christoph; Steinbach, Eckehard; Freundl, Martin] Tech Univ Munich, Chair Media Technol, D-80333 Munich, Germany.
   [Reisslein, Martin] Arizona State Univ, Sch Elect Comp & Energy Engn, Tempe, AZ 85287 USA.
C3 Technical University of Munich; Arizona State University; Arizona State
   University-Tempe
RP Steinbach, E (corresponding author), Tech Univ Munich, Chair Media Technol, D-80333 Munich, Germany.
EM christoph.bachhuber@tum.de; eckehard.steinbach@tum.de;
   martin.freundl@tum.de; reisslein@asu.edu
RI Reisslein, Martin/B-3278-2014
OI Reisslein, Martin/0000-0003-1606-233X; Steinbach,
   Eckehard/0000-0001-8853-2703; Bachhuber, Christoph/0000-0002-9808-0162
FU Friedrich Wilhelm Bessel Research Award from Alexander von Humboldt
   Foundation
FX This work was supported in part by a Friedrich Wilhelm Bessel Research
   Award from the Alexander von Humboldt Foundation. This paper was
   presented in part at the IEEE International Conference on Image
   Processing, Phoenix, AZ, USA, September 2016. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Xiaoqing Zhu.
CR Alt N, 2008, DES AUT TEST EUROPE, P174
   Alt N, 2013, 2013 IEEE INTERNATIONAL SYMPOSIUM ON HAPTIC AUDIO-VISUAL ENVIRONMENTS AND GAMES (HAVE 2013), P24, DOI 10.1109/HAVE.2013.6679605
   Annett M., 2014, P GRAPHICS INTERFACE, P167
   [Anonymous], 2013, PROC C COMPUT SUPPOR
   [Anonymous], P SPIE
   [Anonymous], P IEEE INT C AC SPEE
   [Anonymous], 2016, P ACM MULT SYST DEM
   [Anonymous], 2008, 2008 INT C CONS EL I
   [Anonymous], 2013, MULT EXP WORKSH ICME
   [Anonymous], 2017, P IEEE WIR COMM NETW
   [Anonymous], SPIE J ELECT IMAG
   [Anonymous], IEEE T CIRC IN PRESS
   [Anonymous], ELECTRONICS
   Bachhuber C, 2016, IEEE IMAGE PROC, P2132, DOI 10.1109/ICIP.2016.7532735
   Baldi M, 2000, IEEE ACM T NETWORK, V8, P479, DOI 10.1109/90.865076
   Baltas G, 2015, 2015 IEEE SYMPOSIUM ON COMPUTERS AND COMMUNICATION (ISCC), P397, DOI 10.1109/ISCC.2015.7405547
   Boyaci O, 2009, 2009 11TH IEEE INTERNATIONAL SYMPOSIUM ON MULTIMEDIA (ISM 2009), P194, DOI 10.1109/ISM.2009.46
   Briscoe B, 2016, IEEE COMMUN SURV TUT, V18, P2149, DOI 10.1109/COMST.2014.2375213
   Chang CY, 2012, IEEE SYST J, V6, P414, DOI 10.1109/JSYST.2011.2163986
   Chien SY, 2012, IEEE T MULTIMEDIA, V14, P250, DOI 10.1109/TMM.2011.2173476
   Cloosterman MBG, 2009, IEEE T AUTOMAT CONTR, V54, P1575, DOI 10.1109/TAC.2009.2015543
   Dong Yue, 2004, Proceedings of the 2004 IEEE International Conference on Control Applications (IEEE Cat. No.04CH37596), P242, DOI 10.1109/CCA.2004.1387218
   Doulamis A, 2006, IEEE IJCNN, P4037
   DVORETZKY A, 1956, ANN MATH STAT, V27, P642, DOI 10.1214/aoms/1177728174
   Fettweis GP, 2014, IEEE VEH TECHNOL MAG, V9, P64, DOI 10.1109/MVT.2013.2295069
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Greco C, 2012, IEEE T MULTIMEDIA, V14, P1337, DOI 10.1109/TMM.2012.2195480
   Hespanha JP, 2007, P IEEE, V95, P138, DOI 10.1109/JPROC.2006.887288
   Hill R, 2009, 2009 DIGITAL IMAGE COMPUTING: TECHNIQUES AND APPLICATIONS (DICTA 2009), P89, DOI 10.1109/DICTA.2009.23
   Holub P., 2012, Proc. 20th ACM Int. Conf. Multimedia, P1457, DOI [DOI 10.1145/2393347.2396519, 10.1145/2393347.2396519]
   Inatsuki T, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING (DSP), P1062, DOI 10.1109/ICDSP.2015.7252041
   Jacobs M. C., 1997, Proceedings 1997 Symposium on Interactive 3D Graphics, P49, DOI 10.1145/253284.253306
   Jansen J., 2013, PROCEEDING 23 ACM WO, P37, DOI [10.1145/2460782.2460789, DOI 10.1145/2460782.2460789]
   Jota Ricardo, 2013, P CHI 2013 C HUMAN F, P2291, DOI [DOI 10.1145/2470654.2481317, 10.1145/2470654.2481317]
   Kaiser K.L., 2005, Transmission Lines, Matching, and Crosstalk, p3
   Kawamura S, 2016, P IEEE VIRT REAL ANN, P199, DOI 10.1109/VR.2016.7504722
   Khan MUK, 2013, DES AUT TEST EUROPE, P115
   Kohler E, 2000, ACM T COMPUT SYST, V18, P263, DOI 10.1145/354871.354874
   Lee HC, 2002, IEEE IMAGE PROC, P928
   Liu T., 2004, Proceedings of the 12th annual ACM international conference on Multimedia - MULTIMEDIA '04, P400, DOI DOI 10.1145/1027527.1027622
   Liu YN, 2016, IEEE T MULTIMEDIA, V18, P865, DOI 10.1109/TMM.2016.2538718
   Lu ZK, 2005, PROC SPIE, V5666, P554, DOI 10.1117/12.596845
   Mania Katerina., 2004, Proceedings of the 1st Symposium on Applied Perception in Graphics and Visualization, APGV '04, P39, DOI [10.1145/1012551.1012559, DOI 10.1145/1012551.1012559]
   MASSART P, 1990, ANN PROBAB, V18, P1269, DOI 10.1214/aop/1176990746
   Navakitkanok P, 2004, ITCC 2004: INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY: CODING AND COMPUTING, VOL 2, PROCEEDINGS, P664, DOI 10.1109/ITCC.2004.1286730
   Okumura K, 2011, IEEE INT CONF ROBOT, DOI 10.1109/ICRA.2011.5980080
   Ou YF, 2011, IEEE T CIRC SYST VID, V21, P286, DOI 10.1109/TCSVT.2010.2087833
   Ou YF, 2008, IEEE IMAGE PROC, P689, DOI 10.1109/ICIP.2008.4711848
   Pilz Jens, 2016, 2016 IEEE Conference on Computer Communications: Workshops (INFOCOM WKSHPS), P862, DOI 10.1109/INFCOMW.2016.7562198
   Ribas-Corbera J, 1999, IEEE T CIRC SYST VID, V9, P172, DOI 10.1109/76.744284
   Schreier RM, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P2053, DOI 10.1109/ICME.2006.262618
   Sheikh AM, 2014, IEEE T MULTIMEDIA, V16, P2294, DOI 10.1109/TMM.2014.2357716
   Shi Y, 2009, IEEE T AUTOMAT CONTR, V54, P1668, DOI 10.1109/TAC.2009.2020638
   Sielhorst Tobias., 2007, 2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality (ISMAR), P215, DOI [10.1109/ISMAR.2007.4538850, DOI 10.1109/ISMAR.2007.4538850]
   Song R, 2013, J ZHEJIANG U-SCI C, V14, P374, DOI 10.1631/jzus.C1200333
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Szymanski TH, 2016, IEEE ACM T NETWORK, V24, P123, DOI 10.1109/TNET.2014.2358497
   Tipsuwan Y, 2003, CONTROL ENG PRACT, V11, P1099, DOI 10.1016/S0967-0661(03)00036-4
   Usach-Molina P, 2010, IEEE T CIRC SYST VID, V20, P982, DOI 10.1109/TCSVT.2010.2051276
   Vinel A, 2012, IEEE T VEH TECHNOL, V61, P2319, DOI 10.1109/TVT.2012.2192301
   Walsh GC, 2002, IEEE T CONTR SYST T, V10, P438, DOI 10.1109/87.998034
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Wu JY, 2016, IEEE T CIRC SYST VID, V26, P711, DOI 10.1109/TCSVT.2015.2412774
   Wu JY, 2016, IEEE T MOBILE COMPUT, V15, P641, DOI 10.1109/TMC.2015.2426710
   Yang RN, 2014, IEEE T IND ELECTRON, V61, P512, DOI 10.1109/TIE.2013.2248339
   Zhang LX, 2013, IEEE T IND INFORM, V9, P403, DOI 10.1109/TII.2012.2219540
   Zhang W, 2001, IEEE CONTR SYST MAG, V21, P84, DOI 10.1109/37.898794
NR 68
TC 22
Z9 25
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2018
VL 20
IS 1
BP 238
EP 252
DI 10.1109/TMM.2017.2726189
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FQ3WS
UT WOS:000418289700019
DA 2024-07-18
ER

PT J
AU Wilson, S
   Mohan, CK
AF Wilson, Shyju
   Mohan, C. Krishna
TI An Information Bottleneck Approach to Optimize the Dictionary of Visual
   Data
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dictionary learning; information bottleneck; mutual information; sparse
   representation
ID K-SVD; SPARSE; RECOGNITION; COMPACT
AB In this paper, we propose a novel information theoretic approach to obtain compact and discriminative dictionary of visual data. This approach squeezes discriminative information from the dictionary for efficient representation using information bottleneck. The dictionary is optimized from the initial sparse dictionary, which is learned from action data. In this, a constraint information optimization problem is formulated in which mutual information between the initial and optimized dictionary is minimized while maximizing mutual information between optimized dictionary and class labels. We use an effective similarity measure, Jensen-Shannon divergence with adaptive weightages, for class distributions of each dictionary atom. These adaptive weightages are obtained based on the usage of the dictionary atom among different classes. The resultant dictionary becomes discriminative and compact, while retaining maximum information with fewer atoms. Using simple reconstruction error, we test computational efficiency of the proposed method without compromising classification accuracy on popular benchmark datasets. It is further demonstrated how efficiently discriminative information is retained by comparing the classification performance of the dictionary before and after the removal of redundant dictionary atoms.
C1 [Wilson, Shyju; Mohan, C. Krishna] Indian Inst Technol Hyderabad, Visual Learning & Intelligence Lab, Dept Comp Sci & Engn, Hyderabad 502285, Andhra Prades, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Hyderabad
RP Wilson, S (corresponding author), Indian Inst Technol Hyderabad, Visual Learning & Intelligence Lab, Dept Comp Sci & Engn, Hyderabad 502285, Andhra Prades, India.
EM cs10p006@iith.ac.in; ckm@iith.ac.in
OI Chalavadi, Krishna Mohan/0000-0002-7316-0836
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   BLAHUT RE, 1972, IEEE T INFORM THEORY, V18, P460, DOI 10.1109/TIT.1972.1054855
   Cai SJ, 2014, LECT NOTES COMPUT SC, V8692, P624, DOI 10.1007/978-3-319-10593-2_41
   Chen T, 2014, IEEE T MULTIMEDIA, V16, P612, DOI 10.1109/TMM.2014.2301978
   Chen T, 2013, IEEE T CIRC SYST VID, V23, P1611, DOI 10.1109/TCSVT.2013.2254978
   Cover Thomas M, 1999, Elements of information theory
   Deng C, 2016, IEEE T MULTIMEDIA, V18, P208, DOI 10.1109/TMM.2015.2508146
   Engan K, 1999, INT CONF ACOUST SPEE, P2443, DOI 10.1109/ICASSP.1999.760624
   Golts A, 2016, IEEE J-STSP, V10, P726, DOI 10.1109/JSTSP.2016.2555241
   Guha T, 2014, IEEE T MULTIMEDIA, V16, P980, DOI 10.1109/TMM.2014.2306175
   Nguyen HV, 2012, INT CONF ACOUST SPEE, P2021, DOI 10.1109/ICASSP.2012.6288305
   HULL JJ, 1994, IEEE T PATTERN ANAL, V16, P550, DOI 10.1109/34.291440
   Jiang ZL, 2013, IEEE T PATTERN ANAL, V35, P2651, DOI 10.1109/TPAMI.2013.88
   Krause A, 2008, J MACH LEARN RES, V9, P235
   Krause A, 2006, IPSN 2006: THE FIFTH INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS, P2
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694
   Lazebnik S, 2009, IEEE T PATTERN ANAL, V31, P1294, DOI 10.1109/TPAMI.2008.138
   Lee CT, 2012, IEEE T MULTIMEDIA, V14, P608, DOI 10.1109/TMM.2012.2191398
   Lee SY, 2013, IEEE T MULTIMEDIA, V15, P1719, DOI 10.1109/TMM.2013.2271747
   LIN JH, 1991, IEEE T INFORM THEORY, V37, P145, DOI 10.1109/18.61115
   Liu J., 2008, PROC IEEE C COMPUT V, P1
   Liu LQ, 2016, IEEE T PATTERN ANAL, V38, P224, DOI 10.1109/TPAMI.2015.2441069
   Lobel H, 2015, IEEE T PATTERN ANAL, V37, P2218, DOI 10.1109/TPAMI.2015.2408349
   Madeo S, 2017, IEEE T MULTIMEDIA, V19, P221, DOI 10.1109/TMM.2016.2615521
   Mairal J., 2009, ADV NEURAL INFORM PR, P1033
   Mairal J., 2009, P 26 ANN INT C MACHI, P689, DOI 10.1145/1553374.1553463
   Mairal J., 2008, PROC IEEE C COMPUT V, P1
   Mairal J, 2012, IEEE T PATTERN ANAL, V34, P791, DOI 10.1109/TPAMI.2011.156
   Martinez A.M., 1998, AR FACE DATABASE CVC
   Pham D.-S., 2008, PROC IEEE C COMPUT V, P1
   Qiu Q, 2014, IEEE T PATTERN ANAL, V36, P2173, DOI 10.1109/TPAMI.2014.2316824
   Qiu Q, 2011, IEEE I CONF COMP VIS, P707, DOI 10.1109/ICCV.2011.6126307
   Ramirez I, 2010, PROC CVPR IEEE, P3501, DOI 10.1109/CVPR.2010.5539964
   Rodriguez M. D., 2008, PROC IEEE C COMPUT V, P1
   Rubinstein R., 2008, TECH REP CS200808
   Sadanand S, 2012, PROC CVPR IEEE, P1234, DOI 10.1109/CVPR.2012.6247806
   Schüldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Slonim N, 2000, ADV NEUR IN, V12, P617
   Tishby N., 1999, P 37 ANN ALL C COMM, P368, DOI DOI 10.48550/ARXIV.PHYSICS/0004057
   Tishby N, 2015, 2015 IEEE INFORMATION THEORY WORKSHOP (ITW)
   Wilson S, 2014, INT CONF DIGIT SIG, P597, DOI 10.1109/ICDSP.2014.6900734
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Yang M, 2011, IEEE I CONF COMP VIS, P543, DOI 10.1109/ICCV.2011.6126286
   Yao A, 2010, PROC CVPR IEEE, P2061, DOI 10.1109/CVPR.2010.5539883
   Yeffet L, 2009, IEEE I CONF COMP VIS, P492, DOI 10.1109/ICCV.2009.5459201
NR 46
TC 2
Z9 2
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2018
VL 20
IS 1
BP 96
EP 106
DI 10.1109/TMM.2017.2716835
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FQ3WS
UT WOS:000418289700008
DA 2024-07-18
ER

PT J
AU Wu, QB
   Li, HL
   Wang, Z
   Meng, FM
   Luo, B
   Li, W
   Ngan, KN
AF Wu, Qingbo
   Li, Hongliang
   Wang, Zhou
   Meng, Fanman
   Luo, Bing
   Li, Wei
   Ngan, King N.
TI Blind Image Quality Assessment Based on Rank-Order Regularized
   Regression
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Imagequality assessment; rank-order regularized regression
ID NATURAL SCENE STATISTICS
AB Blind image quality assessment (BIQA) aims to estimate the subjective quality of a query image without access to the reference image. Existing learning-based methods typically train a regression function by minimizing the average error between subjective opinion scores and model predictions. However, minimizing average error does not necessarily lead to correct quality rank-orders between the test images, which is a highly desirable property of image quality models. In this paper, we propose a novel rank-order regularized regression model to address this problem. The key idea is to introduce a pairwise rank-order constraint into the maximum margin regression framework, aiming to better preserve the correct perceptual preference. To the best of our knowledge, this is the first attempt to incorporate rank-order constraints into margin-based quality regression model. By combing with a new local spatial structure feature, we achieve highly consistent quality prediction with human perception. Experimental results show that the proposed method outperforms many state-of-the-art BIQA metrics on popular publicly available IQA databases (i.e., LIVE-II, TID2013, VCL@FER, LIVEMD, and ChallengeDB).
C1 [Wu, Qingbo; Li, Hongliang; Meng, Fanman; Luo, Bing; Li, Wei; Ngan, King N.] Univ Elect Sci & Technol China, Sch Elect Engn, Chengdu 611731, Sichuan, Peoples R China.
   [Wang, Zhou] Univ Waterloo, Dept Elect & Comp Engn, Waterloo, ON N2L 3G1, Canada.
   [Ngan, King N.] Chinese Univ Hong Kong, Dept Elect Engn, Hong Kong, Hong Kong, Peoples R China.
C3 University of Electronic Science & Technology of China; University of
   Waterloo; Chinese University of Hong Kong
RP Wu, QB (corresponding author), Univ Elect Sci & Technol China, Sch Elect Engn, Chengdu 611731, Sichuan, Peoples R China.
EM wqb.uestc@gmail.com; hlli@uestc.edu.cn; Z.Wang@ece.uwaterloo.ca;
   fmmeng@uestc.edu.cn; mathild1987@163.com; weili.cv@gmail.com;
   knngan@ee.cuhk.edu.hk
RI Wu, Qingbo/AAF-6872-2019; Ngan, N/E-8240-2014
OI Wu, Qingbo/0000-0003-2936-6340; Ngan, N/0000-0003-1946-3235; Li,
   Hongliang/0000-0002-7481-095X; Li, Wei/0000-0002-8278-1765
FU National Natural Science Foundation of China [61601102, 61525102,
   61502084]; Natural Sciences and Engineering Research Council of Canada
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61601102, Grant 61525102, and Grant
   61502084, and in part by the Natural Sciences and Engineering Research
   Council of Canada. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Lingfen Sun.
   (Corresponding author: Qingbo Wu.)
CR [Anonymous], 2011, INTERIOR POINT ALGOR
   Bosse S, 2016, IEEE IMAGE PROC, P3773, DOI 10.1109/ICIP.2016.7533065
   Boyd S., 2004, CONVEX OPTIMIZATION
   Bruce V., 2003, Visual perception: Physiology, psychology, and ecology
   Burges Chris, 2005, P 22 INT C MACH LEAR, P89, DOI DOI 10.1145/1102351.1102363
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chapelle O, 2000, ADV NEUR IN, V12, P230
   Chen ZY, 2014, PROC CVPR IEEE, P3003, DOI 10.1109/CVPR.2014.384
   Cherkassky V, 1997, IEEE Trans Neural Netw, V8, P1564, DOI 10.1109/TNN.1997.641482
   Ford A., 1998, COLOUR SPACE CONVERS, V1-31, P1998
   Frank M., 1956, Naval research logistics quarterly, V3, P95, DOI [10.1002/nav.3800030109, DOI 10.1002/NAV.3800030109]
   Gao XB, 2013, IEEE T NEUR NET LEAR, V24, P2013, DOI 10.1109/TNNLS.2013.2271356
   Ghadiyaram D, 2016, IEEE T IMAGE PROCESS, V25, P372, DOI 10.1109/TIP.2015.2500021
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P1098, DOI 10.1109/TMM.2016.2547343
   Guo ZH, 2010, IEEE T IMAGE PROCESS, V19, P1657, DOI 10.1109/TIP.2010.2044957
   Han E. H., 2001, Pacific-asia conference on knowledge discovery and data mining, P53, DOI 10.1007/3-540-45357-1_9
   He LH, 2012, PROC CVPR IEEE, P1146, DOI 10.1109/CVPR.2012.6247795
   Herbrich R, 2000, ADV NEUR IN, P115
   Hochstein S, 2002, NEURON, V36, P791, DOI 10.1016/S0896-6273(02)01091-7
   Hüllermeier E, 2008, ARTIF INTELL, V172, P1897, DOI 10.1016/j.artint.2008.08.002
   Inc, 2015, Gurobi optimizer reference manual
   Jayaraman D, 2012, CONF REC ASILOMAR C, P1693, DOI 10.1109/ACSSC.2012.6489321
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Khellah FM, 2011, IEEE T IMAGE PROCESS, V20, P3270, DOI 10.1109/TIP.2011.2143422
   Li CF, 2011, IEEE T NEURAL NETWOR, V22, P793, DOI 10.1109/TNN.2011.2120620
   Lofb J., 2004, P CACSD C, P284, DOI DOI 10.1109/CACSD.2004.1393890
   Ma KD, 2016, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2016.184
   Mangasarian O.L., 1998, ADV LARGE MARGIN CLA, P135
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Mittal A, 2012, IEEE SIGNAL PROC LET, V19, P75, DOI 10.1109/LSP.2011.2179293
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Moorthy AK, 2010, IEEE SIGNAL PROC LET, V17, P513, DOI 10.1109/LSP.2010.2043888
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Panic M, 2013, COMM COM INF SC, V383, P388
   Ponomarenko N, 2013, LECT NOTES COMPUT SC, V8192, P402, DOI 10.1007/978-3-319-02895-8_36
   Rao RPN, 1999, NAT NEUROSCI, V2, P79, DOI 10.1038/4580
   Reibman AR, 2013, IEEE IMAGE PROC, P413, DOI 10.1109/ICIP.2013.6738085
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Schölkopf B, 2001, LECT NOTES ARTIF INT, V2111, P416, DOI 10.1007/3-540-44581-1_27
   Schölkopf B, 1999, ADVANCES IN KERNEL METHODS, P327
   Sheikh H.R., Live Image Quality Assessment Database
   Sheskin D.J., 2003, HDB PARAMETRIC NONPA, DOI [DOI 10.4324/9780203489536, 10.4324/9780203489536]
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Smola AJ, 2004, STAT COMPUT, V14, P199, DOI 10.1023/B:STCO.0000035301.49549.88
   Song TC, 2014, IEEE SIGNAL PROC LET, V21, P93, DOI 10.1109/LSP.2013.2293335
   Vanderbei RJ, 1999, OPTIM METHOD SOFTW, V11-2, P451, DOI 10.1080/10556789908805759
   Video Quality Experts Group, 2000, FIN REP VID QUALITY
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2011, IEEE SIGNAL PROC MAG, V28, P137, DOI 10.1109/MSP.2011.942295
   Wu JJ, 2014, IEEE SIGNAL PROC LET, V21, P437, DOI 10.1109/LSP.2014.2304714
   Wu QB, 2016, IEEE T CIRC SYST VID, V26, P425, DOI 10.1109/TCSVT.2015.2412773
   Wu QB, 2015, IEEE IMAGE PROC, P339, DOI 10.1109/ICIP.2015.7350816
   Wu QB, 2015, J VIS COMMUN IMAGE R, V32, P205, DOI 10.1016/j.jvcir.2015.08.009
   Xue WF, 2013, PROC CVPR IEEE, P995, DOI 10.1109/CVPR.2013.133
   Yan JZ, 2014, PROC CVPR IEEE, P2987, DOI 10.1109/CVPR.2014.382
   Ye P, 2012, IEEE T IMAGE PROCESS, V21, P3129, DOI 10.1109/TIP.2012.2190086
   Zaric A, 2011, ELMAR PROC, P105
   Zhang F, 2011, IEEE T MULTIMEDIA, V13, P615, DOI 10.1109/TMM.2011.2134079
   Zhang H., 2006, 2006 IEEE COMP SOC C, V2, P2126, DOI DOI 10.1109/CVPR.2006.301
   Zhang M, 2013, IEEE INT NEW CIRC
   Zhou WJ, 2016, IEEE T MULTIMEDIA, V18, P1077, DOI 10.1109/TMM.2016.2542580
NR 62
TC 42
Z9 45
U1 0
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2017
VL 19
IS 11
BP 2490
EP 2504
DI 10.1109/TMM.2017.2700206
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FJ9CT
UT WOS:000413068200011
DA 2024-07-18
ER

PT J
AU Xie, J
   Dai, GX
   Fang, Y
AF Xie, Jin
   Dai, Guoxian
   Fang, Yi
TI Deep Multimetric Learning for Shape-Based 3D Model Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D shape retrieval; 3D shape descriptor; deep neural network; multiple
   shape features; metric learning
ID FEATURES; DESCRIPTORS; ROBUST
AB Recently, feature-learning-based 3D shape retrieval methods have been receiving more and more attention in the 3D shape analysis community. In these methods, the hand-crafted metrics or the learned linear metrics are usually used to compute the distances between shape features. Since there are complex geometric structural variations with 3D shapes, the single hand-crafted metric or learned linear metric cannot characterize the manifold, where 3D shapes lie well. In this paper, by exploring the nonlinearity of the deep neural network and the complementarity among multiple shape features, we propose a novel deep multimetric network for 3D shape retrieval. The developed multimetric network minimizes a discriminative loss function that, for each type of shape feature, the outputs of the network from the same class are encouraged to be as similar as possible and the outputs from different classes are encouraged to be as dissimilar as possible. Meanwhile, the Hilbert-Schmidt independence criterion is employed to enforce the outputs of different types of shape features to be as complementary as possible. Furthermore, the weights of the learned multiple distance metrics can be adaptively determined in our developed deep metric network. The weighted distance metric is then used as the similarity for shape retrieval. We conduct experiments with the proposed method on the four benchmark shape datasets. Experimental results demonstrate that the proposed method can obtain better performance than the learned deep single metric and outperform the state-of-the-art 3D shape retrieval methods.
C1 [Xie, Jin; Dai, Guoxian; Fang, Yi] NYU, Dept Elect & Comp Engn, Multimedia & Vis Computing Lab, Abu Dhabi 129188, U Arab Emirates.
   [Xie, Jin; Dai, Guoxian; Fang, Yi] NYU, Tandon Sch Engn, Dept Comp Sci & Engn, New York, NY 10003 USA.
C3 New York University; New York University Tandon School of Engineering
RP Fang, Y (corresponding author), NYU, Dept Elect & Comp Engn, Multimedia & Vis Computing Lab, Abu Dhabi 129188, U Arab Emirates.
EM jin.xie@nyu.edu; guoxian.dai@nyu.edu; yfang@nyu.edu
RI Dai, Guoxian/AHD-1334-2022
CR Agathos A., 2009, 3DOR, P29
   [Anonymous], EUR WORKSH 3D OBJ RE, DOI DOI 10.2312/3DOR/3DOR08/009-016
   [Anonymous], 2014, P EG3DOR 2014
   [Anonymous], 2009, P ACM INT C IM VID R, DOI DOI 10.1145/1646396.1646430
   Ansary TF, 2007, IEEE T MULTIMEDIA, V9, P78, DOI 10.1109/TMM.2006.886359
   Assfalg J, 2007, IEEE T MULTIMEDIA, V9, P589, DOI 10.1109/TMM.2006.886271
   Aubry M, 2011, IEEE I CONF COMP VIS, P1411, DOI 10.1109/ICCV.2011.6126396
   Bai S, 2016, PROC CVPR IEEE, P5023, DOI 10.1109/CVPR.2016.543
   Bai S, 2015, PATTERN RECOGN LETT, V65, P15, DOI 10.1016/j.patrec.2015.06.022
   Bai X, 2015, IEEE T PATTERN ANAL, V37, P2361, DOI 10.1109/TPAMI.2015.2424863
   Bai X, 2014, IEEE T IMAGE PROCESS, V23, P3935, DOI 10.1109/TIP.2014.2336542
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bronstein AM, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1899404.1899405
   Bronstein MM, 2010, PROC CVPR IEEE, P1704, DOI 10.1109/CVPR.2010.5539838
   Bu SH, 2014, IEEE T MULTIMEDIA, V16, P2154, DOI 10.1109/TMM.2014.2351788
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Chen JY, 2014, IEEE T MULTIMEDIA, V16, P337, DOI 10.1109/TMM.2013.2286580
   Chen Q, 2015, MULTIMED TOOLS APPL, V74, P4907, DOI 10.1007/s11042-013-1850-9
   Daras P, 2010, INT J COMPUT VISION, V89, P229, DOI 10.1007/s11263-009-0277-2
   Darom T, 2012, IEEE T IMAGE PROCESS, V21, P2758, DOI 10.1109/TIP.2012.2183142
   Gao Y, 2010, P ACM INT C MULT FIR, P955
   Gao Y, 2011, IEEE T MULTIMEDIA, V13, P1007, DOI 10.1109/TMM.2011.2160619
   Giachetti A, 2012, COMPUT GRAPH FORUM, V31, P1669, DOI 10.1111/j.1467-8659.2012.03172.x
   Gretton A, 2005, LECT NOTES ARTIF INT, V3734, P63
   Han Z., IEEE T NEUR IN PRESS
   Han ZZ, 2016, IEEE T IMAGE PROCESS, V25, P5331, DOI 10.1109/TIP.2016.2605920
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Lavoué G, 2012, VISUAL COMPUT, V28, P931, DOI 10.1007/s00371-012-0724-x
   Leng BA, 2016, INFORM SCIENCES, V366, P188, DOI 10.1016/j.ins.2015.08.007
   Leng B, 2015, SIGNAL PROCESS, V112, P119, DOI 10.1016/j.sigpro.2014.09.005
   Leng B, 2015, NEUROCOMPUTING, V151, P593, DOI 10.1016/j.neucom.2014.06.084
   Li B, 2013, MULTIMED TOOLS APPL, V62, P821, DOI 10.1007/s11042-011-0873-3
   Li CY, 2013, VISUAL COMPUT, V29, P513, DOI 10.1007/s00371-013-0815-3
   Lian Z., 2015, PROC 8 EUROGRAPHICS, P107
   Litman R, 2014, COMPUT GRAPH FORUM, V33, P127, DOI 10.1111/cgf.12438
   Nocedal J, 2006, SPRINGER SER OPER RE, P135
   OHBUCHI R., 2010, Proceedings of the ACM workshop on 3D object retrieval, P63
   Pickup D., 2014, PROC 7 EUROGRAPHICS, P101
   Saupe D., 2001, Pattern Recognition. 23rd DAGM Symposium. Proceedings (Lecture Notes in Computer Science Vol.2191), P392
   Shi BG, 2015, IEEE SIGNAL PROC LET, V22, P2339, DOI 10.1109/LSP.2015.2480802
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   Siddiqi K, 2008, MACH VISION APPL, V19, P261, DOI 10.1007/s00138-007-0097-8
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Tabia H., 2013, Proceedings of the Sixth Eurographics Workshop on 3D Object Retrieval, P17
   Tabia H, 2014, PROC CVPR IEEE, P4185, DOI 10.1109/CVPR.2014.533
   Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018
   Wong HS, 2007, IEEE T MULTIMEDIA, V9, P1026, DOI 10.1109/TMM.2007.898915
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xie J, 2015, PROC CVPR IEEE, P1275, DOI 10.1109/CVPR.2015.7298732
   Ye J., 2013, Proceedings of the 3rd ACM conference on International conference on multimedia retrieval, P121
   Zhu ZT, 2014, 2014 INTERNATIONAL CONFERENCE ON SECURITY, PATTERN ANALYSIS, AND CYBERNETICS (SPAC), P279, DOI 10.1109/SPAC.2014.6982699
NR 52
TC 34
Z9 34
U1 0
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2017
VL 19
IS 11
BP 2463
EP 2474
DI 10.1109/TMM.2017.2698200
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FJ9CT
UT WOS:000413068200009
DA 2024-07-18
ER

PT J
AU Tasaka, S
AF Tasaka, Shuji
TI Bayesian Hierarchical Regression Models for QoE Estimation and
   Prediction in Audiovisual Communications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audiovisual communications; Bayesian statistical modeling; model
   checking; prediction; quality of experience (QoE)
ID QUALITY; EXPERIENCE
AB This paper presents a trial of establishing a framework for Bayesian statistical modeling of quality of experience (QoE) estimation and prediction in multimedia IP communications. As an example, we take a bandwidth guaranteed interactive audiovisual communication system with a QoE enhancement mechanism. By Bayesian statistical analysis, we demonstrate how QoE is affected by factors of the channel bandwidth, contents of tasks, customization of playout buffering control for QoE enhancement, users' individualities, and gender. Adopting a latent variable approach, we build Bayesian regression models with covariates of the factors and random effect terms having hierarchical priors for the users' attributes. We estimate the posterior probabilities of parameters in the models by performing Markov chain Monte Carlo simulations with WinBUGS. From among the models, we select the most plausible one and explore the sensitivity to priors. We then evaluate the QoE measure, which is the posterior mean of overall satisfaction measured as five-point scores, and examine the effects of the factors on the QoE through odds ratios. Also, the Bayesian model makes predictions of score frequencies and compares them with measured ones to reveal high accuracy of the prediction. In addition, a cross-validatory approach to the model checking is taken for investigating the adequacy of the model; the posterior predictive p-values of a discrepancy function of the model indicate that the model is satisfactory.
C1 [Tasaka, Shuji] Nagoya Ind Sci Res Inst, Nagoya, Aichi 4640819, Japan.
RP Tasaka, S (corresponding author), Nagoya Ind Sci Res Inst, Nagoya, Aichi 4640819, Japan.
EM tasaka@nisri.jp
OI Tasaka, Shuji/0000-0002-2882-1914
FU Japan Society for the Promotion of Science; Grants-in-Aid for Scientific
   Research [17K06454] Funding Source: KAKEN
FX This work was supported in part by the Grant-In-Aid for Scientific
   Research of Japan Society for the Promotion of Science. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Xiaoqing Zhu.
CR [Anonymous], 2013, THE BUGS BOOK
   [Anonymous], 2013, 1003 COST IC
   [Anonymous], P IEEE CONS COMM NET
   [Anonymous], 2006, Data Analysis using Regression and Multilevel/Hierarchical Models, DOI DOI 10.1017/CBO9780511790942
   [Anonymous], 2012, 2012 IEEE COOL CHIPS
   [Anonymous], QOS CONG MAN CONF GU
   [Anonymous], P IEEE ICC2015 LOND
   [Anonymous], 2014, BAYESIAN DATA ANAL
   [Anonymous], INT TEST METHIODS AU
   [Anonymous], P WORLD TEL C JUN
   Brooks P, 2010, IEEE NETWORK, V24, P8, DOI 10.1109/MNET.2010.5430138
   Carlin BP, 2009, CH CRC TEXT STAT SCI, V78, P1
   Congdon P, 2005, WILEY SER PROBAB ST, P1, DOI 10.1002/0470092394
   Congdon P., 2014, Applied Bayesian modelling
   Hameed A, 2016, IEEE T MULTIMEDIA, V18, P764, DOI 10.1109/TMM.2016.2525862
   Hossfeld T, 2011, INT WORK QUAL MULTIM, P131, DOI 10.1109/QoMEX.2011.6065690
   Imbens GW, 2015, CAUSAL INFERENCE FOR STATISTICS, SOCIAL, AND BIOMEDICAL SCIENCES: AN INTRODUCTION, P1, DOI 10.1017/CBO9781139025751
   Ito K, 2001, 2001 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS, VOLS 1-10, CONFERENCE RECORD, P2865, DOI 10.1109/ICC.2001.936673
   Janowski L, 2015, IEEE T MULTIMEDIA, V17, P2210, DOI 10.1109/TMM.2015.2484963
   Janowski L, 2009, INT WORK QUAL MULTIM, P35, DOI 10.1109/QOMEX.2009.5246979
   Kaede S, 2015, 2015 IEEE 18TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL SCIENCE AND ENGINEERING (CSE), P35, DOI 10.1109/CSE.2015.24
   Laghari KUR, 2012, IEEE COMMUN MAG, V50, P58, DOI 10.1109/MCOM.2012.6178834
   Lykourentzou I, 2013, AMB INTELL SMART ENV, V17, P507, DOI 10.3233/978-1-61499-286-8-507
   Menkovski V., 2009, PROCEED INGS 7 INT C, P52
   Mitra K, 2015, IEEE T MOBILE COMPUT, V14, P920, DOI 10.1109/TMC.2013.155
   Pearl J, 2009, CAUSALITY, DOI DOI 10.1017/CBO9780511803161
   Song JR, 2016, IEEE T MULTIMEDIA, V18, P444, DOI 10.1109/TMM.2016.2520090
   Song W, 2014, IEEE T MULTIMEDIA, V16, P738, DOI 10.1109/TMM.2014.2298217
   Tasaka S., 2009, PROC IEEE GLOBAL TEL, P1
   Tasaka S, 2016, IEEE ICC, DOI 10.1109/ICC.2016.7511202
   Tsiropoulou EE, 2016, 2016 11TH INTERNATIONAL WORKSHOP ON SEMANTIC AND SOCIAL MEDIA ADAPTATION AND PERSONALIZATION (SMAP), P77, DOI 10.1109/SMAP.2016.7753388
   Ullah I, 2010, LECT NOTES COMPUT SC, V6155, P2, DOI 10.1007/978-3-642-13986-4_2
   Vegesna S., 2001, IP Quality of Service
NR 33
TC 19
Z9 19
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2017
VL 19
IS 6
BP 1195
EP 1208
DI 10.1109/TMM.2017.2652064
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5YS
UT WOS:000404059400007
DA 2024-07-18
ER

PT J
AU Fan, R
   Zhang, YF
   Li, B
AF Fan, Rui
   Zhang, Yongfei
   Li, Bo
TI Motion Classification-Based Fast Motion Estimation for High-Efficiency
   Video Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video coding; high-efficiency video coding (HEVC); motion estimation
   (ME); motion classification
ID SUCCESSIVE ELIMINATION ALGORITHM; PARALLEL FRAMEWORK; SEARCH ALGORITHM;
   HEVC; COMPLEXITY
AB High efficiency video coding (HEVC), the latest video coding standard, is becoming popular due to its excellent coding performance. However, the significant gain in performance is achieved at the cost of substantially higher encoding complexity than its precedent H.264/AVC, in which motion estimation (ME) is the most time-consuming module that effectively removes temporal redundancy. Test zone search (TZS) is adopted as the default fast ME method in the reference software of HEVC; however, its computational complexity is still too high for realtime applications. Several fast ME algorithms have been recently proposed to further reduce ME complexity; however, these approaches typically lead to non-negligible performance loss. To address this problem, this paper proposes a motion classification-based fast ME algorithm. By exploring the motion relationship of neighboring blocks and the coding cost characteristic, the prediction unit (PU) is first categorized into one of three classes, namely, motion-smooth PU, motion-medium PU and motion-complexPU. Then different search strategies are carefully designed for PUs of each class according to their respective motion and content characteristics. Furthermore, a fast search priority-based partial internal termination scheme is presented to rapidly skip impossible positions that speeds up cost computation during the ME process. Extensive experimental results demonstrate that the proposed algorithm achieves as much as 12.47% and 20.25% reductions in total encoder complexity when compared with TZS under low delay P and random access configuration, respectively, with negligible rate-distortion degradation; thus, it outperforms state-of-the-art fast ME algorithms in terms of both coding performance and complexity reduction.
C1 [Fan, Rui; Zhang, Yongfei; Li, Bo] Beihang Univ, Sch Comp Sci & Engn, Beijing Key Lab Digital Media, Beijing 100191, Peoples R China.
   [Zhang, Yongfei; Li, Bo] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
C3 Beihang University; Beihang University
RP Zhang, YF (corresponding author), Beihang Univ, Sch Comp Sci & Engn, Beijing Key Lab Digital Media, Beijing 100191, Peoples R China.; Zhang, YF (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
EM frwjxwx@163.com; yfzhang@buaa.edu.cn; boli@buaa.edu.cn
RI Li, bo/IWL-9318-2023; Li, Bo/AAA-8968-2020; Zhang, Yongfei/A-1505-2010
OI Li, Bo/0000-0002-7294-6888; Zhang, Yongfei/0000-0002-5080-1733
FU National Key Research and Development Plan [2016YFC0801001]; National
   Natural Science Foundation of China [61272502, 61632001]
FX This work was supported in part by the National Key Research and
   Development Plan under Grant 2016YFC0801001, in part by the National
   Natural Science Foundation of China under Grant 61272502, and in part by
   the State Key Program of National Natural Science Foundation of China
   under Grant 61632001. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Lingfen Sun.
   (Corresponding author: Yongfei Zhang.)
CR [Anonymous], 1981, P NAT TEL C NEW ORL
   [Anonymous], 2001, ITU T SG16Q6 VCEG
   [Anonymous], 2012, INT C COMP TECHN SCI
   [Anonymous], 2013, P 11 JCT VC M
   [Anonymous], 2016, HM REFERENCE SOFTWAR
   [Anonymous], 2012, document JCTVC-H1100 of JCT-VC
   Bossen F, 2012, IEEE T CIRC SYST VID, V22, P1685, DOI 10.1109/TCSVT.2012.2221255
   Cheung CH, 2005, IEEE T MULTIMEDIA, V7, P16, DOI 10.1109/TMM.2004.840609
   Gao LF, 2015, IEEE IMAGE PROC, P2810, DOI 10.1109/ICIP.2015.7351315
   Gao XQ, 2000, IEEE T IMAGE PROCESS, V9, P501, DOI 10.1109/83.826786
   Hu N, 2014, IEEE T CIRC SYST VID, V24, P1310, DOI 10.1109/TCSVT.2014.2306035
   Huang YW, 2004, IEEE T CIRC SYST VID, V14, P898, DOI 10.1109/TCSVT.2004.828321
   Ismail Y, 2012, IEEE T CIRC SYST VID, V22, P28, DOI 10.1109/TCSVT.2011.2148450
   Jeong JH, 2015, INT SOC DESIGN CONF, P275, DOI 10.1109/ISOCC.2015.7401754
   Jing X, 2004, IEEE T MULTIMEDIA, V6, P435, DOI 10.1109/TMM.2004.827517
   Kim I. K., 2013, JCTVCL1002
   Ko YH, 2011, IEEE T CONSUM ELECTR, V57, P726, DOI 10.1109/TCE.2011.5955214
   LI W, 1995, IEEE T IMAGE PROCESS, V4, P105, DOI 10.1109/83.350809
   Liao ZT, 2015, INT SOC DESIGN CONF, P267, DOI 10.1109/ISOCC.2015.7401750
   Moon YH, 2013, IEEE T MULTIMEDIA, V15, P477, DOI 10.1109/TMM.2012.2232648
   Nalluri P, 2015, SIGNAL PROCESS-IMAGE, V39, P280, DOI 10.1016/j.image.2015.09.015
   Parmar N, 2014, INT SOC DESIGN CONF, P260, DOI 10.1109/ISOCC.2014.7087637
   Shen LQ, 2012, IEEE T IMAGE PROCESS, V21, P2582, DOI 10.1109/TIP.2011.2177849
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Yan CG, 2014, IEEE T CIRC SYST VID, V24, P2077, DOI 10.1109/TCSVT.2014.2335852
   Yan CG, 2014, IEEE SIGNAL PROC LET, V21, P573, DOI 10.1109/LSP.2014.2310494
   Yang SH, 2014, ELECTRON LETT, V50, P673, DOI 10.1049/el.2014.0536
   Zhao Liang., 2014, 2014 12th IEEE International Conference on Solid-State and Integrated Circuit Technology (ICSICT), P1
   Zhu C, 2002, IEEE T CIRC SYST VID, V12, P349, DOI 10.1109/TCSVT.2002.1003474
   Zhu S, 2000, IEEE T IMAGE PROCESS, V9, P287, DOI 10.1109/83.821744
   Zhu WJ, 2015, IEEE T MULTIMEDIA, V17, P935, DOI 10.1109/TMM.2015.2428171
NR 32
TC 38
Z9 39
U1 1
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2017
VL 19
IS 5
BP 893
EP 907
DI 10.1109/TMM.2016.2642786
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5XS
UT WOS:000404056000001
DA 2024-07-18
ER

PT J
AU Wang, HY
   Cen, YG
   He, ZH
   Zhao, RZ
   Cen, Y
   Zhang, FZ
AF Wang, Hengyou
   Cen, Yigang
   He, Zhihai
   Zhao, Ruizhen
   Cen, Yi
   Zhang, Fengzhen
TI Robust Generalized Low-Rank Decomposition of Multimatrices for Image
   Recovery
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Alternating direction matrices tri-factorization method (ADMTFM);
   dimensionality reduction; generalized low-rank approximations of
   matrices (GLRAM); image recovery; low-rank matrices
ID STRUCTURE-FROM-MOTION; MATRIX COMPLETION; JOINT SPARSE; REPRESENTATION;
   FACTORIZATION; RESTORATION
AB Low-rank approximation has been successfully used for dimensionality reduction, image noise removal, and image restoration. In existing work, input images are often reshaped to a matrix of vectors before low-rank decomposition. It has been observed that this procedure will destroy the inherent two-dimensional correlation within images. To address this issue, the generalized low-rank approximation of matrices (GLRAM) method has been recently developed, which is able to perform low-rank decomposition of multiple matrices directly without the need for vector reshaping. In this paper, we propose a new robust generalized low-rank matrices decomposition method, which further extends the existing GLRAM method by incorporating rank minimization into the decomposition process. Specifically, our method aims to minimize the sum of nuclear norms and l(1)-norms. We develop a new optimization method, called alternating direction matrices tri-factorization method, to solve the minimization problem. We mathematically prove the convergence of the proposed algorithm. Our extensive experimental results demonstrate that our method significantly outperforms existing GLRAM methods.
C1 [Wang, Hengyou; Cen, Yigang; Zhao, Ruizhen; Zhang, Fengzhen] Beijing Jiaotong Univ, Sch Comp & Informat Technol, Beijing 100044, Peoples R China.
   [Wang, Hengyou; Cen, Yigang; Zhao, Ruizhen; Zhang, Fengzhen] Key Lab Adv Informat Sci & Network Technol Beijin, Beijing 100044, Peoples R China.
   [Wang, Hengyou] Beijing Univ Civil Engn & Architecture, Sch Sci, Beijing 100044, Peoples R China.
   [He, Zhihai] Univ Missouri, Dept Elect & Comp Engn, Columbia, MO 65211 USA.
   [Cen, Yigang] Minzu Univ China, Sch Informat Engn, Beijing 100081, Peoples R China.
C3 Beijing Jiaotong University; Beijing University of Civil Engineering &
   Architecture; University of Missouri System; University of Missouri
   Columbia; Minzu University of China
RP Cen, YG (corresponding author), Beijing Jiaotong Univ, Sch Comp & Informat Technol, Beijing 100044, Peoples R China.; Cen, YG (corresponding author), Minzu Univ China, Sch Informat Engn, Beijing 100081, Peoples R China.
EM 12112064@bjtu.edu.cn; ygcen@bjtu.edu.cn; hezhi@missouri.edu;
   rzhzhao@bjtu.edu.cn; yi_cen@126.com; 13112062@bjtu.edu.cn
RI He, Zhihai/A-5885-2019; Cen, Yigang/AAC-1999-2019
FU National High Technology Research and Development Program (863 Program)
   of China [2014AA015202]; National Natural Science Foundation of China
   [61572067, 61502024, 61272028, 61602538]; Beijing Municipal Natural
   Science Foundation [4162050]; Natural Science Foundation of Guangdong
   Province [2016A030313708]; Science and Technology Plan of Beijing
   Municipal Education Commission [SQKM201610016009]; Beijing University of
   Civil Engineering and Architecture
FX This work was supported in part by the National High Technology Research
   and Development Program (863 Program) of China under Grant 2014AA015202,
   in part by the National Natural Science Foundation of China under Grant
   61572067, Grant 61502024, Grant 61272028, and Grant 61602538, in part by
   Beijing Municipal Natural Science Foundation under Grant 4162050, in
   part by the Natural Science Foundation of Guangdong Province under Grant
   2016A030313708, in part by the Science and Technology Plan of Beijing
   Municipal Education Commission under Grant SQKM201610016009, and in part
   by the Talent Program of Beijing University of Civil Engineering and
   Architecture. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Tommaso Melodia.
   (Corresponding author: Yigang Cen.)
CR Boumal N, 2015, LINEAR ALGEBRA APPL, V475, P200, DOI 10.1016/j.laa.2015.02.027
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Candès E, 2012, COMMUN ACM, V55, P111, DOI 10.1145/2184319.2184343
   Candès EJ, 2008, IEEE SIGNAL PROC MAG, V25, P21, DOI 10.1109/MSP.2007.914731
   Candès EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Candès EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061
   Cen YG, 2013, J APPL MATH, DOI 10.1155/2013/864132
   Chen JH, 2014, IEEE T CYBERNETICS, V44, P1432, DOI 10.1109/TCYB.2013.2286106
   Chen YD, 2013, IEEE T INFORM THEORY, V59, P4324, DOI 10.1109/TIT.2013.2249572
   Cui P, 2012, IEEE T MULTIMEDIA, V14, P102, DOI 10.1109/TMM.2011.2176110
   GOWER J., 2004, Procrustes Problems
   Hong CQ, 2015, IEEE T IMAGE PROCESS, V24, P5659, DOI 10.1109/TIP.2015.2487860
   Huang C, 2014, IEEE T IMAGE PROCESS, V23, P5284, DOI 10.1109/TIP.2014.2363734
   Ji H, 2011, SIAM J IMAGING SCI, V4, P1122, DOI 10.1137/100817206
   Ji TY, 2016, INFORM SCIENCES, V326, P243, DOI 10.1016/j.ins.2015.07.049
   Ke QF, 2005, PROC CVPR IEEE, P739
   Khan I, 2014, IEEE T MULTIMEDIA, V16, P1350, DOI 10.1109/TMM.2014.2308415
   Kundu Sandipan, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P8028, DOI 10.1109/ICASSP.2014.6855164
   Kwak N, 2008, IEEE T PATTERN ANAL, V30, P1672, DOI 10.1109/TPAMI.2008.114
   Lai ZH, 2016, IEEE T NEUR NET LEAR, V27, P723, DOI 10.1109/TNNLS.2015.2422994
   Larsen R.M., 2004, PROPACK SOFTWARE LAR
   Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88
   Liu YY, 2013, PATTERN RECOGN, V46, P163, DOI 10.1016/j.patcog.2012.07.003
   Ma R, 2014, IEEE T SIGNAL PROCES, V62, P1671, DOI 10.1109/TSP.2014.2301139
   Markopoulos PP, 2014, IEEE T SIGNAL PROCES, V62, P5046, DOI 10.1109/TSP.2014.2338077
   Markopoulos PanosP., 2013, ISWCS 2013, The Tenth International Symposium on Wireless Communication Systems, Ilmenau, TU Ilmenau, Germany, August 27-30, 2013, P1
   Meng F, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0108125
   Nie F., 2011, INT JOINT C ART INT, V22, P1433
   Peng YG, 2014, IEEE T CYBERNETICS, V44, P2418, DOI 10.1109/TCYB.2014.2307854
   Peng YG, 2012, IEEE T PATTERN ANAL, V34, P2233, DOI 10.1109/TPAMI.2011.282
   Recht B., 2007, CORR
   SCHONEMA.PH, 1966, PSYCHOMETRIKA, V31, P1, DOI 10.1007/BF02289451
   Shi CJA, 2015, IEEE T MULTIMEDIA, V17, P16, DOI 10.1109/TMM.2014.2375792
   Shi JL, 2015, AER ADV ENG RES, V10, P9
   Sumarsono A, 2015, IEEE T GEOSCI REMOTE, V53, P6286, DOI 10.1109/TGRS.2015.2438079
   Tanner J, 2016, APPL COMPUT HARMON A, V40, P417, DOI 10.1016/j.acha.2015.08.003
   Wang HY, 2014, NEUROCOMPUTING, V145, P374, DOI 10.1016/j.neucom.2014.05.021
   Wen RP, 2015, APPL MATH COMPUT, V258, P537, DOI 10.1016/j.amc.2015.02.041
   Ye JP, 2005, MACH LEARN, V61, P167, DOI 10.1007/s10994-005-3561-6
   Yin M, 2016, IEEE T PATTERN ANAL, V38, P504, DOI 10.1109/TPAMI.2015.2462360
   Yuan X., 2009, SPARSE LOW RANK MATR
   Yuan Y, 2016, IEEE T CYBERNETICS, V46, P2966, DOI 10.1109/TCYB.2015.2484324
   Zhou Z, 2015, IEEE T MULTIMEDIA, V17, P171, DOI 10.1109/TMM.2014.2384396
NR 43
TC 33
Z9 35
U1 1
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2017
VL 19
IS 5
BP 969
EP 983
DI 10.1109/TMM.2016.2638624
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5XS
UT WOS:000404056000007
DA 2024-07-18
ER

PT J
AU Zhang, WD
   Zhang, W
   Liu, K
   Gu, J
AF Zhang, Weidong
   Zhang, Wei
   Liu, Kan
   Gu, Jason
TI Learning to Predict High-Quality Edge Maps for Room Layout Estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep learning; room layout estimation; scene understanding
AB The goal of room layout estimation is to predict the three-dimensional box that represents the room spatial structure from a monocular image. In this paper, a deconvolution network is trained first to predict the edge map of a room image. Compared to the previous fully convolutional networks, the proposed deconvolution network has a multilayer deconvolution process that can refine the edge map estimate layer by layer. The deconvolution network also has fully connected layers to aggregate the information of every region throughout the entire image. During the layout generation process, an adaptive sampling strategy is introduced based on the obtained high-quality edge maps. Experimental results prove that the learned edge maps are highly reliable and can produce accurate layouts of room images.
C1 [Zhang, Weidong; Zhang, Wei; Liu, Kan; Gu, Jason] Shandong Univ, Sch Control Sci & Engn, Jinan 250061, Peoples R China.
C3 Shandong University
RP Zhang, W (corresponding author), Shandong Univ, Sch Control Sci & Engn, Jinan 250061, Peoples R China.
EM chluzhre@gmail.com; davidzhangsdu@gmail.com; sakuraxiafan@gmail.com;
   jasongusdu@gmail.com
OI Gu, Jason Jianjun/0000-0002-7626-1077
FU NSFC [61573222, 61233014]; Major Research Program of Shandong Province
   [2015ZDXX0801A02]; Fundamental Research Funds of Shandong University
   [2016JC014]; Jiangsu Key Laboratory of 3D Printing Equipment and
   Manufacturing [3DL201502]
FX This work was supported in part by the NSFC under Grant 61573222 and
   Grant 61233014, in part by the Major Research Program of Shandong
   Province under Grant 2015ZDXX0801A02, in part by The Fundamental
   Research Funds of Shandong University under Grant 2016JC014, and in part
   by Open Program of Jiangsu Key Laboratory of 3D Printing Equipment and
   Manufacturing 3DL201502. The associate editor coordinating the review of
   this manuscript and approving it for publication was Prof. Shu-Ching
   Chen. (Corresponding author: Wei Zhang.)
CR [Anonymous], 2001, Robotica, DOI DOI 10.1017/S0263574700223217
   Badrinarayanan V., 2017, TPAMI, DOI DOI 10.1109/TPAMI.2016.2644615
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Dasgupta S, 2016, PROC CVPR IEEE, P616, DOI 10.1109/CVPR.2016.73
   Del Pero L, 2013, PROC CVPR IEEE, P153, DOI 10.1109/CVPR.2013.27
   Ding CX, 2015, IEEE T MULTIMEDIA, V17, P2049, DOI 10.1109/TMM.2015.2477042
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Girshick R., 2014, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2014.81, 10.1109/CVPR.2014.81]
   Gupta A., 2010, ADV NEURAL INFORM PR, P1288
   Hedau V, 2009, IEEE I CONF COMP VIS, P1849, DOI 10.1109/ICCV.2009.5459411
   Hoiem D, 2007, INT J COMPUT VISION, V75, P151, DOI 10.1007/s11263-006-0031-y
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee DC, 2009, PROC CVPR IEEE, P2136, DOI 10.1109/CVPRW.2009.5206872
   Liu CX, 2015, PROC CVPR IEEE, P3413, DOI 10.1109/CVPR.2015.7298963
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Mallya A, 2015, IEEE I CONF COMP VIS, P936, DOI 10.1109/ICCV.2015.113
   Mao QR, 2014, IEEE T MULTIMEDIA, V16, P2203, DOI 10.1109/TMM.2014.2360798
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178
   Pero L. D., P IEEE C COMP VIS PA, P2719
   Radford A., 2015, ARXIV
   Ramalingam S, 2013, PROC CVPR IEEE, P3065, DOI 10.1109/CVPR.2013.394
   Ren Y., 2016, CORR
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   Schwing AG, 2012, LECT NOTES COMPUT SC, V7577, P299, DOI 10.1007/978-3-642-33783-3_22
   Schwing AG, 2012, PROC CVPR IEEE, P2815, DOI 10.1109/CVPR.2012.6248006
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Wang HY, 2013, COMMUN ACM, V56, P92, DOI [10.1145/2436258.2436276, 10.1145/2436256.2436276]
   Wang XL, 2015, PROC CVPR IEEE, P539, DOI 10.1109/CVPR.2015.7298652
   Zhang W, 2016, PATTERN RECOGN, V59, P176, DOI 10.1016/j.patcog.2016.01.034
   Zhang W, 2014, 2014 IEEE INTERNATIONAL CONFERENCE ON INFORMATION AND AUTOMATION (ICIA), P1176, DOI 10.1109/ICInfA.2014.6932827
   Zhang Yinda., Large-scale scene understanding challenge: Room layout estimation
   Zhao YB, 2013, PROC CVPR IEEE, P3119, DOI 10.1109/CVPR.2013.401
   Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179
NR 35
TC 32
Z9 34
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2017
VL 19
IS 5
BP 935
EP 943
DI 10.1109/TMM.2016.2642780
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5XS
UT WOS:000404056000004
OA Bronze
DA 2024-07-18
ER

PT J
AU Dong, WS
   Shi, GM
   Li, X
   Peng, KF
   Wu, JJ
   Guo, ZH
AF Dong, Weisheng
   Shi, Guangming
   Li, Xin
   Peng, Kefan
   Wu, Jinjian
   Guo, Zhenhua
TI Color-Guided Depth Recovery via Joint Local Structural and Nonlocal
   Low-Rank Regularization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Color-guided depth recovery; dual autoregressive model; joint
   local/nonlocal regularization; low-rank method; weighted total-variation
ID IMAGE-RESTORATION; TIME; SUPERRESOLUTION; RECONSTRUCTION; COMPLETION;
   RESOLUTION; ALGORITHM
AB High-quality depth recovery from RGB-D data has received increasingly more attention in recent years due to their wide applications from depth-based image rendering to three-dimensional imaging and video. Sharp contrast between high-quality color images and low-quality depth maps presents severe challenges to the development of color-guided depth recovery techniques. Previous works have emphasized either locally varying characteristics of color-depth dependence or nonlocal similarities around the discontinuities of the scene geometry. Therefore, it is desirable to exploit both local and nonlocal structural constraints for optimizing the performance of color-guided depth recovery. In this work, we propose a unified variational approach via joint local and nonlocal regularization. The local regularization term consists of two complementary parts-one characterizing the color-depth dependence in the gradient domain and the other in the spatial domain; nonlocal regularization involves a low-rank constraint suitable for large-scale depth discontinuities. Extensive experimental results are reported to show that our approach outperforms several existing state-of-the-art depth recovery methods on both synthetic and real-world data sets.
C1 [Dong, Weisheng] Xidian Univ, Sch Elect Engn, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
   [Shi, Guangming] Xidian Univ, Sch Elect Engn, Collaborat Innovat Ctr Informat Sensing & Underst, Xian 710071, Peoples R China.
   [Li, Xin] West Virginia Univ, Lane Dept CSEE, Morgantown, WV 26506 USA.
   [Peng, Kefan; Wu, Jinjian] Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China.
   [Guo, Zhenhua] Tsinghua Univ, Grad Sch Shenzhen, Shenzhen 518055, Peoples R China.
C3 Xidian University; Xidian University; West Virginia University; Xidian
   University; Tsinghua University; Tsinghua Shenzhen International
   Graduate School
RP Dong, WS (corresponding author), Xidian Univ, Sch Elect Engn, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
EM wsdong@mail.xidian.edu.cn; gmshi@xidian.edu.cn; xin.li@ieee.org;
   pkfxidian@163.com; jinjian.wu@mail.xidian.edu.cn;
   zhenhua.guo@sz.tsinghua.edu.cn
RI guo, zhenhua/AAD-1578-2020; Li, Xin/A-7884-2011; Wu,
   Jinjian/GQH-0222-2022
OI Li, Xin/0000-0003-2067-2763; 
FU NSF [CCF-1420174]; Natural Science Foundation of China [61622210,
   61471281, 61632019, 61472301, 61390512, 61372131]; Shenzhen Overseas
   High Talent Innovation Fund [KQCX20140521161756231]
FX The work was supported in part by NSF Award CCF-1420174, in part by the
   Natural Science Foundation of China under Grant 61622210, Grant
   61471281, Grant 61632019, Grant 61472301, Grant 61390512, and Grant
   61372131, and in part by the Shenzhen Overseas High Talent Innovation
   Fund under Grant KQCX20140521161756231.
CR Alexiadis DS, 2013, IEEE T MULTIMEDIA, V15, P339, DOI 10.1109/TMM.2012.2229264
   [Anonymous], IEEE T IMAGE PROCESS
   [Anonymous], 2004, INT SOC OPTICS PHOTO, DOI DOI 10.1117/12.524762
   Bose NK, 2006, IEEE T IMAGE PROCESS, V15, P2239, DOI 10.1109/TIP.2006.877406
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Candès EJ, 2010, P IEEE, V98, P925, DOI 10.1109/JPROC.2009.2035722
   Dong WS, 2015, INT J COMPUT VISION, V114, P217, DOI 10.1007/s11263-015-0808-y
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P700, DOI 10.1109/TIP.2012.2221729
   Ferstl D, 2013, IEEE I CONF COMP VIS, P993, DOI 10.1109/ICCV.2013.127
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   He KM, 2010, LECT NOTES COMPUT SC, V6311, P1
   Hirschmüller H, 2007, PROC CVPR IEEE, P2134
   Hu W, 2013, IEEE INT WORKSH MULT, P1, DOI 10.1109/MMSP.2013.6659254
   Huiping Deng, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P646, DOI 10.1109/ICME.2012.2
   Ji H, 2010, PROC CVPR IEEE, P1791, DOI 10.1109/CVPR.2010.5539849
   Jing Li, 2013, Computer Vision - ACCV 2012. 11th Asian Conference on Computer Vision. Revised Selected Papers, P205, DOI 10.1007/978-3-642-37447-0_16
   Khoshelham K, 2012, SENSORS-BASEL, V12, P1437, DOI 10.3390/s120201437
   Kiechle M, 2013, IEEE I CONF COMP VIS, P1545, DOI 10.1109/ICCV.2013.195
   Kohavi R., 1995, IJCAI-95. Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, P1137
   Kopf J, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239453
   Lange R, 2001, IEEE J QUANTUM ELECT, V37, P390, DOI 10.1109/3.910448
   Lee PJ, 2011, IEEE T MULTIMEDIA, V13, P246, DOI 10.1109/TMM.2010.2100372
   Liang H, 2014, IEEE T MULTIMEDIA, V16, P1241, DOI 10.1109/TMM.2014.2306177
   Liu MY, 2013, PROC CVPR IEEE, P169, DOI 10.1109/CVPR.2013.29
   Liu SJ, 2011, IEEE T BROADCAST, V57, P551, DOI 10.1109/TBC.2011.2120750
   Lu S, 2014, PROC CVPR IEEE, P3390, DOI 10.1109/CVPR.2014.433
   Mac Aodha O, 2012, LECT NOTES COMPUT SC, V7574, P71, DOI 10.1007/978-3-642-33712-3_6
   Ndjiki-Nya P, 2011, IEEE T MULTIMEDIA, V13, P453, DOI 10.1109/TMM.2011.2128862
   Park J, 2014, IEEE T IMAGE PROCESS, V23, P5559, DOI 10.1109/TIP.2014.2361034
   Park J, 2011, IEEE I CONF COMP VIS, P1623, DOI 10.1109/ICCV.2011.6126423
   Park SC, 2003, IEEE SIGNAL PROC MAG, V20, P21, DOI 10.1109/MSP.2003.1203207
   Petrazzuoli G, 2014, IEEE T MULTIMEDIA, V16, P1834, DOI 10.1109/TMM.2014.2342201
   Ren Z, 2013, IEEE T MULTIMEDIA, V15, P1110, DOI 10.1109/TMM.2013.2246148
   Schwarz S, 2014, IEEE T IMAGE PROCESS, V23, P214, DOI 10.1109/TIP.2013.2287613
   Unger M, 2009, LECT NOTES COMPUT SC, V5681, P193, DOI 10.1007/978-3-642-03641-5_15
   Wang YL, 2008, SIAM J IMAGING SCI, V1, P248, DOI 10.1137/080724265
   Xie J, 2015, IEEE T MULTIMEDIA, V17, P1525, DOI 10.1109/TMM.2015.2457678
   Yan B, 2012, IEEE T MULTIMEDIA, V14, P936, DOI 10.1109/TMM.2012.2184743
   Yang JY, 2014, IEEE T IMAGE PROCESS, V23, P3443, DOI 10.1109/TIP.2014.2329776
   Yang Q., 2007, PROC IEEE C COMPUT V, P1
   Yu LF, 2013, PROC CVPR IEEE, P1415, DOI 10.1109/CVPR.2013.186
   Zhang XJ, 2008, IEEE T IMAGE PROCESS, V17, P887, DOI 10.1109/TIP.2008.924279
NR 42
TC 59
Z9 59
U1 0
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2017
VL 19
IS 2
BP 293
EP 301
DI 10.1109/TMM.2016.2613824
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EN1UN
UT WOS:000395795800006
OA hybrid
DA 2024-07-18
ER

PT J
AU Jakhetiya, V
   Lin, WS
   Jaiswal, SP
   Guntuku, SC
   Au, OC
AF Jakhetiya, Vinit
   Lin, Weisi
   Jaiswal, Sunil P.
   Guntuku, Sharath Chandra
   Au, Oscar C.
TI Maximum <i>a Posterior</i> and Perceptually Motivated Reconstruction
   Algorithm: A Generic Framework
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Denoising; generic framework; IQA metrics; non-linear filters; visual
   perception
ID IMAGE INTERPOLATION; SUPERRESOLUTION; REMOVAL
AB Most of the existing image reconstruction algorithms are application specific, and have generalization issues due to the need for parameter tuning and an unknown level of signal distortion. Addressing these problems, in this paper, we propose an efficient perceptually motivated and maximum a posterior (MAP)based generic framework for image reconstruction. This can be applied to several image/video processing applications, where there is a necessity to improve reconstruction accuracy and suppress visible artifacts, such as denoising, deinterlacing, interpolation, de-blocking of Jpeg/Jpeg-2000, and demosaicing. The gradient magnitudes are noise insensitive to a moderate levels of noise and we propose to utilize this property for finding pixels with similar edge semantics in the neighborhood when neighboring pixels are noisy. With this view, we incorporate the gradient magnitude similarity based image quality assessment metric with the MAP estimation and, in turn, it can better approximate the variance of the MAP, as compared to nonlinear filters. The proposed generic algorithm (without manually tuning any parameters) is shown to produce a better quality of reconstruction when compared to the state-of-the-art application-specific algorithms, for most of the image processing applications.
C1 [Jakhetiya, Vinit; Jaiswal, Sunil P.] Hong Kong Univ Sci & Technol, Hong Kong, Hong Kong, Peoples R China.
   [Lin, Weisi; Guntuku, Sharath Chandra] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
   [Au, Oscar C.] LegalForce RAPC, Mountain View, CA 94040 USA.
C3 Hong Kong University of Science & Technology; Nanyang Technological
   University
RP Jakhetiya, V (corresponding author), Hong Kong Univ Sci & Technol, Hong Kong, Hong Kong, Peoples R China.
EM vjakhetiya@connect.ust.hk; wslin@ntu.edu.sg; spjaiswal@connect.ust.hk;
   sharathc001@e.ntu.sg; eeau90@gmail.com
RI Guntuku, Sharath Chandra/U-6314-2019; Lin, Weisi/A-3696-2011; Lin,
   Weisi/A-8011-2012
OI Guntuku, Sharath Chandra/0000-0002-2929-0035; Lin,
   Weisi/0000-0001-9866-1947; Bennett University, Computer
   Science/0000-0002-9193-5850
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   [Anonymous], 2012, Front. Bioeng. Biotechnol
   [Anonymous], INT J PHARM SCI
   Bhujle H, 2014, IEEE T IMAGE PROCESS, V23, P356, DOI 10.1109/TIP.2013.2290871
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Cai JF, 2010, J MATH IMAGING VIS, V36, P46, DOI 10.1007/s10851-009-0169-7
   Chan RH, 2005, IEEE T IMAGE PROCESS, V14, P1479, DOI 10.1109/TIP.2005.852196
   Chaudhury KN, 2012, IEEE SIGNAL PROC LET, V19, P745, DOI 10.1109/LSP.2012.2217329
   Chen PY, 2007, IEICE T INF SYST, VE90D, P606, DOI 10.1093/ietisy/e90-d.2.606
   Chen PY, 2008, IEEE SIGNAL PROC LET, V15, P833, DOI 10.1109/LSP.2008.2005047
   Chen T, 2001, IEEE SIGNAL PROC LET, V8, P1, DOI 10.1109/97.889633
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Fang L, 2012, IEEE T MULTIMEDIA, V14, P1359, DOI 10.1109/TMM.2012.2191269
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Huang DA, 2014, IEEE T MULTIMEDIA, V16, P83, DOI 10.1109/TMM.2013.2284759
   Hung KW, 2012, IET IMAGE PROCESS, V6, P877, DOI 10.1049/iet-ipr.2011.0050
   Hung KW, 2012, IEEE T IMAGE PROCESS, V21, P1061, DOI 10.1109/TIP.2011.2168416
   Jakhetiya Vinit, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P5819, DOI 10.1109/ICASSP.2014.6854719
   Jakhetiya V, 2014, IEEE INT SYMP CIRC S, P2293, DOI 10.1109/ISCAS.2014.6865629
   Kang LW, 2015, IEEE T MULTIMEDIA, V17, P921, DOI 10.1109/TMM.2015.2434216
   KEYS RG, 1981, IEEE T ACOUST SPEECH, V29, P1153, DOI 10.1109/TASSP.1981.1163711
   Kim W, 2007, IEEE T CONSUM ELECTR, V53, P1036, DOI 10.1109/TCE.2007.4341583
   Lin CH, 2010, IEEE T IMAGE PROCESS, V19, P2307, DOI 10.1109/TIP.2010.2047906
   Lin WS, 2011, J VIS COMMUN IMAGE R, V22, P297, DOI 10.1016/j.jvcir.2011.01.005
   Liu AM, 2012, IEEE T IMAGE PROCESS, V21, P1500, DOI 10.1109/TIP.2011.2175935
   Liu XM, 2014, IEEE T IMAGE PROCESS, V23, P1491, DOI 10.1109/TIP.2014.2303638
   Mallat S, 2010, IEEE T IMAGE PROCESS, V19, P2889, DOI 10.1109/TIP.2010.2049927
   Narwaria M, 2012, IEEE T SYST MAN CY B, V42, P347, DOI 10.1109/TSMCB.2011.2163391
   Rehman A, 2011, IEEE IMAGE PROC, P217, DOI 10.1109/ICIP.2011.6116065
   Skodras A, 2001, IEEE SIGNAL PROC MAG, V18, P36, DOI 10.1109/79.952804
   Takeda H, 2007, IEEE T IMAGE PROCESS, V16, P349, DOI 10.1109/TIP.2006.888330
   Talebi H, 2013, IEEE T IMAGE PROCESS, V22, P1468, DOI 10.1109/TIP.2012.2231691
   Tan HL, 2013, IEEE T IMAGE PROCESS, V22, P4447, DOI 10.1109/TIP.2013.2273671
   Thaipanich T, 2010, IEEE T CONSUM ELECTR, V56, P2623, DOI 10.1109/TCE.2010.5681149
   Tomasi C., 1988, 6 INT C COMPUTER VIS, P839
   Wang J, 2014, IEEE T CIRC SYST VID, V24, P39, DOI 10.1109/TCSVT.2013.2280068
   Wang SQ, 2012, IEEE T CIRC SYST VID, V22, P516, DOI 10.1109/TCSVT.2011.2168269
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2009, IEEE SIGNAL PROC MAG, V26, P98, DOI 10.1109/MSP.2008.930649
   Wei Dai, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P7343, DOI 10.1109/ICASSP.2014.6855026
   Wei Z, 2013, IEEE T IMAGE PROCESS, V22, P4271, DOI 10.1109/TIP.2013.2271849
   Wu JJ, 2013, IEEE T MULTIMEDIA, V15, P1705, DOI 10.1109/TMM.2013.2268053
   Wu JJ, 2013, IEEE T IMAGE PROCESS, V22, P43, DOI 10.1109/TIP.2012.2214048
   Xie J, 2015, IEEE T MULTIMEDIA, V17, P1525, DOI 10.1109/TMM.2015.2457678
   Xiong B, 2012, IEEE T IMAGE PROCESS, V21, P1663, DOI 10.1109/TIP.2011.2172804
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Yeo CH, 2013, IEEE T CIRC SYST VID, V23, P1170, DOI 10.1109/TCSVT.2013.2240918
   Zhang XJ, 2008, IEEE T IMAGE PROCESS, V17, P887, DOI 10.1109/TIP.2008.924279
   Zhang XF, 2015, ASIAPAC SIGN INFO PR, P715, DOI 10.1109/APSIPA.2015.7415365
   Zhang XF, 2013, IEEE T IMAGE PROCESS, V22, P4613, DOI 10.1109/TIP.2013.2274386
   Zhou D, 2012, IET IMAGE PROCESS, V6, P627, DOI 10.1049/iet-ipr.2011.0534
NR 51
TC 22
Z9 22
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2017
VL 19
IS 1
BP 93
EP 106
DI 10.1109/TMM.2016.2609419
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EH0SX
UT WOS:000391475200008
DA 2024-07-18
ER

PT J
AU Yao, C
   Xiao, JM
   Tillo, T
   Zhao, Y
   Lin, CY
   Bai, HH
AF Yao, Chao
   Xiao, Jimin
   Tillo, Tammam
   Zhao, Yao
   Lin, Chunyu
   Bai, Huihui
TI Depth Map Down-Sampling and Coding Based on Synthesized View Distortion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Depth coding; depth down-/up-sampling; optimal depth down-sampling;
   three-dimensional (3D); view synthesis
ID 3-D VIDEO; INTERPOLATION
AB In this paper, we propose a depth map down-sampling and coding scheme that minimizes the view synthesis distortion. Moreover, a solution for the optimal depth map down-sampling problem that minimizes the depth-caused distortion in the virtual view by exploiting the depth map and the associated texture information along with the up-sampling method to be used in the decoder side is derived. Furthermore, to enhance compression performance, the synthesized view distortion, which is evaluated by emulating the interpolation and the virtual view synthesis process, is used in the optimization objective function for coding mode selection in the video encoder. Experimental results show that both the proposed depth map down-sampling and encoding methods lead to good performance, and the average bit rate reduction is 2.62% compared with 3D-AVC.
C1 [Yao, Chao; Lin, Chunyu; Bai, Huihui] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
   [Yao, Chao; Zhao, Yao; Lin, Chunyu; Bai, Huihui] Beijing Jiaotong Univ, Beijing Key Lab Adv Informat Sci & Network Techno, Beijing 100044, Peoples R China.
   [Xiao, Jimin; Tillo, Tammam] Xian Jiaotong Liverpool Univ, Dept Elect & Elect Engn, Suzhou 215123, Peoples R China.
   [Zhao, Yao] Beijing Jiaotong Univ, Inst Informat Sci, State Key Lab Rail Traff Control & Safety, Beijing 100044, Peoples R China.
C3 Beijing Jiaotong University; Beijing Jiaotong University; Xi'an
   Jiaotong-Liverpool University; Beijing Jiaotong University
RP Yao, C (corresponding author), Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.; Yao, C (corresponding author), Beijing Jiaotong Univ, Beijing Key Lab Adv Informat Sci & Network Techno, Beijing 100044, Peoples R China.
EM yaochao1986@gmail.com; Jimin.Xiao@xjtlu.edu.cn;
   tammam.tillo@xjtlu.edu.cn; yzhao@bjtu.edu.cn; cylin@bjtu.edu.cn;
   hhbai@bjtu.edu.cn
RI Lin, Chunyu/AAI-5185-2021
OI Lin, Chunyu/0000-0003-2847-0349
FU 973 Program [2012CB316401]; National Natural Science Foundation of China
   [61210006, 61501379, 61502506, 61402034]; Jiangsu Science and Technology
   Programme [K20150375]
FX This work was supported in part by the 973 Program under Grant
   2012CB316401, in part by the National Natural Science Foundation of
   China under Grant 61210006, Grant 61501379, Grant 61502506, and Grant
   61402034, and in part by the Jiangsu Science and Technology Programme
   under Grant K20150375. The associate editor coordinating the review of
   this manuscript and approving it for publication was Prof. Chia-Wen Lin.
CR Aflaki P., 2012, ITUTSG16WP3ISOIECJTC
   Aflaki P, 2013, IEEE SIGNAL PROC LET, V20, P87, DOI 10.1109/LSP.2012.2228189
   [Anonymous], M16090 ISOIEC JTC1SC
   [Anonymous], 2012, MPEG2012N12558 ISOIE
   Bjontegaard G., 2001, 13 M ITU T SC16 SG16
   Bruckstein AM, 2003, IEEE T IMAGE PROCESS, V12, P1132, DOI 10.1109/TIP.2003.816023
   Byung Tae Oh J. L., 2012, MPEG2012M24826 ISOIE
   Deng HP, 2012, IEEE IMAGE PROC, P1301, DOI 10.1109/ICIP.2012.6467106
   Ekmekcioglu Erhan, 2008, 2008 3DTV-Conference: The True Vision - Capture, Transmission and Display of 3D Video, P137, DOI 10.1109/3DTV.2008.4547827
   Fehn C, 2004, PROC SPIE, V5291, P93, DOI 10.1117/12.524762
   Fu JJ, 2013, IEEE T MULTIMEDIA, V15, P1340, DOI 10.1109/TMM.2013.2247584
   Gao P, 2014, IEEE T MULTIMEDIA, V16, P1797, DOI 10.1109/TMM.2014.2331013
   Kim WS, 2009, IEEE IMAGE PROC, P721, DOI 10.1109/ICIP.2009.5414304
   Klimaszewski K., 2009, M16758 ISOIEC JTC1SC
   Liu Q, 2012, IEEE SIGNAL PROC LET, V19, P295, DOI 10.1109/LSP.2012.2190060
   LIU S, 2010, P SOC PHOTO-OPT INS, V7744
   Ma SW, 2014, IEEE T MULTIMEDIA, V16, P266, DOI 10.1109/TMM.2013.2284751
   Macchiavello B, 2014, IEEE T MULTIMEDIA, V16, P711, DOI 10.1109/TMM.2014.2299768
   Oh BT, 2014, IEEE T CIRC SYST VID, V24, P1006, DOI 10.1109/TCSVT.2013.2290577
   Oh BT, 2011, IEEE J-STSP, V5, P1344, DOI 10.1109/JSTSP.2011.2164893
   Oh KJ, 2009, IEEE SIGNAL PROC LET, V16, P747, DOI 10.1109/LSP.2009.2024112
   Reichel J., 2007, JOINT SCALABLE VIDEO
   Rusanovskyy D., 2013, 5 M JOINT COLL TEAM
   Shao F, 2012, IEEE T MULTIMEDIA, V14, P157, DOI 10.1109/TMM.2011.2169045
   Smolic A, 2004, IEEE T CIRC SYST VID, V14, P348, DOI 10.1109/TCSVT.2004.823395
   Tikanmäki A, 2008, I SYMP CONSUM ELECTR, P78
   Wildeboer M. O., 2010, 2010 28th Picture Coding Symposium (PCS 2010), P170, DOI 10.1109/PCS.2010.5702451
   Zhang YB, 2011, IEEE T IMAGE PROCESS, V20, P3291, DOI 10.1109/TIP.2011.2158226
NR 28
TC 11
Z9 12
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2016
VL 18
IS 10
BP 2015
EP 2022
DI 10.1109/TMM.2016.2594145
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DX8NC
UT WOS:000384644800009
DA 2024-07-18
ER

PT J
AU Asikuzzaman, M
   Alam, MJ
   Lambert, AJ
   Pickering, MR
AF Asikuzzaman, Md.
   Alam, Md. Jahangir
   Lambert, Andrew J.
   Pickering, Mark R.
TI Robust DT CWT-Based DIBR 3D Video Watermarking Using Chrominance
   Embedding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D video; camcording; depth-image-based rendering (DIBR); dual-tree
   complex wavelet transform (DT CWT); robustness; watermarking
ID IMAGE; EXTENSIONS; COLOR
AB The popularity of 3D video is increasing daily due to the availability of low-cost 3D televisions and high-speed Internet access. However, currently the contents of 3D video can be distributed illegally without any protection. For views generated using a depth-image-based rendering technique, not only the left and right views can be distributed as 3D content, but also the center, left, or right views individually as 2D content. As digital video watermarking is a possible way of protecting these views from unauthorized distribution, in this paper, we propose a digital watermarking method for depth-image-based rendered 3D video. In this method, the watermark is embedded in both of the chrominance channels of a YUV representation of the center view using the dual-tree complex wavelet transform. Then, the left and right views are generated from the watermarked center view and depth map using a depth-image based rendering technique. Finally, the watermark can be extracted from the center, left, and right views in a blind fashion without using the original unwatermarked center, left, or right views. This watermark is robust to geometric distortions, such as upscaling, rotation and cropping, downscaling to an arbitrary resolution, and the most common video distortions, including lossy compression and additive noise. Due to the approximate shift invariance characteristic of the dual-tree complex wavelet transform, the technique is robust against distortions in the left and right views generated using depth-image based rendering. The proposed method can also survive baseline distance adjustment and both 2D and 3D camcording.
C1 [Asikuzzaman, Md.; Lambert, Andrew J.; Pickering, Mark R.] Univ New South Wales, Sch Engn & Informat Technol, Canberra, ACT 2600, Australia.
   [Alam, Md. Jahangir] Powerfront Pty Ltd, Richmond, Vic 3121, Australia.
C3 University of New South Wales Sydney
RP Asikuzzaman, M (corresponding author), Univ New South Wales, Sch Engn & Informat Technol, Canberra, ACT 2600, Australia.
EM M.Asikuzzaman@adfa.edu.au; Jahangir.Alam@powerfront.com;
   A.Lambert@adfa.edu.au; M.Pickering@adfa.edu.au
RI Alam, J/Q-6342-2018; Lambert, Andrew/F-3366-2015;
   Asikuzzaman/J-1218-2019
OI Asikuzzaman/0000-0003-2079-009X; Lambert, Andrew/0000-0003-0390-4446
CR Alattar AM, 2003, IEEE T CIRC SYST VID, V13, P787, DOI 10.1109/TCSVT.2003.815958
   [Anonymous], 2000, FIN REP VID QUAL EXP
   [Anonymous], 2012, BT20211 ITUR
   Asikuzzaman Md, 2014, IEEE Transactions on Information Forensics and Security, V9, P1502, DOI 10.1109/TIFS.2014.2338274
   Asikuzzaman M., 2014, P INT C DIG IM COMP, P1
   Asikuzzaman M., 2013, P VIS COMM IM PROC V, P1
   Asikuzzaman M, 2012, INT C DIG IM COMP TE, P1
   Asikuzzaman M, 2015, 2015 PICTURE CODING SYMPOSIUM (PCS) WITH 2015 PACKET VIDEO WORKSHOP (PV), P277, DOI 10.1109/PCS.2015.7170090
   Asikuzzaman M, 2014, IEEE IMAGE PROC, P5497, DOI 10.1109/ICIP.2014.7026112
   Bradley B., 2012, P SOC PHOTO-OPT INS, V8302
   Cheng E, 2012, INT WORK QUAL MULTIM, P212, DOI 10.1109/QoMEX.2012.6263873
   Coria LE, 2008, IEEE T INF FOREN SEC, V3, P466, DOI 10.1109/TIFS.2008.927421
   Doérr G, 2004, IEEE T SIGNAL PROCES, V52, P2955, DOI 10.1109/TSP.2004.833867
   Doërr G, 2003, SIGNAL PROCESS-IMAGE, V18, P263, DOI 10.1016/S0923-5965(02)00144-3
   Fehn C, 2004, PROC SPIE, V5291, P93, DOI 10.1117/12.524762
   Gunturk BK, 2002, IEEE T IMAGE PROCESS, V11, P997, DOI 10.1109/TIP.2002.801121
   Halici E, 2009, IEEE IMAGE PROC, P4217, DOI 10.1109/ICIP.2009.5413525
   Hartung F, 1999, P SOC PHOTO-OPT INS, V3657, P147, DOI 10.1117/12.344665
   Hefeeda M, 2015, IEEE T MULTIMEDIA, V17, P420, DOI 10.1109/TMM.2015.2389628
   Holliman M, 2000, P SOC PHOTO-OPT INS, V3971, P186, DOI 10.1117/12.384972
   Kim HD, 2012, IEEE T BROADCAST, V58, P533, DOI 10.1109/TBC.2012.2206851
   Kingsbury N, 1999, PHILOS T R SOC A, V357, P2543, DOI 10.1098/rsta.1999.0447
   Kingsbury N., 2005, CONNEXIONS
   Langelaar GC, 2000, IEEE SIGNAL PROC MAG, V17, P20, DOI 10.1109/79.879337
   Lee MJ, 2010, IEEE T MULTIMEDIA, V12, P605, DOI 10.1109/TMM.2010.2061221
   Lee PJ, 2011, IEEE T MULTIMEDIA, V13, P246, DOI 10.1109/TMM.2010.2100372
   Lei JJ, 2015, IEEE T MULTIMEDIA, V17, P457, DOI 10.1109/TMM.2015.2400823
   Lin YH, 2011, IEEE T BROADCAST, V57, P602, DOI 10.1109/TBC.2011.2131470
   Loo P., 2000, P INT C IEE SEM SEC
   Miller ML, 2000, LECT NOTES COMPUT SC, V1768, P146
   Ndjiki-Nya P, 2011, IEEE T MULTIMEDIA, V13, P453, DOI 10.1109/TMM.2011.2128862
   Parraga CA, 1998, J OPT SOC AM A, V15, P563, DOI 10.1364/JOSAA.15.000563
   Pei SC, 2015, IEEE T MULTIMEDIA, V17, P128, DOI 10.1109/TMM.2014.2368255
   RECOMMENDATION ITU-R BT, 2002, METH SUBJ ASS QUAL T
   Reed A., 2010, P SOC PHOTO-OPT INS, V7542
   Sullivan GJ, 2013, IEEE J-STSP, V7, P1001, DOI 10.1109/JSTSP.2013.2283657
   Swanson MD, 1998, P IEEE, V86, P1064, DOI 10.1109/5.687830
   Tam WJ, 2004, PROC SPIE, V5599, P162, DOI 10.1117/12.583105
   Tech G, 2016, IEEE T CIRC SYST VID, V26, P35, DOI 10.1109/TCSVT.2015.2477935
   Urvoy M., 2012, 4 INT WORKSH QUAL MU
   Voloshynovskiy S, 2000, PROC SPIE, V3971, P358, DOI 10.1117/12.384990
   Wang S, 2014, MEASUREMENT, V48, P54, DOI 10.1016/j.measurement.2013.10.028
   Zhang L, 2005, IEEE T BROADCAST, V51, P191, DOI 10.1109/TBC.2005.846190
NR 43
TC 55
Z9 58
U1 0
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2016
VL 18
IS 9
BP 1733
EP 1748
DI 10.1109/TMM.2016.2589208
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DT4GI
UT WOS:000381437800005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wu, JQ
   Zhou, YP
   Chiu, DM
   Zhu, ZR
AF Wu, Jiqiang
   Zhou, Yipeng
   Chiu, Dah Ming
   Zhu, Zirong
TI Modeling Dynamics of Online Video Popularity
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dynamic video popularity; information spreading; popularity prediction;
   user reaction
AB Video popularity (measured by view count) over time is an essential reference for both online video providers and users. According to state-of-the-art works, video popularity is useful for system optimization, load generation, video caching, and video recommendation. Thus, deeper understanding of video popularity evolution is very helpful for improving video service quality and providers' operating efficiency. The core question to be explored in this paper is what key factors govern online video popularity evolution? Through collaboration with our industry partner, Tencent Video, we obtain historical data of video view counts over a period of time, and observe their patterns. We then propose a stochastic fluid model, named as EvoModel, which captures two processes giving rise to different evolution patterns of a given video: (a) the information spreading process and (b) the user reaction process. The driving forces for process (a) can be either via recommendation from the system directly, or word-of-mouth; the extent of the spread is governed by the intrinsic popularity of the video. The factor affecting the second process can be modeled by a user reaction rate. These processes together determine different video popularity evolution patterns. We validate our model by fitting the historical data obtained from a real-world system. Furthermore, we discuss the feasibility of estimating model parameters and predicting popularity.
C1 [Wu, Jiqiang] Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518000, Peoples R China.
   [Wu, Jiqiang; Chiu, Dah Ming] Chinese Univ Hong Kong, Informat Engn Dept, Hong Kong, Hong Kong, Peoples R China.
   [Zhou, Yipeng] Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518000, Peoples R China.
   [Zhu, Zirong] Tencent Comp Syst Co Ltd, Shenzhen 518000, Peoples R China.
C3 Shenzhen University; Chinese University of Hong Kong; Shenzhen
   University; Tencent
RP Zhou, YP (corresponding author), Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518000, Peoples R China.
EM jqwuscut@gmail.com; ypzhou@szu.edu.cn; dmchiu@ie.cuhk.edu.hk;
   chriszhu@tencent.com
RI Chiu, Dah Ming/F-1885-2011
OI Zhou, Yipeng/0000-0003-1533-0865
FU Hong Kong RGC GRF [1420814]; Natural Science Foundation of China
   [61402297]; Foundation of Shenzhen City [KQCX20140519103756206]
FX This work was supported in part by the Hong Kong RGC GRF under Grant
   1420814, in part by the Natural Science Foundation of China under Grant
   61402297, and in part by the Foundation of Shenzhen City under Grant
   KQCX20140519103756206. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Shiwen Mao.
   (Corresponding author: Yipeng Zhou.)
CR Ahmed Mohamed, 2013, P 6 ACM INT C WEB SE, P607, DOI [DOI 10.1145/2433396.2433473, 10.1145/2433396.2433473]
   [Anonymous], 2013, P 6 ACM INT C WEB SE, DOI DOI 10.1145/2433396.2433443
   Avramova Z, 2009, 2009 FIRST INTERNATIONAL CONFERENCE ON EVOLVING INTERNET (INTERNET 2009), P95, DOI 10.1109/INTERNET.2009.22
   Borghol Y, 2011, PERFORM EVALUATION, V68, P1037, DOI 10.1016/j.peva.2011.07.008
   Cha M, 2009, IEEE ACM T NETWORK, V17, P1357, DOI 10.1109/TNET.2008.2011358
   Chen F., 2013, QUALITY SERVICE IWQO, P1
   Daley D.J., 2001, Epidemic modeling: An Introduction
   Figueiredo Flavio, 2011, P 4 ACM INT C WEB SE, P745, DOI DOI 10.1145/1935826.1935925
   Jiang B, 2013, IEEE INFOCOM SER, P2373
   Lai KF, 2013, IEEE T MULTIMEDIA, V15, P224, DOI 10.1109/TMM.2012.2225030
   Li Haitao., 2012, P 22 INT WORKSHOP NE, P83
   Li ZG, 2012, PLANT SCI, V185, P185, DOI 10.1016/j.plantsci.2011.10.006
   Niu D, 2012, IEEE INFOCOM SER, P460, DOI 10.1109/INFCOM.2012.6195785
   Pastor-Satorras R, 2001, PHYS REV LETT, V86, P3200, DOI 10.1103/PhysRevLett.86.3200
   Roy SD, 2013, IEEE T MULTIMEDIA, V15, P1255, DOI 10.1109/TMM.2013.2265079
   Tan B, 2013, IEEE ACM T NETWORK, V21, P566, DOI 10.1109/TNET.2012.2208199
   Traverso S, 2015, IEEE T MULTIMEDIA, V17, P1839, DOI 10.1109/TMM.2015.2458043
   Tu XP, 2013, IEEE T SYST MAN CY-S, V43, P379, DOI 10.1109/TSMCA.2012.2189878
   Wang D, 2013, IEEE INFOCOM SER, P2391
   Wang Z, 2013, IEEE T NETW SERV MAN, V10, P84, DOI 10.1109/TNSM.2012.12.120244
   Wu D, 2009, IEEE INFOCOM SER, P73, DOI 10.1109/INFCOM.2009.5061908
   Yang J, 2010, 2010 IEEE INTERNATIONAL CONFERENCE ON WIRELESS COMMUNICATIONS, NETWORKING AND INFORMATION SECURITY (WCNIS), VOL 2, P593, DOI 10.1109/ICCSIT.2010.5564008
   Zhou YP, 2015, IEEE T MULTIMEDIA, V17, P1273, DOI 10.1109/TMM.2015.2447277
   Zhou YP, 2012, IEEE INFOCOM SER, P1530, DOI 10.1109/INFCOM.2012.6195520
NR 24
TC 48
Z9 50
U1 1
U2 47
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2016
VL 18
IS 9
BP 1882
EP 1895
DI 10.1109/TMM.2016.2579600
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DT4GI
UT WOS:000381437800017
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhu, SY
   Zeng, B
   Zeng, LY
   Gabbouj, M
AF Zhu, Shuyuan
   Zeng, Bing
   Zeng, Liaoyuan
   Gabbouj, Moncef
TI Image Interpolation Based on Non-local Geometric Similarities and
   Directional Gradients
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Directional gradient; geometric similarity; image interpolation; minimum
   mean square error (MMSE)
ID ALGORITHM; SUPERRESOLUTION
AB Image interpolation offers an efficient way to compose a high-resolution (HR) image from the observed low-resolution (LR) image. Advanced interpolation techniques design the interpolation weighting coefficients by solving a minimum mean-square-error (MMSE) problem in which the local geometric similarity is often considered. However, using local geometric similarities cannot usually make the MMSE-based interpolation as reliable as expected. To solve this problem, we propose a robust interpolation scheme by using the nonlocal geometric similarities to construct the HR image. In our proposed method, the MMSE-based interpolation weighting coefficients are generated by solving a regularized least squares problem that is built upon a number of dual-reference patches drawn from the given LR image and regularized by the directional gradients of these patches. Experimental results demonstrate that our proposed method offers a remarkable quality improvement as compared to some state-of-the-art methods, both objectively and subjectively.
C1 [Zhu, Shuyuan; Zeng, Bing; Zeng, Liaoyuan] Univ Elect Sci & Technol China, Sch Elect Engn, Chengdu 610051, Peoples R China.
   [Gabbouj, Moncef] Tampere Univ Technol, Dept Signal Proc, Tampere 33720, Finland.
C3 University of Electronic Science & Technology of China; Tampere
   University
RP Zeng, B (corresponding author), Univ Elect Sci & Technol China, Sch Elect Engn, Chengdu 610051, Peoples R China.
EM eezsy@uestc.edu.cn; eezeng@uestc.edu.cn; lyzeng@uestc.edu.cn;
   moncef.gabbouj@tut.fi
RI Gabbouj, Moncef/G-4293-2014
OI Gabbouj, Moncef/0000-0002-9788-2323
FU National Key Basic Research Program of China [2015CB351804]; National
   Natural Science Foundation of China [61300091, 61370148]; Postdoctoral
   Science Foundation of China [2015M572464]; Science and Technology
   Support Program of Sichuan Province [2016GZ0124]
FX This work was supported by the National Key Basic Research Program of
   China under Grant 2015CB351804, by the National Natural Science
   Foundation of China under Grant 61300091 and Grant 61370148, by the
   Postdoctoral Science Foundation of China under Grant 2015M572464, and by
   the Science and Technology Support Program of Sichuan Province under
   Grant 2016GZ0124. This paper was presented in part at the IEEE
   International Conference on Multimedia and Expo, Torino, Italy,
   June-July 2015. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Adrian Munteanu.
   (Corresponding author: Bing Zeng.)
CR Allebach J, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL III, P707, DOI 10.1109/ICIP.1996.560768
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Chen HH, 2013, PICT COD SYMP, P289, DOI 10.1109/PCS.2013.6737740
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1382, DOI 10.1109/TIP.2012.2231086
   Gao XB, 2012, IEEE T IMAGE PROCESS, V21, P469, DOI 10.1109/TIP.2011.2161482
   Glasner D, 2009, IEEE I CONF COMP VIS, P349, DOI 10.1109/ICCV.2009.5459271
   Huang JJ, 2015, IEEE T IMAGE PROCESS, V24, P3232, DOI 10.1109/TIP.2015.2440751
   Hung KW, 2012, IEEE T IMAGE PROCESS, V21, P1061, DOI 10.1109/TIP.2011.2168416
   Hung KW, 2010, IEEE IMAGE PROC, P3297, DOI 10.1109/ICIP.2010.5652082
   JENSEN K, 1995, IEEE T IMAGE PROCESS, V4, P285, DOI 10.1109/83.366477
   KEYS RG, 1981, IEEE T ACOUST SPEECH, V29, P1153, DOI 10.1109/TASSP.1981.1163711
   Lee S. W., 1993, ICASSP-93. 1993 IEEE International Conference on Acoustics, Speech, and Signal Processing (Cat. No.92CH3252-4), P177, DOI 10.1109/ICASSP.1993.319776
   Li M, 2008, IEEE T IMAGE PROCESS, V17, P1121, DOI 10.1109/TIP.2008.924289
   Li X, 2001, IEEE T IMAGE PROCESS, V10, P1521, DOI 10.1109/83.951537
   Liu XM, 2014, IEEE T IMAGE PROCESS, V23, DOI 10.1109/TIP.2013.2294543
   Liu XM, 2011, IEEE T IMAGE PROCESS, V20, P3455, DOI 10.1109/TIP.2011.2150234
   Mishiba K, 2010, IEEE IMAGE PROC, P2837, DOI 10.1109/ICIP.2010.5652113
   Ni KS, 2009, IEEE T IMAGE PROCESS, V18, P1976, DOI 10.1109/TIP.2009.2023706
   Protter M, 2009, IEEE T IMAGE PROCESS, V18, P36, DOI 10.1109/TIP.2008.2008067
   Tikhonov A.N., 1963, SOV MATH DOKL, V5, P1035, DOI DOI 10.1111/J.1365-246X.2012.05699.X
   Wang Q, 2007, IEEE T IMAGE PROCESS, V16, P889, DOI 10.1109/TIP.2007.891794
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xiong ZW, 2013, IEEE T MULTIMEDIA, V15, P1458, DOI 10.1109/TMM.2013.2264654
   Yin Z, 2014, 3DTV C TRUE VIS CAPT, P1
   Zhang KB, 2015, IEEE T IMAGE PROCESS, V24, P846, DOI 10.1109/TIP.2015.2389629
   Zhang KB, 2012, IEEE T IMAGE PROCESS, V21, P4544, DOI 10.1109/TIP.2012.2208977
   Zhang L, 2006, IEEE T IMAGE PROCESS, V15, P2226, DOI 10.1109/TIP.2006.877407
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang XJ, 2008, IEEE T IMAGE PROCESS, V17, P887, DOI 10.1109/TIP.2008.924279
   Zhang XF, 2009, LECT NOTES COMPUT SC, V5879, P1197, DOI 10.1007/978-3-642-10467-1_121
NR 31
TC 30
Z9 31
U1 0
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2016
VL 18
IS 9
BP 1707
EP 1719
DI 10.1109/TMM.2016.2593039
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DT4GI
UT WOS:000381437800003
DA 2024-07-18
ER

PT J
AU Tian, XM
   Dong, Z
   Yang, KY
   Mei, T
AF Tian, Xinmei
   Dong, Zhe
   Yang, Kuiyuan
   Mei, Tao
TI Query-Dependent Aesthetic Model With Deep Learning for Photo Quality
   Assessment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep aesthetic visual abstraction; deep learning; quality assessment
AB The automatic assessment of photo quality from an aesthetic perspective is a very challenging problem. Most existing research has predominantly focused on the learning of a universal aesthetic model based on hand-crafted visual descriptors. However, this research paradigm can achieve only limited success because 1) such hand-crafted descriptors cannot well preserve abstract aesthetic properties, and 2) such a universal model cannot always capture the full diversity of visual content. To address these challenges, we propose in this paper a novel query-dependent aesthetic model with deep learning for photo quality assessment. In our method, deep aesthetic abstractions are discovered from massive images, whereas the aesthetic assessment model is learned in a query-dependent manner. Our work addresses the first problem by learning mid-level aesthetic feature abstractions via powerful deep convolutional neural networks to automatically capture the underlying aesthetic characteristics of the massive training images. Regarding the second problem, because photographers tend to employ different rules of photography for capturing different images, the aesthetic model should also be query-dependent. Specifically, given an image to be assessed, we first identify which aesthetic model should be applied for this particular image. Then, we build a unique aesthetic model of this type to assess its aesthetic quality. We conducted extensive experiments on two large-scale datasets and demonstrated that the proposed query-dependent model equipped with learned deep aesthetic abstractions significantly and consistently outperforms state-of-the-art hand-crafted feature-based and universal model-based methods.
C1 [Tian, Xinmei; Dong, Zhe] Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230027, Peoples R China.
   [Yang, Kuiyuan; Mei, Tao] Microsoft Res, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Microsoft
RP Tian, XM (corresponding author), Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230027, Peoples R China.
EM xinmei@ustc.edu.cn; ustcdz@mail.ustc.edu.cn; kuyang@microsoft.com;
   tmei@microsoft.com
RI Mei, Tao/GQZ-0596-2022
OI Mei, Tao/0000-0002-5990-7307
FU 973 Project [2015CB351803]; NSFC [61390514, 61201413]; Youth Innovation
   Promotion Association CAS [CX2100060016]; Fundamental Research Funds for
   the Central Universities [WK2100060011, WK2100100021, 61301082];
   Specialized Research Fund for the Doctoral Program of Higher Education
   [WJ2100060003]
FX This work was supported by the 973 Project under Contract 2015CB351803,
   by the NSFC under Contract 61390514 and Contract 61201413, by the Youth
   Innovation Promotion Association CASunder Grant CX2100060016, by the
   Fundamental Research Funds for the Central Universities under Grant
   WK2100060011, Grant WK2100100021, and Grant 61301082, and by the
   Specialized Research Fund for the Doctoral Program of Higher Education
   under Grant WJ2100060003. The guest editor coordinating the review of
   this manuscript and approving it for publication was Dr. Guo-Jun Qi.
CR [Anonymous], 2012, IEEE VIS COMM IM PRO
   [Anonymous], 2011, ACM International Conference on Multimedia MM
   [Anonymous], 2013, P 21 ACM INT C MULTI
   [Anonymous], 2010, P 18 ACM INT C MULTI
   [Anonymous], 2010, ACM MULTIMEDIA 2010
   Bhattacharya S, 2011, ACM T MULTIM COMPUT, V7, DOI 10.1145/2037676.2037678
   Cohen-Or D, 2006, ACM T GRAPHIC, V25, P624, DOI 10.1145/1141911.1141933
   Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23
   Dhar S, 2011, PROC CVPR IEEE, P1657, DOI 10.1109/CVPR.2011.5995467
   Dong Z, 2015, NEUROCOMPUTING, V168, P308, DOI 10.1016/j.neucom.2015.05.095
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Geng B., 2011, Proceedings of the 19th ACM international conference on multimedia, P63
   Jin Y, 2012, COMPUT GRAPH-UK, V36, P955, DOI 10.1016/j.cag.2012.07.007
   Ke Y., 2006, P IEEE COMP SOC C CO, V1, P419, DOI DOI 10.1109/CVPR.2006.303
   Krizhevsky A., 2012, NIPS, V1, P4
   Li CC, 2010, IEEE IMAGE PROC, P3221, DOI 10.1109/ICIP.2010.5651833
   Lo KY, 2012, INT C PATT RECOG, P2186
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu X, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P457, DOI 10.1145/2647868.2654927
   Luo W, 2011, IEEE I CONF COMP VIS, P2206, DOI 10.1109/ICCV.2011.6126498
   Luo YW, 2008, LECT NOTES COMPUT SC, V5304, P386
   Marchesotti L, 2011, IEEE I CONF COMP VIS, P1784, DOI 10.1109/ICCV.2011.6126444
   Moorthy AK, 2010, LECT NOTES COMPUT SC, V6315, P1, DOI 10.1007/978-3-642-15555-0_1
   Murray N, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.110
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Nishiyama M, 2011, PROC CVPR IEEE, P33, DOI 10.1109/CVPR.2011.5995539
   Nister David, 2006, CVPR
   Obrador P., 2009, PROC 1 ACM SIGMM WOR, P65
   Qi GJ, 2012, P IEEE, V100, P2688, DOI 10.1109/JPROC.2012.2201909
   Qi GJ, 2012, IEEE T PATTERN ANAL, V34, P850, DOI 10.1109/TPAMI.2011.191
   Su HH, 2012, IEEE T MULTIMEDIA, V14, P833, DOI 10.1109/TMM.2012.2186123
   Tang XO, 2013, IEEE T MULTIMEDIA, V15, P1930, DOI 10.1109/TMM.2013.2269899
   Vapnik VN, 2000, NATURE STAT LEARNING, DOI DOI 10.1007/978-1-4757-3264-1
   Wong LK, 2009, IEEE IMAGE PROC, P997, DOI 10.1109/ICIP.2009.5413825
   Wu O, 2011, IEEE I CONF COMP VIS, P225, DOI 10.1109/ICCV.2011.6126246
   Yao L, 2012, INT J COMPUT VISION, V96, P353, DOI 10.1007/s11263-011-0478-3
   Yaowen Wu, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P1586, DOI 10.1109/ICPR.2010.392
   Zhang FL, 2013, IEEE T MULTIMEDIA, V15, P1480, DOI 10.1109/TMM.2013.2268051
NR 38
TC 74
Z9 79
U1 1
U2 51
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2015
VL 17
IS 11
SI SI
BP 2035
EP 2048
DI 10.1109/TMM.2015.2479916
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CV2QV
UT WOS:000364102400016
DA 2024-07-18
ER

PT J
AU Fleites, FC
   Wang, HH
   Chen, SC
AF Fleites, Fausto C.
   Wang, Haohong
   Chen, Shu-Ching
TI Enabling Enriched TV Shopping Experience via Computational and Temporal
   Aware View-Centric Multimedia Abstraction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dynamic programming; multimedia content abstraction; region of interest
   (ROI); smart TV; TV shopping; video motion
ID GLOBAL MOTION ESTIMATION; OBJECT DETECTION; ATTENTION MODEL; VIDEO
AB Smart TVs have realized the convergence of TV, Internet, and PC technologies, but still do not provide a seamless content interaction for TV-enabled shopping. To purchase interesting items displayed in a TV show, consumers must resort to a store or the Web, which is an inconvenient way of purchasing products. The fundamental challenge in realizing such a use case consists of understanding the multimedia content being streamed. Such a challenge can be realized by utilizing object detection to facilitate content understanding though it has to be executed as a computationally bound process so that consumers are provided with a responsive and exciting user interface. To this end, we propose a computational-and temporal-aware multimedia abstraction framework that facilitates the efficient execution of object detection tasks. Given computational and temporal rate constraints, the proposed framework selects the optimal video frames that best represent the video content and allows the execution of the object detection task as a computationally bound process. In this sense, the framework is computationally scalable as it can adapt to the given constraints and generate optimal abstraction results accordingly. Additionally, the framework utilizes "object views" as the basis for the frame selection process, which depict salient information and are represented as regions of interest (ROI). In general, an ROI can be a whole frame or a region that discards background information. Experimental results demonstrate the computational scalability of the proposed framework and the benefits of using the regions of interest as the basis of the abstraction process.
C1 [Fleites, Fausto C.] Senzari Inc, Miami, FL 33131 USA.
   [Chen, Shu-Ching] Florida Int Univ, Sch Comp & Informat Sci, Miami, FL 33199 USA.
   [Wang, Haohong] TCL Res Amer, San Jose, CA 95134 USA.
C3 State University System of Florida; Florida International University
RP Fleites, FC (corresponding author), Senzari Inc, Miami, FL 33131 USA.
EM fflei001@cs.fiu.edu; haohong.wang@tcl.com; chens@.cs.fiu.edu
CR [Anonymous], APPL COMPUTER VISION
   Ayvaci A, 2012, IEEE T PATTERN ANAL, V34, P1942, DOI 10.1109/TPAMI.2011.271
   Boykov Y, 2006, HANDBOOK OF MATHEMATICAL MODELS IN COMPUTER VISION, P79, DOI 10.1007/0-387-28831-7_5
   Butko Nicholas J., 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2751, DOI 10.1109/CVPRW.2009.5206540
   Chatzichristofis SA, 2008, LECT NOTES COMPUT SC, V5008, P312
   Chen YM, 2011, IEEE T CIRC SYST VID, V21, P1316, DOI 10.1109/TCSVT.2011.2148490
   Cheng WH, 2005, IEICE T INF SYST, VE88D, P1578, DOI 10.1093/ietisy/e88-d.7.1578
   Chi MC, 2009, IEEE T CIRC SYST VID, V19, P1025, DOI 10.1109/TCSVT.2009.2022822
   Chiang JC, 2010, IEEE INT CON MULTI, P238, DOI 10.1109/ICME.2010.5583354
   Chung YC, 2009, IEEE SIGNAL PROC LET, V16, P977, DOI 10.1109/LSP.2009.2028101
   Cormen T. H., 2009, Introduction to Algorithms, VSecond
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dianting Liu, 2011, Proceedings of the 2011 IEEE International Symposium on Multimedia (ISM 2011), P271, DOI 10.1109/ISM.2011.50
   Felzenszwalb PF, 2011, IEEE T PATTERN ANAL, V33, P721, DOI 10.1109/TPAMI.2010.135
   GREIG DM, 1989, J ROY STAT SOC B MET, V51, P271, DOI 10.1111/j.2517-6161.1989.tb01764.x
   Guo CL, 2010, IEEE T IMAGE PROCESS, V19, P185, DOI 10.1109/TIP.2009.2030969
   Gurwicz Y, 2011, PATTERN RECOGN LETT, V32, P805, DOI 10.1016/j.patrec.2011.01.005
   Haller M, 2009, 2009 10TH INTERNATIONAL WORKSHOP ON IMAGE ANALYSIS FOR MULTIMEDIA INTERACTIVE SERVICES, P49, DOI 10.1109/WIAMIS.2009.5031429
   Huang YW, 2006, J VLSI SIG PROC SYST, V42, P297, DOI 10.1007/s11265-006-4190-4
   Kim J, 2010, IEEE IMAGE PROC, P4669, DOI 10.1109/ICIP.2010.5652848
   Kolmogorov V, 2002, LECT NOTES COMPUT SC, V2352, P82
   Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177
   Leibe B, 2008, INT J COMPUT VISION, V77, P259, DOI 10.1007/s11263-007-0095-3
   Liu D, 2010, IEEE T PATTERN ANAL, V32, P2178, DOI 10.1109/TPAMI.2010.31
   Liu Y, 2008, IEEE T CIRC SYST VID, V18, P134, DOI 10.1109/TCSVT.2007.913754
   Liyin X., 2010, P INT C COMP APPL SY, V2, pV2
   Qi B, 2008, IEEE T IMAGE PROCESS, V17, P958, DOI 10.1109/TIP.2008.921985
   Rapantzikos K, 2007, IET IMAGE PROCESS, V1, P237, DOI 10.1049/iet-ipr:20060040
   Su YB, 2005, IEEE T CIRC SYST VID, V15, P232, DOI 10.1109/TCSVT.2004.841656
   Sun XD, 2005, IEEE T MULTIMEDIA, V7, P981, DOI 10.1109/TMM.2005.854388
   Truong BT, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1198302.1198305
   van Essen G, 2009, 2009 24TH INTERNATIONAL CONFERENCE IMAGE AND VISION COMPUTING NEW ZEALAND (IVCNZ 2009), P425, DOI 10.1109/IVCNZ.2009.5378369
   Vazquez-Reina A, 2010, LECT NOTES COMPUT SC, V6315, P268, DOI 10.1007/978-3-642-15555-0_20
   Vijayanarasimhan S, 2010, PROC CVPR IEEE, P1006, DOI 10.1109/CVPR.2010.5540109
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wang YF, 2012, 2012 11TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA 2012), VOL 2, P446, DOI 10.1109/ICMLA.2012.206
   Xu YW, 2011, IEEE T SYST MAN CY B, V41, P107, DOI 10.1109/TSMCB.2010.2046890
   Yuwen He, 2001, ISCAS 2001. The 2001 IEEE International Symposium on Circuits and Systems (Cat. No.01CH37196), P233, DOI 10.1109/ISCAS.2001.921050
   Zhang J, 2007, INT J COMPUT VISION, V73, P213, DOI 10.1007/s11263-006-9794-4
   Zhang Jingjun, 2007, Proceedings. IEEE SoutheastCon 2007 (IEEE Cat. No.07CH37882)
   Zhang Y, 2009, PROCEEDINGS OF 2009 INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND CYBERNETICS, VOLS 1-6, P2221, DOI 10.1109/ICMLC.2009.5212222
   Zhang Y, 2009, PROCEEDINGS OF THE 2009 2ND INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, VOLS 1-9, P282
NR 42
TC 5
Z9 5
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2015
VL 17
IS 7
BP 1068
EP 1080
DI 10.1109/TMM.2015.2433213
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CK8XC
UT WOS:000356522300013
DA 2024-07-18
ER

PT J
AU Zou, FH
   Chen, YP
   Song, JK
   Zhou, K
   Yang, Y
   Sebe, N
AF Zou, Fuhao
   Chen, Yunpeng
   Song, Jingkuan
   Zhou, Ke
   Yang, Yang
   Sebe, Nicu
TI Compact Image Fingerprint Via Multiple Kernel Hashing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature fusion; fingerprinting; hashing; multiple kernel learning;
   near-duplicate detection
ID VIDEO COPY DETECTION; ROBUST; QUANTIZATION
AB Image fingerprinting is regarded as an alternative approach to watermarking in terms of near-duplicate detection application. It consists of feature extraction and feature indexing. Generally, the former is mainly related to discrimination, robustness, and security while the latter closely focuses on the efficiency of fingerprints search. To enable fast fingerprints searching over a very large database, we propose a new kernelized multiple feature hashing method to convert the real-value fingerprints into compact binary-value fingerprints. During the process of converting, the proposed hashing method jointly utilizes the kernel trick and multiple feature fusion strategy to map the image represented by multiple features into a compact binary code. With the help of the kernel function, the hashing method can be applied to any format (such as string, graph, set, and so on) as long as there is an associated kernel function available for similarity measurement. In addition, taking multiple features into account aims at improving the discriminability since these multiple evidences are complementary to each other. The extensive experimental results show that the proposed algorithm outperforms state-of-the-art kernelized hashing methods by up to 10 percent.
C1 [Zou, Fuhao; Chen, Yunpeng] Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan 430074, Peoples R China.
   [Song, Jingkuan; Sebe, Nicu] Univ Trento, Dept Informat Engn & Comp Sci, I-38100 Trento, Italy.
   [Zhou, Ke] Huazhong Univ Sci & Technol, Wuhan Natl Lab Optoelect, Wuhan 430074, Peoples R China.
   [Yang, Yang] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
C3 Huazhong University of Science & Technology; University of Trento;
   Huazhong University of Science & Technology; University of Electronic
   Science & Technology of China
RP Chen, YP (corresponding author), Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan 430074, Peoples R China.
EM yunpengchen@hust.edu.cn
RI yang, yang/GVT-5210-2022; Chen, Yunpeng/HCI-2977-2022; Lang,
   Ming/HIK-0758-2022; yang, yang/HGT-7999-2022; Sebe,
   Niculae/KEC-2000-2024
OI Chen, Yunpeng/0000-0002-9830-8980; Sebe, Niculae/0000-0002-6597-7248
FU National Basic Research Program (973 Program) of China [2011CB302305];
   National Natural Science Foundation of China [61232004]; EU under
   Project xLiMe
FX The work of F. Zou and K. Zhou was supported in part by the National
   Basic Research Program (973 Program) of China under Grant 2011CB302305
   and by the National Natural Science Foundation of China under Grant
   61232004. The work of J. Song and N. Sebe was supported in part by the
   EU under Project xLiMe. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Alessandro
   Piva. (Corresponding author: Yunpeng Chen.)
CR [Anonymous], 2008, Proceedings of the 21st International Conference on Neural Information Processing Systems
   BENTLEY JL, 1990, PROCEEDINGS OF THE SIXTH ANNUAL SYMPOSIUM ON COMPUTATIONAL GEOMETRY, P187, DOI 10.1145/98524.98564
   Bhat DN, 1998, IEEE T PATTERN ANAL, V20, P415, DOI 10.1109/34.677275
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Chang EY, 1998, P SOC PHOTO-OPT INS, V3527, P58, DOI 10.1117/12.325852
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   diaeresis>tze Hinrich Schu<spacing, 2008, INTRO INFORM RETRIEV, V39
   Esmaeili MM, 2011, IEEE T INF FOREN SEC, V6, P213, DOI 10.1109/TIFS.2010.2097593
   Gao L., P IEEE INT C C UNPUB
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Gu XG, 2013, INT CONF ACOUST SPEE, P1508, DOI 10.1109/ICASSP.2013.6637903
   Guttman A., P ICMD, V1, P47
   He J., 2010, ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P1129
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Joly Alexis., 2008, PROCEEDING 16 ACM IN, P209
   Kim C, 2003, SIGNAL PROCESS-IMAGE, V18, P169, DOI 10.1016/S0923-5965(02)00130-3
   Kulis B, 2009, IEEE I CONF COMP VIS, P2130, DOI 10.1109/ICCV.2009.5459466
   Lee S, 2008, IEEE T CIRC SYST VID, V18, P983, DOI 10.1109/TCSVT.2008.920739
   Ling HF, 2012, IEEE MULTIMEDIA, V19, P60, DOI 10.1109/MMUL.2011.75
   Liu Wei, 2011, Reports in Parasitology, V1, P1
   Liu X., 2012, PROC ACM MULTIMEDIA, P881
   Lv XD, 2012, IEEE T INF FOREN SEC, V7, P1081, DOI 10.1109/TIFS.2012.2190594
   Maani E, 2008, IEEE IMAGE PROC, P1716, DOI 10.1109/ICIP.2008.4712105
   Mei-Lei Lv, 2011, Information Technology Journal, V10, P120, DOI 10.3923/itj.2011.120.126
   Mu YD, 2010, AAAI CONF ARTIF INTE, P539
   Norouzi M, 2013, PROC CVPR IEEE, P3017, DOI 10.1109/CVPR.2013.388
   Petitcolas FAP, 1998, LECT NOTES COMPUT SC, V1525, P218
   Salakhutdinov R, 2009, INT J APPROX REASON, V50, P969, DOI 10.1016/j.ijar.2008.11.006
   Seo JS, 2004, SIGNAL PROCESS-IMAGE, V19, P325, DOI 10.1016/j.image.2003.12.001
   Shakhnarovich G, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P750
   Shao GQ, 2014, IEEE T IMAGE PROCESS, V23, P489, DOI 10.1109/TIP.2013.2287996
   Song J., 2013, P ACM SIGMOD INT C M, P785, DOI DOI 10.1145/2463676.2465274
   Song J., 2011, P 19 ACM INT C MULT, P423, DOI DOI 10.1145/2072298.2072354
   Song JK, 2013, IEEE T MULTIMEDIA, V15, P1997, DOI 10.1109/TMM.2013.2271746
   Tang ZJ, 2014, IEEE T KNOWL DATA EN, V26, P711, DOI 10.1109/TKDE.2013.45
   Tang ZJ, 2012, APPL MATH INFORM SCI, V6, p643S
   Wang J, 2010, PROC CVPR IEEE, P3424, DOI 10.1109/CVPR.2010.5539994
   Xia H, 2012, SIGIR 2012: PROCEEDINGS OF THE 35TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P55, DOI 10.1145/2348283.2348294
   Xu H, 2011, IEEE I CONF COMP VIS, P1631, DOI 10.1109/ICCV.2011.6126424
   Yang GB, 2012, COMPUT SECUR, V31, P33, DOI 10.1016/j.cose.2011.11.004
   Yang Y, 2013, IEEE T MULTIMEDIA, V15, P572, DOI 10.1109/TMM.2012.2234731
   Ye GN, 2013, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2013.282
   Zhang D, 2010, SIGIR 2010: PROCEEDINGS OF THE 33RD ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH DEVELOPMENT IN INFORMATION RETRIEVAL, P18
   Zhang ST, 2012, LECT NOTES COMPUT SC, V7573, P660, DOI 10.1007/978-3-642-33709-3_47
   Zhang W., 2010, Proceedings of the International Conference on Multimedia", MM'10, P1131, DOI DOI 10.1145/1873951.1874168
   Zhenjun Tang, 2013, ICIC Express Letters, V7, P2961
NR 46
TC 25
Z9 27
U1 0
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2015
VL 17
IS 7
BP 1006
EP 1018
DI 10.1109/TMM.2015.2425651
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CK8XC
UT WOS:000356522300008
DA 2024-07-18
ER

PT J
AU Cho, S
   Kim, H
   Kim, HY
   Kim, M
AF Cho, Seunghyun
   Kim, HyunMi
   Kim, Hui Yong
   Kim, Munchurl
TI Efficient In-Loop Filtering Across Tile Boundaries for Multi-Core HEVC
   Hardware Decoders With 4 K/8 K-UHD Video Applications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Hardware decoders; HEVC; in-loop filtering; parallel processing; tile
ID PARALLEL; H.264; CHIP
AB HEVC is a next generation video coding standard designed with modern coding techniques to be especially efficient for coding high-resolution video such as 4 K/8 K-ultra high-definition (UHD) video. Among the advanced coding tools of HEVC, tiles and wavefront parallel processing (WPP) have been newly adopted for parallel processing of such high-resolution (4 K/8 K-UHD) video. To realize UHD video services over portable devices with limited battery power, it is essential to implement multi-core-based and dedicated HEVC hardware decoders that support the tile-and wavefront-based parallel processing. By doing so, each frame is divided into a multiple number of picture partitions which can then be processed by multiple hardware decoder cores in parallel. However, in-loop filtering (ILF) at tile boundaries cannot be easily parallelized by a multi-core HEVC hardware decoder because of the data dependency between samples in different tiles. In this paper, an efficient control method for ILF across tile boundaries is proposed for multi-core HEVC hardware decoders. The proposed method does not require additional in-loop filters for ILF across the tile boundaries and it allows a decoder core to continue to process the next coding tree unit (CTU) without waiting for other decoders until they finish their ILF processing for the neighboring CTUs in other tiles. From experiments, we show the effectiveness of our ILF control method via a quad-core HEVC decoder for 4 K-UHD video implemented on a prototyping FPGA board.
C1 [Cho, Seunghyun; Kim, Munchurl] Korea Adv Inst Sci & Technol, Dept Elect Engn, Taejon 305731, South Korea.
   [Kim, HyunMi; Kim, Hui Yong] ETRI, SoC Res Dept, Taejon 305700, South Korea.
   [Kim, HyunMi; Kim, Hui Yong] ETRI, Realist Broadcasting Media Res Dept, Taejon 305700, South Korea.
C3 Korea Advanced Institute of Science & Technology (KAIST); Electronics &
   Telecommunications Research Institute - Korea (ETRI); Electronics &
   Telecommunications Research Institute - Korea (ETRI)
RP Cho, S (corresponding author), Korea Adv Inst Sci & Technol, Dept Elect Engn, Taejon 305731, South Korea.
EM shcho13@kaist.ac.kr; chaos0218@etri.re.kr; hykim5@etri.re.kr;
   mkim@ee.kaist.ac.kr
RI Kim, Munchurl/AAQ-9591-2020; Kim, Hui Yong/AAK-9197-2020; Kim,
   Munchurl/C-1759-2011
OI Cho, Seunghyun/0000-0003-1985-4420; Kim, Hui Yong/0000-0001-7308-133X
FU MOTIE/KEIT [10039214]
FX This work was supported by the IT R&D Program of MOTIE/KEIT under Grant
   10039214, Video Codec SoC for Ultra High Definition. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Leonel Sousa.
CR [Anonymous], HM REF SOFTW 13 0
   [Anonymous], P SPIE APPL DIG IM P
   [Anonymous], 2012, JCTVC10198 ITUTISOIE
   [Anonymous], 2012, JCTVCH0116 ITUTISOIE
   [Anonymous], 2013, JCTVCL1003 ITUTISOIE
   [Anonymous], 2012, JCTVCH1004 ITUTISOIE
   Bossen F, 2012, IEEE T CIRC SYST VID, V22, P1685, DOI 10.1109/TCSVT.2012.2221255
   Chi CC, 2012, IEEE T CIRC SYST VID, V22, P1827, DOI 10.1109/TCSVT.2012.2223056
   Finchelstein DF, 2009, IEEE T CIRC SYST VID, V19, P1704, DOI 10.1109/TCSVT.2009.2031459
   Fu CM, 2012, IEEE T CIRC SYST VID, V22, P1755, DOI 10.1109/TCSVT.2012.2221529
   Horowitz M., 2012, P SPIE APPL DIGITAL, V8499
   Hwangbo W, 2010, IEEE T MULTIMEDIA, V12, P157, DOI 10.1109/TMM.2010.2041099
   Misra K, 2013, IEEE J-STSP, V7, P969, DOI 10.1109/JSTSP.2013.2271451
   Mody Mihir, 2013, 2013 IEEE Third International Conference on Consumer Electronics - Berlin (ICCE-Berlin). Proceedings, P54, DOI 10.1109/ICCE-Berlin.2013.6698026
   Muchen Li, 2012, Advances in Multimedia Information Processing - PCM 2012. 13th Pacific-Rim Conference on Multimedia. Proceedings, P273, DOI 10.1007/978-3-642-34778-8_25
   Norkin A, 2012, IEEE T CIRC SYST VID, V22, P1746, DOI 10.1109/TCSVT.2012.2223053
   Ohm JR, 2012, IEEE T CIRC SYST VID, V22, P1669, DOI 10.1109/TCSVT.2012.2221192
   Rhee CE, 2014, IEEE T MULTIMEDIA, V16, P947, DOI 10.1109/TMM.2014.2306396
   Schwalb M, 2009, IEEE T MULTIMEDIA, V11, P1, DOI 10.1109/TMM.2008.2008873
   Shen S, 2013, IEICE ELECTRON EXPR, V10, DOI 10.1587/elex.10.20130272
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tikekar M, 2014, IEEE J SOLID-ST CIRC, V49, P61, DOI 10.1109/JSSC.2013.2284362
   Zhang YD, 2012, IEEE T MULTIMEDIA, V14, P510, DOI 10.1109/TMM.2012.2190391
   Zhu JY, 2013, IEICE T FUND ELECTR, VE96A, P2612, DOI 10.1587/transfun.E96.A.2612
NR 24
TC 13
Z9 13
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2015
VL 17
IS 6
BP 778
EP 791
DI 10.1109/TMM.2015.2418995
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CI1TM
UT WOS:000354527500002
DA 2024-07-18
ER

PT J
AU Li, ZY
   Wu, QH
   Salamatian, K
   Xie, GG
AF Li, Zhenyu
   Wu, Qinghua
   Salamatian, Kave
   Xie, Gaogang
TI Video Delivery Performance of a Large-Scale VoD System and the
   Implications on Content Delivery
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Content delivery; delivery performance; Internet video; measurement
ID CHALLENGES
AB Video delivery performance is the main factor that affects Internet video quality. Characterizing the video delivery performance, especially the delivery throughput, can help content providers as well as Internet service providers (ISPs) in system optimization and network planning. Based on a unique dataset consisting of 20 million video download speed measurements, this paper comprehensively studies the video delivery throughput of a large-scale commercial video-on-demand (VoD) system. We observe that user speed exhibits a large variation over time of day as well as across provincial locations. In particular, the worst performance of day is 30% lower than the peak performance. The analysis also reveals that video download speed has a notable impact on Internet video quality, which in turn influences user engagement. The impact, however, becomes limited when the speed increases beyond a certain threshold, which is mostly dependent on the video encoded bitrates. We further examine the interaction between Internet infrastructure and video delivery throughput using the linear regression model and find that crossing the ISP or regional network border yields 15-20% speed loss. Based on these observations, we finally evaluate the potential of edge caching and hybrid CDN-P2P in the improvement of video download performance and video quality.
C1 [Li, Zhenyu; Wu, Qinghua; Xie, Gaogang] Chinese Acad Sci, Inst Comp Technol, Beijing 100190, Peoples R China.
   [Salamatian, Kave] Univ Savoy, F-73000 Annecy Le Vieux, France.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Universite Savoie Mont Blanc
RP Li, ZY (corresponding author), Chinese Acad Sci, Inst Comp Technol, Beijing 100190, Peoples R China.
EM zyli@ict.ac.cn; wuqinghua@ict.ac.cn; kave.salamatian@univ-savoie.fr;
   xie@ict.ac.cn
RI Salamatian, Kavé KS/I-4670-2012; Li, ZhenYu/HSH-9452-2023; li,
   zy/HZM-1892-2023
OI Salamatian, Kave/0000-0001-5557-9134
FU National Basic Research Program of China [2012CB315801]; National
   Natural Science Foundation of China [61133015, 61272473]; National
   High-tech R&D Program of China [2013AA013501]
FX This work was supported by the National Basic Research Program of China
   under Grant 2012CB315801, the National Natural Science Foundation of
   China under Grant 61133015 and Grant 61272473, and the National
   High-tech R&D Program of China under Grant 2013AA013501. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Ali Begen.
CR Ager B, 2012, ACM SIGCOMM COMP COM, V42, P163, DOI 10.1145/2377677.2377714
   [Anonymous], P IEEE INFOCOM
   [Anonymous], P PAM
   [Anonymous], 2010, Proceedings of the 10th ACM SIGCOMM Conference on Internet Measurement. IMC'10, DOI [10.1145/1879141.1879173, DOI 10.1145/1879141.1879173]
   [Anonymous], P 2 ACM SIGCOMM WORK
   [Anonymous], VIEW EXP REP
   [Anonymous], 2011, P SIGCOMM W MUST WOR
   [Anonymous], STAT INT REP
   [Anonymous], STAT REP INT DEV CHI
   [Anonymous], 2010, TPRC
   [Anonymous], QIH BROADB SPEED
   [Anonymous], 2014, Cisco Visual Networking Index: Forecast and Methodology, 2013 - 2018
   [Anonymous], IMPR ONL VID QUAL AC
   Balachandran A., 2013, Proceedings of the 2013 conference on Internet measurement conference, P43
   Balachandran A, 2013, ACM SIGCOMM COMP COM, V43, P339, DOI 10.1145/2534169.2486025
   Canadi Igor., 2012, Proceedings of the 2012 ACM Conference on Internet Measurement Conference, P273
   Chen YS, 2013, IEEE T MULTIMEDIA, V15, P2087, DOI 10.1109/TMM.2013.2280123
   Choi JP, 2010, RAND J ECON, V41, P446
   Dobrian F, 2011, ACM SIGCOMM COMP COM, V41, P362, DOI 10.1145/2043164.2018478
   Erman Jeffrey., 2011, ACM IMC
   Fayazbakhsh SK, 2013, ACM SIGCOMM COMP COM, V43, P147, DOI 10.1145/2534169.2486023
   Hei XJ, 2007, IEEE T MULTIMEDIA, V9, P1672, DOI 10.1109/TMM.2007.907451
   Hongliang Yu, 2006, Operating Systems Review, V40, P333, DOI 10.1145/1218063.1217968
   Huang T.Y., 2012, P 2012 ACM C INT MEA, P225, DOI 10.1145/2398776.2398800
   Huang Y, 2011, P 19 ACM INT C MULT, P213
   Huang Y, 2008, ACM SIGCOMM COMP COM, V38, P375, DOI 10.1145/1402946.1403001
   Jiang JC, 2014, IEEE ACM T NETWORK, V22, P326, DOI 10.1109/TNET.2013.2291681
   Kendall M., 1999, Kendall's advanced theory of statistics: Vol. 2A: Classical inference and the linear model, V2A
   Krishnan R. K., 2012, P INT MEAS C, P211
   Li ZG, 2012, PLANT SCI, V185, P185, DOI 10.1016/j.plantsci.2011.10.006
   Lin JL, 2013, IEEE COMMUN MAG, V51, P120, DOI 10.1109/MCOM.2013.6658663
   Liu X, 2012, ACM SIGCOMM COMP COM, V42, P359, DOI 10.1145/2377677.2377752
   Mitchell T. M., 1997, MACHINE LEARNING
   Pantos Roger., 2014, HTTP live streaming
   Rao A., 2011, Em: Proceedings of the Seventh COnference on emerging Networking EXperiments and Technologies, P1, DOI DOI 10.1145/2079296.2079321
   Shafiq M. Zubair, 2014, ACM SIGMETRICS Performance Evaluation Review, V42, P367, DOI 10.1145/2591971.2591975
   Sun Y, 2013, IEEE NETWORK, V27, P22, DOI 10.1109/MNET.2013.6485092
   Sundaresan S, 2011, ACM SIGCOMM COMP COM, V41, P134, DOI 10.1145/2043164.2018452
   Tian Y, 2013, IEEE T PARALL DISTR, V24, P1908, DOI 10.1109/TPDS.2012.271
   Yin H., 2009, Proceedings of ACM International Conference on Multimedia, P25
   Yin H, 2009, IMC'09: PROCEEDINGS OF THE 2009 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P442
   Yu M., 2012, Proceedings of the 8th international conference on Emerging networking experiments and technologies, P145
   Zhang ZD, 2014, AER ADV ENG RES, V7, P63
NR 43
TC 23
Z9 23
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2015
VL 17
IS 6
BP 880
EP 892
DI 10.1109/TMM.2015.2417771
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CI1TM
UT WOS:000354527500010
DA 2024-07-18
ER

PT J
AU Li, XC
   Larson, M
   Hanjalic, A
AF Li, Xinchao
   Larson, Martha
   Hanjalic, Alan
TI Global-Scale Location Prediction for Social Images Using Geo-Visual
   Ranking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Geo-coordinate prediction; geo-location prediction; geo-visual ranking;
   image location prediction
ID REPRESENTATION
AB We propose an automatic method that addresses the challenge of predicting the geo-location of social images using only the visual content of those images. Our method is able to generate a geo-location prediction for an image globally. In this respect, it contrasts with other existing approaches, specifically with those that generate predictions restricted to specific cities, landmarks, or an otherwise pre-defined set of locations. The essence and the main novelty of our ranking-based method is that for a given query image a geo-location is recommended based on the evidence collected from images that are not only geographically close to this geo-location, but also have sufficient visual similarity to the query image within the considered image collection. Our method is evaluated experimentally on a public dataset of 8.8 million geo-tagged images from Flickr, released by the MediaEval 2013 evaluation benchmark. Experiments show that the proposed method delivers a substantial performance improvement compared to the existing related approaches, particularly for queries with high numbers of neighbors. In addition, a detailed analysis of the method's performance reveals the impact of different visual feature extraction and image matching strategies, as well as the densities and types of images found at different locations, on the prediction accuracy.
C1 [Li, Xinchao; Larson, Martha; Hanjalic, Alan] Delft Univ Technol, Multimedia Comp Grp, NL-2628 CD Delft, Netherlands.
C3 Delft University of Technology
RP Li, XC (corresponding author), Delft Univ Technol, Multimedia Comp Grp, NL-2628 CD Delft, Netherlands.
EM x.li-3@tudelft.nl; m.a.larson@tudelft.nl; a.hanjalic@tudelft.nl
OI Hanjalic, Alan/0000-0002-5771-2549
CR [Anonymous], P ICMR
   [Anonymous], 2009, P 18 INT C WORLD WID
   [Anonymous], P ICMR
   [Anonymous], 2013, P 3 ACM C INT C MULT
   [Anonymous], P 3DPVT
   [Anonymous], 2013, P 21 ACM INT C MULT
   [Anonymous], WORKING NOTES PLACIN
   [Anonymous], P MEDIAEVAL
   [Anonymous], P MEDIAEVAL
   [Anonymous], MULTIMODAL LOCATION
   [Anonymous], P MEDIAEVAL
   [Anonymous], P MEDIAEVAL
   Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018
   Avrithis Y, 2014, INT J COMPUT VISION, V107, P1, DOI 10.1007/s11263-013-0659-3
   Bay H., 2008, COMPUT VIS IMAGE UND, V10, P346, DOI DOI 10.1016/j.cviu.2007.09.014
   Chen DM, 2011, PROC CVPR IEEE, P737, DOI 10.1109/CVPR.2011.5995610
   Gronát P, 2013, PROC CVPR IEEE, P907, DOI 10.1109/CVPR.2013.122
   Hao Qiang., 2009, Proc. of 2009 ACM Multimedia Int. Conf. (ACM-Multimedia'09), P1021
   Hays J, 2008, PROC CVPR IEEE, P3436
   Hays J, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239455
   Jégou H, 2009, PROC CVPR IEEE, P1169, DOI 10.1109/CVPRW.2009.5206609
   Jégou H, 2010, INT J COMPUT VISION, V87, P316, DOI 10.1007/s11263-009-0285-2
   Kalantidis Y, 2011, MULTIMED TOOLS APPL, V51, P555, DOI 10.1007/s11042-010-0651-7
   Kennedy L.S., 2008, P ACM INT C WORLD WI, P297
   Kuhn HW, 2005, NAV RES LOG, V52, P7, DOI 10.1002/nav.20053
   Larson M., 2011, P 1 ACM INT C MULT R, P51
   Li J, 2013, IEEE T MULTIMEDIA, V15, P2058, DOI 10.1109/TMM.2013.2280127
   Li L. T., 2013, Multimedia Tools Appl., P1
   Li YP, 2009, IEEE I CONF COMP VIS, P1957, DOI 10.1109/ICCV.2009.5459432
   Lin TY, 2013, PROC CVPR IEEE, P891, DOI 10.1109/CVPR.2013.120
   Liu JJ, 2013, PROC INT CONF DATA, P505, DOI 10.1109/ICDE.2013.6544851
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luo J., 2009, INT J IMAGE PROCESSI, V3, P143, DOI DOI 10.1007/S11270-006-2859-8
   Luo JB, 2011, MULTIMED TOOLS APPL, V51, P187, DOI 10.1007/s11042-010-0623-y
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Perd'och M, 2009, PROC CVPR IEEE, P9, DOI 10.1109/CVPRW.2009.5206529
   Rattenbury Tye, 2007, 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P103, DOI 10.1145/1277741.1277762
   Riegler M, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P397, DOI 10.1145/2647868.2654894
   Serdyukov P, 2009, PROCEEDINGS 32ND ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P484, DOI 10.1145/1571941.1572025
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Steinhoff U, 2007, LECT NOTES COMPUT SC, V4794, P124
   Wang XJ, 2012, P IEEE, V100, P2705, DOI 10.1109/JPROC.2012.2193109
   Zamir AR, 2010, LECT NOTES COMPUT SC, V6314, P255, DOI 10.1007/978-3-642-15561-1_19
NR 44
TC 9
Z9 9
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2015
VL 17
IS 5
BP 674
EP 686
DI 10.1109/TMM.2015.2413351
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CG3AH
UT WOS:000353148300009
DA 2024-07-18
ER

PT J
AU Zheng, L
   Wang, SJ
   Liu, ZQ
   Tian, Q
AF Zheng, Liang
   Wang, Shengjin
   Liu, Ziqiong
   Tian, Qi
TI Fast Image Retrieval: Query Pruning and Early Termination
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Early termination; image retrieval; impact score; query pruning
ID GEOMETRY
AB Efficiency is of great importance for image retrieval systems. For this pragmatic issue, this paper proposes a fast image retrieval framework to speed up the online retrieval process. To this end, an impact score for local features is proposed in the first place, which considers multiple properties of a local feature, including TF-IDF, scale, saliency, and ambiguity. Then, to decrease memory consumption, the impact score is quantized to an integer, which leads to a novel inverted index organization, called Q-Index. Importantly, based on the impact score, two closely complementary strategies are introduced: query pruning and early termination. On one hand, query pruning discards less important features in the query. On the other hand, early termination visits indexed features only with high impact scores, resulting in the partial traversing of the inverted index. Our approach is tested on two benchmark datasets populated with an additional 1 million images to account as negative examples. Compared with full traversal of the inverted index, we show that our system is capable of visiting less than 10% of the "should-visit" postings, thus achieving a significant speed-up in query time while providing competitive retrieval accuracy.
C1 [Zheng, Liang; Wang, Shengjin; Liu, Ziqiong] Tsinghua Univ, Dept Elect Engn, Beijing 100084, Peoples R China.
   [Tian, Qi] Univ Texas San Antonio, Dept Comp Sci, San Antonio, TX 78249 USA.
C3 Tsinghua University; University of Texas System; University of Texas at
   San Antonio (UTSA)
RP Wang, SJ (corresponding author), Tsinghua Univ, Dept Elect Engn, Beijing 100084, Peoples R China.
EM liangzheng06@gmail.com; wgsgj@tsinghua.edu.cn; ziqiongliu@gmail.com;
   qitian@cs.utsa.edu
OI Zheng, Liang/0000-0002-1464-9500
FU National High Technology Research and Development Program of China (863
   Program) [2012AA011004]; National Science and Technology Support Program
   [2013BAK02B04]; ARO [W911NF-12-1-0057]; NEC Laboratories of America;
   National Science Foundation of China (NSFC) [61429201]
FX Manuscript received September 12, 2014; revised December 30, 2014 and
   February 22, 2015; accepted February 23, 2015. Date of publication March
   04, 2015; date of current version April 15, 2015. This work was
   supported in part by the National High Technology Research and
   Development Program of China (863 Program) under Grant 2012AA011004 and
   in part by the National Science and Technology Support Program under
   Grant 2013BAK02B04. The work of Q. Tian was supported in part by ARO
   Grant W911NF-12-1-0057, by the Faculty Research Award from the NEC
   Laboratories of America, and in part by the National Science Foundation
   of China (NSFC) under Grant 61429201. The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Cees Snoek. (Corresponding authors: Shengjin Wang; Qi Tian.)
CR Anh V. N., 2005, SIGIR 2005. Proceedings of the Twenty-Eighth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P226, DOI 10.1145/1076034.1076075
   [Anonymous], 2012, P 20 ACM MULTIMEDIA
   [Anonymous], 2010, PROC ACM SIGMM INT C
   [Anonymous], 2013, P 21 ACM INT C MULT, DOI 10.1145/2502081.2502171
   [Anonymous], 2006, P 29 ANN INT ACM SIG, DOI DOI 10.1145/1148170.1148235
   [Anonymous], 2012, P 20 ACM INT C MULTI
   Arandjelovic R, 2013, PROC CVPR IEEE, P1578, DOI 10.1109/CVPR.2013.207
   Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018
   Babenko A, 2012, PROC CVPR IEEE, P3069, DOI 10.1109/CVPR.2012.6248038
   Broder Andrei Z, 2003, P 12 INT C INF KNOWL, P426, DOI DOI 10.1145/956863.956944
   Cai Y., 2012, P ACM INT C MULT RET, P16
   Chakrabarti K, 2011, PROC INT CONF DATA, P709, DOI 10.1109/ICDE.2011.5767855
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Ding S, 2011, PROCEEDINGS OF THE 34TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR'11), P993
   Foo J., 2007, P 18 C AUSTRALASIAN, P63
   Gao Y, 2011, IEEE T MULTIMEDIA, V13, P1007, DOI 10.1109/TMM.2011.2160619
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Jégou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Jégou H, 2009, PROC CVPR IEEE, P1169, DOI 10.1109/CVPRW.2009.5206609
   Li L, 2013, IEEE MULTIMEDIA, V20, P13, DOI 10.1109/MMUL.2013.15
   Liu Z, 2014, IEEE T IMAGE PROCESS, V23, P1606, DOI 10.1109/TIP.2014.2305072
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mikulík A, 2010, LECT NOTES COMPUT SC, V6313, P1
   Muja M, 2009, VISAPP 2009: PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 1, P331
   Nister David, 2006, CVPR
   Niu ZX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P993, DOI 10.1145/2647868.2655006
   Perd'och M, 2009, PROC CVPR IEEE, P9, DOI 10.1109/CVPRW.2009.5206529
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Philbin J., 2008, P CVPR, P1
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Shen XH, 2012, PROC CVPR IEEE, P3013, DOI 10.1109/CVPR.2012.6248031
   Simonyan K, 2014, IEEE T PATTERN ANAL, V36, P1573, DOI 10.1109/TPAMI.2014.2301163
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Stewenius H., 2012, Proc. ECCV, P674, DOI DOI 10.1007/978-3-642-33709-348
   Strohman Trevor, 2007, 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P175, DOI 10.1145/1277741.1277774
   Swets DL, 1996, IEEE T PATTERN ANAL, V18, P831, DOI 10.1109/34.531802
   Tao R, 2014, PROC CVPR IEEE, P2099, DOI 10.1109/CVPR.2014.269
   Tolias G, 2013, IEEE I CONF COMP VIS, P1401, DOI 10.1109/ICCV.2013.177
   van Gemert JC, 2010, IEEE T PATTERN ANAL, V32, P1271, DOI 10.1109/TPAMI.2009.132
   Wang XY, 2011, IEEE I CONF COMP VIS, P209, DOI 10.1109/ICCV.2011.6126244
   Wengert C., 2011, P 19 ACM INT C MULT, P1437, DOI [DOI 10.1145/2072298.2072034, 10.1145/2072298.2072034]
   Xia Y, 2013, IEEE I CONF COMP VIS, P3416, DOI 10.1109/ICCV.2013.424
   Xie LX, 2014, COMPUT VIS IMAGE UND, V124, P31, DOI 10.1016/j.cviu.2013.12.011
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757
   Yang X, 2014, INFORM SCIENCES, V277, P794, DOI 10.1016/j.ins.2014.03.014
   Zhang H., 2006, P 14 ANN ACM INT C M, P287
   Zhang SL, 2013, IEEE I CONF COMP VIS, P1673, DOI 10.1109/ICCV.2013.210
   Zhang Shiliang., 2009, MM 09 P 17 ACM INT C, P75, DOI DOI 10.1145/1631272.1631285
   Zhang XP, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P287, DOI 10.1145/2647868.2654937
   Zhang YM, 2011, PROC CVPR IEEE, P809, DOI 10.1109/CVPR.2011.5995528
   Zheng L, 2014, PROC CVPR IEEE, P1947, DOI 10.1109/CVPR.2014.250
   Zheng L, 2014, PROC CVPR IEEE, P1963, DOI 10.1109/CVPR.2014.252
   Zheng L, 2014, IEEE T IMAGE PROCESS, V23, P3604, DOI 10.1109/TIP.2014.2329182
   Zheng L, 2014, IEEE T IMAGE PROCESS, V23, P3368, DOI 10.1109/TIP.2014.2330763
   Zhou WG, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2422956.2422960
   Zhu M., 2008, PROC 16 ACM C INFORM, P679
NR 59
TC 46
Z9 54
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2015
VL 17
IS 5
BP 648
EP 659
DI 10.1109/TMM.2015.2408563
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CG3AH
UT WOS:000353148300007
DA 2024-07-18
ER

PT J
AU Kang, CC
   Xiang, SM
   Liao, SC
   Xu, CS
   Pan, CH
AF Kang, Cuicui
   Xiang, Shiming
   Liao, Shengcai
   Xu, Changsheng
   Pan, Chunhong
TI Learning Consistent Feature Representation for Cross-Modal Multimedia
   Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-modal matching; documents and images; multimedia; retrieval
ID SIMILARITY; REGRESSION; SHRINKAGE
AB The cross-modal feature matching has gained much attention in recent years, which has many practical applications, such as the text-to-image retrieval. The most difficult problem of cross-modal matching is how to eliminate the heterogeneity between modalities. The existing methods (e.g., CCA and PLS) try to learn a common latent subspace, where the heterogeneity between two modalities is minimized so that cross-matching is possible. However, most of these methods require fully paired samples and suffer difficulties when dealing with unpaired data. Besides, utilizing the class label information has been found as a good way to reduce the semantic gap between the low-level image features and high-level document descriptions. Considering this, we propose a novel and effective supervised algorithm, which can also deal with the unpaired data. In the proposed formulation, the basis matrices of different modalities are jointly learned based on the training samples. Moreover, a local group-based priori is proposed in the formulation to make a better use of popular block based features (e.g., HOG and GIST). Extensive experiments are conducted on four public databases: Pascal VOC2007, LabelMe, Wikipedia, and NUS-WIDE. We also evaluated the proposed algorithm with unpaired data. By comparing with existing state-of-the-art algorithms, the results show that the proposed algorithm is more robust and achieves the best performance, which outperforms the second best algorithm by about 5% on both the Pascal VOC2007 and NUS-WIDE databases.
C1 [Kang, Cuicui; Xiang, Shiming; Liao, Shengcai; Xu, Changsheng; Pan, Chunhong] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS
RP Kang, CC (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM cckang@nlpr.ia.ac.cn; smxiang@nlpr.ia.ac.cn; scliao@nlpr.ia.ac.cn;
   csxu@nlpr.ia.ac.cn; chpan@nlpr.ia.ac.cn
RI xu, cj/HJZ-3488-2023
OI Liao, Shengcai/0000-0001-8941-2295
FU National Basic Research Program of China [2012CB316304]; National
   Natural Science Foundation of China [61272331, 91338202, 61203267,
   91438105]
FX This work was supported in part by the National Basic Research Program
   of China under Grant 2012CB316304 and by the National Natural Science
   Foundation of China under Grant 61272331, Grant 91338202, Grant
   61203267, and Grant 91438105. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr.
   Gokhan Tur.
CR [Anonymous], P ICB
   [Anonymous], 2011, P ICML
   [Anonymous], 2012, ARXIV12064660
   [Anonymous], 2009, P ACM INT C IM VID R
   [Anonymous], 2008, Proceedings of the 25th international conference on Machine learning
   [Anonymous], 2003, INTRO LECT CONVEX OP
   [Anonymous], P HUM LANG TECHN ANN
   [Anonymous], SIAM J OPTI IN PRESS
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Wang C, 2009, PROC CVPR IEEE, P1903, DOI [10.1109/CVPR.2009.5206800, 10.1109/CVPRW.2009.5206800]
   Pereira JC, 2014, IEEE T PATTERN ANAL, V36, P521, DOI 10.1109/TPAMI.2013.142
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Everingham Mark, 2007, PASCAL VISUAL OBJECT
   Gong YC, 2014, INT J COMPUT VISION, V106, P210, DOI 10.1007/s11263-013-0658-4
   Guillaumin M, 2010, PROC CVPR IEEE, P902, DOI 10.1109/CVPR.2010.5540120
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   Hwang S. J., 2010, P BMVC, P1
   Hwang SJ, 2010, PROC CVPR IEEE, P2971, DOI 10.1109/CVPR.2010.5540043
   Jia YQ, 2011, IEEE I CONF COMP VIS, P2407, DOI 10.1109/ICCV.2011.6126524
   Kan MN, 2012, LECT NOTES COMPUT SC, V7572, P808, DOI 10.1007/978-3-642-33718-5_58
   Lampert CH, 2010, LECT NOTES COMPUT SC, V6312, P566, DOI 10.1007/978-3-642-15552-9_41
   Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565
   Lei Z, 2012, IEEE T INF FOREN SEC, V7, P1707, DOI 10.1109/TIFS.2012.2210041
   Li AN, 2009, PROC CVPR IEEE, P605, DOI 10.1109/CVPRW.2009.5206659
   Liu J., 2009, P 25 C UNCERTAINTY A, P339, DOI DOI 10.5555/1795114.1795154
   Liu Wei, 2011, Reports in Parasitology, V1, P1
   Masci J, 2014, IEEE T PATTERN ANAL, V36, P824, DOI 10.1109/TPAMI.2013.225
   Mei T, 2014, ACM COMPUT SURV, V46, DOI 10.1145/2536798
   Monay F, 2007, IEEE T PATTERN ANAL, V29, P1802, DOI 10.1109/TPAMI.2007.1097
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Pan YW, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P717, DOI 10.1145/2600428.2609568
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Rasiwasia N, 2007, IEEE T MULTIMEDIA, V9, P923, DOI 10.1109/TMM.2007.900138
   Rosipal R, 2006, LECT NOTES COMPUT SC, V3940, P34, DOI 10.1007/11752790_2
   Sharma A, 2012, PROC CVPR IEEE, P2160, DOI 10.1109/CVPR.2012.6247923
   Sharma A, 2011, PROC CVPR IEEE, P593, DOI 10.1109/CVPR.2011.5995350
   Shengcai Liao, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P79, DOI 10.1109/ICCVW.2009.5457714
   Socher R., 2013, ADV NEURAL INFORM PR, V26, P935, DOI DOI 10.1007/978-3-319-46478-7
   Socher R, 2010, PROC CVPR IEEE, P966, DOI 10.1109/CVPR.2010.5540112
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Tenenbaum JB, 2000, NEURAL COMPUT, V12, P1247, DOI 10.1162/089976600300015349
   Tibshirani R, 1996, J ROY STAT SOC B, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x
   Wang J, 2012, IEEE T PATTERN ANAL, V34, P2393, DOI 10.1109/TPAMI.2012.48
   Wang KY, 2013, IEEE I CONF COMP VIS, P2088, DOI 10.1109/ICCV.2013.261
   Xia TA, 2010, IEEE T SYST MAN CY B, V40, P1438, DOI 10.1109/TSMCB.2009.2039566
   Xiang SM, 2012, IEEE T NEUR NET LEAR, V23, P1738, DOI 10.1109/TNNLS.2012.2212721
   Zhuang Jinfeng., 2011, WSDM, P625
   Zhuang Y., 2013, P 27 AAAI C ART INT, P1070
NR 48
TC 184
Z9 195
U1 2
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2015
VL 17
IS 3
BP 370
EP 381
DI 10.1109/TMM.2015.2390499
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CE1QB
UT WOS:000351585700009
DA 2024-07-18
ER

PT J
AU Tian, XM
   Jia, QH
   Mei, T
AF Tian, Xinmei
   Jia, Qianghuai
   Mei, Tao
TI Query Difficulty Estimation for Image Search With Query Reconstruction
   Error
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image retrieval; image search quality; query difficulty estimation;
   query reconstruction
ID MULTIMEDIA SEARCH; RETRIEVAL; PREDICTION; RERANKING
AB Current image search engines suffer from a radical variance in retrieval performance over different queries. It is therefore desirable to identify those "difficult" queries in order to handle them properly. Query difficulty estimation is an attempt to predict the performance of the search results returned by an image search system. Most existing methods for query difficulty estimation focus on investigating statistical characteristics of the returned images only, while neglecting very important information, i.e., the query and its relationship with returned images. This relationship plays a crucial role in query difficulty estimation and should be explored further. In this paper we propose a novel query difficulty estimation method with query reconstruction error. This method is proposed based on the observation that, given the images returned for an unknown query, we can easily deduce what the query is from those images if the search results are high quality (i.e., lots of relevant images returned); otherwise, it is difficult to deduce the original query. Therefore, we propose to predict the query difficulty by measuring to what extent the original query can be recovered from the image search results. Specifically, we first reconstruct a visual query from the returned images to summarize their visual theme, and then use the reconstruction error, i.e., the distance between the original textual query and the reconstructed visual query, to estimate the query difficulty. We conduct extensive experiments on two real-world Web image datasets and demonstrate the effectiveness of the proposed method.
C1 [Tian, Xinmei; Jia, Qianghuai] Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230027, Peoples R China.
   [Mei, Tao] Microsoft Res, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Microsoft
RP Tian, XM (corresponding author), Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230027, Peoples R China.
EM xinmei@ustc.edu.cn; jqh218@mail.ustc.edu.cn; tmei@microsoft.com
RI Mei, Tao/GQZ-0596-2022
OI Mei, Tao/0000-0002-5990-7307
FU 973 project [2015CB351803]; NSFC [61390514, 61201413]; Specialized
   Research Fund for the Doctoral Program of Higher Education
   [WJ2100060003]; Fundamental Research Funds for the Central Universities
   [WK2100060007, WK2100060011]
FX This work was supported by the 973 project under Contract 2015CB351803,
   the NSFC under Contract 61390514 and Contract 61201413, the Specialized
   Research Fund for the Doctoral Program of Higher Education No.
   WJ2100060003, and the Fundamental Research Funds for the Central
   Universities No. WK2100060007 and No. WK2100060011. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Enrico Magli.
CR [Anonymous], ACM T INT INT SYST
   [Anonymous], RANK CORRELATION MET
   [Anonymous], 2012, ACM T MULTIMEDIA COM
   [Anonymous], 2009, P 17 ACM INT C MULTI, DOI DOI 10.1145/1631272.1631361
   [Anonymous], CVPR
   Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018
   Carbonell JG, 1997, INT JOINT CONF ARTIF, P708
   Carmel D., 2006, Proceedings of the Twenty-Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P390, DOI 10.1145/1148170.1148238
   Carpineto C, 2001, ACM T INFORM SYST, V19, P1, DOI 10.1145/366836.366860
   Chen T, 2014, IEEE T MULTIMEDIA, V16, P612, DOI 10.1109/TMM.2014.2301978
   Chua T.-S., 2009, CIVR, P1, DOI 10.1145/1646396.1646452
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Chum O, 2011, PROC CVPR IEEE, P889, DOI 10.1109/CVPR.2011.5995601
   Cover T. M., 1991, ELEMENTS INFORM THEO
   Cronen-Townsend S., 2002, Proceedings of SIGIR 2002. Twenty-Fifth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P299
   Diaz Fernando, 2007, 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P583, DOI 10.1145/1277741.1277841
   Gibbons J., 1992, Nonparametric Statistical Inference
   González-Díaz I, 2014, IEEE T MULTIMEDIA, V16, P169, DOI 10.1109/TMM.2013.2286083
   He JY, 2008, LECT NOTES COMPUT SC, V4956, P689
   Imran H., 2010, Proceedings 2010 10th IEEE International Conference on Data Mining Workshops (ICDMW 2010), P867, DOI 10.1109/ICDMW.2010.81
   Järvelin K, 2002, ACM T INFORM SYST, V20, P422, DOI 10.1145/582415.582418
   Jing Y, 2008, IEEE T PATTERN ANAL, V30, P1877, DOI 10.1109/TPAMI.2008.121
   Kofler C, 2014, IEEE T MULTIMEDIA, V16, P1973, DOI 10.1109/TMM.2014.2347937
   Kofler Christoph, 2012, P 20 ACM INT C MULT, P319, DOI 10.1145/2393347.2393395
   Krapac J, 2010, PROC CVPR IEEE, P1094, DOI 10.1109/CVPR.2010.5540092
   Kreyszig E., 1997, ADV ENG MATH
   Kurland Oren, 2012, P CIKM, P823, DOI DOI 10.1145/2396761.2396866
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Li Xirong., 2008, Proceedings of the 1st ACM International Conference on Multimedia Information Retrieval, MIR '08, P180
   Li YX, 2012, IEEE T MULTIMEDIA, V14, P1618, DOI 10.1109/TMM.2012.2199292
   Li YX, 2011, LECT NOTES COMPUT SC, V6524, P479
   Liu B, 2011, IEEE MULTIMEDIA, V18, P22, DOI 10.1109/MMUL.2011.37
   Liu D., 2008, P IEEE C COMP VIS PA, P590
   Liu YA, 2009, PROCEEDINGS 32ND ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P500, DOI 10.1145/1571941.1572027
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mei T, 2014, ACM COMPUT SURV, V46, DOI 10.1145/2536798
   Nister David, 2006, CVPR
   Qi GJ, 2012, IEEE T PATTERN ANAL, V34, P850, DOI 10.1109/TPAMI.2011.191
   Rudinac S, 2012, INT J MULTIMED INF R, V1, P263, DOI 10.1007/s13735-012-0018-0
   Rudinac S, 2010, LECT NOTES COMPUT SC, V5993, P645, DOI 10.1007/978-3-642-12275-0_67
   Shtok A, 2012, ACM T INFORM SYST, V30, DOI 10.1145/2180868.2180873
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Tian XM, 2012, IEEE T MULTIMEDIA, V14, P951, DOI 10.1109/TMM.2011.2177647
   Tian XM, 2010, IEEE T IMAGE PROCESS, V19, P805, DOI 10.1109/TIP.2009.2035866
   Tian Xinmei, 2011, P ACM INT C MULT, P363
   Wang M., 2009, Tech. Rep. MSR-TR-2009-30
   Winn J, 2005, IEEE I CONF COMP VIS, P1800
   Xie HT, 2014, IEEE T MULTIMEDIA, V16, P1104, DOI 10.1109/TMM.2014.2305909
   Xing X, 2010, LECT NOTES COMPUT SC, V5993, P581, DOI 10.1007/978-3-642-12275-0_52
   Yan R, 2003, LECT NOTES COMPUT SC, V2728, P238
   Yao T, 2013, IEEE T IMAGE PROCESS, V22, P1642, DOI 10.1109/TIP.2012.2236341
   Yom-Tov E., 2005, SIGIR 2005. Proceedings of the Twenty-Eighth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P512, DOI 10.1145/1076034.1076121
   Yun Zhou, 2007, 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P543, DOI 10.1145/1277741.1277835
   Zhai CX, 2004, ACM T INFORM SYST, V22, P179, DOI 10.1145/984321.984322
   Zhou WG, 2014, IEEE T MULTIMEDIA, V16, P601, DOI 10.1109/TMM.2014.2301979
   Zhou Y., 2006, CIKM, P567
NR 56
TC 5
Z9 5
U1 0
U2 34
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2015
VL 17
IS 1
BP 79
EP 91
DI 10.1109/TMM.2014.2368714
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AX6QW
UT WOS:000347047400008
DA 2024-07-18
ER

PT J
AU Bu, SH
   Liu, ZB
   Han, JW
   Wu, J
   Ji, RR
AF Bu, Shuhui
   Liu, Zhenbao
   Han, Junwei
   Wu, Jun
   Ji, Rongrong
TI Learning High-Level Feature by Deep Belief Networks for 3-D Model
   Retrieval and Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3-D model recognition; 3-D model retrieval; bag-of-words; deep belief
   networks; deep learning
ID 3D SHAPE RETRIEVAL; OBJECT RETRIEVAL; DESCRIPTORS; WORDS
AB 3-D shape analysis has attracted extensive research efforts in recent years, where the major challenge lies in designing an effective high-level 3-D shape feature. In this paper, we propose a multi-level 3-D shape feature extraction framework by using deep learning. The low-level 3-D shape descriptors are first encoded into geometric bag-of-words, from which middle-level patterns are discovered to explore geometric relationships among words. After that, high-level shape features are learned via deep belief networks, which are more discriminative for the tasks of shape classification and retrieval. Experiments on 3-D shape recognition and retrieval demonstrate the superior performance of the proposed method in comparison to the state-of-the-art methods.
C1 [Bu, Shuhui; Liu, Zhenbao; Han, Junwei; Wu, Jun] Northwestern Polytech Univ, Xian 710072, Peoples R China.
   [Ji, Rongrong] Xiamen Univ, Sch Informat Sci & Engn, Dept Cognit Sci, Xiamen 361005, Fujian, Peoples R China.
C3 Northwestern Polytechnical University; Xiamen University
RP Han, JW (corresponding author), Northwestern Polytech Univ, Xian 710072, Peoples R China.
EM bushuhui@nwpu.edu.cn; liuzhenbao@nwpu.edu.cn; jhan@nwpu.edu.cn;
   junwu@nwpu.edu.cn; rrji@xmu.edu.cn
FU National Natural Science Foundation of China [61202185, 61003137,
   91120005, 61473231, 61373076, 61422210, 61103062]; Fundamental Research
   Funds for the Central Universities [310201401-(JCQ01009, JCQ01012),
   2013121026]; Shaanxi Natural Science Fund [2012JQ8037]; Open Project
   Program of the State Key Lab of CAD& CG of Zhejiang University [A1306];
   Xiamen University [985]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61202185, Grant 61003137, Grant
   91120005, Grant 61473231, Grant 61373076, Grant 61422210, and Grant
   61103062, the Fundamental Research Funds for the Central Universities
   under Grant 310201401-(JCQ01009, JCQ01012) and Grant 2013121026, the
   Shaanxi Natural Science Fund under Grant 2012JQ8037, the Open Project
   Program of the State Key Lab of CAD& CG of Zhejiang University under
   Grant A1306, and Xiamen University under the 985 Project. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. K. Selcuk Candan. (Corresponding author: Junwei
   Han.)
CR [Anonymous], 2012, AFFINE INVARIANT PHO
   [Anonymous], P EUR WORKSH 3D OBJ
   [Anonymous], 2010, P 3 EUR WORKSH 3D OB
   [Anonymous], 2011, PROC EUROGRAPHICS 20, P79, DOI DOI 10.2312/3DOR/3DOR11/079-088
   Barra V, 2013, PATTERN RECOGN, V46, P2985, DOI 10.1016/j.patcog.2013.03.019
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bronstein AM, 2008, MONOGR COMPUT SCI, P1, DOI 10.1007/978-0-387-73301-2_1
   Bronstein AM, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1899404.1899405
   Bronstein MM, 2010, PROC CVPR IEEE, P1704, DOI 10.1109/CVPR.2010.5539838
   Bu SH, 2014, VISUAL COMPUT, V30, P867, DOI 10.1007/s00371-014-0970-1
   Castellani U, 2008, COMPUT GRAPH FORUM, V27, P643, DOI 10.1111/j.1467-8659.2008.01162.x
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   Cornea ND, 2005, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P366, DOI 10.1109/SMI.2005.1
   de Amorim RC, 2012, PATTERN RECOGN, V45, P1061, DOI 10.1016/j.patcog.2011.08.012
   Del Bimbo A, 2006, ACM T MULTIM COMPUT, V2, P20
   Dubrovina A., 2010, P S 3D DAT P, V2
   Gao Y, 2014, IEEE T IND ELECTRON, V61, P2088, DOI 10.1109/TIE.2013.2262760
   Gao Y, 2012, IEEE T IMAGE PROCESS, V21, P4290, DOI 10.1109/TIP.2012.2199502
   Gao Y, 2012, IEEE T IMAGE PROCESS, V21, P2269, DOI 10.1109/TIP.2011.2170081
   Gao Y, 2011, IEEE T MULTIMEDIA, V13, P1007, DOI 10.1109/TMM.2011.2160619
   Giorgi D., 2007, 9 CNR IMATI
   Hilaga M, 2001, COMP GRAPH, P203, DOI 10.1145/383259.383282
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Jain V, 2007, COMPUT AIDED DESIGN, V39, P398, DOI 10.1016/j.cad.2007.02.009
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Kazhdan M., 2003, Symposium on Geometry Processing, P156
   Knopp J, 2010, LECT NOTES COMPUT SC, V6316, P589, DOI 10.1007/978-3-642-15567-3_43
   Laga H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2516971.2516975
   Larochelle H., 2008, P 25 INT C MACH LEAR, P536
   Lavoue G., 2011, Eurographics Conference on 3D Object Retrieval, P41, DOI DOI 10.2312/3DOR/3DOR11/041-048
   Lavoué G, 2012, VISUAL COMPUT, V28, P931, DOI 10.1007/s00371-012-0724-x
   Lee H., 2009, P 26 ANN INT C MACH, P609
   Leng BA, 2011, MULTIMED TOOLS APPL, V51, P935, DOI 10.1007/s11042-009-0424-3
   Lian ZH, 2010, IEEE IMAGE PROC, P3181, DOI 10.1109/ICIP.2010.5654226
   Litman R, 2014, IEEE T PATTERN ANAL, V36, P171, DOI 10.1109/TPAMI.2013.148
   Liu Y, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P86
   Liu ZB, 2013, J COMPUT SCI TECH-CH, V28, P836, DOI 10.1007/s11390-013-1382-9
   Mademlis A, 2008, IEEE T MULTIMEDIA, V10, P819, DOI 10.1109/TMM.2008.922790
   Maes C., 2010, BIOMETRICS THEORY AP, DOI [DOI 10.1109/BTAS.2010.5634543, 10.1109/BTAS.2010.5634543]
   Osada R, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P154, DOI 10.1109/SMA.2001.923386
   Ovsjanikov Maks, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P320, DOI 10.1109/ICCVW.2009.5457682
   Papadakis P, 2010, INT J COMPUT VISION, V89, P177, DOI 10.1007/s11263-009-0281-6
   Reuter M., 2005, P 2005 ACM S SOLID P, P101, DOI DOI 10.1145/1060244.1060256
   Sfikas K, 2012, VISUAL COMPUT, V28, P943, DOI 10.1007/s00371-012-0714-z
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   Siddiqi K, 2008, MACH VISION APPL, V19, P261, DOI 10.1007/s00138-007-0097-8
   Sipiran I, 2013, COMPUT GRAPH-UK, V37, P460, DOI 10.1016/j.cag.2013.04.002
   Smeets D, 2009, LECT NOTES COMPUT SC, V5702, P757, DOI 10.1007/978-3-642-03767-2_92
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Tabia H., 2013, Proceedings of the Sixth Eurographics Workshop on 3D Object Retrieval, P17
   Tangelder JH, 2008, MULTIMED TOOLS APPL, V39, P441, DOI 10.1007/s11042-007-0181-0
   Tierny J, 2009, COMPUT GRAPH FORUM, V28, P41, DOI 10.1111/j.1467-8659.2008.01190.x
   Toldo R., 2009, Proceedings of the 2nd Eurographics Conference on 3D Object Retrieval, P21
   Wu HY, 2010, PROC CVPR IEEE, P438, DOI 10.1109/CVPR.2010.5540180
NR 57
TC 96
Z9 118
U1 0
U2 62
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2014
VL 16
IS 8
BP 2154
EP 2167
DI 10.1109/TMM.2014.2351788
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AU4HL
UT WOS:000345571300007
DA 2024-07-18
ER

PT J
AU Hao, J
   Wang, GF
   Seo, B
   Zimmermann, R
AF Hao, Jia
   Wang, Guanfeng
   Seo, Beomjoo
   Zimmermann, Roger
TI Point of Interest Detection and Visual Distance Estimation for
   Sensor-Rich Video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Point of interest; sensor-rich video; visual distance estimation
ID LOCATION; RECOGNITION; OBJECTS
AB Due to technological advances and the popularity of camera sensors, it is now straightforward for users to capture and share videos. A large number of geo-tagged photos and videos have been accumulating continuously on the web, posing a challenging problem for mining this type of media data. In one application scenario, users might desire to know what the Points of Interest (POI) are which contain important objects or places in a video. Existing solutions attempt to examine the content of the videos and recognize objects and events. This is typically time-consuming and computationally expensive and the results can be uneven. Therefore these methods face challenges when applied to large video repositories. We propose a novel technique that leverages sensor-generated meta-data (camera locations and viewing directions) which are automatically acquired as continuous streams together with the video frames. Existing smartphones can easily accommodate such integrated recording tasks. By considering a collective set of videos and leveraging the acquired auxiliary meta-data, our approach is able to detect interesting regions and objects (POIs) and their distances from the camera positions in a fully automated way. Because of its computational efficiency, the proposed method scales well and our experiments show very promising results.
C1 [Hao, Jia; Wang, Guanfeng; Zimmermann, Roger] Natl Univ Singapore, Sch Comp, Singapore 119077, Singapore.
   [Seo, Beomjoo] Hongik Univ, Sch Games, Sejong 339701, South Korea.
C3 National University of Singapore; Hongik University
RP Hao, J (corresponding author), Natl Univ Singapore, Sch Comp, Singapore 119077, Singapore.
EM haojia@comp.nus.edu.sg; wanggf@comp.nus.edu.sg; bseo@hongik.ac.kr;
   rogerz@comp.nus.edu.sg
RI Zimmermann, Roger/D-7944-2015; Wang, Guan-Feng/AAA-3631-2022
OI Zimmermann, Roger/0000-0002-7410-2590; 
FU International Research Centre; Singapore Funding Initiative; Singapore
   National Research Foundation; Hongik University
FX This work was supported in part by the International Research Centre,
   Singapore Funding Initiative, Singapore National Research Foundation,
   administered by the Centre of Social Media Innovations for Communities
   (COSMIC), IDM Programme Office, and by the Hongik University under the
   new faculty research support fund. An earlier version of this paper was
   presented at the 19th ACM International Conference on Multimedia,
   Scottsdale, AZ, USA, November-December 2011. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Kasim Selcuk Candan.
CR [Anonymous], P ACM INT C ADV GEOG
   [Anonymous], 1999, 199966 STANF U
   [Anonymous], 2009, GEOSPATIAL WEB, DOI DOI 10.1007/978-1-84628-827-2_15
   Arase Y., 2010, Proceedings of the international conference on Multimedia, MM '10, (New York, NY, USA), P133
   Ay S., 2008, Proceedings of the 16th ACM international conference on Multimedia (MM '08), P309
   Ay S.A., 2010, Proceedings of the 18th SIGSPATIAL International Conference on Advances in Geographic Information Systems, GIS'10, P280, DOI [DOI 10.1145/1869790.1869830, 10.1145/1869790.1869830]
   Cisco Systems Inc, 2014, CISC VIS NETW IND FO
   Duygulu P, 2002, LECT NOTES COMPUT SC, V2353, P97
   Epshtein Boris., 2007, Proceedings of the ACM International Symposium on Advances in Geographic Information Systems (ACM GIS), P1
   Ester M., 1996, KDD-96 Proceedings. Second International Conference on Knowledge Discovery and Data Mining, P226
   Giannotti F, 2007, KDD-2007 PROCEEDINGS OF THE THIRTEENTH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P330
   Graham C., 1965, Vision and Visual Perception
   Hakeem A, 2006, INT C PATT RECOG, P82
   Hao J., 2011, Proceedings of the 19th ACM International Conference on Multimedia, MM'11, P1013
   Hariharan R, 2004, LECT NOTES COMPUT SC, V3234, P106
   Ji Rongrong., 2009, P 17 ASS COMP MACH I, P105, DOI DOI 10.1145/1631272.1631289
   Kennedy L.S., 2008, P ACM INT C WORLD WI, P297
   Kennedy Lyndon., 2007, Proceedings of the 15th International Conference on Multimedia, P631, DOI DOI 10.1145/1291233.1291384
   Li Q., 2008, P 16 ACM SIGSPATIAL, P1, DOI DOI 10.1145/1463434.1463477
   Liu KE, 2013, NEUROCOMPUTING, V105, P90, DOI 10.1016/j.neucom.2012.05.035
   Lu X., 2010, Proceedings of the 18th ACM International Conference on Multimedia, Firenze Italy, 25 October 2010, DOI [19.1145/1873951.1873972, DOI 10.1145/1873951.1873972]
   Morzy M, 2007, LECT NOTES ARTIF INT, V4571, P667
   Naaman M, 2004, ACM-IEEE J CONF DIG, P53, DOI 10.1145/996350.996366
   Park MH, 2007, LECT NOTES COMPUT SC, V4611, P1130
   Pigeau A., 2005, 13th Annual ACM International Conference on Multimedia, P141, DOI 10.1145/1101149.1101170
   Rodden K., 2003, P SIGCHI C HUMAN FAC, P409, DOI DOI 10.1145/642611.642682
   Seo B., 2011, Proceedings of the 19th ACM international conference on Multimedia, P791
   Sivic J, 2005, IEEE I CONF COMP VIS, P370
   Takeuchi Y., 2005, P 1 INT WORKSH PERS, V149, P91, DOI [10.1007/11833529_64, DOI 10.1007/11833529_64]
   Toyama Kentaro., 2003, P 11 ACM INT C MULTI, P156
   Wang G., 2013, P 21 ACM INT C MULT, P439
   Yavas G, 2005, DATA KNOWL ENG, V54, P121, DOI 10.1016/j.datak.2004.09.004
   Zheng Y., 2009, WWW, P791, DOI [10.1145/1526709.1526816, DOI 10.1145/1526709.1526816]
   Zheng YT, 2009, PROC CVPR IEEE, P1085, DOI 10.1109/CVPRW.2009.5206749
NR 34
TC 9
Z9 10
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2014
VL 16
IS 7
BP 1929
EP 1941
DI 10.1109/TMM.2014.2330802
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AT1UX
UT WOS:000344720300011
DA 2024-07-18
ER

PT J
AU Lou, ZY
   Gevers, T
AF Lou, Zhongyu
   Gevers, Theo
TI Image Alignment by Piecewise Planar Region Matching
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Local image registration; piecewise alignment; region-based image
   alignment; segmentation
ID OPTICAL-FLOW
AB Robust image registration is a challenging problem, especially when dealing with severe changes in illumination and viewpoint. Previous methods assume a global geometric model (e. g., homography) and, hence, are only able to align images under predefined constraints (e. g., planar scenes and parallax-free camera motion). However, these constraints may not hold for natural scenes and uncontrolled imaging conditions. Therefore, this paper proposes a novel method which approximates image regions with planes by incorporating piecewise local geometric models. The approximated planar regions are obtained by exploiting a hierarchical figure-ground segmentation method. Each such planar region assumes an affine transformation. To achieve the alignment of the planar regions, an energy function is defined which employs intensity, a key-point descriptor, and geometric information under a global constraint. By re-segmenting and re-merging planar regions iteratively in an energy minimization framework, the method is able to align images even under significant changes in illumination and viewpoint. Experiments on two datasets show that the proposed method outperforms state-of-the-art, especially in the case of large appearance variations and it is, therefore, applicable to web-images (i.e., unconstrained setting) which are taken from the same scene with different viewpoints.
C1 [Lou, Zhongyu; Gevers, Theo] Univ Amsterdam, Intelligent Syst Lab Amsterdam, Inst Informat, NL-1098 XH Amsterdam, Netherlands.
   [Gevers, Theo] Univ Autonoma Barcelona, Comp Vis Ctr, E-08193 Barcelona, Spain.
C3 University of Amsterdam; Autonomous University of Barcelona; Centre de
   Visio per Computador (CVC)
RP Lou, ZY (corresponding author), Univ Amsterdam, Intelligent Syst Lab Amsterdam, Inst Informat, NL-1098 XH Amsterdam, Netherlands.
EM z.lou@uva.nl; th.gevers@uva.nl
CR Agarwal S, 2009, IEEE I CONF COMP VIS, P72, DOI 10.1109/ICCV.2009.5459148
   [Anonymous], 2008, CVPR, DOI DOI 10.1109/CVPR.2008.4587754
   [Anonymous], 2001, Robotica, DOI DOI 10.1017/S0263574700223217
   Baker S, 2004, INT J COMPUT VISION, V56, P221, DOI 10.1023/B:VISI.0000011205.11775.fd
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Bhat Pravin., 2006, In Proceedings of the IEEE Conference on Computer Vision and Pattern, CVPR 2006, P2491
   Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3
   Brox T, 2011, IEEE T PATTERN ANAL, V33, P500, DOI 10.1109/TPAMI.2010.143
   Carreira J, 2012, IEEE T PATTERN ANAL, V34, P1312, DOI 10.1109/TPAMI.2011.231
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Ho HT, 2013, IEEE T IMAGE PROCESS, V22, P1571, DOI 10.1109/TIP.2012.2233489
   Kanade T., 1981, P 7 INT JOINT C ARTI, V1, P674, DOI DOI 10.5555/1623264.1623280
   Lin WY, 2012, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2012.6247651
   Lin WY, 2011, PROC CVPR IEEE, P345, DOI 10.1109/CVPR.2011.5995314
   Liu C, 2008, LECT NOTES COMPUT SC, V5304, P28, DOI 10.1007/978-3-540-88690-7_3
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Matthews I, 2004, INT J COMPUT VISION, V60, P135, DOI 10.1023/B:VISI.0000029666.37597.d3
   Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x
   Myronenko A, 2007, PROC CVPR IEEE, P146
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Prokaj J., 2012, 2012 IEEE Workshop on Applications of Computer Vision (WACV), P273, DOI 10.1109/WACV.2012.6163028
   Shum HY, 2000, INT J COMPUT VISION, V36, P101, DOI 10.1023/A:1008195814169
   Szeliski R, 2006, FOUND TRENDS COMPUT, V2, P1, DOI 10.1561/0600000009
   Szeliski Richard., 1997, P SIGGRAPH 97 COMPUT, P251, DOI DOI 10.1145/258734.258861
   van de Weijer J, 2004, IEEE IMAGE PROC, P1835
   Vedaldi A., VLFEAT OPEN PORTABLE
   Yu G., 2011, IMAGE PROCESS LINE, P1597
NR 28
TC 27
Z9 28
U1 1
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2014
VL 16
IS 7
BP 2052
EP 2061
DI 10.1109/TMM.2014.2346476
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AT1UX
UT WOS:000344720300021
DA 2024-07-18
ER

PT J
AU Jin, YC
   Wen, YG
   Hu, H
   Montpetit, MJ
AF Jin, Yichao
   Wen, Yonggang
   Hu, Han
   Montpetit, Marie-Jose
TI Reducing Operational Costs in Cloud Social TV: An Opportunity for Cloud
   Cloning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cloud clone; cost minimization; markov decision process; Q-learning;
   social TV
ID SYSTEMS; SCREEN
AB The emergence of social TV has transformed TV experiences, providing a unified media experience across different devices. In response to this trend, we have implemented a multi-screen social TV system, offering video teleportation as an attractive feature. The enabling technology is instantiating a cloud clone to support all media outlets of each user. As the user shifts his attention from one device to the other, the cloud clone might migrate to a better location to reduce its operational cost. This paper investigates this cloud clone migration problem, aiming to minimize the monetary cost on operating video teleportation. Specifically, we formulate it into a Markov Decision Problem, to balance the trade-off between the migration cost and the content transmission cost. Under this framework, four algorithms are proposed to solve this optimization problem. We first characterize an upper and a lower bound for the optimal cost, by considering a random fixed placement and an offline algorithm. We then present a semi-online and a more practical Q-learning approach to make online decisions. Their performances are evaluated based on both simulated and real user traces. The results show that the Q-learning method achieves up to 25% cost compared to random fixed placement in typical scenarios. The savings are affected by the delivery path length, the migration size, and the user behavior pattern. Moreover, our investigations reveal the optimal cloud clone location is either at the nearest or the furthest node to the user along the content delivery path for a single user scenario.
C1 [Jin, Yichao; Wen, Yonggang] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
   [Hu, Han] Natl Univ Singapore, Sch Comp, Singapore 119077, Singapore.
   [Montpetit, Marie-Jose] MIT, Comparat Media Studies Dept, Cambridge, MA 02139 USA.
C3 Nanyang Technological University; National University of Singapore;
   Massachusetts Institute of Technology (MIT)
RP Jin, YC (corresponding author), Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
EM yjin3@ntu.edu.sg; ygwen@ntu.edu.sg; huh@comp.nus.edu.sg; mariejo@mit.edu
RI Wen, Yonggang/P-9406-2017; Wen, Yonggang/B-8848-2011
OI Wen, Yonggang/0000-0002-2751-5114; 
CR [Anonymous], 2003, Conference on Knowledge Discovery and Data Mining (SIGKDD), DOI DOI 10.1145/956750.956815
   Armbrust M, 2010, COMMUN ACM, V53, P50, DOI 10.1145/1721654.1721672
   Bernaschi M., 2005, WMuNeP '05: Proceedings of the 1st ACM workshop on Wireless multimedia networking and performance modeling, P16
   Chakareski J, 2007, IEEE COMMUN MAG, V45, P77, DOI 10.1109/MCOM.2007.284541
   Chang TH, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P2163
   Chen FF, 2012, IEEE INFOCOM SER, P433, DOI 10.1109/INFCOM.2012.6195782
   Cisco Systems Inc, 2008, US GUID CISC MED EXP
   Gopalakrishnan V., 2011, P 2011 ACM SIGCOMM C, P225, DOI DOI 10.1145/2068816.2068838
   Jin Y.-Q., 2013, Polarimetric Scattering and SAR Information Retrieval, P1
   Jin YC, 2013, IEEE INT SYMP CIRC S, P877, DOI 10.1109/ISCAS.2013.6571987
   Jin Yichao., 2013, P 21 ACM INT C MULTI, P435
   Llorca J, 2012, IEEE INFOCOM SER, P1656, DOI 10.1109/INFCOM.2012.6195536
   Lu Y, 2011, IEEE MULTIMEDIA, V18, P4, DOI 10.1109/MMUL.2011.33
   Mei T, 2010, MULTIMEDIA SYST, V16, P335, DOI 10.1007/s00530-010-0195-8
   Montpetit MJ, 2012, P IEEE, V100, P1395, DOI 10.1109/JPROC.2012.2189804
   Peng CY, 2012, IEEE INFOCOM SER, P181, DOI 10.1109/INFCOM.2012.6195556
   Shen B, 2004, IEEE T MULTIMEDIA, V6, P375, DOI 10.1109/TMM.2003.822791
   TESAURO G, 1995, COMMUN ACM, V38, P58, DOI 10.1145/203330.203343
   Tseng VS, 2006, INFORM SOFTWARE TECH, V48, P357, DOI 10.1016/j.infsof.2005.12.014
   Wang XF, 2013, IEEE T MULTIMEDIA, V15, P811, DOI 10.1109/TMM.2013.2239630
   WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698
   Wu Y, 2013, IEEE T MULTIMEDIA, V15, P821, DOI 10.1109/TMM.2013.2240670
   Yichao Jin, 2012, 2012 International Conference on Computing, Networking and Communications (ICNC), P934, DOI 10.1109/ICCNC.2012.6167562
   Yu ZW, 2010, MULTIMEDIA SYST, V16, P231, DOI 10.1007/s00530-010-0196-7
   Zhang YD, 2012, IEEE T MULTIMEDIA, V14, P510, DOI 10.1109/TMM.2012.2190391
   Zhovnirofsky I., 2013, P INT IN PRESS
NR 26
TC 31
Z9 33
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2014
VL 16
IS 6
BP 1739
EP 1751
DI 10.1109/TMM.2014.2329370
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AT1UW
UT WOS:000344720200021
DA 2024-07-18
ER

PT J
AU Sturm, BL
AF Sturm, Bob L.
TI A Simple Method to Determine if a Music Information Retrieval System is
   a "Horse"
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 2-WORK system performance; 5-CONT content description and annotation;
   5-SEAR multimedia search and retrieval
ID GENRE CLASSIFICATION; DISCRIMINATIONS
AB We propose and demonstrate a simple method to explain the figure of merit (FoM) of a music information retrieval (MIR) system evaluated in a dataset, specifically, whether the FoM comes from the system using characteristics confounded with the "ground truth" of the dataset. Akin to the controlled experiments designed to test the supposed mathematical ability of the famous horse "Clever Hans," we perform two experiments to show how three state-of-the-art MIR systems produce excellent FoM in spite of not using musical knowledge. This provides avenues for improving MIR systems, as well as their evaluation. We make available a reproducible research package so that others can apply the same method to evaluating other MIR systems.
C1 [Sturm, Bob L.] Aalborg Univ, Audio Anal Lab, AD MT, DK-2450 Copenhagen, Denmark.
C3 Aalborg University
RP Sturm, BL (corresponding author), Aalborg Univ, Audio Anal Lab, AD MT, DK-2450 Copenhagen, Denmark.
EM bst@create.aau.dk
RI Sturm, Bob/C-2613-2013
OI Sturm, Bob/0000-0003-2549-6367
FU Det Frie Forskningsrad [11-105218]
FX This work was supported in part by Independent Postdoc under Grant
   11-105218 from Det Frie Forskningsrad. The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Chong-Wah Ngo.
CR [Anonymous], 1911, CONTRIBUTION EXPT AN
   [Anonymous], 2004, Journal of negative results in speech and audio sciences
   Aucouturier J.-J., 2009, Language, Evolution, and the Brain
   Aucouturier JJ, 2003, J NEW MUSIC RES, V32, P83, DOI 10.1076/jnmr.32.1.83.16801
   AUCOUTURIER JJ, 2007, P 8 ISMIR C VIENN AU, P425
   Bailey R.A., 2008, Design of Comparative Experiments, VVolume 25
   Bertin-Mahieux T., 2010, Machine Audition : Principles, Algorithms and Systems
   Bertin-Mahieux T, 2008, J NEW MUSIC RES, V37, P115, DOI 10.1080/09298210802479250
   Chase AR, 2001, ANIM LEARN BEHAV, V29, P336, DOI 10.3758/BF03192900
   Costa YMG, 2012, SIGNAL PROCESS, V92, P2723, DOI 10.1016/j.sigpro.2012.04.023
   Craft A., 2008, THESIS U LONDON LOND
   Craft A., 2007, Proceedings of the International Conference on Music Information Retrieval, P73
   Dougherty ER, 2013, EURASIP J BIOINFORM, DOI 10.1186/1687-4153-2013-10
   Flexer A., 2007, P 8 INT C MUS INF RE, P341
   Fu ZY, 2011, IEEE T MULTIMEDIA, V13, P303, DOI 10.1109/TMM.2010.2098858
   Gouyon F., 2004, P ISMIR, P501
   Gouyon F., 2004, Proc. AES 25th International Conference, P196
   Hamming R., 1995, COMMUNICATION    JUN
   HAND DJ, 1994, J ROY STAT SOC A STA, V157, P317, DOI 10.2307/2983526
   Humphrey EJ, 2013, J INTELL INF SYST, V41, P461, DOI 10.1007/s10844-013-0248-5
   Joder C, 2009, IEEE T AUDIO SPEECH, V17, P174, DOI 10.1109/TASL.2008.2007613
   Kim Y.E., 2010, Proceedings of the 11th International Society for Music Information Retrieval Conference, P255
   Lartillot O., 2007, P 10 INT C DIG AUD E, V237, P244, DOI DOI 10.1007/978-3-540-78246-9_31
   Lesimple C., 2012, FRONT PSYCHOL, V3, P1
   Levy M, 2008, J NEW MUSIC RES, V37, P137, DOI 10.1080/09298210802479292
   Liem C., 2011, Proceedings of the 1st International ACM Workshop on Music Information Retrieval with User-Centered and Multimodal Strategies, P1, DOI DOI 10.1145/2072529.2072531
   Markov K, 2012, INT CONF ACOUST SPEE, P1929, DOI 10.1109/ICASSP.2012.6288282
   Marques G., 2011, P 12 INT SOC MUS INF, P795
   Marques G, 2011, J NEW MUSIC RES, V40, P127, DOI 10.1080/09298215.2011.573563
   Mauch M., 2013, PROC 14 INT SOC MUSI, P83
   Pampalk E., 2005, Proceedings of the International Conference on Music Information Retrieval, P628
   PORTER D, 1984, J EXP PSYCHOL-ANIM B, V10, P138
   Ren JM, 2012, IEEE T AUDIO SPEECH, V20, P1134, DOI 10.1109/TASL.2011.2172426
   Scaringella N, 2006, IEEE SIGNAL PROC MAG, V23, P133, DOI 10.1109/MSP.2006.1598089
   Schedl M, 2013, J INTELL INF SYST, V41, P523, DOI 10.1007/s10844-013-0247-6
   Serra X., 2013, Roadmap for Music Information ReSearch
   Silva Panda RenatoEduardo., 2013, 10th International Symposium on Computer Music Multidisciplinary Research (CMMR 2013), P570, DOI DOI 10.1162/NECO.1997.9.8.1735
   Song Y., 2012, ISMIR, P523
   Sturm B. L., P INT WORKS IN PRESS
   Sturm B. L., 2014, LNCS IN PRESS
   Sturm B. L., 2013, CLIN ORTHOPAEDICS RE, V1306
   Sturm B. L., POSTPR 2013 IN PRESS
   Sturm BL, 2014, J NEW MUSIC RES, V43, P147, DOI 10.1080/09298215.2014.894533
   Sturm BL, 2013, IEEE INT CONF MULTI
   Sturm BL, 2013, J INTELL INF SYST, V41, P371, DOI 10.1007/s10844-013-0250-y
   Tzanetakis G, 2002, IEEE T SPEECH AUDI P, V10, P293, DOI 10.1109/TSA.2002.800560
   Urbano J., 2013, THESIS U CARLOS 3 MA
   Urbano J, 2013, J INTELL INF SYST, V41, P345, DOI 10.1007/s10844-013-0249-4
   Watanabe S, 1998, BEHAV PROCESS, V43, P211, DOI 10.1016/S0376-6357(98)00014-X
   Wiggins GA, 2009, 2009 11TH IEEE INTERNATIONAL SYMPOSIUM ON MULTIMEDIA (ISM 2009), P477, DOI 10.1109/ISM.2009.36
   Xia G., 2012, P 11 INT C AUTONOMOU, V1, P205
NR 51
TC 57
Z9 63
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2014
VL 16
IS 6
BP 1636
EP 1644
DI 10.1109/TMM.2014.2330697
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AT1UW
UT WOS:000344720200012
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Allili, MS
   Baaziz, N
   Mejri, M
AF Allili, Mohand Said
   Baaziz, Nadia
   Mejri, Marouene
TI Texture Modeling Using Contourlets and Finite Mixtures of Generalized
   Gaussian Distributions and Applications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Fabric texture defect detection; IR face recognition; Mixture of
   Generalized Gaussian distributions (MoGG); standard/redundant contourlet
   transforms; texture retrieval
ID IMAGE
AB In this paper, we develop a new framework for contourlet- based statisticalmodeling using finiteMixtures of Generalized Gaussian distributions (MoGG). On the one hand, given the rich directional information provided by the contourlet transform (CT), we propose to use a redundant version of the CT, which describes texture structures more accurately. On the other hand, we use MoGG modeling of contourlet coefficients distribution, which allows for precise capturing of a wide range of histogram shapes and provides better description and discrimination of texture than single probability density functions (pdfs). Moreover, we propose three applications for the proposed approach, namely: (1) texture retrieval, (2) fabric texture defect detection, and 3) infrared (IR) face recognition. We compare two implementations of theCT: standard CT (SCT) and redundant CT (RCT). We show that the proposed approach yields better results in the applications studied compared to recent state-of-the-art methods.
C1 [Allili, Mohand Said; Baaziz, Nadia; Mejri, Marouene] Univ Quebec Outaouais, Gatineau, PQ J8Y 3T6, Canada.
C3 University of Quebec; University Quebec Outaouais
RP Allili, MS (corresponding author), Univ Quebec Outaouais, Gatineau, PQ J8Y 3T6, Canada.
EM mohandsaid.allili@uqo.ca; nadia.baaziz@uqo.ca; marouene.mejrii@uqo.ca
RI Allili, Mohand Said/AAB-2958-2022
FU Natural Sciences and Engineering Research Council of Canada (NSERC);
   Canada Foundation for Innovation (CFI)
FX This work was supported in part by the Natural Sciences and Engineering
   Research Council of Canada (NSERC) and in part by the Canada Foundation
   for Innovation (CFI). The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Feng Wu.
CR Allili Mohand Said, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P3143, DOI 10.1109/ICPR.2010.769
   Allili M. S., 2011, SIMILARITY MEASUREME
   Allili MS, 2012, IEEE T IMAGE PROCESS, V21, P1452, DOI 10.1109/TIP.2011.2170701
   Allili MS, 2011, LECT NOTES COMPUT SC, V6855, P446, DOI 10.1007/978-3-642-23678-5_53
   Allili MS, 2010, IEEE T CIRC SYST VID, V20, P1373, DOI 10.1109/TCSVT.2010.2077483
   Allili MS, 2008, J ELECTRON IMAGING, V17, DOI 10.1117/1.2898125
   [Anonymous], 2006, Elements of Information Theory
   [Anonymous], 2005, Statistical and Inductive Inference by Minimum Message Length
   [Anonymous], 2000, Multiresolution Signal Decomposition: Transforms, Subbands, and Wavelets
   Baaziz N, 2005, P IEEE INT C IM PROC, P221
   Bazi Y, 2005, IEEE T GEOSCI REMOTE, V43, P874, DOI 10.1109/TGRS.2004.842441
   Bhattacharjee D., 2012, COMPUTAT INTELL NEUR, V15, P1
   Boulmerka A, 2012, INT C PATT RECOG, P2894
   BURT PJ, 1983, IEEE T COMMUN, V31, P532, DOI 10.1109/TCOM.1983.1095851
   Candès EJ, 2004, COMMUN PUR APPL MATH, V57, P219, DOI 10.1002/cpa.10116
   Casella G., 2010, MONTE CARLO STAT MET
   Chang SG, 2000, IEEE T IMAGE PROCESS, V9, P1532, DOI 10.1109/83.862633
   Chen X, 2005, COMPUT VIS IMAGE UND, V99, P332, DOI 10.1016/j.cviu.2005.03.001
   Cho CS, 2005, IEEE T IND ELECTRON, V52, P1073, DOI 10.1109/TIE.2005.851648
   Choy SK, 2010, IEEE T IMAGE PROCESS, V19, P281, DOI 10.1109/TIP.2009.2033400
   Choy SK, 2007, J MATH IMAGING VIS, V29, P35, DOI 10.1007/s10851-007-0023-8
   DAUBECHIES I, 1988, COMMUN PUR APPL MATH, V41, P909, DOI 10.1002/cpa.3160410705
   DAUGMAN JG, 1988, IEEE T ACOUST SPEECH, V36, P1169, DOI 10.1109/29.1644
   Do M.N., 2003, Contourlets, Beyond Wavelets
   Do MN, 2005, IEEE T IMAGE PROCESS, V14, P2091, DOI 10.1109/TIP.2005.859376
   Do MN, 2002, IEEE T IMAGE PROCESS, V11, P146, DOI 10.1109/83.982822
   Do MN, 2002, IEEE T MULTIMEDIA, V4, P517, DOI 10.1109/TMM.2002.802019
   HE ZH, 2005, P IEEE INT C IM PROC, P513
   Hyvärinen A, 1999, IEEE T NEURAL NETWOR, V10, P626, DOI 10.1109/72.761722
   Jain A. K., 2011, HDB FACE RECOGNITION
   Kong SG, 2005, COMPUT VIS IMAGE UND, V97, P103, DOI 10.1016/j.cviu.2004.04.001
   Kumar A, 2008, IEEE T IND ELECTRON, V55, P348, DOI 10.1109/TIE.1930.896476
   Moulin P, 1999, IEEE T INFORM THEORY, V45, P909, DOI 10.1109/18.761332
   Nielsen F, 2012, INT C PATT RECOG, P1723
   Po DDY, 2006, IEEE T IMAGE PROCESS, V15, P1610, DOI 10.1109/TIP.2006.873450
   Qu HJ, 2007, LECT NOTES COMPUT SC, V4683, P493
   Randen T, 1999, IEEE T PATTERN ANAL, V21, P291, DOI 10.1109/34.761261
   Sezer OG, 2007, PATTERN RECOGN, V40, P121, DOI 10.1016/j.patcog.2006.05.023
   Wilder J, 1996, PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, P182, DOI 10.1109/AFGR.1996.557262
   Zhao W, 2003, ACM COMPUT SURV, V35, P399, DOI 10.1145/954339.954342
   Zhao Yifan, 2008, 2008 International Conference on Computer Science and Software Engineering (CSSE 2008), P221, DOI 10.1109/CSSE.2008.364
NR 41
TC 47
Z9 50
U1 1
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2014
VL 16
IS 3
BP 772
EP 784
DI 10.1109/TMM.2014.2298832
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AD3BJ
UT WOS:000333111500017
DA 2024-07-18
ER

PT J
AU Lim, H
   Park, H
AF Lim, HanShin
   Park, HyunWook
TI An Efficient Multi-View Generation Method From a Single-View Video Based
   on Affine Geometry Information
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 2D to 3D conversion; 3D display; 3D image processing; affine geometry
ID IMAGE; 3D; RECONSTRUCTION; 2D
AB Various techniques for single-view to multi-view video conversion have been developed to visualize existing 2D content on 3D displays. Because the quality of the generated multi-view video is highly dependent on the accuracy of the applied depth information, obtaining reliable depth information from a single-view sequence is an important process. This paper presents an efficient method based on 3D affine geometry information to obtain depth information and to generate horizontal parallax images from a single-view sequence. In the proposed method, positions of the feature points in each horizontal parallax image plane are estimated using a computed infinite homography. The infinite homography is then updated to be applied to subsequent frames and to maintain consistent baseline distances between the original and the generated horizontal parallax images. Experiment results show that the proposed method has advantages over previous metric-structure-from-motion-based approaches that require 3D affine geometry information and additional internal camera parameters.
C1 [Lim, HanShin; Park, HyunWook] Korea Adv Inst Sci & Technol, Dept Elect Engn, Taejon 305701, South Korea.
C3 Korea Advanced Institute of Science & Technology (KAIST)
RP Lim, H (corresponding author), Korea Adv Inst Sci & Technol, Dept Elect Engn, Taejon 305701, South Korea.
EM hwpark@kaist.ac.kr
RI Park, Hyun Wook/C-2038-2011
FU Center for Integrated Smart Sensors; Ministry of Science, ICT & Future
   Planning as Global Frontier Project [CISS-2012M3A6A6054204]; National
   Research Foundation of Korea (NRF); Korean government (MEST)
   [2012-0000125]
FX This work was supported in part by the Center for Integrated Smart
   Sensors funded by the Ministry of Science, ICT & Future Planning as
   Global Frontier Project (CISS-2012M3A6A6054204) and in part by the
   National Research Foundation of Korea (NRF) grant funded by the Korean
   government (MEST) (No. 2012-0000125). The associate editor coordinating
   the review of this manuscript and approving it for publication was Prof.
   Adrian Munteanu.
CR Burazerovic D., 2009, P ICIP
   Chaurasia G, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487238
   Faugeras O., 1993, Three-dimensional computer vision: a geometric viewpoint
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Garcia BJ, 1996, P SOC PHOTO-OPT INS, V2653, P85, DOI 10.1117/12.237420
   Harman P, 2002, PROC SPIE, V4660, P78, DOI 10.1117/12.468020
   Hartley R., 2003, MULTIPLE VIEW GEOMET
   HOIEM D, 2007, P ICCV
   Imre E, 2007, SIGNAL PROCESS-IMAGE, V22, P108, DOI 10.1016/j.image.2006.11.011
   ITU Recommendation bt.500-10, 2000, METH SUBJ ASS QUAL T
   Kim D, 2008, IEEE T BROADCAST, V54, P188, DOI 10.1109/TBC.2007.914714
   Knorr S, 2008, SIGNAL PROCESS-IMAGE, V23, P665, DOI 10.1016/j.image.2008.07.004
   Laveau S., 1994, P ICPR
   Levenberg K., 1944, Quarterly of Applied Mathematics, V2, P164, DOI [10.1090/QAM/10666, DOI 10.1090/QAM/10666]
   Lim H. S., IEEE T CIRC IN PRESS
   Liu F, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531350
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Matsumoto Y, 1997, P SOC PHOTO-OPT INS, V3012, P108, DOI 10.1117/12.274446
   Moustakas K, 2005, IEEE T CIRC SYST VID, V15, P1065, DOI 10.1109/TCSVT.2005.852401
   Nedovic V., 2007, P ICCV
   Park JH, 2003, SIGNAL PROCESS-IMAGE, V18, P401, DOI 10.1016/S0923-5965(03)00013-4
   Peleg S, 2001, IEEE T PATTERN ANAL, V23, P279, DOI 10.1109/34.910880
   Pollefeys M, 1999, IEEE T PATTERN ANAL, V21, P707, DOI 10.1109/34.784285
   Rodríguez T, 2005, MACH VISION APPL, V16, P246, DOI 10.1007/s00138-005-0179-4
   Rotem E, 2005, PROC SPIE, V5664, P198, DOI 10.1117/12.586599
   Schaffalitzky F, 2000, P ICVGIP
   Schnyder L., 2011, P ICIP
   Shreiner D., 2004, OpenGL reference manual, V4th
   Smolic A, 2011, P IEEE, V99, P607, DOI 10.1109/JPROC.2010.2098350
   Torr P. H. S., 1998, P ICCV
   Torr PHS, 1999, INT J COMPUT VISION, V32, P27, DOI 10.1023/A:1008140928553
   Torralba A, 2002, IEEE T PATTERN ANAL, V24, P1226, DOI 10.1109/TPAMI.2002.1033214
   Urey H, 2011, P IEEE, V99, P540, DOI 10.1109/JPROC.2010.2098351
   Wolberg G., 1990, Digital image warping
   Zhang ZY, 1998, INT J COMPUT VISION, V27, P161, DOI 10.1023/A:1007941100561
   Zilly F, 2011, P IEEE, V99, P590, DOI 10.1109/JPROC.2010.2095810
NR 36
TC 7
Z9 7
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2014
VL 16
IS 3
BP 726
EP 737
DI 10.1109/TMM.2014.2299771
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AD3BJ
UT WOS:000333111500013
DA 2024-07-18
ER

PT J
AU Zhang, LM
   Song, ML
   Yang, Y
   Zhao, Q
   Zhao, C
   Sebe, N
AF Zhang, Luming
   Song, Mingli
   Yang, Yi
   Zhao, Qi
   Zhao, Chen
   Sebe, Nicu
TI Weakly Supervised Photo Cropping
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bayesian network; image-level semantics; photo cropping; weakly
   supervised
ID RETRIEVAL; FEATURES; DATABASE; COLOR; SCENE; SHAPE
AB Photo cropping is widely used in the printing industry, photography, and cinematography. Conventional photo cropping methods suffer from three drawbacks: 1) the semantics used to describe photo aesthetics are determined by the experience of model designers and specific data sets, 2) image global configurations, an essential cue to capture photos aesthetics, are not well preserved in the cropped photo, and 3) multi-channel visual features from an image region contribute differently to human aesthetics, but state-of-the-art photo cropping methods cannot automatically weight them. Owing to the recent progress in image retrieval community, image-level semantics, i.e., photo labels obtained without much human supervision, can be efficiently and effectively acquired. Thus, we propose weakly supervised photo cropping, where a manifold embedding algorithm is developed to incorporate image-level semantics and image global configurations with graphlets, or, small-sized connected subgraph. After manifold embedding, a Bayesian Network (BN) is proposed. It incorporates the testing photo into the framework derived from the multi-channel post-embedding graphlets of the training data, the importance of which is determined automatically. Based on the BN, photo cropping can be casted as searching the candidate cropped photo that maximally preserves graphlets from the training photos, and the optimal cropping parameter is inferred by Gibbs sampling. Subjective evaluations demonstrate that: 1) our approach outperforms several representative photo cropping methods, including our previous cropping model that is guided by semantics-free graphlets, and 2) the visualized graphlets explicitly capture photo semantics and global spatial configurations.
C1 [Zhang, Luming; Song, Mingli] Zhejiang Univ, Coll Comp Sci, Hangzhou 310003, Zhejiang, Peoples R China.
   [Yang, Yi] Univ Queensland, Sch Informat Technol & Elect Engn, Brisbane, Qld, Australia.
   [Zhao, Qi] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117548, Singapore.
   [Zhao, Chen] Peking Univ, Sch Elect Engn & Comp Sci, Beijing 100871, Peoples R China.
   [Sebe, Nicu] Univ Trento, Dept Informat Engn & Comp Sci, Trento, Italy.
C3 Zhejiang University; University of Queensland; National University of
   Singapore; Peking University; University of Trento
RP Song, ML (corresponding author), Zhejiang Univ, Coll Comp Sci, Hangzhou 310003, Zhejiang, Peoples R China.
EM zglumg@gmail.com; brooksong@ieee.org; yi.yang@uq.edu.au;
   eleqiz@nus.edu.sg; zhaochen.pku@gmail.com; sebe@disi.unitn.it
RI zhao, qi/KGK-3760-2024; yang, yang/HGT-7999-2022; Sebe,
   Niculae/KEC-2000-2024; Yang, Yi/B-9273-2017; yang, yang/GVT-5210-2022;
   Lei, Ming/JAD-1050-2023; zhang, lu/GRO-2969-2022; yang,
   yang/GWB-9426-2022; Lang, Ming/HIK-0758-2022; Li, Guo/JNR-1700-2023
OI Sebe, Niculae/0000-0002-6597-7248; Yang, Yi/0000-0002-0512-880X; 
FU National Natural Science Foundation of China [61170142]; National Key
   Technology RD Program [2011BAG05B04]; International Science & Technology
   Cooperation Program of China [2013DFG12840]; National High Technology
   Research and Development Program of China [2013AA040601]; Fundamental
   Research Funds for the Central Universities
FX This work was supported in part by National Natural Science Foundation
   of China (61170142), National Key Technology R&D Program (2011BAG05B04),
   International Science & Technology Cooperation Program of China
   (2013DFG12840), National High Technology Research and Development
   Program of China (2013AA040601), and the Fundamental Research Funds for
   the Central Universities. The associate editor coordinating the review
   of this manuscript and approving it for publication was Dr. Chong-Wah
   Ngo.
CR [Anonymous], 2006, ADV NEURAL INF PROCE
   [Anonymous], 2010, ACM MULTIMEDIA 2010
   [Anonymous], 2010, ACM MULTIMEDIA
   [Anonymous], 1995, STORAGE RETRIEVAL IM, DOI DOI 10.1117/12.205308
   Bach F. R., 2005, P INT C MACH LEARN, P1
   Burghouts GJ, 2009, COMPUT VIS IMAGE UND, V113, P48, DOI 10.1016/j.cviu.2008.07.003
   Chen L, 2012, IEEE T MULTIMEDIA, V14, P1057, DOI 10.1109/TMM.2012.2187435
   Dalal N., 2005, PROC IEEE COMPUT SOC, V1, P886
   Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23
   Dhar S, 2011, PROC CVPR IEEE, P1657, DOI 10.1109/CVPR.2011.5995467
   Ding C., 2004, P 21 INT C MACH LEAR, P29, DOI DOI 10.1145/1015330.1015408
   Fei-Fei L, 2005, PROC CVPR IEEE, P524
   Gilks Walter R., 1995, Markov chain Monte Carlo in practice
   Harchaoui Z., 2007, IEEE C COMP VIS PATT, P1, DOI DOI 10.1109/CVPR.2007.383049
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jurie F, 2004, PROC CVPR IEEE, P90
   Ke Y., 2006, P IEEE COMP SOC C CO, V1, P419, DOI DOI 10.1109/CVPR.2006.303
   Kuo YH, 2012, IEEE T MULTIMEDIA, V14, P1079, DOI 10.1109/TMM.2012.2190386
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Li YX, 2012, IEEE T MULTIMEDIA, V14, P1618, DOI 10.1109/TMM.2012.2199292
   Luo W, 2011, IEEE I CONF COMP VIS, P2206, DOI 10.1109/ICCV.2011.6126498
   Luo YW, 2008, LECT NOTES COMPUT SC, V5304, P386
   Ma ZG, 2012, IEEE T MULTIMEDIA, V14, P1662, DOI 10.1109/TMM.2012.2199293
   Nishiyama M., 2009, P 17 ACM INT C MULTI, P669
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Platt JC, 2000, ADV NEUR IN, P61
   Santella A., 2006, Conference on Human Factors in Computing Systems. CHI2006, P771
   She J., 2007, P ACPR, P490
   Song ML, 2012, IEEE T IMAGE PROCESS, V21, P341, DOI 10.1109/TIP.2011.2157514
   Song ML, 2010, IEEE T PATTERN ANAL, V32, P1537, DOI 10.1109/TPAMI.2009.74
   Tao DC, 2009, IEEE T PATTERN ANAL, V31, P260, DOI 10.1109/TPAMI.2008.70
   Wang XC, 2011, IEEE T IMAGE PROCESS, V20, P2627, DOI 10.1109/TIP.2011.2114354
   WERMAN M, 1995, IEEE T PATTERN ANAL, V17, P810, DOI 10.1109/34.400572
   Wong LK, 2009, IEEE IMAGE PROC, P997, DOI 10.1109/ICIP.2009.5413825
   Woods R. E., 2007, DIGITAL IMAGE PROCES, V3
   Xiang SM, 2009, KNOWL INF SYST, V19, P159, DOI 10.1007/s10115-008-0161-3
   Xiong XJ, 2000, INT C PATT RECOG, P897, DOI 10.1109/ICPR.2000.903688
   Yang Y, 2008, IEEE T MULTIMEDIA, V10, P437, DOI 10.1109/TMM.2008.917359
   Yao B, 2007, LECT NOTES COMPUT SC, V4679, P169
   Yeh C.-H., 2010, Proceedings of the international conference on Multimedia - MM'10, page, P211
   Zhang LM, 2013, IEEE T IMAGE PROCESS, V22, P802, DOI 10.1109/TIP.2012.2223226
   Zhang TH, 2009, IEEE T KNOWL DATA EN, V21, P1299, DOI 10.1109/TKDE.2008.212
   Zhou X, 2010, LECT NOTES COMPUT SC, V6315, P141, DOI 10.1007/978-3-642-15555-0_11
NR 44
TC 110
Z9 112
U1 2
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2014
VL 16
IS 1
BP 94
EP 107
DI 10.1109/TMM.2013.2286817
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 279GJ
UT WOS:000328948100009
DA 2024-07-18
ER

PT J
AU Lu, WT
   Li, JX
   Li, T
   Guo, WD
   Zhang, HG
   Guo, J
AF Lu, Wenting
   Li, Jingxuan
   Li, Tao
   Guo, Weidong
   Zhang, Honggang
   Guo, Jun
TI Web Multimedia Object Classification Using Cross-Domain Correlation
   Knowledge
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bag-of-Visual-Phrases Model; Correlation Knowledge; Cross-Domain;
   Multimedia Object Classification; Transfer Learning.
AB Given a collection of web images with the corresponding textual descriptions, in this paper, we propose a novel cross-domain learning method to classify these web multimedia objects by transferring the correlation knowledge among different information sources. Here, the knowledge is extracted from unlabeled objects through unsupervised learning and applied to perform supervised classification tasks. To mine more meaningful correlation knowledge, instead of using commonly used visual words in the traditional bag-of-visual-words (BoW) model, we discover higher level visual components (words and phrases) to incorporate the spatial and semantic information into our image representation model, i.e., bag-of-visual-phrases (BoP). By combining the enriched visual components with the textual words, we calculate the frequently co-occurring pairs among them to construct a cross-domain correlated graph in which the correlation knowledge is mined. After that, we investigate two different strategies to apply such knowledge to enrich the feature space where the supervised classification is performed. By transferring such knowledge, our cross-domain transfer learning method can not only handle large scale web multimedia objects, but also deal with the situation that the textual descriptions of a small portion of web images are missing. Empirical experiments on two different datasets of web multimedia objects are conducted to demonstrate the efficacy and effectiveness of our proposed cross-domain transfer learning method.
C1 [Lu, Wenting; Guo, Weidong] Capital Univ Econ & Business, Beijing 100070, Peoples R China.
   [Li, Jingxuan; Li, Tao] Florida Int Univ, Miami, FL 33199 USA.
   [Zhang, Honggang; Guo, Jun] Beijing Univ Posts & Telecommun, Beijing 100876, Peoples R China.
C3 Capital University of Economics & Business; State University System of
   Florida; Florida International University; Beijing University of Posts &
   Telecommunications
RP Lu, WT (corresponding author), Capital Univ Econ & Business, Beijing 100070, Peoples R China.
EM luwt@cueb.edu.cn; jli003@cs.fiu.edu; taoli@cs.fiu.edu;
   guowd@cueb.edu.cn; zhhg@bupt.edu.cn; guojun@bupt.edu.cn
RI guo, ppdop/KAL-9865-2024; Guo, Jun/AAB-9166-2022
OI Guo, Jun/0000-0001-6944-0731
FU National Social Science Fund of China [11BJY075]; National Natural
   Science Foundation of China [61273217, 61175011, 61171193]; 111 project
   [B08004]; Fundamental Research Funds for the Central Universities; NSF
   [HRD-0833093, CNS-1126619]; Army Research Office [W911NF-10-1-0366,
   W911NF-12-1-0431]
FX This work was supported in part by National Social Science Fund of China
   under Grant No. 11BJY075, National Natural Science Foundation of China
   under Grant No. 61273217, 61175011 and 61171193, the 111 project under
   Grant No. B08004, and the Fundamental Research Funds for the Central
   Universities, and by NSF under grant HRD-0833093 and CNS-1126619, and
   the Army Research Office under grant W911NF-10-1-0366 and
   W911NF-12-1-0431.
CR [Anonymous], P MULT INF RETR
   [Anonymous], P BRIT MACH VIS C SE
   [Anonymous], P IEEE ICPR
   [Anonymous], 2011, 25 AAAI C ART INT
   [Anonymous], VISUAL RECOGNITION C
   [Anonymous], 2012, ACM MM'12'
   [Anonymous], ALGORITHM 457 FINDIN
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI DOI 10.1145/1961189.1961199
   Bishop C, 2007, RECOGNITION PATTERN
   Carter RJ, 2001, NUCLEIC ACIDS RES, V29, P3928, DOI 10.1093/nar/29.19.3928
   Dai W., 2008, NIPS, P353, DOI DOI 10.5555/2981780.2981825
   Dean J, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE SIXTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDE '04), P137
   Han J., 2006, DATA MINING CONCEPTS, DOI 10.1016/C2009-0-61819-5
   JORDAN MI, 1994, NEURAL COMPUT, V6, P181, DOI 10.1162/neco.1994.6.2.181
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Li H., 2008, PROC 16 ACM INT C MU, P813
   Li T, 2005, KNOWL INF SYST, V7, P289, DOI 10.1007/s10115-004-0155-8
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   McCallum A.K., 2002, MALLET MACHINE LEARN
   Raina R., 2007, P 24 INT C MACH LEAR, P759
   Raina R, 2006, P 23 INT C MACH LEAR, P713
   Sadeghi MA, 2011, PROC CVPR IEEE, P1745, DOI 10.1109/CVPR.2011.5995711
   Scholkopf B., 2001, LEARNING KERNELS SUP
   Tuytelaars T, 2010, INT J COMPUT VISION, V88, P284, DOI 10.1007/s11263-009-0271-8
   Wu LZ, 1999, IEEE T MULTIMEDIA, V1, P334, DOI 10.1109/6046.807953
   Wu Y., 2004, ACM INT C MULTIMEDIA, P572, DOI DOI 10.1145/1027527.1027665
   Yin ZJ, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P957
   Zhang J, 2009, EIGHTH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS, PROCEEDINGS, P255, DOI 10.1109/ICMLA.2009.90
   Zhang Shiliang., 2009, MM 09 P 17 ACM INT C, P75, DOI DOI 10.1145/1631272.1631285
   Zheng Li., 2010, Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD '10, P125
   Zheng Q.-F., 2006, Proceedings of the 14th annual ACM international conference on Multimedia, P77
   Zheng Y., 2008, P IEEE CVPR, P1
NR 32
TC 17
Z9 18
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2013
VL 15
IS 8
BP 1920
EP 1929
DI 10.1109/TMM.2013.2280895
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 257OI
UT WOS:000327393900016
DA 2024-07-18
ER

PT J
AU Shao, F
   Jiang, GY
   Lin, WS
   Yu, M
   Dai, QH
AF Shao, Feng
   Jiang, Gangyi
   Lin, Weishi
   Yu, Mei
   Dai, Qionghai
TI Joint Bit Allocation and Rate Control for Coding Multi-View Video Plus
   Depth Based 3D Video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bit allocation; depth-image-based rendering; rate control; view
   synthesis distortion; 3D video coding.
ID 3-D VIDEO; IMAGE; VIEW; MAPS
AB In three-dimensional (3D) video coding, distortion in texture video and depth maps can all affect the quality of the synthesized virtual views. Therefore, under the total bitrate constraint, effective bit allocation between texture and depth information is very important for 3D video coding. In this paper, the major technical contribution is to formulate view synthesis quality for optimal resource allocation in 3D video coding, since such quality is what that matters most to the ultimate user (i.e., the viewer) of the system; to be more specific, a new joint bit allocation and rate control method for multi-view video plus depth (MVD) based 3D video coding is proposed accordingly. We firstly derive a view synthesis distortion model to characterize the effect of coding distortion of texture video and depth maps on the synthesized virtual views. Based on this model, we derive a rate-distortion model to characterize the relationship between the bitrate and the view synthesis distortion, and the optimal bitrate ratio between texture and depth is established adaptively by solving the associated optimization problem. Finally, the rate control algorithm is performed on view level, texture/depth level and frame level. Experimental results show that compared with other methods, the proposed bit allocation method obtains higher performance of view synthesis. Moreover, the proposed rate control method can accurately control the bitrate to satisfy the total bitrate constraint.
C1 [Shao, Feng; Jiang, Gangyi; Yu, Mei] Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Zhejiang, Peoples R China.
   [Lin, Weishi] Nanyang Technol Univ, Sch Comp Engn, Ctr Multimedia & Network Technol, Singapore 639798, Singapore.
   [Dai, Qionghai] Tsinghua Univ, Broadband Networks & Digital Media Lab, Beijing 100084, Peoples R China.
C3 Ningbo University; Nanyang Technological University; Tsinghua University
RP Shao, F (corresponding author), Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Zhejiang, Peoples R China.
EM shaofeng@nbu.edu.cn; jianggangyi@nbu.edu.cn; wslin@ntu.edu.sg;
   yumei@nbu.edu.cn; qhdai@tsinghua.edu.cn
RI Lin, Weisi/A-8011-2012; Lin, Weisi/A-3696-2011; jiang,
   gang/KII-8233-2024; Dai, Qionghai/ABD-5298-2021
OI Lin, Weisi/0000-0001-9866-1947; Dai, Qionghai/0000-0001-7043-3061
FU Natural Science Foundation of China [60902096, 61071120, 61271021,
   61271270]; K. C. Wong Magna Fund in Ningbo University
FX This work was supported in part by the Natural Science Foundation of
   China (grants 60902096, 61071120, 61271021, and 61271270) and in part by
   K. C. Wong Magna Fund in Ningbo University.
CR [Anonymous], 2001, VCEGM33 ITUT SG16Q6
   [Anonymous], 2003, JVTG012R1 ISOIEC JTC
   [Anonymous], 2010, M18356 ISOIEC JTC1SC
   [Anonymous], 2011, P 3DTV C TRUE VIS CA
   [Anonymous], 2009, JTC1SC29WG11 ISOIEC
   [Anonymous], 2009, JVTAE207 ISOIEC MPEG
   [Anonymous], 2008, JVTZ207 ISOIEC JTC1S
   [Anonymous], P IEEE ICIP
   [Anonymous], 2010, N11274 ISOIEC JTC1SC
   Boyd S., 2004, CONVEX OPTIMIZATION
   Chan SC, 2007, IEEE SIGNAL PROC MAG, V24, P22, DOI 10.1109/MSP.2007.905702
   Cheung G, 2011, IEEE T IMAGE PROCESS, V20, P3179, DOI 10.1109/TIP.2011.2158230
   Daribo I, 2011, IEEE T BROADCAST, V57, P533, DOI 10.1109/TBC.2011.2125110
   Ekmekcioglu E, 2011, IEEE J-STSP, V5, P352, DOI 10.1109/JSTSP.2010.2052783
   Fehn C, 2006, P IEEE, V94, P524, DOI 10.1109/JPROC.2006.870688
   Fehn C, 2004, PROC SPIE, V5291, P93, DOI 10.1117/12.524762
   *ISO IEC, 1997, JTC1SC29WG11 ISOIEC
   *ISO IEC, 1993, JTC1SC29WG11 ISOIEC
   *ITU T, 1997, VID COD TEST MOD NEA
   Kamolrat B, 2008, IEEE T CONSUM ELECTR, V54, P887, DOI 10.1109/TCE.2008.4560175
   Kim WS, 2009, IEEE IMAGE PROC, P721, DOI 10.1109/ICIP.2009.5414304
   Kurc M., 2012, P PICT COD S POZN PO
   Lee HJ, 2000, IEEE T CIRC SYST VID, V10, P878, DOI 10.1109/76.867926
   Lee JY, 2011, IEEE IMAGE PROC, P929, DOI 10.1109/ICIP.2011.6116712
   Liu YW, 2011, IEEE T BROADCAST, V57, P562, DOI 10.1109/TBC.2011.2105652
   Liu YW, 2009, SIGNAL PROCESS-IMAGE, V24, P666, DOI 10.1016/j.image.2009.06.002
   Ma SW, 2005, IEEE T CIRC SYST VID, V15, P1533, DOI 10.1109/TCSVT.2005.857300
   Merkle P., 2007, P IEEE INT C IM PROC
   Morvan Y., 2007, P PICT COD S PCS 200, V10, P43
   Oh BT, 2011, IEEE J-STSP, V5, P1344, DOI 10.1109/JSTSP.2011.2164893
   Oh KJ, 2011, IEEE T CIRC SYST VID, V21, P350, DOI 10.1109/TCSVT.2011.2116590
   Ou TS, 2010, PROC SPIE, V7744, DOI 10.1117/12.863266
   Redert A, 2007, THIRD INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P429
   Ruiz-Hidalgo J, 2012, J VIS COMMUN IMAGE R, V23, P42, DOI 10.1016/j.jvcir.2011.08.001
   Shao F, 2012, IEEE T MULTIMEDIA, V14, P157, DOI 10.1109/TMM.2011.2169045
   Shao F, 2011, OPT ENG, V50, DOI 10.1117/1.3665436
   Smolic A, 2011, P IEEE, V99, P607, DOI 10.1109/JPROC.2010.2098350
   Tanimoto M, 2006, SIGNAL PROCESS-IMAGE, V21, P454, DOI 10.1016/j.image.2006.03.009
   Tech G., 2012, P PICT COD S KRAK PO
   Wang QF, 2012, IEEE T CIRC SYST VID, V22, P875, DOI 10.1109/TCSVT.2011.2181229
   Worrall S. T., 2010, P NEM SUMM FUT MED I
   Yan T, 2011, IMAGING SCI J, V59, P202, DOI 10.1179/136821910X12863757400402
   Yuan H., IEEE T BROA IN PRESS
   Yuan H, 2011, IEEE T CIRC SYST VID, V21, P485, DOI 10.1109/TCSVT.2011.2125610
   Zhang J, 2010, IEEE IMAGE PROC, P2865, DOI 10.1109/ICIP.2010.5651934
NR 45
TC 54
Z9 60
U1 0
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2013
VL 15
IS 8
BP 1843
EP 1854
DI 10.1109/TMM.2013.2269897
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 257OI
UT WOS:000327393900010
DA 2024-07-18
ER

PT J
AU Wu, CH
   Lin, JC
   Wei, WL
AF Wu, Chung-Hsien
   Lin, Jen-Chun
   Wei, Wen-Li
TI Two-Level Hierarchical Alignment for Semi-Coupled HMM-Based Audiovisual
   Emotion Recognition With Temporal Course
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Emotion recognition; semi-coupled hidden Markov model (SC-HMM); temporal
   course.
ID SPEECH; PARAMETERS; ANIMATION; STATES; AUDIO; MODEL; FACE
AB A complete emotional expression typically contains a complex temporal course in face-to-face natural conversation. To address this problem, a bimodal hidden Markov model (HMM)-based emotion recognition scheme, constructed in terms of sub-emotional states, which are defined to represent temporal phases of onset, apex, and offset, is adopted to model the temporal course of an emotional expression for audio and visual signal streams. A two-level hierarchical alignment mechanism is proposed to align the relationship within and between the temporal phases in the audio and visual HMM sequences at the model and state levels in a proposed semi-coupled hidden Markov model (SC-HMM). Furthermore, by integrating a sub-emotion language model, which considers the temporal transition between sub-emotional states, the proposed two-level hierarchical alignment-based SC-HMM (2H-SC-HMM) can provide a constraint on allowable temporal structures to determine an optimal emotional state. Experimental results show that the proposed approach can yield satisfactory results in both the posed MHMC and the naturalistic SEMAINE databases, and shows that modeling the complex temporal structure is useful to improve the emotion recognition performance, especially for the naturalistic database (i.e., natural conversation). The experimental results also confirm that the proposed 2H-SC-HMM can achieve an acceptable performance for the systems with sparse training data or noisy conditions.
C1 [Wu, Chung-Hsien; Lin, Jen-Chun; Wei, Wen-Li] Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, Tainan 70101, Taiwan.
C3 National Cheng Kung University
RP Wu, CH (corresponding author), Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, Tainan 70101, Taiwan.
EM chunghsienwu@gmail.com; jenchunlin@gmail.com; lilijinjin@gmail.com
RI Wu, Chung-Hsien/E-7970-2013; Wei, Wen-Li/AAQ-3848-2021; Lin,
   Jen-Chun/AAQ-3701-2021
OI Wu, Chung-Hsien/0000-0002-3947-2123; Wei, Wen-Li/0000-0002-6753-2824;
   Lin, Jen-Chun/0000-0002-9237-4119
FU National Science Council [NSC101-2221-E-006-252-MY3]; Headquarters of
   University Advancement at the National Cheng Kung University; Ministry
   of Education, Taiwan, R.O.C.
FX This work was supported in part by the National Science Council, under
   Contract NSC101-2221-E-006-252-MY3 and the Headquarters of University
   Advancement at the National Cheng Kung University, which is sponsored by
   the Ministry of Education, Taiwan, R.O.C.
CR [Anonymous], 2011, ICDECOM
   [Anonymous], 2008, LREC WORKSH CORP RES
   Boersma P., 2007, PRAAT: doing phonetics by computer
   Bulut M., 2004, P 6 INT C MULT INT, P205
   Busso C, 2011, INT CONF ACOUST SPEE, P5692
   Chen CW, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P3133, DOI 10.1109/IROS.2008.4650788
   Cohen I, 2003, COMPUT VIS IMAGE UND, V91, P160, DOI 10.1016/S1077-3142(03)00081-X
   Cooper HM., 2009, HDB RES SYNTHESIS ME, V2nd edn
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Cowie R, 2003, SPEECH COMMUN, V40, P5, DOI 10.1016/S0167-6393(02)00071-7
   Ekman P, 1978, FACIAL ACTION CODING
   Ekman P., 1999, HDB COGNITION EMOTIO
   FLEISS JL, 1971, PSYCHOL BULL, V76, P378, DOI 10.1037/h0031619
   Fragopanagos N., 2005, NEURAL NETWORKS, V18, P570
   International Organization for Standardization, 1998, N2502 ISOIET JTC1SC2
   Karpouzis K, 2007, LECT NOTES COMPUT SC, V4451, P91
   Kwon O.W., 8 EUROPEAN C SPEECH
   LANDIS JR, 1977, BIOMETRICS, V33, P159, DOI 10.2307/2529310
   LANE H, 1971, J SPEECH HEAR RES, V14, P677, DOI 10.1044/jshr.1404.677
   Lee CM, 2005, IEEE T SPEECH AUDI P, V13, P293, DOI 10.1109/TSA.2004.838534
   Lin JC, 2012, IEEE T MULTIMEDIA, V14, P142, DOI 10.1109/TMM.2011.2171334
   Luengo I., 2005, INTERSPEECH 2005, P493
   Mana N., 2007, P INT C AUD VIS SPEE
   McKeown G, 2010, IEEE INT CON MULTI, P1079, DOI 10.1109/ICME.2010.5583006
   Metallinou A, 2012, IEEE T AFFECT COMPUT, V3, P184, DOI 10.1109/T-AFFC.2011.40
   Metallinou A, 2010, INT CONF ACOUST SPEE, P2462, DOI 10.1109/ICASSP.2010.5494890
   Metallinou A, 2008, IEEE INT SYM MULTIM, P250, DOI 10.1109/ISM.2008.40
   Morrison D, 2007, SPEECH COMMUN, V49, P98, DOI 10.1016/j.specom.2006.11.004
   MURRAY IR, 1993, J ACOUST SOC AM, V93, P1097, DOI 10.1121/1.405558
   Ntalampiras S, 2012, IEEE T AFFECT COMPUT, V3, P116, DOI 10.1109/T-AFFC.2011.31
   Picard R. W., 1997, AFFECTIVE COMPUTING
   Raouzaiou A, 2002, EURASIP J APPL SIG P, V2002, P1021, DOI 10.1155/S1110865702206149
   RUSSELL JA, 1980, J PERS SOC PSYCHOL, V39, P1161, DOI 10.1037/h0077714
   Scherer KR, 2003, SPEECH COMMUN, V40, P227, DOI 10.1016/S0167-6393(02)00084-5
   Schuller B, 2003, INT CONF ACOUST SPEE, P1
   Schuller B, 2007, ICMI'07: PROCEEDINGS OF THE NINTH INTERNATIONAL CONFERENCE ON MULTIMODAL INTERFACES, P30
   Sebe N, 2006, INT C PATT RECOG, P1136
   Song ML, 2008, NEUROCOMPUTING, V71, P1913, DOI 10.1016/j.neucom.2007.07.041
   Tao H, 1999, IEEE T CIRC SYST VID, V9, P264, DOI 10.1109/76.752094
   Tekalp AM, 2000, SIGNAL PROCESS-IMAGE, V15, P387, DOI 10.1016/S0923-5965(99)00055-7
   Tian YI, 2001, IEEE T PATTERN ANAL, V23, P97, DOI 10.1109/34.908962
   Toothaker L.E., 1992, Multiple comparison procedures
   Valstar M. F., 2006, P INT C COMP VIS PAT, V3
   Valstar MF, 2012, IEEE T SYST MAN CY B, V42, P28, DOI 10.1109/TSMCB.2011.2163710
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wagner J, 2011, IEEE T AFFECT COMPUT, V2, P206, DOI 10.1109/T-AFFC.2011.12
   Wang YJ, 2008, IEEE T MULTIMEDIA, V10, P936, DOI 10.1109/TMM.2008.927665
   Wu C.H., 2006, ACM Trans. Asian Lang. Inf. Process. (TALIP), V5, P165, DOI DOI 10.1145/1165255.1165259
   Wu CH, 2011, IEEE T AFFECT COMPUT, V2, P10, DOI 10.1109/T-AFFC.2010.16
   Wu CH, 2009, AFFECTIVE INFORMATION PROCESSING, P93, DOI 10.1007/978-1-84800-306-4_6
   Zeng Z., 2008, IEEE T MULTIMEDIA, V10, P389
   Zeng ZH, 2005, 2005 IEEE International Conference on Multimedia and Expo (ICME), Vols 1 and 2, P828
   Zeng ZH, 2009, IEEE T PATTERN ANAL, V31, P39, DOI 10.1109/TPAMI.2008.52
   Zeng ZH, 2007, IEEE T MULTIMEDIA, V9, P424, DOI 10.1109/TMM.2006.886310
NR 54
TC 43
Z9 46
U1 3
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2013
VL 15
IS 8
BP 1880
EP 1895
DI 10.1109/TMM.2013.2269314
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 257OI
UT WOS:000327393900013
DA 2024-07-18
ER

PT J
AU Yeh, HH
   Yang, CY
   Lee, MS
   Chen, CS
AF Yeh, Hsin-Ho
   Yang, Chun-Yu
   Lee, Ming-Sui
   Chen, Chu-Song
TI Video Aesthetic Quality Assessment by Temporal Integration of Photo-and
   Motion-Based Features
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Aesthetic quality assessment; computational aesthetics; image quality
   assessment; video quality assessment; visual aesthetics.
ID IMAGES
AB This paper presents a new method for accessing the aesthetic quality of videos. It consists of two processes: aesthetic features construction and temporal integration. First, our method combines both photo-based and motion-based visual clues to extract the aesthetic features for each frame in a video. We introduce new motion-based features built from optical flow and salient region extraction, and show their effectiveness to enhance the estimation of aesthetic values. Then, a temporal-order-aware framework that integrates the frame-based features is presented to further improve the evaluation accuracy by taking the time-varying properties into consideration. The experimental results demonstrate that our approach can accomplish remarkable improvement for aesthetic quality assessment of videos.
C1 [Yeh, Hsin-Ho; Yang, Chun-Yu; Chen, Chu-Song] Acad Sinica, Inst Informat Sci, Taipei, Taiwan.
   [Lee, Ming-Sui] Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 10764, Taiwan.
   [Chen, Chu-Song] Acad Sinica, Res Ctr Informat Technol Innovat, Taipei 115, Taiwan.
C3 Academia Sinica - Taiwan; National Taiwan University; Academia Sinica -
   Taiwan
RP Yeh, HH (corresponding author), Acad Sinica, Inst Informat Sci, Taipei, Taiwan.
EM hhyeh@iis.sinica.edu.tw; cyyang@iis.sinica.edu.tw;
   mslee@csie.ntu.edu.tw; song@iis.sinica.edu.tw
FU National Science Council of Taiwan [NSC101-2221-E001-015-MY2]
FX This work was supported in part by the National Science Council of
   Taiwan under Grant NSC101-2221-E001-015-MY2.
CR [Anonymous], 2009, P 17 ACM INT C MULT
   [Anonymous], 2010, P 18 ACM INT C MULTI
   Battiato S, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P373, DOI 10.1109/ICME.2008.4607449
   Beerends JG, 1999, J AUDIO ENG SOC, V47, P355
   Cao Z., 2007, P 24 INT C MACHINE L, P129, DOI DOI 10.1145/1273496.1273513
   Cohen-Or D, 2006, ACM T GRAPHIC, V25, P624, DOI 10.1145/1141911.1141933
   Datta R., 2006, P ECCV, P7
   Dhar S, 2011, PROC CVPR IEEE, P1657, DOI 10.1109/CVPR.2011.5995467
   Joshi D, 2011, IEEE SIGNAL PROC MAG, V28, P94, DOI 10.1109/MSP.2011.941851
   Kawabata H, 2004, J NEUROPHYSIOL, V91, P1699, DOI 10.1152/jn.00696.2003
   Ke Y., 2006, P IEEE COMP SOC C CO, V1, P419, DOI DOI 10.1109/CVPR.2006.303
   Krages B., 2005, Photography: the art of composition
   Li CC, 2009, IEEE J-STSP, V3, P236, DOI 10.1109/JSTSP.2009.2015077
   Li J, 2004, IEEE T IMAGE PROCESS, V13, P338, DOI 10.1109/TIP.2003.821349
   LIU C., 2009, Ph.D. Thesis
   Luo W, 2011, IEEE I CONF COMP VIS, P2206, DOI 10.1109/ICCV.2011.6126498
   Luo YW, 2008, LECT NOTES COMPUT SC, V5304, P386
   Ma Y.F., 2003, P 11 ACM INT C MULT, P374, DOI DOI 10.1145/957013.957094
   Mahadevan V, 2010, IEEE T PATTERN ANAL, V32, P171, DOI 10.1109/TPAMI.2009.112
   Mei T, 2007, IEEE T CIRC SYST VID, V17, P699, DOI 10.1109/TCSVT.2007.896640
   Moorthy AK, 2010, LECT NOTES COMPUT SC, V6315, P1, DOI 10.1007/978-3-642-15555-0_1
   Nishiyama M, 2011, PROC CVPR IEEE, P33, DOI 10.1109/CVPR.2011.5995539
   O'Donovan P, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964958
   Peters G, 2007, IEEE INT CONF INF VI, P316
   Pinson M, 2003, PROC SPIE, V5150, P573, DOI 10.1117/12.509908
   Rahtu E, 2010, LECT NOTES COMPUT SC, V6315, P366, DOI 10.1007/978-3-642-15555-0_27
   Ramachandran V. S., 1999, Journal of Consciousness Studies, V6, P15
   Saini M.K., 2012, Proceedings of the 20th International Conference on Multimedia, P139, DOI DOI 10.1145/2393347.2393373
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P1427, DOI 10.1109/TIP.2010.2042111
   Sheikh HR, 2005, IEEE T IMAGE PROCESS, V14, P1918, DOI 10.1109/TIP.2005.854492
   Yang JL, 2009, IEEE T CIRC SYST VID, V19, P945, DOI 10.1109/TCSVT.2009.2020252
   Yeh C.-H., 2010, Proceedings of the international conference on Multimedia - MM'10, page, P211
   You J., 2009, Proceedings of the 17th ACM International Conference on Multimedia (ACM MM), P561
NR 33
TC 30
Z9 30
U1 1
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2013
VL 15
IS 8
BP 1944
EP 1957
DI 10.1109/TMM.2013.2280250
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 257OI
UT WOS:000327393900018
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Tiakas, E
   Rafailidis, D
   Dimou, A
   Daras, P
AF Tiakas, Eleftherios
   Rafailidis, Dimitrios
   Dimou, Anastasios
   Daras, Petros
TI MSIDX: Multi-Sort Indexing for Efficient Content-Based Image Search and
   Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Approximate similarity search; content-based image retrieval; indexing;
   multi-sort
ID DIMENSIONALITY REDUCTION; SCALE; TREES; QUANTIZATION; SPACES
AB In this paper, a novel approximate indexing scheme for efficient content-based image search and retrieval is presented, called Multi-Sort Indexing (MSIDX). The proposed scheme analyzes high dimensional image descriptor vectors, by employing the value cardinalities of their dimensions. The dimensions' value cardinalities, an inherent characteristic of descriptor vectors, are the number of discrete values in the dimensions. As expected, value cardinalities significantly vary, due to the existence of several extraction methods. Moreover, different quantization and normalization techniques used in the extraction process have a strong impact on the dimensions' value cardinalities. Since dimensions with high value cardinalities have more discriminative power, a multiple sort algorithm is used to reorder the descriptors' dimensions according to their value cardinalities, in order to increase the probability of two similar images to lie within a close constant range. The expected bounds of the constant range are defined in detail, following deterministic and probabilistic analyses. The proposed scheme is fully suitable (a) for real-time indexing of images, and (b) for searching and retrieving relevant images with an efficient query processing algorithm. In our experiments with five real datasets, we show the superiority of the proposed approach against hashing methods, also suitable for approximate similarity search.
C1 [Tiakas, Eleftherios; Rafailidis, Dimitrios; Dimou, Anastasios; Daras, Petros] Ctr Res & Technol Hellas, Inst Informat Technol, Thessaloniki, Greece.
C3 Centre for Research & Technology Hellas
RP Tiakas, E (corresponding author), Ctr Res & Technol Hellas, Inst Informat Technol, Thessaloniki, Greece.
EM tiakas@iti.gr; drafail@iti.gr; dimou@iti.gr; daras@iti.gr
RI Dimou, Anastasios/ABE-3335-2022; Daras, Petros/F-5284-2012
OI Daras, Petros/0000-0003-3814-6710; Rafailidis,
   Dimitrios/0000-0002-7366-3716
FU EC [ICT- 287704]
FX Manuscript received August 29, 2012; revised November 28, 2012; accepted
   December 06, 2012. Date of publication February 20, 2013; date of
   current version September 13, 2013. This work was supported by the EC
   FP7 funded project CUBRIK, ICT- 287704 (http://www.cubrikproject.eu).
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Qibin Sun.
CR Amsaleg L., 2010, TEXMEX DATASETS APPR
   [Anonymous], 2008, Proceedings of the 21st International Conference on Neural Information Processing Systems
   [Anonymous], 2007, MIR
   [Anonymous], 2009, NEURIPS
   Balasubramanian M, 2002, SCIENCE, V295
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007
   BENTLEY JL, 1979, IEEE T SOFTWARE ENG, V5, P333, DOI 10.1109/TSE.1979.234200
   Beyer K, 1999, LECT NOTES COMPUT SC, V1540, P217
   Böhm C, 2001, ACM COMPUT SURV, V33, P322, DOI 10.1145/502807.502809
   Chatzichristofis SA, 2008, LECT NOTES COMPUT SC, V5008, P312
   Chávez E, 2001, ACM COMPUT SURV, V33, P273, DOI 10.1145/502807.502808
   CHAVEZ E, 1999, EUR WORKSH CONT BAS, P57
   Chiueh Tzi-cker., 1994, VLDB 94, P582
   Chum Ondrej., 2008, BMVC, P1
   Ciaccia P, 1997, PROCEEDINGS OF THE TWENTY-THIRD INTERNATIONAL CONFERENCE ON VERY LARGE DATABASES, P426
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   DEHNE F, 1987, INFORM SYST, V12, P171, DOI 10.1016/0306-4379(87)90041-X
   DVORETZKY A, 1956, ANN MATH STAT, V27, P642, DOI 10.1214/aoms/1177728174
   Friedman J. H., 1977, ACM Transactions on Mathematical Software, V3, P209, DOI 10.1145/355744.355745
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Gong Y., IEEE T PATTERN ANAL
   Gong YC, 2011, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2011.5995432
   Guttman A., 1984, SIGMOD Record, V14, P47, DOI 10.1145/971697.602266
   He JF, 2011, PROC CVPR IEEE, P753, DOI 10.1109/CVPR.2011.5995518
   He X., 2003, Locality Preserving Projections
   Heo JP, 2012, PROC CVPR IEEE, P2957, DOI 10.1109/CVPR.2012.6248024
   Huang Z., 2011, SIGMOD, P1021
   Huang Z., 2008, Multimedia, P219
   Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Johnson W.B., 1984, CONTEMP MATH-SINGAP, V26, P189, DOI DOI 10.1090/CONM/026/737400
   Joly A, 2004, IEEE IMAGE PROC, P681
   Joly A, 2011, PROC CVPR IEEE, P873, DOI 10.1109/CVPR.2011.5995709
   Joly Alexis., 2008, PROCEEDING 16 ACM IN, P209
   Kulis B, 2009, IEEE I CONF COMP VIS, P2130, DOI 10.1109/ICCV.2009.5459466
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lv Q., 2007, P 33 INT C VER LARG, P950
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Navarro G, 2002, VLDB J, V11, P28, DOI 10.1007/s007780200060
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Panigrahy R, 2006, PROCEEDINGS OF THE SEVENTHEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1186, DOI 10.1145/1109557.1109688
   Pauleve L, 2010, PATTERN RECOGN LETT, V31, P1348, DOI 10.1016/j.patrec.2010.04.004
   Poullot S., 2007, Proceedings of the ACM International Conference on Image and Video Retrieval, P348
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   SAMET H, 1984, COMPUT SURV, V16, P187, DOI 10.1145/356924.356930
   Samet H, 2006, FDN MULTIDIMENSIONAL
   Shakhnarovich G., 2006, NEAREST NEIGHBOR MET
   Shen HT, 2007, VLDB J, V16, P219, DOI 10.1007/s00778-005-0167-3
   Sivic J, 2006, LECT NOTES COMPUT SC, V4170, P127
   Song J., 2011, P 19 ACM INT C MULT, P423, DOI DOI 10.1145/2072298.2072354
   Stehling RO, 2002, P CIKM
   van de Sande KEA, 2010, IEEE T PATTERN ANAL, V32, P1582, DOI 10.1109/TPAMI.2009.154
   Vidal Ruiz E., 1986, Pattern Recognition Letters, V4, P145, DOI 10.1016/0167-8655(86)90013-9
   Wang J, 2010, PROC CVPR IEEE, P3424, DOI 10.1109/CVPR.2010.5539994
   Weber R., 1998, Proceedings of the Twenty-Fourth International Conference on Very-Large Databases, P194
   Wichterich Marc., 2008, SIGMOD '08: Proceedings of the 2008 ACM SIGMOD international conference on Management of data, P199
NR 59
TC 21
Z9 22
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2013
VL 15
IS 6
BP 1415
EP 1430
DI 10.1109/TMM.2013.2247989
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 222XH
UT WOS:000324765400017
DA 2024-07-18
ER

PT J
AU Yang, YH
   Liu, JY
AF Yang, Yi-Hsuan
   Liu, Jen-Yu
TI Quantitative Study of Music Listening Behavior in a Social and Affective
   Context
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Affective computing; music emotion recognition; music listening
   behavior; social media; user mood recognition
ID PERSONALITY; RECOGNITION; EMOTIONS; LIFE
AB A scientific understanding of emotion experience requires information on the contexts in which the emotion is induced. Moreover, as one of the primary functions of music is to regulate the listener's mood, the individual's short-term music preference may reveal the emotional state of the individual. In light of these observations, this paper presents the first scientific study that exploits the online repository of social data to investigate the connections between a blogger's emotional state, user context manifested in the blog articles, and the content of the music titles the blogger attached to the post. A number of computational models are developed to evaluate the accuracy of different content or context cues in predicting emotional state, using 40,000 pieces of music listening records collected from the social blogging website LiveJournal. Our study shows that it is feasible to computationally model the latent structure underlying music listening and mood regulation. The average area under the receiver operating characteristic curve (AUC) for the content-based and context-based models attains 0.5462 and 0.6851, respectively. The association among user mood, music emotion, and individual's personality is also identified.
C1 [Yang, Yi-Hsuan; Liu, Jen-Yu] Acad Sinica, Res Ctr Informat Technol Innovat, Taipei 11564, Taiwan.
C3 Academia Sinica - Taiwan
RP Yang, YH (corresponding author), Acad Sinica, Res Ctr Informat Technol Innovat, Taipei 11564, Taiwan.
EM yang@citi.sinica.edu.tw; ciaua@citi.sinica.edu.tw
RI 江, 鈺麒/G-1379-2014
OI Liu, Jen-Yu/0000-0003-1299-6688
FU National Science Council of Taiwan [NSC 101-2221-E-001-017]; Academia
   Sinica Career Development Program
FX Manuscript received September 08, 2012; revised January 31, 2013 and
   April 11, 2013; accepted April 25, 2013. Date of publication May 29,
   2013; date of current version September 13, 2013. This work was
   supported by a grant from the National Science Council of Taiwan under
   the contract NSC 101-2221-E-001-017 and the Academia Sinica Career
   Development Program. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Ioannis
   Kompatsiaris.
CR Aizawa A, 2003, INFORM PROCESS MANAG, V39, P45, DOI 10.1016/S0306-4573(02)00021-3
   [Anonymous], 2011, P 12 INT C MUS INF R
   [Anonymous], 2010, 11 INT SOC MUS INF R
   [Anonymous], 2012, ACM T INTEL SYST TEC, DOI DOI 10.1145/2168752.2168754
   [Anonymous], 2012, P INT S COMP MUS MOD
   [Anonymous], 2012, ISMIR
   Bolger N, 2003, ANNU REV PSYCHOL, V54, P579, DOI 10.1146/annurev.psych.54.101601.145030
   Bradley M.M., 1999, C1 U FLOR CTR RES PS
   BRECKLER SJ, 1985, MUSIC PERCEPT, V2, P459
   Cabrera D., 2007, P 13 INT C AUDITORY, P356
   Calvo RA, 2010, IEEE T AFFECT COMPUT, V1, P18, DOI 10.1109/T-AFFC.2010.1
   Chamorro-Premuzic T, 2009, PSYCHOL AESTHET CREA, V3, P149, DOI 10.1037/a0015342
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Clayton M., 2008, OXFORD HDB MUSIC PSY
   COLTHEART M, 1981, Q J EXP PSYCHOL-A, V33, P497, DOI 10.1080/14640748108400805
   Cortes C, 2004, ADV NEUR IN, V16, P313
   Elliott G.T., 2006, CHI 06, P736, DOI DOI 10.1145/1125451.1125599
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Gabrielsson A, 2001, MUSIC SCI, V5, P123, DOI 10.1177/10298649020050S105
   GOLDBERG LR, 1990, J PERS SOC PSYCHOL, V59, P1216, DOI 10.1037/0022-3514.59.6.1216
   Hargreaves D.J., 1999, Psychology of Music, V27, P71
   Hofmann T, 1999, SIGIR'99: PROCEEDINGS OF 22ND INTERNATIONAL CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P50, DOI 10.1145/312624.312649
   Hu X., 2010, THESIS U ILLINOIS UR
   Hunter PG, 2011, EMOTION, V11, P1068, DOI 10.1037/a0023749
   Järvelin K, 2002, ACM T INFORM SYST, V20, P422, DOI 10.1145/582415.582418
   Juslin PN, 2004, J NEW MUSIC RES, V33, P217, DOI 10.1080/0929821042000317813
   Kaminskas M, 2012, COMPUT SCI REV, V6, P89, DOI 10.1016/j.cosrev.2012.04.002
   Kim J, 2008, IEEE T PATTERN ANAL, V30, P2067, DOI 10.1109/TPAMI.2008.26
   Koelstra S, 2012, IEEE T AFFECT COMPUT, V3, P18, DOI 10.1109/T-AFFC.2011.15
   Ladinig O, 2012, PSYCHOL AESTHET CREA, V6, P146, DOI 10.1037/a0024671
   Lan M, 2009, IEEE T PATTERN ANAL, V31, P721, DOI 10.1109/TPAMI.2008.110
   Lartillot O., 2007, P 10 INT C DIG AUD E, V237, P244, DOI DOI 10.1007/978-3-540-78246-9_31
   Laurier C., 2011, THESIS
   Leshed Gilly., 2006, CHI'06 extended abstracts on Human factors in computing systems, P1019, DOI [10.1145/1125451.1125646, DOI 10.1145/1125451.1125646]
   Lin YC, 2011, ACM T MULTIM COMPUT, V7, DOI 10.1145/2037676.2037683
   Liu XY, 2009, IEEE T SYST MAN CY B, V39, P539, DOI 10.1109/TSMCB.2008.2007853
   Lonsdale AJ, 2011, BRIT J PSYCHOL, V102, P108, DOI 10.1348/000712610X506831
   Lu L, 2006, IEEE T AUDIO SPEECH, V14, P5, DOI 10.1109/TSA.2005.860344
   Mairesse F, 2007, J ARTIF INTELL RES, V30, P457, DOI 10.1613/jair.2349
   Miura A, 2007, J COMPUT-MEDIAT COMM, V12, P1452, DOI 10.1111/j.1083-6101.2007.00381.x
   Montgomery D.C., 1998, ENG STAT
   Müller M, 2011, IEEE J-STSP, V5, P1088, DOI 10.1109/JSTSP.2011.2112333
   Nardi BA, 2004, COMMUN ACM, V47, P41, DOI 10.1145/1035134.1035163
   Nathans LL, 2012, Pract. Assess. Res. Eval, V17, P9, DOI [10.7275/5fex-b874., DOI 10.7275/5FEX-B874]
   Nicolaou MA, 2011, IEEE T AFFECT COMPUT, V2, P92, DOI 10.1109/T-AFFC.2011.9
   Pennebaker J.W., 2001, Linguistic inquiry and word count: LIWC 2001, V71, P1
   Picard R. W., 1997, AFFECTIVE COMPUTING
   Rentfrow PJ, 2011, J PERS SOC PSYCHOL, V100, P1139, DOI 10.1037/a0022406
   Rentfrow PJ, 2003, J PERS SOC PSYCHOL, V84, P1236, DOI 10.1037/0022-3514.84.6.1236
   Robertson S., 1995, 4 TEXT RETRIEVAL C T, P73
   RUSSELL JA, 1980, J PERS SOC PSYCHOL, V39, P1161, DOI 10.1037/h0077714
   Salimpoor VN, 2011, NAT NEUROSCI, V14, P257, DOI 10.1038/nn.2726
   Schwartz KD, 2003, J YOUTH ADOLESCENCE, V32, P205, DOI 10.1023/A:1022547520656
   Sebastiani F, 2002, ACM COMPUT SURV, V34, P1, DOI 10.1145/505282.505283
   Seeger Matthias, 2004, Int J Neural Syst, V14, P69, DOI 10.1142/S0129065704001899
   Tang J, 2012, IEEE T AFFECT COMPUT, V3, P132, DOI 10.1109/T-AFFC.2011.23
   Tingle D., 2010, P ISMIR, P55
   Dang TT, 2009, INT CONF KNOWL SYS, P144, DOI 10.1109/KSE.2009.10
   Turney PD, 2003, ACM T INFORM SYST, V21, P315, DOI 10.1145/944012.944013
   van den Tol A.J.M., 2012, THESIS U LIMERICK LI
   van Goethem A, 2011, MUSIC SCI, V15, P208, DOI 10.1177/1029864911401174
   Wang J.-C., 2012, P 20 ACM INT C MULTI, P89, DOI DOI 10.1145/2393347.2396494
   Watson D., 2012, P SOUND MUS COMP C, P11
   Yang YH, 2011, MULTIMEDIA COMPUT CO, P1
   Yang YH, 2011, IEEE T AUDIO SPEECH, V19, P762, DOI 10.1109/TASL.2010.2064164
   Yarkoni T, 2010, J RES PERS, V44, P363, DOI 10.1016/j.jrp.2010.04.001
   Zeng ZH, 2009, IEEE T PATTERN ANAL, V31, P39, DOI 10.1109/TPAMI.2008.52
   Zentner M, 2008, EMOTION, V8, P494, DOI 10.1037/1528-3542.8.4.494
NR 68
TC 33
Z9 35
U1 0
U2 40
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2013
VL 15
IS 6
BP 1304
EP 1315
DI 10.1109/TMM.2013.2265078
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA 222XH
UT WOS:000324765400008
DA 2024-07-18
ER

PT J
AU Liao, R
   Zhang, L
   Sun, Y
   Miao, S
   Chefd'Hotel, C
AF Liao, Rui
   Zhang, Li
   Sun, Ying
   Miao, Shun
   Chefd'Hotel, Christophe
TI A Review of Recent Advances in Registration Techniques Applied to
   Minimally Invasive Therapy
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Biomedical imaging; image fusion; minimally and less invasive
   procedures; projection-to-volume registration; slice-to-volume
   registration; video-to-volume registration; volume-to-volume
   registration
ID X-RAY FLUOROSCOPY; IMAGE-GUIDED INTERVENTIONS; AORTIC-VALVE
   IMPLANTATION; 2D/3D REGISTRATION; 2D-3D REGISTRATION; AUTOMATIC
   REGISTRATION; NONRIGID REGISTRATION; COMPUTED-TOMOGRAPHY; SIMILARITY
   MEASURES; CATHETER ABLATION
AB Minimally invasive and less invasive procedure is becoming more and more common in medical therapy. Image guidance is an indispensable component in minimally invasive procedures by providing critical information about the position of the target sites and the optimal manipulation of the devices, while the field of view is limited to naked eyes due to the small incision. Registration is one of the enabling technologies for computer-aided image guidance, which brings high-resolution pre-operative data into the operating room to provide more realistic information about the patient's anatomy. In this paper, we survey the recent advances in registration techniques applied to minimally and/or less invasive therapy, including a wide variety of therapies in surgery, endoscopy, interventional cardiology, interventional radiology, and hybrid procedures. The registration approaches are categorized into several groups, including projection-to-volume, slice-to-volume, video-to-volume, and volume-to-volume registration. The focus is on recent advances in registration techniques that are specifically developed for minimally and/or less invasive procedures in the following medical specialties: neuroradiology and neurosurgery, cardiac applications, and thoracic-abdominal interventions.
C1 [Liao, Rui; Zhang, Li; Miao, Shun; Chefd'Hotel, Christophe] Siemens Corp, Imaging & Comp Vis, Corp Technol, Princeton, NJ 08540 USA.
   [Sun, Ying] Natl Univ Singapore, Elect & Comp Engn Dept, Singapore 117548, Singapore.
C3 Siemens AG; National University of Singapore
RP Liao, R (corresponding author), Siemens Corp, Imaging & Comp Vis, Corp Technol, Princeton, NJ 08540 USA.
EM rui.liao@siemens.com; lizhang@siemens.com; elesuny@nus.edu.sg;
   shun.miao@siemens.com; christophe.chefdHotel@siemens.com
RI Miao, Shun/K-5734-2015; 廖, 廖睿/HKW-7050-2023
OI Sun, Ying/0000-0002-7224-6726
CR Adamus R, 2009, RADIOLOGY, V251, P543, DOI 10.1148/radiol.2512080423
   Almasslawi DMS, 2011, PROC SPIE, V8009, DOI 10.1117/12.896428
   [Anonymous], 2008, Advances in automatic differentiation, DOI [DOI 10.1007/978-3-540-68942-323, 10.1007/978-3-540-68942-3_23, DOI 10.1007/978-3-540-68942-3_23]
   [Anonymous], 2001, Medical image registration
   Aouadi S, 2008, COMPUT VIS IMAGE UND, V110, P134, DOI 10.1016/j.cviu.2007.05.006
   Archip N, 2007, NEUROIMAGE, V35, P609, DOI 10.1016/j.neuroimage.2006.11.060
   Ashburner J, 2011, NEUROIMAGE, V55, P954, DOI 10.1016/j.neuroimage.2010.12.049
   Baim D. S., 2005, CARDIAC CATHETERIZAT
   Balachandran R, 2009, IEEE T BIO-MED ENG, V56, P37, DOI 10.1109/TBME.2008.2002110
   Barnes J, 2010, NEURORADIOLOGY, V52, P987, DOI 10.1007/s00234-010-0665-x
   Baumhauer M, 2008, J ENDOUROL, V22, P751, DOI 10.1089/end.2007.9827
   Benincasa AB, 2008, MED PHYS, V35, P4251, DOI 10.1118/1.2969064
   Bériault S, 2011, LECT NOTES COMPUT SC, V6891, P259, DOI 10.1007/978-3-642-23623-5_33
   Bernstein M, 2000, NEUROSURGERY, V46, P900, DOI 10.1097/00006123-200004000-00023
   Birkfellner W, 2007, MED PHYS, V34, P246, DOI 10.1118/1.2401661
   Birkfellner W, 2009, MED PHYS, V36, P3420, DOI 10.1118/1.3157111
   Brock KK, 2005, MED PHYS, V32, P1647, DOI 10.1118/1.1915012
   Brost A, 2010, MED IMAGE ANAL, V14, P695, DOI 10.1016/j.media.2010.05.006
   Brost A, 2012, IEEE T MED IMAGING, V31, P870, DOI 10.1109/TMI.2011.2181184
   Brost A, 2009, LECT NOTES COMPUT SC, V5761, P394, DOI 10.1007/978-3-642-04268-3_49
   Brounstein A, 2011, LECT NOTES COMPUT SC, V6891, P235, DOI 10.1007/978-3-642-23623-5_30
   Burschka D, 2005, MED IMAGE ANAL, V9, P413, DOI 10.1016/j.media.2005.05.005
   Calkins H, 2012, EUROPACE, V14, P528, DOI 10.1093/europace/eus027
   Cao A, 2008, MED PHYS, V35, P1593, DOI 10.1118/1.2870216
   Cappato R, 2005, CIRCULATION, V111, P1100, DOI 10.1161/01.CIR.0000157153.30978.67
   Carrell TWG, 2010, J ENDOVASC THER, V17, P527, DOI 10.1583/09-2987MR.1
   Cash DM, 2007, J GASTROINTEST SURG, V11, P844, DOI 10.1007/s11605-007-0090-6
   Chelikani S, 2006, INT J RADIAT ONCOL, V65, P535, DOI 10.1016/j.ijrobp.2005.12.032
   Chen T, 2009, LECT NOTES COMPUT SC, V5761, P43, DOI 10.1007/978-3-642-04268-3_6
   Chen X, 2008, PHYS MED BIOL, V53, P967, DOI 10.1088/0031-9155/53/4/010
   Chen XY, 2006, CAMB ST SOC, P3, DOI 10.1017/CBO9780511499739.001
   Chen Y.-W., 2007, 2007 IEEE/ICME International Conference on Complex Medical Engineering, P1784
   Chintalapani G., P A MICCAI WORKSH 20, P151
   Chou CR, 2010, ADV INTEL SOFT COMPU, V83, P83
   Christensen GE, 1996, IEEE T IMAGE PROCESS, V5, P1435, DOI 10.1109/83.536892
   Cleary K, 2010, ANNU REV BIOMED ENG, V12, P119, DOI 10.1146/annurev-bioeng-070909-105249
   Davatzikos C, 1996, IEEE T MED IMAGING, V15, P785, DOI 10.1109/42.544496
   de Silva R, 2006, CIRCULATION, V114, P2342, DOI 10.1161/CIRCULATIONAHA.105.598524
   Deguchi D., 2011, INT J COMPUT ASSIST, P1
   Deguchi D, 2009, MED IMAGE ANAL, V13, P621, DOI 10.1016/j.media.2009.06.001
   Del Bue A, 2009, IEEE IMAGE PROC, P1061, DOI 10.1109/ICIP.2009.5413705
   Deligianni F, 2006, IEEE T MED IMAGING, V25, P1462, DOI 10.1109/TMI.2006.883452
   Dong S, 2008, LECT NOTES COMPUT SC, V5242, P964, DOI 10.1007/978-3-540-85990-1_116
   Dorgham O., 2010, International Journal of Computer Assisted Radiology and Surgery, V5, pS68
   Duong L., 2009, P SPIE 2009, V7261
   Eberhardt R, 2007, CHEST, V131, P1800, DOI 10.1378/chest.06-3016
   Ector J, 2008, HEART RHYTHM, V5, P957, DOI 10.1016/j.hrthm.2008.03.024
   Elhawary H, 2010, ACAD RADIOL, V17, P1334, DOI 10.1016/j.acra.2010.06.004
   Elhawary H, 2009, LECT NOTES COMPUT SC, V5761, P837, DOI 10.1007/978-3-642-04268-3_103
   Fischer B, 2003, J MATH IMAGING VIS, V18, P81, DOI 10.1023/A:1021897212261
   Fitzpatrick JM, 1998, IEEE T MED IMAGING, V17, P694, DOI 10.1109/42.736021
   Gao G., 2008, P SPIE 2008, V6918
   Gao G, 2012, MED IMAGE ANAL, V16, P38, DOI 10.1016/j.media.2011.05.003
   Gao G, 2010, LECT NOTES COMPUT SC, V6135, P124
   Gorges S., 2006, P MICCAI WORKSH AMI
   Grbic S, 2011, LECT NOTES COMPUT SC, V6891, P219, DOI 10.1007/978-3-642-23623-5_28
   Greene WH, 2008, LECT NOTES COMPUT SC, V5241, P780, DOI 10.1007/978-3-540-85988-8_93
   Greenhalgh RM, 2004, LANCET, V364, P843, DOI 10.1016/S0140-6736(04)16979-1
   Groher M, 2007, LECT NOTES COMPUT SC, V4792, P527
   Groher M, 2007, ACAD RADIOL, V14, P1325, DOI 10.1016/j.acra.2007.07.009
   Groher M, 2009, IEEE T MED IMAGING, V28, P847, DOI 10.1109/TMI.2008.2011519
   Guo Ting, 2006, Comput Aided Surg, V11, P231, DOI 10.1080/10929080600997232
   Gutierrez L. F., 2009, CATHETERIZ CARDIOVAS, V70, P773
   Han X., 2010, MICCAI 2010 CHALLENG
   Hawkes DJ, 2005, MED IMAGE ANAL, V9, P163, DOI 10.1016/j.media.2004.11.007
   Helferty JP, 2007, COMPUT VIS IMAGE UND, V108, P171, DOI 10.1016/j.cviu.2006.10.010
   Hemm S, 2010, MED BIOL ENG COMPUT, V48, P611, DOI 10.1007/s11517-010-0633-y
   Hentschke C. M., 2010, CEUR WORKSHOP P, V574, P162
   Higgins WE, 2008, COMPUT MED IMAG GRAP, V32, P159, DOI 10.1016/j.compmedimag.2007.11.001
   Hoffman B. L., 2012, MINIMALLY INVASIVE S
   Hu YP, 2008, LECT NOTES COMPUT SC, V5241, P737
   Huang XS, 2009, IEEE T MED IMAGING, V28, P1802, DOI 10.1109/TMI.2009.2024684
   Huang XS, 2009, IEEE T MED IMAGING, V28, P1179, DOI 10.1109/TMI.2008.2011557
   Hugo GD, 2010, PHYS MED BIOL, V55, P2637, DOI 10.1088/0031-9155/55/9/014
   Hummel J, 2008, PHYS MED BIOL, V53, P4303, DOI 10.1088/0031-9155/53/16/006
   Hurvitz A, 2008, INT J COMPUT ASS RAD, V3, P493, DOI 10.1007/s11548-008-0264-z
   John M, 2010, LECT NOTES COMPUT SC, V6361, P375
   Jomier J, 2006, LECT NOTES COMPUT SC, V4191, P662
   Khamene A, 2006, MED IMAGE ANAL, V10, P96, DOI 10.1016/j.media.2005.06.002
   Khare R, 2012, PROC SPIE, V8316, DOI 10.1117/12.910911
   Kim J, 2007, TECHNOL CANCER RES T, V6, P337, DOI 10.1177/153303460700600411
   Knecht S, 2008, EUROPACE, V10, P931, DOI 10.1093/europace/eun145
   Kubias A, 2007, LECT NOTES COMPUT SC, V4673, P759
   Lansberg MG, 2011, STROKE, V42, P1608, DOI 10.1161/STROKEAHA.110.609008
   Lee D, 2011, PHYS MED BIOL, V56, P117, DOI 10.1088/0031-9155/56/1/008
   Lei P., 2007, SPIE 2007
   Leon MB, 2010, NEW ENGL J MED, V363, P1597, DOI 10.1056/NEJMoa1008232
   Leven J, 2005, LECT NOTES COMPUT SC, V3749, P811
   Li G., 2010, P EUR C RAD ECR 2010
   Li G., 2011, P INT S MR MED ISMRM, V1187
   Li W., 2009, MED DOSIM
   Li XL, 2006, PHYS MED BIOL, V51, P2745, DOI 10.1088/0031-9155/51/11/004
   Liang P., LNCS, V6326, P344
   Liao HG, 2008, LECT NOTES COMPUT SC, V5242, P373, DOI 10.1007/978-3-540-85990-1_45
   Liao R., 2008, P SPIE 2008, V6918
   Liao R, 2010, LECT NOTES COMPUT SC, V6326, P561
   Liao R, 2010, I S BIOMED IMAGING, P1213, DOI 10.1109/ISBI.2010.5490213
   Linte CA, 2007, LECT NOTES COMPUT SC, V4792, P94
   Linte CA, 2009, INT J COMPUT ASS RAD, V4, P113, DOI 10.1007/s11548-008-0278-6
   Livyatan H, 2003, IEEE T MED IMAGING, V22, P1395, DOI 10.1109/TMI.2003.819288
   Lv X., 2012, P IEEE ISBI 2012
   Ma YL, 2009, PHYS MED BIOL, V54, P5039, DOI 10.1088/0031-9155/54/16/013
   Ma YL, 2012, IEEE T BIO-MED ENG, V59, P122, DOI 10.1109/TBME.2011.2168393
   Mäkelä T, 2002, IEEE T MED IMAGING, V21, P1011, DOI 10.1109/TMI.2002.804441
   Manstad-Hulaas F, 2007, EUR SURG RES, V39, P364, DOI 10.1159/000106512
   Markelj P, 2012, MED IMAGE ANAL, V16, P642, DOI 10.1016/j.media.2010.03.005
   Markelj P, 2008, IEEE T MED IMAGING, V27, P1704, DOI 10.1109/TMI.2008.923984
   Metz C., 2011, MICCAI WORKSH AUG EN
   Miao S., 2011, P IEEE CISP BMEI 201, P550
   Miao S., 2012, P SPIE 2012, V8316
   Miao S, 2011, I S BIOMED IMAGING, P1215, DOI 10.1109/ISBI.2011.5872620
   Milko S, 2009, LECT NOTES COMPUT SC, V5761, P771, DOI 10.1007/978-3-642-04268-3_95
   Mirota DJ, 2012, IEEE T MED IMAGING, V31, P963, DOI 10.1109/TMI.2011.2176500
   Mitrovic U., P SPIE 2012, V8314
   Mitrovic U., 2011, P SPIE 2011, V7962
   Mori K, 2005, LECT NOTES COMPUT SC, V3750, P543, DOI 10.1007/11566489_67
   Munbodh R, 2009, MED PHYS, V36, P4555, DOI 10.1118/1.3213531
   Murphy K, 2011, IEEE T MED IMAGING, V30, P1901, DOI 10.1109/TMI.2011.2158349
   Naini AS, 2010, IEEE T BIO-MED ENG, V57, P2627, DOI 10.1109/TBME.2010.2058110
   Nakamoto M, 2007, LECT NOTES COMPUT SC, V4792, P68
   Nam WH, 2010, I S BIOMED IMAGING, P1201, DOI 10.1109/ISBI.2010.5490210
   Nicolau SA, 2009, MED IMAGE ANAL, V13, P494, DOI 10.1016/j.media.2009.02.003
   Niculescu G., 2007, EMBS 2007
   Nimsky C, 2006, NEUROIMAGE, V30, P1219, DOI 10.1016/j.neuroimage.2005.11.001
   Nithiananthan S, 2011, PROC SPIE, V7964, DOI 10.1117/12.878258
   Noachtar S, 2009, EPILEPSY BEHAV, V15, P66, DOI 10.1016/j.yebeh.2009.02.028
   O'Shea JP, 2006, INT J MED ROBOT COMP, V2, P75, DOI 10.1002/rcs.82
   Olesch J., 2009, SPIE 2009, V72610G
   Paul P, 2009, IEEE T INF TECHNOL B, V13, P976, DOI 10.1109/TITB.2009.2025373
   Penney GP, 1998, IEEE T MED IMAGING, V17, P586, DOI 10.1109/42.730403
   Peters TM, 2009, LECT NOTES COMPUT SC, V5528, P466, DOI 10.1007/978-3-642-01932-6_50
   Pickering MR, 2009, IEEE ENG MED BIO, P5821, DOI 10.1109/IEMBS.2009.5335172
   Prummer M., 2006, P SPIE 2006, V6144
   Rai L, 2008, INT J COMPUT ASS RAD, V3, P315, DOI 10.1007/s11548-008-0241-6
   Rauth TP, 2007, SURGERY, V142, P207, DOI 10.1016/j.surg.2007.04.016
   Reichl T, 2011, LECT NOTES COMPUT SC, V6891, P17, DOI 10.1007/978-3-642-23623-5_3
   Reinertsen I, 2007, MED IMAGE ANAL, V11, P374, DOI 10.1016/j.media.2007.04.002
   Rhode K., 2008, P SPIE 2008, V6918
   Rickers Carsten, 2004, J Interv Cardiol, V17, P37, DOI 10.1111/j.1540-8183.2004.01712.x
   Rodriguez-Porcel M, 2005, MOL THER, V12, P1142, DOI 10.1016/j.ymthe.2005.07.532
   Rossitti S, 2009, INTERV NEURORADIOL, V15, P283, DOI 10.1177/159101990901500305
   ROSSLE M, 1994, NEW ENGL J MED, V330, P165, DOI 10.1056/NEJM199401203300303
   Rougee A., P SPIE 1993, V1897, P161
   Ruijters D., 2007, P SPIE 2007, V6509
   Ruijters D, 2009, INT J COMPUT ASS RAD, V4, P391, DOI 10.1007/s11548-009-0316-z
   Sadowsky O, 2007, LECT NOTES COMPUT SC, V4792, P519
   Säring D, 2012, NEURORADIOLOGY, V54, P171, DOI 10.1007/s00234-011-0836-4
   Saw CB, 2008, MED DOSIM, V33, P149, DOI 10.1016/j.meddos.2008.03.001
   Schaller C, 2009, LECT NOTES COMPUT SC, V5761, P549, DOI 10.1007/978-3-642-04268-3_68
   Schneider M, 2010, PROC CVPR IEEE, P2948, DOI 10.1109/CVPR.2010.5540038
   Schwarz Y, 2006, CHEST, V129, P988, DOI 10.1378/chest.129.4.988
   Shamir RR, 2011, MED IMAGE ANAL, V15, P85, DOI 10.1016/j.media.2010.07.010
   Slomka PJ, 2009, EUR J NUCL MED MOL I, V36, P44, DOI 10.1007/s00259-008-0941-8
   Solomon SB, 1998, CHEST, V114, P1405, DOI 10.1378/chest.114.5.1405
   Songyuan Tang, 2010, 2010 International Conference of Medical Image Analysis and Clinical Application (MIACA), P113, DOI 10.1109/MIACA.2010.5528505
   Spaun G O., 2009, Gastrointest Endosc, V69, pe39e45
   Spiclin Z, 2008, LECT NOTES COMPUT SC, V5241, P762, DOI 10.1007/978-3-540-85988-8_91
   Spoerk J, 2007, MED PHYS, V34, P4302, DOI 10.1118/1.2789500
   Sra J, 2007, J CARDIOVASC ELECTR, V18, P409, DOI 10.1111/j.1540-8167.2006.00734.x
   Strauer B E, 2009, Minerva Cardioangiol, V57, P249
   Subramanian N, 2009, INT J COMPUT ASS RAD, V4, P141, DOI 10.1007/s11548-008-0279-5
   Sun YY, 2007, LECT NOTES COMPUT SC, V4791, P60
   Sundar H., 2006, P SPIE 2006, V6141
   Swanström L, 2009, SURG INNOV, V16, P104, DOI 10.1177/1553350609334344
   Thirion J P, 1998, Med Image Anal, V2, P243, DOI 10.1016/S1361-8415(98)80022-4
   Timinger H, 2005, PHYS MED BIOL, V50, P491, DOI 10.1088/0031-9155/50/3/007
   Tomazevic D, 2006, IEEE T MED IMAGING, V25, P17, DOI 10.1109/TMI.2005.859715
   van der Bom MJ, 2010, MED PHYS, V37, P1884, DOI 10.1118/1.3366252
   Vermandel M, 2006, CELL MOL BIOL, V52, P44, DOI 10.1170/T737
   Wang X, 2011, RADIOTHER ONCOL, V99, P148, DOI 10.1016/j.radonc.2011.05.020
   Wein WG, 2008, MED IMAGE ANAL, V12, P577, DOI 10.1016/j.media.2008.06.006
   Wein W, 2009, LECT NOTES COMPUT SC, V5761, P9, DOI 10.1007/978-3-642-04268-3_2
   Wiebers D, 2003, LANCET, V362, P103, DOI 10.1016/S0140-6736(03)13860-3
   Wilson K, 2006, LECT NOTES COMPUT SC, V4191, P520
   Wilson K, 2008, LECT NOTES COMPUT SC, V5241, P967, DOI 10.1007/978-3-540-85988-8_115
   Wu JZ, 2009, INT J RADIAT ONCOL, V75, P268, DOI 10.1016/j.ijrobp.2009.03.008
   Xu D, 2010, IEEE ENG MED BIO, P3715, DOI 10.1109/IEMBS.2010.5627657
   Xue Z, 2010, COMPUT MED IMAG GRAP, V34, P55, DOI 10.1016/j.compmedimag.2009.05.007
   You W, 2011, I S BIOMED IMAGING, P702, DOI 10.1109/ISBI.2011.5872503
   Younes L, 2009, NEUROIMAGE, V45, pS40, DOI 10.1016/j.neuroimage.2008.10.050
   Yuen SG, 2008, LECT NOTES COMPUT SC, V5241, P711, DOI 10.1007/978-3-540-85988-8_85
   Zhang L, 2010, I S BIOMED IMAGING, P832, DOI 10.1109/ISBI.2010.5490115
   Zheng GY, 2007, LECT NOTES COMPUT SC, V4791, P834
   Zheng GY, 2006, LECT NOTES COMPUT SC, V4190, P25
   Zheng GY, 2006, LECT NOTES COMPUT SC, V4057, P186
   Zhong H, 2006, LECT NOTES COMPUT SC, V4190, P437
   Zhuang XH, 2011, IEEE T MED IMAGING, V30, P1819, DOI 10.1109/TMI.2011.2150240
   Zikic D., 2010, P SOC PHOTO-OPT INS, V7623
NR 188
TC 48
Z9 55
U1 1
U2 35
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2013
VL 15
IS 5
BP 983
EP 1000
DI 10.1109/TMM.2013.2244869
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 222WT
UT WOS:000324763600003
DA 2024-07-18
ER

PT J
AU Zhang, B
   Liu, SQ
   Cao, X
   Liu, F
   Wang, X
   Luo, JW
   Shan, BC
   Bai, J
AF Zhang, Bin
   Liu, Shuangquan
   Cao, Xu
   Liu, Fei
   Wang, Xin
   Luo, Jianwen
   Shan, Baoci
   Bai, Jing
TI Fluorescence Tomography Reconstruction With Simultaneous Positron
   Emission Tomography Priors
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Fluorescence tomography; multi-modality imaging; positron emission
   tomography
ID OPTICAL TOMOGRAPHY; MOLECULAR TOMOGRAPHY; SYSTEM; PERFORMANCE; LIGHT;
   PET
AB In this paper, fluorescence molecular tomography (FMT) imaging guided by priors from simultaneous positron emission tomography (PET) was performed on a multi-modality imaging system combining PET and FMT. The target prior information from PET images was employed to the FMT reconstruction procedure using the iteratively reweighted least-squares method. Numerical simulations and phantom experiments were performed to validate the proposed method. The results indicate that incorporating the PET prior information into the FMT reconstruction can potentially improve the spatial resolution of FMT.
C1 [Zhang, Bin; Cao, Xu; Liu, Fei; Wang, Xin; Luo, Jianwen; Bai, Jing] Tsinghua Univ, Dept Biomed Engn, Beijing 100084, Peoples R China.
   [Liu, Shuangquan; Shan, Baoci] Chinese Acad Sci, Key Lab Nucl Analyt Tech, Inst High Energy Phys, Beijing 100049, Peoples R China.
   [Luo, Jianwen] Tsinghua Univ, Ctr Biomed Imaging Res, Beijing 100084, Peoples R China.
C3 Tsinghua University; Chinese Academy of Sciences; Institute of High
   Energy Physics, CAS; Tsinghua University
RP Zhang, B (corresponding author), Tsinghua Univ, Dept Biomed Engn, Beijing 100084, Peoples R China.
EM shanbc@ihep.ac.cn; deabj@tsinghua.edu.cn
RI Cao, Xu/M-9944-2013; Wang, Xin/GYU-1129-2022; Luo, Jianwen/D-5612-2011;
   bai, jing/IUP-9367-2023
OI Luo, Jianwen/0000-0001-9215-5568; Shan, Baoci/0000-0001-7417-5063; Liu,
   Shuangquan/0000-0001-8354-5927
FU National Basic Research Program of China (973) [2011CB707701]; National
   Natural Science Foundation of China [81071191, 81271617]; National Major
   Scientific Instrument and Equipment Development Project [2011YQ030114];
   National Science and technology support program [2012BAI23B00]
FX This work was supported in part by the National Basic Research Program
   of China (973) under Grant No. 2011CB707701; in part by the National
   Natural Science Foundation of China under Grant No. 81071191, 81271617;
   in part by the National Major Scientific Instrument and Equipment
   Development Project under Grant No. 2011YQ030114; in part by National
   Science and technology support program under Grant No. 2012BAI23B00. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Martin McKeown.
CR Ale A, 2010, MED PHYS, V37, P1976, DOI 10.1118/1.3368603
   ARRIDGE SR, 1993, MED PHYS, V20, P299, DOI 10.1118/1.597069
   Arridge SR, 1999, INVERSE PROBL, V15, pR41, DOI 10.1088/0266-5611/15/2/022
   Baritaux JC, 2011, IEEE T MED IMAGING, V30, P1143, DOI 10.1109/TMI.2011.2136438
   Chaudhari AJ, 2005, PHYS MED BIOL, V50, P5421, DOI 10.1088/0031-9155/50/23/001
   Daubechies I, 2010, COMMUN PUR APPL MATH, V63, P1, DOI 10.1002/cpa.20303
   Davis SC, 2008, REV SCI INSTRUM, V79, DOI 10.1063/1.2919131
   Freyer M, 2010, J BIOMED OPT, V15, DOI 10.1117/1.3431101
   Haller J, 2008, J APPL PHYSIOL, V104, P795, DOI 10.1152/japplphysiol.00959.2007
   Hyde D, 2010, IEEE T MED IMAGING, V29, P365, DOI 10.1109/TMI.2009.2031112
   Hyde D, 2009, J OPT SOC AM A, V26, P919, DOI 10.1364/JOSAA.26.000919
   Hyde D, 2009, NEUROIMAGE, V44, P1304, DOI 10.1016/j.neuroimage.2008.10.038
   Kepshire D, 2009, REV SCI INSTRUM, V80, DOI 10.1063/1.3109903
   Konecky SD, 2008, OPT EXPRESS, V16, P5048, DOI 10.1364/OE.16.005048
   Li CQ, 2009, OPT LETT, V34, P2933, DOI 10.1364/OL.34.002933
   Lin Y, 2007, PHYS MED BIOL, V52, P5569, DOI 10.1088/0031-9155/52/18/007
   Liu SQ, 2011, IEEE T NUCL SCI, V58, P51, DOI 10.1109/TNS.2010.2068310
   Miosso CJ, 2009, IEEE T SIGNAL PROCES, V57, P2424, DOI 10.1109/TSP.2009.2016889
   Nahrendorf M, 2010, P NATL ACAD SCI USA, V107, P7910, DOI 10.1073/pnas.0915163107
   Ntziachristos V, 2005, NAT BIOTECHNOL, V23, P313, DOI 10.1038/nbt1074
   Ntziachristos V, 2002, NAT MED, V8, P757, DOI 10.1038/nm729
   OLeary MA, 1996, OPT LETT, V21, P158, DOI 10.1364/OL.21.000158
   Peter J, 2007, IEEE T NUCL SCI, V54, P1553, DOI 10.1109/TNS.2007.902359
   SCHWEIGER M, 1995, MED PHYS, V22, P1779, DOI 10.1118/1.597634
   Soubret A, 2005, IEEE T MED IMAGING, V24, P1377, DOI 10.1109/TMI.2005.857213
   Stuker F, 2011, IEEE T MED IMAGING, V30, P1265, DOI 10.1109/TMI.2011.2112669
   Zacharakis G, 2005, P NATL ACAD SCI USA, V102, P18252, DOI 10.1073/pnas.0504628102
   Zhang B, 2011, J BIOMED OPT, V16, DOI 10.1117/1.3665438
   Zhu L, 2012, THERANOSTICS, V2, P746, DOI 10.7150/thno.4762
NR 29
TC 8
Z9 9
U1 1
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2013
VL 15
IS 5
BP 1031
EP 1038
DI 10.1109/TMM.2013.2244205
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 222WT
UT WOS:000324763600007
DA 2024-07-18
ER

PT J
AU Lai, CF
   Wang, HG
   Chao, HC
   Nan, GF
AF Lai, Chin-Feng
   Wang, Honggang
   Chao, Han-Chieh
   Nan, Guofang
TI A Network and Device Aware QoS Approach for Cloud-Based Mobile Streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptive Qos; cloud multimedia; network and device-aware
ID ARCHITECTURE; SVC
AB Cloud multimedia services provide an efficient, flexible, and scalable data processing method and offer a solution for the user demands of high quality and diversified multimedia. As intelligent mobile phones and wireless networks become more and more popular, network services for users are no longer limited to the home. Multimedia information can be obtained easily using mobile devices, allowing users to enjoy ubiquitous network services. Considering the limited bandwidth available for mobile streaming and different device requirements, this study presented a network and device-aware Quality of Service (QoS) approach that provides multimedia data suitable for a terminal unit environment via interactive mobile streaming services, further considering the overall network environment and adjusting the interactive transmission frequency and the dynamic multimedia transcoding, to avoid the waste of bandwidth and terminal power. Finally, this study realized a prototype of this architecture to validate the feasibility of the proposed method. According to the experiment, this method could provide efficient self-adaptive multimedia streaming services for varying bandwidth environments.
C1 [Lai, Chin-Feng] Natl Chung Cheng Univ, Dept Comp Sci & Informat Engn, Chiayi, Taiwan.
   [Wang, Honggang] Univ Massachusetts Dartmouth, Dept Elect & Comp Engn, N Dartmouth, MA 02747 USA.
   [Chao, Han-Chieh] Natl ILan Univ, Inst Comp Sci & Informat Engn, Ilan, Taiwan.
   [Nan, Guofang] Tianjin Univ, Inst Syst Engn, Tianjin 300072, Peoples R China.
   [Nan, Guofang] Polytech Univ, Dept Elect, Turin, Italy.
C3 National Chung Cheng University; University of Massachusetts System;
   University Massachusetts Dartmouth; National Ilan University; Tianjin
   University; Polytechnic University of Turin
RP Lai, CF (corresponding author), Natl Chung Cheng Univ, Dept Comp Sci & Informat Engn, Chiayi, Taiwan.
EM cinfon@ieee.org; hwang1@umassd.edu; hcc@niu.edu.tw; gfnan@tju.edu.cn
RI Lai, Chin-Feng/IAP-5353-2023; Wang, Honggang/D-6079-2013
OI Lai, Chin-Feng/0000-0001-7138-0272; Wang, Honggang/0000-0001-9475-2630
FU National Science Council [NSC 101-2628-E-197-001-MY3,
   101-2221-E-197-008-MY3, 101-2219-E-197-004, 101MG07-2]; Science Park
   Administration of the Republic of China, Taiwan
FX This work was supported by the National Science Council and Science Park
   Administration of the Republic of China, Taiwan under Contract NSC
   101-2628-E-197-001-MY3, 101-2221-E-197-008-MY3, 101-2219-E-197-004 and
   101MG07-2. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Chang Wen Chen.
CR [Anonymous], 2001, P 7 ANN INT C MOBILE
   [Anonymous], 2011, INT J COMPUT SCI COM
   Armbrust M, 2010, COMMUN ACM, V53, P50, DOI 10.1145/1721654.1721672
   Chang SY, 2012, COMPUT COMMUN, V35, P1798, DOI 10.1016/j.comcom.2012.06.001
   Chen P, 2007, 9th International Conference on Advanced Communication Technology: Toward Network Innovation Beyond Evolution, Vols 1-3, P955, DOI 10.1109/ICACT.2007.358517
   Choi HC, 2007, IEEE T CONSUM ELECTR, V53, P384, DOI 10.1109/TCE.2007.381705
   Díaz-Sánchez D, 2011, IEEE T CONSUM ELECTR, V57, P970, DOI 10.1109/TCE.2011.5955247
   Ferretti S., 2010, 2010 IEEE 3rd International Conference on Cloud Computing (CLOUD 2010), P548, DOI 10.1109/CLOUD.2010.16
   Garcia A, 2011, IEEE INTERNATIONAL CONFERENCE ON CONSUMER ELECTRONICS (ICCE 2011), P379, DOI 10.1109/ICCE.2011.5722637
   Hang Yuan, 2010, 2010 International Conference on Green Computing (Green Comp), P375, DOI 10.1109/GREENCOMP.2010.5598292
   Hu GQ, 2012, IEEE NETWORK, V26, P21, DOI 10.1109/MNET.2012.6201212
   Huang ZX, 2011, IEEE INFOCOM SER, P201, DOI 10.1109/INFCOM.2011.5935009
   Junqiu Feng, 2010, 2010 International Conference on High Performance Computing & Simulation (HPCS 2010), P203, DOI 10.1109/HPCS.2010.5547135
   Lai C. F., 2010, IEEE COMSOC MMTC E L, V5
   Lai CF, 2011, IEEE SYST J, V5, P555, DOI 10.1109/JSYST.2011.2165190
   Lai YK, 2011, J DISP TECHNOL, V7, P550, DOI 10.1109/JDT.2011.2162314
   Lee H, 2009, IEEE T CONSUM ELECTR, V55, P1682, DOI 10.1109/TCE.2009.5278043
   Lin Y, 2006, IEEE T WIREL COMMUN, V5, P1971, DOI 10.1109/TWC.2006.03381
   Liu Y, 2009, 2009 WRI INTERNATIONAL CONFERENCE ON COMMUNICATIONS AND MOBILE COMPUTING: CMC 2009, VOL 3, P285, DOI 10.1109/CMC.2009.124
   Mingfeng Tan, 2011, Proceedings of the 2011 IEEE 6th International Symposium on Service Oriented System Engineering (SOSE 2011), P251, DOI 10.1109/SOSE.2011.6139114
   Pei Fan, 2011, Proceedings of the 2011 IEEE 4th International Conference on Cloud Computing (CLOUD 2011), P460, DOI 10.1109/CLOUD.2011.54
   Psannis KE, 2007, Q2SWINET'07: PROCEEDINGS OF THE THIRD ACM WORKSHOP ON Q2S AND SECURITY FOR WIRELESS AND MOBILE NETWORKS, P168
   Ramos N., 2007, P IEEE INT S PERS IN, P1
   Saranya S. M., 2011, 2011 International Conference on Recent Trends in Information Technology (ICRTIT 2011), P673, DOI 10.1109/ICRTIT.2011.5972458
   Sohn H., 2008, P IEEE REG 10 C, P1
   Trajkovska I., 2010, P ACM MULT
   Wu S, 2005, 25th IEEE International Conference on Distributed Computing Systems Workshops, Proceedings, P727
   Xin Jin, 2010, Proceedings 2010 IEEE 16th International Conference on Parallel and Distributed Systems (ICPADS 2010), P800, DOI 10.1109/ICPADS.2010.78
   Yaldiz S., 2008, IEEE T CAD INT CIRC, V27
   Yin W., 2011, P IEEE PERCOM WORKSH, P12934
   Yoon H, 2007, IEEE COMMUN LETT, V11, P714, DOI 10.1109/LCOMM.2007.070010
   Zhang G., 2011, P 2011 INT C WIR COM
   Zhang L. N., P C IM SIGN PROC, V1, P349
   Zheng Z., 2010, P IEEE S REL DISTR S, P18493
   Zhou L, 2010, IEEE J SEL AREA COMM, V28, P409, DOI 10.1109/JSAC.2010.100412
   Zhu WW, 2011, IEEE SIGNAL PROC MAG, V28, DOI 10.1109/MSP.2011.940269
NR 36
TC 62
Z9 66
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2013
VL 15
IS 4
BP 747
EP 757
DI 10.1109/TMM.2013.2240270
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 148FK
UT WOS:000319228500004
DA 2024-07-18
ER

PT J
AU Chen, MY
   AlRegib, G
   Juang, BH
AF Chen, Mingyu
   AlRegib, Ghassan
   Juang, Biing-Hwang
TI Feature Processing and Modeling for 6D Motion Gesture Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Gesture recognition; motion gesture; 6D motion tracking
AB A 6D motion gesture is represented by a 3D spatial trajectory and augmented by another three dimensions of orientation. Using different tracking technologies, the motion can be tracked explicitly with the position and orientation or implicitly with the acceleration and angular speed. In this work, we address the problem of motion gesture recognition for command-and-control applications. Our main contribution is to investigate the relative effectiveness of various feature dimensions for motion gesture recognition in both user-dependent and user-independent cases. We introduce a statistical feature-based classifier as the baseline and propose an HMM-based recognizer, which offers more flexibility in feature selection and achieves better performance in recognition accuracy than the baseline system. Our motion gesture database which contains both explicit and implicit motion information allows us to compare the recognition performance of different tracking signals on a common ground. This study also gives an insight into the attainable recognition rate with different tracking devices, which is valuable for the system designer to choose the proper tracking technology.
C1 [Chen, Mingyu; AlRegib, Ghassan; Juang, Biing-Hwang] Georgia Inst Technol, Sch Elect & Comp Engn, Atlanta, GA 30332 USA.
C3 University System of Georgia; Georgia Institute of Technology
RP Chen, MY (corresponding author), Georgia Inst Technol, Sch Elect & Comp Engn, Atlanta, GA 30332 USA.
RI Chen, Mingyu/KUD-1670-2024
OI AlRegib, Ghassan/0000-0001-6818-8001
CR Amma C., 2010, AH 10, P10
   Chen M., 2012, MMSYS 12
   Chen M., 2011, P IEEE ICASSP
   Chen M., 2012, ESPA 12
   Chen MY, 2010, INT CONF SIGN PROCES, P1205, DOI 10.1109/ICOSP.2010.5656100
   Godwin A, 2009, J BIOMECH ENG-T ASME, V131, DOI 10.1115/1.4000109
   Hoffman M, 2010, P IEEE VIRT REAL ANN, P59, DOI 10.1109/VR.2010.5444813
   Kratz S., 2011, Proceedings of the 16th international conference on Intelligent user interfaces, P371
   Lee HK, 1999, IEEE T PATTERN ANAL, V21, P961, DOI 10.1109/34.799904
   Liu JY, 2009, PERVASIVE MOB COMPUT, V5, P657, DOI 10.1016/j.pmcj.2009.07.007
   Mantyjarvi J., 2004, Proceedings of the 3rd international conference on Mobile and ubiquitous multimedia - MUM '04, P25
   Mitra S, 2007, IEEE T SYST MAN CY C, V37, P311, DOI 10.1109/TSMCC.2007.893280
   Pitsikalis V., 2011, Computer Vision and Pattern Recognition Workshops, P1, DOI DOI 10.1109/CVPRW.2011.5981681
   RUBINE D, 1991, COMP GRAPH, V25, P329, DOI 10.1145/127719.122753
   Ruiz J., 2011, CHI 11
   Starner T, 1998, IEEE T PATTERN ANAL, V20, P1371, DOI 10.1109/34.735811
   Teather RJ, 2009, 3DUI : IEEE SYMPOSIUM ON 3D USER INTERFACES 2009, PROCEEDINGS, P43, DOI 10.1109/3DUI.2009.4811204
   Welch G, 2002, IEEE COMPUT GRAPH, V22, P24, DOI 10.1109/MCG.2002.1046626
   Wobbrock JO, 2007, UIST 2007: PROCEEDINGS OF THE 20TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P159
NR 19
TC 42
Z9 43
U1 0
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2013
VL 15
IS 3
BP 561
EP 571
DI 10.1109/TMM.2012.2237024
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 109ZM
UT WOS:000316410900008
DA 2024-07-18
ER

PT J
AU Yang, MC
   Wang, YCF
AF Yang, Min-Chun
   Wang, Yu-Chiang Frank
TI A Self-Learning Approach to Single Image Super-Resolution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Self-learning; sparse representation; super-resolution; support vector
   regression
AB Learning-based approaches for image super-resolution (SR) have attracted the attention from researchers in the past few years. In this paper, we present a novel self-learning approach for SR. In our proposed framework, we advance support vector regression (SVR) with image sparse representation, which offers excellent generalization in modeling the relationship between images and their associated SR versions. Unlike most prior SR methods, our proposed framework does not require the collection of training low and high-resolution image data in advance, and we do not assume the reoccurrence (or self-similarity) of image patches within an image or across image scales. With theoretical supports of Bayes decision theory, we verify that our SR framework learns and selects the optimal SVR model when producing an SR image, which results in the minimum SR reconstruction error. We evaluate our method on a variety of images, and obtain very promising SR results. In most cases, our method quantitatively and qualitatively outperforms bicubic interpolation and state-of-the-art learning-based SR approaches.
C1 [Yang, Min-Chun] Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 10764, Taiwan.
   [Yang, Min-Chun; Wang, Yu-Chiang Frank] Acad Sinica, Res Ctr Informat Technol Innovat, Taipei 115, Taiwan.
   [Wang, Yu-Chiang Frank] Acad Sinica, Inst Informat Sci, Taipei, Taiwan.
   [Wang, Yu-Chiang Frank] Acad Sinica, Res Ctr Informat Technol Innovat CITI, Taipei, Taiwan.
C3 National Taiwan University; Academia Sinica - Taiwan; Academia Sinica -
   Taiwan; Academia Sinica - Taiwan
RP Yang, MC (corresponding author), Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 10764, Taiwan.
EM d96922009@ntu.edu.tw; ycwang@citi.sinica.edu.tw
CR [Anonymous], 2008, P IEEE C COMP VIS PA
   [Anonymous], IEEE COMPUT GRAPH AP
   Baker S, 2002, IEEE T PATTERN ANAL, V24, P1167, DOI 10.1109/TPAMI.2002.1033210
   Capel D., 2004, THESIS
   Chakrabarti A, 2007, IEEE T MULTIMEDIA, V9, P888, DOI 10.1109/TMM.2007.893346
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chang H, 2004, PROC CVPR IEEE, P275, DOI 10.1109/cvpr.2004.1315043
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Farsiu S, 2004, IEEE T IMAGE PROCESS, V13, P1327, DOI 10.1109/TIP.2004.834669
   Fattal R., 2007, ACM T GRAPH, V261
   Glasner D., 2009, P IEEE ITN C COMP VI
   HaCohen Y., 2010, ICCP, P1
   Hardie RC, 1997, IEEE T IMAGE PROCESS, V6, P1621, DOI 10.1109/83.650116
   Irani M., 1991, CVGIP GRAPH MODELS I, V53
   Kim K. I., 2008, EXAMPLE BASED LEARNI
   Kim KI, 2010, IEEE T PATTERN ANAL, V32, P1127, DOI 10.1109/TPAMI.2010.25
   Lin ZC, 2004, IEEE T PATTERN ANAL, V26, P83, DOI 10.1109/TPAMI.2004.1261081
   Mairal J., P ICCV, P2272
   Mairal J, 2010, J MACH LEARN RES, V11, P19
   Nguyen N, 2001, IEEE T IMAGE PROCESS, V10, P573, DOI 10.1109/83.913592
   Ni KS, 2007, IEEE T IMAGE PROCESS, V16, P1596, DOI 10.1109/TIP.2007.896644
   Protter M, 2009, IEEE T IMAGE PROCESS, V18, P36, DOI 10.1109/TIP.2008.2008067
   Protter M, 2009, IEEE T IMAGE PROCESS, V18, P27, DOI 10.1109/TIP.2008.2008065
   Smola A.J., 2003, TUTORIAL SUPPORT VEC
   Sun J., 2010, P IEEE C COMP VIS PA
   Sun J., 2003, P IEEE INT C COMP VI
   Tipping M. E., 2002, ADV NEURAL INF PROCE
   Xiong Z., 2009, P IEEE INT C COMP VI
   Yang CY, 2011, LECT NOTES COMPUT SC, V6494, P497, DOI 10.1007/978-3-642-19318-7_39
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yang M.-C., 2010, P IEEE INT C IM PROC
NR 31
TC 100
Z9 121
U1 0
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2013
VL 15
IS 3
BP 498
EP 508
DI 10.1109/TMM.2012.2232646
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 109ZM
UT WOS:000316410900003
DA 2024-07-18
ER

PT J
AU Biel, JI
   Gatica-Perez, D
AF Biel, Joan-Isaac
   Gatica-Perez, Daniel
TI The YouTube Lens: Crowdsourced Personality Impressions and Audiovisual
   Analysis of Vlogs
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Crowdsourcing; interpersonal perception; multimodal analysis; nonverbal
   behavior; personality; video blogs; vlogs; YouTube
ID ZERO-ACQUAINTANCE; DOMINANCE; CONSENSUS; CUES
AB Despite an increasing interest in understanding human perception in social media through the automatic analysis of users' personality, existing attempts have explored user profiles and text blog data only. We approach the study of personality impressions in social media from the novel perspective of crowdsourced impressions, social attention, and audiovisual behavioral analysis on slices of conversational vlogs extracted from YouTube. Conversational vlogs are a unique case study to understand users in social media, as vloggers implicitly or explicitly share information about themselves that words, either written or spoken cannot convey. In addition, research in vlogs may become a fertile ground for the study of video interactions, as conversational video expands to innovative applications. In this work, we first investigate the feasibility of crowdsourcing personality impressions from vlogging as a way to obtain judgements from a variate audience that consumes social media video. Then, we explore how these personality impressions mediate the online video watching experience and relate to measures of attention in YouTube. Finally, we investigate on the use of automatic nonverbal cues as a suitable lens through which impressions are made, and we address the task of automatic prediction of vloggers' personality impressions using nonverbal cues and machine learning techniques. Our study, conducted on a dataset of 442 YouTube vlogs and 2210 annotations collected in Amazon's Mechanical Turk, provides new findings regarding the suitability of collecting personality impressions from crowdsourcing, the types of personality impressions that emerge through vlogging, their association with social attention, and the level of utilization of nonverbal cues in this particular setting. In addition, it constitutes a first attempt to address the task of automatic vlogger personality impression prediction using nonverbal cues, with promising results.
C1 [Biel, Joan-Isaac; Gatica-Perez, Daniel] Idiap Res Inst, Martigny, Switzerland.
   [Biel, Joan-Isaac; Gatica-Perez, Daniel] Ecole Polytech Fed Lausanne, Lausanne, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne
RP Biel, JI (corresponding author), Idiap Res Inst, Martigny, Switzerland.
EM jibiel@idiap.ch; gatica@idiap.ch
FU Swiss National Science Foundation (SNSF) through the Swiss National
   Center of Competence in Research (NCCR) on Interactive Multimodal
   Information Management (IM)2
FX This work was supported by the Swiss National Science Foundation (SNSF)
   through the Swiss National Center of Competence in Research (NCCR) on
   Interactive Multimodal Information Management (IM)2. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Shrikanth Narayanan.
CR AMBADY N, 1995, J PERS SOC PSYCHOL, V69, P518, DOI 10.1037/0022-3514.69.3.518
   AMBADY N, 1992, PSYCHOL BULL, V111, P256, DOI 10.1037/0033-2909.111.2.256
   Anderson C, 2001, J PERS SOC PSYCHOL, V81, P116, DOI 10.1037//0022-3514.81.1.116
   [Anonymous], PROC INT CONF MULTIM
   [Anonymous], PROC AAAI INT CONF W
   [Anonymous], 2009, SOCIAL ATTENTION AGE
   [Anonymous], SHORTFORM VIDEO PLAT
   [Anonymous], PROC AAAI INT CONF W
   [Anonymous], 1979, SOCIAL MARKERS SPEEC
   [Anonymous], PROC AAAI INT CONF W
   [Anonymous], PROC INT CONF MULTIM
   [Anonymous], PROC INT CONF WEBLOG
   [Anonymous], PROC 44TH ANNU MEETI
   [Anonymous], PROC INT CONF USER M
   [Anonymous], SOCIAL SCIENCE RESEA
   [Anonymous], PROC AAAI INT CONF W
   [Anonymous], THESIS
   [Anonymous], J COMPUTER MEDIATED
   [Anonymous], PROC AAAI INT CONF O
   [Anonymous], TECH REP
   [Anonymous], INT J COMPUT VISION
   [Anonymous], HONEST SIGNALS THEY
   [Anonymous], PROC 7TH INTERNET ME
   [Anonymous], PROC AAAI INT CONF W
   [Anonymous], PROC INT CONF MULTIM
   [Anonymous], PROC SIGIR WORKSHOP
   [Anonymous], PROC AAAI INT CONF W
   [Anonymous], YOUTUBE BLOG
   [Anonymous], PROC INT CONF HUMAN
   [Anonymous], PROC ACM MULTIMEDIA
   [Anonymous], PROC AAAI INT CONF W
   Ashton MC, 2002, J PERS SOC PSYCHOL, V83, P245, DOI 10.1037//0022-3514.83.1.245
   Biel J.-I., 2011, ACM Transactions on Multimedia Computing, Communications, and Applications, V7S, P1, DOI DOI 10.1145/2037676.2037690
   BORKENAU P, 1992, J PERS SOC PSYCHOL, V62, P645, DOI 10.1037/0022-3514.62.4.645
   Brew A, 2010, FRONT ARTIF INTEL AP, V215, P145, DOI 10.3233/978-1-60750-606-5-145
   Buhrmester M, 2011, PERSPECT PSYCHOL SCI, V6, P3, DOI 10.1177/1745691610393980
   Burgess J., 2009, YouTube: Online video and participatory culture
   Carney DR, 2007, J RES PERS, V41, P1054, DOI 10.1016/j.jrp.2007.01.004
   Corder G.W., 2009, NONPARAMETRIC STAT N
   DOVIDIO JF, 1982, SOC PSYCHOL QUART, V45, P106, DOI 10.2307/3033933
   Gatica-Perez D, 2009, IMAGE VISION COMPUT, V27, P1775, DOI 10.1016/j.imavis.2009.01.004
   Gosling SD, 2002, J PERS SOC PSYCHOL, V82, P379, DOI 10.1037//0022-3514.82.3.379
   Gosling SD, 2003, J RES PERS, V37, P504, DOI 10.1016/S0092-6566(03)00046-1
   Huberman BA, 2009, J INF SCI, V35, P758, DOI 10.1177/0165551509346786
   IIZUKA Y, 1992, PERCEPT MOTOR SKILL, V74, P43, DOI 10.2466/PMS.74.1.43-50
   Jayagopi DB, 2009, IEEE T AUDIO SPEECH, V17, P501, DOI 10.1109/TASL.2008.2008238
   KENNY DA, 1992, J PERS SOC PSYCHOL, V62, P88, DOI 10.1037/0022-3514.62.1.88
   Kittur A, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P453
   Knapp M.L., 2005, Nonverbal communication in human interaction, V6th
   Kuhn M, 2008, J STAT SOFTW, V28, P1, DOI 10.18637/jss.v028.i05
   Mairesse F, 2007, J ARTIF INTELL RES, V30, P457, DOI 10.1613/jair.2349
   MCCRAE RR, 1992, J PERS, V60, P175, DOI 10.1111/j.1467-6494.1992.tb00970.x
   SHROUT PE, 1979, PSYCHOL BULL, V86, P420, DOI 10.1037/0033-2909.86.2.420
   Tastle WJ, 2007, INT J APPROX REASON, V45, P531, DOI 10.1016/j.ijar.2006.06.024
   Vazire S, 2004, J PERS SOC PSYCHOL, V87, P123, DOI 10.1037/0022-3514.87.1.123
   Wesch Michael., 2009, Explorations in Media Ecology, V8, P19, DOI DOI 10.1386/EME.8.2.99_1
   Yarkoni T, 2010, J RES PERS, V44, P363, DOI 10.1016/j.jrp.2010.04.001
NR 57
TC 116
Z9 139
U1 2
U2 115
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2013
VL 15
IS 1
BP 41
EP 55
DI 10.1109/TMM.2012.2225032
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA 058PS
UT WOS:000312646600004
DA 2024-07-18
ER

PT J
AU Ewerth, R
   Ballafkir, K
   Mühling, M
   Seiler, D
   Freisleben, B
AF Ewerth, Ralph
   Ballafkir, Khalid
   Muehling, Markus
   Seiler, Dominik
   Freisleben, Bernd
TI Long-Term Incremental Web-Supervised Learning of Visual Concepts via
   Random Savannas
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image classification; incremental learning; random forest; random
   savanna; web-supervised learning
ID IMAGES; OBJECT
AB The idea of using image and video data available in the World-Wide Web (WWW) as training data for classifier construction has received some attention in the past few years. In this paper, we present a novel incremental and scalable web-supervised learning system that continuously learns appearance models for image categories with heterogeneous appearances and improves these models periodically. Simply specifying the name of the concept that has to be learned initializes the proposed system, and there is no further supervision afterwards. Textual and visual information on web sites are used to filter out irrelevant and misleading training images. To obtain a robust, flexible, and updatable way of learning, a novel learning framework is presented that relies on clustering in order to identify visual subclasses before using an ensemble of random forests, called random savanna, for subclass learning. Experimental results demonstrate that the proposed web-supervised learning approach outperforms a support vector machine (SVM), while at the same time being simply parallelizable in the training and testing phases.
C1 [Ewerth, Ralph; Ballafkir, Khalid; Muehling, Markus; Seiler, Dominik; Freisleben, Bernd] Univ Marburg, Dept Math & Comp Sci, D-35032 Marburg, Germany.
C3 Philipps University Marburg
RP Ewerth, R (corresponding author), Univ Marburg, Dept Math & Comp Sci, D-35032 Marburg, Germany.
EM ewerth@informatik.uni-marburg.de; muehling@informatik.uni-marburg.de;
   seiler@informatik.uni-marburg.de; freisleb@informatik.uni-marburg.de;
   ballafki@students.uni-marburg.de
OI Ewerth, Ralph/0000-0003-0918-6297; Freisleben, Bernd/0000-0002-7205-8389
FU Deutsche Forschungsgemeinschaft (German Research Foundation) [PAK 509]
FX This work was supported by the Deutsche Forschungsgemeinschaft (German
   Research Foundation, Project MT, PAK 509). The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Jinhui Tang.
CR [Anonymous], P INT C EL COMM TAIP
   [Anonymous], 2009, Proceedings of the 8th International Conference on Generative Programming and Component Engineering
   [Anonymous], 2007, 11 INT C COMPUTER VI
   [Anonymous], P 1 WORKSH WEB SCAL
   [Anonymous], 2008, An Open and Portable Library of Computer Vision Algorithms
   Breiman L., 2001, Mach. Learn., V45, P5
   Datta R., 2007, Proc. ACM Multimedia, P393
   Ester M., 2000, Knowledge Discovery in Databases - Techniken und Anwendungen
   Everingham Mark, 2007, PASCAL VISUAL OBJECT
   Fan J., 2009, Proceedings of the First ACM workshop on Large-scale multimedia retrieval and mining, P27
   Fei-Fei L, 2004, P IEEE C COMP VIS PA, P178
   FERGUS R, 2005, P ICCV, P1816
   Griffin G., 2007, CALTECH 256 OBJECT C
   Jiang Y.-G., 2009, Proc. ACM MM, P155, DOI DOI 10.1145/1631272.1631296
   Kennedy L.S., 2006, Proc. ACM Multimedia Information Retrieval, P249, DOI DOI 10.1145/1178677.1178712
   Leibe B, 2008, INT J COMPUT VISION, V77, P259, DOI 10.1007/s11263-007-0095-3
   Li LJ, 2010, INT J COMPUT VISION, V88, P147, DOI 10.1007/s11263-009-0265-6
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   PORTER MF, 1980, PROGRAM-AUTOM LIBR, V14, P130, DOI 10.1108/eb046814
   Schroff F, 2011, IEEE T PATTERN ANAL, V33, P754, DOI 10.1109/TPAMI.2010.133
   Tang Jinhui., 2009, Proceedings of ACM international conference on Multimedia, P223, DOI DOI 10.1145/1631272.1631305
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Tuytelaars T, 2010, INT J COMPUT VISION, V88, P284, DOI 10.1007/s11263-009-0271-8
   Ulges A, 2011, IEEE T MULTIMEDIA, V13, P330, DOI 10.1109/TMM.2010.2101051
   Wang CH, 2008, MULTIMEDIA SYST, V14, P205, DOI 10.1007/s00530-008-0128-y
   Wang XJ, 2008, IEEE T PATTERN ANAL, V30, P1919, DOI 10.1109/TPAMI.2008.127
   Witten I. H., 2005, DATA MINING PRACTICA
   Xu H, 2009, PHIL MAG LETT, V89, P465, DOI 10.1080/09500830903019012
   Zhao WL, 2010, IEEE T MULTIMEDIA, V12, P448, DOI 10.1109/TMM.2010.2050651
NR 29
TC 9
Z9 9
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2012
VL 14
IS 4
SI SI
BP 1008
EP 1020
DI 10.1109/TMM.2012.2186956
PN 1
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 976QL
UT WOS:000306599300007
DA 2024-07-18
ER

PT J
AU Ma, ZG
   Nie, FP
   Yang, Y
   Uijlings, JRR
   Sebe, N
AF Ma, Zhigang
   Nie, Feiping
   Yang, Yi
   Uijlings, Jasper R. R.
   Sebe, Nicu
TI Web Image Annotation Via Subspace-Sparsity Collaborated Feature
   Selection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image annotation; shared subspace uncovering; sparse feature selection;
   supervised learning
AB The number of web images has been explosively growing due to the development of network and storage technology. These images make up a large amount of current multimedia data and are closely related to our daily life. To efficiently browse, retrieve and organize the web images, numerous approaches have been proposed. Since the semantic concepts of the images can be indicated by label information, automatic image annotation becomes one effective technique for image management tasks. Most existing annotation methods use image features that are often noisy and redundant. Hence, feature selection can be exploited for a more precise and compact representation of the images, thus improving the annotation performance. In this paper, we propose a novel feature selection method and apply it to automatic image annotation. There are two appealing properties of our method. First, it can jointly select the most relevant features from all the data points by using a sparsity-based model. Second, it can uncover the shared subspace of original features, which is beneficial for multi-label learning. To solve the objective function of our method, we propose an efficient iterative algorithm. Extensive experiments are performed on large image databases that are collected from the web. The experimental results together with the theoretical analysis have validated the effectiveness of our method for feature selection, thus demonstrating its feasibility of being applied to web image annotation.
C1 [Ma, Zhigang; Uijlings, Jasper R. R.; Sebe, Nicu] Univ Trent, Dept Informat Engn & Comp Sci, I-38123 Trento, Italy.
   [Nie, Feiping] Univ Texas Arlington, Dept Comp Sci & Engn, Arlington, TX 76019 USA.
   [Yang, Yi] Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.
C3 University of Trento; University of Texas System; University of Texas
   Arlington; Carnegie Mellon University
RP Ma, ZG (corresponding author), Univ Trent, Dept Informat Engn & Comp Sci, I-38123 Trento, Italy.
EM ma@disi.unitn.it; feipingnie@gmail.com; yiyang@cs.cmu.edu;
   uijlings@disi.unitn.it; sebe@disi.unitn.it
RI yang, yang/GWB-9426-2022; Ma, Zhigang/H-3543-2015; yang,
   yang/HGT-7999-2022; Nie, Feiping/B-3039-2012; Yang, Yi/B-9273-2017;
   Sebe, Niculae/KEC-2000-2024; yang, yang/GVT-5210-2022; Lang,
   Ming/HIK-0758-2022
OI Yang, Yi/0000-0002-0512-880X; Sebe, Niculae/0000-0002-6597-7248; 
FU European Commission; National Basic Research Program of China
   [2012CB316400]; National Science Foundation [IIS-0917072, CNS-0751185]
FX The work of Z. Ma, J. Uijlings, and N. Sebe was supported in part by the
   European Commission under the contract FP7-248984 GLOCAL. The work of F.
   Nie was supported in part by the National Basic Research Program of
   China (2012CB316400). The work of Y. Yang was supported in part by the
   National Science Foundation under Grant IIS-0917072 and Grant
   CNS-0751185. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Qi Tian.
CR Amores J, 2007, IEEE T PATTERN ANAL, V29, P1818, DOI 10.1109/TPAMI.2007.1098
   Ando RK, 2005, J MACH LEARN RES, V6, P1817
   [Anonymous], P ACM INT C IM VID R
   Cai D., 2010, KDD, P333
   Carneiro G, 2007, IEEE T PATTERN ANAL, V29, P394, DOI 10.1109/TPAMI.2007.61
   Cawley G.C., 2006, Advances in Neural Information Processing Systems, P209
   Cheng B, 2010, IEEE T IMAGE PROCESS, V19, P858, DOI 10.1109/TIP.2009.2038764
   Chua T., 2009, CIVR SANT GREEC
   Duda R. O., 2000, PATTERN CLASSIFICATI
   Gao Y., 2006, MULTI-MEDIA '06, P901
   Han Y., 2010, AAAI ATL GA
   Ji SW, 2010, ACM T KNOWL DISCOV D, V4, DOI 10.1145/1754428.1754431
   Kononenko I., 1994, Machine Learning: ECML-94. European Conference on Machine Learning. Proceedings, P171
   Krishnapuram B, 2004, IEEE T PATTERN ANAL, V26, P1105, DOI 10.1109/TPAMI.2004.55
   Law MHC, 2004, IEEE T PATTERN ANAL, V26, P1154, DOI 10.1109/TPAMI.2004.71
   Li H., 2006, P IEEE INT C DAT MIN
   Lu YJ, 2009, IEEE T MULTIMEDIA, V11, P1289, DOI 10.1109/TMM.2009.2030632
   Ma Z., 2011, Proc. 19th ACM Int'l Conf. Multimedia, P283, DOI DOI 10.1145/2072298.2072336
   Nie F., 2010, ADV NEURAL INFORM PR, P1813
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   Shen HF, 2010, IEEE INT CON MULTI, P980, DOI 10.1109/ICME.2010.5583900
   Ulges A, 2011, IEEE T MULTIMEDIA, V13, P330, DOI 10.1109/TMM.2010.2101051
   Wang M, 2009, IEEE T MULTIMEDIA, V11, P465, DOI 10.1109/TMM.2009.2012919
   Wang XJ, 2008, IEEE T PATTERN ANAL, V30, P1919, DOI 10.1109/TPAMI.2008.127
   Wei HL, 2007, IEEE T PATTERN ANAL, V29, P162, DOI 10.1109/TPAMI.2007.250607
   Yang Y, 2009, PR ELECTROMAGN RES S, P311, DOI 10.1145/1631272.1631316
   Yang Y., 2011, P 22 INT JOINT C ART, P1589
   Yang Y, 2008, IEEE T MULTIMEDIA, V10, P437, DOI 10.1109/TMM.2008.917359
   Yang Y, 2012, IEEE T PATTERN ANAL, V34, P723, DOI 10.1109/TPAMI.2011.170
   Zhao Z., 2010, PRESENTED AT THE AAA
   Zhao Zheng, 2007, ICML
   Zhuang YT, 2008, IEEE T MULTIMEDIA, V10, P221, DOI 10.1109/TMM.2007.911822
NR 32
TC 163
Z9 173
U1 0
U2 33
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2012
VL 14
IS 4
SI SI
BP 1021
EP 1030
DI 10.1109/TMM.2012.2187179
PN 1
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 976QL
UT WOS:000306599300008
DA 2024-07-18
ER

PT J
AU Anam, MA
   Andreopoulos, Y
AF Anam, Mohammad Ashraful
   Andreopoulos, Yiannis
TI Throughput Scaling Of Convolution For Error-Tolerant Multimedia
   Applications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Approximate computation; error-tolerant multimedia processing; fast
   convolution
ID MULTICORE
AB Convolution and cross-correlation are the basis of filtering and pattern or template matching in multimedia signal processing. We propose two throughput scaling options for any one-dimensional convolution kernel in programmable processors by adjusting the imprecision (distortion) of computation. Our approach is based on scalar quantization, followed by two forms of tight packing in floating-point (one of which is proposed in this paper) that allow for concurrent calculation of multiple results. We illustrate how our approach can operate as an optional pre- and post-processing layer for off-the-shelf optimized convolution routines. This is useful for multimedia applications that are tolerant to processing imprecision and for cases where the input signals are inherently noisy (error tolerant multimedia applications). Indicative experimental results with a digital music matching system and an MPEG-7 audio descriptor system demonstrate that the proposed approach offers up to 175% increase in processing throughput against optimized (full-precision) convolution with virtually no effect in the accuracy of the results. Based on marginal statistics of the input data, it is also shown how the throughput and distortion can be adjusted per input block of samples under constraints on the signal-to-noise ratio against the full-precision convolution.
C1 [Anam, Mohammad Ashraful; Andreopoulos, Yiannis] UCL, Elect & Elect Engn Dept, London WC1E 7JE, England.
C3 University of London; University College London
RP Anam, MA (corresponding author), UCL, Elect & Elect Engn Dept, London WC1E 7JE, England.
EM uceeman@ee.ucl.ac.uk; iandreop@ee.ucl.ac.uk
FU EPSRC [EP/020015/1]; Commonwealth Scholarship Commission; EPSRC
   [EP/F020015/1] Funding Source: UKRI
FX This work was supported in part by EPSRC, project EP/020015/1 and in
   part by a Ph.D. scholarship from the Commonwealth Scholarship
   Commission. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Yen-Kuang Chen.
CR Anastasia D, 2010, IEEE SIGNAL PROC LET, V17, P375, DOI 10.1109/LSP.2010.2041583
   [Anonymous], 2007, OPTIMIZING APPL MULT
   Casey M., MPEG 7 MULTIMEDIA SO
   Di Stefano L, 2003, MACH VISION APPL, V13, P213
   Ellis DRW, 2008, INT CONF ACOUST SPEE, P57, DOI 10.1109/ICASSP.2008.4517545
   Franchetti F, 2009, IEEE SIGNAL PROC MAG, V26, P90, DOI 10.1109/MSP.2009.934155
   Kadyrov A, 2006, IEEE T PATTERN ANAL, V28, P1882, DOI 10.1109/TPAMI.2006.234
   Kim D, 2010, IEEE SIGNAL PROC MAG, V27, P97, DOI 10.1109/MSP.2009.935384
   Lachambre H, 2011, IEEE T AUDIO SPEECH, V19, P1837, DOI 10.1109/TASL.2010.2089517
   López JA, 2008, IET CIRC DEVICE SYST, V2, P393, DOI 10.1049/iet-cds:20070198
   Ludwig JT, 1996, IEEE J SOLID-ST CIRC, V31, P395, DOI 10.1109/4.494201
   Merhav N, 1998, IEEE T CIRC SYST VID, V8, P378, DOI 10.1109/76.709404
   Oppheneim A. V., 1975, DIGITAL SIGNAL PROCE
   Shantal S., 2009, Eur. J. Sci. Res., V31, P19
   Swamy RK, 2007, IEEE SIGNAL PROC LET, V14, P481, DOI 10.1109/LSP.2006.891333
   Yu RS, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P1483, DOI 10.1109/ICME.2004.1394517
   Zhang K, 2009, IEEE IMAGE PROC, P2357, DOI 10.1109/ICIP.2009.5413502
NR 17
TC 8
Z9 9
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2012
VL 14
IS 3
BP 797
EP 804
DI 10.1109/TMM.2012.2184742
PN 2
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 943ZK
UT WOS:000304166700012
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Li, YR
   Zhang, T
AF Li, Yiran
   Zhang, Tong
TI Reducing DRAM Image Data Access Energy Consumption in Video Processing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE DRAM energy reduction; heterogeneous DRAM architecture; image data
   access; image frame buffer recompression; video coding
ID ARCHITECTURE; COMPRESSION; DECODER; DESIGN
AB This paper presents domain-specific techniques to reduce DRAM energy consumption for image data access in video processing. In mobile devices, video processing is one of the most energy-hungry tasks, and DRAM image data access energy consumption becomes increasingly dominant in overall video processing system energy consumption. Hence, it is highly desirable to develop domain-specific techniques that can exploit unique image data access characteristics to improve DRAM energy efficiency. Nevertheless, prior efforts on reducing DRAM energy consumption in video processing pale in comparison with that on reducing video processing logic energy consumption. In this work, we first apply three simple yet effective data manipulation techniques that exploit image data spatial/temporal correlation to reduce DRAM image data access energy consumption, then propose a heterogeneous DRAM architecture that can better adapt to unbalanced image access in most video processing to further improve DRAM energy efficiency. DRAM modeling and power estimation have been carried out to evaluate these domain-specific design techniques, and the results show that they can reduce DRAM energy consumption by up to 92%.
C1 [Li, Yiran; Zhang, Tong] Rensselaer Polytech Inst, Dept Elect Comp & Syst Engn, Troy, NY 12180 USA.
C3 Rensselaer Polytechnic Institute
RP Li, YR (corresponding author), Rensselaer Polytech Inst, Dept Elect Comp & Syst Engn, Troy, NY 12180 USA.
EM yiran.li@gmail.com; tzhang@ecse.rpi.edu
RI zhang, tong/JAO-3571-2023; ZHANG, TAO/ITV-6162-2023; Zhang,
   tong/IAP-2587-2023
CR Amin Ahmed M., 2010, Proceedings of the 16th ACM/IEEE International Symposium on Low Power Electronics and Design (ISLPED 2010), P383, DOI 10.1145/1840845.1840930
   Benini L, 1997, PR GR LAK SYMP VLSI, P77, DOI 10.1109/GLSV.1997.580414
   Chuang TD, 2009, INT CONF ACOUST SPEE, P2009, DOI 10.1109/ICASSP.2009.4960007
   Cooper-Balis E., 2010, P 43 ANN IEEE ACM IN
   Dikbas S, 2010, SIGNAL PROCESS-IMAGE, V25, P345, DOI 10.1016/j.image.2010.02.004
   Ghosh M, 2007, INT SYMP MICROARCH, P134, DOI 10.1109/MICRO.2007.13
   HIROSE K, 2000, P DES AUT TEST EUR C
   Jin Y, 2008, INT SYMP MICROARCH, P354, DOI 10.1109/MICRO.2008.4771804
   Kim H, 2001, IEEE T CIRC SYST VID, V11, P1160, DOI 10.1109/76.964782
   Lee TY, 2003, IEEE T CIRC SYST VID, V13, P529, DOI 10.1109/TCSVT.2003.813425
   Lee YJ, 2007, IEEE INT SYMP CIRC S, P1621, DOI 10.1109/ISCAS.2007.378829
   Li YR, 2010, SIGNAL PROCESS-IMAGE, V25, P335, DOI 10.1016/j.image.2010.04.002
   Liu TM, 2007, IEEE J SOLID-ST CIRC, V42, P161, DOI 10.1109/JSSC.2006.886542
   Mehendale M., 2001, VLSI Synthesis of DSP Kernels: Algorithmic and Architectural Transformations
   Mehta H, 1996, PR GR LAK SYMP VLSI, P178, DOI 10.1109/GLSV.1996.497616
   Musoll E, 1998, IEEE T VLSI SYST, V6, P568, DOI 10.1109/92.736129
   OSORIO RR, 2006, P 9 EUROMICRO C DIG
   Suresh DC, 2009, IEEE T COMPUT, V58, P1049, DOI 10.1109/TC.2009.39
   Sze V, 2009, IEEE J SOLID-ST CIRC, V44, P2943, DOI 10.1109/JSSC.2009.2028933
   Thuresson M, 2008, IEEE T COMPUT, V57, P916, DOI 10.1109/TC.2008.28
   Udipi AN, 2010, CONF PROC INT SYMP C, P175, DOI 10.1145/1816038.1815983
   Yng TLB, 2008, IEEE T CONSUM ELECTR, V54, P1453, DOI 10.1109/TCE.2008.4637640
   You H., 1998, Proc. of the 10th Workshop on Image Process. and Understanding, P305
   Zheng HZ, 2008, INT SYMP MICROARCH, P210, DOI 10.1109/MICRO.2008.4771792
   Zhou DJ, 2011, IEEE J SOLID-ST CIRC, V46, P777, DOI 10.1109/JSSC.2011.2109550
NR 25
TC 9
Z9 10
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2012
VL 14
IS 2
BP 303
EP 313
DI 10.1109/TMM.2011.2177079
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 924OZ
UT WOS:000302702500006
DA 2024-07-18
ER

PT J
AU Feris, RS
   Siddiquie, B
   Petterson, J
   Zhai, Y
   Datta, A
   Brown, LM
   Pankanti, S
AF Feris, Rogerio Schmidt
   Siddiquie, Behjat
   Petterson, James
   Zhai, Yun
   Datta, Ankur
   Brown, Lisa M.
   Pankanti, Sharath
TI Large-Scale Vehicle Detection, Indexing, and Search in Urban
   Surveillance Videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Large-scale learning; large-scale video collections; vehicle search;
   video surveillance
ID IMAGES
AB We present a novel approach for visual detection and attribute-based search of vehicles in crowded surveillance scenes. Large-scale processing is addressed along two dimensions: 1) large-scale indexing, where hundreds of billions of events need to be archived per month to enable effective search and 2) learning vehicle detectors with large-scale feature selection, using a feature pool containing millions of feature descriptors. Our method for vehicle detection also explicitly models occlusions and multiple vehicle types (e. g., buses, trucks, SUVs, cars), while requiring very few manual labeling. It runs quite efficiently at an average of 66 Hz on a conventional laptop computer. Once a vehicle is detected and tracked over the video, fine-grained attributes are extracted and ingested into a database to allow future search queries such as "Show me all blue trucks larger than 7 ft. length traveling at high speed northbound last Saturday, from 2 pm to 5 pm". We perform a comprehensive quantitative analysis to validate our approach, showing its usefulness in realistic urban surveillance settings. See demos at http://rogerioferis.com/IEEEMultimedia/
C1 [Feris, Rogerio Schmidt; Zhai, Yun; Datta, Ankur; Brown, Lisa M.; Pankanti, Sharath] IBM TJ Watson Ctr, Hawthorne, NY 10532 USA.
   [Siddiquie, Behjat] Univ Maryland, Baltimore, MD 20742 USA.
   [Petterson, James] Australian Natl Univ, Canberra, ACT 1435, Australia.
   [Petterson, James] NICTA, Canberra, ACT 1435, Australia.
C3 University System of Maryland; University of Maryland Baltimore;
   Australian National University; Australian National University
RP Feris, RS (corresponding author), IBM TJ Watson Ctr, Hawthorne, NY 10532 USA.
EM rsferis@us.ibm.com; behjat@cs.umd.edu; james.petterson@nicta.com.au;
   yunzhai@us.ibm.com; ankurd@us.ibm.com; lisabr@us.ibm.com;
   sharat@us.ibm.com
CR Agarwal S, 2004, IEEE T PATTERN ANAL, V26, P1475, DOI 10.1109/TPAMI.2004.108
   Anagnostopoulos CNE, 2008, IEEE T INTELL TRANSP, V9, P377, DOI 10.1109/TITS.2008.922938
   [Anonymous], P PETS WORKSH
   [Anonymous], 2010, P CVPR
   [Anonymous], P WACV
   [Anonymous], 2009, P CVPR
   [Anonymous], P CVPR
   [Anonymous], P CVPR
   [Anonymous], P SPIE DEF SEC C ORL
   [Anonymous], P ACM SIGGRAPH
   [Anonymous], 2005, P CVPR
   [Anonymous], P CVPR
   [Anonymous], P ECCV 2010 WORKSH P
   [Anonymous], P ICCV
   [Anonymous], 2007, Technical Report
   [Anonymous], P ICCV
   [Anonymous], 2009, P CVPR
   [Anonymous], P WACV
   [Anonymous], P WACV
   [Anonymous], IEEE T PATT IN PRESS
   [Anonymous], P ECCV
   [Anonymous], P WACV
   [Anonymous], P AVSS
   [Anonymous], P NAT SCI COUNCIL A
   [Anonymous], 2009, P ICCV
   [Anonymous], P CVPR
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Forsyth DA, 2005, FOUND TRENDS COMPUT, V1, P77, DOI 10.1561/0600000005
   Kuhn HW, 2005, NAV RES LOG, V52, P7, DOI 10.1002/nav.20053
   Merler S, 2007, COMPUT STAT DATA AN, V51, P2487, DOI 10.1016/j.csda.2006.09.001
   Schneiderman H., 2000, P CVPR
   Senior A, 2006, IMAGE VISION COMPUT, V24, P1233, DOI 10.1016/j.imavis.2005.06.007
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Williams C.K.I., PASCAL VISUAL OBJECT
NR 34
TC 88
Z9 97
U1 0
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2012
VL 14
IS 1
BP 28
EP 42
DI 10.1109/TMM.2011.2170666
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 924OL
UT WOS:000302701100004
DA 2024-07-18
ER

PT J
AU Chakareski, J
AF Chakareski, Jacob
TI In-Network Packet Scheduling and Rate Allocation: A Content Delivery
   Perspective
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Active network nodes; content delivery networks; decentralized packet
   scheduling; distributed systems; edge servers; hybrid
   receiver-sender-driven packet scheduling; in-network processing;
   Lagrange multiplier method; multiple user video delivery; proxy-based
   streaming; rate allocation; rate-distortion optimization
AB We investigate two important problems in media delivery via active network agents. First, we consider streaming multiple video assets over a shared backbone network through an intermediate proxy-server to a set of receiving clients. The proxy is located at the junction of the backbone network and the last hop to each of the clients and coordinates the delivery of the videos from the origin media server to the clients. We propose an optimization framework that enables the proxy to coordinate the streaming process such that the overall end-to-end performance of the video streams is maximized for the given data rate resources on the backbone and the last hop links. Prospective video quality requirements for the associated media sessions are also taken into consideration in the analysis. Through experiments, we study in detail the operation of the framework and the influence of the various constraints that it considers. Furthermore, we measure its performance gains relative to a sender-driven system where the media server controls the delivery of the data with no assistance from an intervening proxy. We establish an analytical relationship between the relative improvement of the proxy-based system, the network conditions on the backbone and the last hops, and the number of streams served. The gains of the proxy-driven system measured in our experiments closely match their expected values predicted by this relationship.
   In conjunction with the above scenario, we explore the performance gains due to multi-agent packet scheduling where there are multiple active nodes organizing the packet transmissions along the network path between a server-client pair. To this end, we design an optimization framework that coordinates the multiple scheduling agents such that an end-to-end quality-rate performance metric is maximized. We study experimentally the performance benefits due to multi-agent scheduling relative to single-agent scheduling and conventional server-driven streaming, as a function of the number of intermediate nodes at which the packet scheduling is carried out. We quantify analytically the performance gains and match them with high accuracy to the simulation data.
C1 Ecole Polytech Fed Lausanne, Signal Proc Lab LTS4, CH-1015 Lausanne, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne
RP Chakareski, J (corresponding author), Ecole Polytech Fed Lausanne, Signal Proc Lab LTS4, CH-1015 Lausanne, Switzerland.
FU Swiss National Science Foundation [PZ00P2-126416]; Swiss National
   Science Foundation (SNF) [PZ00P2_126416] Funding Source: Swiss National
   Science Foundation (SNF)
FX Manuscript received November 23, 2010; revised March 19, 2011; accepted
   May 10, 2011. Date of publication May 27, 2011; date of current version
   September 16, 2011. This work was supported by the Swiss National
   Science Foundation under Ambizione Career Grant PZ00P2-126416. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Qibin Sun.
CR Bertsekas DP., 1996, Constrained Optimization and Lagrange Multiplier Methods, V1st edn
   Chakareski J, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P763, DOI 10.1109/ICME.2005.1521535
   Chakareski J, 2006, IEEE T MULTIMEDIA, V8, P207, DOI 10.1109/TMM.2005.864284
   CHAKARESKI J, 2006, P INT PACK VID WORKS, P773
   Chakareski J, 2006, IEEE ACM T NETWORK, V14, P1302, DOI 10.1109/TNET.2006.886299
   Chakareski J, 2010, STUD COMPUT INTELL, V280, P217
   Chang H, 2009, IMC'09: PROCEEDINGS OF THE 2009 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P417
   Chou PA, 2006, IEEE T MULTIMEDIA, V8, P390, DOI 10.1109/TMM.2005.864313
   CHOU PA, 2002, P INT PACK VID WORKS
   Garrett MW, 1993, IEEE ACM T NETWORK, V1, P71, DOI 10.1109/90.222908
   GIROD B, 2002, P TYRRH INT WORKSH D
   Huang CM, 2007, IEEE T MOBILE COMPUT, V6, P411, DOI 10.1109/TMC.2007.55
   *ITU T, 2005, H264 ITUT
   Jain M., 2002, P PASSIVE ACTIVE MEA, P14, DOI DOI 10.1109/JSAC.2003.814505
   KALMAN M, 2005, P IEEE INT C IM PROC, V1, P165
   Nguyen T, 2004, IEEE T MULTIMEDIA, V6, P315, DOI 10.1109/TMM.2003.822790
   REIS AB, 2010, P IEEE INT WORKSH CA
   Sorensen JH, 2010, GLOB TELECOMM CONF, DOI 10.1109/GLOCOM.2010.5684254
   Stevens W, 1994, TCP/ IP Illustrated, V1
   TU W, 2004, P INT PACK VID WORKS
   Tu W, 2009, IEEE T MULTIMEDIA, V11, P716, DOI 10.1109/TMM.2009.2017621
   *ZATT INC, WATCH ONL TV
   Zhu XQ, 2005, IEEE IMAGE PROC, P1513
NR 23
TC 8
Z9 8
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2011
VL 13
IS 5
BP 1092
EP 1102
DI 10.1109/TMM.2011.2157673
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 821XI
UT WOS:000295007300021
DA 2024-07-18
ER

PT J
AU Liu, HR
   Cao, SJ
   Yan, SC
AF Liu, Hairong
   Cao, Shengjiao
   Yan, Shuicheng
TI Automated Assembly of Shredded Pieces From Multiple Photos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Computer aided analysis; home computing; multimedia computing
ID SOLVING JIGSAW PUZZLES; SHAPE; MULTISCALE
AB In this paper, we investigate the problem of automated assembly of shredded pieces from multiple photos, which has a board usage in many multimedia applications. Both shape and appearance information along the boundaries are utilized and extracted for each pieces, and then the candidate matchings between pieces are established based on these features. A weighted graph, called matching graph, whose vertices represent shredded pieces and edges represent candidate matchings is then constructed, and divided into separate subgraphs, with each subgraph corresponding to a desired photo. The assembly results are finally obtained by searching for a valid spanning tree for each subgraph. This proposed method can deal with cases in which materials are lost and/or pieces belonging to multiple photos coexist. And the experimental results well demonstrate the effectiveness and efficiency of our proposed method.
C1 [Liu, Hairong; Cao, Shengjiao; Yan, Shuicheng] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117548, Singapore.
C3 National University of Singapore
RP Liu, HR (corresponding author), Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117548, Singapore.
EM lhrbss@gmail.com; caosj05@gmail.com; eleyans@nus.edu.sg
RI Yan, Shuicheng/HCI-1431-2022; Liu, Hairong/I-6695-2012
FU National Research Foundation/Interactive Digital Media Program,
   Singapore [NRF2008IDMIDM004-029]
FX This work was supported by the National Research Foundation/Interactive
   Digital Media Program, under research Grant NRF2008IDMIDM004-029,
   Singapore. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Chia-Wen Lin.
CR [Anonymous], IEEE T SYST MAN CYBE
   [Anonymous], P 21 WORKSH AUSTR AS
   [Anonymous], P IEEE INT S SIGN PR
   [Anonymous], 149 M PLANK I BIOL C
   [Anonymous], IEEE T PATTERN ANAL
   Barequet G, 1997, IEEE T PATTERN ANAL, V19, P929, DOI 10.1109/34.615444
   BURDEA GC, 1989, IEEE T ROBOTIC AUTOM, V5, P752, DOI 10.1109/70.88097
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chung MG, 1998, ICSP '98: 1998 FOURTH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING, PROCEEDINGS, VOLS I AND II, P877, DOI 10.1109/ICOSP.1998.770751
   Cormen Thomas H., 2001, INTRO ALGORITHMS
   Eppstein D, 2000, HANDBOOK OF COMPUTATIONAL GEOMETRY, P425, DOI 10.1016/B978-044482537-7/50010-3
   FREEMAN H, 1964, IEEE T COMPUT, VEC13, P118, DOI 10.1109/PGEC.1964.263781
   GEIGER D, 1995, IEEE T PATTERN ANAL, V17, P294, DOI 10.1109/34.368194
   Goldberg D, 2004, COMP GEOM-THEOR APPL, V28, P165, DOI 10.1016/j.comgeo.2004.03.007
   Justino E, 2006, FORENSIC SCI INT, V160, P140, DOI 10.1016/j.forsciint.2005.09.001
   Kanungo T, 2002, IEEE T PATTERN ANAL, V24, P881, DOI 10.1109/TPAMI.2002.1017616
   Kleber Florian, 2009, 2009 10th International Conference on Document Analysis and Recognition (ICDAR), P1061, DOI 10.1109/ICDAR.2009.154
   KOSIBA DA, 1994, INT C PATT RECOG, P616, DOI 10.1109/ICPR.1994.576377
   Leitao HCD, 2002, IEEE T PATTERN ANAL, V24, P1239, DOI 10.1109/TPAMI.2002.1033215
   Liu HR, 2008, INT J COMPUT VISION, V80, P104, DOI 10.1007/s11263-008-0131-y
   MARQUES M., 2009, Proceedings of the 2009 ACM symposium on Applied Computing SAC 09, P893
   MOKHTARIAN F, 1992, IEEE T PATTERN ANAL, V14, P789, DOI 10.1109/34.149591
   Ng AY, 2002, ADV NEUR IN, V14, P849
   Nielsen TR, 2008, PATTERN RECOGN LETT, V29, P1924, DOI 10.1016/j.patrec.2008.05.027
   Papaodysseus C, 2002, IEEE T SIGNAL PROCES, V50, P1277, DOI 10.1109/TSP.2002.1003053
   Yao FH, 2003, PATTERN RECOGN LETT, V24, P1819, DOI 10.1016/S0167-8655(03)00006-0
NR 26
TC 26
Z9 28
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2011
VL 13
IS 5
BP 1154
EP 1162
DI 10.1109/TMM.2011.2160845
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 821XI
UT WOS:000295007300026
DA 2024-07-18
ER

PT J
AU Huang, XL
   Wang, G
   Hu, F
   Kumar, S
AF Huang, Xin-Lin
   Wang, Gang
   Hu, Fei
   Kumar, Sunil
TI The Impact of Spectrum Sensing Frequency and Packet-Loading Scheme on
   Multimedia Transmission Over Cognitive Radio Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cognitive radio networks (CRN); discrete particle swarm optimization
   (DPSO); Hughes-Hartogs; multimedia transmission; packet-loading;
   spectrum sensing frequency
AB Recently, multimedia transmission over cognitive radio networks (CRNs) becomes an important topic due to the CR's capability of using unoccupied spectrum for data transmission. Conventional work has focused on typical quality-of-service (QoS) factors such as radio link reliability, maximum tolerable communication delay, and spectral efficiency. However, there is no work considering the impact of CR spectrum sensing frequency and packet-loading scheme on multimedia QoS. Here the spectrum sensing frequency means how frequently a CR user detects the free spectrum. Continuous, frequent spectrum sensing could increase the medium access control (MAC) layer processing overhead and delay, and cause some multimedia packets to miss the receiving deadline, and thus decrease the multimedia quality at the receiver side. In this research, we will derive the math model between the spectrum sensing frequency and the number of remaining packets that need to be sent, as well as the relationship between spectrum sensing frequency and the new channel availability time during which the CRN user is allowed to use a new channel (after the current channel is re-occupied by primary users) to continue packet transmission. A smaller number of remaining packets and a larger value of new channel availability time will help to transmit multimedia packets within a delay deadline. Based on the above relationship model, we select appropriate spectrum sensing frequency under single-channel case, and study the trade-offs among the number of selected channels, optimal spectrum sensing frequency, and packet-loading scheme under multi-channel case. The optimal spectrum sensing frequency and packet-loading solutions for multi-channel case are obtained by using the combination of Hughes-Hartogs and discrete particle swarm optimization (DPSO) algorithms. Our experiments of JPEG2000 packet-stream and H.264 video packet-stream transmission over CRN demonstrate the validity of our spectrum sensing frequency selection and packet-loading scheme.
C1 [Huang, Xin-Lin; Wang, Gang] Harbin Inst Technol, Commun Res Ctr, Harbin 150001, Peoples R China.
   [Hu, Fei] Univ Alabama, Dept Elect & Comp Engn, Tuscaloosa, AL 35487 USA.
   [Kumar, Sunil] San Diego State Univ, Dept Elect & Comp Engn, San Diego, CA 92182 USA.
C3 Harbin Institute of Technology; University of Alabama System; University
   of Alabama Tuscaloosa; California State University System; San Diego
   State University
RP Huang, XL (corresponding author), Harbin Inst Technol, Commun Res Ctr, Harbin 150001, Peoples R China.
EM xlhitcrc@163.com; gwang51@hit.edu.cn; fei@eng.ua.edu;
   skumar@mail.sdsu.edu
RI wang, gang/ITT-0670-2023; Kumar, Sunil/AAT-4942-2020
OI Kumar, Sunil/0000-0001-9957-5661
CR Akyildiz IF, 2006, COMPUT NETW, V50, P2127, DOI 10.1016/j.comnet.2006.05.001
   [Anonymous], 2005, P 14 IST MOB WIR COM
   [Anonymous], 2008, P IEEE MILCOM
   Apostolopoulos J, 2002, IEEE INFOCOM SER, P1736, DOI 10.1109/INFCOM.2002.1019427
   ARIPIN NM, P 2009 3 AS INT C MO, P676
   ARIPIN NM, 2008, P INT GRAD C ENG SCI, P1
   Huang Xin-lin, 2010, Journal of the Harbin Institute of Technology, V42, P1379
   HUANG XL, IEEE T VEH IN PRESS
   HUGHESHARTOGS D, 1989, Patent No. 4833706
   JAMES AM, 2007, THESIS ROCHESTER I T
   Kennedy J, 1995, 1995 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS PROCEEDINGS, VOLS 1-6, P1942, DOI 10.1109/icnn.1995.488968
   Khalife H., 2008, Proc. IEEE GLOBECOM, P1
   KUSHWAHA H, 2007, P IEEE CONS COMM NET
   Kushwaha H, 2008, P IEEE, V96, P155, DOI 10.1109/JPROC.2007.909917
   Lee WY, 2008, IEEE T WIREL COMMUN, V7, P3845, DOI 10.1109/T-WC.2008.070391
   Liang YC, 2008, IEEE T WIREL COMMUN, V7, P1326, DOI 10.1109/TWC.2008.060869
   Liu Y, 2007, IEEE T POWER SYST, V22, P1267, DOI 10.1109/TPWRS.2007.901486
   Sheu RT, 2003, IEE P-COMMUN, V150, P361, DOI 10.1049/ip-com:20030714
   Shiang HP, 2008, IEEE T MULTIMEDIA, V10, P896, DOI 10.1109/TMM.2008.922851
   Su H, 2007, 2007 41ST ANNUAL CONFERENCE ON INFORMATION SCIENCES AND SYSTEMS, VOLS 1 AND 2, P363, DOI 10.1109/CISS.2007.4298329
   Wang W, 2010, IEEE T MULTIMEDIA, V12, P417, DOI 10.1109/TMM.2010.2050653
   WEISS T, 2004, IEEE COMMUN MAG  MAR, V42, P8, DOI DOI 10.1109/MCOM.2004.127376
   Yücek T, 2009, IEEE COMMUN SURV TUT, V11, P116, DOI 10.1109/SURV.2009.090109
   Zhao ZJ, 2009, ACTA PHYS SIN-CH ED, V58, P5118, DOI 10.7498/aps.58.5118
   Zhu GM, 2008, GLOB TELECOMM CONF, DOI 10.1109/GLOCOM.2008.ECP.592
NR 25
TC 30
Z9 33
U1 2
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2011
VL 13
IS 4
BP 748
EP 761
DI 10.1109/TMM.2011.2148701
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 805EX
UT WOS:000293710300014
DA 2024-07-18
ER

PT J
AU Lee, H
   Lee, S
   Bovik, AC
AF Lee, Hyungkeuk
   Lee, Sanghoon
   Bovik, Alan Conrad
TI Cross-Layer Optimization for Downlink Wavelet Video Transmission
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-layer optimization; downlink wavelet video; power allocation;
   visual entropy; wavelet coding
ID RESOURCE-ALLOCATION; SCALABLE VIDEO; QUALITY; ADAPTATION; CAPACITY
AB Cross-layer optimization for efficient multimedia communications is an important emerging issue towards providing better quality-of-service (QoS) over capacity-limited wireless channels. This paper presents a cross-layer optimization approach that operates between the application and physical layers to achieve high fidelity downlink video transmission by optimizing with respect to a quality criterion termed "visual entropy" using Lagrangian relaxation. By utilizing the natural layered structure of wavelet coding, an optimal level of power allocation is determined, which permits the throughput of visual entropy to be maximized over a multi-cell environment. A theoretical approach to optimization using the Shannon capacity and the Karush-Kuhn-Tucker (KKT) conditions is explored when coupling the application with the physical layers. Simulations show that the throughput gain for cross-layer optimization by visual entropy is increased by nearly 80% at the cell boundary as compared with peak signal-to-noise ratio (PSNR).
C1 [Lee, Hyungkeuk; Lee, Sanghoon] Yonsei Univ, Wireless Network Lab, Ctr Informat Technol, Seoul 120749, South Korea.
   [Bovik, Alan Conrad] Univ Texas Austin, LIVE, Dept Elect & Comp Engn, Austin, TX 78712 USA.
C3 Yonsei University; University of Texas System; University of Texas
   Austin
RP Lee, H (corresponding author), Yonsei Univ, Wireless Network Lab, Ctr Informat Technol, Seoul 120749, South Korea.
EM punk-tank@yonsei.ac.kr; slee@yonsei.ac.kr; bovik@ece.utexas.edu
RI Lee, Sanghoon/A-3430-2019; Bovik, Alan/B-6717-2012
OI Lee, Sanghoon/0000-0001-9895-5347; Bovik, Alan/0000-0001-6067-710X
FU Ministry of Knowledge Economy (MKE), Korea [NIPA-2010-C6150-1001-0013];
   Ministry of Education, Science and Technology [2010-0011995]; Division
   Of Computer and Network Systems; Direct For Computer & Info Scie &
   Enginr [0854904] Funding Source: National Science Foundation
FX This work was supported by the Ministry of Knowledge Economy (MKE),
   Korea, under the national HRD support program for convergence
   information technology supervised by the National IT Industry Promotion
   Agency (NIPA) (NIPA-2010-C6150-1001-0013) and Basic Science Research
   Program through the National Research Foundation of Korea (NRF) funded
   by the Ministry of Education, Science and Technology (2010-0011995). The
   associate editor coordinating the reviewof this manuscript and approving
   it for publication was Dr. Zhihai (Henry) He.
CR [Anonymous], P IEEE INT C AC SPEE
   CONCI N, 2005, P IEEE INT C IM PROC, V1, P11
   Eisenberg Y, 2006, IEEE T IMAGE PROCESS, V15, P289, DOI 10.1109/TIP.2005.860600
   GILHOUSEN KS, 1991, IEEE T VEH TECHNOL, V40, P303, DOI 10.1109/25.289411
   Huang JW, 2008, IEEE T CIRC SYST VID, V18, P582, DOI 10.1109/TCSVT.2008.919109
   Jang U, 2008, IEEE T MULTIMEDIA, V10, P1181, DOI 10.1109/TMM.2008.2001380
   Katsaggelos AK, 2005, P IEEE, V93, P135, DOI 10.1109/JPROC.2004.839621
   Kim J, 2006, IEICE T COMMUN, VE89B, P531, DOI 10.1093/ietcom/e89-b.2.531
   LEE H, 2005, P IEEE INTC IM PROC, V3, P41
   LEE H, 2006, P IEEE GLOBECOM SAN
   Lee H, 2006, IEEE SIGNAL PROC LET, V13, P553, DOI 10.1109/LSP.2006.874464
   Lee H, 2008, EURASIP J ADV SIG PR, DOI 10.1155/2008/864606
   Lee H, 2011, WIREL NETW, V17, P103, DOI 10.1007/s11276-010-0267-x
   Lee S, 2002, IEEE T MULTIMEDIA, V4, P129, DOI 10.1109/6046.985561
   Lee S, 2001, IEEE T IMAGE PROCESS, V10, P977, DOI 10.1109/83.931092
   LI W, 2009, P IEEE INT C IM PROC
   Maani E, 2008, IEEE T IMAGE PROCESS, V17, P1663, DOI 10.1109/TIP.2008.2001402
   Mallat S, 1998, IEEE T SIGNAL PROCES, V46, P1027, DOI 10.1109/78.668554
   Pahalawatta P, 2007, IEEE J SEL AREA COMM, V25, P749, DOI 10.1109/JSAC.2007.070511
   Park J, 2009, IEEE T MULTIMEDIA, V11, P1062, DOI 10.1109/TMM.2009.2026084
   Rajashekar U, 2008, IEEE T IMAGE PROCESS, V17, P564, DOI 10.1109/TIP.2008.917218
   Said A, 1996, IEEE T CIRC SYST VID, V6, P243, DOI 10.1109/76.499834
   SHAPIRO JM, 1993, IEEE T SIGNAL PROCES, V41, P3445, DOI 10.1109/78.258085
   SHARIF MH, 2009, P IEEE INT C IM PROC
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Simon M., 2005, COMMUNICATION FADING
   Son H, 2005, IEICE T COMMUN, VE88B, P4094, DOI 10.1093/ietcom/e88-b.10.4094
   Taubman D., 2012, JPEG2000: image compression fundamentals, standards and practice, V642
   Wang HH, 2005, IEEE T CIRC SYST VID, V15, P1505, DOI 10.1109/TCSVT.2005.857305
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2003, IEEE T IMAGE PROCESS, V12, P243, DOI 10.1109/TIP.2003.809015
   Yang FZ, 2005, IEEE SIGNAL PROC LET, V12, P685, DOI 10.1109/LSP.2005.855553
   Zhai F, 2005, SIGNAL PROCESS-IMAGE, V20, P371, DOI 10.1016/j.image.2005.02.002
NR 34
TC 7
Z9 7
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2011
VL 13
IS 4
BP 813
EP 823
DI 10.1109/TMM.2011.2134840
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 805EX
UT WOS:000293710300019
DA 2024-07-18
ER

PT J
AU Lee, PJ
   Effendi
AF Lee, Pei-Jun
   Effendi
TI Nongeometric Distortion Smoothing Approach for Depth Map Preprocessing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Asymmetric smoothing filter; depth image-based rendering; edge-oriented
   smoothing process
ID IMAGES
AB Depth image-based rendering (DIBR) is a process that uses one 2-D color image and its associated depth map to render virtual view 3-D images. One of the main problems in DIBR is how to reduce holes that occur on the generated virtual view images. In general, preprocessing the whole depth image by smoothing filter before image warping can reduce the hole occurrence. However, smoothing the whole depth image not only produces some geometric distortions on the generated virtual directional images, but also increases computation time. This paper proposes an adaptive edge-oriented smoothing process to solve the above problem, in which the adaptive smoothing filter for the depth map is determined by the characteristics of the hole region with or without vertical lines in the color image. The adaptive smoothing filter contains two types: the asymmetric smoothing filter is used to reduce the geometric distortions by smoothing the hole regions with vertical lines that belong to the background; and the horizontal smoothing filter is used to reduce hole occurrence and computation time by smoothing the hole regions without vertical lines. The experiment results show that the proposed method turns out to be a good tradeoff between time saving, hole reduction rate, and virtual view quality.
C1 [Lee, Pei-Jun; Effendi] Natl Chi Nan Univ, Dept Elect Engn, Puli, Nantou, Taiwan.
C3 National Chi Nan University
RP Lee, PJ (corresponding author), Natl Chi Nan Univ, Dept Elect Engn, Puli, Nantou, Taiwan.
EM pjlee@ncnu.edu.tw; s98323906@ncnu.edu.tw
OI Lee, Pei-Jun/0000-0003-2010-0853
FU National Science Council of the Republic of China, Taiwan [NSC
   9-2221-E-260-038-MY3]
FX This work was supported by the National Science Council of the Republic
   of China, Taiwan under Contract No. NSC 9-2221-E-260-038-MY3. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Andrea Cavallaro.
CR [Anonymous], SPIE 3 DIMENSIONAL I
   [Anonymous], THESIS U N CAROLINA
   [Anonymous], 2003, P IIASTED VIIP
   Chang YL, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P1958
   Chen WY, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P1315
   Daribo I, 2007, 2007 IEEE NINTH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P312, DOI 10.1109/MMSP.2007.4412880
   Fehn C, 2004, PROC SPIE, V5599, P66, DOI 10.1117/12.583107
   Fehn C, 2004, PROC SPIE, V5291, P93, DOI 10.1117/12.524762
   FEHN C, 2002, ISOIECC1SC29WG11MPEG
   Flack J, 2003, PROC SPIE, V5006, P206, DOI 10.1117/12.474116
   IJSSELSTEIJN WA, 2000, P SPIE STER DISPL VI, P12
   Kai Che Liu, 2008, 2008 3DTV-Conference: The True Vision - Capture, Transmission and Display of 3D Video, P333
   Luo K, 2009, J ZHEJIANG UNIV-SC A, V10, P1738, DOI 10.1631/jzus.A0820806
   MCMILIAN L, 1997, THESIS U N CAROLINA
   Park YK, 2009, SIGNAL PROCESS-IMAGE, V24, P122, DOI 10.1016/j.image.2008.10.008
   Redert A, 2002, FIRST INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING VISUALIZATION AND TRANSMISSION, P313
   Tam WJ, 2004, PROC SPIE, V5599, P162, DOI 10.1117/12.583105
   Tanimoto M., 2006, P IEEE COMP VIS PATT, P172
   TORRES R, 2006, 3D MODEL OLD MAN ALF
   WOODS A, 1993, P SOC PHOTO-OPT INS, V1915, P36, DOI 10.1117/12.157041
   Yu-Cheng Fan, 2008, 2008 3DTV-Conference: The True Vision - Capture, Transmission and Display of 3D Video, P325
   Zhang L, 2004, IEEE IMAGE PROC, P2993
   Zhang L, 2005, IEEE T BROADCAST, V51, P191, DOI 10.1109/TBC.2005.846190
NR 23
TC 65
Z9 73
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2011
VL 13
IS 2
BP 246
EP 254
DI 10.1109/TMM.2010.2100372
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 738SG
UT WOS:000288661800007
DA 2024-07-18
ER

PT J
AU Hu, YQ
   Cheng, XA
   Chia, LT
   Xie, X
   Rajan, D
   Tan, AH
AF Hu, Yiqun
   Cheng, Xiangang
   Chia, Liang-Tien
   Xie, Xing
   Rajan, Deepu
   Tan, Ah-Hwee
TI Coherent Phrase Model for Efficient Image Near-Duplicate Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bag-of-word (BoW); image near-duplicate (IND); quantization; retrieval;
   TRECVID
ID OBJECT RECOGNITION
AB This paper presents an efficient and effective solution for retrieving image near-duplicate (IND) from image database. We introduce the coherent phrase model which incorporates the coherency of local regions to reduce the quantization error of the bag-of-words (BoW) model. In this model, local regions are characterized by visual phrase of multiple descriptors instead of visual word of single descriptor. We propose two types of visual phrase to encode the coherency in feature and spatial domain, respectively. The proposed model reduces the number of false matches by using this coherency and generates sparse representations of images. Compared to other method, the local coherencies among multiple descriptors of every region improve the performance and preserve the efficiency for IND retrieval. The proposed method is evaluated on several benchmark datasets for IND retrieval. Compared to the state-of-the-art methods, our proposed model has been shown to significantly improve the accuracy of IND retrieval while maintaining the efficiency of the standard bag-of-words model. The proposed method can be integrated with other extensions of BoW.
C1 [Hu, Yiqun; Cheng, Xiangang; Chia, Liang-Tien; Rajan, Deepu; Tan, Ah-Hwee] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
   [Xie, Xing] Microsoft Res Asia, Beijing 100080, Peoples R China.
C3 Nanyang Technological University; Microsoft; Microsoft Research Asia
RP Hu, YQ (corresponding author), Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
EM yqhu@ntu.edu.sg; xiangang@pmail.ntu.edu.sg; asltchia@ntu.edu.sg;
   xingx@microsoft.com; asdrajan@ntu.edu.sg; asahtan@ntu.edu.sg
RI Chia, Liang-Tien/A-9874-2008; Tan, Ah-Hwee/A-3729-2011; Rajan,
   Deepu/A-3666-2011
OI Tan, Ah Hwee/0000-0003-0378-4069
CR [Anonymous], P 14 ACM INT C MULT
   [Anonymous], TREC VIDEO RETRIEVAL
   [Anonymous], P IEEE INT C COMP VI
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   BOIMAN O, 2008, P IEEE COMP SOC C CO
   Cao L., 2007, ICCV
   GHOSH P, 2007, PLATINUM JUBILEE VOL
   Grauman K, 2005, IEEE I CONF COMP VIS, P1458
   GRAUMAN K, 2006, ADV NEURAL INFORM PR, V19, P505
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Ke Y, 2004, PROC CVPR IEEE, P506
   Ke Y., 2004, ACM MULTIMEDIA 04, P869
   Lazebnik S, 2005, IEEE T PATTERN ANAL, V27, P1265, DOI 10.1109/TPAMI.2005.151
   Lazebnik S., 2006, P IEEE CVF C COMP VI, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/CVPR.2006.68]
   Lazebnik S, 2009, IEEE T PATTERN ANAL, V31, P1294, DOI 10.1109/TPAMI.2008.138
   LEE JJ, 2008, MITCSAILT200817
   LING H, 2007, P IEEE INT C COMP VI
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Nister D., 2006, IEEE COMP SOC C COMP, P2161, DOI [10.1109/cvpr.2006.264, DOI 10.1109/CVPR.2006.264, 10.1109/CVPR]
   Savarese S., 2006, P 2006 IEEE COMPUTER, V2, P2033, DOI DOI 10.1109/CVPR.2006.102
   Sivic J, 2004, PROC CVPR IEEE, P488
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   XU D, 2007, P IEEE COMP SOC C CO
   Yuan J, 2007, PROC CVPR IEEE, P1930, DOI 10.1109/CVPR.2007.383222
   ZHANG D.-Q., 2004, PROC ACM INT C MULTI, P877, DOI DOI 10.1145/1027527.1027730
   ZHANG J, 2002, INT J COMPUT VISION, V73, P213
   Zhang J., 2005, Local features and kernels for classification of texture and object categories: An in-depth study
   Zhao WL, 2007, IEEE T MULTIMEDIA, V9, P1037, DOI 10.1109/TMM.2007.898928
   Zhao WL, 2006, LECT NOTES COMPUT SC, V4071, P72
   ZHU J, 2008, P 15 ANN ACM INT C M
NR 32
TC 20
Z9 23
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2009
VL 11
IS 8
BP 1434
EP 1445
DI 10.1109/TMM.2009.2032676
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 520KL
UT WOS:000271843000004
OA Green Published
DA 2024-07-18
ER

PT J
AU Lu, YJ
   Tian, Q
AF Lu, Yijuan
   Tian, Qi
TI Discriminant Subspace Analysis: An Adaptive Approach for Image
   Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE AdaBoost; adaptive discriminant analysis; feature re-weighting; image
   classification; multiple classifiers; relevance feedback
AB Linear discriminant analysis (LDA) and biased discriminant analysis (BDA) are two effective techniques for dimension reduction, which pay attention to different roles of the positive and negative samples in finding discriminating subspace. However, the drawbacks of these two methods are obvious: LDA has limited efficiency in classifying sample data from subclasses with different distributions, and BDA does not account for the underlying distribution of negative samples.
   In order to effectively exploit favorable attributes of both BDA and LDA and avoid their unfavorable ones, we propose a novel adaptive discriminant analysis (ADA) for image classification. ADA can find an optimal discriminative subspace with adaptation to different sample distributions.
   In addition, three novel variants and extensions of ADA are further proposed:
   1) Integrated Boosting (i.Boosting), which enhances and combines a set of ADA classifiers into a more powerful one. i.Boosting integrates feature re-weighting, relevance feedback, and AdaBoost into one framework. With affordable computational cost, i.Boosting can provide a unified and stable solution to ADA prediction result.
   2) Fast adaptive discriminant analysis (FADA). Instead of searching parameters, FADA can directly find a close-to-optimal projection very fast based on different sample distributions.
   3) Two-dimensional adaptive discriminant analysis (2DADA). As opposed to ADA, 2DADA is based on 2-D image matrix representation rather than 1-D vector. So it is simpler, more straightforward, and has lower time complexity to use for image feature extraction.
   Extensive experiments on synthetic data, UCI benchmark data sets, hand-digit data set, four facial image data sets, and COREL color image data sets show the superior performance of our proposed approaches.
C1 [Lu, Yijuan] Texas State Univ, Dept Comp Sci, San Marcos, TX 78666 USA.
   [Tian, Qi] Univ Texas San Antonio, Dept Comp Sci, San Antonio, TX 78249 USA.
C3 Texas State University System; Texas State University San Marcos;
   University of Texas System; University of Texas at San Antonio (UTSA)
RP Lu, YJ (corresponding author), Texas State Univ, Dept Comp Sci, San Marcos, TX 78666 USA.
EM yl12@txstate.edu; qitian@cs.utsa.edu
RI LU, YIJUAN/GNM-8769-2022
OI LU, YIJUAN/0000-0002-9855-8365
FU Research Enhancement Program ( REP); Texas State University; Army
   Research Office ( ARO) [W911NF-05-1-0404]; Department of Homeland
   Security ( DHS)
FX Manuscript received November 12, 2008; revised April 23, 2009. First
   published August 18, 2009; current version published October 16, 2009.
   This work was supported in part by Research Enhancement Program ( REP)
   and start-up funding from Texas State University and Army Research
   Office ( ARO) grant under W911NF-05-1-0404, and in part by the
   Department of Homeland Security ( DHS). The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Z. Jane Wang.
CR [Anonymous], 2004, ADV NEURAL INF PROCE
   [Anonymous], P SPIE STOR RETR IM
   [Anonymous], P IEEE WORKSH APPL C
   [Anonymous], 2010, UCI Machine Learning Repository
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   BREIMAN L, 1997, 504 U CAL STAT DEP
   *COREL, COREL COL IM DAT
   Duda R. O., 2000, PATTERN CLASSIFICATI
   Freund Y., 1999, Journal of Japanese Society for Artificial Intelligence, V14, P771
   Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223
   FRIEDMAN JH, 1989, J AM STAT ASSOC, V84, P165, DOI 10.2307/2289860
   Fu Y, 2008, IEEE T PATTERN ANAL, V30, P2229, DOI 10.1109/TPAMI.2008.154
   Fu Y, 2008, IEEE T IMAGE PROCESS, V17, P226, DOI 10.1109/TIP.2007.914203
   INOUE K, 2006, P ICPR HONG KONG
   Li M, 2005, PATTERN RECOGN LETT, V26, P527, DOI 10.1016/j.patrec.2004.09.007
   LU Y, 2007, P PAC RIM C MULT HON
   LU Y, 2007, P IEEE INT C AC SPEE
   LU Y, 2007, P IEEE INT C MULT EX
   Rowley HA, 1998, IEEE T PATTERN ANAL, V20, P23, DOI 10.1109/34.655647
   Rudin C, 2004, J MACH LEARN RES, V5, P1557
   Rui Y, 1998, IEEE T CIRC SYST VID, V8, P644, DOI 10.1109/76.718510
   Salton G., 1992, INTRO MODERN INFORM
   SANGUANSAT P, 2006, P ICASSP, P345
   Schatten G, 1998, J LAW MED ETHICS, V26, P29, DOI 10.1111/j.1748-720X.1998.tb01903.x
   Sim T, 2003, IEEE T PATTERN ANAL, V25, P1615, DOI 10.1109/TPAMI.2003.1251154
   Smith J.R., 1994, P IEEE INT C IM PROC
   Tian Q, 2005, PATTERN RECOGN, V38, P903, DOI 10.1016/j.patcog.2004.07.013
   TIAN Q, 2004, P INT C MULT EXP JUN
   Tongzon J, 2001, TRANSPORT RES A-POL, V35, P107, DOI 10.1016/S0965-8564(99)00049-X
   WU Y, 2002, P IEEE INT C IM PROC
   WU Y, 2000, P IEEE COMP VIS PATT
   Yan SC, 2007, IEEE T PATTERN ANAL, V29, P40, DOI 10.1109/TPAMI.2007.250598
   Yang J, 2005, PATTERN RECOGN, V38, P1125, DOI 10.1016/j.patcog.2004.11.019
   YU J, 2006, P INT C PATT REC HON
   ZHOU X, 2001, P IEEE CVPR
   Zhou XS, 2003, MULTIMEDIA SYST, V8, P536, DOI 10.1007/s00530-002-0070-3
   ZHOU XS, 1999, P IEEE INT C IM PROC
NR 37
TC 13
Z9 14
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2009
VL 11
IS 7
BP 1289
EP 1300
DI 10.1109/TMM.2009.2030632
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 506GB
UT WOS:000270761300007
DA 2024-07-18
ER

PT J
AU Ding, LF
   Tsung, PK
   Chien, SY
   Chen, WY
   Chen, LG
AF Ding, Li-Fu
   Tsung, Pei-Kuei
   Chien, Shao-Yi
   Chen, Wei-Yin
   Chen, Liang-Gee
TI Content-Aware Prediction Algorithm With Inter-View Mode Decision for
   Multiview Video Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3-D video; disparity estimation; H264/AVC; motion estimation; multiview
   video coding
ID REPRESENTATION
AB 3-D video will become one of the most significant video technologies in the next-generation television. Due to the ultra high data bandwidth requirement for 3-D video, effective compression technology becomes an essential part in the infrastructure. Thus multiview video coding (MVC) plays a critical role. However, MVC systems require much more memory bandwidth and computational complexity relative to mono-view video coding systems. Therefore, an efficient prediction scheme is necessary for encoding. In this paper, a new fast prediction algorithm, content-aware prediction algorithm (CAPA) with inter-view mode decision, is proposed. By utilizing disparity estimation (DE) to find corresponding blocks between different views, the coding information, such as rate-distortion cost, coding modes, and motion vectors, can be effectively shared and reused from the coded view channel. Therefore, the computation for motion estimation (ME) in most view channels can be greatly reduced. Experimental results show that compared with the full search block matching algorithm (FSBMA) applied to both ME and DE, the proposed algorithm saves 98.4-99.1% computational complexity of ME in most view channels with negligible quality loss of only 0.03-0.06 dB in PSNR.
C1 [Ding, Li-Fu; Tsung, Pei-Kuei; Chen, Wei-Yin; Chen, Liang-Gee] Natl Taiwan Univ, Grad Inst Elect Engn, DSP IC Design Lab, Taipei 10617, Taiwan.
   [Ding, Li-Fu; Tsung, Pei-Kuei; Chen, Wei-Yin; Chen, Liang-Gee] Natl Taiwan Univ, Dept Elect Engn, Taipei 10617, Taiwan.
   [Chien, Shao-Yi] Natl Taiwan Univ, Grad Inst Elect Engn, Dept Elect Engn, Media IC & Syst Lab, Taipei 10617, Taiwan.
C3 National Taiwan University; National Taiwan University; National Taiwan
   University
RP Ding, LF (corresponding author), Natl Taiwan Univ, Grad Inst Elect Engn, DSP IC Design Lab, Taipei 10617, Taiwan.
EM lifu@video.ee.ntu.edu.tw; iceworm@video.ee.ntu.eclu.tw;
   sychien@cc.ee.ntu.edu.tw; wychen@video.ee.ntu.edu.tw;
   lgchen@video.ee.ntu.edu.tw
OI Chien, Shao-Yi/0000-0002-0634-6294; CHEN, LIANG-GEE/0000-0001-9746-9355
FU National Science Council, Taiwan [NSC96-2622-E-002-012-CC3]; Hsing Tian
   Kong Culture and Education Development Foundation
FX Manuscript received November 14, 2007: revised June 20, 2008. Current
   version published December 10, 2008. This work was Supported in part by
   the National Science Council, Taiwan. under Grant
   NSC96-2622-E-002-012-CC3 and by the scholarship of Hsing Tian Kong
   Culture and Education Development Foundation. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Kiyoharur Aizawa..
CR Ding LF, 2006, IEEE T CIRC SYST VID, V16, P1324, DOI 10.1109/TCSVT.2006.883510
   Fujii T, 2002, PROC SPIE, V4864, P175, DOI 10.1117/12.454905
   Grammalidis N, 1998, IEEE T CIRC SYST VID, V8, P328, DOI 10.1109/76.678630
   Guo X, 2006, IEEE T CIRC SYST VID, V16, P1527, DOI 10.1109/TCSVT.2006.885724
   Heising G, 2002, IEEE IMAGE PROC, P697
   HUANG YW, 2005, P IEEE INT SOL STAT
   Isgr F., 2003, IEEE T CIRCUITS SYST, V14, P388
   *ISO IEC, 2004, JTC1SC29WG11N6501 IS
   *ISO IEC, 2006, JTC1SC29WG11N6501N78
   *ISO IEC, 2006, JTC1SC29WG11N6501W80
   *ISO IEC, 2006, JTC1SC29WG11N6501M13
   KOGA T, 1981, P NTC NOV
   LAI PL, 2006, P SPIE VISUAL COMMUN
   LI G, 2003, P 2003 JOINT C 4 INT, V4, P218
   Luo Y, 2003, IEEE T BROADCAST, V49, P14, DOI 10.1109/TBC.2003.809293
   Ohm JR, 1999, IEEE T CIRC SYST VID, V9, P389, DOI 10.1109/76.752104
   OHTA Y, 1985, IEEE T PATTERN ANAL, V7, P139, DOI 10.1109/TPAMI.1985.4767639
   Ostermann J., 2004, IEEE Circuits and Systems Magazine, V4, P7, DOI 10.1109/MCAS.2004.1286980
   PO LM, 1995, P IEEE INT C IM PROC, V1, P23
   Smolic A, 2004, IEEE IMAGE PROC, P3287
   Smolic A, 2005, P IEEE, V93, P98, DOI 10.1109/JPROC.2004.839608
   TANIMOTO M, 2004, P 2004 PICT COD S DE
   Wang RS, 2000, IEEE T CIRC SYST VID, V10, P397, DOI 10.1109/76.836284
   Wang Y., 2001, VIDEO PROCESSING COM
   Wilburn B, 2002, PROC SPIE, V4674, P29
   ZHANG C, 2004, P EUR S REND
   ZHU S, 1997, ICICS 97, P9
NR 27
TC 38
Z9 47
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2008
VL 10
IS 8
BP 1553
EP 1564
DI 10.1109/TMM.2008.2007314
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 390KS
UT WOS:000262163600011
DA 2024-07-18
ER

PT J
AU Hu, J
   Choudhury, S
   Gibson, JD
AF Hu, Jing
   Choudhury, Sayantan
   Gibson, Jerry D.
TI Video Capacity of WLANs With a Multiuser Perceptual Quality Constraint
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multiple users; perceptuial video quality; video capacity; WLAN
ID AD HOC NETWORKS; DISTORTION; TRANSMISSION
AB As wireless local area networks (WLANs) become a part of our network infrastructure, it is critical that we understand both the performance provided to the end users and the capacity of these WLANs in terms of the number of supported flows (calls). Since it is clear that video traffic, as well as voice and data, will be carried by these networks, it is particularly important that we investigate these issues for packetized video. In this paper, we investigate the video user capacity of wireless networks subject to a multiuser perceptual quality constraint. As a particular example, we study the transmission of AVC/H.264 coded video streams over an IEEE 802.11a WLAN subject to a constraint on the quality of the delivered video experienced by r% (75%, for example) of the users of the WLAN. This work appears to be the first such effort to address this difficult but important problem. Furthermore, the methodology employed is perfectly general and can be used for different networks, video codecs, transmission channels, protocols, and perceptual quality measures.
C1 [Hu, Jing; Choudhury, Sayantan; Gibson, Jerry D.] Univ Calif Santa Barbara, Dept Elect & Comp Engn, Santa Barbara, CA 91305 USA.
C3 University of California System; University of California Santa Barbara
RP Hu, J (corresponding author), Univ Calif Santa Barbara, Dept Elect & Comp Engn, Santa Barbara, CA 91305 USA.
EM jinghu@ece.ucsb.edu; sayantan@ece.ucsb.edu; gibson@ece.ucsb.edu
FU California Micro Program; Applied Signal Technology; Cisco; Dolby Labs,
   Inc.; Sony-Ericsson; Qualcomm, Inc.; NSF [CCF-0429884, CNS-0435527,
   CCF-0728646]
FX Manuscript received November 01, 2007; revised July 16, 2006. Current
   version published December 10, 2008. This work was supported by the
   California Micro Program, Applied Signal Technology, Cisco, Dolby Labs,
   Inc., Sony-Ericsson, and Qualcomm, Inc., and by NSF Grants CCF-0429884,
   CNS-0435527, and CCF-0728646. The associate editor coordinaitng the
   review of this manuscript and approving it for publication was Prof.
   Deepa Kundur.
CR [Anonymous], 2006, H264AVC SOFTWARE COO
   [Anonymous], 2003, 1 ITUT ISOIEC JTC
   [Anonymous], 1999, 80211 IEEE WG 11
   [Anonymous], 2002, METH SUBJ ASS QUAL T
   [Anonymous], 2005, H263 ITUT
   [Anonymous], IEEE CIRCUITS SYST M
   Chayat N., 1997, P802119796 IEEE
   Chiang TH, 1997, IEEE T CIRC SYST VID, V7, P246, DOI 10.1109/76.554439
   Chiaraluce F., 2004, PICTURE CODING S, P163
   Chou PA, 2006, IEEE T MULTIMEDIA, V8, P390, DOI 10.1109/TMM.2005.864313
   Côté G, 2000, IEEE J SEL AREA COMM, V18, P952, DOI 10.1109/49.848249
   GROUP VQE, 2003, QUEST OBJECTIVE METH
   Hsu CY, 1999, IEEE J SEL AREA COMM, V17, P756, DOI 10.1109/49.768193
   HU J, 2007, INT C MULT EXP JUL
   HU J, 2007, INT S MULT WIR AUG
   *IEEE, 2004, 80211ED100 IEEE
   *ISO IEC, 1381812000 ISOIEC
   *ISO IEC, 2001, 1449612001 ISOIEC
   *ITU R BT, 2004, 1683 ITUR BT
   *ITU T J, 2004, 144 ITU T J
   Kong CW, 2006, IEEE T CIRC SYST VID, V16, P338, DOI 10.1109/TCSVT.2006.870021
   Kwon DK, 2007, IEEE T CIRC SYST VID, V17, P517, DOI 10.1109/TCSVT.2007.894053
   Lee C, 2006, OPT ENG, V45, DOI 10.1117/1.2160515
   LEE CH, 2000, NEUTRON NETWORK NEWS, V10, P6
   Li Q, 2004, IEEE T MULTIMEDIA, V6, P278, DOI 10.1109/TMM.2003.822792
   Li ZG, 2004, IEEE IMAGE PROC, P745
   MA S, 2002, JOINT VIDEO TEAM JVT
   Maeda N, 2003, IEICE T COMMUN, VE86B, P300
   Nahrstedt K., 2007, MULTIMEDIA IP WIRELE
   Ortega A, 1998, IEEE SIGNAL PROC MAG, V15, P23, DOI 10.1109/79.733495
   ORTEGA A, 2007, MULTIMEDIA IP WIRELE
   Pappas T.N., 2000, HDB IMAGE VIDEO PROC
   Qiao Daji., 2002, IEEE T MOBILE COMPUT, V1
   Ribas-Corbera J, 1999, IEEE T CIRC SYST VID, V9, P172, DOI 10.1109/76.744284
   Setton E, 2005, IEEE WIREL COMMUN, V12, P59, DOI 10.1109/MWC.2005.1497859
   SHETTY N, 2006, ACM INT WIR COMM MOB
   STOCKHAMMER T, 2007, MULTIMEDIA IP WIRELE
   Stuhlmuller K., 2000, IEEE J SELECTED AREA, V18
   Sullivan GJ, 1998, IEEE SIGNAL PROC MAG, V15, P74, DOI 10.1109/79.733497
   Tu YK, 2007, IEEE T CIRC SYST VID, V17, P530, DOI 10.1109/TCSVT.2007.894041
   van der Schaar M, 2003, IEEE J SEL AREA COMM, V21, P1752, DOI 10.1109/JSAC.2003.815231
   van der Schaar M., 2007, MULTIMEDIA IP WIRELE
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Wu Y, 2005, IEEE J SEL AREA COMM, V23, P136, DOI 10.1109/JSAC.2004.837362
   WU Y, 2005, JOINT VIDEO TEAM ISO
   Zhang R, 2000, IEEE J SEL AREA COMM, V18, P966, DOI 10.1109/49.848250
   ZHU X, 2005, CONGESTION DISTORTIO
NR 47
TC 7
Z9 7
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2008
VL 10
IS 8
BP 1465
EP 1478
DI 10.1109/TMM.2008.2007329
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 390KS
UT WOS:000262163600004
DA 2024-07-18
ER

PT J
AU Argyriou, A
AF Argyriou, Antonios
TI Error-resilient video encoding and transmission in multirate wireless
   LANs
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE cross-layer design; joint source-channel coding; rate-adaptive PHY;
   real-time video encoding; video streaming; wireless LAN
ID SCALABLE VIDEO; MODE SELECTION; REAL-TIME; ALLOCATION; CHANNELS
AB In this paper, we present a cross-layer approach for video transmission in wireless LANs that employs joint source and application-layer channel coding, together with rate adaptation at the wireless physical layer (PHY). While the purpose of adopting PHY rate adaptation in modern wireless LANs like the IEEE 802.11a/b is to maximize the throughput, in this paper we exploit this feature to increase the robustness of wireless video. More specifically, we investigate the impact of adapting the PHY transmission rate, thus changing the throughput and packet loss channel characteristics, on the rate-distortion performance of a transmitted video sequence. To evaluate the video quality at the decoder, we develop a cross-layer modeling framework that considers jointly the effect of application-layer joint source-channel coding (JSCC), error concealment, and the PHY transmission rate. The resulting models are used by an optimization algorithm that calculates the optimal JSCC allocation for each video frame, and PHY transmission rate for each outgoing transport packet. The comprehensive simulation results obtained with the H.264/AVC codec demonstrate considerable increase in the PSNR of the decoded video when compared with a system that employs separately JSCC and PHY rate adaptation. Furthermore, our performance analysis indicates that the optimal PHY transmission rate calculated by the proposed algorithm, can be significantly different when compared with rate adaptation algorithms that target throughput improvement.
C1 Philips Res Labs, NL-5656 AE Eindhoven, Netherlands.
C3 Philips; Philips Research
RP Argyriou, A (corresponding author), Philips Res Labs, NL-5656 AE Eindhoven, Netherlands.
EM anargyr@ieee.org
RI Argyriou, Antonios/J-5170-2012; Argyriou, Antonios/AAF-9586-2021
OI Argyriou, Antonios/0000-0002-2510-3124; Argyriou,
   Antonios/0000-0002-2510-3124
CR Alouini MS, 2000, WIRELESS PERS COMMUN, V13, P119, DOI 10.1023/A:1008979107539
   [Anonymous], 1999, 80211 IEEE WG 11
   [Anonymous], 2003, ADV VID COD GEN AUD
   [Anonymous], JVT REF SOFTW
   ARGYRIOU A, 2007, P IEEE MULT SIGN PRO
   Argyriou A, 2007, SIGNAL PROCESS-IMAGE, V22, P374, DOI 10.1016/j.image.2007.01.001
   Cheung G, 2000, IEEE T IMAGE PROCESS, V9, P340, DOI 10.1109/83.826773
   CHOI J, 2007, P IEEE INFOCOM ANCH
   Choudhury S, 2007, IEEE J SEL AREA COMM, V25, P796, DOI 10.1109/JSAC.2007.070515
   Côté G, 2000, IEEE J SEL AREA COMM, V18, P952, DOI 10.1109/49.848249
   FARVARDIN N, 1987, IEEE T INFORM THEORY, V33, P827, DOI 10.1109/TIT.1987.1057373
   HARTANTO F, 1999, P 10 IEEE WORKSH LOC
   He ZH, 2002, IEEE T CIRC SYST VID, V12, P511, DOI 10.1109/TCSVT.2002.800313
   Heiskala J., 2001, OFDM WIRELESS LANS T
   *IEEE, 1997, P802119796 IEEE
   Kamerman Ad., 1997, BELL LABS TECH J
   Proakis J., 2008, Digital Communication, Vthird
   PURSLEY MB, 1987, IEEE T COMMUN, V35, P112
   Qiao Daji., 2002, IEEE T MOBILE COMPUT, V1
   Stuber G. L., 2001, PRINCIPLES MOBILE CO
   van der Schaar M, 2005, IEEE WIREL COMMUN, V12, P50, DOI 10.1109/MWC.2005.1497858
   van der Schaar M., 2007, MULTIMEDIA IP WIRELE
   Zhai F, 2006, IEEE T IMAGE PROCESS, V15, P40, DOI 10.1109/TIP.2005.860353
   Zhang Jianhai, 2007, Marine Science Bulletin (Beijing), V9, P3
   Zhang Q, 2004, IEEE T CIRC SYST VID, V14, P1049, DOI 10.1109/TCSVT.2004.831966
   Zhang R, 2000, IEEE J SEL AREA COMM, V18, P966, DOI 10.1109/49.848250
NR 26
TC 21
Z9 22
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2008
VL 10
IS 5
BP 691
EP 700
DI 10.1109/TMM.2008.922776
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 334LR
UT WOS:000258223800003
DA 2024-07-18
ER

PT J
AU Li, CY
   Hsu, CT
AF Li, Chueh-Yu
   Hsu, Chiou-Ting
TI Image retrieval with relevance feedback based on graph-theoretic region
   correspondence estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE content-based image retrieval; generalized adjacency matrix; inexact
   graph matching; maximum likelihood estimation; region correspondence;
   relevance feedback
ID FRAMEWORK; EFFICIENT; ALGORITHM; SPACE
AB This paper presents a graph-theoretic approach for interactive region-based image retrieval. When dealing with image matching problems, we use graphs to represent images, transform the region correspondence estimation problem into an inexact graph matching problem, and propose an optimization technique to derive the solution. We then define the image distance in terms of the estimated region correspondence. In the relevance feedback steps, with the estimated region correspondence, we propose to use a maximum likelihood method to re-estimate the ideal query and the image distance measurement. Experimental results show that the proposed graph-theoretic image matching criterion outperforms the other methods incorporating no spatially adjacent relationship within images. Furthermore, our maximum likelihood method combined with the estimated region correspondence improves the retrieval performance in feedback steps.
C1 [Li, Chueh-Yu; Hsu, Chiou-Ting] Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu, Taiwan.
   [Li, Chueh-Yu] Compal Commun Inc, Taipei, Taiwan.
C3 National Tsing Hua University; Compal Electronics
RP Li, CY (corresponding author), Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu, Taiwan.
EM leejueyu@gmail.com
OI Hsu, Chiou-Ting/0000-0001-8857-2481
FU National Science Council of Taiwan [NSC95-2221-E-007-221,
   NSC95-2220-E-007-034]
FX This work was supported in part by the National Science Council of
   Taiwan. R.O.C., under Contracts NSC95-2221-E-007-221 and
   NSC95-2220-E-007-034. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Anna Hac.
CR Aggarwal G, 2002, IEEE T MULTIMEDIA, V4, P201, DOI 10.1109/TMM.2002.1017734
   BAEZAYATES R, 2000, P IEEE INT S STR PRO
   Bimbo A.D., 1999, Visual Information Retrieval
   Chen YX, 2002, IEEE T PATTERN ANAL, V24, P1252, DOI 10.1109/TPAMI.2002.1033216
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   FISCHER B, 2004, P SPIE MED IM
   GREENSPAN H, 2000, P IEEE WORKSH CONT B
   Grigorova A, 2007, IEEE T MULTIMEDIA, V9, P1183, DOI 10.1109/TMM.2007.902828
   Guo GD, 2002, IEEE T NEURAL NETWOR, V13, P811, DOI 10.1109/TNN.2002.1021882
   He XF, 2003, IEEE T CIRC SYST VID, V13, P39, DOI 10.1109/TCSVT.2002.808087
   Horn R., 1985, Matrix Analysis, DOI [10.1017/CBO9780511810817, 10.1017/CBO9781139020411]
   Hsieh JW, 2003, IEEE T IMAGE PROCESS, V12, P1404, DOI 10.1109/TIP.2003.816013
   Hsu CT, 2005, IEEE T IMAGE PROCESS, V14, P1617, DOI 10.1109/TIP.2005.852202
   ISHIKAWA Y, 1998, P INT C VER LARG DAT
   Jing F, 2004, IEEE T IMAGE PROCESS, V13, P699, DOI 10.1109/TIP.2004.826125
   Jing F, 2004, IEEE T CIRC SYST VID, V14, P672, DOI 10.1109/TCSVT.2004.826775
   KAILING K, 2004, P INT C KNOWL BAS IN
   KHERFI ML, 2002, P IEEE INT C PATT RE
   Kherfi ML, 2007, IEEE T MULTIMEDIA, V9, P893, DOI 10.1109/TMM.2007.893349
   Kiranyaz S, 2007, IEEE T MULTIMEDIA, V9, P102, DOI 10.1109/TMM.2006.886362
   LI CY, 2004, P IEEE INT C IM PROC
   LI CY, 2004, P INT C PATT REC
   LI CY, 2005, P IEEE INT C MULT EX
   LI J, 2000, P ACM MULT
   Luo B, 2001, IEEE T PATTERN ANAL, V23, P1120, DOI 10.1109/34.954602
   MITCHELL T, 1989, ANNU REV COMPUT SCI, V4, P417
   Muneesawang P, 2004, IEEE T MULTIMEDIA, V6, P703, DOI 10.1109/TMM.2004.834866
   RUI Y, 2000, P IEEE INT C COMP VI
   Shapiro L. G., 2001, COMPUTER VISION
   Tao DC, 2006, IEEE T MULTIMEDIA, V8, P716, DOI 10.1109/TMM.2005.861375
   Tao DC, 2006, IEEE T PATTERN ANAL, V28, P1088, DOI 10.1109/TPAMI.2006.134
   Vasconcelos N, 2004, IEEE T SIGNAL PROCES, V52, P2322, DOI 10.1109/TSP.2004.831125
   ZHANG C, 2004, P IEEE INT C MULT EX
   Zhang RF, 2007, IEEE T IMAGE PROCESS, V16, P562, DOI 10.1109/TIP.2006.888350
NR 34
TC 36
Z9 38
U1 1
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2008
VL 10
IS 3
BP 447
EP 456
DI 10.1109/TMM.2008.917421
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 342EJ
UT WOS:000258767100013
DA 2024-07-18
ER

PT J
AU Liu, KH
   Weng, MF
   Tseng, CY
   Chuang, YY
   Chen, MS
AF Liu, Ken-Hao
   Weng, Ming-Fang
   Tseng, Chi-Yao
   Chuang, Yung-Yu
   Chen, Ming-Syan
TI Association and temporal rule mining for post-filtering of semantic
   concept detection in video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE association rule mining; content-based video retrieval and mining;
   post-filtering; semantic concept detection; temporal rule mining
ID FRAMEWORK; RETRIEVAL
AB Automatic semantic concept detection in video is important for effective content-based video retrieval and mining and has gained great attention recently. In this paper, we propose a general post-filtering framework to enhance robustness and accuracy of semantic concept detection using association and temporal analysis for concept knowledge discovery. Co-occurrence of several semantic concepts could imply the presence of other concepts. We use association mining techniques to discover such inter-concept association relationships from annotations. With discovered concept association rules, we propose a strategy to combine associated concept classifiers to improve detection accuracy. In addition, because video is often visually smooth and semantically coherent, detection results from temporally adjacent shots could be used for the detection of the current shot. We propose temporal filter designs for inter-shot temporal dependency mining to further improve detection accuracy. Experiments on the TRECVID 2005 dataset show our post-filtering framework is both efficient and effective in improving the accuracy of semantic concept detection in video. Furthermore, it is easy to integrate our framework with existing classifiers to boost their performance.
C1 [Liu, Ken-Hao; Tseng, Chi-Yao; Chen, Ming-Syan] Natl Taiwan Univ, Dept Elect Engn, Taipei 10764, Taiwan.
   [Weng, Ming-Fang; Chuang, Yung-Yu] Natl Taiwan Univ, Dept Comp Sci & Informat, Taipei 10764, Taiwan.
C3 National Taiwan University; National Taiwan University
RP Liu, KH (corresponding author), Natl Taiwan Univ, Dept Elect Engn, Taipei 10764, Taiwan.
EM kenliu@arbor.ee.mu.edu.tw; mfueng@cmlab.csie.ntu.edu.tw;
   cytseng@arbor.ee.ntu.edu.tw; cyy@csie.ntu.edu.tw;
   mschen@cc.ee.ntu.edu.tw
RI mingfang, weng/HSG-7737-2023
OI mingfang, weng/0000-0003-1636-4717; Chen, Ming-Syan/0000-0002-0711-8197;
   Chuang, Yung-Yu/0000-0002-1383-0017
CR Agrawal R., 1994, P INT VLDB C VLDB 94, P487, DOI DOI 10.5555/645920.672836
   [Anonymous], 2006, ACM INT C MULTIMEDIA, DOI [10.1145/1180639.1180727, DOI 10.1145/1180639.1180727]
   [Anonymous], 2006, P 8 ACM INT WORKSHOP
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chen MS, 1996, IEEE T KNOWL DATA EN, V8, P866, DOI 10.1109/69.553155
   DATTA R, IN PRESS ACM COMPUT
   Ebadollahi S, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P881, DOI 10.1109/ICME.2006.262691
   Han J., 2006, DATA MINING CONCEPTS, DOI 10.1016/C2009-0-61819-5
   Jiang W, 2007, INT CONF ACOUST SPEE, P949
   Kender JR, 2005, PROC CVPR IEEE, P1174
   Lew MS, 2006, ACM T MULTIM COMPUT, V2, P1, DOI 10.1145/1126004.1126005
   Li WM, 2001, 2001 IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P369, DOI 10.1109/ICDM.2001.989541
   MALIK HH, 2006, P 6 INT C WEB ENG, P48
   Manning C.D., 1999, FDN STAT NATURAL LAN
   Naphade MR, 2001, IEEE T MULTIMEDIA, V3, P141, DOI 10.1109/6046.909601
   Naphade MR, 2002, IEEE T CIRC SYST VID, V12, P40
   Naphade MR, 2002, IEEE T NEURAL NETWOR, V13, P793, DOI 10.1109/TNN.2002.1021881
   Platt JC, 2000, ADV NEUR IN, P61
   Quinlan Ross, 1993, C4 5 PROGRAMS MACHIN
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Smith JR, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL II, PROCEEDINGS, P445
   Snoek CGM, 2006, IEEE T PATTERN ANAL, V28, P1678, DOI 10.1109/TPAMI.2006.212
   Snoek CGM, 2005, MULTIMED TOOLS APPL, V25, P5, DOI 10.1023/B:MTAP.0000046380.27575.a5
   Thomas T. M.CoverandJ. A., 1991, ELEMENTS INFORM THEO, DOI 10.1002/0471200611
   Xie LX, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P297, DOI 10.1109/ICME.2006.262457
   Yan R, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P301, DOI 10.1109/ICME.2006.262458
   Yang JB, 2006, IPFA 2006: PROCEEDINGS OF THE 13TH INTERNATIONAL SYMPOSIUM ON THE PHYSICAL & FAILURE ANALYSIS OF INTEGRATED CIRCUITS, P33
   Yin XX, 2003, SIAM PROC S, P331
NR 28
TC 43
Z9 50
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2008
VL 10
IS 2
BP 240
EP 251
DI 10.1109/TMM.2007.911826
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 254HC
UT WOS:000252576700008
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Arya, A
   DiPaola, S
AF Arya, Ali
   DiPaola, Steve
TI Face modeling and animation language for MPEG-4 XMT framework
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE face animation; language; modeling; MPEG; XML; XMT
ID PART 1
AB This paper proposes FML, an XML-based face modeling and animation language. FML provides a structured content description method for multimedia presentations based on face animation. The language can be used as direct input to compatible players, or be compiled within MPEG-4 XMT framework to create MPEG-4 presentations. The language allows parallel and sequential action description, decision-making and dynamic event-based scenarios, model configuration, and behavioral template definition. Facial actions include talking, expressions, head movements, and low-level MPEG-4 FAPs. The ShowFace and iFACE animation frameworks are also reviewed as example FML-based animation systems.
C1 Carleton Univ, Sch Informat Technol, Ottawa, ON K1S 5B6, Canada.
   Simon Fraser Univ, Sch Interact Arts & Technol, Surrey, BC, England.
C3 Carleton University
RP DiPaola, S (corresponding author), Carleton Univ, Sch Informat Technol, Ottawa, ON K1S 5B6, Canada.
EM sdipaola@sfu.ca
RI DiPaola, Stephen/M-5861-2013
OI DiPaola, Stephen/0000-0001-7549-9545
CR ANKENEY J, 1995, TV TECHNOL
   [Anonymous], 1999, P ACM SIGGRAPH
   ARAFA Y, 2002, P 1 INT C AUT AG MUL
   ARYA A, 2003, INT J IMAGE GRAPH SP, V3, P345
   ARYA A, 2003, IASTED INT C COMP GR
   ARYA A, 2005, P 14 INT C CENTR EUR
   Battista S, 1999, IEEE MULTIMEDIA, V6, P74, DOI 10.1109/93.809236
   BREGLER C, 1997, ACM COMPUT GRAPH
   Bulterman D. C. A., 2001, IEEE Multimedia, V8, P82, DOI 10.1109/93.959106
   Cassell J., 2001, P ACM SIGGRAPH
   DECAROLIS B, 2002, P 1 INT C AUT AG MUL
   DIPAOLA S, 2005, P EUR C EL IM VIS AR
   Ekman P, 1978, FACIAL ACTION CODING
   EZZAT T, 1998, P IEEE C COMP AN
   FUNGE J, 1999, P ACM SIGGRAPH
   GRAF HP, 2000, P IEEE C AUT FAC GES
   HIRZALLA N, 1995, IEEE MULTIMEDIA, V2, P24, DOI 10.1109/93.410508
   KALLMANN M, 1999, P IEEE C COMP AN
   KIM M, 2000, P ACM C MULT
   LAWTON G, 2000, IEEE COMPUT, V33, P120
   LEE WS, 1999, P IEEE C COMP AN
   LITTLE TDC, 1994, MULTIMEDIA SYSTEMS
   MARRIOTT A, 2002, P 1 INT C AUT AG MUL
   Nack F, 1999, IEEE MULTIMEDIA, V6, P65, DOI 10.1109/93.790612
   Pandzic I., 2002, MPEG-4 Facial Animation: The Standard, Implementation and Applications
   PANDZIC IS, 2001, P INT C AUGM VIRT RE
   Parke F. I., 2000, COMPUTER FACIAL ANIM
   PRENDINGER H, 2002, APPL ARTIF INTELL, V16
NR 28
TC 3
Z9 4
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2007
VL 9
IS 6
BP 1137
EP 1146
DI 10.1109/TMM.2007.902862
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 215XJ
UT WOS:000249842000005
OA Green Published
DA 2024-07-18
ER

PT J
AU Chang, YW
   Fang, HC
   Chen, CC
   Lian, CJ
   Chen, LG
AF Chang, Yu-Wei
   Fang, Hung-Chi
   Chen, Chun-Chia
   Lian, Chung-Jr
   Chen, Liang-Gee
TI Word-level parallel architecture of JPEG 2000 embedded block coding
   decoder
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE embedded block coding with optimized truncation; image compression; JPEG
   2000
ID IMAGE COMPRESSION; JPEG2000; EBCOT
AB This paper presents a word-level decoding architecture of embedded block coding in JPEG 2000. This architecture decodes one coefficient per cycle based on the proposed word-level decoding algorithm. This algorithm eliminates state variable memories by decoding all bit-planes in parallel. The proposed column-switching scan order overcomes intra bit-plane dependency and inter bit-plane dependency to enable parallel processing. Implementation results show that the proposed architecture is capable of decoding 54 MSamples/s at 54 MHz, which can support HDTV 720p (1280 x 720, 4:2:2) decoding at 30 frames/s.
C1 Natl Taiwan Univ, Grad Inst Elect Engn, Taipei 106, Taiwan.
   Natl Taiwan Univ, Dept Elect Engn, Taipei 106, Taiwan.
C3 National Taiwan University; National Taiwan University
RP Chang, YW (corresponding author), Natl Taiwan Univ, Grad Inst Elect Engn, Taipei 106, Taiwan.
EM wayne@video.ce.ntu.edu.tw; honchi@video.ce.ntu.edu.tw;
   chunchia@video.ee.ntu.edu.tw; cjlian@video.ee.ntu.edu.tw;
   lgchen@video.ee.ntu.edu.tw
OI CHEN, LIANG-GEE/0000-0001-9746-9355
CR Andra K, 2003, IEEE T CIRC SYST VID, V13, P209, DOI 10.1109/TCSVT.2003.809834
   [Anonymous], 2002, JPEG2000: Image Compression Fundamentals, Standards, and Practice
   CHANG YW, 2006, IEEE INT SOL STAT CI, P404
   CHANG YW, 2006, P IEEE INT C AC SPEE, V2, P449
   CHEN HH, 2000, P IEEE INT S CIRC SY, V4, P329
   Chiang JS, 2002, 2002 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL I, PROCEEDINGS, P773
   CHIANG JS, 2004, P IEEE INT S CIRC SY, V3, P865
   Fang HC, 2005, IEEE T CIRC SYST VID, V15, P1086, DOI 10.1109/TCSVT.2005.852618
   FANG HC, 2003, P IEEE INT S CIRC SY, V2, P736
   Fang WS, 2005, MINI-REV MED CHEM, V5, P1
   GUPTA AK, 2005, P IEEE MIDW S CIRC S, P63
   HSIAO YT, 2002, P IEEE INT S CIRC SY, V5, P133
   *ISO IEC, 2000, JTC1SC29WG1N1684 ISO
   *ISO IEC, 2000, JTC1SC29WG1N1855 ISO
   *ISO IEC, 2000, JTC1SC29WG1N1271 ISO
   Lai YK, 2005, IEEE ICCE, P449, DOI 10.1109/ICCE.2005.1429911
   LI Y, 2005, P IEEE INT S CIRC SY, V5, P5198
   Lian CJ, 2003, IEEE T CIRC SYST VID, V13, P219, DOI 10.1109/TCSVT.2003.809833
   MITCHELL JL, 1988, IBM J RES DEV, V32, P753, DOI 10.1147/rd.326.0753
   ONO F, 1989, P IEEE GLOB TEL C GL, P255
   Pastor IM, 2005, CURR ORG CHEM, V9, P1, DOI 10.2174/1385272053369385
   Pastuszak G, 2005, IEEE T CIRC SYST VID, V15, P1182, DOI 10.1109/TCSVT.2005.852720
   Pennebaker W. B., 1992, JPEG STILL IMAGE DAT
   Skodras A, 2001, IEEE SIGNAL PROC MAG, V18, P36, DOI 10.1109/79.952804
   Taubman D, 2000, IEEE T IMAGE PROCESS, V9, P1158, DOI 10.1109/83.847830
   WU B, 2005, P IEEE INT C AC SPEE, V5, P9
NR 26
TC 3
Z9 6
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2007
VL 9
IS 6
BP 1103
EP 1112
DI 10.1109/TMM.2007.902822
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 215XJ
UT WOS:000249842000002
DA 2024-07-18
ER

PT J
AU Wu, YD
   Ma, D
   Deng, RH
AF Wu, Yongdong
   Ma, Di
   Deng, Robert H.
TI Flexible access control to JPEG 2000 image code-streams
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
AB JPEG 2000 is an international standard for still image compression in the 21st century. Part 8 of the standard, named JPSEC, is concerned with all the security aspects, in particular to access control and authentication. This paper presents a novel access control scheme for JPEG 2000 image code-streams. The proposed scheme is secure against collusion attacks and highly efficient. The scheme is also very flexible, allowing access control to JPEG 2000 image code-streams according to any combination of resolution., quality layer and region of interest. The "encrypt once, decrypt many ways" property of our scheme is designed to work seamlessly with the "compress once, decompress many ways" feature of the JPEG 2000 image code-streams. Our prototype implementation shows that the scheme is practical and is completely compatible with the core part of the JPEG 2000 standard.
C1 Inst Infocomm Res, Singapore 119613, Singapore.
   Univ Calif Irvine, Donald Bren Sch Informat & Comp Sci, Irvine, CA 92697 USA.
   Singapore Management Univ, Sch Informat Syst, Singapore 188065, Singapore.
C3 Agency for Science Technology & Research (A*STAR); A*STAR - Institute
   for Infocomm Research (I2R); University of California System; University
   of California Irvine; Singapore Management University
RP Wu, YD (corresponding author), Inst Infocomm Res, Singapore 119613, Singapore.
EM wydong@i2r.a.star.edu.sg; dma1@ics.uci.edu; robertdeng@smu.edu.sg
RI DENG, Robert H./E-8547-2012
OI Deng, Robert/0000-0003-3491-8146; Ma, Di/0000-0001-7330-4716; Wu,
   Yongdong/0000-0002-0850-724X
CR AKI SG, 1983, ACM T COMPUT SYST, V1, P239
   [Anonymous], 2000, JPEG 2000 IMAGE COMP
   [Anonymous], CACM
   BERTINO E, 1993, ADV DATABASE SYST, V759, P17
   CHICK GC, 1990, LECT NOTES COMPUT SC, V435, P316
   Grosbois R, 2001, PROC SPIE, V4472, P95, DOI 10.1117/12.449744
   Harn L., 1990, Computers & Security, V9, P539, DOI 10.1016/0167-4048(90)90132-D
   *ISO IEC, 2007, 154448 ISO IEC
   *ISO IEC, 2004, 154449 ISO IEC FDIS
   *ITU, T800 ITU
   MACKINNON SJ, 1985, IEEE T COMPUT, V34, P797, DOI 10.1109/TC.1985.1676635
   MERKLE RC, 1990, LECT NOTES COMPUT SC, V435, P218, DOI 10.1007/0-387-34805-0_21
   MORI R, 1990, T IEIEC E, V73
   *NAT I STAND TECHN, 1995, 1801 SHS FIPS PUBL
   OHTA K, 1991, P ADV CRYPTOLOGY EUR, V473, P316
   Rabbani M, 2002, SIGNAL PROCESS-IMAGE, V17, P3, DOI 10.1016/S0923-5965(01)00024-8
   Ray I., 2002, P 7 ACM S ACCESS CON, P65
   Rivest RonaldL., 1992, The RC4 encryption algorithm
   SANDHU RS, 1993, COMPUTER, V26, P9, DOI 10.1109/2.241422
   SANDHU RS, 1994, IEEE COMMUN MAG, V32, P40, DOI 10.1109/35.312842
   SANDHU RS, 1988, INFORM PROCESS LETT, V27, P95, DOI 10.1016/0020-0190(88)90099-3
   WEE S, 2003, P IEEE INT C IM PROC, V1, P14
   Wu YD, 2004, IEEE IMAGE PROC, P3439
   Wu YD, 2004, IEEE IMAGE PROC, P3447
   Zhu BB, 2006, CONSUM COMM NETWORK, P1124
NR 25
TC 7
Z9 8
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2007
VL 9
IS 6
BP 1314
EP 1324
DI 10.1109/TMM.2007.902865
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 215XJ
UT WOS:000249842000019
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Zhao, WL
   Ngo, CW
   Tan, HK
   Wu, X
AF Zhao, Wan-Lei
   Ngo, Chong-Wah
   Tan, Hung-Khoon
   Wu, Xiao
TI Near-duplicate keyframe identification with interest point matching and
   pattern learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE local interest point matching; near-duplicate detection; nearest
   neighbor search
AB This paper proposes a new approach for near-duplicate keyframe (NDK) identification by matching, filtering and learning of local interest points (LIPs) with PCA-SIFT descriptors. The issues in matching reliability, filtering efficiency and learning flexibility are novelly exploited to delve into the potential of LIP-based retrieval and detection. In matching, we propose a one-to-one symmetric matching (OOS) algorithm which is found to be highly reliable for NDK identification, due to its capability in excluding false LIP matches compared with other matching strategies. For rapid filtering, we address two issues: speed efficiency and search effectiveness, to support OOS with a new index structure called LIP-IS. By exploring the properties of PCA-SIFT, the filtering capability and speed of LIP-IS are asymptotically estimated and compared to locality sensitive hashing (LSH). Owing to the robustness consideration, the matching of LIPs across keyframes forms vivid patterns that are utilized for discriminative learning and detection with support vector machines. Experimental results on TRECVID-2003 corpus show that our proposed approach outperforms other popular methods including the techniques with LSH in terms of retrieval and detection effectiveness. In addition, the proposed LIP-IS successfully speeds up OOS for more than ten times and possesses several avorable properties compared to LSH.
C1 City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
C3 City University of Hong Kong
RP Zhao, WL (corresponding author), City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
EM cwngo@cs.cityu.edu.hk
OI Ngo, Chong Wah/0000-0003-4182-8261; Wu, Xiao/0000-0002-8322-8558
CR [Anonymous], P TRECVID
   [Anonymous], 2004, P 12 ANN ACM INT C M
   BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007
   Bertsekas D., 1992, Computational optimization and applications, V1, P7
   CHANG E, 1998, P SPIE MULTIMEDIA ST
   Cheung SCS, 2005, IEEE T MULTIMEDIA, V7, P524, DOI 10.1109/TMM.2005.846906
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   GEORGESCU B, 2003, P INT C COMP VIS
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Guttman A., 1984, SIGMOD Record, V14, P47, DOI 10.1145/971697.602266
   Haitsma J, 2002, ISMIR 2002 3 INT C M
   Kashino K, 2003, IEEE T MULTIMEDIA, V5, P348, DOI 10.1109/TMM.2003.813281
   Ke Y, 2004, PROC CVPR IEEE, P506
   Ke Y., 2004, ACM MULTIMEDIA 04, P869
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   NGO CW, 2006, ACM MULTIMEDIA C
   Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054
   SCHRIJVER A, 2003, COMBINATORIAL OPTIMI, VA, P267
   Seo JS, 2004, SIGNAL PROCESS-IMAGE, V19, P325, DOI 10.1016/j.image.2003.12.001
   Sivic J., 2003, P IEEE INT C COMPUTE, P1
   Smith S., 1992, BMVC92. Proceedings of the British Machine Vision Conference, P139
   TRECVID, TREC VID RETR EV
   Wu X, 2006, IEEE SIGNAL PROC MAG, V23, P59
   ZHANG D.-Q., 2004, PROC ACM INT C MULTI, P877, DOI DOI 10.1145/1027527.1027730
NR 26
TC 108
Z9 120
U1 1
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2007
VL 9
IS 5
BP 1037
EP 1048
DI 10.1109/TMM.2007.898928
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 193ZX
UT WOS:000248314800013
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Begen, AC
   Altunbasak, Y
AF Begen, Ali C.
   Altunbasak, Yucel
TI An adaptive media-aware retransmission timeout estimation method for
   low-delay packet video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE autoregressive models; delay jitter; linear prediction; packet delay;
   retransmission timeout (RTO) estimation
AB Time-constrained error recovery is an integral component of reliable low-delay video applications. Regardless of the error-control method adopted by the application, unacknowledged or missing packets must be quickly identified as lost or delayed, so that necessary actions can be taken by the server/client on time. Historically, this problem has been referred to as retransmission timeout (RTO) estimation. Earlier studies show that existing RTO estimators suffer from either long loss detection times or a large number of spurious timeouts. The goal of this study is to address these problems by developing an RTO estimation method specifically tailored for low-delay video applications. In the media-unaware mode, this method exploits the temporal dependence in packet delay to optimally manage the tradeoff between the amount of overwaiting and redundant retransmission rate. As opposed to existing methods, our approach is completely adaptive to the source video characteristics and time-varying network conditions, and does not use any preset parameters. In the media-aware mode, on the other hand, the timeout estimates are jointly optimized based on the importance and urgency of the video packets such that the rendering quality is maximized under the given rate constraints. With a comprehensive set of simulation and experimental results, we show that both the media-unaware and media-aware RTO estimators detect lost packets faster and more accurately than their rivals. Furthermore, our results also substantiate the fact that the media-aware RTO estimator outperforms all other RTO estimators in terms of video quality.
C1 Georgia Inst Technol, Sch Elect & Comp Engn, Ctr Signal & Image Proc, Atlanta, GA 30332 USA.
C3 University System of Georgia; Georgia Institute of Technology
RP Begen, AC (corresponding author), Georgia Inst Technol, Sch Elect & Comp Engn, Ctr Signal & Image Proc, Atlanta, GA 30332 USA.
EM acbegen@ece.gatech.edu; yucel@ece.gatech.edu
RI Begen, Ali C./R-5897-2016
OI Begen, Ali C./0000-0002-0835-3017
CR ALLMAN M, 1999, P ACM SIGCOMM
   BEGEN AC, 2005, IEEE INT C MULTM EXP
   BEGEN AC, 2004, P PICT COD S PCS
   Brockwell P.J., 2003, INTRO TIME SERIES FO, V2nd
   Chou PA, 2006, IEEE T MULTIMEDIA, V8, P390, DOI 10.1109/TMM.2005.864313
   CHOU PA, 2001, MSRTR200135 MICR RES
   GOEL A, 2002, P 10 IEEE INT WORKSH
   Jacobson V., 1988, P ACM SIGCOMM
   JIANG W, 2000, P ACM NOSSDAV
   Kalman M, 2004, IEEE T CIRC SYST VID, V14, P841, DOI 10.1109/TCSVT.2004.828335
   KALMAN M, 2004, P IEEE INT C MULT EX
   KARN P, 1987, P ACM SIGCOMM
   Kim T, 2005, IEEE J SEL AREA COMM, V23, P344, DOI 10.1109/JSAC.2004.839390
   Li Q, 2001, IEEE ACM T NETWORK, V9, P578, DOI 10.1109/90.958327
   LOGUINOV D, 2002, P IEEE INT C COMP CO
   LOGUINOV D, 2001, P IEEE INT C COMP CO
   Ludwig R, 2000, ACM SIGCOMM COMP COM, V30, P17, DOI 10.1145/382179.383014
   Ma LP, 2004, IEEE SIGNAL PROC LET, V11, P569, DOI 10.1109/LSP.2004.827957
   MCCANNE S, NETWORK SIMULATOR ON
   MUKHERJEE A, 1992, MSCIS9283 COMP INT L
   PAPADOPOULOS C, 1996, P ACM NOSSDAV
   RHEE I, 1998, P ACM SIGCOMM
   SINHA R, 2004, P ACM NOSSDAV
   Stoica P, 2004, IEEE SIGNAL PROC MAG, V21, P36, DOI 10.1109/MSP.2004.1311138
   Zegura EW, 1997, IEEE ACM T NETWORK, V5, P770, DOI 10.1109/90.650138
NR 25
TC 4
Z9 13
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2007
VL 9
IS 2
BP 332
EP 347
DI 10.1109/TMM.2006.886282
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 135CO
UT WOS:000244131900012
DA 2024-07-18
ER

PT J
AU Kirenko, IO
   van der Vleuten, RJ
   Shao, L
AF Kirenko, Ihor O.
   van der Vleuten, Rene J.
   Shao, Ling
TI Optimizing scalable video compression for efficient implementation on a
   VLIW media processor
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE embedded compression; scalable video coding; Very-Long Instruction Word
   (VLIW) processor
AB State-of-the-art digital multimedia platforms contain a powerful Very-Long Instruction Word (VLIW) processing core. To obtain optimum performance on such a platform, a media-processing algorithm should exploit all the advantages provided by the VLIW processor's architecture. This paper focuses on a method for adapting a video compression algorithm for implementation on a VLIW processor using algorithm-architecture co-design. We demonstrate the importance of our adaptation approach and show the improved efficiency of the algorithm implementation.
C1 Philips Res Labs, NL-5656 AE Eindhoven, Netherlands.
C3 Philips; Philips Research
RP Kirenko, IO (corresponding author), Philips Res Labs, NL-5656 AE Eindhoven, Netherlands.
EM ihor.kirenko@philips.com; rene.van.der.vleuten@philips.com;
   l.shao@philips.com
RI Shao, Ling/D-3535-2011
OI Shao, Ling/0000-0002-8264-6117
CR de With PHN, 1998, IEEE T CONSUM ELECTR, V44, P545, DOI 10.1109/30.713162
   Kim HY, 1997, IEEE T CONSUM ELECTR, V43, P1074, DOI 10.1109/30.642374
   KLEIHORST R, 1997, J VLSI SIGNAL PROC, P241
   Kleihorst RP, 2000, J VLSI SIG PROC SYST, V24, P31, DOI 10.1023/A:1008162426925
   LOEFFLER C, 1989, P INT C AC SPEECH SI, P988, DOI DOI 10.1109/ICASSP.1989.266596
   *PHIL SEM, 1999, PHIL TRIM DOC SET BO
   RATHNAM S, 1996, P COMPCON 96, P319
   Van der Vleuten R. J., 2000, Proceedings DCC 2000. Data Compression Conference, P23, DOI 10.1109/DCC.2000.838142
   VANDERVLEUTEN RJ, 2000, P INT C IM PROC IEEE, V3, P837
NR 9
TC 2
Z9 3
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2007
VL 9
IS 2
BP 429
EP 434
DI 10.1109/TMM.2006.887996
PG 6
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 135CO
UT WOS:000244131900022
DA 2024-07-18
ER

PT J
AU Polat, E
   Ozden, M
AF Polat, Ediz
   Ozden, Mustafa
TI A nonparametric adaptive tracking algorithm based on multiple feature
   distributions
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE kernel density estimation; mean-shift algorithm; object tracking
ID MEAN SHIFT
AB This paper presents an object tracking framework based on the mean-shift algorithm, which is a nonparametric technique that uses statistical color distribution of objects. Tracking objects through highly similar-colored background is one of the problems that need to be addressed. In various cases where object and background color distributions are very similar, the color distribution obtained from single frame alone is not sufficient to track objects reliably. To deal with this problem, the proposed algorithm utilizes an adaptive statistical background and foreground modeling to detect the change due to motion using kernel density estimation techniques based on multiple recent frames. The use of multiple frames supplies more information than single frame and thus it provides more accurate modeling of both background and foreground. In addition to color distribution, this statistical multiple frame-based motion representation is integrated into a modified mean-shift algorithm to create more robust object tracking framework. The use of motion distribution provides additional discriminative power to the framework. The superior performance with quantitative results of the framework has been validated using experiments on synthetic and real sequence of images.
C1 Kirikkale Univ, Dept Elect & Elect Engn, TR-71451 Kirikkale, Turkey.
C3 Kirikkale University
RP Polat, E (corresponding author), Kirikkale Univ, Dept Elect & Elect Engn, TR-71451 Kirikkale, Turkey.
EM polat@kku.edu.tr; mozden@kku.edu.tr
RI Ozden, Mustafa/HKE-3716-2023
CR Bradski GR, 1998, FOURTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION - WACV'98, PROCEEDINGS, P214, DOI 10.1109/ACV.1998.732882
   CHENG YZ, 1995, IEEE T PATTERN ANAL, V17, P790, DOI 10.1109/34.400568
   COLLINS RT, 2003, 2003 IEEE COMP SOC C
   Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Comaniciu D., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1197, DOI 10.1109/ICCV.1999.790416
   Comaniciu D, 2000, PROC CVPR IEEE, P142, DOI 10.1109/CVPR.2000.854761
   Elgammal A, 2002, P IEEE, V90, P1151, DOI 10.1109/JPROC.2002.801448
   ELGAMMAL A, 2001, C COMP VIS PATT REC
   Georgescu B, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P456
   Haritaoglu I, 2001, PROC CVPR IEEE, P431
   MacCormick J, 2000, INT J COMPUT VISION, V39, P57, DOI 10.1023/A:1008122218374
   McKenna SJ, 1999, IMAGE VISION COMPUT, V17, P225, DOI 10.1016/S0262-8856(98)00104-8
   McKenna SJ, 2000, COMPUT VIS IMAGE UND, V80, P42, DOI 10.1006/cviu.2000.0870
   Polat E, 2003, COMPUT VIS IMAGE UND, V89, P44, DOI 10.1016/S1077-3142(02)00031-0
   Stauffer C., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P246, DOI 10.1109/CVPR.1999.784637
   Wand M.P., 1994, KERNEL SMOOTHING
   Yeasin M, 2004, IEEE T MULTIMEDIA, V6, P398, DOI 10.1109/TMM.2004.827514
   YILMAZ A, 2001, IEEE WORKSH COMP VIS
NR 19
TC 15
Z9 19
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2006
VL 8
IS 6
BP 1156
EP 1163
DI 10.1109/TMM.2006.884624
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 109MG
UT WOS:000242311700006
DA 2024-07-18
ER

PT J
AU Zhao, J
   Yang, F
   Zhang, Q
   Zhang, ZS
   Zhang, FY
AF Zhao, Jin
   Yang, Fan
   Zhang, Qian
   Zhang, Zhensheng
   Zhang, Fuyan
TI LION: Layered overlay multicast with network coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE heterogeneity; network coding; overlay multicast
AB Recent advances in information theory show that the throughput of a multicast session can be improved using network coding. In overlay networks, the available bandwidth between sender and different receivers are different. In this paper, we propose a solution to improve the throughput of an overlay multicast session with heterogeneous receivers by organizing the receivers into layered data distribution meshes and sending substreams to each mesh using layered coding. Our solutions utilize alternative paths and network coding in each mesh. We first formulate the problem into a mathematical programming, whose optimal solution requires global information. We therefore present a distributed heuristic algorithm. The heuristic progressively organizes the receivers into layered meshes. Each receiver can subscribe to a proper number of meshes to maximize its throughput by fully utilizing its available bandwidth. The benefits of organizing the topology into layered mesh and using network coding are demonstrated through extensive simulations. Numerical results indicate that the average throughput of a multicast session is significantly improved (up to 50% to 60%) with only slightly higher delay and network resource consumption.
C1 Nanjing Univ, Dept Comp Sci & Technol, Nanjing 210008, Peoples R China.
   Microsoft Res Asia, Beijing, Peoples R China.
   Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Hong Kong, Peoples R China.
   San Diego Res Ctr, San Diego, CA 92121 USA.
C3 Nanjing University; Microsoft; Microsoft Research Asia; Hong Kong
   University of Science & Technology
RP Zhao, J (corresponding author), Nanjing Univ, Dept Comp Sci & Technol, Nanjing 210008, Peoples R China.
EM zhaojin@graphics.nju.edu.cn; fanyang@microsoft.com; qianzh@cse.ust.hk;
   zzhang@ieee.org; fyzhang@nju.edu.cn
RI Zhang, Qian/B-9058-2009
OI Zhang, Qian/0000-0001-9205-1881
CR Ahlswede R, 2000, IEEE T INFORM THEORY, V46, P1204, DOI 10.1109/18.850663
   Andersen D., 2001, Proc. of SOSP, P131, DOI DOI 10.1145/502059.502048
   [Anonymous], P 41 ALL C COMM CONT
   [Anonymous], 1973, Combinatorial Algorithms
   [Anonymous], P ACM NOSSDAV 2003 M
   Byers JW, 2002, IEEE J SEL AREA COMM, V20, P1528, DOI 10.1109/JSAC.2002.803996
   CASTRO M, 2003, P ACM SOSP LAK BOLT
   Chu YH, 2000, PERF E R SI, V28, P1, DOI 10.1145/345063.339337
   DEERING SE, 1990, ACM T COMPUT SYST, V8, P85, DOI 10.1145/78952.78953
   Ford LR., 1956, CAN J MATH, V8, P399, DOI [10.4153/CJM-1956-045-5, DOI 10.4153/CJM-1956-045-5.12R]
   Gkantsidis C, 2005, IEEE INFOCOM SER, P2235
   Ho T, 2003, 2003 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY - PROCEEDINGS, P442
   Jaggi S, 2003, 2003 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY - PROCEEDINGS, P368
   Jain K, 2003, SIAM PROC S, P266
   Koetter R, 2003, IEEE ACM T NETWORK, V11, P782, DOI 10.1109/TNET.2003.818197
   Li SYR, 2003, IEEE T INFORM THEORY, V49, P371, DOI 10.1109/TIT.2002.807285
   Li ZP, 2005, IEEE INFOCOM SER, P2184
   Lun DS, 2005, IEEE INFOCOM SER, P1608
   MCCANNE S, 1996, P ACM SIGCOMM 96 STA, P117
   Menger K., 1927, FUND MATH, V10, P96, DOI https://doi.org/10.4064/fm-10-1-96-115
   Padmanabhan V.N., 2002, P 12 INT WORKSHOP NE, P177
   Sanders P., 2003, Proceedings of the fifteenth annual ACM symposium on Parallel algorithms and architectures, SPAA '03, P286
   SAROIU S, 2002, P SPIE C MULT COMP N
   Savage S, 1999, COMP COMM R, V29, P289, DOI 10.1145/316194.316233
   WANG M, 2005, P 13 INT WORKSH QUAL, P37
   Wang Z, 1996, IEEE J SEL AREA COMM, V14, P1228, DOI 10.1109/49.536364
   Zegura EW, 1996, IEEE INFOCOM SER, P594, DOI 10.1109/INFCOM.1996.493353
   Zhu Y, 2004, IEEE J SEL AREA COMM, V22, P1237, DOI 10.1109/JSAC.2004.829342
   ZHU Y, 2004, IEEE J SEL AREA COMM, V22, P1
NR 29
TC 72
Z9 96
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2006
VL 8
IS 5
BP 1021
EP 1032
DI 10.1109/TMM.2006.879847
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 091TY
UT WOS:000241052400014
DA 2024-07-18
ER

PT J
AU Maeno, K
   Sun, QB
   Chang, SF
   Suto, M
AF Maeno, K
   Sun, QB
   Chang, SF
   Suto, M
TI New semi-fragile image authentication watermarking techniques using
   random bias and nonuniform quantization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE JPEG2000; nonuniform quantization; random bias; semi-fragile watermark;
   wavelet transform
AB Semi-fragile watermarking techniques aim at detecting malicious manipulations on an image, while allowing acceptable manipulations such as lossy compression. Although both of these manipulations are considered to be pixel value changes, semi-fragile watermarks should be sensitive to malicious manipulations but robust to the degradation introduced by lossy compression and other defined acceptable manipulations. In this paper, after studying the characteristics of both natural images and malicious manipulations, we propose two new semi-fragile authentication techniques robust against lossy compression, using random bias and nonuniform quantization, to improve the performance of the methods proposed by Lin and Chang.
C1 Oki Elect Ind Co Ltd, Warabi, Saitama 3358510, Japan.
   Inst Infocomm Res, Singapore 119613, Singapore.
   Columbia Univ, Dept Elect Engn, New York, NY 10027 USA.
C3 Oki Electric Industry Co., Ltd.; Agency for Science Technology &
   Research (A*STAR); A*STAR - Institute for Infocomm Research (I2R);
   Columbia University
RP Oki Elect Ind Co Ltd, Warabi, Saitama 3358510, Japan.
EM maeno284@oki.com; qibin@i2r.a-star.edu.sg; sfchang@ee.columbia.edu;
   sutou627@oki.com
RI Maeno, Kurato/V-6876-2019
CR [Anonymous], P IEEE INT C IM PROC
   EGGERS JJ, 2000, P IEE C SEC IM IM AU
   EGGERS JJ, 2001, P ICASSP 2001 INT C
   FRIDRICH J, 1998, P IEEE INT C IM PROC
   KAJI M, 1995, GRAPHIC TECHNOLOGY P
   Lin CY, 2000, PROC SPIE, V3971, P140, DOI 10.1117/12.384968
   LIN ET, 2000, P SPIE, V3971
   Maeno K, 2002, PROC SPIE, V4675, P659, DOI 10.1117/12.465327
   Queluz MP, 1999, PROC SPIE, V3657, P85, DOI 10.1117/12.344706
   WALTON S, 1995, DR DOBBS J, V20, P18
   Wolfgang RB, 1999, P SOC PHOTO-OPT INS, V3657, P204, DOI 10.1117/12.344670
   Wolfgang RB, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL III, P219, DOI 10.1109/ICIP.1996.560423
   WONG P, 1999, FINAL PROGRAM P IS T, P374
   WU M, 1998, P IEEE ICIP CHIC IL
   YEUNG M, 1997, P IEEE ICMP SANT BAR
NR 15
TC 86
Z9 98
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2006
VL 8
IS 1
BP 32
EP 45
DI 10.1109/TMM.2005.861293
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 005UR
UT WOS:000234850000004
DA 2024-07-18
ER

PT J
AU Alregib, G
   Altunbasak, Y
AF Alregib, G
   Altunbasak, Y
TI 3TP: An application-layer protocol for streaming 3-D models
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
ID COMPRESSION
AB This paper addresses the problem of streaming progressively compressed three-dimensional (3-D) models over lossy networks. Out of all encoded packets that can be transmitted, we intelligently choose a subset of packets to be transmitted using transport control protocol in order to meet a distortion constraint, while transmitting the remaining packets using user datagram protocol to minimize the end-to-end delay. We call this new application-layer protocol 3-D models transport protocol. We show the effectiveness of this protocol both experimentally and theoretically. We compare the performance of the proposed protocol with systems that do not optimize transmission according to the content of the encoded bitstream. When the maximum distortion is 30, measured using the Hausdorff distance, we achieve savings in delay time ranging from 39% to 68% for packet-loss rates between 1% and 19%.
C1 Georgia Inst Technol, Sch Elect & Comp Engn, Savannah, GA 31407 USA.
   Georgia Inst Technol, Sch Elect & Comp Engn, Atlanta, GA 30332 USA.
C3 University System of Georgia; Georgia Institute of Technology;
   University System of Georgia; Georgia Institute of Technology
RP Georgia Inst Technol, Sch Elect & Comp Engn, Savannah, GA 31407 USA.
EM gregib@ece.gatech.edu; yucel@ece.gatech.edu
OI AlRegib, Ghassan/0000-0001-6818-8001
CR Al-Regib G, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P353, DOI 10.1109/ICME.2002.1035791
   Al-Regib G, 2002, IEEE INFOCOM SER, P743, DOI 10.1109/INFCOM.2002.1019320
   ALLIEZ P, 2001, P EUR MANCH UK
   AlRegib G., 2002, Proceedings of the IEEE International Conference, V2, P2041
   Bajaj C. L., 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P307, DOI 10.1109/VISUAL.1999.809902
   Bajaj C L., 1998, Error resilient transmission of compressed VRML
   Bischoff S, 2002, COMPUT GRAPH-UK, V26, P665, DOI 10.1016/S0097-8493(02)00122-X
   Chen Z., 2003, Proc. Web3D, P161
   Chow MM, 1997, VISUALIZATION '97 - PROCEEDINGS, P347, DOI 10.1109/VISUAL.1997.663902
   Cohen-Or D., 1999, Proceedings Visualization '99 (Cat. No.99CB37067), P67
   DEERING M, 1995, P 22 ANN C COMP GRAP, P13
   Floyd S, 1999, IEEE ACM T NETWORK, V7, P458, DOI 10.1109/90.793002
   GUMHOLD S, 1998, P SIGGRAPH 98, P133
   Hoppe H., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P99, DOI 10.1145/237170.237216
   HOPPE H, 1998, MSRTR982 MICR RES
   Horn U, 1999, SIGNAL PROCESS-IMAGE, V15, P77, DOI 10.1016/S0923-5965(99)00025-9
   Lee A. W., 1998, Proceedings of the 25th annual conference on Computer graphics and interactive techniques, P95, DOI DOI 10.1145/280814.280828
   Li JK, 1998, P IEEE, V86, P1052, DOI 10.1109/5.687829
   MARTIN IM, 2000, P ACM MULTIMEDIA LOS
   Pajarola R, 2000, IEEE T VIS COMPUT GR, V6, P79, DOI 10.1109/2945.841122
   POPOVIC J, 1997, P SIGGRAPH 97, P217
   RONFARD R, 1996, P EUR 96 COMP GRAPH, V15, pC67
   Rossignac J, 1999, IEEE T VIS COMPUT GR, V5, P47, DOI 10.1109/2945.764870
   Taubin G, 1998, ACM T GRAPHIC, V17, P84, DOI 10.1145/274363.274365
   Taubin G, 1998, P IEEE, V86, P1228, DOI 10.1109/5.687837
   TAUBIN G, 1998, P SIGGRAPH 98, P123
   Touma C., 1998, P GRAPHICS INTERFACE
   Wang Y, 1998, P IEEE, V86, P974, DOI 10.1109/5.664283
   YAN Z, 2001, IEEE T CIRCUITS SYST, V11, P138
NR 29
TC 15
Z9 18
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2005
VL 7
IS 6
BP 1149
EP 1156
DI 10.1109/TMM.2005.858404
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 986SY
UT WOS:000233471200016
DA 2024-07-18
ER

PT J
AU Kim, T
   Ammar, MH
AF Kim, T
   Ammar, MH
TI A comparison of heterogeneous video multicast schemes: Layered encoding
   or stream replication
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE multimedia communication; video multicasting; scalable video streaming
ID SUCCESSIVE REFINEMENT; ACHIEVABLE RATES; INFORMATION
AB The heterogeneity of the Internet's transmission resources and end system capability makes it difficult to agree on acceptable traffic characteristics among the multiple receivers of a multicast video stream. Three basic approaches have been proposed to deal with this problem: 1) multicasting the replicated video streams at different rates; 2) multicasting the video encoded in cumulative layers; and 3) multicasting the video encoded in noncumulative layers. Even though there is a common belief that the layering approach is better than the replicated stream approach, there have been no studies that compare these schemes. This paper is devoted to such a systematic comparison. Our starting point is an observation (substantiated by results in the literature) that a bandwidth overhead is incurred by encoding a video stream in layers. We argue that a fair comparison of these schemes needs to take into account this overhead, as well as the specifies of the encoding used in each scheme, protocol complexity, and the topological placement of the video source and the receivers relative to each other. Our results show that the believed superiority of layered multicast transmission relative to replicated stream multicasting is not as clear cut as is widely believed and that there are indeed scenarios where replicated stream multicasting is the preferred approach.
C1 Freescale Semicond, Wireless & Mobile Syst Grp, Austin, TX 78735 USA.
   Georgia Inst Technol, Atlanta, GA 30332 USA.
C3 NXP Semiconductors; Freescale Semiconductor; University System of
   Georgia; Georgia Institute of Technology
RP Kim, T (corresponding author), Freescale Semicond, Wireless & Mobile Syst Grp, Austin, TX 78735 USA.
EM tachyun.kim@freescale.com; ammar@cc.gatech.edu
CR [Anonymous], 1995, 138182 ISOIEC
   [Anonymous], VID COD LOW BIT RAT
   APOSTOLOPOULOS J, 2001, P VCIP 2001 SAN JOS
   BYERS J, 2001, P IEEE INIFOCOM 2001
   CHEUNG SY, 1996, P IEEE INFOCOM 96
   CHU YH, 2000, P ACM SIGMETRICS 200
   DECUETOS P, 2001, P PACKET VIDEO 2001
   ELGAMAL AA, 1982, IEEE T INFORM THEORY, V28, P851, DOI 10.1109/TIT.1982.1056588
   EQUITZ WHR, 1991, IEEE T INFORM THEORY, V37, P269, DOI 10.1109/18.75242
   GOYAL V, 1998, P ICIP 98
   *ISO IEC, 1999, 144962 ISO IEC
   JIANG T, 1998, P ACM SIGMETRICS 98
   Kanlis A, 1996, IEEE T INFORM THEORY, V42, P275, DOI 10.1109/18.481803
   KIM T, 2001, P ACM NOSSDAV 2001
   KIMURA J, 1999, P SPIE INT S VOIC VI
   KONDI LP, 1998, P ICIP 98
   LI X, 1998, P IEEE INFOCOM 98 SA
   MCCANNE S, 1996, P ACM SIGCOMM 96
   Radha HM, 2001, IEEE T MULTIMEDIA, V3, P53, DOI 10.1109/6046.966110
   REIBMAN A, 1999, P ICIP 99 KOB JAP OC
   RIMOLDI B, 1994, IEEE T INFORM THEORY, V40, P253, DOI 10.1109/18.272493
   SHACHAM N, 1992, P IEEE INFOCOM 92 FL
   VAISHAMPAYAN V, 1999, P ICIP 99 KOB JAP OC
   VORONOV G, 2000, ISIT 2000
   YANG Y, 2000, P ICNP 2000 OS JAP
   Zegura EW, 1997, IEEE ACM T NETWORK, V5, P770, DOI 10.1109/90.650138
   [No title captured]
NR 27
TC 19
Z9 21
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2005
VL 7
IS 6
BP 1123
EP 1130
DI 10.1109/TMM.2005.858376
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 986SY
UT WOS:000233471200013
DA 2024-07-18
ER

PT J
AU Su, TC
   Huang, SY
   Chan, CL
   Wang, JS
AF Su, TC
   Huang, SY
   Chan, CL
   Wang, JS
TI Optimal chaining scheme for video-on-demand applications on
   collaborative networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE chaining; collaborative network; streaming; video-on-demand
ID MULTICAST
AB By forwarding the server stream client by client, a chaining-based scheme is a good way to reduce the server streams for streaming applications in well-connected networks. In this paper, we prove that the minimum number of required server streams in such schemes is n - k + 1, where n is the number of client requests and k is a value determined by client buffer sizes and the distribution of requests. In addition, we present an optimal chaining algorithm using a dynamic buffer allocation strategy. Compared to existing chaining schemes, our scheme not only utilizes the backward (basic chaining) and/or forward (adaptive chaining) buffer, but also exploits the buffers of other clients in order to extend the chain as much as possible. In this way, more clients can be chained together and served by the same server stream. Our simulation results show that the requirements of the server streams in the presented scheme are much lower those of existing chaining schemes. We also introduce mechanisms for handling VCR functions and fault exceptions in practical applications.
C1 Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu, Taiwan.
   Ming Chuan Univ, Dept Comp Sci & Informat Engn, Taoyuan, Taiwan.
C3 National Tsing Hua University; Ming Chuan University
RP Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu, Taiwan.
EM tcsu@vc.cs.nthu.edu.tw; sy-huang@mcu.edu.tw; clchan@vc.cs.nthu.edu.tw;
   jswang@cs.nthu.edu.tw
CR Aggarwal CC, 1996, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, P118, DOI 10.1109/MMCS.1996.534963
   Breslau L, 1999, IEEE INFOCOM SER, P126, DOI 10.1109/INFCOM.1999.749260
   CAI Y, 1999, P SPIE ACM C MULT CO, P204
   Chen JK, 1999, IEEE T BROADCAST, V45, P215, DOI 10.1109/11.796263
   CHEU S, 1997, P IEEE INT C MULT CO, P110
   DAN A, 1994, 19347 IBM RC
   EAGER DL, 1998, P 4 INT WORKSH MULT, P18
   Fonseca NLS, 2000, GLOB TELECOMM CONF, P1334, DOI 10.1109/GLOCOM.2000.891847
   Hua K., 1997, PROC SIGCOMM, P89
   Hua K. A., 1998, Proceedings ACM Multimedia 98, P191, DOI 10.1145/290747.290771
   Hua KA, 1997, IEEE INFOCOM SER, P990, DOI 10.1109/INFCOM.1997.631037
   Juhn LS, 1997, IEEE T BROADCAST, V43, P268, DOI 10.1109/11.632927
   MA W, 2000, P IEEE ICME, V2, P991
   Poon WF, 2001, IEEE T BROADCAST, V47, P66, DOI 10.1109/11.920782
   Ramesh S, 2001, IEEE T CIRC SYST VID, V11, P440, DOI 10.1109/76.911167
   Rejaie R., 2000, Proceedings IEEE INFOCOM 2000. Conference on Computer Communications. Nineteenth Annual Joint Conference of the IEEE Computer and Communications Societies (Cat. No.00CH37064), P980, DOI 10.1109/INFCOM.2000.832273
   Saparilla D, 1999, IEEE INFOCOM SER, P464, DOI 10.1109/INFCOM.1999.751379
   SEN S, 1999, P NOSSDAV BASK RIDG
   Zhang ZL, 2000, IEEE ACM T NETWORK, V8, P429, DOI 10.1109/90.865072
NR 19
TC 8
Z9 8
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2005
VL 7
IS 5
BP 972
EP 980
DI 10.1109/TMM.2005.854390
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 967IM
UT WOS:000232084900018
DA 2024-07-18
ER

PT J
AU Panagiotakis, C
   Tziritas, G
AF Panagiotakis, C
   Tziritas, G
TI A speech/music discriminator based on RMS and zero-crossings
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE audio segmentation; speech/music classification; zero-crossing rate
ID CLASSIFICATION
AB Over the last several years, major efforts have been made to develop methods for extracting information from audiovisual media, in order that they may be stored and retrieved in databases automatically, based on their content. In this work we deal with the characterization of an audio signal, which may be part of a larger audiovisual system or may be autonomous, as for example in the case of an audio recording stored digitally on disk. Our goal was to first develop a system for segmentation of the audio signal, and then classification into one of two main categories: speech or music. Among the system's requirements are its processing speed and its ability to function in a real-time environment with a small responding delay. Because of the restriction to two classes, the characteristics that are extracted are considerably reduced and moreover the required computations are straightforward. Experimental results show that efficiency is exceptionally good, without sacrificing performance.
   Segmentation is based on mean signal amplitude distribution, whereas classification utilizes an additional characteristic related to the frequency. The classification algorithm may be used either in conjunction with the segmentation algorithm, in which case it verifies or refutes a music-speech or speech-music change, or autonomously, with given audio segments. The basic characteristics are computed in 20 ms intervals, resulting in the segments' limits being specified within an accuracy of 20 ms. The smallest segment length is one second. The segmentation and classification algorithms were benchmarked on a large data set, with correct segmentation about 97% of the time and correct classification about 95%.
C1 Univ Crete, Dept Comp Sci, GR-71409 Iraklion, Crete, Greece.
C3 University of Crete
RP Univ Crete, Dept Comp Sci, GR-71409 Iraklion, Crete, Greece.
RI Panagiotakis, Costas/I-5115-2019; Tziritas, Georgios/AAO-5855-2021
OI Panagiotakis, Costas/0000-0003-3680-7087; 
CR [Anonymous], P IEEE C AC SPEECH S
   Foote J, 1999, MULTIMEDIA SYST, V7, P2, DOI 10.1007/s005300050106
   KEDEM B, 1986, P IEEE, V74, P1477, DOI 10.1109/PROC.1986.13663
   KRISHNAIAH PR, 1984, HDB STAT NONPARAMETR
   Papoulis A., 1965, PROBABILITY RANDOM V
   SAUNDERS J, 1996, P IEEE ICASSP
   SCHEIER E, 1997, P IEEE ICASSP
   SECK M, 1999, P EUROSPEECH, P2801
   SPANIAS AS, 1994, P IEEE, V82, P1541, DOI 10.1109/5.326413
   Tzanetakis G, 2002, IEEE T SPEECH AUDI P, V10, P293, DOI 10.1109/TSA.2002.800560
   TZANETAKIS G, 1999, P 25 EUR C WORKSH MU
   Wold E, 1996, IEEE MULTIMEDIA, V3, P27, DOI 10.1109/93.556537
   Young T.Y., 1986, HDB PATTERN RECOGNIT
   Zhang T, 2001, IEEE T SPEECH AUDI P, V9, P441, DOI 10.1109/89.917689
NR 14
TC 126
Z9 157
U1 1
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2005
VL 7
IS 1
BP 155
EP 166
DI 10.1109/TMM.2004.840604
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 893CQ
UT WOS:000226697300015
DA 2024-07-18
ER

PT J
AU Liapis, S
   Tziritas, G
AF Liapis, S
   Tziritas, G
TI Color and texture image retrieval using chromaticity histograms and
   wavelet frames
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE bhattacharya distance; color image similarity; discrete wavelet frames;
   texture retrieval
ID CLASSIFICATION; SEGMENTATION; FEATURES; DECOMPOSITION; FILTERS
AB In this paper, we explore image retrieval mechanisms based on a combination of texture and color features. Texture features are extracted using Discrete Wavelet Frames (DWF) analysis, an over-complete decomposition in scale and orientation. Two-dimensional (2-D) or one-dimensional (1-D) histograms of the CIE Lab chromaticity coordinates are used as color features. The I-D histograms of the a, b coordinates were modeled according to the generalized Gaussian distribution. The similarity measure defined on the feature distribution is based on the Bhattacharya distance. Retrieval benchmarking is performed over the Brodatz album and on images from natural scenes, obtained from the VisTex database of MIT Media Laboratory and from the Corel Photo Gallery. As a performance indicator recall (relative number of correct images retrieved) is measured on both texture and color separately and in combination. Experiments show this approach to be as effective as other methods while computationally more tractable.
C1 Univ Crete, Dept Comp Sci, GR-71409 Iraklion, Crete, Greece.
C3 University of Crete
RP Liapis, S (corresponding author), Univ Crete, Dept Comp Sci, GR-71409 Iraklion, Crete, Greece.
EM liapis@csd.uoc.gr; tziritas@csd.uoc.gr
RI Tziritas, Georgios/AAO-5855-2021
CR [Anonymous], IEEE COMPUT
   BOVIK AC, 1990, IEEE T PATTERN ANAL, V12, P55, DOI 10.1109/34.41384
   BRODATZ P, 1966, PHOTOGRAPHIC ALBUM A
   Chang T, 1993, IEEE T IMAGE PROCESS, V2, P429, DOI 10.1109/83.242353
   FLICKNER M, 1995, IEEE COMPUT, V9, P23
   GUDIVADA VN, 1995, IEEE COMPUTER, V28
   Gupta A, 1997, COMMUN ACM, V40, P70, DOI 10.1145/253769.253798
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   JAIN AK, 1991, PATTERN RECOGN, V24, P1167, DOI 10.1016/0031-3203(91)90143-S
   Kashyap RL, 1982, PATTERN RECOGN LETT, V1, P43, DOI 10.1016/0167-8655(82)90050-2
   KUNDU A, 1992, CVGIP-GRAPH MODEL IM, V54, P369, DOI 10.1016/1049-9652(92)90022-P
   LAINE A, 1993, IEEE T PATTERN ANAL, V15, P1186, DOI 10.1109/34.244679
   LIAPIS S, 1997, P INT C DIG SIGN PRO, P1107
   Liu F, 1996, IEEE T PATTERN ANAL, V18, P722, DOI 10.1109/34.506794
   MALLAT SG, 1989, IEEE T PATTERN ANAL, V11, P674, DOI 10.1109/34.192463
   Manjunath BS, 1996, IEEE T PATTERN ANAL, V18, P837, DOI 10.1109/34.531803
   Manjunath BS, 2001, IEEE T CIRC SYST VID, V11, P703, DOI 10.1109/76.927424
   MAO JC, 1992, PATTERN RECOGN, V25, P173, DOI 10.1016/0031-3203(92)90099-5
   MIT Media Laboratory, VIST TEXT IM DAT
   PENTLAND A, 1993, 255 MIT MED LAB
   Pereira F, 2000, SIGNAL PROCESS-IMAGE, V16, P1, DOI 10.1016/S0923-5965(00)00013-8
   PORAT M, 1989, IEEE T BIO-MED ENG, V36, P115, DOI 10.1109/10.16457
   PUZICHA J, 1999, INT C COMP VIS SEPT
   Randen T, 1999, IEEE T PATTERN ANAL, V21, P291, DOI 10.1109/34.761261
   REED TR, 1993, CVGIP-IMAG UNDERSTAN, V57, P359, DOI 10.1006/ciun.1993.1024
   Sikora T, 2001, IEEE T CIRC SYST VID, V11, P696, DOI 10.1109/76.927422
   UNSER M, 1995, IEEE T IMAGE PROCESS, V4, P1549, DOI 10.1109/83.469936
   Young T.Y., 1986, HDB PATTERN RECOGNIT
NR 28
TC 90
Z9 97
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2004
VL 6
IS 5
BP 676
EP 686
DI 10.1109/TMM.2004.834858
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 854XI
UT WOS:000223936800002
DA 2024-07-18
ER

PT J
AU Kundur, D
   Hatzinakos, D
AF Kundur, D
   Hatzinakos, D
TI Toward robust logo watermarking using multiresolution image fusion
   principles
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE data fusion; digital watermarking; image tagging; logo watermarking;
   multimedia security
AB This paper presents a novel robust watermarking approach called FuseMark based on the principles of image fusion for copy protection or robust tagging applications. We consider the problem of logo watermarking in still images and employ multiresolution data fusion principles for watermark embedding and extraction. A human visual system model based on contrast sensitivity is incorporated to bide a higher energy hidden logo in salient image components. Watermark extraction involves both characterization of attacks and logo estimation using a rake-like receiver. Statistical analysis demonstrates how our extraction approach can be used for watermark detection applications to decrease the problem of false negative detection without increasing the false positive detection rate. Simulation results verify theoretical observations and demonstrate the practical performance of FuseMark.
C1 Texas A&M Univ, Dept Elect Engn, College Stn, TX 77843 USA.
   Univ Toronto, Dept Elect & Comp Engn, Toronto, ON M5S 3G4, Canada.
C3 Texas A&M University System; Texas A&M University College Station;
   University of Toronto
RP Texas A&M Univ, Dept Elect Engn, College Stn, TX 77843 USA.
EM deepa@ee.tamu.edu; dimitris@comm.toronto.edu
CR Abidi M.A., 1992, Data fusion in robotics and machine intelligence
   Braudaway GW, 1996, P SOC PHOTO-OPT INS, V2659, P126, DOI 10.1117/12.235469
   BRAUDAWAY GW, 1997, P IEEE INT C IM PROC, V1, P524
   CHIPMAN LJ, 1995, P SOC PHOTO-OPT INS, V2569, P208, DOI 10.1117/12.217576
   COX IJ, 1995, 9510 NEC RES I
   Craver S, 1998, IEEE J SEL AREA COMM, V16, P573, DOI 10.1109/49.668979
   DAUBECHIES I, 1988, COMMUN PUR APPL MATH, V41, P909, DOI 10.1002/cpa.3160410705
   HSU CT, 1996, P IEEE INT C IM PROC, V3, P223
   HSU CT, 1997, P DSP97 JUL GREEC, P217
   Kankanhalli MS, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 1, P568, DOI 10.1109/MMCS.1999.779263
   Knox KT, 1999, P SOC PHOTO-OPT INS, V3657, P397, DOI 10.1117/12.344690
   Kundur D, 1998, OPT EXPRESS, V3, P485, DOI 10.1364/OE.3.000485
   Kundur D, 1997, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL I, P544, DOI 10.1109/ICIP.1997.647970
   Kundur D, 2001, IEEE T SIGNAL PROCES, V49, P2383, DOI 10.1109/78.950793
   KUNDUR D, 1999, IEEE INT C IM PROC O
   KUNDUR D, 1999, THESIS U TORONTO TOR
   Levine MartinD., 1985, VISION MAN MACHINE
   Lin PL, 2000, J SYST SOFTWARE, V50, P107, DOI 10.1016/S0164-1212(99)00081-3
   MENG J, 1998, P IEEE INT C IM PROC, V1
   MILLER ML, 1998, P MULT SEC WORKSH AC, P71
   Mintzer F, 1998, COMMUN ACM, V41, P56
   Niu XM, 2000, IEEE T CONSUM ELECTR, V46, P375, DOI 10.1109/30.846673
   Podilchuk CI, 1998, IEEE J SEL AREA COMM, V16, P525, DOI 10.1109/49.668975
   PRICE R, 1958, P IRE, V46, P555, DOI 10.1109/JRPROC.1958.286870
   Rao AR, 1998, P SOC PHOTO-OPT INS, V3314, P110, DOI 10.1117/12.304677
   Sanders MS, 1993, HUMAN FACTORS ENG DE
   Simon M., 1994, Spread Spectrum Communications Handbook, V1
   Su PC, 1999, P SOC PHOTO-OPT INS, V3657, P296, DOI 10.1117/12.344680
   Tao B, 1997, INT CONF ACOUST SPEE, P2985, DOI 10.1109/ICASSP.1997.595419
   Voyatzis G, 1998, COMPUT GRAPH, V22, P405, DOI 10.1016/S0097-8493(98)00030-2
   WILSON TA, 1995, OPT ENG, V34, P3154, DOI 10.1117/12.213617
   Xia XG, 1998, OPT EXPRESS, V3, P497, DOI 10.1364/OE.3.000497
   Xia XG, 1997, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL I, P548, DOI 10.1109/ICIP.1997.647971
   YOCKY DA, 1995, J OPT SOC AM A, V12, P1834, DOI 10.1364/JOSAA.12.001834
   Zeng WJ, 1999, P SOC PHOTO-OPT INS, V3657, P404, DOI 10.1117/12.344691
NR 35
TC 152
Z9 160
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2004
VL 6
IS 1
BP 185
EP 198
DI 10.1109/TMM.2003.819747
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 765QP
UT WOS:000188295200014
DA 2024-07-18
ER

PT J
AU Saito, H
   Baba, S
   Kanade, T
AF Saito, H
   Baba, S
   Kanade, T
TI Appearance-based virtual view generation from multicamera videos
   captured in the 3-D room
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE image based rendering; model based rendering; multibaseline stereo;
   multiple-view images; shape from silhouette; 3-D model
ID REALITY
AB We present an appearance-based virtual view generation method that allows viewers to fly through a real dynamic scene. The scene is captured by multiple synchronized cameras. Arbitrary views are generated by interpolating two original camera-views near the given viewpoint. The quality of the generated synthetic view is determined by the precision, consistency and density of correspondences between the two images.
   All or most of previous work that uses interpolation extracts the correspondences from these two images. However, not only is it difficult to do so reliably (the task requires a good stereo algorithm), but also the two images alone sometimes do not have enough information, due to problems such as occlusion. Instead, we take advantage of the fact that we have many views, from which we can extract much more reliable and comprehensive three-dimensional (3-D) geometry of the scene as a 3-D model. Dense and precise correspondences between the two images, to be used for interpolation, are obtained using this constructed 3-D model. Pseudo correspondences are even obtained for regions occluded in one of the cameras and then we used to correctly interpolate between the two images. Our method of 3-D modeling from multiple images uses the Multiple Baseline Stereo method and the Shape from Silhouette method. The virtual view sequences are presented for demonstrating the performance of the virtual view generation in the 3-D Room.
C1 Keio Univ, Dept Informat & Comp Sci, Yokohama, Kanagawa 223, Japan.
   Sony Corp, HPS Business Dev Pj, Tokyo, Japan.
   Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.
C3 Keio University; Sony Corporation; Carnegie Mellon University
RP Keio Univ, Dept Informat & Comp Sci, Yokohama, Kanagawa 223, Japan.
EM saito@ics.keio.ac.jp; Shigeyuki.Baba@jp.sony.com; tk@cs.cmu.edu
CR [Anonymous], 9 EUR REND WORKSH
   [Anonymous], P 23 ANN C COMP GRAP
   [Anonymous], CMURITR9834
   Avidan S, 1998, IEEE T VIS COMPUT GR, V4, P293, DOI 10.1109/2945.765324
   BEIER T, 1992, COMP GRAPH, V26, P35, DOI 10.1145/142920.134003
   CHEIN CH, 1986, COMPUT VIS GRAPH 36, P100
   Chen S. E., 1993, Computer Graphics Proceedings, P279, DOI 10.1145/166117.166153
   Curless B., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P303, DOI 10.1145/237170.237269
   Debevec P. E., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P11, DOI 10.1145/237170.237191
   FAUGERAS O, 1995, 2572 INRIA
   Gavrila DM, 1996, PROC CVPR IEEE, P73, DOI 10.1109/CVPR.1996.517056
   Gortler S. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P43, DOI 10.1145/237170.237200
   Heckbert PS, 1999, COMP GEOM-THEOR APPL, V14, P49, DOI 10.1016/S0925-7721(99)00030-9
   Hilton A., 1996, Computer Vision - ECCV '96. 4th Eurpean Conference on Computer Proceedings, P117, DOI 10.1007/BFb0015528
   Jain R., 1995, Proceedings of the International Conference on Multimedia Computing and Systems (Cat. No.95TH8066), P202, DOI 10.1109/MMCS.1995.484925
   Kanade T, 1997, IEEE MULTIMEDIA, V4, P34, DOI 10.1109/93.580394
   KATAYAMA A, 1995, P SOC PHOTO-OPT INS, V2409, P11, DOI 10.1117/12.205854
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   Matusik W, 2000, COMP GRAPH, P369, DOI 10.1145/344779.344951
   Moezzi S, 1997, IEEE MULTIMEDIA, V4, P18, DOI 10.1109/93.580392
   Narayanan PJ, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P3, DOI 10.1109/ICCV.1998.710694
   Neumann J, 2002, INT J COMPUT VISION, V47, P181, DOI 10.1023/A:1014597925429
   OKUTOMI M, 1993, IEEE T PATTERN ANAL, V15, P353, DOI 10.1109/34.206955
   POLLEFEYS, 1998, ICCV 98, P90
   POTMESIL M, 1987, COMPUT VIS GRAPH 40, P277
   RASKAR R, 1998, SIGGRAPH 98, P179
   Seitz SM, 1997, PROC CVPR IEEE, P1067, DOI 10.1109/CVPR.1997.609462
   Tamura H, 2001, IEEE COMPUT GRAPH, V21, P64, DOI 10.1109/38.963462
   TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684
   TSAI RY, 1987, IEEE T ROBOTIC AUTOM, V3, P323, DOI 10.1109/JRA.1987.1087109
   Vedula S., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P722, DOI 10.1109/ICCV.1999.790293
   VEDULA S, 1998, P 4 C VIRT SYST MULT, V1, P326
   WERNER T, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P957, DOI 10.1109/ICCV.1995.466831
   WHEELER MD, 1997, DARPA IM UND WORKSH, P1229
NR 35
TC 25
Z9 30
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2003
VL 5
IS 3
BP 303
EP 316
DI 10.1109/TMM.2003.813283
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 714BM
UT WOS:000184892500003
DA 2024-07-18
ER

PT J
AU Bao, CH
   Sun, QR
AF Bao, Chunhui
   Sun, Qianru
TI Generating Music With Emotions
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Music; Deep learning; Generators; Social networking (online); Natural
   languages; Generative adversarial networks; Emotion recognition;
   Conditional music generation; Seq2Seq; beam search; transformer
AB We focus on the music generation conditional on human emotions, specifically the positive and negative emotions. There is no existing large-scale music datasets with the annotation of human emotion labels. It is thus not intuitive how to generate music conditioned on emotion labels. In this paper, we propose an annotation-free method to build a new dataset where each sample is a triplet of lyric, melody and emotion label (without requiring any labours). Specifically, we first train the automated emotion recognition model using the BERT (pre-trained on GoEmotions dataset) on Edmonds Dance dataset. We use it to automatically "label" the music with the emotion labels recognized from the lyrics. We then train the encoder-decoder based model to generate emotional music on that dataset, and call our overall method as Emotional Lyric andMelody Generator (ELMG). The framework of ELMG is consisted of three modules: 1) an encoder-decoder model trained end-to-end to generate lyric and melody; 2) a music emotion classifier trained on labeled data (our proposed dataset); and 3) a modified beam search algorithm that guides the music generation process by incorporating the music emotion classifier. We conduct objective and subjective evaluations on the generated music pieces, and our results showthatELMGis capable of generating tuneful lyric and melody with specified human emotions.
C1 [Bao, Chunhui; Sun, Qianru] Singapore Management Univ, Sch Comp & Informat Syst, Singapore 178902, Singapore.
C3 Singapore Management University
RP Sun, QR (corresponding author), Singapore Management Univ, Sch Comp & Informat Syst, Singapore 178902, Singapore.
EM chbao.2019@msc.smu.edu.sg; qianrusun@smu.edu.sg
RI Sun, Qianru/HHN-0249-2022
FU A*STAR through its AME YIRG project [A20E6c0101]
FX This work was supported by A*STAR through its AME YIRG project under
   Grant A20E6c0101. The Associate Editor coordinating the review of this
   manuscript and approving it for publication was Mrs. Si Liu.
CR [Anonymous], 2013, INT C LEARN REPRESEN, DOI 10.1109/TCBB.2021.3077905
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Bao HB, 2019, LECT NOTES ARTIF INT, V11838, P499, DOI 10.1007/978-3-030-32233-5_39
   BRIOT JP, 2019, COMPUT SYNTH CREATIV
   C ano E., 2017, P 2017 INT C INT SYS, P118
   Carroll N, 2003, MONIST, V86, P521, DOI 10.5840/monist200386426
   Colaboratory, About us
   CZYZ M, 2021, P INT C DEPENDABILIT, P22
   Demszky D, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P4040
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   DHARIWAL P, 2020, JUKEBOX A GENERATIVE
   Dong HW, 2018, AAAI CONF ARTIF INTE, P34
   Edmonds D., 2021, P 11 WORKSH COMP APP, P221
   ENS J, 2020, MMM EXPLORING CONDIT
   FERREIRA LN, 2019, INT SOC MUSIC INF RE
   Ferreira Lucas N., 2020, P AAAI C ARTIFICIAL, V16, P59
   Freitag Markus, 2017, P 1 WORKSH NEUR MACH, P56
   Goodfellow J., 2014, Adv. Neural Inf. Process. Syst., V27
   Hadjeres F., 2017, P 34 INT C MACH LEAR, V70, P1362, DOI [10.48550/arXiv.1612.01010, 10.48550/arXiv.1612.01010.89A, DOI 10.48550/ARXIV.1612.01010.89A]
   HUANG CF, 2020, P IEEE EURASIA C IOT, P220
   HUNG HT, 2021, INT SOC MUSIC INF RE
   IQBAL T, 2020, J KING SAUD UNIVCOMP
   JI S, 2020, A COMPREHENSIVE SURV
   Kool W, 2019, INT C MACH LEARN, V97, P3499, DOI DOI 10.48550/ARXIV.1903.06059
   KRUMHANSL CL, 1982, PSYCHOL REV, V89, P334, DOI 10.1037/0033-295X.89.4.334
   MADHUMANI GR, 2020, AUTOMATIC NEURAL LYR
   Malheiro R, 2018, IEEE T AFFECT COMPUT, V9, P240, DOI 10.1109/TAFFC.2016.2598569
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Mogren Olof, 2016, CONSTRUCTIVE MACHINE
   Mohammad SM, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P174
   Radford A., 2019, LANGUAGE MODELS ARE
   reddit, About us
   ROBERTS A, 2017, P NIPS WORKSHOP MACH, V3
   Roberts A., 2018, INT C MACHINE LEARNI, P4364, DOI DOI 10.48550/ARXIV.1803.05428.90I
   RUSSELL JA, 1980, J PERS SOCIAL PSYCHO, V39
   Vaswani A, 2017, ADV NEUR IN, V30
   VIJAYAKUMAR A, 2018, P AAAI C ARTIF INTEL, V32, P1
   WELLECK S, 2020, INT C LEARN REPRESEN
   Xu J, 2019, ADV NEUR IN, V32
   YU Y, 2020, P INT C MULTIMEDIA M, P709
   YU Y, 2020, CONDITIONAL HYBRID G
   Yu Y, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3424116
   Zhao YJ, 2019, IEEE CIRC SYST MAG, V19, P19, DOI 10.1109/MCAS.2019.2945210
NR 43
TC 2
Z9 2
U1 10
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3602
EP 3614
DI 10.1109/TMM.2022.3163543
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FE2H4
UT WOS:001144015500008
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Ben, XY
   Gong, C
   Huang, TH
   Li, CY
   Yan, R
   Li, YJ
AF Ben, Xianye
   Gong, Chen
   Huang, Tianhuan
   Li, Chuanye
   Yan, Rui
   Li, Yujun
TI Tackling Micro-Expression Data Shortage via Dataset Alignment and Active
   Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dataset alignment; active learning; micro-expression recognition
AB The research on micro-expression recognition has been drawing great attention in recent years, because of its great potential in the lie detection, clinical diagnosis, and national security. Amongst many challenges, data shortage stands out as it directly prevents an accurate training of micro-expression recognition algorithm. In this work, we present our approach within a dataset alignment and active learning (DAAL) framework. DAAL effectively queries minimum examples to label, as well as transfers features from micro-expression dataset to macro-expression dataset. Specifically, the features from micro-expression dataset are mapped to the macro-expression dataset with a translator, so that the classifier trained in macro-expression dataset can be adjusted and adapted to boost the classification performance on the micro-expression dataset. Besides, the most informative examples in the micro-expression dataset are selected through active learning in an iterative way, which effectively improves the classification ability of the model. Comprehensive experiments on CASME, CASME II, SAMM and SMIC databases firmly demonstrate that the proposed DAAL outperforms previous works by a large margin on micro-expression recognition task.
C1 [Ben, Xianye; Huang, Tianhuan; Li, Chuanye; Li, Yujun] Shandong Univ, Sch Informat Sci & Engn, Qingdao 266237, Peoples R China.
   [Gong, Chen] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Key Lab Intelligent Percept & Syst High Dimens Inf, Minist Educ, Nanjing 210094, Peoples R China.
   [Yan, Rui] Amazon, Bellevue, WA 98004 USA.
C3 Shandong University; Nanjing University of Science & Technology;
   Amazon.com
RP Ben, XY; Li, YJ (corresponding author), Shandong Univ, Sch Informat Sci & Engn, Qingdao 266237, Peoples R China.
EM benxianye@gmail.com; chen.gong@njust.edu.cn; raymondino.yan@gmail.com;
   liyujun@sdu.edu.cn
RI Li, Yujun/HGE-8719-2022
OI Huang, Tianhuan/0000-0002-9516-9583; Ben, Xianye/0000-0001-8083-3501
FU Guangdong Basic and Applied Basic Research Foundation [2022A1515010186];
   Natural Science Foundation of China [61971468, 61973162]; Shandong
   Provincial Key Research and Development Program (Major Scientific and
   Technological Innovation Project) [2019JZZY010119]; Fundamental Research
   Funds for the Central Universities [2022JC017, 30920032202, 30921013114]
FX This work was supported in part by the Guangdong Basic and Applied Basic
   Research Foundation under Grant 2022A1515010186, in part by the Natural
   Science Foundation of China under Grants 61971468 and 61973162, in part
   by the Shandong Provincial Key Research and Development Program (Major
   Scientific and Technological Innovation Project) under Grant
   2019JZZY010119, and in part by the Fundamental Research Funds for the
   Central Universities under Grants 2022JC017, 30920032202, and
   30921013114.
CR Allaert B, 2022, IEEE T AFFECT COMPUT, V13, P147, DOI 10.1109/TAFFC.2019.2949559
   [Anonymous], 2009, Technical report
   Ben XY, 2022, IEEE T PATTERN ANAL, V44, P5826, DOI 10.1109/TPAMI.2021.3067464
   Ben XY, 2018, PATTERN RECOGN LETT, V107, P50, DOI 10.1016/j.patrec.2017.07.010
   Chattopadhyay R, 2013, ACM T KNOWL DISCOV D, V7, DOI 10.1145/2513092.2513094
   Copa L, 2010, PROC SPIE, V7830, DOI 10.1117/12.864861
   Davison AK, 2018, IEEE T AFFECT COMPUT, V9, P116, DOI 10.1109/TAFFC.2016.2573832
   Demir B, 2011, IEEE T GEOSCI REMOTE, V49, P1014, DOI 10.1109/TGRS.2010.2072929
   Deng C, 2019, IEEE T GEOSCI REMOTE, V57, P1741, DOI 10.1109/TGRS.2018.2868851
   Duda R. O., 2001, PATTERN CLASSIFICATI
   Gerhardt L., 1974, IEEE Trans. Autom. Control, VAC-19, P461
   Hsu WN, 2015, AAAI CONF ARTIF INTE, P2659
   Huang SJ, 2014, IEEE T PATTERN ANAL, V36, P1936, DOI 10.1109/TPAMI.2014.2307881
   Huang TH, 2022, IEEE T CIRC SYST VID, V32, P6967, DOI 10.1109/TCSVT.2022.3175959
   Huang XH, 2016, NEUROCOMPUTING, V175, P564, DOI 10.1016/j.neucom.2015.10.096
   Huang XH, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P1, DOI 10.1109/ICCVW.2015.10
   Jia XT, 2018, J COMPUT SCI-NETH, V25, P289, DOI 10.1016/j.jocs.2017.03.016
   Kim DH, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P382, DOI 10.1145/2964284.2967247
   Lei L, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2237, DOI 10.1145/3394171.3413714
   Li B, 2022, MULTIMEDIA SYST, V28, P1099, DOI 10.1007/s00530-022-00896-9
   Li J, 2019, PATTERN ANAL APPL, V22, P1331, DOI 10.1007/s10044-018-0757-5
   Li S, 2019, IEEE T IMAGE PROCESS, V28, P356, DOI 10.1109/TIP.2018.2868382
   Li XB, 2013, IEEE INT CONF AUTOMA, DOI 10.1109/FG.2013.6553717
   Li YT, 2018, IEEE IMAGE PROC, P3094, DOI 10.1109/ICIP.2018.8451376
   Lin JZ, 2018, IEEE J-STARS, V11, P4048, DOI 10.1109/JSTARS.2018.2874225
   Liu YJ, 2021, IEEE T AFFECT COMPUT, V12, P254, DOI 10.1109/TAFFC.2018.2854166
   Liu YJ, 2016, IEEE T AFFECT COMPUT, V7, P299, DOI 10.1109/TAFFC.2015.2485205
   Liu YC, 2019, IEEE INT CONF AUTOMA, P631, DOI 10.1109/fg.2019.8756583
   Lucey P., 2010, IEEE COMP SOC C COMP, P94, DOI 10.1109/CVPRW.2010.5543262
   Mollahosseini A, 2019, IEEE T AFFECT COMPUT, V10, P18, DOI 10.1109/TAFFC.2017.2740923
   Peng M, 2018, IEEE INT CONF AUTOMA, P657, DOI 10.1109/FG.2018.00103
   Peng ZH, 2020, IEEE T CIRC SYST VID, V30, P1022, DOI 10.1109/TCSVT.2019.2900467
   Pfister T, 2011, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2011.6126401
   Rajan S, 2008, IEEE T GEOSCI REMOTE, V46, P1231, DOI 10.1109/TGRS.2007.910220
   Ranganathan H, 2017, IEEE IMAGE PROC, P3934, DOI 10.1109/ICIP.2017.8297020
   Ruiz-Hernandez JA, 2013, IEEE INT CONF AUTOMA
   Schohn G, 2000, P 17 INT C MACHINE L, P839
   Sun B, 2022, IEEE T AFFECT COMPUT, V13, P1037, DOI 10.1109/TAFFC.2020.2986962
   Takalkar MA, 2017, 2017 INTERNATIONAL CONFERENCE ON DIGITAL IMAGE COMPUTING - TECHNIQUES AND APPLICATIONS (DICTA), P688
   thoth.inrialpes.fr, Proximal toolbox
   Tuia D, 2009, IEEE T GEOSCI REMOTE, V47, P2218, DOI 10.1109/TGRS.2008.2010404
   Van Quang N., 2019, P IEEE 14 INT C AUT, P1
   Verma M, 2020, IEEE T IMAGE PROCESS, V29, P1618, DOI 10.1109/TIP.2019.2912358
   Wang D, 2014, IEEE IJCNN, P112, DOI 10.1109/IJCNN.2014.6889457
   Wang YD, 2015, LECT NOTES COMPUT SC, V9003, P525, DOI 10.1007/978-3-319-16865-4_34
   Wu J, 2017, IEEE T MULTIMEDIA, V19, P1156, DOI 10.1109/TMM.2017.2652065
   Xia B, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2936, DOI 10.1145/3394171.3413774
   Xia ZQ, 2020, IEEE T IMAGE PROCESS, V29, P8590, DOI 10.1109/TIP.2020.3018222
   Xia ZQ, 2020, IEEE T MULTIMEDIA, V22, P626, DOI 10.1109/TMM.2019.2931351
   Xie HX, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2871, DOI 10.1145/3394171.3414012
   Xu F, 2017, IEEE T AFFECT COMPUT, V8, P254, DOI 10.1109/TAFFC.2016.2518162
   Yan WJ, 2013, IEEE INT CONF AUTOMA
   Yan WJ, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0086041
   Yan Y, 2016, IEEE T MULTIMEDIA, V18, P2494, DOI 10.1109/TMM.2016.2602938
   Yang Y, 2015, INT J COMPUT VISION, V113, P113, DOI 10.1007/s11263-014-0781-x
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
   Zhu XN, 2018, MULTIMED TOOLS APPL, V77, P3105, DOI 10.1007/s11042-017-4943-z
   Zong Y, 2020, IEEE T CYBERNETICS, V50, P5047, DOI 10.1109/TCYB.2019.2914512
   Zong Y, 2018, IEEE T MULTIMEDIA, V20, P3160, DOI 10.1109/TMM.2018.2820321
NR 59
TC 5
Z9 5
U1 7
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5429
EP 5443
DI 10.1109/TMM.2022.3192727
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300058
DA 2024-07-18
ER

PT J
AU Chen, YC
   Liu, M
   Wang, XP
   Wang, F
   Liu, AA
   Wang, YN
AF Chen, Yongchun
   Liu, Min
   Wang, Xueping
   Wang, Fei
   Liu, An-An
   Wang, Yaonan
TI Refining Noisy Labels With Label Reliability Perception for Person
   Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Noise measurement; Reliability; Training; Task analysis; Annotations;
   Refining; Image classification; Feature-fusion; label dual perception;
   label noise; person Re-ID
AB Most person re-identification (Re-ID) approaches rely excessively on a great quantity of annotated training data. However, due to sampling errors or annotated errors, the label noise is unavoidable, which usually causes a dramatic decrease in the performance of existing Re-ID methods. To address this problem, we propose the label reliability perception (LRP) for person Re-ID by refining noisy labels. Specifically, a feature-fusion block (FFB) is proposed to enhance the discrimen- ability of pedestrians' features by expanding the network's attention span due to the fused feature, which is generated by overlapping the coarse-grained feature obtained by global average pooling and fine-grained features obtained by evenly dividing the feature map in the height dimension and performing global max pooling. In addition, the label dual perception (LDP) is proposed to refine noisy labels instead of filtering samples by evaluating the reliability of each training sample's label. Specifically, we meticulously design five evaluation modes for each sample to perceive the reliability of the labels of the k-nearest neighbor images. Finally, we utilize the most reliable label to replace the noisy label and optimize the network. Extensive experiments prove the superiority of the proposed model over the competing methods; for instance, on Market1501, our method achieves 88.8% rank-1 accuracy and 70.5% mAP (4.7% and 4.3% improvements over the state-of-the-arts) under noise ratio 20%, and similarly on DukeMTMC-ReID, our method achieves 77.7% and 60.3%.
C1 [Chen, Yongchun; Liu, Min; Wang, Xueping; Wang, Fei; Wang, Yaonan] Hunan Univ, Coll Elect & Informat Engn, Changsha 410082, Peoples R China.
   [Chen, Yongchun; Liu, Min; Wang, Xueping; Wang, Fei; Wang, Yaonan] Natl Engn Lab Robot Visual Percept & Control Tech, Changsha 410082, Peoples R China.
   [Liu, An-An] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
C3 Hunan University; Tianjin University
RP Liu, M (corresponding author), Hunan Univ, Coll Elect & Informat Engn, Changsha 410082, Peoples R China.
EM cyc1997@hnu.edu.cn; liu_min@hnu.edu.cn; wang_xueping@hnu.edu.cn;
   wang_fei@hnu.edu.cn; anan0422@gmail.com; yaonan@hnu.edu.cn
OI Wang, Xueping/0000-0003-4862-8975
FU National Natural Science Foundation of China
FX No Statement Available
CR Babenko A, 2015, IEEE I CONF COMP VIS, P1269, DOI 10.1109/ICCV.2015.150
   Bai X, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107036
   Chen DP, 2018, PROC CVPR IEEE, pCP1, DOI 10.1109/CVPR.2018.00128
   Chen GY, 2019, IEEE T IMAGE PROCESS, V28, P4192, DOI 10.1109/TIP.2019.2908062
   Chen J, 2022, IEEE T MULTIMEDIA, V24, P4285, DOI 10.1109/TMM.2021.3114539
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding GD, 2019, IEEE T MULTIMEDIA, V21, P2891, DOI 10.1109/TMM.2019.2916456
   Frénay B, 2014, IEEE T NEUR NET LEAR, V25, P845, DOI 10.1109/TNNLS.2013.2292894
   Ge Y., 2020, P INT C LEARN REPR, P322
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hongxin Wei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13723, DOI 10.1109/CVPR42600.2020.01374
   Lee KH, 2018, PROC CVPR IEEE, P5447, DOI 10.1109/CVPR.2018.00571
   Li YC, 2017, IEEE I CONF COMP VIS, P1928, DOI 10.1109/ICCV.2017.211
   Liang M, 2015, PROC CVPR IEEE, P3367, DOI 10.1109/CVPR.2015.7298958
   Liao SC, 2015, IEEE I CONF COMP VIS, P3685, DOI 10.1109/ICCV.2015.420
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Liu AA, 2021, IEEE T MULTIMEDIA, V23, P4515, DOI 10.1109/TMM.2020.3043084
   Liu JX, 2018, PROC CVPR IEEE, P4099, DOI 10.1109/CVPR.2018.00431
   Liu L, 2020, INT J COMPUT VISION, V128, P261, DOI 10.1007/s11263-019-01247-4
   Lu J, 2018, PR MACH LEARN RES, V80
   Ma C, 2021, IEEE T MULTIMEDIA, V23, P3943, DOI 10.1109/TMM.2020.3034534
   Matsukawa T, 2016, PROC CVPR IEEE, P1363, DOI 10.1109/CVPR.2016.152
   Radenovic F, 2019, IEEE T PATTERN ANAL, V41, P1655, DOI 10.1109/TPAMI.2018.2846566
   Razavian A. S., 2016, ITE Trans. Media Technol. Appl., V4, P251, DOI [DOI 10.3169/MTA.4.251, 10.3169/mta.4.251]
   Ren LL, 2019, IEEE T IMAGE PROCESS, V28, P4970, DOI 10.1109/TIP.2019.2915655
   Sarfraz MS, 2018, PROC CVPR IEEE, P420, DOI 10.1109/CVPR.2018.00051
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Tanaka D, 2018, PROC CVPR IEEE, P5552, DOI 10.1109/CVPR.2018.00582
   Tang Y, 2021, IEEE T IMAGE PROCESS, V30, P8483, DOI 10.1109/TIP.2021.3115672
   Veit A, 2017, PROC CVPR IEEE, P6575, DOI 10.1109/CVPR.2017.696
   Wang AR, 2015, IEEE T MULTIMEDIA, V17, P1887, DOI 10.1109/TMM.2015.2476655
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang JY, 2018, PROC CVPR IEEE, P2275, DOI 10.1109/CVPR.2018.00242
   Wang XP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15077, DOI 10.1109/ICCV48922.2021.01482
   Wang XP, 2021, IEEE T IMAGE PROCESS, V30, P3017, DOI 10.1109/TIP.2021.3056223
   Wang YS, 2018, PROC CVPR IEEE, P8688, DOI 10.1109/CVPR.2018.00906
   Wang YS, 2019, IEEE I CONF COMP VIS, P322, DOI 10.1109/ICCV.2019.00041
   Wu Y, 2019, IEEE T IMAGE PROCESS, V28, P2872, DOI 10.1109/TIP.2019.2891895
   Wu Y, 2018, PROC CVPR IEEE, P5177, DOI 10.1109/CVPR.2018.00543
   Xiao T, 2015, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2015.7298885
   Xu XX, 2015, IEEE T NEUR NET LEAR, V26, P3150, DOI 10.1109/TNNLS.2015.2405574
   Yang QZ, 2021, IEEE T PATTERN ANAL, V43, P2029, DOI 10.1109/TPAMI.2019.2960509
   Yang Y, 2014, LECT NOTES COMPUT SC, V8689, P536, DOI 10.1007/978-3-319-10590-1_35
   Ye M, 2022, IEEE T IMAGE PROCESS, V31, P379, DOI 10.1109/TIP.2021.3131937
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Ye M, 2020, IEEE T INF FOREN SEC, V15, P2655, DOI 10.1109/TIFS.2020.2970590
   Ye M, 2017, IEEE I CONF COMP VIS, P5152, DOI 10.1109/ICCV.2017.550
   Ye M, 2016, IEEE T MULTIMEDIA, V18, P2553, DOI 10.1109/TMM.2016.2605058
   Yu HX, 2020, IEEE T PATTERN ANAL, V42, P956, DOI 10.1109/TPAMI.2018.2886878
   Yu TY, 2019, IEEE I CONF COMP VIS, P552, DOI 10.1109/ICCV.2019.00064
   Zeng SJ, 2024, IEEE T NEUR NET LEAR, V35, P1013, DOI 10.1109/TNNLS.2022.3179133
   Zhang Hongyi, 2018, MIXUP EMPIRICAL RISK, DOI DOI 10.48550/ARXIV.1710.09412
   Zhang Q, 2022, IEEE T MULTIMEDIA, V24, P1545, DOI 10.1109/TMM.2021.3067463
   Zheng F, 2019, PROC CVPR IEEE, P8506, DOI 10.1109/CVPR.2019.00871
   Zheng L, 2017, PROC CVPR IEEE, P3346, DOI 10.1109/CVPR.2017.357
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3159171
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhou HY, 2020, IEEE T MULTIMEDIA, V22, P1496, DOI 10.1109/TMM.2019.2943740
   Zhou Q, 2021, IEEE T IMAGE PROCESS, V30, P1623, DOI 10.1109/TIP.2019.2914575
NR 62
TC 4
Z9 4
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 9479
EP 9490
DI 10.1109/TMM.2023.3253391
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP7K5
UT WOS:001133324200031
DA 2024-07-18
ER

PT J
AU Cheng, J
   Hao, FS
   He, FX
   Liu, L
   Zhang, QS
AF Cheng, Jun
   Hao, Fusheng
   He, Fengxiang
   Liu, Liu
   Zhang, Qieshi
TI Mixer-Based Semantic Spread for Few-Shot Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Few-shot learning; metric learning-based meta-learning
ID NEURAL-NETWORK
AB Key semantics can come from everywhere on an image. Semantic alignment is a key part of few-shot learning but still remains challenging. In this paper, we design a Mixer-Based Semantic Spread (MBSS) algorithm that employs a mixer module to spread the key semantic on the whole image, so that one can directly compare the processed image pairs. We first adopt a convolutional neural network to extract features from both support and query images and separate each of them into multiple Local Descriptor-based Representations (LDRs). The LDRs are then fed into the mixer for semantic spread, where every LDR attracts complementary information from its peers. In this way, the objective semantic is made spread on the whole image in a data-driven manner. The overall pipeline is supervised by a voting-based loss, guaranteeing a good mixer. Visualization results validate the feasibility of our mixer. Comprehensive experiments on three benchmark datasets, miniImageNet, tieredImageNet, and CUB, show that our algorithm achieves the state-of-the-art performance in both 5-way 1-shot and 5-way 5-shot settings.
C1 [Cheng, Jun; Hao, Fusheng; Zhang, Qieshi] Chinese Acad Sci, Shenzhen Inst Adv Technol, CAS Key Lab Human Machine Intelligence Synergy Sys, Shenzhen 518055, Peoples R China.
   [Cheng, Jun; Hao, Fusheng; Zhang, Qieshi] Chinese Univ Hong Kong, Hong Kong, Peoples R China.
   [He, Fengxiang] JD Explore Acad, JDcom, Beijing 100176, Peoples R China.
   [Liu, Liu] Univ Sydney, Fac Engn, UBTECH Sydney Artificial Intelligence Ctr, Sch Comp Sci, Darlington, NSW 2008, Australia.
C3 Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology,
   CAS; Chinese University of Hong Kong; University of Sydney
RP Hao, FS (corresponding author), Chinese Acad Sci, Shenzhen Inst Adv Technol, CAS Key Lab Human Machine Intelligence Synergy Sys, Shenzhen 518055, Peoples R China.; Hao, FS (corresponding author), Chinese Univ Hong Kong, Hong Kong, Peoples R China.
EM jun.cheng@siat.ac.cn; fs.hao@siat.ac.cn; fengxiang.f.he@gmail.com;
   liu.liu1@sydney.edu.au; qs.zhang@siat.ac.cn
OI Zhang, Qieshi/0000-0001-6358-1840; liu, liu/0000-0002-8128-2788
FU National Natural Science Foundation of China [U1713213, 61772508,
   U1913202, U1813205]; Shenzhen Technology Project [JCYJ20180507182610734,
   JSGG20191129094012321]; Shenzhen Engineering Laboratory for 3D Content
   Generating Technologies [[2017]476]; CAS Key Technology Talent Program;
   SIAT Innovation Program for Excellent Young Researchers [E1G032]; ARC
   [DP-180103424]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants U1713213, 61772508, U1913202, and
   U1813205, in part by Shenzhen Technology Project under Grants
   JCYJ20180507182610734 and JSGG20191129094012321, in part by the Shenzhen
   Engineering Laboratory for 3D Content Generating Technologies under
   Grant [2017]476, in part by CAS Key Technology Talent Program, and in
   part by the SIAT Innovation Program for Excellent Young Researchers
   under Grant E1G032. The work of Dr. Liu Liu was supported by ARC Project
   DP-180103424.
CR Allen KR, 2019, PR MACH LEARN RES, V97
   Aoxue Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12573, DOI 10.1109/CVPR42600.2020.01259
   Bauer M, 2017, Arxiv, DOI [arXiv:1706.00326, 10.48550/ARXIV.1706.00326]
   Bertinetto Luca, 2019, PROC INT C LEARN REP
   Bin Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P438, DOI 10.1007/978-3-030-58548-8_26
   Boudiaf M., 2020, Advances in Neural Information Processing Systems (NeurIPS), V33, P2445, DOI DOI 10.5555/3495724.3495930
   Cai Q, 2018, PROC CVPR IEEE, P4080, DOI 10.1109/CVPR.2018.00429
   Chauhan J., 2020, ICLR
   Chen W.-Y., 2019, PROC INT C LEARN REP
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Elsken Thomas, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12362, DOI 10.1109/CVPR42600.2020.01238
   Finn C, 2018, ADV NEUR IN, V31
   Finn C, 2017, PR MACH LEARN RES, V70
   Gidaris S, 2019, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2019.00011
   Gidaris S, 2018, PROC CVPR IEEE, P4367, DOI 10.1109/CVPR.2018.00459
   Goodfellow I, 2014, P ADV NEUR INF PROC, P2371
   Gordon J., 2019, INT C LEARN REPRESEN
   Grant E., 2018, INT C LEARN REPRESEN
   Han-Jia Ye, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8805, DOI 10.1109/CVPR42600.2020.00883
   Hao F., 2019, IEEE ACCESS, V7, p100 501
   Hao FS, 2019, IEEE I CONF COMP VIS, P8459, DOI 10.1109/ICCV.2019.00855
   Hariharan B, 2017, IEEE I CONF COMP VIS, P3037, DOI 10.1109/ICCV.2017.328
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   Hilliard N, 2018, Arxiv, DOI arXiv:1802.04376
   Hu S. X, 2020, PROC INT C LEARN REP, P1
   Hu YF, 2021, IEEE T MULTIMEDIA, V23, P4285, DOI 10.1109/TMM.2020.3039329
   Huang HW, 2021, PATTERN RECOGN, V116, DOI 10.1016/j.patcog.2021.107935
   Huang HX, 2021, IEEE T MULTIMEDIA, V23, P1666, DOI 10.1109/TMM.2020.3001510
   Jinlu Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P741, DOI 10.1007/978-3-030-58452-8_43
   Jong-Chyi Su, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P645, DOI 10.1007/978-3-030-58571-6_38
   Kim J, 2019, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2019.00010
   Kingma D. P., 2014, arXiv
   Koch G., 2015, ICML DEEP LEARNING W, V2
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee K, 2019, PROC CVPR IEEE, P10649, DOI 10.1109/CVPR.2019.01091
   Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79
   Li HY, 2019, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2019.00009
   Li WB, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2957
   Li WB, 2019, PROC CVPR IEEE, P7253, DOI 10.1109/CVPR.2019.00743
   Li WB, 2019, AAAI CONF ARTIF INTE, P8642
   Li ZG, 2017, Arxiv, DOI arXiv:1707.09835
   Lifchitz Y, 2021, INT C PATT RECOG, P10457, DOI 10.1109/ICPR48806.2021.9412178
   Lifchitz Y, 2019, PROC CVPR IEEE, P9250, DOI 10.1109/CVPR.2019.00948
   Ling Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13387, DOI 10.1109/CVPR42600.2020.01340
   Liu Y., 2019, PROC INT C LEARN REP, P1
   Lu J, 2021, IEEE T NEUR NET LEAR, V32, P1433, DOI 10.1109/TNNLS.2020.2984710
   Mangla P, 2020, IEEE WINT CONF APPL, P2207, DOI [10.1109/wacv45572.2020.9093338, 10.1109/WACV45572.2020.9093338]
   Mehrotra A, 2017, Arxiv, DOI arXiv:1703.08033
   Mishra N., 2018, INT C LEARN REPRESEN
   Munkhdalai T, 2018, PR MACH LEARN RES, V80
   Munkhdalai T, 2017, PR MACH LEARN RES, V70
   Nichol A, 2018, Arxiv, DOI [arXiv:1803.02999, DOI 10.48550/ARXIV.1803.02999]
   Oreshkin BN, 2018, ADV NEUR IN, V31
   Passalis N, 2021, IEEE T NEUR NET LEAR, V32, P925, DOI 10.1109/TNNLS.2020.2979745
   Qiao SY, 2018, PROC CVPR IEEE, P7229, DOI 10.1109/CVPR.2018.00755
   Ravi S, 2016, OPTIMIZATION MODEL F
   Ren M., 2018, PROC INT C LEARN REP
   Ren MY, 2019, Arxiv, DOI arXiv:1810.07218
   Rodriguez Pau, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P121, DOI 10.1007/978-3-030-58574-7_8
   Rusu Andrei A., 2019, PROC INT C LEARN REP
   Simon C, 2020, PROC CVPR IEEE, P4135, DOI 10.1109/CVPR42600.2020.00419
   Snell J, 2017, ADV NEUR IN, V30
   Sun QR, 2019, PROC CVPR IEEE, P403, DOI 10.1109/CVPR.2019.00049
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Van Nhan Nguyen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P118, DOI 10.1007/978-3-030-58592-1_8
   Victor G., 2018, PROC INT C LEARN REP
   Vinyals O, 2016, 30 C NEURAL INFORM P, V29
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wang JZ, 2018, IEEE T MULTIMEDIA, V20, P2578, DOI 10.1109/TMM.2018.2855081
   Wang QR, 2019, IEEE T MULTIMEDIA, V21, P1839, DOI 10.1109/TMM.2018.2890360
   Wang QR, 2019, IEEE T MULTIMEDIA, V21, P930, DOI 10.1109/TMM.2018.2869277
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang YX, 2018, PROC CVPR IEEE, P7278, DOI 10.1109/CVPR.2018.00760
   Yan SP, 2019, AAAI CONF ARTIF INTE, P9079
   Yaoyao Liu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P404, DOI 10.1007/978-3-030-58517-4_24
   Yikai Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12833, DOI 10.1109/CVPR42600.2020.01285
   Yonglong Tian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P266, DOI 10.1007/978-3-030-58568-6_16
   Yue Z., 2020, P 34 C NEUR INF PROC, P2734
   Zhang C., 2020, P IEEE CVF C COMP VI, P203
   Zhang RX, 2018, ADV NEUR IN, V31
   Zhang X, 2017, IEEE I CONF COMP VIS, P4605, DOI 10.1109/ICCV.2017.492
   Zhang XT, 2020, Arxiv, DOI arXiv:1811.07100
   Zhu YH, 2021, IEEE T MULTIMEDIA, V23, P1200, DOI 10.1109/TMM.2020.2993952
   US
NR 84
TC 5
Z9 5
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 191
EP 202
DI 10.1109/TMM.2021.3123813
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 9E8KM
UT WOS:000937028400014
DA 2024-07-18
ER

PT J
AU Deng, HA
   Yang, ZG
   Hao, TY
   Li, Q
   Liu, WY
AF Deng, Huan
   Yang, Zhenguo
   Hao, Tianyong
   Li, Qing
   Liu, Wenyin
TI Multimodal Affective Computing With Dense Fusion Transformer for Inter-
   and Intra-Modality Interactions
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Transformers; Feature extraction; Discrete Fourier transforms;
   Computational modeling; Affective computing; Visualization; Fuses;
   Multimodal emotion recognition; multimodal fusion; multimodal
   representation learning; multimodal sentiment analysis
ID SENTIMENT ANALYSIS
AB This paper proposes a dense fusion transformer (DFT) framework to integrate textual, acoustic, and visual information for multimodal affective computing. DFT exploits a modality-shared transformer (MT) module to extract the modality-shared features by modelling unimodal, bimodal, and trimodal interactions jointly. MT constructs a series of dense fusion blocks to fuse utterance-level sequential features of the multiple modalities from the perspectives of low-level and high-level semantics. In particular, MT adopts local and global transformers to learn modality-shared representations by modelling inter- and intra-modality interactions. Furthermore, we devise a modality-specific representation (MR) module with a soft orthogonality constraint to penalize the distance between modality-specific and modality-shared representations, which are fused by a transformer to make affective predictions. Extensive experiments conducted on five public benchmark datasets show that DFT outperforms the state-of-the-art baselines.
C1 [Deng, Huan; Yang, Zhenguo; Liu, Wenyin] Guangdong Univ Technol, Sch Comp Sci, Guangzhou 510627, Peoples R China.
   [Hao, Tianyong] South China Normal Univ, Sch Comp Sci, Guangzhou 510631, Peoples R China.
   [Li, Qing] Hong Kong Polytech Univ, Dept Comp, Hong Kong, Peoples R China.
C3 Guangdong University of Technology; South China Normal University; Hong
   Kong Polytechnic University
RP Yang, ZG (corresponding author), Guangdong Univ Technol, Sch Comp Sci, Guangzhou 510627, Peoples R China.
EM csdengh@outlook.com; yzg@gdut.edu.cn; haoty@m.scnu.edu.cn;
   csqli@comp.polyu.edu.hk; liuwy@gdut.edu.cn
RI Li, Qing/JMH-1365-2023; Hao, Tianyong/HJH-2742-2023
OI Li, Qing/0000-0003-3370-471X; Hao, Tianyong/0000-0002-9792-3949; Liu,
   Wenyin/0000-0002-6237-6607
FU Science and Technology Program of Guangzhou [202102020524]; Guangdong
   Innovative Research Team Program [2014ZT05G157]; Hong Kong RGC CRF
   Project [C1031-18G]
FX This work was supported in part by the Science and Technology Program of
   Guangzhou under Grant 202102020524, in part by Guangdong Innovative
   Research Team Program under Grant 2014ZT05G157, and in part by Hong Kong
   RGC CRF Project under Garnt C1031-18G. This paper is an extended version
   of the conference paper [1] published in ICME 2021. The Associate Editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Metin Sezgin.
CR Akhtar MS, 2020, ACM T KNOWL DISCOV D, V14, DOI 10.1145/3380744
   Bousmalis G., 2016, Advances in Neural Information Processing Systems, V29, P1
   Busso C, 2008, LANG RESOUR EVAL, V42, P335, DOI 10.1007/s10579-008-9076-6
   Chauhan DS, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5647
   Degottex G, 2014, INT CONF ACOUST SPEE, DOI 10.1109/ICASSP.2014.6853739
   Deng H., 2021, P IEEE INT C MULT EX, P1
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Ghaleb E, 2020, IEEE MULTIMEDIA, V27, P37, DOI 10.1109/MMUL.2019.2960219
   Gu Y, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P537, DOI 10.1145/3240508.3240714
   Guo WY, 2021, IEEE T MULTIMEDIA, V23, P1785, DOI 10.1109/TMM.2020.3003648
   Hazarika D, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1122, DOI 10.1145/3394171.3413678
   Kampman O, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 2, P606
   Li ZC, 2022, IEEE T PATTERN ANAL, V44, P9904, DOI 10.1109/TPAMI.2021.3132068
   Li ZC, 2019, IEEE T PATTERN ANAL, V41, P2070, DOI 10.1109/TPAMI.2018.2852750
   Li ZC, 2015, IEEE T MULTIMEDIA, V17, P1989, DOI 10.1109/TMM.2015.2477035
   Liang PP, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P150
   Liu Z, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2247
   Mai SJ, 2022, IEEE T AFFECT COMPUT, V13, P320, DOI 10.1109/TAFFC.2020.3000510
   Mai SJ, 2022, IEEE T MULTIMEDIA, V24, P2488, DOI 10.1109/TMM.2021.3082398
   Mai SJ, 2021, IEEE-ACM T AUDIO SPE, V29, P1424, DOI 10.1109/TASLP.2021.3068598
   Mai SJ, 2020, AAAI CONF ARTIF INTE, V34, P164
   Mai SJ, 2020, IEEE T MULTIMEDIA, V22, P122, DOI 10.1109/TMM.2019.2925966
   Majumder N, 2018, KNOWL-BASED SYST, V161, P124, DOI 10.1016/j.knosys.2018.07.041
   Morency L.-P., 2011, P 13 INT C MULT INT, P169, DOI DOI 10.1145/2070481.2070509
   Morvant E, 2014, LECT NOTES COMPUT SC, V8621, P153, DOI 10.1007/978-3-662-44415-3_16
   Nojavanasghari B, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P284, DOI 10.1145/2993148.2993176
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Pham H, 2019, AAAI CONF ARTIF INTE, P6892
   Poria S, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P873, DOI 10.18653/v1/P17-1081
   Poria S, 2016, IEEE DATA MINING, P439, DOI [10.1109/ICDM.2016.178, 10.1109/ICDM.2016.0055]
   Rahman W, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P2359, DOI 10.18653/v1/2020.acl-main.214
   Sahay S, 2020, PROCEEDINGS OF THE SECOND GRAND CHALLENGE AND WORKSHOP ON MULTIMODAL LANGUAGE (CHALLENGE-HML), VOL 1, P29
   Salakhutdinov R., 2018, P INT C LEARN REPR
   Shepstone SE, 2014, IEEE T MULTIMEDIA, V16, P1999, DOI 10.1109/TMM.2014.2337845
   Sun ZK, 2020, AAAI CONF ARTIF INTE, V34, P8992
   Tsai YHH, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P1823, DOI 10.18653/v1/2020.emnlp-main.143
   Tsai YHH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6558, DOI 10.18653/v1/p19-1656
   Wang YS, 2019, AAAI CONF ARTIF INTE, P7216
   Wang YH, 2021, IEEE T MULTIMEDIA, V23, P2782, DOI 10.1109/TMM.2020.3016222
   Williams J, 2018, FIRST GRAND CHALLENGE AND WORKSHOP ON HUMAN MULTIMODAL LANGUAGE (CHALLENGE-HML), P11
   Wöllmer M, 2013, IEEE INTELL SYST, V28, P46, DOI 10.1109/MIS.2013.34
   Wu YC, 2016, IEEE T MULTIMEDIA, V18, P2135, DOI 10.1109/TMM.2016.2614220
   Xu C, 2013, Arxiv, DOI [arXiv:1304.5634, DOI 10.48550/ARXIV.1304.5634]
   Yao SW, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P4346
   Yuan J, 2008, J ACOUST SOC AM, V124, P2078, DOI 10.1121/1.2968700
   Zadeh A., 2017, P 2017 C EMP METH NA, P1103, DOI 10.18653/v1/D17-1115
   Zadeh A, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2236
   Zadeh A, 2018, AAAI CONF ARTIF INTE, P5642
   Zadeh A, 2018, AAAI CONF ARTIF INTE, P5634
   Zadeh A, 2016, IEEE INTELL SYST, V31, P82, DOI 10.1109/MIS.2016.94
   Zellinger W, 2019, INFORM SCIENCES, V483, P174, DOI 10.1016/j.ins.2019.01.025
   Zhong HS, 2021, IEEE T MULTIMEDIA, V23, P1264, DOI 10.1109/TMM.2020.2995278
NR 52
TC 2
Z9 2
U1 15
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6575
EP 6587
DI 10.1109/TMM.2022.3211197
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500067
DA 2024-07-18
ER

PT J
AU Deng, SL
   Yu, JG
   Wu, ZH
   Gao, HX
   Li, YS
   Yang, Y
AF Deng, Shule
   Yu, Jin-Gang
   Wu, Zihao
   Gao, Hongxia
   Li, Yansheng
   Yang, Yang
TI Learning Relative Feature Displacement for Few-Shot Open-Set Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Few-shot learning; open-set recognition; relative feature displacement;
   randomness drift
AB Few-shot learning (FSL) usually assumes that the query is drawn from the same label space as the support set, while queries from unknown classes may emerge unexpectedly in many open-world application scenarios. Such an open-set issue will limit the practical deployment of FSL systems, which remains largely unexplored. In this paper, we investigate the problem of few-shot open-set recognition (FSOR) and propose a novel solution, called Relative Feature Displacement Network (RFDNet), which empowers FSL systems to reject queries from unknown classes while accurately classifying those from known classes. First, we suggest a different relative feature displacement learning (RFDL) paradigm for FSOR, i.e., meta-learning a feature displacement relative to a pretrained reference feature embedding, based on our insightful observations on the randomness drift issue of previous meta-learning based for FSOR methods, as well as the generalization ability of the feature embedding pretrained for general classification. Second, we design the RFDNet framework to implement the RFDL paradigm, which is mainly featured by a task aware RFD generator and a marginal open-set loss. Comprehensive experiments on three public datasets, i.e., mini ImageNet, CIFARFS and tiered ImageNet, demonstrate that RFDNet can consistently outperform the state-of-the-art methods, achieving improvement of 5.2%, 2.0% and 1.7% respectively, in terms of AUROC for unknown-class rejection under the 5-way 5-shot setting.
C1 [Deng, Shule; Yu, Jin-Gang; Wu, Zihao; Gao, Hongxia] South China Univ Technol, Sch Automat Sci & Engn, Guangzhou 510641, Peoples R China.
   [Yu, Jin-Gang] Pazhou Lab, Guangzhou 510335, Peoples R China.
   [Li, Yansheng] Wuhan Univ, Sch Remote Sensing & Informat Engn, Wuhan 430079, Peoples R China.
   [Yang, Yang] Xi An Jiao Tong Univ, Sch Elect & Informat Engn, Xian 710049, Peoples R China.
C3 South China University of Technology; Pazhou Lab; Wuhan University;
   Xi'an Jiaotong University
RP Yu, JG (corresponding author), South China Univ Technol, Sch Automat Sci & Engn, Guangzhou 510641, Peoples R China.
EM audsl@mail.scut.edu.cn; jingangyu@scut.edu.cn; auwzh@mail.scut.edu.cn;
   hxgao@scut.edu.cn; yansheng.li@whu.edu.cn; yyang@mail.xjtu.edu.cn
OI Yang, Yang/0000-0001-8687-4427
FU National Natural Science Foundation of China [62076099, 61703166]; Key
   Research and Development Program of Guangzhou [202103010003]; Key
   Science and Technology Program of Foshan [2020001006285]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62076099 and 61703166, in part by the
   Key Research and Development Program of Guangzhou under Grant
   202103010003, and in part by the Key Science and Technology Program of
   Foshan under Grant 2020001006285.
CR Altae-Tran H, 2017, ACS CENTRAL SCI, V3, P283, DOI 10.1021/acscentsci.6b00367
   [Anonymous], 2009, CIFAR-100 (canadian institute for advanced research)
   Bendale A, 2016, PROC CVPR IEEE, P1563, DOI 10.1109/CVPR.2016.173
   Bertinetto L., 2016, Advances in neural information processing systems, P523
   Bertinetto Luca, 2018, P INT C LEARN REPR
   Bo Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8795, DOI 10.1109/CVPR42600.2020.00882
   Chen W.Y., 2019, ICLR, DOI DOI 10.1109/MSR.2015.54
   Chen YB, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9042, DOI 10.1109/ICCV48922.2021.00893
   Dhamija A. R., 2018, ADV NEURAL INFORM PR, P9157
   Dhillon G.S., 2020, P INT C LEARN REPR
   Finn C, 2017, PR MACH LEARN RES, V70
   Ge Zongyuan, 2017, BMVC
   Geng CX, 2021, IEEE T PATTERN ANAL, V43, P3614, DOI 10.1109/TPAMI.2020.2981604
   Gidaris S, 2018, PROC CVPR IEEE, P4367, DOI 10.1109/CVPR.2018.00459
   Han-Jia Ye, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8805, DOI 10.1109/CVPR42600.2020.00883
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hongjie Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P102, DOI 10.1007/978-3-030-58580-8_7
   Huang HX, 2021, IEEE T MULTIMEDIA, V23, P1666, DOI 10.1109/TMM.2020.3001510
   Jeong M, 2021, PROC CVPR IEEE, P12561, DOI 10.1109/CVPR46437.2021.01238
   Kang D, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8802, DOI 10.1109/ICCV48922.2021.00870
   Keshari R, 2018, PROC CVPR IEEE, P9349, DOI 10.1109/CVPR.2018.00974
   Koch G., 2015, ICML DEEP LEARNING W, V2
   Kwitt R, 2016, PROC CVPR IEEE, P78, DOI 10.1109/CVPR.2016.16
   Lee K, 2019, PROC CVPR IEEE, P10649, DOI 10.1109/CVPR.2019.01091
   Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79
   Li JJ, 2021, AAAI CONF ARTIF INTE, V35, P8401
   Lifchitz Y, 2019, PROC CVPR IEEE, P9250, DOI 10.1109/CVPR.2019.00948
   Miller EG, 2000, PROC CVPR IEEE, P464, DOI 10.1109/CVPR.2000.855856
   Nakamura A., 2020, P INT C LEARN REPR, P1
   Oreshkin BN, 2018, ADV NEUR IN, V31
   Oza P, 2019, PROC CVPR IEEE, P2302, DOI 10.1109/CVPR.2019.00241
   Perera Pramuditha, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11811, DOI 10.1109/CVPR42600.2020.01183
   Qi H, 2018, PROC CVPR IEEE, P5822, DOI 10.1109/CVPR.2018.00610
   Ravi S., 2017, C TRACK P, P1
   Ren M., 2018, INT C LEARNING REPRE, DOI DOI 10.1109/IPFA.2018.8452547
   Scheirer WJ, 2013, IEEE T PATTERN ANAL, V35, P1757, DOI 10.1109/TPAMI.2012.256
   Schwartz E, 2018, ADV NEUR IN, V31
   Snell J, 2017, ADV NEUR IN, V30
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Sutskever I., 2013, INT C MACHINE LEARNI, P1139
   Svenmarck P., 2018, P NATO BIG DAT ART I, P1
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vinyals O, 2016, 30 C NEURAL INFORM P, V29
   Wang X, 2019, PROC CVPR IEEE, P1831, DOI 10.1109/CVPR.2019.00193
   Wang YQ, 2020, ACM COMPUT SURV, V53, DOI 10.1145/3386252
   Xin Sun, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13477, DOI 10.1109/CVPR42600.2020.01349
   Yonglong Tian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P266, DOI 10.1007/978-3-030-58568-6_16
   Yoo D, 2018, AAAI CONF ARTIF INTE, P4382
   Yoon S.W., 2020, P MACHINE LEARNING R, P10852
   Yoon SW, 2019, PR MACH LEARN RES, V97
   Yoshihashi R, 2019, PROC CVPR IEEE, P4011, DOI 10.1109/CVPR.2019.00414
   Zhao F, 2018, LECT NOTES COMPUT SC, V11219, P20, DOI 10.1007/978-3-030-01267-0_2
   Zhu YH, 2021, IEEE T MULTIMEDIA, V23, P1200, DOI 10.1109/TMM.2020.2993952
NR 53
TC 3
Z9 3
U1 5
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5763
EP 5774
DI 10.1109/TMM.2022.3198880
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500009
DA 2024-07-18
ER

PT J
AU Ding, XY
   Liu, XQ
   Luo, X
   Xu, XS
AF Ding, Xue-Ying
   Liu, Xiao-Qian
   Luo, Xin
   Xu, Xin-Shun
TI DOC: Text Recognition via Dual Adaptation and Clustering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Adaptation models; Text recognition; Task analysis;
   Image recognition; Training; Data models; unsupervised domain
   adaptation; domain shift; clustering
ID NETWORK
AB More recently, unsupervised domain adaptation has been introduced to text image recognition tasks for serious domain shift problem, which can transfer knowledge from source domains to target ones. Moreover, in unsupervised domain adaptation for text recognition, there is no label information in the target domain to supervise the domain adaptation, especially at the character. Several existing methods regard a text image as a whole and perform only on global feature adaptation, neglecting local-level feature adaptation, i.e., characters. Others methods only focus their attention on word-level feature alignment while ignoring the categories of local-level characters. To address these issues, we propose a text recognition model via Dual adaptatiOn and Clustering, DOC for short. Regarding word-level, we construct a Global Discriminator for global feature adaptation to reduce text layout bias between source and target domains. Regarding character-level, we propose an Adaptive Feature Clustering (AFC) module, which can extract invariant character features through a local-level discriminator for adaptation. Moreover, it enhances the local-feature adaptation by a clustering scheme, which evaluates the feature adaptation by leveraging the knowledge from the source domain as much as possible. In this way, it can pay more attention to the differences in fine-grained characters. Extensive experiments on benchmark datasets demonstrate that our framework can achieve state-of-the-art performance.
C1 [Ding, Xue-Ying; Liu, Xiao-Qian; Luo, Xin; Xu, Xin-Shun] Shandong Univ, Sch Software, Jinan 250101, Peoples R China.
C3 Shandong University
RP Xu, XS (corresponding author), Shandong Univ, Sch Software, Jinan 250101, Peoples R China.
EM 202015188@mail.sdu.edu.cn; jlrxqxq370322@126.com; luoxin.lxin@gmail.com;
   xuxinshun@sdu.edu.cn
RI Luo, Xin/HNR-3191-2023
OI Luo, Xin/0000-0002-6901-5476; Liu, Xiaoqain/0000-0002-2187-8598
FU National Natural Science Foundation of China
FX No Statement Available
CR Baek J, 2019, IEEE I CONF COMP VIS, P4714, DOI 10.1109/ICCV.2019.00481
   Bhunia AK, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P963, DOI 10.1109/ICCV48922.2021.00102
   Biten AF, 2019, IEEE I CONF COMP VIS, P4290, DOI 10.1109/ICCV.2019.00439
   BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792
   Chen JY, 2021, PROC CVPR IEEE, P12021, DOI 10.1109/CVPR46437.2021.01185
   Chen XX, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3440756
   Zeiler MD, 2012, Arxiv, DOI [arXiv:1212.5701, DOI 10.48550/ARXIV.1212.5701]
   Deng WX, 2022, IEEE T MULTIMEDIA, V24, P2407, DOI 10.1109/TMM.2021.3080516
   Fang SC, 2021, PROC CVPR IEEE, P7094, DOI 10.1109/CVPR46437.2021.00702
   Ganin Y, 2016, J MACH LEARN RES, V17
   Gupta A, 2016, PROC CVPR IEEE, P2315, DOI 10.1109/CVPR.2016.254
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hsu HK, 2020, IEEE WINT CONF APPL, P738, DOI [10.1109/WACV45572.2020.9093358, 10.1109/wacv45572.2020.9093358]
   Jaderberg M., 2014, ARXIV
   Jing MM, 2023, IEEE T MULTIMEDIA, V25, P2559, DOI 10.1109/TMM.2022.3148592
   Kang L, 2020, IEEE WINT CONF APPL, P3491, DOI 10.1109/WACV45572.2020.9093392
   Karatzas D, 2015, PROC INT CONF DOC, P1156, DOI 10.1109/ICDAR.2015.7333942
   Karatzas D, 2013, PROC INT CONF DOC, P1484, DOI 10.1109/ICDAR.2013.221
   Katti AR, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P4459
   Lee CY, 2016, PROC CVPR IEEE, P2231, DOI 10.1109/CVPR.2016.245
   Li M, 2023, IEEE T MULTIMEDIA, V25, P4094, DOI 10.1109/TMM.2022.3171108
   Li M, 2023, IEEE T MULTIMEDIA, V25, P649, DOI 10.1109/TMM.2021.3129651
   Li YQ, 2023, IEEE T MULTIMEDIA, V25, P3615, DOI 10.1109/TMM.2022.3163517
   Liu W., 2016, PROC BRIT MACH VIS C, P1
   Liu W, 2018, AAAI CONF ARTIF INTE, P7154
   Lucas S. M., 2005, International Journal on Document Analysis and Recognition, V7, P105, DOI 10.1007/s10032-004-0134-3
   Marti U.-V., 2002, International Journal on Document Analysis and Recognition, V5, P39, DOI 10.1007/s100320200071
   Mishra A, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.127
   Risnumawan A, 2014, EXPERT SYST APPL, V41, P8027, DOI 10.1016/j.eswa.2014.07.008
   Shi BG, 2017, IEEE T PATTERN ANAL, V39, P2298, DOI 10.1109/TPAMI.2016.2646371
   Shi BG, 2016, PROC CVPR IEEE, P4168, DOI 10.1109/CVPR.2016.452
   Singh A, 2019, PROC CVPR IEEE, P8309, DOI 10.1109/CVPR.2019.00851
   Phan TQ, 2013, IEEE I CONF COMP VIS, P569, DOI 10.1109/ICCV.2013.76
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   Wang J., 2017, Advances in Neural Information Processing Systems, V30, P334
   Wang JP, 2022, PROCEEDINGS OF THE 60TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), VOL 1: (LONG PAPERS), P7747
   Wang K, 2011, IEEE I CONF COMP VIS, P1457, DOI 10.1109/ICCV.2011.6126402
   Wang TW, 2020, AAAI CONF ARTIF INTE, V34, P12216
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Ye QX, 2005, IMAGE VISION COMPUT, V23, P565, DOI 10.1016/j.imavis.2005.01.004
   Yongqiang Mou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P158, DOI 10.1007/978-3-030-58555-6_10
   Yue X., 2020, ECCV, VVolume 12364, P135, DOI [DOI 10.1007/978-3-030-58529-7_9, 10.1007/978-3-030-58529-7_9]
   Zhan FN, 2019, IEEE I CONF COMP VIS, P9104, DOI 10.1109/ICCV.2019.00920
   Zhang CW, 2021, AAAI CONF ARTIF INTE, V35, P3305
   Zhang Y., 2021, arXiv
   Zhang YB, 2019, PROC CVPR IEEE, P5026, DOI 10.1109/CVPR.2019.00517
   Zhang YP, 2021, IEEE T IMAGE PROCESS, V30, P3922, DOI 10.1109/TIP.2021.3066903
   Zhang YP, 2019, PROC CVPR IEEE, P2735, DOI 10.1109/CVPR.2019.00285
   Zhaoyi Wan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11422, DOI 10.1109/CVPR42600.2020.01144
   Zhong Y, 2000, IEEE T PATTERN ANAL, V22, P385, DOI 10.1109/34.845381
NR 50
TC 1
Z9 1
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 9071
EP 9081
DI 10.1109/TMM.2023.3245404
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP7K5
UT WOS:001133324200003
DA 2024-07-18
ER

PT J
AU Gao, GW
   Tang, L
   Wu, F
   Lu, HM
   Yang, J
AF Gao, Guangwei
   Tang, Lei
   Wu, Fei
   Lu, Huimin
   Yang, Jian
TI JDSR-GAN: Constructing an Efficient Joint Learning Network for Masked
   Face Super-Resolution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Faces; Face recognition; Superresolution; Noise reduction; Task
   analysis; Generators; Noise level; Image denoising; face
   super-resolution; face mask occlusion; generative adversarial network
AB With the growing importance of preventing the COVID-19 virus in cyber-manufacturing security, face images obtained in most video surveillance scenarios are usually low resolution together with mask occlusion. However, most of the previous face super-resolution solutions can not efficiently handle both tasks in one model. In this work, we consider both tasks simultaneously and construct an efficient joint learning network, called JDSR-GAN, for masked face super-resolution tasks. Given a low-quality face image with mask as input, the role of the generator composed of a denoising module and super-resolution module is to acquire a high-quality high-resolution face image. The discriminator utilizes some carefully designed loss functions to ensure the quality of the recovered face images. Moreover, we incorporate the identity information and attention mechanism into our network for feasible correlated feature expression and informative feature learning. By jointly performing denoising and face super-resolution, the two tasks can complement each other and attain promising performance. Extensive qualitative and quantitative results show the superiority of our proposed JDSR-GAN over some competitive methods.
C1 [Gao, Guangwei; Tang, Lei] Nanjing Univ Posts & Telecommun, Inst Adv Technol, Nanjing 210023, Peoples R China.
   [Gao, Guangwei; Tang, Lei] Soochow Univ, Prov Key Lab Comp Informat Proc Technol, Suzhou 215006, Peoples R China.
   [Wu, Fei] Nanjing Univ Posts & Telecommun, Coll Automat, Nanjing 210023, Peoples R China.
   [Lu, Huimin] Kyushu Inst Technol, Dept Mech & Control Engn, Kitakyushu 8048550, Japan.
   [Yang, Jian] Nanjing Univ Sci & Technol, Sch Comp Sci & Technol, Nanjing 210094, Peoples R China.
C3 Nanjing University of Posts & Telecommunications; Soochow University -
   China; Nanjing University of Posts & Telecommunications; Kyushu
   Institute of Technology; Nanjing University of Science & Technology
RP Wu, F (corresponding author), Nanjing Univ Posts & Telecommun, Coll Automat, Nanjing 210023, Peoples R China.
EM csggao@gmail.com; tl_njupt@163.com; wufei_8888@126.com;
   dr.huimin.lu@ieee.org; csjyang@njust.edu.cn
RI Lu, Huimin/H-5571-2011; tang, lei/JCO-4117-2023; li,
   xiaomin/KCX-9845-2024
OI Wu, Fei/0000-0001-5498-4947; Lu, Huimin/0000-0001-9794-3221
FU National Key Research and Development Program of China [2018AAA0100102,
   2018AAA0100100]; National Natural Science Foundation of China [61972212,
   62076139]
FX . This work was supported in part by the National Key Research and
   Development Program of China under Grants 2018AAA0100102 and
   2018AAA0100100, the National Natural Science Foundation of China under
   Grants 61972212 and 62076139. The guest editor coordinating the review
   of this manuscript and approving it for publication was Dr. Cairong
   Zhao. (Guangwei Gao and Lei Tang contributed equally to this work.)
CR [Anonymous], 2017, P IEEE C COMP VIS PA
   Arjovsky M, 2017, Arxiv, DOI [arXiv:1701.07875, DOI 10.48550/ARXIV.1701.07875]
   Brooks T, 2019, PROC CVPR IEEE, P11028, DOI 10.1109/CVPR.2019.01129
   Chaudhuri B, 2019, PROC CVPR IEEE, P9711, DOI 10.1109/CVPR.2019.00995
   Chen CF, 2021, IEEE T IMAGE PROCESS, V30, P1219, DOI 10.1109/TIP.2020.3043093
   Chen Y, 2018, PROC CVPR IEEE, P2492, DOI 10.1109/CVPR.2018.00264
   Gao GW, 2023, IEEE T MULTIMEDIA, V25, P1879, DOI 10.1109/TMM.2022.3192769
   Gao GW, 2022, Arxiv, DOI arXiv:2204.08696
   Gao GW, 2022, AAAI CONF ARTIF INTE, P661
   Gao GW, 2022, IEEE T CIRC SYST VID, V32, P2550, DOI 10.1109/TCSVT.2020.3042178
   Gao GW, 2021, PATTERN RECOGN, V110, DOI 10.1016/j.patcog.2020.107539
   Gao GW, 2017, PATTERN RECOGN, V66, P129, DOI 10.1016/j.patcog.2016.12.021
   Guan WL, 2021, IEEE T CYBERNETICS, V51, P4501, DOI 10.1109/TCYB.2019.2951207
   Guo S, 2019, PROC CVPR IEEE, P1712, DOI 10.1109/CVPR.2019.00181
   Guo Y, 2020, PROC CVPR IEEE, P5406, DOI 10.1109/CVPR42600.2020.00545
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hensel M, 2017, ADV NEUR IN, V30
   Hsu CC, 2019, IEEE T IMAGE PROCESS, V28, P6225, DOI 10.1109/TIP.2019.2924554
   Ioffe S, 2015, Arxiv, DOI [arXiv:1502.03167, DOI 10.48550/ARXIV.1502.03167]
   Gulrajani I, 2017, ADV NEUR IN, V30
   Kaiming He, 2015, 2015 IEEE International Conference on Computer Vision (ICCV). Proceedings, P1026, DOI 10.1109/ICCV.2015.123
   Kazemi V, 2014, PROC CVPR IEEE, P1867, DOI 10.1109/CVPR.2014.241
   Kingma D. P., 2014, arXiv
   Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49
   Li MY, 2021, IEEE T MULTIMEDIA, V23, P468, DOI 10.1109/TMM.2020.2984092
   Li YJ, 2017, PROC CVPR IEEE, P5892, DOI 10.1109/CVPR.2017.624
   Li Z, 2019, PROC CVPR IEEE, P3862, DOI 10.1109/CVPR.2019.00399
   Liao L, 2021, PROC CVPR IEEE, P6535, DOI 10.1109/CVPR46437.2021.00647
   Liu GL, 2018, LECT NOTES COMPUT SC, V11215, P89, DOI 10.1007/978-3-030-01252-6_6
   Liu LC, 2022, IEEE T CYBERNETICS, V52, P265, DOI 10.1109/TCYB.2020.2979320
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Lu T, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5501, DOI 10.1145/3474085.3475682
   Ma C, 2020, PROC CVPR IEEE, P5568, DOI 10.1109/CVPR42600.2020.00561
   Paszke A., 2017, NIPS W
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song YB, 2017, Arxiv, DOI arXiv:1708.00223
   Tian CW, 2020, NEURAL NETWORKS, V121, P461, DOI 10.1016/j.neunet.2019.08.022
   Wan J, 2021, IEEE T IMAGE PROCESS, V30, P121, DOI 10.1109/TIP.2020.3032029
   Wang CY, 2022, IEEE T CIRC SYST VID, V32, P7317, DOI 10.1109/TCSVT.2022.3181828
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yu X, 2018, PROC CVPR IEEE, P908, DOI 10.1109/CVPR.2018.00101
   Yuzhi Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P1, DOI 10.1007/978-3-030-58539-6_1
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang KP, 2018, LECT NOTES COMPUT SC, V11215, P196, DOI 10.1007/978-3-030-01252-6_12
   Zhao CR, 2021, IEEE T IMAGE PROCESS, V30, P4212, DOI 10.1109/TIP.2021.3070182
   Zhu FY, 2016, PROC CVPR IEEE, P420, DOI 10.1109/CVPR.2016.52
   Zhu HY, 2020, PATTERN RECOGN, V107, DOI 10.1016/j.patcog.2020.107354
NR 48
TC 7
Z9 7
U1 10
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1505
EP 1512
DI 10.1109/TMM.2023.3240880
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA G2GW2
UT WOS:000987415000009
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Hou, XX
   Zhang, XK
   Liang, HB
   Shen, LL
   Ming, Z
AF Hou, Xianxu
   Zhang, Xiaokang
   Liang, Hanbang
   Shen, Linlin
   Ming, Zhong
TI Lifelong Age Transformation With a Deep Generative Prior
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE GANs; age transformation
ID FACE; PERCEPTION; SHAPE
AB In this paper, we consider the lifelong age progression and regression task, which requires to synthesize a person's appearance across a wide range of ages. We propose a simple yet effective learning framework to achieve this by exploiting the prior knowledge of faces captured by well-trained generative adversarial networks (GANs). Specifically, we first utilize a pretrained GAN to synthesize face images with different ages, with which we then learn to model the conditional aging process in the GAN latent space. Moreover, we also introduce a cycle consistency loss in the GAN latent space to preserve a person's identity. As a result, our model can reliably predict a person's appearance for different ages by modifying both shape and texture of the head. Both qualitative and quantitative experimental results demonstrate the superiority of our method over concurrent works. Furthermore, we demonstrate that our approach can also achieve high-quality age transformation for painting portraits and cartoon characters without additional age annotations.
C1 [Hou, Xianxu; Zhang, Xiaokang; Liang, Hanbang; Shen, Linlin] Shenzhen Univ, Comp Vis Inst, Coll Comp Sci & Software Engn, Shenzhen 518060, Peoples R China.
   [Hou, Xianxu; Zhang, Xiaokang; Liang, Hanbang; Shen, Linlin] Shenzhen Inst Artificial Intelligence & Robot Soc, Shenzhen 518060, Peoples R China.
   [Hou, Xianxu; Zhang, Xiaokang; Liang, Hanbang; Shen, Linlin] Shenzhen Univ, Guangdong Key Lab Intelligent Informat Proc, Shenzhen 518060, Peoples R China.
   [Ming, Zhong] Shenzhen Univ, Coll Comp Sci & Software Engn, Natl Engn Lab Big Data Syst Comp Technol, Shenzhen 518060, Peoples R China.
C3 Shenzhen University; Shenzhen Institute of Artificial Intelligence &
   Robotics for Society; Shenzhen University; Shenzhen University
RP Shen, LL (corresponding author), Shenzhen Univ, Comp Vis Inst, Coll Comp Sci & Software Engn, Shenzhen 518060, Peoples R China.; Shen, LL (corresponding author), Shenzhen Inst Artificial Intelligence & Robot Soc, Shenzhen 518060, Peoples R China.; Shen, LL (corresponding author), Shenzhen Univ, Guangdong Key Lab Intelligent Informat Proc, Shenzhen 518060, Peoples R China.
EM hxianxu@gmail.com; zhangxiaokang2019@email.szu.edu.cn;
   lianghanbang2019@email.szu.edu.cn; llshen@szu.edu.cn; mingz@szu.edu.cn
FU National Natural Science Foundation of China [91959108, 61836005,
   61672358]; Guangdong Basic and Applied Basic Research Foundation
   [2020A1515111199]
FX The work was supported in part by the National Natural Science
   Foundation of China under Grants 91959108, 61836005, and 61672358, and
   in part by Guangdong Basic and Applied Basic Research Foundation under
   Grant 2020A1515111199. The associate editor coordinating the review of
   this manuscript and approving it for publication was Prof. Heng Tao
   Shen.
CR Abdal R, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3447648
   Abdal R, 2019, IEEE I CONF COMP VIS, P4431, DOI 10.1109/ICCV.2019.00453
   Agustsson E, 2017, IEEE INT CONF AUTOMA, P87, DOI 10.1109/FG.2017.20
   Alaluf Y, 2021, Arxiv, DOI [arXiv:2102.02754, DOI 10.48550/ARXIV.2102.02754]
   [Anonymous], 2017, P IEEE C COMP VIS PA
   Antipov G, 2017, IEEE IMAGE PROC, P2089, DOI 10.1109/ICIP.2017.8296650
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Brock A., 2018, PROC INT C LEARN REP
   BURT DM, 1995, P ROY SOC B-BIOL SCI, V259, P137, DOI 10.1098/rspb.1995.0021
   Duong CN, 2017, IEEE I CONF COMP VIS, P3755, DOI 10.1109/ICCV.2017.403
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Denton E, 2015, ADV NEUR IN, V28
   Duong CN, 2016, PROC CVPR IEEE, P5772, DOI 10.1109/CVPR.2016.622
   Fu Y, 2010, IEEE T PATTERN ANAL, V32, P1955, DOI 10.1109/TPAMI.2010.36
   Goetschalckx L, 2019, IEEE I CONF COMP VIS, P5743, DOI 10.1109/ICCV.2019.00584
   Goodfellow I., 2014, P 27 INT C NEURAL IN, P2672
   Harkonen E., 2020, Advances in Neural Information Processing Systems, V33, P9841
   He ZL, 2019, IEEE I CONF COMP VIS, P9439, DOI 10.1109/ICCV.2019.00953
   He ZL, 2019, IEEE T IMAGE PROCESS, V28, P5464, DOI 10.1109/TIP.2019.2916751
   Hensel M, 2017, ADV NEUR IN, V30
   Hou XX, 2022, NEURAL NETWORKS, V145, P209, DOI 10.1016/j.neunet.2021.10.017
   Hou XX, 2017, IEEE WINT CONF APPL, P1133, DOI 10.1109/WACV.2017.131
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Gulrajani I, 2017, ADV NEUR IN, V30
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiapeng Zhu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P592, DOI 10.1007/978-3-030-58520-4_35
   Karnewar Animesh, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7796, DOI 10.1109/CVPR42600.2020.00782
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T, 2018, P INT C LEARN REPR I
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Kemelmacher-Shlizerman I, 2014, PROC CVPR IEEE, P3334, DOI 10.1109/CVPR.2014.426
   Kim H, 2019, IEEE I CONF COMP VIS, P9055, DOI 10.1109/ICCV.2019.00915
   Lanitis A, 2002, IEEE T PATTERN ANAL, V24, P442, DOI 10.1109/34.993553
   Larsen ABL, 2016, PR MACH LEARN RES, V48
   Lehtinen J., 2020, Advances in Neural Information Processing Systems (NeurIPS), V33, P12104
   Li Q, 2020, AAAI CONF ARTIF INTE, V34, P11378
   Liu M, 2019, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2019.00379
   Liu MY, 2019, IEEE I CONF COMP VIS, P10550, DOI 10.1109/ICCV.2019.01065
   Liu YF, 2021, IEEE T INF FOREN SEC, V16, P2776, DOI 10.1109/TIFS.2021.3065499
   Liu YF, 2019, PROC CVPR IEEE, P11869, DOI 10.1109/CVPR.2019.01215
   Lucic M, 2019, PR MACH LEARN RES, V97
   M. Inc, FAC RES TOOLK
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Miyato T., 2018, 6 INT C LEARNING REP
   Mo S., 2020, PROC CVPR CONTENT CR
   Ngo M., 2021, IEEE Trans. Multimedia, V24, P377
   Or-El Roy, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P739, DOI 10.1007/978-3-030-58539-6_44
   Radford A., 2015, ARXIV
   Ramanathan N, 2008, IEEE INT CONF AUTOMA, P1006
   ROWLAND DA, 1995, IEEE COMPUT GRAPH, V15, P70, DOI 10.1109/38.403830
   Salimans T, 2016, ADV NEUR IN, V29
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shen YJ, 2021, PROC CVPR IEEE, P1532, DOI 10.1109/CVPR46437.2021.00158
   Shen Yujun, 2020, P IEEE CVF C COMP VI, P9243, DOI DOI 10.1109/CVPR42600.2020.00926
   Shu XB, 2015, IEEE I CONF COMP VIS, P3970, DOI 10.1109/ICCV.2015.452
   Song JK, 2022, IEEE T MULTIMEDIA, V24, P791, DOI 10.1109/TMM.2021.3059336
   Suo JL, 2012, IEEE T PATTERN ANAL, V34, P2083, DOI 10.1109/TPAMI.2012.22
   Suo JL, 2010, IEEE T PATTERN ANAL, V32, P385, DOI 10.1109/TPAMI.2009.39
   Tewari A, 2020, PROC CVPR IEEE, P6141, DOI 10.1109/CVPR42600.2020.00618
   Tiddeman B, 2001, IEEE COMPUT GRAPH, V21, P42, DOI 10.1109/38.946630
   Tov O, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459838
   Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002
   Wang ZW, 2018, PROC CVPR IEEE, P7939, DOI 10.1109/CVPR.2018.00828
   Wu ZZ, 2021, PROC CVPR IEEE, P12858, DOI 10.1109/CVPR46437.2021.01267
   Xia W., 2021, arXiv
   Yang HY, 2018, PROC CVPR IEEE, P31, DOI 10.1109/CVPR.2018.00011
   Yao X, 2021, INT C PATT RECOG, P8624, DOI 10.1109/ICPR48806.2021.9412383
   Yeh RA, 2017, PROC CVPR IEEE, P6882, DOI 10.1109/CVPR.2017.728
   Yoo S, 2019, PROC CVPR IEEE, P11275, DOI 10.1109/CVPR.2019.01154
   Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457
   Yunjey Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8185, DOI 10.1109/CVPR42600.2020.00821
   Zhang H, 2019, 36 INT C MACHINE LEA, V97
   Zhang ZF, 2017, PROC CVPR IEEE, P4352, DOI 10.1109/CVPR.2017.463
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 76
TC 1
Z9 1
U1 7
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3125
EP 3139
DI 10.1109/TMM.2022.3155903
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA O7RP6
UT WOS:001045742200014
DA 2024-07-18
ER

PT J
AU Hu, NN
   Ming, Y
   Fan, CX
   Feng, F
   Lyu, B
AF Hu, Nannan
   Ming, Yue
   Fan, Chunxiao
   Feng, Fan
   Lyu, Boyang
TI TSFNet: Triple-Steam Image Captioning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Attention mechanism; image captioning; triplesteam feature fusion;
   visual representation
AB Image captioning is a challenging task that generates a natural language description based on the visual understanding of the given image. Significant region representation is a milestone in image captioning. Despite the great success of existing region-based works, they only focus on the salient objects and encode these objects independently, still plagued by the lack of global contextual information and visual relationships. In fact, the global contextual information and structured visual relationships are exactly the merits of traditional grid features and emerging scene graph features. In this paper, we present a Triple-Steam Feature Fusion Network (TSFNet) to leverage the complementary advantages of the grid, region, and scene graph triple-steam visual representations in image captioning. Concretely, in our TSFNet, a novel Dual-level Attention (DA) mechanism is proposed to simultaneously explore visual intrinsic properties and word-related attributes uniformly of different features. Then attention enhanced features of different modalities are mapped into a joint representation to guide the caption generation. Moreover, we design a new global-aware decoder, which leverages the concatenated representation of triple-steam features and the joint attention representation to obtain global visual guidance information, further refine the complex multimodal reasoning. To verify the effectiveness of our feature fusion model, we perform extensive experiments on the highly competitive MSCOCO dataset to evaluate the model quantitatively and qualitatively. The results illustrate that the proposed framework outperforms many state-of-the-art image captioning approaches in various evaluation metrics, and generates more accurate and abundant captions.
C1 [Hu, Nannan; Ming, Yue; Fan, Chunxiao; Feng, Fan; Lyu, Boyang] Beijing Univ Posts & Telecommun, Sch Elect Engn, Beijing Key Lab Work Safety Intelligent Monitoring, Beijing 100876, Peoples R China.
C3 Beijing University of Posts & Telecommunications
RP Ming, Y (corresponding author), Beijing Univ Posts & Telecommun, Sch Elect Engn, Beijing Key Lab Work Safety Intelligent Monitoring, Beijing 100876, Peoples R China.
EM hunan246@bupt.edu.cn; myname35875235@126.com; fcxg100@163.com;
   fan.feng@bupt.edu.cn; lvboyang@bupt.edu.cn
FU Natural Science Foundation of China [62076030]; Beijing Natural Science
   Foundation of China [L201023]
FX The work was supported in part by the Natural Science Foundation of
   China under Grant 62076030 and the Beijing Natural Science Foundation of
   China under Grant L201023. The Associate Editor coordinating the review
   of this manuscript and approving it for publication was Prof.Lamberto
   Ballan.(Corresponding author: Yue Ming.
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24
   Banerjee S., 2005, P ACL WORKSH INTR EX, P65
   Ben HX, 2022, IEEE T MULTIMEDIA, V24, P904, DOI 10.1109/TMM.2021.3060948
   Che WB, 2020, IEEE T MULTIMEDIA, V22, P2307, DOI 10.1109/TMM.2019.2954750
   Cheng XJ, 2018, PATTERN RECOGN, V74, P474, DOI 10.1016/j.patcog.2017.09.025
   Cornia M, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3177745
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Guo LT, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P765, DOI 10.1145/3343031.3350943
   Huang L, 2019, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2019.00473
   Jiang WT, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3460474
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Nguyen K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1387, DOI 10.1109/ICCV48922.2021.00144
   Kim DJ, 2022, IEEE T PATTERN ANAL, V44, P7348, DOI 10.1109/TPAMI.2021.3119754
   Kim JH, 2016, ADV NEUR IN, V29
   Kingma D. P., 2014, arXiv
   Kusner MJ, 2015, PR MACH LEARN RES, V37, P957
   Lebret R, 2015, PR MACH LEARN RES, V37, P2085
   Li XY, 2019, IEEE T MULTIMEDIA, V21, P2117, DOI 10.1109/TMM.2019.2896516
   Lin Chin-Yew, 2004, P 42 ANN M AOC COMP, P605, DOI [DOI 10.3115/.1218955.1219032, DOI 10.3115/1218955.1219032]
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu AA, 2022, IEEE T CIRC SYST VID, V32, P3685, DOI 10.1109/TCSVT.2021.3107035
   Liu L, 2020, INT J COMPUT VISION, V128, P261, DOI 10.1007/s11263-019-01247-4
   Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345
   Luo Y., 2021, PROC AAAI C ARTIF IN, P1
   Manay Smriti P., 2022, Emerging Research in Computing, Information, Communication and Applications: ERCICA 2020. Lecture Notes in Electrical Engineering (789), P511, DOI 10.1007/978-981-16-1338-8_43
   Meng ZH, 2021, PROC CVPR IEEE, P12674, DOI 10.1109/CVPR46437.2021.01249
   Ordonez V., 2011, ADV NEURAL INFORM PR, P1143
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Shi Z., 2020, Association for Computational Linguistics, P7454
   Socher R., 2014, Trans Assoc Comput Linguist, V2, P207, DOI [DOI 10.1162/TACLA00177, 10.1162/tacl_a_00177, DOI 10.1162/TACL_A_00177]
   Song YQ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P784, DOI 10.1145/3343031.3350996
   Tran A., 2020, IEEECVF C COMPUT VIS, P13035
   Vaswani A, 2017, ADV NEUR IN, V30
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Vinyals O, 2017, IEEE T PATTERN ANAL, V39, P652, DOI 10.1109/TPAMI.2016.2587640
   Wang WX, 2019, AAAI CONF ARTIF INTE, P8957
   Wei HY, 2020, COMPUT VIS IMAGE UND, V201, DOI 10.1016/j.cviu.2020.103068
   Wu J, 2021, IEEE T MULTIMEDIA, V23, P2413, DOI 10.1109/TMM.2020.3011317
   Wu LX, 2021, IEEE T CIRC SYST VID, V31, P3118, DOI 10.1109/TCSVT.2020.3036860
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Xu R, 2015, AAAI CONF ARTIF INTE, P2346
   Yang LC, 2021, AAAI CONF ARTIF INTE, V35, P3136
   Yang X, 2022, IEEE T PATTERN ANAL, V44, P2313, DOI 10.1109/TPAMI.2020.3042192
   Yang X, 2019, IEEE I CONF COMP VIS, P4249, DOI 10.1109/ICCV.2019.00435
   Yang X, 2019, PROC CVPR IEEE, P10677, DOI 10.1109/CVPR.2019.01094
   Yang XW, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P5398, DOI 10.1145/3503161.3548409
   Yao T, 2019, IEEE I CONF COMP VIS, P2621, DOI 10.1109/ICCV.2019.00271
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yao T, 2017, IEEE I CONF COMP VIS, P4904, DOI 10.1109/ICCV.2017.524
   Yingwei Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10968, DOI 10.1109/CVPR42600.2020.01098
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Yu J, 2020, IEEE T CIRC SYST VID, V30, P4467, DOI 10.1109/TCSVT.2019.2947482
   Yu Z, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3743, DOI 10.1145/3394171.3413977
   Yu Z, 2019, PROC CVPR IEEE, P6274, DOI 10.1109/CVPR.2019.00644
   Zellers R, 2018, PROC CVPR IEEE, P5831, DOI 10.1109/CVPR.2018.00611
   Zhang XY, 2021, PROC CVPR IEEE, P15460, DOI 10.1109/CVPR46437.2021.01521
   Zhang ZJ, 2022, IEEE T MULTIMEDIA, V24, P3101, DOI 10.1109/TMM.2021.3093725
NR 59
TC 2
Z9 2
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6904
EP 6916
DI 10.1109/TMM.2022.3215861
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000015
DA 2024-07-18
ER

PT J
AU Jiang, N
   Wang, KR
   Peng, XK
   Yu, XH
   Wang, Q
   Xing, JL
   Li, GR
   Guo, GD
   Ye, QX
   Jiao, JB
   Zhao, J
   Han, ZJ
AF Jiang, Nan
   Wang, Kuiran
   Peng, Xiaoke
   Yu, Xuehui
   Wang, Qiang
   Xing, Junliang
   Li, Guorong
   Guo, Guodong
   Ye, Qixiang
   Jiao, Jianbin
   Zhao, Jian
   Han, Zhenjun
TI Anti-UAV: A Large-Scale Benchmark for Vision-Based UAV Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Target tracking; Radar tracking; Training; Benchmark testing; Unmanned
   aerial vehicles; Semantics; Object tracking; Unmanned aerial vehicle;
   object tracking; deep tracking; multiple modal types
ID OBJECT TRACKING; SURVEILLANCE; NETWORK; PEOPLE; FUSION
AB Unmanned Aerial Vehicles (UAV) have many applications in both commerce and recreation. However, irresponsibly operated UAVs will pose a threat to public safety. Therefore, developing our understanding of UAVs and their uses is of particular interest. This paper considers tracking UAVs, which provide multifaceted information around location, paths and trajectories. To facilitate research on this topic, we introduce a new benchmark, herein referred to as Anti-UAV, which provides a novel direction for UAV tracking with more than 300 video pairs containing over 580 k manually annotated bounding boxes. Addressing anti-UAV research challenges could help to design anti-UAV systems, which in turn may improve surveillance. Accordingly, we have proposed a simple yet effective approach, called dual-flow semantic consistency (DFSC) is proposed for UAV tracking. Modulated by the semantic flow across video sequences, tracker learns more robust class-level semantic information and obtains more discriminative instance-level features. Experiments highlight significant performance gain with the proposed approach over state-of-the-art trackers and the challenging aspects of Anti-UAV.
C1 [Jiang, Nan; Wang, Kuiran; Peng, Xiaoke; Yu, Xuehui; Ye, Qixiang; Jiao, Jianbin; Han, Zhenjun] Univ Chinese Acad Sci, Sch Elect Elect & Commun Engn, Beijing 101408, Peoples R China.
   [Li, Guorong] Univ Chinese Acad Sci, Sch Comp Sci & Technol, Beijing 101408, Peoples R China.
   [Wang, Qiang; Xing, Junliang] Chinese Acad Sci CASIA, Inst Automat, Beijing 100190, Peoples R China.
   [Guo, Guodong] Inst Deep Learning Baidu Res & Nat Engn, Lab Deep Learning Technol & Applicat, Beijing 100095, Peoples R China.
   [Zhao, Jian] Acad Mil Sci, Inst North Elect Equipment, Beijing 100191, Peoples R China.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Chinese Academy of Sciences; University of Chinese Academy of
   Sciences, CAS; Chinese Academy of Sciences; Institute of Automation, CAS
RP Han, ZJ (corresponding author), Univ Chinese Acad Sci, Sch Elect Elect & Commun Engn, Beijing 101408, Peoples R China.; Zhao, J (corresponding author), Acad Mil Sci, Inst North Elect Equipment, Beijing 100191, Peoples R China.
EM jiangnan18@mails.ucas.ac.cn; wangkuiran19@mails.ucas.ac.cn;
   pengxiaoke19@mails.ucas.ac.cn; yuxuehui17@mails.ucas.ac.cn;
   qiang.wang@nlpr.ia.ac.cn; jlxingg@nlpr.ia.ac.cn; liguorong@ucas.ac.cn;
   guoguodong01@baidu.com; qxye@ucas.ac.cn; jiaojb@ucas.ac.cn;
   zhaojian90@u.nus.edu; hanzhj@ucas.ac.cn
RI JIANG, NAN/AHB-1945-2022; Li, Guorong/AAG-1594-2020
OI Jiang, Nan/0000-0002-8838-7427; ye, qi xiang/0000-0003-1215-6259
FU National Natural Science Foundation of China (NSFC) [61836012, 61771447,
   62006244, 61772494]; Young Elite Scientist Sponsorship Program of China
   Association for Science and Technology (CAST) [YESS20200140]; Strategic
   Priority Research Program of Chinese Academy of Sciences [XDA27000000]
FX This work was supported in part by the National Natural Science
   Foundation of China (NSFC) under Grants 61836012, 61771447, 62006244 and
   61772494, in part by the Young Elite Scientist Sponsorship Program of
   China Association for Science and Technology (CAST) under Grant
   YESS20200140, and in part by the Strategic Priority Research Program of
   Chinese Academy of Sciences under Grant XDA27000000.
CR [Anonymous], 2015, PROC 12 IEEE INT C A
   [Anonymous], 2017, P IEEE INT C COMP VI
   Berg A, 2016, IEEE COMPUT SOC CONF, P1248, DOI 10.1109/CVPRW.2016.158
   Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bhat Goutam, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P205, DOI 10.1007/978-3-030-58592-1_13
   Bhat G, 2019, IEEE I CONF COMP VIS, P6181, DOI 10.1109/ICCV.2019.00628
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Brunetti A, 2018, NEUROCOMPUTING, V300, P17, DOI 10.1016/j.neucom.2018.01.092
   Chang MF, 2019, PROC CVPR IEEE, P8740, DOI 10.1109/CVPR.2019.00895
   Chen ZD, 2020, PROC CVPR IEEE, P6667, DOI 10.1109/CVPR42600.2020.00670
   Dai KN, 2020, PROC CVPR IEEE, P6297, DOI 10.1109/CVPR42600.2020.00633
   Danelljan M., 2014, Accurate Scale Estimation for Robust Visual Tracking
   Danelljan M, 2020, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR42600.2020.00721
   Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Danelljan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P621, DOI 10.1109/ICCVW.2015.84
   Danelljan M, 2014, PROC CVPR IEEE, P1090, DOI 10.1109/CVPR.2014.143
   Davis JW, 2007, COMPUT VIS IMAGE UND, V106, P162, DOI 10.1016/j.cviu.2006.06.010
   Davis JW, 2005, WACV 2005: SEVENTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P364
   Dong XP, 2017, IEEE T MULTIMEDIA, V19, P763, DOI 10.1109/TMM.2016.2631884
   Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552
   Felsberg M, 2016, LECT NOTES COMPUT SC, V9914, P824, DOI 10.1007/978-3-319-48881-3_55
   Felsberg M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P639, DOI 10.1109/ICCVW.2015.86
   Fu CH, 2022, IEEE GEOSC REM SEN M, V10, P125, DOI 10.1109/MGRS.2021.3072992
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1134, DOI 10.1109/ICCV.2017.128
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1144, DOI [10.1109/ICCV.2017.128, 10.1109/ICCV.2017.129]
   Gong YQ, 2021, IEEE WINT CONF APPL, P1159, DOI 10.1109/WACV48630.2021.00120
   Guo DY, 2020, PROC CVPR IEEE, P6268, DOI 10.1109/CVPR42600.2020.00630
   Han ZJ, 2020, IEEE T CIRC SYST VID, V30, P155, DOI 10.1109/TCSVT.2018.2888492
   Haritaoglu I, 2000, IEEE T PATTERN ANAL, V22, P809, DOI 10.1109/34.868683
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Huang LH, 2020, AAAI CONF ARTIF INTE, V34, P11037
   Huang LH, 2021, IEEE T PATTERN ANAL, V43, P1562, DOI 10.1109/TPAMI.2019.2957464
   Jung I., 2018, P ECCV, P83
   Ke L., 2018, PROC EUR C COMPUT VI, P713
   Kristan M., 2014, 13 EUR C COMP VIS EC, P191
   Kristanl M, 2019, IEEE INT CONF COMP V, P2206, DOI 10.1109/ICCVW.2019.00276
   Li AN, 2016, IEEE T PATTERN ANAL, V38, P335, DOI 10.1109/TPAMI.2015.2417577
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li CL, 2019, PATTERN RECOGN, V96, DOI 10.1016/j.patcog.2019.106977
   Li CL, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1856, DOI 10.1145/3123266.3123289
   Li CL, 2016, IEEE T IMAGE PROCESS, V25, P5743, DOI 10.1109/TIP.2016.2614135
   Li F, 2018, PROC CVPR IEEE, P4904, DOI 10.1109/CVPR.2018.00515
   Li JS, 2019, IEEE WINT CONF APPL, P932, DOI 10.1109/WACV.2019.00104
   Li X, 2013, ACM T INTEL SYST TEC, V4, DOI 10.1145/2508037.2508039
   Li Y, 2019, AAAI CONF ARTIF INTE, P8666
   Li Z, 2022, IEEE T CIRC SYST VID, V32, P8128, DOI 10.1109/TCSVT.2021.3102944
   Liang NX, 2018, IEEE T MULTIMEDIA, V20, P2289, DOI 10.1109/TMM.2018.2803518
   Liang PP, 2015, IEEE T IMAGE PROCESS, V24, P5630, DOI 10.1109/TIP.2015.2482905
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu Q, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3847, DOI 10.1145/3394171.3413922
   Liu Q, 2020, IEEE T MULTIMEDIA, V22, P666, DOI 10.1109/TMM.2019.2932615
   Lu XK, 2019, NEUROCOMPUTING, V349, P133, DOI 10.1016/j.neucom.2019.02.021
   Lukezic A, 2017, PROC CVPR IEEE, P4847, DOI 10.1109/CVPR.2017.515
   Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352
   Müller M, 2018, LECT NOTES COMPUT SC, V11205, P310, DOI 10.1007/978-3-030-01246-5_19
   Mueller M, 2017, PROC CVPR IEEE, P1387, DOI 10.1109/CVPR.2017.152
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465
   Portmann J, 2014, IEEE INT CONF ROBOT, P1794, DOI 10.1109/ICRA.2014.6907094
   Possegger H, 2015, PROC CVPR IEEE, P2113, DOI 10.1109/CVPR.2015.7298823
   Sarandi I, 2018, Arxiv, DOI arXiv:1809.04987
   Smeulders AWM, 2014, IEEE T PATTERN ANAL, V36, P1442, DOI 10.1109/TPAMI.2013.230
   Tang M, 2018, PROC CVPR IEEE, P4874, DOI 10.1109/CVPR.2018.00512
   Tian SJ, 2021, IEEE T MULTIMEDIA, V23, P120, DOI 10.1109/TMM.2020.2978636
   Torabi A, 2012, COMPUT VIS IMAGE UND, V116, P210, DOI 10.1016/j.cviu.2011.10.006
   Valmadre J, 2018, LECT NOTES COMPUT SC, V11207, P692, DOI 10.1007/978-3-030-01219-9_41
   Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531
   Venkataraman V, 2012, IEEE T IMAGE PROCESS, V21, P4622, DOI 10.1109/TIP.2012.2210233
   Voigtlaender P, 2020, PROC CVPR IEEE, P6577, DOI 10.1109/CVPR42600.2020.00661
   Wang GT, 2019, PROC CVPR IEEE, P3638, DOI 10.1109/CVPR.2019.00376
   Wang LT, 2020, IEEE T COGN DEV SYST, V12, P98, DOI 10.1109/TCDS.2019.2900506
   Wang N, 2013, P ADV NEURAL INFORM
   Wang N, 2018, PROC CVPR IEEE, P4844, DOI 10.1109/CVPR.2018.00509
   Wang Q., 2021, arXiv
   Wang Q, 2019, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2019.00142
   Wang QR, 2019, IEEE T MULTIMEDIA, V21, P930, DOI 10.1109/TMM.2018.2869277
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Wu Z, 2014, IEEE COMPUT SOC CONF, P201, DOI 10.1109/CVPRW.2014.39
   Yan B, 2019, IEEE I CONF COMP VIS, P2385, DOI 10.1109/ICCV.2019.00247
   Yu XH, 2020, IEEE WINT CONF APPL, P1246, DOI 10.1109/WACV45572.2020.9093394
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Zha YF, 2020, IEEE T MULTIMEDIA, V22, P96, DOI 10.1109/TMM.2019.2922125
   Zhang Hongyi, 2018, MIXUP EMPIRICAL RISK, DOI DOI 10.48550/ARXIV.1710.09412
   Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472
   Zhao J., 2018, THESIS
   Zhao J., 2017, P BRIT MACH VIS C BM
   Zhao J, 2021, Arxiv, DOI arXiv:2108.09909
   Zhao J, 2020, INT J COMPUT VISION, V128, P2185, DOI 10.1007/s11263-019-01181-5
   Zhao J, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P792, DOI 10.1145/3240508.3240509
   Zhao J, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1184
   Zhao J, 2017, IEEE COMPUT SOC CONF, P1595, DOI 10.1109/CVPRW.2017.204
   Zhipeng Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P771, DOI 10.1007/978-3-030-58589-1_46
   Zhu Pengfei, 2020, ARXIV
   Zhu Z., 2018, 2018 IEEE International Magnetics Conference (INTERMAG), DOI 10.1109/INTMAG.2018.8508710
NR 100
TC 13
Z9 15
U1 18
U2 44
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 486
EP 500
DI 10.1109/TMM.2021.3128047
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 9B8PL
UT WOS:000934993800012
DA 2024-07-18
ER

PT J
AU Li, ZZ
   Li, G
   Li, TH
   Liu, S
   Gao, W
AF Li, Zhuangzi
   Li, Ge
   Li, Thomas H. H.
   Liu, Shan
   Gao, Wei
TI Semantic Point Cloud Upsampling
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Point cloud compression; Semantics; Feature extraction; Convolution;
   Task analysis; Training; Three-dimensional displays; Deep learning;
   graph aggregation; point cloud upsampling
ID IMAGE SUPERRESOLUTION
AB Downsampled sparse point clouds are beneficial for data transmission and storage, but they are detrimental for semantic tasks due to information loss. In this paper, we examine an upsampling methodology that significantly reconstructs sparse clouds' semantic representations. Specifically, we propose a novel semantic point cloud upsampling (SPU) framework for sparse point cloud classification. An SPU consists of two networks, i.e. an upsampling network and a classification network. They are skillfully unified to intensify semantic representations acting on the upsampling process. In the upsampling network, we first propose a novel graph aggregation convolution to construct hierarchical relations on sparse point clouds. To enhance stability and diversity during point upsampling, we then combine point shuffling and pre-interpolation technologies to build an enhanced upsampling module. Furthermore, we adopt the semantic prior information provided by a sparse point cloud to enhance its upsampling quality. The prior information is applied to an attention mechanism that can highlight key positions of the point cloud. We investigate different loss functions and conduct experiments on classical deep point networks, which effectively demonstrate the promising performance of our framework.
C1 [Li, Zhuangzi; Li, Ge; Gao, Wei] Peking Univ, Sch Elect & Comp Engn, Shenzhen 518055, Peoples R China.
   [Li, Zhuangzi; Gao, Wei] Peng Cheng Natl Lab, Artificial Intelligence Res Ctr, Shenzhen 518055, Peoples R China.
   [Liu, Shan] Tencent, Media Lab, Palo Alto, CA 94301 USA.
   [Li, Thomas H. H.] Peking Univ, AIIT, Hangzhou 311215, Peoples R China.
   [Li, Thomas H. H.] Peking Univ, ITRDIT, Shaoxing 312399, Peoples R China.
C3 Peking University; Peking University; Peking University
RP Gao, W (corresponding author), Peking Univ, Sch Elect & Comp Engn, Shenzhen 518055, Peoples R China.; Gao, W (corresponding author), Peng Cheng Natl Lab, Artificial Intelligence Res Ctr, Shenzhen 518055, Peoples R China.
EM lizhuangzii@163.com; geli@ece.pku.edu.cn; tli@aiit.org.cn;
   shanl@tencent.com; gaowei262@pku.edu.cn
RI huang, shan/JVN-1240-2024; tian, ye/KGL-6485-2024
OI , Shan/0000-0002-1442-1207
CR Alexa M, 2003, IEEE T VIS COMPUT GR, V9, P3, DOI 10.1109/TVCG.2003.1175093
   Blau Y, 2018, PROC CVPR IEEE, P6228, DOI 10.1109/CVPR.2018.00652
   Chang JL, 2020, IEEE T PATTERN ANAL, V42, P2874, DOI 10.1109/TPAMI.2019.2915591
   Cheng SL, 2021, IEEE T IMAGE PROCESS, V30, P4436, DOI 10.1109/TIP.2021.3072214
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Goodfellow I, 2014, ADV NEURAL INFORM PR, P2672
   Guo Y, 2020, PROC CVPR IEEE, P5406, DOI 10.1109/CVPR42600.2020.00545
   Guo YL, 2021, IEEE T PATTERN ANAL, V43, P4338, DOI 10.1109/TPAMI.2020.3005434
   Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179
   Huang H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2421636.2421645
   Ikushima N, 2021, IEEE C EVOL COMPUTAT, P2523, DOI 10.1109/CEC45853.2021.9504713
   Javaheri A, 2021, IEEE T MULTIMEDIA, V23, P4049, DOI 10.1109/TMM.2020.3037481
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kingma D. P., 2014, arXiv
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Li GH, 2019, IEEE I CONF COMP VIS, P9266, DOI 10.1109/ICCV.2019.00936
   Li RH, 2019, IEEE I CONF COMP VIS, P7202, DOI 10.1109/ICCV.2019.00730
   Li Z, 2019, PROC CVPR IEEE, P3862, DOI 10.1109/CVPR.2019.00399
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Lipman Y, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1276377.1276405
   Loshchilov I., 2017, P INT C LEARN REPR
   Mandikal P, 2019, IEEE WINT CONF APPL, P1052, DOI 10.1109/WACV.2019.00117
   Mohri M., 2012, Foundations of Machine Learning
   Qi Charles R, POINTNET DEEP LEARNI
   Qian GC, 2021, PROC CVPR IEEE, P11678, DOI 10.1109/CVPR46437.2021.01151
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Uy MA, 2019, IEEE I CONF COMP VIS, P1588, DOI 10.1109/ICCV.2019.00167
   Wang GY, 2021, J SYST SCI COMPLEX, V34, P68, DOI 10.1007/s11424-020-9266-x
   Wang YF, 2019, PROC CVPR IEEE, P5951, DOI 10.1109/CVPR.2019.00611
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wu H., 2020, PROC BRIT MACH VIS C
   Wu SH, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818073
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Chang AX, 2015, Arxiv, DOI arXiv:1512.03012
   Yang Y, 2021, PATTERN RECOGN, V112, DOI 10.1016/j.patcog.2020.107798
   Ye SQ, 2022, IEEE T VIS COMPUT GR, V28, P3206, DOI 10.1109/TVCG.2021.3058311
   Yu LQ, 2018, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2018.00295
   Yue Qian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P752, DOI 10.1007/978-3-030-58529-7_44
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhu XB, 2020, INFORM SCIENCES, V515, P233, DOI 10.1016/j.ins.2019.12.013
   Zhu XB, 2018, COMPUT GRAPH FORUM, V37, P289, DOI 10.1111/cgf.13568
NR 42
TC 3
Z9 4
U1 11
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3432
EP 3442
DI 10.1109/TMM.2022.3160604
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA O7RP6
UT WOS:001045742200038
DA 2024-07-18
ER

PT J
AU Liu, TP
   Li, J
   Wu, J
   Chang, J
   Song, BH
   Yao, BW
AF Liu, Tianpeng
   Li, Jing
   Wu, Jia
   Chang, Jun
   Song, Beihang
   Yao, Bowen
TI Tracking With Mutual Attention Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE BTE module; mutual attention; self-attention; siamese network; visual
   object tracking
AB Visual tracking is a visual task that tracks a specific target by only giving its first frame location and size. To punish the low-quality but high-scoring tracking results, researchers resorted to foreground reinforcement learning to suppress the scores of positive samples near edges. However, for training with negative samples, all backgrounds are equally labeled as false. In this way, the interdependence and difference between the foreground and the background are not considered. We interpret the underlying reason for drifts as the imbalance between the embedding of background and foreground information. Specifically, some catastrophic tracking results and common tracking errors should not be treated equally but should strengthen the implicit connection between the foreground and background. In this paper, we propose a Mutual Attention (MA) module to strengthen the interdependence between positive and negative samples. It can aggregate the rich contextual interdependence between the target template and the search area, thereby providing an implicit way to update the target template accordingly. As for the difference, we design a background training enhancement (BTE) mechanism to distinguish negative samples with varying degrees of error, that is, to down-weight outrageous and absurd tracking results to improve the robustness of the tracker. The results on a large number of benchmarks indicate the validity of our results, such as OTB-100, VOT-2018, VOT-2019, and LaSOT.
C1 [Liu, Tianpeng; Li, Jing; Chang, Jun; Song, Beihang; Yao, Bowen] Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China.
   [Wu, Jia] Macquarie Univ, Fac Sci & Engn, Dept Comp, Sydney, NSW 2109, Australia.
C3 Wuhan University; Macquarie University
RP Li, J (corresponding author), Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China.; Wu, J (corresponding author), Macquarie Univ, Fac Sci & Engn, Dept Comp, Sydney, NSW 2109, Australia.
EM tianpengliu@whu.edu.cn; leejingcn@whu.edu.cn; jia.wu@mq.edu.au;
   chang.jun@whu.edu.cn; 2020182120069@whu.edu.cn; yao-0612@hotmail.com
RI Yao, Bowen/AAD-1234-2019
OI Wu, Jia/0000-0002-1371-5801
FU Science and Technology Major Project of Hubei Province (Next-Generation
   AI Technologies) [2019AEA170]; Hubei Province Key R&D Program Project
   [2020BAB130]
FX This work was supported in part by the Science and Technology Major
   Project of Hubei Province (Next-Generation AI Technologies) under Grant
   2019AEA170 and in part by the Hubei Province Key R&D Program Project
   under Grant 2020BAB130. The numerical calculations in this article have
   been done on the supercomputing system in the Supercomputing Center of
   Wuhan University.
CR Babenko B, 2011, IEEE T PATTERN ANAL, V33, P1619, DOI 10.1109/TPAMI.2010.226
   Bao Q, 2021, IEEE T MULTIMEDIA, V23, P161, DOI 10.1109/TMM.2020.2980194
   Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bhat G, 2019, IEEE I CONF COMP VIS, P6181, DOI 10.1109/ICCV.2019.00628
   Cao Y, 2019, IEEE INT CONF COMP V, P1971, DOI 10.1109/ICCVW.2019.00246
   Chen X, 2021, PROC CVPR IEEE, P8122, DOI 10.1109/CVPR46437.2021.00803
   Chen Y., 2018, Adv. Neural Inf. Process. Syst, V31
   Chen ZD, 2020, PROC CVPR IEEE, P6667, DOI 10.1109/CVPR42600.2020.00670
   Chen ZZ, 2022, IEEE T MULTIMEDIA, V24, P609, DOI 10.1109/TMM.2021.3056896
   Choi J, 2018, PROC CVPR IEEE, P479, DOI 10.1109/CVPR.2018.00057
   Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2017, IEEE T PATTERN ANAL, V39, P1561, DOI 10.1109/TPAMI.2016.2609928
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   De Ath G, 2019, Arxiv, DOI arXiv:1805.08511
   De Boer PT, 2005, ANN OPER RES, V134, P19, DOI 10.1007/s10479-005-5724-z
   Dunnhofer M, 2019, IEEE INT CONF COMP V, P2290, DOI 10.1109/ICCVW.2019.00282
   Fan BJ, 2022, IEEE T MULTIMEDIA, V24, P2766, DOI 10.1109/TMM.2021.3087347
   Fan H, 2019, PROC CVPR IEEE, P7944, DOI 10.1109/CVPR.2019.00814
   Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1144, DOI [10.1109/ICCV.2017.128, 10.1109/ICCV.2017.129]
   Guo DY, 2021, PROC CVPR IEEE, P9538, DOI 10.1109/CVPR46437.2021.00942
   Guo DY, 2020, PROC CVPR IEEE, P6268, DOI 10.1109/CVPR42600.2020.00630
   Han WC, 2021, PROC CVPR IEEE, P16565, DOI 10.1109/CVPR46437.2021.01630
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Hu J, 2018, ADV NEUR IN, V31
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jamal MB, 2020, SYMMETRY-BASEL, V12, DOI 10.3390/sym12030358
   Kawale J, 2015, P 24 ACM INT C INF K, P811, DOI DOI 10.1145/2806416.2806527
   Kristan M, 2019, LECT NOTES COMPUT SC, V11129, P3, DOI 10.1007/978-3-030-11009-3_1
   Kristanl M, 2019, IEEE INT CONF COMP V, P2206, DOI 10.1109/ICCVW.2019.00276
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li F, 2018, PROC CVPR IEEE, P4904, DOI 10.1109/CVPR.2018.00515
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Li X, 2019, PROC CVPR IEEE, P1369, DOI 10.1109/CVPR.2019.00146
   Lukezic A, 2017, PROC CVPR IEEE, P4847, DOI 10.1109/CVPR.2017.515
   Mazzeo PL, 2020, J AMB INTEL HUM COMP, V11, P3089, DOI 10.1007/s12652-017-0461-0
   Park J., 2018, P BRIT MACH VIS C
   Qin ZQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P763, DOI 10.1109/ICCV48922.2021.00082
   Real E, 2017, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR.2017.789
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Roy AG, 2019, IEEE T MED IMAGING, V38, P540, DOI 10.1109/TMI.2018.2867261
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shen JB, 2022, IEEE T PATTERN ANAL, V44, P8896, DOI 10.1109/TPAMI.2021.3127492
   Shen JB, 2020, IEEE T CYBERNETICS, V50, P3068, DOI 10.1109/TCYB.2019.2936503
   Song YB, 2018, PROC CVPR IEEE, P8990, DOI 10.1109/CVPR.2018.00937
   Sun C, 2018, PROC CVPR IEEE, P8962, DOI 10.1109/CVPR.2018.00934
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Tao R, 2016, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2016.158
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Tripp AiliMari., 2019, Oxford Research Encyclopedia of Politics, DOI DOI 10.1093/ACREFORE/9780190228637.013.713
   Varfolomieiev A, 2021, J REAL-TIME IMAGE PR, V18, P233, DOI 10.1007/s11554-020-00967-y
   Vaswani A, 2017, ADV NEUR IN, V30
   Voigtlaender P, 2020, PROC CVPR IEEE, P6577, DOI 10.1109/CVPR42600.2020.00661
   Wan J, 2021, NEURAL NETWORKS, V136, P233, DOI 10.1016/j.neunet.2020.11.001
   Wan J, 2022, IEEE T NEUR NET LEAR, V33, P2181, DOI 10.1109/TNNLS.2020.3044078
   Wan J, 2021, IEEE T IMAGE PROCESS, V30, P121, DOI 10.1109/TIP.2020.3032029
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/isqed48828.2020.9137057, 10.1109/ISQED48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang Q, 2019, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2019.00142
   Wang Q, 2018, PROC CVPR IEEE, P4854, DOI 10.1109/CVPR.2018.00510
   Woo S., 2021, PROC EUR C COMPUT VI, P3398
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xingping Dong, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P378, DOI 10.1007/978-3-030-58565-5_23
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Xu Q, 2022, IEEE T MULTIMEDIA, V24, P567, DOI 10.1109/TMM.2021.3055362
   Xu TY, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2919201
   Yang TY, 2020, PROC CVPR IEEE, P6717, DOI 10.1109/CVPR42600.2020.00675
   Yang TY, 2018, LECT NOTES COMPUT SC, V11213, P153, DOI 10.1007/978-3-030-01240-3_10
   Yao SY, 2021, IEEE T IMAGE PROCESS, V30, P4814, DOI 10.1109/TIP.2021.3076272
   Yu F., 2015, ARXIV
   Yu YC, 2020, PROC CVPR IEEE, P6727, DOI 10.1109/CVPR42600.2020.00676
   Zhang H, 2022, IEEE COMPUT SOC CONF, P2735, DOI 10.1109/CVPRW56347.2022.00309
   Zhang JM, 2014, LECT NOTES COMPUT SC, V8694, P188, DOI 10.1007/978-3-319-10599-4_13
   Zhang ZP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13319, DOI 10.1109/ICCV48922.2021.01309
   Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472
   Zheng Z., 2021, IEEE Trans. Cybern.
   Zhipeng Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P771, DOI 10.1007/978-3-030-58589-1_46
   Zhu Z., 2018, 2018 IEEE International Magnetics Conference (INTERMAG), DOI 10.1109/INTMAG.2018.8508710
NR 85
TC 5
Z9 5
U1 3
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5330
EP 5343
DI 10.1109/TMM.2022.3190679
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300051
DA 2024-07-18
ER

PT J
AU Ma, GQ
   Bai, YL
   Zhang, W
   Yao, T
   Shihada, B
   Mei, T
AF Ma, Guoqing
   Bai, Yalong
   Zhang, Wei
   Yao, Ting
   Shihada, Basem
   Mei, Tao
TI Boosting Generic Visual-Linguistic Representation With Dynamic Contexts
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Task analysis; Context modeling; Transformers;
   Commonsense reasoning; Training; Representation learning;
   Vision-language pretraining; event reasoning; dynamic contexts
ID LANGUAGE
AB Pretraining large models on generous multi-modal corpora has accelerated the development of visual-linguistic (VL) representation and achieved great success on various vision-and-language downstream tasks. Learning these models is usually executed by predicting the randomly masked words of captions or patches in images. Such approaches, nevertheless, seldom explore the supervision of causalities behind the caption descriptions or the procedure of generating events beyond still images. In this work, we endow the pretrained models with high-level cognition by delving into dynamic contexts to model the visual and linguistic causalities uniformly. Specifically, we format the dynamic contexts of an image as the sentences describing the events before, on, and after image. Unlike traditional caption-wise similarity, we propose a novel dynamic contexts-based similarity (DCS) metric, in which the correlation of potential causes and effects besides immediate visual content are considered to measure the relevance among images. DCS can be further simplified by parameterizing event continuity to relax the requirements on dense contextual event annotations. A new pre-task is designed to minimize the feature distances of dynamically contextual relevant images and incorporate the event causality and commonsense knowledge into the VL representation learning. Models based on our dynamic contexts significantly outperform typical VL models on multiple cross-modal downstream tasks, including the conventional visual commonsense reasoning (VCR), visual question answering (VQA), zero-shot image-text retrieval, and extended image / event ordering tasks.
C1 [Ma, Guoqing; Shihada, Basem] King Abdullah Univ Sci Technol, Thuwal 23955, Saudi Arabia.
   [Bai, Yalong; Zhang, Wei; Yao, Ting; Mei, Tao] Explore Acad, Beijing 100029, Peoples R China.
C3 King Abdullah University of Science & Technology
RP Shihada, B (corresponding author), King Abdullah Univ Sci Technol, Thuwal 23955, Saudi Arabia.
EM guoqing.ma@kaust.edu.sa; ylbai@outlook.com; wzhang.cu@gmail.com;
   tingyao.ustc@gmail.com; bshihada@ieee.org; tmei@live.com
RI Shihada, Basem/R-2341-2019
OI Shihada, Basem/0000-0003-4434-4334; Zhang, Wei/0000-0002-1492-8286; Yao,
   Ting/0000-0001-7587-101X; Ma, Guoqing/0000-0002-7975-9591
CR Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Cer D, 2018, CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018): PROCEEDINGS OF SYSTEM DEMONSTRATIONS, P169
   Chen HF, 2021, IEEE T MULTIMEDIA, V23, P4171, DOI 10.1109/TMM.2020.3037496
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Haoran Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P18, DOI 10.1007/978-3-030-58586-0_2
   Li LH, 2019, Arxiv, DOI [arXiv:1908.03557, DOI 10.48550/ARXIV.1908.03557]
   Hendricks LA, 2021, T ASSOC COMPUT LING, V9, P570, DOI 10.1162/tacl_a_00385
   Hendrycks D, 2020, Arxiv, DOI arXiv:1606.08415
   Holzinger A, 2021, INFORM FUSION, V71, P28, DOI 10.1016/j.inffus.2021.01.008
   Hu RH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1419, DOI 10.1109/ICCV48922.2021.00147
   Huang LH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1148, DOI 10.1145/3474085.3481541
   Huang ZC, 2021, PROC CVPR IEEE, P12971, DOI 10.1109/CVPR46437.2021.01278
   Jae Sung Park, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P508, DOI 10.1007/978-3-030-58558-7_30
   Ji Z., 2021, INT JOINT C ART INT, P765, DOI DOI 10.24963/IJCAI.2021/106
   Kim W, 2021, PR MACH LEARN RES, V139
   Kiros R, 2015, 29 ANN C NEURAL INFO, V28
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Hamilton WL, 2018, Arxiv, DOI [arXiv:1709.05584, 10.48550/arXiv.1709.05584]
   Lachaux MA, 2021, ADV NEUR IN, V34
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Li Chenliang, 2021, P 59 ANN M ASS COMPU, V1, P6309, DOI 10.18653
   Li G, 2020, AAAI CONF ARTIF INTE, V34, P11336
   Li X., 2020, P EUR C COMP VIS, DOI DOI 10.1007/978-3-030-58577-8_8
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lu JS, 2019, ADV NEUR IN, V32
   Marino K, 2021, PROC CVPR IEEE, P14106, DOI 10.1109/CVPR46437.2021.01389
   Marino K, 2019, PROC CVPR IEEE, P3190, DOI 10.1109/CVPR.2019.00331
   Ng E.G., 2021, PROC 25 C COMPUTNATU, P183, DOI DOI 10.18653/V1/2021.CONLL-1.14
   Ordonez V., 2011, ADV NEURAL INFORM PR, P1143
   Plummer BA, 2015, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2015.303
   Radford A., 2019, LANGUAGE MODELS ARE
   Radford Alec, 2018, IMPROVING LANGUAGE U, DOI DOI 10.18653/V1/N18-1202
   Sharma P, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2556
   Su WJ, 2019, ANN NUTR METAB, V75, P31, DOI 10.1159/000501710
   Su Y. H. W., 2018, BERT: Pre-training of deep bidirectional transformers forlanguage understanding
   Vaswani A, 2017, ADV NEUR IN, V30
   Wolf T, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS, P38
   Yen-Chun Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P104, DOI 10.1007/978-3-030-58577-8_7
   Yu F, 2021, AAAI CONF ARTIF INTE, V35, P3208
   Yu Z, 2019, PROC CVPR IEEE, P6274, DOI 10.1109/CVPR.2019.00644
   Zellers R, 2019, PROC CVPR IEEE, P6713, DOI 10.1109/CVPR.2019.00688
   Zhang HW, 2022, IEEE T MULTIMEDIA, V24, P1449, DOI 10.1109/TMM.2021.3065498
   Zhang YY, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1089, DOI 10.1145/3343031.3351033
   Zhu T, 2023, IEEE T MULTIMEDIA, V25, P3375, DOI 10.1109/TMM.2022.3160060
   Zhu YK, 2015, IEEE I CONF COMP VIS, P19, DOI 10.1109/ICCV.2015.11
NR 46
TC 0
Z9 0
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8445
EP 8457
DI 10.1109/TMM.2023.3237164
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000052
OA Green Published
DA 2024-07-18
ER

PT J
AU Ma, ZH
   Yang, X
   Fang, H
   Zhang, WM
   Yu, NH
AF Ma, Zehua
   Yang, Xi
   Fang, Han
   Zhang, Weiming
   Yu, Nenghai
TI OAcode: Overall Aesthetic 2D Barcode on Screen
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Distortion; QR codes; Visualization; Lenses; Demodulation; Robustness;
   Codes; Aesthetic 2D barcode; perspective distortion; lens distortion;
   auto-convolution function
ID BLIND QUALITY ASSESSMENT; QR CODE; WATERMARKING; ALGORITHMS
AB two-dimensional (2D) barcodes have been widely used in various domains. And a series of aesthetic 2D barcode schemes have been proposed to improve the visual quality and readability of 2D barcodes for better integration with marketing materials. Yet we believe that the existing aesthetic 2D barcode schemes are partially aesthetic because they only beautify the data area but retain the position detection patterns with the black-white appearance of traditional 2D barcode schemes. Thus, in this paper, we propose the first overall aesthetic 2D barcode scheme, called OAcode, in which the position detection pattern is canceled. Its detection process is based on the pre designed symmetrical data area of OAcode, whose symmetry could be used as the calibration signal to restore the perspective transformation in the barcode scanning process. Moreover, an enhanced demodulation method is proposed to resist the lens distortion common in the camera-shooting process. The experimental results illustrate that when 5 x 5 cm OAcode is captured with a resolution of 720 x 1280 pixels, at the screen camera distance of 10 cm and the angle less or equal to 25(degrees), OAcode has 100% detection rate and 99.5% demodulation accuracy. For 10 x 10 cm OAcode, it could be extracted by consumer-grade mobile phones at a distance of 90 cm with around 90% accuracy.
C1 [Ma, Zehua; Yang, Xi; Zhang, Weiming; Yu, Nenghai] Univ Sci & Technol China, CAS Key Lab Electromagnet Space Informat, Hefei 230026, Peoples R China.
   [Fang, Han] Natl Univ Singapore, Sch Comp, Singapore 117417, Singapore.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; National University of Singapore
RP Zhang, WM (corresponding author), Univ Sci & Technol China, CAS Key Lab Electromagnet Space Informat, Hefei 230026, Peoples R China.
EM mzh045@mail.ustc.edu.cn; yx9726@mail.ustc.edu.cn; fanghan@nus.edu.sg;
   zhangwm@ustc.edu.cn; ynh@ustc.edu.cn
OI Zhang, Weiming/0000-0001-5576-6108; Ma, Zehua/0000-0002-8153-341X
FU National Natural Science Foundation of China
FX No Statement Available
CR Aliva N., 2020, Visualead
   [Anonymous], 2020, Data matrix
   [Anonymous], 2014, BelgaLogos dataset
   [Anonymous], 2014, CVG-UGR image database
   [Anonymous], 2022, Research on QR code marketing: Effectiveness, adoption, and use cases
   [Anonymous], 2020, QRCode
   Baharav Z, 2013, IEEE INT CON MULTI
   Cata T., 2013, Journal of Mobile Technologies, Knowledge and Society, P1
   Chapman T, 2015, HSPA EVOLUTION: THE FUNDAMENTALS FOR MOBILE BROADBAND, P35, DOI 10.1016/B978-0-08-099969-2.00003-X
   Cheddad A, 2010, SIGNAL PROCESS, V90, P727, DOI 10.1016/j.sigpro.2009.08.010
   Chen CS, 2018, IEEE T CIRC SYST VID, V28, P3300, DOI 10.1109/TCSVT.2017.2741472
   Chen CS, 2019, IEEE T IMAGE PROCESS, V28, P156, DOI 10.1109/TIP.2018.2865681
   Chu HK, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508408
   Cox R., 2012, Qartcodes
   Denso Wave Inc. and Elkhart, 2020, Securing margin
   Fang H, 2022, IEEE T MULTIMEDIA, V24, P955, DOI 10.1109/TMM.2021.3061801
   Fang H, 2020, IEEE T CIRC SYST VID, V30, P4075, DOI 10.1109/TCSVT.2019.2953720
   Fang H, 2019, IEEE T INF FOREN SEC, V14, P1403, DOI 10.1109/TIFS.2018.2878541
   Garateguy GJ, 2014, IEEE T IMAGE PROCESS, V23, P2842, DOI 10.1109/TIP.2014.2321501
   Hossain MS, 2018, INT J ENG BUS MANAG, V10, DOI 10.1177/1847979018812323
   Huang W., 2013, Proceedings of the 19th annual international conference on Mobile computing networking, P139
   Jia J, 2022, P IEEECVF C COMPUTER, P2273
   Jia J, 2022, IEEE T CYBERNETICS, V52, P7094, DOI 10.1109/TCYB.2020.3037208
   Jia J, 2020, IEEE J-STSP, V14, P688, DOI 10.1109/JSTSP.2020.2976566
   Jia J, 2019, IEEE INTERNET THINGS, V6, P9919, DOI 10.1109/JIOT.2019.2933254
   Kang S., 2021, IEEE Access, V9
   Lim J.S., 1990, Two-dimensional Signal and Image Processing
   Lin SS, 2015, IEEE T MULTIMEDIA, V17, P1515, DOI 10.1109/TMM.2015.2437711
   Lin YH, 2013, IEEE T MULTIMEDIA, V15, P2198, DOI 10.1109/TMM.2013.2271745
   Liu JC, 2011, OPT ENG, V50, DOI 10.1117/1.3529430
   Ma K., 2022, P ACM SIGGRAPH C, P1
   Ma ZH, 2021, IEEE T CIRC SYST VID, V31, P4826, DOI 10.1109/TCSVT.2021.3055255
   Min XK, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3470970
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P6054, DOI 10.1109/TIP.2020.2988148
   Min XK, 2018, IEEE T MULTIMEDIA, V20, P2049, DOI 10.1109/TMM.2017.2788206
   Min XK, 2018, IEEE T BROADCAST, V64, P508, DOI 10.1109/TBC.2018.2816783
   Min XK, 2018, SIGNAL PROCESS, V145, P127, DOI 10.1016/j.sigpro.2017.10.025
   Min XK, 2017, IEEE T IMAGE PROCESS, V26, P5462, DOI 10.1109/TIP.2017.2735192
   Ohbuchi E, 2004, 2004 INTERNATIONAL CONFERENCE ON CYBERWORLDS, PROCEEDINGS, P260, DOI 10.1109/CW.2004.23
   Okazaki S, 2012, J ADVERTISING RES, V52, P102, DOI 10.2501/JAR-52-1-102-117
   Petitcolas FAP, 1998, LECT NOTES COMPUT SC, V1525, P218
   Petitcolas FAP, 2000, IEEE SIGNAL PROC MAG, V17, P58, DOI 10.1109/79.879339
   Petitcolas FAP, 1999, P IEEE, V87, P1062, DOI 10.1109/5.771065
   PICKHOLTZ RL, 1982, IEEE T COMMUN, V30, P855, DOI 10.1109/TCOM.1982.1095533
   Podilchuk CI, 2001, IEEE SIGNAL PROC MAG, V18, P33, DOI 10.1109/79.939835
   Provos N., 2003, IEEE Security & Privacy, V1, P32, DOI 10.1109/MSECP.2003.1203220
   Sellen A.J., 2003, MYTH PAPERLESS OFFIC
   Subramanian N, 2021, IEEE ACCESS, V9, P23409, DOI 10.1109/ACCESS.2021.3053998
   Tancik M, 2020, PROC CVPR IEEE, P2114, DOI 10.1109/CVPR42600.2020.00219
   Xu ML, 2019, IEEE T MULTIMEDIA, V21, P1960, DOI 10.1109/TMM.2019.2891420
   Yang Z, 2016, UBICOMP'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING, P904, DOI 10.1145/2971648.2971733
   Zhai GT, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2757-1
   Zhang JH, 2021, MULTIMED TOOLS APPL, V80, P16153, DOI 10.1007/s11042-019-08578-x
NR 53
TC 1
Z9 1
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8687
EP 8698
DI 10.1109/TMM.2023.3239755
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HY2K6
UT WOS:001163004000001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Niu, KP
   Liu, YL
   Wu, EH
   Xing, GY
AF Niu, Kunpeng
   Liu, Yanli
   Wu, Enhua
   Xing, Guanyu
TI A Boundary-Aware Network for Shadow Removal
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Optimization; Image edge detection; Decoding;
   Lighting; Task analysis; Image color analysis; Branch interaction;
   multiscale features; shadow boundary; shadow removal
ID TRACKING
AB Shadow removal is a challenging computer vision and multimedia task that aims to restore image content in shadow regions. The state-of-the-art shadow removal methods introduce artifacts near shadow boundaries or inconsistencies between shadow and nonshadow areas, which can be easily noticed by the human eye at first glance. In this paper, we design a boundary-aware shadow removal network (BA-ShadowNet) that improves shadow removal accuracy by increasing the removal performance at shadow boundaries. In contrast with previously developed methods, which usually consider shadow boundary optimization to be a postprocessing technique, our method performs shadow removal and shadow boundary optimization simultaneously. For this purpose, the proposed BA-ShadowNet is designed as a multiscale encoder-decoder structure, where the decoder consists of a shadow removal branch and a shadow optimization branch. An interaction module is then introduced to fuse and exchange the features of the two branches. This module facilitates the removal branch in perceiving the locations and colors of shadow boundaries. Additionally, it optimizes the boundary branch according to the image context extracted from the removal branch. A three-term loss function is further developed to supervise the shadow removal results and to address the issue of imbalanced supervision between shadow boundary pixels and pixels inside shadows. Extensive experiments conducted on the ISTD+ and SRD datasets demonstrate that the proposed BA-ShadowNet greatly outperforms the state-of-the-art methods with respect to shadow removal.
C1 [Niu, Kunpeng; Liu, Yanli; Xing, Guanyu] Sichuan Univ, Coll Comp Sci, Chengdu 610065, Peoples R China.
   [Wu, Enhua] Chinese Acad Sci, Inst Software, SKLab Comp Sci, Beijing 100190, Peoples R China.
   [Wu, Enhua] Univ Macau, Fac Sci & Technol, Macau 999078, Peoples R China.
C3 Sichuan University; Chinese Academy of Sciences; Institute of Software,
   CAS; University of Macau
RP Xing, GY (corresponding author), Sichuan Univ, Coll Comp Sci, Chengdu 610065, Peoples R China.
EM niukunpeng@stu.scu.edu.cn; yanliliu@scu.edu.cn; ehwu@um.edu.mo;
   xingguanyu@scu.edu.cn
FU National Natural Science Foundation of China [61972271]; Sichuan Science
   and Technology Program [2022YFS0557]; National Natural Science
   Foundation of China [62072449, 62172290]
FX The work of Kunpeng Niu and Yanli Liu was supported in part by the
   National Natural Science Foundation of China under Project 61972271, in
   part by Sichuan Science and Technology Program under Project
   2022YFS0557. Enhua Wu was support by the National Natural Science
   Foundation of China under Project 62072449. Guanyu Xing was support by
   the National Natural Science Foundation of China under Project 62172290.
CR Barrow H. G., 1978, Computer Vision Systems, P3
   Chen ZH, 2020, PROC CVPR IEEE, P5610, DOI 10.1109/CVPR42600.2020.00565
   Chuang YY, 2003, ACM T GRAPHIC, V22, P494, DOI 10.1145/882262.882298
   Cun XD, 2020, AAAI CONF ARTIF INTE, V34, P10680
   Ding HH, 2019, IEEE I CONF COMP VIS, P6818, DOI 10.1109/ICCV.2019.00692
   Finlayson GD, 2006, IEEE T PATTERN ANAL, V28, P59, DOI 10.1109/TPAMI.2006.18
   Fu KR, 2019, IEEE T MULTIMEDIA, V21, P457, DOI 10.1109/TMM.2018.2859746
   Fu L, 2021, PROC CVPR IEEE, P10566, DOI 10.1109/CVPR46437.2021.01043
   Gadde R, 2016, LECT NOTES COMPUT SC, V9905, P597, DOI 10.1007/978-3-319-46448-0_36
   Ghosh S, 2019, NATL CONF COMMUN, DOI 10.1109/ncc.2019.8732250
   Gong H., 2014, P BRIT MACH VIS C BM, P1
   Gryka M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2732407
   Guo RQ, 2013, IEEE T PATTERN ANAL, V35, P2956, DOI 10.1109/TPAMI.2012.214
   Hu XW, 2019, IEEE I CONF COMP VIS, P2472, DOI 10.1109/ICCV.2019.00256
   Hu XW, 2020, IEEE T PATTERN ANAL, V42, P2795, DOI 10.1109/TPAMI.2019.2919616
   Jin YY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5007, DOI 10.1109/ICCV48922.2021.00498
   Jun Wei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13022, DOI 10.1109/CVPR42600.2020.01304
   Kang B, 2018, IEEE T MULTIMEDIA, V20, P2478, DOI 10.1109/TMM.2018.2798282
   Le H, 2022, IEEE T PATTERN ANAL, V44, P9088, DOI 10.1109/TPAMI.2021.3124934
   Le H, 2019, IEEE I CONF COMP VIS, P8577, DOI 10.1109/ICCV.2019.00867
   Liang NX, 2018, IEEE T MULTIMEDIA, V20, P2289, DOI 10.1109/TMM.2018.2803518
   Liu C, 2018, IEEE T MULTIMEDIA, V20, P889, DOI 10.1109/TMM.2017.2760633
   Liu ZH, 2021, PROC CVPR IEEE, P4925, DOI 10.1109/CVPR46437.2021.00489
   Liu ZH, 2021, IEEE T IMAGE PROCESS, V30, P1853, DOI 10.1109/TIP.2020.3048677
   Nadimi S, 2004, IEEE T PATTERN ANAL, V26, P1079, DOI 10.1109/TPAMI.2004.51
   Qian R, 2018, PROC CVPR IEEE, P2482, DOI 10.1109/CVPR.2018.00263
   Qu LQ, 2017, PROC CVPR IEEE, P2308, DOI 10.1109/CVPR.2017.248
   Saravanakumar S., 2010, 2010 International Conference on Signal and Image Processing (ICSIP 2010), P79, DOI 10.1109/ICSIP.2010.5697446
   Shao ZR, 2023, IEEE T MULTIMEDIA, V25, P112, DOI 10.1109/TMM.2021.3121571
   Shor Y, 2008, COMPUT GRAPH FORUM, V27, P577, DOI 10.1111/j.1467-8659.2008.01155.x
   Su JM, 2019, IEEE I CONF COMP VIS, P3798, DOI 10.1109/ICCV.2019.00390
   Sultana M, 2021, IEEE T MULTIMEDIA, V23, P2005, DOI 10.1109/TMM.2020.3006419
   Vicente TFY, 2018, IEEE T PATTERN ANAL, V40, P682, DOI 10.1109/TPAMI.2017.2691703
   Wang JF, 2018, PROC CVPR IEEE, P1788, DOI 10.1109/CVPR.2018.00192
   Wu WY, 2018, PROC CVPR IEEE, P2129, DOI 10.1109/CVPR.2018.00227
   Xiao CX, 2013, COMPUT GRAPH FORUM, V32, P207, DOI 10.1111/cgf.12198
   Yang QX, 2012, IEEE T IMAGE PROCESS, V21, P4361, DOI 10.1109/TIP.2012.2208976
   Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, P4623, DOI 10.1109/TIP.2015.2465159
   Zhang TY, 2019, IEEE T MULTIMEDIA, V21, P2930, DOI 10.1109/TMM.2019.2914870
NR 40
TC 0
Z9 0
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6782
EP 6793
DI 10.1109/TMM.2022.3214422
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000006
DA 2024-07-18
ER

PT J
AU Shan, JY
   Zhou, SF
   Cui, YB
   Fang, Z
AF Shan, Jiayao
   Zhou, Sifan
   Cui, Yubo
   Fang, Zheng
TI Real-Time 3D Single Object Tracking With Transformer
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D single object tracking; lidar point-cloud; siamese network;
   transformer; self attention
AB LiDAR-based 3D single object tracking is a challenging issue in robotics and autonomous driving. Currently, existing approaches usually suffer from the problem that objects at long distance often have very sparse or partially-occluded point clouds, which makes the features extracted by the model ambiguous. Ambiguous features will make it hard to locate the target object and finally lead to bad tracking results. To solve this problem, we utilize the powerful Transformer architecture and propose a Point-Track-Transformer (PTT) module for point cloud-based 3D single object tracking task. Specifically, PTT module generates fine-tuned attention features by computing attention weights, which guides the tracker focusing on the important features of the target and improves the tracking ability in complex scenarios. To evaluate our PTT module, we embed PTT into the dominant method and construct a novel 3D SOT tracker named PTT-Net. In PTT-Net, we embed PTT into the voting stage and proposal generation stage, respectively. PTT module in the voting stage could model the interactions among point patches, which learns context-dependent features. Meanwhile, PTT module in the proposal generation stage could capture the contextual information between object and background. We evaluate our PTT-Net on KITTI and NuScenes datasets. Experimental results demonstrate the effectiveness of PTT module and the superiority of PTT-Net, which surpasses the baseline by a noticeable margin, similar to 10% in the Car category. Meanwhile, our method also has a significant performance improvement in sparse scenarios. In general, the combination of transformer and tracking pipeline enables our PTT-Net to achieve state-of-the-art performance on both two datasets. Additionally, PTT-Net could run in real-time at 40FPS on NVIDIA 1080Ti GPU. Our code is open-sourced for the research community at https://github.com/shanjiayao/PTT.
C1 [Shan, Jiayao; Zhou, Sifan; Cui, Yubo; Fang, Zheng] Northeastern Univ, Fac Robot Sci & Engn, Shenyang 110819, Peoples R China.
   [Shan, Jiayao] Sci & Technol Near Surface Detect Lab, Wuxi 214000, Peoples R China.
C3 Northeastern University - China
RP Fang, Z (corresponding author), Northeastern Univ, Fac Robot Sci & Engn, Shenyang 110819, Peoples R China.
EM shanjiayao97@gmail.com; sifanjay@gmail.com; ybcui21@stumail.neu.edu.cn;
   fangzheng@mail.neu.edu.cn
OI Zhou, Sifan/0000-0003-3602-7566; Cui, Yubo/0000-0001-5302-0484; Fang,
   Zheng/0000-0003-3887-3141
FU National Natural Science Foundation of China [62073066, U20A20197];
   Science and Technology on Near-Surface Detection Laboratory
   [N182608003]; Major Special Science and Technology Project of Liaoning
   Province [2019JH1/10100026]; Aeronautical Science Foundation of China
   [201941050001]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62073066 and U20A20197, in part by the
   Science and Technology on Near-Surface Detection Laboratory under
   Grant6142414200208, in part by the Fundamental Research Funds for the
   Central Universities under Grant N182608003, in part by the Major
   Special Science and Technology Project of Liaoning Province under Grant
   2019JH1/10100026,and in part by the Aeronautical Science Foundation of
   China under Grant 201941050001.
CR Baser E, 2019, IEEE INT VEH SYM, P1426, DOI [10.1109/ivs.2019.8813779, 10.1109/IVS.2019.8813779]
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bibi A, 2016, PROC CVPR IEEE, P1439, DOI 10.1109/CVPR.2016.160
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Caesar Holger, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11618, DOI 10.1109/CVPR42600.2020.01164
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen X, 2021, PROC CVPR IEEE, P8122, DOI 10.1109/CVPR46437.2021.00803
   Chu P, 2021, Arxiv, DOI arXiv:2104.00194
   Comport A. I., 2004, 2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566), P692
   Cui YB, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20010143
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dong XP, 2021, IEEE T PATTERN ANAL, V43, P1515, DOI 10.1109/TPAMI.2019.2956703
   Dong XP, 2018, LECT NOTES COMPUT SC, V11217, P472, DOI 10.1007/978-3-030-01261-8_28
   Engel N., 2020, arXiv, DOI DOI 10.1109/ACCESS.2021.3116304
   Fang Z, 2021, IEEE SENS J, V21, P4995, DOI 10.1109/JSEN.2020.3033034
   Galoogahi HK, 2015, PROC CVPR IEEE, P4630, DOI 10.1109/CVPR.2015.7299094
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Giancola S, 2019, PROC CVPR IEEE, P1359, DOI 10.1109/CVPR.2019.00145
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Han WC, 2021, PROC CVPR IEEE, P16565, DOI 10.1109/CVPR46437.2021.01630
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Hongyuan Du, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P230, DOI 10.1007/978-3-030-58621-8_14
   Hu H, 2019, IEEE I CONF COMP VIS, P3463, DOI 10.1109/ICCV.2019.00356
   Kart U, 2019, PROC CVPR IEEE, P1339, DOI 10.1109/CVPR.2019.00143
   Kart U, 2019, LECT NOTES COMPUT SC, V11129, P148, DOI 10.1007/978-3-030-11009-3_8
   Kristan M, 2016, IEEE T PATTERN ANAL, V38, P2137, DOI 10.1109/TPAMI.2016.2516982
   Ku J, 2018, IEEE INT C INT ROBOT, P5750, DOI 10.1109/IROS.2018.8594049
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Liu Y, 2019, IEEE T MULTIMEDIA, V21, P664, DOI 10.1109/TMM.2018.2863604
   Luo WJ, 2018, PROC CVPR IEEE, P3569, DOI 10.1109/CVPR.2018.00376
   Machida E, 2012, 2012 PROCEEDINGS OF SICE ANNUAL CONFERENCE (SICE), P2207
   Meinhardt T, 2022, Arxiv, DOI arXiv:2101.02702
   Pan XR, 2021, PROC CVPR IEEE, P7459, DOI 10.1109/CVPR46437.2021.00738
   Qi C., 2017, NeurIPS, P5105, DOI [10.1109/ CVPR.2017.16, DOI 10.1109/CVPR.2017.16]
   Qi CR, 2019, IEEE I CONF COMP VIS, P9276, DOI 10.1109/ICCV.2019.00937
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Qi HZ, 2020, PROC CVPR IEEE, P6328, DOI 10.1109/CVPR42600.2020.00636
   Ramachandran P, 2019, ADV NEUR IN, V32
   Shan JY, 2021, IEEE INT C INT ROBOT, P1310, DOI 10.1109/IROS51168.2021.9636821
   Shen JB, 2020, IEEE T CYBERNETICS, V50, P3068, DOI 10.1109/TCYB.2019.2936503
   Shenoi A, 2020, IEEE INT C INT ROBOT, P10335, DOI 10.1109/IROS45743.2020.9341635
   Shi SS, 2019, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2019.00086
   Simon M, 2019, IEEE COMPUT SOC CONF, P1190, DOI 10.1109/CVPRW.2019.00158
   Sun PZ, 2021, Arxiv, DOI arXiv:2012.15460
   Tian SJ, 2021, IEEE T MULTIMEDIA, V23, P120, DOI 10.1109/TMM.2020.2978636
   Vaswani A, 2017, ADV NEUR IN, V30
   Voigtlaender P, 2020, PROC CVPR IEEE, P6577, DOI 10.1109/CVPR42600.2020.00661
   Wang N, 2021, PROC CVPR IEEE, P1571, DOI 10.1109/CVPR46437.2021.00162
   Wang Q, 2019, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2019.00142
   Wang SK, 2020, IEEE ROBOT AUTOM LET, V5, P3206, DOI 10.1109/LRA.2020.2974392
   Weng XS, 2020, IEEE INT C INT ROBOT, P10359, DOI 10.1109/IROS45743.2020.9341164
   Wu F., 2019, PROC 7 INT C LEARN R
   Xingping Dong, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P378, DOI 10.1007/978-3-030-58565-5_23
   Zarzar J, 2020, Arxiv, DOI arXiv:1903.10168
   Zetong Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11037, DOI 10.1109/CVPR42600.2020.01105
   Zhao H., 2020, P 2020 IEEE CVF C CO, P10073, DOI [10.1109/CVPR42600.2020.01009, DOI 10.1109/CVPR42600.2020.01009]
   Zhao H., 2020, arXiv
   Zheng C., 2021, P IEEECVF INT C COMP, P13199
   Zhong HS, 2021, IEEE T MULTIMEDIA, V23, P1264, DOI 10.1109/TMM.2020.2995278
   Zou H, 2020, IEEE INT C INT ROBOT, P8133, DOI 10.1109/IROS45743.2020.9341120
NR 63
TC 22
Z9 22
U1 5
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2339
EP 2353
DI 10.1109/TMM.2022.3146714
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100057
OA Green Submitted
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Sun, JD
   Xue, FF
   Li, J
   Zhu, L
   Zhang, HX
   Zhang, J
AF Sun, Jiande
   Xue, Fanfu
   Li, Jing
   Zhu, Lei
   Zhang, Huaxiang
   Zhang, Jia
TI TSINIT: A Two-Stage Inpainting Network for Incomplete Text
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Incomplete text; image inpainting; incomplete text image inpainting;
   text image; text recognition
AB Although there are lots of studies on scene text recognition, few of them focus on the recognition of the incomplete text. The recognition performance of existing text recognition algorithms on the incomplete text is far from the expected, and the recognition of the incomplete text is still challenging. In this paper, an end-to-end Two-Stage Inpainting Network for Incomplete Text (TSINIT) is proposed to reconstruct the incomplete text into the complete one even when the text is in various styles and with various backgrounds, and the reconstructed text can be recognized by the existing text recognition algorithms correctly. The proposed TSINIT is divided into text extraction module (TEM) and text reconstruction module (TRM) to make the inpainting only focus on the text. TEM separates the incomplete text from the background and character-like regions at the pixel level, which can reduce the ambiguity of text reconstruction caused by the background. TRM reconstructs the incomplete text towards the most possible text with the consideration of the abstract and semantic structures of the text. Furthermore, we build a synthetic incomplete text dataset (SITD), which contains contaminated and abraded text images. SITD is divided into 6 incomplete levels according to the number of pixels in the incomplete regions and the ratio of the incomplete characters to all characters. The experimental results show that the proposed method has better inpainting ability for the incomplete text compared with traditional image inpainting algorithms on the proposed SITD and real images. When using the same text recognition method, the recognition accuracy of the incomplete text on SITD can be improved much more with the help of the proposed TSINIT than with the traditional image inpainting methods.
C1 [Sun, Jiande; Xue, Fanfu; Zhu, Lei; Zhang, Huaxiang; Zhang, Jia] Shandong Normal Univ, Sch Informat Sci & Engn, Jinan 250014, Peoples R China.
   [Li, Jing] Shandong Normal Univ, Sch Journalism & Commun, Jinan 250014, Peoples R China.
C3 Shandong Normal University; Shandong Normal University
RP Xue, FF; Zhang, J (corresponding author), Shandong Normal Univ, Sch Informat Sci & Engn, Jinan 250014, Peoples R China.
EM jiandesun@hotmail.com; fanfuxue@outlook.com; lijingjdsun@hotmail.com;
   leizhu0608@gmail.com; huaxzhang@hotmail.com; zhangjia@sdnu.edu.cn
RI Zhu, Lei/GQQ-1130-2022
OI Zhu, Lei/0000-0002-5348-7532; Zhu, Lei/0000-0002-2993-7142; zhang, hua
   xiang/0000-0001-6259-7533; Zhang, Jia/0000-0002-2513-4115
FU Scientific Research Leader Studio of Jinan [2021GXRC081]; Natural
   Science Foundation of Shandong Province [ZR2021LZH010]; Joint Project
   for Smart Computing of Shandong Natural Science Foundation
   [ZR2020LZH015]; Taishan Scholar Project of Shandong Province, China
   [ts20190924]
FX This work was supported in part by the Scientific Research Leader Studio
   of Jinan under Grant 2021GXRC081, in part by the Natural Science
   Foundation of Shandong Province under Grant ZR2021LZH010, in part by the
   Joint Project for Smart Computing of Shandong Natural Science Foundation
   under Grant ZR2020LZH015, and in part by the Taishan Scholar Project of
   Shandong Province, China under Grant ts20190924.
CR Bhunia AK, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P963, DOI 10.1109/ICCV48922.2021.00102
   Bhunia AK, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14920, DOI 10.1109/ICCV48922.2021.01467
   Cui H, 2020, IEEE T IMAGE PROCESS, V29, P1271, DOI 10.1109/TIP.2019.2940693
   Deli Yu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12110, DOI 10.1109/CVPR42600.2020.01213
   Fang SC, 2021, PROC CVPR IEEE, P7094, DOI 10.1109/CVPR46437.2021.00702
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Graham R. L., 1972, Information Processing Letters, V1, P132, DOI 10.1016/0020-0190(72)90045-2
   Gupta A, 2016, PROC CVPR IEEE, P2315, DOI 10.1109/CVPR.2016.254
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He P, 2016, AAAI CONF ARTIF INTE, P3501
   Ibrahim Y, 2021, INT C PATT RECOG, P3178, DOI 10.1109/ICPR48806.2021.9413009
   Gulrajani I, 2017, ADV NEUR IN, V30
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Karkare R., 2021, arXiv
   LEVENSHT.VI, 1965, DOKL AKAD NAUK SSSR+, V163, P845
   Li XW, 2019, IEEE INT C BIOINFORM, P678, DOI [10.1109/bibm47256.2019.8983136, 10.1109/BIBM47256.2019.8983136]
   Liao MH, 2020, AAAI CONF ARTIF INTE, V34, P11474
   Litman Ron, 2020, CVPR, P11959, DOI 10.1109/CVPR42600.2020.01198
   Liu Y, 2019, LECT NOTES COMPUT SC, V11935, P128, DOI 10.1007/978-3-030-36189-1_11
   Lu X, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P715, DOI 10.1145/3331184.3331217
   Mirza M., 2014, P NIPS DEEP LEARN RE
   Qiao Z, 2021, INT C PATT RECOG, P3328, DOI 10.1109/ICPR48806.2021.9412806
   Shen XB, 2022, IEEE T MULTIMEDIA, V24, P1116, DOI 10.1109/TMM.2021.3119868
   Shi BG, 2019, IEEE T PATTERN ANAL, V41, P2035, DOI 10.1109/TPAMI.2018.2848939
   Shi BG, 2017, IEEE T PATTERN ANAL, V39, P2298, DOI 10.1109/TPAMI.2016.2646371
   Wang L, 2021, COMPUT BIOL MED, V136, DOI 10.1016/j.compbiomed.2021.104751
   Wang TW, 2020, AAAI CONF ARTIF INTE, V34, P12216
   Wang Y, 2018, ADV NEUR IN, V31
   Wang YX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14174, DOI 10.1109/ICCV48922.2021.01393
   Xie CH, 2019, IEEE I CONF COMP VIS, P8857, DOI 10.1109/ICCV.2019.00895
   Xuejian Rong, 2016, Computer Vision - ECCV 2016. 14th European Conference: Workshops. Proceedings: LNCS 9913, P109, DOI 10.1007/978-3-319-46604-0_8
   Yeh RA, 2017, PROC CVPR IEEE, P6882, DOI 10.1109/CVPR.2017.728
   Yi Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P752, DOI 10.1007/978-3-030-58595-2_45
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Zhi Qiao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13525, DOI 10.1109/CVPR42600.2020.01354
   Zhu L, 2020, IEEE T IMAGE PROCESS, V29, P4643, DOI 10.1109/TIP.2020.2974065
   Zhu YY, 2018, IEEE T INTELL TRANSP, V19, P209, DOI 10.1109/TITS.2017.2768827
NR 37
TC 3
Z9 3
U1 5
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5166
EP 5177
DI 10.1109/TMM.2022.3189245
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300040
DA 2024-07-18
ER

PT J
AU Sun, SM
   Yu, T
   Xu, JH
   Zhou, W
   Chen, ZB
AF Sun, Simeng
   Yu, Tao
   Xu, Jiahua
   Zhou, Wei
   Chen, Zhibo
TI GraphIQA: Learning Distortion Graph Representations for Blind Image
   Quality Assessment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Blind image quality assessment; graph representation learning;
   pre-training
ID STATISTICS
AB A good distortion representation is crucial for the success of deep blind image quality assessment (BIQA). However, most previous methods do not effectively model the relationship between distortions or the distribution of samples with the same distortion type but different distortion levels. In this work, we start from the analysis of the relationship between perceptual image quality and distortion-related factors, such as distortion types and levels. Then, we propose a Distortion Graph Representation (DGR) learning framework for IQA, named GraphIQA, in which each distortion is represented as a graph, i.e., DGR. One can distinguish distortion types by learning the contrast relationship between these different DGRs, and can infer the ranking distribution of samples from different levels in a DGR. Specifically, we develop two sub-networks to learn the DGRs: a) Type Discrimination Network (TDN) that aims to embed DGR into a compact code for better discriminating distortion types and learning the relationship between types; b) Fuzzy Prediction Network (FPN) that aims to extract the distributional characteristics of the samples in a DGR and predicts fuzzy degrees based on a Gaussian prior. Experiments show that our GraphIQA achieves state-of-the-art performance on many benchmark datasets of both synthetic and authentic distortions.
C1 [Sun, Simeng; Yu, Tao; Xu, Jiahua; Zhou, Wei; Chen, Zhibo] Univ Sci & Technol China, Dept Elect Engineer & Informat Sci, Hefei 230026, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Chen, ZB (corresponding author), Univ Sci & Technol China, Dept Elect Engineer & Informat Sci, Hefei 230026, Anhui, Peoples R China.
EM smsun20@mail.ustc.edu.cn; yutao666@mail.ustc.edu.cn;
   xujiahua@mail.ustc.edu.cn; weichou@mail.ustc.edu.cn;
   chenzhibo@ustc.edu.cn
RI Zhou, Wei/AAG-8797-2020
OI Yu, Tao/0000-0003-2550-5008; Zhou, Wei/0000-0003-3641-1429; Xu,
   Jiahua/0000-0002-1668-9792
FU NSFC [U1908209, 62021001]; National Key Research and Development Program
   of China [2018AAA0101400]
FX This work was supported in part by NSFC under Grants U1908209 and
   62021001, and the National Key Research and Development Program of China
   2018AAA0101400.
CR Golestaneh SA, 2020, Arxiv, DOI arXiv:2006.03783
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Caron M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9630, DOI 10.1109/ICCV48922.2021.00951
   Chen ZB, 2020, IEEE J-STSP, V14, P103, DOI 10.1109/JSTSP.2020.2968182
   Chen ZB, 2018, IEEE T IMAGE PROCESS, V27, P721, DOI 10.1109/TIP.2017.2766780
   Chen ZB, 2017, IEEE T IMAGE PROCESS, V26, P5138, DOI 10.1109/TIP.2017.2736422
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Fang YM, 2015, IEEE SIGNAL PROC LET, V22, P838, DOI 10.1109/LSP.2014.2372333
   Fu J., 2020, BAYESIAN SPATIO TEMP
   Ghadiyaram D, 2017, J VISION, V17, DOI 10.1167/17.1.32
   Ghadiyaram D, 2016, IEEE T IMAGE PROCESS, V25, P372, DOI 10.1109/TIP.2015.2500021
   Golestaneh S. Alireza, 2014, IEEE Signal Processing Letters, V21, P155, DOI 10.1109/LSP.2013.2296038
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Guan JW, 2017, IEEE T MULTIMEDIA, V19, P2505, DOI 10.1109/TMM.2017.2703148
   Hamilton W. L., 2017, B TECHNICAL COMMITTE, V40, P52
   Hamilton WL, 2017, ADV NEUR IN, V30
   Hancheng Zhu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14131, DOI 10.1109/CVPR42600.2020.01415
   Hassen R, 2013, IEEE T IMAGE PROCESS, V22, P2798, DOI 10.1109/TIP.2013.2251643
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Jayaraman D, 2012, CONF REC ASILOMAR C, P1693, DOI 10.1109/ACSSC.2012.6489321
   Jiang QP, 2018, IEEE T MULTIMEDIA, V20, P2035, DOI 10.1109/TMM.2017.2763321
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Kang L, 2015, IEEE IMAGE PROC, P2791, DOI 10.1109/ICIP.2015.7351311
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Kim J, 2017, IEEE SIGNAL PROC MAG, V34, P130, DOI 10.1109/MSP.2017.2736018
   Kim J, 2017, IEEE J-STSP, V11, P206, DOI 10.1109/JSTSP.2016.2639328
   King DB, 2015, ACS SYM SER, V1214, P1
   Kingma D. P., 2014, arXiv
   Kipf T. N., 2017, 8 INT C LEARN REPR, P1
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Leibe B., 2017, ARXIV170307737CS
   Li DQ, 2019, IEEE T MULTIMEDIA, V21, P1221, DOI 10.1109/TMM.2018.2875354
   Li LD, 2016, IEEE T CYBERNETICS, V46, P39, DOI 10.1109/TCYB.2015.2392129
   Li LD, 2014, IEEE SIGNAL PROC LET, V21, P122, DOI 10.1109/LSP.2013.2294333
   Li QH, 2016, IEEE T MULTIMEDIA, V18, P2457, DOI 10.1109/TMM.2016.2601028
   Lin HH, 2018, Arxiv, DOI arXiv:1803.08489
   Lin HH, 2019, INT WORK QUAL MULTIM
   Linderman GC, 2019, NAT METHODS, V16, P243, DOI 10.1038/s41592-018-0308-4
   Liu HT, 2010, IEEE T CIRC SYST VID, V20, P529, DOI 10.1109/TCSVT.2009.2035848
   Liu XL, 2017, IEEE I CONF COMP VIS, P1040, DOI 10.1109/ICCV.2017.118
   Ma KD, 2018, IEEE T IMAGE PROCESS, V27, P1202, DOI 10.1109/TIP.2017.2774045
   Min XK, 2019, IEEE T INTELL TRANSP, V20, P2879, DOI 10.1109/TITS.2018.2868771
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Radford A., 2016, ICLR
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Shi LK, 2020, IEEE T CIRC SYST VID, V30, P4114, DOI 10.1109/TCSVT.2019.2955011
   Su SL, 2020, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR42600.2020.00372
   Talebi H, 2018, IEEE T IMAGE PROCESS, V27, P3998, DOI 10.1109/TIP.2018.2831899
   Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802
   van den Berg R, 2017, Arxiv, DOI arXiv:1706.02263
   Velikovic P., 2017, Graph attention networks, P1
   Wu JJ, 2020, IEEE T IMAGE PROCESS, V29, P7414, DOI 10.1109/TIP.2020.3002478
   Wu QB, 2020, IEEE T CIRC SYST VID, V30, P3883, DOI 10.1109/TCSVT.2020.2972566
   Wu S., 2019, P INT JOINT C ARTIFI, P4532
   Xu JH, 2021, IEEE T CIRC SYST VID, V31, P1724, DOI 10.1109/TCSVT.2020.3015186
   Xu JT, 2016, IEEE T IMAGE PROCESS, V25, P4444, DOI 10.1109/TIP.2016.2585880
   Xu L, 2017, IEEE T CIRC SYST VID, V27, P1833, DOI 10.1109/TCSVT.2016.2543099
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P4850, DOI 10.1109/TIP.2014.2355716
   Xue WF, 2013, PROC CVPR IEEE, P995, DOI 10.1109/CVPR.2013.133
   Yan B, 2019, IEEE T MULTIMEDIA, V21, P2603, DOI 10.1109/TMM.2019.2904879
   Yan SJ, 2019, IEEE I CONF COMP VIS, P4393, DOI 10.1109/ICCV.2019.00449
   Yang XH, 2021, IEEE T MULTIMEDIA, V23, P4326, DOI 10.1109/TMM.2020.3040529
   Ye P, 2012, PROC CVPR IEEE, P1098, DOI 10.1109/CVPR.2012.6247789
   Zeng H, 2017, Arxiv, DOI arXiv:1708.08190
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang WX, 2021, IEEE T IMAGE PROCESS, V30, P3474, DOI 10.1109/TIP.2021.3061932
   Zhang WX, 2020, IEEE IMAGE PROC, P111, DOI [10.1109/icip40778.2020.9191278, 10.1109/ICIP40778.2020.9191278]
   Zhang WX, 2020, IEEE T CIRC SYST VID, V30, P36, DOI 10.1109/TCSVT.2018.2886771
   Zhou W, 2020, IEEE T IMAGE PROCESS, V29, P4070, DOI 10.1109/TIP.2020.2969777
   Zhou W, 2019, IEEE T IMAGE PROCESS, V28, P3946, DOI 10.1109/TIP.2019.2902831
   Zhu H., 2021, IEEE T CIRCUITS SYST
NR 78
TC 35
Z9 36
U1 13
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2912
EP 2925
DI 10.1109/TMM.2022.3152942
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA M5LW1
UT WOS:001030640600036
OA Green Submitted
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Wan, RJ
   Shi, BX
   Yang, WH
   Wen, BH
   Duan, LY
   Kot, AC
AF Wan, Renjie
   Shi, Boxin
   Yang, Wenhan
   Wen, Bihan
   Duan, Ling-Yu
   Kot, Alex C.
TI Purifying Low-Light Images via Near-Infrared Enlightened Image
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Exposure correction; image enhancement; low-light image; near-infrared
   light
ID QUALITY
AB Cameras usually produce low-quality images under low-light conditions. Though many methods have been proposed to enhance the visibility of low-light images, they are mainly designed for illumination correction and less capable of suppressing the artifacts. In this paper, we propose to enhance the visibility and suppress artifacts by purifying low-light images under the guidance of the NIR enlightened image captured by using the near-infrared light as compensation. Specifically, we introduce a disentanglement framework to disentangle the structure and color components from the NIR enlightened and RGB images, respectively. Correspondingly, we introduce a new dataset with the RGB and NIR enlightened images for training and evaluation purposes. The experimental results show that our proposed method achieves promising results.
C1 [Wan, Renjie] Hong Kong Baptist Univ, Dept Comp Sci, Hong Kong, Peoples R China.
   [Shi, Boxin; Duan, Ling-Yu] Peking Univ, Sch Comp Sci, Natl Engn Res Ctr Visual Technol, Beijing 100871, Peoples R China.
   [Yang, Wenhan] Peng Cheng Lab, Shenzhen 518066, Guangdong, Peoples R China.
   [Wen, Bihan; Kot, Alex C.] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
C3 Hong Kong Baptist University; Peking University; Peng Cheng Laboratory;
   Nanyang Technological University
RP Shi, BX (corresponding author), Peking Univ, Sch Comp Sci, Natl Engn Res Ctr Visual Technol, Beijing 100871, Peoples R China.
EM renjiewan@comp.hkbu.edu.hk; shiboxin@pku.edu.cn; yangwh@pcl.ac.cn;
   bihan.wen@ntu.edu.sg; lingyu@pku.edu.cn; eackot@ntu.edu.sg
RI Wan, Patrick/AAL-2841-2021; Wen, Bihan/B-3123-2017
OI Wan, Patrick/0000-0002-0161-0367; Wen, Bihan/0000-0002-6874-6453; Kot,
   Alex/0000-0001-6262-8125
FU Blue Sky Research Fund of HKBU [BSRF/21-22/16]; Guangdong Basic and
   Applied Basic Research Foundation [2022A1515110692]; National Natural
   Science Foundation of China [62136001, 62088102]; Ministry of Education,
   Singapore through its Academic Research Fund Tier 1 [RG61/22]; PKU-NTU
   Joint Research Institute
FX The work of Renjie Wan was supported in part by the Blue Sky Research
   Fund of HKBU under Grant BSRF/21-22/16 and in part by Guangdong Basic
   and Applied Basic Research Foundation under Grant 2022A1515110692. This
   work was supported in part by National Natural Science Foundation of
   China under Grants 62136001 and 62088102, in part by the Ministry of
   Education, Singapore through its Academic Research Fund Tier 1 under
   Project RG61/22 and Start-up Grant, and in part by the PKU-NTU Joint
   Research Institute sponsored by a donation from the Ng Teng Fong
   Charitable Foundation.
CR Arjovsky M, 2017, PR MACH LEARN RES, V70
   AXTON, 2019, About us
   Berg A, 2018, IEEE COMPUT SOC CONF, P1224, DOI 10.1109/CVPRW.2018.00159
   Chen C, 2018, PROC CVPR IEEE, P3291, DOI 10.1109/CVPR.2018.00347
   Chen YS, 2018, PROC CVPR IEEE, P6306, DOI 10.1109/CVPR.2018.00660
   Cheng Z, 2019, IEEE I CONF COMP VIS, P2521, DOI 10.1109/ICCV.2019.00261
   Duan CW, 2021, OPTIK, V228, DOI 10.1016/j.ijleo.2020.165775
   Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304
   Galteri L, 2017, IEEE I CONF COMP VIS, P4836, DOI 10.1109/ICCV.2017.517
   Gharbi M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073592
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Guangming Wu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P495, DOI 10.1007/978-3-030-58452-8_29
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Hu YM, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3181974
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462
   Krishnan D, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531402
   LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108
   Lee H, 2020, IEEE SIGNAL PROC LET, V27, P251, DOI 10.1109/LSP.2020.2965824
   Li B., 2022, P IEEE INT C MULT EX, P1
   Li BW, 2022, IEEE T CIRC SYST VID, V32, P5944, DOI 10.1109/TCSVT.2022.3164467
   Li CY, 2022, IEEE T PATTERN ANAL, V44, P4225, DOI 10.1109/TPAMI.2021.3063604
   Li P, 2019, Arxiv, DOI arXiv:1805.11519
   Li Z, 2021, IEEE T MULTIMEDIA, V23, P306, DOI 10.1109/TMM.2020.2978640
   Liu JY, 2021, INT J COMPUT VISION, V129, P1153, DOI 10.1007/s11263-020-01418-8
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Lv FF, 2019, PROC CVPR IEEE, P5980, DOI 10.1109/CVPR.2019.00614
   Lv FF, 2020, AAAI CONF ARTIF INTE, V34, P11725
   Ma JX, 2019, ADV NEUR IN, V32
   Ma JY, 2016, INFORM FUSION, V31, P100, DOI 10.1016/j.inffus.2016.02.001
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Matsui Sosuke, 2011, Information and Media Technologies, V6, P202
   Min XK, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3470970
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P6054, DOI 10.1109/TIP.2020.2988148
   Petschnigg G, 2004, ACM T GRAPHIC, V23, P664, DOI 10.1145/1015706.1015777
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shen XY, 2015, IEEE T PATTERN ANAL, V37, P2518, DOI 10.1109/TPAMI.2015.2417569
   Ulyanov D, 2018, PROC CVPR IEEE, P9446, DOI 10.1109/CVPR.2018.00984
   Wan RJ, 2018, IEEE T IMAGE PROCESS, V27, P2927, DOI 10.1109/TIP.2018.2808768
   Wan RJ, 2016, IEEE IMAGE PROC, P21, DOI 10.1109/ICIP.2016.7532311
   Wang J, 2019, IEEE INT CONF COMPUT, DOI 10.1109/iccphot.2019.8747337
   Wang RX, 2019, PROC CVPR IEEE, P6842, DOI 10.1109/CVPR.2019.00701
   Wang XH, 2019, INT CONF ACOUST SPEE, P3807, DOI [10.1109/icassp.2019.8682692, 10.1109/ICASSP.2019.8682692]
   Wei Cui, 2018, 2018 Photonics North (PN), DOI 10.1109/PN.2018.8438843
   Xu K, 2020, PROC CVPR IEEE, P2278, DOI 10.1109/CVPR42600.2020.00235
   Yan Q, 2013, IEEE I CONF COMP VIS, P1537, DOI 10.1109/ICCV.2013.194
   Yang WH, 2020, PROC CVPR IEEE, P3060, DOI 10.1109/CVPR42600.2020.00313
   Yang X, 2018, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR.2018.00193
   Yu LB, 2012, INT CONF ACOUST SPEE, P1377, DOI 10.1109/ICASSP.2012.6288147
   Yuan Y, 2018, IEEE COMPUT SOC CONF, P814, DOI 10.1109/CVPRW.2018.00113
   Zhai GT, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3457905
   Zhai GT, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2757-1
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang R, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073703
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zhang X, 2018, PROC CVPR IEEE, P4786, DOI 10.1109/CVPR.2018.00503
   Zhang YH, 2021, INT J COMPUT VISION, V129, P1013, DOI 10.1007/s11263-020-01407-x
   Zhang YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1632, DOI 10.1145/3343031.3350926
   Zhang Z., 2021, P IEEE INT C MULT EX, P1
   Zheng Z., 2019, PROC CVPR IEEE, p12 192, DOI DOI 10.1109/CVPR.2019.01247
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhuo SJ, 2010, IEEE IMAGE PROC, P2537, DOI 10.1109/ICIP.2010.5652900
NR 63
TC 4
Z9 4
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8006
EP 8019
DI 10.1109/TMM.2022.3232206
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AV4K5
UT WOS:001121212400030
OA hybrid
DA 2024-07-18
ER

PT J
AU Wen, X
   Zhao, SW
   Wang, HB
   Wu, RZ
   Qu, MH
   Hu, TL
   Chen, G
   Tao, JR
   Fan, CJ
AF Wen, Xiang
   Zhao, Shiwei
   Wang, Haobo
   Wu, Runze
   Qu, Manhu
   Hu, Tianlei
   Chen, Gang
   Tao, Jianrong
   Fan, Changjie
TI Multi-Source Multi-Label Learning for User Profiling in Online Games
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multi-Label learning; user profiling; disentangled latent space; online
   games
AB In online games, user profiling plays a vital role in a variety of personalized services. Current solutions typically treat different dimensions or labels (e.g., willing to pay or not, high, medium, or low appetite for some gameplays) of the full user profiles as independent multi-class/binary classification tasks. However, such a one-by-one profiling strategy clearly overlooks the implicit correlations among profiling tasks, which results in degraded performance. To cope with this issue, we make the first attempt to formalize this problem as a multi-label learning task. Accordingly, we develop a unified Multi-Source Multi-Label learning framework (MSML) that well utilizes semantically rich features and labels for boosted user profiling in online games. Specifically, we first introduce a multi-source user representation network that exploits multi-source data in online games to obtain informative user representations. Subsequently, to handle multiple labels, we propose a novel embedding-based multi-label network that consists of two variational autoencoders with disentangled latent spaces. Note that our framework can guarantee the consistency of the training and testing phases by a novel dual-tower design to overcome the limitation of existing approaches that use one coupled decoder for both features and labels. Extensive experiments on six public multi-label datasets and one real-world online game dataset from Justice demonstrate that the proposed framework outperforms the state-of-the-art baseline methods. Moreover, our proposed framework has been successfully deployed in several online games, yielding a significant boost in multi-label user profiling.
C1 [Wen, Xiang; Wang, Haobo; Hu, Tianlei; Chen, Gang] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Peoples R China.
   [Zhao, Shiwei; Wu, Runze; Qu, Manhu; Tao, Jianrong; Fan, Changjie] Net Ease Games, Fuxi AI Lab, Hangzhou 310000, Peoples R China.
C3 Zhejiang University
RP Wang, HB (corresponding author), Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Peoples R China.; Wu, RZ (corresponding author), Net Ease Games, Fuxi AI Lab, Hangzhou 310000, Peoples R China.
EM wenxiang@zju.edu.cn; zhaoshiwei@corp.netease.com; wanghaobo@zju.edu.cn;
   wurunze1@corp.netease.com; qumanhu@corp.netease.com; htl@zju.edu.cn;
   cg@zju.edu.cn; hztaojianrong@corp.netease.com;
   fanchangjie@corp.netease.com
RI Wang, Haobo/JCF-1064-2023
OI Wang, Haobo/0000-0001-8586-3048; Zhao, Shiwei/0000-0002-1017-5897; wen,
   xiang/0000-0001-6681-1584
FU Key Research and Development Program of Zhejiang Province of China
   [2020C01024]; NSF of China [62050099]; Natural Science Foundation of
   Zhejiang Province of China [LY18F020005]
FX This work was supported in part by the Key Research and Development
   Program of Zhejiang Province of China under Grant 2020C01024, in part by
   the NSF of China under Grant 62050099, and in part by the Natural
   Science Foundation of Zhejiang Province of China under Grant
   LY18F020005.
CR Akbari M, 2017, IEEE T KNOWL DATA EN, V29, P2360, DOI 10.1109/TKDE.2017.2722411
   Bai JW, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4313
   Bhatia K, 2015, 29 ANN C NEURAL INFO, V28
   Bielza C, 2011, INT J APPROX REASON, V52, P705, DOI 10.1016/j.ijar.2011.01.007
   Boutell MR, 2004, PATTERN RECOGN, V37, P1757, DOI 10.1016/j.patcog.2004.03.009
   Chen C, 2019, AAAI CONF ARTIF INTE, P3304
   Chen Yao-Nan, 2012, Advances in neural information processing systems, V25, P1529
   Chen ZM, 2021, IEEE T MULTIMEDIA, V23, P1827, DOI 10.1109/TMM.2020.3003779
   Chiang Tsung- Hsien, 2012, PMLR, P81
   Demsar J, 2006, J MACH LEARN RES, V7, P1
   Eke CI, 2019, IEEE ACCESS, V7, P144907, DOI 10.1109/ACCESS.2019.2944243
   Farnadi G, 2018, WSDM'18: PROCEEDINGS OF THE ELEVENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P171, DOI 10.1145/3159652.3159691
   Farseev A., 2016, ACM SIGWEB Newsletter, V10, P1
   Farseev A, 2017, AAAI CONF ARTIF INTE, P95
   del Río AF, 2019, IEEE CONF COMPU INTE
   Flunger R., 2019, P INT C BIG DAT INN, P133
   Guan WL, 2021, IEEE T CYBERNETICS, V51, P4501, DOI 10.1109/TCYB.2019.2951207
   Huang Y, 2015, IEEE T MULTIMEDIA, V17, P1923, DOI 10.1109/TMM.2015.2476658
   Li JW, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P165
   Li R, 2014, WWW'14: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P819, DOI 10.1145/2566486.2568045
   Li S, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4997
   Liang SS, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1764, DOI 10.1145/3219819.3220043
   Liu WW, 2022, IEEE T PATTERN ANAL, V44, P7955, DOI 10.1109/TPAMI.2021.3119334
   Read J, 2009, LECT NOTES ARTIF INT, V5782, P254, DOI 10.1007/978-3-642-04174-7_17
   Samborskii I., P AAAI C ART INT, V33, P10025
   Schlichtkrull M, 2018, LECT NOTES COMPUT SC, V10843, P593, DOI 10.1007/978-3-319-93417-4_38
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Tai F, 2012, NEURAL COMPUT, V24, P2508, DOI 10.1162/NECO_a_00320
   Tao JR, 2019, PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM '19), P2841, DOI 10.1145/3357384.3357830
   Tao JR, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P811, DOI 10.1145/3219819.3219925
   Tsoumakas G, 2010, DATA MINING AND KNOWLEDGE DISCOVERY HANDBOOK, SECOND EDITION, P667, DOI 10.1007/978-0-387-09823-4_34
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang HB, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2477
   Wang K., 2018, P AS C MACH LEARN, P1
   Wang PY, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P130, DOI 10.1145/3292500.3330869
   Wei LW, 2019, LECT NOTES COMPUT SC, V11536, P548, DOI 10.1007/978-3-030-22734-0_40
   Xie HT, 2015, IEEE CONF COMPU INTE, P230, DOI 10.1109/CIG.2015.7317919
   Yan S, 2020, CIKM '20: PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT, P2877, DOI 10.1145/3340531.3412719
   Yeh CK, 2017, AAAI CONF ARTIF INTE, P2838
   Zhang ML, 2007, PATTERN RECOGN, V40, P2038, DOI 10.1016/j.patcog.2006.12.019
   Zhang YS, 2020, IEEE T MULTIMEDIA, V22, P2844, DOI 10.1109/TMM.2020.2966887
   Zhao S, 2019, PERVASIVE MOB COMPUT, V59, DOI 10.1016/j.pmcj.2019.101052
   Zhao S, 2017, IEEE SYST J, V11, P315, DOI 10.1109/JSYST.2015.2431323
   Zhao SW, 2023, ACM T INFORM SYST, V41, DOI 10.1145/3530012
   Zhao SW, 2020, IEEE CONF COMPU INTE, P104, DOI 10.1109/CoG47356.2020.9231585
   Zhou J, 2020, AI OPEN, V1, P57, DOI 10.1016/j.aiopen.2021.01.001
NR 46
TC 0
Z9 0
U1 5
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4135
EP 4147
DI 10.1109/TMM.2022.3171683
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA W1OJ1
UT WOS:001089390200005
DA 2024-07-18
ER

PT J
AU Yang, XY
   Han, MY
   Luo, Y
   Hu, H
   Wen, YG
AF Yang, Xingyu
   Han, Mengya
   Luo, Yong
   Hu, Han
   Wen, Yonggang
TI Two-Stream Prototype Learning Network for Few-Shot Face Recognition
   Under Occlusions
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Face recognition; Prototypes; Feature extraction; Task analysis; Image
   recognition; Training; Streaming media; Face recognition under
   occlusions; few-shot learning; two-stream; feature alignment; similarity
AB Few-shot face recognition under occlusion (FSFRO) aims to recognize novel subjects given only a few, probably occluded face images, and it is challenging and common in real-world scenarios. Unknown occlusions may deteriorate the class prototypes, while an occluded image in the support set may be critical for recognition if the query image is occluded. This motivates us to propose a novel Two-stream Prototype Learning Network (TSPLN) for FSFR under occlusions by simultaneously considering the quality of support images and their relevance to the query image. Specifically, we design a two-stream architecture, which mainly consists of a support-centered stream and query-centered stream, to learn the optimal class prototypes. The former stream is to reduce the negative impact of occluded images on the prototype. This is achieved by exploring the similarities between different images in the support set. In the query-centered stream, we exploit the relevance between the query and support set based on feature alignment (FA). We conduct extensive experiments on two popular datasets: CASIA-WebFace and RMFRD. The experimental results show that our proposed method achieves the state-of-the-art performance for occluded face recognition in the few-shot setting.
C1 [Yang, Xingyu; Han, Mengya; Luo, Yong] Wuhan Univ, Natl Engn Res Ctr Multimedia Software, Sch Comp Sci, Wuhan 430072, Hubei, Peoples R China.
   [Yang, Xingyu; Han, Mengya; Luo, Yong] Hubei Luojia Lab, Wuhan 430072, Hubei, Peoples R China.
   [Hu, Han] Beijing Inst Technol, Sch Informat & Elect, Beijing 100081, Peoples R China.
   [Wen, Yonggang] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
C3 Wuhan University; Beijing Institute of Technology; Nanyang Technological
   University
RP Luo, Y (corresponding author), Wuhan Univ, Natl Engn Res Ctr Multimedia Software, Sch Comp Sci, Wuhan 430072, Hubei, Peoples R China.; Luo, Y (corresponding author), Hubei Luojia Lab, Wuhan 430072, Hubei, Peoples R China.
EM yangxingyu2021@whu.edu.cn; myhan1996@whu.edu.cn; yluo180@gmail.com;
   hhu@bit.edu.cn; ygwen@ntu.edu.sg
RI Wen, Yonggang/P-9406-2017; Han, Mengya/JCD-9489-2023
OI Wen, Yonggang/0000-0002-2751-5114; Hu, Han/0000-0001-7532-0496; Luo,
   Yong/0000-0002-2296-6370
FU National Key Research and Development Program of China [2021YFC3300200];
   Special Fund of Hubei Luojia Laboratory [220100014]; National Natural
   Science Foundation of China [62276195]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2021YFC3300200, in part by the
   Special Fund of Hubei Luojia Laboratory under Grant 220100014, and in
   part by the National Natural Science Foundation of China under Grant
   62276195.
CR Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339
   Cai CH, 2021, INT C PATT RECOG, P9429, DOI 10.1109/ICPR48806.2021.9412864
   Cen F, 2019, IEEE ACCESS, V7, P26595, DOI 10.1109/ACCESS.2019.2901376
   Chen JJ, 2021, ADV NEUR IN
   Chen YA, 2017, IEEE IMAGE PROC, P1202, DOI 10.1109/ICIP.2017.8296472
   Cheng LL, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1099, DOI 10.1145/2733373.2806291
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Doersch C., 2020, NEURIPS, V33, P21981
   Dong CAQ, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P716
   Finn C, 2017, PR MACH LEARN RES, V70
   Guo KH, 2023, IEEE T MULTIMEDIA, V25, P3894, DOI 10.1109/TMM.2022.3168146
   He LX, 2018, PROC CVPR IEEE, P7054, DOI 10.1109/CVPR.2018.00737
   Holkar A, 2022, IMAGE VISION COMPUT, V120, DOI 10.1016/j.imavis.2022.104420
   Hospedales T, 2022, IEEE T PATTERN ANAL, V44, P5149, DOI 10.1109/TPAMI.2021.3079209
   Hu BW, 2021, IEEE T CYBERNETICS, V51, P4373, DOI 10.1109/TCYB.2020.2995496
   Jinlu Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P741, DOI 10.1007/978-3-030-58452-8_43
   Kang D, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8802, DOI 10.1109/ICCV48922.2021.00870
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lan M, 2023, PATTERN RECOGN, V136, DOI 10.1016/j.patcog.2022.109214
   Lan X, 2023, IEEE T MULTIMEDIA, V25, P1798, DOI 10.1109/TMM.2022.3164798
   Li WB, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2957
   Li WB, 2019, PROC CVPR IEEE, P7253, DOI 10.1109/CVPR.2019.00743
   Liang MJ, 2021, Arxiv, DOI arXiv:1911.12476
   Liu GL, 2018, LECT NOTES COMPUT SC, V11215, P89, DOI 10.1007/978-3-030-01252-6_6
   Nakka KK, 2021, ADV NEUR IN, V34
   Perrett T, 2021, PROC CVPR IEEE, P475, DOI 10.1109/CVPR46437.2021.00054
   Qiu HB, 2022, IEEE T PATTERN ANAL, V44, P6939, DOI 10.1109/TPAMI.2021.3098962
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Snell J, 2017, ADV NEUR IN, V30
   Song LX, 2019, IEEE I CONF COMP VIS, P773, DOI 10.1109/ICCV.2019.00086
   Struc Vitomir, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P1334, DOI 10.1109/ICPR.2010.331
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Wang PF, 2023, IEEE T MULTIMEDIA, V25, P3154, DOI 10.1109/TMM.2022.3156282
   Wang YQ, 2020, ACM COMPUT SURV, V53, DOI 10.1145/3386252
   Wang ZM, 2020, IEEE T NEUR NET LEAR, V31, P2387, DOI 10.1109/TNNLS.2019.2935608
   Wang ZY, 2020, Arxiv, DOI arXiv:2003.09093
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Xie J, 2022, P IEEECVF C COMPUTER, P7972
   Yi D, 2014, Arxiv, DOI arXiv:1411.7923
   Yu TY, 2022, AAAI CONF ARTIF INTE, P3179
   Yu YF, 2017, PATTERN RECOGN, V66, P302, DOI 10.1016/j.patcog.2017.01.021
   Yuan ML, 2021, Arxiv, DOI arXiv:2108.11072
   Zeng D, 2021, IET BIOMETRICS, V10, P581, DOI 10.1049/bme2.12029
   Zhang HG, 2023, IEEE T MULTIMEDIA, V25, P2111, DOI 10.1109/TMM.2022.3142955
   Zhang N, 2023, IEEE T MULTIMEDIA, V25, P3217, DOI 10.1109/TMM.2022.3157036
   Zhao F, 2018, IEEE T IMAGE PROCESS, V27, P778, DOI 10.1109/TIP.2017.2771408
   Zhao YZ, 2023, IEEE T MULTIMEDIA, V25, P3737, DOI 10.1109/TMM.2022.3164785
   Zheng WB, 2021, IEEE COMPUT SOC CONF, P4299, DOI 10.1109/CVPRW53098.2021.00486
   Zheng WB, 2020, NEUROCOMPUTING, V376, P25, DOI 10.1016/j.neucom.2019.09.045
   Zhong X, 2023, IEEE T MULTIMEDIA, V25, P1979, DOI 10.1109/TMM.2022.3141886
   Zhu X., 2021, ARXIV210108085, P249
NR 52
TC 1
Z9 1
U1 3
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1555
EP 1563
DI 10.1109/TMM.2023.3253054
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA G2GW2
UT WOS:000987415000015
DA 2024-07-18
ER

PT J
AU Zhao, AT
   Wang, Y
   Li, JB
AF Zhao, Aite
   Wang, Yue
   Li, Jianbo
TI Transferable Self-Supervised Instance Learning for Sleep Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Sleep recognition; sleep diseases; multimodal data; SleepGAN;
   self-supervised learning; instance learning
ID STAGE CLASSIFICATION; NEURAL-NETWORK; SYSTEM
AB Although the importance of sleep is increasingly recognized, the lack of general and transferable algorithms hinders scalable sleep assessment in healthy persons and those with sleep disorders. A deep understanding of the sleep posture, state, or stage is the premise of diagnosing and treating sleep diseases. At present, most existing methods draw support from supervised learning to monitor the whole sleep process. However, in the absence of sufficient labeled sleep data, it is difficult to guarantee the reliability of sleep recognition networks. To solve this problem, we propose a transferable self-supervised instance learning model for three sleep recognition tasks, i.e., sleep posture, state, and stage recognition. Firstly, a SleepGAN is designed to generate sleep data, and then, we combine enough self-supervised rotating sleep data and original data for non-parametric classification at the instance-level, finally, different sleep postures, states, or stages can be distinguished precisely. The proposed model can be applied to multimodal sleep data such as signals and images, and makeup for the inaccuracy caused by insufficient data, and can be transferred to sleep datasets of different sizes. The experimental results show that our algorithm for the physiological changes in the sleep process is superior to several state-of-the-art studies, which may be helpful to promote the intelligence of sleep assessment and monitoring.
C1 [Zhao, Aite; Wang, Yue; Li, Jianbo] Qingdao Univ, Coll Comp Sci & Technol, Qingdao 266071, Peoples R China.
C3 Qingdao University
RP Zhao, AT (corresponding author), Qingdao Univ, Coll Comp Sci & Technol, Qingdao 266071, Peoples R China.
EM zhaoaite@qdu.edu.cn; 2020025817@qdu.edu.cn; lijianbo@qdu.edu.cn
OI Zhao, Aite/0000-0003-3494-175X
FU National Natural Science Foundation of China [62106117]; Natural Science
   Foundation of Shandong Province [ZR2021QF084]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62106117, and in part by the Natural
   Science Foundation of Shandong Province under Grant ZR2021QF084.
CR Banluesombatkul N, 2021, IEEE J BIOMED HEALTH, V25, P1949, DOI 10.1109/JBHI.2020.3037693
   Bin Heyat MB, 2021, CURR DRUG TARGETS, V22, P672, DOI 10.2174/1389450121666201027125828
   Boly M, 2012, P NATL ACAD SCI USA, V109, P5856, DOI 10.1073/pnas.1111133109
   Boostani R, 2017, COMPUT METH PROG BIO, V140, P77, DOI 10.1016/j.cmpb.2016.12.004
   Cao Y., 2020, P ADV NEUR INF PROC
   Chang SY, 2019, IEEE T CIRCUITS-I, V66, P3504, DOI 10.1109/TCSI.2019.2927839
   Chen ZH, 2021, IEEE J BIOMED HEALTH, V25, P3270, DOI 10.1109/JBHI.2020.3006145
   Dafna E, 2012, IEEE ENG MED BIO, P3660, DOI 10.1109/EMBC.2012.6346760
   EL-Manzalawy Y, 2017, IEEE INT C BIOINFORM, P718, DOI 10.1109/BIBM.2017.8217742
   Eldele E, 2021, Arxiv, DOI arXiv:2106.14112
   Eldele E, 2021, IEEE T NEUR SYS REH, V29, P809, DOI 10.1109/TNSRE.2021.3076234
   Enayati M, 2018, IEEE ENG MED BIO, P461, DOI 10.1109/EMBC.2018.8512436
   Ergen T, 2018, IEEE T NEUR NET LEAR, V29, P5159, DOI 10.1109/TNNLS.2017.2770179
   Erik B., 2018, Front. Comput. Neurosci.
   Fang C, 2018, INT J NEURAL SYST, V28, DOI 10.1142/S012906571850017X
   Feng KC, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3017246
   Ghasemzadeh P, 2019, APPL SOFT COMPUT, V75, P523, DOI 10.1016/j.asoc.2018.11.007
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Goyal P, 2019, IEEE I CONF COMP VIS, P6400, DOI 10.1109/ICCV.2019.00649
   Günes S, 2010, EXPERT SYST APPL, V37, P7922, DOI 10.1016/j.eswa.2010.04.043
   Gutmann Michael, 2010, P MACHINE LEARNING R, P297, DOI DOI 10.1145/3292500.3330651
   Hanaoka M, 2001, P ANN INT IEEE EMBS, V23, P1751, DOI 10.1109/IEMBS.2001.1020556
   Hu XX, 2018, IEEE 2018 INTERNATIONAL CONGRESS ON CYBERMATICS / 2018 IEEE CONFERENCES ON INTERNET OF THINGS, GREEN COMPUTING AND COMMUNICATIONS, CYBER, PHYSICAL AND SOCIAL COMPUTING, SMART DATA, BLOCKCHAIN, COMPUTER AND INFORMATION TECHNOLOGY, P1817, DOI 10.1109/Cybermatics_2018.2018.00302
   Phan H, 2019, IEEE T BIO-MED ENG, V66, P1285, DOI 10.1109/TBME.2018.2872652
   Jia ZY, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1324
   Jiang DH, 2019, EXPERT SYST APPL, V121, P188, DOI 10.1016/j.eswa.2018.12.023
   Jo HG, 2010, COMPUT BIOL MED, V40, P629, DOI 10.1016/j.compbiomed.2010.04.007
   Lee H., 2019, P INT C LEARN REPR, P1
   Li YY, 2018, PROC INT WORKSH ADV
   Mansfield S, 2020, IEEE REV BIOMED ENG, V13, P352, DOI 10.1109/RBME.2019.2927200
   Mikolov Tomas, 2013, Advances in Neural Information Processing Systems, P3111, DOI DOI 10.48550/ARXIV.1310.4546
   Mohammadi SM, 2018, IEEE ENG MED BIO, P3501, DOI 10.1109/EMBC.2018.8513009
   Mohsin N, 2016, 7TH IEEE ANNUAL INFORMATION TECHNOLOGY, ELECTRONICS & MOBILE COMMUNICATION CONFERENCE IEEE IEMCON-2016
   Morin F, 2005, P AISTATS, V5, P246
   Mulaffer L, 2017, IEEE ENG MED BIO, P3749, DOI 10.1109/EMBC.2017.8037672
   Odena A, 2017, PR MACH LEARN RES, V70
   Ostadabbas S, 2014, BIOMED CIRC SYST C, P133, DOI 10.1109/BioCAS.2014.6981663
   PARIKH N., 2014, FDN TRENDS OPTIM, V1, P3, DOI DOI 10.1561/2400000003
   Patti C. R., 2015, P 37 ANN INT C IEEE
   Pouyan MB, 2017, 2017 IEEE EMBS INTERNATIONAL CONFERENCE ON BIOMEDICAL & HEALTH INFORMATICS (BHI), P65, DOI 10.1109/BHI.2017.7897206
   Radha M, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-49703-y
   Roebuck A, 2014, PHYSIOL MEAS, V35, pR1, DOI 10.1088/0967-3334/35/1/R1
   Sasidharan A., 2014, International Journal of Clinical and Experimental Physiology, V1, P3
   Scullin MK, 2014, J NEUROL SCI, V336, P243, DOI 10.1016/j.jns.2013.09.009
   Sisodia DS, 2017, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON INVENTIVE COMPUTING AND INFORMATICS (ICICI 2017), P1011, DOI 10.1109/ICICI.2017.8365290
   Sun CL, 2019, IEEE ACCESS, V7, P109386, DOI 10.1109/ACCESS.2019.2933814
   Tang KS, 2021, IOT-BASEL, V2, P119, DOI 10.3390/iot2010007
   Tataraidze A, 2016, IEEE ENG MED BIO, P2839, DOI 10.1109/EMBC.2016.7591321
   Torres C, 2018, IEEE T MULTIMEDIA, V20, P3057, DOI 10.1109/TMM.2018.2829162
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Walch O, 2019, SLEEP, V42, DOI 10.1093/sleep/zsz180
   Walsh L, 2017, IEEE T BIO-MED ENG, V64, P1750, DOI 10.1109/TBME.2016.2621066
   Xiaowei Xu, 2015, 2015 IEEE Biomedical Circuits and Systems Conference (BioCAS), P1, DOI 10.1109/BioCAS.2015.7348281
   Xu XW, 2016, IEEE T BIOMED CIRC S, V10, P1023, DOI 10.1109/TBCAS.2016.2543686
   Xue JH, 2008, NEURAL PROCESS LETT, V28, P169, DOI 10.1007/s11063-008-9088-7
   Yang CQ, 2022, Arxiv, DOI arXiv:2110.15278
   Yang C, 2017, IEEE T MULTIMEDIA, V19, P822, DOI 10.1109/TMM.2016.2626969
   Yang JL, 2016, IEEE ENG MED BIO, P2843, DOI 10.1109/EMBC.2016.7591322
   Ye M, 2019, PROC CVPR IEEE, P6203, DOI 10.1109/CVPR.2019.00637
   Zhang D., 2021, P ADV NEUR INF PROC, P12236
   Zhang DW, 2020, IEEE T PATTERN ANAL, V42, P475, DOI 10.1109/TPAMI.2018.2881114
   Zhang JM, 2018, COMPUT METH PROG BIO, V164, P181, DOI 10.1016/j.cmpb.2018.07.015
   Zhang YZ, 2019, IEEE CONF COMPUT, P443, DOI [10.1109/infcomw.2019.8845115, 10.1109/INFCOMW.2019.8845115]
   Zhang YX, 2023, IEEE T KNOWL DATA EN, V35, P2118, DOI 10.1109/TKDE.2021.3102110
   Zhao AT, 2022, IEEE T MULTIMEDIA, V24, P846, DOI 10.1109/TMM.2021.3060280
   Zhao AT, 2020, IEEE ACCESS, V8, P93907, DOI 10.1109/ACCESS.2020.2994593
NR 66
TC 10
Z9 10
U1 5
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4464
EP 4477
DI 10.1109/TMM.2022.3176751
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA W1OJ1
UT WOS:001089390200028
DA 2024-07-18
ER

PT J
AU Zhao, ZC
   Hu, HM
   Zhang, HD
   Chen, F
   Guo, Q
AF Zhao, Zichen
   Hu, Hai-Miao
   Zhang, Hongda
   Chen, Fei
   Guo, Qiang
TI Improving Color Constancy Using Chromaticity-Line Prior
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Lighting; Image color analysis; IEEE Sections; Benchmark testing;
   Approximation algorithms; Adaptation models; Training; Color constancy;
   chromaticity-line prior; illumination estimation; ill-posed problem
ID ILLUMINATION ESTIMATION; MODEL
AB Color constancy is the ability to remove the effect of illumination on color. Since color constancy is an ill-posed problem, many methods have been proposed based on assumptions to constraint the solution space. However, most existing assumptions require specular pixels or abundant colors, and fail to produce satisfactory results for different scenarios. According to extensive experiments, we observe that the chromaticity distribution of pixels within main color under canonical illumination, which we called canonical pixels, is linear and can also locate the position of illumination under the non-canonical illumination. Therefore, this paper proposes a chromaticity-line prior (CLP) as an additional linear constraint on the ill-posed problem of color constancy. In the calculation of CLP, the simple linear iterative clustering is firstly employed to segment an image into several super-pixel blocks. And the random sampling consensus is utilized to remove non-primary color points and fit the chromaticity-line. Based on the proposed CLP, a color constancy algorithm is implemented correspondingly. Since the main idea of the CLP is to extract the canonical pixels, which is the inherent property of image, the proposed CLP is more general and adaptive in real scenes. The experiments on two public datasets demonstrate that the proposed algorithm not only outperforms state-of-the-art learning-free algorithms, but also achieves results that are competitive to those of learning-based algorithms.
C1 [Zhao, Zichen; Hu, Hai-Miao; Zhang, Hongda; Guo, Qiang] Beihang Univ, Beijing Key Lab Digital Media, Sch Comp Sci & Engn, Beijing 100191, Peoples R China.
   [Hu, Hai-Miao] Beihang Univ, Hangzhou Innovat Inst, Beijing 100191, Peoples R China.
   [Chen, Fei] Sichuan Intelligent Expressway Technol Co LTD, Chengdu 610020, Peoples R China.
C3 Beihang University; Beihang University
RP Hu, HM (corresponding author), Beihang Univ, Beijing Key Lab Digital Media, Sch Comp Sci & Engn, Beijing 100191, Peoples R China.
EM 2558542623@qq.com; frank0139@163.com; zhd739156725@vip.qq.com;
   2685300854@qq.com; 1263836618@qq.com
FU National Key Research andDevelopment Program [2020AAA0130200]; National
   Natural Science Foundation of China [62122011, U21A20514]; Key Research
   and Development Program of Zhejiang Province [2022C01082]; Fundamental
   Research Funds for the Central Universities
FX This work was supported in part by the National Key Research
   andDevelopment Program underGrant 2020AAA0130200, in part by the
   National Natural Science Foundation of China under Grants 62122011 and
   U21A20514, in part by the Key Research and Development Program of
   Zhejiang Province under Grant 2022C01082, and in part by the Fundamental
   Research Funds for the Central Universities. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Alexandros (Alexis) Michael Tourapis.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Akbarinia A, 2018, IEEE T PATTERN ANAL, V40, P2081, DOI 10.1109/TPAMI.2017.2753239
   [Anonymous], 2000, Re-processed version of the gehler color constancy dataset of 568 images
   Aytekin C, 2018, IEEE T IMAGE PROCESS, V27, P530, DOI 10.1109/TIP.2017.2764264
   Barnard K, 2002, IEEE T IMAGE PROCESS, V11, P972, DOI 10.1109/TIP.2002.802531
   Barron JT, 2017, PROC CVPR IEEE, P6950, DOI 10.1109/CVPR.2017.735
   Barron JT, 2015, IEEE I CONF COMP VIS, P379, DOI 10.1109/ICCV.2015.51
   Bianco S, 2010, PATTERN RECOGN, V43, P695, DOI 10.1016/j.patcog.2009.08.007
   Bianco S, 2008, IEEE T IMAGE PROCESS, V17, P2381, DOI 10.1109/TIP.2008.2006661
   Bianco S, 2019, PROC CVPR IEEE, P12204, DOI 10.1109/CVPR.2019.01249
   BUCHSBAUM G, 1980, J FRANKLIN I, V310, P1, DOI 10.1016/0016-0032(80)90058-7
   Chakrabarti A., 2015, NeurIPS
   Chakrabarti A, 2012, IEEE T PATTERN ANAL, V34, P1509, DOI 10.1109/TPAMI.2011.252
   Cheng DL, 2015, PROC CVPR IEEE, P1000, DOI 10.1109/CVPR.2015.7298702
   Cheng DL, 2014, J OPT SOC AM A, V31, P1049, DOI 10.1364/JOSAA.31.001049
   Codruta AO, 2020, IEEE T IMAGE PROCESS, V29, P2653, DOI 10.1109/TIP.2019.2951304
   Dong L, 2017, FRONT COMPUT SCI-CHI, V11, P1023, DOI 10.1007/s11704-016-5538-y
   Finlayson GD, 2004, 12TH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, APPLICATIONS, P37
   Finlayson GD, 2013, IEEE I CONF COMP VIS, P1904, DOI 10.1109/ICCV.2013.239
   Finlayson GD, 2017, IEEE T PATTERN ANAL, V39, P1482, DOI 10.1109/TPAMI.2016.2582171
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Foster DH, 2011, VISION RES, V51, P674, DOI 10.1016/j.visres.2010.09.006
   Funt B, 2004, 12TH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, APPLICATIONS, P47
   Funt B., 1998, 5 EUROPEAN C COMPUTE, P445
   Gao SB, 2017, J OPT SOC AM A, V34, P1448, DOI 10.1364/JOSAA.34.001448
   Gao SB, 2015, IEEE T PATTERN ANAL, V37, P1973, DOI 10.1109/TPAMI.2015.2396053
   Gao SB, 2014, LECT NOTES COMPUT SC, V8690, P158, DOI 10.1007/978-3-319-10605-2_11
   Gao YY, 2018, IEEE T MULTIMEDIA, V20, P335, DOI 10.1109/TMM.2017.2740025
   Gijsenij A, 2011, IEEE T IMAGE PROCESS, V20, P2475, DOI 10.1109/TIP.2011.2118224
   Gijsenij A, 2011, IEEE T PATTERN ANAL, V33, P687, DOI 10.1109/TPAMI.2010.93
   Gijsenij A, 2010, INT J COMPUT VISION, V86, P127, DOI 10.1007/s11263-008-0171-3
   Han XJ, 2020, IEEE T MULTIMEDIA, V22, P1619, DOI 10.1109/TMM.2019.2945197
   Hordley SD, 2006, J OPT SOC AM A, V23, P1008, DOI 10.1364/JOSAA.23.001008
   Hu HM, 2020, IEEE T MULTIMEDIA, V22, P1485, DOI 10.1109/TMM.2019.2944260
   Hu YM, 2017, PROC CVPR IEEE, P330, DOI 10.1109/CVPR.2017.43
   Huang XW, 2020, IEEE T IMAGE PROCESS, V29, P7875, DOI 10.1109/TIP.2020.3007823
   LEE HC, 1986, J OPT SOC AM A, V3, P1694, DOI 10.1364/JOSAA.3.001694
   Lehmann TM, 2001, J OPT SOC AM A, V18, P2679, DOI 10.1364/JOSAA.18.002679
   Li B, 2016, INT J COMPUT VISION, V117, P21, DOI 10.1007/s11263-015-0844-7
   Li B, 2014, IEEE T IMAGE PROCESS, V23, P1194, DOI 10.1109/TIP.2013.2277943
   Lu R, 2009, IEEE I CONF COMP VIS, P1749, DOI 10.1109/ICCV.2009.5459391
   Mackiewicz M, 2016, J OPT SOC AM A, V33, P2166, DOI 10.1364/JOSAA.33.002166
   Mazin B, 2015, IEEE T IMAGE PROCESS, V24, P1944, DOI 10.1109/TIP.2015.2405414
   Moriyama D, 2018, IEEE IMAGE PROC, P2262, DOI 10.1109/ICIP.2018.8451423
   Oh SW, 2017, PATTERN RECOGN, V61, P405, DOI 10.1016/j.patcog.2016.08.013
   Pei SC, 2017, IEEE T MULTIMEDIA, V19, P1956, DOI 10.1109/TMM.2017.2688924
   Qian YL, 2019, VISAPP: PROCEEDINGS OF THE 14TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS, VOL 4, P36, DOI 10.5220/0007406900360046
   Qian YL, 2017, IEEE I CONF COMP VIS, P5459, DOI 10.1109/ICCV.2017.582
   Rohmer K, 2017, IEEE T VIS COMPUT GR, V23, P2474, DOI 10.1109/TVCG.2017.2734426
   ROSENBERG C, 2003, P ADV NEUR INF PROC, P1595
   SHAFER SA, 1985, COLOR RES APPL, V10, P210, DOI 10.1002/col.5080100409
   Shi W, 2016, LECT NOTES COMPUT SC, V9908, P371, DOI 10.1007/978-3-319-46493-0_23
   SIDOROV O, 2019, P IEEE CVF C COMPUT
   Su ZY, 2020, FRONT COMPUT SCI-CHI, V14, P417, DOI 10.1007/s11704-018-8116-1
   TAN TT, 2003, P IEEE COMPUT SOC C, P673
   Van de Weijer J, 2007, IEEE T IMAGE PROCESS, V16, P2207, DOI 10.1109/TIP.2007.901808
   Wan YL, 2014, IEEE T MULTIMEDIA, V16, P637, DOI 10.1109/TMM.2014.2299515
   Wang BS, 2020, IEEE T MULTIMEDIA, V22, P2221, DOI 10.1109/TMM.2019.2954752
   Woo SM, 2018, IEEE T IMAGE PROCESS, V27, P1862, DOI 10.1109/TIP.2017.2785290
   Xiao J, 2020, PROC CVPR IEEE, P3255, DOI 10.1109/CVPR42600.2020.00332
   Yang KF, 2015, PROC CVPR IEEE, P2254, DOI 10.1109/CVPR.2015.7298838
   Yang X, 2018, OPT EXPRESS, V26, P29055, DOI 10.1364/OE.26.029055
NR 62
TC 3
Z9 3
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3642
EP 3656
DI 10.1109/TMM.2022.3163457
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FE2H4
UT WOS:001144015500010
DA 2024-07-18
ER

PT J
AU Gao, P
   Zhang, PW
   Smolic, A
AF Gao, Pan
   Zhang, Pengwei
   Smolic, Aljosa
TI Quality Assessment for Omnidirectional Video: A Spatio-Temporal
   Distortion Modeling Approach
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Measurement; Distortion; Quality assessment; Video recording; Two
   dimensional displays; Visualization; Distortion measurement; Objective
   video quality assessment; omnidirectional video; spatio-temporal
   distortion
ID STRUCTURAL SIMILARITY; PERCEPTUAL QUALITY
AB Omnidirectional video, also known as 360-degree video, has become increasingly popular nowadays due to its ability to provide immersive and interactive visual experiences. However, the ultra high resolution and the spherical observation space brought by the large spherical viewing range make omnidirectional video distinctly different from traditional 2D video. To date, the video quality assessment (VQA) for omnidirectional video is still an open issue. The existing VQA metrics for omnidirectional video only consider the spatial characteristics of distortions, but the temporal change of spatial distortions can also considerably influence human visual perception. In this paper, we propose a spatiotemporal modeling approach to evaluate the quality of the omnidirectional video. Firstly, we construct a spatioral quality assessment unit to evaluate the average distortion in temporal dimension at the eye fixation level, based upon which the smoothed distortion value is recursively calculated and consolidated by the characteristics of temporal variations. Then, we give a detailed solution of how to to integrate the three existing spatial VQA metrics into our approach. Besides, the cross-format omnidirectional video distortion measurement is also investigated. Finally, the spatiotemporal distortion of the whole video sequence is obtained by pooling. Based on the modeling approach, a full reference objective quality assessment metric for omnidirectional video is derived, namely OV-PSNR. The experimental results show that our proposed OV-PSNR greatly improves the prediction performance of the existing VQA metrics for omnidirectional video.
C1 [Gao, Pan; Zhang, Pengwei] Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 211106, Peoples R China.
   [Gao, Pan; Zhang, Pengwei] Nanjing Univ Aeronaut & Astronaut, MIIT Key Lab Pattern Anal & Machine Intelligence, Nanjing 211106, Peoples R China.
   [Smolic, Aljosa] Trinity Coll Dublin, Sch Comp Sci & Stat, Dublin D02 PN40 2, Ireland.
C3 Nanjing University of Aeronautics & Astronautics; Nanjing University of
   Aeronautics & Astronautics; Trinity College Dublin
RP Gao, P; Zhang, PW (corresponding author), Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 211106, Peoples R China.; Gao, P; Zhang, PW (corresponding author), Nanjing Univ Aeronaut & Astronaut, MIIT Key Lab Pattern Anal & Machine Intelligence, Nanjing 211106, Peoples R China.
EM gaopan.1005@gmail.com; zhang.pw@nuaa.edu.cn; smolica@scss.tcd.ie
FU Natural Science Foundation of China [61 701 227]; Natural Science
   Foundation of Jiangsu Province of China [BK20170806]; Science Foundation
   Ireland (SFI) [15/RP/2776]
FX This work was supported in part by the Natural Science Foundation of
   China under Grant 61 701 227, and in part by the Natural Science
   Foundation of Jiangsu Province of China under Grant BK20170806, and in
   part by a research grant from Science Foundation Ireland (SFI) under
   Grant 15/RP/2776.
CR Barkowsky M, 2009, IEEE J-STSP, V3, P266, DOI 10.1109/JSTSP.2009.2015375
   BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007
   Boyce J., 2017, JVETH1030
   Croci S., 2020, QUAL USER EXPER, V5, P1, DOI 10.1007/s41233-020-00032-3
   Croci S, 2019, INT WORK QUAL MULTIM
   Culibrk D, 2011, IEEE T IMAGE PROCESS, V20, P948, DOI 10.1109/TIP.2010.2080279
   Engelke U, 2011, IEEE SIGNAL PROC MAG, V28, P50, DOI 10.1109/MSP.2011.942473
   Fu CW, 2009, IEEE T MULTIMEDIA, V11, P634, DOI 10.1109/TMM.2009.2017626
   Hoffman J. E, 1998, Attention, P119
   Itti L, 2009, VISION RES, V49, P1295, DOI 10.1016/j.visres.2008.09.007
   Lambrecht CJV, 1999, IEEE T CIRC SYST VID, V9, P766, DOI 10.1109/76.780365
   Lambrecht CJV, 1996, P SOC PHOTO-OPT INS, V2668, P450, DOI 10.1117/12.235440
   Lambrecht CJV, 1996, INT CONF ACOUST SPEE, P2291, DOI 10.1109/ICASSP.1996.547739
   Li C, 2019, PROC CVPR IEEE, P10169, DOI 10.1109/CVPR.2019.01042
   Li C, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P932, DOI 10.1145/3240508.3240581
   LI RX, 1994, IEEE T CIRC SYST VID, V4, P438, DOI 10.1109/76.313138
   Lim HT, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P6737, DOI 10.1109/ICASSP.2018.8461317
   Lubin Jeffrey, 1993, P163
   Mahadevan V, 2010, IEEE T PATTERN ANAL, V32, P171, DOI 10.1109/TPAMI.2009.112
   Masry M, 2006, IEEE T CIRC SYST VID, V16, P260, DOI 10.1109/TCSVT.2005.861946
   Masry MA, 2004, SIGNAL PROCESS-IMAGE, V19, P133, DOI 10.1016/j.image.2003.08.001
   Narwaria M, 2012, IEEE T MULTIMEDIA, V14, P525, DOI 10.1109/TMM.2012.2190589
   Nasrabadi AT, 2014, IEEE INT SYM BROADB
   Ninassi A, 2009, IEEE J-STSP, V3, P253, DOI 10.1109/JSTSP.2009.2014806
   Orduna M, 2020, IEEE T CONSUM ELECTR, V66, P22, DOI 10.1109/TCE.2019.2957987
   Pinson MH, 2004, IEEE T BROADCAST, V50, P312, DOI 10.1109/TBC.2004.834028
   Rohaly A. M., 2000, SG09 ITUT, P1
   Seshadrinathan K, 2007, INT CONF ACOUST SPEE, P869
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P1427, DOI 10.1109/TIP.2010.2042111
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P335, DOI 10.1109/TIP.2009.2034992
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Sun YL, 2017, IEEE SIGNAL PROC LET, V24, P1408, DOI 10.1109/LSP.2017.2720693
   Tan KT, 1998, SIGNAL PROCESS, V70, P279, DOI 10.1016/S0165-1684(98)00129-7
   Tan TK, 2016, IEEE T CIRC SYST VID, V26, P76, DOI 10.1109/TCSVT.2015.2477916
   Usman MA, 2018, IEEE T MULTIMEDIA, V20, P2344, DOI 10.1109/TMM.2018.2801722
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, SIGNAL PROCESS-IMAGE, V19, P121, DOI 10.1016/S0923-5965(03)00076-6
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2007, J OPT SOC AM A, V24, pB61, DOI 10.1364/JOSAA.24.000B61
   Watson AB, 2001, J ELECTRON IMAGING, V10, P20, DOI 10.1117/1.1329896
   Winkler S, 1999, PROC SPIE, V3644, P175, DOI 10.1117/12.348438
   Xiu XY, 2017, 2017 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)
   Xu M, 2019, IEEE T CIRC SYST VID, V29, P3516, DOI [10.1109/TCSVT.2018.2886277, 10.1080/17445302.2018.1558727]
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Yang SY, 2017, PROC INT CONF EDU IN, P1, DOI 10.1109/EITT.2017.9
   Ye Y., 2017, M REP JOINT VID EXP, P1
   Yu M, 2015, 2015 IEEE International Symposium on Mixed and Augmented Reality, P31, DOI 10.1109/ISMAR.2015.12
   Yuen M, 1998, SIGNAL PROCESS, V70, P247, DOI 10.1016/S0165-1684(98)00128-5
   Zakharchenko V, 2016, PROC SPIE, V9970, DOI 10.1117/12.2235885
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang P, 2019, PROC CVPR IEEE, P12077, DOI 10.1109/CVPR.2019.01236
NR 51
TC 0
Z9 0
U1 1
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1
EP 16
DI 10.1109/TMM.2020.3044458
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YK9LD
UT WOS:000745524300001
OA Green Published
DA 2024-07-18
ER

PT J
AU Sun, MJ
   Wang, SW
   Zhang, P
   Liu, XW
   Guo, XF
   Zhou, SH
   Zhu, E
AF Sun, Mengjing
   Wang, Siwei
   Zhang, Pei
   Liu, Xinwang
   Guo, Xifeng
   Zhou, Sihang
   Zhu, En
TI Projective Multiple Kernel Subspace Clustering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Kernel; Optimization; Clustering algorithms; Integrated circuits;
   Hilbert space; Redundancy; Clustering methods; Kernel clustering;
   multiple kernel subspace clustering; multi-view information fusion
AB Multiple kernel subspace clustering (MKSC), as an important extension for handling multi-view non-linear subspace data, has shown notable success in a wide variety of machine learning tasks. The key objective of MKSC is to build a flexible and appropriate graph for clustering from the kernel space. However, existing MKSC methods apply a mechanism utilizing the kernel trick to the traditional self-expressive principle, where the similarity graphs are built on the respective high-dimensional (or even infinite) reproducing kernel Hilbert space (RKHS). Regarding this strategy, we argue that the original high-dimensional spaces usually include noise and unreliable similarity measures and, therefore, output a low-quality graph matrix, which degrades clustering performance. In this paper, inspired by projective clustering, we propose the utilization of a complementary similarity graph by fusing the multiple kernel graphs constructed in the low-dimensional partition space, termed projective multiple kernel subspace clustering (PMKSC). By incorporating intrinsic structures with multi-view data, PMKSC alleviates the noise and redundancy in the original kernel space and obtains high-quality similarity to uncover the underlying clustering structures. Furthermore, we design a three-step alternate algorithm with proven convergence to solve the proposed optimization problem. The experimental results on ten multiple kernel benchmark datasets validate the effectiveness of our proposed PMKSC, compared to the state-of-the-art multiple kernel and kernel subspace clustering methods, by a large margin. Our code is available at https://github.com/MengjingSun/PMKSC-code.
C1 [Sun, Mengjing; Wang, Siwei; Zhang, Pei; Liu, Xinwang; Guo, Xifeng; Zhu, En] Natl Univ Def Technol, Sch Comp, Changsha 410073, Peoples R China.
   [Zhou, Sihang] Natl Univ Def Technol, Coll Intelligence Sci & Technol, Changsha 410073, Peoples R China.
C3 National University of Defense Technology - China; National University
   of Defense Technology - China
RP Liu, XW (corresponding author), Natl Univ Def Technol, Sch Comp, Changsha 410073, Peoples R China.
EM sun_mj@nudt.edu.cn; wangsiwei13@nudt.edu.cn; jeaninerpp@gmail.com;
   xinwangliu@nudt.edu.cn; guoxifeng1990@163.com; sihangjoe@gmail.com;
   enzhu@nudt.edu.cn
RI Wang, Siwei/ABD-1733-2021; LIU, Xinwang/L-8089-2019
OI Wang, Siwei/0000-0001-9517-262X; LIU, Xinwang/0000-0001-9066-1475;
   Zhang, Pei/0000-0002-3018-5080; Guo, Xifeng/0000-0002-3556-1516; Zhou,
   Sihang/0000-0003-1491-4594
FU National Key R&D Program of China [2020AAA0107100]; National Natural
   Science Foundation ofChina [61922088, 61906020]
FX This work was supported in part by the National Key R&D Program of China
   under Project 2020AAA0107100, and in part by the National Natural
   Science Foundation ofChina under Projects 61922088 and 61906020.
CR Bezdek J. C., 2003, Neural, Parallel & Scientific Computations, V11, P351
   Cao XC, 2015, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2015.7298657
   Chen YY, 2020, IEEE T MULTIMEDIA, V22, P1985, DOI 10.1109/TMM.2019.2952984
   Du L, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3476
   Elhamifar Ehsan, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2790, DOI 10.1109/CVPRW.2009.5206547
   Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57
   Fu YW, 2010, IEEE T MULTIMEDIA, V12, P717, DOI 10.1109/TMM.2010.2052025
   Gao HC, 2015, IEEE I CONF COMP VIS, P4238, DOI 10.1109/ICCV.2015.482
   Han Y., 2016, IEEE T NEUR NET LEAR, V29, P486
   Hu H, 2014, PROC CVPR IEEE, P3834, DOI 10.1109/CVPR.2014.484
   Huang B, 2020, IEEE T MULTIMEDIA, V22, P2820, DOI 10.1109/TMM.2020.2965482
   Huang HC, 2012, IEEE T FUZZY SYST, V20, P120, DOI 10.1109/TFUZZ.2011.2170175
   Jegelka S, 2009, LECT NOTES ARTIF INT, V5803, P144, DOI 10.1007/978-3-642-04617-9_19
   Kumar P., 2011, Adv. Neural Inf. Process. Syst., P1413, DOI DOI 10.5555/2986459.2986617
   Li M., 2016, P 25 INT JOINT C ART, P1704
   Li RH, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2916
   Li T, 2018, NONNEGATIVE MATRIX F, P149
   Liang WX, 2022, IEEE T KNOWL DATA EN, V34, P3418, DOI 10.1109/TKDE.2020.3025100
   Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88
   Liu XW, 2017, AAAI CONF ARTIF INTE, P2266
   Liu XW, 2016, AAAI CONF ARTIF INTE, P1888
   Ou QY, 2020, IEEE MULTIMEDIA, V27, P91, DOI 10.1109/MMUL.2020.3020169
   Peng X, 2016, IEEE T NEUR NET LEAR, V27, P2499, DOI 10.1109/TNNLS.2015.2490080
   Qi G.-J., 2013, Proceedings of the sixth ACM international conference on Web search and data mining, P617
   Qi G.-J., 2012, Proc. of the 5th ACM Intl. Conf. on Web Search and Data Mining (WSDM), P553, DOI DOI 10.1145/2124295.2124363
   Qi G.-J., 2009, P ACM INT C MULT, P243
   Qi GJ, 2012, IEEE T PATTERN ANAL, V34, P850, DOI 10.1109/TPAMI.2011.191
   Ren ZW, 2021, IEEE T NEUR NET LEAR, V32, P1839, DOI 10.1109/TNNLS.2020.2991366
   Tan JP, 2021, IEEE T MULTIMEDIA, V23, P2943, DOI 10.1109/TMM.2020.3019683
   Tang C, 2019, IEEE T MULTIMEDIA, V21, P2837, DOI 10.1109/TMM.2019.2909860
   Tang C, 2019, IEEE T MULTIMEDIA, V21, P1724, DOI 10.1109/TMM.2018.2889560
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vidal R, 2011, IEEE SIGNAL PROC MAG, V28, P52, DOI 10.1109/MSP.2010.939739
   Wang B., 2020, IEEE T MULTIMEDIA, V23, P216
   Wang HB, 2021, IEEE T MULTIMEDIA, V23, P3828, DOI 10.1109/TMM.2020.3032023
   Wang QQ, 2021, IEEE T MULTIMEDIA, V23, P3483, DOI 10.1109/TMM.2020.3025666
   Wang SP, 2017, IEEE T MULTIMEDIA, V19, P1454, DOI 10.1109/TMM.2017.2663324
   Wang SW, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3778
   Wang Y, 2018, NEURAL NETWORKS, V103, P1, DOI 10.1016/j.neunet.2018.03.006
   Wang Y, 2017, IEEE T NEUR NET LEAR, V28, P57, DOI 10.1109/TNNLS.2015.2498149
   Wang Y, 2015, IEEE T IMAGE PROCESS, V24, P3939, DOI 10.1109/TIP.2015.2457339
   Wen J, 2021, IEEE T MULTIMEDIA, V23, P2493, DOI 10.1109/TMM.2020.3013408
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2149
   Yang SJ, 2019, IEEE T MULTIMEDIA, V21, P2916, DOI 10.1109/TMM.2019.2912735
   Zhang CQ, 2020, IEEE T PATTERN ANAL, V42, P86, DOI 10.1109/TPAMI.2018.2877660
   Zhang CQ, 2017, PROC CVPR IEEE, P4333, DOI 10.1109/CVPR.2017.461
   Zhang CQ, 2015, IEEE I CONF COMP VIS, P1582, DOI 10.1109/ICCV.2015.185
   Zhang GY, 2020, KNOWL-BASED SYST, V189, DOI 10.1016/j.knosys.2019.105126
   Zhang P, 2022, IEEE T KNOWL DATA EN, V34, P4676, DOI 10.1109/TKDE.2020.3045770
   Zhou SH, 2020, INFORM FUSION, V53, P145, DOI 10.1016/j.inffus.2019.06.017
   Zhou SH, 2020, IEEE T NEUR NET LEAR, V31, P1351, DOI 10.1109/TNNLS.2019.2919900
   Zhu ZL, 2014, IEEE T MULTIMEDIA, V16, P2178, DOI 10.1109/TMM.2014.2364976
NR 52
TC 30
Z9 31
U1 3
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2567
EP 2579
DI 10.1109/TMM.2021.3086727
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 1F3XL
UT WOS:000795103700002
DA 2024-07-18
ER

PT J
AU Xu, XZ
   Deng, J
   Cummins, N
   Zhang, ZX
   Zhao, L
   Schuller, BW
AF Xu, Xinzhou
   Deng, Jun
   Cummins, Nicholas
   Zhang, Zixing
   Zhao, Li
   Schuller, Bjoern W.
TI Exploring Zero-Shot Emotion Recognition in Speech Using
   Semantic-Embedding Prototypes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Prototypes; Emotion recognition; Speech recognition; Annotations;
   Predictive models; Training; Electronic mail; Speech emotion
   recognition; paralinguistics; zero-shot learning; semantic-embedding
   prototypes
ID FRAMEWORK; MACHINE
AB Speech Emotion Recognition (SER) makes it possible for machines to perceive affective information. Our previous research differed from conventional SER endeavours in that it focused on recognising unseen emotions in speech autonomously through machine learning. Such a step would enable the automatic leaning of unknown emerging emotional states. This type of learning framework, however, still relied on manual annotations to obtain multiple samples of each emotion. In order to reduce this additional workload, herein, we propose a zero-shot SER framework employing a per-emotion semantic-embedding paradigm to describe emotions in zero-shot SER, instead of using the sample-wise descriptors. Aiming to optimise the relationship between emotions, prototypes, and speech samples, this framework includes two types of learning strategies: Sample-wise learning and emotion-wise learning. These strategies apply a novel learning process to speech samples and emotions, respectively, via specifically designed semantic-embedding prototypes. We verify the utility of these approaches by performing an extensive experimental evaluation on two corpora on three aspects, namely the influence of different types of learning strategies, emotional-pair comparison, and the selections of semantic-embedding prototypes and paralinguistic features. The experimental results indicate that it is applicable to use semantic-embedding prototypes for zero-shot emotion recognition in speech, despite the influence of choosing optimal strategies and prototypes.
C1 [Xu, Xinzhou] Nanjing Univ Posts & Telecommun, Sch Internet Things, Nanjing 210023, Peoples R China.
   [Xu, Xinzhou; Cummins, Nicholas; Schuller, Bjoern W.] Univ Augsburg, Chair Embedded Intelligence Hlth Care & Wellbeing, D-86159 Augsburg, Germany.
   [Deng, Jun] Agile Robots AG, D-81477 Munich, Germany.
   [Cummins, Nicholas] Kings Coll London, Inst Psychiat Psychol & Neurosci, Dept Biostat & Hlth informat, London SE5 8AF, England.
   [Zhang, Zixing; Schuller, Bjoern W.] Imperial Coll London, GLAM Grp Language Audio & Music, London SW7 2AZ, England.
   [Zhao, Li] Southeast Univ, Sch Informat Sci & Engn, Nanjing 210096, Peoples R China.
C3 Nanjing University of Posts & Telecommunications; University of
   Augsburg; University of London; King's College London; Imperial College
   London; Southeast University - China
RP Xu, XZ (corresponding author), Nanjing Univ Posts & Telecommun, Sch Internet Things, Nanjing 210023, Peoples R China.; Xu, XZ (corresponding author), Univ Augsburg, Chair Embedded Intelligence Hlth Care & Wellbeing, D-86159 Augsburg, Germany.
EM xinzhou.xu@njupt.edu.cn; jun.deng@tum.de; nicholas.cummins@ieee.org;
   zixing.zhang@tum.de; zhaoli@seu.edu.cn; schuller@ieee.org
RI Schuller, Björn Wolfgang/D-3241-2011; Deng, Jun/X-6902-2019
OI Schuller, Björn Wolfgang/0000-0002-6478-8699; Deng,
   Jun/0000-0002-2425-7244; Cummins, Nicholas/0000-0002-1178-917X
FU Natural Science Foundation of China [61801241, 61673108, 61801236];
   Natural Science Foundation for Jiangsu Higher Education Institutions
   [18KJB510029]; Natural Science Foundation of Jiangsu [BK20180746];
   NUPTSF [NY217149]; European Union [826506]
FX This work was supported in part by the Natural Science Foundation of
   China under Grants 61801241, 61673108, and 61801236, in part by the
   Natural Science Foundation for Jiangsu Higher Education Institutions
   under Grant 18KJB510029, in part by the Natural Science Foundation of
   Jiangsu under Grant BK20180746, in part by the NUPTSF under Grant
   NY217149, and in part by the European Union's Horizon 2020 research and
   innovation programme under Grant 826506 (sustAGE).
CR Abdelwahab M, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P5084, DOI 10.1109/ICASSP.2018.8461866
   Akata Z, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2487986
   Annadani Y, 2018, PROC CVPR IEEE, P7603, DOI 10.1109/CVPR.2018.00793
   [Anonymous], 2009, Advances in neural information processing systems
   [Anonymous], 2013, INT C LEARN REPRESEN, DOI 10.1109/TCBB.2021.3077905
   [Anonymous], 2017, IEEE T NEUR SYS REH, DOI DOI 10.1109/TNSRE.2017.2721116
   Banziger T., 2010, Blueprint for affective computing: A sourcebook, P271, DOI DOI 10.1037/A0025827
   Bojanowski P., 2017, Transactions of the Association for Computational Linguistics, V5, P135, DOI DOI 10.1162/TACLA00051
   Busso C, 2013, IEEE T AFFECT COMPUT, V4, P386, DOI 10.1109/T-AFFC.2013.26
   Cambria E, 2018, AAAI CONF ARTIF INTE, P1795
   Cambria Erik., 2016, COLING
   Campos V, 2019, COMPUT VIS PATT REC, P349, DOI 10.1016/B978-0-12-814601-9.00018-3
   Changpinyo S, 2020, INT J COMPUT VISION, V128, P166, DOI 10.1007/s11263-019-01193-1
   Changpinyo S, 2017, IEEE I CONF COMP VIS, P3496, DOI 10.1109/ICCV.2017.376
   Changpinyo S, 2016, PROC CVPR IEEE, P5327, DOI 10.1109/CVPR.2016.575
   Cummins N, 2020, IEEE T AFFECT COMPUT, V11, P272, DOI 10.1109/TAFFC.2017.2766145
   Deng J, 2018, IEEE-ACM T AUDIO SPE, V26, P31, DOI 10.1109/TASLP.2017.2759338
   Deng J, 2017, IEEE SIGNAL PROC LET, V24, P500, DOI 10.1109/LSP.2017.2672753
   Devillers L, 2005, NEURAL NETWORKS, V18, P407, DOI 10.1016/j.neunet.2005.03.007
   Le D, 2017, INTERSPEECH, P1108, DOI 10.21437/Interspeech.2017-94
   EKMAN P, 1992, PSYCHOL REV, V99, P550, DOI 10.1037/0033-295X.99.3.550
   Eyben F, 2016, IEEE T AFFECT COMPUT, V7, P190, DOI 10.1109/TAFFC.2015.2457417
   Eyben Florian, 2015, ACM SIGMultimedia Records, V6, P4
   Fu YW, 2018, IEEE SIGNAL PROC MAG, V35, P112, DOI 10.1109/MSP.2017.2763441
   Fu ZY, 2018, IEEE T PATTERN ANAL, V40, P2009, DOI 10.1109/TPAMI.2017.2737007
   Guo YC, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4898
   Han J, 2021, IEEE T AFFECT COMPUT, V12, P553, DOI 10.1109/TAFFC.2019.2928297
   Hussain, 2015, SENTIC COMPUTING, P23, DOI DOI 10.1007/978-3-319-23654-4_2
   Kampffmeyer M, 2019, PROC CVPR IEEE, P11479, DOI 10.1109/CVPR.2019.01175
   Kodirov E, 2017, PROC CVPR IEEE, P4447, DOI 10.1109/CVPR.2017.473
   Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140
   Li J., 2020, ACM MM, P1348
   Li JJ, 2021, IEEE T PATTERN ANAL, V43, P3918, DOI 10.1109/TPAMI.2020.2991050
   Li JJ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1587, DOI 10.1145/3343031.3350901
   Li JJ, 2019, PROC CVPR IEEE, P7394, DOI 10.1109/CVPR.2019.00758
   Li JJ, 2019, IEEE T IMAGE PROCESS, V28, P6103, DOI 10.1109/TIP.2019.2924174
   Liu Y, 2020, NEURAL NETWORKS, V121, P1, DOI 10.1016/j.neunet.2019.08.023
   Lotfian R, 2018, INTERSPEECH, P951, DOI 10.21437/Interspeech.2018-2464
   Luo CZ, 2018, IEEE T IMAGE PROCESS, V27, P637, DOI 10.1109/TIP.2017.2745109
   Mallol-Ragolta A, 2019, INTERSPEECH, P221, DOI 10.21437/Interspeech.2019-2036
   Mao QR, 2017, SPEECH COMMUN, V93, P1, DOI 10.1016/j.specom.2017.06.006
   Mao QR, 2014, IEEE T MULTIMEDIA, V16, P2203, DOI 10.1109/TMM.2014.2360798
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P3111, DOI DOI 10.5555/2999792.2999959
   Mikolov T, 2018, PROCEEDINGS OF THE ELEVENTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION (LREC 2018), P52
   Nelson NL, 2018, COGNITIVE DEV, V47, P117, DOI 10.1016/j.cogdev.2018.04.006
   Norouzi M, 2014, INT C LEARN REPRESEN
   Parada-Cabaleiro E, 2020, LANG RESOUR EVAL, V54, P341, DOI 10.1007/s10579-019-09450-y
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Romera-Paredes B, 2015, PR MACH LEARN RES, V37, P2152
   Schnall A., 2014, P ANN C INT SPEECH C, P2640
   Schönfeld E, 2019, PROC CVPR IEEE, P8239, DOI 10.1109/CVPR.2019.00844
   Schuller B. W., 2020, P INTERSPEECH, P2042
   Schuller B, 2019, COMPUT SPEECH LANG, V53, P156, DOI 10.1016/j.csl.2018.02.004
   Schuller B, 2013, INTERSPEECH, P148
   Schuller BW, 2018, COMMUN ACM, V61, P90, DOI 10.1145/3129340
   Schuller BW, 2018, INTERSPEECH, P122
   SHAVER P, 1987, J PERS SOC PSYCHOL, V52, P1061, DOI 10.1037//0022-3514.52.6.1061
   Socher R., 2013, ADV NEURAL INFORM PR, V26, P935, DOI DOI 10.1007/978-3-319-46478-7
   Song P, 2019, IEEE T AFFECT COMPUT, V10, P265, DOI 10.1109/TAFFC.2017.2705696
   Tzirakis P, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P5089, DOI 10.1109/ICASSP.2018.8462677
   Wang W, 2019, ACM T INTEL SYST TEC, V10, DOI 10.1145/3293318
   Wang XL, 2018, PROC CVPR IEEE, P6857, DOI 10.1109/CVPR.2018.00717
   Watson D, 2017, EMOT REV, V9, P99, DOI 10.1177/1754073916639659
   Xia R, 2017, IEEE T AFFECT COMPUT, V8, P3, DOI 10.1109/TAFFC.2015.2512598
   Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581
   Xian YQ, 2019, IEEE T PATTERN ANAL, V41, P2251, DOI 10.1109/TPAMI.2018.2857768
   Xian YQ, 2016, PROC CVPR IEEE, P69, DOI 10.1109/CVPR.2016.15
   Xu BH, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P15, DOI 10.1145/2911996.2912006
   Xu XZ, 2019, INTERSPEECH, P949, DOI 10.21437/Interspeech.2019-2406
   Xu XZ, 2019, IEEE T MULTIMEDIA, V21, P795, DOI 10.1109/TMM.2018.2865834
   Xu XZ, 2017, IEEE-ACM T AUDIO SPE, V25, P1436, DOI 10.1109/TASLP.2017.2694704
   Zhan C, 2019, IEEE I CONF COMP VIS, P1151, DOI 10.1109/ICCV.2019.00124
   Zhang BQ, 2019, IEEE T AFFECT COMPUT, V10, P85, DOI 10.1109/TAFFC.2017.2684799
   Zhang L, 2017, PROC CVPR IEEE, P3010, DOI 10.1109/CVPR.2017.321
   Zhang SQ, 2018, IEEE T MULTIMEDIA, V20, P1576, DOI 10.1109/TMM.2017.2766843
   Zhang WJ, 2020, IEEE-ACM T AUDIO SPE, V28, P307, DOI 10.1109/TASLP.2019.2955252
   Zhang Y, 2016, INTERSPEECH, P2041, DOI 10.21437/Interspeech.2016-1305
   Zhang ZM, 2015, IEEE I CONF COMP VIS, P4166, DOI 10.1109/ICCV.2015.474
   Zhang ZX, 2019, IEEE T MULTIMEDIA, V21, P1289, DOI 10.1109/TMM.2018.2871949
   Zhao ZP, 2019, INTERSPEECH, P206, DOI 10.21437/Interspeech.2019-1649
NR 80
TC 12
Z9 13
U1 7
U2 43
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2752
EP 2765
DI 10.1109/TMM.2021.3087098
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2A3LX
UT WOS:000809408000005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yang, Y
   Guan, JW
   Huang, SY
   Wan, WG
   Xu, YT
   Liu, JX
AF Yang, Yong
   Guan, Juwei
   Huang, Shuying
   Wan, Weiguo
   Xu, Yating
   Liu, Jiaxiang
TI End-to-End Rain Removal Network Based on Progressive Residual Detail
   Supplement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Rain; Logic gates; Task analysis; Feature extraction; Visualization;
   History; Frequency-domain analysis; Rain removal; progressive network;
   diamond residual block; detail supplement
ID SINGLE; INFORMATION; VISION
AB Methods of rain removal based on deep learning have rapidly developed, and the image quality after rain removal is continuously improving. However, the results of most methods have some common problems, including a loss of details, a blurring of edges, and the existence of artifacts. To remove rain-related information more thoroughly and retain more edge details, this paper proposes an end-to-end rain removal network based on the progressive residual detail supplement (ERRN-PRDS) approach. The entire network structure is designed in an iterative manner to obtain higher-quality rain removal images from coarse to fine. In the network, a diamond residual block is constructed as the main module of iteration to learn the feature information of the background layer. Meanwhile, to keep more texture details in the background layer, a detail supplement mechanism is designed between the iterative layers to transfer more information to the next iterative operation. Experimental results show that this method can remove the rain information more completely and better retain the image edges compared with previous state-of-the-art methods. In addition, because of the sparsity of the detail injection, our network also achieves high-quality results for image denoising tasks.
C1 [Yang, Yong; Guan, Juwei; Liu, Jiaxiang] Jiangxi Univ Finance & Econ, Sch Informat Technol, Nanchang 330032, Jiangxi, Peoples R China.
   [Huang, Shuying] Tiangong Univ, Sch Comp Sci & Technol, Tianjin 300387, Peoples R China.
   [Wan, Weiguo; Xu, Yating] Jiangxi Univ Finance & Econ, Sch Software & Internet Things Engn, Nanchang 330032, Jiangxi, Peoples R China.
C3 Jiangxi University of Finance & Economics; Tiangong University; Jiangxi
   University of Finance & Economics
RP Huang, SY (corresponding author), Tiangong Univ, Sch Comp Sci & Technol, Tianjin 300387, Peoples R China.
EM greatyangy@126.com; jvguan@163.com; shuyinghuang2010@126.com;
   wanwgplus@163.com; xuytde@163.com; forworkliu@gmail.com
OI Liu, Jiaxiang/0000-0003-1764-0322; Guan, Juwei/0000-0001-6337-1253; Wan,
   Weiguo/0000-0002-3537-979X; Huang, Shuying/0000-0003-2771-8461
FU National Natural Science Foundation of China [62072218, 61862030];
   Natural Science Foundation of Jiangxi Province [20192ACB20002,
   20192ACBL21008]; Talent Project of Jiangxi Thousand Talents Program
   [jxsq2019201056]
FX Thisworkwas supported in part by theNationalNatural Science Foundation
   of China under Grants 62072218 and 61862030, in part by the Natural
   Science Foundation of Jiangxi Province under Grants 20192ACB20002 and
   20192ACBL21008, and in part by the Talent Project of Jiangxi Thousand
   Talents Program underGrant jxsq2019201056. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Shaoen Wu.
CR [Anonymous], 2014, INT C MACH LEARN ICM
   Barnes P. M., 2008, Complementary and Alternative Medicine Use Among Adults and Children: United States, 2007, DOI DOI 10.1037/E623942009-001
   De-An Huang, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P164, DOI 10.1109/ICME.2012.92
   Fan ZW, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1751, DOI 10.1145/3240508.3240694
   Fu XY, 2020, IEEE T NEUR NET LEAR, V31, P1794, DOI 10.1109/TNNLS.2019.2926481
   Fu XY, 2017, PROC CVPR IEEE, P1715, DOI 10.1109/CVPR.2017.186
   Fu XY, 2017, IEEE T IMAGE PROCESS, V26, P2944, DOI 10.1109/TIP.2017.2691802
   Fu YH, 2011, INT CONF ACOUST SPEE, P1453
   Garg K, 2004, PROC CVPR IEEE, P528
   Garg K, 2007, INT J COMPUT VISION, V75, P3, DOI 10.1007/s11263-006-0028-6
   Gu JX, 2018, PATTERN RECOGN, V77, P354, DOI 10.1016/j.patcog.2017.10.013
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He ZW, 2020, IEEE T MULTIMEDIA, V22, P1042, DOI 10.1109/TMM.2019.2937688
   Kang LW, 2012, IEEE T IMAGE PROCESS, V21, P1742, DOI 10.1109/TIP.2011.2179057
   Kovesi P., 1999, Fifth International/National Biennial Conference on Digital Image Computing, Techniques, and Applications. DICTA99, P212
   Kovesi P, 2000, PSYCHOL RES-PSYCH FO, V64, P136, DOI 10.1007/s004260000024
   Kriegeskorte N, 2015, ANNU REV VIS SCI, V1, P417, DOI 10.1146/annurev-vision-082114-035447
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li CY, 2020, IEEE T MULTIMEDIA, V22, P704, DOI 10.1109/TMM.2019.2933334
   Li X, 2018, LECT NOTES COMPUT SC, V11211, P262, DOI 10.1007/978-3-030-01234-2_16
   Li Y, 2016, PROC CVPR IEEE, P2736, DOI 10.1109/CVPR.2016.299
   Lin CY, 2020, IEEE T IMAGE PROCESS, V29, P9250, DOI 10.1109/TIP.2020.3025402
   Liu XH, 2019, IEEE I CONF COMP VIS, P7313, DOI 10.1109/ICCV.2019.00741
   Nayar S. K., 2003, Photometric model of a rain drop
   Qian R, 2018, PROC CVPR IEEE, P2482, DOI 10.1109/CVPR.2018.00263
   Ren DW, 2020, IEEE T IMAGE PROCESS, V29, P6852, DOI 10.1109/TIP.2020.2994443
   Ren DW, 2019, PROC CVPR IEEE, P3932, DOI 10.1109/CVPR.2019.00406
   Santhaseelan V, 2015, INT J COMPUT VISION, V112, P71, DOI 10.1007/s11263-014-0759-8
   Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683
   Shi XJ, 2015, ADV NEUR IN, V28
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Tripathi AK, 2014, SIGNAL IMAGE VIDEO P, V8, P1421, DOI 10.1007/s11760-012-0373-6
   Wei Y, ARXIV191207015V3
   Xue XW, 2012, IEEE INT WORKSH MULT, P170, DOI 10.1109/MMSP.2012.6343435
   Yang WH, 2020, IEEE T PATTERN ANAL, V42, P1377, DOI 10.1109/TPAMI.2019.2895793
   Yang WH, 2017, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2017.183
   Yu F., 2015, ARXIV
   Yu J., 2018, P IEEE C COMPUTER VI
   Zhang H, 2020, IEEE T CIRC SYST VID, V30, P3943, DOI 10.1109/TCSVT.2019.2920407
   Zhang H, 2018, PROC CVPR IEEE, P695, DOI 10.1109/CVPR.2018.00079
   Zhang XP, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P461, DOI 10.1109/ICME.2006.262572
NR 41
TC 11
Z9 11
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1622
EP 1636
DI 10.1109/TMM.2021.3068833
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0D8GN
UT WOS:000776227200029
DA 2024-07-18
ER

PT J
AU Zhao, YY
   Dong, MZ
   Wang, YJ
   Feng, D
   Lv, Q
   Dick, RP
   Li, DS
   Lu, T
   Gu, N
   Shang, L
AF Zhao, Yingying
   Dong, Mingzhi
   Wang, Yujiang
   Feng, Da
   Lv, Qin
   Dick, Robert P.
   Li, Dongsheng
   Lu, Tun
   Gu, Ning
   Shang, Li
TI A Reinforcement-Learning-Based Energy-Efficient Framework for Multi-Task
   Video Analytics Pipeline
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Visual analytics; Energy resolution; Streaming media;
   Pipelines; Object detection; Computer vision; energy-efficient; vision;
   multi-task application; reinforcement learning
AB Deep-learning-based video processing has yielded transformative results in recent years. However, the video analytics pipeline is energy-intensive due to high data rates and reliance on complex inference algorithms, which limits its adoption in energy-constrained applications. Motivated by the observation of high and variable spatial redundancy and temporal dynamics in video data streams, we design and evaluate an adaptive-resolution optimization framework to minimize the energy use of multi-task video analytics pipelines. Instead of heuristically tuning the input data resolution of individual tasks, our framework utilizes deep reinforcement learning to dynamically govern the input resolution and computation of the entire video analytics pipeline. By monitoring the impact of varying resolution on the quality of high-dimensional video analytics features, hence the accuracy of video analytics results, the proposed end-to-end optimization framework learns the best non-myopic policy for dynamically controlling the resolution of input video streams to globally optimize energy efficiency. Governed by reinforcement learning, optical flow is incorporated into the framework to minimize unnecessary spatio-temporal redundancy that leads to re-computation, while preserving accuracy. The proposed framework is applied to video instance segmentation which is one of the most challenging computer vision tasks, and achieves better energy efficiency than all baseline methods of similar accuracy on the YouTube-VIS dataset.
C1 [Zhao, Yingying; Dong, Mingzhi; Li, Dongsheng; Lu, Tun; Gu, Ning; Shang, Li] Fudan Univ, Sch Comp Sci, Shanghai 200438, Peoples R China.
   [Zhao, Yingying; Dong, Mingzhi; Lu, Tun; Gu, Ning; Shang, Li] Fudan Univ, Shanghai Key Lab Data Sci, Shanghai 200438, Peoples R China.
   [Wang, Yujiang] Imperial Coll London, Dept Comp, London SW7 2BX, England.
   [Feng, Da] Alibaba Beijing Software Serv Co Ltd, Beijing 100102, Peoples R China.
   [Lv, Qin] Univ Colorado, Boulder, CO 80309 USA.
   [Dick, Robert P.] Univ Michigan, Dept Elect Engn & Comp Sci, Coll Engn, Ann Arbor, MI 48109 USA.
   [Li, Dongsheng] Microsoft Res Asia, Shanghai 200232, Peoples R China.
C3 Fudan University; Fudan University; Imperial College London; University
   of Colorado System; University of Colorado Boulder; University of
   Michigan System; University of Michigan; Microsoft; Microsoft Research
   Asia
RP Wang, YJ (corresponding author), Imperial Coll London, Dept Comp, London SW7 2BX, England.
EM yingyingzhao@fudan.edu.cn; mingzhidong@gmail.com;
   yujiang.wang14@imperial.ac.uk; fengda.fd@alibaba-inc.com;
   qin.lv@colorado.edu; dickrp@umich.edu; dongshengli@fudan.edu.cn;
   lutun@fudan.edu.cn; ninggu@fudan.edu.cn; lishang@fudan.edu.cn
RI zhao, ying/ISA-2502-2023; liu, xq/JDW-2596-2023; Zhao,
   Yingying/KBB-1613-2024; Dick, Robert P/B-7137-2009
OI Li, Dongsheng/0000-0003-3103-8442; Wang, Yujiang/0000-0002-6220-029X;
   DINH, MINH-CHAU/0000-0003-4004-5986; Zhao, Yingying/0000-0001-5902-1306
FU National Natural Science Foundation of China [62090025, 61932007];
   National Science Foundation of the United States [CNS-2008151]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62090025 and 61932007, and in part by
   the National Science Foundation of the United States under Grant
   CNS-2008151. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Chang-Su Kim.
CR Dick RP, 2020, IEEE DES TEST, V37, P7, DOI 10.1109/MDAT.2019.2957352
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Kingma D. P., 2014, arXiv
   Kulkarni P., 2005, 13th Annual ACM International Conference on Multimedia, P229, DOI 10.1145/1101149.1101191
   LiKamWa Robert, 2013, P 11 ANN INT C MOB S, P69
   Lubana ES, 2019, IEEE DATA COMPR CONF, P478, DOI 10.1109/DCC.2019.00056
   Lubana ES, 2018, IEEE T COMPUT AID D, V37, P2371, DOI 10.1109/TCAD.2018.2858340
   Luo BN, 2020, IEEE WINT CONF APPL, P1941, DOI 10.1109/WACV45572.2020.9093483
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Qiurui Wang, 2019, IEEE Transactions on Multimedia, V21, P1839, DOI 10.1109/TMM.2018.2890360
   Radu Valentin, 2017, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, V1, DOI 10.1145/3161174
   Reif Stefan, 2020, e-Energy '20: Proceedings of the Eleventh ACM International Conference on Future Energy Systems, P548, DOI 10.1145/3396851.3402924
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shen J., 2020, ARXIV200603708
   van Hasselt H, 2016, AAAI CONF ARTIF INTE, P2094
   Wang YJ, 2020, PROC CVPR IEEE, P6957, DOI 10.1109/CVPR42600.2020.00699
   Wang YJ, 2019, INT J COMPUT VISION, V127, P625, DOI 10.1007/s11263-018-1130-2
   Wang ZH, 2018, P I MECH ENG I-J SYS, V232, P417, DOI 10.1177/0959651817721404
   Xianglong Feng, 2019, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, V3, DOI 10.1145/3328914
   Xu YS, 2018, PROC CVPR IEEE, P6556, DOI 10.1109/CVPR.2018.00686
   Yang LJ, 2019, IEEE I CONF COMP VIS, P5187, DOI 10.1109/ICCV.2019.00529
   Yang S, 2016, PROC CVPR IEEE, P5525, DOI 10.1109/CVPR.2016.596
   Zhang SF, 2017, IEEE I CONF COMP VIS, P192, DOI 10.1109/ICCV.2017.30
   Zhu XZ, 2017, PROC CVPR IEEE, P4141, DOI 10.1109/CVPR.2017.441
NR 27
TC 3
Z9 3
U1 2
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2150
EP 2163
DI 10.1109/TMM.2021.3076612
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0H8DA
UT WOS:000778959200029
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yang, S
   Wang, YW
   Chen, K
   Zeng, W
   Fei, ZS
AF Yang, Shu
   Wang, Yaowei
   Chen, Ke
   Zeng, Wei
   Fei, Zesong
TI Attribute-Aware Feature Encoding for Object Recognition and Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Correlation; Semantics; Training data; Benchmark testing; Multitasking;
   Feature extraction; Encoding; Object recognition; segmentation;
   attribute learning; feature encoding; regularization
ID PERSON REIDENTIFICATION; CLOTHING RETRIEVAL; IMAGE; CLASSIFICATION;
   INFORMATION; SHAPE
AB Existing multi-task models for object recognition and segmentation have verified the effectiveness of joint optimization of two semantic tasks. However, learning discriminative representations with insufficient training data and redundant contextual information from the background remains challenging. Semantic attributes are designed as powerful and informative mid-level features that 1) share information across categories to model the interclass correlation and that 2) can be localized in the object region to benefit foreground extraction. This paper introduces a novel attribute-aware feature encoding (AFE) module to a multi-task network for object recognition and segmentation with the aim of improving both semantic tasks by regularizing feature encoding with auxiliary attribute learning. Intuitively, attribute learning in our method not only provides extra supervision signals to capture interclass correlation in object classification but also refines the output of object segmentation via weakly supervised attribute localization. The experimental results on two public benchmarks show that our method yields remarkable improvement in both semantic tasks and auxiliary attribute estimation over existing methods.
C1 [Yang, Shu; Fei, Zesong] Beijing Inst Technol, Sch Informat & Elect, Beijing 100081, Peoples R China.
   [Wang, Yaowei] Peng Cheng Lab, Shenzhen 518000, Peoples R China.
   [Chen, Ke] South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510641, Peoples R China.
   [Zeng, Wei] Peking Univ, Sch Elect Engn & Comp Sci, Beijing 100871, Peoples R China.
C3 Beijing Institute of Technology; Peng Cheng Laboratory; South China
   University of Technology; Peking University
RP Fei, ZS (corresponding author), Beijing Inst Technol, Sch Informat & Elect, Beijing 100081, Peoples R China.; Wang, YW (corresponding author), Peng Cheng Lab, Shenzhen 518000, Peoples R China.
EM yangshu91@bit.edu.cn; wangyw@pcl.ac.cn; chenk@scut.edu.cn;
   weizeng@pku.edu.cn; feizesong@bit.edu.cn
OI Yang, Shu/0000-0003-1587-0474
FU National Natural Science Foundation of China [U20B2052]; Program for
   Guangdong Introducing Innovative and Enterpreneurial Teams
   [2017ZT07X183]; Fundamental Research Funds for the Central Universities
   [2019MS022]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant U20B2052, in part by the Program for
   Guangdong Introducing Innovative and Enterpreneurial Teams under Grant
   2017ZT07X183, and in part by the Fundamental Research Funds for the
   Central Universities under Grant 2019MS022.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Akata Z, 2013, PROC CVPR IEEE, P819, DOI 10.1109/CVPR.2013.111
   [Anonymous], 2011, P CVPR WORKSH FIN GR
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Berg AC, 2005, PROC CVPR IEEE, P26
   Boiman O, 2008, PROC CVPR IEEE, P1992, DOI 10.1109/CVPR.2008.4587598
   Bosch A, 2007, IEEE I CONF COMP VIS, P1863
   Boykov Y, 2006, INT J COMPUT VISION, V70, P109, DOI 10.1007/s11263-006-7934-5
   Chandra S, 2016, LECT NOTES COMPUT SC, V9911, P402, DOI 10.1007/978-3-319-46478-7_25
   Chen JM, 2021, COMPUT VIS IMAGE UND, V206, DOI 10.1016/j.cviu.2021.103184
   Chen L, 2019, MULTIMED TOOLS APPL, V78, P4381, DOI 10.1007/s11042-018-5875-y
   Chen L, 2011, IEEE T SYST MAN CY B, V41, P1263, DOI 10.1109/TSMCB.2011.2124455
   Chen ZX, 2018, IEEE T MULTIMEDIA, V20, P2126, DOI 10.1109/TMM.2017.2785253
   Choi MJ, 2012, IEEE T PATTERN ANAL, V34, P240, DOI 10.1109/TPAMI.2011.119
   Chuang KS, 2006, COMPUT MED IMAG GRAP, V30, P9, DOI 10.1016/j.compmedimag.2005.10.001
   Dai JF, 2016, PROC CVPR IEEE, P3150, DOI 10.1109/CVPR.2016.343
   Dalal N., 2005, PROC IEEE COMPUT SOC, V1, P886
   Demirel B, 2017, IEEE I CONF COMP VIS, P1241, DOI 10.1109/ICCV.2017.139
   Evgeniou T., 2004, P 10 ACM SIGKDD INT, P109, DOI DOI 10.1145/1014052.1014067
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772
   Fergus R, 2003, PROC CVPR IEEE, P264
   Fu C.-Y., 2019, ARXIV190103353
   Gavves E, 2015, INT J COMPUT VISION, V111, P191, DOI 10.1007/s11263-014-0741-5
   Han K, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P2040, DOI 10.1145/3240508.3240550
   Han Shou-Dong, 2011, Acta Automatica Sinica, V37, P11, DOI 10.3724/SP.J.1004.2011.00011
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Kaiming, 2017, P IEEE INT C COMPUTE
   Huang C, 2017, IEEE T MULTIMEDIA, V19, P673, DOI 10.1109/TMM.2016.2631122
   Huang ZJ, 2019, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2019.00657
   Jayaraman D, 2014, ADV NEUR IN, V27
   Krause J, 2015, PROC CVPR IEEE, P5546, DOI 10.1109/CVPR.2015.7299194
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694
   Kumar A., 2012, P 29 INT C MACH LEAR, P1383
   Kumar MP, 2015, IEEE T PATTERN ANAL, V37, P1373, DOI 10.1109/TPAMI.2014.2372766
   Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140
   Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594
   Li WK, 2018, CHIN AUTOM CONGR, P4173, DOI 10.1109/CAC.2018.8623687
   Li Y, 2017, PROC CVPR IEEE, P4438, DOI 10.1109/CVPR.2017.472
   Li ZY, 2014, LECT NOTES COMPUT SC, V8694, P350, DOI 10.1007/978-3-319-10599-4_23
   Liang KM, 2015, IEEE I CONF COMP VIS, P2506, DOI 10.1109/ICCV.2015.288
   Liang XD, 2018, IEEE T PATTERN ANAL, V40, P2978, DOI 10.1109/TPAMI.2017.2775623
   Lin D, 2022, IEEE T NEUR NET LEAR, V33, P200, DOI 10.1109/TNNLS.2020.3027603
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin YT, 2019, PATTERN RECOGN, V95, P151, DOI 10.1016/j.patcog.2019.06.006
   Liu HM, 2017, PROC CVPR IEEE, P6259, DOI 10.1109/CVPR.2017.663
   Liu S, 2016, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2016.342
   Liu Tie, 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383047
   Liu X, 2017, AAAI CONF ARTIF INTE, P4190
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Mostajahi M, 2015, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2015.7298959
   Nowak E, 2006, LECT NOTES COMPUT SC, V3954, P490
   Patterson G, 2016, LECT NOTES COMPUT SC, V9910, P85, DOI 10.1007/978-3-319-46466-4_6
   Qian N, 1999, NEURAL NETWORKS, V12, P145, DOI 10.1016/S0893-6080(98)00116-6
   Roig G, 2013, IEEE I CONF COMP VIS, P2312, DOI 10.1109/ICCV.2013.287
   Romera-Paredes Bernardino, 2013, JMLR Workshop and Conference Proceedings, P1444
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683
   Shi ZY, 2017, IEEE T PATTERN ANAL, V39, P2525, DOI 10.1109/TPAMI.2016.2645157
   Shi ZY, 2015, PROC CVPR IEEE, P4184, DOI 10.1109/CVPR.2015.7299046
   Shotton J, 2006, LECT NOTES COMPUT SC, V3951, P1
   Simonyan K., 2014, CORR
   Sinop AK, 2007, IEEE I CONF COMP VIS, P778
   Su C, 2016, LECT NOTES COMPUT SC, V9906, P475, DOI 10.1007/978-3-319-46475-6_30
   Sulistiyo MD, 2018, IEEE INT C INTELL TR, P2698, DOI 10.1109/ITSC.2018.8569372
   Sun R, 2020, J SYST ENG ELECTRON, V31, P45, DOI 10.21629/JSEE.2020.01.06
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wah Catherine, 2011, Technical report
   Wang XW, 2013, IEEE T MULTIMEDIA, V15, P2035, DOI 10.1109/TMM.2013.2279658
   Wimalawarne Kishan, 2014, NIPS, V27, P2825
   Winn J, 2005, IEEE I CONF COMP VIS, P1800
   Yang M. H., 2016, ENCY DATABASE SYSTEM
   Yang S, 2019, IEEE T MULTIMEDIA, V21, P3194, DOI 10.1109/TMM.2019.2919469
   Ying H., 2019, Embedmask: Embedding coupling for one-stage instance segmentation
   Zhang JL, 2021, IEEE T MULTIMEDIA, V23, P3085, DOI 10.1109/TMM.2020.3020691
   Zheng S, 2014, PROC CVPR IEEE, P3214, DOI 10.1109/CVPR.2014.411
NR 78
TC 5
Z9 5
U1 2
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG 11
PY 2021
VL 24
BP 3611
EP 3623
DI 10.1109/TMM.2021.3103605
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2W7ND
UT WOS:000824706200001
DA 2024-07-18
ER

PT J
AU Wu, TY
   Tang, S
   Zhang, R
   Guo, GD
AF Wu, Tianyi
   Tang, Sheng
   Zhang, Rui
   Guo, Guodong
TI Consensus Feature Network for Scene Parsing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Transforms; Semantics; Convolution; Feature extraction; Training;
   Network architecture; Information and communication technology; Scene
   Parsing; Instance Consensus Transform; Category Consensus Transform
ID SEGMENTATION; IMAGES
AB Scene parsing is challenging as it aims to assign one of the semantic categories to each pixel in scene images. Thus, pixel-level features are desired for scene parsing. However, classification networks are dominated by the discriminative portion, so directly applying classification networks to scene parsing will result in inconsistent parsing predictions within one instance and among instances of the same category. To address this problem, we propose two transform units to learn pixel-level consensus features. One is an Instance Consensus Transform (ICT) unit to learn the instance-level consensus features by aggregating features within the same instance. The other is a Category Consensus Transform (CCT) unit to pursue category-level consensus features through keeping the consensus of features among instances of the same category in scene images. The proposed ICT and CCT units are lightweight, data-driven and end-to-end trainable. The features learned by the two units are more coherent in both instance-level and category-level. Furthermore, we present the Consensus Feature Network (CFNet) based on the proposed ICT and CCT units, and demonstrate the effectiveness of each component in our method by performing extensive ablation experiments. Finally, our proposed CFNet achieves competitive performance on four datasets, including Cityscapes, Pascal Context, CamVid, and COCO Stuff.
C1 [Wu, Tianyi; Guo, Guodong] Inst Deep Learning, Baidu Res, Beijing 100085, Peoples R China.
   [Wu, Tianyi; Guo, Guodong] Natl Engn Lab Deep Learning Technol & Applicat, Beijing 100085, Peoples R China.
   [Tang, Sheng; Zhang, Rui] Chinese Acad Sci, Insititue Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
   [Tang, Sheng; Zhang, Rui] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
C3 Baidu; Chinese Academy of Sciences; Chinese Academy of Sciences;
   University of Chinese Academy of Sciences, CAS
RP Tang, S (corresponding author), Chinese Acad Sci, Insititue Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
EM wutianyi01@baidu.com; ts@ict.ac.cn; zhangrui@ict.ac.cn;
   guoguodong01@baidu.com
RI ; Guo, Guodong/M-5066-2015; Tang, Sheng/L-5792-2013
OI Wu, Tianyi/0000-0001-7434-0487; Guo, Guodong/0000-0001-9583-0055; Tang,
   Sheng/0000-0003-3573-2407
FU National Key Research and Development Program of China [2017YFB1002202];
   National Natural Science Foundation of China [61871004]
FX This work was supported in part by National Key Research and Development
   Program of China under Grant 2017YFB1002202, and in part National
   Natural Science Foundation of China under Grant 61871004.
CR [Anonymous], 2015, ARXIV151104377
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Bian JW, 2017, PROC CVPR IEEE, P2828, DOI 10.1109/CVPR.2017.302
   Bilinski P, 2018, PROC CVPR IEEE, P6596, DOI 10.1109/CVPR.2018.00690
   Brostow GJ, 2008, LECT NOTES COMPUT SC, V5302, P44, DOI 10.1007/978-3-540-88682-2_5
   Bulò SR, 2018, PROC CVPR IEEE, P5639, DOI 10.1109/CVPR.2018.00591
   Caesar H, 2018, PROC CVPR IEEE, P1209, DOI 10.1109/CVPR.2018.00132
   Chang SH, 1997, PATTERN RECOGN, V30, P311, DOI 10.1016/S0031-3203(96)00076-3
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709
   Chen YP, 2019, PROC CVPR IEEE, P433, DOI 10.1109/CVPR.2019.00052
   Cheng, 2018, ADV NEURAL INF PROCE, P549, DOI 10.5555/3326943.3326994.
   Cheng BW, 2019, IEEE I CONF COMP VIS, P5217, DOI 10.1109/ICCV.2019.00532
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Ding HH, 2018, PROC CVPR IEEE, P2393, DOI 10.1109/CVPR.2018.00254
   Fu J, 2019, IEEE I CONF COMP VIS, P6747, DOI 10.1109/ICCV.2019.00685
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Harley AW, 2017, IEEE I CONF COMP VIS, P5048, DOI 10.1109/ICCV.2017.539
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Islam MA, 2017, PROC CVPR IEEE, P4877, DOI 10.1109/CVPR.2017.518
   Ke TW, 2018, LECT NOTES COMPUT SC, V11205, P605, DOI 10.1007/978-3-030-01246-5_36
   Kundu A, 2016, PROC CVPR IEEE, P3168, DOI 10.1109/CVPR.2016.345
   Li X, 2019, IEEE I CONF COMP VIS, P9166, DOI 10.1109/ICCV.2019.00926
   Li Y, 2019, IEEE T MULTIMEDIA, V21, P875, DOI 10.1109/TMM.2018.2867720
   Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Liu W., 2015, ARXIV150604579
   Long J., 2015, P IEEE C COMP VIS PA, P3431
   Mottaghi R, 2014, PROC CVPR IEEE, P891, DOI 10.1109/CVPR.2014.119
   Peng C, 2017, PROC CVPR IEEE, P1743, DOI 10.1109/CVPR.2017.189
   Qiurui Wang, 2019, IEEE Transactions on Multimedia, V21, P1839, DOI 10.1109/TMM.2018.2890360
   Rocco I, 2018, ADV NEUR IN, V31
   Rocco I, 2018, PROC CVPR IEEE, P6917, DOI 10.1109/CVPR.2018.00723
   Schalkoff Robert J., 1989, Digital image processing and computer vision
   Schmid C, 1997, IEEE T PATTERN ANAL, V19, P530, DOI 10.1109/34.589215
   Shi HC, 2018, IEEE T MULTIMEDIA, V20, P2670, DOI 10.1109/TMM.2018.2812600
   Shuai B, 2018, IEEE T PATTERN ANAL, V40, P1480, DOI 10.1109/TPAMI.2017.2712691
   Simonyan K., 2014, CORR
   STOCKMAN G, 1982, IEEE T PATTERN ANAL, V4, P229, DOI 10.1109/TPAMI.1982.4767240
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Trulls E, 2013, PROC CVPR IEEE, P2890, DOI 10.1109/CVPR.2013.372
   Tu ZW, 2005, INT J COMPUT VISION, V63, P113, DOI 10.1007/s11263-005-6642-x
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Visin F., 2015, arXiv
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wu TY, 2021, IEEE T IMAGE PROCESS, V30, P1169, DOI 10.1109/TIP.2020.3042065
   Wu TY, 2019, IEEE INT CON MULTI, P940, DOI 10.1109/ICME.2019.00166
   Yang MK, 2018, PROC CVPR IEEE, P3684, DOI 10.1109/CVPR.2018.00388
   Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20
   Yu CQ, 2018, PROC CVPR IEEE, P1857, DOI 10.1109/CVPR.2018.00199
   Yuan Y, 2018, OCNET OBJECT CONTEXT
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang H, 2018, PROC CVPR IEEE, P7151, DOI 10.1109/CVPR.2018.00747
   Zhang R, 2020, IEEE T PATTERN ANAL, V42, P909, DOI 10.1109/TPAMI.2018.2890637
   Zhang R, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3427
   Zhang R, 2017, IEEE I CONF COMP VIS, P2050, DOI 10.1109/ICCV.2017.224
   Zhang XL, 2018, PROC CVPR IEEE, P1325, DOI 10.1109/CVPR.2018.00144
   ZHANG ZY, 1995, ARTIF INTELL, V78, P87, DOI 10.1016/0004-3702(95)00022-4
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11213, P270, DOI 10.1007/978-3-030-01240-3_17
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhu Z, 2019, IEEE I CONF COMP VIS, P593, DOI 10.1109/ICCV.2019.00068
NR 65
TC 1
Z9 1
U1 1
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL 2
PY 2021
VL 24
BP 3208
EP 3217
DI 10.1109/TMM.2021.3094333
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2W7NN
UT WOS:000824707200002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU An, HR
   Hu, HM
   Guo, YF
   Zhou, QL
   Li, B
AF An, Haoran
   Hu, Hai-Miao
   Guo, Yuanfang
   Zhou, Qianli
   Li, Bo
TI Hierarchical Reasoning Network for Pedestrian Attribute Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Cognition; Semantics; Task analysis; Machine
   learning; Correlation; Image color analysis; Pedestrian attribute
   recognition; video surveillance; abstraction levels; hierarchical;
   reason
ID CLASSIFICATION; RETRIEVAL; ALIGNMENT
AB Pedestrian attribute recognition, which can benefit other tasks such as person re-identification and pedestrian retrieval, is very important in video surveillance related tasks. In this paper, we observe that the existing methods tackle this problem from the perspective of multi-label classification without considering the hierarchical relationships among the attributes. In human cognition, the attributes can be categorized according to their semantic/abstraction levels. The high-level attributes can be predicted by reasoning from the low-level and medium-level attributes, while the recognition of the low-level and medium-level attributes can be guided by the high-level attributes. Based on this attribute categorization, we propose a novel Hierarchical Reasoning Network (HR-Net), which can hierarchically predict the attributes at different abstraction levels in different stages of the network. We also propose an attribute reasoning structure to exploit the relationships among the attributes at different semantic levels. Experimental results demonstrate that the proposed network gives superior performances compared to the state-of-the-art techniques.
C1 [An, Haoran; Hu, Hai-Miao; Guo, Yuanfang; Li, Bo] Beihang Univ, Sch Comp Sci & Engn, Beijing 100191, Peoples R China.
   [Zhou, Qianli] Peoples Publ Secur Univ China, Beijing 100038, Peoples R China.
C3 Beihang University; People's Public Security University of China
RP Hu, HM (corresponding author), Beihang Univ, Sch Comp Sci & Engn, Beijing 100191, Peoples R China.
EM waterhran@qq.com; frank0139@163.com; eeandyguo@connect.ust.hk;
   13331112522@189.cn; boli@buaa.edu.cn
RI zhang, luyu/JJC-4227-2023; Li, bo/IWL-9318-2023; cheng,
   cheng/JBR-8359-2023; Li, Ye/JBS-2949-2023; LI, Xiang-Yang/JZE-0275-2024;
   Li, Kun/JLL-6505-2023
OI Li, Kun/0000-0002-3638-2974; Guo, Yuanfang/0000-0003-4592-8083
FU National Natural Science Foundation of China [61772058]; Fundamental
   Research Funds for the Central Universities
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61772058, and in part by the Fundamental
   Research Funds for the Central Universities. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Elisa Ricci.
CR Abdulnabi AH, 2015, IEEE T MULTIMEDIA, V17, P1949, DOI 10.1109/TMM.2015.2477680
   [Anonymous], P EUR C COMP VIS ECC
   [Anonymous], 2018, ARXIV180809102
   [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.90
   [Anonymous], 2009, Applications of Computer Vision (WACV), 2009 Workshop on
   Bai Y, 2018, IEEE T MULTIMEDIA, V20, P2385, DOI 10.1109/TMM.2018.2796240
   Bourdev L, 2011, IEEE I CONF COMP VIS, P1543, DOI 10.1109/ICCV.2011.6126413
   Chen YQ, 2018, PROCEEDINGS OF THE 13TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (VISIGRAPP 2018), VOL 5: VISAPP, P114, DOI 10.5220/0006622901140122
   Chen YT, 2019, IEEE T MULTIMEDIA, V21, P704, DOI 10.1109/TMM.2018.2865860
   Chen ZX, 2018, IEEE T MULTIMEDIA, V20, P2126, DOI 10.1109/TMM.2017.2785253
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng YB, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P789, DOI 10.1145/2647868.2654966
   Domingos P., 2014, INTRO STAT RELATIONA
   Gong BQ, 2013, IEEE T MULTIMEDIA, V15, P369, DOI 10.1109/TMM.2012.2231059
   Han H, 2018, IEEE T PATTERN ANAL, V40, P2597, DOI 10.1109/TPAMI.2017.2738004
   Karanam S, 2019, IEEE T PATTERN ANAL, V41, P523, DOI 10.1109/TPAMI.2018.2807450
   Kaya A, 2015, J BIOMED INFORM, V56, P69, DOI 10.1016/j.jbi.2015.05.011
   Krizhevsky A., 2014, P ADV NEUR INF PROC, P1097
   Layne R., 2014, PERSON REIDENTIFICAT
   Layne R, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.24
   Li DW, 2019, IEEE T IMAGE PROCESS, V28, P1575, DOI 10.1109/TIP.2018.2878349
   Li DW, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P111, DOI 10.1109/ACPR.2015.7486476
   Li SX, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2580939
   Li YN, 2016, LECT NOTES COMPUT SC, V9910, P684, DOI 10.1007/978-3-319-46466-4_41
   Liu XH, 2017, IEEE I CONF COMP VIS, P350, DOI 10.1109/ICCV.2017.46
   Mo HY, 2019, IEEE T MULTIMEDIA, V21, P943, DOI 10.1109/TMM.2018.2867262
   OJALA T, 1994, INT C PATT RECOG, P582, DOI 10.1109/ICPR.1994.576366
   Park BG, 2005, IEEE T PATTERN ANAL, V27, P1982, DOI 10.1109/TPAMI.2005.243
   Pourian N, 2015, IEEE T MULTIMEDIA, V17, P616, DOI 10.1109/TMM.2015.2410734
   Prosser B., 2010, PERSON RE IDENTIFICA, V2, P1
   Ramanan D, 2007, IEEE T PATTERN ANAL, V29, P65, DOI 10.1109/TPAMI.2007.250600
   Sarfraz M., 2017, BRIT MACH VIS C
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sudowe Patrick, 2015, ICCV, P87
   Wang JY, 2017, IEEE I CONF COMP VIS, P531, DOI 10.1109/ICCV.2017.65
   Wei LH, 2019, IEEE T MULTIMEDIA, V21, P986, DOI 10.1109/TMM.2018.2870522
   Wu AC, 2017, IEEE T IMAGE PROCESS, V26, P2588, DOI 10.1109/TIP.2017.2675201
   Yu K., 2016, WEAKLY SUPERVISED LE
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang JJ, 2018, IEEE T MULTIMEDIA, V20, P2801, DOI 10.1109/TMM.2018.2812605
   Zhang XS, 2017, IEEE T MULTIMEDIA, V19, P2533, DOI 10.1109/TMM.2017.2696825
   Zhao X., 2018, P 27 INT JOINT C ART, P3177
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhu F, 2017, PROC CVPR IEEE, P2027, DOI 10.1109/CVPR.2017.219
   Zhu J., 2014, P AS C COMP VIS, P545
NR 45
TC 12
Z9 13
U1 0
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 268
EP 280
DI 10.1109/TMM.2020.2975417
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA PJ6LW
UT WOS:000601877600021
DA 2024-07-18
ER

PT J
AU Cao, Y
   Qi, H
   Gui, J
   Li, KQ
   Tang, YY
   Kwok, JTY
AF Cao, Yuan
   Qi, Heng
   Gui, Jie
   Li, Keqiu
   Tang, Yuan Yan
   Kwok, James Tin-Yau
TI Learning to Hash With Dimension Analysis Based Quantizer for Image
   Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Quantization (signal); Data acquisition; Image retrieval; Nearest
   neighbor methods; Hamming distance; Symmetric matrices; Binary codes;
   Approximate nearest neighbor search; hashing algorithms; image
   retrieval; quantization
ID BIG DATA; BINARY; SEARCH; CODES
AB The last few years have witnessed the rise of the big data era, in which approximate nearest neighbor search is a fundamental problem in many applications, such as large-scale image retrieval. Recently, many research results demonstrate that hashing can achieve promising performance due to its appealing storage and search efficiency. Since the complex optimization problems for loss functions are difficult to solve, most hashing methods decompose the hash codes learning problem into two steps: projection and quantization. In the quantization step, binary codes are widely used because ranking them by Hamming distance is very efficient. However, the huge information loss produced by the quantization step should be reduced in applications, such as image retrieval where high search accuracy is required. Since many two-step hashing methods produce uneven projected dimensions in the projection step, in this paper, we propose a novel dimension analysis based quantization method (DAQ) on two-step hashing methods for image retrieval. We first perform an importance analysis of the projected dimensions and select a subset of them that are more informative than the others, then we divide the selected projected dimensions into several regions with our quantizer. Every region is quantized with its corresponding codebook. Finally, the similarity between two hash codes is estimated by Manhattan distance between their corresponding codebooks, which is also efficient. We conduct experiments on three public benchmarks containing up to one million descriptors and show that the proposed DAQ method consistently leads to significant accuracy improvements over state-of-the-art quantization methods.
C1 [Cao, Yuan] Ocean Univ China, Sch Informat Sci & Engn, Qingdao 266100, Peoples R China.
   [Qi, Heng] Dalian Univ Technol, Sch Comp Sci & Technol, Dalian 116023, Peoples R China.
   [Gui, Jie] Southeast Univ, Sch Cyber Sci & Engn, Nanjing 211189, Peoples R China.
   [Li, Keqiu] Tianjin Univ, Coll Intelligence & Comp, Tianjin 300350, Peoples R China.
   [Tang, Yuan Yan] Univ Macao, Dept Comp & Informat Sci, Macau 999078, Peoples R China.
   [Kwok, James Tin-Yau] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong 999077, Peoples R China.
C3 Ocean University of China; Dalian University of Technology; Southeast
   University - China; Tianjin University; University of Macau; Hong Kong
   University of Science & Technology
RP Qi, H (corresponding author), Dalian Univ Technol, Sch Comp Sci & Technol, Dalian 116023, Peoples R China.; Gui, J (corresponding author), Southeast Univ, Sch Cyber Sci & Engn, Nanjing 211189, Peoples R China.
EM caoyuan57@163.com; hengqi@dlut.edu.cn; guijie@seu.edu.cn;
   keqiu@tju.edu.cn; yuanyant@gmail.com; jamesk@cse.ust.hk
FU National Key R&D Program of China [2019YFB2102400]; NSFC [61772112,
   61572463]; Science Innovation Foundation of Dalian [2019J12GX037];
   Fundamental Research Funds for the Central Universities [2242021R10097]
FX Thiswork is supported in part by the National Key R&D Program of China
   under Grant 2019YFB2102400; in part by the NSFC under Grants 61772112
   and 61572463; in part by the Science Innovation Foundation of Dalian
   under Grant 2019J12GX037, and in part by the Fundamental Research Funds
   for the CentralUniversities underGrant 2242021R10097. The associate
   editor coordinating the reviewof this manuscript and approving it for
   publication was Prof. Mohammed Daoudi.
CR [Anonymous], 2012, P INT C NEUR INF PRO
   [Anonymous], 2009, NIPS
   Beygelzimer A., 2006, ICML, DOI DOI 10.1145/1143844.1143857
   Cao Y, 2018, IEEE ACCESS, V6, P2039, DOI 10.1109/ACCESS.2017.2781360
   Chen ZX, 2017, IEEE T MULTIMEDIA, V19, P123, DOI 10.1109/TMM.2016.2620604
   Datta R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1348246.1348248
   Ding K, 2017, IEEE T MULTIMEDIA, V19, P571, DOI 10.1109/TMM.2016.2625747
   Ercoli S, 2017, IEEE T MULTIMEDIA, V19, P2521, DOI 10.1109/TMM.2017.2697824
   Friedman J. H, T MATH SOFTW, V3, P209
   Gong YC, 2013, PROC CVPR IEEE, P484, DOI 10.1109/CVPR.2013.69
   Gong YC, 2011, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2011.5995432
   Gordo A, 2014, IEEE T PATTERN ANAL, V36, P33, DOI 10.1109/TPAMI.2013.101
   Gui J, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1485, DOI 10.1145/3219819.3219955
   Gui J, 2018, IEEE T PATTERN ANAL, V40, P490, DOI 10.1109/TPAMI.2017.2678475
   Gui J, 2018, IEEE T NEUR NET LEAR, V29, P608, DOI 10.1109/TNNLS.2016.2636870
   He J., 2010, ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P1129
   Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Jiang QY, 2018, IEEE T IMAGE PROCESS, V27, P5996, DOI 10.1109/TIP.2018.2864894
   Joly A, 2011, PROC CVPR IEEE, P873, DOI 10.1109/CVPR.2011.5995709
   KIEFFER JC, 1983, IEEE T INFORM THEORY, V29, P42, DOI 10.1109/TIT.1983.1056622
   Kong W., 2012, P 26 AAAI C ART INT, P634, DOI DOI 10.5555/2900728.2900819
   Kong WH, 2012, SIGIR 2012: PROCEEDINGS OF THE 35TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P45, DOI 10.1145/2348283.2348293
   Kulis B, 2009, IEEE I CONF COMP VIS, P2130, DOI 10.1109/ICCV.2009.5459466
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li P, 2017, ADV NEUR IN, V30
   Li WJ, 2016, IJCAI, P1711
   Li XY, 2020, FRONT ARTIF INTEL AP, V325, P1276, DOI 10.3233/FAIA200229
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu W, 2011, SER INF MANAGE SCI, V10, P1
   Liu X., 2012, PROC ACM MULTIMEDIA, P881
   Lv YM, 2015, IEEE T MULTIMEDIA, V17, P1225, DOI 10.1109/TMM.2015.2437712
   Mu YD, 2010, PROC CVPR IEEE, P3344, DOI 10.1109/CVPR.2010.5540024
   Norouzi M.E., 2011, ICML
   Redondo-Cabrera C, 2012, PROC CVPR IEEE, P3458, DOI 10.1109/CVPR.2012.6248087
   Strecha C, 2012, IEEE T PATTERN ANAL, V34, P66, DOI 10.1109/TPAMI.2011.103
   Tang JH, 2018, IEEE T CIRC SYST VID, V28, P2730, DOI 10.1109/TCSVT.2017.2715227
   UHLMANN JK, 1991, INFORM PROCESS LETT, V40, P175, DOI 10.1016/0020-0190(91)90074-R
   Wang J, 2016, P IEEE, V104, P34, DOI 10.1109/JPROC.2015.2487976
   Wang Jun., 2010, ICML, P1127
   Weiss P, 2008, INT CONF ACOUST SPEE, P1173, DOI 10.1109/ICASSP.2008.4517824
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2156
   Xu B., 2013, International Joint Conference on Artificial Intelligence, P1820
   Zhang QF, 2014, INT CONF MACH LEARN, P807, DOI 10.1109/ICMLC.2014.7009713
   Zhu L, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P726, DOI 10.1145/3123266.3123301
   Zou FH, 2015, IEEE T MULTIMEDIA, V17, P1006, DOI 10.1109/TMM.2015.2425651
NR 46
TC 8
Z9 8
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3907
EP 3918
DI 10.1109/TMM.2020.3033118
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA WJ5OB
UT WOS:000709093100037
DA 2024-07-18
ER

PT J
AU Fang, ZW
   Zhou, JT
   Xiao, Y
   Li, YA
   Yang, F
AF Fang, Zhiwen
   Zhou, Joey Tianyi
   Xiao, Yang
   Li, Yanan
   Yang, Feng
TI Multi-Encoder Towards Effective Anomaly Detection in Videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Anomaly detection; Feature extraction; Image reconstruction; Videos;
   Decoding; Task analysis; Deep learning; Anomaly detection; motion
   encoder; content encoder; multi-encoder single-decoder network
ID ABNORMAL EVENT DETECTION; HISTOGRAMS
AB Given normal training samples, anomaly detection in videos can be regarded as a challenging problem of identifying unexpected events. The state-of-the-art approaches generally resort to the autoencoder model by using a single encoder to capture the motion and content patterns jointly. Nevertheless, due to the lack of accurate labels of normal and abnormal samples, how to detect anomalies is decided by the subjective understanding of models. It infers that different models will prefer to mine different patterns according to the characteristics of models. We call this problem as a pattern bias problem. To alleviate this problem, a novel Multi-Encoder Single-Decoder network, termed as MESDnet, is proposed in the spirit of encoding motion and content cues individually with multiple encoders. MESDnet is of end-to-end learning ability and real-time running speed. Particularly, the differences between adjacent frames and the raw frames are used as the motion and content sources, respectively. Then, a decoder takes charge of detecting anomalies in the way of observing reconstructing error towards the video frames by using the multi-stream encoded motion and content features simultaneously. The experiments on the CUHK Avenue dataset, the UCSD Pedestrian dataset, and the ShanghaiTech Campus dataset verify the effectiveness of MESDnet.
C1 [Fang, Zhiwen; Yang, Feng] Southern Med Univ, Sch Biomed Engn, Guangzhou 510515, Peoples R China.
   [Fang, Zhiwen] Hunan Univ Humanities Sci & Technol, Sch Energy & Mech Elect Engn, Loudi 417000, Peoples R China.
   [Zhou, Joey Tianyi] ASTAR, IHPC, Singapore 138632, Singapore.
   [Xiao, Yang] Huazhong Univ Sci & Technol, Sch Artificial Intelligence & Automat, Natl Key Lab Sci & Technol Multispectral Informat, Wuhan 430074, Peoples R China.
   [Li, Yanan] Wuhan Inst Technol, Sch Comp Sci & Engn, Wuhan 430070, Peoples R China.
C3 Southern Medical University - China; Hunan University Of Humanities,
   Science & Technology; Agency for Science Technology & Research (A*STAR);
   A*STAR - Institute of High Performance Computing (IHPC); Huazhong
   University of Science & Technology; Wuhan Institute of Technology
RP Zhou, JT (corresponding author), ASTAR, IHPC, Singapore 138632, Singapore.
EM fzw310@gmail.com; joey.tianyi.zhou@gmail.com; Yang_Xiao@hust.edu.cn;
   yananli@wit.edu.cn; yangf@smu.edu.cn
RI XIAO, YANG/GPW-5529-2022; Zhou, Joey Tianyi/AAC-5115-2019; Li,
   Yanan/ABL-1507-2022; Yang, Xiao/JCD-7233-2023; yang, xiao/HJI-7815-2023;
   xiao, yang/JCD-7195-2023
OI Zhou, Joey Tianyi/0000-0002-4675-7055; Li, Yanan/0000-0002-6321-2567;
   Yang, Feng/0000-0001-7190-4064
FU National Natural Science Foundation of China [61702182, 61771233,
   61502187, 61906139]; Hunan Provincial Natural Science Foundation of
   China [2018JJ3254]; Agency for Science, Technology and Research (A*STAR)
   under its AME Programmatic Funding Scheme Project [A18A1b0045]; National
   Key Laboratory Open Fund of China [6142113180211]; Hubei Provincial
   Natural Science Foundation of China [2019CFB173]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61702182, 61771233, 61502187, and
   61906139, in part by the Hunan Provincial Natural Science Foundation of
   China under Grant 2018JJ3254, in part by the Agency for Science,
   Technology and Research (A*STAR) under its AME Programmatic Funding
   Scheme Project A18A1b0045, in part by the National Key Laboratory Open
   Fund of China under Grant 6142113180211, and in part by the Hubei
   Provincial Natural Science Foundation of China under Grant 2019CFB173.
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   [Anonymous], 2018, ICML
   Bergmann P, 2019, PROC CVPR IEEE, P9584, DOI 10.1109/CVPR.2019.00982
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bilen H, 2016, PROC CVPR IEEE, P3034, DOI 10.1109/CVPR.2016.331
   Chong YS, 2017, LECT NOTES COMPUT SC, V10262, P189, DOI 10.1007/978-3-319-59081-3_23
   Chu WQ, 2019, IEEE T MULTIMEDIA, V21, P246, DOI 10.1109/TMM.2018.2846411
   Cong Y, 2011, PROC CVPR IEEE, P1807, DOI 10.1109/CVPR.2011.5995434
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dalal N, 2006, LECT NOTES COMPUT SC, V3952, P428, DOI 10.1007/11744047_33
   Del Giorno A, 2016, LECT NOTES COMPUT SC, V9909, P334, DOI 10.1007/978-3-319-46454-1_21
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gonçalves GR, 2018, SIBGRAPI, P110, DOI 10.1109/SIBGRAPI.2018.00021
   Gong D, 2019, IEEE I CONF COMP VIS, P1705, DOI 10.1109/ICCV.2019.00179
   Hasan M, 2016, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2016.86
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinami R, 2017, IEEE I CONF COMP VIS, P3639, DOI 10.1109/ICCV.2017.391
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Ionescu RT, 2019, PROC CVPR IEEE, P7834, DOI 10.1109/CVPR.2019.00803
   Ionescu RT, 2017, IEEE I CONF COMP VIS, P2914, DOI 10.1109/ICCV.2017.315
   Kalogeiton V, 2017, IEEE I CONF COMP VIS, P2001, DOI 10.1109/ICCV.2017.219
   Kim J, 2009, PROC CVPR IEEE, P2913
   Liu C, 2018, IEEE T MULTIMEDIA, V20, P889, DOI 10.1109/TMM.2017.2760633
   Liu W, 2018, PROC CVPR IEEE, P6536, DOI 10.1109/CVPR.2018.00684
   Lu CW, 2013, IEEE I CONF COMP VIS, P2720, DOI 10.1109/ICCV.2013.338
   Luo B, 2017, IEEE T MULTIMEDIA, V19, P1482, DOI 10.1109/TMM.2017.2671447
   Luo WX, 2021, IEEE T PATTERN ANAL, V43, P1070, DOI 10.1109/TPAMI.2019.2944377
   Luo WX, 2017, IEEE I CONF COMP VIS, P341, DOI 10.1109/ICCV.2017.45
   Luo WX, 2017, IEEE INT CON MULTI, P439, DOI 10.1109/ICME.2017.8019325
   Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352
   Mahadevan V, 2010, PROC CVPR IEEE, P1975, DOI 10.1109/CVPR.2010.5539872
   Mathieu M, 2017, P IEEE INT C COMP VI, P2813
   Medel J. R., 2016, Anomaly detection in video using predictive convolutional long short-term memory networks
   Morais R, 2019, PROC CVPR IEEE, P11988, DOI 10.1109/CVPR.2019.01227
   Nguyen D. T., 2019, PR MACH LEARN RES, V97, P4800
   Ntalampiras S, 2011, IEEE T MULTIMEDIA, V13, P713, DOI 10.1109/TMM.2011.2122247
   Prasad NR, 2009, CMC-COMPUT MATER CON, V14, P1, DOI 10.1145/1541880.1541882
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683
   Simonyan K, 2014, ADV NEUR IN, V27
   Smeureanu S, 2017, LECT NOTES COMPUT SC, V10485, P779, DOI 10.1007/978-3-319-68548-9_70
   Sultani W, 2018, PROC CVPR IEEE, P6479, DOI 10.1109/CVPR.2018.00678
   Sun M, 2014, LECT NOTES COMPUT SC, V8689, P787, DOI 10.1007/978-3-319-10590-1_51
   Sun QR, 2017, PATTERN RECOGN, V64, P187, DOI 10.1016/j.patcog.2016.09.016
   Tang S, 2017, IEEE T MULTIMEDIA, V19, P2105, DOI 10.1109/TMM.2017.2729786
   Villegas Ruben, 2017, ICLR
   Wu L, 2019, IEEE T NEUR NET LEAR, V30, P3347, DOI 10.1109/TNNLS.2019.2891244
   Wu SD, 2010, PROC CVPR IEEE, P2054, DOI 10.1109/CVPR.2010.5539882
   Xu D, 2017, COMPUT VIS IMAGE UND, V156, P117, DOI 10.1016/j.cviu.2016.10.010
   Xu K, 2020, IEEE T MULTIMEDIA, V22, P394, DOI 10.1109/TMM.2019.2929931
   Xu K, 2018, IEEE T MULTIMEDIA, V20, P1062, DOI 10.1109/TMM.2018.2818942
   Zhang D, 2005, PROC CVPR IEEE, P611
   Zhang JQ, 2019, IEEE T MULTIMEDIA, V21, P1332, DOI 10.1109/TMM.2018.2871421
   Zhong JX, 2019, PROC CVPR IEEE, P1237, DOI 10.1109/CVPR.2019.00133
   Zhou JT, 2019, IEEE T INF FOREN SEC, V14, P2537, DOI 10.1109/TIFS.2019.2900907
NR 56
TC 24
Z9 24
U1 5
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 4106
EP 4116
DI 10.1109/TMM.2020.3037538
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XA2YU
UT WOS:000720519900015
DA 2024-07-18
ER

PT J
AU Keighrey, C
   Flynn, R
   Murray, S
   Murray, N
AF Keighrey, Conor
   Flynn, Ronan
   Murray, Siobhan
   Murray, Niall
TI A Physiology-Based QoE Comparison of Interactive Augmented Reality,
   Virtual Reality and Tablet-Based Applications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Quality of experience; Semantics; Multimedia systems; Augmented reality;
   Biomedical monitoring; Quality of experience; augmented reality; virtual
   reality; physiological; speech language pathology; aphasia
ID QUALITY; VIDEO; STRESS
AB The availability of affordable head-mounted display technology has facilitated new, potentially more immersive, interactive multimedia experiences. These technologies were traditionally focused on entertainment; however, academia and industry are now exploring applications in other domains such as health, learning and training. Key to the success of these new multimedia experiences is the understanding of a user's perceived quality of experience (QoE). Subjective user ratings have been the primary mechanism to capture insights into a user's experience. Such ratings have generally been captured post experience and reflected using a mean opinion score (MOS). However, user perception is multifactorial and subjective ratings alone do not express the true measure of an experience. As a result, recent efforts to capture QoE have included exploring the use of implicit metrics (e.g., physiological measures). This article presents the results of an experimental QoE evaluation and comparison of immersive applications delivered across three multimedia platforms. The platforms compared were augmented reality, tablet and virtual reality. The QoE methodology employed considered explicit (post-test questionnaire) and implicit (heart rate and electrodermal activity) assessment methods. The results indicate comparatively higher levels of QoE for users of the augmented reality and tablet platforms.
C1 [Keighrey, Conor; Flynn, Ronan; Murray, Niall] Athlone Inst Technol, Athlone N37 HD68, Ireland.
   [Murray, Siobhan] Primary Care Ctr, Hlth Serv Execut, Longford N39 C650, Ireland.
C3 Technological University of the Shannon: Midlands Midwest
RP Keighrey, C (corresponding author), Athlone Inst Technol, Athlone N37 HD68, Ireland.
EM c.keighrey@research.ait.ie; rflynn@ait.ie; siobhan.murray1@hse.ie;
   nmurray@research.ait.ie
RI Keighrey, Conor/AAM-8013-2021
OI Keighrey, Conor/0000-0002-3612-0413; Flynn, Ronan/0000-0002-6475-005X
FU Irish Research Council - Government of Ireland Postgraduate Scholarship
   [GOIPG/2018/1314]
FX This work was supported by the Irish Research Council - Government of
   Ireland Postgraduate Scholarship under Grant GOIPG/2018/1314. The
   associate editor coordinating the review of this manuscript and
   approving it for publicationwas Dr. Sen-Ching Samson Cheung.
CR Al Qassem LMMS, 2016, IEEE GLOB ENG EDUC C, P842, DOI 10.1109/EDUCON.2016.7474650
   [Anonymous], HOM PIP STRESS MAN D
   [Anonymous], 2016, IEEE, DOI DOI 10.1109/QOMEX.2016.7498964
   [Anonymous], P913 ITUT
   Boucsein W, 2012, ELECTRODERMAL ACTIVITY, SECOND EDITION, P1, DOI 10.1007/978-1-4614-1126-0
   BROWN CC, 1967, PSYCHOPHYSIOLOGY, V4, P249, DOI 10.1111/j.1469-8986.1967.tb02764.x
   Chen F, 2016, HUM-COMPUT INT-SPRIN, P185, DOI 10.1007/978-3-319-31700-7_12
   Committee on Vision Assembly of Behavioral and Social Sciences Na- tional Research Council, 1981, PROC TEST COL VIS RE
   Cubelos J, 2020, IEEE T MULTIMEDIA, V22, P69, DOI 10.1109/TMM.2019.2924575
   Drachen A., 2010, Proceedings of the 5th ACM SIGGRAPH Symposium on Video Games, P49, DOI DOI 10.1145/1836135.1836143
   Engelke U, 2017, IEEE J-STSP, V11, P6, DOI 10.1109/JSTSP.2016.2609843
   Gaddam VR, 2016, IEEE T MULTIMEDIA, V18, P1819, DOI 10.1109/TMM.2016.2586304
   Hossfeld T., 2016, Quality and User Experience, V1, P1, DOI [10.1007/S41233-016-0002-1, DOI 10.1007/S41233-016-0002-1, 10.1007/s41233-016-0002-1]
   Hossfeld T, 2011, INT WORK QUAL MULTIM, P131, DOI 10.1109/QoMEX.2011.6065690
   IBM, IBM SPSS IBM AN
   ISO, 85892007 ISO
   Janowski L, 2015, IEEE T MULTIMEDIA, V17, P2210, DOI 10.1109/TMM.2015.2484963
   Keighrev C., 2017, Quality of Multimedia Experience (QoMEX), 2017 Ninth International Conference on, P1
   Keighrey C., POSTTEST QUESTIONNAI
   Keighrey C, 2017, PROCEEDINGS OF THE THEMATIC WORKSHOPS OF ACM MULTIMEDIA 2017 (THEMATIC WORKSHOPS'17), P485, DOI 10.1145/3126686.3126747
   KILPATRICK DG, 1972, PSYCHOPHYSIOLOGY, V9, P218, DOI 10.1111/j.1469-8986.1972.tb00756.x
   Kim T, 2014, IEEE T MULTIMEDIA, V16, P387, DOI 10.1109/TMM.2013.2292592
   Murray N., 2018, IEEE MMTC COMMUN FRO, V13, P6
   Murray N, 2018, IEEE T BROADCAST, V64, P539, DOI 10.1109/TBC.2018.2825297
   Narwaria M, 2018, IEEE T MULTIMEDIA, V20, P2063, DOI 10.1109/TMM.2018.2794266
   Gonzalez DO, 2017, COMPUT ELECTRON AGR, V143, P111, DOI 10.1016/j.compag.2017.10.008
   Puig J, 2012, INT WORK QUAL MULTIM, P188, DOI 10.1109/QoMEX.2012.6263864
   Qualinet, 2013, CISC VIS NETW IND GL
   Raake A, 2014, T-LAB SER TELECOMMUN, P11, DOI 10.1007/978-3-319-02681-7_2
   Schmitt M, 2018, IEEE T MULTIMEDIA, V20, P1781, DOI 10.1109/TMM.2017.2777466
   Shi Y., 2007, CHI 07 EXTENDED ABST, P2651, DOI DOI 10.1145/1240866.1241057
   Snellen H., 1873, Probebuchstaben zur Bestimmung der Sehscharfe: 1
   Swinburn K., 2004, Comprehensive aphasia test
   Williams K, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P231, DOI 10.1145/2702123.2702484
   Yuan ZH, 2015, IEEE T MULTIMEDIA, V17, P957, DOI 10.1109/TMM.2015.2431915
NR 35
TC 32
Z9 32
U1 0
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 333
EP 341
DI 10.1109/TMM.2020.2982046
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA PJ6LW
UT WOS:000601877600026
DA 2024-07-18
ER

PT J
AU Li, GY
   Qiu, LN
   Yu, CG
   Cao, HW
   Liu, Y
   Yang, C
AF Li, Guangyu
   Qiu, Lina
   Yu, Chenguang
   Cao, Houwei
   Liu, Yong
   Yang, Can
TI IPTV Channel Zapping Recommendation With Attention Mechanism
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE IPTV; recommender system; realtime recommendation; fusion method;
   attention mechanism; neural networks
AB Internet Protocol TV (IPTV) normally has the advantage of providing far more TV channels than the traditional TV services, while as the other side of the coin it has the problem of information overload. Users of IPTV usually have difficulties finding channels matching their interests. In this paper, using a large IPTV dataset, we analyze channel zapping behaviors of IPTV users and discover various patterns that can be used to generate more accurate channel zapping recommendations. Based on user behavior analysis, we develop several base and fusion recommender systems that generate in real-time a short list of channels for users to consider whenever they want to switch channels. A deep neural network model that consists of a "Recommender System Attention (RS Attention)" module and a "Channel Attention" module capturing the static and dynamic user switching behaviors is also developed to further improve the recommendation accuracy. Evaluation on the IPTV dataset demonstrates that our fusion recommender can achieve 41% hit ratio with only three candidate channels, and our attention neural network model further pushes it up to 45%. Our recommender systems only take as input user channel zapping sequences, and can be easily adopted by IPTV systems with low data and computation overheads.
C1 [Li, Guangyu; Qiu, Lina; Yu, Chenguang; Liu, Yong] NYU, Dept Elect & Comp Engn, Brooklyn, NY 11201 USA.
   [Cao, Houwei] New York Inst Technol, Dept Comp Sci, Old Westbury, NY 10023 USA.
   [Yang, Can] South China Univ Technol, Dept Comp Sci, Guangzhou 510006, Peoples R China.
C3 New York University; New York University Tandon School of Engineering;
   New York Institute Technology; South China University of Technology
RP Li, GY (corresponding author), NYU, Dept Elect & Comp Engn, Brooklyn, NY 11201 USA.
EM guangyu.li@nyu.edu; lq437@nyu.edu; chenguang.yu@nyu.edu;
   hcao02@nyit.edu; yongliu@nyu.edu; cscyang@scut.edu.cn
RI Yang, Can/AAF-6597-2019; Yu, Chenguang/D-3504-2014
OI Yang, Can/0000-0002-7235-9093; Cao, Houwei/0000-0002-2310-7682
FU Guangzhou Science and Technology Plan key project, China [201704030124]
FX This work was supported in part by Guangzhou Science and Technology Plan
   key project, China, under Grant 201704030124. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Raouf Hamzaoui.
CR Abdollahpouri A, 2018, MULTIMED TOOLS APPL, V77, P8475, DOI 10.1007/s11042-017-4746-2
   Abrahamsson H., 2012, P 2012 INT MEAS C IM, P199
   [Anonymous], 2015, P 2015 C EMP METH NA, DOI [10.18653/v1/D15-1166, DOI 10.48550/ARXIV.1508.04025]
   Ardissono L, 2004, HUM-COMPUT INT-SPRIN, P3
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Breiman L., 2001, Mach. Learn., V45, P5
   Cha M, 2008, IMC'08: PROCEEDINGS OF THE 2008 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P71
   Chang HY, 2014, COMPUT J, V57, P1776, DOI 10.1093/comjnl/bxt093
   Chen M, 2012, INT C INTEL HUM MACH, P260, DOI 10.1109/IHMSC.2012.158
   Chen YS, 2013, IEEE T MULTIMEDIA, V15, P2087, DOI 10.1109/TMM.2013.2280123
   Hsu SH, 2007, LECT NOTES COMPUT SC, V4471, P166
   Keshavan RH, 2010, J MACH LEARN RES, V11, P2057
   Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263
   Lee E, 2014, IEEE T CONSUM ELECTR, V60, P124, DOI 10.1109/TCE.2014.6780934
   Lee WP, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON CONSUMER ELECTRONICS, PROCEEDINGS, P430
   Li Li L. L., P INT C COMP COMM NE, P1
   Liu N, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2568194
   Pagano Pagano R R, P RECSYSTV 2014 CAL
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Paterek A., 2007, P KDD CUP WORKSH, V2007, P5, DOI [DOI 10.1145/1557019.1557072, 10.1145/1557019.1557072]
   Punchihewa A., 2010, Proceedings of the 2010 5th International Conference on Information and Automation for Sustainability (ICIAfS), P45, DOI 10.1109/ICIAFS.2010.5715633
   Qiu TQ, 2009, PERF E R SI, V37, P275
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Seber G. A., 2012, LINEAR REGRESSION AN, V329
   Seo YD, 2020, EXPERT SYST APPL, V143, DOI 10.1016/j.eswa.2019.113045
   Smyth B., 1999, P 3 INT C CASE BASED, P561
   Sutskever I, 2014, ADV NEUR IN, V27
   Suykens JAK, 1999, NEURAL PROCESS LETT, V9, P293, DOI 10.1023/A:1018628609742
   ter Horst ter Horst H. H., WS9808
   Vaswani A, 2017, ADV NEUR IN, V30
   Véras D, 2015, EXPERT SYST APPL, V42, P9046, DOI 10.1016/j.eswa.2015.06.052
   Wang XJ, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2051, DOI 10.1145/3097983.3098096
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yang C, 2015, IEEE T MULTIMEDIA, V17, P1096, DOI 10.1109/TMM.2015.2429552
   Yin W., 2016, Transactions of the Association for computational linguistics, V4, P259, DOI [DOI 10.1162/TACL_A_00097, DOI 10.1162/TACLA00244, 10.1162/tacla00097, DOI 10.1162/TACLA00097]
   Yu CG, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P147, DOI 10.1145/3083187.3083194
   Zare S, 2016, MULTIMED TOOLS APPL, V75, P16059, DOI 10.1007/s11042-015-2913-x
NR 37
TC 11
Z9 12
U1 3
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 538
EP 549
DI 10.1109/TMM.2020.2984094
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA QA6NL
UT WOS:000613560200002
DA 2024-07-18
ER

PT J
AU Long, KX
   Cui, Y
   Ye, CC
   Liu, Z
AF Long, Kaixuan
   Cui, Ying
   Ye, Chencheng
   Liu, Zhi
TI Optimal Wireless Streaming of Multi-Quality 360 VR Video By Exploiting
   Natural, Relative Smoothness-Enabled, and Transcoding-Enabled Multicast
   Opportunities
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Streaming media; Wireless communication; Transcoding; Optimization;
   Resource management; Energy consumption; Head; Wireless streaming;
   virtual reality; 360 video; multi-quality; multicast; smoothness;
   transcoding; convex optimization; DC programming
ID DELIVERY
AB In this paper, we would like to investigate the optimal wireless streaming of a multi-quality tiled 360 virtual reality (VR) video from a server to multiple users. To this end, we propose to maximally exploit potential multicast opportunities by effectively utilizing characteristics of multi-quality tiled 360 VR videos and computation resources at the users' side. In particular, we consider two requirements for quality variation in one field-of-view (FoV), i.e., the absolute smoothness requirement and the relative smoothness requirement, and two video playback modes, i.e., the direct-playback mode (without user transcoding) and transcode-playback mode (with user transcoding). Besides natural multicast opportunities, we introduce two new types of multicast opportunities, namely, relative smoothness-enabled multicast opportunities, which allow a flexible tradeoff between viewing quality and communications resource consumption, and transcoding-enabled multicast opportunities, which allow a flexible tradeoff between computation and communications resource consumptions. Then, we establish a novel mathematical model that reflects the impacts of natural, relative smoothness-enabled, and transcoding-enabled multicast opportunities on the average transmission energy and transcoding energy. Based on this model, we optimize the transmission resource allocation, playback quality level selection, and transmission quality level selection to minimize the energy consumption in the four cases with different requirements for quality variation and video playback modes. By comparing the optimal values in the four cases, we prove that the energy consumption reduces when more multicast opportunities can be utilized. Finally, numerical results show substantial gains of the proposed solutions over existing schemes, and demonstrate the importance of exploiting of the three types of multicast opportunities.
C1 [Long, Kaixuan; Cui, Ying; Ye, Chencheng] Shanghai Jiao Tong Univ, Shanghai, Peoples R China.
   [Liu, Zhi] Univ Elect Commun, Tokyo, Japan.
C3 Shanghai Jiao Tong University; University of Electro-Communications -
   Japan
RP Cui, Y (corresponding author), Shanghai Jiao Tong Univ, Shanghai, Peoples R China.
EM longkaixuan@sjtu.edu.cn; cuiying@sjtu.edu.cn; yechencheng@sjtu.edu.cn;
   liu@ieee.org
RI Liu, Zhi/AAE-5698-2020; Cui, Ying/E-3960-2018
FU STCSM [18DZ2270700]; JSPS KAKENHI [19H04092, 20H04174]; ROIS NII Open
   Collaborative Research 2020 [20FA02]; Grants-in-Aid for Scientific
   Research [19H04092, 20H04174] Funding Source: KAKEN
FX Manuscript received January 26, 2020; revised August 21, 2020; accepted
   October 1, 2020. Date of publication October 12, 2020; date of current
   version October 19, 2021. This work was supported in part by STCSM
   18DZ2270700 and in part by JSPS KAKENHI under Grants 19H04092, 20H04174,
   and in part by ROIS NII Open Collaborative Research 2020 (20FA02). This
   paper was presented in part at the IEEE GLOBECOM 2019 [1]. The associate
   editor coordinating the review of this paper, and approving it for
   publication was Dr. Sanjeev Mehrotra. (Corresponding author: Ying Cui.)
CR Ahmadi H, 2017, PROCEEDINGS OF THE THEMATIC WORKSHOPS OF ACM MULTIMEDIA 2017 (THEMATIC WORKSHOPS'17), P170, DOI 10.1145/3126686.3126743
   [Anonymous], 1999, NONLINEAR PROGRAM
   Bao YN, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA), P1161, DOI 10.1109/BigData.2016.7840720
   Boyd S., 2004, CONVEX OPTIMIZATION
   Boyd S., 2007, Notes on Decomposition Methods, P1
   Chakareski J., 2018, 2018 IEEE INT C COMM, P1, DOI DOI 10.1109/ICC.2018.8422859
   Cheung G, 2017, IEEE IMAGE PROC, P2179, DOI 10.1109/ICIP.2017.8296668
   Corbillon X, 2017, IEEE ICC, DOI 10.1109/ICC.2017.7996611
   Fan CL, 2020, IEEE T MULTIMEDIA, V22, P744, DOI 10.1109/TMM.2019.2931807
   Gaddam VR, 2016, IEEE T MULTIMEDIA, V18, P1819, DOI 10.1109/TMM.2016.2586304
   Guo CJ, 2019, IEEE WIREL COMMUN LE, V8, P145, DOI 10.1109/LWC.2018.2864151
   Guo CJ, 2018, IEEE COMMUN LETT, V22, P2563, DOI 10.1109/LCOMM.2018.2873005
   He D., 2018, P ACM SIGCOMM WORKSH, P27, DOI DOI 10.1145/3229625.3229630
   Ju R, 2017, VR/AR NETWORK '17: PROCEEDINGS OF THE 2017 WORKSHOP ON VIRTUAL REALITY AND AUGMENTED REALITY NETWORK, P19, DOI 10.1145/3097895.3097899
   Lipp T, 2016, OPTIM ENG, V17, P263, DOI 10.1007/s11081-015-9294-x
   Liu X, 2017, HOTNETS-XVI: PROCEEDINGS OF THE 16TH ACM WORKSHOP ON HOT TOPICS IN NETWORKS, P50, DOI 10.1145/3152434.3152443
   Liu Z, 2018, SIGNAL PROCESS, V147, P154, DOI 10.1016/j.sigpro.2018.01.009
   Long K., 2018, P IEEE GLOBECOM, P1
   Long KX, 2019, IEEE GLOB COMM CONF, DOI [10.1109/globecom38437.2019.9014280, 10.1109/vtcspring.2019.8746431]
   Maniotis P, 2020, IEEE T MULTIMEDIA, V22, P2382, DOI 10.1109/TMM.2019.2957993
   Mordor Intelligence, VIRT REAL VR MARK GR
   Nguyen DV, 2019, IEEE J EM SEL TOP C, V9, P29, DOI 10.1109/JETCAS.2019.2899488
   Ozcinar C, 2017, IEEE IMAGE PROC, P2174, DOI 10.1109/ICIP.2017.8296667
   Qian F., 2016, Proceedings of the 5th Workshop on All Things Cellular: Operations, Applications and Challenges, P1, DOI DOI 10.1145/2980055.2980056
   Qian F, 2018, MOBICOM'18: PROCEEDINGS OF THE 24TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P99, DOI 10.1145/3241539.3241565
   Rossi S, 2017, IEEE INT WORKSH MULT
   Song JR, 2020, IEEE T MULTIMEDIA, V22, P2366, DOI 10.1109/TMM.2019.2957976
   Tran TX, 2019, IEEE T VEH TECHNOL, V68, P856, DOI 10.1109/TVT.2018.2881191
   Tse D., 2005, Fundementals of Wireless Communications
   Wang H., 2014, PROC NETW OPERATING, P25
   Xiao MB, 2018, IEEE INFOCOM SER, P953, DOI 10.1109/INFOCOM.2018.8486390
   Xie L, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P315, DOI 10.1145/3123266.3123291
   Xu W, 2020, IEEE T COMMUN, V68, P1494, DOI 10.1109/TCOMM.2019.2954523
   Yu Matt., 2015, P 3 INT WORKSHOP IMM, P1
   Zhou C, 2018, IEEE INFOCOM SER, P962, DOI 10.1109/INFOCOM.2018.8486282
   Zink M, 2019, P IEEE, V107, P639, DOI 10.1109/JPROC.2019.2894817
NR 36
TC 26
Z9 26
U1 1
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3670
EP 3683
DI 10.1109/TMM.2020.3029880
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA WJ5OB
UT WOS:000709093100019
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Qiao, ML
   Xu, M
   Wang, ZL
   Borji, A
AF Qiao, Minglang
   Xu, Mai
   Wang, Zulin
   Borji, Ali
TI Viewport-Dependent Saliency Prediction in 360° Video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 360 degrees video; viewport; saliency prediction; multi-task DNN
ID VISUAL-ATTENTION; MODEL; EYE; IMAGES; SHIFTS; GAZE; HEAD
AB Saliency prediction in traditional images and videos has drawn extensive research interests in recent years. Few works have been proposed for saliency prediction over 360 degrees videos. They focus on directly predicting fixations over the whole panorama. When viewing 360 degrees videos, a person can only observe the content in her viewport, which means that only a fraction of the 360 degrees scene can be seen at any given time. In this paper, we study human attention over viewport of 360 degrees videos and propose a novel visual saliency model, dubbed viewport saliency, to predict fixations over 360 degrees videos. Two contributions are introduced. First, we find that where people look is affected by the content and location of the viewport in 360 degrees video. We study this over 200+ 360 degrees videos viewed by 30+ subjects over two recent benchmark databases. Second, we propose a Multi-Task Deep Neural Network (MT-DNN) method for Viewport Saliency (VS) prediction in 360 degrees video, which considers the input content and location of the viewport. Extensive experiments and analyses show that our method outperforms other state-of-the-art methods in this task. In particular, over the two recent 360 degrees video databases, our MT-DNN raises the average CC score by 0.149 and 0.205, compared to SalGAN and DeepVS methods, respectively.
C1 [Qiao, Minglang; Xu, Mai; Wang, Zulin] Beihang Univ, Sch Elect & Informat Engn, Beijing 100191, Peoples R China.
   [Xu, Mai] Beihang Univ, Hangzhou Innovat Inst, Beijing 100191, Peoples R China.
   [Borji, Ali] MarkableAI Inc, Brooklyn, NY 11201 USA.
C3 Beihang University; Beihang University
RP Xu, M (corresponding author), Beihang Univ, Sch Elect & Informat Engn, Beijing 100191, Peoples R China.
EM minglangqiao@buaa.edu.cn; maixu@buaa.edu.cn; wzulin@buaa.edu.cn;
   aliborji@gmail.com
OI Borji, Ali/0000-0001-8198-0335
FU NSFC [61876013, 61922009, 61573037]
FX This work was supported by the NSFC Projects 61876013, 61922009, and
   61573037 The associate editor coordinating the review of this manuscript
   and approving it for publication was Dr. Wanqing Li.
CR Nguyen A, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1190, DOI 10.1145/3240508.3240669
   [Anonymous], 2008, Advances in neural information processing systems
   [Anonymous], 2017, P 27 NOSSDAV, DOI DOI 10.1145/3083165.3083180
   [Anonymous], 2012, P ADV NEUR INF PROC
   [Anonymous], 2018, LECT NOTES COMPUT SC, DOI DOI 10.1007/978-3-030-01234-2_30
   [Anonymous], 2011, P 19 ACM INT C MULT
   Bak C., 2016, ABS160704730 CORR
   Bak C, 2018, IEEE T MULTIMEDIA, V20, P1688, DOI 10.1109/TMM.2017.2777665
   Bertasius G, 2017, ROBOTICS: SCIENCE AND SYSTEMS XIII
   Borji A, 2021, IEEE T PATTERN ANAL, V43, P679, DOI 10.1109/TPAMI.2019.2935715
   Borji A, 2016, IEEE T NEUR NET LEAR, V27, P1214, DOI 10.1109/TNNLS.2015.2480683
   Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89
   Borji A, 2012, PROC CVPR IEEE, P438, DOI 10.1109/CVPR.2012.6247706
   Bruce N., 2006, P ADV NEUR INF PROC, P155
   Bylinskii Z, 2019, IEEE T PATTERN ANAL, V41, P740, DOI 10.1109/TPAMI.2018.2815601
   Chaabouni S, 2016, IEEE IMAGE PROC, P1604, DOI 10.1109/ICIP.2016.7532629
   Cheng HT, 2018, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2018.00154
   CHENG YZ, 1995, IEEE T PATTERN ANAL, V17, P790, DOI 10.1109/34.400568
   Cornia M, 2018, IEEE T IMAGE PROCESS, V27, P5142, DOI 10.1109/TIP.2018.2851672
   Ding XY, 2019, IEEE T MULTIMEDIA, V21, P124, DOI 10.1109/TMM.2018.2851389
   Erdem E, 2013, J VISION, V13, DOI 10.1167/13.4.11
   Evangelopoulos G, 2013, IEEE T MULTIMEDIA, V15, P1553, DOI 10.1109/TMM.2013.2267205
   Fang YM, 2012, IEEE T MULTIMEDIA, V14, P187, DOI 10.1109/TMM.2011.2169775
   Gao D., 2005, P ADV NEUR INF PROC, P481
   Hadizadeh H, 2014, IEEE T IMAGE PROCESS, V23, P19, DOI 10.1109/TIP.2013.2282897
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Huang X, 2015, IEEE I CONF COMP VIS, P262, DOI 10.1109/ICCV.2015.38
   Huang YF, 2018, LECT NOTES COMPUT SC, V11208, P789, DOI 10.1007/978-3-030-01225-0_46
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Itti L, 2000, VISION RES, V40, P1489, DOI 10.1016/S0042-6989(99)00163-7
   Itti L, 2003, PROC SPIE, V5200, P64, DOI 10.1117/12.512618
   Jiang L, 2018, LECT NOTES COMPUT SC, V11218, P625, DOI 10.1007/978-3-030-01264-9_37
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Khatoonabadi SH, 2015, PROC CVPR IEEE, P5501, DOI 10.1109/CVPR.2015.7299189
   KOCH C, 1985, HUM NEUROBIOL, V4, P219
   Kruthiventi SSS, 2017, IEEE T IMAGE PROCESS, V26, P4446, DOI 10.1109/TIP.2017.2710620
   Lebreton P, 2018, SIGNAL PROCESS-IMAGE, V69, P69, DOI 10.1016/j.image.2018.03.006
   Lee SH, 2014, IEEE IMAGE PROC, P1120, DOI 10.1109/ICIP.2014.7025223
   Li C, 2019, PROC CVPR IEEE, P10169, DOI 10.1109/CVPR.2019.01042
   Li C, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P932, DOI 10.1145/3240508.3240581
   Li J, 2015, IEEE I CONF COMP VIS, P190, DOI 10.1109/ICCV.2015.30
   Li SX, 2018, IEEE T MULTIMEDIA, V20, P155, DOI 10.1109/TMM.2017.2721544
   Li Y, 2013, IEEE I CONF COMP VIS, P3216, DOI 10.1109/ICCV.2013.399
   Ling J, 2018, SIGNAL PROCESS-IMAGE, V69, P60, DOI 10.1016/j.image.2018.03.007
   Liu N, 2015, PROC CVPR IEEE, P362, DOI 10.1109/CVPR.2015.7298633
   Liu YF, 2017, PROC CVPR IEEE, P3224, DOI 10.1109/CVPR.2017.343
   Matsuo K, 2014, IEEE COMPUT SOC CONF, P565, DOI 10.1109/CVPRW.2014.87
   Monroy R, 2018, SIGNAL PROCESS-IMAGE, V69, P26, DOI 10.1016/j.image.2018.05.005
   Pan J., 2017, PROC CVPR SCENE UNDE, P1
   Pan JT, 2016, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2016.71
   Polatsek P, 2016, IEEE SIGNAL PROC LET, V23, P394, DOI 10.1109/LSP.2016.2523339
   Redmon J., 2018, P IEEE C COMP VIS PA
   Rosenholtz R, 1999, VISION RES, V39, P3157, DOI 10.1016/S0042-6989(99)00077-2
   Shi XJ, 2015, ADV NEUR IN, V28
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sitzmann V, 2018, IEEE T VIS COMPUT GR, V24, P1633, DOI 10.1109/TVCG.2018.2793599
   Su YC, 2016, LECT NOTES COMPUT SC, V9909, P454, DOI 10.1007/978-3-319-46454-1_28
   Sun XS, 2017, AAAI CONF ARTIF INTE, P274
   Suzuki T, 2018, IEEE SYS MAN CYBERN, P2079, DOI 10.1109/SMC.2018.00358
   TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5
   Upenik Evgeniy, 2017, 2017 IEEE International Conference on Multimedia and Expo: Workshops (ICMEW), P73, DOI 10.1109/ICMEW.2017.8026231
   Vondrick C, 2016, 30 C NEURAL INFORM P, V29
   Wang WG, 2018, PROC CVPR IEEE, P4894, DOI 10.1109/CVPR.2018.00514
   Xiong B, 2018, LECT NOTES COMPUT SC, V11209, P3, DOI 10.1007/978-3-030-01228-1_1
   Xu M, 2019, IEEE T CIRC SYST VID, V29, P3516, DOI [10.1109/TCSVT.2018.2886277, 10.1080/17445302.2018.1558727]
   Xu M, 2019, IEEE T PATTERN ANAL, V41, P2693, DOI 10.1109/TPAMI.2018.2858783
   Xu M, 2017, IEEE T IMAGE PROCESS, V26, P369, DOI 10.1109/TIP.2016.2628583
   Xu M, 2015, IEEE I CONF COMP VIS, P3907, DOI 10.1109/ICCV.2015.445
   Xu YY, 2018, PROC CVPR IEEE, P5333, DOI 10.1109/CVPR.2018.00559
   Xu YY, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3887
   Yonetani R, 2016, LECT NOTES COMPUT SC, V9906, P187, DOI 10.1007/978-3-319-46475-6_12
   Zhang JM, 2016, IEEE T PATTERN ANAL, V38, P889, DOI 10.1109/TPAMI.2015.2473844
   Zhang MM, 2017, PROC CVPR IEEE, P3539, DOI 10.1109/CVPR.2017.377
   Zhong S.-h., 2013, AAAI Conference on Artificial Intelligence, P1063
   Zhu YC, 2018, SIGNAL PROCESS-IMAGE, V69, P15, DOI 10.1016/j.image.2018.05.010
NR 75
TC 24
Z9 28
U1 4
U2 29
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 748
EP 760
DI 10.1109/TMM.2020.2987682
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA QA6NL
UT WOS:000613560200016
DA 2024-07-18
ER

PT J
AU Que, Y
   Li, SL
   Lee, HJ
AF Que, Yue
   Li, Suli
   Lee, Hyo Jong
TI Attentive Composite Residual Network for Robust Rain Removal from Single
   Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Rain; Periodic structures; Task analysis; Machine learning; Image
   restoration; Decoding; Robustness; Attention mechanism; image deraining;
   rain removal; Res2Net; residual network
ID STREAKS REMOVAL; DECOMPOSITION; MODEL
AB In rainy conditions, imaging devices often capture degraded and blurry images. Most existing works on this problem focus on rain streak removal, but these approaches cannot handle the various types of rain found in images. In this paper, we propose a robust rain removal method for use with single images using an attentive composite residual network. We put forth a single-to-dual encoder-decoder structure, which consists of an attentive net that identifies regions containing rain components during encoding, followed by a dual-channel architecture which recovers the background and detail components of the identified regions during decoding. For the detail subnet, we designed a novel building block, namely a composite residual block (CRB), by constructing multiple residual connections among Res2Net modules. Additionally, we designed another attentive-CRB for the attentive net that uses a squeeze-and-excitation (SE)-Res2Net module, to build a channel-wise attention mechanism. We show that such a deep network can be trained end-to-end from rainy images and that it outperforms the previous state-of-the-art methods on datasets containing different types of rainy images. Experimental results also demonstrate the proposed model's superiority over the competitor's on real rain-affected images, recovering visually clean images and retaining good detail.
C1 [Que, Yue; Li, Suli; Lee, Hyo Jong] Jeonbuk Natl Univ, Div Comp Sci & Engn, CAIIT, Jeonju 54896, South Korea.
   [Li, Suli] Cangzhou Normal Univ, Coll Comp Sci & Engn, Cangzhou 061001, Peoples R China.
C3 Jeonbuk National University; Cangzhou Normal University
RP Lee, HJ (corresponding author), Jeonbuk Natl Univ, Div Comp Sci & Engn, CAIIT, Jeonju 54896, South Korea.
EM yque86@gmail.com; sli88@foxmail.com; hlee@jbnu.ac.kr
OI LI, SULI/0000-0003-0400-313X; QUE, YUE/0000-0001-7167-7279
FU Basic Science Research Program through the National Research Foundation
   of Korea (NRF) - Ministry of Education [2019R1D1A3A03103736]; China
   Scholarship Council
FX This work was supported in part by the Basic Science Research Program
   through the National Research Foundation of Korea (NRF) funded by the
   Ministry of Education underGrant 2019R1D1A3A03103736, and in part by the
   China Scholarship Council.
CR Bossu J, 2011, INT J COMPUT VISION, V93, P348, DOI 10.1007/s11263-011-0421-7
   Chen DY, 2014, IEEE T CIRC SYST VID, V24, P1430, DOI 10.1109/TCSVT.2014.2308627
   Chen YL, 2013, IEEE I CONF COMP VIS, P1968, DOI 10.1109/ICCV.2013.247
   De-An Huang, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P164, DOI 10.1109/ICME.2012.92
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Fu XY, 2017, PROC CVPR IEEE, P1715, DOI 10.1109/CVPR.2017.186
   Fu XY, 2017, IEEE T IMAGE PROCESS, V26, P2944, DOI 10.1109/TIP.2017.2691802
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   Garg K, 2004, PROC CVPR IEEE, P528
   Garg K, 2007, INT J COMPUT VISION, V75, P3, DOI 10.1007/s11263-006-0028-6
   Glorot X., 2011, JMLR Proceedings, V15, P315, DOI DOI 10.1002/ECS2.1832
   Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu JJ, 2019, IEEE WINT CONF APPL, P1043, DOI 10.1109/WACV.2019.00116
   Hu XW, 2019, PROC CVPR IEEE, P8014, DOI 10.1109/CVPR.2019.00821
   Huang DA, 2014, IEEE T MULTIMEDIA, V16, P83, DOI 10.1109/TMM.2013.2284759
   Huynh-Thu Q, 2008, ELECTRON LETT, V44, P800, DOI 10.1049/el:20080522
   Jiang TX, 2019, IEEE T IMAGE PROCESS, V28, P2089, DOI 10.1109/TIP.2018.2880512
   Jin Z., 2019, IEEE T MULTIMEDIA
   Kang LW, 2012, IEEE T IMAGE PROCESS, V21, P1742, DOI 10.1109/TIP.2011.2179057
   Kim JH, 2013, IEEE IMAGE PROC, P914, DOI 10.1109/ICIP.2013.6738189
   King DB, 2015, ACS SYM SER, V1214, P1
   Li G, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1056, DOI 10.1145/3240508.3240636
   Li RT, 2019, PROC CVPR IEEE, P1633, DOI 10.1109/CVPR.2019.00173
   Li SY, 2019, PROC CVPR IEEE, P3833, DOI 10.1109/CVPR.2019.00396
   Li X, 2018, LECT NOTES COMPUT SC, V11211, P262, DOI 10.1007/978-3-030-01234-2_16
   Li Y, 2017, IEEE T IMAGE PROCESS, V26, P3874, DOI 10.1109/TIP.2017.2708841
   Li Y, 2016, PROC CVPR IEEE, P2736, DOI 10.1109/CVPR.2016.299
   Liu X, 2019, PROC CVPR IEEE, P7000, DOI 10.1109/CVPR.2019.00717
   Luo Y, 2015, IEEE I CONF COMP VIS, P3397, DOI 10.1109/ICCV.2015.388
   Mao XJ, 2016, ADV NEUR IN, V29
   Pan JS, 2021, IEEE T PATTERN ANAL, V43, P2449, DOI 10.1109/TPAMI.2020.2969348
   Pan JS, 2018, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2018.00324
   Qian R, 2018, PROC CVPR IEEE, P2482, DOI 10.1109/CVPR.2018.00263
   Ren DW, 2019, PROC CVPR IEEE, P3932, DOI 10.1109/CVPR.2019.00406
   Son CH, 2016, IEEE T IMAGE PROCESS, V25, P2866, DOI 10.1109/TIP.2016.2556618
   Sun SH, 2014, IEEE IMAGE PROC, P4482, DOI 10.1109/ICIP.2014.7025909
   Tian JD, 2018, IEEE T MULTIMEDIA, V20, P2659, DOI 10.1109/TMM.2018.2808763
   Wang TY, 2019, PROC CVPR IEEE, P12262, DOI 10.1109/CVPR.2019.01255
   Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yang WH, 2020, IEEE T PATTERN ANAL, V42, P1377, DOI 10.1109/TPAMI.2019.2895793
   Yang WH, 2019, IEEE T IMAGE PROCESS, V28, P2948, DOI 10.1109/TIP.2019.2892685
   Yang WH, 2017, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2017.183
   Yang X, 2019, IEEE T MULTIMEDIA, V21, P2545, DOI 10.1109/TMM.2019.2908375
   Yang Y, 2019, IEEE T COMPUT IMAG, V5, P262, DOI 10.1109/TCI.2018.2889959
   Yu F, 2018, PROC CVPR IEEE, P2403, DOI 10.1109/CVPR.2018.00255
   Zhang H, 2020, IEEE T CIRC SYST VID, V30, P3943, DOI 10.1109/TCSVT.2019.2920407
   Zhang H, 2018, PROC CVPR IEEE, P695, DOI 10.1109/CVPR.2018.00079
   Zhang K, 2017, 2017 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)
   Zhang XP, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P461, DOI 10.1109/ICME.2006.262572
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhao H, 2017, IEEE T COMPUT IMAG, V3, P47, DOI 10.1109/TCI.2016.2644865
   Zhu L, 2017, IEEE I CONF COMP VIS, P2545, DOI 10.1109/ICCV.2017.276
NR 58
TC 18
Z9 18
U1 2
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3059
EP 3072
DI 10.1109/TMM.2020.3019680
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UU6IS
UT WOS:000698902000009
DA 2024-07-18
ER

PT J
AU Sim, K
   Yang, JC
   Lu, W
   Gao, XB
AF Sim, Kyohoon
   Yang, Jiachen
   Lu, Wen
   Gao, Xinbo
TI MaD-DLS: Mean and Deviation of Deep and Local Similarity for Image
   Quality Assessment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Visualization; Distortion; Image quality;
   Convolution; Standards; Neurons; Image quality assessment; deep feature
   map; weighted mean pooling; standard deviation pooling
ID STRUCTURAL SIMILARITY; INFORMATION; INDEX
AB When human visual system (HVS) looks at a scene, it extracts various features from the image about the scene to understand it. The extracted features are compared with the stored memory on the analogous scene to judge their similarity [1]. By analyzing to the similarity, HVS understands the scene presented on eyes. Based on the neurobiological basis, we propose a 2D full reference (FR) image quality assessment (IQA) method, named mean and deviation of deep and local similarity (MaD-DLS) that compares similarity between many original and distorted deep feature maps from convolutional neural networks (CNNs). MaD-DLS uses a deep learning algorithm, but since it uses the convolutional layers of a pre-trained model, it is free from training. For pooling of local quality scores within a deep similarity map, we employ two important descriptive statistics, (weighted) mean and standard deviation and name it mean and deviation (MaD) pooling. The two statistics each have the physical meaning: the weighted mean reflects effect of visual saliency on quality, whereas the standard deviation reflects effect of distortion distribution within the image on it. Experimental results show that MaD-DLS is superior or competitive to the existing methods and the MaD pooling is effective. The MATLAB source code of MaD-DLS will be available online soon.
C1 [Sim, Kyohoon; Yang, Jiachen] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Lu, Wen] Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China.
   [Gao, Xinbo] Xidian Univ, Sch Elect Engn, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
C3 Tianjin University; Xidian University; Xidian University
RP Yang, JC (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM tlarygns0211@tju.edu.cn; yangjiachen@tju.edu.cn; luwen@xidian.edu.cn;
   xbgao@mail.xidian.edu.cn
RI Yang, Jiachen/ABH-5032-2020
OI Yang, Jiachen/0000-0003-2558-552X; SIM, Kyohoon/0000-0002-5214-7675
FU National Natural Science Foundation of China [61871283]; Foundation of
   Pre-Research on Equipment of China [61400010304]; Major Civil-Military
   Integration Project in Tianjin, China [18ZXJMTG00170]
FX This work was supported in part by the National Natural Science
   Foundation of China (61871283), in part the Foundation of Pre-Research
   on Equipment of China (61400010304), and in part by Major Civil-Military
   Integration Project in Tianjin, China (18ZXJMTG00170).
CR [Anonymous], 2014, ABS14053531 CORR
   [Anonymous], 2003, FINAL REPORT VIDEO Q
   Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding K, 2020, COMP IMAGE QUALITY M
   Ding KY, 2022, IEEE T PATTERN ANAL, V44, P2567, DOI 10.1109/TPAMI.2020.3045810
   Gao F, 2017, NEUROCOMPUTING, V257, P104, DOI 10.1016/j.neucom.2017.01.054
   Gu K, 2016, IEEE T CYBERNETICS, V46, P284, DOI 10.1109/TCYB.2015.2401732
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Le Callet P., 2005, Subjective quality assessment irccyn/ivc database
   Li SN, 2011, IEEE T MULTIMEDIA, V13, P935, DOI 10.1109/TMM.2011.2152382
   Liu AM, 2012, IEEE T IMAGE PROCESS, V21, P1500, DOI 10.1109/TIP.2011.2175935
   LIVINGSTONE M, 1988, SCIENCE, V240, P740, DOI 10.1126/science.3283936
   Ponomarenko Nikolay, 2013, 2013 4th European Workshop on Visual Information Processing (EUVIP), P106
   Prashnani E, 2018, PROC CVPR IEEE, P1808, DOI 10.1109/CVPR.2018.00194
   Sampat MP, 2009, IEEE T IMAGE PROCESS, V18, P2385, DOI 10.1109/TIP.2009.2025923
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Sheikh HR, 2005, IEEE T IMAGE PROCESS, V14, P2117, DOI 10.1109/TIP.2005.859389
   Simonyan K., 2015, P 3 INT C LEARN REPR
   Snowden R, 2012, BASICVISION INTRO VI
   Somasundaran BV, 2018, IEEE IMAGE PROC, P525, DOI 10.1109/ICIP.2018.8451132
   Stangor C., 2018, Introduction to psychology, V1st
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tong YB, 2010, J IMAGING SCI TECHN, V54, DOI 10.2352/J.ImagingSci.Technol.2010.54.3.030503
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2002, IEEE SIGNAL PROC LET, V9, P81, DOI 10.1109/97.995823
   Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Yan B, 2019, IEEE T MULTIMEDIA, V21, P2957, DOI 10.1109/TMM.2019.2914883
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028
   Zhang L, 2012, IEEE IMAGE PROC, P1473, DOI 10.1109/ICIP.2012.6467149
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang W, 2016, IEEE T NEUR NET LEAR, V27, P1266, DOI 10.1109/TNNLS.2015.2461603
   Zhang XF, 2019, IEEE T IMAGE PROCESS, V28, P1163, DOI 10.1109/TIP.2018.2874283
NR 41
TC 46
Z9 46
U1 2
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 4037
EP 4048
DI 10.1109/TMM.2020.3037482
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XA2YU
UT WOS:000720519900010
DA 2024-07-18
ER

PT J
AU Song, G
   Tan, XY
AF Song, Ge
   Tan, Xiaoyang
TI Real-world Cross-modal Retrieval via Sequential Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Plugs; Task analysis; Data models; Learning systems; Brain modeling;
   Adaptation models; Technological innovation; Cross-modal retrieval;
   sequential learning; deep learning; meta learning
AB Cross-modal retrieval is playing an increasingly important role in our daily life with the explosive growth of multimedia data. However, its learning paradigm under real-life environments is less studied, and most existing approaches are developed in the pre-desired settings (e.g., unchanging modalities and explicitly modal-aligned samples). Inspired by the recent achievement in the field of cognition mechanism on how the human brain acquires knowledge, we present a new sequential learning method for real-world cross-modal retrieval. In this method, a unified model is maintained to capture the common knowledge of various modalities but are learned in a sequential manner such that it behaves adaptively according to the evolving distribution of different modalities, and needs no laborious alignment operations among multimodal data before learning. Furthermore, we reformulate the objective of optimization-based meta-learning and propose a novel meta-learning method to overcome the catastrophic forgetting encountered in sequential learning. Extensive experiments are conducted on four popular image-text multimodal datasets and a five-modal dataset, showing that our method achieves state-of-the-art cross-modal retrieval performance without explicit modal-alignment.
C1 [Song, Ge; Tan, Xiaoyang] Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 211106, Peoples R China.
   [Song, Ge; Tan, Xiaoyang] MIIT Key Lab Pattern Anal & Machine Intelligence, Nanjing 211106, Peoples R China.
   [Song, Ge; Tan, Xiaoyang] Collaborat Innovat Ctr Novel Software Technol & I, Nanjing 211106, Peoples R China.
C3 Nanjing University of Aeronautics & Astronautics
RP Tan, XY (corresponding author), Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 211106, Peoples R China.
EM sunge@nuaa.edu.cn; x.tan@nuaa.edu.cn
OI Song, Ge/0000-0002-2159-8203; Tan, Xiaoyang/0000-0002-2683-8667
FU National Science Foundation of China [61976115, 61672280, 61732006]; AI+
   Project of NUAA [56XZA18009, 6140312020413]; Jiangsu Innovation Program
   for Graduate Education [KYCX18_0307]; China Scholarship Council
   [201906830057]
FX This work was supported in part by National Science Foundation of China
   under Grants 61976115, 61672280, and 61732006, in part by AI+ Project of
   NUAA(56XZA18009), research Project no. 6140312020413, in part by Jiangsu
   Innovation Program forGraduate Education (KYCX18_0307), and in part by
   China Scholarship Council (201906830057).
CR Aljundi R, 2018, LECT NOTES COMPUT SC, V11207, P144, DOI 10.1007/978-3-030-01219-9_9
   Andrew G., 2013, ICML, P1247
   Andrychowicz M, 2016, ADV NEUR IN, V29
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Cao Y, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1445, DOI 10.1145/2939672.2939812
   Chaudhry A, 2018, LECT NOTES COMPUT SC, V11215, P556, DOI 10.1007/978-3-030-01252-6_33
   Chaudhry Arslan, 2019, P INT C LEARN REPR
   Deng C, 2018, IEEE T IMAGE PROCESS, V27, P3893, DOI 10.1109/TIP.2018.2821921
   Ding GG, 2014, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2014.267
   Faghri F, 2018, P BRIT MACH VIS C, P1
   Feng FX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P7, DOI 10.1145/2647868.2654902
   Gong YC, 2014, INT J COMPUT VISION, V106, P210, DOI 10.1007/s11263-013-0658-4
   Gu JX, 2018, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR.2018.00750
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang Y, 2018, PROC CVPR IEEE, P6163, DOI 10.1109/CVPR.2018.00645
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Jarvelin K., 2000, SIGIR Forum, V34, P41
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Kemény F, 2016, ACTA PSYCHOL, V164, P27, DOI 10.1016/j.actpsy.2015.10.009
   Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Li C, 2019, AAAI CONF ARTIF INTE, P176
   Li C, 2018, PROC CVPR IEEE, P4242, DOI 10.1109/CVPR.2018.00446
   Li ZZ, 2018, IEEE T PATTERN ANAL, V40, P2935, DOI 10.1109/TPAMI.2017.2773081
   Lin ZJ, 2015, PROC CVPR IEEE, P3864, DOI 10.1109/CVPR.2015.7299011
   Liu H, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1589, DOI 10.1145/3240508.3240684
   Lopez-Paz D, 2017, ADV NEUR IN, V30
   Mithun NC, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1856, DOI 10.1145/3240508.3240712
   Nguyen CV, 2018, INT C LEARN REPR
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Rusu A.A., 2016, ARXIV
   Serr`a J., 2018, ICML, volume 80 of Proceedings of Machine Learning Research, V80, P4555
   Shen YM, 2018, PROC CVPR IEEE, P3598, DOI 10.1109/CVPR.2018.00379
   Song G, 2019, IEEE T MULTIMEDIA, V21, P1261, DOI 10.1109/TMM.2018.2877122
   Wang KY, 2016, IEEE T PATTERN ANAL, V38, P2010, DOI 10.1109/TPAMI.2015.2505311
   Wang KY, 2013, IEEE I CONF COMP VIS, P2088, DOI 10.1109/ICCV.2013.261
   Wang L, 2016, PROC CVPR IEEE, P5005, DOI 10.1109/CVPR.2016.541
   Wei YC, 2017, IEEE T CYBERNETICS, V47, P449, DOI 10.1109/TCYB.2016.2519449
   Wu DY, 2019, PROC CVPR IEEE, P9061, DOI 10.1109/CVPR.2019.00928
   Wu YL, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P825, DOI 10.1145/3240508.3240521
   Xu X, 2019, WORLD WIDE WEB, V22, P657, DOI 10.1007/s11280-018-0541-x
   Yang EK, 2017, AAAI CONF ARTIF INTE, P1618
   Ye ZD, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P852, DOI 10.1145/3240508.3240560
   Yoon Jaehong, 2018, INT C LEARNING REPRE
   Zhan YB, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1137, DOI 10.1145/3240508.3240607
   Zhang DQ, 2014, AAAI CONF ARTIF INTE, P2177
   Zhou BL, 2014, ADV NEUR IN, V27
   Zhu YK, 2015, IEEE I CONF COMP VIS, P19, DOI 10.1109/ICCV.2015.11
NR 48
TC 7
Z9 7
U1 2
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1708
EP 1721
DI 10.1109/TMM.2020.3002177
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA SJ9EZ
UT WOS:000655830300018
DA 2024-07-18
ER

PT J
AU Turan, MAT
   Erzin, E
AF Turan, M. A. Tugtekin
   Erzin, Engin
TI Domain Adaptation for Food Intake Classification With Teacher/Student
   Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Microphones; Sensors; Acoustics; Monitoring; Adaptation models; Sensor
   phenomena and characterization; Data models; Transfer learning;
   knowledge distillation; dietary monitoring; food intake classification;
   throat microphone
ID SPEECH RECOGNITION; NEURAL-NETWORK; SENSORS
AB Automatic dietary monitoring (ADM) stands as a challenging application in wearable healthcare technologies. In this paper, we define an ADM to perform food intake classification (FIC) over throat microphone recordings. We investigate the use of transfer learning to design an improved FIC system. Although labeled data with acoustic close-talk microphones are abundant, throat data is scarce. Therefore, we propose a new adaptation framework based on teacher/student learning. The teacher network is trained over high-quality acoustic microphone recordings, whereas the student network distills deep feature extraction capacity of the teacher over a parallel dataset. Our approach allows us to transfer the representational capacity, adds robustness to the resulting model, and improves the FIC through throat microphone recordings. The classification problem is formulated as a spectra-temporal sequence recognition using the Convolutional LSTM (ConvLSTM) models. We evaluate the proposed approach using a large scale acoustic dataset collected from online recordings, an in-house food intake throat microphone dataset, and a parallel speech dataset. The bidirectional ConvLSTM network with the proposed domain adaptation approach consistently outperforms the SVM- and CNN-based baseline methods and attains 85.2% accuracy for the classification of 10 different food intake items. This translates to 17.8% accuracy improvement with the proposed domain adaptation.
C1 [Turan, M. A. Tugtekin] Koc Univ, Dept Elect & Elect Engn, TR-34450 Istanbul, Turkey.
   [Erzin, Engin] Koc Univ, KUIS AI Lab, Comp Engn Dept, TR-34450 Istanbul, Turkey.
   [Erzin, Engin] Koc Univ, Elect & Elect Engn Dept, TR-34450 Istanbul, Turkey.
C3 Koc University; Koc University; Koc University
RP Turan, MAT (corresponding author), Koc Univ, Dept Elect & Elect Engn, TR-34450 Istanbul, Turkey.
EM mturan@ku.edu.tr; eerzin@ku.edu.tr
RI Turan, Mehmet Ali Tuğtekin/ABI-1396-2020; Erzin, Engin/H-1716-2011
OI Turan, Mehmet Ali Tuğtekin/0000-0002-3822-235X; Erzin,
   Engin/0000-0002-2715-2368
CR Abdel-Hamid O, 2012, INT CONF ACOUST SPEE, P4277, DOI 10.1109/ICASSP.2012.6288864
   Amft O, 2005, LECT NOTES COMPUT SC, V3660, P56
   Amft O, 2009, IEEE PERVAS COMPUT, V8, P62, DOI 10.1109/MPRV.2009.32
   Chan W, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P3264
   Chebotar Y, 2016, INTERSPEECH, P3439, DOI 10.21437/Interspeech.2016-1190
   Cui XD, 2015, INT CONF ACOUST SPEE, P4545, DOI 10.1109/ICASSP.2015.7178831
   Dekens T, 2010, EUR SIGNAL PR CONF, P1978
   Dey N, 2019, SPRINGERBRIEF SPEECH, P43, DOI 10.1007/978-3-319-92225-6_5
   Erzin E, 2009, IEEE T AUDIO SPEECH, V17, P1316, DOI 10.1109/TASL.2009.2016733
   Graves A, 2013, 2013 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P273, DOI 10.1109/ASRU.2013.6707742
   Greff K, 2017, IEEE T NEUR NET LEAR, V28, P2222, DOI 10.1109/TNNLS.2016.2582924
   Hammerla N.Y., 2016, P 25 INT JOINT C ART
   Hannun Awni, 2014, ARXIV
   Hinton G., 2015, COMPUT SCI, V2
   Jacobs D.R., 2012, Nutritional Health, P29
   Jaitly N., 2014, P INT C MACH LEARN, V117
   Jiang Jing, 2007, ANN M ASS COMP LING, P264, DOI DOI 10.1145/1273496.1273558
   Kalantarian H, 2014, 2014 IEEE HEALTHCARE INNOVATION CONFERENCE (HIC), P161, DOI 10.1109/HIC.2014.7038899
   Kim HJ, 2012, ASIAPAC SIGN INFO PR
   Ko T, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P3586
   Korattikara A, 2015, ADV NEUR IN, V28
   Li J., 2017, ARXIV170805466
   Li JY, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P5699, DOI 10.1109/ICASSP.2018.8462209
   Ma XL, 2015, TRANSPORT RES C-EMER, V54, P187, DOI 10.1016/j.trc.2015.03.014
   Makeyev O, 2012, BIOMED SIGNAL PROCES, V7, P649, DOI 10.1016/j.bspc.2012.03.005
   Markov K, 2016, INTERSPEECH, P2364, DOI 10.21437/Interspeech.2016-852
   Mirtchouk M, 2016, UBICOMP'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING, P451, DOI 10.1145/2971648.2971677
   Nishimura J, 2008, 2008 3RD INTERNATIONAL SYMPOSIUM ON WIRELESS PERVASIVE COMPUTING, VOLS 1-2, P130, DOI 10.1109/ISWPC.2008.4556181
   Olubanjo Temiloluwa, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P4384, DOI 10.1109/ICASSP.2014.6854430
   Pässler S, 2012, PHYSIOL MEAS, V33, P1073, DOI 10.1088/0967-3334/33/6/1073
   Panwar L. S., 2017, 2017 3 INT C ADV COM, P1
   Papapanagiotou V, 2017, IEEE ENG MED BIO, P1258, DOI 10.1109/EMBC.2017.8037060
   Patel MS, 2017, ANN INTERN MED, V167, P755, DOI 10.7326/M17-1495
   Sato K, 2006, ANN OTO RHINOL LARYN, V115, P334, DOI 10.1177/000348940611500503
   Sazonov ES, 2010, IEEE T BIO-MED ENG, V57, P626, DOI 10.1109/TBME.2009.2033037
   SCHOELLER DA, 1990, CAN J PHYSIOL PHARM, V68, P941, DOI 10.1139/y90-143
   Shengjie Bi, 2018, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, V2, DOI 10.1145/3264902
   Shi XJ, 2015, ADV NEUR IN, V28
   Shuzo M, 2010, J ADV MECH DES SYST, V4, P158, DOI 10.1299/jamdsm.4.158
   STELLAR E, 1985, AM J CLIN NUTR, V42, P973, DOI 10.1093/ajcn/42.5.973
   Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35
   Tang Y, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P362, DOI 10.1145/3206025.3206052
   Turan M.A. T., 2017, Proceedings of the 2nd International Workshop on Multimedia for Personal Health and Health Care, P45, DOI DOI 10.1145/3132635.3132640
   Turan MAT, 2018, IEEE INT CONF MULTI
   Turan MAT, 2016, IEEE-ACM T AUDIO SPE, V24, P265, DOI 10.1109/TASLP.2015.2499040
   Wang Wenguan, 2018, IEEE Trans Image Process, V27, P38, DOI 10.1109/TIP.2017.2754941
   Weiss Karl, 2016, Journal of Big Data, V3, DOI 10.1186/s40537-016-0043-6
   Xiong W, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P5934, DOI 10.1109/ICASSP.2018.8461870
   Yatani K, 2012, UBICOMP'12: PROCEEDINGS OF THE 2012 ACM INTERNATIONAL CONFERENCE ON UBIQUITOUS COMPUTING, P341
   Yi J., 2018, ARXIV180206941
   Zhang S, 2009, SENSORS-BASEL, V9, P1499, DOI 10.3390/s90301499
   Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907
NR 52
TC 1
Z9 1
U1 3
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 4220
EP 4231
DI 10.1109/TMM.2020.3038315
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XA2YU
UT WOS:000720519900024
DA 2024-07-18
ER

PT J
AU Wu, JJ
   Ma, CW
   Li, LD
   Dong, WS
   Shi, GM
AF Wu, Jinjian
   Ma, Chuanwei
   Li, Leida
   Dong, Weisheng
   Shi, Guangming
TI Probabilistic Undirected Graph Based Denoising Method for Dynamic Vision
   Sensor
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Voltage control; Noise reduction; Streaming media; Correlation;
   Probabilistic logic; Adaptation models; Hardware; Dynamic Vision Sensor;
   Event Stream; Denoising; Probabilistic Undirected Graph Model
ID IMAGE; EVENTS
AB Dynamic Vision Sensor (DVS) is a new type of neuromorphic event-based sensor, which has an innate advantage in capturing fast-moving objects. Due to the interference of DVS hardware itself and many external factors, noise is unavoidable in the output of DVS. Different from frame/image with structural data, the output of DVS is in the form of address-event representation (AER), which means that the traditional denoising methods cannot be used for the output (i.e., event stream) of the DVS. In this paper, we propose a novel event stream denoising method based on probabilistic undirected graph model (PUGM). The motion of objects always shows a certain regularity/trajectory in space and time, which reflects the spatio-temporal correlation between effective events in the stream. Meanwhile, the event stream of DVS is composed by the effective events and random noise. Thus, a probabilistic undirected graph model is constructed to describe such priori knowledge (i.e., spatio-temporal correlation). The undirected graph model is factorized into the product of the cliques energy function, and the energy function is defined to obtain the complete expression of the joint probability distribution. Better denoising effect means a higher probability (lower energy), which means the denoising problem can be transfered into energy optimization problem. Thus, the iterated conditional modes (ICM) algorithm is used to optimize the model to remove the noise. Experimental results on denoising show that the proposed algorithm can effectively remove noise events. Moreover, with the preprocessing of the proposed algorithm, the recognition accuracy on AER data can be remarkably promoted. The source code of the proposed method is available at https://web.xidian.edu.cn/wjj/paper.html.
C1 [Wu, Jinjian; Ma, Chuanwei; Li, Leida; Dong, Weisheng; Shi, Guangming] Xidian Univ, Sch Artificial Intelligence, Xian 710071, Peoples R China.
C3 Xidian University
RP Wu, JJ (corresponding author), Xidian Univ, Sch Artificial Intelligence, Xian 710071, Peoples R China.
EM jinjian.wu@mail.xidian.edu.cn; cwma24@163.com; lileida@cumt.edu.cn;
   wsdong@mail.xidian.edu.cn; gmshi@xidian.edu.cn
RI Li, Li/AEM-3636-2022; Wu, Jinjian/GQH-0222-2022; li, li/HII-4157-2022
FU NSF of China [61772388, 61632019]; National Key R&D Program of China
   [2018AAA0101400]
FX This work was supported in part by the NSF of China under Grants
   61772388 and 61632019 and in part by the National Key R&D Program of
   China under Grant 2018AAA0101400. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr.
   Xiaoqing Zhu. (Corresponding author: Jinjian Wu.)
CR Pérez-Carrasco JA, 2013, IEEE T PATTERN ANAL, V35, P2706, DOI 10.1109/TPAMI.2013.71
   Arias P, 2018, J MATH IMAGING VIS, V60, P70, DOI 10.1007/s10851-017-0742-4
   Arias P, 2015, LECT NOTES COMPUT SC, V9386, P107, DOI 10.1007/978-3-319-25903-1_10
   BARDOW P, 2016, PROC CVPR IEEE, P884, DOI DOI 10.1109/CVPR.2016.102
   Bevilacqua A, 2004, IEEE J SOLID-ST CIRC, V39, P2259, DOI 10.1109/JSSC.2004.836338
   Bishop Christopher M., 2006, Pattern Recognition and Machine Learning, V4
   Boulanger J, 2007, IEEE T PATTERN ANAL, V29, P1096, DOI 10.1109/TPAMI.2007.1064
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Fossum ER, 1997, IEEE T ELECTRON DEV, V44, P1689, DOI 10.1109/16.628824
   GALLEGO G, 2019, ARXIV190408405 CORR
   Guo H, 2016, IEEE INT SYMP CIRC S, P1422, DOI 10.1109/ISCAS.2016.7527517
   HEDENSTIERNA N, 1987, IEEE T COMPUT AID D, V6, P270, DOI 10.1109/TCAD.1987.1270271
   Huang DA, 2014, IEEE T MULTIMEDIA, V16, P83, DOI 10.1109/TMM.2013.2284759
   Jia QS, 2012, I C CONT AUTOMAT ROB, P240, DOI 10.1109/ICARCV.2012.6485165
   Lan XY, 2006, LECT NOTES COMPUT SC, V3952, P269
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li S. Z., 2009, Markov random field modeling in image analysis
   Lichtsteiner Patrick, 2008, IEEE Journal of Solid-State Circuits, V43, P566, DOI 10.1109/JSSC.2007.914337
   Maggioni M, 2011, PROC SPIE, V7870, DOI 10.1117/12.872569
   Mairal J, 2009, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2009.5459452
   Moghaddam B, 2007, IEEE I CONF COMP VIS, P2073, DOI 10.1109/cvpr.2007.383092
   Osher S, 2005, MULTISCALE MODEL SIM, V4, P460, DOI 10.1137/040605412
   Peng X, 2017, IEEE T NEUR NET LEAR, V28, P791, DOI 10.1109/TNNLS.2016.2536741
   Roth S, 2009, INT J COMPUT VISION, V82, P205, DOI 10.1007/s11263-008-0197-6
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Serrano-Gotarredona T, 2013, IEEE J SOLID-ST CIRC, V48, P827, DOI 10.1109/JSSC.2012.2230553
   Xie J, 2015, IEEE T MULTIMEDIA, V17, P1525, DOI 10.1109/TMM.2015.2457678
   Xie XM, 2017, PROCEEDINGS OF 2017 INTERNATIONAL CONFERENCE ON VIDEO AND IMAGE PROCESSING (ICVIP 2017), P176, DOI 10.1145/3177404.3177411
   Xu JT, 2016, OPT ENG, V55, DOI 10.1117/1.OE.55.6.063103
   Xu J, 2015, IEEE I CONF COMP VIS, P244, DOI 10.1109/ICCV.2015.36
   Zhang L, 2018, OPT ENG, V57, DOI 10.1117/1.OE.57.8.083102
   Zhao B, 2015, IEEE T NEUR NET LEAR, V26, P1963, DOI 10.1109/TNNLS.2014.2362542
NR 32
TC 14
Z9 16
U1 2
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1148
EP 1159
DI 10.1109/TMM.2020.2993957
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XG0UQ
UT WOS:000724477100001
DA 2024-07-18
ER

PT J
AU Yin, JL
   Chen, BH
   Peng, YT
   Tsai, CC
AF Yin, Jia-Li
   Chen, Bo-Hao
   Peng, Yan-Tsung
   Tsai, Chung-Chi
TI Deep Battery Saver: End-to-End Learning for Power Constrained Contrast
   Enhancement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Power demand; Image color analysis; Batteries; Image resolution; Task
   analysis; Organic light emitting diodes; Green computing; Organic
   lightemitting diodes; power-constrained contrast enhancement; deep
   learning
AB Due to the problems of power-hungry displays and limited battery life in electronic devices, the concept of "green computing," which entails a reduction in power consumption, is proposed. One often seen green computing is the power-constrained contrast enhancement (PCCE), yet it is much more challenging because of the noticeable local intensity suppressions in images. This paper aims at developing an image-quality-lossless end-to-end learning network called deep battery saver to achieve power savings in emissive displays, i.e., produce power-saved images with high perceptual quality and less power consumption. Built upon the end-to-end network of the displayed image, we propose a variational loss function for enhancing the visual quality and suppressing the power consumption, simultaneously. The basic idea is to integrate both high-level perceptual losses and low-level pixel losses by a deep residual convolutional neural network (CNN) over a devised variational loss function with strong human perceptual consistency. Such deep residual CNN network leads to a visually pleasing image representation during the suppression of power consumption. Experimental results demonstrated the superiority of our deep battery saver to existing PCCE methods.
C1 [Yin, Jia-Li; Chen, Bo-Hao] Yuan Ze Univ, Dept Comp Sci & Engn, Taoyuan 335002, Taiwan.
   [Peng, Yan-Tsung] Natl Chengchi Univ, Dept Comp Sci, Pervas Artificial Intelligence Res PAIR Labs, Taipei 116302, Taiwan.
   [Tsai, Chung-Chi] Qualcomm Technol Inc, San Diego, CA 92121 USA.
C3 Yuan Ze University; National Chengchi University; Qualcomm
RP Chen, BH (corresponding author), Yuan Ze Univ, Dept Comp Sci & Engn, Taoyuan 335002, Taiwan.; Peng, YT (corresponding author), Natl Chengchi Univ, Dept Comp Sci, Pervas Artificial Intelligence Res PAIR Labs, Taipei 116302, Taiwan.
EM s1079104@mail.yzu.edu.tw; bhchen@saturn.yzu.edu.tw;
   ytpeng@cs.nccu.edu.tw; chuntsai@qti.qualcomm.com
RI Peng, Yan-Tsung/AGW-3513-2022; Tsai, Chung-Chi/W-9145-2018
OI Peng, Yan-Tsung/0000-0002-3802-1670; Tsai, Chung-Chi/0000-0003-1792-9978
FU Ministry of Science and Technology, Taiwan (MOST) through MOST AI
   Biomedical Research Center at NCKU [MOST 109-2634-F-019-001, MOST
   108-2221-E-155-034-MY3, MOST 107-2221-E-155-052-MY2]; Ministry of
   Science and Technology, Taiwan (MOST) through Pervasive Artificial
   Intelligence Research (PAIR) Labs [MOST 108-2221-E-155-034-MY3, MOST
   107-2221-E-155-052-MY2, MOST 109-2634-F-004-001]; Qualcomm through a
   Taiwan University Research Collaboration Project [NAT-414673]
FX This work was supported in part by the Ministry of Science and
   Technology, Taiwan (MOST) under Grants MOST 108-2221-E-155-034-MY3 and
   MOST 107-2221-E-155-052-MY2, through MOST AI Biomedical Research Center
   at NCKU under Grant MOST 109-2634-F-019-001, through Pervasive
   Artificial Intelligence Research (PAIR) Labs underGrant MOST
   109-2634-F-004-001, and in part by Qualcomm through a Taiwan University
   Research Collaboration Project under Grant NAT-414673.
CR Aly HA, 2005, IEEE T IMAGE PROCESS, V14, P1647, DOI 10.1109/TIP.2005.851684
   Baroncini V., 2009, 2009 17th European Signal Processing Conference (EUSIPCO 2009), P564
   Chang CH, 2011, IEEE T MULTIMEDIA, V13, P589, DOI 10.1109/TMM.2011.2116775
   Chen B.-H, 2016, P ACM INT C BIG DAT
   Chondro P, 2018, IEEE T CIRC SYST VID, V28, P2200, DOI 10.1109/TCSVT.2017.2723600
   Fattal R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531328
   Ge F, 2018, IEEE ACCESS, V6
   Gu K, 2018, IEEE T NEUR NET LEAR, V29, P1301, DOI 10.1109/TNNLS.2017.2649101
   Hadizadeh H, 2017, IEEE T IMAGE PROCESS, V26, P2882, DOI 10.1109/TIP.2017.2690523
   Huang TH, 2008, IEEE IMAGE PROC, P1752, DOI 10.1109/ICIP.2008.4712114
   Ignatov A, 2017, IEEE I CONF COMP VIS, P3297, DOI 10.1109/ICCV.2017.355
   Jang DW, 2017, IEEE T IMAGE PROCESS, V26, P2561, DOI 10.1109/TIP.2017.2687125
   Jang JH, 2012, IEEE T IMAGE PROCESS, V21, P3479, DOI 10.1109/TIP.2012.2197014
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kang SJ, 2016, J DISP TECHNOL, V12, P519, DOI 10.1109/JDT.2015.2502273
   Kang SJ, 2015, J DISP TECHNOL, V11, P104, DOI 10.1109/JDT.2014.2363086
   Kang SJ, 2015, J DISP TECHNOL, V11, P79, DOI 10.1109/JDT.2014.2361513
   Kang SJ, 2014, IEEE T CIRC SYST VID, V24, P224, DOI 10.1109/TCSVT.2013.2273655
   Kim DH, 2005, IEEE ICCE, P185, DOI 10.1109/ICCE.2005.1429779
   Kumar N, 2018, IEEE T MULTIMEDIA, V20, P298, DOI 10.1109/TMM.2017.2729021
   Lai CC, 2008, IEEE T CONSUM ELECTR, V54, P669, DOI 10.1109/TCE.2008.4560145
   Lai EH, 2018, IEEE 1ST CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2018), P309, DOI 10.1109/MIPR.2018.00071
   Lee C, 2012, IEEE T IMAGE PROCESS, V21, P80, DOI 10.1109/TIP.2011.2159387
   Lombardo A, 2014, IEEE T MULTIMEDIA, V16, P2307, DOI 10.1109/TMM.2014.2350257
   Lu SP, 2015, IEEE T MULTIMEDIA, V17, P577, DOI 10.1109/TMM.2015.2412879
   Mahendran A, 2015, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR.2015.7299155
   Nam YO, 2014, IEEE T IMAGE PROCESS, V23, P3308, DOI 10.1109/TIP.2014.2324288
   Niu YZ, 2018, IEEE T CIRC SYST VID, V28, P849, DOI 10.1109/TCSVT.2016.2634590
   Pei SC, 2017, IEEE T MULTIMEDIA, V19, P1956, DOI 10.1109/TMM.2017.2688924
   Pitié F, 2005, IEEE I CONF COMP VIS, P1434
   Shi Y, 2017, IEEE T MULTIMEDIA, V19, P2804, DOI 10.1109/TMM.2017.2711263
   Shiau YH, 2015, IEEE T INTELL TRANSP, V16, P934, DOI 10.1109/TITS.2014.2347701
   Tsai PS, 2009, IEEE T CIRC SYST VID, V19, P574, DOI 10.1109/TCSVT.2009.2014022
   Wang SQ, 2016, IEEE T MULTIMEDIA, V18, P219, DOI 10.1109/TMM.2015.2510326
   Wu JJ, 2017, IEEE T IMAGE PROCESS, V26, P2682, DOI 10.1109/TIP.2017.2685682
   Xiao Xuezhong, 2006, PACM INT C VIRT REAL, P305, DOI DOI 10.1145/1128923.1128974
   Xu CY, 2019, PATTERN RECOGN LETT, V119, P34, DOI 10.1016/j.patrec.2017.08.007
   Xu HT, 2014, IEEE T MULTIMEDIA, V16, P68, DOI 10.1109/TMM.2013.2283453
   Yang WM, 2016, IEEE T MULTIMEDIA, V18, P313, DOI 10.1109/TMM.2016.2515997
   Yin JL, 2020, IEEE T CIRC SYST VID, V30, P2477, DOI 10.1109/TCSVT.2019.2925208
   Yin JL, 2018, IEEE T MULTIMEDIA, V20, P3045, DOI 10.1109/TMM.2018.2820910
NR 41
TC 12
Z9 12
U1 2
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1049
EP 1059
DI 10.1109/TMM.2020.2992962
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA QO8XK
UT WOS:000623420300017
DA 2024-07-18
ER

PT J
AU Zhang, S
   Liu, YL
   Jin, LW
   Wei, ZR
   Shen, CH
AF Zhang, Sheng
   Liu, Yuliang
   Jin, Lianwen
   Wei, Zhongrong
   Shen, Chunhua
TI OPMP: An Omnidirectional Pyramid Mask Proposal Network for
   Arbitrary-Shape Scene Text Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Proposals; Feature extraction; Image segmentation; Detectors; Shape;
   Robustness; Benchmark testing; Text detection; pyramid sequence
   modeling; omnidirectional pyramid mask proposal
ID STROKE FEATURE TRANSFORM
AB Scene text detection methods have achieved significant progresses. However, stack-omnidirectional text dilemma, under-segmentation of very close text words, and over-segmentation of arbitrary-shape long text lines, are still main challenges. Motivated by these problems, we proposed a two stage method called omnidirectional pyramid mask proposal text detector (OPMP). OPMP removes anchor mechanism that requires heuristic non-maximum suppress processing. Instead, it uses an effective pyramid lengthwise and sidewise residual sequence modeling method to produce arbitrary-shape proposals. To accurately extract the features of text shape, OPMP enhances the backbone layers by a multiple arbitrary-shape fitting mechanism. Finally, a multi-grain text classification module is proposed, which reclassifies each text region robustly. Comprehensive ablation studies demonstrate the effectiveness of each proposed component. In addition, experiments on various benchmarks, including ICDAR2015, MLT, MSRA-TD500, CTW1500, and Total-text, show that our method outperforms previous state-of-the-art methods.
C1 [Zhang, Sheng; Liu, Yuliang; Jin, Lianwen; Wei, Zhongrong] South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510641, Peoples R China.
   [Liu, Yuliang] Univ Adelaide, Adelaide, SA 5005, Australia.
   [Shen, Chunhua] Univ Adelaide, Sch Comp Sci, Adelaide, SA 5005, Australia.
C3 South China University of Technology; University of Adelaide; University
   of Adelaide
RP Jin, LW (corresponding author), South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510641, Peoples R China.
EM zsscut@sina.com; liu.yuliang@mail.scut.edu.cn; lianwen.jin@gmail.com;
   zrwei@foxmail.com; chunhua.shen@adelaide.edu.au
OI Shen, Chunhua/0000-0002-8648-8718; Jin, Lianwen/0000-0002-5456-0957;
   Liu, Yuliang/0000-0002-3037-173X
FU NSFC [61936003]; National Key Research and Development Program of China
   [2016YFB1001405]; Guangdong Intellectual Property Office Project
   [2018-10-1]; GZSTP [201704020134]; GD-NSF [2017A030312006]
FX This work was supported in part by NSFC under Grants 61936003 and GD-NSF
   2017A030312006, in part by the National Key Research and Development
   Program of China under Grant 2016YFB1001405, in part by Guangdong
   Intellectual Property Office Project 2018-10-1, and in part by GZSTP
   under Grant 201704020134.
CR Bai Nong, 2016, ARXIV160609002
   Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314
   Ch'ng CK, 2017, PROC INT CONF DOC, P935, DOI 10.1109/ICDAR.2017.157
   Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709
   Cheng ZZ, 2018, PROC CVPR IEEE, P5571, DOI 10.1109/CVPR.2018.00584
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Deng D, 2018, AAAI CONF ARTIF INTE, P6773
   Epshtein B, 2010, PROC CVPR IEEE, P2963, DOI 10.1109/CVPR.2010.5540041
   Gómez L, 2013, PROC INT CONF DOC, P467, DOI 10.1109/ICDAR.2013.100
   Gupta A, 2016, PROC CVPR IEEE, P2315, DOI 10.1109/CVPR.2016.254
   He DF, 2017, PROC CVPR IEEE, P474, DOI 10.1109/CVPR.2017.58
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He P, 2017, IEEE I CONF COMP VIS, P3066, DOI 10.1109/ICCV.2017.331
   He T, 2016, IEEE T IMAGE PROCESS, V25, P2529, DOI 10.1109/TIP.2016.2547588
   He WH, 2017, IEEE I CONF COMP VIS, P745, DOI 10.1109/ICCV.2017.87
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hu H, 2017, IEEE I CONF COMP VIS, P4950, DOI 10.1109/ICCV.2017.529
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   Huang WQ, 2014, 2014 2ND INTERNATIONAL CONFERENCE ON SYSTEMS AND INFORMATICS (ICSAI), P497, DOI 10.1109/ICSAI.2014.7009338
   Huang WL, 2013, IEEE I CONF COMP VIS, P1241, DOI 10.1109/ICCV.2013.157
   Karaoglu S, 2017, IEEE T MULTIMEDIA, V19, P1063, DOI 10.1109/TMM.2016.2638622
   Karatzas D, 2015, PROC INT CONF DOC, P1156, DOI 10.1109/ICDAR.2015.7333942
   Li H, 2019, IEEE T INTELL TRANSP, V20, P1126, DOI 10.1109/TITS.2018.2847291
   Li H, 2017, IEEE I CONF COMP VIS, P5248, DOI 10.1109/ICCV.2017.560
   Li Y, 2014, IEEE T IMAGE PROCESS, V23, P1666, DOI 10.1109/TIP.2014.2302896
   Liao MH, 2018, PROC CVPR IEEE, P5909, DOI 10.1109/CVPR.2018.00619
   Liao MH, 2018, IEEE T IMAGE PROCESS, V27, P3676, DOI 10.1109/TIP.2018.2825107
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu XB, 2018, PROC CVPR IEEE, P5676, DOI 10.1109/CVPR.2018.00595
   Liu YL, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3052
   Liu YL, 2019, PROC CVPR IEEE, P9604, DOI 10.1109/CVPR.2019.00984
   Liu YL, 2019, PATTERN RECOGN, V90, P337, DOI 10.1016/j.patcog.2019.02.002
   Liu ZC, 2019, PROC CVPR IEEE, P7261, DOI 10.1109/CVPR.2019.00744
   Liu ZC, 2018, PROC CVPR IEEE, P6936, DOI 10.1109/CVPR.2018.00725
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Long SB, 2018, LECT NOTES COMPUT SC, V11206, P19, DOI 10.1007/978-3-030-01216-8_2
   Lyu PY, 2018, PROC CVPR IEEE, P7553, DOI 10.1109/CVPR.2018.00788
   Ma JQ, 2018, IEEE T MULTIMEDIA, V20, P3111, DOI 10.1109/TMM.2018.2818020
   Matas J, 2004, IMAGE VISION COMPUT, V22, P761, DOI 10.1016/j.imavis.2004.02.006
   Nayef N, 2017, PROC INT CONF DOC, P1454, DOI 10.1109/ICDAR.2017.237
   Neubeck A, 2006, INT C PATT RECOG, P850, DOI 10.1109/icpr.2006.479
   Neumann L, 2012, PROC CVPR IEEE, P3538, DOI 10.1109/CVPR.2012.6248097
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ren XH, 2017, IEEE T MULTIMEDIA, V19, P506, DOI 10.1109/TMM.2016.2625259
   Shi BG, 2017, PROC CVPR IEEE, P3482, DOI 10.1109/CVPR.2017.371
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun IY, 2018, PALG S RACE ETHN IND, P1, DOI 10.1007/978-1-349-95807-8
   Tang J, 2019, PATTERN RECOGN, V96, DOI 10.1016/j.patcog.2019.06.020
   Tang YB, 2018, IEEE T MULTIMEDIA, V20, P2276, DOI 10.1109/TMM.2018.2802644
   Tian Z, 2016, LECT NOTES COMPUT SC, V9912, P56, DOI 10.1007/978-3-319-46484-8_4
   Veit Andreas, 2016, Coco-text: Dataset and benchmark for text detection and recognition in natural images
   Wang K, 2010, LECT NOTES COMPUT SC, V6311, P591, DOI 10.1007/978-3-642-15549-9_43
   Wu L, 2015, IEEE T MULTIMEDIA, V17, P1137, DOI 10.1109/TMM.2015.2443556
   Wu Y, 2017, IEEE I CONF COMP VIS, P5010, DOI 10.1109/ICCV.2017.535
   Xie EZ, 2019, AAAI CONF ARTIF INTE, P9038
   Xu YC, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2900589
   Xue C, 2018, ROUT RES ARCHIT, P35
   Xue CH, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P989
   Yan CG, 2018, IEEE T MULTIMEDIA, V20, P3389, DOI 10.1109/TMM.2018.2838320
   Yang QP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1071
   Yao C, 2014, IEEE T IMAGE PROCESS, V23, P4737, DOI 10.1109/TIP.2014.2353813
   Yao C, 2012, PROC CVPR IEEE, P1083, DOI 10.1109/CVPR.2012.6247787
   Ye H, 2017, IEEE TRANSP EL ASIA, P1196
   Ye QX, 2015, IEEE T PATTERN ANAL, V37, P1480, DOI 10.1109/TPAMI.2014.2366765
   Yin XC, 2015, IEEE T PATTERN ANAL, V37, P1930, DOI 10.1109/TPAMI.2014.2388210
   Zhang C., 2019, P IEEE C COMP VIS PA
   Zhang S, 2018, AAAI CONF ARTIF INTE, P2612
   Zhong ZY, 2019, INT J DOC ANAL RECOG, V22, P315, DOI 10.1007/s10032-019-00335-y
   Zhong ZY, 2017, INT CONF ACOUST SPEE, P1208, DOI 10.1109/ICASSP.2017.7952348
   Zhou XY, 2017, PROC CVPR IEEE, P2642, DOI 10.1109/CVPR.2017.283
   Zhu YX, 2018, INT C PATT RECOG, P3735, DOI 10.1109/ICPR.2018.8545067
NR 74
TC 26
Z9 29
U1 0
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 454
EP 467
DI 10.1109/TMM.2020.2978630
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA PJ6LW
UT WOS:000601877600036
DA 2024-07-18
ER

PT J
AU Zhang, YM
   Cai, ZC
   Xiong, GQ
AF Zhang, Yumo
   Cai, Zhanchuan
   Xiong, Gangqiang
TI A New Image Compression Algorithm Based on Non-Uniform Partition and
   U-System
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image compression; JPEG; DCT; U-system; non-uniform rectangular
   partition
AB JPEG lossy image compression is a still image compression algorithm model that is currently widely used in major network media. However, it is unsatisfactory in the quality of compressed images at low bit rates. The objective of this paper is to improve the quality of compressed images and suppress blocking artifacts by improving the JPEG image compression model at low bit rates. First, the image texture adaptive non-uniformrectangular partition (ITANRP) algorithm is proposed which partitions the image into 8 x 8 size image blocks with high texture complexity and 16 x 16 size image blocks with lowtexture complexity. Then, a new transform coding based on the complete orthogonal U-system and all-phase digital filter (APDF) is proposed for coding image blocks with different sizes. Next, a flexible adaptive quantization scheme is designed to quantize image blocks with different sizes by considering the sensitivity of the human visual system (HVS) to different texture complexities. Finally, combining the above method with the JPEGmodel, a novel image compression algorithm model with low algorithm complexity is proposed to solve the problem in JPEG. The experimental results demonstrate that the performance of our algorithm model outperforms the JPEG image compression algorithms, the quality of the compressed image is greatly improved, and the blocking artifacts are also significantly suppressed.
C1 [Zhang, Yumo; Cai, Zhanchuan] Macau Univ Sci & Technol, Fac Informat Technol, Macau 999078, Peoples R China.
   [Xiong, Gangqiang] Guangdong Med Univ, Sch Biomed Engn, Dongguan 523000, Peoples R China.
C3 Macau University of Science & Technology; Guangdong Medical University
RP Cai, ZC (corresponding author), Macau Univ Sci & Technol, Fac Informat Technol, Macau 999078, Peoples R China.
EM yumozhang0221@gmail.com; zccai@must.edu.mo; douglasxiong@qq.com
OI Zhang, Yumo/0000-0001-7905-8763
FU National Basic Research Program of China (973 Program) [2011CB302400];
   Science and Technology Development Fund of Macau [0012/2018/A1,
   0069/2018/A2]; Open Project Program of State Key Laboratory of Virtual
   Reality Technology and Systems, Beihang University [VRLAB2019C02]; Open
   Project Program of the State Key Lab of CAD&CG of Zhejiang University
   [A1910]; National Natural Science Foundation of China [61170320,
   81871433]
FX This work was supported in part by the National Basic Research Program
   of China (973 Program) under Grant 2011CB302400, in part by the Science
   andTechnology Development Fund of Macau underGrants 0012/2018/A1 and
   0069/2018/A2, in part by the Open Project Program of State Key
   Laboratory of Virtual Reality Technology and Systems, Beihang University
   under Grant VRLAB2019C02, in part by the Open Project Program of the
   State Key Lab of CAD&CG of Zhejiang University under Grant A1910, and in
   part by the National Natural Science Foundation of China under Grants
   61170320 and 81871433.
CR AHMED N, 1974, IEEE T COMPUT, VC 23, P90, DOI 10.1109/T-C.1974.223784
   [Anonymous], 2000, SHAPE ANAL CLASSIFIC
   Cai ZC, 2007, APPL NUMER HARMON AN, P525, DOI 10.1007/978-3-7643-7778-6_39
   Cavigelli L, 2017, IEEE IJCNN, P752, DOI 10.1109/IJCNN.2017.7965927
   Fan K, 2017, IEEE T CIRC SYST VID, V27, P2726, DOI 10.1109/TCSVT.2016.2595327
   FENG YY, 1984, SIAM J MATH ANAL, V15, P834, DOI 10.1137/0515063
   Galteri L, 2019, IEEE T MULTIMEDIA, V21, P2131, DOI 10.1109/TMM.2019.2895280
   Hou Z., ACTA ELECT SINICA, V31, P539
   Hou ZX, 2009, SIGNAL PROCESS-IMAGE, V24, P791, DOI 10.1016/j.image.2009.08.002
   Jacquin AE, 1992, IEEE T IMAGE PROCESS, V1, P18, DOI 10.1109/83.128028
   Lee K, 2005, IEEE T IMAGE PROCESS, V14, P36, DOI 10.1109/TIP.2004.838699
   Li PY, 2018, IEEE T MULTIMEDIA, V20, P1960, DOI 10.1109/TMM.2017.2786860
   Liu SZ, 2002, IEEE T CIRC SYST VID, V12, P1139, DOI 10.1109/TCSVT.2002.806819
   [马辉 MA Hui], 2006, [工程图学学报, Journal of Engineering Graphics], V27, P108
   Ma SW, 2015, IEEE SIGNAL PROC MAG, V32, P172, DOI 10.1109/MSP.2014.2371951
   PAIK HW, 1992, PROCEEDINGS OF THE 35TH MIDWEST SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOLS 1 AND 2, P976, DOI 10.1109/MWSCAS.1992.271133
   Qi D., 2011, Discontinuous Orthogonal Function: U-System, V-System, Multiwavelet and Its Application
   Ratnakar V, 2000, IEEE T IMAGE PROCESS, V9, P267, DOI 10.1109/83.821739
   Shen LQ, 2013, IEEE T MULTIMEDIA, V15, P465, DOI 10.1109/TMM.2012.2231060
   Skodras A, 2001, IEEE SIGNAL PROC MAG, V18, P36, DOI 10.1109/79.952804
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tak U. Kin, 2009, 2009 International Conference on Information and Automation (ICIA), P995, DOI 10.1109/ICINFA.2009.5205063
   Tak U. K., 2012, FUTURE WIRELESS NETW, P179
   WALLACE GK, 1991, COMMUN ACM, V34, P30, DOI 10.1145/103085.103089
   Wang Chengyou, 2013, J COMPUT INF SYST, V9, P7227, DOI [10.12733/jcis7086, DOI 10.12733/JCIS7086]
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wu S. W., 1993, ICASSP-93. 1993 IEEE International Conference on Acoustics, Speech, and Signal Processing (Cat. No.92CH3252-4), P389, DOI 10.1109/ICASSP.1993.319829
   Xiong G, 2010, THESIS SUN YS U GUAN
   Young SI, 2019, IEEE T IMAGE PROCESS, V28, P343, DOI 10.1109/TIP.2018.2867943
   Yuan XX, 2019, IEEE T MULTIMEDIA, V21, P1372, DOI 10.1109/TMM.2018.2881069
   Zhang H, 2014, IEEE T CIRC SYST VID, V24, P660, DOI 10.1109/TCSVT.2013.2290578
   Zhou X., 2017, J COMMUN, V12, P72
NR 32
TC 10
Z9 11
U1 1
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1069
EP 1082
DI 10.1109/TMM.2020.2992940
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA QO8XK
UT WOS:000623420300019
DA 2024-07-18
ER

PT J
AU Zhong, X
   Huang, PC
   Mastorakis, S
   Shih, FY
AF Zhong, Xin
   Huang, Pei-Chi
   Mastorakis, Spyridon
   Shih, Frank Y.
TI An Automated and Robust Image Watermarking Scheme Based on Deep Neural
   Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Knowledge engineering; Deep learning; Heuristic algorithms; Neural
   networks; Fitting; Refining; Watermarking; Image watermarking;
   automation; robustness; deep learning; convolutional neural networks
ID PARAMETER; ATTACKS
AB Digital image watermarking is the process of embedding and extracting a watermark covertly on a cover-image. To dynamically adapt image watermarking algorithms, deep learning-based image watermarking schemes have attracted increased attention during recent years. However, existing deep learning-based watermarking methods neither fully apply the fitting ability to learn and automate the embedding and extracting algorithms, nor achieve the properties of robustness and blindness simultaneously. In this paper, a robust and blind image watermarking scheme based on deep learning neural networks is proposed. To minimize the requirement of domain knowledge, the fitting ability of deep neural networks is exploited to learn and generalize an automated image watermarking algorithm. A deep learning architecture is specially designed for image watermarking tasks, which will be trained in an unsupervised manner to avoid human intervention and annotation. To facilitate flexible applications, the robustness of the proposed scheme is achieved without requiring any prior knowledge or adversarial examples of possible attacks. A challenging case of watermark extraction from phone camera-captured images demonstrates the robustness and practicality of the proposal. The experiments, evaluation, and application cases confirm the superiority of the proposed scheme.
C1 [Zhong, Xin; Huang, Pei-Chi; Mastorakis, Spyridon] Univ Nebraska, Dept Comp Sci, Omaha, NE 68182 USA.
   [Shih, Frank Y.] New Jersey Inst Technol, Dept Comp Sci, Newark, NJ 07102 USA.
C3 University of Nebraska System; New Jersey Institute of Technology
RP Zhong, X (corresponding author), Univ Nebraska, Dept Comp Sci, Omaha, NE 68182 USA.
EM xzhong@unomaha.edu; phuang@unomaha.edu; smastorakis@unomaha.edu;
   shih@njit.edu
RI Wang, Xiaojing/HNI-4384-2023
OI Wang, Xiaojing/0000-0001-6921-3619; Zhong, Xin/0000-0001-8339-9245
CR [Anonymous], 2019, DIGIMARC DIGITAL WAT
   Baluja S, 2017, ADV NEURAL INFORM PR, P2069
   Berghel H, 1996, COMPUTER, V29, P101, DOI 10.1109/2.511977
   Chen ZG, 2018, IEEE T MULTIMEDIA, V20, P1973, DOI 10.1109/TMM.2018.2794985
   Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120
   Cox IJ, 1999, P IEEE, V87, P1127, DOI 10.1109/5.771068
   Cox IJ., 2007, DIGITAL WATERMARKING
   Craver S, 1998, IEEE J SEL AREA COMM, V16, P573, DOI 10.1109/49.668979
   Delgado-Guillen LA, 2013, MIDWEST SYMP CIRCUIT, P1363, DOI 10.1109/MWSCAS.2013.6674909
   Fierro-Radilla A., 2019, 2019 7 INT WORKSHOP, P1
   Getoor, 2011, P 28 INT C MACH LEAR, P833
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Huang Y, 2019, IEEE T MULTIMEDIA, V21, P2447, DOI 10.1109/TMM.2019.2907475
   Jaderberg M., 2015, ICLR, DOI DOI 10.48550/ARXIV.1506.02025
   Kandi H, 2017, COMPUT SECUR, V65, P247, DOI 10.1016/j.cose.2016.11.016
   Kang XG, 2003, IEEE T CIRC SYST VID, V13, P776, DOI 10.1109/TCSVT.2003.815957
   KIM WH, 2018, ARXIV180506199
   Kim W, 2006, LECT NOTES COMPUT SC, V4261, P106
   Kingma D. P., 2015, INT C LEARNING REPRE
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Latvala S., 2020, SN COMPUT SCI, P1
   LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9, DOI 10.1007/3-540-49430-8_2
   Li DM, 2019, INFORM SCIENCES, V479, P432, DOI 10.1016/j.ins.2018.02.060
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Mastorakis S, 2019, I C NETWORK PROTOCOL, P1, DOI [10.1109/ICNP.2019.8888081, DOI 10.1109/icnp.2019.8888081]
   Mastorakis S, 2020, IEEE INTERNET THINGS, V7, P4203, DOI 10.1109/JIOT.2020.2966924
   Mun SM, 2019, NEUROCOMPUTING, V337, P191, DOI 10.1016/j.neucom.2019.01.067
   Papernot N, 2016, 1ST IEEE EUROPEAN SYMPOSIUM ON SECURITY AND PRIVACY, P372, DOI 10.1109/EuroSP.2016.36
   Pramila A, 2018, J SYST SOFTWARE, V135, P205, DOI 10.1016/j.jss.2017.10.029
   REED IS, 1960, J SOC IND APPL MATH, V8, P300, DOI 10.1137/0108018
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shih Frank Y, 2017, Digital watermarking and steganography: fundamentals and techniques
   Su ZP, 2018, IEEE T MULTIMEDIA, V20, P2631, DOI 10.1109/TMM.2018.2812599
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Vukoti Vedran, 2018, INT WORKSH INF FOR S, P1
   Wang ZH, 2018, LECT NOTES COMPUT SC, V11302, P253, DOI 10.1007/978-3-030-04179-3_22
   Yamana T., 2013, Proceedings of the Ninth Symposium of the International Working Group on Plant Viruses with Fungal Vectors, Obihiro, Hokkaido, Japan, 19-22 August 2013, P49
   Zhao M, 2018, IEEE T IMAGE PROCESS, V27, P2731, DOI 10.1109/TIP.2018.2810516
NR 38
TC 55
Z9 58
U1 17
U2 111
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1951
EP 1961
DI 10.1109/TMM.2020.3006415
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TC8FT
UT WOS:000668875100010
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Sun, HM
   Cheng, ZX
   Takeuchi, M
   Katto, J
AF Sun, Heming
   Cheng, Zhengxue
   Takeuchi, Masaru
   Katto, Jiro
TI Enhanced Intra Prediction for Video Coding by Using Multiple Neural
   Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Neural networks; Encoding; Image coding; Video compression; Standards;
   High efficiency video coding; High efficiency video coding (HEVC); intra
   prediction; neural network; probability
ID H.264
AB This paper enhances the intra prediction by using multiple neural network modes (NM). Each NM serves as an end-to-end mapping from the neighboring reference blocks to the current coding block. For the provided NMs, we present two schemes (appending and substitution) to integrate the NMs with the traditional modes (TM) defined in high efficiency video coding (HEVC). For the appending scheme, each NM is corresponding to a certain range of TMs. The categorization of TMs is based on the expected prediction errors. After determining the relevant TMs for each NM, we present a probability-aware mode signaling scheme. The NMs with higher probabilities to be the best mode are signaled with fewer bits. For the substitution scheme, we propose to replace the highest and lowest probable TMs. New most probable mode (MPM) generation method is also employed when substituting the lowest probable TMs. Experimental results demonstrate that using multiple NMs will improve the coding efficiency apparently compared with the single NM. Specifically, proposed appending scheme with seven NMs can save 2.6%, 3.8%, and 3.1% BD-rate for Y, U, and V components compared with using single NM in the state-of-the-art works.
C1 [Sun, Heming; Takeuchi, Masaru] Waseda Res Inst Sci & Engn, Tokyo 1698555, Japan.
   [Sun, Heming] PRESTO, JST, 4-1-8 Honcho, Kawaguchi, Saitama 3320012, Japan.
   [Cheng, Zhengxue; Katto, Jiro] Waseda Univ, Grad Sch Fundamental Sci & Engn, Tokyo 1698555, Japan.
C3 Waseda University; Japan Science & Technology Agency (JST); Waseda
   University
RP Cheng, ZX (corresponding author), Waseda Univ, Grad Sch Fundamental Sci & Engn, Tokyo 1698555, Japan.
EM hemingsun@aoni.waseda.jp; zxcheng@asagi.waseda.jp;
   masaru-t@aoni.waseda.jp; katto@waseda.jp
RI Katto, Jiro/AAH-2223-2020; Heming, Sun/G-6882-2018
OI Katto, Jiro/0000-0002-1671-2614; Cheng, Zhengxue/0000-0001-8258-6364;
   Takeuchi, Masaru/0000-0001-8953-0697
FU JST, PRESTO, Japan [JPMJPR19M5]; Fujitsu; JSPS KAKENHI [15H01684];
   Waseda University [2019Q-049]
FX This work was supported in part by JST, PRESTO under Grant JPMJPR19M5,
   Japan, in part by a research fund from Fujitsu, in part by JSPS KAKENHI
   under Grant 15H01684, in part by Waseda University Grant for Special
   Research Projects 2019Q-049.
CR Abdoli M, 2018, IEEE SIGNAL PROC LET, V25, P1690, DOI 10.1109/LSP.2018.2871872
   [Anonymous], 2015, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2015.123
   [Anonymous], 2013, Technical Report JCTVC-L1100
   Auwera G., 2018, JVETK0063 ITUT SG16
   Balle J., 2018, ARXIV180201436EESSIV
   Balle J., 2017, ARXIV161101704
   Bjontegaard G., 2001, CALCULATION AVERAGE
   Bross B., 2018, JVETK0051V1
   Chen HM, 2016, IEEE T IMAGE PROCESS, V25, P3671, DOI 10.1109/TIP.2016.2573585
   Chen J.Y., 2019, JVETP2002V1
   Chen Y, 2013, INT CONF ACOUST SPEE, P1734, DOI 10.1109/ICASSP.2013.6637949
   Cheng ZX, 2019, PROC CVPR IEEE, P10063, DOI 10.1109/CVPR.2019.01031
   Cheng ZX, 2018, PICT COD SYMP, P253, DOI 10.1109/PCS.2018.8456308
   Chernyak R.I., 2014, Applied Mathematical Sciences, V8, P7389, DOI [DOI 10.12988/AMS.2014.49750, 10.12988/ams.2014.49750]
   Choi N., 2018, JVETK0179V1
   Hernandez S., 2019, JVETM0102V5
   Huo S, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351609
   Jia CM, 2017, 2017 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)
   Kalampogia A, 2018, IEEE T MULTIMEDIA, V20, P171, DOI 10.1109/TMM.2017.2713642
   Kingma D. P., 2014, arXiv
   Li C, 2017, IEEE IMAGE PROC, P4577, DOI 10.1109/ICIP.2017.8297149
   Li JH, 2018, IEEE T IMAGE PROCESS, V27, P3236, DOI 10.1109/TIP.2018.2817044
   Li JH, 2018, IEEE T CIRC SYST VID, V28, P947, DOI 10.1109/TCSVT.2016.2633377
   Li JH, 2017, IEEE IMAGE PROC, P1, DOI 10.1109/ICIP.2017.8296231
   Li JH, 2017, IEEE DATA COMPR CONF, P221, DOI 10.1109/DCC.2017.59
   Li SY, 2014, IEEE IMAGE PROC, P3146, DOI 10.1109/ICIP.2014.7025636
   Lin HW, 2019, IEEE T MULTIMEDIA, V21, P3010, DOI 10.1109/TMM.2019.2919433
   Liu D, 2018, LECT NOTES COMPUT SC, V10705, P61, DOI 10.1007/978-3-319-73600-6_6
   Ma X., 2018, JVETK0190R1
   Meng XD, 2018, IEEE DATA COMPR CONF, P187, DOI 10.1109/DCC.2018.00027
   O'Malley S.M., 2016, ARXIV151106085
   Park Park W.-S. W.-S., 2016, P IM VID MULT SIGN P, P1
   Pfaff J, 2018, PROC SPIE, V10752, DOI 10.1117/12.2321273
   Qi XL, 2012, INT CONF ACOUST SPEE, P1217, DOI 10.1109/ICASSP.2012.6288107
   Rippel O., 2018, ARXIV181106981V1
   Said A, 2016, IEEE IMAGE PROC, P534, DOI 10.1109/ICIP.2016.7532414
   Song XD, 2018, IEEE IMAGE PROC, P1133, DOI 10.1109/ICIP.2018.8451589
   Suehring K., 2016, JVETB1010
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Toderici G, 2017, PROC CVPR IEEE, P5435, DOI 10.1109/CVPR.2017.577
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Wilson K, 2014, EXPERT REV VACCINES, V13, P969, DOI 10.1586/14760584.2014.928208
   Xia SF, 2018, IEEE DATA COMPR CONF, P127, DOI 10.1109/DCC.2018.00021
   Xu JY, 2019, IEEE T MULTIMEDIA, V21, P1633, DOI 10.1109/TMM.2018.2885921
   Yan N, 2019, IEEE T CIRC SYST VID, V29, P840, DOI 10.1109/TCSVT.2018.2816932
   Yan N, 2017, IEEE INT SYMP CIRC S, P822
   Yeh CH, 2015, IEEE T MULTIMEDIA, V17, P1508, DOI 10.1109/TMM.2015.2449659
   Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P3983, DOI 10.1109/TIP.2018.2830640
   Zhang T, 2018, IEEE T MULTIMEDIA, V20, P1622, DOI 10.1109/TMM.2017.2775223
   Zhang YG, 2014, PHYSIOL REP, V2, DOI 10.14814/phy2.12147
   Zhao ZN, 2019, IEEE ACM T COMPUT BI, V16, P1753, DOI 10.1109/TCBB.2017.2706682
NR 51
TC 15
Z9 15
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2020
VL 22
IS 11
BP 2764
EP 2779
DI 10.1109/TMM.2019.2963620
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OJ8YR
UT WOS:000584239900002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Xia, ZY
   Yi, P
   Liu, YY
   Jiang, B
   Wang, W
   Zhu, T
AF Xia, Zhiyang
   Yi, Ping
   Liu, Yunyu
   Jiang, Bo
   Wang, Wei
   Zhu, Ting
TI GENPass: A Multi-Source Deep Learning Model for Password Guessing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Password; Neural networks; Deep learning; Gallium nitride; Training;
   Computational modeling; Markov processes; Neural networks; natural
   languages; data analysis; text processing
ID SERVICE ATTACK
AB The password has become today's dominant method of authentication. While brute-force attack methods such as HashCat and John the Ripper have proven unpractical, the research then switches to password guessing. State-of-the-art approaches such as the Markov Model and probabilistic context-free grammar (PCFG) are all based on statistical probability. These approaches require a large amount of calculation, which is time-consuming. Neural networks have proven more accurate and practical in password guessing than traditional methods. However, a raw neural network model is not qualified for cross-site attacks because each dataset has its own features. Our work aims to generalize those leaked passwords and improves the performance in cross-site attacks. In this paper, we propose GENPass, a multi-source deep learning model for generating "general" password. GENPass learns from several datasets and ensures the output wordlist can maintain high accuracy for different datasets using adversarial generation. The password generator of GENPass is PCFG+LSTM (PL). We are the first to combine a neural network with PCFG. Compared with Long short-term memory (LSTM), PL increases the matching rate by 16%-30% in cross-site tests when learning from a single dataset. GENPass uses several PL models to learn datasets and generate passwords. The results demonstrate that the matching rate of GENPass is 20% higher than by simply mixing datasets in the cross-site test. Furthermore, we propose GENPass with probability (GENPass-pro), the updated version of GENPass, which can further increase the matching rate of GENPass.
C1 [Xia, Zhiyang; Yi, Ping; Liu, Yunyu; Jiang, Bo] Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Shanghai 200240, Peoples R China.
   [Wang, Wei; Zhu, Ting] Univ Maryland Baltimore Cty, Dept Comp Sci & Elect Engn, Baltimore, MD 21250 USA.
C3 Shanghai Jiao Tong University; University System of Maryland; University
   of Maryland Baltimore County
RP Yi, P (corresponding author), Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Shanghai 200240, Peoples R China.
EM x56981973@sjtu.edu.cn; yiping@sjtu.edu.cn; lyywenwen@gmail.com;
   bjiang@sjtu.edu.cn; ax29092@umbc.edu; zt@umbc.edu
RI Li, Shiyue/KFA-3709-2024; Lin, Kuan-Yu/JXM-6653-2024
OI wang, wei/0000-0003-3240-1485; Yi, Ping/0000-0003-4530-5118; Liu,
   Yunyu/0000-0002-1539-7050
FU National Natural Science Foundation of China [61571290, 61831007,
   61431008]; National Key Research and Development Program of China
   [2017YFB0802900, 2018YFB0803503, 2017YFB0802300]; NSFC-Zhejiang Joint
   Fund for the Integration of Industrialization and Informatization
   [U1509219]; Information Network Security Key Laboratory of the Ministry
   of Public Security Open Project Support [C18611]; National Science
   Foundation [CNS-1539047, CNS-1652669]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61571290, 61831007, and 61431008, in
   part by the National Key Research and Development Program of China under
   Grants 2017YFB0802900, 2018YFB0803503, and 2017YFB0802300, in part by
   The NSFC-Zhejiang Joint Fund for the Integration of Industrialization
   and Informatization U1509219, in part by Information Network Security
   Key Laboratory of the Ministry of Public Security Open Project Support
   C18611, and in part by the National Science Foundation Grants
   CNS-1539047 and CNS-1652669.
CR [Anonymous], P 10 IEEE INT C MOB
   [Anonymous], CHINA COMMUN
   [Anonymous], P IEEE INT C COMM SY
   [Anonymous], 2016, The Scrypt Password-based Key Derivation Function
   [Anonymous], JOHN RIPPER
   [Anonymous], 2012, Journal of Computational Information Systems
   [Anonymous], P IEEE INT C SMART G
   [Anonymous], P IEEE INT C COMM IC
   [Anonymous], INFORM THEORY INFERE
   [Anonymous], P IEEE 17 INT C SMAR
   [Anonymous], HASHCAT
   [Anonymous], P C NEUR INF PROC SY
   [Anonymous], INT J ENG SCI TECHNO
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Graves A., 2013, GENERATING SEQUENCES
   Gu SC, 2019, PROCEEDINGS OF THE 11TH INTERNATIONAL CONFERENCE ON AGENTS AND ARTIFICIAL INTELLIGENCE (ICAART), VOL 2, P164, DOI 10.5220/0007370301640173
   Herley C, 2012, IEEE SECUR PRIV, V10, P28, DOI 10.1109/MSP.2011.150
   Hitaj B, 2019, LECT NOTES COMPUT SC, V11464, P217, DOI 10.1007/978-3-030-21568-2_11
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   [李元铭 Li Yuanming], 2016, [中国海洋药物, Chinese Journal of Marine Drugs], V35, P1
   Li ZG, 2014, PROCEEDINGS OF THE 23RD USENIX SECURITY SYMPOSIUM, P559
   Liang X, 2013, SPRINGERBRIEF COMPUT, P1, DOI 10.1007/978-1-4614-8857-6
   Lipton Z.C., 2015, COMPUT SCI, V1506, P19, DOI DOI 10.1145/2647868.2654889
   Liu LZ, 2010, 2010 THE 3RD INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND INDUSTRIAL APPLICATION (PACIIA2010), VOL I, P1, DOI 10.1109/MWSYM.2010.5515677
   Ma J, 2014, P IEEE S SECUR PRIV, P689, DOI 10.1109/SP.2014.50
   Melicher W, 2016, PROCEEDINGS OF THE 25TH USENIX SECURITY SYMPOSIUM, P175
   Narayanan A., 2005, P ACM C COMP COMM SE, P364, DOI [10.1145/1102120.1102168, DOI 10.1145/1102120.1102168]
   Narayanan A., 2005, P 12 ACM C COMP COMM, P364, DOI DOI 10.1145/1102120.1102168
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Pearman S, 2017, CCS'17: PROCEEDINGS OF THE 2017 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P295, DOI 10.1145/3133956.3133973
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Stuart A., 1994, KENDALLS ADV THEORY
   Wang D, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1242, DOI 10.1145/2976749.2978339
   Wang XD, 2011, IEEE T SMART GRID, V2, P809, DOI 10.1109/TSG.2011.2167354
   Weir M, 2009, P IEEE S SECUR PRIV, P391, DOI 10.1109/SP.2009.8
   Yi P, 2018, WIREL COMMUN MOB COM, DOI 10.1155/2018/4678746
   Yi P, 2016, IEEE T VEH TECHNOL, V65, P4714, DOI 10.1109/TVT.2016.2549269
   Yi P, 2016, J NETW COMPUT APPL, V59, P325, DOI 10.1016/j.jnca.2015.04.015
   Yi P, 2014, IEEE ICC, P1029, DOI 10.1109/ICC.2014.6883456
   Yi P, 2012, IEEE GLOB COMM CONF, P3037, DOI 10.1109/GLOCOM.2012.6503580
   Yi P, 2013, AD HOC SENS WIREL NE, V17, P269
   Yi P, 2010, IETE TECH REV, V27, P6, DOI 10.4103/0256-4602.58969
   Zaremba W., 2014, RECURRENT NEURAL NET, P1, DOI DOI 10.1016/S0893-6080(96)00073-1
   Zhang YZ, 2017, PR MACH LEARN RES, V70
   Zhou YL, 2019, WIREL COMMUN MOB COM, V2019, DOI 10.1155/2019/8694016
NR 46
TC 17
Z9 23
U1 1
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2020
VL 22
IS 5
BP 1323
EP 1332
DI 10.1109/TMM.2019.2940877
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LJ3VZ
UT WOS:000530097200017
OA hybrid, Green Submitted
DA 2024-07-18
ER

PT J
AU Liu, DY
   An, P
   Ma, R
   Zhan, WF
   Huang, XP
   Yahya, AA
AF Liu, Deyang
   An, Ping
   Ma, Ran
   Zhan, Wenfa
   Huang, Xinpeng
   Yahya, Ali Abdullah
TI Content-Based Light Field Image Compression Method With Gaussian Process
   Regression
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image coding; Lenses; Cameras; Microoptics; Prediction methods;
   Correlation; Gaussian processes; Light field image compression;
   classification image compression; Gaussian process regression; content
   property; HEVC
ID HEVC; OPTIMIZATION
AB Light field (LF) imaging enables new possibilities for digital imaging, such as digital refocusing, changing of focus plane, changing of viewpoint, scene-depth estimation, and 3D scene reconstruction, by capturing both spatial and angular information of light rays. However, one main problem in dealing with LF data is its sheer volume. In this context, efficient compression methods are needed for such a particular type of content. In this paper, we propose a content-based LF image-compression method with Gaussian process regression to improve the compression efficiency and accelerate the prediction procedure. First, the LF image is fed to the intra-frame codec of HEVC. In the prediction procedure, the prediction units (PUs) are classified as non-homogenous texture units, homogenous texture units, and visually flat units, based on the content property of the LF image. For each category, we design a corresponding Gaussian process regression (GPR)-based prediction method. Moreover, we propose a classification mechanism to exactly decide to which category the current PU belongs, so as to adjust the trade-off between the computational burden and the LF image coding efficiency. Experimental results demonstrate that the proposed LF image compression method is superior to several other state-of-the-art compression methods in terms of different quality metrics. Furthermore, the proposed method can also achieve a good visual quality of views rendered from decoded LF contents.
C1 [Liu, Deyang; Zhan, Wenfa; Yahya, Ali Abdullah] Anqing Normal Univ, Anqing, Peoples R China.
   [Liu, Deyang; Zhan, Wenfa; Yahya, Ali Abdullah] Anqing Normal Univ, Univ Key Lab Intelligent Percept & Comp, Anqing 246003, Peoples R China.
   [An, Ping; Ma, Ran; Huang, Xinpeng] Shanghai Univ, Key Lab Adv Display & Syst Applicat, Shanghai 200072, Peoples R China.
C3 Anqing Normal University; Anqing Normal University; Shanghai University
RP Liu, DY (corresponding author), Anqing Normal Univ, Anqing, Peoples R China.; Liu, DY (corresponding author), Anqing Normal Univ, Univ Key Lab Intelligent Percept & Comp, Anqing 246003, Peoples R China.
EM liudeyang@163.com; anping@shu.edu.cn; maran@shu.edu.cn;
   zhanwf12@163.com; xinpeng_huang@163.com; aselwey1@hotmail.com
RI Liu, Deyang/AAX-5429-2020; Liu, Deyang/ABG-2705-2020; Huang,
   xp/JRX-2837-2023
OI Liu, Deyang/0000-0001-7991-8735; Huang, Xinpeng/0000-0002-2373-642X
FU National Natural Science Foundation of China [61801006, 61571285]; Key
   Project on Anhui Provincial Natural Science Study by Colleges and
   Universities [KJ2018A0361]; Open Fund of the Key Laboratory of Advanced
   Display and System Applications [P201801]; Program for Innovative
   Research Team in Anqing Normal University; Foundation of University
   Research and Innovation Platform Team for Intelligent Perception and
   Computing of Anhui Province
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61801006 and 61571285, in part by the
   Key Project on Anhui Provincial Natural Science Study by Colleges and
   Universities under Grant KJ2018A0361, in part by the Open Fund of the
   Key Laboratory of Advanced Display and System Applications under Grant
   P201801, in part by Program for Innovative Research Team in Anqing
   Normal University, and in part by the Foundation of University Research
   and Innovation Platform Team for Intelligent Perception and Computing of
   Anhui Province.
CR Adelson E.H., 1991, Computational Models of Visual Processing, P3
   [Anonymous], N6922 ISOIEC JTC 1SC
   [Anonymous], N80027 ISOIEC JTC 1S
   [Anonymous], 2017, JTC1SC29WG11N16718 I
   [Anonymous], 2016, MDTV
   [Anonymous], P SIGGRAPH
   [Anonymous], 2009, INT C COMP PHOT
   [Anonymous], GALLERY LIGHTFIELD D
   [Anonymous], 2013, Technical Report JCTVC-L1100
   Cao YS, 2015, IEEE T PATTERN ANAL, V37, P2415, DOI 10.1109/TPAMI.2015.2424873
   Conti C, 2016, IEEE INT CONF MULTI, DOI 10.1109/ICMEW.2016.7574667
   Conti C, 2018, SIGNAL PROCESS-IMAGE, V60, P144, DOI 10.1016/j.image.2017.10.006
   Conti C, 2016, SIGNAL PROCESS-IMAGE, V42, P59, DOI 10.1016/j.image.2016.01.008
   Dai F, 2015, IEEE IMAGE PROC, P4733, DOI 10.1109/ICIP.2015.7351705
   Dansereau D., Light Field Toolbox v0.4
   Deng CW, 2012, IEEE T MULTIMEDIA, V14, P1127, DOI 10.1109/TMM.2012.2191270
   Georgiev T, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3442712
   Hahne C, 2014, OPT EXPRESS, V22, P26659, DOI 10.1364/OE.22.026659
   Huang X, 2017, PROCEEDINGS OF THE 2017 INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND ARTIFICIAL INTELLIGENCE (CSAI 2017), P1, DOI 10.1145/3168390.3168392
   Jiang F, 2017, IEEE INT CON MULTI, P67, DOI 10.1109/ICME.2017.8019522
   Koloda J, 2014, IEEE T MULTIMEDIA, V16, P1729, DOI 10.1109/TMM.2014.2330314
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Levoy M, 2006, COMPUTER, V39, P46, DOI 10.1109/MC.2006.270
   Li L, 2017, IEEE J-STSP, V11, P1107, DOI 10.1109/JSTSP.2017.2725198
   Li L, 2017, IEEE DATA COMPR CONF, P131, DOI 10.1109/DCC.2017.10
   Li SX, 2018, IEEE T MULTIMEDIA, V20, P155, DOI 10.1109/TMM.2017.2721544
   Li Y, 2016, IEEE T CIRC SYST VID, V26, P1308, DOI 10.1109/TCSVT.2015.2450333
   Liu D., 2016, DESIGN AUTOMATION C, P1, DOI DOI 10.1109/GLOCOM.2016.7842078
   Liu DY, 2017, INT CONF ACOUST SPEE, P2002, DOI 10.1109/ICASSP.2017.7952507
   Liu DY, 2016, J ELECTRON IMAGING, V25, DOI 10.1117/1.JEI.25.4.043015
   Liu DY, 2016, SIGNAL PROCESS-IMAGE, V47, P438, DOI 10.1016/j.image.2016.08.004
   Liu DY, 2015, 2015 IEEE CHINA SUMMIT & INTERNATIONAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING, P201, DOI 10.1109/ChinaSIP.2015.7230391
   Lucas LER, 2014, EUR SIGNAL PR CONF, P11
   Monteiro R, 2016, IEEE INT CONF MULTI, DOI 10.1109/ICMEW.2016.7574670
   Monteiro RJS, 2017, IEEE J-STSP, V11, P1120, DOI 10.1109/JSTSP.2017.2721358
   Pan ZQ, 2016, IEEE T BROADCAST, V62, P675, DOI 10.1109/TBC.2016.2580920
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Rerabek M., 2016, PROC ICME GRAND CHAL, P1
   Shen LQ, 2014, IEEE T CIRC SYST VID, V24, P1709, DOI 10.1109/TCSVT.2014.2313892
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Viola I, 2017, IEEE J-STSP, V11, P1092, DOI 10.1109/JSTSP.2017.2740167
   WALLACE GK, 1991, COMMUN ACM, V34, P30, DOI 10.1145/103085.103089
   Wang G, 2016, IEEE T IMAGE PROCESS, V25, P5104, DOI 10.1109/TIP.2016.2603602
   Wang SS, 2016, J VIS COMMUN IMAGE R, V35, P120, DOI 10.1016/j.jvcir.2015.12.005
   Wong TT, 2002, IEEE T MULTIMEDIA, V4, P361, DOI 10.1109/TMM.2002.802835
   Wu GC, 2017, IEEE J-STSP, V11, P926, DOI 10.1109/JSTSP.2017.2747126
   Xu JZ, 2016, IEEE T CIRC SYST VID, V26, P50, DOI 10.1109/TCSVT.2015.2478706
   Zhu SY, 2018, IEEE T MULTIMEDIA, V20, P525, DOI 10.1109/TMM.2017.2749162
NR 48
TC 46
Z9 49
U1 3
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2020
VL 22
IS 4
BP 846
EP 859
DI 10.1109/TMM.2019.2934426
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KY2ZI
UT WOS:000522440400002
DA 2024-07-18
ER

PT J
AU Liu, Q
   He, ZY
   Li, X
   Zheng, Y
AF Liu, Qiao
   He, Zhenyu
   Li, Xin
   Zheng, Yuan
TI PTB-TIR: A Thermal Infrared Pedestrian Tracking Benchmark
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Thermal infrared; pedestrian tracking; benchmark; dataset
ID VISUAL OBJECT TRACKING; FUSION
AB Thermal infrared (TIR) pedestrian tracking is one of the important components among numerous applications of computer vision, which has a major advantage: it can track pedestrians in total darkness. The ability to evaluate the TIR pedestrian tracker fairly, on a benchmark dataset, is significant for the development of this field. However, there is not a benchmark dataset. In this paper, we develop a TIR pedestrian tracking dataset for the TIR pedestrian tracker evaluation. The dataset includes 60 thermal sequences with manual annotations. Each sequence has nine attribute labels for the attribute based evaluation. In addition to the dataset, we carry out the large-scale evaluation experiments on our benchmark dataset using nine publicly available trackers. The experimental results help us understand the strengths and weaknesses of these trackers. In addition, in order to gain more insight into the TIR pedestrian tracker, we divide its functions into three components: feature extractor, motion model, and observation model. Then, we conduct three comparison experiments on our benchmark dataset to validate how each component affects the tracker's performance. The findings of these experiments provide some guidelines for future research.
C1 [Liu, Qiao; He, Zhenyu; Li, Xin] Harbin Inst Technol, Sch Comp Sci & Technol, Shenzhen 518055, Peoples R China.
   [Zheng, Yuan] Inner Mongolia Univ, Sch Comp Sci, Hohhot 010021, Peoples R China.
C3 Harbin Institute of Technology; Inner Mongolia University
RP He, ZY (corresponding author), Harbin Inst Technol, Sch Comp Sci & Technol, Shenzhen 518055, Peoples R China.
EM liuqiao.hit@gmail.com; zhenyuhe@hit.edu.cn; xinlihaha@gmail.com;
   zhengyuan@hitsz.edu.cn
RI Liu, Qiao/AAL-5654-2021
OI Liu, Qiao/0000-0003-0885-7976
FU National Natural Science Foundation of China [61672183, 61502119];
   Shenzhen Research Council [JCYJ20170815113552036, JCYJ20170413104556946,
   JCYJ20160406161948211, JCY-J2016022620 1453085]; Natural Science
   Foundation of Guangdong Province [2015A030313544]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61672183 and 61502119, in part by the
   Shenzhen Research Council under Grants JCYJ20170815113552036,
   JCYJ20170413104556946, JCYJ20160406161948211, and JCY-J2016022620
   1453085, and in part by the Natural Science Foundation of Guangdong
   Province under Grant 2015A030313544. The associate editor coordinating
   the review of this manuscript and approving it for publication was Prof.
   Mohammed Daoudi
CR [Anonymous], IN VID AN DAT
   [Anonymous], IEEE OTCBVS WS SERIE
   [Anonymous], P BRIT MACH VIS C
   [Anonymous], 2008, An Open and Portable Library of Computer Vision Algorithms
   [Anonymous], P INT C COMP VIS WOR
   Babenko B, 2009, PROC CVPR IEEE, P983, DOI 10.1109/CVPRW.2009.5206737
   Dai CX, 2007, COMPUT VIS IMAGE UND, V106, P288, DOI 10.1016/j.cviu.2006.08.009
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Danelljan M., 2014, BRIT MACH VIS C, P1, DOI DOI 10.5244/C.28.65
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Davis JW, 2007, COMPUT VIS IMAGE UND, V106, P162, DOI 10.1016/j.cviu.2006.06.010
   Dong XP, 2017, IEEE T MULTIMEDIA, V19, P763, DOI 10.1109/TMM.2016.2631884
   Felsberg M, 2016, LECT NOTES COMPUT SC, V9914, P824, DOI 10.1007/978-3-319-48881-3_55
   Gade R, 2014, MACH VISION APPL, V25, P245, DOI 10.1007/s00138-013-0570-5
   Gao J, 2014, LECT NOTES COMPUT SC, V8691, P188, DOI 10.1007/978-3-319-10578-9_13
   Ge JF, 2009, IEEE T INTELL TRANSP, V10, P283, DOI 10.1109/TITS.2009.2018961
   González A, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16060820
   Haghighat M, 2015, EXPERT SYST APPL, V42, P7905, DOI 10.1016/j.eswa.2015.06.025
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He ZY, 2017, IEEE T CYBERNETICS, V47, P354, DOI 10.1109/TCYB.2016.2514714
   He ZY, 2016, IEEE T IMAGE PROCESS, V25, P3698, DOI 10.1109/TIP.2016.2570553
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Jüngling K, 2010, IEEE INT VEH SYM, P470, DOI 10.1109/IVS.2010.5548132
   Kim DE, 2015, INT CONF UBIQ ROBOT, P22, DOI 10.1109/URAI.2015.7358920
   Ko BC, 2013, OPT ENG, V52, DOI 10.1117/1.OE.52.11.113105
   Kwak JY, 2017, IEEE T INTELL TRANSP, V18, P69, DOI 10.1109/TITS.2016.2569159
   Li CL, 2017, IEEE T SYST MAN CY-S, V47, P673, DOI 10.1109/TSMC.2016.2627052
   Li JF, 2010, J COMPUT, V5, P1606, DOI 10.4304/jcp.5.10.1606-1613
   Li X, 2019, KNOWL-BASED SYST, V166, P71, DOI 10.1016/j.knosys.2018.12.011
   Li X, 2016, KNOWL-BASED SYST, V113, P88, DOI 10.1016/j.knosys.2016.09.014
   Li X, 2014, SENSORS-BASEL, V14, P11245, DOI 10.3390/s140611245
   Liang C, 2016, 2016 INTERNATIONAL CONFERENCE ON IMAGE, VISION AND COMPUTING (ICIVC 2016), P19, DOI 10.1109/ICIVC.2016.7571267
   Liu B, 2019, KNOWL-BASED SYST, V164, P235, DOI 10.1016/j.knosys.2018.10.044
   Liu Q, 2017, KNOWL-BASED SYST, V134, P189, DOI 10.1016/j.knosys.2017.07.032
   Lowe, 1999, P INT C COMP VIS, P1150, DOI DOI 10.1109/ICCV.1999.790410
   Ma C, 2017, IEEE T MULTIMEDIA, V19, P2415, DOI 10.1109/TMM.2017.2694219
   Ma X, 2018, MACH VISION APPL, V29, P749, DOI 10.1007/s00138-018-0930-2
   Ma YL, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16040446
   OJALA T, 1994, INT C PATT RECOG, P582, DOI 10.1109/ICPR.1994.576366
   Portmann J, 2014, IEEE INT CONF ROBOT, P1794, DOI 10.1109/ICRA.2014.6907094
   Qi YK, 2016, PROC CVPR IEEE, P4303, DOI 10.1109/CVPR.2016.466
   Ristic B., 2003, Beyond the Kalman Filter: Particle Filters for Tracking Applications
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Torabi A, 2012, COMPUT VIS IMAGE UND, V116, P210, DOI 10.1016/j.cviu.2011.10.006
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wang JT, 2012, PATTERN RECOGN LETT, V33, P775, DOI 10.1016/j.patrec.2011.12.011
   Wang NY, 2015, IEEE I CONF COMP VIS, P3101, DOI 10.1109/ICCV.2015.355
   Wang X, 2010, INFRARED PHYS TECHN, V53, P280, DOI 10.1016/j.infrared.2010.04.002
   Wang ZY, 2006, ICICIC 2006: FIRST INTERNATIONAL CONFERENCE ON INNOVATIVE COMPUTING, INFORMATION AND CONTROL, VOL 2, PROCEEDINGS, P233
   Wen J, 2019, IEEE T CIRC SYST VID, V29, P390, DOI 10.1109/TCSVT.2018.2799214
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Wu Z, 2014, IEEE COMPUT SOC CONF, P201, DOI 10.1109/CVPRW.2014.39
   Xu FL, 2005, IEEE T INTELL TRANSP, V6, P63, DOI 10.1109/TITS.2004.838222
   Yang T, 2017, MULTIMED TOOLS APPL, V76, P11021, DOI 10.1007/s11042-016-3461-8
   Zhang SL, 2015, IEEE T MULTIMEDIA, V17, P265, DOI 10.1109/TMM.2015.2390044
NR 56
TC 68
Z9 75
U1 1
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2020
VL 22
IS 3
BP 666
EP 675
DI 10.1109/TMM.2019.2932615
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KU2ZL
UT WOS:000519576700008
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhao, LP
   Lin, T
   Zhang, DY
   Zhou, KL
   Wang, SH
AF Zhao, Liping
   Lin, Tao
   Zhang, Dongyu
   Zhou, Kailun
   Wang, Shuhui
TI An Ultra-Low Complexity and High Efficiency Approach for Lossless Alpha
   Channel Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Complexity theory; Channel coding; Image coding; Standards; Video
   coding; Unified modeling language; Image coding; alpha channel coding;
   string matching; prediction coding; variable length code
ID SCREEN CONTENT; COMPRESSION; ALGORITHM
AB Alpha channel is being applied in an increasing number of mobile web applications on mobile devices that require ultra-low power consumption in all cases including compute-intensive video encoding and decoding. Thus, we propose an ultra-low coding complexity and high efficiency alpha channel lossless coding approach. A novel coding framework and four new coding schemes are proposed for alpha channel coding. The framework fuses a string matching technique and a proposed prediction coding scheme named bit-depth preserving prediction (BDPP) together to reduce the correlations within and between repeated identical patterns and neighboring pixels. To achieve a good tradeoff between complexity and efficiency, either the unmatchable bytes are coded directly or the BDPP residuals of unmatchable bytes are coded by a proposed bytewise entropy coding scheme named 0.5-1-2byte-size-code. The other string matching parameters are coded by another proposed bytewise entropy coding scheme named byte-size multi-variable-length-code. To speed up the string-matching search, we apply a fast string search scheme that combines special position search and hash-based search. For the selected typical 236 alpha test images, compared with x265 in the fastest configuration and lossless mode, the proposed lossless approach achieves 14.33% less total compressed bytes with only 2.75% encoding and 1.83% decoding runtime. The proposed approach also outperforms the conventional lossless coding techniques such as LZ4HC, ZLIB, and PNG.
C1 [Zhao, Liping] Shaoxing Univ, Dept Comp Sci & Engn, Shaoxing 312000, Peoples R China.
   [Lin, Tao; Zhang, Dongyu; Zhou, Kailun; Wang, Shuhui] Tongji Univ, Inst VLSI, Coll Elect & Informat Engn, Shanghai 200092, Peoples R China.
C3 Shaoxing University; Tongji University
RP Zhao, LP (corresponding author), Shaoxing Univ, Dept Comp Sci & Engn, Shaoxing 312000, Peoples R China.; Lin, T (corresponding author), Tongji Univ, Inst VLSI, Coll Elect & Informat Engn, Shanghai 200092, Peoples R China.
EM zhaoliping_jian@126.com; lintao@tongji.edu.cn; zhangdongyu24@163.com;
   kailun_zh@tongji.edu.cn; shw@tongji.edu.cn
RI zhao, liping/N-4269-2017
FU Natural Science Foundation of Zhejiang Province [LY19F020015]; National
   Natural Science Foundation of China [61871289, 61601200]; Public Service
   Technology ApplicationResearch Project of Shaoxing city [2018C10015];
   Natural Science Foundation of Shanghai Province [18ZR1440600,
   19ZR1461100]
FX This work was supported in part by the Natural Science Foundation of
   Zhejiang Province (LY19F020015), in part by the National Natural Science
   Foundation of China under Grants 61871289 and 61601200, in part by the
   Public Service Technology ApplicationResearch Project of Shaoxing city
   (2018C10015), and in part by the Natural Science Foundation of Shanghai
   Province (18ZR1440600, 19ZR1461100).
CR Abdoli M, 2018, IEEE SIGNAL PROC LET, V25, P1690, DOI 10.1109/LSP.2018.2871872
   Alakuijala Jyrki, 2017, LOSSLESS TRANSPARENC
   [Anonymous], 2019, JVETM1002
   [Anonymous], 2003, 15948 ISOIEC
   Brady N, 1999, IEEE T CIRC SYST VID, V9, P1170, DOI 10.1109/76.809154
   Chen WG, 1997, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL I, P500, DOI 10.1109/ICIP.1997.647959
   Cuiling Lan, 2015, 2015 Visual Communications and Image Processing (VCIP), P1, DOI 10.1109/VCIP.2015.7457915
   Gailly J., 1996, 1950 RFC
   Gao Wen., 2015, Advanced Video Coding Systems
   Guo LW, 2018, PICT COD SYMP, P26, DOI 10.1109/PCS.2018.8456302
   Guo LW, 2014, IEEE IMAGE PROC, P5556, DOI 10.1109/ICIP.2014.7026124
   Kim DH, 2017, J MATER CHEM C, V5, P1216, DOI 10.1039/c6tc04786f
   Lin T., 2017, M4283 AVS
   Lin T, 2017, J ELECTRON INF TECHN, V39, P351, DOI 10.11999/JEIT160560
   Lin T, 2013, IEEE T CIRC SYST VID, V23, P173, DOI 10.1109/TCSVT.2012.2223871
   Liu WQ, 2018, IEEE T CONSUM ELECTR, V64, P110, DOI 10.1109/TCE.2018.2810480
   Luo F., 2017, M4318 AVS
   Ma S., 2017, TELECOMMUN SCI, V33, P2
   Ma Z, 2017, IEEE T MULTIMEDIA, V19, P2322, DOI 10.1109/TMM.2017.2737944
   Naccari M., 2013, JCTVCO0132
   Naccari M, 2013, IEEE INT WORKSH MULT, P200, DOI 10.1109/MMSP.2013.6659288
   Peng WH, 2016, IEEE J EM SEL TOP C, V6, P393, DOI 10.1109/JETCAS.2016.2608971
   Qiang L, 2009, 2009 INTERNATIONAL SYMPOSIUM ON INTELLIGENT SIGNAL PROCESSING AND COMMUNICATION SYSTEMS (ISPACS 2009), P343, DOI 10.1109/ISPACS.2009.5383832
   Smith A., 1995, 7 MICR TECH
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tsang SH, 2019, IEEE T MULTIMEDIA, V21, P269, DOI 10.1109/TMM.2018.2856078
   Wang S., 2017, M4245 AVS
   Wang S., 2017, TELECOMMUN SCI, V33, P35
   Wang SQ, 2017, IEEE T MULTIMEDIA, V19, P660, DOI 10.1109/TMM.2016.2625276
   Wang Y, 2019, ECOTOXICOLOGY, V28, P1, DOI 10.1007/s10646-018-1987-4
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xu XZ, 2016, IEEE J EM SEL TOP C, V6, P409, DOI 10.1109/JETCAS.2016.2597645
   [赵利平 Zhao Liping], 2018, [计算机学报, Chinese Journal of Computers], V41, P2482
   Zhao LP, 2018, IEEE T MULTIMEDIA, V20, P796, DOI 10.1109/TMM.2017.2758519
   Zhao LP, 2016, IEEE T MULTIMEDIA, V18, P339, DOI 10.1109/TMM.2015.2512539
   Zhao L, 2019, J BRAZ SOC MECH SCI, V41, DOI 10.1007/s40430-018-1510-0
   ZIV J, 1977, IEEE T INFORM THEORY, V23, P337, DOI 10.1109/TIT.1977.1055714
NR 37
TC 14
Z9 14
U1 1
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2020
VL 22
IS 3
BP 786
EP 794
DI 10.1109/TMM.2019.2931414
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KU2ZL
UT WOS:000519576700017
DA 2024-07-18
ER

PT J
AU Gu, K
   Xia, ZF
   Qiao, JF
   Lin, WS
AF Gu, Ke
   Xia, Zhifang
   Qiao, Junfei
   Lin, Weisi
TI Deep Dual-Channel Neural Network for Image-Based Smoke Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Neural networks; Training; Deep learning;
   Convolutional codes; Convolution; Safety; Smoke detection; deep
   learning; convolutional network; dual-channel network; classification
ID FEATURES
AB Smoke detection plays an important role in industrial safety warning systems and fire prevention. Due to the complicated changes in the shape, texture, and color of smoke, identifying the smoke from a given image still remains a substantial challenge, and this has accordingly aroused a considerable amount of research attention recently. To address the problem, we devise a new deep dual-channel neural network (DCNN) for smoke detection. In contrast to popular deep convolutional networks (e.g., Alex-Net, VGG-Net, Res-Net, and Dense-Net and the DNCNN that is specifically devoted to detecting smoke), our proposed end-to-end network is mainly composed of dual channels of deep subnetworks. In the first subnetwork, we sequentially connect multiple convolutional layers and max-pooling layers. Then, we selectively append the batch normalization layer to each convolutional layer for overfitting reduction and training acceleration. The first subnetwork is shown to be good at extracting the detailed information of smoke, such as texture. In the second subnetwork, in addition to the convolutional, batch normalization, and max-pooling layers, we further introduce two important components. One is the skip connection for avoiding the vanishing gradient and improving the feature propagation. The other is the global average pooling for reducing the number of parameters and mitigating the overfitting issue. The second subnetwork can capture the base information of smoke, such as contours. We finally deploy a concatenation operation to combine the aforementioned two deep subnetworks to complement each other. Based on the augmented data obtained by rotating the training images, our proposed DCNN can promptly and stably converge to the perfect performance. Experimental results conducted on the publicly available smoke detection database verify that the proposed DCNN has attained a very high detection rate that exceeds 99.5 on average, superior to state-of-the-art relevant competitors. Furthermore, our DCNN only employs approximately one-third of the parameters needed by the comparatively tested deep neural networks. The source code of DCNN will be released at https://kegu.netlify.com/.
C1 [Gu, Ke; Qiao, Junfei] Beijing Univ Technol, Fac Informat Technol, Beijing Key Lab Computat Intelligence & Intellige, Beijing Adv Innovat Ctr Future Internet Technol, Beijing 100124, Peoples R China.
   [Xia, Zhifang] Beijing Univ Technol, Fac Informat Technol, Beijing Adv Innovat Ctr Future Internet Technol, Beijing 100124, Peoples R China.
   [Xia, Zhifang] State Informat Ctr PR China, Beijing, Peoples R China.
   [Lin, Weisi] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
C3 Beijing University of Technology; Beijing University of Technology;
   Nanyang Technological University
RP Gu, K (corresponding author), Beijing Univ Technol, Fac Informat Technol, Beijing Key Lab Computat Intelligence & Intellige, Beijing Adv Innovat Ctr Future Internet Technol, Beijing 100124, Peoples R China.
EM guke.doctor@gmail.com; spidergir121@163.com; junfeiq@bjut.edu.cn;
   wslin@ntu.edu.sg
RI Lin, Weisi/A-3696-2011; Lin, Weisi/A-8011-2012; Gu, Ke/AAJ-9684-2021
OI Lin, Weisi/0000-0001-9866-1947; 
FU National Science Foundation of China [61703009]; Young Elite Scientist
   Sponsorship Program by China Association for Science and Technology
   [2017QNRC001]; Young Top-Notch Talents Team Program of Beijing Excellent
   Talents Funding [2017000026833ZK40]; National Science and Technology
   Major Project of the Ministry of Science and Technology of China
   [2018ZX07111005]
FX This work was supported in part by the National Science Foundation of
   China under Grant 61703009, the Young Elite Scientist Sponsorship
   Program by China Association for Science and Technology under Grant
   2017QNRC001, the Young Top-Notch Talents Team Program of Beijing
   Excellent Talents Funding under Grant 2017000026833ZK40, and the
   National Science and Technology Major Project of the Ministry of Science
   and Technology of China under Grant 2018ZX07111005. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Vasileios Mezaris.
CR Abadi M, ARXIV, DOI DOI 10.48550/ARXIV.1603.04467
   [Anonymous], 2015, ARXIV PREPRINT ARXIV
   [Anonymous], 2018, What Do We Understand About Convolutional Networks?
   Avendi MR, 2016, MED IMAGE ANAL, V30, P108, DOI 10.1016/j.media.2016.01.005
   Chollet F, 2015, KERAS
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dubey SR, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2577887
   Fagnant DJ, 2015, TRANSPORT RES A-POL, V77, P167, DOI 10.1016/j.tra.2015.04.003
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Glorot X., 2010, INT C ARTIFICIAL INT, P249
   Gonzalez R. C., 2006, PEARSON ED INDIA, V3rd
   Gu K, 2019, IEEE T IND ELECTRON, V66, P3176, DOI 10.1109/TIE.2018.2840515
   Gu K, 2018, IEEE T IND INFORM, V14, P3946, DOI 10.1109/TII.2018.2793950
   Guan JW, 2017, IEEE T MULTIMEDIA, V19, P2505, DOI 10.1109/TMM.2017.2703148
   Gubbi J, 2009, FIRE SAFETY J, V44, P1110, DOI 10.1016/j.firesaf.2009.08.003
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Kordelas GA, 2016, IEEE T MULTIMEDIA, V18, P155, DOI 10.1109/TMM.2015.2505905
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9, DOI 10.1007/3-540-49430-8_2
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li ST, 2013, IEEE T IMAGE PROCESS, V22, P2864, DOI 10.1109/TIP.2013.2244222
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu CB, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0145955
   Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35
   Ning GH, 2018, IEEE T MULTIMEDIA, V20, P1246, DOI 10.1109/TMM.2017.2762010
   Qiu ZF, 2018, IEEE T MULTIMEDIA, V20, P939, DOI 10.1109/TMM.2017.2759504
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Soh PW, 2018, IEEE ACCESS, V6, P38186, DOI 10.1109/ACCESS.2018.2849820
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Takahashi N, 2018, IEEE T MULTIMEDIA, V20, P513, DOI 10.1109/TMM.2017.2751969
   Toreyin B. Ugur, 2005, 2005 13th European Signal Processing Conference, P1
   Wang H., 2017, On the Origin of Deep Learning
   Yin ZJ, 2017, IEEE ACCESS, V5, P18429, DOI 10.1109/ACCESS.2017.2747399
   Yuan F, 2008, PATTERN RECOGN LETT, V29, P925, DOI 10.1016/j.patrec.2008.01.013
   Yuan FN, 2017, IEEE ACCESS, V5, P6833, DOI 10.1109/ACCESS.2017.2697408
   Yuan FN, 2016, INFORM SCIENCES, V372, P225, DOI 10.1016/j.ins.2016.08.040
   Yuan FN, 2012, PATTERN RECOGN, V45, P4326, DOI 10.1016/j.patcog.2012.06.008
   Yuan FN, 2011, FIRE SAFETY J, V46, P132, DOI 10.1016/j.firesaf.2011.01.001
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang SQ, 2018, IEEE T MULTIMEDIA, V20, P1576, DOI 10.1109/TMM.2017.2766843
NR 44
TC 128
Z9 137
U1 1
U2 78
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2020
VL 22
IS 2
BP 311
EP 323
DI 10.1109/TMM.2019.2929009
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KH5FZ
UT WOS:000510676300003
DA 2024-07-18
ER

PT J
AU Xiao, KF
   Mao, SW
   Tugnait, JK
AF Xiao, Kefan
   Mao, Shiwen
   Tugnait, Jitendra K.
TI Robust QoE-Driven DASH Over OFDMA Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article; Proceedings Paper
CT 59th Annual IEEE Global Communications Conference (IEEE GLOBECOM) /
   Conference on Network Coding and Applications (NetCod)
CY DEC 04-08, 2016
CL Washington, DC
SP IEEE, Natl Instruments, AT & T, Huawei, Intel, Qualcomm, Nokia, Samsung, Keysight Technologies, Rohde & Schwarz
DE Videos; Quality of experience; Resource management; OFDM; Streaming
   media; Servers; Optimization; Dynamic Adaptive Streaming over HTTP
   (DASH); rate adaptation; resource allocation; Quality of Experience
   (QoE); Orthogonal Frequency Division Multiple Access (OFDMA)
ID RESOURCE-ALLOCATION; VIDEO; QUALITY; FAIRNESS
AB In this paper, the problem of effective and robust delivery of Dynamic Adaptive Streaming over HTTP (DASH) videos over an orthogonal frequency-division multiplexing access (OFDMA) network is studied. Motivated by a measurement study, we propose to explore the request interval and robust rate prediction for DASH over OFDMA. We first formulate an offline cross-layer optimization problem based on a novel quality of experience (QoE) model. Then the online reformulation is derived and proved to be asymptotically optimal. After analyzing the structure of the online problem, we propose a decomposition approach to obtain a user equipment (UE) rate adaptation problem and a BS resource allocation problem. We introduce stochastic model predictive control (SMPC) to achieve high robustness on video rate adaption and consider the request interval for more efficient resource allocation. Extensive simulations show that the proposed scheme can achieve a better QoE performance compared with other variations and a benchmark algorithm, which is mainly due to its lower rebuffering ratio and more stable bitrate choices.
C1 [Xiao, Kefan; Mao, Shiwen; Tugnait, Jitendra K.] Auburn Univ, Dept Elect & Comp Engn, Auburn, AL 36839 USA.
C3 Auburn University System; Auburn University
RP Mao, SW (corresponding author), Auburn Univ, Dept Elect & Comp Engn, Auburn, AL 36839 USA.
EM kzx0002@auburn.edu; smao@ieee.org; jktugnait@auburn.edu
RI Mao, Shiwen/AAY-4471-2020
OI Tugnait, Jitendra/0000-0002-0220-2453; Mao, Shiwen/0000-0002-7052-0007
FU US NSF [ECCS-1923717, CNS-1822055]; Wireless Engineering Research and
   Education Center (WEREC) at Auburn University
FX This work was supported in part by the US NSF under Grants ECCS-1923717
   and CNS-1822055, and in part by the Wireless Engineering Research and
   Education Center (WEREC) at Auburn University. This work was presented
   at IEEE GLOBECOM, Washington, DC, USA, December 2016. The Associate
   Editor coordinating the review of this manuscript and approving it for
   publication was Dr. S. Mehrotra.
CR Almowuena S, 2016, IEEE T MULTIMEDIA, V18, P102, DOI 10.1109/TMM.2015.2502067
   [Anonymous], CISC VIS NETW IND GL
   [Anonymous], 2011, Proc. second annu. acm conf. multimed. syst.-mmsys'11, DOI DOI 10.1145/1943552.1943574
   [Anonymous], DASH JS PROJ
   [Anonymous], SMOOTH STREAM PROT
   [Anonymous], ARXIV14016476
   Balachandran A, 2013, ACM SIGCOMM COMP COM, V43, P339, DOI 10.1145/2534169.2486025
   Bethanabhotla D, 2015, IEEE T COMMUN, V63, P268, DOI 10.1109/TCOMM.2014.2378774
   Bokani A, 2015, IEEE T MULTIMEDIA, V17, P2297, DOI 10.1109/TMM.2015.2494458
   Boyd S., 2004, CONVEX OPTIMIZATION
   Chen C, 2015, IEEE J-STSP, V9, DOI 10.1109/JSTSP.2014.2337277
   Chen C, 2013, INT CONF ACOUST SPEE, P3602, DOI 10.1109/ICASSP.2013.6638329
   Cisco, 2020, FOR METH 2016 2021
   Dobrian F, 2011, ACM SIGCOMM COMP COM, V41, P362, DOI 10.1145/2043164.2018478
   Go Y, 2015, IEEE T MULTIMEDIA, V17, P1646, DOI 10.1109/TMM.2015.2451951
   Harold J., 2003, Stochastic approximation and recursive algorithm and applications
   Hu SH, 2014, IEEE GLOB COMM CONF, P1336, DOI 10.1109/GLOCOM.2014.7036993
   Huang JW, 2009, IEEE T WIREL COMMUN, V8, P288, DOI 10.1109/T-WC.2009.071266
   Huang TY, 2014, SIGCOMM'14: PROCEEDINGS OF THE 2014 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P187, DOI 10.1145/2619239.2626296
   Jiang JC, 2016, 13TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION (NSDI '16), P137
   Jiang JC, 2014, IEEE ACM T NETWORK, V22, P326, DOI 10.1109/TNET.2013.2291681
   Joseph V, 2014, IEEE INFOCOM SER, P82, DOI 10.1109/INFOCOM.2014.6847927
   Khan A., 2009, PROC IEEE INT C COMM, P1
   Kushner HJ, 2004, IEEE T WIREL COMMUN, V3, P1250, DOI [10.1109/TWC.2004.830826, 10.1109/twc.2004.830826]
   Lederer S., 2012, P 3 MULT SYST C, P89
   Li Z, 2014, IEEE J SEL AREA COMM, V32, P719, DOI 10.1109/JSAC.2014.140405
   Lin K, 2017, IEEE T MULTIMEDIA, V19, P1654, DOI 10.1109/TMM.2017.2678198
   Mao ZW, 2008, IEEE T WIREL COMMUN, V7, P440, DOI 10.1109/TWC.2008.060546
   Palomar DP, 2006, IEEE J SEL AREA COMM, V24, P1439, DOI 10.1109/JSAC.2006.879350
   Ren ZY, 2014, IEEE T VEH TECHNOL, V63, P2139, DOI 10.1109/TVT.2014.2311235
   Shams F, 2014, COMPUT NETW, V65, P129, DOI 10.1016/j.comnet.2014.03.017
   Wang X, 2011, IEEE T INFORM THEORY, V57, P4359, DOI 10.1109/TIT.2011.2145770
   Winstein Keith, 2013, 10 USENIX S NETW SYS, P459
   Xiao K., 2016, Proc. of GLOBECOM, P1
   Xu YH, 2012, 2012 7TH INTERNATIONAL ICST CONFERENCE ON COMMUNICATIONS AND NETWORKING IN CHINA (CHINACOM), P109, DOI 10.1109/ChinaCom.2012.6417458
   Yin XQ, 2015, ACM SIGCOMM COMP COM, V45, P325, DOI 10.1145/2785956.2787486
   You JY, 2010, SIGNAL PROCESS-IMAGE, V25, P482, DOI 10.1016/j.image.2010.02.002
   Zhou C, 2016, IEEE T MULTIMEDIA, V18, P738, DOI 10.1109/TMM.2016.2522650
NR 38
TC 10
Z9 10
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2020
VL 22
IS 2
BP 474
EP 486
DI 10.1109/TMM.2019.2929929
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA KH5FZ
UT WOS:000510676300015
OA hybrid
DA 2024-07-18
ER

PT J
AU Chandrakala, S
   Jayalakshmi, SL
AF Chandrakala, S.
   Jayalakshmi, S. L.
TI Generative Model Driven Representation Learning in a Hybrid Framework
   for Environmental Audio Scene and Sound Event Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Sound event recognition; environmental audio scene recognition; audio
   surveillance; mel frequency cepstral coefficients (MFCCs); hidden markov
   model; support vector machine; representation learning; adapted Gaussian
   mixture model
ID ACOUSTIC SCENES; CLASSIFICATION
AB The analysis of sound information is helpful for audio surveillance, multimedia information retrieval, audio tagging, and forensic applications. Environmental audio scene recognition (EASR) and sound event recognition (SER) for audio surveillance are challenging tasks due to the presence of multiple sound sources, background noises, and the existence of overlapping or polyphonic contexts. We focus on learning robust and compact representations for environmental audio scenes and sound events using mel-frequency cepstral coefficients as basic features, which have proved to be effective in speech and audio-related tasks. In this paper, we propose a common hybrid model-based framework that learns representations with the help of generative models. We explore instance-specific adapted Gaussian mixture models for environmental audio scenes and instance-specific hidden Markov models for sound events to compute a robust, compact, and discriminatory representations. A discriminative model based classifier is then used to recognize these representations as environmental audio scenes and sound events. The performance of the proposed approaches is evaluated using the DCASE2013 scene dataset and TUT-DCASE2016 scene dataset for EASR task. Environmental Sound Classification (ESC-10) and UrbanSound8K datasets are used for SER task. The recognition accuracy of the proposed framework is significantly better than many of the state-of-the-art approaches proposed in the recent literature. The discriminative nature of the model-driven representations leads to improved efficiency for EASR and SER task. The proposed approaches are more suitable for tasks with less training data.
C1 [Chandrakala, S.] SASTRA Univ, Sch Comp, Intelligent Syst Grp, Thanjavur 613401, India.
   [Jayalakshmi, S. L.] Velammal Engn Coll, Dept Comp Sci & Engn, Chennai 600066, Tamil Nadu, India.
C3 Shanmugha Arts, Science, Technology & Research Academy (SASTRA);
   Velammal Engineering College
RP Chandrakala, S (corresponding author), SASTRA Univ, Sch Comp, Intelligent Syst Grp, Thanjavur 613401, India.
EM sckala@gmail.com; sathishjayalakshmi02@gmail.com
RI Jayalakshmi, S./ABI-7497-2020; Jayalakshmi, S L/AAJ-2141-2020
OI S L, JAYALAKSHMI/0000-0003-0890-1420; S, Chandrakala/0000-0003-4723-1984
CR Agcaer S, 2015, EUR SIGNAL PR CONF, P2556, DOI 10.1109/EUSIPCO.2015.7362846
   Agrawal DM, 2017, EUR SIGNAL PR CONF, P1809, DOI 10.23919/EUSIPCO.2017.8081521
   [Anonymous], 2013, P 21 EUR SIGN PROC C
   [Anonymous], 2016, PROF DCASE
   [Anonymous], 2016, IEEE AASP CHALL DETE
   [Anonymous], 2016, P ADV NEUR INF PROC
   [Anonymous], 2016, DETECTION CLASSIFICA
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI DOI 10.1145/1961189.1961199
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Chachada S., 2013, Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2013, P1, DOI [10.1109/APSIPA.2013.6694338, DOI 10.1109/APSIPA.2013.6694338]
   Cheffena M, 2016, IEEE J BIOMED HEALTH, V20, P1073, DOI 10.1109/JBHI.2015.2425932
   Crocco M, 2016, ACM COMPUT SURV, V48, DOI 10.1145/2871183
   Eghbal-zadeh H, 2017, EUR SIGNAL PR CONF, P2749, DOI 10.23919/EUSIPCO.2017.8081711
   Elizalde B., 2016, P DET CLASS AC SCEN, P20
   Eronen AJ, 2006, IEEE T AUDIO SPEECH, V14, P321, DOI 10.1109/TSA.2005.854103
   Gauvain JL, 1994, IEEE T SPEECH AUDI P, V2, P291, DOI 10.1109/89.279278
   Gemmeke JortF., 2013, 2013 IEEE workshop on applications of signal processing to audio and acoustics, P1, DOI DOI 10.1109/WASPAA.2013.6701847
   Han Y., 2016, TECH REP
   Phan H, 2016, IEEE-ACM T AUDIO SPE, V24, P807, DOI 10.1109/TASLP.2016.2530401
   Jing LP, 2017, IEEE T MULTIMEDIA, V19, P2637, DOI 10.1109/TMM.2017.2703939
   Kong QQ, 2019, IEEE-ACM T AUDIO SPE, V27, P777, DOI 10.1109/TASLP.2019.2895254
   Lagrange M, 2015, J ACOUST SOC AM, V138, pEL487, DOI 10.1121/1.4935350
   Mafra G., 2016, Proc. Detection and Classification of Acoustic Scenes and Events (DCASE) Workshop, P85
   Malik H, 2013, IEEE T INF FOREN SEC, V8, P1827, DOI 10.1109/TIFS.2013.2280888
   McLoughlin I, 2015, IEEE-ACM T AUDIO SPE, V23, P540, DOI 10.1109/TASLP.2015.2389618
   Medhat F, 2017, PR INT CONF DATA SC, P389, DOI 10.1109/DSAA.2017.43
   Mesaros A, 2018, IEEE-ACM T AUDIO SPE, V26, P379, DOI 10.1109/TASLP.2017.2778423
   Mesaros A, 2016, EUR SIGNAL PR CONF, P1128, DOI 10.1109/EUSIPCO.2016.7760424
   Ntalampiras S, 2011, IEEE T MULTIMEDIA, V13, P713, DOI 10.1109/TMM.2011.2122247
   Pekalska E, 2002, PATTERN RECOGN LETT, V23, P943, DOI 10.1016/S0167-8655(02)00024-7
   Phan H, 2017, INT CONF ACOUST SPEE, P136, DOI 10.1109/ICASSP.2017.7952133
   Piczak KJ, 2015, IEEE INT WORKS MACH
   Piczak KJ, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1015, DOI 10.1145/2733373.2806390
   Rabaoui A, 2008, IEEE T INF FOREN SEC, V3, P763, DOI 10.1109/TIFS.2008.2008216
   Rakotomamonjy A, 2015, IEEE-ACM T AUDIO SPE, V23, P142, DOI 10.1109/TASLP.2014.2375575
   Ren JF, 2017, IEEE T MULTIMEDIA, V19, P447, DOI 10.1109/TMM.2016.2618218
   Reynolds DA, 2000, DIGIT SIGNAL PROCESS, V10, P19, DOI 10.1006/dspr.1999.0361
   Salamon J, 2017, IEEE SIGNAL PROC LET, V24, P279, DOI 10.1109/LSP.2017.2657381
   Salamon J, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1041, DOI 10.1145/2647868.2655045
   Salamon J, 2015, INT CONF ACOUST SPEE, P171, DOI 10.1109/ICASSP.2015.7177954
   Stowell D, 2015, IEEE T MULTIMEDIA, V17, P1733, DOI 10.1109/TMM.2015.2428998
   Takahashi N, 2018, IEEE T MULTIMEDIA, V20, P513, DOI 10.1109/TMM.2017.2751969
   Wang FZ, 2017, IEEE T MULTIMEDIA, V19, P418, DOI 10.1109/TMM.2016.2613641
   Xu Y, 2017, IEEE-ACM T AUDIO SPE, V25, P1230, DOI 10.1109/TASLP.2017.2690563
   Zhang H, 2018, CHIN OPT LETT, V16, DOI 10.3788/COL201816.020001
   Zhang ZC, 2018, LECT NOTES COMPUT SC, V11257, P356, DOI 10.1007/978-3-030-03335-4_31
NR 46
TC 17
Z9 20
U1 4
U2 38
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2020
VL 22
IS 1
BP 3
EP 14
DI 10.1109/TMM.2019.2925956
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KB6AZ
UT WOS:000506577000003
DA 2024-07-18
ER

PT J
AU Wang, SF
   Peng, GZ
AF Wang, Shangfei
   Peng, Guozhu
TI Weakly Supervised Dual Learning for Facial Action Unit Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Action unit recognition; weakly-supervised; dual learning
ID EXPRESSION RECOGNITION; PAIN
AB Current research on facial action unit (AU) recognition typically requires fully AU-annotated facial images. Compared to facial expression labeling, AU annotation is a time-consuming, expensive, and error-prone process. Inspired by dual learning, we propose a novel weakly supervised dual learning mechanism to train facial action unit classifiers from expression-annotated images. Specifically, we consider AU recognition from facial images as the main task, and face synthesis given AUs as the auxiliary task. For AU recognition, we force the recognized AUs to satisfy the expression-dependent and expression-independent AU dependencies, i.e., the domain knowledge about expressions and AUs. For face synthesis given AUs, we minimize the difference between the synthetic face and the ground truth face, which has identical recognized and given AUs. By optimizing the dual tasks simultaneously, we successfully leverage their intrinsic connections as well as domain knowledge about expressions and AUs to facilitate the learning of AU classifiers from expression-annotated image. Furthermore, we extend the proposed weakly supervised dual learning mechanism to semi-supervised dual learning scenarios with partially AU-annotated images. Experimental results on three benchmark databases demonstrate the effectiveness of the proposed approach for both tasks.
C1 [Wang, Shangfei; Peng, Guozhu] Univ Sci & Technol China, Key Lab Comp & Commun Software Anhui Prov, Hefei 230027, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Wang, SF (corresponding author), Univ Sci & Technol China, Key Lab Comp & Commun Software Anhui Prov, Hefei 230027, Peoples R China.
EM sfwang@ustc.cdu.cn; gzpeng@mail.ustc.edu.cn
OI wang, shangfei/0000-0003-1164-9895
FU National Natural Science Foundation of China [917418129, 61473270,
   61727809]; Anhui Science and Technology Agency [1804a09020038]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 917418129, 61473270, and 61727809 and
   in part by the major project from Anhui Science and Technology Agency
   (1804a09020038). The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Marco Bertini.
CR [Anonymous], 1978, PALO ALTO
   [Anonymous], 2015, P IEEE INT C AUT FAC, DOI DOI 10.1109/FG.2015.7163082
   [Anonymous], 1983, EMFACS 7 EMOTIONAL F
   [Anonymous], IEEE T AFFECT COMPUT
   [Anonymous], IEEE T AFFECT COMPUT
   Dahmane M, 2014, IEEE T MULTIMEDIA, V16, P1574, DOI 10.1109/TMM.2014.2321113
   Dibeklioglu H, 2015, IEEE T MULTIMEDIA, V17, P279, DOI 10.1109/TMM.2015.2394777
   Du SC, 2014, P NATL ACAD SCI USA, V111, pE1454, DOI 10.1073/pnas.1322355111
   Eleftheriadis S, 2015, IEEE I CONF COMP VIS, P3792, DOI 10.1109/ICCV.2015.432
   Ferrari C, 2017, IEEE T MULTIMEDIA, V19, P2666, DOI 10.1109/TMM.2017.2707341
   Fischer Asja, 2012, Progress in Pattern Recognition, Image Analysis, ComputerVision, and Applications. Proceedings 17th Iberoamerican Congress, CIARP 2012, P14, DOI 10.1007/978-3-642-33275-3_2
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   HAGER JC, 1985, BEHAV RES METH INS C, V17, P450, DOI 10.3758/BF03214448
   He D, 2016, ADV NEUR IN, V29
   Koelstra S, 2010, IEEE T PATTERN ANAL, V32, P1940, DOI 10.1109/TPAMI.2010.50
   Larochelle H., 2008, P 25 INT C MACH LEAR, P536
   Li HB, 2017, IEEE T MULTIMEDIA, V19, P2816, DOI 10.1109/TMM.2017.2713408
   Li K, 2014, IEEE T MULTIMEDIA, V16, P299, DOI 10.1109/TMM.2013.2293064
   Li YQ, 2016, PATTERN RECOGN, V60, P890, DOI 10.1016/j.patcog.2016.07.009
   Li YQ, 2013, IEEE T AFFECT COMPUT, V4, P127, DOI 10.1109/T-AFFC.2013.5
   Lucey P., 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P57, DOI 10.1109/FG.2011.5771462
   Lucey P., 2010, IEEE COMP SOC C COMP, P94, DOI 10.1109/CVPRW.2010.5543262
   MICHAEL JR, 1983, BIOMETRIKA, V70, P11, DOI 10.1093/biomet/70.1.11
   Pantic M, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P317, DOI 10.1109/ICME.2005.1521424
   Peng GZ, 2018, PROC CVPR IEEE, P2188, DOI 10.1109/CVPR.2018.00233
   Prkachin KM, 2008, PAIN, V139, P267, DOI 10.1016/j.pain.2008.04.010
   PRKACHIN KM, 1992, PAIN, V51, P297, DOI 10.1016/0304-3959(92)90213-U
   Ruiz A, 2018, IEEE T IMAGE PROCESS, V27, P3969, DOI 10.1109/TIP.2018.2830189
   Ruiz A, 2015, IEEE I CONF COMP VIS, P3703, DOI 10.1109/ICCV.2015.422
   Sandbach G, 2012, IEEE IMAGE PROC, P1813, DOI 10.1109/ICIP.2012.6467234
   SMOLENSKY P, 1986, CUCS32186 U COL BOUL
   Song Yale, 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163081
   Valstar MF, 2012, IEEE T SYST MAN CY B, V42, P28, DOI 10.1109/TSMCB.2011.2163710
   Wang C, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P302, DOI 10.1145/3240508.3240613
   Wang J, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P75, DOI 10.1145/2671188.2749311
   Wang ZH, 2013, IEEE I CONF COMP VIS, P3304, DOI 10.1109/ICCV.2013.410
   WILK MB, 1968, BIOMETRIKA, V55, P1
   Wu BY, 2015, PATTERN RECOGN, V48, P2279, DOI 10.1016/j.patcog.2015.01.022
   Xia Y., 2018, P 35 INT C MACH LEAR, P5379
   Xia YC, 2017, PR MACH LEARN RES, V70
   Xie SY, 2019, IEEE T MULTIMEDIA, V21, P211, DOI 10.1109/TMM.2018.2844085
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Zen G, 2016, IEEE T MULTIMEDIA, V18, P775, DOI 10.1109/TMM.2016.2523421
   Zhang Y, 2018, PROC CVPR IEEE, P5108, DOI 10.1109/CVPR.2018.00536
   Zhang Y, 2018, PROC CVPR IEEE, P2314, DOI 10.1109/CVPR.2018.00246
   Zhao KL, 2015, PROC CVPR IEEE, P2207, DOI 10.1109/CVPR.2015.7298833
   Zhen QK, 2016, IEEE T MULTIMEDIA, V18, P1438, DOI 10.1109/TMM.2016.2557063
NR 47
TC 7
Z9 7
U1 1
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2019
VL 21
IS 12
BP 3218
EP 3230
DI 10.1109/TMM.2019.2916063
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KJ8ZS
UT WOS:000512345200019
DA 2024-07-18
ER

PT J
AU Zhou, YF
   Jiang, RH
   Wu, X
   He, JY
   Weng, S
   Peng, Q
AF Zhou, Yi-Fan
   Jiang, Run-Hao
   Wu, Xiao
   He, Jun-Yan
   Weng, Shuang
   Peng, Qiang
TI BranchGAN: Unsupervised Mutual Image-to-Image Transfer With A Single
   Encoder and Dual Decoders
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image-to-image transfer; deep learning; generative adversarial network
ID COLOR
AB Image-to-image translation is a fundamental task for a wide range of applications, such as image style transfer, video effect generation, cross-domain retrieval, etc. Due to the limited number of labeled data, complex scenes, abstract semantics and various involved domains, image translation remains a challenging task. Compared to the supervised approaches for image translation that need a large collection of paired images for training, the unsupervised methods can significantly reduce the training cost. In this paper, an unsupervised end-to-end generative adversarial network is proposed, named BranchGAN, for mutual image-to-image transfer between two domains. A structure with one single encoder and dual decoders is novelly proposed to capture the cross-domain distributions and generate the images in both domains. Three factors, that is, pixel-level overall style, region semantics, and domain distinguishability are comprehensively considered to constrain the training process of the proposed model, corresponding to reconstruction loss, encoding loss, and adversarial loss, respectively. Experiments conducted on three benchmark datasets demonstrate the effectiveness of the proposed method that outperforms the unsupervised state-of-the-art approaches and has the competitive performance as the supervised method.
C1 [Zhou, Yi-Fan; Jiang, Run-Hao; Wu, Xiao; He, Jun-Yan; Weng, Shuang; Peng, Qiang] Southwest Jiaotong Univ, Sch Informat Sci & Technol, Xipu Campus, Chengdu 611756, Peoples R China.
C3 Southwest Jiaotong University
RP Wu, X (corresponding author), Southwest Jiaotong Univ, Sch Informat Sci & Technol, Xipu Campus, Chengdu 611756, Peoples R China.
EM yfzhou.cs@gmail.com; rhjiang11@gmail.com; wuxiaohk@home.swjtu.edu.cn;
   junyanhe1989@gmail.com; vickyshuang@outlook.com; qpeng@home.swjtu.edu.cn
OI Wu, Xiao/0000-0002-8322-8558; He, Jun-Yan/0000-0002-6628-6924
FU National Natural Science Foundation of China [61772436]; Sichuan Science
   and Technology Innovation Seedling Fund [2017RZ0015]; Foundation for
   Department of Transportation of Henan Province [2019J-2-2]; Fundamental
   Research Funds for the Central Universities
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61772436, in part by the Sichuan Science
   and Technology Innovation Seedling Fund under Grant 2017RZ0015, in part
   by the Foundation for Department of Transportation of Henan Province
   underGrant 2019J-2-2, and in part by the Fundamental Research Funds for
   the Central Universities. The associate editor coordinating the review
   of this manuscript and approving it for publication was Prof. Chang-Su
   Kim.
CR [Anonymous], 2016, P IEEE C COMPUTER VI
   [Anonymous], ARXIV160903126
   [Anonymous], 2017, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2017.632
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], 2015, PROC CVPR IEEE
   [Anonymous], 2017, P ASM 36 INT C OC
   [Anonymous], 2016, NIPS
   Anoosheh A, 2018, IEEE COMPUT SOC CONF, P896, DOI 10.1109/CVPRW.2018.00122
   Arjovsky M, 2017, arXiv preprint arXiv:1701.07875
   Benaim Sagie, 2017, Advances in neural information processing systems, P752
   Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Chu WT, 2018, IEEE T MULTIMEDIA, V20, P2491, DOI 10.1109/TMM.2018.2801718
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Deng JK, 2018, PROC CVPR IEEE, P7093, DOI 10.1109/CVPR.2018.00741
   Donahue J., 2017, ICLR, P1, DOI DOI 10.48550/ARXIV.1605.09782
   Dong WM, 2016, IEEE T VIS COMPUT GR, V22, P1088, DOI 10.1109/TVCG.2015.2440255
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Han Y, 2017, IEEE T MULTIMEDIA, V19, P80, DOI 10.1109/TMM.2016.2608000
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kim T, 2017, PR MACH LEARN RES, V70
   King DB, 2015, ACS SYM SER, V1214, P1
   Kupyn O, 2018, PROC CVPR IEEE, P8183, DOI 10.1109/CVPR.2018.00854
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lei Y, 2017, IEEE T MULTIMEDIA, V19, P740, DOI 10.1109/TMM.2016.2638204
   Li YJ, 2017, PROC CVPR IEEE, P5892, DOI 10.1109/CVPR.2017.624
   Liu Ming Yu, 2016, ADV NEURAL INF PROCE, P469
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Radford A., 2016, 4 INT C LEARN REPR I
   Salimans T., 2016, P 30 C NEUR INF PROC, P2234
   Shrivastava A, 2017, PROC CVPR IEEE, P2242, DOI 10.1109/CVPR.2017.241
   Song ZC, 2017, IEEE T MULTIMEDIA, V19, P702, DOI 10.1109/TMM.2016.2631123
   Taigman Y., 2017, INT C LEARN REPR ICL, P1
   Tylecek R, 2013, LECT NOTES COMPUT SC, V8142, P364, DOI 10.1007/978-3-642-40602-7_39
   Vondrick C., 2016, ADV NEURAL INFORM PR, P613, DOI DOI 10.13016/M26GIH-TNYZ
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Yu LT, 2017, AAAI CONF ARTIF INTE, P2852
   Zhang L, 2018, IEEE T MULTIMEDIA, V20, P128, DOI 10.1109/TMM.2017.2723841
   Zhao J, 2017, PROC INT C LEARN REP
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 44
TC 35
Z9 35
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2019
VL 21
IS 12
BP 3136
EP 3149
DI 10.1109/TMM.2019.2920613
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KJ8ZS
UT WOS:000512345200013
DA 2024-07-18
ER

PT J
AU Tang, C
   Liu, XW
   Wang, PC
   Zhang, CQ
   Li, MM
   Wang, LZ
AF Tang, Chang
   Liu, Xinwang
   Wang, Pichao
   Zhang, Changqing
   Li, Miaomiao
   Wang, Lizhe
TI Adaptive Hypergraph Embedded Semi-Supervised Multi-Label Image
   Annotation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image annotation; Semisupervised learning; Semantics; Computational
   modeling; Task analysis; Training; Computer science; Multi-label image
   annotation; hypergraph learning; semi-supervised learning; feature
   projection
ID CLASSIFICATION; RECOGNITION; MODEL
AB Multilabel image annotation attracts a lot of research interest due to its practicability in multimedia and computer vision fields, while the need for a large amount of labeled training data to achieve promising performance makes it a challenging task. Fortunately, unlabeled and relevant data are widely available and these data can be used to serve the annotation task. To this end, we propose a novel adaptive hypergraph learning (AHL) method for multilabel image annotation in a semisupervised way, in which both the limited labeled data and abundant unlabeled data are utilized to facilitate the annotation performance. In detail, we seek a multilabel propagation scheme by learning a hypergraph which is used to preserve the local geometric structures of data in a high-order manner. Meanwhile, a feature projection is integrated into AHL to obtain a latent feature space where unlabeled instances can be effectively and robustly assigned with multiple labels. Experiments on six widely used image datasets are conducted to evaluate our model and the results demonstrate that the proposed AHL outperforms other state-of-the-art semisupervised methods.
C1 [Tang, Chang; Wang, Lizhe] China Univ Geosci, Sch Comp Sci, Wuhan 430074, Hubei, Peoples R China.
   [Liu, Xinwang; Li, Miaomiao] Natl Univ Def Technol, Sch Comp Sci, Changsha 410073, Hunan, Peoples R China.
   [Wang, Pichao] Alibaba Grp US Inc, San Mateo, CA 94402 USA.
   [Zhang, Changqing] Tianjin Univ, Sch Comp Sci & Technol, Tianjin 300072, Peoples R China.
C3 China University of Geosciences; National University of Defense
   Technology - China; Tianjin University
RP Liu, XW (corresponding author), Natl Univ Def Technol, Sch Comp Sci, Changsha 410073, Hunan, Peoples R China.
EM tangchang@cug.edu.cn; xinwangliu@nudt.edu.cn; pw212@uowmail.edu.au;
   zhangchangqing@tju.edu.cn; miaomiaolinudt@gmail.com;
   lizhe.wang@gmail.com
RI Zhang, Chang/HTO-2939-2023; Tang, Chang/AAU-8995-2020; LIU,
   Xinwang/L-8089-2019; Wang, Lizhe/L-7453-2014
OI Tang, Chang/0000-0002-6515-7696; LIU, Xinwang/0000-0001-9066-1475; 
FU National Science Foundation of China [61701451, 61773392]; Fundamental
   Research Funds for the Central Universities, China University of
   Geosciences (Wuhan) [CUG170654]
FX This work was supported in part by the National Science Foundation of
   China under Grants 61701451 and 61773392, and in part by the Fundamental
   Research Funds for the Central Universities, China University of
   Geosciences (Wuhan) under Grant CUG170654. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Benoit Huet.
CR Ballan L, 2015, MULTIMED TOOLS APPL, V74, P1443, DOI 10.1007/s11042-014-1976-4
   Bao BK, 2012, IEEE T MULTIMEDIA, V14, P199, DOI 10.1109/TMM.2011.2170557
   Bao BK, 2009, P INT C INT MULT COM, P17
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Cai X, 2013, IEEE I CONF COMP VIS, P801, DOI 10.1109/ICCV.2013.104
   Cao XC, 2015, IEEE T IMAGE PROCESS, V24, P2746, DOI 10.1109/TIP.2015.2428055
   Carneiro G, 2007, IEEE T PATTERN ANAL, V29, P394, DOI 10.1109/TPAMI.2007.61
   Chang X., 2016, P BRIT MACH VIS C
   Chen A., 2013, ICML, P1274
   Chen XY, 2013, ACM T MULTIM COMPUT, V10, DOI 10.1145/2542205.2542209
   Cusano C, 2004, PROC SPIE, V5304, P330
   Datta R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1348246.1348248
   Deng C, 2015, SIGNAL PROCESS, V112, P137, DOI 10.1016/j.sigpro.2014.07.017
   Ding XM, 2016, IEEE T MULTIMEDIA, V18, P1616, DOI 10.1109/TMM.2016.2572000
   Duygulu P, 2002, LECT NOTES COMPUT SC, V2353, P97
   Fei Wu, 2015, IEEE Transactions on Big Data, V1, P109, DOI 10.1109/TBDATA.2015.2497270
   Feng SL, 2004, PROC CVPR IEEE, P1002
   Gao SH, 2014, IEEE T MULTIMEDIA, V16, P762, DOI 10.1109/TMM.2014.2299516
   Gao Y, 2014, IEEE T IMAGE PROCESS, V23, P2769, DOI 10.1109/TIP.2014.2319735
   Grangier D, 2008, IEEE T PATTERN ANAL, V30, P1371, DOI 10.1109/TPAMI.2007.70791
   Guillaumin M, 2009, IEEE I CONF COMP VIS, P309, DOI 10.1109/ICCV.2009.5459266
   Guo BL, 2016, IEEE DATA MINING, P919, DOI [10.1109/ICDM.2016.48, 10.1109/ICDM.2016.0113]
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Hong DF, 2018, LECT NOTES COMPUT SC, V11212, P478, DOI 10.1007/978-3-030-01237-3_29
   Hu D, 2006, INT C COMMUN CIRCUIT, P1605, DOI 10.1109/ICCCAS.2006.284980
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   Huang L, 2015, NEUROCOMPUTING, V149, P1573, DOI 10.1016/j.neucom.2014.08.035
   Huang Y, 2015, IEEE T MULTIMEDIA, V17, P1923, DOI 10.1109/TMM.2015.2476658
   Escalante HJ, 2010, COMPUT VIS IMAGE UND, V114, P419, DOI 10.1016/j.cviu.2009.03.008
   Jeon J., 2003, Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P119, DOI DOI 10.1145/860435.860459
   Jing LP, 2017, IEEE T IMAGE PROCESS, V26, P4612, DOI 10.1109/TIP.2017.2719939
   Jing PG, 2018, NEUROCOMPUTING, V274, P50, DOI 10.1016/j.neucom.2016.05.085
   Jing PG, 2017, IEEE T MULTIMEDIA, V19, P1050, DOI 10.1109/TMM.2016.2644866
   Jing XY, 2016, IEEE T IMAGE PROCESS, V25, P2712, DOI 10.1109/TIP.2016.2549459
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140
   Lavrenko V., 2003, NIPS
   Lei CY, 2016, IEEE T MULTIMEDIA, V18, P687, DOI 10.1109/TMM.2015.2477277
   Li BM, 2012, IN C IND ENG ENG MAN, P737, DOI 10.1109/IEEM.2012.6837837
   Li JH, 2016, PROCEEDINGS OF 2016 12TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND SECURITY (CIS), P345, DOI [10.1109/CIS.2016.0084, 10.1109/CIS.2016.83]
   Li T, 2009, IEEE INT CON MULTI, P1508, DOI 10.1109/ICME.2009.5202790
   Li X, 2018, IEEE T MULTIMEDIA, V20, P1169, DOI 10.1109/TMM.2017.2761985
   Liu J, 2006, 2006 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY, VOLS 1-6, PROCEEDINGS, P1006, DOI 10.1109/ISIT.2006.261879
   Liu XW, 2019, IEEE T PATTERN ANAL, V41, P2410, DOI 10.1109/TPAMI.2018.2879108
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu ZW, 2015, IEEE T IMAGE PROCESS, V24, P176, DOI 10.1109/TIP.2014.2375641
   Ma ZG, 2012, IEEE T MULTIMEDIA, V14, P1021, DOI 10.1109/TMM.2012.2187179
   Makadia A, 2008, LECT NOTES COMPUT SC, V5304, P316, DOI 10.1007/978-3-540-88690-7_24
   Moran S., 2014, P INT C MULTIMEDIA R, P113
   Nie FP, 2017, AAAI CONF ARTIF INTE, P2408
   Nie FP, 2016, AAAI CONF ARTIF INTE, P1302
   Patterson G, 2012, PROC CVPR IEEE, P2751, DOI 10.1109/CVPR.2012.6247998
   Somu N, 2016, J MED SYST, V40, DOI 10.1007/s10916-016-0600-8
   Tan QY, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9020109
   Tang C., 2019, P AAAI C ART INT
   Tang C, 2019, IEEE T MULTIMEDIA, V21, P1724, DOI 10.1109/TMM.2018.2889560
   Tang C, 2018, INFORM SCIENCES, V467, P219, DOI 10.1016/j.ins.2018.08.003
   Tang C, 2018, KNOWL-BASED SYST, V160, P49, DOI 10.1016/j.knosys.2018.06.016
   Tang C, 2018, KNOWL-BASED SYST, V145, P109, DOI 10.1016/j.knosys.2018.01.009
   Tang C, 2018, EXPERT SYST APPL, V96, P64, DOI 10.1016/j.eswa.2017.11.053
   Uricchio T, 2017, PATTERN RECOGN, V71, P144, DOI 10.1016/j.patcog.2017.05.019
   Verma Y, 2017, INT J COMPUT VISION, V121, P126, DOI 10.1007/s11263-016-0927-0
   Verma Y, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.25
   Verma Y, 2012, LECT NOTES COMPUT SC, V7574, P836, DOI 10.1007/978-3-642-33712-3_60
   Von Ahn L, 2004, P SIGCHI C HUM FACT, DOI DOI 10.1145/985692.985733
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wang HR, 2018, IEEE SYS MAN CYBERN, P2798, DOI 10.1109/SMC.2018.00477
   Wang J, 2016, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2016.251
   Wang M, 2015, IEEE T KNOWL DATA EN, V27, P2564, DOI 10.1109/TKDE.2015.2415497
   Wang Y., 2016, P 25 INT JOINT C ART, P2153
   Wang Y, 2018, NEURAL NETWORKS, V103, P1, DOI 10.1016/j.neunet.2018.03.006
   Wang Y, 2015, IEEE T IMAGE PROCESS, V24, P3939, DOI 10.1109/TIP.2015.2457339
   Wu BY, 2015, IEEE I CONF COMP VIS, P4157, DOI 10.1109/ICCV.2015.473
   Wu J, 2013, PATTERN RECOGN, V46, P2927, DOI 10.1016/j.patcog.2013.04.008
   Xu JJ, 2014, IEEE T MULTIMEDIA, V16, P403, DOI 10.1109/TMM.2013.2291218
   Zha ZJ, 2009, J VIS COMMUN IMAGE R, V20, P97, DOI 10.1016/j.jvcir.2008.11.009
   Zhang ML, 2015, IEEE T PATTERN ANAL, V37, P107, DOI 10.1109/TPAMI.2014.2339815
   Zhang ML, 2014, IEEE T KNOWL DATA EN, V26, P1819, DOI 10.1109/TKDE.2013.39
   Zhang S, 2016, KSII T INTERNET INF, V10, P2801, DOI 10.3837/tiis.2016.06.019
   Zhang X, 2015, 2015 IEEE 14TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA), P588, DOI 10.1109/ICMLA.2015.154
   Zhu X., 2003, P 20 INT C MACH LEAR, V3, P58, DOI DOI 10.1109/18.850663
   Zhu XF, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3581
NR 82
TC 50
Z9 52
U1 1
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2019
VL 21
IS 11
BP 2837
EP 2849
DI 10.1109/TMM.2019.2909860
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JJ7VX
UT WOS:000494363000012
DA 2024-07-18
ER

PT J
AU Li, H
   Kwong, S
   Chen, CB
   Jia, YH
   Cong, RM
AF Li, Hua
   Kwong, Sam
   Chen, Chuanbo
   Jia, Yuheng
   Cong, Runmin
TI Superpixel Segmentation Based on Square-Wise Asymmetric Partition and
   Structural Approximation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Superpixel; square-wise asymmetric partition; structural approximation;
   combinatorial optimization
AB Superpixel segmentation aims at grouping discretizing pixels into high-level correlative units and reducing the complexity of subsequent tasks, e.g., saliency detection and object tracking. Existing superpixel segmentation algorithms mainly focus on maintaining the geometrical information, while neglecting the irregular structure of superpixels. In this paper, a superpixel segmentation method is proposed to generate approximately structural superpixels with sharp boundary adherence and comprehensive semantic information. The superpixel segmentation is formulated as a square-wise asymmetric partition problem, where the semantic perceptual superpixels are recorded in a square level to preserve abundant semantic information and save storage simultaneously. Moreover, in order to achieve regular-shape superpixel units to better adhere to image boundaries and contours, a combinatorial optimization strategy is devised to achieve an optimal combination of squares and isolated pixels. Experimental comparisons with some state-of-the-art superpixel segmentation methods on the public benchmarks demonstrate the effectiveness of the proposed method quantitatively and qualitatively. In addition, we have applied the method to brain tissue segmentation to illustrate superior performance.
C1 [Li, Hua; Kwong, Sam; Jia, Yuheng; Cong, Runmin] City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
   [Li, Hua; Chen, Chuanbo] Huazhong Univ Sci & Technol, Sch Software Engn, Wuhan 430074, Hubei, Peoples R China.
   [Kwong, Sam] City Univ Hong Kong, Shenzhen Res Inst, Shenzhen 518057, Peoples R China.
   [Cong, Runmin] Beijing Jiaotong Univ, Inst Informat Sci, Beijing Key Lab Adv Informat Sci & Network Techno, Beijing 100044, Peoples R China.
C3 City University of Hong Kong; Huazhong University of Science &
   Technology; Shenzhen Research Institute, City University of Hong Kong;
   City University of Hong Kong; Beijing Jiaotong University
RP Kwong, S (corresponding author), City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
EM huali27-c@my.cityu.edu.hk; cssamk@cityu.edu.hk; chuanboc@163.com;
   yuheng.jia@my.cityu.edu.hk; runmincong@gmail.com
RI Kwong, Sam/C-9319-2012
OI Kwong, Sam/0000-0001-7484-7261; CONG, RUNMIN/0000-0003-0972-4008; LI,
   Hua/0000-0003-0740-0691
FU Natural Science Foundation of China [61672443, 61772344]; Hong Kong RGC
   General Research Funds [9042038 (CityU 11205314), 9042322 (CityU
   11200116)]
FX This work was supported in part by the Natural Science Foundation of
   China under Grant 61672443, 61772344, and in part by the Hong Kong RGC
   General Research Funds under Grants 9042038 (CityU 11205314) and 9042322
   (CityU 11200116).
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Chen CB, 2011, COMPUT ELECTR ENG, V37, P669, DOI 10.1016/j.compeleceng.2011.07.006
   Chen JS, 2017, IEEE T IMAGE PROCESS, V26, P3317, DOI 10.1109/TIP.2017.2651389
   Cong RM, 2016, IEEE SIGNAL PROC LET, V23, DOI 10.1109/LSP.2016.2557347
   Cong RM, 2019, IEEE T MULTIMEDIA, V21, P1660, DOI 10.1109/TMM.2018.2884481
   Cong RM, 2019, IEEE T CIRC SYST VID, V29, P2941, DOI 10.1109/TCSVT.2018.2870832
   Cong RM, 2019, IEEE T CYBERNETICS, V49, P233, DOI 10.1109/TCYB.2017.2771488
   Cong RM, 2018, IEEE T IMAGE PROCESS, V27, P568, DOI 10.1109/TIP.2017.2763819
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dai Tang, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P765, DOI 10.1109/ICME.2012.184
   Dora Lingraj, 2017, IEEE Rev Biomed Eng, V10, P235, DOI 10.1109/RBME.2017.2715350
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Fu HZ, 2014, IEEE T MULTIMEDIA, V16, P1165, DOI 10.1109/TMM.2014.2305571
   Giordano D, 2015, PROC CVPR IEEE, P4814, DOI 10.1109/CVPR.2015.7299114
   Guo CL, 2019, IEEE T IMAGE PROCESS, V28, P2545, DOI 10.1109/TIP.2018.2887029
   Haas S., 2012, Medical Content-Based Retrieval for Clinical Decision Support, P58
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Jampani V, 2018, LECT NOTES COMPUT SC, V11211, P363, DOI 10.1007/978-3-030-01234-2_22
   Kumar K, 2017, 7TH INTERNATIONAL CONFERENCE ON COMPUTER AND COMMUNICATION TECHNOLOGY (ICCCT - 2017), P100, DOI 10.1145/3154979.3154998
   Kumar K, 2019, J VIS COMMUN IMAGE R, V58, P345, DOI 10.1016/j.jvcir.2018.12.009
   Kumar K, 2018, MULTIMED TOOLS APPL, V77, P26635, DOI 10.1007/s11042-018-5882-z
   Kumar K, 2018, MULTIMED TOOLS APPL, V77, P7383, DOI 10.1007/s11042-017-4642-9
   Kumar K, 2018, IEEE T MULTIMEDIA, V20, P323, DOI 10.1109/TMM.2017.2741423
   Kumar K, 2017, 2017 INTERNATIONAL CONFERENCE ON INNOVATIONS IN ELECTRONICS, SIGNAL PROCESSING AND COMMUNICATION (IESC), P106, DOI 10.1109/IESPC.2017.8071874
   Levinshtein A, 2009, IEEE T PATTERN ANAL, V31, P2290, DOI 10.1109/TPAMI.2009.96
   Li CY, 2016, IEEE T IMAGE PROCESS, V26, P5664, DOI 10.1109/TIP.2016.2612882
   Liang NX, 2018, IEEE T MULTIMEDIA, V20, P2289, DOI 10.1109/TMM.2018.2803518
   Liu YJ, 2018, IEEE T PATTERN ANAL, V40, P653, DOI 10.1109/TPAMI.2017.2686857
   Liu YJ, 2016, PROC CVPR IEEE, P651, DOI 10.1109/CVPR.2016.77
   Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918
   Ming-Yu Liu, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2097, DOI 10.1109/CVPR.2011.5995323
   NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   SAMET H, 1985, COMMUN ACM, V28, P973, DOI 10.1145/4284.4290
   Shen JB, 2014, IEEE T IMAGE PROCESS, V23, P1451, DOI 10.1109/TIP.2014.2302892
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Stutz D, 2018, COMPUT VIS IMAGE UND, V166, P1, DOI 10.1016/j.cviu.2017.03.007
   Tu WC, 2018, PROC CVPR IEEE, P568, DOI 10.1109/CVPR.2018.00066
   Wang C, 2015, IEEE T MULTIMEDIA, V17, P29, DOI 10.1109/TMM.2014.2374357
   Wang LJ, 2018, IEEE T CYBERNETICS, V48, P1030, DOI 10.1109/TCYB.2017.2675910
   Wang MR, 2017, SIGNAL PROCESS-IMAGE, V56, P28, DOI 10.1016/j.image.2017.04.007
   Wang P, 2013, INT J COMPUT VISION, V103, P1, DOI 10.1007/s11263-012-0588-6
   Yang MM, 2018, IEEE T MULTIMEDIA, V20, P3008, DOI 10.1109/TMM.2018.2820327
   Zhao Q, 2018, IEEE T MULTIMEDIA, V20, P1406, DOI 10.1109/TMM.2017.2772842
NR 45
TC 9
Z9 9
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2019
VL 21
IS 10
BP 2625
EP 2637
DI 10.1109/TMM.2019.2907047
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JD1IM
UT WOS:000489728400016
DA 2024-07-18
ER

PT J
AU Li, J
   Wei, LS
   Zhang, FB
   Yang, T
   Lu, ZY
AF Li, Jing
   Wei, Lisong
   Zhang, Fangbing
   Yang, Tao
   Lu, Zhaoyang
TI Joint Deep and Depth for Object-Level Segmentation and Stereo Tracking
   in Crowds
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multiple object tracking; object-level segmentation; stereo vision;
   severe occlusion
ID PARTICLE PHD FILTER; MULTIPLE
AB Tracking multiple people in crowds is a fundamental and essential task in the multimedia field. It is often hindered by difficulties, such as dynamic occlusion between objects, cluttered background, and abrupt illumination changes. To respond to this need, in this paper, we combine deep and depth to build a stereo tracking system for crowds. The core of the system is the fusion of the advantages of deep learning and depth information, which is exploited to achieve object segmentation and improve the multiobject tracking performance in severe occlusion. More specifically, first, to obtain more accurate detection observations in the tracking system, we present a novel object-level segmentation method. This method combines the effective detection results of deep learning with depth information to obtain precise object segmentation results. Then, we integrate the segmentation results and three-dimensional (3-D) information to extract 2-D and 3-D characteristics to represent the target, and design three similarity models to realize a stereo tracking method through data association in crowds. Finally, we build a diverse stereo dataset including various challenging indoor and outdoor scenes. The comprehensive experiments verify the effective and robust tracking performance of our system in various scenarios, and the system has rich output results including segmentation results, target distance, and tracking results. Moreover, the qualitative and quantitative comparison results show that the proposed algorithm not only has good object segmentation performance but also improves the tracking performance of completely and partially occluded objects, which is superior to the tested state-of-the-art tracking approaches.
C1 [Li, Jing; Wei, Lisong; Lu, Zhaoyang] Xidian Univ, Sch Telecommun Engn, Xian 710071, Shaanxi, Peoples R China.
   [Zhang, Fangbing; Yang, Tao] Northwestern Polytech Univ, Sch Comp Sci, Xian 710129, Shaanxi, Peoples R China.
C3 Xidian University; Northwestern Polytechnical University
RP Li, J (corresponding author), Xidian Univ, Sch Telecommun Engn, Xian 710071, Shaanxi, Peoples R China.; Yang, T (corresponding author), Northwestern Polytech Univ, Sch Comp Sci, Xian 710129, Shaanxi, Peoples R China.
EM jinglixd@mail.xidian.edu.cn; lswei@stu.xidian.edu.cn;
   fbzhang@stu.xidian.edu.cn; tyang@nwpu.edu.cn; zhylu@xidian.edu.cn
FU National Natural Science Foundation of China [61502364, 61672429,
   61272288]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 61502364, 61672429, and 61272288.
CR [Anonymous], THESIS
   Bae SH, 2018, IEEE T PATTERN ANAL, V40, P595, DOI 10.1109/TPAMI.2017.2691769
   Bae SH, 2014, PROC CVPR IEEE, P1218, DOI 10.1109/CVPR.2014.159
   Bae SH, 2014, IEEE T IMAGE PROCESS, V23, P2820, DOI 10.1109/TIP.2014.2320821
   Barnich O, 2011, IEEE T IMAGE PROCESS, V20, P1709, DOI 10.1109/TIP.2010.2101613
   Bernardin K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/246309
   Bewley A, 2016, IEEE IMAGE PROC, P3464, DOI 10.1109/ICIP.2016.7533003
   Bewley A, 2016, IEEE INT CONF ROBOT, P2212, DOI 10.1109/ICRA.2016.7487371
   Bochinski Erik, 2017, 2017 14th IEEE International Conference on Advanced Video and Signal-Based Surveillance (AVSS), DOI 10.1109/AVSS.2017.8078516
   Breitenstein MD, 2011, IEEE T PATTERN ANAL, V33, P1820, DOI 10.1109/TPAMI.2010.232
   Cavallaro A, 2016, ONLINE MULTITARGET T
   Chen J, 2017, IEEE T CYBERNETICS, V47, P3892, DOI 10.1109/TCYB.2016.2587723
   Chen L, 2018, IEEE INT CON MULTI, DOI 10.1097/PEC.0000000000001553
   Dicle C, 2013, IEEE I CONF COMP VIS, P2304, DOI 10.1109/ICCV.2013.286
   Dong XP, 2017, IEEE T MULTIMEDIA, V19, P763, DOI 10.1109/TMM.2016.2631884
   Feng PM, 2017, IEEE T MULTIMEDIA, V19, P725, DOI 10.1109/TMM.2016.2638206
   Fu ZY, 2018, IEEE ACCESS, V6, P14764, DOI 10.1109/ACCESS.2018.2816805
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Hirschmüller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI 10.1109/TPAMl.2007.1166
   Kang JK, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17071598
   Kim C, 2015, IEEE I CONF COMP VIS, P4696, DOI 10.1109/ICCV.2015.533
   Kim W, 2017, IEEE ACCESS, V5, P27453, DOI 10.1109/ACCESS.2017.2775040
   Lan L, 2018, IEEE T IMAGE PROCESS, V27, P4585, DOI 10.1109/TIP.2018.2843129
   Leal-Taixé L, 2016, IEEE COMPUT SOC CONF, P418, DOI 10.1109/CVPRW.2016.59
   Li J, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17102354
   Lingaswamy S., 2016, P INT C REC TRENDS T, P1
   Liu HB, 2017, CHIN CONT DECIS CONF, P4569, DOI 10.1109/CCDC.2017.7979304
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Milan A., 2016, COMPUT RES REPOSITOR
   Moujahid D., 2015, 2015 3 WORLD C COMPL, P1, DOI [10.1109/ICoCS.2015.7483285, DOI 10.1109/ICOCS.2015.7483285]
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   REID DB, 1979, IEEE T AUTOMAT CONTR, V24, P843, DOI 10.1109/TAC.1979.1102177
   Stauffer C., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P246, DOI 10.1109/CVPR.1999.784637
   Tian YL, 2015, IEEE I CONF COMP VIS, P1904, DOI 10.1109/ICCV.2015.221
   Xiang Y, 2015, IEEE I CONF COMP VIS, P4705, DOI 10.1109/ICCV.2015.534
   Yoon JH, 2016, PROC CVPR IEEE, P1392, DOI 10.1109/CVPR.2016.155
   Yoon JH, 2015, IEEE WINT CONF APPL, P33, DOI 10.1109/WACV.2015.12
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao SC, 2018, IEEE T CYBERNETICS, V48, P2086, DOI 10.1109/TCYB.2017.2727138
   Zhou H, 2019, IEEE T CIRC SYST VID, V29, P1011, DOI 10.1109/TCSVT.2018.2825679
NR 41
TC 12
Z9 12
U1 1
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2019
VL 21
IS 10
BP 2531
EP 2544
DI 10.1109/TMM.2019.2908350
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JD1IM
UT WOS:000489728400009
DA 2024-07-18
ER

EF