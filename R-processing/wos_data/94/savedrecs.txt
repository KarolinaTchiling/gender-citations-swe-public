FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Zhang, DT
   Deng, YH
   Zhou, Y
   Zhu, YF
   Qin, X
AF Zhang, Datong
   Deng, Yuhui
   Zhou, Yi
   Zhu, Yifeng
   Qin, Xiao
TI Improving the Performance of Deduplication-Based Backup Systems via
   Container Utilization Based Hot Fingerprint Entry Distilling
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Data deduplication; data backup; storage system; disk bottleneck; data
   fragmentation
AB Data deduplication techniques construct an index consisting of fingerprint entries to identify and eliminate duplicated copies of repeating data. The bottleneck of disk-based index lookup and data fragmentation caused by eliminating duplicated chunks are two challenging issues in data deduplication. Deduplication-based backup systems generally employ containers storing contiguous chunks together with their fingerprints to preserve data locality for alleviating the two issues, which is still inadequate. To address these two issues, we propose a container utilization based hot fingerprint entry distilling strategy to improve the performance of deduplication-based backup systems. We divide the index into three parts: hot fingerprint entries, fragmented fingerprint entries, and useless fingerprint entries. A container with utilization smaller than a given threshold is called a sparse container. Fingerprint entries that point to non-sparse containers are hot fingerprint entries. For the remaining fingerprint entries, if a fingerprint entry matches any fingerprint of forthcoming backup chunks, it is classified as a fragmented fingerprint entry. Otherwise, it is classified as a useless fingerprint entry. We observe that hot fingerprint entries account for a small part of the index, whereas the remaining fingerprint entries account for the majority of the index. This intriguing observation inspires us to develop a hot fingerprint entry distilling approach named HID. HID segregates useless fingerprint entries from the index to improve memory utilization and bypass disk accesses. In addition, HID separates fragmented fingerprint entries to make a deduplication-based backup system directly rewrite fragmented chunks, thereby alleviating adverse fragmentation. Moreover, HID introduces a feature to treat fragmented chunks as unique chunks. This feature compensates for the shortcoming that a Bloom filter cannot directly identify certain duplicated chunks (i.e., the fragmented chunks). To take full advantage of the preceding feature, we propose an evolved HID strategy called EHID. EHID incorporates a Bloom filter, to which only hot fingerprints are mapped. In doing so, EHID exhibits two salient features: (i) EHID avoids disk accesses to identify unique chunks and the fragmented chunks; (ii) EHID slashes the false positive rate of the integrated Bloom filter. These salient features push EHID into the high-efficiency mode. Our experimental results show our approach reduces the average memory overhead of the index by 34.11% and 25.13% when using the Linux dataset and the FSL dataset, respectively. Furthermore, compared with the state-of-the-art method HAR, EHID boosts the average backup throughput by up to a factor of 2.25 with the Linux dataset, and EHID reduces the average disk I/O traffic by up to 66.21% when it comes to the FSL dataset. EHID also marginally improves the system's restore performance.
C1 [Zhang, Datong; Deng, Yuhui] Jinan Univ, Dept Comp Sci, Guangzhou 510632, Guangdong, Peoples R China.
   [Zhou, Yi] Columbus State Univ, TSYS Sch Comp Sci, 4225 Univ Ave, Columbus, GA 31907 USA.
   [Zhu, Yifeng] Univ Maine, Sch Elect & Comp Engn, Orono, ME USA.
   [Qin, Xiao] Auburn Univ, Dept Comp Sci & Software Engn, Auburn, AL 36849 USA.
C3 Jinan University; University System of Georgia; Columbus State
   University; University of Maine System; University of Maine Orono;
   Auburn University System; Auburn University
RP Deng, YH (corresponding author), Jinan Univ, Dept Comp Sci, Guangzhou 510632, Guangdong, Peoples R China.
EM dtz228@yeah.net; tyhdeng@email.jnu.edu.cn; zhou_yi@columbusstate.edu;
   zhu@eece.maine.edu; xqin@auburn.edu
OI Deng, Yuhui/0000-0002-1522-8943
FU National Natural Science Foundation of China [62072214]; International
   Cooperation Project of Guangdong Province [2020A0505100040]; Science and
   Technology Planning Project of Guangzhou [202103000036]; Open Project
   Program of Wuhan National Laboratory for Optoelectronics [2020WNLOKF006]
FX This work was sponsored by the National Natural Science Foundation of
   China under grant 62072214, the International Cooperation Project of
   Guangdong Province under grant 2020A0505100040, the Science and
   Technology Planning Project of Guangzhou (202103000036), and the Open
   Project Program of Wuhan National Laboratory for Optoelectronics
   (2020WNLOKF006).
CR [Anonymous], 2017, P 2017 USENIX ANN TE
   Bhagwat D, 2009, 2009 IEEE INTERNATIONAL SYMPOSIUM ON MODELING, ANALYSIS & SIMULATION OF COMPUTER AND TELECOMMUNICATION SYSTEMS (MASCOTS), P237
   Biggar H., 2007, Experiencing data de-duplication: Improving efficiency and reducing capacity requirements
   BLOOM BH, 1970, COMMUN ACM, V13, P422, DOI 10.1145/362686.362692
   Cao Zhichao, 2019, P 17 USENIX C FIL ST
   Cao Zhichao, 2018, P 16 USENIX C FIL ST
   Debnath Biplob K, 2010, P 2010 USENIX ANN TE, P1
   Deng YH, 2011, ACM COMPUT SURV, V43, DOI 10.1145/1922649.1922660
   FSL, TRAC SNAPSH PUBL ARC
   Fu Min, 2014, P USENIX ANN TECHN C
   Fu Min, 2015, P 13 USENIX C FIL ST
   Gants J., 2010, Digital universe decade-are you ready?
   Gantz J., 2012, The Digital Universe in 2020: Big Data, Bigger Digital Shadows, and Biggest Growth in the Far East, V2012, P1
   Guo Fanglu, 2011, P 2011 USENIX ANN TE, P1
   Harnik D, 2010, IEEE SECUR PRIV, V8, P40, DOI 10.1109/MSP.2010.187
   Kaczmarczyk Michal., 2012, Proceedings of the 5th Annual International Systems and Storage Conference, P1
   kernel, Linux Kernel Archives
   Koller R, 2010, ACM T STORAGE, V6, DOI 10.1145/1837915.1837921
   Lillibridge M., 2009, P 7 USENIX C FIL STO
   Lillibridge Mark, 2013, P 11 USENIX C FIL ST
   Mao B, 2014, ACM T STORAGE, V10, DOI 10.1145/2512348
   Meister Dirk, 2013, P 6 INT SYST STOR C
   Muthitacharoen A., 2001, Operating Systems Review, V35, P174, DOI 10.1145/502059.502052
   Nam Young Jin, 2012, P IEEE 20 INT S MOD
   Nam Youngjin, 2011, P 13 IEEE INT C HIGH
   Ni F, 2019, PROCEEDINGS OF THE 2019 TENTH ACM SYMPOSIUM ON CLOUD COMPUTING (SOCC '19), P220, DOI 10.1145/3357223.3362731
   Paulo J, 2014, ACM COMPUT SURV, V47, DOI 10.1145/2611778
   Quinlan S, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE FAST'02 CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P89
   Rabin M. O., 1981, Fingerprinting by random polynomials
   Tarasov Vasily, 2012, P 2012 USENIX ANN TE
   Xia Dan FengWen, 2011, P 2011 USENIX ANN TE
   Xia Nai, 2018, P 16 USENIX C FIL ST
   Zhou YT, 2018, IEEE ACCESS, V6, P15743, DOI 10.1109/ACCESS.2018.2800763
   Zhu B, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P269
   Zou Xiangyu, 2021, P 19 USENIX C FIL ST
NR 35
TC 5
Z9 5
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2021
VL 17
IS 4
AR 30
DI 10.1145/3459626
PG 23
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YY2GY
UT WOS:000754612700006
DA 2024-07-18
ER

PT J
AU Kim, J
   Choi, K
   Lee, W
   Kim, J
AF Kim, Joonsung
   Choi, Kanghyun
   Lee, Wonsik
   Kim, Jangwoo
TI Performance Modeling and Practical Use Cases for Black-Box SSDs
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Performance modeling; black-box SSDs; storage system
ID GARBAGE COLLECTION; FLASH; ARCHITECTURE
AB Modern servers are actively deploying Solid-State Drives (SSDs) thanks to their high throughput and low latency. However, current server architects cannot achieve the full performance potential of commodity SSDs, as SSDs are complex devices designed for specific goals (e.g., latency, throughput, endurance, cost) with their internal mechanisms undisclosed to users. In this article, we propose SSDcheck, a novel SSD performance model to extract various internal mechanisms and predict the latency of next access to commodity black-box SSDs. We identify key performance-critical features (e.g., garbage collection, write buffering) and find their parameters (i.e., size, threshold) from each SSD by using our novel diagnosis code snippets. Then, SSDcheck constructs a performance model for a target SSD and dynamically manages the model to predict the latency of the next access. In addition, SSDcheck extracts and provides other useful internal mechanisms (e.g., fetch unit in multi-queue SSDs, background tasks triggering idle-time interval) for the storage system to fully exploit SSDs. By using those useful features and the performance model, we propose multiple practical use cases. Our evaluations show that SSDcheck's performance model is highly accurate, and proposed use cases achieve significant performance improvement in various scenarios.
C1 [Kim, Joonsung; Choi, Kanghyun; Lee, Wonsik; Kim, Jangwoo] Seoul Natl Univ, Seoul 151742, South Korea.
C3 Seoul National University (SNU)
RP Kim, J (corresponding author), Seoul Natl Univ, Seoul 151742, South Korea.
EM joonsung90@snu.ac.kr; kanghyun_choi@snu.ac.kr; wonsik.lee@snu.ac.kr;
   jangwoo@snu.ac.kr
RI Kim, Joonsung/AAZ-3889-2021
OI Kim, Joonsung/0000-0002-5432-7813
FU National Research Foundation of Korea (NRF) - Korean Government
   [NRF-2019R1A5A1027055, NRF-2021R1A2C3014131, NRF-2021M3F3A2A02037893];
   Institute of Information&Communications Technology Planning & Evaluation
   (IITP) - Korean Government (MSIT) [2021-0-00853]; Creative Pioneering
   Researchers Program through Seoul National University; Automation and
   Systems Research Institute (ASRI); Inter-university Semiconductor
   Research Center (ISRC) at Seoul National University
FX work was partly supported by National Research Foundation of Korea (NRF)
   grant funded by the Korean Government (NRF-2019R1A5A1027055,
   NRF-2021R1A2C3014131, NRF-2021M3F3A2A02037893), Institute of
   Information&Communications Technology Planning & Evaluation (IITP) grant
   funded by the Korean Government (MSIT) (No. 2021-0-00853), and Creative
   Pioneering Researchers Program through Seoul National University. We
   also appreciate the support from Automation and Systems Research
   Institute (ASRI), Inter-university Semiconductor Research Center (ISRC)
   at Seoul National University.
CR Agrawal Nitin, 2008, P USENIX ANN TECHN C, P57
   Axboe, 2020, FIO BENCHMARK
   Bjorling M, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P359
   Boboila S, 2011, IEEE S MASS STOR SYS
   Bux W, 2010, PERFORM EVALUATION, V67, P1172, DOI 10.1016/j.peva.2010.07.003
   Cai Y, 2015, I C DEPEND SYS NETWO, P438, DOI 10.1109/DSN.2015.49
   Chang Li-Pin, 2005, ACM T STORAGE, V1, P4
   Chen F, 2011, INT S HIGH PERF COMP, P266, DOI 10.1109/HPCA.2011.5749735
   Chen F, 2009, PERF E R SI, V37, P181
   CHEN J. B., 1993, P 14 ACM S OP SYST P
   Chen SZ, 1996, IEEE T COMPUT, V45, P1116, DOI 10.1109/12.543706
   Colgrove J, 2015, SIGMOD'15: PROCEEDINGS OF THE 2015 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P1683, DOI 10.1145/2723372.2742798
   Desnoyers Peter., 2012, P INT SYSTEMS STORAG, P12
   Dirik C, 2009, CONF PROC INT SYMP C, P279, DOI 10.1145/1555815.1555790
   Easen, 2020, P SNIA
   Gray Jim, 2008, ACM QUEUE, V6
   Grupp Laura M., 2013, P 2008 C USENIX ANN
   Gupta A, 2009, ACM SIGPLAN NOTICES, V44, P229, DOI 10.1145/1508284.1508271
   Hahn SS, 2013, IEEE S MASS STOR SYS
   Hao MZ, 2020, PROCEEDINGS OF THE 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '20), P173
   Hao MZ, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P263
   He Jun, 2015, P 13 USENIX C FIL ST
   Hu Y, 2013, IEEE T COMPUT, V62, P1141, DOI 10.1109/TC.2012.60
   Huang, 2017, P 15 USENIX C FIL ST
   Huang HH, 2011, IEEE S MASS STOR SYS
   HUANG Peng, 2014, P 9 EUR C COMP SYST, P1
   Huang Ping, 2014, P 2014 C USENIX ANN
   Huffman Amber, 2017, P FLASH MEM SUMM
   Jung M, 2014, CONF PROC INT SYMP C, P289, DOI 10.1109/ISCA.2014.6853216
   Jung Myoungsoo, 2013, P ACM SIGMETRICS INT
   Jung Myoungsoo, 2012, P ACM IFIP USENIX IN
   Kang Jeong-Uk, 2014, P USENIX WORKSH HOT
   Kang WH, 2014, SIGMOD'14: PROCEEDINGS OF THE 2014 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P529, DOI 10.1145/2588555.2595632
   Kelsey J, 2002, LECT NOTES COMPUT SC, V2365, P263
   Kim Jaeho, 2015, P 13 USENIX C FIL ST
   Kim J, 2012, IEEE T COMPUT, V61, P636, DOI 10.1109/TC.2011.76
   Kim J, 2018, IEEE COMPUT ARCHIT L, V17, P80, DOI 10.1109/LCA.2017.2779122
   Kim JY, 2009, MATER SCI FORUM, V620-622, P295, DOI 10.4028/www.scientific.net/MSF.620-622.295
   Kim J, 2018, 2018 51ST ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE (MICRO), P455, DOI 10.1109/MICRO.2018.00044
   KIM MY, 1991, IEEE T COMPUT, V40, P801, DOI 10.1109/12.83618
   Kim Y, 2014, IEEE T COMPUT, V63, P888, DOI 10.1109/TC.2012.256
   Lee Changman, 2015, 13 USENIX C FILE STO, P273
   Lee Edward K., 1993, P 1993 ACM SIGMETRIC
   Lee J, 2015, IEEE COMPUT ARCHIT L, V14, P79, DOI 10.1109/LCA.2014.2298394
   Lee J, 2013, IEEE T COMPUT AID D, V32, P247, DOI 10.1109/TCAD.2012.2227479
   Lee J, 2011, INT SYM PERFORM ANAL, P12, DOI 10.1109/ISPASS.2011.5762711
   Lee SW, 2009, ACM SIGMOD/PODS 2009 CONFERENCE, P863
   Lee SJ, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P339
   Li Shan, 2010, P IEEE INT S MOD AN
   Li Yongkun, 2015, P 6 ACM SPEC INT C P
   Ma Dongzhe, 2011, P 2011 ACM SIGMOD IN
   Merchant, 2017, P N9TH INT S MODELIN
   Metz J., 2018, EVOLUTION FUTURE NVM
   Min C., 2012, P 10 USENIX C FIL ST, P1
   Oh, 2014, BLACKHAT US
   Oh Yongseok, 2014, P 12 USENIX C FIL ST
   Ouyang J, 2014, ACM SIGPLAN NOTICES, V49, P471, DOI 10.1145/2541940.2541959
   Park C, 2008, ACM T EMBED COMPUT S, V7, DOI 10.1145/1376804.1376806
   Park Stan, 2012, P 10 USENIX C FIL ST
   Qiu S, 2013, IEEE S MASS STOR SYS
   Rho E, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P257
   Saxena Mohit, 2010, P 2010 C USENIX ANN
   Shen Kai, 2013, P 2013 C USENIX ANN
   Shu Frank, 2007, MANAGEMENT 2007
   Skourtis Dimitris, 2014, P 2014 C USENIX ANN
   Song X, 2014, IEEE COMPUT ARCHIT L, V13, P61, DOI 10.1109/L-CA.2013.22
   Tavakkol A, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P49
   Tavakkol A, 2014, INT CONFER PARA, P417, DOI 10.1145/2628071.2628098
   Van Houdt Benny, 2013, ACM SIGMETRICS PERF, V41, P1
   Wang H, 2013, IEEE S MASS STOR SYS
   Weil SA, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P307
   Weil Sage A., 2007, PDSW, P35
   Wu Guanying, 2012, P 7 ACM EUR C COMP S
   Xilinx, 2020, ZYNQ 7000 XC7Z045
   Yan SQ, 2017, ACM T STORAGE, V13, DOI 10.1145/3121133
   Yang SL, 2015, SOSP'15: PROCEEDINGS OF THE TWENTY-FIFTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P474, DOI 10.1145/2815400.2815421
   Yongkun Li, 2013, Performance Evaluation Review, V41, P179
   Zambelli C, 2012, DES AUT TEST EUROPE, P881
   Zhang Y., 2012, FAST
   Zhao K., 2013, P 11 USENIX C FIL ST
   Zhou Y., 2005, IACR Cryptol. ePrint Arch, P388
NR 81
TC 8
Z9 8
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUN
PY 2021
VL 17
IS 2
AR 14
DI 10.1145/3440022
PG 38
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SV7JF
UT WOS:000663994900006
DA 2024-07-18
ER

PT J
AU Mohan, J
   Martinez, A
   Ponnapalli, S
   Raju, P
   Chidambaram, V
AF Mohan, Jayashree
   Martinez, Ashlie
   Ponnapalli, Soujanya
   Raju, Pandian
   Chidambaram, Vijay
TI CrashMonkey and ACE: Systematically Testing File-System Crash
   Consistency
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Crash consistency; testing; file systems; bugs
AB We present CRASHMONKEY and ACE, a set of tools to systematically find crash-consistency bugs in Linux file systems. CRASHMONKEY is a record-and-replay framework which tests a given workload on the target file system by simulating power-loss crashes while the workload is being executed, and checking if the file system recovers to a correct state after each crash. ACE automatically generates all the workloads to be run on the target file system. We build CRASHMONKEY and ACE based on a new approach to test file-system crash consistency: bounded black-box crash testing (B-3). B-3 tests the file system in a black-box manner using workloads of file-system operations. Since the space of possible workloads is infinite, B-3 bounds this space based on parameters such as the number of file-system operations or which operations to include, and exhaustively generates workloads within this bounded space. B-3 builds upon insights derived from our study of crash-consistency bugs reported in Linux file systems in the last 5 years. We observed that most reported bugs can be reproduced using small workloads of three or fewer file-system operations on a newly created file system, and that all reported bugs result from crashes after fsync()-related system calls. CRASHMONKEY and ACE are able to find 24 out of the 26 crash-consistency bugs reported in the last 5 years. Our tools also revealed 10 new crash-consistency bugs in widely used, mature Linux file systems, 7 of which existed in the kernel since 2014. Additionally, our tools found a crash-consistency bug in a verified file system, FSCQ. The new bugs result in severe consequences like broken rename atomicity, loss of persisted files and directories, and data loss.
C1 [Mohan, Jayashree; Martinez, Ashlie; Ponnapalli, Soujanya; Raju, Pandian; Chidambaram, Vijay] Univ Texas Austin, 110 Inner Campus Dr, Austin, TX 78705 USA.
   [Chidambaram, Vijay] VMWare Res, 110 Inner Campus Dr, Austin, TX 78705 USA.
C3 University of Texas System; University of Texas Austin; VMware, Inc.
RP Mohan, J (corresponding author), Univ Texas Austin, 110 Inner Campus Dr, Austin, TX 78705 USA.
EM jaya@cs.utexas.edu; ashmrtn@utexas.edu; soujanyap95@gmail.com;
   pandian4mail@gmail.com; vijay@cs.utexas.edu
RI Ponnapalli, Soujanya/JXX-5968-2024; Chidambaram, Vijay/HJA-2695-2022
OI Chidambaram, Vijay/0000-0001-7985-6087
FU NSF [CNS-1751277]
FX This material is based uponwork supported by the NSF under CNS-1751277
   and generous donations from VMware, Google, and Facebook.
CR Aghayev A, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P105
   Amazon, 2018, AMAZON EC2 DEMAND PR
   [Anonymous], 2018, BTRFS CHECK
   Apple, 2018, FSYNC 2 MAC OS X DEV
   Bacik Josef, 2018, DM LOG WRITES TARGET
   Bacik Josef, 2013, XFSTESTS ADD RENAME
   Bhat SS, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P69, DOI 10.1145/3132747.3132779
   Bo Liu, 2018, BTRFS FIX UNEXPECTED
   Bornholt J, 2016, ACM SIGPLAN NOTICES, V51, P83, DOI 10.1145/2954679.2872406
   Chajed Tej, 2018, DISABLE LOGGED WRITE
   Chen HG, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P270, DOI 10.1145/3132747.3132776
   Chen HG, 2015, SOSP'15: PROCEEDINGS OF THE TWENTY-FIFTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P18, DOI 10.1145/2815400.2815402
   Chidambaram V., 2012, Proceedings of the 10th USENIX conference on File and Storage Technologies, FAST 2012, San Jose, CA, USA, February 14-17, 2012, P9
   Chidambaram V, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P228, DOI 10.1145/2517349.2522726
   Chidambaram Vijay, 2018, BTRFS STRANGE BEHAV
   Chidambaram Vijay, 2015, THESIS
   Chidambaram Vijay, 2018, BTRFS SYMLINK NOT PE
   Chinner Dave, 2018, WARNING BAD UNLOCK B
   Chinner Dave, 2018, BTRFS SYMLINK NOT PE
   Chinner Dave, 2018, SYMLINK NOT PERSISTE
   Chinner Dave, 2018, INFO TASK HUNG XLOG
   Corbet Jonathan, 2014, BETTER TESTING
   Drysdale D., 2016, Linux Weekly News, V2, P33
   Guan Eryu, 2018, EXT4 UPDATE IDISKSIZ
   Guan Eryu, 2015, FSTESTS GENERIC TEST
   Guan Eryu, 2017, EXT4 FIX FDATASYNC 2
   Hartman Greg Kroah, 2016, BTRFS FIX EMPTY SYML
   HITZ D, 1994, PROCEEDINGS OF THE WINTER 1994 USENIX CONFERENCE, P235
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hu Yige, 2018, P 2018 USENIX ANN TE
   Jones D., 2011, P 13 OTT LIN S
   Kingma D. P., 2014, arXiv
   Kumar H, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P197
   Lee Changman, 2015, 13 USENIX C FILE STO, P273
   Mambretti J, 2015, 2015 INTERNATIONAL CONFERENCE ON CLOUD COMPUTING RESEARCH AND INNOVATION (ICCCRI), P73, DOI 10.1109/ICCCRI.2015.10
   Manana Filipe, 2015, BTRFS REMOVE DELETED
   Manana Filipe, 2015, BTRFS ADD MISSING IN
   Manana Filipe, 2014, BTRFS FIX DIRECTORY
   Manana Filipe, 2018, BTRFS STRANGE BEHAV
   Manana Filipe, 2015, BTRFS FIX STALE DIRE
   Manana Filipe, 2015, BTRFS FIX FSYNC DATA
   Manana Filipe, 2018, BTRFS FIX LOG REPLAY
   Manana Filipe, 2018, BTRFS BLOCKS ALLOCAT
   Manana Filipe, 2016, BTRFS FIX FILE LOSS
   Manana Filipe, 2016, FSTESTS GENERIC TEST
   Manana Filipe, 2015, BTRFS FIX HOLE PUNCH
   Manana Filipe, 2015, BTRFS FIX METADATA I
   Manana Filipe, 2016, BTRFS FIX INCORRECT
   Manana Filipe, 2018, GENERIC TEST FSYNC N
   Manana Filipe, 2018, RE STRANGE BEHAV POS
   Manana Filipe, 2018, BTRFS SYNC LOG LOGGI
   Manana Filipe, 2014, BTRFS FIX FSYNC DATA
   Martinez A, 2017, INT CONF INTEL ENVIR, P68, DOI 10.1109/IE.2017.35
   Mason Chris, 2018, BTRFS INCONSISTENT B
   Mathur Avantika, 2007, P LINUX S, V2, P21
   McKusick MK, 1999, PROCEEDINGS OF THE FREENIX TRACK, P1
   McMillan R., 2012, AMAZON BLAMES GENERA
   Miller R., 2013, DATA CTR OUTAGE CITE
   Miller R., 2012, POWER OUTAGE HITS
   Miller R., 2013, POWER OUTAGE KNOCKS
   Mohan J, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P33
   Mohan Jayashree, 2018, BTRFS HARD LINK NOT
   Mohan Jayashree, 2018, BTRFS INCONSISTENT B
   Nossum Vegard, 2016, FILESYSTEM FUZZING A
   Pillai TS, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P181
   Pillai Thanumalayan Sankaranarayana, 2014, P 11 USENIX S OP SYS, P433
   POSIX, 2018, FSYNC OP GROUP BAS S
   Prabhakaran V, 2005, USENIX ASSOCIATION PROCEEDINGS OF THE GENERAL TRACK: 2005 UNENIX ANNUAL TECHNICAL CONFERENCE, P105
   Rho E, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P257
   Rodeh O, 2013, ACM T STORAGE, V9, DOI 10.1145/2501620.2501623
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   Sigurbjarnarson H, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P1
   Son J, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18010227
   Sweeney A, 1996, PROCEEDINGS OF THE USENIX 1996 ANNUAL TECHNICAL CONFERENCE, P1
   The Open Group, 2018, OPEN GROUP BASE SPEC
   Ts'o Ted, 2018, SYMLINK NOT PERSISTE
   Ts'o Theodore Y., 2018, BTRFS INCONSISTENT B
   UTSASLab, 2018, NEW CRASH CONS BUGS
   UTSASLab, 2018, CRASH CONS BUGS STUD
   Verge J., 2014, INTERNAP DATA CTR OU
   Vyukov Dmitry, 2018, SYZBOT
   Wolffradt R. S. V., 2014, YOUR DATA CTR POWER
   Won Y, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P211
   Yang JF, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE SIXTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDE '04), P273
   Yang Junfeng, 2006, P 7 S OP SYST DES IM
   Yu Chao, 2018, F2FS ENFORCE FSYNC M
   Yu Chao, 2018, F2FS FIX SET KEEP SI
   Yu Chao, 2017, F2FS KEEP ISIZE ONCE
   Yuan J, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P1
   Zhang SL, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P15
   Zheng Mai., 2014, Proceedings of the 11th USENIX Symposium on Operating Systems Design and Implementation OSDI '14, P449
NR 91
TC 4
Z9 5
U1 1
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUN
PY 2019
VL 15
IS 2
AR 14
DI 10.1145/3320275
PG 34
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JL3IK
UT WOS:000495424300007
OA Bronze
DA 2024-07-18
ER

PT J
AU Guz, Z
   Li, H
   Shayesteh, A
   Balakrishnan, V
AF Guz, Zvika
   Li, Harry (Huan)
   Shayesteh, Anahita
   Balakrishnan, Vijay
TI Performance Characterization of NVMe-over-Fabrics Storage Disaggregation
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE NVMe-over-fabrics; performance characterization; network storage
AB Storage disaggregation separates compute and storage to different nodes to allow for independent resource scaling and, thus, better hardware resource utilization. While disaggregation of hard-drives storage is a common practice, NVMe-SSD (i.e., PCIe-based SSD) disaggregation is considered more challenging. This is because SSDs are significantly faster than hard drives, so the latency overheads (due to both network and CPU processing) as well as the extra compute cycles needed for the offloading stack become much more pronounced.
   In this work, we characterize the overheads of NVMe-SSD disaggregation. We show that NVMe-overFabrics (NVMe-oF)-a recently released remote storage protocol specification-reduces the overheads of remote access to a bare minimum, thus greatly increasing the cost-efficiency of Flash disaggregation. Specifically, while recent work showed that SSD storage disaggregation via iSCSI degrades application-level throughput by 20%, we report on negligible performance degradation with NVMe-oF-both when using stress-tests as well as with a more-realistic KV-store workload.
C1 [Guz, Zvika; Li, Harry (Huan); Shayesteh, Anahita; Balakrishnan, Vijay] Samsung Semicond Inc, Memory Platform Lab, 3655 N 1st St, San Jose, CA 95134 USA.
C3 Samsung Electronics; Samsung Semiconductor (SSI)
RP Guz, Z (corresponding author), Samsung Semicond Inc, Memory Platform Lab, 3655 N 1st St, San Jose, CA 95134 USA.
EM zvika.guz@samsung.com; harry.li@samsung.com; anahita.sh@samsung.com;
   vijay.bala@samsung.com
CR Amazon, 2008, AM EL BLOCK STOR
   [Anonymous], 2018, ser. Synthesis Lectures on Computer Architecture, DOI DOI 10.2200/S00516ED2V01Y201306CAC024
   [Anonymous], 2014, USENIX Annual Technical Conference
   Bjorling Matias., 2013, Proceedings of the 6th International Systems and Storage Conference, P22
   Chelsio Communications, 2014, LUST IWARP RDMA 40GB
   Cohen D, 2009, 2009 17TH IEEE SYMPOSIUM ON HIGH-PERFORMANCE INTERCONNECTS (HOTI 2009), P123, DOI 10.1109/HOTI.2009.23
   Cully B., 2014, Proc. of the 12th USENIX Conf. on File and Storage Technol, P17
   Datium, 2018, OP CONV
   Dell Networking, 2015, RDMA CONVERGED ETHER
   Excelero, 2017, EXC NVMESH
   FusionIO, 2013, FUS IO FLASH MEM RAM
   Han S., 2013, 12 ACM WORKSH HOT TO, DOI DOI 10.1145/2535771.2535778
   Harty Kieran, 2016, DONT CONFUSE HYPERCO
   HGST, 2014, LINK SCAL 200 MILL U
   Hoff B., 2016, RDMA INTERCONNECTS P
   Huffman Amber, 2012, REVISION
   IBM Research, 2017, CRAIL
   Intel, 2016, STOR PERF DEV KIT
   Intel, 2016, INT XEON PROC E5 269
   Intel, 2017, SPDK NVME FABR TARG
   Kanev S, 2015, 2015 ACM/IEEE 42ND ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA), P158, DOI 10.1145/2749469.2750392
   Kim Hyeong-Jun, 2016, 8 USENIX WORKSH HOT, P41
   Kim John, 2016, ETHERNET RDMA PROTOC
   Kim John F., 2014, P STOR DEV C SNIA
   Klimovic A, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901337
   Ko Mike, 2003, RELEASE SPECIFICATIO
   Lee EK, 1996, ACM SIGPLAN NOTICES, V31, P84, DOI 10.1145/248209.237157
   Liu J., 2003, Proceedings of the 17th annual international conference on Supercomputing, ICS '03, P295
   Loboz C, 2012, J GRID COMPUT, V10, P85, DOI 10.1007/s10723-012-9211-x
   Lu XY, 2013, PROC INT CONF PARAL, P641, DOI 10.1109/ICPP.2013.78
   Manese Charlie, 2014, FACEBOOK OPEN COMPUT
   Mellanox, 2015, SN2700
   Mellanox, 2015, CONNECT X 4 VPI 100G
   Mickens James., 2014, Proceedings of the 11th USENIX Conference on Networked Systems Design and Implementation. NSDI'14, P257
   Microsoft, 2016, IMPR PERF FIL SERV S
   Minturn D., 2015, ETHERNET STORAGE FOR
   Minturn Dave, 2016, P NVME ALL HANDS M
   MySQL, 2018, MYSQL CUSTOMERS
   MySQL, 1998, MYSQL
   NVM Express, 2016, NVM EXPR OV FABR 1 0
   Ouyang J, 2014, ACM SIGPLAN NOTICES, V49, P471, DOI 10.1145/2541940.2541959
   Parlmer Julia, 2016, TECHNICAL REPORT
   Patiejunas Kestutis, 2016, P DAT SCAL
   Percona Lab, 2008, TPCC MYSQL
   RDMA Consortium, 2002, TECHNICAL REPORT
   Recio Ro, 2005, TECHNICAL REPORT
   Robinson Simon, 2015, TECHNICAL REPORT
   Rostedt Steven., 2008, ftrace-Function Tracer kernel documentation
   Salmon Brandon, 2015, WEB SCALE VS HYPERCO
   Samsung, 2015, PM1725 NVME PCIE SSD
   Schroeder B, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P67
   Sharwood Simon, 2016, DISAGGREGATED HYPER
   Son Y, 2014, 2014 INTERNATIONAL CONFERENCE ON CLOUD AND AUTONOMIC COMPUTING (ICCAC 2014), P258, DOI 10.1109/ICCAC.2014.14
   T10, 2003, SCSI RDMA PROT 2 SRP
   Transaction Processing Performance Council, 2010, TPC C BENCHM STAND S
   VMWare, 2017, VMWARE VSAN
   WALKER B, 2016, STOR DEV C SNIA
   Warfield Andrew, 2013, COHO DATA VIRTUALIZA, V3
   X bit labs, 2016, OCZ DEM 4 TIB 16 TIB
   Xu Qiumin, 2015, P 8 ACM INT SYST STO, DOI DOI 10.1145/2757667.2757684
   Yang Jisoo, 2012, P 10 USENIX C FIL ST, P3
   Yu YJ, 2014, ACM T COMPUT SYST, V32, DOI 10.1145/2619092
NR 62
TC 18
Z9 20
U1 2
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD DEC
PY 2018
VL 14
IS 4
AR 31
DI 10.1145/3239563
PG 18
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ4JO
UT WOS:000457140500003
OA Bronze
DA 2024-07-18
ER

PT J
AU Zuo, PF
   Zhou, QH
   Sun, JZ
   Yang, L
   Zhang, SW
   Hua, Y
   Cheng, J
   He, RF
   Yan, HB
AF Zuo, Pengfei
   Zhou, Qihui
   Sun, Jiazhao
   Yang, Liu
   Zhang, Shuangwu
   Hua, Yu
   Cheng, James
   He, Rongfeng
   Yan, Huabing
TI RACE: One-sided RDMA-conscious Extendible Hashing
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Disaggregated data center; remote direct memory access; hashing index
   structure
AB Memory disaggregation is a promising technique in datacenters with the benefit of improving resource utilization, failure isolation, and elasticity. Hashing indexes have been widely used to provide fast lookup services in distributed memory systems. However, traditional hashing indexes become inefficient for disaggregated memory, since the computing power in the memory pool is too weak to execute complex index requests. To provide efficient indexing services in disaggregated memory scenarios, this article proposes RACE hashing, a one-sided RDMA-Conscious Extendible hashing index with lock-free remote concurrency control and efficient remote resizing. RACE hashing enables all index operations to be efficiently executed by using only one-sided RDMA verbs without involving any compute resource in the memory pool. To support remote concurrent access with high performance, RACE hashing leverages a lock-free remote concurrency control scheme to enable different clients to concurrently operate the same hashing index in the memory pool in a lock-free manner. To resize the hash table with low overheads, RACE hashing leverages an extendible remote resizing scheme to reduce extra RDMA accesses caused by extendible resizing and allow concurrent request execution during resizing. Extensive experimental results demonstrate that RACE hashing outperforms stateof-the-art distributed in-memory hashing indexes by 1.4-13.7x in YCSB hybrid workloads.
C1 [Zuo, Pengfei; Sun, Jiazhao; Yang, Liu; Zhang, Shuangwu; He, Rongfeng] Huawei Cloud, Shenzhen, Guangdong, Peoples R China.
   [Zhou, Qihui; Cheng, James] Chinese Univ Hong Kong, Hong Kong, Peoples R China.
   [Hua, Yu] Huazhong Univ Sci & Technol, Wuhan, Hubei, Peoples R China.
   [He, Rongfeng; Yan, Huabing] Huawei, Chengdu, Sichuan, Peoples R China.
C3 Huawei Technologies; Chinese University of Hong Kong; Huazhong
   University of Science & Technology; Huawei Technologies
RP Zuo, PF (corresponding author), Huawei Cloud, Shenzhen, Guangdong, Peoples R China.
EM pfzuo.cs@gmail.com; qhzhou@cse.cuhk.edu.hk; sunjiazhao@huawei.com;
   yangliu100@huawei.com; zhangshuangwu@huawei.com; csyhua@hust.edu.cn;
   jcheng@cse.cuhk.edu.hk; herongfeng@huawei.com; yanhuabing@huawei.com
RI Cheng, Sheung Chak James/A-3741-2011
OI Yan, Huabing/0000-0002-1666-531X; Zuo, Pengfei/0000-0001-9982-5130;
   zhou, qihui/0000-0002-8927-3325; SUN, Jiazhao/0000-0002-3581-2477
CR Aguilera MK, 2018, PROCEEDINGS OF THE 2018 USENIX ANNUAL TECHNICAL CONFERENCE, P775
   Aguilera MK, 2019, PROCEEDINGS OF THE WORKSHOP ON HOT TOPICS IN OPERATING SYSTEMS (HOTOS '19), P120, DOI 10.1145/3317550.3321433
   [Anonymous], Memcached:A distributed memory object caching system
   Asanovic Krste., 2014, USENIX FAST, V13
   Atikoglu Berk, 2012, Performance Evaluation Review, V40, P53, DOI 10.1145/2318857.2254766
   Birrittella MS, 2015, PROCEEDINGS 2015 IEEE 23RD ANNUAL SYMPOSIUM ON HIGH-PERFORMANCE INTERCONNECTS - HOTI 2015, P1, DOI 10.1109/HOTI.2015.22
   Carbonari A, 2017, HOTNETS-XVI: PROCEEDINGS OF THE 16TH ACM WORKSHOP ON HOT TOPICS IN NETWORKS, P164, DOI 10.1145/3152434.3152447
   Chen ZY, 2020, PROCEEDINGS OF THE 2020 USENIX ANNUAL TECHNICAL CONFERENCE, P799
   Cooper Brian F., 2010, P 1 ACM S CLOUD COMP, P143, DOI [DOI 10.1145/1807128.1807152, 10.1145/1807128.1807152]
   Costa P, 2015, SIGCOMM'15: PROCEEDINGS OF THE 2015 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P551, DOI 10.1145/2785956.2787492
   De Moura AL, 2009, ACM T PROGR LANG SYS, V31, DOI 10.1145/1462166.1462167
   Dragojevic Aleksandar, 2014, NSDI 14, P401
   Fagin R., 1979, ACM Transactions on Database Systems, V4, P315, DOI 10.1145/320083.320092
   Fan Bin, 2013, 10 USENIX S NETW SYS, P371
   Faraboschi P., 2015, P 15 WORKSH HOT TOP
   Franklin MJ, 1997, ACM T DATABASE SYST, V22, P315, DOI 10.1145/261124.261125
   Gao PX, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P249
   Gen-Z Technology, 2022, US
   He YJ, 2020, PROC VLDB ENDOW, V14, P431, DOI 10.14778/3430915.3430932
   Herlihy M, 2008, LECT NOTES COMPUT SC, V5218, P350, DOI 10.1007/978-3-540-87779-0_24
   Hewlett Packard Corporation, 2015, MACH NEW KIND COMP
   Intel Corporation, 2022, Intel rack scale design (intel rsd)
   Kalia A, 2016, PROCEEDINGS OF USENIX ATC '16: 2016 USENIX ANNUAL TECHNICAL CONFERENCE, P437
   Kocberber Onur, 2013, 2013 46th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO). Proceedings, P468, DOI 10.1145/2540708.2540748
   Li BJ, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P137, DOI 10.1145/3132747.3132756
   Li S, 2015, 2015 ACM/IEEE 42ND ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA), P476, DOI 10.1145/2749469.2750416
   Li XF, 2014, MPhil thesis, P1, DOI [10.1145/2592798.2592820, DOI 10.1145/2592798.2592820]
   Lim H, 2017, SIGMOD'17: PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P21, DOI 10.1145/3035918.3064015
   Lim K, 2012, INT S HIGH PERF COMP, P189
   Lim K, 2009, CONF PROC INT SYMP C, P267, DOI 10.1145/1555815.1555789
   Mao Y., 2012, P 7 ACM EUR C COMP S, P183
   Mitchell Christopher, 2013, P USENIX ANN TECHN C, P103
   Mitzenmacher M, 2001, IEEE T PARALL DISTR, V12, P1094, DOI 10.1109/71.963420
   Nam M, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P31
   Nguyen N, 2014, INT CON DISTR COMP S, P627, DOI 10.1109/ICDCS.2014.70
   Nishtala R., 2013, Proceedings of the 10th USENIX Conference on Networked Systems Design and Implementation, NSDI '13, Berkeley, CA, USA, P385
   Pagh R, 2004, J ALGORITHMS, V51, P122, DOI 10.1016/j.jalgor.2003.12.002
   Redox, 2022, About us
   Ruan ZY, 2020, PROCEEDINGS OF THE 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '20), P315
   Shan YZ, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P69
   Shrivastav V, 2019, PROCEEDINGS OF THE 16TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P255
   Tsai SY, 2020, PROCEEDINGS OF THE 2020 USENIX ANNUAL TECHNICAL CONFERENCE, P33
   Tsai SY, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P306, DOI 10.1145/3132747.3132762
   Wang CX, 2020, PROCEEDINGS OF THE 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '20), P261
   Wang TZ, 2018, PROC INT CONF DATA, P461, DOI 10.1109/ICDE.2018.00049
   Wei XD, 2015, SOSP'15: PROCEEDINGS OF THE TWENTY-FIFTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P87, DOI 10.1145/2815400.2815419
   Yu XY, 2014, PROC VLDB ENDOW, V8, P209, DOI 10.14778/2735508.2735511
   Zuo PF, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P461
NR 48
TC 4
Z9 4
U1 2
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAY
PY 2022
VL 18
IS 2
AR 11
DI 10.1145/3511895
PG 29
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 1H3XN
UT WOS:000796478700002
DA 2024-07-18
ER

PT J
AU Zhu, BH
   Chen, YM
   Wang, Q
   Lu, YY
   Shu, JW
AF Zhu, Bohong
   Chen, Youmin
   Wang, Qing
   Lu, Youyou
   Shu, Jiwu
TI Octopus<SUP>+</SUP>: An RDMA-Enabled Distributed Persistent Memory File
   System
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article; Proceedings Paper
CT 14th USENIX Symposium on Operating Systems Design and Implementation
   (OSDI)
CY NOV 04-06, 2020
CL ELECTR NETWORK
SP USENIX Assoc, Alibaba Grp, Amazon, ANT Grp, Alipay, Apple, ByteDance, Facebook, Google, Microsoft, VMware, Oracle, Two Sigma, ACM Queue, DMTF, FreeBSD Fdn
DE Storage system; distributed system; remote direct memory access;
   persistent memory
ID PHASE-CHANGE MEMORY
AB Non-volatile memory and remote direct memory access (RDMA) provide extremely high performance in storage and network hardware. However, existing distributed file systems strictly isolate file system and network layers, and the heavy layered software designs leave high-speed hardware under-exploited.
   In this article, we propose an RDMA-enabled distributed persistent memory file system, Octopus(+), to redesign file system internal mechanisms by closely coupling non-volatile memory and RDMA features. For data operations, Octopus(+) directly accesses a shared persistent memory pool to reduce memory copying overhead, and actively fetches and pushes data all in clients to rebalance the load between the server and network. For metadata operations, Octopus(+) introduces self-identified remote procedure calls for immediate notification between file systems and networking, and an efficient distributed transaction mechanism for consistency. Octopus(+) is enabled with replication feature to provide better availability. Evaluations on Intel Optane DC Persistent Memory Modules show that Octopus(+) achieves nearly the raw bandwidth for large I/Os and orders of magnitude better performance than existing distributed file systems.
C1 [Zhu, Bohong; Chen, Youmin; Wang, Qing; Lu, Youyou; Shu, Jiwu] Tsinghua Univ, Beijing, Peoples R China.
C3 Tsinghua University
RP Shu, JW (corresponding author), Tsinghua Univ, Beijing, Peoples R China.
EM zhubh18@mails.tsinghua.edu.cn; chenym16@mails.tsinghua.edu.cn;
   q-wang18@mails.tsinghua.edu.cn; luyouyou@tsinghua.edu.cn;
   shujw@tsinghua.edu.cn
FU National Key Research & Development Program of China [2018YFB1003301];
   National Natural Science Foundation of China [61832011, 61772300]
FX This work is supported by National Key Research & Development Program of
   China (grant 2018YFB1003301) and the National Natural Science Foundation
   of China (grant 61832011, 61772300).
CR Aguilera MK, 2018, PROCEEDINGS OF THE 2018 USENIX ANNUAL TECHNICAL CONFERENCE, P775
   [Anonymous], 2011, SC 11 P 2011 INT C H
   Chen YM, 2020, TWENTY-FIFTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXV), P1077, DOI 10.1145/3373376.3378515
   CohortFS, 2014, CEPH ACC
   Condit J, 2009, SOSP'09: PROCEEDINGS OF THE TWENTY-SECOND ACM SIGOPS SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P133
   Dong MK, 2019, PROCEEDINGS OF THE TWENTY-SEVENTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '19), P478, DOI 10.1145/3341301.3359637
   Douglas Chet., 2015, RDMA with PMEM: Software Mechanisms for Enabling Access to Remote Persistent Memory
   Dragojevic Aleksandar, 2014, NSDI 14, P401
   Dulloor Subramanya R., 2014, P EUR C COMP SYST EU, P1, DOI [10.1145/2592798.2592814, DOI 10.1145/2592798.2592814]
   GitHub, 2017, CRAIL FAST MULTT DIS
   Gluster, 2020, GLUSTERFS RDMA
   Helong L., 2014, 2014 23 INT C COMP C, P1, DOI [DOI 10.1145/2670979.2670985, DOI 10.1109/ICCCN.2014.6911807]
   Honda M, 2016, PROCEEDINGS OF THE 15TH ACM WORKSHOP ON HOT TOPICS IN NETWORKS (HOTNETS '16), P183, DOI 10.1145/3005745.3005761
   Hwang D, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P187
   InfiniBand Trade Association, 2009, INFINIBAND ARCH SPEC
   Intel, 2020, INT DAT DIR I O TEC
   Intel, 2019, Intel Optane DC persistent memory
   Islam NusratSharmin., 2016, ICS PAGE, P8, DOI DOI 10.1145/2925426.2926290
   Izraelevitz J., 2019, BASIC PERFORMANCE ME
   Josephson W., 2010, P 8 USENIX C FIL STO
   Kadekodi R, 2019, PROCEEDINGS OF THE TWENTY-SEVENTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '19), P494, DOI 10.1145/3341301.3359631
   Kalia A, 2019, PROCEEDINGS OF THE 16TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P1
   Kalia A, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P185
   Kalia A, 2016, PROCEEDINGS OF USENIX ATC '16: 2016 USENIX ANNUAL TECHNICAL CONFERENCE, P437
   Kalia A, 2014, SIGCOMM'14: PROCEEDINGS OF THE 2014 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P295, DOI [10.1145/2619239.2626299, 10.1145/2740070.2626299]
   Kwon Y, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P460, DOI 10.1145/3132747.3132770
   Lee BC, 2009, CONF PROC INT SYMP C, P2, DOI 10.1145/1555815.1555758
   Lee Changman, 2015, P 8 USENIX C FIL STO
   Lee SK, 2019, PROCEEDINGS OF THE TWENTY-SEVENTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '19), P462, DOI 10.1145/3341301.3359635
   Li SY, 2017, SC'17: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS, DOI 10.1145/3126908.3126928
   Lim H., 2014, management, V15, P36
   Lu Y., 2013, File and Storage Technologies(FAST), P257
   Lu YY, 2015, IEEE S MASS STOR SYS
   Lu YY, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P773
   Lu Youyou, 2014, P USENIX C FIL STOR, P75
   LWN.net, 2014, SUPPORTING FILESYSTE
   LWN.net, 2014, SUPP EXT4 NV DIMMS
   Ma T, 2020, TWENTY-FIFTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXV), P757, DOI 10.1145/3373376.3378511
   Mellanox, 2015, RDMA IMPR ALL TACH R
   Mitchell C, 2016, PROCEEDINGS OF USENIX ATC '16: 2016 USENIX ANNUAL TECHNICAL CONFERENCE, P451
   Mitchell Christopher, 2013, 2013 USENIX ANN TECH
   Narravula S, 2007, CCGRID 2007: SEVENTH IEEE INTERNATIONAL SYMPOSIUM ON CLUSTER COMPUTING AND THE GRID, P583
   NVIDIA, 2013, ACCELIO
   Ou JX, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901324
   Ousterhout J, 2015, ACM T COMPUT SYST, V33, DOI 10.1145/2806887
   Pelley S, 2014, CONF PROC INT SYMP C, P265, DOI 10.1109/ISCA.2014.6853222
   Qureshi MK, 2009, CONF PROC INT SYMP C, P24, DOI 10.1145/1555815.1555760
   SAP HANA, 2016, INM COMP REAL TIM AN
   Shan YZ, 2017, PROCEEDINGS OF THE 2017 SYMPOSIUM ON CLOUD COMPUTING (SOCC '17), P323, DOI 10.1145/3127479.3128610
   Strukov DB, 2008, NATURE, V453, P80, DOI 10.1038/nature06932
   Stuedi P., 2014, ACM S CLOUD COMP NOV, P1, DOI DOI 10.1145/2670979.2670994
   Swanson S, 2013, COMPUTER, V46, P52, DOI 10.1109/MC.2013.222
   Talpey Tom., 2015, REM ACC ULTR LOW LAT
   Tsai SY, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P306, DOI 10.1145/3132747.3132762
   Wang YD, 2015, PROCEEDINGS OF SC15: THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS, DOI 10.1145/2807591.2807614
   Wei XD, 2015, SOSP'15: PROCEEDINGS OF THE TWENTY-FIFTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P87, DOI 10.1145/2815400.2815419
   Xu J, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P478, DOI 10.1145/3132747.3132761
   Xu J, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P323
   Yang J, 2020, PROCEEDINGS OF THE 18TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P169
   Yang J, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P221
   Youyou Lu, 2014, 2014 IEEE 32nd International Conference on Computer Design (ICCD), P216, DOI 10.1109/ICCD.2014.6974684
   Zhang YY, 2015, ACM SIGPLAN NOTICES, V50, P3, DOI [10.1145/2694344.2694370, 10.1109/OECC.2015.7340093]
   Zheng SA, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P207
   Zhou P, 2009, CONF PROC INT SYMP C, P14, DOI 10.1145/1555815.1555759
NR 64
TC 9
Z9 10
U1 1
U2 18
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2021
VL 17
IS 3
AR 19
DI 10.1145/3448418
PG 25
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA YY2GJ
UT WOS:000754611000004
DA 2024-07-18
ER

PT J
AU Bittman, D
   Alvaro, P
   Mehra, P
   Long, DDE
   Miller, EL
AF Bittman, Daniel
   Alvaro, Peter
   Mehra, Pankaj
   Long, Darrell D. E.
   Miller, Ethan L.
TI Twizzler: A <i>Data</i>-<i>centric</i> OS for Non-volatile Memory
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Persistentmemory; non-volatile memory; NVM; PMEM; single-level store;
   global address space; memory hierarchy
AB Byte-addressable, non-volatile memory (NVM) presents an opportunity to rethink the entire system stack. We present Twizzler, an operating system redesign for this near-future. Twizzler removes the kernel from the I/O path, provides programs with memory-style access to persistent data using small (64 bit), object-relative cross-object pointers, and enables simple and efficient long-term sharing of data both between applications and between runs of an application. Twizzler provides a clean-slate programming model for persistent data, realizing the vision of UNIX in a world of persistent RAM.
   We show that Twizzler is simpler, more extensible, and more secure than existing I/O models and implementations by building software for Twizzler and evaluating it on NVM DIMMs. Most persistent pointer operations in Twizzler impose less than 0.5 ns added latency. Twizzler operations are up to 13x faster than UNIX, and SQLite queries are up to 4.2x faster than on PMDK. YCSB workloads ran 1.1-2.9x faster on Twizzler than on native and NVM-optimized SQLite backends.
C1 [Bittman, Daniel; Alvaro, Peter; Miller, Ethan L.] UC Santa Cruz, CSE, 1156 High St, Santa Cruz, CA 95064 USA.
   [Long, Darrell D. E.] UC Santa Cruz, BSOE, 1156 High St, Santa Cruz, CA USA.
   [Miller, Ethan L.] Pure Storage, Mountain View, CA USA.
C3 University of California System; University of California Santa Cruz;
   University of California System; University of California Santa Cruz;
   Pure Storage
RP Bittman, D (corresponding author), UC Santa Cruz, CSE, 1156 High St, Santa Cruz, CA 95064 USA.
EM dbittman@ucsc.edu; palvaro@ucsc.edu; pankaj.mehra@ieee.org;
   darrell@ucsc.edu; elm@ucsc.edu
OI Alvaro, Peter/0000-0001-6672-240X; Mehra, Pankaj/0009-0003-3188-243X
FU National Science Foundation [IIP-1266400, IIP-1841545]; Intel
   Corporation
FX This work was supported in part by the National Science Foundation
   (grants IIP-1266400, IIP-1841545), a grant from Intel Corporation, and
   the industrial members of the UCSC Center for Research in Storage
   Systems.
CR Accetta M., 1986, USENIX Association Summer Conference Proceedings, Atlanta 1986, P93
   Alvaro Peter, 2019, P USENIX WORKSH HOT
   Ames Sasha, 2006, P 23 IEEE 14 NASA GO
   [Anonymous], 1965, Proceedings of the November 30-December 1, 1965, fall joint computer conference, part I, AFIPS '65 Fall, part I
   Bailey Katelin, 2011, P 13 WORKSH HOT TOP
   Balcer Piotr, 2015, INTRO PMEMOBJ 1
   Bensoussan A., 1969, P 2 ACM S OP SYST PR
   Bittau A, 2008, P 5 USENIX S NETW SY, P309, DOI [10.5555/1387589.1387611, DOI 10.5555/1387589.1387611]
   Bittman D, 2019, PLOS'19: PROCEEDINGS OF THE 10TH WORKSHOP ON PROGRAMMING LANGUAGES AND OPERATING SYSTEMS, P30, DOI 10.1145/3365137.3365397
   Caulfield Adrian M., 2010, Proceedings 2010 43rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO 2010), P385, DOI 10.1109/MICRO.2010.33
   CHASE JS, 1994, ACM T COMPUT SYST, V12, P271, DOI 10.1145/195792.195795
   Chen GY, 2017, 50TH ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE (MICRO), P191, DOI 10.1145/3123939.3124543
   Chu H., [No title captured]
   Coburn J, 2011, ACM SIGPLAN NOTICES, V46, P105, DOI [10.1145/1961295.1950380, 10.1145/1961296.1950380]
   Condit J, 2009, SOSP'09: PROCEEDINGS OF THE TWENTY-SECOND ACM SIGOPS SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P133
   Cooper Brian F., 2010, P 1 ACM S CLOUD COMP, P143, DOI [DOI 10.1145/1807128.1807152, 10.1145/1807128.1807152]
   DALEY RC, 1968, COMMUN ACM, V11, P306, DOI 10.1145/363095.363139
   DASGUPTA P, 1991, COMPUTER, V24, P34, DOI 10.1109/2.116849
   DEARLE A, 1994, COMPUT SYST, V7, P289
   Debnath B, 2010, PROC VLDB ENDOW, V3, P1414, DOI 10.14778/1920841.1921015
   Dong Xiangyu, 2014, EMERGING MEMORY TECH, P15
   Dulloor Subramanya R., 2014, P EUR C COMP SYST EU, P1, DOI [10.1145/2592798.2592814, DOI 10.1145/2592798.2592814]
   El Hajj I, 2016, ACM SIGPLAN NOTICES, V51, P353, DOI 10.1145/2954679.2872366
   Engler D. R., 1995, Proceedings Fifth Workshop on Hot Topics in Operating Systems (HotOS-V) (Cat. No.95TH8059), P78, DOI 10.1109/HOTOS.1995.513459
   Engler D. R., 1995, Proceedings Fifth Workshop on Hot Topics in Operating Systems (HotOS-V) (Cat. No.95TH8059), P72, DOI 10.1109/HOTOS.1995.513458
   Engler D. R., 1995, Operating Systems Review, V29, P251, DOI 10.1145/224057.224076
   Faraboschi Paolo, 2015, P 15 WORKSH HOT TOP
   Gifford D. K., 1991, Operating Systems Review, V25, P16, DOI 10.1145/121133.121138
   Gopal B, 1999, USENIX ASSOCIATION PROCEEDINGS OF THE THIRD SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '99), P265
   Guerra Jorge, 2012, P USENIX ANN TECHN C
   Heiser G, 2016, ACM T COMPUT SYST, V34, DOI 10.1145/2893177
   Heiser Gernot, 1993, 9314 U NEW S WALES S
   Hewlett Packard Enterprise, 2018, YCSB C
   Hindman B., 2011, Mesos: a platform for fine-grained resource sharing in the data center, P295
   HOSKING AL, 1993, SIGPLAN NOTICES, V28, P288, DOI 10.1145/167962.165907
   Hu QD, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P703
   Izraelevitz Joseph, 2019, ARXIVORG NO ARXIV190
   JUL E, 1988, ACM T COMPUT SYST, V6, P109, DOI 10.1145/35037.42182
   Jul E., 1991, Proceedings. 1991 International Workshop on Object Orientation in Operating Systems (Cat. No.91TH0392-1), P130, DOI 10.1109/IWOOOS.1991.183037
   Kaashoek M. F., 1997, Operating Systems Review, V31, P52, DOI 10.1145/269005.266644
   Krieger O., 2006, Operating Systems Review, V40, P133, DOI 10.1145/1218063.1217949
   Lee D, 2013, 2013 IEEE 15TH INTERNATIONAL CONFERENCE ON HIGH PERFORMANCE COMPUTING AND COMMUNICATIONS & 2013 IEEE INTERNATIONAL CONFERENCE ON EMBEDDED AND UBIQUITOUS COMPUTING (HPCC_EUC), P2014, DOI 10.1109/HPCC.and.EUC.2013.290
   Litton J, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P49
   Lu YY, 2016, ACM T STORAGE, V12, DOI 10.1145/2851504
   Luo Yixin, 2013, P 5 WORKSH EN EFF DE
   Marathe Virendra J., 2017, P 9 USENIX WORKSH HO
   Mehra Pankaj., P 18 INT PARALLEL DI, DOI DOI 10.1109/IPDPS.2004.1303232
   Narayanan D, 2012, ASPLOS XVII: SEVENTEENTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS, P401
   Ni Yuanjiang, 2019, ARXIV PREPRINT ARXIV
   Ni Yuanjiang, 2018, IEEE 2018 INT C INT
   Ogleari MA, 2018, INT S HIGH PERF COMP, P336, DOI 10.1109/HPCA.2018.00037
   Padioleau Y, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE GENERAL TRACK, P99
   Parker-Wood Aleatha, 2014, P 7 INT SYST STOR C
   Roscoe T., 1994, Operating Systems Review, V28, P48, DOI 10.1145/191525.191537
   Rudoff Andy., 2017, PERSISTENT MEMORY PR
   Shan YZ, 2017, PROCEEDINGS OF THE 2017 SYMPOSIUM ON CLOUD COMPUTING (SOCC '17), P323, DOI 10.1145/3127479.3128610
   Shapiro JS, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE GENERAL TRACK, P59
   Shapiro JS, 1999, OPERATING SYSTEMS REVIEW, VOL 33, NO 5, DECEMBER 1999, P170, DOI 10.1145/319344.319163
   Shekita Eugene, 1990, 956 U WISC
   Sirer Emin Gun, 1995, P 15 ACM S OP SYST P
   Skousen A, 1999, IEEE IPCCC, P8, DOI 10.1109/PCCC.1999.749414
   Tseng HW, 2016, CONF PROC INT SYMP C, P53, DOI 10.1109/ISCA.2016.15
   Volos H, 2011, ACM SIGPLAN NOTICES, V46, P91, DOI [10.1145/1961296.1950379, 10.1145/1961295.1950379]
   Wang TC, 2017, 50TH ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE (MICRO), P800, DOI 10.1145/3123939.3123981
   WULF W, 1974, COMMUN ACM, V17, P337, DOI 10.1145/355616.364017
   Xu J, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P323
   Youyou Lu, 2014, 2014 IEEE 32nd International Conference on Computer Design (ICCD), P216, DOI 10.1109/ICCD.2014.6974684
NR 67
TC 5
Z9 6
U1 1
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUN
PY 2021
VL 17
IS 2
AR 11
DI 10.1145/3454129
PG 31
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SV7JF
UT WOS:000663994900003
DA 2024-07-18
ER

PT J
AU Iliadis, I
   Jelitto, J
   Kim, Y
   Sarafijanovic, S
   Venkatesan, V
AF Iliadis, Ilias
   Jelitto, Jens
   Kim, Yusik
   Sarafijanovic, Slavisa
   Venkatesan, Vinodh
TI ExaPlan: Efficient Queueing-Based Data Placement, Provisioning, and Load
   Balancing for Large Tiered Storage Systems
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Tiered storage; queueing theory; optimization; load balancing; workload
   modeling; Design; Management; Performance
AB Multi-tiered storage, where each tier consists of one type of storage device ( e.g., SSD, HDD, or disk arrays), is a commonly used approach to achieve both high performance and cost efficiency in large-scale systems that need to store data with vastly different access characteristics. By aligning the access characteristics of the data, either fixed-sized extents or variable- sized files, to the characteristics of the storage devices, a higher performance can be achieved for any given cost. This article presents ExaPlan, a method to determine both the data-to-tier assignment and the number of devices in each tier that minimize the system's mean response time for a given budget and workload. In contrast to other methods that constrain or minimize the system load, ExaPlan directly minimizes the system's mean response time estimated by a queueing model. Minimizing the mean response time is typically intractable as the resulting optimization problem is both nonconvex and combinatorial in nature. ExaPlan circumvents this intractability by introducing a parameterized data placement approach that makes it a highly scalable method that can be easily applied to exascalesystems. Through experiments that use parameters from real-world storage systems, such as CERN and LOFAR, it is demonstrated that ExaPlan provides solutions that yield lower mean response times than previous works. It supports standalone SSDs and HDDs as well as disk arrays as storage tiers, and although it uses a static workload representation, we provide empirical evidence that underlying dynamic workloads have invariant properties that can be deemed static for the purpose of provisioning a storage system. ExaPlan is also effective as a load-balancing tool used for placing data across devices within a tier, resulting in an up to 3.6-fold reduction of response time compared with a traditional load-balancing algorithm, such as the Longest Processing Time heuristic.
C1 [Iliadis, Ilias; Jelitto, Jens; Sarafijanovic, Slavisa; Venkatesan, Vinodh] IBM Res Zurich, Saumerstr 4, CH-8803 Ruschlikon, Switzerland.
   [Kim, Yusik] IBM Netherlands, David Ricardostr 2-4, NL-1066 JS Amsterdam, Netherlands.
C3 International Business Machines (IBM)
RP Iliadis, I (corresponding author), IBM Res Zurich, Saumerstr 4, CH-8803 Ruschlikon, Switzerland.
EM ili@zurich.ibm.com; jje@zurich.ibm.com; yusik.kim@nl.ibm.com;
   sla@zurich.ibm.com; ven@zurich.ibm.com
RI Venkatesan, Vinodh/AAX-2134-2021
OI Iliadis, Ilias/0000-0002-3860-5828
FU Dutch ministry of ELI; province of Drenthe, The Netherlands; Dutch
   government [FG 2012-002]
FX This work is conducted in the context of the joint ASTRON and IBM DOME
   project and is funded by the Dutch ministry of EL&I, and by the province
   of Drenthe, The Netherlands. This work is supported by the Dutch
   government, under contract number F&G 2012-002.
CR Alvarez GA, 2001, ACM T COMPUT SYST, V19, P483, DOI 10.1145/502912.502915
   Anderson E, 2005, ACM T COMPUT SYST, V23, P337, DOI 10.1145/1113574.1113575
   Anderson E, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE FAST'02 CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P175
   [Anonymous], 2011, P 9 USENIX C FIL STO
   Balcioglu B, 2008, PERFORM EVALUATION, V65, P653, DOI 10.1016/j.peva.2008.02.003
   Brandt K, 2004, IEEE COMPUTER SOCIETY'S 12TH ANNUAL INTERNATIONAL SYMPOSIUM ON MODELING, ANALYSIS, AND SIMULATION OF COMPUTER AND TELECOMMUNICATIONS SYSTEMS - PROCEEDINGS, P419, DOI 10.1109/MASCOT.2004.1348297
   Cancio G., 2010, CHEP 2010
   Clayton N., 2013, PLANNING EASY TIER I
   Dewdney P.E., 2013, SKA1 System Baseline Design
   Dufrasne B., 2016, IBM DS8000 EASYTIER
   EMC Corporation, 2013, WHIT PAP EMC VNX FAS
   Essary D., 2008, ACM T STORAGE, V4, P1, DOI [10.1145/1353452, DOI 10.1145/1353452.1353454]
   GRAHAM R, 1969, SIAM J APPL MATH, V17, P2
   Hansen M, 1996, IEEE C EVOL COMPUTAT, P312, DOI 10.1109/ICEC.1996.542381
   Iliadis I, 2015, I S MOD ANAL SIM COM, P218, DOI 10.1109/MASCOTS.2015.41
   Iyengar A. K., 1999, World Wide Web, V2, P85, DOI 10.1023/A:1019244621570
   Kim Y., IEEE INT WORKSHOP MO
   Kingman J. F. C., 1961, Mathematical Proceedings of the Cambridge Philosophical Society, V57, P902
   Lin Lin, 2011, Proceedings of the 2011 IEEE 19th International Symposium on Modelling, Analysis & Simulation of Computer and Telecommunication Systems (MASCOTS 2011), P318, DOI 10.1109/MASCOTS.2011.41
   Narayanan D, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P253
   Shi Haixiang., 2012, APMRC, 2012 Digest, P1
   Strunk JD, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P313
   van Haarlem MP, 2013, ASTRON ASTROPHYS, V556, DOI 10.1051/0004-6361/201220873
   WOLF J, 1989, PERF E R SI, V17, P1, DOI 10.1145/75372.75373
NR 24
TC 7
Z9 9
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUN
PY 2017
VL 13
IS 2
SI SI
AR 17
DI 10.1145/3078839
PG 41
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FA1TA
UT WOS:000405221600008
DA 2024-07-18
ER

PT J
AU Li, N
   Jiang, H
   Feng, D
   Shi, Z
AF Li, Ning
   Jiang, Hong
   Feng, Dan
   Shi, Zhan
TI Customizable SLO and Its Near-Precise Enforcement for Storage Bandwidth
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Algorithms; Design; Management; Performance; Cloud storage; storage
   management; service-level objective; end-to-end control
AB Cloud service is being adopted as a utility for large numbers of tenants by renting Virtual Machines (VMs). But for cloud storage, unpredictable IO characteristics make accurate Service-Level-Objective (SLO) enforcement challenging. As a result, it has been very difficult to support simple-to-use and technology-agnostic SLO specifying a particular value for a specific metric (e.g., storage bandwidth). This is because the quality of SLO enforcement depends on performance error and fluctuation that measure the precision of SLO enforcement. High precision of SLO enforcement is critical for user-oriented performance customization and user experiences. To address this challenge, this article presents V-Cup, a framework for VM-oriented customizable SLO and its near-precise enforcement. It consists of multiple auto-tuners, each of which exports an interface for a tenant to customize the desired storage bandwidth for a VM and enable the storage bandwidth of the VM to converge on the target value with a predictable precision. We design and implement V-Cup in the Xen hypervisor based on the fair sharing scheduler for VM-level resource management. Our V-Cup prototype evaluation shows that it achieves satisfying performance guarantees through near-precise SLO enforcement.
C1 [Li, Ning; Feng, Dan; Shi, Zhan] Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan, Peoples R China.
   [Jiang, Hong] Univ Texas Arlington, Dept Comp Sci & Engn, Arlington, TX 76019 USA.
   [Li, Ning; Feng, Dan; Shi, Zhan] Huazhong Univ Sci & Technol, Wuhan Natl Lab Optoelect F307, 1037 Luoyu Rd, Wuhan 430074, Peoples R China.
   [Jiang, Hong] Engn Res Bldg,Room 640,Box 19015, Arlington, TX 76010 USA.
C3 Huazhong University of Science & Technology; University of Texas System;
   University of Texas Arlington; Huazhong University of Science &
   Technology
RP Shi, Z (corresponding author), Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan, Peoples R China.
EM leebellwind@hust.edu.cn; hong.jiang@uta.edu; dfeng@hust.edu.cn;
   zshi@hust.edu.cn
FU National High Technology Research and Development Program of China
   [2013AA013203, 2015AA015301, 2015AA016701]; NSFC [61303046, 61472153,
   61402189]; US NSF [CNS-1116606, CNS-1016609]
FX This work is supported by the National High Technology Research and
   Development Program of China Grants No. 2013AA013203, No. 2015AA015301,
   and No. 2015AA016701; NSFC Grants No. 61303046, No. 61472153, and No.
   61402189; and US NSF Grants No. CNS-1116606 and No. CNS-1016609.
CR Axboe J., 2004, P OTT LIN S
   Barham P., 2003, P ACM S OP SYST PRIN
   Bennett J. C. R., 1996, P IEEE C COMP COMM I
   Bradford R., 2007, P INT C VIRT EX ENV
   Chambliss D. D., 2003, P 22 INT S REL DISTR
   Cooper B. F., 2010, P ACM S CLOUD COMP S
   Dan A., 1996, P MULT COMP NETW C
   Duda K. J., 1999, P ACM S OP SYST PRIN
   Franklin Gene F., 1998, DIGITAL CONTROL DYNA, VThird
   FRASER K, 2004, P 1 WORKSH OP SYST A
   Golestani S., 1994, P IEEE C COMP COMM I
   Goyal P, 1997, IEEE ACM T NETWORK, V5, P690, DOI 10.1109/90.649569
   Gulati A., 2012, P USENIX ANN TECHN C
   Gulati A., 2009, P  FIL STOR TECHN FA
   Gulati A., 2010, P S OP SYST DES IMPL
   Hellerstein Joseph L, 2004, Feedback control of computing systems
   Kang J., 2014, P C FIL STOR TECHN F
   Karlsson M., 2005, ACM T STORAGE, V1, P457, DOI DOI 10.1145/1111609.1111612
   Kim J., 2015, P C FIL STOR TECHN F
   Kivity A., 2007, P 2007 LIN S
   Liu J., 2008, EURASIP J ADV SIG PR, V2008, P1, DOI DOI 10.1074/JBC.M802695200
   Lu Z., 2012, P IEEE 18 INT C PAR
   Lumb C. R., 2003, P C FIL STOR TECHN F
   Mansley K., 2007, P WORKSH VIRT XEN HI, P224
   Mashtizadeh A., 2011, P USENIX ANN TECHN C
   Mcdougall R., 2015, PROTOTYPE MODEL BASE
   Nathuji R., 2010, P 3 EUR C COMP SYST
   Nieh J, 2003, ACM T COMPUT SYST, V21, P117, DOI 10.1145/762483.762484
   Povzner A., 2008, P 3 EUR C COMP SYST
   Russell Rusty, 2008, Operating Systems Review, V42, P95, DOI 10.1145/1400097.1400108
   Ryu M., 2013, Proc. of ACM Int. Conf. on Multimedia, P313
   Shreedhar M., 1987, IEEE ACM T NETWORK, V4, P375
   Shue D., 2012, P S OP SYST DES IMPL
   Soltesz S., 2007, P 3 EUR C COMP SYST
   Sugerman J., 2001, P USENIX ANN TECHN C
   Suri S., 1997, P IEEE C COMP COMM I
   Thereska E., 2013, P ACM S OP SYST PRIN
   VMware Infrastructure, 2016, INTR VMWARE INFR
   Wachs M., 2007, P C FIL STOR TECHN F
   Wang A., 2012, P ACM S CLOUD COMP S
   Wei Jin, 2004, Performance Evaluation Review, V32, P37, DOI 10.1145/1012888.1005694
   Wu J. C., 2006, P IEEE C MASS STOR S
   Wu S., 2009, P C FIL STOR TECHN F
   Wu S., 2012, P USENIX LARG INST S
   Wu Y., 2011, P IEEE INT C DISTR C
   ZHANG J, 2006, ACM T STORAGE, V2, P283, DOI DOI 10.1145/1168910.1168913
NR 46
TC 3
Z9 3
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAR
PY 2017
VL 13
IS 1
SI SI
AR 6
DI 10.1145/2998454
PG 25
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ER3WE
UT WOS:000398729600006
DA 2024-07-18
ER

PT J
AU Zheng, SA
   Hoseinzadeh, M
   Swanson, S
   Huang, LP
AF Zheng, Shengan
   Hoseinzadeh, Morteza
   Swanson, Steven
   Huang, Linpeng
TI TPFS: A High-Performance Tiered File System for Persistent Memories and
   Disks
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE File system; persistent memory; data migration
ID PHASE-CHANGE MEMORY
AB Emerging fast, byte-addressable persistent memory (PM) promises substantial storage performance gains compared with traditional disks. We present TPFS, a tiered file system that combines PM and slow disks to create a storage system with near-PM performance and large capacity. TPFS steers incoming file input/output (I/O) to PM, dynamic random access memory (DRAM), or disk depending on the synchronicity, write size, and read frequency. TPFS profiles the application's access stream online to predict the behavior of file access. In the background, TPFS estimates the "temperature" of file data and migrates the write-cold and read-hot file data from PM to disks. To fully utilize disk bandwidth, TPFS coalesces data blocks into large, sequential writes. Experimental results show that with a small amount of PM and a large solid-state drive (SSD), TPFS achieves up to 7.3x and 7.9x throughput improvement compared with EXT4 and XFS running on an SSD alone, respectively. As the amount of PM grows, TPFS's performance improves until it matches the performance of a PM-only file system.
C1 [Zheng, Shengan; Huang, Linpeng] Shanghai Jiao Tong Univ, 800 Dongchuan Rd, Shanghai, Peoples R China.
   [Hoseinzadeh, Morteza; Swanson, Steven] Univ Calif San Diego, 9500 Gilman Dr, San Diego, CA 92103 USA.
C3 Shanghai Jiao Tong University; University of California System;
   University of California San Diego
RP Zheng, SA (corresponding author), Shanghai Jiao Tong Univ, 800 Dongchuan Rd, Shanghai, Peoples R China.
EM shengan@sjtu.edu.cn; mhoseinzadeh@cs.ucsd.edu; swanson@cs.ucsd.edu;
   lphuang@sjtu.edu.cn
RI Zheng, Shengan/ABB-2425-2020
OI Zheng, Shengan/0000-0003-2485-760X; Swanson, Steven/0000-0002-5896-1037
FU Natural Science Foundation of Shanghai [22ZR1435400]; Shanghai Municipal
   Science and Technology Major Project [2021SHZDZX0102]; CCF Huawei
   Populus Grove Fund [CCF-HuaweiSY202202]
FX This work was supported by the Natural Science Foundation of Shanghai
   (grant no. 22ZR1435400) and the Shanghai Municipal Science and
   Technology Major Project (grant no. 2021SHZDZX0102). CCF Huawei Populus
   Grove Fund (grant no. CCF-HuaweiSY202202).
CR Agarwal N, 2017, TWENTY-SECOND INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXII), P631, DOI 10.1145/3037697.3037706
   Arulraj J, 2015, SIGMOD'15: PROCEEDINGS OF THE 2015 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P707, DOI 10.1145/2723372.2749441
   Axboe Jens, 2012, FIO FLEXIBLE IO TEST
   Cano I, 2017, PROCEEDINGS OF NSDI '17: 14TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P51
   Chen E, 2010, IEEE T MAGN, V46, P1873, DOI 10.1109/TMAG.2010.2042041
   Chen H, 2021, PROCEEDINGS OF THE 19TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '21), P17
   Chen YM, 2021, PROCEEDINGS OF THE 19TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '21), P81
   Chinner D., 2015, XFS DAX SUPPORT
   Condit J, 2009, SOSP'09: PROCEEDINGS OF THE TWENTY-SECOND ACM SIGOPS SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P133
   CXL Consortium, 2022, Compute Express Link: The Breakthrough CPU-to-Device Interconnect
   Debnath B, 2010, PROC VLDB ENDOW, V3, P1414, DOI 10.14778/1920841.1921015
   Ding ZF, 2010, INT CONF COMP SCI, P148, DOI 10.1109/ICCSIT.2010.5563643
   Dong MK, 2019, PROCEEDINGS OF THE TWENTY-SEVENTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '19), P478, DOI 10.1145/3341301.3359637
   Dong MK, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P719
   Dulloor SR, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901344
   Dulloor Subramanya R., 2014, P 9 EUR C COMP SYST
   Dulong R, 2021, I C DEPEND SYS NETWO, P186, DOI 10.1109/DSN48987.2021.00033
   Facebook, 2012, ROCKSDB
   Fang R, 2011, PROC INT CONF DATA, P1221, DOI 10.1109/ICDE.2011.5767918
   Google, 2011, LevelDB
   HITZ D, 1994, PROCEEDINGS OF THE WINTER 1994 USENIX CONFERENCE, P235
   Intel, 2018, INT OPT TECHN
   Intel, 2020, INT OPT DC PERS
   Izraelevitz Joseph, 2019, arXiv, DOI [10.48550/ARXIV.1903.05714, DOI 10.48550/ARXIV.1903.05714]
   Kaczmarski M, 2003, IBM SYST J, V42, P322, DOI 10.1147/sj.422.0322
   Kadekodi R, 2019, PROCEEDINGS OF THE TWENTY-SEVENTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '19), P494, DOI 10.1145/3341301.3359631
   Kannan S, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P241
   Kawahara T, 2011, IEEE DES TEST COMPUT, V28, P52, DOI 10.1109/MDT.2010.97
   Krish KR, 2014, IEEE ACM INT SYMP, P502, DOI 10.1109/CCGrid.2014.51
   Kwon Y, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P460, DOI 10.1145/3132747.3132770
   Lee BC, 2009, CONF PROC INT SYMP C, P2, DOI 10.1145/1555815.1555758
   Li C., 2014, USENIX ANN TECH C AT, P501
   Micron, 2017, BATT BACK NVDIMMS
   Narayanan D, 2012, ASPLOS XVII: SEVENTEENTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS, P401
   Qureshi MK, 2009, CONF PROC INT SYMP C, P24, DOI 10.1145/1555815.1555760
   Tadakamadla R, 2019, PROCEEDINGS OF THE 2019 ACM/SPEC INTERNATIONAL CONFERENCE ON PERFORMANCE ENGINEERING (ICPE '19), P255, DOI 10.1145/3297663.3309669
   Tarasov Vasily, 2016, USENIX; login, V41, P6
   Wang ZX, 2020, 2020 53RD ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE (MICRO 2020), P496, DOI 10.1109/MICRO50266.2020.00049
   Wilcox M., 2017, Add support for nv-dimms to ext4
   Wilcox Matthew., 2014, Add support for NV-DIMMs to ext4
   Wu K, 2021, PROCEEDINGS OF THE 19TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '21), P307
   Wu Xiaojian., 2011, Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis (SC '11), p39:1
   Xu C, 2011, DES AUT TEST EUROPE, P734
   Xu J, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P478, DOI 10.1145/3132747.3132761
   Xu J, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P323
   Yang JJS, 2013, NAT NANOTECHNOL, V8, P13, DOI [10.1038/nnano.2012.240, 10.1038/NNANO.2012.240]
   Yang J, 2020, PROCEEDINGS OF THE 18TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P169
   Yang JH, 2017, 2017 31ST IEEE INTERNATIONAL CONFERENCE ON ADVANCED INFORMATION NETWORKING AND APPLICATIONS WORKSHOPS (IEEE WAINA 2017), P1, DOI [10.1109/ULTSYM.2017.8092547, 10.1109/WAINA.2017.29]
   Zhang YY, 2015, ACM SIGPLAN NOTICES, V50, P3, DOI [10.1145/2694344.2694370, 10.1109/OECC.2015.7340093]
NR 49
TC 0
Z9 0
U1 1
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAY
PY 2023
VL 19
IS 2
AR 20
DI 10.1145/3580280
PG 28
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D7OU8
UT WOS:000970592600011
DA 2024-07-18
ER

PT J
AU Yang, B
   Xue, W
   Zhang, TY
   Liu, SC
   Ma, XS
   Wang, XY
   Liu, WG
AF Yang, Bin
   Xue, Wei
   Zhang, Tianyu
   Liu, Shichao
   Ma, Xiaosong
   Wang, Xiyang
   Liu, Weiguo
TI End-to-end I/O Monitoring on Leading Supercomputers
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE I/O monitoring; anomaly detection; I/O diagnosis; bottleneck
   optimization
ID HPC
AB This paper offers a solution to overcome the complexities of production system I/O performance monitoring. We present Beacon, an end-to-end I/O resource monitoring and diagnosis system for the 40960-node Sunway TaihuLight supercomputer, currently the fourth-ranked supercomputer in the world. Beacon simultaneously collects and correlates I/O tracing/profiling data from all the compute nodes, forwarding nodes, storage nodes, and metadata servers. With mechanisms such as aggressive online and offline trace compression and distributed caching/storage, it delivers scalable, low-overhead, and sustainable I/O diagnosis under production use. With Beacon's deployment on TaihuLight for more than three years, we demonstrate Beacon's effectiveness with real-world use cases for I/O performance issue identification and diagnosis. It has already successfully helped center administrators identify obscure design or configuration flaws, system anomaly occurrences, I/O performance interference, and resource under- or over-provisioning problems. Several of the exposed problems have already been fixed, with others being currently addressed. Encouraged by Beacon's success in I/O monitoring, we extend it to monitor interconnection networks, which is another contention point on supercomputers. In addition, we demonstrate Beacon's generality by extending it to other supercomputers. Both Beacon codes and part of collected monitoring data are released.(1)
C1 [Yang, Bin] Shandong Univ, Natl Supercomp Ctr Wuxi, Jinan, Peoples R China.
   [Xue, Wei] Tsinghua Univ, Beijing, Peoples R China.
   [Zhang, Tianyu; Liu, Shichao; Liu, Weiguo] Shandong Univ, Jinan, Peoples R China.
   [Ma, Xiaosong] HBKU, Qatar Comp Res Inst, Doha, Qatar.
   [Wang, Xiyang] Natl Supercomp Ctr Wuxi, Wuxi, Jiangsu, Peoples R China.
C3 Shandong University; Tsinghua University; Shandong University; Qatar
   Foundation (QF); Hamad Bin Khalifa University-Qatar; Qatar Computing
   Research Institute
RP Xue, W (corresponding author), Tsinghua Univ, Beijing, Peoples R China.
EM bin.yang@mail.sdu.edu.cn; xuewei@mail.tsinghua.edu.cn;
   15589952820@163.com; 1048163359@qq.com; xma@qf.org.qa;
   wangxysoftjn@163.com; weiguo.liu@sdu.edu.cn
RI Liu, Weiguo/R-9431-2017; Yang, Bin/GZL-4391-2022
OI Yang, Bin/0000-0002-3783-2228; XUE, WEI/0000-0001-9740-6581; Ma,
   Xiaosong/0000-0003-1261-2496
FU National Key R&D Program of China [2017YFA0604500]; National Natural
   Science Foundation of China [41776010]
FX This work is partially supported by the National Key R&D Program of
   China (Grant No. 2017YFA0604500) and the National Natural Science
   Foundation of China (Grant No. 41776010).
CR Agelastos A, 2014, INT CONF HIGH PERFOR, P154, DOI 10.1109/SC.2014.18
   Ali N, 2009, 2009 IEEE INTERNATIONAL CONFERENCE ON CLUSTER COMPUTING AND WORKSHOPS, P86
   Alliance OpenFabrics, 2010, OpenFabrics enterprise distribution (OFED)
   [Anonymous], 2010, WORKSHOP MANY TASK C
   [Anonymous], 2008, LANL MPI IO TEST
   [Anonymous], 2015, INT S HIGH PERFORMAN
   Bent John., 2009, High Performance Computing Networking, Storage and Analysis, Proceedings of the Conference on, P1
   Braam PeterJ., 2002, LUSTRE SCALABLE HIGH
   Brandt J, 2009, INT PARALL DISTRIB P, P3050
   Carlyle Adam G., 2012, CRAY USER GROUP C, P1
   Carns P., 2012, Technical Report
   Cray, 2017, CRAY BURST BUFF COR
   Dandapanthula N., 2011, Proc. European Conference on Parallel Processing (Euro-Par), P166
   Di S, 2017, IEEE ACM INT SYMP, P442, DOI 10.1109/CCGRID.2017.18
   Dongarra Jack, 2020, TOP 500 LIST
   Donvito Giacinto, 2014, J PHYS C SERIES
   Dorier M, 2014, INT PARALL DISTRIB P, DOI 10.1109/IPDPS.2014.27
   Dubois P., 2013, MYSQL, V5th
   Dudhia, 2005, DESCRIPTION ADV RES, P88, DOI DOI 10.5065/D6DZ069T
   EGGERT PR, 1993, PROCEEDINGS OF THE WINTER 1993 USENIX CONFERENCE, P229
   Ester M., 1996, KDD 96, P226, DOI DOI 10.5555/3001460.3001507
   Forcier J, 2008, Python web development with Django
   Fu H., 2017, SC 17, P1
   Fu HH, 2017, SC'17: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS, DOI 10.1145/3126908.3126909
   Fu HH, 2016, SCI CHINA INFORM SCI, V59, DOI 10.1007/s11432-016-5588-7
   Gainaru A, 2015, INT PARALL DISTRIB P, P1013, DOI 10.1109/IPDPS.2015.116
   Garlick Jim, 2010, Lustre Monitoring Tool (LMT)
   Giltrap DL, 2010, AGR ECOSYST ENVIRON, V136, P292, DOI 10.1016/j.agee.2009.06.014
   Gunasekaran Raghul., 2015, Proceedings of the 10th Parallel Data Storage Workshop, P31, DOI DOI 10.1145/2834976.2834985
   Gunawi HS, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P1
   Heng Lin, 2018, SC18: International Conference for High Performance Computing, Networking, Storage and Analysis. Proceedings, P706, DOI 10.1109/SC.2018.00059
   IBM, 2008, INTR
   Ji X, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P265
   Jokanovic A, 2015, INT PARALL DISTRIB P, P449, DOI 10.1109/IPDPS.2015.87
   Kahle JA, 2019, ISSCC DIG TECH PAP I, V62, P42, DOI 10.1109/ISSCC.2019.8662426
   Kim SJ, 2012, 2012 SC COMPANION: HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS (SCC), P18, DOI 10.1109/SC.Companion.2012.14
   Kim SJ, 2010, LECT NOTES COMPUT SC, V6305, P72
   Kuc R., 2013, ELASTICSEARCH SERVER
   Kuo CS, 2014, IEEE INT C CL COMP, P185, DOI 10.1109/CLUSTER.2014.6968743
   Lawrence Berkeley and ANL, 2017, TOKIO TOT KNOWL I O
   Lim SH, 2017, SC'17: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS, DOI 10.1145/3126908.3126924
   Liu Y, 2016, SC '16: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS, P819, DOI 10.1109/SC.2016.69
   Liu Yang., 2014, P 12 USENIX C FILE S, P213
   Liu ZX, 2016, PROCEEDINGS OF THE 2016 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '16), P101, DOI 10.1145/2934872.2934906
   Lockwood GK, 2017, PROCEEDINGS OF PDSW-DISCS 2017: 2ND JOINT INTERNATIONAL WORKSHOP ON PARALLEL DATA STORAGE & DATA INTENSIVE SCALABLE COMPUTING SYSTEMS, P55, DOI 10.1145/3149393.3149395
   Lofstead J., 2010, Proceedings of the 2010 ACM/IEEE International Conference for High Performance Computing, Networking, Storage and Analysis, SC '10, DOI [DOI 10.1109/SC.2010.32, 10.1109/SC.2010.32.]
   Lofstead J, 2016, SC '16: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS, P585, DOI 10.1109/SC.2016.49
   Luu H, 2013, IEEE INT C CL COMP
   Mueller Frank, INT WORKSHOP APPL PA, P410
   Naas Mohammed Islam, 2021, P WORKSHOP CHALLENGE, P1
   Nathan V., 2017, P SIGCOMM POSTERS DE, P57
   NERSC, 2014, ED
   Neuwirth S, 2016, INT SYM COMP ARCHIT, P9, DOI 10.1109/SBAC-PAD.2016.10
   Newman L. H., 2014, PIZ DAINT SUPERCOMPU
   Noeth M, 2009, J PARALLEL DISTR COM, V69, P696, DOI 10.1016/j.jpdc.2008.09.001
   Oral S, 2014, INT CONF HIGH PERFOR, P217, DOI 10.1109/SC.2014.23
   Ouyang J., 2015, P 24 H INT S HIGH PE, P149, DOI 10.1145/2749246.2749273
   PAPKA M., 2013, Mira: Argonne's 10-petaflops supercomputer
   Patel T, 2019, PROCEEDINGS OF SC19: THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS, DOI 10.1145/3295500.3356183
   Patel T, 2020, PROCEEDINGS OF THE 18TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P91
   Paul AK, 2020, INT C HIGH PERFORM, P202, DOI 10.1109/HiPC50609.2020.00034
   Paul AK, 2017, IEEE INT CONF BIG DA, P233, DOI 10.1109/BigData.2017.8257931
   Qiao Zhenbo, 2020, INT C HIGH PERFORMAN, P1
   Rajesh Neeraj, 2021, P 30 INT S HIGH PERF, P147, DOI [10.1145/3431379.3460640, DOI 10.1145/3431379.3460640]
   Redis W. G., 2016, REDIS
   Schmuck Frank, 2002, C FILE STORAGE TECHN, P1
   Sergent N, 2001, 2001 PACIFIC RIM INTERNATIONAL SYMPOSIUM ON DEPENDABLE COMPUTING, PROCEEDINGS, P137, DOI 10.1109/PRDC.2001.992690
   Shen Shan-Hsiang., 2012, IWQOS, P1
   Snyder S, 2016, PROCEEDINGS OF ESPT 2016: 5TH WORKSHOP ON EXTREME-SCALE PROGRAMMING TOOLS, P9, DOI 10.1109/ESPT.2016.006
   Song Huaiming., 2011, HIGH PERFORMANCE COM, P1, DOI DOI 10.1145/2063384.2063407
   Sugon, 2018, PAI SYSTEM SUGON
   Sungon, 2015, PARASTOR200 DISTR PA
   Tai AT, 2004, 2004 INTERNATIONAL CONFERENCE ON DEPENDABLE SYSTEMS AND NETWORKS, PROCEEDINGS, P805
   Tammana P, 2018, PROCEEDINGS OF THE 15TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION (NSDI'18), P453
   Tammana P, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P233
   TARASOV V, 2012, C FILE STORAGE TECHN, P22
   Turnbull J., 2013, The Logstash Book
   Uselton A., 2010, P 24 IEEE INT PARALL, P1, DOI DOI 10.1109/IPDPS.2010.5470424
   Vazhkudai SS, 2017, SC'17: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS, DOI 10.1145/3126908.3126946
   Vijayakumar K., 2009, Proceedings of the 4th Annual Workshop on Petascale Data Storage, P26
   Vishwanath A, 2010, INT SYMP ADV NETW, P16, DOI 10.1109/ANTS.2010.5983514
   Vranas P., 2012, BLUEGENE Q SEQUOIA E
   Wang YL, 2017, COMPUT PHYS COMMUN, V220, P212, DOI 10.1016/j.cpc.2017.07.009
   Wiedemann MC, 2013, COMPUT SCI-RES DEV, V28, P241, DOI 10.1007/s00450-012-0221-5
   Wright SA, 2013, COMPUT J, V56, P141, DOI 10.1093/comjnl/bxs044
   Wu X., 2013, ACM INT C SUP, P59
   Xiao J., 2021, 2021 INT C MICROWAVE, P1, DOI DOI 10.1109/ICMMT52847.2021.9618562
   Xiaohui Duan, 2018, SC18: International Conference for High Performance Computing, Networking, Storage and Analysis. Proceedings, P148, DOI 10.1109/SC.2018.00015
   Xie B, 2012, INT CONF HIGH PERFOR
   Xing Wu, 2011, 2011 International Conference on Parallel Processing, P196, DOI 10.1109/ICPP.2011.50
   Xu Cong, 2016, CRAY USER GROUP M, P1
   Yildiz O, 2016, INT PARALL DISTRIB P, P750, DOI 10.1109/IPDPS.2016.50
   You E., 2020, VUEJS FRAMEWORK
   Yu Minlan., 2011, NSDI, NSDI'11, P5
   Yu WK, 2008, 2008 IEEE INTERNATIONAL SYMPOSIUM ON PARALLEL & DISTRIBUTED PROCESSING, VOLS 1-8, P1468
   Zheng F, 2013, INT CONF HIGH PERFOR, DOI 10.1145/2503210.2503279
NR 96
TC 3
Z9 3
U1 2
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD FEB
PY 2023
VL 19
IS 1
AR 3
DI 10.1145/3568425
PG 35
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 9I5DE
UT WOS:000939530200003
OA Bronze
DA 2024-07-18
ER

PT J
AU Kisous, R
   Kolikant, A
   Duggal, A
   Sheinvald, S
   Yadgar, G
AF Kisous, Roei
   Kolikant, Ariel
   Duggal, Abhinav
   Sheinvald, Sarai
   Yadgar, Gala
TI The <i>what</i>, The <i>from</i>, and The <i>to</i>: The Migration Games
   in Deduplicated Systems
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Deduplication; data migration; capacity planning
AB Deduplication reduces the size of the data stored in large-scale storage systems by replacing duplicate data blocks with references to their unique copies. This creates dependencies between files that contain similar content and complicates the management of data in the system.
   In this article, we address the problem of data migration, in which files are remapped between different volumes as a result of system expansion or maintenance. The challenge of determining which files and blocks to migrate has been studied extensively for systems without deduplication. In the context of deduplicated storage, however, only simplified migration scenarios have been considered. In this article, we formulate the general migration problem for deduplicated systems as an optimization problem whose objective is to minimize the system's size while ensuring that the storage load is evenly distributed between the system's volumes and that the network traffic required for the migration does not exceed its allocation.
   We then present three algorithms for generating effective migration plans, each based on a different approach and representing a different trade-off between computation time and migration efficiency. Our greedy algorithm provides modest space savings but is appealing thanks to its exceptionally short runtime. Its results can be improved by using larger system representations. Our theoretically optimal algorithm formulates the migration problem as an integer linear programming (ILP) instance. Its migration plans consistently result in smaller and more balanced systems than those of the greedy approach, although its runtime is long and, as a result, the theoretical optimum is not always found. Our clustering algorithm enjoys the best of both worlds: its migration plans are comparable to those generated by the ILP-based algorithm, but its runtime is shorter, sometimes by an order of magnitude. It can be further accelerated at a modest cost in the quality of its results.
C1 [Kisous, Roei; Kolikant, Ariel; Yadgar, Gala] Technion, Comp Sci Dept, IL-3200003 Haifa, Israel.
   [Duggal, Abhinav] Dell EMC, Hopkinton, MA USA.
   [Sheinvald, Sarai] ORT Braude Coll Engn, IL-2161002 Carmiel, Israel.
C3 Technion Israel Institute of Technology; Dell Incorporated; Dell EMC;
   Braude Academic College of Engineering
RP Kisous, R (corresponding author), Technion, Comp Sci Dept, IL-3200003 Haifa, Israel.
EM kisous.roei@campus.technion.ac.il; sarielko@cs.technion.ac.il;
   abhinav.duggal@dell.com; sarai.sheinvald@gmail.com;
   gala@cs.technion.ac.il
OI Kisous, Roei/0000-0002-1311-4678; Sheinvald, Sarai/0000-0002-0524-7390;
   Duggal, Abhinav/0000-0002-8223-5828; Yadgar, Gala/0000-0003-2701-0260
FU Israel Science Foundation [807/20]
FX This research was supported by the Israel Science Foundation (grant no.
   807/20).
CR Aggarwal B., 2010, P USENIX NSDI, P1
   Allu Y, 2018, PROCEEDINGS OF THE 2018 USENIX ANNUAL TECHNICAL CONFERENCE, P705
   Anderson E, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE FAST'02 CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P175
   Anderson E., 2001, Algorithm Engineering. 5th International Workshop, WAE 2001. Proceedings (Lecture Notes in Computer Science Vol.2141), P145
   [Anonymous], INTRO LP SOLVE 5525
   [Anonymous], SYMPHONY DEV HOME PA
   [Anonymous], 2022, DELL POWERSTORE T ST
   [Anonymous], CLUSTER ANAL
   [Anonymous], 2013, PROC 5 USENIX WORKSH
   [Anonymous], 2011, 2011 USENIX C USENIX
   [Anonymous], What Is Data Deduplication? | Benefits Use Cases | NetApp
   [Anonymous], FASTEST MATH PROGRAM
   [Anonymous], SOURCE CODE MIGRATIO
   [Anonymous], TRACES SNAPSHOTS PUB
   Balasubramanian B, 2014, IEEE INFOCOM SER, P592, DOI 10.1109/INFOCOM.2014.6847984
   Bhagwat D, 2009, 2009 IEEE INTERNATIONAL SYMPOSIUM ON MODELING, ANALYSIS & SIMULATION OF COMPUTER AND TELECOMMUNICATION SYSTEMS (MASCOTS), P237
   Cao ZC, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P309
   Chenier F., 2011, Proc of the 12th International Conference on Rehabilitation Robotics (ICORR), P1
   Clements Austin., 2009, ATC'09: 2009 USENIX Annual Technical Conference, P101
   CPLEX Optimizer, US
   Debnath Biplob., 2010, P USENIX ANN TECHNIC, P16
   Dong Wei., 2011, FAST, P15
   Douglis F, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P29
   Dubnicki C., 2009, P 7 USENIX C FIL STO
   Duggal A, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P647
   Fu Min, 2014, 2014 USENIX C USENIX, P181
   Gabor R, 2007, ACM T ARCHIT CODE OP, V4, DOI 10.1145/1275937.1275939
   GNU Linear Programming Kit (GLPK), About us
   Greenacre M., 2013, Multivariate analysis of ecological data
   Guo F, 2011, P 2011 C USENIX ANN, P1
   Harnik D, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P107
   Huang C., 2012, 2012 1 INT C AGRO GE, P1
   Kaczmarczyk Michal., 2012, Proceedings of the 5th Annual International Systems and Storage Conference, P1
   Kisous R, 2022, PROCEEDINGS OF THE 20TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, FAST 2022, P265
   Li C., 2014, USENIX ANN TECH C AT, P501
   Lillibridge M., 2009, P 7 USENIX C FIL STO
   Lillibridge Mark, 2013, P 11 USENIX C FIL ST
   Lin Xing, 2014, 12 USENIX C FILE STO, P265
   Linux Kernel Archives, PUB LINUX KERNEL
   Lu CY, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE FAST'02 CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P219
   MANBER U, 1994, PROCEEDINGS OF THE WINTER 1994 USENIX CONFERENCE, P1
   Matsuzawa K, 2018, SYSTOR'18: PROCEEDINGS OF THE 11TH ACM INTERNATIONAL SYSTEMS AND STORAGE CONFERENCE, P65, DOI 10.1145/3211890.3211894
   Meyer D.T., 2011, Proceedings of the 9th USENIX conference on File and stroage technologies, P1
   Muthitacharoen A., 2001, Operating Systems Review, V35, P174, DOI 10.1145/502059.502052
   Nachman A, 2021, ACM T STORAGE, V17, DOI 10.1145/3453301
   Nachman A, 2020, PROCEEDINGS OF THE 18TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P193
   Nagesh P. C., 2013, 6 INT SYSTEMS STORAG, P1
   Nightingale EdmundB., 2012, P 10 USENIX C OPERAT, P1
   Shilane Philip., 2016, 8 USENIX WORKSH HOT
   SNIA IOTTA Repository, TRACE TYPES 6
   Srinivasan K., 2012, 10 USENIX C FILE STO, V12, P1
   Sun Z, 2016, IEEE S MASS STOR SYS
   Tarasov V., 2012, 2012 F USENIX G ANN, P261
   Weil S.A., 2006, P INT C SUP COM SC N, P1
   Xia W, 2016, PROCEEDINGS OF USENIX ATC '16: 2016 USENIX ANNUAL TECHNICAL CONFERENCE, P101
   Xia W, 2014, PERFORM EVALUATION, V79, P258, DOI 10.1016/j.peva.2014.07.016
   Yan ZP, 2016, OCEANS 2016 MTS/IEEE MONTEREY, DOI 10.1109/OCEANS.2016.7761044
   Zhu B, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P269
   Zitzler E, 2008, LECT NOTES COMPUT SC, V5252, P373, DOI 10.1007/978-3-540-88908-3_14
NR 59
TC 1
Z9 1
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2022
VL 18
IS 4
AR 31
DI 10.1145/3565025
PG 29
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7F8OU
UT WOS:000902100700004
OA Bronze
DA 2024-07-18
ER

PT J
AU Matsuzawa, K
   Hayasaka, M
   Shinagawa, T
AF Matsuzawa, Keiichi
   Hayasaka, Mitsuo
   Shinagawa, Takahiro
TI Practical Quick File Server Migration
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE File server; migration; postcopy
ID NAS
AB Regular file server upgrades are indispensable to improve performance, robustness, and power consumption. In upgrading file servers, it is crucial to quickly migrate file-sharing services between heterogeneous servers with little downtime while minimizing performance interference. We present a practical quick file server migration scheme based on the postcopy approach that defers file copy until after switching servers. This scheme can (1) reduce downtime with on-demand file migration, (2) avoid performance interference using background migration, and (3) support heterogeneous servers with stub-based file management. We discuss several practical issues, such as intermittent crawling and traversal strategy, and present the solutions in our scheme. We also address several protocol-specific issues to achieve a smooth migration. This scheme is good enough to be adopted in production systems, as it has been demonstrated for several years in real operational environments. The performance evaluation demonstrates that the downtime is less than 3 seconds, and the first file access after switching servers does not cause a timeout in the default timeout settings; it takes less than 10 seconds in most cases and up to 84.55 seconds even in a large directory tree with a depth of 16 and a width of 1,000. Although the total migration time is approximately 3 times longer than the traditional precopy approach that copies all files in advance, our scheme allows the clients to keep accessing files with acceptable overhead. We also show that appropriate selection of traversal strategy reduces tail latency by 88%, and the overhead after the migration is negligible.
C1 [Matsuzawa, Keiichi; Hayasaka, Mitsuo] Hitachi Ltd, Yokohama, Kanagawa, Japan.
   [Matsuzawa, Keiichi; Shinagawa, Takahiro] Univ Tokyo, Tokyo, Japan.
C3 Hitachi Limited; University of Tokyo
RP Matsuzawa, K (corresponding author), Hitachi Ltd, Yokohama, Kanagawa, Japan.; Matsuzawa, K (corresponding author), Univ Tokyo, Tokyo, Japan.
EM keiichi.matsuzawa.kd@hitachi.com; hayasaka.hu@hitachi.com;
   shina@ecc.u-tokyo.ac.jp
RI Shinagawa, Takahiro/ACP-6727-2022
OI Shinagawa, Takahiro/0000-0002-7016-7696
CR Anderson OT, 2004, IBM SYST J, V43, P702, DOI 10.1147/sj.434.0702
   [Anonymous], 2017, STORAGEX 8 0
   [Anonymous], 2008, P USENIX ANN TECHN C
   Azagury A., 2002, 19 IEEE S MASS STOR, P259
   Berriman Ellie, 2011, 3490 NETAPP
   Callaghan Brent, 1995, INTERNET REQUESTS CO
   Datadobi, 2018, DOBIMIGRATE
   Dell EMC, 2018, VNX WHAT TOOLS DOES
   Dell Inc, 2013, DELL FLUIDFS NAS SOL
   Deniel Philippe., 2007, Linux Symposium, P113
   Douceur JR, 1999, PERFORMANCE EVALUATION REVIEW, SPECIAL ISSUE, VOL 27 NO 1, JUNE 1999, P59, DOI 10.1145/301464.301480
   Downey AB, 2001, NINTH INTERNATIONAL SYMPOSIUM ON MODELING, ANALYSIS AND SIMULATION OF COMPUTER AND TELECOMMUNICATION SYSTEMS, PROCEEDINGS, P361, DOI 10.1145/384268.378824
   EMC Corporation, 2013, EMC VNX SER VNX FIL
   EMC Corporation, 2009, EMC RAINF FIL MAN AP
   Eshel Marc., 2010, FAST, P155
   F5 Networks Inc, 2013, ARX SER DAT
   French SM, 2007, P LIN S, P131
   Haynes Thomas, 2016, NETWORK FILE SYSTEM
   Hitz D, 1998, PROCEEDINGS OF THE 2ND USENIX WINDOWS NT SYMPOSIUM, P87
   Huebner E, 2006, DIGIT INVEST, V3, P211, DOI 10.1016/j.diin.2006.10.005
   IBM, 2020, ACT FIL MAN
   IBM, 2020, IBM SPECTR SCAL
   Intel Inc, 2013, PLANN GUID UPD IT IN
   Katsurashima W, 2003, IEEE S MASS STOR SYS, P82, DOI 10.1109/MASS.2003.1194841
   LWN.net, 2019, ZUFS ZER COP FIL
   Matsuzawa K, 2018, SYSTOR'18: PROCEEDINGS OF THE 11TH ACM INTERNATIONAL SYSTEMS AND STORAGE CONFERENCE, P65, DOI 10.1145/3211890.3211894
   Merchant Arpan, 2018, 4015 NETAPP
   Meyer D.T., 2011, Proceedings of the 9th USENIX conference on File and stroage technologies, P1
   Microsoft, 2019, AZ FXT EDG FIL
   Microsoft TechNet, 2015, US FIL SERV MIGR TOO
   Microsoft TechNet, 2016, COMM LIN REF ROB
   Microsoft TechNet, 2009, WIND SERV MIGR TOOLS
   Nemoto Jun, 2017, INT J SMART COMPUT A, V1, P1
   NetApp, 2019, NETAPP XCP MIGR TOOL
   Parisi Justin, 2016, 4063 NETAPP
   Rajasingam Naren, 2019, DATA MIGRATION METHO
   Sato Takashi, 2007, P LIN S, V2, P179
   Schroeder Bianca, 2007, ACM Transactions on Storage, V3, DOI 10.1145/1288783.1288785
   Shepler Spencer, 2010, INTERNET REQUESTS CO
   Sisson Tim, 2012, Operating Systems Review, V46, P11
   Standard Performance Evaluation Corporation, 2014, SPEC SFS 2014 SP2 US
   syncsoft, 2017, 2018 STAT RES
   Tarasov Vasily, 2016, login Usenix Mag, V41, P1
   TechTarget, 2017, STORAGE MAGAZINE, V16, P14
   TechTarget, 2017, STORAGE MAGAZINE, V16, P12
   Tridgell Andrew, 1996, ANU RES PUBLICATIONS
   Vangoor BKR, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P59
   Vantara Hitachi, 2019, HITACHI NAS PLATFORM
   Vantara Hitachi, 2019, HITACHI DATA INGESTO
   Vrable M., 2009, ACM Transactions on Storage, V5, P1
   Yasuda Y, 2003, IEEE S MASS STOR SYS, P219, DOI 10.1109/MASS.2003.1194859
NR 51
TC 1
Z9 1
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUN
PY 2020
VL 16
IS 2
AR 13
DI 10.1145/3377322
PG 30
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1RC
UT WOS:000583743900006
DA 2024-07-18
ER

PT J
AU Chen, SH
   Chen, TY
   Chang, YH
   Wei, HW
   Shih, WK
AF Chen, Shuo-Han
   Chen, Tseng-Yi
   Chang, Yuan-Hao
   Wei, Hsin-Wen
   Shih, Wei-Kuan
TI UnistorFS: A Union Storage File System Design for Resource Sharing
   between Memory and Storage on Persistent RAM-Based Systems
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE NVM; page cache; file mapping; embedded system; FAT
ID PHASE-CHANGE MEMORY; HARD
AB With the advanced technology in persistent random access memory (PRAM), PRAM such as three-dimensional XPoint memory and Phase Change Memory (PCM) is emerging as a promising candidate for the next-generation medium for both (main) memory and storage. Previous works mainly focus on how to overcome the possible endurance issues of PRAM while both main memory and storage own a partition on the same PRAM device. However, a holistic software-level system design should be proposed to fully exploit the benefit of PRAM. This article proposes a union storage file system (UnistorFS), which aims to jointly manage the PRAM resource for main memory and storage. The proposed UnistorFS realizes the concept of using the PRAM resource as memory and storage interchangeably to achieve resource sharing while main memory and storage coexist on the same PRAM device with no partition or logical boundary. This approach not only enables PRAM resource sharing but also eliminates unnecessary data movements between main memory and storage since they are already in the same address space and can be accessed directly. At the same time, the proposed UnistorFS ensures the persistence of file data and sanity of the file system after power recycling. A series of experiments was conducted on a modified Linux kernel. The results show that the proposed UnistorFS can eliminate unnecessary memory accesses and outperform other PRAM-based file systems for 0.2-8.7 times in terms of read/write performance.
C1 [Chen, Shuo-Han] Natl Tsing Hua Univ, Hsinchu 300, Taiwan.
   [Chen, Tseng-Yi; Chang, Yuan-Hao] Acad Sinica, Taipei 115, Taiwan.
   [Chen, Tseng-Yi] Yuan Ze Univ, Taipei 115, Taiwan.
   [Wei, Hsin-Wen] Tamkang Univ, New Taipei 235, Taiwan.
   [Shih, Wei-Kuan] Natl Tsing Hua Univ, Hsinchu 300, Taiwan.
C3 National Tsing Hua University; Academia Sinica - Taiwan; Yuan Ze
   University; Tamkang University; National Tsing Hua University
RP Chen, SH (corresponding author), Natl Tsing Hua Univ, Hsinchu 300, Taiwan.
EM shuohanchen@mx.nthu.edu.tw; tsengyi@iis.sinica.edu.tw;
   johnson@iis.sinica.edu.tw; hwwei@mail.tku.edu.tw; wshih@cs.nthu.edu.tw
RI Chang, Yuan-Hao/ABA-6935-2020
OI Chang, Yuan-Hao/0000-0002-1282-2111; Chen, Shuo-Han/0000-0002-1619-4335;
   Chen, Tseng-Yi/0000-0003-2939-2821
FU Ministry of Science and Technology, R.O.C. [103-2221-E-001-012-MY2]
FX This work is supported by the Ministry of Science and Technology,
   R.O.C., under 103-2221-E-001-012-MY2.
CR Aggarwal Nidhi, 2008, 2008 IEEE 14th International Symposium on High Performance Computer Architecture, P317, DOI 10.1109/HPCA.2008.4658649
   Agrawal Nitin, 2007, ACM Transactions on Storage, V3, DOI 10.1145/1288783.1288788
   [Anonymous], SHOCK VIBRATION
   [Anonymous], 2008, Professional Linux Kernel Architecture
   [Anonymous], [No title captured]
   [Anonymous], 2011, P 9 USENIX C FIL STO
   Arulraj J, 2015, SIGMOD'15: PROCEEDINGS OF THE 2015 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P707, DOI 10.1145/2723372.2749441
   Bovet D. P., 2005, Understanding the Linux Kernel: from I/O ports to process management
   Butler WH, 2004, NAT MATER, V3, P845, DOI 10.1038/nmat1272
   Caulfield Adrian M., 2010, Proceedings 2010 43rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO 2010), P385, DOI 10.1109/MICRO.2010.33
   Chen T. -Y., 2017, P 54 ANN DES AUT C 2
   Cheng SW, 2014, ICCAD-IEEE ACM INT, P734, DOI 10.1109/ICCAD.2014.7001433
   Condit J, 2009, SOSP'09: PROCEEDINGS OF THE TWENTY-SECOND ACM SIGOPS SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P133
   Dulloor Subramanya R., 2014, P 9 EUR C COMP SYST
   Fio, 2005, FLEX IO
   Hu JT, 2010, DES AUT CON, P350
   Hur Ibrahim, 2008, 2008 IEEE 14th International Symposium on High Performance Computer Architecture, P305, DOI 10.1109/HPCA.2008.4658648
   Intel, 2016, INTR BREAKTHR MEM TE
   Ipek E, 2010, ASPLOS XV: FIFTEENTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS, P3
   ITRS, 2014, ITRS ROADMAP
   Jacobvitz AN, 2013, INT S HIGH PERF COMP, P222, DOI 10.1109/HPCA.2013.6522321
   Joo Y, 2010, DES AUT TEST EUROPE, P136
   Jung J.-Y., 2013, P 27 INT ACM C INT C, P115
   Jung J, 2010, ACM T STORAGE, V6, DOI 10.1145/1714454.1714457
   Kannan S, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901325
   Lee BC, 2010, IEEE MICRO, V30, P131, DOI 10.1109/MM.2010.24
   Lee BC, 2009, CONF PROC INT SYMP C, P2, DOI 10.1145/1555815.1555758
   Lee E, 2012, ELECTRON LETT, V48, P1053, DOI 10.1049/el.2012.1016
   Lee E, 2015, IEEE T COMPUT, V64, P1349, DOI 10.1109/TC.2014.2329674
   Lee HG, 2011, 2011 IEEE 29TH INTERNATIONAL CONFERENCE ON COMPUTER DESIGN (ICCD), P381, DOI 10.1109/ICCD.2011.6081427
   Liu RS, 2014, ACM SIGPLAN NOTICES, V49, P455, DOI 10.1145/2541940.2541957
   Melhem R., 2012, 2012 42nd Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN), P1
   Micron, 2016, BREAKTHR NONV MEM TE
   Mittal S, 2016, IEEE T PARALL DISTR, V27, P1537, DOI 10.1109/TPDS.2015.2442980
   Nak Hee Seong, 2010, Proceedings 2010 43rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO 2010), P115, DOI 10.1109/MICRO.2010.46
   Qureshi Moinuddin K., 2009, Proceedings of the 2009 42nd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO 2009), P14, DOI 10.1145/1669112.1669117
   Qureshi MK, 2011, INT SYMP MICROARCH, P318
   Qureshi MK, 2009, CONF PROC INT SYMP C, P24, DOI 10.1145/1555815.1555760
   Sangyeun Cho, 2009, Proceedings of the 2009 42nd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO 2009), P347
   Schechter S, 2010, CONF PROC INT SYMP C, P141
   Seong NH, 2010, CONF PROC INT SYMP C, P383, DOI 10.1145/1816038.1816014
   Sha EHM, 2016, IEEE T COMPUT, V65, P2959, DOI 10.1109/TC.2016.2516019
   Shao ZL, 2012, 2012 IEEE COMPUTER SOCIETY ANNUAL SYMPOSIUM ON VLSI (ISVLSI), P398, DOI 10.1109/ISVLSI.2012.81
   Soyoon Lee, 2011, Proceedings of the 2011 IEEE 19th International Symposium on Modelling, Analysis & Simulation of Computer and Telecommunication Systems (MASCOTS 2011), P168, DOI 10.1109/MASCOTS.2011.68
   Vatto K., 2015, ANAL INTEL MICRON 3D
   Vogelsang T., 2010, Proceedings 2010 43rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO 2010), P363, DOI 10.1109/MICRO.2010.42
   Weisstein Eric W., 2017, GAMMA DISTRIBUTION
   Wu Xiaojian., 2011, Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis (SC '11), p39:1
   Yang BD, 2007, IEEE INT SYMP CIRC S, P3014, DOI 10.1109/ISCAS.2007.377981
   Yang MC, 2013, IEEE INT CONF EMBED, P139, DOI 10.1109/RTCSA.2013.6732212
   Yoon DH, 2011, INT S HIGH PERF COMP, P466, DOI 10.1109/HPCA.2011.5749752
   Zhou P, 2009, CONF PROC INT SYMP C, P14, DOI 10.1145/1555815.1555759
   Zhu J. G., 2000, J APPL PHYS, V87, P9
NR 53
TC 8
Z9 9
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD APR
PY 2018
VL 14
IS 1
SI SI
AR 3
DI 10.1145/3177918
PG 22
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GH5ZN
UT WOS:000433517600003
DA 2024-07-18
ER

PT J
AU Lu, LY
   Pillai, TS
   Gopalakrishnan, H
   Arpaci-Dusseau, AC
   Arpaci-Dusseau, RH
AF Lu, Lanyue
   Pillai, Thanumalayan Sankaranarayana
   Gopalakrishnan, Hariharan
   Arpaci-Dusseau, Andrea C.
   Arpaci-Dusseau, Remzi H.
TI WiscKey: Separating Keys from Values in SSD-Conscious Storage
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article; Proceedings Paper
CT 14th USENIX Conference on File and Storage Technologies (FAST)
CY FEB 22-25, 2016
CL Santa Clara, CA
SP NetApp, Google, Veritas, VMWare, Facebook, Microsoft Res, EMC2, Huawei, Redhat, Tintri, Hewlett Packard Enterprise, IBM Res, Radian, SNIA, USENIX, ACM Queue, ADMIN Magazine, CRC Press, Linux Pro Magazine, LXer, No Starch Press, OReilly Media, Raspberry Pi Geek, UserFriendly Org, NSF
DE LevelDB; WiscKey; flash-based SSDs
AB We present WiscKey, a persistent LSM-tree-based key-value store with a performance-oriented data layout that separates keys from values to minimize I/O amplification. The design of WiscKey is highly SSD optimized, leveraging both the sequential and random performance characteristics of the device. We demonstrate the advantages of WiscKey with bothmicro benchmarks and YCSB workloads. Microbenchmark results show that WiscKey is 2.5 x to 111 x faster than LevelDB for loading a database (with significantly better tail latencies) and 1.6 x to 14 x faster for random lookups. WiscKey is faster than both LevelDB and RocksDB in all six YCSB workloads.
C1 [Lu, Lanyue; Pillai, Thanumalayan Sankaranarayana; Gopalakrishnan, Hariharan; Arpaci-Dusseau, Andrea C.; Arpaci-Dusseau, Remzi H.] Univ Wisconsin, Madison, WI 53706 USA.
   [Lu, Lanyue; Pillai, Thanumalayan Sankaranarayana; Gopalakrishnan, Hariharan; Arpaci-Dusseau, Andrea C.; Arpaci-Dusseau, Remzi H.] 1210 W Dayton St, Madison, WI 53706 USA.
C3 University of Wisconsin System; University of Wisconsin Madison
RP Lu, LY (corresponding author), Univ Wisconsin, Madison, WI 53706 USA.
EM lu.lanyue@gmail.com; madthanu@cs.wisc.edu; hgopalakris2@wisc.edu;
   dusseau@cs.wisc.edu; remzi@cs.wisc.edu
FU NSF [CNS-1419199, CNS-1421033, CNS-1319405, CNS-1218405]
FX The research herein was supported by funding from NSF grants
   CNS-1419199, CNS-1421033, CNS-1319405, and CNS-1218405, as well as
   generous donations from EMC, Facebook, Google, Huawei, Microsoft,
   NetApp, Seagate, Samsung, Veritas, and VMware. donations from EMC,
   Facebook, Google, Huawei, Microsoft, NetApp, Seagate, Samsung, Veritas,
   and VMware. Any opinions, findings, and conclusions or recommendations
   expressed in this material are those of the authors and may not reflect
   the views of NSF or other institutions. This article is an extended
   version of a FAST '16 paper by Lu et al. [2016]. The additional material
   here includes a detailed study of tail latency (for both reads and
   writes) and LSM-tree size, more thorough descriptions of LSM-tree
   operation and basic APIs, more graphics depicting key data structures to
   aid in understanding, improved citations and related work, new future
   work and conclusions, and many other small edits and updates.
CR Ahn JS, 2016, IEEE T COMPUT, V65, P902, DOI 10.1109/TC.2015.2435779
   Alagappan Ramnatthan, 2016, P 12 S OP SYST DES I
   Anand A, 2010, P 7 S NETW SYST DES
   Andersen D. G., 2009, P 22 ACM S OP SYST P
   [Anonymous], P 36 INT C VER LARG
   [Anonymous], 2011, P 23 ACM S OP SYST P
   Armstrong Timothy G., 2013, P SIGMOD
   Arpaci-Dusseau R. H., 2014, OPERATING SYSTEMS 3
   Athanassoulis Manos, 2015, 19 INT C EXT DAT TEC
   Atikoglu Berk, 2015, P USENIX ANN TECHN C
   Beaver Doug, 2010, P 9 S OP SYST DES IM
   Bender Michael A., 2007, P 19 ACM S PAR ALG A
   Buchsbaum Adam L., 2000, P 11 ANN ACM SIAM S
   Canadi Igor, 2015, ROCKSDB BLOG
   CAULFIELD A. M., 2010, P 43 ANN IEEE ACM IN
   Chang F, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P205
   Chen  F., 2011, P 17 INT S HIGH PERF
   Chen Jianjun, 2012, P 2012 ACM SIGMOD IN
   Colgrove John, 2015, P 2015 ACM SIGMOD IN
   Cooper B. F., 2010, P ACM S CLOUD COMP S
   Cooper Brian F., 2008, P VLDB END PVLDB 08
   Debnath Biplob, 2011, P 2011 ACM SIGMOD IN
   DeCandia Guiseppe, 2007, P 21 ACM S OP SYST P
   Dong Siying, 2015, ROCKSDB CHALLENGES L
   Fan B., 2013, P USENIX NSDI
   Fusion-I. O, 2015, FUS I O IODRIVE2
   George L., 2011, HBASE DEFINITIVE GUI
   Golan-Gueta Guy, 2015, P EUROSYS C EUROSYS
   Harter Tyler, 2014, P 12 USENIX S FIL ST
   Hyojun Kim, 2012, P 10 USENIX S FIL ST
   Jannen W., 2015, P 13 USENIX C FIL ST
   Lai Chunbo, 2015, P 31 IEEE C MASS DAT
   Lakshman Avinash, 2009, 3 ACM SIGOPS INT WOK
   Lee Changman, 2015, FAST
   Lim Hyeontaek, 2014, P 11 S NETW SYST DES
   Lu Lanyue, 2016, P 14 USENIX C FIL ST
   Mai Haohui, 2015, 8 ANN HADOOP SUMM
   Mao Yandong, 2012, P EUROSYS C EUROSYS
   Marmol Leonardo, 2015, P USENIX ANN TECHN C
   Min Changwoo, 2012, P 10 USENIX C FIL ST
   Nishtala R., 2013, P 10 S NETW SYST DES
   Nyberg Chris, 1994, P 1994 ACM SIGMOD IN
   ONeil P, 1996, ACTA INFORM, V33, P351, DOI 10.1007/s002360050048
   Peter Simon, 2014, P 11 S OP SYST DES I
   Pillai Thanumalayan Sankaranarayana, 2014, OSDI, P433
   Ren K, 2014, P INT C HIGH PERF CO
   Ren Kai, 2013, P USENIX ANN TECHN C
   Sears R., 2012, SIGMOD
   Shetty P.J., 2013, P PART 11 USENIX C F
   Sumbaly Roshan, 2012, P 10 USENIX S FIL ST
   Vasudevan Vijay, 2012, P ACM S CLOUD COMP S
   Wang Peng, 2014, P EUROSYS C EUROSYS
   WU X., 2015, P USENIX ANN TECHN C
NR 53
TC 106
Z9 117
U1 2
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAR
PY 2017
VL 13
IS 1
SI SI
AR 5
DI 10.1145/3033273
PG 28
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA ER3WE
UT WOS:000398729600005
OA Bronze
DA 2024-07-18
ER

PT J
AU Choi, JW
   Shin, DI
   Yu, YJ
   Eom, H
   Yeom, HY
AF Choi, Jae Woo
   Shin, Dong In
   Yu, Young Jin
   Eom, Hyeonsang
   Yeom, Heon Young
TI Towards High-Performance SAN with Fast Storage Devices
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE SAN; fast storage; RDMA; InfiniBand; storage stack optimization; Design;
   Measurement; Performance
ID CLASS MEMORY
AB Storage area network (SAN) is one of the most popular solutions for constructing server environments these days. In these kinds of server environments, HDD-based storage usually becomes the bottleneck of the overall system, but it is not enough to merely replace the devices with faster ones in order to exploit their high performance. In other words, proper optimizations are needed to fully utilize their performance gains. In this work, we first adopted a DRAM-based SSD as a fast backend-storage in the existing SAN environment, and found significant performance degradation compared to its own capabilities, especially in the case of small-sized random I/O pattern, even though a high- speed network was used. We have proposed three optimizations to solve this problem: (1) removing software overhead in the SAN I/O path; (2) increasing parallelism in the procedures for handling I/ O requests; and (3) adopting the temporal merge mechanism to reduce network overheads. We have implemented them as a prototype and found that our approaches make substantial performance improvements by up to 39% and 280% in terms of both the latency and bandwidth, respectively.
C1 [Choi, Jae Woo; Shin, Dong In; Yu, Young Jin; Eom, Hyeonsang; Yeom, Heon Young] Seoul Natl Univ, Distributed Comp Syst Lab, Seoul 151742, South Korea.
C3 Seoul National University (SNU)
RP Eom, H (corresponding author), Seoul Natl Univ, Distributed Comp Syst Lab, Seoul 151742, South Korea.
EM aryuze@hotmail.com; hseom@cse.snu.ac.kr; yeom@snu.ac.kr
FU Technology Innovation Program (Industrial Strategic technology
   development program) [10039163]; Ministry of Knowledge Economy (MKE,
   Korea); Basic Science Research Program through the National Research
   Foundation of Korea (NRF); Ministry of Education, Science and Technology
   [2010-0024969]
FX This work was supported by the Technology Innovation Program (Industrial
   Strategic technology development program, 10039163) funded by the
   Ministry of Knowledge Economy (MKE, Korea). This research was also
   supported by the Basic Science Research Program through the National
   Research Foundation of Korea (NRF) funded by the Ministry of Education,
   Science and Technology (2010-0024969).
CR [Anonymous], 2010, SIGOPS Oper. Syst. Rev., DOI DOI 10.1145/1713254.1713276
   [Anonymous], 2012, NVM Express Revision 1.1
   Bolhovitin V, 2004, SCST GENERIC SCSI TA
   Burr GW, 2008, IBM J RES DEV, V52, P449, DOI 10.1147/rd.524.0449
   Cashin E. L., 2005, LINUX J, V134, P10
   Caulfield Adrian M., 2010, Proceedings 2010 43rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO 2010), P385, DOI 10.1109/MICRO.2010.33
   Freitas RF, 2008, IBM J RES DEV, V52, P439, DOI 10.1147/rd.524.0439
   Humphries C., 2011, FUSION IO IODRIVE PE
   INCITS, 2009, FIBRE CHANNEL BACKBO
   Katcher J., 1997, TECH REP
   Lussier D., 2004, BENCHMARKSQL
   M Ko, 2003, PROCEEDINGS OF THE R
   Machek P., 1997, NETWORK BLOCK DEVICE
   Mellanox, 2012, MELLANOX TECHNOLOGIE
   Palekar A., 2001, PROCEEDINGS OF THE 5, V5, P11
   Peterson L. L., 2007, COMPUTER NETWORKS A
   Pfister Gregory F, 2001, High Performance Mass Storage and Parallel I/O, V42, P102
   Schneider E., 2012, SAP HANA TECHNICAL O
   Taejin Infotech, 2012, HYBRID APPLIANCE HHA
   Tate J., 2012, IBM REDBOOKS TECH RE
   Woodruff B., 2005, PROCEEDINGS OF THE L, V2, P271
   Yang J., 2011, PROCEEDINGS OF THE 9
   Yu Y. J., 2012, PROCEEDINGS OF THE 4
NR 23
TC 3
Z9 3
U1 2
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAR
PY 2014
VL 10
IS 2
AR 5
DI 10.1145/2577385
PG 18
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AF2CV
UT WOS:000334521100001
DA 2024-07-18
ER

PT J
AU Cao, ZC
   Dong, HB
   Wei, YX
   Liu, SY
   Du, DHC
AF Cao, Zhichao
   Dong, Huibing
   Wei, Yixun
   Liu, Shiyong
   Du, David H. C.
TI IS-HBase: An In-Storage Computing Optimized HBase with I/O Offloading
   and Self-Adaptive Caching in Compute-Storage Disaggregated
   Infrastructure
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE In-storage computing; database; HBase; key-value store; performance
   improvement; caching; compute-storage disaggregated infrastructure
ID SYSTEM
AB Active storage devices and in-storage computing are proposed and developed in recent years to effectively reduce the amount of required data traffic and to improve the overall application performance. They are especially preferred in the compute-storage disaggregated infrastructure. In both techniques, a simple computing module is added to storage devices/servers such that some stored data can be processed in the storage devices/servers before being transmitted to application servers. This can reduce the required network bandwidth and offload certain computing requirements from application servers to storage devices/servers. However, several challenges exist when designing an in-storage computing- based architecture for applications. These include what computing functions need to be offloaded, how to design the protocol between in-storage modules and application servers, and how to deal with the caching issue in application servers.
   HBase is an important and widely used distributed Key-Value Store. It stores and indexes key-value pairs in large files in a storage system like HDFS. However, its performance especially read performance, is impacted by the heavy traffics between HBase RegionServers and storage servers in the compute-storage disaggregated infrastructure when the available network bandwidth is limited. We propose an In- Storage-based HBase architecture, called IS-HBase, to improve the overall performance and to address the aforementioned challenges. First, IS-HBase executes a data pre-processing module (In-Storage ScanNer, called ISSN) for some read queries and returns the requested key-value pairs to RegionServers instead of returning data blocks in HFile. IS-HBase carries out compactions in storage servers to reduce the large amount of data being transmitted through the network and thus the compaction execution time is effectively reduced. Second, a set of new protocols is proposed to address the communication and coordination between HBase RegionServers at computing nodes and ISSNs at storage nodes. Third, a new self-adaptive caching scheme is proposed to better serve the read queries with fewer I/O operations and less network traffic. According to our experiments, the IS-HBase can reduce up to 97% network traffic for read queries and the throughput (queries per second) is significantly less affected by the fluctuation of available network bandwidth. The execution time of compaction in IS-HBase is only about 6.31% - 41.84% of the execution time of legacy HBase. In general, IS-HBase demonstrates the potential of adopting in-storage computing for other data-intensive distributed applications to significantly improve performance in compute-storage disaggregated infrastructure.
C1 [Cao, Zhichao; Dong, Huibing; Wei, Yixun; Du, David H. C.] Univ Minnesota, 4-192 Keller Hall,200 Union St SE, Minneapolis, MN 55455 USA.
   [Liu, Shiyong] Ocean Univ China, 23 Eastern Hongkong Rd,OUC Fushan Campus, Qingdao 266005, Shandong, Peoples R China.
C3 University of Minnesota System; University of Minnesota Twin Cities;
   Ocean University of China
RP Cao, ZC (corresponding author), Univ Minnesota, 4-192 Keller Hall,200 Union St SE, Minneapolis, MN 55455 USA.
EM caoxx380@umn.edu; dong0198@umn.edu; wei00161@UMN.EDU; lshyouc@163.com;
   du@umn.edu
OI Cao, Zhichao/0000-0001-6950-1776
FU NSF [1812537]; NSF I/UCRC Center on Intelligent Storage (CRIS); Direct
   For Computer & Info Scie & Enginr; Division Of Computer and Network
   Systems [1812537] Funding Source: National Science Foundation
FX This work was partially supported by NSF award 1812537 and NSF I/UCRC
   Center on Intelligent Storage (CRIS).
CR Angel Sebastian, 2020, P 12 USENIX C HOT TO
   [Anonymous], 2019, APACHE HDFS USERS GU
   [Anonymous], 2017, SAMSUNG KEY VALUE SS
   [Anonymous], 2014, SEAGATE KINETIC HDD
   [Anonymous], 2019, IPTABLES
   [Anonymous], 2019, APACHE HBASE
   [Anonymous], 2019, Amazon s3
   Atikoglu Berk, 2012, Performance Evaluation Review, V40, P53, DOI 10.1145/2318857.2254766
   Bae DH, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P1573, DOI 10.1145/2505515.2507847
   Bindschaedler L, 2020, TWENTY-FIFTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXV), P301, DOI 10.1145/3373376.3378504
   Borthakur D., 2008, HADOOP APACHE PROJECT
   Calder B, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P143
   Cao X, 2017, 2017 THIRD IEEE INTERNATIONAL CONFERENCE ON BIG DATA COMPUTING SERVICE AND APPLICATIONS (IEEE BIGDATASERVICE 2017), P60, DOI 10.1109/BigDataService.2017.30
   Cao ZC, 2020, PROCEEDINGS OF THE 18TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P209
   Chang F, 2008, ACM T COMPUT SYST, V26, DOI 10.1145/1365815.1365816
   Cooper Brian F., 2010, P 1 ACM S CLOUD COMP, P143, DOI [DOI 10.1145/1807128.1807152, 10.1145/1807128.1807152]
   Desai Dharmesh, 2019, CLOUD ADVANTAGE DECO
   Do J., 2013, SIGMOD, P1221
   George L., 2011, HBASE DEFINITIVE GUI
   Gu B, 2016, CONF PROC INT SYMP C, P153, DOI 10.1109/ISCA.2016.23
   Harter Tyler., 2014, 12 USENIX C FILE STO, P199
   Jalaparti V, 2018, PROCEEDINGS OF THE 2018 ACM SYMPOSIUM ON CLOUD COMPUTING (SOCC '18), P186, DOI 10.1145/3267809.3267827
   Jo I, 2016, PROC VLDB ENDOW, V9, P924
   Kang Y, 2013, IEEE S MASS STOR SYS
   Kim Sungchan, 2011, P INT WORKSH ACC DAT
   Liang SW, 2019, I C FIELD PROG LOGIC, P173, DOI 10.1109/FPL.2019.00035
   Minglani M, 2017, INT C PAR DISTRIB SY, P501, DOI 10.1109/ICPADS.2017.00072
   ONeil P, 1996, ACTA INFORM, V33, P351, DOI 10.1007/s002360050048
   Park D., IEEE T COMPUT, DOI [10.1109/TC.2016.2595566, DOI 10.1109/TC.2016.2595566]
   Pfaff B., 2015, 12 USENIX S NETW SYS
   Resines MZ, 2014, J PHYS CONF SER, V513, DOI 10.1088/1742-6596/513/4/042024
   Riedel E., 1998, Proceedings of the Twenty-Fourth International Conference on Very-Large Databases, P62
   Riedel E, 2001, COMPUTER, V34, P68, DOI 10.1109/2.928624
   Riedel Erik., 1997, Active disks-remote execution for network-attached storage
   Shvachko K, 2010, IEEE S MASS STOR SYS
   Sun H, 2018, IEEE ACCESS, V6, P61233, DOI 10.1109/ACCESS.2018.2873579
   Sun Hui, 2018, ARXIV180704151
   Vora M. N., 2011, Proceedings of the 2011 International Conference on Computer Science and Network Technology (ICCSNT), P601, DOI 10.1109/ICCSNT.2011.6182030
   Vuppalapati M, 2020, PROCEEDINGS OF THE 17TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P449
   Wang Jianguo, 2016, IEEE T COMPUT, DOI [10.1109/TC.2016.2608818, DOI 10.1109/TC.2016.2608818]
   Wang Jianguo., 2016, Proceedings of the 12th International Workshop on Data Management on New Hardware, page, P4
NR 41
TC 2
Z9 2
U1 0
U2 18
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAY
PY 2022
VL 18
IS 2
AR 15
DI 10.1145/3488368
PG 42
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 1H3XN
UT WOS:000796478700006
DA 2024-07-18
ER

PT J
AU Basak, J
   Wadhwani, K
   Voruganti, K
AF Basak, Jayanta
   Wadhwani, Kushal
   Voruganti, Kaladhar
TI Storage Workload Identification
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Storage workload identification; workload signature
AB Storage workload identification is the task of characterizing a workload in a storage system (more specifically, network storage system-NAS or SAN) and matching it with the previously known workloads. We refer to storage workload identification as "workload identification" in the rest of this article. Workload identification is an important problem for cloud providers to solve because (1) providers can leverage this information to colocate similar workloads to make the system more predictable and (2) providers can identify workloads and subsequently give guidance to the subscribers as to associated best practices (with respect to configuration) for provisioning those workloads.
   Historically, people have identified workloads by looking at their read/write ratios, random/sequential ratios, block size, and interarrival frequency. Researchers are well aware that workload characteristics change over time and that one cannot just take a point in time view of a workload, as that will incorrectly characterize workload behavior. Increasingly, manual detection of workload signature is becoming harder because (1) it is difficult for a human to detect a pattern and (2) representing a workload signature by a tuple consisting of average values for each of the signature components leads to a large error.
   In this article, we present workload signature detection and a matching algorithm that is able to correctly identify workload signatures and match them with other similar workload signatures. We have tested our algorithm on nine different workloads generated using publicly available traces and on real customer workloads running in the field to show the robustness of our approach.
C1 [Basak, Jayanta] NetApp Inc, EGL, Cinnabar Hills,Off Intermediate Ring Rd, Bangalore 560071, Karnataka, India.
   [Wadhwani, Kushal; Voruganti, Kaladhar] NetApp Inc, Bangalore, Karnataka, India.
   [Wadhwani, Kushal] Vizury Interact Solut Pvt Ltd, 24,4th Floor,Hosur Main Rd, Bangalore 560030, Karnataka, India.
   [Voruganti, Kaladhar] 1 Lagoon Dr, Redwood City, CA 94065 USA.
C3 NetApp, Inc.; NetApp, Inc.
RP Basak, J (corresponding author), NetApp Inc, EGL, Cinnabar Hills,Off Intermediate Ring Rd, Bangalore 560071, Karnataka, India.
EM basak@netapp.com; kushal.wadhwani@vizury.com; kvoruganti@equinix.com
CR Abad C. L., 2012, 2012 IEEE International Symposium on Workload Characterization (IISWC 2012), P100, DOI 10.1109/IISWC.2012.6402909
   [Anonymous], 2009, P INT WORKSH VIRT PE
   [Anonymous], P 2 WORKSH EX EV RES
   Bairavasundaram Lakshmi N., 2012, Operating Systems Review, V46, P32
   Breiman L., 1983, CLASSIFICATION REGRE
   Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555
   Chen YP, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P43
   Duda R. O., 2000, PATTERN CLASSIFICATI
   Elnaffar S., 2002, P ACM INT C INF KNOW
   Gulati Ajay, 2011, P 2 ACM S CLOUD COMP, P19
   Hastie T., 2009, ELEMENTS STAT LEARNI
   Jiang W., 2009, Proccedings of the 7th conference on File and storage technologies, P43
   Kavalanekar Swaroop, 2008, 2008 IEEE International Symposium on Workload Characterization (IISWC), P119, DOI 10.1109/IISWC.2008.4636097
   Koshy J., 2007, PMC BASED PERFORMANC
   Lawrence ND, 2012, J MACH LEARN RES, V13, P1609
   Liu Yang., 2014, P 12 USENIX C FILE S, P213
   Mengzhi Wang, 2004, Performance Evaluation Review, V32, P412, DOI 10.1145/1012888.1005743
   Narayanan D, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P253
   Oh JS, 2008, ICHIT 2008: INTERNATIONAL CONFERENCE ON CONVERGENCE AND HYBRID INFORMATION TECHNOLOGY, PROCEEDINGS, P334, DOI 10.1109/ICHIT.2008.216
   Pentakalos O. I., 1996, P 5 NASA GODD C MASS
   PERELMAN E, 2006, P IEEE INT PAR DISTR
   Pipada P., 2012, P 4 USENIX WORKSH HO
   Riska A, 2006, USENIX ASSOCIATION PROCEEDINGS OF THE 2006 USENIX ANNUAL TECHNICAL CONFERENCE, P97
   Sandeep S. R., 2008, P 1 USENIX WORKSH AN
   Sikalinda P., 2006, THESIS
   SNIA, 2011, SNIA IOTTA REP I O T
   Storage Performance Council (SPC), 2002, STOR PERF COUNC SPC
   Tan P. N., 2016, INTRO DATA MINING
   VEITCH A, 2003, HPLSSP200313
   Witten IH, 2011, MOR KAUF D, P1
   Yadwadkar NeerajaJ., 2010, 8th USENIX Conference on File and Storage Technologies, San Jose, CA, USA, February 23-26, 2010, P183
   Yin S, 2014, IEEE T DEPEND SECURE, V11, P345, DOI 10.1109/TDSC.2013.47
   ZADEH LA, 1965, INFORM CONTROL, V8, P338, DOI 10.1016/S0019-9958(65)90241-X
   Zheng Jiquan, 2010, THESIS
NR 34
TC 6
Z9 6
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUN
PY 2016
VL 12
IS 3
AR 14
DI 10.1145/2818716
PG 30
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DQ7YV
UT WOS:000379426200005
DA 2024-07-18
ER

PT J
AU Basak, J
   Nagesh, PC
AF Basak, Jayanta
   Nagesh, P. C.
TI A User-Friendly Log Viewer for Storage Systems
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Log reduction; filtering; learning
AB System log files contains messages emitted from several modules within a system and carries valuable information about the system state such as device status and error conditions and also about the various tasks within the system such as program names, execution path, including function names and parameters, and the task completion status. For customers with remote support, the system collects and transmits these logs to a central enterprise repository, where these are monitored for alerts, problem forecasting, and troubleshooting.
   Very large log files limit the interpretability for the support engineers. For an expert, a large volume of log messages may not pose any problem; however, an inexperienced person may get flummoxed due to the presence of a large number of log messages. Often it is desired to present the log messages in a comprehensive manner where a person can view the important messages first and then go into details if required.
   In this article, we present a user-friendly log viewer where we first hide the unimportant or inconsequential messages from the log file. A user can then click a particular hidden view and get the details of the hided messages. Messages with low utility are considered inconsequential as their removal does not impact the end user for the aforesaid purpose such as problem forecasting or troubleshooting. We relate the utility of a message to the probability of its appearance in the due context. We present machine-learning-based techniques that computes the usefulness of individual messages in a log file. We demonstrate identification and discarding of inconsequential messages to shrink the log size to acceptable limits. We have tested this over real-world logs and observed that eliminating such low value data can reduce the log files significantly (30% to 55%), with minimal error rates (7% to 20%). When limited user feedback is available, we show modifications to the technique to learn the user intent and accordingly further reduce the error.
C1 [Basak, Jayanta; Nagesh, P. C.] NetApp Inc, Cinnabar Hills,EGL,Intermediate Ring Rd, Bangalore 560071, Karnataka, India.
   [Nagesh, P. C.] Univ Melbourne, Dept Comp & Informat Syst, Melbourne, Vic 3010, Australia.
C3 NetApp, Inc.; University of Melbourne
RP Basak, J (corresponding author), NetApp Inc, Cinnabar Hills,EGL,Intermediate Ring Rd, Bangalore 560071, Karnataka, India.
EM basak@netapp.com; npanyam@student.unimelb.edu.au
CR Agrawal R., 1993, SIGMOD Record, V22, P207, DOI 10.1145/170036.170072
   Alspaugh S., 2014, 28 LARG INST SYST AD, P62
   [Anonymous], 2009, SIGEVOlution
   [Anonymous], SIGKDD EXPLOR NEWSL
   [Anonymous], P SLAML
   Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555
   Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010
   Jiang W., 2009, Proccedings of the 7th conference on File and storage technologies, P43
   Koshy J., 2007, PMC BASED PERFORMANC
   Kramer Time, 2003, EFFECTIVE LOG REDUCT
   Lian Y, 2007, PROCEEDINGS OF THE 3RD INTERNATIONAL YELLOW RIVER FORUM ON SUSTAINABLE WATER RESOURCES MANAGEMENT AND DELTA ECOSYSTEM MAINTENANCE, VOL II, P3
   Maas Andrew L., 2010, P WORKSH DEEP LEARN, V10
   Network Appliance, 2007, PROACT HLTH MAN AUT
   Sarukkai RR, 2000, COMPUT NETW, V33, P377, DOI 10.1016/S1389-1286(00)00044-X
   Shahrestani Alireza, 2010, Proceedings of the 2010 Fourth International Conference on Emerging Security Information, Systems and Technologies (SECURWARE), P182, DOI 10.1109/SECURWARE.2010.37
   WHITLEY D, 1994, STAT COMPUT, V4, P65, DOI 10.1007/BF00175354
   Zheng ZM, 2009, I C DEPEND SYS NETWO, P572, DOI 10.1109/DSN.2009.5270289
NR 17
TC 2
Z9 2
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUN
PY 2016
VL 12
IS 3
AR 17
DI 10.1145/2846101
PG 18
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DQ7YV
UT WOS:000379426200008
DA 2024-07-18
ER

PT J
AU Jones, SN
   Amer, A
   Miller, EL
   Long, DDE
   Pitchumani, R
   Strong, CR
AF Jones, Stephanie N.
   Amer, Ahmed
   Miller, Ethan L.
   Long, Darrell D. E.
   Pitchumani, Rekha
   Strong, Christina R.
TI Classifying Data to Reduce Long-Term Data Movement in Shingled Write
   Disks
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article; Proceedings Paper
CT 31st International Conference on Massive Storage Systems and
   Technologies (MSST 2015)
CY JUN 01-05, 2015
CL Santa Clara, CA
DE Storage; shingled write disks; shingled magnetic recording drives; data
   placement
AB Shingled magnetic recording (SMR) is a means of increasing the density of hard drives that brings a new set of challenges. Due to the nature of SMR disks, updating in place is not an option. Holes left by invalidated data can only be filled if the entire band is reclaimed, and a poor band compaction algorithm could result in spending a lot of time moving block's over the lifetime of the device. We propose using write frequency to separate blocks to reduce data movement and develop a band compaction algorithm that implements this heuristic. We demonstrate how our algorithm results in improved data management, resulting in an up to 45% reduction in required data movements when compared to naive approaches to band management.
C1 [Jones, Stephanie N.; Miller, Ethan L.; Long, Darrell D. E.; Pitchumani, Rekha; Strong, Christina R.] Univ Calif Santa Cruz, 1156 High St, Santa Cruz, CA 95064 USA.
   [Amer, Ahmed] Santa Clara Univ, 1156 High St, Santa Cruz, CA 95064 USA.
C3 University of California System; University of California Santa Cruz;
   Santa Clara University
RP Jones, SN (corresponding author), Univ Calif Santa Cruz, 1156 High St, Santa Cruz, CA 95064 USA.
OI Miller, Ethan/0000-0003-2994-9060
FU National Science Foundation [IIP-1266400, CCF-1219163, CCF-1217648];
   Department of Energy [DE-FC02-10ER26017/DE-SC0005417]
FX This work is supported in part by the National Science Foundation under
   award IIP-1266400 and industrial members of the Center for Research in
   Storage Systems. A. Amer and D. D. E. Long were supported in part by the
   National Science Foundation under awards CCF-1219163 and CCF-1217648, by
   the Department of Energy under award DE-FC02-10ER26017/DE-SC0005417, by
   the industrial members of the Storage Systems Research Center, and by a
   gift from Wells Fargo.
CR Aghayev Abutalib., 2015, P 13 USENIX C FILE S, P135
   Amer Ahmed., 2010, Mass Storage Systems and Technologies (MSST), 2010 IEEE 26th Symposium on, P1, DOI DOI 10.1109/MSST.2010.5496991
   [Anonymous], P 7 USENIX C FIL STO
   [Anonymous], P 20 IEEE INT S MOD
   [Anonymous], P 20 IEEE INT S MOD
   [Anonymous], 2013, P 11 USENIX C FIL ST
   [Anonymous], J NAT DISASTERS
   [Anonymous], P WINT 1995 USENIX T
   Cassuto Y., 2010, Proceedings of the 2010 IEEE 26th Symposium on Mass Storage Systems and Technologies (MSST), P1, DOI DOI 10.1109/MSST.2010.5496971
   Desnoyers P, 2014, ACM T STORAGE, V10, DOI 10.1145/2577384
   Desnoyers Peter, 2013, P 5 USENIX WORKSH HO
   Hall D, 2012, IEEE T MAGN, V48, P1777, DOI 10.1109/TMAG.2011.2179528
   He W., 2014, Proceedings of the 6th USENIX conference on Hot Topics in Storage and File Systems, P5
   Jen-Wei Hsieh, 2006, ACM Transaction on Storage, V2, P22, DOI 10.1145/1138041.1138043
   Jin C, 2014, IEEE S MASS STOR SYS
   Le Moal D, 2012, 2012 IEEE INTERNATIONAL CONFERENCE ON CONSUMER ELECTRONICS (ICCE), P425, DOI 10.1109/ICCE.2012.6161799
   Matthews J. N., 1997, Operating Systems Review, V31, P238, DOI 10.1145/269005.266700
   Narayanan Dushyanth, 2008, P 6 USENIX C FIL STO, V17
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   Seltzer M., 1993, USENIX Association. Proceedings of the Winter 1993 USENIX Conference, P307
   Wang J, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE FAST'02 CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P47
   Wang WG, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P145
NR 22
TC 12
Z9 13
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JAN
PY 2016
VL 12
IS 1
SI SI
AR 2
DI 10.1145/2851505
PG 17
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DI5EI
UT WOS:000373521300002
OA Green Submitted, Bronze
DA 2024-07-18
ER

PT J
AU Huang, C
   Chen, MH
   Li, J
AF Huang, Cheng
   Chen, Minghua
   Li, Jin
TI Pyramid Codes: Flexible Schemes to Trade Space for Access Efficiency in
   Reliable Data Storage Systems
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Algorithms; Performance; Reliability; Storage; erasure codes;
   reconstruction; fault tolerance
AB We design flexible schemes to explore the tradeoffs between storage space and access efficiency in reliable data storage systems. Aiming at this goal, two new classes of erasure-resilient codes are introduced - Basic Pyramid Codes (BPC) and Generalized Pyramid Codes (GPC). Both schemes require slightly more storage space than conventional schemes, but significantly improve the critical performance of read during failures and unavailability. As a by-product, we establish a necessary matching condition to characterize the limit of failure recovery, that is, unless the matching condition is satisfied, a failure case is impossible to recover. In addition, we define a maximally recoverable (MR) property. For all ERC schemes holding the MR property, the matching condition becomes sufficient, that is, all failure cases satisfying the matching condition are indeed recoverable. We show that GPC is the first class of non-MDS schemes holding the MR property.
C1 [Chen, Minghua] Chinese Univ Hong Kong, Hong Kong, Hong Kong, Peoples R China.
C3 Chinese University of Hong Kong
EM cheng.huang@microsoft.com
RI Chen, Minghua/A-7476-2012
OI Chen, Minghua/0000-0003-4763-0037
FU China 973 Program [2012CB315904]; General Research Fund [411209, 411010,
   411011]; Area of Excellence Grant [AoE/E-02/08]; University Grant
   Committee of the Hong Kong SAR, China; Open Project of Shenzhen Key Lab
   of Cloud Computing Technology and Application; Microsoft; Cisco
FX M. Chen's research is partially supported by the China 973 Program
   (Project 2012CB315904), the General Research Fund (Projects 411209,
   411010, and 411011) and an Area of Excellence Grant (Project
   AoE/E-02/08), all established under the University Grant Committee of
   the Hong Kong SAR, China, as well as an Open Project of Shenzhen Key Lab
   of Cloud Computing Technology and Application, and two gift grants from
   Microsoft and Cisco.
CR [Anonymous], 1963, LOW DENSITY PARITY C
   Blahut R. E., 2003, ALGEBRAIC CODES FOR
   Bohra A., 2010, PROCEEDINGS OF THE U
   Bruck J., 2011, TECH REP ETR111
   Chansler R., 2010, PROCEEDINGS OF THE I
   Chen P., 1994, ACM COMPUT SURV
   Chen Y., 1999, PROCEEDINGS OF THE A
   Corbett P., 2004, PROCEEDINGS OF THE U
   Druschel P., 2005, PROCEEDINGS OF THE U
   Druschel P, 2001, PROCEEDINGS OF THE A
   Fikes A., 2010, Google Faculty Summit
   Gantenbein D., 2012, MICROSOFT RES FEATUR
   Gibson G. A., 2007, PROCEEDINGS OF THE U
   Greenan K. M., 2008, PROCEEDINGS OF THE U
   Grolimund D., 2007, PROCEEDINGS OF THE W
   Hamilton J., 2007, PROCEEDINGS OF THE C
   Hosekote D. K., 2007, PROCEEDINGS OF THE U
   Huang C., 2011, PROCEEDINGS OF THE U
   Huang C., 2012, PROCEEDINGS OF THE U
   Huang C., 2008, IEEE T COMPUTERS
   Ii W. V. C., 2005, PROCEEDINGS OF USENI
   Kalfane M., 1995, TECH REP TR 95 048
   Kling P., 2010, HDFS RAID HADOOP USE
   Kubiatowics J., 2001, PROCEEDINGS OF THE I
   Leung S.-T, 2003, PROCEEDINGS OF THE A
   Li J., 2011, PROCEEDINGS OF IEEE
   Li J., 2007, PROCEEDINGS OF THE I
   Li J., 2007, PROCEEDINGS OF THE I
   Long D. E., 2009, PROCEEDINGS OF THE I
   LUBY M, 2001, IEEE T INF THEORY
   Maccormick J., 2002, PROCEEDINGS OF THE U
   MacWilliams F. J., 1978, The Theory of Error-Correcting Codes
   Mazieres D., 2003, PROCEEDINGS OF THE I
   Mehrotra S., 2011, PROCEEDINGS OF THE A
   Menon J., 1995, IEEE T COMPUTERS
   Merchant A., 2004, P INT C ARCH SUPP PR
   Molnar D., 2000, PROCEEDINGS OF THE W
   Papailiopoulos D. S., 2012, PROCEEDINGS OF THE I
   Plank J. S., 2009, PROCEEDINGS OF THE I
   Plank J. S., 2008, PROCEEDINGS OF THE U
   Plank J. S., 1997, SOFTW PRACT EXPER
   Quinlan S., 2010, PROCEEDINGS OF THE U
   Ramchandran K., 2010, IEEE T INF THEORY
   Rao K., 2006, IBM TECH REP RJ10391
   Roth R. M, 1999, IEEE T INF THEORY
   Schrijver A., 2003, ALG COMBINATORICS
   Solomon G, 1960, J SOC INDUSTRIAL APP
   Suh C., 2011, IEEE T INF THEORY
   Tanner R. M., 1981, IEEE T INF THEORY
   Tomlin J. A., 2005, PROCEEDINGS OF THE U
   Ul Haq M. I., 2011, PROCEEDINGS OF THE A
   Welnicki M., 2009, PROCEEDINGS OF THE U
   Wylie J. J., 2010, PROCEEDINGS OF THE I
   Wylie J. J., 2008, PROCEEDINGS OF THE I
   Xu L., 2003, TECH REP
   Xu L., 2006, PROCEEDINGS OF THE I
   Xu L., 2005, P IEEE INT C DEP SYS
   Yekhanin S, 2011, PROCEEDINGS OF THE A
   Yekhanin S., 2012, PROCEEDINGS OF THE U
   Zhao B., 2000, PROCEEDINGS OF THE I
   Zhao B., 2003, PROCEEDINGS OF THE U
   Zhou B., 2008, PROCEEDINGS OF THE U
NR 62
TC 92
Z9 105
U1 1
U2 17
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAR
PY 2013
VL 9
IS 1
AR 3
DI 10.1145/2435204.2435207
PG 28
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 114PF
UT WOS:000316753700003
DA 2024-07-18
ER

PT J
AU Veeraraghavan, K
   Flinn, J
   Nightingale, EB
   Noble, B
AF Veeraraghavan, Kaushik
   Flinn, Jason
   Nightingale, Edmund B.
   Noble, Brian
TI quFiles: The Right File at the Right Time
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Performance; Context-aware file systems; data management;
   distributed storage; copy-on-write versions
AB A quFile is a unifying abstraction that simplifies data management by encapsulating different physical representations of the same logical data. Similar to a quBit (quantum bit), the particular representation of the logical data displayed by a quFile is not determined until the moment it is needed. The representation returned by a quFile is specified by a data-specific policy that can take context into account such as the application requesting the data, the device on which data is accessed, screen size, and battery status. We demonstrate the generality of the quFile abstraction by using it to implement six case studies: resource management, copy-on-write versioning, data redaction, resource-aware directories, application-aware adaptation, and platform-specific encoding. Most quFile policies were expressed using less than one hundred lines of code. Our experimental results show that, with caching and other performance optimizations, quFiles add less than 1% overhead to application-level file system.
C1 [Veeraraghavan, Kaushik; Flinn, Jason; Noble, Brian] Univ Michigan, Dept Elect Engn & Comp Sci, Ann Arbor, MI 48109 USA.
   [Nightingale, Edmund B.] Microsoft Res, Redmond, WA 98052 USA.
C3 University of Michigan System; University of Michigan; Microsoft
RP Veeraraghavan, K (corresponding author), Univ Michigan, Dept Elect Engn & Comp Sci, Ann Arbor, MI 48109 USA.
EM kaushikv@umich.edu; jflinn@umich.edu; ed.nightingale@microsoft.com;
   bnoble@umich.edu
FU NSF [CNS-0346686]
FX J. Flinn was supported by NSF CAREER award CNS-0346686. The views and
   conclusions contained in this document are those of the authors and
   should not be interpreted as representing the official policies, either
   expressed or implied, of NSF, the University of Michigan, Microsoft, or
   the U.S. government.
CR Anand Manish., 2003, MOBICOM 03, P176, DOI [http://doi.acm.org/10.1145/938985.939004, DOI 10.1145/938985.939004]
   [Anonymous], 2005, ACM Transactions on Storage
   Belaramani N, 2006, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD SYMPOSIUM ON NETWORKED SYSTEMS DESIGN & IMPLEMENTATION (NSDI 06), P59
   BERSHAD B. B., 1988, COMPUT SYST, V1, P2
   Bershad B. N., 1995, Operating Systems Review, V29, P267, DOI 10.1145/224057.224077
   Bila N, 2007, MOBISYS '07: PROCEEDINGS OF THE FIFTH INTERNATIONAL CONFERENCE ON MOBILE SYSTEMS, APPLICATIONS, AND SERVICES, P16
   BUNDLE, 2009, BUNDL PROGR GUID
   de Lara E, 2003, PROCEEDINGS OF MOBISYS 2003, P287
   de Lara E, 2001, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD USENIX SYMPOSIUM ON INTERNET TECHNOLOGIES AND SYSTEMS, P159
   Dourish P, 2000, ACM T INFORM SYST, V18, P140, DOI 10.1145/348751.348758
   Engler D. R., 1995, Operating Systems Review, V29, P251, DOI 10.1145/224057.224076
   Flinn J, 1999, OPERATING SYSTEMS REVIEW, VOL 33, NO 5, DECEMBER 1999, P48, DOI 10.1145/319344.319155
   Fox A, 1996, ACM SIGPLAN NOTICES, V31, P160, DOI 10.1145/248209.237177
   *FUSE, 2009, FIL US
   Gehani N.H., 1994, Proceedings of the 20th International Conference on Very Large Data Bases, VLDB'94, P249
   Gifford D. K., 1991, Operating Systems Review, V25, P16, DOI 10.1145/121133.121138
   GNUCASH, 2009, GNUCASH FREE ACC SOF
   *GOOGL, GOOGL DESKT
   Gupta A., 1995, IEEE DATA ENG B, V18, P3
   HOWARD JH, 1988, ACM T COMPUT SYST, V6, P51, DOI 10.1145/35037.35059
   IOANNIDIS S., 2006, P 1 C USENIX WORKSH, P45
   KISTLER JJ, 1992, ACM T COMPUT SYST, V10, P3, DOI 10.1145/146941.146942
   Kjær KE, 2007, PROCEEDINGS OF THE IASTED INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING, P148
   Lopresti D, 2005, P SOC PHOTO-OPT INS, V5676, P183, DOI 10.1117/12.589552
   Love R., 2005, Linux Journal, V2005, P8
   Narayanan D, 2000, THIRD IEEE WORKSHOP ON MOBILE COMPUTING SYSTEMS AND APPLICATIONS, PROCEEDINGS, P31, DOI 10.1109/MCSA.2000.895379
   Nicholson AJ, 2008, MOBICOM'08: PROCEEDINGS OF THE FOURTEENTH ACM INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P46, DOI 10.1145/1409944.1409952
   Nightingale EB, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE SIXTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDE '04), P363
   Noble B. D., 1997, Operating Systems Review, V31, P276, DOI 10.1145/269005.266708
   *NOK, NOK EN PROF
   Peek D, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 2007 USENIX ANNUAL TECHNICAL CONFERENCE, P115
   Peek D, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P219
   Phan T., 2004, P 2 INT C MOBILE SYS, P139
   PILLAI P, 2004, P ACM 2 INT WORKSH V, P72, DOI DOI 10.1145/1026799.1026812
   Ramasubramanian Venugopalan., 2009, NSDV09 P 6 USENIX S, P261
   REDDY A. L. N., 2003, IEEE T PARALL DISTR, V14, P993
   RUSSINOVICH M. E., 2005, MICROSOFT WINDOWS IN, P719
   Salmon Brandon., 2009, FAST 09 P 7 C FILE S, P167
   Santry DS, 1999, OPERATING SYSTEMS REVIEW, VOL 33, NO 5, DECEMBER 1999, P110, DOI 10.1145/319344.319159
   Schilit B., 1994, 1994 1 WORKSHOP MOBI, P85, DOI [DOI 10.1109/WMCSA.1994.16, 10.1109/WMCSA.1994.16]
   Seltzer MI, 1996, PROCEEDINGS OF THE SECOND SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '96), P213, DOI 10.1145/248155.238779
   SHEPLER S, 2003, 3530 RFC IETF
   Sohn T., 2006, 28th International Conference on Software Engineering Proceedings, P462, DOI 10.1145/1134285.1134351
   SPOTLIGHT, 2007, SPOTL OV
   WINDOWS DESKTOP SEARCH, 2008, WINDOWS DESKTOP SEAR
   XERCES, 2009, XERC C XML PARS
   Yumerefendi A., 2007, P 4 S NETWORKED SYST, P159
NR 47
TC 3
Z9 3
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD SEP
PY 2010
VL 6
IS 3
AR 12
DI 10.1145/1837915.1837920
PG 28
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA V24QL
UT WOS:000208424800005
DA 2024-07-18
ER

PT J
AU Bhimani, J
   Yang, ZY
   Yang, JP
   Maruf, A
   Mi, NF
   Pandurangan, R
   Choi, CH
   Balakrishnan, V
AF Bhimani, Janki
   Yang, Zhengyu
   Yang, Jingpei
   Maruf, Adnan
   Mi, Ningfang
   Pandurangan, Rajinikanth
   Choi, Changho
   Balakrishnan, Vijay
TI Automatic Stream Identification to Improve Flash Endurance in Data
   Centers
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Solid state drives; multi-streaming; write amplification factor; I/O
   stream detection; coherency; I/O workload characterization; NAND flash
   endurance
ID TRANSLATION
AB The demand for high performance I/O in Storage-as-a-Service (SaaS) is increasing day by day. To address this demand, NAND Flash-based Solid-state Drives (SSDs) are commonly used in data centers as cache- or top-tiers in the storage rack ascribe to their superior performance compared to traditional hard disk drives (HDDs). Meanwhile, with the capital expenditure of SSDs declining and the storage capacity of SSDs increasing, all-flash data centers are evolving to serve cloud services better than SSD-HDD hybrid data centers. During this transition, the biggest challenge is how to reduce the Write Amplification Factor (WAF) as well as to improve the endurance of SSD since this device has a limited program/erase cycles. A specified case is that storing data with different lifetimes (i.e., I/O streams with similar temporal fetching patterns such as reaccess frequency) in one single SSD can cause high WAF, reduce the endurance, and downgrade the performance of SSDs. Motivated by this, multi-stream SSDs have been developed to enable data with a different lifetime to be stored in different SSD regions. The logic behind this is to reduce the internal movement of data-when garbage collection is triggered, there are high chances of having data blocks with either all the pages being invalid or valid. However, the limitation of this technology is that the system needs to manually assign the same streamID to data with a similar lifetime. Unfortunately, when data arrives, it is not known how important this data is and how long this data will stay unmodified. Moreover, according to our observation, with different definitions of a lifetime (i.e., different calculation formulas based on selected features previously exhibited by data, such as sequentiality, and frequency), streamID identification may have varying impacts on the final WAF of multi-stream SSDs. Thus, in this article, we first develop a portable and adaptable framework to study the impacts of different workload features and their combinations on write amplification. We then propose a feature-based stream identification approach, which automatically co-relates the measurable workload attributes (such as I/O size, I/O rate, and so on.) with high-level workload features (such as frequency, sequentiality, and so on.) and determines a right combination of workload features for assigning streamIDs. Finally, we develop an adaptable stream assignment technique to assign streamID for changing workloads dynamically. Our evaluation results show that our automation approach of stream detection and separation can effectively reduce the WAF by using appropriate features for stream assignment with minimal implementation overhead.
C1 [Bhimani, Janki; Maruf, Adnan] Florida Int Univ, 11200 SW 8th St,CASE 238B, Miami, FL 33199 USA.
   [Yang, Zhengyu; Yang, Jingpei; Pandurangan, Rajinikanth; Choi, Changho; Balakrishnan, Vijay] Samsung Semicond Inc, San Jose, CA USA.
   [Mi, Ningfang] Northeastern Univ, Boston, MA 02115 USA.
   [Yang, Zhengyu] 10100 Venice Blvd, Culver City, CA 90232 USA.
   [Yang, Jingpei] 1600 Amphitheatre Pkwy, Mountain View, CA 94043 USA.
   [Mi, Ningfang] 409 Dana Res Bldg,360 Huntington Ave, Boston, MA 02115 USA.
   [Pandurangan, Rajinikanth] 110 Holger Way, San Jose, CA 95134 USA.
   [Choi, Changho] 3655 N First St, San Jose, CA 95134 USA.
C3 State University System of Florida; Florida International University;
   Samsung Electronics; Samsung Semiconductor (SSI); Northeastern
   University
RP Bhimani, J (corresponding author), Florida Int Univ, 11200 SW 8th St,CASE 238B, Miami, FL 33199 USA.
EM jbhimani@fiu.edu; Yangzy1988@gmail.com; jingpei@google.com;
   amaruf@fiu.edu; ningfang@ece.neu.edu; rajinikanth.p@gmail.com;
   changho.c@samsung.com
RI Yang, Zhengyu/I-2447-2019
OI Yang, Zhengyu/0000-0002-9822-5843
FU National Science Foundation [CNS-1452751, CNS-2008324, CNS-2008072];
   Samsung Semiconductor Inc.
FX This work was partially supported by National Science Foundation Awards
   CNS-2008324 and CNS-2008072, the National Science Foundation Career
   Award CNS-1452751, and the Samsung Semiconductor Inc. Research Grant.
CR Agrawal Nitin, 2008, P USENIX ANN TECHN C, P57
   [Anonymous], SNIA IOTTA REPOSITOR
   [Anonymous], 2020, PERFORMANCE ENDURANC
   [Anonymous], 2012, IEEE Data Eng. Bull.
   [Anonymous], 2020, UMASS TRACE REPOSITO
   [Anonymous], 2020, MULTISTREAM TECHNOLO
   [Anonymous], FIO FLEXIBLE IO BENC
   [Anonymous], 2020, SYSTEMD
   Bae Jinwook, 2019, [IEMEK Journal of Embedded Systems and Applications, 대한임베디드공학회논문지], V14, P71, DOI 10.14372/IEMEK.2019.14.2.71
   Benesty J, 2009, SPRINGER TOP SIGN PR, V2, P37, DOI 10.1007/978-3-642-00296-0_5
   Bhimani J., 2016, P 35 IEEE INT PERFOR
   Bhimani J, 2015, IEEE HIGH PERF EXTR
   Bhimani Janki S., 2017, US Patent App., Patent No. [15/344,422, 15344422]
   Campos D, 2015, PROCEEDINGS OF THE 17TH BRAZILIAN SYMPOSIUM ON SOFTWARE QUALITY (SBQS), P151, DOI 10.1145/3275245.3275261
   Cho H, 2009, DES AUT TEST EUROPE, P507
   Choi Changho, 2020, MULTI STREAMWRITE SS
   Desnoyers Peter., 2012, P INT SYSTEMS STORAG, P12
   DuBois Paul, 1999, MySQL
   Fischer Stephen G, 2018, US Patent App., Patent No. [15/821,708, 15821708]
   Gessert F, 2017, COMPUT SCI-RES DEV, V32, P353, DOI 10.1007/s00450-016-0334-3
   Hingkwan HUEN, 2019, US Patent App, Patent No. [16/219,936, 16219936]
   Hingkwan HUEN, 2020, US Patent, Patent No. [10,592,171, 10592171]
   Kang Jeong-Uk, 2014, P 6 USENIX WORKSHOP
   Kim J, 2002, IEEE T CONSUM ELECTR, V48, P366
   Kim T, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P295
   Kim Taejin, 2018, P 10 USENIX WORKSHOP
   Kuzmin Andrey V., 2016, US Patent, Patent No. [9,229,854, 9229854]
   Lee SW, 2007, ACM T EMBED COMPUT S, V6, DOI 10.1145/1275986.1275990
   Luo YX, 2015, IEEE S MASS STOR SYS
   Menon P, 2014, PROC INT CONF DATA, P1162, DOI 10.1109/ICDE.2014.6816732
   Min Changwoo, 2012, FAST, V12, P1
   Naeini MP, 2018, KNOWL INF SYST, V54, P151, DOI 10.1007/s10115-017-1133-2
   Narayanan D, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P253
   Park HW, 2019, 15TH INTERNATIONAL WORKSHOP ON DATA MANAGEMENT ON NEW HARDWARE (DAMON 2019), DOI 10.1145/3329785.3329935
   Sungjin Lee, 2008, Operating Systems Review, V42, P36, DOI 10.1145/1453775.1453783
   Wei Xie, 2016, 2016 IEEE International Conference on Networking, Architecture and Storage (NAS), P1, DOI 10.1109/NAS.2016.7549413
   Yang F, 2015, IEEE 12TH INT CONF UBIQUITOUS INTELLIGENCE & COMP/IEEE 12TH INT CONF ADV & TRUSTED COMP/IEEE 15TH INT CONF SCALABLE COMP & COMMUN/IEEE INT CONF CLOUD & BIG DATA COMP/IEEE INT CONF INTERNET PEOPLE AND ASSOCIATED SYMPOSIA/WORKSHOPS, P1062, DOI 10.1109/UIC-ATC-ScalCom-CBDCom-IoP.2015.197
   Yang Fei, 2015, P NONVOLATILE MEMORI
   Yang JP, 2017, SYSTOR'17: PROCEEDINGS OF THE 10TH ACM INTERNATIONAL SYSTEMS AND STORAGE CONFERENCE, DOI 10.1145/3078468.3078469
   Yang Pan, 2019, P 11 USENIX WORKSH H
   Yang Zhengyu, 2016, P 2016 IEEE 8 INT C
   Yong Hwanjin, 2018, P 10 USENIX WORKSHOP
   Zhang PY, 2020, ELECTRON LETT, V56, P278, DOI 10.1049/el.2019.3526
NR 43
TC 0
Z9 0
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAY
PY 2022
VL 18
IS 2
AR 17
DI 10.1145/3470007
PG 29
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 1H3XN
UT WOS:000796478700008
DA 2024-07-18
ER

PT J
AU Hu, YG
   Zhu, ZT
   Neal, I
   Kwon, Y
   Cheng, TY
   Chidambaram, V
   Witchel, E
AF Hu, Yige
   Zhu, Zhiting
   Neal, Ian
   Kwon, Youngjin
   Cheng, Tianyu
   Chidambaram, Vijay
   Witchel, Emmett
TI TxFS: Leveraging File-system Crash Consistency to Provide ACID
   Transactions
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article; Proceedings Paper
CT USENIX Annual Technical Conference (ATC)
CY JUL 11-13, 2018
CL Boston, MA
SP USENIX, Facebook, NSF, NetApp, Oracle, Vmware, AWS, IBM, Google, Microsoft, Private Internet Access
DE Operating systems; file systems; crash consistency; ACID transactions
AB We introduce TxFS, a transactional file system that builds upon a file system's atomic-update mechanism such as journaling. Though prior work has explored a number of transactional file systems, TxFS has a unique set of properties: a simple API, portability across different hardware, high performance, low complexity (by building on the file-system journal), and full ACID transactions. We port SQLite, OpenLDAP, and Git to use TxFS and experimentally show that TxFS provides strong crash consistency while providing equal or better performance.
C1 [Hu, Yige; Zhu, Zhiting; Chidambaram, Vijay; Witchel, Emmett] Univ Texas Austin, Comp Sci Dept, 2317 Speedway 2-302, Austin, TX 78712 USA.
   [Cheng, Tianyu] 320 South Capital Highway Texas, Austin, TX USA.
   [Neal, Ian] Univ Michigan, Comp Sci & Engn, Bob & Betty Beyster Bldg,2260 Hayward St, Ann Arbor, MI 48109 USA.
   [Kwon, Youngjin] Korea Adv Inst Sci & Technol, 291 Daehak Ro, Daejeon 34141, South Korea.
C3 University of Texas System; University of Texas Austin; University of
   Michigan System; University of Michigan; Korea Advanced Institute of
   Science & Technology (KAIST)
RP Hu, YG (corresponding author), Univ Texas Austin, Comp Sci Dept, 2317 Speedway 2-302, Austin, TX 78712 USA.
EM yige@cs.utexas.edu; zhitingz@cs.utexas.edu; ian.gl.neal@gmail.com;
   yjkwon@kaist.ac.kr; tianyu.cheng@utexas.edu; vijay@cs.utexas.edu;
   witchel@cs.utexas.edu
RI Kwon, Youngjin/AAE-6378-2021; Chidambaram, Vijay/HJA-2695-2022
OI Chidambaram, Vijay/0000-0001-7985-6087; , Zhiting/0009-0005-9598-3875;
   Neal, Ian/0000-0001-9721-781X
FU NSF [CNS-1618563, CCF-1333594]
FX This work is supported in part by NSF (CNS-1618563, CCF-1333594) and by
   generous donations from VMware, Facebook, and Google.
CR [Anonymous], 2013, EUROSYS 13
   [Anonymous], 2008, P 8 USENIX C OP SYST
   [Anonymous], 2006, FSYNC MAN PAGE
   [Anonymous], 2015, Proceedings of the 13th USENIX Conference on File and Storage Technologies
   Chidambaram V, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P228, DOI 10.1145/2517349.2522726
   Coburn J, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P197, DOI 10.1145/2517349.2522724
   Gehani N.H., 1994, Proceedings of the 20th International Conference on Very Large Data Bases, VLDB'94, P249
   Gray J., 1981, Proceedings of the Seventh International Conference on Very Large Data Bases, P144
   Gray J. N., 1976, Proceedings of the IFIP Working Conference on Modelling in Data Base Management Systems, P365
   Hagmann Robert, 1987, P 11 ACM S OP SYST P, V21
   Hamano Tsukasa, LB LDAP BENCHMARKING
   Harter T, 2012, ACM T COMPUT SYST, V30, DOI 10.1145/2324876.2324878
   HITZ D., 1994, P USENIX WINT TECHN
   Hu YG, 2017, PROCEEDINGS OF THE 16TH WORKSHOP ON HOT TOPICS IN OPERATING SYSTEMS (HOTOS 2017), P100, DOI 10.1145/3102980.3102997
   Hu YG, 2018, PROCEEDINGS OF THE 2018 USENIX ANNUAL TECHNICAL CONFERENCE, P879
   Iyer Sitaram, 2001, P 18 ACM S OP SYST P
   Martinez Ashlie, 2017, P 9 USENIX WORKSH HO
   Microsoft, 2018, ALT US TRANS NTFS
   Min C., 2015, Usenix Atc'15, P221
   Mohan J, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P33
   Murphy Nick, 2001, DESIGN IMPLEMENTATIO
   Nambiar R, 2014, LECT NOTES COMPUT SC, V8391, P1, DOI 10.1007/978-3-319-04936-6_1
   Olson M. A., 1993, USENIX Association. Proceedings of the Winter 1993 USENIX Conference, P205
   Olson MA, 1999, PROCEEDINGS OF THE FREENIX TRACK, P183
   Pillai Thanumalayan Sankaranarayana, 2014, OSDI, P433
   Porter DE, 2009, SOSP'09: PROCEEDINGS OF THE TWENTY-SECOND ACM SIGOPS SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P161
   Purohith D, 2017, PROCEEDINGS OF THE 8TH ASIA-PACIFIC WORKSHOP ON SYSTEMS (APSYS '17), DOI 10.1145/3124680.3124719
   Russinovich M.E., 2005, Microsoft Windows internals: Microsoft Windows Server 2003, Windows XP, and Windows 2000, V4
   Sct S.T., 1998, Journaling the linux ext2fs filesystem
   Shen K., 2014, Proceedings of the 12th USENIX Conference on File and Storage Technologies FAST 14, P287
   Shin Ji-Yong, 2016, P 14 USENIX C FIL ST
   Spahn R., 2014, P 11 USENIX C OP SYS, P113
   Spillane R. P., 2009, P USENIX C FIL STOR, V9, P29
   SQLite, SQLITE TRANS SQL DAT
   Wheeler David., SLOCCount
   Wright CharlesP., 2007, ACM T STORAGE, V3, P4
NR 36
TC 9
Z9 11
U1 1
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUN
PY 2019
VL 15
IS 2
AR 9
DI 10.1145/3318159
PG 20
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA JL3IK
UT WOS:000495424300002
OA Bronze
DA 2024-07-18
ER

PT J
AU Zhang, Y
   Zhao, J
   Liao, XF
   Jin, H
   Gu, L
   Liu, HK
   He, BS
   He, LG
AF Zhang, Yu
   Zhao, Jin
   Liao, Xiaofei
   Jin, Hai
   Gu, Lin
   Liu, Haikun
   He, Bingsheng
   He, Ligang
TI CGraph: A Distributed Storage and Processing System for Concurrent
   Iterative Graph Analysis Jobs
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article; Proceedings Paper
CT USENIX Annual Technical Conference (ATC)
CY JUL 11-13, 2018
CL Boston, MA
SP USENIX, Facebook, NSF, NetApp, Oracle, Vmware, AWS, IBM, Google, Microsoft, Private Internet Access
DE Data access correlations; data access cost; throughput
ID COMPUTATION; ANALYTICS; FRAMEWORK
AB Distributed graph processing platforms usually need to handle massive Concurrent iterative Graph Processing (CGP) jobs for different purposes. However, existing distributed systems face high ratio of data access cost to computation for the CGP jobs, which incurs low throughput. We observed that there are strong spatial and temporal correlations among the data accesses issued by different CGP jobs, because these concurrently running jobs usually need to repeatedly traverse the shared graph structure for the iterative processing of each vertex. Based on this observation, this article proposes a distributed storage and processing system CGraph for the CGP jobs to efficiently handle the underlying static/evolving graph for high throughput. It uses a data-centric load-trigger-pushing model, together with several optimizations, to enable the CGP jobs to efficiently share the graph structure data in the cache/memory and their accesses by fully exploiting such correlations, where the graph structure data is decoupled from the vertex state associated with each job. It can deliver much higher throughput for the CGP jobs by effectively reducing their average ratio of data access cost to computation. Experimental results show that CGraph improves the throughput of the CGP jobs by up to 3.47x in comparison with existing solutions on distributed platforms.
C1 [Zhang, Yu; Zhao, Jin; Liao, Xiaofei; Jin, Hai; Gu, Lin; Liu, Haikun] Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Natl Engn Res Ctr Big Data Technol & Syst, Serv Comp Technol & Syst Lab,Cluster & Grid Comp, Wuhan 430074, Hubei, Peoples R China.
   [He, Bingsheng] Natl Univ Singapore, Dept Comp Sci, Singapore, Singapore.
   [He, Ligang] Univ Warwick, Dept Comp Sci, Coventry, W Midlands, England.
C3 Huazhong University of Science & Technology; National University of
   Singapore; University of Warwick
RP Liao, XF (corresponding author), Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Natl Engn Res Ctr Big Data Technol & Syst, Serv Comp Technol & Syst Lab,Cluster & Grid Comp, Wuhan 430074, Hubei, Peoples R China.
EM zhyu@hust.edu.cn; zjin@hust.edu.cn; xfliao@hust.edu.cn;
   hjin@hust.edu.cn; anheeno@gmail.com; hkliu@hust.edu.cn;
   hebs@comp.nus.edu.sg; ligang.he@warwick.ac.uk
RI Liu, Haikun/JVE-2305-2024
OI He, Bingsheng/0000-0001-8618-4581
FU National Key Research and Development Program of China [2018YFB1003500];
   National Natural Science Foundation of China [61832006, 61825202,
   61702202]
FX This work is supported by National Key Research and Development Program
   of China under grant No. 2018YFB1003500, National Natural Science
   Foundation of China under grant No. 61832006, 61825202, 61702202.
CR Ammar K, 2018, PROC VLDB ENDOW, V11, P1151, DOI 10.14778/3231751.3231764
   [Anonymous], 2012, P 3 ACM S CLOUD COMP
   [Anonymous], 2015, P VLDB ENDOW
   Baluja Shumeet, 2008, P 17 INT C WORLD WID, P895, DOI DOI 10.1145/1367497.1367618
   Bornea MA., 2013, P 2013 ACM SIGMOD IN, P121, DOI DOI 10.1145/2463676.2463718
   Bronson N., 2013, P 2013 USENIX C ANN, P49
   Bu YY, 2014, PROC VLDB ENDOW, V8, P161, DOI 10.14778/2735471.2735477
   Bu YY, 2010, PROC VLDB ENDOW, V3, P285
   Buluc Aydin, 2011, P 2011 INT C HIGH PE, P1
   Chen HH, 2017, SCI CHINA INFORM SCI, V60, DOI 10.1007/s11432-016-5551-7
   Chen Rishan, 2012, P 26 INT S HIGH PERF
   Cheng JF, 2015, PROC INT CONF DATA, P1131, DOI 10.1109/ICDE.2015.7113362
   Yan D, 2014, PROC VLDB ENDOW, V7, P1981, DOI 10.14778/2733085.2733103
   Ekanayake J., 2010, Proc. the 19th ACM International Symposium on High Performance Distributed Computing (HDPC'10), P810, DOI DOI 10.1145/1851476.1851593
   Gonzalez Joseph E., 2012, 10 USENIX S OP SYST, P17
   Guo Z, 2011, EXPERIMENT AND CALCULATION OF REINFORCED CONCRETE AT ELEVATED TEMPERATURES, P1
   Gupta S, 2017, PROC VLDB ENDOW, V10, P757, DOI 10.14778/3067421.3067425
   Han MY, 2015, PROC VLDB ENDOW, V8, P950, DOI 10.14778/2777598.2777604
   He B., 2010, P 1 ACM S CLOUD COMP, P63, DOI DOI 10.1145/1807128.1807139
   Hong SP, 2013, INT CONF HIGH PERFOR, DOI 10.1145/2503210.2503246
   Hu C, 2016, IEEE T KNOWL DATA EN, V28, P1635, DOI 10.1109/TKDE.2016.2538223
   José RS, 2014, SPRINGER PR COMPLEX, P599, DOI 10.1007/978-3-319-04379-1__99
   Ju XE, 2016, PROCEEDINGS OF USENIX ATC '16: 2016 USENIX ANNUAL TECHNICAL CONFERENCE, P523
   Jun SW, 2018, CONF PROC INT SYMP C, P411, DOI 10.1109/ISCA.2018.00042
   Kefalas P, 2016, IEEE T KNOWL DATA EN, V28, P604, DOI 10.1109/TKDE.2015.2496344
   Ko S, 2018, INT CONF MANAGE DATA, P395, DOI 10.1145/3183713.3196915
   Kourtellis N, 2015, IEEE T KNOWL DATA EN, V27, P2494, DOI 10.1109/TKDE.2015.2419666
   Kumar P, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P249
   Low Y, 2012, PROC VLDB ENDOW, V5, P716, DOI 10.14778/2212351.2212354
   Ma LX, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P195
   Malewicz Grzegorz, 2010, P ACM SIGMOD INT C M, P135, DOI [DOI 10.1145/1807167.1807184, 10.1145/1807167.1807184]
   Malicevic J, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P631
   Martella C, 2017, PROC INT CONF DATA, P1083, DOI 10.1109/ICDE.2017.153
   Meyer U, 2001, SIAM PROC S, P797
   Mukkara Anurag, 2018, 2018 51st Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), P1, DOI 10.1109/MICRO.2018.00010
   Page L., 1998, P 7 INT WORLD WID WE, P161
   Roy A, 2015, SOSP'15: PROCEEDINGS OF THE TWENTY-FIFTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P410, DOI 10.1145/2815400.2815408
   Run Chen, 2015, 2015 IEEE International Solid-State Circuits Conference (ISSCC). Digest of Technical Papers, P1, DOI 10.1109/ISSCC.2015.7063068
   Sheng F, 2018, PROCEEDINGS OF THE 2018 ACM SYMPOSIUM ON CLOUD COMPUTING (SOCC '18), P301, DOI 10.1145/3267809.3267811
   Shi JX, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P317
   Song JS, 2018, IEEE T KNOWL DATA EN, V30, P992, DOI 10.1109/TKDE.2017.2779126
   Vaquero LM, 2014, INT CON DISTR COMP S, P144, DOI 10.1109/ICDCS.2014.23
   Verma S, 2017, PROC VLDB ENDOW, V10, P493, DOI 10.14778/3055540.3055543
   Vora Keval., 2017, Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS '17, P223
   Wang K, 2017, TWENTY-SECOND INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXII), P389, DOI 10.1145/3037697.3037744
   Wang SY, 2018, PROCEEDINGS OF THE 2018 USENIX ANNUAL TECHNICAL CONFERENCE, P651
   Wu M, 2015, ACM SoCC'15: Proceedings of the Sixth ACM Symposium on Cloud Computing, P408, DOI 10.1145/2806777.2806849
   Xie CN, 2015, ACM SIGPLAN NOTICES, V50, P194, DOI [10.1145/2688500.2688508, 10.1145/2858788.2688508]
   Xue J., 2014, P 23 INT S HIGH PERF, P227
   Xue JL, 2017, IEEE T COMPUT, V66, P876, DOI 10.1109/TC.2016.2618923
   Yuan PP, 2014, INT CONF HIGH PERFOR, P401, DOI 10.1109/SC.2014.38
   Zhang YX, 2020, ASSESS EVAL HIGH EDU, V45, P54, DOI 10.1080/02602938.2019.1608504
   Zhang Y, 2018, PROCEEDINGS OF THE 2018 USENIX ANNUAL TECHNICAL CONFERENCE, P441
   Zhu XW, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P301
NR 54
TC 9
Z9 9
U1 1
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUN
PY 2019
VL 15
IS 2
AR 10
DI 10.1145/3319406
PG 26
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA JL3IK
UT WOS:000495424300003
DA 2024-07-18
ER

PT J
AU Yang, Y
   Zhu, JW
AF Yang, Yue
   Zhu, Jianwen
TI Write Skew and Zipf Distribution: Evidence and Implications
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Flash memory; workloads; write traffic; Zipf's law
AB Understanding workload characteristics is essential to storage systems design and performance optimization. With the emergence of flash memory as a new viable storage medium, the new design concern of flash endurance arises, necessitating a revisit of workload characteristics, in particular, of the write behavior. Inspired by Web caching studies where a Zipf-like access pattern is commonly found, we hypothesize that write count distribution at the block level may also follow Zipf's Law. To validate this hypothesis, we study 48 block I/O traces collected from a wide variety of real and benchmark applications. Through extensive analysis, we demonstrate that the Zipf-like pattern indeed widely exists in write traffic provided its disguises are removed by statistical processing. This finding implies that write skew in a large class of applications could be analytically expressed and, thus, facilitates design tradeoff explorations adaptive to workload characteristics.
C1 [Yang, Yue] Univ Toronto, Dept Elect & Comp Engn, 10 Kings Coll Rd, Toronto, ON M5S 3G4, Canada.
   [Zhu, Jianwen] Univ Toronto, Dept Elect & Comp Engn, EA312, 10 Kings Coll Rd, Toronto, ON M5S 3G4, Canada.
C3 University of Toronto; University of Toronto
RP Yang, Y (corresponding author), Univ Toronto, Dept Elect & Comp Engn, 10 Kings Coll Rd, Toronto, ON M5S 3G4, Canada.
EM yyang@eecg.toronto.edu; jzhu@eecg.toronto.edu
CR Agrawal Nitin, 2008, P USENIX ANN TECHN C, P57
   Arlitt MF, 1997, IEEE ACM T NETWORK, V5, P631, DOI 10.1109/90.649565
   Axboe J., 2014, Fio-Flexible Io Tester
   Blasius B, 2009, PHYS REV LETT, V103, DOI 10.1103/PhysRevLett.103.218701
   Breslau L, 1999, IEEE INFOCOM SER, P126, DOI 10.1109/INFCOM.1999.749260
   Bux W, 2010, PERFORM EVALUATION, V67, P1172, DOI 10.1016/j.peva.2010.07.003
   Cha M, 2007, IMC'07: PROCEEDINGS OF THE 2007 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P1
   Chang LP, 2009, ACM T DES AUTOMAT EL, V15, DOI 10.1145/1640457.1640463
   Chenier F., 2011, Proc of the 12th International Conference on Rehabilitation Robotics (ICORR), P1
   Chesire M, 2001, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD USENIX SYMPOSIUM ON INTERNET TECHNOLOGIES AND SYSTEMS, P1
   Chlebus E, 2009, APPL MATH LETT, V22, P732, DOI 10.1016/j.aml.2008.07.007
   Cirillo P, 2013, PHYSICA A, V392, P5947, DOI 10.1016/j.physa.2013.07.061
   Clauset A, 2009, SIAM REV, V51, P661, DOI 10.1137/070710111
   Cunha Carlos R., 1995, BUCS95010
   Desnoyers P, 2014, ACM T STORAGE, V10, DOI 10.1145/2577384
   Desnoyers Peter., 2012, P INT SYSTEMS STORAG, P12
   ETW, 2012, ETW EV TRAC WIND
   Gill P, 2007, IMC'07: PROCEEDINGS OF THE 2007 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P15
   Guo L, 2008, PODC'08: PROCEEDINGS OF THE 27TH ANNUAL ACM SYMPOSIUM ON PRINCIPLES OF DISTRIBUTED COMPUTING, P283, DOI 10.1145/1400751.1400789
   Gupta Ashish, 2011, Platelets, DOI 10.3109/09537104.2010.547958
   Hongliang Yu, 2006, Operating Systems Review, V40, P333, DOI 10.1145/1218063.1217968
   Jen-Wei Hsieh, 2006, ACM Transaction on Storage, V2, P22, DOI 10.1145/1138041.1138043
   Kavalanekar S., 2008, P 4 INT S WORKL CHAR
   Kotera Isao., 2008, MEDEA '08: Proceedings of the 9th workshop on MEmory performance, P9
   Lee J., P 6 ANN INT SYSTEMS, DOI DOI 10.1145/2485732.2485745
   Microsoft News Center, 2013, BIG BANG BIG DAT EXP
   Narayanan Dushyanth, 2008, ACM T STORAGE
   Newman MEJ, 2005, CONTEMP PHYS, V46, P323, DOI 10.1080/00107510500052444
   Park C, 2008, ACM T EMBED COMPUT S, V7, DOI 10.1145/1376804.1376806
   Park Dongchul, 2011, P IEEE 27 S MASS STO, P1
   Riska A, 2006, USENIX ASSOCIATION PROCEEDINGS OF THE 2006 USENIX ANNUAL TECHNICAL CONFERENCE, P97
   Storage Networking Industry Association, 2011, IOTTA REP
   Storage Performance Council, 2009, SPC BENCHM 2C SPC 2C
   Storage Performance Council, 2002, OLTP APPL I O
   TANG W., 2003, Proceedings of the 13th international workshop on Network and operating systems support for digital audio and video, P12
   Turner Vernon., 2014, IDC REPORT
   Van Houdt Benny, 2013, Performance Evaluation Review, V41, P191
   Williams A, 2005, WEB INF SYST ENG INT, V2, P3, DOI 10.1007/0-387-27727-7_1
   Yang Yue, 2014, P 30 INT C MASS STOR
   Zipf GK, 1950, J CLIN PSYCHOL, V6, P306
NR 40
TC 36
Z9 44
U1 1
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2016
VL 12
IS 4
AR 21
DI 10.1145/2908557
PG 19
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DV2OQ
UT WOS:000382760500004
DA 2024-07-18
ER

PT J
AU Wildani, A
   Miller, EL
AF Wildani, Avani
   Miller, Ethan L.
TI Can We Group Storage? Statistical Techniques to Identify Predictive
   Groupings in Storage System Accesses
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Algorithms; Measurement; Performance; Data layout; storage
   optimization; tiered storage; predictive modeling
AB Storing large amounts of data for different users has become the new normal in a modern distributed cloud storage environment. Storing data successfully requires a balance of availability, reliability, cost, and performance. Typically, systems design for this balance with minimal information about the data that will pass through them. We propose a series of methods to derive groupings from data that have predictive value, informing layout decisions for data on disk.
   Unlike previous grouping work, we focus on dynamically identifying groupings in data that can be gathered from active systems in real time with minimal impact using spatiotemporal locality. We outline several techniques we have developed and discuss how we select particular techniques for particular workloads and application domains. Our statistical and machine-learning-based grouping algorithms answer questions such as "What can a grouping be based on?" and "Is a given grouping meaningful for a given application?" We design our models to be flexible and require minimal domain information so that our results are as broadly applicable as possible. We intend for this work to provide a launchpad for future specialized system design using groupings in combination with caching policies and architectural distinctions such as tiered storage to create the next generation of scalable storage systems.
C1 [Wildani, Avani] Salk Inst Biol Studies, San Diego, CA 92138 USA.
   [Miller, Ethan L.] Univ Calif Santa Cruz, Dept Comp Sci, Baskin Sch Engn, 1156 High St,MS SOE3, Santa Cruz, CA 95064 USA.
   [Wildani, Avani] Emory Univ, Dept Math & CS, 400 Dowman Dr,W401, Atlanta, GA 30322 USA.
C3 Salk Institute; University of California System; University of
   California Santa Cruz; Emory University
RP Wildani, A (corresponding author), Salk Inst Biol Studies, San Diego, CA 92138 USA.; Miller, EL (corresponding author), Univ Calif Santa Cruz, Dept Comp Sci, Baskin Sch Engn, 1156 High St,MS SOE3, Santa Cruz, CA 95064 USA.; Wildani, A (corresponding author), Emory Univ, Dept Math & CS, 400 Dowman Dr,W401, Atlanta, GA 30322 USA.
EM avani@mathcs.emory.edu; elm@soe.ucsc.edu
OI Wildani, Avani/0000-0001-9457-8863; Miller, Ethan/0000-0003-2994-9060
FU National Science Foundation [CNS-0917396, IIP-0934401]; Department of
   Energy's Petascale Data Storage Institute [DE-FC02-06ER25768]; Storage
   Systems Research Center; Center for Research in Intelligent Storage
FX This research was also supported in part by the National Science
   Foundation under awards CNS-0917396 (part of the American Recovery and
   Reinvestment Act of 2009 [Public Law 111-5]) and IIP-0934401, and by the
   Department of Energy's Petascale Data Storage Institute under award
   DE-FC02-06ER25768. We also thank Sandia National Laboratories and the
   industrial sponsors of the Storage Systems Research Center and the
   Center for Research in Intelligent Storage for their generous support.
CR Adams IF, 2012, ACM T STORAGE, V8, DOI 10.1145/2180905.2180907
   Ahmed A, 2002, IEEE IPCCC, P131, DOI 10.1109/IPCCC.2002.995144
   Amer A., 2002, IEEE INT S MOD AN SI, P293
   [Anonymous], 2001, PATTERN CLASSIFICATI
   [Anonymous], T STORAGE
   Ari Ismail, 2002, P INF, P143
   Arpaci-Dusseau A. C., 2006, Performance Evaluation Review, V33, P29, DOI 10.1145/1138085.1138093
   Barbaro M, 2006, FACE IS EXPOSED AOL
   BELADY LA, 1966, IBM SYST J, V5, P78, DOI 10.1147/sj.52.0078
   COFFMAN EG, 1972, COMMUN ACM, V15, P185, DOI 10.1145/361268.361280
   COLARELLI D, 2002, P 2002 ACM IEEE C SU
   Constantinescu C, 2011, IEEE DATA COMPR CONF, P393, DOI 10.1109/DCC.2011.46
   Cormen T., 1990, ALGORITHMS
   Ding X., 2007, 2007 USENIX ANN TECH, P1
   Doraimani S., 2008, Proceedings of the 17th international symposium on High performance distributed computing, P153
   Dufrasne Bert, 2012, IBM 14 STORAGE SYSTE
   Essary D., 2008, ACM T STORAGE, V4, P1, DOI [10.1145/1353452, DOI 10.1145/1353452.1353454]
   Jaccard P., 1901, B SOCIETE VAUDOISEDE, V37, P241
   Jiang S., 2005, P 4 C USENIX C FIL S, V4, P8
   Kroeger T. M., 1996, P 1996 ANN C USENIX, P26
   Kroeger TM, 2001, USENIX ASSOCIATION PROCEEDINGS OF THE 2001 USENIX ANNUAL TECHNICAL CONFERENCE, P105
   Lancichinetti A, 2009, PHYS REV E, V80, DOI 10.1103/PhysRevE.80.056117
   Li W., 2008, THESIS U MASSACHUSET
   Li ZM, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P173
   Lo S.-w., 2014, P 29 ANN ACM S APPL, P1478
   Magurran AE., 2004, African Journal of Aquatic Science, V29, P256, DOI [DOI 10.2989/16085910409503825, 10.1016/j.cub.2021.07.049]
   MCKUSICK MK, 1984, ACM T COMPUT SYST, V2, P181, DOI 10.1145/989.990
   Narayanan D., 2008, ACM T STORAGE, V4, P3
   Oly J., 2002, Conference Proceedings of the 2002 International Conference on SUPERCOMPUTING, P147, DOI 10.1145/514191.514214
   PINHEIRO E, 2004, ICS 04
   RAND WM, 1971, J AM STAT ASSOC, V66, P846, DOI 10.2307/2284239
   Riska A, 2006, USENIX ASSOCIATION PROCEEDINGS OF THE 2006 USENIX ANNUAL TECHNICAL CONFERENCE, P97
   Schindler J., 2002, C FIL STOR TECHN
   Schmuck F., 2002, P 2002 C FIL STOR TE
   Sivathanu M, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE 2ND USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST'03), P73
   Sivathanu M., 2005, ACM TOS, V1, P133
   Slonim N, 2005, P NATL ACAD SCI USA, V102, P18297, DOI 10.1073/pnas.0507432102
   Sorensen T.A., 1948, BIOL SKRIFTER, V5, P1
   STAELIN C, 1990, CSTR29890
   Tanenbaum A.S., 2006, ACM SIGOPS Operating Systems Review, V40, P104
   Wang J., 2001, IEEE INT S MOD AN SI
   Wildani A., 2014, 22 IEEE INT S MOD AN
   Wildani A., 2010, Petascale Data Storage Workshop (PDSW), 20105th, P1
   Wildani A., 2011, P 4 ANN INT C SYST S, P5
   Wildani A, 2013, PROC INT CONF DATA, P446, DOI 10.1109/ICDE.2013.6544846
   Wu Guanying, 2012, P 7 ACM EUR C COMP S, P253
   Yadwadkar N. J., 2010, P 8 USENIX C FIL STO, V14
   Zaman S, 2009, MOL SYST BIOL, V5, DOI 10.1038/msb.2009.2
   Zhuang XT, 2007, IEEE T COMPUT, V56, P18, DOI 10.1109/TC.2007.250620
NR 49
TC 6
Z9 7
U1 0
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD FEB
PY 2016
VL 12
IS 2
AR 7
DI 10.1145/2738042
PG 33
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DJ0PO
UT WOS:000373906600002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Jung, J
   Won, Y
   Kim, E
   Shin, H
   Jeon, B
AF Jung, Jaemin
   Won, Youjip
   Kim, Eunki
   Shin, Hyungjong
   Jeon, Byeonggil
TI FRASH: Exploiting Storage Class Memory in Hybrid File System for
   Hierarchical Storage
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Measurement; Performance; Flash storage; log-structured file system
AB In this work, we develop a novel hybrid file system, FRASH, for storage-class memory and NAND Flash. Despite the promising physical characteristics of storage-class memory, its scale is an order of magnitude smaller than the current storage device scale. This fact makes it less than desirable for use as an independent storage device. We carefully analyze in-memory and on-disk file system objects in a log-structured file system, and exploit memory and storage aspects of the storage-class memory to overcome the drawbacks of the current log-structured file system. FRASH provides a hybrid view storage-class memory. It harbors an in-memory data structure as well as a on-disk structure. It provides nonvolatility to key data structures which have been maintained in-memory in a legacy log-structured file system. This approach greatly improves the mount latency and effectively resolves the robustness issue. By maintaining on-disk structure in storage-class memory, FRASH provides byte-addressability to the file system object and metadata for page, and subsequently greatly improves the I/O performance compared to the legacy log-structured approach. While storage-class memory offers byte granularity, it is still far slower than its DRAM counter part. We develop a copy-on-mount technique to overcome the access latency difference between main memory and storage-class memory. Our file system was able to reduce the mount time by 92% and file system I/O performance was increased by 16%.
C1 [Jung, Jaemin; Won, Youjip] Hanyang Univ, Dept Elect & Comp Engn, Seoul 133791, South Korea.
   [Kim, Eunki; Shin, Hyungjong; Jeon, Byeonggil] Samsung Elect, Suwon, South Korea.
C3 Hanyang University; Samsung Electronics; Samsung
RP Jung, J (corresponding author), Hanyang Univ, Dept Elect & Comp Engn, Seoul 133791, South Korea.
EM jmjung@ece.hanyan.ac.kr
CR [Anonymous], S VLSI TECH JUN
   [Anonymous], 2008, P C FIL STOR TECHN F
   BITYUCKIY A. B., 2005, FFS3 DESIGN ISSUES
   Chin-Hsien Wu, 2006, ACM Transaction on Storage, V2, P449, DOI 10.1145/1210596.1210600
   DESHPANDE M., 1988, P 7 ANN INT PHOEN C
   Doh InHwan., 2007, 7th ACM IEEE Conference on Embedded Software (EMSOFT '07), P164
   *FREESC, FREESC SEM
   FREITAS R., 2008, TUT 6 USENIX C FIL S
   Intel Corporation, UND FLASH TRANSL LAY
   JEON B., 2008, THESIS HANYANG U SEO
   JUNG J., 2009, P 4 INT WORKSH SUPP, P67
   Kgil T, 2008, CONF PROC INT SYMP C, P327, DOI 10.1109/ISCA.2008.32
   Kim EK, 2007, LECT NOTES COMPUT SC, V4705, P238
   Kim H, 2009, IEEE T CONSUM ELECTR, V55, P545, DOI 10.1109/TCE.2009.5174420
   Lau SW, 1997, COMPUT J, V40, P529, DOI 10.1093/comjnl/40.9.529
   MANNING C., 2001, YAFFS YET ANOTHER FL
   MCKUSICK MK, 1984, ACM T COMPUT SYST, V2, P181, DOI 10.1145/989.990
   McVoy L, 1996, PROCEEDINGS OF THE USENIX 1996 ANNUAL TECHNICAL CONFERENCE, P279
   MERITECH, MER SMDK2240 BOARD
   Miller E. L., 2001, P 8 IEEE WORKSH HOT, P83
   NIKKEI, NIKK EL
   Park SH, 2006, LECT NOTES COMPUT SC, V4017, P415, DOI 10.1007/11796435_42
   Park Y, 2008, APPLIED COMPUTING 2008, VOLS 1-3, P1498
   Raoux S, 2008, IBM J RES DEV, V52, P465, DOI 10.1147/rd.524.0465
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   SCHLACK M., 2004, STORAGE TECHNOLOGY N
   SHIN H., 2008, THESIS HANYANG U SEO
   Wang A. A., 2006, ACM Transaction on Storage, V2, P309, DOI 10.1145/1168910.1168914
   Wilkes J, 1996, ACM T COMPUT SYST, V14, P108, DOI 10.1145/225535.225539
   YEGULALP S., 2007, ECC MEMORY MUST SERV
   YIM KS, 2005, P ACM S APPL COMP SA, P843
NR 31
TC 38
Z9 46
U1 0
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAR
PY 2010
VL 6
IS 1
AR 3
DI 10.1145/1714454.1714457
PG 25
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA V24QF
UT WOS:000208424200003
DA 2024-07-18
ER

PT J
AU Ganesan, A
   Alagappan, R
   Rebello, A
   Arpaci-Dusseau, AC
   Arpaci-Dusseau, RH
AF Ganesan, Aishwarya
   Alagappan, Ramnatthan
   Rebello, Anthony
   Arpaci-Dusseau, Andrea C.
   Arpaci-Dusseau, Remzi H.
TI Exploiting Nil-external Interfaces for Fast Replicated Storage
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article; Proceedings Paper
CT ACM Symposium on Operating Systems Principles (SOSP)
CY OCT 26-29, 2021
CL ELECTR NETWORK
SP Assoc Comp Machinery
DE Fault-tolerance; replication; storage
ID RECOVERY
AB Do some storage interfaces enable higher performance than others? Can one identify and exploit such interfaces to realize high performance in storage systems? This article answers these questions in the affirmative by identifying nil-externality, a property of storage interfaces. A nil-externalizing (nilext) interface may modify state within a storage system but does not externalize its effects or system state immediately to the outside world. As a result, a storage system can apply nilext operations lazily, improving performance.
   In this article, we take advantage of nilext interfaces to build high-performance replicated storage. We implement SKYROS, a nilext-aware replication protocol that offers high performance by deferring ordering and executing operations until their effects are externalized. We show that exploiting nil-externality offers significant benefit: For many workloads, SKYROS provides higher performance than standard consensusbased replication. For example, SKYROS offers 3x lower latency while providing the same high throughput offered by throughput-optimized Paxos.
C1 [Ganesan, Aishwarya; Alagappan, Ramnatthan; Rebello, Anthony; Arpaci-Dusseau, Andrea C.; Arpaci-Dusseau, Remzi H.] Univ Wisconsin, Madison, WI 53706 USA.
   [Ganesan, Aishwarya; Alagappan, Ramnatthan] 201 N Goodwin Ave, Urbana, IL 61801 USA.
   [Rebello, Anthony; Arpaci-Dusseau, Andrea C.; Arpaci-Dusseau, Remzi H.] 1210 W Dayton St, Madison, WI 53706 USA.
   [Ganesan, Aishwarya; Alagappan, Ramnatthan] Univ Illinois, Champaign, IL 61820 USA.
C3 University of Wisconsin System; University of Wisconsin Madison;
   University of Illinois System; University of Illinois Urbana-Champaign
RP Ganesan, A (corresponding author), Univ Wisconsin, Madison, WI 53706 USA.; Ganesan, A (corresponding author), 201 N Goodwin Ave, Urbana, IL 61801 USA.; Ganesan, A (corresponding author), Univ Illinois, Champaign, IL 61820 USA.
EM aganesn2@illinois.edu; ramn@illinois.edu; arebello@wisc.edu;
   dusseau@cs.wisc.edu; remzi@cs.wisc.edu
OI Ganesan, Aishwarya/0000-0002-5078-8436; Rebello,
   Anthony/0000-0003-2614-7935; Arpaci-Dusseau, Andrea/0000-0001-8618-2738;
   Alagappan, Ramnatthan/0000-0001-9911-4208
FU NSF [CNS-1838733, CNS-1763810]; Google; Intel; Seagate; Samsung;
   Microsoft; VMware
FX This material was supported in part by funding from NSF CNS-1838733,
   CNS-1763810, Google, VMware, Intel, Seagate, Samsung, and Microsoft. The
   authors' opinions and findings may not reflect those of NSF or other
   institutions.
CR Abu-Libdeh H., 2013, P 4 ANN S CLOUD COMP, P12
   Aguilera Marcos K., 2020, P 14 USENIX C OPERAT
   Aksoy RC, 2019, PROCEEDINGS OF THE TWENTY-SEVENTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '19), P385, DOI 10.1145/3341301.3359663
   Apache, KAKF
   Apache, 2021, ZooKeeper
   ATTIYA H, 1995, J ASSOC COMPUT MACH, V42, P124, DOI 10.1145/200836.200869
   AWS News Blog, 2020, AWS NEWS BLOG
   Bender Michael A, 2015, USENIX, V40, P22
   Benson T., 2010, P 10 ACM SIGCOMM C I, P267, DOI DOI 10.1145/1879141.1879175
   Bernier Yahn W, 2001, GAM DEV C, V98033
   Bolosky WilliamJ., 2011, P 8 USENIX C NETWORK, P11
   Brodal GS, 2003, SIAM PROC S, P546
   Budhiraja N., 1993, Distributed systems
   Burke M, 2020, PROCEEDINGS OF THE 17TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P591
   Cao ZC, 2020, PROCEEDINGS OF THE 18TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P209
   Charapko A., 2019, P 11 USENIX WORKSHOP
   CHERITON DR, 1987, ACM T COMPUT SYST, V5, P12, DOI 10.1145/7351.7353
   Clements AT, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P1, DOI 10.1145/2517349.2522712
   Conway A, 2020, PROCEEDINGS OF THE 2020 USENIX ANNUAL TECHNICAL CONFERENCE, P49
   Cooper B.F., 2010, P ACM S CLOUD COMPUT, DOI DOI 10.1145/1807128.1807152
   Corbett JC, 2013, ACM T COMPUT SYST, V31, DOI 10.1145/2491245
   Cowling J, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P177
   Cowling James, 2012, 2012 USENIX ANN TECH
   Dang HT, 2020, IEEE ACM T NETWORK, V28, P1726, DOI 10.1109/TNET.2020.2992106
   Dang HuynhTu., 2015, Proceedings_of_the_1st_ACM_SIGCOMM_Symposium on_Software_Defined_Networking_Research, page, P5
   DeWitt D. J., 1984, SIGMOD Record, V14, P1, DOI 10.1145/971697.602261
   Duplyakin D, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P1
   Elnozahy EN, 2002, ACM COMPUT SURV, V34, P375, DOI 10.1145/568522.568525
   Esmet John, 2012, P 4 WORKSHOP HOT TOP
   Facebook, 2016, MyRocks: A spaceand write-optimized MySQL database
   Facebook, 2021, RocksDB
   Facebook, 2021, Merge Operator
   Faleiro JM, 2014, SIGMOD'14: PROCEEDINGS OF THE 2014 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P15, DOI 10.1145/2588555.2610529
   Ganesan A, 2021, PROCEEDINGS OF THE 28TH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, SOSP 2021, P440, DOI 10.1145/3477132.3483543
   Ganesan A, 2020, PROCEEDINGS OF THE 18TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P323
   Ghemawat S., 2003, ACM SIGOPS OPERATING, V37, P29, DOI [DOI 10.1145/945445.945450, DOI 10.1145/1165389.945450, 10.1145/1165389.945450]
   Ghemawhat Sanjay, 2011, LevelDB
   github, 2021, MEMCACHED COMMANDS
   Guo Zhenyu, 2014, P 9 EUR C COMP SYST, DOI [10.1145/2592798.2592800, DOI 10.1145/2592798.2592800]
   Hagmann Robert, 1987, P 11 ACM S OP SYST P
   HERLIHY MP, 1990, ACM T PROGR LANG SYS, V12, P463, DOI 10.1145/78969.78972
   HUDAK P, 1989, COMPUT SURV, V21, P359, DOI 10.1145/72551.72554
   IBM, 2021, Locations for Resource Deployment: Multizone Regions
   Jannen William, 2015, P 14 USENIX S FILE S
   Kalia A, 2014, SIGCOMM'14: PROCEEDINGS OF THE 2014 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P295, DOI [10.1145/2619239.2626299, 10.1145/2740070.2626299]
   Kapritsos M., 2012, OSDI
   Kemme B, 1999, INT CON DISTR COMP S, P424, DOI 10.1109/ICDCS.1999.776544
   Kotla R, 2009, ACM T COMPUT SYST, V27, DOI [10.1145/1323293.1294267, 10.1145/1658357.1658358]
   Lamport L., 2001, SIGACT News, V32, P51
   Lamport Leslie, 2005, Generalized Consensus and Paxos
   Lampson ButlerW., 1983, P 9 ACM S OPERATING
   Le LH, 2019, INT CON DISTR COMP S, P1453, DOI 10.1109/ICDCS.2019.00145
   Lee C, 2015, SOSP'15: PROCEEDINGS OF THE TWENTY-FIFTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P71, DOI 10.1145/2815400.2815416
   Li Cheng, 2012, P 10 S OPERATING SYS
   Li JL, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P467
   Lin Wei., 2008, PACIFICA REPLICATION
   Linux Foundation, 2011, TC NET 8 LIN MAN PAG
   Liskov Barbara, 2012, Viewstamped Replication Revisited
   Liskov Barbara, 1991, P 13 ACM S OPERATING
   Mao Y., 2008, OSDI
   Matsunobu Y, 2020, PROC VLDB ENDOW, V13, P3217, DOI 10.14778/3415478.3415546
   Mehdi SA, 2017, PROCEEDINGS OF NSDI '17: 14TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P453
   Mitchell C, 2016, PROCEEDINGS OF USENIX ATC '16: 2016 USENIX ANNUAL TECHNICAL CONFERENCE, P451
   MOHAN C, 1992, ACM T DATABASE SYST, V17, P94, DOI 10.1145/128765.128770
   Moraru I, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P358, DOI 10.1145/2517349.2517350
   Mu S, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P517
   Nightingale EB, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P1
   Nishtala Rajesh, 2013, P 10 S NETW SYST DES
   Ofer Effi, 2021, Object Storage Traces: A Treasure Trove of Information for Optimizing Cloud Workloads
   ONeil P, 1996, ACTA INFORM, V33, P351, DOI 10.1007/s002360050048
   Ongaro Diego, 2014, 2014 USENIX ANN TECH, P305, DOI DOI 10.1007/0-387-34805-0_21
   Park SJ, 2019, PROCEEDINGS OF THE 16TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P47
   Pedone F, 2002, DISTRIB COMPUT, V15, P97, DOI 10.1007/s004460100061
   Percona, 2013, Fast Updates with TokuDB
   Ports Dan R. K., 2015, P 12 S NETW SYST DES
   Roy Sudip, 2013, P 6 BIENNIAL C INNOV
   Rumble Stephen M., 2011, P 13 WORKSH HOT TOP
   Sandberg Russel, 1986, SUN NETWORK FILE SYS
   SCHNEIDER FB, 1990, COMPUT SURV, V22, P299, DOI 10.1145/98163.98167
   Serenyi Denis, Cluster-Level Storage @ Google
   STROM RE, 1985, ACM T COMPUT SYST, V3, P204, DOI 10.1145/3959.3962
   Tai A, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P977
   Terry DB, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P309, DOI 10.1145/2517349.2522731
   Twitter, 2020, Twitter Cache Trace
   Twitter, 2012, Caching with Twemcache
   van Renesse R, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE SIXTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDE '04), P91
   Wei XD, 2020, PROCEEDINGS OF THE 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '20), P117
   Wester Benjamin, 2009, P 6 S NETW SYST DES
   Xinan Yan, 2020, CoNEXT '20: Proceedings of the 16th International Conference on emerging Networking EXperiments and Technologies, P351, DOI 10.1145/3386367.3431291
   Yang JC, 2020, PROCEEDINGS OF THE 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '20), P191
   Zhang I, 2015, SOSP'15: PROCEEDINGS OF THE TWENTY-FIFTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P263, DOI 10.1145/2815400.2815404
NR 91
TC 0
Z9 0
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2022
VL 18
IS 3
AR 20
DI 10.1145/3542821
PG 35
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 4X9BK
UT WOS:000861131000002
DA 2024-07-18
ER

PT J
AU Li, JW
   Lee, PPC
   Tan, CF
   Qin, C
   Zhang, XS
AF Li, Jingwei
   Lee, Patrick P. C.
   Tan, Chufeng
   Qin, Chuan
   Zhang, Xiaosong
TI Information Leakage in Encrypted Deduplication via Frequency Analysis:
   Attacks and Defenses
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Frequency analysis; encrypted deduplication; cloud storage
AB Encrypted deduplication combines encryption and deduplication to simultaneously achieve both data security and storage efficiency. State-of-the-art encrypted deduplication systems mainly build on deterministic encryption to preserve deduplication effectiveness. However, such deterministic encryption reveals the underlying frequency distribution of the original plaintext chunks. This allows an adversary to launch frequency analysis against the ciphertext chunks and infer the content of the original plaintext chunks. In this article, we study how frequency analysis affects information leakage in encrypted deduplication, from both attack and defense perspectives. Specifically, we target backup workloads and propose a new inference attack that exploits chunk locality to increase the coverage of inferred chunks. We further combine the new inference attack with the knowledge of chunk sizes and show its attack effectiveness against variable-size chunks. We conduct trace-driven evaluation on both real-world and synthetic datasets and show that our proposed attacks infer a significant fraction of plaintext chunks under backup workloads. To defend against frequency analysis, we present two defense approaches, namely MinHash encryption and scrambling. Our trace-driven evaluation shows that our combined MinHash encryption and scrambling scheme effectively mitigates the severity of the inference attacks, while maintaining high storage efficiency and incurring limited metadata access overhead.
C1 [Li, Jingwei; Tan, Chufeng; Zhang, Xiaosong] Univ Elect Sci & Technol China, Chengdu, Peoples R China.
   [Li, Jingwei] Chinese Acad Sci, Inst Informat Engn, State Key Lab Informat Secur, Chengdu, Peoples R China.
   [Lee, Patrick P. C.; Qin, Chuan] Chinese Univ Hong Kong, Hong Kong, Peoples R China.
C3 University of Electronic Science & Technology of China; Chinese Academy
   of Sciences; Chinese University of Hong Kong
RP Lee, PPC (corresponding author), Chinese Univ Hong Kong, Hong Kong, Peoples R China.
EM lijw1987@gmail.com; pclee@cse.cuhk.edu.hk; chufengtan97@gmail.com;
   chintran27@gmail.com; johnsonzxs@uestc.edu.cn
RI Lee, Patrick P. C./I-7165-2013
OI Lee, Patrick P. C./0000-0002-4501-4364; Li, Jingwei/0000-0001-8457-0454
FU National Key R&D Program of China [2017YFB0802300]; National Natural
   Science Foundation of China [61602092, 61972073]; Open Research Project
   of the State Key Laboratory of Information Security, Institute of
   Information Engineering, Chinese Academy of Sciences [2019-MS-05];
   Research Grants Council of Hong Kong [CRF C7036-15G]
FX This work was supported in part by grants by National Key R&D Program of
   China (Grant number 2017YFB0802300), National Natural Science Foundation
   of China (Grant numbers 61602092 and 61972073), Open Research Project of
   the State Key Laboratory of Information Security, Institute of
   Information Engineering, Chinese Academy of Sciences (Grant number
   2019-MS-05), and the Research Grants Council of Hong Kong (CRF
   C7036-15G).
CR Abadi M, 2013, LECT NOTES COMPUT SC, V8042, P374, DOI 10.1007/978-3-642-40041-4_21
   Al-Kadit I.A., 1992, Cryptologia, V16, P97, DOI DOI 10.1080/0161-119291866801
   Allu Y, 2017, COMPUTER, V50, P64, DOI 10.1109/MC.2017.187
   Amvrosiadis George, 2015, P USENIX ANN TECHN C
   Anderson PO, 2010, PHARMACY INFORMATICS, P1, DOI 10.1145/1852658.1852665
   [Anonymous], 2001, Handbook of applied cryptography
   [Anonymous], 2013, AAAI SPRING S TECHN
   [Anonymous], 1981, TRCSE0301 HARV U CTR
   [Anonymous], 2014, FSL TRACES SNAPSHOTS
   [Anonymous], 2017, HIPAA J
   [Anonymous], 2008, Proceedings of the 4th ACM international workshop on Storage security and survivability
   [Anonymous], 2019, UBUNTU IRC LOGS
   [Anonymous], 2009, 7 USENIX C FIL STOR
   [Anonymous], 2010, P USENIX C FIL STOR
   [Anonymous], 2005, HPL200530R1
   Armknecht F, 2017, PROCEEDINGS OF THE 2017 ACM ASIA CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (ASIA CCS'17), P266, DOI 10.1145/3052973.3053019
   Armknecht F, 2015, CCS'15: PROCEEDINGS OF THE 22ND ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P886, DOI 10.1145/2810103.2813630
   Arrington M., 2006, AOL THIS WAS SCREW
   Bellare M, 2013, 22 USENIX SEC S USEN, P179
   Bellare M, 2015, LECT NOTES COMPUT SC, V9020, P516, DOI 10.1007/978-3-662-46447-2_23
   Bellare M, 2013, LECT NOTES COMPUT SC, V7881, P296, DOI 10.1007/978-3-642-38348-9_18
   Bhagwat D, 2009, 2009 IEEE INTERNATIONAL SYMPOSIUM ON MODELING, ANALYSIS & SIMULATION OF COMPUTER AND TELECOMMUNICATION SYSTEMS (MASCOTS), P237
   Bindschaedler V, 2018, PROC VLDB ENDOW, V11, P1715, DOI 10.14778/3236187.3236217
   Black J, 2006, USENIX ASSOCIATION PROCEEDINGS OF THE 2006 USENIX ANNUAL TECHNICAL CONFERENCE, P85
   Brekne T., 2005, Privacy Enhancing Technologies. 5th International Workshop, PET 2005. Revised Selected Papers (Lecture Notes in Computer Science Vol. 3856), P179
   Broder AZ, 1998, COMPRESSION AND COMPLEXITY OF SEQUENCES 1997 - PROCEEDINGS, P21, DOI 10.1109/SEQUEN.1997.666900
   Cash D, 2015, CCS'15: PROCEEDINGS OF THE 22ND ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P668, DOI 10.1145/2810103.2813700
   Cox LR, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE FIFTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P285, DOI 10.1145/1060289.1060316
   Darrow B., 2015, HARVARD AFFILIATE MC
   Douceur JR, 2002, INT CON DISTR COMP S, P617, DOI 10.1109/ICDCS.2002.1022312
   Douglis F, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P29
   Duan Y., 2014, P 6 ED ACM WORKSH CL, P57
   Ghemawat S., 2014, LEVELDB FAST KEY VAL
   Grubbs P, 2017, P IEEE S SECUR PRIV, P655, DOI 10.1109/SP.2017.44
   Grubbs P, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1353, DOI 10.1145/2976749.2978351
   Hackett Robert, 2016, LINKEDIN LOST 167 MI
   Halevi S, 2011, PROCEEDINGS OF THE 18TH ACM CONFERENCE ON COMPUTER & COMMUNICATIONS SECURITY (CCS 11), P491, DOI 10.1145/2046707.2046765
   Harnik D, 2010, IEEE SECUR PRIV, V8, P40, DOI 10.1109/MSP.2010.187
   Ilukhin OV, 2012, RUSS J CARDIOL, P24
   Islam M. S., 2012, Ndss
   Jin K., 2009, P SYSTOR 2009 ISR EX, p7:1, DOI DOI 10.1145/1534530.1534540
   Kallahalla M, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE 2ND USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST'03), P29
   Kellaris G, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1329, DOI 10.1145/2976749.2978386
   Klimt B, 2004, LECT NOTES COMPUT SC, V3201, P217
   Kumar R, 2007, ACTA HORTIC, P621, DOI 10.17660/ActaHortic.2007.735.80
   Lacharite Marie-Sarah, 2015, 20151158 CRYPT EPRIN
   Li C., 2015, P USENIX ANN TECH C, P111
   Li JL, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19040758
   Li JW, 2017, I C DEPEND SYS NETWO, P1, DOI 10.1109/DSN.2017.28
   Lillibridge M., 2013, FAST
   Liu J, 2015, CCS'15: PROCEEDINGS OF THE 22ND ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P874, DOI 10.1145/2810103.2813623
   Ma JG, 2016, ACM T INTERNET TECHN, V16, DOI 10.1145/2806890
   Meyer D.T., 2011, Proceedings of the 9th USENIX conference on File and stroage technologies, P1
   Naveed M, 2015, CCS'15: PROCEEDINGS OF THE 22ND ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P644, DOI 10.1145/2810103.2813651
   Naveed M, 2014, P IEEE S SECUR PRIV, P639, DOI 10.1109/SP.2014.47
   Pouliot D, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1341, DOI 10.1145/2976749.2978401
   Qin C, 2017, ACM T STORAGE, V13, DOI 10.1145/3032966
   Ritzdorf H, 2016, CCSW'16: PROCEEDINGS OF THE 2016 ACM CLOUD COMPUTING SECURITY WORKSHOP, P61, DOI 10.1145/2996429.2996432
   Shah R, 2015, INNOVATION, ENTREPRENEURSHIP, AND THE ECONOMY IN THE US, CHINA, AND INDIA: HISTORICAL PERSPECTIVES AND FUTURE TRENDS, P333
   Shi E, 2011, LECT NOTES COMPUT SC, V7073, P197, DOI 10.1007/978-3-642-25385-0_11
   Storer MW, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P1, DOI 10.1145/1456469.1456471
   Sun Z, 2016, IEEE S MASS STOR SYS
   Wallace JE, 2012, ADVANCING TECHNOLOGIES AND INTELLIGENCE IN HEALTHCARE AND CLINICAL ENVIRONMENTS: BREAKTHROUGHS, P33, DOI 10.4018/978-1-4666-1755-1.ch003
   Xia W, 2016, P IEEE, V104, P1681, DOI 10.1109/JPROC.2016.2571298
   Xia Wen., 2011, Proceedings of the USENIX Annual Technical Conference, USENIXATC'11, P26
   Zhang YP, 2016, PROCEEDINGS OF THE 25TH USENIX SECURITY SYMPOSIUM, P707
   Zhu B, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P269
NR 67
TC 5
Z9 5
U1 1
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD APR
PY 2020
VL 16
IS 1
AR 4
DI 10.1145/3365840
PG 30
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1QN
UT WOS:000583742400004
DA 2024-07-18
ER

PT J
AU Zhu, T
   Zhao, ZY
   Li, FF
   Qian, WN
   Zhou, AY
   Xie, D
   Stutsman, R
   Li, HN
   Hu, HQ
AF Zhu, Tao
   Zhao, Zhuoyue
   Li, Feifei
   Qian, Weining
   Zhou, Aoying
   Xie, Dong
   Stutsman, Ryan
   Li, Haining
   Hu, Huiqi
TI SolarDB: Toward a Shared-Everything Database on Distributed
   Log-Structured Storage
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article; Proceedings Paper
CT USENIX Annual Technical Conference (ATC)
CY JUL 11-13, 2018
CL Boston, MA
SP USENIX
DE Shared-everything architecture; log-structured storage; concurrency
   control
ID SYSTEM
AB Efficient transaction processing over large databases is a key requirement for many mission-critical applications. Although modern databases have achieved good performance through horizontal partitioning, their performance deteriorates when cross-partition distributed transactions have to be executed. This article presents SolarDB, a distributed relational database system that has been successfully tested at a large commercial bank. The key features of SolarDB include (1) a shared-everything architecture based on a two-layer log-structured merge-tree; (2) a new concurrency control algorithm that works with the log-structured storage, which ensures efficient and non-blocking transaction processing even when the storage layer is compacting data among nodes in the background; and (3) find-grained data access to effectively minimize and balance network communication within the cluster. According to our empirical evaluations on TPC-C, Smallbank, and a real-world workload, SolarDB outperforms the existing shared-nothing systems by up to 50x when there are close to or more than 5% distributed transactions.
C1 [Zhu, Tao; Qian, Weining; Zhou, Aoying] East China Normal Univ, 3663 Zhongshan N Rd, Shanghai 200241, Peoples R China.
   [Zhao, Zhuoyue; Li, Feifei; Xie, Dong; Stutsman, Ryan] Univ Utah, 50 Cent Campus Dr, Salt Lake City, UT 84112 USA.
   [Li, Haining; Hu, Huiqi] Bank Commun, 188 Yincheng Middle Rd, Shanghai 200090, Peoples R China.
C3 East China Normal University; Utah System of Higher Education;
   University of Utah; Bank of Communications
RP Zhu, T (corresponding author), East China Normal Univ, 3663 Zhongshan N Rd, Shanghai 200241, Peoples R China.
EM zhutao@stu.ecnu.edu.cn; zyzhao@cs.utah.edu; lifeifei@cs.utah.edu;
   wnqian@sei.ecnu.edu.cn; ayzhou@sei.ecnu.edu.cn; dongx@cs.utah.edu;
   stutsman@cs.utah.edu; lihn@bankcomm.com; hqhu@bankcomm.com
RI Zhao, Zhuoyue/AFF-7425-2022; Zhao, Zhuoyue/IUO-0794-2023; Li,
   Feifei/JTT-8011-2023; zhu, tao/KHY-3114-2024; Zhu, Tao/JEF-1129-2023
OI Zhao, Zhuoyue/0000-0002-1631-718X; Zhu, Tao/0009-0001-1499-8700; Xie,
   Dong/0000-0003-4857-900X
FU 863 Program [2015AA015307]; National Key RD Plan Project
   [2018YFB1003303]; NSFC [61729202, 61432006, 61332006]; NSF [CNS-1750558,
   1619287, 1443046]; Div Of Information & Intelligent Systems; Direct For
   Computer & Info Scie & Enginr [1619287] Funding Source: National Science
   Foundation
FX T. Zhu, W. Qian, and A. Zhou were supported by the 863 Program
   (2015AA015307), National Key R&D Plan Project (2018YFB1003303), and NSFC
   (61432006 and 61332006). F. Li, Z. Zhao, and D. Xie were supported in
   part by NSF grants 1619287 and 1443046. F. Li was also supported in part
   by NSFC grant 61729202. R. Stutsman was supported in part by NSF grant
   CNS-1750558. Any opinions, findings, and conclusions or recommendations
   expressed in this material are those of the authors and do not
   necessarily reflect the views of the National Science Foundation.
CR Alibaba Oceanbase, 2015, OCEANBASE
   [Anonymous], 2013, P 2013 ACM SIGMOD IN, DOI DOI 10.1145/2463676.2463710
   [Anonymous], 2007, VLDB
   [Anonymous], 2010, SIGOPS Oper. Syst. Rev., DOI DOI 10.1145/1713254.1713276
   [Anonymous], P 9 EUR C COMP SYST
   [Anonymous], 2013, IEEE Data Eng. Bull.
   Berenson H., 1995, SIGMOD Record, V24, P1, DOI 10.1145/568271.223785
   Bernstein PA, 2015, SIGMOD'15: PROCEEDINGS OF THE 2015 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P1295, DOI 10.1145/2723372.2737788
   Chandrasekaran S, 2003, PROC INT CONF DATA, P840, DOI 10.1109/ICDE.2003.1260883
   Chang F, 2008, ACM T COMPUT SYST, V26, DOI 10.1145/1365815.1365816
   DeCandia Giuseppe, 2007, Operating Systems Review, V41, P205, DOI 10.1145/1323293.1294281
   Dragojevic A, 2015, SOSP'15: PROCEEDINGS OF THE TWENTY-FIFTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P54, DOI 10.1145/2815400.2815425
   Dragojevic Aleksandar, 2014, NSDI 14, P401
   Goel AK, 2015, PROC VLDB ENDOW, V8, P1716
   Josten JW, 1997, IBM SYST J, V36, P327, DOI 10.1147/sj.362.0327
   Kaltman R, 2008, PROC VLDB ENDOW, V1, P1496, DOI 10.14778/1454159.1454211
   Kemper A, 2011, PROC INT CONF DATA, P195, DOI 10.1109/ICDE.2011.5767867
   KENNEDY K., 1993, MAXIMIZING LOOP PARA
   KUNG HT, 1981, ACM T DATABASE SYST, V6, P213, DOI 10.1145/319566.319567
   Lakshman Avinash, 2010, Operating Systems Review, V44, P35, DOI 10.1145/1773912.1773922
   Levandoski Justin, 2015, P CIDR
   Loesing S, 2015, SIGMOD'15: PROCEEDINGS OF THE 2015 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P663, DOI 10.1145/2723372.2751519
   Mu S., 2014, P USENIX OSDI, P479, DOI DOI 10.5555/2685048.2685086
   Muchnick S., 1997, ADV COMPILER DESIGN
   ONeil P, 1996, ACTA INFORM, V33, P351, DOI 10.1007/s002360050048
   Rödiger W, 2015, PROC VLDB ENDOW, V9, P228
   Sefton-Green J, 2012, CREAT TEACH CREAT SC, P1
   Serafini M, 2014, PROC VLDB ENDOW, V7, P1035, DOI 10.14778/2732977.2732979
   Taft R, 2014, PROC VLDB ENDOW, V8, P245, DOI 10.14778/2735508.2735514
   Tu S, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P18, DOI 10.1145/2517349.2522713
   Vo HT, 2012, PROC VLDB ENDOW, V5, P1004, DOI 10.14778/2336664.2336673
   Vossen G, 1995, LECT NOTES COMPUT SC, V1000, P560
   Wang ZG, 2016, SIGMOD'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P1643, DOI 10.1145/2882903.2882934
   Wei M, 2017, PROCEEDINGS OF NSDI '17: 14TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P35
   Wei XD, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P335
   White B, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE FIFTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P255, DOI 10.1145/1060289.1060313
   Wu YJ, 2016, SIGMOD'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P1689, DOI 10.1145/2882903.2915202
   Yan C, 2016, PROC VLDB ENDOW, V9, P444
   Zaharia M., 2010, P USENIX HOTCLOUD
   Zhang Y, 2015, PROCEEDINGS OF THE FOURTEENTH INTERNATIONAL FEDERATION FOR THE PROMOTION OF MECHANISM AND MACHINE SCIENCE WORLD CONGRESS, P87, DOI 10.6567/IFToMM.14TH.WC.OS2.007
NR 40
TC 7
Z9 8
U1 1
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUN
PY 2019
VL 15
IS 2
AR 11
DI 10.1145/3318158
PG 26
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA JL3IK
UT WOS:000495424300004
OA Bronze
DA 2024-07-18
ER

PT J
AU Einziger, G
   Friedman, R
   Manes, B
AF Einziger, Gil
   Friedman, Roy
   Manes, Ben
TI TinyLFU: A Highly Efficient Cache Admission Policy
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Caching; eviction policy; admission policy; counting bloom filter;
   sketches
AB This article proposes to use a frequency-based cache admission policy in order to boost the effectiveness of caches subject to skewed access distributions. Given a newly accessed item and an eviction candidate from the cache, our scheme decides, based on the recent access history, whether it is worth admitting the new item into the cache at the expense of the eviction candidate.
   This concept is enabled through a novel approximate LFU structure called TinyLFU, which maintains an approximate representation of the access frequency of a large sample of recently accessed items. TinyLFU is very compact and lightweight as it builds upon Bloom filter theory.
   We study the properties of TinyLFU through simulations of both synthetic workloads and multiple real traces from several sources. These simulations demonstrate the performance boost obtained by enhancing various replacement policies with the TinyLFU admission policy. Also, a new combined replacement and eviction policy scheme nicknamed W-TinyLFU is presented. W-TinyLFU is demonstrated to obtain equal or better hit ratios than other state-of-the-art replacement policies on these traces. It is the only scheme to obtain such good results on all traces.
C1 [Einziger, Gil] Nokia Bell Labs, 16 Atir Yeda St, IL-4464321 Kefar Sava, Israel.
   [Friedman, Roy] Technion, Comp Sci Dept, IL-32000 Haifa, Israel.
C3 Technion Israel Institute of Technology
RP Einziger, G (corresponding author), Nokia Bell Labs, 16 Atir Yeda St, IL-4464321 Kefar Sava, Israel.
EM gilga1983@gmail.com; roy@cs.technion.ac.il; Ben.Manes@gmail.com
RI einziger, gil/AAB-3347-2021
FU Israeli Ministry of Science and Technology [10886]
FX Part of this work was supported by the Israeli Ministry of Science and
   Technology under grant #10886.
CR [Anonymous], 1968, MACM384 MIT
   [Anonymous], 1998, Online computation and competitive analysis
   [Anonymous], 2004, Linux journal
   [Anonymous], 1999, P 2 WORKSH INT SERV
   [Anonymous], 2012, COMPUTER ARCHITECTUR
   [Anonymous], 1998, IMPROVING WWW PROXIE
   [Anonymous], 1993, ACM SIGMOD RECORD, DOI DOI 10.1145/170035.170081
   [Anonymous], ELSEVIER COMPUTER NE
   [Anonymous], 2008, INFOCOM 2008 THE 27T
   [Anonymous], 2010, P 4 ACM INT WORKSHOP
   Ari I., 2006, P 23 IEEE 14 NASA GO, P263
   Arlitt M, 2000, PERFORM EVALUATION, V39, P149, DOI 10.1016/S0166-5316(99)00062-0
   Atzmon H, 2002, LECT NOTES COMPUT SC, V2519, P661
   Baek-Young Choi, 2002, Performance Evaluation Review, V30, P272, DOI 10.1145/511399.511376
   Bansal S, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P187
   BELADY LA, 1966, IBM SYST J, V5, P78, DOI 10.1147/sj.52.0078
   Bianchi G., 2010, P FUT NETW MOB SUMM
   Bianchi G, 2011, ACM SIGCOMM COMP COM, V41, P5, DOI 10.1145/2043165.2043167
   Bianchi Giuseppe, 2010, P IEEE INFOCOM
   Breslau L, 1999, IEEE INFOCOM SER, P126, DOI 10.1109/INFCOM.1999.749260
   Cao J, 2009, IEEE INFOCOM SER, P810, DOI 10.1109/INFCOM.2009.5061990
   Chai WK, 2012, LECT NOTES COMPUT SC, V7289, P27, DOI 10.1007/978-3-642-30045-5_3
   Cheng K, 2000, P INT COMP SOFTW APP, V24, P48, DOI 10.1109/CMPSAC.2000.884690
   Cheng X, 2008, INT WORKSH QUAL SERV, P249
   Chengchen Hu, 2010, Proceedings of the 2010 IEEE 30th International Conference on Distributed Computing Systems. ICDCS 2010, P665, DOI 10.1109/ICDCS.2010.57
   Chockler G, 2011, IBM J RES DEV, V55, DOI 10.1147/JRD.2011.2171649
   Cohen E, 2002, ALGORITHMICA, V33, P511, DOI 10.1007/s00453-002-0936-y
   Cohen S., 2003, P 2003 ACM SIGMOD IN, DOI DOI 10.1145/872757.872787
   Cormode G, 2004, LECT NOTES COMPUT SC, V2976, P29, DOI 10.1007/978-3-540-24698-5_7
   Dimitropoulos X, 2008, COMPUT NETW, V52, P3248, DOI 10.1016/j.comnet.2008.08.014
   Einziger Gil, 2015, 2015 IEEE Conference on Computer Communications (INFOCOM). Proceedings, P2560, DOI 10.1109/INFOCOM.2015.7218646
   Einziger G, 2016, COMPUT NETW, V99, P37, DOI 10.1016/j.comnet.2016.01.014
   Einziger G, 2015, 2015 INTERNATIONAL CONFERENCE ON COMPUTING, NETWORKING AND COMMUNICATIONS (ICNC), P255, DOI 10.1109/ICCNC.2015.7069350
   Einziger Gil, 2016, P 17 INT C DISTR COM
   Einziger Gil, 2015, P 24 INT C COMP COMM
   Estan C, 2002, ACM SIGCOMM COMP COM, V32, P323, DOI 10.1145/964725.633056
   Estan Cristian, 2004, COMPUT COMMUN REV, V34, P323
   Fan L, 2000, IEEE ACM T NETWORK, V8, P281, DOI 10.1109/90.851975
   Gill P, 2007, IMC'07: PROCEEDINGS OF THE 2007 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P15
   Goyal Amit, 2011, P 25 AAAI C ART INT
   Goyal Amit, 2012, P 2012 JOINT C EMP M, P1093
   Goyal Amit, 2010, Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, P17
   Huang Q, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P167, DOI 10.1145/2517349.2522722
   Jiang Song, 2005, P ANN C USENIX ANN T, P35
   Johnson T., 1994, P 20 INT C VER LARG, P439
   Karakostas George, 2002, P 7 INT S COMP COMM
   KAREDLA R, 1994, COMPUTER, V27, P38, DOI 10.1109/2.268884
   Kolaczkowski P, 2007, LECT NOTES ARTIF INT, V4585, P485, DOI 10.1007/978-3-540-73451-2_51
   Liberatore M., 2016, UMASS TRACE REPOSITO
   Lu Y, 2008, PERF E R SI, V36, P121, DOI 10.1145/1384529.1375472
   Manes Ben, 2016, CAFFEINE HIGH PERFOR
   Megiddo N, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE 2ND USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST'03), P115
   Megiddo N., 2006, US Patent, Patent No. 6996676
   Narayanasamy S, 2003, NINTH INTERNATIONAL SYMPOSIUM ON HIGH-PERFORMANCE COMPUTER ARCHITECTURE, PROCEEDINGS, P269, DOI 10.1109/HPCA.2003.1183545
   OpenBSD, 2014, TUQUEUE
   Podlipnig S, 2003, ACM COMPUT SURV, V35, P374, DOI 10.1145/954339.954341
   Psaras I., 2012, P 2 ED ICN WORKSH IN, P55, DOI DOI 10.1145/2342488.2342501
   Psounis K, 2002, IEEE ACM T NETWORK, V10, P441, DOI 10.1109/TNET.2002.801414
   Raspall F, 2008, IMC'08: PROCEEDINGS OF THE 2008 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P271
   Serpanos DN, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P727, DOI 10.1109/ICME.2000.871464
   Shah A. K., 2010, TECHNICAL REPORT
   Song Jiang, 2002, Performance Evaluation Review, V30, P31, DOI 10.1145/511399.511340
   Stanojevic R, 2007, IEEE INFOCOM SER, P2153, DOI 10.1109/INFCOM.2007.249
   Tewari Geetika, 2004, TR1304 HARV U
   Tsidon E, 2012, IEEE INFOCOM SER, P1889, DOI 10.1109/INFCOM.2012.6195564
   Williams S., 1996, Computer Communication Review, V26, P293, DOI 10.1145/248157.248182
   YOUNG N, 1991, PROCEEDINGS OF THE SECOND ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P241
   Zhou YY, 2001, USENIX ASSOCIATION PROCEEDINGS OF THE 2001 USENIX ANNUAL TECHNICAL CONFERENCE, P91
NR 68
TC 85
Z9 94
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD DEC
PY 2017
VL 13
IS 4
SI SI
AR 35
DI 10.1145/3149371
PG 31
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FR4LD
UT WOS:000419035900008
DA 2024-07-18
ER

PT J
AU Liu, QY
   Varman, P
AF Liu, Qingyue
   Varman, Peter
TI Ouroboros Wear Leveling for NVRAM Using Hierarchical Block Migration
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Wear leveling; NVRAM; lifetime endurance
ID PHASE-CHANGE MEMORY; CIRCUIT
AB Emerging nonvolatile RAM (NVRAM) technologies have a limit on the number of writes that can be made to any cell, similar to the erasure limits in NAND Flash. This motivates the need for wear leveling techniques to distribute the writes evenly among the cells. Unlike NAND Flash, cells in NVRAM can be rewritten without the need for erasing the entire containing block, avoiding the issues of space reclamation and garbage collection, motivating alternate approaches to the problem. In this article, we propose a hierarchical wear-leveling model called Ouroboros wear leveling. Ouroboros uses a two-level strategy whereby frequent low-cost intraregion wear leveling at small granularity is combined with interregion wear leveling at a larger time interval and granularity. Ouroboros is a hybrid migration scheme that exploits correct demand predictions in making better wear-leveling decisions while using randomization to avoid wear-leveling attacks by deterministic access patterns. We also propose a way to optimize wear-leveling parameter settings to meet a target smoothness level under limited time and space overhead constraints for different memory architectures and trace characteristics. Several experiments are performed on synthetically generated memory traces with special characteristics, two block-level storage traces, and two memory-line-level memory traces. The results show that Ouroboros wear leveling can distribute writes smoothly across the whole NVRAM with no more than 0.2% space overhead and 0.52% time overhead for a 512GB memory.
C1 [Liu, Qingyue; Varman, Peter] Rice Univ, Elec & Comp Eng MS-366,POB 1892, Houston, TX 77251 USA.
   [Varman, Peter] 2022 Duncan Hall,6100 Main St MS 380, Houston, TX 77251 USA.
C3 Rice University
RP Liu, QY (corresponding author), Rice Univ, Elec & Comp Eng MS-366,POB 1892, Houston, TX 77251 USA.
EM ql9@rice.edu; pjv@rice.edu
RI Liu, Qingyue/KMX-8241-2024
FU NSF [CCF 1439075]; Huawei Innovation Research Program [HIRPO20150401];
   Division of Computing and Communication Foundations; Direct For Computer
   & Info Scie & Enginr [1439075] Funding Source: National Science
   Foundation
FX This work is supported by NSF Grant CCF 1439075 and the Huawei
   Innovation Research Program (HIRPO20150401).
CR [Anonymous], 2011, FAST
   [Anonymous], T STORAGE
   Arimoto Y, 2004, MRS BULL, V29, P823, DOI 10.1557/mrs2004.235
   Ban A., 2004, Google Patents. US Patent, Patent No. [6,732,221, 6732221]
   BENAROYA A, 2006, EUR S ALG SPRING, V4168, P100
   Bo Zhao, 2013, THESIS
   Burr GW, 2010, J VAC SCI TECHNOL B, V28, P223, DOI 10.1116/1.3301579
   Chang LP, 2007, APPLIED COMPUTING 2007, VOL 1 AND 2, P1126, DOI 10.1145/1244002.1244248
   Chen CS, 2009, 2009 FIRST INTERNATIONAL CONFERENCE ON ADVANCES IN FUTURE INTERNET, P1, DOI [10.1109/CLEOE-EQEC.2009.5192905, 10.1109/AFIN.2009.8]
   Corporation Intel, 2016, PERS MEM PROGR
   Dong XY, 2008, DES AUT CON, P554
   Ferreira AP, 2010, DES AUT TEST EUROPE, P914
   Henning J. L., 2006, SIGARCH COMPUT ARCHI, V34, P1, DOI [DOI 10.1145/1186736.1186737, 10.1145/1186736.1186737]
   Hu JT, 2013, DES AUT TEST EUROPE, P599
   Huai Y., 2008, AAPPS B, V18, P6
   Ipek E., 2010, ACM SIGARCH COMPUTER, V38, P3, DOI DOI 10.1145/1735970.1736023
   Joo Yongsoo, P C DES AUT TEST EUR, P136
   Kultursay Emre, 2013, IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS 2013), P256
   Lee BC, 2010, COMMUN ACM, V53, P99, DOI 10.1145/1785414.1785441
   Lee BC, 2010, IEEE MICRO, V30, P131, DOI 10.1109/MM.2010.24
   Lee BC, 2009, CONF PROC INT SYMP C, P2, DOI 10.1145/1555815.1555758
   Lewis D.L., 2009, 3DIC, P1
   Liu D, 2013, ASIA S PACIF DES AUT, P279, DOI 10.1109/ASPDAC.2013.6509609
   Meena JS, 2014, NANOSCALE RES LETT, V9, DOI 10.1186/1556-276X-9-526
   Mittal S, 2015, IEEE COMPUT ARCHIT L, V14, P115, DOI 10.1109/LCA.2014.2355193
   Narayanan D., 2008, ACM T STORAGE, V4, P3
   Qureshi Moinuddin K., 2009, Proceedings of the 2009 42nd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO 2009), P14, DOI 10.1145/1669112.1669117
   Rozenberg MJ, 2004, PHYS REV LETT, V92, DOI 10.1103/PhysRevLett.92.178302
   Sangyeun Cho, 2009, Proceedings of the 2009 42nd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO 2009), P347
   Sheikholeslami A, 2000, P IEEE, V88, P667, DOI 10.1109/5.849164
   Wang G., 2014, Advanced Computer Architecture, P190
   Wong HSP, 2010, P IEEE, V98, P2201, DOI 10.1109/JPROC.2010.2070050
   Yang BD, 2007, IEEE INT SYMP CIRC S, P3014, DOI 10.1109/ISCAS.2007.377981
   Yun J, 2012, DES AUT TEST EUROPE, P1513
   Zhou P, 2009, CONF PROC INT SYMP C, P14, DOI 10.1145/1555815.1555759
NR 35
TC 1
Z9 1
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD DEC
PY 2017
VL 13
IS 4
SI SI
AR 30
DI 10.1145/3139530
PG 31
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FR4LD
UT WOS:000419035900003
OA Bronze
DA 2024-07-18
ER

PT J
AU Kang, JB
   Hu, CM
   Wo, TY
   Zhai, Y
   Zhang, BL
   Huai, JP
AF Kang, Junbin
   Hu, Chunming
   Wo, Tianyu
   Zhai, Ye
   Zhang, Benlong
   Huai, Jinpeng
TI MultiLanes: Providing Virtualized Storage for OS-Level Virtualization on
   Manycores
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Measurement; Performance; Fast storage; OS-level virtualization;
   manycores; scalability; performance isolation
AB OS-level virtualization is often used for server consolidation in data centers because of its high efficiency. However, the sharing of storage stack services among the colocated containers incurs contention on shared kernel data structures and locks within I/O stack, leading to severe performance degradation on manycore platforms incorporating fast storage technologies (e.g., SSDs based on nonvolatile memories).
   This article presents MultiLanes, a virtualized storage system for OS-level virtualization on manycores. MultiLanes builds an isolated I/O stack on top of a virtualized storage device for each container to eliminate contention on kernel data structures and locks between them, thus scaling them to manycores. Meanwhile, we propose a set of techniques to tune the overhead induced by storage-device virtualization to be negligible, and to scale the virtualized devices to manycores on the host, which itself scales poorly. To reduce the contention within each single container, we further propose SFS, which runs multiple file-system instances through the proposed virtualized storage devices, distributes all files under each directory among the underlying file-system instances, then stacks a unified namespace on top of them.
   The evaluation of our prototype system built for Linux container (LXC) on a 32-core machine with both a RAM disk and a modern flash-based SSD demonstrates that MultiLanes scales much better than Linux in micro-and macro-benchmarks, bringing significant performance improvements, and that MultiLanes with SFS can further reduce the contention within each single container.
C1 [Kang, Junbin; Hu, Chunming; Wo, Tianyu; Zhai, Ye; Zhang, Benlong; Huai, Jinpeng] Beihang Univ, POB 7-28,37 Xueyuan Rd, Beijing, Peoples R China.
C3 Beihang University
RP Hu, CM; Wo, TY (corresponding author), Beihang Univ, POB 7-28,37 Xueyuan Rd, Beijing, Peoples R China.
EM kangjb@act.buaa.edu.cn; hucm@act.buaa.edu.cn; woty@act.buaa.edu.cn;
   zhaiye@act.buaa.edu.cn; zblgeqian@gmail.com; huaijp@buaa.edu.cn
FU China 973 Program [2014CB340300]; China 863 Program [2013AA01A213]; NSFC
   [61170294, 91118008]; Beijing Higher Education Young Elite Teacher
   Project
FX This work is supported by China 973 Program (No. 2014CB340300), China
   863 Program (2013AA01A213), Projects from NSFC (No. 61170294, 91118008)
   and Beijing Higher Education Young Elite Teacher Project. An earlier
   version of this work was presented in Proceedings of the 12th USENIX
   conference on File and Storage Technologies (FAST 2014) [Kang et al.
   2014].
CR [Anonymous], P 17 INT C ARCH SUPP
   Appavoo J, 2007, ACM T COMPUT SYST, V25, DOI 10.1145/1275517.1275518
   Austin T., 2013, ACM SIGOPS 24 S OP S
   BANGA G, 1999, P 3 USENIX S OP SYST
   Baumann A., 2009, P 22 ACM S OP SYST P
   Bjorling Matias, 2013, 6 ANN INT SYST STOR
   BOYD- WICKIZER S, 2008, 8 USENIX S OP SYST D
   Boyd-Wickizer Silas, 2010, 9 USENIX S OP SYST D
   Bruno John L., 1998, 1998 USENIX ANN TECH
   BUGNION E, 1997, P 16 ACM S OP SYST P
   Cantrill Bryan., 2008, Queue, V6, P16, DOI DOI 10.1145/1454456.1454462
   Caulfield A. M., 2012, P 17 INT C ARCH SUPP
   Caulfield Adrian M., 2010, 43 ANN IEEE ACM INT
   Chapin J., 1995, P 15 ACM S OP SYST P
   Charles Gruenwald III, 2014, THESIS
   Chen Feng, 2011, 17 INT C HIGH PERF C
   Chidambaram Vijay, 2013, ACM SIGOPS 24 S OP S
   Chinner Dave, 2013, SYNC VFS SCALABILITY
   Chinner Dave, 2011, DENTRY MOVE PER SB L
   Cui Y, 2013, ACM T ARCHIT CODE OP, V9, DOI 10.1145/2400682.2400703
   David Tudor, 2013, ACM SIGOPS 24 S OP S
   Dickins Hugh, 2012, MM MEMCG PER MEMCG P
   DuyLe Hai Huang, 2012, P 10 USENIX C FIL ST
   Eqbal Rasha, 2014, THESIS
   Gamsa B, 1999, P 3 USENIX S OP SYST
   Kang J., 2014, P 12 USENIX C FIL ST
   Kang Junbin, 2015, 2015 USENIX ANN TECH
   Kolyshkin Kir, 2012, INTRO CONTAINER FILE
   Lu Lanyue, 2014, 11 USENIX S OP SYST
   Mavridis S, 2014, IEEE S MASS STOR SYS
   McKenney Paul E., 2001, OTT LIN S
   OSMAN S, 2002, 5 S OP SYST DES IMPL
   Patlasov Maxim, 2011, CONTAINERS IN A FILE
   Pendry Jan-Simon, 1995, USENIX 1995 TECHN C
   Qin Dai, 2014, 2014 USENIX ANN TECH
   Russell Rusty, 2008, Operating Systems Review, V42, P95, DOI 10.1145/1400097.1400108
   Seppanen E, 2010, IEEE S MASS STOR SYS
   Sfakianakis Y., 2014, P ACM S CLOUD COMP
   Sironi Filippo, 2015, P 10 EUR C COMP SYST
   Soltesz Stephen, 2007, P 2007 EUROSYS C
   SONG X., 2011, P 6 EUR C COMP SYST
   Sugerman J, 2001, P GEN TRACK 2001 USE
   Verghese Ben, 1998, ASPLOS 8 P 8 INT C A
   Wachs Matthew, 2007, 5 USENIX C FIL STOR
   Wright C. P., 2006, ACM Transaction on Storage, V2, P74, DOI 10.1145/1138041.1138045
   ZADOK E, 1999, P 1999 USENIX ANN TE
   Zheng Da, 2013, INT C HIGH PERF COMP
NR 47
TC 6
Z9 6
U1 2
U2 18
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUN
PY 2016
VL 12
IS 3
AR 12
DI 10.1145/2801155
PG 31
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DQ7YV
UT WOS:000379426200003
DA 2024-07-18
ER

PT J
AU Manzanares, A
   Qin, X
   Ruan, XJ
   Yin, S
AF Manzanares, Adam
   Qin, Xiao
   Ruan, Xiaojun
   Yin, Shu
TI PRE-BUD: Prefetching for Energy-Efficient Parallel I/O Systems with
   Buffer Disks
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Algorithms; Performance; Prefetching; energy conservation;
   buffer disks
ID SERVER; DESIGN
AB A critical problem with parallel I/O systems is the fact that disks consume a significant amount of energy. To design economically attractive and environmentally friendly parallel I/O systems, we propose an energy-aware prefetching strategy (PRE-BUD) for parallel I/O systems with disk buffers. We introduce a new architecture that provides significant energy savings for parallel I/O systems using buffer disks while maintaining high performance. There are two buffer disk configurations: (1) adding an extra buffer disk to accommodate prefetched data, and (2) utilizing an existing disk as the buffer disk. PRE-BUD is not only able to reduce the number of power-state transitions, but also to increase the length and number of standby periods. As such, PRE-BUD conserves energy by keeping data disks in the standby state for increased periods of time. Compared with the first prefetching configuration, the second configuration lowers the capacity of the parallel disk system. However, the second configuration is more cost-effective and energy-efficient than the first one. Finally, we quantitatively compare PRE-BUD with both disk configurations against three existing strategies. Empirical results show that PRE-BUD is able to reduce energy dissipation in parallel disk systems by up to 50 percent when compared against a non-energy aware approach. Similarly, our strategy is capable of conserving up to 30 percent energy when compared to the dynamic power management technique.
C1 [Manzanares, Adam; Qin, Xiao; Ruan, Xiaojun; Yin, Shu] Auburn Univ, Shelby Ctr Engn Technol, Dept Comp Sci & Software Engn, Auburn, AL 36849 USA.
C3 Auburn University System; Auburn University
RP Qin, X (corresponding author), Auburn Univ, Shelby Ctr Engn Technol, Dept Comp Sci & Software Engn, Suite 3101, Auburn, AL 36849 USA.
EM xqin@auburn.edu
RI Yin, Shu/AGO-5485-2022
OI Yin, Shu/0000-0002-5449-4937
FU US National Science Foundation [CCF-0742187, CNS-0757778, CNS-0831502,
   OCI-0753305, DUE-0621307]; Auburn University; Direct For Computer & Info
   Scie & Enginr; Division Of Computer and Network Systems [0917137]
   Funding Source: National Science Foundation; Direct For Computer & Info
   Scie & Enginr; Division of Computing and Communication Foundations
   [0845257] Funding Source: National Science Foundation
FX This work was supported by the US National Science Foundation under
   Grants CCF-0742187, CNS-0757778, CNS-0831502, OCI-0753305, and
   DUE-0621307, and by Auburn University under a startup grant.
CR Benini L, 2000, IEEE T VLSI SYST, V8, P299, DOI 10.1109/92.845896
   Carrera E.V., 2003, Proceedings of the 17th annual international conference on Supercomputing, ICS '03, P86
   Chen F, 2007, INT CONF PARA PROC, P79
   Colarelli Dennis., 2002, SUPERCOMPUTING 02, P1, DOI DOI 10.1109/SC.2002.10058
   DOUGLIS F, 1994, PROCEEDINGS OF THE WINTER 1994 USENIX CONFERENCE, P293
   Gurumurthi S, 2003, CONF PROC INT SYMP C, P169, DOI 10.1109/ISCA.2003.1206998
   Hawkins J, 2005, IEEE ACM T COMPUT BI, V2, P243, DOI 10.1109/TCBB.2005.44
   Helmbold D., 1998, Mobile Networks and Applications, V5, P2000
   Hyeonsang Eom, 2000, Proceedings 14th International Parallel and Distributed Processing Symposium. IPDPS 2000, P315, DOI 10.1109/IPDPS.2000.846001
   Kallahalla M, 2002, IEEE T COMPUT, V51, P1333, DOI 10.1109/TC.2002.1047757
   Kim YJ, 2007, APPLIED COMPUTING 2007, VOL 1 AND 2, P693
   Krishnan P., 1995, ADAPTIVE DISK SPINDO
   KWAN TT, 1995, COMPUTER, V28, P68, DOI 10.1109/2.471181
   LI K, 1994, PROCEEDINGS OF THE WINTER 1994 USENIX CONFERENCE, P279
   Moore B., 2002, ENERGY USER NEWS
   Papathanasiou AthanasiosE., 2004, Proceedings of USENIX Annual Technical Conference, P22
   Pinheiro E., 2004, INT C SUPERCOMPUTING, P68, DOI DOI 10.1145/1006209.1006220
   Power Heat and Sledgehammer, 2002, POW HEAT SLEDG
   Shen H., 2004, Proceedings. 18th International Parallel and Distributed Processing Symposium
   SON S.W., 2006, Proceedings of the 3rd conference on Computing frontiers, P105
   Trizna DB, 2005, OCEANS-IEEE, P532
   Wang J, 2008, IEEE T COMPUT, V57, P359, DOI 10.1109/TC.2007.70821
   Xie T, 2008, IEEE T COMPUT, V57, P748, DOI 10.1109/TC.2008.27
   Zhu Q, 2005, P 20 ACM S OP SYST P, P177, DOI DOI 10.1145/1095809.1095828
   Zhu Q., 2004, P 18 ANN INT C SUPER, P79
   Zhu QB, 2004, 10TH INTERNATIONAL SYMPOSIUM ON HIGH PERFORMANCE COMPUTER ARCHITECTURE, PROCEEDINGS, P118
   Zhuang XT, 2004, ACM SIGPLAN NOTICES, V39, P67, DOI 10.1145/998300.997174
   Zong Ziliang., 2007, PARALLEL DISTRIBUTED, P1
NR 28
TC 15
Z9 22
U1 1
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUN
PY 2011
VL 7
IS 1
AR 3
DI 10.1145/1970343.1970346
PG 29
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 990LE
UT WOS:000307631600003
DA 2024-07-18
ER

PT J
AU Sehgal, P
   Tarasov, V
   Zadok, E
AF Sehgal, Priya
   Tarasov, Vasily
   Zadok, Erez
TI Optimizing Energy and Performance for Server-Class File System Workloads
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Experimentation; Measurement; Performance; Benchmarks; file
   systems; storage systems; energy efficiency
AB Recently, power has emerged as a critical factor in designing components of storage systems, especially for power-hungry data centers. While there is some research into power-aware storage stack components, there are no systematic studies evaluating each component's impact separately. Various factors like workloads, hardware configurations, and software configurations impact the performance and energy efficiency of the system. This article evaluates the file system's impact on energy consumption and performance. We studied several popular Linux file systems, with various mount and format options, using the FileBench workload generator to emulate four server workloads: Web, database, mail, and fileserver, on two different hardware configurations. The file system design, implementation, and available features have a significant effect on CPU/disk utilization, and hence on performance and power. We discovered that default file system options are often suboptimal, and even poor. In this article we show that a careful matching of expected workloads and hardware configuration to a single software configuration-the file system-can improve power-performance efficiency by a factor ranging from 1.05 to 9.4 times.
C1 [Sehgal, Priya] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Stony Brook
RP Sehgal, P (corresponding author), SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
EM psehgal@fsl.cs.sunysb.edu
FU NSF [CCF-0621463, CCF-0937854]; IBM; Direct For Computer & Info Scie &
   Enginr; Division of Computing and Communication Foundations [0937833]
   Funding Source: National Science Foundation
FX This work was made possible in part thanks to NSF awards CCF-0621463 and
   CCF-0937854, 2008 IBM Faculty award, and 2009 NetApp gift.
CR ALLALOUF M., 2009, P ISR EXP SYST C SYS
   Almeida J., 1996, MEASURING BEHAV WORL
   Andersen DG, 2009, SOSP'09: PROCEEDINGS OF THE TWENTY-SECOND ACM SIGOPS SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P1
   [Anonymous], P WORKSH POW AW COMP
   APPLETON R., 1997, LINUX J
   BISSON T., 2007, P IEEE PERF COMP COM
   BRYANT R., 2002, P ANN USENIX TECHN C, P259
   Capps D., 2008, Iozone filesystem benchmark
   CARRERA E., 2003, P 17 INT C SUP
   Colarelli Dennis., 2002, SUPERCOMPUTING 02, P1, DOI DOI 10.1109/SC.2002.10058
   Craven M, 2005, TWENTY-SECOND IEEE/THIRTEENTH NASA GODDARD CONFERENCE ON MASS STORAGE SYSTEMS AND TECHNOLOGIES, PROCEEDINGS, P237, DOI 10.1109/MSST.2005.25
   DENG Y., 2008, INFORM SCI
   DOUGLIS F, 1994, PROCEEDINGS OF THE WINTER 1994 USENIX CONFERENCE, P293
   ELECTRONIC EDUCATIONAL DEVICES, WATTS PRO ES POW MET
   Elnozahy EN, 2003, LECT NOTES COMPUT SC, V2325, P179
   ERMOLINSKIY A., 2009, UCBEECS200940
   Essary D., 2008, Trans. Storage, V4, P1, DOI DOI 10.1145/1353452.1353454
   Gulati A, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 5TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES ( FAST '07), P199
   Gurumurthi S, 2003, CONF PROC INT SYMP C, P169, DOI 10.1109/ISCA.2003.1206998
   Gurumurthi S, 2003, INT SYM PERFORM ANAL, P123, DOI 10.1109/ISPASS.2003.1190239
   Huang H., 2005, SIGOPS OPER SYST REV, V39, P263, DOI [10.1145/1095809.1095836, DOI 10.1145/1095809.1095836]
   JOUKOV N., 2008, P 3 ACM SIGOPS EUROS
   Joukov N, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P89
   KATCHER J., 1997, TR3022 NETW APL
   KOTHIYAL R., 2009, P ISR EXP SYST C ACM
   LI D., 2006, THESIS U NEBRASKA LI
   LI K, 1994, PROCEEDINGS OF THE WINTER 1994 USENIX CONFERENCE, P279
   MANZANARES A., 2008, P IEEE INT S PAR DIS, P1
   McDougall R., 2007, SOLARIS PERFORMANCE
   Narayanan D, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P253
   Nightingale EB, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE SIXTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDE '04), P363
   PAPATHANASIOU A. E., 2002, 792 U ROCH
   PINHEIRO E., 2001, P INT C PAR ARCH COM
   Pinheiro E., 2004, INT C SUPERCOMPUTING, P68, DOI DOI 10.1145/1006209.1006220
   Reiser H., REISERFS V 3 WHITEPA
   Rivoire S., 2007, P ACM SIGMOD INT C M
   Seltzer M. I., 1993, Proceedings. Ninth International Conference on Data Engineering (Cat. No.92CH3258-1), P503, DOI 10.1109/ICDE.1993.344029
   SGI, XFS FIL STRUCT
   Solaris Internals Wiki, 2008, FILEBENCH
   SPEC, 2005, SPECWEB99
   SPEC, 2008, SPECPOWER SSJ2008 V1
   SPEC, 2008, SPECSFS2008
   THE STANDARD PERFORMANCE EVALUATION CORPORATION, 2004, SPEC HPC SUIT
   Tweedie Stephen, 2000, P OTT LIN S
   Wang J, 2008, IEEE T COMPUT, V57, P359, DOI 10.1109/TC.2007.70821
   Washburn D., 2008, More Energy Is Consumed Outside Of The Data Center
   WEISER M, 1994, OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI), P13
   Wikipedia, EXT3
   Wright CP, 2005, USENIX ASSOCIATION PROCEEDINGS OF THE FREENIX/OPEN SOURCE TRACK, P175
   Zhu QB, 2004, 10TH INTERNATIONAL SYMPOSIUM ON HIGH PERFORMANCE COMPUTER ARCHITECTURE, PROCEEDINGS, P118
NR 50
TC 7
Z9 10
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD SEP
PY 2010
VL 6
IS 3
AR 10
DI 10.1145/1837915.1837918
PG 31
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA V24QL
UT WOS:000208424800003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yu, YJ
   Shin, DI
   Eom, H
   Yeom, HY
AF Yu, Young Jin
   Shin, Dong In
   Eom, Hyeonsang
   Yeom, Heon Young
TI NCQ vs. I/O Scheduler: Preventing Unexpected Misbehaviors
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Measurement; Performance; NCQ; SATA 2; hybrid scheduling; starvation
   detection; I/O prioritization
ID PERFORMANCE
AB Native Command Queueing (NCQ) is an optimization technology to maximize throughput by reordering requests inside a disk drive. It has been so successful that NCQ has become the standard in SATA 2 protocol specification, and the great majority of disk vendors have adopted it for their recent disks. However, there is a possibility that the technology may lead to an information gap between the OS and a disk drive. A NCQ-enabled disk tries to optimize throughput without realizing the intention of an OS, whereas the OS does its best under the assumption that the disk will do as it is told without specific knowledge regarding the details of the disk mechanism. Let us call this expectation discord, which may cause serious problems such as request starvations or performance anomaly. In this article, we (1) confirm that expectation discord actually occurs in real systems; (2) propose software-level approaches to solve them; and (3) evaluate our mechanism. Experimental results show that our solution is simple, cheap (no special hardware required), portable, and effective.
C1 [Yu, Young Jin; Shin, Dong In; Eom, Hyeonsang; Yeom, Heon Young] Seoul Natl Univ, Distributed Comp Syst Lab, Seoul 151742, South Korea.
C3 Seoul National University (SNU)
RP Eom, H (corresponding author), Seoul Natl Univ, Distributed Comp Syst Lab, Seoul 151742, South Korea.
EM yjyu@dcslab.snu.ac.kr; dishin@dcslab.snu.ac.kr; hseom@cse.snu.ac.kr;
   yeom@snu.ac.kr
FU Institute of Computer Technology at Seoul National University
FX This research was supported by the Institute of Computer Technology at
   Seoul National University.
CR ABBOTT RK, 1990, PROCEEDINGS : 11TH REAL-TIME SYSTEMS SYMPOSIUM, P113, DOI 10.1109/REAL.1990.128736
   [Anonymous], MITLCSTM528
   [Anonymous], P USENIX ANN TECHN C
   [Anonymous], 2007, SCSI ARCH MOD 4 SAM
   [Anonymous], 2007, SCSI PRIM COMM 4 SPC
   [Anonymous], 2007, SCSI BLOCK COMM 3 SB
   [Anonymous], 1997, TR3022 NETW APPL INC
   BRUNO J., 1999, P IEEE INT C MICR CO
   CAREY M. J., 1989, P 15 INT C VER LARG
   CHEN SZ, 1991, REAL-TIME SYST, V3, P307, DOI 10.1007/BF00364960
   DE JONGE W., 1993, P 13 S OP SYST PRINC
   Dees B, 2005, IEEE POTENTIALS, V24, P4, DOI 10.1109/MP.2005.1549750
   Denehy TE, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE GENERAL TRACK, P177
   GANGER GR, 2001, CMUCS01166
   Gill BS, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 5TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES ( FAST '07), P185
   GRIMSRUD K., 2007, SATA IO FEATURES MOV
   Gulati A., 2007, HPL2007186
   GURUN S., 2005, P ACM INT C EMB SOFT
   HALL C., 2005, P 31 INT C VER LARG
   HELLERSTEIN J., 1998, SIGMOD REC, V27, P3
   Huang L., 2000, ECSLTR81 SUNY
   HUFFMAN A., 2007, 12 INT CORP
   HUFFMAN A., 2003, COMP SERIAL ATA NATI
   Intel and Seagate, 2003, SER ATA NAT COMM QUE
   Iyer S., 2001, Operating Systems Review, V35, P117, DOI 10.1145/502059.502046
   JACOBSON D, 1991, HPLCSP917REV1
   Kaldewey T, 2008, PROCEEDINGS OF THE 14TH IEEE REAL-TIME AND EMBEDDED TECHNOLOGY AND APPLICATIONS SYMPOSIUM, P319, DOI 10.1109/RTAS.2008.31
   LI C, 2004, P 1 WORKSH OP SYST A
   Li MJ, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P81
   Lumb C.R., 2000, P 4 S OP SYST DES IM
   MCWHERTER D. T., 2004, P 20 INT C DAT ENG I
   Mesnier M, 2003, IEEE COMMUN MAG, V41, P84, DOI 10.1109/MCOM.2003.1222722
   MICROSOFT, 2006, I O PRIOR WIND VIST
   Ongaro Diego, 2008, P 4 ACM SIGPLAN SIGO
   PANASAS, OBJ STOR ARCH
   REUTHER L, 2003, P 24 IEEE INT REAL T
   RIEDEL E., 2000, P ACM SIGMOD INT C M
   Riedel Erik, 1998, P 24 INT C VER LARG
   RUEMMLER C, 1994, COMPUTER, V27, P17, DOI 10.1109/2.268881
   SATAIO, 2007, SER ATA INT ORG SER
   SATAIO, 2005, SER ATA INT ORG SER
   SEELAM S., 2005, P LIN S
   SELTZER M., 1990, P USENIX WINT TECHN
   SHENOY P. J., 1998, P ACM SIGMETRICS JOI
   SHIN D. I., 2007, P 15 ANN M IEEE INT
   Sivathanu G., 2006, P 7 S OP SYST DES IM
   Sivathanu M, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE 2ND USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST'03), P73
   TALAGALA N, 1999, CSD991063 U CAL
   TRAEGER A., 2008, ACM T STORAGE, V4, P2
   WANG R, 1999, P 3 S OP SYST DES IM
   WANG Y., 2006, NCQ POWER EFFICIENCY
   WILKES J., 1995, P ACM SIGMETRICS JOI
   Worthington B. L., 1994, P ACM SIGMETRICS C M, P241
   WRIGHT C. P., 2005, P ANN USENIX TECHN C
   Youjip Won, 2006, ACM Transaction on Storage, V2, P255, DOI 10.1145/1168910.1168912
NR 55
TC 15
Z9 16
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAR
PY 2010
VL 6
IS 1
AR 2
DI 10.1145/1714454.1714456
PG 37
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA V24QF
UT WOS:000208424200002
DA 2024-07-18
ER

PT J
AU Xie, XC
   Xiao, LQ
   Du, DHC
AF Xie, Xuchao
   Xiao, Liquan
   Du, David H. C.
TI ZoneTier: A Zone-based Storage Tiering and Caching Co-design to
   Integrate SSDs with SMR Drives
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE ZoneTier; tiering and caching co-design; host-aware; shingled magnetic
   recording (SMR)
AB Integrating solid-state drives (SSDs) and host-aware shingled magnetic recording (HA-SMR) drives can potentially build a cost-effective high-performance storage system. However, existing SSD tiering and caching designs in such a hybrid system are not fully matched with the intrinsic properties of HA-SMR drives due to their lacking consideration of how to handle non-sequential writes (NSWs). We propose ZoneTier, a zone-based storage tiering and caching co-design, to effectively control all the NSWs by leveraging the host-aware property of HA-SMR drives. ZoneTier exploits real-time data layout of SMR zones to optimize zone placement, reshapes NSWs generated from zone demotions to SMR preferred sequential writes, and transforms the inevitable NSWs to cleaning-friendly write traffics for SMR zones. ZoneTier can be easily extended to match host-managed SMR drives using proactive cleaning policy. We implemented a prototype of ZoneTier with user space data management algorithms and real SSD and HA-SMR drives, which are manipulated by the functions provided by libzbc and libaio. Our experiments show that ZoneTier can reduce zone relocation overhead by 29.41% on average, shorten performance recovery time of HA-SMR drives from cleaning by up to 33.37%, and improve performance by up to 32.31% than existing hybrid storage designs.
C1 [Xie, Xuchao; Xiao, Liquan] Natl Univ Def Technol, Coll Comp, Changsha 410073, Hunan, Peoples R China.
   [Du, David H. C.] Univ Minnesota Twin Cities, Dept Comp Sci, Minneapolis, MN 55455 USA.
C3 National University of Defense Technology - China; University of
   Minnesota System; University of Minnesota Twin Cities
RP Xie, XC (corresponding author), Natl Univ Def Technol, Coll Comp, Changsha 410073, Hunan, Peoples R China.
EM xiexuchao@nudt.edu.cn; xiaoliquan@nudt.edu.cn; du@umn.edu
FU NSF [1439622, 152567, 1812537]; China Scholarship Council
FX This work has been supported by NSF Awards No. 1439622, No. 152567, and
   No. 1812537. Xuchao Xie was supported by the China Scholarship Council.
CR Abashkin Mark, 2015, 2015 IEEE 34th International Performance Computing and Communications Conference (IPCCC), P1, DOI 10.1109/PCCC.2015.7410323
   Aghayev A, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P105
   Aghayev A, 2015, ACM T STORAGE, V11, DOI 10.1145/2821511
   Blackwell Trevor., 1995, P USENIX 1995 TECHNI, P23
   Chen F., 2011, Proc. of ACM Int. Conf. on Supercomputing, P22
   Feldman T., 2013, Login Mag. USENIX SAGE, V38, P22
   Guerra J., 2011, P 9ST USENIX C FAST, P20
   Haghdoost A, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P315
   He WP, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P121
   Huang SB, 2016, HEAD FACE MED, V12, DOI 10.1186/s13005-015-0098-1
   Jin C, 2014, IEEE S MASS STOR SYS
   Jones SN, 2016, ACM T STORAGE, V12, DOI 10.1145/2851505
   Kim HJ, 2014, ACM T STORAGE, V10, DOI 10.1145/2668128
   Li C., 2014, USENIX ANN TECH C AT, P501
   Li Cheng, 2014, P USENIX WORKSH HOT
   Macko Peter, 2017, P 34 S MASS STOR SYS
   Manzanares A., 2016, P 8 USENIX WORKSH HO
   Megiddo N, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE 2ND USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST'03), P115
   Mesnier M, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P57
   Microsoft, REFS OV
   Murata H, 2013, I S INTELL SIG PROC, P51, DOI 10.1109/ISPACS.2013.6704521
   Ni Yuanjiang, 2016, P 9 ACM INT SYST STO, P8
   Oe K, 2015, INT SYMPOS COMPUT NE, P371, DOI 10.1109/CANDAR.2015.42
   Oe K, 2014, 2014 SECOND INTERNATIONAL SYMPOSIUM ON COMPUTING AND NETWORKING (CANDAR), P406, DOI 10.1109/CANDAR.2014.90
   Park Dongchul, 2012, P 10 USENIX C FIL ST
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   Saxena M., 2012, P 7 ACM EUR C COMP S, P267, DOI DOI 10.1145/2168836.2168863
   Song Jiang, 2002, Performance Evaluation Review, V30, P31, DOI 10.1145/511399.511340
   Suresh Anand, 2012, L12105 CMUPD
   WANG CY, 2017, SENSORS BASEL, V17, DOI DOI 10.3390/S17122946
   Wu FG, 2017, IEEE T COMPUT, V66, P1932, DOI 10.1109/TC.2017.2713360
   Wu Fenggang, 2016, P 8 USENIXWORKSHOP H
   Xiao WJ, 2016, PR IEEE COMP DESIGN, P64, DOI 10.1109/ICCD.2016.7753262
   Xie XC, 2018, PROC INT CONF PARAL, DOI 10.1145/3225058.3225067
   Xie XC, 2018, IEEE ACCESS, V6, P20916, DOI 10.1109/ACCESS.2018.2825109
   Xu WX, 2014, FRONT COMPUT SCI-CHI, V8, P367, DOI 10.1007/s11704-014-3499-6
   Yang JL, 2013, ADV MATER RES-SWITZ, V826, P10, DOI 10.4028/www.scientific.net/AMR.826.10
   Yang MC, 2019, IEEE T COMPUT, V68, P111, DOI 10.1109/TC.2018.2845383
   Yang MC, 2017, ICCAD-IEEE ACM INT, P17, DOI 10.1109/ICCAD.2017.8203755
   Zhou YY, 2001, USENIX ASSOCIATION PROCEEDINGS OF THE 2001 USENIX ANNUAL TECHNICAL CONFERENCE, P91
NR 40
TC 11
Z9 14
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2019
VL 15
IS 3
AR 19
DI 10.1145/3335548
PG 25
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JT2DC
UT WOS:000500805400004
DA 2024-07-18
ER

PT J
AU Xie, W
   Chen, Y
   Roth, PC
AF Xie, Wei
   Chen, Yong
   Roth, Philip C.
TI Exploiting Internal Parallelism for Address Translation in Solid-State
   Drives
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Flash translation layer; SSD; parallelism; DFTL; address translation
ID FLASH; PERFORMANCE
AB Solid-state Drives (SSDs) have changed the landscape of storage systems and present a promising storage solution for data-intensive applications due to their low latency, high bandwidth, and low power consumption compared to traditional hard disk drives. SSDs achieve these desirable characteristics using internal parallelism-parallel access to multiple internal flash memory chips-and a Flash Translation Layer (FTL) that determines where data are stored on those chips so that they do not wear out prematurely. However, current state-of-the-art cache-based FTLs like the Demand-based Flash Translation Layer (URI) do not allow IO schedulers to take full advantage of internal parallelism, because they impose a tight coupling between the logical-to-physical address translation and the data access. To address this limitation, we introduce a new FTL design called Parallel-DFTL that works with the DFTL to decouple address translation operations from data accesses. Parallel-DFTL separates address translation and data access operations into different queues, allowing the SSD to use concurrent flash accesses for both types of operations. We also present a ParallelLRU cache replacement algorithm to improve the concurrency of address translation operations. To compare Parallel-DF11 against existing RI approaches, we present a Parallel-DIM. performance model and compare its predictions against those for DFI'L and an ideal page-mapping approach. We also implemented the Parallel-DFTL approach in an SSD simulator using real device parameters, and used trace-driven simulation to evaluate Parallel-DFTL's efficacy. Our evaluation results show that Parallel-DFTL improved the overall performance by up to 32% for the real IO workloads we tested, and by up to two orders of magnitude with synthetic test workloads. We also found that Parallel-DFTL is able to achieve reasonable performance with a very small cache size and that it provides the best benefit for those workloads with large request size or with high write ratio.
C1 [Xie, Wei; Chen, Yong] Texas Tech Univ, 902 Boston Ave, Lubbock, TX 79409 USA.
   [Roth, Philip C.] Oak Ridge Natl Lab, One Bethel Valley Rd,POB 2008 MS 6173, Oak Ridge, TN 37831 USA.
C3 Texas Tech University System; Texas Tech University; United States
   Department of Energy (DOE); Oak Ridge National Laboratory
RP Xie, W (corresponding author), Texas Tech Univ, 902 Boston Ave, Lubbock, TX 79409 USA.
EM wei.xie@ttu.edu; yong.chen@ttu.edu; rothpc@ornl.gov
OI Chen, Yong/0000-0002-9961-9051
FU U.S. Department of Energy, Office of Science, Office of Advanced
   Scientific Computing Research; National Science Foundation [CNS-1162488,
   CNS-1338078, IIP-1362134, CCF-1409946, CCF-1718336]
FX This material is based on work supported by the U.S. Department of
   Energy, Office of Science, Office of Advanced Scientific Computing
   Research. This research is supported by the National Science Foundation
   under grant CNS-1162488, CNS-1338078, IIP-1362134, CCF-1409946, and
   CCF-1718336.
CR Agrawal Nitin, 2008, P USENIX ANN TECHN C, P57
   Albrecht Christoph, 2013, P USENIX ANN TECHN C
   [Anonymous], 2016, FIN IO TRAC
   [Anonymous], 2016, EXCHANGE1 BLOCK IO T
   [Anonymous], 2012, PROCEEDING IEEE VEHI
   [Anonymous], 2017, DRAMLESS SSD ROUNDUP
   Bux W., 2010, Performance_Evaluation
   Chen F., 2011, Proc. of ACM Int. Conf. on Supercomputing, P22
   Chen Feng, P IEEE 17 INT S HIGH
   Chiang ML, 1999, J SYST SOFTWARE, V48, P213, DOI 10.1016/S0164-1212(99)00059-X
   Chiang ML, 1997, ISCE '97 - PROCEEDINGS OF 1997 IEEE INTERNATIONAL SYMPOSIUM ON CONSUMER ELECTRONICS, P177, DOI 10.1109/ISCE.1997.658381
   Chiang ML, 1999, SOFTWARE PRACT EXPER, V29, P267, DOI 10.1002/(SICI)1097-024X(199903)29:3<267::AID-SPE233>3.0.CO;2-T
   Dai Dong, 2014, P ACM IEEE SUP C SC
   Desnoyers Peter., 2012, P INT SYSTEMS STORAG, P12
   Dirik Cagdas, 2009, P ACM SIGARCH COMP A
   Gao CM, 2014, IEEE S MASS STOR SYS
   Gupta Aayush., 2009, DFTL: a flash translation layer employing demand-based selective caching of page-level address mappings, V44-3
   Hsieh JW, 2014, IEEE T COMPUT, V63, P3079, DOI 10.1109/TC.2013.169
   Hu J, 2010, IEEE INT SYMP CIRC S, P85, DOI 10.1109/ISCAS.2010.5537028
   Hu X.-Y., 2009, System and Storage Conference (SYSTOR), DOI DOI 10.1145/1534530.1534544
   Hu Y, 2011, P 25 INT C SUPERCOMP
   Hu Yang, P 2010 IEEE 26 S MAS
   Jen-Wei Hsieh, 2006, ACM Transaction on Storage, V2, P22, DOI 10.1145/1138041.1138043
   Jung Myoungsoo, P 2014 IEEE 20 INT S
   Jung Myoungsoo, 2012, P 4 USENIX C HOT TOP
   Jung Myoungsoo, P 39 ANN INT S COMP
   Kim JW, 2002, IEEE T CONSUM ELECTR, V48, P275, DOI 10.1109/TCE.2002.1010132
   Kim Y, 2009, SIMUL: 2009 FIRST INTERNATIONAL CONFERENCE ON ADVANCES IN SYSTEM SIMULATION, P125, DOI 10.1109/SIMUL.2009.17
   Lee SW, 2007, ACM T EMBED COMPUT S, V6, DOI 10.1145/1275986.1275990
   Liu Jialin, 2013, P IEEE INT C BIG DAT
   Liu Ning., 2012, 28th Symposium on Mass Storage Systems and Technologies MSST, P1
   Lu Yin, 2014, P 28 INT C SUP ICS 1
   Ma Dongzhe, 2011, P 2011 ACM SIGMOD IN
   Min C., 2012, P 10 USENIX C FIL ST, P1
   Nam EH, 2011, IEEE T COMPUT, V60, P653, DOI 10.1109/TC.2010.209
   Park C., 2010, COMPUT ARCH LETT, V9
   Park Dongchul, 2011, P IEEE 27 S MASS STO, P1
   Sang-Phil Lim, 2010, Proceedings of the 2010 International Workshop on Storage Network Architecture and Parallel I/Os (SNAPI 2010), P3, DOI 10.1109/SNAPI.2010.9
   Seong YJ, 2010, IEEE T COMPUT, V59, P905, DOI 10.1109/TC.2010.63
   Shimpi A. L, 2013, SAMSUNG SSD 840 EVO
   Shin I, 2011, IEEE T CONSUM ELECTR, V57, P1728, DOI 10.1109/TCE.2011.6131147
   Thakur R, 1999, FRONTIERS '99 - THE SEVENTH SYMPOSIUM ON THE FRONTIERS OF MASSIVELY PARALLEL COMPUTATION, PROCEEDINGS, P182, DOI 10.1109/FMPC.1999.750599
   Van Houdt Benny, 2013, ACM SIGMETRICS Performance Evaluation Review
   Wei QS, 2014, IEEE S MASS STOR SYS
   Wei Qingsong., 2011, S MASS STORAGE SYSTE, P1, DOI DOI 10.1109/MSST.2011.5937217
   Xie W, 2017, PARALLEL COMPUT, V61, P3, DOI 10.1016/j.parco.2016.10.006
   Xie W, 2015, PROCEEDINGS 2015 IEEE INTERNATIONAL CONFERENCE ON BIG DATA, P327, DOI 10.1109/BigData.2015.7363772
   Xie Wei, 2016, P 11 IEEE INT C NETW
   Xie Wei, 2017, P 31 IEEE INT PAR DI
NR 49
TC 7
Z9 7
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD DEC
PY 2018
VL 14
IS 4
AR 32
DI 10.1145/3239564
PG 30
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ4JO
UT WOS:000457140500004
OA Bronze
DA 2024-07-18
ER

PT J
AU Chen, YM
   Shu, JW
   Ou, JX
   Lu, YY
AF Chen, Youmin
   Shu, Jiwu
   Ou, Jiaxin
   Lu, Youyou
TI HiNFS: A Persistent Memory File System with Both Buffering and
   Direct-Access
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Persistent memory; file system; direct access; buffering
AB Persistent memory provides data persistence at main memory with emerging non-volatile main memories (NVMMs). Recent persistent memory file systems aggressively use direct access, which directly copy data between user buffer and the storage layer, to avoid the double-copy overheads through the OS page cache. However, we observe they all suffer from slow writes due to NVMMs' asymmetric read-write performance and much slower performance than DRAM.
   In this article, we propose HiNFS, a high-performance file system for non-volatile main memory, to combine both buffering and direct access for fine-grained file system operations. HiNFS uses an NVMM-aware Write Buffer to buffer the lazy-persistent file writes in DRAM, while performing direct access to NVMM for eager-persistent file writes. It directly reads file data from both DRAM and NVMM, by ensuring read consistency with a combination of the DRAM Block Index and Cacheline Bitmap to track the latest data between DRAM and NVMM. HiNFS also employs a Buffer Benefit Model to identify the eager-persistent file writes before issuing I/Os. Evaluations show that HiNFS significantly improves throughput by up to 184% and reduces execution time by up to 64% comparing with state-of-the-art persistent memory file systems PMFS and EXT4-DAX.
C1 [Chen, Youmin; Shu, Jiwu; Ou, Jiaxin; Lu, Youyou] Tsinghua Univ, Dept Comp Sci & Technol, Beijing, Peoples R China.
C3 Tsinghua University
RP Lu, YY (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, Beijing, Peoples R China.
EM chenym16@mails.tsinghua.edu.cn; shujw@tsinghua.edu.cn;
   ojx11@mails.tsinghua.edu.cn; luyouyou@tsinghua.edu.cn
RI Lu, Youyou/AAE-4401-2020
OI Lu, Youyou/0000-0002-6214-5390; Chen, Youmin/0000-0003-4171-4299
FU National Natural Science Foundation of China [61772300, 61232003];
   Beijing Municipal Science and Technology Commission of China
   [D151100000815003]; China Postdoctoral Science Foundation [2016T90094,
   2015M580098]; Young Elite Scientists Sponsorship Program of China
   Association for Science and Technology (CAST)
FX This work is supported by the National Natural Science Foundation of
   China (Grant No. 61772300, 61232003), the Beijing Municipal Science and
   Technology Commission of China (Grant No. D151100000815003), China
   Postdoctoral Science Foundation (Grant No. 2016T90094, 2015M580098).
   Youyou Lu is also supported by the Young Elite Scientists Sponsorship
   Program of China Association for Science and Technology (CAST).
CR [Anonymous], 2013, P 1 ACM SIGOPS C TIM
   [Anonymous], 1973, OPERATING SYSTEMS TH
   [Anonymous], J EMERG TECHNOL COMP
   [Anonymous], TR3022 NETW APPL INC
   [Anonymous], 2011, FAST
   [Anonymous], EUROSYS
   [Anonymous], P S OP SYST PRINC SO
   [Anonymous], P ACM SIGOPS 22 ANN
   [Anonymous], P 6 USENIX C FIL STO
   [Anonymous], 2015, CS20151011 U CAL DEP
   [Anonymous], 2012, 2012 INT JT C NEUR N, DOI DOI 10.1109/HPCA.2012.6169027
   Burr GW, 2010, J VAC SCI TECHNOL B, V28, P223, DOI 10.1116/1.3301579
   Campos D, 2015, PROCEEDINGS OF THE 17TH BRAZILIAN SYMPOSIUM ON SOFTWARE QUALITY (SBQS), P151, DOI 10.1145/3275245.3275261
   Chen Shimin, 2011, P 5 BIENN C INN DAT, P21, DOI DOI 10.1145/2029956.2029964
   Coburn J, 2011, ACM SIGPLAN NOTICES, V46, P105, DOI [10.1145/1961295.1950380, 10.1145/1961296.1950380]
   DENNING PJ, 1968, COMMUN ACM, V11, P323, DOI 10.1145/363095.363141
   Doller E., 2009, Phase change memory and its impacts on memory hierarchy
   Dulloor Subramanya R., 2014, P EUR C COMP SYST EU, P1, DOI [10.1145/2592798.2592814, DOI 10.1145/2592798.2592814]
   ESOS LABORATORY, 2013, MOB BENCHM TOOL
   Huang J, 2014, PROC VLDB ENDOW, V8, P389, DOI 10.14778/2735496.2735502
   Hwang T, 2015, ACM T STORAGE, V11, DOI 10.1145/2629619
   Intel Cooperation, 2015, NVDIMM NAM SPEC
   Intel Cooperation, 2016, INT ARCH INSTR SET E
   Jiaxin Ou, 2014, 2014 IEEE International Parallel & Distributed Processing Symposium (IPDPS), P787, DOI 10.1109/IPDPS.2014.86
   Jishen Zhao, 2013, 2013 46th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO). Proceedings, P421, DOI 10.1145/2540708.2540744
   Jo H, 2006, IEEE T CONSUM ELECTR, V52, P485, DOI 10.1109/TCE.2006.1649669
   Johnson T., 1994, P 20 INT C VER LARG, P439
   Josephson W., 2010, P 8 USENIX C FIL STO
   Jung M., 2013, Proceedings of the International ACM Conference on International Conference on Supercomputing, ICS '13, P103
   Kang SY, 2009, IEEE T COMPUT, V58, P744, DOI 10.1109/TC.2008.224
   Lee BC, 2010, IEEE MICRO, V30, P131, DOI 10.1109/MM.2010.24
   Lee BC, 2009, CONF PROC INT SYMP C, P2, DOI 10.1145/1555815.1555758
   Lee Changman, 2015, FAST
   Lee E., 2014, ACM T STORAGE, V10
   Lu Y., 2015, 2015 IEEE INT SOLID, P1
   Lu YY, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P773
   Lu YY, 2016, ACM T STORAGE, V12, DOI 10.1145/2851504
   Lu Youyou, 2014, P USENIX C FIL STOR, P75
   Lu Youyou, 2013, P 11 USENIX C FIL ST
   Lu YC, 2017, INT CONF AWARE SCI, P1, DOI [10.1109/ICAwST.2017.8256425, 10.1109/PESGM.2017.8273799]
   Megiddo N, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE 2ND USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST'03), P115
   Min Changwoo, 2012, FAST, V12, P1
   Nightingale EB, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P1
   Ou Jiaxin, 2016, P 32 S MASS STOR SYS
   Pelley S, 2014, CONF PROC INT SYMP C, P265, DOI 10.1109/ISCA.2014.6853222
   Qureshi MK, 2009, CONF PROC INT SYMP C, P24, DOI 10.1145/1555815.1555760
   Ramos Luiz E, 2011, P INT C SUP, P85
   Roselli D, 2000, USENIX ASSOCIATION PROCEEDINGS OF THE 2000 USENIX ANNUAL TECHNICAL CONFERENCE, P41
   Ruemmler C., 1993, USENIX Association. Proceedings of the Winter 1993 USENIX Conference, P405
   Sun Long, 2015, P ACM INT C COMP FRO
   Volos H., 2014, P 9 EUR C COMP SYST
   Volos H, 2015, Proceedings of the 16th Annual Middleware Conference, P37, DOI 10.1145/2814576.2814806
   Volos H, 2011, ACM SIGPLAN NOTICES, V46, P91, DOI [10.1145/1961296.1950379, 10.1145/1961295.1950379]
   Willick D. L., 1993, Proceedings the 13th International Conference on Distributed Computing Systems (Cat. No.93CH3282-1), P2, DOI 10.1109/ICDCS.1993.287729
   Wu Xiaojian., 2011, Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis (SC '11), p39:1
   Xu J, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P323
   Yang Jun, 2015, P 13 USENIX C FIL ST, P167
   Youyou Lu, 2014, 2014 IEEE 32nd International Conference on Computer Design (ICCD), P216, DOI 10.1109/ICCD.2014.6974684
   Zhang Jiacheng, 2016, P 2016 USENIX ANN TE
   Zhang Y., 2015, IEEE 31 S MASS STORA, P1, DOI [DOI 10.1109/MSST.2015.7208275, 10.1109/MSST.2015.7208275, DOI 10.1007/S11042-015-2982-X]
   Zhang YY, 2015, ACM SIGPLAN NOTICES, V50, P3, DOI [10.1145/2694344.2694370, 10.1109/OECC.2015.7340093]
   Zhou P, 2009, CONF PROC INT SYMP C, P14, DOI 10.1145/1555815.1555759
NR 62
TC 24
Z9 27
U1 0
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD APR
PY 2018
VL 14
IS 1
SI SI
AR 4
DI 10.1145/3204454
PG 30
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GH5ZN
UT WOS:000433517600004
DA 2024-07-18
ER

PT J
AU Liu, Q
   Feng, D
   Hu, YC
   Shi, Z
   Fu, M
AF Liu, Qing
   Feng, Dan
   Hu, Yuchong
   Shi, Zhan
   Fu, Min
TI High-Performance General Functional Regenerating Codes with Near-Optimal
   Repair Bandwidth
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Erasure codes; failure recovery; repair bandwidth; failure tolerance;
   performance and evaluation
ID DISTRIBUTED STORAGE; CONSTRUCTION
AB Erasure codes are widely used in modern distributed storage systems to prevent data loss and server failures. Regenerating codes are a class of erasure codes that trade storage efficiency and computation for repair bandwidth reduction. However, their nonunified coding parameters and huge computational overhead prohibit their applications. Hence, we first propose a family of General Functional Regenerating (GFR) codes with uncoded repair, balancing storage efficiency and repair bandwidth with general parameters. The GFR codes take advantage of a heuristic repair algorithm, which makes efforts to employ as little repair bandwidth as possible to repair a single failure. Second, we also present a scheduled shift multiplication (SSM) algorithm, which accelerates the matrix product over the Galois field by scheduling the order of coding operations, so encoding and repairing of GFR codes can be executed by fast bitwise shifting and exclusiveOR. Compared to the traditional table-lookup multiplication algorithm, our SSM algorithm gains 1.2 to 2 X speedup in our experimental evaluations, with little effect on the repair success rate.
C1 [Liu, Qing; Feng, Dan; Hu, Yuchong; Shi, Zhan; Fu, Min] Huazhong Univ Sci & Technol, Wuhan Natl Lab Optoelect F307, 1037 Luoyu Rd, Wuhan 430074, Peoples R China.
C3 Huazhong University of Science & Technology
RP Feng, D (corresponding author), Huazhong Univ Sci & Technol, Wuhan Natl Lab Optoelect F307, 1037 Luoyu Rd, Wuhan 430074, Peoples R China.
EM qing@hust.edu.cn; dfeng@hust.edu.cn; yuchonghu@hust.edu.cn;
   zshi@hust.edu.cn; fumin@hust.edu.cn
FU National Basic Research Program of China (973 Program) [2011CB302301];
   National Natural Science Foundation of China [61025008, 61232004,
   61173043]; National High Technology Research and Development Program of
   China (863 Program) [2013AA013203]; National Key Technology R&D Program
   of China [2011BAH04B02]; Fundamental Research Funds for the Central
   Universities [2013TS043]; Natural Science Foundation of Hubei Province
   [2015CFB192]
FX This work was supported in part by the National Basic Research Program
   of China (973 Program) (2011CB302301), the National Natural Science
   Foundation of China (61025008, 61232004, 61173043), the National High
   Technology Research and Development Program of China (863 Program)
   (2013AA013203), the National Key Technology R&D Program of China
   (2011BAH04B02), the Fundamental Research Funds for the Central
   Universities (2013TS043), and the Natural Science Foundation of Hubei
   Province (2015CFB192).
CR [Anonymous], 2012, P USENIX ANN TECHN C
   Anvin H. P., 2015, MATH RAID 6
   Burkhardt W. A., 1993, Digest of Papers FTCS-23 The Twenty-Third International Symposium on Fault-Tolerant Computing, P432, DOI 10.1109/FTCS.1993.627346
   Chen HCH, 2014, IEEE T COMPUT, V63, P31, DOI 10.1109/TC.2013.167
   Cullina Daniel, 2009, ARXIV09102245
   Dimakis AG, 2011, P IEEE, V99, P476, DOI 10.1109/JPROC.2010.2096170
   Dimakis AG, 2010, IEEE T INFORM THEORY, V56, P4539, DOI 10.1109/TIT.2010.2054295
   Greenan KM, 2008, I S MOD ANAL SIM COM, P121
   Greenan Kevin M., 2010, P 2 USENIX C HOT TOP, V5
   Hu YF, 2011, 2011 AASRI CONFERENCE ON APPLIED INFORMATION TECHNOLOGY (AASRI-AIT 2011), VOL 1, P1
   Hu Yuchong, 2012, P 10 USENIX C FIL ST, V21
   Intel VTune Amplifier XE 2015, 2015, WHATS NEW INT VTUNE
   Kalcher S, 2011, IEEE INT C CL COMP, P290, DOI 10.1109/CLUSTER.2011.40
   Khan Osama, 2012, P 10 USENIX C FIL ST
   Kubiatowicz J, 2000, ACM SIGPLAN NOTICES, V35, P190, DOI 10.1145/384264.379239
   Li Mingqiang, 2014, P 12 USENIX C FIL ST, P147
   Li Runhui, 2013, ARXIV13023344
   Liu Q, 2015, IEEE ACM INT SYMP, P372, DOI 10.1109/CCGrid.2015.38
   Luo JQ, 2014, IEEE T COMPUT, V63, P2259, DOI 10.1109/TC.2013.23
   Luo JQ, 2012, ACM T STORAGE, V8, DOI 10.1145/2093139.2093141
   Papailiopoulos DS, 2012, IEEE INFOCOM SER, P2801, DOI 10.1109/INFCOM.2012.6195703
   Plank J. S., 2013, P 11 USENIX C FIL ST, P95
   Plank JS, 2006, NCA 2006: FIFTH IEEE INTERNATIONAL SYMPOSIUM ON NETWORK COMPUTING AND APPLICATIONS, PROCEEDINGS, P173
   Plank JS, 2009, INT J HIGH PERFORM C, V23, P242, DOI 10.1177/1094342009106191
   PLANK JAMESS., 2009, FAST, P253
   Plank JS, 1997, SOFTWARE PRACT EXPER, V27, P995, DOI 10.1002/(SICI)1097-024X(199709)27:9<995::AID-SPE111>3.0.CO;2-6
   Rashmi KV, 2014, SIGCOMM'14: PROCEEDINGS OF THE 2014 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P331, DOI [10.1145/2619239.2626325, 10.1145/2740070.2626325]
   Rashmi KV, 2011, IEEE T INFORM THEORY, V57, P5227, DOI 10.1109/TIT.2011.2159049
   Rashmi KV, 2009, ANN ALLERTON CONF, P1243, DOI 10.1109/ALLERTON.2009.5394538
   REED IS, 1960, J SOC IND APPL MATH, V8, P300, DOI 10.1137/0108018
   Sathiamoorthy M, 2013, PROC VLDB ENDOW, V6, P325, DOI 10.14778/2535573.2488339
   Schroeder B, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 5TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES ( FAST '07), P1
   Shah NB, 2012, IEEE T INFORM THEORY, V58, P1837, DOI 10.1109/TIT.2011.2173792
   Shum K. W., 2012, Proceedings of the 2012 IEEE International Symposium on Information Theory - ISIT, P1192, DOI 10.1109/ISIT.2012.6283043
   Shvachko K., 2010, SYMPOSIUM, P1, DOI DOI 10.1109/MSST.2010.5496972
   Suh CH, 2011, IEEE T INFORM THEORY, V57, P1425, DOI 10.1109/TIT.2011.2105003
   Tamo Itzhak, 2011, ARXIV11120371
   Wang Anyu, 2012, ARXIV12070879
   Zhiying Wang, 2011, 2011 49th Annual Allerton Conference on Communication, Control, and Computing (Allerton), P1374
   Zhu Y, 2012, IEEE INT CONF MOB, P1, DOI 10.1109/MASS.2012.6502496
NR 40
TC 21
Z9 27
U1 0
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUN
PY 2017
VL 13
IS 2
SI SI
AR 15
DI 10.1145/3051122
PG 28
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FA1TA
UT WOS:000405221600006
DA 2024-07-18
ER

PT J
AU Fryer, D
   Sun, K
   Mahmood, R
   Cheng, TH
   Benjamin, S
   Goel, A
   Brown, AD
AF Fryer, Daniel
   Sun, Kuei
   Mahmood, Rahat
   Cheng, Tinghao
   Benjamin, Shaun
   Goel, Ashvin
   Brown, Angela Demke
TI Recon: Verifying File System Consistency at Runtime
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Reliability; Verification; Performance; Measurement; Metadata
   consistency; runtime verification; file system checker
ID CHECKING
AB File system bugs that corrupt metadata on disk are insidious. Existing reliability methods, such as check-sums, redundancy, or transactional updates, merely ensure that the corruption is reliably preserved. Typical workarounds, based on using backups or repairing the file system, are painfully slow. Worse, the recovery may result in further corruption.
   We present Recon, a system that protects file system metadata from buggy file system operations. Our approach leverages file systems that provide crash consistency using transactional updates. We define declarative statements called consistency invariants for a file system. These invariants must be satisfied by each transaction being committed to disk to preserve file system integrity. Recon checks these invariants at commit, thereby minimizing the damage caused by buggy file systems.
   The major challenges to this approach are specifying invariants and interpreting file system behavior correctly without relying on the file system code. Recon provides a framework for file-system specific metadata interpretation and invariant checking. We show the feasibility of interpreting metadata and writing consistency invariants for the Linux ext3 file system using this framework. Recon can detect random as well as targeted file-system corruption at runtime as effectively as the offline e2fsck file-system checker, with low overhead.
C1 [Fryer, Daniel; Sun, Kuei; Mahmood, Rahat; Cheng, Tinghao; Benjamin, Shaun; Goel, Ashvin; Brown, Angela Demke] Univ Toronto, Toronto, ON M5S 1A1, Canada.
C3 University of Toronto
RP Fryer, D (corresponding author), Univ Toronto, Toronto, ON M5S 1A1, Canada.
EM ashvin@eecq.toronto.edu
FU NSERC
FX This research was supported by NSERC through the Discovery Grants
   program.
CR [Anonymous], P S OP SYST PRINC SO
   [Anonymous], P USENIX TECHN C
   [Anonymous], ZFS LAST WORD FILE S
   [Anonymous], P S OP SYST PRINC SO
   [Anonymous], P USENIX TECHN C
   [Anonymous], P WORKSH HOT TOP SYS
   [Anonymous], P S OP SYST PRINC SO
   [Anonymous], P ACM SIGPLAN C PROG
   [Anonymous], P S OP SYST PRINC SO
   Arnold J, 2009, EUROSYS'09: PROCEEDINGS OF THE FOURTH EUROSYS CONFERENCE, P187
   Bairavasundaram Lakshmi N., 2008, ACM Transaction on Storage, V4, DOI 10.1145/1416944.1416947
   Chen F, 2007, ACM SIGPLAN NOTICES, V42, P569, DOI 10.1145/1297105.1297069
   Custer H., 1994, INSIDE WINDOWS NT FI
   Danial A., 2012, CLOC - Count Lines of Code
   Demsky B, 2006, IEEE T SOFTWARE ENG, V32, P931, DOI 10.1109/TSE.2006.122
   Ganger GR, 2000, ACM T COMPUT SYST, V18, P127, DOI 10.1145/350853.350863
   Gunawi HS., 2008, P 8 USENIX C OPERATI, P131
   MEYER D. T., 2010, P USENIX C FILE STOR, P1
   Perkins JH, 2009, SOSP'09: PROCEEDINGS OF THE TWENTY-SECOND ACM SIGOPS SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P87
   Prabhakaran V, 2005, I C DEPEND SYS NETWO, P802, DOI 10.1109/DSN.2005.65
   Sct S.T., 1998, Journaling the linux ext2fs filesystem
   Sivathanu G, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P15
   Sivathanu M, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE 2ND USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST'03), P73
   Sokolsky O, 2006, ELECTRON NOTES THEOR, V144, P91, DOI 10.1016/j.entcs.2006.02.006
   Sundararaman S., 2010, P USENIX TECHNICAL C, P281
   Sweeney A, 1996, PROCEEDINGS OF THE USENIX 1996 ANNUAL TECHNICAL CONFERENCE, P1
   Yang JF, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P131
   Yang JF, 2006, P IEEE S SECUR PRIV, P243
   Yang JF, 2006, ACM T COMPUT SYST, V24, P393, DOI 10.1145/1189256.1189259
   Zhang Yuhua, 2010, Proceedings of the 2010 Second International Conference on Computer Modeling and Simulation (ICCMS 2010), P29, DOI 10.1109/ICCMS.2010.35
NR 30
TC 19
Z9 26
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2012
VL 8
IS 4
AR 15
DI 10.1145/2385603.2385608
PG 29
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 052QF
UT WOS:000312212500004
DA 2024-07-18
ER

PT J
AU Hsieh, JW
   Wu, CH
   Chiu, GM
AF Hsieh, Jen-Wei
   Wu, Chung-Hsien
   Chiu, Ge-Ming
TI MFTL: A Design and Implementation for MLC Flash Memory Storage Systems
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Algorithms; Performance; Multi-level cell flash memory; FAT file
   system; storage management
ID TRANSLATION LAYER
AB NAND flash memory has gained its popularity in a variety of applications as a storage medium due to its low power consumption, nonvolatility, high performance, physical stability, and portability. In particular, Multi-Level Cell (MLC) flash memory, which provides a lower cost and higher density solution, has occupied the largest part of NAND flash-memory market share. However, MLC flash memory also introduces new challenges: (1) Pages in a block must be written sequentially. (2) Information to indicate a page being obsoleted cannot be recorded in its spare area due to the limitation on the number of partial programming. Since most of applications access NAND flash memory under FAT file system, this article designs an MLC Flash Translation Layer (MFTL) for flash-memory storage systems which takes constraints of MLC flash memory and access behaviors of FAT file system into consideration. A series of trace-driven simulations was conducted to evaluate the performance of the proposed scheme. Although MFTL is designed for MLC flash memory and FAT file system, it is applicable to SLC flash memory and other file systems as well. Our experiment results show that the proposed MFTL could achieve a good performance for various access patterns even on SLC flash memory.
C1 [Hsieh, Jen-Wei; Chiu, Ge-Ming] Natl Taiwan Univ Sci & Technol, Dept Comp Sci & Informat Engn, Taipei, Taiwan.
C3 National Taiwan University of Science & Technology
RP Hsieh, JW (corresponding author), Natl Taiwan Univ Sci & Technol, Dept Comp Sci & Informat Engn, Taipei, Taiwan.
EM jenwei@mail.ntust.edu.tw
RI Hsieh, Jen-Wei/X-1989-2019
OI Wu, Chung-Hsien/0000-0002-3947-2123
FU National Science Council [NSC100-2221-E-011-074]
FX This work is supported by the National Science Council, under grant
   NSC100-2221-E-011-074.
CR ALEPH ONE COMPANY, YET AN FLASH FIL SYS
   [Anonymous], 2011, PROC 9 USENIX C FILE
   [Anonymous], 2020, 128 LAYER NAND FLASH
   [Anonymous], UND FLASH TRANSL LAY
   [Anonymous], 1995, ECMA107
   Ban A., 1999, U.S. Patent, Patent No. 5937425
   Chang LP, 2010, IEEE T COMPUT, V59, P1337, DOI 10.1109/TC.2010.14
   Chang LP, 2009, ACM T DES AUTOMAT EL, V15, DOI 10.1145/1640457.1640463
   Chang YH, 2007, DES AUT CON, P212, DOI 10.1109/DAC.2007.375155
   Chang YH, 2010, ACM T STORAGE, V6, DOI 10.1145/1807060.1807062
   Chin-Hsien Wu, 2006, ACM Transaction on Storage, V2, P449, DOI 10.1145/1210596.1210600
   Cho H, 2009, DES AUT TEST EUROPE, P507
   Gupta A, 2009, ACM SIGPLAN NOTICES, V44, P229, DOI 10.1145/1508284.1508271
   Hsieh JW, 2008, IEEE T COMPUT, V57, P1571, DOI 10.1109/TC.2008.61
   Im S, 2010, J SYST ARCHITECT, V56, P641, DOI 10.1016/j.sysarc.2010.09.005
   INTEL, SOFTW CONC IMPL RES
   INTEL, FLASH CACH MEM PUTS
   Jen-Wei Hsieh, 2006, ACM Transaction on Storage, V2, P22, DOI 10.1145/1138041.1138043
   Joo Y., 2006, CODES+ISSS '06: Internatinal Conference on Hardware/Software Codesign and System Synthesis, P229
   JUNG D., 2010, ACM T EMBED COMPUT S, V9
   Kim J, 2002, IEEE T CONSUM ELECTR, V48, P366
   Lee JH, 2005, J SYST ARCHITECT, V51, P111, DOI 10.1016/j.sysarc.2004.10.002
   Lee SW, 2007, ACM T EMBED COMPUT S, V6, DOI 10.1145/1275986.1275990
   Li-Pin Chang, 2006, ACM Transaction on Storage, V1, P381, DOI 10.1145/1111609.1111610
   PARK C., 2003, P 21 INT C COMP DES
   Park Chanik, 2004, P 4 ACM INT C EMB SO, P114, DOI [10.1145/1017753.1017775, DOI 10.1145/1017753.1017775]
   Park H, 2010, J SYST ARCHITECT, V56, P208, DOI 10.1016/j.sysarc.2010.03.006
   PCSTATS, 2011, BEG GUID FLASH MEM D
   Samsung Electronics, SAMS K9LBG08U0M V1 0
   Song H, 2009, J SYST ARCHITECT, V55, P15, DOI 10.1016/j.sysarc.2008.07.004
   Sungjin Lee, 2008, Operating Systems Review, V42, P36, DOI 10.1145/1453775.1453783
   TESTMETRIX INC, VTE2100
   Woodhouse David., 2001, JFFS : The Journalling Flash File System
   Wu Chin-Hsien., 2006, Proceedings of the 2006 IEEE/ACM International Conference on Computer-Aided Design (ICCAD '06), P601
NR 34
TC 4
Z9 4
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAY
PY 2012
VL 8
IS 2
AR 7
DI 10.1145/2180905.2180908
PG 29
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 990LY
UT WOS:000307633600003
DA 2024-07-18
ER

PT J
AU Tian, L
   Cao, Q
   Jiang, H
   Feng, D
   Xie, CS
   Xin, Q
AF Tian, Lei
   Cao, Qiang
   Jiang, Hong
   Feng, Dan
   Xie, Changsheng
   Xin, Qin
TI Online Availability Upgrades for Parity-Based RAIDs through
   Supplementary Parity Augmentations
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Reliability; Availability upgrade; RAID; RAID reconstruction
ID SYSTEMS
AB In this article, we propose a simple but powerful online availability upgrade mechanism, Supplementary Parity Augmentations(SPA), to address the availability issue in parity-based RAID systems. The basic idea of SPA is to store and update the supplementary parity units on one or a few newly augmented spare disks for online RAID systems in the operational mode, thus achieving the goals of improving the reconstruction performance while tolerating multiple disk failures and latent sector errors simultaneously. By applying the exclusive OR operations appropriately among supplementary parity, full parity, and data units, SPA can reconstruct the data on the failed disks with a fraction of the original overhead that is proportional to the supplementary parity coverage, thus significantly reducing the overhead of data regeneration and decreasing recovery time in parity-based RAID systems. Our extensive trace-driven simulation study shows that SPA can significantly improve the reconstruction performance of the RAID5 and RAID5+0 systems, at an acceptable performance overhead imposed in the operational mode. Moreover, our reliability analytical modeling and sequential Monte-Carlo simulation demonstrate that SPA is consistently more than double the MTTDL of the RAID5 system and improves the reliability of the RAID5+0 system noticeably.
C1 [Tian, Lei; Feng, Dan; Xie, Changsheng] Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan, Peoples R China.
   [Tian, Lei; Jiang, Hong] Univ Nebraska, Dept Comp Sci & Engn, Lincoln, NE 68588 USA.
C3 Huazhong University of Science & Technology; University of Nebraska
   System; University of Nebraska Lincoln
RP Feng, D (corresponding author), Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan, Peoples R China.
EM dfeng@hust.edu.cn
RI Tian, Lei/I-9575-2014
OI Tian, Lei/0000-0002-8990-4799
FU U.S. NSF [CNS-1016609, IIS-0916859, CCF-0937993, CCF-0621526]; National
   Basic Research Program of China ("973" program) [2011CB302301,
   2011CB302303]; National High Technology Research and Development Program
   of China ("863" program) [2009AA01AF401, 2009AA01A402]; Changjiang
   Innovative Group of Education of China [IRT0725]; National NSF of China
   (NSFC) [61025008, 60703046, 60873028, 60933002]
FX This work was supported by the U.S. NSF under grants CNS-1016609,
   IIS-0916859, CCF-0937993, and CCF-0621526, the National Basic Research
   Program of China ("973" program) under grants nos. 2011CB302301 and
   2011CB302303, the National High Technology Research and Development
   Program of China ("863" program) under grants nos. 2009AA01AF401 and
   2009AA01A402, Changjiang Innovative Group of Education of China no.
   IRT0725, and the National NSF of China (NSFC) under grants nos.
   61025008, 60703046, 60873028, and 60933002.
CR ANDERSON D., 2007, HARD DRIVES TODAY TO
   [Anonymous], 2007, K9XXG08XXM Flash Memory Specification
   [Anonymous], P US ANN TECHN C USE
   BAIRAVASUNDARAM L. N., 2007, ACM SIGMETRICS 07 JO
   Balakrishnan M, 2010, EUROSYS'10: PROCEEDINGS OF THE EUROSYS 2010 CONFERENCE, P15
   BIRRELL A, 2007, SIGOPS OPER SYST REV, V41, P88
   BLAUM M, 1995, IEEE T COMPUT, V44, P192, DOI 10.1109/12.364531
   Bucy J. S., 2008, CMUPDL08101
   Corbett Peter, 2004, P USENIX C FIL STOR
   Dholakia Ajay, 2008, ACM Transaction on Storage, V4, DOI 10.1145/1353452.1353453
   ELERATH J. G., 2007, P ANN IEEE IFIP INT
   GIBSON G. A., 2007, REFLECTIONS FAILURE
   GOLDING R., 1995, P USENIX 95 C
   Heath T., 2005, P ACM SIGPLAN S PRIN
   Holland Mark, 1993, P 23 INT S FAULT TOL
   ILIADIS I., 2008, P ACM SIGMETRICS 08
   JIANG W., 2008, P USENIX C FIL STOR
   LEE EK, 1993, IEEE T COMPUT, V42, P651, DOI 10.1109/12.277289
   Lee JYB, 2002, IEEE T PARALL DISTR, V13, P499, DOI 10.1109/TPDS.2002.1003860
   Mi NF, 2009, PERF E R SI, V37, P205
   Narayanan Dushyanth, 2008, P USENIX C FIL STOR
   Nathuji R, 2007, P IEEE INT C AUT COM
   NET BSD FOUNDATION, 2008, NETBSD GUID
   OSTERGAARD J, 2004, SOFTWARE RAID HOWTO
   PATTERSON D. A., 1988, P ACM SIGMOD 88 INT
   PINHEIRO E, 2007, P USENIX C FIL STOR
   Plank JS, 1997, SOFTWARE PRACT EXPER, V27, P995, DOI 10.1002/(SICI)1097-024X(199709)27:9<995::AID-SPE111>3.0.CO;2-6
   PLANK JS, 2008, P USENIX C FIL STOR
   RISKA A., 2008, P IEEE INT C DEP SYS
   RISKA A., 2008, P USENIX 08 C
   ROSELLI D., 2000, P USENIX 00 C
   SAVAGE S., 1996, P USENIX 96 C
   SCHROEDER B, 2007, P USENIX C FIL STOR
   SCHWARZ T. J. E., 2004, P ANN IEEE ACM INT S
   SEAGATE, 2007, CHEET 15K 5 HARD DRI
   SIVATHANU M., 2004, P USENIX C FIL STOR
   TECHTARGET, 2008, ENT CLASS RAID FUNCT
   TIAN L., 2007, P USENIX C FIL STOR
   TIAN L., 2009, TRUNLCSE20090006
   UMASS TRACE REPOSITORY, 2007, OLTP APPL SEARCH ENG
   Weddle Charles, 2007, P USENIX C FIL STOR
   WHITTINGTON W., 2007, DESKTOP NEARLINE ENT
   Wu S., 2009, USENIX C FILE STORAG, V9, P239
   XIE T, 2009, P 29 IEEE INT C DIST, P631
   Xie T, 2008, IEEE T COMPUT, V57, P1386, DOI 10.1109/TC.2008.76
NR 45
TC 2
Z9 2
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAY
PY 2011
VL 6
IS 4
AR 17
DI 10.1145/1970338.1970341
PG 23
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 990KX
UT WOS:000307630800003
DA 2024-07-18
ER

PT J
AU Sun, JH
   Li, SB
   Xu, J
   Huang, J
AF Sun, Jinghan
   Li, Shaobo
   Xu, Jun
   Huang, Jian
TI The Security War in File Systems: An Empirical Study from A
   Vulnerability-centric Perspective
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Storage systems; file system vulnerabilities
ID CHECKING
AB This article presents a systematic study on the security of modern file systems, following a vulnerability-centric perspective. Specifically, we collected 377 file system vulnerabilities committed to the CVE database in the past 20 years. We characterize them from four dimensions: why the vulnerabilities appear, how the vulnerabilities can be exploited, what consequences can arise, and how the vulnerabilities are fixed. This way, we build a deep understanding of the attack surfaces faced by file systems, the threats imposed by the attack surfaces, and the good and bad practices in mitigating the attacks in file systems. We envision that our study will bring insights towards the future development of file systems, the enhancement of file system security, and the relevant vulnerability-mitigating solutions.
C1 [Sun, Jinghan; Li, Shaobo; Huang, Jian] Univ Illinois, Urbana, IL 61820 USA.
   [Xu, Jun] Univ Utah, Salt Lake City, UT 84102 USA.
C3 University of Illinois System; University of Illinois Urbana-Champaign;
   Utah System of Higher Education; University of Utah
RP Sun, JH (corresponding author), Univ Illinois, Urbana, IL 61820 USA.
EM js39@illinois.edu; shaobol2@illinois.edu; junxzm@cs.utah.edu;
   jianh@illinois.edu
OI Xu, Jun/0009-0002-8128-5062
FU NetApp Faculty Fellowship Award; NSF CAREER Award [2144796]
FX This work was partially supported by a NetApp Faculty Fellowship Award
   and NSF CAREER Award 2144796.
CR Agrawal H., 1993, Proceedings. Conference on Software Maintenance 1993. CSM-93 (Cat. No.93CH3360-5), P348, DOI 10.1109/ICSM.1993.366927
   Ahmad A, 2018, 25TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2018), DOI 10.14722/ndss.2018.23284
   Alexopoulos N, 2022, PROCEEDINGS OF THE 31ST USENIX SECURITY SYMPOSIUM, P359
   Amani S, 2016, ACM SIGPLAN NOTICES, V51, P175, DOI 10.1145/2954679.2872404
   [Anonymous], 2020, Ext4 Disk Layout
   [Anonymous], 2019, Researcher Reservation Guidelines
   [Anonymous], 2021, Request CVE IDs
   [Anonymous], REISERFS
   [Anonymous], Journaled File System Technology for Linux
   [Anonymous], 2022, ZDNet
   [Anonymous], 2019, VulDB
   [Anonymous], Apple File System secure encryption mechanism
   [Anonymous], APFS: Apple File System
   [Anonymous], 2020, Network File System
   [Anonymous], OVERVIEW LINUX VIRTU
   [Anonymous], 2019, Haoop Distributed File System
   [Anonymous], 2011, Metadata Replication for Ext4
   [Anonymous], HFS: Hierarchical File System
   [Anonymous], CEPH
   Aublin PL, 2018, EUROSYS '18: PROCEEDINGS OF THE THIRTEENTH EUROSYS CONFERENCE, DOI 10.1145/3190508.3190547
   Bairavasundaram Lakshmi N, 2009, USENIX ANN TECHN C, V9
   Bornholt J, 2016, ACM SIGPLAN NOTICES, V51, P83, DOI 10.1145/2954679.2872406
   Cai M, 2019, APSYS'19: PROCEEDINGS OF THE 10TH ACM SIGOPS ASIA-PACIFIC WORKSHOP ON SYSTEMS, P8, DOI 10.1145/3343737.3343753
   Cappos Justin, 2010, P 17 ACM C COMPUTER
   Card Remy, 1995, P 1 DUTCH INT S LIN
   Cerdeira D, 2020, P IEEE S SECUR PRIV, P1416, DOI 10.1109/SP40000.2020.00061
   Chen HG, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P270, DOI 10.1145/3132747.3132776
   Chen HG, 2015, SOSP'15: PROCEEDINGS OF THE TWENTY-FIFTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P18, DOI 10.1145/2815400.2815402
   Chen Haogang, 2011, APSYS 11 ASIA PACIFI
   Debian, 2022, Security Bug Tracker
   Fonseca P, 2010, I C DEPEND SYS NETWO, P221, DOI 10.1109/DSN.2010.5544315
   Galloway A, 2009, LECT NOTES COMPUT SC, V5403, P74, DOI 10.1007/978-3-540-93900-9_10
   Ghemawat S., 2003, Operating Systems Review, V37, P29, DOI 10.1145/1165389.945450
   GlusterFS, GlusterFS architecture
   Govindavajhala S, 2003, P IEEE S SECUR PRIV, P154, DOI 10.1109/SECPRI.2003.1199334
   Gruenbacher Andreas, 2003, P 2003 USENIX ANN TE
   Gu RH, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P653
   Gunawi Haryadi S., 2007, Operating Systems Review, V41, P293, DOI 10.1145/1323293.1294290
   Gunawi H. S., 2014, P ACM S CLOUD COMP S, P1
   Halcrow M.A., 2005, Proceedings of the 2005 Linux Symposium, V1, P201
   Hitz Dave, 1994, USENIX WINTER 1994 T
   Huang J, 2016, PROCEEDINGS OF USENIX ATC '16: 2016 USENIX ANNUAL TECHNICAL CONFERENCE, P465
   Kannan S, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P241
   Kim S, 2019, PROCEEDINGS OF THE TWENTY-SEVENTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '19), P147, DOI 10.1145/3341301.3359662
   Koo J, 2021, PROCEEDINGS OF THE 15TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '21), P75, DOI 10.5281/zenodo.4659803
   Lee Changman, 2015, 13 USENIX C FILE STO, P273
   Leesatapornwongsa Tanakorn, 2016, P 21 INT C ARCHITECT
   Lin X, 2018, 34TH ANNUAL COMPUTER SECURITY APPLICATIONS CONFERENCE (ACSAC 2018), P418, DOI 10.1145/3274694.3274720
   Liu J, 2018, PROCEEDINGS OF THE 2018 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '18), P490, DOI 10.1145/3230543.3230582
   Lu KJ, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI 10.14722/ndss.2017.23387
   Lu Lanyue, 2013, P 11 USENIX C FILE S
   Lu Lanyue, 2014, 11 USENIX S OP SYST
   Lu S, 2008, ACM SIGPLAN NOTICES, V43, P329, DOI 10.1145/1353536.1346323
   Mathur Avantika, 2007, P LIN S, V2
   Min C, 2015, SOSP'15: PROCEEDINGS OF THE TWENTY-FIFTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P361, DOI 10.1145/2815400.2815422
   Min Changwoo, 2015, 2015 USENIX ANN TECH
   Mouw E., 2001, LINUX KERNEL PROCFS
   Mu DL, 2018, PROCEEDINGS OF THE 27TH USENIX SECURITY SYMPOSIUM, P919
   National Institute of Standards and Technology, 2022, Attack Surface
   Nelson L, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P252, DOI 10.1145/3132747.3132748
   NVD, About us
   Onoma AK, 1998, COMMUN ACM, V41, P81, DOI 10.1145/274946.274960
   OpenAFS, ABOUT US
   Pawlowski Brian, 1994, USENIX SUMMER TECHNI
   Pawlowski Brian, 2000, P 2 INT SYST ADM NET
   Pillai Thanumalayan Sankaranarayana, 2014, P 11 USENIX S OP SYS, P433
   Pinto S, 2019, ACM COMPUT SURV, V51, DOI 10.1145/3291047
   Plotkin GD, 2016, ACM SIGPLAN NOTICES, V51, P69, DOI 10.1145/2914770.2837657
   Prabhu S, 2020, PROCEEDINGS OF THE 17TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P953
   Rodeh O, 2013, ACM T STORAGE, V9, DOI 10.1145/2501620.2501623
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   Schumilo Sergej, 2017, 26 USENIX SECURITY S
   Serebryany K., 2012, USENIX ATC 2012, P309
   Serebryany Konstantin, 2009, Proceedings of the Workshop on Binary Instrumentation and Applications, WBIA'09, P62, DOI DOI 10.1145/1791194.1791203
   Shen Zhiming, 2019, ACM Transactions on Storage, V19
   Shvachko K, 2010, IEEE S MASS STOR SYS
   Sigurbjarnarson H, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P1
   Singh Satyam, 2021, Exploiting NFS Share
   Smalley Stephen, 2001, NAI Labs Report, V1, P139
   Snyder P., 1990, Proceedings of the Autumn 1990 EUUG Conference, P241
   Steiner Jennifer G., 1988, USENIX WINT TECHN C
   Tweedie S.C., 1998, The Fourth Annual Linux Expo
   Wang R. Y., 1993, Proceedings. Fourth Workshop on Workstation Operating Systems (Cat. No.93TH0553-8), P71, DOI 10.1109/WWOS.1993.348169
   Wei JP, 2005, USENIX ASSOCIATION PROCEEDINGS OF THE 4TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P155
   Weil SA, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P307
   Wikipedia contributors, 2022, Attack surface-Wikipedia, The Free Encyclopedia
   Xu FW, 2016, LECT NOTES COMPUT SC, V9780, P59, DOI 10.1007/978-3-319-41540-6_4
   Xu M, 2016, ACM COMPUT SURV, V49, DOI 10.1145/2963145
   Xu W, 2019, P IEEE S SECUR PRIV, P818, DOI 10.1109/SP.2019.00035
   Yamaguchi F, 2014, P IEEE S SECUR PRIV, P590, DOI 10.1109/SP.2014.44
   Yang JF, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE SIXTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDE '04), P273
   Yang JF, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P131
   Yoo S, 2012, SOFTW TEST VERIF REL, V22, P67, DOI [10.1002/stv.430, 10.1002/stvr.430]
   Zhai Ennan, 2014, P 11 USENIX S OP SYS
NR 94
TC 0
Z9 0
U1 2
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2023
VL 19
IS 4
AR 34
DI 10.1145/3606020
PG 26
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X6NQ6
UT WOS:001099601100006
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Do, J
   Ferreira, VC
   Bobarshad, H
   Torabzadehkashi, M
   Rezaei, S
   Heydarigorji, A
   Souza, D
   Goldstein, BF
   Santiago, L
   Kim, MS
   Lima, PMV
   França, FMG
   Alves, V
AF Do, Jaeyoung
   Ferreira, Victor C.
   Bobarshad, Hossein
   Torabzadehkashi, Mahdi
   Rezaei, Siavash
   Heydarigorji, Ali
   Souza, Diego
   Goldstein, Brunno F.
   Santiago, Leandro
   Kim, Min Soo
   Lima, Priscila M., V
   Franca, Felipe M. G.
   Alves, Vladimir
TI Cost-effective, Energy-efficient, and Scalable Storage Computing for
   Large-scale AI Applications
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Computational storage; in-storage processing; solid-state drive;
   similarity search; neural network; object tracking
ID HIGH-PERFORMANCE; TRACKING
AB The growing volume of data produced continuously in the Cloud and at the Edge poses significant challenges for large-scale AI applications to extract and learn useful information from the data in a timely and efficient way. The goal of this article is to explore the use of computational storage to address such challenges by distributed near-data processing. We describe Newport, a high-performance and energy-efficient computational storage developed for realizing the full potential of in-storage processing. To the best of our knowledge, Newport is the first commodity SSD that can be configured to run a server-like operating system, greatly minimizing the effort for creating and maintaining applications running inside the storage. We analyze the benefits of using Newport by running complex AI applications such as image similarity search and object tracking on a large visual dataset. The results demonstrate that data-intensive AI workloads can be efficiently parallelized and offloaded, even to a small set of Newport drives with significant performance gains and energy savings. In addition, we introduce a comprehensive taxonomy of existing computational storage solutions together with a realistic cost analysis for high-volume production, giving a good big picture of the economic feasibility of the computational storage technology.
C1 [Do, Jaeyoung] Microsoft Res, Redmond, WA 98052 USA.
   [Ferreira, Victor C.; Goldstein, Brunno F.; Santiago, Leandro; Lima, Priscila M., V; Franca, Felipe M. G.] Univ Fed Rio de Janeiro, Rio De Janeiro, Brazil.
   [Bobarshad, Hossein; Torabzadehkashi, Mahdi; Alves, Vladimir] NGD Syst, Irvine, CA USA.
   [Rezaei, Siavash; Heydarigorji, Ali; Kim, Min Soo] Univ Calif Irvine, Irvine, CA USA.
   [Souza, Diego] Wespa Intelligent Syst, Rio De Janeiro, Brazil.
C3 Microsoft; Universidade Federal do Rio de Janeiro; University of
   California System; University of California Irvine
RP Do, J (corresponding author), Microsoft Res, Redmond, WA 98052 USA.
EM jaedo@microsoft.com
RI França, Felipe M.G./J-5910-2012; França, Felipe/AAC-4302-2021;
   Goldstein, Brunno/HKO-9957-2023; Torabzadehkashi/AAB-7903-2021; Rezaei,
   Siavash/AAW-3922-2020; Torabzadehkashi/O-7998-2017
OI França, Felipe M.G./0000-0002-8980-6208; França,
   Felipe/0000-0002-8980-6208; Goldstein, Brunno/0000-0002-5450-3557;
   Torabzadehkashi/0000-0002-7765-3064;
   Torabzadehkashi/0000-0002-7765-3064; Bobarshad,
   Hossein/0000-0001-5494-3926; Santiago de Araujo,
   Leandro/0000-0002-3631-8761; HeydariGorji, Ali/0000-0002-6218-8745;
   Souza, Diego/0009-0001-9699-1181
FU Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior - Brasil
   (CAPES) [001]; National Science Foundation [1660071]; Div Of Industrial
   Innovation & Partnersh; Directorate For Engineering [1660071] Funding
   Source: National Science Foundation
FX This work is supported in part by the Coordenacao de Aperfeicoamento de
   Pessoal de Nivel Superior - Brasil (CAPES) - Finance Code 001. Also,
   this material is based upon work supported by the National Science
   Foundation under Grant No. 1660071.
CR Abu-El-Haija Sami, 2016, arXiv
   Aleksander I., 1984, Sensor Review, V4, P120, DOI 10.1108/eb007637
   Aleksander I., 2009, P 17 EUR S ART NEUR
   [Anonymous], 2014, ESANN 2014 22 EUR S
   [Anonymous], 2013, P 27 INT ACM C INT C, DOI [DOI 10.1145/2464996.2465003, 10.1145/2464996.2465003]
   [Anonymous], 2004, Int. J. Comput. Vis., DOI [DOI 10.1023/B:VISI.0000029664.99615.94, 10.1023/B:VISI.0000029664.99615.94]
   [Anonymous], 2017, IEEE INT S CIRC SYST
   [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.465
   [Anonymous], 2018, DIGITIZATION WORLD E
   Aumüller M, 2020, INFORM SYST, V87, DOI 10.1016/j.is.2019.02.006
   Beaver D., 2010, P USENIX OSDI
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Borji A, 2019, COMPUT VIS MEDIA, V5, P117, DOI 10.1007/s41095-019-0149-9
   Briscoe Neil, 2000, PC NETW ADVIS, V120, P2
   Chung TS, 2009, J SYST ARCHITECT, V55, P332, DOI 10.1016/j.sysarc.2009.03.005
   Cornwell M, 2012, COMMUN ACM, V55, P59, DOI 10.1145/2380656.2380672
   Dagum L, 1998, IEEE COMPUT SCI ENG, V5, P46, DOI 10.1109/99.660313
   Darrell T., 2005, NEAREST NEIGHBOR MET
   Do J., 2013, SIGMOD, P1221
   Do J, 2019, COMMUN ACM, V62, P54, DOI 10.1145/3286588
   Do Nascimiento Daniel N., 2015, P EUR S ART NEUR NET, DOI [10.13140/RG.2.1.3387.5687, DOI 10.13140/RG.2.1.3387.5687]
   Domeniconi C, 2002, IEEE T PATTERN ANAL, V24, P1281, DOI 10.1109/TPAMI.2002.1033219
   Eshghi K., 2013, INSIDE SOLID STATE D, P19
   Eshratifar AE, 2021, IEEE T MOBILE COMPUT, V20, P565, DOI 10.1109/TMC.2019.2947893
   Fasheh Mark., 2006, Proceedings of the 2006 Linux Symposium, V1, P289
   França HL, 2010, I C CONT AUTOMAT ROB, P2437
   Friedman Ted, 2019, OVERCOME 4 MAJOR CHA
   Gantz J., 2012, The Digital Universe in 2020: Big Data, Bigger Digital Shadows, and Biggest Growth in the Far East, V2012, P1
   Graham RL, 2006, LECT NOTES COMPUT SC, V3911, P228
   Gropp W, 1996, PARALLEL COMPUT, V22, P789, DOI 10.1016/0167-8191(96)00024-5
   Gu B, 2016, CONF PROC INT SYMP C, P153, DOI 10.1109/ISCA.2016.23
   Hastie T, 1996, ADV NEUR IN, V8, P409
   He WH, 2013, CARDIOVASC TOXICOL, V13, P1, DOI 10.1007/s12012-012-9174-y
   Held D, 2016, LECT NOTES COMPUT SC, V9905, P749, DOI 10.1007/978-3-319-46448-0_45
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   HeydariGorji Ali, 2020, ARXIV200708077
   HeydariGorji Ali, 2020, STANNIS LOW POWER AC
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Janukowicz Jeff, 2018, TECHNICAL REPORT
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jo I, 2016, PROC VLDB ENDOW, V9, P924
   Jun SW, 2015, 2015 ACM/IEEE 42ND ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA), P1, DOI 10.1145/2749469.2750412
   Kim S, 2016, INFORM SCIENCES, V327, P183, DOI 10.1016/j.ins.2015.07.056
   Koo G, 2017, 50TH ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE (MICRO), P219, DOI 10.1145/3123939.3124553
   Kristan M, 2016, IEEE T PATTERN ANAL, V38, P2137, DOI 10.1109/TPAMI.2016.2516982
   KUO T.-W., 2004, ACM T EMBED COMPUT S, V3, P4
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   MacKay DJC, 1996, ELECTRON LETT, V32, P1645, DOI 10.1049/el:19961141
   Mehra Pankaj, 2019, P FLASH MEM SUMM
   Merkel D., 2014, LINUX J, V2014, P2, DOI DOI 10.5555/2600239.2600241
   Narayanan I, 2016, PROCEEDINGS OF THE 9TH ACM INTERNATIONAL SYSTEMS AND STORAGE CONFERENCE (SYSTOR'16), DOI [10.1145/2964791.2901489, 10.1145/2928275.2928278]
   Nikhil Rishiyur S., 2009, SIGDA Newsl, V39, P1, DOI [10.1145/1862876.1862877, DOI 10.1145/1862876.1862877]
   Ohshima Shigeo, 2016, P FLASH MEM SUMM
   ONFI online, 2017, OP NAND FLASH INT SP
   Redmon J., 2016, PROC CVPR IEEE, DOI [10.1109/CVPR.2016.91, DOI 10.1109/CVPR.2016.91]
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Rezaei S., 2016, 2016 International Conference on ReConFigurable Computing and FPGAs ReConFig, P1
   Rezaei S, 2019, PERSPECT ASIAN TOUR, P1, DOI 10.1007/978-981-13-2463-5_1
   Rezaei S, 2018, PR IEEE COMP DESIGN, P374, DOI 10.1109/ICCD.2018.00063
   Roddy S., 2019, ARM NN EASY WAY DEPL
   Samragh M., 2019, CoRR
   Seshadri S, 2014, MANAGEMENT OF INFERTILITY FOR THE MRCOG AND BEYOND, 3RD EDITION, P67
   Shanker R, 2009, PROC ASME CONF SMART, P41
   Shvachko K., 2010, SYMPOSIUM, P1, DOI DOI 10.1109/MSST.2010.5496972
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   SolarWinds, 2018, CAN GZIP COMPR REAL
   Soltis Steven R., 1997, IEEE T PARALL DISTR, V1, P1
   Srinivasan Mohan, 2014, FLASHCACHE
   Tiwari Devesh, 2013, FAST 13, P119
   Torabzadehkashi Mahdi, 2019, 2019 IEEE 21st International Conference on High Performance Computing and Communications; IEEE 17th International Conference on Smart City; IEEE 5th International Conference on Data Science and Systems (HPCC/SmartCity/DSS). Proceedings, P1878, DOI 10.1109/HPCC/SmartCity/DSS.2019.00259
   Torabzadehkashi M, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0265-5
   Torabzadehkashi M, 2019, EUROMICRO WORKSHOP P, P430, DOI 10.1109/EMPDP.2019.8671589
   Torabzadehkashi M, 2018, IEEE SYM PARA DISTR, P1260, DOI 10.1109/IPDPSW.2018.00195
   Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531
   Wang G, 2014, IEEE IND ELEC, P661, DOI 10.1109/IECON.2014.7048571
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Zhao K., 2013, P 11 USENIX C FILE S, P243
NR 79
TC 24
Z9 26
U1 0
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2020
VL 16
IS 4
AR 21
DI 10.1145/3415580
PG 37
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA PA3GG
UT WOS:000595524400002
OA Bronze
DA 2024-07-18
ER

PT J
AU Cao, ZC
   Wen, H
   Ge, XZ
   Ma, JE
   Diehl, J
   Du, DHC
AF Cao, Zhichao
   Wen, Hao
   Ge, Xiongzi
   Ma, Jingwe
   Diehl, Jim
   Du, David H. C.
TI TDDFS A Tier-Aware Data Deduplication-Based File System
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Data deduplication; tiered storage; data migration; file system
AB With the rapid increase in the amount of data produced and the development of new types of storage devices, storage tiering continues to be a popular way to achieve a good tradeoff between performance and cost-effectiveness. In a basic two-tier storage system, a storage tier with higher performance and typically higher cost (the fast tier) is used to store frequently-accessed (active) data while a large amount of less-active data are stored in the lower-performance and low-cost tier (the slow tier). Data are migrated between these two tiers according to their activity. In this article, we propose a Tier-aware Data Deduplication-based File System, called TDDFS, which can operate efficiently on top of a two-tier storage environment.
   Specifically, to achieve better performance, nearly all file operations are performed in the fast tier. To achieve higher cost-effectiveness, files are migrated from the fast tier to the slow tier if they are no longer active, and this migration is done with data deduplication. The distinctiveness of our design is that it maintains the non-redundant (unique) chunks produced by data deduplication in both tiers if possible. When a file is reloaded (called a reloaded file) from the slow tier to the fast tier, if some data chunks of the file already exist in the fast tier, then the data migration of these chunks from the slow tier can be avoided. Our evaluation shows that TDDFS achieves close to the best overall performance among various file-tiering designs for two-tier storage systems.
C1 [Cao, Zhichao; Wen, Hao; Diehl, Jim; Du, David H. C.] Univ Minnesota, 4-192 Keller Hall,200 Union St SE, Minneapolis, MN 55455 USA.
   [Ge, Xiongzi] NetApp, 7301 Kit Creek Rd, Res Triangle Pk, NC 27709 USA.
   [Ma, Jingwe] Nankai Univ, Coll IT, Tianjin, Peoples R China.
   [Ma, Jingwe] Baidu Online Network Technol Co Ltd, Beijing, Peoples R China.
C3 University of Minnesota System; University of Minnesota Twin Cities;
   Nankai University; Baidu
RP Cao, ZC (corresponding author), Univ Minnesota, 4-192 Keller Hall,200 Union St SE, Minneapolis, MN 55455 USA.
EM caoxx380@umn.edu; wenxx159@umn.edu; gexx132@umn.edu; mjwtom@gmail.com;
   jdiehl@umn.edu; du@umn.edu
RI Wen, Hao/AAV-1085-2021
OI Wen, Hao/0000-0003-3516-4454
FU NSF [1421913, 1439622, 1525617, 1812537]
FX This work was partially supported by NSF awards 1421913, 1439622,
   1525617, and 1812537.
CR Agrawal Nitin, 2008, P USENIX ANN TECHN C, P57
   [Anonymous], 2012, USENIX ANN TECHN C M
   [Anonymous], 2008, P USENIX ANN TECHN C
   [Anonymous], 2002, P 1 USENIX C FIL STO
   [Anonymous], 2009, ZFS DEDUPLICATION
   Baderdinni Anant, 2013, US Patent, Patent No. 8,478,939
   Brown Aaron, 2010, BLOCK LEVEL INLINE D
   Cao ZC, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P309
   Cao Zhichao, 2019, P 17 USENIX C FIL ST
   Chen M, 2014, MOBILE NETW APPL, V19, P171, DOI 10.1007/s11036-013-0489-0
   Debnath B., 2010, P USENIX ANN TECHN C
   Fu Min, 2014, 2014 USENIX C USENIX, P181
   Fu Min, 2015, 13 USENIX C FIL STOR
   Fu YJ, 2011, IEEE INT C CL COMP, P112, DOI 10.1109/CLUSTER.2011.20
   Harnik D, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P277
   Kim SH, 2015, MEDIAT INFLAMM, V2015, DOI 10.1155/2015/904142
   Kitchin R, 2014, GEOJOURNAL, V79, P1, DOI 10.1007/s10708-013-9516-8
   Li C., 2014, USENIX ANN TECH C AT, P501
   Li WJ, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P301
   Lillibridge M., 2013, FAST
   Malhotra Jyoti, 2015, PERV COMP ICPC 2015, P1
   Mandal S, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P315
   Meyer DT, 2012, ACM T STORAGE, V7, DOI 10.1145/2078861.2078864
   Min J, 2011, IEEE T COMPUT, V60, P824, DOI 10.1109/TC.2010.263
   Paulo J, 2014, ACM COMPUT SURV, V47, DOI 10.1145/2611778
   Reinsel D., 2018, The digitization of the world from edge to core
   ROSELLI D, P 2000 USENIX ANN, P41
   Roselli Drew., 1998, Characteristics of file system workloads
   Shvachko K, 2010, IEEE S MASS STOR SYS
   Siewert Samuel Burk, 2010, US Patent App, Patent No. [12/ 364,271, 12/]
   Srinivasan Kiran, 2012, 10 USENIX C FILE STO, P299
   Stevens M, 2017, LECT NOTES COMPUT SC, V10401, P570, DOI 10.1007/978-3-319-63688-7_19
   Tarasov Vasily, 2014, P 2014 OTT LIN S
   Tsuchiya Y, 2011, IEEE S MASS STOR SYS
   Wang H., 2014, P 12 USENIX C FIL ST, P229
   Wang H, 2012, INT CONF IMAGE ANAL, P15
   Weil SA, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P307
   Xia W, 2016, PROCEEDINGS OF USENIX ATC '16: 2016 USENIX ANNUAL TECHNICAL CONFERENCE, P101
   Xia W, 2016, P IEEE, V104, P1681, DOI 10.1109/JPROC.2016.2571298
   Zhu B, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P269
NR 40
TC 9
Z9 11
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD APR
PY 2019
VL 15
IS 1
SI SI
AR 4
DI 10.1145/3295461
PG 26
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HU9GE
UT WOS:000465601900004
OA Bronze
DA 2024-07-18
ER

PT J
AU Wei, QS
   Chen, JX
   Chen, C
AF Wei, Qingsong
   Chen, Jianxi
   Chen, Cheng
TI Accelerating File System Metadata Access with Byte-Addressable
   Nonvolatile Memory
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Performance; Consistency; Design; File system; metadata acceleration;
   nonvolatile memory
ID PHASE-CHANGE MEMORY; TECHNOLOGY; STORAGE; RAM
AB File system performance is dominated by small and frequent metadata access. Metadata is stored as blocks on the hard disk drive. Partial metadata update results in whole-block read or write, which significantly amplifies disk I/O. Furthermore, a huge performance gap between the CPU and disk aggravates this problem. In this article, a file system metadata accelerator (referred to as FSMAC) is proposed to optimize metadata access by efficiently exploiting the persistency and byte-addressability of Nonvolatile Memory (NVM). The FSMAC decouples data and metadata access path, putting data on disk and metadata in byte-addressable NVM at runtime. Thus, data is accessed in a block from I/O the bus and metadata is accessed in a byte-addressable manner from the memory bus. Metadata access is significantly accelerated and metadata I/O is eliminated because metadata in NVM is no longer flushed back to the disk periodically. A lightweight consistency mechanism combining fine-grained versioning and transaction is introduced in the FSMAC. The FSMAC is implemented on a real NVDIMM platform and intensively evaluated under different workloads. Evaluation results show that the FSMAC accelerates the file system up to 49.2 times for synchronized I/O and 7.22 times for asynchronized I/O. Moreover, it can achieve significant performance speedup in network storage and database environment, especially for metadata-intensive or write-dominated workloads.
C1 [Wei, Qingsong; Chen, Cheng] ASTAR, Data Storage Inst, Singapore 117608, Singapore.
   [Chen, Jianxi] Huazhong Univ Sci & Technol, Wuhan Natl Lab Optoelect, Wuhan 430074, Peoples R China.
C3 Agency for Science Technology & Research (A*STAR); A*STAR - Data Storage
   Institute; Huazhong University of Science & Technology
RP Chen, JX (corresponding author), Huazhong Univ Sci & Technol, Wuhan Natl Lab Optoelect, Wuhan 430074, Peoples R China.
EM WEI_Qingsong@dsi.a-star.edu.sg; chenjx@mail.hust.edu.cn
RI WEI, Qingsong/P-4159-2019
FU A*STAR (Agency for Science, Technology and Research), Singapore
   [112-172-0010]
FX This work is supported by A*STAR (Agency for Science, Technology and
   Research), Singapore under Grant No. 112-172-0010.
CR AgigaTech, 2012, AGIGARAM DDR3 NONV D
   [Anonymous], P USENIX C FIL STOR
   [Anonymous], 2008, P USENIX ANN TECHN C
   [Anonymous], 1997, WINDOWS NT FILE SYST
   [Anonymous], 2014, AERIE FLEXIBLE FILE
   [Anonymous], P ACM S OP SYST PRIN
   [Anonymous], 2012, FAST
   Best S., 2002, JFS FOR LIN
   Bonwick J., 2007, Zfs: The last word in file systems
   Burr GW, 2008, IBM J RES DEV, V52, P449, DOI 10.1147/rd.524.0449
   Card R., 1994, PROCESSDINGS 1 DUTCH, P5
   Caulfield A. M., 2012, P INT C ARCH SUPP PR
   Chen J. X., 2013, P 29 IEEE C MASS DAT
   Chidambaram V., 2012, P 10 USENIX C FIL ST
   Coburn J., 2011, P INT C ARCH SUPP PR
   Doh I. H., 2007, P INT C EMB SYST EMS
   Dulloor Subramanya R., 2014, P 9 EUR C COMP SYST
   Fryer D., 2012, P USENIX C FIL STOR
   Greenan K. M., 2006, P INT C EMB SOFTW EM
   Jang J. E., 2012, P 18 IEEE REAL TIM E, P5
   Josephson W.K., 2010, PROC USENIX C FILE S, V6, P85
   Jung J, 2010, ACM T STORAGE, V6, DOI 10.1145/1714454.1714457
   Kawahara T, 2011, IEEE DES TEST COMPUT, V28, P52, DOI 10.1109/MDT.2010.97
   Lee BC, 2009, CONF PROC INT SYMP C, P2, DOI 10.1145/1555815.1555758
   Lee E., 2012, P 28 IEEE C MASS DAT
   Lee E., 2013, P USENIX C FIL STOR
   Lu Y., 2013, P USENIX C FIL STOR
   Mathur A., 2007, 2007 LIN S, VTwo, P21
   Mckusick M. K., 1986, FSCK UNIX FILE SYSTE
   Moraru I., 2011, CMUPDL1114
   Narayanan Dushyanth, 2012, P INT C ARCH SUPP PR
   Park Y. W., 2008, P 2008 ACM S APPL CO
   Park Y, 2011, IEEE T COMPUT, V60, P321, DOI 10.1109/TC.2010.135
   Qureshi MK, 2009, CONF PROC INT SYMP C, P24, DOI 10.1145/1555815.1555760
   Raoux S, 2008, IBM J RES DEV, V52, P465, DOI 10.1147/rd.524.0465
   Reiser Hans., 2004, ReiserFS
   Sourceforge, 2007, FSMARK
   Sweeney A., 1996, P USENIX ANN TECHN C
   Traeger Avishay, 2008, ACM Transaction on Storage, V4, DOI 10.1145/1367829.1367831
   Tweedie S. C., 1998, P 4 ANN LIN EXP DURH
   Venkataraman S., 2011, P USENIX C FIL STOR
   Viking Technology, 2012, ARXCIS NV TM NONV DI
   Volos Haris, 2011, P INT C ARCH SUPP PR
   Wang A. A., 2006, ACM Transaction on Storage, V2, P309, DOI 10.1145/1168910.1168914
   Wu Xiaojian, 2011, P 2011 INT C HIGH PE
NR 45
TC 9
Z9 11
U1 0
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUL
PY 2015
VL 11
IS 3
AR 12
DI 10.1145/2766453
PG 28
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CN9RP
UT WOS:000358786900002
DA 2024-07-18
ER

PT J
AU You, GW
   Hwang, SW
   Jain, N
AF You, Gae-Won
   Hwang, Seung-Won
   Jain, Navendu
TI Ursa: Scalable Load and Power Management in Cloud Storage Systems
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Algorithms; Design; Management; Performance; Load management; power
   management; storage; optimization; linear programming
AB Enterprise and cloud data centers are comprised of tens of thousands of servers providing petabytes of storage to a large number of users and applications. At such a scale, these storage systems face two key challenges: (1) hot-spots due to the dynamic popularity of stored objects; and (2) high operational costs due to power and cooling. Existing storage solutions, however, are unsuitable to address these challenges because of the large number of servers and data objects. This article describes the design, implementation, and evaluation of Ursa, a system that scales to a large number of storage nodes and objects, and aims to minimize latency and bandwidth costs during system reconfiguration. Toward this goal, Ursa formulates an optimization problem that selects a subset of objects from hot-spot servers and performs topology-aware migration to minimize reconfiguration costs. As exact optimization is computationally expensive, we devise scalable approximation techniques for node selection and efficient divide-and-conquer computation. We also show that the same dynamic reconfiguration techniques can be leveraged to reduce power costs by dynamically migrating data off under-utilized nodes, and powering up servers neighboring existing hot-spots to reduce reconfiguration costs. Our evaluation shows that Ursa achieves cost-effective load management, is time-responsive in computing placement decisions (e. g., about two minutes for 10K nodes and 10M objects), and provides power savings of 15%-37%.
C1 [You, Gae-Won; Hwang, Seung-Won] Pohang Univ Sci & Technol, Dept Comp Sci & Engn, Pohang, South Korea.
C3 Pohang University of Science & Technology (POSTECH)
RP Hwang, SW (corresponding author), Pohang Univ Sci & Technol, Dept Comp Sci & Engn, Pohang, South Korea.
EM swhwang@postech.ac.kr
FU MKE (The Ministry of Knowledge Economy), Korea; Microsoft Research; NIPA
   (National IT Industry Promotion Agency) [NIPA-2011-C1810-1102-0008]
FX This research was supported by the MKE (The Ministry of Knowledge
   Economy), Korea and Microsoft Research, under IT/SW Creative research
   program supervised by the NIPA (National IT Industry Promotion Agency)
   (NIPA-2011-C1810-1102-0008).
CR Abbadi A. E., 2011, P VLDB C
   Abbadi A. E., 2011, P SIGMOD C
   Ahmad I., 2010, P FAST C
   Anderson E, 2005, ACM T COMPUT SYST, V23, P337, DOI 10.1145/1113574.1113575
   [Anonymous], [No title captured]
   [Anonymous], 2012, WINDOWS AZURE
   Babu S., 2010, P ICAC C
   Balakrishnan M., 2007, P HOTOS C
   Barroso LA, 2007, COMPUTER, V40, P33, DOI 10.1109/MC.2007.443
   Chase J. S., 2001, P SOSP C
   Curino C., 2010, P VLDB C
   Dahlin M., 2002, P OSD C
   Daudjee K, 2010, P CLOUDDB C
   Elmore A., 2010, Who's Driving this Cloud? Towards Efficient Migration for Elastic and Autonomic Multitenant Databases
   Ghemawat S., 2006, P OSDI C
   Ghemawat S., 2004, P OSDI C
   Ghemawat S., 2003, ACM SIGOPS OPERATING, V37, P29, DOI [DOI 10.1145/945445.945450, DOI 10.1145/1165389.945450, 10.1145/1165389.945450]
   Harizopoulos S., 2010, P SIGMOD C
   Hiller F.S., 2005, Introduction to Operations Research, VEighth
   Lang W, 2009, SIGMOD RECORD, V38, P35, DOI 10.1145/1815948.1815956
   Litwin W., 1980, P VLDB C
   Maltzahn C., 2006, P OSDI C
   Narayanan D., 2008, ACM T STORAGE, V4, P3
   Neogi A., 2008, P MIDDL C
   Ooi B. C., 2010, P VLDB C
   Packard H., 2005, P DSOM C
   Rangaswami R., 2010, P FAST C
   Sankar S., 2008, P MASCOTS C
   Schindler J., 2008, P HIPC C
   Sengupta S., 2009, P SIGCOMM C
   Thereska E., 2008, P OSDI C
   Wang X., 2010, P ICDE C
   Wylie J. J., 2005, P FAST C
   Yin Q., 2009, P MIDDL C
   Zeldovich N., 2011, P CIDR C
   Zeng H.-J., 2011, P MIDDL C
   Zhou K., 2005, P CIT C
NR 37
TC 7
Z9 10
U1 0
U2 20
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAR
PY 2013
VL 9
IS 1
AR 1
DI 10.1145/2435204.2435205
PG 29
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 114PF
UT WOS:000316753700001
DA 2024-07-18
ER

PT J
AU Carns, P
   Harms, K
   Allcock, W
   Bacon, C
   Lang, S
   Latham, R
   Ross, R
AF Carns, Philip
   Harms, Kevin
   Allcock, William
   Bacon, Charles
   Lang, Samuel
   Latham, Robert
   Ross, Robert
TI Understanding and Improving Computational Science Storage Access through
   Continuous Characterization
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Measurement; Performance; I/O characterization; parallel file systems
AB Computational science applications are driving a demand for increasingly powerful storage systems. While many techniques are available for capturing the I/O behavior of individual application trial runs and specific components of the storage system, continuous characterization of a production system remains a daunting challenge for systems with hundreds of thousands of compute cores and multiple petabytes of storage. As a result, these storage systems are often designed without a clear understanding of the diverse computational science workloads they will support.
   In this study, we outline a methodology for scalable, continuous, systemwide I/O characterization that combines storage device instrumentation, static file system analysis, and a new mechanism for capturing detailed application-level behavior. This methodology allows us to identify both system-wide trends and application-specific I/O strategies. We demonstrate the effectiveness of our methodology by performing a multilevel, two-month study of Intrepid, a 557-teraflop IBM Blue Gene/P system. During that time, we captured application-level I/O characterizations from 6,481 unique jobs spanning 38 science and engineering projects. We used the results of our study to tune example applications, highlight trends that impact the design of future storage systems, and identify opportunities for improvement in I/O characterization methodology.
C1 [Carns, Philip; Lang, Samuel; Latham, Robert; Ross, Robert] Argonne Natl Lab, Div Math & Comp Sci, Argonne, IL 60439 USA.
   [Harms, Kevin; Allcock, William; Bacon, Charles] Argonne Natl Lab, Argonne Leadership Comp Facil, Argonne, IL 60439 USA.
C3 United States Department of Energy (DOE); Argonne National Laboratory;
   United States Department of Energy (DOE); Argonne National Laboratory
RP Carns, P (corresponding author), Argonne Natl Lab, Div Math & Comp Sci, 9700 S Cass Ave, Argonne, IL 60439 USA.
EM carns@mcs.aml.gov
OI Harms, Kevin/0000-0002-3019-7532; Allcock, William/0000-0002-7984-6847;
   Latham, Rob/0000-0002-5285-6375
FU Office of Advanced Scientific Computing Research, Office of Science,
   U.S. Department of Energy [DE-AC02-06CH11357]; Office of Science of the
   U.S. Department of Energy [DE-AC02-Q6CH11357]
FX This work was supported by the Office of Advanced Scientific Computing
   Research, Office of Science, U.S. Department of Energy, under Contract
   DE-AC02-06CH11357. This research used resources of the Argonne
   Leadership Computing Facility at Argonne National Laboratory, which is
   supported by the Office of Science of the U.S. Department of Energy
   under contract DE-AC02-Q6CH11357.
CR Agrawal Nitin, 2008, Performance Evaluation Review, V36, P52, DOI 10.1145/1453175.1453184
   Anderson E., 2009, Proccedings of the 7th conference on File and stroage technologies table of contents, P139
   [Anonymous], 2008, P USENIX ANN TECHN C
   CARNS P., 2009, P WORKSH INT ARCH SC
   Darshan, 2010, DARSHAN
   Dayal S., 2008, CMUPDL08109
   FWang QXin., 2004, P 21 IEEE 12 NASA GO, P139
   Ganger G.R., 1995, Proceedings of the Computer Measurement Group (CMG) Conference, P1263
   Godard S., 2010, Sysstat utilities home page
   Kim YR, 2010, PSYCHIAT INVEST, V7, P1, DOI [10.4306/pi.2010.7.1.1, 10.1186/1743-8977-7-20]
   KLUNDT R, 2008, SAND20083684 SAND NA
   Konwinski A., 2007, Proceedings of the 2nd International Workshop on Petascale data storage: held in conjunction with Supercomputing '07, P56, DOI [10.1145/1374596.1374610, DOI 10.1145/1374596.1374610]
   Lang S., 2009, Proceedings of the Conference on High Performance Computing Networking, Storage and Analysis, p40:1
   LANL-TRACE, 2010, HPC 5 OP SOURC SOFTW
   LIAO W., 2008, P ACM IEEE C SUP
   Nieuwejaar N, 1996, IEEE T PARALL DISTR, V7, P1075, DOI 10.1109/71.539739
   Noeth M, 2009, J PARALLEL DISTR COM, V69, P696, DOI 10.1016/j.jpdc.2008.09.001
   Reed D.A., 1993, P SCALABLE PARALLEL, P104
   Roth P.C., 2007, PDSW 07 P 2 INT WORK, P50, DOI DOI 10.1145/1374596.1374609
   SCHMUCK F., 2002, P FAST C FIL STOR TE
   Seelam S., 2008, P IEEE INT PAR DISTR
   Smirni E, 1997, LECT NOTES COMPUT SC, V1245, P169, DOI 10.1007/BFb0022205
   Traeger A., 2008, Trans. Storage, V4, P1
   Uselton A., 2010, P 24 IEEE INT PAR DI
   Vetter JS, 2001, ACM SIGPLAN NOTICES, V36, P123, DOI 10.1145/568014.379590
   VIJAYAKUMAR K., 2011, P INT C PAR PROC
   Vijayakumar K., 2009, Proceedings of the 4th Annual Workshop on Petascale Data Storage, P26
   Wright N. J., 2009, P 10 LCI INT C HIGH
   Yu Hao., 2006, P 12 INT S HIGH PERF
NR 29
TC 132
Z9 149
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD OCT
PY 2011
VL 7
IS 3
AR 8
DI 10.1145/2027066.2027068
PG 26
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 990LO
UT WOS:000307632600002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Koller, R
   Rangaswami, R
AF Koller, Ricardo
   Rangaswami, Raju
TI I/O Deduplication: Utilizing Content Similarity to Improve I/O
   Performance
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Performance; Storage systems; I/O deduplication; content-based
   caching; content-based I/O scheduling
AB Duplication of data in storage systems is becoming increasingly common. We introduce I/O Deduplication, a storage optimization that utilizes content similarity for improving I/O performance by eliminating I/O operations and reducing the mechanical delays during I/O operations. I/O Deduplication consists of three main techniques: content-based caching, dynamic replica retrieval, and selective duplication. Each of these techniques is motivated by our observations with I/O workload traces obtained from actively-used production storage systems, all of which revealed surprisingly high levels of content similarity for both stored and accessed data. Evaluation of a prototype implementation using these workloads showed an overall improvement in disk I/O performance of 28 to 47% across these workloads. Further breakdown also showed that each of the three techniques contributed significantly to the overall performance improvement.
C1 [Koller, Ricardo] Florida Int Univ, Sch Comp & Informat Sci, Miami, FL 33199 USA.
C3 State University System of Florida; Florida International University
RP Koller, R (corresponding author), Florida Int Univ, Sch Comp & Informat Sci, 11200 SW 8th St,ECS 234, Miami, FL 33199 USA.
EM rkoll001@cs.fiu.edu
OI Rangaswami, Raju/0009-0000-5243-9451
FU NSF [CNS-0747038, CNS-0821345, IIS-0837716]
FX This work was supported by the NSF grants CNS-0747038, CNS-0821345, and
   IIS-0837716.
CR AKYUREK S, 1995, ACM T COMPUT SYST, V13, P89, DOI 10.1145/201045.201046
   [Anonymous], 2009, P USENIX ANN TECHNIC
   [Anonymous], P USENIX C FIL STOR
   [Anonymous], P ACM SOSP
   Bhadkamkar M., 2009, P USENIX C FIL STOR
   BLOOM BH, 1970, COMMUN ACM, V13, P422, DOI 10.1145/362686.362692
   BRIN S., 1995, P ACM SIGMOD C
   Ellard D., 2003, P USENIX C FIL STOR
   EMC CORPORATION, EMC INV
   GILL B., 2008, P USENIX C FIL STOR
   GRAY J., 1988, P INT C VER LARG DAT
   GRAY J, 2000, P IEEE INT C DAT ENG
   GUERRA J., 2008, ACM OPER SYST REV, V42, P6
   GUPTA D., 2008, P USENIX S OP SYST D
   Hsu WW, 2005, ACM T COMPUT SYST, V23, P424, DOI 10.1145/1113574.1113577
   HUANG H., 2005, P ACM SOSP
   IBM Corporation, IBM SYST STOR SAN VO
   JAIN N., 2005, P USENIX C FIL STOR
   Jiang S, 2005, P USENIX ANN TECHN C
   KULKARNI P., 2004, P USENIX ANN TECHN C
   Leung A.W., 2008, 2008 USENIX ANN TECH, P213
   LI X, 2005, P USENIX C FIL STOR
   Lillibridge M., 2009, P USENIX C FIL STOR
   MATTSON RL, 1970, IBM SYST J, V9, P78, DOI 10.1147/sj.92.0078
   Megiddo N., 2003, P USENIX C FIL STOR
   MILOS G, 2009, P USENIX ANN TECHN C
   MORREY C.B, 2003, P IEEE NASA MSST
   NETWORK APPLIANCE INC., NETAPP 5 SER HET STO
   ORJI C. U, 1993, P ACM SIGMOD
   Rhea S., 2008, P USENIX ANN TECHN C
   RUEMMLER C, 1991, HPLCSP9130
   SOLWORTH J.A, 1991, P 1 INT C PAR DISTR
   Tolia N, 2003, P USENIX ANN TECHN C
   VONGSATHORN P, 1990, SOFTWARE PRACT EXPER, V20, P225, DOI 10.1002/spe.4380200302
   Waldspurger C. A., 2002, P USENIX S OP SYST D
   WONG CK, 1980, ACM COMPUT SURV, V12, P167
   Wong T.M., 2002, P USENIX ANN TECHN C
   Yu X., 2000, P USENIX S OP SYST D
   ZHANG C., 2002, P USENIX C FIL STOR
   Zhu Benjamin, 2008, P USENIX C FIL STOR
NR 40
TC 96
Z9 135
U1 0
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
J9 ACM T STORAGE
JI ACM Trans. Storage
PD SEP
PY 2010
VL 6
IS 3
AR 13
DI 10.1145/1837915.1837921
PG 26
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA V24QL
UT WOS:000208424800006
DA 2024-07-18
ER

PT J
AU Yang, SL
   Liu, J
   Arpaci-Dusseau, A
   Arpaci-Dusseau, R
AF Yang, Suli
   Liu, Jing
   Arpaci-Dusseau, Andrea
   Arpaci-Dusseau, Remzi
TI Principled Schedulability Analysis for Distributed Storage Systems Using
   Thread Architecture Models
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Request scheduling; thread architecture; performance isolation
ID PETRI-NET
AB In this article, we present an approach to systematically examine the schedulability of distributed storage systems, identify their scheduling problems, and enable effective scheduling in these systems. We use Thread Architecture Models (TAMs) to describe the behavior and interactions of different threads in a system, and show both how to construct TAMs for existing systems and utilize TAMs to identify critical scheduling problems. We specify three schedulability conditions that a schedulable TAM should satisfy: completeness, local enforceability, and independence; meeting these conditions enables a system to easily support different scheduling policies. We identify five common problems that prevent a system from satisfying the schedulability conditions, and show that these problems arise in existing systems such as HBase, Cassandra, MongoDB, and Riak, making it difficult or impossible to realize various scheduling disciplines. We demonstrate how to address these schedulability problems using both direct and indirect solutions, with different trade-offs. To show how to apply our approach to enable scheduling in realistic systems, we develop Tamed-HBase and Muzzled-HBase, sets of modifications to HBase that can realize the desired scheduling disciplines, including fairness and priority scheduling, even when presented with challenging workloads.
C1 [Yang, Suli; Liu, Jing; Arpaci-Dusseau, Andrea; Arpaci-Dusseau, Remzi] Univ Wisconsin, Madison, WI 53706 USA.
C3 University of Wisconsin System; University of Wisconsin Madison
RP Yang, SL (corresponding author), Univ Wisconsin, Madison, WI 53706 USA.
EM suli@cs.wisc.edu; jingliu@cs.wisc.edu; dusseau@cs.wisc.edu;
   remzi@cs.wisc.edu
OI Arpaci-Dusseau, Andrea/0000-0001-8618-2738; Liu,
   Jing/0000-0003-2485-4038
FU NSF [CNS-1421033, CNS-1218405, CNS-1838733]; DOE [DE-SC0014935]; U.S.
   Department of Energy (DOE) [DE-SC0014935] Funding Source: U.S.
   Department of Energy (DOE)
FX This material was supported by funding from NSF Grants No. CNS-1421033,
   No. CNS-1218405, and No. CNS-1838733, DOE Grant No. DE-SC0014935, and
   donations from EMC, Huawei, Microsoft, NetApp, and VMware. Any opinions,
   findings, and conclusions or recommendations expressed in this material
   are those of the authors and may not reflect the views of NSF, DOE, or
   other institutions.
CR Anandkumar A, 2008, PERF E R SI, V36, P133, DOI 10.1145/1384529.1375473
   Andersson B, 2001, 22ND IEEE REAL-TIME SYSTEMS SYMPOSIUM, PROCEEDINGS, P193, DOI 10.1109/REAL.2001.990610
   Angel S., 2014, P USENIX S OP SYST D
   [Anonymous], 2016, P 25 ACM INT S HIGH
   [Anonymous], 2009, SYSTEM MODELING CONT
   Armstrong Joe, 1996, Concurrent Programming in Erlang
   Barham P, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE SIXTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDE '04), P259
   Beaver D., 2010, US C OP SYST DES IMP
   Bonald Thomas, 2015, ACM SIGMETRICS Performance Evaluation Review, V43, P31, DOI 10.1145/2745844.2745869
   Boxma Onno, 1996, CWI TRACT, V105, P1
   Burrows M, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P335
   Cassandra, 2016, CASS ISS MOV AW SEDA
   Chen M.Y., 2004, Proc. NSDI, P23
   Chen S, 2019, TWENTY-FOURTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXIV), P107, DOI 10.1145/3297858.3304005
   CHETTO H, 1989, IEEE T SOFTWARE ENG, V15, P1261, DOI 10.1109/TSE.1989.559777
   CHIU DM, 1989, COMPUT NETWORKS ISDN, V17, P1, DOI 10.1016/0169-7552(89)90019-6
   Chodorow Kristina, 2013, MongoDB: The Definitive Guide, V2nd
   CLARK DD, 1989, IEEE COMMUN MAG, V27, P23, DOI 10.1109/35.29545
   Cooper Brian F., 2010, P 1 ACM S CLOUD COMP, P143, DOI [DOI 10.1145/1807128.1807152, 10.1145/1807128.1807152]
   Davidrajuh R, 2012, IEEE SYS MAN CYBERN, P1201, DOI 10.1109/ICSMC.2012.6377895
   Dean J, 2013, COMMUN ACM, V56, P74, DOI 10.1145/2408776.2408794
   Elser Josh, 2017, COMMUNICATION
   Faggioli D., 2009, Proceedings of the 2009 ACM symposium on Applied Computing (SAC '09), P1984, DOI 10.1145/1529282.1529723
   Fink Bryan, 2017, COMMUNICATION
   Fink Bryan., 2012, Proceedings of the eleventh ACM SIGPLAN workshop on Erlang workshop, P43
   Fried J, 2020, PROCEEDINGS OF THE 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '20), P281
   George L., 2011, HBASE DEFINITIVE GUI
   Ghemawat S., 2003, ACM SIGOPS OPERATING, V37, P29, DOI [DOI 10.1145/945445.945450, DOI 10.1145/1165389.945450, 10.1145/1165389.945450]
   Ghodsi An, 2011, Computer Communication Review, V41, P507, DOI 10.1145/2018584.2018586
   Gu Dikang, 2017, COMMUNICATION
   Gulati A., 2009, FAST '09: Proccedings of the 7th conference on File and storage technologies, P85
   Gulati A, 2007, PERF E R SI, V35, P13
   Gupta A., 1991, Performance Evaluation Review, V19, P120, DOI 10.1145/107972.107985
   Gupta D, 2006, LECT NOTES COMPUT SC, V4290, P342
   HBase, 2013, HBASE ISS TRACK PAG
   HBase, 2016, HBASE ISS TRACK PAG
   Hildebrand Dean, 2021, Colossus under the hood: a peek into Google's scalable storage system
   Hunt P, 2010, P USENIX ATC BOST MA, P11, DOI 10.5555/1855840.1855851
   IO Visor Project, 2021, BPF COMP COLL BCC
   Jansen Pierre G., 2003, TRCTIT0323 U TWENT
   Karatza H.D., 2000, International Journal of Simulation Systems, Science Technology, P12
   Kay J., 1993, Computer Communication Review, V23, P259, DOI 10.1145/167954.166262
   Kelly FP, 1998, J OPER RES SOC, V49, P237, DOI 10.2307/3010473
   Kleinrock L, 1976, Queueing Systems, V2
   Klophaus Rusty., 2010, ACM SIGPLAN COMMERCI, P14
   Kolisch R, 1997, EUR J OPER RES, V96, P205, DOI 10.1016/S0377-2217(96)00170-1
   KOLLERSTROM J, 1974, J APPL PROBAB, V11, P544, DOI 10.2307/3212698
   Kumar A, 2015, ACM SIGCOMM COMP COM, V45, P1, DOI 10.1145/2785956.2787478
   Lakshman Avinash, 2009, P 3 ACM SIGOPS INT W
   Legrand A, 2003, CCGRID 2003: 3RD IEEE/ACM INTERNATIONAL SYMPOSIUM ON CLUSTER COMPUTING AND THE GRID, PROCEEDINGS, P138, DOI 10.1109/CCGRID.2003.1199362
   LEUNG JYT, 1982, PERFORM EVALUATION, V2, P237, DOI 10.1016/0166-5316(82)90024-4
   Mace J, 2016, PROCEEDINGS OF THE 2016 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '16), P144, DOI 10.1145/2934872.2934878
   Mace Jonathan, 2015, 12 USENIX S NETW SYS, P589
   Matloff N., 2008, INTRO DISCRETE EVENT
   MENGA G, 1984, IEEE T COMPON HYBR, V7, P241, DOI 10.1109/TCHMT.1984.1136363
   MongoDB, 2021, SERV INT BAT PATT
   MongoDB, 2020, COMM DISP
   MongoDB, 2016, MONGODB ISS TRACK PA
   Muthukkaruppan Kannan., 2011, P INT WORKSHOP HIGH
   Nguyen H., 2013, P 10 INT C AUTONOMIC, P69
   Nokleberg C, 2021, COMMUN ACM, V64, P42, DOI 10.1145/3446796
   Ousterhout K, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P184, DOI 10.1145/3132747.3132766
   Pai VS, 1999, PROCEEDINGS OF THE 1999 USENIX ANNUAL TECHNICAL CONFERENCE, P199
   Pelikan Cache, 2020, TAM TAIL LAT ACH PRE
   PETERSON JL, 1977, COMPUT SURV, V9, P223, DOI 10.1145/356698.356702
   Pu QF, 2016, 13TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION (NSDI '16), P393
   Puthalath Lalith Suresh, 2016, THESIS TU BERLIN
   RAMAMRITHAM K, 1989, IEEE T COMPUT, V38, P1110, DOI 10.1109/12.30866
   Reda W, 2017, PROCEEDINGS OF THE TWELFTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS (EUROSYS 2017), P95, DOI 10.1145/3064176.3064209
   Ricci Robert, 2014, USENIX;login, V39, P6
   Rizzo L., 1997, Computer Communication Review, V27, P31, DOI 10.1145/251007.251012
   Schroeder B, 2006, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD SYMPOSIUM ON NETWORKED SYSTEMS DESIGN & IMPLEMENTATION (NSDI 06), P239
   Serenyi Denis., 2017, CLUSTER LEVEL STORAG
   SHIH HM, 1991, 1991 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS 1-3, P2548, DOI 10.1109/ROBOT.1991.132010
   Shirazi B.A., 1995, Scheduling and Load Balancing in Parallel and Distributed Systems
   Shue D., 2012, P 10 USENIX S OP SYS, P349
   Shue David., 2014, Proceedings of the Ninth European Conference on Computer Systems, P17
   Shvachko K, 2010, IEEE S MASS STOR SYS
   The Apache Software Foundation, 2021, WELC AP SERVICEMIX
   The Linux Foundation, 2021, EBPF INTR TUT COMM R
   Thereska E, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P182, DOI 10.1145/2517349.2522723
   Toptal A, 2010, INT J PROD RES, V48, P5235, DOI 10.1080/00207540903121065
   Urgaonkar B., 2005, SIGMETRICS Perform. Eval. Rev, V33, P291, DOI [10.1145/1071690.1064252, DOI 10.1145/1071690.1064252]
   Van der Aalst WMP, 1998, J CIRCUIT SYST COMP, V8, P21, DOI 10.1142/S0218126698000043
   von Behren R., 2003, Operating Systems Review, V37, P268, DOI 10.1145/1165389.945471
   Wachs M, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 5TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES ( FAST '07), P61
   Wang Andrew., 2012, P 3 ACM S CLOUD COMP, P1
   Welsh Matt, 2001, P 18 ACM S OP SYST P
   Yang SL, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P161
   Zeng JA, 2015, IEEE INT C CL COMP, P32, DOI 10.1109/CLUSTER.2015.14
   ZHOU MC, 1991, IEEE T ROBOTIC AUTOM, V7, P515, DOI 10.1109/70.86081
   Zhu Timothy., 2014, P ACM S CLOUD COMPUT, P1
NR 92
TC 0
Z9 0
U1 3
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAY
PY 2023
VL 19
IS 2
AR 17
DI 10.1145/3574323
PG 47
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D7OU8
UT WOS:000970592600008
DA 2024-07-18
ER

PT J
AU Jaffer, S
   Mahdaviani, K
   Schroeder, B
AF Jaffer, Shehbaz
   Mahdaviani, Kaveh
   Schroeder, Bianca
TI Improving the Endurance of Next Generation SSD's using WOM-v Codes
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE SSD; endurance; Write Once Memory Code; QLC flash
AB High density Solid State Drives, such as QLC drives, offer increased storage capacity, but a magnitude lower Program and Erase (P/E) cycles, limiting their endurance and hence usability. We present the design and implementation of non-binary, Voltage-Based Write-Once-Memory (WOM-v) Codes to improve the lifetime of QLC drives. First, we develop a FEMU based simulator test-bed to evaluate the gains of WOM-v codes on real world workloads. Second, we propose and implement two optimizations, an efficient garbage collection mechanism and an encoding optimization to drastically improve WOM-v code endurance without compromising performance. Third, we propose analytical approaches to obtain estimates of the endurance gains under WOM-v codes. We analyze the Greedy garbage collection technique with uniform page access distribution and the Least Recently Written (LRW) garbage collection technique with skewed page access distribution in the context of WOM-v codes. We find that although both approaches overestimate the number of required erase operations, the model based on greedy garbage collection with uniform page access distribution provides tighter bounds. A careful evaluation, including microbenchmarks and trace-driven evaluation, demonstrates that WOM-v codes can reduce Erase cycles for QLC drives by 4.4x-11.1x for real world workloads with minimal performance overheads resulting in improved QLC SSD lifetime.
C1 [Jaffer, Shehbaz; Mahdaviani, Kaveh; Schroeder, Bianca] Univ Toronto, 40 St George St, Toronto, ON M5S 2E4, Canada.
C3 University of Toronto
RP Jaffer, S (corresponding author), Univ Toronto, 40 St George St, Toronto, ON M5S 2E4, Canada.
EM shehbaz@cs.toronto.edu; mandaviani@cs.toronto.edu; bianca@cs.toronto.edu
CR [Anonymous], 2020, W DIGITAL TOSHIBA TA
   [Anonymous], 2022, SLC VSMLC VS TLC VS
   [Anonymous], 2014, FAST
   [Anonymous], 2022, QLC NAND WHAT CAN WE
   [Anonymous], 2022, TLC VS QLC SSDS ULTI
   [Anonymous], 2022, 4 QLC WORKLOADS WHY
   [Anonymous], 2013, Proceedings of the 5th USENIX Conference on Hot Topics in Storage and File Systems. HotStorage'13
   [Anonymous], 2022, WOM V SOURCE CODE
   [Anonymous], 2021, FEMU TLC QLC NAND SU
   Arpith K., 2020, P 12 USENIXWORKSHOP
   Bjorling M, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P359
   Desnoyers Peter, 2012, ACM T STORAGE
   Jaffer S, 2022, PROCEEDINGS OF THE 20TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, FAST 2022, P117
   Jaffer Shehbaz, 2020, P 12 USENIXWORKSHOP
   Kavalanekar S, 2008, I S WORKL CHAR PROC, P111
   Koller R, 2010, ACM T STORAGE, V6, DOI 10.1145/1837915.1837921
   Li HC, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P83
   Li JH, 2020, I S WORKL CHAR PROC, P37, DOI 10.1109/IISWC50251.2020.00013
   Maneas S, 2022, PROCEEDINGS OF THE 20TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, FAST 2022, P165
   Maneas S, 2020, PROCEEDINGS OF THE 18TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P137
   Margaglia F, 2015, IEEE S MASS STOR SYS
   Margaglia F, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P95
   Narayanan D, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P253
   NitinAgrawal William J., 2007, P 5 USENIX C FILE ST
   Pan YQ, 2020, 2020 IEEE 20TH INTERNATIONAL CONFERENCE ON SOFTWARE QUALITY, RELIABILITY, AND SECURITY (QRS 2020), P329, DOI 10.1109/QRS51102.2020.00051
   RIVEST RL, 1982, INFORM CONTROL, V55, P1, DOI 10.1016/S0019-9958(82)90344-8
   Rosenblum M., 1992, THESIS U CALIFORNIA
   SNIA IOTTA Trace Repository, 2020, YCSB ROCKSDB SSD TRA
   Tai A, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P977
   Thomas T. M.CoverandJ. A., 1991, ELEMENTS INFORM THEO, DOI 10.1002/0471200611
   Yaakobi E, 2010, IEEE GLOBE WORK, P1856, DOI 10.1109/GLOCOMW.2010.5700263
   Yaakobi Eitan, 2015, P IEEE INT S INFORM
   Yaakobi Eitan, 2018, P 10 USENIX WORKSHOP
   Yadgar G., 2015, P 13 USENIX C FILE S, P257
   Yadgar G, 2021, ACM T STORAGE, V17, DOI 10.1145/3423137
   Yadgar G, 2018, ACM T STORAGE, V14, DOI 10.1145/3177886
NR 36
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2022
VL 18
IS 4
AR 29
DI 10.1145/3565027
PG 32
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7F8OU
UT WOS:000902100700002
DA 2024-07-18
ER

PT J
AU Kang, DH
   Lee, SW
   Eom, YI
AF Kang, Dong Hyun
   Lee, Sang-Won
   Eom, Young Ik
TI LDJ: Version Consistency Is Almost Free on Commercial Storage Devices
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Crash consistency; file system; compression
AB In this article, we propose a simple but practical and efficient optimization scheme for journaling in ext4, called lightweight data journaling (LD3). By compressing journaled data prior to writing, LD3 can perform comparable to or even faster than the default ordered journaling (OJ) mode in ext4 on top of both HDDs and flash storage devices, while still guaranteeing the version consistency of the data journaling (DJ) mode. This surprising result can be explained with three main reasons. First, on modern storage devices, the sequential write pattern dominating in DJ mode is more and more high-performant than the random one in OJ mode. Second, the compression significantly reduces the amount of journal writes, which will in turn make the write completion faster and prolong the lifespan of storage devices. Third, the compression also enables the atomicity of each journal write without issuing an intervening FLUSH command between journal data blocks and commit block, thus halving the number of costly FLUSH calls in LD3. We have prototyped our LD3 by slightly modifying the existing ext4 with jbd2 for journaling and also e2fsck for recovery; less than 300 lines of source code were changed. Also, we carried out a comprehensive evaluation using four standard benchmarks and three real applications. Our evaluation results clearly show that LD3 outperforms the OJ mode by up to 9.6x on the real applications.
C1 [Kang, Dong Hyun] Dongguk Univ Gyeongju, Gyeongju, South Korea.
   [Lee, Sang-Won; Eom, Young Ik] Sungkyunkwan Univ, Suwon, South Korea.
C3 Dongguk University; Sungkyunkwan University (SKKU)
RP Eom, YI (corresponding author), Sungkyunkwan Univ, Suwon, South Korea.
EM dhkang@dongguk.ac.kr; swlee@skku.edu; yieom@skku.edu
RI Kang, Donghyun/S-7850-2018
OI Kang, Donghyun/0000-0003-4362-9944
FU Next-Generation Information Computing Development Program through the
   National Research Foundation of Korea (NRF) - Ministry of Science, ICT
   [NRF-2015M3C4A7065696]; Institute of Information & communications
   Technology Planning & Evaluation (IITP) - Korea government (MSIT)
   [2015-0-00314]
FX This research was supported by Next-Generation Information Computing
   Development Program through the National Research Foundation of Korea
   (NRF) funded by the Ministry of Science, ICT (No. NRF-2015M3C4A7065696)
   and supported by Institute of Information & communications Technology
   Planning & Evaluation (IITP) grant funded by the Korea government (MSIT)
   (No. 2015-0-00314, NVRam-based High-performance Open Source DBMS
   Development).
CR Aghayev A, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P105
   [Anonymous], 2015, Proceedings of the 13th USENIX Conference on File and Storage Technologies
   Chen QS, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P241
   Chidambaram V, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P228, DOI 10.1145/2517349.2522726
   Chidambaram Vijay, 2012, P 10 USENIX C FIL ST
   Choe W, 2013, IEEE ICCE, P254, DOI 10.1109/ICCE.2013.6486883
   Dou XZ, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P73
   Douglis F, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P29
   Hahn SS, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P759
   He WP, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P121
   Hu Yige, 2017, P 16 WORKSHOP HOT TO, P1
   HUFFMAN DA, 1952, P IRE, V40, P1098, DOI 10.1109/JRPROC.1952.273898
   Kang DH, 2017, PROCEEDINGS OF THE 8TH ASIA-PACIFIC WORKSHOP ON SYSTEMS (APSYS '17), DOI 10.1145/3124680.3124718
   Kang JH, 2014, PHYS REV X, V4, DOI 10.1103/PhysRevX.4.031005
   Kang WH, 2014, SIGMOD'14: PROCEEDINGS OF THE 2014 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P529, DOI 10.1145/2588555.2595632
   Kim HS, 2016, BMC CANCER, V16, DOI 10.1186/s12885-016-2141-4
   Kim H, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P271
   Kim W.-H., 2014, Proceedings of the 12th USENIX Conference on File and Storage Technologies (FAST 14), P273
   Lautenschlager F, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P229
   Lee, 2015, USENIX ATC, V15, P235
   Lee Changman, 2015, 13 USENIX C FILE STO, P273
   Ma A., 2013, P 11 USENIX C FILE S, P1
   Min C., 2015, Usenix Atc'15, P221
   Min Changwoo, P 10 USENIX C FIL ST
   Park D, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P787
   Pillai TS, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P181
   Porter D.A., 2009, PHASE TRANSFORMATION, V3rd, P1
   Prabhakaran Vijayan, 2005, P OF THE 20 ACM S ON, V39, P206
   Rahmani H, 2014, 2014 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING CONFERENCE, P311, DOI 10.1109/VCIP.2014.7051566
   Rajimwale A, 2011, I C DEPEND SYS NETWO, P518, DOI 10.1109/DSN.2011.5958264
   Rodeh O, 2013, ACM T STORAGE, V9, DOI 10.1145/2501620.2501623
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   Shen K., 2014, Proceedings of the 12th USENIX Conference on File and Storage Technologies FAST 14, P287
   Sweeney A., 1996, P USENIX ANN TECHN C
   Tarasov Vasily, 2016, login Usenix Mag, V41, P1
   Weiss Z., 2015, 13 USENIX C FIL STOR, P111
   Yeon J, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P201
   ZIV J, 1977, IEEE T INFORM THEORY, V23, P337, DOI 10.1109/TIT.1977.1055714
   ZIV J, 1978, IEEE T INFORM THEORY, V24, P530, DOI 10.1109/TIT.1978.1055934
   Zuck Aviad, 2014, P 2 WORKSH INT NVM F, P1
NR 40
TC 1
Z9 1
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD FEB
PY 2020
VL 15
IS 4
AR 28
DI 10.1145/3365918
PG 20
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NG7LP
UT WOS:000564162600007
DA 2024-07-18
ER

PT J
AU Lu, YY
   Shu, JW
   Zhang, JC
AF Lu, Youyou
   Shu, Jiwu
   Zhang, Jiacheng
TI Mitigating Synchronous I/O Overhead in File Systems on Open-Channel SSDs
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Synchronous I/O; file system; flash memory; open-channel SSD
ID FLASH
AB Synchronous I/O has long been a design challenge in file systems. Although open-channel solid state drives (SSDs) provide better performance and endurance to file systems, they still suffer from synchronous I/Os due to the amplified writes and worse hot/cold data grouping. The reason lies in the controversy design choices between flash write and read/erase operations. While fine-grained logging improves performance and endurance in writes, it hurts indexing and data grouping efficiency in read and erase operations. In this article, we propose a flash-friendly data layout by introducing a built-in persistent staging layer to provide balanced read, write, and garbage collection performance. Based on this, we design a new flash file system (FS) named StageFS, which decouples the content and structure updates. Content updates are logically logged to the staging layer in a persistence-efficient way, which achieves better write performance and lower write amplification. The updated contents are reorganized into the normal data area for structure updates, with improved hot/cold grouping and in a page-level indexing way, which is more friendly to read and garbage collection operations. Evaluation results show that, compared to recent flash-friendly file system (F2FS), StageFS effectively improves performance by up to 211.4% and achieves low garbage collection overhead for workloads with frequent synchronization.
C1 [Lu, Youyou; Shu, Jiwu; Zhang, Jiacheng] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
C3 Tsinghua University
RP Shu, JW (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
EM luyouyou@tsinghua.edu.cn; shujw@tsinghua.edu.cn;
   zhang-jc13@mails.tsinghua.edu.cn
RI Zhang, Jiacheng/HTS-3961-2023; Lu, Youyou/AAE-4401-2020
OI Lu, Youyou/0000-0002-6214-5390
FU National Key Research & Development Program of China [2018YFB1003301];
   National Natural Science Foundation of China [61772300, 61832011];
   Research and Development Plan in Key Field of Guangdong Province
   [2018B010109002]; Huawei Technologies Co. Ltd.; Young Elite Scientists
   Sponsorship Program of China Association for Science and Technology
FX This work is supported by National Key Research & Development Program of
   China (Grant No. 2018YFB1003301), the National Natural Science
   Foundation of China (Grant No. 61772300, 61832011), Research and
   Development Plan in Key Field of Guangdong Province (Grant No.
   2018B010109002), and Huawei Technologies Co. Ltd. Youyou Lu is also
   supported by the Young Elite Scientists Sponsorship Program of China
   Association for Science and Technology.
CR [Anonymous], 2012, P 10 USENIX C FIL ST
   [Anonymous], 2008, P 2008 USENIX ANN TE
   [Anonymous], 2011, FAST
   [Anonymous], P 9 USENIX C FIL STO
   [Anonymous], 2017, P 15 USENIX C FIL ST
   [Anonymous], 2011, P 23 ACM S OP SYST P
   Cao GS, 2014, MICRO NANO LETT, V9, P16, DOI 10.1049/mnl.2013.0612
   Chang F, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P205
   Chen PM, 1996, ACM SIGPLAN NOTICES, V31, P74, DOI 10.1145/248209.237154
   Chidambaram V, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P228, DOI 10.1145/2517349.2522726
   Condit J, 2009, SOSP'09: PROCEEDINGS OF THE TWENTY-SECOND ACM SIGOPS SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P133
   Frost Christopher, 2007, Operating Systems Review, V41, P307, DOI 10.1145/1323293.1294291
   Grupp L. M., 2012, FAST 12 P 10 USENIX
   Gupta A, 2009, ACM SIGPLAN NOTICES, V44, P229, DOI 10.1145/1508284.1508271
   Hitz Dave, 1994, P 1994 USENIX WINT T
   Jannen W., 2015, P 13 USENIX C FIL ST, P301, DOI DOI 10.5555/2750482.2750505
   Jeong S., 2013, Proceedings of the 2013 USENIX conference on Annual Technical Conference, P309
   Josephson W., 2010, P 8 USENIX C FIL STO
   Kim Hyojun, 2012, P 10 USENIX C FIL ST, P1
   Kwon Y, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P460, DOI 10.1145/3132747.3132770
   Lee Changman, 2015, FAST
   Lee Eunji, 2013, P 11 USENIX C FIL ST
   Lee Sang-Won., 2007, P 2007 ACM SIGMOD IN, P55
   Lee Sungjin, 2016, P 14 USENIX C FIL ST
   Li SY, 2017, SC'17: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS, DOI 10.1145/3126908.3126928
   Lu LY, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P133
   Lu Y., 2015, 2015 IEEE INT SOLID, P1
   Lu YY, 2016, IEEE T COMPUT, V65, P627, DOI 10.1109/TC.2015.2419664
   Lu Youyou, 2014, P IEEE 32 INT C COMP
   Lu Youyou, 2014, P USENIX C FIL STOR, P75
   Lu Youyou, 2013, P 11 USENIX C FIL ST
   McKusick MK, 1999, PROCEEDINGS OF THE FREENIX TRACK, P1
   Nightingale EB, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P1
   ONeil P, 1996, ACTA INFORM, V33, P351, DOI 10.1007/s002360050048
   Ou JX, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901324
   Ouyang J, 2014, ACM SIGPLAN NOTICES, V49, P471, DOI 10.1145/2541940.2541959
   Park Stan, 2013, IEEE ICCE, P225
   Pelley S, 2014, CONF PROC INT SYMP C, P265, DOI 10.1109/ISCA.2014.6853222
   Ren Kai., 2013, USENIX ANN TECHNICAL, P145
   Sct S.T., 1998, Journaling the linux ext2fs filesystem
   Shetty P.J., 2013, 11 USENIX C FILE STO, P17
   Wu Guanying, 2012, P 7 ACM EUR C COMP S, P253
   Xu J, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P323
   Zhang Jiacheng, 2016, P 2016 USENIX ANN TE
NR 44
TC 10
Z9 11
U1 1
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2019
VL 15
IS 3
AR 17
DI 10.1145/3319369
PG 25
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JT2DC
UT WOS:000500805400002
DA 2024-07-18
ER

PT J
AU Vangoor, BKR
   Agarwal, P
   Mathew, M
   Ramachandran, A
   Sivaraman, S
   Tarasov, V
   Zadok, E
AF Vangoor, Bharath Kumar Reddy
   Agarwal, Prafful
   Mathew, Manu
   Ramachandran, Arun
   Sivaraman, Swaminathan
   Tarasov, Vasily
   Zadok, Erez
TI Performance and Resource Utilization of FUSE User-Space File Systems
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE User-space file systems; Linux FUSE
AB Traditionally, file systems were implemented as part of operating systems kernels, which provide a limited set of tools and facilities to a programmer. As the complexity of file systems grew, many new file systems began being developed in user space. Low performance is considered the main disadvantage of user-space file systems but the extent of this problem has never been explored systematically. As a result, the topic of user-space file systems remains rather controversial: while some consider user-space file systems a "toy" not to be used in production, others develop full-fledged production file systems in user space. In this article, we analyze the design and implementation of a well-known user-space file system framework, FUSE, for Linux. We characterize its performance and resource utilization for a wide range of workloads. We present FUSE performance and also resource utilization with various mount and configuration options, using 45 different workloads that were generated using Filebench on two different hardware configurations. We instrumented FUSE to extract useful statistics and traces, which helped us analyze its performance bottlenecks and present our analysis results. Our experiments indicate that depending on the workload and hardware used, performance degradation (throughput) caused by FUSE can be completely imperceptible or as high as -83%, even when optimized; and latencies of FUSE file system operations can be increased from none to 4x when compared to Ext4. On the resource utilization side, FUSE can increase relative CPU utilization by up to 31% and underutilize disk bandwidth by as much as -80% compared to Ext4, though for many data-intensive workloads the impact was statistically indistinguishable. Our conclusion is that user-space file systems can indeed be used in production (non-"toy") settings, but their applicability depends on the expected workloads.
C1 [Vangoor, Bharath Kumar Reddy] Avere Syst Inc, Pittsburgh, PA 15212 USA.
   [Vangoor, Bharath Kumar Reddy; Agarwal, Prafful; Mathew, Manu; Ramachandran, Arun; Sivaraman, Swaminathan] SUNY Stony Brook, 336 New Comp Sci, Stony Brook, NY 11794 USA.
   [Tarasov, Vasily] IBM Res Almaden, 650 Harry Rd, San Jose, CA 95120 USA.
   [Zadok, Erez] SUNY Stony Brook, 349 New Comp Sci, Stony Brook, NY 11794 USA.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Stony Brook; International Business Machines (IBM); State
   University of New York (SUNY) System; State University of New York
   (SUNY) Stony Brook
RP Vangoor, BKR (corresponding author), Avere Syst Inc, Pittsburgh, PA 15212 USA.; Vangoor, BKR (corresponding author), SUNY Stony Brook, 336 New Comp Sci, Stony Brook, NY 11794 USA.
EM vangoor.bharath@gmail.com; praagarwal@cs.stonybrook.edu;
   manmathew@cs.stonybrook.edu; arunrm87@gmail.com;
   sswaminathan92@gmail.com; vtarasov@us.ibm.com; ezk@cs.stonybrook.edu
FU NSF [CNS-1251137, CNS-1302246, CNS-1305360, CNS-1622832]; ONR
   [12055763]; Dell-EMC; NetApp; IBM
FX This work was made possible in part thanks to Dell-EMC, NetApp, and IBM
   support; and NSF awards CNS-1251137, CNS-1302246, CNS-1305360, and
   CNS-1622832; and ONR award 12055763.
CR Accetta M., 1986, Proceedings of the Summer 1986 USENIX Conference, P93
   Ahltorp Magnus, 2000, P MAN ADM DISTR ENV
   Alexandrov A. D., 1997, Proceedings of the USENIX 1997 Annual Technical Conference, P77
   [Anonymous], 2002, P 1 USENIX C FIL STO
   [Anonymous], 2008, USENIX FAST
   AT&T Bell Laboratories, 1995, PLAN 9 PROGR MAN
   Avfs, 2019, AVFS VIRT FIL
   Axboe Jens, 2009, PER BACKING DEVICE W
   Bent J., 2009, 0902117 LAUR LANL
   Callaghan B., 1995, RFC 1813
   Calvelli Claudio, 2019, PERLFS
   Cao Z, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P329
   Card R., 1994, PROCESSDINGS 1 DUTCH, P5
   Carlson Mark, 2015, SNIA ADV STORAGE INF
   Chen M, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P301
   Condict Michael, 1994, P 1 S OP SYST DES IM
   Corbet Jonathan, 2009, DEFENSE PERBDI WRITE
   Cornell B, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE FREENIX TRACK 2004 USENIX ANNUAL TECHNICAL CONFERENCE, P19
   DESNOYERS M., 2008, Using the Linux Kernel Tracepoints
   Fitzhardinge Jeremy, 2019, USERFS
   FUSE BDI Max Ratio, 2019, FUSE BDI MAX RAT DIS
   fuse-appendix, 2018, FUSE LIB OPT APIS
   Ghemawat S., 2003, ACM SIGOPS OPERATING, V37, P29, DOI [DOI 10.1145/945445.945450, DOI 10.1145/1165389.945450, 10.1145/1165389.945450]
   Hartig Hermann, 1997, P 16 S OP SYST PRINC
   Henson V., 2006, P 2 WORKSH HOT TOP S
   Ishiguro S, 2012, 2012 SC COMPANION: HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS (SCC), P760, DOI 10.1109/SC.Companion.2012.104
   Joukov N, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P89
   Linus Fuse, 2019, LIN TORV DOESNT UND
   macfuse, 2013, FUSE MACOS
   Machek Pavel, 2019, USERVFS
   Machek Pavel, 2019, PODFUK PORTABLE DODG
   Mcusick Marshall K, 1984, ACM T COMPUT SYST, V2, DOI DOI 10.1145/989.990
   Messmer Sebastian, 2015, CRYFS SECURE ENCRYPT
   Narayan Sumit., 2010, Proceedings of the 12th Annual Linux Symposium in Ottawa, P189
   NetBSD Foundation, 2013, AN RUMP KERN
   nimble_casl, 2019, NIMBL HYBR STOR ARCH
   Pease David, 2010, P 2010 IEEE 26 S MAS
   Rajgarhia Aditya, 2010, P 25 S APPL COMP
   Rath N., 2013, S3ql: File system that stores all its data online using storage services like amazon s3, google storage or open stack
   Steere D., 1990, P SUMM USENIX TECHN
   Sundararaman S, 2011, EUROSYS 11: PROCEEDINGS OF THE EUROSYS 2011 CONFERENCE, P77
   Sweeney A, 1996, PROCEEDINGS OF THE USENIX 1996 ANNUAL TECHNICAL CONFERENCE, P1
   Szeredi M., 2005, FILESYSTEM USERSPACE
   Tarasov Vasily, 2016, login Usenix Mag, V41, P1
   Tarasov Vasily, 2015, HOTSTORAGE 15
   Torvalds L., 2007, SPLICE KERNEL TRAP
   Torwalds Linux, 2019, RE PATCH 0 7 OVERLAY
   Ungureanu C., 2010, P FAST C
   Weil Sage, 2019, LINUS VS FUSE
   Westerlund Assar, 1998, P ANN USENIX TECHN C
   Zadok E, 2000, USENIX ASSOCIATION PROCEEDINGS OF THE 2000 USENIX ANNUAL TECHNICAL CONFERENCE, P55
NR 51
TC 7
Z9 7
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUN
PY 2019
VL 15
IS 2
AR 15
DI 10.1145/3310148
PG 49
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JL3IK
UT WOS:000495424300008
OA Bronze
DA 2024-07-18
ER

PT J
AU Trivedi, A
   Ioannou, N
   Metzler, B
   Stuedi, P
   Pfefferle, J
   Kourtis, K
   Koltsidas, I
   Gross, TR
AF Trivedi, Animesh
   Ioannou, Nikolas
   Metzler, Bernard
   Stuedi, Patrick
   Pfefferle, Jonas
   Kourtis, Kornilios
   Koltsidas, Ioannis
   Gross, Thomas R.
TI FlashNet: Flash/Network Stack Co-Design
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE RDMA; flash; network storage; performance; operating systems
AB During the past decade, network and storage devices have undergone rapid performance improvements, delivering ultra-low latency and several Gbps of bandwidth. Nevertheless, current network and storage stacks fail to deliver this hardware performance to the applications, often due to the loss of I/O efficiency from stalled CPU performance. While many efforts attempt to address this issue solely on either the network or the storage stack, achieving high-performance for networked-storage applications requires a holistic approach that considers both.
   In this article, we present FlashNet, a software I/O stack that unifies high-performance network properties with flash storage access and management. FlashNet builds on RDMA principles and abstractions to provide a direct, asynchronous, end-to-end data path between a client and remote flash storage. The key insight behind FlashNet is to co-design the stack's components (an RDMA controller, a flash controller, and a file system) to enable cross-stack optimizations and maximize I/O efficiency. In micro-benchmarks, FlashNet improves 4kB network I/O operations per second (IOPS by 38.6% to 1.22M, decreases access latency by 43.5% to 50.4 mu s, and prolongs the flash lifetime by 1.6-5.9x for writes. We illustrate the capabilities of FlashNet by building a Key-Value store and porting a distributed data store that uses RDMA on it. The use of FlashNet's RDMA API improves the performance of KV store by 2x and requires minimum changes for the ported data store to access remote flash devices.
C1 [Trivedi, Animesh; Ioannou, Nikolas; Metzler, Bernard; Stuedi, Patrick; Pfefferle, Jonas; Kourtis, Kornilios] IBM Res, Saeumerstr 4, CH-8803 Zurich, Switzerland.
   [Koltsidas, Ioannis] Google, Brandschenkestr 110, CH-8002 Zurich, Switzerland.
   [Gross, Thomas R.] Swiss Fed Inst Technol, Comp Sci Dept, CH-8092 Zurich, Switzerland.
C3 International Business Machines (IBM); Google Incorporated; Swiss
   Federal Institutes of Technology Domain; ETH Zurich
RP Trivedi, A (corresponding author), IBM Res, Saeumerstr 4, CH-8803 Zurich, Switzerland.
EM atr@zurich.ibm.com; nio@zurich.ibm.com; bmt@zurich.ibm.com;
   stu@zurich.ibm.com; jpf@zurich.ibm.com; kou@zurich.ibm.com;
   iko@zurich.ibm.com; thomas.gross@inf.ethz.ch
OI Trivedi, Animesh/0000-0003-3586-7168
CR Ahmad I., 2011, 2011 USENIX ANN TECH, P45
   [Anonymous], 2018, RDMA COMM MAN
   [Anonymous], 2014, USENIX Annual Technical Conference
   [Anonymous], IEEE Trans. Neural Netw. Learn. Syst.
   [Anonymous], 2009, Hadoop: The definitive guide
   Apache Crail (Incubating), 2018, HIGH PERF DISTR DAT
   Axboe Jens, 2018, FLEXIBLE I O TESTER
   Balakrishnan Mahesh, 2012, P 9 S NETW SYST DES
   Barroso L, 2017, COMMUN ACM, V60, P47, DOI 10.1145/3015146
   Bates Stephen, 2015, Donard: NVM Express for Peer-2-Peer between SSDs and other PCIe Devices
   Belay A., 2014, P OSDI, P49
   Bjorling Matias., 2013, Proceedings of the 6th International Systems and Storage Conference. SYSTOR'13, V22, P1, DOI [10.1145/2485732.2485740, DOI 10.1145/2485732.2485740]
   Blumrich M. A., 1994, Proceedings the 21st Annual International Symposium on Computer Architecture (Cat. No.94CH3397-7), P142, DOI 10.1109/ISCA.1994.288154
   Buzzard G, 1996, PROCEEDINGS OF THE SECOND SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '96), P245, DOI 10.1145/248155.238784
   Caulfield AM, 2012, ASPLOS XVII: SEVENTEENTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS, P387
   Caulfield AdrianM., 2013, Proceedings of the 40th Annual International Symposium on Computer Architecture, ISCA'13, P464
   Chadalapaka M., 2003, P ACM SIGCOMM WORKSH, P209, DOI DOI 10.1145/944747.944754
   Chai L., 2007, P 2 INT WORKSH PET D, P5, DOI DOI 10.1145/1374596.1374599
   Chang L.-P., 2004, ACM Trans. on Embedded Computing Syst, V3, P837
   Cully B., 2014, P 12 USENIX C FILE S, P17
   DeBergalis M, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE 2ND USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST'03), P175
   Douglas Chet, 2015, RDMAWITH PMEM SOFTWA
   Dragojevic A, 2015, SOSP'15: PROCEEDINGS OF THE TWENTY-FIFTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P54, DOI 10.1145/2815400.2815425
   Dragojevic Aleksandar, 2014, NSDI 14, P401
   Engler D. R., 1995, Operating Systems Review, V29, P251, DOI 10.1145/224057.224076
   Fitch Blake G., 2013, BLUE GENE ACTIVE STO
   Frey PW, 2009, INT CON DISTR COMP S, P553, DOI 10.1109/ICDCS.2009.32
   Frey PhilipWerner, 2010, THESIS, DOI [10.3929/ethz-a-006133695, DOI 10.3929/ETHZ-A-006133695]
   Gibson G. A., 1997, Performance Evaluation Review, V25, P272, DOI 10.1145/258623.258696
   Gibson GA, 1998, ACM SIGPLAN NOTICES, V33, P92, DOI 10.1145/291006.291029
   Guz Z, 2017, SYSTOR'17: PROCEEDINGS OF THE 10TH ACM INTERNATIONAL SYSTEMS AND STORAGE CONFERENCE, DOI 10.1145/3078468.3078483
   Hady F., 2012, C FIL STOR TECH FAST, P25
   Hat Red, 2018, GLUSTERFS
   Herlihy M, 2008, LECT NOTES COMPUT SC, V5218, P350, DOI 10.1007/978-3-540-87779-0_24
   Hildebrand D, 2005, Twenty-Second IEEE/Thirteenth NASA Goddard Conference on Mass Storage Systems and Technologies, Proceedings, P18, DOI 10.1109/MSST.2005.14
   Hoefler Torsten, 2015, P 15 USENIX C HOT TO
   Intel, 2018, INT 3D XPOINT TECHN
   Intel, 2018, DPDK DAT PLAN DEV KI
   Intel, 2018, INT OPT SSD 900P SER
   Ioannou N, 2018, I S MOD ANAL SIM COM, P277, DOI 10.1109/MASCOTS.2018.00035
   Jeong Eun Young, 2014, 11 USENIX S NETW SYS, P489
   Joglekar A, 2005, USENIX ASSOCIATION PROCEEDINGS OF THE 4TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P267
   Johnson Scott M., 2014, VIOLIN MICROSOFTS HI
   Jones Rick, 2018, NETPERF NETWORK PERF
   Josephson W.K., 2010, PROC USENIX C FILE S, V6, P85
   Kaashoek M. F., 1997, Operating Systems Review, V31, P52, DOI 10.1145/269005.266644
   Kalia A, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P185
   Kalia A, 2014, SIGCOMM'14: PROCEEDINGS OF THE 2014 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P295, DOI [10.1145/2619239.2626299, 10.1145/2740070.2626299]
   Kim Hyeong-Jun, 2016, 8 USENIX WORKSH HOT, P41
   Klimovic A, 2017, ACM SIGPLAN NOTICES, V52, P345, DOI 10.1145/3093336.3037732
   Klimovic A, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901337
   KNOWLTON KC, 1965, COMMUN ACM, V8, P623, DOI 10.1145/365628.365655
   Koukis E, 2010, CLUSTER COMPUT, V13, P349, DOI 10.1007/s10586-009-0106-y
   Lee Changman, 2015, 13 USENIX C FILE STO, P273
   Lee EK, 1996, ACM SIGPLAN NOTICES, V31, P84, DOI 10.1145/248209.237157
   Lee SJ, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P339
   Lesokhin I, 2017, ACM SIGPLAN NOTICES, V52, P449, DOI 10.1145/3093336.3037710
   Li B, 2009, NAS: 2009 IEEE INTERNATIONAL CONFERENCE ON NETWORKING, ARCHITECTURE, AND STORAGE, P303, DOI 10.1109/NAS.2009.58
   Lim Hyeontaek, 2014, 11 USENIX S NETW SYS, P429
   Lu XY, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA), P253, DOI 10.1109/BigData.2016.7840611
   Magoutis K, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE 2ND USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST'03), P189
   Magoutis K, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE GENERAL TRACK, P1
   Marinos I, 2014, ACM SIGCOMM COMP COM, V44, P175, DOI 10.1145/2740070.2626311
   Mellanox Technologies, 2018, SOFTW RDMA CONV ETH
   Mellanox Technologies, 2018, RDMA AW NETW PROGR U
   Metzler Bernard, 2018, SOFTIWARP SOFTWARE I
   Mickens James., 2014, Proceedings of the 11th USENIX Conference on Networked Systems Design and Implementation. NSDI'14, P257
   Mitchell Christopher, 2013, P USENIX ANN TECHN C, P103
   Nanavati M, 2016, COMMUN ACM, V59, P56, DOI 10.1145/2814342
   Noureddine Wael, 2015, IMPLEMENTING NVME FA
   NVM Express Inc, 2016, NVM EXPR FABR SPEC 1
   Ousterhout J, 2015, ACM T COMPUT SYST, V33, DOI 10.1145/2806887
   Pai VS, 1999, USENIX ASSOCIATION PROCEEDINGS OF THE THIRD SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '99), P15
   Pesterev A., 2012, P 7 ACM EUROPEAN C C, P337, DOI DOI 10.1145/2168836.2168870
   Peter Simon, 2014, P 11 USENIX C OP SYS
   Pfefferle J, 2015, ACM SIGPLAN NOTICES, V50, P17, DOI [10.1145/2817817.2731200, 10.1145/2731186.2731200]
   Pletka Roman, 2018, MANAGEMENT NEXT GENE
   Rizzo Luigi., 2012, 2012 USENIX Annual Technical Conference (USENIX ATC 12), P101
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   Schurmann Felix, 2014, Supercomputing. 29th International Conference, ISC 2014. Proceedings: LNCS 8488, P331, DOI 10.1007/978-3-319-07518-1_21
   Seshadri Sudharsan, 2014, 11 USENIX S OP SYST
   Shan YZ, 2017, PROCEEDINGS OF THE 2017 SYMPOSIUM ON CLOUD COMPUTING (SOCC '17), P323, DOI 10.1145/3127479.3128610
   Shin D. I., 2013, P 5 USENIX C HOT TOP
   Srinivasan V, 2016, PROC VLDB ENDOW, V9, P1389, DOI 10.14778/3007263.3007276
   Stuedi Patrick., 2012, 2012 USENIX Annual Technical Conference, Boston, MA, USA, June 13-15, 2012
   Stuedi Patrick, 2017, IEEE Data Eng. Bull, V40, P38
   Talagala N., 2012, NATIVE FLASH SUPPORT
   Talpey Tom, 2015, REMOTE ACCESS ULTRA
   Trivedi A., 2011, P 2 AS PAC WORKSH SY
   Trivedi A, 2015, INT CON DISTR COMP S, P674, DOI 10.1109/ICDCS.2015.74
   Trivedi Animesh, 2013, P 14 USENIX C HOT TO
   Tsai SY, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P306, DOI 10.1145/3132747.3132762
   von Eicken T., 1995, Operating Systems Review, V29, P40, DOI 10.1145/224057.224061
   Wei XD, 2015, SOSP'15: PROCEEDINGS OF THE TWENTY-FIFTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P87, DOI 10.1145/2815400.2815419
   Weiss Z., 2015, 13 USENIX C FIL STOR, P111
   Welch B, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P17
   Wilkes J., 1992, HPLOSR9213
   Xiao-Yu Hu, 2011, Proceedings of the 2011 IEEE 19th International Symposium on Modelling, Analysis & Simulation of Computer and Telecommunication Systems (MASCOTS 2011), P237, DOI 10.1109/MASCOTS.2011.50
   Xinidis D, 2005, TWENTY-SECOND IEEE/THIRTEENTH NASA GODDARD CONFERENCE ON MASS STORAGE SYSTEMS AND TECHNOLOGIES, PROCEEDINGS, P261, DOI 10.1109/MSST.2005.23
   Xu Qiumin, 2015, P 8 ACM INT SYST STO, DOI DOI 10.1145/2757667.2757684
   Zhang YC, 2015, PROCEEDINGS OF 2015 INTERNATIONAL CONFERENCE ON ORANGE TECHNOLOGIES (ICOT), P3
   Zhang YW, 2017, KBNETS '17: PROCEEDINGS OF THE 2017 WORKSHOP ON KERNEL-BYPASS NETWORKS, P43, DOI 10.1145/3098583.3098591
NR 102
TC 5
Z9 5
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD DEC
PY 2018
VL 14
IS 4
AR 30
DI 10.1145/3239562
PG 29
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ4JO
UT WOS:000457140500002
DA 2024-07-18
ER

PT J
AU Yoo, J
   Oh, J
   Lee, S
   Won, Y
   Ha, JY
   Lee, J
   Shim, J
AF Yoo, Jinsoo
   Oh, Joontaek
   Lee, Seongjin
   Won, Youjip
   Ha, Jin-Yong
   Lee, Jongsung
   Shim, Junseok
TI OrcFS: Orchestrated File System for Flash Storage
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Log-structured File System; Flash memories; Garbage Collection
ID TRANSLATION
AB In this work, we develop the Orchestrated File System (OrcFS) for Flash storage. OrcFS vertically integrates the log-structured file system and the Flash-based storage device to eliminate the redundancies across the layers. A few modern file systems adopt sophisticated append-only data structures in an effort to optimize the behavior of the file system with respect to the append-only nature of the Flash memory. While the benefit of adopting an append-only data structure seems fairly promising, it makes the stack of software layers full of unnecessary redundancies, leaving substantial room for improvement. The redundancies include (i) redundant levels of indirection (address translation), (ii) duplicate efforts to reclaim the invalid blocks (i.e., segment cleaning in the file system and garbage collection in the storage device), and (iii) excessive over-provisioning (i.e., separate over-provisioning areas in each layer). OrcFS eliminates these redundancies via distributing the address translation, segment cleaning (or garbage collection), bad block management, and wear-leveling across the layers. Existing solutions suffer from high segment cleaning overhead and cause significant write amplification due to mismatch between the file system block size and the Flash page size. To optimize the I/O stack while avoiding these problems, OrcFS adopts three key technical elements.
   First, OrcFS uses disaggregate mapping, whereby it partitions the Flash storage into two areas, managed by a file system and Flash storage, respectively, with different granularity. In OrcFS, the metadata area and data area are maintained by 4Kbyte page granularity and 256Mbyte superblock granularity. The superblock-based storage management aligns the file system section size, which is a unit of segment cleaning, with the superblock size of the underlying Flash storage. It can fully exploit the internal parallelism of the underlying Flash storage, exploiting the sequential workload characteristics of the log-structured file system. Second, OrcFS adopts quasi-preemptive segment cleaning to prohibit the foreground I/O operation from being interfered with by segment cleaning. The latency to reclaim the free space can be prohibitive in OrcFS due to its large file system section size, 256Mbyte. OrcFS effectively addresses this issue via adopting a polling-based segment cleaning scheme. Third, the OrcFS introduces block patching to avoid unnecessary write amplification in the partial page program. OrcFS is the enhancement of the F2FS file system. We develop a prototype OrcFS based on F2FS and server class SSD with modified firmware (Samsung 843TN). OrcFS reduces the device mapping table requirement to 1/465 and 1/4 compared with the page mapping and the smallest mapping scheme known to the public, respectively. Via eliminating the redundancy in the segment cleaning andgarbage collection, the OrcFS reduces 1/3 of the write volume under heavy random write workload. OrcFS achieves 56% performance gain against EXT4 in varmail workload.
C1 [Yoo, Jinsoo; Oh, Joontaek; Won, Youjip] Hanyang Univ, Seoul, South Korea.
   [Lee, Seongjin] Gyeongsang Natl Univ, Jinju, South Korea.
   [Ha, Jin-Yong; Lee, Jongsung; Shim, Junseok] Samsung Elect, Suwon, South Korea.
C3 Hanyang University; Gyeongsang National University; Samsung; Samsung
   Electronics
RP Won, Y (corresponding author), Hanyang Univ, Seoul, South Korea.; Lee, S (corresponding author), Gyeongsang Natl Univ, Jinju, South Korea.
EM jedisty@hanyang.ac.kr; na94jun@hanyang.ac.kr; insight@gnu.ac.kr;
   yjwon@hanyang.ac.kr; jy200.ha@samsung.com; js0007.lee@samsung.com;
   junseok.shim@samsung.com
RI Lee, Jae-Won/AAY-1647-2020; Lee, Seongjin/AAI-2365-2020; Lee, Ji
   Hyung/HNI-8824-2023
OI Lee, Seongjin/0000-0003-0760-1880; Lee, Jongsung/0000-0003-4080-0611
FU Basic Research Lab Program through the NRF - Ministry of Science
   ICTFuture Planning [2017R1A4A1015498]; BK21 plus program through the NRF
   - Ministry of Education of Korea; ICT R&D program of MSIP/IITP
   [R7117-16-0232]; Ministry of Science ICT& Future Planning under the ITRC
   [IITP-2016-H8501-16-1006]; National Research Foundation of Korea
   [22A20130012575] Funding Source: Korea Institute of Science & Technology
   Information (KISTI), National Science & Technology Information Service
   (NTIS)
FX This research was supported by Basic Research Lab Program through the
   NRF funded by the Ministry of Science ICT&Future Planning (No.
   2017R1A4A1015498), the BK21 plus program through the NRF funded by the
   Ministry of Education of Korea, the ICT R&D program of MSIP/IITP
   (R7117-16-0232, Development of extreme I/O storage technology for 32Gbps
   data services), and the Ministry of Science ICT& Future Planning under
   the ITRC support program (IITP-2016-H8501-16-1006) supervised by the
   IITP.
CR Agrawal Nitin, 2008, P USENIX ANN TECHN C, P57
   [Anonymous], P 12 INT LIN SYST TE
   [Anonymous], 2017, SENSORS-BASEL, DOI DOI 10.HTTPS://D0I.0RG/10.1007/S00500-017-2619-2
   Axboe J, 2005, Fio-flexible i/o tester synthetic bench- mark
   Banker K., 2011, MONGODB IN ACTION
   Berry Frank, 2015, P FLASH MEM SUMM SAN
   Campos D, 2015, PROCEEDINGS OF THE 17TH BRAZILIAN SYMPOSIUM ON SOFTWARE QUALITY (SBQS), P151, DOI 10.1145/3275245.3275261
   Chang F, 2008, ACM T COMPUT SYST, V26, DOI 10.1145/1365815.1365816
   Chang YH, 2007, DES AUT CON, P212, DOI 10.1109/DAC.2007.375155
   Chow David, 2007, US Patent No, Patent No. [11/864,684, 864684]
   Czezatke C, 2000, USENIX ASSOCIATION PROCEEDINGS OF THE FREENIX TRACK, P77
   Da Zheng, 2015, OPTIMIZE UNSYNCHRONI, P1
   f2fs-tools, 2012, FORMATTING TOOLS FLA
   Ghemawat S., 2014, LevelDB, A fast and lightweight key/value database library by Google
   Gupta A, 2009, ACM SIGPLAN NOTICES, V44, P229, DOI 10.1145/1508284.1508271
   Hunter Adrian, 2008, P RAPP TECHN
   Josephson WK, 2010, ACM T STORAGE, V6, DOI 10.1145/1837915.1837922
   Kang J., 2006, Proceedings of the International Conference on Embedded Software (EMSOFT), P161
   Kang Jeong-Uk, 2014, USENIX WORKSH HOT TO, P13
   KAWAGUCHI A, 1995, PROCEEDINGS OF THE 1995 USENIX TECHNICAL CONFERENCE, P155
   Kim J., 2010, The Fifth International Workshop on Software Support for Portable Storage, P7
   Kim JW, 2002, IEEE T CONSUM ELECTR, V48, P275, DOI 10.1109/TCE.2002.1010132
   Kingston Technology, 2013, UND OV PROV
   Konishi R., 2006, Operating Systems Review, V40, P102, DOI 10.1145/1151374.1151375
   Kwon Hunki, 2010, P 10 ACM INT C EMB S, P169, DOI DOI 10.1145/1879021.1879044
   Kwon O, 2007, LECT NOTES COMPUT SC, V4490, P913
   Lakshman Avinash., 2009, P 28 ACM S PRINCIPLE, P5
   Lee Changman, 2015, 13 USENIX C FILE STO, P273
   Lee J, 2011, INT SYM PERFORM ANAL, P12, DOI 10.1109/ISPASS.2011.5762711
   Lee SW, 2007, ACM T EMBED COMPUT S, V6, DOI 10.1145/1275986.1275990
   Lee SJ, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P339
   Lim H, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P1
   Manning C., 2010, HOW YAFFS WORKS
   Marmol L., 2015, P USENIX ANN TECH C, P207
   Mearian Lucas., 2016, Ssd prices plummet again, close in on hdds
   Micron, 2016, TECHN INN RED
   ONeil P, 1996, ACTA INFORM, V33, P351, DOI 10.1007/s002360050048
   Ouyang J, 2014, ACM SIGPLAN NOTICES, V49, P471, DOI 10.1145/2541940.2541959
   Park J, 2009, IEICE ELECTRON EXPR, V6, P297, DOI 10.1587/elex.6.297
   Rocks DB, 2014, PERS KEY VAL STOR FA
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   Samsung, 2015, NEXT GEN SAMS 3BIT V
   Samsung Electronics Co, 2014, APPL NOTE
   Saxena Mohit., 2010, P 2010 USENIX C USEN, P14
   Seltzer M., 1995, TCON'95: Proceedings of the USENIX 1995 Technical Confer- ence Proceedings on USENIX 1995 Technical Conference Proceedings, P21
   Shu F., 2007, Management, V2, P1
   smartmontools, 2010, SMARTMONTOOLS PACKAG
   Smith K., 2011, SandForce, Flash Memory Summit, P1
   SSD843Tn, 2014, SSD843TN
   StarWind, 2014, LOG STRUCT FIL SYST
   Sungjin Lee, 2008, Operating Systems Review, V42, P36, DOI 10.1145/1453775.1453783
   Tarasov Vasily., 2016, Filebench: A flexible framework for file system benchmarking
   Weiss Z., 2015, 13 USENIX C FIL STOR, P111
   Wu G., 2012, P 10 USENIX C FIL ST, V12, P10
   Yang Jingpei, 2014, P INT NVM FLASH OP S
   Yarnall T. M, 2013, P 2013 OFC NFOEC
   Yoo Jinsoo., 2013, Mass Storage Systems and Technologies (MSST), 2013 IEEE 29th Symposium on, P1
   Yudong Yang, 2015, ACM SIGMETRICS Performance Evaluation Review, V43, P63
   Zhang JC, 2016, PROCEEDINGS OF USENIX ATC '16: 2016 USENIX ANNUAL TECHNICAL CONFERENCE, P87
   Zhang Yiying, 2015, P C MASS STOR SYST T, P1650
   Zhang Yong, 2006, P COMP TECHN DEV, V4, P48
NR 61
TC 7
Z9 7
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAY
PY 2018
VL 14
IS 2
AR 17
DI 10.1145/3162614
PG 26
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GI6TM
UT WOS:000434635800006
DA 2024-07-18
ER

PT J
AU Iliadis, I
   Venkatesan, V
AF Iliadis, Ilias
   Venkatesan, Vinodh
TI Rebuttal to "Beyond MTTDL: A Closed-Form RAID-6 Reliability Equation"
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Performance; Reliability; MTTDL; unrecoverable or latent sector errors;
   RAID; reliability analysis; stochastic modeling
AB A recent article on the reliability of RAID-6 storage systems overlooks certain relevant prior work published in the past 20 years and concludes that the widely used mean time to data loss (MTTDL) metric does not provide accurate results. In this note, we refute this position by invoking uncited relevant prior work and demonstrating that the MTTDL remains a useful metric.
C1 [Iliadis, Ilias; Venkatesan, Vinodh] IBM Res Zurich, CH-8803 Ruschlikon, Switzerland.
C3 International Business Machines (IBM)
RP Iliadis, I (corresponding author), IBM Res Zurich, Saumerstr 4, CH-8803 Ruschlikon, Switzerland.
EM ili@zurich.ibm.com; ven@zurich.ibm.com
RI Venkatesan, Vinodh/AAX-2134-2021
OI Iliadis, Ilias/0000-0002-3860-5828
CR [Anonymous], 1988, A case for redundant arrays of inexpensive disks RAID, DOI DOI 10.1145/50202.50214
   CHEN PM, 1994, ACM COMPUT SURV, V26, P145, DOI 10.1145/176979.176981
   Dholakia Ajay, 2008, ACM Transaction on Storage, V4, DOI 10.1145/1353452.1353453
   Dholakia A., 2006, ACM SIGMETRICS PERFO, V34, P373
   DHOLAKIA A, 2006, 3652 IBM RZ
   Elerath JG, 2014, ACM T STORAGE, V10, DOI 10.1145/2577386
   Greenan K. M., 2010, P HOTSTORAGE, P1
   Iliadis I, 2011, ACM T STORAGE, V7, DOI 10.1145/1970348.1970350
   Thomasian Alexander, 2009, ACM Transaction on Storage, V5, DOI 10.1145/1629075.1629076
   Venkatesan Vinodh, 2013, Quantitative Evaluation of Systems. 10th International Conference, QEST 2013. Proceedings: LNCS 8054, P241, DOI 10.1007/978-3-642-40196-1_20
   Venkatesan V., 2012, 2012 Ninth International Conference on Quantitative Evaluation of Systems (QEST 2012), P209, DOI 10.1109/QEST.2012.32
   Venkatesan V., 2012, 2012 IEEE 20th International Symposium on Modelling, Analysis & Simulation of Computer and Telecommunication Systems (MASCOTS), P189, DOI 10.1109/MASCOTS.2012.31
   Venkatesan V., 2011, Proceedings of the 2011 IEEE 19th International Symposium on Modelling, Analysis & Simulation of Computer and Telecommunication Systems (MASCOTS 2011), P307, DOI 10.1109/MASCOTS.2011.53
   Venkatesan V., 2012, THESIS EPFL LAUSANNE
   Venkatesan V., 2012, 3827 IBM RZ
   Venkatesan V, 2013, I S MOD ANAL SIM COM, P293, DOI 10.1109/MASCOTS.2013.38
NR 16
TC 9
Z9 10
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAR
PY 2015
VL 11
IS 2
AR 9
DI 10.1145/2700311
PG 10
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CF1KG
UT WOS:000352303400005
DA 2024-07-18
ER

PT J
AU Sun, ZW
   Skjellum, A
   Ward, L
   Curry, ML
AF Sun, Zhiwei
   Skjellum, Anthony
   Ward, Lee
   Curry, Matthew L.
TI A Lightweight Data Location Service for Nondeterministic Exascale
   Storage Systems
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Algorithms; Performance; Efficient search; Exascale;
   nondeterministic; scalability; storage
AB In this article, we present LWDLS, a lightweight data location service designed for Exascale storage systems (storage systems with order of 10(18) bytes) and geo-distributed storage systems (large storage systems with physically distributed locations). LWDLS provides a search-based data location solution, and enables free data placement, movement, and replication. In LWDLS, probe and prune protocols are introduced that reduce topology mismatch, and a heuristic flooding search algorithm (HFS) is presented that achieves higher search efficiency than pure flooding search while having comparable search speed and coverage to the pure flooding search. LWDLS is lightweight and scalable in terms of incorporating low overhead, high search efficiency, no global state, and avoiding periodic messages. LWDLS is fully distributed and can be used in nondeterministic storage systems and in deterministic storage systems to deal with cases where search is needed. Extensive simulations modeling large-scale High Performance Computing (HPC) storage environments provide representative performance outcomes. Performance is evaluated by metrics including search scope, search efficiency, and average neighbor distance. Results show that LWDLS is able to locate data efficiently with low cost of state maintenance in arbitrary network environments. Through these simulations, we demonstrate the effectiveness of protocols and search algorithm of LWDLS.
C1 [Sun, Zhiwei; Skjellum, Anthony] Univ Alabama Birmingham, Dept Comp & Informat Sci, Birmingham, AL 35233 USA.
   [Ward, Lee; Curry, Matthew L.] Sandia Natl Labs, Livermore, CA 94550 USA.
C3 University of Alabama System; University of Alabama Birmingham; United
   States Department of Energy (DOE); Sandia National Laboratories
RP Sun, ZW (corresponding author), Univ Alabama Birmingham, Dept Comp & Informat Sci, Birmingham, AL 35233 USA.
EM zhwsun@uab.edu
FU United States Department of Energy [DE-AC04-94AL85000]; National Science
   Foundation [OCI-1064247, CCF-1239962]; U.S. Department of Energy's
   National Nuclear Security Administration [AC04-94AL85000]
FX This work was supported in part by the United States Department of
   Energy under Contract DE-AC04-94AL85000 and by the National Science
   Foundation under grant OCI-1064247 and grant CCF-1239962. Sandia
   National Laboratories is a multiprogram laboratory managed and operated
   by Sandia Corporation, a wholly owned subsidiary of Lockheed Martin
   Corporation, for the U.S. Department of Energy's National Nuclear
   Security Administration under contract DE-AC04-94AL85000.
CR Antony I., 2001, LECT NOTES COMPUTER, P329, DOI DOI 10.1007/3-540-45518-3_18
   Bent John., 2009, High Performance Computing Networking, Storage and Analysis, Proceedings of the Conference on, P1
   Brandstatter Kevin, 2013, P IEEE INT PAR DISTR
   Buford John, 2013, MICROSOFT POWERPOINT
   Carns Philip H., 2000, P 4 ANN LIN SHOWC C, P1
   Crisóstomo S, 2012, COMPUT NETW, V56, P142, DOI 10.1016/j.comnet.2011.08.014
   Curry Matthew L., 2012, SAND20121087
   DeCandia Giuseppe, 2007, Operating Systems Review, V41, P205, DOI 10.1145/1323293.1294281
   Denzel Wolfgang E., 2008, P 1 INT C SIM TOOLS
   Dongarra Jack, 2010, P DEP EN WORKSH CROS
   Gaeta R, 2011, IEEE T PARALL DISTR, V22, P2055, DOI 10.1109/TPDS.2011.82
   Ghemawat Sanjay, 2003, P 19 ACM S OP SYST P, P96
   Gkantsidis C, 2005, IEEE INFOCOM SER, P1526
   Gupta A., 2003, P 9 C HOT TOPICS OPE, V9, P2
   Hornig R., 2008, P 1 INT ICST C SIM T, P60, DOI [10.4108/ICST.SIMUTOOLS2008.3027, DOI 10.4108/ICST.SIMUTOOLS2008.3027]
   Jiang S, 2008, IEEE T PARALL DISTR, V19, P601, DOI 10.1109/TPDS.2007.70772
   Lakshman Avinash, 2010, Operating Systems Review, V44, P35, DOI 10.1145/1773912.1773922
   Lanham N, 2003, ACM SIGCOMM COMP COM, V33, P407
   Lin T, 2009, IEEE T PARALL DISTR, V20, P654, DOI 10.1109/TPDS.2008.134
   Liu YH, 2005, IEEE T PARALL DISTR, V16, P163, DOI 10.1109/TPDS.2005.21
   Liu YH, 2008, IEEE T PARALL DISTR, V19, P1591, DOI 10.1109/TPDS.2008.24
   Loo BT, 2004, LECT NOTES COMPUT SC, V3279, P141
   Lv Q., 2002, Performance Evaluation Review, V30, P258, DOI 10.1145/511399.511369
   Maymounkov P, 2002, LECT NOTES COMPUT SC, V2429, P53
   Newman MEJ, 2001, PHYS REV E, V64, DOI [10.1103/PhysRevE.64.016132, 10.1103/PhysRevE.64.016131]
   Nowoczynski Paul., 2008, Petascale Data StorageWorkshop, V3rd, P1
   Oikonomou K, 2010, COMPUT NETW, V54, P1615, DOI 10.1016/j.comnet.2010.01.007
   Pearson K, 1905, NATURE, V72, P294, DOI 10.1038/072294b0
   Ratnasamy S, 2001, ACM SIGCOMM COMP COM, V31, P161, DOI 10.1145/964723.383072
   Ripeanu M, 2002, IEEE INTERNET COMPUT, V6, P50, DOI 10.1109/4236.978369
   Schmuck Frank, 2002, C FILE STORAGE TECHN, P1
   Schwan Philip, 2003, P LINUX S, V9
   Shen HY, 2006, PERFORM EVALUATION, V63, P195, DOI 10.1016/j.peva.2005.01.004
   Stauffer Alexandre O., 2004, CSNI0409001 CORR
   Stoica I, 2001, ACM SIGCOMM COMP COM, V31, P149, DOI 10.1145/964723.383071
   Tang Hong, 2003, P INT C HIGH PERF CO
   Tolley Bruce., 2011, Solarflare fujitsu low latency test report - Solarflare Low-Latency TestReport.pdf
   Weil SA, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P307
   Weil Sage A., 2006, P ACM IEEE C SUP SC0
   Yang M, 2010, IEEE T COMPUT, V59, P1158, DOI 10.1109/TC.2009.175
   Yang Tao, 2004, P HIGH PERF COMP NET
   Zhao B. Y., 2001, UCBCSD011141 EECS DE
NR 42
TC 0
Z9 1
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUL
PY 2014
VL 10
IS 3
AR 12
DI 10.1145/2629451
PG 22
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AN3TU
UT WOS:000340512400004
OA hybrid
DA 2024-07-18
ER

PT J
AU Plank, JS
   Blaum, M
AF Plank, James S.
   Blaum, Mario
TI Sector-Disk (SD) Erasure Codes for Mixed Failure Modes in RAID Systems
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Algorithms; Design; Reliability; Theory; Erasure codes; RAID; storage
   systems; sector failures; fault tolerance
AB Traditionally, when storage systems employ erasure codes, they are designed to tolerate the failures of entire disks. However, the most common types of failures are latent sector failures, which only affect individual disk sectors, and block failures which arise through wear on SSD's. This article introduces SD codes, which are designed to tolerate combinations of disk and sector failures. As such, they consume far less storage resources than traditional erasure codes. We specify the codes with enough detail for the storage practitioner to employ them, discuss their practical properties, and detail an open-source implementation.
C1 [Plank, James S.] Univ Tennessee, Dept EECS, Knoxville, TN 37991 USA.
   [Blaum, Mario] IBM Almaden Res Ctr, IBM Res Div, San Jose, CA 95120 USA.
C3 University of Tennessee System; University of Tennessee Knoxville;
   International Business Machines (IBM)
RP Plank, JS (corresponding author), Univ Tennessee, Dept EECS, Knoxville, TN 37991 USA.
EM plank@cs.utk.edu
OI Plank, James/0000-0002-9841-6076
FU National Science Foundation [CSR-1016636, CSR-1128847]; IBM Faculty
   Award; Division Of Computer and Network Systems; Direct For Computer &
   Info Scie & Enginr [1016636] Funding Source: National Science Foundation
FX This work was supported by the National Science Foundation, under grants
   CSR-1016636 and REU Supplement CSR-1128847, and by an IBM Faculty Award.
CR Amvrosiadis G., 2012, P INT C DEP SYST NET
   [Anonymous], 2011, DEVELOPMENT
   [Anonymous], 2010, P 8 USENIX S FIL STO
   [Anonymous], 2008, P 6 USENIX C FIL STO
   [Anonymous], 2012, P USENIX ANN TECHN C
   [Anonymous], 1977, THEORY ERROR CORRE 1
   Balakrishnan M, 2010, ACM T STORAGE, V6, DOI 10.1145/1807060.1807061
   BLAUM M, 1995, IEEE T COMPUT, V44, P192, DOI 10.1109/12.364531
   Blaum M, 1999, IEEE T INFORM THEORY, V45, P46, DOI 10.1109/18.746771
   Blaum M., 2012, RJ10498 IBM
   Blaum M., 2013, ARXIV13051221CSIT
   Blomer J., 1995, Tech. Rep. TR-95-048
   Bowers K. D., 2009, P 16 ACM C COMP COMM
   Cadambe V., 2011, P AS C SIGN SYST COM
   Calder B, 2011, P 23 ACM S OP SYST P
   Chen B., 2010, P CLOUD COMP SEC WOR
   Corbett P., 2004, P 3 USENIX C FIL STO, P1
   Dholakia Ajay, 2008, ACM Transaction on Storage, V4, DOI 10.1145/1353452.1353453
   Dimakis AG, 2011, P IEEE, V99, P476, DOI 10.1109/JPROC.2010.2096170
   Edwards JohnK., 2008, ATC 08, P129
   Elerath JG, 2009, IEEE T COMPUT, V58, P289, DOI 10.1109/TC.2008.163
   Ghemawat Sanjay., 2003, SOSP'03
   Gopalan P., 2012, IEEE T INFORM THEORY, V58, P11
   Greenan K., 2008, P 16 IEEE S MOD AN S
   Greenan K. M., 2010, P 26 IEEE S MASS STO
   Greenan K. M., 2009, P 5 WORKSH HOT TOP D
   Hafner JL, 2008, IBM J RES DEV, V52, P413, DOI 10.1147/rd.524.0413
   Hafner J. L., 2006, P IEEE INT C DEP SYS
   Hu Y., 2012, FAST
   HUANG C, 2007, P 6 IEEE INT S NETW
   Josephson W., 2010, P 8 USENIX C FIL STO
   Kenchammana-Hosekote D, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 5TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES ( FAST '07), P261
   Khan Osama, 2012, P 10 USENIX C FIL ST
   Klein H., 2009, P 23 IEEE INT S PAR
   Lastras-Montano L. A., 2011, P INF THEOR APPL WOR
   Li M., 2009, ACM T STORAGE, V4, P4
   Li X., 2010, P INT C DEP SYST NET
   LUBY M, 2002, P IEEE S FDN COMP SC
   Luo JQ, 2012, ACM T STORAGE, V8, DOI 10.1145/2093139.2093141
   Networks Onion, 2001, JAV FEC LIB V1 0 3
   Oh Yongseok., 2012, FAST, V12
   Oprea Alina, 2010, FAST, P57
   Ousterhout J., 1989, Operating Systems Review, V23, P11, DOI 10.1145/65762.65765
   Partow A., 2000, SCHIFRA REED SOLOMON
   Peterson W. W., 1972, ERROR CORRECTING COD
   Plank J. S., 2013, P 11 USENIX C FIL ST, P95
   Plank J. S., 2009, FAST 2009, P253
   Plank JS, 1997, SOFTWARE PRACT EXPER, V27, P995, DOI 10.1002/(SICI)1097-024X(199709)27:9<995::AID-SPE111>3.0.CO;2-6
   Rhea S., 2003, P 2 USENIX C FIL STO
   Schroeder B., 2007, P 5 USENIX C FIL STO
   Seltzer M., 1993, USENIX Association. Proceedings of the Winter 1993 USENIX Conference, P307
   Storer M. W., 2009, ACM T STORAGE, V5, P2
   Storer MW, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P1, DOI 10.1145/1456469.1456471
   Suh C., 2010, P IEEE INT S INF THE
   Wang ZY, 2010, IEEE GLOBE WORK, P1905, DOI 10.1109/GLOCOMW.2010.5700274
   Warner B., 2008, TAHOE SECURE DISTRIB
   Welch B, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P17
   Xiang L., 2010, P ACM SIGMETRICS
   Xu L., 2005, P ACM INT WORKSH STO
NR 59
TC 51
Z9 60
U1 0
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JAN
PY 2014
VL 10
IS 1
AR 4
DI 10.1145/2560013
PG 17
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AB6EC
UT WOS:000331879400004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wang, Y
   Shu, JW
   Zhang, GY
   Xue, W
   Zheng, WM
AF Wang, Yang
   Shu, Jiwu
   Zhang, Guangyan
   Xue, Wei
   Zheng, Weimin
TI SOPA: Selecting the Optimal Caching Policy Adaptively
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Algorithms; Performance; Design; Caching policies; policy adaptation;
   policy switch
AB With the development of storage technology and applications, new caching policies are continuously being introduced. It becomes increasingly important for storage systems to be able to select the matched caching policy dynamically under varying workloads. This article proposes SOPA, a cache framework to adaptively select the matched policy and perform policy switches in storage systems. SOPA encapsulates the functions of a caching policy into a module, and enables online policy switching by policy reconstruction. SOPA then selects the policy matched with the workload dynamically by collecting and analyzing access traces. To reduce the decision-making cost, SOPA proposes an asynchronous decision making process. The simulation experiments show that no single caching policy performed well under all of the different workloads. With SOPA, a storage system could select the appropriate policy for different workloads. The real-system evaluation results show that SOPA reduced the average response time by up to 20.3% and 11.9% compared with LRU and ARC, respectively.
C1 [Wang, Yang] Tsinghua Univ, Dept Comp Sci, Beijing 100084, Peoples R China.
C3 Tsinghua University
RP Wang, Y (corresponding author), Tsinghua Univ, Dept Comp Sci, Beijing 100084, Peoples R China.
EM shujw@tsinghua.edu.cn
RI WU, ZHEN/GRN-7688-2022; zheng, wei/IQT-9639-2023
OI WU, ZHEN/0000-0001-8719-057X; 
FU National Natural Science Foundation of China [10576018]; National Grand
   Fundamental Research 973 Program of China [2004CB318205]; Program for
   New Century Excellent Talents in University [NCET-05-0067]; National
   High-Tech Research and Development Plan of China [2009AA01A403]
FX This research was supported by the National Natural Science Foundation
   of China under Grant No. 10576018, the National Grand Fundamental
   Research 973 Program of China under Grant No. 2004CB318205, the Program
   for New Century Excellent Talents in University (NCET-05-0067), and the
   National High-Tech Research and Development Plan of China (No.
   2009AA01A403).
CR Ari I., 2002, P WORKSH DISTR DAT S
   BANSAL S., 2004, P USENIX FIL STOR TE, P142
   Cao P., 1997, P USENIX S INT TECHN
   CHEN Z., 2005, P ACM SIGMETRICS C M
   Chen ZF, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE GENERAL TRACK, P269
   CHOU H. T., 1985, P VLDB C
   GILL B. S., 2005, P USENIX C FIL STOR
   GOTTWALS M., 2004, P INT C AUT COMP ICA
   JIANG S, 2005, P USENIX C FIL STOR
   Johnson T., 1994, P VLDB C, P297
   Lee D, 2001, IEEE T COMPUT, V50, P1352, DOI 10.1109/tc.2001.970573
   Li XH, 2005, USENIX ASSOCIATION PROCEEDINGS OF THE 4TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P115
   McVoy L, 1996, PROCEEDINGS OF THE USENIX 1996 ANNUAL TECHNICAL CONFERENCE, P279
   Megiddo N, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE 2ND USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST'03), P115
   MENON J, 1994, DISTRIB PARALLEL DAT, V2, P261, DOI 10.1007/BF01266331
   MENON J., 1988, P IEEE COMP SOC INT
   NELSON MN, 1988, ACM T COMPUT SYST, V6, P134, DOI 10.1145/35037.42183
   O'Neil E. J., 1993, SIGMOD Record, V22, P297, DOI 10.1145/170036.170081
   ROBINSON JT, 1990, PERF E R SI, V18, P134, DOI 10.1145/98460.98523
   RUEMMLER C, 1993, HPLOSR9323
   SALMON B., 2003, P WORKSH ALG ARCH SE
   Song Jiang, 2002, Performance Evaluation Review, V30, P31, DOI 10.1145/511399.511340
   YADGAR G., 2007, P USENIX FIL STOR TE
   Zhou YY, 2001, USENIX ASSOCIATION PROCEEDINGS OF THE 2001 USENIX ANNUAL TECHNICAL CONFERENCE, P91
NR 24
TC 13
Z9 13
U1 0
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUL
PY 2010
VL 6
IS 2
AR 7
DI 10.1145/1807060.1807064
PG 18
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA V24QG
UT WOS:000208424300004
DA 2024-07-18
ER

PT J
AU May, MJ
AF May, Michael J.
TI Donag: Generating Efficient Patches and Diffs for Compressed Archives
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article; Proceedings Paper
CT ACM Symposium on Operating Systems Principles (SOSP)
CY OCT 26-29, 2021
CL ELECTR NETWORK
SP Assoc Comp Machinery
DE Differencing; delta files; compression; canonical forms; ZIP archives
AB Differencing between compressed archives is a common task in file management and synchronization. Applications include source code distribution, application updates, and document synchronization. General purpose binary differencing tools can create and apply patches to compressed archives, but don't consider the internal structure of the compressed archive or the file lifecycle. Therefore, they miss opportunities to save space based on the archive's internal structure and metadata. To address the gap, we develop a content-aware, format independent theory for differencing on compressed archives and propose a canonical form and digest for compressed archives. Based on them, we present Donag, a content-aware differencing and patching algorithm that produces smaller patches than general purpose binary differencing tools on versioned archives by exploiting the compressed archives' internal structure. Donag uses the VCDiff and BSDiff engines internally. We compare Donag's patches to ones produced by bsdiff, xdelta3, and Delta++ on three classes of compressed archives: open-source code repositories, large and small applications, and office productivity documents (DOCX, XLSX, PPTX). Donag's patches are typically 10% to 89% smaller than those produced by bsdiff, xdelta3, and Delta++, with reasonable memory overhead and throughput on commodity hardware. In the worst case, Donag's patches are negligibly larger.
C1 [May, Michael J.] Kinneret Coll Sea Galilee, Asparna Res Ctr, Tzemach Junct Rd 92, IL-15132 Mp Jordan Valley, Israel.
RP May, MJ (corresponding author), Kinneret Coll Sea Galilee, Asparna Res Ctr, Tzemach Junct Rd 92, IL-15132 Mp Jordan Valley, Israel.
EM michael@asparna.com
RI May, Michael J./AAN-8538-2021
OI May, Michael J./0000-0003-4571-7972
CR Adams Stephen, 2009, SOFTWARE UPDATES COU
   Barabucci G, 2016, COMPUT STAND INTER, V46, P52, DOI 10.1016/j.csi.2015.12.005
   Barabucci Gioele, 2013, DISSERTATION
   Bittinger Reed, 2000, US Patent, Patent No. [US6148340A, 6148340]
   Boyer John, 2008, CANONICAL XML VERSIO
   Broder AZ, 1998, COMPRESSION AND COMPLEXITY OF SEQUENCES 1997 - PROCEEDINGS, P21, DOI 10.1109/SEQUEN.1997.666900
   Carlson W. E., 1991, Computer Graphics, V25, P67, DOI 10.1145/126724.126726
   David James Clarke IV, 2004, CNE NETWARE 6 STUDY, V1st
   Davison Wayne, 2020, RSYNC
   Denoue L, 2018, PROCEEDINGS OF THE ACM SYMPOSIUM ON DOCUMENT ENGINEERING (DOCENG 2018), DOI 10.1145/3209280.3229107
   Deutsch P., 1996, RFC1951 IETF
   Ehrmann David, 2019, VCDIFF JAVA
   Evans Garrick D., 2011, US Patent, Patent No. [US8024382B2, 8024382]
   Ferragina P., 2006, WWW '06, P751, DOI DOI 10.1145/1135777.1135891
   Google Code Labs, 2011, CRX PACK FORM
   HUFFMAN DA, 1952, P IRE, V40, P1098, DOI 10.1109/JRPROC.1952.273898
   Hunt J. J., 1998, ACM Transactions on Software Engineering and Methodology, V7, P192, DOI 10.1145/279310.279321
   Klein ST, 2008, INT J FOUND COMPUT S, V19, P137, DOI 10.1142/S0129054108005589
   Klein ST, 2007, IEEE DATA COMPR CONF, P113
   Korn D., 2012, 3284 RFC IETF
   Korn DG, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE GENERAL TRACK, P219
   Kurose J. F., 2020, Computer Networking: A Top-Down Approach, V8th
   LELEWER DA, 1987, COMPUT SURV, V19, P261, DOI 10.1145/45072.45074
   Lempsink Eelco, 2009, GDIFF GENERIC DIFF P
   Lin X., 2014, P 12 USENIX C FIL ST, V14, P256
   Macdonald Josh, 2016, XDELTA3
   Malensek Matthew, 2019, JBSDIFF
   May MJ, 2019, ACM T STORAGE, V15, DOI 10.1145/3295463
   Microsoft, 2020, WIND SYS
   Microsoft Docs, 2019, NTFS OV
   Nelson Stephen L., 2015, EXCEL DATA ANAL DUMM, V3rd
   OPhone Platform, 2010, STRUCT ANDR PACK APK
   Percival Colin, 2006, BINARY DIFF PATCH UT
   PKWare Inc, 2012, APPNOTETXT ZIP FIL F
   Samteladze N, 2014, IEEE INTERNET COMPUT, V18, P50, DOI 10.1109/MIC.2013.82
   Shilane Philip, 2012, 10 USENIX C FILE STO
   STORER JA, 1982, J ACM, V29, P928, DOI 10.1145/322344.322346
   Sul Torsten, 2003, LOSSLESS COMPRESSION, P269
   Sullivan Sean C., 2004, ZIPDIFF ONLINE
   Watanabe Scott, 2010, SOLARIS 10 ZFS ESSEN, V1st
   Xia W, 2014, PERFORM EVALUATION, V79, P258, DOI 10.1016/j.peva.2014.07.016
NR 41
TC 0
Z9 0
U1 1
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2022
VL 18
IS 3
AR 26
DI 10.1145/3507919
PG 41
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 4X9BK
UT WOS:000861131000008
DA 2024-07-18
ER

PT J
AU Dong, SY
   Kryczka, A
   Jin, YQ
   Stumm, M
AF Dong, Siying
   Kryczka, Andrew
   Jin, Yanqin
   Stumm, Michael
TI RocksDB: Evolution of Development Priorities in a Key-value Store
   Serving Large-scale Applications
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Key-value stores; large-scale applications; RocksDB; SSD; compaction;
   databases
ID ENGINE
AB This article is an eight-year retrospective on development priorities for RocksDB, a key-value store developed at Facebook that targets large-scale distributed systems and that is optimized for Solid State Drives (SSDs). We describe how the priorities evolved over time as a result of hardware trends and extensive experiences running RocksDB at scale in production at a number of organizations: from optimizing write amplification, to space amplification, to CPU utilization. We describe lessons from running large-scale applications, including that resource allocation needs to be managed across different RocksDB instances, that data formats need to remain backward- and forward-compatible to allow incremental software rollouts, and that appropriate support for database replication and backups are needed. Lessons from failure handling taught us that data corruption errors needed to be detected earlier and that data integrity protection mechanisms are needed at every layer of the system. We describe improvements to the key-value interface. We describe a number of efforts that in retrospect proved to be misguided. Finally, we describe a number of open problems that could benefit from future research.
C1 [Dong, Siying; Kryczka, Andrew; Jin, Yanqin] Facebook Inc, 1 Hacker Way, Menlo Pk, CA 94025 USA.
   [Stumm, Michael] Univ Toronto, Dept Elect & Comp Engn, 10 Kings Coll Rd, Toronto, ON M5S 3G4, Canada.
C3 Facebook Inc; University of Toronto
RP Dong, SY (corresponding author), Facebook Inc, 1 Hacker Way, Menlo Pk, CA 94025 USA.
EM siying.d@fb.com; andrewkr@fb.com; yanqin@fb.com; stumm@eecg.toronto.edu
OI Stumm, Michael/0000-0002-9377-2493
CR Abadi D.J., 2008, P 2008 ACM SIGMOD IN, P967, DOI DOI 10.1145/1376616.1376712
   Aghayev A, 2019, PROCEEDINGS OF THE TWENTY-SEVENTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '19), P353, DOI 10.1145/3341301.3359656
   Ahn JS, 2016, IEEE T COMPUT, V65, P902, DOI 10.1109/TC.2015.2435779
   [Anonymous], 2013, P 2013 ACM SIGMOD IN, DOI DOI 10.1145/2463676.2463710
   Apache, 2021, Cassandra.
   Apache Software Foundation, 2021, AP HBASE
   Apache Software Foundation, 2016, COMP AP CASS DOC
   Apache Software Foundation, 2015, HBASE ADD COMP POL E
   Athanassoulis M., 2016, EDBT, P461
   Balmau O, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P363
   Bjorling Matias, 2020, P USENIX LIN STOR FI
   Borthakur Dhruba, 2008, Hadoop Apache Project, V53, P2
   Callaghan Mark, 2016, MONGOROCKS WIREDTIGE
   Cao ZC, 2020, PROCEEDINGS OF THE 18TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P209
   Carbone P., 2015, IEEE DATA ENG B, V36, P28, DOI DOI 10.1109/IC2EW.2016.56
   Chang F, 2008, ACM T COMPUT SYST, V26, DOI 10.1145/1365815.1365816
   Chen A, 2016, SOLID STATE ELECTRON, V125, P25, DOI 10.1016/j.sse.2016.07.006
   Chen GJ, 2016, SIGMOD'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P1087, DOI 10.1145/2882903.2904441
   Corbett JC, 2013, ACM T COMPUT SYST, V31, DOI 10.1145/2491245
   Dayan N, 2017, SIGMOD'17: PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P79, DOI 10.1145/3035918.3064054
   Dixit Harish Dattatraya, 2021, ARXIV PREPRINT ARXIV
   Dong S., 2017, CIDR
   Dong SY, 2021, PROCEEDINGS OF THE 19TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '21), P33
   Evans Jason, 2019, JEMALLOC MEMORY ALLO
   Facebook, 2019, US TIM EXP ROCKSDB D
   Facebook, 2021, MERG OP ROCKSDB DOC
   Facebook, 2021, ROCKSDB UNIVERSAL CO
   Facebook, 2021, RAT LIM ROCKSDB DOC
   Facebook, 2021, WRIT AH LOG ROCKSDB
   Facebook, 2020, SST FIL MANG ROCKSDB
   Facebook, 2021, COL FAM ROCKSDB DOC
   Facebook, 2020, WRIT BUFF MAN ROCKSD
   Facebook, 2021, MEMTABLE ROCKSDB DOC
   Facebook, 2021, COMP FILT ROCKSDB DO
   Facebook RocksDB team, 2021, PERS KEY VAL STOR EN
   Faleiro Jose, 2019, P 18 INT WORKSH HIGH
   Frankie T., 2012, P 50 ANN SE REG C NE, P59
   Ghemawat S., 2011, LevelDB
   Ghemawat S., 2003, ACM SIGOPS OPERATING, V37, P29, DOI [DOI 10.1145/945445.945450, DOI 10.1145/1165389.945450, 10.1145/1165389.945450]
   Golan-Gueta G., 2015, EUROSYS, P1
   Gong C., 2019, IJCNN, P1, DOI [DOI 10.1109/IJCNN.2019.8851699, 10.1109/ijcnn.2019.8851699]
   Google, 2021, PROT
   Gu Dikang, 2018, INSTAGRAM ENG BLOG
   Hochschild Peter H., 2021, HotOS '21: Proceedings of the Workshop on Hot Topics in Operating Systems, P9, DOI 10.1145/3458336.3465297
   Huang DX, 2020, PROC VLDB ENDOW, V13, P3072, DOI 10.14778/3415478.3415535
   Intel, 2021, MEM OPT DAT CENTR WO
   Intel, 2021, TRIM OV
   Iron, 2021, SIMPL FLEX REL SERV
   Kimura H, 2015, SIGMOD'15: PROCEEDINGS OF THE 2015 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P691, DOI 10.1145/2723372.2746480
   Kreps Jay, 2016, INTRO KAFKA STREAMS
   Kuszmaul B., 2010, TOKUDB FRACTAL TREE
   Lever Chuck, 2012, END TO END DATA INTE
   Lim H, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P1
   Lin Mike, 2019, DNANEXUS DEV BLOG
   Lu LY, 2017, ACM T STORAGE, V13, DOI 10.1145/3033273
   Mansfield Scott, 2016, NETFLIX TECHNOLOGY B
   Mao QZ, 2021, VLDB J, V30, P361, DOI 10.1007/s00778-020-00638-1
   Marchukov Mark, 2017, FACEBOOK ENG BLOG
   Matsunobu Y, 2020, PROC VLDB ENDOW, V13, P3217, DOI 10.14778/3415478.3415546
   Matsunobu Yoshinori, 2017, FACEBOOK ENG BLOG
   Microsoft, 2021, SQL SERV
   Mituzas Domas, 2013, FLASHCACHE FACEBOOK
   MongoDB, 2021, WIREDTIGER STORAGE E
   MongoDB, 2021, DAT MOD APPL
   MySQL, 2021, INTR INNODEDB
   Noghabi SA, 2017, PROC VLDB ENDOW, V10, P1634, DOI 10.14778/3137765.3137770
   Olson MA, 1999, PROCEEDINGS OF THE FREENIX TRACK, P183
   ONeil P, 1996, ACTA INFORM, V33, P351, DOI 10.1007/s002360050048
   oracle, 2021, OR DAT
   Ouaknine Keren, 2017, P INT C COMPUTE DATA, P155, DOI DOI 10.1145/3093241.3093278
   Owens M., 2006, The Definitive Guide to SQLite
   Percona LLC, 2021, MONGOROCKS
   Petersen Martin K, 2008, P LIN S, V4
   Petersen Martin K., 2010, ELIMINATING SILENT D
   Picoli Ivan Luiz, 2020, CIDR
   Qihoo, 2021, PIK
   Raju P, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P497, DOI 10.1145/3132747.3132765
   Ren K, 2017, PROC VLDB ENDOW, V10, P2037, DOI 10.14778/3151106.3151108
   RocksDB, 2020, LEV COMP
   SALTZER JH, 1984, ACM T COMPUT SYST, V2, P277, DOI 10.1145/357401.357402
   Savor T, 2016, 2016 IEEE/ACM 38TH INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING COMPANION (ICSE-C), P21, DOI 10.1145/2889160.2889223
   Sears Russell, 2012, P ACM SIGMOD INT C M, P217
   Sharma Arun, 2016, FACEBOOK ENG BLOG
   Sharma Arun, 2019, WE USE ROCKSDB ROCKS
   Shetty P.J., 2013, 11 USENIX C FILE STO, P17
   Sivathanu Gopalan, 2004, FSL0404 STON BROOK U
   Slee M., 2007, THRIFT SCALABLE CROS
   Taft R, 2020, SIGMOD'20: PROCEEDINGS OF THE 2020 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P1493, DOI 10.1145/3318464.3386134
   Tai A, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P977
   Tamasi Levi, 2021, FACEBOOK ENG BLOG
   University of Wisconsin Computer Sciences Department, 2012, SHOR STOR MAN MULT V
   Vincon T., 2018, EDBT, P457
   Wang Peng, 2014, P 9 EUR C COMP SYST, DOI DOI 10.1145/2592798.2592804
   Wang Qi, 2018, JEMALLOC TUNING
   Wikipedia contributors, 2020, DAT INT FIELD
   Xu Ning, 2016, ENG BLOG
   Xu Tao, 2015, ROCKSDB PERSONALIZED
   Yang Fei, 2015, P NONV MEM WORKSH
   Zhang JC, 2017, ACM T EMBED COMPUT S, V16, DOI 10.1145/3126545
   Zhang Y., 2010, FAST, P29
   Zhang YMZ, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.83
NR 101
TC 37
Z9 38
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2021
VL 17
IS 4
AR 26
DI 10.1145/3483840
PG 32
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YY2GY
UT WOS:000754612700002
OA Bronze
DA 2024-07-18
ER

PT J
AU Cheng, W
   Li, CY
   Zeng, LF
   Qian, YJ
   Li, X
   Brinkmann, A
AF Cheng, Wen
   Li, Chunyan
   Zeng, Lingfang
   Qian, Yingjin
   Li, Xi
   Brinkmann, Andre
TI NVMM-Oriented Hierarchical Persistent Client Caching for Lustre
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Lustre; non-volatile memory; direct access; persistent caching;
   hierarchical storage management
ID SYSTEM
AB In high-performance computing (HPC), data and metadata are stored on special server nodes and client applications access the servers' data and metadata through a network, which induces network latencies and resource contention. These server nodes are typically equipped with (slow) magnetic disks, while the client nodes store temporary data on fast SSDs or even on non-volatile main memory (NVMM). Therefore, the full potential of parallel file systems can only be reached if fast client side storage devices are included into the overall storage architecture.
   In this article, we propose an NVMM-based hierarchical persistent client cache for the Lustre file system (NVMM-LPCC for short). NVMM-LPCC implements two caching modes: a read and write mode (RW-NVMM-LPCC for short) and a read only mode (RO-NVMM-LPCC for short). NVMM-LPCC integrates with the Lustre Hierarchical Storage Management (HSM) solution and the Lustre layout lock mechanism to provide consistent persistent caching services for I/O applications running on client nodes, meanwhile maintaining a global unified namespace of the entire Lustre file system. The evaluation results presented in this article show that NVMM-LPCC can increase the average read throughput by up to 35.80 times and the average write throughput by up to 9.83 times compared with the native Lustre system, while providing excellent scalability.
C1 [Cheng, Wen; Li, Chunyan; Zeng, Lingfang] Huazhong Univ Sci & Technol, Wuhan Natl Lab Optoelect, Luoyu Rd 1037, Wuhan 430074, Peoples R China.
   [Qian, Yingjin; Li, Xi] Data Direct Networks, 99 Danba Rd, Shanghai, Peoples R China.
   [Brinkmann, Andre] Johannes Gutenberg Univ Mainz, Zentrum Datenverarbeitung, Saarstr 21, D-55122 Mainz, Germany.
C3 Huazhong University of Science & Technology; Johannes Gutenberg
   University of Mainz
RP Zeng, LF (corresponding author), Huazhong Univ Sci & Technol, Wuhan Natl Lab Optoelect, Luoyu Rd 1037, Wuhan 430074, Peoples R China.
EM chengwen@hust.edu.cn; lichunyan@hust.edu.cn; lfzeng@hust.edu.cn;
   qian@ddn.com; lixi@ddn.com; brinkman@uni-mainz.de
RI li, chunyan/E-7476-2019; Brinkmann, André/H-9888-2016
OI Zeng, Lingfang/0000-0003-3130-3015
FU National Natural Science Foundation of China [61472153, 61821003,
   61832020]; Hubei Natural Science Foundation [2015CFB192]
FX This work is supported in part by the National Natural Science
   Foundation of China (61472153, 61821003, 61832020) and the Hubei Natural
   Science Foundation (2015CFB192).
CR ACM, T STORAGE, V17
   ACM, T STORAGE, V17
   Akinaga H, 2010, P IEEE, V98, P2237, DOI 10.1109/JPROC.2010.2070830
   Axboe Jens, 2019, FIO FLEXIBLE I O TES
   Boito FZ, 2018, ACM COMPUT SURV, V51, DOI 10.1145/3152891
   Braam Peter, 2005, **NON-TRADITIONAL**
   Brinkmann A, 2020, J COMPUT SCI TECH-CH, V35, P4, DOI 10.1007/s11390-020-9801-1
   Chen YM, 2018, ACM T STORAGE, V14, DOI 10.1145/3204454
   Congiu G, 2016, IEEE INT C CL COMP, P120, DOI 10.1109/CLUSTER.2016.37
   Dong MK, 2019, PROCEEDINGS OF THE TWENTY-SEVENTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '19), P478, DOI 10.1145/3341301.3359637
   Dulloor Subramanya R., 2014, P EUR C COMP SYST EU, P1, DOI [10.1145/2592798.2592814, DOI 10.1145/2592798.2592814]
   Eshel Marc., 2010, FAST, P155
   Herodotou H, 2019, PROC VLDB ENDOW, V13, P43, DOI 10.14778/3357377.3357381
   Hoseinzadeh Morteza, 2019, CORR
   Howells D., 2006, P LIN S, V1, P427
   Huang J, 2014, PROC VLDB ENDOW, V8, P389, DOI 10.14778/2735496.2735502
   Islam N.S., 2016, P OF THE 2016 INT C, P1
   Jackson Adrian, 2018, CORR
   Kadekodi R, 2019, PROCEEDINGS OF THE TWENTY-SEVENTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '19), P494, DOI 10.1145/3341301.3359631
   Krish KR, 2016, IEEE ACM INT SYMP, P403, DOI 10.1109/CCGrid.2016.61
   Lee BC, 2009, CONF PROC INT SYMP C, P2, DOI 10.1145/1555815.1555758
   Liu N, 2012, IEEE S MASS STOR SYS
   Lu YY, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P773
   Memaripour A, 2018, PR IEEE COMP DESIGN, P413, DOI 10.1109/ICCD.2018.00069
   Mikolajick T, 2001, MICROELECTRON RELIAB, V41, P947, DOI 10.1016/S0026-2714(01)00049-X
   Qian YJ, 2019, PROCEEDINGS OF SC19: THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS, DOI 10.1145/3295500.3356139
   Qian YJ, 2017, SC'17: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS, DOI 10.1145/3126908.3126932
   Schmuck F, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE FAST'02 CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P231
   Shan Hongzhang, 2007, P CRAY US GROUP M CU
   Shan YZ, 2017, PROCEEDINGS OF THE 2017 SYMPOSIUM ON CLOUD COMPUTING (SOCC '17), P323, DOI 10.1145/3127479.3128610
   Shi L, 2012, IEEE INT C CL COMP, P546, DOI 10.1109/CLUSTER.2012.41
   Swift, 2014, P 9 EUR C COMP SYST, DOI DOI 10.1145/2592798.2592810
   Tarasov Vasily, 2016, login Usenix Mag, V41, P1
   Vef MA, 2020, J COMPUT SCI TECH-CH, V35, P72, DOI 10.1007/s11390-020-9797-6
   Vef MA, 2018, ACM T STORAGE, V14, DOI 10.1145/3149376
   Volos H, 2011, ACM SIGPLAN NOTICES, V46, P91, DOI [10.1145/1961296.1950379, 10.1145/1961295.1950379]
   Wan LP, 2017, J PARALLEL DISTR COM, V100, P16, DOI 10.1016/j.jpdc.2016.10.002
   Wang T, 2016, SC '16: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS, P807, DOI 10.1109/SC.2016.68
   Wei Xue, 2017, LCOC LUSTRE CACHE CL
   Wilcox Matthew., 2014, Add support for NV-DIMMs to ext4
   Wong HSP, 2010, P IEEE, V98, P2201, DOI 10.1109/JPROC.2010.2070050
   Xi Li, 2018, LUSTRE PERSISTENT CL
   Xu J, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P323
   Zhang JC, 2018, INT C PAR DISTRIB SY, P51, DOI [10.1109/PADSW.2018.8644859, 10.1109/ICPADS.2018.00018]
NR 44
TC 9
Z9 11
U1 1
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD FEB
PY 2021
VL 17
IS 1
AR 6
DI 10.1145/3404190
PG 22
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QG9WR
UT WOS:000617930800005
DA 2024-07-18
ER

PT J
AU Marmol, L
   Chowdhury, M
   Rangaswami, R
AF Marmol, Leonardo
   Chowdhury, Mohammad
   Rangaswami, Raju
TI LIBPM: Simplifying Application Usage of Persistent Memory
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Persistent memory; next-generation applications; application persistence
AB Persistent Memory devices present properties that are uniquely different from prior technologies for which applications have been built. Unfortunately, the conventional approach to building applications fail to either efficiently utilize these new devices or provide programmers a seamless development experience. We have built LIBPM, a Persistent Memory Library that implements an easy-to-use container abstraction for consuming PM. LibPM's containers are data hosting units that can store arbitrarily complex data types while preserving their integrity and consistency. Consequently, LIBPM's containers provide a generic interface to applications, allowing applications to store and manipulate arbitrarily structured data with strong durability and consistency properties, all without having to navigate all the myriad pitfalls of programming PM directly. By providing a simple and high-performing transactional update mechanism, LIBPM allows applications to manipulate persistent data at the speed of memory. The container abstraction and automatic persistent data discovery mechanisms within LIBPM also simplify porting legacy applications to PM. From a performance perspective, LIBPM closely matches and often exceeds the performance of state-of-the-art application libraries for PM. For instance, LIBPM 's performance is 195x better for write intensive workloads and 2.6x better for read intensive workloads when compared with the state-of-the-art PMEM.IO persistent memory library.
C1 [Marmol, Leonardo; Chowdhury, Mohammad; Rangaswami, Raju] Florida Int Univ, Sch Comp & Informat Sci, 11200 SW 8th St, Miami, FL 33199 USA.
C3 State University System of Florida; Florida International University
RP Marmol, L (corresponding author), Florida Int Univ, Sch Comp & Informat Sci, 11200 SW 8th St, Miami, FL 33199 USA.
EM marmol@cs.fiu.edu; mchow017@cs.fiu.edu; raju@cs.fiu.edu
OI Rangaswami, Raju/0009-0000-5243-9451
FU NSF [CCF-1718335, CSR-1563883]; Intel ISRA award
FX We thank the members of the Intel persistent memory research group and
   the anonymous reviewers of this article for their insightful feedback.
   This work was supported in part by NSF awards CCF-1718335, CSR-1563883,
   and an Intel ISRA award.
CR [Anonymous], P 37 ANN INT S COMP
   [Anonymous], P 15 ED ASPLOS ARCH
   [Anonymous], P ACM S OP SYST PRIN
   BONWICK J, 1994, PROCEEDINGS OF THE SUMMER 1994 USENIX CONFERENCE, P87
   Caulfield Adrian M., 2010, Proceedings 2010 43rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO 2010), P385, DOI 10.1109/MICRO.2010.33
   Caulfield A. M., 2012, P INT C ARCH SUPP PR
   Chowdhury Mohammad, 2017, P 33 INT C MASS STOR
   Coburn J., 2011, P INT C ARCH SUPP PR
   Dulloor S. R., 2014, EUROPEAN C COMPUTER
   Guerra Jorge, 2012, P USENIX ANN TECHN C
   Hwang T, 2015, ACM T STORAGE, V11, DOI 10.1145/2629619
   Intel Corporation, 2014, INT ARCH INSTR SET E
   Intel Corporation, 2015, INT ARCH SET EXT PRO
   Kemper Alfons, 1995, The VLDB Journal, V4, P519
   Kim Wook-Hee, 2016, NVWAL EXPLOITING NVR, DOI [10.1145/2954679.2872392, DOI 10.1145/2954679.2872392]
   Lantz Philip, 2014, P USENIX ANN TECHN C
   Necula G. C., 2002, LECT NOTES COMPUTER
   Oukid I, 2016, SIGMOD'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P371, DOI 10.1145/2882903.2915251
   Rudoff Andy, 2014, NVM LIB
   Schwartz E.J., 2011, USENIX Security Symposium, P25
   Shacham H., 2004, ACM COMPUTER COMMUNI, P298, DOI 10.1145/1030083.1030124
   Volos Haris, 2011, P INT C ARCH SUPP PR
   Wu X., 2011, P SC
   Xu J, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P323
   Yang Jun, 2015, P 13 USENIX C FIL ST, P167
   Zhang Y., 2015, P 20 INT C ARCH SUPP
NR 26
TC 7
Z9 8
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD DEC
PY 2018
VL 14
IS 4
AR 34
DI 10.1145/3278141
PG 18
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ4JO
UT WOS:000457140500006
OA Bronze
DA 2024-07-18
ER

PT J
AU Kesavan, R
   Singh, R
   Grusecki, T
   Patel, Y
AF Kesavan, Ram
   Singh, Rohit
   Grusecki, Travis
   Patel, Yuvraj
TI Efficient Free Space Reclamation in WAFL
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Storage system; file system; garbage collection; free space reclamation;
   snapshots; deduplication; file system performance
AB NetApp (R) WAFL (R) is a transactional file system that uses the copy-on-write mechanism to support fast write performance and efficient snapshot creation. However, copy-on-write increases the demand on the file system to find free blocks quickly, which makes rapid free space reclamation essential. Inability to find free blocks quickly may impede allocations for incoming writes. Efficiency is also important, because the task of reclaiming free space may consume CPU and other resources at the expense of client operations. In this article, we describe the evolution (over more than a decade) of the WAFL algorithms and data structures for reclaiming space with minimal impact to the overall performance of the storage appliance.
C1 [Kesavan, Ram; Singh, Rohit; Grusecki, Travis] NetApp Inc, 495 East Java Dr, Sunnyvale, CA 94089 USA.
   [Patel, Yuvraj] Univ Wisconsin, 1210 W Dayton St, Madison, WI 53706 USA.
C3 NetApp, Inc.; University of Wisconsin System; University of Wisconsin
   Madison
RP Kesavan, R (corresponding author), NetApp Inc, 495 East Java Dr, Sunnyvale, CA 94089 USA.
EM ram.kesavan@gmail.com; rh0@netapp.com; travisg@netapp.com;
   yuvraj@cs.wisc.edu
CR Bender MA, 2007, SPAA'07: PROCEEDINGS OF THE NINETEENTH ANNUAL SYMPOSIUM ON PARALLELISM IN ALGORITHMS AND ARCHITECTURES, P81
   Cao M., 2008, LINUX S, P263
   Corbett Peter, 2004, P USENIX C FIL STOR
   Curtis-Maury Matthew, 2016, P S OP SYST DES IMPL
   Curtis-Maury Matthew, 2017, P INT C PAR PROC ICP
   Denz PR, 2016, PROC INT CONF PARAL, P386, DOI 10.1109/ICPP.2016.51
   Edwards JohnK., 2008, ATC 08, P129
   Grusecki Travis R., 2012, IMPROVING BLOCK SHAR
   Hertel C., 2003, Implementing CIFS: The Common Internet File System
   HITZ D, 1994, PROCEEDINGS OF THE WINTER 1994 USENIX CONFERENCE, P235
   Kesavan R, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P1
   Kesavan Ram, 2014, Patent US, Patent No. 8812450
   Kumar Harendra, 2017, PROCEEDINGS OF CONFE
   Mathur Avantika., 2007, login Usenix Mag, V32
   Mckusick M. K., 1986, FSCK UNIX FILE SYSTE
   MCKUSICK MK, 1984, ACM T COMPUT SYST, V2, P181, DOI 10.1145/989.990
   NetApp Inc, 2010, DAT ONTAP 8
   Patterson Hugo, 2002, P USENIX C FIL STOR
   Ridge Peter M., 1995, THE BOOK OF SCSI
   Rodeh O, 2013, ACM T STORAGE, V9, DOI 10.1145/2501620.2501623
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   Sandberg Russel, 1985, P SUMM 1985 USENIX T, P19
   Standard Performance Evaluation Corporation, 2014, SPEC SFS 2014
   Storage Performance Council, STOR PERF COUNC 1 BE
   Subramanian Ananthan, 2017, Patent US, Patent No. [20170031772, 0031772]
   Sun Microsystems, ZFS OPENSOLARIS COMM
   Sweeney A., 1996, USENIX ANN TECHN C M
   Thornburgh R.H., 2000, STORAGE AREA NETWORK
   Tweedie S., 1998, P 4 ANN LIN EXP LINU
   Wright David D., 2014, Patent US, Patent No. 8819208
NR 30
TC 7
Z9 7
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD OCT
PY 2017
VL 13
IS 3
SI SI
AR 23
DI 10.1145/3125647
PG 29
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FL6DV
UT WOS:000414336300006
DA 2024-07-18
ER

PT J
AU Pillai, TS
   Alagappan, R
   Lu, LY
   Chidambaram, V
   Arpaci-Dusseau, AC
   Arpaci-Dusseau, RH
AF Pillai, Thanumalayan Sankaranarayana
   Alagappan, Ramnatthan
   Lu, Lanyue
   Chidambaram, Vijay
   Arpaci-Dusseau, Andrea C.
   Arpaci-Dusseau, Remzi H.
TI Application Crash Consistency and Performance with CCFS
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE File systems; crash consistency; reordering; performance
AB Recent research has shown that applications often incorrectly implement crash consistency. We present the Crash-Consistent File System (ccfs), a file system that improves the correctness of application-level crash consistency protocols while maintaining high performance. A key idea in ccfs is the abstraction of a stream. Within a stream, updates are committed in program order, improving correctness; across streams, there are no ordering restrictions, enabling scheduling flexibility and high performance. We empirically demonstrate that applications running atop ccfs achieve high levels of crash consistency. Further, we show that ccfs performance under standard file-system benchmarks is excellent, in the worst case on par with the highest performing modes of Linux ext4, and in some cases notably better. Overall, we demonstrate that both application correctness and high performance can be realized in a modern file system.
C1 [Pillai, Thanumalayan Sankaranarayana; Alagappan, Ramnatthan; Lu, Lanyue; Arpaci-Dusseau, Andrea C.; Arpaci-Dusseau, Remzi H.] Univ Wisconsin, 1210 W Dayton St, Madison, WI 53706 USA.
   [Chidambaram, Vijay] Univ Texas Austin, 2317 Speedway, Austin, TX 78712 USA.
C3 University of Wisconsin System; University of Wisconsin Madison;
   University of Texas System; University of Texas Austin
RP Pillai, TS (corresponding author), Univ Wisconsin, 1210 W Dayton St, Madison, WI 53706 USA.
EM madthanu@cs.wisc.edu; ra@cs.wisc.edu; ll@cs.wisc.edu;
   vijay@cs.utexas.edu; dusseau@cs.wisc.edu; remzi@cs.wisc.edu
RI Chidambaram, Vijay/HJA-2695-2022; LU, LU/JEZ-4760-2023
OI Alagappan, Ramnatthan/0000-0001-9911-4208; Lu, Lu/0000-0003-3175-7770;
   Chidambaram, Vijay/0000-0001-7985-6087; Arpaci-Dusseau,
   Andrea/0000-0001-8618-2738
FU NSF [CNS-1419199, CNS-1421033, CNS-1319405, CNS-1218405]; DOE
   [DE-SC0014935]
FX The research herein was supported by funding from NSF grants
   CNS-1419199, CNS-1421033, CNS-1319405, and CNS-1218405, DOE grant
   DE-SC0014935, as well as donations from EMC, Facebook, Google, Huawei,
   Microsoft, NetApp, Samsung, Seagate, Veritas, and VMware. Any opinions,
   findings, and conclusions or recommendations expressed in this material
   are those of the authors and may not reflect the views of NSF, DOE, or
   other institutions. This article is an extended version of a FAST' 17
   paper by T.S. Pillai et al. [37]. The additional material here includes
   a detailed description of the stream API, detailed discussion of the
   limitations of the stream abstraction and the ccfs file system, more
   thorough discussion of developer effort required to make applications
   crash consistent atop ccfs, more figures and code snippets to illustrate
   usage of streams, and several other minor edits and updates.
CR Alagappan Ramnatthan, 2016, P 12 S OP SYST DES I
   [Anonymous], 2016, P 21 INT C ARCH SUPP
   [Anonymous], 2013, P 24 ACM S OP SYST P
   [Anonymous], 2011, SYNTHESIS LECT COMPU
   [Anonymous], 2016, LINUX DOCUMENTATION
   [Anonymous], 2011, P 23 ACM S OP SYST P
   Arpaci-Dusseau R. H., 2018, Operating systems: Three easy pieces
   Birrell Andrew D., 1989, SRCRR35
   Burnett Nathan C., 2006, THESIS
   Chidambaram V., 2012, Proceedings of the 10th USENIX conference on File and Storage Technologies, FAST 2012, San Jose, CA, USA, February 14-17, 2012, P9
   Chutani S., 1992, Proceedings of the USENIX Winter 1992 Technical Conference, P43
   Condit J., 2009, P 22 ACM S OP SYST P
   Corbet Jonathan, 2009, BETTER THAN POSIX
   Czezatke Christian, 2000, P USENIX ANN TECHN C
   Frost Christopher, 2007, Operating Systems Review, V41, P307, DOI 10.1145/1323293.1294291
   Ganger G. R., 1994, Proceedings of the First USENIX Symposium on Operating Systems Design and Implementation (OSDI), P49
   Ganger GR, 2000, ACM T COMPUT SYST, V18, P127, DOI 10.1145/350853.350863
   Gharachorloo Kourosh, 1992, P 17 ANN INT S COMP
   Hagmann Robert, 1987, P 11 ACM S OP SYST P
   Harris T., 2001, DISC
   HERLIHY M, 1991, ACM T PROGR LANG SYS, V13, P124, DOI 10.1145/114005.102808
   Jacobson D. M., 1991, HPLCSP917
   Kim Jaeho, 2009, SOFT
   Linus Torvalds, 2009, LIN 2 6 29
   LU L., 2013, P 11 USENIX S FIL ST
   Lu Lanyue, 2014, P 11 S OP SYST DES I
   Lu Lanyue, 2016, P 14 USENIX C STOR T
   Maria DB, 2016, FUSION IO NVMFS ATOM
   Mercurial, 2016, DEAL REP DIRST CORR
   Microsoft, 2016, ALT US T NTFS
   Min Changwoo, 2015, P USENIX ANN TECHN C
   MOHAN C, 1992, ACM T DATABASE SYST, V17, P94, DOI 10.1145/128765.128770
   Nightingale Edmund B., 2006, P 7 S OP SYST DES IM
   ONeil P, 1996, ACTA INFORM, V33, P351, DOI 10.1007/s002360050048
   Park Stan, 2013, P EUROSYS C EUROSYS
   Pelley Steven, 2014, P 41 INT S COMP ARCH
   Pillai TS, 2015, COMMUN ACM, V58, P46, DOI 10.1145/2788401
   Pillai Thanumalayan Sankaranarayana, 2017, P 15 USENIX C FIL ST
   Pillai Thanumalayan Sankaranarayana, 2014, OSDI, P433
   Porter Donald E., 2008, P 8 S OP SYST DES IM
   Prabhakaran V, 2005, USENIX ASSOCIATION PROCEEDINGS OF THE GENERAL TRACK: 2005 UNENIX ANNUAL TECHNICAL CONFERENCE, P105
   Prabhakaran Vijayan, 2008, P 8 S OP SYST DES IM
   Recht B., 2011, ADV NEURAL INFORM PR, P693
   Reuter A, 1993, T PROCESSING CONCEPT
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   RUEMMLER C, 1994, COMPUTER, V27, P17, DOI 10.1109/2.268881
   Seltzer M., 1990, Proceedings of the Winter 1990 USENIX Conference, P313
   Seltzer M., 1993, USENIX Association. Proceedings of the Winter 1993 USENIX Conference, P307
   Seltzer Margo I., 1993, THESIS
   Seltzer MI, 2000, USENIX ASSOCIATION PROCEEDINGS OF THE 2000 USENIX ANNUAL TECHNICAL CONFERENCE, P71
   Shin Ji-Yong, 2016, P 14 USENIX C FIL ST
   Spillane Richard P., 2009, P 7 USENIX S FIL STO
   SQLite, 2016, IS SQLITE
   SQLite, 2016, SQL UND SQLITE
   Tarasov Vasily, 2016, login Usenix Mag, V41, P1
   Verma Rajat, 2015, P 13 USENIX C FIL ST
   Wright Charles P., 2007, ACM Transactions on Storage, V3, P1, DOI 10.1145/1242520.1242521
   Yang Junfeng, 2006, P 7 S OP SYST DES IM
   Zheng M., 2014, P 11 S OP SYST DES I, P449
NR 59
TC 2
Z9 2
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD OCT
PY 2017
VL 13
IS 3
SI SI
AR 19
DI 10.1145/3119897
PG 29
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FL6DV
UT WOS:000414336300002
OA Bronze, Green Submitted
DA 2024-07-18
ER

PT J
AU Lu, YY
   Shu, JW
   Sun, L
AF Lu, Youyou
   Shu, Jiwu
   Sun, Long
TI Blurred Persistence: Efficient Transactions in Persistent Memory
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article; Proceedings Paper
CT 31st International Conference on Massive Storage Systems and
   Technologies (MSST 2015)
CY JUN 01-05, 2015
CL Santa Clara, CA
DE Design; Performance; Persistent memory; transaction consistency;
   persistence
ID PHASE-CHANGE MEMORY; HIGH-PERFORMANCE
AB Persistent memory provides data durability in main memory and enables memory-level storage systems. To ensure consistency of such storage systems, memory writes need to be transactional and are carefully moved across the boundary between the volatile CPU cache and the persistent main memory. Unfortunately, cache management in the CPU cache is hardware-controlled. Legacy transaction mechanisms, which are designed for disk-based storage systems, are inefficient in ordered data persistence of transactions in persistent memory. In this article, we propose the Blurred Persistence mechanism to reduce the transaction overhead of persistent memory by blurring the volatility-persistence boundary. Blurred Persistence consists of two techniques. First, Execution in Log executes a transaction in the log to eliminate duplicated data copies for execution. It allows persistence of the volatile uncommitted data, which are detectable with reorganized log structure. Second, Volatile Checkpoint with Bulk Persistence allows the committed data to aggressively stay volatile by leveraging the data durability in the log, as long as the commit order across threads is kept. By doing so, it reduces the frequency of forced persistence and improves cache efficiency. Evaluations show that our mechanism improves system performance by 56.3% to 143.7% for a variety of workloads.
C1 [Lu, Youyou; Shu, Jiwu; Sun, Long] Tsinghua Univ, Dept Comp Sci & Tech, Beijing 100084, Peoples R China.
C3 Tsinghua University
RP Shu, JW (corresponding author), Tsinghua Univ, Dept Comp Sci & Tech, Beijing 100084, Peoples R China.
EM luyouyou@tsinghua.edu.cn; shujw@tsinghua.edu.cn;
   sun-l12@mails.tsinghua.edu.cn
RI Lu, Youyou/AAE-4401-2020
OI Lu, Youyou/0000-0002-6214-5390
FU National Natural Science Foundation of China [61502266, 61232003];
   Beijing Municipal Science and Technology Commission of China
   [D151100000815003]; National High Technology Research and Development
   Program of China [2013AA013201]; China Postdoctoral Science Foundation
   [2015M580098]
FX This work is supported by the National Natural Science Foundation of
   China (Grant No. 61502266, 61232003), the Beijing Municipal Science and
   Technology Commission of China (Grant No. D151100000815003), the
   National High Technology Research and Development Program of China
   (Grant No. 2013AA013201), and China Postdoctoral Science Foundation
   (Grant No. 2015M580098).
CR AMD, 2011, AMD64 ARCH PROGR MAN, V3
   [Anonymous], 2008, P 8 USENIX C OP SYST
   [Anonymous], 2011, FAST
   Chidambaram Vijay, 2012, P 10 USENIX C FIL ST
   COBURN J, 2011, P 16 INT C ARCH SUPP, P105
   Coburn J, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P197, DOI 10.1145/2517349.2522724
   Condit J, 2009, SOSP'09: PROCEEDINGS OF THE TWENTY-SECOND ACM SIGOPS SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P133
   Eunji Lee, 2013, P 11 USENIX C FIL ST
   FAL Lab, 2014, TOK CAB MOD IMPL DBM
   Felber P, 2008, PPOPP'08: PROCEEDINGS OF THE 2008 ACM SIGPLAN SYMPOSIUM ON PRINCIPLES AND PRACTICE OF PARALLEL PROGRAMMING, P237, DOI 10.1145/1345206.1345241
   Frost Christopher, 2007, Operating Systems Review, V41, P307, DOI 10.1145/1323293.1294291
   Gray Jim, 1981, COMPUTING SURVEYS
   Guerrero J. M., 2012, Proceedings of the First International Conference on Robotics and Associated High-technologies and Equipment for Agriculture. Applications of automated systems and robotics for crop protection in sustainable precision agriculture, (RHEA-2012) Pisa, Italy - September 19-21, 2012, P319
   Harris Tim., 2010, Synthesis Lectures on Computer Architecture, V2nd
   Herlihy M., 1993, INT S COMPUTER ARCHI, DOI DOI 10.1145/165123.165164
   Intel, 2014, INT C STM COMP PROT
   Intel, 2013, 319433015 INT, V319433-015
   Jishen Zhao, 2013, 2013 46th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO). Proceedings, P421, DOI 10.1145/2540708.2540744
   Kultursay Emre, 2013, IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS 2013), P256
   Lee BC, 2009, CONF PROC INT SYMP C, P2, DOI 10.1145/1555815.1555758
   Long Sun, 2015, P ACM INT C COMP FRO
   Lowell D. E., 1997, Operating Systems Review, V31, P92, DOI 10.1145/269005.266665
   LU Y, 2013, P 31 IEEE INT C COMP, P115
   Lu Y., 2015, 2015 IEEE INT SOLID, P1
   Lu YY, 2014, IEEE NON-VOLATILE ME
   Lu YY, 2015, IEEE T COMPUT, V64, P2819, DOI 10.1109/TC.2015.2389828
   Lu Youyou, 2015, IEEE T COMP IN PRESS
   Lu Youyou, 2014, P IEEE 32 INT C COMP
   Lu Youyou, 2014, P USENIX C FIL STOR, P75
   Lu Youyou, 2013, P 11 USENIX C FIL ST
   Meza Justin, 2013, P 5 WORKSH EN EFF DE
   Mohan C., 1992, ACM T DATABASE SYSTE
   Moraru Iulian, 2011, CMUPDL11114V2
   Narayanan Dushyanth, 2012, P 17 INT C ARCH SUPP, P401, DOI DOI 10.1145/2150976.2151018
   Nightingale EB, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P1
   Ouyang XY, 2011, INT S HIGH PERF COMP, P301, DOI 10.1109/HPCA.2011.5749738
   Pelley S, 2014, CONF PROC INT SYMP C, P265, DOI 10.1109/ISCA.2014.6853222
   Prabhakaran Vijayan, 2005, P OF THE 20 ACM S ON, V39, P206
   Qureshi MK., 2011, SYNTHESIS LECT COMPU, V6, P1
   Qureshi MK, 2009, CONF PROC INT SYMP C, P24, DOI 10.1145/1555815.1555760
   Raghu Ramakrishnan and Johannes Gehrke, 2000, DATABASE MANAGEMENT
   SATYANARYANAN M, 1994, ACM T COMPUT SYST, V12, P33, DOI 10.1145/174613.174615
   Seltzer MI, 2000, USENIX ASSOCIATION PROCEEDINGS OF THE 2000 USENIX ANNUAL TECHNICAL CONFERENCE, P71
   Subramanya R., 2014, P 9 EUR C COMP SYST, P1
   Tweedie S.C., 1998, The Fourth Annual Linux Expo
   Volos H, 2011, ACM SIGPLAN NOTICES, V46, P91, DOI [10.1145/1961296.1950379, 10.1145/1961295.1950379]
   Wang Z., 2014, COMPUTER ARCHITECTUR
   Wu Xiaojian, 2011, P 2011 INT C HIGH PE
   Zhou P, 2009, CONF PROC INT SYMP C, P14, DOI 10.1145/1555815.1555759
NR 49
TC 21
Z9 25
U1 0
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JAN
PY 2016
VL 12
IS 1
SI SI
AR 3
DI 10.1145/2851504
PG 29
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DI5EI
UT WOS:000373521300003
DA 2024-07-18
ER

PT J
AU Lu, LY
   Arpaci-Dusseau, AC
   Arpaci-Dusseau, RH
   Lu, S
AF Lu, Lanyue
   Arpaci-Dusseau, Andrea C.
   Arpaci-Dusseau, Remzi H.
   Lu, Shan
TI A Study of Linux File System Evolution
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Algorithms; Performance; Reliability; File systems; patch; bug;
   failure; performance; reliability
AB We conduct a comprehensive study of file-system code evolution. By analyzing eight years of Linux file-system changes across 5079 patches, we derive numerous new (and sometimes surprising) insights into the file-system development process; our results should be useful for both the development of file systems themselves as well as the improvement of bug-finding tools.
C1 [Lu, Lanyue; Arpaci-Dusseau, Andrea C.; Arpaci-Dusseau, Remzi H.; Lu, Shan] Univ Wisconsin, Dept Comp Sci, Madison, WI 53706 USA.
C3 University of Wisconsin System; University of Wisconsin Madison
RP Lu, LY (corresponding author), Univ Wisconsin, Dept Comp Sci, 1210 W Dayton St, Madison, WI 53706 USA.
EM ll@cs.wisc.edu; dusseau@cs.wisc.edu; remzi@cs.wisc.edu;
   shanlu@cs.wisc.edu
RI LU, LU/JEZ-4760-2023
OI Lu, Shan/0000-0002-0757-4600; Lu, Lu/0000-0003-3175-7770
FU National Science Foundation [CNS-1218405, CCF-0937959, CSR-1017518,
   CCF-1016924, CNS-1319405]
FX This material is based on work supported by the National Science
   Foundation under the following grants: CNS-1218405, CCF-0937959,
   CSR-1017518, CCF-1016924, CNS-1319405, as well as generous donations
   from EMC, Facebook, Fusion-io, Google, Huawei, Microsoft, NetApp,
   Samsung, and VMware. Any opinions, findings, and conclusions or
   recommendations expressed in this material are those of the authors and
   do not necessarily reflect the views of NSF or other institutions.
CR [Anonymous], P 6 S OP SYST DES IM
   [Anonymous], 2009, MICRO
   Bairavasundaram L. N., 2007, P ACM SIGMETRICS C M
   Bairavasundaram Lakshmi N., 2008, P 6 USENIX S FIL STO
   Bessey A., 2010, COMMUN
   Best Steve., 2000, JFS OVERVIEW
   Boboila Simona, 2010, P 8 USENIX S FIL STO
   Bonwick J., 2007, Zfs: The last word in file systems
   Buchholz F., 2006, STRUCTURE REISER FIL
   Bugzilla, 2012, KERN BUG TRACK
   Chou Andy, 2001, SOSP, P73, DOI 10.1145/502034.502042
   Coverity, 2011, COV SCAN 2011 OP SOU
   Engler D., 2001, Operating Systems Review, V35, P57, DOI 10.1145/502059.502041
   Fonseca Pedro, 2010, P INT C DEP SYST NET
   FSDEVEL, 2012, LIN FIL DEV LIST
   Ghemawat S., 2003, ACM SIGOPS OPERATING, V37, P29, DOI [DOI 10.1145/945445.945450, DOI 10.1145/1165389.945450, 10.1145/1165389.945450]
   Gunawi HS, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P207
   HOARE CAR, 1974, COMMUN ACM, V17, P549, DOI 10.1145/355620.361161
   Hyojun Kim, 2012, P 10 USENIX S FIL ST
   Jobs Steve, 2006, APPL WORLD WID DEV C
   Jula Horatiu, 2008, P 8 S OP SYST DES IM
   Klein G., 2009, P 22 ACM S OP SYST P
   Li Zhenmin, 2006, P WORKSH ARCH SYST S
   Li Zhenmin, 2005, P 10 EUROPEAN SOFTWA
   LU L., 2013, P 11 USENIX S FIL ST
   Lu S., 2008, P 13 INT C ARCH SUPP
   Marshall Cathy, 2008, P FAST 08
   MASON C., 2007, The Btrfs Filesystem
   Mathur A., 2007, P OTT LIN S OLS 07
   Mckusick M. K., 1986, FSCK UNIX FILE SYSTE
   MCKUSICK MK, 1984, ACM T COMPUT SYST, V2, P181, DOI 10.1145/989.990
   Morrissey Sean., 2010, IOS FORENSIC ANAL IP
   Padioleau Yoann, 2008, P EUROSYS C EUROSYS
   Palix Nicolas, 2011, P 15 INT C ARCH SUPP
   Prabhakaran Vijayan, 2005, P OF THE 20 ACM S ON, V39, P206
   Raymond E., 1999, CATHEDRAL BAZAAR MUS
   Rodeh Ohad, 2012, RJ10501ALM1207004 IB
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   Rubio-Gonzalez Cindy., 2009, P ACM SIGPLAN 2009 C
   Saha Suman, 2011, P WORKSH PROGR LANG
   Sahoo S.K., 2010, ACM IEEE INT C SOFTW
   Sct S.T., 1998, Journaling the linux ext2fs filesystem
   Shvachko K, 2010, IEEE S MASS STOR SYS
   Simons Tony, 2011, 1 GALAXY NEXUS ROM A
   Sullivan M, 1991, P 21 INT S FAULT TOL
   SULLIVAN M, 1992, P 1992 INT S FAULT T, P475
   Sweeney A., 1996, P USENIX ANN TECHN C
   Wang X., 2012, P 10 S OP SYST DES I
   Wang Yin, 2008, P 8 S OP SYST DES IM
   Wikipedia, 2012, IBM J FIL SYST
   Xiong W., 2010, P 9 S OP SYST DES IM
   YANG J., 2004, P 6 S OP SYST DES IM
   Yang Junfeng, 2006, P 7 S OP SYST DES IM
   YIN Z., 2011, P JOINT M EUR SOFTW
NR 54
TC 31
Z9 38
U1 0
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JAN
PY 2014
VL 10
IS 1
AR 3
DI 10.1145/2560012
PG 32
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AB6EC
UT WOS:000331879400003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Ma, A
   Dragga, C
   Arpaci-Dusseau, AC
   Arpaci-Dusseau, RH
   Mckusick, MK
AF Ma, Ao
   Dragga, Chris
   Arpaci-Dusseau, Andrea C.
   Arpaci-Dusseau, Remzi H.
   Mckusick, Marshall Kirk
TI Ffsck: The Fast File-System Checker
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Measurement; Performance; Reliability; File-system checking; file-system
   consistency
ID DESIGN
AB Failures, errors, and bugs can corrupt file systems and cause data loss, despite the presence of journals and similar preventive techniques. While consistency checkers such as fsck can detect corruption and repair a damaged image, they are generally created as an afterthought, to be run only at rare intervals. Thus, checkers operate slowly, causing significant downtime for large scale storage systems.
   We address this dilemma by treating the checker as a key component of the overall file system, rather than a peripheral add-on. To this end, we present a modified ext3 file system, rext3, to directly support the fast file-system checker, ffsck. Rext3 colocates and self-identifies its metadata blocks, removing the need for costly seeks and tree traversals during checking. These modifications allow ffsck to scan and repair the file system at rates approaching the full sequential bandwidth of the underlying device. In addition, we demonstrate that rext3 generally performs competitively with ext3 and exceeds it in handling random reads and large writes. Finally, we apply our principles to FreeBSD's FFS file system and its checker, doing so in a lightweight fashion that preserves the file-system layout while still providing some of the performance gains from ffsck.
C1 [Ma, Ao; Dragga, Chris; Arpaci-Dusseau, Andrea C.; Arpaci-Dusseau, Remzi H.] Univ Wisconsin, Madison, WI 53706 USA.
   [Ma, Ao] EMC Corp, Madison, WI USA.
   [Mckusick, Marshall Kirk] McKusick Com, Berkeley, CA 94709 USA.
C3 University of Wisconsin System; University of Wisconsin Madison; Dell
   Incorporated; Dell EMC
RP Dragga, C (corresponding author), Univ Wisconsin, Dept Comp Sci, 1210 W Dayton St, Madison, WI 53706 USA.
EM dragga@cs.wisc.edu
FU National Science Foundation [CNS-1218405, CSR-1017518, CCF-1016924,
   CNS-1319405]
FX This material is based on work supported by the National Science
   Foundation under the following grants: CNS-1218405, CSR-1017518,
   CCF-1016924, and CNS-1319405, as well as generous donations from EMC,
   Facebook, Fusion-io, Google, Huawei, Microsoft, NetApp, Samsung, and
   VMware. Any opinions, findings, and conclusions or recommendations
   expressed in this material are those of the authors and do not
   necessarily reflect the views of NSF or other institutions.
CR Anderson D., 2003, P 2 USENIX S FIL STO
   [Anonymous], 2009, E2FSCK IS TAKING FOR
   [Anonymous], 2006, LONG DOES IT TAKE FS
   Bairavasundaram L. N., 2007, P ACM SIGMETRICS C M
   Bairavasundaram LN, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P223
   Bartlett W, 2004, IEEE T DEPEND SECURE, V1, P87, DOI 10.1109/TDSC.2004.4
   Baumann R, 2005, IEEE DES TEST COMPUT, V22, P258, DOI 10.1109/MDT.2005.69
   Best Steve., 2000, JFS OVERVIEW
   Bonwick Jeff, 2008, ZFS: The Last Word in File Systems
   Bovet D. P., 2005, Understanding the Linux Kernel: from I/O ports to process management
   Carreira J. C. M., 2012, P EUROSYS C EUROSYS
   Chidambaram Vijay, 2012, P 10 USENIX S FIL ST
   ENGLER D., 2004, P 5 INT C VER MOD CH
   Engler Dawson R., 2001, Proceedings of the 18th ACM Symposium on Operating Systems Principles, P57
   Fryer Daniel, 2012, P 10 USENIX S FIL ST
   Funk R., 2007, FSCK XFS
   Ganger G. R., 1994, Proceedings of the First USENIX Symposium on Operating Systems Design and Implementation (OSDI), P49
   GUNAWI HS, 2008, P 8 S OP SYST DES IM
   Hagmann Robert, 1987, P 11 ACM S OP SYST P
   Henson V., 2006, P IEEE 2 WORKSH HOT
   HITZ D., 1994, P USENIX WINT TECHN
   Keeton K., 2002, ACM SIGOPS EUROPEAN, P93
   Ma A., 2013, P 11 C FIL STOR TECH
   Mathur A., 2007, P OTT LIN S
   May T. C., 1979, IEEE T ELECTRON DEV, V26, P1
   McKusick M. K., 2013, LOGIN, V38, P2
   Mckusick M. K., 1986, FSCK UNIX FILE SYSTE
   McKusick M. K., 2002, P BSDCON 02
   McKusick MarshallKirk., 2005, DESIGN IMPLEMENTATIO
   MCKUSICK MK, 1984, ACM T COMPUT SYST, V2, P181, DOI 10.1145/989.990
   NetApp, 2011, OV WAFL CHECK
   Patterson D. A., 1988, SIGMOD Record, V17, P109, DOI 10.1145/971701.50214
   Peacock JK, 1998, PROCEEDINGS OF THE USENIX 1998 ANNUAL TECHNICAL CONFERENCE, P77
   Piernas J, 2007, IEEE T COMPUT, V56, P267, DOI 10.1109/TC.2007.36
   Rai A., 2008, MAKE EXT3 FSCK WAY F
   Reiser Hans., 2004, ReiserFS
   Rodeh Ohad, 2012, RJ10501ALM1207004 IB
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   Schroeder B., 2009, P JOINT INT C MEAS M
   Schwarz ThomasJ.E., 2004, P 12 ANN M IEEE INT
   Sct S.T., 1998, Journaling the linux ext2fs filesystem
   Sinofsky S., 2012, BUILDING NEXT GENERA
   Smith K. A., 1997, P SIGM C
   Smith KA, 1996, PROCEEDINGS OF THE USENIX 1996 ANNUAL TECHNICAL CONFERENCE, P15
   Stein C. A., 2001, P USENIX ANN TECHN C
   *SUN MICR, 2006, ZFS LAST WORD FIL SY
   Sundaram Rajesh, 2006, The Private Lives of Disk Drives
   Svanberg V., 2009, FSCK TAKES TOO LONG
   Sweeney A., 1996, P USENIX ANN TECHN C
   Swift Michael M., 2003, P 19 ACM S OP SYST P
   The Data Clinic, 2004, HARD DISK FAIL
   YANG J., 2004, P 6 S OP SYST DES IM
   Yang Junfeng, 2006, P 7 S OP SYST DES IM
   Zhang Yupu, 2010, P 8 USENIX C FIL STO
   ZIEGLER JF, 1979, SCIENCE, V206, P776, DOI 10.1126/science.206.4420.776
NR 55
TC 17
Z9 23
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JAN
PY 2014
VL 10
IS 1
AR 2
DI 10.1145/2560011
PG 28
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AB6EC
UT WOS:000331879400002
DA 2024-07-18
ER

PT J
AU Jiang, S
   Ding, XN
   Xu, YH
   Davis, K
AF Jiang, Song
   Ding, Xiaoning
   Xu, Yuehai
   Davis, Kei
TI A Prefetching Scheme Exploiting both Data Layout and Access History on
   Disk
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Algorithms; Performance; Design; Experimentation; Prefetching; spatial
   locality; hard disk; buffer cache
AB Prefetching is an important technique for improving effective hard disk performance. A prefetcher seeks to accurately predict which data will be requested and load it ahead of the arrival of the corresponding requests. Current disk prefetch policies in major operating systems track access patterns at the level of file abstraction. While this is useful for exploiting application-level access patterns, for two reasons file-level prefetching cannot realize the full performance improvements achievable by prefetching. First, certain prefetch opportunities can only be detected by knowing the data layout on disk, such as the contiguous layout of file metadata or data from multiple files. Second, nonsequential access of disk data (requiring disk head movement) is much slower than sequential access, and the performance penalty for mis-prefetching a randomly located block, relative to that of a sequential block, is correspondingly greater.
   To overcome the inherent limitations of prefetching at logical file level, we propose to perform prefetching directly at the level of disk layout, and in a portable way. Our technique, called DiskSeen, is intended to be supplementary to, and to work synergistically with, any present file-level prefetch policies. DiskSeen tracks the locations and access times of disk blocks and, based on analysis of their temporal and spatial relationships, seeks to improve the sequentiality of disk accesses and overall prefetching performance. It also implements a mechanism to minimize mis-prefetching, on a per-application basis, to mitigate the corresponding performance penalty.
   Our implementation of the DiskSeen scheme in the Linux 2.6 kernel shows that it can significantly improve the effectiveness of prefetching, reducing execution times by 20%-60% for microbenchmarks and real applications such as grep, CVS, and TPC-H. Even for workloads specifically designed to expose its weaknesses, DiskSeen incurs only minor performance loss.
C1 [Jiang, Song; Xu, Yuehai] Wayne State Univ, Dept Elect & Comp Engn, Detroit, MI 48202 USA.
   [Ding, Xiaoning] New Jersey Inst Technol, Dept Comp Sci, Newark, NJ 07102 USA.
   [Davis, Kei] Los Alamos Natl Lab, CCS Div, Los Alamos, NM 87545 USA.
C3 Wayne State University; New Jersey Institute of Technology; United
   States Department of Energy (DOE); Los Alamos National Laboratory
RP Jiang, S (corresponding author), Wayne State Univ, Dept Elect & Comp Engn, Detroit, MI 48202 USA.
EM sjiang@wayne.edu
RI Ding, Xiaoning/C-9933-2014
OI Ding, Xiaoning/0000-0002-9947-0437; Davis, Kei/0000-0002-4134-1798
FU National Science Foundation [CCF-0702500, CCF-0845711, CNS-1117772,
   CNS-1217948]; Direct For Computer & Info Scie & Enginr; Division of
   Computing and Communication Foundations [0845711] Funding Source:
   National Science Foundation
FX This research was supported in part by National Science Foundation
   grants CCF-0702500, CCF-0845711, CAREER CCF-0845711, CNS-1117772, and
   CNS-1217948.
CR [Anonymous], P USENIX ANN TECHN C
   BAEK S. H., 2008, P USENIX ANN TECHN C
   Butt A. R., 2005, Performance Evaluation Review, V33, P157, DOI 10.1145/1071690.1064231
   Cao P, 1996, ACM T COMPUT SYST, V14, P311, DOI 10.1145/235543.235544
   Cao P., 1994, P USENIX SUMM TECHN
   Chang F., 1999, P 3 S OP SYST DES IM
   Chen X, 2003, COMPUTER, V36, P63
   Diaz P., 2009, P 36 ANN INT S COMP
   Douceur JR, 1999, PERFORMANCE EVALUATION REVIEW, SPECIAL ISSUE, VOL 27 NO 1, JUNE 1999, P59, DOI 10.1145/301464.301480
   Fraser K, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE GENERAL TRACK, P325
   Ganger GR, 1997, P USENIX ANN TECHN C
   Gill B. S., 2007, P 5 USENIX C FIL STO
   GRIFFIOEN J, 1994, P USENIX SUMM TECHN
   Huang H., 2005, P 20 ACM S OP SYST P, P263, DOI DOI 10.1145/1095810.1095836
   Jiang S, 2005, P 4 USENIX C FIL STO
   Kroeger TM, 2001, USENIX ASSOCIATION PROCEEDINGS OF THE 2001 USENIX ANNUAL TECHNICAL CONFERENCE, P105
   Li ZM, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P173
   Liang S., 2007, P 27 IEEE INT C DIST
   MCKUSICK MK, 1984, ACM T COMPUT SYST, V2, P181, DOI 10.1145/989.990
   MOWRY TC, 1996, P 2 USENIX S OP SYST
   MPI-IO, 2013, MPI 2 EXT MESS PASS
   PAI R, 2004, P LIN S
   Papathanasiou A. E., 2005, P 10 WOKRSH HOT TOP
   Patterson R. H., 1995, Operating Systems Review, V29, P79, DOI 10.1145/224057.224064
   Schindler J, 2000, PERF E R SI, V28, P112, DOI 10.1145/345063.339397
   SCHLOSSER SW, 2005, P 4 USENIX C FIL STO
   Schmuck F., 2002, P 1 USENIX C FIL STO
   Sehindler J., 2002, P 1 USENIX C FILE ST
   Smith A. J., 1978, ACM Transactions on Database Systems, V3, P223, DOI 10.1145/320263.320276
   Tomkins A., 1997, Performance Evaluation Review, V25, P100, DOI 10.1145/258623.258680
   Vogels W, 1999, OPERATING SYSTEMS REVIEW, VOL 33, NO 5, DECEMBER 1999, P93, DOI 10.1145/319344.319158
   WebStone, 2013, WEBSTONE BENCHM WEB
   Xu Yuehai, 2011, P 9 USENIX C FIL STO
   Zhang Xuefeng, 2010, Proceedings of the Third International Conference on Advances in Circuits, Electronics and Micro-Electronics (CENICS 2010), P1, DOI 10.1109/CENICS.2010.8
NR 34
TC 35
Z9 38
U1 1
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2013
VL 9
IS 3
AR 10
DI 10.1145/2508010
PG 23
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 216XX
UT WOS:000324321200004
OA Bronze
DA 2024-07-18
ER

PT J
AU Meyer, DT
   Bolosky, WJ
AF Meyer, Dutch T.
   Bolosky, William J.
TI A Study of Practical Deduplication
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Measurement; Performance; Deduplication; Windows; filesystem; data;
   study
AB We collected file system content data from 857 desktop computers at Microsoft over a span of 4 weeks. We analyzed the data to determine the relative efficacy of data deduplication, particularly considering whole-file versus block-level elimination of redundancy. We found that whole-file deduplication achieves about three quarters of the space savings of the most aggressive block-level deduplication for storage of live file systems, and 87% of the savings for backup images. We also studied file fragmentation, finding that it is not prevalent, and updated prior file system metadata studies, finding that the distribution of file sizes continues to skew toward very large unstructured files.
C1 [Meyer, Dutch T.; Bolosky, William J.] Univ British Columbia, Microsoft Res, Vancouver, BC V5Z 1M9, Canada.
C3 University of British Columbia
RP Meyer, DT (corresponding author), Univ British Columbia, Microsoft Res, Vancouver, BC V5Z 1M9, Canada.
EM dmeyer@cs.ubc.ca; bolosky@microsoft.com
FU Microsoft
FX Microsoft as whole funded and enabled this research.
CR AGRAWAL N., 2007, P 5 USENIX C FIL STO
   [Anonymous], 2009, P USENIX ANN TECHNIC
   [Anonymous], P 7 USENIX C FIL STO
   [Anonymous], 1981, TRCSE0301 HARV U CTR
   [Anonymous], 1997, WINDOWS NT FILE SYST
   [Anonymous], 2008, FAST
   [Anonymous], 2011, PROC USENIX C FILE S
   [Anonymous], 2010, P 8 USENIX C FIL STO
   BACKUPREAD, 2010, BACKUPREAD FUNCT
   Bhagwat D., 2009, P 17 IEEE INT S MOD
   BLOOM BH, 1970, COMMUN ACM, V13, P422, DOI 10.1145/362686.362692
   BOLOSKY WJ, 2000, P 4 USENIX WIND SYST
   DORWARD S., 2002, P 1 USENIX C FIL STO
   Douceur J. R., 1999, P ACM SIGMETRICS INT
   Dubnicki C., 2009, P 7 USENIX C FIL STO
   HUANG H., 2005, P 20 ACM S OP SYST P
   KULKARNI P., 2004, P USENIX ANN TECHN C
   Lillibridge M., 2009, P 7 USENIX C FIL STO
   Mathur Avantika, 2007, P LIN S, V2
   MILLER D. R., 2009, STORAGE EC 4 PRINCIP
   MILLER E., 2009, P SYSTOR ISR EXP SYS
   MS ATIME, 2010, DIS LAST ACC TIM WIN
   MS FILESYSTEM, 2010, FIL SYST
   MURPHY N., 2009, P 12 WORKSH HOT TOP
   POLICRONIADES C., 2004, P USENIX ANN TECHN C
   Rivest R., 1992, MD5 MESSAGE DIGEST A
   SATYANARAYANAN M., 1981, P 8 ACM S OP SYST PR
   SCHEDULED TASKS, 2010, DESCR SCHED TASKS WI
   SELTZER M., 1997, P 1997 ACM SIGMETRIC
   Sweeney A., 1996, P USENIX ANN TECHN C
   UNGUREANU E., 2010, P 8 USENIX C FIL STO
   VOGELS W., 1999, P 17 ACM S OP SYST P
   VSS, 2010, VOL SHAD COP SERV
NR 33
TC 198
Z9 215
U1 0
U2 22
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JAN
PY 2012
VL 7
IS 4
AR 14
DI 10.1145/2078861.2078864
PG 20
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 990LP
UT WOS:000307632700003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Pang, SJ
   Deng, YH
   Zhang, GX
   Zhou, Y
   Huang, YQ
   Qin, X
AF Pang, Shujie
   Deng, Yuhui
   Zhang, Genxiong
   Zhou, Yi
   Huang, Yaoqin
   Qin, Xiao
TI PSA-Cache: A Page-state-aware Cache Scheme for Boosting 3D NAND Flash
   Performance
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE 3D NAND flash; copyback; parity symmetry; cache management
ID GARBAGE COLLECTION; STORAGE; OPTIMIZATION
AB Garbage collection (GC) plays a pivotal role in the performance of 3D NAND flash memory, where Copyback has been widely used to accelerate valid page migration during GC. Unfortunately, copyback is constrained by the parity symmetry issue: data read from an odd/even page must be written to an odd/even page. After migrating two odd/even consecutive pages, a free page between the two migrated pages will be wasted. Such wasted pages noticeably lower free space on flash memory and cause extra GCs, thereby degrading solidstate-disk (SSD) performance. To address this problem, we propose a page-state-aware cache scheme called PSA-Cache, which prevents page waste to boost the performance of NAND Flash-based SSDs. To facilitate making write-back scheduling decisions, PSA-Cache regulates write-back priorities for cached pages according to the state of pages in victim blocks. With high write-back-priority pages written back to flash chips, PSA-Cache effectively fends off page waste by breaking odd/even consecutive pages in subsequent garbage collections. We quantitatively evaluate the performance of PSA-Cache in terms of the number of wasted pages, the number of GCs, and response time. We compare PSA-Cache with two state-of-the-art schemes, GCaR and TTflash, in addition to a baseline scheme LRU. The experimental results unveil that PSA-Cache outperforms the existing schemes. In particular, PSA-Cache curtails the number of wasted pages of GCaR and TTflash by 25.7% and 62.1%, respectively. PSA-Cache immensely cuts back the number of GC counts by up to 78.7% with an average of 49.6%. Furthermore, PSA-Cache slashes the average write response time by up to 85.4% with an average of 30.05%.
C1 [Pang, Shujie; Deng, Yuhui; Zhang, Genxiong; Huang, Yaoqin] Jinan Univ, Dept Comp Sci, Guangzhou 510632, Guangdong, Peoples R China.
   [Zhou, Yi] Columbus State Univ, TSYS Sch Comp Sci, 4225 Univ Ave, Columbus, GA 31907 USA.
   [Qin, Xiao] Auburn Univ, Dept Comp Sci & Software Engn, Auburn, AL 36849 USA.
C3 Jinan University; University System of Georgia; Columbus State
   University; Auburn University System; Auburn University
RP Deng, YH (corresponding author), Jinan Univ, Dept Comp Sci, Guangzhou 510632, Guangdong, Peoples R China.
EM p_shujie@163.com; tyhdeng@jnu.edu.cn; zgx787839110@163.com;
   zhou_yi@columbusstate.edu; 329671859@qq.com; xqin@auburn.edu
OI , Yi/0000-0002-1460-322X; Qin, Xiao/0000-0002-8345-3587; Deng,
   Yuhui/0000-0002-1522-8943; Zhang, Genxiong/0000-0003-2128-9838; Pang,
   Shujie/0000-0003-1741-1319
FU National Natural Science Foundation of China [62072214, 61572232];
   Guangdong Basic and Applied Basic Research Foundation [2021B1515120048];
   Industry University Research Collaboration Project of Zhuhai
   [ZH22017001210048PWC]; Open Project Program of Wuhan National Laboratory
   for Optoelectronics [2020WNLOKF006]
FX This work is sponsored by the National Natural Science Foundation of
   China under Grants No. 62072214 and No. 61572232, Guangdong Basic and
   Applied Basic Research Foundation under Grant No. 2021B1515120048, the
   Industry University Research Collaboration Project of Zhuhai under Grant
   No. ZH22017001210048PWC, and the Open Project Program of Wuhan National
   Laboratory for Optoelectronics under Grant No. 2020WNLOKF006.
CR [Anonymous], 2011, ACM 11 P INT C SUPER
   Chang L.-P., 2004, ACM Trans. on Embedded Computing Syst, V3, P837
   Deng YH, 2011, J SYST ARCHITECT, V57, P214, DOI 10.1016/j.sysarc.2010.12.003
   Gao CM, 2020, IEEE T COMPUT AID D, V39, P2230, DOI 10.1109/TCAD.2019.2949541
   Hong D., 2019, IEEE NON-VOLATILE ME
   Hu Xiameng., 2015, USENIX ANN TECHNICAL, P57
   Hu Y, 2015, PROCEEDINGS OF THE 2015 IEEE INTERNATIONAL CONFERENCE ON NETWORKING, ARCHITECTURE AND STORAGE (NAS), P54, DOI 10.1109/NAS.2015.7255206
   Jang J, 2009, 2009 SYMPOSIUM ON VLSI TECHNOLOGY, DIGEST OF TECHNICAL PAPERS, P192
   Jeong J, 2012, IEEE T CONSUM ELECTR, V58, P470, DOI 10.1109/TCE.2012.6227449
   Kim H, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P239, DOI 10.1145/1378600.1378627
   Laboratory for Advanced System Software, 2007, UMASS TRACE REPOSITO
   Lee J, 2011, INT SYM PERFORM ANAL, P12, DOI 10.1109/ISPASS.2011.5762711
   Liu CY, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P67
   Manuel Antonio, 2015, U.S. Patent, Patent No. 9286989
   Ming-Chang Yang, 2014, 2014 International Conference on Smart Computing (SMARTCOMP), P66, DOI 10.1109/SMARTCOMP.2014.7043841
   Oh E. C., 2015, uS Patent, Patent No. [8,964,481, 8964481]
   ONFI, 2021, OP NAND FLASH INT SP
   Samsung, SAMSUNG 980 PRONVME
   Samsung, SAMSUNG K9XXG08UXA N
   Shim H, 2010, IEEE S MASS STOR SYS
   Shu JW, 2020, IEEE T PARALL DISTR, V31, P2923, DOI 10.1109/TPDS.2020.3006655
   SNIA IOTTA, 2007, MSR CAMBRIDGE TRACES
   TOSHIBA, 2013, NAND MEMORY TOGGLE D
   TOSHIBA, 2016, 3D FLASH MEMORY TOGG
   Wang SZ, 2021, IEEE T COMPUT AID D, V40, P430, DOI 10.1109/TCAD.2020.3001262
   Woo Tae Chang, 2014, International Journal of Electrical Energy, V2, P13
   Wu F, 2018, DES AUT CON, DOI [10.1145/3195970.3196051, 10.1109/ICOPS35962.2018.9575531]
   Wu SZ, 2020, IEEE T COMPUT AID D, V39, P4587, DOI 10.1109/TCAD.2020.2974346
   Wu SZ, 2017, IEEE T PARALL DISTR, V28, P2852, DOI 10.1109/TPDS.2017.2692757
   Yan SQ, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P15
   Yu Cai, 2013, Intel Technology Journal, V17, P140
   Zhang GX, 2023, IEEE T COMPUT AID D, V42, P2336, DOI 10.1109/TCAD.2022.3214679
   Zhang WH, 2020, IEEE T PARALL DISTR, V31, P332, DOI 10.1109/TPDS.2019.2934458
   Zhou K, 2020, IEEE T PARALL DISTR, V31, P2496, DOI 10.1109/TPDS.2020.2994075
   Zhou Y, 2017, ACM T STORAGE, V13, DOI 10.1145/3051123
NR 35
TC 1
Z9 1
U1 1
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAY
PY 2023
VL 19
IS 2
AR 18
DI 10.1145/3574324
PG 27
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D7OU8
UT WOS:000970592600009
DA 2024-07-18
ER

PT J
AU Zheng, JW
   Li, ZH
   Qiu, YH
   Lin, H
   Xiao, H
   Li, Y
   Liu, YH
AF Zheng, Jianwei
   Li, Zhenhua
   Qiu, Yuanhui
   Lin, Hao
   Xiao, He
   Li, Yang
   Liu, Yunhao
TI WebAssembly-based Delta Sync for Cloud Storage Services
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article; Proceedings Paper
CT ACM Symposium on Operating Systems Principles (SOSP)
CY OCT 26-29, 2021
CL ELECTR NETWORK
SP Assoc Comp Machinery
DE Cloud storage service; delta synchronization; web browser; WebAssembly
AB Delta synchronization (sync) is crucial to the network-level efficiency of cloud storage services, especially when handling large files with small increments. Practical delta sync techniques are, however, only available for PC clients and mobile apps, but notweb browsers-the most pervasive and OS-independent access method. To bridge this gap, prior work concentrates on either reversing the delta sync protocol or utilizing the native client, all striving around the tradeoffs among efficiency, applicability, and usability and thus forming an "impossible triangle." Recently, we note the advent of WebAssembly (WASM), a portable binary instruction format that is efficient in both encoding size and load time. In principle, the unique advantages of WASM can make web-based applications enjoy near-native runtime speed without significant cloud-side or clientside changes. Thus, we implement a straightforward WASM-based delta sync solution, WASMrsync, finding its quasi-asynchronous working manner and conventional In-situ Separate Memory Allocation greatly increase sync time and memory usage. To address them, we strategically devise sync-async code decoupling and streaming compilation, together with Informed In-place File Construction. The resulting solution, WASMrsync+, achieves comparable sync time as the state-of-the-art (most efficient) solution with nearly only half of memory usage, letting the "impossible triangle" reach a reconciliation.
C1 [Zheng, Jianwei; Li, Zhenhua; Qiu, Yuanhui; Lin, Hao; Xiao, He; Li, Yang; Liu, Yunhao] Tsinghua Univ, Beijing, Peoples R China.
C3 Tsinghua University
RP Li, ZH (corresponding author), Tsinghua Univ, Beijing, Peoples R China.
EM zhengjw19@mails.tsinghua.edu.cn; lizhenhua1983@gmail.com;
   qiuyh0924@163.com; linhaomails@gmail.com; xiaoh16@gmail.com;
   liyang14thu@gmail.com; yunhaoliu@gmail.com
FU National Natural Science Foundation of China (NSFC) [61822205, 61902211,
   62202266]; Microsoft Research Asia
FX This work is supported in part by the National Natural Science
   Foundation of China (NSFC) under grants 61822205, 61902211 and 62202266,
   as well as Microsoft Research Asia.
CR Aappleby, 2021, MURM HASH FUNCT
   Alakuijala, 2016, ARXIV
   Aliyun.com, 2021, AL ECS EL COMP SERV
   Aliyun.com, 2021, AL OSS OBJ STOR SERV
   [Anonymous], 1992, RFC1321
   [Anonymous], 2010, FAST
   asmjs.org, 2021, ASMJS STRICT SUBS JA
   Aumasson J.-P., 2012, INT C CRYPT IND, P489
   Banga G., 1997, Proceedings of the USENIX 1997 Annual Technical Conference, P289
   Basques, 2021, READING FILES JAVASC
   Bocchi E, 2015, IEEE INT CONF CL NET, P106, DOI 10.1109/CloudNet.2015.7335291
   Budiu Raluca, 2019, MENTAL MODELS CLOUD
   Cui Y, 2015, MOBICOM '15: PROCEEDINGS OF THE 21ST ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P592, DOI 10.1145/2789168.2790094
   Denis Frank, 2021, JAVASCRIPT IMPLEMENT
   Deutsch Peter., 1996, Zlib compressed data format specification version 3.3
   Dokan, 2021, WEB WORK
   Dokan, 2021, DOK US MOD FIL SYST
   Drago I., 2013, P IMC, P205
   Drago I., 2012, the 2012 ACM conference, P481, DOI [10.1145/2398776.2398827, DOI 10.1145/2398776.2398827]
   Erkoc Mehmet Fatih, 2011, P FOE
   Finney R, 2018, CANCER INFORM, V17, DOI 10.1177/1176935118771972
   Goltzsche D, 2019, MIDDLEWARE'19: PROCEEDINGS OF THE 2019 MIDDLEWARE'19: 20TH INTERNATIONAL MIDDLEWARE CONFERENCE, P123, DOI 10.1145/3361525.3361541
   Google, 2021, NAT CLIENT GOOGL CHR
   Google, 2021, INTR CHROM 61
   Housel B. C., 1996, ACM MOBICOM 96, P108, DOI [10.1145/236387.236416, DOI 10.1145/236387.236416]
   Hunt J.W., 1976, An Algorithm for Differential File Comparison
   Hunt JJ, 1996, LECT NOTES COMPUT SC, V1167, P49, DOI 10.1007/BFb0023080
   jamesruan, 2021, JAV VERS SPOOKYHASH
   Jamil D., 2011, Int. J. Eng. Sci. Technol., V3, P2672
   Jenkins Bob, 2021, SPOOKYHASH 128 BIT N
   Jinlong E, 2018, IEEE T PARALL DISTR, V29, P56, DOI 10.1109/TPDS.2017.2750161
   Kim W., 2009, P 7 INT C ADV MOBILE, P2
   Korn DG, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE GENERAL TRACK, P219
   Letz Stephane, 2017, COMPILING FAUST AUDI
   Li Z., 2016, CONTENT DISTRIBUTION
   Li Zhen, 2016, 2016 19th International Symposium on Wireless Personal Multimedia Communications (WPMC), P287
   Li ZH, 2014, PROCEEDINGS OF THE 2014 ACM INTERNET MEASUREMENT CONFERENCE (IMC'14), P115, DOI 10.1145/2663716.2663747
   Li ZH, 2013, TSINGHUA SCI TECHNOL, V18, P286, DOI 10.1109/TST.2013.6522587
   Lillibridge Mark., 2009, PROCCEDINGS 7 C FILE, P111
   mozilla.org, 2021, MAK WEBASSEMBLY EV F
   mozilla.org, 2021, US WEBASSEMBLY
   mozilla.org, 2021, US FIL WEB APPL
   Noll Landon Curt, 2021, FNV HASH
   Pike Geoff, 2021, CITYHASH
   Pour Rezaei Parisa, 2015, THESIS TAMPERE U TEC
   samba.org, 2021, RSYNC WEB SIT
   seafile.com, 2021, SEAF ENT FIL SYNC SH
   Tridgell A., 1996, The rsync algorithm
   Tridgell Andrew, 1999, THESIS
   VAMPIRE, 2021, EBACS ECRYPT BENCHMA
   WebAssembly, 2021, WEBASSEMBLYCOMPILE U
   WebAssembly, 2021, WEBASSEMBLY NEW PORT
   Wikipedia, 2021, INTR V8 JAVASCRIPT E
   Wikipedia, 2021, DELT ENC
   Xia W., 2011, SILO SIMILARITY LOCA, P26
   Xia W, 2016, PROCEEDINGS OF USENIX ATC '16: 2016 USENIX ANNUAL TECHNICAL CONFERENCE, P101
   Xia W, 2016, P IEEE, V104, P1681, DOI 10.1109/JPROC.2016.2571298
   Xia W, 2015, IEEE T COMPUT, V64, P1162, DOI 10.1109/TC.2014.2308181
   Xiao H, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P155
   Xiao He, 2017, P HOTSTORAGE, P1
   Zakai A, 2018, COMPUT SCI ENG, V20, P11, DOI 10.1109/MCSE.2018.110150345
   Zakai Alon, 2021, EMSCRIPTEN COMPLETE
   Zhang QL, 2017, INT CON DISTR COMP S, P264, DOI 10.1109/ICDCS.2017.77
   Zhenhua Li, 2013, Middleware 2013. ACM/IFIP/USENIX 14th International Middleware Conference. Proceedings: LNCS 8275, P307, DOI 10.1007/978-3-642-45065-5_16
NR 64
TC 1
Z9 1
U1 0
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2022
VL 18
IS 3
AR 24
DI 10.1145/3502847
PG 31
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 4X9BK
UT WOS:000861131000006
OA Bronze
DA 2024-07-18
ER

PT J
AU Zhang, BQ
   Du, DHC
AF Zhang, Baoquan
   Du, David H. C.
TI NVLSM: A Persistent Memory Key-Value Store Using Log-Structured Merge
   Tree with Accumulative Compaction
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article; Proceedings Paper
CT 14th USENIX Symposium on Operating Systems Design and Implementation
   (OSDI)
CY NOV 04-06, 2020
CL ELECTR NETWORK
SP USENIX Assoc, Alibaba Grp, Amazon, ANT Grp, Alipay, Apple, ByteDance, Facebook, Google, Microsoft, VMware, Oracle, Two Sigma, ACM Queue, DMTF, FreeBSD Fdn
DE Non-volatile memory; key-value store; log-structured merge tree
ID NONVOLATILE; PERFORMANCE; INDEX
AB Computer systems utilizing byte-addressable Non-Volatile Memory (NVM) asmemory/storage can provide low-latency data persistence. The widely used key-value stores using Log-Structured Merge Tree (LSMTree) are still beneficial for NVMsystems in aspects of the space and write efficiency. However, the significant write amplification introduced by the leveled compaction of LSM-Tree degrades the write performance of the key-value store and shortens the lifetime of the NVM devices. The existing studies propose new compaction methods to reduce write amplification. Unfortunately, they result in a relatively large read amplification. In this article, we propose NVLSM, a key-value store for NVM systems using LSM-Tree with new accumulative compaction. By fully utilizing the byte-addressability of NVM, accumulative compaction uses pointers to accumulate data into multiple floors in a logically sorted run to reduce the number of compactions required. We have also proposed a cascading searching scheme for reads among the multiple floors to reduce read amplification. Therefore, NVLSM reduces write amplification with small increases in read amplification. We compare NVLSM with key-value stores using LSM-Tree with two other compaction methods: leveled compaction and fragmented compaction. Our evaluations show that NVLSM reduces write amplification by up to 67% compared with LSM-Tree using leveled compaction without significantly increasing the read amplification. In write-intensive workloads, NVLSM reduces the average latency by 15.73%-41.2% compared to other key-value stores.
C1 [Zhang, Baoquan; Du, David H. C.] Univ Minnesota Twin Cities, 319-15th Ave SE, Minneapolis, MN 55455 USA.
C3 University of Minnesota System; University of Minnesota Twin Cities
RP Zhang, BQ (corresponding author), Univ Minnesota Twin Cities, 319-15th Ave SE, Minneapolis, MN 55455 USA.
EM zhan4281@umn.edu; du@umn.edu
FU NSF I/UCRC Center Research in Intelligent Storage; NSF [1439622,
   1525617, 1812537]; Division Of Computer and Network Systems; Direct For
   Computer & Info Scie & Enginr [1812537] Funding Source: National Science
   Foundation
FX Thiswork was partially supported by NSF I/UCRC Center Research in
   Intelligent Storage and NSF awards 1439622, 1525617, and 1812537.
CR [Anonymous], 2011, HotOS
   Arulraj J, 2018, PROC VLDB ENDOW, V11, P553, DOI 10.1145/3164135.3164147
   Arulraj J, 2016, PROC VLDB ENDOW, V10, P337
   Arulraj J, 2015, SIGMOD'15: PROCEEDINGS OF THE 2015 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P707, DOI 10.1145/2723372.2749441
   Atikoglu Berk, 2012, Performance Evaluation Review, V40, P53, DOI 10.1145/2318857.2254766
   Cao ZC, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P129
   Chakrabarti DR, 2014, ACM SIGPLAN NOTICES, V49, P433, DOI [10.1145/2660193.2660224, 10.1145/2714064.2660224]
   Chan HHW, 2018, PROCEEDINGS OF THE 2018 USENIX ANNUAL TECHNICAL CONFERENCE, P1007
   Chatzistergiou A, 2015, PROC VLDB ENDOW, V8, P497, DOI 10.14778/2735479.2735483
   Chazelle B, 1986, ALGORITHMICA, V1, P133, DOI 10.1007/BF01840440
   Chen SM, 2015, PROC VLDB ENDOW, V8, P786, DOI 10.14778/2752939.2752947
   Chilimbi TM, 1999, ACM SIGPLAN NOTICES, V34, P13, DOI 10.1145/301631.301635
   Cooper Brian F., 2010, P 1 ACM S CLOUD COMP, P143, DOI [DOI 10.1145/1807128.1807152, 10.1145/1807128.1807152]
   Dayan N, 2018, INT CONF MANAGE DATA, P505, DOI 10.1145/3183713.3196927
   Dayan N, 2017, SIGMOD'17: PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P79, DOI 10.1145/3035918.3064054
   Debnath Biplob., 2016, ACM SIGOPS Operating Systems Review, V49, P18, DOI DOI 10.1145/2883591.2883597
   DeCandia Giuseppe, 2007, Operating Systems Review, V41, P205, DOI 10.1145/1323293.1294281
   Dong Siying, 2017, CIDR, V3
   Facebook, 2015, RocksDB
   Ferreira AP, 2010, DES AUT TEST EUROPE, P914
   Ghemawat S., 2011, LevelDB
   Google, 2017, LEVELDB BENCHM
   Hady FT, 2017, P IEEE, V105, P1822, DOI 10.1109/JPROC.2017.2731776
   HarEl Nadav, 2017, SCYLLA COMPACTION ST
   Harter Tyler, 2014, P 12 USENIX C FIL ST, V14
   He HJ, 2017, INT BIODETER BIODEGR, V119, P520, DOI 10.1016/j.ibiod.2016.10.007
   Hwang D, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P187
   HyperDex, 2016, HYPERLEVELDB FORK LE
   Intel, 2018, PMEMKV
   Intel, 2017, T PERS MEM DEV KITS
   Intel, 2016, PERS MEM DEV KIT
   Izraelevitz J, 2016, ACM SIGPLAN NOTICES, V51, P427, DOI 10.1145/2954679.2872410
   Kaiyrakhmet O, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P191
   Kannan S, 2018, PROCEEDINGS OF THE 2018 USENIX ANNUAL TECHNICAL CONFERENCE, P993
   Kuszmaul Bradley C, 2014, COMPARISON FRACTAL T
   Lakshman Avinash, 2010, Operating Systems Review, V44, P35, DOI 10.1145/1773912.1773922
   Lee BC, 2009, CONF PROC INT SYMP C, P2, DOI 10.1145/1555815.1555758
   Lee SK, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P257
   Li J., 2017, Nvmrocks: Rocksdb on non-volatile memory systems
   Li YA, 2009, PROC INT CONF DATA, P1303, DOI 10.1109/ICDE.2009.226
   linux, 2017, CLFLUSH
   linux, 2017, MFENC
   Lu LY, 2017, ACM T STORAGE, V13, DOI 10.1145/3033273
   Lu Y., 2017, IEEE T PARALL DISTR
   McKenney Paul E, 2005, LINUX J, V136, P2
   Mei F, 2018, PROCEEDINGS OF THE 2018 ACM SYMPOSIUM ON CLOUD COMPUTING (SOCC '18), P477, DOI 10.1145/3267809.3267829
   Memaripour A, 2017, PROCEEDINGS OF THE TWELFTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS (EUROSYS 2017), P499, DOI 10.1145/3064176.3064215
   Mittal S, 2016, IEEE T PARALL DISTR, V27, P1537, DOI 10.1109/TPDS.2015.2442980
   Mogul JeffreyC., 2009, Operating system support for nvm hybrid main memory
   Moraru Iulian., 2013, Proceedings of the First ACM SIGOPS Conference on Timely Results in Operating Systems, P1
   Mumford Chris, 2011, LEVELDB IMPLEMENTATI
   Nam M, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P31
   Niu SJ, 2015, 2015 13TH IEEE INTERNATIONAL CONFERENCE ON DATA ENGINEERING WORKSHOPS (ICDEW), P149, DOI 10.1109/ICDEW.2015.7129568
   ONeil P, 1996, ACTA INFORM, V33, P351, DOI 10.1007/s002360050048
   Oukid I, 2016, SIGMOD'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P371, DOI 10.1145/2882903.2915251
   PUGH W, 1990, COMMUN ACM, V33, P668, DOI 10.1145/78973.78977
   Raju P, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P497, DOI 10.1145/3132747.3132765
   Raoux S, 2014, MRS BULL, V39, P703, DOI 10.1557/mrs.2014.139
   Ren JL, 2015, PROCEEDINGS OF THE 48TH ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE (MICRO-48), P672, DOI 10.1145/2830772.2830802
   Ren K, 2017, PROC VLDB ENDOW, V10, P2037, DOI 10.14778/3151106.3151108
   Sears Russell, 2012, P ACM SIGMOD INT C M, P217
   Smullen CW IV, 2011, INT S HIGH PERF COMP, P50, DOI 10.1109/HPCA.2011.5749716
   Tse J, 1998, IEEE T COMPUT, V47, P509, DOI 10.1109/12.677225
   TULLSEN DM, 1993, CONF PROC INT SYMP C, P278, DOI 10.1145/173682.165163
   Wang J, 2013, P INT COMP SOFTW APP, P240, DOI 10.1109/COMPSAC.2013.40
   Wei W, 2017, ACM T ARCHIT CODE OP, V14, DOI 10.1145/3106340
   Williams Dan, 2018, PERSISTENT MEMORY
   Wu XN, 2015, METHODS MOL BIOL, V1306, P71, DOI 10.1007/978-1-4939-2648-0_5
   Xu J, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P323
   Yabandeh Maysam, 2018, COMPACTION ROCKSDB
   Yahoo, 2010, COR WORKL YCSB
   Yang Jun, 2015, P 13 USENIX C FIL ST, P167
   Yao Ting, 2017, P 33 INT C MASS STOR, P1
   Yue YL, 2017, IEEE T PARALL DISTR, V28, P961, DOI 10.1109/TPDS.2016.2609912
   Zhang YY, 2015, ACM SIGPLAN NOTICES, V50, P3, DOI [10.1145/2694344.2694370, 10.1109/OECC.2015.7340093]
   Zuo PF, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P461
NR 76
TC 11
Z9 11
U1 1
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2021
VL 17
IS 3
AR 23
DI 10.1145/3453300
PG 26
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA YY2GJ
UT WOS:000754611000008
DA 2024-07-18
ER

PT J
AU Ganesan, A
   Alagappan, R
   Arpaci-Dusseau, AC
   Arpaci-Dusseau, RH
AF Ganesan, Aishwarya
   Alagappan, Ramnatthan
   Arpaci-Dusseau, Andrea C.
   Arpaci-Dusseau, Remzi H.
TI Strong and Efficient Consistency with Consistency-aware Durability
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Consistency models; durability; replication; persistence
AB We introduce consistency-aware durability or CAD, a new approach to durability in distributed storage that enables strong consistency while delivering high performance. We demonstrate the efficacy of this approach by designing cross-client monotonic reads, a novel and strong consistency property that provides monotonic reads across failures and sessions in leader-based systems; such a property can be particularly beneficial in geo-distributed and edge-computing scenarios. We build ORCA, a modified version of ZooKeeper that implements CAD and cross-client monotonic reads. We experimentally show that ORCA provides strong consistency while closely matching the performance of weakly consistent ZooKeeper. Compared to strongly consistent ZooKeeper, ORCA provides significantly higher throughput (1.8 3.3x) and notably reduces latency, sometimes by an order of magnitude in geo-distributed settings. We also implement CAD in Redis and show that the performance benefits are similar to that of CAD'S implementation in ZooKeeper.
C1 [Ganesan, Aishwarya; Alagappan, Ramnatthan; Arpaci-Dusseau, Andrea C.; Arpaci-Dusseau, Remzi H.] Univ Wisconsin, Madison, WI 53706 USA.
   [Ganesan, Aishwarya; Alagappan, Ramnatthan; Arpaci-Dusseau, Andrea C.; Arpaci-Dusseau, Remzi H.] 1210 W Dayton St, Madison, WI 53706 USA.
   [Ganesan, Aishwarya; Alagappan, Ramnatthan; Arpaci-Dusseau, Andrea C.; Arpaci-Dusseau, Remzi H.] VMware Res, 3425 Hillview Ave, Palo Alto, CA 94304 USA.
C3 University of Wisconsin System; University of Wisconsin Madison; VMware,
   Inc.
RP Ganesan, A (corresponding author), Univ Wisconsin, Madison, WI 53706 USA.; Ganesan, A (corresponding author), 1210 W Dayton St, Madison, WI 53706 USA.; Ganesan, A (corresponding author), VMware Res, 3425 Hillview Ave, Palo Alto, CA 94304 USA.
EM ag@cs.wisc.edu; ra@cs.wisc.edu; dusseau@cs.wisc.edu; remzi@cs.wisc.edu
OI Alagappan, Ramnatthan/0000-0001-9911-4208; Arpaci-Dusseau,
   Andrea/0000-0001-8618-2738
FU NSF [CNS-1421033, CNS-1763810, CNS-1838733]; DOE [DESC0014935]; VMware;
   Facebook fellowship
FX This material was supported by funding from NSF grants CNS-1421033,
   CNS-1763810 and CNS-1838733, DOE grant DESC0014935, and VMware.
   Aishwarya Ganesan is supported by a Facebook fellowship. Any opinions,
   findings, and conclusions or recommendations expressed in this material
   are those of the authors and may not reflect the views of NSF, DOE, or
   any other institutions. This article is an extended version of a FAST
   '20 paper by Ganesan et al. [17]. The additional material here includes
   a description of how we implement Cad in Redis (another leader-based
   system), new experiments to evaluate the performance of Cad in Redis,
   new graphs showing the performance for additional YCSB workloads, more
   figures explaining the write path in Cad and the geo-distributed
   experimental setup, additional discussions related to asynchronous
   persistence and read path in followers, and many other small edits and
   improvements.
CR ALAGAPPAN R., 2016, P 12 USENIX C OP SYS
   Alagappan Ramnatthan, 2018, P 13 USENIX C OP SYS
   [Anonymous], 2014, LOGIN
   [Anonymous], 2014, ATC
   [Anonymous], ZooKeeper
   [Anonymous], 2013, P 24 ACM S OP SYST P
   [Anonymous], 2018, P 16 USENIX C FIL ST
   Apache, ZOOKEEPER LEAD ACT
   Apache, ZOOKEEPER GUAR PROP
   Apache, ZOOKEEPER CONF PAR
   Apache, ZOOKEEPER OV
   Bailis Peter, 2013, P ACM SIGMOD INT C M
   Bolosky William J., 2011, P 8 INT S NETW SYST
   Burns Randal C., 2001, P INT C DISTR COMP S
   Cipar James, 2012, P EUROSYS C EUROSYS
   Cooper B. F., 2010, P ACM S CLOUD COMP S
   Corbet Jonathan, 2009, SYNC
   de Mesquita JB, 2010, PA STUD HUM RIGHTS, P104
   Demers Alan, 1987, P 26 ACM S PRINC DIS
   Ganesan Aishwarya, 2020, P 18 USENIX C FIL ST
   Geng Yilong, 2018, P 15 S NETW SYST DES
   Gray C., 1989, P 12 ACM S OP SYST P, P1, DOI [10.1145/74850.74870, DOI 10.1145/74850.74870]
   Guerraoui Rachid, 2016, P 12 USENIX C OP SYS
   HERLIHY MP, 1990, ACM T PROGR LANG SYS, V12, P463, DOI 10.1145/78969.78972
   Ingo Henrik, 2020, DISCUSSION H INGO
   LAMPORT L, 1982, ACM T PROGR LANG SYS, V4, P382, DOI 10.1145/357172.357176
   Lee Collin, 2015, P 25 ACM S OP SYST P
   Liskov Barbara, 2012, MITCSAILTR2012021
   Lloyd W, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P401
   Lu Haonan, 2015, P 25 ACM S OP SYST P
   Mehdi Syed Akbar, 2017, P 14 S NETW SYST DES
   MongoDB, 2020, MONGODB REPLICATION
   MongoDB, 2020, MONGODB READ PREFERE
   MongoDB, 2019, READ LINEARIZABLE
   MongoDB, 2018, NONBLOCKING SECONDAR
   Moraru I, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P358, DOI 10.1145/2517349.2517350
   Mortazavi Seyed Hossein, 2018, P USENIX WORKSH HOT
   Nightingale Edmund B., 2006, P 7 S OP SYST DES IM
   Ongaro D., 2014, THESIS
   Ports Dan R. K., 2015, P 12 S NETW SYST DES
   Ranganathan Karthik, 2019, LOW LATENCY READS GE
   Ratner D, 2001, MOBILE NETW APPL, V6, P525, DOI 10.1023/A:1011862121702
   TERRY D. B., 1994, P 3 INT C PAR DISTR
   Terry D, 2013, COMMUN ACM, V56, P82, DOI 10.1145/2500500
   Viotti P, 2016, ACM COMPUT SURV, V49, DOI 10.1145/2926965
   Wester Benjamin, 2009, P 6 S NETW SYST DES
NR 46
TC 1
Z9 1
U1 1
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD FEB
PY 2021
VL 17
IS 1
AR 4
DI 10.1145/3423138
PG 27
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QG9WR
UT WOS:000617930800003
DA 2024-07-18
ER

PT J
AU Zheng, Q
   Cranor, CD
   Jain, A
   Ganger, GR
   Gibson, GA
   Amvrosiadis, G
   Settlemyer, BW
   Grider, G
AF Zheng, Qing
   Cranor, Charles D.
   Jain, Ankush
   Ganger, Gregory R.
   Gibson, Garth A.
   Amvrosiadis, George
   Settlemyer, Bradley W.
   Grider, Gary
TI Streaming Data Reorganization at Scale with DeltaFS Indexed Massive
   Directories
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE In situ processing; computational storage
ID STORAGE ENGINE; SUPPORT
AB Complex storage stacks providing data compression, indexing, and analytics help leverage the massive amounts of data generated today to derive insights. It is challenging to perform this computation, however, while fully utilizing the underlying storage media. This is because, while storage servers with large core counts are widely available, single-core performance and memory bandwidth per core grow slower than the core count per die. Computational storage offers a promising solution to this problem by utilizing dedicated compute resources along the storage processing path. We present DeltaFS Indexed Massive Directories (IMDs), a new approach to computational storage. DeltaFS IMDs harvest available (i.e., not dedicated) compute, memory, and network resources on the compute nodes of an application to perform computation on data. We demonstrate the efficiency of DeltaFS IMDs by using them to dynamically reorganize the output of a real world simulation application across 131,072 CPU cores. DeltaFS !MIN speed up reads by 1,740x while only slightly slowing down the writing of data during simulation I/O for in situ data processing.
C1 [Zheng, Qing; Cranor, Charles D.; Jain, Ankush; Ganger, Gregory R.; Gibson, Garth A.; Amvrosiadis, George] Carnegie Mellon Univ, 5000 Forbes Ave, Pittsburgh, PA 15213 USA.
   [Settlemyer, Bradley W.; Grider, Gary] Los Alamos Natl Lab, POB 1663, Los Alamos, NM 87544 USA.
C3 Carnegie Mellon University; United States Department of Energy (DOE);
   Los Alamos National Laboratory
RP Zheng, Q (corresponding author), Carnegie Mellon Univ, 5000 Forbes Ave, Pittsburgh, PA 15213 USA.
EM zhengq@cmu.edu; chuck@cmu.edu; ankushj@cmu.edu; ganger@cmu.edu;
   garth@cmu.edu; gamvrosi@cmu.edu; bws@lanl.gov; ggrider@lanl.gov
OI Cranor, Charles/0000-0003-2427-9627; Zheng, Qing/0000-0002-7457-9874
FU U.S. Department of Energy/National Nuclear Security Administration
   [89233218CNA000001]
FX This manuscript has been approved for unlimited release and has been
   assigned LA-UR-20-26110. This work has been authored by an employee of
   Triad National Security, LLC, which operates Los Alamos National
   Laboratory under Contract No. 89233218CNA000001 with the U.S. Department
   of Energy/National Nuclear Security Administration. We thank the member
   companies of the PDL Consortium (Alibaba Group, Amazon, Datrium,
   Facebook, Google, Hewlett Packard Enterprise, Hitachi, Ltd., IBM
   Research, Intel Corporation, Microsoft Research, NetApp, Inc., Oracle
   Corporation, Salesforce, Samsung Semiconductor Inc., Seagate Technology,
   Two Sigma, and Western Digital).
CR Acharya A, 1998, ACM SIGPLAN NOTICES, V33, P81, DOI 10.1145/291006.291026
   Anand Ashok, 2010, P 7 USENIX C NETW SY, P29
   [Anonymous], 2012, P IEEE 28 S MASS STO
   [Anonymous], 2014, FAST
   [Anonymous], 2013, P 27 INT ACM C INT C, DOI [DOI 10.1145/2464996.2465003, 10.1145/2464996.2465003]
   [Anonymous], 2010, P 8 USENIX C FIL STO
   [Anonymous], 2008, PHYS PLASMAS
   [Anonymous], 2004, P 3 USENIX C FIL STO
   [Anonymous], 2010, MSST, DOI [10.1109/MSST.2010.5496981., DOI 10.1109/MSST.2010.5496981]
   [Anonymous], 2012, Mass Storage Systems and Technologies (MSST), 2012 IEEE 28th Symposium on, DOI [10.1109/MSST.2012.6232366, DOI 10.1109/MSST.2012.6232366]
   Atchley S., 2011, Proceedings of the 2011 IEEE 19th Annual Symposium on High-Performance Interconnects (HOTI 2011), P51, DOI 10.1109/HOTI.2011.17
   Atikoglu Berk, 2012, Performance Evaluation Review, V40, P53, DOI 10.1145/2318857.2254766
   Ayachit Utkarsh, 2016, IEEEACM INT C HIGH P
   Bender MA, 2012, PROC VLDB ENDOW, V5, P1627
   Bennett J.C., 2012, HIGH PERFORMANCE COM, P1
   Bent John, 2016, USENIX, V41, P2
   Bigelow D., 2010, P 2010 IEEE 26 S MAS, P1, DOI 10.1109/MSST.2010.5496975
   Birrell A. D., 1983, Operating Systems Review, V17, DOI 10.1145/773379.806609
   BLOOM BH, 1970, COMMUN ACM, V13, P422, DOI 10.1145/362686.362692
   Bonwick Jeff, 2003, TECHNICAL REPORT
   Byna Suren, 2013, P CRAY US GROUP CUG
   Byna Suren, 2015, P CRAY US GROUP CUG
   Byna S, 2012, INT CONF HIGH PERFOR
   Carns P., 2005, P 18 ANN C CONFL MAN, P1, DOI [10.1109/IPDPS.2005.128, DOI 10.1109/IPDPS.2005.128]
   Chang F, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P205
   Chen Chen Chen Chen, 2018, Mitigation and Adaptation Strategies for Global Change, V23, P101, DOI 10.1007/s11027-016-9731-y
   Chen J. H., 2009, Computational Science and Discovery, V2, DOI 10.1088/1749-4699/2/1/015001
   Chou J., 2011, P 2011 INT C HIGH PE, P30, DOI DOI 10.1145/2063384.2063424
   Chou JR, 2011, IEEE INT C CL COMP, P455, DOI 10.1109/CLUSTER.2011.86
   Davies D, 2011, CHANDOS ASIAN STUD, P1
   Dayan N, 2017, SIGMOD'17: PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P79, DOI 10.1145/3035918.3064054
   Dean J, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE SIXTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDE '04), P137
   Desnoyers Peter J., 2007, P 2007 USENIX ANN TE
   Devulapalli Ananth, 2009, TECHNICAL REPORT
   Do J., 2013, SIGMOD, P1221
   Doerfler Douglas, 2017, P CRAY US GROUP CUG
   Dong B., 2016, P 25 ACM INT S HIGH, P57
   Dorier M, 2012, IEEE INT C CL COMP, P155, DOI 10.1109/CLUSTER.2012.26
   Escriva R, 2012, ACM SIGCOMM COMP COM, V42, P25, DOI 10.1145/2377677.2377681
   Fan B, 2014, PROCEEDINGS OF THE 2014 CONFERENCE ON EMERGING NETWORKING EXPERIMENTS AND TECHNOLOGIES (CONEXT'14), P75, DOI 10.1145/2674005.2674994
   Greenberg Hugh N., 2015, P 7 USENIX C HOT TOP
   Grun P, 2015, PROCEEDINGS 2015 IEEE 23RD ANNUAL SYMPOSIUM ON HIGH-PERFORMANCE INTERCONNECTS - HOTI 2015, P34, DOI 10.1109/HOTI.2015.19
   Gu B, 2016, CONF PROC INT SYMP C, P153, DOI 10.1109/ISCA.2016.23
   HE J, 2012, 2012 SC COMP HIGH, P1, DOI DOI 10.1109/MSST.2012.6232382
   IBM, 2014, IBM PUREDATA SYST AN
   Im Junsu, 2020, P USENIX ANN TECHN C
   Jagadish HV, 1997, PROCEEDINGS OF THE TWENTY-THIRD INTERNATIONAL CONFERENCE ON VERY LARGE DATABASES, P16
   Jin YQ, 2017, INT S HIGH PERF COMP, P373, DOI 10.1109/HPCA.2017.15
   Jinoh Kim, 2011, Proceedings of the IEEE Symposium on Large Data Analysis and Visualization (LDAV 2011), P65, DOI 10.1109/LDAV.2011.6092319
   Kang Y., 2013, Mass Storage Systems and Technologies (MSST), 2013 IEEE 29th Symposium on, P1, DOI DOI 10.1109/MSST.2013.6558444
   Kang Y, 2019, SYSTOR '19: PROCEEDINGS OF THE 12TH ACM INTERNATIONAL SYSTEMS AND STORAGE CONFERENCE, P144, DOI 10.1145/3319647.3325831
   Keeton K., 1998, SIGMOD Record, V27, P42, DOI 10.1145/290593.290602
   LANL, 2016, LANL TRIN
   LANL NERSC and SNL, 2016, APEX WORKFL
   Lee CG, 2019, I S MOD ANAL SIM COM, P384, DOI 10.1109/MASCOTS.2019.00048
   Lee S, 2011, IEEE T CONSUM ELECTR, V57, P1732, DOI 10.1109/TCE.2011.6131148
   Li MZY, 2010, 2010 SECOND INTERNATIONAL CONFERENCE ON E-LEARNING, E-BUSINESS, ENTERPRISE INFORMATION SYSTEMS, AND E-GOVERNMENT (EEEE 2010), VOL II, P1, DOI 10.1080/03075079.2010.482981
   Li SY, 2017, SC'17: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS, DOI 10.1145/3126908.3126928
   Lim H, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P1
   Liu Ning., 2012, 28th Symposium on Mass Storage Systems and Technologies MSST, P1
   Lofstead J, 2016, SC '16: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS, P585, DOI 10.1109/SC.2016.49
   Lofstead J, 2009, INT PARALL DISTRIB P, P778
   Lu LY, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P133
   Luo C, 2020, VLDB J, V29, P393, DOI 10.1007/s00778-019-00555-y
   Marmol L., 2015, P USENIX ANN TECH C, P207
   Mitzenmacher M, 2001, IEEE T PARALL DISTR, V12, P1094, DOI 10.1109/71.963420
   Oldfield RA, 2012, SCI PROGRAMMING-NETH, V20, P181, DOI [10.3233/SPR-2012-0345, 10.1155/2012/842791]
   ONeil P, 1996, ACTA INFORM, V33, P351, DOI 10.1007/s002360050048
   Oracle, 2013, TECHN OV OR EX DAT M
   Ovsyannikov A, 2016, PROCEEDINGS OF PDSW-DISCS 2016 - 1ST JOINT INTERNATIONAL WORKSHOP ON PARALLEL DATA STORAGE AND DATA INTENSIVE SCALABLE COMPUTING SYSTEMS, P1, DOI [10.1109/PDSW-DISCS.2016.5, 10.1109/PDSW-DISCS.2016.005]
   Pagh R, 2004, J ALGORITHMS, V51, P122, DOI 10.1016/j.jalgor.2003.12.002
   Pandey P, 2017, SIGMOD'17: PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P775, DOI 10.1145/3035918.3035963
   Piernas Juan., 2007, SC 07, P1, DOI DOI 10.1145/1362622.1362660
   Ren K, 2017, PROC VLDB ENDOW, V10, P2037, DOI 10.14778/3151106.3151108
   Ren K, 2014, INT CONF HIGH PERFOR, P237, DOI 10.1109/SC.2014.25
   Ren Kai., 2013, USENIX ANN TECHNICAL, P145
   Riedel E, 2001, COMPUTER, V34, P68, DOI 10.1109/2.928624
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   Ross RB, 2020, J COMPUT SCI TECH-CH, V35, P121, DOI 10.1007/s11390-020-9802-0
   Schwan Philip, 2003, Proceedings of the 2003 linux symposium, V2003, P380
   Sears Russell, 2012, P ACM SIGMOD INT C M, P217
   Shetty P.J., 2013, 11 USENIX C FILE STO, P17
   Sim H, 2015, PROCEEDINGS OF SC15: THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS, DOI 10.1145/2807591.2807622
   SNIA, 2019, COMP STOR ARCH PROGR
   Sodani A, 2016, IEEE MICRO, V36, P34, DOI 10.1109/MM.2016.25
   Soumagne J., 2013, 2013 IEEE International Conference on Cluster Computing (CLUSTER), P1
   Tiwari Devesh., 2013, FAST, P119
   Vishwanath V., 2011, Proceedings of the IEEE Symposium on Large Data Analysis and Visualization (LDAV 2011), P9, DOI 10.1109/LDAV.2011.6092178
   Vishwanath V., 2011, Proceedings of 2011 SC, P19, DOI [10.1145/2063384.2063409, DOI 10.1145/2063384.2063409]
   Wang J., 2016, P INT WORKSH DAT MAN, DOI 10.1145/2933349.2933353
   Wang JG, 2017, SIGMOD'17: PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P993, DOI 10.1145/3035918.3064007
   Weil Sage A., 2007, PDSW, P35
   Woods L, 2014, PROC VLDB ENDOW, V7, P963, DOI 10.14778/2732967.2732972
   Wu KS, 2006, ACM T DATABASE SYST, V31, P1, DOI 10.1145/1132863.1132864
   Wu SM, 2018, DES AUT TEST EUROPE, P563, DOI 10.23919/DATE.2018.8342070
   Wu XN, 2015, METHODS MOL BIOL, V1306, P71, DOI 10.1007/978-1-4939-2648-0_5
   Xiujiang Li, 2014, 2014 21st Asia-Pacific Software Engineering Conference (APSEC), P27, DOI 10.1109/APSEC.2014.13
   Yu XY, 2020, PROC INT CONF DATA, P1802, DOI 10.1109/ICDE48307.2020.00174
   Zhang HC, 2018, INT CONF MANAGE DATA, P323, DOI 10.1145/3183713.3196931
   Zheng F, 2013, INT CONF HIGH PERFOR, DOI 10.1145/2503210.2503279
   Zheng F, 2013, INT PARALL DISTRIB P, P320, DOI 10.1109/IPDPS.2013.46
   Zheng Fang., 2010, Parallel Distributed Processing (IPDPS), 2010 IEEE International Symposium on, P1
   Zheng Q, 2017, PROCEEDINGS OF PDSW-DISCS 2017: 2ND JOINT INTERNATIONAL WORKSHOP ON PARALLEL DATA STORAGE & DATA INTENSIVE SCALABLE COMPUTING SYSTEMS, P7, DOI 10.1145/3149393.3149398
   Zheng Qing, 2015, P 10 PAR DAT STOR WO, P1
   Zuck A., 2014, P 2 WORKSH INT NVM F
NR 105
TC 2
Z9 2
U1 1
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2020
VL 16
IS 4
AR 23
DI 10.1145/3415581
PG 31
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA PA3GG
UT WOS:000595524400003
OA Bronze, Green Submitted
DA 2024-07-18
ER

PT J
AU Won, Y
   Oh, J
   Jung, J
   Choi, G
   Son, S
   Hwang, J
   Cho, S
AF Won, Youjip
   Oh, Joontaek
   Jung, Jaemin
   Choi, Gyeongyeol
   Son, Seongbae
   Hwang, Jooyoung
   Cho, Sangyeun
TI Bringing Order to Chaos: Barrier-Enabled I/O Stack for Flash Storage
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article; Proceedings Paper
CT 16th USENIX Conference on File and Storage Technologies (FAST)
CY FEB 12-15, 2018
CL Oakland, CA
SP USENIX, ACM SIGOPS
DE Filesystem; storage; block device; linux
AB This work is dedicated to eliminating the overhead required for guaranteeing the storage order in the modern IO stack. The existing block device adopts a prohibitively expensive approach in ensuring the storage order among write requests: interleaving the write requests with Transfer-and-Flush. For exploiting the cache barrier command for flash storage, we overhaul the IO scheduler, the dispatch module, and the filesystem so that these layers are orchestrated to preserve the ordering condition imposed by the application with which the associated data blocks are made durable. The key ingredients of Barrier-Enabled IO stack are Epoch-based IO scheduling, Order-Preserving Dispatch, and Dual-Mode Journaling. Barrier-enabled IO stack can control the storage order without Transfer-and-Flush overhead. We implement the barrier-enabled IO stack in server as well as in mobile platforms. SQLite performance increases by 270% and 75%, in server and in smartphone, respectively. In a server storage, BarrierFS brings as much as by 43x and by 73x performance gain in MySQL and SQLite, respectively, against EXT4 via relaxing the durability of a transaction.
C1 [Won, Youjip; Oh, Joontaek; Choi, Gyeongyeol; Son, Seongbae] Hanyang Univ, Seoul, South Korea.
   [Jung, Jaemin] Texas A&M Univ, College Stn, TX USA.
   [Hwang, Jooyoung; Cho, Sangyeun] Samsung Elect, Suwon, South Korea.
C3 Hanyang University; Texas A&M University System; Texas A&M University
   College Station; Samsung; Samsung Electronics
RP Oh, J (corresponding author), Hanyang Univ, Seoul, South Korea.
EM yjwon@hanyang.ac.kr; na94jun@hanyang.ac.kr; jmjung@tamu.edu;
   chl4651@hanyang.ac.kr; afireguy@hanyang.ac.kr;
   jooyoung.hwang@samsung.com; sangyeun.cho@samsung.com
FU Basic Research Lab Program (NRF) [2017R1A4A1015498]; BK21 plus (NRF);
   ICT R&D program (IITP) [R7117-16-0232]; Future OS project (IITP)
   [2018-0-00549]
FX This work was done while Jaemin Jung was a graduate student at Hanyang
   University.; This work is funded by Basic Research Lab Program (NRF, No.
   2017R1A4A1015498), the BK21 plus (NRF), ICT R&D program (IITP,
   R7117-16-0232) and Future OS project (IITP, No. 2018-0-00549).
CR AB MySQL, 2007, MYSQL 5 1 REF MAN
   [Anonymous], 2008, P 8 USENIX C OP SYST
   [Anonymous], 2015, JESD84B51 JEDEC
   [Anonymous], 2008, P 6 USENIX C FIL STO
   [Anonymous], 2012, P 10 USENIX C FIL ST
   [Anonymous], P ACM S OP SYST PRIN
   [Anonymous], 2016, JESD220C
   [Anonymous], 2016, P USENIX ANN TECHN C
   AXBOE J., 2004, P OTT LIN S OTT ONT
   Best Steve., 2000, JFS OVERVIEW
   Chang Yu-Ming, 2015, P DES AUT C DAC I5
   Chen F., 2011, P IEEE S HIGH PERF C
   Chen QS, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P241
   Chidambaram V., 2012, P 10 USENIX C FIL ST
   Chidambaram Vijay, 2013, P ACM S OP SYST PRIN
   Chidambaram Vijay, 2015, THESIS
   Cho YS, 2013, IEEE J SOLID-ST CIRC, V48, P948, DOI 10.1109/JSSC.2013.2237974
   Cipar James, 2012, P ACM EUR C COMP SYS
   Cobb Danny, 2012, P INT DEV FOR
   Corbet Jonathan, 2010, Barriers and journaling filesystems
   Corbet Jonathan, 2010, The end of block barriers
   DABEK F, 2001, P ACM S OP SYST PRIN
   Dees B, 2005, IEEE POTENTIALS, V24, P4, DOI 10.1109/MP.2005.1549750
   Elmasri Ramez, 2008, FUNDAMENTALS DATABAS, P815
   Fischer Stephen A., 2016, US Patent, Patent No. [9,383,998, 9383998]
   FROST C, 2007, P ACM S OP SYST PRIN
   Gim JM, 2010, ACM T STORAGE, V6, DOI 10.1145/1807060.1807063
   Guo J, 2013, DES AUT TEST EUROPE, P859
   HELLWIG C, Patchwork block: update documentation for req_ flush / req_ fua
   Helm Mark, 2014, P IEEE INT SOL STAT
   Jeong Sooman, 2013, P USENIX ANN TECHN C
   KANG J., 2015, P USENIX ANN TECHN C
   Kang Woon-Hak, 2013, P ACM SPEC INT GROUP
   Kesavan R, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P1
   KIM H.- J., 2011, P 2011 INT C FRONT C, V133, P745
   Kim Y, 2015, CLUSTER COMPUT, V18, P963, DOI 10.1007/s10586-015-0421-4
   Kopytov Alexey, 2004, SYSBENCH MANUAL
   Lee C., 2015, FAST
   Lee Seunghak, 2014, P USENIX ANN TECHN C
   Lee Wongun, 2015, P USENIX ANN TECHN C
   Lu Lanyue, 2014, P USENIX S OP SYST D
   Lu Youyou, P IEEE IEEE INT C CO
   Martinez Ashlie, 2017, P 9 USENIX WORKSH HO
   Mathur Avantika, 2007, P LIN S 2007
   MCKUSICK MK, 1999, P USENIX ANN TECHN C
   Min C, 2016, PROCEEDINGS OF USENIX ATC '16: 2016 USENIX ANNUAL TECHNICAL CONFERENCE, P71
   Min Changwoo, 2015, P USENIX ANN TECHN C
   MOHAN C, 1992, ACM T DATABASE SYST, V17, P94, DOI 10.1145/128765.128770
   Narayanan D., 2008, ACM T STORAGE, V4, P3
   Nightingale Edmund B., 2006, P USENIX S OP SYST D
   Okun M., 2002, P INT C ALG ARCH PAR
   Ou JX, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901324
   Ouyang Xiangyong, 2011, P IEEE S HIGH PERF C
   Park SY, 2010, IEEE COMPUT ARCHIT L, V9, P9, DOI 10.1109/L-CA.2010.3
   Park Stan, 2013, P ACM EUR C COMP SYS
   Pillai TS, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P181
   Prabhakaran Vijayan, 2005, P ACM S OP SYST PRIN
   Purohith D, 2017, PROCEEDINGS OF THE 8TH ASIA-PACIFIC WORKSHOP ON SYSTEMS (APSYS '17), DOI 10.1145/3124680.3124719
   Rev H., 2014, SCSI COMMANDS REFERE
   Rodeh O, 2013, ACM T STORAGE, V9, DOI 10.1145/2501620.2501623
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   Sct S.T., 1998, Journaling the linux ext2fs filesystem
   Sehgal Priya, 2010, P USENIX C FIL STOR
   SELTZER MI, 2000, P USENIX ANN TECHN C
   Shilamkar Girish., 2007, J CHECKSUMS
   SK hynix, 2015, EMMC5 1 SOL SK HYN
   SQLite, 2018, WELL KNOWN US SQLITE
   Sweeney A, 1996, PROCEEDINGS OF THE USENIX 1996 ANNUAL TECHNICAL CONFERENCE, P1
   TS'O T, 2015, USING CACHE BARRIER
   Uhm KH, 2016, IEEE ICCE
   Verma Rajat, 2015, P USENIX C FIL STOR
   Wang Y., 2013, 10 USENIX S NETW SYS
   Weiss Zev, 2015, P USENIX C FIL STOR
   Xu Q, 2015, P ACM INT SYST STOR
   Zhang C., 2014, P ACM EDAC IEEE DES
NR 75
TC 2
Z9 2
U1 3
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2018
VL 14
IS 3
AR 24
DI 10.1145/3242091
PG 29
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA HJ4JQ
UT WOS:000457140700005
DA 2024-07-18
ER

PT J
AU Vef, MA
   Tarasov, V
   Hildebrand, D
   Brinkmann, A
AF Vef, Marc-Andre
   Tarasov, Vasily
   Hildebrand, Dean
   Brinkmann, Andre
TI Challenges and Solutions for Tracing Storage Systems: A Case Study with
   Spectrum Scale
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Parallel file system; GPFS; trace analysis; performance
AB IBM Spectrum Scale's parallel file system General Parallel File System (GPFS) has a 20-year development history with over 100 contributing developers. Its ability to support strict POSIX semantics across more than 10K clients leads to a complex design with intricate interactions between the cluster nodes. Tracing has proven to be a vital tool to understand the behavior and the anomalies of such a complex software product. However, the necessary trace information is often buried in hundreds of gigabytes of by-product trace records. Further, the overhead of tracing can significantly impact running applications and file system performance, limiting the use of tracing in a production system.
   In this research article, we discuss the evolution of the mature and highly scalable GPFS tracing tool and present the exploratory study of GPFS' new tracing interface, FlexTrace, which allows developers and users to accurately specify what to trace for the problem they are trying to solve. We evaluate our methodology and prototype, demonstrating that the proposed approach has negligible overhead, even under intensive I/O workloads and with low-latency storage devices.
C1 [Vef, Marc-Andre; Brinkmann, Andre] Johannes Gutenberg Univ Mainz, Data Ctr ZDV, Anselm Franz von Bentzel Weg 12, D-55128 Mainz, Germany.
   [Tarasov, Vasily; Hildebrand, Dean] IBM Res, 650 Harry Rd, San Jose, CA 95120 USA.
C3 Johannes Gutenberg University of Mainz; International Business Machines
   (IBM)
RP Vef, MA (corresponding author), Johannes Gutenberg Univ Mainz, Data Ctr ZDV, Anselm Franz von Bentzel Weg 12, D-55128 Mainz, Germany.
EM vef@uni-mainz.de; vtarasov@us.ibm.com; dhildeb@us.ibm.com;
   brinkman@uni-mainz.de
RI Brinkmann, André/H-9888-2016
OI Vef, Marc-Andre/0000-0001-7398-3034
FU Storage Systems research group at IBM Research-Almaden
FX Further, we gratefully acknowledge the Storage Systems research group at
   IBM Research-Almaden for their support and sharing their valuable
   feedback.
CR Aggarwal A., 1997, Patent No. [5,675,741, 5675741]
   Ahrens Matt, 2003, P USENIX C FIL STOR
   Ali N, 2009, 2009 IEEE INTERNATIONAL CONFERENCE ON CLUSTER COMPUTING AND WORKSHOPS, P86
   [Anonymous], 2016, TECHNICAL REPORT
   [Anonymous], 2006, P OTT LIN S
   [Anonymous], 2007, NSDI
   [Anonymous], 2011, DTRACE DYNAMIC TRACI
   Aranya A, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P129
   Becker D., 2007, 2007 IEEE International Parallel and Distributed Processing Symposium, P1
   BeeGFS configurations, 2017, BEEGFS LOGG CONF
   Bent John., 2009, P C HIGH PERFORMANCE, P21
   Braam P. J., 2002, P OTT LIN S, P50
   Brunst H, 2001, LECT NOTES COMPUT SC, V2074, P751
   Cantrill BM, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE GENERAL TRACK 2004 USENIX ANNUAL TECHNICAL CONFERENCE, P15
   Carns P., 2013, P CRAY US GROUP M, V2013
   Cohen William, 2005, REDHAT MAGAZINE
   Corbet Jonathan, 2013, BF TRACING FILTERS
   Desnoyers Mathieu, 2006, OLS OTT LIN S CIT, V2006, P209
   Ellard D., 2003, P USENIX C FIL STOR
   Ellard D., 2003, P ANN USENIX C LARG
   Erlingsson U, 2012, ACM T COMPUT SYST, V30, DOI 10.1145/2382553.2382555
   Frings W., 2009, PROC 2009 ACMIEEE C, P1
   Geels Dennis Michael, 2006, THESIS
   Ghemawat S., 2003, ACM SIGOPS OPERATING, V37, P29, DOI [DOI 10.1145/945445.945450, DOI 10.1145/1165389.945450, 10.1145/1165389.945450]
   Gluster, 2017, GLUST STOR
   Gluster storage logging, 2017, MANAGING RED HAT STO
   Haskin RL, 1996, DIGEST OF PAPERS: COMPCON SPRING 96, FORTY-FIRST IEEE COMPUTER SOCIETY INTERNATIONAL CONFERENCE - INTELLECTUAL LEVERAGE, P226, DOI 10.1109/CMPCON.1996.501773
   Heichler J., 2014, INTRO BEEGFS
   Hildebrand D., 2015, LOGIN USENIX MAG, V40, P16
   Hildebrand Dean, 2014, HIGH PERFORMANCE PAR
   IBM, 2016, IBM SPECTR SCAL VERS
   ibmessflash, 2017, IBM EL STOR SERV DEL
   Kernel summit, 2016, KERNEL SUMMIT 2011 S
   Kim SJ, 2012, 2012 SC COMPANION: HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS (SCC), P18, DOI 10.1109/SC.Companion.2012.14
   Krishnakumar R., 2005, Linux Journal, V2005, P11
   Lee S. C., 2001, P 2001 IEEE WORKSH I, V6
   Linux tracepoints, 2016, USING LINUX KERNEL T
   Luk CK, 2005, ACM SIGPLAN NOTICES, V40, P190, DOI 10.1145/1064978.1065034
   Luo XQ, 2017, INT PARALL DISTRIB P, P585, DOI 10.1109/IPDPS.2017.45
   Lustre, 2016, LUSTR FIL SYST
   Luu Huong, 2015, 24th International Symposium on High-Performance Parallel and Distributed Computing, P33, DOI DOI 10.1145/2749246.2749269
   Mace J, 2015, SOSP'15: PROCEEDINGS OF THE TWENTY-FIFTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P378, DOI 10.1145/2815400.2815415
   Mdtest, 2015, MDTEST MET BENCHM
   Meehan John, 2017, P C INN DAT SYST RES
   Mesnier Michael P, 2007, Trace: parallel trace replay with approximate causal events
   Noeth M, 2009, J PARALLEL DISTR COM, V69, P696, DOI 10.1016/j.jpdc.2008.09.001
   Oral Sarp, 2016, PRESENTATION SLIDES
   Patil S, 2012, 2012 SC COMPANION: HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS (SCC), P30, DOI 10.1109/SC.Companion.2012.16
   Patil Swapnil., 2011, Proceedings of the 9th USENIX Conference on File and Stroage Technologies, FAST'11, P13
   Perf, 2015, LIN PERF
   Pollack K., 2005, I O TRACES TOOLS ANA
   Qian YJ, 2017, SC'17: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS, DOI 10.1145/3126908.3126932
   Qiu XJ, 2015, IEEE T PARALL DISTR, V26, P3330, DOI 10.1109/TPDS.2014.2371831
   Ren K, 2014, INT CONF HIGH PERFOR, P237, DOI 10.1109/SC.2014.25
   Rodeh O, 2013, ACM T STORAGE, V9, DOI 10.1145/2501620.2501623
   Rostedt S., 2009, P 11 REAL TIM LIN WO, P117
   Sambasivan R. R, 2011, P 8 USENIX C NETW SY, P43
   Schmuck F, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE FAST'02 CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P231
   Sharma SD, 2016, J COMPUT SCI TECH-CH, V31, P1161, DOI 10.1007/s11390-016-1690-y
   Shvachko K., 2010, 2010 IEEE 26th Symposium on Mass Storage Systems and Technologies (MSST), P1
   Snyder S, 2016, PROCEEDINGS OF ESPT 2016: 5TH WORKSHOP ON EXTREME-SCALE PROGRAMMING TOOLS, P9, DOI 10.1109/ESPT.2016.006
   Tarasov V., 2012, P 10 USENIX C FIL ST, P22
   Thereska E., 2006, Performance Evaluation Review, V34, P3, DOI 10.1145/1140103.1140280
   Uselton A., 2010, P 24 IEEE INT PARALL, P1, DOI DOI 10.1109/IPDPS.2010.5470424
   Vef Marc-Andre, 2016, SYSTEM ANAL CHARACTE
   Vef Marc-Andre, 2016, THESIS
   Weil SA, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P307
   Wright SA, 2013, COMPUT J, V56, P141, DOI 10.1093/comjnl/bxs044
   Xing J, 2009, PROCEEDINGS OF THE CONFERENCE ON HIGH PERFORMANCE COMPUTING NETWORKING, STORAGE AND ANALYSIS
   Yang Shuangyang, 2011, P IEEE INT WORKSH ST
   Zhou S., 1984, P USENIX SUMM C JUN, P407
NR 71
TC 13
Z9 16
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAY
PY 2018
VL 14
IS 2
AR 18
DI 10.1145/3149376
PG 24
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GI6TM
UT WOS:000434635800007
DA 2024-07-18
ER

PT J
AU Xiong, Q
   Wu, F
   Lu, ZH
   Zhu, Y
   Zhou, Y
   Chu, YB
   Xie, CS
   Huang, P
AF Xiong, Qin
   Wu, Fei
   Lu, Zhonghai
   Zhu, Yue
   Zhou, You
   Chu, Yibing
   Xie, Changsheng
   Huang, Ping
TI Characterizing 3D Floating Gate NAND Flash: Observations, Analyses, and
   Implications
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE 3D floating gate NAND flash; MLC; error pattern
ID MEMORY; RETENTION
AB As both NAND flash memory manufacturers and users are turning their attentions from planar architecture towards three-dimensional (3D) architecture, it becomes critical and urgent to understand the characteristics of 3D NAND flash memory. These characteristics, especially those different from planar NAND flash, can significantly affect design choices of flash management techniques. In this article, we present a characterization study on the state-of-the-art 3D floating gate (FG) NAND flash memory through comprehensive experiments on an FPGA-based 3D NAND flash evaluation platform. We make distinct observations on its performance and reliability, such as operation latencies and various error patterns, followed by careful analyses from physical and circuit-level perspectives. Although 3D FG NAND flash provides much higher storage densities than planar NAND flash, it faces new performance challenges of garbage collection overhead and program performance variations and more complicated reliability issues due to, e.g., distinct location dependence and value dependence of errors. We also summarize the differences between 3D FG NAND flash and planar NAND flash and discuss implications on the designs of NAND flash management techniques brought by the architecture innovation. We believe that our work will facilitate developing novel 3D FG NAND flash-oriented designs to achieve better performance and reliability.
C1 [Xiong, Qin; Zhu, Yue; Zhou, You] Huazhong Univ Sci & Technol, Wuhan Natl Lab Optoelect, 1037 Luoyu Rd, Wuhan 430074, Hubei, Peoples R China.
   [Wu, Fei; Xie, Changsheng] Huazhong Univ Sci & Technol, Minist Educ China, Wuhan Natl Lab Optoelect, 1037 Luoyu Rd, Wuhan 430074, Hubei, Peoples R China.
   [Wu, Fei; Xie, Changsheng] Huazhong Univ Sci & Technol, Minist Educ China, Key Lab Data Storage Syst, 1037 Luoyu Rd, Wuhan 430074, Hubei, Peoples R China.
   [Lu, Zhonghai] KTH Royal Inst Technol, Sch Informat & Commun Technol, 16 Kistagangen, S-16440 Kista, Sweden.
   [Chu, Yibing] Renice Technol Co Ltd, 15 Keyuan Rd, Shenzhen 518057, Peoples R China.
   [Huang, Ping] Temple Univ, Dept Comp Informat Sci, 1925 N 12th St, Philadelphia, PA 19122 USA.
C3 Huazhong University of Science & Technology; Huazhong University of
   Science & Technology; Huazhong University of Science & Technology; Royal
   Institute of Technology; Pennsylvania Commonwealth System of Higher
   Education (PCSHE); Temple University
RP Wu, F (corresponding author), Huazhong Univ Sci & Technol, Minist Educ China, Wuhan Natl Lab Optoelect, 1037 Luoyu Rd, Wuhan 430074, Hubei, Peoples R China.; Wu, F (corresponding author), Huazhong Univ Sci & Technol, Minist Educ China, Key Lab Data Storage Syst, 1037 Luoyu Rd, Wuhan 430074, Hubei, Peoples R China.
EM qinxiong@kth.se; wufei@hust.edu.cn; zhonghai@kth.se; yuezhu@hust.edu.cn;
   zhouyou@hust.edu.cn; eric@renice-tech.com; cs_xie@hust.edu.cn;
   templestorager@temple.edu
RI Lu, Zhonghai/AAP-2154-2020
FU National Natural Science Foundation of China [61300047, 61472152,
   61572209]; Wuhan Science and Technology Project [2017010201010108];
   Shenzhen Basic Research Project [ICYJ20170307160135308]; Fundamental
   Research Funds for the Central Universities [2016YXMS019]; 111 Project
   [B07038]; Key Laboratory of Data Storage System, Ministry of Education
FX This research is sponsored by the National Natural Science Foundation of
   China under Grant No. 61300047, No. 61472152, No. 61572209; Wuhan
   Science and Technology Project No. 2017010201010108; Shenzhen Basic
   Research Project No. ICYJ20170307160135308; the Fundamental Research
   Funds for the Central Universities No. 2016YXMS019, and the 111 Project
   No. B07038. This work is also supported by Key Laboratory of Data
   Storage System, Ministry of Education.
CR [Anonymous], 2013, PROC INT C EMBEDDED
   Aritome S, 2013, IEEE T ELECTRON DEV, V60, P1327, DOI 10.1109/TED.2013.2247606
   Baglee D. A., 1984, P INT RELIABILITY PH, P152, DOI 10.1109/IRPS.1984.362035
   Boboila Simona, 2010, P 8 USENIX C FIL STO, P115
   Cai Y, 2015, INT S HIGH PERF COMP, P551, DOI 10.1109/HPCA.2015.7056062
   Cai Y, 2013, 2013 IEEE 31ST INTERNATIONAL CONFERENCE ON COMPUTER DESIGN (ICCD), P123, DOI 10.1109/ICCD.2013.6657034
   Cai Y, 2015, I C DEPEND SYS NETWO, P438, DOI 10.1109/DSN.2015.49
   Cai Y, 2013, DES AUT TEST EUROPE, P1285
   Cai Y, 2012, DES AUT TEST EUROPE, P521
   Cai Y, 2012, PR IEEE COMP DESIGN, P94, DOI 10.1109/ICCD.2012.6378623
   Caulfield A. M., 2009, P 42 ANN IEEE ACM IN, P24
   Cooke Jim, 2017, P FLASH MEM SUMM, P1
   Degraeve R, 2004, IEEE T ELECTRON DEV, V51, P1392, DOI 10.1109/TED.2004.833583
   DESNOYERS P, 2010, ACM SIGOPS OPER SYST, V44, P50, DOI DOI 10.1145/1740390.1740402
   Fowler RH, 1928, P R SOC LOND A-CONTA, V119, P173, DOI 10.1098/rspa.1928.0091
   Ghetti A, 2009, IEEE T ELECTRON DEV, V56, P1746, DOI 10.1109/TED.2009.2024031
   Grupp L.M., 2013, Proceedings of the 2013 USENIX Conference on Annual Technical Conference, USENIX ATC'13, USENIX Association, Berkeley, CA, USA, P79
   International Data Corporation, WORLD IS STOR
   Jae-Duk Lee, 2006, 21st Non-Volatile Semiconductor Memory Workshop. (IEEE Cat. No. 06EX1246), P31, DOI 10.1109/.2006.1629481
   Jung M, 2014, CONF PROC INT SYMP C, P289, DOI 10.1109/ISCA.2014.6853216
   Kim J, 2013, BMC CARDIOVASC DISOR, V13, DOI 10.1186/1471-2261-13-55
   Kim K, 2008, 2008 INTERNATIONAL SYMPOSIUM ON VLSI TECHNOLOGY, SYSTEMS AND APPLICATIONS (VLSI-TSA), PROCEEDINGS OF TECHNICAL PROGRAM, P5, DOI 10.1109/VTSA.2008.4530774
   Lee C.H., 2006, PROC S VLSI TECHNOLO, P21
   Lee K, 2013, IEEE ELECTR DEVICE L, V34, P48, DOI 10.1109/LED.2012.2222013
   Micheloni R, 2016, 3D FLASH MEMORIES, P1, DOI 10.1007/978-94-017-7512-0
   Micheloni Rino, 2010, Inside NAND Flash Memories
   Parat K., 2015, IEDM, P1, DOI [10.1109/IEDM.2015.7409618, DOI 10.1109/IEDM.2015.7409618]
   Park KT, 2008, IEEE J SOLID-ST CIRC, V43, P919, DOI 10.1109/JSSC.2008.917558
   Seo MS, 2012, IEEE T ELECTRON DEV, V59, P2078, DOI 10.1109/TED.2012.2200682
   Suh KD, 1995, IEEE J SOLID-ST CIRC, V30, P1149, DOI 10.1109/4.475701
   Whang S., 2010, 2010 INT EL DEV M DE, P668
   Xiong Q., 2017, P ACM SIGMETRICS INT, P32
   Yoo H, 2011, PM2HW2N 11: PROCEEDINGS OF THE SIXTH ACM INTERNATIONAL WORKSHOP ON PERFORMANCE MONITORING, MEASUREMENT, AND EVALUATION OF HETEROGENEOUS WIRELESS AND WIRED NETWORKS, P1
   Zhao K., 2013, P USENIX C FIL STOR, P243
   Zhou Y., 2015, Mass Storage Systems and Technologies (MSST), 2015 31st Symposium on, P1
NR 35
TC 35
Z9 36
U1 14
U2 90
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAY
PY 2018
VL 14
IS 2
AR 16
DI 10.1145/3162616
PG 31
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GI6TM
UT WOS:000434635800005
DA 2024-07-18
ER

PT J
AU Shafaei, M
   Hajkazemi, MH
   Desnoyers, P
   Aghayev, A
AF Shafaei, Mansour
   Hajkazemi, Mohammad Hossein
   Desnoyers, Peter
   Aghayev, Abutalib
TI Modeling Drive-Managed SMR Performance
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Shingled Magnetic Recording (SMR); modeling; performance
AB Accurately modeling drive-managed Shingled Magnetic Recording (SMR) disks is a challenge, requiring an array of approaches including both existing disk modeling techniques as well as new techniques for inferring internal translation layer algorithms. In this work, we present the first predictive simulation model of a generally available drive-managed SMR disk. Despite the use of unknown proprietary algorithms in this device, our model that is derived from external measurements is able to predict mean latency within a few percent, and with an Root Mean Square (RMS) cumulative latency error of 25% or less for most workloads tested. These variations, although not small, are in most cases less than three times the drive-to-drive variation seen among seemingly identical drives.
C1 [Shafaei, Mansour; Hajkazemi, Mohammad Hossein; Desnoyers, Peter] Northeastern Univ, Coll Comp Sci, Boston, MA 02115 USA.
   [Aghayev, Abutalib] Carnegie Mellon Univ, Comp Sci Dept, Pittsburgh, PA 15213 USA.
C3 Northeastern University; Carnegie Mellon University
RP Shafaei, M (corresponding author), Northeastern Univ, Coll Comp Sci, Boston, MA 02115 USA.
EM shafaei@ece.neu.edu; hajkazemi@ece.neu.edu; pjd@ccs.neu.edu;
   agayev@cs.cmu.edu
OI Desnoyers, Peter/0000-0002-6194-2806
FU National Science Foundation [CNS-1149232]; NetApp Faculty Fellowship
FX This work was supported by the National Science Foundation under grant
   CNS-1149232, and by a NetApp Faculty Fellowship.
CR Aghayev A, 2015, ACM T STORAGE, V11, DOI 10.1145/2821511
   Aghayev Abutalib, P 15 USENIX C FIL ST, P105
   Amer A, 2011, IEEE T MAGN, V47, P3691, DOI 10.1109/TMAG.2011.2157115
   Amer Ahmed., 2010, Mass Storage Systems and Technologies (MSST), 2010 IEEE 26th Symposium on, P1, DOI DOI 10.1109/MSST.2010.5496991
   [Anonymous], CMUPDL08101 CARN MEL
   Balakrishnan S., 2014, PROC ITH USENIX CONJ, P351
   Cassuto Y., 2010, Proceedings of the 2010 IEEE 26th Symposium on Mass Storage Systems and Technologies (MSST), P1, DOI DOI 10.1109/MSST.2010.5496971
   Chung-I Lin, 2012, 2012 IEEE 20th International Symposium on Modelling, Analysis & Simulation of Computer and Telecommunication Systems (MASCOTS), P321, DOI 10.1109/MASCOTS.2012.44
   Desnoyers P, 2014, ACM T STORAGE, V10, DOI 10.1145/2577384
   FIO, FLEXIBLE IO TESTER
   Hall D, 2012, IEEE T MAGN, V48, P1777, DOI 10.1109/TMAG.2011.2179528
   HE W, 2017, P 15 USENIX C FIL, V48, P121
   He W., 2014, Proceedings of the 6th USENIX conference on Hot Topics in Storage and File Systems, P5
   INCITS T10 Technical Committee, 2014, T10BSRINCITS536
   INCITS T13 Technical Committee, 2015, T13BSRINCITS537 INCI
   Jin C., 2014, P COMSOL C, P1, DOI DOI 10.1109/MSST.2014.6855539
   Jones S.K., 2015, More than just parents: The importance of siblings as supportive others during the transition to college, P1
   Kadekodi Saurabh, 2015, P 7 USENIX WORKSH HO
   Krevat Elie., 2011, Proceedings of the 13th USENIX conference on Hot topics in operating systems, HotOS'13, P14
   Le Moal D, 2012, 2012 IEEE INTERNATIONAL CONFERENCE ON CONSUMER ELECTRONICS (ICCE), P425, DOI 10.1109/ICCE.2012.6161799
   Le Q. M., 2011, Proceedings of the 2011 IEEE 19th International Symposium on Modelling, Analysis & Simulation of Computer and Telecommunication Systems (MASCOTS 2011), P444, DOI 10.1109/MASCOTS.2011.58
   Narayanan Dushyanth, P 6 USENIX C FIL STO, P1
   Piramanayagam SN, 2007, J APPL PHYS, V102, DOI 10.1063/1.2750414
   RUEMMLER C, 1994, COMPUTER, V27, P17, DOI 10.1109/2.268881
   Schindler J, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE FAST'02 CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P259
   Schindler Jiri, 2011, 9 USENIX C FILE STOR
   Shriver E., 1998, Performance Evaluation Review, V26, P182, DOI 10.1145/277858.277906
   Suresh Anand, 2012, SHINGLED MAGNETIC RE
   Tan S. C., 2012, APMRC 2012 DIGEST, P1
   Wood R, 2009, IEEE T MAGN, V45, P917, DOI 10.1109/TMAG.2008.2010676
   Worthington Bruce L., 1995, ONLINE EXTRACTION SC, V23
NR 31
TC 11
Z9 12
U1 0
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD DEC
PY 2017
VL 13
IS 4
SI SI
AR 38
DI 10.1145/3139242
PG 22
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FR4LD
UT WOS:000419035900011
DA 2024-07-18
ER

PT J
AU Paulo, J
   Pereira, J
AF Paulo, Joao
   Pereira, Jose
TI Efficient Deduplication in a Distributed Primary Storage Infrastructure
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Primary storage; deduplication; distributed systems
AB A large amount of duplicate data typically exists across volumes of virtual machines in cloud computing infrastructures. Deduplication allows reclaiming these duplicates while improving the cost-effectiveness of large-scale multitenant infrastructures. However, traditional archival and backup deduplication systems impose prohibitive storage overhead for virtual machines hosting latency-sensitive applications. Primary deduplication systems reduce such penalty but rely on special cluster filesystems, centralized components, or restrictive workload assumptions. Also, some of these systems reduce storage overhead by confining deduplication to off-peak periods that may be scarce in a cloud environment.
   We present DEDIS, a dependable and fully decentralized system that performs cluster-wide off-line deduplication of virtual machines' primary volumes. DEDIS works on top of any unsophisticated storage backend, centralized or distributed, as long as it exports a basic shared block device interface. Also, DEDIS does not rely on data locality assumptions and incorporates novel optimizations for reducing deduplication overhead and increasing its reliability.
   The evaluation of an open-source prototype shows that minimal I/O overhead is achievable even when deduplication and intensive storage I/O are executed simultaneously. Also, our design scales out and allows collocating DEDIS components and virtual machines in the same servers, thus, sparing the need of additional hardware.
C1 INESC TEC, High Assurance Software Lab HASLab, Braga, Portugal.
   [Paulo, Joao; Pereira, Jose] Univ Minho, Dept Informat, Campus Gualtar, P-4710057 Braga, Portugal.
C3 INESC TEC; Universidade do Minho
RP Paulo, J (corresponding author), Univ Minho, Dept Informat, Campus Gualtar, P-4710057 Braga, Portugal.
EM jtpaulo@di.uminho.pt; jop@di.uminho.pt
RI Pereira, Jose/B-3256-2008; Paulo, João/GLU-7859-2022; Paulo, Joao
   A/AAO-3981-2020
OI Pereira, Jose/0000-0002-3341-9217; Paulo, João/0000-0001-9752-2822;
   Paulo, Joao A/0000-0002-4291-413X
FU ERDF - European Regional Development Fund through COMPETE Programme;
   National Funds through the FCT - Fundacao para a Ciencia e a Tecnologia
   (Portuguese Foundation for Science and Technology) [SFRH-BD-71372-2010];
   Fundação para a Ciência e a Tecnologia [SFRH/BD/71372/2010] Funding
   Source: FCT
FX This work is funded by ERDF - European Regional Development Fund through
   the COMPETE Programme (operational programme for competitiveness) and by
   National Funds through the FCT - Fundacao para a Ciencia e a Tecnologia
   (Portuguese Foundation for Science and Technology) within Ph.D
   scholarship SFRH-BD-71372-2010.
CR Al-Rfou Rami, 2010, TECHNICAL REPORT
   Anderson D. P., 2002, TECHNICAL REPORT
   [Anonymous], MSRTR200230
   [Anonymous], 2009, P USENIX ANN TECHNIC
   [Anonymous], P USENIX C FIL STOR
   [Anonymous], P USENIX ANN TECHN C
   [Anonymous], P USENIX C FIL STOR
   [Anonymous], 2010, P USENIX C USENIX AN
   [Anonymous], P EUR C COMP SYST EU
   Bhagwat Deepavali, 2009, P INT S MOD AN SIM C
   Bolosky W. J., 2000, P USENIX WIND SYST S
   Citrix Systems Inc., 2014, BLKT DOC
   Coker Russell, 2015, BONNIE
   Dong Wei, 2011, P USENIX C FIL STOR
   El-Shimi A., 2012, P USENIX ANN TECHN C
   EMC, 2012, NEW DIG UN STUD REV
   Frey D., 2012, P 3 ACM S CLOUD COMP
   Fu Yinjin, 2012, P ACM IFIP USENIX IN
   Guo F., 2011, P USENIX ANN TECHN C
   Hong B., 2004, P C MASS STOR SYST M
   HP, 2011, CISC VIS NETW IND GL
   Iacono Dan., 2013, ENTERPRISE STORAGE E
   Jin Keren, 2009, P INT SYST STOR C SY
   Jones M Tim, 2010, VIRTIO I O VIRTUALIZ
   Kaczmarczyk Michal, 2012, P INT SYST STOR C SY
   Kaiser Jurgen, 2012, P C MASS STOR SYST M
   Katcher Jeffrey, 1997, TECHNICAL REPORT
   Koller R, 2010, ACM T STORAGE, V6, DOI 10.1145/1837915.1837921
   Li Y.-K., 2014, ACM T STORAGE
   Liguori A., 2008, P USENIX WORKSH I O
   Lillibridge M., 2009, P USENIX C FIL STOR
   Meister D., 2010, P C MASS STOR SYST M
   Meyer D.T., 2011, P USENIX C FIL STOR
   Meyer DT, 2012, ACM T STORAGE, V7, DOI 10.1145/2078861.2078864
   Ng Chun-Ho, 2011, P ACM IFIP USENIX IN
   Olson M. A., 1999, P USENIX ANN TECHN C
   OpenSolaris, 2014, ZFS DOC
   OpenStack Foundation, 2016, CIND DOC
   Ozawa T., 2014, ACCORD
   Paulo J., 2012, P INT S SEC VIRT INF
   Paulo J., 2014, DISTRIBUTED APPL INT
   Paulo J., 2013, INT J COMPUTER SYSTE, V29
   Paulo Joao, 2014, COMPUT SURV, V47, P1
   Paulo Joao, 2011, FAST LAT AM S DEP CO
   Rhea S., 2008, P USENIX ANN TECHN C
   Russell Rusty, 2008, Operating Systems Review, V42, P95, DOI 10.1145/1400097.1400108
   Shilane Philip, 2012, P USENIX WORKSH HOT
   Srinivasan K., 2012, P USENIX C FIL STOR
   TARASOV V., 2012, USENIX ANN TECHN C A
   Tsuchiya Y., 2011, P C MASS STOR SYST M
   Ungureanu C., 2010, P USENIX C FIL STOR
   Wei Jiansheng, 2010, P C MASS STOR SYST M
   Wildani Avani, 2013, P INT C DAT ENG ICDE
   Yang Tianming, 2010, P INT PAR DISTR PROC
   You Lawrence L., 2005, P INT C DAT ENG ICDE
   Zhu Benjamin, 2008, P USENIX C FIL STOR
NR 56
TC 14
Z9 17
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2016
VL 12
IS 4
AR 20
DI 10.1145/2876509
PG 35
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DV2OQ
UT WOS:000382760500003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Grawinkel, M
   Nagel, L
   Brinkmann, A
AF Grawinkel, Matthias
   Nagel, Lars
   Brinkmann, Andre
TI LoneStar RAID: Massive Array of Offline Disks for Archival Systems
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article; Proceedings Paper
CT 31st International Conference on Massive Storage Systems and
   Technologies (MSST 2015)
CY JUN 01-05, 2015
CL Santa Clara, CA
DE Reliability; Design; Performance; Measurement; MAID; disk-based;
   archival; reliability
AB The need for huge storage archives rises with the ever growing creation of data. With today's big data and data analytics applications, some of these huge archives become active in the sense that all stored data can be accessed at any time. Running and evolving these archives is a constant tradeoff between performance, capacity, and price. We present the LoneStar RAID, a disk-based storage architecture, which focuses on high reliability, low energy consumption, and cheap reads. It is designed for MAID systems with up to hundreds of disk drives per server and is optimized for "write once, read sometimes" workloads. We use dedicated data and parity disks, and export the data disks as individually accessible buckets. By intertwining disk groups into a two-dimensional RAID and improving single-disk reliability with intradisk redundancy, the system achieves an elastic fault tolerance that can at least recover all 3-disk failures. Furthermore, we integrate a cache to offload parity updates and a journal to track the RAID's state. The LoneStar RAID scheme provides a mean time to data loss (MTTDL) that competes with today's erasure codes and is optimized to require only a minimal set of running disk drives.
C1 [Grawinkel, Matthias; Nagel, Lars; Brinkmann, Andre] Johannes Gutenberg Univ Mainz, Zentrum Datenverarbeitung, D-55099 Mainz, Germany.
C3 Johannes Gutenberg University of Mainz
RP Grawinkel, M; Nagel, L; Brinkmann, A (corresponding author), Johannes Gutenberg Univ Mainz, Zentrum Datenverarbeitung, D-55099 Mainz, Germany.
EM grawinkel@uni-mainz.de; nagell@uni-mainz.de; brinkman@uni-mainz.de
RI Brinkmann, Andre/H-9888-2016; Nagel, Lars/B-7013-2018
OI Brinkmann, Andre/0000-0003-3083-2775; Nagel, Lars/0000-0002-1444-9541
CR Adams I. F., 2010, P 18 IEEE INT S MOD
   [Anonymous], P 15 IEEE PAC RIM IN
   [Anonymous], P 39 INT C VER LARG
   [Anonymous], P 4 USENIX C FIL STO
   [Anonymous], ACM QUEUE
   Bairavasundaram Lakshmi N., 2008, ACM Transaction on Storage, V4, DOI 10.1145/1416944.1416947
   Bairavasundaram L. N., 2007, P 2007 ACM SIGMETRIC
   Balakrishnan S., 2014, PROC USENIX OSDI
   Colarelli D., 2002, P USENIX C FIL STOR
   Dholakia Ajay, 2008, ACM Transaction on Storage, V4, DOI 10.1145/1353452.1353453
   Felter W., 2011, P 27 IEEE C MASS STO
   Gao Y., 2010, P 5 IEEE INT C NETW
   Grawinkel M., 2011, P 17 IEEE INT C PAR
   Grawinkel M., 2014, P 9 IEEE INT C NETW
   Grawinkel M., 2015, P 13 USENIX C FIL ST
   Grawinkel M., 2011, P 19 IEEE INT S MOD
   Greenan K M, 2008, P 4 WORKSH HOT TOP S
   Greenan K. M., 2010, P 26 IEEE C MASS STO
   Hafner James Lee, 2006, TECHNICAL REPORT
   HITZ D., 1994, P USENIX WINT TECHN
   Holland Mark., 1992, Parity declustering for continuous operation in redundant disk arrays, V27
   Huang C., 2012, P ANN TECHN C ATC 12
   Iliadis I., 2008, P ACM SIGMETRICS INT
   J affe E., 2009, P ISRAELI EXPT SYSTE
   Jones L., 2010, PANASAS TIERED PARIT
   LI D, 2004, P 11 ACM SIGOPS EUR
   Narayanan D., 2008, ACM T STORAGE, V4, P3
   Paris J, 2008, P 7 INT INF TEL TECH
   Paris J.-F., 2010, P 5 IEEE INT C NETW
   Paris J.-F., 2012, P 31 IEEE PERF COMP
   Pinheiro E., 2004, P 18 ANN INT C SUP I
   PLANK J, 2008, P 6 USENIX C FIL STO
   Plank J. S., 2013, P 11 USENIX C FIL ST, P95
   Schroeder B., 2010, P 8 USENIX C FIL STO
   Schroeder B., 2007, P 5 USENIX C FIL STO
   Schwarz T. J. E., 1993, TECHNICAL REPORT
   SCHWARZ TJE, 2004, P 12 IEEE INT S MOD
   STODOLSKY D, 1994, ACM T COMPUT SYST, V12, P206, DOI 10.1145/185514.185516
   Storer M, 2008, P 6 USENIX C FIL STO
   Thomasian Alexander, 2009, ACM Transaction on Storage, V5, DOI 10.1145/1629075.1629076
   Wang Jun, 2008, IEEE T COMPUTERS, V57
   Weddle C., 2007, ACM TOS, V3
   Wildani A., 2010, P 5 PET DAT STOR WOR
   Woitaszek M., 2007, P 24 IEEE C MASS STO
   Xiao L., 2011, P 27 IEEE C MASS STO
NR 45
TC 3
Z9 3
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JAN
PY 2016
VL 12
IS 1
SI SI
AR 5
DI 10.1145/2840810
PG 29
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DI5EI
UT WOS:000373521300005
DA 2024-07-18
ER

PT J
AU Li, RB
   Ren, X
   Zhao, X
   He, SW
   Stumm, M
   Yuan, D
AF Li, Ruibin
   Ren, Xiang
   Zhao, Xu
   He, Siwei
   Stumm, Michael
   Yuan, Ding
TI ctFS: Replacing File Indexing with Hardware Memory Translation through
   Contiguous File Allocation for Persistent Memory
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Persistent memory; file system; page table; memory allocation; data
   center
AB Persistent byte-addressable memory (PM) is poised to become prevalent in future computer systems. PMs are significantly faster than disk storage, and accesses to PMs are governed by the Memory Management Unit (MMU) just as accesses with volatile RAM. These unique characteristics shift the bottleneck from I/O to operations such as block address lookup-for example, in write workloads, up to 45% of the overhead in ext4-DAX is due to building and searching extent trees to translate file offsets to addresses on persistent memory.
   We propose a novel contiguous file system, ctFS, that eliminates most of the overhead associated with indexing structures such as extent trees in the file system. ctFS represents each file as a contiguous region of virtual memory, hence a lookup from the file offset to the address is simply an offset operation, which can be efficiently performed by the hardware MMU at a fraction of the cost of software-maintained indexes. Evaluating ctFS on real-world workloads such as LevelDB shows it outperforms ext4-DAX and SplitFS by 3.6x and 1.8x, respectively.
C1 [Li, Ruibin; Ren, Xiang; Zhao, Xu; He, Siwei; Stumm, Michael; Yuan, Ding] Univ Toronto, Sandford Fleming 2002E,10 Kings Coll Rd, Toronto, ON M5S 3G4, Canada.
C3 University of Toronto
RP Li, RB (corresponding author), Univ Toronto, Sandford Fleming 2002E,10 Kings Coll Rd, Toronto, ON M5S 3G4, Canada.
EM robinlrb.li@mail.utoronto.ca; jenny.ren@mail.utoronto.ca; i@xuzhao.net;
   siwei.he@mail.utoronto.ca; stumm@eecg.toronto.edu; yuan@ece.utoronto.ca
RI He, siwei/ISV-3299-2023
OI Zhao, Xu/0000-0003-2906-8677; He, Siwei/0000-0001-5805-8027; Stumm,
   Michael/0000-0002-9377-2493; Yoshida, Nobuko/0000-0002-3925-8557
FU Canada Research Chair fund; NSERC discovery grant
FX This research was supported by the Canada Research Chair fund, an NSERC
   discovery grant, and a VMware gift.
CR Alexandra Fedorova Sasha), 2019, WHY MMAP IS FASTER S
   [Anonymous], 2013, EUROSYS 13
   Bonwick Jeff, 2022, ZFS LAST WORD N FILE
   Chen YM, 2021, PROCEEDINGS OF THE 19TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '21), P81
   Condit J, 2009, SOSP'09: PROCEEDINGS OF THE TWENTY-SECOND ACM SIGOPS SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P133
   Cooper Brian F., 2010, P 1 ACM S CLOUD COMP, P143, DOI [DOI 10.1145/1807128.1807152, 10.1145/1807128.1807152]
   corbet, 2005, Address space randomization in 2.6
   DavidWoodhouse, 2001, P OTTAWA LINUX S
   Dong MK, 2019, PROCEEDINGS OF THE TWENTY-SEVENTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '19), P478, DOI 10.1145/3341301.3359637
   Dulloor Subramanya R., 2014, P 9 EUR C COMP SYST
   Edge J., 2013, Randomizing the kernel
   Evans Jason, 2006, P BSDCAN C OTT CAN
   Ganger GR, 2000, ACM T COMPUT SYST, V18, P127, DOI 10.1145/350853.350863
   Ghemawat S., 2022, TCMALLOC
   Google Inc, 2022, LEVELDB
   Gorman Mel, 2022, PAGE TABLE MANAGEMEN
   Griesemer Robert, 2022, GOLANG
   Hagmann R., 1987, Operating Systems Review, V21, P155, DOI 10.1145/37499.37518
   HITZ D, 1994, PROCEEDINGS OF THE WINTER 1994 USENIX CONFERENCE, P235
   Intel, 2022, INT AVX 512 INSTR
   Izraelevitz Joseph, 2019, arXiv, DOI [10.48550/ARXIV.1903.05714, DOI 10.48550/ARXIV.1903.05714]
   Jannen W, 2015, ACM T STORAGE, V11, DOI 10.1145/2798729
   Kadekodi R, 2019, PROCEEDINGS OF THE TWENTY-SEVENTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '19), P494, DOI 10.1145/3341301.3359631
   KNOWLTON KC, 1965, COMMUN ACM, V8, P623, DOI 10.1145/365628.365655
   Koo J, 2021, PROCEEDINGS OF THE 15TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '21), P75, DOI 10.5281/zenodo.4659803
   Kwon Y, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P460, DOI 10.1145/3132747.3132770
   Lee E., 2013, P 11 USENIX C FIL ST, P73
   Lenovo, 2021, INT OPT PERS MEM 100
   Meta Platform Inc, 2022, DIR I O ROCKSDB WIK
   Mohan C, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P1
   Neal I, 2021, PROCEEDINGS OF THE 19TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '21), P97
   Oh G, 2016, SIGMOD'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P343, DOI 10.1145/2882903.2882910
   Park S, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P241
   Reuter A, 1992, T PROCESSING CONCEPT
   RITCHIE DM, 1974, COMMUN ACM, V17, P365, DOI 10.1145/361011.361061
   Soh Yun Joon, 2020, P 11 AS PAC WORKSH S, P1, DOI [10.1145/3409963.3410489, DOI 10.1145/3409963.3410489]
   Tanenbaum Andrew., 2018, Modern operating systems
   Volos H., 2014, P 9 EUR C COMP SYST
   Wu Xiaojian., 2011, Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis (SC '11), p39:1
   Xu J, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P323
NR 40
TC 0
Z9 0
U1 1
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2022
VL 18
IS 4
AR 30
DI 10.1145/3565026
PG 24
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7F8OU
UT WOS:000902100700003
OA Green Published, Green Submitted, hybrid, Green Accepted
DA 2024-07-18
ER

PT J
AU Einziger, G
   Eytan, O
   Friedman, R
   Manes, B
AF Einziger, Gil
   Eytan, Ohad
   Friedman, Roy
   Manes, Benjamin
TI Lightweight Robust Size Aware Cache Management
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article; Proceedings Paper
CT ACM Symposium on Operating Systems Principles (SOSP)
CY OCT 26-29, 2021
CL ELECTR NETWORK
SP Assoc Comp Machinery
DE Software cache management; size aware caching; storage; CDN
AB Modern key-value stores, object stores, Internet proxy caches, and Content Delivery Networks (CDN) often manage objects of diverse sizes, e.g., blobs, video files of different lengths, images with varying resolutions, and small documents. In such workloads, size-aware cache policies outperform size-oblivious algorithms. Unfortunately, existing size-aware algorithms tend to be overly complicated and computationally expensive.
   Our work follows a more approachable pattern; we extend the prevalent (size-oblivious) TinyLFU cache admission policy to handle variable-sized items. Implementing our approach inside two popular caching libraries only requires minor changes. We show that our algorithms yield competitive or better hit-ratios and byte hit-ratios compared to the state-of-the-art size-aware algorithms such as AdaptSize, LHD, LRB, and GDSF. Further, a runtime comparison indicates that our implementation is faster by up to 3x compared to the best alternative, i.e., it imposes a much lower CPU overhead.
C1 [Einziger, Gil] Ben Gurion Univ Negev, Comp Sci Dept, Beer Sheva, Israel.
   [Eytan, Ohad; Friedman, Roy] Technion, Comp Sci Dept, Haifa, Israel.
   [Manes, Benjamin] Vector, 135 Stillman St, San Francisco, CA 94107 USA.
C3 Ben Gurion University; Technion Israel Institute of Technology
RP Einziger, G (corresponding author), Ben Gurion Univ Negev, Comp Sci Dept, Beer Sheva, Israel.
EM gilein@bgu.ac.il; ohadey@cs.technion.ac.il; roy@cs.technion.ac.il;
   ben.manes@gmail.com
OI Eytan, Ohad/0000-0001-8655-794X
FU Cyber Security Research Center; Data Science Research Center at
   Ben-Gurion University
FX The work was partially funded by the Cyber Security Research Center and
   the Data Science Research Center at Ben-Gurion University.
CR Abrams Marc., 1995, Caching proxies: Limitations and potentials
   Adas D., 2017, GLOBECOM 2017 2017 I, P1
   Akhtar Z, 2019, PROCEEDINGS OF THE 15TH INTERNATIONAL CONFERENCE ON EMERGING NETWORKING EXPERIMENTS AND TECHNOLOGIES (CONEXT '19), P305, DOI 10.1145/3359989.3365423
   [Anonymous], 1993, ACM SIGMOD RECORD, DOI DOI 10.1145/170035.170081
   [Anonymous], 2010, P 4 ACM INT WORKSHOP
   Arlitt M, 2000, PERFORM EVALUATION, V39, P149, DOI 10.1016/S0166-5316(99)00062-0
   Bansal S, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P187
   Beckmann N, 2018, PROCEEDINGS OF THE 15TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION (NSDI'18), P389
   Beckmann Nathan, 2018, SIMULATION CODE LHD
   BELADY LA, 1966, IBM SYST J, V5, P78, DOI 10.1147/sj.52.0078
   Berg B, 2020, PROCEEDINGS OF THE 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '20), P769
   Berger DS, 2018, P ACM MEAS ANAL COMP, V2, DOI 10.1145/3224427
   Berger DS, 2017, PROCEEDINGS OF NSDI '17: 14TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P483
   Berger Daniel S., 2017, WEBCACHESIM C CACHE
   Blankstein A, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P499
   Cherkasova L.., 1998, Improving WWW Proxies Performance With Greedy- Dual-Size-Frequency Caching Policy
   Chockler G, 2011, IBM J RES DEV, V55, DOI 10.1147/JRD.2011.2171649
   Cohen S., 2003, P 2003 ACM SIGMOD IN, DOI DOI 10.1145/872757.872787
   Cormode G, 2005, J ALGORITHMS, V55, P58, DOI 10.1016/j.jalgor.2003.12.001
   Dgraph, 2020, Ristretto: A High Performance Memory-Bound Go Cache
   Einziger G., 2017, ACM T STORAGE TOS 17
   Einziger G, 2018, MIDDLEWARE'18: PROCEEDINGS OF THE 2018 ACM/IFIP/USENIX MIDDLEWARE CONFERENCE, P94, DOI 10.1145/3274808.3274816
   Fitzpatrick Brad, 2004, Linux J., V124
   Gold H, 2020, CNN BUSINESS
   Hasan Syed, 2014, IEEE INFOCOM 2014 - IEEE Conference on Computer Communications, P460, DOI 10.1109/INFOCOM.2014.6847969
   Hennessy J. L., 2012, Computer Architecture A Quantitative Approach, V5th
   Jiang S, 2005, USENIX Association Proceedings of the General Track: 2005 UNENIX Annual Technical Conference, P323
   Johnson T., 1994, P 20 INT C VER LARG, P439
   Karakostas G, 2002, IEEE SYMP COMP COMMU, P207, DOI 10.1109/ISCC.2002.1021680
   Lee C, 2017, SYSTOR'17: PROCEEDINGS OF THE 10TH ACM INTERNATIONAL SYSTEMS AND STORAGE CONFERENCE, DOI 10.1145/3078468.3078479
   Lee D, 2001, IEEE T COMPUT, V50, P1352, DOI 10.1109/tc.2001.970573
   Li C, 2018, SYSTOR'18: PROCEEDINGS OF THE 11TH ACM INTERNATIONAL SYSTEMS AND STORAGE CONFERENCE, P59, DOI 10.1145/3211890.3211891
   Manes Ben, 2016, Caffeine: A High Performance Caching Library for Java 8
   Megiddo N, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE 2ND USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST'03), P115
   Mertz J, 2018, ACM COMPUT SURV, V50, DOI 10.1145/3145813
   Narayanan D, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P253
   Neglia Giovanni, 2017, ACM T MODEL PERFORM, V2, P4
   Park S., 2017, PROC 33 IEEE INT C M, P1
   Redis-Labs, 2020, USING REDIS LRU CACH
   Rodriguez LV, 2021, PROCEEDINGS OF THE 19TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '21), P341
   Shah Anirban Ketan, 2010, ALGORITHM IMPLEMENTA
   Song Jiang, 2002, Performance Evaluation Review, V30, P31, DOI 10.1145/511399.511340
   Song ZY, 2020, PROCEEDINGS OF THE 17TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P529
   Song Zhenyu., 2020, SIMULATOR CDN CACHIN
   Starobinski D, 2001, PERFORM EVALUATION, V46, P125, DOI 10.1016/S0166-5316(01)00045-1
   Subramanian R, 2006, INT SYMP MICROARCH, P385
   Tanenbaum A.S, 2001, Modern Operating Systems
   Vietri Giuseppe, 2018, P 10 USENIX C HOT TO
   Waldspurger CA, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P487
   YOUNG N, 1991, PROCEEDINGS OF THE SECOND ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P241
   Zhong Chen, 2021, PROC 14 ACM INT SYST
   Zhou K, 2018, INTERNATIONAL CONFERENCE ON SUPERCOMPUTING (ICS 2018), P284, DOI 10.1145/3205289.3205299
NR 52
TC 1
Z9 1
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2022
VL 18
IS 3
AR 27
DI 10.1145/3507920
PG 23
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 4X9BK
UT WOS:000861131000009
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Liu, WH
   Wu, F
   Chen, X
   Zhang, M
   Wang, Y
   Lu, XF
   Xie, CS
AF Liu, Weihua
   Wu, Fei
   Chen, Xiang
   Zhang, Meng
   Wang, Yu
   Lu, Xiangfeng
   Xie, Changsheng
TI Characterization Summary of Performance, Reliability, and Threshold
   Voltage Distribution of 3D Charge-Trap NAND Flash Memory
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE 3D CT NAND flash; performance; reliability; threshold voltage;
   characterization
ID READ VOLTAGE
AB Solid-state drive (SSD) gradually dominates in the high-performance storage scenarios. Three-dimension (3D) NAND flash memory owning high-storage capacity is becoming a mainstream storage component of SSD. However, the interferences of the new 3D charge-trap (CT) NAND flash are getting unprecedentedly complicated, yielding to many problems regarding reliability and performance. Alleviating these problems needs to understand the characteristics of 3D CT NAND flash memory deeply. To facilitate such understanding, in this article, we delve into characterizing the performance, reliability, and threshold voltage (V-th) distribution of 3D CT NAND flash memory. We make a summary of these characteristics with multiple interferences and variations and give several new insights and a characterization methodology. Especially, we characterize the skewed V-th distribution, V-th shift laws, and the exclusive layer variation in 3D NAND flash memory. The characterization is the backbone of designing more reliable and efficient flash-based storage solutions.
C1 [Liu, Weihua; Wu, Fei; Chen, Xiang; Zhang, Meng; Wang, Yu; Xie, Changsheng] Huazhong Univ Sci & Technol, Wuhan Natl Lab Optoelect, 1037 Luoyu Rd, Wuhan 430074, Hubei, Peoples R China.
   [Liu, Weihua; Wu, Fei; Chen, Xiang; Zhang, Meng; Wang, Yu; Xie, Changsheng] Huazhong Univ Sci & Technol, Key Lab Informat Storage Syst, Minist Educ China, 1037 Luoyu Rd, Wuhan 430074, Hubei, Peoples R China.
   [Lu, Xiangfeng] Beijing Memblaze Technol Co Ltd, Bldg B2,Dongsheng Pk,66 Xixiaokou Rd, Beijing 100192, Peoples R China.
C3 Huazhong University of Science & Technology; Huazhong University of
   Science & Technology
RP Wu, F (corresponding author), Huazhong Univ Sci & Technol, Wuhan Natl Lab Optoelect, 1037 Luoyu Rd, Wuhan 430074, Hubei, Peoples R China.; Wu, F (corresponding author), Huazhong Univ Sci & Technol, Key Lab Informat Storage Syst, Minist Educ China, 1037 Luoyu Rd, Wuhan 430074, Hubei, Peoples R China.
EM liuweihua@hust.edu.cn; wufei@hust.edu.cn; d201780773@hust.edu.cn;
   zgmeng@hust.edu.cn; ywang_wnlo@hust.edu.cn; tmsi@hotmail.com;
   cs_xie@hust.edu.cn
RI Zhang, Can/JUU-9511-2023; yuan, lin/JDW-7387-2023; zhang,
   meng/GQP-4394-2022
OI zhang, meng/0000-0003-1677-3393
FU National Natural Science Foundation of China [U2001203, 61872413,
   62102156, 61902137]; Fundamental Research Funds for the Central
   Universities, HUST [2021JYCXJJ033]; Key Project of Shandong Wisdom Joint
   Fund [ZR2019LZH009]; 111 Project [B07038]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants No. U2001203, No. 61872413, No.
   62102156, and No. 61902137, in part by the Fundamental Research Funds
   for the Central Universities, HUST: No. 2021JYCXJJ033, in part by the
   Key Project of Shandong Wisdom Joint Fund under Grant No. ZR2019LZH009,
   and in part by the 111 Project under Grant B07038.
CR Arteaga D, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P355
   Cai Y, 2017, P IEEE, V105, P1666, DOI 10.1109/JPROC.2017.2713127
   Cai Y, 2015, INT S HIGH PERF COMP, P551, DOI 10.1109/HPCA.2015.7056062
   Cai Y, 2013, 2013 IEEE 31ST INTERNATIONAL CONFERENCE ON COMPUTER DESIGN (ICCD), P123, DOI 10.1109/ICCD.2013.6657034
   Cai Y, 2015, I C DEPEND SYS NETWO, P438, DOI 10.1109/DSN.2015.49
   Cai Y, 2013, DES AUT TEST EUROPE, P1285
   Chen S.-H., 2019, P 56 ANN DES AUT C D, P1
   Cheng Y, 2016, PROCEEDINGS OF USENIX ATC '16: 2016 USENIX ANNUAL TECHNICAL CONFERENCE, P379
   Cui JH, 2020, J SYST ARCHITECT, V103, DOI 10.1016/j.sysarc.2019.101685
   Cui LL, 2019, PR IEEE COMP DESIGN, P668, DOI 10.1109/ICCD46524.2019.00096
   Du Y., 2018, 2018 IEEE International Conference on Communications (ICC), P1, DOI DOI 10.1109/NAPS.2017.8107305
   Gao CM, 2019, MICRO'52: THE 52ND ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE, P493, DOI 10.1145/3352460.3358323
   Goda A, 2020, IEEE T ELECTRON DEV, V67, P1373, DOI 10.1109/TED.2020.2968079
   JESD, 2016, JESD218B01 JEDEC
   Juhyung Kim, 2010, 2010 68th Annual Device Research Conference (DRC 2010), P99, DOI 10.1109/DRC.2010.5551854
   Kang D, 2017, IEEE J SOLID-ST CIRC, V52, P210, DOI 10.1109/JSSC.2016.2604297
   Kim BS, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P281
   Kim H, 2017, IEEE INT MEM WORKSH, P7
   Kim J, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P799
   Lee S, 2018, ISSCC DIG TECH PAP I, P340, DOI 10.1109/ISSCC.2018.8310323
   Li F, 2019, POSTGRAD MED, V131, P342, DOI 10.1080/00325481.2019.1613119
   Li HB, 2016, IEEE T ELECTRON DEV, V63, P3527, DOI 10.1109/TED.2016.2593913
   Li Q, 2017, ASIA S PACIF DES AUT, P560, DOI 10.1109/ASPDAC.2017.7858383
   Liebault J, 2002, SURF INTERFACE ANAL, V34, P668, DOI 10.1002/sia.1384
   Liu WH, 2021, PROCEEDINGS OF THE 2021 DESIGN, AUTOMATION & TEST IN EUROPE CONFERENCE & EXHIBITION (DATE 2021), P1729, DOI 10.23919/DATE51398.2021.9473974
   Liu WH, 2021, IEEE T COMPUT AID D, V40, P66, DOI 10.1109/TCAD.2020.2994266
   Liu WH, 2019, DES AUT TEST EUROPE, P312, DOI [10.23919/date.2019.8714941, 10.23919/DATE.2019.8714941]
   Luo YX, 2018, INT S HIGH PERF COMP, P504, DOI 10.1109/HPCA.2018.00050
   Luo YX, 2016, IEEE J SEL AREA COMM, V34, P2294, DOI 10.1109/JSAC.2016.2603608
   Margaglia F, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P95
   Micheloni R, 2016, 3D FLASH MEMORIES, P1, DOI 10.1007/978-94-017-7512-0
   Mielke NR, 2017, P IEEE, V105, P1725, DOI 10.1109/JPROC.2017.2725738
   Nie Shiqiang, 2020, 57 ACM IEEE DES AUT, P16
   Papandreou N., 2019, INT RELIAB PHY SYM, P1
   Papandreou N, 2014, PR GR LAK SYMP VLSI, P151, DOI 10.1145/2591513.2591594
   Parnell T, 2014, IEEE GLOB COMM CONF, P2351, DOI 10.1109/GLOCOM.2014.7037159
   Peleato B, 2015, IEEE T COMMUN, V63, P3069, DOI 10.1109/TCOMM.2015.2453413
   Wang KL, 2019, SCI CHINA INFORM SCI, V62, DOI 10.1007/s11432-018-9490-1
   Wang QP, 2020, PROCEEDINGS OF THE 2020 USENIX ANNUAL TECHNICAL CONFERENCE, P713
   Wu F, 2020, IEEE T COMPUT AID D, V39, P909, DOI 10.1109/TCAD.2019.2897706
   Wu F, 2018, PR IEEE COMP DESIGN, P381, DOI 10.1109/ICCD.2018.00064
   Xiong Q, 2018, ACM T STORAGE, V14, DOI 10.1145/3162616
   Yamazaki S., 2016, INT RELIAB PHY SYM
   Zhang M, 2020, IEEE T ELECTRON DEV, V67, P5490, DOI 10.1109/TED.2020.3030867
   Zhang M, 2020, DES AUT TEST EUROPE, P302, DOI 10.23919/DATE48585.2020.9116324
   Zhang M, 2019, IEEE T COMPUT AID D, V38, P2312, DOI 10.1109/TCAD.2018.2878132
   Zhao NN, 2018, INT PARALL DISTRIB P, P1163, DOI 10.1109/IPDPS.2018.00125
NR 47
TC 9
Z9 9
U1 3
U2 49
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAY
PY 2022
VL 18
IS 2
AR 16
DI 10.1145/3491230
PG 25
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 1H3XN
UT WOS:000796478700007
DA 2024-07-18
ER

PT J
AU Kim, S
   Xu, M
   Kashyap, S
   Yoon, J
   Xu, W
   Kim, T
AF Kim, Seulbae
   Xu, Meng
   Kashyap, Sanidhya
   Yoon, Jungyeon
   Xu, Wen
   Kim, Taesoo
TI Finding Bugs in File Systems with an Extensible Fuzzing Framework
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article; Proceedings Paper
CT 27th ACM Symposium on Operating Systems Principles (SOSP)
CY OCT 27-30, 2019
CL Huntsville, CANADA
SP Assoc Comp Machinery, ACM SIGOPS, USENIX, Akamai, Alibaba, Amazon, ByteDance, Facebook, Huawei, Microsoft, NetApp, NSF, SIGOPS, Twitter, Vector Inst, VMware, Univ Waterloo, Ant Financial, Alipay
DE File systems; fuzzing; bug finding; crash consistency
AB File systems are too large to be bug free. Although handwritten test suites have been widely used to stress file systems, they can hardly keep up with the rapid increase in file system size and complexity, leading to new bugs being introduced. These bugs come in various flavors: buffer overflows to complicated semantic bugs. Although bug-specific checkers exist, they generally lack a way to explore file system states thoroughly. More importantly, no turnkey solution exists that unifies the checking effort of various aspects of a file system under one umbrella.
   In this article, to highlight the potential of applying fuzzing to find any type of file system bugs in a generic way, we propose Hydra, an extensible fuzzing framework. Hydra provides building blocks for file system fuzzing, including input mutators, feedback engines, test executors, and bug post-processors. As a result, developers only need to focus on building the core logic for finding bugs of their interests. We showcase the effectiveness of Hydra with four checkers that hunt crash inconsistency, POSIX violations, logic assertion failures, and memory errors. So far, Hydra has discovered 157 new bugs in Linux file systems, including three in verified file systems (FSCQ and Yxv6).
C1 [Kim, Seulbae; Xu, Meng; Kashyap, Sanidhya; Yoon, Jungyeon; Xu, Wen; Kim, Taesoo] Georgia Inst Technol, 756 West Peachtree St NW, Atlanta, GA 30308 USA.
C3 University System of Georgia; Georgia Institute of Technology
RP Kim, S (corresponding author), Georgia Inst Technol, 756 West Peachtree St NW, Atlanta, GA 30308 USA.
EM seulbae@gatech.edu; meng.xu@gatech.edu; sanidhya@gatech.edu;
   jungyeon@gatech.edu; wen.xu@gatech.edu; taesoo@gatech.edu
OI Kim, Seulbae/0000-0001-9990-7953
FU NSF [CNS-1563848, CNS-1704701, CRI-1629851, CNS-1749711]; ONR
   [N00014-18-1-2662, N00014-15-1-2162, N00014-17-1-2895]; DARPA AIMEE;
   ETRI IITP/KEIT [2014-3-00035]
FX This research was supported in part by NSF awards CNS-1563848,
   CNS-1704701, CRI-1629851, and CNS-1749711 ONR under grants
   N00014-18-1-2662, N00014-15-1-2162, N00014-17-1-2895, DARPA AIMEE, and
   ETRI IITP/KEIT[2014-3-00035], and gifts from Facebook, Mozilla, Intel,
   VMware, and Google.
CR [Anonymous], 2015, P 25 ACM S OP SYST P
   [Anonymous], 2015, P 25 ACM S OP SYST P
   [Anonymous], 2017, P 26 ACM S OP SYST P
   [Anonymous], 2007, P 29 INT C SOFTW ENG
   Bacik Josef, 2017, BTRFS ADD EXTENT REF
   Bartlett W, 2004, IEEE T DEPEND SECURE, V1, P87, DOI 10.1109/TDSC.2004.4
   Bohme M., 2016, P 23 ACM C COMP COMM
   Bohme M., 2017, P 24 ACM C COMP COMM
   Bornholt J, 2016, ACM SIGPLAN NOTICES, V51, P83, DOI 10.1145/2954679.2872406
   Cao M, 2007, P USENIX LIN STOR FI
   Chajed Tej, 2018, FSCQ DEV COMMENT LOG
   Chen Haogang, 2017, P 26 ACM S OP SYST P
   Corbett P., 2004, P 3 USENIX C FIL STO, P1
   Fonseca Pedro, 2014, 11 USENIX S OPERATIN
   Fryer Daniel, 2012, P 10 USENIX C FIL ST
   Gan S., 2018, P 39 IEEE S SEC PRIV
   Gribincea B., 2009, EXT4 DATA LOSS
   Han H., 2017, P 24 ACM C COMP COMM
   Ileri Atalay, 2018, P 13 USENIX S OP SYS
   Jeong Dae R., 2019, P 40 IEEE S SEC PRIV
   Jones D., 2018, LINUX SYSTEM CALL FU
   Kara J., 2014, EXT4 FORBID J ASYNC
   Kerrisk Michael, 2019, FSYNC FDATASYNC SYNC
   Kim S., 2019, P 27 ACM S OP SYST P
   Koskinen E., 2016, P 43 ACM S PRINC PRO, P97
   LLVM Dev Team, 2019, LIBFUZZER LIB COV GU
   Lu K., 2017, P 2017 ANN NETW DIST P 2017 ANN NETW DIST
   Lu LY, 2014, ACM T STORAGE, V10, DOI 10.1145/2560012
   Lu Shan, 2005, P WORKSH EV SOFTW DE, V5
   Min C., 2015, P 25 ACM S OP SYST P P 25 ACM S OP SYST P
   MITRE Corporation, 2009, CVE-2009-1235
   Mohan Jayashree, 2018, P 13 USENIX S OP SYS
   Molnar Ingo, 2019, RUNTIME LOCKING CORR
   NCC Group, 2017, AFL QEMU FUZZ FULL S
   Pailoor S, 2018, PROCEEDINGS OF THE 27TH USENIX SECURITY SYMPOSIUM, P729
   Peng Hui, 2018, P 39 IEEE S SEC PRIV
   Prabhakaran Vijayan, 2005, P 20 ACM S OP SYST P
   Purdila Octavian, 2010, P 9 ROED INT C ROEDU
   Rawat S., 2017, P 24 ACM C COMP COMM
   Red Hat Inc, 2018, UT MAN XFS FIL
   Rodeh O, 2013, ACM T STORAGE, V9, DOI 10.1145/2501620.2501623
   Ryabinin A., 2014, UBSan: run-time undefined behavior sanity checker
   Schumilo S, 2017, PROCEEDINGS OF THE 26TH USENIX SECURITY SYMPOSIUM (USENIX SECURITY '17), P167
   Sigurbjarnarson Helgi, 2016, P 12 USENIX S OP SYS
   Xu Wen, 2019, P 40 IEEE S SEC PRIV
   Yang Junfeng, 2006, P 27 IEEE S SEC PRIV
   Yang Junfeng, 2004, P 6 USENIX S OP SYST
   Yang Junfeng, 2006, P 7 USENIX S OP SYST
   Yu Chao, 2018, F2FS DISABLE F2FS CH
   Zalewski Michal, 2014, BASH BUG OTHER 2 RCE
   Zalewski Michal, 2019, AM FUZZY LOP 2 52B
   Zeller Andreas, 2019, DELTA DEBUGGING AUTO
NR 52
TC 4
Z9 4
U1 2
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUN
PY 2020
VL 16
IS 2
AR 10
DI 10.1145/3391202
PG 35
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA OJ1RC
UT WOS:000583743900003
OA Bronze
DA 2024-07-18
ER

PT J
AU Chen, F
   Hou, BB
   Lee, RB
AF Chen, Feng
   Hou, Binbing
   Lee, Rubao
TI Internal Parallelism of Flash Memory-Based Solid-State Drives
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Experimentation; Measurement; Performance; Flash memory; solid
   state drive; internal parallelism; storage systems
AB A unique merit of a solid-state drive (SSD) is its internal parallelism. In this article, we present a set of comprehensive studies on understanding and exploiting internal parallelism of SSDs. Through extensive experiments and thorough analysis, we show that exploiting internal parallelism of SSDs can not only substantially improve input/output (I/O) performance but also may lead to some surprising side effects and dynamics. For example, we find that with parallel I/Os, SSD performance is no longer highly sensitive to access patterns (random or sequential), but rather to other factors, such as data access interferences and physical data layout. Many of our prior understandings about SSDs also need to be reconsidered. For example, we find that with parallel I/Os, write performance could outperform reads and is largely independent of access patterns, which is opposite to our long-existing common understanding about slow random writes on SSDs. We have also observed a strong interference between concurrent reads and writes as well as the impact of physical data layout to parallel I/O performance. Based on these findings, we present a set of case studies in database management systems, a typical data-intensive application. Our case studies show that exploiting internal parallelism is not only the key to enhancing application performance, and more importantly, it also fundamentally changes the equation for optimizing applications. This calls for a careful reconsideration of various aspects in application and system designs. Furthermore, we give a set of experimental studies on new-generation SSDs and the interaction between internal and external parallelism in an SSD-based Redundant Array of Independent Disks (RAID) storage. With these critical findings, we finally make a set of recommendations to system architects and application designers for effectively exploiting internal parallelism.
C1 [Chen, Feng; Hou, Binbing] Louisiana State Univ, Dept Comp Sci & Engn, Baton Rouge, LA 70803 USA.
   [Lee, Rubao] Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA.
C3 Louisiana State University System; Louisiana State University;
   University System of Ohio; Ohio State University
RP Chen, F (corresponding author), Louisiana State Univ, Dept Comp Sci & Engn, Baton Rouge, LA 70803 USA.
EM fchen@csc.lsu.edu; bhou@csc.lsu.edu; liru@cse.ohio-state.edu
RI Hou, Binbing/K-3188-2019
FU National Science Foundation [CCF-1453705, CCF-0913150, OCI-1147522,
   CNS-1162165]; Louisiana Board of Regents [LEQSF(2014-17)-RD-A-01,
   LEQSF-EPS(2015)-PFUND-391]; Intel Corporation
FX This work was supported in part by National Science Foundation under
   grants CCF-1453705, CCF-0913150, OCI-1147522, and CNS-1162165, Louisiana
   Board of Regents under grants LEQSF(2014-17)-RD-A-01 and
   LEQSF-EPS(2015)-PFUND-391, as well as generous support from Intel
   Corporation.
CR Abadi D.J., 2008, P 2008 ACM SIGMOD IN, P967, DOI DOI 10.1145/1376616.1376712
   Agrawal D., 2009, P 35 INT C VER LARG
   [Anonymous], 2010, P 8 USENIX C FIL STO
   [Anonymous], 2011, PROC 9 USENIX C FILE
   [Anonymous], 2008, P 2008 USENIX ANN TE
   [Anonymous], P 4 BIENN C INN DAT
   [Anonymous], 2010, P USENIX C FIL STOR
   [Anonymous], 2014, P 19 INT C ARCH SUPP
   [Anonymous], 2010, P 8 USENIX C FIL STO
   Birrell A., 2005, DESIGN HIGH PERFORMA
   Canim M., 2009, P 35 INT C VER LARG
   Chen F., 2009, P ACM INT C MEAS MOD
   Chen F, 2014, PROCEEDINGS OF THE ASME 9TH INTERNATIONAL MANUFACTURING SCIENCE AND ENGINEERING CONFERENCE, 2014, VOL 2
   Chen Feng, 2011, P IEEE 17 INT S HIGH
   Chen S., 2009, P 2009 ACM SIGMOD C
   Das D., 2014, P 2 WORKSH INT NVM F
   Denehy T. E., 2014, P 9 INT C ARCH SUPP
   DIRIK C, 2009, P 36 INT S COMP ARCH
   Do J., 2009, P 5 INT WORKSH DAT M
   Ellefson Janene, 2013, 2013 FLASH MEM SUMM
   Gal E, 2005, ACM COMPUT SURV, V37, P138, DOI 10.1145/1089733.1089735
   GRAEFE G, 1993, COMPUT SURV, V25, P73, DOI 10.1145/152610.152611
   GRAEFE G, 2007, P 3 INT WORKSH DAT M
   Guo Xufeng, 2013, P IEEE 21 INT S MOD
   Hu Y, 2013, IEEE T COMPUT, V62, P1141, DOI 10.1109/TC.2012.60
   IYER S, 2001, P 18 S OP SYST PRINC
   Jiang S., 2005, P 3 USENIX C FIL STO
   Jung  M., 2012, P 4 USENIX WORKSH HO
   Jung Myoungsoo, 2014, P 2014 IEEE 20 INT S
   Jung Myoungsoo, 2013, P ACM SIGMETRICS INT
   Jung Myoungsoo, 2012, P 39 ANN INT S COMP
   Jung Myoungsoo, 2014, P 41 INT S COMP ARCH
   Kgil T., 2008, P 35 INT C COMP ARCH
   Koltsidas I., 2008, P 34 INT C VER LARG
   Lee R., 2009, P 35 INT C VER LARG
   Lee S., 2008, P 2008 ACM SIGMOD C
   Lee S., 2009, P 2009 ACM SIGMOD C
   Lee S., 2007, P 2007 ACM SIGMOD C
   Li Y., 2009, P 25 INT C DAT ENG I
   MESNIER M, 2011, INTEL OPEN STORAGE T
   Mesnier M., 2007, P 2007 ACM INT C MEA
   Micron, 2007, MICR 8 16 32 64GB SL
   Nath S., 2008, P 34 INT C VER LARG
   ONeil P E, 2009, The Star Schema Benchmark (SSB)
   PARK C, 2006, P 21 IEEE NONV SEM M
   Park Stan, 2012, P 10 USENIX C FIL ST
   Patterson D. A., 1988, P 1988 ACM SIGMOD IN
   PC Perspective, 2009, OCZ AP SER 250GB SOL
   Polte M., 2008, P 3 PET DAT STOR WOR
   Pritchett T., 2010, P INT S COMP ARCH IS
   Roh Hongchan, 2012, P VLDB END VLDB 12
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   Samsung, 2007, DAT K9LBG08U0M
   SATA, 2011, SER ATA REV 2 6
   Shah M. A., 2008, P 4 INT WORKSH DAT M
   Shah S, 2007, SOFTWARE PRACT EXPER, V37, P1515, DOI 10.1002/spe.816
   Stoica R., 2009, P 5 INT WORKSH DAT M
   Stonebraker Michael, 2007, P 3 BIENN C INN DAT
   Tsirogiannis D., 2009, P 2009 ACM SIGMOD C
   Wang Peng, 2014, P 9 EUR C COMP SYST, P1
   Wu Guanying, 2014, P 10 USENIX C FIL ST
   Youngjae Kim, 2011, P 27 IEEE S MASS STO
NR 62
TC 59
Z9 68
U1 0
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUN
PY 2016
VL 12
IS 3
AR 13
DI 10.1145/2818376
PG 39
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DQ7YV
UT WOS:000379426200004
DA 2024-07-18
ER

PT J
AU Moon, S
   Reddy, ALN
AF Moon, Sangwhan
   Reddy, A. L. Narasimha
TI Does RAID Improve Lifetime of SSD Arrays?
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Reliability; Flash memory; SSD; write amplification; RAID; lifetime;
   MTTDL
ID FLASH; RELIABILITY
AB Parity protection at the system level is typically employed to compose reliable storage systems. However, careful consideration is required when SSD-based systems employ parity protection. First, additional writes are required for parity updates. Second, parity consumes space on the device, which results in write amplification from less efficient garbage collection at higher space utilization.
   This article analyzes the effectiveness of SSD-based RAID and discusses the potential benefits and drawbacks in terms of reliability. A Markov model is presented to estimate the lifetime of SSD-based RAID systems in different environments. In a small array, our results show that parity protection provides benefit only with considerably low space utilizations and low data access rates. However, in a large system, RAID improves data lifetime even when we take write amplification into account.
C1 [Moon, Sangwhan; Reddy, A. L. Narasimha] Texas A&M Univ, Dept Elect & Comp Engn, College Stn, TX 77843 USA.
C3 Texas A&M University System; Texas A&M University College Station
RP Moon, S (corresponding author), Texas A&M Univ, Dept Elect & Comp Engn, College Stn, TX 77843 USA.
EM sangwhan@tamu.edu; reddy@tamu.edu
CR [Anonymous], 2011, PROC 9 USENIX C FILE
   [Anonymous], 1988, A case for redundant arrays of inexpensive disks RAID, DOI DOI 10.1145/50202.50214
   [Anonymous], 2011, P 27 IEEE S MASS STO
   [Anonymous], 2011, P FAST 2
   [Anonymous], 2010, P 8 USENIX C FIL STO
   Baek S., 2007, PROC 7 ACM IEEE INT, P154
   Balakrishnan M, 2010, ACM T STORAGE, V6, DOI 10.1145/1807060.1807061
   Birrell A., 2007, Operating Systems Review, V41, P88, DOI 10.1145/1243418.1243429
   Boboila S., 2010, P 8 USENIX C FIL STO, P9
   Cai Y, 2012, DES AUT TEST EUROPE, P521
   Caulfield A. M., 2009, P 42 ANN IEEE ACM IN, P24
   Chang LP, 2007, APPLIED COMPUTING 2007, VOL 1 AND 2, P1126, DOI 10.1145/1244002.1244248
   Chen BN, 2008, IEEE WRK SIG PRO SYS, P94, DOI 10.1109/SIPS.2008.4671744
   de la Iglesia E., 2012, FLASH MEM SUMM SANT
   Desnoyers Peter., 2012, P INT SYSTEMS STORAG, P12
   Fujitsu, 2014, FUJITSU PRIMERGY SER
   Gal E, 2005, ACM COMPUT SURV, V37, P138, DOI 10.1145/1089733.1089735
   Gathman J., 2013, FLASH MEM SUMM SANT
   Greenan Kevin M., 2009, P 5 WORKSH HOT TOP S
   Gregori S, 2003, P IEEE, V91, P602, DOI 10.1109/JPROC.2003.811709
   Grochowski E., 2012, FLASH MEM SUMM SANT
   Grupp Laura M., 2012, P 10 USENIX C FIL ST, P2
   Hairong Sun, 2011, P 7 IEEE INT WORKSH
   HP, 2016, HP ENT SOL STAT DRIV
   Hu X.-Y., 2009, System and Storage Conference (SYSTOR), DOI DOI 10.1145/1534530.1534544
   Jagmohan A., 2010, Mass Storage Systems and Technologies (MSST), 2010 IEEE 26th Symposium on, P1
   Jeremic Nikolaus., 2011, P 4 ANN INT C SYSTEM, P14
   Kim HJ, 2002, IEICE T INF SYST, VE85D, P950
   Kim Hyojun, 2008, P 6 USENIX C FILE ST
   Lee Jongmin., 2007, Proceedings of the 7th ACM IEEE International Conference on Embedded Software (EMSOFT '07), P174
   Mao Bo., 2010, Parallel Distributed Processing (IPDPS), 2010 IEEE International Symposium on, P1
   Mielke N, 2008, INT RELIAB PHY SYM, P9, DOI 10.1109/RELPHY.2008.4558857
   Moon S., 2013, 5 USENIX WORKSH HOT
   Moon S., 2012, 012 IEEE 28th Symposium on Mass Storage Systems and Technologies (MSST), P1
   Oh Y., 2012, USENIX FAST, P25
   Ousterhout J., 1989, Operating Systems Review, V23, P11, DOI 10.1145/65762.65765
   Plank J. S., 2005, P FAST 2005 4 USENIX
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   SALEH AM, 1990, IEEE T RELIAB, V39, P114, DOI 10.1109/24.52622
   Saxena M., 2012, P 7 ACM EUR C COMP S, P267, DOI DOI 10.1145/2168836.2168863
   Saxena Mohit., 2013, Presented as part of the 11th USENIX Conference on File and Storage Technologies (FAST 13), P215
   Schwarz TJE, 2004, IEEE COMPUTER SOCIETY'S 12TH ANNUAL INTERNATIONAL SYMPOSIUM ON MODELING, ANALYSIS, AND SIMULATION OF COMPUTER AND TELECOMMUNICATIONS SYSTEMS - PROCEEDINGS, P409, DOI 10.1109/MASCOT.2004.1348296
   Spec Inc, 2011, FLASH MEM SUMM SANT
   Stead S., 2012, FLASH MEM SUMM SANT
   Sun F., 2006, IEEE WORKSH SIGN PRO
   Thomas K., 2012, SOLID STATE DRIVES B
   Xin Q, 2003, IEEE S MASS STOR SYS, P146, DOI 10.1109/MASS.2003.1194851
   Yaakobi E., 2012, 2012 International Conference on Computing, Networking and Communications (ICNC), P486, DOI 10.1109/ICCNC.2012.6167470
   Yaakobi E, 2010, IEEE GLOBE WORK, P1856, DOI 10.1109/GLOCOMW.2010.5700263
   Zhang Yu-liang, 2012, Journal of Insect Science (Tucson), V12, P1
NR 50
TC 22
Z9 22
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUN
PY 2016
VL 12
IS 3
AR 11
DI 10.1145/2764915
PG 29
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DQ7YV
UT WOS:000379426200002
DA 2024-07-18
ER

PT J
AU Lee, E
   Bahn, H
   Noh, SH
AF Lee, Eunji
   Bahn, Hyokyung
   Noh, Sam H.
TI A Unified Buffer Cache Architecture that Subsumes Journaling
   Functionality via Nonvolatile Memory
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Algorithms; Performance; Buffer cache; file system; nonvolatile
   memory; journaling; reliability
AB Journaling techniques are widely used in modern file systems as they provide high reliability and fast recovery from system failures. However, it reduces the performance benefit of buffer caching as journaling accounts for a bulk of the storage writes in real system environments. To relieve this problem, we present a novel buffer cache architecture that subsumes the functionality of caching and journaling by making use of nonvolatile memory such as PCM or STT-MRAM. Specifically, our buffer cache supports what we call the in-place commit scheme. This scheme avoids logging, but still provides the same journaling effect by simply altering the state of the cached block to frozen. As a frozen block still provides the functionality of a cache block, we show that in-place commit does not degrade cache performance. We implement our scheme on Linux 2.6.38 and measure the throughput and execution time of the scheme with various file I/O benchmarks. The results show that our scheme improves the throughput and execution time by 89% and 34% on average, respectively, compared to the existing Linux buffer cache with ext4 without any loss of reliability.
C1 [Lee, Eunji] Seoul Natl Univ, Dept Comp Sci & Engn, Seoul 151, South Korea.
   [Bahn, Hyokyung] Ewha Womans Univ, Dept Comp Sci & Engn, Seoul, South Korea.
   [Noh, Sam H.] Hongik Univ, Sch Comp & Informat Engn, Seoul, South Korea.
C3 Seoul National University (SNU); Ewha Womans University; Hongik
   University
RP Bahn, H (corresponding author), Ewha Womans Univ, Dept Comp Sci & Engn, Seoul, South Korea.
EM bahn@ewha.ac.kr
FU National Research Foundation of Korea (NRF); Korea Government (MEST)
   [2011-0028825, 2012R1A2A2A01045733]
FX This work was supported by the National Research Foundation of Korea
   (NRF) grant funded by the Korea Government (MEST), No. 2011-0028825 and
   No. 2012R1A2A2A01045733.
CR [Anonymous], 2012, DEN MEM REP 44TBYTE
   [Anonymous], 1997, Tech. Rep. TR3022
   [Anonymous], 2011, 7 INT C WIR COMM NET
   Baek S., 2006, P IEEE PIMRC HELS FI, P1
   Baek S, 2013, ACM T EMBED COMPUT S, V12, DOI 10.1145/2442116.2442131
   Chen P. M., 1996, P 7 ACM INT C ARCH S
   Condit J, 2009, SOSP'09: PROCEEDINGS OF THE TWENTY-SECOND ACM SIGOPS SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P133
   Edel NK, 2004, IEEE COMPUTER SOCIETY'S 12TH ANNUAL INTERNATIONAL SYMPOSIUM ON MODELING, ANALYSIS, AND SIMULATION OF COMPUTER AND TELECOMMUNICATIONS SYSTEMS - PROCEEDINGS, P596, DOI 10.1109/MASCOT.2004.1348317
   Fang R., 2011, P 27 IEEE INT C DAT, P11
   Ipek E, 2010, ACM SIGPLAN NOTICES, V45, P3, DOI 10.1145/1735971.1736023
   Kim H., 2012, P USENIX C FIL STOR
   Lee B. C., 2009, P 36 ACM IEEE INT S
   Lee BC, 2010, COMMUN ACM, V53, P99, DOI 10.1145/1785414.1785441
   Lee Eunji., 2012, Mass Storage Systems and Technologies (MSST), 2012 IEEE 28th Symposium on, P1
   Mogul J. C., 2009, P 12 USENIX WORKSH H
   Nirschl T, 2007, INT EL DEVICES MEET, P461, DOI 10.1109/IEDM.2007.4418973
   Norcutt W., IOZone filesystem benchmark
   Phillips D., 2007, P LIN S, P135
   Prabhakaran  V., 2005, P USENIX ANN TECHN C
   Proctor A., 2012, NONVOLATILE MEMORY I
   Qureshi MK, 2009, CONF PROC INT SYMP C, P24, DOI 10.1145/1555815.1555760
   Rubin M., 2011, P LIN FDN COLL SUMM
   Soyoon Lee, 2011, Proceedings of the 2011 IEEE 19th International Symposium on Modelling, Analysis & Simulation of Computer and Telecommunication Systems (MASCOTS 2011), P168, DOI 10.1109/MASCOTS.2011.68
   Yang BD, 2007, IEEE INT SYMP CIRC S, P3014, DOI 10.1109/ISCAS.2007.377981
   Young Jin Kim, 2010, Journal of KISS: Computing Practices, V16, P247
   Zhou P, 2009, CONF PROC INT SYMP C, P14, DOI 10.1145/1555815.1555759
NR 26
TC 15
Z9 15
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JAN
PY 2014
VL 10
IS 1
AR 1
DI 10.1145/2560010
PG 17
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AB6EC
UT WOS:000331879400001
DA 2024-07-18
ER

PT J
AU Natanzon, A
   Bachmat, E
AF Natanzon, Assaf
   Bachmat, Eitan
TI Dynamic Synchronous/Asynchronous Replication
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Management; Remote replication
AB Online, remote, data replication is critical for today's enterprise IT organization. Availability of data is key to the success of the organization. A few hours of downtime can cost from thousands to millions of dollars With increasing frequency, companies are instituting disaster recovery plans to ensure appropriate data availability in the event of a catastrophic failure or disaster that destroys a site (e.g. flood, fire, or earthquake).
   Synchronous and asynchronous replication technologies have been available for a long period of time. Synchronous replication has the advantage of no data loss, but due to latency, synchronous replication is limited by distance and bandwidth. Asynchronous replication on the other hand has no distance limitation, but leads to some data loss which is proportional to the data lag. We present a novel method, implemented within EMC Recover-Point, which allows the system to dynamically move between these replication options without any disruption to the I/O path. As latency grows, the system will move from synchronous replication to semi-synchronous replication and then to snapshot shipping. It returns to synchronous replication as more bandwidth is available and latency allows.
C1 [Natanzon, Assaf] EMC Corp, Herzliyya, Israel.
   [Natanzon, Assaf; Bachmat, Eitan] Ben Gurion Univ Negev, Dept Comp Sci, IL-84105 Beer Sheva, Israel.
C3 Dell Incorporated; Dell EMC; Dell EMC Israel; Ben Gurion University
RP Bachmat, E (corresponding author), Ben Gurion Univ Negev, Dept Comp Sci, IL-84105 Beer Sheva, Israel.
EM ebachmat@cs.bgu.ac.il
OI Bachmat, Eitan/0000-0001-7153-100X
FU ISF [580/11]; EMC
FX The second author was partially supported by an ISF research grant
   580/11, And an EMC research grant.
CR [Anonymous], P ISR EXP SYST C SYS
   Azagury AC, 2003, IBM SYST J, V42, P268, DOI 10.1147/sj.422.0268
   Chang F, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE FAST'02 CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P103
   Cooper BF, 2008, PROC VLDB ENDOW, V1, P1277
   Cormen T.H., 1990, Introduction to Algorithms
   Ghemawat S., 2003, ACM SIGOPS OPERATING, V37, P29, DOI [DOI 10.1145/945445.945450, DOI 10.1145/1165389.945450, 10.1145/1165389.945450]
   HITZ D, 1994, PROCEEDINGS OF THE WINTER 1994 USENIX CONFERENCE, P235
   Ji MW, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE GENERAL TRACK, P253
   Keeton K, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P59
   Krishnamurthy S, 2003, IEEE T PARALL DISTR, V14, P1112, DOI 10.1109/TPDS.2003.1247672
   Lillibridge M., 2009, P 7 USENIX C FIL STO
   Liskov B., 1991, Operating Systems Review, V25, P226, DOI 10.1145/121133.121169
   Matthews J. N., 1997, Operating Systems Review, V31, P238, DOI 10.1145/269005.266700
   Patterson H, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE FAST'02 CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P117
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   Sovran Y, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P385
   Strunk JD, 2000, USENIX ASSOCIATION PROCEEDINGS OF THE FOURTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P165
   Wang YL, 2007, ARES 2007: SECOND INTERNATIONAL CONFERENCE ON AVAILABILITY, RELIABILITY AND SECURITY, PROCEEDINGS, P499
   Zhu B, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P269
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
NR 27
TC 4
Z9 8
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2013
VL 9
IS 3
AR 8
DI 10.1145/2508011
PG 19
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 216XX
UT WOS:000324321200002
OA Bronze
DA 2024-07-18
ER

PT J
AU Chang, YH
   Hsieh, CK
   Huang, PC
   Hsiu, PC
AF Chang, Yuan-Hao
   Hsieh, Cheng-Kang
   Huang, Po-Chun
   Hsiu, Pi-Cheng
TI A Caching-Oriented Management Design for the Performance Enhancement of
   Solid-State Drives
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Experimentation; Management; Measurement; Performance; Flash
   memory; solid-state disk; performance; cache
ID FLASH FILE SYSTEM; MEMORY; RELIABILITY
AB While solid-state drives are excellent alternatives to hard disks in mobile devices, a number of performance and reliability issues need to be addressed. In this work, we design an efficient flash management scheme for the performance improvement of low-cost MLC flash memory devices. Specifically, we design an efficient flash management scheme for multi-chipped flash memory devices with cache support, and develop a two-level address translation mechanism with an adaptive caching policy. We evaluated the approach on real workloads. The results demonstrate that it can improve the performance of multi-chipped solid-state drives through logical-to-physical mappings and concurrent accesses to flash chips.
C1 [Chang, Yuan-Hao] Acad Sinica, Inst Informat Sci, Taipei 115, Taiwan.
   [Hsieh, Cheng-Kang; Hsiu, Pi-Cheng] Acad Sinica, Res Ctr Informat Technol Innovat, Taipei 115, Taiwan.
   [Huang, Po-Chun] Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 106, Taiwan.
C3 Academia Sinica - Taiwan; Academia Sinica - Taiwan; National Taiwan
   University
RP Chang, YH (corresponding author), Acad Sinica, Inst Informat Sci, Taipei 115, Taiwan.
EM johnson@iis.sinica.edu.tw; changun@citi.sinica.edu.tw;
   b91048@csie.sinica.edu.tw; pchsiu@citi.sinica.edu.tw
RI Hsiu, Pi-Cheng/ABC-3210-2020; Hsiu, Pi-Cheng/GSJ-1102-2022; Chang,
   Yuan-Hao/ABA-6935-2020
OI Hsiu, Pi-Cheng/0000-0001-8035-4033; Chang, Yuan-Hao/0000-0002-1282-2111
FU National Science Council [100-2628-E-027-008-MY2, 100-2220-E-001-001]
FX This work was supported in part by the National Science Council under
   grant Nos. 100-2628-E-027-008-MY2 and 100-2220-E-001-001.
CR Agrawal Nitin, 2008, P USENIX ANN TECHN C, P57
   [Anonymous], 2005, File System Forensic Analysis
   [Anonymous], 2020, 128 LAYER NAND FLASH
   Ban A., 2004, Google Patents. US Patent, Patent No. [6,732,221, 6732221]
   Ben-Aroya A, 2006, LECT NOTES COMPUT SC, V4168, P100
   Birrell A., 2007, Operating Systems Review, V41, P88, DOI 10.1145/1243418.1243429
   Caulfield AM, 2009, ACM SIGPLAN NOTICES, V44, P217, DOI 10.1145/1508284.1508270
   Chang Li-Pin., 2004, Proceedings of the 2004 ACM Symposium on Applied Computing (SAC '04), P862
   Chang LP, 2002, EIGHTH IEEE REAL-TIME AND EMBEDDED TECHNOLOGY AND APPLICATIONS SYMPOSIUM, PROCEEDINGS, P187, DOI 10.1109/RTTAS.2002.1137393
   CHANG Y.-H., 2007, P 44 AMC IEEE DES AU
   CHANG Y.-H., 2009, P S ACM IEEE DES AUT
   CHANG Y.-H., 2009, P 46 ACM IEEE DES AU
   Chang YH, 2007, DES AUT CON, P212, DOI 10.1109/DAC.2007.375155
   Chang YH, 2011, IEEE T COMPUT, V60, P305, DOI 10.1109/TC.2010.126
   DOH I. H., 2009, P ACM S APPL COMP SA
   DRAMeXchange, 2009, FLASH CONTR PRIC
   Gupta Aayush, 2009, SIGPLAN Notices, V44, P229, DOI 10.1145/1508284.1508271
   Intel Corporation, 1998, UND FLASH TRANSL LAY
   INTEL CORPORATION, 1995, FTL LOGG EXCH DAT FT
   Jo H, 2006, IEEE T CONSUM ELECTR, V52, P485, DOI 10.1109/TCE.2006.1649669
   Kang SY, 2009, IEEE T COMPUT, V58, P744, DOI 10.1109/TC.2008.224
   KAWAGUCHI A, 1995, PROCEEDINGS OF THE 1995 USENIX TECHNICAL CONFERENCE, P155
   Kim H, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P239, DOI 10.1145/1378600.1378627
   Kim S, 2008, ICISS 2008: INTERNATIONAL CONFERENCE ON INFORMATION SCIENCE AND SECURITY, PROCEEDINGS, P62, DOI 10.1109/ICISS.2008.23
   Kuo T.-W., 2008, P IEEE ACM INT C COM
   Lee C, 2008, IEEE T COMPUT, V57, P1002, DOI 10.1109/TC.2008.14
   Lim SH, 2006, IEEE T COMPUT, V55, P906, DOI 10.1109/TC.2006.96
   Narayanan D., 2008, ACM T STORAGE, V4, P3
   Park S.Y., 2006, CASES 2006, DOI [10.1145/1176760.1176789, DOI 10.1145/1176760.1176789]
   Park Sungmin., 2008, Modeling, Analysis and Simulation of Computers and Telecommunication Systems, P1
   Park Y, 2008, APPLIED COMPUTING 2008, VOLS 1-3, P1498
   Samsung Electronics, 2005, K9K8G08U0M 1G 8 BIT
   Samsung Electronics, 2006, K9GAG08U0M 2G X 8BIT
   Spivak M, 2006, ACM SIGPLAN NOTICES, V41, P22, DOI 10.1145/1134650.1134655
   STMicroelectronics, 2005, NAND08GX3C2A 8GBIT M
   Traeger Avishay, 2008, ACM Transaction on Storage, V4, DOI 10.1145/1367829.1367831
   TSAI Y.-L., 2006, P IEEE INT C SENS NE, V1
   Wu Chin-Hsien., 2006, Proceedings of the 2006 IEEE/ACM International Conference on Computer-Aided Design (ICCAD '06), P601
   Xin Q, 2003, IEEE S MASS STOR SYS, P146, DOI 10.1109/MASS.2003.1194851
   YANG C.-L., 2006, P 9 IEEE INT S OBJ C
   Yim KS, 2004, IEEE T CONSUM ELECTR, V50, P192, DOI 10.1109/TCE.2004.1277861
   Young Jun Cho, 2008, 2008 6th IEEE International Conference on Industrial Informatics (INDIN), P1620, DOI 10.1109/INDIN.2008.4618363
NR 42
TC 2
Z9 2
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD FEB
PY 2012
VL 8
IS 1
AR 3
DI 10.1145/2093139.2093142
PG 21
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 990LT
UT WOS:000307633100003
DA 2024-07-18
ER

PT J
AU Xiang, LP
   Xu, YL
   Lui, JCS
   Chang, Q
   Pan, YB
   Li, RH
AF Xiang, Liping
   Xu, Yinlong
   Lui, John C. S.
   Chang, Qian
   Pan, Yubiao
   Li, Runhui
TI A Hybrid Approach to Failed Disk Recovery Using RAID-6 Codes: Algorithms
   and Performance Evaluation
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Algorithms; Reliability; Theory; Disk failure; EVENODD code; RAID
   recovery; RDP code; recovery algorithm
ID FAILURE
AB The current parallel storage systems use thousands of inexpensive disks to meet the storage requirement of applications. Data redundancy and/or coding are used to enhance data availability, for instance, Row-diagonal parity (RDP) and EVENODD codes, which are widely used in RAID-6 storage systems, provide data availability with up to two disk failures. To reduce the probability of data unavailability, whenever a single disk fails, disk recovery will be carried out. We find that the conventional recovery schemes of RDP and EVENODD codes for a single failed disk only use one parity disk. However, there are two parity disks in the system, and both can be used for single disk failure recovery. In this article, we propose a hybrid recovery approach that uses both parities for single disk failure recovery, and we design efficient recovery schemes for RDP code (RDOR-RDP) and EVENODD code (RDOR-EVENODD). Our recovery scheme has the following attractive properties: (1) "read optimality" in the sense that our scheme issues the smallest number of disk reads to recover a single failed disk and it reduces approximately 1/4 of disk reads compared with conventional schemes; (2) "load balancing property" in that all surviving disks will be subjected to the same (or almost the same) amount of additional workload in rebuilding the failed disk.
   We carry out performance evaluation to quantify the merits of RDOR-RDP and RDOR-EVENODD on some widely used disks with DiskSim. The offline experimental results show that RDOR-RDP and RDOR-EVENODD outperform the conventional recovery schemes of RDP and EVENODD codes in terms of total recovery time and recovery workload on individual surviving disk. However, the improvements are less than the theoretical value (approximately 25%), as RDOR-RDP and RDOR-EVENODD change the disk access pattern from purely sequential to a more random one compared with their conventional schemes.
C1 [Xiang, Liping; Xu, Yinlong; Chang, Qian; Pan, Yubiao; Li, Runhui] Univ Sci & Technol China, Sch Comp Sci & Technol, Hefei, Peoples R China.
   [Lui, John C. S.] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Hong Kong, Hong Kong, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese University of Hong Kong
RP Xiang, LP (corresponding author), Univ Sci & Technol China, Sch Comp Sci & Technol, Hefei, Peoples R China.
EM ylxu@ustc.edu.cn; cslui@cse.cuhk.edu.hk
FU National Nature Science Foundation of China [61073038]; National High
   Technology Research and Development Program of China [2009AAO1A348]
FX This work is supported by the National Nature Science Foundation of
   China under Grant No. 61073038 and the National High Technology Research
   and Development Program of China under Grant No. 2009AAO1A348.
CR [Anonymous], CHEET 15K 5 FIBR CHA
   [Anonymous], DDR2 SDRAM
   [Anonymous], RJ10415 IBM RES
   [Anonymous], THESIS CARNEGIE MELL
   [Anonymous], 3298 NETW APPL INC
   [Anonymous], 2003, MUCH INFORM
   [Anonymous], 2006, P 1 ACM SIGOPSEUROSY
   [Anonymous], 1993, COURSE COMBINATORICS
   BLAUM M, 1995, IEEE T COMPUT, V44, P192, DOI 10.1109/12.364531
   Bucy J. S., 2008, CMUPDL08101
   CHEN PM, 1994, ACM COMPUT SURV, V26, P145, DOI 10.1145/176979.176981
   Corbett P, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P1
   Dimakis AG, 2010, IEEE T INFORM THEORY, V56, P4539, DOI 10.1109/TIT.2010.2054295
   Emrich Tobias., 2010, Proceedings of the Sixth International Workshop on Data Management on New Hardware, DaMoN '10, P3
   Ghemawat S., 2003, ACM SIGOPS OPERATING, V37, P29, DOI [DOI 10.1145/945445.945450, DOI 10.1145/1165389.945450, 10.1145/1165389.945450]
   Greenan K.M., 2010, IEEE 26 S MASS STORA, P1, DOI DOI 10.1109/MSST.2010.5496983
   Hafner J.L., 2005, USENIX FAST, P15
   Holland M., 1993, Digest of Papers FTCS-23 The Twenty-Third International Symposium on Fault-Tolerant Computing, P422, DOI 10.1109/FTCS.1993.627345
   HOLLAND M, 1994, DISTRIB PARALLEL DAT, V2, P295, DOI 10.1007/BF01266332
   HOLLAND M, 1992, SIGPLAN NOTICES, V27, P23, DOI 10.1145/143371.143383
   Joukov N, 2007, 24TH IEEE CONFERENCE ON MASS STORAGE SYSTEMS AND TECHNOLOGIES, PROCEEDINGS, P199, DOI 10.1109/MSST.2007.4367974
   Kubiatowicz J, 2000, ACM SIGPLAN NOTICES, V35, P190, DOI 10.1145/384264.379239
   Lee JYB, 2002, IEEE T PARALL DISTR, V13, P499, DOI 10.1109/TPDS.2002.1003860
   Ma S. L., 1994, Designs, Codes and Cryptography, V4, P221, DOI 10.1007/BF01388454
   MENON J, 1992, ACM COMP AR, V20, P318, DOI 10.1145/146628.140392
   Merchant A, 1996, IEEE T COMPUT, V45, P367, DOI 10.1109/12.485575
   MUNTZ RR, 1990, VERY LARGE DATA BASES, P162
   Pinheiro E, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 5TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES ( FAST '07), P17
   Plank J., 2008, P 6 USENIX C FILE ST, P1
   Plank J. S., 2009, FAST 2009, P253
   Pless V., 1998, Introduction to the Theory of Error-Correcting Codes
   Schroeder B, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 5TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES ( FAST '07), P1
   Thomasian A, 1997, IEEE T PARALL DISTR, V8, P640, DOI 10.1109/71.595583
   Tian L., 2007, FAST, V7, P301
   Wu S., 2009, USENIX C FILE STORAG, V9, P239
   Xiang LP, 2010, PERF E R SI, V38, P119, DOI 10.1145/1811099.1811054
   Xin Q, 2004, 13TH IEEE INTERNATIONAL SYMPOSIUM ON HIGH PERFORMANCE DISTRIBUTED COMPUTING, PROCEEDINGS, P172, DOI 10.1109/HPDC.2004.1323523
   Xin Q, 2003, IEEE S MASS STOR SYS, P146, DOI 10.1109/MASS.2003.1194851
   Xu LH, 1999, IEEE T INFORM THEORY, V45, P272, DOI 10.1109/18.746809
NR 39
TC 37
Z9 38
U1 0
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD OCT
PY 2011
VL 7
IS 3
AR 11
DI 10.1145/2027066.2027071
PG 34
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 990LO
UT WOS:000307632600005
DA 2024-07-18
ER

PT J
AU Plank, JS
   Buchsbaum, AL
   Vander Zanden, BT
AF Plank, James S.
   Buchsbaum, Adam L.
   Vander Zanden, Bradley T.
TI Minimum Density RAID-6 Codes
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Reliability; Performance; Algorithms; Storage; erasure codes; fault
   tolerance; RAID; RAID-6
AB RAID-6 codes protect disk array storage systems from two-disk failures. This article presents a complete treatment of a class of RAID-6 codes, called minimum density RAID-6 codes, that have an optimal blend of performance properties. There are two families of minimal density RAID-6 codes: Blaum-Roth codes and Liberation codes, and a separate special-purpose code called the Liber8tion code. The first of these have been known since the late 1990's, while the latter two are new constructions. In this article, we motivate, demonstrate, and evaluate the minimum density codes, comparing them to EVENODD and RDP codes, which represent the state-of-the-art in RAID-6. Following that, we prove that the codes indeed fit the RAID-6 methodology, and cite their implementation in an open-source library.
C1 [Plank, James S.] Univ Tennessee, Dept Elect Engn & Comp Sci, Knoxville, TN 37996 USA.
C3 University of Tennessee System; University of Tennessee Knoxville
RP Plank, JS (corresponding author), Univ Tennessee, Dept Elect Engn & Comp Sci, Knoxville, TN 37996 USA.
EM plank@cs.utk.edu
OI Plank, James/0000-0002-9841-6076
FU National Science Foundation [CNS-0615221, CSR-1016636]
FX This work is supported by the National Science Foundation under grants
   CNS-0615221 and CSR-1016636. A. Buchsbaum completed this work while a
   member of the technical staff at AT&T Labs-Research. The precursors to
   this work are the two conference papers Plank [2008b] and Plank [2008a].
CR [Anonymous], 2010, P 8 USENIX S FIL STO
   [Anonymous], 2006, TR3298 NETW APPL
   [Anonymous], 1977, THEORY ERROR CORRE 1
   Anvin HPeter., 2007, The mathematics of RAID-6
   BAIRAVASUNDARAM L, 2008, P 6 US C FIL STOR TE
   BLAUM M, 1995, IEEE T COMPUT, V44, P192, DOI 10.1109/12.364531
   Blaum M, 1999, IEEE T INFORM THEORY, V45, P46, DOI 10.1109/18.746771
   Blomer J., 1995, Tech. Rep. TR-95-048
   Bohossian V, 2006, 2006 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY, VOLS 1-6, PROCEEDINGS, P2799, DOI 10.1109/ISIT.2006.261572
   CORBETT P, 2004, P 3 US C FIL STOR TE
   Elerath JG, 2007, I C DEPEND SYS NETWO, P175, DOI 10.1109/DSN.2007.41
   EMC Corporation, 2007, H2891 EMC CORP
   Hafner JL, 2008, IBM J RES DEV, V52, P413, DOI 10.1147/rd.524.0413
   HAFNER J. L., 2006, P INT C DEP SYST NET
   Hafner JL, 2005, USENIX ASSOCIATION PROCEEDINGS OF THE 4TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P211
   Hafner JL, 2005, USENIX ASSOCIATION PROCEEDINGS OF THE 4TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P183
   Huang C, 2005, USENIX ASSOCIATION PROCEEDINGS OF THE 4TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P197
   HUANG C, 2007, P 6 IEEE INT S NETW
   Lancaster P., 1985, The Theory of Matrices, V2nd edition
   LUO J, 2009, P INT C DEP SYST NET
   Moon TK, 2005, ERROR CORRECTION CODING: MATHEMATICAL METHODS AND ALGORITHMS
   Oprea Alina, 2010, FAST, P57
   Oral S., 2010, P 8 USENIX C FILE ST, P143
   Peterson W. W., 1972, ERROR CORRECTING COD
   Plank J. S., 2008, CS08627 U TENN
   Plank J. S., 2009, FAST 2009, P253
   PLANK J. S, 2008, P 7 IEEE INT S NETW
   Plank JS, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P97
   Plank JS, 1997, SOFTWARE PRACT EXPER, V27, P995, DOI 10.1002/(SICI)1097-024X(199709)27:9<995::AID-SPE111>3.0.CO;2-6
   PLANK JS, 2005, P INT C DEP SYST NET
   REED IS, 1960, J SOC IND APPL MATH, V8, P300, DOI 10.1137/0108018
   STORAGE NETWORKING INDUSTRY ASSOCIATION, 2010, 2010 SNIA DICT
   Storer MW, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P1, DOI 10.1145/1456469.1456471
   Tu T., 2010, FAST'10, P129
   VANLINT JH, 1982, INTRO CODING THEORY
   WYLIE J. J., 2007, P INT C DEP SYST NET
   Xu LH, 1999, IEEE T INFORM THEORY, V45, P1817, DOI 10.1109/18.782102
   Xu LH, 1999, IEEE T INFORM THEORY, V45, P272, DOI 10.1109/18.746809
   Zhu B, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P269
NR 39
TC 11
Z9 14
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAY
PY 2011
VL 6
IS 4
AR 16
DI 10.1145/1970338.1970340
PG 22
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 990KX
UT WOS:000307630800002
DA 2024-07-18
ER

PT J
AU Zhang, M
   Hua, Y
   Zuo, PF
   Liu, LR
AF Zhang, Ming
   Hua, Yu
   Zuo, Pengfei
   Liu, Lurong
TI Localized Validation Accelerates Distributed Transactions on
   Disaggregated Persistent Memory
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Disaggregated data center; persistent memory; distributed transaction
   processing; one-sided remote direct memory access
AB Persistent memory (PM) disaggregation significantly improves the resource utilization and failure isolation to build a scalable and cost-effective remote memory pool in modern data centers. However, due to offering limited computing power and overlooking the bandwidth and persistence properties of real PMs, existing distributed transaction schemes, which are designed for legacy DRAM-based monolithic servers, fail to efficiently work on the disaggregated PM. In this article, we propose FORD, a Fast One-sided RDMA-based Distributed transaction system for the new disaggregated PM architecture. FORD thoroughly leverages one-sided remote direct memory access to handle transactions for bypassing the remote CPU in the PM pool. To reduce the round trips, FORD batches the read and lock operations into one request to eliminate extra locking and validations for the read-write data. To accelerate the transaction commit, FORD updates all remote replicas in a single round trip with parallel undo logging and data visibility control. Moreover, considering the limited PM bandwidth, FORD enables the backup replicas to be read to alleviate the load on the primary replicas, thus improving the throughput. To efficiently guarantee the remote data persistency in the PM pool, FORD selectively flushes data to the backup replicas to mitigate the network overheads. Nevertheless, the original FORD wastes some validation round trips if the read-only data are not modified by other transactions. Hence, we further propose a localized validation scheme to transfer the validation operations for the read-only data from remote to local as much as possible to reduce the round trips. Experimental results demonstrate that FORD significantly improves the transaction throughput by up to 3x and decreases the latency by up to 87.4% compared with state-of-the-art systems.
C1 [Zhang, Ming; Hua, Yu; Zuo, Pengfei; Liu, Lurong] Huazhong Univ Sci & Technol, Luoyu Rd 1037, Wuhan 430074, Hubei, Peoples R China.
C3 Huazhong University of Science & Technology
RP Hua, Y (corresponding author), Huazhong Univ Sci & Technol, Luoyu Rd 1037, Wuhan 430074, Hubei, Peoples R China.
EM csmzhang@hust.edu.cn; csyhua@hust.edu.cn; pfzuo@hust.edu.cn;
   lrliu@hust.edu.cn
RI Zhang, Ming/ACT-2330-2022
OI Zhang, Ming/0000-0001-6967-1732; Zuo, Pengfei/0000-0001-9982-5130
FU National Natural Science Foundation of China (NSFC) [62125202,
   U22B2022]; Key Laboratory of Information Storage System, Ministry of
   Education of China
FX This work was supported in part by the National Natural Science
   Foundation of China (NSFC) under grants 62125202 and U22B2022, and Key
   Laboratory of Information Storage System, Ministry of Education of
   China.
CR Aguilera MK, 2018, PROCEEDINGS OF THE 2018 USENIX ANNUAL TECHNICAL CONFERENCE, P775
   Akinaga H, 2010, P IEEE, V98, P2237, DOI 10.1109/JPROC.2010.2070830
   Amaro E, 2020, PROCEEDINGS OF THE FIFTEENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS (EUROSYS'20), DOI 10.1145/3342195.3387522
   Anderson TE, 2020, PROCEEDINGS OF THE 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '20), P1011
   Apalkov D, 2013, ACM J EMERG TECH COM, V9, DOI 10.1145/2463585.2463589
   Azure SQL Database, 2022, US READ ONL REPL OFF
   BERNSTEIN PA, 1981, COMPUT SURV, V13, P185, DOI 10.1145/356842.356846
   Bhardwaj A, 2022, PROCEEDINGS OF THE 2022 14TH ACM WORKSHOP ON HOT TOPICS IN STORAGE AND FILE SYSTEMS, HOTSTORAGE 2022, P37, DOI 10.1145/3538643.3539752
   BREWER EA, 2000, PODC, V7, DOI [10.1145/343477.343502, DOI 10.1145/343477.343502]
   Brewer E, 2012, COMPUTER, V45, P23, DOI 10.1109/MC.2012.37
   Cai QC, 2018, PROC VLDB ENDOW, V11, P1604, DOI 10.14778/3236187.3236209
   Calciu I, 2021, ASPLOS XXVI: TWENTY-SIXTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS, P79, DOI 10.1145/3445814.3446713
   Cao ZC, 2020, PROCEEDINGS OF THE 18TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P209
   Chen JQ, 2020, PROCEEDINGS OF THE 18TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P239
   Chen YZ, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901349
   Cooper Brian F., 2010, P 1 ACM S CLOUD COMP, P143, DOI [DOI 10.1145/1807128.1807152, 10.1145/1807128.1807152]
   CXL Consortium, 2022, COMP EXPR LINKT BREA
   Douglas Chet, 2015, P 2015 STORAGE DEVEL
   Dragojevic A, 2015, SOSP'15: PROCEEDINGS OF THE TWENTY-FIFTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P54, DOI 10.1145/2815400.2815425
   Dragojevic Aleksandar, 2014, NSDI 14, P401
   Edgecore Networks Corporation, 2022, DCS800 DAT CTR SWITC
   Gao PX, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P249
   Gilbert S., 2002, SIGACT News, V33, P51, DOI 10.1145/564585.564601
   Gray C. G., 1989, Operating Systems Review, V23, P202, DOI 10.1145/74851.74870
   Grun Paul, 2018, P 2018 PERSISTENT ME
   Gu JC, 2017, PROCEEDINGS OF NSDI '17: 14TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P649
   Guerraoui R, 2022, PROCEEDINGS OF THE 2022 USENIX ANNUAL TECHNICAL CONFERENCE, P101
   Guo ZH, 2021, INT CONF MANAGE DATA, P658, DOI 10.1145/3448016.3457294
   Guo ZY, 2022, ASPLOS '22: PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS, P417, DOI 10.1145/3503222.3507762
   Hakkarinen D, 2015, IEEE T PARALL DISTR, V26, P1323, DOI 10.1109/TPDS.2014.2320502
   Hewlett Packard Corporation, 2022, MACH NEW KIND COMP
   Ho C., 2008, NSDI 08 P 5 USENIX S, P175
   Intel Corporation, 2022, INT RACK SCAL DES IN
   Intel Corporation, 2022, Intel Reports Second-Quarter 2022 Financial Results
   Intel Corporation, 2022, INT OPT TM PERS MEM
   Jiang TY, 2022, PROCEEDINGS OF THE 20TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, FAST 2022, P217
   Jung M, 2022, PROCEEDINGS OF THE 2022 14TH ACM WORKSHOP ON HOT TOPICS IN STORAGE AND FILE SYSTEMS, HOTSTORAGE 2022, P45, DOI 10.1145/3538643.3539745
   Kalia Anuj, 2020, SoCC '20: Proceedings of the 11th ACM Symposium on Cloud Computing, P105, DOI 10.1145/3419111.3421294
   Kalia A, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P185
   Kalia A, 2016, PROCEEDINGS OF USENIX ATC '16: 2016 USENIX ANNUAL TECHNICAL CONFERENCE, P437
   Katsarakis A, 2021, PROCEEDINGS OF THE SIXTEENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS (EUROSYS '21), P145, DOI 10.1145/3447786.3456234
   Kim D, 2018, PROCEEDINGS OF THE 2018 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '18), P297, DOI 10.1145/3230543.3230572
   KUNG HT, 1981, ACM T DATABASE SYST, V6, P213, DOI 10.1145/319566.319567
   Lamport L, 2009, PODC'09: PROCEEDINGS OF THE 2009 ACM SYMPOSIUM ON PRINCIPLES OF DISTRIBUTED COMPUTING, P312, DOI 10.1145/1582716.1582783
   Lee C, 2020, INT S HIGH PERF COMP, P502, DOI 10.1109/HPCA47549.2020.00048
   Lee SK, 2019, PROCEEDINGS OF THE TWENTY-SEVENTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '19), P462, DOI 10.1145/3341301.3359635
   Lee SS, 2021, PROCEEDINGS OF THE 28TH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, SOSP 2021, P488, DOI 10.1145/3477132.3483561
   Lim K, 2012, INT S HIGH PERF COMP, P189
   Lim K, 2009, CONF PROC INT SYMP C, P267, DOI 10.1145/1555815.1555789
   Lin Q, 2016, SIGMOD'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P1659, DOI 10.1145/2882903.2882923
   Live Webcast, 2018, P SNIA NETWORKING ST
   Lu YY, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P773
   Ma T, 2020, TWENTY-FIFTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXV), P757, DOI 10.1145/3373376.3378511
   Mu S., 2014, P USENIX OSDI, P479, DOI DOI 10.5555/2685048.2685086
   Nantero Corporation, 2022, NANT NRAM
   Novakovic S, 2019, SYSTOR '19: PROCEEDINGS OF THE 12TH ACM INTERNATIONAL SYSTEMS AND STORAGE CONFERENCE, P97, DOI 10.1145/3319647.3325827
   NVIDIA Corporation, 2023, RDMA AW NETW PROGR U
   NVIDIA Corporation and Affiliates, 2021, NVID CONN 5
   NVIDIA Corporation and Affiliates, 2021, NVID CONN 7
   NVIDIA Corporation and Affiliates, 2022, NVID CONN 6
   Oracle, 2022, WHAT IS OLTP
   Ruan ZY, 2020, PROCEEDINGS OF THE 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '20), P315
   Schuh HN, 2021, PROCEEDINGS OF THE 28TH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, SOSP 2021, P740, DOI 10.1145/3477132.3483555
   Shamis A, 2019, INT CONF MANAGE DATA, P433, DOI 10.1145/3299869.3300069
   Shan YZ, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P69
   Shan YZ, 2017, PROCEEDINGS OF THE 2017 SYMPOSIUM ON CLOUD COMPUTING (SOCC '17), P323, DOI 10.1145/3127479.3128610
   Shrivastav V, 2019, PROCEEDINGS OF THE 16TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P255
   TATP, 2011, Telecom application transaction processing benchmark
   The H-Store Team, 2022, SMALLBANK BENCHM
   The Transaction Processing Council, 2022, TPC C BENCHM
   Thoziyoor S, 2008, CONF PROC INT SYMP C, P51, DOI 10.1109/ISCA.2008.16
   Tirmazi M, 2020, PROCEEDINGS OF THE FIFTEENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS (EUROSYS'20), DOI 10.1145/3342195.3387517
   TomTalpey Tony Hurson, 2020, RDMAEXTENSIONS ENHAN
   Tsai SY, 2020, PROCEEDINGS OF THE 2020 USENIX ANNUAL TECHNICAL CONFERENCE, P33
   Tsai SY, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P306, DOI 10.1145/3132747.3132762
   Verbitski A, 2017, SIGMOD'17: PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P1041, DOI 10.1145/3035918.3056101
   Vmware Research, 2021, REM MEM
   Wang CX, 2020, PROCEEDINGS OF THE 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '20), P261
   Wang JC, 2021, PROCEEDINGS OF THE 15TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '21), P199
   Wei X., 2021, P USENIX ANN TECH C, P523
   Wei XD, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P233
   Wei XD, 2015, SOSP'15: PROCEEDINGS OF THE TWENTY-FIFTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P87, DOI 10.1145/2815400.2815419
   Wong HSP, 2010, P IEEE, V98, P2201, DOI 10.1109/JPROC.2010.2070050
   Xie C., 2014, 11 USENIX S OPERATIN, P495, DOI DOI 10.5555/2685048.2685087
   Xie C, 2015, SOSP'15: PROCEEDINGS OF THE TWENTY-FIFTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P279, DOI 10.1145/2815400.2815430
   Yang J, 2020, PROCEEDINGS OF THE 17TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P111
   Yang J, 2020, PROCEEDINGS OF THE 18TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P169
   Yang J, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P221
   Yang JC, 2020, PROCEEDINGS OF THE 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '20), P191
   Zamanian E, 2020, SIGMOD'20: PROCEEDINGS OF THE 2020 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P511, DOI 10.1145/3318464.3389724
   Zamanian E, 2017, PROC VLDB ENDOW, V10, P685, DOI 10.14778/3055330.3055335
   Zhang I, 2015, SOSP'15: PROCEEDINGS OF THE TWENTY-FIFTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P263, DOI 10.1145/2815400.2815404
   Zhang M, 2022, PROCEEDINGS OF THE 20TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, FAST 2022, P51
   Zhang YY, 2015, ACM SIGPLAN NOTICES, V50, P3, DOI [10.1145/2694344.2694370, 10.1109/OECC.2015.7340093]
   Zuo PF, 2021, PROCEEDINGS OF THE 2021 USENIX ANNUAL TECHNICAL CONFERENCE, P15
NR 95
TC 0
Z9 0
U1 2
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2023
VL 19
IS 3
AR 21
DI 10.1145/3582012
PG 35
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P9QZ4
UT WOS:001053965500001
DA 2024-07-18
ER

PT J
AU Pan, C
   Wang, XL
   Luo, YW
   Wang, ZL
AF Pan, Cheng
   Wang, Xiaolin
   Luo, Yingwei
   Wang, Zhenlin
TI Penalty- and Locality-aware Memory Allocation in Redis Using Enhanced
   AET
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Penalty- and locality-aware; cache modeling; memory allocation
AB Due to large data volume and low latency requirements of modern web services, the use of an in-memory key-value (KV) cache often becomes an inevitable choice (e.g., Redis and Memcached). The in-memory cache holds hot data, reduces request latency, and alleviates the load on background databases. Inheriting from the traditional hardware cache design, many existing KV cache systems still use recency-based cache replacement algorithms, e.g., least recently used or its approximations. However, the diversity of miss penalty distinguishes a KV cache from a hardware cache. Inadequate consideration of penalty can substantially compromise space utilization and request service time. KV accesses also demonstrate locality, which needs to be coordinated with miss penalty to guide cache management.
   In this article, we first discuss how to enhance the existing cache model, the Average Eviction Time model, so that it can adapt to modeling a KV cache. After that, we apply the model to Redis and propose pRedis, Penalty- and Locality-aware Memory Allocation in Redis, which synthesizes data locality and miss penalty, in a quantitative manner, to guide memory allocation and replacement in Redis. At the same time, we also explore the diurnal behavior of a KV store and exploit long-term reuse. We replace the original passive eviction mechanism with an automatic dump/load mechanism, to smooth the transition between access peaks and valleys. Our evaluation shows that pRedis effectively reduces the average and tail access latency with minimal time and space overhead. For both real-world and synthetic workloads, our approach delivers an average of 14.0%similar to 52.3% latency reduction over a state-of-the-art penalty-aware cache management scheme, Hyperbolic Caching (HC), and shows more quantitative predictability of performance. Moreover, we can obtain even lower average latency (1.1%similar to 5.5%) when dynamically switching policies between pRedis and HC.
C1 [Pan, Cheng; Wang, Xiaolin; Luo, Yingwei] Peking Univ, Dept Comp Sci, 5 Yiheyuan Rd, Beijing 100871, Peoples R China.
   [Pan, Cheng; Wang, Xiaolin; Luo, Yingwei] Peng Cheng Lab, 2 Xingke 1st St, Shenzhen 518040, Guangdong, Peoples R China.
   [Wang, Zhenlin] Michigan Technol Univ, Dept Comp Sci, Houghton, MI 49931 USA.
C3 Peking University; Peng Cheng Laboratory; Michigan Technological
   University
RP Luo, YW (corresponding author), Peking Univ, Dept Comp Sci, 5 Yiheyuan Rd, Beijing 100871, Peoples R China.; Luo, YW (corresponding author), Peng Cheng Lab, 2 Xingke 1st St, Shenzhen 518040, Guangdong, Peoples R China.
EM pancheng@pku.edu.cn; wxl@pku.edu.cn; lyw@pku.edu.cn; zlwang@mtu.edu
OI Wang, Zhenlin/0000-0002-0429-4371; Pan, Cheng/0000-0003-4995-8407
FU National Key R&D Program of China [2018YFB1003505]; National Science
   Foundation of China [62032001, 61672053, U1611461, 62032008]; National
   Science Foundation [CSR1618384]
FX The research is supported in part by the National Key R&D Program of
   China under Grant No. 2018YFB1003505, and the National Science
   Foundation of China (Grants No. 62032001, No. 61672053, No. U1611461,
   and No. 62032008) and National Science Foundation Grant No. CSR1618384.
CR Almeida PS, 2007, INFORM PROCESS LETT, V101, P255, DOI 10.1016/j.ipl.2006.10.007
   Atikoglu Berk, 2012, Performance Evaluation Review, V40, P53, DOI 10.1145/2318857.2254766
   Atre N, 2020, SIGCOMM '20: PROCEEDINGS OF THE 2020 ANNUAL CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION ON THE APPLICATIONS, TECHNOLOGIES, ARCHITECTURES, AND PROTOCOLS FOR COMPUTER COMMUNICATION, P495, DOI 10.1145/3387514.3405883
   Beckmann N, 2015, INT S HIGH PERF COMP, P64, DOI 10.1109/HPCA.2015.7056022
   Berger DS, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P195
   Berger DS, 2017, PROCEEDINGS OF NSDI '17: 14TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P483
   Bjornsson Hjortur, 2013, P 4 ANN S CLOUD COMP, P1
   Blankstein A, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P499
   BLOOM BH, 1970, COMMUN ACM, V13, P422, DOI 10.1145/362686.362692
   Brock J, 2015, PROC INT CONF PARAL, P749, DOI 10.1109/ICPP.2015.84
   Byrne D, 2018, ACM SIGPLAN NOTICES, V53, P84, DOI [10.1145/3210563.3210571, 10.1145/3299706.3210571]
   Cao P, 1997, PROCEEDINGS OF THE USENIX SYMPOSIUM ON INTERNET TECHNOLOGIES AND SYSTEMS, P193
   Cascaval C, 2005, PACT 2005: 14TH INTERNATIONAL CONFERENCE ON PARALLEL ARCHITECTURES AND COMPILATION TECHNIQUES, P339
   Cidon A., 2015, HOTCLOUD
   Cidon A, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P321
   Cidon A, 2016, 13TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION (NSDI '16), P379
   Dai HP, 2016, SIGMETRICS/PERFORMANCE 2016: PROCEEDINGS OF THE SIGMETRICS/PERFORMANCE JOINT INTERNATIONAL CONFERENCE ON MEASUREMENT AND MODELING OF COMPUTER SCIENCE, P139, DOI [10.1145/2964791.2901451, 10.1145/2896377.2901451]
   Hao F, 2012, IEEE ACM T NETWORK, V20, P295, DOI 10.1109/TNET.2011.2173351
   Hu XM, 2018, ACM T STORAGE, V14, DOI 10.1145/3185751
   Hu XM, 2017, IEEE T COMPUT, V66, P862, DOI 10.1109/TC.2016.2618920
   Hu Xiameng., 2015, USENIX ANN TECHNICAL, P57
   Hu Xiameng, 2016, P USENIX ANN TECHN C
   Kim Y. H., 1991, Performance Evaluation Review, V19, P212, DOI 10.1145/107972.107995
   Li CJ, 2015, AER ADV ENG RES, V26, P5
   MATTSON RL, 1970, IBM SYST J, V9, P78, DOI 10.1147/sj.92.0078
   Niu QP, 2012, INT PARALL DISTRIB P, P1284, DOI 10.1109/IPDPS.2012.117
   Ou JQ, 2015, PROC INT CONF PARAL, P530, DOI 10.1109/ICPP.2015.62
   Pagh R, 2013, ANN IEEE SYMP FOUND, P80, DOI 10.1109/FOCS.2013.17
   Pan C, 2018, 9TH ASIA-PACIFIC SYSTEMS WORKSHOP 2018 (APSYS'18), DOI 10.1145/3265723.3265736
   Shim J, 1999, IEEE T KNOWL DATA EN, V11, P549, DOI 10.1109/69.790804
   Suh GE, 2001, P 15 INT C SUP, P1
   Sundarrajan A, 2017, CONEXT'17: PROCEEDINGS OF THE 2017 THE 13TH INTERNATIONAL CONFERENCE ON EMERGING NETWORKING EXPERIMENTS AND TECHNOLOGIES, P55, DOI 10.1145/3143361.3143368
   Waldspurger C. A., 2015, 13 USENIX C FILE STO, P95
   Waldspurger CA, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P487
   Wang Y, 2010, ACM T STORAGE, V6, DOI 10.1145/1807060.1807064
   Wires J., 2014, P S OP SYST DES IMPL, P335
   Xie ZH, 2017, INT C PAR DISTRIB SY, P190, DOI 10.1109/ICPADS.2017.00035
   Ye CC, 2017, INT J PARALLEL PROG, V45, P30, DOI 10.1007/s10766-015-0384-3
   Yoon M, 2014, IEEE INFOCOM SER, P1429, DOI 10.1109/INFOCOM.2014.6848077
   Young Neal E, 1991, THESIS PRINCETON U
   Zhang X, 2009, EUROSYS'09: PROCEEDINGS OF THE FOURTH EUROSYS CONFERENCE, P89
   Zhou P, 2004, ACM SIGPLAN NOTICES, V39, P177, DOI 10.1145/1037187.1024415
NR 42
TC 5
Z9 7
U1 1
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUN
PY 2021
VL 17
IS 2
AR 15
DI 10.1145/3447573
PG 45
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SV7JF
UT WOS:000663994900007
OA Bronze
DA 2024-07-18
ER

PT J
AU Zhang, YM
   Li, HB
   Liu, SY
   Xu, JW
   Xue, GT
AF Zhang, Yiming
   Li, Huiba
   Liu, Shengyun
   Xu, Jiawei
   Xue, Guangtao
TI PBS: An Efficient Erasure-Coded Block Storage System Based on
   Speculative Partial Writes
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Erasure coding; partial write; block storage system; cloud oblivious
   applications
AB Block storage provides virtual disks that can be mounted by virtual machines (VMs). Although erasure coding (EC) has been widely used in many cloud storage systems for its high efficiency and durability, current EC schemes cannot provide high-performance block storage for the cloud. This is because they introduce significant overhead to small write operations (which perform partial write to an entire EC group), whereas cloud-oblivious applications running on VMs are often small-write-intensive. We identify the root cause for the poor performance of partial writes in state-of-the-art EC schemes: for each partial write, they have to perform a time-consuming write-after-read operation that reads the current value of the data and then computes and writes the parity delta, which will be used to "patch" the parity in journal replay.
   In this article, we present a speculative partial write scheme (called PARIX) that supports fast small writes in erasure-coded storage systems. We transform the original formula of parity calculation to use the data deltas (between the current/original data values), instead of the parity deltas, to calculate the parities in journal replay. For each partial write, this allows PARIX to speculatively log only the new value of the data without reading its original value. For a series of n partial writes to the same data, PAR1X performs pure write (instead of write-after-read) for the last n - 1 ones while only introducing a small penalty of an extra network round-trip time to the first one. Based on PARIX, we design and implement PARIX Block Storage (PBS), an efficient block storage system that provides high-performance virtual disk service for VMs running cloud-oblivious applications. PBS not only supports fast partial writes but also realizes efficient full writes, background journal replay, and fast failure recovery with strong consistency guarantees. Both microbenchmarks and trace-driven evaluation show that PBS provides efficient block storage and outperforms state-of-the-art EC-based systems by orders of magnitude.
C1 [Zhang, Yiming; Li, Huiba; Liu, Shengyun] NUDT, Changsha, Peoples R China.
   [Zhang, Yiming] Natl Univ Def Technol, NiceX Lab, Sanyi Rd, Changsha 410073, Peoples R China.
   [Li, Huiba; Liu, Shengyun] Natl Univ Def Technol, Changsha, Peoples R China.
   [Li, Huiba; Liu, Shengyun] Alibaba, Xinxi Rd, Beijing, Peoples R China.
   [Xu, Jiawei; Xue, Guangtao] Shanghai Jiao Tong Univ, Dongchuan Rd, Shanghai, Peoples R China.
C3 National University of Defense Technology - China; National University
   of Defense Technology - China; National University of Defense Technology
   - China; Alibaba Group; Shanghai Jiao Tong University
RP Zhang, YM (corresponding author), NUDT, Changsha, Peoples R China.; Zhang, YM (corresponding author), Natl Univ Def Technol, NiceX Lab, Sanyi Rd, Changsha 410073, Peoples R China.
EM zhangyiming@nudt.edu.cn; lihuiba@gmail.com; liushengyun813@gmail.com;
   titan_xjw@sjtu.edu.cn; xue-gt@cs.sjtu.edu.cn
RI Zhang, Yiming/HGB-7344-2022; xu, jiawei/GWM-9710-2022; xu,
   jiawei/JCD-7577-2023
FU National Key R&D Program of China [2018YFB2101102]; National Natural
   Science Foundation of China [NSFC 61772541, 61872376]; Joint Key Project
   of the NSFC [U1736207]
FX Y. Zhang and H. Li are co-primary authors. The work was done when Y.
   Zhang was visiting Shanghai Jiao Tong University. This work was
   supported by the National Key R&D Program of China (2018YFB2101102), the
   National Natural Science Foundation of China (NSFC 61772541 and
   61872376), and the Joint Key Project of the NSFC (U1736207).
CR Abd-El-Malek Michael., 2005, FAST 05, P5
   Aguilera MK, 2005, I C DEPEND SYS NETWO, P336, DOI 10.1109/DSN.2005.96
   Aiken S, 2003, IEEE S MASS STOR SYS, P123, DOI 10.1109/MASS.2003.1194849
   AmazonWeb Services, AM SIMPL EM SERV
   AmazonWeb Services, AM EL BLOCK STOR
   [Anonymous], 2014, FAST
   [Anonymous], GEN PAR FIL SYST
   [Anonymous], 2010, SIGOPS Oper. Syst. Rev., DOI DOI 10.1145/1713254.1713276
   Apache Hadoop, HDFS Architecture Guide
   Bhagwan R, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE FIRST SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION (NSDI'04), P337
   Cashin L., 2005, LINUX J, V2005, P10
   Chan JeremyC. W., 2014, P 12 USENIX C FILE S, P163
   Chen HB, 2017, ACM T STORAGE, V13, DOI 10.1145/3129900
   Chidambaram V., 2012, Proceedings of the 10th USENIX conference on File and Storage Technologies, FAST 2012, San Jose, CA, USA, February 14-17, 2012, P9
   Ford Daniel, 2010, P USENIX OSDI
   Ghemawat S., 2003, Operating Systems Review, V37, P29, DOI 10.1145/1165389.945450
   Glendenning L, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P15
   Google Cloud, CLOUD SQL
   Gray C. G., 1989, Operating Systems Review, V23, P202, DOI 10.1145/74851.74870
   Hildebrand D, 2005, Twenty-Second IEEE/Thirteenth NASA Goddard Conference on Mass Storage Systems and Technologies, Proceedings, P18, DOI 10.1109/MSST.2005.14
   Huang Cheng, 2012, 2012 USENIX ANN TECH, P15
   Jin C, 2011, 2011 IEEE POW EN SOC, P1
   Kalia A, 2014, SIGCOMM'14: PROCEEDINGS OF THE 2014 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P295, DOI [10.1145/2619239.2626299, 10.1145/2740070.2626299]
   Khan Osama., 2012, FAST, P20
   Kubiatowicz J, 2000, ACM SIGPLAN NOTICES, V35, P190, DOI 10.1145/384264.379239
   LAMPORT L, 1978, COMMUN ACM, V21, P558, DOI 10.1145/359545.359563
   Lee EK, 1996, ACM SIGPLAN NOTICES, V31, P84, DOI 10.1145/248209.237157
   LEUNG A. W., 2008, P USENIX ANN TECHN C
   Li HB, 2019, PROCEEDINGS OF THE FOURTEENTH EUROSYS CONFERENCE 2019 (EUROSYS '19), DOI 10.1145/3302424.3303967
   Li W, 2013, PROG COMPUT FLUID DY, V13, P357, DOI 10.1504/PCFD.2013.057098
   Lu X, 2013, INT J COMPUTER GAMES, V2013, P1
   Lu XC, 2006, SCI CHINA SER F, V49, P681, DOI 10.1007/s11432-006-2030-6
   Man7.org, FSTRIM 8
   Mickens James., 2014, Proceedings of the 11th USENIX Conference on Networked Systems Design and Implementation. NSDI'14, P257
   Microsoft, MICR AZ STOR
   Microsoft Azure, WIND VIRT DESKT
   Miller R., 2008, Failure rates in google data centers
   Muralidhar S., 2014, P 11 USENIX C OP SYS, P383
   Narayanan D., 2008, ACM T STORAGE, V4, P3
   Nightingale Edmund B., 2012, P 9 USENIX S OP SYST
   ONeil P, 1996, ACTA INFORM, V33, P351, DOI 10.1007/s002360050048
   Oracle Open Source, PROJ OCFS
   Pei Xiaoqiang, 2016, P 35 ANN IEEE INT C, P1, DOI DOI 10.1109/1NFOCOM.2016.7524347
   PLANK JAMESS., 2009, FAST, P253
   Rashmi KV, 2014, SIGCOMM'14: PROCEEDINGS OF THE 2014 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P331, DOI [10.1145/2619239.2626325, 10.1145/2740070.2626325]
   Rizzo L., 1997, Computer Communication Review, V27, P24, DOI 10.1145/263876.263881
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   Sathiamoorthy M, 2013, PROC VLDB ENDOW, V6, P325, DOI 10.14778/2535573.2488339
   Sheepdog, SHEEPD PROJ
   Shen ZR, 2018, PROC INT CONF PARAL, DOI 10.1145/3225058.3225065
   SNIA, MSR Cambridge Traces
   STODOLSKY D, 1993, CONF PROC INT SYMP C, P64, DOI 10.1145/173682.165143
   Storer MW, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P1, DOI 10.1145/1456469.1456471
   VMware, VSAN
   Weatherspoon H, 2002, LECT NOTES COMPUT SC, V2429, P328
   Weddle Charles, 2007, ACM Transactions on Storage, V3, DOI 10.1145/1288783.1289721
   Weil SA, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P307
   Welch B, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P17
   Wicker S.B., 1999, Reed-Solomon codes and their applications
   Wiki Fedora, INFR FED RETR
   Wiki OpenStack, CINDER
   Wilkes J., 1995, Operating Systems Review, V29, P96, DOI 10.1145/224057.224065
   Zhang YM, 2019, ACM T STORAGE, V15, DOI 10.1145/3289604
   Zhang YM, 2017, IEEE ACM T NETWORK, V25, P2040, DOI 10.1109/TNET.2017.2669215
NR 64
TC 19
Z9 21
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD APR
PY 2020
VL 16
IS 1
AR 6
DI 10.1145/3365839
PG 25
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1QN
UT WOS:000583742400006
DA 2024-07-18
ER

PT J
AU Kesavan, R
   Curtis-Maury, M
   Devadas, V
   Mishra, K
AF Kesavan, Ram
   Curtis-Maury, Matthew
   Devadas, Vinay
   Mishra, Kesari
TI Countering Fragmentation in an Enterprise Storage System
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Storage system; file system; fragmentation; file system performance;
   snapshot; deduplication
AB As a file system ages, it can experience multiple forms of fragmentation. Fragmentation of the free space in the file system can lower write performance and subsequent read performance. Client operations as well as internal operations, such as deduplication, can fragment the layout of an individual file, which also impacts file read performance. File systems that allow sub-block granular addressing can gather intra-block fragmentation, which leads to wasted free space. Similarly, wasted space can also occur when a file system writes a collection of blocks out to object storage as a single large object, because the constituent blocks can become free at different times. The impact of fragmentation also depends on the underlying storage media. This article studies each form of fragmentation in the NetApp (R) WAFL (R) file system, and explains how the file system leverages a storage virtualization layer for defragmentation techniques that physically relocate blocks efficiently, including those in read-only snapshots. The article analyzes the effectiveness of these techniques at reducing fragmentation and improving overall performance across various storage media.
C1 [Kesavan, Ram] Google, 1600 Amphitheatre Pkwy, Mountain View, CA 94043 USA.
   [Kesavan, Ram; Curtis-Maury, Matthew; Devadas, Vinay; Mishra, Kesari] NetApp Inc, Durham, NC USA.
   [Curtis-Maury, Matthew; Devadas, Vinay] NetApp, 7301 Kit Creek Rd, Durham, NC 27709 USA.
   [Mishra, Kesari] 1223 Crescent Terrace, Sunny Vale, CA 94087 USA.
C3 Google Incorporated; NetApp, Inc.
RP Kesavan, R (corresponding author), Google, 1600 Amphitheatre Pkwy, Mountain View, CA 94043 USA.
EM ram.kesavan@gmail.com; mcm@netapp.com; vdevadas@netapp.com;
   km@netapp.com
CR Ahn WH, 2002, MASCOTS 2002: 10TH IEEE INTERNATIONAL SYMPOSIUM ON MODELING, ANALYSIS, AND SIMULATION OF COMPUTER AND TELECOMMUNICATIONS SYSTEMS, PROCEEDINGS, P71, DOI 10.1109/MASCOT.2002.1167062
   [Anonymous], 1988, P INT C MAN DAT SIGM
   [Anonymous], 2011, P 9 USENIX C FIL STO
   BARTLETT W., 2004, IEEE T DEPEND SECURE, V1, P1
   Bjorling Matias, 2017, P C FIL STOR TECHN F
   Conway JM, 2017, POLIT ECON, P17
   Corbato F. J., 1968, TECHNICAL REPORT
   Corbett Peter, 2004, P C FIL STOR TECHN F
   Curtis-Maury Matthew, 2016, P S OP SYST DES IMPL
   Curtis-Maury Matthew, 2017, P INT C PAR PROC ICP
   Edwards John K, 2008, P USENIX ANN TECHN C
   GOEL A, 2012, ACM SIGOPS OPER SYST, V46, P41
   Hahn Sangwook Shane, 2017, P USENIX ANN TECHN C
   He Jun, 2017, P EUR C COMP SYST EU
   He Weiping, 2017, P C FIL STOR TECHN F
   HITZ D., 1994, P USENIX WINT TECHN
   Ji Cheng, 2016, P USENIX WORKSH HOT
   KADAMB R, 2015, IEEE T PARALL DISTR, V35
   Kadekodi Saurabh, 2018, P USENIX ANN TECHN C
   Kesavan R, 2018, PROC INT CONF PARAL, DOI 10.1145/3225058.3225072
   Kesavan R, 2017, ACM T STORAGE, V13, DOI 10.1145/3125647
   Kesavan Ram, 2019, P C FIL STOR TECHN F
   Kesavan Ram, 2017, P C FIL STOR TECHN F
   Kim Hyukjoong, 2017, P C FIL STOR TECHN F
   Lantz John, 2018, ENABLE CLOUD CONNECT
   Lee Changman, 2015, P 13 USENIX C FIL ST
   Lee Youngjae, 2010, P ACM S APPL COMP
   Lu Youyou, 2014, P C FIL STOR TECHN F
   Mathur Avantika, 2007, P LIN S, V2
   MCKUSICK MK, 1984, ACM T COMPUT SYST, V2, P181, DOI 10.1145/989.990
   Patterson Hugo, 2002, P USENIX C FIL STOR
   Rodeh O, 2013, ACM T STORAGE, V9, DOI 10.1145/2501620.2501623
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   Sato Takashi, 2007, P LIN S, V2, P179
   Seltzer Margo, 1995, P USENIX ANN TECHN C
   Smith K. A., 1997, Performance Evaluation Review, V25, P203, DOI 10.1145/258623.258689
   SUN Microsystems, 2008, ZFS OPENSOLARIS COMM
   Sundaram Rajesh, 2006, The Private Lives of Disk Drives
   Weil Sage A., 2006, P S OP SYST IMPL OSD
   Xu Jian, 2016, P C FIL STOR TECHN F
   Xu Qiumin, 2015, ACM SIGMETRICS PERFO, V43
   Xu Qiumin, 2015, P 8 ACM INT SYST STO
   Yang Jingpei, 2014, P 2 WORKSH INT NVM F
   Zhang Zhihui, 2003, P C FIL STOR TECHN F
   Zuck Aviad, 2014, P 8 INT C NEXT GEN M
NR 45
TC 6
Z9 6
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD FEB
PY 2020
VL 15
IS 4
AR 25
DI 10.1145/3366173
PG 35
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NG7LP
UT WOS:000564162600004
DA 2024-07-18
ER

PT J
AU Choi, JY
   Nam, EH
   Seong, YJ
   Yoon, JH
   Lee, S
   Kim, HS
   Park, J
   Woo, YJ
   Lee, S
   Min, SL
AF Choi, Jin-Yong
   Nam, Eyee Hyun
   Seong, Yoon Jae
   Yoon, Jin Hyuk
   Lee, Sookwan
   Kim, Hong Seok
   Park, Jeongsu
   Woo, Yeong-Jae
   Lee, Sheayun
   Min, Sang Lyul
TI HIL: A Framework for Compositional FTL Development and Provably-Correct
   Crash Recovery
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Flash memory; flash translation layer (FTL); solid-state drive (SSD);
   crash recovery
ID FLASH TRANSLATION
AB We present a framework called Hierarchically Interacting Logs (HIL) for constructing Flash Translation Layers (FTLs). The main goal of the HIL framework is to heal the Achilles heel the crash recovery of FTLs (hence, its name). Nonetheless, the framework itself is general enough to encompass not only block-mapped and page-mapped FTLs but also many of their variants, including hybrid ones, because of its compositional nature.
   Crash recovery within the HIL framework proceeds in two phases: structural recovery and functional recovery. During the structural recovery, residual effects due to program operations ongoing at the time of the crash are eliminated in an atomic manner using shadow paging. During the functional recovery, operations that would have been performed if there had been no crash are replayed in a redo-only fashion. Both phases operate in an idempotent manner, preventing repeated crashes during recovery from causing any additional problems.
   We demonstrate the practicality of the proposed HIL framework by implementing a prototype and showing that its performance during normal execution and also during crash recovery is at least as good as those of state-of-the-art SSDs.
C1 [Choi, Jin-Yong; Nam, Eyee Hyun; Seong, Yoon Jae; Kim, Hong Seok] FADU Inc, 8 Bongeunsa Ro 68 Gil, Seoul 06153, South Korea.
   [Yoon, Jin Hyuk; Lee, Sookwan; Park, Jeongsu; Woo, Yeong-Jae; Min, Sang Lyul] Seoul Natl Univ, Dept Comp Sci & Engn, Seoul 08826, South Korea.
   [Lee, Sheayun] Kookmin Univ, Sch Comp Sci, Seoul 02727, South Korea.
   [Yoon, Jin Hyuk] SK Telecom, 6 Hwangsaeul Ro 258beon Gil, Seongnam Si 13595, Gyeonggi Do, South Korea.
   [Park, Jeongsu] SK Hynix Inc, 9 Seongnam Daero 343beon Gil, Seongnam Si 13558, Gyeonggi Do, South Korea.
C3 Seoul National University (SNU); Kookmin University; SK Group; SK
   Telecom; SK Group; SK Hynix
RP Choi, JY (corresponding author), FADU Inc, 8 Bongeunsa Ro 68 Gil, Seoul 06153, South Korea.
EM jychoi@fadu.io; ehnam@fadu.io; yjseong@fadu.io; jhyoon@naver.com;
   sklee23@gmail.com; hskim@fadu.io; jspark@archi.snu.ac.kr;
   yjwoo@archi.snu.ac.kr; sheayun@kookmin.ac.kr; symin@snu.ac.kr
FU National Research Foundation of Korea (NRF) - Korea government (MSIP)
   [NRF-2010-0015149]; PF Class Heterogeneous High Performance Computer
   Development [NRF-2016M3C4A7952587]; MIST (Ministry of Science and ICT),
   Korea, under the National Program for Excellence in SW [2016-0-00021]
FX This work was supported in part by the National Research Foundation of
   Korea (NRF) grant funded by the Korea government (MSIP) (Grant No.
   NRF-2010-0015149). This work was supported in part by the PF Class
   Heterogeneous High Performance Computer Development (Grant No.
   NRF-2016M3C4A7952587). This research was supported in part by the MIST
   (Ministry of Science and ICT), Korea, under the National Program for
   Excellence in SW (2016-0-00021) supervised by the IITP (Institute for
   Information & communications Technology Promotion).
CR Agrawal Nitin, 2008, P USENIX ANN TECHN C, P57
   [Anonymous], 2008, P 8 USENIX C OP SYST
   [Anonymous], PROCEEDINGS OF THE 1995 USENIX TECHNICAL CONFERENCE
   [Anonymous], 2012, 4 USENIX WORKSHOP HO
   [Anonymous], 2008, P 8 ACM INT C EMB SO
   [Anonymous], ATEC 05
   [Anonymous], 1987, Concurrency Control and Recovery in Database Systems
   Aritome S., 2015, NAND FLASH MEMORY TE
   ARPACIDUSSEAU R.H., 2015, 3 EASY PIECES
   Binkert Nathan, 2011, Computer Architecture News, V39, P1, DOI 10.1145/2024716.2024718
   Birrell A., 2007, Operating Systems Review, V41, P88, DOI 10.1145/1243418.1243429
   Cai Y, 2017, INT S HIGH PERF COMP, P49, DOI 10.1109/HPCA.2017.61
   Chang Y.-M., 2016, P 2016 IEEE 8 INT ME, P1
   Chiang ML, 1999, J SYST SOFTWARE, V48, P213, DOI 10.1016/S0164-1212(99)00059-X
   Dayan N, 2016, SIGMOD'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P327, DOI 10.1145/2882903.2915219
   Gal E, 2005, ACM COMPUT SURV, V37, P138, DOI 10.1145/1089733.1089735
   Ganger GR, 2000, ACM T COMPUT SYST, V18, P127, DOI 10.1145/350853.350863
   GRAY J, 1981, COMPUT SURV, V13, P223, DOI 10.1145/356842.356847
   Grupp L.M., 2012, P 10 USENIX C FILE S, P2
   Gupta A, 2009, ACM SIGPLAN NOTICES, V44, P229, DOI 10.1145/1508284.1508271
   Hitz D., 1994, Proceedings of the USENIX Winter 1994 Technical Conference, P19
   Huang PC, 2013, ACM T DES AUTOMAT EL, V18, DOI 10.1145/2505013
   Intel Corp, 2015, INT SOL STAT DRIV DC
   Intel Corporation and Seagate Technology, 2003, CISC VIS NETW IND GL
   Jin Hyuk Yoon, 2008, IEEE Computer Architecture Letters, V7, P17, DOI 10.1109/L-CA.2007.17
   Kang J., 2006, Proceedings of the International Conference on Embedded Software (EMSOFT), P161
   Kim D, 2016, IEEE T COMPUT, V65, P1720, DOI 10.1109/TC.2015.2462819
   Kim HS, 2017, ACM T EMBED COMPUT S, V16, DOI 10.1145/3126550
   Kim JW, 2002, IEEE T CONSUM ELECTR, V48, P275, DOI 10.1109/TCE.2002.1010132
   Kwon Hunki, 2010, P 10 ACM INT C EMB S, P169, DOI DOI 10.1145/1879021.1879044
   Lee J.Y., 2013, ED PRIMARY SCH MATH, V16, P1, DOI DOI 10.1145/2500727.2500741
   Lee SW, 2007, ACM T EMBED COMPUT S, V6, DOI 10.1145/1275986.1275990
   Lu YY, 2013, 2013 IEEE 31ST INTERNATIONAL CONFERENCE ON COMPUTER DESIGN (ICCD), P115, DOI 10.1109/ICCD.2013.6657033
   Ma D., 2011, Proc. of ACM SIGMOD Int. Conf. on Manage. of Data, P1
   MOHAN C, 1992, ACM T DATABASE SYST, V17, P94, DOI 10.1145/128765.128770
   Open Source Development Lab, IOM
   Ouyang XY, 2011, INT S HIGH PERF COMP, P301, DOI 10.1109/HPCA.2011.5749738
   Park C, 2008, ACM T EMBED COMPUT S, V7, DOI 10.1145/1376804.1376806
   Prince B, 2014, VERTICAL 3D MEMORY TECHNOLOGIES, P1, DOI 10.1002/9781118760475
   Ramakrishnan R., 2000, Database Management Systems
   Rogers A. M., 2010, uS Patent, Patent No. [7,818,610, 7818610]
   Samsung Electronics Co. Ltd, 2015, PM1725 NVME PCIE SSD
   Seong YJ, 2010, IEEE T COMPUT, V59, P905, DOI 10.1109/TC.2010.63
   Sungjin Lee, 2008, Operating Systems Review, V42, P36, DOI 10.1145/1453775.1453783
   Tseng HW, 2011, DES AUT CON, P35
   Wu G., 2012, P 10 USENIX C FIL ST, P1
   WU M, 1994, SIGPLAN NOTICES, V29, P86, DOI 10.1145/195470.195506
NR 47
TC 6
Z9 6
U1 0
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD DEC
PY 2018
VL 14
IS 4
AR 36
DI 10.1145/3281030
PG 29
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ4JO
UT WOS:000457140500008
DA 2024-07-18
ER

PT J
AU Gunawi, HS
   Suminto, RO
   Sears, R
   Golliher, C
   Sundararaman, S
   Lin, X
   Emami, T
   Sheng, WG
   Bidokhti, N
   Mccaffrey, C
   Srinivasan, D
   Panda, B
   Baptist, A
   Grider, G
   Fields, PM
   Harms, K
   Ross, RB
   Jacobson, A
   Ricci, R
   Webb, K
   Alvaro, P
   Runesha, HB
   Hao, MZ
   Li, HC
AF Gunawi, Haryadi S.
   Suminto, Riza O.
   Sears, Russell
   Golliher, Casey
   Sundararaman, Swaminathan
   Lin, Xing
   Emami, Tim
   Sheng, Weiguang
   Bidokhti, Nematollah
   Mccaffrey, Caitie
   Srinivasan, Deepthi
   Panda, Biswaranjan
   Baptist, Andrew
   Grider, Gary
   Fields, Parks M.
   Harms, Kevin
   Ross, Robert B.
   Jacobson, Andree
   Ricci, Robert
   Webb, Kirk
   Alvaro, Peter
   Runesha, H. Birali
   Hao, Mingzhe
   Li, Huaicheng
TI Fail-Slow at Scale: Evidence of Hardware Performance Faults in Large
   Production Systems
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article; Proceedings Paper
CT 16th USENIX Conference on File and Storage Technologies (FAST)
CY FEB 12-15, 2018
CL Oakland, CA
SP USENIX, ACM SIGOPS
DE Hardware fault; performance; fail-slow; fail-stutter; limpware; jitter
AB Fail-slow hardware is an under-studied failure mode. We present a study of 114 reports of fail-slow hardware incidents, collected from large-scale cluster deployments in 14 institutions. We show that all hardware types such as disk, SSD, CPU, memory, and network components can exhibit performance faults. We made several important observations such as faults convert from one form to another, the cascading root causes and impacts can be long, and fail-slow faults can have varying symptoms. From this study, we make suggestions to vendors, operators, and systems designers.
C1 [Gunawi, Haryadi S.; Suminto, Riza O.] Univ Chicago, Chicago, IL 60637 USA.
   [Sears, Russell; Golliher, Casey] Pure Storage, Mountain View, CA USA.
   [Sundararaman, Swaminathan] Parallel Machines, Tel Aviv, Israel.
   [Lin, Xing; Emami, Tim] NetApp, Sunnyvale, CA USA.
   [Sheng, Weiguang; Bidokhti, Nematollah] Huawei, Shenzhen, Peoples R China.
   [Mccaffrey, Caitie] Twitter, San Francisco, CA USA.
   [Srinivasan, Deepthi; Panda, Biswaranjan] Nutanix, San Jose, CA USA.
   [Baptist, Andrew] IBM Corp, Armonk, NY USA.
   [Grider, Gary; Fields, Parks M.] Los Alamos Natl Lab, Los Alamos, NM USA.
   [Harms, Kevin; Ross, Robert B.] Argonne Natl Lab, Argonne, IL 60439 USA.
   [Jacobson, Andree] New Mexico Consortium, Los Alamos, NM USA.
   [Ricci, Robert; Webb, Kirk] Univ Utah, Salt Lake City, UT 84112 USA.
   [Alvaro, Peter] Univ Calif Santa Cruz, Santa Cruz, CA 95064 USA.
   [Runesha, H. Birali] Univ Chicago, Res Comp Ctr, Chicago, IL 60637 USA.
   [Hao, Mingzhe; Li, Huaicheng] Univ Chicago, Chicago, IL 60637 USA.
C3 University of Chicago; Pure Storage; NetApp, Inc.; Huawei Technologies;
   Twitter, Inc.; International Business Machines (IBM); United States
   Department of Energy (DOE); Los Alamos National Laboratory; United
   States Department of Energy (DOE); Argonne National Laboratory; Utah
   System of Higher Education; University of Utah; University of California
   System; University of California Santa Cruz; University of Chicago;
   University of Chicago
RP Gunawi, HS (corresponding author), Univ Chicago, Chicago, IL 60637 USA.
EM haryadi@cs.uchicago.edu; riza@cs.uchicago.edu; sears@purestorage.com;
   casey.golliher@purestorage.com;
   swaminathan.sundararaman@parallelmachines.com; xing.lin@netapp.com;
   Tim.Emami@netapp.com; weiguangsheng@huawei.com;
   nematollah.bidokhti@huawei.com; caitiem20@gmail.com;
   deepthi.srinivasan@nutanix.com; biswara.panda@nutanix.com;
   abaptist@us.ibm.com; ggrider@lanl.gov; parks@lanl.gov;
   harms@alcf.anl.gov; rross@mcs.anl.gov; andree@newmexicoconsortium.org;
   ricci@cs.utah.edu; kwebb@cs.utah.edu; palvaro@ucsc.edu;
   runesha@uchicago.edu; hmz20000@uchicago.edu; huaicheng@cs.uchicago.edu
RI l, H/HJA-6959-2022; L, H/AAM-2796-2021
OI Suminto, Riza/0000-0003-2248-5961; Alvaro, Peter/0000-0001-6672-240X;
   Ross, Robert/0000-0002-5435-5857
FU NSF [CCF-1336580, CNS-1350499, CNS-1526304, CNS-1563956]; DOE Office of
   Science User Facility [DE-AC02-06CH11357]
FX This material was supported by funding from NSF (grant Nos. CCF-1336580,
   CNS-1350499, CNS-1526304, and CNS-1563956) and DOE Office of Science
   User Facility (contract# DE-AC02-06CH11357).
CR Alagappan Ramnatthan, 2016, P 12 S OP SYST DES I
   [Anonymous], SYMP
   [Anonymous], 2004, P 6 S OP SYST DES I
   [Anonymous], 2017, OPEN HARDWARE MONITO
   [Anonymous], 2011, P 23 ACM S OP SYST P
   [Anonymous], 2016, P 7 ACM S CLOUD COMP
   Arpaci-Dusseau Andrea C., 2001, P 8 WORKSH HOT TOP O
   Attariyan Mona, 2010, P 9 S OP SYST DES IM
   Bairavasundaram Lakshmi N., 2007, P 2007 ACM C MEAS MO
   Bairavasundaram Lakshmi N., 2008, P 6 USENIX S FIL STO
   Baumann RC, 2005, IEEE T DEVICE MAT RE, V5, P305, DOI 10.1109/TDMR.2005.853449
   Brewer Eric, 2016, P 14 USENIX S FIL ST
   Cai Yu, 2015, P INT C DEP SSYST NE
   Cai Yu, 2015, P 15 INT S HIGH PERF
   Candea George, 2003, P 9 WORKSH HOT TOP O
   Chan Christine S., 2013, P GREENM WORKSH GREE
   Clement A., 2009, P 6 USENIX S NETW SY, P1
   Dean Daniel J, 2014, SOCC
   DO T., 2013, P 4 ACM S CLOUD COMP
   Do Thanh, 2013, P 11 USENIX S FIL ST
   El-Sayed Nosayba, 2012, P 2012 ACM INT C MEA
   Ganesan Aishwarya, 2017, P 15 USENIX S FIL ST
   Hao Mingzhe, 2017, P 26 ACM S OP SYST P
   Hao Mingzhe, 2016, P 14 USENIX S FIL ST
   Huang P, 2017, PROCEEDINGS OF THE 16TH WORKSHOP ON HOT TOPICS IN OPERATING SYSTEMS (HOTOS 2017), P150, DOI 10.1145/3102980.3103005
   Kadav A., 2009, P 22 ACM S OP SYST P
   Kasick Michael P., 2010, P 8 USENIX S FIL STO
   Kim Jaeho, 2015, P 13 USENIX S FIL ST
   Leesatapornwongsa Tanakorn, 2016, P 21 INT C ARCH SUPP
   Ma Ao, 2015, P 13 USENIX S FIL ST
   Meza Justin, 2015, P 2015 ACM INT C MEA
   Micron, 2011, NAND FLASH MED MAN R
   Pillai Thanumalayan Sankaranarayana, 2017, P 15 USENIX S FIL ST
   Prabhakaran Vijayan, 2005, P 20 ACM S OP SYST P
   Schroeder Bianca, 2009, P 2009 ACM INT C MEA
   Schroeder Bianca, 2007, P 5 USENIX S FIL STO
   Schroeder Bianca, 2016, P 14 USENIX S FIL ST
   Schroeder Bianca, 2010, P 8 USENIX S FIL STO
   Strom Brian D., 2007, IEEE T MAGNETICS TMA, V43, P9
   Suminto Riza O., 2017, P 8 ACM S CLOUD COMP
   Yaakobi Eitan, 2012, P INT C COMP NETW CO
   Yan Shiqin, 2017, P 15 USENIX S FIL ST
NR 42
TC 24
Z9 27
U1 3
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2018
VL 14
IS 3
AR 23
DI 10.1145/3242086
PG 26
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA HJ4JQ
UT WOS:000457140700004
OA Bronze
DA 2024-07-18
ER

PT J
AU Luo, HZ
   Liu, Q
   Hu, JT
   Li, Q
   Shi, L
   Zhuge, QF
   Sha, EHM
AF Luo, Huizhang
   Liu, Qing
   Hu, Jingtong
   Li, Qiao
   Shi, Liang
   Zhuge, Qingfeng
   Sha, Edwin H-M
TI Write Energy Reduction for PCM via Pumping Efficiency Improvement
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Phase change memory (PCM); charge pump; pumping efficiency; write
   regrouping
ID PHASE-CHANGE MEMORY; DESIGN; POWER
AB The emerging Phase Change Memory (PCM) is considered to be a promising candidate to replace DRAM as the next generation main memory due to its higher scalability and lower leakage power. However, the high write power consumption has become a major challenge in adopting PCM as main memory. In addition to the fact that writing to PCM cells requires high write current and voltage, current loss in the charge pumps also contributes a large percentage of high power consumption. The pumping efficiency of a PCM chip is a concave function of the write current. Leveraging the characteristics of the concave function, the overall pumping efficiency can be improved if the write current is uniform. In this article. we propose a peakto-average (PTA) write scheme, which smooths the write current fluctuation by regrouping write units. In particular, we calculate the current requirements for each write unit by their values when they are evicted from the last level cache (11C). When the write units are waiting in the memory controller, we regroup the write units by LLC-assisted PTA to reach the current-uniform goal. Experimental results show that LLC-assisted PTA achieved 13.4% of overall energy saving compared to the baseline.
C1 [Luo, Huizhang; Liu, Qing] New Jersey Inst Technol, 323 Dr Martin Luther King Jr Blvd, Newark, NJ 07102 USA.
   [Hu, Jingtong] Univ Pittsburgh, 4200 Fifth Ave, Pittsburgh, PA 15260 USA.
   [Li, Qiao] City Univ Hong Kong, Kowloon Tong, Tat Chee Ave, Hong Kong, Peoples R China.
   [Shi, Liang; Zhuge, Qingfeng; Sha, Edwin H-M] East China Normal Univ, 3663 North Zhongshan Rd, Shanghai 200062, Peoples R China.
C3 New Jersey Institute of Technology; Pennsylvania Commonwealth System of
   Higher Education (PCSHE); University of Pittsburgh; City University of
   Hong Kong; East China Normal University
RP Luo, HZ (corresponding author), New Jersey Inst Technol, 323 Dr Martin Luther King Jr Blvd, Newark, NJ 07102 USA.
EM huizhang.luo@njit.edu; qing.liu@njit.edu; jthu@pitt.edu;
   qiaoli045@gmail.com; shi.liang.hk@gmail.com; qfzhuge@gmail.com;
   edwinsha@gmail.com
OI Li, Qiao/0000-0002-4579-4268; Hu, Jingtong/0000-0003-4029-4034
FU NSFC [61472052, 61772092]; US NSF [CCF-1812861]; NJIT Research Startup
   Funds
FX This work is partially supported by NSFC 61472052, NSFC (61772092), US
   NSF under Grant No. CCF-1812861, and NJIT Research Startup Funds.
CR [Anonymous], 2018, ACM T STORAGE, V14
   Binkert Nathan, 2011, Computer Architecture News, V39, P1, DOI 10.1145/2024716.2024718
   Ferreira AP, 2010, DES AUT TEST EUROPE, P914
   Guthaus MR, 2001, WWC-4: IEEE INTERNATIONAL WORKSHOP ON WORKLOAD CHARACTERIZATION, P3, DOI 10.1109/WWC.2001.990739
   Hu JT, 2013, ACM T EMBED COMPUT S, V12, DOI 10.1145/2442116.2442127
   Jiang L, 2014, CONF PROC INT SYMP C, P397, DOI 10.1109/ISCA.2014.6853194
   Jiang L, 2012, INT SYMP MICROARCH, P1, DOI 10.1109/MICRO.2012.10
   Joshi M, 2011, INT S HIGH PERF COMP, P345, DOI 10.1109/HPCA.2011.5749742
   Lee BC, 2009, CONF PROC INT SYMP C, P2, DOI 10.1145/1555815.1555758
   Lee KJ, 2008, IEEE J SOLID-ST CIRC, V43, P150, DOI 10.1109/JSSC.2007.908001
   Luo HZ, 2016, ASIA S PACIF DES AUT, P450, DOI 10.1109/ASPDAC.2016.7428053
   Nair PJ, 2015, INT S HIGH PERF COMP, P309, DOI 10.1109/HPCA.2015.7056042
   Palangappa PM, 2016, INT S HIGH PERF COMP, P90, DOI 10.1109/HPCA.2016.7446056
   Palumbo G, 2006, IEE P-CIRC DEV SYST, V153, P136, DOI 10.1049/ip-cds:20041235
   Palumbo G, 2010, IEEE CIRC SYST MAG, V10, P31, DOI 10.1109/MCAS.2009.935695
   Qureshi MK, 2012, CONF PROC INT SYMP C, P380, DOI [10.1109/ISCA.2012.6237033, 10.1145/2366231.2337203]
   Qureshi MK, 2009, CONF PROC INT SYMP C, P24, DOI 10.1145/1555815.1555760
   Ramos Luiz E, 2011, P INT C SUP, P85
   Rodríguez-Rodríguez R, 2013, DES AUT TEST EUROPE, P93
   Sangyeun Cho, 2009, Proceedings of the 2009 42nd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO 2009), P347
   Xi Zhang, 2011, Advanced Parallel Processing Technologies. Proceedings 9th International Symposium, APPT 2011, P31, DOI 10.1007/978-3-642-24151-2_3
   Xia F, 2014, PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON SUPERCOMPUTING, (ICS'14), P211
   Yang BD, 2007, IEEE INT SYMP CIRC S, P3014, DOI 10.1109/ISCAS.2007.377981
   Youngdon Choi, 2012, 2012 IEEE International Solid-State Circuits Conference (ISSCC), P46, DOI 10.1109/ISSCC.2012.6176872
   Yue JH, 2013, INT S HIGH PERF COMP, P282, DOI 10.1109/HPCA.2013.6522326
   Zhang DJ, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0151864
   Zhao MY, 2015, ASIA S PACIF DES AUT, P502, DOI 10.1109/ASPDAC.2015.7059056
NR 27
TC 1
Z9 2
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2018
VL 14
IS 3
AR 27
DI 10.1145/3200139
PG 21
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ4JQ
UT WOS:000457140700008
DA 2024-07-18
ER

PT J
AU Kashyap, A
AF Kashyap, Anil
TI Workload Characterization for Enterprise Disk Drives
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Storage workload characterization; Enterprise storage systems
AB The article presents an analysis of drive workloads from enterprise storage systems. The drive workloads are obtained from field return units from a cross-section of enterprise storage system vendors and thus provides a view of the workload characteristics over a wide spectrum of end-user applications. The workload parameters that have been characterized include transfer lengths, access patterns, throughput, and utilization. The study shows that reads are the dominant workload accounting for 80% of the accesses to the drive. Writes are dominated by short block random accesses while reads range from random to highly sequential. A trend analysis over the period 2010-2014 shows that the workload has remained fairly constant even as the capacities of the drives shipped has steadily increased. The study shows that the data stored on disk drives is relatively cold-on average less than 4% of the drive capacity is accessed in a given 2h interval.
C1 [Kashyap, Anil] Seagate Technol, Cupertino, CA 95014 USA.
   [Kashyap, Anil] 1280 Disc Dr, Shakopee, MN 55379 USA.
C3 Seagate Technology
RP Kashyap, A (corresponding author), Seagate Technol, Cupertino, CA 95014 USA.; Kashyap, A (corresponding author), 1280 Disc Dr, Shakopee, MN 55379 USA.
CR [Anonymous], P IEEE INT S WORKL C
   Cano I., 2016, P ACM S CLOUD COMP S
   Chen Y., 2011, P ACM S OP SYST PRIN
   Graefe G, 2009, COMMUN ACM, V52, P48, DOI 10.1145/1538788.1538805
   Gray Jim., 1987, Proceedings of the 1987 ACM SIGMOD international conference on Management of data, P395
   Gulati A., 2009, P 2 INT WORKSH VIRT
   Leung AndrewW., 2008, USENIX ATC
   Riska A., 2009, P IEEE INT S WORKL C
   Riska A., 2006, P 2006 USENIX ANN TE
   Roselli D., 2000, P 2000 USENIX ANN TE
NR 10
TC 3
Z9 3
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAY
PY 2018
VL 14
IS 2
AR 19
DI 10.1145/3151847
PG 15
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GI6TM
UT WOS:000434635800008
DA 2024-07-18
ER

PT J
AU Joo, Y
   Park, S
   Bahn, H
AF Joo, Yongsoo
   Park, Sangsoo
   Bahn, Hyokyung
TI Exploiting I/O Reordering and I/O Interleaving to Improve Application
   Launch Performance
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Application launch performance; I/O reordering; I/O interleaving;
   prefetching
AB Application prefetchers improve application launch performance through either I/O reordering or I/O interleaving. However, there has been no proposal to combine the two techniques together, missing the opportunity for further optimization. We present a new application prefetching technique to take advantage of both the approaches. We evaluated our method with a set of applications to demonstrate that it reduces cold start application launch time by 50%, which is an improvement of 22% from the I/O reordering technique.
C1 [Joo, Yongsoo] Kookmin Univ, Sch Comp Sci, 77 Jeongneung Ro, Seoul 02707, South Korea.
   [Park, Sangsoo; Bahn, Hyokyung] Ewha Womans Univ, Dept Comp Sci & Engn, 52 Ewhayeodae Gil, Seoul 03760, South Korea.
C3 Kookmin University; Ewha Womans University
RP Park, S (corresponding author), Ewha Womans Univ, Dept Comp Sci & Engn, 52 Ewhayeodae Gil, Seoul 03760, South Korea.
EM ysjoo@kookmin.ac.kr; sangsoo.park@ewha.ac.kr; bahn@ewha.ac.kr
OI Joo, Yongsoo/0000-0001-8192-5029
FU Basic Science Research Program through the National Research Foundation
   of Korea (NRF) - Ministry of Education [NRF-2015R1D1A1A01058311]
FX This research was supported by Basic Science Research Program through
   the National Research Foundation of Korea (NRF) funded by the Ministry
   of Education (NRF-2015R1D1A1A01058311).
CR AKYUREK S, 1995, ACM T COMPUT SYST, V13, P89, DOI 10.1145/201045.201046
   [Anonymous], 2009, FAST
   BUCY JS, 2003, CMUCS03102
   Ding XN, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 2007 USENIX ANNUAL TECHNICAL CONFERENCE, P261
   Huang H., 2005, P 20 ACM S OP SYST P, P263, DOI DOI 10.1145/1095810.1095836
   Joo Yongsoo., 2011, FAST, P259
   Li ZM, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P173
   LUND K, 2003, P 11 ACM INT C MULT, P65
   Povzner A, 2008, EUROSYS'08: PROCEEDINGS OF THE EUROSYS 2008 CONFERENCE, P13, DOI 10.1145/1357010.1352595
   Reddy ALN, 2005, ACM T MULTIM COMPUT, V1, P37, DOI 10.1145/1047936.1047941
   RUSSINOVICH ME, 2004, MICROSOFT WINDOWS IN, P458
   Schindler J., 2011, P 9 USENIX C FILE ST, P133
   Worthington B. L., 1994, Performance Evaluation Review, V22, P241, DOI 10.1145/183019.183045
   Zhang Xuechen., 2011, Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis, SC '11, p18:1
NR 14
TC 7
Z9 10
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAR
PY 2017
VL 13
IS 1
SI SI
AR 8
DI 10.1145/3024094
PG 17
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ER3WE
UT WOS:000398729600008
DA 2024-07-18
ER

PT J
AU Wang, W
   Xie, T
   Sharma, A
AF Wang, Wei
   Xie, Tao
   Sharma, Abhinav
TI SWANS: An Interdisk Wear-Leveling Strategy for RAID-0 Structured SSD
   Arrays
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Algorithms; Performance; NAND flash memory; solid state disk;
   wear-leveling; SSD array
AB NAND flash memory-based solid state disks (SSDs) have been widely used in enterprise servers. However, flash memory has limited write endurance, as a block becomes unreliable after a finite number of program/erase cycles. Existing wear-leveling techniques are essentially intradisk data distribution schemes, as they can only even wear out across the flash medium within a single SSD. When multiple SSDs are organized in an array manner in server applications, an interdisk wear-leveling technique, which can ensure a uniform wear-out distribution across SSDs, is much needed. In this article, we propose a novel SSD-array level wear-leveling strategy called SWANS ((S) under bar moothing (W) under bar ear (A) under bar cross (N) under bar (S) under bar SDs) for an SSD array structured in a RAID-0 format, which is frequently used in server applications. SWANS dynamically monitors and balances write distributions across SSDs in an intelligent way. Further, to evaluate its effectiveness, we build an SSD array simulator on top of a validated single SSD simulator. Next, SWANS is implemented in its array controller. Comprehensive experiments with real-world traces show that SWANS decreases the standard deviation of writes across SSDs on average by 16.7x. The gap in the total bytes written between the most written SSD and the least written SSD in an 8-SSD array shrinks at least 1.3x.
C1 [Wang, Wei] San Diego State Univ, Computat Sci Res Ctr, San Diego, CA 92182 USA.
   [Xie, Tao] San Diego State Univ, Dept Comp Sci, San Diego, CA 92182 USA.
   [Sharma, Abhinav] Qualcomm Inc, San Diego, CA 92121 USA.
C3 California State University System; San Diego State University;
   California State University System; San Diego State University; Qualcomm
RP Wang, W (corresponding author), San Diego State Univ, Computat Sci Res Ctr, San Diego, CA 92182 USA.
EM wangw8210@gmail.com; txie@mail.sdsu.edu; asharma.sd001@gmail.com
FU U.S. National Science Foundation [CNS(CAREER)-0845105]
FX This work was supported in part by the U.S. National Science Foundation
   under grant CNS(CAREER)-0845105.
CR Agrawal Nitin, 2008, P USENIX ANN TECHN C, P57
   [Anonymous], P INT C HARDW SOFTW
   [Anonymous], 2011, ACM 11 P INT C SUPER
   Balaev M, 2012, NATURE OF TRAUMA IN AMERICAN NOVELS, P1
   Balakrishnan M, 2010, EUROSYS'10: PROCEEDINGS OF THE EUROSYS 2010 CONFERENCE, P15
   Bez R, 2003, P IEEE, V91, P489, DOI 10.1109/JPROC.2003.811702
   Birrell A., 2007, Operating Systems Review, V41, P88, DOI 10.1145/1243418.1243429
   Boboila S., 2010, P 8 USENIX C FIL STO, P9
   Chang LP, 2009, ACM T DES AUTOMAT EL, V15, DOI 10.1145/1640457.1640463
   Chen F, 2009, PERF E R SI, V37, P181
   Cherkasova L, 2004, IEEE ACM T NETWORK, V12, P781, DOI 10.1109/TNET.2004.836125
   Cullen Bruce, 2014, SSD DRIVE FAILURE WH
   Ganger Gregory R., 1999, TECHNICAL REPORT
   Gasior Geoff., 2013, Introducing the ssd endurance experiment
   Gomez ME, 2002, P INT S PERF EV COMP
   GRAY J, 1990, VERY LARGE DATA BASES, P148
   Gray J., 2008, ACM Queue, V6, P18, DOI 10.1145/1413254.1413261
   Gupta Aayush., 2009, DFTL: a flash translation layer employing demand-based selective caching of page-level address mappings, V44
   He Jiahua., 2010, P 2010 ACMIEEE INT C, P1
   Hu YZ, 2010, EURASIP J ADV SIG PR, DOI 10.1155/2010/454705
   Intel, 2013, INT SOL STAT DRIV SP
   Intel, 2014, INT SSD PROD COMP
   Jiang AX, 2010, IEEE T INFORM THEORY, V56, P5290, DOI 10.1109/TIT.2010.2059833
   Jung Dawoon., 2007, Proceedings of the 2007 international conference on Compilers, architecture, and synthesis for embedded systems, P160, DOI DOI 10.1145/1289881
   Kim Y, 2009, SIMUL: 2009 FIRST INTERNATIONAL CONFERENCE ON ADVANCES IN SYSTEM SIMULATION, P125, DOI 10.1109/SIMUL.2009.17
   Ku Andrew., 2011, Investigation: Is Your SSD More Reliable Than A Hard Drive?
   Lee SW, 2007, ACM T EMBED COMPUT S, V6, DOI 10.1145/1275986.1275990
   Lee Sang-Won., 2009, Proc. of the 35th SIGMOD Intl Conf. on Management of Data, P863
   Leutenegger S. T., 1993, SIGMOD Record, V22, P22, DOI 10.1145/170036.170042
   Mao B, 2012, ACM T STORAGE, V8, DOI 10.1145/2093139.2093143
   Mesnier M., 2001, Intel open storage toolkit
   Murugan Muthukumar., 2011, P 2011 IEEE 27 S MAS, P1
   Narayanan D, 2009, EUROSYS'09: PROCEEDINGS OF THE FOURTH EUROSYS CONFERENCE, P145
   NetApp, 2014, NETAPP EF540 TECHN S
   Pinheiro E, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 5TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES ( FAST '07), P17
   Schroeder B, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 5TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES ( FAST '07), P1
   SNIA IOTTA Repository, 2011, BUILD EC SERV TRAC
   Technologies Cactus, 2008, TECHNICAL REPORT
   Wu Q., 2011, P WORKSH HOT TOP STO
   Zertal Soraya, 2011, SIMULATION, V87, P12
NR 40
TC 10
Z9 17
U1 0
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUN
PY 2016
VL 12
IS 3
AR 10
DI 10.1145/2756555
PG 21
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DQ7YV
UT WOS:000379426200001
DA 2024-07-18
ER

PT J
AU Jannen, W
   Yuan, J
   Zhan, Y
   Akshintala, A
   Esmet, J
   Jiao, YZ
   Mittal, A
   Pandey, P
   Reddy, P
   Walsh, L
   Bender, MA
   Farach-Colton, M
   Johnson, R
   Kuszmaul, BC
   Porter, DE
AF Jannen, William
   Yuan, Jun
   Zhan, Yang
   Akshintala, Amogh
   Esmet, John
   Jiao, Yizheng
   Mittal, Ankur
   Pandey, Prashant
   Reddy, Phaneendra
   Walsh, Leif
   Bender, Michael A.
   Farach-Colton, Martin
   Johnson, Rob
   Kuszmaul, Bradley C.
   Porter, Donald E.
TI B<i>e</i>trFS: Write-Optimization in a Kernel File System
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article; Proceedings Paper
CT 13th USENIX Conference on File and Storage Technologies (FAST)
CY FEB 16-19, 2015
CL Santa Clara, CA
SP USENIX, ACM SIGOPS
DE Design; Performance; Theory; B-epsilon-trees; file system; write
   optimization
AB The B-epsilon-tree File System, or BetrFS (pronounced "better eff ess"), is the first in-kernel file system to use a write-optimized data structure (WODS). WODS are promising building blocks for storage systems because they support both microwrites and large scans efficiently. Previous WODS-based file systems have shown promise but have been hampered in several ways, which BetrFS mitigates or eliminates altogether. For example, previous WODS-based file systems were implemented in user space using FUSE, which superimposes many reads on a write-intensive workload, reducing the effectiveness of the WODS. This article also contributes several techniques for exploiting write-optimization within existing kernel infrastructure. BetrFS dramatically improves performance of certain types of large scans, such as recursive directory traversals, as well as performance of arbitrary microdata operations, such as file creates, metadata updates, and small writes to files. BetrFS can make small, random updates within a large file 2 orders of magnitude faster than other local file systems. BetrFS is an ongoing prototype effort and requires additional data-structure tuning to match current general-purpose file systems on some operations, including deletes, directory renames, and large sequential writes. Nonetheless, many applications realize significant performance improvements on BetrFS. For instance, an in-place rsync of the Linux kernel source sees roughly 1.6-22x speedup over commodity file systems.
C1 [Jannen, William; Yuan, Jun; Zhan, Yang; Akshintala, Amogh; Jiao, Yizheng; Pandey, Prashant; Bender, Michael A.; Johnson, Rob; Porter, Donald E.] SUNY Stony Brook, New Comp Sci, Stony Brook, NY 11794 USA.
   [Esmet, John] AppNexus, Brooklyn, NY 11209 USA.
   [Mittal, Ankur] Microsoft, Redmond, WA 98052 USA.
   [Reddy, Phaneendra] EMC, Edison, NJ 08817 USA.
   [Walsh, Leif] Two Sigma Investments, Two Sigma, New York, NY 10013 USA.
   [Farach-Colton, Martin] Rutgers State Univ, Dept Comp Sci, Piscataway, NJ 08854 USA.
   [Kuszmaul, Bradley C.] MIT, Cambridge, MA 02139 USA.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Stony Brook; Microsoft; Rutgers University System; Rutgers
   University New Brunswick; Massachusetts Institute of Technology (MIT)
RP Jannen, W (corresponding author), SUNY Stony Brook, New Comp Sci, Stony Brook, NY 11794 USA.
EM wjannen@cs.stonybrook.edu; junyuan@cs.stonybrook.edu;
   yazhan@cs.stonybrook.edu; aakshintala@cs.stonybrook.edu;
   john.esmet@gmail.com; yjiao@cs.stonybrook.edu; ankurmit2006@gmail.com;
   ppandey@cs.stonybrook.edu; pyreddy@cs.stonybrook.edu; leif@twosigma.com;
   bender@cs.stonybrook.edu; farach@cs.rutgers.edu; rob@cs.stonybrook.edu;
   bradley@mit.edu; porter@cs.stonybrook.edu
OI Farach-Colton, Martin/0000-0003-3616-7788; Bender,
   Michael/0000-0001-7639-530X
FU NSF [CNS-1409238, CNS-1408782, CNS-1408695, CNS-1405641, CNS-1149229,
   CNS-1161541, CNS-1228839, IIS-1247750, CCF-1314547]; Office of the Vice
   President for Research at Stony Brook University; Direct For Computer &
   Info Scie & Enginr; Division of Computing and Communication Foundations
   [1314547] Funding Source: National Science Foundation; Division Of
   Computer and Network Systems; Direct For Computer & Info Scie & Enginr
   [1149229, 1405641, 1408695, 1409238] Funding Source: National Science
   Foundation; Division of Computing and Communication Foundations; Direct
   For Computer & Info Scie & Enginr [1533644, 1217708] Funding Source:
   National Science Foundation; Div Of Information & Intelligent Systems;
   Direct For Computer & Info Scie & Enginr [1247750, 1247726] Funding
   Source: National Science Foundation
FX This research was supported in part by NSF grants CNS-1409238,
   CNS-1408782, CNS-1408695, CNS-1405641, CNS-1149229, CNS-1161541,
   CNS-1228839, IIS-1247750, CCF-1314547, and the Office of the Vice
   President for Research at Stony Brook University.
CR AGGARWAL A, 1988, COMMUN ACM, V31, P1116, DOI 10.1145/48529.48535
   Andersen DG, 2009, SOSP'09: PROCEEDINGS OF THE TWENTY-SECOND ACM SIGOPS SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P1
   [Anonymous], ZFS LAST WORD FILE S
   BENDER MA, 2015, MAGAZINE, V40, P5
   Bender MA, 2010, PODS 2010: PROCEEDINGS OF THE TWENTY-NINTH ACM SIGMOD-SIGACT-SIGART SYMPOSIUM ON PRINCIPLES OF DATABASE SYSTEMS, P305, DOI 10.1145/1807085.1807125
   Bender MA, 2007, SPAA'07: PROCEEDINGS OF THE NINETEENTH ANNUAL SYMPOSIUM ON PARALLELISM IN ALGORITHMS AND ARCHITECTURES, P81
   Bent John., 2009, High Performance Computing Networking, Storage and Analysis, Proceedings of the Conference on, P1
   Brodal GS, 2010, PROC APPL MATH, V135, P1448
   Brodal GS, 2003, SIAM PROC S, P546
   Buchsbaum AL, 2000, PROCEEDINGS OF THE ELEVENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P859
   Chang F, 2008, ACM T COMPUT SYST, V26, DOI 10.1145/1365815.1365816
   Chazelle B, 1986, ALGORITHMICA, V1, P133, DOI 10.1007/BF01840440
   COMER D, 1979, COMPUT SURV, V11, P121, DOI 10.1145/356770.356776
   Douthitt David, 2011, INSTANT 10 20 BOOST
   Esmet J., 2012, HotStorage
   FUSE, 2015, FIL SYST US
   Hitz Dave, 1994, TECHNICAL REPORT
   Jannen W., 2015, P 13 USENIX C FIL ST, P301, DOI DOI 10.5555/2750482.2750505
   Lakshman Avinash, 2010, Operating Systems Review, V44, P35, DOI 10.1145/1773912.1773922
   Lee Changman, 2015, 13 USENIX C FILE STO, P273
   Lim H, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P1
   Macko P., 2010, 8th USENIX Conference on File and Storage Technologies, San Jose, CA, USA, February 23-26, 2010, P15
   ONeil P, 1996, ACTA INFORM, V33, P351, DOI 10.1007/s002360050048
   PengWang Guangyu Sun, 2014, P 9 EUR C COMP SYST, V16, P14
   Quick LZ, 2015, FAST COMPRESSION LIB
   Ren Kai., 2013, USENIX ANN TECHNICAL, P145
   Reuter A, 1993, T PROCESSING CONCEPT
   Rodeh O, 2013, ACM T STORAGE, V9, DOI 10.1145/2501620.2501623
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   Sears R, 2008, PROC VLDB ENDOW, V1, P526, DOI 10.14778/1453856.1453914
   Sears Russell, 2012, P ACM SIGMOD INT C M, P217
   SELTZER M, 1993, PROCEEDINGS OF THE WINTER 1993 USENIX CONFERENCE, P307
   Seltzer M., 1995, TCON'95: Proceedings of the USENIX 1995 Technical Confer- ence Proceedings on USENIX 1995 Technical Conference Proceedings, P21
   Shetty P.J., 2013, 11 USENIX C FILE STO, P17
   Sweeney A, 1996, PROCEEDINGS OF THE USENIX 1996 ANNUAL TECHNICAL CONFERENCE, P1
   Tokutek Inc, 2013, TOKUMX MONGODB PERF
   Tokutek Inc, 2013, TOKUDB MYSQL PERF
   Tweedie, 1994, P 1 DUTCH INT S LIN, P1
   Wu X., 2015, 2015 USENIX ANN TECH, P71
NR 39
TC 28
Z9 35
U1 1
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2015
VL 11
IS 4
SI SI
AR 18
DI 10.1145/2798729
PG 29
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DB0GY
UT WOS:000368187800004
OA Bronze
DA 2024-07-18
ER

PT J
AU Tong, QY
   Li, XH
   Miao, YB
   Wang, YW
   Liu, XM
   Deng, RH
AF Tong, Qiuyun
   Li, Xinghua
   Miao, Yinbin
   Wang, Yunwei
   Liu, Ximeng
   Deng, Robert H.
TI Owner-free Distributed Symmetric Searchable Encryption Supporting
   Conjunctive Queries
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Symmetric searchable encryption; multi-user; conjunctive query;
   dual-cloud architecture
ID EFFICIENT
AB Symmetric Searchable Encryption (SSE), as an ideal primitive, can ensure data privacy while supporting retrieval over encrypted data. However, existing multi-user SSE schemes require the data owner to share the secret key with all query users or always be online to generate search tokens. While there are some solutions to this problem, they have at least one weakness, such as non-supporting conjunctive query, result decryption assistance of the data owner, and unauthorized access. To solve the above issues, we propose an Owner-free Distributed Symmetric searchable encryption supporting Conjunctive query (ODiSC). Specifically, we first evaluate the Learning-Parity-with-Noise weak Pseudorandom Function (LPN-wPRF) in dual-cloud architecture to generate search tokens with the data owner free from sharing key and being online. Then, we provide fine-grained conjunctive query in the distributed architecture using additive secret sharing and symmetric-key hidden vector encryption. Finally, formal security analysis and empirical performance evaluation demonstrate that ODiSC is adaptively simulation-secure and efficient.
C1 [Tong, Qiuyun; Li, Xinghua; Miao, Yinbin; Wang, Yunwei] Xidian Univ, Sch Cyber Engn, Xian 710071, Peoples R China.
   [Li, Xinghua] Minist Educ, Engn Res Ctr Big Data Secur, Xian 710071, Peoples R China.
   [Liu, Ximeng] Fuzhou Univ, Coll Comp & Data Sci, Fuzhou 350108, Peoples R China.
   [Deng, Robert H.] Singapore Management Univ, Sch Informat Syst, Singapore 188065, Singapore.
C3 Xidian University; Fuzhou University; Singapore Management University
RP Li, XH; Miao, YB (corresponding author), Xidian Univ, Sch Cyber Engn, Xian 710071, Peoples R China.; Li, XH (corresponding author), Minist Educ, Engn Res Ctr Big Data Secur, Xian 710071, Peoples R China.
EM qytong0820@163.com; xhli1@mail.xidian.edu.cn; ybmiao@xidian.edu.cn;
   wywxidian@foxmail.com; snbnix@gmail.com; robertdeng@smu.edu.sg
RI Liu, Ximeng/AAE-2151-2019; li, xinghua/D-1775-2015
OI Liu, Ximeng/0000-0002-4238-3295; , Tong/0000-0003-4715-5627; Miao,
   Yinbin/0000-0001-5437-3572; Deng, Robert/0000-0003-3491-8146
FU National Natural Science Foundation of China [62125205, 62072361]; Key
   Research and Development Program of Shaanxi [2023KXJ-190]; Fundamental
   Research Funds for the Central Universities [XJSJ23188, YJSJ23007]
FX This work was supported by the National Natural Science Foundation of
   China (No. 62125205, No. 62072361), the Key Research and Development
   Program of Shaanxi (No. 2023KXJ-190), and the Fundamental Research Funds
   for the Central Universities (No. XJSJ23188, No. YJSJ23007).
CR Ali A, 2021, PROCEEDINGS OF THE 30TH USENIX SECURITY SYMPOSIUM, P1811
   [Anonymous], 2013, P ACM SIGSAC C COMP, DOI 10.1145/2508859.2516730
   BEAVER D, 1992, LECT NOTES COMPUT SC, V576, P420
   Bogdanov D, 2012, INT J INF SECUR, V11, P403, DOI 10.1007/s10207-012-0177-2
   Bogdanov D, 2008, LECT NOTES COMPUT SC, V5283, P192
   Boneh D, 2018, LECT NOTES COMPUT SC, V11240, P699, DOI 10.1007/978-3-030-03810-6_25
   Cash D, 2014, 21ST ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2014), DOI 10.14722/ndss.2014.23264
   Cash D, 2013, LECT NOTES COMPUT SC, V8042, P353, DOI 10.1007/978-3-642-40041-4_20
   Cheng K, 2018, PROCEEDINGS OF THE 2018 ACM ASIA CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (ASIACCS'18), P237, DOI 10.1145/3196494.3196535
   Cui J, 2019, INFORM SCIENCES, V489, P63, DOI 10.1016/j.ins.2019.03.043
   Curtmola R, 2011, J COMPUT SECUR, V19, P895, DOI 10.3233/JCS-2011-0426
   Demmler D, 2015, 22ND ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2015), DOI 10.14722/ndss.2015.23113
   Dinur I, 2021, LECT NOTES COMPUT SC, V12828, P517, DOI 10.1007/978-3-030-84259-8_18
   Kermanshahi SK, 2021, IEEE T DEPEND SECURE, V18, P2419, DOI 10.1109/TDSC.2019.2950934
   Lai SQ, 2018, PROCEEDINGS OF THE 2018 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (CCS'18), P745, DOI 10.1145/3243734.3243753
   Li JY, 2022, IEEE T CLOUD COMPUT, V10, P2005, DOI 10.1109/TCC.2020.3024226
   Lin HY, 2012, IEEE T PARALL DISTR, V23, P995, DOI 10.1109/TPDS.2011.252
   Liu L, 2019, IEEE INTERNET THINGS, V6, P9841, DOI 10.1109/JIOT.2019.2932444
   Liu XQ, 2020, IEEE T DEPEND SECURE, V17, P1322, DOI 10.1109/TDSC.2018.2876831
   Liu Y, 2022, IEEE T INF FOREN SEC, V17, P69, DOI 10.1109/TIFS.2019.2946476
   Mandavi RA, 2022, PROCEEDINGS OF THE 31ST USENIX SECURITY SYMPOSIUM, P1723
   Miao YB, 2021, IEEE T DEPEND SECURE, V18, P1804, DOI 10.1109/TDSC.2019.2940573
   Patel S, 2019, PROCEEDINGS OF THE 2019 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (CCS'19), P79, DOI 10.1145/3319535.3354213
   Patranabis Sikhar, 2017, Cryptology ePrint Archive
   Schneider T, 2019, PROCEEDINGS OF THE 2019 ACM ASIA CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (ASIACCS '19), P315, DOI 10.1145/3321705.3329800
   Song DXD, 2000, P IEEE S SECUR PRIV, P44, DOI 10.1109/SECPRI.2000.848445
   Sun SF, 2022, IEEE T DEPEND SECURE, V19, P452, DOI 10.1109/TDSC.2020.2973633
   Tong QY, 2022, IEEE T SERV COMPUT, V15, P3606, DOI 10.1109/TSC.2021.3083512
   Tong QY, 2023, IEEE T KNOWL DATA EN, V35, P5386, DOI 10.1109/TKDE.2022.3152033
   Tong QY, 2023, IEEE T KNOWL DATA EN, V35, P5159, DOI 10.1109/TKDE.2022.3152168
   Tong QY, 2022, IEEE T CLOUD COMPUT, V10, P2964, DOI 10.1109/TCC.2020.3031209
   Wang Jianfeng, 2022, CCS '22: Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security, P2825, DOI 10.1145/3548606.3559345
   Wang Mingyue, 2021, PROC IEEE INT C COMM, P1
   Wang XY, 2020, IEEE INFOCOM SER, P2253, DOI [10.1109/infocom41043.2020.9155505, 10.1109/INFOCOM41043.2020.9155505]
   Wang YJ, 2006, COMPUT MATH APPL, V51, P1625, DOI 10.1016/j.camwa.2006.05.002
   Xia ZH, 2020, Arxiv, DOI arXiv:2009.06893
   Yang Y, 2022, IEEE T INF FOREN SEC, V17, P1083, DOI 10.1109/TIFS.2022.3156818
   Zhang K, 2021, IEEE T DEPEND SECURE, V18, P2875, DOI 10.1109/TDSC.2020.2968425
   Zheng YD, 2022, IEEE T INF FOREN SEC, V17, P880, DOI 10.1109/TIFS.2022.3152395
   Zhou L, 2017, COMPUT SECUR, V69, P84, DOI 10.1016/j.cose.2016.11.013
   Zhu Y., 2013, P 2013 INT WORKSH SE, P55, DOI [DOI 10.1145/2484402.2484415, 10.1145/2484402.2484415]
NR 41
TC 0
Z9 0
U1 1
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2023
VL 19
IS 4
AR 38
DI 10.1145/3607255
PG 25
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X6NQ6
UT WOS:001099601100010
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Einziger, G
   Himelbrand, O
   Waisbard, E
AF Einziger, Gil
   Himelbrand, Omri
   Waisbard, Erez
TI Boosting Cache Performance by Access Time Measurements
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Cross domain caching; access times aware caching; dynamic caching;
   access time dataset
ID ALGORITHMS
AB Most modern systems utilize caches to reduce the average data access time and optimize their performance. Recently proposed policies implicitly assume uniform access times, but variable access times naturally appear in domains such as storage, web search, and DNS resolution.
   Our work measures the access times for various items and exploits variations in access times as an additional signal for caching algorithms. Using such a signal, we introduce adaptive access time-aware cache policies that consistently improve the average access time compared with the best alternative in diverse workloads. Our adaptive algorithm attains an average access time reduction of up to 46% in storage workloads, up to 16% in web searches, and 8.4% on average when considering all experiments in our study.
C1 [Einziger, Gil; Himelbrand, Omri] Ben Gurion Univ Negev, Comp Sci Dept, IL-84105 Beer Sheva, Israel.
   [Waisbard, Erez] Open Univ, Comp Sci Dept, 1 Univ Rd, IL-43107 Raanana, Israel.
C3 Ben Gurion University; Open University Israel
RP Einziger, G (corresponding author), Ben Gurion Univ Negev, Comp Sci Dept, IL-84105 Beer Sheva, Israel.
CR Akhtar S, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3106157
   [Anonymous], 2023, ACM T STORAGE, V19
   [Anonymous], 2002, NMSU TraceBase
   AOL, 2006, AOL US SESS COLL
   Apache, 2010, AP CASS
   Apache, 2012, AP SOLR
   Araldo A, 2014, IEEE GLOB COMM CONF, P1108, DOI 10.1109/GLOCOM.2014.7036957
   Arlitt M, 2000, PERFORM EVALUATION, V39, P149, DOI 10.1016/S0166-5316(99)00062-0
   Arlitt Martin, 1999, PROC 2 WORKSHOP INTE
   Bakkal E, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P739, DOI 10.1145/2766462.2767813
   BELADY LA, 1966, IBM SYST J, V5, P78, DOI 10.1147/sj.52.0078
   Berger DS, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P195
   Blankstein A, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P499
   Cao P, 1997, PROCEEDINGS OF THE USENIX SYMPOSIUM ON INTERNET TECHNOLOGIES AND SYSTEMS, P193
   Chakraborty A, 2011, J SUPERCOMPUT, V56, P56, DOI 10.1007/s11227-009-0342-1
   Cormode G, 2004, LECT NOTES COMPUT SC, V2976, P29, DOI 10.1007/978-3-540-24698-5_7
   Dropwizard, 2011, DROPW SNEAK WAY MAK
   DuckDuckGo, 2008, SEARCH ENGINE
   Einziger G, 2018, MIDDLEWARE'18: PROCEEDINGS OF THE 2018 ACM/IFIP/USENIX MIDDLEWARE CONFERENCE, P94, DOI 10.1145/3274808.3274816
   Einziger G, 2017, ACM T STORAGE, V13, DOI 10.1145/3149371
   Fan L, 2000, IEEE ACM T NETWORK, V8, P281, DOI 10.1109/90.851975
   Forney BC, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE FAST'02 CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P61
   Ghandeharizadeh S, 2014, ACM/IFIP/USENIX MIDDLEWARE 2014, P289, DOI 10.1145/2663165.2663317
   Ghodke S., 2018, Alexa top 1 million sites
   Hennessy J. L., 2012, Computer Architecture A Quantitative Approach, V5th
   Hou BB, 2017, ACM T STORAGE, V13, DOI 10.1145/3149374
   Huang YN, 2013, INT CONF CLOUD COMP, P248, DOI 10.1109/CloudCom.2013.140
   Jeong J, 2003, NINTH INTERNATIONAL SYMPOSIUM ON HIGH-PERFORMANCE COMPUTER ARCHITECTURE, PROCEEDINGS, P327, DOI 10.1109/HPCA.2003.1183550
   Kaggle Hofe Siy, 2020, 2017 2019 SEARCH ENG
   Karakostas G, 2002, IEEE SYMP COMP COMMU, P207, DOI 10.1109/ISCC.2002.1021680
   KAREDLA R, 1994, COMPUTER, V27, P38, DOI 10.1109/2.268884
   Ketan Prof, 2010, O 1 ALGORITHM IMPLEM
   Kingma D. P., 2014, arXiv
   Lee C, 2017, SYSTOR'17: PROCEEDINGS OF THE 10TH ACM INTERNATIONAL SYSTEMS AND STORAGE CONFERENCE, DOI 10.1145/3078468.3078479
   Lee D, 2001, IEEE T COMPUT, V50, P1352, DOI 10.1109/tc.2001.970573
   Li C, 2018, SYSTOR'18: PROCEEDINGS OF THE 11TH ACM INTERNATIONAL SYSTEMS AND STORAGE CONFERENCE, P59, DOI 10.1145/3211890.3211891
   Li C, 2017, INT WIREL COMMUN, P2169, DOI 10.1109/IWCMC.2017.7986619
   Li CJ, 2015, AER ADV ENG RES, V26, P5
   Liang S, 2007, LECT NOTES COMPUT SC, V4731, P373
   Liberatore M., 2016, UMASS TRACE REPOSITO
   Liu C., 2006, DNS and BIND
   Lv YF, 2010, LECT NOTES COMPUT SC, V6184, P558
   Manes Ben, 2016, Caffeine: A High Performance Caching Library for Java 8
   Megiddo N, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE 2ND USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST'03), P115
   Neglia G, 2017, ACM TRANS MODELING P, V2, DOI 10.1145/3149001
   Nesterov Yu. E., 1983, Doklady Akademii Nauk SSSR, V269, P543
   Ozcan R, 2011, ACM T WEB, V5, DOI 10.1145/1961659.1961663
   Park Sejin, 2017, PROC 33 IEEE INT C M
   Qureshi MK, 2006, CONF PROC INT SYMP C, P167, DOI 10.1145/1150019.1136501
   Redisson, 2012, RED RED JAV CLIENT
   Song Jiang, 2002, Performance Evaluation Review, V30, P31, DOI 10.1145/511399.511340
   Urdaneta G, 2009, COMPUT NETW, V53, P1830, DOI 10.1016/j.comnet.2009.02.019
   UserBenchmark, USERBENCHMARK
   Wood Timothy, 2009, Operating Systems Review, V43, P27, DOI 10.1145/1618525.1618529
   Xiaoming Du, 2021, Middleware '21: Proceedings of the 22nd International Middleware Conference, P119, DOI 10.1145/3464298.3493389
   Zhang M, 2019, IEEE S MASS STOR SYS, P257, DOI 10.1109/MSST.2019.00006
NR 56
TC 1
Z9 1
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD FEB
PY 2023
VL 19
IS 1
AR 8
DI 10.1145/3572778
PG 29
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 9I5DE
UT WOS:000939530200008
OA Bronze
DA 2024-07-18
ER

PT J
AU Lin, LF
   Deng, YH
   Zhou, Y
   Zhu, YF
AF Lin, Lifang
   Deng, Yuhui
   Zhou, Yi
   Zhu, Yifeng
TI InDe: An Inline Data Deduplication Approach via Adaptive Detection of
   Valid Container Utilization
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Data deduplication; restore performance; storage system
AB Inline deduplication removes redundant data in real-time as data is being sent to the storage system. However, it causes data fragmentation: logically consecutive chunks are physically scattered across various containers after data deduplication. Many rewrite algorithms aim to alleviate the performance degradation due to fragmentation by rewriting fragmented duplicate chunks as unique chunks into new containers. Unfortunately, these algorithms determine whether a chunk is fragmented based on a simple pre-set fixed value, ignoring the variance of data characteristics between data segments. Accordingly, when backups are restored, they often fail to select an appropriate set of old containers for rewrite, generating a substantial number of invalid chunks in retrieved containers.
   To address this issue, we propose an inline deduplication approach for storage systems, called InDe, which uses a greedy algorithm to detect valid container utilization and dynamically adjusts the number of old container references in each segment. InDe fully leverages the distribution of duplicated chunks to improve the restore performance while maintaining high backup performance. We define an effectiveness metric, valid container referenced counts (VCRC), to identify appropriate containers for the rewrite. We design a rewrite algorithm F-greedy that detects valid container utilization to rewrite low-VCRC containers. According to the VCRC distribution of containers, F-greedy dynamically adjusts the number of old container references to only share duplicate chunks with high-utilization containers for each segment, thereby improving the restore speed. To take full advantage of the above features, we further propose another rewrite algorithm called F-greedy+ based on adaptive interval detection of valid container utilization. F-greedy+ makes a more accurate estimation of the valid utilization of old containers by detecting trends of VCRC's change in two directions and selecting referenced containers in the global scope. We quantitatively evaluate InDe using three real-world backup workloads. The experimental results show that compared with two state-of-the-art algorithms (Capping and SMR), our scheme improves the restore speed by 1.3x-2.4x while achieving almost the same backup performance.
C1 [Lin, Lifang; Deng, Yuhui] Jinan Univ, Dept Comp Sci, Guangzhou, Guangdong, Peoples R China.
   [Zhou, Yi] Columbus State Univ, TSYS Sch Comp Sci, 4225 Univ Ave, Columbus, GA 31907 USA.
   [Zhu, Yifeng] Univ Maine, Dept Elect & Comp Engn, Orono, ME USA.
C3 Jinan University; University System of Georgia; Columbus State
   University; University of Maine System; University of Maine Orono
RP Deng, YH (corresponding author), Jinan Univ, Dept Comp Sci, Guangzhou, Guangdong, Peoples R China.
EM lflin@stu2019.jnu.edu.cn; tyhdeng@email.jnu.edu.cn;
   zhou_yi@columbusstate.edu; zhu@eece.maine.edu
RI zhan, xiao/JEZ-3810-2023; Yang, Xiao/JDN-0082-2023; wang,
   wei/JBS-7400-2023; LI, Xiang/JBJ-8387-2023; Chen, Xin/JDN-2017-2023;
   Wang, Yanlin/JGC-6782-2023; zhu, zhu/JDN-0159-2023
OI , Yi/0000-0002-1460-322X; Deng, Yuhui/0000-0002-1522-8943
FU National Natural Science Foundation of China [62072214]; Guangdong Basic
   and Applied Basic Research Foundation [2021B1515120048]; International
   Cooperation Project of Guangdong Province [2020A0505100040]; Science and
   Technology Planning Project of Guangzhou [202103000036]; Open Project
   Program ofWuhan National Laboratory for Optoelectronics [2020WNLOKF006];
   Industry-University-Research Collaboration Project of Zhuhai
   [ZH22017001210048PWC]
FX This work is sponsored by the National Natural Science Foundation of
   China under Grant No. 62072214, Guangdong Basic and Applied Basic
   Research Foundation under Grant No. 2021B1515120048, the International
   Cooperation Project of Guangdong Province under Grant No.
   2020A0505100040, the Science and Technology Planning Project of
   Guangzhou under Grant No. 202103000036, the Open Project Program ofWuhan
   National Laboratory for Optoelectronics Grant No. 2020WNLOKF006, and the
   Industry-University-Research Collaboration Project of Zhuhai under Grant
   No. ZH22017001210048PWC.
CR [Anonymous], 2011, P USENIX ANN TECHNIC
   Bauer R., 2018, HDD VS SSD WHAT DOES
   Cao ZC, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P129
   Chavan A, 2016, IEEE T PARALL DISTR, V27, P558, DOI 10.1109/TPDS.2015.2409872
   Dell Technologies, 2021, IDC BUS VAL STOR SOL
   Deng YH, 2017, J INF SCI ENG, V33, P1103, DOI 10.6688/JISE.2017.33.5.1
   FSL, 2021, TRAC SNAPSH PUBL ARC
   Fu M, 2016, IEEE T PARALL DISTR, V27, P855, DOI 10.1109/TPDS.2015.2410781
   Fu Min, 2014, 2014 USENIX C USENIX, P181
   Fu Min, 2015, 13 USENIX C FIL STOR
   Guo F, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P733
   Kaczmarczyk Michal., 2012, Proceedings of the 5th Annual International Systems and Storage Conference, P1
   Kohavi R, 2007, KDD-2007 PROCEEDINGS OF THE THIRTEENTH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P959
   Lai RY, 2014, LECT NOTES COMPUT SC, V8630, P457, DOI 10.1007/978-3-319-11197-1_35
   Lillibridge M., 2013, FAST
   Lin Lifang, 2021, P 27 INT C PARALLEL
   Lin X., 2014, P 12 USENIX C FIL ST, V14, P256
   Liu J, 2014, INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY AND MANAGEMENT ENGINEERING (ITME 2014), P1
   Luo SM, 2020, IEEE T CLOUD COMPUT, V8, P1199, DOI 10.1109/TCC.2015.2511752
   Ma JW, 2017, ACM T STORAGE, V13, DOI 10.1145/3078837
   Mseddi A, 2021, IEEE T CLOUD COMPUT, V9, P155, DOI 10.1109/TCC.2018.2858792
   Nachman A, 2020, PROCEEDINGS OF THE 18TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P193
   Ni F, 2019, PROCEEDINGS OF THE 2019 TENTH ACM SYMPOSIUM ON CLOUD COMPUTING (SOCC '19), P220, DOI 10.1145/3357223.3362731
   Strzelczak Przemyslaw., 2013, FAST, P161
   Sun Z, 2016, IEEE S MASS STOR SYS
   Tan YJ, 2021, IEEE T PARALL DISTR, V32, P214, DOI 10.1109/TPDS.2020.3012704
   Tan YJ, 2018, IEEE T PARALL DISTR, V29, P2254, DOI 10.1109/TPDS.2018.2828842
   Tarasov V., 2012, 2012 F USENIX G ANN, P261
   Wu CQ, 2015, IEEE T CLOUD COMPUT, V3, P169, DOI 10.1109/TCC.2014.2358220
   Wu J, 2019, IEEE T PARALL DISTR, V30, P119, DOI 10.1109/TPDS.2018.2852642
   Wu SZ, 2019, IEEE T PARALL DISTR, V30, P2117, DOI 10.1109/TPDS.2019.2898942
   Xia N, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P325
   Xia W, 2014, IEEE DATA COMPR CONF, P203, DOI 10.1109/DCC.2014.38
   Xia W, 2020, IEEE T PARALL DISTR, V31, P2017, DOI 10.1109/TPDS.2020.2984632
   Yang R., 2021, ACM/IMS Trans. Data Sci., V2, P1
   Zhang DT, 2021, ACM T STORAGE, V17, DOI 10.1145/3459626
   Zhang Y., 2015, PROC IEEE C COMPUT C, P1337
   Zhao NN, 2020, PROCEEDINGS OF THE 2020 USENIX ANNUAL TECHNICAL CONFERENCE, P769
   Zhou YT, 2018, IEEE ACCESS, V6, P15743, DOI 10.1109/ACCESS.2018.2800763
   Zhu B, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P269
   Zou XY, 2021, PROCEEDINGS OF THE 19TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '21), P171
NR 41
TC 2
Z9 2
U1 1
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD FEB
PY 2023
VL 19
IS 1
AR 6
DI 10.1145/3568426
PG 27
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 9I5DE
UT WOS:000939530200006
DA 2024-07-18
ER

PT J
AU Qi, SQ
   Feng, D
   Su, N
   Mei, LJ
   Liu, JN
AF Qi, Shigui
   Feng, Dan
   Su, Nan
   Mei, Linjun
   Liu, Jingning
TI CDF-LDPC: A New Error Correction Method for SSD to Improve the Read
   Performance
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Reliability; Performance; Algorithms; Solid-state drives; low density
   parity check; read performance; error correction code; error detection
   code
ID RANDOM TELEGRAPH NOISE; FLASH; PARALLELISM
AB The raw error rate of a Solid-State drive (SSD) increases gradually with the increase of Program/Erase (P/E) cycles, retention time, and read cycles. Traditional approaches often use Error Correction Code (ECC) to ensure the reliability of SSDs. For error-free flash memory pages, time costs spent on ECC are redundant and make read performance suboptimal. This article presents a CRC-Detect-First LDPC (CDF-LDPC) algorithm to optimize the read performance of SSDs. The basic idea is to bypass Low-Density Parity-Check (LDPC) decoding of error-free flash memory pages, which can be found using a Cyclic Redundancy Check (CRC) code. Thus, error-free pages can be read directly without sacrificing the reliability of SSDs. Experiment results show that the read performance is improved more than 50% compared with traditional approaches. In particular, when idle time of benchmarks and SSD parallelism are exploited, CDF-LDPC can be performed more efficiently. In this case, the read performance of SSDs can be improved up to about 80% compared to that of the state-of-art.
C1 [Qi, Shigui; Feng, Dan; Su, Nan; Mei, Linjun; Liu, Jingning] Huazhong Univ Sci & Technol, Wuhan Natl Lab Optoelect, Wuhan, Peoples R China.
   [Qi, Shigui; Feng, Dan; Su, Nan; Mei, Linjun; Liu, Jingning] Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan, Peoples R China.
   [Qi, Shigui] Xuchang Univ, Xuchang, Henan, Peoples R China.
C3 Huazhong University of Science & Technology; Huazhong University of
   Science & Technology; Xuchang University
RP Feng, D (corresponding author), Huazhong Univ Sci & Technol, Wuhan Natl Lab Optoelect, Wuhan, Peoples R China.; Feng, D (corresponding author), Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan, Peoples R China.
EM qisg@hust.edu.cn; dfeng@hust.edu.cn; sunan2010@mail.hust.edu.cn;
   ljmei@hust.edu.cn; jnliu@hust.edu.cn
RI su, nan/GRS-5213-2022
FU National High-Tech R & D Program of China (863 Program) [2015AA016701,
   2015AA015301, 2013AA013203]; National Natural Science Foundation of
   China (NSFC) [61173043, 61303046, 61402189]; Key Laboratory of
   Information Storage System, Ministry of Education, China
FX This work is supported by National High-Tech R & D Program of China (863
   Program) under Grants No. 2015AA016701, No. 2015AA015301, and No.
   2013AA013203 and National Natural Science Foundation of China (NSFC)
   under Grants No. 61173043, No. 61303046, and No. 61402189. This work was
   also supported by Key Laboratory of Information Storage System, Ministry
   of Education, China.
CR Agrawal Nitin, 2008, P USENIX ANN TECHN C, P57
   [Anonymous], 2011, ACM 11 P INT C SUPER
   Bucy J.S., 2008, DISKSIM SIMULATION E
   Cai Y, 2013, 2013 IEEE 31ST INTERNATIONAL CONFERENCE ON COMPUTER DESIGN (ICCD), P123, DOI 10.1109/ICCD.2013.6657034
   Cai Y, 2015, I C DEPEND SYS NETWO, P438, DOI 10.1109/DSN.2015.49
   Cai Y, 2011, ANN IEEE SYM FIELD P, P101, DOI 10.1109/FCCM.2011.28
   Caulfield A. M., 2009, P 42 ANN IEEE ACM IN, P24
   Chen F, 2011, INT S HIGH PERF COMP, P266, DOI 10.1109/HPCA.2011.5749735
   Choi H, 2010, IEEE T VLSI SYST, V18, P843, DOI 10.1109/TVLSI.2009.2015666
   Compagnoni CM, 2009, IEEE ELECTR DEVICE L, V30, P984, DOI 10.1109/LED.2009.2026658
   Cooke J., 2007, FLASH MEMORY TECHNOL
   Fukuda K, 2007, INT EL DEVICES MEET, P169, DOI 10.1109/IEDM.2007.4418893
   GALLAGER RG, 1962, IRE T INFORM THEOR, V8, P21, DOI 10.1109/tit.1962.1057683
   Guanying Wu, 2010, Proceedings 18th IEEE/ACM International Symposium on Modelling, Analysis & Simulation of Computer and Telecommunication Systems (MASCOTS 2010), P57, DOI 10.1109/MASCOTS.2010.15
   Huang P., 2014, P 2014 USENIX ANN TE, P489
   Jung M, 2012, CONF PROC INT SYMP C, P404
   Koopman P, 2004, 2004 INTERNATIONAL CONFERENCE ON DEPENDABLE SYSTEMS AND NETWORKS, PROCEEDINGS, P145
   Koopman P., 2015, BEST CRC POLYNOMIALS
   Lee JD, 2003, IEEE ELECTR DEVICE L, V24, P748, DOI 10.1109/LED.2003.820645
   Lee JD, 2002, IEEE ELECTR DEVICE L, V23, P264, DOI 10.1109/55.998871
   Lung M, 2014, INT S HIGH PERF COMP, P524
   MacKay DJC, 1996, ELECTRON LETT, V32, P1645, DOI 10.1049/el:19961141
   Mielke N, 2006, INT RELIAB PHY SYM, P29, DOI 10.1109/RELPHY.2006.251188
   Pan YY, 2013, IEEE T COMPUT, V62, P1051, DOI 10.1109/TC.2012.54
   Pearl J., 1988, PROBABILISTIC REASON
   Qi S., 2014, P IEEE WORKSH SIGN P, P145
   Qi SG, 2015, 2015 IEEE TRUSTCOM/BIGDATASE/ISPA, VOL 1, P918, DOI 10.1109/Trustcom.2015.465
   Tanakamaru S., 2011, 2011 IEEE International Solid-State Circuits Conference (ISSCC 2011), P204, DOI 10.1109/ISSCC.2011.5746283
   Wu Q, 2014, IEEE T COMPUT, V63, P2500, DOI 10.1109/TC.2013.114
   Zhao K., 2013, P USENIX C FIL STOR, P243
   Zhao K, 2014, INT S HIGH PERF COMP, P536, DOI 10.1109/HPCA.2014.6835962
   Zhao WC, 2014, ABSTR APPL ANAL, DOI 10.1155/2014/963072
NR 32
TC 6
Z9 6
U1 0
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAR
PY 2017
VL 13
IS 1
SI SI
AR 7
DI 10.1145/3017430
PG 22
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ER3WE
UT WOS:000398729600007
DA 2024-07-18
ER

PT J
AU Esiner, E
   Kachkeev, A
   Braunfeld, S
   Küpçü, A
   Özkasap, Ö
AF Esiner, Ertem
   Kachkeev, Adilet
   Braunfeld, Samuel
   Kupcu, Alptekin
   Ozkasap, Oznur
TI FlexDPDP: Flexlist-Based Optimized Dynamic Provable Data Possession
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Cloud storage; skip list; authenticated dictionary; provable data
   possession; data integrity
AB With increasing popularity of cloud storage, efficiently proving the integrity of data stored on an untrusted server has become significant. Authenticated skip lists and rank-based authenticated skip lists (RBASL) have been used to provide support for provable data update operations in cloud storage. However, in a dynamic file scenario, an RBASL based on block indices falls short when updates are not proportional to a fixed block size; such an update to the file, even if small, may result in O(n) updates on the data structure for a file with n blocks.
   To overcome this problem, we introduce FlexList, a flexible length-based authenticated skip list. FlexList translates variable-size updates to O(inverted right perpendicularu/Binverted left perpendicular) insertions, removals, or modifications, where u is the size of the update and B is the (average) block size. We further present various optimizations on the four types of skip lists (regular, authenticated, rank-based authenticated, and FlexList). We build such a structure in O(n) time and parallelize this operation for the first time. We compute one single proof to answer multiple (non) membership queries and obtain efficiency gains of 35%, 35%, and 40% in terms of proof time, energy, and size, respectively. We propose a method of handling multiple updates at once, achieving efficiency gains of up to 60% at the server side and 90% at the client side. We also deployed our implementation of FlexDPDP (dynamic provable data possession (DPDP) with FlexList instead of RBASL) on PlanetLab, demonstrating that FlexDPDP performs comparable to the most efficient static storage scheme (provable data possession (PDP)) while providing dynamic data support.
C1 [Esiner, Ertem; Kachkeev, Adilet; Braunfeld, Samuel; Kupcu, Alptekin; Ozkasap, Oznur] Koc Univ, Comp Sci & Engn, Istanbul, Turkey.
   [Esiner, Ertem; Kachkeev, Adilet; Braunfeld, Samuel; Kupcu, Alptekin; Ozkasap, Oznur] Koc Univ, Dept Comp Engn, TR-34450 Istanbul, Turkey.
C3 Koc University; Koc University
RP Esiner, E (corresponding author), Koc Univ, Comp Sci & Engn, Istanbul, Turkey.; Esiner, E (corresponding author), Koc Univ, Dept Comp Engn, TR-34450 Istanbul, Turkey.
EM eesiner@ku.edu.tr; akachkeev@ku.edu.tr; s.braunfeld@rutgers.edu;
   akupcu@ku.edu.tr; oozkasap@ku.edu.tr
RI Ozkasap, Oznur/X-2517-2019; Küpçü, Alptekin/A-2224-2013
OI Ozkasap, Oznur/0000-0003-4343-0986; Küpçü, Alptekin/0000-0003-2099-2206;
   Braunfeld, Samuel/0000-0003-3531-9970
FU Scientific and Technological Research Council of Turkey (TUBITAK)
   [112E115, 109M761]; European Union COST Actions [IC0804, IC1206,
   IC1306]; Turk Telekom Inc. [11315-06]; Koc Sistem Inc.
FX We acknowledge the support of the Scientific and Technological Research
   Council of Turkey (TUBITAK) under projects 112E115 and 109M761; European
   Union COST Actions IC0804, IC1206, and IC1306; Turk Telekom Inc. under
   grant 11315-06; and Koc Sistem Inc.
CR Abraham I, 2006, DISTRIB COMPUT, V18, P387, DOI 10.1007/s00446-005-0151-6
   Adelson-Velskii M., 1963, P USSR ACAD SCI, V146, P263
   Anagnostopoulos Aris, 2001, P 2001 ISC C ISC 01
   [Anonymous], 2015, BOOST C LIB
   Ateniese G, 2011, ACM T INFORM SYST SE, V14, DOI 10.1145/1952982.1952994
   Ateniese Giuseppe, 2009, P 2009 ASIACRYPT C A
   Ateniese Giuseppe, 2008, P 2008 SECURECOMM C
   Bayer R., 1972, Acta Informatica, V1, P290, DOI 10.1007/BF00289509
   Bessani A, 2013, ACM T STORAGE, V9, DOI 10.1145/2535929
   Blibech Kaouthar, 2006, P 2006 ICCSA C ICCSA
   Blibech Kaouthar, 2005, P 2005 SWS C SWS 05
   BOEHM HJ, 1995, SOFTWARE PRACT EXPER, V25, P1315, DOI 10.1002/spe.4380251203
   Bowers Kevin D., 2009, P 16 ACM C COMP COMM, P187
   Cachin Christian, 2009, SIGACT News, V40, P81, DOI 10.1145/1556154.1556173
   Cachin Christian, 2006, P 2006 DSN C DSN 06
   Cash D, 2017, J CRYPTOL, V30, P22, DOI 10.1007/s00145-015-9216-2
   Chandran Nishanth, 2013, P 2014 TCC C TCC 14
   Chockler G, 2009, COMPUTER, V42, P60, DOI 10.1109/MC.2009.126
   Chockler Gregory, 2002, P 2002 ACM PODC C PO
   Crosby SA, 2011, ACM T INFORM SYST SE, V14, DOI 10.1145/2019599.2019602
   Curtmola Reza, 2008, P 2008 ICDCS C ICDCS
   Di Battista Giuseppe, 2007, P 2007 DBSEC C DBSEC
   Dodis Yevgeniy, 2009, P 2009 TCC C TCC 09
   Erway CC, 2015, ACM T INFORM SYST SE, V17, DOI 10.1145/2699909
   Esiner Ertem, 2014, P 2014 ICC C ICC 14
   Etemad Mohammad, 2013, P 2013 ACNS C ACNS 1
   Etemad Mohammad, 2013, P 2013 ICISC C ICISC
   Gafni E, 2003, DISTRIB COMPUT, V16, P1, DOI [10.1007/s00446-002-0070-8, 10.1007/S00446-002-0070-8]
   Goodrich MT, 2007, LECT NOTES COMPUT SC, V4525, P94
   Goodrich Michael T., 2008, P 2008 ISC C ISC 08
   Goodrich Michael T., 2001, TECHNICAL REPORT
   Goodrich Michael T., 2001, P 2001 DARPA C DARPA
   Goodson Garth, 2004, P 2004 DSN C DSN 04
   Hendricks James, 2007, P 2007 ACM C SOSP 07
   Jayanti P, 1998, J ACM, V45, P451, DOI 10.1145/278298.278305
   Juels Ari, 2007, P 2007 ACM CCS C CCS
   Liskov B., 2006, ICDCS'06: Proceedings of the 26th IEEE International Conference on Distributed Computing Systems, page, P34
   Malkhi D, 1998, DISTRIB COMPUT, V11, P203, DOI 10.1007/s004460050050
   Meiklejohn Sarah, 2010, P 2010 USENIX SEC C
   Merkle R. C., 1987, LECT NOTES COMPUTER, V293, P269
   Naor M, 2000, IEEE J SEL AREA COMM, V18, P561, DOI 10.1109/49.839932
   Oprea A., 2005, P 2005 NDSS C NDSS 0
   Papamanthou Charalampos, 2007, P 2007 ICICS C ICICS
   PlanetLab, 2007, NOD REQ
   Polivy Daniel J., 2002, P 2002 ACM XML SEC W
   PUGH W, 1990, COMMUN ACM, V33, P668, DOI 10.1145/78973.78977
   Pugh William, 1990, TECHNICAL REPORT
   Schwarz T. S. J., 2006, P 2006 IEEE ICDCS C
   Shacham H, 2013, J CRYPTOL, V26, P442, DOI 10.1007/s00145-012-9129-2
   Shi Elaine, 2013, P 2013 ACM CCS C CCS
   Stanton Paul T., 2010, Operating Systems Review, V44, P45, DOI 10.1145/1740390.1740401
   Wang Qian, 2009, P 2009 ESORICS C ESO
   Zhang Y., 2013, Proceedings of the 8th ACM SIGSAC Symposium on Information, Computer and Communications Security, P183, DOI DOI 10.1145/2484313.2484339
   Zheng Qingji, 2011, P 2011 CODASPY C COD
   Zhou Minqi, 2010, P 2010 IEEE SKG C SK
NR 55
TC 21
Z9 24
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2016
VL 12
IS 4
AR 23
DI 10.1145/2943783
PG 44
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DV2OQ
UT WOS:000382760500006
DA 2024-07-18
ER

PT J
AU di Vimercati, SD
   Foresti, S
   Paraboschi, S
   Pelosi, G
   Samarati, P
AF di Vimercati, Sabrina De Capitani
   Foresti, Sara
   Paraboschi, Stefano
   Pelosi, Gerardo
   Samarati, Pierangela
TI Shuffle Index: Efficient and Private Access to Outsourced Data
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article; Proceedings Paper
CT 13th USENIX Conference on File and Storage Technologies (FAST)
CY FEB 16-19, 2015
CL Santa Clara, CA
SP USENIX, ACM SIGOPS
DE Security; Design; Management; Shuffle index; private access; content
   confidentiality; access confidentiality; pattern confidentiality
ID SECURE
AB Data outsourcing and cloud computing have been emerging at an ever-growing rate as successful approaches for allowing users and companies to rely on external services for storing and managing data. As data and access to them are not under the control of the data owner, there is a clear need to provide proper confidentiality protection. Such requirements concern the confidentiality not only of the stored data (content) but also of the specific accesses (or patterns of them) that users make on such data.
   In this article, we address these issues and propose an approach for guaranteeing content, access, and pattern confidentiality in a data outsourcing scenario. The proposed solution is based on the definition of a shuffle index structure, which adapts traditional B+-trees and, by applying a combination of techniques (covers, caches, and shuffling), ensures confidentiality of the data and of queries over them, protecting each single access as well as sequences thereof. The proposed solution also supports update operations over the data, while making reads and writes not recognizable as such by the server. We show that the shuffle index exhibits a limited performance cost, thus resulting effectively usable in practice.
C1 [di Vimercati, Sabrina De Capitani; Foresti, Sara; Samarati, Pierangela] Univ Milan, Dipartimento Informat, I-26013 Crema, CR, Italy.
   [Paraboschi, Stefano] Univ Bergamo, Dipartimento Ingn Gest Informaz & Prod, I-24044 Dalmine, BG, Italy.
   [Pelosi, Gerardo] Politecn Milan, Dipartimento Elettron Informaz & Bioingn, I-20133 Milan, MI, Italy.
C3 University of Milan; University of Bergamo; Polytechnic University of
   Milan
RP di Vimercati, SD (corresponding author), Univ Milan, Dipartimento Informat, Via Bramante 65, I-26013 Crema, CR, Italy.
EM sabrina.decapitani@unimi.it; sara.foresti@unimi.it; parabosc@unibg.it;
   gerardo.pelosi@polimi.it; pierangela.samarati@unimi.it
RI Pelosi, Gerardo/I-4538-2013; Foresti, Sara/K-6243-2012
OI Pelosi, Gerardo/0000-0002-3812-5429; Paraboschi,
   Stefano/0000-0003-0399-1738; Samarati, Pierangela/0000-0001-7395-4620
FU EC (ABC4EU) [312797]; Italian MIUR within project "GenData"; EC
   (ESCUDO-CLOUD) [644579]
FX This work was supported in part by the EC under grant agreements 312797
   (ABC4EU) and 644579 (ESCUDO-CLOUD), and the Italian MIUR within project
   "GenData 2020."
CR Agrawal D., 2013, P 29 INT C DAT ENG I
   Agrawal R., 2004, P 30 ACM INT C MAN D
   [Anonymous], 2003, P 10 ACM C COMP COMM
   [Anonymous], P 32 INT C VER LARG
   [Anonymous], P 6 C COMP SYST EURO
   Atluri V., 2011, P 12 ANN INT DIG GOV
   Bellare M., 1997, P 38 ANN S FDN COMP
   Bellare M, 2008, J CRYPTOL, V21, P469, DOI 10.1007/s00145-008-9026-x
   Bowers K. D., 2009, P 16 ACM C COMP COMM
   Ceselli A., 2005, ACM Transactions on Information and Systems Security, V8, P119, DOI 10.1145/1053283.1053289
   Chang Y.-C., 2005, P 3 INT C APPL CRYPT
   Curtmola R., 2006, P 13 ACM C COMP COMM, p79 88
   De Capitani di Vimercati S., 2011, P 31 INT C DISTR COM
   De Capitani di Vimercati S., 2008, P WORKSH PRIV EL SOC
   De Capitani di Vimercati S., 2013, P 18 EUR S RES COMP
   Di Vimercati S, 2011, P 16 EUR S RES COMP
   Di Vimercati S.D.C., 2012, P 7 INT C RISKS SEC
   di Vimercati SD, 2013, J COMPUT SECUR, V21, P425, DOI 10.3233/JCS-130468
   Ding XH, 2011, IEEE T INF FOREN SEC, V6, P189, DOI 10.1109/TIFS.2010.2101062
   Foresti S, 2011, ADV INFORM SECUR, V51, P1, DOI 10.1007/978-1-4419-7659-8
   Goldreich O, 1996, J ACM, V43, P431, DOI 10.1145/233551.233553
   Gray J., 1994, P 20 ACM INT C MAN D
   HACIGUMUS H, 2004, P 9 INT C DAT SYST A
   Hacigumus H., 2002, P ACM INT C MAN DAT
   HACIGUMUS H, 2002, P 18 INT C DAT ENG I
   Hore B, 2012, VLDB J, V21, P333, DOI 10.1007/s00778-011-0245-7
   Islam M. S., 2014, P 4 ACM C DAT APPL S
   Jhawar R., 2012, P 2012 IEEE INT C CO
   Kandias M., 2011, P 6 C CRIT INF INFR
   Lin P., 2004, P 2 INT WORKSH SEC I
   Lin P., 2004, P 1 INT C SEC DAT MA
   Ostrovsky R., 2007, P 10 INT C PRACT THE
   Pang H, 2013, IEEE T KNOWL DATA EN, V25, P1533, DOI 10.1109/TKDE.2012.77
   Ren K, 2012, IEEE INTERNET COMPUT, V16, P69, DOI 10.1109/MIC.2012.14
   Shmueli E., 2005, P 19 ANN IFIP WG 11
   Silverman B. W., 1986, CHAPMAN HALL MONOGRA
   Sion Radu., 2007, NDSS
   Song D. X., 2000, P 21 IEEE S SEC PRIV
   Stefanov E., 2013, P 34 IEEE S SEC PRIV
   Stefanov E., 2013, P 20 ACM C COMP COMM
   Sun W., 2013, P 8 ACM S INF COMP C
   Wang C., 2010, P 30 INT C DISTR COM
   Wang C, 2012, IEEE T PARALL DISTR, V23, P1467, DOI 10.1109/TPDS.2011.282
   Wang S., 2011, P 8 INT C SEC DAT MA
   Wang Z. F., 2005, P 5 INT C COMP INF T
   Williams P., 2012, P ACM C COMP COMM SE
   Williams P., 2008, P 15 ACM C COMP COMM
   Yang K., 2011, P 16 EUR S RES COMP
NR 48
TC 11
Z9 12
U1 1
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2015
VL 11
IS 4
SI SI
AR 19
DI 10.1145/2747878
PG 55
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DB0GY
UT WOS:000368187800005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Li, ZC
   Chen, M
   Mukker, A
   Zadok, E
AF Li, Zhichao
   Chen, Ming
   Mukker, Amanpreet
   Zadok, Erez
TI On the Trade-Offs among Performance, Energy, and Endurance in a
   Versatile Hybrid Drive
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Management; Measurement; Hybrid drive; solid-state drive;
   trade-offs; versatility
ID MANAGEMENT
AB There are trade-offs among performance, energy, and device endurance for storage systems. Designs optimized for one dimension or workload often suffer in another. Therefore, it is important to study the trade-offs to enable adaptation to workloads and dimensions. As Flash SSD has emerged, hybrid drives have been studied more closely. However, hybrids are mainly designed for high throughput, efficient energy consumption, or improving endurance-leaving quantitative study on the trade-offs unexplored. Past endurance studies also lack a concrete model to help study the trade-offs. Last, previous designs are often based on inflexible policies that cannot adapt easily to changing conditions.
   We designed and developed GreenDM, a versatile hybrid drive that combines Flash-based SSDs with traditional HDDs. The SSD can be used as cache or as primary storage for hot data. We present our endurance model together with GreenDM to study these trade-offs. GreenDM presents a block interface and requires no modifications to existing software. GreenDM offers tunable parameters to enable the system to adapt to many workloads. We have designed, developed, and carefully evaluated GreenDM with a variety of workloads using commodity SSD and HDD drives. We demonstrate the importance of versatility to enable adaptation to various workloads and dimensions.
C1 [Li, Zhichao; Chen, Ming; Mukker, Amanpreet; Zadok, Erez] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11790 USA.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Stony Brook
RP Li, ZC (corresponding author), SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11790 USA.
EM zhicli@cs.stonybrook.edu; mchen@cs.stonybrook.edu;
   amanpreet@cs.stonybrook.edu; ezk@cs.stonybrook.edu
RI Li, Zhichao/L-6245-2016
FU NSF [IIS-1251137, CNS-1302246]; Division Of Computer and Network
   Systems; Direct For Computer & Info Scie & Enginr [1302246] Funding
   Source: National Science Foundation
FX We thank Vasily Tarasov for help with Filebench. We thank Rajesh Aavuty
   for help with manuscript review and experiment preparation. This work
   was supported in part by NSF awards IIS-1251137 and CNS-1302246.
CR Andersen DG, 2009, SOSP'09: PROCEEDINGS OF THE TWENTY-SECOND ACM SIGOPS SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P1
   [Anonymous], 2012, IEEE PES Innovative Smart Grid Technologies, Tianjin, China
   [Anonymous], 2011, C FIL STOR TECHN FAS
   [Anonymous], [No title captured]
   Barroso Luiz Andre, 2009, The Datacenter as a Computer: An Introduction to the Design of Warehouse-Scale Machines, V4, P1, DOI [DOI 10.2200/S00516ED2V01Y201306CAC024, DOI 10.2200/S00193ED1V01Y200905CAC006]
   Bisson T, 2007, IEEE IPCCC, P236, DOI 10.1109/PCCC.2007.358900
   Brown A. B., 1997, Performance Evaluation Review, V25, P214, DOI 10.1145/258623.258690
   Chen F., 2011, Proc. of ACM Int. Conf. on Supercomputing, P22
   Colarelli Dennis., 2002, SUPERCOMPUTING 02, P1, DOI DOI 10.1109/SC.2002.10058
   Delaluz V, 2002, DES AUT CON, P697, DOI 10.1109/DAC.2002.1012714
   Dell Flash Array, 2015, COMP FLASH OPT SOL
   Desnoyers P., 2013, HOTSTORAGE 13, P1
   Freitas RF, 2009, ACM SIGMOD/PODS 2009 CONFERENCE, P985
   Gantz J.F., 2010, The Digital Universe Decade - Are You Ready?
   GitHub, 2015, FAC FLASHC
   Guenter B, 2011, IEEE INFOCOM SER, P1332, DOI 10.1109/INFCOM.2011.5934917
   Guerra J., 2011, P 9ST USENIX C FAST, P20
   Gurumurthi S, 2003, CONF PROC INT SYMP C, P169, DOI 10.1109/ISCA.2003.1206998
   Jiang WH, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P111
   Joukov N., 2008, P 3 ACM SIGOPS EUROS, P1
   Joukov N, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P89
   Jung Myoungsoo, 2013, ACM INT C MEAS MOD C, P203, DOI DOI 10.1145/2465529.2465548
   Kadav Asim, 2009, HOTSTORAGE 09, P4
   Kaushik R.T., 2010, Proceedings of the 2010 international conference on Power aware computing and systems, HotPower'10, P1
   Kim HJ, 2014, ACM T STORAGE, V10, DOI 10.1145/2668128
   Kim Y., IEEE INT WORKSHOP MO
   Koomey J, 2011, Growth in Data Centre Electricity Use 2005 to 2010
   Lee S., 2012, Proceedings of the 10th USENIX Conference on File and Storage Technologies, FAST'12, USENIX Association, Berkeley, CA, USA, P26
   Li S., 2007, P LIN S OTT ONT CAN, V2, P1
   Li Z., 2011, P 4 ISR EXP SYST C A, P8
   Li Z., 2012, Proceedings of the 10th USENIX Conference on File and Storage Technologies, FAST'12, P6
   Li ZQ, 2011, INT J POLYM SCI, V2011, DOI 10.1155/2011/803428
   Luo T, 2012, PROC VLDB ENDOW, V5, P1076, DOI 10.14778/2336664.2336679
   Mohan V., 2010, P 2 USENIX C HOT TOP, P8
   NetApp, 2015, NETAPP DAT ONT 8 3 F
   nimblestorage, 2015, DRIV STOR LIF EFF DA
   Panabaker R., 2006, Hybrid Hard Disk and Ready-Drive Technology: Improving Performance and Power for Windows Vista Mobile PCs
   Pinheiro E., 2004, INT C SUPERCOMPUTING, P68, DOI DOI 10.1145/1006209.1006220
   Pinheiro E, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 5TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES ( FAST '07), P17
   SNIA Iotta Repository, 2011, FIU SRCMAP TRAC REP
   Soundararajan Gokul., 2010, Proceedings of the 8th USENIX conference on File and storage technologies, FAST'10, P8
   Sourceforge, 2015, BTIER BET
   Storer MW, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P1, DOI 10.1145/1456469.1456471
   Strunk John D., 2012, Operating Systems Review, V46, P50
   Tao Xie, 2008, 2008 IEEE International Symposium on Modeling, Analysis & Simulation of Computers & Telecommunication Systems (MASCOTS), DOI 10.1109/MASCOT.2008.4770583
   Tarasov V., 2011, P HOTOS 13 13 USENIX, P8
   Tintri, 2013, TINTR VMSTORE
   Verma A., 2010, P 8 USENIX C FIL STO, P8
   vFlash, 2012, VIRT FLASH TECH REV
   Watts up, 2010, SMART CIRC CONTR
   Weddle C, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 5TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES ( FAST '07), P245
   Western Digital, 2013, WD BLUE
   Wikipedia, 2015, DEV MAPP
   Wikipedia, 2015, FUS DRIV
   Wikipedia, 2015, GAMM DISTR
   Wikipedia, 2014, ZCAV
   Wilson A. W., 2008, OPERATION IMPLEMENTA
   Wright CP, 2005, USENIX ASSOCIATION PROCEEDINGS OF THE FREENIX/OPEN SOURCE TRACK, P175
   Xiaoyun Zhu, 2009, Operating Systems Review, V43, P62, DOI 10.1145/1496909.1496922
   Zhen Li, 2014, 2014 17th International Symposium on Electromagnetic Launch Technology (EML), P1, DOI 10.1109/EML.2014.6920633
   Zhu Q, 2005, P 20 ACM S OP SYST P, P177, DOI DOI 10.1145/1095809.1095828
   Zhu Y., 2004, RTAS '04: Proceedings of the 10th IEEE Real-Time and Embedded Technology and Applications Symposium, P33
NR 62
TC 8
Z9 9
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUL
PY 2015
VL 11
IS 3
AR 13
DI 10.1145/2700312
PG 27
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CN9RP
UT WOS:000358786900003
DA 2024-07-18
ER

PT J
AU Kim, HJ
   Seshadri, S
   Dickey, CL
   Chiu, L
AF Kim, Hyojun
   Seshadri, Sangeetha
   Dickey, Clement L.
   Chiu, Lawrence
TI Evaluating Phase Change Memory for Enterprise Storage Systems: A Study
   of Caching and Tiering Approaches
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Measurement; Performance; Phase change memory; flash memory; enterprise
   workloads
AB Storage systems based on Phase Change Memory (PCM) devices are beginning to generate considerable attention in both industry and academic communities. But whether the technology in its current state will be a commercially and technically viable alternative to entrenched technologies such as flash-based SSDs remains undecided. To address this, it is important to consider PCM SSD devices not just from a device standpoint, but also from a holistic perspective.
   This article presents the results of our performance study of a recent all-PCM SSD prototype. The average latency for a 4KiB random read is 6.7 mu s, which is about 16x faster than a comparable eMLC flash SSD. The distribution of I/O response times is also much narrower than flash SSD for both reads and writes. Based on the performance measurements and real-world workload traces, we explore two typical storage use cases: tiering and caching. We report that the IOPS/$ of a tiered storage system can be improved by 12-66% and the aggregate elapsed time of a server-side caching solution can be improved by up to 35% by adding PCM.
   Our results show that (even at current price points) PCM storage devices show promising performance as a new component in enterprise storage systems.
C1 [Kim, Hyojun; Seshadri, Sangeetha; Dickey, Clement L.; Chiu, Lawrence] IBM Almaden Res, San Jose, CA 95120 USA.
C3 International Business Machines (IBM)
RP Kim, HJ (corresponding author), IBM Almaden Res, San Jose, CA 95120 USA.
EM hyojun@us.ibm.com
CR Akel Ameen., 2011, P 3 USENIX C HOT TOP, P2
   Åkerman J, 2005, SCIENCE, V308, P508, DOI 10.1126/science.1110549
   [Anonymous], P 11 USENIX C FIL ST
   Bedeschi F, 2004, 2004 SYMPOSIUM ON VLSI CIRCUITS, DIGEST OF TECHNICAL PAPERS, P442, DOI 10.1109/VLSIC.2004.1346644
   Byan S., 2012, IEEE S MASS STORAGE, P1
   Hoya K., 2006, ISSCC Dig. Tech. Papers, P459
   Kim JinKyu., 2008, 8th ACM IEEE Conference on Embedded Software (EMSOFT '08), P31
   Kwang-Jin Lee, 2007, 2007 IEEE International Solid-State Circuits Conference (IEEE Cat. No.07CH37858), P472
   Lee BC, 2009, CONF PROC INT SYMP C, P2, DOI 10.1145/1555815.1555758
   Mogul J.C., 2009, P 12 C HOT TOPICS OP, P14
   Qureshi MK, 2012, CONF PROC INT SYMP C, P380, DOI [10.1109/ISCA.2012.6237033, 10.1145/2366231.2337203]
   Qureshi MK, 2009, CONF PROC INT SYMP C, P24, DOI 10.1145/1555815.1555760
   Raoux S, 2008, IBM J RES DEV, V52, P465, DOI 10.1147/rd.524.0465
   Sie C H., 1969, Memory cell using bistable resistivity in amorphous As-Te-Ge film
   Zilberberg O, 2013, ACM COMPUT SURV, V45, DOI 10.1145/2480741.2480746
   [No title captured]
   [No title captured]
   [No title captured]
NR 18
TC 88
Z9 105
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD OCT
PY 2014
VL 10
IS 4
SI SI
AR 15
DI 10.1145/2668128
PG 21
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AT0LD
UT WOS:000344626700003
DA 2024-07-18
ER

PT J
AU Adams, IF
   Storer, MW
   Miller, EL
AF Adams, Ian F.
   Storer, Mark W.
   Miller, Ethan L.
TI Analysis of Workload Behavior in Scientific and Historical Long-Term
   Data Repositories
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Measurement; Performance; Archival storage; tertiary storage; trace
   analysis
ID FILE MIGRATION
AB The scope of archival systems is expanding beyond cheap tertiary storage: scientific and medical data is increasingly digital, and the public has a growing desire to digitally record their personal histories. Driven by the increase in cost efficiency of hard drives, and the rise of the Internet, content archives have become a means of providing the public with fast, cheap access to long-term data. Unfortunately, designers of purpose-built archival systems are either forced to rely on workload behavior obtained from a narrow, anachronistic view of archives as simply cheap tertiary storage, or extrapolate from marginally related enterprise workload data and traditional library access patterns.
   To close this knowledge gap and provide relevant input for the design of effective long-term data storage systems, we studied the workload behavior of several systems within this expanded archival storage space. Our study examined several scientific and historical archives, covering a mixture of purposes, media types, and access models-that is, public versus private. Our findings show that, for more traditional private scientific archival storage, files have become larger, but update rates have remained largely unchanged. However, in the public content archives we observed, we saw behavior that diverges from the traditional "write-once, read-maybe" behavior of tertiary storage. Our study shows that the majority of such data is modified-sometimes unnecessarily-relatively frequently, and that indexing services such as Google and internal data management processes may routinely access large portions of an archive, accounting for most of the accesses. Based on these observations, we identify areas for improving the efficiency and performance of archival storage systems.
C1 [Adams, Ian F.; Miller, Ethan L.] Univ Calif Santa Cruz, Santa Cruz, CA 95064 USA.
   [Storer, Mark W.] NetApp, Sunnyvale, CA 94089 USA.
C3 University of California System; University of California Santa Cruz;
   NetApp, Inc.
RP Adams, IF (corresponding author), Univ Calif Santa Cruz, 1156 High St, Santa Cruz, CA 95064 USA.
EM iadams@soe.ucsc.edu; mark.storer@netapp.com; elm@ucsc.edu
OI Miller, Ethan/0000-0003-2994-9060
FU National Science Foundation [CNS-0917396, IIP-0934401]; Storage Systems
   Research Center; Center for Research in Intelligent Storage; Direct For
   Computer & Info Scie & Enginr [0917396] Funding Source: National Science
   Foundation; Directorate For Engineering [0934401] Funding Source:
   National Science Foundation; Division Of Computer and Network Systems
   [0917396] Funding Source: National Science Foundation; Div Of Industrial
   Innovation & Partnersh [0934401] Funding Source: National Science
   Foundation
FX This research was supported in part by the National Science Foundation
   under grants CNS-0917396 (part of the American Recovery and Reinvestment
   Act of 2009 [Public Law 111-5]) and IIP-0934401, and by the industrial
   sponsors of the Storage Systems Research Center and the Center for
   Research in Intelligent Storage.
CR Agrawal Nitin, 2009, ACM Transactions on Storage, V5, DOI 10.1145/1629080.1629086
   Agrawal N, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 5TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES ( FAST '07), P31
   ALASKA STATE, 2010, AL DIG ARCH
   AMAZON, 2011, AM SIMPL STOR SERV
   Anderson Eric, 2009, Operating Systems Review, V43, P70, DOI 10.1145/1496909.1496923
   ANDERSON E, 2009, P 7 USENIX C FIL STO
   [Anonymous], 2011, CHRON LIF SAV YOUR M
   [Anonymous], 2006, P 1 ACM SIGOPSEUROSY
   BAIRAVASUNDARAM L. N., 2007, P SIGMETRICS C MEAS
   Bairavasundaram LN, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P223
   Baker M., 2005, P 1 IEEE WORKSH HOT
   Bent J, 2009, PROCEEDINGS OF THE CONFERENCE ON HIGH PERFORMANCE COMPUTING NETWORKING, STORAGE AND ANALYSIS
   COLARELLI D, 2002, P ACM IEEE C SUP SC
   DAMOULAKIS J, 2007, OPINION TAPE BACKUP
   Dayal S., 2008, CMUPDL08109
   DWR CALIFORNIA, 2010, CAL DEP WAT RES WAT
   Gibson T J, 1998, P 24 INT C RES MAN P, P976
   Gibson T J, 1998, P 15 IEEE S MSS, P355
   IBM, 2010, IBM 3380 DIR ACC STO
   JAFFE E, 2009, P ISR EXP SYST C SYS
   Jensen DavidW., 1993, P 7 INT C SUPERCOMPU, p387a
   LEUNG A. W., 2008, P USENIX ANN TECHN C
   Lillibridge M, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE GENERAL TRACK, P29
   Maniatis P, 2005, ACM T COMPUT SYST, V23, P2, DOI 10.1145/1047915.1047917
   MILLER EL, 1993, PROCEEDINGS OF THE WINTER 1993 USENIX CONFERENCE, P421
   Moore R.L., 2007, Archiving 2007
   PINHEIRO E, 2004, P 18 INT C SUP
   Pinheiro E, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 5TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES ( FAST '07), P17
   POLLACK K. T, 2005, P 21 INT C DAT ENG I
   Quinlan S, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE FAST'02 CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P89
   Roselli D, 2000, USENIX ASSOCIATION PROCEEDINGS OF THE 2000 USENIX ANNUAL TECHNICAL CONFERENCE, P41
   Schroeder B, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 5TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES ( FAST '07), P1
   SMITH AJ, 1981, IEEE T SOFTWARE ENG, V7, P403, DOI 10.1109/TSE.1981.230843
   SMITH AJ, 1981, COMMUN ACM, V24, P521, DOI 10.1145/358722.358737
   Storer M, 2008, P 6 USENIX C FIL STO
   Storer MW, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 2007 USENIX ANNUAL TECHNICAL CONFERENCE, P143
   STRANGE S, 1992, 92700 UCBCSD
   THERESKA E., 2006, P SIGMETRICS C MEAS
   TRAEGER A., 2008, ACM T STORAGE, V4, P2
   Vogels W, 1999, OPERATING SYSTEMS REVIEW, VOL 33, NO 5, DECEMBER 1999, P93, DOI 10.1145/319344.319158
   WASHINGTON STATE, 2010, WASH STAT DIG ARCH
   WILDANI A, 2010, P 5 INT WORKSH PET D
   WILDANI A., 2009, P 17 IEEE INT S MOD
   Zheng Zhang, 2007, Operating Systems Review, V41, P27, DOI 10.1145/1243418.1243423
   Zhu B, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P269
   Zhu Q, 2005, P 20 ACM S OP SYST P
NR 46
TC 22
Z9 28
U1 0
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAY
PY 2012
VL 8
IS 2
AR 6
DI 10.1145/2180905.2180907
PG 27
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 990LY
UT WOS:000307633600002
DA 2024-07-18
ER

PT J
AU Kim, SH
   Shim, J
   Lee, E
   Jeong, S
   Kang, I
   Kim, JS
AF Kim, Sang-Hoon
   Shim, Jaehoon
   Lee, Euidong
   Jeong, Seongyeop
   Kang, Ilkueon
   Kim, Jin-Soo
TI Empowering Storage Systems Research with NVMeVirt: A Comprehensive NVMe
   Device Emulator
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Virtual device; emulator; SSD; key-value SSD; ZNS SSD; NVMe device
AB There have been drastic changes in the storage device landscape recently. At the center of the diverse storage landscape lies the NVMe interface, which allows high-performance and flexible communication models required by these next-generation device types. However, its hardware-oriented definition and specification are bottlenecking the development and evaluation cycle for new revolutionary storage devices. Furthermore, existing emulators lack the capability to support the advanced storage configurations that are currently in the spotlight.
   In this article, we present NVMeVirt, a novel approach to facilitate software-defined NVMe devices. A user can define any NVMe device type with custom features, and NVMeVirt allows it to bridge the gap between the host I/O stack and the virtual NVMe device in software. We demonstrate the advantages and features of NVMeVirt by realizing various storage types and configurations, such as conventional SSDs, low-latency high-bandwidth NVM SSDs, zoned namespace SSDs, and key-value SSDs with the support of PCI peer-to-peer DMA and NVMe-oF target offloading. We also make cases for storage research with NVMeVirt, such as studying the performance characteristics of database engines and extending the NVMe specification for the improved key-value SSD performance.
C1 [Kim, Sang-Hoon] Ajou Univ, Suwon, South Korea.
   [Shim, Jaehoon; Lee, Euidong; Jeong, Seongyeop; Kang, Ilkueon; Kim, Jin-Soo] Seoul Natl Univ, Seoul, South Korea.
C3 Ajou University; Seoul National University (SNU)
RP Kim, SH (corresponding author), Ajou Univ, Suwon, South Korea.
EM sanghoonkim@ajou.ac.kr; mattjs@snu.ac.kr; led5198@snu.ac.kr;
   seongyeop.jeongv@snu.ac.kr; kangilkueon@snu.ac.kr; jinsoo.kim@snu.ac.kr
OI Kang, Ilkueon/0009-0000-7391-0900; Kim, Sang-Hoon/0000-0002-2105-1734
FU Electronics and Telecommunications Research Institute (ETRI) - Korean
   government [23ZS1310]; National Research Foundation of Korea (NRF)
   [2019R1A2C2089773]; Institute of Information & communications Technology
   Planning & Evaluation (IITP) - Korea government (MSIT)
   [IITP-2021-0-01363]
FX This work was supported by Electronics and Telecommunications Research
   Institute (ETRI) grant funded by the Korean government (23ZS1310), the
   National Research Foundation of Korea (NRF) grant (No.
   2019R1A2C2089773), and Institute of Information & communications
   Technology Planning & Evaluation (IITP) grant (No. IITP-2021-0-01363)
   funded by the Korea government (MSIT).
CR Amit Nadav, 2011, P 2011 USENIX ANN TE
   [Anonymous], 2011, The DiskSim Simulation Environment (v4.0)
   [Anonymous], scriptable database and system performance benchmark
   [Anonymous], 2004, PCI SIG
   Bae J, 2021, PROCEEDINGS OF THE 19TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '21), P387
   Bjorling M, 2021, PROCEEDINGS OF THE 2021 USENIX ANNUAL TECHNICAL CONFERENCE, P689
   Bjorling M, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P359
   Couchbase Labs, ForestDB Benchmark
   db ranking, DB-Engines Ranking
   DeepSpeed, ZeRO-Inference: Democratizing massive model inference
   Do J, 2020, ACM T STORAGE, V16, DOI 10.1145/3415580
   Facebook Research, Metaseq
   Gao CM, 2022, ACM T STORAGE, V18, DOI 10.1145/3487064
   Gouk D, 2018, 2018 51ST ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE (MICRO), P469, DOI 10.1109/MICRO.2018.00045
   Gu B, 2016, CONF PROC INT SYMP C, P153, DOI 10.1109/ISCA.2016.23
   Han K, 2021, PROCEEDINGS OF THE 15TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '21), P147
   Hayden Major, MySQLTuner
   Ho CC, 2018, DES AUT CON, DOI 10.1145/3195970.3195982
   Im J, 2020, PROCEEDINGS OF THE 2020 USENIX ANNUAL TECHNICAL CONFERENCE, P173
   Intel, Intel Optane SSD 9 Series
   Intel, Intel I/O Acceleration Technology
   Izraelevitz Joseph, 2019, arXiv, DOI [10.48550/ARXIV.1903.05714, DOI 10.48550/ARXIV.1903.05714]
   Jackson Mike., 2012, PCI Express Technology: Comprehensive Guide to Generations 1.x, 2.x
   Jens Axboe, fio: Flexible I/O Tester
   Jin YQ, 2017, INT S HIGH PERF COMP, P373, DOI 10.1109/HPCA.2017.15
   Jun SW, 2018, CONF PROC INT SYMP C, P411, DOI 10.1109/ISCA.2018.00042
   Jung M, 2020, PROCEEDINGS OF THE 2020 USENIX ANNUAL TECHNICAL CONFERENCE, P649
   Jung M, 2018, IEEE COMPUT ARCHIT L, V17, P37, DOI 10.1109/LCA.2017.2750658
   Kang Y, 2019, SYSTOR '19: PROCEEDINGS OF THE 12TH ACM INTERNATIONAL SYSTEMS AND STORAGE CONFERENCE, P144, DOI 10.1145/3319647.3325831
   Kim Hyeong-Jun, 2016, P 8 USENIX WORKSHOP
   Kim J, 2021, ACM T STORAGE, V17, DOI 10.1145/3440022
   Kim Sang-Hoon, 2019, P 11 USENIX WORKSH H
   kvceph, Open memory platform development kit
   Kwak J, 2020, ACM T STORAGE, V16, DOI 10.1145/3385073
   Kwon M, 2022, PROCEEDINGS OF THE 20TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, FAST 2022, P147
   Lee YS, 2016, FUTURE GENER COMP SY, V65, P76, DOI 10.1016/j.future.2016.03.003
   Li HC, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P83
   Liang SW, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P395
   Lu Y., 2013, File and Storage Technologies(FAST), P257
   Malladi KT, 2016, PROCEEDINGS OF 2016 IEEE 18TH INTERNATIONAL CONFERENCE ON HIGH PERFORMANCE COMPUTING AND COMMUNICATIONS; IEEE 14TH INTERNATIONAL CONFERENCE ON SMART CITY; IEEE 2ND INTERNATIONAL CONFERENCE ON DATA SCIENCE AND SYSTEMS (HPCC/SMARTCITY/DSS), P1115, DOI [10.1109/HPCC-SmartCity-DSS.2016.106, 10.1109/HPCC-SmartCity-DSS.2016.0157]
   Malladi Krishna T., 2017, P 2017 INT C NETWORK, P1
   Microsoft, Megatron-DeepSpeed
   Noll L, FNV Hash
   NVIDIA, The NVIDIA Magnum IO GPUDIrect Storage Overview Guide
   NVIDIA, GPUDirect Storage: A Direct Path Between Storage and GPU Memory
   NVM Express, NVMe-oF Specification
   Ouyang J, 2014, ACM SIGPLAN NOTICES, V49, P471, DOI 10.1145/2541940.2541959
   pgtune, PGTune: configuration for PostgreSQL based on the maximum performance for a given hardware configuration
   QMEU, QEMU: A Generic and Open Source Machine Emulator and Virtualizer
   Saha MP, 2021, DES AUT CON, P1105, DOI 10.1109/DAC18074.2021.9586111
   Samsung Electronics, OpenMPDK KVSSD
   Samsung Electronics, Samsung Enterprise SSD
   Tavakkol A, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P49
   Tech Power Up, Samsung 970 Pro 512 GB
   Weiland M, 2019, PROCEEDINGS OF SC19: THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS, DOI 10.1145/3295500.3356159
   Wilkening M, 2021, ASPLOS XXVI: TWENTY-SIXTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS, P717, DOI 10.1145/3445814.3446763
   Wu Kan, 2019, P 11 USENIX WORKSH H
   Yang ZY, 2017, INT CONF CLOUD COMP, P154, DOI 10.1109/CloudCom.2017.14
   Yoo J, 2013, IEEE S MASS STOR SYS
   Zhang Y., 2012, FAST
NR 60
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2023
VL 19
IS 4
AR 32
DI 10.1145/3625006
PG 26
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X6NQ6
UT WOS:001099601100004
DA 2024-07-18
ER

PT J
AU Kassa, HT
   Akers, J
   Ghosh, M
   Cao, ZC
   Gogte, V
   Dreslinski, R
AF Kassa, Hiwot Tadese
   Akers, Jason
   Ghosh, Mrinmoy
   Cao, Zhichao
   Gogte, Vaibhav
   Dreslinski, Ronald
TI Power-optimized Deployment of Key-value Stores Using Storage Class
   Memory
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Key value stores; RocksDB; data centers; Intel Optane Memory; persistent
   memory; optimization
AB High-performance flash-based key-value stores in data-centers utilize large amounts of DRAM to cache hot data. However, motivated by the high cost and power consumption of DRAM, server designs with lower DRAM-per-compute ratio are becoming popular. These low-cost servers enable scale-out services by reducing server workload densities. This results in improvements to overall service reliability, leading to a decrease in the total cost of ownership (TCO) for scalable workloads. Nevertheless, for key-value stores with large memory footprints, these reduced DRAM servers degrade performance due to an increase in both IO utilization and data access latency. In this scenario, a standard practice to improve performance for sharded databases is to reduce the number of shards per machine, which degrades the TCO benefits of reduced DRAM low-cost servers. In this work, we explore a practical solution to improve performance and reduce the costs and power consumption of key-value stores running on DRAM-constrained servers by using Storage Class Memories (SCM).
   SCMs in a DIMM form factor, although slower than DRAM, are sufficiently faster than flash when serving as a large extension to DRAM. With new technologies like Compute Express Link, we can expand the memory capacity of servers with high bandwidth and low latency connectivity with SCM. In this article, we use Intel Oplane PMem 100 Series SCMs (DCPMM) in AppDirect mode to extend the available memory of our existing single-socket platform deployment of RocksDB (one of the largest key-value stores at Meta). We first designed a hybrid cache in RocksDB to harness both DRAM and SCM hierarchically. We then characterized the performance of the hybrid cache for three of the largest RocksDB use cases at Meta (ChatApp, BLOB Metadata, and Hive Cache). Our results demonstrate that we can achieve up to 80% improvement in throughput and 20% improvement in P95 latency over the existing small DRAM single-socket platform, while maintaining a 43-48% cost improvement over our large DRAM dual-socket platform. To the best of our knowledge, this is the first study of the DCPMM platform in a commercial data center.
C1 [Kassa, Hiwot Tadese; Gogte, Vaibhav; Dreslinski, Ronald] Univ Michigan, Ann Arbor, MI 48109 USA.
   [Akers, Jason; Ghosh, Mrinmoy; Cao, Zhichao] Meta Inc, Menlo Pk, CA USA.
C3 University of Michigan System; University of Michigan
RP Kassa, HT (corresponding author), Univ Michigan, Ann Arbor, MI 48109 USA.
EM hiwot@umich.edu; jasonakers@fb.com; mghosh@fb.com; zhichao@fb.com;
   vgogte@umich.edu; rdreslin@umich.edu
RI Ghosh, Mrinmoy/ABB-4479-2021
OI Ghosh, Mrinmoy/0000-0001-6817-622X; Akers, Jason/0000-0003-2814-3065
CR Alcorn J. Paul, 2019, INTEL OPTANE DIMM PR
   Anderson TE, 2020, PROCEEDINGS OF THE 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '20), P1011
   Annamalai Muthu, 2015, ZIPPYDB MODERN DISTR
   [Anonymous], 2004, Linux journal
   Atikoglu Berk, 2012, Performance Evaluation Review, V40, P53, DOI 10.1145/2318857.2254766
   Bailey K.A., 2013, Proceedings of the 1st Workshop on Interactions of NVM/FLASH with Operating Systems and Workloads
   Bock S, 2016, PR IEEE COMP DESIGN, P420, DOI 10.1109/ICCD.2016.7753318
   Bronson Nathan, 2013, P USENIX ANN TECHN C
   Cao ZC, 2020, PROCEEDINGS OF THE 18TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P209
   Chang HS, 2015, ICCAD-IEEE ACM INT, P22, DOI 10.1109/ICCAD.2015.7372545
   Chen GJ, 2016, SIGMOD'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P1087, DOI 10.1145/2882903.2904441
   CHOW CK, 1974, IBM J RES DEV, V18, P194, DOI 10.1147/rd.183.0194
   COBURN J, 2011, P 16 INT C ARCH SUPP, P105
   Condit J, 2009, SOSP'09: PROCEEDINGS OF THE TWENTY-SECOND ACM SIGOPS SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P133
   CXL. 2022, COMPUTE EXPRESS LINK
   DeCandia Giuseppe, 2007, Operating Systems Review, V41, P205, DOI 10.1145/1323293.1294281
   Dhiman G, 2009, DES AUT CON, P664
   Dulloor Subramanya R., 2014, P EUR C COMP SYST EU, P1, DOI [10.1145/2592798.2592814, DOI 10.1145/2592798.2592814]
   Eisenman A, 2018, EUROSYS '18: PROCEEDINGS OF THE THIRTEENTH EUROSYS CONFERENCE, DOI 10.1145/3190508.3190524
   Facebook, 2009, HIVE A PETABYTE SCAL
   Facebook, 2022, ROCKSDB TRACE REPLAY
   Facebook, 2015, INTRO YOSEMITE 1 OPE
   Facebook, 2021, ROCKSDB USERS USE CA
   Facebook, 2022, DB BENCH
   Facebook, 2022, RocksDB
   Fio, 2022, FIO
   Gill G, 2020, PROC VLDB ENDOW, V13, P1304, DOI 10.14778/3389133.3389145
   Google, 2011, LevelDB
   Gottesman Y, 2016, PROCEEDINGS OF THE 9TH ACM INTERNATIONAL SYSTEMS AND STORAGE CONFERENCE (SYSTOR'16), DOI 10.1145/2928275.2933273
   Handy Jim, 2019, INTELS OPTANE DIMM P
   Hassan Ahmad, 2015, P 12 ACM INT C COMP
   Huang YH, 2018, PROCEEDINGS OF THE 2018 USENIX ANNUAL TECHNICAL CONFERENCE, P967
   Hyperscalers, 2019, RACKGO X YOSEMITE VA
   Hyperscalers, 2019, RACKGO X LEOPARD CAV
   IgorsIstocniks, 2019, WHATSAPP MOVED 15B U
   Imamura S, 2020, PROCEEDINGS OF INTERNATIONAL CONFERENCE ON HIGH PERFORMANCE COMPUTING IN ASIA-PACIFIC REGION WORKSHOPS (HPC ASIA 2020 WORKSHOPS), P1, DOI 10.1145/3373271.3373272
   Intel, 2022, 3D XPOINT BREAKTHROU
   Intel, 2019, INT SERV BOARD S2600
   Intel, 2020, INTEL OPTANE 600 PER
   Intel, 2022, IPMCTL
   Intel, 2022, INTEL MEMORY LATENCY
   Intel, 2019, INT XEON GOLD 6252 P
   Intel, 2022, INTEL OPTANE PERSIST
   Izraelevitz J., 2019, BASIC PERFORMANCE ME
   Jeong J, 2020, 2020 53RD ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE (MICRO 2020), P525, DOI 10.1109/MICRO50266.2020.00051
   Jung J.-Y., 2013, P 27 INT ACM C INT C, P115
   Knowlton Scott, 2019, INTRO COMPUTE EXPRES
   Kumar Sanjeev, 2012, SOCIAL NETWORKING SC
   Li Y, 2017, IEEE INT C CL COMP, P152, DOI 10.1109/CLUSTER.2017.130
   Liu HL, 2017, 2017 IEEE 3RD INTERNATIONAL CONFERENCE ON BIG DATA SECURITY ON CLOUD (BIGDATASECURITY, IEEE 3RD INTERNATIONAL CONFERENCE ON HIGH PERFORMANCE AND SMART COMPUTING, (HPSC) AND 2ND IEEE INTERNATIONAL CONFERENCE ON INTELLIGENT DATA AND SECURITY (IDS), P26, DOI 10.1109/BigDataSecurity.2017.20
   Liu L, 2019, IEEE T PARALL DISTR, V30, P2223, DOI 10.1109/TPDS.2019.2908175
   Ma T, 2020, TWENTY-FIFTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXV), P757, DOI 10.1145/3373376.3378511
   MATLAB, 2022, FIT POWER SERIES MOD
   MATLAB, 2022, FIT SINE MODELS USIN
   MATLAB, 2022, GPFIT GEN PAR PAR ES
   Memkind, 2022 MEMKIND LIB
   Micron, 2021, 3D XPOINT TECHN
   Muralidhar S., 2014, P 11 USENIX C OP SYS, P383
   NDCTL, 2022, NDCTL DAXCTL
   NDCTL. 2022, NDCTL US GUID MAN NA
   Nishtala R., 2013, Proceedings of the 10th USENIX Conference on Networked Systems Design and Implementation, NSDI '13, Berkeley, CA, USA, P385
   OCP, 2018, OCP TWIN LAK 1S SERV
   OCP, 2018, OCP TIOG PASS 2S SER
   ONeil P, 1996, ACTA INFORM, V33, P351, DOI 10.1007/s002360050048
   Ou JX, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901324
   Oukid I, 2016, SIGMOD'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P371, DOI 10.1145/2882903.2915251
   Pan Satadru, P 19 USENIX C FILE S, P217
   Patil O, 2019, MEMSYS 2019: PROCEEDINGS OF THE INTERNATIONAL SYMPOSIUM ON MEMORY SYSTEMS, P288, DOI 10.1145/3357526.3357541
   Peng I, 2020, INT PARALL DISTRIB P, P916, DOI 10.1109/IPDPS47924.2020.00098
   Peng IB, 2019, MEMSYS 2019: PROCEEDINGS OF THE INTERNATIONAL SYMPOSIUM ON MEMORY SYSTEMS, P304, DOI 10.1145/3357526.3357568
   Psaropoulos G, 2019, 15TH INTERNATIONAL WORKSHOP ON DATA MANAGEMENT ON NEW HARDWARE (DAMON 2019), DOI 10.1145/3329785.3329917
   Qureshi MK, 2009, CONF PROC INT SYMP C, P24, DOI 10.1145/1555815.1555760
   Ramos Luiz E, 2011, P INT C SUP, P85
   Redis, 2022, REDIS
   Shanbhag Anil, 2020, P 16 INT WORKSHOP DA
   Shi X, 2020, PROCEEDINGS OF THE 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '20), P407
   Thusoo A., 2010, Proceedings of the 2010 ACM SIGMOD International Conference on Management of data, SIGMOD '10, P1013, DOI [DOI 10.1145/1807167.1807278, 10.1145/1807167.1807278]
   van Renen A, 2019, 15TH INTERNATIONAL WORKSHOP ON DATA MANAGEMENT ON NEW HARDWARE (DAMON 2019), DOI 10.1145/3329785.3329930
   Volos Haris, 2011, SIGPLAN Notices, V46, P91, DOI 10.1145/1961296.1950379
   Wang ZX, 2020, 2020 53RD ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE (MICRO 2020), P496, DOI 10.1109/MICRO50266.2020.00049
   Wu K., 2017, EARLY EVALUATION INT
   Wu K, 2018, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE, AND ANALYSIS (SC'18)
   Wu Yinjun., 2020, P 16 INT WORKSH DAT, P1
   Xia F, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P349
   Yang J, 2020, PROCEEDINGS OF THE 18TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P169
   Zarubin M, 2020, PROCEEDINGS OF THE 13TH ACM INTERNATIONAL SYSTEMS AND STORAGE CONFERENCE (SYSTOR 2020), P98, DOI 10.1145/3383669.3398283
   Zhang L, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P897
   Zhang Z., 2017, 2017 IEEE International Magnetics Conference (INTERMAG), DOI 10.1109/INTMAG.2017.8007685
   Zhao J., 2013, P 46 ANN IEEEACM INT, P421, DOI DOI 10.1145/2540708.2540744
NR 89
TC 4
Z9 4
U1 1
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAY
PY 2022
VL 18
IS 2
AR 13
DI 10.1145/3511905
PG 26
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 1H3XN
UT WOS:000796478700004
OA Bronze
DA 2024-07-18
ER

PT J
AU Litz, H
   Gonzalez, J
   Klimovic, A
   Kozyrakis, C
AF Litz, Heiner
   Gonzalez, Javier
   Klimovic, Ana
   Kozyrakis, Christos
TI RAIL: Predictable, Low Tail Latency for NVMe Flash
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE SSD; OpenChannel; LightNVM; Linux
ID MEMORY; PERFORMANCE
AB Flash-based storage is replacing disk for an increasing number of data center applications, providing orders of magnitude higher throughput and lower average latency. However, applications also require predictable storage latency. Existing Flash devices fail to provide low tail read latency in the presence of write operations. We propose two novel techniques to address SSD read tail latency, including Redundant Array of Independent LUNs (RAIL) which avoids serialization of reads behind user writes as well as latency-aware hot-cold separation (HC) which improves write throughput while maintaining low tail latency. RAIL leverages the internal parallelism of modern Flash devices and allocates data and parity pages to avoid reads getting stuck behind writes. We implement RAIL in the Linux Kernel as part of the LightNVM Flash translation layer and show that it can reduce read tail latency by 7x at the 99.99th percentile, while reducing relative bandwidth by only 33%.
C1 [Litz, Heiner] Univ Calif Santa Cruz, Santa Cruz, CA 95064 USA.
   [Gonzalez, Javier] Samsung, Suwon, South Korea.
   [Klimovic, Ana] Swiss Fed Inst Technol, CH-8092 Zurich, Switzerland.
   [Kozyrakis, Christos] Stanford Univ, Stanford, CA 94305 USA.
C3 University of California System; University of California Santa Cruz;
   Swiss Federal Institutes of Technology Domain; ETH Zurich; Stanford
   University
RP Litz, H (corresponding author), Univ Calif Santa Cruz, Santa Cruz, CA 95064 USA.
EM hlitz@ucsc.edu; javier@javigon.com; aldimovic@ethz.ch;
   kozyraki@stanford.edu
OI Klimovic, Ana/0000-0001-8559-0529
FU Samsung; NSF [CCF-1942754, CNS-1841545]
FX This work has been supported by Samsung and by NSF grants CCF-1942754
   and CNS-1841545.
CR Agrawal Nitin, 2008, P USENIX ANN TECHN C, P57
   Amvrosiadis G, 2015, SOSP'15: PROCEEDINGS OF THE TWENTY-FIFTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P457, DOI 10.1145/2815400.2815424
   Banker K., 2011, MONGODB IN ACTION
   Belay A., 2014, P 11 USENIX C OP SYS, P49
   Bellard F, 2005, USENIX Association Proceedings of the FREENIX/Open Source Track, P41
   Bez R, 2003, P IEEE, V91, P489, DOI 10.1109/JPROC.2003.811702
   Birk Y, 1997, PROCEEDINGS OF THE IEEE 7TH INTERNATIONAL WORKSHOP ON NETWORK AND OPERATING SYSTEM SUPPORT FOR DIGITAL AUDIO AND VIDEO, P13, DOI 10.1109/NOSDAV.1997.629305
   Bjorling M, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P359
   Bjorling Matias, 2016, GETTING STARTED OPEN
   Bjorling Matias., 2013, Proceedings of the 6th International Systems and Storage Conference, P22
   Bjorling Matias., 2016, OPEN CHANNEL SOLID S
   Caulfield AM, 2009, ACM SIGPLAN NOTICES, V44, P217, DOI 10.1145/1508284.1508270
   Chakraborttii Chandranil, 2020, SoCC '20: Proceedings of the 11th ACM Symposium on Cloud Computing, P120, DOI 10.1145/3419111.3421300
   Chakraborttii C., 2020, ICML PKDD
   Chang L.-P., 2004, ACM Trans. on Embedded Computing Syst, V3, P837
   Chen F, 2016, ACM T STORAGE, V12, DOI 10.1145/2818376
   Chen F, 2011, INT S HIGH PERF COMP, P266, DOI 10.1109/HPCA.2011.5749735
   Chen SM, 2010, SIGMOD RECORD, V39, P5, DOI 10.1145/1942776.1942778
   Chodorow Kristina, 2013, MongoDB: The Definitive Guide, V2nd
   Colgrove J, 2015, SIGMOD'15: PROCEEDINGS OF THE 2015 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P1683, DOI 10.1145/2723372.2742798
   Cooper Brian F., 2010, P 1 ACM S CLOUD COMP, P143, DOI [DOI 10.1145/1807128.1807152, 10.1145/1807128.1807152]
   Dean J, 2013, COMMUN ACM, V56, P74, DOI 10.1145/2408776.2408794
   Desnoyers P, 2014, ACM T STORAGE, V10, DOI 10.1145/2577384
   Desnoyers Peter., 2012, P INT SYSTEMS STORAG, P12
   Do T., 2013, P 4 ANN S CLOUD COMP, P1
   Du YM, 2014, IEEE INT C CL COMP, P212, DOI 10.1109/CLUSTER.2014.6968742
   Gardner Kristen, 2015, ACM SIGMETRICS Performance Evaluation Review, V43, P347
   Gonzalez Javier, 2017, P NONV MEM WORKSH
   Gupta A., 2011, FAST, P91
   Hao MZ, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P263
   He Jun., 2015, Proceedings of the 13th USENIX Conference on File and Storage Technologies, FAST'15, P119
   Hsu WW, 2003, IBM SYST J, V42, P347, DOI 10.1147/sj.422.0347
   Hu X.-Y., 2009, System and Storage Conference (SYSTOR), DOI DOI 10.1145/1534530.1534544
   Huang CH, 2012, ELECTRON P THEOR COM, P15, DOI 10.4204/EPTCS.96.2
   Huang J, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P375
   Huang SS, 2010, I C DATA ENGIN WORKS, P41, DOI 10.1109/ICDEW.2010.5452747
   Im S, 2011, IEEE T COMPUT, V60, P80, DOI 10.1109/TC.2010.197
   Intel Corp, 2015, INT SOL STAT DRIV DC
   Jens Axboe, 2015, FIO FLEXIBLE IO TEST
   Jung Myoungsoo., 2012, P 4 USENIX C HOT TOP, P9
   Kang Jeong-Uk, 2014, P USENIX WORKSH HOT
   Kavalanekar Swaroop, 2008, 2008 IEEE International Symposium on Workload Characterization (IISWC), P119, DOI 10.1109/IISWC.2008.4636097
   Kim J., 2013, Research in Adaptive & Convergent Systems, P1, DOI [10.1016/j.brainres.2013.01, DOI 10.1016/J.BRAINRES.2013.01, 10.1109/ISR.2013.6695655]
   Kim J., 2015, P 13 USENIX C FIL ST, P183
   Klimovic A, 2017, TWENTY-SECOND INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXII), P345, DOI 10.1145/3037697.3037732
   Lee Changman, 2015, 13 USENIX C FILE STO, P273
   Lee J., P 6 ANN INT SYSTEMS, DOI DOI 10.1145/2485732.2485745
   Lee J, 2011, INT SYM PERFORM ANAL, P12, DOI 10.1109/ISPASS.2011.5762711
   Lee S., 2011, Proceedings of the ACM Symposium on Applied Computing SAC '11, P374
   Lee Yangsup., 2009, Proceedings of the 7th IEEE/ACM international conference on Hardware/software codesign and system synthesis, P163
   Lu Y., 2013, File and Storage Technologies(FAST), P257
   Micron Technology Inc, 2011, NAND FLASH MED MAN T
   Min Changwoo, 2012, FAST, V12, P1
   NVM Express Inc, 2015, NVM EXPR OPT PCI EXP
   Ouyang J, 2014, ACM SIGPLAN NOTICES, V49, P471, DOI 10.1145/2541940.2541959
   Park Stan., 2012, FAST, P13
   Patterson D. A., 1988, SIGMOD Record, V17, P109, DOI 10.1145/971701.50214
   Peter S, 2016, ACM T COMPUT SYST, V33, DOI 10.1145/2812806
   Rajashekhar Manju, FATCHACHE
   Rashmi KV, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P401
   Riska A, 2006, USENIX ASSOCIATION PROCEEDINGS OF THE 2006 USENIX ANNUAL TECHNICAL CONFERENCE, P97
   Riska A, 2009, I S WORKL CHAR PROC, P158, DOI [10.1109/IISWC.2009.5306787, 10.1145/1639562.1639589]
   Samsung Electronics Co, 2015, SAMS PM1725 NVME PCL
   Sandoval Omar, KYBER MULTIQUEUE I O
   Seagate Technology LLC, 2017, SSD OV PROV ITS BEN
   Shen Kai., 2013, Proceedings of the 2013 USENIX Conference on Annual Technical Conference, USENIX ATC'13, P67
   Shin W, 2015, INT CONF BIG DATA, P111, DOI 10.1109/35021BIGCOMP.2015.7072819
   Shue David, 2014, P 9 EUR C COMP SYST, P1
   Shvachko K, 2010, IEEE S MASS STOR SYS
   Skourtis Dimitris, 2014, 2014 USENIX ANN TECH, P463
   Smith K, 2013, EDN Network
   Stoica R, 2013, PROC VLDB ENDOW, V6, P733, DOI 10.14778/2536360.2536372
   Sun H, 2013, I S MOD ANAL SIM COM, P212, DOI 10.1109/MASCOTS.2013.29
   Tavakkol A., 2016, ACM TRANS MODELING P, V1, DOI DOI 10.1145/2829974
   Vulimiri A, 2013, PROCEEDINGS OF THE 2013 ACM INTERNATIONAL CONFERENCE ON EMERGING NETWORKING EXPERIMENTS AND TECHNOLOGIES (CONEXT '13), P283, DOI 10.1145/2535372.2535392
   Wang Peng, 2014, P 9 EUR C COMP SYST, DOI DOI 10.1145/2592798.2592804
   Wu G., 2012, P 10 USENIX C FIL ST, V12, P10
   Wu Suzhen., 2016, Proceedings of the 2016 International Conference on Supercomputing, P28
   Wu Z., 2015, NSDI, P543
   Yan SQ, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P15
   Yang JP, 2017, SYSTOR'17: PROCEEDINGS OF THE 10TH ACM INTERNATIONAL SYSTEMS AND STORAGE CONFERENCE, DOI 10.1145/3078468.3078469
   Zhang JC, 2016, PROCEEDINGS OF USENIX ATC '16: 2016 USENIX ANNUAL TECHNICAL CONFERENCE, P87
NR 82
TC 10
Z9 10
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD FEB
PY 2022
VL 18
IS 1
AR 5
DI 10.1145/3465406
PG 21
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZN5OY
UT WOS:000765084700002
DA 2024-07-18
ER

PT J
AU Fukatani, T
   Le, HH
   Yokota, H
AF Fukatani, Takayuki
   Le, Hieu Hanh
   Yokota, Haruo
TI Lightweight Dynamic Redundancy Control with Adaptive Encoding for
   Server-based Storage
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Server-based storage; data redundancy; replication; erasure coding
AB With the recent performance improvements in commodity hardware, low-cost commodity server-based storage has become a practical alternative to dedicated-storage appliances. Because of the high failure rate of commodity servers, data redundancy across multiple servers is required in a server-based storage system. However, the extra storage capacity for this redundancy significantly increases the system cost. Although erasure coding (EC) is a promising method to reduce the amount of redundant data, it requires distributing and encoding data among servers. There remains a need to reduce the performance impact of these processes involving much network traffic and processing overhead. Especially, the performance impact becomes significant for random-intensive applications. In this article, we propose a new lightweight redundancy control for server-based storage. Our proposed method uses a new local filesystem-based approach that avoids distributing data by adding data redundancy to locally stored user data. Our method switches the redundancy method of user data between replication and EC according to workloads to improve capacity efficiency while achieving higher performance. Our experiments showup to 230% better online-transaction-processing performance for our method compared with CephFS, a widely used alternative system. We also confirmed that our proposed method prevents unexpected performance degradation while achieving better capacity efficiency.
C1 [Fukatani, Takayuki; Le, Hieu Hanh; Yokota, Haruo] Tokyo Inst Technol, Meguro Ku, 2-12-1 Ookayama, Tokyo, Japan.
   [Fukatani, Takayuki] Hitachi Ltd, Totsuka Ku, 292 Yoshida Cho, Yokohama, Kanagawa, Japan.
C3 Tokyo Institute of Technology; Hitachi Limited
RP Fukatani, T (corresponding author), Tokyo Inst Technol, Meguro Ku, 2-12-1 Ookayama, Tokyo, Japan.; Fukatani, T (corresponding author), Hitachi Ltd, Totsuka Ku, 292 Yoshida Cho, Yokohama, Kanagawa, Japan.
EM fukatani@de.cs.titech.ac.jp; hanhlh@de.cs.titech.ac.jp;
   yokota@cs.titech.ac.jp
RI Yokota, Haruo/N-6975-2014
OI Yokota, Haruo/0000-0001-9788-0443
CR Aghayev A, 2019, PROCEEDINGS OF THE TWENTY-SEVENTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '19), P353, DOI 10.1145/3341301.3359656
   Amazon Web Services, 2020, AM EC2 I3 INST
   [Anonymous], 2008, P USENIX ANN TECHN C
   Apache.org, 2019, WELC AP ZOOKEEPER
   Barroso L. A., 2018, SYNTHESIS LECT COMPU, V3rd
   Beserra P. V., 2012, 2012 IEEE 6th International Workshop on the Maintenance and Evolution of Service-Oriented and Cloud-Based Systems (MESOCA 2012), P7, DOI 10.1109/MESOCA.2012.6392602
   Borthakur Dhruba, 2007, Hadoop Proj. Website, V2007, P21
   Bovet D. P., 2005, Understanding the Linux Kernel: from I/O ports to process management
   Boyer EricB., 2012, GLUSTERFS ONE STORAG
   Cano I, 2017, PROCEEDINGS OF NSDI '17: 14TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P51
   Chan JeremyC. W., 2014, P 12 USENIX C FILE S, P163
   Chen YP, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P43
   Corbet Johnathan., 2010, OPEN HANDLE
   Corbet Johnathan, 2010, PUNCHING HOLES FILES
   Fan Bin, 2011, Technical Report, CMU-PDL-11-112
   Fikes A., 2010, Storage architecture and challenges
   Fukatani Takayuki., 2018, INT J SMART COMPUT A, V2, P23
   Fukatani Takayuki., 2019, P 38 INT S REL DISTR, P353
   Ghemawat S., 2003, ACM SIGOPS OPERATING, V37, P29, DOI [DOI 10.1145/945445.945450, DOI 10.1145/1165389.945450, 10.1145/1165389.945450]
   Gibson Garth., 2010, DISKREDUCE V2 0 HDFS
   Huang CH, 2012, ELECTRON P THEOR COM, P15, DOI 10.4204/EPTCS.96.2
   IEEE and The Open Group, 2018, 10031T2017 IEEE
   Izraelevitz J., 2019, BASIC PERFORMANCE ME
   kernel.org, 2020, LINUX KERNEL ARCH
   Khan Osama., 2012, FAST, P20
   Kim W, 2009, J OBJECT TECHNOL, V8, P65, DOI 10.5381/jot.2009.8.1.c4
   Koh S, 2017, I S WORKL CHAR PROC, P76, DOI 10.1109/IISWC.2017.8167758
   Lee C, 2017, SYSTOR'17: PROCEEDINGS OF THE 10TH ACM INTERNATIONAL SYSTEMS AND STORAGE CONFERENCE, DOI 10.1145/3078468.3078479
   Lee Dong-Yun, 2017, P 2017 INT C MASSIVE, V10
   Li RH, 2017, IEEE T PARALL DISTR, V28, P2500, DOI 10.1109/TPDS.2017.2678505
   Madhyastha Harsha V, 2012, P 10 USENIX C FIL ST
   Mathur Avantika, 2007, P LINUX S, V2, P21
   Matsuzawa K, 2020, ACM T STORAGE, V16, DOI 10.1145/3377322
   Mattos Steve., 2012, P FLASH MEM SUMM
   Meyer DT, 2012, ACM T STORAGE, V7, DOI 10.1145/2078861.2078864
   Mickens James., 2014, Proceedings of the 11th USENIX Conference on Networked Systems Design and Implementation. NSDI'14, P257
   Microsoft, 2020, MICR AZ LSV2 SER
   Mohammed Rafi K. C, 2016, P STOR DEV C SDC 16
   Narayanamurthy Srinivasan, 2016, P STOR DEV C SDC 16
   Narayanan D, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P253
   Nemoto Jun, 2017, INT J SMART COMPUT A, V1, P1
   Niu JP, 2018, IEEE ACCESS, V6, P13385, DOI 10.1109/ACCESS.2018.2803302
   Plank J.S., 2013, The UsenixMagazine, V38, P44
   Plank James S, 2007, CS07603 U TENN
   Pontes R, 2017, SYSTOR'17: PROCEEDINGS OF THE 10TH ACM INTERNATIONAL SYSTEMS AND STORAGE CONFERENCE, DOI 10.1145/3078468.3078480
   Rashmi K., 2015, USENIX C FIL STOR TE, P81
   Sharon Shachar., 2020, P STOR DEV C SDC 20
   ShuzhanWei Yongkun Li, 2017, P IEEE C COMP COMM, P1
   spec.org, 2014, SPEC SFS
   Tarasov Vasily, 2016, USENIX; login, V41, P6
   Umrao Vikhyat., 2017, CEPH COOKBOOK PRACTI
   Vajha M, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P139
   Vangoor BKR, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P59
   vmware, 2018, VMWARE VSAN 6 7 TECH
   Wallace Grant., 2012, Proceedings of the 10th USENIX conference on File and Storage Technologies, FAST'12, P4
   Weil SA, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P307
   Wires J, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P213
   Xia M, 2015, P 13 USENIX C FIL ST, P213
   Xu Qiumin, 2015, P 8 ACM INT SYST STO, DOI DOI 10.1145/2757667.2757684
   Xue S, 2020, PROCEEDINGS OF THE 2020 USENIX ANNUAL TECHNICAL CONFERENCE, P97
   Yang Y, 2016, ACM T STORAGE, V12, DOI 10.1145/2908557
   Zhang YM, 2020, ACM T STORAGE, V16, DOI 10.1145/3365839
   Zhou TL, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P317
NR 63
TC 0
Z9 0
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2021
VL 17
IS 4
AR 28
DI 10.1145/3456292
PG 38
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YY2GY
UT WOS:000754612700004
DA 2024-07-18
ER

PT J
AU Rebello, A
   Patel, Y
   Alagappan, R
   Arpaci-Dusseau, AC
   Arpaci-Dusseau, RH
AF Rebello, Anthony
   Patel, Yuvraj
   Alagappan, Ramnatthan
   Arpaci-Dusseau, Andrea C.
   Arpaci-Dusseau, Remzi H.
TI Can Applications Recover from fsync Failures?
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Durability; persistence; fsync failures; fsync; file system
AB We analyze how file systems and modern data-intensive applications react to fsync failures. First, we characterize how three Linux file systems (ext4, XFS, Btrfs) behave in the presence of failures. We find commonalities across file systems (pages are always marked clean, certain block writes always lead to unavailability) as well as differences (page content and failure reporting is varied). Next, we study how five widely used applications (PostgreSQL, LMDB, LevelDB, SQLite, Redis) handle fsync failures. Our findings show that although applications use many failure-handling strategies, none are sufficient: fsync failures can cause catastrophic outcomes such as data loss and corruption. Our findings have strong implications for the design of file systems and applications that intend to provide strong durability guarantees.
C1 [Rebello, Anthony; Patel, Yuvraj; Arpaci-Dusseau, Andrea C.; Arpaci-Dusseau, Remzi H.] Univ Wisconsin, Dept Comp Sci, 1210 W Dayton St, Madison, WI 53706 USA.
   [Alagappan, Ramnatthan] VMware Res Grp, Madison, WI 53706 USA.
C3 University of Wisconsin System; University of Wisconsin Madison
RP Rebello, A (corresponding author), Univ Wisconsin, Dept Comp Sci, 1210 W Dayton St, Madison, WI 53706 USA.
EM arebello@wisc.edu; yuvraj@cs.wisc.edu; ralagappan@vmware.com;
   dusseau@cs.wisc.edu; remzi@cs.wisc.edu
OI Alagappan, Ramnatthan/0000-0001-9911-4208; Rebello,
   Anthony/0000-0003-2614-7935; Arpaci-Dusseau, Andrea/0000-0001-8618-2738
FU VMware; Intel; Seagate; Samsung; NSF [CNS-1421033, CNS-1763810,
   CNS-1838733]; DOE [DE-SC0014935]; U.S. Department of Energy (DOE)
   [DE-SC0014935] Funding Source: U.S. Department of Energy (DOE)
FX We thank our sponsors: VMware, Intel, Seagate, and Samsung, for their
   generous support. Thismaterial was also supported by funding from NSF
   under award numbers CNS-1421033, CNS-1763810, and CNS-1838733, and DOE
   DE-SC0014935. Any opinions, findings, and conclusions or recommendations
   expressed in this material are those of the authors and do not
   necessarily reflect the views of NSF, DOE, or any other institutions.
CR Arpaci-Dusseau R. H., 2018, Operating systems: Three easy pieces
   Bairavasundaram LN, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P223
   Bairavasundaram LN, 2007, PERF E R SI, V35, P289
   Bairavasundaram LN, 2008, I C DEPEND SYS NETWO, P502, DOI 10.1109/DSN.2008.4630121
   Cao JR, 2018, INTERNATIONAL CONFERENCE ON SUPERCOMPUTING (ICS 2018), P1, DOI 10.1145/3205289.3205302
   Chidambaram V., 2012, Proceedings of the 10th USENIX conference on File and Storage Technologies, FAST 2012, San Jose, CA, USA, February 14-17, 2012, P9
   Chidambaram V, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P228, DOI 10.1145/2517349.2522726
   Chu Howard, 2020, LIGHTNING MEMORY MAP
   Corbet Jonathan, 2020, IMPROVED BLOCK LAYER
   Duplyakin D, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P1
   Forfang Christian, THESIS NORWEGIAN U S
   FUSE (Filesystem in Userspace), 2020, REF IMPL LIN FUSE FI
   Ganesan A, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P149
   Ganger G. R., 1994, Proceedings of the First USENIX Symposium on Operating Systems Design and Implementation (OSDI), P49
   Google, 2020, LevelDB
   Gunawi Haryadi S., 2007, Operating Systems Review, V41, P293, DOI 10.1145/1323293.1294290
   Gunawi HS, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P207
   Hagmann R., 1987, Operating Systems Review, V21, P155, DOI 10.1145/37499.37518
   IEEE and The Open Group, 2020, POSIX SPEC FSYNC
   Jaffer S, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P783
   Kari Hannu H, 1997, THESIS HELSINKI U TE
   Kluyver T, 2016, POSITIONING AND POWER IN ACADEMIC PUBLISHING: PLAYERS, AGENTS AND AGENDAS, P87, DOI 10.3233/978-1-61499-649-1-87
   Krioukov A, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P127
   Lai Allen, 2020, BUG 27805553 HARD ER
   Mathur Avantika, 2007, Usenix Association, V32, P25
   MOGUL JC, 1994, PROCEEDINGS OF THE SUMMER 1994 USENIX CONFERENCE, P99
   Mohan J, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P33
   Munro Thomas, 2020, FSYNC ERRORS POSTGRE
   Pillai TS, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P181
   Pillai Thanumalayan Sankaranarayana, 2014, P 11 USENIX S OP SYS, P433
   PostgreSQL Global Develop. Group, PostgreSQL
   Prabhakaran V, 2005, I C DEPEND SYS NETWO, P802, DOI 10.1109/DSN.2005.65
   Prabhakaran Vijayan, 2005, P OF THE 20 ACM S ON, V39, P206
   Rebello A, 2020, PROCEEDINGS OF THE 2020 USENIX ANNUAL TECHNICAL CONFERENCE, P753
   Rebello Anthony, 2020, FUSE FILE SYSTEM EMU
   Redis Labs, 2020, RED PERS
   Ringer Craig, 2020, POSTGRESQLS HANDLING
   Rodeh O, 2013, ACM T STORAGE, V9, DOI 10.1145/2501620.2501623
   Schroeder B, 2010, ACM T STORAGE, V6, DOI 10.1145/1837915.1837917
   Seltzer M., 1990, Proceedings of the Winter 1990 USENIX Conference, P313
   Silvers C, 2000, USENIX ASSOCIATION PROCEEDINGS OF THE FREENIX TRACK, P285
   SQLite, 2020, SQLITE
   Sweeney A, 1996, PROCEEDINGS OF THE USENIX 1996 ANNUAL TECHNICAL CONFERENCE, P1
   SystemTap, 2020, SYSTEMTAP
   The FreeBSD Project, 2020, FREEBSD VFS LAYER RE
   The Linux Kernel Organization, MAN PAG DMSET
   The Linux Kernel Organization, 2020, FSYNC 2 LIN PROGR MA
   The PostgreSQL Global Development Group, 2020, POSTGRESQL WRIT AH L
   The Stack Exchange network, 2020, IS DAT J SAF EXT4 OP
   Theodore Tso, 2020, WHY DOES EXT4 CLEAR
   Vondra Tomas, 2019, POSTGRESQL VS FSYNC
   WiredTiger, 2020, WT 4045 DONT RETR FS
   Won Y, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P211
   Zhang Y., 2010, FAST, P29
   Zhang Y., 2015, IEEE 31 S MASS STORA, P1, DOI [DOI 10.1109/MSST.2015.7208275, 10.1109/MSST.2015.7208275, DOI 10.1007/S11042-015-2982-X]
NR 55
TC 1
Z9 1
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUN
PY 2021
VL 17
IS 2
AR 12
DI 10.1145/3450338
PG 30
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SV7JF
UT WOS:000663994900004
DA 2024-07-18
ER

PT J
AU Shen, ZY
   Chen, F
   Jia, YC
   Shao, ZL
AF Shen, Zhaoyan
   Chen, Feng
   Jia, Yichen
   Shao, Zili
TI DIDACache: An Integration of Device and Application for Flash-based
   Key-value Caching
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE NAND flash memory; key-value caching; open-channel SSD
AB Key-value caching is crucial to today's low-latency Internet services. Conventional key-value cache systems, such as Memcached, heavily rely on expensive DRAM memory. To lower Total Cost of Ownership, the industry recently is moving toward more cost-efficient flash-based solutions, such as Facebook's McDipper [14] and Twitter's Fatcache [56]. These cache systems typically take commercial SSDs and adopt a Memcached-like scheme to store and manage key-value cache data in flash. Such a practice, though simple, is inefficient due to the huge semantic gap between the key-value cache manager and the underlying flash devices.
   In this article, we advocate to reconsider the cache system design and directly open device-level details of the underlying flash storage for key-value caching. We propose an enhanced flash-aware key-value cache manager, which consists of a novel unified address mapping module, an integrated garbage collection policy, a dynamic over-provisioning space management, and a customized wear-leveling policy, to directly drive the flash management. A thin intermediate library layer provides a slab-based abstraction of low-level flash memory space and an API interface for directly and easily operating flash devices. A special flash memory SSD hardware that exposes flash physical details is adopted to store key-value items. This co-design approach bridges the semantic gap and well connects the two layers together, which allows us to leverage both the domain knowledge of key-value caches and the unique device properties. In this way, we can maximize the efficiency of key-value caching on flash devices while minimizing its weakness. We implemented a prototype, called DIDACache, based on the Open-Channel SSD platform. Our experiments on real hardware show that we can significantly increase the throughput by 35.5%, reduce the latency by 23.6%, and remove unnecessary erase operations by 28%.
C1 [Shen, Zhaoyan; Shao, Zili] Hong Kong Polytech Univ, Hong Kong, Peoples R China.
   [Chen, Feng] Louisiana State Univ, Dept Comp Sci & Engn, 3272-L Patrick K Taylor Hall, Baton Rouge, LA 70803 USA.
   [Jia, Yichen] Louisiana State Univ, Dept Comp Sci & Engn, 140C Coates Hall, Baton Rouge, LA 70803 USA.
   [Shen, Zhaoyan] Shandong Univ, Sch Comp Sci & Technol, Binhai Rd 72, Qingdao, Peoples R China.
   [Shao, Zili] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.
C3 Hong Kong Polytechnic University; Louisiana State University System;
   Louisiana State University; Louisiana State University System; Louisiana
   State University; Shandong University; Chinese University of Hong Kong
RP Shen, ZY (corresponding author), Hong Kong Polytech Univ, Hong Kong, Peoples R China.; Shen, ZY (corresponding author), Shandong Univ, Sch Comp Sci & Technol, Binhai Rd 72, Qingdao, Peoples R China.
EM shenzhaoyan@126.com; fchen@csc.lsu.edu; yjia@csc.lsu.edu;
   zilishao@gmail.com
RI Jia, Yichen/GRX-1463-2022; Shao, Zili/AAX-3339-2020; Shen,
   Zhaoyan/T-3711-2019
OI Shao, Zili/0000-0002-2173-2847; Jia, Yichen/0000-0001-8346-1651
FU Research Grants Council of the Hong Kong Special Administrative Region,
   China [GRF 152223/15E, GRF 152736/16E, GRF 152066/17E]; Louisiana Board
   of Regents [LEQSF(2014-17)-RD-A-01]; U.S. National Science Foundation
   [CCF-1453705, CCF-1629291]; Direct For Computer & Info Scie & Enginr;
   Division of Computing and Communication Foundations [1453705] Funding
   Source: National Science Foundation
FX This work is partially supported by the Research Grants Council of the
   Hong Kong Special Administrative Region, China (GRF 152223/15E, GRF
   152736/16E, GRF 152066/17E), Louisiana Board of Regents
   LEQSF(2014-17)-RD-A-01, and U.S. National Science Foundation
   (CCF-1453705, CCF-1629291).
CR Agrawal Nitin, 2008, 2008 USENIX ANN TECH
   Anand  A., 2010, P USENIX S NETW SYST
   [Anonymous], 2014, P 6 USENIX WORKSH HO
   [Anonymous], P USENIX C FIL STOR
   Atikoglu B., 2012, P ACM SIGMETRICS PER
   Bjorling Matias, 2017, P USENIX C FIL STOR
   Carra Damiano, 2014, P INT C COMM ICC 14
   Chen F., 2011, P USENIX C FIL STOR
   Debnath Biplob, 2011, P ACM SIGMOD INT C M
   Dirik C., 2009, P INT S COMP ARCH IS
   Eisenman A., 2017, ARXIV170202588
   Facebook, MCDIPPER KEY VAL CAC
   Feng Chen, 2009, P INT C MEAS MOD COM
   Feng Chen, 2011, P INT S HIGH PERF CO
   Gal E, 2005, ACM COMPUT SURV, V37, P138, DOI 10.1145/1089733.1089735
   Gokhale Salil, 2010, P USENIX WORKSH HOT
   Gonzalez J, 2016, P 7 NONV MEM WORKSH, P1
   Grupp Laura M., 2009, P INT S MICR MICR 09
   GUPTA A., 2009, P INT C ARCH SUPP PR
   Hu X., 2015, 2015 USENIX ANN TECH
   Jian Huang, 2017, P USENIX C FIL STOR
   Jin Yanqin, 2017, P IEEE INT S HIGH PE
   Klimovic A, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901337
   Lee Benjamin C., 2009, P ACM ASS COMP MACH
   Leventhal A, 2008, COMMUN ACM, V51, P47, DOI 10.1145/1364782.1364796
   Lilly Paul, 2013, FACEBOOK DITCHES DRA
   LIM H., 2011, P ACM S OP SYST PRIN
   Liu Duo, 2011, P IEEE REAL TIM SYST
   Lu Lanyue, 2016, P USENIX C FIL STOR
   Mai Zheng, 2014, P USENIX S OP SYST D
   Mai Zheng, 2013, P USENIX C FIL STOR
   Margaglia Fabio, 2016, P USENIX C FIL STOR
   Marmol Leonardo, 2015, P USENIX ANN TECHN C
   Marmol Leonardo, 2015, P USENIX WORKSH HOT
   Marsh B., 1994, P HAW C SYST SCI
   Mesnier Michael P., 2011, P ACM S OP SYST PRIN
   NISHTALA R., 2013, P USENIX S NETW SYST
   Oh Y., 2012, FAST, V12
   Ouyang Jian, 2014, P INT C ARCH SUPP PR
   Ouyang Xiangyong, 2012, INT C PAR PROC ICCP
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   SamSung, SAMS 840 PRO
   Saxena Mohit, 2012, P EUR C COMP SYST EU
   Schlosser Steve, DISKSIM 4 0
   Seshadri S., 2014, S OPERATING SYSTEMS, P67
   Shafaei Mansour, 2016, P USENIX WORKSH HOT
   Shen Zhaoyan, 2016, P USENIX WORKSH HOT
   Shen Zhaoyan, 2017, P USENIX C FIL STOR
   Soundararajan Gokul, 2010, P USENIX C FIL STOR
   Tang Linpeng, 2015, P USENIX C FIL STOR
   Twitter, FATC
   Wang Peng, 2015, P EUR C COMP SYST EU
   WU X., 2015, P USENIX ANN TECHN C
   Yan S., 2017, P USENIX C FIL STOR
   Yang Jingpei, 2014, P 2 WORKSH INT NVM F
   Yong Guan, 2013, P ANN ACM SIGPLAN SI
   Zhang Heng, 2016, P USENIX C FIL STOR
   Zhang Yiying, 2013, P USENIX C FIL STOR
   Zhang Yiying, 2015, P INT C MASS STOR SY
   [No title captured]
NR 60
TC 19
Z9 21
U1 0
U2 18
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2018
VL 14
IS 3
AR 26
DI 10.1145/3203410
PG 32
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ4JQ
UT WOS:000457140700007
DA 2024-07-18
ER

PT J
AU Wu, SZ
   Mao, B
   Chen, XL
   Jiang, H
AF Wu, Suzhen
   Mao, Bo
   Chen, Xiaolan
   Jiang, Hong
TI LDM: Log Disk Mirroring with Improved Performance and Reliability for
   SSD-Based Disk Arrays
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE SSD-based disk arrays; log technique; disk buffer; performance
   evaluation; reliability analysis
ID RAID; AVAILABILITY
AB With the explosive growth in data volume, the I/O bottleneck has become an increasingly daunting challenge for big data analytics. Economic forces, driven by the desire to introduce flash-based Solid-State Drives (SSDs) into the high-end storage market, have resulted in hybrid storage systems in the cloud. However, a single flash-based SSD cannot satisfy the performance, reliability, and capacity requirements of enterprise or HPC storage systems in the cloud. While an array of SSDs organized in a RAID structure, such as RAID5, provides the potential for high storage capacity and bandwidth, reliability and performance problems will likely result from the parity update operations. In this article, we propose a Log Disk Mirroring scheme (LDM) to improve the performance and reliability of SSD-based disk arrays. LDM is a hybrid disk array architecture that consists of several SSDs and two hard disk drives (HDDs). In an LDM array, the two HDDs are mirrored as a write buffer that temporally absorbs the small write requests. The small and random write data are written on the mirroring buffer by using the logging technique that sequentially appends new data. The small write data are merged and destaged to the SSD-based disk array during the system idle periods. Our prototype implementation of the LDM array and the performance evaluations show that the LDM array significantly outperforms the pure SSD-based disk arrays by a factor of 20.4 on average, and outperforms HPDA by a factor of 5.0 on average. The reliability analysis shows that the MTTDL of the LDM array is 2.7 times and 1.7 times better than that of pure SSD-based disk arrays and HPDA disk arrays.
C1 [Wu, Suzhen; Chen, Xiaolan] Xiamen Univ, Dept Comp Sci, Xiamen 361005, Peoples R China.
   [Mao, Bo] Xiamen Univ, Software Sch, Xiamen 361005, Peoples R China.
   [Jiang, Hong] Univ Texas Arlington, Dept Comp Sci & Engn, Arlington, TX 76010 USA.
C3 Xiamen University; Xiamen University; University of Texas System;
   University of Texas Arlington
RP Mao, B (corresponding author), Xiamen Univ, Software Sch, Xiamen 361005, Peoples R China.
EM suzhen@xmu.edu.cn; maobo@xmu.edu.cn; xiaolchen@qq.com;
   hong.jiang@uta.edu
FU National Natural Science Foundation of China [61100033, 61472336,
   61402385]; U.S. NSF [NSF-CNS-1116606, NSF-CNS-1016609]; Scientific
   Research Foundation for the Returned Overseas Chinese Scholars, State
   Education Ministry; Fundamental Research Funds for Central Universities
   [20720140515]; Division Of Computer and Network Systems; Direct For
   Computer & Info Scie & Enginr [1016609] Funding Source: National Science
   Foundation
FX This work is supported by the National Natural Science Foundation of
   China under Grant No. 61100033, No. 61472336, and No. 61402385; the U.S.
   NSF under Grant No. NSF-CNS-1116606 and No. NSF-CNS-1016609; and the
   Scientific Research Foundation for the Returned Overseas Chinese
   Scholars, State Education Ministry. This work is also sponsored by the
   Fundamental Research Funds for the Central Universities (No.
   20720140515).
CR Agrawal Nitin, 2008, P USENIX ANN TECHN C, P57
   [Anonymous], 1988, A case for redundant arrays of inexpensive disks RAID, DOI DOI 10.1145/50202.50214
   [Anonymous], 2011, P 27 IEEE S MASS STO
   [Anonymous], 2012, P IEEE 28 S MASS STO
   [Anonymous], 2008, SAMSUNG REPORT
   Balakrishnan M, 2010, ACM T STORAGE, V6, DOI 10.1145/1807060.1807061
   Balakrishnan M, 2010, EUROSYS'10: PROCEEDINGS OF THE EUROSYS 2010 CONFERENCE, P15
   Caulfield A.M., 2010, Proc. of the 2010 ACM/IEEE Int. Conf. for High Performance Comput., Network, P1
   Chan JeremyC. W., 2014, P 12 USENIX C FILE S, P163
   Chen F, 2009, PERF E R SI, V37, P181
   Chung CC, 2014, IEEE T VLSI SYST, V22, P1470, DOI 10.1109/TVLSI.2013.2275737
   Dirik C, 2009, CONF PROC INT SYMP C, P279, DOI 10.1145/1555815.1555790
   Du YM, 2011, LECT NOTES COMPUT SC, V6985, P248, DOI 10.1007/978-3-642-24403-2_20
   GOLDING R, 1995, PROCEEDINGS OF THE 1995 USENIX TECHNICAL CONFERENCE, P201
   Greenan K., 2009, P 5 WORKSH HOT TOP S
   Grupp L. M., 2012, VIRTUALIZING STORAGE
   Grupp L. M., 2012, P 10 USENIX C FIL ST
   Im S, 2011, IEEE T COMPUT, V60, P80, DOI 10.1109/TC.2010.197
   Jeremic N., 2011, P 4 ANN INT C SYSTEM, P1
   Jiang H., 2012, P 26 INT C LARG INST, P17
   Kao H.-l., 2013, IEEE MTT S INT MICR, P1
   Kim Y., IEEE INT WORKSHOP MO
   Lin Lin, 2011, Proceedings of the 2011 IEEE 19th International Symposium on Modelling, Analysis & Simulation of Computer and Telecommunication Systems (MASCOTS 2011), P318, DOI 10.1109/MASCOTS.2011.41
   Mao B., 2015, P 33 IEEE INT C COMP, P476
   Mao B, 2015, INT PARALL DISTRIB P, P633, DOI 10.1109/IPDPS.2015.47
   Mao B, 2014, ACM T STORAGE, V10, DOI 10.1145/2512348
   Mao B, 2012, ACM T STORAGE, V8, DOI 10.1145/2093139.2093143
   Mao Bo., 2010, Parallel Distributed Processing (IPDPS), 2010 IEEE International Symposium on, P1
   Meza Justin, 2015, ACM SIGMETRICS Performance Evaluation Review, V43, P177, DOI 10.1145/2745844.2745848
   Min Changwoo, P 10 USENIX C FIL ST
   Moon S., 2013, P 5 USENIX WORKSH HO
   Narayanan D, 2009, EUROSYS'09: PROCEEDINGS OF THE FOURTH EUROSYS CONFERENCE, P145
   Ren J, 2011, INT S HIGH PERF COMP, P278, DOI 10.1109/HPCA.2011.5749736
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   Schroeder B, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 5TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES ( FAST '07), P1
   Soundararajan Gokul., 2010, Proceedings of the 8th USENIX conference on File and storage technologies, FAST'10, P8
   Suzhen Wu, 2015, Algorithms and Architectures for Parallel Processing. 15th International Conference, ICA3PP 2015. Proceedings: LNCS 9531, P18, DOI 10.1007/978-3-319-27140-8_2
   [王运 Wang Yun], 2014, [铀矿地质, Uranium Geology], V30, P1
   Wu S., 2009, USENIX C FILE STORAG, V9, P239
   Wu SZ, 2015, IEEE T COMPUT, V64, P2637, DOI 10.1109/TC.2014.2366734
   Wu SZ, 2011, IEEE T COMPUT, V60, P64, DOI 10.1109/TC.2010.206
   Xie T, 2010, IEEE T PARALL DISTR, V21, P1330, DOI 10.1109/TPDS.2009.175
   YI LT, 2013, P 31 INT C COMP DES, P455
   ZHANG Y, 2013, SHUIZIYUAN BAOHU, V29, P13, DOI DOI 10.3969/j.issn.1004-6933.2013.06.003
NR 44
TC 24
Z9 30
U1 0
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2016
VL 12
IS 4
AR 22
DI 10.1145/2892639
PG 21
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DV2OQ
UT WOS:000382760500005
DA 2024-07-18
ER

PT J
AU Chen, TY
   Wei, HW
   Yeh, TT
   Hsu, TS
   Shih, WK
AF Chen, Tseng-Yi
   Wei, Hsin-Wen
   Yeh, Tsung-Tai
   Hsu, Tsan-Sheng
   Shih, Wei-Kuan
TI An Energy-Efficient and Reliable Storage Mechanism for Data-Intensive
   Academic Archive Systems
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Algorithms; Economics; Experimentation; Reliability; Management;
   Energy-aware storage system; RAID; power consumption; reliability; disk
   spin-down
ID RELIABILITY
AB Previous studies proposed energy-efficient solutions, such as multispeed disks and disk spin-down methods, to conserve power in their respective storage systems. However, inmost cases, the authors did not analyze the reliability of their solutions. According to research conducted by Google and the IDEMA standard, frequently setting the disk status to standby mode will increase the disk's Annual Failure Rate and reduce its lifespan. To resolve the issue, we propose an evaluation function called E(3)SaRC (Economic Evaluation of Energy Saving with Reliability Constraint), which considers the cost of hardware failure when applying energy-saving schemes. We also present an adaptive write cache mechanism called CacheRAID. The mechanism tries to mitigate the random access problems that implicitly exist in RAID techniques and thereby reduce the energy consumption of RAID disks. CacheRAID also addresses the issue of system reliability by applying a control mechanism to the spin-down algorithm. Our experimental results show that the CacheRAID storage system can reduce the power consumption of the conventional software RAID 5 system by 65% to 80%. Moreover, according to the E(3)SaRC measurement, the overall saved cost of CacheRAID is the largest among the systems that we compared.
C1 [Chen, Tseng-Yi; Shih, Wei-Kuan] Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu 300, Taiwan.
   [Wei, Hsin-Wen] Tamkang Univ, Dept Elect Engn, New Taipei City, Taiwan.
   [Yeh, Tsung-Tai] Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47907 USA.
   [Hsu, Tsan-Sheng] Acad Sinica, Inst Informat Sci, Taipei, Taiwan.
C3 National Tsing Hua University; Tamkang University; Purdue University
   System; Purdue University; Academia Sinica - Taiwan
RP Chen, TY (corresponding author), Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu 300, Taiwan.
EM tychen@cs.nthu.edu.tw; hwwei@mail.tku.edu.tw; yeh14@purdue.edu;
   tshsu@iis.sinica.edu.tw; wshih@cs.nthu.edu.tw
OI Chen, Tseng-Yi/0000-0003-2939-2821
FU Ministry of Science and Technology, Republic of China
   [MOST-103-2221-E-001-033, MOST-103-2221-E-032-035,
   MOST-103-2622-E-009-012, MOST-103-2218-E-194-007]
FX This work was supported by grants from MOST-103-2221-E-001-033,
   MOST-103-2221-E-032-035, MOST-103-2622-E-009-012,
   MOST-103-2622-E-009-012, and MOST-103-2218-E-194-007 from the Ministry
   of Science and Technology, Republic of China.
CR Academia Sinica, 2008, TAIW E LEARN DIG ARC
   Adams Ian F., 2012, ACM T STORAGE, V8
   [Anonymous], 2010, SIGOPS Oper. Syst. Rev., DOI DOI 10.1145/1713254.1713276
   [Anonymous], 2007, P 5 USENIX C FIL STO
   [Anonymous], 2010, P 8 USENIX C FIL STO
   Chen Tseng-Yi, 2014, SOFTWARE PRACTICE EX
   CHEN TY, 2012, P 2012 IEEE 5 INT C, P117, DOI DOI 10.1109/UCC.2012.26
   Colarelli Dennis., 2002, SUPERCOMPUTING 02, P1, DOI DOI 10.1109/SC.2002.10058
   Cole Gerry, 2000, TP3381 SEAG PERS STO
   Deng YH, 2011, ACM COMPUT SURV, V43, DOI 10.1145/1922649.1922660
   Facebook Inc, 2012, FLASHC
   Gantz John F., 2008, An Updated Forecast of Worldwide Information Growth Through 2011
   Hikida Satoshi, 2012, Database Systems for Advanced Applications. Proceedings 17th International Conference, DASFAA 2012, P138, DOI 10.1007/978-3-642-29035-0_10
   IETF, 2000, INT SMALL COMP SYST
   International Disk Drive Equipment & Materials Association (IDEMA), 1998, R298 IDEMA
   *IOZONE, 2006, IOZONE FIL BENCHM
   IXBT Labs, 2005, HDD DIET POW CONS HE
   Joukov N, 2008, EUROSYS'08: PROCEEDINGS OF THE EUROSYS 2008 CONFERENCE, P69, DOI 10.1145/1357010.1352600
   Kaushik R.T., 2010, Proceedings of the 2010 international conference on Power aware computing and systems, HotPower'10, P1
   Li X, 2011, 2011 SECOND ETP/IITA CONFERENCE ON TELECOMMUNICATION AND INFORMATION (TEIN 2011), VOL 1, P1, DOI 10.1109/ICCSN.2011.6014661
   Li Xiaozhou., 2011, ACM SIGMETRICS PERFO, V38, P4
   Narayanan D., 2008, ACM T STORAGE, V4, P3
   Pinheiro E., 2004, INT C SUPERCOMPUTING, P68, DOI DOI 10.1145/1006209.1006220
   SanDisk Inc, 2013, SPEC SANDISK EXTR 2
   Schroeder B, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 5TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES ( FAST '07), P1
   Seagate Corporation, 2014, HARD DRIV DAT
   Storer MW, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P1, DOI 10.1145/1456469.1456471
   Tyndall George W., 2010, ROLE HEAD DISK INTER
   Vasudevan Vijay, 2011, Operating Systems Review, V45, P34, DOI 10.1145/1945023.1945029
   Verma A., 2010, P 8 USENIX C FILE ST, P20
   Wang A. A., 2006, ACM Transaction on Storage, V2, P309, DOI 10.1145/1168910.1168914
   Wang J, 2008, IEEE T COMPUT, V57, P359, DOI 10.1109/TC.2007.70821
   Weddle Charles, 2007, ACM Transactions on Storage, V3, DOI 10.1145/1288783.1289721
   Xie T, 2008, IEEE T COMPUT, V57, P748, DOI 10.1109/TC.2008.27
   Xie T, 2011, J PARALLEL DISTR COM, V71, P198, DOI 10.1016/j.jpdc.2010.08.006
   Xu X, 2013, IEEE ACM INT SYMP, P530, DOI 10.1109/CCGrid.2013.82
   Yeh TT, 2010, P IRODS US GROUP M
   Yin Shu, 2009, CLUSTER COMPUTING AN, P1
   Zhu Q, 2005, P 20 ACM S OP SYST P, P177, DOI DOI 10.1145/1095809.1095828
   Zhu QB, 2004, 10TH INTERNATIONAL SYMPOSIUM ON HIGH PERFORMANCE COMPUTER ARCHITECTURE, PROCEEDINGS, P118
NR 40
TC 6
Z9 6
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAR
PY 2015
VL 11
IS 2
AR 10
DI 10.1145/2720021
PG 21
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CF1KG
UT WOS:000352303400006
DA 2024-07-18
ER

PT J
AU Trifonov, P
AF Trifonov, P.
TI Low-Complexity Implementation of RAID Based on Reed-Solomon Codes
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Algorithms; Performance; Reliability; Reed-Solomon codes; fast
   algorithms; FFT; RAID
ID FAST FOURIER-TRANSFORMS; COMPUTATION
AB Fast algorithms are proposed for encoding and reconstructing data in RAID based on Reed-Solomon codes. The proposed approach is based on the cyclotomic fast Fourier transform algorithm and enables one to significantly reduce the number of expensive Galois field multiplications required. The complexity of the obtained algorithms is much lower than those for existing MDS array codes. Software implementation of the proposed algorithms is discussed. The performance results show that the new algorithms provide. substantially better performance compared with the standard algorithm.
C1 [Trifonov, P.] St Petersburg State Polytech Univ, St Petersburg 194021, Russia.
C3 Peter the Great St. Petersburg Polytechnic University
RP Trifonov, P (corresponding author), St Petersburg State Polytech Univ, Distributed Comp & Networking Dept, Polytechnicheskaya Str 21,Off 104, St Petersburg 194021, Russia.
EM petert@dcn.icc.spbstu.ru
RI trifonov, peter/GWR-1173-2022; Trifonov, Peter/AAN-8159-2020; Trifonov,
   Petr Vladimirovich/ABB-2071-2020
OI Trifonov, Peter/0000-0001-6960-0942; 
FU  [MK-5407.2013.9]
FX This work was developed in collaboration with EMC Corporation. This work
   was also supported by grant MK-5407.2013.9 of the president of Russia.
CR Al Ghouwayel A, 2007, 2007 INTERNATIONAL SYMPOSIUM ON COMMUNICATIONS AND INFORMATION TECHNOLOGIES, VOLS 1-3, P146, DOI 10.1109/ISCIT.2007.4392002
   [Anonymous], 1996, FINITE FIELDS
   Bellini S, 2011, IEEE T COMMUN, V59, P2110, DOI 10.1109/TCOMM.2011.060911.090145
   Bhaskar Raghav., 2003, SPAA '03, P256
   BLAHUT RE, 1984, THEORY PRACTICE ERRO
   Blaum M., 1995, IEEE T COMPUT, V44, P2
   Blomer J., 1995, XOR BASED ERASURE RE
   Chen N, 2009, IEEE SIGNAL PROC LET, V16, P279, DOI 10.1109/LSP.2009.2014292
   Chen N, 2009, IEEE T SIGNAL PROCES, V57, P1010, DOI 10.1109/TSP.2008.2009891
   Chu E, 2000, COMP MATH SERIES, P3
   Corbett P, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P1
   Costa E, 2004, EUR T TELECOMMUN, V15, P337, DOI 10.1002/ett.982
   Didier F., 2009, ARXIV13015536V1
   Fedorenko SV, 2006, PROBL INFORM TRANSM+, V42, P139, DOI 10.1134/S0032946006020074
   Fedorenko S. V., 2011, IEEE T INFORM THEORY
   Fedorenko SV, 2011, IEEE INT SYMP INFO, P1200, DOI 10.1109/ISIT.2011.6033725
   Hsieh P.-H., 2004, P 19 IEEE INT S DEF
   Huang C, 2007, SIXTH IEEE INTERNATIONAL SYMPOSIUM ON NETWORK COMPUTING AND APPLICATIONS, PROCEEDINGS, P79, DOI 10.1109/NCA.2007.37
   Knuth D.E., 1973, ART COMPUTER PROGRAM, VII
   Lin TC, 2007, IEEE T COMMUN, V55, P2240, DOI 10.1109/TCOMM.2007.910595
   MacWilliams F. J., 1978, The Theory of Error-Correcting Codes
   Moon TK, 2005, ERROR CORRECTION CODING: MATHEMATICAL METHODS AND ALGORITHMS
   Plank J. S., 2014, UTEECS14721 U TENN
   Plank J. S., 2005, CS05570 U TENN
   Plank James S., 2013, P 11 US C FIL STOR T
   Plank JS, 1997, SOFTWARE PRACT EXPER, V27, P995, DOI 10.1002/(SICI)1097-024X(199709)27:9<995::AID-SPE111>3.0.CO;2-6
   Soro A., 2010, 7 IEEE CONS COMM NET
   Tamo I, 2011, IEEE INT SYMP INFO, P1240, DOI 10.1109/ISIT.2011.6033733
   Trifonov P., 2012, P IEEE INF THEOR WOR
   Trifonov P., 2007, P 11 INT S PROBL RED
   Trifonov PV, 2003, PROBL INFORM TRANSM+, V39, P231, DOI 10.1023/A:1026171930630
   Wu XB, 2012, IEEE T SIGNAL PROCES, V60, P1149, DOI 10.1109/TSP.2011.2178844
   Wu XB, 2011, IEEE T SIGNAL PROCES, V59, P2136, DOI 10.1109/TSP.2011.2106778
   Wu XB, 2011, IEEE WRK SIG PRO SYS, P1, DOI 10.1109/SiPS.2011.6088940
   Xu L., 1999, IEEE T INFORM THEORY, V45, P1
NR 35
TC 8
Z9 8
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD FEB
PY 2015
VL 11
IS 1
AR 1
DI 10.1145/2700308
PG 25
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CD8CR
UT WOS:000351323300001
DA 2024-07-18
ER

PT J
AU Sundararaman, S
   Zhang, YP
   Subramanian, S
   Arpaci-Dusseau, AC
   Arpaci-Dusseau, RH
AF Sundararaman, Swaminathan
   Zhang, Yupu
   Subramanian, Sriram
   Arpaci-Dusseau, Andrea C.
   Arpaci-Dusseau, Remzi H.
TI Making the Common Case the Only Case with Anticipatory Memory Allocation
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Reliability; Preallocation; static analysis; fault recovery;
   file systems
AB We present anticipatory memory allocation (AMA), a new method to build kernel code that is robust to memory-allocation failures. AMA avoids the usual difficulties in handling allocation failures through a novel combination of static and dynamic techniques. Specifically, a developer, with assistance from AMA static analysis tools, determines how much memory a particular call into a kernel subsystem will need, and then preallocates said amount immediately upon entry to the kernel; subsequent allocation requests are serviced from the preallocated pool and thus guaranteed never to fail. We describe the static and runtime components of AMA, and then present a thorough evaluation of Linux ext2-mfr, a case study in which we transform the Linux ext2 file system into a memory-failure robust version of itself. Experiments reveal that ext2-mfr avoids memory-allocation failures successfully while incurring little space or time overhead.
C1 [Sundararaman, Swaminathan; Zhang, Yupu; Subramanian, Sriram; Arpaci-Dusseau, Andrea C.; Arpaci-Dusseau, Remzi H.] Univ Wisconsin Madison, Dept Comp Sci, Madison, WI USA.
C3 University of Wisconsin System; University of Wisconsin Madison
RP Sundararaman, S (corresponding author), Univ Wisconsin Madison, Dept Comp Sci, Madison, WI USA.
EM swami@cs.wisc.edu; yupu@cs.wisc.edu; srirams@cs.wisc.edu;
   dusseau@cs.wisc.edu; remzi@cs.wisc.edu
FU National Science Foundation [CCF-0621487, CNS-0509474, CNS-0834392,
   CCF-0811697, CCF-0937959]; NetApp; Google
FX This material is based on work supported by the National Science
   Foundation under the following grants: CCF-0621487, CNS-0509474,
   CNS-0834392, CCF-0811697, CCF-0811697, CCF-0937959, as well as by
   generous donations from NetApp and Google.
CR ALBERT E., 2009, P INT S MEM MAN ISMM
   [Anonymous], 2006, P 13 ACM C COMP COMM
   [Anonymous], P 20 ACM S OP SYST P
   [Anonymous], P INT C DEP SYST NET
   [Anonymous], P ACM SIGPLAN C PROG
   [Anonymous], P 6 S OP SYST DES IM
   [Anonymous], 1987, Concurrency Control and Recovery in Database Systems
   Arpaci-Dusseau A. C., 2003, Operating Systems Review, V37, P90, DOI 10.1145/1165389.945455
   AUSTIN T. M., 2004, P ACM SIGPLAN 2005 C, P290
   BAIRAVASUNDARAM L. N., 2006, P INT C DEP SYST NET
   BONWICK J., 1994, P USENIX SUMM TECHN
   Bovet DanielP., 2006, UNDERSTANDING LINUX
   BRABERMAN V., 2008, P INT S MEM MAN ISMM
   Brown A., 2001, P 1 WORKSH EV ARCH S
   CHIN W.-N., 2005, P STAT AN S SAS 05
   CHIN W.-N., 2008, P INT S MEM MAN ISMM
   CHOU A., 2001, P 28 ANN INT S COMP
   Chou Andy, 2001, SOSP, P73, DOI 10.1145/502034.502042
   DHURJATI D., 2003, P ACM SIGPLAN C LANG
   Dijkstra E.W., 1977, EWD623 MATH BANKERS
   Engler D., 2001, Operating Systems Review, V35, P57, DOI 10.1145/502059.502041
   ENGLER D., 2004, P 5 INT C VER MOD CH
   Engler D.R., 1995, S OPERATING SYSTEMS, P251
   Garbervetsky Diego., 2009, JTRES 09 P 7 INT WOR, P140
   GUNAWI H. S., 2007, P 21 ACM S OP SYST P, P283
   Gunawi HS, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P207
   HALLEM S., 2002, P ACM SIGPLAN C PROG
   Hofmann M, 2006, LECT NOTES COMPUT SC, V3924, P22
   HOFMANN M., 2003, P 30 SIGPLAN SIGACT
   Kleiman S. R., 1986, USENIX Association Summer Conference Proceedings, Atlanta 1986, P238
   Lu S., 2008, P 13 INT C ARCH SUPP
   MORTON A., 2007, REPATCH JBD SLAB CLE
   MUSUVATHI M., 2002, P 5 S OP SYST DES IM
   Necula GC, 2002, LECT NOTES COMPUT SC, V2304, P213
   Prabhakaran Vijayan, 2005, P OF THE 20 ACM S ON, V39, P206
   Rinard M., 2004, P 6 S OP SYST DES IM
   RUBIO-GONZALEZ C., 2009, P ACM SIGPLAN C PROG
   SUNDARARAMAN S., 2010, P 8 USENIX S FIL STO
   TUGS, 2010, STACKANALYZER STACK
   TWOHEY P., 2006, P IEEE C SEC PRIV SP
   UNNIKRISHNAN L., 2009, P INT S MEM MAN ISMM
   Weiser M., 1981, 5th International Conference on Software Engineering, P439
   YANG J., 2004, P 6 S OP SYST DES IM
   Yang Junfeng, 2006, P 7 S OP SYST DES IM
NR 44
TC 2
Z9 2
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JAN
PY 2012
VL 7
IS 4
AR 13
DI 10.1145/2078861.2078863
PG 24
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 990LP
UT WOS:000307632700002
DA 2024-07-18
ER

PT J
AU Wang, R
   Li, YK
   Xu, YL
   Xie, H
   Lui, JCS
   He, SB
AF Wang, Rui
   Li, Yongkun
   Xu, Yinlong
   Xie, Hong
   Lui, John C. S.
   He, Shuibing
TI Toward Fast and Scalable Random Walks over Disk-Resident Graphs via
   Efficient I/O Management
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Graph processing system; random walk; graph storage
AB Traditional graph systems mainly use the iteration-based model, which iteratively loads graph blocks into memory for analysis so as to reduce random I/Os. However, this iteration-based model limits the efficiency and scalability of running random walk, which is a fundamental technique to analyze large graphs. In this article, we first propose a state-aware I/O model to improve the I/O efficiency of running random walk, then we develop a block-centric indexing and buffering scheme for managing walk data, and leverage an asynchronous walk updating strategy to improve random walk efficiency. We implement an I/O-efficient graph system, GraphWalker, which is efficient to handle very large disk-resident graphs and also scalable to run tens of billions of random walks with only a single commodity machine. Experiments show that GraphWalker can achieve more than an order of magnitude speedup when compared with DrunkardMob, which is tailored for random walks based on the classical graph system GraphChi, as well as two state-of-the-art single-machine graph systems, Graphene and GraFSoft. Furthermore, when compared with the most recent distributed system KnightKing, GraphWalker still achieves comparable performance with only a single machine, thereby making it a more cost-effective alternative.
C1 [Wang, Rui; Li, Yongkun] Univ Sci & Technol China, 96 Jinzhai Rd, Hefei, Peoples R China.
   [Xu, Yinlong] Univ Sci & Technol China, Anhui Prov Key Lab High Performance Comp, 96 Jinzhai Rd, Hefei, Peoples R China.
   [Xie, Hong] Chongqing Univ, 174 Shazhengjie, Chongqing, Peoples R China.
   [Lui, John C. S.] Chinese Univ Hong Kong, Shatin, Hong Kong, Peoples R China.
   [He, Shuibing] Zhejiang Univ, 38 Zheda Rd, Hangzhou, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS; Chongqing University; Chinese University of
   Hong Kong; Zhejiang University
RP Li, YK (corresponding author), Univ Sci & Technol China, 96 Jinzhai Rd, Hefei, Peoples R China.
EM rwang067@mail.ustc.edu.cn; ykli@ustc.edu.cn; ylxu@ustc.edu.cn;
   xiehong2018@cqu.edu.cn; cslui@cse.cuhk.edu.hk; heshuibing@zju.edu.cn
RI Wang, Rui/AEF-9301-2022
OI Wang, Rui/0000-0001-8915-4169; Lui, John C.S/0000-0001-7466-0384; Xu,
   Yinlong/0000-0001-9586-0561
FU National Key R&D Program of China [2018YFB1800203]; Youth Innovation
   Promotion Association CAS; GRF [14200420]; National Science Foundation
   of China [62172361]
FX The work of Y. Li was supported in part by the National Key R&D Program
   of China (2018YFB1800203), Youth Innovation Promotion Association CAS.
   The work of J. C. S. Lui was supported in part by the GRF (14200420).
   The work of S. He was supported in part by the National Science
   Foundation of China (62172361).
CR Ai LY, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P125
   ANLAB Traces, TWITTER
   [Anonymous], 2003, P WWW
   [Anonymous], 2015, P FAST
   [Anonymous], 2014, P OSDI
   [Anonymous], 2002, P 11 INT C WORLD WID, DOI DOI 10.1145/511446.511513
   [Anonymous], 2008, P 17 INT C WORLD WID, DOI DOI 10.1145/1367497.1367525
   Bar-Yossef Ziv., 2000, P VLDB
   Chen HZ, 2018, EUROSYS '18: PROCEEDINGS OF THE THIRTEENTH EUROSYS CONFERENCE, DOI 10.1145/3190508.3190545
   Chen Rong, 2015, P EUROSYS
   Chen W, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P199, DOI 10.1145/1557019.1557047
   Chul-Ho Lee, 2012, Performance Evaluation Review, V40, P319, DOI 10.1145/2318857.2254795
   Debnath S., 2008, P WWW
   Elyasi N, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P309
   Fogaras D, 2005, INTERNET MATH, V2, P333, DOI 10.1080/15427951.2005.10129104
   Friendster, HOM PAG
   Gonzalez Hector., 2007, P VLDB
   Gonzalez Joseph E., 2012, 10 USENIX S OP SYST, P17
   Graph500, HOM PAG
   Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754
   Henzinger MR, 1999, COMPUT NETW, V31, P1291, DOI 10.1016/S1389-1286(99)00016-X
   Hong S, 2012, ASPLOS XVII: SEVENTEENTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS, P349
   Hotho Andreas, 2006, P LWA
   Jamali M, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P397
   Jeh G., 2002, PROC 8 ACM SIGKDD IN, P538
   Jun SW, 2018, CONF PROC INT SYMP C, P411, DOI 10.1109/ISCA.2018.00042
   Ke Yang, 2021, SOSP '21: Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles CD-ROM, P311, DOI 10.1145/3477132.3483575
   Kempe David, 2003, P 9 ACM SIGKDD INT C, P137, DOI DOI 10.1145/956750.956769
   Khan A, 2018, PROCEEDINGS OF THE 2018 USENIX ANNUAL TECHNICAL CONFERENCE, P401
   Khorasani Farzad, 2014, P 23 INT S HIGH PERF, P239, DOI DOI 10.1145/2600212.2600227
   Kyrola Aapo., 2013, RECSYS, P257
   Kyrola Aapo, 2012, P OSDI
   Langville A., 2004, INTERNET MATH, V1, P335, DOI DOI 10.1080/15427951.2004.10129091
   Li RH, 2014, PROC INT CONF DATA, P736, DOI 10.1109/ICDE.2014.6816696
   Liu H, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P285
   Liu Hang, 2015, P SC
   Lochert C, 2003, IEEE IV2003: INTELLIGENT VEHICLES SYMPOSIUM, PROCEEDINGS, P156, DOI 10.1109/IVS.2003.1212901
   Low Y, 2012, PROC VLDB ENDOW, V5, P716, DOI 10.14778/2212351.2212354
   Maass S, 2017, PROCEEDINGS OF THE TWELFTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS (EUROSYS 2017), P527, DOI 10.1145/3064176.3064191
   Malewicz Grzegorz, 2010, P ACM SIGMOD INT C M, P135, DOI [DOI 10.1145/1807167.1807184, 10.1145/1807167.1807184]
   Nguyen D, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P456, DOI 10.1145/2517349.2522739
   Page L., 1999, PAGERANK CITATION RA, DOI DOI 10.1109/IISWC.2012.6402911
   Pan J-Y, 2004, P 10 ACM SIGKDD INT, P653, DOI DOI 10.1145/1014052.1014135
   Przulj N, 2004, BIOINFORMATICS, V20, P3508, DOI 10.1093/bioinformatics/bth436
   Przulj N, 2007, BIOINFORMATICS, V23, pE177, DOI 10.1093/bioinformatics/btl301
   Ribeiro B., 2010, P 10 ANN C INT MEAS
   Roy A, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P472, DOI 10.1145/2517349.2522740
   Rui Wang, 2019, International Journal of High Performance Computing and Networking, V14, P92
   RUSMEVICHIENTON.P, 2001, P AAAI FALL S US UNC
   Shun JL, 2013, ACM SIGPLAN NOTICES, V48, P135, DOI 10.1145/2517327.2442530
   Teixeira CHC, 2015, SOSP'15: PROCEEDINGS OF THE TWENTY-FIFTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P425, DOI 10.1145/2815400.2815410
   Tong HH, 2006, IEEE DATA MINING, P613
   Vora K, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P429
   Vora K, 2016, PROCEEDINGS OF USENIX ATC '16: 2016 USENIX ANNUAL TECHNICAL CONFERENCE, P507
   Wang YZH, 2016, ACM SIGPLAN NOTICES, V51, P123, DOI [10.1145/2851141.2851145, 10.1145/3016078.2851145]
   Web Data Commons, 2012 COMM CRAWL GRAP
   Xiaowei Zhu, 2015, P USENIX ATC
   Yahoo, YAH WEBSC PROGR
   Yang K, 2019, PROCEEDINGS OF THE TWENTY-SEVENTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '19), P524, DOI 10.1145/3341301.3359634
   Zhao PP, 2017, LECT NOTES COMPUT SC, V10178, P323, DOI 10.1007/978-3-319-55699-4_20
   Zhu XW, 2020, PROC VLDB ENDOW, V13, P1020, DOI 10.14778/3384345.3384351
   Zhu XW, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P301
NR 62
TC 0
Z9 0
U1 1
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2022
VL 18
IS 4
AR 36
DI 10.1145/3533579
PG 33
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7F8OU
UT WOS:000902100700009
DA 2024-07-18
ER

PT J
AU MCallister, S
   Berg, B
   Tutuncu-Macias, J
   Yang, JC
   Gunasekar, S
   Lu, J
   Berger, DS
   Beckmann, N
   Ganger, GR
AF MCallister, Sara
   Berg, Benjamin
   Tutuncu-Macias, Julian
   Yang, Juncheng
   Gunasekar, Sathya
   Lu, Jimmy
   Berger, Daniel S.
   Beckmann, Nathan
   Ganger, Gregory R.
TI Kangaroo: Theory and Practice of Caching Billions of Tiny Objects on
   Flash
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article; Proceedings Paper
CT ACM Symposium on Operating Systems Principles (SOSP)
CY OCT 26-29, 2021
CL ELECTR NETWORK
SP Assoc Comp Machinery
DE Flash; caching; tiny objects
ID PERFORMANCE
AB Many social-media and IoT services have very largeworking sets consisting of billions of tiny (similar to 100 B) objects. Large, flash-based caches are important to serving these working sets at acceptable monetary cost. However, caching tiny objects on flash is challenging for two reasons: (i) SSDs can read/write data only in multi-KB "pages" that are much larger than a single object, stressing the limited number of times flash can be written; and (ii) very few bits per cached object can be kept in DRAM without losing flash's cost advantage. Unfortunately, existing flash-cache designs fall short of addressing these challenges: write-optimized designs require too much DRAM, and DRAM-optimized designs require too many flash writes.
   We present Kangaroo, a new flash-cache design that optimizes both DRAM usage and flash writes to maximize cache performance while minimizing cost. Kangaroo combines a large, set-associative cache with a small, log-structured cache. The set-associative cache requires minimal DRAM, while the log-structured cache minimizes Kangaroo's flash writes. Experiments using traces from Meta and Twitter show that Kangaroo achieves DRAM usage close to the best prior DRAM-optimized design, flash writes close to the best prior write-optimized design, and miss ratios better than both. Kangaroo's design is Pareto-optimal across a range of allowed write rates, DRAM sizes, and flash sizes, reducing misses by 29% over the state of the art. These results are corroborated by analytical models presented herein and with a test deployment of Kangaroo in a production flash cache at Meta.
C1 [MCallister, Sara; Berg, Benjamin; Yang, Juncheng; Beckmann, Nathan; Ganger, Gregory R.] Carnegie Mellon Univ, 5000 Forbes Ave, Pittsburgh, PA 15232 USA.
   [Tutuncu-Macias, Julian] Goldman Sachs, 200 West St, New York, NY 10282 USA.
   [Gunasekar, Sathya; Lu, Jimmy] Meta, 1 Hacker Way, Menlo Pk, CA 94025 USA.
   [Berger, Daniel S.] Microsoft Res, Microsoft Bldg 99,14820 NE 36th St, Redmond, WA 98052 USA.
   [Berger, Daniel S.] Univ Washington, Seattle, WA 98195 USA.
C3 Carnegie Mellon University; Microsoft; University of Washington;
   University of Washington Seattle
RP MCallister, S (corresponding author), Carnegie Mellon Univ, 5000 Forbes Ave, Pittsburgh, PA 15232 USA.
RI ; Berger, Daniel/B-3756-2014
OI McAllister, Sara/0000-0001-5253-7094; Tutuncu-Macias,
   Julian/0000-0003-4286-7721; Lu, Jimmy/0000-0002-0560-9160; Beckmann,
   Nathan/0000-0001-6301-714X; Berger, Daniel/0000-0002-3911-1512; Berg,
   Benjamin/0000-0002-4147-6860
FU NDSEG Fellowship; Facebook Graduate Fellowships; Google Research Scholar
   Award; 2020 Google Faculty Award;  [NSF-CMMI-1938909]; 
   [NSF-CSR-1763701]
FX Sara McAllister is supported by an NDSEG Fellowship, Benjamin Berg and
   Juncheng Yang by Facebook Graduate Fellowships, and Nathan Beckmann by a
   Google Research Scholar Award. This work was supported by
   NSF-CMMI-1938909, NSF-CSR-1763701, and a 2020 Google Faculty Award.
CR Aho Alfred V., 1971, J. ACM
   [Anonymous], 2007, NEW YORK TIMES
   [Anonymous], 2016, The Varnish Book for Varnish 4.0
   [Anonymous], 2004, Linux journal
   [Anonymous], 2012, LevelDB
   [Anonymous], 2021, INVESTOR TWITTER MAY
   [Anonymous], Azure cache for redis
   [Anonymous], BIG DATA STAT GROWTH
   [Anonymous], 1990, Computer Architecture: A Quantitative Approach
   [Anonymous], Apache traffic server
   [Anonymous], REDIS FLASH
   [Anonymous], FATCACHE
   [Anonymous], 2020, INVESTOR FB APR
   [Anonymous], Rocksdb
   [Anonymous], AMAZON DYNAMODB
   Beckmann N, 2018, PROCEEDINGS OF THE 15TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION (NSDI'18), P389
   Beckmann N, 2015, INT S HIGH PERF COMP, P538, DOI 10.1109/HPCA.2015.7056061
   Beckmann N, 2015, INT S HIGH PERF COMP, P64, DOI 10.1109/HPCA.2015.7056022
   Bender MA, 2019, SPAA'19: PROCEEDINGS OF THE 31ST ACM SYMPOSIUM ON PARALLELISM IN ALGORITHMS AND ARCHITECTURESS, 2019, P265, DOI 10.1145/3323165.3323210
   Berg B, 2020, PROCEEDINGS OF THE 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '20), P769
   Berger DS, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P195
   Berger DS, 2017, PROCEEDINGS OF NSDI '17: 14TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P483
   Berger Daniel S., 2018, ACM SIGMETRICS
   Blankstein A, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P499
   Boboila Simona, 2010, USENIX FAST
   Bronson N., 2013, P 2013 USENIX C ANN, P49
   Chandramouli Badrish, 2018, VLDB
   Chang F, 2008, ACM T COMPUT SYST, V26, DOI 10.1145/1365815.1365816
   Chatzieleftheriou Andromachi, 2020, USENIX HotStorage
   Cidon A., 2017, USENIX ATC
   Cidon A, 2016, 13TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION (NSDI '16), P379
   Coffman EG, 1999, OPER RES LETT, V25, P109, DOI 10.1016/S0167-6377(99)00037-1
   Coffman Jr Edward G., 1973, Operating Systems Theory
   DAN A, 1990, PERF E R SI, V18, P143, DOI 10.1145/98460.98525
   Davis Gary, 2018, 2018 IEEE International Conference on Consumer Electronics (ICCE). Proceedings, DOI 10.1109/ICCE.2018.8326056
   Debnath Biplob, 2011, ACM SIGMOD
   Eisenman A, 2019, PROCEEDINGS OF THE 16TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P65
   Fan B., 2013, NSDI, P1
   Freiling Peter, personal communication
   Gallo Massimo, 2012, Performance Evaluation Review, V40, P395, DOI 10.1145/2318857.2254810
   Gartrell Alex., Mcdipper: A key-value cache for flash storage
   He J, 2017, PROCEEDINGS OF THE TWELFTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS (EUROSYS 2017), P127, DOI 10.1145/3064176.3064187
   Jaleel A, 2010, CONF PROC INT SYMP C, P60, DOI 10.1145/1815961.1815971
   KESSLER RE, 1994, IEEE T COMPUT, V43, P664, DOI 10.1109/12.286300
   Lee Changman, 2015, F2fs: A new file system for flash storage
   Li C, 2017, ACM T STORAGE, V13, DOI 10.1145/3094785
   Li CL, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P1875, DOI 10.1145/3178876.3186176
   Li CL, 2017, PROCEEDINGS OF THE 2017 SYMPOSIUM ON CLOUD COMPUTING (SOCC '17), P170, DOI 10.1145/3127479.3129255
   Lim H, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P1
   Lu LY, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P133
   Mammarella M, 2009, SOSP'09: PROCEEDINGS OF THE TWENTY-SECOND ACM SIGOPS SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P147
   Mukkara A, 2016, ACM SIGPLAN NOTICES, V51, P113, DOI 10.1145/2954679.2872363
   Netflix Technology Blog, 2016, Application data caching using ssds
   Netflix Technology Blog, 2018, Evolution of application data caching: From ram to ssd
   OToole James, 1994, USENIX OSDI
   Pedarsani R, 2016, IEEE ACM T NETWORK, V24, P836, DOI 10.1109/TNET.2015.2394482
   Perez S., 2021, Tech Crunch
   Raju P, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P497, DOI 10.1145/3132747.3132765
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   Rosensweig EJ, 2010, IEEE INFOCOM SER
   RuiWang Christopher Conrad, 2013, USENIX HotCloud
   Sen Rathijit, 2013, Performance Evaluation Review, V41, P279
   Shen ZY, 2018, ACM T STORAGE, V14, DOI 10.1145/3203410
   Shen Zhaoyan, 2016, USENIX HOTSTORAGE
   Song ZY, 2020, PROCEEDINGS OF THE 17TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P529
   Tang Chunqiang, 2020, USENIX OSDI
   Tang Linpeng, 2015, USENIX FAST
   Ugander J, 2011, Arxiv, DOI [arXiv:1111.4503, 10.48550/arxiv.1111.4503]
   Waldspurger CA, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P487
   Wu Xingbo, 2015, USENIX ATC
   Xu ST, 2016, PROC VLDB ENDOW, V10, P301
   Yang JC, 2021, ACM T STORAGE, V17, DOI 10.1145/3468521
   Yang JC, 2020, PROCEEDINGS OF THE 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '20), P191
   Yang JC, 2021, PROCEEDINGS OF THE 18TH USENIX SYMPOSIUM ON NETWORKED SYSTEM DESIGN AND IMPLEMENTATION, P503
   Yue Yao, 2020, Taming tail latency and achieving predictability
NR 75
TC 1
Z9 1
U1 1
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2022
VL 18
IS 3
AR 21
DI 10.1145/3542928
PG 33
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 4X9BK
UT WOS:000861131000003
OA Bronze
DA 2024-07-18
ER

PT J
AU Zhang, JC
   Cui, LX
   Li, P
   Liu, XG
   Wang, G
AF Zhang, Jiachen
   Cui, Lixiao
   Li, Peng
   Liu, Xiaoguang
   Wang, Gang
TI Toward Virtual Machine Image Management for Persistent Memory
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article; Proceedings Paper
CT 14th USENIX Symposium on Operating Systems Design and Implementation
   (OSDI)
CY NOV 04-06, 2020
CL ELECTR NETWORK
SP USENIX Assoc, Alibaba Grp, Amazon, ANT Grp, Alipay, Apple, ByteDance, Facebook, Google, Microsoft, VMware, Oracle, Two Sigma, ACM Queue, DMTF, FreeBSD Fdn
DE Persistent memory; memory virtualization
AB Persistent memory's (PM) byte-addressability and high capacity will also make it emerging for virtualized environment. Modern virtualmachine monitors virtualize PM using either I/O virtualization ormemory virtualization. However, I/O virtualization will sacrifice PM's byte-addressability, and memory virtualization does not get the chance of PM image management. In this article, we enhance QEMU's memory virtualization mechanism. The enhanced system can achieve both PM's byte-addressability inside virtual machines and PM image management outside the virtual machines. We also design pcow, a virtual machine image format for PM, which is compatible with our enhanced memory virtualization and supports storage virtualization features including thin-provisioning, base image, snapshot, and striping. Address translation is performed with the help of the Extended Page Table, thus much faster than image formats implemented in I/O virtualization. We also optimize pcow considering PM's characteristics. We perform exhaustive performance evaluations on an x86 server equipping with Intel's Optane DC persistent memory. The evaluation demonstrates that our scheme boosts the overall performance by up to 50x compared with qcow2, an image format implemented in I/O virtualization, and brings almost no performance overhead compared with the native memory virtualization. The striping feature can also scale-out the virtual PM's bandwidth performance.
C1 [Zhang, Jiachen; Cui, Lixiao; Li, Peng; Liu, Xiaoguang; Wang, Gang] Nankai Univ, Tianjin 300350, Peoples R China.
C3 Nankai University
RP Zhang, JC (corresponding author), Nankai Univ, Tianjin 300350, Peoples R China.
EM jczhang@nbjl.nankai.edu.cn; cuilx@nbjl.nankai.edu.cn;
   lipeng@nbjl.nankai.edu.cn; liuxg@nbjl.nankai.edu.cn;
   wgzwp@nbjl.nankai.edu.cn
RI wang, gang/ITT-0670-2023; Cui, Lixiao/ITT-8376-2023
OI Cui, Lixiao/0000-0002-4017-0974
FU National Science Foundation of China [61872201, 61702521, U1833114];
   Science and Technology Development Plan of Tianjin [18ZXZNGX00140,
   18ZXZNGX00200, 20JCZDJC00610]; Fundamental Research Funds for the
   Central Universities; TKLNDST
FX This work is partially supported by National Science Foundation of China
   (Grants No. 61872201, No. 61702521, No. U1833114); Science and
   Technology Development Plan of Tianjin (Grants No. 18ZXZNGX00140, No.
   18ZXZNGX00200, and No. 20JCZDJC00610); and the Fundamental Research
   Funds for the Central Universities and TKLNDST.
CR Apalkov D, 2013, ACM J EMERG TECH COM, V9, DOI 10.1145/2463585.2463589
   Axboe Jens., 2014, FLEX I O TEST
   Chen QS, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P241
   Gupta Pankaj., 2019, VIRTIO PMEM PATCHWOR
   Hwang D, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P187
   Intel, 2014, KIT KIT KIT
   Intel, 2017, BTT BLOCK TRANSL TAB
   Intel, 2018, PMEM RED
   Intel, 2018, US PERS MEM DEV LIN
   Intel, 2019, INT OPT 600 PERS MEM
   Joshi A, 2018, CONF PROC INT SYMP C, P452, DOI 10.1109/ISCA.2018.00045
   Kannan S, 2018, PROCEEDINGS OF THE 2018 USENIX ANNUAL TECHNICAL CONFERENCE, P993
   Kevin Wolf, 2017, QCOW2 IMAGE FORMAT
   Kivity Avi, 2019, MEMORY API QEMU DOCU
   Liang L, 2016, PROCEEDINGS OF THE SEVENTH ACM SYMPOSIUM ON CLOUD COMPUTING (SOCC 2016), P126, DOI 10.1145/2987550.2987551
   Lu YY, 2014, PR IEEE COMP DESIGN, P209
   McDougall Richard, 2010, Operating Systems Review, V44, P40, DOI 10.1145/1899928.1899933
   Merrifield T, 2016, ACM SIGPLAN NOTICES, V51, P25, DOI 10.1145/3007611.2892258
   Peng B, 2018, PROCEEDINGS OF THE 2018 USENIX ANNUAL TECHNICAL CONFERENCE, P665
   Pillai Thanumalayan Sankaranarayana, 2015, QUEUE, V13, P20
   Raoux S, 2008, IBM J RES DEV, V52, P465, DOI 10.1147/rd.524.0465
   Redis Labs, 2009, RED BENCHM
   Redis Labs, 2009, Redis
   Rudoff Andy, 2017, Login: The Usenix Magazine, V42, P34
   Russell Rusty, 2008, Operating Systems Review, V42, P95, DOI 10.1145/1400097.1400108
   SNIA NVM Programming Technical Working Group, 2017, NVM PROGR MOD VERS 1
   Song X, 2014, IEEE COMPUT ARCHIT L, V13, P61, DOI 10.1109/L-CA.2013.22
   Tang C., 2011, USENIX ANN TECHNICAL, P16
   VMware, 2018, PMEM PERS MEM NVDIMM
   VMware, 2011, VIRT DISK FORM 5 0
   Weiss Z., 2015, 13 USENIX C FIL STOR, P111
   Xu C, 2015, INT S HIGH PERF COMP, P476, DOI 10.1109/HPCA.2015.7056056
   Xu J, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P478, DOI 10.1145/3132747.3132761
   Xu JW, 2016, J SYST SOFTWARE, V121, P144, DOI 10.1016/j.jss.2016.02.021
   Zhang JC, 2019, IEEE S MASS STOR SYS, P116, DOI 10.1109/MSST.2019.00-11
   Zhang JC, 2018, INT C PAR DISTRIB SY, P51, DOI [10.1109/PADSW.2018.8644859, 10.1109/ICPADS.2018.00018]
   Zhang Y., 2015, IEEE 31 S MASS STORA, P1, DOI [DOI 10.1109/MSST.2015.7208275, 10.1109/MSST.2015.7208275, DOI 10.1007/S11042-015-2982-X]
   Zuo Pengfei, 2017, P 33 S MASS STOR SYS
NR 38
TC 0
Z9 0
U1 1
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2021
VL 17
IS 3
AR 20
DI 10.1145/3450976
PG 24
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA YY2GJ
UT WOS:000754611000005
DA 2024-07-18
ER

PT J
AU Harnik, D
   Hershcovitch, M
   Shatsky, Y
   Epstein, A
   Kat, R
AF Harnik, Danny
   Hershcovitch, Moshik
   Shatsky, Yosef
   Epstein, Amir
   Kat, Ronen
TI Sketching Volume Capacities in Deduplicated Storage
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Deduplication; capacity management; estimation
AB The adoption of deduplication in storage systems has introduced significant new challenges for storage management. Specifically, the physical capacities associated with volumes are no longer readily available. In this work, we introduce a new approach to analyzing capacities in deduplicated storage environments. We provide sketch-based estimations of fundamental capacity measures required for managing a storage system: How much physical space would be reclaimed if a volume or group of volumes were to be removed from a system (the reclaimable capacity) and how much of the physical space should be attributed to each of the volumes in the system (the attributed capacity). Our methods also support capacity queries for volume groups across multiple storage systems, e.g., how much capacity would a volume group consume after being migrated to another storage system? We provide analytical accuracy guarantees for our estimations as well as empirical evaluations. Our technology is integrated into a prominent all-flash storage array and exhibits high performance even for very large systems. We also demonstrate how this method opens the door for performing placement decisions at the data-center level and obtaining insights on deduplication in the field.
C1 [Harnik, Danny; Hershcovitch, Moshik; Kat, Ronen] IBM Res, IBM R&D Israel, Hashachar Tower,4 Ariel Sharon St, IL-5320047 Giv'atayim, Israel.
   [Shatsky, Yosef] IBM Syst, IBM R&D Israel, Hashachar Tower,4 Ariel Sharon St, IL-5320047 Giv'atayim, Israel.
   [Epstein, Amir] Citi Innovat Lab TLV, Tel Aviv, Israel.
   [Epstein, Amir] Citi Innovat Lab, Kiryat Atidim Bldg 8,3rd Floor, Tel Aviv, Israel.
RP Harnik, D (corresponding author), IBM Res, IBM R&D Israel, Hashachar Tower,4 Ariel Sharon St, IL-5320047 Giv'atayim, Israel.
EM dannyh@il.ibm.com; moshikh@il.ibm.com; yshatsky@gmail.com;
   amir.epstein@citi.com; ronenkat@il.ibm.com
CR [Anonymous], 2011, P 9 USENIX C FIL STO
   [Anonymous], 2018, INT DAT RED
   [Anonymous], 2018, 6 1 DAT RED DRR REP
   Aronovich L., 2009, P ACM INT SYST STOR, P1
   Aronovich L, 2012, PROCEEDINGS OF THE PRAGUE STRINGOLOGY CONFERENCE 2012, P3
   Bar-Yossef Z., 2002, Randomization and Approximation Techniques in Computer Science. 6th International Workshop, RANDOM 2002. Proceedings (Lecture Notes in Computer Science Vol.2483), P1
   Bhagwat D, 2009, 2009 IEEE INTERNATIONAL SYMPOSIUM ON MODELING, ANALYSIS & SIMULATION OF COMPUTER AND TELECOMMUNICATION SYSTEMS (MASCOTS), P237
   Deutsch P., 1996, P USENIX C FIL STOR
   Deutsch P., 1996, 1950 RFC NETW WORK G
   Douglis Fred, 2011, P 25 LARG INST SYST
   FLAJOLET P, 1985, J COMPUT SYST SCI, V31, P182, DOI 10.1016/0022-0000(85)90041-8
   Forman George, 2009, Operating Systems Review, V43, P84, DOI 10.1145/1496909.1496926
   Frey D., 2012, Proc. of the ACM Symposium on Cloud Computing (SoCC), P17
   Gibbons Phillip B., 2001, P 13 ANN ACM S PAR A, P281, DOI DOI 10.1145/378580.378687
   Greene W. A., 1993, Proceedings of the 31st Annual Southeast Conference, P127
   Harnik D, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P277
   Harnik Danny, 2013, P USENIX C FIL STOR
   HUFFMAN DA, 1952, P IRE, V40, P1098, DOI 10.1109/JRPROC.1952.273898
   Lillibridge M., 2009, P 7 USENIX C FIL STO
   Lu MH, 2012, ALGORITHMS, V5, P236, DOI 10.3390/a5020236
   Meyer D.T., 2011, Proceedings of the 9th USENIX conference on File and stroage technologies, P1
   Motwani Rajeev, 1995, RANDOMIZED ALGORITHM
   Nagesh P. C., 2013, P 6 INT SYST STOR C
   Shilane Philip, 2016, P 8 USENIX WORKSH HO
   Waldspurger C. A., 2015, 13 USENIX C FILE STO, P95
   Wires J, 2017, PROCEEDINGS OF THE 2017 SYMPOSIUM ON CLOUD COMPUTING (SOCC '17), P535, DOI 10.1145/3127479.3132021
   Xie Fei, 2013, 2013 USENIX ANN TECH, P181
   Yinjin Fu, 2012, Middleware 2012. ACM/IFIP/USENIX 13th International Middleware Conference. Proceedings, P354, DOI 10.1007/978-3-642-35170-9_18
   ZIV J, 1977, IEEE T INFORM THEORY, V23, P337, DOI 10.1109/TIT.1977.1055714
NR 29
TC 3
Z9 3
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD FEB
PY 2020
VL 15
IS 4
AR 24
DI 10.1145/3369737
PG 23
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NG7LP
UT WOS:000564162600003
DA 2024-07-18
ER

PT J
AU Kumar, P
   Huang, HH
AF Kumar, Pradeep
   Huang, H. Howie
TI GRAPHONE: A Data Store for Real-time Analytics on Evolving Graphs
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Graph systems; graph data management; unified graph data store; batch
   analytics; stream analytics
ID INTERNET; ALGORITHM
AB There is a growing need to perform a diverse set of real-time analytics (batch and stream analytics) on evolving graphs to deliver the values of big data to users. The key requirement from such applications is to have a data store to support their diverse data access efficiently, while concurrently ingesting fine-grained updates at a high velocity. Unfortunately, current graph systems, either graph databases or analytics engines, are not designed to achieve high performance for both operations; rather, they excel in one area that keeps a private data store in a specialized way to favor their operations only. To address this challenge, we have designed and developed GRAPHONE, a graph data store that abstracts the graph data store away from the specialized systems to solve the fundamental research problems associated with the data store design. It combines two complementary graph storage formats (edge list and adjacency list) and uses dual versioning to decouple graph computations from updates. Importantly, it presents a new data abstraction, GraphView, to enable data access at two different granularities of data ingestions (called data visibility) for concurrent execution of diverse classes of real-time graph analytics with only a small data duplication. Experimental results show that GRAPHONE is able to deliver 11.40x and 5.36x average speedup in ingestion rate against LLAMA and Stinger, the two state-of-the-art dynamic graph systems, respectively. Further, they achieve an average speedup of 8.75x and 4.14x against LLAMA and 12.80x and 3.18x against Stinger for BFS and PageRank analytics (batch version), respectively. GRAPHONE also gains over 2,000x speedup against Kickstarter, a state-of-the-art stream analytics engine in ingesting the streaming edges and performing streaming BPS when treating first half as a base snapshot and rest as streaming edge in a synthetic graph. GRAPHONE also achieves an ingestion rate of two to three orders of magnitude higher than graph databases. Finally, we demonstrate that it is possible to run concurrent stream analytics from the same data store.
C1 [Kumar, Pradeep] William & Mary, Dept Comp Sci, 251 Jamestown Rd, Williamsburg, VA 23185 USA.
   [Huang, H. Howie] George Washington Univ, Dept Elect & Comp Engn, 800 22nd St NW, Washington, DC 20052 USA.
C3 George Washington University
RP Kumar, P (corresponding author), William & Mary, Dept Comp Sci, 251 Jamestown Rd, Williamsburg, VA 23185 USA.
EM pkumar@wm.edu; howie@gwu.edu
OI Kumar, Pradeep/0000-0001-9314-4772
FU National Science Foundation CAREER award [1350766, 1618706, 1717774];
   Division Of Computer and Network Systems; Direct For Computer & Info
   Scie & Enginr [1717774, 1350766] Funding Source: National Science
   Foundation
FX This work was supported in part by National Science Foundation CAREER
   award 1350766 and grants 1618706 and 1717774. Authors' addresses: P.
   Kumar, Department of Computer Science, William and Mary, 251 Jamestown
   Road, Williamsburg, VA, 23185; email: pkumar@wm.edu;H.H.Huang,
   Department of Electrical and Computer Engineering, GeorgeWashington
   University, 800 22nd Street NW, Washington, DC, 20052; email:
   howie@gwu.edu.
CR Abadi D.J., 2005, CIDR, P277
   Abadi DJ, 2003, VLDB J, V12, P120, DOI 10.1007/s00778-003-0095-z
   Ai LY, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P125
   Akoglu L, 2015, DATA MIN KNOWL DISC, V29, P626, DOI 10.1007/s10618-014-0365-y
   Albert R, 1999, NATURE, V401, P130, DOI 10.1038/43601
   Anicic Darko., 2011, Proceedings of the 20th international conference on World wide web, P635
   [Anonymous], 2016, P 4 INT WORKSH GRAPH
   [Anonymous], 2008, SOCIAL NETWORK ANAL
   Beamer S, 2012, INT CONF HIGH PERFOR
   Bhattarai Bibek, 2019, P 2019 INT C MAN DAT
   Brandes U, 2001, J MATH SOCIOL, V25, P163, DOI 10.1080/0022250X.2001.9990249
   Bronson N., 2013, P 2013 USENIX C ANN, P49
   Chakrabarti D., 2004, P SIAM INT C DAT MIN
   Cheag Raymond, 2012, P 7 ACM EUR C COMP S
   Chen Rong, 2015, EUROSYS, P1
   Choudhury Sutanay, 2015, P 18 INT C EXT DAT T
   Cipar J., 2012, Pro- ceedings of the 7th ACM European Conference on Computer Systems, EuroSys '12, P169, DOI [10.1145/2168836.2168854, DOI 10.1145/2168836.2168854]
   Dat Dong, 2017, P 26 INT S HIGH PERF, P219
   Dhulipala L, 2019, PROCEEDINGS OF THE 40TH ACM SIGPLAN CONFERENCE ON PROGRAMMING LANGUAGE DESIGN AND IMPLEMENTATION (PLDI '19), P918, DOI 10.1145/3314221.3314598
   Easley D., 2010, Networks, Crowds, and Markets: Reasoning about a highly connected world, V8
   Ediger D, 2012, IEEE HIGH PERF EXTR
   Faloutsos M, 1999, COMP COMM R, V29, P251, DOI 10.1145/316194.316229
   Hang Liu, 2017, P 15 USENIX C FIL ST
   Hong Sungpack., 2012, ACM SIGARCH Computer Architecture News
   Hu Y, 2017, IEEE HIGH PERF EXTR
   Hu Yang., 2018, Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis, P14
   Huberman BA, 1999, NATURE, V401, P131, DOI 10.1038/43604
   Ilan Wentao, 2014, P EUR C COMP SYST EU
   Jeong H, 2000, NATURE, V407, P651, DOI 10.1038/35036627
   Ju XE, 2016, PROCEEDINGS OF USENIX ATC '16: 2016 USENIX ANNUAL TECHNICAL CONFERENCE, P523
   Kang U., 2011, P KDD
   Kent, 2015, COMPREHENSIVE MULTIS, DOI DOI 10.17021/1179829
   Kent AD, 2015, COMPUT SECUR, V48, P150, DOI 10.1016/j.cose.2014.09.001
   Khurana U, 2013, PROC INT CONF DATA, P997, DOI 10.1109/ICDE.2013.6544892
   Kim K, 2016, SIGMOD'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P1675, DOI 10.1145/2882903.2882905
   Kreps Jay, 2011, P NETDB, P1, DOI DOI 10.1007/BF00640482
   Kumar P, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P41
   Kumar P, 2017, IEEE INT CONGR BIG, P65, DOI 10.1109/BigDataCongress.2017.18
   Kumar Pradeep, 2019, P 17 USENIX C FIL ST
   Kumar Pradeep, 2016, P INT C HIGH PERF CO
   Kwak Haewoon, 2010, P C WORLD WID WEB WW
   Kyrola A., 2012, P USENIX S OP SYST D
   Lakhotia K, 2019, PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM '19), P339, DOI 10.1145/3357384.3358063
   Leskovec J., 2005, P 11 ACM SIGKDD INT, P177
   Lin H, 2018, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE, AND ANALYSIS (SC'18)
   Liu H, 2015, PROCEEDINGS OF SC15: THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS, DOI 10.1145/2807591.2807594
   Liu Hang, 2016, P SIGMOD INT C MAN D
   Liu Ji flang, 2018, P INT C HIGH PERF CO
   Low Y, 2012, PROC VLDB ENDOW, V5, P716, DOI 10.14778/2212351.2212354
   Maass Steffen, 2017, P 12 EUR C COMP SYST
   Macko P, 2015, PROC INT CONF DATA, P363, DOI 10.1109/ICDE.2015.7113298
   Malicevic J, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P631
   Mariappan Mugilan, 2019, P EUROPEAN C COMPUTE, P25
   McColl R. C., 2014, P 1 WORKSH PAR PROGR, P11
   McSherry Frank, 2018, P C INN DAT SYST RES
   McSherry Frank, 2015, HOTOS
   Murray Derek G., 2013, S OP SYST PRINC SOSP
   Nguyen Donald, 2013, P ACM S OP SYST PRIN
   Page L., 1999, PAGERANK CITATION RA
   Reinders James, 2007, Intel threading building blocks-outfitting C++ for multi-core processor parallelism
   Ren CH, 2011, PROC VLDB ENDOW, V4, P726
   Robinson I., 2013, Graph databases
   Romero Daniel M, 2011, P 20 INT C WORLD WID, P695, DOI [10.1145/1963405.1963503, DOI 10.1145/1963405.1963503]
   Roy A., 2015, P ACM S OP SYST PRIN
   Roy A., 2013, P ACM S OP SYST PRIN
   Sallinen Scott., 2016, Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, P30
   Seo J, 2013, PROC VLDB ENDOW, V6, P1906, DOI 10.14778/2556549.2556572
   Sevenich M, 2016, PROC VLDB ENDOW, V9, P1257, DOI 10.14778/3007263.3007265
   Sheng Feng, 2013, P ACM S CLOUD COMP S
   Shi XG, 2016, SIGMOD'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P417, DOI 10.1145/2882903.2882950
   Shun Julian, 2013, P 18 ACM SIGPLAN S P
   Slim Bin, 2013, P SIGMOD INT C MAN D
   Then M, 2017, PROC VLDB ENDOW, V10, P877, DOI 10.14778/3090163.3090166
   Turcotte MM., 2018, Data Sci Cyber Secur, V16, P4, DOI DOI 10.1142/9781786345646001
   Vora K, 2017, TWENTY-SECOND INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXII), P237, DOI 10.1145/3037697.3037748
   Wang G., 2013, CIDR, V13, P3
   Wu Ming, 2015, P 6 ACM S CLOUD COMP
   Xie WL, 2015, PROC INT CONF DATA, P1143, DOI 10.1109/ICDE.2015.7113363
   Yan D, 2018, IEEE T PARALL DISTR, V29, P99, DOI 10.1109/TPDS.2017.2743708
   Yu HWani0, 2013, P 19 ACM SIGKDD INT
   Zeng Kai, 2013, P 39 INT C VER LARG
   Zhang Da, 2015, P 13 USENIX C FIL ST
   Zhang KY, 2015, ACM SIGPLAN NOTICES, V50, P183, DOI [10.1145/2688500.2688507, 10.1145/2858788.2688507]
   Zhang W, 2018, 2018 18TH IEEE/ACM INTERNATIONAL SYMPOSIUM ON CLUSTER, CLOUD AND GRID COMPUTING (CCGRID), P183, DOI 10.1109/CCGRID.2018.00033
   Zhang YH, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P614, DOI 10.1145/3132747.3132777
   Zhou LH, 2016, 2016 3RD INTERNATIONAL CONFERENCE ON INFORMATION AND COMMUNICATION TECHNOLOGY FOR EDUCATION (ICTE 2016), P301
   Zhu X., 2015, 2015 USENIX ANN TECH, P375
NR 87
TC 26
Z9 30
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD FEB
PY 2020
VL 15
IS 4
AR 29
DI 10.1145/3364180
PG 40
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NG7LP
UT WOS:000564162600008
OA Bronze
DA 2024-07-18
ER

PT J
AU Alagappan, R
   Ganesan, A
   Lee, E
   Albarghouthi, A
   Chidambaram, V
   Arpaci-Dusseau, AC
   Arpaci-Dusseau, RH
AF Alagappan, Ramnatthan
   Ganesan, Aishwarya
   Lee, Eric
   Albarghouthi, Aws
   Chidambaram, Vijay
   Arpaci-Dusseau, Andrea C.
   Arpaci-Dusseau, Remzi H.
TI Protocol-Aware Recovery for Consensus-Based Distributed Storage
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article; Proceedings Paper
CT 16th USENIX Conference on File and Storage Technologies (FAST)
CY FEB 12-15, 2018
CL Oakland, CA
SP USENIX, ACM SIGOPS
DE Storage faults; data corruption; fault tolerance; consensus
ID GRAPEVINE
AB We introduce protocol-aware recovery (PAR), a new approach that exploits protocol-specific knowledge to correctly recover from storage faults in distributed systems. We demonstrate the efficacy of PAR through the design and implementation of corruption-tolerant replication (CTRL), a Par mechanism specific to replicated state machine (RSM) systems. We experimentally show that the CTRL versions of two systems, LogCabin and ZooKeeper, safely recover from storage faults and provide high availability, while the unmodified versions can lose data or become unavailable. We also show that the CTRL versions achieve this reliability with little performance overheads.
C1 [Alagappan, Ramnatthan; Ganesan, Aishwarya; Albarghouthi, Aws; Arpaci-Dusseau, Andrea C.; Arpaci-Dusseau, Remzi H.] Univ Wisconsin, 1210 W Dayton St, Madison, WI 53706 USA.
   [Lee, Eric; Chidambaram, Vijay] Univ Texas Austin, 2317 Speedway, Austin, TX 78712 USA.
C3 University of Wisconsin System; University of Wisconsin Madison;
   University of Texas System; University of Texas Austin
RP Alagappan, R (corresponding author), Univ Wisconsin, 1210 W Dayton St, Madison, WI 53706 USA.
EM ra@cs.wisc.edu; ag@cs.wisc.edu; ericlee123@utexas.edu; aws@cs.wisc.edu;
   vijay@cs.utexas.edu; dusseau@cs.wisc.edu; remzi@cs.wisc.edu
RI Chidambaram, Vijay/HJA-2695-2022
OI Alagappan, Ramnatthan/0000-0001-9911-4208; Chidambaram,
   Vijay/0000-0001-7985-6087; Arpaci-Dusseau, Andrea/0000-0001-8618-2738
FU NSF [CNS-1421033, CNS-1218405]; DOE [DE-SC0014935]; U.S. Department of
   Energy (DOE) [DE-SC0014935] Funding Source: U.S. Department of Energy
   (DOE)
FX This material was supported by funding from NSF grants CNS-1421033 and
   CNS-1218405, DOE grant DE-SC0014935, and donations from EMC, Huawei,
   Microsoft, and VMware. Any opinions, findings, and conclusions or
   recommendations expressed in this material are those of the authors and
   may not reflect the views of NSF, DOE, or other institutions. This
   article is an extended version of a FAST'18 paper by Alagappan et al.
   [7]. The additional material here includes a discussion on how Par can
   be applied to other systems, a proof of why crashes and corruptions
   cannot be always disentangled, an overview diagram that summarizes the
   entire recovery protocol, new performance experiments, new figures
   explaining leader-initiated snapshots, and many other small updates.
CR Abraham I, 2006, DISTRIB COMPUT, V18, P387, DOI 10.1007/s00446-005-0151-6
   ALAGAPPAN R., 2016, P 12 USENIX C OP SYS
   Alagappan R, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P15
   [Anonymous], P INT C DEP SYST NET
   [Anonymous], 2006, P ACM S OP SYST PRIN
   [Anonymous], THESIS
   [Anonymous], P 15 ACM S OP SYST P
   [Anonymous], 2016, P 14 USENIX C FIL ST
   Apache, 2008, ZOOKEEPER
   Apache, 2017, KAKF
   Apache, 2008, ZOOKEEPER GUAR PROP
   Apache Cassandra, 2017, CASS REPL
   Apache ZooKeeper, 2014, APPL ORG US ZOOKEEPE
   Arpaci-Dusseau R., 2015, OPERATING SYSTEMS 3
   Bairavasundaram L. N., 2007, P 2007 ACM SIGMETRIC
   Bairavasundaram Lakshmi N., 2008, P 6 USENIX S FIL STO
   Balakrishnan Mahesh, 2012, P 9 S NETW SYST DES
   BIRRELL AD, 1982, COMMUN ACM, V25, P260, DOI 10.1145/358468.358487
   Bolosky William J., 2011, P 8 S NETW SYST DES
   Caulfield A. M., 2009, P 42 ANN IEEE ACM IN, P24
   Chandra T, 2007, PODC'07: PROCEEDINGS OF THE 26TH ANNUAL ACM SYMPOSIUM ON PRINCIPLES OF DISTRIBUTED COMPUTING, P398
   Chidambaram V, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P228, DOI 10.1145/2517349.2522726
   Chidambaram Vijay, 2012, P 10 USENIX S FIL ST
   Clement Allen, 2009, P 22 ACM S OP SYST P
   Corbet Jonathan, 2008, RESPONDING EXT4 J CO
   Correia Miguel, 2012, 2012 USENIX ANN TECH
   Dean Jeff, 2010, BUILDING LARGE SCALE
   DeCandia Giuseppe, 2007, Operating Systems Review, V41, P205, DOI 10.1145/1323293.1294281
   Do Thanh, 2013, P 11 C FIL STOR TECH
   epaxos, 2012, EP SOURC COD
   etcd, 2014, ETCD PROD US
   Fryer Daniel, 2014, P 12 USENIX S FIL ST
   Fryer Daniel, 2012, P 10 USENIX S FIL ST
   Ganesan A, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P149
   Ganesan A, 2017, ACM T STORAGE, V13, DOI 10.1145/3125497
   Ghemawat S., 2003, ACM SIGOPS OPERATING, V37, P29, DOI [DOI 10.1145/945445.945450, DOI 10.1145/1165389.945450, 10.1145/1165389.945450]
   Goerzen John, 2017, SILENT DATA CORRUPTI
   Grawinkel Matthias, 2011, P 19 ANN M IEEE INT
   Greenan Kevin M., 2009, P 5 WORKSH HOT TOP S
   Hamilton James, 2007, P 21 ANN LARG INST S
   Harris Robert, 2007, DATA CORRUPTION IS W
   Junqueira FP, 2011, I C DEPEND SYS NETWO, P245, DOI 10.1109/DSN.2011.5958223
   Kuvaiskii Dmitrii, 2016, P EUROSYS C EUROSYS
   Lamport L., 2001, SIGACT News, V32, P51
   Liu S., 2016, P 12 USENIX C OP SYS
   Lorch Jacob R, 2006, P EUROSYS C EUROSYS
   Marandi Parisa Jalili, 2016, 2016 USENIX ANN TECH
   Meza Justin, 2015, ACM SIGMETRICS Performance Evaluation Review, V43, P177, DOI 10.1145/2745844.2745848
   MongoDB, 2017, MONGODB REPL
   Moraru I, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P358, DOI 10.1145/2517349.2517350
   Myers James., 2014, DATA INTEGRITY SOLID
   Narayanan I, 2016, PROCEEDINGS OF THE 9TH ACM INTERNATIONAL SYSTEMS AND STORAGE CONFERENCE (SYSTOR'16), DOI [10.1145/2964791.2901489, 10.1145/2928275.2928278]
   Ongaro D., 2014, THESIS
   Ongaro Diego, 2014, 2014 USENIX ANN TECH, DOI DOI 10.5555/2643634.2643666
   Ongaro Diego, 2014, RAFT TLA SPECIFICATI
   Panzer-Steindel B., 2007, DATA INTEGRITY
   Pillai Thanumalayan Sankaranarayana, 2017, P 15 USENIX C FIL ST
   Pillai Thanumalayan Sankaranarayana, 2014, P 11 USENIX S OP SYS, P433
   Prabhakaran Vijayan, 2005, P 20 ACM S OP SYST P
   Redis, 2015, RED REPL
   Ricci Robert, 2014, USENIX;login, V39, P6
   SCHNEIDER FB, 1990, COMPUT SURV, V22, P299, DOI 10.1145/98163.98167
   Schroeder Bianca, 2010, P 8 USENIX S FIL STO
   SCHROEDER MD, 1984, ACM T COMPUT SYST, V2, P3, DOI 10.1145/2080.2081
   Schwarz Thomas, 2016, P 24 ANN M IEEE INT
   Slootmaekers Romain, 2012, SIGPLAN OCAML US DEV, V62
   Stackoverflow, 2015, CAN EXT4 DET CORR FI
   Stackoverflow, 2013, ZOOKEEPER CLEAR STAT
   Swift Michael M., 2003, P 19 ACM S OP SYST P
   Tso Theodore, 2008, WHAT DO J CHECKSUM I
   van Renesse R, 2015, IEEE T DEPEND SECURE, V12, P472, DOI 10.1109/TDSC.2014.2355848
   Wang Yang, 2013, P 10 S NETW SYST DES
   Yupu Zhang, 2010, P 8 USENIX S FIL STO
   ZooKeeper Jira Issues, 2012, UN LOAD DAT DISK RES
NR 74
TC 3
Z9 4
U1 3
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2018
VL 14
IS 3
AR 21
DI 10.1145/3241062
PG 30
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA HJ4JQ
UT WOS:000457140700002
OA Bronze
DA 2024-07-18
ER

PT J
AU Zhou, Y
   Wu, F
   Huang, P
   He, XB
   Xie, CS
   Zhou, J
AF Zhou, You
   Wu, Fei
   Huang, Ping
   He, Xubin
   Xie, Changsheng
   Zhou, Jian
TI Understanding and Alleviating the Impact of the Flash Address
   Translation on Solid State Devices
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE SSD; NAND flash memory; FTL; address translation; Design; Management;
   Performance
AB Flash-based solid state devices (SSDs) have been widely employed in consumer and enterprise storage systems. However, the increasing SSD capacity imposes great pressure on performing efficient logical to physical address translation in a page-level flash translation layer (FTL). Existing schemes usually employ a built-inRAMto storemapping information, called mapping cache, to speed up the address translation. Since only a fraction of the mapping table can be cached due to limited cache space, a large number of extra flash accesses are required for cache management and garbage collection, degrading the performance and lifetime of an SSD. In this paper, we first apply analytical models to investigate the key factors that incur extra flash accesses during address translation. Then, we propose a novel page-level FTL with an efficient translation page-level caching mechanism, named TPFTL, to minimize the extra flash accesses. TPFTL employs a twolevel least recently used (LRU) list with space-efficient optimizations to organize cached mapping entries. Inspired by the models, we further design a workload-adaptive loading policy combined with an efficient replacement policy to increase the cache hit rate and reduce the writebacks of replaced dirty entries. Finally, we evaluate TPFTL using extensive trace-driven simulations. Our evaluation results show that compared to the state-of-the-art FTLs, TPFTL significantly reduces the extra operations caused by address translation, achieving reductions on system response time and write amplification by up to 27.1% and 32.2%, respectively.
C1 [Zhou, You; Wu, Fei; Xie, Changsheng; Zhou, Jian] Huazhong Univ Sci & Technol, Wuhan Natl Lab Optoelect, 1037 Luoyu Rd, Wuhan 430074, Peoples R China.
   [Huang, Ping; He, Xubin] Temple Univ, Dept Comp & Informat Sci, 1925 N 12th St, Philadelphia, PA 19122 USA.
   [He, Xubin] Virginia Commonwealth Univ, Richmond, VA 23284 USA.
C3 Huazhong University of Science & Technology; Pennsylvania Commonwealth
   System of Higher Education (PCSHE); Temple University; Virginia
   Commonwealth University
RP Wu, F (corresponding author), Huazhong Univ Sci & Technol, Wuhan Natl Lab Optoelect, 1037 Luoyu Rd, Wuhan 430074, Peoples R China.
EM zhouyou@hust.edu.cn; wufei@hust.edu.cn; templestorager@temple.edu;
   xubin.he@temple.edu; cs_xie@hust.edu.cn; jzhou.research@gmail.com
FU National Basic Research Program of China (973 Program) [2011CB302303];
   National Natural Science Foundation of China [61300047, 61370063,
   61472152, 61572209]; Key Laboratory of Data Storage System, Ministry of
   Education of China; U.S. National Science Foundation [CNS-1320349,
   CNS-1218960]; Division Of Computer and Network Systems; Direct For
   Computer & Info Scie & Enginr [1700719] Funding Source: National Science
   Foundation
FX This work was sponsored in part by the National Basic Research Program
   of China (973 Program) under Grant No. 2011CB302303 and the National
   Natural Science Foundation of China No. 61300047, No. 61370063, No.
   61472152, and No. 61572209. This work is also supported by Key
   Laboratory of Data Storage System, Ministry of Education of China. The
   work conducted at VCU/Temple was partially sponsored by U.S. National
   Science Foundation under Grants CNS-1320349 and CNS-1218960.
CR Agrawal Nitin, 2008, 2008 USENIX ANN TECH
   [Anonymous], 2012, P 10 USENIX C FIL ST
   [Anonymous], 2011, PROC 9 USENIX C FILE
   [Anonymous], P 6 USENIX C FIL STO
   Arteaga D., 2014, P ACM INT C SYST STO
   Bates Ken, 2013, TRACES UMASS TRACE R
   Chung Tae-Sun, 2006, P SPRING INT C EMB U
   Debnath Biplob, 2011, P MASS STOR SYST TEC
   Desnoyers Peter, 2012, P 5 ACM ANN INT SYST
   Gal E, 2005, ACM COMPUT SURV, V37, P138, DOI 10.1145/1089733.1089735
   Goodson G., 2010, P WORKSH US EM STOR
   Gray J., 2008, ACM Queue, V6, P18, DOI 10.1145/1413254.1413261
   GUPTA A., 2009, P INT C ARCH SUPP PR
   Hu Xiaoyu, 2009, P SYSTOR ISR EXP SYS
   Hu Yang, 2010, P MASS STOR SYST TEC
   Jagmohan Ashish, 2010, P MASS STOR SYST TEC
   Jiang S., 2011, P MASS STOR SYST TEC
   Jung Myoungsoo, 2014, P 20 IEEE INT S HIGH
   Ko Sohyang, 2008, P ADV SOFTW ENG ITS
   Lee SW, 2007, ACM T EMBED COMPUT S, V6, DOI 10.1145/1275986.1275990
   Li  C., 2014, P 6 USENIX WORKSH HO
   Lu Youyou, 2013, P 11 USENIX C FIL ST
   Ma Dongzhe, 2011, P 2011 ACM SIGMOD IN
   Megiddo N., 2003, P 2 USENIX C FIL STO
   Microsoft, 2013, MSR CAMBR TRAC
   Moon Sangwhan, 2012, P MASS STOR SYST TEC
   Park D., 2010, ACM SIGMETRICS PERFO
   Ping Huang, 2014, P 9 EUR C COMP SYST
   Qin Z. W., 2011, P 17 IEEE REAL TIM E
   Samsung, 2014, SSD 840 EVO 2 5 SATA
   Sungjin Lee, 2008, Operating Systems Review, V42, P36, DOI 10.1145/1453775.1453783
   Tarasov Vasily, 2013, P USENIX C FIL STOR
   Vatto Kristian, 2013, LSI ANNOUNCES SANDFO
   Wang Chundong, 2013, P C DES AUT TEST EUR
   Wang Mingbang, 2011, P 14 IEEE INT C COMM
   Wei Qingsong, 2011, P MASS STOR SYST TEC
   Wu Guanying, 2010, P MASS STOR SYST TEC
   Xia Qianbin, 2015, P 23 IEEE INT S MOD
   Xu Zhiyong, 2012, P 31 IEEE INT PERF C
   Yang Yue, 2014, P MASS STOR SYST TEC
   You Zhou, 2015, P 10 ACM EUR C COMP
NR 41
TC 8
Z9 8
U1 0
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUN
PY 2017
VL 13
IS 2
SI SI
AR 14
DI 10.1145/3051123
PG 29
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FA1TA
UT WOS:000405221600005
DA 2024-07-18
ER

PT J
AU Li, MQ
   Lee, PPC
AF Li, Mingqiang
   Lee, Patrick P. C.
TI STAIR Codes: A General Family of Erasure Codes for Tolerating Device and
   Sector Failures
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Algorithms; Performance; Reliability; Theory; Erasure codes; device
   failures; sector failures; reliability analysis
ID MDS ARRAY CODES; RAID; TUTORIAL
AB Practical storage systems often adopt erasure codes to tolerate device failures and sector failures, both of which are prevalent in the field. However, traditional erasure codes employ device-level redundancy to protect against sector failures, and hence incur significant space overhead. Recent sector-disk (SD) codes are available only for limited configurations. By making a relaxed but practical assumption, we construct a general family of erasure codes called STAIR codes, which efficiently and provably tolerate both device and sector failures without any restriction on the size of a storage array and the numbers of tolerable device failures and sector failures. We propose the upstairs encoding and downstairs encoding methods, which provide complementary performance advantages for different configurations. We conduct extensive experiments on STAIR codes in terms of space saving, encoding/decoding speed, and update cost. We demonstrate that STAIR codes not only improve space efficiency over traditional erasure codes, but also provide better computational efficiency than SD codes based on our special code construction. Finally, we present analytical models that characterize the reliability of STAIR codes, and show that the support of a wider range of configurations by STAIR codes is critical for tolerating sector failure bursts discovered in the field.
C1 [Li, Mingqiang; Lee, Patrick P. C.] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.
C3 Chinese University of Hong Kong
RP Lee, PPC (corresponding author), Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.
EM mingqiangli.cn@gmail.com; pclee@cse.cuhk.edu.hk
RI Lee, Patrick P. C./I-7165-2013
OI Lee, Patrick P. C./0000-0002-4501-4364
FU University Grants Committee of Hong Kong [AoE/E-02/08, ECS CUHK419212]
FX This work was supported in part by grants from the University Grants
   Committee of Hong Kong (project numbers AoE/E-02/08 and ECS CUHK419212).
CR [Anonymous], 2010, P 8 USENIX S FIL STO
   [Anonymous], 2013, 11 USENIX C FIL STOR
   Bairavasundaram LN, 2007, PERF E R SI, V35, P289
   BLAUM M, 1995, IEEE T COMPUT, V44, P192, DOI 10.1109/12.364531
   Blaum M, 1996, IEEE T INFORM THEORY, V42, P529, DOI 10.1109/18.485722
   Blaum M., 2013, RJ10511ALM1308007 IB
   Blaum M., 2012, U.S. Patent, Patent No. [13/036,845, 13036845]
   Blaum M, 2006, 2006 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY, VOLS 1-6, PROCEEDINGS, P2784, DOI 10.1109/ISIT.2006.261569
   Blaum M, 2013, IEEE T INFORM THEORY, V59, P4510, DOI 10.1109/TIT.2013.2252395
   Blomer J., 1995, TR95048 U CAL INT CO
   Boboila Simona, 2010, P 8 USENIX C FIL STO, P115
   Caulfield A. M., 2009, P 42 ANN IEEE ACM IN, P24
   Corbett P, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P1
   Dholakia Ajay, 2008, ACM Transaction on Storage, V4, DOI 10.1145/1353452.1353453
   Dholakia A., 2011, ACM T STORAGE, V7, P1
   ELIAS P, 1954, IRE T INFORM THEOR, P29, DOI 10.1109/TIT.1954.1057464
   Feng GL, 2005, IEEE T COMPUT, V54, P1473, DOI 10.1109/TC.2005.200
   Feng GL, 2005, IEEE T COMPUT, V54, P1071, DOI 10.1109/TC.2005.150
   Greenan K. M., 2010, P HOTSTORAGE, P1
   Grupp L.M., 2012, P 10 USENIX C FILE S, P2
   Hafner J. L., 2006, P INT C DEP SYST NET, P1
   Hafner JL, 2005, USENIX ASSOCIATION PROCEEDINGS OF THE 4TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P211
   Huang C., 2005, P 4 USENIX C FIL STO, P889
   Huang C, 2013, ACM T STORAGE, V9, DOI 10.1145/2435204.2435207
   Huang Cheng, 2012, P USENIX ATC
   Iliadis I, 2008, PROCEEDINGS OF THE 2008 IEEE INTERNATIONAL CONFERENCE ON NETWORKING, ARCHITECTURE, AND STORAGE, P10, DOI 10.1109/NAS.2008.20
   Intel, 2005, WHIT PAP
   LI M, 2009, ACM T STORAGE, V4, P1
   Li M., 2011, RC25218C1110004 IBM
   Li Mingqiang, 2014, P 12 USENIX C FIL ST, P147
   Oprea A., 2010, P 8 USENIX C FIL STO, P1
   Pinheiro E, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 5TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES ( FAST '07), P17
   Plank J., 2013, USENIX C FILE STOR T, P299
   Plank JS, 2006, NCA 2006: FIFTH IEEE INTERNATIONAL SYMPOSIUM ON NETWORK COMPUTING AND APPLICATIONS, PROCEEDINGS, P173
   Plank JS, 2014, ACM T STORAGE, V10, DOI 10.1145/2560013
   Plank JS, 2011, ACM T STORAGE, V6, DOI 10.1145/1970338.1970340
   Plank James S., 2013, P 11 USENIX C FIL ST, P95
   Plank JS, 2005, SOFTWARE PRACT EXPER, V35, P189, DOI 10.1002/spe.631
   Plank JS, 1997, SOFTWARE PRACT EXPER, V27, P995, DOI 10.1002/(SICI)1097-024X(199709)27:9<995::AID-SPE111>3.0.CO;2-6
   REED IS, 1960, J SOC IND APPL MATH, V8, P300, DOI 10.1137/0108018
   Sathiamoorthy M, 2013, PROC VLDB ENDOW, V6, P325, DOI 10.14778/2535573.2488339
   Schroeder B, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 5TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES ( FAST '07), P1
   Schwarz TJE, 2004, IEEE COMPUTER SOCIETY'S 12TH ANNUAL INTERNATIONAL SYMPOSIUM ON MODELING, ANALYSIS, AND SIMULATION OF COMPUTER AND TELECOMMUNICATIONS SYSTEMS - PROCEEDINGS, P409, DOI 10.1109/MASCOT.2004.1348296
   Tucek Joseph, 2013, P 11 USENIX C FIL ST
   White J., 2010, TR3298 NETAPP INC
   Wildani A., 2009, P 17 ANN M IEEE ACM, P1
   Xu LH, 1999, IEEE T INFORM THEORY, V45, P1817, DOI 10.1109/18.782102
   Xu LH, 1999, IEEE T INFORM THEORY, V45, P272, DOI 10.1109/18.746809
NR 48
TC 26
Z9 27
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD OCT
PY 2014
VL 10
IS 4
SI SI
AR 14
DI 10.1145/2658991
PG 30
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AT0LD
UT WOS:000344626700002
DA 2024-07-18
ER

PT J
AU Shin, DI
   Yu, YJ
   Kim, HS
   Eom, H
   Yeom, HY
AF Shin, Dong In
   Yu, Young Jin
   Kim, Hyeong S.
   Eom, Hyeonsang
   Yeom, Heon Young
TI Request Bridging and Interleaving: Improving the Performance of Small
   Synchronous Updates under Seek-Optimizing Disk Subsystems
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Performance; Measurement; Experimentation; Request merging; disk
   scheduling algorithm; disk drive; trace-driven analysis; cross-layer
   optimizations
AB Write-through caching in modern disk drives enables the protection of data in the event of power failures as well as from certain disk errors when the write-back cache does not. Host system can achieve these benefits at the price of significant performance degradation, especially for small disk writes. We present new block-level techniques to address the performance problem of write-through caching disks. Our techniques are strongly motivated by some interesting results when the disk-level caching is turned off. By extending the conventional request merging, request bridging increases the request size and amortizes the inherent delays in the disk drive across more bytes of data. Like sector interleaving, request interleaving rearranges requests to prevent the disk head from missing the target sector position in close proximity, and thus reduces disk latency. We have evaluated our block-level approach using a variety of I/O workloads and shown that it increases disk I/O throughput by up to about 50%. For some real-world workloads, the disk performance is comparable or even superior to that of using the write-back disk cache. In practice, our simple yet effective solutions achieve better tradeoffs between data reliability and disk performance when applied to write-through caching disks.
C1 [Shin, Dong In; Yu, Young Jin; Kim, Hyeong S.; Eom, Hyeonsang; Yeom, Heon Young] Seoul Natl Univ, Distributed Comp Syst Lab, Seoul 151742, South Korea.
C3 Seoul National University (SNU)
RP Eom, H (corresponding author), Seoul Natl Univ, Distributed Comp Syst Lab, 599 Gwanak Ro, Seoul 151742, South Korea.
EM dishin@dcslab.snu.ac.kr; yjyu@dcslab.snu.ac.kr; hskim@dcslab.snu.ac.kr;
   hseom@cse.snu.ac.kr; yeom@dcslab.snu.ac.kr
CR [Anonymous], HPL92152
   Bairavasundaram LN, 2007, PERF E R SI, V35, P289
   BRUNELLE A. D., 2009, BLKTRACE USER GUIDE
   Dees B, 2005, IEEE POTENTIALS, V24, P4, DOI 10.1109/MP.2005.1549750
   DELL, 2004, CONTROLLERS DELL OPE
   DIMITRIJEVIC Z, 2001, DISKBENCH USER LEVEL
   Ding XN, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 2007 USENIX ANNUAL TECHNICAL CONFERENCE, P261
   GANGER GR, 2001, CMUCS01166
   Gill BS, 2005, USENIX ASSOCIATION PROCEEDINGS OF THE 4TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P129
   GUEST J. E., 1974, IBM TECH DISCLOSURE, V17, P1460
   HP, 2002, HP A5856A RAID 4SI P
   Huang Lan., 2000, IMPLEMENTATION ROTAT
   IBM, 2004, RAID HARD DRIV WRIT
   Iyer S., 2001, Operating Systems Review, V35, P117, DOI 10.1145/502059.502046
   JACOBSON D, 1991, HPLCSP917REV1
   JANG K., 2007, IEEE T CONSUM ELECTR, V53, P4
   Jiang S, 2005, USENIX ASSOCIATION PROCEEDINGS OF THE 4TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P101
   KATCHER J., 1991, HPLCSSP917REV1
   Lumb CR, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE FAST'02 CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P275
   MCDOUGALL R., 2004, SESS 3 USEN IN PRESS
   MCKUSICK M. K., 1999, P ANN TECHN C USENIX
   MCKUSICK MK, 1984, ACM T COMPUT SYST, V2, P181, DOI 10.1145/989.990
   PAPATHANASIOU A. E., 2007, P 2 ACM SIGOPS EUROS
   PIGGIN N., 2003, ANTICIPATORY 1 O SCH
   Prabhakaran  V., 2005, P USENIX ANN TECHN C
   Reuther L, 2003, RTSS 2003: 24TH IEEE INTERNATIONAL REAL-TIME SYSTEMS SYMPOSIUM, PROCEEDINGS, P374, DOI 10.1109/REAL.2003.1253285
   Riska A, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 2007 USENIX ANNUAL TECHNICAL CONFERENCE, P247
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   RUEMMLER C, 1994, COMPUTER, V27, P17, DOI 10.1109/2.268881
   SCHINDLER J., 2002, P USENIX C FIL STOR
   SCHINDLER J., 2000, P ACM SIGMETRICS INT
   SELTZER M., 1993, P WINT USENIX C, P405
   Seltzer MI, 2000, USENIX ASSOCIATION PROCEEDINGS OF THE 2000 USENIX ANNUAL TECHNICAL CONFERENCE, P71
   Shanley T., 1999, PCI SYSTEM ARCHITECT, Vfourth
   Shin DI, 2007, I S MOD ANAL SIM COM, P410
   Sivathanu M, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE 2ND USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST'03), P73
   SIVATHANU M., 2004, ACM SIGMETRICS PERFO, P374
   Worthington B. L., 1994, Performance Evaluation Review, V22, P241, DOI 10.1145/183019.183045
   Worthington B. L., 1995, Performance Evaluation Review, V23, P146, DOI 10.1145/223586.223604
   Youjip Won, 2006, ACM Transaction on Storage, V2, P255, DOI 10.1145/1168910.1168912
NR 40
TC 3
Z9 3
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUL
PY 2011
VL 7
IS 2
AR 4
DI 10.1145/1970348.1970349
PG 31
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 990LL
UT WOS:000307632300001
DA 2024-07-18
ER

PT J
AU Wu, SZ
   Tu, ZH
   Zhou, YX
   Wang, ZC
   Shen, ZR
   Chen, W
   Wang, W
   Wang, WC
   Mao, B
AF Wu, Suzhen
   Tu, Zhanhong
   Zhou, Yuxuan
   Wang, Zuocheng
   Shen, Zhirong
   Chen, Wei
   Wang, Wei
   Wang, Weichun
   Mao, Bo
TI FASTSync: A FAST Delta Sync Scheme for Encrypted Cloud Storage in
   High-bandwidth Network Environments
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Cloud storage; delta synchronization; message-locked encryption;
   ciphertext synchronization; high-bandwidth network
AB More and more data are stored in cloud storage, which brings two major challenges. First, the modified files in the cloud should be quickly synchronized to ensure data consistency, e.g., delta synchronization (sync) achieves efficient cloud sync by synchronizing only the updated part of the file. Second, the huge data in the cloud needs to be deduplicated and encrypted, e.g., Message-Locked Encryption (MLE) implements data deduplication by encrypting the content among different users. However, when combined, a few updates in the content can cause large sync traffic amplification for both keys and ciphertext in the MLE-based cloud storage, significantly degrading the cloud sync efficiency. A feature-based encryption sync scheme, FeatureSync, is proposed to address the delta amplification problem. However, with further improvement of the network bandwidth, the performance of FeatureSync stagnates. In our preliminary experimental evaluations, we find that the bottleneck of the computational overhead in the high-bandwidth network environments is the main bottleneck in FeatureSync. In this article, we propose an enhanced feature-based encryption sync scheme FASTSync to optimize the performance of FeatureSync in high-bandwidth network environments. The performance evaluations on a lightweight prototype implementation of FASTSync show that FASTSync reduces the cloud sync time by 70.3% and the encryption time by 37.3%, on average, compared with FeatureSync.
C1 [Wu, Suzhen; Tu, Zhanhong; Zhou, Yuxuan; Wang, Zuocheng; Shen, Zhirong; Mao, Bo] Xiamen Univ, 422 Siming South Rd, Xiamen 361005, Fujian, Peoples R China.
   [Chen, Wei; Wang, Wei; Wang, Weichun] Hikivision, 555 Qianruo Rd, Hangzhou 310051, Zhejiang, Peoples R China.
C3 Xiamen University
RP Mao, B (corresponding author), Xiamen Univ, 422 Siming South Rd, Xiamen 361005, Fujian, Peoples R China.; Wang, W (corresponding author), Hikivision, 555 Qianruo Rd, Hangzhou 310051, Zhejiang, Peoples R China.
EM suzhen@xmu.edu.cn; 869607491@qq.com; zhouyuxuan@stu.xmu.edu.cn;
   865712963@qq.com; shenzr@xmu.edu.cn; chenwei7@hikvision.com;
   wangweiwhxt1@hikvision.com; wangweichun@hikvision.com; maobo@xmu.edu.cn
RI Zhou, Yuxuan/GRS-3780-2022
FU National Natural Science Foundation of China [U1705261, 61972325,
   61872305]; Open Project Program of Wuhan National Laboratory for
   Optoelectronics [2021WNLOKF011]; Research Project of Zhejiang Lab
   [2021DA0AM01/002]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants U1705261, 61972325, and 61872305, in
   part by the Open Project Program of Wuhan National Laboratory for
   Optoelectronics under Grant 2021WNLOKF011, and in part by the Research
   Project of Zhejiang Lab under Grant 2021DA0AM01/002.
CR [Anonymous], 2015, Sookasa's encrypted cloud storage
   [Anonymous], 2022, OPENSSL
   [Anonymous], 2022, FastCDC
   [Anonymous], 2022, Linux Kernel
   [Anonymous], 2022, HashMap
   [Anonymous], 2022, Microsoft OneDrive
   Apple iCloud, 2022, about us
   Bellare M, 2013, LECT NOTES COMPUT SC, V7881, P296, DOI 10.1007/978-3-642-38348-9_18
   Bobbarjung D. R., 2006, ACM Transaction on Storage, V2, P424, DOI 10.1145/1210596.1210599
   Boxcrypto, 2022, about us
   Broder AZ, 1998, COMPRESSION AND COMPLEXITY OF SEQUENCES 1997 - PROCEEDINGS, P21, DOI 10.1109/SEQUEN.1997.666900
   Cui Y, 2015, MOBICOM '15: PROCEEDINGS OF THE 21ST ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P592, DOI 10.1145/2789168.2790094
   Douceur JR, 2002, INT CON DISTR COMP S, P617, DOI 10.1109/ICDCS.2002.1022312
   Douglis F, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE GENERAL TRACK, P113
   Dropbox, 2022, about us
   Github, 2022, About us
   Google Drive, 2022, about us
   He Y., 2020, P 36 S MASS STOR SYS
   Keelveedhi Sriram, 2013, P 22 USENIX SEC S SE
   Kulkarni P, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE GENERAL TRACK 2004 USENIX ANNUAL TECHNICAL CONFERENCE, P59
   Li JN, 2019, Intern Sym Radi Tran, P269, DOI 10.1109/MSST.2019.00007
   Li JW, 2020, PROCEEDINGS OF THE FIFTEENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS (EUROSYS'20), DOI 10.1145/3342195.3387531
   Li JW, 2017, I C DEPEND SYS NETWO, P1, DOI 10.1109/DSN.2017.28
   Li Zhenhua, 2013, P 14 INT MIDDL C MID
   Linux Traffic Control, 2022, about us
   MacDonald Josh, 2000, Ph.D. thesis
   Mao B, 2016, IEEE T PARALL DISTR, V27, P2010, DOI 10.1109/TPDS.2015.2475273
   Mao B, 2016, IEEE T COMPUT, V65, P1775, DOI 10.1109/TC.2015.2455979
   Mohiuddin I, 2019, FUTURE GENER COMP SY, V90, P307, DOI 10.1016/j.future.2018.08.013
   Muthitacharoen A., 2001, Operating Systems Review, V35, P174, DOI 10.1145/502059.502052
   Qin C, 2017, ACM T STORAGE, V13, DOI 10.1145/3032966
   Seafile, 2022, Enterprise file sync, share platform with high reliability, and performance
   Shilane P, 2012, ACM T STORAGE, V8, DOI 10.1145/2385603.2385606
   Suel Torsten, 2002, Handbook of Lossless Compression
   Wu SZ, 2021, INT CON DISTR COMP S, P337, DOI 10.1109/ICDCS51616.2021.00040
   Wu SZ, 2019, INT CON DISTR COMP S, P282, DOI 10.1109/ICDCS.2019.00036
   Wu SZ, 2019, IEEE T PARALL DISTR, V30, P2117, DOI 10.1109/TPDS.2019.2898942
   Xia W, 2016, PROCEEDINGS OF USENIX ATC '16: 2016 USENIX ANNUAL TECHNICAL CONFERENCE, P101
   Xia W, 2016, P IEEE, V104, P1681, DOI 10.1109/JPROC.2016.2571298
   Xia W, 2014, PERFORM EVALUATION, V79, P258, DOI 10.1016/j.peva.2014.07.016
   Xiao H, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P155
   Zhang QL, 2017, INT CON DISTR COMP S, P264, DOI 10.1109/ICDCS.2017.77
   Zhou YK, 2015, IEEE S MASS STOR SYS
NR 43
TC 1
Z9 1
U1 2
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2023
VL 19
IS 4
AR 37
DI 10.1145/3607536
PG 22
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X6NQ6
UT WOS:001099601100009
DA 2024-07-18
ER

PT J
AU Yang, TW
   Pollen, S
   Uysal, M
   Merchant, A
   Wolfmeister, H
   Khalid, J
AF Yang, Tzu-Wei
   Pollen, Seth
   Uysal, Mustafa
   Merchant, Arif
   Wolfmeister, Homer
   Khalid, Junaid
TI CacheSack: Theory and Experience of Google's Admission Optimization for
   Datacenter Flash Caches
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Flash caches; distributed storage systems
AB This article describes the algorithm, implementation, and deployment experience of CacheSack, the admission algorithm for Google datacenter flash caches. CacheSack minimizes the dominant costs of Google's datacenter flash caches: disk IO and flash footprint. CacheSack partitions cache traffic into disjoint categories, analyzes the observed cache benefit of each subset, and formulates a knapsack problem to assign the optimal admission policy to each subset. Prior to this work, Google datacenter flash cache admission policies were optimized manually, with most caches using the Lazy Adaptive Replacement Cache algorithm. Production experiments showed that CacheSack significantly outperforms the prior static admission policies for a 7.7% improvement of the total cost of ownership, as well as significant improvements in disk reads (9.5% reduction) and flash wearout (17.8% reduction).
C1 [Yang, Tzu-Wei; Uysal, Mustafa; Merchant, Arif] Google, 1600 Amphitheatre Pkwy, Mountain View, CA 94043 USA.
   [Pollen, Seth; Wolfmeister, Homer; Khalid, Junaid] Google, 811 E Washington Ave,Suite 700, Madison, WI 53703 USA.
C3 Google Incorporated; Google Incorporated
RP Yang, TW (corresponding author), Google, 1600 Amphitheatre Pkwy, Mountain View, CA 94043 USA.
EM twyang@google.com; pollen@google.com; uysal@google.com;
   aamerchant@google.com; wolfmeister@google.com; junaidkhalid@google.com
RI Uysal, Mustafa/KPB-0466-2024
OI Uysal, Mustafa/0000-0002-7881-9046; Merchant, Arif/0000-0002-0913-1459
CR Albrecht C., 2013, USENIX ANN TECHNICAL, P91
   ANDREW AM, 1979, INFORM PROCESS LETT, V9, P216, DOI 10.1016/0020-0190(79)90072-3
   [Anonymous], 1968, Technical report
   Barr Jeff., 2021, AQUA (Advanced Query Accelerator)-A Speed Boost for Your Amazon Redshift Queries
   Beckmann N, 2018, PROCEEDINGS OF THE 15TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION (NSDI'18), P389
   Beckmann N, 2017, INT S HIGH PERF COMP, P109, DOI 10.1109/HPCA.2017.43
   BELADY LA, 1966, IBM SYST J, V5, P78, DOI 10.1147/sj.52.0078
   Berg B, 2020, PROCEEDINGS OF THE 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '20), P769
   Berger DS, 2018, HOTNETS-XVII: PROCEEDINGS OF THE 2018 ACM WORKSHOP ON HOT TOPICS IN NETWORKS, P134, DOI 10.1145/3286062.3286082
   Blankstein A, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P499
   Brent R. P., 1973, Algorithms for Minimization without Derivatives
   Chang F, 2008, ACM T COMPUT SYST, V26, DOI 10.1145/1365815.1365816
   Corbett JC, 2013, ACM T COMPUT SYST, V31, DOI 10.1145/2491245
   DANTZIG GB, 1957, OPER RES, V5, P266, DOI 10.1287/opre.5.2.266
   Einziger G, 2017, ACM T STORAGE, V13, DOI 10.1145/3149371
   Eisenman A, 2019, PROCEEDINGS OF THE 16TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P65
   FAGIN R, 1977, J COMPUT SYST SCI, V14, P222, DOI 10.1016/S0022-0000(77)80014-7
   Gartrell Alex., 2013, McDipper: A key-value cache for Flash storage
   Ghemawat S., 2003, P 19 ACM S OPERATING, P20
   Hildebrand Dean, 2021, Colossus under the hood: a peek into Google's scalable storage system
   Huang S, 2016, ACM T STORAGE, V12, DOI 10.1145/2737832
   Jain A, 2016, CONF PROC INT SYMP C, P78, DOI 10.1109/ISCA.2016.17
   Jiang B, 2018, ACM TRANS MODELING P, V3, DOI 10.1145/3239164
   Li JH, 2020, I S WORKL CHAR PROC, P37, DOI 10.1109/IISWC50251.2020.00013
   Liu Evan, 2020, PMLR, P6237
   McAllister S, 2021, PROCEEDINGS OF THE 28TH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, SOSP 2021, P243, DOI 10.1145/3477132.3483568
   Megiddo N, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE 2ND USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST'03), P115
   Mesnier M, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P57
   Min Changwoo, 2012, FAST, V12, P1
   Pancham Pancham, 2014, International Journal of Computer Applications, V98, P27
   Qureshi MK, 2006, INT SYMP MICROARCH, P423
   Santana Ricardo, 2015, P 7 USENIXWORKSHOP H
   Seo M.J., 2017, 5 INT C LEARNING REP
   Shute J, 2013, PROC VLDB ENDOW, V6, P1068, DOI 10.14778/2536222.2536232
   Song ZY, 2020, PROCEEDINGS OF THE 17TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P529
   Tang Linpeng, 2015, 13 USENIX C FIL STOR, P373
   Thallam Rajesh, 2020, BigQuery explained: An overview of BigQuery's architecture
   Vaswani A, 2017, ADV NEUR IN, V30
   Vietri Giuseppe, 2018, P 10 USENIX C HOT TO
   Waldspurger CA, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P487
   Wang H, 2018, PROC INT CONF PARAL, DOI 10.1145/3225058.3225126
   Yang JL, 2013, ADV MATER RES-SWITZ, V826, P10, DOI 10.4028/www.scientific.net/AMR.826.10
   Zhou Giulio, 2021, P MACH LEARN SYST, V3, P350
   Zhou Huapeng, 2016, The Evolution of Advanced Caching in the Facebook CDN
NR 44
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAY
PY 2023
VL 19
IS 2
AR 13
DI 10.1145/3582014
PG 24
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D7OU8
UT WOS:000970592600004
OA Bronze
DA 2024-07-18
ER

PT J
AU May, MJ
   Laron, E
   Zoabi, K
   Gerhardt, H
AF May, Michael J.
   Laron, Etamar
   Zoabi, Khalid
   Gerhardt, Havah
TI On the Lifecycle of the File
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Files; file lifecycle; file relationships; complex events; mental model;
   rule-based systems
AB Users and Operating Systems (OSs) have vastly different views of files. OSs use files to persist data and structured information. To accomplish this, OSs treat files as named collections of bytes managed in hierarchical file systems. Despite their critical role in computing, little attention is paid to the lifecycle of the file, the evolution of file contents, or the evolution of file metadata. In contrast, users have rich mental models of files: they group files into projects, send data repositories to others, work on documents over time, and stash them aside for future use. Current OSs and Revision Control Systems ignore such mental models, persisting a selective, manually designated history of revisions. Preserving the mental model allows applications to better match how users view their files, making file processing and archiving tools more effective. We propose two mechanisms that OSs can adopt to better preserve the mental model: File Lifecycle Events (FLEs) that record a file's progression and Complex File Events (CFEs) that combine them into meaningful patterns. We present the Complex File Events Engine (CoFEE), which uses file system monitoring and an extensible rulebase (Drools) to detect FLEs and convert them into complex ones. CFEs are persisted in NoSQL stores for later querying.
C1 [May, Michael J.; Zoabi, Khalid; Gerhardt, Havah] Kinneret Coll Sea Galilee, Software Engn Dept, IL-15132 Tzemach Junction, Israel.
   [May, Michael J.; Laron, Etamar] Kinneret Asparna Res Ctr, IL-15132 Tzemach Junction, Israel.
   [Laron, Etamar] Add Asparna Ltd, Magal, Israel.
RP May, MJ (corresponding author), Kinneret Coll Sea Galilee, Software Engn Dept, IL-15132 Tzemach Junction, Israel.; May, MJ (corresponding author), Kinneret Asparna Res Ctr, IL-15132 Tzemach Junction, Israel.
EM michael@asparna.com; etamar@asparna.com; khalid@asparna.com;
   havah@asparna.com
RI May, Michael J./AAN-8538-2021
OI May, Michael J./0000-0003-4571-7972
CR Agrawal Nitin, 2009, ACM Transactions on Storage, V5, DOI 10.1145/1629080.1629086
   Agrawal Nitin, 2007, ACM Transactions on Storage, V3, DOI 10.1145/1288783.1288788
   Allred Christian, 2009, P STOR DEV C
   Amsterdamer Y, 2011, PROC VLDB ENDOW, V5, P346, DOI 10.14778/2095686.2095693
   [Anonymous], P USENIX WINT 1994 T
   [Anonymous], 2013, PROV DM PROV DATA MO
   [Anonymous], 2005, ACM Transactions on Storage
   [Anonymous], P 3 USENIX WORKSH TH
   [Anonymous], IEEE T VISUALIZATION
   Apple Inc., 2018, BACK YOUR MAC TIM MA
   AWS, 2006, AM SIMPL STOR SERV D
   Ben Lahmar Houssem, 2017, P 9 USENIX WORKSH TH
   Bhashkar Aishwary, 2016, Patent, Patent No. [US20160378988A1, 20160378988]
   Chapman A., 2012, P 4 USENIX WORKSH TH
   Chen Hsiang-Ting, 2011, P ACM SIGGRAPH 2011
   Chen X, 2004, IEEE T INFORM THEORY, V50, P1545, DOI 10.1109/TIT.2004.830793
   Conradi R, 1998, ACM COMPUT SURV, V30, P232, DOI 10.1145/280277.280280
   Continella A, 2016, ANN COMPUT SECURITY, P336, DOI 10.1145/2991079.2991110
   Cornell B, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE FREENIX TRACK 2004 USENIX ANNUAL TECHNICAL CONFERENCE, P19
   Darcy Jeff, 2011, USENIX; Login, V36, P14
   Diestelkamper Ralf, 2017, P 9 USENIX WORKSH TH
   Digital Equipment Corporation, 1980, TOPS 20 US GUID
   Dinneen Jesse David, 2016, P 79 ASIS T ANN M CR
   Douceur JR, 1999, PERFORMANCE EVALUATION REVIEW, SPECIAL ISSUE, VOL 27 NO 1, JUNE 1999, P59, DOI 10.1145/301464.301480
   Esiner E, 2016, FUTURE GENER COMP SY, V55, P17, DOI 10.1016/j.future.2015.08.001
   Forman George., 2005, KDD 05, P394
   Gehani Ashish, 2016, P 8 USENIX WORKSH TH
   Git Project, 2018, GIT LOG SHOW COMM LO
   Harter T, 2012, ACM T COMPUT SYST, V30, DOI 10.1145/2324876.2324878
   Hitz Dave, 1994, 3002 NETWORKAPPLIANC
   Howison M., 2012, P 4 USENIX WORKSH TH
   Inman J., 2017, USENIX, V42, P26
   JBoss Community, 2018, DROOLS BUS RUL MAN S
   Jung HM, 2012, INT J SECUR APPL, V6, P421
   Karampatziakis N., 2012, P C DET INTR MALW VU
   Khan S, 2016, ADV ENG INFORM, V30, P244, DOI 10.1016/j.aei.2016.04.003
   Koop D, 2016, LECT NOTES COMPUT SC, V9672, P109, DOI 10.1007/978-3-319-40593-3_9
   Kung Stefan, 2018, DELETING MOVING RENA
   Laden G, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 5TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES ( FAST '07), P107
   Lim SH, 2017, SC'17: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS, DOI 10.1145/3126908.3126924
   Lin PC, 2011, IEEE T COMPUT, V60, P594, DOI 10.1109/TC.2010.95
   Lucarella D., 1988, Electronic Publishing, V1, P105
   Macko P., 2012, P 4 USENIX WORKSH TH
   Macko P., 2011, P 3 USENIX WORKSH TH
   Mcusick Marshall K, 1984, ACM T COMPUT SYST, V2, DOI DOI 10.1145/989.990
   Microsoft, 2014, DESCR WORD CREAT TEM
   Min C, 2015, SOSP'15: PROCEEDINGS OF THE TWENTY-FIFTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P361, DOI 10.1145/2815400.2815422
   MUNISWAMY-REDDY K.-K, 2006, P 2006 USENIX ANN TE
   Neo Technology Inc., 2016, BOLT PROT PROT STAND
   Neo4j, 2018, CYPH QUER LANG REF L
   Neo4j, 2018, NEO4J DAT
   Pachler Uwe, 2012, JPATHWATCH JAVA LIB
   Paulo J, 2014, ACM COMPUT SURV, V47, DOI 10.1145/2611778
   Roselli D., 2000, P 2000 USENIX ANN TE
   Roussev V, 2009, IEEE SECUR PRIV, V7, P49, DOI 10.1109/MSP.2009.40
   Santry D. J., 1999, Proceedings of the Seventh Workshop on Hot Topics in Operating Systems, P2, DOI 10.1109/HOTOS.1999.798369
   Schreiber Andreas, 2017, P 9 USENIX WORKSH TH
   Seltzer M., 2009, P 12 WORKSH HOT TOP
   Smith K. A., 1997, Performance Evaluation Review, V25, P203, DOI 10.1145/258623.258689
   Soules C., 2003, P 2 USENIX C FIL STO
   Spillane R., 2009, P 1 WORKSH THEOR PRA
   Sultana Salmin., 2013, Proceedings of the Third ACM Conference on Data and Application Security and Privacy, CODASPY '13, P153
   Torvalds Linus, 2005, RE MERGE GIT PASKY 2
   Traeger Avishay, 2008, ACM Transaction on Storage, V4, DOI 10.1145/1367829.1367831
   Venugopal D, 2008, MOB INF SYST, V4, P33, DOI 10.1155/2008/712353
   Xie Yulai, 2011, 3 WORKSH THEOR PRACT
   Yadan Omry, 2012, JNOTIFY FILE SYSTEM
   Zhou Ke, 2012, P 5 ANN INT SYST STO, DOI 10.1145/2367589.2367597
   2004, USENIX ASS P 3, P115
NR 69
TC 4
Z9 4
U1 2
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD APR
PY 2019
VL 15
IS 1
SI SI
AR 1
DI 10.1145/3295463
PG 45
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HU9GE
UT WOS:000465601900001
DA 2024-07-18
ER

PT J
AU Zhan, Y
   Jiao, YZ
   Porter, DE
   Conway, A
   Knorr, E
   Farach-Colton, M
   Bender, MA
   Yuan, J
   Jannen, W
   Johnson, R
AF Zhan, Yang
   Jiao, Yizheng
   Porter, Donald E.
   Conway, Alex
   Knorr, Eric
   Farach-Colton, Martin
   Bender, Michael A.
   Yuan, Jun
   Jannen, William
   Johnson, Rob
TI Efficient Directory Mutations in a Full-Path-Indexed File System
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article; Proceedings Paper
CT 16th USENIX Conference on File and Storage Technologies (FAST)
CY FEB 12-15, 2018
CL Oakland, CA
SP USENIX, ACM SIGOPS
DE B-epsilon-trees; file system; write optimization
AB Full-path indexing can improve I/O efficiency for workloads that operate on data organized using traditional, hierarchical directories, because data is placed on persistent storage in scan order. Prior results indicate, however, that renames in a local file system with full-path indexing are prohibitively expensive.
   This article shows how to use full-path indexing in a file system to realize fast directory scans, writes, and renames. The article introduces a range-rename mechanism for efficient key-space changes in a write-optimized dictionary. This mechanism is encapsulated in the key-value Application Programming Interface (API) and simplifies the overall file system design.
   We implemented this mechanism in B-epsilon-trees File System (BetrFS), an in-kernel, local file system for Linux. This new version, BetrFS 0.4, performs recursive greps 1.5x faster and random writes 1.2x faster than BetrFS 0.3, but renames are competitive with indirection-based file systems for a range of sizes. BetrFS 0.4 outper-forms BetrFS 0.3, as well as traditional file systems, such as ext4, Extents File System (XFS), and Z File System (ZFS), across a variety of workloads.
C1 [Zhan, Yang; Jiao, Yizheng; Porter, Donald E.] Univ N Carolina, Dept Comp Sci, Chapel Hill, NC 27599 USA.
   [Conway, Alex; Knorr, Eric; Farach-Colton, Martin] Rutgers State Univ, Dept Comp Sci, Piscataway, NJ 08854 USA.
   [Bender, Michael A.; Yuan, Jun] SUNY Stony Brook, New Comp Sci, Stony Brook, NY 11794 USA.
   [Jannen, William] Williams Coll, Comp Sci Dept, Williamstown, MA 01267 USA.
   [Johnson, Rob] VMware Res, 3425 Hillview Ave, Palo Alto, CA 94304 USA.
C3 University of North Carolina; University of North Carolina Chapel Hill;
   Rutgers University System; Rutgers University New Brunswick; State
   University of New York (SUNY) System; State University of New York
   (SUNY) Stony Brook; Williams College; VMware, Inc.
RP Zhan, Y (corresponding author), Univ N Carolina, Dept Comp Sci, Chapel Hill, NC 27599 USA.
EM yzhan@cs.unc.edu; yizheng@cs.unc.edu; porter@cs.unc.edu;
   alexander.conway@rutgers.edu; erk58@scarletmail.rutgers.edu;
   farach@cs.rutgers.edu; bender@cs.stonybrook.edu;
   junyuan@cs.stonybrook.edu; jannen@cs.williams.edu; robj@vmware.com
OI Farach-Colton, Martin/0000-0003-3616-7788
FU NSF [CNS-1409238, CNS-1408782, CNS-1408695, CNS-1405641, IIS 1251137,
   IIS-1247750, CCF 1617618, CCF 1439084, CCF-1314547]; NIH [CA198952-01];
   VMware; NetApp Faculty Fellowships; EMC
FX This research was supported in part by NSF grants CNS-1409238,
   CNS-1408782, CNS-1408695, CNS-1405641, IIS 1251137, IIS-1247750, CCF
   1617618, CCF 1439084, CCF-1314547, and by NIH grant CA198952-01. The
   work was also supported by VMware, by EMC, and by NetApp Faculty
   Fellowships.
CR Ahmad Y, 2012, PROC VLDB ENDOW, V5, P968
   Ahmad Y, 2009, PROC VLDB ENDOW, V2, P1566, DOI 10.14778/1687553.1687592
   [Anonymous], 2013, P 6 INT SYSTEMS STOR
   [Anonymous], 2014, P 12 USENIX C FILE S
   [Anonymous], 1959, Proceedings of the Western Joint Computer Conference, DOI [10.1145/1457838.1457895, DOI 10.1145/1457838.1457895]
   ARASU A, 2003, P 2003 ACM SIGMOD IN, P665
   Balmau O, 2017, PROCEEDINGS OF THE TWELFTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS (EUROSYS 2017), P80, DOI 10.1145/3064176.3064193
   Bender MA, 2002, LECT NOTES COMPUT SC, V2461, P139
   Bender MA, 2007, SPAA'07: PROCEEDINGS OF THE NINETEENTH ANNUAL SYMPOSIUM ON PARALLELISM IN ALGORITHMS AND ARCHITECTURES, P81
   Bender Michael A., 2015, login Usenix Mag., V40, P5
   Brodal GS, 2010, PROC APPL MATH, V135, P1448
   Brodal GS, 2003, SIAM PROC S, P546
   Buchsbaum AL, 2000, PROCEEDINGS OF THE ELEVENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P859
   Chang F, 2008, ACM T COMPUT SYST, V26, DOI 10.1145/1365815.1365816
   Cipar J., 2012, Pro- ceedings of the 7th ACM European Conference on Computer Systems, EuroSys '12, P169, DOI [10.1145/2168836.2168854, DOI 10.1145/2168836.2168854]
   Conway A, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P45
   Esmet J., 2012, HotStorage
   Finis J, 2015, PROC VLDB ENDOW, V8, P986, DOI 10.14778/2794367.2794369
   Ghemawat S., 2003, Operating Systems Review, V37, P29, DOI 10.1145/1165389.945450
   Golan-Gueta Guy, 2015, P 10 EUR C COMP SYST
   Hong Mingsheng., 2007, Proceedings of the 2007 ACM SIGMOD international conference on Management of data (SIGMOD'07), P761
   Jannen W., 2015, P 13 USENIX C FIL ST, P301, DOI DOI 10.5555/2750482.2750505
   Jannen W, 2015, ACM T STORAGE, V11, DOI 10.1145/2798729
   Kim S.-H., 2012, P 2012 BELIV WORKSHO, DOI [10.1145/2442576.2442591, DOI 10.1145/2442576.2442591]
   Konishi R., 2006, Operating Systems Review, V40, P102, DOI 10.1145/1151374.1151375
   Lakshman Avinash, 2010, Operating Systems Review, V44, P35, DOI 10.1145/1773912.1773922
   Lovelace Mary, 2004, VSAM DEMYSTIFIED
   Lu LY, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P133
   Mathur Avantika, 2007, P LINUX S, V2, P21
   MCKUSICK MK, 1984, ACM T COMPUT SYST, V2, P181, DOI 10.1145/989.990
   Mei F, 2017, PROCEEDINGS OF THE 2017 SYMPOSIUM ON CLOUD COMPUTING (SOCC '17), P142, DOI 10.1145/3127479.3127486
   Olson Jason, 2007, MSDN MAGAZINE
   ONeil P, 1996, ACTA INFORM, V33, P351, DOI 10.1007/s002360050048
   Papagiannis A, 2016, PROCEEDINGS OF USENIX ATC '16: 2016 USENIX ANNUAL TECHNICAL CONFERENCE, P537
   Peery C, 2005, LECT NOTES COMPUT SC, V3367, P200
   Porter DE, 2009, SOSP'09: PROCEEDINGS OF THE TWENTY-SECOND ACM SIGOPS SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P161
   Ren Kai., 2013, USENIX ANN TECHNICAL, P145
   Rodeh O, 2013, ACM T STORAGE, V9, DOI 10.1145/2501620.2501623
   Sears R, 2008, PROC VLDB ENDOW, V1, P526, DOI 10.14778/1453856.1453914
   Sears Russell, 2012, P 2012 ACM SIGMOD IN, P217, DOI [10.1145/2213836.2213862, DOI 10.1145/2213836.2213862]
   Shetty P.J., 2013, 11 USENIX C FILE STO, P17
   Spillane R. P., 2009, P USENIX C FIL STOR, V9, P29
   Sweeney A, 1996, PROCEEDINGS OF THE USENIX 1996 ANNUAL TECHNICAL CONFERENCE, P1
   Thomson A., 2015, 13th USENIX Confer- ence on File and Storage Technologies (FAST 15), P1, DOI [DOI 10.1002/14356007.A10_173.PUB2, 10.1002/14356007.a10_173.pub2]
   Tsai CC, 2015, SOSP'15: PROCEEDINGS OF THE TWENTY-FIFTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P441, DOI 10.1145/2815400.2815405
   Weil S.A., 2004, Proceedings of the 2004 ACM/IEEE conference on Supercomputing, P4
   Weil SA, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P307
   Wu XN, 2015, METHODS MOL BIOL, V1306, P71, DOI 10.1007/978-1-4939-2648-0_5
   Yuan J, 2017, ACM T STORAGE, V13, DOI 10.1145/3032969
   Yuan J, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P1
   Zeldovich N, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P263
NR 51
TC 3
Z9 4
U1 3
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2018
VL 14
IS 3
AR 22
DI 10.1145/3241061
PG 27
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA HJ4JQ
UT WOS:000457140700003
OA Bronze
DA 2024-07-18
ER

PT J
AU Haghdoost, A
   He, W
   Fredin, J
   Du, DHC
AF Haghdoost, Alireza
   He, Weiping
   Fredin, Jerry
   Du, David H. C.
TI hfplayer: Scalable Replay for Intensive Block I/O Workloads
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Storage performance; workload replay; storage area network; I/O workload
AB We introduce new methods to replay intensive block I/O workloads more accurately. These methods can be used to reproduce realistic workloads for benchmarking, performance validation, and tuning of a high-performance block storage device/system. In this article, we study several sources in the stock operating system that introduce uncertainty in the workload replay. Based on the remedies of these findings, we design and develop a new replay tool called hfplayer that replays intensive block I/O workloads in a similar unscaled environment with more accuracy. To replay a given workload trace in a scaled environment with faster storage or host server, the dependency between I/O requests becomes crucial since the timing and ordering of I/O requests is expected to change according to these dependencies. Therefore, we propose a heuristic way of speculating I/O dependencies in a block I/O trace. Using the generated dependency graph, hfplayer tries to propagate I/O related performance gains appropriately along the I/O dependency chains and mimics the original application behavior when it executes in a scaled environment with slower or faster storage system and servers. We evaluate hfplayer with a wide range of workloads using several accuracy metrics and find that it produces better accuracy when compared to other replay approaches.
C1 [Haghdoost, Alireza; He, Weiping; Du, David H. C.] Univ Minnesota, Minneapolis, MN 55455 USA.
   [Fredin, Jerry] NetApp Inc, Sunnyvale, CA USA.
   [Haghdoost, Alireza; He, Weiping; Du, David H. C.] 200 Union St SE, Minneapolis, MN 55455 USA.
   [Fredin, Jerry] 3718 N Rock Rd, Wichita, KS 67226 USA.
C3 University of Minnesota System; University of Minnesota Twin Cities;
   NetApp, Inc.
RP Haghdoost, A (corresponding author), Univ Minnesota, Minneapolis, MN 55455 USA.; Haghdoost, A (corresponding author), 200 Union St SE, Minneapolis, MN 55455 USA.
EM alireza@cs.umn.edu; weihe@cs.umn.edu; jerry@jerryfredin.com;
   du@cs.umn.edu
FU NSF I/UCRC Center for Research in Intelligent Storage (CRIS); National
   Science Foundation (NSF) [130523, 1439622, 1525617]; NetApp; Direct For
   Computer & Info Scie & Enginr; Division Of Computer and Network Systems
   [1439622, 1217569] Funding Source: National Science Foundation; Division
   Of Computer and Network Systems; Direct For Computer & Info Scie &
   Enginr [1525617] Funding Source: National Science Foundation
FX This work was supported by NSF I/UCRC Center for Research in Intelligent
   Storage (CRIS) and the National Science Foundation (NSF) under awards
   130523, 1439622, and 1525617, as well as by NetApp.
CR ANDERSON E., 2004, P 3 USENIX C FIL STO
   Anderson Eric, 2008, P 2008 FIL STOR SYST
   [Anonymous], FAST 07
   Bjorling Matias., 2013, Proceedings of the 6th International Systems and Storage Conference, P22
   Bourilkov Dimitri, P HEPIX FALL 2014 WO
   Brunelle AlanD., 2011, btrecord and btreplay user guide
   Haghdoost A, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P315
   Haghdoost Alireza, 2017, LINUX BUG FIX PATCH
   Joukov N., 2005, Proceedings of the 4th conference on USENIX Conference on File and Storage Technologies -, V4, P25
   Morton Al, 2006, RFC4737 IETF
   Percona, 2012, REPL DAT LOAD PERC P
   Percona-Lab, 2017, TPCC MYSQL
   Pereira Thiago Emmanuel, 2016, P 32 INT C MASS STOR
   Schbel-Theuer Thomas., 2012, blkreplay and sonar diagrams
   Schroeder Bianca., 2006, NSDI 06, P18
   Sivathanu Sankaran, 2011, P 9 USENIX C FIL STO
   SNIA, 2017, IOTTA REP HOM
   SPDK, 2017, Storage performance development kit
   Spinics. net, 2014, RE FIO REPL
   Storage Networking Industry Association (SNIA), 2010, BLOCK O TRAC COMM SE
   Storage Performance Council, 2017, SPC 1 BENCHM RES TOP
   Tarasov V., 2012, P 10 USENIX C FIL ST, P22
   TARASOV V, 2013, THESIS
   Tarasov Vasily, 2016, USENIX; login, V41, P6
   Tarihi Mojtaba, 2015, ACM SIGMETRICS Performance Evaluation Review, V43, P297, DOI 10.1145/2745844.2745856
   Traeger Avishay, 2008, ACM Transaction on Storage, V4, DOI 10.1145/1367829.1367831
   Weber R., 2005, 402 ANSI INCITS
   Weiss Z, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P373, DOI 10.1145/2517349.2522734
   Zhu N., 2005, ACM SIGMETRICS PERFO, V33, P392
   Zhuo Liu, 2010, Proceedings of the 2010 IEEE International Conference on Cluster Computing (CLUSTER 2010), P68, DOI 10.1109/CLUSTER.2010.40
NR 30
TC 2
Z9 3
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD DEC
PY 2017
VL 13
IS 4
SI SI
AR 39
DI 10.1145/3149392
PG 25
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FR4LD
UT WOS:000419035900012
DA 2024-07-18
ER

PT J
AU Hou, BB
   Chen, F
AF Hou, Binbing
   Chen, Feng
TI GDS-LC: A Latency- and Cost-Aware Client Caching Scheme for Cloud
   Storage
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Cloud storage; storage systems; caching algorithms
AB Successfully integrating cloud storage as a primary storage layer in the I/O stack is highly challenging. This is essentially due to two inherent critical issues: the high and variant cloud I/O latency and the per-I/O pricing model of cloud storage. To minimize the associated latency and monetary cost with cloud I/Os, caching is a crucial technology, as it directly influences how frequently the client has to communicate with the cloud. Unfortunately, current cloud caching schemes are mostly designed to optimize miss reduction as the sole objective and only focus on improving system performance while ignoring the fact that various cache misses could have completely distinct effects in terms of latency and monetary cost.
   In this article, we present a cost-aware caching scheme, called GDS-LC, which is highly optimized for cloud storage caching. Different from traditional caching schemes that merely focus on improving cache hit ratios and the classic cost-aware schemes that can only achieve a single optimization target, GDS-LC offers a comprehensive cache design by considering not only the access locality but also the object size, associated latency, and price, aiming at enhancing the user experience with cloud storage from two aspects: access latency and monetary cost. To achieve this, GDS-LC virtually partitions the cache space into two regions: a high-priority latency-aware region and a low-priority price-aware region. Each region is managed by a cost-aware caching scheme, which is based on GreedyDual-Size (GDS) and designed for a cloud storage scenario by adopting clean-dirty differentiation and latency normalization. The GDS-LC framework is highly flexible, and we present a further enhanced algorithm, called GDS-LCF, by incorporating access frequency in caching decisions. We have built a prototype to emulate a typical cloud client cache and evaluate GDS-LC and GDS-LCF with Amazon Simple Storage Services (S3) in three different scenarios: local cloud, Internet cloud, and heterogeneous cloud. Our experimental results show that our caching schemes can effectively achieve both optimization goals: low access latency and low monetary cost. It is our hope that this work can inspire the community to reconsider the cache design in the cloud environment, especially for the purpose of integrating cloud storage into the current storage stack as a primary layer.
C1 [Hou, Binbing; Chen, Feng] Louisiana State Univ, Dept Comp Sci & Engn, Baton Rouge, LA 70803 USA.
C3 Louisiana State University System; Louisiana State University
RP Chen, F (corresponding author), Louisiana State Univ, Dept Comp Sci & Engn, Baton Rouge, LA 70803 USA.
EM bhou@csc.lsu.edu; fchen@csc.lsu.edu
RI Hou, Binbing/K-3188-2019
FU Louisiana Board of Regents [LEQSF(2014-17)-RD-A-01]; National Science
   Foundation [CCF-1453705, CCF-1629291]; Direct For Computer & Info Scie &
   Enginr; Division of Computing and Communication Foundations [1629291]
   Funding Source: National Science Foundation; Division of Computing and
   Communication Foundations; Direct For Computer & Info Scie & Enginr
   [1453705] Funding Source: National Science Foundation
FX This work was supported in part by the Louisiana Board of Regents under
   grant LEQSF(2014-17)-RD-A-01 and the National Science Foundation under
   grants CCF-1453705 and CCF-1629291.
CR Abu-Libdeh H., 2010, P 1 ACM S CLOUD COMP
   Acharya S., 2000, FAKIR MOHANKRUTA RAM
   Amazon, 2016, AM S3 PRIC
   Amazon, 2017, AWS FREE TIER
   Amazon.com, 2016, NETFL CAS STUD
   [Anonymous], 2016, Home page
   [Anonymous], 2016, OneDrive
   [Anonymous], 2012, P USENIX FAST
   [Anonymous], 2017, Google Cloud Platform for Data Center Professionals
   Araldo Andrea, 2014, P 2014 IEEE GLOB COM
   Bazarbayev S., 2013, P 9 WORKSH HOT TOP D
   Bermudez Ignacio, 2013, P 32 ANN IEEE INT C
   Bessani A., 2014, P USENIX ANN TECHN C
   Bessani A, 2013, ACM T STORAGE, V9, DOI 10.1145/2535929
   BlueCoat, 2017, EXECUTIVE SUMMARY
   Bonvin N, 2010, P 1 ACM S CLOUD COMP
   CAO P, 1997, P 1997 USENIX S INT
   Cao Pei, 1994, P 1994 USENIX SUMM T
   Chen F, 2014, PROCEEDINGS OF THE ASME 9TH INTERNATIONAL MANUFACTURING SCIENCE AND ENGINEERING CONFERENCE, 2014, VOL 2
   Chen Feng, 2010, P 2010 INT S LOW POW
   CHOI J, 2000, P 2000 ACM SIGMETRIC
   Choi Jongmoo, 1999, P 1999 ANN USENIX TE
   ClarkNet, 2016, CLARKNET HTTP
   CTERA, 2017, HOM PAG
   Drago Idilio, 2012, P 2012 ACM INT MEAS
   Drago Idilio, 2013, P 2013 ACM INT MEAS
   Dropbox, 2016, HOM PAG
   Eaton P. R., 1999, CLUMP IMPROVING FILE
   Ford D., 2010, P 9 USENIX S OP SYST
   FORNEY BC, 2002, P 1 USENIX C FIL STO
   Glass Gideon, 1997, P 1997 ACM SIGMETRIC
   Gulati Ajay, 2011, P 2 ACM S CLOUD COMP, P1
   Hartanto Flex, 2002, P 2002 IEEE INT C MU
   Hou Binbing, 2017, ACM T STORAGE, V13
   Hou Binbing, 2016, P 32 INT C MASS STOR
   Hu Y., 2012, FAST
   InTheCloud, 2016, SPOT MOV ONT GOOGL C
   Jeong J., 2003, P 9 INT S HIGH PERF
   Jiang S, 2005, P 4 USENIX C FIL STO
   Jiang S., 2005, P 2005 USENIX ANN TE
   Jiang Song, 2002, P 2002 INT C MEAS MO
   Kim Hyojun, 2008, P 6 USENIX C FIL STO
   Kim JM, 2000, P 4 C S OP SYST DES
   Li Conglong., 2015, P 10 EUROPEAN C COMP, P1
   Li Zhenmin, 2004, P 1 USENIX C FIL STO
   Liang Shuang, 2007, P 21 INT S DISTR COM
   Mager Thomas, 2012, P 12 IEEE INT C PEER
   McDougall R., 2005, Filebench
   Megiddo N., 2003, P 2 USENIX C FIL STO
   Microsoft, 2016, ESG MICR AZ STORSIMP
   Nanopoulos A, 2003, IEEE T KNOWL DATA EN, V15, P1155, DOI 10.1109/TKDE.2003.1232270
   Nasuni, 2016, HOM PAG
   Nasuni, 2016, NAS CACH CONF
   NetApp, 2016, NETAPP STEELST CLOUD
   ONeil Elizabeth J., 1993, P 1993 ACM INT C MAN
   Ou Zhonghong, 2015, P 8 IEEE ACM INT C U
   Panzura, 2016, PANZ DEB VERS 3 0 IT
   Panzura, 2016, HOM PAG
   Patterson R. Hugo, 1995, P 15 S OP SYST PRINC
   Qureshi M., 2006, P 39 ANN IEEE ACM IN
   S3Backer, 2016, GOOGL COD ARCH S3BAC
   S3FS, 2016, GOOGL COD ARCH S3FS
   Shafiq M. Zubair, 2014, ACM SIGMETRICS Performance Evaluation Review, V42, P567, DOI 10.1145/2591971.2592021
   StorageServers, 2013, DROPB US AM S3 SERV
   StorSimple, 2016, MICR AZ STORSIMPLE
   Suh GE, 2004, J SUPERCOMPUT, V28, P7, DOI 10.1023/B:SUPE.0000014800.27383.8f
   Tang W., 2003, P 13 INT WORKSH NETW
   TwinStrata, 2016, EMC CLOUDARRAY
   VideoCache, 2017, HOM PAG
   Wang H, 2012, P 20 INT WORKSH QUAL
   Wenjin Hu, 2010, Operating Systems Review, V44, P110, DOI 10.1145/1842733.1842751
   YANG Q, 2001, P 7 ACM SIGKDD INT C
   Yang S., 2016, P 32 INT C MASS STOR
   YOUNG N, 1994, ALGORITHMICA, V11, P525, DOI 10.1007/BF01189992
   Yuan Dong, 2011, Operating Systems Review, V45, P101, DOI 10.1145/1945023.1945036
   Zhang Rui, 2011, P 4 IEEE INT C CLOUD
   Zhou Y., 2001, P 2001 USENIX ANN TE
NR 77
TC 12
Z9 12
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD DEC
PY 2017
VL 13
IS 4
SI SI
AR 40
DI 10.1145/3149374
PG 33
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FR4LD
UT WOS:000419035900013
DA 2024-07-18
ER

PT J
AU Ganesan, A
   Alagappan, R
   Arpaci-Dusseau, AC
   Arpaci-Dusseau, RH
AF Ganesan, Aishwarya
   Alagappan, Ramnatthan
   Arpaci-Dusseau, Andrea C.
   Arpaci-Dusseau, Remzi H.
TI Redundancy Does Not Imply Fault Tolerance: Analysis of Distributed
   Storage Reactions to File-System Faults
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE File-system faults; data corruption; fault tolerance
AB We analyze how modern distributed storage systems behave in the presence of file-system faults such as data corruption and read and write errors. We characterize eight popular distributed storage systems and uncover numerous problems related to file-system fault tolerance. We find that modern distributed systems do not consistently use redundancy to recover from file-system faults: a single file-system fault can cause catastrophic outcomes such as data loss, corruption, and unavailability. We also find that the above outcomes arise due to fundamental problems in file-system fault handling that are common across many systems. Our results have implications for the design of next-generation fault-tolerant distributed and cloud storage systems.
C1 [Ganesan, Aishwarya; Alagappan, Ramnatthan; Arpaci-Dusseau, Andrea C.; Arpaci-Dusseau, Remzi H.] Univ Wisconsin, 1210 W Dayton St, Madison, WI 53706 USA.
C3 University of Wisconsin System; University of Wisconsin Madison
RP Ganesan, A (corresponding author), Univ Wisconsin, 1210 W Dayton St, Madison, WI 53706 USA.
EM ag@cs.wisc.edu; ra@cs.wisc.edu; dusseau@cs.wisc.edu; remzi@cs.wisc.edu
OI Alagappan, Ramnatthan/0000-0001-9911-4208; Arpaci-Dusseau,
   Andrea/0000-0001-8618-2738
FU NSF [CNS-1419199, CNS-1421033, CNS-1319405, CNS-1218405]; DOE
   [DE-SC0014935]
FX This material was supported by funding from NSF grants CNS-1419199,
   CNS-1421033, CNS-1319405, and CNS-1218405, DOE grant DE-SC0014935, as
   well as donations from EMC, Facebook, Google, Huawei, Microsoft, NetApp,
   Samsung, Seagate, Veritas, and VMware. Any opinions, findings, and
   conclusions or recommendations expressed in this material are those of
   the authors and may not reflect the views of NSF, DOE, or other
   institutions. This article is an extended version of a FAST '17 article
   by Ganesan et al. [29]. The additional material here includes a behavior
   analysis of a few systems (Redis, Cassandra, and Kafka) in the presence
   of bit corruptions, a study of Cassandra in a different configuration,
   more graphics depicting key on-disk data structures of the various
   systems, more thorough description of the observations across systems
   with figures to aid in understanding, a summary of the results, figures
   to illustrate our fault injection methodology, and many other small
   edits and updates.
CR Adler Mark., Adler32 Collisions
   ALAGAPPAN R., 2016, P 12 USENIX C OP SYS
   Alagappan Ramnatthan, 2015, P 15 USENIX C HOT TO
   [Anonymous], P 21 ACM S OP SYST P
   [Anonymous], 2017, CORDS TOOL RESULTS
   [Anonymous], 2015, SIGMETRICS
   [Anonymous], 2014, P ACM S CLOUD COMP S
   [Anonymous], P 6 S NETW SYST DES
   [Anonymous], P INT C DEP SYST NET
   [Anonymous], 2014, LOGIN
   [Anonymous], 2016, P 14 USENIX C FIL ST
   Arpaci-Dusseau R., 2015, OPERATING SYSTEMS 3
   Bairavasundaram L. N., 2007, P 2007 ACM SIGMETRIC
   Bairavasundaram Lakshmi N., 2008, P 6 USENIX S FIL STO
   BARTON JH, 1990, IEEE T COMPUT, V39, P575, DOI 10.1109/12.54853
   Bhartia Rahul., MongoDB on AWS Guidelines and Best Practices
   Bindel D., 2000, P 9 INT C ARCH SUPP
   Brewer E., 2016, Disks for data centers
   Chou Andy, 2001, P 18 ACM S OP SYST P
   CockroachDB, RES DISK CORR STOR E
   CockroachDB, DISK CORR READ WRIT
   Correia Miguel, 2012, P 2012 USENIX ANN TE
   Data Center Knowledge, MA GNOL DAT IS GON G
   Datastax, NETFL CASS US CAS
   DataStax, READ REP REP READ PA
   Dawson S., 1996, P 2 INT COMP PERF DE
   Dean Je., Building Large-Scale Internet Services
   Elerath J, 2009, COMMUN ACM, V52, P38, DOI 10.1145/1516046.1516059
   Fiala D., 2012, P INT C HIGH PERF CO
   Fryer Daniel, 2014, P 12 USENIX S FIL ST
   Fryer Daniel, 2012, P 10 USENIX S FIL ST
   Ganesan A, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P149
   Ghemawat Sanjay., 2003, SOSP'03
   Gill Phillipa, 2011, P ACM SIGCOMM 2011 C
   Gray Jim, 1985, PN87614 TAND
   Gu W, 2003, P INT C DEP SYST NET
   Guo Huayang, 2011, P 23 ACM S OP SYST P
   Hamilton James, 2007, P 21 ANN LARG INST S
   Han Seungjae, 1995, P INT COMP PERF DEP
   Harris Robert., Data corruption is worse than you know
   Kafka, DAT CORR EIO LEADS D
   Keeton Kimberley, 2004, P 3 USENIX S FIL STO
   Kingsbury Kyle., Jepsen
   Kuris Ron., Cassandra From tarball to production
   Leesatapornwongsa Tanakorn, 2014, P 11 S OP SYST DES I
   Liu S., 2016, P 12 USENIX C OP SYS
   LogCabin, REACT DISK ERR CORR
   Mi Ningfang, 2008, P INT C DEP SYST NET
   Myers James., Data Integrity in Solid State Drives
   Narayanan I, 2016, PROCEEDINGS OF THE 9TH ACM INTERNATIONAL SYSTEMS AND STORAGE CONFERENCE (SYSTOR'16), DOI [10.1145/2964791.2901489, 10.1145/2928275.2928278]
   Netflix, CASS NETFL
   ONeil P, 1996, ACTA INFORM, V33, P351, DOI 10.1007/s002360050048
   Oracle, PREV DAT CORR HARD
   Oracle, FUS IO DAT INT
   Panzer-Steindel B., 2007, DATA INTEGRITY
   Pillai Thanumalayan Sankaranarayana, 2014, OSDI, P433
   Prabhakaran Vijayan, 2005, P INT C DEP SYST NET
   Prabhakaran Vijayan, 2005, P 20 ACM S OP SYST P
   Redis, SIL DAT CORR RED
   RethinkDB, INT READ RES
   RethinkDB, RETHINKDB DAT STOR
   RethinkDB, RETHINKDB DOC ISS
   RethinkDB, SIL DAT LOSS MET COR
   RethinkDB, RETHINKDB FAQ
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   Rubin Michael., Google moves from ext2 to ext4
   SALTZER JH, 1984, ACM T COMPUT SYST, V2, P277, DOI 10.1145/357401.357402
   Schroeder Bianca, 2007, P 5 USENIX S FIL STO
   Schroeder Bianca, 2010, P 8 USENIX S FIL STO
   Siewiorek D.P., 1993, P 23 INT S FAULT TOL
   Sivathanu Gopalan, 2005, P 2005 ACM WORKSH ST
   Spreitzer MJ, 1999, WIREL NETW, V5, P357, DOI 10.1023/A:1019175717085
   Sridharan Vilas, 2015, P 20 INT C ARCH SUPP
   Stott D. T, 2000, P 4 INT COMP PERF DE
   Subramanian Sriram, 2010, P 26 INT C DAT ENG I
   Swift Michael M., 2003, P 19 ACM S OP SYST P
   Tsai T. K., 1995, P 8 INT C MOD TECHN
   Uber, UB ENG TECH STACK 2
   Uber, UB ENG TECH STACK 1
   Verstrynge Jerome., Timestamps in Cassandra
   Wang Yang, 2013, P 10 S NETW SYST DES
   Yuan Ding, 2014, P 11 S OP SYST DES I
   Zhang Yupu, 2014, P 12 USENIX S FIL ST
   Zhang Yupu, 2010, P 8 USENIX C FIL STO
   ZooKeeper, ZOOK SERV BEC UN LEA
   ZooKeeper, CRASH DET CORR
   ZooKeeper, CLUST UN SPAC WRIT E
NR 87
TC 10
Z9 12
U1 1
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD OCT
PY 2017
VL 13
IS 3
SI SI
AR 20
DI 10.1145/3125497
PG 33
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FL6DV
UT WOS:000414336300003
OA Bronze
DA 2024-07-18
ER

PT J
AU Liu, Q
   Feng, D
   Jiang, H
   Hu, YC
   Jiao, TF
AF Liu, Qing
   Feng, Dan
   Jiang, Hong
   Hu, Yuchong
   Jiao, Tianfeng
TI Systematic Erasure Codes with Optimal Repair Bandwidth and Storage
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Distributed storage system; erasure codes; failure tolerance; repair
   bandwidth
ID REGENERATING CODES
AB Erasure codes are widely used in distributed storage systems to prevent data loss. Traditional codes suffer from a typical repair-bandwidth problem in which the amount of data required to reconstruct the lost data, referred to as the repair bandwidth, is often far more than the theoretical minimum. While many novel codes have been proposed in recent years to reduce the repair bandwidth, these codes either require extra storage and computation overhead or are only applicable to some special cases.
   To address the weaknesses of the existing solutions to the repair-bandwidth problem, we propose Z Codes, a general family of codes capable of achieving the theoretical lower bound of repair bandwidth versus storage. To the best of our knowledge, the Z codes are the first general systematic erasure codes that jointly achieve optimal repair bandwidth and storage. Further, we generalize the Z codes to the GZ codes to gain the Maximum Distance Separable (MDS) property. Our evaluations of a real system indicate that Z/GZ and Reed-Solomon (RS) codes show approximately close encoding and repairing speeds, while GZ codes achieve over 37.5% response time reduction for repairing the same size of data, compared to the RS and Cauchy Reed-Solomon (CRS) codes.
C1 [Liu, Qing; Feng, Dan; Hu, Yuchong; Jiao, Tianfeng] Huazhong Univ Sci & Technol, Minist Educ China, Sch Comp Sci & Technol, Wuhan Natl Lab Optoelect,Key Lab Informat Storage, Wuhan 430074, Hubei, Peoples R China.
   [Jiang, Hong] Univ Nebraska, Dept Comp Sci & Engn, Lincoln, NE 68588 USA.
   [Jiang, Hong] Univ Texas Arlington, Dept Comp Sci & Engn, Arlington, TX USA.
C3 Huazhong University of Science & Technology; University of Nebraska
   System; University of Nebraska Lincoln; University of Texas System;
   University of Texas Arlington
RP Feng, D (corresponding author), Huazhong Univ Sci & Technol, Minist Educ China, Sch Comp Sci & Technol, Wuhan Natl Lab Optoelect,Key Lab Informat Storage, Wuhan 430074, Hubei, Peoples R China.
EM qing@hust.edu.cn; dfeng@hust.edu.cn; jiang@cse.unl.edu;
   yuchonghu@hust.edu.cn; tfjiao@hust.edu.cn
FU National Basic Research Program of China (973 Program) [2011CB302301];
   National Natural Science Foundation of China [61025008, 61232004,
   61173043]; National High Technology Research and Development Program of
   China (863 Program) [2013AA013203]; National Key Technology R&D Program
   of China [2011BAH04B02]; Fundamental Research Funds for the Central
   Universities [2013TS043]; Natural Science Foundation of Hubei Province
   [2015CFB192]
FX This work is supported in part by National Basic Research Program of
   China (973 Program) (2011CB302301); National Natural Science Foundation
   of China (61025008, 61232004, 61173043); National High Technology
   Research and Development Program of China (863 Program) (2013AA013203);
   National Key Technology R&D Program of China (2011BAH04B02); The
   Fundamental Research Funds for the Central Universities (2013TS043);
   Natural Science Foundation of Hubei Province (2015CFB192).
CR [Anonymous], 2012, MATRIX COMPUTATIONS
   [Anonymous], P 3 USENIX C HOT TOP
   [Anonymous], 2017, UMass trace repository, UMass Smart* dataset - 2017 release
   [Anonymous], 1995, TR95048 ICSI
   Chen HCH, 2014, IEEE T COMPUT, V63, P31, DOI 10.1109/TC.2013.167
   Cheng Huang, 2012, P USENIX ATC
   Dimakis Alexandros G., 2010, IEEE T INFORM THEORY, V56, P9
   Khan Osama, 2012, P US C FIL STOR TECH
   Li Mingqiang, 2014, P 12 USENIX C FIL ST, P147
   Liu Q, 2015, IEEE ACM INT SYMP, P372, DOI 10.1109/CCGrid.2015.38
   Liu Qing, 2015, P 2015 IEEE 34 INT S
   Papailiopoulos DS, 2012, IEEE INFOCOM SER, P2801, DOI 10.1109/INFCOM.2012.6195703
   Pawar S., 2011, Proceedings of the 2011 IEEE International Symposium on Information Theory - ISIT, P2338, DOI 10.1109/ISIT.2011.6033980
   Plank J.S., 2005, Proc. of the 4th USENIX Conference on File and Storage Technologies, P1
   Plank JS, 2006, NCA 2006: FIFTH IEEE INTERNATIONAL SYMPOSIUM ON NETWORK COMPUTING AND APPLICATIONS, PROCEEDINGS, P173
   Plank James S., 2013, P 11 US C FIL STOR T
   Plank James S., 2013, P US C FIL STOR TECH
   Plank JS, 2005, SOFTWARE PRACT EXPER, V35, P189, DOI 10.1002/spe.631
   Rashmi K., 2015, USENIX C FIL STOR TE, P81
   Rashmi K., 2013, HOTSTORAGE
   REED IS, 1960, J SOC IND APPL MATH, V8, P300, DOI 10.1137/0108018
   Sathiamoorthy M., 2013, VLDB J, V6, P325
   Tamo I, 2013, IEEE T INFORM THEORY, V59, P1597, DOI 10.1109/TIT.2012.2227110
   TANNER RM, 1981, IEEE T INFORM THEORY, V27, P533, DOI 10.1109/TIT.1981.1056404
NR 24
TC 4
Z9 4
U1 2
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD OCT
PY 2017
VL 13
IS 3
SI SI
AR 26
DI 10.1145/3109479
PG 27
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FL6DV
UT WOS:000414336300009
DA 2024-07-18
ER

PT J
AU Lee, E
   Kim, J
   Bahn, H
   Lee, S
   Noh, SH
AF Lee, Eunji
   Kim, Julie
   Bahn, Hyokyung
   Lee, Sunjin
   Noh, Sam H.
TI Reducing Write Amplification of Flash Storage through Cooperative Data
   Management with NVM
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Flash memory; nonvolatile memory; endurance; write amplification;
   Design; Algorithms; Performance
AB Write amplification is a critical factor that limits the stable performance of flash-based storage systems. To reduce write amplification, this article presents a new technique that cooperatively manages data in flash storage and nonvolatile memory (NVM). Our scheme basically considers NVM as the cache of flash storage, but allows the original data in flash storage to be invalidated if there is a cached copy in NVM, which can temporarily serve as the original data. This scheme eliminates the copy-out operation for a substantial number of cached data, thereby enhancing garbage collection efficiency. Simulated results show that the proposed scheme reduces the copy-out overhead of garbage collection by 51.4% and decreases the standard deviation of response time by 35.4% on average. Measurement results obtained by implementing the proposed scheme in BlueDBM, 1 an open-source flash development platform developed by MIT, show that the proposed scheme reduces the execution time and increases IOPS by 2-21% and 3-18%, respectively, for the workloads that we considered. This article is an extended version of Lee et al. [2016], which was presented at the 32nd International Conference on Massive Data Storage Systems and Technology in 2016.
C1 [Lee, Eunji] Chungbuk Natl Univ, Dept Comp Sci, Chundaero 1, Cheongju, Chungbuk, South Korea.
   [Kim, Julie; Bahn, Hyokyung] Ewha Womans Univ, Dept Comp Sci & Engn, 52 Ewhayeodae Gil, Seoul, South Korea.
   [Lee, Sunjin] Inha Univ, Dept Comp Sci, 100 Inha Ro, Incheon, South Korea.
   [Noh, Sam H.] UNIST, Sch Elect & Comp Engn, 50 UNIST Gil, Ulsan, South Korea.
C3 Chungbuk National University; Ewha Womans University; Inha University;
   Ulsan National Institute of Science & Technology (UNIST)
RP Bahn, H (corresponding author), Ewha Womans Univ, Dept Comp Sci & Engn, 52 Ewhayeodae Gil, Seoul, South Korea.
EM eunji@cbun.ac.kr; kjulee114@gmail.com; bahn@ewha.ac.kr;
   sungjin.lee@inha.ac.kr; samhnoh@gmail.com
FU Basic Science Research Program through the National Research Foundation
   of Korea (NRF) - Ministry of Science, ICT & Future Planning
   [2015R1A2A2A05027651]; National Research Foundation of Korea (NRF) -
   Korea government (MEST) [2014R1A1A3053505]; IT R&D program MKE/KEIT
   [10041608]
FX This work was supported by Basic Science Research Program through the
   National Research Foundation of Korea (NRF) funded by the Ministry of
   Science, ICT & Future Planning (No. 2015R1A2A2A05027651) and the
   National Research Foundation of Korea (NRF) grant funded by the Korea
   government (MEST) (No. 2014R1A1A3053505) and by the IT R&D program
   MKE/KEIT (No. 10041608).
CR Agrawal Nitin, 2008, P USENIX ANN TECHN C, P57
   [Anonymous], 2009, UMASS TRAC REP
   Apalkov D, 2013, ACM J EMERG TECH COM, V9, DOI 10.1145/2463585.2463589
   Boboila Simona, 2010, P 8 USENIX C FIL STO, P115
   Condit J, 2009, SOSP'09: PROCEEDINGS OF THE TWENTY-SECOND ACM SIGOPS SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P133
   Desnoyers P., 2012, P 5 ACM INT SYST STO
   Desnoyers P, 2014, ACM T STORAGE, V10, DOI 10.1145/2577384
   Doh I. H., 2011, COMPUT J, V8, P3
   Dong Hyun Kang, 2014, 2014 22nd Annual IEEE International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems (MASCOTS). Proceedings, P239, DOI 10.1109/MASCOTS.2014.38
   Dulloor Subramanya R., 2014, P 9 EUR C COMP SYST
   Guo J., 2016, P 11 IEEE ACM IFIP I, P5
   Horn R. L., 2015, U. S. Patent, Patent No. [9,141,532, 9141532]
   Huffman A., 2014, INTEL DEV FORUM, V20
   Jagmohan A., 2010, P 26 IEEE S MASS STO
   JEDEC, 2012, MAST TRAC 128GB SSD
   Kang Jeong-Uk, 2014, 6 USENIX WORKSH HOT
   Kim J., 2002, IEEE T CONSUM ELECTR, V48, P2
   Lee BC, 2010, COMMUN ACM, V53, P99, DOI 10.1145/1785414.1785441
   Lee E., 2013, Proceedings of the 11th USENIX Conference on File and Storage Technologies, P73
   Lee E., 2016, P 32 INT C MASS DAT
   Lee S, 2014, IEEE T COMPUT, V63, P2187, DOI 10.1109/TC.2013.98
   Lee SJ, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P339
   Li Y, 2013, COMPUTER, V46, P23, DOI 10.1109/MC.2013.190
   Lu Youyou., 2013, FAST, P257
   Mogul J. C., 2009, P USENIX WORKSH HOT
   Moon S, 2016, ACM T STORAGE, V12, DOI 10.1145/2764915
   Qureshi MK, 2009, CONF PROC INT SYMP C, P24, DOI 10.1145/1555815.1555760
   Shu F., 2007, T13 TECHNICAL COMMIT
   Skourtis Dimitris, 2014, 2014 USENIX ANN TECH, P463
   Yang MC, 2013, DES AUT CON
   Yang Y., 2015, P 8 ACM INT SYST STO
   Zilberberg O, 2013, ACM COMPUT SURV, V45, DOI 10.1145/2480741.2480746
NR 32
TC 17
Z9 19
U1 1
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUN
PY 2017
VL 13
IS 2
SI SI
AR 12
DI 10.1145/3060146
PG 13
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FA1TA
UT WOS:000405221600003
DA 2024-07-18
ER

PT J
AU Zhang, YH
   Blanton, M
AF Zhang, Yihua
   Blanton, Marina
TI Efficient Dynamic Provable Possession of Remote Data via Update Trees
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Security; Verification; Algorithms; Provable data possession; outsourced
   storage; integrity verification; balanced tree
AB The emergence and wide availability of remote storage service providers prompted work in the security community that allows clients to verify integrity and availability of the data that they outsourced to a not fully trusted remote storage server at a relatively low cost. Most recent solutions to this problem allow clients to read and update (i.e., insert, modify, or delete) stored data blocks while trying to lower the overhead associated with verifying the integrity of the stored data. In this work, we develop a novel scheme, performance of which favorably compares with the existing solutions. Our solution additionally enjoys a number of new features, such as a natural support for operations on ranges of blocks, revision control, and support for multiple user access to shared content. The performance guarantees that we achieve stem from a novel data structure called a balanced update tree and removing the need for interaction during update operations in addition to communicating the updates themselves.
C1 [Zhang, Yihua; Blanton, Marina] Univ Notre Dame, Dept Comp Sci & Engn, Notre Dame, IN 46556 USA.
C3 University of Notre Dame
RP Zhang, YH; Blanton, M (corresponding author), Univ Notre Dame, Dept Comp Sci & Engn, Notre Dame, IN 46556 USA.
EM yzhang16@nd.edu; mblanton@nd.edu
OI Blanton, Marina/0009-0008-9934-2700
FU Air Force Office of Scientific Research [FA9550-09-1-0223,
   FA9550-13-1-0066]; National Science Foundation [CNS-1223699,
   CNS-1319090]
FX This work was supported in part by grant numbers FA9550-09-1-0223 and
   FA9550-13-1-0066 from the Air Force Office of Scientific Research and
   grant numbers CNS-1223699 and CNS-1319090 from the National Science
   Foundation. Any opinions, findings, and conclusions or recommendations
   expressed in this publication are those of the authors and do not
   necessarily reflect the views of the funding agencies.
CR ADELSONVELSKII GM, 1962, DOKL AKAD NAUK SSSR+, V146, P263
   Anagnostopoulos A., 2001, INFORM SECURITY, P379, DOI DOI 10.1007/3-540-45439-X
   [Anonymous], 2013, IEEE T COMPUT, DOI DOI 10.1109/TC.2011.245
   [Anonymous], STORAGESS
   [Anonymous], T STORAGE
   [Anonymous], USENIX C FIL STOR TE
   [Anonymous], DARPA INF SURV C EXP
   [Anonymous], IEEE ICC COMM INF SY
   [Anonymous], IT CLOUD SERV US S 2
   [Anonymous], SECURECOMM 08
   Ateniese G, 2007, CCS'07: PROCEEDINGS OF THE 14TH ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P598
   Ateniese G, 2009, LECT NOTES COMPUT SC, V5912, P319, DOI 10.1007/978-3-642-10366-7_19
   BENTLEY JL, 1979, INFORM PROCESS LETT, V8, P244, DOI 10.1016/0020-0190(79)90117-0
   Bowers KD, 2009, CCS'09: PROCEEDINGS OF THE 16TH ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P187
   Bowers KevinD., 2009, Proc. of ACM-CCSW '09, P43, DOI DOI 10.1145/1655008.1655015
   Chang EC, 2008, LECT NOTES COMPUT SC, V5283, P223
   Curtmola R, 2008, INT CON DISTR COMP S, P411, DOI 10.1109/ICDCS.2008.68
   de Berg M., 2000, Computational Geometry, Vsecond, P212
   Dodis Yevgeniy., 2009, TCC
   ELLIS CA, 1989, SIGMOD REC, V18, P399, DOI 10.1145/66926.66963
   Erway CC, 2009, CCS'09: PROCEEDINGS OF THE 16TH ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P213
   Goodrich MT, 2008, LECT NOTES COMPUT SC, V5222, P80, DOI 10.1007/978-3-540-85886-7_6
   Juels A, 2007, CCS'07: PROCEEDINGS OF THE 14TH ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P584
   LAMPORT L, 1978, COMMUN ACM, V21, P558, DOI 10.1145/359545.359563
   Li JY, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE SIXTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDE '04), P121
   Lifei Wei, 2010, Proceedings 2010 30th International Conference on Distributed Computing Systems Workshops (ICDCS 2010 Workshops), P52, DOI 10.1109/ICDCSW.2010.36
   Liu XF, 2013, IEEE T PARALL DISTR, V24, P1182, DOI 10.1109/TPDS.2012.331
   Oprea A, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 16TH USENIX SECURITY SYMPOSIUM, P183
   Papamanthou C, 2007, LECT NOTES COMPUT SC, V4681, P1, DOI 10.1007/978-3-540-77048-0_1
   Popa R.A., 2011, Proceedings of the 2011 USENIX conference on USENIX annual technical conference
   PUGH W, 1990, COMMUN ACM, V33, P668, DOI 10.1145/78973.78977
   Sebe F, 2008, IEEE T KNOWL DATA EN, V20, P1034, DOI 10.1109/TKDE.2007.190647
   Shacham H, 2008, LECT NOTES COMPUT SC, V5350, P90, DOI 10.1007/978-3-540-89255-7_7
   Stefanov E, 2012, 28TH ANNUAL COMPUTER SECURITY APPLICATIONS CONFERENCE (ACSAC 2012), P229
   Wang, 2012, Proc. 10th Intl Conf. Applied Cryptography and Network Security, P507, DOI DOI 10.1007/978-3-642-31284-7_30
   Wang BY, 2013, IEEE INFOCOM SER, P2904
   Wang BY, 2014, IEEE T CLOUD COMPUT, V2, P43, DOI [10.1109/TCC.2014.2299807, 10.1109/CLOUD.2012.46]
   Wang C., INT WORKSHOP QUALITY, P1
   Wang Q, 2009, LECT NOTES COMPUT SC, V5789, P355, DOI 10.1007/978-3-642-04444-1_22
   Zeng K, 2008, LECT NOTES COMPUT SC, V5308, P419, DOI 10.1007/978-3-540-88625-9_28
   Zheng QianLing Zheng QianLing, 2011, China Tropical Medicine, V11, P237, DOI 10.1145/1943513.1943546
NR 41
TC 19
Z9 20
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD FEB
PY 2016
VL 12
IS 2
AR 9
DI 10.1145/2747877
PG 45
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DJ0PO
UT WOS:000373906600004
OA Bronze
DA 2024-07-18
ER

PT J
AU Xu, LH
   Cipar, J
   Krevat, E
   Tumanov, A
   Gupta, N
   Kozuch, MA
   Ganger, GR
AF Xu, Lianghong
   Cipar, James
   Krevat, Elie
   Tumanov, Alexey
   Gupta, Nitin
   Kozuch, Michael A.
   Ganger, Gregory R.
TI Agility and Performance in Elastic Distributed Storage
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Performance; Cloud storage; distributed file systems; elastic
   storage; agility; power; write offloading
ID MAPREDUCE
AB Elastic storage systems can be expanded or contracted to meet current demand, allowing servers to be turned off or used for other tasks. However, the usefulness of an elastic distributed storage system is limited by its agility: how quickly it can increase or decrease its number of servers. Due to the large amount of data they must migrate during elastic resizing, state of the art designs usually have to make painful trade-offs among performance, elasticity, and agility.
   This article describes the state of the art in elastic storage and a new system, called SpringFS, that can quickly change its number of active servers, while retaining elasticity and performance goals. SpringFS uses a novel technique, termed bounded write offloading, that restricts the set of servers where writes to overloaded servers are redirected. This technique, combined with the read offloading and passive migration policies used in SpringFS, minimizes the work needed before deactivation or activation of servers. Analysis of real-world traces from Hadoop deployments at Facebook and various Cloudera customers and experiments with the SpringFS prototype confirm SpringFS's agility, show that it reduces the amount of data migrated for elastic resizing by up to two orders of magnitude, and show that it cuts the percentage of active servers required by 67-82%, outdoing state-of-the-art designs by 6-120%.
C1 [Xu, Lianghong; Tumanov, Alexey; Gupta, Nitin; Ganger, Gregory R.] Carnegie Mellon Univ, Dept Elect & Comp Engn, Pittsburgh, PA 15213 USA.
   [Cipar, James; Krevat, Elie] Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.
   [Kozuch, Michael A.] Intel Labs, Pittsburgh, PA USA.
C3 Carnegie Mellon University; Carnegie Mellon University; Intel
   Corporation
RP Xu, LH (corresponding author), Carnegie Mellon Univ, Dept Elect & Comp Engn, Pittsburgh, PA 15213 USA.
EM lianghon@andrew.cmu.edu
FU Intel as part of the Intel Science and Technology Center for Cloud
   Computing (ISTC-CC)
FX This research was sponsored in part by Intel as part of the Intel
   Science and Technology Center for Cloud Computing (ISTC-CC). Experiments
   were enabled by generous hardware donations from Intel, NetApp, and APC.
CR AMUR H., 2010, P 1 ACM S CLOUD COMP, P217
   [Anonymous], AP HAD PROJ
   [Anonymous], P 6 C COMP SYST EURO
   [Anonymous], ARTCULTURA UBERLANDI
   [Anonymous], 2008, UCBEECS2008127
   [Anonymous], P INFOCOM
   [Anonymous], AFS 3 PROGRAMMERS RE
   [Anonymous], MIT INTEL UNVEIL NEW
   [Anonymous], P WORKSH POW AW COMP
   [Anonymous], P 8 USENIX S OP SYST
   [Anonymous], P IEEE 9 INT S MOD A
   [Anonymous], P IEEE 10 INT S WORK
   [Anonymous], 2007, DATA INTENSIVE SUPER
   Borthakur Dhruba, 2007, Hadoop Proj. Website, V2007, P21
   Chang F, 2008, ACM T COMPUT SYST, V26, DOI 10.1145/1365815.1365816
   Chen Y, 2012, PROC VLDB ENDOW, V5, P1802, DOI 10.14778/2367502.2367519
   Dean J, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE SIXTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDE '04), P137
   Ghemawat S., 2003, ACM SIGOPS OPERATING, V37, P29, DOI [DOI 10.1145/945445.945450, DOI 10.1145/1165389.945450, 10.1145/1165389.945450]
   Isard M, 2009, SOSP'09: PROCEEDINGS OF THE TWENTY-SECOND ACM SIGOPS SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P261
   Narayanan D., 2008, ACM T STORAGE, V4, P3
   Saito Y, 2004, ACM SIGPLAN NOTICES, V39, P48, DOI 10.1145/1037187.1024400
   Weddle Charles, 2007, ACM Transactions on Storage, V3, DOI 10.1145/1288783.1289721
NR 22
TC 2
Z9 2
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD OCT
PY 2014
VL 10
IS 4
SI SI
AR 16
DI 10.1145/2668129
PG 27
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AT0LD
UT WOS:000344626700004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Lu, RM
   Xu, EC
   Zhang, YM
   Zhu, FY
   Zhu, ZS
   Wang, MT
   Zhu, ZP
   Xue, GT
   Shu, JW
   Li, ML
   Wu, JS
AF Lu, Ruiming
   Xu, Erci
   Zhang, Yiming
   Zhu, Fengyi
   Zhu, Zhaosheng
   Wang, Mengtian
   Zhu, Zongpeng
   Xue, Guangtao
   Shu, Jiwu
   Li, Minglu
   Wu, Jiesheng
TI From Missteps to Milestones: A Journey to Practical Fail-Slow Detection
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Fail-slow failures; machine learning; datasets; root cause reasoning
AB The newly emerging "fail-slow" failures plague both software and hardware where the victim components are still functioning yet with degraded performance. To address this problem, this article presents Perseus, a practical fail-slow detection framework for storage devices. Perseus leverages a light regression-based model to quickly pinpoint and analyze fail-slow failures at the granularity of drives. Within a 10-month close monitoring on 248K drives, Perseus managed to find 304 fail-slow cases. Isolating them can reduce the (node-level) 99.99th tail latency by 48%. We assemble a large-scale fail-slow dataset (including 41K normal drives and 315 verified fail-slow drives) from our production traces, based on which we provide root cause analysis on fail-slow drives covering a variety of ill-implemented scheduling, hardware defects, and environmental factors. We have released the dataset to the public for fail-slow study.
C1 [Lu, Ruiming; Xue, Guangtao] Shanghai Jiao Tong Univ, Dongchuan Rd, Shanghai 200240, Peoples R China.
   [Xu, Erci; Zhu, Fengyi; Zhu, Zhaosheng; Wang, Mengtian; Zhu, Zongpeng; Wu, Jiesheng] Alibaba Inc, Dengcai Rd, Hangzhou 310013, Peoples R China.
   [Xu, Erci] Shanghai Jiao Tong Univ, Dengcai Rd, Hangzhou 310013, Peoples R China.
   [Zhang, Yiming; Shu, Jiwu] Xiamen Univ, Siming South Rd, Xiamen 361005, Peoples R China.
   [Li, Minglu] Shanghai Jiao Tong Univ, Dongchuan Rd, Shanghai 321004, Peoples R China.
   [Li, Minglu] Zhejiang Normal Univ, Dongchuan Rd, Shanghai 321004, Peoples R China.
C3 Shanghai Jiao Tong University; Alibaba Group; Shanghai Jiao Tong
   University; Xiamen University; Shanghai Jiao Tong University; Zhejiang
   Normal University
RP Xue, GT (corresponding author), Shanghai Jiao Tong Univ, Dongchuan Rd, Shanghai 200240, Peoples R China.; Zhang, YM (corresponding author), Xiamen Univ, Siming South Rd, Xiamen 361005, Peoples R China.
EM lrm318@sjtu.edu.cn; jostep90@gmail.com; sdiris@gmail.com;
   qieqie.zfy@alibaba-inc.com; zzs222916@alibaba-inc.com;
   mengtian.wmt@alibaba-inc.com; zongpeng.zzp@alibaba-inc.com;
   gt_xue@sjtu.edu.cn; shujw@tsinghua.edu.cn; mlli@sjtu.edu.cn;
   jiesheng.wu@alibaba-inc.com
RI yiming, zhang/IXN-8360-2023
OI yiming, zhang/0009-0001-9484-9149; Shu, Jiwu/0000-0002-7362-2789
FU NSFC [62102424, 62072306]; Alibaba Innovation Research (AIR) program,
   National Key R&D Program of China [2022YFB4500302]; Program of Hunan
   Postdoc Innovation [2021RC2069]; Program of Shanghai Academic Research
   Leader [20XD1402100]
FX This research was supported by NSFC (62102424, 62072306), the Alibaba
   Innovation Research (AIR) program, National Key R&D Program of China
   (2022YFB4500302), Program of Hunan Postdoc Innovation (2021RC2069), and
   Program of Shanghai Academic Research Leader (20XD1402100). This journal
   submission presents substantial improvements of a previous version
   published in USENIX FAST 2023 (Perseus) [31]. Specifically, the journal
   submission includes more extensive experiments on evaluating Perseus and
   previous unsuccessful attempts. It also provides discussions that were
   not addressed before, e.g., regarding the design of Perseus and lessons
   learned from fail-slow root causes.
CR Abdi H, 2010, WIRES COMPUT STAT, V2, P433, DOI 10.1002/wics.101
   Alagappan Ramnatthan, 2016, P 12 USENIX S OPERAT
   Alter J, 2019, PROCEEDINGS OF SC19: THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS, DOI 10.1145/3295500.3356172
   [Anonymous], S.M.A.R.T. (Self-Monitoring, Analysis and Reporting Technology)
   Arzani B, 2018, PROCEEDINGS OF THE 15TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION (NSDI'18), P419
   Bairavasundaram LN, 2007, PERF E R SI, V35, P289
   Bairavasundaram LN, 2008, I C DEPEND SYS NETWO, P502, DOI 10.1109/DSN.2008.4630121
   Bairavasundaram LakshmiN., 2008, P 6 USENIX C FILE ST
   Cai Y, 2015, I C DEPEND SYS NETWO, P438, DOI 10.1109/DSN.2015.49
   Chandra TD, 1996, J ACM, V43, P225, DOI 10.1145/226643.226647
   Chen W, 2000, DSN 2000: INTERNATIONAL CONFERENCE ON DEPENDABLE SYSTEMS AND NETWORKS, PROCEEDINGS, P191, DOI 10.1109/ICDSN.2000.857535
   Cheong W, 2018, ISSCC DIG TECH PAP I, P338, DOI 10.1109/ISSCC.2018.8310322
   Chicco D, 2020, BMC GENOMICS, V21, DOI 10.1186/s12864-019-6413-7
   Choi B, 2021, PROCEEDINGS OF THE SIXTEENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS (EUROSYS '21), P441, DOI 10.1145/3447786.3456252
   Clement A., 2009, P 6 USENIX S NETW SY
   Corder G. W., 2014, Nonparametric statistics: a step-by-step approach, DOI DOI 10.1002/9781118165881
   Do Thanh, 2013, P 4 ANN S CLOUD COMP, DOI [10.1145/2523616.2523627, DOI 10.1145/2523616.2523627]
   Gunawi HS, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P1
   Gunawi HS, 2016, PROCEEDINGS OF THE SEVENTH ACM SYMPOSIUM ON CLOUD COMPUTING (SOCC 2016), P1, DOI 10.1145/2987550.2987583
   Gupta Trinabh, 2013, P 10 USENIX S NETW S
   Han SJ, 2021, PROCEEDINGS OF THE 19TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '21), P417
   Hao MZ, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P263
   Huang P, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P1
   Huang P, 2017, PROCEEDINGS OF THE 16TH WORKSHOP ON HOT TOPICS IN OPERATING SYSTEMS (HOTOS 2017), P150, DOI 10.1145/3102980.3103005
   Huang P, 2014, 36TH INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING (ICSE 2014), P60, DOI 10.1145/2568225.2568232
   Kuznetsov Volodymyr, 2010, P 2010 USENIX ANN TE
   Leners JB, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P279
   Li JX, 2018, EUROSYS '18: PROCEEDINGS OF THE THIRTEENTH EUROSYS CONFERENCE, DOI 10.1145/3190508.3190552
   Lou C, 2020, PROCEEDINGS OF THE 17TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P559
   Lu RM, 2022, PROCEEDINGS OF THE 2022 USENIX ANNUAL TECHNICAL CONFERENCE, P1005
   Lu Ruiming, 2023, P 21 USENIX C FILE S
   Ma A, 2015, ACM T STORAGE, V11, DOI 10.1145/2820615
   Maneas S, 2020, PROCEEDINGS OF THE 18TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P137
   MATTHEWS BW, 1975, BIOCHIM BIOPHYS ACTA, V405, P442, DOI 10.1016/0005-2795(75)90109-9
   Meza Justin, 2015, ACM SIGMETRICS Performance Evaluation Review, V43, P177, DOI 10.1145/2745844.2745848
   Narayanan I, 2016, PROCEEDINGS OF THE 9TH ACM INTERNATIONAL SYSTEMS AND STORAGE CONFERENCE (SYSTOR'16), DOI [10.1145/2964791.2901489, 10.1145/2928275.2928278]
   Panda B, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P47
   Pillai TS, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P181
   Prabhakaran Vijayan, 2005, P OF THE 20 ACM S ON, V39, P206
   Renzelmann Matthew J., 2012, P 10 USENIX S OP SYS
   Schroeder B, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P67
   Schroeder B, 2010, ACM T STORAGE, V6, DOI 10.1145/1837915.1837917
   Schroeder B, 2009, PERF E R SI, V37, P193
   Schubert E, 2017, ACM T DATABASE SYST, V42, DOI 10.1145/3068335
   Smith H., 1998, APPL REGRESSION ANAL, DOI DOI 10.1002/9781118625590.CH15
   Suminto Riza O., 2017, P 8 ACM S CLOUD COMP
   Tan C, 2019, PROCEEDINGS OF THE 16TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P599
   Tan YM, 2012, INT CON DISTR COMP S, P285, DOI 10.1109/ICDCS.2012.65
   Walker Benjamin, 2016, P STOR DEV C
   Xu E, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P961
   Zhang Q, 2018, PROCEEDINGS OF THE 15TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION (NSDI'18), P519
NR 51
TC 0
Z9 0
U1 3
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2023
VL 19
IS 4
AR 33
DI 10.1145/3617690
PG 28
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X6NQ6
UT WOS:001099601100005
DA 2024-07-18
ER

PT J
AU Sun, DS
   Tan, RX
   Chai, YP
AF Sun, Diansen
   Tan, Ruixiong
   Chai, Yunpeng
TI A Universal SMR-aware Cache Framework with Deep Optimization for DM-SMR
   and HM-SMR Disks
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Shingled magnetic recording; cache system; hybrid storage
AB To satisfy the enormous storage capacities required for big data, data centers have been adopting high-density shingledmagnetic recording (SMR) disks. However, the weak fine-grained random write performance of SMR disks caused by their inherent write amplification and unbalanced read-write performance poses a severe challenge. Many studies have proposed solid-state drive (SSD) cache systems to address this issue. However, existing cache algorithms, such as the least recently used (LRU) algorithm, which is used to optimize cache popularity, and the MOST algorithm, which is used to optimize the write amplification factor, cannot exploit the full performance of the proposed cache systems because of their inappropriate optimization objectives. This article proposes a new SMR-aware cache framework called SAC+ to improve SMR-based hybrid storage. SAC+ integrates the two dominant types of SMR drives-namely, drive-managed and host-managed SMR drives-and provides a universal framework implementation. In addition, SAC+ integrally combines the drive characteristics to optimize I/O performance. The results of evaluations conducted using real-world traces indicate that SAC+ reduces the I/O time by 36-93% compared with state-of-the-art algorithms.
C1 [Sun, Diansen; Tan, Ruixiong; Chai, Yunpeng] Renmin Univ China, 59 Zhongguancun St, Beijing 100872, Peoples R China.
C3 Renmin University of China
RP Chai, YP (corresponding author), Renmin Univ China, 59 Zhongguancun St, Beijing 100872, Peoples R China.
EM diansensun@gmail.com; tanruixiong@ruc.edu.cn; ypchai@ruc.edu.cn
OI Sun, Diansen/0000-0002-7681-9640
FU National Natural Science Foundation of China [61972402, 61972275]
FX This work is supported by the National Natural Science Foundation of
   China (Nos. 61972402 and 61972275).
CR Aghayev A, 2015, ACM T STORAGE, V11, DOI 10.1145/2821511
   Balakrishnan S., 2014, PROC ITH USENIX CONJ, P351
   Cassuto Y, 2010, IEEE S MASS STOR SYS
   Dropbox, 2018, EXT MAG POCK INN 1 P
   github, 2016, DM ZON ZON BLOCK DEV
   HGST, 2014, HGST UNV INT DYN STO
   INCITS T10 Technical Committee, 2017, 550 INCITS AM NAT ST
   ITT Committee, 2015, 537 ITT ANSI INCITS
   Lee Changman, 2015, 13 USENIX C FILE STO, P273
   Lee C, 2017, SYSTOR'17: PROCEEDINGS OF THE 10TH ACM INTERNATIONAL SYSTEMS AND STORAGE CONFERENCE, DOI 10.1145/3078468.3078479
   Lee D, 1999, PERFORMANCE EVALUATION REVIEW, SPECIAL ISSUE, VOL 27 NO 1, JUNE 1999, P134, DOI 10.1145/301464.301487
   libzbc, 2020, ZBC DEV MAN LIB
   Liu WG, 2019, J COMPUT SCI TECH-CH, V34, P61, DOI 10.1007/s11390-019-1899-7
   Ma Chenlin, 2019, ACM T EMBED COMPUT S, V18, P1
   Ma Wenjian, 2016, P 22 NAT C INF STOR
   Megiddo N, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE 2ND USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST'03), P115
   Narayanan D, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P253
   Reinsel D., 2018, DIGITIZATION WORLD E, P16
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   Seagate, 2016, SEAG ARCH HDD PROD M
   Seagate, 2015, SEAG ARCH HDD PROD M
   Song Jiang, 2002, Performance Evaluation Review, V30, P31, DOI 10.1145/511399.511340
   Sun DS, 2020, TWENTY-FIFTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXV), P1047, DOI 10.1145/3373376.3378474
   Vogler C, 2016, APPL PHYS LETT, V108, DOI 10.1063/1.4943629
   Wang C., 2017, P IEEE S MASS STORAG, P1
   Wang Y, 2013, IEEE T MAGN, V49, P5208, DOI 10.1109/TMAG.2013.2260349
   Weller D, 2014, IEEE T MAGN, V50, DOI 10.1109/TMAG.2013.2281027
   Wikipedia contributors, 2019, PEARSON CORRELATION
   Wu Fenggang, 2019, P HOTSTORAGE
   Xie XC, 2019, ACM T STORAGE, V15, DOI 10.1145/3335548
   Xie XC, 2018, IEEE ACCESS, V6, P20916, DOI 10.1109/ACCESS.2018.2825109
   Zheng Xuda, 2021, P ICA3PP
   Zhou YY, 2004, IEEE T PARALL DISTR, V15, P505, DOI 10.1109/TPDS.2004.13
   Zhu JG, 2008, IEEE T MAGN, V44, P125, DOI 10.1109/TMAG.2007.911031
NR 34
TC 1
Z9 1
U1 1
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2023
VL 19
IS 3
AR 26
DI 10.1145/3588442
PG 35
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P9QZ4
UT WOS:001053965500006
DA 2024-07-18
ER

PT J
AU Feng, GY
   Cao, HQ
   Zhu, XW
   Yu, BW
   Wang, YW
   Ma, ZX
   Chen, SQ
   Chen, WG
AF Feng, Guanyu
   Cao, Huanqi
   Zhu, Xiaowei
   Yu, Bowen
   Wang, Yuanwei
   Ma, Zixuan
   Chen, Shengqi
   Chen, Wenguang
TI TriCache: A User-Transparent Block Cache Enabling High-Performance
   Out-of-Core Processing with In-Memory Programs
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Page cache; block cache; buffer management; solid-state drives
ID SYSTEM
AB Out-of-core systems rely on high-performance cache sub-systems to reduce the number of I/O operations. Although the page cache in modern operating systems enables transparent access to memory and storage devices, it suffers from efficiency and scalability issues on cache misses, forcing out-of-core systems to design and implement their own cache components, which is a non-trivial task.
   This study proposes TriCache, a cache mechanism that enables in-memory programs to efficiently process out-of-core datasets without requiring any code rewrite. It provides a virtual memory interface on top of the conventional block interface to simultaneously achieve user transparency and sufficient out-of-core performance. A multi-level block cache design is proposed to address the challenge of per-access address translations required by a memory interface. It can exploit spatial and temporal localities in memory or storage accesses to render storage-to-memory address translation and page-level concurrency control adequately efficient for the virtual memory interface.
   Our evaluation shows that in-memory systems operating on top of TriCache can outperform Linux OS page cache by more than one order of magnitude, and can deliver performance comparable to or even better than that of corresponding counterparts designed specifically for out-of-core scenarios.
C1 [Feng, Guanyu; Cao, Huanqi; Yu, Bowen; Wang, Yuanwei; Ma, Zixuan; Chen, Shengqi; Chen, Wenguang] Tsinghua Univ, Dept Comp Sci & Technol, 1 Qinghuayuan St, Beijing 100084, Peoples R China.
   [Feng, Guanyu; Cao, Huanqi; Yu, Bowen; Wang, Yuanwei; Ma, Zixuan; Chen, Shengqi; Chen, Wenguang] Tsinghua Univ, BNRist, 1 Qinghuayuan St, Beijing 100084, Peoples R China.
   [Zhu, Xiaowei] Ant Grp, 1 East 3rd Ring Middle Rd, Beijing 100020, Peoples R China.
C3 Tsinghua University; Tsinghua University
RP Chen, WG (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, 1 Qinghuayuan St, Beijing 100084, Peoples R China.; Chen, WG (corresponding author), Tsinghua Univ, BNRist, 1 Qinghuayuan St, Beijing 100084, Peoples R China.
EM fgy18@mails.tsinghua.edu.cn; caohq18@mails.tsinghua.edu.cn;
   robert.zxw@antgroup.com; yubw15@mails.tsinghua.edu.cn;
   wangyw20@mails.tsinghua.edu.cn; ma-zx19@mails.tsinghua.edu.cn;
   csq20@mails.tsinghua.edu.cn; cwg@tsinghua.edu.cn
OI Chen, Shengqi/0000-0002-2310-5249; Feng, Guanyu/0000-0002-7754-4693;
   CHEN, WENGUANG/0000-0002-4281-1018; Wang, Yuanwei/0000-0003-4201-3654;
   Cao, Huanqi/0000-0002-3870-106X; Ma, Zixuan/0000-0001-9630-7645
FU NSFC [U20B2044]; National Key Research and Development Plan of China
   [2017YFA0604500]
FX This work was partially supported by NSFC U20B2044 and the National Key
   Research and Development Plan of China under grant 2017YFA0604500.
CR Aguilera MK, 2018, PROCEEDINGS OF THE 2018 USENIX ANNUAL TECHNICAL CONFERENCE, P775
   Amit N, 2020, PROCEEDINGS OF THE FIFTEENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS (EUROSYS'20), DOI 10.1145/3342195.3387518
   Amit N, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P27
   Bingmann T, 2016, Arxiv, DOI arXiv:1608.05634
   Boldi Paolo, 2004, P 13 INT WORLD WID W, P595, DOI DOI 10.1145/988672.988752
   Boldi Paolo, 2011, P 20 INT C WORLD WID, P587, DOI DOI 10.1145/1963405.1963488
   Boncz PA, 2008, COMMUN ACM, V51, P77, DOI 10.1145/1409360.1409380
   Cao ZC, 2020, PROCEEDINGS OF THE 18TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P209
   Costa P, 2015, ACM SIGCOMM COMP COM, V45, P551, DOI 10.1145/2829988.2787492
   Dong Siying, 2017, P 8 BIENN C INN DAT, P1
   Gao PX, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P249
   Gu JC, 2017, PROCEEDINGS OF NSDI '17: 14TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P649
   Hellerstein Joseph M, 2007, Architecture of a Database System
   Huang SS, 2010, I C DATA ENGIN WORKS, P41, DOI 10.1109/ICDEW.2010.5452747
   Kimura H, 2015, SIGMOD'15: PROCEEDINGS OF THE 2015 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P691, DOI 10.1145/2723372.2746480
   Klimovic A, 2017, TWENTY-SECOND INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXII), P345, DOI 10.1145/3037697.3037732
   Klimovic A, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901337
   Leis V, 2018, PROC INT CONF DATA, P185, DOI 10.1109/ICDE.2018.00026
   Lepers B, 2020, PROCEEDINGS OF THE 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '20), P425
   Lepers B, 2019, PROCEEDINGS OF THE TWENTY-SEVENTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '19), P447, DOI 10.1145/3341301.3359628
   Lersch L, 2020, PROC VLDB ENDOW, V13, P1091, DOI 10.14778/3384345.3384356
   Lim K, 2012, INT S HIGH PERF COMP, P189
   Lin ZY, 2014, IEEE INT CONF BIG DA, P159, DOI 10.1109/BigData.2014.7004226
   Papagiannis A, 2021, PROCEEDINGS OF THE SIXTEENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS (EUROSYS '21), P277, DOI 10.1145/3447786.3456242
   Papagiannis A, 2020, PROCEEDINGS OF THE 2020 USENIX ANNUAL TECHNICAL CONFERENCE, P813
   Papagiannis A, 2018, PROCEEDINGS OF THE 2018 ACM SYMPOSIUM ON CLOUD COMPUTING (SOCC '18), P490, DOI 10.1145/3267809.3267824
   Peng IB, 2019, PROCEEDINGS OF MCHPC'19: 2019 IEEE/ACM WORKSHOP ON MEMORY CENTRIC HIGH PERFORMANCE COMPUTING (MCHPC), P71, DOI 10.1109/MCHPC49590.2019.00017
   Priebe C. E., 2015, P 13 USENIX C FIL ST, P45
   Protic J, 1996, IEEE PARALL DISTRIB, V4, P63, DOI 10.1109/88.494605
   Raybuck A, 2021, PROCEEDINGS OF THE 28TH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, SOSP 2021, P392, DOI 10.1145/3477132.3483550
   Roghanchi S, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P342, DOI 10.1145/3132747.3132771
   Ruan ZY, 2020, PROCEEDINGS OF THE 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '20), P315
   Shan YZ, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P69
   Shan YZ, 2017, PROCEEDINGS OF THE 2017 SYMPOSIUM ON CLOUD COMPUTING (SOCC '17), P323, DOI 10.1145/3127479.3128610
   Shun JL, 2013, ACM SIGPLAN NOTICES, V48, P135, DOI 10.1145/2517327.2442530
   Singler J., 2008, P 1 INT WORKSHOP MUL, P15, DOI [10.1145/1370082.1370089, DOI 10.1145/1370082.1370089]
   Song NY, 2016, ACM T STORAGE, V12, DOI 10.1145/2846100
   Sort Benchmark Committee, ABOUT US
   Su KL, 2005, 2005 IEEE International Workshop on Safety, Security and Rescue Robots, P1
   Tanenbaum A. S., 2015, Modern operating systems, V4th
   The kernel development community, US LIN KERN DOC
   Van Essen B, 2015, CLUSTER COMPUT, V18, P15, DOI 10.1007/s10586-013-0309-0
   van Renen A, 2018, INT CONF MANAGE DATA, P1541, DOI 10.1145/3183713.3196897
   Wikipedia Contributors, 2022, MEM MAPP FIL
   Wikipedia Contributors, 2022, NVMEXPRESS
   Wikipedia Contributors, 2022, U2 WIK
   Wikipedia Contributors, 2022, PCI EXPR
   Wikipedia Contributors, 2022, MEM PAG
   Wikipedia Contributors, 2022, PAG CACH
   Yadgar G, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 5TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES ( FAST '07), P169
   Yang ZY, 2017, INT CONF CLOUD COMP, P154, DOI 10.1109/CloudCom.2017.14
   Yaniv I, 2016, SIGMETRICS/PERFORMANCE 2016: PROCEEDINGS OF THE SIGMETRICS/PERFORMANCE JOINT INTERNATIONAL CONFERENCE ON MEASUREMENT AND MODELING OF COMPUTER SCIENCE, P337, DOI [10.1145/2964791.2901456, 10.1145/2896377.2901456]
   Zaharia M, 2016, COMMUN ACM, V59, P56, DOI 10.1145/2934664
   Zheng D, 2013, INT CONF HIGH PERFOR, DOI 10.1145/2503210.2503225
   Zhong K., 2021, arXiv
   Zhou XJ, 2021, INT CONF MANAGE DATA, P2195, DOI 10.1145/3448016.3452819
   Zhu XW, 2020, PROC VLDB ENDOW, V13, P1020, DOI 10.14778/3384345.3384351
NR 57
TC 0
Z9 0
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAY
PY 2023
VL 19
IS 2
AR 15
DI 10.1145/3583139
PG 30
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D7OU8
UT WOS:000970592600006
OA Bronze
DA 2024-07-18
ER

PT J
AU Zou, XY
   Yuan, JS
   Shilane, P
   Xia, W
   Zhang, HJ
   Wang, X
AF Zou, Xiangyu
   Yuan, Jingsong
   Shilane, Philip
   Xia, Wen
   Zhang, Haijun
   Wang, Xuan
TI From Hyper-dimensional Structures to Linear Structures: Maintaining
   Deduplicated Data's Locality
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article; Proceedings Paper
CT ACM Symposium on Operating Systems Principles (SOSP)
CY OCT 26-29, 2021
CL ELECTR NETWORK
SP Assoc Comp Machinery
DE Fragmentation; restore; garbage collection
AB Data deduplication is widely used to reduce the size of backup workloads, but it has the known disadvantage of causing poor data locality, also referred to as the fragmentation problem. This results fromthe gap between the hyper-dimensional structure of deduplicated data and the sequential nature of many storage devices, and this leads to poor restore and garbage collection (GC) performance. Current research has considered writing duplicates to maintain locality (e.g., rewriting) or caching data in memory or SSD, but fragmentation continues to lower restore and GC performance.
   Investigating the locality issue, we design a method to flatten the hyper-dimensional structured deduplicated data to a one-dimensional format, which is based on classification of each chunk's lifecycle, and this creates our proposed data layout. Furthermore, we present a novelmanagement-friendly deduplication framework, called MFDedup, that applies our data layout and maintains locality as much as possible. Specifically, we use two key techniques inMFDedup: Neighbor-duplicate-focus indexing (NDF) and Across-version-aware Reorganization scheme (AVAR). NDF performs duplicate detection against a previous backup, then AVAR rearranges chunks with an offline and iterative algorithm into a compact, sequential layout, which nearly eliminates random I/O during file restores after deduplication.
   Evaluation results with five backup datasets demonstrate that, compared with state-of-the-art techniques, MFDedup achieves deduplication ratios that are 1.12x to 2.19x higher and restore throughputs that are 1.92x to 10.02x faster due to the improved data layout. While the rearranging stage introduces overheads, it is more than offset by a nearly-zero overhead GC process. Moreover, the NDF index only requires indices for two backup versions, while the traditional index grows with the number of versions retained.
C1 [Zou, Xiangyu; Yuan, Jingsong; Xia, Wen; Zhang, Haijun; Wang, Xuan] Harbin Inst Technol, Shenzhen, Peoples R China.
   [Shilane, Philip] Dell Technol, 131 Pheasant Lane, Newtown, PA 18940 USA.
   [Xia, Wen] HUST, Wuhan Natl Lab Optoelect, Wuhan, Peoples R China.
   [Xia, Wen] HIT Campus Univ Town Shenzhen, Shenzhen 518055, Peoples R China.
C3 Harbin Institute of Technology; Huazhong University of Science &
   Technology
RP Xia, W (corresponding author), Harbin Inst Technol, Shenzhen, Peoples R China.; Xia, W (corresponding author), HUST, Wuhan Natl Lab Optoelect, Wuhan, Peoples R China.; Xia, W (corresponding author), HIT Campus Univ Town Shenzhen, Shenzhen 518055, Peoples R China.
EM xiangyu.zou@hotmail.com; js.yuann@gmail.com; philip.shilane@dell.com;
   xiawen@hit.edu.cn; hjzhang@hit.edu.cn; wangxuan@cs.hitsz.edu.cn
RI Zhang, Haijun/N-8470-2015
OI Zou, Xiangyu/0000-0001-5104-8301; Shilane, Philip/0000-0003-1235-0502
FU National Natural Science Foundation of China [61972441, 61972112,
   61832004]; Guangdong Basic and Applied Basic Research Foundation
   [2021A1515012634, 2021B1515020088]; Shenzhen Science and Technology
   Program [JCYJ20210324131203009, JCYJ20190806143405318,
   JCYJ20200109113427092]; HITSZ-J&A Joint Laboratory of Digital Design and
   Intelligent Fabrication [A-2021A01]; Open Project Program of Wuhan
   National Laboratory for Optoelectronics [2018WNLOKF008]
FX This work was partly supported by the National Natural Science
   Foundation of China under Grants No. 61972441, No. 61972112, and No.
   61832004; the Guangdong Basic and Applied Basic Research Foundation
   under Grants No. 2021A1515012634 and No. 2021B1515020088; the Shenzhen
   Science and Technology Program under Grants No. JCYJ20210324131203009,
   No. JCYJ20190806143405318, and No. JCYJ20200109113427092; the HITSZ-J&A
   Joint Laboratory of Digital Design and Intelligent Fabrication under
   Grant No. HITSZ-J & A-2021A01; and the Open Project Program of Wuhan
   National Laboratory for Optoelectronics No. 2018WNLOKF008. Any opinions,
   findings, and conclusions or recommendations expressed in this material
   are those of the author(s) and do not necessarily reflect the views of
   the funding agencies.
CR Allu Y, 2018, PROCEEDINGS OF THE 2018 USENIX ANNUAL TECHNICAL CONFERENCE, P705
   Alvarez C, 2011, TR3505
   Amvrosiadis G, 2016, PROCEEDINGS OF USENIX ATC '16: 2016 USENIX ANNUAL TECHNICAL CONFERENCE, P479
   Amvrosiadis George, 2015, P USENIX ANN TECHNIC
   [Anonymous], 2011, P USENIX ANN TECHNIC
   [Anonymous], 2010, P USENIX C USENIX AN
   [Anonymous], 2010, CISC VIS NETW IND GL
   [Anonymous], 2011, PROC USENIX C FILE S
   Aronovich L., Proceedings of the 2009 Israeli Experimental Systems Conference (SYSTOR '09), p6:1
   Asaro Tony, 2007, DATA DEDUPLICATION D, P2
   Bhagwat D, 2009, 2009 IEEE INTERNATIONAL SYMPOSIUM ON MODELING, ANALYSIS & SIMULATION OF COMPUTER AND TELECOMMUNICATION SYSTEMS (MASCOTS), P237
   Bolosky WJ, 2000, USENIX ASSOCIATION PROCEEDINGS OF THE 4TH UNSENIX WINDOWS SYSTEMS SYMPOSIUM, P13
   Cao ZC, 2020, PROCEEDINGS OF THE 18TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P209
   Cao ZC, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P129
   Cao ZC, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P309
   Douglis F, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P29
   Dubnicki C., 2009, P 7 USENIX C FIL STO
   El-Shimi Ahmed, 2012, P USENIX ANN TECHNIC
   Eshghi Kave, 2005, Hewlett-Packard Labs technical report TR 30, V30
   Fu M, 2016, IEEE T PARALL DISTR, V27, P855, DOI 10.1109/TPDS.2015.2410781
   Fu Min, 2014, P USENIX ANN TECHN C
   Fu Min, 2015, P 13 USENIX C FILE S
   Harnik D, 2020, ACM T STORAGE, V15, DOI 10.1145/3369737
   IBM Corporation, 2002, CISC VIS NETW IND GL
   Intel, 2016, INT INT STOR ACC LIB
   Jamshed M, 2017, PROCEEDINGS OF NSDI '17: 14TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P113
   Jin Keren, 2009, P ISR EXP SYST C SYS
   Kaczmarczyk M., 2012, Proceedings of the 5th Annual International Systems and Storage Conference, P15
   Li YK, 2015, ACM T STORAGE, V11, DOI 10.1145/2641572
   Lillibridge Mark, 2009, PROCCEDINGS 7 C FILE
   Lillibridge Mark, 2013, P 11 USENIX C FILE S
   Mao B, 2014, ACM T STORAGE, V10, DOI 10.1145/2512348
   McClure T., 2009, EMC CENTERA OPTIMIZI
   Meister Dirk, 2013, P 6 INT SYST STOR C
   Meyer DT, 2012, ACM T STORAGE, V7, DOI 10.1145/2078861.2078864
   Min J, 2011, IEEE T COMPUT, V60, P824, DOI 10.1109/TC.2010.263
   Muthitacharoen A., 2001, Operating Systems Review, V35, P174, DOI 10.1145/502059.502052
   Nachman A, 2020, PROCEEDINGS OF THE 18TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P193
   Ni F, 2019, PROCEEDINGS OF THE 2019 TENTH ACM SYMPOSIUM ON CLOUD COMPUTING (SOCC '19), P220, DOI 10.1145/3357223.3362731
   Policroniades C, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE GENERAL TRACK 2004 USENIX ANNUAL TECHNICAL CONFERENCE, P73
   Quinlan S, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE FAST'02 CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P89
   Rhea Sean C., 2008, P USENIX ANN TECHNIC
   Shilane Philip, 2016, P 8 USENIX C HOT TOP
   Srinivasan Kiran, 2012, P 10 USENIX C FILE S
   Tarasov V., 2012, 2012 F USENIX G ANN, P261
   Vrable M., 2009, ACM Transactions on Storage, V5, P1
   Wallace Grant., 2012, Proceedings of the 10th USENIX conference on File and Storage Technologies, FAST'12, P4
   Wu J, 2019, IEEE T PARALL DISTR, V30, P119, DOI 10.1109/TPDS.2018.2852642
   Xia W, 2016, PROCEEDINGS OF USENIX ATC '16: 2016 USENIX ANNUAL TECHNICAL CONFERENCE, P101
   Xia W, 2016, P IEEE, V104, P1681, DOI 10.1109/JPROC.2016.2571298
   Xia Wen., 2011, Proceedings of the USENIX Annual Technical Conference, USENIXATC'11, P26
   You LL, 2005, PROC INT CONF DATA, P804
   Young Jin Nam, 2012, 2012 IEEE 20th International Symposium on Modelling, Analysis & Simulation of Computer and Telecommunication Systems (MASCOTS), P201, DOI 10.1109/MASCOTS.2012.32
   Youngjin Nam, 2011, Proceedings of the 2011 IEEE 13th International Conference on High Performance Computing and Communication (HPCC 2011). 2011 IEEE International Workshop on Future Trends of Distributed Computing Systems (FTDCS 2011). Workshops of the 2011 International Conference on Ubiquitous Intelligence and Computing (UIC 2011). Workshops of the 2011 International Conference on Autonomic and Trusted Computing (ATC 2011), P581, DOI 10.1109/HPCC.2011.82
   Zhang Y., 2015, PROC IEEE C COMPUT C, P1337
   Zhang YC, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P121
   Zhao NN, 2020, PROCEEDINGS OF THE 2020 USENIX ANNUAL TECHNICAL CONFERENCE, P769
   Zhu B, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P269
NR 58
TC 10
Z9 10
U1 1
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2022
VL 18
IS 3
AR 25
DI 10.1145/3507921
PG 28
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 4X9BK
UT WOS:000861131000007
OA Bronze
DA 2024-07-18
ER

PT J
AU Wang, ZG
   Chen, HB
   Wang, YY
   Tang, CZ
   Wang, H
AF Wang, Zhaoguo
   Chen, Haibo
   Wang, Youyun
   Tang, Chuzhe
   Wang, Huan
TI The Concurrent Learned Indexes for Multicore Data Storage
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Learned indexes; concurrent algorithms; indexing
ID TREE
AB We present XIndex, which is a concurrent index library and designed for fast queries. It includes a concurrent ordered index (XIndex-R) and a concurrent hash index (XIndex-H). Similar to a recent proposal of the learned index, the indexes in XIndex use learned models to optimize index efficiency. Compared with the learned index, for the ordered index, XIndex-R is able to handle concurrent writes effectively and adapts its structure according to runtime workload characteristics. For the hash index, XIndex-H is able to avoid the resize operation blocking concurrent writes. Furthermore, the indexes in XIndex can index string keys much more efficiently than the learned index. We demonstrate the advantages of XIndex with YCSB, TPC-C (KV), which is a TPC-C-inspired benchmark for key-value stores, and micro-benchmarks. Compared with ordered indexes of Masstree and Wormhole, XIndex-R achieves up to 3.2x and 4.4x performance improvement on a 24-core machine. Compared with hash indexes of Intel TBB HashMap, XIndex-H achieves up to 3.1x speedup. The performance further improves by 91% after adding the optimizations on indexing string keys. The library is open-sourced.(1)
C1 [Wang, Zhaoguo; Chen, Haibo; Wang, Youyun; Tang, Chuzhe; Wang, Huan] Shanghai Jiao Tong Univ, Shanghai AI Lab, Inst Parallel & Distributed Syst, 800 Dongchuan Rd, Shanghai, Peoples R China.
   [Wang, Zhaoguo; Chen, Haibo; Wang, Youyun; Tang, Chuzhe; Wang, Huan] Minist Educ, Engn Res Ctr Domain Specif Operating Syst, Shanghai, Peoples R China.
C3 Shanghai Jiao Tong University; Shanghai Artificial Intelligence
   Laboratory
RP Chen, HB (corresponding author), Shanghai Jiao Tong Univ, Shanghai AI Lab, Inst Parallel & Distributed Syst, 800 Dongchuan Rd, Shanghai, Peoples R China.; Chen, HB (corresponding author), Minist Educ, Engn Res Ctr Domain Specif Operating Syst, Shanghai, Peoples R China.
EM haibochen@sjtu.edu.cn
OI Tang, Chuzhe/0000-0001-7757-5708; wang, zhaoguo/0000-0002-0220-5726
FU National Natural Science Foundation of China [61902242, 61925206,
   62132014]; HighTech Support Program from Shanghai Committee of Science
   and Technology [20ZR1428100]; National Key Research and Development
   Program of China [2020AAA0108500]
FX This work is supported by the National Natural Science Foundation of
   China (No. 61902242, 61925206, 62132014), the HighTech Support Program
   from Shanghai Committee of Science and Technology (No. 20ZR1428100), and
   the National Key Research and Development Program of China (No.
   2020AAA0108500).
CR [Anonymous], 2013, P 2013 ACM SIGMOD IN, DOI DOI 10.1145/2463676.2463710
   Balmau O, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P363
   BAYER R, 1977, ACTA INFORM, V9, P1
   Bingmann Timo, 2013, STX B+ Tree C++ Template Classes
   Binna R, 2018, INT CONF MANAGE DATA, P521, DOI 10.1145/3183713.3196896
   BLOOM BH, 1970, COMMUN ACM, V13, P422, DOI 10.1145/362686.362692
   Bohannon P, 2001, SIGMOD REC, V30, P163, DOI 10.1145/376284.375681
   Bonomi F, 2006, LECT NOTES COMPUT SC, V4168, P684
   Broder A. Z, 1998, IEEE/ACM Transactions on Networking, P254
   Bronson NG, 2010, ACM SIGPLAN NOTICES, V45, P257, DOI 10.1145/1837853.1693488
   Bronson NG, 2010, PPOPP 2010: PROCEEDINGS OF THE 2010 ACM SIGPLAN SYMPOSIUM ON PRINCIPLES AND PRACTICE OF PARALLEL PROGRAMMING, P257, DOI 10.1145/1693453.1693488
   Cha S. K., 2001, Proceedings of the 27th International Conference on Very Large Data Bases, P181
   Chazelle B, 1986, ALGORITHMICA, V1, P163, DOI 10.1007/BF01840441
   Chazelle B, 1986, ALGORITHMICA, V1, P133, DOI 10.1007/BF01840440
   Chen ZY, 2020, PROCEEDINGS OF THE 2020 USENIX ANNUAL TECHNICAL CONFERENCE, P799
   Clements AT, 2012, ACM SIGPLAN NOTICES, V47, P199, DOI 10.1145/2248487.2150998
   Clements AT, 2012, ASPLOS XVII: SEVENTEENTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS, P199
   Dai YF, 2020, PROCEEDINGS OF THE 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '20), P155
   DeBrabant J, 2013, PROC VLDB ENDOW, V6, P1942, DOI 10.14778/2556549.2556575
   Ding JL, 2020, SIGMOD'20: PROCEEDINGS OF THE 2020 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P969, DOI 10.1145/3318464.3389711
   Easton William B., 1971, P 3 ACM S OP SYST PR, P95, DOI [10.1145/800212.806505, DOI 10.1145/800212.806505]
   Ferragina P, 2020, PROC VLDB ENDOW, V13, P1162, DOI 10.14778/3389133.3389135
   FREDKIN E, 1960, COMMUN ACM, V3, P490, DOI 10.1145/367390.367400
   Galakatos A, 2019, INT CONF MANAGE DATA, P1189, DOI 10.1145/3299869.3319860
   Graefe G, 2012, ACM T DATABASE SYST, V37, DOI 10.1145/2338626.2338630
   Grossi R., 2015, ACM J. Exp. Algorithmics, V19, DOI DOI 10.1145/2656332
   HERLIHY MP, 1990, ACM T PROGR LANG SYS, V12, P463, DOI 10.1145/78969.78972
   Idreos S., 2018, IEEE Data Eng. Bull, V41, P64
   Idreos S, 2018, INT CONF MANAGE DATA, P535, DOI 10.1145/3183713.3199671
   Idreos Stratos, 2019, IEEE DATA ENG B, V42, P47
   Kannan S, 2018, PROCEEDINGS OF THE 2018 USENIX ANNUAL TECHNICAL CONFERENCE, P993
   Kipf Andreas, 2020, aiDM '20: Proceedings of the Third International Workshop on Exploiting Artificial Intelligence Techniques for Data Management, DOI 10.1145/3401071.3401659
   Kraska T, 2018, INT CONF MANAGE DATA, P489, DOI 10.1145/3183713.3196909
   Kraska Tim, 2019, CIDR 2019
   KUNG HT, 1981, ACM T DATABASE SYST, V6, P213, DOI 10.1145/319566.319567
   LAMPORT L, 1977, COMMUN ACM, V20, P806, DOI 10.1145/359863.359878
   Leis V, 2013, PROC INT CONF DATA, P38, DOI 10.1109/ICDE.2013.6544812
   Lepers B, 2019, PROCEEDINGS OF THE TWENTY-SEVENTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '19), P447, DOI 10.1145/3341301.3359628
   Leskovec J, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P497
   Levandoski JJ, 2013, PROC INT CONF DATA, P302, DOI 10.1109/ICDE.2013.6544834
   Li PF, 2020, SIGMOD'20: PROCEEDINGS OF THE 2020 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P2119, DOI 10.1145/3318464.3389703
   Li Pengfei, 2019, ABS190506256 CORR
   Lu LY, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P133
   Mao Y., 2012, P 7 ACM EUR C COMP S, P183
   McKenney Paul E, 1998, Parallel and Distributed Computing and Systems, V509518, P509
   MOHAN C, 1992, ACM T DATABASE SYST, V17, P94, DOI 10.1145/128765.128770
   Nathan V, 2020, SIGMOD'20: PROCEEDINGS OF THE 2020 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P985, DOI 10.1145/3318464.3380579
   ONeil P, 1996, ACTA INFORM, V33, P351, DOI 10.1007/s002360050048
   OpenStreetMap Foundation, 2019, OPENSTREETMAP AWS
   Papagiannis A, 2018, PROCEEDINGS OF THE 2018 ACM SYMPOSIUM ON CLOUD COMPUTING (SOCC '18), P490, DOI 10.1145/3267809.3267824
   Pheatt Chuck, 2008, Journal of Computing Sciences in Colleges, V23, P298
   Putze F, 2007, LECT NOTES COMPUT SC, V4525, P108
   Schnaitter K, 2007, I C DATA ENGIN WORKS, P459, DOI 10.1109/ICDEW.2007.4401029
   Sears Russell, 2012, P ACM SIGMOD INT C M, P217
   Shalev O, 2006, J ACM, V53, P379, DOI 10.1145/1147954.1147958
   Tu S, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P18, DOI 10.1145/2517349.2522713
   Wang Peng, 2014, P 9 EUR C COMP SYST, DOI DOI 10.1145/2592798.2592804
   Wu XB, 2019, PROCEEDINGS OF THE FOURTEENTH EUROSYS CONFERENCE 2019 (EUROSYS '19), DOI 10.1145/3302424.3303955
   Yao T, 2020, PROCEEDINGS OF THE 2020 USENIX ANNUAL TECHNICAL CONFERENCE, P17
   Zhang HC, 2018, INT CONF MANAGE DATA, P323, DOI 10.1145/3183713.3196931
   Zhang H, 2016, SIGMOD'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P1567, DOI 10.1145/2882903.2915222
   Zheng Wenting., 2014, 11th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 14), P465
NR 62
TC 1
Z9 1
U1 2
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD FEB
PY 2022
VL 18
IS 1
AR 8
DI 10.1145/3478289
PG 35
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZN5OY
UT WOS:000765084700005
DA 2024-07-18
ER

PT J
AU Li, YK
   Chan, HHW
   Lee, PPC
   Xu, YL
AF Li, Yongkun
   Chan, Helen H. W.
   Lee, Patrick P. C.
   Xu, Yinlong
TI Enabling Efficient Updates in KV Storage via Hashing: Design and
   Performance Evaluation
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Key-value storage; LSM-tree; Hashing
AB Persistent key-value (KV) stores mostly build on the Log-Structured Merge (LSM) tree for high write performance, yet the LSM-tree suffers from the inherently high I/O amplification. KV separation mitigates I/O amplification by storing only keys in the LSM-tree and values in separate storage. However, the current KV separation design remains inefficient under update-intensive workloads due to its high garbage collection (GC) overhead in value storage. We propose HashKV, which aims for high update performance atop KV separation under update-intensive workloads. HashKV uses hash-based data grouping, which deterministically maps values to storage space to make both updates and GC efficient. We further relax the restriction of such deterministic mappings via simple but useful design extensions. We extensively evaluate various design aspects of HashKV. We show that Hash KV achieves 4.6x update throughput and 53.4% less write traffic compared to the current KV separation design. In addition, we demonstrate that we can integrate the design of Hash KV with state-of-the-art KV stores and improve their respective performance.
C1 [Li, Yongkun; Xu, Yinlong] Univ Sci & Technol China, Sch Comp Sci & Technol, Hefei 230027, Anhui, Peoples R China.
   [Chan, Helen H. W.; Lee, Patrick P. C.] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese University of Hong Kong
RP Lee, PPC (corresponding author), Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.
EM ykli@ustc.edu.cn; chanhwhelen@gmail.com; pclee@cse.cuhk.edu.hk;
   ylxu@ustc.edu.cn
RI Lee, Patrick P. C./I-7165-2013
OI Lee, Patrick P. C./0000-0002-4501-4364
FU Research Grants Council of Hong Kong [CRF C7036-15G]; National Nature
   Science Foundation of China [61772484, 61772486]
FX The work was supported by the Research Grants Council of Hong Kong (CRF
   C7036-15G) and National Nature Science Foundation of China (61772484 and
   61772486).
CR Agrawal Nitin, 2008, P USENIX ANN TECHN C, P57
   [Anonymous], 2012, PROC 10 USE NIX C FI
   [Anonymous], 2011, Airways (Pty) Ltd v Aviation Union of South Africa Others 2011 (3) SA 148 (SCA) paras 25-26, P25, DOI DOI 10.1145/1989323.1989327
   [Anonymous], 2010, P VLDB ENDOW
   [Anonymous], P 7 USENIX S OP SYST
   Atikoglu Berk, 2012, Performance Evaluation Review, V40, P53, DOI 10.1145/2318857.2254766
   Balmau O, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P363
   Beaver D., 2010, P USENIX OSDI
   Chan HHW, 2018, PROCEEDINGS OF THE 2018 USENIX ANNUAL TECHNICAL CONFERENCE, P1007
   Chen Luo, 2018, LSM BASED STORAGE TE
   Chen YL, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P539
   Cooper Brian F., 2010, P 1 ACM S CLOUD COMP, P143, DOI [DOI 10.1145/1807128.1807152, 10.1145/1807128.1807152]
   DeCandia Giuseppe, 2007, Operating Systems Review, V41, P205, DOI 10.1145/1323293.1294281
   Eubanks JC, 2015, 2015 IEEE 1ST WORKSHOP ON EVERYDAY VIRTUAL REALITY (WEVR), P1, DOI 10.1109/WEVR.2015.7151686
   Fan Bin, 2013, 10 USENIX S NETW SYS, P371
   Fitzpatrick Brad, 2004, Linux J., V124
   Jen-Wei Hsieh, 2006, ACM Transaction on Storage, V2, P22, DOI 10.1145/1138041.1138043
   Kavalanekar S, 2008, I S WORKL CHAR PROC, P111
   Lee Jongsung, 2013, P 6 INT SYST STOR C, V12
   Li Y., 2015, Proceedings of the 6th ACM/SPEC International Conference on Performance Engineering, P305
   Lim H, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P1
   Lim Hyeontaek, 2014, 11 USENIX S NETW SYS, P429
   Lu LY, 2017, ACM T STORAGE, V13, DOI 10.1145/3033273
   Maccormick John, 2009, ACM Transaction on Storage, V4, DOI 10.1145/1480439.1480440
   Marmol L., 2015, P USENIX ANN TECH C, P207
   Matthews J. N., 1997, Operating Systems Review, V31, P238, DOI 10.1145/269005.266700
   Nishtala R., 2013, Proceedings of the 10th USENIX Conference on Networked Systems Design and Implementation, NSDI '13, Berkeley, CA, USA, P385
   ONeil P, 1996, ACTA INFORM, V33, P351, DOI 10.1007/s002360050048
   Oracle, 2017, OR BERK DB JAV ED GE
   Papagiannis A, 2016, PROCEEDINGS OF USENIX ATC '16: 2016 USENIX ANNUAL TECHNICAL CONFERENCE, P537
   Raju P, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P497, DOI 10.1145/3132747.3132765
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   Rumble S.M., 2014, Proceedings of the 12th USENIX Conference on File and Storage Technologies (FAST 14), P1
   Sears Russell, 2012, P ACM SIGMOD INT C M, P217
   Shen ZY, 2018, ACM T STORAGE, V14, DOI 10.1145/3203410
   Shetty P.J., 2013, 11 USENIX C FILE STO, P17
   Teng DJ, 2018, ACM T STORAGE, V14, DOI 10.1145/3162615
   Ting Yao, 2017, P 33 INT C MASS STOR
   Weil SA, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P307
   Wu XN, 2015, METHODS MOL BIOL, V1306, P71, DOI 10.1007/978-1-4939-2648-0_5
   Xia F, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P349
   Yue Yinliang, 2017, IEEE T PARALL DISTR, V28, P4
   Zhang H., 2017, SCI PROGRAM, V2017, P1, DOI DOI 10.1109/coase.2017.8256292
NR 43
TC 2
Z9 3
U1 1
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2019
VL 15
IS 3
AR 20
DI 10.1145/3340287
PG 29
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JT2DC
UT WOS:000500805400005
DA 2024-07-18
ER

PT J
AU Luby, M
   Padovani, R
   Richardson, TJ
   Minder, L
   Aggarwal, P
AF Luby, Michael
   Padovani, Roberto
   Richardson, Thomas J.
   Minder, Lorenz
   Aggarwal, Pooja
TI Liquid Cloud Storage
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Distributed information systems; data storage systems; data warehouses;
   information science; information theory; information entropy; error
   compensation; time-varying channels; error correction codes;
   Reed-Solomon codes; network coding; signal-to-noise ratio; throughput;
   distributed algorithms; algorithm design and analysis; reliability;
   reliability engineering; reliability theory; fault tolerance;
   redundancy; robustness; failure analysis; equipment failure
AB A liquid system provides durable object storage based on spreading redundantly generated data across a network of hundreds to thousands of potentially unreliable storage nodes. A liquid system uses a combination of a large code, lazy repair, and flow storage organization. We show that a liquid system can be operated to enable flexible and essentially optimal combinations of storage durability, storage overhead, repair bandwidth usage, and access performance.
C1 [Luby, Michael; Minder, Lorenz; Aggarwal, Pooja] Qualcomm Technol Inc, San Diego, CA USA.
   [Luby, Michael; Minder, Lorenz; Aggarwal, Pooja] Int Comp Sci Inst, 1947 Ctr St, Berkeley, CA 94704 USA.
   [Padovani, Roberto] Univ Calif San Diego, Jacobs Sch Engn, La Jolla, CA 92093 USA.
   [Richardson, Thomas J.] Qualcomm Flarion Technol, 500 Somerset Corp Blvd, Bridgewater, NJ 08807 USA.
C3 Qualcomm; University of California System; University of California San
   Diego; Qualcomm
RP Luby, M (corresponding author), Int Comp Sci Inst, 1947 Ctr St, Berkeley, CA 94704 USA.
EM luby@icsi.berkeley.edu; roberto.padovani@gmail.com;
   tomr@qti.qualcomm.com; lminder@icsi.berkeley.edu;
   pooja@icsi.berkeley.edu
OI , Pooja Aggarwal/0009-0006-4375-4824; Luby, Michael/0000-0002-6239-8072
CR [Anonymous], HARD DRIVE DATA STAT
   [Anonymous], 2009, 5510 RFC
   [Anonymous], US ASS 7 US S OP
   [Anonymous], 2015, 7540 RFC
   [Anonymous], 2011, 6330 RFC
   Bhagwan R., 2004, S NETW SYST DES IMPL, V1
   Bloemer J., 1995, TR95048 ICSI
   Calder B., 2011, S OP SYST PRINC
   Chen Y, 2012, TECHNICAL REPORT
   Chen Y. L., 2017, USENIX ANN TECHN C M
   Cidon A., 2013, USENIX ANN TECHN C M
   Cowling J., 2016, DROPBOXS EXABYTE STO
   DIMAKIS A, 2016, IEEE INFOCOM SER, pNI421
   Dimakis A., 2016, ONLINE WIKI BIBLIO D
   Dimakis AG, 2010, IEEE T INFORM THEORY, V56, P4539, DOI 10.1109/TIT.2010.2054295
   Ford Daniel, 2010, P USENIX OSDI
   Gopalan P, 2012, IEEE T INFORM THEORY, V58, P6925, DOI 10.1109/TIT.2012.2208937
   Huang C., 2012, USENIX ANN TECHN C M
   Joshi G, 2012, ANN ALLERTON CONF, P326, DOI 10.1109/Allerton.2012.6483236
   LI R., 2017, USENIX ANN TECHN C M
   Luby M., 2016, ARXIV161003541V5
   Muralidhar S., 2014, P 11 USENIX C OP SYS, P383
   Rashmi K. V., 2014, ACM C SIGCOMM
   Rashmi K. V., 2015, 13 USENIX FIL STOR T, V13
   Recio R., 2007, RFC 5040, Tech. Rep
   Rizzo L., 1997, Computer Communication Review, V27, P24, DOI 10.1145/263876.263881
   Rodrigues R, 2005, LECT NOTES COMPUT SC, V3640, P226, DOI 10.1007/11558989_21
   Samsung, 2016, SM863A SPEC SHEET
   Sathiamoorthy M, 2013, PROC VLDB ENDOW, V6, P325, DOI 10.14778/2535573.2488339
   Shokrollahi A, 2009, FOUND TRENDS COMMUN, V6, P213, DOI 10.1561/0100000060
   Silberstein Mark., 2014, Proceedings of International Conference on Systems and Storage, P1
   Sit E., 2006, INT WORKSHOP PEER TO, V5
   Weatherspoon H, 2002, LECT NOTES COMPUT SC, V2429, P328
NR 33
TC 14
Z9 14
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD APR
PY 2019
VL 15
IS 1
SI SI
AR 2
DI 10.1145/3281276
PG 49
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HU9GE
UT WOS:000465601900002
DA 2024-07-18
ER

PT J
AU Kim, D
   Park, KH
   Youn, CH
AF Kim, Dongjin
   Park, Kyu Ho
   Youn, Chan-Hyun
TI SUPA: A Single Unified Read-Write Buffer and Pattern-Change-Aware FTL
   for the High Performance of Multi-Channel SSD
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Multi-channel SSD; NAND flash memory; Read-write unified buffer;
   Pattern-based management; Pattern handler; Flash Translation Layer
ID FLASH; TRANSLATION; MANAGEMENT; POLICY
AB To design the write buffer and flash translation layer (FTL) for a solid-state drive (SSD), previous studies have tried to increase overall SSD performance by parallel I/O and garbage collection overhead reduction. Recent works have proposed pattern-based managements, which uses the request size and read-or write-intensiveness to apply different policies to each type of data. In our observation, the locations of read and write requests are closely related, and the pattern of each type of data can be changed. In this work, we propose SUPA, a single unified read-write buffer and pattern-change-aware FTL on multi-channel SSD architecture. To increase both read and write hit ratios on the buffer based on locality, we use a single unified read-write buffer for both clean and dirty blocks. With proposed buffer, we can increase buffer hit ratio up to 8.0% and reduce 33.6% and 7.5% of read and write latencies, respectively. To handle pattern-changed blocks, we add a pattern handler between the buffer and the FTL, which monitors channel status and handles data by applying one of the two different policies according to the pattern changes. With pattern change handling process, we can reduce 1.0% and 15.4% of read and write latencies, respectively. In total, our evaluations show that SUPA can get up to 2.0 and 3.9 times less read and write latency, respectively, without loss of lifetime in comparison to previous works.
C1 [Kim, Dongjin; Park, Kyu Ho; Youn, Chan-Hyun] Korea Adv Inst Sci & Technol, Sch Elect Engn, 291 Daehak Ro, Daejeon 34141, South Korea.
C3 Korea Advanced Institute of Science & Technology (KAIST)
RP Kim, D (corresponding author), Korea Adv Inst Sci & Technol, Sch Elect Engn, 291 Daehak Ro, Daejeon 34141, South Korea.
EM dongjinkim@kaist.ac.kr; kpark@kaist.ac.kr; chyoun@kaist.ac.kr
RI Park, Kyu Ho/C-1869-2011; Youn, Chan-Hyun/C-1729-2011
FU 'The Cross-Ministry Giga KOREA Project' - Korea government(MSIT)
   [GK17P0100]; National Research Council of Science & Technology(NST) -
   Korea government (MSIT) [CAP-17-03-KISTI]
FX This work was supported by 'The Cross-Ministry Giga KOREA Project' grant
   funded by the Korea government(MSIT) (No. GK17P0100, Development of
   Tele-Experience Service SW Platform based on Giga Media) and the
   National Research Council of Science & Technology(NST) grant by the
   Korea government (MSIT) (No. CAP-17-03-KISTI).
CR [Anonymous], 2011, P ACM S APPL COMP SA
   [Anonymous], 2012, PROCEEDING IEEE VEHI
   Ban A., 1995, U. S. patent, Patent No. [US08027131, 08027131]
   Bjorling M., 2011, Extended flashsim
   Bucy John, 2008, CARNEGIE MELLON U TE
   Chang DW, 2014, ACM T DES AUTOMAT EL, V19, DOI 10.1145/2555616
   Chang LP, 2009, ACM T DES AUTOMAT EL, V15, DOI 10.1145/1640457.1640463
   Cho H, 2009, DES AUT TEST EUROPE, P507
   Dongchul Park, 2011, Proceedings of the 2011 IEEE 19th International Symposium on Modelling, Analysis & Simulation of Computer and Telecommunication Systems (MASCOTS 2011), P248, DOI 10.1109/MASCOTS.2011.29
   Gupta A, 2009, ACM SIGPLAN NOTICES, V44, P229, DOI 10.1145/1508284.1508271
   Han SJ, 2016, IEEE ICCE, DOI 10.1109/ICCE.2016.7430530
   He D, 2014, ACM T ARCHIT CODE OP, V11, DOI 10.1145/2677160
   Jo H, 2006, IEEE T CONSUM ELECTR, V52, P485, DOI 10.1109/TCE.2006.1649669
   Kang J., 2006, Proceedings of the International Conference on Embedded Software (EMSOFT), P161
   Kang SY, 2009, IEEE T COMPUT, V58, P744, DOI 10.1109/TC.2008.224
   Kim H, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P239, DOI 10.1145/1378600.1378627
   Kim Jin Hyuk, 2008, P 3 INT WORKSH SOFTW, V3, P33
   Kim JW, 2002, IEEE T CONSUM ELECTR, V48, P275, DOI 10.1109/TCE.2002.1010132
   Kim Y, 2009, SIMUL: 2009 FIRST INTERNATIONAL CONFERENCE ON ADVANCES IN SYSTEM SIMULATION, P125, DOI 10.1109/SIMUL.2009.17
   Lee SW, 2007, ACM T EMBED COMPUT S, V6, DOI 10.1145/1275986.1275990
   Lee Yong-Goo, 2008, ACM, P21
   Li-Pin Chang, 2011, 2011 9th IEEE Symposium on Embedded Systems for Real-Time Multimedia, P37, DOI 10.1109/ESTIMedia.2011.6088524
   Li-Pin Chang, 2008, 13th Asia and South Pacific Design Automation Conference ASP-DAC 2008, P428
   Ma D., 2011, Proc. of ACM SIGMOD Int. Conf. on Manage. of Data, P1
   Microsoft, 2009, SSD EXTENSION DISKSI
   MySQL Benchmark Tool, 2002, DBT2 BENCHM TOOL
   Park C, 2008, ACM T EMBED COMPUT S, V7, DOI 10.1145/1376804.1376806
   Park J, 2009, IEICE ELECTRON EXPR, V6, P297, DOI 10.1587/elex.6.297
   Park S.-y., 2006, P 2006 INT C COMP AR, P234, DOI DOI 10.1145/1176760.1176789
   Park SH, 2012, IEEE T COMPUT, V61, P134, DOI 10.1109/TC.2010.212
   SAMSUNG, 2006, DAT K9XXG08UXM NAND
   SAMSUNG, 2015, DAT SAMS SSD 850 PRO
   Sang-Phil Lim, 2010, Proceedings of the 2010 International Workshop on Storage Network Architecture and Parallel I/Os (SNAPI 2010), P3, DOI 10.1109/SNAPI.2010.9
   Se Jin Kwon, 2015, Design Automation for Embedded Systems, V19, P101, DOI 10.1007/s10617-014-9138-5
   Seo D, 2008, IEEE T CONSUM ELECTR, V54, P1228, DOI 10.1109/TCE.2008.4637611
   Seol Jinho., 2009, Proceedings of the 2009 international conference on Compilers, architecture, and synthesis for embedded systems, P137
   Seong YJ, 2010, IEEE T COMPUT, V59, P905, DOI 10.1109/TC.2010.63
   Shim G, 2012, IEEE INT C COMPUT, P445, DOI 10.1109/ICCSE.2012.68
   Shim G, 2011, ACM T STORAGE, V6, DOI 10.1145/1970338.1970339
   Sungjin Lee, 2008, Operating Systems Review, V42, P36, DOI 10.1145/1453775.1453783
   Trace Repository UMass, 2007, OLTP APPL SEARCH ENG
   Wang JH, 2011, IEEE EMBED SYST LETT, V3, P109, DOI 10.1109/LES.2011.2168941
   Wei QS, 2014, IEEE S MASS STOR SYS
   Wei Qingsong., 2011, S MASS STORAGE SYSTE, P1, DOI DOI 10.1109/MSST.2011.5937217
   Wei Xie, 2016, 2016 IEEE International Conference on Networking, Architecture and Storage (NAS), P1, DOI 10.1109/NAS.2016.7549413
   William D., 2003, NORCOTT DON CAPPS
   Wu Chin-Hsien., 2006, Proceedings of the 2006 IEEE/ACM International Conference on Computer-Aided Design (ICCAD '06), P601
   Wu GL, 2010, PROCEEDINGS OF THE FIRST SINO-USA FORUM ON ENGLISH, THE WEB AND EDUCATION 2009, P1
   Xie Wei, 2016, PARALLEL COMPUT
NR 49
TC 5
Z9 5
U1 0
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD DEC
PY 2017
VL 13
IS 4
SI SI
AR 32
DI 10.1145/3129901
PG 30
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FR4LD
UT WOS:000419035900005
DA 2024-07-18
ER

PT J
AU Wan, JG
   Xu, P
   He, XB
   Wang, JB
   Li, JY
   Xie, CS
AF Wan, Jiguang
   Xu, Peng
   He, Xubin
   Wang, Jibin
   Li, Junyao
   Xie, Changsheng
TI H-Scale: A Fast Approach to Scale Disk Arrays via Hybrid Stripe
   Deployment
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Measurement; Performance; RAID; RAID scaling; hybrid stripe
ID PERFORMANCE
AB To satisfy the explosive growth of data in large-scale data centers, where redundant arrays of independent disks (RAIDs), especially RAID-5, are widely deployed, effective storage scaling and disk expansion methods are desired. However, a way to reduce the data migration overhead and maintain the reliability of the original RAID are major concerns of storage scaling. To address these problems, we propose a new RAID scaling scheme, H-Scale, to achieve fast RAID scaling via hybrid stripe layouts. H-Scale takes advantage of the loose restriction of stripe structures to choose migrated data and to create hybrid stripe structures. The main advantages of our scheme include: (1) dramatically reducing the data migration overhead and thus speeding up the scaling process, (2) maintaining the original RAID's reliability, (3) balancing the workload among disks after scaling, and (4) providing a general scaling approach for different RAID levels. Our theoretical analysis show that H-Scale outperforms existing scaling solutions in terms of data migration, I/O overheads, and parity update operations. Evaluation results on a prototype implementation demonstrate that H-Scale speeds up the online scaling process by up to 60% under SPC traces, and similar improvements on scaling time and user response time are also achieved by evaluations using standard benchmarks.
C1 [Wan, Jiguang; Xu, Peng; Wang, Jibin; Li, Junyao; Xie, Changsheng] Huazhong Univ Sci & Technol, Wuhan Natl Lab Optoelect, Wuhan 430074, Hubei, Peoples R China.
   [He, Xubin] Virginia Commonwealth Univ, Dept Elect & Comp Engn, Richmond, VA 23284 USA.
   [He, Xubin] 601 West Main St,POB 843072, Richmond, VA 23284 USA.
C3 Huazhong University of Science & Technology; Virginia Commonwealth
   University
RP Xie, CS (corresponding author), Huazhong Univ Sci & Technol, Wuhan Natl Lab Optoelect, Wuhan 430074, Hubei, Peoples R China.
EM jgwan@hust.edu.cn; xp04193@hust.edu.cn; xhe2@vcu.edu;
   wangjibin@gmail.com; abc_11@foxmail.com; cs_xie@hust.edu.cn
RI Xu, Pengcheng/J-1429-2012
FU National Natural Science Foundation of China [61472152, 61432007,
   61572209, 61300047]; Fundamental Research Funds for the Central
   Universities [HUST: 2015QN069]; Director Fund of WNLO; Key Laboratory of
   Data Storage System, Ministry of Education; U.S. National Science
   Foundation [CNS-1320349, CNS-1218960]
FX This work was sponsored in part by the National Natural Science
   Foundation of China under Grant No. 61472152, No. 61432007, No.
   61572209, and No. 61300047; the Fundamental Research Funds for the
   Central Universities HUST: 2015QN069; the Director Fund of WNLO. This
   work is also supported by the Key Laboratory of Data Storage System,
   Ministry of Education. The work performed at VCU is partially sponsored
   by U.S. National Science Foundation under Grants No. CNS-1320349 and No.
   CNS-1218960. Any opinions, findings, and conclusions or recommendations
   expressed in this material are those of the authors and do not
   necessarily reflect the views of the funding agencies.
CR Anderson Michael H., 2002, [2002, Patent No. US], Patent No. 6442649
   [Anonymous], 2007, 27 INT C DISTRIBUTED
   Birk Y, 1997, PROCEEDINGS OF THE IEEE 7TH INTERNATIONAL WORKSHOP ON NETWORK AND OPERATING SYSTEM SUPPORT FOR DIGITAL AUDIO AND VIDEO, P13, DOI 10.1109/NOSDAV.1997.629305
   Brinkmann A., 2000, SPAA 2000. Twelfth Annual ACM Symposium on Parallel Algorithms and Architectures, P119, DOI 10.1145/341800.341815
   Brown Neil, 2007, ONLINE RAID 5 RESIZI
   Cain HW, 2001, HPCA: SEVENTH INTERNATIONAL SYMPOSIUM ON HIGH-PERFORMANCE COMPUTING ARCHITECTURE, PROCEEDINGS, P229, DOI 10.1109/HPCA.2001.903266
   Chentao Wu, 2012, 2012 41st International Conference on Parallel Processing (ICPP 2012), P460, DOI 10.1109/ICPP.2012.32
   Choy David Mun-Hien, 1998, Patent, Patent No. [US 5758118, 5758118]
   Corbett P, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P1
   Franklin Chris R., 2006, Patent No. US 7111117, Patent No. 7111117
   Gibson Garth A., 1999, P USENIX 1999 EXTR L
   Goel A, 2002, PROC INT CONF DATA, P473, DOI 10.1109/ICDE.2002.994760
   Gonzalez J., 2004, P INT WORKSH STOR NE, P17
   Hetzler S. R., 2012, U.S. Patent, Patent No. [8 239 622, 8239622]
   HOLLAND M, 1992, SIGPLAN NOTICES, V27, P23, DOI 10.1145/143371.143383
   Huang JZ, 2015, IEEE T PARALL DISTR, V26, P1704, DOI 10.1109/TPDS.2014.2326156
   I.D.C. (IDC), 2012, TECHNICAL REPORT
   Jin C, 2009, ICS'09: PROCEEDINGS OF THE 2009 ACM SIGARCH INTERNATIONAL CONFERENCE ON SUPERCOMPUTING, P360
   Katcher Jeffrey, 1997, TR3022 NETAPP INC
   Korst J, 1997, ACM MULTIMEDIA 97, PROCEEDINGS, P219, DOI 10.1145/266180.266372
   LEE EK, 1993, IEEE T COMPUT, V42, P651, DOI 10.1109/12.277289
   Legg C.B., 1999, U.S. Patent, Patent No. 6000010
   Leutenegger S. T., 1993, SIGMOD Record, V22, P22, DOI 10.1145/170036.170042
   Llanos DR, 2006, SIGMOD REC, V35, P6, DOI 10.1145/1228268.1228270
   Menon J., 1992, Digest of Papers. COMPCON Spring 1992. Thirty-Seventh IEEE Computer Society International Conference (Cat. No.92CH3098-1), P410, DOI 10.1109/CMPCON.1992.186748
   Miranda A., 2011, P 18 INT C HIGH PERF, P1
   Muntz R, 1998, INT J INTELL SYST, V13, P1137, DOI 10.1002/(SICI)1098-111X(199812)13:12<1137::AID-INT4>3.0.CO;2-M
   Muppalaneni N., 2000, Proceedings 14th International Parallel and Distributed Processing Symposium. IPDPS 2000, P663, DOI 10.1109/IPDPS.2000.846051
   Nagle D., 2004, P 2004 ACM IEEE C SU
   Patterson D. A., 1988, SIGMOD Record, V17, P109, DOI 10.1145/971701.50214
   Patterson DA, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE SIXTEENTH SYSTEMS ADMINISTRATION CONFERENCE (LISA XVI), P185
   Salem K., 1986, International Conference on Data Engineering (Cat. No.86CH2261-6), P336
   Schoenthal Scott, 2010, Patent No. [US 7694173, 7694173]
   SPC traces, 2007, SPC TRAC
   Weil SA, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P307
   Welch B, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P17
   Wilkes J, 1996, ACM T COMPUT SYST, V14, P108, DOI 10.1145/225535.225539
   Wu CT, 2012, IEEE INT C CL COMP, P284, DOI 10.1109/CLUSTER.2012.24
   Xin Q, 2003, IEEE S MASS STOR SYS, P146, DOI 10.1109/MASS.2003.1194851
   Zhang G., 2007, ACM T STORAGE, V3, P1
   Zhang GY, 2013, ACM T STORAGE, V9, DOI 10.1145/2491054
   Zhang GY, 2010, IEEE T COMPUT, V59, P345, DOI 10.1109/TC.2009.150
   Zheng W., 2011, USENIX C FILE STORAG, P149
NR 43
TC 2
Z9 2
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUN
PY 2016
VL 12
IS 3
AR 16
DI 10.1145/2822895
PG 30
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DQ7YV
UT WOS:000379426200007
DA 2024-07-18
ER

PT J
AU Zhang, J
   Jiang, XF
   Qin, X
   Ku, WS
   Alghamdi, MI
AF Zhang, Ji
   Jiang, Xunfei
   Qin, Xiao
   Ku, Wei-Shinn
   Alghamdi, Mohammed I.
TI Frog: A Framework for Context-Based File Systems
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Performance of Systems; File systems; context aware; multiview
AB This article presents a framework, Frog, for Context-Based File Systems (CBFSs) that aim at simplifying the development of context-based file systems and applications. Unlike existing informed-based context-aware systems, Frog is a unifying informed-based framework that abstracts context-specific solutions as views, allowing applications to make view selections according to application behaviors. The framework can not only eliminate overheads induced by traditional context analysis, but also simplify the interactions between the context-based file systems and applications. Rather than propagating data through solution-specific interfaces, views in Frog can be selected by inserting their names in file path strings. With Frog in place, programmers can migrate an application from one solution to another by switching among views rather than changing programming interfaces. Since the data consistency issues are automatically enforced by the framework, file-system developers can focus their attention on context-specific solutions. We implement two prototypes to demonstrate the strengths and overheads of our design. Inspired by an observation that there are more than 50% of small files (<4KB) in a file system, we create a Bi-context Archiving Virtual File System (BAVFS) that utilizes conservative and aggressive prefetching for the contexts of random and sequential reads. To improve the performance of random read-and-write operations, the Bi-context Hybrid Virtual File System (BHVFS) combines the update-in-place and update-out-of-place solutions for read-intensive and write-intensive contexts. Our experimental results show that the benefits of Frog-based CBFSs outweigh the overheads introduced by integrating multiple context-specific solutions.
C1 [Zhang, Ji; Qin, Xiao; Ku, Wei-Shinn] Auburn Univ, Dept Comp Sci & Software Engn, Shelby Ctr 3101, Auburn, AL 36849 USA.
   [Jiang, Xunfei] Earlham Coll, Dept Comp Sci, Richmond, IN 47374 USA.
   [Alghamdi, Mohammed I.] Al Baha Univ, Dept Comp Sci, Al Baha City, Saudi Arabia.
C3 Auburn University System; Auburn University; Earlham College; Al Baha
   University
RP Zhang, J (corresponding author), Auburn Univ, Dept Comp Sci & Software Engn, Shelby Ctr 3101, Auburn, AL 36849 USA.
EM jizhang@auburn.edu; jiangxu@earlham.edu; xqin@auburn.edu;
   weishinn@auburn.edu; mialmushilah@bu.edu.sa
RI Al-Ghamdi, Mohammed. I./ISU-9198-2023; Masood, Khalid/I-7688-2013;
   Al-Ghamdi, Mohammed. I./I-3651-2018
OI Al-Ghamdi, Mohammed. I./0000-0002-9794-554X; Al-Ghamdi, Mohammed.
   I./0000-0002-9794-554X; Ku, Wei Shinn/0000-0001-8636-4689
FU US National Science Foundation (CAREER) [CCF-0845257]; US National
   Science Foundation (CSR) [CNS-0757778, CNS-0917137]; US National Science
   Foundation (CPA) [CCF-0742187]; US National Science Foundation
   (CyberTrust) [CNS-0831502]; US National Science Foundation (CRI)
   [CNS-0855251]; US National Science Foundation (CITEAM) [OCI-0753305]
FX The work reported in this article was supported by the US National
   Science Foundation under Grants CCF-0845257(CAREER), CNS-0757778 (CSR),
   CCF-0742187 (CPA), CNS-0917137 (CSR), CNS-0831502 (CyberTrust),
   CNS-0855251 (CRI), and OCI-0753305 (CITEAM).
CR Agrawal Nitin, 2007, ACM Transactions on Storage, V3, DOI 10.1145/1288783.1288788
   [Anonymous], WD1600AAJS SPEC
   [Anonymous], THESIS U CALIFORNIA
   [Anonymous], THE IBM JFS PROJ
   [Anonymous], SIGOPS OPERATING SYS
   [Anonymous], 2002, DATABASE MANAGEMENT
   [Anonymous], 2009, Hadoop: The definitive guide
   [Anonymous], THE SGI XFS PROJ
   [Anonymous], 2010, P 8 USENIX C FIL STO
   Carns P., 2009, P 2009 IEEE INT S PA, P1
   Chuanpeng Li, 2007, Operating Systems Review, V41, P189, DOI 10.1145/1272998.1273017
   Fagin R., 1979, ACM Transactions on Database Systems, V4, P315, DOI 10.1145/320083.320092
   Gantz J.F., 2008, IDC white paper, V2, P1
   Gehani N.H., 1994, Proceedings of the 20th International Conference on Very Large Data Bases, VLDB'94, P249
   Ghemawat S., 2003, ACM SIGOPS OPERATING, V37, P29, DOI [DOI 10.1145/945445.945450, DOI 10.1145/1165389.945450, 10.1145/1165389.945450]
   Gu T, 2004, VTC2004-SPRING: 2004 IEEE 59TH VEHICULAR TECHNOLOGY CONFERENCE, VOLS 1-5, PROCEEDINGS, P2656
   Jain S, 2008, EUROSYS'08: PROCEEDINGS OF THE EUROSYS 2008 CONFERENCE, P95, DOI 10.1145/1357010.1352603
   Katcher J., 1997, PostMark: a New FileSystem Benchmark, P1
   Kotz D., 1991, Proceedings of the First International Conference on Parallel and Distributed Information Systems (Cat. No.91TH0393-4), P182, DOI 10.1109/PDIS.1991.183101
   Krause A, 2006, IEEE T MOBILE COMPUT, V5, P113, DOI 10.1109/TMC.2006.18
   Kroeger T. M., 1999, Proceedings of the Seventh Workshop on Hot Topics in Operating Systems, P14, DOI 10.1109/HOTOS.1999.798371
   Kroeger TM, 2001, USENIX ASSOCIATION PROCEEDINGS OF THE 2001 USENIX ANNUAL TECHNICAL CONFERENCE, P105
   LEE M, 1993, REAL-TIME SYSTEMS SYMPOSIUM: PROCEEDINGS, P98, DOI 10.1109/REAL.1993.393511
   Leverich J., 2010, ACM SIGOPS OPERATING, V44, P61
   Liang Shuang., 2007, Proceedings of the 27th IEEE International Conference on Distributed Computing Systems (ICDCS'07), P64
   Ma XN, 2001, INT CON DISTR COMP S, P31, DOI 10.1109/ICDSC.2001.918930
   Mathur Avantika, 2007, P LINUX S, V2, P21
   McKusick MarshallKirk., 1999, ATEC'99: Proceedings of the Annual Technical Conference on 1999 USENIX Annual Technical Conference, P24
   MCKUSICK MK, 1984, ACM T COMPUT SYST, V2, P181, DOI 10.1145/989.990
   Muller K., 1991, Operating Systems Review, V25, P56, DOI 10.1145/121133.286600
   Noble B. D., 1997, Operating Systems Review, V31, P276, DOI 10.1145/269005.266708
   Patil S.V., 2007, Proceedings of the 2nd international workshop on Petascale data storage: held in conjunction with Supercomputing'07, PDSW'07, P26
   Patterson R. H., 1995, Operating Systems Review, V29, P79, DOI 10.1145/224057.224064
   Piernas J., 2002, Conference Proceedings of the 2002 International Conference on SUPERCOMPUTING, P137, DOI 10.1145/514191.514213
   Roman M., 2002, IEEE Pervasive Computing, V1, P74, DOI 10.1109/MPRV.2002.1158281
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   Schindler J., 2011, Proceedings of the 9th USENIX conference on File and stroage technologies, FAST'11, P10
   Seltzer M.I., 2000, Proceedings of the annual conference on USENIX Annual Technical Conference, ATEC '00, P6
   Shvachko K., 2010, 2010 IEEE 26th Symposium on Mass Storage Systems and Technologies (MSST), P1
   Siewiorek D, 2003, SEVENTH IEEE INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS, PROCEEDINGS, P248, DOI 10.1109/ISWC.2003.1241422
   Soundararajan Gokul., 2008, P 2008 USENIX ANN TE, P377
   Sundararaman S, 2011, EUROSYS 11: PROCEEDINGS OF THE EUROSYS 2011 CONFERENCE, P77
   Tanenbaum A. S., 2006, Operating Systems Review, V40, P100, DOI 10.1145/1113361.1113364
   Veeraraghavan K, 2010, ACM T STORAGE, V6, DOI 10.1145/1837915.1837920
   Wang WG, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P145
   Weijie Chu, 2011, Proceedings of the 2011 Eighth International Conference on Information Technology: New Generations (ITNG), P1064, DOI 10.1109/ITNG.2011.189
   Weil SA, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P307
   Wright C. P., 2006, ACM Transaction on Storage, V2, P74, DOI 10.1145/1138041.1138045
   Zadok E, 1999, PROCEEDINGS OF THE 1999 USENIX ANNUAL TECHNICAL CONFERENCE, P57
   Zhang ZP, 2007, POLYM COMPOSITE, V28, P175, DOI 10.1002/pc.20281
NR 50
TC 3
Z9 3
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUL
PY 2015
VL 11
IS 3
AR 11
DI 10.1145/2720022
PG 28
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CN9RP
UT WOS:000358786900001
DA 2024-07-18
ER

PT J
AU Lee, E
   Bahn, H
AF Lee, Eunji
   Bahn, Hyokyung
TI Caching Strategies for High-Performance Storage Media
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Algorithms; Performance; Buffer cache; phase-change memory; hard
   disk; caching
ID PHASE-CHANGE MEMORY; SYSTEM; LRU
AB Due to the large access latency of hard disks during data retrieval in computer systems, buffer caching mechanisms have been studied extensively in database and operating systems. By storing requested data into the buffer cache, subsequent requests can be directly serviced without accessing slow disk storage. Meanwhile, high-speed storage media like PCM (phase-change memory) have emerged recently, and one may wonder if the traditional buffer cache will be still effective for these high-speed storage media. This article answers the question by showing that the buffer cache is still effective in such. environments due to the software overhead and the bimodal data access characteristics. Based on this observation, we present a new buffer cache management scheme appropriately designed for the system where the speed gap between cache and storage is narrow. To this end, we analyze the condition that caching will be effective and find the characteristics of access patterns that can be exploited in managing buffer cache for high performance storage like PCM.
C1 [Lee, Eunji] Seoul Natl Univ, Seoul 151, South Korea.
   [Bahn, Hyokyung] Ewha Womans Univ, Dept Comp Sci & Engn, Seoul, South Korea.
C3 Seoul National University (SNU); Ewha Womans University
RP Lee, E (corresponding author), Chungbuk Natl Univ, Dept Software, Cheongju, South Korea.
EM eunji@cbnu.ac.kr; bahn@ewha.ac.kr
FU National Research Foundation of Korea (NRF) grant - Korean government
   (MEST) [2011-0028825]
FX This work was supported by the National Research Foundation of Korea
   (NRF) grant funded by the Korean government (MEST) (No. 2011-0028825).
CR [Anonymous], 2011, FAST 5
   [Anonymous], 2008, P C FIL STOR TECHN F
   Baek S., 2006, P IEEE PIMRC HELS FI, P1
   Baek S., 2009, P WORKSH INT BETW OP
   Bailey Katelin, 2011, P 13 WORKSH HOT TOP
   Caufield A. M., 2010, P 43 ANN IEEE ACM IN
   Condit J, 2009, SOSP'09: PROCEEDINGS OF THE TWENTY-SECOND ACM SIGOPS SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P133
   Dhiman G., 2009, P 46 ACM IEEE DES AU
   Edel NK, 2004, IEEE COMPUTER SOCIETY'S 12TH ANNUAL INTERNATIONAL SYMPOSIUM ON MODELING, ANALYSIS, AND SIMULATION OF COMPUTER AND TELECOMMUNICATIONS SYSTEMS - PROCEEDINGS, P596, DOI 10.1109/MASCOT.2004.1348317
   FALOUTSOS C, 1995, IEEE T COMPUT, V44, P546, DOI 10.1109/12.376169
   Freitas RF, 2008, IBM J RES DEV, V52, P439, DOI 10.1147/rd.524.0439
   Ipek E, 2010, ACM SIGPLAN NOTICES, V45, P3, DOI 10.1145/1735971.1736023
   Jiang S, 2005, USENIX ASSOCIATION PROCEEDINGS OF THE 4TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P101
   Jiang S, 2005, IEEE T COMPUT, V54, P939, DOI 10.1109/TC.2005.130
   Jo H, 2006, IEEE T CONSUM ELECTR, V52, P485, DOI 10.1109/TCE.2006.1649669
   Johnson T., 1994, P 20 INT C VER LARG, P439
   Jung H, 2008, IEEE T CONSUM ELECTR, V54, P1215, DOI 10.1109/TCE.2008.4637609
   Kang SY, 2009, IEEE T COMPUT, V58, P744, DOI 10.1109/TC.2008.224
   Kim JM, 2000, USENIX ASSOCIATION PROCEEDINGS OF THE FOURTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P119
   Kwon, 2011, J SYST SOFTW, V84, P1507
   Lee BC, 2010, COMMUN ACM, V53, P99, DOI 10.1145/1785414.1785441
   Lee D, 2001, IEEE T COMPUT, V50, P1352, DOI 10.1109/tc.2001.970573
   Lee E, 2012, ELECTRON LETT, V48, P1053, DOI 10.1049/el.2012.1016
   Lee Eunji., 2012, Mass Storage Systems and Technologies (MSST), 2012 IEEE 28th Symposium on, P1
   Megiddo N, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE 2ND USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST'03), P115
   Mogul J.C., 2009, P 12 C HOT TOPICS OP, P14
   O'Neil E. J., 1993, SIGMOD Record, V22, P297, DOI 10.1145/170036.170081
   Park S.-y., 2006, P 2006 INT C COMP AR, P234, DOI DOI 10.1145/1176760.1176789
   Qureshi MK, 2009, CONF PROC INT SYMP C, P24, DOI 10.1145/1555815.1555760
   Seong NH, 2010, CONF PROC INT SYMP C, P383, DOI 10.1145/1816038.1816014
   Soyoon Lee, 2011, Proceedings of the 2011 IEEE 19th International Symposium on Modelling, Analysis & Simulation of Computer and Telecommunication Systems (MASCOTS 2011), P168, DOI 10.1109/MASCOTS.2011.68
   Wright C.E., 2006, Paleoecological and paleoenvironmental analysis of the middle Devonian Dundee Formation at Whitehouse, Lucas County, Ohio, P1
   Wu X, 2011, INT J ADV ROBOT SYST, V8, P1
   Zhou P, 2009, CONF PROC INT SYMP C, P14, DOI 10.1145/1555815.1555759
NR 34
TC 8
Z9 8
U1 0
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUL
PY 2014
VL 10
IS 3
AR 11
DI 10.1145/2633691
PG 22
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AN3TU
UT WOS:000340512400003
DA 2024-07-18
ER

PT J
AU Xie, YL
   Muniswamy-Reddy, KK
   Feng, D
   Li, Y
   Long, DDE
AF Xie, Yulai
   Muniswamy-Reddy, Kiran-Kumar
   Feng, Dan
   Li, Yan
   Long, Darrell D. E.
TI Evaluation of a Hybrid Approach for Efficient Provenance Storage
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
AB Provenance is the metadata that describes the history of objects. Provenance provides new functionality in a variety of areas, including experimental documentation, debugging, search, and security. As a result, a number of groups have built systems to capture provenance. Most of these systems focus on provenance collection, a few systems focus on building applications that use the provenance, but all of these systems ignore an important aspect: efficient long-term storage of provenance.
   In this article, we first analyze the provenance collected from multiple workloads and characterize the properties of provenance with respect to long-term storage. We then propose a hybrid scheme that takes advantage of the graph structure of provenance data and the inherent duplication in provenance data. Our evaluation indicates that our hybrid scheme, a combination of Web graph compression ( adapted for provenance) and dictionary encoding, provides the best trade-off in terms of compression ratio, compression time, and query performance when compared to other compression schemes.
C1 [Xie, Yulai; Feng, Dan] Huazhong Univ Sci & Technol, Sch Comp Sci, Wuhan Natl Lab Optoelect, Wuhan 430074, Peoples R China.
   [Muniswamy-Reddy, Kiran-Kumar] Harvard Univ, Sch Engn & Appl Sci, Cambridge, MA 02138 USA.
   [Li, Yan; Long, Darrell D. E.] Univ Calif Santa Cruz, Baskin Sch Engn, Santa Cruz, CA 95064 USA.
C3 Huazhong University of Science & Technology; Harvard University;
   University of California System; University of California Santa Cruz
RP Xie, YL (corresponding author), Huazhong Univ Sci & Technol, Sch Comp Sci, Wuhan Natl Lab Optoelect, 1037 Luoyu Rd, Wuhan 430074, Peoples R China.
EM yulai.xiexie@gmail.com
RI Muniswamy-Reddy, Kiran-Kumar/A-4539-2012
FU SSRC; CRIS; National Science Foundation; Los Almos National Laboratory;
   LSI; IBM Research; NetApp; Samsung Information Systems America; Seagate
   Technology; Northrop Grumman; Symantec; Hitachi; CITRIS; Department of
   Energy Office of Science; NASA Ames Research Center; Xyratex
FX The authors would like to thank the sponsors of the SSRC and CRIS,
   including the National Science Foundation, Los Almos National
   Laboratory, LSI, IBM Research, NetApp, Samsung Information Systems
   America, Seagate Technology, Northrop Grumman, Symantec, Hitachi,
   CITRIS, the Department of Energy Office of Science, the NASA Ames
   Research Center and Xyratex.
CR Adler M., 2001, P IEEE DAT COMPR C
   [Anonymous], 2009, P USENIX ANN TECHN C
   Barga R. S., 2007, CONCUR COMPUT PRACT, P1
   Boldi P., 2004, P INT DAT COMPR C
   Boldi P., 2004, P 13 INT WORLS WID W
   Boncz Peter, 2002, Thesis
   Bose Rajendra., 2004, P 16 INT C SCI STAT
   Cao B., 2009, P 3 IEEE INT WORKSH
   Chapman Adriane V., 2008, P ACM SIGMOD INT C M
   Cheah Y.-W., 2011, P 2 INT WORKSH TRAC
   Chen ZY, 2001, SIGMOD REC, V30, P271, DOI 10.1145/376284.375692
   Futrelle J., 2009, P 7 INT WORKSH MIDDL
   Goldstein J., 1998, P INT C DAT ENG
   Graefe G., 1991, 1991 Symposium on Applied Computing (Cat. No.91TH0355-8), P22, DOI 10.1109/SOAC.1991.143840
   Groth P., 2006, An architecture for provenance systems
   Groth P., 2005, P INT ACM S HIGH PER
   Jayapandian M, 2007, NUCLEIC ACIDS RES, V35, pD566, DOI 10.1093/nar/gkl859
   King S. T., 2003, P ACM S OP SYST PRIN
   Liefke H, 2000, P ACM SIGMOD INT C M
   Missier P., 2010, P 22 INT C SCI STAT
   Moreau L, 2011, FUTURE GENER COMP SY, V27, P743, DOI 10.1016/j.future.2010.07.005
   Muniswamy-Reddy K.-K., 2006, P USENIX ANN TECHN C
   Poss M., 2003, P INT C VER LARG DAT
   Randall Keith, 2001, 175 COMP SYST RES CT
   Roth M. A., 1993, SIGMOD Record, V22, P31, DOI 10.1145/163090.163096
   Shah S., 2007, P USENIX ANN TECHN C
   SIMMHAN Y, 2006, P IEEE INT C WEB SER
   Suel T., 2001, P IEEE DAT COMPR C
   Tolani PM, 2002, PROC INT CONF DATA, P225, DOI 10.1109/ICDE.2002.994712
   Vahdat A., 1997, CSD979748, P8
   Widom J., 2005, P INT C INN DAT SYST
   Witten I.H., 1999, Managing Gigabytes: Compressing and Indexing Documents and Images
   Xie Yulai, 2011, 3 WORKSH THEOR PRACT
   Xie Yulai, 2012, P 21 ACM INT C INF K
   ZIV J, 1977, IEEE T INFORM THEORY, V23, P337, DOI 10.1109/TIT.1977.1055714
   Zukowski Marcin, 2006, PROC IEEE ICDE 06, P59, DOI [10.1109/ICDE.2006.150, DOI 10.1109/ICDE.2006.150]
NR 36
TC 27
Z9 31
U1 0
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2013
VL 9
IS 4
AR 14
DI 10.1145/2501986
PG 29
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 281YI
UT WOS:000329136500004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yang, ZR
   Li, JW
   Ren, YJ
   Lee, PPC
AF Yang, Zuoru
   Li, Jingwei
   Ren, Yanjing
   Lee, Patrick P. C.
TI Tunable Encrypted Deduplication with Attack-resilient Key Management
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Encrypted deduplication; cloud storage
ID SIDE CHANNELS; SPACE
AB Conventional encrypted deduplication approaches retain the deduplication capability on duplicate chunks after encryption by always deriving the key for encryption/decryption from the chunk content, but such a deterministic nature causes information leakage due to frequency analysis. We present TED, a tunable encrypted deduplication primitive that provides a tunable mechanism for balancing the tradeoff between storage efficiency and data confidentiality. The core idea of TED is that its key derivation is based on not only the chunk content but also the number of duplicate chunk copies, such that duplicate chunks are encrypted by distinct keys in a controlled manner. In particular, TED allows users to configure a storage blowup factor, under which the information leakage quantified by an information-theoretic measure is minimized for any input workload. In addition, we extend TED with a distributed key management architecture and propose two attack-resilient key generation schemes that trade between performance and fault tolerance. We implement an encrypted deduplication prototype TEDStore to realize TED in networked environments. Evaluation on real-world file system snapshots shows that TED effectively balances the tradeoff between storage efficiency and data confidentiality, with small performance overhead.
C1 [Yang, Zuoru; Lee, Patrick P. C.] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.
   [Li, Jingwei; Ren, Yanjing] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu, Sichuan, Peoples R China.
C3 Chinese University of Hong Kong; University of Electronic Science &
   Technology of China
RP Li, JW (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu, Sichuan, Peoples R China.
EM zryang@cse.cuhk.edu.hk; jwli@uestc.edu.cn; tinoryj@gmail.com;
   pclee@cse.cuhk.edu.hk
RI Ren, Yanjing/KHE-3299-2024
OI Ren, Yanjing/0000-0001-8882-6293
FU National Natural Science Foundation of China [61972073]; Key Research
   Funds of Sichuan Province [2021YFG0167]; Sichuan Science and Technology
   Program [2020JDTD0007]; Fundamental Research Funds for Chinese Central
   Universities [ZYGX2020ZB027, ZYGX2021J018]; CUHK [2020/21 (4055148)]
FX This work was supported in part by grants by the National Natural
   Science Foundation of China (61972073), the Key Research Funds of
   Sichuan Province (2021YFG0167), the Sichuan Science and Technology
   Program (2020JDTD0007), the Fundamental Research Funds for Chinese
   Central Universities (ZYGX2020ZB027, ZYGX2021J018), and CUHK Direct
   Grant 2020/21 (4055148).
CR Abadi M, 2013, LECT NOTES COMPUT SC, V8042, P374, DOI 10.1007/978-3-642-40041-4_21
   Adya A, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE FIFTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P1
   Al-Kadit I.A., 1992, Cryptologia, V16, P97, DOI DOI 10.1080/0161-119291866801
   Amvrosiadis George., 2015, Proc. of USENIX ATC, P151
   Anderson PO, 2010, PHARMACY INFORMATICS, P1, DOI 10.1145/1852658.1852665
   [Anonymous], 2022, HIPAA Journal
   [Anonymous], 2012, P 10 USENIX C FIL ST
   [Anonymous], 2008, Proceedings of the 4th ACM international workshop on Storage security and survivability
   [Anonymous], 2015, P 2015 USENIX ANN TE
   [Anonymous], 2009, 7 USENIX C FIL STOR
   [Anonymous], 2005, HPL200530R1
   Appleby Austin, 2022, SMHASHER
   Armknecht F, 2017, PROCEEDINGS OF THE 2017 ACM ASIA CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (ASIA CCS'17), P266, DOI 10.1145/3052973.3053019
   Armknecht F, 2015, CCS'15: PROCEEDINGS OF THE 22ND ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P886, DOI 10.1145/2810103.2813630
   Ateniese G, 2007, CCS'07: PROCEEDINGS OF THE 14TH ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P598
   Baignères T, 2004, LECT NOTES COMPUT SC, V3329, P432
   Bellare M, 2013, 22 USENIX SEC S USEN, P179
   Bellare M, 2015, LECT NOTES COMPUT SC, V9020, P516, DOI 10.1007/978-3-662-46447-2_23
   Bellare M, 2013, LECT NOTES COMPUT SC, V7881, P296, DOI 10.1007/978-3-642-38348-9_18
   Bessani A., 2014, USENIX ATC 14, P169
   Bessani A, 2013, ACM T STORAGE, V9, DOI 10.1145/2535929
   Bhagwat D, 2009, 2009 IEEE INTERNATIONAL SYMPOSIUM ON MODELING, ANALYSIS & SIMULATION OF COMPUTER AND TELECOMMUNICATION SYSTEMS (MASCOTS), P237
   Bindschaedler V, 2018, PROC VLDB ENDOW, V11, P1715, DOI 10.14778/3236187.3236217
   Black J, 2006, USENIX ASSOCIATION PROCEEDINGS OF THE 2006 USENIX ANNUAL TECHNICAL CONFERENCE, P85
   Boyd S.P., 2004, Convex optimization, DOI [10.1017/CBO9780511804441, DOI 10.1017/CBO9780511804441]
   Broder AZ, 1998, COMPRESSION AND COMPLEXITY OF SEQUENCES 1997 - PROCEEDINGS, P21, DOI 10.1109/SEQUEN.1997.666900
   Cao ZC, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P309
   Cormode G, 2005, J ALGORITHMS, V55, P58, DOI 10.1016/j.jalgor.2003.12.001
   Cox LR, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE FIFTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P285, DOI 10.1145/1060289.1060316
   Cui Helei., 2018, 2018 IEEE Conference on Dependable and Secure Computing (DSC), P1
   Douceur JR, 2002, INT CON DISTR COMP S, P617, DOI 10.1109/ICDCS.2002.1022312
   Duan Y., 2014, P 6 ED ACM WORKSH CL, P57
   File System and Storage Lab at Stony Brook University, 2022, FSL TRAC SNAPSH PUBL
   Ghemawat Sanjay, 2022, LevelDB
   Goyal V., 2006, P 2006 INT C PRIVACY, P1
   Granlund Torbjorn, 2022, GNUMP GNU MULTIPLE P
   Grubbs P, 2017, P IEEE S SECUR PRIV, P655, DOI 10.1109/SP.2017.44
   Halevi S, 2011, PROCEEDINGS OF THE 18TH ACM CONFERENCE ON COMPUTER & COMMUNICATIONS SECURITY (CCS 11), P491, DOI 10.1145/2046707.2046765
   Harnik D, 2010, IEEE SECUR PRIV, V8, P40, DOI 10.1109/MSP.2010.187
   IDC, 2022, DAT AG 2025
   Intel Corporation, 2022, INTEL ADV ENCRYPTION
   Jin K., 2009, Proceedings of SYSTOR 2009: The Israeli Experimental Systems Conference, P7
   Jingwei Li, 2016, 2016 46th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN). Proceedings, P618, DOI 10.1109/DSN.2016.62
   Juels A, 2007, CCS'07: PROCEEDINGS OF THE 14TH ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P584
   Kallahalla M, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE 2ND USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST'03), P29
   Katz J., 2014, INTRO MODERN CRYPTOG
   Kerschbaum F, 2015, CCS'15: PROCEEDINGS OF THE 22ND ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P656, DOI 10.1145/2810103.2813629
   Krohn MN, 2004, P IEEE S SECUR PRIV, P226
   KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694
   Lacharité MS, 2018, IACR T SYMMETRIC CRY, V2018, P277, DOI 10.13154/tosc.v2018.i1.277-313
   Lewi K, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1167, DOI 10.1145/2976749.2978376
   Li C., 2015, P USENIX ANN TECH C, P111
   Li JA, 2021, ARCH AGRON SOIL SCI, V67, P1313, DOI 10.1080/03650340.2020.1795136
   Li JW, 2022, IEEE T COMPUT, V71, P959, DOI 10.1109/TC.2021.3067326
   Li JW, 2020, PROCEEDINGS OF THE FIFTEENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS (EUROSYS'20), DOI 10.1145/3342195.3387531
   Li JW, 2017, I C DEPEND SYS NETWO, P1, DOI 10.1109/DSN.2017.28
   Lillibridge M., 2013, FAST
   Liu J, 2018, LECT NOTES COMPUT SC, V10808, P374, DOI 10.1007/978-3-319-76953-0_20
   Liu J, 2015, CCS'15: PROCEEDINGS OF THE 22ND ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P874, DOI 10.1145/2810103.2813623
   Meister D., 2013, 11 USENIX C FIL STOR, P175
   Mendes R, 2021, IEEE T CLOUD COMPUT, V9, P1349, DOI 10.1109/TCC.2019.2916856
   Meyer DT, 2012, ACM T STORAGE, V7, DOI 10.1145/2078861.2078864
   Mulazzani Martin, 2011, P 20 USENIX C SEC SE, P5
   Naor M, 2004, J ACM, V51, P231, DOI 10.1145/972639.972643
   Naveed M, 2015, CCS'15: PROCEEDINGS OF THE 22ND ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P644, DOI 10.1145/2810103.2813651
   OpenSSL, 2022, OPENSSL CRYPT SSL TL
   Paillier P, 1999, LECT NOTES COMPUT SC, V1560, P223
   PAPADIMITRIOU CH, 1981, J ACM, V28, P765, DOI 10.1145/322276.322287
   Pooranian Z, 2018, IEEE CONF COMPUT, P444, DOI 10.1109/INFCOMW.2018.8406888
   Puzio P, 2013, INT CONF CLOUD COMP, P363, DOI 10.1109/CloudCom.2013.54
   Qin C, 2017, ACM T STORAGE, V13, DOI 10.1145/3032966
   Rabin Michael O., 1981, TR1581 HARV U DEP CO
   Resch J.K., 2011, Proc. of USENIX FAST, P14
   Ritzdorf H, 2016, CCSW'16: PROCEEDINGS OF THE 2016 ACM CLOUD COMPUTING SECURITY WORKSHOP, P61, DOI 10.1145/2996429.2996432
   SHAMIR A, 1979, COMMUN ACM, V22, P612, DOI 10.1145/359168.359176
   Stanek J, 2014, LECT NOTES COMPUT SC, V8437, P99, DOI 10.1007/978-3-662-45472-5_8
   Storer Mark W., 2009, ACM Transaction on Storage, V5, DOI 10.1145/1534912.1534914
   Storer MarkW., 2008, Proceedings of the 4th ACM international workshop on Storage security and survivability, P1
   Sun Z, 2016, IEEE S MASS STOR SYS
   Wei Zhang, 2012, 2012 IEEE 5th International Conference on Cloud Computing (CLOUD), P550, DOI 10.1109/CLOUD.2012.78
   Wilcox-O'Hearn Zooko, 2022, DREW PERTTULA ATTACK
   Xia W, 2016, PROCEEDINGS OF USENIX ATC '16: 2016 USENIX ANNUAL TECHNICAL CONFERENCE, P101
   Xia Wen., 2011, Proceedings of the USENIX Annual Technical Conference, USENIXATC'11, P26
   Yang Y, 2016, ACM T STORAGE, V12, DOI 10.1145/2908557
   Zhang W., 2015, 2015 31st Symposium on Mass Storage Systems and Technologies (MSST), P1
   Zhao YJ, 2017, PROCEEDINGS OF THE 2017 ACM ASIA CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (ASIA CCS'17), P449, DOI 10.1145/3052973.3053012
   Zhou Y., 2015, Mass Storage Systems and Technologies (MSST), 2015 31st Symposium on, P1
   Zhu B, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P269
   Zuo PF, 2018, INT PARALL DISTRIB P, P1153, DOI 10.1109/IPDPS.2018.00124
NR 89
TC 1
Z9 1
U1 1
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2022
VL 18
IS 4
AR 32
DI 10.1145/3510614
PG 38
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7F8OU
UT WOS:000902100700005
DA 2024-07-18
ER

PT J
AU Kwon, D
   Lee, W
   Kim, D
   Boo, J
   Kim, J
AF Kwon, Dongup
   Lee, Wonsik
   Kim, Dongryeong
   Boo, Junehyuk
   Kim, Jangwoo
TI SmartFVM: A Fast, Flexible, and Scalable Hardware-based Virtualization
   for Commodity Storage Devices
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Computational storage
AB A computational storage device incorporating a computation unit inside or near its storage unit is a highly promising technology to maximize a storage server's performance. However, to apply such computational storage devices and take their full potential in virtualized environments, server architects must resolve a fundamental challenge: cost-effective virtualization. This critical challenge can be directly addressed by the following questions: (1) how to virtualize two different hardware units (i.e., computation and storage), and (2) how to integrate them to construct virtual computational storage devices, and (3) how to provide them to users. However, the existing methods for computational storage virtualization severely suffer from their low performance and high costs due to the lack of hardware-assisted virtualization support.
   In this work, we propose SmartFVM-Engine, an PGA card designed to maximize the performance and cost-effectiveness of computational storage virtualization. SmartFVM-Engine introduces three key ideas to achieve the design goals. First, it achieves high virtualization performance by applying hardware-assisted virtualization to both computation and storage units. Second, it further improves the performance by applying hardware-assisted resource orchestration for the virtualid units. Third, it achieves high cost-effectiveness by dynamically constructing and scheduling virtual computational storage devices. To the best of our knowledge, this is the first work to implement a hardware-assisted virtualization mechanism for modern computational storage devices.
C1 [Kwon, Dongup; Lee, Wonsik; Kim, Dongryeong; Boo, Junehyuk; Kim, Jangwoo] Seoul Natl Univ, 1 Gwanak Ro, Seoul 08826, South Korea.
C3 Seoul National University (SNU)
RP Kim, J (corresponding author), Seoul Natl Univ, 1 Gwanak Ro, Seoul 08826, South Korea.
EM dongup@snu.ac.kr; wonsik.lee@snu.ac.kr; dongryeong@snu.ac.kr;
   junehyuk@snu.ac.kr; jangwoo@snu.ac.kr
OI Boo, Junehyuk/0000-0002-3656-6618; Kim, Jangwoo/0000-0003-2193-5748; ,
   Wonsik/0000-0002-9109-6874; Kwon, Dongup/0000-0003-0757-4165; Kim,
   dongryeong/0000-0002-4503-0137
FU National Research Foundation of Korea (NRF) - Korean Government
   [NRF-2021M3F3A2A02037893, NRF-2020M3H6A1085527]; Institute of
   Information & Communications Technology Planning & Evaluation - Korean
   Government [1711080972, 2021000853]; Creative Pioneering Researchers
   Program through Seoul National University; Automation and Systems
   Research Institute (ASRI); Inter-university Semiconductor Research
   Center at Seoul National University; Samsung Electronics
FX This work was supported in part by the Samsung Electronics, National
   Research Foundation of Korea (NRF) funded by the Korean Government under
   Grants NRF-2021M3F3A2A02037893, and NRF-2020M3H6A1085527; in part by the
   Institute of Information & Communications Technology Planning &
   Evaluation funded by the Korean Government under Grant 1711080972,
   2021000853; in part by the Creative Pioneering Researchers Program
   through Seoul National University; and in part by the Automation and
   Systems Research Institute (ASRI) and Inter-university Semiconductor
   Research Center at Seoul National University.
CR Ajay G., 2009, FAST, P85
   [Anonymous], FLEXIBLE I O TESTER
   [Anonymous], 2021, LINUX KVM
   Bergman S, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P167
   Gu B, 2016, CONF PROC INT SYMP C, P153, DOI 10.1109/ISCA.2016.23
   Jun SW, 2015, 2015 ACM/IEEE 42ND ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA), P1, DOI 10.1145/2749469.2750412
   Khawaja A, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P107
   Korolija D, 2020, PROCEEDINGS OF THE 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '20), P991
   Kwon D, 2020, PROCEEDINGS OF THE 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '20), P955
   Kwon D, 2018, CONF PROC INT SYMP C, P491, DOI 10.1109/ISCA.2018.00048
   Landau Alex, 2011, WIOV
   Lee JH, 2020, IEEE COMPUT ARCHIT L, V19, P110, DOI 10.1109/LCA.2020.3009347
   Li HC, 2020, TWENTY-FIFTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXV), P591, DOI 10.1145/3373376.3378531
   Ma JC, 2020, TWENTY-FIFTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXV), P827, DOI 10.1145/3373376.3378482
   Mailthody VS, 2019, MICRO'52: THE 52ND ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE, P224, DOI 10.1145/3352460.3358320
   Nalawade SB, 2017, 2017 INTERNATIONAL CONFERENCE ON RECENT INNOVATIONS IN SIGNAL PROCESSING AND EMBEDDED SYSTEMS (RISE), P479, DOI 10.1109/RISE.2017.8378204
   Peng B, 2018, PROCEEDINGS OF THE 2018 USENIX ANNUAL TECHNICAL CONFERENCE, P665
   QEMU, 2021, GEN OP SOURC MACH EM
   Riedel E., 1998, Proceedings of the Twenty-Fourth International Conference on Very-Large Databases, P62
   Ruan ZY, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P379
   Russell Rusty, 2008, Operating Systems Review, V42, P95, DOI 10.1145/1400097.1400108
   Schmid R, 2020, PROCEEDINGS OF THE FIFTEENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS (EUROSYS'20), DOI 10.1145/3342195.3387557
   Tavakkol A, 2018, CONF PROC INT SYMP C, P397, DOI 10.1109/ISCA.2018.00041
   Ting KK, 2002, LECT NOTES COMPUT SC, V2438, P577
   Yang ZY, 2018, 2018 IEEE 8TH INTERNATIONAL SYMPOSIUM ON CLOUD AND SERVICE COMPUTING (SC2), P67, DOI 10.1109/SC2.2018.00016
   Yoon J, 2020, ANN I S COM, P693, DOI 10.1109/ISCA45697.2020.00063
   Zha Y, 2020, TWENTY-FIFTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXV), P845, DOI 10.1145/3373376.3378491
   Zhang J, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P477
NR 28
TC 2
Z9 2
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAY
PY 2022
VL 18
IS 2
AR 12
DI 10.1145/3511213
PG 27
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 1H3XN
UT WOS:000796478700003
DA 2024-07-18
ER

PT J
AU Song, XJ
   Xie, T
   Fischer, S
AF Song, Xiaojia
   Xie, Tao
   Fischer, Stephen
TI Two Reconfigurable NDP Servers: Understanding the Impact of Near-Data
   Processing on Data Center Applications
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Near data processing; FPGA; ARM; NDP server; reconfigurability; data
   center applications; data-intensive; compute-intensive
AB Existing near-data processing (NDP)-powered architectures have demonstrated their strength for some data-intensive applications. Data center servers, however, have to serve not only data-intensive but also computeintensive applications. An in-depth understanding of the impact of NDP on various data center applications is still needed. For example, can a compute-intensive application also benefit from NDP? In addition, current NDP techniques focus on maximizing the data processing rate by always utilizing all computing resources at all times. Is this "always running in full gear" strategy consistently beneficial for an application? To answer these questions, we first propose two reconfigurable NDP-powered servers called RANS (Reconfigurable ARM-based NDP Server) and RFNS (Reconfigurable FPGA-based NDP Server). Next, we implement a single-engine prototype for each of them based on a conventional data center and then evaluate their effectiveness. Experimental results measured from the two prototypes are then extrapolated to estimate the properties of the two full-size reconfigurable NDP servers. Finally, several new findings are presented. For example, we find that while RANS can only benefit data-intensive applications, RFNS can offer benefits for both data-intensive and compute-intensive applications. Moreover, we find that for certain applications the reconfigurability of RANS/RFNS can deliver noticeable energy efficiency without any performance degradation.
C1 [Song, Xiaojia; Xie, Tao] San Diego State Univ, Dept Comp Sci, 5500 Campanile Dr, San Diego, CA 92182 USA.
   [Fischer, Stephen] Samsung Semicond Inc, 3655 N 1st St, San Jose, CA 95134 USA.
C3 California State University System; San Diego State University; Samsung
   Electronics; Samsung Semiconductor (SSI)
RP Xie, T (corresponding author), San Diego State Univ, Dept Comp Sci, 5500 Campanile Dr, San Diego, CA 92182 USA.
EM xsong2@sdsu.edu; txie@sdsu.edu; sg.fischer@samsung.com
FU Samsung Memory Solutions Laboratory (MSL)
FX This research was supported by Samsung Memory Solutions Laboratory
   (MSL). We thank our colleagues from MSL who provided insights and
   expertise that greatly assisted the research.
CR Adya A, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P739
   Ahn J, 2015, 2015 ACM/IEEE 42ND ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA), P105, DOI 10.1145/2749469.2750386
   [Anonymous], 2013, P 27 INT ACM C INT C, DOI [DOI 10.1145/2464996.2465003, 10.1145/2464996.2465003]
   [Anonymous], 2015, Allwinner a64 a quad core 64-bit arm cortex a53 soc for tablets
   Asanovic Krste, 2014, 12 USENIX C FIL STOR
   Bairavasundaram LN, 2007, PERF E R SI, V35, P289
   Balasubramonian R, 2014, IEEE MICRO, V34, P36, DOI 10.1109/MM.2014.55
   Barbalace A, 2017, PROCEEDINGS OF THE 16TH WORKSHOP ON HOT TOPICS IN OPERATING SYSTEMS (HOTOS 2017), P56, DOI 10.1145/3102980.3102990
   Bi J, 2017, IEEE T AUTOM SCI ENG, V14, P1172, DOI 10.1109/TASE.2015.2503325
   Boroumand A, 2017, IEEE COMPUT ARCHIT L, V16, P46, DOI 10.1109/LCA.2016.2577557
   Chen CLP, 2014, INFORM SCIENCES, V275, P314, DOI 10.1016/j.ins.2014.01.015
   Cho S, 2015, ACM T STORAGE, V11, DOI 10.1145/2644818
   Davidson GeorgeS., 2006, Data-centric computing with the netezza architecture
   De A, 2013, ANN IEEE SYM FIELD P, P9, DOI 10.1109/FCCM.2013.46
   Deng HW, 2014, PROC SPIE, V9247, DOI 10.1117/12.2064087
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   DIODES Incorporated, 2018, PI3DBS16212 2 1 MUX
   Do J., 2013, SIGMOD, P1221
   Ekanayake J., 2008, eScience, P277, DOI DOI 10.1109/ESCIENCE.2008.59
   Fidus Systems Inc, 2017, FID SID 100
   Gao MY, 2015, INT CONFER PARA, P113, DOI 10.1109/PACT.2015.22
   Gu B, 2016, CONF PROC INT SYMP C, P153, DOI 10.1109/ISCA.2016.23
   He HJ, 2008, 2008 INTERNATIONAL SYMPOSIUM ON INTELLIGENT INFORMATION TECHNOLOGY APPLICATION, VOL III, PROCEEDINGS, P239, DOI 10.1109/IITA.2008.461
   Intel, 2017, INT XEON GOLD 6154 P
   István Z, 2017, PROC VLDB ENDOW, V10, P1202, DOI 10.14778/3137628.3137632
   Jo I, 2016, PROC VLDB ENDOW, V9, P924
   Jun SW, 2015, 2015 ACM/IEEE 42ND ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA), P1, DOI 10.1145/2749469.2750412
   Kliazovich D., 2013, OPTICAL INTERCONNECT, P47
   Koo G, 2017, 50TH ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE (MICRO), P219, DOI 10.1145/3123939.3124553
   Mayhew D, 2003, HOT INTERCONNECTS 11, P21
   Pinheiro E, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 5TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES ( FAST '07), P17
   Putzolu D, 2000, IEEE COMMUN MAG, V38, P160, DOI 10.1109/35.825654
   Rodinia, 2009, ROD ACC COMP INT APP
   Samsung, 2017, MISS PEAK NGSFF ALL
   Samsung, 2016, SMARTSSDO COMP STOR
   Serafini M, 2014, PROC VLDB ENDOW, V7, P1035, DOI 10.14778/2732977.2732979
   Song XJ, 2019, LECT NOTES COMPUT SC, V11501, P81, DOI 10.1007/978-3-030-20656-7_5
   Song XJ, 2018, 2018 18TH IEEE/ACM INTERNATIONAL SYMPOSIUM ON CLUSTER, CLOUD AND GRID COMPUTING (CCGRID), P193, DOI 10.1109/CCGRID.2018.00034
   Taft R, 2014, PROC VLDB ENDOW, V8, P245, DOI 10.14778/2735508.2735514
   Talbot J., 2011, Proceedings of the second international workshop on MapRe- duce and its applications, P9
   Tiwari Devesh., 2013, FAST, P119
   Woods L, 2014, PROC VLDB ENDOW, V7, P963, DOI 10.14778/2732967.2732972
   Wu C., 2016, ARXIV160103115
   Wu X, 2011, ACM SIGPLAN NOTICES, V46, P113, DOI 10.1145/2038037.1941569
   Wulf W. A., 1995, Computer Architecture News, V23, P20, DOI 10.1145/216585.216588
   Xilinx, 2017, XIL XIL VIRT ULTRASC
   Yoshimi M, 2017, ACM T RECONFIG TECHN, V10, DOI 10.1145/3079759
   Zhang D., 2014, P 23 INT S HIGH PERF
NR 48
TC 4
Z9 4
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2021
VL 17
IS 4
AR 31
DI 10.1145/3460201
PG 27
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YY2GY
UT WOS:000754612700007
DA 2024-07-18
ER

PT J
AU Li, XL
   Yang, ZR
   Li, JH
   Li, RH
   Lee, PPC
   Huang, Q
   Hu, YC
AF Li, Xiaolu
   Yang, Zuoru
   Li, Jinhong
   Li, Runhui
   Lee, Patrick P. C.
   Huang, Qun
   Hu, Yuchong
TI Repair Pipelining for Erasure-coded Storage: Algorithms and Evaluation
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Erasure coding; distributed storage systems
ID RECONSTRUCTION; MAPREDUCE
AB We propose repair pipelining, a technique that speeds up the repair performance in general erasure-coded storage. By carefully scheduling the repair of failed data in small-size units across storage nodes in a pipelined manner, repair pipelining reduces the single-block repair time to approximately the same as the normal read time for a single block in homogeneous environments. We further design different extensions of repair pipelining algorithms for heterogeneous environments and multi-block repair operations. We implement a repair pipelining prototype, called ECPipe, and integrate it as a middleware system into two versions of Hadoop Distributed File System (HDFS) (namely, HDFS-RAID and HDFS-3) as well as Quantcast File System. Experiments on a local testbed and Amazon EC2 show that repair pipelining significantly improves the performance of degraded reads and full-node recovery over existing repair techniques.
C1 [Li, Xiaolu; Yang, Zuoru; Li, Jinhong; Li, Runhui; Lee, Patrick P. C.] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.
   [Huang, Qun] Peking Univ, Dept Comp Sci & Technol, EECS, Beijing, Peoples R China.
   [Hu, Yuchong] Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan, Peoples R China.
C3 Chinese University of Hong Kong; Peking University; Huazhong University
   of Science & Technology
RP Lee, PPC (corresponding author), Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.
EM lixl@cse.cuhk.edu.hk; zryang@cse.cuhk.edu.hk; jhli@cse.cuhk.edu.hk;
   lrhdiy@gmail.com; pclee@cse.cuhk.edu.hk; huangqun@pku.edu.cn;
   yuchonghu@hust.edu.cn
RI Lee, Patrick P. C./I-7165-2013
OI Lee, Patrick P. C./0000-0002-4501-4364; HUANG, Qun/0000-0002-2387-6131
FU Research Grants Council of Hong Kong; GRF [14216316, AoE/P-404/18];
   National Natural Science Foundation of China [61872414, 61502191,
   61802365]
FX This work was supported in part by the Research Grants Council of Hong
   Kong (Grants No. GRF 14216316 and No. AoE/P-404/18) and National Natural
   Science Foundation of China (Grants No. 61872414, No. 61502191, and No.
   61802365).
CR Aguilera Marcos K., 2013, P INT C PRINC DISTR
   Ahmad Faraz., 2014, 2014 USENIX ANN TECH, P1
   [Anonymous], 2011, DEVELOPMENT
   [Anonymous], 2014, PROC POWER SYST COMP, DOI DOI 10.1109/PSCC.2014.7038498
   Bai Y., 2019, PROC 48 INT C PARALL, P1, DOI DOI 10.23919/oceans40490.2019.8962582
   Bhagwan R, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE FIRST SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION (NSDI'04), P337
   Calder B, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P143
   Chen YL, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P539
   Chowdhury M, 2013, ACM SIGCOMM COMP COM, V43, P231, DOI 10.1145/2534169.2486021
   Chun BG, 2006, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD SYMPOSIUM ON NETWORKED SYSTEMS DESIGN & IMPLEMENTATION (NSDI 06), P45
   Dean J, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE SIXTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDE '04), P137
   Dimakis AG, 2010, IEEE T INFORM THEORY, V56, P4539, DOI 10.1109/TIT.2010.2054295
   Facebook, 2020, FAC HADOOP
   Ford D., 2010, P 9 USENIX C OP SYST, P61
   Ghemawat S., 2003, ACM SIGOPS OPERATING, V37, P29, DOI [DOI 10.1145/945445.945450, DOI 10.1145/1165389.945450, 10.1145/1165389.945450]
   Hadoop, 2020, HAD 3 1 1 HDFS
   Hoare C.A.R., 1961, Communications of the ACM, V4, P321, DOI [DOI 10.1145/366622.366647, DOI 10.1145/366622.366644]
   HOLLAND M, 1992, SIGPLAN NOTICES, V27, P23, DOI 10.1145/143371.143383
   Hou HX, 2019, IEEE T INFORM THEORY, V65, P4730, DOI 10.1109/TIT.2019.2902835
   Hu YC, 2017, ACM T STORAGE, V13, DOI 10.1145/3149349
   Huang CH, 2012, ELECTRON P THEOR COM, P15, DOI 10.4204/EPTCS.96.2
   Huang JZ, 2015, IEEE T PARALL DISTR, V26, P516, DOI 10.1109/TPDS.2014.2311808
   Jalaparti V, 2015, SIGCOMM'15: PROCEEDINGS OF THE 2015 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P407, DOI 10.1145/2785956.2787488
   Khan O., 2012, P 10 USENIX C FIL ST, P251
   Li Jun, 2010, P 29 IEEE C COMP COM, P2892
   Li RH, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P567
   Li RH, 2014, I C DEPEND SYS NETWO, P419, DOI 10.1109/DSN.2014.47
   Li XL, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P331
   Mitra S, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901328
   Muralidhar S., 2014, P 11 USENIX C OP SYS, P383
   NI LM, 1993, COMPUTER, V26, P62, DOI 10.1109/2.191995
   Ongaro D, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P29
   Ovsiannikov M, 2013, PROC VLDB ENDOW, V6, P1092, DOI 10.14778/2536222.2536234
   Pamies-Juarez L, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P81
   Plank J. S., 2009, FAST 2009, P253
   Plank J.S., 2013, The UsenixMagazine, V38, P44
   Prakash N, 2018, IEEE T INFORM THEORY, V64, P5783, DOI 10.1109/TIT.2018.2806342
   Rashmi K., 2015, USENIX C FIL STOR TE, P81
   Rashmi KV, 2014, SIGCOMM'14: PROCEEDINGS OF THE 2014 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P331, DOI [10.1145/2619239.2626325, 10.1145/2740070.2626325]
   Rashmi K.V., 2013, P 5 USENIX C HOT TOP, P8
   REED IS, 1960, J SOC IND APPL MATH, V8, P300, DOI 10.1137/0108018
   Sathiamoorthy M, 2013, PROC VLDB ENDOW, V6, P325, DOI 10.14778/2535573.2488339
   Shen ZR, 2020, INT PARALL DISTRIB P, P42, DOI 10.1109/IPDPS47924.2020.00015
   Shen ZR, 2019, I C DEPEND SYS NETWO, P556, DOI 10.1109/DSN.2019.00062
   Shvachko K, 2010, IEEE S MASS STOR SYS
   Silberstein Mark., 2014, Proceedings of International Conference on Systems and Storage, P1
   Vajha M, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P139
   Weatherspoon H, 2002, LECT NOTES COMPUT SC, V2429, P328
   Xu FL, 2019, CONCURR COMP-PRACT E, V31, DOI 10.1002/cpe.5031
   Zhirong Shen, 2016, 2016 46th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN). Proceedings, P323, DOI 10.1109/DSN.2016.37
NR 50
TC 9
Z9 10
U1 0
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUN
PY 2021
VL 17
IS 2
AR 13
DI 10.1145/3436890
PG 29
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SV7JF
UT WOS:000663994900005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Maneas, S
   Mahdaviani, K
   Emami, T
   Schroeder, B
AF Maneas, Stathis
   Mahdaviani, Kaveh
   Emami, Tim
   Schroeder, Bianca
TI Reliability of SSDs in Enterprise Storage Systems: A Large-Scale Field
   Study
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Solid-state drives; reliability; enterprise storage systems; field
   study; statistical analysis
ID NAND FLASH MEMORY; SOLID-STATE DRIVES; RETENTION; MODEL
AB This article presents the first large-scale field study of NAND-based SSDs in enterprise storage systems (in contrast to drives in distributed data center storage systems). The study is based on a very comprehensive set of field data, covering 1.6 million SSDs of a major storage vendor (NetApp). The drives comprise three different manufacturers, 18 different models, 12 different capacities, and all major flash technologies (SLC, cMLC, eMLC, 3D-TLC). The data allows us to study a large number of factors that were not studied in prior works, including the effect of firmware versions, the reliability of TLC NAND, and the correlations between drives within a RAID system. This article presents our analysis, along with a number of practical implications derived from it.
C1 [Maneas, Stathis; Mahdaviani, Kaveh; Schroeder, Bianca] Univ Toronto, 27 Kings Coll Circle, Toronto, ON M5S 1A1, Canada.
   [Emami, Tim] NetApp, 1395 Crossman Ave, Sunnyvale, CA 94089 USA.
C3 University of Toronto; NetApp, Inc.
RP Maneas, S (corresponding author), Univ Toronto, 27 Kings Coll Circle, Toronto, ON M5S 1A1, Canada.
EM smaneas@cs.toronto.edu; mahdaviani@cs.toronto.edu; Tim.Emami@netapp.com;
   bianca@cs.toronto.edu
OI Maneas, Stathis/0000-0002-9742-3960
CR Agarwal R, 2010, IEEE GLOBE WORK, P1846, DOI 10.1109/GLOCOMW.2010.5700261
   Agrawal Nitin, 2008, P USENIX ANN TECHN C, P57
   Alter J, 2019, PROCEEDINGS OF SC19: THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS, DOI 10.1145/3295500.3356172
   [Anonymous], 2009, P USENIX ANN TECHN C
   [Anonymous], 1988, A case for redundant arrays of inexpensive disks RAID, DOI DOI 10.1145/50202.50214
   [Anonymous], 2009, P 42 ANN IEEE ACM IN, DOI DOI 10.1145/1669112.1669118
   [Anonymous], 2020, NVM EXPRESS SPECIFIC
   Arnholt AT., 2017, BSDA BASIC STAT DATA
   Bairavasundaram Lakshmi N., 2008, ACM Transaction on Storage, V4, DOI 10.1145/1416944.1416947
   Bairavasundaram LN, 2007, PERF E R SI, V35, P289
   Belgal HP, 2002, INT RELIAB PHY SYM, P7, DOI 10.1109/RELPHY.2002.996604
   Bjorling M., 2019, P USENIX VAULT
   Boboila Simona, 2010, P 8 USENIX C FIL STO, P115
   BRAND A, 1993, INT REL PHY, P127, DOI 10.1109/RELPHY.1993.283291
   Cai Y, 2015, INT S HIGH PERF COMP, P551, DOI 10.1109/HPCA.2015.7056062
   Cai Y, 2013, 2013 IEEE 31ST INTERNATIONAL CONFERENCE ON COMPUTER DESIGN (ICCD), P123, DOI 10.1109/ICCD.2013.6657034
   Cai Y, 2012, DES AUT TEST EUROPE, P521
   Cai Y, 2012, PR IEEE COMP DESIGN, P94, DOI 10.1109/ICCD.2012.6378623
   CAPPELLETTI P, 1994, INTERNATIONAL ELECTRON DEVICES MEETING 1994 - IEDM TECHNICAL DIGEST, P291, DOI 10.1109/IEDM.1994.383410
   Chakraborttii Chandranil, 2020, SoCC '20: Proceedings of the 11th ACM Symposium on Cloud Computing, P120, DOI 10.1145/3419111.3421300
   Chen F, 2011, INT S HIGH PERF COMP, P266, DOI 10.1109/HPCA.2011.5749735
   Chen F, 2009, PERF E R SI, V37, P181
   Corbett P, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P1
   Dean J, 2013, COMMUN ACM, V56, P74, DOI 10.1145/2408776.2408794
   Degraeve R, 2004, IEEE T ELECTRON DEV, V51, P1392, DOI 10.1109/TED.2004.833583
   Desnoyers P, 2014, ACM T STORAGE, V10, DOI 10.1145/2577384
   Edwards JohnK., 2008, ATC 08, P129
   El-Sayed Nosayba, 2012, Performance Evaluation Review, V40, P163, DOI 10.1145/2318857.2254778
   Elerath JG, 2000, P A REL MAI, P194, DOI 10.1109/RAMS.2000.816306
   Elerath JG, 2000, P A REL MAI, P71, DOI 10.1109/RAMS.2000.816286
   GOEL A, 2012, ACM SIGOPS OPER SYST, V46, P41
   Grupp Laura M., 2012, P 10 USENIX C FIL ST, V7, P10
   Gunawi HS, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P1
   Hao MZ, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P263
   Hitz D., 1994, USENIX WINTER, V94
   Hu X.-Y., 2009, System and Storage Conference (SYSTOR), DOI DOI 10.1145/1534530.1534544
   Hur S.H., 2004, Proc. NVSM, P44
   Jae-Duk Lee, 2006, 21st Non-Volatile Semiconductor Memory Workshop. (IEEE Cat. No. 06EX1246), P31, DOI 10.1109/.2006.1629481
   Joo SJ, 2006, JPN J APPL PHYS 1, V45, P6210, DOI 10.1143/JJAP.45.6210
   Jung Myoungsoo, 2013, ACM INT C MEAS MOD C, P203, DOI DOI 10.1145/2465529.2465548
   Kang Jeong-Uk, 2014, P 6 USENIX C HOT TOP
   Kesavan R, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P33
   Kesavan R, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P135
   Kesavan Ram, 2018, P 47 INT C PAR PROC, P1
   Kim BS, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P281
   Kumar H, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P197
   Li Jun, 2020, ACM T DES AUTOM ELEC, V26
   Ma A, 2015, ACM T STORAGE, V11, DOI 10.1145/2820615
   Mahdisoltani Farzaneh, 2017, P USENIX ANN TECHN C, P391
   Manousakis I, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P53
   Maricq A, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P409
   Meza Justin, 2015, ACM SIGMETRICS Performance Evaluation Review, V43, P177, DOI 10.1145/2745844.2745848
   Mielke N, 2008, INT RELIAB PHY SYM, P9, DOI 10.1109/RELPHY.2008.4558857
   Mielke N, 2006, INT RELIAB PHY SYM, P29, DOI 10.1109/RELPHY.2006.251188
   Mielke NR, 2017, P IEEE, V105, P1725, DOI 10.1109/JPROC.2017.2725738
   Myoungsoo Jung, 2012, Middleware 2012. ACM/IFIP/USENIX 13th International Middleware Conference. Proceedings, P164, DOI 10.1007/978-3-642-35170-9_9
   Narayanan I, 2016, PROCEEDINGS OF THE 9TH ACM INTERNATIONAL SYSTEMS AND STORAGE CONFERENCE (SYSTOR'16), DOI [10.1145/2964791.2901489, 10.1145/2928275.2928278]
   Panda B, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P47
   Park Stan., 2012, FAST, P13
   Pinheiro E, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 5TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES ( FAST '07), P17
   Schroeder B, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 5TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES ( FAST '07), P1
   Schroeder B, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P67
   Schroeder B, 2010, ACM T STORAGE, V6, DOI 10.1145/1837915.1837917
   Schwarz Thomas, 2006, NASA IEEE C MASS STO
   Shah S, 2005, P REL MAINT S, P226, DOI 10.1109/RAMS.2005.1408366
   Shah S, 2004, P A REL MAI, P163
   Skourtis Dimitris, 2014, 2014 USENIX ANN TECH, P463
   Suh KD, 1995, IEEE J SOLID-ST CIRC, V30, P1149, DOI 10.1109/4.475701
   Tseng HW, 2011, DES AUT CON, P35
   Tucek Joseph, 2013, P 11 USENIX C FIL ST
   van Ingen C., 2005, MSRTR2005
   Xu E, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P961
   Yang J, 1999, P A REL MAI, P403, DOI 10.1109/RAMS.1999.744151
   Zheng M, 2017, ACM T COMPUT SYST, V34, DOI 10.1145/2992782
   Zoned Storage, NVME ZON NAM
NR 75
TC 3
Z9 4
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD FEB
PY 2021
VL 17
IS 1
AR 3
DI 10.1145/3423088
PG 27
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QG9WR
UT WOS:000617930800002
DA 2024-07-18
ER

PT J
AU Pletka, R
   Koltsidas, I
   Ioannou, N
   Tomic, S
   Papandreou, N
   Parnell, T
   Pozidis, H
   Fry, A
   Fisher, T
AF Pletka, Roman
   Koltsidas, Ioannis
   Ioannou, Nikolas
   Tomic, Sasa
   Papandreou, Nikolaos
   Parnell, Thomas
   Pozidis, Haralampos
   Fry, Aaron
   Fisher, Tim
TI Management of Next-Generation NAND Flash to Achieve Enterprise-Level
   Endurance and Latency Targets
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE NVM controller design; NVM-based storage; flash memory; level shifting;
   endurance; wear leveling; data placement
ID GARBAGE COLLECTION; ERROR RATE; MEMORY; PERFORMANCE; ALGORITHMS; DESIGN
AB Despite its widespread use in consumer devices and enterprise storage systems, NAND flash faces a growing number of challenges. While technology advances have helped to increase the storage density and reduce costs, they have also led to reduced endurance and larger block variations, which cannot be compensated solely by stronger ECC or read-retry schemes but have to be addressed holistically.
   Our goal is to enable low-cost NAND flash in enterprise storage for cost efficiency. We present novel flash-management approaches that reduce write amplification, achieve better wear leveling, and enhance endurance without sacrificing performance. We introduce block calibration, a technique to determine optimal read-threshold voltage levels that minimize error rates, and novel garbage-collection as well as data-placement schemes that alleviate the effects of block health variability and show how these techniques complement one another and thereby achieve enterprise storage requirements.
   By combining the proposed schemes, we improve endurance by up to 15x compared to the baseline endurance of NAND flash without using a stronger ECC scheme. The flash-management algorithms presented herein were designed and implemented in simulators, hardware test platforms, and eventually in the flash controllers of production enterprise all-flash arrays. Their effectiveness has been validated across thousands of customer deployments since 2015.
C1 [Pletka, Roman; Koltsidas, Ioannis; Ioannou, Nikolas; Tomic, Sasa; Papandreou, Nikolaos; Parnell, Thomas; Pozidis, Haralampos] IBM Res Zurich, Saumerstr 4, Ruschlikon, Switzerland.
   [Fry, Aaron; Fisher, Tim] IBM Syst, 10777 Westheimer Rd, Houston, TX 77042 USA.
C3 International Business Machines (IBM)
RP Pletka, R (corresponding author), IBM Res Zurich, Saumerstr 4, Ruschlikon, Switzerland.
EM rap@zurich.ibm.com; iko@zurich.ibm.com; nio@zurich.ibm.com;
   sat@zurich.ibm.com; npo@zurich.ibm.com; tpa@zurich.ibm.com;
   hap@zurich.ibm.com; aaronfry@us.ibm.com; fisher@us.ibm.com
OI Pletka, Roman/0000-0002-6162-3741
CR [Anonymous], 2009, P 42 ANN IEEE ACM IN, DOI DOI 10.1145/1669112.1669118
   [Anonymous], 2010, P 2 USENIX C HOT TOP
   Axboe J., 2014, Fio-Flexible Io Tester
   Ben-Aroya A, 2006, LECT NOTES COMPUT SC, V4168, P100
   Boboila S., 2010, P 8 USENIX C FIL STO, P9
   Bouganim Luc, 2009, BIENN C INN DAT SYST
   Bux W, 2010, PERFORM EVALUATION, V67, P1172, DOI 10.1016/j.peva.2010.07.003
   Cai Y, 2013, 2013 IEEE 31ST INTERNATIONAL CONFERENCE ON COMPUTER DESIGN (ICCD), P123, DOI 10.1109/ICCD.2013.6657034
   Cai Y, 2013, DES AUT TEST EUROPE, P1285
   Cai Y, 2012, PR IEEE COMP DESIGN, P94, DOI 10.1109/ICCD.2012.6378623
   Chang L.-P., 2004, ACM Trans. on Embedded Computing Syst, V3, P837
   Chang LP, 2007, APPLIED COMPUTING 2007, VOL 1 AND 2, P1126, DOI 10.1145/1244002.1244248
   Dean J, 2013, COMMUN ACM, V56, P74, DOI 10.1145/2408776.2408794
   Desnoyers Peter., 2012, SYSTOR, P1, DOI DOI 10.1145/2367589.2367603
   Etsion Y, 2012, IEEE T COMPUT, V61, P1535, DOI 10.1109/TC.2011.197
   Gal E, 2005, ACM COMPUT SURV, V37, P138, DOI 10.1145/1089733.1089735
   Grossi A, 2015, IEEE T DEVICE MAT RE, V15, P363, DOI 10.1109/TDMR.2015.2448108
   Han-Joon Kim, 1999, Proceedings. Twenty-Third Annual International Computer Software and Applications Conference (Cat. No.99CB37032), P284, DOI 10.1109/CMPSAC.1999.812717
   Hu X.-Y., 2009, System and Storage Conference (SYSTOR), DOI DOI 10.1145/1534530.1534544
   Huang J, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P375
   IBM, 2015, FLASHSYSTEM 900
   JEDEC, 2017, STRESS TEST DRIV QUA
   Lee Y., 2012, P IEEE INT SOL STAT
   Menon J., 1998, High Performance Computing Systems and Applications, P119
   Mielke N, 2008, INT RELIAB PHY SYM, P9, DOI 10.1109/RELPHY.2008.4558857
   Min Changwoo, 2012, FAST, V12, P1
   Pan YY, 2013, IEEE T VLSI SYST, V21, P1350, DOI 10.1109/TVLSI.2012.2210256
   Papandreaou Nikolaos, 2013, 2013 18 INT C DIG SI, P1
   Papandreou N, 2014, PR GR LAK SYMP VLSI, P151, DOI 10.1145/2591513.2591594
   Park KT, 2015, IEEE J SOLID-ST CIRC, V50, P204, DOI [10.1109/JSSC.2014.2352293, 10.1109/ISSCC.2014.6757458]
   Peleato B, 2015, IEEE ICC, P295, DOI 10.1109/ICC.2015.7248337
   Pletka RA, 2016, PROCEEDINGS OF THE 9TH ACM INTERNATIONAL SYSTEMS AND STORAGE CONFERENCE (SYSTOR'16), DOI 10.1145/2928275.2928279
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   Schroeder B, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P67
   Stoica R, 2013, PROC VLDB ENDOW, V6, P733, DOI 10.14778/2536360.2536372
   Sun Fei, 2006, P IEEE WORKSH SIGN P, P241, DOI [10.1049/iet-cds:20060275, DOI 10.1049/IET-CDS:20060275]
   Van Houdt Benny, 2013, Performance Evaluation Review, V41, P191
   Van Houdt B, 2013, PERFORM EVALUATION, V70, P692, DOI 10.1016/j.peva.2013.08.010
   Van Houdt Benny, 2013, EVALUATION REV, V41, P191, DOI [10.1145/2494232.2465543, DOI 10.1145/2494232.2465543]
   Wells S. E., 1994, uS Patent, Patent No. [5,341,339, 5341339]
   Xiao-Yu Hu, 2011, Proceedings of the 2011 IEEE 19th International Symposium on Modelling, Analysis & Simulation of Computer and Telecommunication Systems (MASCOTS 2011), P237, DOI 10.1109/MASCOTS.2011.50
   [杨靖 Yang Jing], 2013, [铁道科学与工程学报, Journal of Rail Way Science and Engineering], V10, P11
   Yang Y, 2016, ACM T STORAGE, V12, DOI 10.1145/2908557
   Yangwook Kang, 2010, Proceedings 18th IEEE/ACM International Symposium on Modelling, Analysis & Simulation of Computer and Telecommunication Systems (MASCOTS 2010), P407, DOI 10.1109/MASCOTS.2010.52
   Zhao K., 2013, P 11 USENIX C FILE S, P243
   Zuolo L, 2015, IEEE T COMPUT AID D, V34, P1627, DOI 10.1109/TCAD.2015.2422834
NR 46
TC 17
Z9 18
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD DEC
PY 2018
VL 14
IS 4
AR 33
DI 10.1145/3241060
PG 25
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ4JO
UT WOS:000457140500005
DA 2024-07-18
ER

PT J
AU Yan, SQ
   Li, HC
   Hao, MZ
   Tong, MH
   Sundararaman, S
   Chien, AA
   Gunawi, HS
AF Yan, Shiqin
   Li, Huaicheng
   Hao, Mingzhe
   Tong, Michael Hao
   Sundararaman, Swaminathan
   Chien, Andrew A.
   Gunawi, Haryadi S.
TI Tiny-Tail Flash: Near-Perfect Elimination of Garbage Collection Tail
   Latencies in NAND SSDs
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE TTFlash; flash-based SSD
AB Flash storage has become the mainstream destination for storage users. However, SSDs do not always deliver the performance that users expect. The core culprit of flash performance instability is the well-known garbage collection (GC) process, which causes long delays as the SSD cannot serve (blocks) incoming I/Os, which then induces the long tail latency problem. We present TTFlash as a solution to this problem. TTFlash is a "tiny-tail" flash drive (SSD) that eliminates GC-induced tail latencies by circumventing GC-blocked I/Os with four novel strategies: plane-blocking GC, rotating GC, GC-tolerant read, and GC-tolerant flush. These four strategies leverage the timely combination of modern SSD internal technologies such as powerful controllers, parity-based redundancies, and capacitor-backed RAM. Our strategies are dependent on the use of intra-plane copyback operations. Through an extensive evaluation, we show that TTFlash comes significantly close to a "no-GC" scenario. Specifically, between the 99 and 99.99th percentiles, TTFlash is only 1.0 to 2.6x slower than the no-GC case, while a base approach suffers from 5-138x GC-induced slowdowns.
C1 [Yan, Shiqin; Li, Huaicheng; Hao, Mingzhe; Tong, Michael Hao; Chien, Andrew A.; Gunawi, Haryadi S.] Univ Chicago, 1100 E 58th St, Chicago, IL 60637 USA.
   [Sundararaman, Swaminathan] Parallel Machines, Sunnyvale, CA 94095 USA.
C3 University of Chicago
RP Yan, SQ (corresponding author), Univ Chicago, 1100 E 58th St, Chicago, IL 60637 USA.
EM shiqin@cs.uchicago.edu; huaicheng@cs.uchicago.edu;
   hmz20000@uchicago.edu; michaelht@cs.uchicago.edu;
   swaminathan.sundararaman@gmail.com; achien@cs.uchicago.edu;
   haryadi@cs.uchicago.edu
RI L, H/AAM-2796-2021; l, H/HJA-6959-2022
FU NSF [CCF-1336580, CNS-1350499, CNS-1526304, CNS-1405959, CNS-1563956]
FX This material is based on work supported by the NSF (grant nos.
   CCF-1336580, CNS-1350499, CNS-1526304, CNS-1405959, and CNS-1563956) as
   well as generous donations from EMC, Google, Huawei, NetApp, and CERES
   Research Center.
CR Agrawal Nitin, 2008, 2008 USENIX ANN TECH
   Aldershoff Jan Willem, 2014, REPORT SSD MARKET DO
   Amvrosiadis George, 2015, P 25 ACM S OP SYST P
   [Anonymous], 2007, P 21 ACM SIGOPS S OP
   Barr J., 2014, NEW SSD BACKED ELAST
   Birk Yitzhak, P 7 INT WORKSH NETW
   Bjorling Matias, P 15 USENIX S FIL ST
   Chang L H, 2004, THESIS YUAN ZE U TAI, P1
   Colgrove John., P 2015 ACM SIGMOD IN
   Crucial, 2013, THE CRUC M550 SSD
   Dean Jeffrey, 2013, COMMUNICATIONS ACM, V56
   Do Thanh., P 4 ACM S CLOUD COMP
   GUNAWI H.S., P 5 ACM S CLOUD COMP
   Gunawi HaryadiS., P 7 ACM S CLOUD COMP
   Gupta Aayush, P 9 USENIX S FIL STO
   Hao Mingzhe, 2016, P 14 USENIX S FIL ST
   Harris Robin, 2015, WHY SSDS DONT PERFOR
   He Jun, 2015, P 13 USENIX S FIL ST
   Hernandez Pedro, 2015, MICROSOFT ROLLS OUT
   Hu Yang, P 25 INT C SUP ICS 1
   Huang Sheng-Min, P IEEE ACM C T EMB C
   Im Soojun, 2010, IEEE T COMPUT, V60, P1
   Jian Ouyang, 2014, P 18 INT C ARCH SUPP
   Kang Woon-Hak., P 2013 ACM SIGMOD IN
   Kavalanekar Swaroop, 2008, IEEE INT S WORKL CHA
   Kerekes Zsolt, 2016, WHATS STATE DWPD END
   Kerekes Zsolt, 2015, WHAT HAPPENS INSIDE
   Kim Hyojun, P 6 USENIX S FIL STO
   Kim Jaeho, P INT C DEP SYST NET
   Kim Jaeho, 2015, P 13 USENIX S FIL ST
   Kim Youngjae, P 27 IEEE S MASS STO
   LEE C., 2015, P USENIX C FIL STOR
   Lee J., IEEE INT S PERF AN S
   Lee Sehwan, P 2011 ACM S APPL CO
   Lee Yangsup, P 7 IEEE ACM INT C H
   LightNVM, 2016, OPCHANN SOL STAT DRI
   Margaglia Fabio, 2016, P 14 USENIX S FIL ST
   Micron, 2006, NAND FLASH 101 INTR
   Micron, 2015, L74A NAND DAT
   Micron, 2015, MICR P420M ENT PCIE
   Min Changwoo, 2012, P 10 USENIX C FIL ST
   Ping Huang, P 2014 EUROSYS C EUR
   Pott T., 2014, SUPERCAPACITORS HAVE
   Sandisk, 2014, SANDISK PREEMPTIVE G
   Saxena Mohit, 2013, P 11 USENIX S FIL ST
   Schroeder Bianca, 2016, P 14 USENIX S FIL ST
   Skourtis Dimitris, P 2014 USENIX ANN TE
   SNIA, 2019, SNIA STOR NETW IND A
   Tarasov V., 2016, FILEBENCH
   The OpenSSD Project, 2016, THE OPENSSD PROJ
   Tiwari Devesh, 2013, P 11 USENIX S FIL ST
   Wu Guanying, 2012, P 10 USENIX S FIL ST
   Yadgar Gala, 2015, P 13 USENIX S FIL ST
   Yan Shiqin, 2017, TINYTAILFLASH SOURCE
   Yang Suli, 2015, P 25 ACM S OP SYST P
   Yoo Jinsoo, P 29 IEEE S MASS STO
NR 56
TC 52
Z9 58
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD OCT
PY 2017
VL 13
IS 3
SI SI
AR 22
DI 10.1145/3121133
PG 26
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FL6DV
UT WOS:000414336300005
OA Bronze
DA 2024-07-18
ER

PT J
AU Li, YK
   Xu, M
   Ng, CH
   Lee, PPC
AF Li, Yan-Kit
   Xu, Min
   Ng, Chun-Ho
   Lee, Patrick P. C.
TI Efficient Hybrid Inline and Out-of-Line Deduplication for Backup Storage
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Implementation; Experiments; Performance; Deduplication; backup
   storage
AB Backup storage systems often remove redundancy across backups via inline deduplication, which works by referring duplicate chunks of the latest backup to those of existing backups. However, inline deduplication degrades restore performance of the latest backup due to fragmentation, and complicates deletion of expired backups due to the sharing of data chunks. While out-of-line deduplication addresses the problems by forward-pointing existing duplicate chunks to those of the latest backup, it introduces additional I/Os of writing and removing duplicate chunks.
   We design and implement RevDedup, an efficient hybrid inline and out-of-line deduplication system for backup storage. It applies coarse-grained inline deduplication to remove duplicates of the latest backup, and then fine-grained out-of-line reverse deduplication to remove duplicates from older backups. Our reverse deduplication design limits the I/O overhead and prepares for efficient deletion of expired backups. Through extensive testbed experiments using synthetic and real-world datasets, we show that RevDedup can bring high performance to the backup, restore, and deletion operations, while maintaining high storage efficiency comparable to conventional inline deduplication.
C1 [Li, Yan-Kit; Xu, Min; Ng, Chun-Ho; Lee, Patrick P. C.] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.
C3 Chinese University of Hong Kong
RP Lee, PPC (corresponding author), Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.
EM windkithk@gmail.com; pickxu@gmail.com; ngch.hk@gmail.com;
   pclee@cse.cuhk.edu.hk
RI Xu, Min/AAV-6824-2021; Lee, Patrick P. C./I-7165-2013
OI Xu, Min/0000-0001-6238-6058; Lee, Patrick P. C./0000-0002-4501-4364
FU Research Grant Council of Hong Kong [GRF CUHK 413813]
FX This work was supported in part by grant GRF CUHK 413813 from the
   Research Grant Council of Hong Kong.
CR Andrews Bill., 2013, STRAIGHT TALK DISK B
   [Anonymous], P 6 INT SYST STOR C
   [Anonymous], P 20 IEEE INT S MOD
   [Anonymous], 2009, FAST
   [Anonymous], P 6 INT SYST STOR C
   [Anonymous], 2009, ACM INT C P SERIES
   [Anonymous], P USENIX 2002 C FIL
   [Anonymous], P 10 USENIX C FIL ST
   [Anonymous], ANN INT SYST STOR C
   [Anonymous], 2011, P USENIX ANN TECHN C
   [Anonymous], 2010, FAST
   [Anonymous], 2013, P 4 AS PAC WORKSH SY
   [Anonymous], 2009, 7 USENIX C FIL STOR
   Bhagwat D., 2009, P 17 IEEEACM INT S M, P1
   Black J, 2006, USENIX ASSOCIATION PROCEEDINGS OF THE 2006 USENIX ANNUAL TECHNICAL CONFERENCE, P85
   BLOOM BH, 1970, COMMUN ACM, V13, P422, DOI 10.1145/362686.362692
   Botelho F.C., 2013, Proc. of USENIX Conf. on File and Storage Technologies (FAST), P81
   Debnath B., 2010, P USENIX ANN TECHN C
   Lillibridge M., 2013, FAST
   Mao B, 2014, ACM T STORAGE, V10, DOI 10.1145/2512348
   Meister Dirk., 2010, MASS STORAGE SYSTEMS, P1
   RABIN MO, 1981, TR1581 HARV U CTR RE
   Rhea Sean., 2008, P USENIX ANN TECHNIC, P143
   Srinivasan Kiran, 2012, 10 USENIX C FILE STO, P299
   Strzelczak Przemyslaw., 2013, FAST, P161
   Zhu B, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P269
   ZIV J, 1977, IEEE T INFORM THEORY, V23, P337, DOI 10.1109/TIT.1977.1055714
NR 27
TC 43
Z9 44
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD FEB
PY 2015
VL 11
IS 1
AR 2
DI 10.1145/2641572
PG 21
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CD8CR
UT WOS:000351323300002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Saxena, M
   Swift, MM
AF Saxena, Mohit
   Swift, Michael M.
TI Design and Prototype of a Solid-State Cache
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Algorithms; Performance; Experimentation; Solid-state cache;
   device interface; consistency; durability; prototype
AB The availability of high-speed solid-state storage has introduced a new tier into the storage hierarchy. Low-latency and high-TOPS solid-state drives (SSDs) cache data in front of high-capacity disks. However, most existing SSDs are designed to be a drop-in disk replacement, and hence are mismatched for use as a cache.
   This article describes FlashTier, a system architecture built upon a solid-state cache (SSC), which is a flash device with an interface designed for caching. Management software at the operating system block layer directs caching. The FlashTier design addresses three limitations of using traditional SSDs for caching. First, FlashTier provides a unified logical address space to reduce the cost of cache block management within both the OS and the SSD. Second, FlashTier provides a new SSC block interface to enable a warm cache with consistent data after a crash. Finally, FlashTier leverages cache behavior to silently evict data blocks during garbage collection to improve performance of the SSC.
   We first implement an SSC simulator and a cache manager in Linux to perform an in-depth evaluation and analysis of FlashTier's design techniques. Next, we develop a prototype of SSC on the OpenSSD Jasmine hardware platform to investigate the benefits and practicality of FlashTier design. Our prototyping experiences provide insights applicable to managing modern flash hardware, implementing other SSD prototypes and new OS storage stack interface extensions.
   Overall, we find that FlashTier improves cache performance by up to 168% over consumer-grade SSDs. and up to 52% over high-end SSDs. It also improves flash lifetime for write-intensive workloads by up to 60% compared to SSD caches with a traditional flash interface.
C1 [Saxena, Mohit; Swift, Michael M.] Univ Wisconsin, Madison, WI 53706 USA.
C3 University of Wisconsin System; University of Wisconsin Madison
RP Saxena, M (corresponding author), Univ Wisconsin, Madison, WI 53706 USA.
CR Aayush Gupta, 2009, P ASPLOS
   AGRAWAL N., 2008, P USENIX ATC
   Akel Ameen, 2011, P HOTSTORAGE
   [Anonymous], 2013, P FAST
   [Anonymous], BARRIERS JOURNALING
   Apple Inc, 2013, FUS DRIV
   Balakrishnan M., 2013, P SOSP
   Bisson Timothy, 2007, P MASCOTS
   BOBOILA S., 2010, P FAST
   Bunker T., 2012, TECHNICAL REPORT
   Byan Steve, 2011, P FAST POST
   Caulfield Adrian M., 2010, P IEEE MICR
   Chen Feng, 2011, P FAST
   Computer Systems Laboratory SKKU, 2012, EMB SYST DES CLASS
   Davis John D., 2009, P WORKSH INT SOL STA
   Doyle Scott, 2010, P INT IDF EMC 2013 F
   EnhanceIO, 2012, ENHANCEIO
   Facebook Inc., 2013, FAC FLASHCACHE
   Fusion-io Inc., 2013, IOMEMORY APPL SDK
   Fusion-io Inc., 2013, IOXTREME PCI E SSD D
   Fusion-io Inc., 2013, DIRECTCACHE
   Google Inc., 2012, GOOGL SPARS HASH
   Gregg Brendan, 2008, SUN BLOG SOLARIS L2A
   Guerra Jorge, 2011, P FAST
   Gunawi Haryadi S., 2007, Operating Systems Review, V41, P293, DOI 10.1145/1323293.1294290
   Gupta Aayush, 2011, P FAST
   Helland Pat., 1988, Group Commit Timers and High-Volume Transaction Systems
   Intel, 1998, AP684
   Intel Corp, 2012, INT 300 SER SSD
   Intel Corp, 2011, INT SMART RESP TECHN
   JOSEPHSON W.K., 2010, P FAST
   Kgil T., 2006, P CASES
   Kim Y, 2009, SIMUL: 2009 FIRST INTERNATIONAL CONFERENCE ON ADVANCES IN SYSTEM SIMULATION, P125, DOI 10.1109/SIMUL.2009.17
   Koller R., 2013, P FAST
   Koller Ricardo, 2010, P FAST
   Lee S., 2010, P WORKSH ARCH RES PR
   Lee SW, 2007, ACM T EMBED COMPUT S, V6, DOI 10.1145/1275986.1275990
   Lee Sungjin, 2009, P US ATC
   Mack Ryan, 2012, BUILDING FACEBOOK TI
   Mesnier Michael, 2011, P SOSP
   Narayanan Dushyanth, 2008, P FAST
   Nellans D., 2011, 2 ANN NONVOLATILE ME, P17
   NetApp Inc., 2013, FLASH CACH ENT
   OCZ, 2012, OCZ SYN CACH SSD
   OCZ Technologies, 2012, VERT 3 SSD
   Oh Yongseok, 2012, P FAST
   OpenSSD Project Website, 2013, IND JASM PLATF
   Oracle Corp, 2012, OR DAT SMART FLASH C
   Ouyang XY, 2011, INT S HIGH PERF COMP, P301, DOI 10.1109/HPCA.2011.5749738
   Prabhakaran Vijayan, 2008, P OSDI
   Prabhakaran Vijayan, 2005, P OF THE 20 ACM S ON, V39, P206
   Roberts D, 2009, COMMUN ACM, V52, P98, DOI 10.1145/1498765.1498791
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   Saxena Mohit, 2010, P US ATC
   Saxena Mohit, 2012, P EUROSYS
   Tom Archer, 2006, MSDN BLOG MICROSOFT
   Wong Theodore M., 2002, P US ATC
   Wu Michael, 1994, P ASPLOS 6
   Yadgar Gala, 2007, P FAST
   ZHANG Y., 2012, P FAST
   Zhang Yiying, 2013, P FAST
NR 61
TC 2
Z9 3
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUL
PY 2014
VL 10
IS 3
AR 10
DI 10.1145/2629491
PG 34
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AN3TU
UT WOS:000340512400002
DA 2024-07-18
ER

PT J
AU Bessani, A
   Correia, M
   Quaresma, B
   André, F
   Sousa, P
AF Bessani, Alysson
   Correia, Miguel
   Quaresma, Bruno
   Andre, Fernando
   Sousa, Paulo
TI DEPSKY: Dependable and Secure Storage in a Cloud-of-Clouds
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Algorithms; Measurement; Performance; Reliability; Security; Cloud
   computing; cloud storage; Byzantine quorum systems
AB The increasing popularity of cloud storage services has lead companies that handle critical data to think about using these services for their storage needs. Medical record databases, large biomedical datasets, historical information about power systems and financial data are some examples of critical data that could be moved to the cloud. However, the reliability and security of data stored in the cloud still remain major concerns. In this work we present DepSky, a system that improves the availability, integrity, and confidentiality of information stored in the cloud through the encryption, encoding, and replication of the data on diverse clouds that form a cloud-of-clouds. We deployed our system using four commercial clouds and used PlanetLab to run clients accessing the service from different countries. We observed that our protocols improved the perceived availability, and in most cases, the access latency, when compared with cloud providers individually. Moreover, the monetary costs of using DepSky in this scenario is at most twice the cost of using a single cloud, which is optimal and seems to be a reasonable cost, given the benefits.
C1 [Bessani, Alysson; Quaresma, Bruno; Andre, Fernando] Univ Lisbon, Fac Ciencias, P-1699 Lisbon, Portugal.
C3 Universidade de Lisboa
RP Bessani, A (corresponding author), Univ Lisbon, Fac Ciencias, P-1699 Lisbon, Portugal.
RI ; Correia, Miguel/L-6890-2015
OI Bessani, Alysson/0000-0002-8386-1628; Correia,
   Miguel/0000-0001-7873-5531
FU EC FP7 through project TCLOUDS [ICT-257243]; FCT
   [PTDC/EIA-EIA/115211/2009]; Multi-annual Program (LASIGE); INESC-ID
   [PEst-OE/EEI/LA0021/2011]; Fundação para a Ciência e a Tecnologia
   [PTDC/EIA-EIA/115211/2009] Funding Source: FCT
FX This work was partially supported by the EC FP7 through project TCLOUDS
   (ICT-257243), by the FCT through project RC-Clouds
   (PTDC/EIA-EIA/115211/2009), the Multi-annual Program (LASIGE), and
   contract PEst-OE/EEI/LA0021/2011 (INESC-ID).
CR Abraham I, 2006, DISTRIB COMPUT, V18, P387, DOI 10.1007/s00446-005-0151-6
   Abu-Libdeh H., 2010, Proceedings of the 1st ACM symposium on Cloud computing, SoCC'10, pA(sic)g, P229
   Alchieri Eduardo A. P., 2008, 2008 IEEE International Conference on Web Services (ICWS), P21, DOI 10.1109/ICWS.2008.54
   [Anonymous], CMUSEI2011TN006
   [Anonymous], 1992, 1305 RFC
   [Anonymous], P 26 IEEE INT C DIST
   [Anonymous], 1988, A case for redundant arrays of inexpensive disks RAID, DOI DOI 10.1145/50202.50214
   [Anonymous], TECHNOLOGY REV
   [Anonymous], 2012, P USENIX FAST
   [Anonymous], P USENIX ANN TECHN C
   Attiya H., 2003, P 22 IEEE S REL DIST, P174
   Basescu C, 2012, I C DEPEND SYS NETWO
   Bessani AN, 2008, EUROSYS'08: PROCEEDINGS OF THE EUROSYS 2008 CONFERENCE, P163, DOI 10.1145/1357010.1352610
   Bowers KD, 2009, CCS'09: PROCEEDINGS OF THE 16TH ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P187
   Brantner M., 2008, P 2008 ACM SIGMOD IN, P251, DOI [DOI 10.1145/1376616.1376645, 10.1145/1376616.1376645]
   Cachin C, 2006, I C DEPEND SYS NETWO, P115
   Chockler G, 2009, COMPUTER, V42, P60, DOI 10.1109/MC.2009.126
   Chockler Gregory, 2002, P 21 ANN S PRINC DIS, P78
   Dahlin M, 2003, IEEE ACM T NETWORK, V11, P300, DOI 10.1109/TNET.2003.810312
   Dekker M.A. C., 2012, Critical Cloud Computing-A CIIP perspective on cloud computing services
   Feldman A.J., 2010, S OPERATING SYSTEM D, P337
   Gafni E, 2003, DISTRIB COMPUT, V16, P1, DOI [10.1007/s00446-002-0070-8, 10.1007/S00446-002-0070-8]
   Ghemawat S., 2003, ACM SIGOPS OPERATING, V37, P29, DOI [DOI 10.1145/945445.945450, DOI 10.1145/1165389.945450, 10.1145/1165389.945450]
   Gibson GA, 1998, ACM SIGPLAN NOTICES, V33, P92, DOI 10.1145/291006.291029
   Goodson GR, 2004, 2004 INTERNATIONAL CONFERENCE ON DEPENDABLE SYSTEMS AND NETWORKS, PROCEEDINGS, P135
   Greer M., 2010, P 4 WORKSH REC ADV I
   Hamilton J, 2007, USENIX ASSOCIATION PROCEEDING OF THE 21ST LARGE INSTALLATION SYSTEMS ADMINISTRATION CONFERENCE, P231
   Hendricks James, 2007, Operating Systems Review, V41, P73, DOI 10.1145/1323293.1294269
   Henry A., 2009, P 7 USENIX C FIL STO
   Herlihy M, 2002, INT CON DISTR COMP S, P522
   HERLIHY MP, 1990, ACM T PROGR LANG SYS, V12, P463, DOI 10.1145/78969.78972
   Hunt P., 2010, P USENIX ANN TECHN C, P145
   Jayanti P, 1998, J ACM, V45, P451, DOI 10.1145/278298.278305
   Krawczyk Hugo, 1993, Proceedings of the 13th Annual International Cryptology Conference on Advances in Cryptology, P136
   LAMPORT L, 1982, ACM T PROGR LANG SYS, V4, P382, DOI 10.1145/357172.357176
   LAMPORT L, 1986, DISTRIB COMPUT, V1, P203
   LISKOV B, 2006, P 26 IEEE INT C DIST
   Mahajan P, 2011, ACM T COMPUT SYST, V29, DOI 10.1145/2063509.2063512
   Malkhi D, 1998, DISTRIB COMPUT, V11, P203, DOI 10.1007/s004460050050
   Malkhi D, 1998, SYM REL DIST SYST, P51, DOI 10.1109/RELDIS.1998.740474
   Martin JP, 2002, LECT NOTES COMPUT SC, V2508, P311
   May M, 2010, NAT MED, V16, P6, DOI 10.1038/nm0110-6a
   McCullough John C., 2010, P ANN TECHNICAL C AT, P47
   Metz C, 2009, REGISTER
   Muniswamy-Reddy Kiran-Kumar., 2010, P USENIX C FILE STOR, P197
   Plank James S, 2007, CS07603 U TENN
   RABIN MO, 1989, J ACM, V36, P335, DOI 10.1145/62044.62050
   Raphael J., 2011, The 10 worst cloud outages
   Sarno D., 2009, LOS ANGELES TIMES
   Schoenmakers B., 1999, Advances in Cryptology - CRYPTO'99. 19th Annual International Cryptology Conference. Proceedings, P148
   SHAMIR A, 1979, COMMUN ACM, V22, P612, DOI 10.1145/359168.359176
   Shraer A., 2010, P ACM CLOUD COMP SEC
   Storer MW, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 2007 USENIX ANNUAL TECHNICAL CONFERENCE, P143
   Terry D. B., 1994, Proceedings of the Third International Conference on Parallel and Distributed Information Systems (Cat. No.94TH0668-4), P140, DOI 10.1109/PDIS.1994.331722
   Vogels W, 2009, COMMUN ACM, V52, P40, DOI 10.1145/1435417.1435432
   Vrable M., 2009, ACM Transactions on Storage, V5, P1
   Vukolic Marko, 2010, SIGACT News, V41, P105, DOI 10.1145/1855118.1855137
   Weil SA, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P307
NR 58
TC 228
Z9 268
U1 0
U2 26
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2013
VL 9
IS 4
AR 12
DI 10.1145/2535929
PG 33
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 281YI
UT WOS:000329136500002
DA 2024-07-18
ER

PT J
AU Kadekodi, S
   Silas, S
   Clausen, D
   Merchant, A
AF Kadekodi, Saurabh
   Silas, Shashwat
   Clausen, David
   Merchant, Arif
TI Practical Design Considerations for Wide Locally Recoverable Codes
   (LRCs)
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Reliability; erasure codes; distributed storage systems
ID CONSTRUCTIONS
AB Most of the data in large-scale storage clusters is erasure coded. At exascale, optimizing erasure codes for low storage overhead, efficient reconstruction, and easy deployment is of critical importance. Locally recoverable codes (LRCs) have deservedly gained central importance in this field, because they can balance many of these requirements. In our work, we study wide LRCs; LRCs with large number of blocks per stripe and low storage overhead. These codes are a natural next step for practitioners to unlock higher storage savings, but they come with their own challenges. Of particular interest is their reliability, since wider stripes are prone to more simultaneous failures.
   We conduct a practically minded analysis of several popular and novel LRCs. We find that wide LRC reliability is a subtle phenomenon that is sensitive to several design choices, some of which are overlooked by theoreticians, and others by practitioners. Based on these insights, we construct novel LRCs called Uniform Cauchy LRCs, which show excellent performance in simulations and a 33% improvement in reliability on unavailability events observed by a wide LRC deployed in a Google storage cluster. We also show that these codes are easy to deploy in a manner that improves their robustness to common maintenance events. Along the way, we also give a remarkably simple and novel construction of distance-optimal LRCs (other constructions are also known), which may be of interest to theory-minded readers.
C1 [Kadekodi, Saurabh; Silas, Shashwat; Clausen, David; Merchant, Arif] Google, Mountain View, CA 94043 USA.
C3 Google Incorporated
RP Kadekodi, S (corresponding author), Google, Mountain View, CA 94043 USA.
EM shash-wat@alumni.stanford.edu; shash-wat@alumni.stanford.edu;
   dclausen@google.com; aamerchant@google.com
OI Kadekodi, Saurabh/0000-0001-5582-0354; Merchant,
   Arif/0000-0002-0913-1459
CR Agarwal Abhishek, 2018, IEEE T INF THEOR, V2018
   Amazon, 2023, AMAZON S3 FAQS
   Backblaze, 2013, ERASURE CODING USED
   Backblaze, 2013, DISK RELIABILITY DAT
   Balaji SB, 2015, IEEE INT SYMP INFO, P1881, DOI 10.1109/ISIT.2015.7282782
   Barg A, 2022, DESIGN CODE CRYPTOGR, V90, P939, DOI 10.1007/s10623-022-01020-8
   Barg A, 2017, IEEE T INFORM THEORY, V63, P4928, DOI 10.1109/TIT.2017.2700859
   Ben-Yair Shimrit, 2020, UPDATING GOOGLE PHOT
   Bloemer J., 1995, An XOR-based erasure-resilient coding scheme
   Brewer Eric, 2016, DISKS DATA CENTERS
   Brewer Eric, 2018, SPINNING DISKS THEIR
   Cadambe V, 2013, INT SYMP NETW COD
   Cai H, 2022, IEEE T INFORM THEORY, V68, P204, DOI 10.1109/TIT.2021.3120016
   Cidon Asaf, 2013, P USENIX ATC
   Ford Daniel, 2010, USENIX S OP SYST DES
   Gibson Garth Alan, 1991, THESIS U CALIFORNIA
   Gopalan P, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P2092
   Gopalan P, 2014, IEEE T INFORM THEORY, V60, P5245, DOI 10.1109/TIT.2014.2332338
   Gopalan P, 2012, IEEE T INFORM THEORY, V58, P6925, DOI 10.1109/TIT.2012.2208937
   Gopi S, 2022, IEEE T INFORM THEORY, V68, P7198, DOI 10.1109/TIT.2022.3176807
   Gopi S, 2020, IEEE T INFORM THEORY, V66, P6066, DOI 10.1109/TIT.2020.2990981
   Gopi Sivakanth, 2017, ELECT C COMPUTATIONA
   Grezet Matthias, 2019, IEEE COMMUN LETT, V2019
   Guruswami Venkatesan, 2020, IEEE T INF THEOR
   Haddock W, 2019, 2019 SPRING SIMULATION CONFERENCE (SPRINGSIM), DOI 10.23919/springsim.2019.8732912
   Hu YC, 2021, PROCEEDINGS OF THE 19TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '21), P233
   Huang C., 2012, P 2012 USENIX C ANN, P2
   Huang C, 2007, SIXTH IEEE INTERNATIONAL SYMPOSIUM ON NETWORK COMPUTING AND APPLICATIONS, PROCEEDINGS, P79, DOI 10.1109/NCA.2007.37
   Jin LF, 2019, IEEE T INFORM THEORY, V65, P4658, DOI 10.1109/TIT.2019.2901492
   Kadekodi S, 2022, PROCEEDINGS OF THE 16TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, OSDI 2022, P413
   Kadekodi S, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P345
   Kadekodi Saurabh, 2020, USENIX S OPERATING S
   Kadekodi Saurabh, 2020, THESIS CARNEGIE MELL
   KathrynHaymaker Beth Malmskog, 2016, ARXIV
   Kolosov O, 2018, PROCEEDINGS OF THE 2018 USENIX ANNUAL TECHNICAL CONFERENCE, P865
   Li Y, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P135
   Liu J, 2018, IEEE T INFORM THEORY, V64, P889, DOI 10.1109/TIT.2017.2713245
   Liu S., 2021, ARXIV
   Luo GJ, 2021, IEEE T COMMUN, V69, P4987, DOI 10.1109/TCOMM.2021.3083320
   Micheli G, 2020, IEEE T INFORM THEORY, V66, P167, DOI 10.1109/TIT.2019.2939464
   Nightingale EdmundB., 2012, P 10 USENIX C OPERAT, P1
   Noer Geoffrey, 2021, CLOUD STORAGE DELIVE
   Papailiopoulos D. S., 2012, IEEE INT S INFORM TH
   Rashmi K.V., 2013, P 5 USENIX C HOT TOP, P8
   Reinsel D., 2018, The digitization of the world from edge to core
   Sathiamoorthy M, 2013, PROC VLDB ENDOW, V6, P325, DOI 10.14778/2535573.2488339
   Seagate, 2018, DIGITIZATION THEWORL
   Shahabinejad Mostafa, 2018, LOCALLY REPAIRABLE L
   Silberstein N, 2013, IEEE INT SYMP INFO, P1819, DOI 10.1109/ISIT.2013.6620541
   Tamo I, 2016, IEEE T INFORM THEORY, V62, P6661, DOI 10.1109/TIT.2016.2555813
   Tamo I, 2016, IEEE T INFORM THEORY, V62, P3070, DOI 10.1109/TIT.2016.2518663
   Tamo I, 2014, IEEE INT SYMP INFO, P691, DOI 10.1109/ISIT.2014.6874921
   Tamo I, 2014, IEEE T INFORM THEORY, V60, P4661, DOI 10.1109/TIT.2014.2321280
   VAST, 2019, PROVIDING RESILIENCE
   Zhang Guanghui, 2020, IEEE COMMUN LETT, V2020
NR 55
TC 0
Z9 0
U1 2
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2023
VL 19
IS 4
AR 31
DI 10.1145/3626198
PG 26
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X6NQ6
UT WOS:001099601100003
OA hybrid
DA 2024-07-18
ER

PT J
AU Zhang, YW
   Yao, T
   Wan, JG
   Xie, CS
AF Zhang, Yiwen
   Yao, Ting
   Wan, Jiguang
   Xie, Changsheng
TI Building GC-free Key-value Store on HM-SMR Drives with ZoneFS
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article; Proceedings Paper
CT ACM Symposium on Operating Systems Principles (SOSP)
CY OCT 26-29, 2021
CL ELECTR NETWORK
SP Assoc Comp Machinery
DE Key-value store; host-managed SMR; LSM-Tree; garbage collection; gear
   compaction
AB Host-managed shingled magnetic recording drives (HM-SMR) are advantageous in capacity to harness the explosive growth of data. For key-value (KV) stores based on log-structured merge trees (LSM-trees), the HM-SMR drive is an ideal solution owning to its capacity, predictable performance, and economical cost. However, building an LSM-tree-based KV store on HM-SMR drives presents severe challenges in maintaining the performance and space utilization efficiency due to the redundant cleaning processes for applications and storage devices (i.e., compaction and garbage collection). To eliminate the overhead of on-disk garbage collection (GC) and improve compaction efficiency, this article presents GearDB, a GC-free KV store tailored for HM-SMR drives. GearDB improves the write performance and space efficiency through three new techniques: a new on-disk data layout, compaction windows, and a novel gear compaction algorithm. We further augment the read performance of GearDB with a new SSTable layout and read ahead mechanism. We implement GearDB with LevelDB, and use zonefs to access a real HM-SMR drive. Our extensive experiments confirm that GearDB achieves both high performance and space efficiency, i.e., on average 1.7x and 1.5x better than LevelDB in random write and read, respectively, with up to 86.9% space efficiency.
C1 [Zhang, Yiwen; Wan, Jiguang; Xie, Changsheng] Huazhong Univ Sci & Technol, Wuhan Natl Lab Optoelect, 1037 Luoyu Rd, Wuhan 430074, Hubei, Peoples R China.
   [Yao, Ting] Huawei Technol Co Ltd, Cloud Storage Serv Prod Dept, Shenzhen 518129, Guangdong, Peoples R China.
C3 Huazhong University of Science & Technology; Huawei Technologies
RP Yao, T (corresponding author), Huawei Technol Co Ltd, Cloud Storage Serv Prod Dept, Shenzhen 518129, Guangdong, Peoples R China.
EM zhangyiwen@hust.edu.cn; yaoting17@huawei.com; jgwan@hust.edu.cn;
   cs_xie@hust.edu.cn
OI Zhang, Yiwen/0000-0001-5279-4816
FU Creative Reaserch Group Project of NSFC [61821003]; National Natural
   Science Foundation of China [62072196]; National Key Research and
   Development Program of China [2018YFB1003305]
FX This work was sponsored in part by the Creative Reaserch Group Project
   of NSFC No. 61821003, the National Natural Science Foundation of China
   under Grant No. 62072196, and the National Key Research and Development
   Program of China No. 2018YFB1003305.
CR Aghayev Abutalib., 2015, P 13 USENIX C FILE S, P135
   Amer A, 2010, IEEE S MASS STOR SYS
   [Anonymous], 2015, P 15 WORKSHOP HOT TO
   Balmau O, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P363
   Bjorling M., 2019, PROC LINUX STORAGE F, V1
   Cassuto Y, 2010, IEEE S MASS STOR SYS
   Chang F, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P205
   Chi-Young Ku S. P. M., 2015, P STOR DEV C
   Choi Changho, 2016, Increasing SSD Performance and Lifetime with Multi-stream Technology
   Cooper B.F., 2010, P ACM S CLOUD COMPUT, DOI DOI 10.1145/1807128.1807152
   Dayan N, 2017, SIGMOD'17: PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P79, DOI 10.1145/3035918.3064054
   Facebook, ROCKSDB PERSISTENT K
   Feldman T., 2013, USENIX LOGIN MAG, V38, P22
   Ghemawat S., 2016, Leveldb
   Gibson G, 2011, CMU-PDL-11-107
   Golan-Gueta Guy, 2015, P 10 EUR C COMP SYST
   Gonzalez Javier, 2014, P NONVOLATILE MEMORY
   HGST, 2018, ULTR DC HC600 SMR SE
   HGST, 2015, HGST DEL WORLDS 1 10
   HGST, 2017, Libzbc Version 5.4. 1
   HGST, 2017, ULTR HS14 14TB 3 5 I
   INCITS T10 Technical Committee, 2017, T10BSRINCITS550 AM N
   INCITS T13 Technical Committee, ZON DEV AT COMM SET
   Jagadish HV, 1997, PROCEEDINGS OF THE TWENTY-THIRD INTERNATIONAL CONFERENCE ON VERY LARGE DATABASES, P16
   Jin C, 2014, IEEE S MASS STOR SYS
   Kadekodi Saurabh, 2015, P 7 USENIX WORKSH HO
   Kannan S, 2018, PROCEEDINGS OF THE 2018 USENIX ANNUAL TECHNICAL CONFERENCE, P993
   Kim Taejin, 2018, P 10 USENIX WORKSHOP
   Lakshman Avinash, 2010, Operating Systems Review, V44, P35, DOI 10.1145/1773912.1773922
   Le Moal Damien, 2020, zonefs: Mapping POSIX File System Interface to Raw Zoned Block Device Accesses
   Lee Changman, 2015, 13 USENIX C FILE STO, P273
   Lee SJ, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P339
   Lu LY, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P133
   Luo Q., 2015, P STORAGE DEVELOPER
   Macko P., 2017, P IEEE MASS STOR SYS, P1
   Manzanares A., 2016, P 8 USENIX WORKSH HO
   Marmol Leonardo, 2014, P 6 USENIX WORKSH HO, P207
   ONeil P, 1996, ACTA INFORM, V33, P351, DOI 10.1007/s002360050048
   Pitchumani Rekha, 2015, P 8 ACM INT SYST STO
   Raju P, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P497, DOI 10.1145/3132747.3132765
   Ren K, 2017, PROC VLDB ENDOW, V10, P2037, DOI 10.14778/3151106.3151108
   Rodeh O, 2013, ACM T STORAGE, V9, DOI 10.1145/2501620.2501623
   Seagate, 2014, ARCH HDDS SEAG
   Seagate, SEAG KIN OP STOR VIS
   Sears Russell, 2012, P 2012 ACM SIGMOD IN, P217, DOI [10.1145/2213836.2213862, DOI 10.1145/2213836.2213862]
   Shetty P.J., 2013, 11 USENIX C FILE STO, P17
   Ting Yao, 2017, P IEEE 33 S MASSIVE
   Western Digital, 2016, DM ZON
   Western Digital, 2019, ZONEFS
   Western Digital Corporation Hans Holmberg, 2020, ZENFS ZON ROCKSDB WH
   Wu Fenggang, 2016, P 8 USENIX WORKSHOP
   XingboWu Yuehai Xu, 2015, P USENIX ANN TECHNIC
   Yao T, 2018, INT PARALL DISTRIB P, P306, DOI 10.1109/IPDPS.2018.00040
NR 53
TC 5
Z9 5
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2022
VL 18
IS 3
AR 23
DI 10.1145/3502846
PG 23
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 4X9BK
UT WOS:000861131000005
DA 2024-07-18
ER

PT J
AU Han, RZ
   Gatla, OR
   Zheng, M
   Cao, JR
   Zhang, D
   Dai, D
   Chen, Y
   Cook, J
AF Han, Runzhou
   Gatla, Om Rameshwar
   Zheng, Mai
   Cao, Jinrui
   Zhang, Di
   Dai, Dong
   Chen, Yong
   Cook, Jonathan
TI A Study of Failure Recovery and Logging of High-Performance Parallel
   File Systems
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Parallel file systems; file system checkers; reliability; failure
   handling; logging; high performance computing; storage systems
AB Large-scale parallel file systems (PFSs) play an essential role in high-performance computing (HPC). However, despite their importance, their reliability is much less studied or understood compared with that of local storage systems or cloud storage systems. Recent failure incidents at real HPC centers have exposed the latent defects in PFS clusters as well as the urgent need for a systematic analysis.
   To address the challenge, we perform a study of the failure recovery and logging mechanisms of PFSs in this article. First, to trigger the failure recovery and logging operations of the target PFS, we introduce a blackbox fault injection tool called PFAULT, which is transparent to PFSs and easy to deploy in practice. PFAULT emulates the failure state of individual storage nodes in the PFS based on a set of pre-defined fault models and enables examining the PFS behavior under fault systematically.
   Next, we apply PFAULT to study two widely used PFSs: Lustre and BeeGFS. Our analysis reveals the unique failure recovery and logging patterns of the target PFSs and identifies multiple cases where the PFSs are imperfect in terms of failure handling. For example, Lustre includes a recovery component called LFSCK to detect and fix PFS-level inconsistencies, but we find that LFSCK itself may hang or trigger kernel panics when scanning a corrupted Lustre. Even after the recovery attempt of LFSCK, the subsequent workloads applied to Lustre may still behave abnormally (e.g., hang or report I/O errors). Similar issues have also been observed in BeeGFS and its recovery component BeeGFS-FSCK. We analyze the root causes of the abnormal symptoms observed in depth, which has led to a new patch set to be merged into the coming Lustre release. In addition, we characterize the extensive logs generated in the experiments in detail and identify the unique patterns and limitations of PFSs in terms of failure logging. We hope this study and the resulting tool and dataset can facilitate follow-up research in the communities and help improve PFSs for reliable high-performance computing.
C1 [Han, Runzhou; Gatla, Om Rameshwar; Zheng, Mai] Iowa State Univ, 613 Morrill Rd, Ames, IA 50011 USA.
   [Cao, Jinrui] SUNY Coll Plattsburgh, 101 Broad St, Plattsburgh, NY 12901 USA.
   [Zhang, Di; Dai, Dong] North Carolina Univ, 9201 Univ City Blvd, Charlotte, NC 28223 USA.
   [Chen, Yong] Texas Tech Univ, 2500 Broadway, Lubbock, TX 79409 USA.
   [Cook, Jonathan] New Mexico State Univ, 1780 E Univ Ave, Las Cruces, NM 88003 USA.
C3 Iowa State University; State University of New York (SUNY) System; SUNY
   Plattsburgh; Texas Tech University System; Texas Tech University; New
   Mexico State University
RP Han, RZ (corresponding author), Iowa State Univ, 613 Morrill Rd, Ames, IA 50011 USA.
EM hanrz@iastate.edu; ogatla@iastate.edu; mai@iastate.edu;
   will_cao@nmsu.edu; dzhang16@uncc.edu; dong.dai@uncc.edu;
   yong.chen@ttu.edu; jcook@cs.nmsu.edu
RI Zhang, Di/IUN-3768-2023; Dai, Dong/ABG-7810-2020; Zhang,
   Di/HRA-5319-2023
OI Dai, Dong/0000-0003-4078-8149; Zheng, Mai/0000-0002-0741-3436
FU NSF [CCF-1717630/1853714, CCF-1910747, CNS-1943204]
FX This work was supported in part by NSF under grants CCF-1717630/1853714,
   CCF-1910747, and CNS-1943204. Any opinions, findings, and conclusions
   expressed in this material are those of the authors and do not
   necessarily reflect the views of the sponsor.
CR Alagappan R, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P15
   Ali N, 2009, 2009 IEEE INTERNATIONAL CONFERENCE ON CLUSTER COMPUTING AND WORKSHOPS, P86
   Alquraan A, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P51
   Alvaro P, 2015, SIGMOD'15: PROCEEDINGS OF THE 2015 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P331, DOI 10.1145/2723372.2723711
   [Anonymous], 2016, HPC User Site Census
   [Anonymous], 2001, ASPECTJ
   [Anonymous], 2017, SQLITE DOCUMENTS
   [Anonymous], 2019, SIMPLE LOGGING FACAD
   [Anonymous], 2020, LLVM COMPILER INFRAS
   [Anonymous], 2001, APACHE LOG4J LOGGING
   [Anonymous], 2015, WALA HOME PAGE
   [Anonymous], 1999, JAVA BYTECODE ENG TO
   [Anonymous], 2017, LFSCK ONLINE FILE SY
   [Anonymous], 2020, BEEGFS DOCUMENTATION
   [Anonymous], 2017, E2FSPROGS EXT234 FIL
   [Anonymous], 2017, ORANGEFS PROJECT
   [Anonymous], 2017, NVM EXPRESS FABRICS
   [Anonymous], 2017, DEBUGFS
   [Anonymous], 2020, APACHE HADOOP YARN
   [Anonymous], 2020, SOCKET 2 LINUX MANUA
   [Anonymous], 2017, NETWORK PARTITION
   [Anonymous], 2020, LUSTRE PATCH LU 1398
   [Anonymous], BEEGFS FILE SYSTEM
   [Anonymous], JEPSEN
   [Anonymous], LUSTRE FILE SYSTEM
   [Anonymous], 2017, DAT DOWNL
   [Anonymous], 2017, LINUX SCSI TARGET FR
   [Anonymous], JAVA VIRTUAL MACHINE
   [Anonymous], 2006, HADOOP DISTRIBUTED F
   [Anonymous], 2015, HPC 5 OPEN SOURCE SO
   [Anonymous], 2017, DARSHAN HPC I CHARAC
   [Anonymous], 2017, Lustre Software Release 2.x: Operations Manual
   [Anonymous], TRAIL REFLECTION API
   [Anonymous], 2020, GITLAB REPOSITORY PF
   [Anonymous], 2017, MONTAGE ASTRONOMICAL
   [Anonymous], 2017, E2FSCK 8 LINUX MANUA
   [Anonymous], 2019, Top500 supercomputer sites
   Apache Cassandra, 2008, US
   Apache Hadoop, 2019, US
   Apache HBase, 2020, US
   Apache Zookeeper, ABOUT US
   Bairavasundaram Lakshmi N., 2008, ACM Transaction on Storage, V4, DOI 10.1145/1416944.1416947
   Bairavasundaram LN, 2007, PERF E R SI, V35, P289
   Bairavasundaram Lakshmi N., 2009, Proceedings of the 2009 Conference on USENIX Annual Technical Conference, USENIX'09, P7
   Banabic R., 2011, 2011 IEEE/IFIP 41st International Conference on Dependable Systems and Networks Workshops (DSN-W), P188, DOI 10.1109/DSNW.2011.5958811
   BARTON JH, 1990, IEEE T COMPUT, V39, P575, DOI 10.1109/12.54853
   Cao JR, 2016, PROCEEDINGS OF PDSW-DISCS 2016 - 1ST JOINT INTERNATIONAL WORKSHOP ON PARALLEL DATA STORAGE AND DATA INTENSIVE SCALABLE COMPUTING SYSTEMS, P49, DOI [10.1109/PDSW-DISCS.2016.12, 10.1109/PDSW-DISCS.2016.013]
   Cao Jinrui, 2018, P 2018 INT C SUPERCO
   Carreira JoaoCarlos Menezes., 2012, P 7 ACM EUROPEAN C C, P239
   Chen HG, 2015, SOSP'15: PROCEEDINGS OF THE TWENTY-FIFTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P18, DOI 10.1145/2815400.2815402
   CloudLab, US
   Conway A, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P45
   Dai Dong, 2019, 2019 35 S MASS STORA
   Dawson S, 1996, IEEE INTERNATIONAL COMPUTER PERFORMANCE AND DEPENDABILITY SYMPOSIUM - IPDS'96, PROCEEDINGS, P56, DOI 10.1109/IPDS.1996.540200
   Du M, 2017, CCS'17: PROCEEDINGS OF THE 2017 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1285, DOI 10.1145/3133956.3134015
   FUSE, Linux FUSE (Filesystem in Userspace) interface
   Ganesan A, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P149
   Gatla OR, 2018, ACM T STORAGE, V14, DOI 10.1145/3281031
   Gatla OR, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P105
   Gatla Om Rameshwar, 2017, P 9 USENIX WORKSHOP
   Ghemawat S., 2003, ACM SIGOPS OPERATING, V37, P29, DOI [DOI 10.1145/945445.945450, DOI 10.1145/1165389.945450, 10.1145/1165389.945450]
   Gill P, 2011, ACM SIGCOMM COMP COM, V41, P350, DOI 10.1145/2043164.2018477
   GPFS Failures at Ohio Supercomputer Center (OSC), 2016, US
   Gunawi H.S., 2008, P USENIX S OPERATING
   Gunawi HS, 2016, PROCEEDINGS OF THE SEVENTH ACM SYMPOSIUM ON CLOUD COMPUTING (SOCC 2016), P1, DOI 10.1145/2987550.2987583
   Gunawi Haryadi S., 2011, P 8 USENIX S NETWORK
   Gunawi Haryadi S., 2018, 16 USENIX C FILE STO
   Han Runzhou, 2020, P 5 INT PARALLEL DAT
   High Performance Computing Center Texas Tech University, 2017, US
   Huang DC, 2013, I S MOD ANAL SIM COM, P111, DOI 10.1109/MASCOTS.2013.19
   Jeong DR, 2019, P IEEE S SECUR PRIV, P754, DOI 10.1109/SP.2019.00017
   Joshi P., 2013, P 1 ACM SIGOPS C TIM, P1
   Joshi P, 2011, OOPSLA 11: PROCEEDINGS OF THE 2011 ACM INTERNATIONAL CONFERENCE ON OBJECT ORIENTED PROGRAMMING SYSTEMS LANGUAGES AND APPLICATIONS, P171
   Kim S, 2019, PROCEEDINGS OF THE TWENTY-SEVENTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '19), P147, DOI 10.1145/3341301.3359662
   Leesatapornwongsa Tanakorn, 2014, P 11 USENIX S OPERAT, P399
   Lu J, 2019, PROCEEDINGS OF THE TWENTY-SEVENTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '19), P114, DOI 10.1145/3341301.3359645
   Lu Lanyue., 2013, Proceedings of the 11th USENIX Conference on File and Storage Technologies, FAST'13, P31
   Ma Ao, 2015, 13 USENIX C FIL STOR
   Manès VJM, 2021, IEEE T SOFTWARE ENG, V47, P2312, DOI 10.1109/TSE.2019.2946563
   Mesnier Michael P, 2007, Trace: parallel trace replay with approximate causal events
   MILLER BP, 1990, COMMUN ACM, V33, P32, DOI 10.1145/96267.96279
   Min C, 2015, SOSP'15: PROCEEDINGS OF THE TWENTY-FIFTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P361, DOI 10.1145/2815400.2815422
   Multiple Switch Outages at Ohio Supercomputer Center (OSC), 2016, US
   Myers Glenford J, 2004, The art of software testing, V2
   Nightingale EB, 2011, EUROSYS 11: PROCEEDINGS OF THE EUROSYS 2011 CONFERENCE, P343
   Open MPI, 2004, US
   Patterson D., 1988, CASE REDUNDANT ARRAY, V17
   Pillai TS., 2014, P 11 USENIX S OPERAT
   Power Outage Event at High Performance Computing Center (HPCC) in Texas, 2016, US
   Prabhakaran Vijayan, 2005, P OF THE 20 ACM S ON, V39, P206
   Primmer M, 1996, HEWLETT-PACKARD J, V47, P94
   Roth P.C., 2007, PDSW 07 P 2 INT WORK, P50, DOI DOI 10.1145/1374596.1374609
   Sandberg R., 1988, Innovations in internetworking. chapter Design and implementation of the Sun network filesystem, P379
   Schroeder B, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 5TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES ( FAST '07), P1
   Schroeder B, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P67
   Scott Colin, 2015, FUZZING RAFT FUN PUB
   Seelam S, 2008, 2008 IEEE INTERNATIONAL SYMPOSIUM ON PARALLEL & DISTRIBUTED PROCESSING, VOLS 1-8, P3588
   Seungjae Han, 1995, Proceedings. International Computer Performance and Dependability Symposium (Cat. No.95TH8034), P204, DOI 10.1109/IPDS.1995.395831
   Smith K. A., 1997, Performance Evaluation Review, V25, P203, DOI 10.1145/258623.258689
   Stott D. T., 2000, Proceedings IEEE International Computer Performance and Dependability Symposium. IPDS 2000, P91, DOI 10.1109/IPDS.2000.839467
   Stuardo CA, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P359
   Subramanian S, 2010, PROC INT CONF DATA, P509, DOI 10.1109/ICDE.2010.5447821
   Sun J, 2020, DISP R CNN STEREO 3D, DOI 10.1016/j.cell.2020.06.010
   Tucek Joseph, 2013, P 11 USENIX C FIL ST
   Pham VT, 2020, IEEE INT CONF SOFTW, P460, DOI 10.1109/ICST46399.2020.00062
   Vetter JS, 2001, ACM SIGPLAN NOTICES, V36, P123, DOI 10.1145/568014.379590
   Xia M, 2015, P 13 USENIX C FIL ST, P213
   Xu E, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P961
   Xu EC, 2018, PROCEEDINGS OF 2018 IEEE/ACM 3RD JOINT INTERNATIONAL WORKSHOP ON PARALLEL DATA STORAGE & DATA INTENSIVE SCALABLE COMPUTING SYSTEMS (PDSW-DISCS), P45, DOI 10.1109/PDSW-DISCS.2018.00010
   Xu M, 2020, P IEEE S SECUR PRIV, P1643, DOI 10.1109/SP40000.2020.00078
   Xu W, 2009, SOSP'09: PROCEEDINGS OF THE TWENTY-SECOND ACM SIGOPS SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P117
   Xu W, 2019, P IEEE S SECUR PRIV, P818, DOI 10.1109/SP.2019.00035
   Yang JF, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P131
   Yuan D, 2012, ACM T COMPUT SYST, V30, DOI 10.1145/2110356.2110360
   Yuan XH, 2020, TWENTY-FIFTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXV), P1141, DOI 10.1145/3373376.3378484
   Zhang Di, 2021, P 13 ACM WORKSHOP HO
   Zhang Duo, 2021, P 14 ACM INT C SYSTE
   Zheng M, 2017, ACM T COMPUT SYST, V34, DOI 10.1145/2992782
   Zheng Mai., 2014, Proceedings of the 11th USENIX Symposium on Operating Systems Design and Implementation OSDI '14, P449
NR 119
TC 9
Z9 9
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAY
PY 2022
VL 18
IS 2
AR 14
DI 10.1145/3483447
PG 44
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 1H3XN
UT WOS:000796478700005
OA Bronze
DA 2024-07-18
ER

PT J
AU Im, J
   Bae, J
   Chung, C
   Arvind
   Lee, S
AF Im, Junsu
   Bae, Jinwook
   Chung, Chanwoo
   Arvind
   Lee, Sungjin
TI Design of LSM-tree-based Key-value SSDs with Bounded Tails
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Log-structured merge-tree; key-value store; key-value SSD; tail latency
AB Key-value store based on a log-structured merge-tree (LSM-tree) is preferable to hash-based key-value store, because an LSM-tree can support a wider variety of operations and show better performance, especially for writes. However, LSM-tree is difficult to implement in the resource constrained environment of a key-value SSD (KV-SSD), and, consequently, KV-SSDs typically use hash-based schemes. We present PinK, a design and implementation of an LSM-tree-based KV-SSD, which compared to a hash-based KV-SSD, reduces 99th percentile tail latency by 73%, improves average read latency by 42%, and shows 37% higher throughput. The key idea in improving the performance of an LSM-tree in a resource constrained environment is to avoid the use of Bloom filters and instead, use a small amount of DRAM to keep/pin the top levels of the LSM-tree. We also find that PinK is able to provide a flexible design space for a wide range of KV workloads by leveraging the read-write tradeoff in LSM-trees.
C1 [Im, Junsu; Bae, Jinwook; Lee, Sungjin] Daegu Gyeongbuk Inst Sci & Technol, 333 Techno Jungang Daero, Daegu, South Korea.
   [Chung, Chanwoo; Arvind] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
C3 Daegu Gyeongbuk Institute of Science & Technology (DGIST); Massachusetts
   Institute of Technology (MIT)
RP Lee, S (corresponding author), Daegu Gyeongbuk Inst Sci & Technol, 333 Techno Jungang Daero, Daegu, South Korea.
EM junsu_im@dgist.ac.kr; jinwook.bae@dgist.ac.kr; cwchung@csail.mit.edu;
   arvind@csail.mit.edu; sungjin.lee@dgist.ac.kr
OI Lee, Sungjin/0000-0002-9753-2286; Im, Junsu/0000-0002-7485-7870
FU Samsung Research Funding & Incubation Center of Samsung Electronics
   [SRFC-IT1701-11]; National Research Foundation (NRF) of Korea
   [NRF-2018R1A5A1060031]; NSF [CCF-1725303]; Samsung Semiconductor (GRO)
FX An earlier version of this articlewas presented at the 2020 USENIX
   Annual Technical Conference, July 15-17, 2020 [21]. This work was
   supported by Samsung Research Funding & Incubation Center of Samsung
   Electronics under Project Number SRFC-IT1701-11. We also thank Samsung
   Electronics for providing KV-SSD prototypes. The DGIST team was
   supported by the National Research Foundation (NRF) of Korea
   (NRF-2018R1A5A1060031). Arvind and Chanwoo Chung was partially funded by
   NSF (CCF-1725303) and Samsung Semiconductor (GRO grants).
CR Agrawal Nitin, 2008, 2008 USENIX ANN TECH
   [Anonymous], 2020, PINK SW SOURCE CODE
   [Anonymous], 2020, PINK HW SOURCE CODE
   Ashkiani S, 2018, INT PARALL DISTRIB P, P430, DOI 10.1109/IPDPS.2018.00053
   Atikoglu Berk, 2012, Performance Evaluation Review, V40, P53, DOI 10.1145/2318857.2254766
   Bae DH, 2018, CONF PROC INT SYMP C, P425, DOI 10.1109/ISCA.2018.00043
   Bender MA, 2012, PROC VLDB ENDOW, V5, P1627
   Chandramouli B, 2018, INT CONF MANAGE DATA, P275, DOI 10.1145/3183713.3196898
   Chazelle B, 1986, ALGORITHMICA, V1, P133, DOI 10.1007/BF01840440
   Chung C, 2019, TWENTY-FOURTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXIV), P939, DOI 10.1145/3297858.3304022
   Colgrove J, 2015, SIGMOD'15: PROCEEDINGS OF THE 2015 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P1683, DOI 10.1145/2723372.2742798
   Cooper Brian F., 2010, P 1 ACM S CLOUD COMP, P143, DOI [DOI 10.1145/1807128.1807152, 10.1145/1807128.1807152]
   Dayan N, 2018, INT CONF MANAGE DATA, P505, DOI 10.1145/3183713.3196927
   Dayan N, 2017, SIGMOD'17: PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P79, DOI 10.1145/3035918.3064054
   Debnath B, 2010, PROC VLDB ENDOW, V3, P1414, DOI 10.14778/1920841.1921015
   Facebook Inc., ROCKSDB PERSISTENT K
   Gupta A, 2009, ACM SIGPLAN NOTICES, V44, P229, DOI 10.1145/1508284.1508271
   Herlihy M, 2008, LECT NOTES COMPUT SC, V5218, P350, DOI 10.1007/978-3-540-87779-0_24
   IC Knowledge LLC, 2020, LITH 2020 EC 3D ER
   Im J, 2020, PROCEEDINGS OF THE 2020 USENIX ANNUAL TECHNICAL CONFERENCE, P173
   Jens Axboe, 2005, FIO FLEX I O TEST SY
   Jin YQ, 2017, INT S HIGH PERF COMP, P373, DOI 10.1109/HPCA.2017.15
   Jun SW, 2015, 2015 ACM/IEEE 42ND ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA), P1, DOI 10.1145/2749469.2750412
   Kang Y, 2019, SYSTOR '19: PROCEEDINGS OF THE 12TH ACM INTERNATIONAL SYSTEMS AND STORAGE CONFERENCE, P144, DOI 10.1145/3319647.3325831
   Kim Sang-Hoon, 2019, P USENIX WORKSH HOT
   Kourtis K, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P1
   Lakshman Avinash, 2010, Operating Systems Review, V44, P35, DOI 10.1145/1773912.1773922
   Lee CG, 2019, I S MOD ANAL SIM COM, P384, DOI 10.1109/MASCOTS.2019.00048
   Lepers B, 2019, PROCEEDINGS OF THE TWENTY-SEVENTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '19), P447, DOI 10.1145/3341301.3359628
   Lu LY, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P133
   Luo C, 2020, VLDB J, V29, P393, DOI 10.1007/s00778-019-00555-y
   Marmol L., 2014, HotStorage, P8
   NGD Systems Inc, 2018, NGD CAT NVME SSD
   ONeil P, 1996, ACTA INFORM, V33, P351, DOI 10.1007/s002360050048
   Pagh R, 2004, J ALGORITHMS, V51, P122, DOI 10.1016/j.jalgor.2003.12.002
   Ren Garth Gibson Kai, 2013, P USENIX ANN TECHN C
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   Samsung Electornics, 2018, SAM SMART SSD
   Samsung Electronics, 2016, SAM INTR WORLD LARG
   Samsung Electronics, 2017, SAMS KEY AL SSD EN H
   Samsung Electronics, KV SSD HOST SOFTW PA
   Samsung Electronics, 2019, 960PRO SSD SPEC
   Samsung Electronics, 2018, KV SSD FIRMW INTR
   Samsung Electronics, 2018, 860EVO SSD SPEC
   Sheehy J., 2010, Basho White Paper
   SNIA, KEY VAL STOR API SPE
   Twitter Inc, FATC MEMC SSD
   Wang J, 2013, P INT COMP SOFTW APP, P240, DOI 10.1109/COMPSAC.2013.40
   Xilinx, 2018, XIL ZYNQ ULTRASCALE
   Xu ST, 2016, PROC VLDB ENDOW, V10, P301
NR 50
TC 2
Z9 2
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUN
PY 2021
VL 17
IS 2
AR 10
DI 10.1145/3452846
PG 27
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SV7JF
UT WOS:000663994900002
OA Bronze
DA 2024-07-18
ER

PT J
AU Kwak, J
   Lee, S
   Park, K
   Jeong, J
   Song, YH
AF Kwak, Jaewook
   Lee, Sangjin
   Park, Kibin
   Jeong, Jinwoo
   Song, Yong Ho
TI Cosmos plus OpenSSD: Rapid Prototype for Flash Storage Systems
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Flash memory; storage system; solid state drive (SSD); flash translation
   layer (FTL)
ID MEMORY; PARALLELISM
AB As semiconductor technology has advanced, many storage systems have begun to use non-volatile memories as storage media. The organization and architecture of storage controllers have become more complex to meet various design requirements in terms of performance, response time, quality of service (QoS), and so on. In addition, due to the evolution of memory technology and the emergence of new applications, storage controllers employ new firmware algorithms and hardware modules. When designing storage controllers, engineers often evaluate the performance impact of using new software and hardware components using software simulators. However, this technique often yields limited evaluation accuracy because of the difficulty of modeling complex operations of components and the interactions among them. In this article, we present a reconfigurable flash storage controller design that serves as a rapid prototype. This design can be synthesized into a field-programmable gate array device and used in a realistic performance evaluation environment. We show the usefulness of our design by demonstrating the performance impact of design parameters.
C1 [Kwak, Jaewook; Lee, Sangjin; Park, Kibin; Jeong, Jinwoo; Song, Yong Ho] Hanyang Univ, 222 Wangsimni Ro, Seoul 04763, South Korea.
C3 Hanyang University
RP Kwak, J (corresponding author), Hanyang Univ, 222 Wangsimni Ro, Seoul 04763, South Korea.
EM jwkwak@enc.hanyang.ac.kr; sjlee@enc.hanyang.ac.kr;
   kbpark@enc.hanyang.ac.kr; jwjeong@enc.hanyang.ac.kr;
   yhsong@enc.hanyang.ac.kr
FU R&D program of MOTIE/KEIT [10077609]
FX This work was supported by the R&D program of MOTIE/KEIT. [10077609,
   Developing Processor-Memory-Storage Integrated Architecture for Low
   Power, High-Performance Big Data Servers].
CR An S, 2015, INT SOC DESIGN CONF, P281, DOI 10.1109/ISOCC.2015.7401757
   Binkert Nathan, 2011, Computer Architecture News, V39, P1, DOI 10.1145/2024716.2024718
   Bjorling M, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P359
   Cai Y, 2011, ANN IEEE SYM FIELD P, P101, DOI 10.1109/FCCM.2011.28
   Cha J, 2013, ETRI J, V35, P166, DOI 10.4218/etrij.13.0212.0273
   Chen F, 2011, INT S HIGH PERF COMP, P266, DOI 10.1109/HPCA.2011.5749735
   Davis John D., 2009, P 1 WORKSH INT SOL S
   Gao CM, 2014, IEEE S MASS STOR SYS
   Gouk D, 2018, 2018 51ST ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE (MICRO), P469, DOI 10.1109/MICRO.2018.00045
   Gu B, 2016, CONF PROC INT SYMP C, P153, DOI 10.1109/ISCA.2016.23
   Higashimori T, 2016, INT J STROKE, V11, P118
   Ho KC, 2013, ISSCC DIG TECH PAP I, V56, P222, DOI 10.1109/ISSCC.2013.6487709
   Iometer Project, 2014, IOM 1 1 0
   Jhin Jhuyeong, 2018, P 12 INT C UB INF MA, P75
   Jonghong Kim, 2010, Proceedings of the 2010 IEEE Workshop on Signal Processing Systems (SiPS 2010), P392, DOI 10.1109/SIPS.2010.5624877
   Jung M, 2018, IEEE COMPUT ARCHIT L, V17, P37, DOI 10.1109/LCA.2017.2750658
   Jung M, 2012, CONF PROC INT SYMP C, P404
   Kim JY, 2015, POSTHARVEST BIOL TEC, V110, P43, DOI 10.1016/j.postharvbio.2015.07.015
   Kim Y, 2009, SIMUL: 2009 FIRST INTERNATIONAL CONFERENCE ON ADVANCES IN SYSTEM SIMULATION, P125, DOI 10.1109/SIMUL.2009.17
   Lee J., 2009, P 2009 ACM S APPL CO, P318
   Lee KK, 2012, EVID-BASED COMPL ALT, V2012, DOI 10.1155/2012/130875
   Lee S., 2010, WARP 5 ANN WORKSH AR
   Li S, 2010, IEEE T VLSI SYST, V18, P1412, DOI 10.1109/TVLSI.2009.2024154
   Mao B, 2015, 2015 33RD IEEE INTERNATIONAL CONFERENCE ON COMPUTER DESIGN (ICCD), P447, DOI 10.1109/ICCD.2015.7357145
   Nam EH, 2011, IEEE T COMPUT, V60, P653, DOI 10.1109/TC.2010.209
   NVMe Express, 2017, NVME EXPR SPEC 1 3
   OpenSSD Project, 2011, JASM OPENSSD
   Picoli IL, 2017, PROCEEDINGS OF THE 8TH ASIA-PACIFIC WORKSHOP ON SYSTEMS (APSYS '17), DOI 10.1145/3124680.3124741
   Seshadri S, 2014, MANAGEMENT OF INFERTILITY FOR THE MRCOG AND BEYOND, 3RD EDITION, P67
   Song LL, 2002, IEEE J SOLID-ST CIRC, V37, P1565, DOI 10.1109/JSSC.2002.803931
   Song Yong Ho, 2014, FLASH MEM SUMM
   Tavakkol A, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P49
   Torabzadehkashi M, 2018, IEEE SYM PARA DISTR, P1260, DOI 10.1109/IPDPSW.2018.00195
   Tsai HY, 2012, IEEE ASIAN SOLID STA, P61
   Wei Y, 2014, 2014 INTERNATIONAL CONFERENCE ON OPTICAL NETWORK DESIGN AND MODELING, P1
   Yoo J, 2013, IEEE S MASS STOR SYS
   Zhang JC, 2016, PROCEEDINGS OF USENIX ATC '16: 2016 USENIX ANNUAL TECHNICAL CONFERENCE, P87
NR 37
TC 33
Z9 33
U1 2
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2020
VL 16
IS 3
AR 15
DI 10.1145/3385073
PG 35
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1QZ
UT WOS:000583743600001
DA 2024-07-18
ER

PT J
AU Kolosov, O
   Yadgar, G
   Liram, M
   Tamo, I
   Barg, A
AF Kolosov, Oleg
   Yadgar, Gala
   Liram, Matan
   Tamo, Itzhak
   Barg, Alexander
TI On Fault Tolerance, Locality, and Optimality in Locally Repairable Codes
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Erasure codes; local repair
ID DISTRIBUTED STORAGE
AB Erasure codes in large-scale storage systems allow recovery of data from a failed node. A recently developed class of codes, locally repairable codes (LRCs), offers tradeoffs between storage overhead and repair cost. LRCs facilitate efficient recovery scenarios by adding parity blocks to the system. However, these additional blocks may eventually increase the number of blocks that must be reconstructed. Existing LRCs differ in their use of the parity blocks, in their locality semantics, and in their parameter space. Thus, existing theoretical models cannot directly compare different LRCs to determine which code offers the best recovery performance, and at what cost.
   We perform the first systematic comparison of existing LRC approaches. We analyze Xorbas, Azure's LRCs, and Optimal-LRCs in light of two new metrics: average degraded read cost and normalized repair cost. We show the tradeoff between these costs and the code's fault tolerance, and that different approaches offer different choices in this tradeoff. Our experimental evaluation on a Ceph cluster further demonstrates the different effects of realistic system bottlenecks on the benefit from each LRC approach. Despite these differences, the normali ed repair cost metric can reliably identify the LRC approach that would achieve the lowest repair cost in each setup.
C1 [Kolosov, Oleg] Tel Aviv Univ, Tel Aviv, Israel.
   [Kolosov, Oleg; Yadgar, Gala; Liram, Matan] Technion Israel Inst Technol, CS Taub Bldg, IL-3200003 Haifa, Israel.
   [Tamo, Itzhak] Tel Aviv Univ, Dept Elect Engn, POB 39040, IL-6997801 Tel Aviv, Israel.
   [Barg, Alexander] Univ Maryland, Dept ECE ISR, College Pk, MD 20742 USA.
   [Barg, Alexander] Russian Acad Sci, Inst Problems Informat Transmiss IITP, Moscow, Russia.
C3 Tel Aviv University; Technion Israel Institute of Technology; Tel Aviv
   University; University System of Maryland; University of Maryland
   College Park; Russian Academy of Sciences
RP Barg, A (corresponding author), Univ Maryland, Dept ECE ISR, College Pk, MD 20742 USA.; Barg, A (corresponding author), Russian Acad Sci, Inst Problems Informat Transmiss IITP, Moscow, Russia.
EM abarg@umd.edu
RI Kolosov, Oleg/JDN-0179-2023; Tamo, Itzhak/M-7288-2015
OI Kolosov, Oleg/0009-0005-9105-5094; Yadgar, Gala/0000-0003-2701-0260
FU ISF [1030/15]; NSF-BSF grant [2015814]; NSF [CCF1618603, CCF1814487]
FX The research of I. Tamo was supported by ISF grant 1030/15 and NSF-BSF
   grant 2015814. The research of A. Barg was supported by NSF grants
   CCF1618603 and CCF1814487.
CR Amazon, 2017, AM EC2 REG AV ZON
   Amazon, 2017, AM EBS VOL
   [Anonymous], 2017, Amazon EC2 Instance Types
   Apache Hadoop, 2017, HDFS ER COD
   Blaum M., 1994, P 21 ANN INT S COMP
   Ceph, 2017, JER ER COD PLUG
   Ceph, 2017, LOC REP ER COD PLUG
   Chen YL, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P539
   Dimakis AG, 2010, IEEE T INFORM THEORY, V56, P4539, DOI 10.1109/TIT.2010.2054295
   Estrada-Galiñanes V, 2018, I C DEPEND SYS NETWO, P183, DOI 10.1109/DSN.2018.00030
   Gad EE, 2013, IEEE INT SYMP INFO, P887, DOI 10.1109/ISIT.2013.6620354
   GitHub, 2018, OPT LRC MATL SOURC C
   Gopalan P, 2012, IEEE T INFORM THEORY, V58, P6925, DOI 10.1109/TIT.2012.2208937
   Guruswami Venkatesan, 2016, P 48 ANN ACM SIGACT
   Huang C, 2013, ACM T STORAGE, V9, DOI 10.1145/2435204.2435207
   Huang Cheng, 2012, 2012 USENIX ANN TECH, P15
   Kadekodi S, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P345
   Khan Osama., 2012, FAST, P20
   Kolosov O., 2018, ARXIV180200157
   Kolosov O, 2018, PROCEEDINGS OF THE 2018 USENIX ANNUAL TECHNICAL CONFERENCE, P865
   Kolosov Oleg, 2018, THESIS
   KUBIATOWICZ J, 2000, P INT C ARCH SUPP PR
   Li J, 2016, IEEE T INFORM THEORY, V62, P4848, DOI 10.1109/TIT.2016.2590429
   Li MQ, 2014, ACM T STORAGE, V10, DOI 10.1145/2658991
   Li RH, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P567
   Li XL, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P331
   Liu J, 2018, IEEE T INFORM THEORY, V64, P889, DOI 10.1109/TIT.2017.2713245
   Mitra S, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901328
   Nightingale EdmundB., 2012, P 10 USENIX C OPERAT, P1
   Oggier F, 2011, IEEE INFOCOM SER, P1215, DOI 10.1109/INFCOM.2011.5934901
   Pamies-Juarez L, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P81
   Plank J., 2013, USENIX C FILE STOR T, P299
   Plank JS, 2014, ACM T STORAGE, V10, DOI 10.1145/2560013
   PLANK JAMESS., 2009, FAST, P253
   Rashmi K., 2015, USENIX C FIL STOR TE, P81
   Rashmi KV, 2014, SIGCOMM'14: PROCEEDINGS OF THE 2014 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P331, DOI [10.1145/2619239.2626325, 10.1145/2740070.2626325]
   Rashmi KV, 2011, IEEE T INFORM THEORY, V57, P5227, DOI 10.1109/TIT.2011.2159049
   Rashmi K.V., 2013, P 5 USENIX C HOT TOP, P8
   REED IS, 1960, J SOC IND APPL MATH, V8, P300, DOI 10.1137/0108018
   Rosenfeld Eitan, 2013, P 7 ANN WORKSH INT V
   Sathiamoorthy M, 2013, PROC VLDB ENDOW, V6, P325, DOI 10.14778/2535573.2488339
   Shen ZR, 2017, IEEE T PARALL DISTR, V28, P877, DOI 10.1109/TPDS.2016.2591040
   Shor R, 2018, SYSTOR'18: PROCEEDINGS OF THE 11TH ACM INTERNATIONAL SYSTEMS AND STORAGE CONFERENCE, P76, DOI 10.1145/3211890.3211896
   Silberstein Mark., 2014, Proceedings of International Conference on Systems and Storage, P1
   Tamo I, 2014, IEEE T INFORM THEORY, V60, P4661, DOI 10.1109/TIT.2014.2321280
   Tamo I, 2013, IEEE T INFORM THEORY, V59, P1597, DOI 10.1109/TIT.2012.2227110
   Vajha M, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P139
   Wang Feiyi, 2013, P 8 PAR DAT STOR WOR
   Wang ZY, 2010, IEEE GLOBE WORK, P1905, DOI 10.1109/GLOCOMW.2010.5700274
   Wang Zhufan, 2019, P 2019 USENIX ANN TE
   Weil SA, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P307
   Weil Sage A., 2006, P ACM IEEE C SUP SC, P122, DOI [10.1145/1188455.1188582, DOI 10.1145/1188455.1188582]
   Weil Sage A., 2007, PDSW, P35
   Xie Xin, 2019, P 2019 35 S MASS STO
   Yang AB, 2015, PROCEEDINGS OF CROSS-CULTURAL OCCUPATIONAL HEALTH PSYCHOLOGY FORUM, P213
   Ye M, 2017, IEEE T INFORM THEORY, V63, P2001, DOI 10.1109/TIT.2017.2661313
   Zeh Alexander, 2016, ARXIV160102763
   Zhang GY, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P279
   Zhou TL, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P317
NR 59
TC 10
Z9 10
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUN
PY 2020
VL 16
IS 2
AR 11
DI 10.1145/3381832
PG 32
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1RC
UT WOS:000583743900004
DA 2024-07-18
ER

PT J
AU Sivathanu, M
   Vuppalapati, M
   Gulavani, BS
   Rajan, K
   Leeka, J
   Mohan, J
   Kedia, P
AF Sivathanu, Muthian
   Vuppalapati, Midhul
   Gulavani, Bhargav S.
   Rajan, Kaushik
   Leeka, Jyoti
   Mohan, Jayashree
   Kedia, Piyus
TI INSTalytics: Cluster Filesystem Co-design for Big-data Analytics
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Storage replication; data center storage; big data query processing
AB We present the design, implementation, and evaluation of INSTalytics, a co-designed stack of a cluster file system and the compute layer, for efficient big-data analytics in large-scale data centers. INSTalytics amplifies the well-known benefits of data partitioning in analytics systems; instead of traditional partitioning on one dimension, INSTalytics enables data to be simultaneously partitioned on four different dimensions at the same storage cost, enabling a larger fraction of queries to benefit from partition filtering and joins without network shuffle.
   To achieve this, INSTalytics uses compute-awareness to customize the three-way replication that the cluster file system employs for availability. A new heterogeneous replication layout enables INSTalytics to preserve the same recovery cost and availability as traditional replication. INSTalytics also uses compute-awareness to expose a new sliced-read API that improves performance of joins by enabling multiple compute nodes to read slices of a data block efficiently via co-ordinated request scheduling and selective caching at the storage nodes.
   We have built a prototype implementation of INSTalytics in a production analytics stack, and we show that recovery performance and availability is similar to physical replication, while providing significant improvements in query performance, suggesting a new approach to designing cloud-scale big-data analytics systems.
C1 [Sivathanu, Muthian; Vuppalapati, Midhul; Gulavani, Bhargav S.; Rajan, Kaushik; Leeka, Jyoti] Microsoft Res India, Vigyan 9,Lavelle Rd, Bangalore 560001, Karnataka, India.
   [Mohan, Jayashree] Univ Texas Austin, 2317 Speedway, Austin, TX 78712 USA.
   [Kedia, Piyus] Indraprastha Inst Informat Technol Delhi, Okhla Ind Estate,Phase 3, Delhi 110020, India.
C3 Microsoft; University of Texas System; University of Texas Austin;
   Indraprastha Institute of Information Technology Delhi
RP Sivathanu, M (corresponding author), Microsoft Res India, Vigyan 9,Lavelle Rd, Bangalore 560001, Karnataka, India.
EM muthian@microsoft.com; midhul.v@gmail.com; bharg@microsoft.com;
   krajan@microsoft.com; jyleeka@microsoft.com; jaya@cs.utexas.edu;
   piyus@iiitd.ac.in
CR AMPLab, AMP BIG DAT BENCHM
   Armbrust M, 2015, SIGMOD'15: PROCEEDINGS OF THE 2015 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P1383, DOI 10.1145/2723372.2742797
   Arpaci-Dusseau A. C., 2001, Operating Systems Review, V35, P43, DOI 10.1145/502059.502040
   Bindschaedler L, 2018, EUROSYS '18: PROCEEDINGS OF THE THIRTEENTH EUROSYS CONFERENCE, DOI 10.1145/3190508.3190532
   BLAUM M, 1995, IEEE T COMPUT, V44, P192, DOI 10.1109/12.364531
   Boutin E, 2014, 11 USENIX S OP SYST, P285
   Curino C, 2019, PROCEEDINGS OF THE 16TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P177
   Dean J, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE SIXTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDE '04), P137
   Dittrich J, 2012, PROC VLDB ENDOW, V5, P1591, DOI 10.14778/2350229.2350272
   Dittrich J, 2010, PROC VLDB ENDOW, V3, P518
   Eltabakh MY, 2011, PROC VLDB ENDOW, V4, P575, DOI 10.14778/2002938.2002943
   Ford Daniel, 2010, P USENIX OSDI
   Ghemawat S., 2003, Operating Systems Review, V37, P29, DOI 10.1145/1165389.945450
   HSIAO HI, 1990, PROCEEDINGS : 6TH INTERNATIONAL CONFERENCE ON DATA ENGINEERING, P456
   Huang Cheng, 2012, 2012 USENIX ANN TECH, P15
   Isard M., 2007, Operating Systems Review, V41, P59, DOI 10.1145/1272998.1273005
   Jain S, 2013, ACM SIGCOMM COMP COM, V43, P3, DOI 10.1145/2534169.2486019
   Lee EK, 1996, ACM SIGPLAN NOTICES, V31, P84, DOI 10.1145/248209.237157
   Melnik S, 2011, COMMUN ACM, V54, P114, DOI 10.1145/1953122.1953148
   Pavlo A, 2009, ACM SIGMOD/PODS 2009 CONFERENCE, P165
   Ramakrishnan Raghu, 2017, P SIGMOD C
   Ramamurthy R, 2003, VLDB J, V12, P89, DOI 10.1007/s00778-003-0093-1
   Rashmi KV, 2014, SIGCOMM'14: PROCEEDINGS OF THE 2014 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P331, DOI [10.1145/2619239.2626325, 10.1145/2740070.2626325]
   REDELL DD, 1980, COMMUN ACM, V23, P81, DOI 10.1145/358818.358822
   Shanbhag A, 2017, PROCEEDINGS OF THE 2017 SYMPOSIUM ON CLOUD COMPUTING (SOCC '17), P229, DOI 10.1145/3127479.3131613
   Shvachko K., 2010, SYMPOSIUM, P1, DOI DOI 10.1109/MSST.2010.5496972
   Sivathanu M, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE 2ND USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST'03), P73
   Sun LW, 2014, PROC VLDB ENDOW, V7, P1617, DOI 10.14778/2733004.2733044
   Tai A, 2016, PROCEEDINGS OF USENIX ATC '16: 2016 USENIX ANNUAL TECHNICAL CONFERENCE, P337
   Thekkath C. A., 1997, FRANGIPANI SCALABLE, V31
   Thusoo A, 2009, PROC VLDB ENDOW, V2, P1626, DOI 10.14778/1687553.1687609
   Zaharia M, 2016, COMMUN ACM, V59, P56, DOI 10.1145/2934664
   Zhang HY, 2018, EUROSYS '18: PROCEEDINGS OF THE THIRTEENTH EUROSYS CONFERENCE, DOI 10.1145/3190508.3190534
   Zhou JR, 2012, VLDB J, V21, P611, DOI 10.1007/s00778-012-0280-z
   Zhou JR, 2010, PROC INT CONF DATA, P1060, DOI 10.1109/ICDE.2010.5447802
NR 35
TC 3
Z9 4
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD FEB
PY 2020
VL 15
IS 4
AR 23
DI 10.1145/3369738
PG 30
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NG7LP
UT WOS:000564162600002
DA 2024-07-18
ER

PT J
AU Pei, SY
   Yang, J
   Yang, Q
AF Pei, Shuyi
   Yang, Jing
   Yang, Qing
TI REGISTOR: A Platform for Unstructured Data Processing Inside SSD Storage
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article; Proceedings Paper
CT 11th ACM International Systems and Storage Conference (SYSTOR)
CY JUN, 2018
CL Haifa, ISRAEL
SP ACM
DE Regular expressions; processing in storage; near data processing; SSD
   storage; hardware accelerator
ID ARCHITECTURE
AB This article presents REGISTOR, a platform for regular expression grabbing inside storage. The main idea of Registor is accelerating regular expression (regex) search inside storage where large data set is stored, eliminating the I/O bottleneck problem. A special hardware engine for regex search is designed and augmented inside a flash SSD that processes data on-the-fly during data transmission from NAND flash to host. To make the speed of regex search match the internal bus speed of a modern SSD, a deep pipeline structure is designed in Registor hardware consisting of a file semantics extractor, matching candidates finder, regex matching units (REMUs), and results organizer. Furthermore, each stage of the pipeline makes the use of maximal parallelism possible. To make Registor readily usable by high-level applications, we have developed a set of APIs and libraries in Linux allowing Registor to process files in the SSD by recombining separate data blocks into files efficiently. A working prototype of Registor has been built in our newly designed NVMe-SSD. Extensive experiments and analyses have been carried out to show that Registor achieves high throughput, reduces the I/O bandwidth requirement by up to 97%, and reduces CPU utilization by as much as 82% for regex search in large datasets.
C1 [Pei, Shuyi; Yang, Jing; Yang, Qing] Univ Rhode Isl, 45 Upper Coll Rd, Kingston, RI 02881 USA.
   [Pei, Shuyi; Yang, Jing; Yang, Qing] Shenzhen Dapu Microelect Co Ltd, Longgang Ctr, Tianan Cyber Pk,Huangge Rd, Shenzhen 518100, Guangdong, Peoples R China.
C3 University of Rhode Island
RP Pei, SY (corresponding author), Univ Rhode Isl, 45 Upper Coll Rd, Kingston, RI 02881 USA.; Pei, SY (corresponding author), Shenzhen Dapu Microelect Co Ltd, Longgang Ctr, Tianan Cyber Pk,Huangge Rd, Shenzhen 518100, Guangdong, Peoples R China.
EM spei@ele.uri.edu; jyang@ele.uri.edu; qyang@ele.uri.edu
FU NSF [CCF-1439011, CCF-1421823]; URI; Shenzhen Dapu Microelectronics Co.,
   Ltd.
FX This research was supported in part by NSF grants CCF-1439011 and
   CCF-1421823. Any opinions, findings, and conclusions or recommendations
   expressed in this material are those of the author(s) and do not
   necessarily reflect the views of the NSF. This work was also partly
   supported by a research contract between URI and Shenzhen Dapu
   Microelectronics Co., Ltd.
CR Ahn J, 2015, 2015 ACM/IEEE 42ND ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA), P336, DOI 10.1145/2749469.2750385
   Akter S, 2016, ELECTRON MARK, V26, P173, DOI 10.1007/s12525-016-0219-0
   [Anonymous], 2017, SNORT NETW INTR DET
   [Anonymous], 2014, P 11 USENIX BROOM C
   Barbalace A, 2017, PROCEEDINGS OF THE 16TH WORKSHOP ON HOT TOPICS IN OPERATING SYSTEMS (HOTOS 2017), P56, DOI 10.1145/3102980.3102990
   Becchi M, 2008, I S WORKL CHAR PROC, P73
   Brodie BC, 2006, CONF PROC INT SYMP C, P191, DOI 10.1145/1150019.1136500
   Cameron RD, 2014, INT CONFER PARA, P139, DOI 10.1145/2628071.2628079
   Cox R., 2009, REGULAR EXPRESSION M
   Dlugosch Paul, 2014, PARALLEL DISTRIBUTED, V25, P12
   Fang YW, 2017, 50TH ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE (MICRO), P55, DOI 10.1145/3123939.3123983
   Fang YW, 2015, PROCEEDINGS OF THE 48TH ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE (MICRO-48), P533, DOI 10.1145/2830772.2830809
   Ficara Domenico, 2008, COMPUT COMMUN REV, V38, P5
   Gandomi A, 2015, INT J INFORM MANAGE, V35, P137, DOI 10.1016/j.ijinfomgt.2014.10.007
   Gao MY, 2015, INT CONFER PARA, P113, DOI 10.1109/PACT.2015.22
   Github, 2017, PERF COMP REG EXPR E
   Gogte V., 2016, 2016 49 ANN IEEEACM, P1
   Gu B, 2016, CONF PROC INT SYMP C, P153, DOI 10.1109/ISCA.2016.23
   Hazel Philip, 2019, PCRE PERL COMPATIBLE
   Hopcroft John E, 2001, Introduction to Automata Theory, Languages, and Computation, V32, P60, DOI DOI 10.1145/568438.568455
   Katal A, 2013, INT CONF CONTEMP, P404, DOI 10.1109/IC3.2013.6612229
   Knuth D. E., 1977, SIAM Journal on Computing, V6, P323, DOI 10.1137/0206024
   Kosako K., 2019, PCRE PERL COMPATIBLE
   Kumar Sailesh, 2006, COMPUT COMMUN REV, V36, P4
   Kumar S, 2014, INT CONFER PARA, P475, DOI 10.1145/2628071.2628118
   Levine John., 2009, FLEX BISON TEXT PROC, V1st
   Lin CH, 2013, IEEE T COMPUT, V62, P1906, DOI 10.1109/TC.2012.254
   Lin D., 2012, HIGH PERFORMANCE COM, P1
   Micron, 2018, MT29F2T08CUHBBM4 3R
   Micron, 2018, MT29F8G16ADADAH4 IT
   Microsoft, 2018, BEE3 EST FEBR 26 200
   Mytkowicz T, 2014, ACM SIGPLAN NOTICES, V49, P529, DOI 10.1145/2541940.2541988
   PCI-SIG, 2018, FREQ ASK QUEST PCI E
   Pei SY, 2018, SYSTOR'18: PROCEEDINGS OF THE 11TH ACM INTERNATIONAL SYSTEMS AND STORAGE CONFERENCE, P13, DOI 10.1145/3211890.3211900
   Project Gutenberg, 2018, ENT PROJ GUT WORKS M
   RegexLib, 2017, REG EXPR LIB
   Roy I, 2016, INT PARALL DISTRIB P, P1123, DOI 10.1109/IPDPS.2016.94
   Salapura V, 2012, PR IEEE COMP DESIGN, P1, DOI 10.1109/ICCD.2012.6378606
   Schadt EE, 2010, NAT REV GENET, V11, P647, DOI 10.1038/nrg2857
   Sidhu R., 2001, 9 ANN IEEE S FIELD P, P227, DOI DOI 10.1109/FCCM.2001.22
   Sidler D, 2017, SIGMOD'17: PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P403, DOI 10.1145/3035918.3035954
   Subramaniyan A, 2017, 44TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA 2017), P600, DOI [10.1145/3079856.3080207, 10.1145/3140659.3080207]
   Tan L, 2005, CONF PROC INT SYMP C, P112
   Tandon P, 2016, PROC INT CONF DATA, P469, DOI 10.1109/ICDE.2016.7498263
   THOMPSON K, 1968, COMMUN ACM, V11, P419, DOI 10.1145/363347.363387
   Titan IC, 2018, HYP F1 10G REG FIL S
   Tiwari Devesh., 2013, FAST, P119
   van Lunteren J, 2012, INT SYMP MICROARCH, P461, DOI 10.1109/MICRO.2012.49
   van Lunteren J, 2012, IEEE INFOCOM SER, P1737, DOI 10.1109/INFCOM.2012.6195546
   Wadden P, 2016, CAMBR MEDIEV CELT ST, P1
   Xi S.L., 2015, Proceedings of the 11th International Workshop on Data Management on New Hardware, P2
   Yang Y.-H.E., 2008, Proceedings ANCS, P30
   Yu X., 2013, Proceedings of the ACM International Conference on Computing Frontiers, P18, DOI DOI 10.1145/2482767.2482791
   2007, IEEE INTELLIGENT SYS, V22, P79
   2015, 2015 ACM 42 ANN INT, P1, DOI DOI 10.1145/2749469.2750412
   2016, CONF PROC INT SYMP C, P53, DOI DOI 10.1109/ISCA.2016.15
NR 56
TC 17
Z9 18
U1 0
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD APR
PY 2019
VL 15
IS 1
SI SI
AR 7
DI 10.1145/3310149
PG 24
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA HU9GE
UT WOS:000465601900007
OA Bronze
DA 2024-07-18
ER

PT J
AU Yan, WR
   Yao, J
   Cao, Q
   Xie, CS
   Jiang, H
AF Yan, Wenrui
   Yao, Jie
   Cao, Qiang
   Xie, Changsheng
   Jiang, Hong
TI ROS: A Rack-based Optical Storage System with Inline Accessibility for
   Long-Term Data Preservation
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Archive storage; optical disc; hierarchical storage; file system;
   storage management
AB The combination of the explosive growth in digital data and the demand to preserve much of these data in the long term has made it imperative to find a more cost-effective way than HDD arrays and a more easily accessible way than tape libraries to store massive amounts of data. While modern optical discs are capable of guaranteeing more than 50-year data preservation without media replacement, individual optical discs' lack of the performance and capacity relative to HDDs or tapes has significantly limited their use in datacenters. This article presents a Rack-scale Optical disc library System, or ROS in short, which provides a PB-level total capacity and inline accessibility on thousands of optical discs built within a 42U Rack. A rotatable roller and robotic arm separating and fetching discs are designed to improve disc placement density and simplify the mechanical structure. A hierarchical storage system based on SSDs, hard disks, and optical discs is proposed to effectively hide the delay of mechanical operation. However, an optical library file system (OLFS) based on FUSE is proposed to schedule mechanical operation and organize data on the tiered storage with a POSIX user interface to provide an illusion of inline data accessibility. We further optimize OLFS by reducing unnecessary user/kernel context switches inheriting from legacy FUSE framework. We evaluate ROS on a few key performance metrics, including operation delays of the mechanical structure and software overhead in a prototype PB-level ROS system. The results show that ROS stacked on Samba and FUSE as network-attached storage (NAS) mode almost saturates the throughput provided by underlying samba via 10GbE network for external users, as well as in this scenario provides about 53ms file write and 15ms read latency, exhibiting its inline accessibility. Besides, ROS is able to effectively hide and virtualize internal complex operational behaviors and be easily deployable in datacenters.
C1 [Yan, Wenrui; Yao, Jie; Cao, Qiang; Xie, Changsheng] Minist Educ China, Key Lab Informat Storage Syst, Wuhan Natl Lab Optoelect, 1037 Luoyu Rd, Wuhan 430074, Hubei, Peoples R China.
   [Yan, Wenrui; Yao, Jie; Cao, Qiang; Xie, Changsheng] Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, 1037 Luoyu Rd, Wuhan 430074, Hubei, Peoples R China.
   [Jiang, Hong] Univ Texas Arlington, Dept CSE, Arlington, TX 76019 USA.
C3 Huazhong University of Science & Technology; Huazhong University of
   Science & Technology; University of Texas System; University of Texas
   Arlington
RP Yao, J; Cao, Q (corresponding author), Minist Educ China, Key Lab Informat Storage Syst, Wuhan Natl Lab Optoelect, 1037 Luoyu Rd, Wuhan 430074, Hubei, Peoples R China.; Yao, J; Cao, Q (corresponding author), Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, 1037 Luoyu Rd, Wuhan 430074, Hubei, Peoples R China.
EM wenrui_yan@hust.edu.cn; jackyao@hust.edu.cn; caoqiang@hust.edu.cn;
   cs_xie@hust.edu.cn; hong.jiang@uta.edu
FU Nature Science Foundation of China [61872156, 61821003]; Fundamental
   Research Funds for the Central Universities [2018KFYXKJC037]; US NSF
   [CCF-1704504, CCF-1629625]; Alibaba Group through Alibaba Innovative
   Research (AIR) Program
FX This work is supported in part by Nature Science Foundation of China
   under Grant No. 61872156 and No. 61821003, the Fundamental Research
   Funds for the Central Universities No. 2018KFYXKJC037, the US NSF under
   Grant No. CCF-1704504 and No. CCF-1629625, and Alibaba Group through
   Alibaba Innovative Research (AIR) Program.
CR Academy of Motion Picture Arts Sciences and Technology Council, 2007, TECHNICAL REPORT
   Balakrishnan S., 2014, PROC ITH USENIX CONJ, P351
   Beaver D., 2010, P USENIX OSDI
   Bent John., 2009, P C HIGH PERFORMANCE, p21:1
   Biskeborn B., 2010, MASS STOR SYST TECHN, P1, DOI [10.1109/MSST.2010.5496989, DOI 10.1109/MSST.2010.5496989]
   Boyd S, 2011, IEEE T SEMICONDUCT M, V24, P117, DOI 10.1109/TSM.2010.2087395
   Chang V, 2015, AD HOC NETW, V35, P65, DOI 10.1016/j.adhoc.2015.07.012
   Cornell Brian., 2004, P ANN C USENIX ANN T, P27
   Crockford Douglas, 2016, JAVASCRIPT OBJ NOT
   Deepika G., 2011, Proceedings of the 2011 National Conference on Innovations in Emerging Technology (NCOIET), P145, DOI 10.1109/NCOIET.2011.5738819
   Fujiwara Hiroshi, 2016, WHAT IS IMPORTANCE D
   Ganger G. R., 1997, Proceedings of the USENIX 1997 Annual Technical Conference, P1
   Godard B, 2003, EUR J HUM GENET, V11, pS88, DOI 10.1038/sj.ejhg.5201114
   Google, 2018, ARCH CLOUD STOR NEAR
   Grawinkel M., 2015, 13 USENIX C FILE STO, P15
   Gu M, 2014, LIGHT-SCI APPL, V3, DOI 10.1038/lsa.2014.58
   Gu M, 2010, OPT PHOTONICS NEWS, V21, P28, DOI 10.1364/OPN.21.7.000028
   Gupta P., 2016, 2016 NAT POW SYST C, P1, DOI DOI 10.1109/MSST.2016.7897083
   Katayama K, 2015, J APPL PHYS, V117, DOI 10.1063/1.4907769
   Kumar S, 2003, TECHNOVATION, V23, P749, DOI 10.1016/S0166-4972(02)00040-8
   Legtchenko S, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P213
   Miller R., 2014, INSIDE FACEBOOKS BLU
   Minemura H, 2006, JPN J APPL PHYS 1, V45, P1213, DOI 10.1143/JJAP.45.1213
   Muralidhar S., 2014, P 11 USENIX C OP SYS, P383
   NASA, 2017, LOU MASS STOR
   Nikoobakht B, 2003, CHEM MATER, V15, P1957, DOI 10.1021/cm0207321
   Optical Storage Technology Association, 2003, UN DISK FORM SPEC
   Panasonic Corp, 2016, DAT ARCH LB DH8 SER
   Patiejunas Kestutis, 2014, TECHNICAL REPORT
   Perlmutter M., 2017, LOST PICTURE SHOW HO
   Rajgarhia A., 2010, P 2010 ACM S APPL CO, P206, DOI DOI 10.1145/1774088.1774130
   Rosenthal A, 2010, J BIOMED INFORM, V43, P342, DOI 10.1016/j.jbi.2009.08.014
   Schroeder B, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P67
   Svrcek I., 2009, Accelerated Life Cycle Comparison of Millenniata Archival DVD
   Tarasov V., 2018, FILEBENCH
   Thompson C, 2014, INT CONF SYST SIGNAL, P11
   Ungureanu C., 2010, FAST, P225
   Wamsteker W, 2000, ASTROPHYS SPACE SCI, V273, P155, DOI 10.1023/A:1002780814330
   Watanabe Akinobu, 2013, TECHNICAL REPORT
   Weil SA, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P307
   Yamamoto N, 2004, FIFTH IEEE/ACM INTERNATIONAL WORKSHOP ON GRID COMPUTING, PROCEEDINGS, P461, DOI 10.1109/GRID.2004.47
   Zhang SL, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P15
   Zijlstra P, 2009, NATURE, V459, P410, DOI 10.1038/nature08053
NR 43
TC 3
Z9 3
U1 0
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2018
VL 14
IS 3
AR 28
DI 10.1145/3231599
PG 26
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ4JQ
UT WOS:000457140700009
DA 2024-07-18
ER

PT J
AU Miao, YS
   Han, WT
   Li, KW
   Wu, M
   Yang, F
   Zhou, LD
   Prabhakaran, V
   Chen, EH
   Chen, WG
AF Miao, Youshan
   Han, Wentao
   Li, Kaiwei
   Wu, Ming
   Yang, Fan
   Zhou, Lidong
   Prabhakaran, Vijayan
   Chen, Enhong
   Chen, Wenguang
TI ImmortalGraph: A System for Storage and Analysis of Temporal Graphs
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Algorithms; Concurrent computing; temporal graph; graph
   algorithms
ID FRAMEWORK
AB Temporal graphs that capture graph changes over time are attracting increasing interest from research communities, for functions such as understanding temporal characteristics of social interactions on a time-evolving social graph. ImmortalGraph is a storage and execution engine designed and optimized specifically for temporal graphs. Locality is at the center of ImmortalGraph's design: temporal graphs are carefully laid out in both persistent storage and memory, taking into account data locality in both time and graph-structure dimensions. ImmortalGraph introduces the notion of locality-aware batch scheduling in computation, so that common "bulk" operations on temporal graphs are scheduled to maximize the benefit of in-memory data locality. The design of ImmortalGraph explores an interesting interplay among locality, parallelism, and incremental computation in supporting common mining tasks on temporal graphs. The result is a high-performance temporal-graph system that is up to 5 times more efficient than existing database solutions for graph queries. The locality optimizations in ImmortalGraph offer up to an order of magnitude speedup for temporal iterative graph mining compared to a straightforward application of existing graph engines on a series of snapshots.
C1 [Miao, Youshan; Han, Wentao; Li, Kaiwei; Wu, Ming; Yang, Fan; Zhou, Lidong; Prabhakaran, Vijayan; Chen, Enhong; Chen, Wenguang] Univ Sci & Technol China, Hefei 230026, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Miao, YS (corresponding author), Univ Sci & Technol China, Hefei 230026, Peoples R China.
EM mooncake@mail.ustc.edu.cn; hwt04@mails.tsinghua.edu.cn;
   likw14@mails.tsinghua.edu.cn; miw@microsoft.com; fanyang@microsoft.com;
   lidongz@microsoft.com; vijayanp@microsoft.com; cheneh@ustc.edu.cn;
   cwg@tsinghua.edu.cn
RI Li, Kai-Wei/JCF-0055-2023
FU National High-Tech Research and Development Plan (863 Project)
   [2012AA010903]; National Science Foundation for Distinguished Young
   Scholars of China [61325010]
FX We sincerely thank the anonymous reviewers and Transactions on Storage
   editors for their valuable comments and suggestions. We also want to
   thank Thomas Moscibroda, who provided invaluable suggestions about
   applications that helped start this project, as well as Sean McDirmid
   and Nathan Holdstein for their suggestions about editing. This work has
   been partially supported by the National High-Tech Research and
   Development Plan (863 Project) 2012AA010903, as well as the National
   Science Foundation for Distinguished Young Scholars of China (Grant No.
   61325010).
CR Aggarwal Charu C., 2010, ADV DATABASE SYSTEMS, V40
   [Anonymous], 1995, Technical Report
   [Anonymous], 2010, Proceedings of the Eighth Workshop on Mining and Learning with Graphs, DOI 10.1145/1830252.1830262
   [Anonymous], 2010, P 2010 IEEE GLOB TEL
   [Anonymous], 2010, P 9 USENIX C OP SYST
   Boldi Paolo, 2008, SIGIR Forum, V42, P33, DOI 10.1145/1480506.1480511
   Brin S, 1998, COMPUT NETWORKS ISDN, V30, P107, DOI 10.1016/S0169-7552(98)00110-X
   Chen Rong, 2013, IPADSTR2013001 SHANG
   Cheng Raymond, 2012, P 7 ACM EUR C COMP S, P85, DOI 10.1145/2168836.2168846
   Frigo M., 1999, 40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039), P285, DOI 10.1109/SFFCS.1999.814600
   Gonzalez Joseph E., 2012, P 10 USENIX S OP SYS, P17
   Khayyat Z., 2013, P 8 ACM EUR C COMP S, P169, DOI DOI 10.1145/2465351.2465369
   Khurana U, 2013, PROC INT CONF DATA, P997, DOI 10.1109/ICDE.2013.6544892
   Kyrola A., 2012, 10 USENIX S OPERATIN, P31
   Leskovec J., 2005, P 11 ACM SIGKDD INT, P177
   LOMET D, 1989, SIGMOD REC, V18, P315, DOI 10.1145/66926.66956
   Lomet David., 2005, SIGMOD 05 P 2005 ACM, P939
   Low Y, 2012, PROC VLDB ENDOW, V5, P716, DOI 10.14778/2212351.2212354
   Malewicz Grzegorz, 2010, P ACM SIGMOD INT C M, P135, DOI [DOI 10.1145/1807167.1807184, 10.1145/1807167.1807184]
   Mislove A. E., 2009, THESIS RICE U HOUSTO
   Muller K., 1991, Operating Systems Review, V25, P56, DOI 10.1145/121133.286600
   Murray DG, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P439, DOI 10.1145/2517349.2522738
   Neo4j, 2013, NEO4J GRAPH DAT
   Nguyen D, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P456, DOI 10.1145/2517349.2522739
   Prabhakaran V., 2012, P 2012 USENIX ANN TE, V12
   Ren CH, 2011, PROC VLDB ENDOW, V4, P726
   Roditty L, 2004, LECT NOTES COMPUT SC, V3221, P580
   Romero Daniel M, 2011, P 20 INT C WORLD WID, P695, DOI [10.1145/1963405.1963503, DOI 10.1145/1963405.1963503]
   Roy A, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P472, DOI 10.1145/2517349.2522740
   Salzberg B, 1999, ACM COMPUT SURV, V31, P158, DOI 10.1145/319806.319816
   Shun JL, 2013, ACM SIGPLAN NOTICES, V48, P135, DOI 10.1145/2517327.2442530
   Thereska Eno, 2012, P 4 USENIX C HOT TOP
   Venkataraman S., 2013, Proceedings of the 8th ACM European Conference on Computer Systems, P197, DOI DOI 10.1145/2465351.2465371
   Wilson C, 2009, EUROSYS'09: PROCEEDINGS OF THE FOURTH EUROSYS CONFERENCE, P205
   Yang Lei., 2007, CIKM, P1011
   Zaharia Matei., 2012, NSDI 12
NR 36
TC 38
Z9 40
U1 0
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUL
PY 2015
VL 11
IS 3
AR 14
DI 10.1145/2700302
PG 34
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CN9RP
UT WOS:000358786900004
DA 2024-07-18
ER

PT J
AU Huang, JZ
   Zhang, FH
   Qin, X
   Xie, CS
AF Huang, Jianzhong
   Zhang, Fenghao
   Qin, Xiao
   Xie, Changsheng
TI Exploiting Redundancies and Deferred Writes to Conserve Energy in
   Erasure-Coded Storage Clusters
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Performance; Clustered storage system; power efficiency; erasure
   codes; selective activation policy
ID RELIABILITY
AB We present a power-efficient scheme for erasure-coded storage clusters-ECS2-which aims to offer high energy efficiency with marginal reliability degradation. ECS2 utilizes data redundancies and deferred writes to conserve energy. In ECS2 parity blocks are buffered exclusively in active data nodes whereas parity nodes are placed into low-power mode. (k + r, k) RS-coded ECS2 can achieve inverted right perpendicular(r + 1)/2inverted left perpendicular-fault tolerance for k active data nodes and r-fault tolerance for all k + r nodes. ECS2 employs the following three optimizing approaches to improve the energy efficiency of storage clusters. (1) An adaptive threshold policy takes system configurations and I/O workloads into account to maximize standby time periods; (2) a selective activation policy minimizes the number of power-transitions in storage nodes; and (3) a region-based buffer policy speeds up the synchronization process by migrating parity blocks in a batch method. After implementing an ECS2-based prototype in a Linux cluster, we evaluated its energy efficiency and performance using four different types of I/O workloads. The experimental results indicate that compared to energy-oblivious erasure-coded storage, ECS2 can save the energy used by storage clusters up to 29.8% and 28.0% in read-intensive and write-dominated workloads when k = 6 and r = 3, respectively. The results also show that ECS2 accomplishes high power efficiency in both normal and failed cases without noticeably affecting the I/O performance of storage clusters.
C1 [Huang, Jianzhong; Zhang, Fenghao] Huazhong Univ Sci & Technol, Wuhan Natl Lab OptoElect, Wuhan, Peoples R China.
   [Qin, Xiao] Auburn Univ, Samuel Ginn Coll Engn, Shelby Ctr Engn Technol, Dept Comp Sci & Software Engn, Auburn, AL 36849 USA.
   [Xie, Changsheng] Huazhong Univ Sci & Technol, Dept Comp, Wuhan, Hubei, Peoples R China.
C3 Huazhong University of Science & Technology; Auburn University System;
   Auburn University; Huazhong University of Science & Technology
RP Xie, CS (corresponding author), Huazhong Univ Sci & Technol, Dept Comp, Wuhan, Hubei, Peoples R China.
EM cs_xie@hust.edu.cn
RI huang, jianzhong/HOC-1522-2023; Huang, Jianzhong/KEI-1516-2024
FU National Basic Research Program of China [2011CB302303]; NSF of China
   [60933002]; National High Technology Research and Development Program of
   China [2013AA013203]; Fundamental Research Funds for the Central
   Universities [2012QN100]; U.S. National Science Foundation [CCF-0845257,
   CNS-0917137, CCF-0742187]; Direct For Computer & Info Scie & Enginr;
   Division of Computing and Communication Foundations [0845257] Funding
   Source: National Science Foundation
FX This work is supported in part by the National Basic Research Program of
   China under Grant No. 2011CB302303, the NSF of China under Grant No.
   60933002, the National High Technology Research and Development Program
   of China under Grant No. 2013AA013203, and Fundamental Research Funds
   for the Central Universities (No. 2012QN100). X. Qin's work was
   supported by the U.S. National Science Foundation under Grants
   CCF-0845257(CAREER), CNS-0917137(CSR), and CCF-0742187(CPA).
CR Aguilera MK, 2005, I C DEPEND SYS NETWO, P336, DOI 10.1109/DSN.2005.96
   AMUR H., 2010, P 1 ACM S CLOUD COMP, P217
   [Anonymous], 2007, I O SEARCH ENG I O U
   [Anonymous], 2010, ACM SIGMOD RECORD
   Bairavasundaram LN, 2007, PERF E R SI, V35, P289
   BLAUM M, 1995, IEEE T COMPUT, V44, P192, DOI 10.1109/12.364531
   Borthakur D., 2008, HADOOP APACHE PROJECT
   Borthakur D, 2010, HDFS ERASURE CODES H
   Chen F, 2006, ISLPED '06: Proceedings of the 2006 International Symposium on Low Power Electronics and Design, P412, DOI 10.1109/LPE.2006.4271878
   Colarelli Dennis., 2002, SUPERCOMPUTING 02, P1, DOI DOI 10.1109/SC.2002.10058
   Corbett P, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P1
   Fan Bin., 2009, P 4 ANN WORKSHOP PET, P6
   Ganesh L, 2007, P 11 USENIX WORKSH H
   Ghemawat S., 2003, ACM SIGOPS OPERATING, V37, P29, DOI [DOI 10.1145/945445.945450, DOI 10.1145/1165389.945450, 10.1145/1165389.945450]
   Greenan K.M., 2008, Proceedings of the Fourth conference on Hot topics in system dependability, P4
   Hafner J., 2010, US Patent App, Patent No. [12/710,123, 12710123]
   Hafner J. L., 2006, RJ10391 IBM
   Hafner JL, 2005, USENIX ASSOCIATION PROCEEDINGS OF THE 4TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P211
   HOLLAND M, 1994, DISTRIB PARALLEL DAT, V2, P295, DOI 10.1007/BF01266332
   Kavalanekar Swaroop, 2008, 2008 IEEE International Symposium on Workload Characterization (IISWC), P119, DOI 10.1109/IISWC.2008.4636097
   Lang W, 2010, PROC VLDB ENDOW, V3, P129, DOI 10.14778/1920841.1920862
   Leverich J., 2010, ACM SIGOPS OPERATING, V44, P61
   Li D, 2004, P 11 WORKSH ACM SIGO
   Lumb C, 2000, P 4 USENIX C S OP SY
   Meisner D, 2009, ACM SIGPLAN NOTICES, V44, P205, DOI 10.1145/1508284.1508269
   Moore RichardL., 2007, Disk and tape storage cost models. Archiving 2007
   Narayanan D., 2008, ACM T STORAGE, V4, P3
   Ongaro D, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P29
   Phanishayee A, 2008, P 6 USENIX C FIL STO
   Pinheiro E., 2006, Performance Evaluation Review, V34, P15, DOI 10.1145/1140103.1140281
   Pinheiro E., 2004, INT C SUPERCOMPUTING, P68, DOI DOI 10.1145/1006209.1006220
   Plank J, 2005, P 4 USENIX C FIL STO
   Plank J. S., 2009, FAST 2009, P253
   Plank JS, 1997, SOFTWARE PRACT EXPER, V27, P995, DOI 10.1002/(SICI)1097-024X(199709)27:9<995::AID-SPE111>3.0.CO;2-6
   Rao KK, 2011, IEEE T DEPEND SECURE, V8, P404, DOI 10.1109/TDSC.2010.21
   REED IS, 1960, J SOC IND APPL MATH, V8, P300, DOI 10.1137/0108018
   Samsung, 2008, DAT SHEET SAMS DDR2
   Seagate, 2011, DAT SHEET SEAG DISK
   Storer M, 2008, P 6 USENIX C FIL STO
   Tsirogiannis D., 2010, SIGMOD, P231, DOI DOI 10.1145/1807167.1807194
   Wang J, 2008, IEEE T COMPUT, V57, P359, DOI 10.1109/TC.2007.70821
   Weatherspoon H, 2002, LECT NOTES COMPUT SC, V2429, P328
   Weihang Jiang, 2008, ACM Transaction on Storage, V4, DOI 10.1145/1416944.1416946
   Wilcox-O'Hearn Z, 2011, ZFEC 1 4 2 OPEN SOUR
   Xiaoyu Yao, 2006, Operating Systems Review, V40, P249, DOI 10.1145/1218063.1217959
   Xin Q, 2003, IEEE S MASS STOR SYS, P146, DOI 10.1109/MASS.2003.1194851
   Zhang Z, 2010, ACM SIGOPS OPER SYST
   Zhu Q, 2005, P 20 ACM S OP SYST P, P177, DOI DOI 10.1145/1095809.1095828
   Zhu QB, 2004, 10TH INTERNATIONAL SYMPOSIUM ON HIGH PERFORMANCE COMPUTER ARCHITECTURE, PROCEEDINGS, P118
   Zyhd, 2010, ZH 101 PORT EL POW F
   Zyhd, 2010, P 20 ACM S OP SYST P
NR 51
TC 13
Z9 16
U1 0
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUL
PY 2013
VL 9
IS 2
AR 4
DI 10.1145/2491472.2491473
PG 29
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 192IZ
UT WOS:000322479000001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Shilane, P
   Huang, M
   Wallace, G
   Hsu, W
AF Shilane, Phlip
   Huang, Mark
   Wallace, Grant
   Hsu, Windsor
TI WAN-Optimized Replication of Backup Datasets Using Stream-Informed Delta
   Compression
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Performance; Reliability; Backup storage; deduplication; delta
   compression; network replication
AB Replicating data off site is critical for disaster recovery reasons, but the current approach of transferring tapes is cumbersome and error prone. Replicating across a wide area network (WAN) is a promising alternative, but fast network connections are expensive or impractical in many remote locations, so improved compression is needed to make WAN replication truly practical. We present a new technique for replicating backup datasets across a WAN that not only eliminates duplicate regions of files (deduplication) but also compresses similar regions of files with delta compression, which is available as a feature of EMC Data Domain systems.
   Our main contribution is an architecture that adds stream-informed delta compression to already existing deduplication systems and eliminates the need for new, persistent indexes. Unlike techniques based on knowing a file's version or that use a memory cache, our approach achieves delta compression across all data replicated to a server at any time in the past. From a detailed analysis of datasets and statistics from hundreds of customers using our product, we achieve an additional 2X compression from delta compression beyond deduplication and local compression, which enables customers to replicate data that would otherwise fail to complete within their backup window.
EM philip.shilane@emc.com; mark.huang@emc.com; grant.wallace@emc.com;
   windsor.hsu@emc.com
RI Shilane, Philip/V-9134-2019
OI Shilane, Philip/0000-0003-1235-0502
CR [Anonymous], P USENIX ANN TECHN C
   [Anonymous], 2010, P USENIX C USENIX AN
   [Anonymous], 2011, PROC USENIX C FILE S
   [Anonymous], 2009, 7 USENIX C FIL STOR
   [Anonymous], 1981, FINGERPRINTING RANDO
   ARONOVICH L., 2009, P ISR EXP SYST C SYS, P6
   Bhagwat D., 2009, P 17 IEEE INT S MOD
   Bobbarjung D. R., 2006, ACM Transaction on Storage, V2, P424, DOI 10.1145/1210596.1210599
   Brin S., 1995, SIGMOD Record, V24, P398, DOI 10.1145/568271.223855
   Broder A. Z., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P327, DOI 10.1145/276698.276781
   Broder AZ, 1998, COMPRESSION AND COMPLEXITY OF SEQUENCES 1997 - PROCEEDINGS, P21, DOI 10.1109/SEQUEN.1997.666900
   Broder AZ, 2000, LECT NOTES COMPUT SC, V1848, P1
   Burns RandalC., 1997, Proceedings of the fifth workshop on I/O in parallel and distributed systems, P27
   Chan MC, 1999, IEEE INFOCOM SER, P117, DOI 10.1109/INFCOM.1999.749259
   Chen Y, 2004, ITCC 2004: INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY: CODING AND COMPUTING, VOL 1, PROCEEDINGS, P778
   Douglis F, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE GENERAL TRACK, P113
   EMC CORPORATION, 2010, DAT DOM BOOST SOFTW
   Eshghi K., 2007, P 5 USENIX C FIL STO
   GAILLY J. L., 2003, GZIP COMPRESSOR
   Guo F., 2011, P USENIX ANN TECHN C
   Hunt J. J., 1998, ACM Transactions on Software Engineering and Methodology, V7, P192, DOI 10.1145/279310.279321
   JAIN N, 2005, P 4 USENIX C FIL STO
   Kulkarni P, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE GENERAL TRACK 2004 USENIX ANNUAL TECHNICAL CONFERENCE, P59
   MacDonald Josh, 2000, Ph.D. thesis
   MANBER U, 1994, PROCEEDINGS OF THE WINTER 1994 USENIX CONFERENCE, P1
   Min J, 2011, IEEE T COMPUT, V60, P824, DOI 10.1109/TC.2010.263
   Mogul J., 1997, PROC ACM SIGCOMM C A, V27, P181, DOI DOI 10.1145/263109.263162
   Muthitacharoen A., 2001, Operating Systems Review, V35, P174, DOI 10.1145/502059.502052
   PARK K., 2007, P USENIX ANN TECHN C, P14
   PARK N., 2010, P IEEE INT S WORKL C
   Patterson Hugo, 2002, P 1 USENIX C FIL STO
   Policroniades C, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE GENERAL TRACK 2004 USENIX ANNUAL TECHNICAL CONFERENCE, P73
   Quinlan S., 2002, P 1 USENIX C FIL STO
   RIVERBED TECHNOLOGY, 2011, RIV STEELH PROD FAM
   SHILANE P., 2012, P 4 USENIX C HOT TOP
   Shilane P., 2012, P 10 USENIX C FIL ST, P1
   Spring NT, 2000, ACM SIGCOMM COMP COM, V30, P87, DOI 10.1145/347057.347408
   Suel T., 2004, P 20 INT C DAT ENG
   Suel Torsten, 2002, Lossless Compression Handbook
   Trendafilov Dimitre., 2002, zdelta: An efficient delta compression tool
   Tridgell A., 2000, THESIS AUSTR NATL U
   Wallace Grant., 2012, PROC USENIX FAST
   You L., 2004, P MSST, P227
   You LL, 2011, ACM T STORAGE, V7, DOI 10.1145/1970348.1970351
   Zhu B, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P269
NR 45
TC 40
Z9 49
U1 0
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2012
VL 8
IS 4
AR 13
DI 10.1145/2385603.2385606
PG 26
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 052QF
UT WOS:000312212500002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Chang, YH
   Hsu, PY
   Lu, YF
   Kuo, TW
AF Chang, Yuan-Hao
   Hsu, Ping-Yi
   Lu, Yung-Feng
   Kuo, Tei-Wei
TI A Driver-Layer Caching Policy for Removable Storage Devices
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Algorithms; Design; Experimentation; Management; Measurement;
   Performance; Flash memory; caching; driver; file system; removable
   storage; USB; Windows; merging; padding
AB The growing popularity of flash memory is expected to draw attention to the limitations of file-system performance over flash memory. This work was motivated by the modular designs of operating system components such as bus and device drivers. A filter-driver-layered caching design is proposed to resolve the performance gap among file systems and to improve their performance with the considerations of flash memory characteristics. An efficient hybrid tree structure is presented to organize and manipulate the intervals of cached writes. Algorithms are proposed in the merging, padding, and removing of the data of writes. The effectiveness of the proposed approach is demonstrated with some analysis study of FAT-formatted and NTFS-formatted USB flash disks. The proposed cohesive caching policy was implemented as a filter driver in Windows XP/Vista for performance evaluation. In the experiments, a ten-fold or larger performance improvement was usually achieved when the cache size was only 64KB. Other substantial improvements were also observed in the experiments. For example, the proposed design enabled FAT-formatted and NTFS-formatted flash-memory devices to copy Linux image files 93% and 14% faster than conventional flash drives, respectively.
C1 [Chang, Yuan-Hao] Natl Taipei Univ Technol, Dept Elect Engn, Taipei 106, Taiwan.
   [Hsu, Ping-Yi; Kuo, Tei-Wei] Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 106, Taiwan.
   [Lu, Yung-Feng] Natl Taichung Inst Technol, Dept Comp Sci & Informat Engn, Taichung, Taiwan.
C3 National Taipei University of Technology; National Taiwan University
RP Chang, YH (corresponding author), Natl Taipei Univ Technol, Dept Elect Engn, Taipei 106, Taiwan.
EM johnsonchang@ntut.edu.tw; r94922007@csic.ntu.edu.tw; yflu@ntit.edu.tw;
   ktw@csic.ntu.edu.tw
RI Chang, Yuan-Hao/ABA-6935-2020
OI Chang, Yuan-Hao/0000-0002-1282-2111; KUO, TEI-WEI/0000-0003-1974-0394
FU NSC [99-2218-E-027-005, 98-2221-E-002-120MY3]; Excellent Research
   Projects of National Taiwan University [98R80304]; Ministry of Economic
   Affairs in Taiwan [98-EC-17-A-01-S1-034]
FX This work was supported in part by the NSC under grant Nos.
   99-2218-E-027-005 and 98-2221-E-002-120MY3, by the Excellent Research
   Projects of National Taiwan University under grant No. 98R80304, and by
   the Ministry of Economic Affairs under grant No. 98-EC-17-A-01-S1-034 in
   Taiwan.
CR [Anonymous], 2005, File System Forensic Analysis
   BAKER M., 1992, P INT C ARCH SUPP PR
   BENNETT A. D., 2006, United States Patent, Patent No. 7139864
   Bovet D. P., 2005, Understanding the Linux Kernel: from I/O ports to process management
   BRIN S., 1995, P ACM SIGMOD
   Chang LP, 2002, EIGHTH IEEE REAL-TIME AND EMBEDDED TECHNOLOGY AND APPLICATIONS SYMPOSIUM, PROCEEDINGS, P187, DOI 10.1109/RTTAS.2002.1137393
   CHOI J., 2007, P INT C EMB SOFTW EM
   Cormen Thomas H., 2001, INTRO ALGORITHMS
   DIPERT B., 2004, EDN MAG
   DOH I. H., 2009, P ACM S APPL COMP SA
   GILL B. S., 2005, P USENIX ANN TECHN C
   HARARI E., 2005, United States Patent, Patent No. 6914846
   HITZ D., 2005, FILE SYSTEM DESIGN N
   JAIN N., 2005, P USENIX C FIL STOR
   Jo H, 2006, IEEE T CONSUM ELECTR, V52, P485, DOI 10.1109/TCE.2006.1649669
   Kang SY, 2009, IEEE T COMPUT, V58, P744, DOI 10.1109/TC.2008.224
   Kim H, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P239, DOI 10.1145/1378600.1378627
   KULKARNI P., 2004, P USENIX ANN TECHN C
   LI K., 2008, P USENIX C FIL STOR
   MANBER U., 1994, P USENIX WINT TECHN
   Megiddo N, 2004, COMPUTER, V37, P58, DOI 10.1109/MC.2004.1297303
   MICROSOFT, 2006, WIND DRIV KIT DOC
   MICROSOFT, 2006, WIND READYDRIVE HYBR
   Microsoft, 2003, NTFS WORKS
   Muthitacharoen A., 2001, Operating Systems Review, V35, P174, DOI 10.1145/502059.502052
   NIGHTINGALE T., 1999, P USENIX C
   ONEY W., 2003, PROGRAMMING MICROSOF, P773
   Park S.-y., 2006, P 2006 INT C COMP AR, P234, DOI DOI 10.1145/1176760.1176789
   Quinlan S, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE FAST'02 CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P89
   ROSICH M. N., 1996, United States Patent, Patent No. 5551002
   TERRELL J. R., 1995, United States Patent, Patent No. 6026027
   WU M, 1994, SIGPLAN NOTICES, V29, P86, DOI 10.1145/195470.195506
   YANG Q., 1996, P INT S COMP ARCH IS
NR 33
TC 3
Z9 3
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUN
PY 2011
VL 7
IS 1
AR 1
DI 10.1145/1970343.1970344
PG 23
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 990LE
UT WOS:000307631600001
DA 2024-07-18
ER

PT J
AU Wu, HN
   Wang, SX
   Jin, ZF
   Zhang, YH
   Ma, RY
   Fan, SJ
   Chao, RL
AF Wu, Haonan
   Wang, Shuxian
   Jin, Zhanfeng
   Zhang, Yuhang
   Ma, Ruyun
   Fan, Sijin
   Chao, Ruili
TI CostCounter: A Better Method for Collision Mitigation in Cuckoo Hashing
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Cuckoo Hashing; throughput optimization; FPGA search and update
ID LOOKUP
AB Hardware is often required to support fast search and high-throughput applications. Consequently, the performance of search algorithms is limited by storage bandwidth. Hence, the search algorithm must be optimized accordingly. We propose a CostCounter (CC) algorithm based on cuckoo hashing and an Improved CostCounter (ICC) algorithm. A better path can be selected when collisions occur using a cost counter to record the kick-out situation. Our simulation results indicate that the CC and ICC algorithms can achieve more significant performance improvements than Random Walk (RW), Breadth First Search (BFS), and MinCounter (MC). With two buckets and two slots per bucket, under the 95% memory load rate of the maximum load rate, CC and ICC are optimized on read-write times over 20% and 80% compared to MC and BFS, respectively. Furthermore, the CC and ICC algorithms achieve a slight improvement in storage efficiency compared with MC. In addition, we implement RW, MC, and the proposed algorithms using fine-grained locking to support a high throughput rate. From the test on field programmable gate arrays, we verify the simulation results and our algorithms optimize the maximum throughput over 23% compared to RW and 9% compared to MC under 95% of the memory capacity. The test results indicate that our CC and ICC algorithms can achieve better performance in terms of hardware bandwidth and memory load efficiency without incurring a significant resource cost.
C1 [Wu, Haonan; Wang, Shuxian; Jin, Zhanfeng; Zhang, Yuhang; Ma, Ruyun; Fan, Sijin; Chao, Ruili] East China Normal Univ, Sch Commun & Elect Engn, 500 Dongchuan Rd, Shanghai 200241, Peoples R China.
C3 East China Normal University
RP Wu, HN (corresponding author), East China Normal Univ, Sch Commun & Elect Engn, 500 Dongchuan Rd, Shanghai 200241, Peoples R China.
EM 51205904021@stu.ecnu.edu.cn; sxwang@ee.ecnu.edu.cn;
   51205904020@stu.ecnu.edu.cn; 603252590@qq.com;
   51215904020@stu.ecnu.edu.cn; 1392579147@qq.com; 2470438078@qq.com
OI Wu, Haonan/0000-0002-7701-6129; Ma, Ruyun/0000-0001-9007-6196
CR Akamai, 2020, 2020 STAT INT SEC 20
   [Anonymous], 2020, Cisco Annual Internet Report (2018-2023) White Paper
   [Anonymous], 1998, The art of computer programming: Sorting and searching
   Bando M, 2012, IEEE ACM T NETWORK, V20, P1262, DOI 10.1109/TNET.2012.2188643
   Breslow AD, 2016, PROCEEDINGS OF USENIX ATC '16: 2016 USENIX ANNUAL TECHNICAL CONFERENCE, P281
   Chen H, 2011, IEEE COMMUN SURV TUT, V13, P541, DOI 10.1109/SURV.2011.072210.00075
   Dobai R, 2017, APPL SOFT COMPUT, V56, P173, DOI 10.1016/j.asoc.2017.03.009
   Erlingsson U., 2006, P 7 WORKSH DISTR DAT, P1
   Fan B, 2014, PROCEEDINGS OF THE 2014 CONFERENCE ON EMERGING NETWORKING EXPERIMENTS AND TECHNOLOGIES (CONEXT'14), P75, DOI 10.1145/2674005.2674994
   Fan Bin, 2013, 10 USENIX S NETW SYS, P371
   Fotakis DT, 2005, THEOR COMPUT SYST, V38, P229, DOI 10.1007/s00224-004-1195-x
   Frieze A, 2011, SIAM J COMPUT, V40, P291, DOI 10.1137/090770928
   GONNET GH, 1991, HDB ALGORITHMS DATA
   Groléat T, 2014, INT J NETW MANAG, V24, P253, DOI 10.1002/nem.1863
   István Z, 2015, ACM T RECONFIG TECHN, V8, DOI 10.1145/2629582
   [金琪 Jin Qi], 2020, [计算机科学, Computer Science], V47, P80
   Kekely L, 2014, IEEE INT SYMP DESIGN, P219, DOI 10.1109/DDECS.2014.6868793
   Kekely M, 2020, MICROPROCESS MICROSY, V73, DOI 10.1016/j.micpro.2019.102950
   Kirsch A, 2009, SIAM J COMPUT, V39, P1543, DOI 10.1137/080728743
   Levy G, 2017, IEEE COMMUN MAG, V55, P212, DOI 10.1109/MCOM.2017.1700132
   Li XF, 2014, MPhil thesis, P1, DOI [10.1145/2592798.2592820, DOI 10.1145/2592798.2592820]
   Lin CH, 2017, IEEE T PARALL DISTR, V28, P2639, DOI 10.1109/TPDS.2017.2674664
   Mandelbrod M, 2012, THEOR COMPUT SYST, V50, P279, DOI 10.1007/s00224-010-9297-0
   Pagh R, 2004, J ALGORITHMS, V51, P122, DOI 10.1016/j.jalgor.2003.12.002
   Pontarelli S, 2016, IEEE INTERNET COMPUT, V20, P46, DOI 10.1109/MIC.2016.55
   Pontarelli S, 2016, IEEE T COMPUT, V65, P326, DOI 10.1109/TC.2015.2417524
   Reviriego P, 2019, IEEE COMMUN LETT, V23, P1857, DOI 10.1109/LCOMM.2019.2930508
   Reviriego P, 2019, INFORM PROCESS LETT, V147, P55, DOI 10.1016/j.ipl.2019.03.008
   Sun YY, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P553
   Sun Y, 2017, IEEE T PARALL DISTR, V28, P619, DOI 10.1109/TPDS.2016.2594763
   Tong D, 2015, 2015 IEEE 29TH INTERNATIONAL PARALLEL AND DISTRIBUTED PROCESSING SYMPOSIUM WORKSHOPS, P105, DOI 10.1109/IPDPSW.2015.149
   Triplett Josh., 2011, PROC USENIX ATC, P11
   Wang X, 2018, IEEE T VLSI SYST, V26, P143, DOI 10.1109/TVLSI.2017.2753843
   Wellem T, 2019, IEEE ACCESS, V7, P92476, DOI 10.1109/ACCESS.2019.2927863
   Zuo PF, 2019, ACM T STORAGE, V15, DOI 10.1145/3322096
NR 35
TC 0
Z9 0
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2023
VL 19
IS 3
AR 28
DI 10.1145/3596910
PG 24
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P9QZ4
UT WOS:001053965500008
DA 2024-07-18
ER

PT J
AU Ge, XZ
   Cao, ZC
   Du, DHC
   Ganesan, P
   Hahn, D
AF Ge, Xiongzi
   Cao, Zhichao
   Du, David H. C.
   Ganesan, Pradeep
   Hahn, Dennis
TI HintStor: A Framework to Study I/O Hints in Heterogeneous Storage
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE I/O access hints; heterogeneous storage systems; block storage; data
   management
ID DATA-MANAGEMENT
AB To bridge the giant semantic gap between applications and modern storage systems, passing a piece of tiny and useful information, called I/O access hints, from upper layers to the storage layer may greatly improve application performance and ease data management in storage systems. This is especially true for heterogeneous storage systems that consist of multiple types of storage devices. Since ingesting external access hints will likely involve laborious modifications of legacy I/O stacks, it is very hard to evaluate the effect and take advantages of access hints. In this article, we design a generic and flexible framework, called HintStor, to quickly play with a set of I/O access hints and evaluate their impacts on heterogeneous storage systems. HintStor provides a new application/user-level interface, a file system plugin, and performs data management with a generic block storage data manager. We demonstrate the flexibility of HintStor by evaluating four types of access hints: file system data classification, stream ID, cloud prefetch, and I/O task scheduling on a Linux platform. The results show that HintStor can execute and evaluate various I/O access hints under different scenarios with minor modifications to the kernel and applications.
C1 [Ge, Xiongzi; Ganesan, Pradeep; Hahn, Dennis] NetApp, 7301 Kit Creek Rd, Res Triangle Pk, NC 27709 USA.
   [Cao, Zhichao; Du, David H. C.] Univ Minnesota, 4-192 Keller Hall,200 Union St SE, Minneapolis, MN 55455 USA.
   [Ganesan, Pradeep] 7886 169 St, Surrey, BC V4N 0J4, Canada.
   [Hahn, Dennis] 13111 Castlewood Cir, Wichita, KS 67230 USA.
C3 University of Minnesota System; University of Minnesota Twin Cities
RP Ge, XZ (corresponding author), NetApp, 7301 Kit Creek Rd, Res Triangle Pk, NC 27709 USA.
EM xiongzi@netapp.com; caoxx380@umn.edu; du@umn.edu; gp.esgan@gmail.com
OI Cao, Zhichao/0000-0001-6950-1776
FU NSF I/UCRC Center for Research in Intelligent Storage with NetApp
   collaboration; NSF [1439662, 1812537]; Direct For Computer & Info Scie &
   Enginr; Division Of Computer and Network Systems [1812537] Funding
   Source: National Science Foundation
FX This work was partially supported by NSF I/UCRC Center for Research in
   Intelligent Storage with NetApp collaboration and the following NSF
   awards 1439662 and 1812537.
CR Aghayev A, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P105
   Amazon, 2018, AM EL BLOCK STOR
   Amvrosiadis G, 2015, SOSP'15: PROCEEDINGS OF THE TWENTY-FIFTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P457, DOI 10.1145/2815400.2815424
   ANL, 2018, HINTS XFS
   [Anonymous], 2017, TECHN COMM T13 AT AT
   [Anonymous], 2007, UNDERSTANDING LINUX
   Bhimani Janki, 2018, P IEEE INT C CLOUD C
   Cao ZC, 2019, ACM T STORAGE, V15, DOI 10.1145/3295461
   Chang F, 1999, USENIX ASSOCIATION PROCEEDINGS OF THE THIRD SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '99), P1
   Cheng Yue., 2015, Proceedings of the 24th International Symposium on High-Performance Parallel and Distributed Computing, P45, DOI DOI 10.1145/2749246.2749252
   Corbet Jonathan, 2005, Linux device drivers, VThird
   Du David, 2006, P IEEE C MASS STORAG, V6
   Edge Jake, 2016, STORAGE STANDARDS UP
   Edge Jake, 2016, STREAM IDS I O HINTS
   Ge XZ, 2018, I S MOD ANAL SIM COM, P94, DOI 10.1109/MASCOTS.2018.00017
   Ge Xiongzi, 2014, P S CLOUD COMPUTING
   Github, 2017, FIO
   Github, 2021, STAND LIN IO TRAC
   Github, 2017, FILEBENCH
   Google, 2018, PERSISTENT DISK
   Gray J., 2008, ACM Queue, V6, P18, DOI 10.1145/1413254.1413261
   Guerra J., 2011, P 9ST USENIX C FAST, P20
   HaoWen David H. C., 2018, IEEE T CLOUD COMPUT, V9, P627
   Hua Y, 2009, PROCEEDINGS OF THE CONFERENCE ON HIGH PERFORMANCE COMPUTING NETWORKING, STORAGE AND ANALYSIS
   Hwang Euiseok, 2016, IEEE T MAGN, V53, P1
   Intel Labs, 2021, OP STOR TOOLK INT LA
   Kakoulli E, 2018, PROC VLDB ENDOW, V11, P1914, DOI 10.14778/3229863.3236223
   Kernel.org, 2017, SYSFS FIL EXP KERN O
   Kernel.org, 2017, BTRFS
   Kernel.org, 2017, FIEM IOCTL
   kernel.org, 2019, DM LIN
   Kernel.org, 2017, KCOP
   Kernel.org, 2021, SCSI INT GUID
   Kim Sangwook., 2015, USENIX Annual Technical Conference, P193
   Koller R., 2013, PROC USENIX C FILE S, P45
   Kougkas A, 2018, HPDC '18: PROCEEDINGS OF THE 27TH INTERNATIONAL SYMPOSIUM ON HIGH-PERFORMANCE PARALLEL AND DISTRIBUTED COMPUTING, P219, DOI 10.1145/3208040.3208059
   Kumar AV, 2020, PROCEEDINGS OF THE 18TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P283
   Kwon Y, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P460, DOI 10.1145/3132747.3132770
   Lee SK, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P257
   Li BZ, 2019, PERFORM EVALUATION, V130, P86, DOI 10.1016/j.peva.2018.11.003
   Linux, 2017, TC 8
   Linux, 2017, ION 1
   Linux, 2017, FADV 2
   Liu Y, 2015, COMPUT J, V58, P2061, DOI 10.1093/comjnl/bxu156
   LRZ, 2018, US GPFS
   Luo T, 2012, PROC VLDB ENDOW, V5, P1076, DOI 10.14778/2336664.2336679
   Macko Peter, 2017, ARXIV PREPRINT ARXIV
   Macko Peter, 2017, P 34 S MASS STOR SYS
   Mesnier M, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P57
   Microsoft, 2018, MICR AZ STOR
   MPI-forum, 2021, MPI I O FIL INF
   Nanavati M, 2016, COMMUN ACM, V59, P56, DOI 10.1145/2814342
   NERSC, 2018, OPT I O PERF LUSTR F
   Redhat, 2017, DEV MAPP RES PAG
   Redhat, 2018, I O LIM BLOCK SIZ AL
   Redhat, 2017, LVM2 RES PAG
   Rho E, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P257
   Seagate, 2017, ENT CAP 3 5 HDD HEL
   Shetti MM, 2021, IEEE INT SYMP PARAL, P202, DOI 10.1109/ISPA-BDCloud-SocialCom-SustainCom52081.2021.00039
   Sivathanu M, 2004, USENIX Association Proceedings of the Sixth Symposium on Operating Systems Design and Implementation (OSDE '04), P379
   SNIA.org, 2013, SCSI FC STAND UPD
   SONAMMANDAL GK, 2016, P C FIL STOR TECHN, P315
   T10.org, 2018, SCSI STAND ARCH
   Thomas-Krenn, 2017, LINUX STORAGE STACK
   Wang H., 2014, P 12 USENIX C FIL ST, P229
   Weiss Z., 2015, 13 USENIX C FIL STOR, P111
   Wen H, 2018, I S MOD ANAL SIM COM, P194, DOI 10.1109/MASCOTS.2018.00027
   Wikipedia, 2021, NVM EXPR
   Wu F., 2019, P 11 USENIX WORKSHOP
   Wu FG, 2021, IEEE T COMPUT, V70, P347, DOI 10.1109/TC.2020.2988257
   Wu FG, 2017, IEEE T COMPUT, V66, P1932, DOI 10.1109/TC.2017.2713360
   Wu Fenggang, 2018, P 10 USENIXWORKSHOP
   Wu Fenggang, 2016, P 8 USENIX WORKSHOP
   Zhang J, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P477
NR 74
TC 1
Z9 1
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAY
PY 2022
VL 18
IS 2
AR 18
DI 10.1145/3489143
PG 24
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 1H3XN
UT WOS:000796478700009
DA 2024-07-18
ER

PT J
AU Li, J
   Xu, XF
   Cai, ZG
   Liao, JW
   Li, KL
   Gerofi, B
   Ishikawa, Y
AF Li, Jun
   Xu, Xiaofei
   Cai, Zhigang
   Liao, Jianwei
   Li, Kenli
   Gerofi, Balazs
   Ishikawa, Yutaka
TI Pattern-Based Prefetching with Adaptive Cache Management Inside of
   Solid-State Drives
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE SSDs; SSD cache; prefetching; frequent access pattern; adaptive cache
   management; I/O time
AB This article proposes a pattern-based prefetching scheme with the support of adaptive cache management, at the flash translation layer of solid-state drives (SSDs). It works inside of SSDs and has features of OS dependence and uses transparency. Specifically, it first mines frequent block access patterns that reflect the correlation among the occurred I/O requests. Then, it compares the requests in the current time window with the identified patterns to direct prefetching data into the cache of SSDs. More importantly, to maximize the cache use efficiency, we build a mathematical model to adaptively determine the cache partition on the basis of I/O workload characteristics, for separately buffering the prefetched data and the written data. Experimental results show that our proposal can yield improvements on average read latency by 1 . 8%-36 . 5% without noticeably increasing the write latency, in contrast to conventional SSD-inside prefetching schemes.
C1 [Li, Jun; Xu, Xiaofei; Cai, Zhigang; Liao, Jianwei] Southwest Univ China, Chongqing 400715, Peoples R China.
   [Li, Kenli] Hunan Univ, Changsha 410082, Hunan, Peoples R China.
   [Gerofi, Balazs; Ishikawa, Yutaka] RIKEN, Ctr Comp Sci, Kobe, Hyogo 6500047, Japan.
C3 Hunan University; RIKEN
RP Liao, JW (corresponding author), Southwest Univ China, Chongqing 400715, Peoples R China.
EM junli95@email.swu.edu.cn; nakamura@email.swu.edu.cn; czg@swu.edu.cn;
   liaojianwei@il.is.s.u-tokyo.ac.jp; lkl@hnu.edu.cn; bgeroli@riken.jp;
   ishikawa@is.s.u-tokyo.ac.jp
RI Ishikawa, Yutaka/C-5335-2016; Li, Jun/AHE-0453-2022; Liao,
   Jianwei/C-5339-2016; XU, Xiaofei/KVX-8192-2024
OI Li, Jun/0000-0001-5235-6496; 
FU National Natural Science Foundation of China [61872299, 62032019];
   Chongqing Graduate Research and Innovation Project [CYB21110]; Chongqing
   Talent (Youth) [CQYC202005094]; State Key Laboratory for and Novel
   Software Technology [KFKT2021B06]
FX This work was partially supported by National Natural Science Foundation
   of China (No. 61872299, No. 62032019), Chongqing Graduate Research and
   Innovation Project (No. CYB21110), Chongqing Talent (Youth, No.
   CQYC202005094), and the Opening Project of State Key Laboratory for and
   Novel Software Technology (No. KFKT2021B06).
CR [Anonymous], COSMOS OPENSSD PLATF
   [Anonymous], Search Engine I/O
   [Anonymous], 2012, FAST
   Chang YH, 2012, ACM T EMBED COMPUT S, V11, DOI 10.1145/2146417.2146426
   Chang YH, 2010, IEEE T COMPUT, V59, P53, DOI 10.1109/TC.2009.134
   Cui JH, 2018, DES AUT TEST EUROPE, P1247, DOI 10.23919/DATE.2018.8342206
   GROENEVELD RA, 1984, STATISTICIAN, V33, P391, DOI 10.2307/2987742
   Gupta A., 2011, P USENIX C FIL STOR
   He Jun, 2013, P 22 INT S HIGHPERFO, P25
   Jiang S, 2013, ACM T STORAGE, V9, DOI 10.1145/2508010
   Jun Li, 2019, 2019 35th Symposium on Mass Storage Systems and Technologies (MSST). Proceedings, P126, DOI 10.1109/MSST.2019.00-10
   Jun SW, 2015, 2015 ACM/IEEE 42ND ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA), P1, DOI 10.1145/2749469.2750412
   Kang WH, 2014, SIGMOD'14: PROCEEDINGS OF THE 2014 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P529, DOI 10.1145/2588555.2595632
   Kwon Miryeong, 2017, P IEEE INT S WORKL C, DOI [10. 1109/IISWC.2017.8167759, DOI 10.1109/IISWC.2017.8167759]
   Laga A, 2016, IEEE NON-VOLATILE ME
   Lee C, 2017, SYSTOR'17: PROCEEDINGS OF THE 10TH ACM INTERNATIONAL SYSTEMS AND STORAGE CONFERENCE, DOI 10.1145/3078468.3078479
   Li J, 2020, IEEE T COMPUT AID D, V39, P3956, DOI 10.1109/TCAD.2020.3012252
   Li ZM, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P173
   Liao JW, 2016, IEEE T PARALL DISTR, V27, P2698, DOI 10.1109/TPDS.2015.2496595
   Liao JW, 2015, IEEE COMPUT ARCHIT L, V14, P1, DOI 10.1109/LCA.2014.2329871
   Matsui C, 2017, P IEEE, V105, P1812, DOI 10.1109/JPROC.2017.2716958
   Micheloni R, 2017, P IEEE, V105, P583, DOI 10.1109/JPROC.2017.2678018
   Narayanan D, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P253
   Park C, 2008, ACM T EMBED COMPUT S, V7, DOI 10.1145/1376804.1376806
   Pei SY, 2019, ACM T STORAGE, V15, DOI 10.1145/3310149
   PELED L, 2018, MEMORY PREFETCHING N
   Shriver E, 1999, PROCEEDINGS OF THE 1999 USENIX ANNUAL TECHNICAL CONFERENCE, P71
   Tan P.N., 2016, Introduction to data mining
   Tiwari Devesh, 2013, P USENIX C FIL STOR
   Tseng HW, 2016, CONF PROC INT SYMP C, P53, DOI 10.1109/ISCA.2016.15
   Uppal AJ, 2012, IEEE S MASS STOR SYS
   Wang YL, 2018, J SUPERCOMPUT, V74, P141, DOI 10.1007/s11227-017-2119-2
   Xu R, 2018, DES AUT TEST EUROPE, P273, DOI 10.23919/DATE.2018.8342018
   Xu XF, 2020, DES AUT TEST EUROPE, P720, DOI 10.23919/DATE48585.2020.9116382
   Yadgar Gala, 2016, P USENIX WORKSH HOT
   Zhang WH, 2018, INTERNATIONAL CONFERENCE ON SUPERCOMPUTING (ICS 2018), P22, DOI 10.1145/3205289.3205319
NR 36
TC 1
Z9 1
U1 1
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD FEB
PY 2022
VL 18
IS 1
AR 7
DI 10.1145/3474393
PG 25
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZN5OY
UT WOS:000765084700004
DA 2024-07-18
ER

PT J
AU Zhan, Y
   Conway, A
   Jiao, YZ
   Mukherjee, N
   Groombridge, I
   Bender, MA
   Farach-Colton, M
   Jannen, W
   Johnson, R
   Porter, DE
   Yuan, J
AF Zhan, Yang
   Conway, Alex
   Jiao, Yizheng
   Mukherjee, Nirjhar
   Groombridge, Ian
   Bender, Michael A.
   Farach-Colton, Martin
   Jannen, William
   Johnson, Rob
   Porter, Donald E.
   Yuan, Jun
TI Copy-on-Abundant-Write for Nimble File System Clones
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE B-epsilon-trees; file system; write optimization; clone
AB Making logical copies, or clones, of files and directories is critical to many real-world applications and workflows, including backups, virtual machines, and containers. An ideal clone implementation meets the following performance goals: (1) creating the clone has low latency; (2) reads are fast in all versions (i.e., spatial locality is always maintained, even after modifications); (3) writes are fast in all versions; (4) the overall system is space efficient. Implementing a clone operation that realizes all four properties, which we call a nimble clone, is a long-standing open problem.
   This article describes nimble clones in B-epsilon-tree File System (BetrFS), an open-source, full-path-indexed, and write-optimized file system. The key observation behind our work is that standard copy-on-write heuristics can be too coarse to be space efficient, or too fine-grained to preserve locality. On the other hand, a write-optimized key-value store, such as a B-epsilon-tree or an log-structured merge-tree (ISM)-tree, can decouple the logical application of updates from the granularity at which data is physically copied. In our write-optimized clone implementation, data sharing among clones is only broken when a clone has changed enough to warrant making a copy, a policy we call copy-on-abundant-write.
   We demonstrate that the algorithmic work needed to batch and amortize the cost of BetrFS clone operations does not erode the performance advantages of baseline BetrFS; BetrFS performance even improves in a few cases. BetrFS cloning is efficient; for example, when using the clone operation for container creation, BetrFS outperforms a simple recursive copy by up to two orders-of-magnitude and outperforms file systems that have specialized Linux Containers (LXC) backends by 3-4x.
C1 [Zhan, Yang; Jiao, Yizheng; Mukherjee, Nirjhar; Porter, Donald E.] Univ N Carolina, Dept Comp Sci, Chapel Hill, NC 27599 USA.
   [Conway, Alex] VMware Res Grp, Dept Comp Sci, Palo Alto, CA 94304 USA.
   [Groombridge, Ian; Yuan, Jun] Pace Univ, Dept Comp Sci, New York, NY 10038 USA.
   [Bender, Michael A.] SUNY Stony Brook, New Comp Sci, Stony Brook, NY 11794 USA.
   [Farach-Colton, Martin] Rutgers State Univ, Dept Comp Sci, Piscataway, NJ 08854 USA.
   [Jannen, William] Williams Coll, Comp Sci Dept, Williamstown, MA 01267 USA.
   [Johnson, Rob] VMware Res, 3425 Hillview Ave, Palo Alto, CA 94304 USA.
C3 University of North Carolina; University of North Carolina Chapel Hill;
   VMware, Inc.; Pace University; State University of New York (SUNY)
   System; State University of New York (SUNY) Stony Brook; Rutgers
   University System; Rutgers University New Brunswick; Williams College;
   VMware, Inc.
RP Zhan, Y (corresponding author), Univ N Carolina, Dept Comp Sci, Chapel Hill, NC 27599 USA.
EM yzhan@cs.unc.edu; conway@ajhconway.com; yizheng@cs.unc.edu;
   nirjhar@unc.edu; ig73104n@pace.edu; bender@cs.stonybrook.edu;
   farach@cs.rutgers.edu; jannen@cs.williams.edu; robj@vmware.com;
   porter@cs.unc.edu; jyuan2@pace.edu
OI Bender, Michael/0000-0001-7639-530X; Farach-Colton,
   Martin/0000-0003-3616-7788; Conway, Alex/0000-0003-4890-7413
FU NSF [CCF-1715777, CCF-1724745, CCF-1725543, CSR-1763680, CCF-1716252,
   CCF-1617618, CCF-1712716, CNS-1938709, CNS-1938180]; VMware; NetApp
   Faculty Fellowships; EMC
FX This research was supported in part by NSF grants CCF-1715777,
   CCF-1724745, CCF-1725543, CSR-1763680, CCF-1716252, CCF-1617618,
   CCF-1712716, CNS-1938709, and CNS-1938180. The work was also supported
   by VMware, by EMC, and by NetApp Faculty Fellowships.
CR [Anonymous], 2021, ACM T STORAGE, V17
   [Anonymous], 2005, ACM Transactions on Storage
   Bender MA, 2002, LECT NOTES COMPUT SC, V2461, P139
   Bender MA, 2019, SPAA'19: PROCEEDINGS OF THE 31ST ACM SYMPOSIUM ON PARALLELISM IN ALGORITHMS AND ARCHITECTURESS, 2019, P265, DOI 10.1145/3323165.3323210
   Bender MA, 2017, PODS'17: PROCEEDINGS OF THE 36TH ACM SIGMOD-SIGACT-SIGAI SYMPOSIUM ON PRINCIPLES OF DATABASE SYSTEMS, P69, DOI 10.1145/3034786.3056117
   Bender MA, 2007, SPAA'07: PROCEEDINGS OF THE NINETEENTH ANNUAL SYMPOSIUM ON PARALLELISM IN ALGORITHMS AND ARCHITECTURES, P81
   Bender Michael A., 2019, P 30 ANN ACM SIAM S, P2527, DOI DOI 10.1137/1.9781611975482.155
   Bender Michael A., 2015, login Usenix Mag., V40, P5
   BOBROW DG, 1972, COMMUN ACM, V15, P135, DOI 10.1145/361268.361271
   Bolosky Bill, 2000, P 4 USENIX WIND SYST
   Brodal GS, 2003, SIAM PROC S, P546
   Chutani S., 1992, Proceedings of the USENIX Winter 1992 Technical Conference, P43
   Conway A, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P45
   Conway Alex, 2019, 11 USENIX WORKSH HOT
   Conway Alex, 2017, LOGIN MAGAZINE, V42, P22
   Conway Alexander, 2018, ICALP LIPICS, V107
   Digital Equipment Corporation (DEC), 1988, TOPS 20 US MAN
   Dragga C, 2016, ACM T STORAGE, V12, DOI 10.1145/2857056
   Edwards JohnK., 2008, ATC 08, P129
   Esmet J., 2012, P 4 USENIX WORKSH HO
   Finis Jan, 2015, VLDB
   HITZ D, 1994, PROCEEDINGS OF THE WINTER 1994 USENIX CONFERENCE, P235
   HOWARD JH, 1988, ACM T COMPUT SYST, V6, P51, DOI 10.1145/35037.35059
   Jannen W., 2015, P 13 USENIX C FIL ST, P301, DOI DOI 10.5555/2750482.2750505
   Jannen W, 2015, ACM T STORAGE, V11, DOI 10.1145/2798729
   Konishi R., 2006, Operating Systems Review, V40, P102, DOI 10.1145/1151374.1151375
   LEHMAN PL, 1981, ACM T DATABASE SYST, V6, P650, DOI 10.1145/319628.319663
   McKusick MK, 1999, PROCEEDINGS OF THE FREENIX TRACK, P1
   Muniswamy-Reddy KK, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P115
   Nayak Prashanth, 2013, DETAILED STUDY LINUX
   ONeil P, 1996, ACTA INFORM, V33, P351, DOI 10.1007/s002360050048
   Pike R., 1990, UKUUG. UNIX - The Legend Evolves. Proceedings of the Summer 1990 UKUUG Conference, P1
   Rodeh Ohad, 2008, ACM Transaction on Storage, V3, p15:1, DOI 10.1145/1326542.1326544
   Rodeh O, 2013, ACM T STORAGE, V9, DOI 10.1145/2501620.2501623
   Santry DS, 1999, OPERATING SYSTEMS REVIEW, VOL 33, NO 5, DECEMBER 1999, P110, DOI 10.1145/319344.319159
   Schroeder Mike, 1985, P 10 ACM S OP SYST P
   Soules CA, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE 2ND USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST'03), P43
   Spillane Richard P., 2016, P 8 USENIX WORKSH HO
   TARASOV V, 2019, CLUSTER COMPUT, P1
   Tarasov V, 2017, 2017 IEEE 2ND INTERNATIONAL WORKSHOPS ON FOUNDATIONS AND APPLICATIONS OF SELF* SYSTEMS (FAS*W), P199, DOI 10.1109/FAS-W.2017.148
   Wu Xingbo, 2015, P 6 AS PAC WORKSH SY, V15
   Xu J, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P478, DOI 10.1145/3132747.3132761
   Yuan J, 2017, ACM T STORAGE, V13, DOI 10.1145/3032969
   Yuan J, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P1
   Zhan Y, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P123
   Zhan Y, 2018, ACM T STORAGE, V14, DOI 10.1145/3241061
   Zhao Frank, 2016, STOR DEV C
NR 47
TC 0
Z9 0
U1 1
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD FEB
PY 2021
VL 17
IS 1
AR 5
DI 10.1145/3423495
PG 27
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QG9WR
UT WOS:000617930800004
DA 2024-07-18
ER

PT J
AU Cha, H
   Nam, M
   Jin, K
   Seo, J
   Nam, B
AF Cha, Hokeun
   Nam, Moohyeon
   Jin, Kibeom
   Seo, Jiwon
   Nam, Beomseok
TI <i>B</i><SUP>3</SUP>-Tree: Byte-Addressable Binary B-Tree for Persistent
   Memory
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Non-volatile memory; data structure; persistent indexing
ID PERFORMANCE; STORAGE
AB In this work, we propose B-3-tree, a hybrid index for persistent memory that leverages the byte-addressability of the in-memory index and the page locality of B-trees. As in the byte-addressable in-memory index, B-3-tree is updated by 8-byte store instructions. Also, as in disk-based index, B-3-tree is failure-atomic since it makes every 8-byte store instruction transform a consistent index into another consistent index without the help of expensive logging. Since expensive logging becomes unnecessary, the number of cacheline flush instructions required for B-3-tree is significantly reduced. Our performance study shows that B-3-tree outperforms other state-of-the-art persistent indexes in terms of insert and delete performance. While B-3-tree shows slightly worse performance for point query performance, the range query performance of B-3-tree is 2x faster than FAST and FAIR B-tree because the leaf page size of B-3-tree can be set to 8x larger than that of FAST and FAIR B-tree without degrading insertion performance. We also show that read transactions can access B-3- tree without acquiring a shared lock because B-3-tree remains always consistent while a sequence of 8-byte write operations are making changes to it. As a result, B-3-tree provides high concurrency level comparable to FAST and FAIR B-tree.
C1 [Cha, Hokeun] Sungkyunkwan Univ, Coll Informat & Commun Engn, 2066 Jangan Gu, Suwon, South Korea.
   [Nam, Moohyeon; Jin, Kibeom] Ulsan Natl Inst Sci & Technol UNIST, 50 UNIST Gil, Ulsan Metropolitan City, South Korea.
   [Seo, Jiwon] Hanyang Univ, 222 Wangsim Ri, Seoul, South Korea.
   [Nam, Beomseok] Sungkyunkwan Univ, Coll Comp, 2066 Jangan Gu, Suwon, Gyeonggi Do, South Korea.
C3 Sungkyunkwan University (SKKU); Ulsan National Institute of Science &
   Technology (UNIST); Hanyang University; Sungkyunkwan University (SKKU)
RP Nam, B (corresponding author), Sungkyunkwan Univ, Coll Comp, 2066 Jangan Gu, Suwon, Gyeonggi Do, South Korea.
EM chahg0129@skku.edu; moohyeon.nam@gmail.com; seojiwon@hanyang.ac.kr;
   bnam@skku.edu
RI Nam, Beomseok/E-9071-2010
OI Cha, Hokeun/0009-0007-4305-5721
FU National Research Foundation of Korea [2016M3C4A7952587,
   2018R1A2B3006681]; Institute for Information & Communications Technology
   Promotion (IITP) - Ministry of Science and ICT, Korea [2018-0-00549];
   NST [B551179-12-04-00, 19ZS1220]
FX This research was supported by National Research Foundation of Korea
   (No. 2018R1A2B3006681, Improving Data Processing Performance with
   Byte-Addressable Non-Volatile Memory) and National Research Foundation
   of Korea (No. 2016M3C4A7952587, PF Class Heterogeneous High Performance
   Computer Development), and Institute for Information & Communications
   Technology Promotion (IITP) (grant No. 2018-0-00549, Extremely Scalable
   Order-preserving Operating System for Manycore and Non-volatile Memory)
   funded by Ministry of Science and ICT, Korea, and NST (B551179-12-04-00,
   ETRI R&D grant 19ZS1220).
CR [Anonymous], 2011, FAST 5
   [Anonymous], 2013, P 24 ACM S OP SYST P
   Arulraj J, 2015, SIGMOD'15: PROCEEDINGS OF THE 2015 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P707, DOI 10.1145/2723372.2749441
   Brown Trevor, 2015, P 34 ACM S PRINC DIS
   Chen SM, 2015, PROC VLDB ENDOW, V8, P786, DOI 10.14778/2752939.2752947
   Cohen Nachshon, 2015, P 27 ACM S PAR ALG A
   Condit J., 2009, P 22 ACM S OP SYST P
   Fang R, 2011, PROC INT CONF DATA, P1221, DOI 10.1109/ICDE.2011.5767918
   Gim Y, 2016, J COSMOL ASTROPART P, DOI 10.1088/1475-7516/2016/11/022
   Han H, 2015, INT SYM DEFEC FAU TO, P13, DOI 10.1109/DFT.2015.7315128
   Hart TE, 2007, J PARALLEL DISTR COM, V67, P1270, DOI 10.1016/j.jpdc.2007.04.010
   HP Enterprise Lab, 2018, QUARTZ
   Huai Y., 2008, AAPPS B, V18, P6
   HUANG J., 2014, PROC VLDB ENDOW, V8, P4
   Hwang Deukyeon, 2018, P 16 USENIX C FIL ST
   Intel, 2018, INT MICR PROD BREAKT
   Kaiyrakhmet Olzhas, 2019, P 17 USENIX C FIL ST
   Kim C., 2010, P 2010 ACM SIGMOD IN
   Kolli A, 2016, ACM SIGPLAN NOTICES, V51, P399, DOI 10.1145/2954679.2872381
   Lee J, 2013, REAL TIM SYST SYMP P, P11, DOI 10.1109/RTSS.2013.10
   Lee Se Kwon, 2017, P 15 USENIX C FIL ST
   Lee W., 2015, P 2015 USENIX AN TEC
   LEHMAN PL, 1981, ACM T DATABASE SYST, V6, P650, DOI 10.1145/319628.319663
   Leis Victor, 2014, P 30 INT C DAT ENG I
   Lersch L, 2019, PROC VLDB ENDOW, V13, P574, DOI 10.14778/3372716.3372728
   Mittal S, 2016, IEEE T PARALL DISTR, V27, P1537, DOI 10.1109/TPDS.2015.2442980
   Mittal S, 2015, IEEE T PARALL DISTR, V26, P1524, DOI 10.1109/TPDS.2014.2324563
   Nam Moohyeon, 2019, P 17 USENIX C FIL ST
   Oh G, 2015, PROC VLDB ENDOW, V8, P1454
   Ou JX, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901324
   Oukid Ismail, 2016, P 2016 ACM SIGMOD IN
   Rao J, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P78
   Rao Jun, 2000, P 2000 ACM SIGMOD IN
   Seo Jihye, 2017, P 22 INT C ARCH SUPP
   Shen Kai, 2014, P 11 USENIX C FIL ST
   Silberschatz A., 2005, DATABASE SYSTEMS CON, V5
   Srinath Santhosh, 2007, P IEEE 13 INT S HIGH
   Volos Haris, 2015, P 15 ANN MIDDL C MID
   Volos Haris, 2011, P 16 INT C ARCH SUPP
   Won Youjip, 2018, P 11 USENIX C FIL ST
   Wong HSP, 2010, P IEEE, V98, P2201, DOI 10.1109/JPROC.2010.2070050
   Yoon W, 2014, EURASIP J WIREL COMM, DOI 10.1186/1687-1499-2014-11
   Zhang Yiying, 2015, 2015 31 S MASS STOR, P1
   Zuo Pengfei, 2017, 33 INT C MASSIVE STO
NR 44
TC 9
Z9 10
U1 2
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2020
VL 16
IS 3
AR 17
DI 10.1145/3394025
PG 27
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1QZ
UT WOS:000583743600003
DA 2024-07-18
ER

PT J
AU Aghayev, A
   Weil, S
   Kuchnik, M
   Nelson, M
   Ganger, GR
   Amvrosiadis, G
AF Aghayev, Abutalib
   Weil, Sage
   Kuchnik, Michael
   Nelson, Mark
   Ganger, Gregory R.
   Amvrosiadis, George
TI The Case for Custom Storage Backends in Distributed Storage Systems
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article; Proceedings Paper
CT 27th ACM Symposium on Operating Systems Principles (SOSP)
CY OCT 27-30, 2019
CL Huntsville, CANADA
SP Assoc Comp Machinery, ACM SIGOPS, USENIX, Akamai, Alibaba, Amazon, ByteDance, Facebook, Huawei, Microsoft, NetApp, NSF, SIGOPS, Twitter, Vector Inst, VMware, Univ Waterloo, Ant Financial, Alipay
DE Ceph; object storage; distributed file system; storage backend; file
   system
AB For a decade, the Ceph distributed file system followed the conventional wisdom of building its storage back-end on top of local file systems. This is a preferred choice for most distributed file systems today, because it allows them to benefit from the convenience and maturity of battle-tested code. Ceph's experience, however, shows that this comes at a high price. First, developing a zero-overhead transaction mechanism is challenging. Second, metadata performance at the local level can significantly affect performance at the distributed level. Third, supporting emerging storage hardware is painstakingly slow.
   Ceph addressed these issues with BlueStore, a new backend designed to run directly on raw storage devices. In only two years since its inception, BlueStore outperformed previous established backends and is adopted by 70% of users in production. By running in user space and fully controlling the I/O stack, it has enabled space-efficient metadata and data checksums, fast overwrites of erasure-coded data, inline compression, decreased performance variability, and avoided a series of performance pitfalls of local file systems. Finally, it makes the adoption of backward-incompatible storage hardware possible, an important trait in a changing storage landscape that is learning to embrace hardware diversity.
C1 [Aghayev, Abutalib; Kuchnik, Michael; Ganger, Gregory R.; Amvrosiadis, George] Carnegie Mellon Univ, 5000 Forbes Ave, Pittsbrugh, PA 15213 USA.
   [Weil, Sage; Nelson, Mark] Red Hat Inc, 100 East Davie St, Raleigh, NC 27601 USA.
C3 Carnegie Mellon University
RP Aghayev, A (corresponding author), Carnegie Mellon Univ, 5000 Forbes Ave, Pittsbrugh, PA 15213 USA.
EM agayev@cs.cmu.edu; sweil@redhat.com; mkuchnik@cmu.edu;
   mnelson@redhat.com; ganger@ece.cmu.edu; gamvrosi@cmu.edu
FU NDSEG Fellowship
FX Michael Kuchnik is supported by an NDSEG Fellowship.
CR Aghayev A, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P105
   Aghayev Abutalib., 2015, P 13 USENIX C FILE S, P135
   Aghayev Abutalib, 2019, CMUPDL19102
   [Anonymous], 2010, GOOGLE FACULTY SUMMI
   Axboe Jens, 2009, QUEUE SYSFS FILES
   Axboe Jens, 2016, THROTTLED BACKGROUND
   Axboe Jens., 2016, FLEXIBLE I O TESTER
   Bjorling M., 2019, P USENIX VAULT
   Bjorling M, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P359
   Bjorling Matias, 2019, NEW NVME SPECIFICATI
   Blagodarenko A, 2016, SCALING LDISKFS FUTU
   Blagodarenko Artem, 2017, SCALING LDISKFS FUTU
   Brooks Jr Frederick P., 1986, NO SILVER BULLET ESS, DOI [10.1109/MC.1987.1663532, DOI 10.1109/MC.1987.1663532]
   Btrfs, 2019, BTRFS CHANGELOG
   Chang F, 2008, ACM T COMPUT SYST, V26, DOI 10.1145/1365815.1365816
   Chao Luoqing, 2015, IMPLEMENT OBJECT STO
   Chinner Dave, 2015, SMR LAYOUT OPTIMIZAT
   Chinner Dave, 2019, RE PAGECACHE LOCKING
   Chinner Dave, 2010, XFS DELAYED LOGGING
   Clouder Alibaba, 2018, ALIBABA DEPLOYS ALIB
   Cohen William, 2016, AVOID WASTING MEGABY
   Corbet Jonathan, 2009, Supporting transactions in btrfs
   Corbet Jonathan, 2018, POSTGRESQLS FSYNC SU
   Corbet Jonathan, 2011, NO I O DIRTY THROTTL
   David C., 2018, CEPH USERS LUMINOUS
   Dmitriev Anton, 2017, CEPH USERS ALL OSD F
   Dong Siying, 2018, DIRECT I O CLOSE SHO
   Edge J., 2015, ORANGEFS DISTRIBUTED
   Edge J., 2015, FILESYSTEM SUPPORT S
   Edge Jake, 2015, XFS THERE BACK THERE
   Engler D. R., 1995, Operating Systems Review, V29, P251, DOI 10.1145/224057.224076
   Foley Mary Jo, 2018, MICROSOFT READIES NE
   Ghemawat S., 2003, ACM SIGOPS OPERATING, V37, P29, DOI [DOI 10.1145/945445.945450, DOI 10.1145/1165389.945450, 10.1145/1165389.945450]
   Hao MZ, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P263
   Hellwig Christoph, 2009, login Usenix Mag, V34, P5
   Howard J. H., 1987, Operating Systems Review, V21, P1, DOI 10.1145/37499.37500
   Hruska Joel, 2019, W DIGITAL DEMO DUAL
   Hu YG, 2018, PROCEEDINGS OF THE 2018 USENIX ANNUAL TECHNICAL CONFERENCE, P879
   Huang CH, 2012, ELECTRON P THEOR COM, P15, DOI 10.4204/EPTCS.96.2
   Hupfeld F, 2008, CONCURR COMP-PRACT E, V20, P2049, DOI 10.1002/cpe.1304
   INCITS T10 Technical Committee, 2014, 536 T10BSR INCITS
   Jannen W, 2015, ACM T STORAGE, V11, DOI 10.1145/2798729
   Johnson T., 1994, P 20 INT C VER LARG, P439
   Kennedy John, 2018, ALTERNATIVES USING T
   Kennedy John, 2018, T NTFS
   Kim J., 2015, P 13 USENIX C FIL ST, P183
   Lakshman Avinash, 2010, Operating Systems Review, V44, P35, DOI 10.1145/1773912.1773922
   Lampson Butler, 1979, Crash Recovery in a Distributed Data Storage System
   Lee J, 2013, CURR APPL PHYS, V13, P309, DOI 10.1016/j.cap.2012.08.008
   Leventhal Adam, 2016, APFS DETAIL OVERVIEW
   Macko Peter, 2017, CORR
   Magic Pocket & Hardware Engineering Teams, 2019, SMR WHAT WE LEARNED
   Magic Pocket & Hardware Engineering Teams, 2018, EXTENDING MAGIC POCK
   Manzanares A., 2016, P 8 USENIX WORKSH HO
   Marowsky-Bree Lars, 2018, CEPH USER SURVEY 201
   Mazieres D, 1997, SIXTH WORKSHOP ON HOT TOPICS IN OPERATING SYSTEMS, PROCEEDINGS, P56, DOI 10.1109/HOTOS.1997.595183
   MCKUSICK MK, 1984, ACM T COMPUT SYST, V2, P181, DOI 10.1145/989.990
   Mellor Chris, 2019, TOSHIBA EMBRACES SHI
   Min C., 2015, Usenix Atc'15, P221
   Olson M.A., 1999, ATEC 99 P ANN C USEN, P43
   Olson Michael A., 1993, USENIX WINTER
   ONeil P, 1996, ACTA INFORM, V33, P351, DOI 10.1007/s002360050048
   OpenStack Foundation, 2017, 2017 ANN REP
   Palmer Adrian, 2015, SMRFFS EXT4 SMR FRIE
   Patil Swapnil., 2011, Proceedings of the 9th USENIX Conference on File and Stroage Technologies, FAST'11, P13
   Piernas J., 2002, Conference Proceedings of the 2002 International Conference on SUPERCOMPUTING, P137, DOI 10.1145/514191.514213
   Poornima G, 2016, METADATA PERFORMANCE
   Porter DE, 2009, SOSP'09: PROCEEDINGS OF THE TWENTY-SECOND ACM SIGOPS SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P161
   Prewitt Lee, 2019, SMR ZNS 2 SIDES SAME
   Red Hat Inc, 2019, GLUSTERFS ARCH
   Ren Kai., 2013, USENIX ANN TECHNICAL, P145
   Rosenblum M., 1991, Operating Systems Review, V25, P1, DOI 10.1145/121133.121137
   Schmuck F., 1991, Operating Systems Review, V25, P239, DOI 10.1145/121133.121171
   Schwarz TJE, 2004, IEEE COMPUTER SOCIETY'S 12TH ANNUAL INTERNATIONAL SYMPOSIUM ON MODELING, ANALYSIS, AND SIMULATION OF COMPUTER AND TELECOMMUNICATIONS SYSTEMS - PROCEEDINGS, P409, DOI 10.1109/MASCOT.2004.1348296
   Sct S.T., 1998, Journaling the linux ext2fs filesystem
   Seastar, 2019, SHARED NOTHING DESIG
   Seltzer M. I., 1993, Proceedings. Ninth International Conference on Data Engineering (Cat. No.92CH3258-1), P503, DOI 10.1109/ICDE.1993.344029
   Shen K., 2014, Proceedings of the 12th USENIX Conference on File and Storage Technologies FAST 14, P287
   Shilov A., 2017, Seagate ships 35th millionth SMR HDD, confirms HAMR-based drives in late 2018
   Shilov A., 2019, W DIGITAL HALF DATA
   Shvachko K., 2010, 2010 IEEE 26th Symposium on Mass Storage Systems and Technologies (MSST), P1
   Siebenmann Chris, 2011, ORDER READDIR RETURN
   Siebenmann Chris, 2013, ZFS T GROUPS ZFS INT
   Silicon Graphics Inc, 2006, XFS ALL GROUPS
   Spillane R. P., 2009, P USENIX C FIL STOR, V9, P29
   Stas Starikevich, 2016, CEPH USERS RADOSGW P
   Stender Jan, 2010, Proceedings of the 2010 International Workshop on Storage Network Architecture and Parallel I/Os (SNAPI 2010), P51, DOI 10.1109/SNAPI.2010.14
   Stonebraker M., 1986, SIGMOD Record, V15, P340, DOI 10.1145/16856.16888
   STONEBRAKER M, 1981, COMMUN ACM, V24, P412, DOI 10.1145/358699.358703
   Sumedh N., 2013, CODING PERFORMANCE D
   ThinkParQ, 2018, INTRO BEEGFS
   Weil Sage, 2009, RE RFC BIG FAT T IOC
   Weil Sage, 2011, PATCH V3 INTRO SYS S
   Weil Sage, 2009, RFC BIG FAT T IOCTL
   Weil SA, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P307
   Weil Sage A., 2006, P ACM IEEE C SUP SC, P122, DOI [10.1145/1188455.1188582, DOI 10.1145/1188455.1188582]
   Weil Sage A., 2007, PDSW, P35
   Welch B, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P17
   Western Digital, 2019, ZONED STORAGE
   Western Digital Inc, 2018, ZBC DEV MAN LIB
   Wright Charles P., 2007, ACM Transactions on Storage, V3, P1, DOI 10.1145/1242520.1242521
   Wu Fengguang, 2012, I O LESS DIRTY THROT
   Yan SQ, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P15
   Yao T, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P159
   Ying Lawrence, 2017, DYNAMIC HYBRID SMR O
   ZAR team, 2019, WRITE HOLE PHENOMENO
   Zhang Z., 2007, ACM SIGOPS OPERATING, V41, P175
   Zheng Q, 2018, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE, AND ANALYSIS (SC'18)
   Zhuravlev Alexey, 2016, ZFS METADATA PERFORM
NR 109
TC 6
Z9 6
U1 2
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUN
PY 2020
VL 16
IS 2
AR 9
DI 10.1145/3386362
PG 31
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA OJ1RC
UT WOS:000583743900002
OA Bronze
DA 2024-07-18
ER

PT J
AU Li, C
   Shilane, P
   Douglis, F
   Wallace, G
AF Li, Cheng
   Shilane, Philip
   Douglis, Fred
   Wallace, Grant
TI Pannier: Design and Analysis of a Container-Based Flash Cache for
   Compound Objects
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Flash cache; access count; eviction algorithm; block consolidation; I/O
   throttling
AB Classic caching algorithms leverage recency, access count, and/or other properties of cached blocks at per-block granularity. However, for media such as flash which have performance and wear penalties for small overwrites, implementing cache policies at a larger granularity is beneficial. Recent research has focused on buffering small blocks and writing in large granularities, sometimes called containers, but it has not explored the ramifications and best strategies for caching compound blocks consisting of logically distinct, but physically co-located, blocks. Containers may have highly diverse blocks, with mixtures of frequently accessed, infrequently accessed, and invalidated blocks.
   We propose and evaluate Pannier, a flash cache layer that provides high performance while extending flash lifespan. Pannier uses three main techniques: (1) leveraging block access counts to manage cache containers, (2) incorporating block liveness as a property to improve flash cache space efficiency, and (3) designing a multi-step feedback controller to ensure a flash cache reaches its desired lifespan while maintaining performance. Our evaluation shows that Pannier improves flash cache performance and extends lifespan beyond previous per-block and container-aware caching policies. More fundamentally, our investigation highlights the importance of creating new policies for caching compound blocks in flash.
C1 [Li, Cheng] VMware Inc, 3401 Hillview Ave, Palo Alto, CA 94304 USA.
   [Shilane, Philip; Douglis, Fred; Wallace, Grant] Dell EMC, 100 Canal Pointe Blvd, Princeton, NJ 08540 USA.
C3 VMware, Inc.; Dell Incorporated; Dell EMC
RP Li, C (corresponding author), VMware Inc, 3401 Hillview Ave, Palo Alto, CA 94304 USA.
EM chenglii@cs.rutgers.edu; philip.shilane@dell.com; fred.douglis@dell.com;
   grant.wallace@dell.com
RI Shilane, Philip/V-9134-2019
OI Douglis, Fred/0000-0003-2472-0339; Shilane, Philip/0000-0003-1235-0502
CR Agrawal Nitin, 2008, P USENIX ANN TECHN C, P57
   [Anonymous], P 6 USENIX C FIL STO
   [Anonymous], 2011, ACM 11 P INT C SUPER
   Badam A., 2011, Proceedings of the 8th USENIX Conference on Networked Systems Design and Implemen- tation, NSDI'11, P211
   BELADY LA, 1966, IBM SYST J, V5, P78, DOI 10.1147/sj.52.0078
   Breslau L, 1999, IEEE INFOCOM SER, P126, DOI 10.1109/INFCOM.1999.749260
   Busari M, 2002, COMPUT NETW, V38, P779, DOI 10.1016/S1389-1286(01)00285-7
   Cao P, 1997, PROCEEDINGS OF THE USENIX SYMPOSIUM ON INTERNET TECHNOLOGIES AND SYSTEMS, P193
   Caulfield AM, 2009, ACM SIGPLAN NOTICES, V44, P217, DOI 10.1145/1508284.1508270
   Chen F, 2009, PERF E R SI, V37, P181
   Cheng Y, 2016, PROCEEDINGS OF USENIX ATC '16: 2016 USENIX ANNUAL TECHNICAL CONFERENCE, P379
   Dean J, 2013, COMMUN ACM, V56, P74, DOI 10.1145/2408776.2408794
   Einziger G, 2014, EUROMICRO WORKSHOP P, P146, DOI 10.1109/PDP.2014.34
   Gill B. S., 2008, P 6 USENIX C FIL STO
   Gupta A, 2009, ACM SIGPLAN NOTICES, V44, P229, DOI 10.1145/1508284.1508271
   Jaleel A, 2010, CONF PROC INT SYMP C, P60, DOI 10.1145/1815961.1815971
   Jeon M, 2012, ACM T WEB, V6, DOI 10.1145/2382616.2382619
   Jo H, 2006, IEEE T CONSUM ELECTR, V52, P485, DOI 10.1109/TCE.2006.1649669
   Johnson T., 1994, P 20 INT C VER LARG, P439
   KAREDLA R, 1994, COMPUTER, V27, P38, DOI 10.1109/2.268884
   Kavalanekar Swaroop, 2008, 2008 IEEE International Symposium on Workload Characterization (IISWC), P119, DOI 10.1109/IISWC.2008.4636097
   Kim Y, 2009, SIMUL: 2009 FIRST INTERNATIONAL CONFERENCE ON ADVANCES IN SYSTEM SIMULATION, P125, DOI 10.1109/SIMUL.2009.17
   Koller R., 2013, PROC USENIX C FILE S, P45
   Lee SW, 2007, ACM T EMBED COMPUT S, V6, DOI 10.1145/1275986.1275990
   Li C., 2014, USENIX ANN TECH C AT, P501
   Li C, 2015, Proceedings of the 16th Annual Middleware Conference, P50, DOI 10.1145/2814576.2814734
   Li ZC, 2015, ACM T STORAGE, V11, DOI 10.1145/2700312
   Liang YS, 2016, PROCEEDINGS OF THE 9TH ACM INTERNATIONAL SYSTEMS AND STORAGE CONFERENCE (SYSTOR'16), DOI 10.1145/2928275.2928286
   Megiddo N, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE 2ND USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST'03), P115
   Mellor Chris, 2016, QLC FLASH IS TRICKY
   Micron, 2013, MICR MLC SSD SPEC
   Muralidhar S., 2014, P 11 USENIX C OP SYS, P383
   Narayanan Dushyanth, 2008, P 6 USENIX C FIL STO, P110
   Oh Y., 2012, P 10 USENIX C FIL ST, P10
   Ouyang J, 2014, ACM SIGPLAN NOTICES, V49, P471, DOI 10.1145/2541940.2541959
   Phalke V., 1995, Performance Evaluation Review, V23, P291, DOI 10.1145/223586.223620
   Qin D., 2014, USENIX ANN TECH C AT, P451
   Qureshi MK, 2007, CONF PROC INT SYMP C, P381, DOI 10.1145/1273440.1250709
   ROBINSON JT, 1990, PERF E R SI, V18, P134, DOI 10.1145/98460.98523
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   Samsung, 2015, SAMS SERV SSD SPEC
   Sandisk, 2015, SANDISK SATA SOLID S
   Saxena Mohit, 2012, P EUR C COMP SYST EU
   Shim H., 2013, 2013 USENIX ANN TECH, P157
   Smaragdakis Y, 1999, PERFORMANCE EVALUATION REVIEW, SPECIAL ISSUE, VOL 27 NO 1, JUNE 1999, P122, DOI 10.1145/301464.301486
   Song Jiang, 2002, Performance Evaluation Review, V30, P31, DOI 10.1145/511399.511340
   Sonneborn L., 1965, SIAM J CONTROL
   Tang Linpeng, 2015, 13 USENIX C FIL STOR, P373
   Temam O, 1998, ACM SIGPLAN NOTICES, V33, P218, DOI 10.1145/291006.291050
   Wang Jun, 2002, P 1 USENIX C FIL STO, P40
   Wilkes John, 1995, ACM T COMPUT SYST, V14, P1
   WU M, 1994, SIGPLAN NOTICES, V29, P86, DOI 10.1145/195470.195506
   Yoo J, 2013, IEEE S MASS STOR SYS
   Zhou YY, 2001, USENIX ASSOCIATION PROCEEDINGS OF THE 2001 USENIX ANNUAL TECHNICAL CONFERENCE, P91
NR 54
TC 15
Z9 16
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD OCT
PY 2017
VL 13
IS 3
SI SI
AR 24
DI 10.1145/3094785
PG 34
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FL6DV
UT WOS:000414336300007
DA 2024-07-18
ER

PT J
AU Chen, C
   Yang, J
   Wei, QS
   Wang, CD
   Xue, MD
AF Chen, Cheng
   Yang, Jun
   Wei, Qingsong
   Wang, Chundong
   Xue, Mingdi
TI Optimizing File Systems with Fine-grained Metadata Journaling on
   Byte-addressable NVM
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE File system; metadata journaling; non-volatile memory; Performance;
   Consistency; Design
ID MEMORY
AB Journaling file systems have been widely adopted to support applications that demand data consistency. However, we observed that the overhead of journaling can cause up to 48.2% performance drop under certain kinds of workloads. On the other hand, the emerging high-performance, byte-addressable Nonvolatile Memory (NVM) has the potential to minimize such overhead by being used as the journal device. The traditional journaling mechanism based on block devices is nevertheless unsuitable for NVM due to the write amplification of metadata journal we observed. In this article, we propose a fine-grained metadata journal mechanism to fully utilize the low-latency byte-addressable NVM so that the overhead of journaling can be significantly reduced. Based on the observation that conventional block-based metadata journal contains up to 90% clean metadata that is unnecessary to be journalled, we design a fine-grained journal format for byte-addressable NVM which contains only modified metadata. Moreover, we redesign the process of transaction committing, checkpointing, and recovery in journaling file systems utilizing the new journal format. Therefore, thanks to the reduced amount of ordered writes for journals, the overhead of journaling can be reduced without compromising the file system consistency. To evaluate our fine-grained metadata journaling mechanism, we have implemented a journaling file system prototype based on Ext4 and JBD2 in Linux. Experimental results show that our NVM-based fine-grained metadata journaling is up to 15.8x faster than the traditional approach under FileBench workloads.
C1 [Chen, Cheng; Yang, Jun; Wei, Qingsong; Wang, Chundong; Xue, Mingdi] ASTAR, Data Storage Inst, Data Ctr Technol Div, 2 Fusionopolis Way,08-01 Innovis S 138634, Singapore, Singapore.
C3 Agency for Science Technology & Research (A*STAR); A*STAR - Data Storage
   Institute
RP Wei, QS (corresponding author), ASTAR, Data Storage Inst, Data Ctr Technol Div, 2 Fusionopolis Way,08-01 Innovis S 138634, Singapore, Singapore.
EM CHEN_Cheng@dsi.a-star.edu.sg; yangju@dsi.a-star.edu.sg;
   WEI_Qingsong@dsi.a-star.edu.sg; wangc@dsi.a-star.edu.sg;
   XUE_Mingdi@dsi.a-star.edu.sg
RI WEI, Qingsong/P-4159-2019
CR Amvrosiadis George, 2015, P 25 ACM S OP SYST P
   Bent John, 2012, P IEEE 28 S MASS STO
   Boboila Simona, 2012, P IEEE 28 S MASS STO
   Chen Cheng, 2016, P IEEE 32 S MASS STO
   Chen Jianxi, 2013, P IEEE 29 S MASS SYS
   Chen SM, 2015, PROC VLDB ENDOW, V8, P786, DOI 10.14778/2752939.2752947
   Chua L, 2011, APPL PHYS A-MATER, V102, P765, DOI 10.1007/s00339-011-6264-9
   Condit J, 2009, SOSP'09: PROCEEDINGS OF THE TWENTY-SECOND ACM SIGOPS SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P133
   Custer H., 1992, Microcomputer Applications
   Dulloor Subramanya R., 2014, P EUR C COMP SYST EU, P1, DOI [10.1145/2592798.2592814, DOI 10.1145/2592798.2592814]
   Everspin, 2004, 2 GENERATION MRAM SP
   Fryer D, 2014, ACM T STORAGE, V10, DOI 10.1145/2675113
   Fryer D, 2012, ACM T STORAGE, V8, DOI 10.1145/2385603.2385608
   Gao Shen, 2011, P 20 ACM INT C INF K, P2401
   Intel, 2015, 3D XPOINT NEXT BREAK
   Jannen W., 2015, P 13 USENIX C FIL ST, P301, DOI DOI 10.5555/2750482.2750505
   Jeong Daeho., 2015, 13th USENIX Conference on File and Storage Technologies (FAST 15), P191
   Kang Dongwoo, 2015, P IEEE 31 S MASS SYS
   Katcher Jeffrey, 1997, TR3022
   Kawahara Takayuki, 2011, IEEE DES TEST COMPUT, V28
   Koller R., 2013, PROC USENIX C FILE S, P45
   Lee E., 2013, P 11 USENIX C FIL ST, P73
   Lee Eunji, 2012, P IEEE 28 S MASS STO
   Lee J, 2015, EURASIP J WIREL COMM, P1, DOI 10.1186/s13638-015-0359-5
   Li Shuangchen, 2015, P IEEE NONV MEM SYST
   Li Zheng, 2015, P IEEE NONV MEM SYST
   Margaglia F, 2016, P 14 USENIX C FIL ST
   Marmol Leonardo, 2014, P 6 USENIX WORKSH HO, P207
   Mathur Avantika, 2007, P LINUX S, V2, P21
   McDougall Richard, 2005, FILEBENCH APPL LEVEL
   Narayanan Dushyanth, 2012, P 17 INT C ARCH SUPP, P401, DOI DOI 10.1145/2150976.2151018
   Prabhakaran V., 2005, IEEE INT C DEP SYST
   Qureshi Moinuddin K., 2009, P ACM COMP ARCH NEWS
   Raoux S, 2008, IBM J RES DEV, V52, P465, DOI 10.1147/rd.524.0465
   Rocha PE, 2012, BRAZ SYM COMPUT SYST, P93, DOI 10.1109/SBESC.2012.26
   Santana R., 2016, ACM SIGOPS OPERATING, V49, P27
   Santos J., 2013, FLEXIBLE FILE SYSTEM
   Sehgal Priya, 2015, P IEEE 31 S MASS STO
   Skourtis Dimitris, 2014, P 2014 USENIX ANN TE
   Strunk John D., 2012, Operating Systems Review, V46, P50
   Sweeney A., 1996, P USENIX ANN TECHN C, V15
   Tsai Chia- Che, 2015, P 25 ACM S OP SYST
   Tweedie S., 2000, Ottawa Linux Symposium, P24
   VikingTechnology, 2014, ARXCIS NV TM NONV DI
   Volos H, 2011, ACM SIGPLAN NOTICES, V46, P91, DOI [10.1145/1961296.1950379, 10.1145/1961295.1950379]
   Wang CD, 2015, PROCEEDINGS OF THE 2015 IEEE INTERNATIONAL CONFERENCE ON NETWORKING, ARCHITECTURE AND STORAGE (NAS), P186, DOI 10.1109/NAS.2015.7255223
   Wei QS, 2015, ACM T STORAGE, V11, DOI 10.1145/2766453
   Weiss Z., 2015, 13 USENIX C FIL STOR, P111
   Wu Xiaojian, 2011, P 2011 INT ACM S HIG
   Yang Jun, 2015, P 13 USENIX C FIL ST, P167
   Yin LX, 2015, 18TH INTERNATIONAL WORKSHOP ON COMPUTATIONAL ELECTRONICS (IWCE 2015)
   Zhang Yiying, 2015, P IEEE 31 S MASS STO
   Zhou P, 2009, CONF PROC INT SYMP C, P14, DOI 10.1145/1555815.1555759
NR 53
TC 10
Z9 14
U1 0
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUN
PY 2017
VL 13
IS 2
SI SI
AR 13
DI 10.1145/3060147
PG 25
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FA1TA
UT WOS:000405221600004
DA 2024-07-18
ER

PT J
AU Yuan, J
   Zhan, Y
   Jannen, W
   Pandey, P
   Akshintala, A
   Chandnani, K
   Deo, P
   Kasheff, Z
   Walsh, L
   Bender, MA
   Farach-Colton, M
   Johnson, R
   Kuszmaul, BC
   Porter, DE
AF Yuan, Jun
   Zhan, Yang
   Jannen, William
   Pandey, Prashant
   Akshintala, Amogh
   Chandnani, Kanchan
   Deo, Pooja
   Kasheff, Zardosht
   Walsh, Leif
   Bender, Michael A.
   Farach-Colton, Martin
   Johnson, Rob
   Kuszmaul, Bradley C.
   Porter, Donald E.
TI Writes Wrought Right, and Other Adventures in File System Optimization
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article; Proceedings Paper
CT 14th USENIX Conference on File and Storage Technologies (FAST)
CY FEB 22-25, 2016
CL Santa Clara, CA
SP NetApp, Google, Veritas, VMWare, Facebook, Microsoft Res, EMC2, Huawei, Redhat, Tintri, Hewlett Packard Enterprise, IBM Res, Radian, SNIA, USENIX, ACM Queue, ADMIN Magazine, CRC Press, Linux Pro Magazine, LXer, No Starch Press, OReilly Media, Raspberry Pi Geek, UserFriendly Org, NSF
DE Design; Performance; Theory; B-epsilon -trees; file system; write
   optimization
AB File systems that employ write-optimized dictionaries (WODs) can perform random-writes, metadata updates, and recursive directory traversals orders ofmagnitude faster than conventional file systems. However, previous WOD-based file systems have not obtained all of these performance gains without sacrificing performance on other operations, such as file deletion, file or directory renaming, or sequential writes.
   Using three techniques, late-binding journaling, zoning, and range deletion, we show that there is no fundamental trade-off in write-optimization. These dramatic improvements can be retained while matching conventional file systems on all other operations.
   BetrFS 0.2 delivers order-of-magnitude better performance than conventional file systems on directory scans and small random writes and matches the performance of conventional file systems on rename, delete, and sequential I/O. For example, BetrFS 0.2 performs directory scans 2.2 x faster, and small random writes over two orders of magnitude faster, than the fastest conventional file system. But unlike BetrFS 0.1, it renames and deletes files commensurate with conventional file systems and performs large sequential I/O at nearly disk bandwidth. The performance benefits of these techniques extend to applications as well. BetrFS 0.2 continues to outperform conventional file systems on many applications, such as as rsync, git-diff, and tar, but improves git-clone performance by 35% over BetrFS 0.1, yielding performance comparable to other file systems.
C1 [Yuan, Jun] Farmingdale State Coll, Comp Syst Dept, Farmingdale, NY 11735 USA.
   [Zhan, Yang; Akshintala, Amogh] Univ North Carolina Chapel Hill, Chapel Hill, NC USA.
   [Zhan, Yang; Jannen, William; Pandey, Prashant; Akshintala, Amogh; Chandnani, Kanchan; Deo, Pooja; Porter, Donald E.] SUNY Stony Brook, Stony Brook, NY 11794 USA.
   [Chandnani, Kanchan] Apple Inc, Cupertino, CA USA.
   [Deo, Pooja] Arista Networks, Santa Clara, CA USA.
   [Kasheff, Zardosht] Facebook, Menlo Pk, CA USA.
   [Walsh, Leif] Two Sigma, New York, NY USA.
   [Farach-Colton, Martin] Rutgers State Univ, Dept Comp Sci, Piscataway, NJ 08854 USA.
   [Kuszmaul, Bradley C.] Oracle, Redwood City, CA USA.
   [Yuan, Jun; Bender, Michael A.; Johnson, Rob] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
   [Zhan, Yang; Akshintala, Amogh] Dept Comp Sci, Sitterson Hall,201 S Columbia St, Chapel Hill, NC 27516 USA.
   [Jannen, William] Williams Coll, Williamstown, MA USA.
   [Chandnani, Kanchan] 355 North Wolfe Rd,Apt 521, Sunnyvale, CA 94085 USA.
   [Kasheff, Zardosht] 770 Broadway,15th Floor, New York, NY 10003 USA.
   [Walsh, Leif] 102 Pierrepont St,Apt 3F, Brooklyn, NY 11201 USA.
   [Kuszmaul, Bradley C.] 37 Vaille Ave, Lexington, MA 02421 USA.
   [Porter, Donald E.] Univ North Carolina Chapel Hill, Dept Comp Sci, Campus Box 3175,Brooks Comp Sci Bldg, Chapel Hill, NC 27599 USA.
   [Kuszmaul, Bradley C.] MIT, Cambridge, MA 02139 USA.
C3 State University of New York (SUNY) System; Farmingdale State College;
   University of North Carolina; University of North Carolina Chapel Hill;
   University of North Carolina School of Medicine; State University of New
   York (SUNY) System; State University of New York (SUNY) Stony Brook;
   Apple Inc; Facebook Inc; Rutgers University System; Rutgers University
   New Brunswick; Oracle; State University of New York (SUNY) System; State
   University of New York (SUNY) Stony Brook; Williams College; University
   of North Carolina School of Medicine; University of North Carolina;
   University of North Carolina Chapel Hill; Massachusetts Institute of
   Technology (MIT)
RP Yuan, J (corresponding author), Farmingdale State Coll, Comp Syst Dept, Farmingdale, NY 11735 USA.
EM junyuan@cs.stonybrook.edu; yzhan@cs.unc.edu; jannen@cs.williams.edu;
   aakshintala@cs.unc.edu; ckanchan92@gmail.com; pdeo@cs.stonybrook.edu;
   zardosht@gmail.com; leif.walsh@gmail.com; bender@cs.stonybrook.edu;
   martin@farach-colton.edu; rob@cs.stonybrook.edu; Kuszmaul@gmail.com;
   porter@cs.unc.edu
OI Bender, Michael/0000-0001-7639-530X; Farach-Colton,
   Martin/0000-0003-3616-7788
FU NSF [CNS-1409238, CNS-1408782, CNS-1408695, CNS-1405641, CNS-1149229,
   CNS-1161541, CNS-1228839, IIS-1247750, CCF-1314547, CNS-1526707];
   VMware; Direct For Computer & Info Scie & Enginr; Division Of Computer
   and Network Systems [1755615, 1408695] Funding Source: National Science
   Foundation; Direct For Computer & Info Scie & Enginr; Division of
   Computing and Communication Foundations [1439084] Funding Source:
   National Science Foundation; Direct For Computer & Info Scie & Enginr;
   Div Of Information & Intelligent Systems [1247726, 1251137] Funding
   Source: National Science Foundation; Division Of Computer and Network
   Systems; Direct For Computer & Info Scie & Enginr [1408782, 1405641]
   Funding Source: National Science Foundation
FX This research was supported in part by NSF grants CNS-1409238,
   CNS-1408782, CNS-1408695, CNS-1405641, CNS-1149229, CNS-1161541,
   CNS-1228839, IIS-1247750, CCF-1314547, CNS-1526707, and VMware.
CR Andersen DG, 2009, SOSP'09: PROCEEDINGS OF THE TWENTY-SECOND ACM SIGOPS SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P1
   BENDER MA, 2015, MAGAZINE, V40, P5
   Bonwick Jeff, 2005, ZFS LASTWORD FILE SY
   Brodal GS, 2010, PROC APPL MATH, V135, P1448
   Brodal GS, 2003, SIAM PROC S, P546
   Esmet J., 2012, HotStorage
   Garimella N., 2006, Understanding and exploiting snapshot technology for data protection, Part 1: Snapshot technology overview
   Hatzieleftheriou Andromachi, 2011, P 2011 USENIX C USEN, P19
   Henson V., 2006, 2 WORKSHOP HOT TOPIC
   Jannen W., 2015, P 13 USENIX C FIL ST, P301, DOI DOI 10.5555/2750482.2750505
   Jannen W, 2015, ACM T STORAGE, V11, DOI 10.1145/2798729
   Kang Junbin, 2015, 2015 USENIX ANN TECH
   Lee Changman, 2015, 13 USENIX C FILE STO, P273
   Lester Nicholas., 2005, CIKM 05, P776
   Leung AndrewW., 2009, FAST 09, P153
   Lim H, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P1
   Lomet D, 1999, SIGMOD RECORD, VOL 28, NO 2 - JUNE 1999, P73, DOI 10.1145/304181.304189
   Lu Lanyue, 2014, P USENIX S OP SYST D, P81
   Mammarella M, 2009, SOSP'09: PROCEEDINGS OF THE TWENTY-SECOND ACM SIGOPS SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P147
   McKusick M. K., 2014, DESIGN IMPLEMENTATIO, P632
   McKusick MK, 1999, PROCEEDINGS OF THE FREENIX TRACK, P1
   Mitra Soumyadeb., 2008, SIGMOD, P623, DOI DOI 10.1145/1376616.1376680
   ONeil P, 1996, ACTA INFORM, V33, P351, DOI 10.1007/s002360050048
   Ren Kai., 2013, USENIX ANN TECHNICAL, P145
   Reuter A, 1992, T PROCESSING CONCEPT
   Rodeh O, 2013, ACM T STORAGE, V9, DOI 10.1145/2501620.2501623
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   Santry Douglas., 2014, USENIX ATC, P13
   Sears Russell, 2012, P ACM SIGMOD INT C M, P217
   Shetty P.J., 2013, 11 USENIX C FILE STO, P17
   Sweeney A, 1996, PROCEEDINGS OF THE USENIX 1996 ANNUAL TECHNICAL CONFERENCE, P1
   Tokutek, 2013, TOKUDB MYSQL PERF
   Verghese B, 1998, ACM SIGPLAN NOTICES, V33, P181, DOI 10.1145/291006.291044
   Wachs M, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 5TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES ( FAST '07), P61
   Weil S.A., 2004, Proceedings of the 2004 ACM/IEEE conference on Supercomputing, P4
   Weil SA, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P307
   Wu XN, 2015, METHODS MOL BIOL, V1306, P71, DOI 10.1007/978-1-4939-2648-0_5
NR 37
TC 10
Z9 11
U1 1
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAR
PY 2017
VL 13
IS 1
SI SI
AR 3
DI 10.1145/3032969
PG 26
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA ER3WE
UT WOS:000398729600003
DA 2024-07-18
ER

PT J
AU Aghayev, A
   Shafaei, M
   Desnoyers, P
AF Aghayev, Abutalib
   Shafaei, Mansour
   Desnoyers, Peter
TI Skylight-A Window on Shingled Disk Operation
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article; Proceedings Paper
CT 13th USENIX Conference on File and Storage Technologies (FAST)
CY FEB 16-19, 2015
CL Santa Clara, CA
SP USENIX, ACM SIGOPS
DE Design; Algorithms; Experimentation; Measurement; Performance;
   Reliability; Shingled magnetic recording; shingle translation layer;
   emulation; microbenchmarks; disks
ID FLASH TRANSLATION LAYER
AB We introduce Skylight, a novel methodology that combines software and hardware techniques to reverse engineer key properties of drive-managed Shingled Magnetic Recording (SMR) drives. The software part of Skylight measures the latency of controlled I/O operations to infer important properties of drive-managed SMR, including type, structure, and size of the persistent cache; type of cleaning algorithm; type of block mapping; and size of bands. The hardware part of Skylight tracks drive head movements during these tests, using a high-speed camera through an observation window drilled through the cover of the drive. These observations not only confirm inferences from measurements, but resolve ambiguities that arise from the use of latency measurements alone. We show the generality and efficacy of our techniques by running them on top of three emulated and two real SMR drives, discovering valuable performance-relevant details of the behavior of the real SMR drives.
C1 [Aghayev, Abutalib; Shafaei, Mansour; Desnoyers, Peter] Northeastern Univ, Coll Comp & Informat Sci, Boston, MA 02115 USA.
C3 Northeastern University
RP Aghayev, A (corresponding author), Northeastern Univ, Coll Comp & Informat Sci, 440 Huntington Ave,202 West Village H, Boston, MA 02115 USA.
EM agayev@gmail.com; shafaei@ece.neu.edu; pjd@.ccs.neu.edu
OI Desnoyers, Peter/0000-0002-6194-2806
FU National Science Foundation [CNS-1149232]; NetApp Faculty Fellowship
FX This work was supported by the National Science Foundation, under grant
   CNS-1149232, and by NetApp Faculty Fellowship.
CR Amer Ahmed., 2010, Mass Storage Systems and Technologies (MSST), 2010 IEEE 26th Symposium on, P1, DOI DOI 10.1109/MSST.2010.5496991
   Bairavasundaram Lakshmi N., 2008, ACM Transaction on Storage, V4, DOI 10.1145/1416944.1416947
   Bouganim Luc., 2009, P INT C INN DAT SYST
   Boyle William B., 2013, US Patent, Patent No. [8,443,167, 8443167]
   Cassuto Y., 2010, Proceedings of the 2010 IEEE 26th Symposium on Mass Storage Systems and Technologies (MSST), P1, DOI DOI 10.1109/MSST.2010.5496971
   Chen F, 2009, PERF E R SI, V37, P181
   Chung-I Lin, 2012, 2012 IEEE 20th International Symposium on Modelling, Analysis & Simulation of Computer and Telecommunication Systems (MASCOTS), P321, DOI 10.1109/MASCOTS.2012.44
   Dobisz EA, 2008, P IEEE, V96, P1836, DOI 10.1109/JPROC.2008.2007600
   DRAMeXchange, 2014, NAND FLASH SPOT PRIC
   Feldman T., 2013, Login Mag. USENIX SAGE, V38, P22
   Feldman Tim, 2014, COMMUNICATION
   Feldman Tim, 2014, HOST AWARE SMR OPENZ
   Feldman Timothy Richard, 2011, US Patent Appl, Patent No. [13/026,535, 13026535]
   FIO, FLEXIBLE IO TESTER
   Gibson G, 2011, CMU-PDL-11-107
   Gibson G., 2009, CMUPDL09104
   Gim JM, 2010, ACM T STORAGE, V6, DOI 10.1145/1807060.1807063
   Hall D. R., 2014, uS Patent, Patent No. [8,687,303, 8687303]
   Hall D, 2012, IEEE T MAGN, V48, P1777, DOI 10.1109/TMAG.2011.2179528
   Hall David Robison, 2013, US Patent, Patent No. [8,578,122, 8578122]
   He W., 2014, Proceedings of the 6th USENIX conference on Hot Topics in Storage and File Systems, P5
   HGST, 2014, HGST UNV INT DYN STO
   INCITS T10 Technical Committee, 2014, T10 BSR INCITS
   Jin C, 2014, IEEE S MASS STOR SYS
   Kim J, 2002, IEEE T CONSUM ELECTR, V48, P366
   Krevat Elie., 2011, Proceedings of the 13th USENIX conference on Hot topics in operating systems, HotOS'13, P14
   Krioukov A., 2008, P 6 USENIX C FIL STO, P9
   Kryder MH, 2008, P IEEE, V96, P1810, DOI 10.1109/JPROC.2008.2004315
   Le Moal D, 2012, 2012 IEEE INTERNATIONAL CONFERENCE ON CONSUMER ELECTRONICS (ICCE), P425, DOI 10.1109/ICCE.2012.6161799
   Le Q. M., 2011, Proceedings of the 2011 IEEE 19th International Symposium on Modelling, Analysis & Simulation of Computer and Telecommunication Systems (MASCOTS 2011), P444, DOI 10.1109/MASCOTS.2011.58
   Linux Device-Mapper, 2001, DEV MAPP RES PAG
   Park C, 2008, ACM T EMBED COMPUT S, V7, DOI 10.1145/1376804.1376806
   Piramanayagam SN, 2007, J APPL PHYS, V102, DOI 10.1063/1.2750414
   Pitchumani R., 2012, 2012 IEEE 20th International Symposium on Modelling, Analysis & Simulation of Computer and Telecommunication Systems (MASCOTS), P339, DOI 10.1109/MASCOTS.2012.46
   Poudyal Sundar, 2013, US Patent Appl, Patent No. [13/799,827, 13799827]
   Riley Drew, 2013, SAMSUNGS GLOBAL SUMM
   Rosenblum M., 1991, Operating Systems Review, V25, P1, DOI 10.1145/121133.121137
   SATA-IO, 2011, SER ATA REV 3 1 SPEC
   Schlosser StevenW., 2005, Proceedings of the 4th conference on USENIX Conference on File and Storage Technologies, V4, P17
   Seagate, 2014, SEAG SHIPS WORLDS 1
   Seagate, 2013, SEG TECHN PLC FISC 4
   Seagate, 2013, TER HDD
   Seagate, 2013, SEG DESK HDD ST5000D
   Talagala Nisha, 1999, UCBCSD991063 EECS DE
   Tan S, 2013, IEEE T MAGN, V49, P2677, DOI 10.1109/TMAG.2013.2245872
   Thompson DA, 2000, IBM J RES DEV, V44, P311, DOI 10.1147/rd.443.0311
   Wang SM, 2013, IEEE T MAGN, V49, P3644, DOI 10.1109/TMAG.2012.2237545
   Wood R, 2009, IEEE T MAGN, V45, P917, DOI 10.1109/TMAG.2008.2010676
   Worthington B. L., 1995, Performance Evaluation Review, V23, P146, DOI 10.1145/223586.223604
NR 49
TC 44
Z9 47
U1 2
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2015
VL 11
IS 4
SI SI
AR 16
DI 10.1145/2821511
PG 28
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DB0GY
UT WOS:000368187800002
DA 2024-07-18
ER

PT J
AU Wu, XJ
   Qiu, S
   Reddy, ALN
AF Wu, Xiaojian
   Qiu, Sheng
   Reddy, A. L. Narasimha
TI SCMFS: A File System for Storage Class Memory and its Extensions
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Measurement; Performance; Storage class memory; file systems;
   storage management; performance measurement
ID PHASE-CHANGE MEMORY
AB Modern computer systems have been built around the assumption that persistent storage is accessed via a slow, block-based interface. However, emerging nonvolatile memory technologies (sometimes referred to as storage class memory (SCM)), are poised to revolutionize storage systems. The SCM devices can be attached directly to the memory bus and offer fast, fine-grained access to persistent storage.
   In this article, we propose a new file system-SCMFS, which is specially designed for Storage Class Memory. SCMFS is implemented on the virtual address space and utilizes the existing memory management module of the operating system to help mange the file system space. As a result, we largely simplified the file system operations of SCMFS, which allowed us a better exploration of performance gain from SCM. We have implemented a prototype in Linux and evaluated its performance through multiple benchmarks. The experimental results show that SCMFS outperforms other memory resident file systems, tmpfs, ramfs and ext2 on ramdisk, and achieves about 70% of memory bandwidth for file read/write operations.
C1 [Wu, Xiaojian; Qiu, Sheng; Reddy, A. L. Narasimha] Texas A&M Univ, Dept Elect & Comp Engn, College Stn, TX 77843 USA.
C3 Texas A&M University System; Texas A&M University College Station
RP Wu, XJ (corresponding author), Texas A&M Univ, Dept Elect & Comp Engn, College Stn, TX 77843 USA.
EM tristan.woo@neo.tamu.edu; herbert1984106@neo.tamu.edu;
   reddy@ece.tamu.edu
CR [Anonymous], 2011, PROC 9 USENIX C FILE
   [Anonymous], TR3022 NETW APPL INC
   [Anonymous], 2011, FAST
   [Anonymous], P OTT LIN S REDHAT I
   [Anonymous], 2011, P FAST 2
   [Anonymous], P USENIX ANN TECHN C
   Chen PM, 1996, ACM SIGPLAN NOTICES, V31, P74, DOI 10.1145/248209.237154
   Coburn J, 2011, ACM SIGPLAN NOTICES, V46, P105, DOI [10.1145/1961295.1950380, 10.1145/1961296.1950380]
   Condit J, 2009, SOSP'09: PROCEEDINGS OF THE TWENTY-SECOND ACM SIGOPS SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P133
   Josephson W.K., 2010, PROC USENIX C FILE S, V6, P85
   Lee BC, 2010, IEEE MICRO, V30, P131, DOI 10.1109/MM.2010.24
   Lee BC, 2009, CONF PROC INT SYMP C, P2, DOI 10.1145/1555815.1555758
   Mucci PhilipJ., 1999, P DEP DEFENSE HPCMP, P7
   Narayanan D, 2012, ASPLOS XVII: SEVENTEENTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS, P401
   Ouyang XY, 2011, INT S HIGH PERF COMP, P301, DOI 10.1109/HPCA.2011.5749738
   Qureshi M., 2010, P 16 IEEE INT S HIGH, P1
   Qureshi Moinuddin K., 2009, Proceedings of the 2009 42nd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO 2009), P14, DOI 10.1145/1669112.1669117
   Qureshi MK, 2011, INT S HIGH PERF COMP, P478, DOI 10.1109/HPCA.2011.5749753
   Ramos Luiz E, 2011, P INT C SUP, P85
   Raoux S, 2008, IBM J RES DEV, V52, P465, DOI 10.1147/rd.524.0465
   Seong NH, 2010, CONF PROC INT SYMP C, P383, DOI 10.1145/1816038.1816014
   Snyder P., 1990, Proceedings of the Autumn 1990 EUUG Conference, P241
   Strukov DB, 2008, NATURE, V453, P80, DOI 10.1038/nature06932
   Volos H, 2011, ACM SIGPLAN NOTICES, V46, P91, DOI [10.1145/1961296.1950379, 10.1145/1961295.1950379]
   Wang AIA, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE GENERAL TRACK, P15
   WU M, 1994, SIGPLAN NOTICES, V29, P86, DOI 10.1145/195470.195506
   Zhou P, 2009, CONF PROC INT SYMP C, P14, DOI 10.1145/1555815.1555759
NR 27
TC 33
Z9 35
U1 0
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2013
VL 9
IS 3
AR 7
DI 10.1145/2501620.2501621
PG 23
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 216XX
UT WOS:000324321200001
DA 2024-07-18
ER

PT J
AU Kim, H
   Agrawal, N
   Ungureanu, C
AF Kim, Hyojun
   Agrawal, Nitin
   Ungureanu, Cristian
TI Revisiting Storage for Smartphones
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Performance; Measurement; Storage systems; mobile; smartphones; mobile
   storage; Android
AB Conventional wisdom holds that storage is not a big contributor to application performance on mobile devices. Flash storage (the type most commonly used today) draws little power, and its performance is thought to exceed that of the network subsystem. In this article, we present evidence that storage performance does indeed affect the performance of several common applications such as Web browsing, maps, application install, email, and Facebook. For several Android smartphones, we find that just by varying the underlying flash storage, performance over WiFi can typically vary between 100% and 300% across applications; in one extreme scenario, the variation jumped to over 2000%. With a faster network (set up over USB), the performance variation rose even further. We identify the reasons for the strong correlation between storage and application performance to be a combination of poor flash device performance, random I/O from application databases, and heavy-handed use of synchronous writes. Based on our findings, we implement and evaluate a set of pilot solutions to address the storage performance deficiencies in smartphones.
RP Kim, H (corresponding author), Georgia Inst Technol, Atlanta, GA 30332 USA.
EM nitin@nec-labs.com
CR ALPHONSO LABS, PULS NEWS READ
   [Anonymous], WG80211 IEEE
   [Anonymous], MOBIOPP
   [Anonymous], 2009, INT TECHNOLOGY ROADM
   [Anonymous], 2010, PROC 8 INT C MOBILE, DOI [DOI 10.1145/1814433, 10.1145/1814433.1814441, DOI 10.1145/1814433.1814441]
   [Anonymous], 2010, P USENIX ANN TECH C
   [Anonymous], 2012, MONKEYRUNNER ANDR DE
   [Anonymous], 2011, DAT SOFTW RISK FREE
   [Anonymous], P 1 ACM WORKSH MOB C
   [Anonymous], 2011, P 23 ACM S OP SYST P
   [Anonymous], 2010, P USENIX S OP SYST D
   [Anonymous], 2011, GARTN HIGHL KEY PRED
   [Anonymous], YAFFS YET ANOTHER FL
   [Anonymous], 2011, HTC EVO 4G
   BICKFORD Jeffrey, 2011, P 9 INT C MOB SYST A
   BLKTRACE, 2006, BLOCK I O LAYER TRAC
   BUSYBOX, 2008, BUS UN UT
   CARBOU M., 2010, USB REVERSE TETHERIN
   Castellucci S., 2011, Proceedings of the 2011 annual conference extended abstracts on Human factors in computing systems, P1507
   Chen Shimin, 2011, P 5 BIENN C INN DAT, P21, DOI DOI 10.1145/2029956.2029964
   Chun BG, 2011, EUROSYS 11: PROCEEDINGS OF THE EUROSYS 2011 CONFERENCE, P301
   COMPETE, 2011, COMP RANK TOP 50 WEB
   CRYSTALMARK, 2012, CRYSTALDISKMARK BENC
   DATTA K., 2010, CLOCKWORKMOD ROM MAN
   Dietz M., 2011, P 20 USENIX SEC S P 20 USENIX SEC S
   Douglis F., 1994, Proceedings of the First USENIX Symposium on Operating Systems Design and Implementation (OSDI), P25
   Falaki Hossein., 2010, Diversity in smartphone usage, P179, DOI DOI 10.1145/1814433.1814453
   Flinn J, 1999, OPERATING SYSTEMS REVIEW, VOL 33, NO 5, DECEMBER 1999, P48, DOI 10.1145/319344.319155
   FLINN J., 2003, P 2 USENIX S FIL STO
   Geambasu R, 2011, EUROSYS 11: PROCEEDINGS OF THE EUROSYS 2011 CONFERENCE, P1
   GUNDOTRA V., 2011, ANDROID MOMENTUM MOB
   Hackborn Dianne, 2010, MULTITASKING ANDROID
   HALPERIN D, 2011, PROC ACM SIGC 2011, V41, P38, DOI DOI 10.1145/2043164.2018442
   HTC, 2011, HTC DES
   Huai Y., 2008, AAPPS B, V18, P33
   Huang Junxian., 2010, MobiSys, P165, DOI [10.1145/1814433.1814452, DOI 10.1145/1814433.1814452]
   Joo Yongsoo, 2011, P 9 USENIX C FIL STO
   Kim Hyojun, 2012, P 10 USENIX C FIL ST, P1
   Kim J, 2002, IEEE T CONSUM ELECTR, V48, P366
   KISTLER JJ, 1992, ACM T COMPUT SYST, V10, P3, DOI 10.1145/146941.146942
   Konishi R., 2006, Operating Systems Review, V40, P102, DOI 10.1145/1151374.1151375
   Koukoumidis E, 2011, ACM SIGPLAN NOTICES, V46, P171, DOI [10.1145/1961295.1950387, 10.1145/1961296.1950387]
   LG, 2011, LG G2X DAT
   MOTOROLA, 2011, MOT WEBT REL YOUR SM
   NEXUS ONE, 2011, GOOGL NEX ON
   Noble B. D., 1997, Operating Systems Review, V31, P276, DOI 10.1145/269005.266708
   PENTIN, 2010, GARTN MOB PRED
   REDLICENSE LABS, 2012, RL BENCHM SQLITE
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   Roy A, 2011, EUROSYS 11: PROCEEDINGS OF THE EUROSYS 2011 CONFERENCE, P139
   SAMSUNG CORP, 2011, SAMS SHIPS IND 1 MUL
   Satyanarayanan M, 2009, IEEE PERVAS COMPUT, V8, P14, DOI 10.1109/MPRV.2009.82
   Schlosser SW, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P87
   SD ASSOCIATION, 2012, SD SPEED CLASS UHS S
   Sivathanu M, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE 2ND USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST'03), P73
   SOURCEFORGE, 2012, IP IP NETW PERF TOOL
   SQLITE, 2012, US DAT
   SQLITE, 2011, SQLITE BACK WRIT AH
   STARBURST, 2012, STARB DATA2SD
   Tolia N, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P227
   Ts'o T., 2010, ANDROID WILL BE USIN
   UNREVOKED, 2012, UNR 3 SET YOUR PHON
   VEERARAGHAVAN K., 2010, P 8 USENIX C FIL STO
   Wang Z., 2011, P ACM INT WORKSH MOB
   WEBKIT, 2012, ANDR WEBKIT PACK
NR 65
TC 70
Z9 77
U1 0
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2012
VL 8
IS 4
AR 14
DI 10.1145/2385603.2385607
PG 25
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 052QF
UT WOS:000312212500003
DA 2024-07-18
ER

PT J
AU Klonatos, Y
   Makatos, T
   Marazakis, M
   Flouris, MD
   Bilas, A
AF Klonatos, Yannis
   Makatos, Thanos
   Marazakis, Manolis
   Flouris, Michail D.
   Bilas, Angelos
TI Transparent Online Storage Compression at the Block-Level
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Performance; Experimentation; Measurement; Block-level
   compression; SSD-based I/O cache
AB In this work, we examine how transparent block-level compression in the I/O path can improve both the space efficiency and performance of online storage. We present ZBD, a block-layer driver that transparently compresses and decompresses data as they flow between the file-system and storage devices. Our system provides support for variable-size blocks, metadata caching, and persistence, as well as block allocation and cleanup. ZBD targets maintaining high performance, by mitigating compression and decompression overheads that can have a significant impact on performance by leveraging modern multicore CPUs through explicit work scheduling. We present two case-studies for compression. First, we examine how our approach can be used to increase the capacity of SSD-based caches, thus increasing their cost-effectiveness. Then, we examine how ZBD can improve the efficiency of online disk-based storage systems.
   We evaluate our approach in the Linux kernel on a commodity server with multicore CPUs, using Post-Mark, SPECsfs2008, TPC-C, and TPC-H. Preliminary results show that transparent online block-level compression is a viable option for improving effective storage capacity, it can improve I/O performance up to 80% by reducing I/O traffic and seek distance, and has a negative impact on performance, up to 34%, only when single-thread I/O latency is critical. In particular, for SSD-based caching, our results indicate that, in line with current technology trends, compressed caching trades off CPU utilization for performance and enhances SSD efficiency as a storage cache up to 99%.
C1 [Klonatos, Yannis; Bilas, Angelos] Univ Crete, Dept Comp Sci, GR-71409 Iraklion, Greece.
   [Klonatos, Yannis; Makatos, Thanos; Marazakis, Manolis; Flouris, Michail D.; Bilas, Angelos] Fdn Res & Technol Hellas, Hellas, Greece.
C3 University of Crete; Foundation for Research & Technology - Hellas
   (FORTH)
RP Klonatos, Y (corresponding author), Fdn Res & Technol Hellas FORTH, Inst Comp Sci, 100 N Plastira Ave, GR-70013 Iraklion, Greece.
EM yannis.klonatos@epfl.ch
RI Flouris, Michail D/A-2308-2009
OI Bilas, Angelos/0000-0003-2975-4124
FU European Commission [FP7-ICT-216181, NoE-004408, FP7-ICT-217068,
   FP7-STREP-248615]
FX This work was partially supported by the European Commission under the
   7th Framework Programs through the STREAM (FP7-ICT-216181), HiPEAC
   (NoE-004408), HiPEAC2 (FP7-ICT-217068), and IOLanes (FP7-STREP-248615)
   projects.
CR Agrawal Nitin, 2008, P USENIX ANN TECHN C, P57
   [Anonymous], SPECMAIL2009 PUBL RE
   [Anonymous], 2009, P USENIX ANN TECHN C
   [Anonymous], COMPRESSED LOOPBACK
   [Anonymous], P 29 VLDB C
   [Anonymous], FUS IOS SOL STAT STO
   [Anonymous], CBD COMPRESSED BLOCK
   [Anonymous], P ACM SIGOPS EW
   [Anonymous], 2010, COMPCACHE COMPRESSED
   [Anonymous], **NON-TRADITIONAL**
   [Anonymous], FUSECOMPRESS MOUNTAB
   [Anonymous], TPC H AD HOC DEC SUP
   [Anonymous], FALCONSTOR HOTZONE M
   [Anonymous], TOP 10 NONCL TPC H P
   [Anonymous], LOGFS FINALLY SCALAB
   [Anonymous], BEST PRACT NTFS COMP
   [Anonymous], 1994, WTEC 94
   [Anonymous], YAFFS NAND FLASH FIL
   [Anonymous], MAXIQ SSD CACH PERF
   [Anonymous], SURVEY MODERN FILE C
   [Anonymous], EXPL FEAT WIND READY
   [Anonymous], SPECSFS2008 CIFS PUB
   [Anonymous], UND NTFS COMPR
   [Anonymous], ACM COMPUT SURV
   [Anonymous], DATABASE TEST 2 DBT
   [Anonymous], OV TPC BENCHM C ORD
   [Anonymous], SPECSFS2008 SPECS BE
   [Anonymous], 2008, LZO REAL TIME DATA C
   [Anonymous], SQUASHFS
   [Anonymous], JFFS JOURNALLING FLA
   [Anonymous], OR SOL ZFS
   APPEL AW, 1991, SIGPLAN NOTICES, V26, P96, DOI 10.1145/106973.106984
   Ayers L., 1997, E2COMPR TRANSPARENT
   Bobbarjung D. R., 2006, ACM Transaction on Storage, V2, P424, DOI 10.1145/1210596.1210599
   BURROWS M, 1992, SIGPLAN NOTICES, V27, P2, DOI 10.1145/143371.143376
   CORMACK GV, 1985, COMMUN ACM, V28, P1336, DOI 10.1145/214956.214963
   Deutsch L.P., 1996, ZLIB COMPRESSED DATA
   Dirik C, 2009, CONF PROC INT SYMP C, P279, DOI 10.1145/1555815.1555790
   Douglis F., 1993, USENIX Association. Proceedings of the Winter 1993 USENIX Conference, P519
   Katcher Jeffrey., 1997, POSTMARK NEW FILE SY
   Kgil Taeho., 2006, P 2006 INT C COMPILE, P103, DOI DOI 10.1145/1176760.1176774
   Kim H., 2008, P 6 USENIX C FILE ST
   Lee S.W., 2008, Proceedings of the 2008 ACM SIGMOD international conference on Management of data, SIGMOD '08, P1075
   Leventhal A, 2008, COMMUN ACM, V51, P47, DOI 10.1145/1364782.1364796
   Makatos Thanos, 2010, Proceedings of the 2010 International Workshop on Storage Network Architecture and Parallel I/Os (SNAPI 2010), P61, DOI 10.1109/SNAPI.2010.15
   Makatos T, 2010, EUROSYS'10: PROCEEDINGS OF THE EUROSYS 2010 CONFERENCE, P1
   Meisner D, 2009, ACM SIGPLAN NOTICES, V44, P205, DOI 10.1145/1508284.1508269
   Narayanan D, 2009, EUROSYS'09: PROCEEDINGS OF THE FOURTH EUROSYS CONFERENCE, P145
   Ng WK, 1997, IEEE T KNOWL DATA EN, V9, P314, DOI 10.1109/69.591455
   Rizzo I., 1997, Operating Systems Review, V31, P36, DOI 10.1145/250007.250012
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   SMITH MEG, 1985, J ACM, V32, P344, DOI 10.1145/3149.3152
   WELCH TA, 1984, COMPUTER, V17, P8, DOI 10.1109/MC.1984.1659158
   Wilson PR, 1999, PROCEEDINGS OF THE 1999 USENIX ANNUAL TECHNICAL CONFERENCE, P101
   Yang L, 2005, 2005 INTERNATIONAL CONFERENCE ON HARDWARE/SOFTWARE CODESIGN AND SYSTEM SYNTHESIS, P93
   Zhu B., 2008, P 6 USENIX C FILE ST, p18:1
   ZIV J, 1977, IEEE T INFORM THEORY, V23, P337, DOI 10.1109/TIT.1977.1055714
NR 57
TC 3
Z9 4
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAY
PY 2012
VL 8
IS 2
AR 5
DI 10.1145/2180905.2180906
PG 33
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 990LY
UT WOS:000307633600001
DA 2024-07-18
ER

PT J
AU Balakrishnan, M
   Kadav, A
   Prabhakaran, V
   Malkhi, D
AF Balakrishnan, Mahesh
   Kadav, Asim
   Prabhakaran, Vijayan
   Malkhi, Dahlia
TI Differential RAID: Rethinking RAID for SSD Reliability
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Performance; Reliability; RAID; SSD; flash
AB SSDs exhibit very different failure characteristics compared to hard drives. In particular, the bit error rate (BER) of an SSD climbs as it receives more writes. As a result, RAID arrays composed from SSDs are subject to correlated failures. By balancing writes evenly across the array, RAID schemes can wear out devices at similar times. When a device in the array fails towards the end of its lifetime, the high BER of the remaining devices can result in data loss. We propose Diff-RAID, a parity-based redundancy solution that creates an age differential in an array of SSDs. Diff-RAID distributes parity blocks unevenly across the array, leveraging their higher update rate to age devices at different rates. To maintain this age differential when old devices are replaced by new ones, Diff-RAID reshuffles the parity distribution on each drive replacement. We evaluate Diff-RAID's reliability by using real BER data from 12 flash chips on a simulator and show that it is more reliable than RAID-5, in some cases by multiple orders of magnitude. We also evaluate Diff-RAID's performance using a software implementation on a 5-device array of 80 GB Intel X25-M SSDs and show that it offers a trade-off between throughput and reliability.
C1 [Balakrishnan, Mahesh; Prabhakaran, Vijayan; Malkhi, Dahlia] Microsoft Res Silicon Valley, Mountain View, CA 94043 USA.
   [Kadav, Asim] Univ Wisconsin Madison, Madison, WI USA.
C3 Microsoft; University of Wisconsin System; University of Wisconsin
   Madison
RP Balakrishnan, M (corresponding author), Microsoft Res Silicon Valley, 1065 La Avenida, Mountain View, CA 94043 USA.
EM maheshba@microsoft.com
CR [Anonymous], P 22 S OP SYST PRINC
   [Anonymous], INT X18 M X25 M SATA
   BITAR R, 2008, SUN820588110
   DESNOYERS P, 2009, P 1 WORKSH HOT TOP S
   Greenan K. M., 2009, P 5 WORKSH HOT TOP D
   GRUPP L. M., 2009, P ANN INT S MICR MIC
   Harris R., 2007, WHY RAID 5 STOPS WOR
   Hutsell W., IN DEPTH LOOK RAMSAN
   Kadav A., 2009, P 1 WORKSH HOT TOP S
   Matthews J., 2008, Trans. Storage, V4, P1
   Mielke N, 2008, INT RELIAB PHY SYM, P9, DOI 10.1109/RELPHY.2008.4558857
   Narayanan D, 2009, EUROSYS'09: PROCEEDINGS OF THE FOURTH EUROSYS CONFERENCE, P145
NR 12
TC 83
Z9 93
U1 1
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUL
PY 2010
VL 6
IS 2
AR 4
DI 10.1145/1807060.1807061
PG 22
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA V24QG
UT WOS:000208424300001
DA 2024-07-18
ER

PT J
AU Zhang, YM
   Li, HB
   Liu, SY
   Huang, P
AF Zhang, Yiming
   Li, Huiba
   Liu, Shengyun
   Huang, Peng
TI Hybrid Block Storage for Efficient Cloud Volume Service
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE SSD-HDD hybrid; block storage; cloud volume service
AB The migration of traditional desktop and server applications to the cloud brings challenge of high performance, high reliability, and low cost to the underlying cloud storage. To satisfy the requirement, this article proposes a hybrid cloud-scale block storage system called Ursa. Trace analysis shows that the I/O patterns served by block storage have only limited locality to exploit. Therefore, instead of using solid state drives (SSDs) as a cache layer, Ursa proposes hybrid storage structure that directly stores primary replicas on SSDs and replicates backup replicas on hard disk drives (HDDs). At the core of Ursa's hybrid storage design is an adaptive journal that can bridge the performance gap between primary SSDs and backup HDDs for random writes by transforming small backup writes into journal appends, which are then asynchronously replayed and merged to backup HDDs. To efficiently index the journal, we design a novel range-optimized merge-tree structure that combines a continuous range of keys into a single composite key {offset,length}. Ursa integrates the hybrid structurewith designs for high reliability, scalability, and availability. Experiments show that Ursa in its hybrid mode achieves almost the same performance as in its SSD-only mode (storing all replicas on SSDs), and outperforms other block stores (Ceph and Sheepdog) even in their SSD-only mode while achieving much higher CPU efficiency (IOPS and throughput per core).
C1 [Zhang, Yiming] Xiamen Univ, Zengcuoan Rd, Xiamen 361005, Fujian, Peoples R China.
   [Li, Huiba] Alibaba Ctr, Alibaba Grp, Beijing 100102, Peoples R China.
   [Liu, Shengyun] Shanghai Jiao Tong Univ, Dongchuan Rd, Shanghai 200240, Peoples R China.
   [Huang, Peng] Univ Michigan, Thompson St, Ann Arbor, MI 48109 USA.
C3 Xiamen University; Alibaba Group; Shanghai Jiao Tong University;
   University of Michigan System; University of Michigan
RP Zhang, YM (corresponding author), Xiamen Univ, Zengcuoan Rd, Xiamen 361005, Fujian, Peoples R China.; Liu, SY (corresponding author), Shanghai Jiao Tong Univ, Dongchuan Rd, Shanghai 200240, Peoples R China.
EM zhangyiming@xmu.edu.cn; huiba.lhb@alibaba-inc.com;
   shengyun.liu@sjtu.edu.cn; ryanph@umich.edu
RI Yan, Lu/KHW-7015-2024; yiming, zhang/IXN-8360-2023
OI yiming, zhang/0009-0001-9484-9149
FU National Key Research and Development Program of China [2022YFB4500302];
   OS Innovation Lab Project of Xiamen University [TC20220808044-2023-01]
FX This work is supported by the National Key Research and Development
   Program of China (grant no. 2022YFB4500302) and the OS Innovation Lab
   Project of Xiamen University and Huawei (grant no.
   TC20220808044-2023-01).
CR [Anonymous], 2010, SIGOPS Oper. Syst. Rev., DOI DOI 10.1145/1713254.1713276
   aws.amazon.com, About us
   Baker M. G., 1991, Operating Systems Review, V25, P198, DOI 10.1145/121133.121164
   blocksandfiles.com, About us
   Bolosky William J., 2011, P 8 S NETW SYST DES
   Burrows M, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P335
   ceph.com, About us
   Chan JeremyC. W., 2014, P 12 USENIX C FILE S, P163
   Chen F., 2011, Proc. of ACM Int. Conf. on Supercomputing, P22
   Chidambaram V., 2012, Proceedings of the 10th USENIX conference on File and Storage Technologies, FAST 2012, San Jose, CA, USA, February 14-17, 2012, P9
   Chidambaram V, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P228, DOI 10.1145/2517349.2522726
   Cully B., 2014, Proc. of the 12th USENIX Conf. on File and Storage Technol, P17
   Dean J, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE SIXTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDE '04), P137
   DeCandia Giuseppe, 2007, Operating Systems Review, V41, P205, DOI 10.1145/1323293.1294281
   en aviagen com, About us
   en.wikipedia.org, About us
   facebook.com, ABOUT US
   Ghemawat S., 2003, Operating Systems Review, V37, P29, DOI 10.1145/1165389.945450
   github.com, About us
   Gray C. G., 1989, Operating Systems Review, V23, P202, DOI 10.1145/74851.74870
   HERLIHY MP, 1990, ACM T PROGR LANG SYS, V12, P463, DOI 10.1145/78969.78972
   iotta.snia.org, About us
   Kaiyrakhmet O, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P191
   Kim Y., IEEE INT WORKSHOP MO
   Li C., 2014, USENIX ANN TECH C AT, P501
   Li HB, 2020, IEEE INFOCOM SER, P2165, DOI [10.1109/INFOCOM41043.2020.9155513, 10.1109/infocom41043.2020.9155513]
   Li HB, 2019, PROCEEDINGS OF THE FOURTEENTH EUROSYS CONFERENCE 2019 (EUROSYS '19), DOI 10.1145/3302424.3303967
   Li HB, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P581
   Lu LY, 2017, ACM T STORAGE, V13, DOI 10.1145/3033273
   Maneas S, 2020, PROCEEDINGS OF THE 18TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P137
   Mickens James., 2014, Proceedings of the 11th USENIX Conference on Networked Systems Design and Implementation. NSDI'14, P257
   Narayanan D, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P253
   Narayanan I, 2016, PROCEEDINGS OF THE 9TH ACM INTERNATIONAL SYSTEMS AND STORAGE CONFERENCE (SYSTOR'16), DOI [10.1145/2964791.2901489, 10.1145/2928275.2928278]
   Oki B. M., 1988, Proceedings of the Seventh Annual ACM Symposium on Principles of Distributed Computing, P8, DOI 10.1145/62546.62549
   Olson MA, 1999, PROCEEDINGS OF THE FREENIX TRACK, P183
   ONeil P, 1996, ACTA INFORM, V33, P351, DOI 10.1007/s002360050048
   Ongaro D., 2014, 2014 USENIX ANN TECH, P305
   Ongaro D, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P29
   open-cas.github.io, About us
   oss.oracle.com, About us
   pcgamer.com, About us
   Porto Daniel, 2015, P 10 EUR C COMP SYST, DOI [10.1145/2741948.2741979, DOI 10.1145/2741948.2741979]
   qcloud.com, About us
   Raju P, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P497, DOI 10.1145/3132747.3132765
   sheepdog.github.io, About us
   Shen YR, 2020, TWENTY-FIFTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXV), P955, DOI 10.1145/3373376.3378469
   Soundararajan Gokul., 2010, Proceedings of the 8th USENIX conference on File and storage technologies, FAST'10, P8
   Wang Y., 2012, Proceedings of the 2012 USENIX conference on Annual Technical Conference, P38
   Weil SA, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P307
   Xu E, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P961
   Yin Lujia, 2021, P USENIX ANN TECHN C
   Zaharia Matei, 2012, Proceedings of the 9th USENIX Conference on Networked Systems Design and Implementation, NSDI'12, P1, DOI DOI 10.1111/J.1095-8649.2005.00662.X
   Zhang YM, 2020, ACM T STORAGE, V16, DOI 10.1145/3365839
   Zhang YM, 2019, ACM T STORAGE, V15, DOI 10.1145/3289604
   Zhong WS, 2021, PROCEEDINGS OF THE 19TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '21), P51
NR 55
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2023
VL 19
IS 4
AR 35
DI 10.1145/3596446
PG 25
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X6NQ6
UT WOS:001099601100007
DA 2024-07-18
ER

PT J
AU Liao, XJ
   Lu, YY
   Yang, Z
   Shu, JW
AF Liao, Xiaojian
   Lu, Youyou
   Yang, Zhe
   Shu, Jiwu
TI Efficient Crash Consistency for NVMe over PCIe and RDMA
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Storage protocol; crash consistency; file system; SSD; NVMe
AB This article presents crash-consistent Non-Volatile Memory Express (ccNVMe), a novel extension of the NVMe that defines how host software communicates with the non-volatile memory (e.g., solid-state drive) across a PCI Express bus and RDMA-capable networks with both crash consistency and performance efficiency. Existing storage systems pay a huge tax on crash consistency, and thus cannot fully exploit the multi-queue parallelism and low latency of the NVMe and RDMA interfaces. ccNVMe alleviates this major bottleneck by coupling the crash consistency to the data dissemination. This new idea allows the storage system to achieve crash consistency by taking the free rides of the data dissemination mechanism of NVMe, using only two lightweight memory-mapped I/Os (MMIOs), unlike traditional systems that use complex update protocol and synchronized block I/Os. ccNVMe introduces a series of techniques including transaction-aware MMIO/doorbell and I/O command coalescing to reduce the PCIe traffic as well as to provide atomicity. We present how to build a high-performance and crash-consistent file system named MQFS atop ccNVMe. We experimentally show that MQFS increases the IOPS of RocksDB by 36% and 28% compared to a state-of-the-art file system and Ext4 without journaling, respectively.
C1 [Liao, Xiaojian; Lu, Youyou; Yang, Zhe; Shu, Jiwu] Tsinghua Univ, 30 Shuangqing Rd, Haidian Qu, Beijing Shi, Peoples R China.
C3 Tsinghua University
RP Liao, XJ (corresponding author), Tsinghua Univ, 30 Shuangqing Rd, Haidian Qu, Beijing Shi, Peoples R China.
EM liaoxiaojian@mail.tsinghua.edu.cn
OI Yang, Zhe/0000-0002-2031-5937; Shu, Jiwu/0000-0002-7362-2789; Lu,
   Youyou/0000-0002-6214-5390
FU National Natural Science Foundation of China [62022051, 61832011]
FX This work is funded by the National Natural Science Foundation of China
   (Grants No. 62022051 and No. 61832011).
CR Abulila A, 2019, TWENTY-FOURTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXIV), P971, DOI 10.1145/3297858.3304061
   [Anonymous], 2008, P 8 USENIX C OP SYST
   Axboe Jens, 2017, FIO FLEXIBLE IO TEST
   Bae DH, 2018, CONF PROC INT SYMP C, P425, DOI 10.1109/ISCA.2018.00043
   Best S, 2000, USENIX ASSOCIATION PROCEEDINGS OF THE 4TH ANNUAL LINUX SHOWCASE AND CONFERENCE, ATLANTA, P163
   Bhat SS, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P69, DOI 10.1145/3132747.3132779
   Chao C., 1997, Mime: a high performance parallel storage device with strong recovery guarantees
   Chidambaram V., 2012, Proceedings of the 10th USENIX conference on File and Storage Technologies, FAST 2012, San Jose, CA, USA, February 14-17, 2012, P9
   Chidambaram V, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P228, DOI 10.1145/2517349.2522726
   Facebook, 2022, PERS KEY VAL STOR FA
   Frost Christopher, 2007, Operating Systems Review, V41, P307, DOI 10.1145/1323293.1294291
   Guz Z, 2017, SYSTOR'17: PROCEEDINGS OF THE 10TH ACM INTERNATIONAL SYSTEMS AND STORAGE CONFERENCE, DOI 10.1145/3078468.3078483
   Hu YG, 2018, PROCEEDINGS OF THE 2018 USENIX ANNUAL TECHNICAL CONFERENCE, P879
   Huaicheng Li, 2020, ASPLOS '20: Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, P591, DOI 10.1145/3373376.3378531
   Hwang J, 2020, PROCEEDINGS OF THE 17TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P127
   Intel Corporation, 2022, INT OPT SSD DC P5800
   Kaashoek M., 2001, Logical Disk: A Simple New Approach to Improving File System Performance
   Kalia A, 2016, PROCEEDINGS OF USENIX ATC '16: 2016 USENIX ANNUAL TECHNICAL CONFERENCE, P437
   Kang Junbin, 2015, 2015 USENIX ANN TECH
   Kang Woon-Hak, 2013, SIGMOD 13, P97, DOI DOI 10.1145/2463676.2465326
   Kim D, 2022, PROCEEDINGS OF THE 20TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, FAST 2022, P329
   Kim J, 2021, PROCEEDINGS OF THE 2021 USENIX ANNUAL TECHNICAL CONFERENCE, P893
   Klimovic A, 2017, TWENTY-SECOND INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXII), P345, DOI 10.1145/3037697.3037732
   Klimovic A, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901337
   Kwon Y, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P460, DOI 10.1145/3132747.3132770
   Lee G, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P603
   Liao XJ, 2021, PROCEEDINGS OF THE 28TH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, SOSP 2021, P132, DOI 10.1145/3477132.3483592
   Liao XJ, 2021, PROCEEDINGS OF THE 2021 USENIX ANNUAL TECHNICAL CONFERENCE, P877
   Liao XJ, 2020, PROCEEDINGS OF THE 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '20), P549
   Linux kernel development community, 2022, EXT4 DAT STRUCT ALG
   Lu Lanyue, 2014, P USENIX S OP SYST D, P81
   Lu YY, 2019, ACM T STORAGE, V15, DOI 10.1145/3319369
   Lu YY, 2013, 2013 IEEE 31ST INTERNATIONAL CONFERENCE ON COMPUTER DESIGN (ICCD), P115, DOI 10.1109/ICCD.2013.6657033
   Lu Youyou., 2013, FAST, P257
   Lu Youyou, 2014, P USENIX C FIL STOR, P75
   Microsoft, 2022, WIND NTFS
   Min J, 2021, SIGCOMM '21: PROCEEDINGS OF THE 2021 ACM SIGCOMM 2021 CONFERENCE, P106, DOI 10.1145/3452296.3472940
   Mohan J, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P33
   Nightingale EB, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P1
   NVMe organization, 2022, NONV MEM EXPR
   NVMe organization, 2022, NVMEXPRESS MOV FUT
   NVMe organization, 2022, NVME TCP
   NVMe organization, 2022, NVME 14 SPEC REV 14C
   NVMe organization, 2022, NVME 12 SPEC
   NVMe organization, 2022, NVME SSD PERS MEM RE
   NVMe organization, 2022, NVME 14 SPEC
   Ouyang XY, 2011, INT S HIGH PERF COMP, P301, DOI 10.1109/HPCA.2011.5749738
   Park D, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P787
   PCI-SIG organization, 2022, PCI EXPRESS BASE SPE
   Pillai TS, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P181
   Porter DE, 2009, SOSP'09: PROCEEDINGS OF THE TWENTY-SECOND ACM SIGOPS SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P161
   Sears R, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P29
   Son Y, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P227
   Sweeney A, 1996, PROCEEDINGS OF THE USENIX 1996 ANNUAL TECHNICAL CONFERENCE, P1
   Tarasov Vasily, 2017, FILEBENCH MODEL BASE
   Won Y, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P211
   Yang ZH, 2020, IEEE INT CONF COMM, DOI 10.1109/iccworkshops49005.2020.9145189
   Zawadzki Tomasz, 2021, SPDK V2104 ZNS NVME
   Zhang JC, 2016, PROCEEDINGS OF USENIX ATC '16: 2016 USENIX ANNUAL TECHNICAL CONFERENCE, P87
   Zhang J, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P477
NR 60
TC 3
Z9 4
U1 0
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD FEB
PY 2023
VL 19
IS 1
AR 7
DI 10.1145/3568428
PG 35
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 9I5DE
UT WOS:000939530200007
OA Bronze
DA 2024-07-18
ER

PT J
AU Zhang, YM
   Wang, L
   Gai, S
   Ke, QW
   Li, WH
   Song, ZL
   Xue, GT
   Shu, JW
AF Zhang, Yiming
   Wang, Li
   Gai, Shun
   Ke, Qiwen
   Li, Wenhao
   Song, Zhenlong
   Xue, Guangtao
   Shu, Jiwu
TI Oasis: Controlling Data Migration in Expansion of Object-based Storage
   Systems
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Object storage; cluster expansion; datamigration; data placement; CRUSH;
   block storage system
ID PERFORMANCE; SERVICE; OVERLAY; CHORD
AB Object-based storage systems have been widely used for various scenarios such as file storage, block storage, blob (e.g., large videos) storage, and so on, where the data is placed among a large number of object storage devices (OSDs). Data placement is critical for the scalability of decentralized object-based storage systems. The state-of-the-art CRUSH placement method is a decentralized algorithm that deterministically places object replicas onto storage devices without relying on a central directory. While enjoying the benefits of decentralization such as high scalability, robustness, and performance, CRUSH-based storage systems suffer from uncontrolled data migration when expanding the capacity of the storage clusters (i.e., adding new OSDs), which is determined by the nature of CRUSH and will cause significant performance degradation when the expansion is nontrivial.
   This article presents MapX, a novel extension to CRUSH that uses an extra time-dimension mapping (from object creation times to cluster expansion times) for controlling data migration after cluster expansions. Each expansion is viewed as a new layer of the CRUSH map represented by a virtual node beneath the CRUSH root. MapX controls the mapping from objects onto layers by manipulating the timestamps of the intermediate placement groups (PGs). MapX is applicable to a large variety of object-based storage scenarios where object timestamps can be maintained as higher-level metadata. We have applied MapX to the state-of-the-art Ceph-RBD (RADOS Block Device) to implement a migration-controllable, decentralized object-based block store (called Oasis). Oasis extends the RBD metadata structure to maintain and retrieve approximate object creation times (for migration control) at the granularity of expansion layers. Experimental results show that the MapX-based Oasis block store outperforms the CRUSH-based Ceph-RBD (which is busy in migrating objects after expansions) by 3.17x similar to 4.31x in tail latency, and 76.3% (respectively, 83.8%) in IOPS for reads (respectively, writes).
C1 [Zhang, Yiming; Ke, Qiwen] Xiamen Univ, NICEX Lab, Xiamen, Peoples R China.
   [Wang, Li] Kylin Software Corp, Tianjin, Peoples R China.
   [Gai, Shun; Song, Zhenlong] Natl Univ Def Technol NUDT, Changsha, Peoples R China.
   [Li, Wenhao; Xue, Guangtao] Shanghai Jiao Tong Univ SJTU, Shanghai, Peoples R China.
   [Shu, Jiwu] Xiamen Univ, Xiamen, Peoples R China.
   [Shu, Jiwu] Tsinghua Univ, Beijing, Peoples R China.
C3 Xiamen University; National University of Defense Technology - China;
   Shanghai Jiao Tong University; Xiamen University; Tsinghua University
RP Zhang, YM (corresponding author), Xiamen Univ, NICEX Lab, Xiamen, Peoples R China.
EM zhangyiming@xmu.edu.cn; linux_liwang@163.com; gaishun97@outlook.com;
   kerneld@qq.com; fire1997ice@sjtu.edu.cn; songzhl@sina.com;
   xue-gt@cs.sjtu.edu.cn; shujw@tsinghua.edu.cn
RI yiming, zhang/IXN-8360-2023
OI yiming, zhang/0009-0001-9484-9149; Shu, Jiwu/0000-0002-7362-2789; ke, qi
   wen/0000-0003-3733-5456
FU National Key R&D Program of China [2022YFB4500302]; National Natural
   Science Foundation of China [NSFC 61872376, 61932001, 62072306]; Program
   of Shanghai Academic Research Leader [20XD1402100]
FX This work is supported by the National Key R&D Program of China
   (2022YFB4500302), and the National Natural Science Foundation of China
   (NSFC 61872376, 61932001, and 62072306), and Program of Shanghai
   Academic Research Leader (20XD1402100).
CR Aiken S, 2003, IEEE S MASS STOR SYS, P123, DOI 10.1109/MASS.2003.1194849
   Anand Ashok., 2010, NSDI, P433
   Andersen DG, 2009, SOSP'09: PROCEEDINGS OF THE TWENTY-SECOND ACM SIGOPS SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P1
   [Anonymous], 2012, P 9 USENIX C NETWORK
   [Anonymous], 2011, Airways (Pty) Ltd v Aviation Union of South Africa Others 2011 (3) SA 148 (SCA) paras 25-26, P25, DOI DOI 10.1145/1989323.1989327
   Antony I., 2001, LECT NOTES COMPUTER, P329, DOI DOI 10.1007/3-540-45518-3_18
   AWS Team, AMAZON S3
   Beaver D., 2010, P USENIX OSDI
   Bigian Armond, BLOBSTORE TWITTERS I
   Braam P, 2019, Arxiv, DOI arXiv:1903.01955
   Cashin L., 2005, LINUX J, V2005, P10
   Castro Miguel, 2005, USENIX S NETWORKED S
   Ceph Team, CRUSH TOOL
   Ceph Team, CEPH FIL SYST
   Ceph Team, CEPH OBJ GAT
   Ceph Team, CEPH BLOCK STOR
   Ceph Team, CEPH RADOS
   Chan JeremyC. W., 2014, P 12 USENIX C FILE S, P163
   Chang F, 2008, ACM T COMPUT SYST, V26, DOI 10.1145/1365815.1365816
   CHEN PM, 1994, ACM COMPUT SURV, V26, P145, DOI 10.1145/176979.176981
   Chidambaram V., 2012, FAST, P9
   Chidambaram V, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P228, DOI 10.1145/2517349.2522726
   Condit J, 2009, SOSP'09: PROCEEDINGS OF THE TWENTY-SECOND ACM SIGOPS SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P133
   Dean J, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE SIXTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDE '04), P137
   Debnath B, 2010, PROC VLDB ENDOW, V3, P1414, DOI 10.14778/1920841.1921015
   DeCandia Giuseppe, 2007, Operating Systems Review, V41, P205, DOI 10.1145/1323293.1294281
   Fio Team, FIO NAV
   Ganesan P, 2004, INT CON DISTR COMP S, P263
   Ghemawat S., 2003, Operating Systems Review, V37, P29, DOI 10.1145/1165389.945450
   Hadoop Team, HDFS ARCHITECTURE GU
   Harter Tyler., 2014, 12 USENIX C FILE STO, P199
   HARTMAN JH, 1995, ACM T COMPUT SYST, V13, P274, DOI 10.1145/210126.210131
   Harvey N., 2003, USENIX S INTERNET TE
   HERLIHY MP, 1990, ACM T PROGR LANG SYS, V12, P463, DOI 10.1145/78969.78972
   Hildebrand D, 2005, Twenty-Second IEEE/Thirteenth NASA Goddard Conference on Mass Storage Systems and Technologies, Proceedings, P18, DOI 10.1109/MSST.2005.14
   Hofmann Owen, 2012, USENIX OPERATING SYS
   Huang MQ, 2017, I COMP CONF WAVELET, P83, DOI 10.1109/ICCWAMTIP.2017.8301454
   Jin C, 2011, IEEE S MASS STOR SYS
   Kaashoek MF, 2003, LECT NOTES COMPUT SC, V2735, P98
   Karger DR, 2004, LECT NOTES COMPUT SC, V3279, P288
   Lakshman Avinash, 2010, Operating Systems Review, V44, P35, DOI 10.1145/1773912.1773922
   Lakshman Avinash, 2009, ACM SIGMOD INT C MAN
   Lee EK, 1996, ACM SIGPLAN NOTICES, V31, P84, DOI 10.1145/248209.237157
   LEUNG A. W., 2008, P USENIX ANN TECHN C
   Li HB, 2019, PROCEEDINGS OF THE FOURTEENTH EUROSYS CONFERENCE 2019 (EUROSYS '19), DOI 10.1145/3302424.3303967
   Li HB, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P581
   Lim H, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P1
   Lu LY, 2017, ACM T STORAGE, V13, DOI 10.1145/3033273
   MCKUSICK MK, 1984, ACM T COMPUT SYST, V2, P181, DOI 10.1145/989.990
   Meyer DT, 2008, EUROSYS'08: PROCEEDINGS OF THE EUROSYS 2008 CONFERENCE, P41, DOI 10.1145/1357010.1352598
   Mickens James., 2014, Proceedings of the 11th USENIX Conference on Networked Systems Design and Implementation. NSDI'14, P257
   MinIO Team, MULT OBJ STOR
   Mislove A, 2004, LECT NOTES COMPUT SC, V3279, P162
   Muralidhar S., 2014, P 11 USENIX C OP SYS, P383
   NiceX Lab, MAPX COD
   Noghabi SA, 2016, SIGMOD'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P253, DOI 10.1145/2882903.2903738
   Ongaro D, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P29
   Piernas J., 2002, Conference Proceedings of the 2002 International Conference on SUPERCOMPUTING, P137, DOI 10.1145/514191.514213
   Ratnasamy S, 2001, ACM SIGCOMM COMP COM, V31, P161, DOI 10.1145/964723.383072
   RedHat Team, BLUESTORE
   Ren K, 2014, INT CONF HIGH PERFOR, P237, DOI 10.1109/SC.2014.25
   RocksDB Team, PERS KEY VAL STOR FA
   Shen HY, 2006, PERFORM EVALUATION, V63, P195, DOI 10.1016/j.peva.2005.01.004
   Spirovska K, 2017, INT CON DISTR COMP S, P2626, DOI 10.1109/ICDCS.2017.192
   STODOLSKY D, 1993, CONF PROC INT SYMP C, P64, DOI 10.1145/173682.165143
   Stoica I, 2001, ACM SIGCOMM COMP COM, V31, P149, DOI 10.1145/964723.383071
   Swift Team, OPENSTACK SWIFT STOR
   Wang Y., 2013, 10 USENIX S NETW SYS
   WARFIELD A, 2005, P 10 USENIX WORKSH H
   Weil S. A., 2006, P IEEE ACM C SUP, P654
   Weil SA, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P307
   Zhang YM, 2020, ACM T STORAGE, V16, DOI 10.1145/3365839
   Zhang YM, 2019, ACM T STORAGE, V15, DOI 10.1145/3289604
   Zhang YM, 2017, IEEE INFOCOM SER
   Zhang YM, 2017, IEEE ACM T NETWORK, V25, P2040, DOI 10.1109/TNET.2017.2669215
   Zhang YM, 2012, IEEE T KNOWL DATA EN, V24, P1556, DOI 10.1109/TKDE.2011.258
   Zhang YM, 2010, IEEE J SEL AREA COMM, V28, P28, DOI 10.1109/JSAC.2010.100104
   Zhao BY, 2004, IEEE J SEL AREA COMM, V22, P41, DOI 10.1109/JSAC.2003.818784
NR 78
TC 2
Z9 2
U1 2
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD FEB
PY 2023
VL 19
IS 1
AR 2
DI 10.1145/3568424
PG 22
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 9I5DE
UT WOS:000939530200002
DA 2024-07-18
ER

PT J
AU Nicolaou, N
   Cadambe, V
   Prakash, N
   Trigeorgi, A
   Konwar, K
   Medard, M
   Lynch, N
AF Nicolaou, Nicolas
   Cadambe, Viveck
   Prakash, N.
   Trigeorgi, Andria
   Konwar, Kishori
   Medard, Muriel
   Lynch, Nancy
TI ARES: Adaptive, Reconfigurable, Erasure coded, Atomic Storage
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Atomicity; distributed storage; reconfiguration; fault-tolerance;
   erasure-codes
ID IMPLEMENTATIONS
AB Emulating a shared atomic, read/write storage system is a fundamental problem in distributed computing. Replicating atomic objects among a set of data hosts was the norm for traditional implementations (e.g., [11]) in order to guarantee the availability and accessibility of the data despite host failures. As replication is highly storage demanding, recent approaches suggested the use of erasure-codes to offer the same fault-tolerance while optimizing storage usage at the hosts. Initial works focused on a fixed set of data hosts. To guarantee longevity and scalability, a storage service should be able to dynamically mask hosts failures by allowing new hosts to join, and failed host to be removed without service interruptions. This work presents the first erasure-code -based atomic algorithm, called Ares, which allows the set of hosts to be modified in the course of an execution. Ares is composed of three main components: (i) a reconfiguration protocol, (ii) a read/write protocol, and (iii) a set of data access primitives (DAPs). The design of Ares is modular and is such to accommodate the usage of various erasure-code parameters on a per-configuration basis. We provide bounds on the latency of read/write operations and analyze the storage and communication costs of the Ares algorithm.
C1 [Nicolaou, Nicolas] Algolysis Ltd, Ellados 12A, CY-4630 Erimi, Limassol, Cyprus.
   [Cadambe, Viveck] Penn State Univ, University Pk, PA 16802 USA.
   [Prakash, N.] Intel Corp, 5200 NE Elam Young Pkwy, Hillsboro, OR 97124 USA.
   [Trigeorgi, Andria] Univ Cyprus, Kallipoleos 75, CY-1678 Nicosia, Cyprus.
   [Konwar, Kishori; Medard, Muriel; Lynch, Nancy] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
C3 Pennsylvania Commonwealth System of Higher Education (PCSHE);
   Pennsylvania State University; Pennsylvania State University -
   University Park; Intel Corporation; University of Cyprus; Massachusetts
   Institute of Technology (MIT)
RP Nicolaou, N (corresponding author), Algolysis Ltd, Ellados 12A, CY-4630 Erimi, Limassol, Cyprus.
EM nicolas@algolysis.com; vxc12@engr.psu.edu; prakashn@mit.edu;
   aatrigeO1@cs.ucy.ac.cy; kishori@csail.mit.edu; medard@mit.edu;
   lynch@csail.mit.edu
RI Trigeorgi, Andria/JBV-5793-2023
FU Center for Science of Information NSF [CCF-0939370]; NSF [CCF-1461559,
   CCF-1553248, RPF/POST-DOC/0916/0090]; AFOSR [FA9550-14-1-0403]
FX This work was partially funded by the Center for Science of Information
   NSF Award CCF-0939370, NSF Award CCF-1461559, AFOSR Contract Number:
   FA9550-14-1-0403, NSF CCF-1553248 and RPF/POST-DOC/0916/0090.
CR Abebe M, 2018, INT CON DISTR COMP S, P255, DOI 10.1109/ICDCS.2018.00034
   Aguilera MK, 2010, BULL EUR ASSOC THEOR, P84
   Aguilera MK, 2009, PODC'09: PROCEEDINGS OF THE 2009 ACM SYMPOSIUM ON PRINCIPLES OF DISTRIBUTED COMPUTING, P17, DOI 10.1145/1582716.1582726
   Ansible, 2022, OV HOW ANS WORKS
   Anta Antonio Fernandez, 2015, P INT C PRINCIPLES D
   Anta Antonio Fernandez, 2016, P INT C PRINCIPLES D
   ATTIYA H, 1995, J ASSOC COMPUT MACH, V42, P124, DOI 10.1145/200836.200869
   Burihabwa D, 2016, LECT NOTES COMPUT SC, V9687, P160, DOI 10.1007/978-3-319-39577-7_13
   Cachin C, 2006, I C DEPEND SYS NETWO, P115
   Cadambe VR, 2017, DISTRIB COMPUT, V30, P49, DOI 10.1007/s00446-016-0275-x
   Cadambe VR, 2014, 2014 IEEE 13TH INTERNATIONAL SYMPOSIUM ON NETWORK COMPUTING AND APPLICATIONS (NCA 2014), P253, DOI 10.1109/NCA.2014.44
   Chen YL, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P539
   Chentao Wu, 2012, 2012 41st International Conference on Parallel Processing (ICPP 2012), P460, DOI 10.1109/ICPP.2012.32
   Chockler G, 2005, DISTRIB COMPUT, V18, P73, DOI 10.1007/s00446-005-0123-x
   Chockler G, 2009, J PARALLEL DISTR COM, V69, P100, DOI 10.1016/j.jpdc.2008.07.007
   Dobre D, 2019, IEEE T PARALL DISTR, V30, P2547, DOI 10.1109/TPDS.2019.2919285
   DUTTA P., 2004, Proceedings of the 23rd ACM symposium on Principles of Distributed Computing (PODC), P236, DOI DOI 10.1145/1011767.1011802
   Dutta P, 2008, LECT NOTES COMPUT SC, V5218, P182, DOI 10.1007/978-3-540-87779-0_13
   emulab, 2022, EMULAB NETWORK TESTB
   Fan R, 2003, LECT NOTES COMPUT SC, V2848, P75
   Gafni E, 2015, LECT NOTES COMPUT SC, V9363, P140, DOI 10.1007/978-3-662-48653-5_10
   Georgiou C, 2008, LECT NOTES COMPUT SC, V5218, P289, DOI 10.1007/978-3-540-87779-0_20
   Georgiou C, 2009, J PARALLEL DISTR COM, V69, P62, DOI 10.1016/j.jpdc.2008.05.004
   Gilbert S, 2003, 2003 INTERNATIONAL CONFERENCE ON DEPENDABLE SYSTEMS AND NETWORKS, PROCEEDINGS, P259, DOI 10.1109/DSN.2003.1209936
   Gilbert Seth, 2003, THESIS MIT
   github, 2022, PYECLIB
   github, 2022, PYSYNCOBJ
   HERLIHY MP, 1990, ACM T PROGR LANG SYS, V12, P463, DOI 10.1145/78969.78972
   Huang JZ, 2015, IEEE T PARALL DISTR, V26, P1704, DOI 10.1109/TPDS.2014.2326156
   Intel Storage Acceleration Library, 2022, ZKVL4N
   Jehl L, 2015, LECT NOTES COMPUT SC, V9363, P154, DOI 10.1007/978-3-662-48653-5_11
   Joshi G, 2017, ACM TRANS MODELING P, V2, DOI 10.1145/3055281
   Konwar KM, 2016, INT PARALL DISTRIB P, P720, DOI 10.1109/IPDPS.2016.55
   Konwar Kishori M., 2016, P INT C DISTRIBUTED
   Lakshman Avinash, 2010, Operating Systems Review, V44, P35, DOI 10.1145/1773912.1773922
   Lamport L, 1998, ACM T COMPUT SYST, V16, P133, DOI 10.1145/279227.279229
   Lynch N, 2002, LECT NOTES COMPUT SC, V2508, P173
   Lynch N. A., 1996, DISTRIBUTED ALGORITH
   Lynch NA, 1997, DIG PAP INT SYMP FAU, P272, DOI 10.1109/FTCS.1997.614100
   Michael Ellis, 2017, LEIBNIZ INT P INFORM, V91, DOI [10.4230/LIPIcs.DISC.2017.36, DOI 10.4230/LIPICS.DISC.2017.36]
   Nakamoto S., 2008, DECENT BUS REV, V21260, DOI https://bitcoin.org/bitcoin.pdf
   Nicolaou N, 2021, Arxiv, DOI arXiv:1805.03727
   Nicolaou N, 2019, INT CON DISTR COMP S, P2195, DOI 10.1109/ICDCS.2019.00216
   Ongaro Diego, 2014, 2014 USENIX ANN TECH, P305, DOI DOI 10.1007/0-387-34805-0_21
   Pless V., 2003, FUNDAMENTALS ERROR C
   Rashmi KV, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P401
   Shraer Alexander., 2010, Proceedings of the 4th International Workshop on Large Scale Distributed Systems and Middleware, P22
   Spiegelman Alexander, 2017, P 31 INT S DISTRIBUT
   WANG S., 2017, NUMER ALGORITHMS, P1
   Wu CT, 2013, PROC INT CONF PARAL, P542, DOI 10.1109/ICPP.2013.68
   Xiang Y, 2016, IEEE ACM T NETWORK, V24, P2443, DOI 10.1109/TNET.2015.2466453
   Xiang Y, 2015, INT CON DISTR COMP S, P790, DOI 10.1109/ICDCS.2015.111
   Xiaoyang Zhang, 2018, IEEE INFOCOM 2018 - IEEE Conference on Computer Communications, P1808, DOI 10.1109/INFOCOM.2018.8485961
   Yinghao Yu, 2018, SC18: International Conference for High Performance Computing, Networking, Storage and Analysis. Proceedings, P1, DOI 10.1109/SC.2018.00004
   Zeromq, 2022, US
   Zhang GY, 2015, IEEE T COMPUT, V64, P32, DOI 10.1109/TC.2013.210
   Zhang H, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P167
   Zhou PP, 2019, IEEE T COMPUT, V68, P556, DOI 10.1109/TC.2018.2876827
NR 58
TC 0
Z9 0
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2022
VL 18
IS 4
AR 33
DI 10.1145/3510613
PG 39
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7F8OU
UT WOS:000902100700006
DA 2024-07-18
ER

PT J
AU Yang, Y
   Cao, Q
   Yao, J
   Jiang, H
   Yang, L
AF Yang, Yang
   Cao, Qiang
   Yao, Jie
   Jiang, Hong
   Yang, Li
TI Batch-file Operations to Optimize Massive Files Accessing: Analysis,
   Design, and Application
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Batch-file operations; two-phase access; layout-aware scheduler
AB Existing local file systems, designed to support a typical single-file access mode only, can lead to poor performance when accessing a batch of files, especially small files. This single-file mode essentially serializes accesses to batched files one by one, resulting in a large number of non-sequential, random, and often dependent I/Os between file data and metadata at the storage ends. Such access mode can further worsen the efficiency and performance of applications accessing massive files, such as data migration. We first experimentally analyze the root cause of such inefficiency in batch-file accesses. Then, we propose a novel batch-file access approach, referred to as BFO for its set of optimized Batch-File Operations, by developing novel BFOr and BFOw operations for fundamental read and write processes, respectively, using a two-phase access for metadata and data jointly. The BFO offers dedicated interfaces for batch-file accesses and additional processes integrated into existing file systems without modifying their structures and procedures. In addition, based on BFOr and BFOw, we also propose the novel batch-file migration BFOm to accelerate the data migration for massive small files. We implement a BFO prototype on ext4, one of the most popular file systems. Our evaluation results show that the batch-file read and write performances of BFO are consistently higher than those of the traditional approaches regardless of access patterns, data layouts, and storage media, under synthetic and real-world file sets. BFO improves the read performance by up to 22.4x and 1.8x with HDD and SSD, respectively, and it boosts the write performance by up to I 11.4x and 2.9x with HDD and SSD, respectively. BFO also demonstrates consistent performance advantages for data migration in both local and remote situations.
C1 [Yang, Yang; Cao, Qiang; Yao, Jie; Yang, Li] Huazhong Univ Sci & Technol, Minist Educ China, Key Lab Informat Storage Syst, Wuhan Natl Lab Optoelect, Luoyu Rd 1037, Wuhan, Hubei, Peoples R China.
   [Yang, Yang; Cao, Qiang; Yao, Jie; Yang, Li] Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Luoyu Rd 1037, Wuhan, Hubei, Peoples R China.
   [Jiang, Hong] Univ Texas Arlington, Dept Comp Sci & Engn, 500 UTA Blvd, Arlington, TX 76019 USA.
C3 Huazhong University of Science & Technology; Huazhong University of
   Science & Technology; University of Texas System; University of Texas
   Arlington
RP Cao, Q (corresponding author), Huazhong Univ Sci & Technol, Minist Educ China, Key Lab Informat Storage Syst, Wuhan Natl Lab Optoelect, Luoyu Rd 1037, Wuhan, Hubei, Peoples R China.; Cao, Q (corresponding author), Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Luoyu Rd 1037, Wuhan, Hubei, Peoples R China.
EM yangyang_hust@hust.edu.cn; caoqiang@hust.edu.cn; jackyao@hust.edu.cn;
   hong.jiang@uta.edu; yl_for_work@foxmail.com
FU Creative Research Group Project of NSFC [61821003]; National key
   research and development program of China [2018YFA0701804,
   2018YFA0701805]; NSFC [61872156]; U.S. NSF [CCF-1704504, CCF-1629625];
   Alibaba Group through Alibaba Innovative Research (AIR) Program
FX This work is supported in part by Creative Research Group Project of
   NSFC Grant No. 61821003, National key research and development program
   of China (Grants No. 2018YFA0701804 and No. 2018YFA0701805), NSFC Grant
   No. 61872156, the U.S. NSF under Grant No. CCF-1704504 and No.
   CCF-1629625, and Alibaba Group through Alibaba Innovative Research (AIR)
   Program.
CR Allcock W., 2005, ACM/IEEE SC 2005 Conference (SC05), P54, DOI [DOI 10.1109/SC.2005.72, 10.1109/SC.2005.72]
   Andersen MP, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P39
   [Anonymous], 2017, CHIN BELT ROAD IN
   Beaver D., 2010, P USENIX OSDI
   Binfer, 2018, HIGH SPEED FILE TRAN
   Chidambaram V., 2012, FAST, P9
   Ding XN, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 2007 USENIX ANNUAL TECHNICAL CONFERENCE, P261
   Esmet J., 2012, P 4 USENIX WORKSH HO
   Fan TW, 2018, OPT LETT, V43, P1, DOI 10.1364/OL.43.000001
   Fu S, 2015, INT J ANTENN PROPAG, V2015, P12
   Gu YH, 2007, COMPUT NETW, V51, P1777, DOI 10.1016/j.comnet.2006.11.009
   Hanushevsky Andrew, 2018, BBCP
   Hsu Windsor, 2012, P 10 USENIX C FILE S, P5
   Jannen W., 2015, P 13 USENIX C FIL ST, P301, DOI DOI 10.5555/2750482.2750505
   Joo Y, 2017, ACM T STORAGE, V13, DOI 10.1145/3024094
   Kelly T, 2003, ACM SIGCOMM COMP COM, V33, P83, DOI 10.1145/956981.956989
   Kim S, 2015, ELEC DES ADV PACKAG, P67, DOI 10.1109/EDAPS.2015.7383669
   Lee Changman, 2015, 13 USENIX C FILE STO, P273
   Li T, 2017, IEEE T PARALL DISTR, V28, P1430, DOI 10.1109/TPDS.2016.2619344
   Liu Y, 2014, PROC VLDB ENDOW, V7, P1496, DOI 10.14778/2733004.2733021
   Lu LY, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P133
   Mckusick M. K., 2007, FSCK UNIX FILE SYSTE
   Nexor, 2018, SEC EFF MAN REL FIL
   Pillai TS, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P181
   Ren Kai., 2013, USENIX ANN TECHNICAL, P145
   Rodeh O, 2013, ACM T STORAGE, V9, DOI 10.1145/2501620.2501623
   Sangtae Ha, 2008, Operating Systems Review, V42, P64, DOI 10.1145/1400097.1400105
   Settlemyer BW, 2011, IEEE S MASS STOR SYS
   Sweeney A, 1996, PROCEEDINGS OF THE USENIX 1996 ANNUAL TECHNICAL CONFERENCE, P1
   Tweedie Stephen, 2000, P OTT LIN S
   Yan WR, 2017, PROCEEDINGS OF THE TWELFTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS (EUROSYS 2017), P161, DOI 10.1145/3064176.3064207
   Yang WD, 2019, ACM T KNOWL DISCOV D, V13, DOI 10.1145/3363575
   Yu WK, 2007, CCGRID 2007: SEVENTH IEEE INTERNATIONAL SYMPOSIUM ON CLUSTER COMPUTING AND THE GRID, P267
   Yuan J, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P1
   Zhang SL, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P15
NR 35
TC 0
Z9 0
U1 1
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2020
VL 16
IS 3
AR 19
DI 10.1145/3394286
PG 25
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1QZ
UT WOS:000583743600005
DA 2024-07-18
ER

PT J
AU Lee, M
   Kang, DH
   Eom, YI
AF Lee, Minho
   Kang, Dong Hyun
   Eom, Young Ik
TI M-CLOCK: Migration-optimized Page Replacement Algorithm for Hybrid
   Memory Architecture
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Phase change memory; hybrid memory architecture; page replacement
   algorithm
ID MAIN MEMORY
AB Phase Change Memory (PCM) has drawn great attention as a main memory due to its attractive characteristics such as non-volatility, byte-addressability, and in-place update. However, since the capacity of PCM is not fully mature yet, hybrid memory architecture that consists of DRAM and PCM has been suggested as a main memory. In addition, page replacement algorithm based on hybrid memory architecture is actively being studied, because existing page replacement algorithms cannot be used on hybrid memory architecture in that they do not consider the two weaknesses of PCM: high write latency and low endurance. In this article, to mitigate the above hardware limitations of PCM, we revisit the page cache layer for the hybrid memory architecture and propose a novel page replacement algorithm, called M-CLOCK, to improve the performance of hybrid memory architecture and the lifespan of PCM. In particular, M-CLOCK aims to reduce the number of PCM writes that negatively affect the performance of hybrid memory architecture. Experimental results clearly show that M-CLOCK outperforms the state-of-the-art page replacement algorithms in terms of the number of PCM writes and effective memory access time by up to 98% and 9.4 times, respectively.
C1 [Lee, Minho; Kang, Dong Hyun] Sungkyunkwan Univ, Dept Elect & Comp Engn, Seoul, South Korea.
   [Eom, Young Ik] Sungkyunkwan Univ, Coll Software, Seoul, South Korea.
C3 Sungkyunkwan University (SKKU); Sungkyunkwan University (SKKU)
RP Eom, YI (corresponding author), Sungkyunkwan Univ, Coll Software, Seoul, South Korea.
EM minhozx@skku.edu; dhkangd@skku.edu; yieom@skku.edu
RI Dong Hyun, Kang/HZK-3801-2023; Kang, Donghyun/S-7850-2018
OI Dong Hyun, Kang/0000-0002-6559-0369; Kang, Donghyun/0000-0003-4362-9944
FU Basic Science Research Program through the National Research Foundation
   of Korea (NRF) - Ministry of Science and ICT [NRF-2017R1A2B3004660];
   Next-Generation Information Computing Development Program through the
   National Research Foundation of Korea (NRF) - Ministry of Science, ICT
   [NRF-2015M3C4A7065696]
FX This research was supported by Basic Science Research Program through
   the National Research Foundation of Korea (NRF) funded by the Ministry
   of Science and ICT (No. NRF-2017R1A2B3004660) and Next-Generation
   Information Computing Development Program through the National Research
   Foundation of Korea (NRF) funded by the Ministry of Science, ICT (No.
   NRF-2015M3C4A7065696).
CR [Anonymous], 2013, INT J DISTRIBUTED SE
   [Anonymous], 2015, PROC IEEE NON VOLATI, DOI DOI 10.1109/NVMSA.2015.7304363
   [Anonymous], 2018, ACM T STORAGE, V14
   Chen XY, 2016, PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON BIOMEDICAL AND BIOLOGICAL ENGINEERING, P1
   Dhiman G, 2009, DES AUT CON, P664
   Dulloor SubramanyaR., 2016, P 11 EUROPEAN C COMP, P1
   Eilert Sean, 2009, P IEEE INT MEM WORKS, P1, DOI DOI 10.1109/IMW.2009.5090604
   Ferreira AP, 2010, DES AUT TEST EUROPE, P914
   Hoeju Chung, 2011, 2011 IEEE International Solid-State Circuits Conference (ISSCC 2011), P500, DOI 10.1109/ISSCC.2011.5746415
   Jiang S, 2005, USENIX Association Proceedings of the General Track: 2005 UNENIX Annual Technical Conference, P323
   Kang DH, 2016, IEEE T CONSUM ELECTR, V62, P136, DOI 10.1109/TCE.2016.7514672
   Kim D, 2012, DES AUT CON, P888
   Kim HJ, 2014, ACM T STORAGE, V10, DOI 10.1145/2668128
   Lee BC, 2010, IEEE MICRO, V30, P131, DOI 10.1109/MM.2010.24
   Lee T, 2014, P IEEE RAP SYST PROT, P115, DOI 10.1109/RSP.2014.6966901
   Mogul J.C., 2009, Conference on Hot topics in operating systems, P1
   Nethercote Nicholas., 2003, ELECTRON NOTES THEOR, V89, P2
   Qureshi MK, 2009, CONF PROC INT SYMP C, P24, DOI 10.1145/1555815.1555760
   Ramos Luiz E., 2012, P INT C SUP ICS 12, P95
   Seok H, 2011, APPL COMPUT REV, V11, P38, DOI 10.1145/2107756.2107760
   Tanenbaum AndrewS., 1987, OPERATING SYSTEMS DE
   Wikipedia, 2014, MEM REFR
   Yoon H, 2012, PR IEEE COMP DESIGN, P337, DOI 10.1109/ICCD.2012.6378661
   Zhong K, 2014, BMC CANCER, V14, DOI 10.1186/1471-2407-14-703
   Zhou P, 2009, CONF PROC INT SYMP C, P14, DOI 10.1145/1555815.1555759
NR 25
TC 3
Z9 3
U1 1
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2018
VL 14
IS 3
AR 25
DI 10.1145/3216730
PG 17
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ4JQ
UT WOS:000457140700006
DA 2024-07-18
ER

PT J
AU Hall, RJ
AF Hall, Robert J.
TI Tools for Predicting the Reliability of Large-Scale Storage Systems
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Tools; large scale; storage systems
AB Data-intensive applications require extreme scaling of their underlying storage systems. Such scaling, together with the fact that storage systems must be implemented in actual data centers, increases the risk of data loss from failures of underlying components. Accurate engineering requires quantitatively predicting reliability, but this remains challenging due to the need to account for extreme scale, redundancy scheme type and strength, distribution architecture, and component dependencies. This article introduces CQSIM-R, a tool suite for predicting the reliability of large-scale storage system designs and deployments. CQSIM-R includes (a) direct calculations based on an only-drives-fail failure model and (b) an event-based simulator for detailed prediction that handles failures of and failure dependencies among arbitrary (drive or nondrive) components. These are based on a common combinatorial framework for modeling placement strategies. The article demonstrates CQSIM-R using models of common storage systems, including replicated and erasure coded designs. New results, such as the poor reliability scaling of spread-placed systems and a quantification of the impact of data center distribution and rack-awareness on reliability, demonstrate the usefulness and generality of the tools. Analysis and empirical studies show the tools' soundness, performance, and scalability.
C1 [Hall, Robert J.] AT&T Labs Res, 1 AT&T Way, Bedminster, NJ 07921 USA.
C3 AT&T
RP Hall, RJ (corresponding author), AT&T Labs Res, 1 AT&T Way, Bedminster, NJ 07921 USA.
EM hall@research.att.com
CR ANGUS JE, 1988, IEEE T RELIAB, V37, P312, DOI 10.1109/24.3761
   [Anonymous], P IEEE IFIP INT C DE
   [Anonymous], P 2 USENIX C HOT TOP
   [Anonymous], P 5 USENIX C FIL STO
   Bairavasundaram Lakshmi N., 2007, P 2007 ACM INT C MEA
   Borthakur Dhruba, 2007, TECHNICAL REPORT
   CERN-LHC, 2016, COMPUTING
   CHEN PM, 1994, ACM COMPUT SURV, V26, P145, DOI 10.1145/176979.176981
   Cidon Asaf, 2013, P 2013 US ANN TECHN
   Elerath JG, 2014, ACM T STORAGE, V10, DOI 10.1145/2577386
   Ford Daniel, 2010, P 9 US S OP SYST DES
   Guerriero V., 2012, J MODERN MATH FRONTI, V1, P21
   Iliadis I, 2015, ACM T STORAGE, V11, DOI 10.1145/2700311
   Kao H.-W., 2013, P IEEE 29 S MASS STO
   Ovsiannikov M, 2013, PROC VLDB ENDOW, V6, P1092, DOI 10.14778/2536222.2536234
   Patiejunas Kestutis, 2014, P WORKSH DES STOR AR
   Rao K. K., 2006, P INT C DEP SYST NET
   Resch Jason, 2013, ARXIV13104702
   SCHROEDER B, 2007, P 5 US C FIL STOR TE
   Schroeder B, 2010, ACM T STORAGE, V6, DOI 10.1145/1837915.1837917
   Storer MW, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P1, DOI 10.1145/1456469.1456471
   Venkatesan V, 2012, P 9 INT C QUAN EV SY
   Venkatesan Vinodh, 2011, P 19 INT S MOD AN SI
   Venkatesan Vinodh, 2012, P 20 INT S MOD AN SI
   Weil S.A., 2006, P 7 USENIX S OP SYST
   Weil S.A., 2006, P 2006 ACM IEEE C SU
   Xin Qin, 2003, P 20 IEEE C MASS STO
NR 27
TC 7
Z9 10
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2016
VL 12
IS 4
AR 24
DI 10.1145/2911987
PG 30
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DV2OQ
UT WOS:000382760500007
DA 2024-07-18
ER

PT J
AU Ma, A
   Traylor, R
   Douglis, F
   Chamness, M
   Lu, GL
   Sawyer, D
   Chandra, S
   Hsu, W
AF Ma, Ao
   Traylor, Rachel
   Douglis, Fred
   Chamness, Mark
   Lu, Guanlin
   Sawyer, Darren
   Chandra, Surendar
   Hsu, Windsor
TI RAIDShield: Characterizing, Monitoring, and Proactively Protecting
   Against Disk Failures
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article; Proceedings Paper
CT 13th USENIX Conference on File and Storage Technologies (FAST)
CY FEB 16-19, 2015
CL Santa Clara, CA
SP USENIX, ACM SIGOPS
DE Reliability; Experimentation; Magnetic disks; RAID; reliability; disk
   errors
AB Modern storage systems orchestrate a group of disks to achieve their performance and reliability goals. Even though such systems are designed to withstand the failure of individual disks, failure of multiple disks poses a unique set of challenges. We empirically investigate disk failure data from a large number of production systems, specifically focusing on the impact of disk failures on RAID storage systems. Our data covers about one million SATA disks from six disk models for periods up to 5 years. We show how observed disk failures weaken the protection provided by RAID. The count of reallocated sectors correlates strongly with impending failures.
   With these findings we designed RAIDSHIELD, which consists of two components. First, we have built and evaluated an active defense mechanism that monitors the health of each disk and replaces those that are predicted to fail imminently. This proactive protection has been incorporated into our product and is observed to eliminate 88% of triple disk errors, which are 80% of all RAID failures. Second, we have designed and simulated a method of using the joint failure probability to quantify and predict how likely a RAID group is to face multiple simultaneous disk failures, which can identify disks that collectively represent a risk of failure even when no individual disk is flagged in isolation. We find in simulation that RAID-level analysis can effectively identify most vulnerable RAID-6 systems, improving the coverage to 98% of triple errors.
   We conclude with discussions of operational considerations in deploying RAIDSHIELD more broadly and new directions in the analysis of disk errors. One interesting approach is to combine multiple metrics, allowing the values of different indicators to be used for predictions. Using newer field data that reports an additional metric, medium errors, we find that the relative efficacy of reallocated sectors and medium errors varies across disk models, offering an additional way to predict failures.
C1 [Ma, Ao; Traylor, Rachel; Douglis, Fred; Chamness, Mark; Lu, Guanlin; Sawyer, Darren] EMC Corp, Beijing, Peoples R China.
   [Traylor, Rachel] Univ Texas Arlington, Arlington, TX USA.
   [Chandra, Surendar; Hsu, Windsor] Datrium Inc, Sunnyvale, CA USA.
C3 Dell Incorporated; Dell EMC; Dell EMC China; University of Texas System;
   University of Texas Arlington
RP Ma, A (corresponding author), EMC DataDomain, 2421 Mission Coll Blvd, Santa Clara, CA 95054 USA.
EM ao.ma@emc.com; Rachel.Traylor@emc.com; fred.douglis@emc.com;
   mark.chamness@emc.com; guanlin.lu@emc.com; Darren.Sawyer@emc.com;
   surendar@acm.org; windsor.hsu@gmail.com
CR Allen B., 2004, Linux Journal, V2004, P9
   Alvarez GA, 1997, ACM COMP AR, P62, DOI 10.1145/384286.264132
   Amer A, 2014, INT CONF COMPUT NETW, P907, DOI 10.1109/ICCNC.2014.6785458
   Anderson D, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE 2ND USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST'03), P245
   [Anonymous], 1988, A case for redundant arrays of inexpensive disks RAID, DOI DOI 10.1145/50202.50214
   [Anonymous], 2010, P 2 USENIX C HOT TOP
   Arpaci-Dusseau RH, 2001, EIGHTH WORKSHOP ON HOT TOPICS IN OPERATING SYSTEMS, PROCEEDINGS, P33, DOI 10.1109/HOTOS.2001.990058
   Bairavasundaram LN, 2007, PERF E R SI, V35, P289
   Bairavasundaram Lakshmi N., 2008, P 6 USENIX C FIL STO
   Bartlett W, 2004, IEEE T DEPEND SECURE, V1, P87, DOI 10.1109/TDSC.2004.4
   Bodík P, 2010, EUROSYS'10: PROCEEDINGS OF THE EUROSYS 2010 CONFERENCE, P111
   Bonwick Jeff, 2008, SNIA SOFTW DEV C
   CHEN PM, 1994, ACM COMPUT SURV, V26, P145, DOI 10.1145/176979.176981
   Corbett Peter, 2004, P 3 USENIX C FIL STO, P14
   Dholakia Ajay, 2008, ACM Transaction on Storage, V4, DOI 10.1145/1353452.1353453
   Dubnicki C., 2009, P 7 USENIX C FIL STO
   Elerath J, 2009, COMMUN ACM, V52, P38, DOI 10.1145/1516046.1516059
   Elerath JG, 2014, ACM T STORAGE, V10, DOI 10.1145/2577386
   Ghemawat S., 2003, ACM SIGOPS OPERATING, V37, P29, DOI [DOI 10.1145/945445.945450, DOI 10.1145/1165389.945450, 10.1145/1165389.945450]
   GIBSON GA, 1992, THESIS
   Goel Atul, 2012, Operating Systems Review, V46, P41
   Goldszmidt Moises, 2012, USENIX HOTSTORAGE 12
   HAFNER JL, 2005, P 4 C USENIX C FIL S
   Hamerly G, 2001, P 18 INT C MACH LEAR, P202, DOI DOI 10.5555/645530.655825
   Huang Cheng, 2012, P USENIX ANN TECHN C
   Hughes GF, 2002, IEEE T RELIAB, V51, P350, DOI 10.1109/TR.2002.802886
   Jain Navendu, 2005, P 4 C USENIX C FIL S
   Kari H. H., 1993, Proceedings. The IEEE International Workshop on Defect and Fault Tolerance in VLSI Systems (Cat. No.93TH0571-0), P49, DOI 10.1109/DFTVS.1993.595628
   Kari H. H., 1997, THESIS
   Khan Osama, 2012, P 10 USENIX C FIL ST
   Krioukov Andrew, 2008, P 6 USENIX C FIL STO
   Li M., 2009, ACM T STORAGE, V4, P1
   Ma Ao, 2015, 13 USENIX C FIL STOR, P241
   Mellor C., 2014, KRYDERS LAW CRAPS OU
   Murray J.F., 2005, Journal of Machine Learning research, P816
   Murray Joseph F., 2003, ICANN ICONIP, P4
   Paris Jehan-Francois, 2010, Proceedings of the 2010 IEEE International Conference on Networking, Architecture, and Storage (NAS 2010), P119, DOI 10.1109/NAS.2010.37
   Patterson Hugo, 2002, P FAST 2002 C FIL ST, P1
   Pinheiro E, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 5TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES ( FAST '07), P17
   Plank J. S., 2013, P 11 USENIX C FIL ST, P95
   Plank JS, 2014, ACM T STORAGE, V10, DOI 10.1145/2560013
   Prabhakaran Vijayan, 2005, P OF THE 20 ACM S ON, V39, P206
   Schroeder B, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 5TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES ( FAST '07), P1
   Schwarz TJE, 2004, IEEE COMPUTER SOCIETY'S 12TH ANNUAL INTERNATIONAL SYMPOSIUM ON MODELING, ANALYSIS, AND SIMULATION OF COMPUTER AND TELECOMMUNICATIONS SYSTEMS - PROCEEDINGS, P409, DOI 10.1109/MASCOT.2004.1348296
   Shvachko K., 2010, 2010 IEEE 26th Symposium on Mass Storage Systems and Technologies (MSST), P1
   TALAGALA N., 1999, P IEEE WORKSH FAULT
   Wallace Grant, 2012, P 10 C FIL STOR TECH
   Walter C, 2005, SCI AM, V293, P32, DOI 10.1038/scientificamerican0805-32
NR 48
TC 53
Z9 63
U1 1
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2015
VL 11
IS 4
SI SI
AR 17
DI 10.1145/2820615
PG 28
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DB0GY
UT WOS:000368187800003
DA 2024-07-18
ER

PT J
AU Kwon, SJ
   Cho, HJ
   Chung, TS
AF Kwon, Se Jin
   Cho, Hyung-Ju
   Chung, Tae-Sun
TI Hybrid Associative Flash Translation Layer for the Performance
   Optimization of Chip-Level Parallel Flash Memory
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Performance; Flash memory; solid state drive; flash translation
   layer; endurance; write amplification
ID RELIABILITY; STRATEGY
AB Flash memory is used widely in the data storage market, particularly low-price MultiLevel Cell (MLC) flash memory, which has been adopted by large-scale storage systems despite its low performance. To overcome the poor performance of MLC flash memory, a system architecture has been designed to optimize chip-level parallelism. This design increases the size of the page unit and the block unit, thereby simultaneously executing operations on multiple chips. Unfortunately, its Flash Translation Layer (FTL) generates many unused sectors in each page, which leads to unnecessary write operations. Furthermore, it reuses an earlier log block scheme, although it generates many erase operations because of its low space utilization. To solve these problems, we propose a hybrid associative FTL (Hybrid-FTL) to enhance the performance of the chip-level parallel flash memory system. Hybrid-FTL reduces the number of write operations by utilizing all of the unused sectors. Furthermore, it reduces the overall number of erase operations by classifying data as hot, cold, or fragment data. Hybrid-FTL requires less mapping information in the DRAM and in the flash memory compared with previous FTL algorithms.
C1 [Kwon, Se Jin; Cho, Hyung-Ju; Chung, Tae-Sun] Ajou Univ, Dept Informat & Comp Engn, Suwon 443749, South Korea.
C3 Ajou University
RP Kwon, SJ (corresponding author), Ajou Univ, Dept Informat & Comp Engn, Suwon 443749, South Korea.
EM sejin1109@ajou.ac.kr
OI Kwon, Se Jin/0000-0002-6295-7014
FU Basic Science Research Program through National Research Foundation of
   Korea (NRF); Ministry of Education, Science and Technology
   [NRF-2012R1A1A20434422, NRF-2013R1A1A2A10012956, NRF-2013R1A1A2061390]
FX This research was supported by Basic Science Research Program through
   the National Research Foundation of Korea (NRF) funded by the Ministry
   of Education, Science and Technology (NRF-2012R1A1A20434422,
   NRF-2013R1A1A2A10012956, NRF-2013R1A1A2061390).
CR [Anonymous], 2011, ACM 11 P INT C SUPER
   Caulfield AM, 2009, ACM SIGPLAN NOTICES, V44, P217, DOI 10.1145/1508284.1508270
   Chang LP, 2010, IEEE T COMPUT, V59, P1337, DOI 10.1109/TC.2010.14
   Chang YH, 2011, IEEE T COMPUT, V60, P305, DOI 10.1109/TC.2010.126
   Cho H, 2009, DES AUT TEST EUROPE, P507
   Chu YS, 2009, DES AUT TEST EUROPE, P405
   Chung TS, 2009, J SYST ARCHITECT, V55, P332, DOI 10.1016/j.sysarc.2009.03.005
   Electronics Micron, 2012, TECHN NOT NAND FLASH
   Electronics Micron, 2007, TECHN NOT NAND FLASH
   Electronics Micron, 2012, DAT DDR3 SDRAM
   Electronics Samsung, 2012, PAG PROGR ADDR MLC N
   Electronics Samsung, 2012, NAND FLASH MEM K9FLG
   Electronics Samsung, 2012, NAND FLASH MEM K9GAG
   Gal E, 2005, ACM COMPUT SURV, V37, P138, DOI 10.1145/1089733.1089735
   Gupta A, 2009, ACM SIGPLAN NOTICES, V44, P229, DOI 10.1145/1508284.1508271
   Ha B., 2011, P 5 INT C UB INF MAN
   Jen-Wei Hsieh, 2006, ACM Transaction on Storage, V2, P22, DOI 10.1145/1138041.1138043
   Jin Seongwook., 2011, Proceedings of the 2011 ACM Symposium on Applied Computing, P360
   Jung D, 2010, ACM T EMBED COMPUT S, V9, DOI 10.1145/1721695.1721706
   Kim J, 2002, IEEE T CONSUM ELECTR, V48, P366
   Kim J, 2010, IEICE T INF SYST, VE93D, P1644, DOI 10.1587/transinf.E93.D.1644
   Lee S., 2008, SIGOPS OPERATING SYS, V42, P36
   Lee SW, 2007, ACM T EMBED COMPUT S, V6, DOI 10.1145/1275986.1275990
   Park C, 2008, ACM T EMBED COMPUT S, V7, DOI 10.1145/1376804.1376806
NR 24
TC 5
Z9 5
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2013
VL 9
IS 4
AR 13
DI 10.1145/2535931
PG 24
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 281YI
UT WOS:000329136500003
DA 2024-07-18
ER

PT J
AU Luo, JQ
   Bowers, KD
   Oprea, A
   Xu, LH
AF Luo, Jianqiang
   Bowers, Kevin D.
   Oprea, Alina
   Xu, Lihao
TI Efficient Software Implementations of Large Finite Fields
   <i>GF</i>(2<i><SUP>n</SUP></i>) for Secure Storage Applications
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Algorithms; Performance; Security; Finite field arithmetic; cloud
   storage systems; cryptographic algorithms
AB Finite fields are widely used in constructing error-correcting codes and cryptographic algorithms. In practice, error-correcting codes use small finite fields to achieve high-throughput encoding and decoding. Conversely, cryptographic systems employ considerably larger finite fields to achieve high levels of security. We focus on developing efficient software implementations of arithmetic operations in reasonably large finite fields as needed by secure storage applications.
   In this article, we study several arithmetic operation implementations for finite fields ranging from GF(2(32)) to GF(2(128)). We implement multiplication and division in these finite fields by making use of precomputed tables in smaller fields, and several techniques of extending smaller field arithmetic into larger field operations. We show that by exploiting known techniques, as well as new optimizations, we are able to efficiently support operations over finite fields of interest. We perform a detailed evaluation of several techniques, and show that we achieve very practical performance for both multiplication and division.
   Finally, we show how these techniques find applications in the implementation of HAIL, a highly available distributed cloud storage layer. Using the newly implemented arithmetic operations in GF(2(64)), HAIL improves its performance by a factor of two, while simultaneously providing a higher level of security.
C1 [Luo, Jianqiang; Xu, Lihao] Wayne State Univ, Detroit, MI 48202 USA.
   [Bowers, Kevin D.; Oprea, Alina] RSA Labs, Cambridge Ctr 11, Cambridge, MA 02142 USA.
C3 Wayne State University
RP Luo, JQ (corresponding author), Wayne State Univ, 5057 Woodward Ave, Detroit, MI 48202 USA.
EM jianqiang@wayne.edu
CR [Anonymous], INT SSE4 PROGR REF
   [Anonymous], 2016, HDB APPL CRYPTOGRAPH
   ARANHA D. F., 2010, RELIC EFFICIENT LIB
   Aranha D. F., 2010, P 1 INT C CRYPT INF
   Avanzi R., 2007, P INT WORKSH AR FIN, P21
   BAILEY D. V, 1998, P ANN INT CRYPT C CR
   Beachy J. A., 2006, ABSTRACT ALGEBRA
   Ben-Or M., 1981, 22nd Annual Symposium on Foundations of Computer Science, P394, DOI 10.1109/SFCS.1981.37
   Bowers K. D., 2009, P 16 ACM C COMP COMM
   DEWIN E., 1996, P ANN INT C THEORY A
   DIFFIE W, 1976, IEEE T INFORM THEORY, V22, P644, DOI 10.1109/TIT.1976.1055638
   GAO S., 1997, FOUND COMPUT MATH
   GREENAN K. M., 2007, UCSCSSRC0709
   GREENAN K. M., 2008, P INT S MOD AN SIM C
   Guajardo J, 2006, ACTA APPL MATH, V93, P3, DOI 10.1007/s10440-006-9046-1
   Hankerson D., 2000, P WORKSH CRYPT HARDW
   HARPER G., 1992, P ANN INT C THEORY A
   Huang C., 2003, Fast Software Implementation of Finite Field Operations
   INTEL, 2011, INT ADV ENCRYPT STAN
   Kravitz D. W., 1993, U.S. Patent, Patent No. [US5231668A, 5231668]
   Lidl R., 1997, FINITE FIELDS, V2nd
   LOPEZ J, 2000, P ANN INT C CRYPT IN
   MacWilliams F. J., 1978, The Theory of Error-Correcting Codes
   MILLER V. S., 1986, P ANN INT CRYPT C CR
   NATIONAL INSTITUTE FOR STANDARDS AND TECHNOLOGY (NIST), 2009, 1863 NIST FIPS DSS
   Patterson D. A., 1988, P 1988 ACM SIGMOD IN
   Plank J. S., 2008, CS08627 U TENN
   PLANK J. S., 2009, P 7 US C FIL STOR TE
   Plank James S., 2007, FAST GALOIS FIELD AR
   Plank JS, 1997, SOFTWARE PRACT EXPER, V27, P995, DOI 10.1002/(SICI)1097-024X(199709)27:9<995::AID-SPE111>3.0.CO;2-6
   REED IS, 1960, J SOC IND APPL MATH, V8, P300, DOI 10.1137/0108018
   SCHROEPPEL R., 1995, P ANN INT CRYPT C CR
   Seroussi G., 1998, TABLE LOW WEIGHT BIN
   Shoup V, 1995, J SYMB COMPUT, V20, P363, DOI 10.1006/jsco.1995.1055
   VALLEE B., 1998, P 3 INT S ALG NUMB T
NR 35
TC 17
Z9 23
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD FEB
PY 2012
VL 8
IS 1
AR 2
DI 10.1145/2093139.2093141
PG 27
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 990LT
UT WOS:000307633100002
DA 2024-07-18
ER

PT J
AU Huang, Z
   Biersack, E
   Peng, YX
AF Huang, Zhen
   Biersack, Ernst
   Peng, Yuxing
TI Reducing Repair Traffic in P2P Backup Systems: Exact Regenerating Codes
   on Hierarchical Codes
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Experimentation; Management; Performance; Reliability; Theory;
   Regenerating Codes; Hierarchical Codes; repair degree; storage; P2P
   backup systems; maintenance; durability; reliability
AB Peer to peer backup systems store data on "unreliable" peers that can leave the system at any moment. In this case, the only way to assure durability of the data is to add redundancy using either replication or erasure codes. Erasure codes are able to provide the same reliability as replication requiring much less storage space. Erasure coding breaks the data into blocks that are encoded and then stored on different nodes. However, when storage nodes permanently abandon the system, new redundant blocks must be created, which is referred to as repair. For "classical" erasure codes, generating a new block requires the transmission of k blocks over the network, resulting in a high repair traffic. Recently, two new classes of erasure codes, Regenerating Codes and Hierarchical Codes, have been proposed that significantly reduce the repair traffic. Regenerating Codes reduce the amount of data uploaded by each peer involved in the repair, while Hierarchical Codes reduce the number of nodes participating in the repair. In this article we propose to combine these two codes to devise a new class of erasure codes called ER-Hierarchical Codes that combine the advantages of both.
C1 [Huang, Zhen; Peng, Yuxing] Natl Univ Def Technol, Dept Comp, Natl Lab Parallel & Distributed Proc, Changsha 410073, Hunan, Peoples R China.
   [Biersack, Ernst] Eurecom, Sophia Antipolis, France.
C3 National University of Defense Technology - China; IMT - Institut
   Mines-Telecom; EURECOM
RP Huang, Z (corresponding author), Natl Univ Def Technol, Dept Comp, Natl Lab Parallel & Distributed Proc, Changsha 410073, Hunan, Peoples R China.
EM mathswww84@gmail.com
RI peng, yu/GXW-2071-2022
FU China Scholarship Council; National Basic Research Program of China
   [2011CB302601]
FX Z. Huang was partly supported by the China Scholarship Council. The work
   of Y. Peng was supported by the National Basic Research Program of China
   under Grant No. 2011CB302601.
CR Adya A, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE FIFTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P1
   [Anonymous], 2010, CCGrid, DOI DOI 10.1109/CCGRID.2010.71
   Blake Charles., 2003, Proceedings of the 9th conference on Hot Topics in Operating Systems - Volume 9, HOTOS'03, V9, P1
   CHUN B.-G., 2006, P USENIX INT WORKSH
   CHUN B.-G., 2006, P USENIX S NETW SYST
   CHUN B.-G., 2008, Proc. NSDI'08, P393
   Dabek F., 2001, Operating Systems Review, V35, P202, DOI 10.1145/502059.502054
   Dimakis A.G., 2010, SURVEY NETWORK CODES, Vabs/1004.4438
   Dimakis AG, 2010, IEEE T INFORM THEORY, V56, P4539, DOI 10.1109/TIT.2010.2054295
   DISCHINGER M., 2007, P ACM INT MEAS C IMC
   Druschel P, 2001, EIGHTH WORKSHOP ON HOT TOPICS IN OPERATING SYSTEMS, PROCEEDINGS, P75, DOI 10.1109/HOTOS.2001.990064
   DUMINUCO A., 2009, THESIS TELECOM PARIS
   DUMINUCO A., 2009, J OPEER TO PEER NETW, V2, P52
   DUMINUCO A., 2007, P ACM C EM NETW EXP
   Duminuco A, 2009, INT CON DISTR COMP S, P376, DOI 10.1109/ICDCS.2009.14
   Godfrey B., 2006, REPOSITORY AVAILABIL
   Haeberlen A, 2005, USENIX ASSOCIATION PROCEEDINGS OF THE 2ND SYMPOSIUM ON NETWORKED SYSTEMS DESIGN & IMPLEMENTATION (NSDI '05), P143
   HARDDRIVER, 2010, COST HARD DRIV STOR
   Kubiatowicz J, 2000, ACM SIGPLAN NOTICES, V35, P190, DOI 10.1145/384264.379239
   Lin WK, 2004, FOURTH INTERNATIONAL CONFERENCE ON PEER-TO-PEER COMPUTING, PROCEEDINGS, P90, DOI 10.1109/PTP.2004.1334935
   Mitzenmacher M, 2004, 2004 IEEE INFORMATION THEORY WORKSHOP, PROCEEDINGS, P271
   Pamies-Juarez L., 2011, COST ANAL REDUNDANCY
   Parnies-Juarez L., 2010, P IEEE INT C PEER TO, P1
   PLANK J. S., 2005, P USENIX C FIL STOR
   PLANK JS, 2008, P USENIX C FIL STOR
   RAMCHANDRAN K., 2010, EXACT REGENERATION C
   Rashmi KV, 2009, ANN ALLERTON CONF, P1243, DOI 10.1109/ALLERTON.2009.5394538
   Rashmi K. V., 2010, ABS10054178 CORR
   Richardson Tom, 2008, Modern Coding Theory
   RODRIGUES R., 2005, P USENIX INT WORKSH
   SHAH N. B., 2010, IEEE T INFORM UNPUB
   SHOJANIA H., 2009, P 29 INT C DISTR COM
   WEATHERSPOON H., 2002, P USENIX INT WORKSH
   Wu YN, 2009, IEEE INT SYMP INFO, P2276, DOI 10.1109/ISIT.2009.5205898
   Zhang Z., 2010, MSRTR201052
NR 35
TC 9
Z9 9
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD OCT
PY 2011
VL 7
IS 3
AR 10
DI 10.1145/2027066.2027070
PG 41
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 990LO
UT WOS:000307632600004
DA 2024-07-18
ER

PT J
AU Tomazic, S
   Pavlovic, V
   Milovanovic, J
   Sodnik, J
   Kos, A
   Stancin, S
   Milutinovic, V
AF Tomazic, Saso
   Pavlovic, Vesna
   Milovanovic, Jasna
   Sodnik, Jaka
   Kos, Anton
   Stancin, Sara
   Milutinovic, Veljko
TI Fast File Existence Checking in Archiving Systems
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Algorithms; Performance; File systems management; hash-table;
   archiving; files backup/recovery; files sorting/searching; performance
   evaluation
AB This article presents a new FastHash-based File Existence Checking (FHFEC) method for archiving systems. During the archiving process, there are many submissions which are actually unchanged files that do not need to be re-archived. In this system, instead of comparing the entire files, only digests of the files are compared. Strong cryptographic hash functions with a low probability of collision can be used as digests. We propose a fast algorithm to check if a certain hash, that is, a corresponding file, is already stored in the system. The algorithm is based on dividing the whole domain of hashes into equally sized regions, and on the existence of a pointer array, which has exactly one pointer for each region. Each pointer points to the location of the first stored hash from the corresponding region and has a null value if no hash from that region exists. The entire structure can be stored in random access memory or, alternatively, on a dedicated hard disk. A statistical performance analysis has been performed that shows that in certain cases FHFEC performs nearly optimally. Extensive simulations have confirmed these analytical results. The performance of FHFEC has been compared to the performance of a binary search (BIS) and B+tree, which are commonly used in file systems and databases for table indices. The results show that FHFEC significantly outperforms both of them.
C1 [Tomazic, Saso; Sodnik, Jaka; Kos, Anton; Stancin, Sara] Univ Ljubljana, Fac Elect Engn, Ljubljana, Slovenia.
   [Pavlovic, Vesna; Milovanovic, Jasna; Milutinovic, Veljko] Univ Belgrade, Fac Elect Engn, Belgrade 11001, Serbia.
C3 University of Ljubljana; University of Belgrade
RP Tomazic, S (corresponding author), Univ Ljubljana, Fac Elect Engn, Trzaska 25, Ljubljana, Slovenia.
EM saso.tomazic@fe.uni-lj.si; vesna.pavlovic@gmail.com;
   jasna.milovanovic@gmail.com; jaka.sodnik@fe.uni-lj.si;
   anton.kos@fe.uni-lj.si; sara.stancin@fe.uni-lj.si; vm@etf.rs
RI Tomazic, Saso/A-2018-2008; Marinkovic, Vesna/AAC-5222-2022
OI Marinkovic, Vesna/0000-0003-0526-899X; Tomazic,
   Saso/0000-0002-2968-8879; Kos, Anton/0000-0001-6234-8561
FU Slovenian Research Agency [P2-0246]
FX This work was supported by the Slovenian Research Agency, under research
   program P2-0246.
CR Bayer R., 1972, Acta Informatica, V1, P173, DOI 10.1007/BF00288683
   BINGMANN T., 2010, SPEED TEST RESULTS
   BINGMANN T., 2010, STX B TREE C TEMPLAT
   BOHN R., 2008, MUCH INFORM GLOBAL I
   BRODER A. Z., 1993, METHODS COMMUNICATIO
   Cormen Thomas H., 2001, INTRO ALGORITHMS
   CORWIN E. M., 2010, AVERAGE CASE BINARY
   Cox LR, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE FIFTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P285, DOI 10.1145/1060289.1060316
   IBM, 2010, GROUP HASH IMPL
   Jovanov E, 2002, IEEE T COMPUT, V51, P1026, DOI 10.1109/TC.2002.1032623
   Knuth D. E., 1997, ART COMPUTER PROGRAM, V2
   KULKARNI P., 2004, P USENIX TECHN C
   Lyman P., 2003, How much information?
   MUTHITACHAROEN A., 2001, P S OP SYST PRINC
   *NAT I STAND TECHN, 2002, 1802 FIPS NAT I STAN
   Papoulis A., 1965, PROBABILITY RANDOM V
   Parlante N., 2001, LINKED LIST BASICS
   POLICRONIADES C., 2004, P USENIX C
   Quinlan S., 2002, P 1 USENIX C FIL STO
   RUDAN S., 2006, PSI T INTERNET RES, V2, P2
   Rudan S, 2006, IPSI BDG TRANS INTER, V2, P17
NR 21
TC 3
Z9 4
U1 1
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUN
PY 2011
VL 7
IS 1
AR 2
DI 10.1145/1970343.1970345
PG 21
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 990LE
UT WOS:000307631600002
DA 2024-07-18
ER

PT J
AU Shim, G
   Park, Y
   Park, KH
AF Shim, Gyudong
   Park, Youngwoo
   Park, Kyu Ho
TI A Hybrid Flash Translation Layer with Adaptive Merge for SSDs
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Algorithms; Performance; Flash translation layer; NAND flash memory;
   solid state disks; garbage collection; striping
AB The Flash Translation Layer (FTL) in Solid-State Disks (SSDs) maps logical addresses to physical addresses for disk drive virtualization. In order to reduce garbage collection overhead, we propose full associative striped block-level mapping. In addition, an adaptive merge is proposed to avoid excessive data block reconstructions during garbage collection. With these mechanisms, the write latency is improved up to 78% in comparison with the previous multichannel hybrid FTLs in a sample PC trace. The performance improvements stem from 52% reduced garbage collection.
C1 [Shim, Gyudong; Park, Youngwoo; Park, Kyu Ho] Korea Adv Inst Sci & Technol, Dept Elect Engn, Taejon 305701, South Korea.
C3 Korea Advanced Institute of Science & Technology (KAIST)
RP Shim, G (corresponding author), Korea Adv Inst Sci & Technol, Dept Elect Engn, Gwahak Ro 373-1 Guseong Dong, Taejon 305701, South Korea.
EM gdshim@core.kaist.ac.kr
RI Park, Kyu Ho/C-1869-2011
CR Agrawal Nitin, 2008, P USENIX ANN TECHN C, P57
   [Anonymous], 2008, P 8 ACM INT C EMB SO
   Caulfield AM, 2009, ACM SIGPLAN NOTICES, V44, P217, DOI 10.1145/1508284.1508270
   Chang LP, 2002, EIGHTH IEEE REAL-TIME AND EMBEDDED TECHNOLOGY AND APPLICATIONS SYMPOSIUM, PROCEEDINGS, P187, DOI 10.1109/RTTAS.2002.1137393
   ELECTRONICS S., 2007, K9F8G08UXM 1G X 8 BI
   ELECTRONICS S., 2006, K9XXG08UXA 1G X 8 BI
   Gupta A, 2009, ACM SIGPLAN NOTICES, V44, P229, DOI 10.1145/1508284.1508271
   Jung D, 2010, ACM T EMBED COMPUT S, V9, DOI 10.1145/1721695.1721706
   Kang JU, 2007, J SYST ARCHITECT, V53, P644, DOI 10.1016/j.sysarc.2007.01.010
   Kang SY, 2009, IEEE T COMPUT, V58, P744, DOI 10.1109/TC.2008.224
   Kim J, 2002, IEEE T CONSUM ELECTR, V48, P366
   KIM J.H., P INT WORKSH SOFTW S
   Lee SW, 2007, ACM T EMBED COMPUT S, V6, DOI 10.1145/1275986.1275990
   Park C, 2008, ACM T EMBED COMPUT S, V7, DOI 10.1145/1376804.1376806
   Park J, 2009, IEICE ELECTRON EXPR, V6, P297, DOI 10.1587/elex.6.297
   Shin JY, 2009, ICS'09: PROCEEDINGS OF THE 2009 ACM SIGARCH INTERNATIONAL CONFERENCE ON SUPERCOMPUTING, P338, DOI 10.1145/1542275.1542324
   Sungjin Lee, 2008, Operating Systems Review, V42, P36, DOI 10.1145/1453775.1453783
NR 17
TC 12
Z9 12
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAY
PY 2011
VL 6
IS 4
AR 15
DI 10.1145/1970338.1970339
PG 27
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 990KX
UT WOS:000307630800001
DA 2024-07-18
ER

PT J
AU Li, PF
   Hua, Y
   Zuo, PF
   Chen, ZY
   Sheng, JJ
AF Li, Pengfei
   Hua, Yu
   Zuo, Pengfei
   Chen, Zhangyu
   Sheng, Jiajie
TI A High-performance RDMA-oriented Learned Key-value Store for
   Disaggregated Memory Systems
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Disaggregated memory system; learned index; key-value store
ID INDEX; END
AB Disaggregated memory systems separate monolithic servers into different components, including compute and memory nodes, to enjoy the benefits of high resource utilization, flexible hardware scalability, and efficient data sharing. By exploiting the high-performance RDMA (Remote Direct Memory Access), the compute nodes directly access the remote memory pool without involving remote CPUs. Hence, the ordered keyvalue (KV) stores (e.g., B-trees and learned indexes) keep all data sorted to provide range query services via the high-performance network. However, existing ordered KVs fail to work well on the disaggregated memory systems, due to either consuming multiple network roundtrips to search the remote data or heavily relying on the memory nodes equipped with insufficient computing resources to process data modifications. In this article, we propose a scalable RDMA-oriented KV store with learned indexes, called ROLEX, to coalesce the ordered KV store in the disaggregated systems for efficient data storage and retrieval. ROLEX leverages a retraining-decoupled learned index scheme to dissociate the model retraining from data modification operations via adding a bias and some data movement constraints to learned models. Based on the operation decoupling, data modifications are directly executed in compute nodes via one-sided RDMA verbs with high scalability. The model retraining is hence removed from the critical path of data modification and asynchronously executed in memory nodes by using dedicated computing resources. ROLEX efficiently alleviates the fragmentation and garbage collection issues, due to allocating and reclaiming space via fixed-size leaves that are accessed via the atomic-size leaf numbers. Our experimental results on YCSB and real-world workloads demonstrate that ROLEX achieves competitive performance on the static workloads, as well as significantly improving the performance on dynamic workloads by up to 2.2x over state-of-the-art schemes on the disaggregated memory systems. We have released the open-source codes for public use in GitHub.
C1 [Li, Pengfei; Hua, Yu; Zuo, Pengfei; Chen, Zhangyu; Sheng, Jiajie] Huazhong Univ Sci & Technol, Wuhan Natl Lab Optoelect, Sch Comp Sci & Technol, Luoyu Rd 1037, Wuhan 430074, Peoples R China.
C3 Huazhong University of Science & Technology
RP Hua, Y (corresponding author), Huazhong Univ Sci & Technol, Wuhan Natl Lab Optoelect, Sch Comp Sci & Technol, Luoyu Rd 1037, Wuhan 430074, Peoples R China.
EM cspfli@hust.edu.cn; csyhua@hust.edu.cn; pfzuo@hust.edu.cn;
   chenzy@hust.edu.cn; jjsheng@hust.edu.cn
OI Zuo, Pengfei/0000-0001-9982-5130
FU National Natural Science Foundation of China (NSFC) [62125202, U22B2022,
   61821003]
FX This work was supported in part by the National Natural Science
   Foundation of China (NSFC) under Grant No. 62125202, U22B2022, and
   61821003.
CR Aguilera MK, 2018, PROCEEDINGS OF THE 2018 USENIX ANNUAL TECHNICAL CONFERENCE, P775
   amazon, 2021, Amazon s3
   Amazon, 2021, Amazon Elastic Block Store
   Binnig C, 2016, PROC VLDB ENDOW, V9, P528, DOI 10.14778/2904483.2904485
   Carbonari A, 2017, HOTNETS-XVI: PROCEEDINGS OF THE 16TH ACM WORKSHOP ON HOT TOPICS IN NETWORKS, P164, DOI 10.1145/3152434.3152447
   Chen YM, 2019, PROCEEDINGS OF THE FOURTEENTH EUROSYS CONFERENCE 2019 (EUROSYS '19), DOI 10.1145/3302424.3303968
   COMER D, 1979, COMPUT SURV, V11, P121, DOI 10.1145/356770.356776
   Dai YF, 2020, PROCEEDINGS OF THE 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '20), P155
   Ding JL, 2020, PROC VLDB ENDOW, V14, P74, DOI 10.14778/3425879.3425880
   Ding JL, 2020, SIGMOD'20: PROCEEDINGS OF THE 2020 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P969, DOI 10.1145/3318464.3389711
   Dragojevic A, 2015, SOSP'15: PROCEEDINGS OF THE TWENTY-FIFTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P54, DOI 10.1145/2815400.2815425
   Dragojevic Aleksandar, 2014, NSDI 14, P401
   Ferragina P, 2020, PROC VLDB ENDOW, V13, P1162, DOI 10.14778/3389133.3389135
   Galakatos A, 2019, INT CONF MANAGE DATA, P1189, DOI 10.1145/3299869.3319860
   Gao PX, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P249
   Guo ZY, 2022, ASPLOS '22: PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS, P417, DOI 10.1145/3503222.3507762
   Hilprecht B, 2020, PROC VLDB ENDOW, V13, P992, DOI 10.14778/3384345.3384349
   HP, 2021, The Machine
   Hwang D, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P187
   Intel Corporation, 2021, Intel Rack Scale Design Architecture
   Kalia A, 2019, PROCEEDINGS OF THE 16TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P1
   Kalia A, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P185
   Kalia A, 2014, SIGCOMM'14: PROCEEDINGS OF THE 2014 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P295, DOI [10.1145/2619239.2626299, 10.1145/2740070.2626299]
   Kraska T, 2018, INT CONF MANAGE DATA, P489, DOI 10.1145/3183713.3196909
   Lan Hai, 2023, Proc. ACM Manag. Data, V1
   LEHMAN PL, 1981, ACM T DATABASE SYST, V6, P650, DOI 10.1145/319628.319663
   Li PF, 2023, PROC VLDB ENDOW, V16, P2212, DOI 10.14778/3598581.3598593
   Li PF, 2020, SIGMOD'20: PROCEEDINGS OF THE 2020 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P2119, DOI 10.1145/3318464.3389703
   Lim K, 2012, INT S HIGH PERF COMP, P189
   Lim K, 2009, CONF PROC INT SYMP C, P267, DOI 10.1145/1555815.1555789
   Lu YY, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P773
   Mao Y., 2012, P 7 ACM EUR C COMP S, P183
   Mitchell C, 2016, PROCEEDINGS OF USENIX ATC '16: 2016 USENIX ANNUAL TECHNICAL CONFERENCE, P451
   Ruan ZY, 2020, PROCEEDINGS OF THE 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '20), P315
   Salama A., 2017, IEEE Data Engineering Bulletin, V40, P27
   Shan YZ, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P69
   Shan YZ, 2017, PROCEEDINGS OF THE 2017 SYMPOSIUM ON CLOUD COMPUTING (SOCC '17), P323, DOI 10.1145/3127479.3128610
   Shen JC, 2023, PROCEEDINGS OF THE 21ST USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, FAST 2023, P81
   Shrivastav V, 2019, PROCEEDINGS OF THE 16TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P255
   Tang CZ, 2020, PROCEEDINGS OF THE 25TH ACM SIGPLAN SYMPOSIUM ON PRINCIPLES AND PRACTICE OF PARALLEL PROGRAMMING (PPOPP '20), P308, DOI 10.1145/3332466.3374547
   Tirmazi M, 2020, PROCEEDINGS OF THE FIFTEENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS (EUROSYS'20), DOI 10.1145/3342195.3387517
   Tsai SY, 2020, PROCEEDINGS OF THE 2020 USENIX ANNUAL TECHNICAL CONFERENCE, P33
   Tsai SY, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P306, DOI 10.1145/3132747.3132762
   Vienne J., 2012, 2012 IEEE 20th Annual Symposium on High-Performance Interconnects (HOTI), P48, DOI 10.1109/HOTI.2012.19
   Wang CX, 2020, PROCEEDINGS OF THE 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '20), P261
   Wang Q, 2022, INT CONF MANAGE DATA, P1033, DOI 10.1145/3514221.3517824
   Wei XD, 2020, PROCEEDINGS OF THE 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '20), P117
   Wei XD, 2015, SOSP'15: PROCEEDINGS OF THE TWENTY-FIFTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P87, DOI 10.1145/2815400.2815419
   Xie Q, 2014, VLDB J, V23, P915, DOI 10.1007/s00778-014-0355-0
   Yahoo, 2019, Yahoo! Cloud Serving Benchmark (YCSB)
   Yang J, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P221
   Zamanian E, 2017, PROC VLDB ENDOW, V10, P685, DOI 10.14778/3055330.3055335
   Zhang M, 2022, PROCEEDINGS OF THE 20TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, FAST 2022, P51
   Zhu YB, 2015, ACM SIGCOMM COMP COM, V45, P523, DOI 10.1145/2785956.2787484
   Ziegler T, 2019, INT CONF MANAGE DATA, P741, DOI 10.1145/3299869.3300081
   Zuo PF, 2021, PROCEEDINGS OF THE 2021 USENIX ANNUAL TECHNICAL CONFERENCE, P15
NR 56
TC 0
Z9 0
U1 1
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2023
VL 19
IS 4
AR 30
DI 10.1145/3620674
PG 30
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X6NQ6
UT WOS:001099601100002
DA 2024-07-18
ER

PT J
AU Kwon, M
   Lee, S
   Choi, H
   Hwang, J
   Jung, M
AF Kwon, Miryeong
   Lee, Seungjun
   Choi, Hyunkyu
   Hwang, Jooyoung
   Jung, Myoungsoo
TI Realizing Strong Determinism Contract on Log-Structured Merge Key-Value
   Stores
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Key-value stores; software/hardware co-design; deterministic latency
ID STORAGE ENGINE; FLASH
AB We propose Vigil-KV, a hardware and software co-designed framework that eliminates long-tail latency almost perfectly by introducing strong latency determinism. To make Get latency deterministic, Vigil-KV first enables a predictable latency mode (PLM) interface on a real datacenter-scale NVMe SSD, having knowledge about the nature of the underlying flash technologies. Vigil-KV at the system-level then hides the non-deterministic time window (associated with SSD's internal tasks and/or write services) by internally scheduling the different device states of PLM across multiple physical functions. Vigil-KV further schedules compaction/flush operations and client requests being aware of PLM's restrictions thereby integrating strong latency determinism into LSM KVs. We implement Vigil-KV upon a 1.92TB NVMe SSD prototype and Linux 4.19.91, but other LSM KVs can adopt its concept. We evaluate diverse Facebook and Yahoo scenarios with Vigil-KV, and the results show that Vigil-KV can reducethe tail latency of a baseline KV system by 3.19x while reducing the average latency by 34%, on average.
C1 [Kwon, Miryeong; Lee, Seungjun; Choi, Hyunkyu; Jung, Myoungsoo] Korea Adv Inst Sci & Technol KAIST, Daejeon 34065, South Korea.
   [Hwang, Jooyoung] Samsung, Seoul, South Korea.
C3 Korea Advanced Institute of Science & Technology (KAIST)
RP Jung, M (corresponding author), Korea Adv Inst Sci & Technol KAIST, Daejeon 34065, South Korea.
EM mkwon@camelab.org; sjlee@camelab.org; hoi@camelab.org;
   jooyoung.hwang@samsung.com; m.jung@kaist.ac.kr
OI Lee, Seung Jun/0009-0002-7224-9529
FU Samsung [G01220530]; Samsung HiPER [G01220296]; KAIST IDEC; KAIST
   start-up package [G01190015]; NRF [2021R1A2C4001773]; IITP
   [2021-0-00524, 2022-0-00117]
FX This work is mainly supported by Samsung (G01220530) and Samsung HiPER
   (G01220296). This work is also in part supported by NRF's
   2021R1A2C4001773, IITP's 2021-0-00524 & 2022-0-00117, KAIST start-up
   package (G01190015), and KAIST IDEC.
CR Agrawal Nitin, 2008, 2008 USENIX ANN TECH
   Alameldeen AR, 2011, ISCA 2011: PROCEEDINGS OF THE 38TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE, P461, DOI 10.1145/2024723.2000118
   Amazon, AMAZON FOUND EVERY 1
   Anderson M, 2021, Arxiv, DOI arXiv:2107.04140
   [Anonymous], Instagram
   [Anonymous], 2011, Facebook
   Balakrishnan M, 2010, ACM T STORAGE, V6, DOI 10.1145/1807060.1807061
   Balmau O, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P753
   Balmau O, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P363
   Cai Y, 2017, P IEEE, V105, P1666, DOI 10.1109/JPROC.2017.2713127
   Cai Y, 2017, INT S HIGH PERF COMP, P49, DOI 10.1109/HPCA.2017.61
   Cai Y, 2013, 2013 IEEE 31ST INTERNATIONAL CONFERENCE ON COMPUTER DESIGN (ICCD), P123, DOI 10.1109/ICCD.2013.6657034
   Cai Y, 2015, I C DEPEND SYS NETWO, P438, DOI 10.1109/DSN.2015.49
   Cai Y, 2012, DES AUT TEST EUROPE, P521
   Cai Yu, 2013, P 2013 DESIGN AUTOMA
   Callaghan M., 2013, P 2013 ACM SIGMOD IN, P1185, DOI DOI 10.1145/2463676.2465296
   Cao ZC, 2020, PROCEEDINGS OF THE 18TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P209
   Chang L.-P., 2004, ACM Trans. on Embedded Computing Syst, V3, P837
   Chen H, 2021, PROCEEDINGS OF THE 19TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '21), P17
   Choi W, 2018, HPDC '18: PROCEEDINGS OF THE 27TH INTERNATIONAL SYMPOSIUM ON HIGH-PERFORMANCE PARALLEL AND DISTRIBUTED COMPUTING, P243, DOI 10.1145/3208040.3208048
   Conway A, 2020, PROCEEDINGS OF THE 2020 USENIX ANNUAL TECHNICAL CONFERENCE, P49
   Cooper Brian F., 2010, P 1 ACM S CLOUD COMP, P143, DOI [DOI 10.1145/1807128.1807152, 10.1145/1807128.1807152]
   Crankshaw D, 2017, PROCEEDINGS OF NSDI '17: 14TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P613
   Dayan N, 2018, INT CONF MANAGE DATA, P505, DOI 10.1145/3183713.3196927
   Dayan N, 2017, SIGMOD'17: PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P79, DOI 10.1145/3035918.3064054
   Dong SY, 2021, ACM T STORAGE, V17, DOI 10.1145/3483840
   Dong Siying, 2017, P 8 BIENN C INN DAT
   Eisenman A, 2018, EUROSYS '18: PROCEEDINGS OF THE THIRTEENTH EUROSYS CONFERENCE, DOI 10.1145/3190508.3190524
   Gan Y, 2019, TWENTY-FOURTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXIV), P19, DOI 10.1145/3297858.3304004
   Ghemawat S., LevelDB
   Google, BENCHM MOB PAG SPEED
   gRPC, gRPC: A high performance, open source universal RPC framework
   Gupta A, 2009, ACM SIGPLAN NOTICES, V44, P229, DOI 10.1145/1508284.1508271
   Ha K, 2016, IEEE T COMPUT AID D, V35, P1079, DOI 10.1109/TCAD.2015.2504868
   Huang Ping, 2014, P 2014 USENIX ANN TE
   Im J, 2020, PROCEEDINGS OF THE 2020 USENIX ANNUAL TECHNICAL CONFERENCE, P173
   Jiang Tianyang, 2021, P 19 USENIX C FILE S
   Jimenez X., 2014, FAST
   Jo H, 2006, IEEE T CONSUM ELECTR, V52, P485, DOI 10.1109/TCE.2006.1649669
   Jung M, 2020, IEEE T COMPUT AID D, V39, P1674, DOI 10.1109/TCAD.2019.2919035
   Jung M, 2014, CONF PROC INT SYMP C, P289, DOI 10.1109/ISCA.2014.6853216
   Jung Myoungsoo, 2013, ACM INT C MEAS MOD C, P203, DOI DOI 10.1145/2465529.2465548
   Kaiyrakhmet O, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P191
   Kang J., 2006, P 6 ACM IEEE INT C E
   Kang SY, 2009, IEEE T COMPUT, V58, P744, DOI 10.1109/TC.2008.224
   Kang WH, 2014, SIGMOD'14: PROCEEDINGS OF THE 2014 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P529, DOI 10.1145/2588555.2595632
   Kannan S, 2018, PROCEEDINGS OF THE 2018 USENIX ANNUAL TECHNICAL CONFERENCE, P993
   Kim BS, 2018, PROCEEDINGS OF THE 2018 USENIX ANNUAL TECHNICAL CONFERENCE, P677
   Kim BS, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P281
   Kim H, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P239, DOI 10.1145/1378600.1378627
   Kim J, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P799
   Koh S, 2019, I S WORKL CHAR PROC, P216, DOI [10.1109/iiswc47752.2019.9042009, 10.1109/IISWC47752.2019.9042009]
   Koh Sungjoon, 2018, P 10 USENIX WORKSHOP
   Kumar Sanjeev, SOCIAL NETWORKING SC
   Lee S, 2022, CONF PROC INT SYMP C, P289, DOI 10.1145/3470496.3527397
   Lepers B, 2019, PROCEEDINGS OF THE TWENTY-SEVENTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '19), P447, DOI 10.1145/3341301.3359628
   Li Huaicheng, 2021, P ACM SIGOPS 28 S OP
   Li Q, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P125
   Li YK, 2021, PROCEEDINGS OF THE 2021 USENIX ANNUAL TECHNICAL CONFERENCE, P673
   Lim H, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P1
   Liu CY, 2019, TWENTY-FOURTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXIV), P955, DOI 10.1145/3297858.3304035
   Lu LY, 2017, ACM T STORAGE, V13, DOI 10.1145/3033273
   Mao B, 2012, ACM T STORAGE, V8, DOI 10.1145/2093139.2093143
   Matsunobu Y, 2020, PROC VLDB ENDOW, V13, P3217, DOI 10.14778/3415478.3415546
   Mielke N, 2008, INT RELIAB PHY SYM, P9, DOI 10.1109/RELPHY.2008.4558857
   Ming-Chang Yang, 2014, 2014 International Conference on Smart Computing (SMARTCOMP), P66, DOI 10.1109/SMARTCOMP.2014.7043841
   Myoungsoo Jung, 2012, Middleware 2012. ACM/IFIP/USENIX 13th International Middleware Conference. Proceedings, P164, DOI 10.1007/978-3-642-35170-9_9
   NVM Express Inc, NVM express specification
   Park Gyuyoung, 2018, P 10 USENIX WORKSHOP
   Park Stan, 2012, P FILE STORAGE TECHN
   Raju P, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P497, DOI 10.1145/3132747.3132765
   Ren K, 2017, PROC VLDB ENDOW, V10, P2037, DOI 10.14778/3151106.3151108
   RPC, APACHE THRIFT
   Schroeder B, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P67
   Sears Russell, 2012, P 2012 ACM SIGMOD IN, P217, DOI [10.1145/2213836.2213862, DOI 10.1145/2213836.2213862]
   Shahidi N, 2016, SC '16: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS, P561, DOI 10.1109/SC.2016.47
   Sun HB, 2016, IEEE T VLSI SYST, V24, P2654, DOI 10.1109/TVLSI.2016.2535224
   TABB Group, VAL MILL FIND OPT SP
   Tavakkol A, 2018, CONF PROC INT SYMP C, P397, DOI 10.1109/ISCA.2018.00041
   Van Houdt Benny, 2013, Performance Evaluation Review, V41, P191
   Wang Shucheng, 2020, P 18 USENIX C FILE S
   Wang W, 2018, PROC VLDB ENDOW, V12, P128, DOI 10.14778/3282495.3282499
   Wu Guanying, 2012, P 10 USENIX S FIL ST
   Yan SQ, 2017, ACM T STORAGE, V13, DOI 10.1145/3121133
   Yang Pan, 2019, P 11 USENIX WORKSH H
   Zhang J, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P477
   Zhang Jie, 2020, P 2020 IEEE INT S HI
   Zhang Jie, 2021, P 2021 ACMIEEE 48 AN
   Zhang M, 2016, IEEE S MASS STOR SYS
   Zhao Kai., 2013, P 11 USENIX C FILE S
   Zhao Mark, 2022, ISCA '22: Proceedings of the 49th Annual International Symposium on Computer Architecture, P1042, DOI 10.1145/3470496.3533044
   Zhu Xiaoyong, FEATHR LINKEDINS FEA
NR 92
TC 0
Z9 0
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAY
PY 2023
VL 19
IS 2
AR 11
DI 10.1145/3582695
PG 29
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D7OU8
UT WOS:000970592600002
DA 2024-07-18
ER

PT J
AU Lembke, J
   Roman, PL
   Eugster, P
AF Lembke, James
   Roman, Pierre-Louis
   Eugster, Patrick
TI DEFUSE: An Interface for Fast and Correct User Space File System Access
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article; Proceedings Paper
CT ACM Symposium on Operating Systems Principles (SOSP)
CY OCT 26-29, 2021
CL ELECTR NETWORK
SP Assoc Comp Machinery
DE Linux kernel; FUSE; user-space file systems
AB Traditionally, the only option for developers was to implement file systems (FSs) via drivers within the operating system kernel. However, there exists a growing number of file systems (FSs), notably distributed FSs for the cloud, whose interfaces are implemented solely in user space to (i) isolate FS logic, (ii) take advantage of user space libraries, and/or (iii) for rapid FS prototyping. Common interfaces for implementing FSs in user space exist, but they do not guarantee POSIX compliance in all cases, or suffer fromconsiderable performance penalties due to high amounts of wait context switches between kernel and user space processes.
   We propose DEFUSE: an interface for user space FSs that provides fast accesses while ensuring access correctness and requiring no modifications to applications. DEFUSE achieves significant performance improvements over existing user space FS interfaces thanks to its novel design that drastically reduces the number of wait context switches for FS accesses. Additionally, to ensure access correctness, DEFUSE maintains POSIX compliance for FS accesses thanks to three novel concepts of bypassed file descriptor (FD) lookup, FD stashing, and user space paging. Our evaluation spanning a variety of workloads shows that by reducing the number of wait context switches per workload from as many as 16,000 or 41,000 with filesystem in user space down to 9 on average, DEFUSE increases performance 2x over existing interfaces for typical workloads and by as many as 10x in certain instances.
C1 [Lembke, James; Eugster, Patrick] Purdue Univ, 610 Purdue Mall, W Lafayette, IN 47907 USA.
   [Lembke, James] Milwaukee Sch Engn, 1025 N Broadway, Milwaukee, WI 53202 USA.
   [Roman, Pierre-Louis; Eugster, Patrick] Univ Svizzera Italiana USI, Via Giuseppe Buffi 13, CH-6900 Lugano, Switzerland.
   [Eugster, Patrick] Tech Univ Darmstadt, Karolinenpl 5, D-64289 Darmstadt, Germany.
C3 Purdue University System; Purdue University; Milwaukee School
   Engineering; Universita della Svizzera Italiana; Technical University of
   Darmstadt
RP Lembke, J (corresponding author), Purdue Univ, 610 Purdue Mall, W Lafayette, IN 47907 USA.; Lembke, J (corresponding author), Milwaukee Sch Engn, 1025 N Broadway, Milwaukee, WI 53202 USA.
EM lembkej@purdue.edu; romanp@usi.ch; eugstp@usi.ch
OI Lembke, James/0000-0002-1503-7215; Roman,
   Pierre-Louis/0000-0001-5741-1490
FU ERC [617805]; NSF [1618923]; SNSF [200021_197353]; Swiss National
   Science Foundation (SNF) [200021_197353] Funding Source: Swiss National
   Science Foundation (SNF); Direct For Computer & Info Scie & Enginr;
   Division Of Computer and Network Systems [1618923] Funding Source:
   National Science Foundation; European Research Council (ERC) [617805]
   Funding Source: European Research Council (ERC)
FX Work funded in parts by ERC grant no. 617805, NSF grant no. 1618923, and
   SNSF grant no. 200021_197353.
CR Ahmad Faraz., 2012, PUMA PURDUE U BENCHM
   [Anonymous], GDB: The GNU Project Debugger
   [Anonymous], FUSE GOOGLE CLOUD ST
   [Anonymous], TPCX BB SPECIFICATIO
   [Anonymous], LINUX VIRTUAL FILE S
   [Anonymous], GLUSTERFS SCALE OUT
   [Anonymous], ACCESS DBFS USING LO
   [Anonymous], Google Cloud Storage
   [Anonymous], USER SPACE PAGE FAUL
   [Anonymous], 2012, P 9 USENIX C NETWORK
   [Anonymous], LINUX MANUAL BPF PER
   [Anonymous], REACT HOOKS
   [Anonymous], LINUX KERNEL D SPLIC
   [Anonymous], Journaled File System Technology for Linux
   [Anonymous], Lustre parallel file system
   [Anonymous], IOzone Filesystem Benchmark
   [Anonymous], AMAZON S3
   [Anonymous], NFS GANESHA FILE SYS
   [Anonymous], AMAZON S3 FUSE
   [Anonymous], DATABRICKS FILE SYST
   [Anonymous], LINUX KERNEL USERFAU
   [Anonymous], FUSE HIGH LEVEL INTE
   [Anonymous], ALLUXIO FUSE
   [Anonymous], TMPFS DOCUMENTATION
   [Anonymous], ACCESS DBFS DATABRIC
   [Anonymous], PLASTIC FILE SYSTEM
   [Anonymous], LIB HDFS
   [Anonymous], ORANGEFS DIRECT INTE
   [Anonymous], EXT4 EXT2 EXT3 WIKI
   [Anonymous], APACHE HADOOP 241 FI
   [Anonymous], TAHOE LAFS TAHOE LEA
   [Anonymous], SYSIO LIB
   [Anonymous], LIBFUSE SSHFS IMPLEM
   [Anonymous], LINUX MANUAL OVERVIE
   [Anonymous], 2016, COMMUNICATION   1115
   [Anonymous], AVFS A VIRTUAL FILE
   [Anonymous], LINUX USER MANUAL TI
   [Anonymous], SYSTEM CALL WRAPPERS
   [Anonymous], 1997, ANLMCSTM234
   [Anonymous], ACCESSFS PERMISSION
   [Anonymous], MOUNTABLE HDFS
   [Anonymous], NATIVE HDFS FUSE
   [Anonymous], gsutil tool
   [Anonymous], FUSE EXAMPLE FUSEXMP
   [Anonymous], APACHE SPARK, DOI DOI 10.1186/S40537-018-0149-0
   [Anonymous], MOOSE FILE SYSTEM MO
   [Anonymous], IBM SPECTRUM SCALE F
   [Anonymous], EMACS HOOKS
   [Anonymous], FAT FILESYSTEM LIB R
   [Anonymous], LIBFUSE FILESYSTEM U
   [Anonymous], SOLUCORP VIRTUALFS
   [Anonymous], SPARK PYSPARK DAEMON
   [Anonymous], AMAZON WEB SERVICES
   Armbrust M, 2015, SIGMOD'15: PROCEEDINGS OF THE 2015 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P1383, DOI 10.1145/2723372.2742797
   Behrens D., 2015, 12th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 15), P605
   Bent John., 2009, P C HIGH PERFORMANCE, p21:1
   Bijlani A, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P121
   Bonnie David J., 2013, SMALL FILE AGGREGATI
   Borthakur Dhruba, 2008, Hadoop Apache Project, V53, P2
   Burihabwa D, 2018, INT CONF CLOUD COMP, P67, DOI 10.1109/CloudCom2018.2018.00027
   Caldwell B, 2020, INT CON DISTR COMP S, P665, DOI 10.1109/ICDCS47774.2020.00090
   Carns PH, 2000, USENIX ASSOCIATION PROCEEDINGS OF THE 4TH ANNUAL LINUX SHOWCASE AND CONFERENCE, ATLANTA, P317
   Carns P, 2009, INT PARALL DISTRIB P, P524, DOI 10.1079/9781845933975.0001
   Corbett Peter., 1995, IPPS'95 Workshop on Input/Output in Parallel and Distributed Systems, P1
   Deniel Philippe., 2007, Linux Symposium, P113
   Essertel GM, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P799
   Hadzilacos Vassos., 1994, A Modular Approach to Fault-Tolerant Broadcasts and Related Problems
   Hupfeld Felix., 2007, P 3 VLDB WORKSHOP DA
   Inman Jeffrey Thornton, 2017, USENIX MAG
   Ishiguro S, 2012, 2012 SC COMPANION: HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS (SCC), P760, DOI 10.1109/SC.Companion.2012.104
   Jannen W., 2015, P 13 USENIX C FIL ST, P301, DOI DOI 10.5555/2750482.2750505
   Kadekodi R, 2019, PROCEEDINGS OF THE TWENTY-SEVENTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '19), P494, DOI 10.1145/3341301.3359631
   Kantee Antti., 2007, P 6 EUROPEAN BSD C E
   Leslie B, 2005, J COMPUT SCI TECH-CH, V20, P654, DOI 10.1007/s11390-005-0654-4
   Li Haoyuan, 2018, PhD thesis
   Message Passing Interface Forum, 2021, MPI MESSAGE PASSING
   Microsoft Corporation, 2000, MICR EXT FIRMW IN FA
   Muniswamy-Reddy KK, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P115
   Narayan S., 2010, P 12 ANN LIN S OTT, P189
   ObjectiveFS, US
   Patlasov Maxim, 2015, LINUX VAULT
   Peng IB, 2019, PROCEEDINGS OF MCHPC'19: 2019 IEEE/ACM WORKSHOP ON MEMORY CENTRIC HIGH PERFORMANCE COMPUTING (MCHPC), P71, DOI 10.1109/MCHPC49590.2019.00017
   Pillai Manoj., 2019, P LINUX STORAGE FILE
   Rajachandrasekar Raghunath., 2013, P 22 INT S HIGH PERF, P143
   SATYANARAYANAN M, 1990, IEEE T COMPUT, V39, P447, DOI 10.1109/12.54838
   Sebbar A, 2020, J AMB INTEL HUM COMP, V11, P5875, DOI 10.1007/s12652-020-02099-4
   Sharwood Simon., 2018, LINUX LITERALLY LOSE
   Shvachko K, 2010, IEEE S MASS STOR SYS
   Spillane R. P., 2007, P WORKSH EXP COMP SC, P22
   Sundararaman S, 2011, EUROSYS 11: PROCEEDINGS OF THE EUROSYS 2011 CONFERENCE, P77
   Tarasov Vasily., 2015, P 7 USENIX WORKSHOP
   Thain D., 2001, Cluster Computing, V4, P39, DOI 10.1023/A:1011412209850
   Thakur R, 1996, FRONTIERS '96 - THE SIXTH SYMPOSIUM ON FRONTIERS OF MASSIVELY PARALLEL COMPUTING, PROCEEDINGS, P180, DOI 10.1109/FMPC.1996.558080
   Vangoor BKR, 2019, ACM T STORAGE, V15, DOI 10.1145/3310148
   Wang WS, 2021, J SYST ARCHITECT, V113, DOI 10.1016/j.sysarc.2020.101902
   Weil SA, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P307
   Wright SA, 2012, IEEE SYM PARA DISTR, P1352, DOI 10.1109/IPDPSW.2012.172
   Zadok E, 2000, USENIX ASSOCIATION PROCEEDINGS OF THE 2000 USENIX ANNUAL TECHNICAL CONFERENCE, P55
   Zadok Erez., 1999, Linux Expo Conference Proceedings, P141
   Zhu Yue, 2018, P INT WORKSH RUNT OP
NR 100
TC 2
Z9 2
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2022
VL 18
IS 3
AR 22
DI 10.1145/3494556
PG 29
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 4X9BK
UT WOS:000861131000004
DA 2024-07-18
ER

PT J
AU Macko, P
   Hennessey, J
AF Macko, Peter
   Hennessey, Jason
TI Survey of Distributed File System Design Choices
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Distributed file systems; distributed storage; design options; taxonomy;
   survey
ID CODES
AB Decades of research on distributed file systems and storage systems exists. New researchers and engineers have a lot of literature to study, but only a comparatively small number of high-level design choices are available when creating a distributed file system. And within each aspect of the system, typically several common approaches are used. So, rather than surveying distributed file systems, this article presents a survey of important design decisions and, within those decisions, the most commonly used options. It also presents a qualitative exploration of their tradeoffs. We include several relatively recent designs and their variations that illustrate other tradeoff choices in the design space, despite being underexplored. In doing so, we provide a primer on distributed file systems, and we also show areas that are overexplored and underexplored, in the hopes of inspiring new research.
C1 [Macko, Peter; Hennessey, Jason] NetApp Inc, 1601 Trapelo Rd, Waltham, MA 02451 USA.
C3 NetApp, Inc.
RP Macko, P (corresponding author), NetApp Inc, 1601 Trapelo Rd, Waltham, MA 02451 USA.
EM peter.macko@netapp.com; jason.hennessey@netapp.com
CR Abadi DJ, 2012, COMPUTER, V45, P37, DOI 10.1109/MC.2012.33
   ADYA A, 2002, FARSITE FEDERATED AV
   Anderson Darrell C., 2000, INTERPOSED REQUEST R, P259
   [Anonymous], 2007, THESIS
   [Anonymous], 2002, P 5 USENIX S OP SYST
   [Anonymous], 2019, P 2019 USENIX ANN TE
   [Anonymous], 2013, P 2013 ACM SIGMOD IN, DOI DOI 10.1145/2463676.2463710
   [Anonymous], 2000, P 4 USENIX S OP SYST
   [Anonymous], 2002, P 1 USENIX C FIL STO
   [Anonymous], 2017, 100312017 IEEE
   [Anonymous], 2019, P 17 USENIX C FIL ST
   [Anonymous], 2015, P 13 USENIX C FIL ST
   Benet Juan, 2014, ARXIV1407356
   Castro M, 2000, USENIX ASSOCIATION PROCEEDINGS OF THE FOURTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P273
   CHUNG JY, 2015, P 10 EUR C COMP SYST, V16, DOI DOI 10.1186/S12868-015-0140-Z
   Cowling J, 2016, Inside the magic pocket
   Dabek F., 2001, Operating Systems Review, V35, P202, DOI 10.1145/502059.502054
   DeCandia Giuseppe, 2007, Operating Systems Review, V41, P205, DOI 10.1145/1323293.1294281
   Eisler M, 2007, USENIX Association Proceedings of the 5th Usenix Conference on File and Storage Technologies ( FAST '07), P139
   Fagin R., 1979, ACM Transactions on Database Systems, V4, P315, DOI 10.1145/320083.320092
   Freedman MJ, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE FIRST SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION (NSDI'04), P239
   Ghemawat S., 2003, Operating Systems Review, V37, P29, DOI 10.1145/1165389.945450
   Hasan R, 2005, ITCC 2005: International Conference on Information Technology: Coding and Computing, Vol 2, P205, DOI 10.1109/ITCC.2005.42
   Hildrum K., 2002, P 14 ANN ACM S PARAL, P41
   HITZ D, 1994, PROCEEDINGS OF THE WINTER 1994 USENIX CONFERENCE, P235
   HOWARD JH, 1988, ACM T COMPUT SYST, V6, P51, DOI 10.1145/35037.35059
   Huang Cheng, 2012, 2012 USENIX ANN TECH, P15
   Kadekodi S, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P345
   Kang Junbin, 2015, 2015 USENIX ANN TECH
   Karger David, 1997, P 29 ANN ACM S THEOR, P654, DOI DOI 10.1145/258533.258660
   Kesavan R, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P135
   Kubiatowicz J, 2000, ACM SIGPLAN NOTICES, V35, P190, DOI 10.1145/384264.379239
   Kuszmaul BC, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P15
   Le Preslav, 2019, WE OPTIMIZED MAGIC P
   Lockwood Glenn, 2017, WHATS SO BAD POSIX I
   Lustre, 2017, INTR LUSTR ARCH
   Maymounkov P, 2002, LECT NOTES COMPUT SC, V2429, P53
   Miltchev S, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1380584.1380588
   Muralidhar S., 2014, P 11 USENIX C OP SYS, P383
   Muthitacharoen Athicha, 2002, IVY READ WRITE PEER
   Niazi S, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P89
   Nightingale EdmundB., 2012, P 10 USENIX C OPERAT, P1
   Pacheco L, 2016, SYM REL DIST SYST, P147, DOI [10.1109/SRDS.2016.25, 10.1109/SRDS.2016.027]
   Pan S, 2021, PROCEEDINGS OF THE 19TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '21), P217
   Plank J.S., 2013, The UsenixMagazine, V38, P44
   Pomeranz I, 2007, ASIAN TEST SYMPOSIUM, P25, DOI 10.1109/ATS.2007.18
   Ramakrishnan R, 2017, SIGMOD'17: PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P51, DOI 10.1145/3035918.3056100
   REED IS, 1960, J SOC IND APPL MATH, V8, P300, DOI 10.1137/0108018
   Ren K, 2014, INT CONF HIGH PERFOR, P237, DOI 10.1109/SC.2014.25
   Rhea S., 2003, P 2 USENIX C FIL STO
   Sathiamoorthy M, 2013, PROC VLDB ENDOW, V6, P325, DOI 10.14778/2535573.2488339
   SATYANARAYANAN M, 1989, ANNU REV COMPUT SCI, V4, P73
   SATYANARAYANAN M, 1990, IEEE T COMPUT, V39, P447, DOI 10.1109/12.54838
   Shepler Spencer, 2010, 5661 RFC IETF
   Shvachko K, 2010, IEEE S MASS STOR SYS
   Stoica I, 2001, ACM SIGCOMM COMP COM, V31, P149, DOI 10.1145/964723.383071
   STONEBRAKER M., 1986, IEEE DATABASE ENG B, V9, P2
   Stribling J., 2009, NSDI, P43
   Thaler David, 1996, Technical Report CSE-TR-316-96
   Thaler DG, 1998, IEEE ACM T NETWORK, V6, P1, DOI 10.1109/90.663936
   THANH TD, 2008, P INT C NETW COMP AD, P144
   Thomson Alexander, 2012, P 2012 ACM INT C MAN
   Thomson JB, 2015, EXPL PRACT PAST EMPI, P3
   Viotti P, 2016, ACM COMPUT SURV, V49, DOI 10.1145/2926965
   Vogels W, 2009, COMMUN ACM, V52, P40, DOI 10.1145/1435417.1435432
   Wang Di, 2012, DISTRIBUTED NAMESPAC
   Weil S.A., 2006, P 2006 ACM IEEE C SU
   Weil SA, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P307
   Welch B., 1986, 6th International Conference on Distributed Computing Systems Proceedings (Cat. No. 86CH2293-9), P184
   Xu J, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P323
   Yang AB, 2015, PROCEEDINGS OF CROSS-CULTURAL OCCUPATIONAL HEALTH PSYCHOLOGY FORUM, P213
   Yang J, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P221
   Zheng Qing, 2015, P 10 PAR DAT STOR WO
   Zheng Qing, 2014, P 9 PAR DAT STOR WOR
NR 74
TC 4
Z9 4
U1 2
U2 17
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD FEB
PY 2022
VL 18
IS 1
AR 4
DI 10.1145/3465405
PG 34
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZN5OY
UT WOS:000765084700001
OA Bronze
DA 2024-07-18
ER

PT J
AU Kuszmaul, BC
   Frigo, M
   Paluska, JM
   Sandler, A
AF Kuszmaul, Bradley C.
   Frigo, Matteo
   Paluska, Justin Mazzola
   Sandler, Alexander (Sasha)
TI Everyone Loves File: Oracle File Storage Service
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Distributed filesystem; B-tree-based filesystem; Paxos; two-phase
   commit; cloud filesystem
AB Oracle File Storage Service (FSS) is an elastic filesystem provided as a managed NFS service. A pipelined Paxos implementation underpins a scalable block store that provides linearizable multipage limited-size transactions. Above the block store, a scalable B-tree holds filesystem metadata and provides linearizable multikey limited-size transactions. Self-validating B-tree nodes and housekeeping operations performed as separate transactions allow each key in a B-tree transaction to require only one page in the underlying block transaction. The filesystem provides snapshots by using versioned key-value pairs. The system is programmed using a nonblocking lock-free progranuning style. Presentation servers maintain no persistent local state making them scalable and easy to failover. A non-scalable Paxos-replicated hash table holds configuration information required to bootstrap the system. An additional B-tree provides conversational multi-key minitransactions for control-plane information. The system throughput can be predicted by comparing an estimate of the network bandwidth needed for replication to the network bandwidth provided by the hardware. Latency on an unloaded system is about 4 times higher than a Linux NFS server backed by NVMe, reflecting the cost of replication. FSS has been in production since January 2018 and holds tens of thousands of customer file systems comprising many petabytes of data.
C1 [Kuszmaul, Bradley C.; Frigo, Matteo; Paluska, Justin Mazzola; Sandler, Alexander (Sasha)] Oracle, 95 Network Dr, Burlington, MA 01803 USA.
C3 Oracle
RP Kuszmaul, BC (corresponding author), Oracle, 95 Network Dr, Burlington, MA 01803 USA.
EM kuszmaul@gmail.com; athena@fftw.org; jmp@justinmp.com;
   sasha.sandler@oracle.com
OI Kuszmaul, Bradley/0000-0001-6305-4290
CR Aguilera MK, 2009, ACM T COMPUT SYST, V27, DOI 10.1145/1629087.1629088
   Alibaba, 2018, AL EL BLOCK STOR
   Allen Hervey, 2005, P PAC NETW OP GROUP
   Amazon, 2018, AM EL FIL SYST
   Amazon, 2018, AM EL BLOCK STOR
   Amazon, 2018, AM FSX
   Amdahl G. M., 1967, P AM FED INF PROC SO, V30
   [Anonymous], 1997, 2001 IETF RFC
   [Anonymous], 2014, USENIX ANN TECHNICAL
   [Anonymous], P RFC 3168 SEPT 2001
   [Anonymous], 2003, 3530 IETF RFC
   Apache Software Foundation, 2009, ZOOKEEPER INT
   Apple Inc, 2004, TN1150 APPL INC
   Bayer R., 1972, Acta Informatica, V1, P173, DOI 10.1007/BF00288683
   Best Steve, 2000, IBM DEVELOPERWORKS
   Blumofe RD, 1996, J PARALLEL DISTR COM, V37, P55, DOI 10.1006/jpdc.1996.0107
   Boehm Hans-J., 2009, P 1 USENIX C HOT TOP
   Bolosky William J., 2011, P 8 S NETW SYST DES
   BRENT RP, 1974, J ACM, V21, P201, DOI 10.1145/321812.321815
   Brodal G S., 2012, SODA, P602, DOI 10.1137/1.9781611973099.51
   Budhiraja N., 1993, DISTRIBUTED SYSTEMS, V2, P199
   Callaghan B., 1995, 1813 IETF RFC
   Card R., 1994, PROCESSDINGS 1 DUTCH, P5
   Cascaval C, 2008, COMMUN ACM, V51, P40, DOI 10.1145/1400214.1400228
   Chandra T, 2007, PODC'07: PROCEEDINGS OF THE 26TH ANNUAL ACM SYMPOSIUM ON PRINCIPLES OF DISTRIBUTED COMPUTING, P398
   Conway A, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P45
   Corbett JC, 2013, ACM T COMPUT SYST, V31, DOI 10.1145/2491245
   DAVIDSON SB, 1985, COMPUT SURV, V17, P341, DOI 10.1145/5505.5508
   Detlefs DL, 2002, DISTRIB COMPUT, V15, P255, DOI 10.1007/s00446-002-0079-z
   Detlefs DL, 2000, LECT NOTES COMPUT SC, V1914, P59
   Dillon M., 2008, The Hammer Filesystem
   Fasheh Mark., 2006, Proceedings of the 2006 Linux Symposium, V1, P289
   Graefe G, 2010, ACM T DATABASE SYST, V35, DOI 10.1145/1806907.1806908
   GRAHAM RL, 1969, SIAM J APPL MATH, V17, P416, DOI 10.1137/0117039
   Gray Jim N., 1978, LECT NOTES COMPUTER, V60
   Harris T, 2003, ACM SIGPLAN NOTICES, V38, P388, DOI 10.1145/949343.949340
   HDFS, 2012, ADD SUPP VAR LENGTH
   HDFS, 2013, HDFS ARCH
   HERLIHY M, 1991, ACM T PROGR LANG SYS, V13, P124, DOI 10.1145/114005.102808
   Herlihy M., 1993, INT S COMPUTER ARCHI, DOI DOI 10.1145/165123.165164
   HERLIHY MP, 1990, ACM T PROGR LANG SYS, V12, P463, DOI 10.1145/78969.78972
   Hitz D., 1994, P USENIX WINT 1994 T
   Hobel Valentin, 2016, LIZARDFS SOFTWARE DE
   IBM, 1983, IBM SYST 370 EXT ARC
   IBM, 1966, DAT FIL HDB
   Jannen W, 2015, ACM T STORAGE, V11, DOI 10.1145/2798729
   Jensen E. H., 1987, Technical Report. UCRL-97663
   Jones M. Tim, 2004, CEPH LINUX PETABYTE
   Kasampalis Sakis, 2010, THESIS
   Lamport L, 1998, ACM T COMPUT SYST, V16, P133, DOI 10.1145/279227.279229
   Lamport L., 2001, ACM SIGACT NEWS, V32
   Lampson Butler, 1980, DISTRIBUTED SYSTEMS, V100
   LEHMAN PL, 1981, ACM T DATABASE SYST, V6, P650, DOI 10.1145/319628.319663
   Lev Y., 2007, P 2 ACM SIGPLAN WORK
   Lewis A. J., 2002, LVM HOWTO
   Lindsay Bruce G., 1979, RJ2571 IBM RES LAB
   Lindsay Bruce G., 1980, DISTRIBUTED DATA BAS
   Lustre, 2003, LUSTR FILE SYST
   Mathur Avantika, 2007, P LIN S, V2
   MCKUSICK MK, 1984, ACM T COMPUT SYST, V2, P181, DOI 10.1145/989.990
   Microsoft, 2018, MICR SMB PROT CIFS P
   Microsoft, 2017, MICR AZ BLOB STOR
   MILLER BP, 1990, COMMUN ACM, V33, P32, DOI 10.1145/96267.96279
   Moose, 2018, MOOSEFS FACT SHEET
   Oki B. M., 1988, Proceedings of the Seventh Annual ACM Symposium on Principles of Distributed Computing, P8, DOI 10.1145/62546.62549
   Ongaro D., 2014, USENIX ANN TECHNICAL, DOI DOI 10.5555/2643634.2643666
   Oracle, 2016, OR CLOUD INFR BLOCK
   REED IS, 1960, J SOC IND APPL MATH, V8, P300, DOI 10.1137/0108018
   Reiser Hans T., 2006, REISER4
   Ren Kai., 2013, USENIX ANN TECHNICAL, P145
   Reuter A, 1993, T PROCESSING CONCEPT
   Rodeh Ohad, 2008, ACM Transaction on Storage, V3, p15:1, DOI 10.1145/1326542.1326544
   Rodeh O, 2013, ACM T STORAGE, V9, DOI 10.1145/2501620.2501623
   Russinovich Mark, 2000, ITPROTODAY      1022
   Siebenmann Chris, 2018, WHAT ZFS GANG BLOCKS
   Siebenmann Chris, 2017, ZFSS RECORDSIZE HOLE
   Stacey Jon, 2009, JONS VIEW BLOG
   Sun Microsystems, 2006, ZFS ON DISK SPEC DRA
   Sweeney A, 1996, PROCEEDINGS OF THE USENIX 1996 ANNUAL TECHNICAL CONFERENCE, P1
   Xiang LX, 2015, SPAA'15: PROCEEDINGS OF THE 27TH ACM SYMPOSIUM ON PARALLELISM IN ALGORITHMS AND ARCHITECTURES, P234, DOI 10.1145/2755573.2755577
   Yuan J, 2017, ACM T STORAGE, V13, DOI 10.1145/3032969
   Zhan Y, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P123
NR 82
TC 0
Z9 0
U1 1
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD APR
PY 2020
VL 16
IS 1
AR 3
DI 10.1145/3377877
PG 29
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1QN
UT WOS:000583742400003
DA 2024-07-18
ER

PT J
AU Li, Y
   Chen, XB
   Zheng, N
   Hao, JP
   Zhang, T
AF Li, Yin
   Chen, Xubin
   Zheng, Ning
   Hao, Jingpeng
   Zhang, Tong
TI An Exploratory Study on Software-Defined Data Center Hard Disk Drives
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Reliability; error-tolerance; filesystem design; local erasure coding
AB This article presents a design framework aiming to reduce mass data storage cost in data centers. Its underlying principle is simple: Assume one may noticeably reduce the HDD manufacturing cost by significantly (i.e., at least several orders of magnitude) relaxing raw HDD reliability, which ensures the eventual data storage integrity via low-cost system-level redundancy. This is called system-assisted HDD bit cost reduction. To better utilize both capacity and random IOPS of HDDs, it is desirable to mix data with complementary requirements on capacity and random IOPS in each HDD. Nevertheless, different capacity and random IOPS requirements may demand different raw HDD reliability vs. bit cost trade-offs and hence different forms of system-assisted bit cost reduction. This article presents a software-centric design framework to realize data-adaptive system-assisted bit cost reduction for data center HDDs. Implementation is solely handled by the filesystem and demands only minor change of the error correction coding (ECC) module inside HDDs. Hence, it is completely transparent to all the other components in the software stack (e.g., applications, OS kernel, and drivers) and keeps fundamental HIM) design practice (e.g., firmware, media, head, and servo) intact. We carried out analysis and experiments to evaluate its implementation feasibility and effectiveness. We integrated the design techniques into ext4 to further quantitatively measure its impact on system speed performance.
C1 [Li, Yin; Chen, Xubin; Hao, Jingpeng; Zhang, Tong] Rensselaer Polytech Inst, Dept Elect Comp 8t Syst Engn, 110 8th St, Troy, NY 12180 USA.
   [Zheng, Ning] Scaleflux Inc, 97 East Brokaw Rd Suite 260, San Jose, CA 95112 USA.
C3 Rensselaer Polytechnic Institute
RP Li, Y (corresponding author), Rensselaer Polytech Inst, Dept Elect Comp 8t Syst Engn, 110 8th St, Troy, NY 12180 USA.
EM liyin1985@gmail.com; chenx22@rpi.edu; ningzhengrpi@gmail.com;
   haoj@rpi.edu; tzhang@ecse.rpi.edu
RI Zhang, tong/IAP-2587-2023; zhang, tong/JAO-3571-2023; ZHANG,
   TAO/ITV-6162-2023
FU National Science Foundation (NSF) [CNS-1814890]
FX This work is supported by the National Science Foundation (NSF) under
   grant CNS-1814890.
CR [Anonymous], 1988, A case for redundant arrays of inexpensive disks RAID, DOI DOI 10.1145/50202.50214
   [Anonymous], 2011, SIGMOD 2011
   Bairavasundaram LN, 2007, PERF E R SI, V35, P289
   Bates S., 2013, P FLASH MEM SUMM AUG, P1
   BLAUM M, 1995, IEEE T COMPUT, V44, P192, DOI 10.1109/12.364531
   Brewer E., 2016, Disks for data centers
   CHEN PM, 1994, ACM COMPUT SURV, V26, P145, DOI 10.1145/176979.176981
   Daniel EricD., 1999, MAGNETIC RECORDING 1
   Dimakis AG, 2010, IEEE T INFORM THEORY, V56, P4539, DOI 10.1109/TIT.2010.2054295
   Ferris Timothy A., 2015, US Patent, Patent No. [9,093,105, 9093105]
   Ganger Gregory R., 2003, TECHNICAL REPORT
   Greenan K.M., 2010, IEEE 26 S MASS STORA, P1, DOI DOI 10.1109/MSST.2010.5496983
   Huang CH, 2012, ELECTRON P THEOR COM, P15, DOI 10.4204/EPTCS.96.2
   Krishnan AR, 2009, IEEE T MAGN, V45, P3830, DOI 10.1109/TMAG.2009.2023233
   Kroeger T. M., 1999, Proceedings of the Seventh Workshop on Hot Topics in Operating Systems, P14, DOI 10.1109/HOTOS.1999.798371
   Kurtas E., 2005, Coding and Signal Processing for Magnetic Recording Systems
   Li Y, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P135
   Lim F, 2010, IEEE T MAGN, V46, P1548, DOI 10.1109/TMAG.2009.2038281
   Lin S., 2004, Error Control Coding, Vsecond
   Menon Jaishankar Moothedath, 2014, US Patent, Patent No. [8,645,622, 8645622]
   Mesnier M, 2004, INTERNATIONAL CONFERENCE ON AUTONOMIC COMPUTING, PROCEEDINGS, P44, DOI 10.1109/ICAC.2004.1301346
   Miura K, 2009, IEEE T MAGN, V45, P3722, DOI 10.1109/TMAG.2009.2023850
   Moser A, 2002, J PHYS D APPL PHYS, V35, pR157, DOI 10.1088/0022-3727/35/19/201
   Pavlo A, 2009, ACM SIGMOD/PODS 2009 CONFERENCE, P165
   Prabhakaran Vijayan, 2005, P OF THE 20 ACM S ON, V39, P206
   Sathiamoorthy M, 2013, PROC VLDB ENDOW, V6, P325, DOI 10.14778/2535573.2488339
   Schroeder B, 2010, ACM T STORAGE, V6, DOI 10.1145/1837915.1837917
   Seigler MA, 2008, IEEE T MAGN, V44, P119, DOI 10.1109/TMAG.2007.911029
   Shah S, 2005, P REL MAINT S, P226, DOI 10.1109/RAMS.2005.1408366
   Shiroishi Y, 2009, IEEE T MAGN, V45, P3816, DOI 10.1109/TMAG.2009.2024879
   Tanakamaru S, 2013, IEEE J SOLID-ST CIRC, V48, P2920, DOI 10.1109/JSSC.2013.2280078
   Weller D, 2014, IEEE T MAGN, V50, DOI 10.1109/TMAG.2013.2281027
   Wicker SB., 1994, Reed-Solomon Codes and Their Applications
   Wood R, 2015, IEEE T MAGN, V51, DOI 10.1109/TMAG.2015.2439634
   Zhang S, 2013, IEEE ANTENNAS PROP, P2243, DOI 10.1109/APS.2013.6711780
NR 35
TC 2
Z9 2
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2019
VL 15
IS 3
AR 18
DI 10.1145/3319405
PG 22
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JT2DC
UT WOS:000500805400003
OA Bronze
DA 2024-07-18
ER

PT J
AU Zhang, YM
   Li, DS
   Liu, L
AF Zhang, Yiming
   Li, Dongsheng
   Liu, Ling
TI Leveraging Glocality for Fast Failure Recovery in Distributed RAM
   Storage
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Globality; locality; cube-based networks; distributed RAM storage; fast
   recovery
AB Distributed RAM storage aggregates the RAM of servers in data center networks (DCN) to provide extremely high I/O performance for large-scale cloud systems. For quick recovery of storage server failures, Mem-Cube [53] exploits the proximity of the BCube network to limit the recovery traffic to the recovery servers' 1-hop neighborhood. However, the previous design is applicable only to the symmetric BCube(n, k) network with n(k+1) nodes and has suboptimal recovery performance due to congestion and contention.
   To address these problems, in this article, we propose CubeX, which (i) generalizes the "1-hop" principle of MemCube for arbitrary cube-based networks and (ii) improves the throughput and recovery performance of RAM-based key-value (KV) store via cross-layer optimizations. At the core of CubeX is to leverage the glocality (= globality + locality) of cube-based networks: It scatters backup data across a large number of disks globally distributed throughout the cube and restricts all recovery traffic within the small local range of each server node. Our evaluation shows that CubeX not only efficiently supports RAM-based KV store for cube-based networks but also significantly outperforms MemCube and RAMCIoud in both throughput and recovery time.
C1 [Zhang, Yiming; Li, Dongsheng] Natl Univ Def Technol, Sch Comp, PDL, Changsha 410073, Hunan, Peoples R China.
   [Liu, Ling] Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA.
C3 National University of Defense Technology - China; University System of
   Georgia; Georgia Institute of Technology
RP Zhang, YM; Li, DS (corresponding author), Natl Univ Def Technol, Sch Comp, PDL, Changsha 410073, Hunan, Peoples R China.
EM zhangyiming@nudt.eu.cn; dsli@nudt.edu.cn
RI Zhang, Yiming/HGB-7344-2022; Li, Dongsheng/HHC-4903-2022
FU National Natural Science Foundation of China [61772541, 61872376];
   National Science Foundation [NSF SaTC 1564097]; IBM
FX This work is supported by the National Natural Science Foundation of
   China (61772541 and 61872376), the National Science Foundation under
   Grants NSF SaTC 1564097 and IBM faculty award.
CR Aiken S, 2003, IEEE S MASS STOR SYS, P123, DOI 10.1109/MASS.2003.1194849
   Anand Ashok., 2010, NSDI, P433
   Andersen DG, 2009, SOSP'09: PROCEEDINGS OF THE TWENTY-SECOND ACM SIGOPS SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P1
   [Anonymous], 2014, FAST
   [Anonymous], 2010, SIGOPS Oper. Syst. Rev., DOI DOI 10.1145/1713254.1713276
   [Anonymous], 2011, Airways (Pty) Ltd v Aviation Union of South Africa Others 2011 (3) SA 148 (SCA) paras 25-26, P25, DOI DOI 10.1145/1989323.1989327
   [Anonymous], 2013, P 11 USENIX C FIL ST
   Antirez, UPDATE MEMCACHED RED
   Borthakur D., HDFS ARCHITECTURE GU
   Cashin L., 2005, LINUX J, V2005, P10
   Chang F, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P205
   Chidambaram V, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P228, DOI 10.1145/2517349.2522726
   Chowdhury M, 2011, ACM SIGCOMM COMP COM, V41, P98, DOI 10.1145/2043164.2018448
   Chowdhury M, 2013, ACM SIGCOMM COMP COM, V43, P231, DOI 10.1145/2534169.2486021
   Debnath B, 2010, PROC VLDB ENDOW, V3, P1414, DOI 10.14778/1920841.1921015
   Dragojevic Aleksandar, 2014, NSDI 14, P401
   Fan Bin, 2013, 10 USENIX S NETW SYS, P371
   Ferner C. S., 1992, Fourth Symposium on the Frontiers of Massively Parallel Computation (Cat. No.92CH3185-6), P254, DOI 10.1109/FMPC.1992.234951
   Fox A., 2002, Proceedings of the Twenty-eighth International Conference on Very Large Data Bases, P873
   Ghemawat S., 2003, ACM SIGOPS OPERATING, V37, P29, DOI [DOI 10.1145/945445.945450, DOI 10.1145/1165389.945450, 10.1145/1165389.945450]
   Gill P, 2011, ACM SIGCOMM COMP COM, V41, P350, DOI 10.1145/2043164.2018477
   Gray Jim., 1987, Proceedings of the 1987 ACM SIGMOD international conference on Management of data, P395
   Guo CX, 2015, ACM SIGCOMM COMP COM, V45, P139, DOI 10.1145/2785956.2787496
   Guo CX, 2009, SIGCOMM 2009, P63
   Hildebrand D, 2005, Twenty-Second IEEE/Thirteenth NASA Goddard Conference on Mass Storage Systems and Technologies, Proceedings, P18, DOI 10.1109/MSST.2005.14
   Hofmann Owen, 2012, USENIX OPERATING SYS
   Hunt Patrick, 2010, P 2010 USENIX ANN TE, P11, DOI DOI 10.5555/1855840.1855851
   Lee EK, 1996, ACM SIGPLAN NOTICES, V31, P84, DOI 10.1145/248209.237157
   Li HY, 2010, AEROSOL AIR QUAL RES, V10, P95, DOI 10.4209/aaqr.2009.08.0049
   Lim Hyeontaek, 2014, 11 USENIX S NETW SYS, P429
   Lu Guohan, 2011, P NSDI 11
   Lu X, 2013, INT J COMPUTER GAMES, V2013, P1
   Matthews Jeanna Neefe, 1997, P ACM S OP SYST PRIN
   Mickens James., 2014, Proceedings of the 11th USENIX Conference on Networked Systems Design and Implementation. NSDI'14, P257
   Miller Rich, FAILURE RATES GOOGLE
   Muralidhar S., 2014, P 11 USENIX C OP SYS, P383
   Norem Josh, SAMSUNG SSD 960 EVO
   Ongaro D, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P29
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   Rumble Stephen M., 2011, P WORKSH HOT TOP OP
   Shao Bin, 2013, SIGMOD, P505, DOI [DOI 10.1145/2463676.2467799, 10.1145/2463676.2467799]
   SOSP, 2011, SOSP 2011 PC M SOSP
   Vrable Michael., 2012, FAST, P19
   Wang Y., 2013, 10 USENIX S NETW SYS
   Wu H., 2009, PROC 5 INT C EMERG N, P25
   Zhang Y, 2015, BMC ENDOCR DISORD, V15, DOI 10.1186/s12902-015-0008-3
   Zhu YB, 2015, ACM SIGCOMM COMP COM, V45, P479, DOI 10.1145/2785956.2787483
   2006, SCI CHINA SER F, V49, P681, DOI DOI 10.1007/S11432-006-2030-6
   1995, ACM T COMPUT SYST, V13, P274
   2011, COMMUN ACM, V54, P95, DOI DOI 10.1145/1897852.1897877
NR 50
TC 17
Z9 21
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD APR
PY 2019
VL 15
IS 1
SI SI
AR 3
DI 10.1145/3289604
PG 24
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HU9GE
UT WOS:000465601900003
DA 2024-07-18
ER

PT J
AU Hatzieleftheriou, A
   Anastasiadis, SV
AF Hatzieleftheriou, Andromachi
   Anastasiadis, Stergios V.
TI Client-Side Journaling for Durable Shared Storage
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Cloud storage; distributed filesystems; crash consistency; failure
   recovery; scalability
AB Hardware consolidation in the datacenter often leads to scalability bottlenecks from heavy utilization of critical resources, such as the storage and network bandwidth. Client-side caching on durable media is already applied at block level to reduce the storage backend load but has received criticism for added overhead, restricted sharing, and possible data loss at client crash. We introduce a journal to the kernel-level client of an object-based distributed filesystem to improve durability at high I/O performance and reduced shared resource utilization. Storage virtualization at the file interface achieves clear consistency semantics across data and metadata, supports native file sharing among clients, and provides flexible configuration of durable data staging at the host. Over a prototype that we have implemented, we experimentally quantify the performance and efficiency of the proposed Arion system in comparison to a production system. We run microbenchmarks and application-level workloads over a local cluster and a public cloud. We demonstrate reduced latency by 60% and improved performance up to 150% at reduced server network and disk bandwidth by 41% and 77%, respectively. The performance improvement reaches 92% for 16 relational databases as clients and gets as high as 11.3x with two key-value stores as clients.
C1 [Hatzieleftheriou, Andromachi] Univ Ioannina, Ioannina, Greece.
   [Anastasiadis, Stergios V.] Univ Ioannina, Dept Comp Sci & Engn, GR-45110 Ioannina, Greece.
   [Hatzieleftheriou, Andromachi] Microsoft Res, 21 Stn Rd, Cambridge CB1 2FB, England.
C3 University of Ioannina; University of Ioannina; Microsoft
RP Hatzieleftheriou, A (corresponding author), Univ Ioannina, Ioannina, Greece.; Hatzieleftheriou, A (corresponding author), Microsoft Res, 21 Stn Rd, Cambridge CB1 2FB, England.
EM t-anhatz@microsoft.com; stergios@cse.uoi.gr
RI Anastasiadis, Stergios/ABC-7954-2020
OI Anastasiadis, Stergios/0000-0003-1542-7878
CR Amazon EC2, 2017, AM EC2 INST TYP
   Amazon EFS, 2015, AM EL FIL SYST
   [Anonymous], P FAST
   [Anonymous], P USENIX C FIL STOR
   [Anonymous], 1987, Concurrency Control and Recovery in Database Systems
   Appuswamy Raja, 2014, P 2014 USENIX WORKSH
   Arteaga D, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P355
   Arteaga Dulcardo, 2014, P INT C SYSTEMS STOR, P1
   Bailis P, 2013, PROC VLDB ENDOW, V7, P181, DOI 10.14778/2732232.2732237
   BAKER M, 1992, SIGPLAN NOTICES, V27, P10, DOI 10.1145/143371.143380
   Balakrishnan M, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P325, DOI 10.1145/2517349.2522732
   Barroso Luiz Andr., 2013, The Datacenter as a Computer: An Introduction to the Design of Warehouse-Scale Machines, V2nd
   Berenson H., 1995, SIGMOD Record, V24, P1, DOI 10.1145/568271.223785
   BERNSTEIN PA, 1983, ACM T DATABASE SYST, V8, P465, DOI 10.1145/319996.319998
   Bessani A., 2014, USENIX ATC 14, P169
   Bhagwat Deepavali., 2015, P 13 USENIX C FILE S, P287
   Birman KP, 2012, COMPUTER, V45, P50, DOI 10.1109/MC.2011.387
   BobMcGee, 2016, EC2 INST TYP EX NETW
   Bolosky WJ, 2000, PERF E R SI, V28, P34, DOI 10.1145/345063.339345
   Bovet D. P., 2005, Understanding the Linux Kernel: from I/O ports to process management
   Burckhardt S, 2012, LECT NOTES COMPUT SC, V7211, P67, DOI 10.1007/978-3-642-28869-2_4
   Byan S., 2012, 2012 IEEE 28th Symposium on Mass Storage Systems and Technologies, DOI 10.1109/MSST.2012.6232368
   Calder B, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P143
   Chang F, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P205
   Chidambaram V, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P228, DOI 10.1145/2517349.2522726
   Chidambaram Vijay, 2012, P 2012 USENIX C FIL, P73
   Conley M, 2015, ACM SoCC'15: Proceedings of the Sixth ACM Symposium on Cloud Computing, P302, DOI 10.1145/2806777.2806781
   Cooper Brian F., 2010, P 1 ACM S CLOUD COMP, P143, DOI [DOI 10.1145/1807128.1807152, 10.1145/1807128.1807152]
   Corbett JC, 2013, ACM T COMPUT SYST, V31, DOI 10.1145/2491245
   Dragojevic A, 2015, SOSP'15: PROCEEDINGS OF THE TWENTY-FIFTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P54, DOI 10.1145/2815400.2815425
   Eidson J., 2006, MEASUREMENT CONTROL
   Harter T, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P71
   Harter Tyler., 2014, 12 USENIX C FILE STO, P199
   Hatzieleftheriou Andromachi, 2015, P 13 USENIX C FIL ST, P59
   HERLIHY MP, 1990, ACM T PROGR LANG SYS, V12, P463, DOI 10.1145/78969.78972
   Hildebrand Dean, 2011, P 2011 USENIX WORKSH
   HOWARD JH, 1988, ACM T COMPUT SYST, V6, P51, DOI 10.1145/35037.35059
   Howells D., 2006, P LIN S, V1, P427
   IBM Spectrum, 2017, IBM SPECTR SCAL VERS
   Josephson W.K., 2010, PROC USENIX C FILE S, V6, P85
   Kazar MichaelL., 1990, Proceedings of the Summer 1990 USENIX Conference, P151
   KISTLER JJ, 1992, ACM T COMPUT SYST, V10, P3, DOI 10.1145/146941.146942
   Koller R., 2013, PROC USENIX C FILE S, P45
   Lee Dong-Yun, 2017, P 2017 IEEE INT C MA, P10
   Lee E, 2014, ACM T STORAGE, V10, DOI 10.1145/2560010
   Lu Lanyue, 2014, P USENIX S OP SYST D, P81
   Lu YY, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P773
   MANN T, 1994, ACM T COMPUT SYST, V12, P123, DOI 10.1145/176575.176577
   McGee Bob, 2016, EC2 INST TYP EX NETW
   Meyer D. T., 2011, USENIX LOGIN, V36, P6
   Meyer DT, 2008, EUROSYS'08: PROCEEDINGS OF THE EUROSYS 2008 CONFERENCE, P41, DOI 10.1145/1357010.1352598
   Mickens James., 2014, Proceedings of the 11th USENIX Conference on Networked Systems Design and Implementation. NSDI'14, P257
   MILLS DL, 1995, IEEE ACM T NETWORK, V3, P245, DOI 10.1109/90.392384
   NELSON MN, 1988, ACM T COMPUT SYST, V6, P134, DOI 10.1145/35037.42183
   Oki B. M., 1988, Proceedings of the Seventh Annual ACM Symposium on Principles of Distributed Computing, P8, DOI 10.1145/62546.62549
   Ongaro D, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P29
   Oppenheimer D, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE 4TH USENIX SYMPOSIUM ON INTERNET TECHNOLOGIES AND SYSTEMS (USITS'03), P1
   PAWLOWSKI B, 1994, PROCEEDINGS OF THE SUMMER 1994 USENIX CONFERENCE, P137
   Pfaff B, 2006, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD SYMPOSIUM ON NETWORKED SYSTEMS DESIGN & IMPLEMENTATION (NSDI 06), P353
   Qin D., 2014, USENIX ANN TECH C AT, P451
   Rajimwale A, 2011, I C DEPEND SYS NETWO, P518, DOI 10.1109/DSN.2011.5958264
   RBD, 2010, CEPHS RADOS BLOCK DE
   REED DP, 1983, ACM T COMPUT SYST, V1, P3, DOI 10.1145/357353.357355
   Schmuck F, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE FAST'02 CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P231
   Seagate, 2007, PROD MAN CHEET 15K 5
   Sheehy J, 2015, COMMUN ACM, V58, P36, DOI 10.1145/2733108
   Tarasov V., 2013, File and Storage Technologies FAST, P307
   Tarasov Vasily., 2013, HOTSTORAGE, P11
   Tarasov Vasily, 2016, USENIX; login, V41, P6
   Terry D. B., 1994, Proceedings of the Third International Conference on Parallel and Distributed Information Systems (Cat. No.94TH0668-4), P140, DOI 10.1109/PDIS.1994.331722
   The Austin Group, 2008, POSIX 1 2008, V2
   Thomas R. H., 1979, ACM Transactions on Database Systems, V4, P180, DOI 10.1145/320071.320076
   Thomson A., 2015, 13th USENIX Confer- ence on File and Storage Technologies (FAST 15), P1, DOI [DOI 10.1002/14356007.A10_173.PUB2, 10.1002/14356007.a10_173.pub2]
   Vaghani Satyam B., 2010, Operating Systems Review, V44, P57, DOI 10.1145/1899928.1899935
   van Moolenbroek David C., 2014, P 2014 ACM INT SYST
   Warfield, 2011, P 9 USENIX C FIL STO
   Weil SA, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P307
   Zhang Irene, 2014, UWCSE141201
   Zheng Wenting., 2014, 11th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 14), P465
NR 79
TC 0
Z9 0
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD DEC
PY 2017
VL 13
IS 4
SI SI
AR 36
DI 10.1145/3149372
PG 34
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FR4LD
UT WOS:000419035900009
DA 2024-07-18
ER

PT J
AU Yadgar, G
   Shor, R
AF Yadgar, Gala
   Shor, Roman
TI Experience from Two Years of Visualizing Flash with SSDPlayer
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE SSD; flash; simulation; visualization; analysis; storage system
   management
ID PERFORMANCE; DISABILITIES
AB Data visualization is a thriving field of computer science, with widespread impact on diverse scientific disciplines, from medicine and meteorology to visual data mining. Advances in large-scale storage systems, as well as low-level storage technology, played a significant role in accelerating the applicability and adoption of modern visualization techniques. Ironically, "the cobbler's children have no shoes": Researchers who wish to analyze storage systems and devices are usually limited to a variety of static histograms and basic displays.
   The dynamic nature of data movement on flash has motivated the introduction of SSDPlayer, a graphical tool for visualizing the various processes that cause data movement on solid-state drives (SSDs). In 2015, we used the initial version of SSDPlayer to demonstrate how visualization can assist researchers and developers in their understanding of modern, complex flash-based systems. While we continued to use SSDPlayer for analysis purposes, we found it extremely useful for education and presentation purposes as well. In this article, we describe our experience from two years of using, sharing, and extending SSDPlayer and how similar techniques can further advance storage systems research and education.
C1 [Yadgar, Gala; Shor, Roman] Technion, Comp Sci Dept, Haifa, Israel.
   [Yadgar, Gala; Shor, Roman] Technion Israel Inst Technol, Comp Sci Dept, IL-3200003 Haifa, Israel.
C3 Technion Israel Institute of Technology; Technion Israel Institute of
   Technology
RP Yadgar, G (corresponding author), Technion, Comp Sci Dept, Haifa, Israel.; Yadgar, G (corresponding author), Technion Israel Inst Technol, Comp Sci Dept, IL-3200003 Haifa, Israel.
EM gala@cs.technion.ac.il; shroman@cs.technion.ac.il
OI Yadgar, Gala/0000-0003-2701-0260
FU GIF [I-1356-407.6/2016]
FX This work was partially supported by GIF grant no. I-1356-407.6/2016.
CR Aghayev Abutalib, 2015, P 13 USENIX C FIL ST
   Agrawal Nitin, 2008, 2008 USENIX ANN TECH
   [Anonymous], 2013, HOTSTORAGE
   [Anonymous], 2010, P 9 USENIX C OP SYST
   [Anonymous], 2006, COOPERATIVE CACHING
   [Anonymous], 2015, H117527 EMC
   [Anonymous], 2012, P USENIX ANN TECHN C
   [Anonymous], 2011, P FAST 2
   [Anonymous], 2006, P 7 S OP SYST DES IM
   [Anonymous], 2008, P C FIL STOR TECHN F
   Azuma RT, 1997, PRESENCE-VIRTUAL AUG, V6, P355, DOI 10.1162/pres.1997.6.4.355
   Balakrishnan M, 2010, ACM T STORAGE, V6, DOI 10.1145/1807060.1807061
   Bux W, 2010, PERFORM EVALUATION, V67, P1172, DOI 10.1016/j.peva.2010.07.003
   Carter J., 2001, IEEE Transactions on Professional Communication, V44, P225, DOI 10.1109/47.968105
   Chen CM, 2010, WIRES COMPUT STAT, V2, P387, DOI 10.1002/wics.89
   Colgrove J., 2015, P ACM SIGMOD INT C M
   Crow KL, 2008, TECHTRENDS, V52, P51
   Desnoyers P, 2014, ACM T STORAGE, V10, DOI 10.1145/2577384
   Desnoyers Peter, 2013, P 5 USENIX WORKSH HO
   DIBIASE D, 1992, CARTOGR GEOGR INFORM, V19, P201, DOI 10.1559/152304092783721295
   Douglis Fred, 2017, P USENIX C FIL STOR
   Friendly M, 2008, A Brief History of Data Visualization, P15, DOI [10.1007/978-3-540-33037-0_2, DOI 10.1007/978-3-540-33037-0_2]
   Fu Min, 2014, P USENIX ANN TECHN C
   GILL B., 2008, P USENIX C FIL STOR
   GILL B. S., 2005, P USENIX ANN TECHN C
   Gill Binny S., 2007, USENIX C FIL STOR TE
   Greenan Kevin M., 2009, P 5 WORKSH HOT TOP S
   Griffith D. A., 2013, Spatial autocorrelation and spatial filtering:Gaining understanding through theory and scientific visualization
   Guz Zvika, 2008, P ANN S PAR ALG ARCH
   Im S, 2011, IEEE T COMPUT, V60, P80, DOI 10.1109/TC.2010.197
   Jauhari Rajiv, 1990, P INT C VERY LARGE D
   Jefferson Luke., 2006, P 8 INT ACM SIGACCES, P40, DOI DOI 10.1145/1168987.1168996
   Johnson C, 2004, IEEE COMPUT GRAPH, V24, P13, DOI 10.1109/MCG.2004.20
   Kandlikar Yogesh, 2016, Patent, Patent No. [9,383,892, 9383892]
   Keim DA, 2002, IEEE T VIS COMPUT GR, V8, P1, DOI 10.1109/2945.981847
   Kim Jaeho, 2013, P 43 ANN IEEE IFIP I
   Kim Y., 2009, P 1 INT C ADV SYST S
   Laden Guy, 2007, P USENIX C FIL STOR
   Lagueux Richard A., 2003, Patent, Patent No. [6,538,669, 6538669]
   Laramee R., 2014, Future Challenges and Unsolved Problems in Multi-field Visualization, P205, DOI DOI 10.1007/978-1-4471-6497-5
   Lautenschlager Florian, 2017, P USENIX C FIL STOR
   Lee Sehwan, 2011, P ACM S APPL COMP SA
   Lu XJ, 2003, NUCLEIC ACIDS RES, V31, P5108, DOI 10.1093/nar/gkg680
   Margaglia F, 2016, P 14 USENIX C FIL ST
   Margaglia Fabio, 2015, P IEEE 31 S MASS STO
   Megiddo N., 2003, P USENIX C FIL STOR
   Moskewicz Matthew W., 2001, P 38 ANN DES AUT C D
   Naps T., 2003, SIGCSE Bulletin, V35, P124, DOI 10.1145/960492.960540
   Naps Thomas L., 2002, P WORK GROUP REP ITI
   Narayanan D., 2008, ACM T STORAGE, V4, P3
   Odeh Saher, 2014, P 30 INT C MASS STOR
   Oh Yongseok., 2012, FAST, V12
   Presmeg N.C., 2006, Handbook of research on the psychology of mathematics education, P205, DOI DOI 10.1163/9789087901127_009
   Reinecke Philipp, 2017, HPE201705
   Shaull Ross, 2014, P USENIX ANN TECHN C
   Shor Roman, 2017, SSDPLAYER VISUALIZAT
   Siglead Inc, 2012, SIGNAS 2 SIGL NAND A
   Srinivasan K., 2012, P USENIX C FIL STOR
   Stoica R, 2013, PROC VLDB ENDOW, V6, P733, DOI 10.14778/2536360.2536372
   Sungjin Lee, 2008, Operating Systems Review, V42, P36, DOI 10.1145/1453775.1453783
   Takagi H., 2003, ACM SIGACCESS ACCESS, V77, P177
   Weil Sage A., P PDSW 2007
   Weil Sage A., 2006, P ACM IEEE C SUP SC
   Wires Jake, 2017, P USENIX C FIL STOR
   Wise J. A., 1995, P IEEE S INF VIS INF
   Wong B, 2011, NAT METHODS, V8, P441, DOI 10.1038/nmeth.1618
   Wong T.M., 2002, P USENIX ANN TECHN C
   Xiang Luojie, 2012, P IEEE GLOB COMM C G
   Yaakobi Eitan, 2015, P IEEE INT S INF THE
   Yadgar G., 2015, USENIX C FIL STOR TE, P257
   YADGAR G, 2007, P USENIX C FIL STOR
   Yadgar G, 2011, ACM T COMPUT SYST, V29, DOI 10.1145/1963559.1963561
   Yadgar Gala, 2017, SSDPLAYER VISUALIZAT
   Yadgar Gala, 2015, P 7 USENIX WORKSH HO
   Yadgar Gala, 2013, P 29 IEEE S MASS STO
   Zhang Zhe., 2008, P 28 IEEE INT C DIST
   Zhu Benjamin, 2008, P USENIX C FIL STOR
NR 77
TC 1
Z9 1
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD DEC
PY 2017
VL 13
IS 4
SI SI
AR 31
DI 10.1145/3149356
PG 24
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FR4LD
UT WOS:000419035900004
DA 2024-07-18
ER

PT J
AU Lee, S
   Shin, D
   Kim, Y
   Kim, J
AF Lee, Sungjin
   Shin, Dongkun
   Kim, Youngjin
   Kim, Jihong
TI Exploiting Sequential and Temporal Localities to Improve Performance of
   NAND Flash-Based SSDs
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE NAND Flash Memory; Solid-State Drives; Storage Systems; Flash
   translation layer; address mapping; garbage collection
ID TRANSLATION LAYER; MEMORY
AB NAND flash-based Solid-State Drives (SSDs) are becoming a viable alternative as a secondary storage solution for many computing systems. Since the physical characteristics of NAND flash memory are different from conventional Hard-Disk Drives (HDDs), flash-based SSDs usually employ an intermediate software layer, called a Flash Translation Layer (FTL). The FTL runs several firmware algorithms for logical-to-physical mapping, I/O interleaving, garbage collection, wear-leveling, and so on. These FTL algorithms not only have a great effect on storage performance and lifetime, but also determine hardware cost and data integrity. In general, a hybrid FTL scheme has been widely used in mobile devices because it exhibits high performance and high data integrity at a low hardware cost. Recently, a demand-based FTL based on page-level mapping has been rapidly adopted in high-performance SSDs. The demand-based FTL more effectively exploits the device-level parallelism than the hybrid FTL and requires a small amount of memory by keeping only popular mapping entries in DRAM. Because of this caching mechanism, however, the demand-based FTL is not robust enough for power failures and requires extra reads to fetch missing mapping entries from NAND flash. In this article, we propose a new flash translation layer called LAST++. The proposed LAST++ scheme is based on the hybrid FTL, thus it has the inherent benefits of the hybrid FTL, including low resource requirements, strong robustness for power failures, and high read performance. By effectively exploiting the locality of I/O references, LAST++ increases device-level parallelism and reduces garbage collection overheads. This leads to a great improvement of I/O performance and makes it possible to overcome the limitations of the hybrid FTL. Our experimental results show that LAST++ outperforms the demand-based FTL by 27% for writes and 7% for reads, on average, while offering higher robustness against sudden power failures. LAST++ also improves write performance by 39%, on average, over the existing hybrid FTL.
C1 [Lee, Sungjin] MIT, Comp Sci & Artificial Intelligence Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
   [Lee, Sungjin] Inha Univ, Dept Comp Sci & Informat Engn, Inchon, South Korea.
   [Shin, Dongkun] Sungkyunkwan Univ, Coll Informat & Commun Engn, Suwon, Gyeonggi Do, South Korea.
   [Kim, Youngjin] Ajou Univ, Dept Elect & Comp Engn, Suwon 441749, South Korea.
   [Kim, Jihong] Seoul Natl Univ, Seoul 151, South Korea.
C3 Massachusetts Institute of Technology (MIT); Inha University;
   Sungkyunkwan University (SKKU); Ajou University; Seoul National
   University (SNU)
RP Lee, S (corresponding author), MIT, Comp Sci & Artificial Intelligence Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USA.; Lee, S (corresponding author), Inha Univ, Dept Comp Sci & Informat Engn, Inchon, South Korea.
EM sungjin.lee@inha.ac.kr; dongkun@skku.edu; youngkim@ajou.ac.kr;
   jihong@davinci.snu.ac.kr
FU National Research Foundation of Korea (NRF) grant
   [NRF-2013R1A6A3A03063762]; National Research Foundation of Korea (NRF)
   grant - Ministry of Science, ICT and Future Planning (MSIP)
   [NRF-2013R1A2A2A01068260]
FX This work was supported by the National Research Foundation of Korea
   (NRF) grant (NRF-2013R1A6A3A03063762). The work of Jihong Kim was
   supported by the National Research Foundation of Korea (NRF) grant
   funded by the Ministry of Science, ICT and Future Planning (MSIP)
   (NRF-2013R1A2A2A01068260). The ICT at Seoul National University and IDEC
   provided research facilities for this study.
CR Agrawal Nitin, 2008, P USENIX ANN TECHN C, P57
   [Anonymous], 2011, P 27 S MASS STOR SYS
   Ban Amir, 1995, US Patent, Patent No. [5,404,485, 5404485]
   Birrell A., 2007, Operating Systems Review, V41, P88, DOI 10.1145/1243418.1243429
   Chang L.-P., 2007, SAC '07: Proc. of the 2007 ACM symposium on Applied computing, P1126
   Chang LP, 2010, IEEE T COMPUT, V59, P1337, DOI 10.1109/TC.2010.14
   Chiang ML, 1999, J SYST SOFTWARE, V48, P213, DOI 10.1016/S0164-1212(99)00059-X
   Cho H, 2009, DES AUT TEST EUROPE, P507
   Gupta A, 2009, ACM SIGPLAN NOTICES, V44, P229, DOI 10.1145/1508284.1508271
   Han-Joon Kim, 1999, Proceedings. Twenty-Third Annual International Computer Software and Applications Conference (Cat. No.99CB37032), P284, DOI 10.1109/CMPSAC.1999.812717
   Heileman G. L., 2005, Proc. 7th ALENEX, P141
   Johnson T., 1994, P 20 INT C VER LARG, P439
   Kang J., 2006, Proceedings of the International Conference on Embedded Software (EMSOFT), P161
   Kim J, 2002, IEEE T CONSUM ELECTR, V48, P366
   Lawton G, 2006, COMPUTER, V39, P16, DOI 10.1109/MC.2006.22
   Lee S., 2009, Proceedings of the 2009 Conference on USENIX Annual Technical Conference, USENIX'09, USENIX Association, Berkeley, CA, USA, P9
   Lee SW, 2007, ACM T EMBED COMPUT S, V6, DOI 10.1145/1275986.1275990
   Leibowitz B, 2010, IEEE J SOLID-ST CIRC, V45, P889, DOI 10.1109/JSSC.2010.2040230
   Micron Technology Inc, 2012, MT29F16G08 MLC NAND
   Moon S, 2010, LECT NOTES COMPUT SC, V6399, P13, DOI 10.1007/978-3-642-16256-5_4
   MORRIS R, 1968, COMMUN ACM, V11, P38, DOI 10.1145/362851.362882
   Park D, 2010, PERF E R SI, V38, P365, DOI 10.1145/1811099.1811089
   Park SH, 2014, IEEE T COMPUT, V63, P1085, DOI 10.1109/TC.2012.281
   Park SH, 2009, IEEE T CONSUM ELECTR, V55, P1392, DOI 10.1109/TCE.2009.5278005
   Sang-Phil Lim, 2010, Proceedings of the 2010 International Workshop on Storage Network Architecture and Parallel I/Os (SNAPI 2010), P3, DOI 10.1109/SNAPI.2010.9
   Shim G, 2012, IEEE INT C COMPUT, P445, DOI 10.1109/ICCSE.2012.68
   Sungjin Lee, 2008, Operating Systems Review, V42, P36, DOI 10.1145/1453775.1453783
   Thontirawong Peera, 2014, 2014 International Computer Science and Engineering Conference (ICSEC), P421, DOI 10.1109/ICSEC.2014.6978234
   Tucek Joseph, 2013, P 11 USENIX C FIL ST
   Xu ZY, 2012, IEEE IPCCC, P142, DOI 10.1109/PCCC.2012.6407747
NR 30
TC 8
Z9 11
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUN
PY 2016
VL 12
IS 3
AR 15
DI 10.1145/2905054
PG 39
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DQ7YV
UT WOS:000379426200006
DA 2024-07-18
ER

PT J
AU Mao, B
   Jiang, H
   Wu, SZ
   Fu, YJ
   Tian, L
AF Mao, Bo
   Jiang, Hong
   Wu, Suzhen
   Fu, Yinjin
   Tian, Lei
TI Read-Performance Optimization for Deduplication-Based Storage Systems in
   the Cloud
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Storage systems; data deduplication; virtual machine; solid-state drive;
   read performance; Design; Performance
AB Data deduplication has been demonstrated to be an effective technique in reducing the total data transferred over the network and the storage space in cloud backup, archiving, and primary storage systems, such as VM ( virtual machine) platforms. However, the performance of restore operations from a deduplicated backup can be significantly lower than that without deduplication. The main reason lies in the fact that a file or block is split into multiple small data chunks that are often located in different disks after deduplication, which can cause a subsequent read operation to invoke many disk IOs involving multiple disks and thus degrade the read performance significantly. While this problem has been by and large ignored in the literature thus far, we argue that the time is ripe for us to pay significant attention to it in light of the emerging cloud storage applications and the increasing popularity of the VM platform in the cloud. This is because, in a cloud storage or VM environment, a simple read request on the client side may translate into a restore operation if the data to be read or a VM suspended by the user was previously deduplicated when written to the cloud or the VM storage server, a likely scenario considering the network bandwidth and storage capacity concerns in such an environment.
   To address this problem, in this article, we propose SAR, an SSD (solid-state drive)-Assisted Read scheme, that effectively exploits the high random-read performance properties of SSDs and the unique data-sharing characteristic of deduplication-based storage systems by storing in SSDs the unique data chunks with high reference count, small size, and nonsequential characteristics. In this way, many read requests to HDDs are replaced by read requests to SSDs, thus significantly improving the read performance of the deduplicationbased storage systems in the cloud. The extensive trace-driven and VM restore evaluations on the prototype implementation of SAR show that SAR outperforms the traditional deduplication-based and flash-based cache schemes significantly, in terms of the average response times.
C1 [Mao, Bo; Wu, Suzhen] Xiamen Univ, Xiamen 361005, Peoples R China.
   [Jiang, Hong; Tian, Lei] Univ Nebraska, Lincoln, NE USA.
   [Fu, Yinjin] Natl Univ Def Technol, Changsha 410073, Hunan, Peoples R China.
C3 Xiamen University; University of Nebraska System; University of Nebraska
   Lincoln; National University of Defense Technology - China
RP Wu, SZ (corresponding author), Xiamen Univ, Xiamen 361005, Peoples R China.
EM suzhen@xmu.edu.cn
RI Tian, Lei/I-9575-2014; Fu, Yinjin/AFX-1702-2022
OI Tian, Lei/0000-0002-8990-4799; Fu, Yinjin/0000-0001-9107-1338
FU China National Science Foundation [61100033]; US NSF [NSF-CNS-1116606,
   NSF-CNS-1016609, NSF-IIS-0916859]; Scientific Research Foundation for
   the Returned Overseas Chinese Scholars; State Education Ministry; Huawei
   Innovation Research Program
FX This work is supported by the China National Science Foundation no.
   61100033, the US NSF under Grant No. NSF-CNS-1116606, NSF-CNS-1016609,
   NSF-IIS-0916859, the Scientific Research Foundation for the Returned
   Overseas Chinese Scholars, State Education Ministry, and the Huawei
   Innovation Research Program.
CR Andersen David G., 2009, P ACM SIGOPS 22 S OP
   [Anonymous], UCSCSSRC1103
   [Anonymous], 2009, P USENIX ANN TECHNIC
   [Anonymous], P USENIX ANN TECHN C
   [Anonymous], 2010, P USENIX C USENIX AN
   [Anonymous], 2009, P 14 INT C ARCH SUPP
   [Anonymous], P 8 USENIX C FIL STO
   [Anonymous], 2011, P 9 USENIX C FIL STO
   [Anonymous], 2008, USENIX FAST
   [Anonymous], 2011, P 9 USENIX C FIL STO
   [Anonymous], P USENIX ANN TECHN C
   [Anonymous], 2011, PROC USENIX C FILE S
   [Anonymous], 2010, P 8 USENIX C FIL STO
   Armbrust M., 2009, USBEECS200928 U CAL
   Bhagwat D., 2006, P 14 IEEE INT S MOD
   El-Shimi A., 2012, P USENIX ANN TECHN C
   ESG, 2008, DAT PROT SURV
   Guo F., 2011, P USENIX ANN TECHN C
   Gupta Diwaker, 2008, P 8 USENIX S OP SYST
   Hansen J., 2010, P 1 ACM S CLOUD COMP
   Himelstein M., 2011, P SNW SPRING 2011
   Jin K., 2009, P ACM SYSTOR
   Kim Y., 2008, CSE08017 PENNS STAT
   Koller R., 2010, P 8 USENIX C FIL STO
   Koltsidas I, 2008, PROC VLDB ENDOW, V1, P514, DOI 10.14778/1453856.1453913
   Lillibridge M., 2009, P USENIX FAST
   Lillibridge Mark, 2013, P 11 USENIX C FIL ST
   Meister D., 2010, P IEEE 26 S MASS STO
   Meister D., 2012, P INT C HIGH PERF CO
   Muthitacharoenand A., 2001, P 18 ACM S OP SYST P
   Nath P., 2008, P 17 INT S HIGH PERF
   Polte M., 2008, P 3 PET DAT STOR WOR
   Quinlan S., 2002, P 1 USENIX C FIL STO
   Ren J., 2010, P 30 INT C DISTR COM
   Rhea S., 2008, P USENIX ANN TECHN C
   Srinivasan Kiran, 2012, P 10 USENIX C FIL ST
   Tan Y., 2011, P IEEE INT PAR DISTR
   Xiao W., 2008, P 28 INT C DISTR COM
   Yang T., 2010, P IEEE INT S PAR DIS
   Zhang X., 2010, P IEEE INT C CLUST C
   Zhu Q, 2005, P 20 ACM S OP SYST P, P177, DOI DOI 10.1145/1095809.1095828
NR 41
TC 47
Z9 115
U1 0
U2 18
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAR
PY 2014
VL 10
IS 2
AR 6
DI 10.1145/2512348
PG 22
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AF2CV
UT WOS:000334521100002
DA 2024-07-18
ER

PT J
AU Luo, XH
   Shu, JW
AF Luo, Xianghong
   Shu, Jiwu
TI Generalized X-Code: An Efficient RAID-6 Code for Arbitrary Size of Disk
   Array
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Algorithms; Performance; Reliability; Theory; Generalized X-code;
   computational complexity; number of disks; size of disk array; storage
AB Many RAID-6 codes have been proposed in the literature, but each has its limitations. Horizontal code has the ability to adapt to the arbitrary size of a disk array but its high computational complexity is a major shortcoming. In contrast, the computational complexity of vertical code (e.g. X-code) often achieves the theoretical optimality, but vertical code is limited to using a prime number as the size of the disk array In this article, we propose a novel efficient RAID-6 code for arbitrary size of disk array: generalized X-code. We move the redundant elements along their calculation diagonals in X-code onto two specific disks and change two data elements into redundant elements in order to realize our new code. The generalized X-code achieves optimal encoding and updating complexity and low decoding complexity; in addition, it has the ability to adapt to arbitrary size of disk array. Furthermore, we also provide a method for generalizing horizontal code to achieve optimal encoding and updating complexity while keeping the code's original ability to adapt to arbitrary size of disk array.
C1 [Luo, Xianghong; Shu, Jiwu] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
C3 Tsinghua University
RP Shu, JW (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
EM luo-xh09@mails.tsinghua.edu.cn; shujw@tsinghua.edu.cn
FU National Natural Science Foundation of China [60925006]; National Grand
   Fundamental Research 863 Program of China [2009AA01A403]; Intel
   International Cooperation Program [Intel-CRC-2010-06]
FX This work is supported by the National Natural Science Foundation of
   China (Grant No. 60925006), the National Grand Fundamental Research 863
   Program of China (Grant No. 2009AA01A403) and Intel International
   Cooperation Program (Intel-CRC-2010-06).
CR [Anonymous], 2004, P 3 USENIX C FIL STO, P2
   [Anonymous], P 2010 INT C IEEE MA
   Bairavasundaram LN, 2007, PERF E R SI, V35, P289
   BLAUM M, 1995, IEEE T COMPUT, V44, P192, DOI 10.1109/12.364531
   Blaum M, 1999, IEEE T INFORM THEORY, V45, P46, DOI 10.1109/18.746771
   Blaum M, 1996, IEEE T INFORM THEORY, V42, P529, DOI 10.1109/18.485722
   CHEN PM, 1994, ACM COMPUT SURV, V26, P145, DOI 10.1145/176979.176981
   Hafner JL, 2005, USENIX ASSOCIATION PROCEEDINGS OF THE 4TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P211
   HAFNER JL, 2004, 10321 RJ IBM RES
   Jin C, 2009, ICS'09: PROCEEDINGS OF THE 2009 ACM SIGARCH INTERNATIONAL CONFERENCE ON SUPERCOMPUTING, P360
   LI M, 2009, ACM T STORAGE, V4, P1
   PATTERSON D, 1989, P IEEE COMPCON SPRIN, P112
   Patterson D. A., 1988, SIGMOD Record, V17, P109, DOI 10.1145/971701.50214
   Pinheiro E, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 5TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES ( FAST '07), P17
   Plank JS, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P97
   Plank JS, 2009, INT J HIGH PERFORM C, V23, P242, DOI 10.1177/1094342009106191
   Schroeder B, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 5TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES ( FAST '07), P1
   Xu LH, 1999, IEEE T INFORM THEORY, V45, P1817, DOI 10.1109/18.782102
   Xu LH, 1999, IEEE T INFORM THEORY, V45, P272, DOI 10.1109/18.746809
NR 19
TC 3
Z9 4
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD SEP
PY 2012
VL 8
IS 3
AR 10
DI 10.1145/2339118.2339121
PG 16
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 014BR
UT WOS:000309350600003
DA 2024-07-18
ER

PT J
AU Mao, B
   Jiang, H
   Wu, SZ
   Tian, L
   Feng, D
   Chen, JX
   Zeng, LF
AF Mao, Bo
   Jiang, Hong
   Wu, Suzhen
   Tian, Lei
   Feng, Dan
   Chen, Jianxi
   Zeng, Lingfang
TI HPDA: A Hybrid Parity-Based Disk Array for Enhanced Performance and
   Reliability
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Performance; Reliability; Storage systems; RAID; SSD;
   reliability; performance
ID RAID
AB Flash-based Solid State Drive (SSD) has been productively shipped and deployed in large scale storage systems. However, a single flash-based SSD cannot satisfy the capacity, performance and reliability requirements of the modern storage systems that support increasingly demanding data-intensive computing applications. Applying RAID schemes to SSDs to meet these requirements, while a logical and viable solution, faces many challenges. In this article, we propose a Hybrid Parity-based Disk Array architecture (short for HPDA), which combines a group of SSDs and two hard disk drives (HDDs) to improve the performance and reliability of SSD-based storage systems. In HPDA, the SSDs (data disks) and part of one HDD (parity disk) compose a RAID4 disk array. Meanwhile, a second HDD and the free space of the parity disk are mirrored to form a RAID1-style write buffer that temporarily absorbs the small write requests and acts as a surrogate set during recovery when a disk fails. The write data is reclaimed to the data disks during the lightly loaded or idle periods of the system. Reliability analysis shows that the reliability of HPDA, in terms of MTTDL (Mean Time To Data Loss), is better than that of either pure HDD-based or SSD-based disk array. Our prototype implementation of HPDA and the performance evaluations show that HPDA significantly outperforms either HDD-based or SSD-based disk array.
C1 [Mao, Bo; Jiang, Hong; Tian, Lei] Univ Nebraska, Lincoln, NE 68583 USA.
   [Wu, Suzhen] Xiamen Univ, Xiamen, Peoples R China.
   [Feng, Dan; Chen, Jianxi; Zeng, Lingfang] Huazhong Univ Sci & Technol, Wuhan 430074, Peoples R China.
C3 University of Nebraska System; University of Nebraska Lincoln; Xiamen
   University; Huazhong University of Science & Technology
RP Wu, SZ (corresponding author), Univ Nebraska, Lincoln, NE 68583 USA.
EM suzhen@xmu.edu.cn
RI Tian, Lei/I-9575-2014
OI Tian, Lei/0000-0002-8990-4799; Zeng, Lingfang/0000-0003-3130-3015
FU US NSF [NSF-CNS-1016609, NSF-IIS-0916859]; National Basic Research 973
   Program of China [2011CB302301]; China National Science Foundation
   [61025008, 61100033]; Fundamental Research Funds for Central
   Universities [2010121066]; Direct For Computer & Info Scie & Enginr;
   Division of Computing and Communication Foundations [0937988] Funding
   Source: National Science Foundation; Division Of Computer and Network
   Systems; Direct For Computer & Info Scie & Enginr [1117032] Funding
   Source: National Science Foundation
FX This work was supported by the US NSF under Grant No. NSF-CNS-1016609
   and NSF-IIS-0916859, and the National Basic Research 973 Program of
   China under Grant No. 2011CB302301, the China National Science
   Foundation No. 61025008 and No. 61100033 and the Fundamental Research
   Funds for Central Universities (No. 2010121066).
CR Agrawal N, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 5TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES ( FAST '07), P31
   Agrawal Nitin, 2008, P USENIX ANN TECHN C, P57
   Andersen DG, 2009, SOSP'09: PROCEEDINGS OF THE TWENTY-SECOND ACM SIGOPS SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P1
   [Anonymous], UND FLASH TRANSL LAY
   Balakrishnan M, 2010, EUROSYS'10: PROCEEDINGS OF THE EUROSYS 2010 CONFERENCE, P15
   Chen F, 2009, PERF E R SI, V37, P181
   Chen Feng, 2011, P 9 USENIX C FIL STO
   Chen Y, 2000, LECT NOTES COMPUT SC, V1900, P1302
   CHEN Z., 2005, P ACM SIGOPS 20 S OP, P177
   DIRIK C, 2009, P 36 INT S COMP ARCH
   Elerath JG, 2007, I C DEPEND SYS NETWO, P175, DOI 10.1109/DSN.2007.41
   FENG D., 2008, P 16 ANN M IEEE INT, P113
   GOLDING R, 1995, PROCEEDINGS OF THE 1995 USENIX TECHNICAL CONFERENCE, P201
   GREENAN K. M., 2010, P WORKSH HOT TOP STO
   Greenan Kevin M., 2009, P 5 WORKSH HOT TOP S
   GUPTA A., 2009, P 14 INT C ARCH SUPP, P229
   Gupta A., 2011, FAST, P91
   Hu YM, 1996, 23RD ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE, PROCEEDINGS, P169, DOI 10.1145/232974.232991
   Hu YM, 1999, FIFTH INTERNATIONAL SYMPOSIUM ON HIGH-PERFORMANCE COMPUTER ARCHITECTURE, PROCEEDINGS, P204, DOI 10.1109/HPCA.1999.744364
   Im S, 2011, IEEE T COMPUT, V60, P80, DOI 10.1109/TC.2010.197
   KADAV A., 2009, P WORKSH HOT TOP STO
   Kim H, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P239, DOI 10.1145/1378600.1378627
   Kim Y., 2008, MixedStore: An enterprise-scale storage system combining Solid-state and Hard Disk Drives
   Kim Y, 2011, IEEE S MASS STOR SYS
   Koltsidas I, 2008, PROC VLDB ENDOW, V1, P514, DOI 10.14778/1453856.1453913
   MENON J., 1995, P 4 INT S HIGH PERF
   Mi NF, 2008, LECT NOTES COMPUT SC, V5346, P265
   Mogi K., 1996, SIGMOD Record, V25, P183, DOI 10.1145/235968.233331
   NARAYANAN D., 2009, P 4 EUR C COMP SYST
   PARIS J., 2009, P 17 ANN M IEEE INT, P1
   Patterson D. A., 1988, SIGMOD Record, V17, P109, DOI 10.1145/971701.50214
   Riska A, 2006, USENIX ASSOCIATION PROCEEDINGS OF THE 2006 USENIX ANNUAL TECHNICAL CONFERENCE, P97
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   RUEMMLER C, 1993, PROCEEDINGS OF THE WINTER 1993 USENIX CONFERENCE, P405
   Savage S, 1996, PROCEEDINGS OF THE USENIX 1996 ANNUAL TECHNICAL CONFERENCE, P27
   Schroeder B, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 5TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES ( FAST '07), P1
   Soundararajan Gokul., 2010, Proceedings of the 8th USENIX conference on File and storage technologies, FAST'10, P8
   STODOLSKY D, 1993, CONF PROC INT SYMP C, P64, DOI 10.1145/173682.165143
   Storer MW, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P1, DOI 10.1145/1456469.1456471
   Tian L, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 5TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES ( FAST '07), P277
   Uysal M, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE 2ND USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST'03), P89
   Wilkes J., 1995, Operating Systems Review, V29, P96, DOI 10.1145/224057.224065
   Wu S., 2009, USENIX C FILE STORAG, V9, P239
   Xie T, 2010, IEEE T PARALL DISTR, V21, P1330, DOI 10.1109/TPDS.2009.175
NR 44
TC 45
Z9 57
U1 0
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD FEB
PY 2012
VL 8
IS 1
AR 4
DI 10.1145/2093139.2093143
PG 20
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 990LT
UT WOS:000307633100004
DA 2024-07-18
ER

PT J
AU Josephson, WK
   Bongo, LA
   Li, K
   Flynn, D
AF Josephson, William K.
   Bongo, Lars A.
   Li, Kai
   Flynn, David
TI DFS: A File System for Virtualized Flash Storage
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Measurement; Performance; Flash memory; filesystem
AB We present the design, implementation, and evaluation of Direct File System (DFS) for virtualized flash storage. Instead of using traditional layers of abstraction, our layers of abstraction are designed for directly accessing flash memory devices. DFS has two main novel features. First, it lays out its files directly in a very large virtual storage address space provided by FusionIO's virtual flash storage layer. Second, it leverages the virtual flash storage layer to perform block allocations and atomic updates. As a result, DFS performs better and is much simpler than a traditional Unix file system with similar functionalities. Our microbenchmark results show that DFS can deliver 94,000 I/O operations per second (IOPS) for direct reads and 71,000 IOPS for direct writes with the virtualized flash storage layer on FusionIO's ioDrive. For direct access performance, DFS is consistently better than ext3 on the same platform, sometimes by 20%. For buffered access performance, DFS is also consistently better than ext3, and sometimes by over 149%. Our application benchmarks show that DFS outperforms ext3 by 7% to 250% while requiring less CPU power.
C1 [Josephson, William K.; Bongo, Lars A.; Li, Kai] Princeton Univ, Princeton, NJ 08544 USA.
C3 Princeton University
RP Josephson, WK (corresponding author), Princeton Univ, Princeton, NJ 08544 USA.
EM wkj@cs.princeton.edu
CR AGRAWAL N, 2008, P USENIX TECHN C
   [Anonymous], 2009, FUSIONIO IODRIVE SPE
   [Anonymous], 2008, P C FIL STOR TECHN F
   BIRRELL A., 2007, ACM OPER SYST REV, V41, P2
   Brants T., 2006, Web 1T 5-gram Version 1, pLDC2006T13
   Card R., 1994, PROCESSDINGS 1 DUTCH, P5
   CARLSON A, 2008, CMUML08107
   Douceur J. R., 1999, P ACM SIGMETRICS INT
   Douglis F., 1994, Proceedings of the First USENIX Symposium on Operating Systems Design and Implementation (OSDI), P25
   HITZ D., 2001, TR3002 NETAPPCORP
   INTEL CORPORATION, 2009, INT X25 E SATA SOL S
   Intel Corporation, 1998, UND FLASH TRANSL LAY
   Jo H, 2006, IEEE T CONSUM ELECTR, V52, P485, DOI 10.1109/TCE.2006.1649669
   KAWAGUCHI A., 1995, P WINT USENIX TECHN
   Kim J, 2002, IEEE T CONSUM ELECTR, V48, P366
   LI K., 1994, CSD94814 U CAL
   Llanos DR, 2006, SIGMOD REC, V35, P6, DOI 10.1145/1228268.1228270
   Manning C., 2002, YAFFS NAND SPECIFIC
   MARSH B., 1994, P 27 HAW ITN C ARCH
   Mathur A., 2007, P OTT LIN S
   MCKUSICK MK, 1984, ACM T COMPUT SYST, V2, P181, DOI 10.1145/989.990
   Norcott W., 2009, IOZONE FILESYSTEM BE
   Park S.Y., 2006, CASES 2006, DOI [10.1145/1176760.1176789, DOI 10.1145/1176760.1176789]
   Prabhakaran V., 2008, OSDI
   RAJIMWALE A., 2009, BLOCK MANAGEME UNPUB
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   Tanenbaum A. S., 2006, Operating Systems Review, V40, P100, DOI 10.1145/1113361.1113364
   TRANSACTION PROCESSING PERFORMANCE COUNCIL, 2008, TPC BENCHM H DEC SUP
   Troyanskaya O, 2001, BIOINFORMATICS, V17, P520, DOI 10.1093/bioinformatics/17.6.520
   Tweedie Stephen, 2000, P OTT LIN S
   Ulmer C., 2008, HIGH PERFORMANCE EMB
   Woodhouse David., 2001, JFFS : The Journalling Flash File System
   WU M, 1994, P 6 INT C ARCH SUPP
NR 33
TC 60
Z9 110
U1 0
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD SEP
PY 2010
VL 6
IS 3
AR 14
DI 10.1145/1837915.1837922
PG 25
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA V24QL
UT WOS:000208424800007
DA 2024-07-18
ER

PT J
AU Sha, ZB
   Li, J
   Zhang, FX
   Huang, M
   Cai, ZG
   Trahay, F
   Liao, J
AF Sha, Zhibing
   Li, Jun
   Zhang, Fengxiang
   Huang, Min
   Cai, Zhigang
   Trahay, Francois
   Liao, Jianwei
TI Visibility Graph-based Cache Management for DRAM Buffer Inside
   Solid-state Drives
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Solid-state drives; Cache Management; Temporal and Spatial Locality;
   Visibility Graph; batch adjustment and eviction
ID TIME-SERIES; OPTIMIZATION; LOCALITY; SCHEME
AB Most solid-state drives (SSDs) adopt an on-board Dynamic Random Access Memory (DRAM) to buffer the write data, which can significantly reduce the amount of write operations committed to the flash array of SSD if data exhibits locality in write operations. This article focuses on efficiently managing the small amount of DRAM cache inside SSDs. The basic idea is to employ the visibility graph technique to unify both temporal and spatial locality of references of I/O accesses, for directing cache management in SSDs. Specifically, we propose to adaptively generate the visibility graph of cached data pages and then support batch adjustment of adjacent or nearby (hot) cached data pages by referring to the connection situations in the visibility graph. In addition, we propose to evict the buffered data pages in batches by also referring to the connection situations, to maximize the internal flushing parallelism of SSD devices without worsening I/O congestion. The trace-driven simulation experiments showthat our proposal can yield improvements on cache hits by between 0.8% and 19.8%, and the overall I/O latency by 25.6% on average, compared to state-of-the-art cache management schemes inside SSDs.
C1 [Sha, Zhibing; Li, Jun; Zhang, Fengxiang; Huang, Min; Cai, Zhigang; Liao, Jianwei] Southwest Univ China, Chongqing, Peoples R China.
   [Trahay, Francois] Telecom SudParis, F-91011 Evry, France.
C3 IMT - Institut Mines-Telecom; Institut Polytechnique de Paris; Telecom
   SudParis
RP Liao, J (corresponding author), Southwest Univ China, Chongqing, Peoples R China.
EM shazb171318515@163.com; lijun19991111@126.com; zhangfx@swu.edu.cn;
   hmin@swu.edu.cn; czg@swu.edu.cn; liaotoad@gmail.com;
   francois.trahay@telecom-sudparis.eu
RI ; Zhang, Fengxiang/D-8147-2017; Liao, Jianwei/C-5339-2016
OI Li, Jun/0000-0001-5235-6496; Zhigang, Cai/0000-0002-8406-8461; Zhang,
   Fengxiang/0000-0003-2476-9853; sha, zhibing/0000-0001-7242-6287; Liao,
   Jianwei/0000-0001-6149-6650
FU National Natural Science Foundation of China [61872299]; Natural Science
   Foundation Project of CQ CSTC [cstc2021ycjh-bgzxm0199, 2022NSCQ-MSX0789]
FX This work was partially supported by "National Natural Science
   Foundation of China (No. 61872299) and "the Natural Science Foundation
   Project of CQ CSTC (No. cstc2021ycjh-bgzxm0199, 2022NSCQ-MSX0789)".
CR [Anonymous], 2020, AL BLOCK TRAC
   Chang T., 2021, TCAD, V41, P91
   Chen H, 2021, CONCURR COMP-PRACT E, V33, DOI 10.1002/cpe.5395
   Donner RV, 2012, ACTA GEOPHYS, V60, P589, DOI 10.2478/s11600-012-0032-x
   Du CJ, 2019, IEEE T CONSUM ELECTR, V65, P134, DOI 10.1109/TCE.2019.2910890
   ERDOS P, 1988, J COMB THEORY B, V45, P86, DOI 10.1016/0095-8956(88)90057-3
   Gao YY, 2020, MEASUREMENT, V149, DOI 10.1016/j.measurement.2019.107036
   Gill BS, 2005, USENIX ASSOCIATION PROCEEDINGS OF THE 4TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P129
   Hu J., 2010, IEEE INT S MODELING
   Iacobello G, 2018, PHYS LETT A, V382, P1, DOI 10.1016/j.physleta.2017.10.027
   Jain A, 2016, CONF PROC INT SYMP C, P78, DOI 10.1109/ISCA.2016.17
   Jiang S, 2005, USENIX ASSOCIATION PROCEEDINGS OF THE 4TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P101
   Jo H, 2006, IEEE T CONSUM ELECTR, V52, P485, DOI 10.1109/TCE.2006.1649669
   Kandemir M, 1999, IEEE T COMPUT, V48, P159, DOI 10.1109/12.752657
   Khan SU, 2016, IEEE T MAGN, V52, DOI 10.1109/TMAG.2015.2487678
   Kim BS, 2018, PROCEEDINGS OF THE 2018 USENIX ANNUAL TECHNICAL CONFERENCE, P677
   Kim BS, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P281
   Kim H, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P239, DOI 10.1145/1378600.1378627
   Kim K, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0229645
   Lacasa L, 2008, P NATL ACAD SCI USA, V105, P4972, DOI 10.1073/pnas.0709247105
   Lee C, 2017, SYSTOR'17: PROCEEDINGS OF THE 10TH ACM INTERNATIONAL SYSTEMS AND STORAGE CONFERENCE, DOI 10.1145/3078468.3078479
   Li H, 2021, J CIRCUIT SYST COMP, V30, DOI 10.1142/S0218126621501863
   Li J, 2020, IEEE T COMPUT AID D, V39, P3956, DOI 10.1109/TCAD.2020.3012252
   Liao JW, 2016, IEEE T PARALL DISTR, V27, P2698, DOI 10.1109/TPDS.2015.2496595
   Liu CY, 2021, INT S HIGH PERF COMP, P426, DOI 10.1109/HPCA51647.2021.00043
   Luque B, 2009, PHYS REV E, V80, DOI 10.1103/PhysRevE.80.046103
   Megiddo N, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE 2ND USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST'03), P115
   Narayanan D, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P253
   Park S., 2006, INT C COMPILERS ARCH
   ROBINSON JT, 1990, PERF E R SI, V18, P134, DOI 10.1145/98460.98523
   Sha ZB, 2022, DES AUT TEST EUROPE, P891, DOI 10.23919/DATE54114.2022.9774532
   Shim H, 2010, IEEE S MASS STOR SYS
   Song Jiang, 2002, Performance Evaluation Review, V30, P31, DOI 10.1145/511399.511340
   Sun H, 2021, IEEE T PARALL DISTR, V32, P1437, DOI 10.1109/TPDS.2021.3052028
   Tarihi M, 2016, IEEE T COMPUT, V65, P1678, DOI 10.1109/TC.2015.2455978
   Tran N, 2004, IEEE T PARALL DISTR, V15, P362, DOI 10.1109/TPDS.2004.1271185
   Wang H, 2018, PROC INT CONF PARAL, DOI 10.1145/3225058.3225126
   Wang MY, 2017, IEEE T VLSI SYST, V25, P2419, DOI 10.1109/TVLSI.2017.2712366
   Wang W., 2020, ACMIEEE DESIGN AUTOM
   Wang YL, 2018, J SUPERCOMPUT, V74, P141, DOI 10.1007/s11227-017-2119-2
   Wu GY, 2012, ACM T STORAGE, V8, DOI 10.1145/2093139.2093140
   Wu S., 2016, INT C SUPERCOMPUTING
   Xu XF, 2020, DES AUT TEST EUROPE, P720, DOI 10.23919/DATE48585.2020.9116382
   Zhang WH, 2018, INTERNATIONAL CONFERENCE ON SUPERCOMPUTING (ICS 2018), P22, DOI 10.1145/3205289.3205319
   Zhou YY, 2001, USENIX ASSOCIATION PROCEEDINGS OF THE 2001 USENIX ANNUAL TECHNICAL CONFERENCE, P91
   Zhu QB, 2004, 10TH INTERNATIONAL SYMPOSIUM ON HIGH PERFORMANCE COMPUTER ARCHITECTURE, PROCEEDINGS, P118
NR 46
TC 0
Z9 0
U1 3
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2023
VL 19
IS 3
AR 25
DI 10.1145/3586576
PG 21
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P9QZ4
UT WOS:001053965500005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Li, JH
   Wang, QP
   Lee, PPC
   Shi, C
AF Li, Jinhong
   Wang, Qiuping
   Lee, Patrick P. C.
   Shi, Chao
TI An In-depth Comparative Analysis of Cloud Block Storage Workloads:
   Findings and Implications
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Cloud block storage; trace analysis; I/O characterization
AB Cloud block storage systems support diverse types of applications in modern cloud services. Characterizing their input/output (I/O) activities is critical for guiding better system designs and optimizations. In this article, we present an in-depth comparative analysis of production cloud block storage workloads through the block-level I/O traces of billions of I/O requests collected from two production systems, Alibaba Cloud and Tencent Cloud Block Storage. We study their characteristics of load intensities, spatial patterns, and temporal patterns. We also compare the cloud block storage workloads with the notable public block-level I/O workloads from the enterprise data centers at Microsoft Research Cambridge, and we identify the commonalities and differences of the three sources of traces. To this end, we provide 6 findings through the high-level analysis and 16 findings through the detailed analysis on load intensity, spatial patterns, and temporal patterns. We discuss the implications of our findings on load balancing, cache efficiency, and storage cluster management in cloud block storage systems.
C1 [Li, Jinhong; Wang, Qiuping; Lee, Patrick P. C.] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.
   [Shi, Chao] Alibaba Grp, Hangzhou, Peoples R China.
C3 Chinese University of Hong Kong; Alibaba Group
RP Lee, PPC (corresponding author), Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.
EM jhli@cse.cuhk.edu.hk; qpwang@cse.cuhk.edu.hk; pclee@cse.cuhk.edu.hk;
   chao.shi@alibaba-inc.com
RI Lee, Patrick P. C./I-7165-2013
OI Lee, Patrick P. C./0000-0002-4501-4364
FU Alibaba Innovation Research (AIR) program; Research Matching Grant
   Scheme
FX This work was supported in part by the Alibaba Innovation Research (AIR)
   program and the Research Matching Grant Scheme.
CR Ahmad Irfan, 2007, 2007 IEEE International Symposium on Workload Characterization, P149, DOI 10.1109/IISWC.2007.4362191
   Alibaba, 2022, Alibaba Block Trace
   Alibaba, 2022, AL CLOUD BLOCK STOR
   Amazon, 2022, AM EBS
   Arteaga D., 2014, PROC INT C SYST STOR, P1
   Arteaga D, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P355
   Atikoglu Berk, 2012, Performance Evaluation Review, V40, P53, DOI 10.1145/2318857.2254766
   Beaver D., 2010, P USENIX OSDI
   Bhadkamkar M., 2009, Proceedings of the 7th USENIX Conference on File and Storage Technologies, P183
   Cai Y, 2015, INT S HIGH PERF COMP, P551, DOI 10.1109/HPCA.2015.7056062
   Cao ZC, 2020, PROCEEDINGS OF THE 18TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P209
   Chan JeremyC. W., 2014, P 12 USENIX C FILE S, P163
   Chiueh TC, 2014, P INT C SYST STOR, P1, DOI [10.1145/2611354.2611360, DOI 10.1145/2611354.2611360]
   Desnoyers Peter., 2012, P INT SYSTEMS STORAG, P12
   Han SJ, 2021, PROCEEDINGS OF THE 19TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '21), P417
   Harter T, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P181
   He J, 2017, PROCEEDINGS OF THE TWELFTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS (EUROSYS 2017), P127, DOI 10.1145/3064176.3064187
   Hsu WW, 2003, IBM SYST J, V42, P347, DOI 10.1147/sj.422.0347
   Kavalanekar Swaroop, 2008, 2008 IEEE International Symposium on Workload Characterization (IISWC), P119, DOI 10.1109/IISWC.2008.4636097
   Lee C, 2017, SYSTOR'17: PROCEEDINGS OF THE 10TH ACM INTERNATIONAL SYSTEMS AND STORAGE CONFERENCE, DOI 10.1145/3078468.3078479
   Li HB, 2019, PROCEEDINGS OF THE FOURTEENTH EUROSYS CONFERENCE 2019 (EUROSYS '19), DOI 10.1145/3302424.3303967
   Li JH, 2020, I S WORKL CHAR PROC, P37, DOI 10.1109/IISWC50251.2020.00013
   Li Q, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P125
   Liu R.-S., 2012, Proceedings of the 10th USENIX Conference on File and Storage Technologies, P11
   Liu SY, 2019, PROCEEDINGS OF THE 2019 TENTH ACM SYMPOSIUM ON CLOUD COMPUTING (SOCC '19), P403, DOI 10.1145/3357223.3362705
   Liu ZX, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P143
   Maneas S, 2020, PROCEEDINGS OF THE 18TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P137
   Meyer DT, 2008, EUROSYS'08: PROCEEDINGS OF THE EUROSYS 2008 CONFERENCE, P41, DOI 10.1145/1357010.1352598
   Mickens James., 2014, Proceedings of the 11th USENIX Conference on Networked Systems Design and Implementation. NSDI'14, P257
   Microsoft, 2022, MSR CAMBR TRAC
   Min Changwoo, 2012, FAST, V12, P1
   Mishra Asit K., 2010, Performance Evaluation Review, V37, P34, DOI 10.1145/1773394.1773400
   Narayanan D, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P253
   Riska A, 2006, USENIX ASSOCIATION PROCEEDINGS OF THE 2006 USENIX ANNUAL TECHNICAL CONFERENCE, P97
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   Saxena M., 2012, P 7 ACM EUR C COMP S, P267, DOI DOI 10.1145/2168836.2168863
   Soundararajan Gokul., 2010, Proceedings of the 8th USENIX conference on File and storage technologies, FAST'10, P8
   Spearman C, 1904, AM J PSYCHOL, V15, P72, DOI 10.2307/1412159
   Tarihi Mojtaba, 2015, ACM SIGMETRICS Performance Evaluation Review, V43, P297, DOI 10.1145/2745844.2745856
   Tencent, 2022, TENC BLOCK STOR
   Verma Akshat., 2010, Proceedings of the 8th USENIX Conference on File and Storage Technologies, FAST '10, P267
   Wajahat M, 2019, I S MOD ANAL SIM COM, P138, DOI 10.1109/MASCOTS.2019.00024
   Waldspurger C. A., 2015, 13 USENIX C FILE STO, P95
   Wang H, 2018, PROC INT CONF PARAL, DOI 10.1145/3225058.3225126
   Wang QP, 2022, PROCEEDINGS OF THE 20TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, FAST 2022, P429
   Wang SC, 2020, PROCEEDINGS OF THE 18TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P253
   Wires J., 2014, P S OP SYST DES IMPL, P335
   Xu E, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P961
   Yadgar G, 2021, ACM T STORAGE, V17, DOI 10.1145/3423137
   Yang J, 2019, SYSTOR '19: PROCEEDINGS OF THE 12TH ACM INTERNATIONAL SYSTEMS AND STORAGE CONFERENCE, P155, DOI 10.1145/3319647.3325840
   Yang JC, 2020, PROCEEDINGS OF THE 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '20), P191
   Zhang YM, 2020, ACM T STORAGE, V16, DOI 10.1145/3365839
   Zhang Y, 2020, PROCEEDINGS OF THE 2020 USENIX ANNUAL TECHNICAL CONFERENCE, P785
   Zhou D, 2015, I S WORKL CHAR PROC, P12, DOI 10.1109/IISWC.2015.8
NR 54
TC 2
Z9 2
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAY
PY 2023
VL 19
IS 2
AR 16
DI 10.1145/3572779
PG 31
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D7OU8
UT WOS:000970592600007
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhou, Y
   Wang, F
   Feng, D
AF Zhou, Yang
   Wang, Fang
   Feng, Dan
TI A Disk Failure Prediction Method Based on Active Semi-supervised
   Learning
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Disk failure prediction; semi-supervised learning; active learning;
   machine learning
AB Disk failure has always been a major problem for data centers, leading to data loss. Current disk failure prediction approaches are mostly offline and assume that the disk labels required for training learning models are available and accurate. However, these offline methods are no longer suitable for disk failure prediction tasks in large-scale data centers. Behind this explosive amount of data, most methods do not consider whether it is not easy to get the label values during the training or the obtained label values are not completely accurate. These problems further restrict the development of supervised learning and offline modeling in disk failure prediction. In this article, Active Semi-supervised Learning Disk-failure Prediction (ASLDP), a novel disk failure prediction method is proposed, which uses active learning and semi-supervised learning. According to the characteristics of data in the disk lifecycle, ASLDP carries out active learning for those clear labeled samples, which selects valuable samples with the most significant probability uncertainty and eliminates redundancy. For those samples that are unclearly labeled or unlabeled, ASLDP uses semi-supervised learning for pre-labeled by calculating the conditional values of the samples and enhances the generalization ability by active learning. Compared with several state-of-the-art offline and online learning approaches, the results on four realistic datasets from Backblaze and Baidu demonstrate that ASLDP achieves stable failure detection rates of 80-85% with low false alarm rates. In addition, we use a dataset from Alibaba to evaluate the generality of ASLDP. Furthermore, ASLDP can overcome the problem of missing sample labels and data redundancy in large data centers, which are not considered and implemented in all offline learning methods for disk failure prediction to the best of our knowledge. Finally, ASLDP can predict the disk failure 4.9 days in advance with lower overhead and latency.
C1 [Zhou, Yang; Wang, Fang; Feng, Dan] Huazhong Univ Sci & Technol, Wuhan Natl Lab Optoelect, Wuhan, Peoples R China.
C3 Huazhong University of Science & Technology
RP Wang, F (corresponding author), Huazhong Univ Sci & Technol, Wuhan Natl Lab Optoelect, Wuhan, Peoples R China.
EM zhouyang1024@hust.edu.cn; fang@hust.edu.cn; dfeng@hust.edu.cn
FU NSFC [61832020, 61821003]
FX This work is supported in part by the NSFC No. 61832020, No. 61821003.
CR Alibaba Cloud Computing TIANCHI, 2021, LARGE SCALE SSD FAIL
   Allen Bruce, 2004, LINUX J, V2004, P9
   Anantharaman P, 2018, IEEE INT CONGR BIG, P251, DOI 10.1109/BigDataCongress.2018.00044
   [Anonymous], 2009, Technical report
   Backblaze, 2016, RAW HARD DRIVE TEST
   Backblaze, 2014, Hard drive SMART stats
   Backblaze, 2015, WHAT IS BEST HARD DR
   Baidu, 2013, BAIDU DATASET
   Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962
   Botezatu M, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P39, DOI 10.1145/2939672.2939699
   Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1007/bf00058655
   Chen X, 2017, INT CONF DAT MIN WOR, P850, DOI 10.1109/ICDMW.2017.154
   Cheng Sheng-Jun, 2013, J POWER SOURCES, P10
   Deng Y, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4012
   Elsken T, 2019, J MACH LEARN RES, V20
   Gama J, 2014, ACM COMPUT SURV, V46, DOI 10.1145/2523813
   Gomes HM, 2017, MACH LEARN, V106, P1469, DOI 10.1007/s10994-017-5642-8
   Gunawi HS, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P1
   Han SJ, 2020, INT CON DISTR COMP S, P628, DOI 10.1109/ICDCS47774.2020.00044
   Hashemi M, 2018, PR MACH LEARN RES, V80
   Huang Ping, 2020, P 2020 US ANN TECHN, P111
   Hughes GF, 2002, IEEE T RELIAB, V51, P350, DOI 10.1109/TR.2002.802886
   Kraska T, 2018, INT CONF MANAGE DATA, P489, DOI 10.1145/3183713.3196909
   Leng Y, 2013, KNOWL-BASED SYST, V44, P121, DOI 10.1016/j.knosys.2013.01.032
   Levin A, 2019, IEEE INT CONGR BIG, P165, DOI 10.1109/BigDataCongress.2019.00036
   Li J, 2017, RELIAB ENG SYST SAFE, V164, P55, DOI 10.1016/j.ress.2017.03.004
   Li J, 2014, I C DEPEND SYS NETWO, P383, DOI 10.1109/DSN.2014.44
   Liang C, 2018, Arxiv, DOI arXiv:1801.06481
   Liang Chen, 2018, P AAAI C ARTIFICIAL, V32
   Lillicrap T., 2015, arXiv, DOI 10.48550/arXiv.1509.02971
   Lima FDS, 2017, 2017 6TH BRAZILIAN CONFERENCE ON INTELLIGENT SYSTEMS (BRACIS), P222, DOI 10.1109/BRACIS.2017.72
   Lu J, 2015, DECIS SUPPORT SYST, V74, P12, DOI 10.1016/j.dss.2015.03.008
   Lu SD, 2020, PROCEEDINGS OF THE 18TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P151
   Mandisoltani F, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P391
   Masood Adnan, 2019, AIOps: Predictive Analytics & Machine Learning in Operations, P359
   Miyato T, 2019, IEEE T PATTERN ANAL, V41, P1979, DOI 10.1109/TPAMI.2018.2858821
   Montiel J, 2018, J MACH LEARN RES, V19
   Murray JF, 2005, J MACH LEARN RES, V6, P783
   Murray Joseph F., 2003, P ARTIFICIAL NEURAL
   Patterson D. A., 1988, SIGMOD Record, V17, P109, DOI 10.1145/971701.50214
   Pereira Francisco Lucas F., 2019, 2019 8th Brazilian Conference on Intelligent Systems (BRACIS). Proceedings, P586, DOI 10.1109/BRACIS.2019.00108
   Pitakrat Teerat, 2013, P 4 INT ACM SIGSOFT, P1, DOI [10.1145/2465470.2465473, DOI 10.1145/2465470.2465473]
   Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778
   Rosenfeld, 2005, SEMISUPERVISED LEARN
   Salfner F, 2010, ACM COMPUT SURV, V42, DOI 10.1145/1670679.1670680
   Schroeder Bianca, 2007, ACM Transactions on Storage, V3, DOI 10.1145/1288783.1288785
   Sun XY, 2019, PROCEEDINGS OF THE 2019 56TH ACM/EDAC/IEEE DESIGN AUTOMATION CONFERENCE (DAC), DOI 10.1145/3316781.3317918
   Talukdar P.P., 2012, P 7 WORKSH BUILD ED, P307
   Wang Y, 2014, IEEE T IND INFORM, V10, P419, DOI 10.1109/TII.2013.2264060
   Wang Yu., 2011, 2011 PROGNOSTICS SYS, P1
   WeiWang Zhi-Hua, 2008, P 25 INT C MACHINE L, P1152
   Xiao J, 2018, PROC INT CONF PARAL, DOI 10.1145/3225058.3225106
   Xie YW, 2019, IEEE S MASS STOR SYS, P193, DOI 10.1109/MSST.2019.000-3
   Xie YW, 2018, PR IEEE COMP DESIGN, P561, DOI 10.1109/ICCD.2018.00089
   Xu C, 2016, IEEE T COMPUT, V65, P3502, DOI 10.1109/TC.2016.2538237
   Xu F, 2021, I C DEPEND SYS NETWO, P263, DOI 10.1109/DSN48987.2021.00039
   Xu Y, 2018, PROCEEDINGS OF THE 2018 USENIX ANNUAL TECHNICAL CONFERENCE, P481
   Yang HZ, 2020, I C DEPEND SYS NETWO, P17, DOI 10.1109/DSN-S50200.2020.00017
   Yang L, 2020, PROC VLDB ENDOW, V13, P1976, DOI 10.14778/3407790.3407803
   Zhan J, 2020, IEEE T PARALL DISTR, V31, P2155, DOI 10.1109/TPDS.2020.2985346
   Zhang J, 2019, INT CONF MANAGE DATA, P415, DOI 10.1145/3299869.3300085
   Zhang J, 2019, PROC INT CONF PARAL, DOI 10.1145/3337821.3337881
   Zhang YH, 2014, EXPERT SYST APPL, V41, P2372, DOI 10.1016/j.eswa.2013.09.035
   Zhou Y, 2021, PROC INT CONF PARAL, DOI 10.1145/3472456.3472490
   Zhou Y, 2020, 2020 16TH INTERNATIONAL CONFERENCE ON MOBILITY, SENSING AND NETWORKING (MSN 2020), P642, DOI 10.1109/MSN50589.2020.00107
   Zhou Y, 2019, IEEE IJCNN
   Zhu BP, 2013, IEEE S MASS STOR SYS, DOI 10.1109/msst.2013.6558427
   Zufle Marwin, 2020, Measurement, Modelling and Evaluation of Computing Systems. 20th International GI/ITG Conference, MMB 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12040), P19, DOI 10.1007/978-3-030-43024-5_2
NR 68
TC 2
Z9 2
U1 5
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2022
VL 18
IS 4
AR 35
DI 10.1145/3523699
PG 33
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7F8OU
UT WOS:000902100700008
DA 2024-07-18
ER

PT J
AU Li, C
   Chen, H
   Ruan, CY
   Ma, XS
   Xu, YL
AF Li, Cheng
   Chen, Hao
   Ruan, Chaoyi
   Ma, Xiaosong
   Xu, Yinlong
TI Leveraging NVMe SSDs for Building a Fast, Cost-effective, LSM-tree-based
   KV Store
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Key-value store; SSD; write-ahead log
ID NVRAM; ENGINE
AB Key-value (KV) stores support many crucial applications and services. They perform fast in-memory processing but are still often limited by I/O performance. The recent emergence of high-speed commodity nonvolatile memory express solid-state drives (NVMe SSDs) has propelled new KV system designs that take advantage of their ultra-low latency and high bandwidth. Meanwhile, to switch to entirely new data layouts and scale up entire databases to high-end SSDs requires considerable investment.
   As a compromise, we propose SpanDB, an LSM-tree-based KV store that adapts the popular RocksDB system to utilize selective deployment of high-speed SSDs. SpanDB allows users to host the bulk of their data on cheaper and larger SSDs (and even hard disc drives with certain workloads), while relocating write-ahead logs (WAL) and the top levels of the LSM-tree to a much smaller and faster NVMe SSD. To better utilize this fast disk, SpanDB provides high-speed, parallel WAL writes via SPDK, and enables asynchronous request processing to mitigate inter-thread synchronization overhead and work efficiently with polling-based I/O. To ease the live data migration between fast and slow disks, we introduce TopFS, a stripped-down file system providing familiar file interface wrappers on top of SPDK I/O. Our evaluation shows that SpanDB simultaneously improves RocksDB's throughput by up to 8.8x and reduces its latency by 9.5-58.3%. Compared with KVell, a system designed for high-end SSDs, SpanDB achieves 96-140% of its throughput, with a 2.3-21.6x lower latency, at a cheaper storage configuration.
C1 [Li, Cheng; Chen, Hao; Ruan, Chaoyi; Xu, Yinlong] Univ Sci & Technol China, NHPCC Room 503,East Campus,JinZhai Rd 96, Hefei, Anhui, Peoples R China.
   [Ma, Xiaosong] Qatar Comp Res Inst, POB 34110,HBKU Res Complex, Doha, Qatar.
   [Xu, Yinlong] Anhui Prov Key Lab High Performance Comp, Hefei, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Qatar Foundation (QF); Hamad Bin Khalifa University-Qatar;
   Qatar Computing Research Institute
RP Xu, YL (corresponding author), Univ Sci & Technol China, NHPCC Room 503,East Campus,JinZhai Rd 96, Hefei, Anhui, Peoples R China.; Xu, YL (corresponding author), Anhui Prov Key Lab High Performance Comp, Hefei, Peoples R China.
EM chengli7@ustc.edu.cn; cighao@mail.ustc.edu.cn; rcy@mail.ustc.edu.cn;
   xma@hbku.edu.qa; ylxu@ustc.edu.cn
RI Wang, Shiyao/JLL-7826-2023
FU National Nature Science Foundation of China [61832011, 61802358,
   61772486]; USTC Research Funds of the Double First-Class Initiative
   [YD2150002006]
FX This work was supported in part by National Nature Science Foundation of
   China (Grants No. 61832011, No. 61802358, and No. 61772486), and USTC
   Research Funds of the Double First-Class Initiative (Grant No.
   YD2150002006).
CR AndrewPavlo Jianhong Li, 2017, NVMROCKS ROCKSDB NON
   Armstrong Timothy G., 2013, P ACM SIGMOD INT C M
   Audibert Andrew, SCALABLE METADATA SE
   Bailleu M, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P173
   Balmau O, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P753
   Balmau O, 2017, PROCEEDINGS OF THE TWELFTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS (EUROSYS 2017), P80, DOI 10.1145/3064176.3064193
   Balmau O, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P363
   Bronson Nathan, 2013, P USENIX ANN TECHN C
   Cao Zhichao, 2020, P 18 USENIX C FIL ST
   CDW-G, STOR HARD DRIV CDW G
   Chan Helen H. W., 2018, P USENIX ANN TECHN C
   Chen YM, 2020, TWENTY-FIFTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXV), P1077, DOI 10.1145/3373376.3378515
   Conway Alexander, 2020, P USENIX ANN TECHN C
   Cooper Brian F., 2010, P 1 ACM S CLOUD COMP, P143, DOI [DOI 10.1145/1807128.1807152, 10.1145/1807128.1807152]
   Dayan Niv, 2017, P ACM INT C MAN DAT
   Dean J, 2009, DESIGNS LESSONS ADVI
   Debnath B, 2010, PROC VLDB ENDOW, V3, P1414, DOI 10.14778/1920841.1921015
   Di Martino C, 2014, I C DEPEND SYS NETWO, P610, DOI 10.1109/DSN.2014.62
   Dong Siying, 2017, WORKLOAD DIVERSITY R
   Eisenman A, 2018, EUROSYS '18: PROCEEDINGS OF THE THIRTEENTH EUROSYS CONFERENCE, DOI 10.1145/3190508.3190524
   Enberg P, 2019, PROCEEDINGS OF THE WORKSHOP ON HOT TOPICS IN OPERATING SYSTEMS (HOTOS '19), P81, DOI 10.1145/3317550.3321426
   Enterprise Storage Forum, HIGH CAP SSDS BIG CA
   Facebook, HOOD BUILDING OPEN S
   Facebook, CASSANDRA ROCKSDB IN
   Gao Yu, 2018, P 26 ACM JOINT M EUR
   Ghemawat S., 2014, LevelDB, A fast and lightweight key/value database library by Google
   GitHub, HHVM
   Gugnani Shashank, 2018, P IEEE ACM 11 INT C
   Hagmann Robert, 1987, P 11 ACM S OP SYST P
   Han K, 2020, IEEE T COMPUT, V69, P260, DOI 10.1109/TC.2019.2947897
   Huang G, 2019, INT CONF MANAGE DATA, P651, DOI 10.1145/3299869.3314041
   Huang J, 2014, PROC VLDB ENDOW, V8, P389, DOI 10.14778/2735496.2735502
   Huang YH, 2018, PROCEEDINGS OF THE 2018 USENIX ANNUAL TECHNICAL CONFERENCE, P967
   i- programmer, ROCKSDB STER
   Im Junsu, 2020, P USENIX ANN TECHN C
   Intel, SPDK: Storage Performance Development Kit
   Intel, breakthrough performance for demanding storage workloads
   Kaiyrakhmet Olzhas, 2019, P 17 USENIX C FIL ST
   Kang Yangwook, 2019, P 12 ACM INT C SYST
   Kannan S, 2018, PROCEEDINGS OF THE 2018 USENIX ANNUAL TECHNICAL CONFERENCE, P993
   Kim WH, 2016, ACM SIGPLAN NOTICES, V51, P385, DOI 10.1145/2954679.2872392
   Kinkade David, 1985, IEEE DATABASE ENG B, V8, P3
   Kwon Y, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P460, DOI 10.1145/3132747.3132770
   Lakshman Avinash, 2010, Operating Systems Review, V44, P35, DOI 10.1145/1773912.1773922
   Lee Gyusun, 2019, P USENIX ANN TECHN C
   Lepers B, 2019, PROCEEDINGS OF THE TWENTY-SEVENTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '19), P447, DOI 10.1145/3341301.3359628
   Li WJ, 2020, 17TH ACM INTERNATIONAL CONFERENCE ON COMPUTING FRONTIERS 2020 (CF 2020), P208, DOI 10.1145/3387902.3392621
   Li YK, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P739
   Lim H., 2016, P 14 USENIX C FIL ST
   LinkedIn, BENCHMARKING APACHE
   Liu ZX, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P143
   Lu LY, 2017, ACM T STORAGE, V13, DOI 10.1145/3033273
   MariaDB, GROUP COMM BIN LOG
   MOHAN C, 1992, ACM T DATABASE SYST, V17, P94, DOI 10.1145/128765.128770
   MySQL, MYSQL REFERENCE MANU
   Nishtala R., 2013, Proceedings of the 10th USENIX Conference on Networked Systems Design and Implementation, NSDI '13, Berkeley, CA, USA, P385
   Pelley S, 2013, PROC VLDB ENDOW, V7, P121, DOI 10.14778/2732228.2732231
   Peng B, 2018, PROCEEDINGS OF THE 2018 USENIX ANNUAL TECHNICAL CONFERENCE, P665
   Qiu S, 2013, IEEE S MASS STOR SYS
   Raina Ashwini, 2020, PRISMDB READ AWARE L
   Raju P, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P497, DOI 10.1145/3132747.3132765
   Ren K, 2017, PROC VLDB ENDOW, V10, P2037, DOI 10.14778/3151106.3151108
   RocksDB, A Persistent Key Value Store for Flash and RAM Storage
   Samsung, Ultra-Low Latency with Samsung Z-NAND SSD
   Samsung, SSD STOR CAP INCR IM
   SPDK, BLOBFS BLOBST FIL BL
   Sridharan V, 2015, ACM SIGPLAN NOTICES, V50, P297, DOI [10.1145/2775054.2694348, 10.1145/2694344.2694348]
   Toshiba, TOSH MEM INTR XL FLA
   Wang TZ, 2014, PROC VLDB ENDOW, V7, P865, DOI 10.14778/2732951.2732960
   Welsh M., 2001, Operating Systems Review, V35, P230, DOI 10.1145/502059.502057
   Wu Kan, 2019, P 11 USENIX WORKSH H
   Wu XN, 2015, METHODS MOL BIOL, V1306, P71, DOI 10.1007/978-1-4939-2648-0_5
   Xia F, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P349
   Xue S, 2020, PROCEEDINGS OF THE 2020 USENIX ANNUAL TECHNICAL CONFERENCE, P97
   Yang ZY, 2017, INT CONF CLOUD COMP, P154, DOI 10.1109/CloudCom.2017.14
   Yang Ziye, 2018, 2018 IEEE 8 INT S, P67, DOI DOI 10.1109/SC2.2018.00016
   Yao T, 2020, PROCEEDINGS OF THE 2020 USENIX ANNUAL TECHNICAL CONFERENCE, P17
   Yoon H, 2018, PROCEEDINGS OF THE 2018 ACM SYMPOSIUM ON CLOUD COMPUTING (SOCC '18), P162, DOI 10.1145/3267809.3267846
   Zhang J, 2020, PROCEEDINGS OF THE 18TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P121
   Zhang Qiang, 2020, P 36 IEEE INT C DAT
   Zhang T, 2020, PROCEEDINGS OF THE 18TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P225
   Zheng SA, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P207
   Zheng Wenting, 2014, OSDI
   Zhu XW, 2020, PROC VLDB ENDOW, V13, P1020, DOI 10.14778/3384345.3384351
NR 84
TC 9
Z9 9
U1 1
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2021
VL 17
IS 4
AR 27
DI 10.1145/3480963
PG 29
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YY2GY
UT WOS:000754612700003
DA 2024-07-18
ER

PT J
AU Yang, JC
   Yue, Y
   Rashmi, KV
AF Yang, Juncheng
   Yue, Yao
   Rashmi, K., V
TI A Large-scale Analysis of Hundreds of In-memory Key-value Cache Clusters
   at Twitter
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article; Proceedings Paper
CT 14th USENIX Symposium on Operating Systems Design and Implementation
   (OSDI)
CY NOV 04-06, 2020
CL ELECTR NETWORK
SP USENIX Assoc, Alibaba Grp, Amazon, ANT Grp, Alipay, Apple, ByteDance, Facebook, Google, Microsoft, VMware, Oracle, Two Sigma, ACM Queue, DMTF, FreeBSD Fdn
DE Cache; in-memory key-value cache; key-value store; workload analysis;
   datasets; Twitter
ID WORKLOAD CHARACTERIZATION
AB Modern web services use in-memory caching extensively to increase throughput and reduce latency. There have been several workload analyses of production systems that have fueled research in improving the effectiveness of in-memory caching systems. However, the coverage is still sparse considering the wide spectrum of industrial cache use cases. In this work, we significantly further the understanding of real-world cache workloads by collecting production traces from 153 in-memory cache clusters at Twitter, sifting through over 80 TB of data, and sometimes interpreting the workloads in the context of the business logic behind them. We perform a comprehensive analysis to characterize cache workloads based on traffic pattern, time-to-live (TTL), popularity distribution, and size distribution. A fine-grained view of different workloads uncover the diversity of use cases: many are far more write-heavy or more skewed than previously shown and some display unique temporal patterns. We also observe that TTL is an important and sometimes defining parameter of cache working sets. Our simulations show that ideal replacement strategy in production caches can be surprising, for example, FIFO works the best for a large number of workloads.
C1 [Yang, Juncheng; Rashmi, K., V] Carnegie Mellon Univ, 5000 Forbes Ave, Pittsburgh, PA 15213 USA.
   [Yue, Yao] Twitter, 1355 Market St, San Francisco, CA 94103 USA.
C3 Carnegie Mellon University; Twitter, Inc.
RP Yang, JC (corresponding author), Carnegie Mellon Univ, 5000 Forbes Ave, Pittsburgh, PA 15213 USA.
EM juncheny@cs.cmu.edu; yao@twitter.com; rvinayak@cs.cmu.edu
RI Li, Chun/KBC-9591-2024; Yu, Chongxiu/KDM-7354-2024
OI Yu, Chongxiu/0000-0002-8221-6221
FU NSF [CNS 1901410, CNS 1956271]; Facebook Distributed Systems Research
   grant; Facebook PhD Fellowship
FX This work was supported in part by NSF Grants No. CNS 1901410 and No.
   CNS 1956271, in part by Facebook Distributed Systems Research grant, and
   in part by Facebook PhD Fellowship.
CR Adya A, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P739
   Altinel Mehmet, 2008, U.S. Patent, Patent No. [7,395,258, 7395258]
   [Anonymous], 1998, IMPROVING WWW PROXIE
   [Anonymous], 1993, ACM SIGMOD RECORD, DOI DOI 10.1145/170035.170081
   [Anonymous], Ptmalloc
   Arlitt M, 2000, IEEE NETWORK, V14, P30, DOI 10.1109/65.844498
   Arlitt M., 1999, Performance Evaluation Review, V27, P25, DOI 10.1145/332944.332951
   Arlitt MF, 1997, IEEE ACM T NETWORK, V5, P631, DOI 10.1109/90.649565
   Atikoglu Berk, 2012, Performance Evaluation Review, V40, P53, DOI 10.1145/2318857.2254766
   Baker M. G., 1991, Operating Systems Review, V25, P198, DOI 10.1145/121133.121164
   Beckmann N, 2018, PROCEEDINGS OF THE 15TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION (NSDI'18), P389
   Beckmann N, 2015, INT S HIGH PERF COMP, P64, DOI 10.1109/HPCA.2015.7056022
   Berg B, 2020, PROCEEDINGS OF THE 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '20), P769
   Berger DS, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P195
   Berger DS, 2017, PROCEEDINGS OF NSDI '17: 14TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P483
   Blankstein A, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P499
   Breslau L, 1999, IEEE INFOCOM SER, P126, DOI 10.1109/INFCOM.1999.749260
   Bronson N., 2013, P 2013 USENIX C ANN, P49
   Byrne D, 2019, MEMSYS 2019: PROCEEDINGS OF THE INTERNATIONAL SYMPOSIUM ON MEMORY SYSTEMS, P353, DOI 10.1145/3357526.3357562
   Cao ZC, 2020, PROCEEDINGS OF THE 18TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P209
   Chandramouli B, 2018, INT CONF MANAGE DATA, P275, DOI 10.1145/3183713.3196898
   Chen JQ, 2020, PROCEEDINGS OF THE 18TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P239
   Chen Yimin., 2015, Proceedings of the 78th ASIST Annual Meeting: Information Science with Impact: Research in and for the Community, P81, DOI [10.1145/2741948.2741967, DOI 10.1145/2741948.2741967]
   Cheng Y, 2016, PROCEEDINGS OF USENIX ATC '16: 2016 USENIX ANNUAL TECHNICAL CONFERENCE, P379
   Cidon A, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P321
   Cidon A, 2016, 13TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION (NSDI '16), P379
   Cidon Asaf., 2015, PROC USENIX HOTCLOUD
   Cooper Brian F., 2010, P 1 ACM S CLOUD COMP, P143, DOI [DOI 10.1145/1807128.1807152, 10.1145/1807128.1807152]
   Dean J, 2013, COMMUN ACM, V56, P74, DOI 10.1145/2408776.2408794
   Didona D, 2019, PROCEEDINGS OF THE 16TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P79
   Duplyakin D, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P1
   Einziger G, 2017, ACM T STORAGE, V13, DOI 10.1145/3149371
   Eisenbud DE, 2016, 13TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION (NSDI '16), P523
   Eisenman A, 2019, PROCEEDINGS OF THE 16TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P65
   Eisenman A, 2018, EUROSYS '18: PROCEEDINGS OF THE THIRTEENTH EUROSYS CONFERENCE, DOI 10.1145/3190508.3190524
   Estro Tyler, 2020, P 12 USENIX WORKSH H
   Fan Bin, 2013, 10 USENIX S NETW SYS, P371
   Holland DavidA., 2013, Proceedings of the 2013 USENIX Conference on Annual Technical Conference, USENIX ATC'13, P127
   Hong Y.-J., 2013, Proceedings of the 4th annual Symposium on Cloud Computing, page, P13, DOI DOI 10.1145/2523616.2525970
   Hu Xiameng., 2015, USENIX ANN TECHNICAL, P57
   Huang Q., 2014, Proceedings of the 13th ACM Workshop on Hot Topics in Networks, page, P8
   Huang Q, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P167, DOI 10.1145/2517349.2522722
   Hwang Jinho., 2013, Proc. International Conference on Autonomic Computing (ICAC), P33
   Ihm Sunghwan, 2011, IMC 11, P295
   Jia YC, 2020, ACM T STORAGE, V16, DOI 10.1145/3383124
   Jin X, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P121, DOI 10.1145/3132747.3132764
   Jung JY, 2001, IMW 2001: PROCEEDINGS OF THE FIRST ACM SIGCOMM INTERNET MEASUREMENT WORKSHOP, P153
   Kejriwal A, 2016, PROCEEDINGS OF USENIX ATC '16: 2016 USENIX ANNUAL TECHNICAL CONFERENCE, P57
   Lee D, 2001, IEEE T COMPUT, V50, P1352, DOI 10.1109/tc.2001.970573
   Li BJ, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P137, DOI 10.1145/3132747.3132756
   Li C., 2014, USENIX ANN TECH C AT, P501
   Li CF, 2015, DES AUT CON, DOI 10.1145/2744769.2744863
   Li C, 2017, ACM T STORAGE, V13, DOI 10.1145/3094785
   Li C, 2015, Proceedings of the 16th Annual Middleware Conference, P50, DOI 10.1145/2814576.2814734
   Li S, 2015, 2015 ACM/IEEE 42ND ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA), P476, DOI 10.1145/2749469.2750416
   Lim H, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P1
   Lim Hyeontaek, 2014, 11 USENIX S NETW SYS, P429
   Liu ZX, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P143
   Luo Q., 2002, ACM SIGMOD INT C MAN, P600, DOI DOI 10.1145/564691.564763
   Megiddo N, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE 2ND USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST'03), P115
   Nishtala R., 2013, Proceedings of the 10th USENIX Conference on Networked Systems Design and Implementation, NSDI '13, Berkeley, CA, USA, P385
   Ousterhout J. K., 1985, Operating Systems Review, V19, P15, DOI 10.1145/323627.323631
   Rashmi KV, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P401
   Reed B., 1996, Operating Systems Review, V30, P12, DOI 10.1145/230908.230913
   Reiss C., 2011, Technical report
   Reiss Charles, 2012, P ACM S CLOUD COMP, DOI [10.1145/2391229.2391236, DOI 10.1145/2391229.2391236]
   ROBINSON JT, 1990, PERF E R SI, V18, P134, DOI 10.1145/98460.98523
   Rumble S.M., 2014, Proceedings of the 12th USENIX Conference on File and Storage Technologies (FAST 14), P1
   Saxena M., 2012, P 7 ACM EUR C COMP S, P267, DOI DOI 10.1145/2168836.2168863
   Shen ZY, 2018, ACM T STORAGE, V14, DOI 10.1145/3203410
   Shi W., 2002, P 7 INT WORKSH WEB C, P1
   Singels A., 2014, Proceedings of the Annual Congress - South African Sugar Technologists' Association, P1
   Tang Linpeng, 2015, 13 USENIX C FIL STOR, P373
   Vietri G., 2018, P 10 USENIX WORKSH H, P1
   Vogels W, 1999, OPERATING SYSTEMS REVIEW, VOL 33, NO 5, DECEMBER 1999, P93, DOI 10.1145/319344.319158
   Waldspurger CA, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P487
   Wang QP, 2020, PROCEEDINGS OF THE 2020 USENIX ANNUAL TECHNICAL CONFERENCE, P713
   WELCH TA, 1984, COMPUTER, V17, P8, DOI 10.1109/MC.1984.1659158
   Wendell P., 2011, Proceedings of the 2011 ACM SIGCOMM conference on Internet measurement conference, P549
   Wikimedia, CACHING OVERVIEW WIK
   Wikimedia, ANAL DATA LAKE TRAFF
   Wilkes  J., 2011, MORE GOOGLE CLUSTER
   Wu Kan, P 19 USENIX C FIL ST
   Yang JC, 2020, PROCEEDINGS OF THE 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '20), P191
   Yang JC, 2017, PROCEEDINGS OF THE 2017 SYMPOSIUM ON CLOUD COMPUTING (SOCC '17), P66, DOI 10.1145/3127479.3131210
   Yang Juncheng, P 18 USENIX S NETW S
   Yang Juncheng, libcachesim
   Zhang L., 2020, PLANT BIOTECHNOL J, V18, P1796, DOI DOI 10.1111/pbi.13341
   Zhang Lei, 2020, P 12 USENIX WORKSH H
   Zhang W., 2014, P 9 INT WORKSH FEEDB
   Zhou K, 2018, INTERNATIONAL CONFERENCE ON SUPERCOMPUTING (ICS 2018), P284, DOI 10.1145/3205289.3205299
   Zhou YY, 2001, USENIX ASSOCIATION PROCEEDINGS OF THE 2001 USENIX ANNUAL TECHNICAL CONFERENCE, P91
   Zhu Timothy, 2012, P 4 USENIX WORKSH HO
NR 93
TC 17
Z9 17
U1 1
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2021
VL 17
IS 3
AR 17
DI 10.1145/3468521
PG 35
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA YY2GJ
UT WOS:000754611000002
OA Bronze
DA 2024-07-18
ER

PT J
AU Zhou, TL
   Tian, C
AF Zhou, Tianli
   Tian, Chao
TI Fast Erasure Coding for Data Storage: A Comprehensive Study of the
   Acceleration Techniques
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Erasure code; performance
ID SCHEME; RAID
AB Various techniques have been proposed in the literature to improve erasure code computation efficiency, including optimizing bitmatrix design and computation schedule, common XOR (exclusive-OR) operation reduction, caching management techniques, and vectorization techniques. These techniques were largely proposed individually, and, in this work, we seek to use them jointly. To accomplish this task, these techniques need to be thoroughly evaluated individually and their relation better understood. Building on extensive testing, we develop methods to systematically optimize the computation chain together with the underlying bitmatrix. This led to a simple design approach of optimizing the bitmatrix by minimizing a weighted computation cost function, and also a straightforward coding procedure-follow a computation schedule produced from the optimized bitmatrix to apply XOR-level vectorization. This procedure provides better performances than most existing techniques (e.g., those used in ISA-L and Jerasure libraries), and sometimes can even compete against well-known but less general codes such as EVENODD, RDP, and STAR codes. One particularly important observation is that vectorizing the XOR operations is a better choice than directly vectorizing finite field operations, not only because of the flexibility in choosing finite field size and the better encoding throughput, but also its minimal migration efforts onto newer CPUs.
C1 [Zhou, Tianli; Tian, Chao] Texas A&M Univ, Wisenbaker Engn Bldg 3128,188 Bizzell St, College Stn, TX 77843 USA.
C3 Texas A&M University System; Texas A&M University College Station
RP Zhou, TL (corresponding author), Texas A&M Univ, Wisenbaker Engn Bldg 3128,188 Bizzell St, College Stn, TX 77843 USA.
EM zhoutianli01@tamu.edu; chao.tian@tamu.edu
RI tian, chao/GYD-3462-2022
CR [Anonymous], 2008, Proceedings of the 4th ACM international workshop on Storage security and survivability
   BLAUM M, 1995, IEEE T COMPUT, V44, P192, DOI 10.1109/12.364531
   Blomer Johannes, 1995, TR95048 U CAL BERK
   Bowers KD, 2009, CCS'09: PROCEEDINGS OF THE 16TH ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P187
   CHEN PM, 1994, ACM COMPUT SURV, V26, P145, DOI 10.1145/176979.176981
   Corbett P, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P1
   Huang C, 2008, IEEE T COMPUT, V57, P889, DOI 10.1109/TC.2007.70830
   Huang C, 2007, 2007 IEEE INFORMATION THEORY WORKSHOP, VOLS 1 AND 2, P218, DOI 10.1109/ITW.2007.4313077
   Kubiatowicz J, 2000, ACM SIGPLAN NOTICES, V35, P190, DOI 10.1145/384264.379239
   Lin S., 2001, ERROR CONTROL CODING
   Litwin W, 2000, SIGMOD RECORD, V29, P237, DOI 10.1145/335191.335418
   Luo JQ, 2014, IEEE T COMPUT, V63, P2259, DOI 10.1109/TC.2013.23
   OVSIANNIKOV M., 2013, P VLDB ENDOWMENT, V6, P11
   Plank J., 2013, USENIX C FILE STOR T, P299
   Plank J. S., 2011, IEEE Information Theory Workshop (ITW 2011), P503, DOI 10.1109/ITW.2011.6089512
   Plank J. S., 2008, CS08627 U TENN
   Plank J. S., 2009, FAST 2009, P253
   PLANK J. S., 2013, TUTORIAL ERASURE COD
   Plank J. S., 2012, P 42 ANN IEEE IFIP I, P1
   Plank JS, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P97
   Plank JS, 2006, NCA 2006: FIFTH IEEE INTERNATIONAL SYMPOSIUM ON NETWORK COMPUTING AND APPLICATIONS, PROCEEDINGS, P173
   REED IS, 1960, J SOC IND APPL MATH, V8, P300, DOI 10.1137/0108018
   Weatherspoon H, 2002, LECT NOTES COMPUT SC, V2429, P328
   Zhou TL, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P317
NR 24
TC 17
Z9 20
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD APR
PY 2020
VL 16
IS 1
AR 7
DI 10.1145/3375554
PG 24
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1QN
UT WOS:000583742400007
DA 2024-07-18
ER

PT J
AU Zhou, D
   Fang, V
   Xie, T
   Pan, W
   Kesavan, R
   Lin, T
   Patel, N
AF Zhou, Deng
   Fang, Vania
   Xie, Tao
   Pan, Wen
   Kesavan, Ram
   Lin, Tony
   Patel, Naresh
TI Empirical Evaluation and Enhancement of Enterprise Storage System
   Request Scheduling
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE WAFL; enterprise storage system; file-level request scheduling
AB Since little has been reported in the literature concerning enterprise storage system file-level request scheduling, we do not have enough knowledge about how various scheduling factors affect performance. Moreover, we are in lack of a good understanding on how to enhance request scheduling to adapt to the changing characteristics of workloads and hardware resources. To answer these questions, we first build a request scheduler prototype based on WAFL (R), a mainstream file system running on numerous enterprise storage systems worldwide. Next, we use the prototype to quantitatively measure the impact of various scheduling configurations on performance on a NetApp (R) 's enterprise-class storage system. Several observations have been made. For example, we discover that in order to improve performance, the priority of write requests and non-preempted restarted requests should be boosted in some workloads. Inspired by these observations, we further propose two scheduling enhancement heuristics called SORD (size-oriented request dispatching) and QATS (queue-depth aware time slicing). Finally, we evaluate them by conducting a wide range of experiments using workloads generated by SPC-1 and SFS2014 on both HDD-based and all-flash platforms. Experimental results show that the combination of the two can noticeably reduce average request latency under some workloads.
C1 [Zhou, Deng; Xie, Tao; Pan, Wen] San Diego State Univ, Dept Comp Sci, San Diego, CA 92182 USA.
   [Fang, Vania; Kesavan, Ram; Lin, Tony; Patel, Naresh] NetApp Inc, Sunnyvale, CA 94089 USA.
C3 California State University System; San Diego State University; NetApp,
   Inc.
RP Xie, T (corresponding author), San Diego State Univ, Dept Comp Sci, San Diego, CA 92182 USA.
EM dzhou@sdsu.edu; Vania.Fang@netapp.com; txie@sdsu.edu; wpan@sdsu.edu;
   ram.kesavan@netapp.com; Tony.Lin@netapp.com; Naresh.Patel@netapp.com
RI Pan, Wen/JZT-1832-2024
FU U.S. National Science Foundation [CNS-1320738]
FX This work is sponsored in part by the U.S. National Science Foundation
   under grant CNS-1320738.
CR Agrawal P, 2008, PROC VLDB ENDOW, V1, P958, DOI 10.14778/1453856.1453960
   [Anonymous], 2015, P320H HHHL PCIE ENTE
   [Anonymous], 2015, SPEC SFS 2014 BENCHM
   [Anonymous], 2008, ACM T STORAGE
   [Anonymous], 2010, ACM SIGOPS OPER SYST
   [Anonymous], 2015, NETAPP FAS8080 EX DA
   Bandwidth, 2015, BANDWIDTH COMPUTING
   Batsakis Alexandros, 2009, ACM T STORAGE, V5, P4
   Berriman Ellie, 2015, IDC WORLDWIDE Q ENTE
   Bruno J, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 2, P400, DOI 10.1109/MMCS.1999.778459
   CHEN SZ, 1991, REAL-TIME SYST, V3, P307, DOI 10.1007/BF00364960
   Curtis-Maury M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P419
   Dawkins S, 2012, ACM SIGOPS OPERATING, V46, P1
   Edwards JohnK., 2008, ATC 08, P129
   EMC, 2015, EMC IS ONEFS TECHN O
   Gill BinnyS., 2005, FAST 05 P 4 C USENIX, P10
   Gupta Anoop., 1991, SIGMETRICS PERFORM E, V19, P120, DOI [DOI 10.1145/107972.107985, 10.1145/107971.107985, DOI 10.1145/107971.107985]
   Hitz D., 1994, USENIX WINTER, V94
   Iyer S., 2001, Operating Systems Review, V35, P117, DOI 10.1145/502059.502046
   Kim JS, 2009, AIP CONF PROC, V1199, P295, DOI 10.1145/1629335.1629375
   Kim Y, 2015, P 13 USENIX C FIL ST, P67
   LEUNG A. W., 2008, P USENIX ANN TECHN C
   Macko P., 2010, 8th USENIX Conference on File and Storage Technologies, San Jose, CA, USA, February 23-26, 2010, P15
   McKusick M. K., 2004, ACM Queue, V2, P58, DOI 10.1145/1035594.1035622
   Narasimha Reddy A. L., 1993, Proceedings ACM Multimedia 93, P225, DOI 10.1145/166266.166292
   Povzner A, 2008, EUROSYS'08: PROCEEDINGS OF THE EUROSYS 2008 CONFERENCE, P13, DOI 10.1145/1357010.1352595
   Rompogiannakis Y., 1998, Proceedings ACM Multimedia 98, P297, DOI 10.1145/290747.290785
   Shenoy P. J., 1998, Performance Evaluation Review, V26, P44, DOI 10.1145/277858.277871
   Storage Performance Council, 2017, PC BENCHM 1 SPC 1 SP
   Thomasian Alexander, 2011, Computer Architecture News, V39, P8, DOI 10.1145/2024716.2024719
   Traeger Avishay, 2008, ACM Transaction on Storage, V4, DOI 10.1145/1367829.1367831
   Xu Yuehai., 2011, FAST, P119
   Yang C.-W., 2006, Performance Evaluation Review, V34, P97, DOI 10.1145/1140103.1140290
   Yasa Giridhar Appaji Nag, 2012, ACM SIGOPS OPERATING, V46, P57
   Zaharia M, 2010, EUROSYS'10: PROCEEDINGS OF THE EUROSYS 2010 CONFERENCE, P265
   ZFS at OpenSolaris community, 2015, ZFS OPENSOLARIS COMM
   [No title captured]
NR 37
TC 0
Z9 0
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAY
PY 2018
VL 14
IS 2
AR 14
DI 10.1145/3193741
PG 27
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GI6TM
UT WOS:000434635800003
DA 2024-07-18
ER

PT J
AU Ho, CC
   Chang, YM
   Chang, YH
   Kuo, TW
AF Ho, Chien-Chung
   Chang, Yu-Ming
   Chang, Yuan-Hao
   Kuo, Tei-Wei
TI An SLC-Like Programming Scheme for MLC Flash Memory
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Flash memory; reliability; MLC flash memory; SLC performance; cell
   programming method; chip density; multilevel cell technique;
   threshold-voltage relationship; bit error rate; hardware; programming;
   threshold voltage
ID TRANSLATION LAYER
AB Although the multilevel cell (MLC) technique is widely adopted by flash-memory vendors to boost the chip density and lower the cost, it results in serious performance and reliability problems. Different from past work, a new cell programming method is proposed to not only significantly improve chip performance but also reduce the potential bit error rate. In particular, a single-level cell (SLC)-like programming scheme is proposed to better explore the threshold-voltage relationship to denote different MLC bit information, which in turn drastically provides a larger window of threshold voltage similar to that found in SLC chips. It could result in less programming iterations and simultaneously a much less reliability problem in programming flash-memory cells. In the experiments, the new programming scheme could accelerate the programming speed up to 742% and even reduce the bit error rate up to 471% for MLC pages.
C1 [Ho, Chien-Chung] Natl Chung Cheng Univ, Dept Comp Sci Informat Engn, Chiayi, Taiwan.
   [Chang, Yu-Ming] Macronix Int Co Ltd, Emerging Syst Lab, Hsinchu, Taiwan.
   [Chang, Yuan-Hao] Acad Sinica, Inst Informat Sci, Taipei, Taiwan.
   [Kuo, Tei-Wei] Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei, Taiwan.
C3 National Chung Cheng University; Macronix International Company;
   Academia Sinica - Taiwan; National Taiwan University
RP Ho, CC (corresponding author), Natl Chung Cheng Univ, Dept Comp Sci Informat Engn, Chiayi, Taiwan.
EM ccho@cs.ccu.edu.tw; d00922001@csie.ntu.edu.tw;
   johnson@iis.sinica.edu.tw; ktw@csie.ntu.edu.tw
RI Chang, Yuan-Hao/ABA-6935-2020
OI Chang, Yuan-Hao/0000-0002-1282-2111; KUO, TEI-WEI/0000-0003-1974-0394
FU Ministry of Science and Technology [106-2218-E-194-012-MY3,
   106-3114-E-002-008, 105-2221-E-001-013-MY3, 105-2221-E-001-004-MY2]
FX This work was supported in part by the Ministry of Science and
   Technology under grants 106-2218-E-194-012-MY3, 106-3114-E-002-008,
   105-2221-E-001-013-MY3, and 105-2221-E-001-004-MY2.
CR [Anonymous], 2009, MICRO
   Ban Amir, 1995, US Patent, Patent No. [5,404,485, 5404485]
   Chang Yu-Ming, 2015, P 2015 52 ACM EDAC I
   Chang YH, 2013, ACM T EMBED COMPUT S, V13, DOI 10.1145/2512467
   Chen Chih-Ping, 2012, P 2012 S VLSI TECHN, DOI [10.1109/VLSIT.2012.6242476, DOI 10.1109/VLSIT.2012.6242476]
   Cho Hyunjin, 2009, P 2009 DES AUT TEST
   Cho YS, 2013, IEEE J SOLID-ST CIRC, V48, P948, DOI 10.1109/JSSC.2013.2237974
   Compagnoni CM, 2009, IEEE ELECTR DEVICE L, V30, P984, DOI 10.1109/LED.2009.2026658
   Exchange Trace, 2010, SNIA IOTTA REP
   Gupta Aayush, P 14 INT C ARCH SUPP, P229
   Im S, 2011, IEEE T COMPUT, V60, P80, DOI 10.1109/TC.2010.197
   Jagmohan A., 2010, P 2010 IEEE 26 S MAS
   Japan Toshiba Tokyo, 2012, TOSH MOS DIG INT CIR
   Kim J, 2002, IEEE T CONSUM ELECTR, V48, P366
   Lee SW, 2006, LECT NOTES COMPUT SC, V4097, P879
   Lu Chih-Yuan, 2007, P INT S ADV GAT STAC
   Park KT, 2008, IEEE J SOLID-ST CIRC, V43, P919, DOI 10.1109/JSSC.2008.917558
   Park KT, 2015, IEEE J SOLID-ST CIRC, V50, P204, DOI [10.1109/JSSC.2014.2352293, 10.1109/ISSCC.2014.6757458]
   Park SH, 2009, IEEE T CONSUM ELECTR, V55, P1392, DOI 10.1109/TCE.2009.5278005
   Qin Zhiwei, 2011, P 2011 48 ACM EDAC I
   Shin Seung-Hwan, 2012, P 2012 S VLSI CIRC V, DOI [10.1109/VLSIC.2012.6243825, DOI 10.1109/VLSIC.2012.6243825]
   Shin YunSeung, 2005, P 2005 S VLSI CIRC V, DOI [10.1109/VLSIC.2005.1469355, DOI 10.1109/VLSIC.2005.1469355]
   Shinohara Takayuki, 1999, US Patent, Patent No. [5,905,993, 5905993]
   South Korea Samsung, 2014, SAMS K90KGY8S7C CCKO
   Suh KD, 1995, IEEE J SOLID-ST CIRC, V30, P1149, DOI 10.1109/4.475701
   Tanaka H., 2007, Proc. of IEEE Symp. on VLSI Tech, P14, DOI DOI 10.1109/VLSIT.2007.4339708
   TPC, 2018, TPC C BENCHM REV 5 1
   Wei Qingsong., 2011, S MASS STORAGE SYSTE, P1, DOI DOI 10.1109/MSST.2011.5937217
   Wu Chin-Hsien, 2006, P 2006 IEEE ACM INT, DOI [10.1109/ICCAD.2006.320107, DOI 10.1109/ICCAD.2006.320107]
NR 29
TC 2
Z9 3
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD APR
PY 2018
VL 14
IS 1
SI SI
AR 11
DI 10.1145/3129257
PG 26
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GH5ZN
UT WOS:000433517600011
DA 2024-07-18
ER

PT J
AU Qin, C
   Li, JW
   Lee, PPC
AF Qin, Chuan
   Li, Jingwei
   Lee, Patrick P. C.
TI The Design and Implementation of a Rekeying-Aware Encrypted
   Deduplication Storage System
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Encryption; deduplication; rekeying; cloud storage
ID MESSAGE-LOCKED ENCRYPTION
AB Rekeying refers to an operation of replacing an existing key with a new key for encryption. It renews security protection to protect against key compromise and enable dynamic access control in cryptographic storage. However, it is non-trivial to realize efficient rekeying in encrypted deduplication storage systems, which use deterministic content-derived encryption keys to allow deduplication on ciphertexts. We design and implement a rekeying-aware encrypted deduplication (REED) storage system. REED builds on a deterministic version of all-or-nothing transform, such that it enables secure and lightweight rekeying, while preserving the deduplication capability. We propose two REED encryption schemes that trade between performance and security and extend REED for dynamic access control. We implement a REED prototype with various performance optimization techniques and demonstrate how we can exploit similarity to mitigate key generation overhead. Our trace-driven testbed evaluation shows that our REED prototype maintains high performance and storage efficiency.
C1 [Qin, Chuan; Lee, Patrick P. C.] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.
   [Li, Jingwei] Univ Elect Sci & Technol China, Ctr Cyber Secur, Chengdu, Sichuan, Peoples R China.
C3 Chinese University of Hong Kong; University of Electronic Science &
   Technology of China
RP Li, JW (corresponding author), Univ Elect Sci & Technol China, Ctr Cyber Secur, Chengdu, Sichuan, Peoples R China.
EM cqin@cse.cuhk.edu.hk; jwli@uestc.edu.cn; pclee@cse.cuhk.edu.hk
RI Lee, Patrick P. C./I-7165-2013; Li, Jingwei/L-1328-2016
OI Lee, Patrick P. C./0000-0002-4501-4364; Li, Jingwei/0000-0001-8457-0454
FU HKRGC [GRF CUHK413813, CRF C7036-15G]; Cisco University Research Program
   Fund (CG) from Silicon Valley Community Foundation [593822]; Fundamental
   Research Funds for the Central Universities [ZYGX2016KYQD115]; National
   Natural Science Foundation of China [61602092]
FX This work was supported in part by grants GRF CUHK413813 and CRF
   C7036-15G from HKRGC, Cisco University Research Program Fund (CG#593822)
   from Silicon Valley Community Foundation, Fundamental Research Funds for
   the Central Universities (ZYGX2016KYQD115), and National Natural Science
   Foundation of China (61602092).
CR Abadi M, 2013, LECT NOTES COMPUT SC, V8042, P374, DOI 10.1007/978-3-642-40041-4_21
   Abdalla M, 2000, LECT NOTES COMPUT SC, V1976, P546
   Adya A, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE FIFTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P1
   Amazon, 2014, ARCH GEN DAT SEC COM
   Anderson PO, 2010, PHARMACY INFORMATICS, P1, DOI 10.1145/1852658.1852665
   [Anonymous], 2011, DEVELOPMENT
   [Anonymous], 2012, TECHNICAL REPORT
   [Anonymous], 2009, ACM INT C P SERIES
   [Anonymous], 2011, P 9 USENIX C FIL STO
   [Anonymous], P NETW DISTR SYST SE
   [Anonymous], 2008, Proceedings of the 4th ACM international workshop on Storage security and survivability
   [Anonymous], P 10 USENIX C FIL ST
   [Anonymous], 2011, P USENIX ANN TECHN C
   [Anonymous], 2010, FAST
   [Anonymous], 2015, P 2015 USENIX ANN TE
   [Anonymous], 2009, 7 USENIX C FIL STOR
   Armknecht F, 2015, CCS'15: PROCEEDINGS OF THE 22ND ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P886, DOI 10.1145/2810103.2813630
   Ateniese G., 2006, ACM Transactions on Information and Systems Security, V9, P1, DOI 10.1145/1127345.1127346
   Ateniese G, 2007, CCS'07: PROCEEDINGS OF THE 14TH ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P598
   Backes M, 2006, LECT NOTES COMPUT SC, V4189, P327
   Bellare M, 2013, 22 USENIX SEC S USEN, P179
   Bellare M, 2015, LECT NOTES COMPUT SC, V9020, P516, DOI 10.1007/978-3-662-46447-2_23
   Bellare M, 2013, LECT NOTES COMPUT SC, V7881, P296, DOI 10.1007/978-3-642-38348-9_18
   Bethencourt J., 2011, CP ABE TOOLKIT
   Bethencourt J, 2007, P IEEE S SECUR PRIV, P321, DOI 10.1109/sp.2007.11
   Bhagwat D, 2009, 2009 IEEE INTERNATIONAL SYMPOSIUM ON MODELING, ANALYSIS & SIMULATION OF COMPUTER AND TELECOMMUNICATION SYSTEMS (MASCOTS), P237
   Black J, 2006, USENIX ASSOCIATION PROCEEDINGS OF THE 2006 USENIX ANNUAL TECHNICAL CONFERENCE, P85
   Boneh D, 2005, LECT NOTES COMPUT SC, V3621, P258
   Boneh D, 2004, J CRYPTOL, V17, P297, DOI 10.1007/s00145-004-0314-9
   Boneh D, 1996, PROCEEDINGS OF THE SIXTH ANNUAL USENIX SECURITY SYMPOSIUM: FOCUSING ON APPLICATIONS OF CRYPTOGRAPHY, P91
   Broder AZ, 1998, COMPRESSION AND COMPLEXITY OF SEQUENCES 1997 - PROCEEDINGS, P21, DOI 10.1109/SEQUEN.1997.666900
   Cox LR, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE FIFTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P285, DOI 10.1145/1060289.1060316
   Csaplar D., 2011, BUILDING BUSINESS RE
   Debian Security Advisory, 2008, DSA 1571 1 OP PRED R
   Diesburg SM, 2010, ACM COMPUT SURV, V43, DOI 10.1145/1824795.1824797
   Douceur JR, 2002, INT CON DISTR COMP S, P617, DOI 10.1109/ICDCS.2002.1022312
   Duan Y., 2014, P 6 ED ACM WORKSH CL, P57
   File systems and Storage Lab, 2014, FSL TRAC SNAPSH PUBL
   Goldwasser S, 2008, Lecture notes on cryptography
   Goyal V., 2006, P 2006 INT C PRIVACY, P1
   Halevi S, 2011, PROCEEDINGS OF THE 18TH ACM CONFERENCE ON COMPUTER & COMMUNICATIONS SECURITY (CCS 11), P491, DOI 10.1145/2046707.2046765
   Harnik D, 2010, IEEE SECUR PRIV, V8, P40, DOI 10.1109/MSP.2010.187
   Jingwei Li, 2016, 2016 46th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN). Proceedings, P618, DOI 10.1109/DSN.2016.62
   Juels A, 2007, CCS'07: PROCEEDINGS OF THE 14TH ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P584
   Kallahalla M, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE 2ND USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST'03), P29
   Kaminsky Dan, 2011, THESE ARE NOT CERTS
   Li C., 2015, P USENIX ANN TECH C, P111
   Liu J, 2015, CCS'15: PROCEEDINGS OF THE 22ND ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P874, DOI 10.1145/2810103.2813623
   Musthaler Linda, 2013, CLOUD ENCRYPTION CON
   National Institute of Health (NIH),, 2015, NIH SEC BEST PRACT C
   OpenSSL, 2015, OPENSSL CRYPT SSL TL
   Puttaswamy Krishna P. N., 2011, P 2 ACM S CLOUD COMP
   Rabin M. O, 1981, TRCSE0301 HARV U
   Rahumed A., 2011, Proceedings of the 2011 International Conference on Parallel Processing Workshops (ICPPW 2011), P160, DOI 10.1109/ICPPW.2011.17
   Reardon J, 2013, P IEEE S SECUR PRIV, P301, DOI 10.1109/SP.2013.28
   Rivest RL, 1997, LECT NOTES COMPUT SC, V1267, P210
   Shoup V, 2000, LECT NOTES COMPUT SC, V1807, P207
   Sotirov A., 2008, MD5 CONSIDERED HARMF
   Stein LD, 2010, GENOME BIOL, V11, DOI 10.1186/gb-2010-11-5-207
   Sun Zhu, 2016, P 32 S MASS STOR SYS
   U. S. Computer Emergency Readiness Team, 2014, OPENSSL HEARTBL VULN
   Watanabe D, 2013, INT CONF CLOUD COMP, P493, DOI 10.1109/CloudCom.2013.72
   WEBSTER AF, 1986, LECT NOTES COMPUT SC, V218, P523
   Yinjin Fu, 2012, Middleware 2012. ACM/IFIP/USENIX 13th International Middleware Conference. Proceedings, P354, DOI 10.1007/978-3-642-35170-9_18
   Zachary N., 2005, P 4 USENIX C FIL STO
   Zheng X., 2015, P 10 ACM S INF COMP, P63
   Zhou Y., 2015, Mass Storage Systems and Technologies (MSST), 2015 31st Symposium on, P1
   Zhu B, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P269
NR 68
TC 36
Z9 37
U1 1
U2 16
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAR
PY 2017
VL 13
IS 1
SI SI
AR 9
DI 10.1145/3032966
PG 30
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ER3WE
UT WOS:000398729600009
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Fryer, D
   Qin, M
   Sun, J
   Lee, KW
   Brown, AD
   Goel, A
AF Fryer, Daniel
   Qin, Mike
   Sun, Jack
   Lee, Kah Wai
   Brown, Angela Demke
   Goel, Ashvin
TI Checking the Integrity of Transactional Mechanisms
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Reliability; Verification; Performance; Measurement; Metadata
   consistency; atomicity; durability; runtime verification; file system
   checker; Ext3; Btrfs
AB Data corruption is the most common consequence of file-system bugs. When such corruption occurs, offline check and recovery tools must be used, but they are error prone and cause significant downtime. Previously we showed that a runtime checker for the Ext3 file system can verify that metadata updates are consistent, helping detect corruption in metadata blocks at transaction commit time. However, corruption can still occur when a bug in the file system's transactional mechanism loses, misdirects, or corrupts writes. We show that a runtime checker must enforce the atomicity and durability properties of the file system on every write, in addition to checking transactions at commit time, to provide the strong guarantee that every block write will maintain file system consistency.
   We identify the invariants that need to be enforced on journaling and shadow paging file systems to preserve the integrity of committed transactions. We also describe the key properties that make it feasible to check these invariants for a file system. Based on this characterization, we have implemented runtime checkers for Ext3 and Btrfs. Our evaluation shows that both checkers detect data corruption effectively, and they can be used during normal operation with low overhead.
C1 [Fryer, Daniel; Qin, Mike; Sun, Jack; Lee, Kah Wai; Brown, Angela Demke; Goel, Ashvin] Univ Toronto, Toronto, ON M5S 1A1, Canada.
C3 University of Toronto
RP Brown, AD (corresponding author), Univ Toronto, Toronto, ON M5S 1A1, Canada.
EM demke@cs.toronto.edu
FU NSERC
FX This research was supported by NSERC through the Discovery Grants and
   Graduate Scholarships programs.
CR [Anonymous], P WORKSH HOT TOP SYS
   ARPACI-DUSSEAU A.C., 2013, P USENIX C FIL STOR
   Bairavasundaram L., 2009, P USENIX ANN TECHN C
   Behrens S., 2011, BTRFS RUNTIME INTEGR
   Bonwick Jeff, 2008, ZFS: The Last Word in File Systems
   Carreira JoaoCarlos Menezes., 2012, P 7 ACM EUROPEAN C C, P239
   Chidambaram V., 2012, P 10 USENIX C FIL ST
   Custer H., 1994, INSIDE WINDOWS NT FI
   Do T., 2013, P USENIX C FIL STOR
   Filebench, 2011, FILEBENCH VERSION 1
   Fryer D, 2012, ACM T STORAGE, V8, DOI 10.1145/2385603.2385608
   Griffin D, 2008, JBD CORRECTLY UNESCA
   Gunawi Haryadi S., 2007, Operating Systems Review, V41, P293, DOI 10.1145/1323293.1294290
   Gunawi Haryadi S., 2008, P 8 USENIX C OP SYST, P131
   Hitz D., 1994, P USENIX ANN TECHN C
   Kara J, 2010, EXT4 ALWAYS J QUOTA
   Kara J, 2012, JBD WHITE J SUPERBLO
   Ma Ao, 2013, P USENIX C FIL STOR
   Macko P., 2010, P USENIX C FIL STOR
   Mesnier M, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P57
   Meyer D.T., 2011, Proceedings of the 9th USENIX conference on File and stroage technologies, P1
   Miller R, 2008, JOYENT SERVICES BACK
   Prabhakaran Vijayan, 2005, P OF THE 20 ACM S ON, V39, P206
   Rodeh O, 2013, ACM T STORAGE, V9, DOI 10.1145/2501620.2501623
   Sandeen E., 2012, EXT4 FIX UNJOURNALED
   Sct S.T., 1998, Journaling the linux ext2fs filesystem
   Sivathanu G, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P15
   Sivathanu M, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE 2ND USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST'03), P73
   Sivathanu Muthian, 2005, P USENIX C FIL STOR
   Sundararaman S., 2010, P USENIX C FIL STOR
   Sweeney A., 1996, P USENIX ANN TECHN C
   Ts'o T., 2012, RE APPARENT SERIOUS
   Yang JF, 2006, ACM T COMPUT SYST, V24, P393, DOI 10.1145/1189256.1189259
   Zhang Y., 2010, P USENIX C FIL STOR
NR 34
TC 5
Z9 6
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD OCT
PY 2014
VL 10
IS 4
SI SI
AR 17
DI 10.1145/2675113
PG 23
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AT0LD
UT WOS:000344626700005
DA 2024-07-18
ER

PT J
AU Elerath, JG
   Schindler, J
AF Elerath, Jon G.
   Schindler, Jiri
TI Beyond MTTDL: A Closed-Form RAID 6 Reliability Equation
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Storage systems; RAID reliability; Design; Algorithms; Reliability;
   Measurement
AB We introduce a new closed-form equation for estimating the number of data-loss events for a redundant array of inexpensive disks in a RAID-6 configuration. The equation expresses operational failures, their restorations, latent (sector) defects, and disk media scrubbing by time-based distributions that can represent non-homogeneous Poisson processes. It uses two-parameter Weibull distributions that allows the distributions to take on many different shapes, modeling increasing, decreasing, or constant occurrence rates. This article focuses on the statistical basis of the equation. It also presents time-based distributions of the four processes based on an extensive analysis of field data collected over several years from 10,000s of commercially available systems with 100,000s of disk drives. Our results for RAID-6 groups of size 16 indicate that the closed-form expression yields much more accurate results compared to the MTTDL reliability equation and matching computationally-intensive Monte Carlo simulations.
C1 [Elerath, Jon G.] Reliabil Consulting Serv, Placerville, CA 95667 USA.
   [Schindler, Jiri] NetApp, Sunnyvale, CA 94089 USA.
C3 NetApp, Inc.
RP Schindler, J (corresponding author), NetApp, 495 East Java Dr, Sunnyvale, CA 94089 USA.
EM jon.elerath@gmail.com; jiri.schindler@netapp.com
CR [Anonymous], COMMUNICATION
   [Anonymous], P 21 INT S COMP ARCH
   [Anonymous], NETAPP DAT ONTAP 8 O
   [Anonymous], BETT RAID STRAT HIGH
   [Anonymous], P 15 IEEE PAC RIM IN
   [Anonymous], 2008, P 6 USENIX C FIL STO
   [Anonymous], IEEE T RELIAB
   [Anonymous], P IEEE INT C NETW AR
   [Anonymous], 905 NETAPP
   [Anonymous], 1988, P ACM SIGMOD INT C M
   [Anonymous], 1993, RELIABILITY LIFE TES
   [Anonymous], P 2 USENIX C HOT TOP
   [Anonymous], RAID REL ANTH PRIM
   [Anonymous], 2007, EMC CLARiiON RAID 6 Technology: A Detailed Review
   [Anonymous], P 5 USENIX C FIL STO
   [Anonymous], P 39 INT C DEP SYST
   [Anonymous], P IEEE REL MAINT S
   [Anonymous], IEEE T COMPUT
   [Anonymous], ACM QUEUE
   [Anonymous], J PARALLEL DISTRIB C
   [Anonymous], P 36 INT C DEP SYST
   [Anonymous], ACM SIGMETRICS PERFO
   ASCHER H, 1983, TECHNOMETRICS, V25, P320, DOI 10.2307/1267848
   Bairavasundaram LN, 2007, PERF E R SI, V35, P289
   Bazovsky I., 1961, RELIABILITY THEORY P
   Corbett P., 2004, P 3 USENIX C FIL STO, P1
   Kececioglu D.B., 1993, RELIABILITY LIFE TES, V2
   MALHOTRA M, 1993, J PARALLEL DISTR COM, V17, P146, DOI 10.1006/jpdc.1993.1013
   Nelson W., 1982, Applied life data analysis, DOI DOI 10.1002/0471725234
   Nelson W., 1990, ACCELERATED TESTING
   Nelson W. B., 2003, RECURRENT EVENTS DAT
   Schroeder B., 2007, P 5 USENIX C FIL STO
   Thomasian Alexander, 2009, ACM Transaction on Storage, V5, DOI 10.1145/1629075.1629076
   THOMPSON WA, 1981, TECHNOMETRICS, V23, P1, DOI 10.2307/1267969
   Tobias P. A., 2011, APPL RELIABILITY
NR 35
TC 24
Z9 25
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAR
PY 2014
VL 10
IS 2
AR 7
DI 10.1145/2577386
PG 21
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AF2CV
UT WOS:000334521100003
DA 2024-07-18
ER

PT J
AU Rodeh, O
   Bacik, J
   Mason, C
AF Rodeh, Ohad
   Bacik, Josef
   Mason, Chris
TI BTRFS: The Linux B-Tree Filesystem
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Algorithms; Performance; B-trees; concurrency; copy-on-write;
   filesystem; RAID; shadowing; snapshots
AB BTRFS is a Linux filesystem that has been adopted as the default filesystem in some popular versions of Linux. It is based on copy-on-write, allowing for efficient snapshots and clones. It uses B-trees as its main on-disk data structure. The design goal is to work well for many use cases and workloads. To this end, much effort has been directed to maintaining even performance as the filesystem ages, rather than trying to support a particular narrow benchmark use-case.
   Linux filesystems are installed on smartphones as well as enterprise servers. This entails challenges on many different fronts.
   - Scalability. The filesystem must scale in many dimensions: disk space, memory, and CPUs.
   - Data integrity. Losing data is not an option, and much effort is expended to safeguard the content. This includes checksums, metadata duplication, and RAID support built into the filesystem.
   - Disk diversity. The system should work well with SSDs and hard disks. It is also expected to be able to use an array of different sized disks, which poses challenges to the RAID and striping mechanisms.
   This article describes the core ideas, data structures, and algorithms of this filesystem. It sheds light on the challenges posed by defragmentation in the presence of snapshots, and the tradeoffs required to maintain even performance in the face of a wide spectrum of workloads.
C1 [Rodeh, Ohad] IBM Corp, Armonk, NY USA.
C3 International Business Machines (IBM)
RP Rodeh, O (corresponding author), IBM Corp, Armonk, NY USA.
EM orodeh@us.ibm.com
CR [Anonymous], 1988, A case for redundant arrays of inexpensive disks RAID, DOI DOI 10.1145/50202.50214
   Bonwick J., LAST WORD FILE SYSTE
   CALLAGHAN B, 1995, 1813 RFC IETF
   CHANG F, 2006, P 7 USENIX S OP SYST, P15
   COMER D, 1979, COMPUT SURV, V11, P121, DOI 10.1145/356770.356776
   Dean Jeff., LEVELDB
   Edwards JohnK., 2008, ATC 08, P129
   Gailly Jean loup, ZLIB
   Heizer I., 1996, COMMON INTE IN PRESS
   Hellwig C., 2009, USENIX LOGIN MAGAZIN
   Henson V., 2003, FILE STORAGE TECHNOL
   Hitz Dave, 1994, USENIX
   Macko P., 2010, P 8 USENIX C FIL STO
   Mason C., 2007, BTRFS
   Mathur Avantika, 2007, P LIN S, V2
   ONeil P, 1996, ACTA INFORM, V33, P351, DOI 10.1007/s002360050048
   REED IS, 1960, J SOC IND APPL MATH, V8, P300, DOI 10.1137/0108018
   Reiser H., 2001, REISERFS
   Ren K., 2012, CMUPDL12103
   Rodeh O., 2006, H248 IBM
   Rodeh O., 2010, RJ10464 IBM
   Rodeh O., 2008, ACM T STORAGE, V3, P4
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   Sears Russell, 2012, P 2012 ACM SIGMOD IN, P217, DOI [10.1145/2213836.2213862, DOI 10.1145/2213836.2213862]
   Shepler S., 2000, 3010 RFC IETF
   Shetty P., 2013, P 11 USENIX C FIL ST
   Sweeney A., 1996, P USENIX ANN TECHN C
   [No title captured]
NR 28
TC 262
Z9 336
U1 1
U2 20
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2013
VL 9
IS 3
AR 9
DI 10.1145/2501620.2501623
PG 32
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 216XX
UT WOS:000324321200003
DA 2024-07-18
ER

PT J
AU Li, JX
   Zhang, YM
   Lu, S
   Gunawi, HS
   Gu, XH
   Huang, F
   Li, DS
AF Li, Jiaxin
   Zhang, Yiming
   Lu, Shan
   Gunawi, Haryadi S.
   Gu, Xiaohui
   Huang, Feng
   Li, Dongsheng
TI Performance Bug Analysis and Detection for Distributed Storage and
   Computing Systems
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Storage and computing systems performance; blocking bugs
AB This article systematically studies 99 distributed performance bugs from five widely deployed distributed storage and computing systems (Cassandra, HBase, HDFS, Hadoop MapReduce and ZooKeeper). We present the TaxPerf database, which collectively organizes the analysis results as over 400 classification labels and over 2,500 lines of bug re-description. TaxPerf is classified into six bug categories (and 18 bug subcategories) by their root causes; resource, blocking, synchronization, optimization, configuration, and logic. TaxPerf can be used as a benchmark for performance bug studies and debug tool designs. Although it is impractical to automatically detect all categories of performance bugs in TaxPerf, we find that an important category of blocking bugs can be effectively solved by analysis tools. We analyze the cascading nature of blocking bugs and design an automatic detection tool called PCatch, which (i) performs program analysis to identify code regions whose execution time can potentially increase dramatically with the workload size; (ii) adapts the traditional happens-beforemodel to reason about software resource contention and performance dependency relationship; and (iii) uses dynamic tracking to identify whether the slowdown propagation is contained in one job. Evaluation shows that PCatch can accurately detect blocking bugs of representative distributed storage and computing systems by observing system executions under small-scale workloads.
C1 [Li, Jiaxin; Zhang, Yiming; Huang, Feng; Li, Dongsheng] Natl Univ Def Technol, Sanyi Rd, Changsha, Hunan, Peoples R China.
   [Zhang, Yiming] Xiamen Univ, Sanyi Rd, Changsha, Hunan, Peoples R China.
   [Lu, Shan; Gunawi, Haryadi S.] Univ Chicago, 5801 S Ellis Ave, Chicago, IL 60637 USA.
   [Gu, Xiaohui] North Carolina State Univ, 2101 Hillsborough St, Raleigh, NC 27695 USA.
C3 National University of Defense Technology - China; Xiamen University;
   University of Chicago; North Carolina State University
RP Zhang, YM (corresponding author), Natl Univ Def Technol, Sanyi Rd, Changsha, Hunan, Peoples R China.; Zhang, YM (corresponding author), Xiamen Univ, Sanyi Rd, Changsha, Hunan, Peoples R China.
EM licyh@nudt.edu.cn; zhangyiming@nicexlab.com; shanlu@uchicago.edu;
   haryadi@cs.uchicago.edu; xgu@ncsu.edu; fenghuang@nudt.edu.cn;
   dsli@nudt.edu.cn
RI Luo, Shan/JEP-7010-2023; yiming, zhang/IXN-8360-2023; Li,
   Dongsheng/HHC-4903-2022
OI yiming, zhang/0009-0001-9484-9149; Gunawi, Haryadi/0000-0003-3680-8450
FU National Key Research and Development Program of China [2022YFB4500302];
   Scientific Research Program of National University of Defense Technology
   [ZK20-03]; National Natural Science Foundation of China [62025208];
   Natural Science Foundation of Hunan Province of China [2022JJ40555]
FX This work is supported by the National Key Research and Development
   Program of China (grant no. 2022YFB4500302), the Scientific Research
   Program of National University of Defense Technology (grant no.
   ZK20-03), the National Natural Science Foundation of China (grant no.
   62025208), and the Natural Science Foundation of Hunan Province of China
   (grant no. 2022JJ40555).
CR Agrawal Hiralal, 1990, PLDI, P246, DOI 10.1145/93542.93576
   Aguilera M. K., 2003, Operating Systems Review, V37, P74, DOI 10.1145/1165389.945454
   Altman E, 2010, ACM SIGPLAN NOTICES, V45, P739, DOI 10.1145/1932682.1869519
   [Anonymous], 2010, Proceedings of the 7th USENIX Conference on Networked Systems Design and Implementation, NSDI'10, (USA)
   [Anonymous], 2014, P 11 USENIX C OPERAT
   Apache, MAPREDUCE 4576
   Attariyan Mona., 2010, OSDI PAGES, P237
   Attariyan Mona, 2012, P 10 USENIX C OP SYS, P307
   Chow Michael, 2014, P 11 S OP SYST DES I, P217
   Coppa E, 2012, ACM SIGPLAN NOTICES, V47, P89, DOI 10.1145/2345156.2254076
   Curtsinger C, 2015, SOSP'15: PROCEEDINGS OF THE TWENTY-FIFTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P184, DOI 10.1145/2815400.2815409
   David F, 2014, ACM SIGPLAN NOTICES, V49, P291, DOI [10.1145/2714064.2660210, 10.1145/2660193.2660210]
   Dean Daniel J., 2014, P ACM S CLOUD COMP S, P1, DOI [10.1145/2670979.2670987, DOI 10.1145/2670979.2670987]
   Deng DD, 2013, ACM SIGPLAN NOTICES, V48, P785, DOI [10.1145/2544173.2509539, 10.1145/2509136.2509539]
   Dufour B., 2008, A scalable technique for characterizing the usage of temporaries in framework-intensive Java applications, P59, DOI DOI 10.1145/1453101.1453111
   Goldsmith Simon F., 2007, P 6 JOINT M EUR SOFT, P395, DOI 10.1145/1287624.1287681
   Gulwani S, 2010, PLDI '10: PROCEEDINGS OF THE 2010 ACM SIGPLAN CONFERENCE ON PROGRAMMING LANGUAGE DESIGN AND IMPLEMENTATION, P292, DOI 10.1145/1806596.1806630
   Gulwani S, 2009, LECT NOTES COMPUT SC, V5643, P51, DOI 10.1007/978-3-642-02658-4_7
   Gunawi H. S., 2014, P ACM S CLOUD COMP S, P1
   Gupta D, 2011, ACM T COMPUT SYST, V29, DOI 10.1145/1963559.1963560
   hadoop.apache, HDFS ARCH
   hbase.apache, AP HBASE PROJ
   Herodotou H., 2011, Proceedings of the 2nd ACM Symposium on Cloud Computing; ACM, DOI [DOI 10.1145/2038916.2038934, 10.1145/2038916.2038934]
   Huang P, 2014, 36TH INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING (ICSE 2014), P60, DOI 10.1145/2568225.2568232
   IBM, MAIN PAG WALAWIKI
   jboss javassist, JAV
   Jin GL, 2012, ACM SIGPLAN NOTICES, V47, P77, DOI 10.1145/2345156.2254075
   Killian C, 2007, PLDI'07: PROCEEDINGS OF THE 2007 ACM SIGPLAN CONFERENCE ON PROGRAMMING LANGUAGE DESIGN AND IMPLEMENTATION, P179, DOI 10.1145/1250734.1250755
   Killian Charles., 2010, Proceedings of the 18th ACM SIGSOFT International Symposium on Foundations of Software Engineering, FSE '10, P17, DOI DOI 10.1145/1882291.1882297
   Lakshman Avinash, 2010, Operating Systems Review, V44, P35, DOI 10.1145/1773912.1773922
   LAMPORT L, 1978, COMMUN ACM, V21, P558, DOI 10.1145/359545.359563
   Leesatapornwongsa T, 2016, ACM SIGPLAN NOTICES, V51, P517, DOI 10.1145/2954679.2872374
   Li JX, 2018, EUROSYS '18: PROCEEDINGS OF THE THIRTEENTH EUROSYS CONFERENCE, DOI 10.1145/3190508.3190552
   Liu HP, 2017, TWENTY-SECOND INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXII), P677, DOI 10.1145/3037697.3037735
   Liu HP, 2016, FSE'16: PROCEEDINGS OF THE 2016 24TH ACM SIGSOFT INTERNATIONAL SYMPOSIUM ON FOUNDATIONS OF SOFTWARE ENGINEERING, P715, DOI 10.1145/2950290.2950309
   Liu TP, 2014, ACM SIGPLAN NOTICES, V49, P3, DOI [10.1145/2555243.2555244, 10.1145/2692916.2555244]
   Mace J, 2015, SOSP'15: PROCEEDINGS OF THE TWENTY-FIFTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P378, DOI 10.1145/2815400.2815415
   Musuvathi M., 2008, Proceedings of the 8th USENIX conference on Operating systems design and implementation, OSDI'08, P267
   Nanavati M., 2013, P 8 ACM EUROPEAN C C, P141
   Netzer R. H. B., 1991, SIGPLAN Notices, V26, P133, DOI 10.1145/109626.109640
   Nistor A, 2015, 2015 IEEE/ACM 37TH IEEE INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING, VOL 1, P902, DOI 10.1109/ICSE.2015.100
   Nistor A, 2013, PROCEEDINGS OF THE 35TH INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING (ICSE 2013), P562, DOI 10.1109/ICSE.2013.6606602
   Olivo O, 2015, ACM SIGPLAN NOTICES, V50, P369, DOI [10.1145/2813885.2737966, 10.1145/273924.2737966]
   Oracle, HPROF HEAP CPU PROF
   Shen K, 2005, USENIX ASSOCIATION PROCEEDINGS OF THE 4TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P309
   Song LH, 2014, ACM SIGPLAN NOTICES, V49, P561, DOI [10.1145/2660193.2660234, 10.1145/2714064.2660234]
   Stewart Christopher, 2006, P 2 C HOT TOPICS SYS, P1
   Tan JQ, 2010, INT CON DISTR COMP S, DOI 10.1109/ICDCS.2010.63
   ul Alam MM, 2017, PROCEEDINGS OF THE TWELFTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS (EUROSYS 2017), P298, DOI 10.1145/3064176.3064186
   Wang S, 2018, ACM SIGPLAN NOTICES, V53, P154, DOI [10.1145/3173162.3173206, 10.1145/3296957.3173206]
   Wang Y., Natural Gas Industry B, V1, P129, DOI [10.1016/j.ngib.2014.11.002, DOI 10.1016/J.NGIB.2014.11.002]
   WEISER M, 1984, IEEE T SOFTWARE ENG, V10, P352, DOI 10.1109/TSE.1984.5010248
   Wert A, 2013, PROCEEDINGS OF THE 35TH INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING (ICSE 2013), P552, DOI 10.1109/ICSE.2013.6606601
   Xiao Xusheng, 2013, P 2013 INT S SOFTW T, P90, DOI DOI 10.1145/2483760.2483784
   Xu GQ, 2010, PLDI '10: PROCEEDINGS OF THE 2010 ACM SIGPLAN CONFERENCE ON PROGRAMMING LANGUAGE DESIGN AND IMPLEMENTATION, P174, DOI 10.1145/1806596.1806617
   Xu GQ, 2009, PLDI'09 PROCEEDINGS OF THE 2009 ACM SIGPLAN CONFERENCE ON PROGRAMMING LANGUAGE DESIGN AND IMPLEMENTATION, P419, DOI 10.1145/1542476.1542523
   Xu W, 2009, SOSP'09: PROCEEDINGS OF THE TWENTY-SECOND ACM SIGOPS SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P117
   Yu T., 2016, P 25 INT S SOFTW TES, P389
   Yu X, 2014, ACM SIGPLAN NOTICES, V49, P193, DOI 10.1145/2541940.2541968
   Zalewski Piotr, IBM THREAD MONITOR D
   Zaparanuks D, 2012, ACM SIGPLAN NOTICES, V47, P67, DOI 10.1145/2345156.2254074
   Zhang Xiangyu., 2006, SIGSOFT 06FSE 14, P81, DOI DOI 10.1145/1181775.1181786
   Zhao X, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P603
   zookeeper.apache, AP ZOOKEEPER PROJ
NR 64
TC 1
Z9 1
U1 2
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2023
VL 19
IS 3
AR 23
DI 10.1145/3580281
PG 33
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P9QZ4
UT WOS:001053965500003
DA 2024-07-18
ER

PT J
AU Jia, YC
   Shao, ZL
   Chen, F
AF Jia, Yichen
   Shao, Zili
   Chen, Feng
TI SlimCache: An Efficient Data Compression Scheme for Flash-based
   Key-value Caching
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Flash memory; SSD; key-value caching; data compression
AB Flash-based key-value caching is becoming popular in data centers for providing high-speed key-value services. These systems adopt slab-based space management on flash and provide a low-cost solution for key-value caching. However, optimizing cache efficiency for flash-based key-value cache systems is highly challenging, due to the huge number of key-value items and the unique technical constraints of flash devices. In this article, we present a dynamic on-line compression scheme, called SlimCache, to improve the cache hit ratio by virtually expanding the usable cache space through data compression. We have investigated the effect of compression granularity to achieve a balance between compression ratio and speed, and we leveraged the unique workload characteristics in key-value systems to efficiently identify and separate hot and cold data. To dynamically adapt to workload changes during runtime, we have designed an adaptive hot/cold area partitioning method based on a cost model. To avoid unnecessary compression, SlimCache also estimates data compressibility to determine whether the data are suitable for compression or not. We have implemented a prototype based on Twitter's Fatcache. Our experimental results show that SlimCache can accommodate more key-value items in flash by up to 223.4%, effectively increasing throughput and reducing average latency by up to 380.1% and 80.7%, respectively.
C1 [Jia, Yichen; Chen, Feng] Louisiana State Univ, Dept Comp Sci & Engn, Baton Rouge, LA 70803 USA.
   [Shao, Zili] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.
C3 Louisiana State University System; Louisiana State University; Chinese
   University of Hong Kong
RP Jia, YC (corresponding author), Louisiana State Univ, Dept Comp Sci & Engn, Baton Rouge, LA 70803 USA.
EM yjia@csc.lsu.edu; shao@cse.cuhk.edu.hk; fchen@csc.lsu.edu
RI Jia, Yichen/GRX-1463-2022
OI Jia, Yichen/0000-0001-8346-1651
FU U.S. National Science Foundation [CCF-1453705, CCF-1629291,
   CCF-1910958]; Louisiana Board of Regents [LEQSF(2014-17)-RD-A-01];
   Research Grants Council of the Hong Kong Special Administrative Region,
   China [GRF 15273616, GRF 15206617, GRF 15224918]; Chinese University of
   Hong Kong [4055096]
FX The work described in this article was partially supported by the U.S.
   National Science Foundation under Grants No. CCF-1453705, No.
   CCF-1629291, and No. CCF-1910958, the Louisiana Board of Regents under
   Grant No. LEQSF(2014-17)-RD-A-01, the Research Grants Council of the
   Hong Kong Special Administrative Region, China under Grants No. GRF
   15273616, No. GRF 15206617, and No. GRF 15224918, and Direct Grant for
   Research, The Chinese University of Hong Kong (Project No. 4055096).
CR ABADI DJ, 2006, P ACM SIGMOD INT C M
   Abali B, 2001, IEEE T COMPUT, V50, P1219, DOI 10.1109/12.966496
   Agrawal Nitin, 2008, 2008 USENIX ANN TECH
   An ZQ, 2017, IEEE INT C CL COMP, P582, DOI 10.1109/CLUSTER.2017.69
   [Anonymous], 1999, P 18 ANN JOINT C IEE
   [Anonymous], 2017, P 15 USENIX C FIL ST
   [Anonymous], 2005, P USENIX ANN TECHN C
   [Anonymous], 2008, MIR 08
   Atikoglu Berk, 2012, P ACM INT C MEAS MOD
   Beltran Vicenc, 2008, P 14 IEEE INT C PAR
   Blott Michaela, 2013, 5 USENIX WORKSHOP HO
   Carra Damiano, 2014, P IEEE INT C COMM IC
   Chan HHW, 2018, PROCEEDINGS OF THE 2018 USENIX ANNUAL TECHNICAL CONFERENCE, P1007
   Chen Feng, 2009, P ACM SIGMETRICS C M
   Chen Feng, 2011, P 17 INT S HIGH PERF
   Choi S, 2017, 2017 4TH INTERNATIONAL CONFERENCE ON INDUSTRIAL ENGINEERING AND APPLICATIONS (ICIEA), P10, DOI 10.1109/IEA.2017.7939169
   Cidon A, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P321
   Cooper B.F., 2010, P ACM S CLOUD COMPUT, DOI DOI 10.1145/1807128.1807152
   CORMACK GV, 1985, COMMUN ACM, V28, P1336, DOI 10.1145/214956.214963
   de Castro Rodrigo S., 2003, P 15 S COMP ARCH HIG
   Debnath Biplob, 2011, P ACM SIGMOD INT C M
   Eisenman A, 2019, PROCEEDINGS OF THE 16TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P65
   Eisenman A, 2018, EUROSYS '18: PROCEEDINGS OF THE THIRTEENTH EUROSYS CONFERENCE, DOI 10.1145/3190508.3190524
   Eisenman Assaf, 2017, FLASHIELD KEY VALUE
   Foong A, 2016, IEEE INT MEM WORKSH
   Fu KW, 2013, IEEE INTERNET COMPUT, V17, P42, DOI 10.1109/MIC.2013.28
   Fu Kingwa, 2017, WEIBOSCOPE OPEN DATA
   GNU, 2018, GZIP
   Harnik D, 2013, P 11 USENIX C FIL ST
   Huang S, 2016, ACM T STORAGE, V12, DOI 10.1145/2737832
   Huang YH, 2018, PROCEEDINGS OF THE 2018 USENIX ANNUAL TECHNICAL CONFERENCE, P967
   IBM, 2015, IBM REAL TIM COMPR I
   Intel, 2012, OPT SSD
   Intel, 2018, INT SSD
   Intel, 2020, INT OPT MEM
   Jia YC, 2018, I S MOD ANAL SIM COM, P209, DOI 10.1109/MASCOTS.2018.00029
   Jiang J, 2015, ANIM NUTR, V1, P373, DOI 10.1016/j.aninu.2015.12.006
   Jiang Song, 2005, P USENIX ANN TECHN C
   Jin X, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P121, DOI 10.1145/3132747.3132764
   Kakkad SV, 1999, PROCEEDINGS OF THE FIFTH USENIX CONFERENCE ON OBJECT-ORIENTED TECHNOLOGIES AND SYSTEMS (COOTS '99), P99
   Kannan S, 2018, PROCEEDINGS OF THE 2018 USENIX ANNUAL TECHNICAL CONFERENCE, P993
   Kourtis K, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P1
   Kulkarni Sanjeev R., 2002, LECT NOTES ELE201 IN
   Lang Harald, 2016, P INT C MAN DAT SIGM
   Leemis Larry, 2019, ZIPF
   Li BJ, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P137, DOI 10.1145/3132747.3132756
   Li Sheng, 2015, P 42 ACM IEEE ANN IN
   Lim Hyeontaek, 2014, P USENIX NSDI
   Lim Hyeontaek, 2011, P 23 S OP SYST PRINC
   Lim K., 2013, P 40 ANN INT S COMP
   Lu Lanyue, 2016, P 14 USENIX C FIL SO
   lz4, 2019, EXTR FAST COMPR
   Makatos Thanos, 2010, P 5 EUR C COMP SYST
   Mao Y., 2012, EUROSYS
   Marmol L., 2015, P USENIX ANN TECH C, P207
   Marripudi Venkata Lakshmi., 2015, Global Journal of Advanced Engineering Technologies, V4, P271
   Megiddo N., 2003, P 2 USENIX C FIL STO
   Memcached, 2018, DISTR MEM OBJ CACH S
   MongoDB, 2019, WIR STOR ENG
   Muller I., 2014, P 17 INT C EXT DAT T
   Nakar Dorom, 2004, P 23D IEEE CONV EL E
   Nishtala Rajesh, 2013, P 10 USENIX S NETW S
   Ouyang Xiangyong, 2012, P 41 INT C PAR PROC
   Papagiannis A, 2016, PROCEEDINGS OF USENIX ATC '16: 2016 USENIX ANNUAL TECHNICAL CONFERENCE, P537
   Poess Meikel, 2003, P 29 INT C VER LARG
   Python Software Foundation, 2019, TEXT GEN BAS MARK CH
   Reddit, 2015, REDD COMM
   Roelofs G., 2017, ZLIB
   Samsung, 2017, ULTR LAT SAMS Z NAND
   Samsung, 2018, SAM Z SSD SZ985 ULTR
   Seo BK, 2015, IEEE COMPUT ARCHIT L, V14, P123, DOI 10.1109/LCA.2014.2350984
   Serpanos DN, 1998, P SOC PHOTO-OPT INS, V3527, P320, DOI 10.1117/12.325826
   Shankar D, 2015, PROCEEDINGS 2015 IEEE INTERNATIONAL CONFERENCE ON BIG DATA, P539, DOI 10.1109/BigData.2015.7363797
   SHANNON CE, 1948, BELL SYST TECH J, V27, P379, DOI 10.1002/j.1538-7305.1948.tb01338.x
   Trevisan Luca, 2012, KOLMOGOROV COMPLEXIT
   Wang KF, 2018, PROCEEDINGS OF THE 2018 ACM SYMPOSIUM ON CLOUD COMPUTING (SOCC '18), P464, DOI 10.1145/3267809.3267847
   Wu CG, 2018, PROC INT CONF DATA, P401, DOI 10.1109/ICDE.2018.00044
   Wu XN, 2015, METHODS MOL BIOL, V1306, P71, DOI 10.1007/978-1-4939-2648-0_5
   Xu Shuotao, 2016, Proceedings of the VLDB Endowment, V10, P4
   Xu YH, 2016, INT J INNOV TECHNOL, V13, DOI 10.1142/S0219877016400162
   Yu ZW, 2016, ACM T KNOWL DISCOV D, V11, DOI 10.1145/2930671
   Zhang Heng, 2016, P 14 USENIX C FIL ST
   Zhou Pin, 2004, P 11 INT C ARCH SUPP
   Zipf GeorgeKingsley., 1929, REPRINTED HARVARD ST, VXL
NR 85
TC 2
Z9 3
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUN
PY 2020
VL 16
IS 2
AR 14
DI 10.1145/3383124
PG 34
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1RC
UT WOS:000583743900007
DA 2024-07-18
ER

PT J
AU Zuo, PF
   Hua, Y
   Wu, J
AF Zuo, Pengfei
   Hua, Yu
   Wu, Jie
TI Level Hashing: A High-performance and Flexible-resizing Persistent
   Hashing Index Structure
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Persistent memory; hashing index structure; write optimization; crash
   consistency
ID MEMORY; TREES
AB Non-volatile memory (NVM) technologies as persistent memory are promising candidates to complement or replace DRAM for building future memory systems, due to having the advantages of high density, low power, and non-volatility. In main memory systems, hashing index structures are fundamental building blocks to provide fast query responses. However, hashing index structures originally designed for dynamic random access memory (DRAM) become inefficient for persistent memory due to new challenges including hardware limitations of NVM and the requirement of data consistency. To address these challenges, this article proposes level hashing, a write-optimized and high-performance hashing index scheme with low-overhead consistency guarantee and cost-efficient resizing. Level hashing provides a sharing-based two-level hash table, which achieves constant-scale worst-case time complexity for search, insertion, deletion, and update operations, and rarely incurs extra NVM writes. To guarantee the consistency with low overhead, level hashing leverages log-free consistency schemes for deletion, insertion, and resizing operations, and an opportunistic log-free scheme for update operation. To cost-efficiently resize this hash table, level hashing leverages an in-place resizing scheme that only needs to rehash 1/3 of buckets instead of the entire table to expand a hash table and rehash 2/3 of buckets to shrink a hash table, thus significantly improving the resizing performance and reducing the number of rehashed buckets. Extensive experimental results show that the level hashing speeds up insertions by 1.4x-3.0x, updates by 1.2x-2.1x, expanding by over 4.3x, and shrinking by over 1.4x while maintaining high search and deletion performance compared with start-of-the-art hashing schemes.
C1 [Zuo, Pengfei; Hua, Yu; Wu, Jie] Huazhong Univ Sci & Technol, Sch Comp, Wuhan Natl Lab Optoelect, Luoyu Rd 1037, Wuhan 430074, Hubei, Peoples R China.
C3 Huazhong University of Science & Technology
RP Hua, Y (corresponding author), Huazhong Univ Sci & Technol, Sch Comp, Wuhan Natl Lab Optoelect, Luoyu Rd 1037, Wuhan 430074, Hubei, Peoples R China.
EM pfzuo@hust.edu.cn; csyhua@hust.edu.cn; wujie@hust.edu.cn
FU National Natural Science Foundation of China (NSFC) [61772212, 461-476]
FX This work was supported by National Natural Science Foundation of China
   (NSFC) under Grant 61772212. The preliminary version appears in the
   Proceedings of the 13th USENIX Symposium on Operating Systems Design and
   Implementation (OSDI), 2018, pages: 461-476, as "Write-Optimized and
   High-Performance Hashing Index Scheme for Persistent Memory."
CR Akinaga H, 2010, P IEEE, V98, P2237, DOI 10.1109/JPROC.2010.2070830
   [Anonymous], 2017, P 22 INT C ARCHITECT
   [Anonymous], 1998, The art of computer programming: Sorting and searching
   [Anonymous], 2018, Redis
   [Anonymous], KNOWLEDGE DATA ENG I
   [Anonymous], INTR INT OPT TECHN B
   Apalkov D, 2013, ACM J EMERG TECH COM, V9, DOI 10.1145/2463585.2463589
   Atikoglu Berk, 2012, Performance Evaluation Review, V40, P53, DOI 10.1145/2318857.2254766
   Bendre M, 2015, PROC VLDB ENDOW, V8, P2001
   BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007
   Breslow AD, 2016, PROCEEDINGS OF USENIX ATC '16: 2016 USENIX ANNUAL TECHNICAL CONFERENCE, P281
   Byers J, 2003, LECT NOTES COMPUT SC, V2735, P80
   Chen SM, 2015, PROC VLDB ENDOW, V8, P786, DOI 10.14778/2752939.2752947
   Chen SM, 2011, MODELLING SIMULATION, P200
   Coburn J, 2011, ACM SIGPLAN NOTICES, V46, P105, DOI [10.1145/1961295.1950380, 10.1145/1961296.1950380]
   COMER D, 1979, COMPUT SURV, V11, P121, DOI 10.1145/356770.356776
   Condit J, 2009, SOSP'09: PROCEEDINGS OF THE TWENTY-SECOND ACM SIGOPS SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P133
   Cooper Brian F., 2010, P 1 ACM S CLOUD COMP, P143, DOI [DOI 10.1145/1807128.1807152, 10.1145/1807128.1807152]
   Debnath Biplob, 2015, ACM SIGOPS Operating Systems Review, V49, P18, DOI 10.1145/2883591.2883597
   DeWitt D. J., 1984, SIGMOD Record, V14, P1, DOI 10.1145/971697.602261
   Dulloor Subramanya R., 2014, P EUR C COMP SYST EU, P1, DOI [10.1145/2592798.2592814, DOI 10.1145/2592798.2592814]
   Fan Bin, 2013, 10 USENIX S NETW SYS, P371
   GAO H, 2004, P 18 INT PAR DISTR P
   Herlihy M, 2008, LECT NOTES COMPUT SC, V5218, P350, DOI 10.1007/978-3-540-87779-0_24
   Hwang D, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P187
   Intel, 2018, Intel architecture instruction set extensions programming reference
   Kim WH, 2016, ACM SIGPLAN NOTICES, V51, P385, DOI 10.1145/2954679.2872392
   Kocberber Onur, 2013, 2013 46th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO). Proceedings, P468, DOI 10.1145/2540708.2540748
   Lee SK, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P257
   Li S, 2015, 2015 ACM/IEEE 42ND ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA), P476, DOI 10.1145/2749469.2750416
   Li XH, 2014, PROCEEDINGS OF THE THIRD NORTHEAST ASIA INTERNATIONAL SYMPOSIUM ON LANGUAGE, LITERATURE AND TRANSLATION, VOLS 1 AND 2, P1
   Lim H, 2017, SIGMOD'17: PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P21, DOI 10.1145/3035918.3064015
   Liu YJ, 2014, PROCEEDINGS OF THE 2014 ACM SYMPOSIUM ON PRINCIPLES OF DISTRIBUTED COMPUTING (PODC'14), P242, DOI 10.1145/2611462.2611495
   Liu ZY, 2017, PROCEEDINGS OF THE 29TH ACM SYMPOSIUM ON PARALLELISM IN ALGORITHMS AND ARCHITECTURES (SPAA'17), P235, DOI 10.1145/3087556.3087582
   Mueller W, 2005, IEEE INT EL DEV M IE
   Narayanan Dushyanth, 2012, P 17 INT C ARCH SUPP, P401, DOI DOI 10.1145/2150976.2151018
   Oukid I, 2016, SIGMOD'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P371, DOI 10.1145/2882903.2915251
   PAGH R, 2001, P ESA, P00001
   PIGGIN N, 2008, DDDS DYNAMIC DYNAMIC
   PITTEL B, 1987, J ALGORITHM, V8, P236, DOI 10.1016/0196-6774(87)90040-X
   Qureshi MK, 2009, CONF PROC INT SYMP C, P24, DOI 10.1145/1555815.1555760
   Rumble S.M., 2014, Proceedings of the 12th USENIX Conference on File and Storage Technologies (FAST 14), P1
   Seo J, 2017, IEEE ENG MED BIO, P917, DOI 10.1109/EMBC.2017.8036974
   Shalev O, 2006, J ACM, V53, P379, DOI 10.1145/1147954.1147958
   Shun JL, 2014, PROCEEDINGS OF THE 26TH ACM SYMPOSIUM ON PARALLELISM IN ALGORITHMS AND ARCHITECTURES (SPAA'14), P96, DOI 10.1145/2612669.2612687
   Sun YY, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P553
   Thoziyoor S, 2008, CONF PROC INT SYMP C, P51, DOI 10.1109/ISCA.2008.16
   Venkataraman Shivaram, 2011, P 9 USENIX C FIL STO, P5
   Volos H, 2015, Proceedings of the 16th Annual Middleware Conference, P37, DOI 10.1145/2814576.2814806
   Volos H, 2011, ACM SIGPLAN NOTICES, V46, P91, DOI [10.1145/1961296.1950379, 10.1145/1961295.1950379]
   Wong HSP, 2010, P IEEE, V98, P2201, DOI 10.1109/JPROC.2010.2070050
   Xia F, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P349
   Xu J, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P323
   Yang Jun, 2015, P 13 USENIX C FIL ST, P167
   Yu XY, 2014, PROC VLDB ENDOW, V8, P209, DOI 10.14778/2735508.2735511
   Zhou P, 2009, CONF PROC INT SYMP C, P14, DOI 10.1145/1555815.1555759
   Zuo PF, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P461
   Zuo PF, 2018, IEEE T PARALL DISTR, V29, P985, DOI 10.1109/TPDS.2017.2782251
   Zuo Pengfei., 2017, Proceedings of the 33st Symposium on Mass Storage Systems and Technologies, MSST, V17, P1
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   2018, JAVA HASHMAP
   2018, LIBCUCKOO HIGH PERFO
   [No title captured]
NR 67
TC 7
Z9 10
U1 1
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUN
PY 2019
VL 15
IS 2
AR 13
DI 10.1145/3322096
PG 30
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JL3IK
UT WOS:000495424300006
DA 2024-07-18
ER

PT J
AU Shin, JY
   Balakrishnan, M
   Marian, T
   Weatherspoon, H
AF Shin, Ji-Yong
   Balakrishnan, Mahesh
   Marian, Tudor
   Weatherspoon, Hakim
TI Isotope: ACID Transactions for Block Storage
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article; Proceedings Paper
CT 14th USENIX Conference on File and Storage Technologies (FAST)
CY FEB 22-25, 2016
CL Santa Clara, CA
SP NetApp, Google, Veritas, VMWare, Facebook, Microsoft Res, EMC2, Huawei, Redhat, Tintri, Hewlett Packard Enterprise, IBM Res, Radian, SNIA, USENIX, ACM Queue, ADMIN Magazine, CRC Press, Linux Pro Magazine, LXer, No Starch Press, OReilly Media, Raspberry Pi Geek, UserFriendly Org, NSF
DE Design; Experimentation; Management; Performance; Transaction; block
   storage; isolation
AB Existing storage stacks are top heavy and expect little from block storage. As a result, new high-level storage abstractions-and new designs for existing abstractions-are difficult to realize, requiring developers to implement from scratch complex functionality such as failure atomicity and fine-grained concurrency control. In this article, we argue that pushing transactional isolation into the block store (in addition to atomicity and durability) is both viable and broadly useful, resulting in simpler high-level storage systems that provide strong semantics without sacrificing performance. We present Isotope, a new block store that supports ACID transactions over block reads and writes. Internally, Isotope uses a new multiversion concurrency control protocol that exploits fine-grained, subblock parallelism in workloads and offers both strict serializability and snapshot isolation guarantees. We implemented several high-level storage systems over Isotope, including two key-value stores that implement the LevelDB API over a hash table and B-tree, respectively, and a POSIX file system. We show that Isotope's block-level transactions enable systems that are simple (100s of lines of code), robust (i.e., providing ACID guarantees), and fast (e.g., 415MB/s for random file writes). We also show that these systems can be composed using Isotope, providing applications with transactions across different high-level constructs such as files, directories, and key-value pairs.
C1 [Shin, Ji-Yong] Cornell Univ, Ithaca, NY 14853 USA.
   [Shin, Ji-Yong; Balakrishnan, Mahesh] Yale Univ, Dept Comp Sci, New Haven, CT 06511 USA.
   [Marian, Tudor] Google Inc, 1600 Amphitheatre Pkwy, Mountain View, CA 94043 USA.
   [Weatherspoon, Hakim] Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA.
C3 Cornell University; Yale University; Google Incorporated; Cornell
   University
RP Shin, JY (corresponding author), Cornell Univ, Ithaca, NY 14853 USA.; Shin, JY (corresponding author), Yale Univ, Dept Comp Sci, New Haven, CT 06511 USA.
EM jyshin@cs.yale.edu; mahesh@cs.yale.edu; tudorm@google.com;
   hweather@cs.cornell.edu
OI Shin, Ji Yong/0000-0002-1595-4849
FU DARPA MRC [FA8750-11-2-0256]; DARPA, CSSG [D11AP00266]; NSF [0424422,
   1047540, 1053757, 1151268, 1422544]; NIST [60NANB15D327]; Cisco; Intel;
   Direct For Computer & Info Scie & Enginr; Division Of Computer and
   Network Systems [1422544] Funding Source: National Science Foundation;
   Direct For Computer & Info Scie & Enginr; Division Of Computer and
   Network Systems [1151268] Funding Source: National Science Foundation;
   Direct For Computer & Info Scie & Enginr; Division of Computing and
   Communication Foundations [1047540] Funding Source: National Science
   Foundation
FX This work is partially funded and supported by a SLOAN Research
   Fellowship received by Hakim Weatherspoon, a Facebook Faculty Award
   received by Mahesh Balakrishnan, DARPA MRC (FA8750-11-2-0256) and CSSG
   (D11AP00266), NSF (0424422, 1047540, 1053757, 1151268, 1422544), NIST
   (60NANB15D327), Cisco, and Intel.
CR Aghayev Abutalib., 2015, P 13 USENIX C FILE S, P135
   Aguilera Marcos K., 2007, Operating Systems Review, V41, P159, DOI 10.1145/1323293.1294278
   Amiri K., 2000, Proceedings 20th IEEE International Conference on Distributed Computing Systems, P298, DOI 10.1109/ICDCS.2000.840942
   [Anonymous], 2008, P 8 USENIX C OP SYST
   [Anonymous], 1992, HPLCSP929
   [Anonymous], 1987, Concurrency Control and Recovery in Database Systems
   Badam A., 2011, Proceedings of the 8th USENIX Conference on Networked Systems Design and Implemen- tation, NSDI'11, P211
   Balakrishnan Mahesh, 2012, P 9 S NETW SYST DES
   Berenson H., 1995, SIGMOD Record, V24, P1, DOI 10.1145/568271.223785
   Bilas A., 2004, MSST, P315
   Coburn J, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P197, DOI 10.1145/2517349.2522724
   Coburn J, 2011, ACM SIGPLAN NOTICES, V46, P105, DOI [10.1145/1961295.1950380, 10.1145/1961296.1950380]
   Cooper Brian F., 2010, P 1 ACM S CLOUD COMP, P143, DOI [DOI 10.1145/1807128.1807152, 10.1145/1807128.1807152]
   Cornell B, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE FREENIX TRACK 2004 USENIX ANNUAL TECHNICAL CONFERENCE, P19
   Cully B., 2014, P 12 USENIX C FILE S, P17
   De Jonge W., 1993, Operating Systems Review, V27, P15, DOI 10.1145/173668.168621
   DeWitt D. J., 1984, SIGMOD Record, V14, P1, DOI 10.1145/971697.602261
   Driscoll James R., 1986, P 18 ANN ACM S THEOR, P109
   English Robert M., 1992, P USENIX WINT 92 TEC, P237
   Fan Bin, 2013, 10 USENIX S NETW SYS, P371
   Ganger GregoryR., 2001, Blurring the line between OSes and storage devices
   Guerraoui R, 2008, PPOPP'08: PROCEEDINGS OF THE 2008 ACM SIGPLAN SYMPOSIUM ON PRINCIPLES AND PRACTICE OF PARALLEL PROGRAMMING, P175, DOI 10.1145/1345206.1345233
   Harris T., 2010, T MEMORY
   HITZ D, 1994, PROCEEDINGS OF THE WINTER 1994 USENIX CONFERENCE, P235
   IOzone, 2016, IOZONE FIL BENCHM
   Jose Jithin, 2013, P WORKSH INT NVM FLA
   KUNG HT, 1981, ACM T DATABASE SYST, V6, P213, DOI 10.1145/319566.319567
   Lowell D. E., 1997, Operating Systems Review, V31, P92, DOI 10.1145/269005.266665
   MacCormick J, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE SIXTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDE '04), P105
   Mesnier M, 2003, IEEE COMMUN MAG, V41, P84, DOI 10.1109/MCOM.2003.1222722
   Meyer DT, 2008, EUROSYS'08: PROCEEDINGS OF THE EUROSYS 2008 CONFERENCE, P41, DOI 10.1145/1357010.1352598
   Microsoft, 2016, STOR SPAC
   Mirosoft, 2016, WINFS
   MOHAN C, 1992, ACM T DATABASE SYST, V17, P94, DOI 10.1145/128765.128770
   Muniswamy-Reddy KK, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P115
   Nightingale EdmundB., 2012, P 10 USENIX C OPERAT, P1
   OLSON MA, 1993, PROCEEDINGS OF THE WINTER 1993 USENIX CONFERENCE, P205
   Pennarun Avery, 2016, EVERYTHING YOU NEVER
   Porter DE, 2009, SOSP'09: PROCEEDINGS OF THE TWENTY-SECOND ACM SIGOPS SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P161
   Quinlan S, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE FAST'02 CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P89
   Reid Colin, 2011, PROC VLDB ENDOW, V4, P11
   SALTZER JH, 1984, ACM T COMPUT SYST, V2, P277, DOI 10.1145/357401.357402
   SanDisk, 2015, SANDISK FUS AT MULT
   SanDisk, 2015, SANDISK FUS AUT COMM
   Santry DS, 1999, OPERATING SYSTEMS REVIEW, VOL 33, NO 5, DECEMBER 1999, P110, DOI 10.1145/319344.319159
   SATYANARYANAN M, 1994, ACM T COMPUT SYST, V12, P33, DOI 10.1145/174613.174615
   Saxena M., 2012, P 7 ACM EUR C COMP S, P267, DOI DOI 10.1145/2168836.2168863
   Saxena Mohit., 2012, DaMoN, P33
   Seagate, 2016, SEAG KIN OP STOR PLA
   Sears R, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P29
   Shavit N, 1997, DISTRIB COMPUT, V10, P99, DOI 10.1007/s004460050028
   Shin J.-Y., 2013, USENIX FAST, P213
   Sivathanu M, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE 2ND USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST'03), P73
   Skourtis Dimitris, 2014, 2014 USENIX ANN TECH, P463
   Soundararajan Gokul., 2010, Proceedings of the 8th USENIX conference on File and storage technologies, FAST'10, P8
   Sovran Y, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P385
   Stein Lex, 2005, WORKSH HOT TOP OP SY
   Thomson A., 2015, 13th USENIX Confer- ence on File and Storage Technologies (FAST 15), P1, DOI [DOI 10.1002/14356007.A10_173.PUB2, 10.1002/14356007.a10_173.pub2]
   Volos H, 2011, ACM SIGPLAN NOTICES, V46, P91, DOI [10.1145/1961296.1950379, 10.1145/1961295.1950379]
   Wang R. Y., 1998, Operating Systems Review, P29
   Wilkes J, 1996, ACM T COMPUT SYST, V14, P108, DOI 10.1145/225535.225539
   Wright CharlesP., 2007, ACM T STORAGE, V3, P4
   Zhang Yu-liang, 2012, Journal of Insect Science (Tucson), V12, P1
NR 63
TC 4
Z9 4
U1 2
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAR
PY 2017
VL 13
IS 1
SI SI
AR 4
DI 10.1145/3032967
PG 25
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA ER3WE
UT WOS:000398729600004
OA Bronze
DA 2024-07-18
ER

PT J
AU Stefanovici, I
   Schroeder, B
   O'Shea, G
   Thereska, E
AF Stefanovici, Ioan
   Schroeder, Bianca
   O'Shea, Greg
   Thereska, Eno
TI Treating the Storage Stack Like a Network
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article; Proceedings Paper
CT 14th USENIX Conference on File and Storage Technologies (FAST)
CY FEB 22-25, 2016
CL Santa Clara, CA
SP NetApp, Google, Veritas, VMWare, Facebook, Microsoft Res, EMC2, Huawei, Redhat, Tintri, Hewlett Packard Enterprise, IBM Res, Radian, SNIA, USENIX, ACM Queue, ADMIN Magazine, CRC Press, Linux Pro Magazine, LXer, No Starch Press, OReilly Media, Raspberry Pi Geek, UserFriendly Org, NSF
DE Design; Management; Performance; Data centers; storage; software-defined
   storage; SDS; routing; storage stack
AB In a data center, an IO from an application to distributed storage traverses not only the network but also several software stages with diverse functionality. This set of ordered stages is known as the storage or IO stack. Stages include caches, hypervisors, IO schedulers, file systems, and device drivers. Indeed, in a typical data center, the number of these stages is often larger than the number of network hops to the destination. Yet, while packet routing is fundamental to networks, no notion of IO routing exists on the storage stack. The path of an IO to an endpoint is predetermined and hard coded. This forces IO with different needs (e.g., requiring different caching or replica selection) to flow through a one-size-fits-all IO stack structure, resulting in an ossified IO stack.
   This article proposes sRoute, an architecture that provides a routing abstraction for the storage stack. sRoute comprises a centralized control plane and "sSwitches" on the data plane. The control plane sets the forwarding rules in each sSwitch to route IO requests at runtime based on application-specific policies. A key strength of our architecture is that it works with unmodified applications and Virtual Machines (VMs). This article shows significant benefits of customized IO routing to data center tenants: for example, a factor of 10 for tail IO latency, more than 60% better throughput for a customized replication protocol, a factor of 2 in throughput for customized caching, and enabling live performance debugging in a running system.
C1 [Stefanovici, Ioan; O'Shea, Greg] Microsoft Res, 21 Stn Rd, Cambridge CB1 2FB, England.
   [Schroeder, Bianca] Univ Toronto, Dept Comp Sci, 10 Kings Coll Rd,Rm 3302, Toronto, ON M5S 3G4, Canada.
   [Thereska, Eno] Confluent Inc, 101 Univ Ave,Suite 111, Palo Alto, CA 94301 USA.
   [Thereska, Eno] Imperial Coll London, London, England.
C3 Microsoft; University of Toronto; Imperial College London
RP Stefanovici, I (corresponding author), Microsoft Res, 21 Stn Rd, Cambridge CB1 2FB, England.
EM t-iostef@microsoft.com; bianca@cs.toronto.edu; gregos@microsoft.com;
   eno.thereska@gmail.com
CR Abd-El-Malek Michael., 2005, FAST 05, P5
   Angel S., 2014, P USENIX S OP SYST D
   [Anonymous], OR SOL ZFS ADM GUID
   [Anonymous], 2007, NSDI
   [Anonymous], P 1 WORKSH HOT TOP S
   Arpaci-Dusseau A. C., 2006, Performance Evaluation Review, V33, P29, DOI 10.1145/1138085.1138093
   Arpaci-Dusseau A. C., 2003, Operating Systems Review, V37, P90, DOI 10.1145/1165389.945455
   Arpaci-Dusseau A. C., 2001, Operating Systems Review, V35, P43, DOI 10.1145/502059.502040
   Baker J., 2011, Megastore: Providing scalable
   Ballani H, 2015, ACM SIGCOMM COMP COM, V45, P493, DOI 10.1145/2829988.2787493
   Barham P., 2004, P 6 C S OSDI BERK CA, V6, P18
   Bershad B. N., 1995, Operating Systems Review, V29, P267, DOI 10.1145/224057.224077
   Calder B, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P143
   Cao P, 1996, ACM T COMPUT SYST, V14, P311, DOI 10.1145/235543.235544
   Casado M, 2007, ACM SIGCOMM COMP COM, V37, P1, DOI 10.1145/1282427.1282382
   Chang F., 2006, P USENIX OSDI
   Cooper BF, 2008, PROC VLDB ENDOW, V1, P1277
   CORBETT J.C., 2012, P USENIX OSDI
   Cully B., 2014, P 12 USENIX C FILE S, P17
   Dean J, 2013, COMMUN ACM, V56, P74, DOI 10.1145/2408776.2408794
   DeCandia Giuseppe, 2007, Operating Systems Review, V41, P205, DOI 10.1145/1323293.1294281
   Engler D. R., 1995, Operating Systems Review, V29, P251, DOI 10.1145/224057.224076
   Ferguson AD, 2013, ACM SIGCOMM COMP COM, V43, P327, DOI 10.1145/2534169.2486003
   FreeBSD, 2014, FREEBSD GEOM STOR FR
   Greenberg Albert, 2015, SDN CLOUD SIGCOMM 20
   Greenberg Albert., 2009, P ACM SIGCOMM
   HARTY K, 1992, SIGPLAN NOTICES, V27, P187, DOI 10.1145/143371.143511
   Hitz D., 1994, Proceedings of the USENIX Winter 1994 Technical Conference, P19
   Huang Q, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P167, DOI 10.1145/2517349.2522722
   Intel Corporation, 2014, IOMETER BENCHM
   Isard M., 2007, Operating Systems Review, V41, P60, DOI 10.1145/1243418.1243426
   Jain S, 2013, ACM SIGCOMM COMP COM, V43, P3, DOI 10.1145/2534169.2486019
   Kaashoek M. F., 1997, Operating Systems Review, V31, P52, DOI 10.1145/269005.266644
   Kazemian P., 2013, NSDI
   Koponen T., 2010, P 9 USENIX C OP SYST, P351
   KRUEGER K, 1993, SIGPLAN NOTICES, V28, P48, DOI 10.1145/167962.165867
   Lakshman Avinash, 2010, Operating Systems Review, V44, P35, DOI 10.1145/1773912.1773922
   Lamport L, 1998, ACM T COMPUT SYST, V16, P133, DOI 10.1145/279227.279229
   Li Cheng, 2012, P USENIX OSDI OSDI 1
   Love Robert, 2010, Linux Kernel Development
   McKeown N, 2008, ACM SIGCOMM COMP COM, V38, P69, DOI 10.1145/1355734.1355746
   Mesnier M, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P57
   Microsoft, 2010, VIRT HARD DISK PERF
   Microsoft Corporation, 2014, MIN MIN DRIV DRIV PA
   Microsoft Corporation, 2014, FIL SYST MIN DRIV MS
   Microsoft Research, 2014, MICR RES STOR TOOLK
   Narayanan D., 2008, ACM T STORAGE, V4, P3
   Narayanan D., 2008, P 8 USENIX C OPERATI, P15
   Netronome, 2016, AG SERV NETW PLATF
   Peter Simon, 2014, P 11 USENIX C OP SYS
   Putnam A, 2014, CONF PROC INT SYMP C, P13, DOI 10.1109/ISCA.2014.6853195
   Qazi ZA, 2013, ACM SIGCOMM COMP COM, V43, P27, DOI 10.1145/2534169.2486022
   Rodeh O, 2013, ACM T STORAGE, V9, DOI 10.1145/2501620.2501623
   Seltzer MI, 1996, PROCEEDINGS OF THE SECOND SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '96), P213, DOI 10.1145/248155.238779
   Sherry Justine, 2012, P ACM SIGCOMM HELS F, P12
   Sigelman Benjamin H, 2010, Technical Report
   SNIA, 2007, EXCH SERV TRAC
   Stefanovici I, 2015, ACM SOCC'15: PROCEEDINGS OF THE SIXTH ACM SYMPOSIUM ON CLOUD COMPUTING, P174, DOI 10.1145/2806777.2806933
   Terry D. B., 1995, P ACM SOSP
   Terry Douglas B, 2013, P ACM SOSP
   Thereska E, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P182, DOI 10.1145/2517349.2522723
   Tolia Niraj., 2006, Proceedings of the 3rd conference on Networked Systems Design Implementation - Volume 3, NSDI'06, V3, P19
   Transaction Processing Performance Council, 2014, TPC BENCHM E REV 1 1
   Waldspurger C. A., 2015, FAST
   Wong TM, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE GENERAL TRACK, P161
   Yan Hong., 2007, NSDI
NR 66
TC 2
Z9 2
U1 1
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAR
PY 2017
VL 13
IS 1
SI SI
AR 2
DI 10.1145/3032968
PG 27
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA ER3WE
UT WOS:000398729600002
DA 2024-07-18
ER

PT J
AU Huang, S
   Wei, QS
   Feng, D
   Chen, JX
   Chen, C
AF Huang, Sai
   Wei, Qingsong
   Feng, Dan
   Chen, Jianxi
   Chen, Cheng
TI Improving Flash-Based Disk Cache with Lazy Adaptive Replacement
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Algorithms; Design; Performance; Cache algorithm; endurance; flash
   memory; solid-state drive
AB For years, the increasing popularity of flash memory has been changing storage systems. Flash-based solid-state drives (SSDs) are widely used as a new cache tier on top of hard disk drives (HDDs) to speed up data-intensive applications. However, the endurance problem of flash memory remains a concern and is getting worse with the adoption of MLC and TLC flash. In this article, we propose a novel cache management algorithm for flash-based disk cache named Lazy Adaptive Replacement Cache (LARC). LARC adopts the idea of selective caching to filter out seldom accessed blocks and prevent them from entering cache. This avoids cache pollution and preserves popular blocks in cache for a longer period of time, leading to a higher hit rate. Meanwhile, by avoiding unnecessary cache replacements, LARC reduces the volume of data written to the SSD and yields an SSD-friendly access pattern. In this way, LARC improves the performance and endurance of the SSD at the same time. LARC is self-tuning and incurs little overhead. It has been extensively evaluated by both trace-driven simulations and synthetic benchmarks on a prototype implementation. Our experiments show that LARC outperforms state-of-art algorithms for different kinds of workloads and extends SSD lifetime by up to 15.7 times.
C1 [Huang, Sai; Feng, Dan; Chen, Jianxi] Huazhong Univ Sci & Technol, Wuhan Natl Lab Optoelect, Wuhan 430074, Peoples R China.
   [Huang, Sai; Feng, Dan; Chen, Jianxi] Huazhong Univ Sci & Technol, Sch Comp, Wuhan 430074, Peoples R China.
   [Wei, Qingsong; Chen, Cheng] ASTAR, Data Storage Inst, Wuhan, Peoples R China.
   [Huang, Sai; Feng, Dan; Chen, Jianxi] Huazhong Univ Sci & Technol, 1037 Luoyu Rd, Wuhan 430074, Peoples R China.
   [Wei, Qingsong; Chen, Cheng] Data Storage Inst, 2 Fusionopolis Way,08-01 Innovis, Singapore 138634, Singapore.
C3 Huazhong University of Science & Technology; Huazhong University of
   Science & Technology; Huazhong University of Science & Technology;
   Agency for Science Technology & Research (A*STAR); A*STAR - Data Storage
   Institute
RP Huang, S; Feng, D; Chen, JX (corresponding author), Huazhong Univ Sci & Technol, Wuhan Natl Lab Optoelect, Wuhan 430074, Peoples R China.; Huang, S; Feng, D; Chen, JX (corresponding author), Huazhong Univ Sci & Technol, Sch Comp, Wuhan 430074, Peoples R China.; Wei, QS; Chen, C (corresponding author), ASTAR, Data Storage Inst, Wuhan, Peoples R China.; Huang, S; Feng, D; Chen, JX (corresponding author), Huazhong Univ Sci & Technol, 1037 Luoyu Rd, Wuhan 430074, Peoples R China.; Wei, QS; Chen, C (corresponding author), Data Storage Inst, 2 Fusionopolis Way,08-01 Innovis, Singapore 138634, Singapore.
EM seth.hg@gmail.com; WEI_Qingsong@dsi.a-star.edu.sg; dfeng@hust.edu.cn;
   chenjx@hust.edu.cn; CHEN_Cheng@dsi.a-star.edu.sg
OI chen, cheng/0000-0002-2622-4075
FU National Basic Research Program of China [2011CB302301]; Agency for
   Science, Technology and Research (A* STAR), Singapore [112-172-0010];
   National High Technology Research and Development Program ("863"
   Program) of China [2013AA013203]; National Natural Science Foundation of
   China (NSFC) [61025008, 61232004, 61173043]; Natural Science Foundation
   of Hubei Province [2011CDB036]
FX This work was supported by the National Basic Research Program of China
   (973 Program) under grant 2011CB302301; the Agency for Science,
   Technology and Research (A* STAR), Singapore under grant 112-172-0010;
   the National High Technology Research and Development Program ("863"
   Program) of China under grant 2013AA013203; the National Natural Science
   Foundation of China (NSFC) under grants 61025008, 61232004, and
   61173043; and the Natural Science Foundation of Hubei Province under
   grant 2011CDB036.
CR Agrawal Nitin, 2008, P USENIX ANN TECHN C, P57
   [Anonymous], ACM SIGARCH COMPUTER
   [Anonymous], TARGET
   [Anonymous], 2007, Blktrace user guide
   Appuswamy R., 2013, IEEE 29 S MASS STOR, P1
   Badam Anirudh., 2011, NSDI, P16
   BELADY LA, 1966, IBM SYST J, V5, P78, DOI 10.1147/sj.52.0078
   Canim M, 2010, PROC VLDB ENDOW, V3, P1435, DOI 10.14778/1920841.1921017
   Chen F., 2011, Proc. of ACM Int. Conf. on Supercomputing, P22
   Cooper Brian F., 2010, P 1 ACM S CLOUD COMP, P143, DOI [DOI 10.1145/1807128.1807152, 10.1145/1807128.1807152]
   Debnath Biplob., 2010, P USENIX ANN TECHNIC, P16
   EMC, 2012, EMC INFR VMWARE CLOU
   Grupp L. M., 2012, FAST, V7, P2
   Guerra J., 2011, P 9ST USENIX C FAST, P20
   Holland David A., 2013, P 2013 USENIX ANN TE
   HP, 2014, AD OPT HP 3PAR STORE
   Huang P., 2014, P 2014 USENIX ANN TE, P489
   Huang YJ, 2013, IEEE SYMP ADAPT DYNA, P1, DOI 10.1109/ADPRL.2013.6614981
   IBM, 2013, IBM SYST STOR DS8000
   Johnson T., 1994, P 20 INT C VER LARG, P439
   Kavalanekar Swaroop, 2008, 2008 IEEE International Symposium on Workload Characterization (IISWC), P119, DOI 10.1109/IISWC.2008.4636097
   Kgil T, 2008, CONF PROC INT SYMP C, P327, DOI 10.1109/ISCA.2008.32
   Koller R, 2010, ACM T STORAGE, V6, DOI 10.1145/1837915.1837921
   Lakshman Avinash, 2010, Operating Systems Review, V44, P35, DOI 10.1145/1773912.1773922
   Leventhal A, 2008, COMMUN ACM, V51, P47, DOI 10.1145/1364782.1364796
   Megiddo N, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE 2ND USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST'03), P115
   Narayanan D, 2009, EUROSYS'09: PROCEEDINGS OF THE FOURTH EUROSYS CONFERENCE, P145
   Oh Y., 2012, USENIX FAST, P25
   Peters Mark., 2009, NETAPPS SOLID STATE
   Ren J, 2011, INT S HIGH PERF COMP, P278, DOI 10.1109/HPCA.2011.5749736
   ROBINSON JT, 1990, PERF E R SI, V18, P134, DOI 10.1145/98460.98523
   Samsung, 2013, SAMS SOL STAT DRIV T
   Saxena M., 2012, P 7 ACM EUR C COMP S, P267, DOI DOI 10.1145/2168836.2168863
   Saxena Mohit., 2010, P 2010 USENIX C USEN, P14
   Schroeder Bianca., 2006, NSDI 06, P18
   Seagate, 2014, DURACLASS TECHNOLOGY
   Seagate, 2012, DAT SHEET MOM XT SSH
   Smaragdakis Y, 1999, PERFORMANCE EVALUATION REVIEW, SPECIAL ISSUE, VOL 27 NO 1, JUNE 1999, P122, DOI 10.1145/301464.301486
   Song Jiang, 2002, Performance Evaluation Review, V30, P31, DOI 10.1145/511399.511340
   University of Massachusetts, 2007, UMASS TRAC REP
   Zhou YY, 2001, USENIX ASSOCIATION PROCEEDINGS OF THE 2001 USENIX ANNUAL TECHNICAL CONFERENCE, P91
NR 41
TC 52
Z9 57
U1 0
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD FEB
PY 2016
VL 12
IS 2
AR 8
DI 10.1145/2737832
PG 24
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DJ0PO
UT WOS:000373906600003
DA 2024-07-18
ER

PT J
AU Jung, M
   Choi, W
   Gao, SW
   Wilson, EH
   Donofrio, D
   Shalf, J
   Kandemir, MT
AF Jung, Myoungsoo
   Choi, Wonil
   Gao, Shuwen
   Wilson, Ellis Herbert, III
   Donofrio, David
   Shalf, John
   Kandemir, Mahmut Taylan
TI NANDFlashSim: High-Fidelity, Microarchitecture-Aware NAND Flash Memory
   Simulation
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Non-volatile memory; NAND flash memory; cycle-level simulation; solid
   state disk; performance evaluation
AB As the popularity of NAND flash expands in arenas from embedded systems to high-performance computing, a high-fidelity understanding of its specific properties becomes increasingly important. Further, with the increasing trend toward multiple-die, multiple-plane architectures and high-speed interfaces, flash memory systems are expected to continue to scale and cheapen, resulting in their broader proliferation. However, when designing NAND-based devices, making decisions about the optimal system configuration is nontrivial, because flash is sensitive to a number of parameters and suffers from inherent latency variations, and no available tools suffice for studying these nuances. The parameters include the architectures, such as multidie and multiplane, diverse node technologies, bit densities, and cell reliabilities. Therefore, we introduce NANDFlashSim, a high-fidelity, latency-variation-aware, and highly configurable NAND-flash simulator, which implements a detailed timing model for 16 state-of-the-art NAND operations. Using NANDFlashSim, we notably discover the following. First, regardless of the operation, reads fail to leverage internal parallelism. Second, MLC provides lower I/O bus contention than SLC, but contention becomes a serious problem as the number of dies increases. Third, many-die architectures outperform many-plane architectures for disk-friendly workloads. Finally, employing a high-performance I/O bus or an increased page size does not enhance energy savings. Our simulator is available at http://nfs.camelab.org.
C1 [Jung, Myoungsoo] Yonsei Univ, Sch Integrated Technol, 85 Songdogwahak Ro, Inchon 21983, South Korea.
   [Choi, Wonil; Kandemir, Mahmut Taylan] Penn State Univ, Sch Elect Engn & Comp Sci, 342 Informat Sci & Technol Bldg, University Pk, PA 16802 USA.
   [Gao, Shuwen] Intel, Visual & Parallel Comp Grp, 1900 Prairie City Rd, Folsom, CA USA.
   [Wilson, Ellis Herbert, III] Panasas, 1501 Reedsdale St,Suite 400 North, Pittsburgh, PA 15233 USA.
   [Donofrio, David; Shalf, John] Univ Calif Berkeley, Lawrence Berkeley Natl Lab, Computat Res Div, 1 Cyclotron Rd, Berkeley, CA 94720 USA.
C3 Yonsei University; Pennsylvania Commonwealth System of Higher Education
   (PCSHE); Pennsylvania State University; Pennsylvania State University -
   University Park; Intel Corporation; United States Department of Energy
   (DOE); Lawrence Berkeley National Laboratory; University of California
   System; University of California Berkeley
RP Jung, M (corresponding author), Yonsei Univ, Sch Integrated Technol, 85 Songdogwahak Ro, Inchon 21983, South Korea.
EM m.jung@yonsei.ac.kr; wuc138@cse.psu.edu; shuwen.gao@intel.com;
   ellis@ellisv3.com; ddonofrio@lbl.gov; jshalf@lbl.gov;
   kandemir@cse.psu.edu
RI Jung, Myoungsoo/F-4565-2019
OI Shalf, John/0000-0002-0608-3690
FU MSIP [IITP-2015-R0346-15-1008, NRF-2015M3C4A 7065645]; DOE grant
   [DE-AC02-05CH1123]; NSF [1213052, 1205618, 1302557, 1526750, 1409095,
   1439021]; Direct For Computer & Info Scie & Enginr; Division of
   Computing and Communication Foundations [1439021] Funding Source:
   National Science Foundation
FX This research was supported in part by MSIP grants
   IITP-2015-R0346-15-1008 and NRF-2015M3C4A 7065645, DOE grant
   DE-AC02-05CH1123, and NSF grants 1213052, 1205618, 1302557, 1526750,
   1409095, and 1439021. Myoungsoo Jung is the corresponding author.
CR AGRAWAL N., 2008, P USENIX ATC
   Bucy J, 2008, PARALLEL DATA LAB
   Chang L.-P., 2004, ACM Trans. on Embedded Computing Syst, V3, P837
   Fisher Ryan, 2008, FLASHMEMORY SUMMIT
   Grupp L. M., 2009, P MICRO
   Grupp Laura M., 2013, P USENIX 2013 ANN TE
   HU Y., 2011, P ICS
   Hynix Inc, 2009, NAND FLASH MEM MLC D
   Intel and Seagate, 2003, SERIAL ATA NATIVE CO
   Jung  M., 2012, P 4 USENIX WORKSH HO
   JUNG M., 2009, P IWSSPS
   Kim Chulbum, 2011, P VLSIC
   Kim Youngjae, 2009, P SIMUL
   LBNL, NEW BREED SUP IMPR G
   Lee J, 2009, EURASIP J WIREL COMM, DOI 10.1155/2009/302092
   Lee S., 2008, P SIGMOD
   Lee Seungjae, 2004, P ISSCC
   Lee Sungjin, 2009, P USENIX ATC
   Maghraoui Kaoutar El, 2010, P WORSP SPIEW
   Micron Technology Inc, 2009, NAND FLASH MEM DAT M
   Micron Technology Inc., 2007, NAND FLASH MEM MLC D
   Mohan V, 2013, IEEE T COMPUT AID D, V32, P1031, DOI 10.1109/TCAD.2013.2249557
   ON FI Working Group, 2011, OP NAND FLASH INT
   Park SY, 2010, IEEE COMPUT ARCHIT L, V9, P9, DOI 10.1109/L-CA.2010.3
   Patterson DA, 2004, COMMUN ACM, V47, P71, DOI 10.1145/1022594.1022596
   Roohparvar Frankie F., 2007, U.S. Patent, Patent No. [20070133249 A1, 20070133249]
   SNIA, 2006, IOTTA REP
   Wehner Michael F., 2011, JAMES, V3
   Wei Michael, 2011, P USENIX FAST
NR 29
TC 12
Z9 13
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD FEB
PY 2016
VL 12
IS 2
AR 6
DI 10.1145/2700310
PG 32
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DJ0PO
UT WOS:000373906600001
OA Bronze, Green Submitted
DA 2024-07-18
ER

PT J
AU Rodeh, O
   Helman, H
   Chambliss, D
AF Rodeh, Ohad
   Helman, Haim
   Chambliss, David
TI Visualizing Block IO Workloads
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Algorithms; Performance; Visualization
AB Massive block IO systems are the workhorses powering many of today's largest applications. Databases, health care systems, and virtual machine images are examples for block storage applications. The massive scale of these workloads, and the complexity of the underlying storage systems, makes it difficult to pinpoint problems when they occur. This work attempts to shed light on workload patterns through visualization, aiding our intuition.
   We describe our experience in the last 3 years of analyzing and visualizing customer traces from XIV, an IBM enterprise block storage system. We also present results from applying the same visualization technology to Linux filesystems.
   We show how visualization aids our understanding of workloads and how it assists in resolving customer performance problems.
C1 [Rodeh, Ohad] DNAnexus, Mountain View, CA 94040 USA.
   [Helman, Haim] Robin Syst, Milpitas, CA USA.
   [Chambliss, David] IBM Almaden Res Ctr, San Jose, CA USA.
C3 International Business Machines (IBM)
RP Rodeh, O (corresponding author), DNAnexus, 1975 El Camino Real, Mountain View, CA 94040 USA.
EM ohad.rodeh@gmail.com; haim.helman@gmail.com; chamb@us.ibm.com
CR [Anonymous], 1986, VISUAL DISPLAY QUANT
   Arnheim Rudolf., 2004, Art and Visual Perception: A Psychology of the Creative Eye
   Dufrasne Bertrand, 2012, SOLID STATE DRIVE CA
   Gregg B, 2010, COMMUN ACM, V53, P48, DOI 10.1145/1785414.1785435
   McDougall R., 2005, Filebench
   Rodeh O, 2013, ACM T STORAGE, V9, DOI 10.1145/2501620.2501623
   Tufte ER, 1997, Beautiful Evidence
   Tufte ER, 1990, Envisioning Information
   Wade NicholasJ., 2001, VISUAL PERCEPTION IN, V2nd
   [No title captured]
   [No title captured]
NR 11
TC 4
Z9 5
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAR
PY 2015
VL 11
IS 2
AR 6
DI 10.1145/2651422
PG 18
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CF1KG
UT WOS:000352303400002
DA 2024-07-18
ER

PT J
AU Wei, QS
   Chen, C
   Xue, MD
   Yang, J
AF Wei, Qingsong
   Chen, Cheng
   Xue, Mingdi
   Yang, Jun
TI Z-MAP: A Zone-Based Flash Translation Layer with Workload Classification
   for Solid-State Drive
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Management; Design; Measurement; Performance; Flash memory; solid-state
   drive; space management; workload classification; flash translation
   layer
AB Existing space management and address mapping schemes for flash-based Solid-State-Drive (SSD) operate either at page or block granularity, with inevitable limitations in terms of memory requirement, performance, garbage collection, and scalability. To overcome these limitations, we proposed a novel space management and address mapping scheme for flash referred to as Z-MAP, which manages flash space at granularity of Zone. Each Zone consists of multiple numbers of flash blocks. Leveraging workload classification, Z-MAP explores Page-mapping Zone (Page Zone) to store random data and handle a large number of partial updates, and Block-mapping Zone (Block Zone) to store sequential data and lower the overall mapping table. Zones are dynamically allocated and a mapping scheme for a Zone is determined only when it is allocated. Z-MAP uses a small part of Flash memory or phase change memory as a streaming Buffer Zone to log data sequentially and migrate data into Page Zone or Block Zone based on workload classification. A two-level address mapping is designed to reduce the overall mapping table and address translation latency. Z-MAP classifies data before it is permanently stored into Flash memory so that different workloads can be isolated and garbage collection overhead can be minimized.
   Z-MAP has been extensively evaluated by trace-driven simulation and a prototype implementation on OpenSSD. Our benchmark results conclusively demonstrate that Z-MAP can achieve up to 76% performance improvement, 81% mapping table reduction, and 88% garbage collection overhead reduction compared to existing Flash Translation Layer (FTL) schemes.
C1 [Wei, Qingsong; Chen, Cheng; Xue, Mingdi; Yang, Jun] ASTAR, Data Storage Inst, Singapore, Singapore.
C3 Agency for Science Technology & Research (A*STAR); A*STAR - Data Storage
   Institute
RP Wei, QS (corresponding author), ASTAR, Data Storage Inst, Singapore, Singapore.
EM WEI_Qingsong@dsi.a-star.edu.sg
RI WEI, Qingsong/P-4159-2019
OI chen, cheng/0000-0002-2622-4075
FU Science and Engineering Research Council Grant; Agency for Science,
   Technology and Research (A-STAR), Singapore [112-172-0010]
FX This work is supported by Science and Engineering Research Council
   Grant, Agency for Science, Technology and Research (A-STAR), Singapore,
   under grant 112-172-0010.
CR Agrawal Nitin, 2008, P USENIX ANN TECHN C, P57
   [Anonymous], 2009, P USENIX ANN TECHN C
   [Anonymous], 2011, P 27 S MASS STOR SYS
   [Anonymous], 2011, P FAST 2
   [Anonymous], 2011, ACM 11 P INT C SUPER
   Badam A., 2011, P 10 USENIX S NETW S
   Benini L, 2000, IEEE T VLSI SYST, V8, P299, DOI 10.1109/92.845896
   Chen F, 2009, PERF E R SI, V37, P181
   Chen Feng, 2011, P 9 USENIX C FIL STO
   Cheveresan R., 2007, ICS 07, P73, DOI [DOI 10.1145/1274971.1274984, 10.1145/1274971.1274984]
   Grupp L. M., 2012, FAST 12 P 10 USENIX
   Gupta A, 2009, ACM SIGPLAN NOTICES, V44, P229, DOI 10.1145/1508284.1508271
   Hildebrand Dean., 2006, ICS 06 P 20 ANN INT, P116
   Hu Y., 2010, P 2010 IEEE 26 S MAS
   Hyun Jin Choi, 2009, ACM Transaction on Storage, V4, DOI 10.1145/1480439.1480443
   Kang J., 2006, Proceedings of the International Conference on Embedded Software (EMSOFT), P161
   Kim H, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P239, DOI 10.1145/1378600.1378627
   Kim J, 2002, IEEE T CONSUM ELECTR, V48, P366
   Kwon Hunki, 2010, P 10 ACM INT C EMB S, P169, DOI DOI 10.1145/1879021.1879044
   Lee J., 2013, P 4 AS PAC WORKSH SY
   Lee ST, 2007, J CLIN NEUROL, V3, P18, DOI 10.3988/jcn.2007.3.1.18
   Lim S. P., 2010, P 2010 INT WORKSH ST
   Liu Ren-Shuo., 2012, Target, V11, P00
   Mina C., 2012, P 10 USENIX C FIL ST
   Pan Yangyang., 2011, Proceedings of the 9th USENIX Conference on File and Storage Technologies, P245
   Park D., 2009, 09023 TR U MINN
   Ramakrishnan R., 2007, DATABASE MANAGEMENT
   Ren J, 2011, INT S HIGH PERF COMP, P278, DOI 10.1109/HPCA.2011.5749736
   SNIA, 2011, BLOCK 10 TRAC
   Sungjin Lee, 2008, Operating Systems Review, V42, P36, DOI 10.1145/1453775.1453783
   Sweeney A., 1996, P USENIX ANN TECHN C
   Wei Q. S., 2011, P 2011 IEEE 27 S MAS
   Wu G., 2011, P 9 USENIX C FIL STO
NR 33
TC 3
Z9 3
U1 0
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD FEB
PY 2015
VL 11
IS 1
AR 4
DI 10.1145/2629663
PG 33
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CD8CR
UT WOS:000351323300004
DA 2024-07-18
ER

PT J
AU Altiparmak, N
   Tosun, AS
AF Altiparmak, Nihat
   Tosun, Ali Saman
TI Generalized Optimal Response Time Retrieval of Replicated Data from
   Storage Arrays
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Algorithms; Performance; Declustering; replication; storage
   arrays; generalized retrieval; maximum flow; linear programming
ID ALLOCATION
AB Declustering techniques reduce query response times through parallel I/O by distributing data among parallel disks. Recently, replication-based approaches were proposed to further reduce the response time. Efficient retrieval of replicated data from multiple disks is a challenging problem. Existing retrieval techniques are designed for storage arrays with identical disks, having no initial load or network delay. In this article, we consider the generalized retrieval problem of replicated data where the disks in the system might be heterogeneous, the disks may have initial load, and the storage arrays might be located on different sites. We first formulate the generalized retrieval problem using a Linear Programming (LP) model and solve it with mixed integer programming techniques. Next, the generalized retrieval problem is formulated as a more efficient maximum flow problem. We prove that the retrieval schedule returned by the maximum flow technique yields the optimal response time and this result matches the LP solution. We also propose a low-complexity online algorithm for the generalized retrieval problem by not guaranteeing the optimality of the result. Performance of proposed and state of the art retrieval strategies are investigated using various replication schemes, query types, query loads, disk specifications, network delays, and initial loads.
C1 [Altiparmak, Nihat; Tosun, Ali Saman] Univ Texas San Antonio, Dept Comp Sci, San Antonio, TX USA.
C3 University of Texas System; University of Texas at San Antonio (UTSA)
RP Altiparmak, N (corresponding author), Univ Texas San Antonio, Dept Comp Sci, San Antonio, TX USA.
EM naltipar@cs.utsa.edu; tosun@cs.utsa.edu
RI Altiparmak, Nihat/AAE-3932-2019; Tosun, Ali S/K-4861-2013
OI Altiparmak, Nihat/0000-0002-7038-368X; 
FU US National Science Foundation (NSF) [CNS-0855247]
FX This research was supported by US National Science Foundation (NSF)
   Grant CNS-0855247.
CR Abdel-Ghaffar KAS, 1997, LECT NOTES COMPUT SC, V1186, P409
   Adaptec, 2010, AD HIGH PERF HYBR AR
   Agrawal Nitin., 2008, Proc. Annual Technical Conference (ATC), P57
   Altiparmak N, 2012, IEEE T PARALL DISTR, V23, P538, DOI 10.1109/TPDS.2011.177
   Anderson R. J., 1992, SPAA '92. 4th Annual ACM Symposium on Parallel Algorithms and Architectures, P168, DOI 10.1145/140901.140919
   [Anonymous], IBM ILOG CPLEX optimization studio
   [Anonymous], 2012, ZEB HYBR STOR ARR
   [Anonymous], COMPLEX COMPUT COMPU
   [Anonymous], 2011, VIOL 6000 FLASH MEM
   [Anonymous], 2010, VIOL 3200 FLASH MEM
   [Anonymous], 1997, Linear Programming 1: Introduction
   Atallah M.J., 2000, P ACM S PRINC DAT SY, P205
   Bader D.A., 2005, ISCA PDCS, P41
   BECKMANN N, 1990, SIGMOD REC, V19, P322, DOI 10.1145/93605.98741
   Bhatia R, 2000, LECT NOTES COMPUT SC, V1777, P525
   Bhatia R., 2000, Proceedings of 16th International Conference on Data Engineering (Cat. No.00CB37073), P271, DOI 10.1109/ICDE.2000.839419
   Chen C.-M., 2002, PROC ACM PODS, P29
   Chen C.-M., 2003, P C INF KNOWL MAN CI
   DU HC, 1982, ACM T DATABASE SYST, V7, P82, DOI 10.1145/319682.319698
   EqualLogic, 2011, EQ PS6100XS HYBR STO
   Faloutsos C., 1993, Proceedings of the Second International Conference on Parallel and Distributed Information Systems (Cat. No.93TH0493-7), P18, DOI 10.1109/PDIS.1993.253077
   Fan C., 1994, P 8 INT PAR PROC S
   Ferhatosmanoglu Hakan., 2004, PODS, P125
   FRIKKEN K, 2002, P 13 INT C DAT EXP S, P669
   Frikken KB, 2005, LECT NOTES COMPUT SC, V3363, P144
   Gaede V, 1998, ACM COMPUT SURV, V30, P170, DOI 10.1145/280277.280279
   GHANDEHARIZADEH S, 1990, VERY LARGE DATA BASES, P481
   Ghandeharizadeh S., 1990, Sixth International Conference on Data Engineering (Cat. No.90CH2840-7), P466, DOI 10.1109/ICDE.1990.113500
   GOLDBERG AV, 1988, J ACM, V35, P921, DOI 10.1145/48014.61051
   Guttman A., 1984, SIGMOD Record, V14, P47, DOI 10.1145/971697.602266
   Hong B, 2011, IEEE T PARALL DISTR, V22, P1025, DOI 10.1109/TPDS.2010.156
   Hua K.A., 1997, P DAT EXP SYST APPL, P401
   Kavalanekar S, 2008, I S WORKL CHAR PROC, P111
   KIM K, 1993, IEEE T PARALL DISTR, V4, P361, DOI 10.1109/71.219753
   KIM MH, 1988, P ACM SIGMOD INT C M, P173
   Koyutürk M, 2005, INFORM SYST, V30, P47, DOI 10.1016/j.is.2003.08.003
   Ling Tony Chen, 1994, Proceedings of the Thirteenth ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems. PODS 1994, P36, DOI 10.1145/182591.182596
   MEHLHORN K, 1995, COMMUN ACM, V38, P96, DOI 10.1145/204865.204889
   Mitzenmacher M, 2001, IEEE T PARALL DISTR, V12, P1094, DOI 10.1109/71.963420
   Narayanan D, 2008, DISTRIB PARALLEL DAT, P15
   Narayanan D., 2008, P 8 USENIX C OPERATI, P15
   Narayanan D, 2009, EUROSYS'09: PROCEEDINGS OF THE FOURTH EUROSYS CONFERENCE, P145
   Oktay KY, 2009, LECT NOTES COMPUT SC, V5704, P375, DOI 10.1007/978-3-642-03869-3_37
   Orenstein G, 2003, NIMB DAT S CLASS ENT
   Orenstein G., 2003, IP STORAGE NETWORKIN
   Prabhakar S, 1998, PROC INT CONF DATA, P94, DOI 10.1109/ICDE.1998.655763
   PRABHAKAR S, 1998, P 10 INT S PAR ALG A, P78
   Ramsan, 2010, RAMS 630 FLASH SOL S
   Samet H., 1989, DESIGN ANAL SPATIAL
   Sanders P., 2000, P 11 ACM SIAM S DISC
   Sun, 2009, IOTT RESP
   Sun, 2009, SUN STOR F5100 FLASH
   Sun, 2009, SUN STOR 7000 UN STO
   Sun, 2009, INFORM SYST
   Tosun AS, 2007, IEEE T PARALL DISTR, V18, P1578, DOI 10.1109/TPDS.2007.1082
   Tosun AS, 2007, INFORM SCIENCES, V177, P1309, DOI 10.1016/j.ins.2006.08.011
   Tosun AS, 2008, INT CON DISTR COMP S, P486, DOI 10.1109/ICDCS.2008.72
   Tosun AS, 2005, ITCC 2005: INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY: CODING AND COMPUTING, VOL 1, P232, DOI 10.1109/ITCC.2005.112
   Tosun AS, 2005, ITCC 2005: INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY: CODING AND COMPUTING, VOL 1, P226, DOI 10.1109/ITCC.2005.124
   Tosun AS, 2005, LECT NOTES COMPUT SC, V3588, P818
   Tosun AS, 2002, INT CONF PARA PROC, P506, DOI 10.1109/ICPPW.2002.1039772
   Zebi, 2012, DED INT ACC OV
   [No title captured]
NR 63
TC 4
Z9 5
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUL
PY 2013
VL 9
IS 2
AR 5
DI 10.1145/2491472.2491474
PG 36
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 192IZ
UT WOS:000322479000002
DA 2024-07-18
ER

PT J
AU Sankar, S
   Shaw, M
   Vaid, K
   Gurumurthi, S
AF Sankar, Sriram
   Shaw, Mark
   Vaid, Kushagra
   Gurumurthi, Sudhanva
TI Datacenter Scale Evaluation of the Impact of Temperature on Hard Disk
   Drive Failures
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Experimentation; Reliability; Datacenter; hard disk drives;
   temperature impact
AB With the advent of cloud computing and online services, large enterprises rely heavily on their datacenters to serve end users. A large datacenter facility incurs increased maintenance costs in addition to service unavailability when there are increased failures. Among different server components, hard disk drives are known to contribute significantly to server failures; however, there is very little understanding of the major determinants of disk failures in datacenters. In this work, we focus on the interrelationship between temperature, workload, and hard disk drive failures in a large scale datacenter. We present a dense storage case study from a population housing thousands of servers and tens of thousands of disk drives, hosting a large-scale online service at Microsoft. We specifically establish correlation between temperatures and failures observed at different location granularities: (a) inside drive locations in a server chassis, (b) across server locations in a rack, and (c) across multiple racks in a datacenter. We show that temperature exhibits a stronger correlation to failures than the correlation of disk utilization with drive failures. We establish that variations in temperature are not significant in datacenters and have little impact on failures. We also explore workload impacts on temperature and disk failures and show that the impact of workload is not significant. We then experimentally evaluate knobs that control disk drive temperature, including workload and chassis design knobs. We corroborate our findings from the real data study and show that workload knobs show minimal impact on temperature. Chassis knobs like disk placement and fan speeds have a larger impact on temperature. Finally, we also show the proposed cost benefit of temperature optimizations that increase hard disk drive reliability.
C1 [Sankar, Sriram; Shaw, Mark; Vaid, Kushagra] Microsoft Corp, Redmond, WA 98052 USA.
   [Gurumurthi, Sudhanva] Univ Virginia, Dept Comp Sci, Charlottesville, VA 22903 USA.
C3 Microsoft; University of Virginia
RP Sankar, S (corresponding author), Microsoft Corp, Redmond, WA 98052 USA.
EM srsankar@microsoft.com
CR [Anonymous], P 1 ACM S CLOUD COMP
   [Anonymous], P 11 INT JOINT C MEA
   Cole G, 2000, TP3381 SEAG, P1
   El-Sayed, 2012, P 12 ACM SIGMETRICS
   Elerath JG, 2004, P A REL MAI, P151, DOI 10.1109/RAMS.2004.1285439
   Facebook, 2011, OP COMP PROJ FAC
   Govindan M. S. S., 2009, P WORKSH EN EFF DES
   GRAY J, 2005, MSRTR2005166 MICR RE
   Greenberg S., 2006, ACEEE SUMM STUD EN E
   Guo GX, 2003, IEEE T MAGN, V39, P2103, DOI 10.1109/TMAG.2003.814287
   Gurumurthi S, 2005, CONF PROC INT SYMP C, P38, DOI 10.1109/ISCA.2005.24
   Gurumurthi S, 2003, P INT S PERF AN SYST, P23
   Hamilton J, 2008, DATACENTER TCO MODEL
   Hamilton J, 2007, P CIDR
   Hoelzle Urs., 2009, The Datacenter as a Computer: An Introduction to the Design of Warehouse-Scale Machines, V1st
   HP, 2003, ASS COMP SER ATT SCS
   HP, 2011, SSA70 STOR DISK ENCL
   Intel, 2008, RED DAT CTR COST AIR
   Kim YJ, 2006, INT S HIGH PERF COMP, P179
   Microsoft, 2009, DAT KNOWL
   Namek R. Y, 2011, DATACENTERDYNAMICS
   PARK I., 2007, Improve debugging and performance tuning with etw
   Patterson MK, 2008, INTERSOC C THERMAL T, P1167, DOI 10.1109/ITHERM.2008.4544393
   Pinheiro E, 2007, P FAST C FIL STOR TE
   Sankar S, 2008, P INT S COMP ARCH
   SCHROEDER B, 2006, P INT S DEP SYST NET
   Schroeder B, 2007, P 5 USENIX C FIL STO, P13
   SCHWARTZ T, 2006, P 14 NASA GODD 23 IE
   Seagate, 2011, SEAG CONST ES DRIV D
   Yang J, 1999, P A REL MAI, P403, DOI 10.1109/RAMS.1999.744151
NR 30
TC 37
Z9 42
U1 1
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUL
PY 2013
VL 9
IS 2
AR 6
DI 10.1145/2491472.2491475
PG 24
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 192IZ
UT WOS:000322479000003
DA 2024-07-18
ER

PT J
AU Schroeder, B
   Damouras, S
   Gill, P
AF Schroeder, Bianca
   Damouras, Sotirios
   Gill, Phillipa
TI Understanding Latent Sector Errors and How to Protect Against Them
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Latent sector errors; failure data; field data; failure modeling;
   storage reliability; data loss; scrubbing; redundancy; parity;
   Reliability; Measurement
AB Latent sector errors (LSEs) refer to the situation where particular sectors on a drive become inaccessible. LSEs are a critical factor in data reliability, since a single LSE can lead to data loss when encountered during RAID reconstruction after a disk failure or in systems without redundancy. LSEs happen at a significant rate in the field [Bairavasundaram et al. 2007], and are expected to grow more frequent with new drive technologies and increasing drive capacities. While two approaches, data scrubbing and intra-disk redundancy, have been proposed to reduce data loss due to LSEs, none of these approaches has been evaluated on real field data.
   This article makes two contributions. We provide an extended statistical analysis of latent sector errors in the field, specifically from the view point of how to protect against LSEs. In addition to providing interesting insights into LSEs, we hope the results (including parameters for models we fit to the data) will help researchers and practitioners without access to data in driving their simulations or analysis of LSEs. Our second contribution is an evaluation of five different scrubbing policies and five different intra-disk redundancy schemes and their potential in protecting against LSEs. Our study includes schemes and policies that have been suggested before, but have never been evaluated on field data, as well as new policies that we propose based on our analysis of LSEs in the field.
C1 [Schroeder, Bianca] Univ Toronto, Dept Comp Sci, Toronto, ON M5S 1A1, Canada.
C3 University of Toronto
RP Schroeder, B (corresponding author), Univ Toronto, Dept Comp Sci, Toronto, ON M5S 1A1, Canada.
EM bianco@cs.toronto.edu
FU NSERC Discovery grant
FX This work was supported in part by an NSERC Discovery grant.
CR [Anonymous], 1988, A case for redundant arrays of inexpensive disks RAID, DOI DOI 10.1145/50202.50214
   [Anonymous], 2006, P 1 ACM SIGOPSEUROSY
   Bairavasundaram LN, 2007, PERF E R SI, V35, P289
   BLAUM M., 1994, P 21 INT C COMP ARCH
   Corbett P, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P1
   Dholakia Ajay, 2008, ACM Transaction on Storage, V4, DOI 10.1145/1353452.1353453
   Elerath J, 2009, COMMUN ACM, V52, P38, DOI 10.1145/1516046.1516059
   GUNAWI H. S., 2007, P 21 ACM S OP SYST P, P283
   HAFNER J. L., 2005, P 4 C USENIX C FIL S, V4
   Hafner JL, 2006, I C DEPEND SYS NETWO, P217, DOI 10.1109/DSN.2006.40
   Iliadis I, 2008, PERF E R SI, V36, P241, DOI 10.1145/1384529.1375485
   MCKUSICK MK, 1984, ACM T COMPUT SYST, V2, P181, DOI 10.1145/989.990
   OPREA A., 2010, P FAST 10
   Plank JS, 2009, INT J HIGH PERFORM C, V23, P242, DOI 10.1177/1094342009106191
   Prabhakaran Vijayan, 2005, P OF THE 20 ACM S ON, V39, P206
   RISKA A., 2008, P 37 ANN IEEE IFIP I
   Schwarz T., 2004, P IEEE COMP SOC 12 A
   WYLIE J. J., 2007, P 37 ANN IEEE IFIP I
   ZHANG Y., 2010, P 8 C FIL STOR TECHN
NR 19
TC 56
Z9 66
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD SEP
PY 2010
VL 6
IS 3
AR 9
DI 10.1145/1837915.1837917
PG 23
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA V24QL
UT WOS:000208424800002
DA 2024-07-18
ER

PT J
AU Khatib, MG
   Hartel, PH
AF Khatib, Mohammed G.
   Hartel, Pieter H.
TI Optimizing MEMS-Based Storage Devices for Mobile Battery-Powered Systems
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Economics; Management; Performance; Probe storage; energy efficiency;
   green storage; mobile systems; design space
ID MEMORY
AB An emerging storage technology, called MEMS-based storage, promises nonvolatile storage devices with ultrahigh density, high rigidity, a small form factor, and low cost. For these reasons, MEMS-based storage devices are suitable for battery-powered mobile systems such as PDAs. For deployment in such systems, MEMS-based storage devices must consume little energy. This work mainly targets reducing the energy consumption of this class of devices.
   We derive the operation modes of a MEMS-based storage device and systemically devise a policy in each mode for energy saving. Three types of policies are presented: power management, shutdown, and data-layout policy. Combined, these policies reduce the total energy consumed by a MEMS-based storage device. A MEMS-based storage device that enforces these policies comes close to Flash with respect to energy consumption and response time. However, enhancement on the device level is still needed; we present some suggestions to resolve this issue.
C1 [Khatib, Mohammed G.; Hartel, Pieter H.] Univ Twente, Enschede, Netherlands.
C3 University of Twente
RP Khatib, MG (corresponding author), Univ Twente, Enschede, Netherlands.
EM m.g.khatib@utwente.nl
OI Hartel, Pieter/0000-0002-0411-0421
FU Technology Foundation STW; Applied Science Division of NWO; Technology
   Program of the Ministry of Economic Affairs [TES.06369]
FX This research is supported by the Technology Foundation STW, Applied
   Science Division of NWO and The Technology Program of the Ministry of
   Economic Affairs under project TES.06369.
CR Abelmann L, 2003, IEE P-SCI MEAS TECH, V150, P218, DOI 10.1049/ip-smt:20030693
   Benini L, 2000, IEEE T VLSI SYST, V8, P299, DOI 10.1109/92.845896
   Bennewitz R, 2002, NANOTECHNOLOGY, V13, P499, DOI 10.1088/0957-4484/13/4/312
   Bo Hong, 2006, ACM Transaction on Storage, V2, P1, DOI 10.1145/1138041.1138042
   BUCY HS, 2003, DISKSIM SIMULATION E
   Carley LR, 2000, J APPL PHYS, V87, P6680, DOI 10.1063/1.372807
   Griffin JL, 2000, USENIX ASSOCIATION PROCEEDINGS OF THE FOURTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P227
   Griffin JT, 2000, PERF E R SI, V28, P56, DOI 10.1145/345063.339354
   Gurumurthi S., 2005, THESIS PENNSYLVANIA
   Hong B., 2006, ACM Transaction on Storage, V2, P139, DOI 10.1145/1149976.1149978
   Hyokyung Bahn, 2009, ACM Transaction on Storage, V5, DOI 10.1145/1502777.1502778
   Jacob B, 2008, MEMORY SYSTEMS: CACHE, DRAM, DISK, P1
   Kernel Tracing, 2009, KERNEL TRACE SYSTEMS
   KHATIB M. G., 2008, TRCTIT0803 U TWENT
   Lai SK, 2008, IBM J RES DEV, V52, P529, DOI 10.1147/rd.524.0529
   Lantz MA, 2007, J MICROELECTROMECH S, V16, P130, DOI 10.1109/JMEMS.2006.886032
   MCCARTHY S., 2002, P USENIX C FIL STOR
   Pantazi A, 2008, IBM J RES DEV, V52, P493, DOI 10.1147/rd.524.0493
   Prall K, 2007, 2007 22ND IEEE NON-VOLATILE SEMICONDUCTOR MEMORY WORKSHOP, P5, DOI 10.1109/NVSMW.2007.4290561
   RANGASWAMI R., 2007, ACM T STORAGE, V3, P31
   Schlosser SW, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P87
   Schlosser SW, 2000, ACM SIGPLAN NOTICES, V35, P1, DOI 10.1145/384264.378996
   SCHLOSSER SW, 2004, CMUPDL04104
   SIVANZIMET M, 2001, THESIS U CALIFORNIA
   TOSHIBA, 2004, MK4001MTD 0 85 HDD 4
   Uysal M, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE 2ND USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST'03), P89
   Varsamou M, 2008, IEEE T MAGN, V44, P547, DOI 10.1109/TMAG.2007.915059
   Yung-Hsiang Lu, 2000, Proceedings Design, Automation and Test in Europe Conference and Exhibition 2000 (Cat. No. PR00537), P20, DOI 10.1109/DATE.2000.840010
   [No title captured]
NR 29
TC 1
Z9 1
U1 0
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAR
PY 2010
VL 6
IS 1
AR 1
DI 10.1145/1714454.1714455
PG 37
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA V24QF
UT WOS:000208424200001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Jackowski, A
   Gryz, L
   Welnicki, M
   Dubnicki, C
   Iwanicki, K
AF Jackowski, Andrzej
   Gryz, Leszek
   Welnicki, Michal
   Dubnicki, Cezary
   Iwanicki, Konrad
TI Derrick: A Three-layer Balancer for Self-managed Continuous Scalability
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Data balancing; distributed storage; capacity utilization
ID CODES
AB Data arrangement determines the capacity, resilience, and performance of a distributed storage system. A scalable self-managed system must place its data efficiently not only during stable operation but also after an expansion, planned downscaling, or device failures. In this article, we present Derrick, a data balancing algorithm addressing these needs, which has been developed for HYDRAstor, a highly scalable commercial storage system. Derrick makes its decisions quickly in case of failures but takes additional time to find a nearly optimal data arrangement and a plan for reaching it when the device population changes. Compared to balancing algorithms in two other state-of-the-art systems, Derrick provides better capacity utilization, reduced data movement, and improved performance. Moreover, it can be easily adapted to meet custom placement requirements.
C1 [Jackowski, Andrzej; Gryz, Leszek; Welnicki, Michal; Dubnicki, Cezary] 9LivesData, Ul Ekologiczna 1-19, PL-02798 Warsaw, Poland.
   [Iwanicki, Konrad] Wydzial Matemat Informatyki & Mech, Ul Banacha 2, PL-02097 Warsaw, Poland.
RP Jackowski, A (corresponding author), 9LivesData, Ul Ekologiczna 1-19, PL-02798 Warsaw, Poland.
EM jackowski@9livesdata.com; gryz@9livesdata.com; welnicki@9livesdata.com;
   dubnicki@9livesdata.com; iwanicki@mimuw.edu.pl
CR [Anonymous], 2009, FAST
   [Anonymous], 2009, Principles of computer system design: an introduction
   Aye Kyar Nyo, 2014, THESIS
   Beaver D., 2010, P 9 USENIX S OP SYST, P1
   Calder B, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P143
   Ceph, 2022, PLAC GROUPS
   Ceph, 2022, BAL PLUG
   Ceph, 2022, PLAC GROUPS PGS
   Ceph, 2022, V0 94 10 HAMMER
   Corbett JC, 2013, ACM T COMPUT SYST, V31, DOI 10.1145/2491245
   Coughlin Tom., 2021, C1Q 2021 HDD UPDATE
   DISTANTE F, 1989, IEEE T RELIAB, V38, P28, DOI 10.1109/24.24571
   Douceur JR, 2001, NINTH INTERNATIONAL SYMPOSIUM ON MODELING, ANALYSIS AND SIMULATION OF COMPUTER AND TELECOMMUNICATION SYSTEMS, PROCEEDINGS, P311, DOI 10.1109/MASCOT.2001.948882
   Golab L, 2013, Arxiv, DOI arXiv:1312.0285
   Holt Greg., 2011, BUILDING CONSISTENT
   Hou HX, 2019, IEEE T INFORM THEORY, V65, P4730, DOI 10.1109/TIT.2019.2902835
   HPE, 2018, STORAGEEXPERTS WHATS
   Hsiao HC, 2013, IEEE T PARALL DISTR, V24, P951, DOI 10.1109/TPDS.2012.196
   Huang Cheng, 2012, 2012 USENIX ANN TECH, P15
   IBM, 2018, IBM CLOUD OBJ STOR S
   IDC, 2022, ENT STOR SYST MARK S
   Johnson AW, 2002, DISCRETE APPL MATH, V119, P37, DOI 10.1016/S0166-218X(01)00264-5
   Kadekodi S, 2022, PROCEEDINGS OF THE 16TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, OSDI 2022, P413
   Kadekodi S, 2020, PROCEEDINGS OF THE 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '20), P369
   Kadekodi S, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P345
   Kamath GM, 2014, IEEE T INFORM THEORY, V60, P4637, DOI 10.1109/TIT.2014.2329872
   Karger DavidR., 1997, P 29 ANN ACM S THEOR, P654, DOI DOI 10.1145/258533.258660
   Lakshman Avinash, 2010, Operating Systems Review, V44, P35, DOI 10.1145/1773912.1773922
   Li HB, 2019, PROCEEDINGS OF THE FOURTEENTH EUROSYS CONFERENCE 2019 (EUROSYS '19), DOI 10.1145/3302424.3303967
   Luby M, 2006, CONSUM COMM NETWORK, P192
   OpenStack Foundation, 2022, INCR PART POW
   OpenStack Foundation, 2022, ADM GUID
   Palmer Julia, 2021, MAGIC QUADRANT DISTR
   Pan S, 2021, PROCEEDINGS OF THE 19TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '21), P217
   Paulsen John, 2021, ENERGY ASSISTED MAGN
   Qingsong Wei, 2010, Proceedings of the 2010 IEEE International Conference on Cluster Computing (CLUSTER 2010), P188, DOI 10.1109/CLUSTER.2010.24
   Shvachko K, 2010, IEEE S MASS STOR SYS
   Skowron Piotr, 2013, P 6 INT SYSTEMS STOR, P1
   Strzelczak Przemyslaw., 2013, FAST, P161
   Wang L, 2020, PROCEEDINGS OF THE 18TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P1
   Weil SA, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P307
   Weil Sage A., 2006, P ACM IEEE C SUP SC, P122, DOI [10.1145/1188455.1188582, DOI 10.1145/1188455.1188582]
   Xie W, 2017, INT PARALL DISTRIB P, P876, DOI 10.1109/IPDPS.2017.88
   Zhang M, 2017, SYM REL DIST SYST, P144, DOI 10.1109/SRDS.2017.19
NR 44
TC 0
Z9 0
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2023
VL 19
IS 3
AR 27
DI 10.1145/3594543
PG 34
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P9QZ4
UT WOS:001053965500007
DA 2024-07-18
ER

PT J
AU Lawson, M
   Gropp, W
   Lofstead, J
AF Lawson, Margaret
   Gropp, William
   Lofstead, Jay
TI EMPRESS: Accelerating Scientific Discovery through Descriptive Metadata
   Management
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Descriptive metadata; data tagging; high-level indexing; EMPRESS; HDF5;
   accelerating scientific discovery; Decaf; ATDM
ID IN-SITU VISUALIZATION; TEMPESTEXTREMES; OPTIMIZATION; EXTRACTION;
   FRAMEWORK; EFFICIENT; SELECTION; TRACKING; MAP
AB High-performance computing scientists are producing unprecedented volumes of data that take a long time to load for analysis. However, many analyses only require loading in the data containing particular features of interest and scientists have many approaches for identifying these features. Therefore, if scientists store information (descriptive metadata) about these identified features, then for subsequent analyses they can use this information to only read in the data containing these features. This can greatly reduce the amount of data that scientists have to read in, thereby accelerating analysis. Despite the potential benefits of descriptive metadata management, no prior work has created a descriptive metadata system that can help scientists working with a wide range of applications and analyses to restrict their reads to data containing features of interest. In this article, we present EMPRESS, the first such solution. EMPRESS offers all of the features needed to help accelerate discovery: It can accelerate analysis by up to 300x, supports a wide range of applications and analyses, is high-performing, is highly scalable, and requires minimal storage space. In addition, EMPRESS offers features required for a production-oriented system: scalable metadata consistency techniques, flexible system configurations, fault tolerance as a service, and portability.
C1 [Lawson, Margaret; Gropp, William] Univ Illinois, 1205 W Clark St, Urbana, IL 61801 USA.
   [Lawson, Margaret] 747 6th St S, Kirkland, WA 98033 USA.
   [Lofstead, Jay] Sandia Natl Labs, POB 5800 MS 1319, Albuquerque, NM 87185 USA.
C3 University of Illinois System; University of Illinois Urbana-Champaign;
   United States Department of Energy (DOE); Sandia National Laboratories
RP Lawson, M (corresponding author), Univ Illinois, 1205 W Clark St, Urbana, IL 61801 USA.; Lawson, M (corresponding author), 747 6th St S, Kirkland, WA 98033 USA.
EM mlawson4@illinois.edu; wgropp@illinois.edu; gflofst@sandia.gov
OI Lofstead, Jay/0000-0002-4697-2919
FU U.S. Department of Energy Office of Science, under the SSIO grant
   series, SIRIUS project; Data Management grant series; United States
   Department of Energy through the Computational Sciences Graduate
   Fellowship (DOE CSGF) [DE-SC0020347]; State of Illinois
FX This work was also supported by the U.S. Department of Energy Office of
   Science, under the SSIO grant series, SIRIUS project and the Data
   Management grant series, Decaf project, program manager Lucy Nowell.
   Margaret Lawson acknowledges support from the United States Department
   of Energy through the Computational Sciences Graduate Fellowship (DOE
   CSGF) under grant number: DE-SC0020347. This work was supported in part
   by the State of Illinois.
CR Abbasi H, 2010, CLUSTER COMPUT, V13, P277, DOI 10.1007/s10586-010-0135-6
   Albrand S, 2010, J PHYS CONF SER, V219, DOI 10.1088/1742-6596/219/4/042030
   Alger MJ, 2018, MON NOT R ASTRON SOC, V478, P5556, DOI 10.1093/mnras/sty1308
   Aliabadi S, 2002, LECT NOTES COMPUT SC, V2552, P519
   [Anonymous], 2022, PANASAS
   [Anonymous], 2014, P 12 USENIX C FILE S
   Auricchio Ferdinando, 2018, PARALLEL COMPUT EVER, V32, P57
   Balaji P, 2011, PARALLEL PROCESS LET, V21, P45, DOI 10.1142/S0129626411000060
   Banesh Divya, 2018, PROC WORKSHOP ENVIRV, P27, DOI 10.2312/envirvis.20181134
   Barone M.F., 2021, AIAA SCITECH 2021 FO, DOI [10.2514/6.2021-1750, DOI 10.2514/6.2021-1750]
   Bennett J.C., 2012, HIGH PERFORMANCE COM, P1
   Berrocal E, 2016, LECT NOTES COMPUT SC, V9833, P419, DOI 10.1007/978-3-319-43659-3_31
   Betoule M, 2013, ASTRON ASTROPHYS, V552, DOI 10.1051/0004-6361/201220610
   Beyer J, 2013, IEEE T VIS COMPUT GR, V19, P2868, DOI 10.1109/TVCG.2013.142
   Biswas A, 2018, PROCEEDINGS OF IN SITU INFRASTRUCTURES FOR ENABLING EXTREME-SCALE ANALYSIS AND VISUALIZATION (ISAV 2018), P13, DOI 10.1145/3281464.3281467
   Biswas M, 2018, 2018 4TH INTERNATIONAL CONFERENCE FOR CONVERGENCE IN TECHNOLOGY (I2CT)
   Bonnie David John, 2018, TECHNICAL REPORT
   Bremer PT, 2011, IEEE T VIS COMPUT GR, V17, P1307, DOI 10.1109/TVCG.2010.253
   Byna Suren, 2017, CRAY USER GROUP C CU
   Carr H., 2003, Data Visualisation 2003. Joint Eurographics/IEEE TCVG. Symposium on Visualization, P49
   Chen HB, 2017, INT CONF CLOUD ENG, P59, DOI 10.1109/IC2E.2017.22
   Cubuk ED, 2015, PHYS REV LETT, V114, DOI 10.1103/PhysRevLett.114.108001
   Dayal Jai, 2016, DOUBLY DISTRIBUTED T
   Deelman E., 2005, Scientific Programming, V13, P219
   Deutsch Peter, 1996, RFC 1950
   Devine TR, 2016, MON NOT R ASTRON SOC, V459, P1519, DOI 10.1093/mnras/stw655
   Dewdney PE, 2009, P IEEE, V97, P1482, DOI 10.1109/JPROC.2009.2021005
   Di S, 2018, IEEE T PARALL DISTR, V29, P129, DOI 10.1109/TPDS.2017.2749300
   Docan C, 2012, CLUSTER COMPUT, V15, P163, DOI 10.1007/s10586-011-0162-y
   Dorier M, 2016, IEEE INT C CL COMP, P269, DOI 10.1109/CLUSTER.2016.25
   Feki S, 2008, SASO 2008: SECOND IEEE INTERNATIONAL CONFERENCE ON SELF-ADAPTIVE AND SELF-ORGANIZING SYSTEMS, PROCEEDINGS, P265, DOI 10.1109/SASO.2008.47
   Folk Mike, 2011, P EDBT ICDT 2011 WOR, P36
   Franzone PC, 2018, FRONT PHYSIOL, V9, DOI 10.3389/fphys.2018.00268
   Fujishiro I, 2008, LECT NOTES COMPUT SC, V4759, P176
   Gabriel E, 2010, CONCURR COMP-PRACT E, V22, P2230, DOI 10.1002/cpe.1586
   Garcia-Gasulla M, 2020, INT J HIGH PERFORM C, V34, P42, DOI 10.1177/1094342019842919
   Gauci A, 2010, Arxiv, DOI arXiv:1005.0390
   Gong ZH, 2013, IEEE ACM INT SYMP, P343, DOI 10.1109/CCGrid.2013.58
   Gong ZH, 2012, INT PARALL DISTRIB P, P873, DOI 10.1109/IPDPS.2012.83
   Gosink L., 2006, SSDBM, P149, DOI [10.1109/SSDBM.2006.27, DOI 10.1109/SSDBM.2006.27]
   Gosink LJ, 2007, IEEE T VIS COMPUT GR, V13, P1400, DOI 10.1109/TVCG.2007.70519
   Gosink LJ, 2011, IEEE T VIS COMPUT GR, V17, P264, DOI 10.1109/TVCG.2010.80
   Graff P, 2014, MON NOT R ASTRON SOC, V441, P1741, DOI 10.1093/mnras/stu642
   Grawinkel M., 2015, 13 USENIX C FILE STO, P15
   Gropp W., 2014, USING ADV MPI MODERN
   Gropp W., 1999, Using MPI-2: Advanced Features of the Message Passing Interface, Vsecond
   Gropp W., 2011, P INT C SUP ICS 11, P172, DOI [10.1145/1995896.1995924, DOI 10.1145/1995896.1995924]
   Gropp William, 1999, USING MPI PORTABLE P, V1
   Gu JM, 2018, LECT NOTES COMPUT SC, V10776, P51, DOI 10.1007/978-3-319-69953-0_4
   Gyulassy A, 2005, IEEE VISUALIZATION 2005, PROCEEDINGS, P535
   Habib S, 2016, Arxiv, DOI arXiv:1603.09303
   Hong SKY, 2017, Arxiv, DOI arXiv:1708.03417
   Hsu MC, 2011, COMPUT FLUIDS, V49, P93, DOI 10.1016/j.compfluid.2011.05.002
   Hsuan-Te C, 2015, INT C PAR DISTRIB SY, P362, DOI 10.1109/ICPADS.2015.53
   Huertas-Company M, 2018, ASTROPHYS J, V858, DOI 10.3847/1538-4357/aabfed
   Ivezic Zeljko, 2008, arXiv
   JAMO, 2018, JAMO JGI ARCH MET OR
   Jenkins John, 2012, Database and Expert Systems Applications. Proceedings of the 23rd International Conference, DEXA 2012, P16, DOI 10.1007/978-3-642-32597-7_2
   Jenkins J., 2012, Proceedings of the international conference on high performance computing, networking, storage and analysis (supercomputing), P1
   Khan A, 2019, FUTURE GENER COMP SY, V101, P398, DOI 10.1016/j.future.2019.06.006
   Korenblum D, 2011, J DIGIT IMAGING, V24, P739, DOI 10.1007/s10278-010-9328-z
   Ku S, 2009, NUCL FUSION, V49, DOI 10.1088/0029-5515/49/11/115021
   Kuhn Michael, 2020, 2020 International Conference on Computational Science and Computational Intelligence (CSCI), P1224, DOI 10.1109/CSCI51800.2020.00229
   Lakshminarasimhan S, 2014, CLUSTER COMPUT, V17, P1101, DOI 10.1007/s10586-014-0358-z
   Lakshminarasimhan S, 2011, LECT NOTES COMPUT SC, V6852, P366, DOI 10.1007/978-3-642-23400-2_34
   Landge AG, 2014, INT CONF HIGH PERFOR, P1020, DOI 10.1109/SC.2014.88
   Lawson M, 2018, PROCEEDINGS OF 2018 IEEE/ACM 3RD JOINT INTERNATIONAL WORKSHOP ON PARALLEL DATA STORAGE & DATA INTENSIVE SCALABLE COMPUTING SYSTEMS (PDSW-DISCS), P13, DOI 10.1109/PDSW-DISCS.2018.00004
   Lawson Margaret, 2018, THESIS DARMOUTH COLL
   Leung AndrewW., 2009, PROCCEDINGS 7 C FILE, P153
   Li Jianwei, 2003, SC 03, P39, DOI DOI 10.1109/SC.2003.10053
   Liu YJ, 2016, Arxiv, DOI [arXiv:1605.01156, DOI 10.48550/ARXIV.1605.01156]
   Livnat Y, 1996, IEEE T VIS COMPUT GR, V2, P73, DOI 10.1109/2945.489388
   Lofstead J., 2010, Proceedings of the 2010 ACM/IEEE International Conference for High Performance Computing, Networking, Storage and Analysis, SC '10, DOI [DOI 10.1109/SC.2010.32, 10.1109/SC.2010.32.]
   Lofstead J, 2016, SC '16: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS, P585, DOI 10.1109/SC.2016.49
   Lofstead J, 2012, IEEE INT C CL COMP, P90, DOI 10.1109/CLUSTER.2012.79
   Lofstead J, 2011, HPDC 11: PROCEEDINGS OF THE 20TH INTERNATIONAL SYMPOSIUM ON HIGH PERFORMANCE DISTRIBUTED COMPUTING, P49
   Lofstead J, 2009, INT PARALL DISTRIB P, P778
   Lofstead Jay, 2012, HIGH PERFORMANCE COM
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   Ludäscher B, 2006, CONCURR COMP-PRACT E, V18, P1039, DOI 10.1002/cpe.994
   Luttgau Jakob, 2018, [Supercomputing Frontiers and Innovations, Supercomputing Frontiers and Innovations], V5, P31
   Ma KL, 2009, IEEE COMPUT GRAPH, V29, P14, DOI 10.1109/MCG.2009.120
   Machado E, 2016, IEEE IJCNN, P123, DOI 10.1109/IJCNN.2016.7727189
   Malewicz Grzegorz, 2007, Journal of Grid Computing, V5, P197, DOI 10.1007/s10723-007-9065-9
   Lopez JM, 2020, Arxiv, DOI arXiv:1908.00587
   Meister D., 2012, IEEE P INT C HIGH PE, P1, DOI 10.1109/SC.2012.14
   Menon Harshitha, 2015, TECHNICAL REPORT
   Message Passing Interface Forum, 2012, MPI MESSAGE PASSING
   Myers K, 2016, TECHNOMETRICS, V58, P329, DOI 10.1080/00401706.2016.1158740
   Nagle David., 2004, SC 04, P53
   Nielson GM, 2003, IEEE T VIS COMPUT GR, V9, P283, DOI 10.1109/TVCG.2003.1207437
   Oldfield Ron A., 2006, INT WORKSHOP HIGH PE
   Ovsyannikov A, 2016, PROCEEDINGS OF PDSW-DISCS 2016 - 1ST JOINT INTERNATIONAL WORKSHOP ON PARALLEL DATA STORAGE AND DATA INTENSIVE SCALABLE COMPUTING SYSTEMS, P1, DOI [10.1109/PDSW-DISCS.2016.5, 10.1109/PDSW-DISCS.2016.005]
   Pallickara SL, 2012, FUTURE GENER COMP SY, V28, P112, DOI 10.1016/j.future.2011.05.010
   Papadomanolakis S., 2006, P 2006 ACM SIGMOD IN, P551
   Post FH, 2003, COMPUT GRAPH FORUM, V22, P775, DOI 10.1111/j.1467-8659.2003.00723.x
   Potter Douglas, 2017, Computational Astrophysics and Cosmology, V4, DOI 10.1186/s40668-017-0021-1
   Prabhat, 2012, PROCEDIA COMPUT SCI, V9, P866, DOI 10.1016/j.procs.2012.04.093
   Prabhu Tarun, 2017, PROGRAMMING PERFORMA, P56
   Qing Zheng, 2018, SC18: International Conference for High Performance Computing, Networking, Storage and Analysis. Proceedings, P30, DOI 10.1109/SC.2018.00006
   Quintanas-Corominas A, 2018, EUR J MECH A-SOLID, V71, P278, DOI 10.1016/j.euromechsol.2018.03.021
   Racah E, 2017, ADV NEUR IN, V30
   Rew R., 2006, 22 INT C INTERACTIVE
   Ross Robert, 2018, TECHNICAL REPORT
   Roth P.C., 2007, PDSW 07 P 2 INT WORK, P50, DOI DOI 10.1145/1374596.1374609
   RothWendelberger Joanne, 2017, TECHNICAL REPORT
   Rudyy O, 2019, INT PARALL DISTRIB P, P567, DOI 10.1109/IPDPS.2019.00066
   Sevilla Michael A., 2018, 10 USENIX WORKSHOP H
   Silver D., 1993, Journal of Visual Communication and Image Representation, V4, P46, DOI 10.1006/jvci.1993.1005
   Sim Hyogi, 2017, INT C HIGH PERFORMAN
   Skillman S. W., 2014, arXiv
   SQLite, 2022, SQLite
   Starfish, 2017, STARFISH
   Stockinger Kurt, 2005, IEEE VISUALIZATION C
   Tang HJ, 2017, IEEE INT C CL COMP, P359, DOI 10.1109/CLUSTER.2017.53
   Tauheed F, 2012, PROC INT CONF DATA, P941, DOI 10.1109/ICDE.2012.56
   Thakar AR, 2008, COMPUT SCI ENG, V10, P30, DOI 10.1109/MCSE.2008.15
   Thakur R, 1999, FRONTIERS '99 - THE SEVENTH SYMPOSIUM ON THE FRONTIERS OF MASSIVELY PARALLEL COMPUTATION, PROCEEDINGS, P182, DOI 10.1109/FMPC.1999.750599
   Thompson D., 2011, Proceedings of the IEEE Symposium on Large Data Analysis and Visualization (LDAV 2011), P23, DOI 10.1109/LDAV.2011.6092313
   Top500, 2022, TOP500 LISTS
   Tull Craig E., 2013, SPOT SUITE PROJECT
   Tzeng Fan-Yin, 2005, ACM IEEE C SUPERCOMP
   U.S. Department of Energy, 2019, US DEP ENERGY INTEL
   Ullrich PA, 2021, GEOSCI MODEL DEV, V14, P5023, DOI 10.5194/gmd-14-5023-2021
   Ullrich PA, 2017, GEOSCI MODEL DEV, V10, P1069, DOI 10.5194/gmd-10-1069-2017
   Ulmer C, 2018, PROCEEDINGS OF THE ACM WORKSHOP ON SCIENTIFIC CLOUD COMPUTING (SCIENCECLOUD'18), DOI 10.1145/3217880.3217888
   VIC, 2022, VIC, DOI [10.5281/zenodo.5781377.svg, DOI 10.5281/ZENODO.5781377.SVG]
   Wang CL, 2008, IEEE T VIS COMPUT GR, V14, P1547, DOI 10.1109/TVCG.2008.140
   Wang CL, 2006, IEEE T VIS COMPUT GR, V12, P1029, DOI 10.1109/TVCG.2006.159
   Wang CL, 2007, IEEE T VIS COMPUT GR, V13, P122, DOI 10.1109/TVCG.2007.15
   Weil SA, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P307
   Welch B, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P17
   Welch B, 2013, IEEE S MASS STOR SYS
   Widanagamaachchi W., 2012, 2012 IEEE Symposium on Large Data Analysis and Visualization (LDAV 2012), P9, DOI 10.1109/LDAV.2012.6378962
   Wu K., 2009, SCI DISCOVERY ADV
   Wu T, 2017, SC'17: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS, DOI 10.1145/3126908.3126934
   Yu HF, 2010, IEEE COMPUT GRAPH, V30, P45, DOI 10.1109/MCG.2010.55
   Yu Su, 2012, 2012 41st International Conference on Parallel Processing (ICPP 2012), P249, DOI 10.1109/ICPP.2012.33
   Zaspel P., 2011, Proceedings of the 2011 IEEE International Conference on Network/Cloud Computing and Applications (NCCA 2011), P73, DOI 10.1109/NCCA.2011.19
   Zhang Wei, 2019, INT C HIGH PERFORMAN, P5
   Zhao M, 2009, J CLIMATE, V22, P6653, DOI 10.1175/2009JCLI3049.1
   Zheng Qing, 2015, P 10 PAR DAT STOR WO, P1
   Zhenhuan Gong, 2012, 2012 41st International Conference on Parallel Processing (ICPP 2012), P239, DOI 10.1109/ICPP.2012.39
   Zhou B, 2018, COMPUT GRAPH FORUM, V37, P37, DOI 10.1111/cgf.13399
NR 144
TC 0
Z9 0
U1 1
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2022
VL 18
IS 4
AR 34
DI 10.1145/3523698
PG 49
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7F8OU
UT WOS:000902100700007
DA 2024-07-18
ER

PT J
AU Tai, A
   Smolyar, I
   Wei, M
   Tsafrir, D
AF Tai, Amy
   Smolyar, Igor
   Wei, Michael
   Tsafrir, Dan
TI Optimizing Storage Performance with Calibrated Interrupts
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE NVMe; hardware interrupts; calibrating interrupts
AB After request completion, an I/O device must decide whether to minimize latency by immediately firing an interrupt or to optimize for throughput by delaying the interrupt, anticipating that more requests will complete soon and help amortize the interrupt cost. Devices employ adaptive interrupt coalescing heuristics that try to balance between these opposing goals. Unfortunately, because devices lack the semantic information about which I/O requests are latency-sensitive, these heuristics can sometimes lead to disastrous results.
   Instead, we propose addressing the root cause of the heuristics problem by allowing software to explicitly specify to the device if submitted requests are latency-sensitive. The device then "calibrates" its interrupts to completions of latency-sensitive requests. We focus on NVMe storage devices and show that it is natural to express these semantics in the kernel and the application and only requires a modest two-bit change to the device interface. Calibrated interrupts increase throughput by up to 35%, reduce CPU consumption by as much as 30%, and achieve up to 37% lower latency when interrupts are coalesced.
C1 [Tai, Amy; Wei, Michael] VMware Res, 3401 Hillview Ave, Palo Alto, CA 94304 USA.
   [Smolyar, Igor; Tsafrir, Dan] Technion Israel Inst Technol, Fac Comp Sci, IL-3200003 Haifa, Israel.
C3 VMware, Inc.; Technion Israel Institute of Technology
RP Tai, A (corresponding author), VMware Res, 3401 Hillview Ave, Palo Alto, CA 94304 USA.
EM amy.tai.2009@gmail.com; xrevolver@gmail.com
OI Tai, Amy/0000-0001-6725-9189
CR Ahmad Irfan, 2011, USENIX ANN TECHN C U
   Alizadeh Mohammad, 2012, P 9 S NETWORKED SYST, P253
   [Anonymous], 1998, RFC 2401
   [Anonymous], 2014, USENIX Annual Technical Conference
   [Anonymous], 2010, P USENIX S OP SYST D
   [Anonymous], 2021, Linux Kernel Documentation
   Axboe Jens, 2016, LINUX KERNEL MAILING
   Axboe Jens, The Flexible I/O tester
   Begunkov Pavel, 2019, LINUX KERNEL MAILING
   Belay A., 2014, P 11 USENIX C OP SYS, P49
   Bjorling Matias, 2013, ACM INT C PROCEEDING
   Bush Keith, 2019, LINUX NVME MAILING L
   Cisco Systems Inc., 2021, CISC ASA SER COMM RE
   Conway Alexander, 2020, USENIX ANN TECHN C U
   Cooper Brian F., 2010, P 1 ACM S CLOUD COMP, P143, DOI [DOI 10.1145/1807128.1807152, 10.1145/1807128.1807152]
   Corbet J., DRIVER PORTING NETWO
   Corbet Jonathan, BATCH PROCESSING NET
   Fall Kevin R, 2011, TCP IP ILLUSTRATED, V1
   Gont Fernando, 2011, 768 RFC
   Gont Fernando, 2012, SURVEY SECURITY HARD
   Gruss D, 2017, LECT NOTES COMPUT SC, V10379, P161, DOI 10.1007/978-3-319-62105-0_11
   Hufferd John, 2013, FIBRE CHANNEL ETHERN
   Intel, 2014, DAT PLAN DEV KIT
   Intel, 2012, INT DAT DIR I O TECH
   Intel Corporation, INT OPT TECHN DAT CT
   Intel Corporation, 2017, INT OPT SSD 900P SER
   Intel Corporation, 2014, INT SSD DC P3700 SER
   Intel Corporation, 2019, INT SSD DC P4618 SER
   Intel Corporation, 2018, INT OPT SSD DC P5800
   Intel Corporation, 2017, INT OPT SSD DC P4800
   Intel Corporation, 2014, INT ETH CONV NETW AD
   Intel Corporation, INT OPT SSD DC D4800
   Jones R., 1995, Netperf: A network performance benchmark
   Kapoor R, 2012, P ACM SOCC SAN JOS C, DOI [10.1145/2391229.2391238, DOI 10.1145/2391229.2391238]
   Kim Byungseok, 2017, 9 USENIX WORKSH HOT
   Kim Hyeong-Jun, 2016, 8 USENIX WORKSH HOT, P41
   Kim S, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P345
   Kivity Avi, 2018, WASTED PROCESSING TI
   Koh Sungjoon, 2019, IEEE INT S WORKL CHA
   Koh Sungjoon, 2018, USENIX WORKSH HOT TO
   Kourtis K, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P1
   Kozierok Charles M., TCP IP GUIDE
   Le Moal Damien, 2017, LIN STOR FIL C VAULT
   Lee G, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P603
   Lepers B, 2019, PROCEEDINGS OF THE TWENTY-SEVENTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '19), P447, DOI 10.1145/3341301.3359628
   Leverich J., 2014, ACM EUROSYS
   Long Li, 2019, LINUX KERNEL MAILING
   Mellanox Technologies, 2020, EN LARG REC OFFL LRO
   Mellanox Technologies, 2018, MELL CONNECTX 5 VPI
   Merriam-Webster, 2020, CALIBRATE
   Microsoft Corporation, 2019, MICR DOC OPT PERF LS
   Microsoft Corporation, 2018, OOB DAT TCP
   Ming Lei, 2019, LINUX NVME MAILING L
   Mogul JC, 1997, ACM T COMPUT SYST, V15, P217, DOI 10.1145/263326.263335
   Nayak Rikin J., 2017, INT C INF COMM TECHN
   Palo Alto Networks, 2018, PRES TCP URG FLAG PO
   Papagiannis A, 2016, PROCEEDINGS OF USENIX ATC '16: 2016 USENIX ANNUAL TECHNICAL CONFERENCE, P537
   Peter Simon, 2014, P 11 USENIX C OP SYS
   Pismenny Boris, ACM INT C ARCH SUPP
   Samsung, 2017, SAMS SSD 850 PRO
   SPDK, Storage Performance Development Kit
   Spurgeon C.E., 2000, Ethernet: The Definitive Guide
   Swanson Steven, 2013, COMPUTER
   Tallis Billy, 2017, INTEL OPTANE SSD DC
   The RoCE Initiative, ROCE IS RDMA CONV ET
   Tsafrir D., 2007, EXPT COMPUTER SCI EX, P3
   Uffenbeck John E., 1997, 80X86 FAMILY DESIGN
   Western Digital Corporation, ULTR DC SN200
   Xu Qiumin, 2015, ACM INT SYST STOR C
   Yang Jisoo, 2012, P 10 USENIX C FIL ST, P3
   Yang Ting., 2008, OSDI, P73
   Yates Tom, IMPROVEMENTS BLOCK L
   Yoon Young, 2001, PHRACK MAGAZINE
   Yu YJ, 2014, ACM T COMPUT SYST, V32, DOI 10.1145/2619092
   Zhang J, 2020, PROCEEDINGS OF THE 18TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P121
   Zhang J, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P477
NR 76
TC 3
Z9 3
U1 2
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD FEB
PY 2022
VL 18
IS 1
AR 3
DI 10.1145/3505139
PG 32
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZN5OY
UT WOS:000765084700007
DA 2024-07-18
ER

PT J
AU Ghoshal, D
   Ramakrishnan, L
AF Ghoshal, Devarshi
   Ramakrishnan, Lavanya
TI Programming Abstractions for Managing Workflows on Tiered Storage
   Systems
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Data management; scientific workflows; multi-tiered storage; burst
   buffer
AB Scientific workflows in High Performance Computing (HPC) environments are processing large amounts of data. The storage hierarchy on HPC systems is getting deeper, driven by new technologies (NVRAMs, SSDs, etc.) There is a need for new programming abstractions that allow users to seamlessly manage data at the workflow level on multi-tiered storage systems, and provide optimal workflow performance and use of storage resources. In previous work, we introduced a software architecture Managing Data on Tiered Storage for Scientific Workflows (MaDaTS) that used a Virtual Data Space (VDS) abstraction to hide the complexities of the underlying storage system while allowing users to control data management strategies. In this article, we detail the data-centric programming abstractions that allow users to manage a workflow around its data on the storage layer. The programming abstractions simplify data management for scientific workflows on multi-tiered storage systems, without affecting workflow performance or storage capacity. We measure the overheads and effectiveness introduced by the programming abstractions of MaDaTS. Our results show that these abstractions can optimally use the storage capacity in lesser capacity storage tiers, and simplify data management without adding any performance overheads.
C1 [Ghoshal, Devarshi; Ramakrishnan, Lavanya] Lawrence Berkeley Natl Lab, 1 Cyclotron Rd, Berkeley, CA 94720 USA.
C3 United States Department of Energy (DOE); Lawrence Berkeley National
   Laboratory
RP Ghoshal, D (corresponding author), Lawrence Berkeley Natl Lab, 1 Cyclotron Rd, Berkeley, CA 94720 USA.
EM dghoshal@lbl.gov; lramakrishnan@lbl.gov
FU U.S. Department of Energy, Office of Science and Office of Advanced
   Scientific Computing Research (ASCR) [DE-AC02-05CH11231]
FX This work and the resources at NERSC are supported by the U.S.
   Department of Energy, Office of Science and Office of Advanced
   Scientific Computing Research (ASCR) under Contract No.
   DE-AC02-05CH11231.
CR Akram Asif, 2006, 2006 10 IEEE INT ENT
   [Anonymous], 2018, COUNT LINES CODE
   [Anonymous], 2013, TITAN
   [Anonymous], 2016, CORI
   [Anonymous], 2015, N CAROLINA FLOODPLAI
   Chen Chao, 2016, 2016 IEEE INT C NETW
   Daley Christopher, 2016, 11 WORKSH WORKFL SUP
   Deelman E., 2005, Scientific Programming, V13, P219
   Deelman Ewa, 2014, WORKFLOWS ESCIENCE S
   Docan C, 2012, CLUSTER COMPUT, V15, P163, DOI 10.1007/s10586-011-0162-y
   Fedak G, 2009, J NETW COMPUT APPL, V32, P961, DOI 10.1016/j.jnca.2009.04.002
   Foster I, 2002, 14TH INTERNATIONAL CONFERENCE ON SCIENTIFIC AND STATISTICAL DATABASE MANAGEMENT, PROCEEDINGS, P37, DOI 10.1109/SSDM.2002.1029704
   Franklin M, 2005, SIGMOD RECORD, V34, P27, DOI 10.1145/1107499.1107502
   Ghoshal Devarshi, 2017, ACM S HIGH PERF PAR
   Hendrix V, 2016, IEEE ACM INT SYMP, P146, DOI 10.1109/CCGrid.2016.54
   Herbein Stephen., 2016, P 25 ACM INT S HIGH
   Iliadis I, 2015, I S MOD ANAL SIM COM, P218, DOI 10.1109/MASCOTS.2015.41
   Jin C., 2008, Adaptive io system adios
   Liu David T., 2004, 30 INT C VER LARG DA
   Liu N, 2012, IEEE S MASS STOR SYS
   Meng XD, 2014, INT C PAR DISTRIB SY, P376, DOI 10.1109/PADSW.2014.7097831
   MichaelWilde MihaelHategan, 2011, PARALLEL COMPUT, V37, P9
   Monti HM, 2013, IEEE T PARALL DISTR, V24, P1841, DOI 10.1109/TPDS.2012.279
   Prabhakar R, 2011, INT CON DISTR COMP S, P1, DOI 10.1109/ICDCS.2011.33
   Romanus Melissa, 2016, P ACMINTERNATIONALWO
   Shin W, 2019, INT PARALL DISTRIB P, P511, DOI 10.1109/IPDPS.2019.00061
   Sim Hyogi., 2015, High Performance Computing, Networking, Storage and Analysis, 2015 SC-International Conference for, P1
   Vairavanathan E., 2012, Proceedings of the 2012 12th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGrid 2012), P326, DOI 10.1109/CCGrid.2012.109
   Wang Teng, 2015, DEV BURST BUFFER SYS
   Zaharia M., 2012, 9 USENIX S NETWORKED
   Zhang G, 2010, IEEE S MASS STOR SYS
   Zhang Zhe, 2007, 2007 ACMIEEE C SUP S
   Zheng F, 2013, INT PARALL DISTRIB P, P320, DOI 10.1109/IPDPS.2013.46
NR 33
TC 0
Z9 0
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2021
VL 17
IS 4
AR 29
DI 10.1145/3457119
PG 21
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YY2GY
UT WOS:000754612700005
OA Green Published, Bronze
DA 2024-07-18
ER

PT J
AU Hong, D
   Ha, K
   Ko, M
   Chun, M
   Kim, Y
   Lee, S
   Kim, J
AF Hong, Duwon
   Ha, Keonsoo
   Ko, Minseok
   Chun, Myoungjun
   Kim, Yoona
   Lee, Sungjin
   Kim, Jihong
TI Reparo: A Fast RAID Recovery Scheme for Ultra-large SSDs
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article; Proceedings Paper
CT 14th USENIX Symposium on Operating Systems Design and Implementation
   (OSDI)
CY NOV 04-06, 2020
CL ELECTR NETWORK
SP USENIX Assoc, Alibaba Grp, Amazon, ANT Grp, Alipay, Apple, ByteDance, Facebook, Google, Microsoft, VMware, Oracle, Two Sigma, ACM Queue, DMTF, FreeBSD Fdn
DE Die failure; ultra-large SSD; RAID; storage system
ID HIGH-PERFORMANCE
AB Arecent ultra-large SSD (e.g., a 32-TB SSD) providesmany benefits in building cost-efficient enterprise storage systems. Owing to its large capacity, however, when such SSDs fail in a RAID storage system, a long rebuild overhead is inevitable for RAID reconstruction that requires a huge amount of data copies among SSDs. Motivated by modern SSD failure characteristics, we propose a new recovery scheme, called reparo, for a RAID storage system with ultra-large SSDs. Unlike existing RAID recovery schemes, reparo repairs a failed SSD at the NAND die granularity without replacing it with a new SSD, thus avoiding most of the inter-SSD data copies during a RAID recovery step. When a NAND die of an SSD fails, reparo exploits a multi-core processor of the SSD controller in identifying failed LBAs from the failed NAND die and recovering data from the failed LBAs. Furthermore, reparo ensures no negative post-recovery impact on the performance and lifetime of the repaired SSD. Experimental results using 32-TB enterprise SSDs show that reparo can recover from a NAND die failure about 57 times faster than the existing rebuild method while little degradation on the SSD performance and lifetime is observed after recovery.
C1 [Hong, Duwon; Chun, Myoungjun; Kim, Yoona; Kim, Jihong] Seoul Natl Univ, 1 Gwanak Ro, Seoul 08826, South Korea.
   [Ha, Keonsoo; Ko, Minseok] Samsung Elect, 1 Samsungjeonja Ro, Hwaseong Si 18448, Gyeonggi Do, South Korea.
   [Lee, Sungjin] Daegu Gyeongbuk Inst Sci & Technol DGIST, 333 Techno Jungang Daero, Daegu 42988, South Korea.
C3 Seoul National University (SNU); Samsung; Daegu Gyeongbuk Institute of
   Science & Technology (DGIST)
RP Kim, J (corresponding author), Seoul Natl Univ, 1 Gwanak Ro, Seoul 08826, South Korea.
EM duwon.hong@davinci.snu.ac.kr; keonsoo.ha@samsung.com;
   minseok2.ko@samsung.com; mjchun@davinci.snu.ac.kr;
   yoonakim@davinci.snu.ac.kr; sungjin.lee@dgist.ac.kr;
   jihong@davinci.snu.ac.kr
OI Hong, Duwon/0000-0002-6954-6787; Lee, Sungjin/0000-0002-9753-2286
FU Samsung Research Funding Incubation Center of Samsung Electronics,
   Republic of Korea [SRFC-IT2002-06]
FX This work was supported by Samsung Research Funding Incubation Center of
   Samsung Electronics, Republic of Korea under Project Number
   SRFC-IT2002-06. The ICT at Seoul National University provided research
   facilities for this study.
CR Alter J, 2019, PROCEEDINGS OF SC19: THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS, DOI 10.1145/3295500.3356172
   Alvarez GA, 1998, CONF PROC INT SYMP C, P109, DOI 10.1109/ISCA.1998.694767
   [Anonymous], 2020, OVER PROVISIONING
   [Anonymous], 1988, P ACM SIGMOD INT C M
   Broadcom, 2018, 12GB S MEGARAID TRIM
   Cai Y, 2017, P IEEE, V105, P1666, DOI 10.1109/JPROC.2017.2713127
   Cai Y, 2012, DES AUT TEST EUROPE, P521
   CHEN PM, 1994, ACM COMPUT SURV, V26, P145, DOI 10.1145/176979.176981
   Cho Sangyeun, 2014, P WORKSH HOT TOP STO
   Hansen Ulrich, 2012, The ssd endurance race: Who's got the write stuff?
   HOLLAND M, 1992, SIGPLAN NOTICES, V27, P23, DOI 10.1145/143371.143383
   Im S, 2011, IEEE T COMPUT, V60, P80, DOI 10.1109/TC.2010.197
   Jens Axboe, 2020, FIO
   Kim Bryan S, 2019, P USENIX C FIL STOR
   Kim Eden, 2014, ENTERPRISE APPL CREA
   Kim J, 2016, IEEE T COMPUT, V65, P1116, DOI 10.1109/TC.2014.2375179
   Kim M, 2018, PR GR LAK SYMP VLSI, P255, DOI 10.1145/3194554.3194591
   Kim T., 2019, P USENIX C FIL STOR
   Kim Y, 2011, IEEE S MASS STOR SYS
   Lee Sehwan, 2011, P ACM S APPL COMP
   Lee Yangsup, 2009, P INT C HARDW SOFTW
   Maneas Stathis, 2020, P USENIX C FIL STOR
   Micron, 2011, TN-29-59: Bad block management
   MUNTZ RR, 1990, VERY LARGE DATA BASES, P162
   Park H, 2015, IEEE S MASS STOR SYS
   Samsung, 2014, CISC VIS NETW IND GL
   Schroeder B, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 5TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES ( FAST '07), P1
   Schroeder Bianca, 2016, P USENIX C FIL STOR
   Seagate Technology, 2011, RED RAID REC DOWNT
   Shadley Scott, 2011, SSD RAIN
   Siu-Cheung Chau, 2002, Cluster Computing, V5, P97, DOI 10.1023/A:1012705007864
   Tai Ying Y, 2016, P FLASH MEM SUMM
   Ulink, 2019, DRIVEMASTER
   Wan JG, 2014, IEEE T PARALL DISTR, V25, P1638, DOI 10.1109/TPDS.2013.225
   Wang Shunzhuo, 2019, P DES AUT C
   Yang J, 1999, P A REL MAI, P403, DOI 10.1109/RAMS.1999.744151
   Yi Qin, 2012, 2012 IEEE 7th International Conference on Networking, Architecture, and Storage (NAS), P293, DOI 10.1109/NAS.2012.40
   Zhang GY, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P279
   Zheng Mai, 2013, P USENIX C FIL STOR
NR 39
TC 2
Z9 2
U1 1
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2021
VL 17
IS 3
AR 21
DI 10.1145/3450977
PG 24
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA YY2GJ
UT WOS:000754611000006
DA 2024-07-18
ER

PT J
AU Wang, H
   Zhang, JW
   Huang, P
   Yi, XB
   Cheng, B
   Zhou, K
AF Wang, Hua
   Zhang, Jiawei
   Huang, Ping
   Yi, Xinbo
   Cheng, Bin
   Zhou, Ke
TI Cache What You Need to Cache: Reducing Write Traffic in Cloud Cache via
   "One-Time-Access-Exclusion" Policy
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE SSD; photo caching; machine learning; social network
ID REPLACEMENT; PERFORMANCE; ALGORITHMS
AB The SSD has been playing a significantly important role in caching systems due to its high performance-to-cost ratio. Since the cache space is typically much smaller than that of the backend storage by one order of magnitude or even more, write density (defined as writes per unit time and space) of the SSD cache is therefore much more intensive than that of HDD storage, which brings about tremendous challenges to the SSD's lifetime. Meanwhile, under social network workloads, quite a lot writes to the SSD cache are unnecessary. For example, our study on Tencent's photo caching shows that about 61% of total photos are accessed only once, whereas they are still swapped in and out of the cache. Therefore, if we can predict these kinds of photos proactively and prevent them from entering the cache, we can eliminate unnecessary SSD cache writes and improve cache space utilization.
   To cope with the challenge, we put forward a "one-time-access criteria" that is applied to the cache space and further propose a "one-time-access-exclusion" policy. Based on these two techniques, we design a prediction-based classifier to facilitate the policy. Unlike the state-of-the-art history-based predictions, our prediction is non-history oriented, which is challenging to achieve good prediction accuracy. To address this issue, we integrate a decision tree into the classifier, extract social-related information as classifying features, and apply cost-sensitive learning to improve classification precision. Due to these techniques, we attain a prediction accuracy greater than 80%. Experimental results show that the one-time-access-exclusion approach results in outstanding cache performance in most aspects. Take LRU, for instance: applying our approach improves the hit rate by 4.4%, decreases the cache writes by 56.8%, and cuts the average access latency by 5.5%.
C1 [Wang, Hua; Zhang, Jiawei; Yi, Xinbo; Zhou, Ke] Huazhong Univ Sci & Technol, Wuhan Natl Lab Optoelect, Wuhan, Peoples R China.
   [Wang, Hua; Zhang, Jiawei; Yi, Xinbo; Zhou, Ke] Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan, Peoples R China.
   [Huang, Ping] Temple Univ, Dept Comp & Informat Sci, Philadelphia, PA 19122 USA.
   [Huang, Ping] Huazhong Univ Sci & Technol, Wuhan, Peoples R China.
   [Cheng, Bin] Shenzhen Tencent Comp Syst Co Ltd, Shenzhen, Peoples R China.
C3 Huazhong University of Science & Technology; Huazhong University of
   Science & Technology; Pennsylvania Commonwealth System of Higher
   Education (PCSHE); Temple University; Huazhong University of Science &
   Technology; Tencent
RP Zhou, K (corresponding author), Huazhong Univ Sci & Technol, Wuhan Natl Lab Optoelect, Wuhan, Peoples R China.; Zhou, K (corresponding author), Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan, Peoples R China.
EM hwang@hust.edu.cn; jiaweizhang@hust.edu.cn; templestorager@temple.edu;
   orglanss@hust.edu.cn; bencheng@tencent.com; zhke@hust.edu.cn
OI Zhang, Jiawei/0000-0001-8911-0422
FU Innovation Group Project of the National Natural Science Foundation of
   China [61821003]
FX This work was supported in part by the Innovation Group Project of the
   National Natural Science Foundation of China (no. 61821003).
CR Alpaydin E, 2014, ADAPT COMPUT MACH LE, P1
   [Anonymous], 2016, ACM T EMBEDDED COMPU
   Bai X, 2016, SIGIR'16: PROCEEDINGS OF THE 39TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P639, DOI 10.1145/2911451.2911513
   BELADY LA, 1966, IBM SYST J, V5, P78, DOI 10.1147/sj.52.0078
   Blewitt ME, 2008, NAT GENET, V40, P663, DOI 10.1038/ng.142
   Breslau L, 1999, IEEE INFOCOM SER, P126, DOI 10.1109/INFCOM.1999.749260
   Chen FH, 2015, DES AUT TEST EUROPE, P1209
   Cheng-Yang Fu, 2011, Proceedings 2011 Design, Automation & Test in Europe
   Cidon Asaf., 2015, PROC USENIX HOTCLOUD
   Crane R, 2008, P NATL ACAD SCI USA, V105, P15649, DOI 10.1073/pnas.0803685105
   Deng ZX, 2017, 50TH ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE (MICRO), P232, DOI 10.1145/3123939.3124548
   Eisenman A, 2019, PROCEEDINGS OF THE 16TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P65
   Elkan C., 2001, IJCAI 2001, V17, P973, DOI DOI 10.5555/1642194.1642224
   Fan X, 2016, MITOCHONDRIAL DNA B, V1, P16, DOI 10.1080/23802359.2015.1137800
   Gal E, 2005, ACM COMPUT SURV, V37, P138, DOI 10.1145/1089733.1089735
   Gelli F, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P907, DOI 10.1145/2733373.2806361
   Guo L, 2008, PODC'08: PROCEEDINGS OF THE 27TH ANNUAL ACM SYMPOSIUM ON PRINCIPLES OF DISTRIBUTED COMPUTING, P283, DOI 10.1145/1400751.1400789
   Huang P., 2014, P 2014 USENIX ANN TE, P489
   Huang P, 2016, PROC INT CONF PARAL, P169, DOI 10.1109/ICPP.2016.26
   Huang Q, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P167, DOI 10.1145/2517349.2522722
   Huang SB, 2016, HEAD FACE MED, V12, DOI 10.1186/s13005-015-0098-1
   Jiménez DA, 2017, 50TH ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE (MICRO), P436, DOI 10.1145/3123939.3123942
   Jimenez X., 2014, PROC USENIX C FILE S, P47
   KAREDLA R, 1994, COMPUTER, V27, P38, DOI 10.1109/2.268884
   Keramidas G, 2007, PR IEEE COMP DESIGN, P245, DOI 10.1109/ICCD.2007.4601909
   Kharbutli M, 2008, IEEE T COMPUT, V57, P433, DOI 10.1109/TC.2007.70816
   Khosla A, 2014, WWW'14: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P867, DOI 10.1145/2566486.2567996
   Liu R.-S., 2012, Proceedings of the 10th USENIX Conference on File and Storage Technologies, P11
   Megiddo N, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE 2ND USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST'03), P115
   Peled L, 2015, 2015 ACM/IEEE 42ND ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA), P285, DOI 10.1145/2749469.2749473
   Qiang Yang, 2001, KDD-2001. Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P473, DOI 10.1145/502512.502584
   Qureshi MK, 2009, CONF PROC INT SYMP C, P24, DOI 10.1145/1555815.1555760
   Samarasinghe R, 2005, AINA 2005: 19th International Conference on Advanced Information Networking and Applications, Vol 2, P325
   Shafiq M, 2016, EURASIAN J EDUC RES, P1
   Song Jiang, 2002, Performance Evaluation Review, V30, P31, DOI 10.1145/511399.511340
   Soundararajan Gokul., 2010, Proceedings of the 8th USENIX conference on File and storage technologies, FAST'10, P8
   Sundarrajan A, 2017, CONEXT'17: PROCEEDINGS OF THE 2017 THE 13TH INTERNATIONAL CONFERENCE ON EMERGING NETWORKING EXPERIMENTS AND TECHNOLOGIES, P55, DOI 10.1145/3143361.3143368
   Szabo G, 2010, COMMUN ACM, V53, P80, DOI 10.1145/1787234.1787254
   Tang LP, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P111
   Tang Linpeng, 2015, 13 USENIX C FIL STOR, P373
   Teran Elvira, 2016, Microarchitecture (MICRO), 2016 49th Annual IEEE/ACM International Symposium on, P1
   Wu Guanying, 2012, P 7 ACM EUR C COMP S, P253
   Yang YJ, 2016, MATH PROBL ENG, V2016, DOI 10.1155/2016/5725143
   Ye R, 2017, 2017 INTERNATIONAL CONFERENCE ON NETWORKING, ARCHITECTURE, AND STORAGE (NAS), P165
   Zhao QY, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1513, DOI 10.1145/2783258.2783401
   Zhou K, 2019, IEEE T PARALL DISTR, V30, P2033, DOI 10.1109/TPDS.2019.2902392
   Zhou K, 2018, PR IEEE COMP DESIGN, P569, DOI 10.1109/ICCD.2018.00091
   Zhou Ke, 2017, P 2017 IEEE 33 S MAS
NR 48
TC 6
Z9 6
U1 2
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2020
VL 16
IS 3
AR 18
DI 10.1145/3397766
PG 24
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1QZ
UT WOS:000583743600004
DA 2024-07-18
ER

PT J
AU Zhang, SL
   Roy, R
   Rumancik, L
   Wang, AIA
AF Zhang, Shuanglong
   Roy, Robert
   Rumancik, Leah
   Wang, An-I Andy
TI The Composite-File File System: Decoupling One-to-One Mapping of Files
   and Metadata for Better Performance
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; performance; file systems; metadata
AB The design and implementation of traditional file systems typically use the one-to-one mapping of logical files to their physical metadata representations. File system optimizations generally follow this rigid mapping and miss opportunities for an entire class of optimizations.
   We designed, implemented, and evaluated a composite-file file system, which allows many-to-one mappings of files to metadata. Through exploring different mapping strategies, our empirical evaluation shows up to a 27% performance improvement under web server and software development workloads, for both disks and SSDs. This result demonstrates that our approach of relaxing file-to-metadata mapping is promising.
C1 [Zhang, Shuanglong] Google, 1600 Amphitheatre Pkwy, Mountain View, CA 94043 USA.
   [Roy, Robert; Rumancik, Leah; Wang, An-I Andy] Florida State Univ, 253 Love Bldg,269 Love Bldg, Tallahassee, FL 32306 USA.
C3 Google Incorporated; State University System of Florida; Florida State
   University
RP Zhang, SL (corresponding author), Google, 1600 Amphitheatre Pkwy, Mountain View, CA 94043 USA.
EM shuanglong@google.com; roy@cs.fsu.edu; ler13f@my.fsu.edu;
   awang@cs.fsu.edu
RI Wang, Andy/HME-0514-2023; chen, wang/KGK-5932-2024; Wang, An-I
   Andy/F-7938-2011; wang, andy/GRJ-8724-2022
FU Florida State University; National Science Foundation [CNS-144387]
FX This work was sponsored by Florida State University and the National
   Science Foundation under grant CNS-144387. Any opinions, findings,
   conclusions, or recommendations expressed in this material are those of
   the authors and do not necessarily reflect the views of the Florida
   State University, the National Science Foundation, Google, or the U.S.
   government.
CR Abd-El-Malek Michael., 2005, FAST 05, P5
   Agrawal R., P 20 INT C VERY LARG
   Albrecht R., 2017, Web Performance: Cache Efficiency Exercise
   [Anonymous], 1991, The Art of Computer Systems Performance Analysis: Techniques for Experimental Design, Measurement, Simulation, and Modeling
   Beaver D., 2010, P USENIX OSDI
   BLOOM BH, 1970, COMMUN ACM, V13, P422, DOI 10.1145/362686.362692
   Chandrasekar S., 2013, International Conference on Computer Communication and Informatics (ICCCI), P1, DOI [10.1109/iccci.2013.6466147, DOI 10.1109/ICCCI.2013.6466147]
   Chidambaram V., 2012, Proceedings of the 10th USENIX conference on File and Storage Technologies, FAST 2012, San Jose, CA, USA, February 14-17, 2012, P9
   Ding XN, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 2007 USENIX ANNUAL TECHNICAL CONFERENCE, P261
   Dong ZW, 2010, INT CONF COMP SCI, P600, DOI 10.1109/ICCSIT.2010.5564627
   Edel NK, 2004, IEEE COMPUTER SOCIETY'S 12TH ANNUAL INTERNATIONAL SYMPOSIUM ON MODELING, ANALYSIS, AND SIMULATION OF COMPUTER AND TELECOMMUNICATIONS SYSTEMS - PROCEEDINGS, P596, DOI 10.1109/MASCOT.2004.1348317
   Ganger G. R., 1997, Proceedings of the USENIX 1997 Annual Technical Conference, P1
   Garrison John A., 2009, ACM Transaction on Storage, V5, DOI 10.1145/1502777.1502780
   Harter T, 2012, ACM T COMPUT SYST, V30, DOI 10.1145/2324876.2324878
   HEIDEMANN JS, 1994, ACM T COMPUT SYST, V12, P58, DOI 10.1145/174613.174616
   Jiang S, 2013, ACM T STORAGE, V9, DOI 10.1145/2508010
   Kroeger T. M., 2001, P USENIX 2001 ANN TE
   Li Z., 2004, P 3 USENIX C FIL STO
   McKusick M. K., 1990, P USENIX SUMM C
   MULLENDER SJ, 1984, SOFTWARE PRACT EXPER, V14, P365, DOI 10.1002/spe.4380140407
   Mulvey JM, 2007, QUANT FINANC, V7, P175, DOI 10.1080/14697680701198028
   PKWARE, 2018, ZIP FIL FORM SPEC
   Ren Kai., 2013, USENIX ANN TECHNICAL, P145
   Rodeh O, 2013, ACM T STORAGE, V9, DOI 10.1145/2501620.2501623
   Roselli D., 2000, P 2000 USENIX ANN TE
   Soundararajan Gokul., 2008, P 2008 USENIX ANN TE, P377
   Szeredi M., 2017, Filesystem in Userspace
   Terry M., 2017, DUPLICITY
   Vangoor B. K. R., 2017, P 15 USENIX C FIL TE
   Yu W., 2007, P 7 INT S CLUST COMP
NR 30
TC 2
Z9 2
U1 2
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD APR
PY 2020
VL 16
IS 1
AR 5
DI 10.1145/3366684
PG 18
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1QN
UT WOS:000583742400005
OA Bronze
DA 2024-07-18
ER

PT J
AU Xie, B
   Oral, S
   Zimmer, C
   Choi, JY
   Dillow, D
   Klasky, S
   Lofstead, J
   Podhorszki, N
   Chase, JS
AF Xie, Bing
   Oral, Sarp
   Zimmer, Christopher
   Choi, Jong Youl
   Dillow, David
   Klasky, Scott
   Lofstead, Jay
   Podhorszki, Norbert
   Chase, Jeffrey S.
TI Characterizing Output Bottlenecks of a Production Supercomputer:
   Analysis and Implications
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE High-performance computing; file systems; benchmarking
AB This article studies the I/O write behaviors of the Titan supercomputer and its Lustre parallel file stores under production load. The results can inform the design, deployment, and configuration of file systems along with the design of I/O software in the application, operating system, and adaptive I/O libraries.
   We propose a statistical benchmarking methodology to measure write performance across I/O configurations, hardware settings, and system conditions. Moreover, we introduce two relative measures to quantify the write-performance behaviors of hardware components under production load. In addition to designing experiments and benchmarking on Titan, we verify the experimental results on one real application and one real application I/O kernel, XGC and HACC IO, respectively. These two are representative and widely used to address the typical I/O behaviors of applications.
   In summary, we find that Titan's I/O system is variable across the machine at fine time scales. This variability has two major implications. First, stragglers lessen the benefit of coupled I/O parallelism (striping). Peak median output bandwidths are obtained with parallel writes to many independent files, with no striping or write sharing of files across clients (compute nodes). I/O parallelism is most effective when the application- or its I/O libraries-distributes the I/O load so that each target stores files for multiple clients and each client writes files on multiple targets in a balanced way with minimal contention. Second, our results suggest that the potential benefit of dynamic adaptation is limited. In particular, it is not fruitful to attempt to identify "good locations" in the machine or in the file system: component performance is driven by transient load conditions and past performance is not a useful predictor of future performance. For example, we do not observe diurnal load patterns that are predictable.
C1 [Xie, Bing; Oral, Sarp; Zimmer, Christopher; Choi, Jong Youl; Klasky, Scott; Podhorszki, Norbert] Oak Ridge Natl Lab, 1 Bethel Valley Rd, Oak Ridge, TN 37830 USA.
   [Dillow, David; Lofstead, Jay] Sandia Natl Labs, 1515 Eubank SE, Albuquerque, NM 87123 USA.
   [Chase, Jeffrey S.] Duke Univ, D306 Levine Sci Res Ctr, Durham, NC 27708 USA.
C3 United States Department of Energy (DOE); Oak Ridge National Laboratory;
   United States Department of Energy (DOE); Sandia National Laboratories;
   Duke University
RP Xie, B (corresponding author), Oak Ridge Natl Lab, 1 Bethel Valley Rd, Oak Ridge, TN 37830 USA.
EM xieb@ornl.gov; oralhs@ornl.gov; zimmercj@ornl.gov; choij@ornl.gov;
   dave@thedillows.org; klasky@ornl.gov; gflofst@sandia.gov;
   pnorbert@ornl.gov; chase@cs.duke.edu
RI Xie, Bing/GLR-4119-2022
OI Xie, Bing/0000-0002-5409-4378; Lofstead, Jay/0000-0002-4697-2919; Oral,
   Sarp/0000-0001-8745-7078; Podhorszki, Norbert/0000-0001-9647-542X;
   Klasky, Scott/0000-0003-3559-5772
FU Duke University; U.S. National Science Foundation [CNS-1245997]; Office
   of Science of the Department of Energy [DE-AC05-00OR22725]; U.S.
   Department of Energy's National Nuclear Security Administration
   [DE-NA0003525]
FX Bing Xie conducted much of this research as a graduate student at Duke
   University, with support from Duke University and also from the U.S.
   National Science Foundation under Grant No. CNS-1245997. The work used
   resources of the Oak Ridge Leadership Computing Facility, located in the
   National Center for Computational Sciences at the Oak Ridge National
   Laboratory, which is supported by the Office of Science of the
   Department of Energy under Contract No. DE-AC05-00OR22725. The work also
   used resources of Sandia National Laboratories. Sandia National
   Laboratories is a multi-mission laboratory managed and operated by
   National Technology and Engineering Solutions of Sandia, LLC, a wholly
   owned subsidiary of Honeywell International, Inc., for the U.S.
   Department of Energy's National Nuclear Security Administration under
   contract No. DE-NA0003525.
CR [Anonymous], 2015, INT S HIGH PERFORMAN
   Carns P, 2011, ACM T STORAGE, V7, DOI 10.1145/2027066.2027068
   Carns P, 2009, INT PARALL DISTRIB P, P524, DOI 10.1079/9781845933975.0001
   Chacón L, 2004, COMPUT PHYS COMMUN, V163, P143, DOI 10.1016/j.cpc.2004.08.005
   Chang CS, 2008, PHYS PLASMAS, V15, DOI 10.1063/1.2937116
   Chen J. H., 2009, Computational Science and Discovery, V2, DOI 10.1088/1749-4699/2/1/015001
   Chen YP, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P43
   Cui Y., 2010, SC 10, P1
   Dillow David A., 2011, P 30 IEEE INT PERF C, P21
   Dorier M, 2014, INT CONF HIGH PERFOR, P623, DOI 10.1109/SC.2014.56
   Ezell M., 2014, P CRAY USER GROUP C, P1
   Feiyi Wang Sarp, 2009, TM2009 ORNL, V117, P1
   Kim Y. H., 2010, 2010 IEEE 37th International Conference on Plasma Sciences (ICOPS 2010), DOI 10.1109/PLASMA.2010.5534009
   Kim Y, 2015, J SUPERCOMPUT, V71, P761, DOI 10.1007/s11227-014-1321-8
   Klasky S., 2003, Proceedings of the 2003 ACM/IEEE conference on Supercomputing, P24
   KRONENBERG NP, 1986, ACM T COMPUT SYST, V4, P130, DOI 10.1145/214419.214421
   Ku S, 2006, J PHYS CONF SER, V46, P87, DOI 10.1088/1742-6596/46/1/012
   Kunkel J, 2015, LECT NOTES COMPUT SC, V9137, P257, DOI 10.1007/978-3-319-20119-1_19
   Lang S., 2009, P ACMIEEE INT C HIGH, P40
   Liu Q, 2014, CONCURR COMP-PRACT E, V26, P1453, DOI 10.1002/cpe.3125
   Lofstead J., 2010, Proceedings of the 2010 ACM/IEEE International Conference for High Performance Computing, Networking, Storage and Analysis, SC '10, DOI [DOI 10.1109/SC.2010.32, 10.1109/SC.2010.32.]
   Lofstead J, 2009, INT PARALL DISTRIB P, P778
   Madireddy S, 2018, LECT NOTES COMPUT SC, V10876, P184, DOI 10.1007/978-3-319-92040-5_10
   McKenna R, 2016, IEEE INT C CL COMP, P255, DOI 10.1109/CLUSTER.2016.58
   Nowark David A., 1999, OAK RIDG INT WORKSH, P1
   Oral S., 2010, P 8 USENIX C FILE ST, P143
   Park IY, 2017, 2017 IEEE INTERNATIONAL SYMPOSIUM ON ELECTROMAGNETIC COMPATIBILITY & SIGNAL/POWER INTEGRITY (EMCSI), P181, DOI 10.1109/ISEMC.2017.8077863
   Shan H., 2007, P CRAY USER GROUP M, P1
   Shan H., 2008, P ACMIEEE INT C HIGH, P42
   Shipman G., 2012, P CRAY USER GROUP C, P1
   Shipman G., 2009, P CRAY USER GROUP M, P1
   Thakur R, 1999, FRONTIERS '99 - THE SEVENTH SYMPOSIUM ON THE FRONTIERS OF MASSIVELY PARALLEL COMPUTATION, PROCEEDINGS, P182, DOI 10.1109/FMPC.1999.750599
   Tian Y, 2011, IEEE INT C CL COMP, P93, DOI 10.1109/CLUSTER.2011.18
   Uselton A., 2010, P 24 IEEE INT PARALL, P1, DOI DOI 10.1109/IPDPS.2010.5470424
   Wan LP, 2017, 2017 19TH IEEE INTERNATIONAL CONFERENCE ON HIGH PERFORMANCE COMPUTING AND COMMUNICATIONS (HPCC) / 2017 15TH IEEE INTERNATIONAL CONFERENCE ON SMART CITY (SMARTCITY) / 2017 3RD IEEE INTERNATIONAL CONFERENCE ON DATA SCIENCE AND SYSTEMS (DSS), P1, DOI 10.1109/HPCC-SmartCity-DSS.2017.1
   Weil SA, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P307
   Xie Bing, 2017, HPC I O DAT CTR WORK, P1
   Xie Bing, 2017, THESIS
   Xie Bing, 2012, HIGH PERF COMP NETW, P1, DOI DOI 10.1111/J.1369-7625.2012.00804.X]
NR 39
TC 7
Z9 8
U1 1
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD FEB
PY 2020
VL 15
IS 4
AR 26
DI 10.1145/3335205
PG 39
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NG7LP
UT WOS:000564162600005
OA hybrid
DA 2024-07-18
ER

PT J
AU Yao, T
   Wan, JG
   Huang, P
   He, XB
   Wu, F
   Xie, CS
AF Yao, Ting
   Wan, Jiguang
   Huang, Ping
   He, Xubin
   Wu, Fei
   Xie, Changsheng
TI Building Efficient Key-Value Stores via a Lightweight Compaction Tree
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Lightweight compaction; LSM-tree; write amplification; SMR drives
AB Log-Structure Merge tree (LSM-tree) has been one of the mainstream indexes in key-value systems supporting a variety of write-intensive Internet applications in today's data centers. However, the performance of LSM-tree is seriously hampered by constantly occurring compaction procedures, which incur significant write amplification and degrade the write throughput. To alleviate the performance degradation caused by compactions, we introduce a lightweight compaction tree (LWC-tree), a variant of LSM-tree index optimized for minimizing the write amplification and maximizing the system throughput. The lightweight compaction drastically decreases write amplification by appending data in a table and only merging the metadata that have much smaller size. Using our proposed LWC-tree, we have implemented three key-value LWC-stores on different storage mediums including Shingled Magnetic Recording (SMR) drives, Solid State Drives (SSD), and conventional Hard Disk Drives (HDDs). The LWC-store is particularly optimized for SMR drives, as it eliminates the multiplicative I/O amplification from both LSM-trees and SMR drives. Due to the lightweight compaction procedure, LWC-store reduces the write amplification by a factor of up to 5x compared to the popular LevelDB key-value store. Moreover, the random write throughput of the LWC-tree on SMR drives is significantly improved by up to 467% even compared with LevelDB on conventional HDDs. Furthermore, LWC-tree has wide applicability and delivers impressive performance improvement in various conditions, including different storage mediums (i.e., SMR, HDD, SSD) and various value sizes and access patterns (i.e., uniform and Zipfian).
C1 [Yao, Ting; Wan, Jiguang; Wu, Fei; Xie, Changsheng] Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan Natl Lab Optoelect, Wuhan, Hubei, Peoples R China.
   [Huang, Ping; He, Xubin] Temple Univ, Dept Comp & Informat Sci, Philadelphia, PA 19122 USA.
C3 Huazhong University of Science & Technology; Pennsylvania Commonwealth
   System of Higher Education (PCSHE); Temple University
RP Yao, T; Wu, F (corresponding author), Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan Natl Lab Optoelect, Wuhan, Hubei, Peoples R China.
EM tingyao@hust.edu.cn; jgwan@hust.edu.cn; templestorager@temple.edu;
   xubin.he@temple.edu; wufei@hust.edu.cn; cs_xie@hust.edu.cn
FU National Natural Science Foundation of China [61472152, 61300047,
   61432007, 61572209]; Fundamental Research Funds for the Central
   Universities [HUST:2015QN069]; 111 Project [B07038]; WNLO; Key
   Laboratory of Data Storage System, Ministry of Education; U.S. National
   Science Foundation (NSF) [CNS-1702474, CNS-1700719]
FX A preliminary version of this work was presented at the 33rd
   International Conference on Massive Storage Systems and Technology (MSST
   2017) as A Light-weight Compaction Tree to Reduce I/O Amplification
   toward Efficient Key-Value Stores. This research is partially sponsored
   by the National Natural Science Foundation of China under Grant No.
   61472152, No. 61300047, No. 61432007, and No. 61572209; the Fundamental
   Research Funds for the Central Universities HUST:2015QN069; the 111
   Project (No. B07038); the Director Fund of WNLO; the Key Laboratory of
   Data Storage System, Ministry of Education; and the U.S. National
   Science Foundation (NSF) under Grant No. CNS-1702474 and No.
   CNS-1700719.
CR Aghayev A., 2015, P 13 USENIX C FIL ST
   Aghayev A, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P105
   Ahn JS, 2016, IEEE T COMPUT, V65, P902, DOI 10.1109/TC.2015.2435779
   Amer A, 2010, IEEE S MASS STOR SYS
   [Anonymous], 2011, Airways (Pty) Ltd v Aviation Union of South Africa Others 2011 (3) SA 148 (SCA) paras 25-26, P25, DOI DOI 10.1145/1989323.1989327
   Atikoglu Berk, 2012, Performance Evaluation Review, V40, P53, DOI 10.1145/2318857.2254766
   Cassuto Y., 2010, Proceedings of the 2010 IEEE 26th Symposium on Mass Storage Systems and Technologies (MSST), P1, DOI DOI 10.1109/MSST.2010.5496971
   Chang F, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P205
   Cooper BF, 2008, PROC VLDB ENDOW, V1, P1277
   Cooper Brian F., 2010, P 1 ACM S CLOUD COMP, P143, DOI [DOI 10.1145/1807128.1807152, 10.1145/1807128.1807152]
   Debnath B, 2010, PROC VLDB ENDOW, V3, P1414, DOI 10.14778/1920841.1921015
   Feldman T., 2013, Login Mag. USENIX SAGE, V38, P22
   Fitzpatrick Brad., 2011, Memcached: a distributed memory object caching system
   Ghemawat S., 2016, Leveldb
   Gibson G, 2011, CMU-PDL-11-107
   HE W, 2017, P 15 USENIX C FIL, V48, P121
   Huang P., 2014, P 2014 USENIX ANN TE, P489
   Huang Ping, 2014, P 9 EUR C COMP SYST, P22, DOI DOI 10.1145/2592798.2592818
   Ke Zhou, 2017, P 2017 IEEE 33 S MAS
   Lai Chunbo., 2015, IEEE 31 S MASS STORA, P1
   Lakshman Avinash, 2009, LADIS
   LI CJ, 2015, P 10 EUR C COMP SYST, V26, P5
   Lim H, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P1
   Lu LY, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P133
   Manzanares A., 2016, P 8 USENIX WORKSH HO
   Marmol Leonardo, 2014, P 6 USENIX WORKSH HO, P207
   Nyberg Chris, 1994, P 1994 ACM SIGMOD IN
   ONeil P, 1996, ACTA INFORM, V33, P351, DOI 10.1007/s002360050048
   Peng Wang, 2014, P 9 EUR C COMP SYST
   Pichumani Rekha, 2015, P ACM INT SYST STOR
   Sanfilippo S., 2009, Redis
   Sears Russell, 2012, P ACM SIGMOD INT C M, P217
   Tagawa I., 2009, P IEEE INT MAGN C IN
   Wang H, 2013, BMC PLANT BIOL, V13, DOI 10.1186/1471-2229-13-55
   WU X., 2015, P USENIX ANN TECHN C
   Wu XB, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901332
   Yang J, 2013, SCI WORLD J, DOI 10.1155/2013/812469
   Yue YL, 2017, IEEE T PARALL DISTR, V28, P961, DOI 10.1109/TPDS.2016.2609912
   Zhou Y., 2015, P 10 EUR C COMP SYST, P12
NR 39
TC 21
Z9 24
U1 4
U2 39
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD DEC
PY 2017
VL 13
IS 4
SI SI
AR 29
DI 10.1145/3139922
PG 28
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FR4LD
UT WOS:000419035900002
DA 2024-07-18
ER

PT J
AU Ma, JW
   Stones, RJ
   Ma, YX
   Wang, JG
   Ren, JJ
   Wang, G
   Liu, XG
AF Ma, Jingwei
   Stones, Rebecca J.
   Ma, Yuxiang
   Wang, Jingui
   Ren, Junjie
   Wang, Gang
   Liu, Xiaoguang
TI Lazy Exact Deduplication
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Deduplication; lazy method; GPU; disk I/O
ID PERFORMANCE
AB Deduplication aims to reduce duplicate data in storage systems by removing redundant copies of data blocks, which are compared to one another using fingerprints. However, repeated on-disk fingerprint lookups lead to high disk traffic, which results in a bottleneck.
   In this article, we propose a "lazy"data deduplication method, which buffers incoming fingerprints that are used to perform on-disk lookups in batches, with the aim of improving subsequent prefetching. In deduplication in general, prefetching is used to improve the cache hit rate by exploiting locality within the incoming fingerprint stream. For lazy deduplication, we design a buffering strategy that preserves locality in order to facilitate prefetching. Furthermore, as the proportion of deduplication time spent on I/O decreases, the proportion spent on fingerprint calculation and chunking increases. Thus, we also utilize parallel approaches (utilizing multiple CPU cores and a graphics processing unit) to further improve the overall performance.
   Experimental results indicate that the lazy method improves fingerprint identification performance by over 50% compared with an "eager"method with the same data layout. The GPU improves the hash calculation by a factor of 4.6 and multithreaded chunking by a factor of 4.16. Deduplication performance can be improved by over 45% on SSD and 80% on HDD in the last round on the real datasets.
C1 [Ma, Jingwei; Stones, Rebecca J.; Ma, Yuxiang; Wang, Jingui; Ren, Junjie; Wang, Gang; Liu, Xiaoguang] Nankai Univ, Coll Comp & Control Engn, Tongyan Rd 38,Haihe Educ Pk, Tianjin 300350, Peoples R China.
C3 Nankai University
RP Wang, G; Liu, XG (corresponding author), Nankai Univ, Coll Comp & Control Engn, Tongyan Rd 38,Haihe Educ Pk, Tianjin 300350, Peoples R China.
EM mjwtom@nbjl.nankai.edu.cn; rebecca.stones82@nbjl.nankai.edu.cn;
   mayuxiang@nbjl.nankai.edu.cn; wangjingui@nbjl.nankai.edu.cn;
   renjunjie@nbjl.nankai.edu.cn; wgzwp@nbjl.nankai.edu.cn;
   liuxg@nbjl.nankai.edu.cn
RI MA, JINGWEI/H-8829-2013; wang, gang/ITT-0670-2023
OI MA, JINGWEI/0000-0002-8854-9756; 
FU NSF of China [61373018, 61602266, 11550110491]; Natural Science
   Foundation of Tianjin; PhD Candidate Research Innovation Fund of Nankai
   University
FX This work is partially supported by NSF of China (grant numbers:
   61373018, 61602266, 11550110491), the Natural Science Foundation of
   Tianjin, and the PhD Candidate Research Innovation Fund of Nankai
   University.
CR [Anonymous], P ACM ISR EXP SYST C
   [Anonymous], P ACM AS PAC WORKSH
   [Anonymous], NVIDIA CUDA
   [Anonymous], J SUPERCOMPUTING
   [Anonymous], P IASTED INT C PAR D
   [Anonymous], 2010, P USENIX C USENIX AN
   [Anonymous], 2011, Airways (Pty) Ltd v Aviation Union of South Africa Others 2011 (3) SA 148 (SCA) paras 25-26, P25, DOI DOI 10.1145/1989323.1989327
   [Anonymous], VIA NAN PROC
   [Anonymous], T STORAGE
   [Anonymous], 2009, USENIX ANN TECHN C
   [Anonymous], 2009, 7 USENIX C FIL STOR
   [Anonymous], 1981, FINGERPRINTING RANDO
   Bhagwat D, 2009, 2009 IEEE INTERNATIONAL SYMPOSIUM ON MODELING, ANALYSIS & SIMULATION OF COMPUTER AND TELECOMMUNICATION SYSTEMS (MASCOTS), P237
   Bhatotia Pramod., 2012, USENIX FAST'12, FAST'12, P14
   BLOOM BH, 1970, COMMUN ACM, V13, P422, DOI 10.1145/362686.362692
   Bose P, 2008, INFORM PROCESS LETT, V108, P210, DOI 10.1016/j.ipl.2008.05.018
   Botelho F.C., 2013, Proc. of USENIX Conf. on File and Storage Technologies (FAST), P81
   Chulmin Kim, 2011, Proceedings of the 2011 Seventh International Conference on Networked Computing and Advanced Information Management (NCM), P101
   Fu Min, 2014, 2014 USENIX C USENIX, P181
   Guo Fanglu., 2011, Proceedings of the 2011 USENIX Conference on USENIX Annual Technical Conference, USENIXATC'11, P25
   Kim J, 2012, J POWER ELECTRON, V12, P1, DOI 10.6113/JPE.2012.12.1.1
   Liang Ma, 2010, Proceedings of the 2010 IEEE International Conference on Networking, Architecture, and Storage (NAS 2010), P395, DOI 10.1109/NAS.2010.29
   Lin X., 2015, Proceedings of the 7th USENIX Conference on Hot Topics in Storage and File Systems, HotStorage'15, P11
   Lin X, 2015, EAI ENDORSED TRANS S, V2, DOI 10.4108/icst.tridentcom.2015.259963
   Lu GL, 2012, IEEE S MASS STOR SYS
   Ma JG, 2016, ACM T INTERNET TECHN, V16, DOI 10.1145/2806890
   MANBER U, 1994, PROCEEDINGS OF THE WINTER 1994 USENIX CONFERENCE, P1
   Mao B, 2014, ACM T STORAGE, V10, DOI 10.1145/2512348
   Meister Dirk., 2010, MASS STORAGE SYSTEMS, P1
   Meyer D.T., 2011, Proceedings of the 9th USENIX conference on File and stroage technologies, P1
   Meyer DT, 2012, ACM T STORAGE, V7, DOI 10.1145/2078861.2078864
   Min J, 2011, IEEE T COMPUT, V60, P824, DOI 10.1109/TC.2010.263
   Pagh Rasmus., 2001, CUCKOO HASHING
   Paulo J, 2014, ACM COMPUT SURV, V47, DOI 10.1145/2611778
   Policroniades C, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE GENERAL TRACK 2004 USENIX ANNUAL TECHNICAL CONFERENCE, P73
   Quinlan S, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE FAST'02 CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P89
   Shilane P, 2012, ACM T STORAGE, V8, DOI 10.1145/2385603.2385606
   Srinivasan K., 2012, 10 USENIX C FILE STO, V12, P1
   Tan YJ, 2015, CLUSTER COMPUT, V18, P79, DOI 10.1007/s10586-014-0397-5
   Tarasov V., 2012, 2012 F USENIX G ANN, P261
   Waldrop MM, 2016, NATURE, V530, P144, DOI 10.1038/530144a
   Xia W, 2015, IEEE T COMPUT, V64, P1162, DOI 10.1109/TC.2014.2308181
   Xia Wen., 2011, Proceedings of the USENIX Annual Technical Conference, USENIXATC'11, P26
   Zhu B, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P269
NR 44
TC 7
Z9 8
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUN
PY 2017
VL 13
IS 2
SI SI
AR 11
DI 10.1145/3078837
PG 26
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FA1TA
UT WOS:000405221600002
DA 2024-07-18
ER

PT J
AU Diesburg, S
   Meyers, C
   Stanovich, M
   Wang, AIA
   Kuenning, G
AF Diesburg, Sarah
   Meyers, Christopher
   Stanovich, Mark
   Wang, An-I Andy
   Kuenning, Geoff
TI TrueErase: Leveraging an Auxiliary Data Path for Per-File Secure
   Deletion
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Secure deletion; assured deletion; file systems; storage; security; NAND
   flash
ID SYSTEM; DRIVES; DESIGN; MEMORY
AB One important aspect of privacy is the ability to securely delete sensitive data from electronic storage in such a way that it cannot be recovered; we call this action secure deletion. Short of physically destroying the entire storage medium, existing software secure-deletion solutions tend to be piecemeal at best - they may only work for one type of storage or file system, may force the user to delete all files instead of selected ones, may require the added complexities of encryption and key storage, may require extensive changes and additions to the computer's operating system or storage firmware, and may not handle system crashes gracefully.
   We present TrueErase, a holistic secure-deletion framework for individual systems that contain sensitive data. Through design, implementation, verification, and evaluation on both a hard drive and NAND flash, TrueErase shows that it is possible to construct a per-file, secure-deletion framework that can accommodate different storage media and legacy file systems, require limited changes to legacy systems, and handle common crash scenarios. TrueErase can serve as a building block by cryptographic systems that securely delete information by erasing encryption keys. The overhead is dependent on spatial locality, number of sensitive files, and workload (computational-or I/O-bound).
C1 [Diesburg, Sarah] Univ Northern Iowa, 311 Innovat Teaching & Technol Ctr, Cedar Falls, IA 50614 USA.
   [Meyers, Christopher; Stanovich, Mark] Florida State Univ, 253 Love Bldg, Tallahassee, FL 32306 USA.
   [Wang, An-I Andy] Florida State Univ, 269 Love Bldg, Tallahassee, FL 32306 USA.
   [Kuenning, Geoff] Harvey Mudd Coll, Dept Comp Sci, 301 Platt Blvd, Claremont, CA 91711 USA.
C3 University of Northern Iowa; State University System of Florida; Florida
   State University; State University System of Florida; Florida State
   University; Claremont Colleges; Harvey Mudd College
RP Diesburg, S (corresponding author), Univ Northern Iowa, 311 Innovat Teaching & Technol Ctr, Cedar Falls, IA 50614 USA.
EM sarah.diesburg@uni.edu; meyers@cs.fsu.edu; stanovic@cs.fsu.edu;
   awang@cs.uni.edu; geoff@cs.hmc.edu
RI wang, andy/GRJ-8724-2022; Wang, An-I Andy/F-7938-2011; Wang,
   Andy/HME-0514-2023
OI Diesburg, Sarah/0000-0001-8558-1980; Kuenning, Geoff/0000-0002-3882-2072
FU NSF grants [CNS-0845672/CNS-1065127]; DoE grant [P200A060279];
   Philanthropic Educational Organization; FSU Research Foundation; UNI
   Research Foundation
FX This work is supported by NSF grants CNS-0845672/CNS-1065127, DoE grant
   P200A060279, Philanthropic Educational Organization, FSU Research
   Foundation, and UNI Research Foundation. Any opinions, findings,
   conclusions, or recommendations expressed in this material are those of
   the authors and do not necessarily reflect the views of the NSF, DoE,
   PEO, FSU, or UNI.
CR Allen B., 2007, BAD BLOCK HOWTO SMAR
   [Anonymous], P 12 INT LIN SYST TE
   [Anonymous], 1997, Tech. Rep. TR3022
   [Anonymous], 2014, FAST
   [Anonymous], 2005, ACM Transactions on Storage
   [Anonymous], P USENIX C FIL STOR
   [Anonymous], 2011, C FIL STOR TECHN FAS
   [Anonymous], 2006, DISKONCHIP 2000 DIP
   Apple Inc., 2012, MAC OS X SEC CONF MA
   Bauer S, 2001, USENIX ASSOCIATION PROCEEDINGS OF THE 10TH USENIX SECURITY SYMPOSIUM, P153
   Boneh D, 1996, PROCEEDINGS OF THE SIXTH ANNUAL USENIX SECURITY SYMPOSIUM: FOCUSING ON APPLICATIONS OF CRYPTOGRAPHY, P91
   Bonetti G., 2013, Proceedings of the 29th Annual Computer Security Applications Conference, V12, P269, DOI DOI 10.1145/2523649.2523660
   Boyko V., 1999, ADV CRYPTOLOGY CRYPT, P783
   Breeuwsma Marcel, 2009, FORENSIC DATA RECOVE
   Butler KRB, 2008, CCS'08: PROCEEDINGS OF THE 15TH ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P403
   Cooke J., 2007, FLASH MEMORY TECHNOL
   Cranor LF, 2004, IEEE SECUR PRIV, V2, P16, DOI 10.1109/MSP.2004.69
   DBAN, 2012, HARD DRIV DISK WIP D
   Di Crescenzo G, 1999, LECT NOTES COMPUT SC, V1563, P500
   Diesburg S, 2012, P 28 ANN COMP SEC AP
   Diesburg S.M., 2008, P 4 ACM INT WORKSHOP, P11
   Enck W, 2008, ANN COMPUT SECURITY, P55, DOI 10.1109/ACSAC.2008.41
   GANGER GR, 2001, CMUCS01166
   Garlick J., 2012, SCRUB UTILITY
   Gauthier M., 2012, SECURE RM SOURCEFORG
   Geambasu Roxana., 2009, Proceedings of the 18th Conference on USENIX Security Symposium, P299
   Gough V., 2005, ENCFS ENCFS ENCRYPTE
   Hughes GF, 2002, IEEE SPECTRUM, V39, P37, DOI 10.1109/MSPEC.2002.1021942
   Hughes Graham, 2004, TECHNICAL REPORT
   Jimenez X., 2014, PROC USENIX C FILE S, P47
   Josephson W. K., 2010, ACM T STORAGE, V6, P14
   Joukov N., 2005, P 3 IEEE INT SEC STO, P70
   Joukov N., 2006, Proceedings of the Second ACM Workshop on Storage Security and Survivability, P61
   Kim J, 2012, ACM T EMBED COMPUT S, V11, DOI 10.1145/2180887.2180895
   King C, 2011, DIGIT INVEST, V8, pS111, DOI 10.1016/j.diin.2011.05.013
   KINGSTON, 2012, SEC USB FLASH DRIV
   Kleiman S. R., 1986, USENIX Association Summer Conference Proceedings, Atlanta 1986, P238
   Lee J, 2008, APPLIED COMPUTING 2008, VOLS 1-3, P1710
   Lim SH, 2006, IEEE T COMPUT, V55, P906, DOI 10.1109/TC.2006.96
   Lu Youyou., 2013, FAST, P257
   Manning C., 2012, YAFFS WORKS YAFFS
   McKusick M. K., 1984, ACM T COMPUT SYST TO, V2, P181
   MICRON, 2011, BAD BLOCK MAN NAND F
   Muller G., 2004, STATUS OUTLOOK EMERG, P567
   Nightingale EB, 2008, ACM T COMPUT SYST, V26, DOI 10.1145/1394441.1394442
   Patel N., 2009, INTEL PULLS SSD TOOL
   Perlman R., 2005, EPHEMERIZER MAKING D
   Peterson ZNJ, 2005, USENIX ASSOCIATION PROCEEDINGS OF THE 4TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P143
   Pinheiro E, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 5TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES ( FAST '07), P17
   Reardon J., 2013, CCS, P271
   Reardon J., 2013, P 2013 IEEE S SEC PR
   Reardon J., 2011, ARXIV11060917
   Reardon J., 2012, P 21 USENIX SEC S
   Rivest RL, 1997, LECT NOTES COMPUT SC, V1267, P210
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   Rusty Russell P., 1994, FILESYSTEM HIERARCHY
   Sct S.T., 1998, Journaling the linux ext2fs filesystem
   SEAGATE, 2012, PROT YOUR DAT SEAG S
   SHRED, 2012, SHRED LIN UN MAN PAG
   Shu F., 2007, DATA SET MANAGEMENT
   Sivathanu G, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P15
   Sivathanu M, 2005, USENIX ASSOCIATION PROCEEDINGS OF THE 4TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P1
   Sivathanu M, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE 2ND USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST'03), P73
   Sivathanu Muthian., 2004, OSDI 04, P26
   SMART, 2014, SMART MON TOOLS
   Sousa RC, 2005, CR PHYS, V6, P1013, DOI 10.1016/j.crhy.2005.10.007
   Sun K, 2008, IEEE T CONSUM ELECTR, V54, P100
   Thibadeau R, 2006, IEEE SECUR PRIV, V4, P26, DOI 10.1109/MSP.2006.136
   U.S. NIST, 2006, US NIST SPEC PUBL
   United States Census Bureau, 2014, COMP INT US US 2013
   *US DEP DEF, 1995, NAT IND SEC PROGR OP
   Venkatesh A., 2011, EVOLVING PATTERNS HO
   Vier T., 2012, WIPE SECURE FILE DEL
   Wolchok S., 2010, NDSS
   Wong HSP, 2010, P IEEE, V98, P2201, DOI 10.1109/JPROC.2010.2070050
   Woodhouse D., 2008, JFFS2 JOURNALLING FL
   Woodhouse D., 2001, OTT LIN S
   Wright C, 2008, LECT NOTES COMPUT SC, V5352, P243, DOI 10.1007/978-3-540-89862-7_21
   Wu G., 2012, P 10 USENIX C FIL ST, V12, P10
   WU M, 1994, SIGPLAN NOTICES, V29, P86, DOI 10.1145/195470.195506
   Zhang Yiying., 2012, Proceedings of the 10th USENIX conference on File and Storage Technologies, FAST'12, P1
   Zhao K., 2013, P USENIX C FIL STOR, P243
NR 82
TC 11
Z9 15
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2016
VL 12
IS 4
AR 18
DI 10.1145/2854882
PG 37
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DV2OQ
UT WOS:000382760500001
DA 2024-07-18
ER

PT J
AU Wu, CH
   Huang, KY
AF Wu, Chin-Hsien
   Huang, Kuo-Yi
TI Data Sorting in Flash Memory
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Algorithm; Design; Performance; Flash memory; memory structures;
   sorting/searching; storage management
AB Because flash memory now provides an economical solution for various portable devices and embedded systems, an NAND flash-based storage system has replaced the hard disk drive in many applications. Recently, the implementation of database systems using an NAND flash-based storage system has become an important research topic. In particular, the external sorting is an important operation in database systems. With the very distinctive characteristics of flash memory, the typical external sorting system that adopts a clustered sorting process can result in performance degradation and reduce the reliability of flash memory. In this article, we will propose an unclustered sorting method that considers the unique characteristics of flash memory, and we then propose a decision rule to exploit the advantages of both clustered and unclustered sorting. The decision rule can separate records according to their record length, sort them appropriately by the clustered and unclustered sorting, and merge the sorted results. The experimental results show that the proposed method can improve performance in an NAND flash-based storage system (i.e., solid-state drive).
EM chwu@mail.ntust.edu.tw; M9902122@mail.ntust.edu.tw
FU Ministry of Science and Technology [MOST 103-2221-E-011-065-MY2]
FX This work was supported in part by a research grant from the Ministry of
   Science and Technology under Grant MOST 103-2221-E-011-065-MY2.
CR [Anonymous], 1998, SORTING SEARCHING
   Boboila S, 2011, IEEE S MASS STOR SYS
   Cossentine T., 2013, INT J DATABASE MANAG, V5, P1, DOI 10.5121/ijdms.2013.5101
   Desnoyers P, 2014, ACM T STORAGE, V10, DOI 10.1145/2577384
   Graefe G, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1132960.1132964
   Larson P.-A., 1998, SIGMOD Record, V27, P472, DOI 10.1145/276305.276346
   Larson PÅ, 2003, IEEE T KNOWL DATA EN, V15, P961, DOI 10.1109/TKDE.2003.1209012
   Lee SW, 2007, ACM T EMBED COMPUT S, V6, DOI 10.1145/1275986.1275990
   Li X., 1993, Proceedings SUPERCOMPUTING '93, P666
   Park H, 2009, J SYST SOFTWARE, V82, P1298, DOI 10.1016/j.jss.2009.02.028
   Ramakrishnan Raghu, 2002, Database Management Systems
   SALZBERG B, 1990, SIGMOD REC, V19, P94, DOI 10.1145/93605.98719
   Samsung Electronics, 2011, NAND FLASH MEM DAT S
   Track Metadata, 2015, MILL SONG DAT
   Wu Chin-Hsien, 2012, P ACM S APPL COMP SA, P1847
   Yiannis J, 2007, VLDB J, V16, P269, DOI 10.1007/s00778-006-0005-2
   Zhang WY, 1997, PROCEEDINGS OF THE TWENTY-THIRD INTERNATIONAL CONFERENCE ON VERY LARGE DATABASES, P376
NR 17
TC 4
Z9 6
U1 3
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAR
PY 2015
VL 11
IS 2
AR 7
DI 10.1145/2665067
PG 25
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CF1KG
UT WOS:000352303400003
DA 2024-07-18
ER

PT J
AU Wu, GY
   He, XB
   Eckart, B
AF Wu, Guanying
   He, Xubin
   Eckart, Ben
TI An Adaptive Write Buffer Management Scheme for Flash-Based SSDs
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Measurement; Performance; SSD; NAND flash memory; flash-aware
   cache; write buffer
ID TRANSLATION LAYER; FILE SYSTEM; POLICY
AB Solid State Drives (SSD's) have shown promise to be a candidate to replace traditional hard disk drives. The benefits of SSD's over HDD's include better durability, higher performance, and lower power consumption, but due to certain physical characteristics of NAND flash, which comprise SSD's, there are some challenging areas of improvement and further research. We focus on the layout and management of the small amount of RAM that serves as a cache between the SSD and the system that uses it. Of the techniques that have previously been proposed to manage this cache, we identify several sources of inefficient cache space management due to the way pages are clustered in blocks and the limited replacement policy. We find that in many traces hot pages reside in otherwise cold blocks, and that the spatial locality of most clusters can be fully exploited in a limited time period, so we develop a hybrid page/block architecture along with an advanced replacement policy, called BPAC, or Block-Page Adaptive Cache, to exploit both temporal and spatial locality. Our technique involves adaptively partitioning the SSD on-disk cache to separately hold pages with high temporal locality in a page list and clusters of pages with low temporal but high spatial locality in a block list. In addition, we have developed a novel mechanism for flash-based SSD's to characterize the spatial locality of the disk I/O workload and an approach to dynamically identify the set of low spatial locality clusters. We run trace-driven simulations to verify our design and find that it outperforms other popular flash-aware cache schemes under different workloads. For instance, compared to a popular flash aware cache algorithm BPLRU, BPAC reduces the number of cache evictions by up to 79.6% and 34% on average.
C1 [Wu, Guanying; He, Xubin] Virginia Commonwealth Univ, Sch Engn, Richmond, VA 23284 USA.
   [Eckart, Ben] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.
C3 Virginia Commonwealth University; Carnegie Mellon University
RP He, XB (corresponding author), Virginia Commonwealth Univ, Sch Engn, Monroe Pk Campus,West Hall,601 W Main St,POB 8430, Richmond, VA 23284 USA.
EM wug@vcu.edu; xhe2@vcu.edu; eckart@cmu.edu
FU U.S. National Science Foundation (NSF) [CCF-1102605, CCF-1102624,
   CNS-1102629]; Division of Computing and Communication Foundations;
   Direct For Computer & Info Scie & Enginr [1102605] Funding Source:
   National Science Foundation
FX This research was supported by the U.S. National Science Foundation
   (NSF) under Grant Nos. CCF-1102605, CCF-1102624, and CNS-1102629. Any
   opinions, findings, and conclusions or recommendations expressed in this
   material are those of the author(s) and do not necessarily reflect the
   views of the funding agency.
CR Agrawal Nitin, 2008, P USENIX 2008 ANN TE
   [Anonymous], 2011, P FAST 2
   [Anonymous], P 4 C USENIX C FIL S
   [Anonymous], 2010, P 8 USENIX C FIL STO
   Balakrishnan M, 2010, ACM T STORAGE, V6, DOI 10.1145/1807060.1807061
   Bansal S, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P187
   Bez R, 2003, P IEEE, V91, P489, DOI 10.1109/JPROC.2003.811702
   CHANG Y.-H., 2007, P IEEE ACM DES AUT C
   Chang YH, 2010, ACM T STORAGE, V6, DOI 10.1145/1807060.1807062
   Chen F., 2011, P 17 IEEE INT S HIGH
   GILL B. S., 2005, P 4 C USENIX C FIL S
   Gupta A, 2009, ACM SIGPLAN NOTICES, V44, P229, DOI 10.1145/1508284.1508271
   HAT R., 2010, JOURNALLING FLASH SY
   HEWLETT-PACKARD LABORATORIES, CELLO99 TRAC
   Hu J, 2010, IEEE INT SYMP CIRC S, P85, DOI 10.1109/ISCAS.2010.5537028
   HUTSELL W., 2008, FLASH SOLID STATE DI
   INTEL, 2009, INT X25 M SATA SOLID
   Jo H, 2006, IEEE T CONSUM ELECTR, V52, P485, DOI 10.1109/TCE.2006.1649669
   Johnson T., 1994, P 20 INT C VER LARG, P439
   Josephson WK, 2010, ACM T STORAGE, V6, DOI 10.1145/1837915.1837922
   Jung J, 2010, ACM T STORAGE, V6, DOI 10.1145/1714454.1714457
   Kang J.-U, 2006, P INT C EMB SOFTW
   Kang SY, 2009, IEEE T COMPUT, V58, P744, DOI 10.1109/TC.2008.224
   KAREDLA R, 1994, COMPUTER, V27, P38, DOI 10.1109/2.268884
   KIM H., 2008, P USENIX C FIL STOR
   Kim J, 2002, IEEE T CONSUM ELECTR, V48, P366
   Kim JM, 2000, USENIX ASSOCIATION PROCEEDINGS OF THE FOURTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P119
   Lee D, 2001, IEEE T COMPUT, V50, P1352, DOI 10.1109/tc.2001.970573
   Lee S., 2009, Usenix ATC
   Lee Yong-Goo., 2008, EMSOFT '08, P21
   Manning Charles., 2010, Yet another flash file system
   MASON L., 2009, RETHINKING SSDS
   Megiddo N, 2004, COMPUTER, V37, P58, DOI 10.1109/MC.2004.1297303
   Megiddo N, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE 2ND USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST'03), P115
   NEWEGG, 2009, W DIG VELOCIRAPOTR W
   NEWEGG, 2009, INT X25 M MAINSTR SS
   Nicola V. F., 1992, Performance Evaluation Review, V20, P35, DOI 10.1145/149439.133084
   O'Neil E. J., 1993, SIGMOD Record, V22, P297, DOI 10.1145/170036.170081
   PARK D.-J, 2005, P UKC C UKC
   Park S.-y., 2006, P 2006 INT C COMP AR, P234, DOI DOI 10.1145/1176760.1176789
   Ren J., 2011, P 17 IEEE INT S HIGH
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   Shim Hyotaek., 2010, Proceedings of the 2010 IEEE 26th Symposium on Mass Storage Systems and Technologies (MSST '10), P1
   SHIMPI A. L., 2009, INTEL X235 M G2 DISS
   SILICONSYSTEMS, 2005, INCR FLASH SOLID STA
   SIMPLESCALAR LLC, 2009, SIMPL TOOL SET
   Song Jiang, 2002, Performance Evaluation Review, V30, P31, DOI 10.1145/511399.511340
   Storage Performance Council, 2010, SPC TRAC FIL FORM SP
   Sun GY, 2010, INT S HIGH PERF COMP, P141
   Sungjin Lee, 2008, Operating Systems Review, V42, P36, DOI 10.1145/1453775.1453783
   UMASS, 2007, UMASS TRAC REP
   Wang Y, 2010, ACM T STORAGE, V6, DOI 10.1145/1807060.1807064
   WESTERN DIGITAL, 2008, WD VELOCIRAPTOR SATA
   Wu Chin-Hsien., 2006, Proceedings of the 2006 IEEE/ACM International Conference on Computer-Aided Design (ICCAD '06), P601
   Zhou YY, 2004, IEEE T PARALL DISTR, V15, P505, DOI 10.1109/TPDS.2004.13
   Zhou YY, 2001, USENIX ASSOCIATION PROCEEDINGS OF THE 2001 USENIX ANNUAL TECHNICAL CONFERENCE, P91
NR 56
TC 31
Z9 42
U1 0
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD FEB
PY 2012
VL 8
IS 1
AR 1
DI 10.1145/2093139.2093140
PG 24
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 990LT
UT WOS:000307633100001
DA 2024-07-18
ER

PT J
AU Li, HC
   Putra, ML
   Shi, R
   Kurnia, FI
   Lin, X
   Do, J
   Kistijantoro, AI
   Ganger, GR
   Gunawi, HS
AF Li, Huaicheng
   Putra, Martin L.
   Shi, Ronald
   Kurnia, Fadhil, I
   Lin, Xing
   Do, Jaeyoung
   Kistijantoro, Achmad Imam
   Ganger, Gregory R.
   Gunawi, Haryadi S.
TI Extending and Programming the NVMe I/O Determinism Interface for Flash
   Arrays
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Software/hardware co-design; predictable latency; SSD
ID MEMORY
AB Predictable latency on flash storage is a long-pursuit goal, yet unpredictability stays due to the unavoidable disturbance from many well-known SSD internal activities. To combat this issue, the recent NVMe IO Determinism (IOD) interface advocates host-level controls to SSD internalmanagement tasks. Although promising, challenges remain on how to exploit it for truly predictable performance.
   We present IODA,1 an I/O deterministic flash array design built on top of small but powerful extensions to the IOD interface for easy deployment. IODA exploits data redundancy in the context of IOD for a strong latency predictability contract. In IODA, SSDs are expected to quickly fail an I/O on purpose to allowpredictable I/Os through proactive data reconstruction. In the case of concurrent internal operations, IODA introduces busy remaining time exposure and predictable-latency-window formulation to guarantee predictable data reconstructions. Overall, IODA only adds five new fields to the NVMe interface and a small modification in the flash firmware while keeping most of the complexity in the host OS. Our evaluation shows that IODA improves the 95-99.99th latencies by up to 75x. IODA is also the nearest to the ideal, no disturbance case compared to seven state-of-the-art preemption, suspension, GC coordination, partitioning, tiny-tail flash controller, prediction, and proactive approaches.
C1 [Li, Huaicheng; Ganger, Gregory R.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   [Li, Huaicheng; Putra, Martin L.; Shi, Ronald; Gunawi, Haryadi S.] Univ Chicago, Chicago, IL 60637 USA.
   [Kurnia, Fadhil, I] Univ Massachusetts, Amherst, MA 01003 USA.
   [Lin, Xing] NetApp, Amherst, MA USA.
   [Do, Jaeyoung] Microsoft Res, Redmond, WA USA.
   [Kistijantoro, Achmad Imam] Bandung Inst Technol, Bandung, Indonesia.
C3 Carnegie Mellon University; University of Chicago; University of
   Massachusetts System; University of Massachusetts Amherst; Microsoft;
   Institute Technology of Bandung
RP Li, HC (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.; Li, HC (corresponding author), Univ Chicago, Chicago, IL 60637 USA.
RI Kistijantoro, Imam/AAC-7608-2021
OI Kistijantoro, Imam/0000-0003-2065-6675; Gunawi,
   Haryadi/0000-0003-3680-8450; DO, JAEYOUNG/0000-0003-1275-1621; Kurnia,
   Fadhil/0000-0001-5936-5264; Li, Huaicheng/0000-0002-3155-0203
CR Amvrosiadis G, 2015, SOSP'15: PROCEEDINGS OF THE TWENTY-FIFTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P457, DOI 10.1145/2815400.2815424
   Ananthanarayanan G., 2013, PROC USENIX S NETW S, V13, P185
   Ananthanarayanan G, 2010, P 9 USENIX C OP SYST, P265
   [Anonymous], 2015, P 15 WORKSHOP HOT TO
   Architecting It, 2018, WHY DETERMINISTIC ST
   Arpaci-Dusseau Andrea C., 2001, P 18 ACM S OPERATING
   Barroso L, 2017, COMMUN ACM, V60, P47, DOI 10.1145/3015146
   Bennett Jon C. R., 2012, Memory Management System and Method
   Bjorling M, 2021, PROCEEDINGS OF THE 2021 USENIX ANNUAL TECHNICAL CONFERENCE, P689
   Bjorling M, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P359
   Caulfield AM, 2009, ACM SIGPLAN NOTICES, V44, P217, DOI 10.1145/1508284.1508270
   Chang F, 1999, USENIX ASSOCIATION PROCEEDINGS OF THE THIRD SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '99), P1
   Chang Yun-Sheng, 2020, P 14 USENIX S OPERAT
   Chen F, 2011, INT S HIGH PERF COMP, P266, DOI 10.1109/HPCA.2011.5749735
   Colgrove John, 2015, P 2015 ACM SIGMOD IN
   Cooper Brian F., 2010, P 1 ACM S CLOUD COMP, P143, DOI [DOI 10.1145/1807128.1807152, 10.1145/1807128.1807152]
   Dean J, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE SIXTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDE '04), P137
   Dean J, 2013, COMMUN ACM, V56, P74, DOI 10.1145/2408776.2408794
   Elyasi Nima, 2019, P IEEE INT S WORKLOA
   GitHub, 2021, IODA GITH HOM
   GitHub, DFC OP SOURC COMM
   GitHub, 2018, FEMU GITH HOM
   GitHub, 2020, SYSB
   GitHub, FILEBENCH
   GitHub, 2020, HIBENCH BIGD MICR SU
   GitLab, 2017, EM D430S
   Grupp L. M., 2012, FAST 12 P 10 USENIX
   Grupp Laura M., 2013, P 2013 USENIX ANN TE
   Gunawi Haryadi S., 2004, P 6 USENIX S OPERATI
   Hao MZ, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P168, DOI 10.1145/3132747.3132774
   Hao Mingzhe, 2020, P 14 USENIX S OPERAT
   Hoei Jung Sheng, 2019, REDUNDANT ARRAY INDE
   Hu Y, 2011, P 25 INT C SUPERCOMP
   Hu Yaochen, 2017, P 8 ACM S CLOUD COMP
   Huaicheng Li, 2020, ASPLOS '20: Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, P591, DOI 10.1145/3373376.3378531
   Huang Jian, 2017, P 15 USENIX S FILE S
   Hyun Jea Woong, 2015, ERASE SUSPENDRESUME
   Incits, 2022, DAT SET MAN COMM PRO
   Intel, 2021, ACH CONS LOW LAT YOU
   Intel, 2021, INT OPT PERS MEM PME
   Jiang Tianyang, 2021, P 19 USENIX S FILE S
   Jung M, 2020, IEEE T COMPUT AID D, V39, P1674, DOI 10.1109/TCAD.2019.2919035
   Karkra Kapil, 2018, Using Software to Reduce High Tail Latencies on SSDs
   Kim J, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P799
   Kim Jaeho, 2015, P 13 USENIX S FIL ST
   Kim J, 2018, 2018 51ST ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE (MICRO), P455, DOI 10.1109/MICRO.2018.00044
   Kim Myungsuk, 2020, P 25 ACM INT C ARCHI
   Kim S, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P813
   Kim T, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P295
   Kim Y, 2011, IEEE S MASS STOR SYS
   Kim Y, 2014, IEEE T COMPUT, V63, P888, DOI 10.1109/TC.2012.256
   Klimovic A, 2017, TWENTY-SECOND INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXII), P345, DOI 10.1145/3037697.3037732
   Kwon Miryeong, 2020, P 18 USENIX S FILE S
   Lee J, 2013, IEEE T COMPUT AID D, V32, P247, DOI 10.1109/TCAD.2012.2227479
   Lee J, 2011, INT SYM PERFORM ANAL, P12, DOI 10.1109/ISPASS.2011.5762711
   Lee SJ, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P339
   Li HC, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P83
   Li Huaicheng, 2021, P 28 ACM S OPERATING
   Li Nanqinqin, 2022, P 15 ACM INT C SYSTE
   LightNVM, Rev. 2.0
   Litz H, 2022, ACM T STORAGE, V18, DOI 10.1145/3465406
   Liu CY, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P67
   Liu Chun-Yi, 2021, P 26 ACM INT C ARCHI
   Maas M, 2016, ACM SIGPLAN NOTICES, V51, P457, DOI 10.1145/2954679.2872386
   Mesnier M, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P57
   Micron, 2020, MICRON 9100 U2 HHHL
   Missimer K, 2018, REAL TIM SYST SYMP P, P185, DOI 10.1109/RTSS.2018.00036
   Navasca C, 2019, PROCEEDINGS OF THE TWENTY-SEVENTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '19), P538, DOI 10.1145/3341301.3359643
   NVMExpress, 2020, NVMEXPRESS BAS SPEC
   OpenSSD, COSM OPENSSD PLATF, P1023
   Ouyang Jian, 2014, P 19 ACM INT C ARCHI
   Petersen Chris., 2018, Enabling NVMe I/O Determinism @Scale
   Pletka R, 2018, ACM T STORAGE, V14, DOI 10.1145/3241060
   Rashmi KV, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P401
   Samsung, 2020, ALL FLASH NVME REFER
   Samsung, 2020, ULTR LAT SAMS Z NAND
   Samsung, 2014, MZHPV128HDGM SM951 1
   Seshadri S., 2014, P 11 USENIX S OPERAT
   Shin Ji-Yong, 2013, P 11 USENIX S FILE S
   Silverton Consulting, 2016, GREYBEARDS STOR
   Skourtis Dimitris, 2014, P 2014 USENIX ANN TE
   SNIA, 2016, SNIA I O TRAC DAT FI
   Stenfort Ross, 2020, NVMe Cloud SSD Specification
   Storage Search, 2020, WHATS STATE DWPD END
   Suresh Lalith, 2015, P 12 USENIX S NETW S
   Tai A, 2021, PROCEEDINGS OF THE 15TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '21), P129
   Venkataraman S, 2014, P 11 USENIX S OPERAT
   Violin, 2020, STOR LAT FLASH ARR
   Western Digital, 2015, SPEEDS FEEDS NEEDS U
   Wikipedia, ADD INCR MULT DECR
   Wikipedia, 2021, NONV RAND ACC MEM
   Wu Guanying, 2012, P 10 USENIX S FILE S
   Wu SZ, 2018, INT PARALL DISTRIB P, P296, DOI 10.1109/IPDPS.2018.00039
   Wu William, 2014, PREEMPTIVE GARBAGE C
   Yan SQ, 2017, ACM T STORAGE, V13, DOI 10.1145/3121133
   Zaharia Matei, 2008, P 8 USENIX S OPERATI
   Zhang Jie, 2018, P 13 USENIX S OPERAT
   Zhang Yiying, 2012, P 10 USENIX S FILE S
   ZheWu Curtis Yu, 2015, P 12 USENIX S NETWOR
   Zuck Aviad, 2019, P 17 WORKSHOP HOT TO
NR 100
TC 0
Z9 0
U1 3
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD FEB
PY 2023
VL 19
IS 1
AR 5
DI 10.1145/3568427
PG 33
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 9I5DE
UT WOS:000939530200005
DA 2024-07-18
ER

PT J
AU Gao, CM
   Ye, M
   Xue, CJ
   Zhang, YT
   Shi, L
   Shu, JW
   Yang, J
AF Gao, Congming
   Ye, Min
   Xue, Chun Jason
   Zhang, Youtao
   Shi, Liang
   Shu, Jiwu
   Yang, Jun
TI Reprogramming 3D TLC Flash Memory based Solid State Drives
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE 3D TLC SSD; reprogramming; reliability; RAID 5
ID PERFORMANCE
AB NAND flash memory-based SSDs have been widely adopted. The scaling of SSD has evolved from plannar (2D) to 3D stacking. For reliability and other reasons, the technology node in 3D NAND SSD is larger than in 2D, but data density can be increased via increasing bit-per-cell. In this work, we develop a novel reprogramming scheme for TLCs in 3D NAND SSD, such that a cell can be programmed and reprogrammed several times before it is erased. Such reprogramming can improve the endurance of a cell and the speed of programming, and increase the amount of bits written in a cell per program/erase cycle, i.e., effective capacity. Our work is the first to perform a real 3D NAND SSD test to validate the feasibility of the reprogram operation. From the collected data, we derive the restrictions of performing reprogramming due to reliability challenges. Furthermore, a reprogrammable SSD (ReSSD) is designed to structure reprogram operations. ReSSD is evaluated in a case study in RAID 5 system (RSS-RAID). Experimental results show that RSS-RAID can improve the endurance by 35.7%, boost write performance by 15.9%, and increase effective capacity by 7.71%, with negligible overhead compared with conventional 3D SSD-based RAID 5 system.
C1 [Gao, Congming; Shu, Jiwu] Tsinghua Univ, Dept Comp Sci & Technol, 30 Shuangqing Rd, Beijing 100084, Peoples R China.
   [Ye, Min; Xue, Chun Jason] City Univ Hong Kong, Dept Comp Sci, Kowloon, 83 Tat Chee Ave, Hong Kong, Peoples R China.
   [Zhang, Youtao; Yang, Jun] Univ Pittsburgh, Dept Comp Sci, 210 S Bouquet St, Pittsburgh, PA 15260 USA.
   [Shi, Liang] East China Normal Univ, Dept Comp Sci & Technol, 500 Dongchuan Rd, Shanghai 200241, Peoples R China.
C3 Tsinghua University; City University of Hong Kong; Pennsylvania
   Commonwealth System of Higher Education (PCSHE); University of
   Pittsburgh; East China Normal University
RP Shu, JW (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, 30 Shuangqing Rd, Beijing 100084, Peoples R China.
EM gaocm92@gmail.com; vic.ye@yeestor.com; jasonxue@cityu.edu.hk;
   zhangyt@cs.pitt.edu; shi.liang.hk@gmail.com; shujw@tsinghua.edu.cn;
   juy9@pitt.edu
OI YE, Min/0000-0001-5828-3806
FU National Key Research & Development Program of China [2018YFB1003301];
   National Natural Science Foundation of China [61832011]; Zhejiang Lab
   [2020KC0AB03]; NSF [2011146, 1738783, 1910413, 1718080]; Research Grants
   Council of the Hong Kong Special Administrative Region, China [CityU
   11219319]; China Postdoctoral Science Foundation [2020M680568,
   2021T140376]; Direct For Computer & Info Scie & Enginr; Division Of
   Computer and Network Systems [1738783] Funding Source: National Science
   Foundation
FX An earlier version of this article was presented at the 52nd IEEE/ACM
   International Symposium on Microarchitecture, MICRO'19, October 2019
   [22]. This work is supported in part by National Key Research &
   Development Program of China (Grant No. 2018YFB1003301), the National
   Natural Science Foundation of China (Grant No. 61832011), Zhejiang Lab
   (Grant No. 2020KC0AB03), NSF Grant No. 2011146, 1738783, 1910413,
   1718080, the Research Grants Council of the Hong Kong Special
   Administrative Region, China (Project No. CityU 11219319), China
   Postdoctoral Science Foundation No. 2020M680568, 2021T140376.
CR Agrawal Nitin, 2008, P ANN TECHN C USENIX
   Arrhenius S., 1889, Zeitschrift fur Physikalische Chemie, V4, P226, DOI DOI 10.1515/ZPCH-1889-0416.S2CID100032801
   Cai Y, 2017, P IEEE, V105, P1666, DOI 10.1109/JPROC.2017.2713127
   Cai Y, 2017, INT S HIGH PERF COMP, P49, DOI 10.1109/HPCA.2017.61
   Cai Y, 2013, 2013 IEEE 31ST INTERNATIONAL CONFERENCE ON COMPUTER DESIGN (ICCD), P123, DOI 10.1109/ICCD.2013.6657034
   Cai Y, 2015, I C DEPEND SYS NETWO, P438, DOI 10.1109/DSN.2015.49
   Chang Li-Pin., 2004, P T EMB COMP SYST AC
   Chang Yu-Ming., 2016, P HARDW SOFTW COD SY
   Chang YH, 2007, DES AUT CON, P212, DOI 10.1109/DAC.2007.375155
   CHEN SZ, 1993, J PARALLEL DISTR COM, V17, P58, DOI 10.1006/jpdc.1993.1005
   Chen Tseng-Yi., 2017, P ANN DES AUT C ACM
   Chen WC, 2014, PROCEEDINGS OF TECHNICAL PROGRAM - 2014 INTERNATIONAL SYMPOSIUM ON VLSI TECHNOLOGY, SYSTEMS AND APPLICATION (VLSI-TSA)
   Choi W, 2018, 2018 51ST ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE (MICRO), P482, DOI 10.1109/MICRO.2018.00046
   Choi Wonil., 2017, P MEAS ANAL COMP SYS
   Chung CC, 2014, IEEE T VLSI SYST, V22, P1470, DOI 10.1109/TVLSI.2013.2275737
   Gao CM, 2019, MICRO'52: THE 52ND ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE, P493, DOI 10.1145/3352460.3358323
   Gao CM, 2014, IEEE S MASS STOR SYS
   Gao. Congming, 2019, P MASS STOR SYST TEC
   Haratsch. Erich F., 2017, P SEAG FLASH MEM SUM
   Ho. Chien-Chung, 2018, P DES AUT C IEEE
   Hruska Joel, 2018, SAMSUNG NOW MASS PRO
   Hu Yang., 2011, P INT C SUP ACM
   Hurford James, 2003, P LANG EVOL COMP WOR
   Im S, 2011, IEEE T COMPUT, V60, P80, DOI 10.1109/TC.2010.197
   Inc. Micron Technology, 2016, MICR 3D NAND FLASH M
   Jang J, 2009, 2009 SYMPOSIUM ON VLSI TECHNOLOGY, DIGEST OF TECHNICAL PAPERS, P192
   JEDEC SOLID STATE TECHNOLOGY ASSOCIATION, 2011, JESD47H01 JED SOL ST
   Jeong Jaeyong, 2014, FAST
   Johnson Timothy J., 1998, U.S. Patent, Patent No. [5,805,788, 5805788]
   Jung TS, 1996, IEEE J SOLID-ST CIRC, V31, P1575, DOI 10.1109/JSSC.1996.542301
   KaijieWu Edwin, 2018, P T DES AUT ELECT SY
   Kim C., P INT SOL STAT CIRC
   Kim H, 2017, IEEE INT MEM WORKSH, P7
   Kim JH, 2015, DES AUT TEST EUROPE, P555
   Kim M, 2017, DES AUT CON, DOI 10.1145/3061639.3062264
   Kim Sang-Woo, 2008, P SIGMOD INT C MAN D
   Lee S, 2016, ISSCC DIG TECH PAP I, V59, P138, DOI 10.1109/ISSCC.2016.7417945
   Li C, 2016, PROC INT CONF PARAL, P396, DOI 10.1109/ICPP.2016.52
   Liu Chun-yi, 2018, P C FIL STOR TECHN U
   Margaglia F, 2015, IEEE S MASS STOR SYS
   Margaglia F, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P95
   Meng Fei, 2014, P ANN TECHN C USENIX
   Micheloni R, 2016, 3D FLASH MEMORIES, P1, DOI 10.1007/978-94-017-7512-0
   Micheloni R., 2013, Inside Solid State Drives (SSDs)
   Micheloni Rino, 2010, Inside NAND Flash Memories
   Micron, 2018, MT29F1T08EEHAFJ4 3R
   Mielke N, 2004, IEEE T DEVICE MAT RE, V4, P335, DOI 10.1109/TDMR.2004.836721
   Moon Sangwhan, 2013, P WORKSH HOT TOP STO
   Narayanan D, 2009, EUROSYS'09: PROCEEDINGS OF THE FOURTH EUROSYS CONFERENCE, P145
   Prabhu Pravin, 2011, P INT C TRUST TRUSTW
   SanDisk, 2013, UNEXP POW LOSS PROT
   Shahidi Narges, 2016, P INT C HIGH PERF CO
   Shi L, 2016, IEEE T VLSI SYST, V24, P334, DOI 10.1109/TVLSI.2015.2393299
   Tjioe Jonathan, 2012, P NETW ARCH STOR IEE
   Van Houdt B, 2013, PERFORM EVALUATION, V70, P692, DOI 10.1016/j.peva.2013.08.010
   Western Digital, 2017, W DIG ANN IND 1 96 L
   Wikipedia contributors, 2019, PAR PRINC
   Yadgar Gala, 2015, P C FILE STOR TECHN
   YEESTOR, 2018, Ys9083xt/ys9081xt ssd platform
   Yoon Jung H., 2017, P FLASH MEM SUMM
NR 60
TC 4
Z9 4
U1 1
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD FEB
PY 2022
VL 18
IS 1
AR 9
DI 10.1145/3487064
PG 33
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZN5OY
UT WOS:000765084700006
DA 2024-07-18
ER

PT J
AU Yadgar, G
   Gabel, M
   Jaffer, S
   Schroeder, B
AF Yadgar, Gala
   Gabel, Moshe
   Jaffer, Shehbaz
   Schroeder, Bianca
TI SSD-based Workload Characteristics and Their Performance Implications
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE SSD; I/O workload analysis; locality; workload characterization
ID FLASH TRANSLATION; LAYER
AB Storage systems are designed and optimized relying on wisdom derived from analysis studies of file-system and block-level workloads. However, while SSDs are becoming a dominant building block in many storage systems, their design continues to build on knowledge derived from analysis targeted at hard disk optimization. Though still valuable, it does not cover important aspects relevant for SSD performance. In a sense, we are "searching under the streetlight," possibly missing important opportunities for optimizing storage system design.
   We present the first I/O workload analysis designed with SSDs in mind. We characterize traces from four repositories and examine their "temperature" ranges, sensitivity to page size, and "logical locality." We then take the first step towards correlating these characteristics with three standard performance metrics: write amplification, read amplification, and flash read costs. Our results show that SSD-specific characteristics strongly affect performance, often in surprising ways.
C1 [Yadgar, Gala] Technion, Comp Sci Dept, CS Taub Bldg, IL-3200003 Haifa, Israel.
   [Gabel, Moshe; Jaffer, Shehbaz; Schroeder, Bianca] Univ Toronto, Dept Comp Sci, 10 Kings Coll Rd, Toronto, ON M5S 3G4, Canada.
C3 Technion Israel Institute of Technology; University of Toronto
RP Yadgar, G (corresponding author), Technion, Comp Sci Dept, CS Taub Bldg, IL-3200003 Haifa, Israel.
EM gala@cs.technion.ac.il; mgabel@cs.toronto.edu;
   shehbaz.jaffer@mail.utoronto.ca; bianca@cs.toronto.edu
RI Jaffer, Shehbaz/ABA-4337-2021; Jaffer, Shehbaz/AAZ-9307-2021
OI Yadgar, Gala/0000-0003-2701-0260
CR Agarwal Neha, 2017, P 22 INT C ARCH SUPP
   Agrawal Nitin, 2007, ACM Transactions on Storage, V3, DOI 10.1145/1288783.1288788
   Agrawal Nitin, 2008, 2008 USENIX ANN TECH
   [Anonymous], 2017, P 12 EUR C COMP SYST
   [Anonymous], 2017, P 15 USENIX C FIL ST
   [Anonymous], 2017, P 15 USENIX C FIL ST
   [Anonymous], 2011, P 9 USENIX C FIL STO
   [Anonymous], 2010, P 8 USENIX C FIL STO
   Atikoglu Berk, 2012, P 12 ACM SIGM PERF J
   Bouganim L, 2009, P C INN DAT SYST RES
   Brunelle Alan D., 2008, BLKTRACE USER GUIDE
   Bucy J.S., 2008, DISKSIM SIMULATION E
   Chen Y., 2011, P 23 ACM S OP SYST P
   Chiang ML, 1999, SOFTWARE PRACT EXPER, V29, P267, DOI 10.1002/(SICI)1097-024X(199903)29:3<267::AID-SPE233>3.0.CO;2-T
   Chiao ML, 2011, IEEE T COMPUT, V60, P753, DOI 10.1109/TC.2011.67
   Cooper B.F., 2010, P ACM S CLOUD COMPUT, DOI DOI 10.1145/1807128.1807152
   Courville J, 2016, IEEE S MASS STOR SYS
   Desnoyers P, 2014, ACM T STORAGE, V10, DOI 10.1145/2577384
   Desnoyers Peter, 2013, P 9 USENIX WORKSH HO
   Gulati Ajay, 2009, P INT C PAR ARCH COM
   Gupta A, 2009, ACM SIGPLAN NOTICES, V44, P229, DOI 10.1145/1508284.1508271
   Hadizadeh M., 2020, P DES AUT TEST EUR C
   Hajkazemi Mohammad Hossein, 2019, P USENIX ANN TECHN C
   He Weiping, 2017, P 15 USENIX C FIL ST
   Im S, 2010, J SYST ARCHITECT, V56, P641, DOI 10.1016/j.sysarc.2010.09.005
   Intel, INT 64M20C CLIENT CO
   Jen-Wei Hsieh, 2006, ACM Transaction on Storage, V2, P22, DOI 10.1145/1138041.1138043
   Jian Ouyang, 2014, P ACM INT C ARCH SUP
   Kaiser Jurgen, 2013, P 6 INT SYST STOR C
   Kang Jeong-Uk, 2014, 6 USENIX WORKSH HOT
   Kashyap A, 2018, ACM T STORAGE, V14, DOI 10.1145/3151847
   Kavalanekar S., 2008, P IEEE INT S ONWORKL
   Kim J, 2013, I C DEPEND SYS NETWO
   Kim Taejin, 2019, P 17 USENIX C FIL ST
   Koller R, 2010, ACM T STORAGE, V6, DOI 10.1145/1837915.1837921
   Kremer K., 2019, SYSTOR, P167
   Leung A.W., 2008, 2008 USENIX ANN TECH, P213
   Li Cheng, 2014, P 7 USENIX WORKSH HO
   Li HC, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P83
   Li YL, 2012, ELECTROCHEM COMMUN, V18, P12, DOI 10.1016/j.elecom.2012.01.023
   Liu CY, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P67
   Margaglia F, 2015, IEEE S MASS STOR SYS
   Margaglia F, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P95
   Min Changwoo, 2012, P USENIX C FIL STOR, P1
   Mohan Jayashree, 2017, P 9 USENIX WORKSH HO
   Narayanan D, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P253
   Narayanan K, 2009, 2009 OHIO COLLABORATIVE CONFERENCE ON BIOINFORMATICS, PROCEEDINGS, P9, DOI 10.1109/OCCBIO.2009.27
   Park D, 2011, IEEE S MASS STOR SYS
   Rho Eunhee, 2018, P 16 USENIX C FIL ST
   RISKA A, 2006, P USENIX ANN TECHN C
   Roselli D, 2000, P USENIX ANN TECHN C
   Samsung Electronics, 2011, SAMSUNG NAND TECHNOL
   Samsung Electronics, 2011, 16GB F NAND FLASH MU, V1.1
   Saxena Mohit, 2013, P 11 USENIX C FIL ST
   Shen Zhaoyan, 2019, P 39 IEEE INT C DIST
   Shibata N., 2019, P IEEE INT SOL STAT
   SNIA IOTTA Trace Repository, 2020, YCSB ROCKSDB SSD TRA
   Stoica R, 2013, PROC VLDB ENDOW, V6, P733, DOI 10.14778/2536360.2536372
   University of Massachusetts Amherst, 2014, UMASS TRACE REPOSITO
   Waldspurger Carl, 2017, P USENIX ANN TECHN C
   Wang Feng, 2004, P IEEE S MASS STOR S
   Wu F, 2018, DES AUT CON, DOI [10.1145/3195970.3196051, 10.1109/ICOPS35962.2018.9575531]
   Wu FG, 2021, IEEE T COMPUT, V70, P347, DOI 10.1109/TC.2020.2988257
   Wu Kan, 2019, P 11 USENIX WORKSH H
   Yadgar G., 2016, P 8 USENIX WORKSH HO, P1
   Yadgar G, 2018, ACM T STORAGE, V14, DOI 10.1145/3177886
   Yadgar Gala, 2015, P USENIX WORKSH HOT
   Yadgar Gala, 2015, P 14 USENIX C FIL ST
   Yamashita R, 2017, ISSCC DIG TECH PAP I, P196, DOI 10.1109/ISSCC.2017.7870328
   Yang Pan, 2019, P 11 USENIX WORKSH H
   Yang Y, 2014, IEEE S MASS STOR SYS
   Zhang Jie, 2020, P 18 USENIX C FIL ST
   Zhou D., 2015, P IEEE INT S WORKL C
   Zhou YY, 2001, USENIX ASSOCIATION PROCEEDINGS OF THE 2001 USENIX ANNUAL TECHNICAL CONFERENCE, P91
NR 74
TC 36
Z9 38
U1 2
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD FEB
PY 2021
VL 17
IS 1
AR 8
DI 10.1145/3423137
PG 26
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QG9WR
UT WOS:000617930800007
DA 2024-07-18
ER

PT J
AU Zhang, GY
   Wang, ZF
   Ma, XS
   Yang, SL
   Huang, ZC
   Zheng, WM
AF Zhang, Guangyan
   Wang, Zhufan
   Ma, Xiaosong
   Yang, Songlin
   Huang, Zican
   Zheng, Weimin
TI Determining Data Distribution for Large Disk Enclosures with 3-D Data
   Templates
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Data distribution; data recovery; large disk enclosures; mutually
   orthogonal Latin squares; performance consistency
ID PERFORMANCE
AB Conventional RAID solutions with fixed layouts partition large disk enclosures so that each RAID group uses its own disks exclusively. This achieves good performance isolation across underlying disk groups, at the cost of disk under-utilization and slow RAID reconstruction from disk failures.
   We propose RAID+, a new RAID construction mechanism that spreads both normal I/O and reconstruction workloads to a larger disk pool in a balanced manner. Unlike systems conducting randomized placement, RAID+ employs deterministic addressing enabled by the mathematical properties of mutually orthogonal Latin squares, based on which it constructs 3-D data templates mapping a logical data volume to uniformly distributed disk blocks across all disks. While the total read/write volume remains unchanged, with or without disk failures, many more disk drives participate in data service and disk reconstruction.
   Our evaluation with a 60-drive disk enclosure using both synthetic and real-world workloads shows that RAID+ significantly speeds up data recovery while delivering better normal I/O performance and higher multi-tenant system throughput.
C1 [Zhang, Guangyan; Wang, Zhufan; Yang, Songlin; Huang, Zican; Zheng, Weimin] Tsinghua Univ, Beijing 100084, Peoples R China.
   [Ma, Xiaosong] HBKU, Qatar Comp Res Inst, Doha, Qatar.
C3 Tsinghua University; Qatar Foundation (QF); Hamad Bin Khalifa
   University-Qatar; Qatar Computing Research Institute
RP Zhang, GY (corresponding author), Tsinghua Univ, Beijing 100084, Peoples R China.
EM gyzh@tsinghua.edu.cn; wangzf16@mails.tsinghua.edu.cn; xma@qf.org.qa;
   yangsl14@mails.tsinghua.edu.cn; huangzc16@mails.tsinghua.edu.cn;
   zwm-dcs@tsinghua.edu.cn
RI zheng, wei/IQT-9639-2023; WU, ZHEN/GRN-7688-2022; 杨, 松林/Q-2006-2019
OI WU, ZHEN/0000-0001-8719-057X; 杨, 松林/0000-0001-8885-4083
FU National Key R&D Program of China [2018YFB0203902]; National Natural
   Science Foundation of China [61672315]
FX This work was supported by the National Key R&D Program of China under
   Grant No. 2018YFB0203902, and the National Natural Science Foundation of
   China under Grant No. 61672315.
CR Alvarez GA, 1998, CONF PROC INT SYMP C, P109, DOI 10.1109/ISCA.1998.694767
   [Anonymous], 2009, FAST
   [Anonymous], 1988, A case for redundant arrays of inexpensive disks RAID, DOI DOI 10.1145/50202.50214
   [Anonymous], 2017, UMass trace repository, UMass Smart* dataset - 2017 release
   Bachmat E., 2002, Performance Evaluation Review, V30, P55, DOI 10.1145/511399.511342
   Beaver D., 2010, P USENIX OSDI
   Boldi Paolo, 2008, SIGIR Forum, V42, P33, DOI 10.1145/1480506.1480511
   Bonwick Jeff, 2007, ZFS LASTWORD FILE SY
   Bose R. C., 1960, Transactions of the American Mathematics Society, V95, P191
   Brinkmann A., 2000, SPAA 2000. Twelfth Annual ACM Symposium on Parallel Algorithms and Architectures, P119, DOI 10.1145/341800.341815
   Chentao Wu, 2012, 2012 41st International Conference on Parallel Processing (ICPP 2012), P460, DOI 10.1109/ICPP.2012.32
   Cooper Brian F., 2010, P 1 ACM S CLOUD COMP, P143, DOI [DOI 10.1145/1807128.1807152, 10.1145/1807128.1807152]
   Corbett P, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P1
   Deenadhayalan Veera, 2011, GPFS NATIVE RAID 100
   Edwards Todd, 2017, SANTRICITY OS 11 40
   Eyerman S, 2008, IEEE MICRO, V28, P42, DOI 10.1109/MM.2008.44
   Goel A, 2002, PROC INT CONF DATA, P473, DOI 10.1109/ICDE.2002.994760
   González J, 2004, NAT HAZARD EARTH SYS, V4, P17, DOI 10.5194/nhess-4-17-2004
   Gracia-Tinedo R, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P243
   Hafner JL, 2006, I C DEPEND SYS NETWO, P217, DOI 10.1109/DSN.2006.40
   Holland M., 1993, Digest of Papers FTCS-23 The Twenty-Third International Symposium on Fault-Tolerant Computing, P422, DOI 10.1109/FTCS.1993.627345
   HOLLAND M, 1992, SIGPLAN NOTICES, V27, P23, DOI 10.1145/143371.143383
   Holland Mark C., 2001, THESIS
   Hou Robert Y., 1993, P HAW INT C SYST SCI, V1, P70
   Hsu WW, 2005, ACM T COMPUT SYST, V23, P424, DOI 10.1145/1113574.1113577
   Hu YM, 1996, 23RD ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE, PROCEEDINGS, P169, DOI 10.1145/232974.232991
   Huang Q, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P167, DOI 10.1145/2517349.2522722
   Huawei, 2014, RAID 2 0 TECHN WHIT
   IBM, 2017, IBM spectrum scale RAID
   IBM, 2017, IBM XIV storage system architecture and implementation
   Jenkins R., 1997, HASH FUNCTIONS HASH
   Jeong C, 2014, IEEE INT SYMP INFO, P61, DOI 10.1109/ISIT.2014.6874795
   Kari HH, 1997, IEEE T RELIAB, V46, P193, DOI 10.1109/24.589946
   Khan O., 2012, P 10 USENIX C FIL ST, P251
   LEE EK, 1993, IEEE T COMPUT, V42, P651, DOI 10.1109/12.277289
   Lee JYB, 2002, IEEE T PARALL DISTR, V13, P499, DOI 10.1109/TPDS.2002.1003860
   Lensing PH, 2016, IEEE ACM INT SYMP, P366, DOI 10.1109/CCGrid.2016.28
   Lumb CR, 2000, USENIX ASSOCIATION PROCEEDINGS OF THE FOURTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P87
   Mann HB, 1942, ANN MATH STAT, V13, P418, DOI 10.1214/aoms/1177731539
   Menon J., 1992, Digest of Papers. COMPCON Spring 1992. Thirty-Seventh IEEE Computer Society International Conference (Cat. No.92CH3098-1), P410, DOI 10.1109/CMPCON.1992.186748
   Miranda A, 2014, FAST, V2014, P133
   Miranda A, 2014, ACM T STORAGE, V10, DOI 10.1145/2632230
   MongoDB, 2019, FSYNC ADM COMMAND MO
   MUNTZ RR, 1990, VERY LARGE DATA BASES, P162
   Nagle David., 2004, SC 04, P53
   Narayanan D, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P253
   Netapp Corporation, 2017, LONG DOES IT APPR TA
   Nightingale EdmundB., 2012, P 10 USENIX C OPERAT, P1
   Nightingale T, 1999, PROCEEDINGS OF THE 1999 USENIX ANNUAL TECHNICAL CONFERENCE, P295
   Oracle Corporation, 2013, BETT RAID STRAT HIGH
   Plank JS, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P97
   Seo B., 2005, ACM T STORAGE TOS J, V1, P316
   Sivathanu M, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P15
   Staimer Marc, 2010, POSTRAID ALTERNATIVE
   Swartz Karl L., 2010, 3PAR fast raid: high performance without compromise
   Thereska E, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P213
   Tian L, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 5TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES ( FAST '07), P277
   Wachs M, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 5TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES ( FAST '07), P61
   Wan JG, 2014, IEEE T PARALL DISTR, V25, P1638, DOI 10.1109/TPDS.2013.225
   Wang G, 2007, 13TH PACIFIC RIM INTERNATIONAL SYMPOSIUM ON DEPENDABLE COMPUTING, PROCEEDINGS, P61, DOI 10.1109/PRDC.2007.9
   Wang Gang, 2008, 2008 7th IEEE International Symposium on Network Computing and Applications (NCA), P93, DOI 10.1109/NCA.2008.27
   Wang G, 2009, IEEE 15TH PACIFIC RIM INTERNATIONAL SYMPOSIUM ON DEPENDABLE COMPUTING, PROCEEDINGS, P177, DOI 10.1109/PRDC.2009.36
   Wang G, 2008, PROCEEDINGS OF THE 2008 14TH IEEE INTERNATIONAL CONFERENCE ON PARALLEL AND DISTRIBUTED SYSTEMS, P533, DOI 10.1109/ICPADS.2008.55
   Wang GW, 2008, 2008 PROCEEDINGS OF INFORMATION TECHNOLOGY AND ENVIRONMENTAL SYSTEM SCIENCES: ITESS 2008, VOL 1, P73, DOI 10.1109/PRDC.2008.33
   Wang Zhufan, 2018, RELIABILITY ANAL RAI
   Weil SA, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P307
   Weil Sage A., 2006, P ACM IEEE C SUP SC, P122, DOI [10.1145/1188455.1188582, DOI 10.1145/1188455.1188582]
   Welch B., 2008, P 6 USENIX C FIL STO
   Wilkes J, 1996, ACM T COMPUT SYST, V14, P108, DOI 10.1145/225535.225539
   Wu S., 2009, USENIX C FILE STORAG, V9, P239
   Xie T, 2008, IEEE T COMPUT, V57, P1386, DOI 10.1109/TC.2008.76
   Xin Q, 2004, 13TH IEEE INTERNATIONAL SYMPOSIUM ON HIGH PERFORMANCE DISTRIBUTED COMPUTING, PROCEEDINGS, P172, DOI 10.1109/HPDC.2004.1323523
   Xu L., 1999, IEEE T INFORM THEORY, V45, P1
   Zhang G., 2007, ACM T STORAGE, V3, P1
   Zhang GY, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P279
   Zhang GY, 2010, IEEE T COMPUT, V59, P345, DOI 10.1109/TC.2009.150
   Zheng W., 2011, USENIX C FILE STORAG, P149
   Zhu Xiaowei, 2015, P USENIX ANN TECH C, P375
NR 78
TC 3
Z9 3
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD FEB
PY 2020
VL 15
IS 4
AR 27
DI 10.1145/3342858
PG 38
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NG7LP
UT WOS:000564162600006
DA 2024-07-18
ER

PT J
AU Hu, XM
   Wang, XL
   Zhou, L
   Luo, YW
   Wang, ZL
   Ding, C
   Ye, CC
AF Hu, Xiameng
   Wang, Xiaolin
   Zhou, Lan
   Luo, Yingwei
   Wang, Zhenlin
   Ding, Chen
   Ye, Chencheng
TI Fast Miss Ratio Curve Modeling for Storage Cache
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Cache system; data locality; miss ratio curve
ID WORKING-SETS
AB The reuse distance (least recently used (LRU) stack distance) is an essential metric for performance prediction and optimization of storage cache. Over the past four decades, there have been steady improvements in the algorithmic efficiency of reuse distance measurement. This progress is accelerating in recent years, both in theory and practical implementation.
   In this article, we present a kinetic model of LRU cache memory, based on the average eviction time (AET) of the cached data. The AET model enables fast measurement and use of low-cost sampling. It can produce the miss ratio curve in linear time with extremely low space costs. On storage trace benchmarks, AET reduces the time and space costs compared to former techniques. Furthermore, AET is a composable model that can characterize shared cache behavior through sampling and modeling individual programs or traces.
C1 [Hu, Xiameng; Wang, Xiaolin; Zhou, Lan; Luo, Yingwei] Peking Univ, 5 Yiheyuan Rd, Beijing 100871, Peoples R China.
   [Luo, Yingwei] Peking Univ, SECE, Shenzhen Key Lab Cloud Comp Technol & Applicat, Shenzhen, Peoples R China.
   [Wang, Zhenlin] Michigan Technol Univ, 1400 Townsend Dr, Houghton, MI 49931 USA.
   [Ding, Chen] Univ Rochester, 500 Joseph C Wilson Blvd, Rochester, NY 14627 USA.
   [Ye, Chencheng] Huazhong Univ Sci & Technol, Luoyu Rd 1037, Wuhan 430074, Hubei, Peoples R China.
C3 Peking University; Peking University; Michigan Technological University;
   University of Rochester; Huazhong University of Science & Technology
RP Hu, XM (corresponding author), Peking Univ, 5 Yiheyuan Rd, Beijing 100871, Peoples R China.
EM hxm@pku.edu.cn; wxl@pku.edu.cn; lanzhou@pku.edu.cn; lyw@pku.edu.cn;
   zlwang@mtu.edu; cding@cs.rochester.edu; yechencheng@gmail.com
RI ding, chen/KDN-1285-2024; Ye, chencheng/U-5568-2019; XU,
   Lin/JDM-5554-2023
OI XU, Lin/0000-0003-1781-1638; Ding, Chen/0000-0003-4968-6659; Wang,
   Zhenlin/0000-0002-0429-4371
FU National Science Foundation of China [61232008, 61472008, 61672053,
   U1611461]; Shenzhen Key Research Project [JCYJ20170412150946024];
   National Science Foundation [CSR-1618384, CSR-1422342, CCF-1717877,
   CCF-1629376, CNS-1319617]; IBM CAS Faculty Fellowship; Division Of
   Computer and Network Systems; Direct For Computer & Info Scie & Enginr
   [1422342] Funding Source: National Science Foundation
FX The research is supported in part by the National Science Foundation of
   China (Grants No. 61232008, No. 61472008, No. 61672053, and No.
   U1611461); Shenzhen Key Research Project No: JCYJ20170412150946024; the
   National Science Foundation (Contracts No. CSR-1618384, No. CSR-1422342,
   No. CCF-1717877, No. CCF-1629376, and No. CNS-1319617); and an IBM CAS
   Faculty Fellowship.
CR Allen AO., 2014, PROBABILITY STAT QUE, V2nd
   Almasi George., 2002, Proceedings of the ACM SIGPLAN Workshop on Memory System Performance, P37
   [Anonymous], 1973, OPERATING SYSTEMS TH
   Beckmann N, 2017, INT S HIGH PERF COMP, P109, DOI 10.1109/HPCA.2017.43
   Beyls K, 2006, LECT NOTES COMPUT SC, V4208, P220
   Bjornsson Hjortur, 2013, P 4 ANN S CLOUD COMP, DOI [10.1145/2523616.2527081, DOI 10.1145/2523616.2527081]
   Brock J, 2015, PROC INT CONF PARAL, P749, DOI 10.1109/ICPP.2015.84
   Canim M, 2010, PROC VLDB ENDOW, V3, P1435, DOI 10.14778/1920841.1921017
   Cascaval C, 2005, PACT 2005: 14TH INTERNATIONAL CONFERENCE ON PARALLEL ARCHITECTURES AND COMPILATION TECHNIQUES, P339
   Chandra D, 2005, INT S HIGH PERF COMP, P340, DOI 10.1109/HPCA.2005.27
   Chen ZF, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE GENERAL TRACK, P269
   DENNING PJ, 1978, COMMUN ACM, V21, P750, DOI 10.1145/359588.359598
   DENNING PJ, 1972, COMMUN ACM, V15, P191, DOI 10.1145/361268.361281
   DENNING PJ, 1980, IEEE T SOFTWARE ENG, V6, P64, DOI 10.1109/TSE.1980.230464
   Denning PJ, 2015, GREAT PRINCIPLES OF COMPUTING, P1
   Ding C, 2014, J COMPUT SCI TECH-CH, V29, P692, DOI 10.1007/s11390-014-1460-7
   Drudi Zachary, 2015, LIPICS LEIBN INT P I, V40
   DUESTERWALD E, 2003, P INT C PAR ARCH COM
   Eklov D, 2010, INT SYM PERFORM ANAL, P55, DOI 10.1109/ISPASS.2010.5452069
   Fusy Eric, 2007, P 2007 INT C AN ALG
   Gill BS, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P49
   Hu X., 2015, 2015 USENIX ANN TECH
   Jiang YL, 2010, LECT NOTES COMPUT SC, V6011, P264, DOI 10.1007/978-3-642-11970-5_15
   Kgil Taeho., 2006, P 2006 INT C COMPILE, P103, DOI DOI 10.1145/1176760.1176774
   Kim Y. H., 1991, Performance Evaluation Review, V19, P212, DOI 10.1145/107972.107995
   MATTSON RL, 1970, IBM SYST J, V9, P78, DOI 10.1147/sj.92.0078
   Megiddo N, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE 2ND USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST'03), P115
   Narayanan D., 2008, ACM T STORAGE, V4, P3
   Olken F, 1981, Technical Report LBL- 12370
   Olken Frank, 1981, TECHNICAL REPORT
   Schuff DL, 2010, PACT 2010: PROCEEDINGS OF THE NINETEENTH INTERNATIONAL CONFERENCE ON PARALLEL ARCHITECTURES AND COMPILATION TECHNIQUES, P53, DOI 10.1145/1854273.1854286
   Shen X, 2007, ACM SIGPLAN NOTICES, V42, P55, DOI 10.1145/1190215.1190227
   SLUTZ DR, 1974, COMMUN ACM, V17, P563, DOI 10.1145/355620.361167
   Song Jiang, 2002, Performance Evaluation Review, V30, P31, DOI 10.1145/511399.511340
   Suh G.E., 2014, ACM International Conference on Supercomputing 25th Anniversary, P323
   Suh GE, 2001, P 15 INT C SUP, P1
   Tam DK, 2009, ACM SIGPLAN NOTICES, V44, P121, DOI 10.1145/1508284.1508259
   VITTER JS, 1985, ACM T MATH SOFTWARE, V11, P37, DOI 10.1145/3147.3165
   Waldspurger C. A., 2015, 13 USENIX C FILE STO, P95
   Waldspurger CA, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P487
   Wang Xiaolin, 2015, CCGRID
   Wires J., 2014, P S OP SYST DES IMPL, P335
   Wong TM, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE GENERAL TRACK, P161
   Xiang XY, 2013, ACM SIGPLAN NOTICES, V48, P343, DOI 10.1145/2499368.2451153
   Xiang XY, 2011, ACM SIGPLAN NOTICES, V46, P91, DOI 10.1145/2038037.1941567
   Xiaoya Xiang, 2011, Proceedings 2011 International Conference on Parallel Architectures and Compilation Techniques (PACT), P350, DOI 10.1109/PACT.2011.66
   Yadgar G, 2008, INT CON DISTR COMP S, P722, DOI 10.1109/ICDCS.2008.29
   Zhang X, 2009, EUROSYS'09: PROCEEDINGS OF THE FOURTH EUROSYS CONFERENCE, P89
   Zhong YT, 2009, ACM T PROGR LANG SYS, V31, DOI 10.1145/1552309.1552310
   Zhong YT, 2008, ISMM'08: PROCEEDINGS OF THE 2008 INTERNATIONAL SYMPOSIUM ON MEMORY MANAGEMENT, P91
   Zhou P, 2004, ACM SIGPLAN NOTICES, V39, P177, DOI 10.1145/1037187.1024415
NR 51
TC 24
Z9 30
U1 0
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAY
PY 2018
VL 14
IS 2
AR 12
DI 10.1145/3185751
PG 34
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GI6TM
UT WOS:000434635800001
OA Bronze
DA 2024-07-18
ER

PT J
AU Teng, DJ
   Guo, L
   Lee, RB
   Chen, F
   Zhang, YF
   Ma, SY
   Zhang, XD
AF Teng, Dejun
   Guo, Lei
   Lee, Rubao
   Chen, Feng
   Zhang, Yanfeng
   Ma, Siyuan
   Zhang, Xiaodong
TI A Low-cost Disk Solution Enabling LSM-tree to Achieve High Performance
   for Mixed Read/Write Workloads
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE LSM-tree; compaction; buffer cache
AB LSM-tree has been widely used in data management production systems for write-intensive workloads. However, as read and write workloads co-exist under LSM-tree, data accesses can experience long latency and low throughput due to the interferences to buffer caching from the compaction, a major and frequent operation in LSM-tree. After a compaction, the existing data blocks are reorganized and written to other locations on disks. As a result, the related data blocks that have been loaded in the buffer cache are invalidated since their referencing addresses are changed, causing serious performance degradations.
   To re-enable high-speed buffer caching during intensive writes, we propose Log-Structured buffered-Merge tree (simplified as LSbM-tree) by adding a compaction buffer on disks to minimize the cache invalidations on buffer cache caused by compactions. The compaction buffer efficiently and adaptively maintains the frequently visited datasets. In LSbM, strong locality objects can be effectively kept in the buffer cache with minimum or no harmful invalidations. With the help of a small on-disk compaction buffer, LSbM achieves a high query performance by enabling effective buffer caching, while retaining all the merits of LSM-tree for write-intensive data processing and providing high bandwidth of disks for range queries. We have implemented LSbM based on LevelDB. We show that with a standard buffer cache and a hard disk, LSbM can achieve 2x performance improvement over LevelDB. We have also compared LSbM with other existing solutions to show its strong cache effectiveness.
C1 [Teng, Dejun; Lee, Rubao; Zhang, Yanfeng; Ma, Siyuan; Zhang, Xiaodong] Ohio State Univ, Comp Sci & Engn Dept, Columbus, OH 43210 USA.
   [Guo, Lei] Google Inc, Mountain View, CA USA.
   [Chen, Feng] Louisiana State Univ, Dept Comp Sci & Engn, Baton Rouge, LA 70803 USA.
   [Zhang, Yanfeng] Northeastern Univ, Shenyang, Liaoning, Peoples R China.
C3 University System of Ohio; Ohio State University; Google Incorporated;
   Louisiana State University System; Louisiana State University;
   Northeastern University - China
RP Teng, DJ (corresponding author), Ohio State Univ, Comp Sci & Engn Dept, Columbus, OH 43210 USA.
EM teng.102@osu.edu; leguo@google.com; liru@cse.ohio-state.edu;
   fchen@csc.lsu.edu; zhangyf@mail.neu.edu.cn; ma.588@osu.edu;
   zhang@cse.ohio-state.edu
RI Zhang, Yanfeng/B-9126-2014
FU National Science Foundation [OCI-1147522, CNS-1162165, CCF-1453705,
   CCF-1513944, CCF-1629403, CCF-1629291]; Louisiana Board of Regents
   [LEQSF(2014-17)-RD-A-01]; Division of Computing and Communication
   Foundations; Direct For Computer & Info Scie & Enginr [1453705] Funding
   Source: National Science Foundation
FX This work has been partially supported by the National Science
   Foundation under grants OCI-1147522, CNS-1162165, CCF-1453705,
   CCF-1513944, CCF-1629403, and CCF-1629291 and a grant from the Louisiana
   Board of Regents LEQSF(2014-17)-RD-A-01.
CR Ahmad MY, 2015, PROC VLDB ENDOW, V8, P850, DOI 10.14778/2757807.2757810
   [Anonymous], 2014, P 9 EUROPEAN C COMPU
   Apache, 2017, CASS
   Apache, 2017, HBASE
   Atikoglu Berk, 2012, Performance Evaluation Review, V40, P53, DOI 10.1145/2318857.2254766
   Basho, 2017, RIAK
   Bendre M, 2015, PROC VLDB ENDOW, V8, P2001
   Broder Andrei, 2003, Internet Math, V1, DOI DOI 10.1080/15427951.2004.10129096
   Chang F, 2008, ACM T COMPUT SYST, V26, DOI 10.1145/1365815.1365816
   Chen F, 2011, INT S HIGH PERF COMP, P266, DOI 10.1109/HPCA.2011.5749735
   Cooper Brian F., 2010, P 1 ACM S CLOUD COMP, P143, DOI [DOI 10.1145/1807128.1807152, 10.1145/1807128.1807152]
   Facebook, 2017, ROCKSDB
   Golan-Gueta G., 2015, P 10 EUR C COMP SYST, p32:1, DOI DOI 10.1145/2741948.2741973
   Google, 2017, LEVELDB
   Guo L., 2016, ARXIV160602015
   Jagadish HV, 1997, PROCEEDINGS OF THE TWENTY-THIRD INTERNATIONAL CONFERENCE ON VERY LARGE DATABASES, P16
   Li Yanrui, 2010, J Biomed Biotechnol, V2010, P716515, DOI 10.1155/2010/716515
   Lim H, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P149
   Lu LY, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P133
   Mcguire Ryan, 2014, COMPACTION IMPROVEME
   ONeil P, 1996, ACTA INFORM, V33, P351, DOI 10.1007/s002360050048
   Sears Russell, 2012, P ACM SIGMOD INT C M, P217
   Shetty P.J., 2013, 11 USENIX C FILE STO, P17
   Teng DJ, 2017, INT CON DISTR COMP S, P68, DOI 10.1109/ICDCS.2017.70
   Wu XN, 2015, METHODS MOL BIOL, V1306, P71, DOI 10.1007/978-1-4939-2648-0_5
NR 25
TC 6
Z9 8
U1 0
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAY
PY 2018
VL 14
IS 2
AR 15
DI 10.1145/3162615
PG 26
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GI6TM
UT WOS:000434635800004
DA 2024-07-18
ER

PT J
AU Han, L
   Shen, ZY
   Liu, D
   Shao, ZL
   Huang, HHW
   Li, T
AF Han, Lei
   Shen, Zhaoyan
   Liu, Duo
   Shao, Zili
   Huang, H. Howie
   Li, Tao
TI A Novel ReRAM-Based Processing-in-Memory Architecture for Graph
   Traversal
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE ReRAM; BFS; processing-in-memory; architecture
AB Graph algorithms such as graph traversal have been gaining ever-increasing importance in the era of big data. However, graph processing on traditional architectures issues many random and irregular memory accesses, leading to a huge number of data movements and the consumption of very large amounts of energy. To minimize the waste of memory bandwidth, we investigate utilizing processing-in-memory (PIM), combined with non-volatile metal-oxide resistive random access memory (ReRAM), to improve both computation and I/O performance.
   We propose a new ReRAM-based processing-in-memory architecture called RPBFS, in which graph data can be persistently stored and processed in place. We study the problem of graph traversal, and we design an efficient graph traversal algorithm in RPBFS. Benefiting from low data movement overhead and high bank-level parallel computation, RPBFS shows a significant performance improvement compared with both the CPU-based and the GPU-based BFS implementations. On a suite of real-world graphs, our architecture yields a speedup in graph traversal performance of up to 33.8x, and achieves a reduction in energy over conventional systems of up to 142.8x.
C1 [Han, Lei; Shen, Zhaoyan; Shao, Zili] Hong Kong Polytech Univ, Dept Comp, Mong ManWai Bldg, Hong Kong, Hong Kong, Peoples R China.
   [Liu, Duo] Chongqing Univ, Coll Comp Sci, 174 Shazhengjie, Chongqing, Peoples R China.
   [Huang, H. Howie] George Washington Univ, Dept Elect & Comp Engn, 801 22nd St NW, Washington, DC USA.
   [Li, Tao] Univ Florida, Dept Elect & Comp Engn, 339D Larsen Hall, Gainesville, FL USA.
C3 Hong Kong Polytechnic University; Chongqing University; George
   Washington University; State University System of Florida; University of
   Florida
RP Shao, ZL (corresponding author), Hong Kong Polytech Univ, Dept Comp, Mong ManWai Bldg, Hong Kong, Hong Kong, Peoples R China.
EM cslhan@comp.polyu.edu.hk; cszyshen@comp.polyu.edu.hk; liuduo@cqu.edu.cn;
   cszlshao@comp.polyu.edu.hk; howie@gwu.edu; taoli@ece.ufl.edu
RI Shao, Zili/AAX-3339-2020; Shen, Zhaoyan/T-3711-2019
OI Shao, Zili/0000-0002-2173-2847; 
FU Research Grants Council of the Hong Kong Special Administrative Region,
   China [GRF 15222315/15E, GRF 152736/16E, GRF 152066/17E]; National
   Natural Science Foundation of China [61672116]; Chongqing High-Tech
   Research Program [cstc2016jcyjA0332]
FX The work described in this article is partially supported by the grants
   from the Research Grants Council of the Hong Kong Special Administrative
   Region, China (GRF 15222315/15E, GRF 152736/16E, and GRF 152066/17E), by
   the National Natural Science Foundation of China (61672116), and by the
   Chongqing High-Tech Research Program cstc2016jcyjA0332.
CR Ahn J, 2015, 2015 ACM/IEEE 42ND ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA), P336, DOI 10.1145/2749469.2750385
   Akin B, 2015, 2015 ACM/IEEE 42ND ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA), P131, DOI 10.1145/2749469.2750397
   Alibart F, 2012, NANOTECHNOLOGY, V23, DOI 10.1088/0957-4484/23/7/075201
   [Anonymous], 2016, NANOADV
   [Anonymous], 2017, P 2017 IEEE 6 NONVOL, DOI DOI 10.1109/NVMSA.2017.8064464
   [Anonymous], 2003, P WORKSH EC PEER PEE
   [Anonymous], 1999, WWW 1999
   [Anonymous], 2013, Proceedings of the 27th international ACM conference on International conference on supercomputing, DOI DOI 10.1145/2464996.2465013
   [Anonymous], ARXIV170806248
   [Anonymous], INTEL POWER GADGET
   [Anonymous], 2017, TECHNICAL REPORT
   [Anonymous], P 21 ACM SIGPLAN S P
   [Anonymous], 2011, Proceedings of the Design, Automation Test in Europe
   [Anonymous], 2013, 50 ACM EDAC IEEE DES
   Balasubramonian R, 2014, IEEE MICRO, V34, P36, DOI 10.1109/MM.2014.55
   Beamer S, 2015, I S WORKL CHAR PROC, P56, DOI 10.1109/IISWC.2015.12
   Beamer S, 2013, SCI PROGRAMMING-NETH, V21, P137, DOI [10.3233/SPR-130370, 10.1155/2013/702694]
   Chen RX, 2015, INT J PHOTOENERGY, V2015, DOI 10.1155/2015/183468
   Chi P, 2016, CONF PROC INT SYMP C, P27, DOI 10.1109/ISCA.2016.13
   Cormen T.H., 2009, INTRO ALGORITHMS
   Dagum L, 1998, IEEE COMPUT SCI ENG, V5, P46, DOI 10.1109/99.660313
   Dong Xiangyu., 2014, Emerging Memory Technologies, P15
   Karypis G, 1998, SIAM J SCI COMPUT, V20, P359, DOI 10.1137/S1064827595287997
   Leskovec J., 2015, Large Network Dataset Collection
   Li J., 2011, Memory Workshop (IMW), 2011 3rd IEEE International, P1
   Liu D, 2014, IEEE T COMPUT AID D, V33, P1450, DOI 10.1109/TCAD.2014.2341922
   Liu H, 2016, SIGMOD'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P403, DOI 10.1145/2882903.2882959
   Liu Hang., 2015, Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, page, P68
   Liu XF, 2015, ADV METEOROL, V2015, DOI 10.1155/2015/950262
   Merrill D, 2012, ACM SIGPLAN NOTICES, V47, P117, DOI 10.1145/2370036.2145832
   Mirzadeh N., 2015, P 5 WORKSH ARCH SYST, P1
   Niu DM, 2013, ICCAD-IEEE ACM INT, P17, DOI 10.1109/ICCAD.2013.6691092
   Nurvitadhi E, 2014, ANN IEEE SYM FIELD P, P25, DOI 10.1109/FCCM.2014.15
   Ozdal MM, 2016, CONF PROC INT SYMP C, P166, DOI 10.1109/ISCA.2016.24
   Pugsley SH, 2014, INT SYM PERFORM ANAL, P190, DOI 10.1109/ISPASS.2014.6844483
   Qin ZW, 2011, DES AUT CON, P17
   Roy A, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P472, DOI 10.1145/2517349.2522740
   Salihoglu S., 2013, P 25 INT C SCI STAT, P22
   Seshadri Vivek, 2013, 2013 46th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO). Proceedings, P185, DOI 10.1145/2540708.2540725
   Shafiee A, 2016, CONF PROC INT SYMP C, P14, DOI 10.1109/ISCA.2016.12
   Shim H., 2003, ACM Transactions on Embedded Computing Systems, V2, P98, DOI 10.1145/513918.514138
   Song LH, 2017, INT S HIGH PERF COMP, P541, DOI 10.1109/HPCA.2017.55
   Sun YL, 2017, IEEE NON-VOLATILE ME
   Sungpack Hong, 2011, Proceedings 2011 International Conference on Parallel Architectures and Compilation Techniques (PACT), P78, DOI 10.1109/PACT.2011.14
   Wong HSP, 2012, P IEEE, V100, P1951, DOI 10.1109/JPROC.2012.2190369
   Xu C, 2015, INT S HIGH PERF COMP, P476, DOI 10.1109/HPCA.2015.7056056
   Xu C, 2014, ICCAD-IEEE ACM INT, P55, DOI 10.1109/ICCAD.2014.7001329
   Zhang D., 2014, P 23 INT S HIGH PERF
   Zhang H, 2016, DES AUT TEST EUROPE, P756
   Zhang JL, 2017, FPGA'17: PROCEEDINGS OF THE 2017 ACM/SIGDA INTERNATIONAL SYMPOSIUM ON FIELD-PROGRAMMABLE GATE ARRAYS, P207, DOI 10.1145/3020078.3021737
   Zhong JL, 2014, IEEE T PARALL DISTR, V25, P1543, DOI 10.1109/TPDS.2013.111
   Zhu Q., 2013, IEEE HIGH PERFORMANC, P1
   Zhu Q, 2013, P 2013 IEEE INT 3D S
NR 53
TC 23
Z9 24
U1 0
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD APR
PY 2018
VL 14
IS 1
SI SI
AR 9
DI 10.1145/3177916
PG 26
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GH5ZN
UT WOS:000433517600009
DA 2024-07-18
ER

PT J
AU Sun, YL
   Wang, Y
   Yang, HZ
AF Sun, Yuliang
   Wang, Yu
   Yang, Huazhong
TI Bidirectional Database Storage and SQL Query Exploiting RRAM-Based
   Process-in-Memory Structure
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE NVM-based databases; process-in-memory; RRAM
ID SYSTEM
AB With the coming of the "Big Data" era, a high-energy-efficiency database is demanded for the Internet of things (IoT) application scenarios. The emerging Resistive Random Access Memory (RRAM) has been considered as an energy-efficient replacement of DRAM for next-generation main memory. In this article, we propose an RRAM-based SQL query unit with process-in-memory (PIM) characteristics. A bidirectional storage structure for a database in RRAM crossbar array is proposed that avoids redundant data transfer to cache and reduces cache miss rate compared with the storage method in DRAM for an in-memory database. The proposed RRAM-based SQL query unit can support a representative subset of SQL queries in memory and thus can further reduce the data transfer cost. The corresponding query optimization method is proposed to fully utilize the PIM characteristics. Simulation results show that the energy efficiency of the proposed RRAM-based SQL query unit is increased by 4 to 6 orders of magnitudes compared with the traditional architecture.
C1 [Sun, Yuliang; Wang, Yu; Yang, Huazhong] Tsinghua Univ, TNList, Elect Engn Dept, Rohm Bldg, Beijing, Peoples R China.
C3 Tsinghua University
RP Wang, Y (corresponding author), Tsinghua Univ, TNList, Elect Engn Dept, Rohm Bldg, Beijing, Peoples R China.
EM sunyuliang13@mail.tsinghua.edu.cn; yu-wang@mail.tsinghua.edu.cn;
   yanghz@tsinghua.edu.cn
RI WANG, Yu/B-7985-2011; Wang, Fei/KEH-6292-2024
FU National Key R&D Program of China [2017YFA0207600]; National Natural
   Science Foundation of China [61622403, 61621091]; Equipment pre-Research
   [6141A02022608]; Ministry of Education [6141A02022608]; Tsinghua
   National Laboratory for Information Science and Technology (TNList)
FX This work was supported by the National Key R&D Program of China (grant
   no. 2017YFA0207600), the National Natural Science Foundation of China
   (grant nos. 61622403, 61621091), the Joint Fund of Equipment
   pre-Research and Ministry of Education (grant no. 6141A02022608), and
   the Tsinghua National Laboratory for Information Science and Technology
   (TNList).
CR [Anonymous], PIPELAYER PIPELINED
   [Anonymous], IEEE T COMPUT AIDED
   [Anonymous], PCM
   Bakkum P., 2010, Proceedings of the 3rd Workshop on General-Purpose Computation on Graphics Processing Units - GPGPU '10 (New York, New York, USA, 2010), P94, DOI DOI 10.1145/1735688.1735706
   Casper Jared, 2014, Proceedings of the 2014 ACM/SIGDA international symposium on Field-programmable gate arrays, P151, DOI DOI 10.1145/2554688.2554787
   Catanzaro M., 2012, 2012 IEEE 25th International SOC Conference (SOCC), P94, DOI 10.1109/SOCC.2012.6398384
   Chatzistergiou A, 2015, PROC VLDB ENDOW, V8, P497, DOI 10.14778/2735479.2735483
   Chaudhuri S., 1998, Proceedings of the Seventeenth ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems. PODS 1998, P34, DOI 10.1145/275487.275492
   Chen S. Y-S, 2011, 2011 Symposium on VLSI Circuits. Digest of Technical Papers, P66
   Chi P, 2016, CONF PROC INT SYMP C, P27, DOI 10.1109/ISCA.2016.13
   Dennl C, 2013, ANN IEEE SYM FIELD P, P25, DOI 10.1109/FCCM.2013.38
   Dennl C, 2012, ANN IEEE SYM FIELD P, P45, DOI 10.1109/FCCM.2012.18
   Gao LG, 2016, IEEE ELECTR DEVICE L, V37, P870, DOI 10.1109/LED.2016.2573140
   Gehrke J, 2004, IEEE PERVAS COMPUT, V3, P46, DOI 10.1109/MPRV.2004.1269131
   Hu M, 2016, DES AUT CON, DOI 10.1145/2897937.2898010
   Jo I, 2016, PROC VLDB ENDOW, V9, P924
   Kimura H, 2015, SIGMOD'15: PROCEEDINGS OF THE 2015 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P691, DOI 10.1145/2723372.2746480
   Kramer MJ, 2015, IEEE J SOLID-ST CIRC, V50, P2891, DOI 10.1109/JSSC.2015.2463110
   Li H., 2011, Nonvolatile Memory Design: Magnetic, Resistive, and Phase Change
   McKee S. A., 2004, P 1 C COMPUTING FRON, P162, DOI [10.1145/977091.977115, DOI 10.1145/977091.977115]
   Plattner H, 2011, IN- MEMORY DATA MANAGEMENT: AN INFLECTION POINT FOR ENTERPRISE APPLICATIONS, P1, DOI 10.1007/978-3-642-19363-7
   Shafiee A, 2016, CONF PROC INT SYMP C, P14, DOI 10.1109/ISCA.2016.12
   Silberschatz A., 1997, Database system concepts, V4
   Tu YC, 2014, SIGMOD REC, V43, P21, DOI 10.1145/2627692.2627696
   Viglas SD, 2015, SIGMOD'15: PROCEEDINGS OF THE 2015 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P1707, DOI 10.1145/2723372.2731082
   Wu LS, 2015, IEEE MICRO, V35, P34, DOI 10.1109/MM.2015.51
   Yu SM, 2013, ADV MATER, V25, P1774, DOI 10.1002/adma.201203680
   Zhao W, 2007, ACM J EMERG TECH COM, V3, DOI 10.1145/1229175.1229176
   Ziener D, 2016, ACM T RECONFIG TECHN, V9, DOI 10.1145/2845087
NR 29
TC 4
Z9 5
U1 0
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD APR
PY 2018
VL 14
IS 1
SI SI
AR 8
DI 10.1145/3177917
PG 19
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GH5ZN
UT WOS:000433517600008
DA 2024-07-18
ER

PT J
AU Kim, SH
   Lee, J
   Kim, JS
AF Kim, Sang-Hoon
   Lee, Jinhyuk
   Kim, Jin-Soo
TI GCMix: An Efficient Data Protection Scheme against the Paired Page
   Interference
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Flash memory; flash memory cells; multi-level cells; paired page
   interference; locality
AB In multi-level cell (MLC) NAND flash memory, two logical pages are overlapped on a single physical page. Even after a logical page is programmed, the data can be corrupted if the programming of the coexisting logical page is interrupted.
   This phenomenon is called paired page interference. This article proposes a novel software technique to deal with the paired page interference without any additional hardware or extra page write. The proposed technique utilizes valid pages in the victim block during garbage collection (GC) as the backup against the interference, and pairs them with incoming pages written by the host. This approach eliminates undesirable page copy to backup pages against the interference. However, such a strategy has an adverse effect on the hot/cold separation policy, which is essential to improve the efficiency of GC. To limit the downside, we devise a metric to estimate the benefit of GCMix on-the-fly so that GCMix can be adaptively utilized only when the benefit outweighs the overhead. Evaluations using synthetic and real workloads show GCMix can effectively deal with the paired page interference, reducing the write amplification factor by up to 17.5% compared to the traditional technique, while providing comparable I/O performance.
C1 [Kim, Sang-Hoon] Virginia Polytech Inst & State Univ, Bradley Dept Elect & Comp Engn, 250 Perry St, Blacksburg, VA 24061 USA.
   [Kim, Sang-Hoon] Korea Adv Inst Sci & Technol, Daejeon, South Korea.
   [Lee, Jinhyuk] Samsung Elect Co, Memory Div, 1 Samsungjeonja Ro, Hwaseong 18448, South Korea.
   [Lee, Jinhyuk] Sungkyunkwan Univ, Suwon, South Korea.
   [Kim, Jin-Soo] Sungkyunkwan Univ, Coll Informat & Commun Engn, 2066 Seobu Ro, Suwon 16419, South Korea.
C3 Virginia Polytechnic Institute & State University; Korea Advanced
   Institute of Science & Technology (KAIST); Samsung; Samsung Electronics;
   Sungkyunkwan University (SKKU); Sungkyunkwan University (SKKU)
RP Kim, SH (corresponding author), Virginia Polytech Inst & State Univ, Bradley Dept Elect & Comp Engn, 250 Perry St, Blacksburg, VA 24061 USA.; Kim, SH (corresponding author), Korea Adv Inst Sci & Technol, Daejeon, South Korea.
EM sanghoon@vt.edu; jinhyuk79.lee@samsung.com; jinsookim@skku.edu
RI Kim, Sang-Hoon/AAD-8797-2021
OI Kim, Sang-Hoon/0000-0002-2105-1734
FU National Research Foundation of Korean (NRF) - Korea Government (MSIT)
   [2016R1A2A1A05005494]
FX This work was supported by the National Research Foundation of Korean
   (NRF) grant funded by the Korea Government (MSIT) (No.
   2016R1A2A1A05005494).
CR [Anonymous], 2012, P 10 USENIX C FIL ST
   Ban Amir, 1995, US Patent, Patent No. 540485
   Chen F, 2009, PERF E R SI, V37, P181
   Chiang ML, 1999, SOFTWARE PRACT EXPER, V29, P267, DOI 10.1002/(SICI)1097-024X(199903)29:3<267::AID-SPE233>3.0.CO;2-T
   Dirik C., 2009, P 36 ANN INT S COMP
   Grupp L.M., 2013, Proceedings of the 2013 USENIX Conference on Annual Technical Conference, USENIX ATC'13, USENIX Association, Berkeley, CA, USA, P79
   GUPTA A., 2009, P INT C ARCH SUPP PR
   Im S, 2010, J SYST ARCHITECT, V56, P641, DOI 10.1016/j.sysarc.2010.09.005
   Jen-Wei Hsieh, 2006, ACM Transaction on Storage, V2, P22, DOI 10.1145/1138041.1138043
   Kavalanekar Swaroop, 2008, P 2008 IEEE INT S WO
   KAWAGUCHI A, 1995, P WINT 1995 USENIX T
   Kim Hyojun, 2008, P 2008 INT C CONS EL
   Kim JW, 2002, IEEE T CONSUM ELECTR, V48, P275, DOI 10.1109/TCE.2002.1010132
   Kwon Min Cheol, 2011, US Patent, Patent No. 20110093650
   Lee J, 2014, IEEE T COMPUT AID D, V33, P1110, DOI 10.1109/TCAD.2014.2309857
   Lee Jongsung, 2013, P 6 INT SYST STOR C
   Lee S.W., 2008, Proceedings of the 2008 ACM SIGMOD international conference on Management of data, SIGMOD '08, P1075
   Lee S.-W., 2009, P 2009 ACM SIGMOD IN
   Lee SW, 2006, LECT NOTES COMPUT SC, V4097, P879
   Lee SW, 2007, ACM T EMBED COMPUT S, V6, DOI 10.1145/1275986.1275990
   Moshayedi M., 2008, ACM Queue, V8, P32
   Narayanan Dushyanth, 2008, MSRTR2008169
   Park Dongchul, 2011, P 2011 IEEE 27 S MAS
   Park Khyugmin, 2008, P 2008 CELF EMB LIN
   Prabhakaran Vijayan, 2005, P 2000 USENIX ANN TE
   ROOHPARVAR F. F, 2008, US Patent, Patent No. 7366013
   Samsung Electronics Co. Ltd., 2006, 2G X 8 BIT NAND FLAS
   Samsung Electronics Co. Ltd, 2010, 32GB A DIE NAND FLAS
   Sanvido MAA, 2008, P IEEE, V96, P1864, DOI 10.1109/JPROC.2008.2004319
   SNIA IOTTA Repository, 2011, MICR ENT TRAC EXCH S
   Sungjin Lee, 2008, Operating Systems Review, V42, P36, DOI 10.1145/1453775.1453783
   VASUDEVA ANIL., 2011, Are SSDs ready for enterprise storage systems?
   Wang WG, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P145
   Yoon Han Bin, 2011, US Patent, Patent No. 20110199822
   Yu Jae-Sung, 2010, US Patent, Patent No. 7755950
   Zipf G. K., 1932, SELECTED STUDIES PRI
NR 36
TC 0
Z9 0
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD DEC
PY 2017
VL 13
IS 4
SI SI
AR 37
DI 10.1145/3149373
PG 23
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FR4LD
UT WOS:000419035900010
DA 2024-07-18
ER

PT J
AU Zeng, LF
   Zhang, ZH
   Wang, Y
   Feng, D
   Kent, KB
AF Zeng, Lingfang
   Zhang, Zehao
   Wang, Yang
   Feng, Dan
   Kent, Kenneth B.
TI CosaFS: A Cooperative Shingle-Aware File System
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE File system; shingled magnetic recording; shingled write disk;
   solid-state drive; garbage collection
AB In this article, we design and implement a cooperative shingle-aware file system, called CosaFS, on heterogeneous storage devices that mix solid-state drives (SSDs) and shingled magnetic recording (SMR) technology to improve the overall performance of storage systems. The basic idea of CosaFS is to classify objects as hot or cold objects based on a proposed Lookahead with Recency Weight scheme. If an object is identified as a hot (small) object, then it will be served by SSD. Otherwise, cold (large) objects are stored on SMR. For an SMR, large objects can be accessed in large sequential blocks, rendering the performance of their accesses comparable with that of accessing the same large sequential blocks as if they were stored on a hard drive. Small objects, such as inodes and directories, are stored on the SSD where "seeks" for such objects are nearly free. With thorough empirical studies, we demonstrate that CosaFS, as a cooperative shingle-aware file system, with metadata separation and cache-assistance, is a very effective way to handle the disk-based data demanded by the shingled writes and outperforms the device-and host-side shingle-aware file systems in terms of throughput, IOPS, and access latency as well.
C1 [Zeng, Lingfang; Zhang, Zehao; Feng, Dan] Huazhong Univ Sci & Technol, Wuhan, Hubei, Peoples R China.
   [Wang, Yang] Chinese Acad Sci, Shenzhen Inst Adv Technol, Beijing, Peoples R China.
   [Kent, Kenneth B.] Univ New Brunswick, Fac Comp Sci, Fredericton, NB E3B 5A3, Canada.
   [Zeng, Lingfang; Zhang, Zehao; Feng, Dan] Luoyu Rd 1037, Wuhan 430074, Hubei, Peoples R China.
   [Wang, Yang] 1068 Xueyuan Blvd, Shenzhen 518055, Peoples R China.
C3 Huazhong University of Science & Technology; Chinese Academy of
   Sciences; Shenzhen Institute of Advanced Technology, CAS; University of
   New Brunswick
RP Wang, Y (corresponding author), 1068 Xueyuan Blvd, Shenzhen 518055, Peoples R China.
EM lfzeng@hust.edu.cn; zhangzehao@hust.edu.cn; yang.wang1@siat.ac.cn;
   dfeng@hust.edu.cn; ken@unb.ca
OI Kent, Kenneth B/0000-0003-2764-823X; Zeng, Lingfang/0000-0003-3130-3015
FU China National Basic Research 973 Program [2015CB352400]; National
   Natural Science Foundation of China [61472153, 61672513]; Hubei Natural
   Science Foundation [2015CFB192]; Science and Technology Planning Project
   of Guangdong Province [2015B010129011, 2016A030313183]; Research Program
   of Shenzhen [JSGG20150512145714248, JSGG20160229200957727]; Shenzhen
   Fundamental Research Foundation [JCYJ20150630114942277]; Shenzhen
   Oversea High-Caliber Personnel Innovation Funds [KQCX20170331161854];
   Wuhan National Laboratory for Optoelectronics Key Laboratory of
   Information Storage System; Ministry of Education of China School of
   Computer Science and Technology; Huazhong University of Science and
   Technology; Natural Sciences and Engineering Research Council of Canada
   (NSERC)
FX This work is supported in part by the China National Basic Research 973
   Program (No. 2015CB352400), the National Natural Science Foundation of
   China (61472153 and 61672513), the Hubei Natural Science Foundation
   (2015CFB192), Science and Technology Planning Project of Guangdong
   Province (2015B010129011 and 2016A030313183), Research Program of
   Shenzhen (JSGG20150512145714248 and JSGG20160229200957727), Shenzhen
   Fundamental Research Foundation (JCYJ20150630114942277), Shenzhen
   Oversea High-Caliber Personnel Innovation Funds (KQCX20170331161854),
   Wuhan National Laboratory for Optoelectronics Key Laboratory of
   Information Storage System, Ministry of Education of China School of
   Computer Science and Technology, Huazhong University of Science and
   Technology, and in part by the Natural Sciences and Engineering Research
   Council of Canada (NSERC).
CR Aghayev Abutalib., 2015, P 13 USENIX C FILE S, P135
   Amer A, 2011, IEEE T MAGN, V47, P3691, DOI 10.1109/TMAG.2011.2157115
   Amer Ahmed., 2010, Mass Storage Systems and Technologies (MSST), 2010 IEEE 26th Symposium on, P1, DOI DOI 10.1109/MSST.2010.5496991
   Cassuto Y., 2010, Proceedings of the 2010 IEEE 26th Symposium on Mass Storage Systems and Technologies (MSST), P1, DOI DOI 10.1109/MSST.2010.5496971
   Chang Dar-Der, 2011, US Patent, Patent No. [20 110 304 935, 20110304935]
   Feldman T., 2013, Login Mag. USENIX SAGE, V38, P22
   Gibson G., 2011, CMUPDL11107 CARN MEL
   Gibson Garth, 2009, CMUPDL09104 CARN MEL
   Gim JM, 2010, ACM T STORAGE, V6, DOI 10.1145/1807060.1807063
   Hatfield Jim, 2011, PROJECT PROPOSAL BAN
   Hatfield Jim, 2011, ACS 3 BANDED DEVICES
   He W, 2014, 6 USENIX WORKSH HOT, P1
   Jin C, 2014, IEEE S MASS STOR SYS
   Kasiraj P., 2009, US Patent, Patent No. [7,490,212, 7490212]
   Moal Damien Le, 2012, P INT C CONS EL ICCE
   New Richard, 2011, US Patent, Patent No. [7,996,645, 7996645]
   Park Dongchul, 2011, P IEEE 27 S MASS STO, P1
   Pitchumani R., 2012, 2012 IEEE 20th International Symposium on Modelling, Analysis & Simulation of Computer and Telecommunication Systems (MASCOTS), P339, DOI 10.1109/MASCOTS.2012.46
   Suresh Anand, 2012, CMUPDL12105 CARN MEL
   Tagawa I., 2009, P IEEE INT MAGN C
   Wood R, 2009, IEEE T MAGN, V45, P917, DOI 10.1109/TMAG.2008.2010676
NR 21
TC 5
Z9 5
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD DEC
PY 2017
VL 13
IS 4
SI SI
AR 34
DI 10.1145/3149482
PG 23
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FR4LD
UT WOS:000419035900007
DA 2024-07-18
ER

PT J
AU Viotti, P
   Dobre, D
   Vukolic, M
AF Viotti, Paolo
   Dobre, Dan
   Vukolic, Marko
TI Hybris: Robust Hybrid Cloud Storage
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Cloud storage; hybrid cloud; reliability; consistency
ID CONSISTENCY
AB Besides well-known benefits, commodity cloud storage also raises concerns that include security, reliability, and consistency. We present Hybris key-value store, the first robust hybrid cloud storage system, aiming at addressing these concerns leveraging both private and public cloud resources.
   Hybris robustly replicates metadata on trusted private premises (private cloud), separately from data, which are dispersed (using replication or erasure coding) across multiple untrusted public clouds. Hybris maintains metadata stored on private premises at the order of few dozens of bytes per key, avoiding the scalability bottleneck at the private cloud. In turn, the hybrid design allows Hybris to efficiently and robustly tolerate cloud outages but also potential malice in clouds without overhead. Namely, to tolerate up to f malicious clouds, in the common case of the Hybris variant with data replication, writes replicate data across f + 1 clouds, whereas reads involve a single cloud. In the worst case, only up to f additional clouds are used. This is considerably better than earlier multi-cloud storage systems that required costly 3f + 1 clouds to mask f potentially malicious clouds. Finally, Hybris leverages strong metadata consistency to guarantee to Hybris applications strong data consistency without any modifications to the eventually consistent public clouds.
   We implemented Hybris in Java and evaluated it using a series of micro and macro-benchmarks. Our results show that Hybris significantly outperforms comparable multi-cloud storage systems and approaches the performance of bare-bone commodity public cloud storage.
C1 [Viotti, Paolo] Eurecom, Biot, France.
   [Dobre, Dan] NEC Labs Europe, Heidelberg, Germany.
   [Vukolic, Marko] IBM Res Zurich, Saumerstr 4, CH-8803 Ruschlikon, Switzerland.
   [Viotti, Paolo] Sorbonne Sorbonne Univ, UPMC, LIP6, 4 Pl Jussieu, F-75252 Paris 05, France.
C3 IMT - Institut Mines-Telecom; EURECOM; NEC Corporation; International
   Business Machines (IBM); Sorbonne Universite
RP Viotti, P (corresponding author), Eurecom, Biot, France.; Viotti, P (corresponding author), Sorbonne Sorbonne Univ, UPMC, LIP6, 4 Pl Jussieu, F-75252 Paris 05, France.
EM paolo.viotti@lip6.fr; dan@dobre.net; mvu@zurich.ibm.com
FU EU project CloudSpaces [FP7-317555]; EU project SECCRIT [FP7-312758]
FX This work is partially supported by the EU projects CloudSpaces
   (FP7-317555) and SECCRIT (FP7-312758).
CR Abraham I, 2006, DISTRIB COMPUT, V18, P387, DOI 10.1007/s00446-005-0151-6
   Abu-Libdeh H., 2010, Proceedings of the 1st ACM symposium on Cloud computing, SoCC'10, pA(sic)g, P229
   Adya A, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE FIFTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P1
   Amazon, 2016, AM DYNAMODB PRINC
   Apache, 2016, AP JCLOUDS
   Armbrust M, 2010, COMMUN ACM, V53, P50, DOI 10.1145/1721654.1721672
   Bailis P, 2013, COMMUN ACM, V56, P55, DOI 10.1145/2447976.2447992
   Bermbach D, 2014, INT CONF CLOUD ENG, P47, DOI 10.1109/IC2E.2014.37
   Bessani A., 2014, USENIX ATC 14, P169
   Bessani A, 2013, ACM T STORAGE, V9, DOI 10.1145/2535929
   BEZERRA CE, 2014, PROCEEDINGS OF THE I, P331
   Bowers KD, 2009, CCS'09: PROCEEDINGS OF THE 16TH ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P187
   Brewer E, 2012, COMPUTER, V45, P23, DOI 10.1109/MC.2012.37
   Byung-Gon Chun, 2007, Operating Systems Review, V41, P189, DOI 10.1145/1323293.1294280
   Cachin C, 2014, LECT NOTES COMPUT SC, V8756, P1, DOI 10.1007/978-3-319-11764-5_1
   Cano I, 2016, PROCEEDINGS OF THE SEVENTH ACM SYMPOSIUM ON CLOUD COMPUTING (SOCC 2016), P29, DOI 10.1145/2987550.2987584
   Chihoub HE, 2012, IEEE INT C CL COMP, P293, DOI 10.1109/CLUSTER.2012.56
   Chockler G, 2013, LECT NOTES COMPUT SC, V8205, P569
   CloudSpaces, 2015, CLOUDSPACES EU FP7 P
   Consul, 2016, CONS CONS PROT
   Consul, 2016, CONS DISTR SERV DISC
   Cooper Brian F., 2010, P 1 ACM S CLOUD COMP, P143, DOI [DOI 10.1145/1807128.1807152, 10.1145/1807128.1807152]
   Corbett JC, 2013, ACM T COMPUT SYST, V31, DOI 10.1145/2491245
   Correia M, 2004, SYM REL DIST SYST, P174, DOI 10.1109/RELDIS.2004.1353018
   Correia Miguel., 2012, Presented as part of the 2012 USENIX Annual Technical Conference (USENIX ATC 12), P453
   Drago I., 2013, P IMC, P205
   Drago I., 2012, the 2012 ACM conference, P481, DOI [10.1145/2398776.2398827, DOI 10.1145/2398776.2398827]
   DWORK C, 1988, J ACM, V35, P288, DOI 10.1145/42282.42283
   Eurecom, 2016, HYBR ROB HYBR CLOUD
   Fan R, 2003, LECT NOTES COMPUT SC, V2848, P75
   Ganesan A, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P149
   Lopez PG, 2014, ACM/IFIP/USENIX MIDDLEWARE 2014, P49, DOI 10.1145/2663165.2663332
   Ghemawat S., 2003, ACM SIGOPS OPERATING, V37, P29, DOI [DOI 10.1145/945445.945450, DOI 10.1145/1165389.945450, 10.1145/1165389.945450]
   Gibson GA, 1998, ACM SIGPLAN NOTICES, V33, P92, DOI 10.1145/291006.291029
   Gilbert S., 2002, SIGACT News, V33, P51, DOI 10.1145/564585.564601
   Golab W, 2011, PODC 11: PROCEEDINGS OF THE 2011 ACM SYMPOSIUM PRINCIPLES OF DISTRIBUTED COMPUTING, P197
   Gunawi HS, 2016, PROCEEDINGS OF THE SEVENTH ACM SYMPOSIUM ON CLOUD COMPUTING (SOCC 2016), P1, DOI 10.1145/2987550.2987583
   HALALAI R, 2014, PROCEEDINGS OF THE I, P67
   Hamilton J., 2009, The cost of latency
   HERLIHY M, 1991, ACM T PROGR LANG SYS, V13, P124, DOI 10.1145/114005.102808
   HERLIHY MP, 1990, ACM T PROGR LANG SYS, V12, P463, DOI 10.1145/78969.78972
   Hunt Patrick, 2010, P 2010 USENIX ANN TE, P11, DOI DOI 10.5555/1855840.1855851
   Ilasescu C, 2012, P IEEE IFIP INT C DE, P1
   Junqueira Flavin P., 2013, Operating Systems Review, V47, P9
   Kapitza R., 2012, P 7 ACM EUR C COMP S, V12, P295, DOI DOI 10.1145/2168836.2168866
   Koda R, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 2007 USENIX ANNUAL TECHNICAL CONFERENCE, P129
   LAMPORT L, 1979, IEEE T COMPUT, V28, P690, DOI 10.1109/TC.1979.1675439
   Liu SY, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P485
   Lynch N, 2002, LECT NOTES COMPUT SC, V2508, P173
   Mahajan P, 2011, ACM T COMPUT SYST, V29, DOI 10.1145/2063509.2063512
   ONGARO D, 2014, P USENIX ANN TECHN C
   PEASE M, 1980, J ACM, V27, P228, DOI 10.1145/322186.322188
   Plank JamesS., 2008, JERASURE LIB 200 FAC
   Reed B., 2008, proceedings of the 2nd Workshop on Large-Scale Distributed Systems and Middleware, P2, DOI [10.1145/1529974.1529978, DOI 10.1145/1529974.1529978]
   Rodrigues R, 2005, LECT NOTES COMPUT SC, V3640, P226, DOI 10.1007/11558989_21
   Sivaramakrishnan KC, 2015, ACM SIGPLAN NOTICES, V50, P413, DOI [10.1145/2813885.2737981, 10.1145/2737924.2737981]
   SoftLayer, 2016, IBM SOFTLAYER
   Stefanov Emil., 2013, CCS
   Syncany, 2016, SEC FIL SYNCHR SOFTW
   Terry D. B., 1994, Proceedings of the Third International Conference on Parallel and Distributed Information Systems (Cat. No.94TH0668-4), P140, DOI 10.1109/PDIS.1994.331722
   Terry DB, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P309, DOI 10.1145/2517349.2522731
   Torres-Rojas F. J., 1999, Proceedings of the Eighteenth Annual ACM Symposium on Principles of Distributed Computing, P163, DOI 10.1145/301308.301350
   Veronese GS, 2013, IEEE T COMPUT, V62, P16, DOI 10.1109/TC.2011.221
   Viotti P, 2016, ACM COMPUT SURV, V49, DOI 10.1145/2926965
   VMware, 2013, SNOWD LEAK WINDF HYB
   Vogels W, 2009, COMMUN ACM, V52, P40, DOI 10.1145/1435417.1435432
   Vukolic Marko, 2010, SIGACT News, V41, P105, DOI 10.1145/1855118.1855137
   Wang Y., 2012, Proceedings of the 2012 USENIX conference on Annual Technical Conference, P38
   Weatherspoon H, 2002, LECT NOTES COMPUT SC, V2429, P328
   Weil SA, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P307
   Wu Z, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P292, DOI 10.1145/2517349.2522730
   Yin J., 2003, P 19 ACM S OP SYST P, P253, DOI [10.1145/945445.945470, DOI 10.1145/945445.945470]
   ZEINEDDINE H, 2011, PROCEEDINGS OF THE I, P165
   Zhang H, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P167
   ZooKeeper Observers, 2009, OBS MAK ZOOKEEPER SC
NR 75
TC 7
Z9 7
U1 1
U2 23
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD OCT
PY 2017
VL 13
IS 3
SI SI
AR 27
DI 10.1145/3119896
PG 32
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FL6DV
UT WOS:000414336300010
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Hou, BB
   Chen, F
   Ou, ZH
   Wang, R
   Mesnier, M
AF Hou, Binbing
   Chen, Feng
   Ou, Zhonghong
   Wang, Ren
   Mesnier, Michael
TI Understanding I/O Performance Behaviors of Cloud Storage from a Client's
   Perspective
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Cloud storage; storage systems; performance analysis; measurement;
   performance optimization; Design; Experimentation; Measurement;
   Performance
AB Cloud storage has gained increasing popularity in the past few years. In cloud storage, data is stored in the service provider's data centers, and users access data via the network. For such a new storage model, our prior wisdom about conventional storage may not remain valid nor applicable to the emerging cloud storage. In this article, we present a comprehensive study to gain insight into the unique characteristics of cloud storage and optimize user experiences with cloud storage from a client's perspective. Unlike prior measurement work that mostly aims to characterize cloud storage providers or specific client applications, we focus on analyzing the effects of various client-side factors on the user-experienced performance. Through extensive experiments and quantitative analysis, we have obtained several important findings. For example, we find that (1) a proper combination of parallelism and request size can achieve optimized bandwidths, (2) a client's capabilities and geographical location play an important role in determining the end-to-end user-perceivable performance, and (3) the interference among mixed cloud storage requests may cause performance degradation. Based on our findings, we showcase a sampling-and inference-based method to determine a proper combination for different optimization goals. We further present a set of case studies on client-side chunking and parallelization for typical cloud-based applications. Our studies show that specific attention should be paid to fully exploiting the capabilities of clients and the great potential of cloud storage services.
C1 [Hou, Binbing; Chen, Feng] Louisiana State Univ, Dept Comp Sci & Engn, 102G Elect Engn Bldg, Baton Rouge, LA 70803 USA.
   [Ou, Zhonghong] Beijing Univ Posts & Telecommun, Dept Comp Sci & Engn, Xitucheng Rd 10, Beijing 100876, Peoples R China.
   [Wang, Ren; Mesnier, Michael] Intel Labs, Hillsboro, OR USA.
   [Wang, Ren; Mesnier, Michael] Intel Corp, Intel Labs, 2111 NE 25th Ave, Hillsboro, OR 97124 USA.
C3 Louisiana State University System; Louisiana State University; Beijing
   University of Posts & Telecommunications; Intel Corporation; Intel
   Corporation
RP Chen, F (corresponding author), Louisiana State Univ, Dept Comp Sci & Engn, 102G Elect Engn Bldg, Baton Rouge, LA 70803 USA.; Ou, ZH (corresponding author), Beijing Univ Posts & Telecommun, Dept Comp Sci & Engn, Xitucheng Rd 10, Beijing 100876, Peoples R China.
EM bhou@csc.lsu.edu; fchen@csc.lsu.edu; zhonghong.ou@bupt.edu.cn;
   ren.wang@intel.com; michael.mesnier@intel.com
FU Louisiana Board of Regents [LEQSF(2014-17)-RD-A-01,
   LEQSF-EPS(2015)-PFUND-391]; National Science Foundation [CCF-1453705,
   CCF-1629291]; Intel Corporation; Direct For Computer & Info Scie &
   Enginr; Division of Computing and Communication Foundations [1629291]
   Funding Source: National Science Foundation
FX This work was supported in part by the Louisiana Board of Regents under
   grants LEQSF(2014-17)-RD-A-01 and LEQSF-EPS(2015)-PFUND-391, the
   National Science Foundation under grants CCF-1453705 and CCF-1629291,
   and generous support from Intel Corporation.
CR Abu-Libdeh H., 2010, P 1 ACM S CLOUD COMP
   Amazon, 2015, AM EBS
   Amazon, 2015, AM S3
   Amazon, 2015, AM EFS
   Amazon, 2010, AM S3 OBJ SIZ LIM NO
   Amazon, 2015, AM S3 TCP WIND SCAL
   [Anonymous], 2013, Proceedings of the 2013 conference on Internet measurement conference
   [Anonymous], SIGOPS OPER SYST REV
   [Anonymous], 2010, P 10 ACM SIGCOMM C I
   [Anonymous], 2012, P USENIX FAST
   [Anonymous], 2014, P FIL STOR TECHN
   Bergen A., 2011, P 2011 IEEE PAC RIM
   Bermudez I., 2013, P 32 IEEE INT C COMP
   Bessani A., 2014, P USENIX ANN TECHN C
   Bocchi E., 2015, IEEE TRANS CLOUD COM, P1
   Bocchi E, 2014, IEEE INT CONF CL NET, P395, DOI 10.1109/CloudNet.2014.6969027
   Bonvin N, 2010, P 1 ACM S CLOUD COMP
   Boto, 2015, S3 API REF
   Boto, 2015, INTR BOT S3 INT
   Brad Calder, 2011, P 23 ACM S OP SYST P, P119
   Chen F, 2014, PROCEEDINGS OF THE ASME 9TH INTERNATIONAL MANUFACTURING SCIENCE AND ENGINEERING CONFERENCE, 2014, VOL 2
   Chen Feng, 2009, P INT C MEAS MOD COM
   Cooper B.F., 2010, P ACM S CLOUD COMPUT, DOI DOI 10.1145/1807128.1807152
   Cui Yong, 2015, P 21 ANN INT C MOB C, P582
   Ding X., 2007, Proceedings of the 2007 USENIX Annual Technical Conference, p20:1
   Drago I., 2013, P 2013 ACM C INT MEA
   Drago I., 2014, P 2014 IEEE INT C CO
   Drago I., 2012, P 2012 INT MEASC BOS, DOI [10.1145/2398776.2398827, DOI 10.1145/2398776.2398827]
   ELLARD D, 2003, P 2 USENIX C FIL STO
   Ford D., 2010, P 9 USENIX S OP SYST
   Google, 2015, GOOGL DRIV
   Gracia-Tinedo R, 2013, IEEE INT CONF CLOUD, P301, DOI 10.1109/CLOUD.2013.25
   Gulati Ajay, 2011, P 2 ACM S CLOUD COMP, P1
   Higgins B.D., 2012, P 10 INT C MOBILE SY, P155, DOI [DOI 10.1145/2307636.2307651, 10.1145/2307636.2307651]
   Hou Binbing, 2016, P 32 INT C MASS STOR
   Hu Y., 2012, FAST
   IHS, 2012, SUBSCR CLOUD STOR SE
   Jiang S, 2005, P 4 USENIX C FIL STO
   Li Zhenhua, 2013, P INT MIDDL C MIDDL
   Liu H, 2002, 2002 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS, VOLS 1-5, CONFERENCE PROCEEDINGS, P2191, DOI 10.1109/ICC.2002.997235
   Mager Thomas, 2012, P 12 IEEE INT C PEER
   MarketsandMarkets, 2015, CLOUD STOR MARK SOL
   Meng X., 2010, P 2 INT WORKSH CLOUD
   Microsoft, 2015, ON
   OpenStack, 2011, OP SWIFT
   Ou Zhonghong, 2015, P 8 IEEE ACM INT C U
   Van Jacobson, 1992, RFC 1323 TCP EXTENSI
   Wang HP, 2012, ICEEM 2012: 2012 2ND INTERNATIONAL CONFERENCE ON ECONOMIC, EDUCATION AND MANAGEMENT, VOL 2, P12
   Wu Z., 2015, NSDI, P543
   Yuan Dong, 2011, Operating Systems Review, V45, P101, DOI 10.1145/1945023.1945036
   Zhang Rui, 2011, P 4 INT IEEE C CLOUD
NR 51
TC 13
Z9 16
U1 1
U2 17
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUN
PY 2017
VL 13
IS 2
SI SI
AR 16
DI 10.1145/3078838
PG 36
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FA1TA
UT WOS:000405221600007
DA 2024-07-18
ER

PT J
AU Gim, J
   Hwang, T
   Won, Y
   Kant, K
AF Gim, Jongmin
   Hwang, Taeho
   Won, Youjip
   Kant, Krishna
TI SmartCon: Smart Context Switching for Fast Storage Devices
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Algorithms; Performance; Nonvolatile memory; solid state disk;
   context switch; I/O subsystem
ID CLASS MEMORY; SYSTEM; PERFORMANCE
AB Handling of storage IO in modern operating systems assumes that such devices are slow and CPU cycles are valuable. Consequently, to effectively exploit the underlying hardware resources, for example, CPU cycles, storage bandwidth and the like, whenever an IO request is issued to such device, the requesting thread is switched out in favor of another thread that may be ready to execute. Recent advances in nonvolatile storage technologies and multicore CPUs make both of these assumptions increasingly questionable, and an unconditional context switch is no longer desirable. In this article, we propose a novel mechanism called SmartCon, which intelligently decides whether to service a given IO request in interrupt-driven manner or busy-wait-based manner based on not only the device characteristics but also dynamic parameters such as IO latency, CPU utilization, and IO size. We develop an analytic performance model to project the performance of SmartCon for forthcoming devices. We implement SmartCon mechanism on Linux 2.6 and perform detailed evaluation using three different IO devices: Ramdisk, low-end SSD, and high-end SSD. We find that SmartCon yields up to a 39% performance gain over the mainstream block device approach for Ramdisk, and up to a 45% gain for PCIe-based SSD and SATA-based SSDs. We examine the detailed behavior of TLB, L1, L2 cache and show that SmartCon achieves significant improvement in all cache misbehaviors.
C1 [Gim, Jongmin] Texas A&M Univ, Elect & Comp Engn, College Stn, TX 77843 USA.
   [Hwang, Taeho; Won, Youjip] Hanyang Univ, Div Elect & Comp Engn, Seoul, South Korea.
   [Kant, Krishna] Temple Univ, Dept Comp & Informat Sci, Philadelphia, PA 19122 USA.
C3 Texas A&M University System; Texas A&M University College Station;
   Hanyang University; Pennsylvania Commonwealth System of Higher Education
   (PCSHE); Temple University
RP Hwang, T (corresponding author), Hanyang Univ, Div Elect & Comp Engn, Seoul, South Korea.
EM gim.jongmin@tamu.edu; htaeh@hanyang.ac.kr; yjwon@hanyang.ac.kr;
   kkant@temple.edu
RI Kant, Krishna/GOK-0405-2022
FU IT R&D program MKE/KEIT [10041608, 10035202]; MSIP (Ministry of Science,
   ICT & Future Planning), Korea, under the ITRC (Information Technology
   Research Center) [NIPA-2014-H0301-14-1017]
FX This work is supported by IT R&D program MKE/KEIT (No. 10041608,
   Embedded System Software for New-memory based Smart Device), and
   partially supported by IT R&D program MKE/KEIT. [No. 10035202, Large
   Scale hyper-MLC SSD Technology Development]. This research was also
   supported by the MSIP (Ministry of Science, ICT & Future Planning),
   Korea, under the ITRC (Information Technology Research Center) support
   program (NIPA-2014-H0301-14-1017) supervised by the NIPA (National IT
   Industry Promotion Agency).
CR AGARWAL A, 1988, ACM T COMPUT SYST, V6, P393, DOI 10.1145/48012.48037
   Ahmadi M. R., 2010, P INT S TEL IST 10
   Akel Ameen, 2011, P USENX HOTSTORAGE
   [Anonymous], P 24 ACM S OP SYST P
   [Anonymous], P WORKSH EXP COMP SC
   [Anonymous], 2012, NVM Express Revision 1.1
   Bhattacharya Suparna, 2004, P LIN S
   Burr GW, 2008, IBM J RES DEV, V52, P449, DOI 10.1147/rd.524.0449
   Caulfield Adrian M., 2010, Proceedings 2010 43rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO 2010), P385, DOI 10.1109/MICRO.2010.33
   Chen F, 2014, PROCEEDINGS OF THE ASME 9TH INTERNATIONAL MANUFACTURING SCIENCE AND ENGINEERING CONFERENCE, 2014, VOL 2
   Coburn J., 2011, P INT C ARCH SUPP PR
   Corbet Jonathan, 2005, Linux device drivers, VThird
   David F. M., 2007, P WORKSH EXP COMP SC
   Elmeleegy Khaled, 2004, P USENIX ANN TECHN C
   Freitas RF, 2008, IBM J RES DEV, V52, P439, DOI 10.1147/rd.524.0439
   FusionIO, 2010, IODRIVE PROD FAM US
   Hosomi M, 2005, INT EL DEVICES MEET, P473
   *IEEE, 2003, 10031 IEEE
   Intel, 2011, LIGHT PEAK TECHN
   Ipek E., 2009, P ACM SIGOPS S OP SY
   Jeong S., 2013, Proceedings of the 2013 USENIX conference on Annual Technical Conference, P309
   Johnson Ryan., 2009, DAMON 09 P 5 INT WOR, P21, DOI DOI 10.1145/1565694.1565700
   Jones M. T., 2006, LINUX INITIAL RAM DI
   Jung J., 2009, P INT WORKSH SOFTW S
   Jung J, 2010, ACM T STORAGE, V6, DOI 10.1145/1714454.1714457
   Kant K, 1999, IEEE T KNOWL DATA EN, V11, P731, DOI 10.1109/69.806933
   Kant K., 2008, P INT WORKSH OS TECH
   Karlin A. R., 1991, Operating Systems Review, V25, P41, DOI 10.1145/121133.286599
   Katcher Jeffrey., 1997, POSTMARK NEW FILE SY
   KOBAYASHI M, 1986, IEEE T COMPUT, V35, P720, DOI 10.1109/TC.1986.1676823
   Lantz Philip, 2014, P 2014 USENIX ANN TE, P433
   Lee Benjamin C., 2009, P ANN INT S COMP ARC
   Li J, 2008, DES AUT CON, P278, DOI 10.1109/EUC.2008.57
   Liu F, 2008, PACT'08: PROCEEDINGS OF THE SEVENTEENTH INTERNATIONAL CONFERENCE ON PARALLEL ARCHITECTURES AND COMPILATION TECHNIQUES, P91, DOI 10.1145/1454115.1454130
   McDougall R., 2005, Filebench
   McVoy L, 1996, PROCEEDINGS OF THE USENIX 1996 ANNUAL TECHNICAL CONFERENCE, P279
   Microsoft Corp MSDN., 2010, RAMDISK STOR DRIV SA
   OBrien Kevin, 2013, OCZ REVODRIVE 3 X2 4
   Orenstien Doron, 2004, LOW POWER PROCESSOR
   Ousterhout JohnK., 1990, USENIX SUMMER, P247
   Park S., 2009, P WORKSH INT ARCH SC
   Poley Henk, 2014, LOOK BACK SINGLE THR
   Qureshi MK, 2009, CONF PROC INT SYMP C, P24, DOI 10.1145/1555815.1555760
   Qureshi Moinuddin K., 2010, P ANN INT S COMP ARC
   Salah K, 2009, COMPUT COMMUN, V32, P179, DOI 10.1016/j.comcom.2008.10.001
   Samsung Electronics, 2007, 1GB C DIE DDR3 SDRAM
   Samsung Electronics, 2005, ONENAND SPEC VER 1 2
   Schmid Patrick, 2008, INTEL X25 M SOLID ST
   Stärner J, 2004, ACM SIGPLAN NOTICES, V39, P146, DOI 10.1145/998300.997184
   TAM D., 2007, P WORKSH INT OP SYST
   Venkataraman Shivaram, 2011, P US C FIL STOR TECH
   Venkatasubramanian G, 2009, INT SYM COMP ARCHIT, P153, DOI 10.1109/SBAC-PAD.2009.21
   Volos Haris, 2011, P INT C ARCH SUPP PR
   WEICKER RP, 1984, COMMUN ACM, V27, P1013, DOI 10.1145/358274.358283
   WIGGINS A, 2003, P 8 AS PAC COMP SYST
   Wu Xiaoxia, 2009, P ANN INT S COMP ARC
   Yamada S, 2008, 2008 IEEE INTERNATIONAL SYMPOSIUM ON PARALLEL & DISTRIBUTED PROCESSING, VOLS 1-8, P2298
   Yan J, 2008, PROCEEDINGS OF THE 14TH IEEE REAL-TIME AND EMBEDDED TECHNOLOGY AND APPLICATIONS SYMPOSIUM, P80, DOI 10.1109/RTAS.2008.6
   Yang Jisoo, 2012, P 10 USENIX C FIL ST, P3
NR 59
TC 4
Z9 4
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAR
PY 2015
VL 11
IS 2
AR 5
DI 10.1145/2631922
PG 25
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CF1KG
UT WOS:000352303400001
DA 2024-07-18
ER

PT J
AU Gim, JM
   Won, YJ
AF Gim, Jongmin
   Won, Youjip
TI Extract and Infer Quickly: Obtaining Sector Geometry of Modern Hard Disk
   Drives
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Measurement; Hard disk; performance characterization; sector
   geometry; seek time; track skew; zone
ID PERFORMANCE
AB The modern hard disk drive is a complex and complicated device. It consists of 2-4 heads, thousands of sectors per track, several hundred thousands of tracks, and tens of zones. The beginnings of adjacent tracks are placed with a certain angular offset. Sectors are placed on the tracks and accessed in some order. Angular offset and sector placement order vary widely subject to vendors and models. The success of an efficient file and storage subsystem design relies on the proper understanding of the underlying storage device characteristics. The characterization of hard disk drives has been a subject of intense research for more than a decade. The scale and complexity of state-of-the-art hard disk drive technology calls for a new way of extracting and analyzing the characteristics of the hard disk drive. In this work, we develop a novel disk characterization suite, DIG (Disk Geometry Analyzer), which allows us to rapidly extract and characterize the key performance metrics of the modern hard disk drive. Development of this tool is accompanied by thorough examination of four off-the-shelf hard disk drives. DIG consists of three key ingredients: O(1) a track boundary detection algorithm; O(log n) a zone boundary detection algorithm; and hybrid sampling based seek time profiling. We particularly focus on addressing the scalability aspect of disk characterization. With DIG, we are able to extract key metrics of hard disk drives, for example, track sizes, zone information, sector geometry and so on, within 3-20 minutes. DIG allows us to determine the sector layout mechanism of the underlying hard disk drive, for example, hybrid serpentine, cylinder serpentine, and surface serpentine, and to a build complete sector map from LBN to the three dimensional space of (Cylinder, Head, Sector). Examining the hard disk drives with DIG, we made a number of important observations. In modern hard disk drives, head switch overhead is far greater than track switch overhead. It seems that hard disk drive vendors put greater emphasis on reducing the number of head switches for data access. Most disk vendors use surface serpentine, cylinder serpentine, or hybrid serpentine schemes in laying sectors on the platters. The legacy seek time model, which takes the form of a + b root d leaves much to be desired for use in modern hard disk drives especially for short seeks (less than 5000 tracks). We compare the performance of the DIG against the existing state-of-the-art disk profiling algorithm. Compared to the existing state-of-the-art disk characterization algorithm, the DIG algorithm significantly decreases the time to extract comprehensive sector geometry information from 1920 minutes to 7 minutes and 1927 minutes to 180 minutes in best and worst case scenarios, respectively.
C1 [Gim, Jongmin; Won, Youjip] Hanyang Univ, Seoul, South Korea.
C3 Hanyang University
RP Won, Y (corresponding author), Hanyang Univ, 17 Haeng Dang Dong, Seoul, South Korea.
EM jmkim@ece.hanyang.ac.kr; jmwon@ece.hanyang.ac.kr
FU KOSEF through National Research Laboratory at Hanyang University
   [ROA-2007-000-20114-0]; Samsung Electronics
FX This work is sponsored by KOSEF through the National Research Laboratory
   at Hanyang University (ROA-2007-000-20114-0) and Samsung Electronics.
CR Aboutabl M., 1998, P ACM SIGMETRICS C M, V26, P280
   Allen B., 2004, Linux Journal, V2004, P9
   ANDERSON D., 2003, P 2 USENIX C FIL STO
   [Anonymous], P USENIX ANN TECHN C
   BAIRAVASUNDARAM L. N., 2008, P USENIX ANN TECHN C
   Bairavasundaram LN, 2007, PERF E R SI, V35, P289
   CHANG J., 2008, P COMP SCI ITS APPL
   CHANG J., 2008, P IEEE INT WORKSH ST
   DAVY W, 1998, Patent No. 5808821
   Denehy TE, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE GENERAL TRACK, P177
   DI MARCO A., 2007, DISITR0707 U GEN
   ELLIOT R. C., 2005, INFORM TECHNOLOGY SC
   Huang H, 2007, 24TH IEEE CONFERENCE ON MASS STORAGE SYSTEMS AND TECHNOLOGIES, PROCEEDINGS, P185, DOI 10.1109/MSST.2007.4367973
   Huang L., 2000, ECSLTR81 SUNY
   JACOBSON D, 1991, HPLCSP917REV1
   LOHMEYER J., 2005, SCSI 3 STANDARDS ARC
   Lumb CR, 2000, USENIX ASSOCIATION PROCEEDINGS OF THE FOURTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P87
   MACON JR J. F., 1997, US Patent, Patent No. 5600817
   MASIEWICZ J., 2004, T131532D AM NAT PROJ, V1
   MATRIXSTORE, 2008, LONG 100X BETT HDD E
   MCKUSICK MK, 1984, ACM T COMPUT SYST, V2, P181, DOI 10.1145/989.990
   Mesut Ö, 2002, IEEE T CONSUM ELECTR, V48, P802, DOI 10.1109/TCE.2002.1037078
   NISHA T., 1999, CSD991063 U CAL
   Park S, 2003, LECT NOTES COMPUT SC, V2968, P486
   RUEMMLER C, 1994, COMPUTER, V27, P17, DOI 10.1109/2.268881
   SCHINDLER J., 2004, P 3 USENIX C FIL STO
   SCHINDLER J, 2002, P C FIL STOR TECHN F
   Schlosser SW, 2005, USENIX ASSOCIATION PROCEEDINGS OF THE 4TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P225
   SEAGATE, 1999, SCSI INT PROD MAN 2
   SHIM J. S., 1998, US Patent, Patent No. 6282367
   SHIN D. I., 2007, P 15 ANN M IEEE INT
   Triantafillou P, 2002, IEEE T KNOWL DATA EN, V14, P140, DOI 10.1109/69.979978
   Worthington B. L., 1995, Performance Evaluation Review, V23, P146, DOI 10.1145/223586.223604
   Youjip Won, 2006, ACM Transaction on Storage, V2, P255, DOI 10.1145/1168910.1168912
NR 34
TC 8
Z9 8
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUL
PY 2010
VL 6
IS 2
AR 6
DI 10.1145/1807060.1807063
PG 26
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA V24QG
UT WOS:000208424300003
DA 2024-07-18
ER

PT J
AU Qin, M
   Zheng, Q
   Lee, J
   Settlemyer, B
   Wen, F
   Reddy, N
   Gratz, P
AF Qin, Mian
   Zheng, Qing
   Lee, Jason
   Settlemyer, Bradley
   Wen, Fei
   Reddy, Narasimha
   Gratz, Paul
TI KVRangeDB: RangeQueries for a Hash-based Key-Value Device
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Key value stores; KVSSD; range queries
AB Key-value (KV) software has proven useful to a wide variety of applications including analytics, time-series databases, and distributed file systems. To satisfy the requirements of diverse workloads, KV stores have been carefully tailored to best match the performance characteristics of underlying solid-state block devices. Emerging KV storage device is a promising technology for both simplifying the KV software stack and improving the performance of persistent storage-based applications. However, while providing fast, predictable put and get operations, existing KV storage devices do not natively support range queries that are critical to all three types of applications described above.
   In this article, we present KVRangeDB, a software layer that enables processing range queries for existing hash-based KV solid-state disks (KVSSDs). As an effort to adapt to the performance characteristics of emerging KVSSDs, KVRangeDB implements log-structured merge tree key index that reduces compaction I/O, merges keys when possible, and provides separate caches for indexes and values. We evaluated the KVRangeDB under a set of representative workloads, and compared its performance with two existing database solutions: a Rocksdb variant ported to work with the KVSSD, and Wisckey, a key-value database that is carefully tuned for conventional block devices. On filesystem aging workloads, KVRangeDB outperforms Wisckey by 23.7x in terms of throughput and reduce CPU usage and external write amplifications by 14.3x and 9.8x, respectively.
C1 [Qin, Mian; Wen, Fei; Reddy, Narasimha; Gratz, Paul] Texas A&M Univ, 3128 TAMU, College Stn, TX 77843 USA.
   [Zheng, Qing; Lee, Jason] Los Alamos Natl Lab, POB 1663, Los Alamos, NM 87545 USA.
   [Settlemyer, Bradley] Nvidia, 11001 Lakeline Blvd, Austin, TX 78717 USA.
C3 Texas A&M University System; Texas A&M University College Station;
   United States Department of Energy (DOE); Los Alamos National Laboratory
RP Qin, M (corresponding author), Texas A&M Univ, 3128 TAMU, College Stn, TX 77843 USA.
EM celery1124@tamu.edu; qzheng@lanl.gov; jasonlee@lanl.gov;
   bsettlemyer@nvidia.com; fei8wen@gmail.com; reddy@tamu.edu;
   pgratz@gratz1.com
RI Wen, Fei/ACK-2119-2022; Qin, celery1124/ISS-5540-2023
OI Wen, Fei/0000-0002-8789-8495; Qin, celery1124/0000-0003-1753-3914;
   Zheng, Qing/0000-0002-7457-9874; Gratz, Paul/0000-0001-7120-7189; Reddy,
   Narasimha/0000-0003-4625-8819; Lee, Jason/0000-0003-1604-1395;
   Settlemyer, Bradley/0000-0002-9299-2654
FU Samsung; Los Alamos National Lab; NSF [1439722, 1823403, 2203033]
FX This work is supported in part by grants from Samsung, Los Alamos
   National Lab and NSF Grants 1439722, 1823403 and 2203033.
CR Aghayev A, 2019, PROCEEDINGS OF THE TWENTY-SEVENTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '19), P353, DOI 10.1145/3341301.3359656
   Apache, 2013, ABOUT US
   Axboe Jens, 2020, KEY VALUE STORAGE AP
   Bjorling M, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P359
   Bjorling Matias, 2020, LINUX STORAGE FILESY
   BLOOM BH, 1970, COMMUN ACM, V13, P422, DOI 10.1145/362686.362692
   Bronson N., 2013, P 2013 USENIX C ANN, P49
   Cao ZC, 2020, PROCEEDINGS OF THE 18TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P209
   Chang F, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P205
   Chen YM, 2020, TWENTY-FIFTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXV), P1077, DOI 10.1145/3373376.3378515
   CMU/PDL File Systems, 2013, FAST EFF FIL MET LSM
   Cooper Brian F., 2010, P 1 ACM S CLOUD COMP, P143, DOI [DOI 10.1145/1807128.1807152, 10.1145/1807128.1807152]
   Dean J., 2017, LEVELDB GOOGLES FAST
   DeCandia Giuseppe, 2007, Operating Systems Review, V41, P205, DOI 10.1145/1323293.1294281
   Facebook, 2015, RocksDB
   Im J, 2020, PROCEEDINGS OF THE 2020 USENIX ANNUAL TECHNICAL CONFERENCE, P173
   Jin YQ, 2017, INT S HIGH PERF COMP, P373, DOI 10.1109/HPCA.2017.15
   Kaiyrakhmet O, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P191
   Kang Y, 2019, SYSTOR '19: PROCEEDINGS OF THE 12TH ACM INTERNATIONAL SYSTEMS AND STORAGE CONFERENCE, P144, DOI 10.1145/3319647.3325831
   Kim Sang-Hoon, 2019, P 11 USENIX WORKSHOP
   Klimovic A, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P427
   Koo J, 2021, PROCEEDINGS OF THE 15TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '21), P75, DOI 10.5281/zenodo.4659803
   Li BJ, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P137, DOI 10.1145/3132747.3132756
   Lu LY, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P133
   Luo SQ, 2020, SIGMOD'20: PROCEEDINGS OF THE 2020 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P2071, DOI 10.1145/3318464.3389731
   Marmol L., 2015, P USENIX ANN TECH C, P207
   nvmexpress, 2020, NVME COMM SET SPEC
   ONeil P, 1996, ACTA INFORM, V33, P351, DOI 10.1007/s002360050048
   Pan S, 2021, PROCEEDINGS OF THE 19TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '21), P217
   PengWang Guangyu Sun, 2014, P 9 EUROPEAN C COMPU
   Pilman M, 2017, PROC VLDB ENDOW, V10, P1526, DOI 10.14778/3137628.3137659
   Pitchumani R, 2020, PROCEEDINGS OF THE 18TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P309
   Qin Mian, 2021, P 14 ACM INT C SYSTE, DOI [10.1145/3456727.3463781, DOI 10.1145/3456727.3463781]
   Raju P, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P497, DOI 10.1145/3132747.3132765
   Ren K, 2014, INT CONF HIGH PERFOR, P237, DOI 10.1109/SC.2014.25
   Ren Kai., 2013, USENIX ANN TECHNICAL, P145
   Wei XD, 2020, PROCEEDINGS OF THE 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '20), P117
   Weil SA, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P307
   Yang JC, 2020, PROCEEDINGS OF THE 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '20), P191
   Zhang HC, 2018, INT CONF MANAGE DATA, P323, DOI 10.1145/3183713.3196931
   Zheng Qing, 2018, SCALING EMBEDDED INS
NR 41
TC 3
Z9 3
U1 1
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2023
VL 19
IS 3
AR 24
DI 10.1145/3582013
PG 21
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P9QZ4
UT WOS:001053965500004
OA Bronze
DA 2024-07-18
ER

PT J
AU Xia, W
   Pu, LF
   Zou, XY
   Shilane, P
   Li, SY
   Zhang, HJ
   Wang, X
AF Xia, Wen
   Pu, Lifeng
   Zou, Xiangyu
   Shilane, Philip
   Li, Shiyi
   Zhang, Haijun
   Wang, Xuan
TI The Design of Fast and Lightweight Resemblance Detection for Efficient
   Post-Deduplication Delta Compression
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Post-deduplication delta compression; resemblance detection; SIMD;
   parallel rolling hash; content-defined sampling
AB Post-deduplication delta compression is a data reduction technique that calculates and stores the differences of very similar but non-duplicate chunks in storage systems, which is able to achieve a very high compression ratio. However, the low throughput of widely used resemblance detection approaches (e.g., N-Transform) usually becomes the bottleneck of delta compression systems due to introducing high computational overhead. Generally, this overhead mainly consists of two parts: (1) calculating the rolling hash byte by byte across data chunks and (2) applying multiple transforms on all of the calculated rolling hash values.
   In this article, we propose Odess, a fast and lightweight resemblance detection approach, that greatly reduces the computational overhead for resemblance detection while achieving high detection accuracy and a high compression ratio. Odess first utilizes a novel Subwindow-based Parallel Rolling (SWPR) hash method using Single Instruction Multiple Data [1] (SIMD) to accelerate calculation of rolling hashes (corresponding to the first part of the overhead). Odess then uses a novel Content-Defined Sampling method to generate a much smaller proxy hash set from the whole rolling hash set and quickly applies transforms on this small hash set for resemblance detection (corresponding to the second part of the overhead).
   Evaluation results show that during the stage of resemblance detection, the Odess approach is similar to 31.4x and similar to 7.9x faster than the state-of-the-art N-Transform and Finesse (a recent variant of N-Transform [39]), respectively. When considering an end-to-end data reduction storage system, the Odess-based system's throughput is about 3.20x and 1.41x higher than the N-Transform- and Finesse-based systems' throughput, respectively, while maintaining the high compression ratio of N-Transform and achieving similar to 1.22x higher compression ratio over Finesse.
C1 [Xia, Wen] HIT Campus Univ Town Shenzhen, Shenzhen 518055, Peoples R China.
   [Pu, Lifeng; Zou, Xiangyu; Li, Shiyi; Zhang, Haijun; Wang, Xuan] Harbin Inst Technol, Shenzhen, Peoples R China.
   [Shilane, Philip] Dell Technol, 131 Pheasant Lane, Newtown, PA 18940 USA.
C3 Harbin Institute of Technology
RP Zou, XY (corresponding author), Harbin Inst Technol, Shenzhen, Peoples R China.
EM xiawen@hit.edu.cn; plfeng97@gmail.com; xiangyu.zou@hotmail.com;
   philip.shilane@dell.com; lsy011025@gmail.com; hjzhang@hit.edu.cn;
   wangxuan@cs.hitsz.edu.cn
RI Shiyi, Li/HPE-1321-2023; Zhang, Haijun/N-8470-2015
OI Shilane, Philip/0000-0003-1235-0502; Zou, Xiangyu/0000-0001-5104-8301
FU National Natural Science Foundation of China [61972441, 61972112,
   61832004]; Guangdong Basic and Applied Basic Research Foundation
   [2021B1515020088]; Shenzhen Science and Technology Innovation Program
   [RCYX20210609104510007, JCYJ20210324131203009, JCYJ20200109113427092,
   GXWD20201230155427003-20200821172511002]; Guangdong Provincial Key
   Laboratory of Novel Security Intelligence Technologies
   [2022B1212010005]; HITSZ-J&A Joint Laboratory of Digital Design and
   Intelligent Fabrication [HITSZ-JA-2021A01]
FX This research was partly supported by the National Natural Science
   Foundation of China under grant nos. 61972441, 61972112, and 61832004;
   the Guangdong Basic and Applied Basic Research Foundation under grant
   no. 2021B1515020088; Shenzhen Science and Technology Innovation Program
   under grant nos. RCYX20210609104510007, JCYJ20210324131203009,
   JCYJ20200109113427092, and GXWD20201230155427003-20200821172511002;
   Guangdong Provincial Key Laboratory of Novel Security Intelligence
   Technologies under grant no. 2022B1212010005; and the HITSZ-J&A Joint
   Laboratory of Digital Design and Intelligent Fabrication under grant no.
   HITSZ-J&A-2021A01.
CR Amiri H, 2020, J PARALLEL DISTR COM, V135, P83, DOI 10.1016/j.jpdc.2019.09.012
   [Anonymous], 2012, P 10 USENIX C FIL ST
   [Anonymous], 2009, 7 USENIX C FIL STOR
   Aronovich L., Proceedings of the 2009 Israeli Experimental Systems Conference (SYSTOR '09), p6:1
   Asaro Tony, 2007, DATA DEDUPLICATION D, P2
   Broder AZ, 1998, COMPRESSION AND COMPLEXITY OF SEQUENCES 1997 - PROCEEDINGS, P21, DOI 10.1109/SEQUEN.1997.666900
   Broder AZ, 2000, J COMPUT SYST SCI, V60, P630, DOI 10.1006/jcss.1999.1690
   Broder AZ, 2000, LECT NOTES COMPUT SC, V1848, P1
   Douglis F, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE GENERAL TRACK, P113
   Forman George., 2005, KDD 05, P394
   Fu Min, 2015, 13 USENIX C FIL STOR
   Gupta D, 2010, COMMUN ACM, V53, P85, DOI 10.1145/1831407.1831429
   Holst A., 2021, StatistaJune
   HUFFMAN DA, 1952, P IRE, V40, P1098, DOI 10.1109/JRPROC.1952.273898
   Jaccard P., 1901, B SOCIT VAUDOISE SCI, V37, P547, DOI [10.5169/seals-266440, DOI 10.5169/SEALS-266440]
   Jain N, 2005, USENIX ASSOCIATION PROCEEDINGS OF THE 4TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P281
   Kulkarni P, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE GENERAL TRACK 2004 USENIX ANNUAL TECHNICAL CONFERENCE, P59
   Lin Xing, 2014, P 12 USENIX C FIL ST, P256
   MacDonald Josh, 2000, Ph.D. thesis
   Meister Dirk, 2013, P 6 INT SYST STOR C
   Muthitacharoen A., 2001, Operating Systems Review, V35, P174, DOI 10.1145/502059.502052
   Ni F, 2019, PROCEEDINGS OF THE 2019 TENTH ACM SYMPOSIUM ON CLOUD COMPUTING (SOCC '19), P220, DOI 10.1145/3357223.3362731
   Ni F, 2019, SYSTOR '19: PROCEEDINGS OF THE 12TH ACM INTERNATIONAL SYSTEMS AND STORAGE CONFERENCE, P86, DOI 10.1145/3319647.3325834
   Pucha Himabindu, 2007, P 4 USENIX S NETWORK, P15
   Quinlan S, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE FAST'02 CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P89
   Rabin M. O., 1981, Fingerprinting by random polynomials
   Reinsel David, 2018, 16 IDC
   Shilane P., 2012, 4 USENIX C HOT TOPIC, P201
   Shilane P, 2012, ACM T STORAGE, V8, DOI 10.1145/2385603.2385606
   Wildani A, 2013, PROC INT CONF DATA, P446, DOI 10.1109/ICDE.2013.6544846
   Xia W, 2016, PROCEEDINGS OF USENIX ATC '16: 2016 USENIX ANNUAL TECHNICAL CONFERENCE, P101
   Xia W, 2016, P IEEE, V104, P1681, DOI 10.1109/JPROC.2016.2571298
   Xia W, 2016, IEEE T COMPUT, V65, P1692, DOI 10.1109/TC.2015.2456015
   Xia W, 2014, PERFORM EVALUATION, V79, P258, DOI 10.1016/j.peva.2014.07.016
   Xia Wen, 2011, 2011 USENIX ANN TECH
   Xu LH, 2017, SIGMOD'17: PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P1355, DOI 10.1145/3035918.3035938
   Xu LH, 2015, ACM SOCC'15: PROCEEDINGS OF THE SIXTH ACM SYMPOSIUM ON CLOUD COMPUTING, P222, DOI 10.1145/2806777.2806840
   You LL, 2005, PROC INT CONF DATA, P804
   Zhang YC, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P121
   Zhou F, 2003, LECT NOTES COMPUT SC, V2672, P1
   Zhu B, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P269
   ZIV J, 1977, IEEE T INFORM THEORY, V23, P337, DOI 10.1109/TIT.1977.1055714
   ZIV J, 1978, IEEE T INFORM THEORY, V24, P530, DOI 10.1109/TIT.1978.1055934
   Zwillinger Daniel, 1999, CRC STANDARD PROBABI, P31
NR 44
TC 9
Z9 9
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2023
VL 19
IS 3
AR 22
DI 10.1145/3584663
PG 30
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA P9QZ4
UT WOS:001053965500002
DA 2024-07-18
ER

PT J
AU Akgun, IU
   Aydin, AS
   Burford, A
   McNeill, M
   Arkhangelskiy, M
   Zadok, E
AF Akgun, Ibrahim Umit
   Aydin, Ali Selman
   Burford, Andrew
   McNeill, Michael
   Arkhangelskiy, Michael
   Zadok, Erez
TI Improving Storage Systems Using Machine Learning
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Operating systems; storage systems; Machine Learning; storage
   performance optimization
ID FILE; FRAMEWORK
AB Operating systems include many heuristic algorithms designed to improve overall storage performance and throughput. Because such heuristics cannot work well for all conditions and workloads, system designers resorted to exposing numerous tunable parameters to users-thus burdening users with continually optimizing their own storage systems and applications. Storage systems are usually responsible for most latency in I/O-heavy applications, so even a small latency improvement can be significant. Machine learning (ML) techniques promise to learn patterns, generalize from them, and enable optimal solutions that adapt to changing workloads. We propose that ML solutions become a first-class component in OSs and replace manual heuristics to optimize storage systems dynamically. In this article, we describe our proposedML architecture, called KML. We developed a prototype KML architecture and applied it to two case studies: optimizing readahead and NFS read-size values. Our experiments show that KML consumes less than 4 KB of dynamic kernel memory, has a CPU overhead smaller than 0.2%, and yet can learn patterns and improve I/O throughput by as much as 2.3x and 15x for two case studies-even for complex, never-seen-before, concurrently running mixed workloads on different storage devices.
C1 [Akgun, Ibrahim Umit; Aydin, Ali Selman; Burford, Andrew; McNeill, Michael; Arkhangelskiy, Michael; Zadok, Erez] SUNY Stony Brook, Dept Comp Sci, New Comp Sci Bldg, Stony Brook, NY 11794 USA.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Stony Brook
RP Akgun, IU (corresponding author), SUNY Stony Brook, Dept Comp Sci, New Comp Sci Bldg, Stony Brook, NY 11794 USA.
EM iakgun@cs.stonybrook.edu; aaydin@cs.stonybrook.edu;
   aburford@cs.stonybrook.edu; mmcneill@cs.stonybrook.edu;
   markhangelsk@cs.stonybrook.edu; ezk@cs.stonybrook.edu
OI McNeill, Michael/0000-0003-1082-2916; Zadok, Erez/0000-0001-5248-9184
FU IBM; SUNY/IBMAlliance award; NSF [CNS-1729939, CNS-1900706, CCF-1918225,
   CNS-1951880, CNS-2106263, CNS-2106434, CNS-2214980]
FX This work wasmade possible in part thanks to Dell-EMC, NetApp, Facebook,
   and IBM support; a SUNY/IBMAlliance award; and NSF awards CNS-1729939,
   CNS-1900706, CCF-1918225, CNS-1951880, CNS-2106263, CNS-2106434, and
   CNS-2214980.
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Abd-El-Malek Michael, 2005, P FAST 05 C FILE STO
   Agarwal R., 2020, ARXIV
   Ahmed A, 2002, IEEE IPCCC, P131, DOI 10.1109/IPCCC.2002.995144
   Akgun Ibrahim Umit, 2020, P 13 ACM INT SYSTEMS
   Al Maruf H, 2020, PROCEEDINGS OF THE 2020 USENIX ANNUAL TECHNICAL CONFERENCE, P843
   Amvrosiadis George, 2019, DAT STOR RES VIS 202
   [Anonymous], 2023, ACM T STORAGE, V19
   [Anonymous], 2019, RocksDB
   Atikoglu Berk, 2012, Performance Evaluation Review, V40, P53, DOI 10.1145/2318857.2254766
   Bengio Yoshua, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P437, DOI 10.1007/978-3-642-35289-8_26
   Bo Dong, 2010, Proceedings of the 2010 IEEE 2nd International Conference on Cloud Computing Technology and Science (CloudCom 2010), P41, DOI 10.1109/CloudCom.2010.60
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324
   Cao Z, 2020, PROCEEDINGS OF THE 18TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P43
   Cao Z, 2018, PROCEEDINGS OF THE 2018 USENIX ANNUAL TECHNICAL CONFERENCE, P893
   Cao Z, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P329
   Cao Zhen, 2018, P ANN USENIX TECHNIC
   Cao ZC, 2020, PROCEEDINGS OF THE 18TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P209
   Chakraborttii C., 2020, ICML PKDD
   Chen H, 2019, INT SYMP DISTR COMPU, P13, DOI 10.1109/DCABES48411.2019.00011
   Cherubini G, 2017, IEEE DATA MINING, P823, DOI 10.1109/ICDM.2017.99
   Choi J, 2019, IEEE GLOB COMM CONF, DOI 10.1109/globecom38437.2019.9013332
   CNTK, 2020, CNTK
   Cortez E, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P153, DOI 10.1145/3132747.3132772
   Dai YF, 2020, PROCEEDINGS OF THE 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '20), P155
   De Sa C, 2017, 44TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA 2017), P561, DOI 10.1145/3079856.3080248
   Delimitrou C, 2014, ACM SIGPLAN NOTICES, V49, P127, DOI 10.1145/2541940.2541941
   Delimitrou C, 2013, ACM SIGPLAN NOTICES, V48, P77, DOI 10.1145/2499368.2451125
   DESNOYERS M., 2008, Using the Linux Kernel Tracepoints
   Ding XN, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 2007 USENIX ANNUAL TECHNICAL CONFERENCE, P261
   Dlib, 2020, DLIB C LIB
   Dong M, 2018, PROCEEDINGS OF THE 15TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION (NSDI'18), P343
   ELL, 2020, EMBEDDED LEARNING LI
   Fox C., 2008, Modeling, Analysis and Simulation of Computers and Telecommunication Systems, P1
   Ganfure GO, 2020, IEEE T COMPUT AID D, V39, P3311, DOI 10.1109/TCAD.2020.3012173
   Gupta S, 2015, PR MACH LEARN RES, V37, P1737
   Haiyan Hu, 2010, Proceedings 2010 9th International Conference on Grid and Cloud Computing (GCC 2010), P213, DOI 10.1109/GCC.2010.51
   Hall LO, 2003, IEEE SYS MAN CYBERN, P2851
   Hao MZ, 2020, PROCEEDINGS OF THE 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '20), P173
   Hao MZ, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P168, DOI 10.1145/3132747.3132774
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hubara I, 2018, J MACH LEARN RES, V18
   Jeyakumar JV., 2020, ADV NEURAL INFORM PR, V33, P4211
   Jingde Chen, 2020, APSys 20. Proceedings of the 2020 SIGOPS Asia-Pacific Workshop on Systems, P67, DOI 10.1145/3409963.3410492
   Juszczak Chet, 1994, P WINTER 1994 USENIX, P1
   Kaelbling LP, 1996, J ARTIF INTELL RES, V4, P237, DOI 10.1613/jair.301
   KIEFER J, 1952, ANN MATH STAT, V23, P462, DOI 10.1214/aoms/1177729392
   Kim D, 2018, PROCEEDINGS OF THE 27TH USENIX SECURITY SYMPOSIUM, P851
   Kraska T, 2018, INT CONF MANAGE DATA, P489, DOI 10.1145/3183713.3196909
   Kraska Tim, 2019, P 9 BIENN C INN DAT
   Kroeger TM, 2001, USENIX ASSOCIATION PROCEEDINGS OF THE 2001 USENIX ANNUAL TECHNICAL CONFERENCE, P105
   Laga A., 2016, 2016 5th Non-Volatile Memory Systems and Applications Symposium (NVMSA), P1
   Lai LZ, 2017, Arxiv, DOI arXiv:1703.03073
   Lee S, 2018, J SUPERCOMPUT, V74, P2870, DOI 10.1007/s11227-018-2333-6
   Li Daixuan, 2021, arXiv
   Li Z., 2014, P 6 USENIX C HOT TOP
   Liang Shuang., 2007, Proceedings of the 27th IEEE International Conference on Distributed Computing Systems (ICDCS'07), P64
   Liao JW, 2017, IEEE T CLOUD COMPUT, V5, P550, DOI 10.1109/TCC.2015.2417560
   Liaw Richard, 2018, arXiv
   Lin DD, 2016, PR MACH LEARN RES, V48
   Lin J., 2022, ARXIV
   Linux, 2021, LIN KERNELMODULE SIG
   LTTng, 2019, LTTNG OP SOURC TRAC
   Maas M, 2020, TWENTY-FIFTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXV), P541, DOI 10.1145/3373376.3378525
   Manning Paul, 2009, BEST PRACTICES RUNNI
   Nair V., 2010, P 27 INT C MACHINE L, P807
   Nalajala Anusha., 2019, 2019 10th International Conference on Computing, Communication and Networking Technologies (ICCCNT), P1
   Negi Atul., 2005, TENCON_2005_-_2005_IEEE_Region_10_Conference, P1, DOI DOI 10.1109/TENCON.2005.300837
   Oracle Corporation, 2020, MYSQL
   Paszke A, 2019, ADV NEUR IN, V32
   Pearson K., 1895, P R SOC LOND, V58, P240, DOI [DOI 10.1098/RSPL.1895.0041, 10.1098/rspl.1895.0041]
   Pearson K, 1901, PHILOS MAG, V2, P559, DOI 10.1080/14786440109462720
   Ras G., 2018, Explainable and interpretable models in computer vision and machine learning, P19, DOI DOI 10.1007/978-3-319-98131-4_2
   Ravichandran Natarajan, 2005, THESIS U HOUSTON
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   RUEMMLER C, 1994, COMPUTER, V27, P17, DOI 10.1109/2.268881
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Sa CD, 2018, Arxiv, DOI arXiv:1803.03383
   Samek W, 2021, Arxiv, DOI [arXiv:2003.07631, 10.1109/JPROC.2021.3060483]
   Schindler Jiri, 2011, 9 USENIX C FILE STOR
   Sehgal P., 2010, Proc. of the USENIX Conference on File and Storage Technologies (FAST'10), P253
   Shriver E., 1998, Performance Evaluation Review, V26, P182, DOI 10.1145/277858.277906
   Shriver E, 1999, PROCEEDINGS OF THE 1999 USENIX ANNUAL TECHNICAL CONFERENCE, P71
   Siracusano G, 2020, Arxiv, DOI arXiv:2009.02353
   Sironi F, 2012, DES AUT CON, P856
   SOD, 2020, SOD AN EMB MOD COMP
   Somashekar G, 2021, PROCEEDINGS OF THE 1ST WORKSHOP ON MACHINE LEARNING AND SYSTEMS (EUROMLSYS'21), P7, DOI 10.1145/3437984.3458828
   Somasundaram Kalyanasundaram, 2020, IMPACT SLOW NFS DATA
   Subedi P, 2018, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE, AND ANALYSIS (SC'18)
   *T PROC PERF COUNC, 1999, TPC BENCHM H DEC SUP
   Tarasov Vasily, 2011, P HOTOS 13 13 USENIX
   TensorFlow Lite, 2020, TENSORFLOW LIT
   Tran N, 2004, IEEE T PARALL DISTR, V15, P362, DOI 10.1109/TPDS.2004.1271185
   Umit Akgun Ibrahim, 2021, P 13 ACM WORKSHOP HO, DOI DOI 10.1145/3465332.3470875
   Uppal AJ, 2012, IEEE S MASS STOR SYS
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vietri Giuseppe, 2018, P 10 USENIX C HOT TO
   Whittle GAS, 2003, IEEE S MASS STOR SYS, P230, DOI 10.1109/MASS.2003.1194860
   wikipedia, 2022, RECURRENT NEURAL NET
   Won J, 2018, IEEE ACCESS, V6, P46213, DOI 10.1109/ACCESS.2018.2864820
   Wu Fengguang, 2008, Operating Systems Review, V42, P75, DOI 10.1145/1400097.1400106
   Xu CF, 2011, COMPUT J, V54, P1741, DOI 10.1093/comjnl/bxq091
   Xu XF, 2020, DES AUT TEST EUROPE, P720, DOI 10.23919/DATE48585.2020.9116382
   Yadgar G, 2021, ACM T STORAGE, V17, DOI 10.1145/3423137
   Yang CK, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE FREENIX TRACK, P157
   Yiming Qiu, 2021, HotOS '21: Proceedings of the Workshop on Hot Topics in Operating Systems, P175, DOI 10.1145/3458336.3465288
   Zheng SA, 2017, PR IEEE COMP DESIGN, P49, DOI 10.1109/ICCD.2017.17
NR 107
TC 1
Z9 1
U1 2
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD FEB
PY 2023
VL 19
IS 1
AR 9
DI 10.1145/3568429
PG 30
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 9I5DE
UT WOS:000939530200009
DA 2024-07-18
ER

PT J
AU Shu, JW
   Chen, YM
   Wang, Q
   Zhu, BH
   Li, JR
   Lu, YY
AF Shu, Jiwu
   Chen, Youmin
   Wang, Qing
   Zhu, Bohong
   Li, Junru
   Lu, Youyou
TI TH-DPMS: Design and Implementation of an RDMA-enabled Distributed
   Persistent Memory Storage System
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Storage system; distributed system; remote direct memory access;
   persistent memory
ID PHASE-CHANGE MEMORY; CONSISTENCY
AB The rapidly increasing data in recent years requires the datacenter infrastructure to store and process data with extremely high throughput and low latency. Fortunately, persistent memory (PM) and RDMA technologies bring new opportunities towards this goal. Both of them are capable of delivering more than 10 GB/s of bandwidth and sub-microsecond latency. However, our past experiences and recent studies show that it is non-trivial to build an efficient and distributed storage system with such new hardware. In this article, we design and implement TH-DPMS (TsingHua Distributed Persistent Memory System) based on persistent memory and RDMA, which unifies the memory, file system, and key-value interface in a single system. TH-DPMS is designed based on a unified distributed persistent memory abstract, pDSM. pDSM acts as a generic layer to connect the PMs of different storage nodes via high-speed RDMA network and organizes them into a global shared address space. It provides the fundamental functionalities, including global address management, space management, fault tolerance, and crash consistency guarantees. Applications are enabled to access pDSM with a group of flexible and easy-to-use APIs by using either raw read/write interfaces or the transactional ones with ACID guarantees. Based on pDSM, we implement a distributed file system and a key-value store named pDFS and pDKVS, respectively. Together, they uphold TH-DPMS with high-performance, low-latency, and fault-tolerant data storage. We evaluate TH-DPMS with both micro-benchmarks and real-world memory-intensive workloads. Experimental results show that TH-DPMS is capable of delivering an aggregated bandwidth of 120 GB/s with 6 nodes. When processing memory-intensive workloads such as YCSB and Graph500, TH-DPMS improves the performance by one order of magnitude compared to existing systems and keeps consistent high efficiency when the workload size grows to multiple terabytes.
C1 [Shu, Jiwu; Chen, Youmin; Wang, Qing; Zhu, Bohong; Li, Junru; Lu, Youyou] Tsinghua Univ, Dept Comp Sci & Technol, 30 Shuangqing Rd,201 East Main Bldg, Beijing 100084, Peoples R China.
C3 Tsinghua University
RP Lu, YY (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, 30 Shuangqing Rd,201 East Main Bldg, Beijing 100084, Peoples R China.
EM shujw@tsinghua.edu.cn; chenym16@mails.tsinghua.edu.cn;
   q-wang18@mails.tsinghua.edu.cn; zhubh18@mails.tsinghua.edu.cn;
   lijr19@mails.tsinghua.edu.cn; luyouyou@tsinghua.edu.cn
OI Li, Junru/0000-0001-7387-7899; Chen, Youmin/0000-0003-4171-4299
FU National Key Research and Development Program of China [2018YFB1003301];
   National Natural Science Foundation of China [61772300, 61832011];
   Huawei
FX This work is supported by National Key Research and Development Program
   of China (Grant No. 2018YFB1003301), the National Natural Science
   Foundation of China (Grant No. 61772300, 61832011), and Huawei.
CR Aguilera MK, 2018, PROCEEDINGS OF THE 2018 USENIX ANNUAL TECHNICAL CONFERENCE, P775
   Arulraj J, 2015, SIGMOD'15: PROCEEDINGS OF THE 2015 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P707, DOI 10.1145/2723372.2749441
   Baek IG, 2004, IEEE INTERNATIONAL ELECTRON DEVICES MEETING 2004, TECHNICAL DIGEST, P587, DOI 10.1109/IEDM.2004.1419228
   Berkeley Architecture Research, 2020, FIR PROJ
   Boyd-Wickizer Silas, 2014, OPLOG LIB SCAL ING U
   CARTER JB, 1995, ACM T COMPUT SYST, V13, P205, DOI 10.1145/210126.210127
   Chen SM, 2015, PROC VLDB ENDOW, V8, P786, DOI 10.14778/2752939.2752947
   Chen YM, 2019, IEEE T COMPUT, V68, P1147, DOI 10.1109/TC.2018.2870137
   Chen YM, 2019, PROCEEDINGS OF THE FOURTEENTH EUROSYS CONFERENCE 2019 (EUROSYS '19), DOI 10.1145/3302424.3303968
   Chen YM, 2018, ACM T STORAGE, V14, DOI 10.1145/3204454
   Chen Youmin, 2019, ARXIVCSOS190810740
   Coburn J, 2011, ACM SIGPLAN NOTICES, V46, P105, DOI [10.1145/1961295.1950380, 10.1145/1961296.1950380]
   Condit J, 2009, SOSP'09: PROCEEDINGS OF THE TWENTY-SECOND ACM SIGOPS SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P133
   Cooper Brian F., 2010, P 1 ACM S CLOUD COMP, P143, DOI [DOI 10.1145/1807128.1807152, 10.1145/1807128.1807152]
   Dong Mingkai, 2019, P 27 ACM S OP SYST P
   Dragojevic A, 2015, SOSP'15: PROCEEDINGS OF THE TWENTY-FIFTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P54, DOI 10.1145/2815400.2815425
   Dragojevic Aleksandar, 2014, NSDI 14, P401
   Dulloor Subramanya R., 2014, P EUR C COMP SYST EU, P1, DOI [10.1145/2592798.2592814, DOI 10.1145/2592798.2592814]
   Fabian JH, 2000, MESA MG, P117
   Färber F, 2011, SIGMOD REC, V40, P45, DOI 10.1145/2094114.2094126
   Ghose S, 2018, P ACM MEAS ANAL COMP, V2, DOI 10.1145/3224419
   Hoseinzadeh Morteza, 2019, ARXIV190411560
   HP Development Company, 2020, MACH PROJ
   Hwang D, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P187
   Hwang T, 2015, ACM T STORAGE, V11, DOI 10.1145/2629619
   Ichiki T, 2020, CARDIORENAL SYNDROME IN HEART FAILURE, P23, DOI 10.1007/978-3-030-21033-5_3
   IDC, 2020, DIGITAL UNIVERSE OPP
   Intel, 2019, Intel Optane DC persistent memory
   Intel Corp, 2020, PMDK PERS MEM DEV KI
   Intel Corporation, 2020, PMEMKV
   Intel Corporation, 2019, DISTR AS OBJ STOR
   Intel Corporation, 2020, INT DAT DIR I O TECH
   Islam NusratSharmin., 2016, ICS PAGE, P8, DOI DOI 10.1145/2925426.2926290
   Izraelevitz J., 2019, BASIC PERFORMANCE ME
   Jose J., 2011, 2011 International Conference on Parallel Processing, P743, DOI 10.1109/ICPP.2011.37
   Kadekodi R, 2019, PROCEEDINGS OF THE TWENTY-SEVENTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '19), P494, DOI 10.1145/3341301.3359631
   Kalia A, 2019, PROCEEDINGS OF THE 16TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P1
   Kalia A, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P185
   Kalia A, 2016, PROCEEDINGS OF USENIX ATC '16: 2016 USENIX ANNUAL TECHNICAL CONFERENCE, P437
   Kalia A, 2014, SIGCOMM'14: PROCEEDINGS OF THE 2014 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P295, DOI [10.1145/2619239.2626299, 10.1145/2740070.2626299]
   Kannan S, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P241
   Karger David, 1997, P 29 ANN ACM S THEOR, P654, DOI DOI 10.1145/258533.258660
   KASHYAP S., 2019, ARXIV190902092
   KELEHER P, 1994, PROCEEDINGS OF THE WINTER 1994 USENIX CONFERENCE, P115
   Kwon Y, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P460, DOI 10.1145/3132747.3132770
   Lee BC, 2009, CONF PROC INT SYMP C, P2, DOI 10.1145/1555815.1555758
   Lee SK, 2019, PROCEEDINGS OF THE TWENTY-SEVENTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '19), P462, DOI 10.1145/3341301.3359635
   Li BJ, 2019, SIGCOMM '19 - PROCEEDINGS OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P90, DOI 10.1145/3341302.3342071
   Li BJ, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P137, DOI 10.1145/3132747.3132756
   Li K., 1988, Proceedings of the 1988 International Conference on Parallel Processing, P94
   Li SY, 2017, SC'17: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS, DOI 10.1145/3126908.3126928
   Liu MX, 2017, TWENTY-SECOND INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXII), P329, DOI 10.1145/3037697.3037714
   Lu YY, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P773
   Lu YY, 2016, ACM T STORAGE, V12, DOI 10.1145/2851504
   Lu YY, 2014, PR IEEE COMP DESIGN, P209
   Ma Teng, 2020, P 25 INT C ARCH SUPP
   Mellanox Technologies, 2019, CONNECTX 6 VPI CARD
   Mitchell Christopher, 2013, P USENIX ANN TECHN C, P103
   Nam M, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P31
   Nelson Jacob, 2015, USENIX ATC, P291
   Nishtala R., 2013, Proceedings of the 10th USENIX Conference on Networked Systems Design and Implementation, NSDI '13, Berkeley, CA, USA, P385
   Novakovic S, 2019, SYSTOR '19: PROCEEDINGS OF THE 12TH ACM INTERNATIONAL SYSTEMS AND STORAGE CONFERENCE, P97, DOI 10.1145/3319647.3325827
   Novakovic S, 2014, ACM SIGPLAN NOTICES, V49, P3, DOI 10.1145/2541940.2541965
   Ongaro D., 2014, 2014 USENIX ANN TECH, P305
   Ou JX, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901324
   Oukid I, 2016, SIGMOD'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P371, DOI 10.1145/2882903.2915251
   Poke Marius, 2015, P 24 INT S HIGH PERF, P107, DOI DOI 10.1145/2749246.2749267
   Qureshi MK, 2009, CONF PROC INT SYMP C, P24, DOI 10.1145/1555815.1555760
   Red Hat. Inc, 2019, GLUSTERFS
   Shamis A, 2019, INT CONF MANAGE DATA, P433, DOI 10.1145/3299869.3300069
   Shan YZ, 2017, PROCEEDINGS OF THE 2017 SYMPOSIUM ON CLOUD COMPUTING (SOCC '17), P323, DOI 10.1145/3127479.3128610
   Stuedi P, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P767
   Swift, 2014, P 9 EUR C COMP SYST, DOI DOI 10.1145/2592798.2592810
   Venkataraman Shivaram, 2011, P 9 USENIX C FIL STO, P5
   Volos Haris, 2011, SIGPLAN Notices, V46, P91, DOI 10.1145/1961296.1950379
   Wei XD, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P233
   Wei XD, 2015, SOSP'15: PROCEEDINGS OF THE TWENTY-FIFTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P87, DOI 10.1145/2815400.2815419
   Weil Sage A., 2007, PDSW, P35
   Wu Xiaojian., 2011, Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis (SC '11), p39:1
   Xia F, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P349
   Xu J, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P478, DOI 10.1145/3132747.3132761
   Xu J, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P323
   Yang J, 2020, PROCEEDINGS OF THE 17TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P111
   Yang J, 2020, PROCEEDINGS OF THE 18TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P169
   Yang J, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P221
   Yang Jun, 2015, P 13 USENIX C FIL ST, P167
   Zaharia Matei, 2010, 2 USENIX WORKSHOP HO
   Zeng KS, 2017, DES AUT TEST EUROPE, P1769, DOI 10.23919/DATE.2017.7927279
   Zhang YY, 2015, ACM SIGPLAN NOTICES, V50, P3, DOI [10.1145/2694344.2694370, 10.1109/OECC.2015.7340093]
   Zheng SA, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P207
   Zhou P, 2009, CONF PROC INT SYMP C, P14, DOI 10.1145/1555815.1555759
   Zhou YY, 2015, NEURODEGENER DIS, V15, P1, DOI 10.1159/000369466
   Zuo PF, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P461
NR 93
TC 13
Z9 14
U1 1
U2 19
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2020
VL 16
IS 4
AR 24
DI 10.1145/3412852
PG 31
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA PA3GG
UT WOS:000595524400004
DA 2024-07-18
ER

PT J
AU Ye, LQ
   Feng, D
   Hu, YC
   Wei, XL
AF Ye, Liuqing
   Feng, Dan
   Hu, Yuchong
   Wei, Xueliang
TI Hybrid Codes: Flexible Erasure Codes with Optimized Recovery Performance
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Erasure coding
ID DISTRIBUTED STORAGE; ACCESS; SYSTEM
AB Erasure codes are being extensively deployed in practical storage systems to prevent data loss with low redundancy. However, these codes require excessive disk I/Os and network traffic for recovering unavailable data. Among all erasure codes, Minimum Storage Regenerating (MSR) codes can achieve optimal repair bandwidth under the minimum storage during recovery, but some open issues remain to be addressed before applying them in real systems.
   Facing with the huge burden during recovery, erasure-coded storage systems need to be developed with high repair efficiency. Aiming at this goal, a new class of coding scheme is introduced-Hybrid Regenerating Codes (Hybrid-RC). The codes utilize the superiority of MSR codes to compute a subset of data blocks while some other parity blocks are used for reliability maintenance. As a result, our design is near-optimal with respect to storage and network traffic and shows great improvements in recovery performance.
C1 [Ye, Liuqing; Feng, Dan; Hu, Yuchong; Wei, Xueliang] Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan Natl Lab Optoelect, Key Lab Informat Storage Syst,Minist Educ China, 1037 Luoyu Rd, Wuhan 430074, Peoples R China.
C3 Huazhong University of Science & Technology
RP Ye, LQ (corresponding author), Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan Natl Lab Optoelect, Key Lab Informat Storage Syst,Minist Educ China, 1037 Luoyu Rd, Wuhan 430074, Peoples R China.
EM liuqingye@hust.edu.cn; dfeng@hust.edu.cn; yuchonghu@hust.edu.cn;
   xueliang_wei@hust.edu.cn
FU National Natural Science Foundation of China [61821003, 61832007,
   U1705261, 61772222]
FX This work was supported by the National Natural Science Foundation of
   China No. 61821003, No. 61832007, No. U1705261, and No. 61772222.
CR Amer A, 2007, I W STOR NETW ARCH, P11, DOI 10.1109/SNAPI.2007.20
   [Anonymous], 2018, STORAGE UMASS TRACE
   [Anonymous], 2010, STORAGE ARCHITECTURE
   Borthakur Dhruba, 2007, Hadoop Proj. Website, V2007, P21
   Chen HCH, 2014, IEEE T COMPUT, V63, P31, DOI 10.1109/TC.2013.167
   Dimakis AG, 2010, IEEE T INFORM THEORY, V56, P4539, DOI 10.1109/TIT.2010.2054295
   Dimakis Alexandros G., 2007, P 3 WORKSH NETW COD
   Ford Daniel, 2010, P USENIX OSDI
   Gad EE, 2013, IEEE INT SYMP INFO, P887, DOI 10.1109/ISIT.2013.6620354
   Ghemawat S., 2003, Operating Systems Review, V37, P29, DOI 10.1145/1165389.945450
   HOWARD JH, 1988, ACM T COMPUT SYST, V6, P51, DOI 10.1145/35037.35059
   Huang C, 2013, ACM T STORAGE, V9, DOI 10.1145/2435204.2435207
   Huang Cheng, 2012, 2012 USENIX ANN TECH, P15
   Khan Osama., 2012, FAST, P20
   Kralevska K, 2018, IEEE T BIG DATA, V4, P516, DOI 10.1109/TBDATA.2017.2749255
   Liu Q, 2017, J NONLINEAR SCI, V2017, P1
   Liu Q, 2015, SYM REL DIST SYST, P212, DOI 10.1109/SRDS.2015.18
   Luo JQ, 2014, IEEE T COMPUT, V63, P2259, DOI 10.1109/TC.2013.23
   MacWilliams F. J., 1977, The theory of error-correcting codes. II
   Muralidhar S., 2014, P 11 USENIX C OP SYS, P383
   Pamies-Juarez L, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P81
   Plank J., 2013, USENIX C FILE STOR T, P299
   PLANK JAMESS., 2009, FAST, P253
   Plank JS, 1997, SOFTWARE PRACT EXPER, V27, P995, DOI 10.1002/(SICI)1097-024X(199709)27:9<995::AID-SPE111>3.0.CO;2-6
   Rashmi K., 2015, USENIX C FIL STOR TE, P81
   Rashmi KV, 2014, SIGCOMM'14: PROCEEDINGS OF THE 2014 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P331, DOI [10.1145/2619239.2626325, 10.1145/2740070.2626325]
   Rashmi KV, 2011, IEEE T INFORM THEORY, V57, P5227, DOI 10.1109/TIT.2011.2159049
   Rashmi K.V., 2013, P 5 USENIX C HOT TOP, P8
   Sathiamoorthy M, 2013, PROC VLDB ENDOW, V6, P325, DOI 10.14778/2535573.2488339
   SATYANARAYANAN M, 1990, IEEE T COMPUT, V39, P447, DOI 10.1109/12.54838
   Shah N. B., 2010, COMM NCC 2010 NAT C, P1
   Shah NB, 2012, IEEE T INFORM THEORY, V58, P2134, DOI 10.1109/TIT.2011.2178588
   Siewiorek D., 2014, Reliable Computer Systems: Design and Evaluatuion
   Storage Networking Industry Association, 2018, STORAGE NETWORKING I
   Suh CH, 2011, IEEE T INFORM THEORY, V57, P1425, DOI 10.1109/TIT.2011.2105003
   Tamo I, 2014, IEEE T INFORM THEORY, V60, P2028, DOI 10.1109/TIT.2014.2305698
   Tamo I, 2013, IEEE T INFORM THEORY, V59, P1597, DOI 10.1109/TIT.2012.2227110
   Vajha M, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P139
   Wang ZY, 2016, IEEE T INFORM THEORY, V62, P4466, DOI 10.1109/TIT.2016.2553675
   Xin Q, 2003, IEEE S MASS STOR SYS, P146, DOI 10.1109/MASS.2003.1194851
   Yang AB, 2015, PROCEEDINGS OF CROSS-CULTURAL OCCUPATIONAL HEALTH PSYCHOLOGY FORUM, P213
   Ye M, 2017, IEEE T INFORM THEORY, V63, P6307, DOI 10.1109/TIT.2017.2730863
   Ye M, 2017, IEEE T INFORM THEORY, V63, P2001, DOI 10.1109/TIT.2017.2661313
   Zhou TL, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P317
NR 44
TC 5
Z9 6
U1 1
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2020
VL 16
IS 4
AR 26
DI 10.1145/3407193
PG 26
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA PA3GG
UT WOS:000595524400006
DA 2024-07-18
ER

PT J
AU Jaffer, S
   Maneas, S
   Hwang, A
   Schroeder, B
AF Jaffer, Shehbaz
   Maneas, Stathis
   Hwang, Andy
   Schroeder, Bianca
TI The Reliability of Modern File Systems in the face of SSD Errors
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Recovery; file systems; solid state drives; reliability
ID NAND FLASH MEMORY; RETENTION; RECOVERY
AB As solid state drives (SSDs) are increasingly replacing hard disk drives, the reliability of storage systems depends on the failure modes of SSDs and the ability of the file system layered on top to handle these failure modes. While the classical paper on IRON File Systems provides a thorough study of the failure policies of three file systems common at the time, we argue that 13 years later it is time to revisit file system reliability with SSDs and their reliability characteristics in mind, based on modern file systems that incorporate journaling, copy-on-write, and log-structured approaches and are optimized for flash. This article presents a detailed study, spanning ext4, Btrfs, and F2FS, and covering a number of different SSD error modes. We develop our own fault injection framework and explore over 1,000 error cases. Our results indicate that 16% of these cases result in a file system that cannot be mounted or even repaired by its system checker. We also identify the key file system metadata structures that can cause such failures, and, finally, we recommend some design guidelines for file systems that are deployed on top of SSDs.
C1 [Jaffer, Shehbaz; Maneas, Stathis; Hwang, Andy; Schroeder, Bianca] Univ Toronto, 27 Kings Coll Circle, Toronto, ON M5S 1A1, Canada.
C3 University of Toronto
RP Jaffer, S (corresponding author), Univ Toronto, 27 Kings Coll Circle, Toronto, ON M5S 1A1, Canada.
EM shehbaz@cs.toronto.edu; smaneas@cs.toronto.edu; hwang@cs.toronto.edu;
   bianca@cs.toronto.edu
RI Jaffer, Shehbaz/ABA-4337-2021; Jaffer, Shehbaz/AAZ-9307-2021
OI Maneas, Stathis/0000-0002-9742-3960
CR Agrawal Nitin, 2008, P USENIX ANN TECHN C, P57
   [Anonymous], 2019, F2FS BUG REPORT
   [Anonymous], 2019, ERRNO LINUX PROGRAMM
   [Anonymous], 2019, SCSI ERROR HANDLING
   [Anonymous], 2019, SATA Specification
   [Anonymous], 2019, GITHUB CODE REPOSITO
   [Anonymous], 2019, NVM EXPRESS SPECIFIC
   [Anonymous], 2009, P 42 ANN IEEE ACM IN, DOI DOI 10.1145/1669112.1669118
   [Anonymous], 2019, F2FS PATCH FILE
   [Anonymous], 2019, F2FS BUG REPORT WRIT
   [Anonymous], 2019, FS VERITY FILE SYSTE
   [Anonymous], 2019, BTRFS MKFS MAN PAGE
   [Anonymous], 2019, BTRFS BUG REPORT
   Bairavasundaram Lakshmi N., 2008, ACM Transaction on Storage, V4, DOI 10.1145/1416944.1416947
   Bairavasundaram LN, 2008, I C DEPEND SYS NETWO, P502, DOI 10.1109/DSN.2008.4630121
   Belgal HP, 2002, INT RELIAB PHY SYM, P7, DOI 10.1109/RELPHY.2002.996604
   Bjorling M, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P359
   Boboila S., 2010, P 8 USENIX C FIL STO, P9
   BRAND A, 1993, INT REL PHY, P127, DOI 10.1109/RELPHY.1993.283291
   Cai Y, 2017, P IEEE, V105, P1666, DOI 10.1109/JPROC.2017.2713127
   Cai Y, 2017, INT S HIGH PERF COMP, P49, DOI 10.1109/HPCA.2017.61
   Cai Y, 2015, INT S HIGH PERF COMP, P551, DOI 10.1109/HPCA.2015.7056062
   Cai Y, 2013, 2013 IEEE 31ST INTERNATIONAL CONFERENCE ON COMPUTER DESIGN (ICCD), P123, DOI 10.1109/ICCD.2013.6657034
   Cai Y, 2012, DES AUT TEST EUROPE, P521
   Cai Y, 2012, PR IEEE COMP DESIGN, P94, DOI 10.1109/ICCD.2012.6378623
   Cao JR, 2018, INTERNATIONAL CONFERENCE ON SUPERCOMPUTING (ICS 2018), P1, DOI 10.1145/3205289.3205302
   CAPPELLETTI P, 1994, INTERNATIONAL ELECTRON DEVICES MEETING 1994 - IEDM TECHNICAL DIGEST, P291, DOI 10.1109/IEDM.1994.383410
   Chen F, 2009, PERF E R SI, V37, P181
   Chidambaram V., 2012, Proceedings of the 10th USENIX conference on File and Storage Technologies, FAST 2012, San Jose, CA, USA, February 14-17, 2012, P9
   Degraeve R, 2004, IEEE T ELECTRON DEV, V51, P1392, DOI 10.1109/TED.2004.833583
   Edge Jake, 2018, FILE LEVEL INTEGRITY
   Fryer D, 2012, ACM T STORAGE, V8, DOI 10.1145/2385603.2385608
   Ganesan A, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P149
   Gatla OR, 2018, ACM T STORAGE, V14, DOI 10.1145/3281031
   Grupp L. M., 2012, FAST 12 P 10 USENIX
   Gunasekara H, 2018, 2018 INTERNATIONAL CONFERENCE ON PRODUCTION AND OPERATIONS MANAGEMENT SOCIETY (POMS)
   Gunawi HS, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P207
   Jae-Duk Lee, 2006, 21st Non-Volatile Semiconductor Memory Workshop. (IEEE Cat. No. 06EX1246), P31, DOI 10.1109/.2006.1629481
   Joo SJ, 2006, JPN J APPL PHYS 1, V45, P6210, DOI 10.1143/JJAP.45.6210
   Jung Myoungsoo, 2013, ACM INT C MEAS MOD C, P203, DOI DOI 10.1145/2465529.2465548
   Kumar H, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P197
   Lee Changman, 2015, 13 USENIX C FILE STO, P273
   Liu R.-S., 2012, Proceedings of the 10th USENIX Conference on File and Storage Technologies, P11
   Luo YX, 2018, P ACM MEAS ANAL COMP, V2, DOI [10.1145/3224432, 10.1145/3292040.3219659]
   Luo YX, 2018, INT S HIGH PERF COMP, P504, DOI 10.1109/HPCA.2018.00050
   Martinez Ashlie, 2017, P 9 USENIX WORKSH HO
   Mathur Avantika, 2007, P LINUX S, V2, P21
   Meza Justin, 2015, ACM SIGMETRICS Performance Evaluation Review, V43, P177, DOI 10.1145/2745844.2745848
   Mielke N, 2008, INT RELIAB PHY SYM, P9, DOI 10.1109/RELPHY.2008.4558857
   Mielke N, 2006, INT RELIAB PHY SYM, P29, DOI 10.1109/RELPHY.2006.251188
   Mohan Jayashree, 2018, P 13 USENIX S OP SYS
   Munegowda Keshava, 2014, P 2 INT C EM RES COM, P342
   Narayanan I, 2016, PROCEEDINGS OF THE 9TH ACM INTERNATIONAL SYSTEMS AND STORAGE CONFERENCE (SYSTOR'16), DOI [10.1145/2964791.2901489, 10.1145/2928275.2928278]
   Panda B, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P47
   Papandreou N, 2014, PR GR LAK SYMP VLSI, P151, DOI 10.1145/2591513.2591594
   Prabhakaran Vijayan, 2005, P OF THE 20 ACM S ON, V39, P206
   Rodeh O, 2013, ACM T STORAGE, V9, DOI 10.1145/2501620.2501623
   Sanvido MAA, 2008, P IEEE, V96, P1864, DOI 10.1109/JPROC.2008.2004319
   Schroeder B, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P67
   SUH KD, 1995, ISSCC DIG TECH PAP I, V38, P128, DOI 10.1109/ISSCC.1995.535460
   Tomonori Fujita., 2006, P LINUX S LINUXSYMPO, P303
   Tseng HW, 2011, DES AUT CON, P35
   Yan SQ, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P15
   Yang Hong, 2006, 2006 8 INT C SOL STA
   Yongkun Wang, 2010, Proceedings of the 2010 IEEE International Conference on Networking, Architecture, and Storage (NAS 2010), P467, DOI 10.1109/NAS.2010.12
   Zhao K., 2013, P 11 USENIX C FILE S, P243
   Zheng M., 2013, Proceedings of the 11th USENIX Conference on File and Storage Technologies (FAST '13), P271
   Zheng M, 2017, ACM T COMPUT SYST, V34, DOI 10.1145/2992782
NR 68
TC 6
Z9 6
U1 0
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD APR
PY 2020
VL 16
IS 1
AR 2
DI 10.1145/3375553
PG 28
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1QN
UT WOS:000583742400002
DA 2024-07-18
ER

PT J
AU Wen, WD
   Li, Y
   Li, WH
   Deng, LF
   He, YX
AF Wen, Weidong
   Li, Yang
   Li, Wenhai
   Deng, Lingfeng
   He, Yanxiang
TI CORES: Towards Scan-Optimized Columnar Storage for Nested Records
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Columnar storage; nested schema; filtering-pushdown; sequential scan;
   skipping scheme; bitset composition
ID SQL; EFFICIENT; DATABASE; QUERY
AB The relatively high cost of record deserialization is increasingly becoming the bottleneck of column-based storage systems in tree-structured applications [58]. Due to record transformation in the storage layer, unnecessary processing costs derived from fields and rows irrelevant to queries may be very heavy in nested schemas, significantly wasting the computational resources in large-scale analytical workloads. This leads to the question of how to reduce both the deserialization and 10 costs of queries with highly selective filters following arbitrary paths in a nested schema.
   We present CORES (Column-Oriented Regeneration Embedding Scheme) to push highly selective filters down into column-based storage engines, where each filter consists of several filtering conditions on a field. By applying highly selective filters in the storage layer, we demonstrate that both the deserialization and I0 costs could be significantly reduced. We show how to introduce fine-grained composition on filtering results. We generalize this technique by two pair-wise operations, roll up and drilldown, such that a series of conjunctive filters can effectively deliver their payloads in nested schema. The proposed methods are implemented on an open-source platform. For practical purposes, we highlight how to build a column storage engine and how to drive a query efficiently based on a cost model. We apply this design to the nested relational model especially when hierarchical entities are frequently required by ad hoc queries. The experiments, including a real workload and the modified TPCH benchmark, demonstrate that CORES improves the performance by 0.7x-26.9x compared to state-of-the-art platforms in scan-intensive workloads.
C1 [Wen, Weidong; Li, Yang; Li, Wenhai; Deng, Lingfeng; He, Yanxiang] Wuhan Univ, Sch Comp, 229 Bayi Rd, Wuhan 430072, Hubei, Peoples R China.
C3 Wuhan University
RP Li, WH (corresponding author), Wuhan Univ, Sch Comp, 229 Bayi Rd, Wuhan 430072, Hubei, Peoples R China.
EM wwd@whu.edu.cn; liyang@whu.edu.cn; lwh@whu.edu.cn; lingfengdeng@163.com;
   yxhe@whu.edu.cn
RI Wang, Guang/JFS-8374-2023
FU National High Technology Research and Development Program of China
   [2017YFC08038, 2013AA12A301]; National Science Foundation of China
   [61572373, 61472290, 60903035]
FX This research was sponsored by the National High Technology Research and
   Development Program of China, grant 2017YFC08038 and 2013AA12A301, and
   the National Science Foundation of China, grant 61572373, 61472290, and
   60903035.
CR Afrati FN, 2014, PROC VLDB ENDOW, V7, P1131, DOI 10.14778/2732977.2732987
   Ailamaki Anastassia, 2002, DATA PAGE LAYOUTS RE, P198
   Alsubaiee S, 2014, PROC VLDB ENDOW, V7, P841, DOI 10.14778/2732951.2732958
   Alsubaiee S, 2014, PROC VLDB ENDOW, V7, P1905, DOI 10.14778/2733085.2733096
   [Anonymous], 2004, ADAPT DES
   Apache, 2017, AP HIV TM
   Bancilhon F., 1982, Proceedings of Very Large Data Bases. Eighth International Conference on Very Large Data Bases, P263
   Behzad Babak, 2013, P INT C HIGH PERF CO, P68, DOI [DOI 10.1145/2503210.2503278, 10.1145/2503210.2503278]
   Beyer K, 1999, SIGMOD RECORD, VOL 28, NO 2 - JUNE 1999, P359, DOI 10.1145/304181.304214
   Bhadkamkar Medha, 2009, ACM Transaction on Storage, V5, DOI 10.1145/1534912.1534915
   Boncz Peter., 2006, SIGMOD 06, P479
   Borkar V, 2011, PROC INT CONF DATA, P1151, DOI 10.1109/ICDE.2011.5767921
   Chasseur C, 2013, P 16 INT WORKSH WEB, V13, P1
   Chen SH, 2018, ACM T STORAGE, V14, DOI 10.1145/3177918
   Chen XQ, 2016, OPT MATER EXPRESS, V6, P610, DOI 10.1364/OME.6.000610
   Cormode G, 2011, FOUND TRENDS DATABAS, V4, P1, DOI 10.1561/1900000004
   Cornell D. W., 1987, Proceedings of the Third International Conference on Data Engineering (Cat. No.87CH2407-5), P30
   EGENHOFER MJ, 1994, IEEE T KNOWL DATA EN, V6, P86, DOI 10.1109/69.273029
   Floratou A, 2014, PROC VLDB ENDOW, V7, P1295, DOI 10.14778/2732977.2733002
   Gracia-Tinedo Raul, 2017, P USENIX C FIL STOR
   He B, 2012, IEEE T KNOWL DATA EN, V24, P1570, DOI 10.1109/TKDE.2011.73
   Jia Jianfeng, 2017, P IEEE INT C BIG DAT
   Kaufmann M, 2013, PROC VLDB ENDOW, V6, P1444, DOI 10.14778/2536274.2536333
   Lahiri T, 2015, PROC INT CONF DATA, P1253, DOI 10.1109/ICDE.2015.7113373
   Lamb A, 2012, PROC VLDB ENDOW, V5, P1790, DOI 10.14778/2367502.2367518
   Lee E, 2014, ACM T STORAGE, V10, DOI 10.1145/2633691
   Lemire Daniel, 2016, ACM INT C P SERIES, P77, DOI [10.1145/2938503:2938515, DOI 10.1145/2938503:2938515, 10.1145/2938503.2938515, DOI 10.1145/2938503.2938515]
   Liu H, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P285
   Liu ZH, 2014, SIGMOD'14: PROCEEDINGS OF THE 2014 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P1247, DOI 10.1145/2588555.2595628
   Lu P, 2013, PROC INT CONF DATA, P326, DOI 10.1109/ICDE.2013.6544836
   Mane Sagar S., 2015, DATA MINING KNOWL EN, V7, P1
   Melnik S, 2011, COMMUN ACM, V54, P114, DOI 10.1145/1953122.1953148
   Paredaens J., 1988, Proceedings of the Seventh ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems, P29, DOI 10.1145/308386.308402
   Paul H.-B., 1987, SIGMOD Record, V16, P196, DOI 10.1145/38714.38737
   Raman V, 2013, PROC VLDB ENDOW, V6, P1080, DOI 10.14778/2536222.2536233
   ROTH MA, 1988, ACM T DATABASE SYST, V13, P389, DOI 10.1145/49346.49347
   Rys M., 1994, Proceedings Eighth International Parallel Processing Symposium (Cat. No.94TH0652-8), P894, DOI 10.1109/IPPS.1994.288200
   Scholl M. H., 1987, Proceedings of the Thirteenth International Conference on Very Large Data Bases: 1987 13th VLDB, P137
   Shanbhag A, 2016, PROC VLDB ENDOW, V9, P1569, DOI 10.14778/3007263.3007311
   Shute J, 2013, PROC VLDB ENDOW, V6, P1068, DOI 10.14778/2536222.2536232
   Shvachko K., 2010, 2010 IEEE 26th Symposium on Mass Storage Systems and Technologies (MSST), P1
   Soulier L, 2017, ACM COMPUT SURV, V50, DOI 10.1145/3092696
   Stockinger K, 2001, 2001 INTERNATIONAL DATABASE ENGINEERING & APPLICATIONS SYMPOSIUM, PROCEEDINGS, P47, DOI 10.1109/IDEAS.2001.938070
   Stonebraker Mike., 2005, VLDB'05
   Sun LW, 2014, PROC VLDB ENDOW, V7, P1617, DOI 10.14778/2733004.2733044
   Sun YL, 2018, ACM T STORAGE, V14, DOI 10.1145/3177917
   Tahara D, 2014, SIGMOD'14: PROCEEDINGS OF THE 2014 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P815, DOI 10.1145/2588555.2612183
   Tatarowicz AL, 2012, PROC INT CONF DATA, P102, DOI 10.1109/ICDE.2012.26
   Wandelt S, 2014, SIGMOD REC, V43, P64, DOI 10.1145/2627692.2627706
   Wang ZY, 2017, SIGMOD'17: PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P883, DOI 10.1145/3035918.3035956
   Welch Brent, 2008, P USENIX C FIL STOR, V2
   Wu CH, 2015, ACM T STORAGE, V11, DOI 10.1145/2665067
   Xuan Pengfei, 2016, PARALLEL COMPUT, V61
   Yoshitaka A, 1999, IEEE T KNOWL DATA EN, V11, P81, DOI 10.1109/69.755617
   Yu Ying, 2009, Instrument Techniques and Sensor, P1
   Zaharia Matei, 2012, P USENIX S OP SYST D, V2
NR 56
TC 1
Z9 1
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2019
VL 15
IS 3
AR 16
DI 10.1145/3321704
PG 46
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JT2DC
UT WOS:000500805400001
DA 2024-07-18
ER

PT J
AU Kim, WH
   Seo, J
   Kim, J
   Nam, B
AF Kim, Wook-Hee
   Seo, Jihye
   Kim, Jinwoong
   Nam, Beomseok
TI clfB-tree: Cacheline Friendly Persistent B-tree for NVRAM
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Non-volatile memory; data structure; persistent indexing
ID MEMORY
AB Emerging byte-addressable non-volatile memory (NVRAM) is expected to replace block device storages as an alternative low-latency persistent storage device. If NVRAM is used as a persistent storage device, a cache line instead of a disk page will be the unit of data transfer, consistency, and durability.
   In this work, we design and develop clfB-tree-a B-tree structure whose tree node fits in a single cache line. We employ existing write combining store buffer and restricted transactional memory to provide a failure-atomic cache line write operation. Using the failure-atomic cache line write operations, we atomically update a clfB-tree node via a single cache line flush instruction without major changes in hardware. However, there exist many processors that do not provide SW interface for transactional memory. For those processors, our proposed clfB-tree achieves atomicity and consistency via in-place update, which requires maximum four cache line flushes. We evaluate the performance of clfB-tree on an NVRAM emulation board with ARM Cortex A-9 processor and a workstation that has Intel Xeon E7-4809 v3 processor. Our experimental results show clfB-tree outperforms wB-tree and CDDS B-tree by a large margin in terms of both insertion and search performance.
C1 [Kim, Wook-Hee; Seo, Jihye; Kim, Jinwoong; Nam, Beomseok] Ulsan Natl Inst Sci & Technol, Sch Elect & Comp Engn, Ulsan 44919, Ulsan, South Korea.
C3 Ulsan National Institute of Science & Technology (UNIST)
RP Nam, B (corresponding author), Ulsan Natl Inst Sci & Technol, Sch Elect & Comp Engn, Ulsan 44919, Ulsan, South Korea.
EM okie90@unist.ac.kr; jihye@unist.ac.kr; jwkim@unist.ac.kr;
   bsnam@unist.ac.kr
RI Nam, Beomseok/E-9071-2010
FU MKE/KEIT [10041608]; MSIP [R0190-15-2012]; National Research Foundation
   of Korea [2014R1A1A2058843, NRF-2016M3C4A7952634]
FX We thank the anonymous reviewers for their suggestions and comments on
   the early draft of this article. This research was supported by MKE/KEIT
   (Grant No. 10041608, Embedded System Software for New Memory based Smart
   Devices), MSIP (Grant No. R0190-15-2012, High Performance Big Data
   Analytics Platform Performance Acceleration Technologies Development),
   National Research Foundation of Korea (No. 2014R1A1A2058843 and No.
   NRF-2016M3C4A7952634, Development of Machine Learning Framework for Peta
   Flops Scale).
CR [Anonymous], 2011, P 16 INT C ARCH SUPP
   [Anonymous], 2011, FAST 5
   Bayer R., 1977, ACM Transactions on Database Systems, V2, P11, DOI 10.1145/320521.320530
   Chen SM, 2015, PROC VLDB ENDOW, V8, P786, DOI 10.14778/2752939.2752947
   Condit J., 2009, P 22 ACM S OP SYST P
   Dulloor Subramanya R., 2014, P EUR C COMP SYST EU, P1, DOI [10.1145/2592798.2592814, DOI 10.1145/2592798.2592814]
   Eunji Lee, 2013, P 11 USENIX C FIL ST
   Guerra Jorge, 2012, P USENIX ANN TECHN C
   Hwang Taeho, 2014, ACM T STOR TOS, V11, P1
   Jian Huang, 2014, P VLDB ENDOW, V8
   Kim D, 2013, IEEE T CONSUM ELECTR, V59, P556, DOI 10.1109/TCE.2013.6626238
   Kim Junghoon, 2014, IEEE T CONSUM ELECT, V6
   Kim W.-H., 2014, P 11 USENIX C FIL ST
   Lee T., 2014, P 25 IEEE INT S RAP
   Lee W., 2015, P 2015 USENIX AN TEC
   Liu Ren-Shuo, 2014, P 19 INT C ARCH SUPP
   Lu Youyou, 2015, P 31 INT C MASS STOR
   Moraru Iulian, 2013, P ACM C TIM RES OP S
   Oh G, 2015, PROC VLDB ENDOW, V8, P1454
   Oukid Ismail, 2016, P 2016 ACM SIGMOD IN
   Pelley S, 2014, CONF PROC INT SYMP C, P265, DOI 10.1109/ISCA.2014.6853222
   Volos Haris, 2011, P 16 INT C ARCH SUPP
   Wang Zhaoguo, 2014, P ACM SIGOPS EUR EUR
   Wu X., 2011, P ACM IEEE SC2011 C
   Yang Jun, 2015, P 13 USENIX C FIL ST
NR 25
TC 15
Z9 18
U1 0
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD APR
PY 2018
VL 14
IS 1
SI SI
AR 5
DI 10.1145/3129263
PG 17
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GH5ZN
UT WOS:000433517600005
DA 2024-07-18
ER

PT J
AU Liu, MX
   Zhang, MX
   Chen, K
   Qian, XH
   Wu, YW
   Zheng, WM
   Ren, JL
AF Liu, Mengxing
   Zhang, Mingxing
   Chen, Kang
   Qian, Xuehai
   Wu, Yongwei
   Zheng, Weimin
   Ren, Jinglei
TI DudeTx: Durable Transactions Made Decoupled
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Non-volatile memory; durable transaction; decoupled
ID RANDOM-ACCESS MEMORY; NONVOLATILE; STORAGE
AB Emerging non-volatile memory (NVM) offers non-volatility, byte-addressability, and fast access at the same time. It is suggested that programs should access NVM directly through CPU load and store instructions. To guarantee crash consistency, durable transactions are regarded as a common choice of applications for accessing persistent memory data. However, existing durable transaction systems employ either undo logging, which requires a fence for every memory write, or redo logging, which requires intercepting all memory reads within transactions. Both approaches incur significant overhead.
   This article presents DudeTx, a crash-consistent durable transaction system that avoids the drawbacks of both undo and redo logging. DudeTx uses shadow DRAM to decouple the execution of a durable transaction into three fully asynchronous steps. The advantage is that only minimal fences and no memory read instrumentation are required. This design enables an out-of-the-box concurrency control mechanism, transactional memory or fine-grained locks, to be used as an independent component. The evaluation results show that DudeTx adds durability to a software transactional memory system with only 7.4%-24.6% throughput degradation. Compared to typical existing durable transaction systems, DudeTx provides 1.7x -4.4x higher throughput. Moreover, DudeTx can be implemented with hardware transactional memory or lock-based concurrency control, leading to a further 1.7x and 3.3x speedup, respectively.
C1 [Liu, Mengxing; Zhang, Mingxing; Chen, Kang; Wu, Yongwei; Zheng, Weimin] Tsinghua Univ, Dept Comp Sci & Technol, TNLIST, Beijing 100084, Peoples R China.
   [Zhang, Mingxing] Tsinghua Univ, Grad Sch Shenzhen, Beijing 100084, Peoples R China.
   [Qian, Xuehai] Univ Southern Calif, Los Angeles, CA USA.
   [Ren, Jinglei] Microsoft Res, Redmond, WA USA.
   [Zhang, Mingxing] Sangfor Technol Inc, Shenzhen, Peoples R China.
   [Ren, Jinglei] Microsoft Res Asia, Beijing, Peoples R China.
C3 Tsinghua University; Tsinghua University; Tsinghua Shenzhen
   International Graduate School; University of Southern California;
   Microsoft; Microsoft; Microsoft Research Asia
RP Chen, K; Wu, YW (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, TNLIST, Beijing 100084, Peoples R China.
EM liu-mx15@mails.tsinghua.edu.cn; zhang-mingxing@tsinghua.edu.cn;
   chenkang@tsinghua.edu.cn; xuehai.qian@usc.edu; wuyw@tsinghua.edu.cn;
   zwm-dcs@tsinghua.edu.cn; jinren@microsoft.com
RI WU, ZHEN/GRN-7688-2022; zheng, wei/IQT-9639-2023
OI WU, ZHEN/0000-0001-8719-057X; 
FU National Key Research and Development Program of China [2016YFB1000504];
   Natural Science Foundation of China [61433008, 61373145, 61572280,
   U1435216, 61402198]; National Basic Research (973) Program of China
   [2014CB340402]; NSF [CRII-1657333]; Spanish Government; European ERDF
   [TIN2010-21291-C02-01, CSD2007-00050]
FX This work is supported by National Key Research and Development Program
   of China (2016YFB1000504), Natural Science Foundation of China
   (61433008, 61373145, 61572280, U1435216, 61402198), and National Basic
   Research (973) Program of China (2014CB340402). This work is also
   supported by NSF CRII-1657333, Spanish Government and European ERDF
   under TIN2010-21291-C02-01 and Consolider CSD2007-00050.
CR Akinaga H, 2010, P IEEE, V98, P2237, DOI 10.1109/JPROC.2010.2070830
   [Anonymous], 2014, ACM SIGARCH COMPUTER
   [Anonymous], 2013, LZ4 EXTREMELY FAST C
   [Anonymous], P USENIX ANN TECH C
   [Anonymous], P 46 ANN IEEE ACM IN
   [Anonymous], 2017, P 22 INT C ARCHITECT
   [Anonymous], 2014, P 9 EUR C COMP SYST
   [Anonymous], 2011, PROC INT DEV STRUCT
   [Anonymous], PROC 11 IET INT
   [Anonymous], IN DEPTH ANAL CONCUR
   [Anonymous], TELECOM APPL T PROCE
   [Anonymous], P VLDB ENDOWMENT
   [Anonymous], 2012, ARCH INSTR SET EXT P
   [Anonymous], 2012, PROC 10 USENIX C OPE
   [Anonymous], NVM LIB
   [Anonymous], INT MICR PROD BREAKT
   [Anonymous], P 14 US C FIL STOR T
   [Anonymous], 1995, CHARACTERISTICS WWW
   [Anonymous], TPC C BENCHM V5
   [Anonymous], 2016, Deprecating the PCOMMIT Instruction
   [Anonymous], P 6 INT C ARCH SUPP
   Apalkov D, 2013, ACM J EMERG TECH COM, V9, DOI 10.1145/2463585.2463589
   Arulraj J, 2015, SIGMOD'15: PROCEEDINGS OF THE 2015 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P707, DOI 10.1145/2723372.2749441
   Atikoglu Berk, 2012, Performance Evaluation Review, V40, P53, DOI 10.1145/2318857.2254766
   Atwood G., 2011, FLASH MEM SUMM, P9
   Bernstein Philip A, 1986, CONCURRENCY CONTROL
   Boehm HJ, 2016, ACM SIGPLAN NOTICES, V51, P55, DOI [10.1145/2926697.2926704, 10.1145/3241624.2926704]
   Chakrabarti DR, 2014, ACM SIGPLAN NOTICES, V49, P433, DOI [10.1145/2660193.2660224, 10.1145/2714064.2660224]
   Chen Shimin, 2011, P 5 BIENN C INN DAT, P21, DOI DOI 10.1145/2029956.2029964
   Chi P, 2014, I SYMPOS LOW POWER E, P69, DOI 10.1145/2627369.2627630
   Chidambaram V, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P228, DOI 10.1145/2517349.2522726
   Coburn J, 2011, ACM SIGPLAN NOTICES, V46, P105, DOI [10.1145/1961295.1950380, 10.1145/1961296.1950380]
   Cooper Brian F., 2010, P 1 ACM S CLOUD COMP, P143, DOI [DOI 10.1145/1807128.1807152, 10.1145/1807128.1807152]
   Dragojevic A, 2015, SOSP'15: PROCEEDINGS OF THE TWENTY-FIFTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P54, DOI 10.1145/2815400.2815425
   Dulloor Subramanya R., 2014, P EUR C COMP SYST EU, P1, DOI [10.1145/2592798.2592814, DOI 10.1145/2592798.2592814]
   Eilert S, 2009, 2009 IEEE INTERNATIONAL MEMORY WORKSHOP, P72
   Felber P, 2010, IEEE T PARALL DISTR, V21, P1793, DOI 10.1109/TPDS.2010.49
   Felber P, 2008, PPOPP'08: PROCEEDINGS OF THE 2008 ACM SIGPLAN SYMPOSIUM ON PRINCIPLES AND PRACTICE OF PARALLEL PROGRAMMING, P237, DOI 10.1145/1345206.1345241
   Freitas RF, 2008, IBM J RES DEV, V52, P439, DOI 10.1147/rd.524.0439
   Giles E.R., 2015, 2015 31 S MASS STORA, P1
   HAERDER T, 1983, COMPUT SURV, V15, P287, DOI 10.1145/289.291
   Herlihy M., 1993, INT S COMPUTER ARCHI, DOI DOI 10.1145/165123.165164
   Hu QD, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P703
   Jiaojiao Ren, 2019, International Journal of Control, V92, P755, DOI 10.1080/00207179.2017.1369573
   Johnson R, 2010, PROC VLDB ENDOW, V3, P681
   Kimura H, 2015, SIGMOD'15: PROCEEDINGS OF THE 2015 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P691, DOI 10.1145/2723372.2746480
   Kolli A, 2016, ACM SIGPLAN NOTICES, V51, P399, DOI 10.1145/2954679.2872381
   Kultursay Emre, 2013, IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS 2013), P256
   Lee BC, 2010, IEEE MICRO, V30, P131, DOI 10.1109/MM.2010.24
   Lim H, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P1
   Lu Y., 2015, 2015 IEEE INT SOLID, P1
   Lu YY, 2013, 2013 IEEE 31ST INTERNATIONAL CONFERENCE ON COMPUTER DESIGN (ICCD), P115, DOI 10.1109/ICCD.2013.6657033
   Mittal S, 2016, IEEE T PARALL DISTR, V27, P1537, DOI 10.1109/TPDS.2015.2442980
   Moore K. E., 2006, Twelfth International Symposium on High-Performance Computer Architecture, P254
   Narayanan D., 2012, P 17 INT C ARCH SUPP, V40, P401
   Qureshi MK, 2009, CONF PROC INT SYMP C, P24, DOI 10.1145/1555815.1555760
   Ramadan HE, 2008, INT SYMP MICROARCH, P246, DOI 10.1109/MICRO.2008.4771795
   Raoux S, 2008, IBM J RES DEV, V52, P465, DOI 10.1147/rd.524.0465
   Ren JL, 2015, PROCEEDINGS OF THE 48TH ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE (MICRO-48), P672, DOI 10.1145/2830772.2830802
   Riegel T, 2007, SPAA'07: PROCEEDINGS OF THE NINETEENTH ANNUAL SYMPOSIUM ON PARALLELISM IN ALGORITHMS AND ARCHITECTURES, P221
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   Rumble S.M., 2014, Proceedings of the 12th USENIX Conference on File and Storage Technologies (FAST 14), P1
   Suzuki K., 2015, P 2015 IEEE INT MEMO, P1
   Tu S, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P18, DOI 10.1145/2517349.2522713
   Volos H, 2011, ACM SIGPLAN NOTICES, V46, P91, DOI [10.1145/1961296.1950379, 10.1145/1961295.1950379]
   Wan H., 2016, 2016 5th Non-Volatile Memory Systems and Applications Symposium (NVMSA), P1
   Wang C, 2007, INT SYM CODE GENER, P34
   Wang TZ, 2014, PROC VLDB ENDOW, V7, P865, DOI 10.14778/2732951.2732960
   Wei XD, 2015, SOSP'15: PROCEEDINGS OF THE TWENTY-FIFTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P87, DOI 10.1145/2815400.2815419
   Wu P, 2009, CONCURR COMP-PRACT E, V21, P7, DOI 10.1002/cpe.1336
   Yang Jun, 2015, P 13 USENIX C FIL ST, P167
   Yoon Jung H, 2013, FLASH MEM SUMM
   Youyou Lu, 2014, 2014 IEEE 32nd International Conference on Computer Design (ICCD), P216, DOI 10.1109/ICCD.2014.6974684
NR 73
TC 2
Z9 2
U1 0
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD APR
PY 2018
VL 14
IS 1
SI SI
AR 7
DI 10.1145/3177920
PG 28
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GH5ZN
UT WOS:000433517600007
DA 2024-07-18
ER

PT J
AU Song, NY
   Son, Y
   Han, H
   Yeom, HY
AF Song, Nae Young
   Son, Yongseok
   Han, Hyuck
   Yeom, Heon Young
TI Efficient Memory-Mapped I/O on Fast Storage Device
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Memory-mapped; nonvolatile memory; data-intensive; virtual memory system
AB In modern operating systems, memory-mapped I/O (mmio) is an important access method that maps a file or file-like resource to a region of memory. The mapping allows applications to access data from files through memory semantics (i.e., load/store) and it provides ease of programming. The number of applications that use mmio are increasing because memory semantics can provide better performance than file semantics (i.e., read/write). As more data are located in the main memory, the performance of applications can be enhanced owing to the effect of a large cache. When mmio is used, hot data tend to reside in the main memory and cold data are located in storage devices such as HDD and SSD; data placement in the memory hierarchy depends on the virtual memory subsystem of the operating system. Generally, the performance of storage devices has a direct impact on the performance of mmio. It is widely expected that better storage devices will lead to better performance. However, the expectation is limited when fast storage devices are used since the virtual memory subsystem does not reflect the performance feature of those devices.
   In this article, we examine the Linux virtual memory subsystem and mmio path to determine the influence of fast storage on the existing Linux kernel. Throughout our investigation, we find that the overhead of the Linux virtual memory subsystem, negligible on the HDD, prevents applications from using the full performance of fast storage devices. To reduce the overheads and fully exploit the fast storage devices, we present several optimization techniques. We modify the Linux kernel to implement our optimization techniques and evaluate our prototyped system with low-latency storage devices. Experimental results show that our optimized mmio has up to 7x better performance than the original mmio. We also compare our system to a system that has enough memory to keep all data in the main memory. The system with insufficient memory and our mmio achieves 92% performance of the resource-rich system. This result implies that our virtual memory subsystem for mmap can effectively extend the main memory with fast storage devices.
C1 [Song, Nae Young; Son, Yongseok; Yeom, Heon Young] Seoul Natl Univ, Distributed Comp Syst Lab, Seoul 151742, South Korea.
   [Han, Hyuck] Dongduk Womens Univ, 60 Hwarang Ro 13 Gil, Seoul, South Korea.
C3 Seoul National University (SNU); Dongduk Women's University
RP Han, H (corresponding author), Dongduk Womens Univ, 60 Hwarang Ro 13 Gil, Seoul, South Korea.
EM nai0315@gmail.com; sysganda@gmail.com; hhyuck96@dongduk.ac.kr;
   yeom@snu.ac.kr
FU Next-Generation Information Computing Development Program through
   National Research Foundation of Korea (NRF); Korean government (MSIP)
   [NRF-2015R1A2A2A01005995]; Basic Science Research Program through
   National Research Foundation of Korea (NRF) - Ministry of Education
   [NRF-2014R1A1A2055032]; Ministry of Science, ICT & Future Planning
   [2015M3C4A7065581, 2015M3C4A7065645]
FX This research was supported by the Next-Generation Information Computing
   Development Program through the National Research Foundation of Korea
   (NRF), funded by the Ministry of Science, ICT & Future Planning
   (2015M3C4A7065581, 2015M3C4A7065645) and NRF funded by the Korean
   government (MSIP) (NRF-2015R1A2A2A01005995). Dr. Han's work was partly
   supported by the Basic Science Research Program through the National
   Research Foundation of Korea (NRF) funded by the Ministry of Education
   (NRF-2014R1A1A2055032). A portion of this work appeared in the 2012 SC
   Companion: High Performance Computing, Networking, Storage and Analysis
   (SCC) [Song et al. 2012].
CR Amit N, 2014, ACM SIGPLAN NOTICES, V49, P349, DOI 10.1145/2541940.2541969
   [Anonymous], 2013, EUROSYS 13
   [Anonymous], 2015, EXTREME3804
   Badam Anirudh., 2011, NSDI, P16
   Callaghan M., 2013, P 2013 ACM SIGMOD IN, P1185, DOI DOI 10.1145/2463676.2465296
   Caulfield A.M., 2010, Proc. of the 2010 ACM/IEEE Int. Conf. for High Performance Comput., Network, P1
   Caulfield A. M., 2010, MICRO
   Caulfield AM, 2012, ASPLOS XVII: SEVENTEENTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS, P387
   Caulfield Adrian M., 2011, HOTSTORAGE
   Choi JW, 2014, ACM T STORAGE, V10, DOI 10.1145/2577385
   Coburn J, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P197, DOI 10.1145/2517349.2522724
   Coburn J, 2011, ACM SIGPLAN NOTICES, V46, P105, DOI [10.1145/1961295.1950380, 10.1145/1961296.1950380]
   Condit J, 2009, SOSP'09: PROCEEDINGS OF THE TWENTY-SECOND ACM SIGOPS SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P133
   Cooper Brian F., 2010, P 1 ACM S CLOUD COMP, P143, DOI [DOI 10.1145/1807128.1807152, 10.1145/1807128.1807152]
   Dulloor Subramanya R., 2014, P EUR C COMP SYST EU, P1, DOI [10.1145/2592798.2592814, DOI 10.1145/2592798.2592814]
   Fitzpatrick Brad., 2011, Memcached: a distributed memory object caching system
   Han H, 2011, CLUSTER COMPUT, V14, P325, DOI 10.1007/s10586-011-0164-9
   Katti R. R., 1994, U.S. Patent, Patent No. [5 289 410, 5289410]
   Kim HJ, 2014, ACM T STORAGE, V10, DOI 10.1145/2668128
   Lee BC, 2009, CONF PROC INT SYMP C, P2, DOI 10.1145/1555815.1555758
   Li D, 2012, INT PARALL DISTRIB P, P945, DOI 10.1109/IPDPS.2012.89
   Mogul J.C., 2009, P 12 C HOT TOPICS OP, P14
   Norcott W., 2003, IOZONE FILESYSTEM BE
   Park S.-y., 2006, P 2006 INT C COMP AR, P234, DOI DOI 10.1145/1176760.1176789
   Raoux S, 2008, IBM J RES DEV, V52, P465, DOI 10.1147/rd.524.0465
   Son Y, 2015, CLUSTER COMPUT, V18, P1075, DOI 10.1007/s10586-015-0465-5
   Son Yongseok, 2015, P 8 ACM INT SYST STO
   SONG NY, 2012, SC COMPANION, P766, DOI DOI 10.1109/SC.COMPANION.2012.105
   Vahalia U., 1996, UNIX Internals: The New Frontiers
   Van Essen B, 2012, 2012 SC COMPANION: HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS (SCC), P731, DOI 10.1109/SC.Companion.2012.99
   Van Essen B, 2012, INT PARALL DISTRIB P, P703, DOI 10.1109/IPDPS.2012.69
   Vasudevan Vijay, 2012, P 3 ACM S CLOUD COMP, P8
   Wang C, 2012, INT PARALL DISTRIB P, P957, DOI 10.1109/IPDPS.2012.90
   WU M, 1994, SIGPLAN NOTICES, V29, P86, DOI 10.1145/195470.195506
   Wu XJ, 2013, ACM T STORAGE, V9, DOI 10.1145/2501620.2501621
   Yu YJ, 2014, ACM T COMPUT SYST, V32, DOI 10.1145/2619092
   Yu YoungJin., 2012, Proceedings of the 4th USENIX Conference on Hot Topics in Storage and File Systems. HotStorage'12, P7
NR 37
TC 24
Z9 26
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2016
VL 12
IS 4
AR 19
DI 10.1145/2846100
PG 27
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DV2OQ
UT WOS:000382760500002
DA 2024-07-18
ER

PT J
AU Desnoyers, P
AF Desnoyers, Peter
TI Analytic Models of SSD Write Performance
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Solid-state drives; solid-state storage systems; write amplification;
   flash memory; Design; Performance; Algorithms
ID FLASH TRANSLATION; IDENTIFICATION
AB Solid-state drives (SSDs) update data by writing a new copy, rather than overwriting old data, causing prior copies of the same data to be invalidated. These writes are performed in units of pages, while space is reclaimed in units of multipage erase blocks, necessitating copying of any remaining valid pages in the block before reclamation. The efficiency of this cleaning process greatly affects performance under random workloads; in particular, in SSDs, the write bottleneck is typically internal media throughput, and write amplification due to additional internal copying directly reduces application throughput.
   We present the first nearly-exact closed-form solution for write amplification under greedy cleaning for uniformly-distributed random traffic, validate its accuracy via simulation, and show that its inaccuracies are negligible for reasonable block sizes and overprovisioning ratios. In addition, we also present the first models which predict performance degradation for both LRW (least-recently-written) cleaning and greedy cleaning under simple nonuniform traffic conditions; simulation results show the first model to be exact and the second to be accurate within 2%. We extend the LRW model to arbitrary combinations of random traffic and demonstrate its use in predicting cleaning performance for real-world workloads.
   Using these analytic models, we examine the strategy of separating "hot" and "cold" data, showing that for our traffic model, such separation eliminates any loss in performance due to nonuniform traffic. We then show how a system which segregates hot and cold data into different block pools may shift free space between these pools in order to achieve improved performance, and how numeric methods may be used with ourmodel to find the optimum operating point, which approaches a write amplification of 1.0 for increasingly skewed traffic. We examine online methods for achieving this optimal operating point and show a control strategy based on our model which achieves high performance for a number of real-world block traces.
C1 Northeastern Univ, Coll Comp & Informat Sci, Boston, MA 02115 USA.
C3 Northeastern University
RP Desnoyers, P (corresponding author), Northeastern Univ, Coll Comp & Informat Sci, Boston, MA 02115 USA.
EM pjd@ccs.neu.edu
FU National Science Foundation [CNS-1149232]; Faculty Award from IBM
   Corporation
FX This work has been supported by the National Science Foundation under
   grant CNS-1149232, and by a Faculty Award from IBM Corporation.
CR Agarwal R, 2010, IEEE GLOBE WORK, P1846, DOI 10.1109/GLOCOMW.2010.5700261
   Back S, 2007, LECT NOTES COMPUT SC, V4599, P46, DOI 10.1007/978-3-540-73625-7_7
   Ban A., 2004, Google Patents. US Patent, Patent No. [6,732,221, 6732221]
   Bates K., 2007, OLTP APPL I O UMASS
   Ben-Aroya A, 2006, LECT NOTES COMPUT SC, V4168, P100
   BLACKWELL T, 1995, PROCEEDINGS OF THE 1995 USENIX TECHNICAL CONFERENCE, P277
   Boboila S., 2011, P IEEE S MASS STOR S
   Bux W, 2010, PERFORM EVALUATION, V67, P1172, DOI 10.1016/j.peva.2010.07.003
   Bux Werner, 2009, 3757 IBM RES RZ, P1
   Chang LP, 2007, APPLIED COMPUTING 2007, VOL 1 AND 2, P1126, DOI 10.1145/1244002.1244248
   Chang LP, 2002, EIGHTH IEEE REAL-TIME AND EMBEDDED TECHNOLOGY AND APPLICATIONS SYMPOSIUM, PROCEEDINGS, P187, DOI 10.1109/RTTAS.2002.1137393
   Chiang ML, 1999, J SYST SOFTWARE, V48, P213, DOI 10.1016/S0164-1212(99)00059-X
   Chiang ML, 1997, ISCE '97 - PROCEEDINGS OF 1997 IEEE INTERNATIONAL SYMPOSIUM ON CONSUMER ELECTRONICS, P177, DOI 10.1109/ISCE.1997.658381
   Chung TS, 2009, J SYST ARCHITECT, V55, P332, DOI 10.1016/j.sysarc.2009.03.005
   Desnoyers P., 2012, P 5 ANN INT SYST STO
   Gal E, 2005, ACM COMPUT SURV, V37, P138, DOI 10.1145/1089733.1089735
   Hu X.-Y., 2009, System and Storage Conference (SYSTOR), DOI DOI 10.1145/1534530.1534544
   Hu X.-Y., 2010, 3771 IBM RZ
   Jen-Wei Hsieh, 2006, ACM Transaction on Storage, V2, P22, DOI 10.1145/1138041.1138043
   Jung S, 2010, IEEE T CONSUM ELECTR, V56, P339, DOI 10.1109/TCE.2010.5505937
   Kim HJ, 2002, IEICE T INF SYST, VE85D, P950
   Lee HS, 2009, IEEE T CONSUM ELECTR, V55, P2005, DOI 10.1109/TCE.2009.5373762
   Lee SW, 2007, ACM T EMBED COMPUT S, V6, DOI 10.1145/1275986.1275990
   Lin WH, 2012, DES AUT TEST EUROPE, P117
   Matthews J. N., 1997, Operating Systems Review, V31, P238, DOI 10.1145/269005.266700
   Menon J., 1998, High Performance Computing Systems and Applications, P119
   Narayanan D., 2008, ACM T STORAGE, V4, P3
   Park Dongchul, 2011, P IEEE 27 S MASS STO, P1
   Qi F, 2009, 09022524 ARXIV
   Robinson J. T., 1996, Operating Systems Review, V30, P29, DOI 10.1145/240799.240803
   Rosenblum M., 1991, Operating Systems Review, V25, P1, DOI 10.1145/121133.121137
   Rosenblum M., 1992, THESIS U CALIFORNIA
   Wang J, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE FAST'02 CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P47
   WU M, 1994, SIGPLAN NOTICES, V29, P86, DOI 10.1145/195470.195506
   Xiang Luojie, 2012, 2012 International Conference on Computing, Networking and Communications (ICNC), P497, DOI 10.1109/ICCNC.2012.6167472
NR 35
TC 56
Z9 69
U1 0
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAR
PY 2014
VL 10
IS 2
AR 8
DI 10.1145/2577384
PG 25
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AF2CV
UT WOS:000334521100004
DA 2024-07-18
ER

PT J
AU Zhang, GY
   Zheng, WM
   Li, KQ
AF Zhang, Guangyan
   Zheng, Weimin
   Li, Keqin
TI Design and Evaluation of a New Approach to RAID-0 Scaling
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Algorithms; Design; Experimentation; Management; Performance; Access
   aggregation; data migration; lazy checkpoint; RAID scaling
AB Scaling up a RAID-0 volume with added disks can increase its storage capacity and I/O bandwidth simultaneously. For preserving a round-robin data distribution, existing scaling approaches require all the data to be migrated. Such large data migration results in a long redistribution time as well as a negative impact on application performance. In this article, we present a new approach to RAID-0 scaling called FastScale. First, FastScale minimizes data migration, while maintaining a uniform data distribution. It moves only enough data blocks from old disks to fill an appropriate fraction of new disks. Second, FastScale optimizes data migration with access aggregation and lazy checkpoint. Access aggregation enables data migration to have a larger throughput due to a decrement of disk seeks. Lazy checkpoint minimizes the number of metadata writes without compromising data consistency. Using several real system disk traces, we evaluate the performance of FastScale through comparison with SLAS, one of the most efficient existing scaling approaches. The experiments show that FastScale can reduce redistribution time by up to 86.06% with smaller application I/O latencies. The experiments also illustrate that the performance of RAID-0 scaled using FastScale is almost identical to, or even better than, that of the round-robin RAID-0.
C1 [Zhang, Guangyan; Zheng, Weimin] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
   [Li, Keqin] SUNY Coll New Paltz, Dept Comp Sci, New Paltz, NY 12561 USA.
C3 Tsinghua University; State University of New York (SUNY) System; SUNY
   New Paltz
RP Zhang, GY (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
EM gyzh@tsinghua.edu.cn; zwm-dcs@tsinghua.edu.cn; lik@newpaltz.edu
RI WU, ZHEN/GRN-7688-2022; zheng, wei/IQT-9639-2023
OI WU, ZHEN/0000-0001-8719-057X; 
FU National Natural Science Foundation of China [60903183, 61170008,
   61272055]; National High Technology Research and Development 863 Program
   of China [2013AA01A210]; National Grand Fundamental Research 973 Program
   of China [2014CB340402]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 60903183, 61170008, and 61272055, the National High
   Technology Research and Development 863 Program of China under Grant
   2013AA01A210, and the National Grand Fundamental Research 973 Program of
   China under Grant No. 2014CB340402.
CR Alemany J., 1997, TR970202 U WASH
   [Anonymous], [No title captured]
   [Anonymous], 2003, Computer Architecture
   Brigham Young University, 2010, TPC C POSTGR 20 IT D
   Brinkmann A., 2000, PROC 12 ANN ACM S PA, P119
   Brown N., 2006, ONLINE RAID 5 RESIZI
   Bucy J. S., 2008, CMUPDL08101
   Franklin C.R., 2006, U.S. Patent, Patent No. [10/033,997, 10033997]
   Goel A, 2002, PROC INT CONF DATA, P473, DOI 10.1109/ICDE.2002.994760
   Gonzalez JL, 2007, LECT NOTES COMPUT SC, V4804, P1214
   Hetzler S.R., 2008, U.S. Patent, Patent No. 20080276057
   Hitachi, 2001, HARD DISK DRIV SPEC
   HONICKY R, 2003, P 17 INT PAR DISTR P
   HONICKY RJ, 2004, P 18 INT PAR DISTR P
   Kim CS, 2001, PROCEEDINGS OF THE EIGHTH INTERNATIONAL CONFERENCE ON PARALLEL AND DISTRIBUTED SYSTEMS, P500, DOI 10.1109/ICPADS.2001.934859
   Legg C.B., 1999, U.S. Patent, Patent No. 6000010
   Muller K. G., 2009, SIMPY 2 0 1 DOCUMENT
   Patterson D., 1988, P 1988 ACM SIGMOD C, V17, P109
   Patterson DA, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE SIXTEENTH SYSTEMS ADMINISTRATION CONFERENCE (LISA XVI), P185
   Santos JR, 2000, PERF E R SI, V28, P44, DOI 10.1145/345063.339352
   Seo B., 2005, ACM T STORAGE, V1, P316
   SIVATHANU M, 2004, P 3 USENIX C FIL STO
   UMass Trace Repository, 2007, OLTP APPL I O SEARCH
   Weil S. A., 2006, P INT C SUP SC
   Wilkes J, 1996, ACM T COMPUT SYST, V14, P108, DOI 10.1145/225535.225539
   Wu S., 2009, USENIX C FILE STORAG, V9, P239
   Zhang G., 2007, ACM T STORAGE, V3, P1
   Zheng Weimin, 2011, P 9 USENIX C FIL STO
NR 28
TC 3
Z9 3
U1 0
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2013
VL 9
IS 4
AR 11
DI 10.1145/2491054
PG 31
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 281YI
UT WOS:000329136500001
DA 2024-07-18
ER

PT J
AU Hatzieleftheriou, A
   Anastasiadis, SV
AF Hatzieleftheriou, Andromachi
   Anastasiadis, Stergios V.
TI Improving Bandwidth Efficiency for Consistent Multistream Storage
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Experimentation; Measurement; Performance; Reliability;
   Journaling; logging; concurrency; small writes
ID CACHE
AB Synchronous small writes play a critical role in system availability because they safely log recent state modifications for fast recovery from crashes. Demanding systems typically dedicate separate devices to logging for adequate performance during normal operation and redundancy during state reconstruction. However, storage stacks enforce page-sized granularity in data transfers from memory to disk. Thus, they consume excessive storage bandwidth to handle small writes, which hurts performance. The problem becomes worse, as filesystems often handle multiple concurrent streams, which effectively generate random I/O traffic. In a journaled filesystem, we introduce wasteless journaling as a mount mode that coalesces synchronous concurrent small writes of data into full page-sized journal blocks. Additionally, we propose selective journaling to automatically activate wasteless journaling on data writes with size below a fixed threshold. We implemented a functional prototype of our design over a widely-used filesystem. Our modes are compared against existing methods using microbenchmarks and application-level workloads on stand-alone servers and a multitier networked system. We examine synchronous and asynchronous writes. Coalescing small data updates to the journal sequentially preserves filesystem consistency while it reduces consumed bandwidth up to several factors, decreases recovery time up to 22%, and lowers write latency up to orders of magnitude.
C1 [Hatzieleftheriou, Andromachi; Anastasiadis, Stergios V.] Univ Ioannina, Dept Comp Sci, GR-45110 Ioannina, Greece.
C3 University of Ioannina
RP Anastasiadis, SV (corresponding author), Univ Ioannina, Dept Comp Sci, GR-45110 Ioannina, Greece.
EM stergios@cs.uoi.gr
RI Anastasiadis, Stergios V/L-7310-2014; Anastasiadis,
   Stergios/ABC-7954-2020
OI Anastasiadis, Stergios V/0000-0003-1542-7878; 
FU EU [303090/YD763]; Greek State
FX The present research was supported in part by project INTERSAFE
   303090/YD763 of the INTERREG IIIA program co-funded by the EU and the
   Greek State.
CR Anand Ashok., 2008, OSDI, P161
   [Anonymous], P WORKSH I O VIRT CO
   [Anonymous], MICR EXCH SERV JETS
   [Anonymous], P PET DAT STOR WORKS
   [Anonymous], P USENIX ANN TECHN C
   [Anonymous], P USENIX C FIL STOR
   [Anonymous], 2008, P USENIX ANN TECHN C
   [Anonymous], 2011, C INN DAT SYST RES C
   [Anonymous], LINUXEXPO
   [Anonymous], 2004, P 2 INT C EMB NETW S
   [Anonymous], T PROCESSING CONCEPT
   [Anonymous], TR111 DEC SYST RES C
   [Anonymous], DATABASE ADM COMPLET
   [Anonymous], TPC BENCHM C STAND S
   [Anonymous], P USENIX C FIL STOR
   [Anonymous], 1997, TR3022 NETAPP
   [Anonymous], SEAG CHEET 15K 5 SAS
   [Anonymous], P WORKSH HOT TOP STO
   [Anonymous], 2011, SIGMOD 11 P 2011 INT, DOI [DOI 10.1145/1989323.1989438, 10.1145/1989323.1989438]
   [Anonymous], P USENIX ANN TECHN C
   [Anonymous], AM NAT STAN IN PRESS
   [Anonymous], P ACM EUR C COMP SYS
   [Anonymous], NILFS CONTINUOUS SNA
   [Anonymous], ACM T STORAGE
   [Anonymous], SER ATA HIGH SPEED S
   [Anonymous], P INT C DEP SYST NET
   [Anonymous], 104 SRC DIG EQ CORP
   Batsakis A, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P67
   Bent John., 2009, High Performance Computing Networking, Storage and Analysis, Proceedings of the Conference on, P1
   Bovet D. P., 2005, Understanding the Linux Kernel: from I/O ports to process management
   Brito A, 2009, INT CON DISTR COMP S, P173, DOI 10.1109/ICDCS.2009.35
   Calder B, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P143
   Cao M., 2008, LINUX S, P263
   Carns P., 2009, P IEEE INT PARALLEL, P1
   Chandrasekaran Sirish., 2004, Proceedings of VLDB Conference, V30, P348
   Chang F, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P205
   Chen F, 2009, PERF E R SI, V37, P181
   Chen PM, 1996, ACM SIGPLAN NOTICES, V31, P74, DOI 10.1145/248209.237154
   Chidambaram V., 2012, Proceedings of the 10th USENIX conference on File and Storage Technologies, FAST 2012, San Jose, CA, USA, February 14-17, 2012, P9
   Desnoyers PJ, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 2007 USENIX ANNUAL TECHNICAL CONFERENCE, P45
   DeWitt D. J., 1984, SIGMOD Record, V14, P1, DOI 10.1145/971697.602261
   Elnozahy EN, 2004, IEEE T DEPEND SECURE, V1, P97, DOI 10.1109/TDSC.2004.15
   Grupp L.M., 2012, P 10 USENIX C FILE S, P2
   Hagmann R., 1987, Operating Systems Review, V21, P155, DOI 10.1145/37499.37518
   Hildebrand Dean., 2006, ICS 06 P 20 ANN INT, P116
   HITZ D, 1994, PROCEEDINGS OF THE WINTER 1994 USENIX CONFERENCE, P235
   Hu YM, 2002, IEEE T PARALL DISTR, V13, P290, DOI 10.1109/71.993208
   Huang TC, 2012, SOFTWARE PRACT EXPER, V42, P303, DOI 10.1002/spe.1069
   Itzkovitz A, 1999, USENIX ASSOCIATION PROCEEDINGS OF THE THIRD SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '99), P215
   Kwon Y, 2008, PROC VLDB ENDOW, V1, P574
   Mammarella M, 2009, SOSP'09: PROCEEDINGS OF THE TWENTY-SECOND ACM SIGOPS SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P147
   Mesnier M, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P57
   Min Changwoo, 2012, FAST, V12, P1
   Narayanan D, 2009, EUROSYS'09: PROCEEDINGS OF THE FOURTH EUROSYS CONFERENCE, P145
   Nightingale EB, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P1
   Oral S., 2010, P 8 USENIX C FILE ST, P143
   Ouyang XY, 2011, INT S HIGH PERF COMP, P301, DOI 10.1109/HPCA.2011.5749738
   Prabhakaran V, 2005, USENIX ASSOCIATION PROCEEDINGS OF THE GENERAL TRACK: 2005 UNENIX ANNUAL TECHNICAL CONFERENCE, P105
   Prabhakaran Vijayan, 2005, P OF THE 20 ACM S ON, V39, P206
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   Satyanarayanan M., 1993, Operating Systems Review, V27, P146, DOI 10.1145/173668.168631
   Schindler J, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE FAST'02 CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P259
   Sears R, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P29
   Seltzer M., 1995, TCON'95: Proceedings of the USENIX 1995 Technical Confer- ence Proceedings on USENIX 1995 Technical Conference Proceedings, P21
   Seltzer MI, 2000, USENIX ASSOCIATION PROCEEDINGS OF THE 2000 USENIX ANNUAL TECHNICAL CONFERENCE, P71
   Shin DI, 2011, ACM T STORAGE, V7, DOI 10.1145/1970348.1970349
   Thakur R, 1999, FRONTIERS '99 - THE SEVENTH SYMPOSIUM ON THE FRONTIERS OF MASSIVELY PARALLEL COMPUTATION, PROCEEDINGS, P182, DOI 10.1109/FMPC.1999.750599
   Verissimo Paulo., 2001, DISTRIBUTED SYSTEMS
   Wang RY, 1999, USENIX ASSOCIATION PROCEEDINGS OF THE THIRD SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '99), P29
   Weil SA, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P307
   Woodhouse D., 2001, P LIN S
   Xiangyong Ouyang, 2011, 2011 International Conference on Parallel Processing, P375, DOI 10.1109/ICPP.2011.85
   Zhang ZP, 2007, POLYM COMPOSITE, V28, P175, DOI 10.1002/pc.20281
NR 73
TC 1
Z9 2
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAR
PY 2013
VL 9
IS 1
AR 2
DI 10.1145/2435204.2435206
PG 27
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 114PF
UT WOS:000316753700002
DA 2024-07-18
ER

PT J
AU Tran, N
   Chiang, F
   Li, JY
AF Nguyen Tran
   Chiang, Frank
   Li, Jinyang
TI Efficient Cooperative Backup with Decentralized Trust Management
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Reliability; Cooperative backup; social network; erasure code
AB Existing backup systems are unsatisfactory: commercial backup services are reliable but expensive while peer-to-peer systems are cheap but offer limited assurance of data reliability. This article introduces Friendstore, a system that provides inexpensive and reliable backup by giving users the choice to store backup data only on nodes they trust (typically those owned by friends and colleagues). Because it is built on trusted nodes, Friendstore is not burdened by the complexity required to cope with potentially malicious participants. Friendstore only needs to detect and repair accidental data loss and to ensure balanced storage exchange. The disadvantage of using only trusted nodes is that Friendstore cannot achieve perfect storage utilization. Friendstore is designed for a heterogeneous environment where nodes have very different access link speeds and available disk spaces. To ensure long-term data reliability, a node with limited upload bandwidth refrains from storing more data than its calculated maintainable capacity. A high bandwidth node might be limited by its available disk space. We introduce a simple coding scheme, called XOR(1,2), which doubles a node's ability to store backup information in the same amount of disk space at the cost of doubling the amount of data transferred during restore. Analysis and simulations using long-term node activity traces show that a node can reliably back up tens of gigabytes of data even with low upload bandwidth.
C1 [Nguyen Tran; Chiang, Frank; Li, Jinyang] NYU, New York, NY 10003 USA.
C3 New York University
RP Tran, N (corresponding author), NYU, New York, NY 10003 USA.
EM trandinh@cs.nyu.edu
FU NSF CAREER award [CNS-0747052]; Google Ph.D. fellowship in distributed
   systems
FX This project was partially supported by the NSF CAREER award
   CNS-0747052. N. Tran is also supported by a Google Ph.D. fellowship in
   distributed systems.
CR Adya A., 2002, P 5 S OP SYST DES IM
   AIYER A., 2005, P S OP SYYST PRINC S
   [Anonymous], P 19 ACM S OP SYST P
   [Anonymous], P IEEE S SEC PRIV
   [Anonymous], 1988, P ACM SIGMOD INT C M
   [Anonymous], P 2 S NETW SYST DES
   [Anonymous], P USENIX ANN TECHN C
   BAKER M., 2006, P SIGOPS EUR C COMP
   Batten C, 2002, MITLCSTM632
   Bhagwan R, 2004, P ACM USENIX S NETW
   BLAKE C, 2003, P 9 WORKSH HOT TOP O
   BOLOSKY WJ, 2000, P INT C MEAS MOD COM
   Bowers KD, 2009, CCS'09: PROCEEDINGS OF THE 16TH ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P187
   Bowers KevinD., 2009, Proc. of ACM-CCSW '09, P43, DOI DOI 10.1145/1655008.1655015
   CHANG F, 2002, P 1 USENIX C FIL STO
   Chun B. G., 2006, P 3 S NETW SYST DES
   COHEN B., 2002, P WORKSH EC PEER TO
   COX L, 2002, P 5 S OP SYST DES IM
   COX LP, 2003, P 19 ACM S OP SYST P
   DABEK F., 2006, P 4 INT WORKSH PEER
   DABEK F., 2004, P 1 ACM S NETW SYST
   ELLISON C., 1986, 2693 RFC
   FORD B., 2006, P S OP SYST DES IMPL
   GARRISS S., 2006, P 3 S NETW SYST DES
   GIBSON GA, 1993, J PARALLEL DISTR COM, V17, P4, DOI 10.1006/jpdc.1993.1002
   HOGG T, 2004, P 5 ACM C EL COMM
   HUANG C, 2007, P ACM SIGCOMM DAT CO
   Kamvar M. T., 2003, P 12 INT C WORLD WID, P640
   LI H., 2006, P USENIX OP SYST DES
   LILLIBRIDGE M, 2003, P USENIX ANN TECHN C
   Maniatis P, 2005, ACM T COMPUT SYST, V23, P2, DOI 10.1145/1047915.1047917
   MARTI S, 2004, P 3 INT WORKSH PEER
   MISLOVE A, 2006, P 5 WORKSH HOT TOP N
   Mislove A, 2007, IMC'07: PROCEEDINGS OF THE 2007 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P29
   MUTHITACHAROEN A., 2001, P 18 S OP SYST PRINC
   NGAN T.-W., 2003, P 5 INT WORKSH PEER
   PINHEIRO E, 2007, P 5 US C FIL STOR TE
   PLANK J. S., 2005, P IEEE INT C DEP SYS
   POPESCU BC, 2004, P 12 CAMBR INT WORKS
   RAMABHADRAN S., 2006, P 25 IEEE C COMP COM
   RHEA S, 2003, P USENIX C FIL STOR
   ROWSTRON A, 2001, P 18 ACM S OP SYST P
   Rowstron A., 2001, IFIP ACM INT C DISTR
   Sabater J., 2002, SIGECOM EXCHANGES, V3, P44, DOI DOI 10.1145/844331.844337
   SCHROEDER B, 2007, P 5 US C FIL STOR TE
   SIRER E. G., 2007, P EUR C COMP SYST EU
   STORER M. W., 2007, P USENIX ANN TECHN C, P142
   TATI K, 2006, P 5 INT WORKSH PEER
   Toivonen R, 2006, PHYSICA A, V371, P851, DOI 10.1016/j.physa.2006.03.050
   Tran D. N., 2008, P 1 INT WORKSH SOC N
   YU H., 2005, P ACM SIGCOMM C INT
   ZHANG Z., 2004, P USENIX WORLDS
NR 52
TC 6
Z9 32
U1 0
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD SEP
PY 2012
VL 8
IS 3
AR 8
DI 10.1145/2339118.2339119
PG 25
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 014BR
UT WOS:000309350600001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhang, XC
   Xu, YH
   Jiang, S
AF Zhang, Xuechen
   Xu, Yuehai
   Jiang, Song
TI <i>YouChoose</i>: Choosing your Storage Device as a Performance
   Interface to Consolidated I/O Service
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Performance; I/O QoS; machine learning; performance interface;
   quality of service; model
AB Currently the QoS requirements for storage systems are usually presented in the form of service-level agreement (SLA) to bound I/O measures such as latency and throughput of I/O requests. However, SLA is not an effective performance interface for users to specify their required I/O service quality for two major reasons. First, for users it is difficult to determine appropriate latency and throughput bounds to ensure their required application performance without resource over-provisioning. Second, for storage system administrators it is a challenge to estimate a user's real resource demand because the specified SLA measures are not consistently correlated with the user's resource demand. This makes resource provisioning and scheduling less informative and can greatly reduce system efficiency.
   We propose the concept of reference storage system (RSS), which can be a storage system chosen by users and whose performance can be measured offline and mimicked online, as a performance interface between applications and storage servers. By designating an RSS to represent I/O performance requirement, a user can expect the performance received from a shared storage server servicing his I/O workload is not worse than the performance received from the RSS servicing the same workload. The storage system is responsible for implementing the RSS interface. The key enabling techniques are a machine learning model that derives request-specific performance requirements and an RSS-centric scheduling that efficiently allocates resource among requests from different users. The proposed scheme, named as YouChoose, supports the user-chosen performance interface through efficiently implementing and migrating virtual storage devices in a host storage system. Our evaluation based on trace-driven simulations shows that YouChoose can precisely implement the RSS performance interface, achieve a strong performance assurance and isolation, and improve the efficiency of a consolidated storage system consisting of different types of storage devices.
C1 [Zhang, Xuechen; Xu, Yuehai; Jiang, Song] Wayne State Univ, Dept Elect & Comp Engn, Detroit, MI 48202 USA.
C3 Wayne State University
RP Zhang, XC (corresponding author), Wayne State Univ, Dept Elect & Comp Engn, Detroit, MI 48202 USA.
EM xczhang@wayne.edu
FU US NSF CAREER [CCF-0845711]; Direct For Computer & Info Scie & Enginr;
   Division of Computing and Communication Foundations [0845711] Funding
   Source: National Science Foundation
FX This research is supported by US NSF CAREER award CCF-0845711.
CR ANDERSON E, 2002, P USENIX C FIL STOR
   ANDERSON E., 2001, HPLSSP200104
   [Anonymous], P 9 INT C COMP PERF
   [Anonymous], 2009, STORAGE AREA NETWORK
   Bruno J., 1999, P IEEE INT C MULT CO
   CHAMBLISS D., 2003, P S REL DISTR SYST
   CRUZ R., 1995, IEEE J SEL AREA COMM, V13, P6
   DISKSIM, 2009, DISKSIM SIMULATION E
   FLASH SIMULATOR, 2009, SIM VAR FTL SCHEM
   GIBSON GA, 2000, COMMUNICATI ACM, V43, P11
   GULATI A., 2009, P USENIX C FIL STOR
   GULATI A., 2007, P ACM SIGMETRICS C
   Gulati Ajay, 2010, P 9 USENIX S OP SYST
   Gupta A., 2009, P 14 INT C ARCH SUPP
   HEWLETT-PACKARD COMPANY, 2001, OPENM TECHN REF GUID
   HUANG L., 2004, P ACM SIGMETRICS C
   JIN W., 2007, P ACM SIGMETRICS C
   KELLY T, 2004, HPLSSP2004108
   LU C, 2002, P USENIX C FIL STOR
   LUMB C., 2003, P USENIX ANN TECHN C
   Mesnier M., 2007, P ACM SIGMETRICS C
   PENG G., 2008, P INT C DISTR COMP S
   POPOVICI F., 2003, P USENIX ANN TECHN C
   Recker S., 2003, P 10 INT C TEL
   RUEMMLER C., 1994, COMPUTER, V27, P3
   Sariowan H., 1995, P 4 INT C COMP COMM
   SCHLOSSER S., 2005, USENIX C FIL STOR TE
   STOICA I., 1997, P ACM SIGCOMM C
   Tran N., 2001, ICS 2001 P 15 INT C, P473
   UMASS TRACE REPOSITORY, 2009, OLTP APPL SEARCH ENG
   Uttamchandani S., 2005, P USENIX ANN TECHN C
   WACHS M., 2007, P USENIX C FIL STOR
   Wang M., 2004, P 12 MASCOTS C
   WILKES J., 1996, ACM T COMPUT SYST, V14, P1
   Worthington B., 1994, P ACM SIGMETRICS C
   ZHANG J., 2006, ACM T STORAGE, V2, P3
NR 36
TC 2
Z9 3
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
J9 ACM T STORAGE
JI ACM Trans. Storage
PD OCT
PY 2011
VL 7
IS 3
AR 9
DI 10.1145/2027066.2027069
PG 18
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 990LO
UT WOS:000307632600003
DA 2024-07-18
ER

PT J
AU Iliadis, I
   Haas, R
   Hu, XY
   Eleftheriou, E
AF Iliadis, Ilias
   Haas, Robert
   Hu, Xiao-Yu
   Eleftheriou, Evangelos
TI Disk Scrubbing Versus Intradisk Redundancy for RAID Storage Systems
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Performance; Reliability; Unrecoverable or latent sector errors; RAID;
   reliability analysis; MTTDL; stochastic modeling
AB Two schemes proposed to cope with unrecoverable or latent media errors and enhance the reliability of RAID systems are examined. The first scheme is the established, widely used, disk scrubbing scheme, which operates by periodically accessing disk drives to detect media-related unrecoverable errors. These errors are subsequently corrected by rebuilding the sectors affected. The second scheme is the recently proposed intradisk redundancy scheme, which uses a further level of redundancy inside each disk, in addition to the RAID redundancy across multiple disks. A new model is developed to evaluate the extent to which disk scrubbing reduces the unrecoverable sector errors. The probability of encountering unrecoverable sector errors is derived analytically under very general conditions regarding the characteristics of the read/write process of uniformly distributed random workloads and for a broad spectrum of disk scrubbing schemes, which includes the deterministic and random scrubbing schemes. We show that the deterministic scrubbing scheme is the most efficient one. We also derive closed-form expressions for the percentage of unrecoverable sector errors that the scrubbing scheme detects and corrects, the throughput performance, and the minimum scrubbing period achievable under operation with random, uniformly distributed I/O requests. Our results demonstrate that the reliability improvement due to disk scrubbing depends on the scrubbing frequency and the load of the system, and, for heavy-write workloads, may not reach the reliability level achieved by a simple interleaved parity-check (IPC)-based intradisk redundancy scheme, which is insensitive to the load. In fact, for small unrecoverable sector error probabilities, the IPC-based intradisk redundancy scheme achieves essentially the same reliability as that of a system operating without unrecoverable sector errors. For heavy loads, the reliability achieved by the scrubbing scheme can be orders of magnitude less than that of the intradisk redundancy scheme. Finally, the I/O and throughput performances are evaluated by means of analysis and event-driven simulation.
RP Iliadis, I (corresponding author), IBM Res Zurich, Saumerstr 4, CH-8803 Ruschlikon, Switzerland.
EM ili@zurich.ibm.com; rha@zurich.ibm.com; xhu@zurich.ibm.com;
   ele@zurich.ibm.com
OI Iliadis, Ilias/0000-0002-3860-5828
CR [Anonymous], 2010, P 8 USENIX S FIL STO
   [Anonymous], 2002, PROBABILISTIC STAT R
   [Anonymous], 1988, A case for redundant arrays of inexpensive disks RAID, DOI DOI 10.1145/50202.50214
   [Anonymous], 2006, P 1 ACM SIGOPSEUROSY
   [Anonymous], DISKSIM SIMULATION E
   Bairavasundaram LN, 2007, PERF E R SI, V35, P289
   BLAUM M, 1995, IEEE T COMPUT, V44, P192, DOI 10.1109/12.364531
   CHEN PM, 1994, ACM COMPUT SURV, V26, P145, DOI 10.1145/176979.176981
   Corbett P, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P1
   Dholakia A., 2006, Performance Evaluation Review, V34, P373, DOI 10.1145/1140103.1140326
   Dholakia Ajay, 2008, ACM Transaction on Storage, V4, DOI 10.1145/1353452.1353453
   Elerath JG, 2007, I C DEPEND SYS NETWO, P175, DOI 10.1109/DSN.2007.41
   Gang Wang., 2008, Computer Vision and Pattern Recognition, IEEE Computer Society Conference on, P1
   Greenan K. M., 2010, P HOTSTORAGE, P1
   Hafner J., 2004, 10321 IBM RJ
   *HIT GLOB STOR TEC, 2007, HIT DISK DRIV PROD D
   Iliadis I, 2008, PROCEEDINGS OF THE 2008 IEEE INTERNATIONAL CONFERENCE ON NETWORKING, ARCHITECTURE, AND STORAGE, P10, DOI 10.1109/NAS.2008.20
   Iliadis I, 2009, 2009 IEEE INTERNATIONAL SYMPOSIUM ON MODELING, ANALYSIS & SIMULATION OF COMPUTER AND TELECOMMUNICATION SYSTEMS (MASCOTS), P111
   Iliadis I, 2008, PERF E R SI, V36, P241, DOI 10.1145/1384529.1375485
   Kleinrock L., 1975, Queueing Systems-Volume 1: Theory, V1
   Mi NF, 2008, I C DEPEND SYS NETWO, P492, DOI 10.1109/DSN.2008.4630120
   Oprea Alina, 2010, FAST, P57
   Paris Jehan-Francois., 2006, StorageSS '06, P47
   Pinheiro E, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 5TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES ( FAST '07), P17
   Riska A., 2008, USENIX 2008 ANN TECH, P43
   Riska A, 2006, USENIX ASSOCIATION PROCEEDINGS OF THE 2006 USENIX ANNUAL TECHNICAL CONFERENCE, P97
   RUEMMLER C, 1994, COMPUTER, V27, P17, DOI 10.1109/2.268881
   Sawyer D. C., 1994, NASACR195762
   Schroeder B, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 5TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES ( FAST '07), P1
   Schwarz TJE, 2004, IEEE COMPUTER SOCIETY'S 12TH ANNUAL INTERNATIONAL SYMPOSIUM ON MODELING, ANALYSIS, AND SIMULATION OF COMPUTER AND TELECOMMUNICATIONS SYSTEMS - PROCEEDINGS, P409, DOI 10.1109/MASCOT.2004.1348296
   Shah S, 2005, P REL MAINT S, P226, DOI 10.1109/RAMS.2005.1408366
   Thomasian Alexander, 2009, ACM Transactions on Storage (TOS), V5, P1
   Wolff RW, 1989, Stochastic Modeling and the Theory of Queues
NR 33
TC 16
Z9 17
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUL
PY 2011
VL 7
IS 2
AR 5
DI 10.1145/1970348.1970350
PG 42
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 990LL
UT WOS:000307632300002
DA 2024-07-18
ER

PT J
AU Gatla, OR
   Zhang, D
   Xu, W
   Zheng, M
AF Gatla, Om Rameshwar
   Zhang, Duo
   Xu, Wei
   Zheng, Mai
TI Understanding Persistent-memory-related Issues in the Linux Kernel
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Persistent memory; kernel patches; bug detection; reliability
ID DETECTING DATA RACES
AB Persistent memory (PM) technologies have inspired a wide range of PM-based system optimizations. However, building correct PM-based systems is difficult due to the unique characteristics of PM hardware. To better understand the challenges as well as the opportunities to address them, this article presents a comprehensive study of PM-related issues in the Linux kernel. By analyzing 1,553 PM-related kernel patches in depth and conducting experiments on reproducibility and tool extension, we derive multiple insights in terms of PM patch categories, PM bug patterns, consequences, fix strategies, triggering conditions, and remedy solutions. We hope our results could contribute to the development of robust PM-based storage systems.
C1 [Gatla, Om Rameshwar; Zhang, Duo; Xu, Wei; Zheng, Mai] Iowa State Univ, Dept Elect & Comp Engn, Ames, IA 50011 USA.
C3 Iowa State University
RP Gatla, OR (corresponding author), Iowa State Univ, Dept Elect & Comp Engn, Ames, IA 50011 USA.
EM ogatla@iastate.edu; duozhang@iastate.edu; weixu@iastate.edu;
   mai@iastate.edu
OI Zheng, Mai/0000-0002-0741-3436; Gatla, Om Rameshwar/0000-0001-6442-2526
FU NSF [CNS-1566554, CNS-1855565, CNS-1943204]
FX This work was supported in part by NSF under grants CNS-1566554,
   CNS-1855565, CNS-1943204, and a gift from Western Digital/IDEMA.
CR agigatech.com, AGIGA KOMODO NVDIMM-N
   Albrecht Christoph, 2013, P USENIX ANN TECHN C
   Arafa Y, 2018, IEEE HIGH PERF EXTR
   Bellard F, 2005, USENIX Association Proceedings of the FREENIX/Open Source Track, P41
   Bhardwaj Ankit, 2022, P 14 ACM WORKSH HOT
   Cadar C., 2008, USENIX S OP SYST DES
   Cao JR, 2018, INTERNATIONAL CONFERENCE ON SUPERCOMPUTING (ICS 2018), P1, DOI 10.1145/3205289.3205302
   Cao JR, 2016, PROCEEDINGS OF PDSW-DISCS 2016 - 1ST JOINT INTERNATIONAL WORKSHOP ON PARALLEL DATA STORAGE AND DATA INTENSIVE SCALABLE COMPUTING SYSTEMS, P49, DOI [10.1109/PDSW-DISCS.2016.12, 10.1109/PDSW-DISCS.2016.013]
   Chen Haogang, 2011, P 2 AS PAC WORKSH SY
   Chen QR, 2020, PROCEEDINGS OF THE 28TH ACM JOINT MEETING ON EUROPEAN SOFTWARE ENGINEERING CONFERENCE AND SYMPOSIUM ON THE FOUNDATIONS OF SOFTWARE ENGINEERING (ESEC/FSE '20), P362, DOI 10.1145/3368089.3409727
   Chen YM, 2018, ACM T STORAGE, V14, DOI 10.1145/3204454
   Chipounov V, 2012, ACM T COMPUT SYST, V30, DOI 10.1145/2110356.2110358
   Chou Andy, 2001, P 18 ACM S OP SYST P
   CHUA LO, 1971, IEEE T CIRCUITS SYST, VCT18, P507, DOI 10.1109/TCT.1971.1083337
   cxl, About us
   CXL Consortium, ABOUT US
   Dell EMC, 2021, DCPMM: User's Guide
   Deng DD, 2013, ACM SIGPLAN NOTICES, V48, P785, DOI [10.1145/2544173.2509539, 10.1145/2509136.2509539]
   Dolan-Gavitt Brendan, 2015, P 5 PROGR PROT REV E, P1, DOI DOI 10.1145/2843859.2843867
   Gao Q, 2011, ACM SIGPLAN NOTICES, V46, P239, DOI 10.1145/1961296.1950394
   Garcia De Carellan Eduardo Berrocal, 2018, Discover Persistent Memory Programming Errors with Pmemcheck
   Gatla OR, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P105
   git.ece.iastate, Classification of PM Bug Cases
   github.com, syzkaller-kernel fuzzer
   Gogte V, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P45
   Gorjiara H, 2021, ASPLOS XXVI: TWENTY-SIXTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS, P415, DOI 10.1145/3445814.3446735
   Gunawi H. S., 2014, P ACM S CLOUD COMP S, P1
   Gunawi HS, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P1
   Gunawi HS, 2016, PROCEEDINGS OF THE SEVENTH ACM SYMPOSIUM ON CLOUD COMPUTING (SOCC 2016), P1, DOI 10.1145/2987550.2987583
   Guo Zhenyu, 2013, P 14 WORKSH HOT TOP
   Han RZ, 2022, ACM T STORAGE, V18, DOI 10.1145/3483447
   Han RZ, 2020, PROCEEDINGS OF 2020 IEEE/ACM FIFTH INTERNATIONAL PARALLEL DATA SYSTEMS WORKSHOP (PDSW 2020), P46, DOI 10.1109/PDSW51947.2020.00013
   Han Runzhou, 2022, P 31 INT S HIGH PERF
   Hosomi M, 2005, INT EL DEVICES MEET, P473
   hpe.com, HPE NVDIMM-N Drivers for Microsoft Windows
   hpe.com, HPE NVDIMM-N
   IBM, eXFlash DIMM
   Inc. Linux Kernel Organization, CXL-PMEM Driver
   intel.com, Intel Optane DC Persistent Memory
   intel.com, Persistent Memory FAQ
   Intel Corporation, 2020, Intel Optane Persistent Memory Module: DSM Specification v2.0
   JEDEC, About us
   Jeong DR, 2019, P IEEE S SECUR PRIV, P754, DOI 10.1109/SP.2019.00017
   Kamat Saisha, 2023, P 37 IEEE INT PAR DI
   Kasikci B, 2015, SOSP'15: PROCEEDINGS OF THE TWENTY-FIFTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P344, DOI 10.1145/2815400.2815412
   kernel.org, Submitting Patches: The Essential Guide to Getting Your Code into the Kernel
   kernel.org, dax: Convert to bitmask for Flags
   kernel.org, LIBNVDIMM: Non-Volatile Devices
   Kernel.org Bugzilla, About us
   Kim S, 2019, PROCEEDINGS OF THE TWENTY-SEVENTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '19), P147, DOI 10.1145/3341301.3359662
   Kmemcheck, 2023, ACM Transactions on Storage, V19
   Kuznetsov V, 2012, ACM SIGPLAN NOTICES, V47, P193, DOI 10.1145/2345156.2254088
   Lazar D, 2014, P 5 AS PAC WORKSH SY, DOI [10.1145/2637166.2637237, DOI 10.1145/2637166.2637237]
   Lee SK, 2019, PROCEEDINGS OF THE TWENTY-SEVENTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '19), P462, DOI 10.1145/3341301.3359635
   Lersch L, 2019, PROC VLDB ENDOW, V13, P574, DOI 10.14778/3372716.3372728
   Li Li, 2017, P 8 INT GREEN SUST C
   Li Z, 2006, P 1 WORKSH ARCH SYST, P25, DOI DOI 10.1145/1181309.1181314
   Liang Z, 2020, LECT NOTES COMPUT SC, V12082, P40, DOI 10.1007/978-3-030-48842-0_3
   Liu HP, 2019, PROCEEDINGS OF THE WORKSHOP ON HOT TOPICS IN OPERATING SYSTEMS (HOTOS '19), P155, DOI 10.1145/3317550.3321438
   Liu SH, 2020, TWENTY-FIFTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXV), P1187, DOI 10.1145/3373376.3378452
   Liu SH, 2019, TWENTY-FOURTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXIV), P411, DOI 10.1145/3297858.3304015
   LKL: Linux Kernel Library, About us
   lrita.github.io, Managing Persistent Memory
   Lu Lanyue, 2013, P 11 USENIX C FIL ST
   Lu S., 2005, WORKSH EV SOFTW DEF, V5
   Lu S, 2008, ACM SIGPLAN NOTICES, V43, P329, DOI 10.1145/1353536.1346323
   Lu S, 2012, IEEE T PARALL DISTR, V23, P1060, DOI 10.1109/TPDS.2011.254
   lwn.net, DAX: Page Cache Bypass for Filesystems on Memory Storage
   Machiry A, 2017, PROCEEDINGS OF THE 26TH USENIX SECURITY SYMPOSIUM (USENIX SECURITY '17), P1007
   Mahmud Tabassum, 2023, P 21 USENIX C FIL ST
   Mahmud Tabassum, 2022, P 14 ACM WORKSH HOT, P1
   Martinez Ashlie, 2017, P 9 USENIX C HOT TOP
   Min C, 2015, SOSP'15: PROCEEDINGS OF THE TWENTY-FIFTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P361, DOI 10.1145/2815400.2815422
   Mohan Jayashree, 2018, P 13 USENIX S OP SYS
   ndctl, About us
   Neal I, 2020, PROCEEDINGS OF THE 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '20), P1047
   O'Leary Kevin P., 2018, How to Detect Persistent Memory Programming Errors Using Intel Inspector-Persistence Inspector
   Persistent Memory Development Kit (PMDK), About us
   pmem.io, Update on PMDK and Our Long Term Support Strategy
   Quinn A, 2019, PROCEEDINGS OF THE WORKSHOP ON HOT TOPICS IN OPERATING SYSTEMS (HOTOS '19), P163, DOI 10.1145/3317550.3321428
   Rudoff Andy, 2021, PERSISTENT MEMORY 14
   snia.org, NVM Programming Guide
   snia.org, ULLtraDIMM SSD Overview
   software.intel.com, PIN-A dynamic binary instrumentation tool
   Storage Networking Industry Association, About us
   Sun XD, 2020, PROCEEDINGS OF THE 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '20), P735
   The Kernel Address Sanitizer (KASAN), About us
   The LLVM Compiler Infrastructure, About us
   The Undefined Behavior Sanitizer (UBSAN), About us
   Tucek Joseph, 2013, P 11 USENIX C FIL ST
   uefi.org, Advanced Configuration and Power Interface Specification
   Von Behren Paul, 2015, P 13 USENIX C FIL ST
   Wen C, 2022, PROC INT CONF SOFTW, P474, DOI 10.1145/3510003.3510178
   Wong HSP, 2010, P IEEE, V98, P2201, DOI 10.1109/JPROC.2010.2070050
   Xu E, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P961
   Xu Erci, 2018, P 3 IEEE ACM INT WOR
   Xu J, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P323
   Xu M, 2020, P IEEE S SECUR PRIV, P1643, DOI 10.1109/SP40000.2020.00078
   Xu TY, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P244, DOI 10.1145/2517349.2522727
   Xu W, 2019, P IEEE S SECUR PRIV, P818, DOI 10.1109/SP.2019.00035
   Yang Jun, 2015, P 13 USENIX C FIL ST, P167
   Yang JF, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P131
   Zhang Di, 2021, HotStorage '21: Proceedings of the 13th ACM Workshop on Hot Topics in Storage and File Systems, P86, DOI 10.1145/3465332.3470873
   Zhang Di, 2023, P 37 IEEE INT PAR DI
   Zhang Duo, 2021, P 14 ACM INT C SYST, P1
   Zhang Duo, 2021, Bench- Council Transactions on Benchmarks, Standards and Evaluations, V1, P1
   Zhang Duo, 2022, P IEEE INT C NETW AR
   Zheng M, 2017, ACM T COMPUT SYST, V34, DOI 10.1145/2992782
   Zheng M, 2014, IEEE T PARALL DISTR, V25, P104, DOI 10.1109/TPDS.2013.44
   Zheng M, 2011, ACM SIGPLAN NOTICES, V46, P135, DOI 10.1145/2038037.1941574
   Zheng Mai, 2014, P 11 USENIX S OP SYS
   Zhou J, 2023, IEEE T COMPUT, V72, P1747, DOI 10.1109/TC.2022.3223302
NR 112
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2023
VL 19
IS 4
AR 36
DI 10.1145/3605946
PG 28
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA X6NQ6
UT WOS:001099601100008
OA Bronze, Green Submitted
DA 2024-07-18
ER

PT J
AU Iliadis, I
AF Iliadis, Ilias
TI Reliability Evaluation of Erasure-coded Storage Systems with Latent
   Errors
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Storage; unrecoverable or latent sector errors; reliability analysis;
   MTTDL; EAFDL; RAID; MDS codes; stochastic modeling
ID HIGH-PERFORMANCE
AB Large-scale storage systems employ erasure-coding redundancy schemes to protect against device failures. The adverse effect of latent sector errors on the Mean Time to Data Loss (MTTDL) and the Expected Annual Fraction of Data Loss (EAFDL) reliability metrics is evaluated. A theoretical model capturing the effect of latent errors and device failures is developed, and closed-form expressions for the metrics of interest are derived. The MTTDL and EAFDL of erasure-coded systems are obtained analytically for (i) the entire range of bit error rates; (ii) the symmetric, clustered, and declustered data placement schemes; and (iii) arbitrary device failure and rebuild time distributions under network rebuild bandwidth constraints. The range of error rates that deteriorate system reliability is derived analytically. For realistic values of sector error rates, the results obtained demonstrate that MTTDL degrades, whereas, for moderate erasure codes, EAFDL remains practically unaffected. It is demonstrated that, in the range of typical sector error rates and for very powerful erasure codes, EAFDL degrades as well. It is also shown that the declustered data placement scheme offers superior reliability.
C1 [Iliadis, Ilias] IBM Res Europe Zurich, Saumerstr 4, CH-8803 Ruschlikon, Switzerland.
RP Iliadis, I (corresponding author), IBM Res Europe Zurich, Saumerstr 4, CH-8803 Ruschlikon, Switzerland.
EM ili@zurich.ibm.com
OI Iliadis, Ilias/0000-0002-3860-5828
CR AmazonWeb Services, 2021, AM SIMPL STOR SERV A
   [Anonymous], 2022, BACKBLAZE DRIVE STAT
   [Anonymous], 2022, SEAGATE EXOS X20 DAT
   [Anonymous], 2012, USENIX ASS NEWSLETT
   [Anonymous], 2019, TAPE ROADMAP INFORM
   [Anonymous], 2013, PROC 5 USENIX WORKSH
   [Anonymous], 2011, SIGMOD 11 P 2011 INT, DOI [DOI 10.1145/1989323.1989438, 10.1145/1989323.1989438]
   Borthakur Dhruba, 2009, HDFS ERASURE CODES H
   Calder B, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P143
   Chamazcoti SA, 2019, IEEE T EMERG TOP COM, V7, P435, DOI 10.1109/TETC.2017.2693424
   CHEN PM, 1994, ACM COMPUT SURV, V26, P145, DOI 10.1145/176979.176981
   Cidon Asaf, 2013, P USENIX ATC
   DELL/EMC Whitepaper, 2019, POWERVAULT ME4 SER A
   Dholakia Ajay, 2008, ACM Transaction on Storage, V4, DOI 10.1145/1353452.1353453
   Elerath JG, 2014, ACM T STORAGE, V10, DOI 10.1145/2577386
   Ford D., 2010, P 9 USENIX C OP SYST, P61
   Ghemawat S., 2003, Operating Systems Review, V37, P29, DOI 10.1145/1165389.945450
   Greenan K. M., 2010, P HOTSTORAGE, P1
   Huang Cheng, 2012, 2012 USENIX ANN TECH, P15
   Iliadis Ilias, 2014, 2014 22nd Annual IEEE International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems (MASCOTS). Proceedings, P375, DOI 10.1109/MASCOTS.2014.53
   Iliadis I, 2008, PROCEEDINGS OF THE 2008 IEEE INTERNATIONAL CONFERENCE ON NETWORKING, ARCHITECTURE, AND STORAGE, P10, DOI 10.1109/NAS.2008.20
   Iliadis I, 2015, ACM T STORAGE, V11, DOI 10.1145/2700311
   Iliadis I, 2014, IEEE PAC RIM INT SYM, P169, DOI 10.1109/PRDC.2014.30
   Iliadis I, 2011, ACM T STORAGE, V7, DOI 10.1145/1970348.1970350
   Iliadis I, 2009, 2009 IEEE INTERNATIONAL SYMPOSIUM ON MODELING, ANALYSIS & SIMULATION OF COMPUTER AND TELECOMMUNICATION SYSTEMS (MASCOTS), P111
   Iliadis Ilias, 2022, P 15 INT C COMMUNICA, P1
   Iliadis Ilias, 2021, P 14 INT C COMMUNICA, P15
   Iliadis Ilias, 2018, INTJ ADV NETW SERV, V11, P113
   Iliadis Ilias, 2019, CURR CONTENTS, V12, P259
   Iliadis Ilias, 2019, P 12 INT C COMMUNICA, P1
   Iliadis Ilias, 2015, INT J ADV SYST MEAS, V8, P178
   Iliadis Ilias, 2017, CURR CONTENTS, V10, P118
   Kishani M, 2020, IEEE T COMPUT, V69, P649, DOI 10.1109/TC.2019.2962691
   Li B, 2018, ACM TRANS MODELING P, V3, DOI 10.1145/3159172
   Li Y, 2019, ACM T STORAGE, V15, DOI 10.1145/3319405
   Luby M, 2019, ACM T STORAGE, V15, DOI 10.1145/3281276
   MALHOTRA M, 1993, J PARALLEL DISTR COM, V17, P146, DOI 10.1006/jpdc.1993.1013
   Meza Justin, 2015, ACM SIGMETRICS Performance Evaluation Review, V43, P177, DOI 10.1145/2745844.2745848
   Muralidhar S., 2014, P 11 USENIX C OP SYS, P383
   Oprea Alina, 2010, FAST, P57
   Ovsiannikov M, 2013, PROC VLDB ENDOW, V6, P1092, DOI 10.14778/2536222.2536234
   Pâris JF, 2012, IEEE IPCCC, P324, DOI 10.1109/PCCC.2012.6407774
   Patterson D. A., 1988, SIGMOD Record, V17, P109, DOI 10.1145/971701.50214
   Pinheiro E, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 5TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES ( FAST '07), P17
   Plank James S., 2013, P 11 USENIX C FILE S
   Rashmi KV, 2014, SIGCOMM'14: PROCEEDINGS OF THE 2014 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P331, DOI [10.1145/2619239.2626325, 10.1145/2740070.2626325]
   Rodrigues R, 2005, LECT NOTES COMPUT SC, V3640, P226, DOI 10.1007/11558989_21
   Schroeder B, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P67
   Schroeder B, 2010, ACM T STORAGE, V6, DOI 10.1145/1837915.1837917
   Schroeder B, 2010, IEEE T DEPEND SECURE, V7, P337, DOI 10.1109/TDSC.2009.4
   Schwarz TJE, 2004, IEEE COMPUTER SOCIETY'S 12TH ANNUAL INTERNATIONAL SYMPOSIUM ON MODELING, ANALYSIS, AND SIMULATION OF COMPUTER AND TELECOMMUNICATIONS SYSTEMS - PROCEEDINGS, P409, DOI 10.1109/MASCOT.2004.1348296
   Shvachko K, 2010, IEEE S MASS STOR SYS
   Silberstein Mark., 2014, Proceedings of International Conference on Systems and Storage, P1
   Thomasian Alexander, 2009, ACM Transaction on Storage, V5, DOI 10.1145/1629075.1629076
   Venkatesan Vinodh, 2013, Quantitative Evaluation of Systems. 10th International Conference, QEST 2013. Proceedings: LNCS 8054, P241, DOI 10.1007/978-3-642-40196-1_20
   Venkatesan V., 2012, 2012 Ninth International Conference on Quantitative Evaluation of Systems (QEST 2012), P209, DOI 10.1109/QEST.2012.32
   Venkatesan V., 2012, 2012 IEEE 20th International Symposium on Modelling, Analysis & Simulation of Computer and Telecommunication Systems (MASCOTS), P189, DOI 10.1109/MASCOTS.2012.31
   Venkatesan V., 2011, Proceedings of the 2011 IEEE 19th International Symposium on Modelling, Analysis & Simulation of Computer and Telecommunication Systems (MASCOTS 2011), P307, DOI 10.1109/MASCOTS.2011.53
   Venkatesan V, 2013, I S MOD ANAL SIM COM, P293, DOI 10.1109/MASCOTS.2013.38
   Weatherspoon H, 2002, LECT NOTES COMPUT SC, V2429, P328
   Zhang M, 2019, IEEE T PARALL DISTR, V30, P2836, DOI 10.1109/TPDS.2019.2921551
NR 61
TC 1
Z9 1
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD FEB
PY 2023
VL 19
IS 1
AR 4
DI 10.1145/3568313
PG 47
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 9I5DE
UT WOS:000939530200004
DA 2024-07-18
ER

PT J
AU Wang, SC
   Lu, ZY
   Cao, Q
   Jiang, H
   Yao, J
   Dong, YY
   Yang, PY
   Xie, CS
AF Wang, Shucheng
   Lu, Ziyi
   Cao, Qiang
   Jiang, Hong
   Yao, Jie
   Dong, Yuanyuan
   Yang, Puyuan
   Xie, Changsheng
TI Exploration and Exploitation for Buffer-Controlled HDD-Writes for
   SSD-HDD Hybrid Storage Server
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Hybrid storage; IO scheduling; tail latency
ID FLASH
AB Hybrid storage servers combining solid-state drives (SSDs) and hard-drive disks (HDDs) provide cost-effectiveness and mu s-level responsiveness for applications. However, observations from cloud storage system Pangu manifest that HDDs are often underutilized while SSDs are overused, especially under intensive writes. It leads to fast wear-out and high tail latency to SSDs. On the other hand, our experimental study reveals that a series of sequential and continuous writes to HDDs exhibit a periodic, staircase-shaped pattern of write latency, i.e., low (e.g., 35 mu s), middle (e.g., 55 mu s), and high latency (e.g., 12 ms), resulting from buffered writes within HDD's controller. It inspires us to explore and exploit the potential mu s-level IO delay of HDDs to absorb excessive SSD writes without performance degradation.
   We first build an HDD writing model for describing the staircase behavior and design a profiling process to initialize and dynamically recalibrate the model parameters. Then, we propose a Buffer-Controlled Write approach (BCW) to proactively control buffered writes so that low- and mid-latency periods are scheduled with application data and high-latency periods are filled with padded data. Leveraging BCW, we design a mixed IO scheduler (MIOS) to adaptively steer incoming data to SSDs and HDDs. A multi-HDD scheduling is further designed to minimize HDD-write latency. We perform extensive evaluations under production workloads and benchmarks. The results show that MIOS removes up to 93% amount of data written to SSDs, reduces average and 99th-percentile latencies of the hybrid server by 65% and 85%, respectively.
C1 [Wang, Shucheng; Lu, Ziyi] HUST, Wuhan Natl Lab Optoelect, 1037 Luoyu Rd, Wuhan 430074, Hubei, Peoples R China.
   [Cao, Qiang; Xie, Changsheng] HUST, Minist Educ, Key Lab Informat Storage Syst, 1037 Luoyu Rd, Wuhan 430074, Hubei, Peoples R China.
   [Jiang, Hong] Univ Texas Arlington, Dept Comp Sci & Engn, 701 S Nedderman Dr, Arlington, TX 76019 USA.
   [Yao, Jie] HUST, Sch Comp Sci & Technol, 1037 Luoyu Rd, Wuhan 430074, Hubei, Peoples R China.
   [Dong, Yuanyuan; Yang, Puyuan] Alibaba Grp, Hangzhou 311121, Zhejiang, Peoples R China.
C3 Huazhong University of Science & Technology; Huazhong University of
   Science & Technology; University of Texas System; University of Texas
   Arlington; Huazhong University of Science & Technology; Alibaba Group
RP Cao, Q (corresponding author), HUST, Minist Educ, Key Lab Informat Storage Syst, 1037 Luoyu Rd, Wuhan 430074, Hubei, Peoples R China.
EM wsczq@hust.edu.cn; luziyi@hust.edu.cn; caoqiang@hust.edu.cn;
   hong.jiang@uta.edu; jackyao@hust.edu.cn; yuanyuan.dyy@alibaba-inc.com;
   puyuan.ypy@alibaba-inc.com; cs_xie@hust.edu.cn
OI Wang, Shucheng/0000-0002-8275-6500
FU National Key Research and Development Program of China [2018YFA0701800];
   NSFC [61821003, 61872156]; US NSF [CCF-1704504, CCF-1629625]; Alibaba
   Group through Alibaba Innovative Research (AIR) Program
FX This work is supported in part by National Key Research and Development
   Program of China (No. 2018YFA0701800), NSFC (No. 61821003 and 61872156),
   the US NSF under grant numbers CCF-1704504 and CCF-1629625, and Alibaba
   Group through Alibaba Innovative Research (AIR) Program.
CR Ahmadian S, 2019, DES AUT TEST EUROPE, P1196, DOI [10.23919/DATE.2019.8714805, 10.23919/date.2019.8714805]
   Alibaba Clouder, 2018, PANG HIGH PERF DISTR
   Andersen DG, 2010, IEEE MICRO, V30, P52, DOI 10.1109/MM.2010.71
   Athanassoulis Manos., 2011, SIGMOD C, P865
   Aupy G, 2018, INT PARALL DISTRIB P, P660, DOI 10.1109/IPDPS.2018.00075
   AXBOE, FIO FLEXIBLE IO TEST
   Balmau O, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P753
   Boboila S., 2010, P 8 USENIX C FIL STO, P9
   Bovet D. P., 2005, Understanding the Linux Kernel: from I/O ports to process management
   DeCandia Giuseppe, 2007, Operating Systems Review, V41, P205, DOI 10.1145/1323293.1294281
   Deng F, 2020, PROC INT CONF PARAL, DOI 10.1145/3404397.3404437
   Didona D, 2019, PROCEEDINGS OF THE 16TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P79
   Elyasi N, 2017, TWENTY-SECOND INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXII), P375, DOI 10.1145/3037697.3037728
   FUJITSU, 2007, MBC2073RC MBC2036RC, P60
   Gao CM, 2014, IEEE S MASS STOR SYS
   Ghemawat S., 2003, ACM SIGOPS OPERATING, V37, P29, DOI [DOI 10.1145/945445.945450, DOI 10.1145/1165389.945450, 10.1145/1165389.945450]
   Gupta A, 2009, ACM SIGPLAN NOTICES, V44, P229, DOI 10.1145/1508284.1508271
   Huang Q, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P167, DOI 10.1145/2517349.2522722
   Intel Corporation, 2016, ENT CLASS VERS DESKT
   Intel Corporation,, 2018, PROD BRIEF INT OPT S
   Intel Corporation, 2019, PROD BRIEF INT 660P
   Jiang CF, 2019, IEEE ACCESS, V7, P22495, DOI 10.1109/ACCESS.2019.2897898
   Jung M, 2014, CONF PROC INT SYMP C, P289, DOI 10.1109/ISCA.2014.6853216
   Kang Jeong-Uk, 2014, 6 USENIX WORKSH HOT
   Kgil Taeho., 2006, P 2006 INT C COMPILE, P103, DOI DOI 10.1145/1176760.1176774
   Kim J, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P799
   Kim JY, 2009, MATER SCI FORUM, V620-622, P295, DOI 10.4028/www.scientific.net/MSF.620-622.295
   Kim S, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P813
   Kougkas A, 2018, HPDC '18: PROCEEDINGS OF THE 27TH INTERNATIONAL SYMPOSIUM ON HIGH-PERFORMANCE PARALLEL AND DISTRIBUTED COMPUTING, P219, DOI 10.1145/3208040.3208059
   Li C., 2014, USENIX ANN TECH C AT, P501
   Li HB, 2019, PROCEEDINGS OF THE FOURTEENTH EUROSYS CONFERENCE 2019 (EUROSYS '19), DOI 10.1145/3302424.3303967
   Liu QX, 2018, PROCEEDINGS OF THE 2018 ACM SYMPOSIUM ON CLOUD COMPUTING (SOCC '18), P347, DOI 10.1145/3267809.3267830
   Liu SY, 2019, PROCEEDINGS OF THE 2019 TENTH ACM SYMPOSIUM ON CLOUD COMPUTING (SOCC '19), P403, DOI 10.1145/3357223.3362705
   Mao B, 2012, ACM T STORAGE, V8, DOI 10.1145/2093139.2093143
   Mathur Avantika, 2007, P LINUX S, V2, P21
   Min Changwoo, 2012, FAST, V12, P1
   Murugan M, 2011, IEEE S MASS STOR SYS
   Ou JX, 2014, INT PARALL DISTRIB P, DOI 10.1109/IPDPS.2014.86
   Palankar M R, 2008, P 2008 INT WORKSH DA, P55, DOI DOI 10.1145/1383519.1383526
   Park Stan., 2012, FAST, P13
   Puyuan Yang, 2013, Web-Age Information Management. WAIM 2013 International Workshops: HardBD, MDSP, BigEM, TMSN, LQPM, BDMS. Proceedings. LNCS 7901, P28, DOI 10.1007/978-3-642-39527-7_5
   Ramakrishnan R, 2017, SIGMOD'17: PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P51, DOI 10.1145/3035918.3056100
   Samsung Electronics, 2017, SAMS SSD 960 EVO M 2
   Seagate Technology, 2016, ENH CACH ADV TURBOBO
   Seagate Technology, 2018, BARR PROC SATA HDD D
   Seagate Technology, 2019, BARR COMP SATA PROD
   Soundararajan Gokul., 2010, Proceedings of the 8th USENIX conference on File and storage technologies, FAST'10, P8
   Tavakkol A, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P49
   Tavakkol A, 2018, CONF PROC INT SYMP C, P397, DOI 10.1109/ISCA.2018.00041
   Wang H., 2014, P 12 USENIX C FIL ST, P229
   Wang H, 2013, PROCEEDINGS OF THE 5TH (2013) INTERNATIONAL CONFERENCE ON FINANCIAL RISK AND CORPORATE FINANCE MANAGEMENT, VOLS I AND II, P1, DOI 10.1109/ijcnn.2013.6706812
   Western Digital Corporation, 2019, WD RED NAS HARD DRIV
   Western Digital Corporation,, 2019, PROD BRIEF WD GOLD E
   Wu SZ, 2018, INT PARALL DISTRIB P, P296, DOI 10.1109/IPDPS.2018.00039
   Xu E, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P961
   Yan SQ, 2017, ACM T STORAGE, V13, DOI 10.1145/3121133
   Yang Pan, 2019, P 11 USENIX WORKSH H
   Yeong-JaeWoo Jin-Soo., 2013, P 11 ACM INT C EMB S, P6
   Yu YJ, 2010, ACM T STORAGE, V6, DOI 10.1145/1714454.1714456
   Zheng SA, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P207
   Zhou K, 2018, INTERNATIONAL CONFERENCE ON SUPERCOMPUTING (ICS 2018), P284, DOI 10.1145/3205289.3205299
NR 61
TC 4
Z9 4
U1 0
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD FEB
PY 2022
VL 18
IS 1
AR 6
DI 10.1145/3465410
PG 29
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZN5OY
UT WOS:000765084700003
DA 2024-07-18
ER

PT J
AU Nachman, A
   Sheinvald, S
   Kolikant, A
   Yadgar, G
AF Nachman, Aviv
   Sheinvald, Sarai
   Kolikant, Ariel
   Yadgar, Gala
TI GoSeed: Optimal Seeding Plan for Deduplicated Storage
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article; Proceedings Paper
CT 14th USENIX Symposium on Operating Systems Design and Implementation
   (OSDI)
CY NOV 04-06, 2020
CL ELECTR NETWORK
SP USENIX Assoc, Alibaba Grp, Amazon, ANT Grp, Alipay, Apple, ByteDance, Facebook, Google, Microsoft, VMware, Oracle, Two Sigma, ACM Queue, DMTF, FreeBSD Fdn
DE Deduplication; data migration; capacity planning
ID EFFICIENT
AB Deduplication decreases the physical occupancy of files in a storage volume by removing duplicate copies of data chunks, but creates data-sharing dependencies that complicate standard storage management tasks. Specifically, data migration plans must consider the dependencies between files that are remapped to new volumes and files that are not. Thus far, only greedy approaches have been suggested for constructing such plans, and it is unclear how they compare to one another and how much they can be improved.
   We set to bridge this gap for seeding-migration in which the target volume is initially empty. We prove that even this basic instance of data migration is NP-hard in the presence of deduplication. We then present GoSeed, a formulation of seeding as an integer linear programming (ILP) problem, and three acceleration methods for applying it to real-sized storage volumes. Our experimental evaluation shows that, while the greedy approaches perform well on "easy" problem instances, the cost of their solution can be significantly higher than that of GoSeed's solution on "hard" instances, for which they are sometimes unable to find a solution at all.
C1 [Nachman, Aviv; Kolikant, Ariel; Yadgar, Gala] Technion, Comp Sci Dept, IL-3200003 Haifa, Israel.
   [Sheinvald, Sarai] ORT Braude Coll Engn, IL-2161002 Carmiel, Israel.
C3 Technion Israel Institute of Technology; Braude Academic College of
   Engineering
RP Nachman, A (corresponding author), Technion, Comp Sci Dept, IL-3200003 Haifa, Israel.
EM aviv.nachman@cs.technion.ac.il; sarai@braude.ac.il;
   sarielko@cs.technion.ac.il; gala@cs.technion.ac.il
OI Yadgar, Gala/0000-0003-2701-0260
FU Israel Science Foundation [807/20]
FX This research was supported by the Israel Science Foundation (grant No.
   807/20).
CR ABARA J, 1989, INTERFACES, V19, P20, DOI 10.1287/inte.19.4.20
   Aggarwal Bhavish, 2010, 7 USENIX C NETW SYST
   Allu Yamini, 2018, USENIX ANN TECHN C U
   Anderson E, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE FAST'02 CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P175
   Anderson Eric, 2001, 5 INT WORKSH ALG ENG
   [Anonymous], INTRO LP SOLVE 5525
   [Anonymous], SNIA IOTTA REPOSITOR
   [Anonymous], CPLEX OPT
   [Anonymous], GLPK GNU LINEAR PROG
   [Anonymous], FASTEST MATH PROGRAM
   [Anonymous], TRACES SNAPSHOTS PUB
   Bessani A, 2013, ACM T STORAGE, V9, DOI 10.1145/2535929
   Bhagwat D, 2009, 2009 IEEE INTERNATIONAL SYMPOSIUM ON MODELING, ANALYSIS & SIMULATION OF COMPUTER AND TELECOMMUNICATION SYSTEMS (MASCOTS), P237
   Cao ZC, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P309
   Chen Feng, 2011, 9 USENIX C FIL STROA
   Chen LC, 2014, ACM SIGPLAN NOTICES, V49, P65, DOI 10.1145/2674025.2576204
   Clements Austin T., 2009, C USENIX ANN TECHN C
   Dantzig G. B., 2003, Linear Programming 2: Theory and Extensions
   Debnath Biplob, 2010, USENIX C USENIX ANN
   Dong Wei, 2011, 9 USENIX C FIL STOR
   Douglis F, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P29
   Douglis Fred, 2011, 25 INT C LARG INST S
   Dubnicki Cezary, 2009, 7 C FIL STOR TECHN F
   Duggal Abhinav, 2019, 2019 USENIX ANN TECH
   EMC Corporation, 2015, Introduction To The EMC XtremIO Storage Array (Ver. 4.0)
   Feng JX, 2013, IEEE S MASS STOR SYS
   Fu M., 2014, USENIX ANN TECHN C U
   Fu Min, 2015, 13 USENIX C FIL STOR
   Guo Fanglu, 2011, USENIX C USENIX ANN
   Gupta Aayush, 2011, 9 USENIX C FIL STROA
   Gupta Diwaker, 2008, 8 USENIX C OP SYST D
   Harnik D, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P277
   Harnik D, 2010, IEEE SECUR PRIV, V8, P40, DOI 10.1109/MSP.2010.187
   Harnik Danny, 2019, 17 USENIX C FIL STOR
   Kaczmarczyk Michal, 2012, 5 INT SYST STOR C SY
   Kallahalla Mahesh, 2002, ERGASTULUM QUICKLY F
   Karp R, 1972, COMPLEXITY COMPUTER, V40, P85, DOI 10.1007/978-3-540-68279-08
   Ladanyi Laszlo, SYMPHONY DEV HOME PA
   Li Cheng, 2014, 2014 USENIX Annual Technical Conference (USENIX ATC 14), P281
   Li J, 2014, IEEE T PARALL DISTR, V25, P1615, DOI 10.1109/TPDS.2013.284
   Lillibridge M., 2009, PROC USENIX FAST
   Lillibridge M., 2013, FAST
   Lin Xing, 2014, 12 USENIX C FIL STOR
   Lu CY, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE FAST'02 CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P219
   MANBER U, 1994, PROCEEDINGS OF THE WINTER 1994 USENIX CONFERENCE, P1
   Matsuzawa K, 2018, SYSTOR'18: PROCEEDINGS OF THE 11TH ACM INTERNATIONAL SYSTEMS AND STORAGE CONFERENCE, P65, DOI 10.1145/3211890.3211894
   Meister D, 2012, INT CONF HIGH PERFOR
   Meyer Dutch T., 2011, 9 USENIX C FIL STOR
   Morrey III Charles B., 2006, 23 IEEE S MASS STOR
   Muthitacharoen A., 2001, Operating Systems Review, V35, P174, DOI 10.1145/502059.502052
   Nachman Aviv, 2020, 18 USENIX C FIL STOR
   Nagesh P. C., 2013, 6 INT SYST STOR C SY
   Nam Youngjin., 2011, 2011 IEEE INT C HIGH
   Richards A, 2002, P AMER CONTR CONF, V1-6, P1936, DOI 10.1109/ACC.2002.1023918
   Sharma Prateek, 2012, 21 INT S HIGH PERF P
   Shilane Philip., 2016, 8 USENIX WORKSH HOT
   Srinivasan K., 2012, 10 USENIX C FILE STO, V12, P1
   Storer M.W., 2008, ACM INT WORKSH STOR
   Strunk John D., 2008, 6 USENIX C FIL STOR
   Sun Z, 2016, IEEE S MASS STOR SYS
   Tarasov Vasily, 2012, USENIX ANN TECHN C U
   Tran Nguyen, 2011, USENIX C USENIX ANN
   Waldspurger CA, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE FIFTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P181, DOI 10.1145/1060289.1060307
   Wallace Grant., 2012, Proceedings of the 10th USENIX conference on File and Storage Technologies, FAST'12, P4
   Xia Nai, 2018, 16 USENIX C FIL STOR
   Xia W, 2016, PROCEEDINGS OF USENIX ATC '16: 2016 USENIX ANNUAL TECHNICAL CONFERENCE, P101
   Xia W, 2014, PERFORM EVALUATION, V79, P258, DOI 10.1016/j.peva.2014.07.016
   Yan Zhichao, 2016, 8 USENIX WORKSH HOT
   Zhang YH, 2016, CHINA COMMUN, V13, P16, DOI 10.1109/CC.2016.7559071
   Zhu B, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P269
   Zhu Charlie Shucheng, 2012, 8 INT HAIF VER C HAR
NR 71
TC 3
Z9 3
U1 2
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2021
VL 17
IS 3
AR 24
DI 10.1145/3453301
PG 28
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA YY2GJ
UT WOS:000754611000009
DA 2024-07-18
ER

PT J
AU Wei, XD
   Chen, R
   Chen, HB
   Zang, BY
AF Wei, Xingda
   Chen, Rong
   Chen, Haibo
   Zang, Binyu
TI XStore: Fast RDMA-Based Ordered Key-Value Store Using Remote Learned
   Cache
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article; Proceedings Paper
CT 14th USENIX Symposium on Operating Systems Design and Implementation
   (OSDI)
CY NOV 04-06, 2020
CL ELECTR NETWORK
SP USENIX Assoc, Alibaba Grp, Amazon, ANT Grp, Alipay, Apple, ByteDance, Facebook, Google, Microsoft, VMware, Oracle, Two Sigma, ACM Queue, DMTF, FreeBSD Fdn
DE RDMA-based key-value store; machine learning model; tree-based index
   structure; index caching
ID DISTRIBUTED TRANSACTIONS
AB RDMA(Remote Direct MemoryAccess) has gained considerable interests in network-attached in-memory key-value stores. However, traversing the remote tree-based index in ordered key-value stores with RDMA becomes a critical obstacle, causing an order-of-magnitude slowdown and limited scalability due to multiple round trips. Using index cache with conventional wisdom-caching partial data and traversing them locally-usually leads to limited effect because of unavoidable capacity misses, massive random accesses, and costly cache invalidations.
   We argue that the machine learning (ML) model is a perfect cache structure for the tree-based index, termed learned cache. Based on it, we design and implement XStore, an RDMA-based ordered key-value store with a new hybrid architecture that retains a tree-based index at the server to perform dynamic workloads (e.g., inserts) and leverages a learned cache at the client to perform static workloads (e.g., gets and scans). The key idea is to decouple ML model retraining from index updating by maintaining a layer of indirection from logical to actual positions of key-value pairs. It allows a stale learned cache to continue predicting a correct position for a lookup key. XStore ensures correctness using a validation mechanism with a fallback path and further uses speculative execution to minimize the cost of cache misses. Evaluations with YCSB benchmarks and production workloads show that a single XStore server can achieve over 80 million read-only requests per second. This number outperforms state-of-the-art RDMA-based ordered key-value stores (namely, DrTMTree, Cell, and eRPC+Masstree) by up to 5.9x (from 3.7x). For workloads with inserts, XStore still provides up to 3.5x (from 2.7x) throughput speedup, achieving 53M reqs/s. The learned cache can also reduce clientside memory usage and further provides an efficient memory-performance tradeoff, e.g., saving 99% memory at the cost of 20% peak throughput.
C1 [Wei, Xingda; Chen, Rong; Chen, Haibo; Zang, Binyu] Shanghai Jiao Tong Univ, Shanghai, Peoples R China.
   [Wei, Xingda; Chen, Rong] Shanghai Artificial Intelligence Lab, Shanghai, Peoples R China.
   [Chen, Haibo; Zang, Binyu] Minist Educ, Engn Res Ctr Domain Specif Operating Syst, Shanghai, Peoples R China.
C3 Shanghai Jiao Tong University
RP Chen, R (corresponding author), Shanghai Jiao Tong Univ, Shanghai, Peoples R China.; Chen, R (corresponding author), Shanghai Artificial Intelligence Lab, Shanghai, Peoples R China.
EM wxdwfc@sjtu.edu.cn; rongchen@sjtu.edu.cn; haibochen@sjtu.edu.cn;
   byzang@sjtu.edu.cn
RI Chen, Rong/HJH-3926-2023
FU National Key Research and Development Program of China [2020YFB2104100];
   National Natural Science Foundation of China [61732010, 61925206];
   Huawei Technologies
FX This work was supported in part by the National Key Research and
   Development Program of China (No. 2020YFB2104100), the National Natural
   Science Foundation of China (No. 61732010 and 61925206), and a research
   grant from Huawei Technologies.
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Aguilera MK, 2019, PROCEEDINGS OF THE WORKSHOP ON HOT TOPICS IN OPERATING SYSTEMS (HOTOS '19), P120, DOI 10.1145/3317550.3321433
   [Anonymous], 2021, Memcached
   [Anonymous], 2021, OPENSTREETMAP 8 AWS
   [Anonymous], 2021, INTELS MATH KERNEL L
   Atikoglu Berk, 2012, Performance Evaluation Review, V40, P53, DOI 10.1145/2318857.2254766
   Blundell C., 2006, IEEE Computer Architecture Letters, V5, P65, DOI 10.1109/L-CA.2006.18
   Bronson N., 2013, P 2013 USENIX C ANN, P49
   Cao W, 2018, PROC VLDB ENDOW, V11, P1849, DOI 10.14778/3229863.3229872
   Cassell B, 2017, IEEE T PARALL DISTR, V28, P3537, DOI 10.1109/TPDS.2017.2729545
   Chen HB, 2017, ACM T COMPUT SYST, V35, DOI 10.1145/3092701
   Chen YZ, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901349
   Chen YM, 2019, PROCEEDINGS OF THE FOURTEENTH EUROSYS CONFERENCE 2019 (EUROSYS '19), DOI 10.1145/3302424.3303968
   Cooper Brian F., 2010, P 1 ACM S CLOUD COMP, P143, DOI [DOI 10.1145/1807128.1807152, 10.1145/1807128.1807152]
   Cooper Brian F., 2021, YCSB Core Workloads
   Corbett JC, 2013, ACM T COMPUT SYST, V31, DOI 10.1145/2491245
   Dai YF, 2020, PROCEEDINGS OF THE 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '20), P155
   Ding JL, 2020, SIGMOD'20: PROCEEDINGS OF THE 2020 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P969, DOI 10.1145/3318464.3389711
   Dragojevic A, 2015, SOSP'15: PROCEEDINGS OF THE TWENTY-FIFTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P54, DOI 10.1145/2815400.2815425
   Dragojevic Aleksandar, 2014, NSDI 14, P401
   Galakatos A, 2019, INT CONF MANAGE DATA, P1189, DOI 10.1145/3299869.3319860
   Graefe Goetz., 2004, VLDB 2004: Proceedings of the Thirtieth International Conference on Very Large Data Bases, P672
   Guo CX, 2016, PROCEEDINGS OF THE 2016 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '16), P202
   Gupta M, 2016, J MACH LEARN RES, V17
   High-Performance Big Data (HiBD), 2021, RDMA BAS MEMC RDMA M
   Jonas Eric, 2019, Cloud Programming Simplified: A Berkeley View on Serverless Computing
   Kalia A, 2019, PROCEEDINGS OF THE 16TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P1
   Kalia A, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P185
   Kalia A, 2016, PROCEEDINGS OF USENIX ATC '16: 2016 USENIX ANNUAL TECHNICAL CONFERENCE, P437
   Kalia A, 2014, SIGCOMM'14: PROCEEDINGS OF THE 2014 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P295, DOI [10.1145/2619239.2626299, 10.1145/2740070.2626299]
   Klimovic A, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P427
   Kraska T, 2018, INT CONF MANAGE DATA, P489, DOI 10.1145/3183713.3196909
   Lepers B, 2019, PROCEEDINGS OF THE TWENTY-SEVENTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '19), P447, DOI 10.1145/3341301.3359628
   Li BJ, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P137, DOI 10.1145/3132747.3132756
   Lim Hyeontaek, 2014, 11 USENIX S NETW SYS, P429
   Mao Y., 2012, P 7 ACM EUR C COMP S, P183
   Mitchell C, 2016, PROCEEDINGS OF USENIX ATC '16: 2016 USENIX ANNUAL TECHNICAL CONFERENCE, P451
   Mitchell Christopher, 2013, 2013 USENIX ANN TECH
   Nathan V, 2020, SIGMOD'20: PROCEEDINGS OF THE 2020 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P985, DOI 10.1145/3318464.3380579
   Nishtala R., 2013, Proceedings of the 10th USENIX Conference on Networked Systems Design and Implementation, NSDI '13, Berkeley, CA, USA, P385
   Pagh R, 2004, J ALGORITHMS, V51, P122, DOI 10.1016/j.jalgor.2003.12.002
   Paszke A, 2019, ADV NEUR IN, V32
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Shamis A, 2019, INT CONF MANAGE DATA, P433, DOI 10.1145/3299869.3300069
   Shi JX, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P317
   Sidler D, 2020, PROCEEDINGS OF THE FIFTEENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS (EUROSYS'20), DOI 10.1145/3342195.3387519
   Sowell B, 2012, PROC VLDB ENDOW, V5, P884
   Sreekanti V, 2020, PROC VLDB ENDOW, V13, P2438, DOI 10.14778/3407790.3407836
   Su MM, 2017, PROCEEDINGS OF THE TWELFTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS (EUROSYS 2017), P1, DOI 10.1145/3064176.3064189
   Tang CZ, 2020, PROCEEDINGS OF THE 25TH ACM SIGPLAN SYMPOSIUM ON PRINCIPLES AND PRACTICE OF PARALLEL PROGRAMMING (PPOPP '20), P308, DOI 10.1145/3332466.3374547
   The Transaction Processing Council, TPC C BENCHM V5 11
   Tsai SY, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P306, DOI 10.1145/3132747.3132762
   Verbitski A, 2017, SIGMOD'17: PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P1041, DOI 10.1145/3035918.3056101
   Wang YC, 2014, AMCIS 2014 PROCEEDINGS
   Wang ZJ, 2014, J CHEM-NY, V2014, DOI 10.1155/2014/475389
   Wei XD, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P233
   Wei XD, 2015, SOSP'15: PROCEEDINGS OF THE TWENTY-FIFTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P87, DOI 10.1145/2815400.2815419
   Xie XT, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P723
   You S., 2017, Advances in Neural Information Processing Systems, V30, P2981
   Youyun Wang, 2020, APSys 20. Proceedings of the 2020 SIGOPS Asia-Pacific Workshop on Systems, P17, DOI 10.1145/3409963.3410496
   Zamanian E, 2017, PROC VLDB ENDOW, V10, P685, DOI 10.14778/3055330.3055335
   Zhang H, 2016, SIGMOD'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P1567, DOI 10.1145/2882903.2915222
   Ziegler T, 2019, INT CONF MANAGE DATA, P741, DOI 10.1145/3299869.3300081
NR 63
TC 3
Z9 3
U1 2
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2021
VL 17
IS 3
AR 18
DI 10.1145/3468520
PG 32
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA YY2GJ
UT WOS:000754611000003
DA 2024-07-18
ER

PT J
AU Papagiannis, A
   Saloustros, G
   Xanthakis, G
   Kalaentzis, G
   Gonzalez-Ferez, P
   Bilas, A
AF Papagiannis, Anastasios
   Saloustros, Giorgos
   Xanthakis, Giorgos
   Kalaentzis, Giorgos
   Gonzalez-Ferez, Pilar
   Bilas, Angelos
TI Kreon: An Efficient Memory-Mapped Key-Value Store for Flash Storage
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Key-value stores; LSM-tree; copy-on-write; memory-mapped I/O
AB Persistent key-value stores have emerged as a main component in the data access path of modern data processing systems. However, they exhibit high CPU and I/O overhead. Nowadays, due to power limitations, it is important to reduce CPU overheads for data processing.
   In this article, we propose Kreon, a key-value store that targets servers with flash-based storage, where CPU overhead and I/O amplification are more significant bottlenecks compared to I/O randomness. We first observe that two significant sources of overhead in key-value stores are: (a) The use of compaction in LogStructured Merge-Trees (LSM-Tree) that constantly perform merging and sorting of large data segments and (b) the use of an I/O cache to access devices, which incurs overhead even for data that reside in memory. To avoid these, Kreon performs data movement from level to level by using partial reorganization instead of full data reorganization via the use of a full index per-level. Kreon uses memory-mapped I/O via a custom kernel path to avoid a user-space cache.
   For a large dataset, Kreon reduces CPU cycles/op by up to 5.8x, reduces I/O amplification for inserts by up to 4.61x, and increases insert ops/s by up to 5.3x, compared to RocksDB.
C1 [Papagiannis, Anastasios; Saloustros, Giorgos; Xanthakis, Giorgos; Kalaentzis, Giorgos; Bilas, Angelos] FORTH, Inst Comp Sci, Iraklion, Greece.
   [Gonzalez-Ferez, Pilar] Univ Murcia, Dept Comp Engn, Murcia, Spain.
   [Papagiannis, Anastasios; Xanthakis, Giorgos; Kalaentzis, Giorgos; Bilas, Angelos] Univ Crete, Dept Comp Sci, Iraklion, Greece.
C3 Foundation for Research & Technology - Hellas (FORTH); University of
   Murcia; University of Crete
RP Papagiannis, A (corresponding author), FORTH, Inst Comp Sci, Iraklion, Greece.
EM apapag@ics.forth.gr; gesalous@ics.forth.gr; gxanth@ics.forth.gr;
   gkalaent@ics.forth.gr; pilargf@um.es; bilas@ics.forth.gr
RI Xanthakis, Giorgos/ABA-5325-2021; Gonzalez-Ferez, Pilar/ABE-3995-2021
OI Xanthakis, Giorgos/0000-0001-8591-006X; Gonzalez-Ferez,
   Pilar/0000-0003-1681-5442; Bilas, Angelos/0000-0003-2975-4124
FU European Commission FEDER funds [RTI2018-098156-B-C53]; Facebook
   Graduate Fellowship; Spanish MCIU; Spanish AEI
FX P. Gonzalez-Ferez gratefully acknowledges the financial support of
   Spanish MCIU and AEI and the European Commission FEDER funds under Grant
   RTI2018-098156-B-C53. A. Papagiannis is also supported by the Facebook
   Graduate Fellowship.
CR Ahn Jung-Sang, 2019, P 11 USENIX C HOT TO, V9
   [Anonymous], 2008, P 2008 ACM SIGMOD IN, DOI [10.1145/1376616.1376713, DOI 10.1145/1376616.1376713]
   [Anonymous], 2012, IEEE Data Eng. Bull.
   Axboe J., 2017, Flexible i/o tester
   Axboe Jens, 2019, Efficient IO with io_uring
   Balmau O, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P753
   Balmau O, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P363
   Batsaras Nikos, 2020, GIORGOS SALOUSTROS A
   BAYER R, 1977, ACTA INFORM, V9, P1
   Bayer R., 2002, Organization and maintenance of large ordered indexes
   BENDER MA, 2015, MAGAZINE, V40, P5
   Bierbaum N, 2002, CONF LOCAL COMPUT NE, P733, DOI 10.1109/LCN.2002.1181853
   Bohannon P, 2001, SIGMOD REC, V30, P163, DOI 10.1145/376284.375681
   Burns R, 2001, NINTH INTERNATIONAL SYMPOSIUM ON MODELING, ANALYSIS AND SIMULATION OF COMPUTER AND TELECOMMUNICATION SYSTEMS, PROCEEDINGS, P302, DOI 10.1109/MASCOT.2001.948881
   Cao GS, 2014, MICRO NANO LETT, V9, P16, DOI 10.1049/mnl.2013.0612
   Chan HHW, 2018, PROCEEDINGS OF THE 2018 USENIX ANNUAL TECHNICAL CONFERENCE, P1007
   Chazelle B, 1986, ALGORITHMICA, V1, P133, DOI 10.1007/BF01840440
   Chen Y, 2012, PROC VLDB ENDOW, V5, P1802, DOI 10.14778/2367502.2367519
   Chen YM, 2020, TWENTY-FIFTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXV), P1077, DOI 10.1145/3373376.3378515
   Chodorow Kristina, 2013, MongoDB: The Definitive Guide, V2nd
   Choi J., 2017, 9 USENIX WORKSHOP HO
   Clements AT, 2012, ASPLOS XVII: SEVENTEENTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS, P199
   Cooper Brian F., 2010, P 1 ACM S CLOUD COMP, P143, DOI [DOI 10.1145/1807128.1807152, 10.1145/1807128.1807152]
   Cooper Brian F., 2018, CORE WORKLOADS
   Dayan N, 2018, INT CONF MANAGE DATA, P505, DOI 10.1145/3183713.3196927
   Dayan N, 2017, SIGMOD'17: PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P79, DOI 10.1145/3035918.3064054
   DeCandia Giuseppe, 2007, Operating Systems Review, V41, P205, DOI 10.1145/1323293.1294281
   Dong Siying, 2017, CIDR 2017
   Dragojevic Aleksandar, 2014, NSDI 14, P401
   Golan-Gueta Guy, 2015, P 10 EUR C COMP SYST
   González-Férez P, 2014, IEEE S MASS STOR SYS
   Graefe G, 2012, ACM T DATABASE SYST, V37, DOI 10.1145/2338626.2338630
   Graefe Goetz., 2004, VLDB 2004: Proceedings of the Thirtieth International Conference on Very Large Data Bases, P672
   Gregg B, 2016, QUEUE, V14, P91, DOI [10.1145/2927299.2927301, DOI 10.1145/2927299.2927301]
   Jagadish HV, 1997, PROCEEDINGS OF THE TWENTY-THIRD INTERNATIONAL CONFERENCE ON VERY LARGE DATABASES, P16
   Jannen W., 2015, P 13 USENIX C FIL ST, P301, DOI DOI 10.5555/2750482.2750505
   Jannink J., 1995, SIGMOD Record, V24, P33, DOI 10.1145/202660.202666
   Jose J., 2011, 2011 International Conference on Parallel Processing, P743, DOI 10.1109/ICPP.2011.37
   Kalia A, 2016, PROCEEDINGS OF USENIX ATC '16: 2016 USENIX ANNUAL TECHNICAL CONFERENCE, P437
   Kalia A, 2014, SIGCOMM'14: PROCEEDINGS OF THE 2014 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P295, DOI [10.1145/2619239.2626299, 10.1145/2740070.2626299]
   Kannan S, 2018, PROCEEDINGS OF THE 2018 USENIX ANNUAL TECHNICAL CONFERENCE, P993
   Kuszmaul Bradley, 2014, CISC VIS NETW IND GL
   Lai CB, 2015, IEEE S MASS STOR SYS
   LAMPORT L, 1977, COMMUN ACM, V20, P806, DOI 10.1145/359863.359878
   Lepers B, 2019, PROCEEDINGS OF THE TWENTY-SEVENTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '19), P447, DOI 10.1145/3341301.3359628
   Li BJ, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P137, DOI 10.1145/3132747.3132756
   Li YA, 2009, PROC INT CONF DATA, P1303, DOI 10.1109/ICDE.2009.226
   Lotfi-Kamran P, 2012, CONF PROC INT SYMP C, P500, DOI 10.1109/ISCA.2012.6237043
   Lu LY, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P133
   Marinos I, 2014, ACM SIGCOMM COMP COM, V44, P175, DOI 10.1145/2740070.2626311
   Mei F, 2018, PROCEEDINGS OF THE 2018 ACM SYMPOSIUM ON CLOUD COMPUTING (SOCC '18), P477, DOI 10.1145/3267809.3267829
   Mitchell Christopher, 2013, 2013 USENIX ANN TECH
   Olson M.A., 1999, ATEC 99 P ANN C USEN, P43
   ONeil P, 1996, ACTA INFORM, V33, P351, DOI 10.1007/s002360050048
   Papagiannis A, 2020, PROCEEDINGS OF THE 2020 USENIX ANNUAL TECHNICAL CONFERENCE, P813
   Papagiannis A, 2018, PROCEEDINGS OF THE 2018 ACM SYMPOSIUM ON CLOUD COMPUTING (SOCC '18), P490, DOI 10.1145/3267809.3267824
   Papagiannis A, 2016, PROCEEDINGS OF USENIX ATC '16: 2016 USENIX ANNUAL TECHNICAL CONFERENCE, P537
   Raju P, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P497, DOI 10.1145/3132747.3132765
   Rodeh Ohad, 2008, ACM Transaction on Storage, V3, p15:1, DOI 10.1145/1326542.1326544
   Samuels Allen, 2018, The Consequences of Infinite Storage Bandwidth
   Sears Russell, 2012, P ACM SIGMOD INT C M, P217
   Shetty P.J., 2013, 11 USENIX C FILE STO, P17
   Song NY, 2016, ACM T STORAGE, V12, DOI 10.1145/2846100
   Su MM, 2017, PROCEEDINGS OF THE TWELFTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS (EUROSYS 2017), P1, DOI 10.1145/3064176.3064189
   Van Essen B, 2015, CLUSTER COMPUT, V18, P15, DOI 10.1007/s10586-013-0309-0
   Wang YD, 2015, PROCEEDINGS OF SC15: THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS, DOI 10.1145/2807591.2807614
   Wei XD, 2015, SOSP'15: PROCEEDINGS OF THE TWENTY-FIFTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P87, DOI 10.1145/2815400.2815419
   Wu XN, 2015, METHODS MOL BIOL, V1306, P71, DOI 10.1007/978-1-4939-2648-0_5
   Yoon H, 2018, PROCEEDINGS OF THE 2018 ACM SYMPOSIUM ON CLOUD COMPUTING (SOCC '18), P162, DOI 10.1145/3267809.3267846
NR 69
TC 6
Z9 7
U1 0
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD FEB
PY 2021
VL 17
IS 1
AR 7
DI 10.1145/3418414
PG 32
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QG9WR
UT WOS:000617930800006
DA 2024-07-18
ER

PT J
AU Ji, C
   Pan, RW
   Chang, LP
   Shi, L
   Zhu, ZW
   Liang, Y
   Kuo, TW
   Xue, CJ
AF Ji, Cheng
   Pan, Riwei
   Chang, Li-Pin
   Shi, Liang
   Zhu, Zongwei
   Liang, Yu
   Kuo, Tei-Wei
   Xue, Chun Jason
TI Inspection and Characterization of App File Usage in Mobile Devices
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Measurements; i/O system; file usage; storage space; memory space
AB While the computing power of mobile devices has been quickly evolving in recent years, the growth of mobile storage capacity is, however, relatively slower. A common problem shared by budget-phone users is that they frequently run out of storage space. This article conducts a deep inspection of file usage of mobile applications and their potential implications on user experience. Our major findings are as follows: First, mobile applications could rapidly consume storage space by creating temporary cache files, but these cache files quickly become obsolete after being re-used for a short period of time. Second, file access patterns of large files, especially executable files, appear highly sparse and random, and therefore large portions of file space are never visited. Third, file prefetching brings an excessive amount of file data into page cache but only a few prefetched data are actually used. The unnecessary memory pressure causes premature memory reclamation and prolongs application launching time. Through the feasibility study of two preliminary optimizations, we demonstrated a high potential to eliminate unnecessary storage and memory space consumption with a minimal impact on user experience.
C1 [Ji, Cheng] Nanjing Univ Sci & Technol, Nanjing, Peoples R China.
   [Pan, Riwei; Liang, Yu; Kuo, Tei-Wei; Xue, Chun Jason] City Univ Hong Kong, Hong Kong, Peoples R China.
   [Chang, Li-Pin] Natl Chiao Tung Univ, Hsinchu, Taiwan.
   [Shi, Liang] East China Normal Univ, Shanghai, Peoples R China.
   [Zhu, Zongwei] Univ Sci & Technol China, Suzhou Inst Adv Study, Shanghai, Peoples R China.
C3 Nanjing University of Science & Technology; City University of Hong
   Kong; National Yang Ming Chiao Tung University; East China Normal
   University; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS
RP Zhu, ZW (corresponding author), Univ Sci & Technol China, Suzhou Inst Adv Study, Shanghai, Peoples R China.
EM cheng.ji@njust.edu.cn; riweipan@cityu.edu.hk; lpchang@cs.nctu.edu.tw;
   shi.liang.hk@gmail.com; zzw1988@ustc.edu.cn; yliang22c@my.cityu.edu.hk;
   teiwei.kuo@cityu.edu.hk; jasonxue@cityu.edu.hk
OI Liang, Yu/0000-0002-7754-9427; PAN, Riwei/0000-0002-0664-1031; KUO,
   TEI-WEI/0000-0003-1974-0394
FU Research Grants Council of the Hong Kong Special Administrative Region,
   China [CityU 11219319, MOST 107-2628-E-009-002-MY3]; Ministry of Science
   and Technology, Taiwan; NSFC [61772092]
FX This work is partially supported by a grant from the Research Grants
   Council of the Hong Kong Special Administrative Region, China (Project
   No. CityU 11219319), MOST 107-2628-E-009-002-MY3, Ministry of Science
   and Technology, Taiwan and NSFC 61772092.
CR Amos Waterland, 2014, STRESS IS DELIBERATE
   Courville J, 2016, IEEE S MASS STOR SYS
   Hahn Sangwook Shane, 2017, P USENIX INT C ADV T
   Harter T, 2012, ACM T COMPUT SYST, V30, DOI 10.1145/2324876.2324878
   Jeong Daeho., 2015, 13th USENIX Conference on File and Storage Technologies (FAST 15), P191
   Ji C, 2017, ACM T EMBED COMPUT S, V16, DOI 10.1145/3126511
   Jiang S, 2013, ACM T STORAGE, V9, DOI 10.1145/2508010
   Kim B, 2014, 2014 IEEE 3RD GLOBAL CONFERENCE ON CONSUMER ELECTRONICS (GCCE), P709, DOI 10.1109/GCCE.2014.7031128
   Kim H, 2013, INT WORKSHOP MICROPR, P14, DOI 10.1109/MTV.2013.18
   Kim H, 2012, ACM T STORAGE, V8, DOI 10.1145/2385603.2385607
   Kim JH, 2015, LECT NOTES ELECTR EN, V331, P143, DOI 10.1007/978-94-017-9618-7_14
   Kim SH, 2017, ACM T EMBED COMPUT S, V16, DOI 10.1145/3126509
   Lee, 2015, USENIX ATC, V15, P235
   Lee Jongmin, 2013, P ACM AS PAC WORKSH, P16
   Lee K, 2012, EMSOFT '12: PROCEEDINGS OF THE TENTH AMC INTERNATIONAL CONFERENCE ON EMBEDDED SOFTWARE 2012, P23
   Liu D, 2017, IEEE T COMPUT, V66, P1918, DOI 10.1109/TC.2017.2711620
   Luo XQ, 2017, INT PARALL DISTRIB P, P585, DOI 10.1109/IPDPS.2017.45
   Nguyen D.T., 2015, PROC ANN INT C MOBIL, P287, DOI DOI 10.1145/2742647.2742661
   Purdy Kevin, 2014, INSUFFICIENT STORAGE
   Sato Takashi, 2007, P LIN S, V2, P179
   Shepard C., 2011, ACM SIGMETRICS Performance Evaluation Review, V38, P15, DOI /10.1145/1925019.1925023
   Summerson Cameron, 2016, 5 WAYS FREE UP SPACE
NR 22
TC 11
Z9 12
U1 0
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2020
VL 16
IS 4
AR 25
DI 10.1145/3404119
PG 25
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA PA3GG
UT WOS:000595524400005
DA 2024-07-18
ER

PT J
AU Sun, K
   Fryer, D
   Wang, R
   Patel, S
   Chu, J
   Lakier, M
   Brown, AD
   Goel, A
AF Sun, Kuei
   Fryer, Daniel
   Wang, Russell
   Patel, Sagar
   Chu, Joseph
   Lakier, Matthew
   Brown, Angela Demke
   Goel, Ashvin
TI Spiffy: Enabling File-System Aware Storage Applications
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Annotation language; metadata parsing and serialization; file-system
   traversal; robustness; generic file-system aware applications; Ext4;
   Btrfs; F2FS
AB Many file-system applications such as defragmentation tools, file-system checkers, or data recovery tools, operate at the storage layer. Today, developers of these file-system aware storage applications require detailed knowledge of the file-system format, which requires significant time to learn, often by trial and error, due to insufficient documentation or specification of the format. Furthermore, these applications perform ad-hoc processing of the file-system metadata, leading to bugs and vulnerabilities.
   We propose Spiffy, an annotation language for specifying the on-disk format of a file system. File-system developers annotate the data structures of a file system, and we use these annotations to generate a library that allows identifying, parsing, and traversing file-system metadata, providing support for both offline and online storage applications. This approach simplifies the development of storage applications that work across different file systems because it reduces the amount of file-system-specific code that needs to be written.
   We have written annotations for the Linux Ext4, Btrfs, and F2FS file systems, and developed several applications for these file systems, including a type-specific metadata corruptor, a file-system converter, an online storage layer cache that preferentially caches files for certain users, and a runtime file-system checker. Our experiments show that applications built with the Spiffy library for accessing file-system metadata can achieve good performance and are robust against file-system corruption errors.
C1 [Sun, Kuei; Fryer, Daniel; Wang, Russell; Patel, Sagar; Chu, Joseph; Lakier, Matthew; Brown, Angela Demke; Goel, Ashvin] Univ Toronto, Edward S Rogers Sr Dept Elect & Comp Engn, 10 Kings Coll Rd,Room SFB540, Toronto, ON M5S 3G4, Canada.
C3 University of Toronto
RP Sun, K (corresponding author), Univ Toronto, Edward S Rogers Sr Dept Elect & Comp Engn, 10 Kings Coll Rd,Room SFB540, Toronto, ON M5S 3G4, Canada.
EM kuei.sun@mail.utoronto.ca; dfryer@cs.toronto.edu;
   russell.wang@mail.utoronto.ca; sagu.patel@mail.utoronto.ca;
   josephchu21@gmail.com; matthew.lakier@mail.utoronto.ca;
   demke@cs.toronto.edu; ashvin@eecg.toronto.edu
RI Chu, Joseph/IAP-6176-2023
FU NSERC Discovery
FX This work was supported by NSERC Discovery.
CR Amani Sidney, 2012, EUROSYS DOCT WORKSH
   [Anonymous], 2016, American fuzzy lop
   [Anonymous], 2002, ACM SIGPLAN NOTICES, DOI [DOI 10.1145/565816.503286, 10.1145/503272.503286, DOI 10.1145/503272.503286]
   [Anonymous], 2011, DATABASE SYSTEMS, DOI DOI 10.2390/BIECOLL-JIB-2011-165
   [Anonymous], 1994, LaTeX: A Document Preparation System: User's Guide and Reference Manual
   Bairavasundaram LN, 2008, I C DEPEND SYS NETWO, P502, DOI 10.1109/DSN.2008.4630121
   Bangert Julian, 2014, 11 USENIX S OPERATIN, P615
   Beazley David, 2013, PLY PYTHON LEX YACC
   Buckeye Brian, 2006, RECOVERING DELETED F
   Chen HG, 2015, SOSP'15: PROCEEDINGS OF THE TWENTY-FIFTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P18, DOI 10.1145/2815400.2815402
   Chow Michael, 2014, P 11 S OP SYST DES I, P217
   Danial A., 2009, OPEN SOURCE
   Fisher Kathleen, 2011, P 14 INT C DAT THEOR, P11, DOI [DOI 10.1145/1938551.1938556, 10.1145/1938551.1938556]
   Fryer D, 2012, ACM T STORAGE, V8, DOI 10.1145/2385603.2385608
   Gamma Erich, 1995, DESIGN PATTERNS ELEM
   Garcia-Molina Hector, 2000, Database system implementation, V672
   Gardner P, 2014, LECT NOTES COMPUT SC, V8410, P169
   Gedak Curtis, 2012, MANAGE PARTITIONS GP
   Gunawi Haryadi S., 2007, Operating Systems Review, V41, P293, DOI 10.1145/1323293.1294290
   Gunawi Haryadi S., 2008, P 8 USENIX C OP SYST, P131
   Hesselink WH, 2009, ELECTRON NOTES THEOR, V259, P67, DOI 10.1016/j.entcs.2009.12.018
   Hickson Ian, 2011, WDHTML520110525 W3C
   Kent Overstreet, 2016, LIN BCACH
   Lee Changman, 2015, 13 USENIX C FILE STO, P273
   Lu LY, 2014, ACM T STORAGE, V10, DOI 10.1145/2560012
   Ma Ao, 2013, P USENIX C FIL STOR
   Mesnier M, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P57
   Microsoft TechNet, CONVERT FAT DISKS NT
   Patterson Meredith, 2014, HAMMER PARSER GENERA
   Rodeh O, 2013, ACM T STORAGE, V9, DOI 10.1145/2501620.2501623
   Ronacher Armin, 2011, JINJA2 DOCUMENTATION
   Sivathanu G, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P15
   Sivathanu M, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE 2ND USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST'03), P73
   Sivathanu Muthian, 2005, P USENIX C FIL STOR
   Steedman Douglas., 1993, Abstract syntax notation one (ASN. 1): the tutorial and reference
   Stefanovici I, 2015, ACM SOCC'15: PROCEEDINGS OF THE SIXTH ACM SYMPOSIUM ON CLOUD COMPUTING, P174, DOI 10.1145/2806777.2806933
   Sun Kuei, 2018, P 10 USENIX WORKSH H
   Sun Kuei Jack, 2013, THESIS
   Theodore Ts'o, 2017, E2FSPROGS EXT2 3 4 F
   Torvalds Linus, 2007, SPARSE SEMANTIC PARS
   Varda Kenton, 2008, Technical Report
   Warren Tom, APPLE IS UPGRADING M
   Wilson AN, 2008, TLS-TIMES LIT SUPPL, P6
   Yang JF, 2006, ACM T COMPUT SYST, V24, P393, DOI 10.1145/1189256.1189259
   Zhao X, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P603
   Zhou F, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P45
NR 46
TC 0
Z9 0
U1 0
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2020
VL 16
IS 3
AR 16
DI 10.1145/3386368
PG 39
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1QZ
UT WOS:000583743600002
DA 2024-07-18
ER

PT J
AU Yang, F
   Chen, YM
   Mao, HY
   Lu, YY
   Shu, JW
AF Yang, Fan
   Chen, Youmin
   Mao, Haiyu
   Lu, Youyou
   Shu, Jiwu
TI ShieldNVM: An Efficient and Fast Recoverable System for Secure
   Non-Volatile Memory
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Non-volatile memory; crash consistency; memory security
ID ENCRYPTION; PERFORMANCE
AB Data encryption and authentication are essential for secure non-volatile memory (NVM). However, the introduced security metadata needs to be atomically written back to NVM along with data, so as to provide crash consistency, which unfortunately incurs high overhead. To support fine-grained data protection and fast recovery for a secure NVM system without compromising the performance, we propose ShieldNVM. It first proposes an epoch-based mechanism to aggressively cache the security metadata in the metadata cache while retaining the consistency of them in NVM. Deferred spreading is also introduced to reduce the calculating overhead for data authentication. Leveraging the ability of data hash message authentication codes, we can always recover the consistent but old security metadata to its newest version. By recording a limited number of dirty addresses of the security metadata, ShieldNVM achieves fast recovering the secure NVM system after crashes. Compared to Osiris, a state-of-the-art secure NVM, ShieldNVM reduces system runtime by 39.1% and hash message authentication code computation overhead by 80.5% on average over NVM workloads. When system crashes happen, ShieldNVM's recovery time is orders of magnitude faster than Osiris. In addition, ShieldNVM also recovers faster than AGIT, which is the Osiris-based state-of-the-art mechanism addressing the recovery time of the secure NVM system. Once the recovery process fails, instead of dropping all data due to malicious attacks, ShieldNVM is able to detect and locate the area of the tampered data with the help of the tracked addresses.
C1 [Yang, Fan; Chen, Youmin; Mao, Haiyu; Lu, Youyou; Shu, Jiwu] Tsinghua Univ, Beijing, Peoples R China.
C3 Tsinghua University
RP Lu, YY; Shu, JW (corresponding author), Tsinghua Univ, Beijing, Peoples R China.
EM yangf17@mails.tsinghua.edu.cn; chenym16@mails.tsinghua.edu.cn;
   mhy15@mails.tsinghua.edu.cn; luyouyou@tsinghua.edu.cn;
   shujw@tsinghua.edu.cn
RI Mao, Haiyu/ABD-7355-2020; Yang, Fei/JLM-3367-2023; Yang,
   Fan/GYU-4249-2022; yang, fei/HPF-9658-2023
OI Mao, Haiyu/0000-0002-7393-4504; Chen, Youmin/0000-0003-4171-4299
FU National Key Research and Development Program of China [2018YFB1003301];
   National Natural Science Foundation of China [61832011]; Research and
   Development Plan in Key Field of Guangdong Province [2018B010109002]
FX This work was supported by National Key Research and Development Program
   of China (grant no. 2018YFB1003301), the National Natural Science
   Foundation of China (grant no. 61832011), and the Research and
   Development Plan in Key Field of Guangdong Province (grant no.
   2018B010109002).
CR Abu Zubair K, 2019, PROCEEDINGS OF THE 2019 46TH INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA '19), P157, DOI 10.1145/3307650.3322252
   Alshboul M, 2018, CONF PROC INT SYMP C, P439, DOI 10.1109/ISCA.2018.00044
   [Anonymous], 2016, Deprecating the PCOMMIT Instruction
   Awad A, 2019, PROCEEDINGS OF THE 2019 46TH INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA '19), P104, DOI 10.1145/3307650.3322250
   Awad A, 2016, ACM SIGPLAN NOTICES, V51, P263, DOI 10.1145/2954679.2872377
   Binkert Nathan, 2011, Computer Architecture News, V39, P1, DOI 10.1145/2024716.2024718
   Chen S, 2019, PROCEEDINGS OF THE 2019 56TH ACM/EDAC/IEEE DESIGN AUTOMATION CONFERENCE (DAC), DOI 10.1145/3316781.3317810
   Chen YM, 2018, ACM T STORAGE, V14, DOI 10.1145/3204454
   Chhabra S, 2011, ISCA 2011: PROCEEDINGS OF THE 38TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE, P177, DOI 10.1145/2024723.2000086
   Coburn J, 2011, ACM SIGPLAN NOTICES, V46, P105, DOI [10.1145/1961295.1950380, 10.1145/1961296.1950380]
   Condit J, 2009, SOSP'09: PROCEEDINGS OF THE TWENTY-SECOND ACM SIGOPS SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P133
   Dulloor Subramanya R., 2014, P EUR C COMP SYST EU, P1, DOI [10.1145/2592798.2592814, DOI 10.1145/2592798.2592814]
   Eduardo B., 2017, ENHANCING HIGH PERFO
   Gassend B, 2003, NINTH INTERNATIONAL SYMPOSIUM ON HIGH-PERFORMANCE COMPUTER ARCHITECTURE, PROCEEDINGS, P295, DOI 10.1109/HPCA.2003.1183547
   Gogte V, 2018, ACM SIGPLAN NOTICES, V53, P46, DOI [10.1145/3192366.3192367, 10.1145/3296979.3192367]
   Gu JY, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P913
   Gupta S, 2019, MICRO'52: THE 52ND ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE, P466, DOI 10.1145/3352460.3358321
   Hsu TCH, 2017, PROCEEDINGS OF THE TWELFTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS (EUROSYS 2017), P468, DOI 10.1145/3064176.3064204
   Hu QD, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P703
   Hwang D, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P187
   Intel, 2015, Intel and Micron Produce Breakthrough Memory Technology
   Intel, 2018, Intel architecture instruction set extensions programming reference
   Izraelevitz J, 2016, ACM SIGPLAN NOTICES, V51, P427, DOI 10.1145/2954679.2872410
   Joshi A, 2018, CONF PROC INT SYMP C, P452, DOI 10.1109/ISCA.2018.00045
   Joshi A, 2017, INT S HIGH PERF COMP, P361, DOI 10.1109/HPCA.2017.50
   Joshi A, 2015, PROCEEDINGS OF THE 48TH ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE (MICRO-48), P660, DOI 10.1145/2830772.2830805
   Kolli A., 2016, 49 ANN IEEEACM INT S, P1
   Kolli A, 2017, 44TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA 2017), P481, DOI [10.1145/3079856.3080229, 10.1145/3140659.3080229]
   Kolli A, 2016, ACM SIGPLAN NOTICES, V51, P399, DOI 10.1145/2954679.2872381
   Kultursay Emre, 2013, IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS 2013), P256
   Lee BC, 2009, CONF PROC INT SYMP C, P2, DOI 10.1145/1555815.1555758
   Lee J, 2016, IEEE T COMPUT, V65, P3384, DOI 10.1109/TC.2016.2538218
   Lehman TS, 2018, INT SYM PERFORM ANAL, P33, DOI 10.1109/ISPASS.2018.00012
   Lehman Tamara Silbergleit., 2016, IEEE/ACM International Symposium on Microarchitecture (MICRO), P1, DOI DOI 10.1109/MICRO.2016.7783741
   Liu MX, 2017, TWENTY-SECOND INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXII), P329, DOI 10.1145/3037697.3037714
   Liu SH, 2019, PROCEEDINGS OF THE 2019 46TH INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA '19), P143, DOI 10.1145/3307650.3322206
   Liu S, 2018, INT S HIGH PERF COMP, P310, DOI 10.1109/HPCA.2018.00035
   Lu Y., 2015, 2015 IEEE INT SOLID, P1
   Mao HY, 2017, DES AUT TEST EUROPE, P1623, DOI 10.23919/DATE.2017.7927251
   Mulnix D., 2015, INTEL XEON PROCESSOR
   Nalli S, 2017, TWENTY-SECOND INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXII), P135, DOI 10.1145/3037697.3037730
   Nam M, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P31
   Narayanan D., 2012, P 17 INT C ARCH SUPP, V40, P401
   Ni Y, 2018, 10 USENIX WORKSH HOT
   Oukid I, 2016, SIGMOD'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P371, DOI 10.1145/2882903.2915251
   Pelley S, 2014, CONF PROC INT SYMP C, P265, DOI 10.1109/ISCA.2014.6853222
   Pmem.io, 2019, PERSISTENT MEMORY DE
   Rakshit J, 2017, DES AUT CON, DOI 10.1145/3061639.3062205
   Ramalhete P, 2019, I C DEPEND SYS NETWO, P151, DOI 10.1109/DSN.2019.00028
   Ren JL, 2015, PROCEEDINGS OF THE 48TH ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE (MICRO-48), P672, DOI 10.1145/2830772.2830802
   Rogers B, 2007, INT SYMP MICROARCH, P183, DOI 10.1109/MICRO.2007.16
   Saileshwar G, 2018, 2018 51ST ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE (MICRO), P416, DOI 10.1109/MICRO.2018.00041
   Saileshwar G, 2018, INT S HIGH PERF COMP, P454, DOI 10.1109/HPCA.2018.00046
   Shin S, 2017, 50TH ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE (MICRO), P178, DOI 10.1145/3123939.3124539
   Suh GE, 2003, 36TH INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE, PROCEEDINGS, P339
   Swami S, 2018, DES AUT CON, DOI 10.1145/3195970.3196123
   Swami S, 2018, DES AUT CON, DOI 10.1145/3195970.3195983
   Swami S, 2018, IEEE COMPUT ARCHIT L, V17, P192, DOI 10.1109/LCA.2018.2863281
   Swami S, 2017, DES AUT TEST EUROPE, P906, DOI 10.23919/DATE.2017.7927117
   Swami S, 2016, DES AUT CON, DOI 10.1145/2897937.2898087
   Taassori Meysam, 2018, ACM SIGPLAN Notices, V53, P665, DOI 10.1145/3296957.3177155
   Volos Haris, 2011, SIGPLAN Notices, V46, P91, DOI 10.1145/1961296.1950379
   Wu Xiaojian., 2011, Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis (SC '11), p39:1
   Xu J, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P323
   Yan CY, 2006, CONF PROC INT SYMP C, P179, DOI 10.1145/1150019.1136502
   Yang F, 2019, PROCEEDINGS OF THE 2019 56TH ACM/EDAC/IEEE DESIGN AUTOMATION CONFERENCE (DAC), DOI 10.1145/3316781.3317869
   Ye Mao, 2018, P 2018 51 ANN IEEE A
   Young V, 2015, ACM SIGPLAN NOTICES, V50, P33, DOI [10.1145/2775054.2694387, 10.1145/2694344.2694387]
   Youyou Lu, 2014, 2014 IEEE 32nd International Conference on Computer Design (ICCD), P216, DOI 10.1109/ICCD.2014.6974684
   Zhou L, 2013, J GENET GENOMICS, V40, P421, DOI 10.1016/j.jgg.2013.04.006
   Zuo PF, 2019, MICRO'52: THE 52ND ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE, P479, DOI 10.1145/3352460.3358290
   Zuo PF, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P461
NR 72
TC 8
Z9 8
U1 0
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUN
PY 2020
VL 16
IS 2
AR 12
DI 10.1145/3381835
PG 31
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OJ1RC
UT WOS:000583743900005
DA 2024-07-18
ER

PT J
AU Wang, J
   Bao, WD
   Zheng, L
   Zhu, XM
   Yu, PS
AF Wang, Ji
   Bao, Weidong
   Zheng, Lei
   Zhu, Xiaomin
   Yu, Philip S.
TI An Attention-augmented Deep Architecture for Hard Drive Status
   Monitoring in Large-scale Storage Systems
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Hard drive failure; SMART; deep neural network; recurrent neural
   network; attention mechanism
ID NEURAL-NETWORKS
AB Data centers equipped with large-scale storage systems are critical infrastructures in the era of big data. The enormous amount of hard drives in storage systems magnify the failure probability, which may cause tremendous loss for both data service users and providers. Despite a set of reactive fault-tolerant measures such as RAID, it is still a tough issue to enhance the reliability of large-scale storage systems. Proactive prediction is an effective method to avoid possible hard-drive failures in advance. A series of models based on the SMART statistics have been proposed to predict impending hard-drive failures. Nonetheless, there remain some serious yet unsolved challenges like the lack of explainability of prediction results. To address these issues, we carefully analyze a dataset collected from a real-world large-scale storage system and then design an attention-augmented deep architecture for hard-drive health status assessment and failure prediction. The deep architecture, composed of a feature integration layer, a temporal dependency extraction layer, an attention layer, and a classification layer, cannot only monitor the status of hard drives but also assist in failure cause diagnoses. The experiments based on real-world datasets show that the proposed deep architecture is able to assess the hard-drive status and predict the impending failures accurately. In addition, the experimental results demonstrate that the attention-augmented deep architecture can reveal the degradation progression of hard drives automatically and assist administrators in tracing the cause of hard drive failures.
C1 [Wang, Ji; Bao, Weidong] Natl Univ Def Technol, Coll Syst Engn, Changsha 410073, Hunan, Peoples R China.
   [Zheng, Lei; Yu, Philip S.] Univ Illinois, Dept Comp Sci, Chicago, IL 60607 USA.
   [Zhu, Xiaomin] Natl Univ Def Technol, State Key Lab High Performance Comp, Coll Syst Engn, Changsha 410073, Hunan, Peoples R China.
C3 National University of Defense Technology - China; University of
   Illinois System; University of Illinois Chicago; University of Illinois
   Chicago Hospital; National University of Defense Technology - China
RP Wang, J (corresponding author), Natl Univ Def Technol, Coll Syst Engn, Changsha 410073, Hunan, Peoples R China.
EM wangji@nudt.edu.cn; wdbao@nudt.edu.cn; lzheng21@uic.edu;
   xmzhu@nudt.edu.cn; psyu@uic.edu
RI Yu, Philip S/A-2815-2012
FU National Natural Science Foundation of China [61872378, 61572511,
   91648204]; Science Fund for Distinguished Young Scholars in Hunan
   Province [2018JJ1032]
FX This research was supported by the National Natural Science Foundation
   of China under grant nos. 61872378, 61572511, and no. 91648204, as well
   as the Science Fund for Distinguished Young Scholars in Hunan Province
   under grant no. 2018JJ1032.
CR Abadi Martin, 2016, TENSORFLOW LARGE SCA, V16, P265
   Backblaze, 2016, BACKBL HARD DRIV DAT
   Backblaze, 2018, BACKBL HARD DRIV DAT
   Backblaze, 2016, HARD DRIV REL REV 20
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Botezatu M, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P39, DOI 10.1145/2939672.2939699
   Cho K., 2014, ARXIV14061078
   Eckart B, 2008, I S MOD ANAL SIM COM, P85
   Elerath JG, 2004, P A REL MAI, P151, DOI 10.1109/RAMS.2004.1285439
   Fearnhead P, 2006, STAT COMPUT, V16, P203, DOI 10.1007/s11222-006-8450-8
   Goldszmidt M, 2012, DEPENDABILITY AND COMPUTER ENGINEERING: CONCEPTS FOR SOFTWARE-INTENSIVE SYSTEMS, P1, DOI 10.4018/978-1-60960-747-0.ch001
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Hahnloser RHR, 2000, NATURE, V405, P947, DOI 10.1038/35016072
   Hamerly G, 2001, P 18 INT C MACH LEAR, P202, DOI DOI 10.5555/645530.655825
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hochreiter S, 1998, INT J UNCERTAIN FUZZ, V6, P107, DOI 10.1142/S0218488598000094
   Hughes GF, 2002, IEEE T RELIAB, V51, P350, DOI 10.1109/TR.2002.802886
   Jean S, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P1
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Li J, 2017, RELIAB ENG SYST SAFE, V164, P55, DOI 10.1016/j.ress.2017.03.004
   Li J, 2016, SYM REL DIST SYST, P71, DOI [10.1109/SRDS.2016.17, 10.1109/SRDS.2016.019]
   Ma A, 2015, ACM T STORAGE, V11, DOI 10.1145/2820615
   Ma FL, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1903, DOI 10.1145/3097983.3098088
   Mandisoltani F, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P391
   Murray JF, 2005, J MACH LEARN RES, V6, P783
   Pang SA, 2016, IEEE IJCNN, P4850, DOI 10.1109/IJCNN.2016.7727837
   Pinheiro E, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 5TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES ( FAST '07), P17
   RIEDMILLER M, 1993, 1993 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS, VOLS 1-3, P586, DOI 10.1109/ICNN.1993.298623
   Rodriguez P, 1999, CONNECT SCI, V11, P5, DOI 10.1080/095400999116340
   Salehinejad H, 2017, TRAINING NEURAL NETW
   Sankar S, 2013, ACM T STORAGE, V9, DOI 10.1145/2491472.2491475
   Schroeder B, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 5TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES ( FAST '07), P1
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26, DOI DOI 10.1007/S12654-012-0173-1
   Tompson Jonathan, 2014, ARXIV14062984, DOI DOI 10.5555/2968826.2969027
   Wang Y, 2014, IEEE T IND INFORM, V10, P419, DOI 10.1109/TII.2013.2264060
   Wang Yu., 2011, 2011 PROGNOSTICS SYS, P1
   West M., 1997, Springer Series in Statistics, V2nd
   Williams RJ, 1989, NEURAL COMPUT, V1, P270, DOI 10.1162/neco.1989.1.2.270
   Xu C, 2016, IEEE T COMPUT, V65, P3502, DOI 10.1109/TC.2016.2538237
   Ying Z, 2010, 2010 INTERNATIONAL COLLOQUIUM ON COMPUTING, COMMUNICATION, CONTROL, AND MANAGEMENT (CCCM2010), VOL IV, P391
   Zhu B., 2013, IEEE S MASS STOR SYS, P1, DOI DOI 10.1109/MSST.2013.6558427
NR 44
TC 9
Z9 12
U1 1
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2019
VL 15
IS 3
AR 21
DI 10.1145/3340290
PG 26
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JT2DC
UT WOS:000500805400006
DA 2024-07-18
ER

PT J
AU Saad, MM
   Palmieri, R
   Ravindran, B
AF Saad, Mohamed M.
   Palmieri, Roberto
   Ravindran, Binoy
TI Lerna: Parallelizing Dependent Loops Using Speculation
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article; Proceedings Paper
CT 11th ACM International Systems and Storage Conference (SYSTOR)
CY JUN, 2018
CL Haifa, ISRAEL
SP ACM
DE Code parallelization; LLVM; transactions
ID NOREC
AB We present Lerna, an end-to-end tool that automatically and transparently detects and extracts parallelism from data-dependent sequential loops. Lerna uses speculation combined with a set of techniques including code profiling, dependency analysis, instrumentation, and adaptive execution. Speculation is needed to avoid conservative actions and detect actual conflicts. Lerna targets applications that are hard-to-parallelize due to data dependency. Our experimental study involves the parallelization of 13 applications with data dependencies. Results on a 24-core machine show an average of 2.7x speedup for micro-benchmarks and 2.5x for the macro-benchmarks.
C1 [Saad, Mohamed M.] Alexandria Univ, Alexandria 21526, Egypt.
   [Palmieri, Roberto] Lehigh Univ, Bethlehem, PA 18015 USA.
   [Ravindran, Binoy] Virginia Tech, Blacksburg, VA 24061 USA.
C3 Egyptian Knowledge Bank (EKB); Alexandria University; Lehigh University;
   Virginia Polytechnic Institute & State University
RP Saad, MM (corresponding author), Alexandria Univ, Alexandria 21526, Egypt.
EM msaad@alexu.edu.eg; palmieri@lehigh.edu; binoy@vt.edu
RI ; Ravindran, Binoy/C-6016-2017
OI Palmieri, Roberto/0000-0002-1530-4088; Ravindran,
   Binoy/0000-0002-8663-739X
FU Air Force Office of Scientific Research [FA955017-1-0367]
FX This work, developed as part of the HydraVM project at Virginia Tech, is
   supported in part by Air Force Office of Scientific Research under
   grants FA9550-14-1-0187 and FA9550-16-1-0371. The authors gratefully
   acknowledge the highly insightful feedback from scientists at the US
   Naval Surface Warfare Center Dahlgren Division in developing the HydraVM
   project. This material is based upon work supported by the Air Force
   Office of Scientific Research under award number FA955017-1-0367.
CR AB MySQL, 1995, MYSQL WORLDS MOST PO
   Abadi M, 2009, ACM SIGPLAN NOTICES, V44, P185, DOI 10.1145/1594835.1504203
   Aho AlfredV., 1977, Principles of Compiler Design
   [Anonymous], 2012, ARCH INSTR SET EXT P
   Arnold M, 2000, ACM SIGPLAN NOTICES, V35, P47, DOI 10.1145/354222.353175
   Bader DA, 2005, LECT NOTES COMPUT SC, V3769, P465
   Barreto Joao, 2012, Middleware 2012. ACM/IFIP/USENIX 13th International Middleware Conference. Proceedings, P187, DOI 10.1007/978-3-642-35170-9_10
   Bienia C, 2008, PACT'08: PROCEEDINGS OF THE SEVENTEENTH INTERNATIONAL CONFERENCE ON PARALLEL ARCHITECTURES AND COMPILATION TECHNIQUES, P72, DOI 10.1145/1454115.1454128
   Calciu Irina, 2014, P 9 WORKSH T COMP TR
   Chan B., 2002, UMT BENCHMARK CODE
   Chen M, 2003, INT SYM CODE GENER, P301, DOI 10.1109/CGO.2003.1191554
   Chen MK, 2003, CONF PROC INT SYMP C, P434, DOI 10.1109/ISCA.2003.1207020
   Cheng Doreen Y., 1993, RND93005 NASA AM RES
   Chowdhury RA, 2004, LECT NOTES COMPUT SC, V2985, P24
   Chung J., 2008, P IEEE INT S WORKL C
   Dalessandro L, 2012, INT CONFER PARA, P171
   Dalessandro L, 2011, ACM SIGPLAN NOTICES, V46, P39, DOI 10.1145/1961296.1950373
   Dalessandro L, 2010, ACM SIGPLAN NOTICES, V45, P67, DOI 10.1145/1837853.1693464
   Dang Francis, 2001, P INT PAR DISTR PROC
   DeVuyst M., 2011, 6 INT C HIGH PERF EM, P127, DOI DOI 10.1145/1944862.1944882
   DICE D, 2006, P 20 INT S DISTR COM
   DIPASQUALE N, 2005, MASPLAS 05
   Felber P, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901346
   Felber P, 2008, PPOPP'08: PROCEEDINGS OF THE 2008 ACM SIGPLAN SYMPOSIUM ON PRINCIPLES AND PRACTICE OF PARALLEL PROGRAMMING, P237, DOI 10.1145/1345206.1345241
   Gonzalez-Mesa MA, 2014, ACM T ARCHIT CODE OP, V11, P23, DOI 10.1145/2633048
   Grosser T., 2011, P 1 INT WORKSH POL C, V2011, P1
   Hammond L, 1998, ACM SIGPLAN NOTICES, V33, P58, DOI 10.1145/291006.291020
   Harris T., 2010, Synthesis Lectures on Computer Architecture, V5, P1
   HEATH D, 1992, ECONOMETRICA, V60, P77, DOI 10.2307/2951677
   Huang SS, 2008, LECT NOTES COMPUT SC, V5142, P76, DOI 10.1007/978-3-540-70592-5_5
   Issa Shady, 2017, P 31 INT S DISTR COM, V91, DOI 10.4230/LIPIcs.DISC.2017.28
   Karjanto Natanael, 2015, ARXIV150403074
   Kasahara H., 2001, Languages and Compilers for Parallel Computing, P189, DOI [10.1007/3-540-45574-4_13, DOI 10.1007/3-540-45574-4_13]
   Kim S.-H., 2012, P 2012 BELIV WORKSHO, DOI [10.1145/2442576.2442591, DOI 10.1145/2442576.2442591]
   Lattner C, 2004, INT SYM CODE GENER, P75, DOI 10.1109/CGO.2004.1281665
   Lim A. W., 1997, Conference Record of POPL '97: The 24th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, P201, DOI 10.1145/263699.263719
   Matveev A, 2015, ACM SIGPLAN NOTICES, V50, P59, DOI [10.1145/10.1145/2694344.2694393, 10.1145/2775054.2694393]
   Mehrara M, 2009, PLDI'09 PROCEEDINGS OF THE 2009 ACM SIGPLAN CONFERENCE ON PROGRAMMING LANGUAGE DESIGN AND IMPLEMENTATION, P166, DOI 10.1145/1542476.1542495
   MINH CC, 2007, P 34 ANN INT S COMP
   Mohamedin M, 2017, IEEE T PARALL DISTR, V28, P2299, DOI 10.1109/TPDS.2017.2668415
   Muller M., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P154
   MULLER S, 2014, P INT C HARM QUAL PO, P645
   Naeem NA, 2009, ISMM'09: PROCEEDINGS OF THE 2009 ACM SIGPLAN INTERNATIONAL SYMPOSIUM ON MEMORY MANAGEMENT, P79
   Pottenger William Morton, 1995, THESIS
   Raman A, 2010, ASPLOS XV: FIFTEENTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS, P65
   Ramaseshan Ravi, 2008, WORKSH PROGR ISS MUL, V12
   Reinders J., 2013, Transactional synchronization in Haswell
   Rossbach CJ, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P49, DOI 10.1145/2517349.2522715
   Ruan WJ, 2014, ACM T ARCHIT CODE OP, V11, DOI 10.1145/2688904
   Rugina R, 1999, ACM SIGPLAN NOTICES, V34, P72, DOI 10.1145/329366.301111
   Saad M., 2016, THESIS
   Saad Mohamed M., 2019, P 24 ACM SIGPLAN S P
   Saad Mohamed M., 2016, SPAA 16, P109
   Saad Mohamed M., 2012, P 4 USENIX WORKSH HO
   Saha B, 2006, INT SYMP MICROARCH, P185
   Salz Joel H., 1991, P 1991 INT C PAR PRO, P174
   Steffan J. Greggory, 2000, SCALABLE APPROACH TH, V28
   Streit Kevin, 2013, P 3 INT WORKSH AD SE, P7
   Vandierendonck H, 2010, PACT 2010: PROCEEDINGS OF THE NINETEENTH INTERNATIONAL CONFERENCE ON PARALLEL ARCHITECTURES AND COMPILATION TECHNIQUES, P389, DOI 10.1145/1854273.1854322
   von Koch TJKE, 2013, ACM SIGPLAN NOTICES, V48, P13, DOI 10.1145/2517326.2451518
   von Praun C, 2008, PPOPP'08: PROCEEDINGS OF THE 2008 ACM SIGPLAN SYMPOSIUM ON PRINCIPLES AND PRACTICE OF PARALLEL PROGRAMMING, P185, DOI 10.1145/1345206.1345234
   Waterland A, 2014, ACM SIGPLAN NOTICES, V49, P575, DOI 10.1145/2541940.2541985
   Wei Liu, 2006, Proceedings of the 2006 ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming PPoPP'06, P158, DOI 10.1145/1122971.1122997
   1992, INT J PARALLEL PROGR, V21, P313
   1999, IEEE T PARALLEL DIST, V10, P160
   INT J PARALLEL PROGR, V28, P537
   1999, IEEE T COMPUT, V48, P866
   1974, COMMUN ACM, V17, P83
NR 68
TC 1
Z9 1
U1 0
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD APR
PY 2019
VL 15
IS 1
SI SI
AR 6
DI 10.1145/3310368
PG 24
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA HU9GE
UT WOS:000465601900006
OA Bronze
DA 2024-07-18
ER

PT J
AU Sun, Z
   Kuenning, G
   Mandal, S
   Shilane, P
   Tarasov, V
   Xiao, N
   Zadok, E
AF Sun, Zhen Jason
   Kuenning, Geoff
   Mandal, Sonam
   Shilane, Philip
   Tarasov, Vasily
   Xiao, Nong
   Zadok, Erez
TI Cluster and Single-Node Analysis of Long-Term Deduplication Patterns
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE User study; large data set; data routing algorithms
AB Deduplication has become essential in disk-based backup systems, but there have been few long-term studies of backup workloads. Most past studies either were of a small static snapshot or covered only a short period that was not representative of how a backup system evolves over time. For this article, we first collected 21 months of data from a shared user file system; 33 users and over 4,000 snapshots are covered. We then analyzed the dataset, examining a variety of essential characteristics across two dimensions: single-node deduplication and cluster deduplication. For single-node deduplication analysis, our primary focus was individual-user data. Despite apparently similar roles and behavior among all of our users, we found significant differences in their deduplication ratios. Moreover, the data that some users share with others had a much higher deduplication ratio than average. For cluster deduplication analysis, we implemented seven published data-routing algorithms and created a detailed comparison of their performance with respect to deduplication ratio, load distribution, and communication overhead. We found that per-file routing achieves a higher deduplication ratio than routing by super-chunk (multiple consecutive chunks), but it also leads to high data skew (imbalance of space usage across nodes). We also found that large chunking sizes are better for cluster deduplication, as they significantly reduce data-routing overhead, while their negative impact on deduplication ratios is small and acceptable. We draw interesting conclusions from both single-node and cluster deduplication analysis and make recommendations for future deduplication systems design.
C1 [Sun, Zhen Jason; Xiao, Nong] Natl Univ Def Technol, Sch Comp Sci, Changsha 410073, Hunan, Peoples R China.
   [Kuenning, Geoff] Harvey Mudd Coll, Dept Comp Sci, 301 Platt Blvd, Claremont, CA 91711 USA.
   [Mandal, Sonam] SUNY Stony Brook, New Comp Sci 336, Stony Brook, NY 11794 USA.
   [Shilane, Philip] Dell EMC, Hopkinton, MA USA.
   [Tarasov, Vasily] IBM Res, Yorktown Hts, NY USA.
   [Zadok, Erez] SUNY Stony Brook, New Comp Sci 349, Stony Brook, NY 11794 USA.
   [Shilane, Philip] 131 Pheasant Lane, Newtown, PA 18940 USA.
   [Tarasov, Vasily] IBM Res Almaden, 650 Harry Rd, San Jose, CA 95120 USA.
C3 National University of Defense Technology - China; Claremont Colleges;
   Harvey Mudd College; State University of New York (SUNY) System; State
   University of New York (SUNY) Stony Brook; Dell Incorporated; Dell EMC;
   International Business Machines (IBM); State University of New York
   (SUNY) System; State University of New York (SUNY) Stony Brook;
   International Business Machines (IBM)
RP Sun, Z (corresponding author), Natl Univ Def Technol, Sch Comp Sci, Changsha 410073, Hunan, Peoples R China.
EM tcsunzhen@gmail.com; geoff@cs.hmc.edu; sonam.dp42@gmail.com;
   Philip.Shilane@dell.com; vtarasov@us.ibm.com; xiao-n@vip.sina.com;
   ezk@cs.stonybrook
RI Shilane, Philip/V-9134-2019
OI Kuenning, Geoff/0000-0002-3882-2072; Shilane, Philip/0000-0003-1235-0502
FU Dell-EMC; NetApp; IBM; NSF [CNS-1251137, CNS-1302246, CNS-1305360,
   CNS-1622832]; ONR [12055763]; National Natural Science Foundation of
   China [61433019, U1435217]; China 863 program [2015AA015305]
FX This work was made possible in part thanks to Dell-EMC, NetApp, and IBM
   support; NSF awards CNS-1251137, CNS-1302246, CNS-1305360, and
   CNS-1622832; ONR award 12055763; the National Natural Science Foundation
   of China under Grants No. 61433019 and U1435217; and China 863 program
   grant 2015AA015305.
CR Amvrosiadis George., 2015, Proc. of USENIX ATC, P151
   [Anonymous], 2012, 2012 USENIX ANN TECH
   [Anonymous], 2012, P 75 IEEE VEH TECHN, DOI DOI 10.1109/VETECS.2012.6240325
   [Anonymous], P ISR EXP SYST C SYS
   [Anonymous], 2015, HPL201577
   [Anonymous], P IEEE INT S MOD AN
   [Anonymous], CISC VIS NETW IND GL
   [Anonymous], 2011, P 9 USENIX C FIL STO
   [Anonymous], P 10 USENIX C FIL ST
   [Anonymous], INT J ADV COMPUT TEC
   [Anonymous], 2009, SYSTOR 09
   [Anonymous], 2014, P 30 S MASS STORAGE, DOI DOI 10.1109/MSST.2014.6855555
   [Anonymous], 2008, FAST
   [Anonymous], 2016, J SENSOR
   [Anonymous], T STORAGE
   [Anonymous], 2010, 40 INT C COMP IND EN
   [Anonymous], 2015, 2015 IEEE 81 VEH TEC, DOI DOI 10.1109/VTCSPRING.2015.7145948
   [Anonymous], 2009, 7 USENIX C FIL STOR
   [Anonymous], FSLHOMES DAT SET TOO
   [Anonymous], P ANN TECHN C
   Bhagwat D., 2009, P IEEE INT S MODELIN, P1
   Cao Z, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P329
   Debnath Biplob., 2010, P USENIX ANN TECHNIC, P16
   Douglis Fred., 2011, Proceedings of the 25th International Conference on Large Installation System Administration, P13
   Frey D., 2012, Proc. of the ACM Symposium on Cloud Computing (SoCC), P17
   Fu YJ, 2013, J COMPUT SCI TECH-CH, V28, P1012, DOI 10.1007/s11390-013-1394-5
   Guo Fanglu., 2011, Proceedings of the 2011 USENIX Conference on USENIX Annual Technical Conference, USENIXATC'11, P25
   Jin K., 2009, P SYSTOR 2009 ISR EX, p7:1, DOI DOI 10.1145/1534530.1534540
   Koller R, 2010, ACM T STORAGE, V6, DOI 10.1145/1837915.1837921
   Li C., 2015, P USENIX ANN TECH C, P111
   Lillibridge M., 2013, FAST
   Lin X., 2015, Proceedings of the 7th USENIX Conference on Hot Topics in Storage and File Systems, HotStorage'15, P11
   Meister D., 2012, IEEE P INT C HIGH PE, P1, DOI 10.1109/SC.2012.14
   Meister D., 2013, 11 USENIX C FIL STOR, P175
   Meyer DT, 2012, ACM T STORAGE, V7, DOI 10.1145/2078861.2078864
   Park Nohhyun., 2010, Workload Characterization (IISWC), 2010 IEEE International Symposium on, P1
   Srinivasan Kiran, 2012, P 10 USENIX C FIL ST
   Tarasov V., 2012, 2012 F USENIX G ANN, P261
   Ungureanu C., 2010, FAST, P225
   Vaughn C, 2015, I S MOD ANAL SIM COM, P208, DOI 10.1109/MASCOTS.2015.40
   Villars R.L., 2011, IDC WHITE PAPER
   Wei J.S., 2010, PROC 2010 IEEE 26 S, P1
   Xia Wen., 2011, Proceedings of the USENIX Annual Technical Conference, USENIXATC'11, P26
   Yinjin Fu, 2012, Middleware 2012. ACM/IFIP/USENIX 13th International Middleware Conference. Proceedings, P354, DOI 10.1007/978-3-642-35170-9_18
   Zhou Y., 2015, Mass Storage Systems and Technologies (MSST), 2015 31st Symposium on, P1
NR 45
TC 10
Z9 10
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAY
PY 2018
VL 14
IS 2
AR 13
DI 10.1145/3183890
PG 27
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GI6TM
UT WOS:000434635800002
OA Bronze
DA 2024-07-18
ER

PT J
AU Yadgar, G
   Yaakobi, E
   Margaglia, F
   Li, Y
   Yucovich, A
   Bundak, N
   Gilon, L
   Yakovi, N
   Schuster, A
   Brinkmann, A
AF Yadgar, Gala
   Yaakobi, Eitan
   Margaglia, Fabio
   Li, Yue
   Yucovich, Alexander
   Bundak, Nachum
   Gilon, Lior
   Yakovi, Nir
   Schuster, Assaf
   Brinkmann, Andre
TI An Analysis of Flash Page Reuse With WOM Codes
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE WOM codes; NAND flash; flash translation layer; SSD; offline analysis
ID WRITE-ONCE MEMORIES; PERFORMANCE
AB Flash memory is prevalent in modern servers and devices. Coupled with the scaling down of flash technology, the popularity of flash memory motivates the search for methods to increase flash reliability and lifetime. Erasures are the dominant cause of flash cell wear, but reducing them is challenging because flash is a write-once medium-memory cells must be erased prior to writing.
   An approach that has recently received considerable attention relies on write-once memory (WOM) codes, designed to accommodate additional writes on write-once media. However, the techniques proposed for reusing flash pages with WOM codes are limited in their scope. Many focus on the coding theory alone, whereas others suggest FTL designs that are application specific, or not applicable due to their complexity, overheads, or specific constraints of multilevel cell (MLC) flash.
   This work is the first that addresses all aspects of page reuse within an end-to-end analysis of a general-purpose FTL on MLC flash. We use a hardware evaluation setup to directly measure the short-and long-term effects of page reuse on SSD durability and energy consumption, and show that FTL design must explicitly take them into account. We then provide a detailed analytical model for deriving the optimal garbage collection policy for such FTL designs, and for predicting the benefit from reuse on realistic hardware and workload characteristics.
C1 [Yadgar, Gala; Yaakobi, Eitan; Yucovich, Alexander; Bundak, Nachum; Gilon, Lior; Yakovi, Nir; Schuster, Assaf] Technion, Comp Sci Dept, Haifa, Israel.
   [Margaglia, Fabio] Pure Storage, Mountain View, CA USA.
   [Li, Yue] CALTECH, Pasadena, CA 91125 USA.
   [Brinkmann, Andre] Johannes Gutenberg Univ Mainz, Mainz, Germany.
C3 Technion Israel Institute of Technology; Pure Storage; California
   Institute of Technology; Johannes Gutenberg University of Mainz
RP Yadgar, G (corresponding author), Technion, Comp Sci Dept, Haifa, Israel.
EM gala@cs.technion.ac.il; yaakobi@cs.technion.ac.il; margagl@uni-mainz.de;
   yli@caltech.edu; yucovich@technion.ac.il; nb@campus.technion.ac.il;
   sglior@campus.technion.ac.il; nir.yakovi@campus.technion.ac.il;
   assaf@cs.technion.ac.il; brinkman@uni-mainz.de
RI Brinkmann, André/H-9888-2016
OI Yadgar, Gala/0000-0003-2701-0260
FU United States-Israel Binational Science Foundation (BSF) [2010075]; NSF
   [CCF-1218005]; Israel Science Foundation (ISF) grant [1624/14]; EU Marie
   Curie Initial Training Network SCALUS grant [238808]; German-Israeli
   Foundation for Scientific Research and Development (GIF) grant
   [I-1356-407.6/2016]
FX Thiswork was supported in part by United States-Israel Binational
   Science Foundation (BSF) grant 2010075, NSF grant CCF-1218005, Israel
   Science Foundation (ISF) grant 1624/14, EU Marie Curie Initial Training
   Network SCALUS grant 238808, and German-Israeli Foundation for
   Scientific Research and Development (GIF) grant I-1356-407.6/2016.
CR Agrawal Nitin, 2008, 2008 USENIX ANN TECH
   [Anonymous], 2012, P 10 USENIX C FIL ST
   [Anonymous], 2011, P FAST 2
   [Anonymous], 2009, MICRO
   [Anonymous], 2008, P C FIL STOR TECHN F
   [Anonymous], 2010, P 8 USENIX C FIL STO
   Berman A, 2013, P IEEE INT S INF THE
   Burshtein D, 2013, IEEE T INFORM THEORY, V59, P5088, DOI 10.1109/TIT.2013.2255732
   Burshtein David, 2015, P IEEE INT S INF THE
   Bux W, 2010, PERFORM EVALUATION, V67, P1172, DOI 10.1016/j.peva.2010.07.003
   Cai Y., 2013, P 31 IEEE INT C COMP
   Chiao ML, 2011, IEEE T COMPUT, V60, P753, DOI 10.1109/TC.2011.67
   COHEN GD, 1986, IEEE T INFORM THEORY, V32, P697, DOI 10.1109/TIT.1986.1057221
   Colgrove J., 2015, P ACM SIGMOD INT C M
   Desnoyers P, 2014, ACM T STORAGE, V10, DOI 10.1145/2577384
   Desnoyers Peter, 2013, P 5 USENIX WORKSH HO
   Gad E. En, 2015, P IEEE INT S INF THE
   Hu X. -Y., 2010, RZ3771 IBM
   Huang Sai, 2013, P IEEE 29 S MASS STO
   Im Jae-Woo, 2015, P IEEE INT SOL STAT
   Im S, 2010, J SYST ARCHITECT, V56, P641, DOI 10.1016/j.sysarc.2010.09.005
   Jacobvitz A. N., 2012, P 50 ANN ALL C COMM
   Jagmohan A., 2010, P 26 IEEE S MASS STO
   Jimenez X., 2014, FAST
   Kaiser Jurgen, 2013, P 6 INT SYST STOR C
   Kgil Taeho, 2008, P 35 ANN INT S COMP
   Luojie X., 2012, P IEEE GLOB COMM C G
   Margaglia F, 2016, P 14 USENIX C FIL ST
   Margaglia Fabio, 2015, P IEEE 31 S MASS STO
   Mielke N., 2008, P IEEE INT REL PHYS
   Mohan V, 2013, IEEE T COMPUT AID D, V32, P1031, DOI 10.1109/TCAD.2013.2249557
   Narayanan D., 2008, ACM T STORAGE, V4, P3
   Odeh S., 2014, P IEEE 30 S MASS STO, DOI [10.1109/MSST.2014.6855549, http://dx.doi.org/10.1109/MSST.2014.6855549, DOI 10.1109/MSST.2014.6855549]
   Oh Yongseok., 2012, FAST, V12
   OpenSSD, 2015, JASM OPENSSD PLATF
   Park H., 2015, P IEEE 31 S MASS STO
   Park KT, 2008, IEEE J SOLID-ST CIRC, V43, P919, DOI 10.1109/JSSC.2008.917558
   RIVEST RL, 1982, INFORM CONTROL, V55, P1, DOI 10.1016/S0019-9958(82)90344-8
   Salajegheh Mastooreh, 2011, FAST
   Saxena  M., 2012, P 7 ACM EUR C COMP S
   Shpilka A, 2014, IEEE T INFORM THEORY, V60, P1481, DOI 10.1109/TIT.2013.2294464
   Shpilka A, 2013, IEEE T INFORM THEORY, V59, P4520, DOI 10.1109/TIT.2013.2251455
   Siglead, 2014, NAND FLASH MEM TEST
   Smith K, 2013, EDN Network
   SNIA, 2014, SNIA IOTTA
   Stoica R, 2013, PROC VLDB ENDOW, V6, P733, DOI 10.14778/2536360.2536372
   Van Houdt B, 2014, PERFORM EVALUATION, V82, P1, DOI 10.1016/j.peva.2014.08.003
   Yaakobi E, 2012, IEEE T INFORM THEORY, V58, P5985, DOI 10.1109/TIT.2012.2200291
   Yaakobi Eitan, 2015, P IEEE INT S INF THE
   Yaakobi Eitan, 2012, P INT C COMP NETW CO
   Yaakobi Eitan, 2010, P IEEE GLOB WORKSH G
   Yadgar G., 2015, USENIX C FIL STOR TE, P257
   Yadgar Gala, 2016, CS201602
   Yadgar Gala, 2016, P 8 USENIX C HOT TOP
   Yadgar Gala, 2015, P 7 USENIX C HOT TOP
   Yang J, 2013, SCI WORLD J, DOI 10.1155/2013/812469
   Zhao K., 2013, P 11 USENIX C FIL ST
NR 57
TC 7
Z9 7
U1 1
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD APR
PY 2018
VL 14
IS 1
SI SI
AR 10
DI 10.1145/3177886
PG 39
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GH5ZN
UT WOS:000433517600010
DA 2024-07-18
ER

PT J
AU Chen, HB
   Zhang, H
   Dong, MK
   Wang, ZG
   Xia, YB
   Guan, HB
   Zang, BY
AF Chen, Haibo
   Zhang, Heng
   Dong, Mingkai
   Wang, Zhaoguo
   Xia, Yubin
   Guan, Haibing
   Zang, Binyu
TI Efficient and Available In-Memory KV-Store with Hybrid Erasure Coding
   and Replication
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Primary-backup replication; erasure coding; KV-store; transactions
AB In-memory key/value store (KV-store) is a key building block for many systems like databases and large websites. Two key requirements for such systems are efficiency and availability, which demand a KV-store to continuously handle millions of requests per second. A common approach to availability is using replication, such as primary-backup (PBR), which, however, requires M + 1 times memory to tolerate M failures. This renders scarce memory unable to handle useful user jobs.
   This article makes the first case of building highly available in-memory KV-store by integrating erasure coding to achieve memory efficiency, while not notably degrading performance. A main challenge is that an in-memory KV-store has much scattered metadata. A single KV put may cause excessive coding operations and parity updates due to excessive small updates to metadata. Our approach, namely Cocytus, addresses this challenge by using a hybrid scheme that leverages PBR for small-sized and scattered data (e.g., metadata and key), while only applying erasure coding to relatively large data (e.g., value). To mitigate well-known issues like lengthy recovery of erasure coding, Cocytus uses an online recovery scheme by leveraging the replicated metadata information to continuously serve KV requests. To further demonstrate the usefulness of Cocytus, we have built a transaction layer by using Cocytus as a fast and reliable storage layer to store database records and transaction logs. We have integrated the design of Cocytus to Memcached and extend it to support inmemory transactions. Evaluation using YCSB with different KV configurations shows that Cocytus incurs low overhead for latency and throughput, can tolerate node failures with fast online recovery, while saving 33% to 46% memory compared to PBR when tolerating two failures. A further evaluation using the SmallBank OLTP benchmark shows that in-memory transactions can run atop Cocytus with high throughput, low latency, and low abort rate and recover fast from consecutive failures.
C1 [Chen, Haibo; Zhang, Heng; Dong, Mingkai; Wang, Zhaoguo; Xia, Yubin; Guan, Haibing; Zang, Binyu] Shanghai Jiao Tong Univ, Inst Parallel & Distributed Syst, Shanghai Key Lab Scalable Comp & Syst, Room 3402,Software Bldg,800 Dongchuan Rd, Shanghai 200240, Peoples R China.
C3 Shanghai Jiao Tong University
RP Zang, BY (corresponding author), Shanghai Jiao Tong Univ, Inst Parallel & Distributed Syst, Shanghai Key Lab Scalable Comp & Syst, Room 3402,Software Bldg,800 Dongchuan Rd, Shanghai 200240, Peoples R China.
EM haibochen@sjtu.edu.cn; shinedark@sjtu.edu.cn; mingkaidong@gmail.com;
   tigerwang1986@gmail.com; xiayubin@sjtu.edu.cn; hbguan@sjtu.edu.cn;
   byzang@sjtu.edu.cn
RI guan, haibing/G-8142-2011
OI guan, haibing/0000-0002-4714-7400
FU National Key Research & Development Program of China [2016YFB1000104];
   China National Natural Science Foundation [61572314, 61525204,
   61672345]; Top-notch Youth Talents Program of China, Shanghai Science
   and Technology Development Fund [14511100902]; Zhangjiang Hi-Tech
   program [201501-YP-B108-012]
FX This work is supported in part by National Key Research & Development
   Program of China (No. 2016YFB1000104), China National Natural Science
   Foundation (No. 61572314, 61525204 and 61672345), the Top-notch Youth
   Talents Program of China, Shanghai Science and Technology Development
   Fund (No. 14511100902), Zhangjiang Hi-Tech program (No.
   201501-YP-B108-012).
CR Alomari M, 2008, PROC INT CONF DATA, P576, DOI 10.1109/ICDE.2008.4497466
   [Anonymous], 2014, P 9 EUR C COMP SYST
   [Anonymous], 2011, FAST
   [Anonymous], 2004, Linux journal
   [Anonymous], 2013, P 2013 ACM SIGMOD IN, DOI DOI 10.1145/2463676.2463710
   [Anonymous], 2012, IEEE Data Eng. Bull
   [Anonymous], 2012, P ACM SIGMOD INT C M, DOI DOI 10.1145/2213836.2213838
   [Anonymous], 2014, P ACM EUROSYS
   [Anonymous], 2013, IEEE Data Eng. Bull.
   [Anonymous], 2012, P 10 USENIX C OP SYS
   [Anonymous], 2013, NSDI
   Atikoglu Berk, 2012, Performance Evaluation Review, V40, P53, DOI 10.1145/2318857.2254766
   Bolosky William J., 2011, P C NETW SYST DES IM
   Bressoud TC, 1996, ACM T COMPUT SYST, V14, P80, DOI 10.1145/225535.225538
   Bu YY, 2013, PROCEEDINGS OF THE ACM SIGPLAN INTERNATIONAL SYMPOSIUM ON MEMORY MANAGEMENT (ISMM '13), P119
   Budhiraja N., 1993, DISTRIBUTED SYSTEMS, V2, P199
   Cahill MJ, 2009, ACM T DATABASE SYST, V34, DOI 10.1145/1620585.1620587
   CHANDY KM, 1985, ACM T COMPUT SYST, V3, P63, DOI 10.1145/214451.214456
   Chen SM, 2010, SIGMOD RECORD, V39, P5, DOI 10.1145/1942776.1942778
   CHEN YZ, 2016, P 11 EUR C COMP SYST, V53, P26
   Clement A, 2009, SOSP'09: PROCEEDINGS OF THE TWENTY-SECOND ACM SIGOPS SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P277
   Coburn J, 2011, ACM SIGPLAN NOTICES, V46, P105, DOI [10.1145/1961295.1950380, 10.1145/1961296.1950380]
   Cooper Brian F., 2010, P 1 ACM S CLOUD COMP, P143, DOI [DOI 10.1145/1807128.1807152, 10.1145/1807128.1807152]
   Corbett JC, 2013, ACM T COMPUT SYST, V31, DOI 10.1145/2491245
   Difallah DE, 2013, PROC VLDB ENDOW, V7, P277, DOI 10.14778/2732240.2732246
   Dragojevic A, 2015, SOSP'15: PROCEEDINGS OF THE TWENTY-FIFTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P54, DOI 10.1145/2815400.2815425
   Egghe L, 2005, J AM SOC INF SCI TEC, V56, P935, DOI 10.1002/asi.20186
   Goel A, 2014, SIGMOD'14: PROCEEDINGS OF THE 2014 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P541, DOI 10.1145/2588555.2595642
   Huang Cheng, 2012, P USENIX ATC
   KALIA A, 2014, P 2014 ACM C SIGC, V44, P295, DOI DOI 10.1145/2619239.2626299
   Lai Chunbo., 2015, IEEE 31 S MASS STORA, P1
   Lamport L., 2001, ACM SIGACT NEWS, V32
   Liu Ran, 2014, P USENIX ANN TECHN C, P219
   Lu LY, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P133
   Mitchell Christopher, 2013, 2013 USENIX ANN TECH
   Muralidhar S., 2014, P 11 USENIX C OP SYS, P383
   Nishtala R., 2013, Proceedings of the 10th USENIX Conference on Networked Systems Design and Implementation, NSDI '13, Berkeley, CA, USA, P385
   Ongaro D, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P29
   PLANK J., 2013, UTCS13703
   Plank J. S., 2008, CS08627 U TENN
   Rashmi K., 2015, USENIX C FIL STOR TE, P81
   Rashmi K. V., 2016, P C OP SYST DES IMPL
   Rashmi K. V., 2013, P C USENIX HOTSTORAG
   REED IS, 1960, J SOC IND APPL MATH, V8, P300, DOI 10.1137/0108018
   Rizzo L., 1997, Computer Communication Review, V27, P24, DOI 10.1145/263876.263881
   Sathiamoorthy M, 2013, PROC VLDB ENDOW, V6, P325, DOI 10.14778/2535573.2488339
   Shah NB, 2012, IEEE T INFORM THEORY, V58, P1837, DOI 10.1109/TIT.2011.2173792
   Silberstein Mark., 2014, Proceedings of International Conference on Systems and Storage, P1
   Stuedi Patrick., 2012, 2012 USENIX Annual Technical Conference, Boston, MA, USA, June 13-15, 2012
   Twitter Inc, 2012, TWEMC IS TWITT MEMC
   van Renesse R, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE SIXTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDE '04), P91
   VikingTechnology, 2014, ARXCIS NV TM NONV DI
   Wang P, 2014, I C DEPEND SYS NETWO, P562, DOI 10.1109/DSN.2014.58
   Wang Y., 2012, Proceedings of the 2012 USENIX conference on Annual Technical Conference, P38
   Wei XD, 2015, SOSP'15: PROCEEDINGS OF THE TWENTY-FIFTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P87, DOI 10.1145/2815400.2815419
   Welch B, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P17
   Yang Jun, 2015, P 13 USENIX C FIL ST, P167
   Yin J., 2003, P 19 ACM S OP SYST P, P253, DOI [10.1145/945445.945470, DOI 10.1145/945445.945470]
   Zawodny J., 2009, LINUX MAG, V79
   Zhang H, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P167
   Zhang YC, 2015, PROCEEDINGS OF 2015 INTERNATIONAL CONFERENCE ON ORANGE TECHNOLOGIES (ICOT), P3
NR 61
TC 28
Z9 31
U1 0
U2 16
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD OCT
PY 2017
VL 13
IS 3
SI SI
AR 25
DI 10.1145/3129900
PG 30
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FL6DV
UT WOS:000414336300008
DA 2024-07-18
ER

PT J
AU Agrawal, N
   Arulraj, L
   Arpaci-Dusseau, AC
   Arpaci-Dusseau, RH
AF Agrawal, Nitin
   Arulraj, Leo
   Arpaci-Dusseau, Andrea C.
   Arpaci-Dusseau, Remzi H.
TI Emulating Goliath Storage Systems with David
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Performance; Measurement; Storage systems; file systems;
   emulation; scalability
AB Benchmarking file and storage systems on large file-system images is important, but difficult and often infeasible. Typically, running benchmarks on such large disk setups is a frequent source of frustration for file-system evaluators; the scale alone acts as a strong deterrent against using larger, albeit realistic, benchmarks. To address this problem, we develop David: a system that makes it practical to run large benchmarks using modest amount of storage or memory capacities readily available on most computers. David creates a "compressed" version of the original file-system image by omitting all file data and laying out metadata more efficiently; an online storage model determines the runtime of the benchmark workload on the original uncompressed image. David works under any file system, as demonstrated in this article with ext3 and btrfs. We find that David reduces storage requirements by orders of magnitude; David is able to emulate a 1-TB target workload using only an 80 GB available disk, while still modeling the actual runtime accurately. David can also emulate newer or faster devices, for example, we show how David can effectively emulate a multidisk RAID using a limited amount of memory.
C1 [Arulraj, Leo; Arpaci-Dusseau, Andrea C.; Arpaci-Dusseau, Remzi H.] Univ Wisconsin Madison, Madison, WI USA.
C3 University of Wisconsin System; University of Wisconsin Madison
EM nitin@nec-labs.com; arulraj@cs.wisc.edu; dusseau@cs.wisc.edu;
   remzi@cs.wisc.edu
FU National Science Foundation [CCF-0621487, CNS-0509474, CNS-0834392,
   CCF-0811697, CCF-0937959]; Ne-tApp; Sun Microsystems; Google
FX This material is based on work supported by the National Science
   Foundation under the following grants: CCF-0621487, CNS-0509474,
   CNS-0834392, CCF-0811697, CCF-0811697, CCF-0937959, as well as by
   generous donations from Ne-tApp, Sun Microsystems, and Google. Any
   opinions, findings, and conclusions or recommendations expressed in this
   material are those of the authors and do not necessarily reflect the
   views of NSF or other institutions.
CR AGRAWAL N., 2009, P 7 C FIL STOR TECHN
   AGRAWAL N., 2011, P 9 C FIL STOR TECHN
   AGRAWAL N., 2007, P 5 USENIX S FIL STO
   AGRAWAL N., 2007, 5 YEAR STUDY FILE SY
   ANDERSON E., 2001, HPLSSP200104
   [Anonymous], P US ANN TECHN C USE
   [Anonymous], 1997, TR3022 NETW APPL INC
   BUCY JS, 2003, CMUCS03102
   Chen P. M., 1993, Performance Evaluation Review, V21, P1, DOI 10.1145/166962.166966
   Ganger GR, 1998, IEEE T COMPUT, V47, P667, DOI 10.1109/12.689646
   GRIFFIN J. L., 2002, P 1 USENIX S FIL STO
   GUPTA D., 2006, P 3 C NETW SYST DES
   Kaashoek M. F., 1997, Operating Systems Review, V31, P52, DOI 10.1145/269005.266644
   MAYFIELD J, 1995, PR CONF ART INT APPL, P87, DOI 10.1109/CAIA.1995.378786
   McDougall R., APPL LEVEL FILE SYST
   MILLER E. L., 1996, P 5 NASA GODD C MASS
   RIEDEL E., 2002, P 1 USENIX S FIL STO, P14
   Rinard M., 2004, P 6 S OP SYST DES IM
   RUEMMLER C, 1994, COMPUTER, V27, P17, DOI 10.1109/2.268881
   Sct S.T., 1998, Journaling the linux ext2fs filesystem
   SHRIVER E., 1997, THESIS NEW YORK
   Sivathanu M, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE 2ND USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST'03), P73
   STANDARD PERFORMANCE EVALUATION CORP, SPECMAIL2009 BENCHM
   Sweeney A., 1996, P USENIX ANN TECHN C
   TRAEGER A., 2009, USENIX FAST BIRDS FE
   WITTLE M, 1993, PROCEEDINGS OF THE SUMMER 1993 USENIX CONFERENCE, P111
   ZADOK E., 2008, FIL STOR SYST BENCHM
NR 27
TC 3
Z9 3
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JAN
PY 2012
VL 7
IS 4
AR 12
DI 10.1145/2078861.2078862
PG 21
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 990LP
UT WOS:000307632700001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU You, LL
   Pollack, KT
   Long, DDE
   Gopinath, K
AF You, Lawrence L.
   Pollack, Kristal T.
   Long, Darrell D. E.
   Gopinath, K.
TI PRESIDIO: A Framework for Efficient Archival Data Storage
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Algorithms; Design; Archival storage systems; data compression;
   content-addressable storage; CAS; progressive compression
ID DESIGN
AB The ever-increasing volume of archival data that needs to be reliably retained for long periods of time and the decreasing costs of disk storage, memory, and processing have motivated the design of low-cost, high-efficiency disk-based storage systems. However, managed disk storage is still expensive. To further lower the cost, redundancy can be eliminated with the use of interfile and intrafile data compression. However, it is not clear what the optimal strategy for compressing data is, given the diverse collections of data. To create a scalable archival storage system that efficiently stores diverse data, we present PRESIDIO, a framework that selects from different space-reduction efficent storage methods (ESMs) to detect similarity and reduce or eliminate redundancy when storing objects. In addition, the framework uses a virtualized content addressable store (VCAS) that hides from the user the complexity of knowing which space-efficient techniques are used, including chunk-based deduplication or delta compression. Storing and retrieving objects are polymorphic operations independent of their content-based address. A new technique, harmonic super-fingerprinting, is also used for obtaining successively more accurate (but also more costly) measures of similarity to identify the existing objects in a very large data set that are most similar to an incoming new object.
   The PRESIDIO design, when reported earlier, had comprehensively introduced for the first time the notion of deduplication, which is now being offered as a service in storage systems by major vendors. As an aid to the design of such systems, we evaluate and present various parameters that affect the efficiency of a storage system using empirical data.
C1 [Pollack, Kristal T.; Long, Darrell D. E.] Univ Calif Santa Cruz, Storage Syst Res Ctr, Santa Cruz, CA 95064 USA.
   [Gopinath, K.] Indian Inst Sci, Dept Comp Sci & Automat, Bangalore 560012, Karnataka, India.
C3 University of California System; University of California Santa Cruz;
   Indian Institute of Science (IISC) - Bangalore
RP You, LL (corresponding author), Google Inc, Mountain View, CA 94043 USA.
EM kristal@cs.ucsc.edu; gopi@csa.iisc.ernet.in
CR Adya A., 2002, P 5 S OP SYST DES IM
   Ajtai M, 2002, J ACM, V49, P318, DOI 10.1145/567112.567116
   ALVAREZ C., 2010, TR3505 NETAPP
   [Anonymous], 2003, WWW, DOI DOI 10.1145/775152.775246
   [Anonymous], P 5 S OP SYST DES IM
   [Anonymous], 1994, 124 DIG SYST RES CTR
   [Anonymous], FIPS PUB 180 3 SEC H
   [Anonymous], 1976, 41 CSTR BELL LAB
   Bhagwat Deepavali., 2006, MASCOTS, P413
   Bindel D., 2000, P 9 INT C ARCH SUPP
   Brin S, 1998, COMPUT NETWORKS ISDN, V30, P107, DOI 10.1016/S0169-7552(98)00110-X
   BRIN S, 1998, ANATOMY LARGE SCALE
   Broder A. Z., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P327, DOI 10.1145/276698.276781
   Broder A. Z., 1993, PROC SEQUENCES 2 MET, P143
   Broder Andrei Z., 1997, P 6 INT WORLD WID WE, p1157 , DOI [10.1016/S0169-7552(97)00031-7, DOI 10.1016/S0169-7552(97)00031-7]
   Broder AZ, 1998, COMPRESSION AND COMPLEXITY OF SEQUENCES 1997 - PROCEEDINGS, P21, DOI 10.1109/SEQUEN.1997.666900
   Broder AZ, 2000, J COMPUT SYST SCI, V60, P630, DOI 10.1006/jcss.1999.1690
   Buchsbaum AL, 2003, J ACM, V50, P825, DOI 10.1145/950620.950622
   Buchsbaum AL, 2000, PROCEEDINGS OF THE ELEVENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P175
   BUNEMAN P, 2002, P ACM SIGMOD INT C M
   BURNS R., 2002, P 19 IEEE S MASS STO
   BURNS R, 1996, THESIS U CALIFORNIA
   Burns R. C., 1998, Proceedings of the Seventeenth Annual ACM Symposium on Principles of Distributed Computing, P267, DOI 10.1145/277697.277747
   Burns RandalC., 1997, Proceedings of the fifth workshop on I/O in parallel and distributed systems, P27
   Burns RC, 1997, IEEE IPCCC, P429, DOI 10.1109/PCCC.1997.581547
   Chang F, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P205
   Charikar Moses S., 2002, P 34 ANN ACM S THEOR, P380, DOI DOI 10.1145/509907.509965
   Chen Y., 1999, Digital 99 Libraries. Fourth ACM Conference on Digital Libraries, P28, DOI 10.1145/313238.313249
   CRESPO A, 1998, P 3 ACM INT C DIG LI, P69
   DEAN J, 1999, P 8 INT WORLD WID WE
   Douglis F., 2003, P USENIX ANN TECHN C
   Dubnicki Cezary., 2009, FAST 09 P 7 C FILE S, P197
   *EMC CORP, 2002, EMC CENT CONT ADDR S
   FREE SOFTWARE FOUNDATION, 2000, GZIP DAT COMPR PROGR
   Ghemawat Sanjay., 2003, SOSP'03
   GIBSON T, 1998, THESIS U MARYLAND BA
   Goldberg AV, 1998, P IEEE INT FORUM RES, P147, DOI 10.1109/ADL.1998.670389
   Gray J., 2000, Proceedings of 16th International Conference on Data Engineering (Cat. No.00CB37073), P3, DOI 10.1109/ICDE.2000.839382
   GRAY J., 2002, MSTR0254
   HENSON V, 2003, P 9 WORKSH HOT TOP O
   Henzinger M., 2006, Proceedings of the Twenty-Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P284, DOI 10.1145/1148170.1148222
   HITACHI GLOBAL STORAGE TECHNOLOGIES, 2004, HIT HARD DISK DRIV S
   HITZ D, 1994, PROCEEDINGS OF THE WINTER 1994 USENIX CONFERENCE, P235
   Hollingsworth J. K., 1997, Software Engineering Notes, V22, P104, DOI 10.1145/258368.258399
   Hong B., 2004, P 21 IEEE 12 NASA GO
   IBM, 1998, OEM HARD DISK DRIV
   IBM, 2005, IBM TIV STOR MAN
   Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876
   Jain AK, 1999, ACM COMPUT SURV, V31, P264, DOI 10.1145/331499.331504
   Jain N, 2005, USENIX ASSOCIATION PROCEEDINGS OF THE 4TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P281
   KOHL JT, 1993, PROCEEDINGS OF THE WINTER 1993 USENIX CONFERENCE, P435
   Koller R, 2010, ACM T STORAGE, V6, DOI 10.1145/1837915.1837921
   Korn D., 2002, 3284 RFC IETF
   Korn DG, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE GENERAL TRACK, P219
   Kulkarni P, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE GENERAL TRACK 2004 USENIX ANNUAL TECHNICAL CONFERENCE, P59
   LEE EK, 1996, P 7 INT C ARCH SUPP, V31, P84
   LEWIS B., 2008, DEDUPLICATION COMES
   Liefke H, 2000, SIGMOD REC, V29, P153, DOI 10.1145/335191.335405
   Lillibridge Mark., 2009, PROCCEDINGS 7 C FILE, P111
   Litwin W, 1996, ACM T DATABASE SYST, V21, P480, DOI 10.1145/236711.236713
   Litwin W., 1996, Proceedings First IFCIS International Conference on Cooperative Information Systems, P196, DOI 10.1109/COOPIS.1996.555011
   Litwin W., 1993, SIGMOD Record, V22, P327, DOI 10.1145/170036.170084
   LONG D. D. E., 2002, SCALABLE ON LINE ASS
   LORIE R. A., 2001, PROJECT PRESERVATION, V5
   LORIE R. A., 2004, STOR SYST RES CTR SE
   Lyman P., 2003, How much information?
   MacDonald Josh, 2000, Ph.D. thesis
   MAHALINGAM M, 2002, HPL2002199
   Manasse M., 2003, FINDING SIMILAR THIN
   MANBER U, 1993, 9333 TR U AR DEP COM
   Manku GS, 2007, P 16 INT C WORLD WID, P141, DOI DOI 10.1145/1242572.1242592
   MOGUL J., 1997, P C APPL TECHN ARCH
   MUTHITACHAROEN A, 2001, P 18 ACM S OP SYST P, P174, DOI DOI 10.1145/502034.502052
   Nelson M., 1996, The data compression book
   OTOO E. J., 1986, P 5 ACM SIGACT SIGMO, P100
   OUKSEL M, 1983, P 3 ACM S PRINC DAT, P90
   Ouyang Z, 2002, WISE 2002: PROCEEDINGS OF THE THIRD INTERNATIONAL CONFERENCE ON WEB INFORMATION SYSTEMS ENGINEERING, P257, DOI 10.1109/WISE.2002.1181662
   POLLACK K. T, 2005, P 21 INT C DAT ENG I
   Quinlan S, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE FAST'02 CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P89
   RABIN MO, 1981, TR1581 HARV U CTR RE
   Rajasekar AK, 2001, LECT NOTES COMPUT SC, V2110, P72
   Riggle CM, 1998, IEEE T MAGN, V34, P2362, DOI 10.1109/20.703881
   Rivest R., 1992, document RFC 1321
   Rochkind M. J., 1975, IEEE Transactions on Software Engineering, VSE-1, P364, DOI 10.1109/TSE.1975.6312866
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   ROTHENBERG J, 1995, SCI AM, V272, P42, DOI 10.1038/scientificamerican0195-42
   SAITO Y., 2002, P ACM SIGOPS EUR WOR
   Santry DS, 1999, OPERATING SYSTEMS REVIEW, VOL 33, NO 5, DECEMBER 1999, P110, DOI 10.1145/319344.319159
   Sayood K., 2003, Lossless Compression Handbook
   Schmuck F, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE FAST'02 CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P231
   SECURITY INNOVATION, 2006, REG COMPL DEM INTR C
   SELTZER M, 1991, PROCEEDINGS OF THE WINTER 1991 USENIX CONFERENCE, P173
   Tanenbaum A. S., 2006, Operating Systems Review, V40, P100, DOI 10.1145/1113361.1113364
   TICHY WF, 1985, SOFTWARE PRACT EXPER, V15, P637, DOI 10.1002/spe.4380150703
   TRENDAFILOV D, 2004, TRCIS200402 POL U
   Trendafilov D., 2002, Tech. Rep., TR-CIS-2002-02
   Tridgell A., 1999, EFFICIENT ALGORITHMS
   Ungurearm C., 2010, P 8 USENIX C FIL STO, P17
   VO K.-P., 2007, P INT C SOFTW DAT TE, P201
   Wiederhold G., 1983, DATABASE DESIGN
   Witten I.H., 1999, Managing Gigabytes: Compressing and Indexing Documents and Images
   YANG T., 2009, TRUNLCSE20090004
   YOU L. L., 2006, THESIS U CALIFORNIA
   YOU LL, 2004, P 21 IEEE 12 NASA GO, P227
   Zadok E., 2004, P 12 NASA GODDARD 21
   Zhu B, 2008, P 6 USENIX C FIL STO, P18
   ZIV J, 1977, IEEE T INFORM THEORY, V23, P337, DOI 10.1109/TIT.1977.1055714
   [No title captured]
NR 108
TC 15
Z9 16
U1 0
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUL
PY 2011
VL 7
IS 2
AR 6
DI 10.1145/1970348.1970351
PG 60
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 990LL
UT WOS:000307632300003
DA 2024-07-18
ER

PT J
AU Sundararaman, S
   Subramanian, S
   Rajimwale, A
   Arpaci-Dusseau, AC
   Arpaci-Dusseau, RH
   Swift, MM
AF Sundararaman, Swaminathan
   Subramanian, Sriram
   Rajimwale, Abhishek
   Arpaci-Dusseau, Andrea C.
   Arpaci-Dusseau, Remzi H.
   Swift, Michael M.
TI Membrane: Operating System Support for Restartable File Systems
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Algorithms; Reliability; Checkpointing; restartability; fault
   recovery; file systems
AB We introduce Membrane, a set of changes to the operating system to support restartable file systems. Membrane allows an operating system to tolerate a broad class of file system failures, and does so while remaining transparent to running applications; upon failure, the file system restarts, its state is restored, and pending application requests are serviced as if no failure had occurred. Membrane provides transparent recovery through a lightweight logging and checkpoint infrastructure, and includes novel techniques to improve performance and correctness of its fault-anticipation and recovery machinery. We tested Membrane with ext2, ext3, and VFAT. Through experimentation, we show that Membrane induces little performance overhead and can tolerate a wide range of file system crashes. More critically, Membrane does so with little or no change to existing file systems, thus improving robustness to crashes without mandating intrusive changes to existing file-system code.
C1 [Sundararaman, Swaminathan; Subramanian, Sriram; Rajimwale, Abhishek; Arpaci-Dusseau, Andrea C.; Arpaci-Dusseau, Remzi H.; Swift, Michael M.] Univ Wisconsin Madison, Madison, WI 53706 USA.
C3 University of Wisconsin System; University of Wisconsin Madison
RP Sundararaman, S (corresponding author), Univ Wisconsin Madison, Madison, WI 53706 USA.
EM swami@cs.wisc.edu
OI Subramanian, Sriram/0000-0002-5266-8366
FU National Science Foundation [CCF-0621487, CNS-0509474, CNS-0834392,
   CCF-811697, CCF-0811697, CCF-0937959]; NetApp; Sun Microsystems; Google
FX This material is based upon work supported by the National Science
   Foundation under grants CCF-0621487, CNS-0509474, CNS-0834392,
   CCF-811697, CCF-0811697, CCF-0937959, as well as by generous donations
   from NetApp, Sun Microsystems, and Google. Any opinions, findings, and
   conclusions or recommendations expressed in this material are those of
   the authors and do not necessarily reflect the views of NSF or other
   institutions.
CR [Anonymous], P 20 ACM S OP SYST P
   [Anonymous], 2005, ACM Transactions on Storage
   [Anonymous], 2009, BTRFS
   Bonwick J., 2007, Zfs: The last word in file systems
   Candea G, 2004, USENIX Association Proceedings of the Sixth Symposium on Operating Systems Design and Implementation (OSDE '04), P31
   Candea George, 2003, P 9 WORKSH HOT TOP O
   Chapin J., 1995, P 15 ACM S OP SYST P
   Chou Andy, 2001, SOSP, P73, DOI 10.1145/502034.502042
   CRANOR C.D., 1999, P USENIX ANN TECHN C
   David F. M., 2008, P 8 S OP SYST DES IM
   DEMSKY B., 2003, P 18 ACM SIGPLAN C O
   Engler D., 2001, Operating Systems Review, V35, P57, DOI 10.1145/502059.502041
   Erlingsson U, 2006, Usenix Association 7th Usenix Symposium on Operating Systems Design and Implementation, P75
   Fraser K., 2004, P WORKSH OP SYST ARC
   Gunawi HS, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P207
   HAGMANN R., 1987, P 11 ACM S OP SYST P
   Herder JN, 2006, P 6 EUR DEP COMP C
   Herder JN, 2007, I C DEPEND SYS NETWO, P41, DOI 10.1109/DSN.2007.46
   HITZ D., 1994, P USENIX WINT TECHN
   Kidder Tracy., 1981, SOUL NEW MACHINE
   Kleiman S. R., 1986, USENIX Association Summer Conference Proceedings, Atlanta 1986, P238
   KOLDINGER E., 1992, P 5 INT C ARCH SUPP
   Kropp N. P., 1998, P 28 INT S FAULT TOL
   LARUS J., 2005, SEM U WISC MAD
   LEVASSEUR J., 2004, P 6 USENIX OSDI
   Lu S., 2008, P 13 INT C ARCH SUPP
   MATHUR A., 2007, OTT LIN S OLS 07
   McVoy L. W., 1991, USENIX Association. Proceedings of the Winter 1991 USENIX Conference, P33
   MILOJICIC D., 2000, P 9 ACM SIGOPS EUR W
   MOGUL J. C., 1994, P USENIX SUMM TECHN
   Prabhakaran Vijayan, 2005, P OF THE 20 ACM S ON, V39, P206
   Reiser Hans., 2004, ReiserFS
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   SCHROCK E., 2005, UFS SVM VS ZFS CODE
   Shapiro JS, 2002, IEEE SOFTWARE, V19, P26, DOI 10.1109/52.976938
   Sweeney A., 1996, P USENIX ANN TECHN C
   SWIFT M. M., 2004, P 6 S OP SYST DES IM, P1
   Swift Michael M., 2003, P 19 ACM S OP SYST P
   TALAGALA N., 1999, P IEEE WORKSH FAULT
   TS'O T., 2002, P USENIX ANN TECHN C
   WEIMER W., 2004, P 19 ACM SIGPLAN C O
   Williams D., 2008, P 8 USENIX OSDI
   YANG J., 2006, P 7 S OP SYSTEMSDESI
   YANG J., 2004, P 6 S OP SYST DES IM
   ZHOU F., 2006, P 7 S OP SYST DES IM
NR 45
TC 4
Z9 6
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD SEP
PY 2010
VL 6
IS 3
AR 11
DI 10.1145/1837915.1837919
PG 30
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA V24QL
UT WOS:000208424800004
DA 2024-07-18
ER

PT J
AU Chang, YH
   Lin, JH
   Hsieh, JW
   Kuo, TW
AF Chang, Yuan-Hao
   Lin, Jian-Hong
   Hsieh, Jen-Wei
   Kuo, Tei-Wei
TI A Strategy to Emulate NOR Flash with NAND Flash
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Experimentation; Management; Measurement; Performance; NAND;
   NOR; flash memory; data prefetching
AB This work is motivated by a strong market demand for the replacement of NOR flash memory with NAND flash memory to cut down the cost of many embedded-system designs, such as mobile phones. Different from LRU-related caching or buffering studies, we are interested in prediction-based prefetching based on given execution traces of application executions. An implementation strategy is proposed for the storage of the prefetching information with limited SRAM and run-time overheads. An efficient prediction procedure is presented based on information extracted from application executions to reduce the performance gap between NAND flash memory and NOR flash memory in reads. With the behavior of a target application extracted from a set of collected traces, we show that data access to NOR flash memory can respond effectively over the proposed implementation.
C1 [Chang, Yuan-Hao] Natl Taipei Univ Technol, Dept Elect Engn, Taipei 106, Taiwan.
   [Lin, Jian-Hong; Kuo, Tei-Wei] Natl Taiwan Univ, Grad Inst Networking & Multimedia, Dept Comp Sci & Informat Engn, Taipei 106, Taiwan.
   [Hsieh, Jen-Wei] Natl Taiwan Univ Sci & Technol, Dept Comp Sci & Informat Engn, Taipei 106, Taiwan.
C3 National Taipei University of Technology; National Taiwan University;
   National Taiwan University of Science & Technology
RP Chang, YH (corresponding author), Natl Taipei Univ Technol, Dept Elect Engn, Taipei 106, Taiwan.
EM johnsonchang@ntut.edu.tw; r94944003@csie.ntu.edu.tw;
   jenwei@mail.ntust.edu.tw; ktw@csie.ntu.edu.tw
RI Hsieh, Jen-Wei/X-1989-2019; Chang, Yuan-Hao/ABA-6935-2020
OI Chang, Yuan-Hao/0000-0002-1282-2111; KUO, TEI-WEI/0000-0003-1974-0394
FU NSC [99-2218-E-027-005, 98-2221-E-002-120-MY3]; National Taiwan
   University [98R0062-05]; ROC Ministry of Economic Affairs in Taiwan
   [98-EC-17-A-01-S1-034]
FX This work was supported in part by the NSC under grant Nos.
   99-2218-E-027-005 and 98-2221-E-002-120-MY3, by the Excellent Research
   Projects of National Taiwan University under grant No. 98R0062-05, and
   by the ROC Ministry of Economic Affairs under grant No.
   98-EC-17-A-01-S1-034 in Taiwan.
CR [Anonymous], US Patent, Patent No. [540,448, 540448]
   [Anonymous], 2020, 128 LAYER NAND FLASH
   Chang Li-Pin., 2004, Proceedings of the 2004 ACM Symposium on Applied Computing (SAC '04), P862
   Chang LP, 2002, EIGHTH IEEE REAL-TIME AND EMBEDDED TECHNOLOGY AND APPLICATIONS SYMPOSIUM, PROCEEDINGS, P187, DOI 10.1109/RTTAS.2002.1137393
   CHANG Y.-H., 2007, P 13 IEEE INT C EMB
   DENNING PJ, 1972, COMMUN ACM, V15, P191, DOI 10.1145/361268.361281
   *DRAMEXCHANGE, 2007, NAND FLASH CONTR PRI
   *INT CORP, SOFTW IMPL RES FLASH
   Intel Corporation, FTL LOGG EXCH DAT FT
   Intel Corporation, 1998, UND FLASH TRANSL LAY
   Intel Corporation, FLASH CACH MEM PUTS
   JOO Y., 2006, P INT C HARDW SOFTW
   KAWAGUCHI A, 1995, PROCEEDINGS OF THE 1995 USENIX TECHNICAL CONFERENCE, P155
   KIM B., 2003, P INT C COMP DES ICC
   Lee JH, 2005, J SYST ARCHITECT, V51, P111, DOI 10.1016/j.sysarc.2004.10.002
   PARK C., 2004, P INT C EMB SOFTW EM
   PARK C, 2004, P INT S LOW POW EL D
   PAZ Z, 2003, ALTERNATIVES USING N
   QUINNELL RA, 2005, MEET DIFFERENT NEEDS
   Samsung Electronics, 2006, KFW8G16Q2M DEBX 512M
   Samsung Electronics, 2010, MEM OUTP EV BEST PAS
   Samsung Electronics, 2005, ONENAND FEAT PERF
   Santarini M., 2005, NAND VERSUS NOR
   Silicon Storage Technology, 2010, SST39LF040 45 4MBIT
   Spectek, 2007, FNNM40A 16GB NAND FL
   STMicroelectronics, 2005, NAND08GX3C2A 8GBIT M
   Tal A., 2003, 2 TECHNOLOGIES COMP
   WU CH, 2006, P IEEE ACM INT C COM
   WU M, 1994, SIGPLAN NOTICES, V29, P86, DOI 10.1145/195470.195506
   Xin Q, 2003, IEEE S MASS STOR SYS, P146, DOI 10.1109/MASS.2003.1194851
   Yim KS, 2004, IEEE T CONSUM ELECTR, V50, P192, DOI 10.1109/TCE.2004.1277861
NR 31
TC 9
Z9 14
U1 0
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUL
PY 2010
VL 6
IS 2
AR 5
DI 10.1145/1807060.1807062
PG 23
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA V24QG
UT WOS:000208424300002
DA 2024-07-18
ER

PT J
AU Bergman, S
   Cassel, N
   Bjorling, M
   Silberstein, M
AF Bergman, Shai
   Cassel, Niklas
   Bjorling, Matias
   Silberstein, Mark
TI ZNSwap: un-Block your Swap
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Swap; SSD; ZNS
ID FLASH; PERFORMANCE
AB We introduce ZNSwap, a novel swap subsystem optimized for the recent Zoned Namespace (ZNS) SSDs. ZNSwap leverages ZNS's explicit control over data management on the drive and introduces a space-efficient host-side Garbage Collector (GC) for swap storage co-designed with the OS swap logic. ZNSwap enables cross-layer optimizations, such as direct access to the in-kernel swap usage statistics by the GC to enable fine-grain swap storage management, and correct accounting of the GC bandwidth usage in the OS resource isolation mechanisms to improve performance isolation in multi-tenant environments. We evaluate ZNSwap using standard Linux swap benchmarks and two production key-value stores. ZNSwap shows significant performance benefits over the Linux swap on traditional SSDs, such as stable throughput for different memory access patterns, and 10x lower 99th percentile latency and 5x higher throughput for memcached key-value store under realistic usage scenarios.
C1 [Bergman, Shai; Silberstein, Mark] Technion, IL-3200003 Haifa, Israel.
   [Cassel, Niklas; Bjorling, Matias] Western Digital, Vesterbrogade 39, DK-1620 Copenhagen V, Denmark.
C3 Technion Israel Institute of Technology
RP Bergman, S (corresponding author), Technion, IL-3200003 Haifa, Israel.
EM shaiberg1@tx.technion.ac.il; Niklas.Cassel@wdc.com;
   Matias.Bjorling@wdc.com; mark@ee.technion.ac.il
OI Bjorling, Matias/0000-0003-1657-4052; Silberstein,
   Mark/0000-0001-9659-068X
FU Israel Science Foundation [980/21, 1027/18]; Western Digital
FX We gratefully acknowledge support from Israel Science Foundation (grants
   980/21 and 1027/18) and financial support from Western Digital.
CR [Anonymous], 2016, MAK SWAPP SCAL
   [Anonymous], 2021, RED FLASH
   [Anonymous], 2021, FAC CGROUPV2 MEM CON
   [Anonymous], 2018, Nvm express 2.0 zoned namespace command set specification
   [Anonymous], 2021, CLOC COUNT LINES COD
   [Anonymous], 2004, Linux journal
   [Anonymous], 2021, ARCHL SSD OPT
   [Anonymous], 2021, LKP
   [Anonymous], 2021, OPENSTACK OV CPU RAM
   [Anonymous], 2021, RED HAT DISC UN BLOC
   [Anonymous], 2021, VM SCAL
   [Anonymous], 2021, KIOX PCIE 5 0 SSD JU
   [Anonymous], 2021, SWAP SPAC AM EC2
   [Anonymous], 2021, SWAP FIL AM EC2
   [Anonymous], 2019, P OPT FIB COMM C EXH
   [Anonymous], 2021, SWAP LIN MAN PAG
   [Anonymous], 2016, REC SWAPP
   [Anonymous], 2021, MULT LRU NEXT GEN
   [Anonymous], 2021, MEMCG BACK AS RECL
   [Anonymous], 2021, DEB SSD OPT
   [Anonymous], 2009, SWAPF SWAP ALL US DI
   [Anonymous], 2020, SWAP TRY SCAN MOR FR
   [Anonymous], 2021, Redis
   [Anonymous], 2021, UB TRIM SWAP PART
   [Anonymous], 2021, SOL STAT STOR PERF T
   Atikoglu Berk, 2012, Performance Evaluation Review, V40, P53, DOI 10.1145/2318857.2254766
   Bjorling M., 2020, P VAULT LIN STOR FIL
   Bjorling M, 2021, PROCEEDINGS OF THE 2021 USENIX ANNUAL TECHNICAL CONFERENCE, P689
   Bjorling M, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P359
   Cooper Brian F., 2010, P 1 ACM S CLOUD COMP, P143, DOI [DOI 10.1145/1807128.1807152, 10.1145/1807128.1807152]
   Desnoyers P, 2014, ACM T STORAGE, V10, DOI 10.1145/2577384
   Gonzalez J, 2016, P 7 NONV MEM WORKSH, P1
   Grupp L.M., 2012, P 10 USENIX C FILE S, P2
   Gupta A, 2009, ACM SIGPLAN NOTICES, V44, P229, DOI 10.1145/1508284.1508271
   Holmberg Hans, 2021, dm-zap: Host-based FTL for ZNS SSDs
   Hu Xiao-Yu, 2009, P SYSTOR 2009 ISR EX, P1
   Hyun C., 2011, 23 ACM S OP SYST PRI
   Im J, 2020, PROCEEDINGS OF THE 2020 USENIX ANNUAL TECHNICAL CONFERENCE, P173
   Jiang S, 2011, IEEE S MASS STOR SYS
   Jung Dawoon, 2005, P INT WORKSHOP SOFTW
   Kang DH, 2017, PROCEEDINGS OF THE 8TH ASIA-PACIFIC WORKSHOP ON SYSTEMS (APSYS '17), DOI 10.1145/3124680.3124718
   Kang Jeong-Uk, 2014, 6 USENIX WORKSH HOT
   Ko S, 2008, INTERNATIONAL CONFERENCE ON COMPUTATIONAL SCIENCES AND ITS APPLICATIONS, PROCEEDINGS, P151, DOI 10.1109/ICCSA.2008.54
   Le Moal Damien, 2017, dm-zoned: Zoned Block Device device mapper
   Lee G, 2020, ANN I S COM, P1103, DOI 10.1109/ISCA45697.2020.00093
   Lee J, 2014, IEEE INT CONF TRUST, P673, DOI 10.1109/TrustCom.2014.87
   Lee J, 2013, INT CONF ELECTRO INF
   Leverich Jacob., 2014, Mutilate: High-Performance Memcached Load Generator
   Li C, 2021, ACM T STORAGE, V17, DOI 10.1145/3480963
   Lin MW, 2012, IEEE T CONSUM ELECTR, V58, P435, DOI 10.1109/TCE.2012.6227444
   Lin MW, 2012, IEEE T CONSUM ELECTR, V58, P419, DOI 10.1109/TCE.2012.6227442
   Nguyen Trong-Dat, 2016, P 6 INT C EM DAT TEC, P139
   Ohshima S., 2018, KEYNOTE 3 FLASH MEMO
   Park S.-y., 2006, P 2006 INT C COMP AR, P234, DOI DOI 10.1145/1176760.1176789
   Park SeongJae, 2019, P 11 USENIX WORKSHOP
   SAMSUNG, 2019, ULTR LAT SAMS NAND S
   Saxena Mohit., 2010, P USENIX ANN TECHNIC
   Stoica R, 2013, PROC VLDB ENDOW, V6, P733, DOI 10.14778/2536360.2536372
   Van Houdt B, 2013, PERFORM EVALUATION, V70, P692, DOI 10.1016/j.peva.2013.08.010
   Xu Shuotao, 2016, PhD thesis
   Yadgar Gala, 2015, P 7 USENIX WORKSH HO
   Yang J., 2018, Information Technology-New Generations, P627
   Yang JP, 2017, SYSTOR'17: PROCEEDINGS OF THE 10TH ACM INTERNATIONAL SYSTEMS AND STORAGE CONFERENCE, DOI 10.1145/3078468.3078469
   Zhang JC, 2017, ACM T EMBED COMPUT S, V16, DOI 10.1145/3126545
NR 64
TC 0
Z9 0
U1 1
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAY
PY 2023
VL 19
IS 2
AR 12
DI 10.1145/3582434
PG 25
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D7OU8
UT WOS:000970592600003
DA 2024-07-18
ER

PT J
AU He, KW
   An, YJ
   Luo, YJ
   Liu, XG
   Wang, G
AF He, Kewen
   An, Yujie
   Luo, Yijing
   Liu, Xiaoguang
   Wang, Gang
TI FlatLSM: Write-Optimized LSM-Tree for PM-Based KV Stores
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Persistent memory; key-value stores; LSM-Tree
ID MEMORY
AB The Log-Structured Merge Tree (LSM-Tree) is widely used in key-value (KV) stores because of its excwrite performance. But LSM-Tree-based KV stores still have the overhead of write-ahead log and write stall caused by slow L-0 flush and L-0-L-1 compaction. New byte-addressable, persistent memory (PM) devices bring an opportunity to improve the write performance of LSM-Tree. Previous studies on PM-based LSM-Tree have not fully exploited PM's "dual role" of main memory and external storage. In this article, we analyze two strategies of memtables based on PM and the reasons write stall problems occur in the first place. Inspired by the analysis result, we propose FlatLSM, a specially designed flat LSM-Tree for non-volatile memory based KV stores. First, we propose PMTable with separated index and data. The PM Log utilizes the Buffer Log to store KVs of size less than 256B. Second, to solve the write stall problem, FlatLSM merges the volatile memtables and the persistent L-0 into large PMTables, which can reduce the depth of LSM-Tree and concentrate I/O bandwidth on L-0-L-1 compaction. To mitigate write stall caused by flushing large PMTables to SSD, we propose a parallel flush/compaction algorithm based on KV separation. We implemented FlatLSM based on RocksDB and evaluated its performance on Intel's latest PM device, the Intel Optane DC PMM with the state-of-the-art PM-based LSM-Tree KV stores, FlatLSM improves the throughput 5.2x on random write workload and 2.55x on YCSB-A.
C1 [He, Kewen; An, Yujie; Luo, Yijing; Liu, Xiaoguang; Wang, Gang] Nankai Univ, 38 Tongyan Rd, Tianjin 300350, Peoples R China.
C3 Nankai University
RP Liu, XG; Wang, G (corresponding author), Nankai Univ, 38 Tongyan Rd, Tianjin 300350, Peoples R China.
EM hekw@nbjl.nankai.edu.cn; anyj@nbjl.nankai.edu.cn;
   luoyij@nbjl.nankai.edu.cn; liuxg@nbjl.nankai.edu.cn;
   wgzwp@nbjl.nankai.edu.cn
RI luo, yijing/GYJ-0273-2022; wang, gang/ITT-0670-2023
OI liu, xiaoguang/0000-0002-9010-3278; Wang, Gang/0000-0003-0387-2501
FU NSF of China [62272253, 62272252, 62141412]; Fundamental Research Funds
   for the Central Universities
FX This work was partly supported by the NSF of China (62272253, 62272252,
   62141412) and Fundamental Research Funds for the Central Universities.
CR [Anonymous], 2018, TITAN ROCKSDB PLUGIN
   Apache, 2014, HBASE
   Arulraj J, 2015, SIGMOD'15: PROCEEDINGS OF THE 2015 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P707, DOI 10.1145/2723372.2749441
   Balmau O, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P753
   Balmau O, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P363
   Beaver D., 2010, P 9 USENIX S OP SYST, P1
   Chang F, 2008, ACM T COMPUT SYST, V26, DOI 10.1145/1365815.1365816
   Chen YM, 2020, TWENTY-FIFTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXV), P1077, DOI 10.1145/3373376.3378515
   Cooper Brian F., 2010, P 1 ACM S CLOUD COMP, P143, DOI [DOI 10.1145/1807128.1807152, 10.1145/1807128.1807152]
   Dayan N, 2019, INT CONF MANAGE DATA, P449, DOI 10.1145/3299869.3319903
   DeCandia Giuseppe, 2007, Operating Systems Review, V41, P205, DOI 10.1145/1323293.1294281
   Eisenman A, 2018, EUROSYS '18: PROCEEDINGS OF THE THIRTEENTH EUROSYS CONFERENCE, DOI 10.1145/3190508.3190524
   Facebook, 2013, About us
   Facebook, 2008, CASSANDRA
   Gilad E, 2020, PROCEEDINGS OF THE FIFTEENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS (EUROSYS'20), DOI 10.1145/3342195.3387523
   Google, 2011, LevelDB
   Hwang D, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P187
   Intel, 2019, WHAT IS INT OPT DC P
   Intel, 2019, Intel Optane DC persistent memory
   Intel Corporation, 2019, PERS MEM DEV KIT
   Izraelevitz Joseph, 2019, arXiv, DOI [10.48550/ARXIV.1903.05714, DOI 10.48550/ARXIV.1903.05714]
   Kaiyrakhmet O, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P191
   Kannan S, 2018, PROCEEDINGS OF THE 2018 USENIX ANNUAL TECHNICAL CONFERENCE, P993
   Lepers B, 2019, PROCEEDINGS OF THE TWENTY-SEVENTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '19), P447, DOI 10.1145/3341301.3359628
   Li J., 2017, Nvmrocks: Rocksdb on non-volatile memory systems
   Li YK, 2021, PROCEEDINGS OF THE 2021 USENIX ANNUAL TECHNICAL CONFERENCE, P673
   Lu LY, 2017, ACM T STORAGE, V13, DOI 10.1145/3033273
   Mao Y., 2012, P 7 ACM EUR C COMP S, P183
   Mei F, 2018, PROCEEDINGS OF THE 2018 ACM SYMPOSIUM ON CLOUD COMPUTING (SOCC '18), P477, DOI 10.1145/3267809.3267829
   Nam M, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P31
   ONeil P, 1996, ACTA INFORM, V33, P351, DOI 10.1007/s002360050048
   Oukid I, 2016, SIGMOD'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P371, DOI 10.1145/2882903.2915251
   Raju P, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P497, DOI 10.1145/3132747.3132765
   Raoux S, 2008, IBM J RES DEV, V52, P465, DOI 10.1147/rd.524.0465
   Sears Russell, 2012, P 2012 ACM SIGMOD IN, P217, DOI [10.1145/2213836.2213862, DOI 10.1145/2213836.2213862]
   SNIA NVM Programming Technical Working Group, 2017, NVM PROGR MOD
   Xia F, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P349
   Xu C, 2015, INT S HIGH PERF COMP, P476, DOI 10.1109/HPCA.2015.7056056
   Yan BY, 2021, PROC VLDB ENDOW, V14, P1872, DOI 10.14778/3467861.3467875
   Yang J, 2020, PROCEEDINGS OF THE 18TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P169
   Yao T, 2020, PROCEEDINGS OF THE 2020 USENIX ANNUAL TECHNICAL CONFERENCE, P17
   Yao T, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P159
   Zhan L, 2020, IEEE ACCESS, V8, P154518, DOI 10.1109/ACCESS.2020.3017651
   Zuo PF, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P461
NR 44
TC 1
Z9 1
U1 1
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAY
PY 2023
VL 19
IS 2
AR 19
DI 10.1145/3579855
PG 26
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D7OU8
UT WOS:000970592600010
DA 2024-07-18
ER

PT J
AU Wang, Q
   Lu, YY
   Li, JR
   Xie, MH
   Shu, JW
AF Wang, Qing
   Lu, Youyou
   Li, Junru
   Xie, Minhui
   Shu, Jiwu
TI Nap: Persistent Memory Indexes for NUMA Architectures
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Persistent memory; non-uniform memory access; indexes
ID PERFORMANCE; TREES
AB We present NAP, a black-box approach that converts concurrent persistent memory (PM) indexes into nonuniform memory access (NUMA)-aware counterparts. Based on the observation that real-world workloads always feature skewed access patterns, NAP introduces a NUMA-aware layer (NAL) on the top of existing concurrent PM indexes, and steers accesses to hot items to this layer. The NAL maintains (1) per node partial views in PM for serving insert/update/delete operations with failure atomicity and (2) a global view in DRAM for serving lookup operations. The NAL eliminates remote PM accesses to hot items without inducing extra local PM accesses. Moreover, to handle dynamic workloads, NAP adopts a fast NAI, switch mechanism. We convert five state-of-the-art PM indexes using NAP. Evaluation on a four-node machine with Optane DC Persistent Memory shows that NAP can improve the throughput by up to 2.3x and 1.56x under write-intensive and read-intensive workloads, respectively.
C1 [Wang, Qing; Lu, Youyou; Li, Junru; Xie, Minhui; Shu, Jiwu] Tsinghua Univ, Dept Comp Sci & Technol, 30 Shuangqing Rd,201 East Main Bldg, Beijing 100084, Peoples R China.
C3 Tsinghua University
RP Shu, JW (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, 30 Shuangqing Rd,201 East Main Bldg, Beijing 100084, Peoples R China.
EM q-wang18@mails.tsinghua.edu.cn; luyouyou@tsinghua.edu.cn;
   lijr19@mails.tsinghua.edu.cn; xmh19@mails.tsinghua.edu.cn;
   shujw@tsinghua.edu.cn
OI Li, Junru/0000-0001-7387-7899
FU National Key Research & Development Program of China [2018YFB1003301];
   National Natural Science Foundation of China [62022051, 61832011,
   61772300]; Huawei [YBN2019125112]
FX This work is supported by the National Key Research & Development
   Program of China (Grant No. 2018YFB1003301), the National Natural
   Science Foundation of China (Grant No. 62022051, 61832011, 61772300),
   and Huawei (Grant No. YBN2019125112).
CR Achermann R, 2020, TWENTY-FIFTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXV), P283, DOI 10.1145/3373376.3378468
   Anderson TE, 2020, PROCEEDINGS OF THE 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '20), P1011
   [Anonymous], 2020, SEQUENTIAL LOCKS
   [Anonymous], 2020, IMPL P MASSTREE FAST
   [Anonymous], 2020, PMDK IMPLEMENTATION
   [Anonymous], 2021, INT XEON PROC SCAL F
   [Anonymous], 2020, AUTONUMA OTHER APPRO
   [Anonymous], 2021, INT 64 IA 32 ARCH OP
   [Anonymous], 2021, INT OPT PERS MEM 200
   [Anonymous], 2020, PERSISTENT MEMORY DE
   [Anonymous], 2020, DISTR READER WRITER
   [Anonymous], 2015, ACM Trans. Parallel Comput., DOI [DOI 10.1145/2686884, 10.1145/2686884]
   [Anonymous], 2020, PROC COUNT MON PCM
   Atikoglu Berk, 2012, Performance Evaluation Review, V40, P53, DOI 10.1145/2318857.2254766
   Bhardwaj Ankit, P 15 USENIX S OP SYS
   Calciu I, 2017, TWENTY-SECOND INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXII), P206, DOI 10.1145/3037697.3037721
   Cao ZC, 2020, PROCEEDINGS OF THE 18TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P209
   Chabbi M, 2015, ACM SIGPLAN NOTICES, V50, P215, DOI [10.1145/2688500.2688503, 10.1145/2858788.2688503]
   Chen JQ, 2020, PROCEEDINGS OF THE 18TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P239
   Chen SM, 2015, PROC VLDB ENDOW, V8, P786, DOI 10.14778/2752939.2752947
   Chen YM, 2021, PROCEEDINGS OF THE 19TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '21), P81
   Chen YM, 2020, TWENTY-FIFTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXV), P1077, DOI 10.1145/3373376.3378515
   Chen Youmin, 2020, Proc. VLDB Endow., V13, P2634, DOI [10.14778/3407790.3407850, DOI 10.14778/3407790.3407850]
   Chen ZY, 2020, PROCEEDINGS OF THE 2020 USENIX ANNUAL TECHNICAL CONFERENCE, P799
   Cohen N, 2019, TWENTY-FOURTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXIV), P441, DOI 10.1145/3297858.3304046
   Cooper Brian F., 2010, P 1 ACM S CLOUD COMP, P143, DOI [DOI 10.1145/1807128.1807152, 10.1145/1807128.1807152]
   Cormode G, 2005, J ALGORITHMS, V55, P58, DOI 10.1016/j.jalgor.2003.12.001
   Daase Bjorn, 2021, P 2021 INT C MAN DAT
   Daly H., 2018, P 32 INT S DISTR COM
   Dashti M, 2013, ACM SIGPLAN NOTICES, V48, P381, DOI 10.1145/2499368.2451157
   David T, 2015, ACM SIGPLAN NOTICES, V50, P631, DOI [10.1145/2775054.2694359, 10.1145/2694344.2694359]
   Dice D, 2011, SPAA 11: PROCEEDINGS OF THE TWENTY-THIRD ANNUAL SYMPOSIUM ON PARALLELISM IN ALGORITHMS AND ARCHITECTURES, P65
   Didona D, 2019, PROCEEDINGS OF THE 16TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P79
   Dong MK, 2019, PROCEEDINGS OF THE TWENTY-SEVENTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '19), P478, DOI 10.1145/3341301.3359637
   Fraser Keir, 2004, Ph.D. Dissertation
   Friedman M, 2021, PROCEEDINGS OF THE 42ND ACM SIGPLAN INTERNATIONAL CONFERENCE ON PROGRAMMING LANGUAGE DESIGN AND IMPLEMENTATION (PLDI '21), P1218, DOI 10.1145/3453483.3454105
   Friedman M, 2020, PROCEEDINGS OF THE 41ST ACM SIGPLAN CONFERENCE ON PROGRAMMING LANGUAGE DESIGN AND IMPLEMENTATION (PLDI '20), P377, DOI 10.1145/3385412.3386031
   Gu JY, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P913
   Haria S, 2020, TWENTY-FIFTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXV), P775, DOI 10.1145/3373376.3378472
   Hendler D, 2010, SPAA '10: PROCEEDINGS OF THE TWENTY-SECOND ANNUAL SYMPOSIUM ON PARALLELISM IN ALGORITHMS AND ARCHITECTURES, P355
   Huang Q., 2014, Proceedings of the 13th ACM Workshop on Hot Topics in Networks, page, P8
   Hwang D, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P187
   Jin X, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P121, DOI 10.1145/3132747.3132764
   Kadekodi R, 2019, PROCEEDINGS OF THE TWENTY-SEVENTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '19), P494, DOI 10.1145/3341301.3359631
   Kalia Anuj, 2020, SoCC '20: Proceedings of the 11th ACM Symposium on Cloud Computing, P105, DOI 10.1145/3419111.3421294
   Kalia A, 2019, PROCEEDINGS OF THE 16TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P1
   Kashyap S, 2018, EUROSYS '18: PROCEEDINGS OF THE THIRTEENTH EUROSYS CONFERENCE, DOI 10.1145/3190508.3190510
   Kashyap S, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P603
   Kim Jonghyeon, 2021, P 2021 USENIX ANN TE, P715
   Kim WH, 2021, PROCEEDINGS OF THE 28TH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, SOSP 2021, P424, DOI 10.1145/3477132.3483589
   Krishnan RM, 2020, TWENTY-FIFTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXV), P335, DOI 10.1145/3373376.3378483
   Krishnan R Madhava, 2021, 2021 USENIX ANN TECH, P773
   Lee SK, 2019, PROCEEDINGS OF THE TWENTY-SEVENTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '19), P462, DOI 10.1145/3341301.3359635
   Lee SK, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P257
   Li XZ, 2016, 13TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION (NSDI '16), P31
   Lim H, 2017, SIGMOD'17: PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P21, DOI 10.1145/3035918.3064015
   Lim H, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P1
   Liu JH, 2020, PROC VLDB ENDOW, V13, P1078, DOI 10.14778/3384345.3384355
   Liu ZX, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P143
   Lu BT, 2020, PROC VLDB ENDOW, V13, P1147, DOI 10.14778/3389133.3389134
   Ma SZ, 2022, PSYCHOL HEALTH, V37, P1200, DOI 10.1080/08870446.2021.1934470
   Mao Y., 2012, P 7 ACM EUR C COMP S, P183
   Mathew A, 2020, PROC VLDB ENDOW, V13, P1332, DOI 10.14778/3397230.3397232
   Memaripour A, 2020, TWENTY-FIFTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXV), P789, DOI 10.1145/3373376.3378456
   Nam M, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P31
   NAWAB F., 2017, P 31 INT S DISTR COM
   Oukid I, 2016, SIGMOD'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P371, DOI 10.1145/2882903.2915251
   Panwar A, 2021, ASPLOS XXVI: TWENTY-SIXTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS, P194, DOI 10.1145/3445814.3446709
   Peng IB, 2019, MEMSYS 2019: PROCEEDINGS OF THE INTERNATIONAL SYMPOSIUM ON MEMORY SYSTEMS, P304, DOI 10.1145/3357526.3357568
   Radovic Z, 2003, NINTH INTERNATIONAL SYMPOSIUM ON HIGH-PERFORMANCE COMPUTER ARCHITECTURE, PROCEEDINGS, P241, DOI 10.1109/HPCA.2003.1183542
   Shu JW, 2020, ACM T STORAGE, V16, DOI 10.1145/3412852
   Smolyar I, 2020, TWENTY-FIFTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXV), P101, DOI 10.1145/3373376.3378509
   Tu S, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P18, DOI 10.1145/2517349.2522713
   Venkataraman Shivaram, 2011, P 9 USENIX C FIL STO, P5
   Wang Ying, 2020, P 36 INT C MASS STOR
   Wei X., 2021, P USENIX ANN TECH C, P523
   Xu J, 2019, TWENTY-FOURTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXIV), P427, DOI 10.1145/3297858.3304077
   Xu J, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P323
   Yang J, 2020, PROCEEDINGS OF THE 18TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P169
   Yang Jun, 2015, P 13 USENIX C FIL ST, P167
   Yang JC, 2020, PROCEEDINGS OF THE 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '20), P191
   Zhang W, 2020, PROCEEDINGS OF THE 14TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '20), P1029
   Zhou XJ, 2019, PROC VLDB ENDOW, V13, P421, DOI 10.14778/3372716.3372717
   Zuo PF, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P461
NR 84
TC 4
Z9 4
U1 1
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD FEB
PY 2022
VL 18
IS 1
AR 2
DI 10.1145/3507922
PG 35
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZN5OY
UT WOS:000765084700008
DA 2024-07-18
ER

PT J
AU Wu, FG
   Li, BZ
   Du, DHC
AF Wu, Fenggang
   Li, Bingzhe
   Du, David H. C.
TI FluidSMR: Adaptive Management for Hybrid SMR Drives
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Hybrid SMR drive; data management; format conversion
AB Hybrid Shingled Magnetic Recording (H-SMR) drives are the most recently developed SMR drives, which allow dynamic conversion of the recording format between Conventional Magnetic Recording (CMR) and SMR on a single disk drive. We identify the unique opportunities of H-SMR drives to manage the tradeoffs between performance and capacity, including the possibility of adjusting the SMR area capacity based on storage usage and the flexibility of dynamic data swapping between the CMR area and SMR area.
   We design and implement FluidSMR, an adaptive management scheme for hybrid SMR Drives, to fully utilize H-SMR drives under different workloads and capacity usages. FluidSMR has a two-phase allocation scheme to support a growing usage of the H-SMR drive. The scheme can intelligently determine the sizes of the CMR and the SMR space in an H-SMR drive based on the dynamic changing of workloads. Moreover, FluidSMR uses a cache in the CMR region, managed by a proposed loop-back log policy, to reduce the overhead of updates to the SMR region.
   Evaluations using enterprise traces demonstrate that FluidSMR outperforms baseline schemes in various workloads by decreasing the average I/O latency and effectively reducing/controlling the performance impact of the format conversion between CMR and SMR.
C1 [Wu, Fenggang; Du, David H. C.] Univ Minnesota Twin Cities, 200 Union St SE, Minneapolis, MN 55455 USA.
   [Li, Bingzhe] Oklahoma State Univ, 215 Gen Acad Bldg, Stillwater, OK 74078 USA.
C3 University of Minnesota System; University of Minnesota Twin Cities;
   Oklahoma State University System; Oklahoma State University - Stillwater
RP Wu, FG (corresponding author), Univ Minnesota Twin Cities, 200 Union St SE, Minneapolis, MN 55455 USA.
EM wuxx0835@umn.edu; bingzhe.li@okstate.edu; du@umn.edu
FU NSF I/UCRC Center Research in Intelligent Storage; NSF [1439622,
   1525617, 1536447, 1812537]
FX This work was partially supported by NSF I/UCRC Center Research in
   Intelligent Storage and the following NSF awards 1439622, 1525617,
   1536447, and 1812537.
CR Aghayev A, 2015, ACM T STORAGE, V11, DOI 10.1145/2821511
   Amer A, 2010, IEEE S MASS STOR SYS
   Amer A, 2011, IEEE T MAGN, V47, P3691, DOI 10.1109/TMAG.2011.2157115
   BLOOM BH, 1970, COMMUN ACM, V13, P422, DOI 10.1145/362686.362692
   Boyle Bill., Realms api
   Bucy J.S., 2008, The disksim simulation environment version 4.0 reference manual (cmu-pdl-08-101), P26
   Callaghan M., 2013, P 2013 ACM SIGMOD IN, P1185, DOI DOI 10.1145/2463676.2465296
   Cao ZC, 2020, PROCEEDINGS OF THE 18TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P209
   Cao ZC, 2019, ACM T STORAGE, V15, DOI 10.1145/3295461
   Cassuto Y, 2010, IEEE S MASS STOR SYS
   Chen F., 2011, Proc. of ACM Int. Conf. on Supercomputing, P22
   Chentao Wu, 2012, 2012 41st International Conference on Parallel Processing (ICPP 2012), P460, DOI 10.1109/ICPP.2012.32
   de Berg M., 2000, COMPUTATIONAL GEOMET
   Dropbox, 2018, EXT MAG POCK INN 1 P
   Fan Zhu, 2018, Physical Modifications of Starch, P1, DOI 10.1007/978-981-13-0725-6_1
   Feldman T., 2018, USENIX LOGIN, V43, P44
   Feldman Timothy., 2017, Flex device interface
   Feldman Timothy., Flex overview
   Gao Kaizhong, 2017, US Patent, Patent No. [9,728,206, 9728206]
   Gao Kaizhong, 2016, US Patent, Patent No. [9,508,362, 9508362]
   Ge XZ, 2018, I S MOD ANAL SIM COM, P94, DOI 10.1109/MASCOTS.2018.00017
   Google, DYN HYBR SMR OCP PRO
   Guerra J., 2011, P 9ST USENIX C FAST, P20
   Hajkazemi MH, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P821
   Hall D, 2012, IEEE T MAGN, V48, P1777, DOI 10.1109/TMAG.2011.2179528
   He WP, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P121
   He Weiping, 2014, P HOTSTORAGE 14
   INCITS T10 Technical Committee, 2015, INF TECHN ZON BLOCK
   INCITS T13 Technical Committee, ZON DEV ATA COMM SET
   Jin C, 2014, IEEE S MASS STOR SYS
   Kadekodi Saurabh, 2015, P ACM WORKSH HOT TOP
   Kim Y., IEEE INT WORKSHOP MO
   Lin C.-I., 2012, P 20 IEEE INT S MOD
   Macko P., 2017, P IEEE MASS STOR SYS, P1
   Macko Peter, 2017, ARXIV170509701
   Manzanares A., 2016, P 8 USENIX WORKSH HO
   Mao Y, 2014, IEEE T PARALL DISTR, V25, P1945, DOI 10.1109/TPDS.2013.279
   Narayanan D, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P253
   Park Dongchul, 2012, P USENIX C FIL STOR
   Pitchumani R., 2015, Proceedings of the 8th ACM International Systems and Storage Conference, P18
   Rodrigues Goldwyn., 2014, FLUSHING OUT PDFLUSH
   RUEMMLER C, 1994, COMPUTER, V27, P17, DOI 10.1109/2.268881
   Sang-Phil Lim, 2010, Proceedings of the 2010 International Workshop on Storage Network Architecture and Parallel I/Os (SNAPI 2010), P3, DOI 10.1109/SNAPI.2010.9
   Seagate Technology, NEW FLEX DYN REC MET
   Strunk John D., 2012, Operating Systems Review, V46, P50
   Wang Chunling, 2017, P 33 IEEE S MASS STO
   Western Digital, DYN HYBR SMR
   Wilkes J, 1996, ACM T COMPUT SYST, V14, P108, DOI 10.1145/225535.225539
   Wu CT, 2015, PROC INT CONF PARAL, P450, DOI 10.1109/ICPP.2015.54
   Wu CT, 2012, IEEE INT C CL COMP, P284, DOI 10.1109/CLUSTER.2012.24
   Wu FG, 2021, IEEE T COMPUT, V70, P347, DOI 10.1109/TC.2020.2988257
   Wu FG, 2017, IEEE T COMPUT, V66, P1932, DOI 10.1109/TC.2017.2713360
   Wu Fenggang, 2016, P WORKSH HOT TOP STO
   Wu Fenggang, 2018, P 10 USENIX WORKSH H
   Wu Fenggang, 2019, P 11 USENIX WORKSH H
   Xiao WJ, 2016, PR IEEE COMP DESIGN, P64, DOI 10.1109/ICCD.2016.7753262
   Xie XC, 2019, ACM T STORAGE, V15, DOI 10.1145/3335548
   Yang MC, 2019, IEEE T COMPUT, V68, P111, DOI 10.1109/TC.2018.2845383
   Yang MC, 2017, ICCAD-IEEE ACM INT, P17, DOI 10.1109/ICCAD.2017.8203755
   Yao T, 2019, PROCEEDINGS OF THE 17TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P159
   Zhang BQ, 2020, IEEE T COMPUT, V69, P777, DOI 10.1109/TC.2020.2966194
   Zhang GY, 2010, IEEE T COMPUT, V59, P345, DOI 10.1109/TC.2009.150
   Zhang Guangyan, 2007, ACM T STORAGE, V3
   Zheng W., 2011, USENIX C FILE STORAG, P149
NR 64
TC 3
Z9 3
U1 0
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2021
VL 17
IS 4
AR 32
DI 10.1145/3465404
PG 30
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YY2GY
UT WOS:000754612700008
DA 2024-07-18
ER

PT J
AU Chikhaoui, A
   Lemarchand, L
   Boukhalfa, K
   Boukhobza, J
AF Chikhaoui, Amina
   Lemarchand, Laurent
   Boukhalfa, Kamel
   Boukhobza, Jalil
TI Multi-objective Optimization of Data Placement in a Storage-as-a-Service
   Federated Cloud
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article; Proceedings Paper
CT 14th USENIX Symposium on Operating Systems Design and Implementation
   (OSDI)
CY NOV 04-06, 2020
CL ELECTR NETWORK
SP USENIX Assoc, Alibaba Grp, Amazon, ANT Grp, Alipay, Apple, ByteDance, Facebook, Google, Microsoft, VMware, Oracle, Two Sigma, ACM Queue, DMTF, FreeBSD Fdn
DE Data placement; optimization; cloud; cloud federation; NSGAII
ID EVOLUTIONARY ALGORITHMS; COST; PROFIT
AB Cloud federation enables service providers to collaborate to provide better services to customers. For cloud storage services, optimizing customer object placement for a member of a federation is a real challenge. Storage, migration, and latency costs need to be considered. These costs are contradictory in some cases. In this article, we modeled object placement as a multi-objective optimization problem. The proposed model takes into account parameters related to the local infrastructure, the federated environment, customer workloads, and their SLAs. For resolving this problem, we propose CDP-NSGAIIIR, a Constraint Data Placement matheuristic based on NSGAII with Injection and Repair functions. The injection function aims to enhance the solutions' quality. It consists to calculate some solutions using an exact method then inject them into the initial population of NSGAII. The repair function ensures that the solutions obey the problem constraints and so prevents from exploring large sets of unfeasible solutions. It reduces drastically the execution time of NSGAII. Experimental results show that the injection function improves the HV of NSGAII and the exact method by up to 94% and 60%, respectively, while the repair function reduces the execution time by an average of 68%.
C1 [Chikhaoui, Amina; Lemarchand, Laurent] Univ Brest, Lab STICC, CNRS, UMR 6285, F-29200 Brest, France.
   [Chikhaoui, Amina; Boukhalfa, Kamel] Univ Sci & Technol Houari Boumediene, LSI Lab, Algiers, Algeria.
   [Chikhaoui, Amina] Ecole Normale Super, Algiers, Algeria.
   [Boukhobza, Jalil] ENSTA Bretagne, Lab STICC, CNRS, UMR 6285, Brest, France.
C3 Universite de Bretagne Occidentale; Centre National de la Recherche
   Scientifique (CNRS); University Science & Technology Houari Boumediene;
   Universite de Bretagne Occidentale; Centre National de la Recherche
   Scientifique (CNRS); ENSTA Bretagne
RP Chikhaoui, A (corresponding author), Univ Brest, Lab STICC, CNRS, UMR 6285, F-29200 Brest, France.; Chikhaoui, A (corresponding author), Univ Sci & Technol Houari Boumediene, LSI Lab, Algiers, Algeria.; Chikhaoui, A (corresponding author), Ecole Normale Super, Algiers, Algeria.
EM achikhaoui@usthb.dz; laurent.lemarchand@univ-brest.fr;
   k.boukhalfa@usthb.dz; jalil.boukhobza@ensta-bretagne.fr
RI Boukhobza, Jalil/I-4595-2019; Boukhalfa, Kamel/N-6428-2016
OI Boukhobza, Jalil/0000-0002-2194-4006; Boukhalfa,
   Kamel/0000-0002-9746-579X
CR Alsina J, 2016, INT CONF CLOUD COMP, P352, DOI [10.1109/CloudCom.2016.59, 10.1109/CloudCom.2016.0062]
   Altmann J, 2014, FUTURE GENER COMP SY, V41, P79, DOI 10.1016/j.future.2014.08.014
   Ardekani M.S., 2014, Proceedings of the 11th USENIX Conference on Operating Systems Design and Implementation, OSDI'14, (Berkeley, CA, USA), P367
   Assis MRM, 2016, J NETW COMPUT APPL, V72, P51, DOI 10.1016/j.jnca.2016.06.014
   Assis M. R. M., 2016, DEV INTEROPERABLE FE, P1
   Audet C, 2021, EUR J OPER RES, V292, P397, DOI 10.1016/j.ejor.2020.11.016
   Bouaziz R, 2018, REAL-TIME SYST, V54, P424, DOI 10.1007/s11241-018-9299-6
   Boukhelef D, 2019, FUTURE GENER COMP SY, V93, P176, DOI 10.1016/j.future.2018.10.030
   Boukhelef D, 2016, LECT NOTES COMPUT SC, V9827, P223, DOI 10.1007/978-3-319-44403-1_14
   Boukhobza J, 2017, ENERG MANAG EMBED S, P1
   Boukhobza J., 2013, DATA INTENSIVE STORA, P241, DOI [10.4018/978-1-4666-3934-8.ch015, DOI 10.4018/978-1-4666-3934-8.CH015]
   Boyang Yu, 2015, 2015 IEEE Conference on Computer Communications (INFOCOM). Proceedings, P603, DOI 10.1109/INFOCOM.2015.7218428
   Brunelle Alan D., 2008, BLKTRACE USER GUIDE
   Celesti A, 2012, ACHIEVING FEDERATED AND SELF-MANAGEABLE CLOUD INFRASTRUCTURES: THEORY AND PRACTICE, P1, DOI 10.4018/978-1-4666-1631-8.ch001
   Chikhaoui A, 2018, FED CONF COMPUT SCI, P1025, DOI 10.15439/2018F237
   Coello CAC, 2002, COMPUT METHOD APPL M, V191, P1245, DOI 10.1016/S0045-7825(01)00323-1
   Cooper Brian F., 2010, P 1 ACM S CLOUD COMP, P143, DOI [DOI 10.1145/1807128.1807152, 10.1145/1807128.1807152]
   Darzanos G, 2019, IEEE ACM T NETWORK, V27, P2111, DOI 10.1109/TNET.2019.2943810
   Deb K, 2002, IEEE T EVOLUT COMPUT, V6, P182, DOI 10.1109/4235.996017
   Edmonds A, 2012, IEEE INTERNET COMPUT, V16, P15, DOI 10.1109/MIC.2012.65
   Eswaran K.P., 1974, PROC IFIPS C, P304
   Gu Y, 2014, TSINGHUA SCI TECHNOL, V19, P13, DOI 10.1109/TST.2014.6733204
   Guo LZ, 2012, COMM COM INF SC, V308, P119
   Hota A, 2019, ADV INTELL SYST COMP, V711, P99, DOI 10.1007/978-981-10-8055-5_10
   Hou BB, 2017, ACM T STORAGE, V13, DOI 10.1145/3078838
   Iturriaga S, 2016, IEEE ACM INT SYMP, P596, DOI 10.1109/CCGrid.2016.34
   Jiao B, 2018, EUROMICRO WORKSHOP P, P478, DOI 10.1109/PDP2018.2018.00082
   Jiao L, 2014, IEEE INFOCOM SER, P28, DOI 10.1109/INFOCOM.2014.6847921
   Kakoulli E, 2017, SIGMOD'17: PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P65, DOI 10.1145/3035918.3064023
   Kephart JO, 2003, COMPUTER, V36, P41, DOI 10.1109/MC.2003.1160055
   Khattar N, 2019, ARAB J SCI ENG, V44, P9455, DOI 10.1007/s13369-019-04048-6
   Kim Y., IEEE INT WORKSHOP MO
   Kogias DG, 2016, COMPUTER, V49, P96, DOI 10.1109/MC.2016.344
   Kumar H, 2019, INT J SYST ASSUR ENG, V10, P953, DOI 10.1007/s13198-019-00826-5
   Lee D, 2015, IEEE T CONSUM ELECTR, V61, P215, DOI 10.1109/TCE.2015.7150596
   Lemarchand L, 2018, ADV OPER RES, V2018, DOI 10.1155/2018/8720643
   Li CL, 2019, FUTURE GENER COMP SY, V100, P921, DOI 10.1016/j.future.2019.05.003
   Li HX, 2013, IEEE INFOCOM SER, P25
   Li ZC, 2015, ACM T STORAGE, V11, DOI 10.1145/2700312
   Liu XY, 2015, CIT/IUCC/DASC/PICOM 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER AND INFORMATION TECHNOLOGY - UBIQUITOUS COMPUTING AND COMMUNICATIONS - DEPENDABLE, AUTONOMIC AND SECURE COMPUTING - PERVASIVE INTELLIGENCE AND COMPUTING, P2267, DOI 10.1109/CIT/IUCC/DASC/PICOM.2015.334
   Liu Xiyang., 2016, MATH PROB ENG, V2016
   Lopez Pires Fabio, 2013, 2013 IEEE/ACM 6th International Conference on Utility and Cloud Computing (UCC), P203, DOI 10.1109/UCC.2013.44
   Mahi M, 2018, APPL SOFT COMPUT, V62, P571, DOI 10.1016/j.asoc.2017.11.019
   Mansouri Y, 2019, IEEE T CLOUD COMPUT, V7, P705, DOI 10.1109/TCC.2017.2659728
   Mansouri Y, 2018, ACM COMPUT SURV, V50, DOI 10.1145/3136623
   Mansouri Y, 2016, J NETW COMPUT APPL, V75, P223, DOI 10.1016/j.jnca.2016.08.029
   Moreno-Vozmediano R, 2016, COMM COM INF SC, V567, P325, DOI 10.1007/978-3-319-33313-7_25
   Mukhopadhyay A, 2014, IEEE T EVOLUT COMPUT, V18, P4, DOI 10.1109/TEVC.2013.2290086
   Nedjah N, 2015, INT J BIO-INSPIR COM, V7, P1, DOI 10.1504/IJBIC.2015.067991
   Ouarnoughi Hamza, 2014, REACTION, DOI [10.1145/3041710.3041715, DOI 10.1145/3041710.3041715]
   Pirahandeh Mehdi, 2018, 2018 Tenth International Conference on Ubiquitous and Future Networks (ICUFN), P619, DOI 10.1109/ICUFN.2018.8436594
   Ray BK, 2019, J SUPERCOMPUT, V75, P885, DOI 10.1007/s11227-018-2620-2
   Rebai S, 2015, CONSUM COMM NETWORK, P732, DOI 10.1109/CCNC.2015.7158069
   Resende M.G.C., 2018, Handbook of heuristics, P177
   Riquelme N, 2015, PROC LAT AM COMPUT C, P286, DOI 10.1109/clei.2015.7360024
   Roukh A, 2017, INFORM SYST, V68, P44, DOI 10.1016/j.is.2017.01.003
   Saber T, 2014, LECT NOTES COMPUT SC, V8457, P115, DOI 10.1007/978-3-319-07644-7_9
   Salcedo-Sanz S, 2009, COMPUT SCI REV, V3, P175, DOI 10.1016/j.cosrev.2009.07.001
   Sharaf MA, 2009, ACM SIGMOD/PODS 2009 CONFERENCE, P785
   Sofia AS, 2018, J NETW SYST MANAG, V26, P463, DOI 10.1007/s10922-017-9425-0
   Taherkordi A, 2018, IEEE ACCESS, V6, P74120, DOI 10.1109/ACCESS.2018.2883149
   Terry DB, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P309, DOI 10.1145/2517349.2522731
   Toosi A. N., 2011, Proceedings of the 2011 IEEE 13th International Conference on High Performance Computing and Communication (HPCC 2011). 2011 IEEE International Workshop on Future Trends of Distributed Computing Systems (FTDCS 2011). Workshops of the 2011 International Conference on Ubiquitous Intelligence and Computing (UIC 2011). Workshops of the 2011 International Conference on Autonomic and Trusted Computing (ATC 2011), P279, DOI 10.1109/HPCC.2011.44
   Toosi AN, 2014, ACM COMPUT SURV, V47, DOI 10.1145/2593512
   Toosi AN, 2012, INT CONF UTIL CLOUD, P3, DOI 10.1109/UCC.2012.42
   Van Veldhuizen DA, 2007, Evolutionary Algorithms for Solving Multi-Objective Problems, DOI [10.1007/978-0-387-36797-2, DOI 10.1007/978-0-387-36797-2_5]
   Viotti P, 2017, ACM T STORAGE, V13, DOI 10.1145/3119896
   Voss Stefan, 2009, ANN INFORM SYSTEMS, V10
   Wang PW, 2020, COMPUT INFORM, V39, P51, DOI 10.31577/cai_2020_1-2_51
   Wen ZY, 2017, IEEE T SERV COMPUT, V10, P929, DOI 10.1109/TSC.2016.2543719
   While L, 2006, IEEE T EVOLUT COMPUT, V10, P29, DOI 10.1109/TEVC.2005.851275
   Wu YZ, 2015, IEEE I C EMBED SOFTW, P198, DOI 10.1109/HPCC-CSS-ICESS.2015.89
   Xiao WH, 2017, IEEE T PARALL DISTR, V28, P3114, DOI 10.1109/TPDS.2017.2708120
   Xu XL, 2019, COMPUT INTELL-US, V35, P476, DOI 10.1111/coin.12197
   Zhang LQ, 2013, IEEE J SEL AREA COMM, V31, P2710, DOI 10.1109/JSAC.2013.131211
   Zhang M, 2018, DISTRIB PARALLEL DAT, V36, P339, DOI 10.1007/s10619-017-7215-z
   Zhang N, 2011, PROC VLDB ENDOW, V5, P274, DOI 10.14778/2095686.2095687
   Zhang Q, 2010, J INTERNET SERV APPL, V1, P7, DOI 10.1007/s13174-010-0007-6
   Zitzler E, 2003, IEEE T EVOLUT COMPUT, V7, P117, DOI 10.1109/TEVC.2003.810758
NR 79
TC 8
Z9 8
U1 3
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD AUG
PY 2021
VL 17
IS 3
AR 22
DI 10.1145/3452741
PG 32
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA YY2GJ
UT WOS:000754611000007
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Kougkas, A
   Devarajan, H
   Sun, XH
AF Kougkas, Anthony
   Devarajan, Hariharan
   Sun, Xian-He
TI Bridging Storage Semantics Using Data Labels and Asynchronous I/O
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Label-based I/O; storage bridging; heterogeneous I/O; datalabels;
   task-based; I/O exascale I/O; energy-aware I/O; elastic storage
AB In the era of data-intensive computing, large-scale applications, in both scientific and the BigData communities, demonstrate unique I/O requirements leading to a proliferation of different storage devices and software stacks, many ofwhich have conflicting requirements. Further, newhardware technologies and system designs create a hierarchical composition that may be ideal for computational storage operations. In this article, we investigate how to support a wide variety of conflicting I/O workloads under a single storage system. We introduce the idea of a Label, a new data representation, and, we present LABIOS: a new, distributed, Label-based I/O system. LABIOS boosts I/O performance by up to 17x via asynchronous I/O, supports heterogeneous storage resources, offers storage elasticity, and promotes in situ analytics and software defined storage support via data provisioning. LABIOS demonstrates the effectiveness of storage bridging to support the convergence of HPC and BigData workloads on a single platform.
C1 [Kougkas, Anthony; Devarajan, Hariharan; Sun, Xian-He] IIT, Dept Comp Sci, 10 West 35th St, Chicago, IL 60616 USA.
C3 Illinois Institute of Technology
RP Kougkas, A (corresponding author), IIT, Dept Comp Sci, 10 West 35th St, Chicago, IL 60616 USA.
EM akougkas@iit.edu; hdevarajan@iit.edu; sun@iit.edu
RI Devarajan, Hariharan/HJG-8967-2022
OI Kougkas, Anthony/0000-0003-3943-663X; Devarajan,
   Hariharan/0000-0001-5625-3494
FU National Science Foundation [OCI-1835764, CSR-1814872]
FX This material is based upon work supported by the National Science
   Foundation under Grants No. OCI-1835764 and No. CSR-1814872.
CR Ahn DH, 2020, FUTURE GENER COMP SY, V110, P202, DOI 10.1016/j.future.2020.04.006
   Amazon Inc, 2018, AMAZON S3
   [Anonymous], 2010, P 26 C UNCERTAINTY A
   [Anonymous], 1996, Parallel Programming Using C++
   Bauer M, 2012, INT CONF HIGH PERFOR
   Berl A, 2010, COMPUT J, V53, P1045, DOI 10.1093/comjnl/bxp080
   Bertsimas D, 2002, MANAGE SCI, V48, P550, DOI 10.1287/mnsc.48.4.550.208
   Bhagwat Deepavali M., 2018, U.S. Patent App, Patent No. [15/397,632, 15397632]
   Bhagwat Deepavali M., 2018, U.S. Patent App., Patent No. [15/ 397,601, 15397601]
   Bhimji W, 2016, ACCELERATING SCI NER
   Biddiscombe John., 2011, Eurographics Symposium on Parallel Graphics and Visualization, P91, DOI DOI 10.2312/EGPGV/EGPGV11/091-100
   Breitenfeld M. Scot, 2017, ARXIV1712 ARXIV1712
   Bryan GH, 2002, MON WEATHER REV, V130, P2917, DOI 10.1175/1520-0493(2002)130<2917:ABSFMN>2.0.CO;2
   Carns PH, 2000, USENIX ASSOCIATION PROCEEDINGS OF THE 4TH ANNUAL LINUX SHOWCASE AND CONFERENCE, ATLANTA, P317
   Carns P, 2009, INT PARALL DISTRIB P, P524, DOI 10.1079/9781845933975.0001
   Chameleon.org, 2018, CHAM SYST CHAM SYST
   Chang F, 2008, ACM T COMPUT SYST, V26, DOI 10.1145/1365815.1365816
   Cheriere Nathanael, 2018, THESIS INRIA RENNES THESIS INRIA RENNES
   Cloud Native Computing Foundation, 2018, NATS SERV C CLIENT NATS SERV C CLIENT
   Conway Steve., 2015, When Data Needs More Firepower: The HPC, Analytics Convergence
   Cui XL, 2014, J SUPERCOMPUT, V70, P1249, DOI 10.1007/s11227-014-1225-7
   Curry MatthewL., 2015, Motivation and design of the sirocco storage system version 1.0
   Curtis-Maury M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P419
   D'Ambrosio M., 2011, PROC ACM SIGCOMM WOR, P7, DOI DOI 10.1145/2018584.2018587
   Das S., 2009, P HOTCLOUD, P131
   Devarajan Hariharan, 2017, P ACM SIGHPC DATACLO P ACM SIGHPC DATACLO, V17
   Docan C, 2012, CLUSTER COMPUT, V15, P163, DOI 10.1007/s10586-011-0162-y
   Fengfeng Pan, 2014, Big Data Benchmarks, Performance Optimization, and Emerging Hardware. 4th and 5th Workshops, BPOE 2014. Revised Selected Papers, P85, DOI 10.1007/978-3-319-13021-7_7
   Folk M., 1999, P SUPERCOMPUTING, V99, P5
   Gates Alan., 2012, Hcatalog: An integration tool
   Geambasu R., 2010, OSDI, P323
   Giesen J, 2009, FUND INFORM, V90, P67, DOI 10.3233/FI-2009-0005
   Google Inc, 2018, CITYHASH LIB
   Grant W. Shane, 2017, CER C 11 LIB SER U S CER C 11 LIB SER U S
   Hecht M. G. D., 2015, P IDSR
   Heichler J., 2014, INTRO BEEGFS
   Helong L., 2014, 2014 23 INT C COMP C, P1, DOI [DOI 10.1145/2670979.2670985, DOI 10.1109/ICCCN.2014.6911807]
   Hey T, 2012, COMM COM INF SC, V317, P1
   High Performance Data Division Intel Enterprise Edition for Lustre* Software, 2014, WHITE PAPER BIG DAT WHITE PAPER BIG DAT
   IBM, 2018, HDFS TRANSP HDFS TRANSP
   Intel, 2018, HAD AD LUSTR HAL HAD AD LUSTR HAL
   Iskra K, 2008, PPOPP'08: PROCEEDINGS OF THE 2008 ACM SIGPLAN SYMPOSIUM ON PRINCIPLES AND PRACTICE OF PARALLEL PROGRAMMING, P153, DOI 10.1145/1345206.1345230
   Kim Y. H., 2010, 2010 IEEE 37th International Conference on Plasma Sciences (ICOPS 2010), DOI 10.1109/PLASMA.2010.5534009
   Kougkas A, 2018, HPDC '18: PROCEEDINGS OF THE 27TH INTERNATIONAL SYMPOSIUM ON HIGH-PERFORMANCE PARALLEL AND DISTRIBUTED COMPUTING, P219, DOI 10.1145/3208040.3208059
   Kougkas A, 2018, IEEE INT C CL COMP, P290, DOI 10.1109/CLUSTER.2018.00046
   Kougkas A, 2017, INT J HIGH PERFORM C, V31, P335, DOI 10.1177/1094342016677084
   Kougkas A, 2016, PROCEEDINGS OF PDSW-DISCS 2016 - 1ST JOINT INTERNATIONAL WORKSHOP ON PARALLEL DATA STORAGE AND DATA INTENSIVE SCALABLE COMPUTING SYSTEMS, P43, DOI [10.1109/PDSW-DISCS.2016.012, 10.1109/PDSW-DISCS.2016.11]
   Kougkas Anthony, 2018, P 32 ACM INT C SUP I P 32 ACM INT C SUP I
   Kui Gao, 2009, Proceedings of the 2009 International Conference on Parallel Processing (ICPP 2009), P470, DOI 10.1109/ICPP.2009.68
   Li Jianwei, 2003, SC 03, P39, DOI DOI 10.1109/SC.2003.10053
   Li J, 2014, EUROMICRO, P85, DOI 10.1109/ECRTS.2014.23
   Li KL, 2015, IEEE T COMPUT, V64, P191, DOI 10.1109/TC.2013.205
   Lim H.C., 2010, PROCEEDING 7 INT C A, P1
   Liu J, 2016, IEEE INT SYMP INFO, P1451, DOI 10.1109/ISIT.2016.7541539
   Liu YH, 2015, PROC INT CONF PARAL, P879, DOI 10.1109/ICPP.2015.97
   Lockwood G.K., 2017, Storage 2020: A vision for the future of HPC storage
   Memached, 2018, EXTST PLUG EXTST PLUG
   Mulia WD, 2013, IETE TECH REV, V30, P382, DOI 10.4103/0256-4602.123121
   Oldfield RA, 2014, PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON SUPERCOMPUTING, (ICS'14), P83, DOI 10.1145/2597652.2597668
   Olston C., 2008, Proceedings of the 2008 ACM SIGMOD International Conference on Manage- ment of Data, P1099
   Piernas J., 2007, P 2007 ACM IEEE C SU, P28
   Puchinger J, 2010, INFORMS J COMPUT, V22, P250, DOI 10.1287/ijoc.1090.0344
   Raicu I, 2010, CLUSTER COMPUT, V13, P291, DOI 10.1007/s10586-010-0132-9
   Reed DA, 2015, COMMUN ACM, V58, P56, DOI 10.1145/2699414
   Ren K, 2014, INT CONF HIGH PERFOR, P237, DOI 10.1109/SC.2014.25
   Riedel E., 1998, Proceedings of the Twenty-Fourth International Conference on Very-Large Databases, P62
   Riedel E, 2001, COMPUTER, V34, P68, DOI 10.1109/2.928624
   Shapiro Michael W., 2017, U.S. Patent, Patent No. [9,787,773, 9787773]
   Taylor Monty, 2018, OPENSTACK OBJECT STO
   Thakur R, 1999, FRONTIERS '99 - THE SEVENTH SYMPOSIUM ON THE FRONTIERS OF MASSIVELY PARALLEL COMPUTATION, PROCEEDINGS, P182, DOI 10.1109/FMPC.1999.750599
   Thusoo A, 2009, PROC VLDB ENDOW, V2, P1626, DOI 10.14778/1687553.1687609
   Tiwari Devesh, 2013, FAST 13, P119
   Vilayannur M., 2005, FAST'05: Proceedings of the 4th conference on USENIX Conference on File and Storage Technologies, P2
   Wang Zhenyu., 2000, Task-driven computing
   Weatherspoon H, 2002, LECT NOTES COMPUT SC, V2429, P328
   Weets JF, 2015, 2015 INTERNATIONAL CONFERENCE ON GREEN COMPUTING AND INTERNET OF THINGS (ICGCIOT), P545, DOI 10.1109/ICGCIoT.2015.7380524
   Weil SA, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P307
   Xu J, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P323
   Zaharia Matei, 2010, 2 USENIX WORKSHOP HO
   Zaharia Matei., 2012, NSDI 12
   Zhang SL, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P15
   Zheng FH, 2010, IEEE INT C SOL DIEL
   Zheng Q, 2014, 2014 9TH PARALLEL DATA STORAGE WORKSHOP (PDSW), P1, DOI 10.1109/PDSW.2014.7
   Zhou Shujia., 2008, Parallel and Distributed Processing, P1
NR 84
TC 2
Z9 2
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD NOV
PY 2020
VL 16
IS 4
AR 22
DI 10.1145/3415579
PG 34
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TP8JF
UT WOS:000677839400001
DA 2024-07-18
ER

PT J
AU Gatla, OR
   Zheng, M
   Hameed, M
   Dubeyko, V
   Manzanares, A
   Blagojevic, F
   Guyot, C
   Mateescu, R
AF Gatla, Om Rameshwar
   Zheng, Mai
   Hameed, Muhammad
   Dubeyko, Viacheslav
   Manzanares, Adam
   Blagojevic, Filip
   Guyot, Cyril
   Mateescu, Robert
TI Towards Robust File System Checkers
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Fault tolerance; data corruption; file-system faults
AB File systems may become corrupted for many reasons despite various protection techniques. Therefore, most file systems come with a checker to recover the file system to a consistent state. However, existing checkers are commonly assumed to be able to complete the repair without interruption, which may not be true in practice. In this work, we demonstrate via fault injection experiments that checkers of widely used file systems (EXT4, XFS, BLIPS, and FM) may leave the file system in an uncorrectable state if the repair procedure is interrupted unexpectedly. To address the problem, we first fix the ordering issue in the undo logging of elf sck and then build a general logging library (i.e., rf sck-lib) for strengthening checkers. To demonstrate the practicality, we integrate rfsck-lib with existing checkers and create two new checkers: rfsck-ext, a robust checker for Ext-family file systems, and rf sck-xf s, a robust checker for XFS file systems, both of which require only tens of lines of modification to the original versions. Both rf sck-ext and rfsck-xfs are resilient to faults in our experiments. Also, both checkers incur reasonable performance overhead (i.e., up to 12%) compared to the original unreliable versions. Moreover, rf sck-ext outperforms the patched e2f sck by up to nine times while achieving the same level of robustness.
C1 [Gatla, Om Rameshwar; Zheng, Mai] Iowa State Univ, 2520 Osborn Dr, Ames, IA 50011 USA.
   [Hameed, Muhammad] New Mexico State Univ, 1290 Frenger Mall, Las Cruces, NM 88003 USA.
   [Dubeyko, Viacheslav; Manzanares, Adam; Blagojevic, Filip; Guyot, Cyril; Mateescu, Robert] Western Digital Res, 591 SanDisk Dr, Milpitas, CA 95035 USA.
C3 Iowa State University; New Mexico State University
RP Gatla, OR (corresponding author), Iowa State Univ, 2520 Osborn Dr, Ames, IA 50011 USA.
EM ogatla@iastate.edu; mai@iastate.edu; mkhameed@nmsu.edu;
   vyacheslav.dubeyko@wdc.com; adam.manzanares@wdc.com;
   filip.blagojevic@wdc.com; cyril.guyot@wdc.com; robert.mateescu@wdc.com
RI Zheng, Mai/ABC-4670-2020
OI Dubeyko, Viacheslav/0000-0003-0800-8106
FU National Science Foundation (NSF) [CNS-1566554, CCF-1717630]
FX We thank the anonymous reviewers for their insightful feedback, and the
   Linux practitioners including Theodore Ts'o, Christoph Hellwig, and Ric
   Wheeler, for the invaluable discussion at the Linux FAST Summit. This
   work was supported in part by National Science Foundation (NSF) under
   grants CNS-1566554 and CCF-1717630. Any opinions, findings, and
   conclusions expressed in this material are those of the authors and do
   not necessarily reflect the views of NSF.
CR Agarwal Nitin, 2008, P 2008 USENIX ANN TE, V57
   [Anonymous], 2007, PATCH 1 3 E2FSPROGS
   [Anonymous], 2009, P 42 ANN IEEE ACM IN, DOI DOI 10.1145/1669112.1669118
   [Anonymous], 2015, PATCH 16 31 E2UNDO D
   [Anonymous], 2009, The Datacenter as a Computer: An Introduction to the Design of Warehouse-Scale Machines
   [Anonymous], 2015, P USENIX C FILE STOR
   Bairavasundaram LN, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P223
   Bairavasundaram LN, 2007, PERF E R SI, V35, P289
   Belgal HP, 2002, INT RELIAB PHY SYM, P7, DOI 10.1109/RELPHY.2002.996604
   Bornholt James, 2016, ACM SIGPLAN Notices, V51, P83, DOI 10.1145/2954679.2872406
   BRAND A, 1993, INT REL PHY, P127, DOI 10.1109/RELPHY.1993.283291
   Cai Y, 2012, DES AUT TEST EUROPE, P521
   Cao JR, 2018, INTERNATIONAL CONFERENCE ON SUPERCOMPUTING (ICS 2018), P1, DOI 10.1145/3205289.3205302
   Cao JR, 2016, PROCEEDINGS OF PDSW-DISCS 2016 - 1ST JOINT INTERNATIONAL WORKSHOP ON PARALLEL DATA STORAGE AND DATA INTENSIVE SCALABLE COMPUTING SYSTEMS, P49, DOI [10.1109/PDSW-DISCS.2016.12, 10.1109/PDSW-DISCS.2016.013]
   Carreira JoaoCarlos Menezes., 2012, P 7 ACM EUROPEAN C C, P239
   Chen F., 2009, P ACM JOINT INT C ME
   Chen HG, 2015, SOSP'15: PROCEEDINGS OF THE TWENTY-FIFTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P18, DOI 10.1145/2815400.2815402
   CHEN PM, 1994, ACM COMPUT SURV, V26, P145, DOI 10.1145/176979.176981
   Chidambaram V, 2013, SOSP'13: PROCEEDINGS OF THE TWENTY-FOURTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P228, DOI 10.1145/2517349.2522726
   Chidambaram Vijay, 2012, P 10 C FIL STOR TECH
   Conway A, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P45
   Fryer Daniel, 2012, P 10 C FIL STOR TECH
   Gabrys R., 2012, Proceedings of the 2012 IEEE International Symposium on Information Theory - ISIT, P1000, DOI 10.1109/ISIT.2012.6282078
   Ganger GR, 2000, ACM T COMPUT SYST, V18, P127, DOI 10.1145/350853.350863
   Gatla OR, 2018, PROCEEDINGS OF THE 16TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P105
   Gatla Om Rameshwar, 2017, P 9 USENIX WORKSH HO
   Ghemawat S., 2003, Operating Systems Review, V37, P29, DOI 10.1145/1165389.945450
   GParted Forum, 2009, E2FSCK IS TAK FOR
   Gunawi HS, 2008, PROCEEDINGS OF THE 6TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '08), P207
   Gunawi HS, 2016, PROCEEDINGS OF THE SEVENTH ACM SYMPOSIUM ON CLOUD COMPUTING (SOCC 2016), P1, DOI 10.1145/2987550.2987583
   Gunawi HS., 2008, P 8 USENIX C OPERATI, P131
   Guo Zhenyu, 2013, P 14 WORKSH HOT TOP
   Huang Cheng, 2012, 2012 USENIX ANN TECH, P15
   JaguarPC Forum, 2006, LONG DOES IT TAK FSC
   Jimenez X., 2014, PROC USENIX C FILE S, P47
   Krioukov Andrew, 2008, FAST, V8, P1
   Kurata H., 2006, IEEE Symp. VLSI Circuits, P112, DOI DOI 10.1109/VLSIC.2006.1705335
   Lee Changman, 2015, 13 USENIX C FILE STO, P273
   Lu Lanyue, 2014, P USENIX S OP SYST D, P81
   Lu Lanyue., 2013, Proceedings of the 11th USENIX Conference on File and Storage Technologies, FAST'13, P31
   Lu Youyou, 2013, 11 USENIX C FIL STOR, V13
   Ma A., 2013, P 11 USENIX C FILE S, P1
   Martinez Ashlie, 2017, P 9 USENIX WORKSH HO
   MCKUSICK MK, 1984, ACM T COMPUT SYST, V2, P181, DOI 10.1145/989.990
   Min C, 2015, SOSP'15: PROCEEDINGS OF THE TWENTY-FIFTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P361, DOI 10.1145/2815400.2815422
   Mohan C., 1992, ACM T DATABASE SYSTE
   Ong T., 1993, P S VLSI TECHN VLSI
   Pamies-Juarez L, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P81
   Pillai TS., 2014, P 11 USENIX S OPERAT
   Prabhakaran Vijayan, 2005, P OF THE 20 ACM S ON, V39, P206
   Rajimwale A, 2011, I C DEPEND SYS NETWO, P518, DOI 10.1109/DSN.2011.5958264
   Schroeder B, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 5TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES ( FAST '07), P1
   Schroeder B, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P67
   Sct S.T., 1998, Journaling the linux ext2fs filesystem
   Shvachko K., 2010, SYMPOSIUM, P1, DOI DOI 10.1109/MSST.2010.5496972
   Sigurbjarnarson Helgi, 2016, P 12 USENIX S OP SYS
   Smith K. A., 1997, Performance Evaluation Review, V25, P203, DOI 10.1145/258623.258689
   Svanberg V., 2009, FSCK TAKES TOO LONG
   Sweeney A., 1996, P 1996 USENIX ANN TE, V15
   Tseng Huang-Wei, 2011, P 48 DES AUT C DAC 1
   Tucek Joseph, 2013, P 11 USENIX C FIL ST
   Wang Simeng, 2016, P IEEE INT C NETW AR
   Weil SA, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P307
   Xia M, 2015, P 13 USENIX C FIL ST, P213
   Yadgar G., 2015, P 13 USENIX C FILE S, P257
   Yang JF, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P131
   Yu Cai, 2014, ACM SIGMETRICS Performance Evaluation Review, V42, P491
   Zhang Y., 2010, FAST, P29
   Zhang Y., 2012, FAST
   Zheng M, 2017, ACM T COMPUT SYST, V34, DOI 10.1145/2992782
   Zheng Mai., 2014, Proceedings of the 11th USENIX Symposium on Operating Systems Design and Implementation OSDI '14, P449
NR 71
TC 10
Z9 11
U1 1
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD DEC
PY 2018
VL 14
IS 4
AR 35
DI 10.1145/3281031
PG 25
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ4JO
UT WOS:000457140500007
DA 2024-07-18
ER

PT J
AU Wang, CD
   Wei, QS
   Wu, LK
   Wang, SB
   Chen, C
   Xiao, XK
   Yang, J
   Xue, MD
   Yang, YC
AF Wang, Chundong
   Wei, Qingsong
   Wu, Lingkun
   Wang, Sibo
   Chen, Cheng
   Xiao, Xiaokui
   Yang, Jun
   Xue, Mingdi
   Yang, Yechao
TI Persisting RB-Tree into NVM in a Consistency Perspective
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Red-black tree; cascade versioning
ID PHASE-CHANGE MEMORY; MAIN
AB Byte-addressable non-volatile memory (NVM) is going to reshape conventional computer systems. With advantages of low latency, byte-addressability, and non-volatility, NVM can be directly put on the memory bus to replace DRAM. As a result, both system and application softwares have to be adjusted to perceive the fact that the persistent layer moves up to the memory. However, most of the current in-memory data structures will be problematic with consistency issues if not well tuned with NVM. This article places emphasis on an important in-memory structure that is widely used in computer systems, i.e., the Red/Black-tree (RB-tree). Since it has a long and complicated update process, the RB-tree is prone to inconsistency problems with NVM. This article presents an NVM-compatible consistent RB-tree with a new technique named cascade-versioning. The proposed RB-tree (i) is all-time consistent and scalable and (ii) needs no recovery procedure after system crashes. Experiment results show that the RB-tree for NVM not only achieves the aim of consistency with insignificant spatial overhead but also yields comparable performance to an ordinary volatile RB-tree.
C1 [Wang, Chundong] Singapore Univ Technol & Design, 8 Somapah Rd, Singapore, Singapore.
   [Wang, Chundong; Wei, Qingsong; Wu, Lingkun; Chen, Cheng; Yang, Jun; Xue, Mingdi; Yang, Yechao] ASTAR, Data Storage Inst, 1 Fusionopolis Way, Singapore, Singapore.
   [Wang, Sibo] Univ Queensland, Brisbane, Qld 4072, Australia.
   [Wang, Sibo; Chen, Cheng; Xiao, Xiaokui] Nanyang Technol Univ, 50 Nanyang Ave, Singapore, Singapore.
C3 Singapore University of Technology & Design; Agency for Science
   Technology & Research (A*STAR); A*STAR - Data Storage Institute;
   University of Queensland; Nanyang Technological University
RP Wei, QS (corresponding author), ASTAR, Data Storage Inst, 1 Fusionopolis Way, Singapore, Singapore.
EM cd_wang@outlook.com; WEI_Qingsong@dsi.a-star.edu.sg;
   wangsibo.victor@gmail.com; CHEN_Cheng@dsi.a-star.edu.sg;
   xkxiao@ntu.edu.sg; yangju@dsi.a-star.edu.sg;
   XUE_Mingdi@dsi.a-star.edu.sg; yangyc@dsi.a-star.edu.sg
RI WEI, Qingsong/P-4159-2019; Wang, Sibo/ABE-6444-2020; Xiao,
   Xiaokui/A-3739-2011
OI Wang, Sibo/0000-0003-1892-6971; Xiao, Xiaokui/0000-0003-0914-4580
CR [Anonymous], 2008, P 8 USENIX C OP SYST
   [Anonymous], 2011, FAST
   [Anonymous], 2016, 248966033 INT
   [Anonymous], CMUPDL11114 CARN MEL
   [Anonymous], ARXCIS NV DIMM
   [Anonymous], P IEEE INT EL DEV M
   [Anonymous], 2012, HPL2012236
   [Anonymous], 3D XPOINT TECHN
   Becker B., 1996, VLDB Journal, V5, P264, DOI 10.1007/s007780050028
   Chang HS, 2015, ACM T DES AUTOMAT EL, V20, DOI 10.1145/2699831
   Chang YM, 2016, ACM T DES AUTOMAT EL, V22, DOI 10.1145/2905364
   Chen Cheng, 2016, P IEEE INT C MASSIVE, P1
   Chen SM, 2015, PROC VLDB ENDOW, V8, P786, DOI 10.14778/2752939.2752947
   Coburn J, 2011, ACM SIGPLAN NOTICES, V46, P105, DOI [10.1145/1961295.1950380, 10.1145/1961296.1950380]
   Condit J, 2009, SOSP'09: PROCEEDINGS OF THE TWENTY-SECOND ACM SIGOPS SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P133
   Dhiman G, 2009, DES AUT CON, P664
   Ferreira AP, 2010, DES AUT TEST EUROPE, P914
   Gal E, 2005, ACM COMPUT SURV, V37, P138, DOI 10.1145/1089733.1089735
   Hennessy J.L., 2006, Computer Architecture: A Quantitative Approach'', V4th
   Hu JT, 2015, IEEE T VLSI SYST, V23, P654, DOI 10.1109/TVLSI.2014.2321571
   Lee BC, 2009, CONF PROC INT SYMP C, P2, DOI 10.1145/1555815.1555758
   Liu D, 2013, ASIA S PACIF DES AUT, P279, DOI 10.1109/ASPDAC.2013.6509609
   Luo HZ, 2016, DES AUT CON, DOI 10.1145/2897937.2898106
   Mandelman JA, 2002, IBM J RES DEV, V46, P187, DOI 10.1147/rd.462.0187
   Nalli S, 2017, ACM SIGPLAN NOTICES, V52, P135, DOI 10.1145/3093336.3037730
   Narayanan Dushyanth, 2012, P 17 INT C ARCH SUPP, P401, DOI DOI 10.1145/2150976.2151018
   Pelley S, 2014, CONF PROC INT SYMP C, P265, DOI 10.1109/ISCA.2014.6853222
   Qiu KN, 2016, IEEE T COMPUT, V65, P2313, DOI 10.1109/TC.2015.2479605
   Qureshi Moinuddin K., 2009, Proceedings of the 2009 42nd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO 2009), P14, DOI 10.1145/1669112.1669117
   Qureshi MK, 2009, CONF PROC INT SYMP C, P24, DOI 10.1145/1555815.1555760
   Sun ZY, 2011, INT SYMP MICROARCH, P329
   Volos H, 2015, Proceedings of the 16th Annual Middleware Conference, P37, DOI 10.1145/2814576.2814806
   Volos H, 2011, ACM SIGPLAN NOTICES, V46, P91, DOI [10.1145/1961296.1950379, 10.1145/1961295.1950379]
   Wang TZ, 2015, ACM T EMBED COMPUT S, V14, DOI 10.1145/2697394
   Wei QS, 2017, SC'17: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS, DOI 10.1145/3126908.3126940
   Wei QS, 2015, ACM T STORAGE, V11, DOI 10.1145/2766453
   WU M, 1994, SIGPLAN NOTICES, V29, P86, DOI 10.1145/195470.195506
   Wu Xiaojian., 2011, Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis (SC '11), p39:1
   Xu Cong, 2013, P 50 ACM EDAC IEEE D, P1, DOI DOI 10.1145/2463209.2488867
   Yang Jun, 2015, P 13 USENIX C FIL ST, P167
   Zhao MY, 2014, PR IEEE COMP DESIGN, P16, DOI 10.1109/ICCD.2014.6974656
NR 41
TC 8
Z9 10
U1 0
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD APR
PY 2018
VL 14
IS 1
SI SI
AR 6
DI 10.1145/3177915
PG 27
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GH5ZN
UT WOS:000433517600006
DA 2024-07-18
ER

PT J
AU Hu, YC
   Li, XL
   Zhang, M
   Lee, PPC
   Zhang, XY
   Zhou, P
   Feng, D
AF Hu, Yuchong
   Li, Xiaolu
   Zhang, Mi
   Lee, Patrick P. C.
   Zhang, Xiaoyang
   Zhou, Pan
   Feng, Dan
TI Optimal Repair Layering for Erasure-Coded Data Centers: From Theory to
   Practice
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Erasure coding
ID DISTRIBUTED STORAGE; REGENERATING CODES; SYSTEM
AB Repair performance in hierarchical data centers is often bottlenecked by cross-rack network transfer. Recent theoretical results show that the cross-rack repair traffic can be minimized through repair layering, whose idea is to partition a repair operation into inner-rack and cross-rack layers. However, how repair layering should be implemented and deployed in practice remains an open issue. In this article, we address this issue by proposing a practical repair layering framework called DoubleR. We design two families of practical double regenerating codes (DRC), which not only minimize the cross-rack repair traffic but also have several practical properties that improve state-of-the-art regenerating codes. We implement and deploy DoubleR atop the Hadoop Distributed File System (HDFS) and show that DoubleR maintains the theoretical guarantees of DRC and improves the repair performance of regenerating codes in both node recovery and degraded read operations.
C1 [Hu, Yuchong; Zhang, Xiaoyang; Zhou, Pan; Feng, Dan] Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan 430074, Hubei, Peoples R China.
   [Li, Xiaolu; Zhang, Mi; Lee, Patrick P. C.] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.
C3 Huazhong University of Science & Technology; Chinese University of Hong
   Kong
RP Lee, PPC (corresponding author), Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.
EM yuchonghu@hust.edu.cn; lixl@cse.cuhk.edu.hk; mzhang@cse.cuhk.edu.hk;
   pclee@cse.cuhk.edu.hk; zhangxiaoyang1993@gmail.com; panzhou@hust.edu.cn;
   dfeng@hust.edu.cn
RI Lee, Patrick P. C./I-7165-2013
OI Lee, Patrick P. C./0000-0002-4501-4364
FU National Natural Science Foundation of China [61502191, 61401169,
   61502190, 61772222]; Fundamental Research Funds for the Central
   Universities [2017KFYXJJ065]; Hubei Provincial Natural Science
   Foundation of China [2016CFB226]; Key Laboratory of Information Storage
   System Ministry of Education of China; Research Grants Council of Hong
   Kong [GRF 14216316, CRF C7036-15G]
FX This work was supported in part by the National Natural Science
   Foundation of China (grants 61502191, 61401169, 61502190, and 61772222),
   Fundamental Research Funds for the Central Universities (grant
   2017KFYXJJ065), the Hubei Provincial Natural Science Foundation of China
   (grant 2016CFB226), Key Laboratory of Information Storage System
   Ministry of Education of China, and Research Grants Council of Hong Kong
   (grants GRF 14216316 and CRF C7036-15G).
CR Aguilera Marcos K., 2013, INT C PRINC DISTR SY
   Ahmad Faraz., 2014, 2014 USENIX ANN TECH, P1
   [Anonymous], 2015, PROC USENIX ATC
   [Anonymous], P IEEE INT S MODELIN
   [Anonymous], 2010, P 2 USENIX C HOT TOP
   [Anonymous], 2017, IEEE T INFORM THEORY
   Benson T., 2010, P 10 ACM SIGCOMM C I, P267, DOI DOI 10.1145/1879141.1879175
   Bhagwan R., 2004, NSDI 04, P25
   Calder B, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P143
   Chen HCH, 2014, IEEE T COMPUT, V63, P31, DOI 10.1109/TC.2013.167
   Cho B., 2012, P 2012 USENIX ANN TE, P40
   Chowdhury M, 2013, ACM SIGCOMM COMP COM, V43, P231, DOI 10.1145/2534169.2486021
   Cisco Systems, 2016, OV DENS BEST PRACT
   Dean Jeffrey, 2004, OSDI 04, P10
   Dimakis AG, 2010, IEEE T INFORM THEORY, V56, P4539, DOI 10.1109/TIT.2010.2054295
   Ford D., 2010, P 9 USENIX C OP SYST, P61
   Gastón B, 2013, IEEE DATA COMPR CONF, P491, DOI 10.1109/DCC.2013.72
   Ghemawat S., 2003, ACM SIGOPS OPERATING, V37, P29, DOI [DOI 10.1145/945445.945450, DOI 10.1145/1165389.945450, 10.1145/1165389.945450]
   Goparaju S, 2017, IEEE T INFORM THEORY, V63, P6318, DOI 10.1109/TIT.2017.2690662
   Greenberg A, 2009, SIGCOMM 2009, P51
   Huang Cheng, 2012, 2012 USENIX ANN TECH, P15
   Hui YC, 2016, IEEE INT SYMP INFO, P245, DOI 10.1109/ISIT.2016.7541298
   JALAPARTI V, 2015, P ACM SIGCOMM, V45, P407, DOI DOI 10.1145/2785956.2787488
   Khan O., 2012, FAST
   Li Mingqiang, 2016, IEEE INTERNET COMPUT, VPP, P1, DOI DOI 10.1080/15472450.2016.1215921
   Li RH, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P567
   Li RH, 2015, IEEE T COMPUT, V64, P1898, DOI 10.1109/TC.2014.2349518
   Mitra S., 2016, P 11 EUR C COMP SYST, P30
   Muralidhar S., 2014, P 11 USENIX C OP SYS, P383
   Ovsiannikov M, 2013, PROC VLDB ENDOW, V6, P1092, DOI 10.14778/2536222.2536234
   Pamies-Juarez L, 2016, 14TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST '16), P81
   Papailiopoulos DS, 2012, IEEE INFOCOM SER, P2801, DOI 10.1109/INFCOM.2012.6195703
   Pernas Jaume, 2013, P 2013 IEEE INT S IN
   Rashmi K., 2015, USENIX C FIL STOR TE, P81
   Rashmi KV, 2011, IEEE T INFORM THEORY, V57, P5227, DOI 10.1109/TIT.2011.2159049
   Rashmi K.V., 2013, P 5 USENIX C HOT TOP, P8
   RASHMI KV, 2014, PROC ACM CONF SIGC, V44, P331, DOI DOI 10.1145/2619239.2626325
   REED IS, 1960, J SOC IND APPL MATH, V8, P300, DOI 10.1137/0108018
   Sasidharan B., 2016, ARXIV160707335
   Sathiamoorthy M, 2013, PROC VLDB ENDOW, V6, P325, DOI 10.14778/2535573.2488339
   Schroeder B, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 5TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES ( FAST '07), P1
   Shah NB, 2012, IEEE T INFORM THEORY, V58, P2134, DOI 10.1109/TIT.2011.2178588
   Shah NB, 2012, IEEE T INFORM THEORY, V58, P1837, DOI 10.1109/TIT.2011.2173792
   Shvachko K., 2010, 2010 IEEE 26th Symposium on Mass Storage Systems and Technologies (MSST), P1
   Silberstein Mark., 2014, Proceedings of International Conference on Systems and Storage, P1
   Suh CH, 2011, IEEE T INFORM THEORY, V57, P1425, DOI 10.1109/TIT.2011.2105003
   Tamo I, 2013, IEEE T INFORM THEORY, V59, P1597, DOI 10.1109/TIT.2012.2227110
   Tebbi MA, 2014, INFO THEOR WORKSH, P55, DOI 10.1109/ITW.2014.6970791
   Vahdat A, 2010, IEEE MICRO, V30, P29, DOI 10.1109/MM.2010.72
   Weihang Jiang, 2008, ACM Transaction on Storage, V4, DOI 10.1145/1416944.1416946
   Wu YN, 2009, IEEE INT SYMP INFO, P2276, DOI 10.1109/ISIT.2009.5205898
   Xia M, 2015, P 13 USENIX C FIL ST, P213
   Ye M, 2017, IEEE T INFORM THEORY, V63, P6307, DOI 10.1109/TIT.2017.2730863
   Ye M, 2017, IEEE T INFORM THEORY, V63, P2001, DOI 10.1109/TIT.2017.2661313
   Zhirong Shen, 2016, 2016 46th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN). Proceedings, P323, DOI 10.1109/DSN.2016.37
NR 55
TC 39
Z9 42
U1 0
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD DEC
PY 2017
VL 13
IS 4
SI SI
AR 33
DI 10.1145/3149349
PG 24
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FR4LD
UT WOS:000419035900006
DA 2024-07-18
ER

PT J
AU Chen, M
   Bangera, GB
   Hildebrand, D
   Jalia, F
   Kuenning, G
   Nelson, H
   Zadok, E
AF Chen, Ming
   Bangera, Geetika Babu
   Hildebrand, Dean
   Jalia, Farhaan
   Kuenning, Geoff
   Nelson, Henry
   Zadok, Erez
TI vNFS: Maximizing NFS Performance with Compounds and Vectorized I/O
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE NFS; vNFS; compound procedures; POSIX; File-system API
AB Modern systems use networks extensively, accessing both services and storage across local and remote networks. Latency is a key performance challenge, and packing multiple small operations into fewer large ones is an effective way to amortize that cost, especially after years of significant improvement in bandwidth but not latency. To this end, the NFSv4 protocol supports a compounding feature to combine multiple operations. Yet compounding has been underused since its conception because the synchronous POSIX file-system API issues only one (small) request at a time.
   We propose vNFS, an NFSv4.1-compliant client that exposes a vectorized high-level API and leverages NFS compound procedures to maximize performance. We designed and implemented vNFS as a user-space RPC library that supports an assortment of bulk operations on multiple files and directories. We found it easy to modify several UNIX utilities, an HTTP/2 server, and Filebench to use vNFS. We evaluated vNFS under a wide range of workloads and network latency conditions, showing that vNFS improves performance even for low-latency networks. On high-latency networks, vNFS can improve performance by as much as two orders of magnitude.
C1 [Chen, Ming; Bangera, Geetika Babu; Jalia, Farhaan; Zadok, Erez] SUNY Stony Brook, 349 New Comp Sci, Stony Brook, NY 11790 USA.
   [Hildebrand, Dean] IBM Res Almaden, 650 Harry Rd, San Jose, CA 95120 USA.
   [Kuenning, Geoff] Harvey Mudd Coll, Dept Comp Sci, 301 Platt Blvd, Claremont, CA 91711 USA.
   [Nelson, Henry] Ward Melville High Sch, 380 Old Town Rd, East Setauket, NY 11733 USA.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Stony Brook; International Business Machines (IBM); Claremont
   Colleges; Harvey Mudd College
RP Chen, M (corresponding author), SUNY Stony Brook, 349 New Comp Sci, Stony Brook, NY 11790 USA.
EM mchen@cs.stonybrook.edu; gbangera@cs.stonybrook.edu; dhildeb@us.ibm.com;
   farhaan@cs.stonybrook.edu; geoff@cs.hmc.edu; hcnelson99@gmail.com;
   ezk@cs.stonybrook.edu
OI Kuenning, Geoff/0000-0002-3882-2072
FU NSF [CNS-1251137, CNS-1302246, CNS-1305360, CNS-1622832]; ONR
   [N00014-16-1-2264]; IBM; Dell-EMC; NetApp
FX This work was made possible in part thanks to Dell-EMC, NetApp, and IBM
   support; NSF awards CNS-1251137, CNS-1302246, CNS-1305360, and
   CNS-1622832; and ONR award N00014-16-1-2264.
CR [Anonymous], 2005, P 22 IEEE 13 NASA GO
   Atlidakis Vaggelis., 2016, EUROSYS PAGE, P19
   Axboe J., 2007, CFQ IO Scheduler
   Batsakis Alexandros, 2009, ACM T STORAGE, V5, P4
   Belshe M., 2015, RFC 7540
   Bent J., 2004, First Symposium on Networked Systems Design and Implementation (NSDI '04), P365
   Beroux Werner, 2016, RENAME IT
   Callaghan B., 1995, RFC 1813
   Chen M, 2016, PROCEEDINGS OF THE 9TH ACM INTERNATIONAL SYSTEMS AND STORAGE CONFERENCE (SYSTOR'16), DOI 10.1145/2928275.2928282
   Chen M, 2017, PROCEEDINGS OF FAST '17: 15TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P301
   Cheshire Stuart., 2005, TCP Performance problems caused by interaction between Nagle's Algorithm and Delayed ACK
   Deniel Philippe., 2007, Linux Symposium, P113
   Duchamp Dan, 1994, P SUMM 1994 USENIX T, P143
   Ellard D, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE FREENIX TRACK, P101
   Fitzpatrick Jason, 2016, BULK RENAME UTILITY
   Haardt M., 1999, FTW 3 LINUX PROGRAMM
   Han Sangjin, 2012, USENIX OSDI
   Haynes T., 2015, NFS VERSION IN PRESS
   Ibrahim M, 2015, INT C ELECT COMPUT
   Jensen Kim, 2016, ADVANCEDRENAMER
   Juszczak Chet, 1994, P WINTER 1994 USENIX, P1
   Lever Chuck, CLOSE TO OPEN CACHE
   Linux Programmer's Manual, OP 2 OP POSS CREAT F
   Linux Programmer's Manual, LIO LIST 3 IN LIST I
   McDonald Alex, 2012, USENIX MAGAZINE, V37, P28
   Nagle J., 1984, 896 RFC NETW WORK GR
   Potten Antoine, 2016, ANT RENAMER
   Purohit Amit, 2003, P 2003 ACM WORKSH HO, P109
   Rumble S. M., 2011, P 13 USENIX C HOT TO
   Shepler S., 2010, 5661 RFC NETW WORK G
   Shepler S., 2003, 3530 RFC NETW WORK G
   SPEC, 2001, SPEC SFS97 R1 V30
   Steere David C., 1997, P 9 ACM S OP SYST PR
   Sun Microsystems, 1989, 1094 RFC NETW WORK G
   Tarasov Vasily, 2016, USENIX; login, V41, P6
   Tarasov Vasily, 2015, HOTSTORAGE 15
   Tsai CC, 2015, SOSP'15: PROCEEDINGS OF THE TWENTY-FIFTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P441, DOI 10.1145/2815400.2815405
   Vasudevan Vijay., 2012, Proceedings of the Third ACM Symposium on Cloud Computing, SoCC '12, p8:1
   Vasudevan Vijay, 2011, P 13 USENIX C HOT TO, P31
   Vilayannur M, 2008, Tech. Rep. ANL/MCS-TM-302
   Waldrop MM, 2016, NATURE, V530, P144, DOI 10.1038/530144a
   Zhang S., 2016, P 14 USENIX C FIL ST
NR 42
TC 4
Z9 5
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD OCT
PY 2017
VL 13
IS 3
SI SI
AR 21
DI 10.1145/3116213
PG 24
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FL6DV
UT WOS:000414336300004
DA 2024-07-18
ER

PT J
AU Dragga, C
   Santry, DJ
AF Dragga, Chris
   Santry, Douglas J.
TI GCTrees: Garbage Collecting Snapshots
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article; Proceedings Paper
CT 31st International Conference on Massive Storage Systems and
   Technologies (MSST 2015)
CY JUN 01-05, 2015
CL Santa Clara, CA
DE Algorithms; Design; Performance; File systems; data management;
   snapshots; clones
AB File-system snapshots have been a key component of enterprise storage management since their inception. Creating and managing them efficiently, while maintaining flexibility and low overhead, has been a constant struggle. Although the current state-of-the-art mechanism-hierarchical reference counting-performs reasonably well for traditional small-file workloads, these workloads are increasingly vanishing from the enterprise data center, replaced instead with virtual machine and database workloads. These workloads center around a few very large files, violating the assumptions that allow hierarchical reference counting to operate efficiently. To better cope with these workloads, we introduce Generational Chain Trees (GCTrees), a novel method of space management that uses concepts of block lineage across snapshots rather than explicit reference counting. As a proof of concept, we create a prototype file system-gcext4, a modified version of ext4 that uses GCTrees as a basis for snapshots and copy-on-write. In evaluating this prototype empirically, we find that although they have a somewhat higher overhead for traditional workloads, GCTrees have dramatically lower overhead than hierarchical reference counting for large-file workloads, improving by a factor of 34 or more in some cases. Furthermore, gcext4 performs comparably to ext4 across all workloads, showing that GCTrees impose minor cost for their benefits.
C1 [Dragga, Chris] NetApp Inc, Adv Technol Grp, 7301 Kit Creek Rd, Res Triangle Pk, NC 27709 USA.
   [Santry, Douglas J.] NetApp Inc, Adv Technol Grp, D-85551 Kirchheim, Germany.
C3 NetApp, Inc.; NetApp, Inc.
RP Dragga, C (corresponding author), NetApp Inc, Adv Technol Grp, 7301 Kit Creek Rd, Res Triangle Pk, NC 27709 USA.; Santry, DJ (corresponding author), NetApp Inc, Adv Technol Grp, D-85551 Kirchheim, Germany.
EM cdragga@netapp.com; dsantry@netapp.com
CR Amir G., 2011, NEXT3 SNAPSHOT DESIG
   [Anonymous], 2005, ACM Transactions on Storage
   [Anonymous], 2010, RJ10464 IBM
   Daniel S, 2005, I S WORKL CHAR PROC, P174, DOI 10.1109/IISWC.2005.1526014
   Dragga Chris, 2015, P 31 INT C MASS STOR, P12
   Edwards JohnK., 2008, ATC 08, P129
   Hitz Dave, 1994, P USENUX WINT TECH C
   Mathur Avantika, 2007, P LINUX S, V2, P21
   McKusick Marshall Kirk, 1999, P USENIX ANN TECHN C, P18
   MCKUSICK MK, 1984, ACM T COMPUT SYST, V2, P181, DOI 10.1145/989.990
   Nicolae B, 2011, J PARALLEL DISTR COM, V71, P169, DOI 10.1016/j.jpdc.2010.08.004
   Pike R., 1990, UKUUG. UNIX - The Legend Evolves. Proceedings of the Summer 1990 UKUUG Conference, P1
   QUINLAN S, 1991, SOFTWARE PRACT EXPER, V21, P1289, DOI 10.1002/spe.4380211203
   Quinlan S., 2003, FOSSIL ARCH FILE SER
   Rodeh Ohad, 2008, ACM Transaction on Storage, V3, p15:1, DOI 10.1145/1326542.1326544
   Rodeh O, 2013, ACM T STORAGE, V9, DOI 10.1145/2501620.2501623
   Santry D. S., 2000, SIGOPS OPER SYST REV, V34, P18, DOI DOI 10.1145/346152.346180
   Soules CA, 2003, USENIX ASSOCIATION PROCEEDINGS OF THE 2ND USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES (FAST'03), P43
   Twigg Andy, 2011, HOTSTORAGE 11, P1
NR 19
TC 5
Z9 5
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JAN
PY 2016
VL 12
IS 1
SI SI
AR 4
DI 10.1145/2857056
PG 32
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DI5EI
UT WOS:000373521300004
DA 2024-07-18
ER

PT J
AU Cho, S
   Park, C
   Won, Y
   Kang, S
   Cha, J
   Yoon, S
   Choi, J
AF Cho, Seokhei
   Park, Changhyun
   Won, Youjip
   Kang, Sooyong
   Cha, Jaehyuk
   Yoon, Sungroh
   Choi, Jongmoo
TI Design Tradeoffs of SSDs: From Energy Consumption's Perspective
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Measurement; SSD; NAND flash; energy consumption; FTL;
   parallelism; simulator
AB In this work, we studied the energy consumption characteristics of various SSD design parameters. We developed an accurate energy consumption model for SSDs that computes aggregate, as well as component-specific, energy consumption of SSDs in sub-msec time scale. In our study, we used five different FTLs (page mapping, DFTL, block mapping, and two different hybrid mappings) and four different channel configurations (two, four, eight, and 16 channels) under seven different workloads (from large-scale enterprise systems to small-scale desktop applications) in a combinatorial manner. For each combination of the aforementioned parameters, we examined the energy consumption for individual hardware components of an SSD (micro-controller, DRAM, NAND flash, and host interface). The following are some of our findings. First, DFTL is the most energy-efficient address-mapping scheme among the five FTLs we tested due to its good write amplification and small DRAM footprint. Second, a significant fraction of energy is being consumed by idle flash chips waiting for the completion of NAND operations in the other channels. FTL should be designed to fully exploit the internal parallelism so that energy consumption by idle chips is minimized. Third, as a means to increase the internal parallelism, increasing way parallelism (the number of flash chips in a channel) is more effective than increasing channel parallelism in terms of peak energy consumption, performance, and hardware complexity. Fourth, in designing high-performance and energy-efficient SSDs, channel switching delay, way switching delay, and page write latency need to be incorporated in an integrated manner to determine the optimal configuration of internal parallelism.
C1 [Cho, Seokhei; Park, Changhyun] Hanyang Univ, Dept Comp Software, Seoul 133791, South Korea.
   [Won, Youjip; Kang, Sooyong; Cha, Jaehyuk] Hanyang Univ, Div Comp Sci & Engn, Seoul, South Korea.
   [Yoon, Sungroh] Seoul Natl Univ, Dept Elect & Comp Engn, Seoul 151742, South Korea.
   [Choi, Jongmoo] Dankook Univ, Dept Software, Yongin 448701, Gyeonggi Do, South Korea.
C3 Hanyang University; Hanyang University; Seoul National University (SNU);
   Dankook University
RP Won, Y (corresponding author), Hanyang Univ, Div Comp Sci & Engn, Seoul, South Korea.
EM misost@hanyang.ac.kr; pch1984@hanyang.ac.kr; yjwon@hanyang.ac.kr;
   sykang@hanyang.ac.kr; chajh@hanyang.ac.kr; sryoon@snu.ac.kr;
   choijm@dankook.ac.kr
RI Cha, Jaeho/AAP-6886-2020
OI Cha, Jaehyuk/0000-0003-4869-901X
FU IT R&D program MKE/KEIT [10035202]; MSIP (Ministry of Science, ICT &
   Future Planning), Korea, under the ITRC (Information Technology Research
   Center) [NIPA-2014-H0301-14-1017]; National Research Foundation of Korea
   [22A20130012575] Funding Source: Korea Institute of Science & Technology
   Information (KISTI), National Science & Technology Information Service
   (NTIS)
FX This work is sponsored by IT R&D program MKE/KEIT [No. 10035202, Large
   Scale hyper-MLC SSD Technology Development] and by the MSIP (Ministry of
   Science, ICT & Future Planning), Korea, under the ITRC (Information
   Technology Research Center) support program (NIPA-2014-H0301-14-1017)
   supervised by the NIPA (National IT Industry Promotion Agency).
CR Agrawal Nitin, 2008, P USENIX ANN TECHN C, P57
   [Anonymous], [No title captured]
   [Anonymous], [No title captured]
   [Anonymous], 2008, COMP AID DES 2008 IC
   [Anonymous], T STORAGE
   Ban A., 1999, U.S. Patent, Patent No. 5937425
   Ban Amir, 1995, US Patent, Patent No. [5,404,485, 5404485]
   Caulfield A. M., 2009, P 42 ANN IEEE ACM IN, P24
   Cheng-Yang Fu, 2011, Proceedings 2011 Design, Automation & Test in Europe
   Debnath Biplob, 2011, 2011 IEEE 27 S MASS, P1
   ESOSLab, 2012, LIN APPL TRAC
   FRACHTENBERG E, 2011, P 2011 INT C HIGH PE, P27
   Fusion-IO, 2011, WPCH031511
   Gal E, 2005, ACM COMPUT SURV, V37, P138, DOI 10.1145/1089733.1089735
   Grider Gary, 2011, EXASCALE FSIO CAN WE
   Gupta A, 2009, ACM SIGPLAN NOTICES, V44, P229, DOI 10.1145/1508284.1508271
   He Jiahua., 2010, P 2010 ACMIEEE INT C, P1
   Hu Yang., 2010, Mass Storage Systems and Technologies (MSST), 2010 IEEE 26th Symposium on, P1, DOI DOI 10.1109/INFCOM.2010.5462089
   IC-insights, 2012, IC INS RES B
   Inoue T., 2011, Proceedings of the 2011 International Conference on Complex, Intelligent and Software Intensive Systems (CISIS 2011), P612, DOI 10.1109/CISIS.2011.101
   Intel, 2009, INT X25 M X18 M MAIN
   Intel, 2012, INT SOL STAT DRIV 32
   Intel, 2009, MD332B NAND FLASH ME
   Jung Myoungsoo., 2012, Mass Storage Systems and Technologies (MSST), 2012 IEEE 28th Symposium on, P1
   Kavalanekar Swaroop, 2008, 2008 IEEE International Symposium on Workload Characterization (IISWC), P119, DOI 10.1109/IISWC.2008.4636097
   Kim J, 2002, IEEE T CONSUM ELECTR, V48, P366
   Lee S, 2013, DES AUT TEST EUROPE, P1522
   Liu D, 2012, DES AUT TEST EUROPE, P1447
   Liu D, 2012, IEEE T VLSI SYST, V20, P1094, DOI 10.1109/TVLSI.2011.2142015
   Micron, 2013, 128GB 1TB AS SYNCHR
   Murugan Muthukumar., 2011, P 2011 IEEE 27 S MAS, P1
   Narayanan D, 2009, EUROSYS'09: PROCEEDINGS OF THE FOURTH EUROSYS CONFERENCE, P145
   Ni X, 2012, IEEE INT C CL COMP, P364, DOI 10.1109/CLUSTER.2012.82
   ONFI, 2011, OP NAND FLASH INT ON
   Park J, 2009, LECT NOTES COMPUT SC, V5860, P24, DOI 10.1007/978-3-642-10265-3_3
   Park SH, 2009, IEEE T CONSUM ELECTR, V55, P1392, DOI 10.1109/TCE.2009.5278005
   Pillai Padmanabhan, 2012, FAWNSOFT ENERGY EFFI
   Poess M, 2010, PROC INT CONF DATA, P1006, DOI 10.1109/ICDE.2010.5447806
   Samsung, 2012, 64 GB A DIE TOGGL NA
   Samsung, 2005, K9XXG08UXM NAND FLAS
   Schall D, 2010, PROCEEDINGS OF THE THIRD C* CONFERENCE ON COMPUTER SCIENCE AND SOFTWARE ENGINEERING 2010 (C3S2E '10), P1, DOI 10.1145/1822327.1822328
   Shim Hyotaek., 2010, Proceedings of the 2010 IEEE 26th Symposium on Mass Storage Systems and Technologies (MSST '10), P1
   Shmidt D., 2002, TNDOC017
   SPC, 2009, UM TRAC REP
   Strande S.M., 2012, Proceedings of the 1st Conference of the Extreme Science and Engineering Discovery Environment: Bridging from the eXtreme to the campus and beyond, P3
   Tsirogiannis D., 2010, SIGMOD, P231, DOI DOI 10.1145/1807167.1807194
   Wei QS, 2011, IEEE S MASS STOR SYS
   Wu Q., 2011, Proc. of the 3rd USENIX Conf. on Hot Topics in Storage and File Syst, P4
   Yada H, 2000, IEEE T MAGN, V36, P2213, DOI 10.1109/20.908363
   Yoo Balgeun, 2011, P USENIX HOTSTORAGE
   Yoo Jinsoo, 2013, P 2013 IEEE 29 S P M
   Zhiguang Chen, 2011, Proceedings of the 2011 IEEE 19th International Symposium on Modelling, Analysis & Simulation of Computer and Telecommunication Systems (MASCOTS 2011), P475, DOI 10.1109/MASCOTS.2011.31
NR 52
TC 10
Z9 11
U1 0
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD MAR
PY 2015
VL 11
IS 2
AR 8
DI 10.1145/2644818
PG 24
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CF1KG
UT WOS:000352303400004
DA 2024-07-18
ER

PT J
AU Hwang, T
   Jung, J
   Won, Y
AF Hwang, Taeho
   Jung, Jaemin
   Won, Youjip
TI HEAPO: Heap-Based Persistent Object Store
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Algorithms; Performance; Persistent heap; Persistent objects;
   Nonvolatile memory
ID CLASS MEMORY; SYSTEM
AB In this work, we developed a Heap-Based Persistent Object Store (HEAPO) to manage persistent objects in byte-addressable Nonvolatile RAM (NVRAM). HEAPO defines its own persistent heap layout, the persistent object format, name space organization, object sharing and protection mechanism, and undo-only log-based crash recovery, all of which are effectively tailored for NVRAM. We put our effort into developing a lightweight and flexible layer to exploit the DRAM-like access latency of NVRAM. To address this objective, we developed (i) a native management layer for NVRAM to eliminate redundancy between in-core and on-disk copies of the metadata, (ii) an expandable object format, (iii) a burst trie-based global name space with local name space caching, (iv) static address binding, and (v) minimal logging for undo-only crash recovery. We implemented HEAPO at commodity OS (Linux 2.6.32) and measured the performance. By eliminating metadata redundancy, HEAPO improved the speed of creating, attaching, and expanding an object by 1.3x, 4.5x, and 3.8x, respectively, compared to memory-mapped file-based persistent object store. Burst trie-based name space organization of HEAPO yielded 7.6x better lookup performance compared to hashed B-tree-based name space of EXT4. We modified memcachedb to use HEAPO in maintaining its search structure. For hash table update, HEAPO-based memcachedb yielded 3.4x performance improvement against original memcachedb implementation which uses mmap () over ramdisk approach to maintain the key-value store in memory.
C1 [Hwang, Taeho; Jung, Jaemin; Won, Youjip] Hanyang Univ, Dept Comp & Software, Seoul, South Korea.
C3 Hanyang University
RP Won, Y (corresponding author), Hanyang Univ, Dept Comp & Software, Seoul, South Korea.
EM htaeh@hanyang.ac.kr; jmjung@hanyang.ac.kr; yjwon@hanyang.ac.kr
FU IT R&D program MKE/KEIT [10041608, 10035202]; MSIP (Ministry of Science,
   ICT&Future Planning), Korea, under the ITRC (Information Technology
   Research Center) [NIPA-2014-H0301-14-1017]; National Research Foundation
   of Korea [22A20130012575] Funding Source: Korea Institute of Science &
   Technology Information (KISTI), National Science & Technology
   Information Service (NTIS)
FX This work is supported by IT R&D program MKE/KEIT (No. 10041608,
   Embedded System Software for New-memory based Smart Device), and
   partially supported by IT R&D program MKE/KEIT. [No.10035202, Large
   Scale hyper-MLC SSD Technology Development]. This research was also
   supported by the MSIP (Ministry of Science, ICT&Future Planning), Korea,
   under the ITRC (Information Technology Research Center) support program
   (NIPA-2014-H0301-14-1017) supervised by the NIPA (National IT Industry
   Promotion Agency).
CR Akel A., 2011, P WORKSH HOT TOP STO
   Ames S., 2006, P IEEE C MASS STOR S
   [Anonymous], FLASH MEMORY SUMMIT
   Badam Anirudh, 2011, P USENIX C NETW SYST
   Cao M., 2005, P OTT LIN S OLS 05
   Chang F, 2008, ACM T COMPUT SYST, V26, DOI 10.1145/1365815.1365816
   Chodorow Kristina, 2010, FREE OP SOURC SOFTW
   Chu S., 2008, MEMCACHEDB
   Coburn J., 2011, P INT C ARCH SUPP PR
   Condit J., 2009, P ACM SIGOPS S OP SY
   Dearle A., 1992, P HAW INT C SYST SCI
   Diblotechnology, 2013, MEM CHANN STOR
   Freitas RF, 2008, IBM J RES DEV, V52, P439, DOI 10.1147/rd.524.0439
   Guerra Jorge, 2012, P USENIX ANN TECHN C
   Harter T, 2012, ACM T COMPUT SYST, V30, DOI 10.1145/2324876.2324878
   Heinz S, 2002, ACM T INFORM SYST, V20, P192, DOI 10.1145/506309.506312
   Jantunen I., 2010, P UBICOMM
   Jung J, 2010, ACM T STORAGE, V6, DOI 10.1145/1714454.1714457
   Kang Y., 2011, P IEEE C MASS STOR S
   Kang Y. M., 2006, P S VLSI TECHN
   Kim S., 2012, P 2012 INT S VLSI TE
   Kryder MH, 2009, IEEE T MAGN, V45, P3406, DOI 10.1109/TMAG.2009.2024163
   LEVER C, 2000, 001 U MICH CTR INF T
   Liu Z., 2012, P INT S MOD AN SIM C
   Mathur A., 2007, P OTT LIN S OLS 07
   Microsoft Corp. msdn, 2010, RAMD
   Moraru I., 2011, P ANN PAR DAT LAB WO
   Moss J. E. B., 1990, P WORKSH GARB COLL G
   Narayanan Dushyanth, 2012, P INT C ARCH SUPP PR
   Objective Analysis Semiconductor Market Research, 2009, PCM BEC REAL MEM DES
   Olson MA, 1999, P FREENIX TRACK 1999
   Perez T., 2012, THESIS PONTIFICIA U
   Qureshi MK, 2009, CONF PROC INT SYMP C, P24, DOI 10.1145/1555815.1555760
   Shekita E., 1990, P WORKSH PERS OBJ SY
   *SUN MICR INC, 2004, SOL ZFS FIL STOR SOL
   Sweeney A., 1996, P USENIX 1996 TECHN
   Venkataraman S., 2011, P USENIX C FIL STOR
   Volos H., 2011, P ANN NONV MEM WORKS
   Volos Haris, 2011, P INT C ARCH SUPP PR
   Wang A. A., 2006, ACM Transaction on Storage, V2, P309, DOI 10.1145/1168910.1168914
   Wang C., 2012, P INT PAR DISTR PROC
   Wang K. L., 2012, SPIN, V2, DOI 10.1142/S2010324712500099
   Wong P., 2002, P OTT LIN S
   Wu Xiaojian, 2011, P INT C HIGH PERF CO
NR 44
TC 33
Z9 38
U1 0
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD FEB
PY 2015
VL 11
IS 1
AR 3
DI 10.1145/2629619
PG 21
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA CD8CR
UT WOS:000351323300003
DA 2024-07-18
ER

PT J
AU Miranda, A
   Effert, S
   Kang, Y
   Miller, EL
   Popov, I
   Brinkmann, A
   Friedetzky, T
   Cortes, T
AF Miranda, Alberto
   Effert, Sascha
   Kang, Yangwook
   Miller, Ethan L.
   Popov, Ivan
   Brinkmann, Andre
   Friedetzky, Tom
   Cortes, Toni
TI Random Slicing: Efficient and Scalable Data Placement for Large-Scale
   Storage Systems
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Experimentation; Performance; Reliability; PRNG; randomized data
   distribution; storage management; scalability
ID CHOSEN-PREFIX COLLISIONS; MD5
AB The ever-growing amount of data requires highly scalable storage solutions. The most flexible approach is to use storage pools that can be expanded and scaled down by adding or removing storage devices. To make this approach usable, it is necessary to provide a solution to locate data items in such a dynamic environment. This article presents and evaluates the Random Slicing strategy, which incorporates lessons learned from table-based, rule-based, and pseudo-randomized hashing strategies and is able to provide a simple and efficient strategy that scales up to handle exascale data. Random Slicing keeps a small table with information about previous storage system insert and remove operations, drastically reducing the required amount of randomness while delivering a perfect load distribution.
C1 [Miranda, Alberto; Cortes, Toni] Barcelona Supercomp Ctr, Barcelona 08034, Spain.
   [Effert, Sascha] Christmann Informat Tech Medien, D-31241 Ilsede, Germany.
   [Kang, Yangwook; Miller, Ethan L.] Univ Calif Santa Cruz, Santa Cruz, CA 95064 USA.
   [Popov, Ivan; Brinkmann, Andre] Johannes Gutenberg Univ Mainz, Zentrum Datenverarbeitung ZDV, D-55099 Mainz, Germany.
   [Friedetzky, Tom] Univ Durham, Sci Labs, Durham DH1 3LE, England.
   [Cortes, Toni] Tech Univ Catalonia, Barcelona, Spain.
C3 Universitat Politecnica de Catalunya; Barcelona Supercomputer Center
   (BSC-CNS); University of California System; University of California
   Santa Cruz; Johannes Gutenberg University of Mainz; Durham University;
   Universitat Politecnica de Catalunya
RP Miranda, A (corresponding author), Barcelona Supercomp Ctr, 31 Jordi Girona, Barcelona 08034, Spain.
EM alberto.miranda@bsc.es; sascha.effert@christmann.info; ywkang@ucsc.edu;
   elm@ucsc.edu; ipopov@uni-mainz.de; brinkman@uni-mainz.de;
   tom.friedetzky@durharn.ac.uk; toni.cortes@bsc.es
RI Miranda, Alberto/T-7790-2019; Brinkmann, Andre/H-9888-2016
OI Miranda, Alberto/0000-0002-1386-628X; Miller, Ethan/0000-0003-2994-9060;
   Cortes, Toni/0000-0002-2537-8937; Brinkmann, Andre/0000-0003-3083-2775
FU Spanish Ministry of Economy and Competitiveness [TIN2012-34557]; Catalan
   Government [2009-SGR-980]; EU Marie Curie Initial Training Network
   SCALUS [238808]; National Science Foundation [CCF-0937938, IIP-0934401];
   Storage Systems Research Center at the University of California, Santa
   Cruz; Direct For Computer & Info Scie & Enginr; Division Of Computer and
   Network Systems [1266400] Funding Source: National Science Foundation
FX This work was partially supported by the Spanish Ministry of Economy and
   Competitiveness under the TIN2012-34557 grant, the Catalan Government
   under the 2009-SGR-980 grant, the EU Marie Curie Initial Training
   Network SCALUS under grant agreement no. 238808, the National Science
   Foundation under grants CCF-0937938 and IIP-0934401, and by the
   industrial sponsors of the Storage Systems Research Center at the
   University of California, Santa Cruz.
CR Amarasinghe S., 2010, EXASCALE SOTFWARE ST
   [Anonymous], P ACM SIGMOD INT C M
   [Anonymous], P 12 NASA GODD 21 IE
   [Anonymous], P SPAA
   [Anonymous], 1977, SOLUTIONS KLEES RECT
   [Anonymous], 1996, THESIS U CALIFORNIA
   Azagury A, 2003, IEEE S MASS STOR SYS, P165, DOI 10.1109/MASS.2003.1194853
   Azar Y, 1999, SIAM J COMPUT, V29, P180, DOI 10.1137/S0097539795288490
   Blaum M., 1994, Proceedings the 21st Annual International Symposium on Computer Architecture (Cat. No.94CH3397-7), P245, DOI 10.1109/ISCA.1994.288145
   Brent R.P., 1992, P 5 AUSTR SUPERCOMPU, P95
   Brinkmann A., 2000, SPAA 2000. Twelfth Annual ACM Symposium on Parallel Algorithms and Architectures, P119, DOI 10.1145/341800.341815
   Brinkmann A., 2008, P 12 INT C PRINC DIS
   Brinkmann A., 2007, P 27 INT C DISTR COM, DOI [10.1109/icdcs.2007.103, DOI 10.1109/ICDCS.2007.103]
   CHOU WS, 1995, APPL ALGEBR ENG COMM, V6, P245, DOI 10.1007/BF01235718
   Corbett P, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES, P1
   Cortes T, 2001, USENIX ASSOCIATION PROCEEDINGS OF THE 2001 USENIX ANNUAL TECHNICAL CONFERENCE, P119
   De Berg M., 2008, Computational Geometry: Algorithms and Applications, V17
   DeCandia Giuseppe, 2007, Operating Systems Review, V41, P205, DOI 10.1145/1323293.1294281
   Devulapalli A, 2008, I W STOR NETW ARCH, P65, DOI 10.1109/SNAPI.2008.14
   Eastlake D., 2001, US Secure Hash Algorithm, V1
   Gonzalez J., 2008, P INT C GRID COMP HI
   Honicky R. J., 2004, P 18 IEEE INT PAR DI
   Honicky R. J., 2003, P 17 IEEE INT PAR DI
   Johnson N. L., 1977, Urn Models and Their Application
   Karger DavidR., 1997, P 29 ANN ACM S THEOR, P654, DOI DOI 10.1145/258533.258660
   Knuth D. E., 1997, ART COMPUTER PROGRAM, V2, P192
   Lehmer D.H, 1951, P 2 S LARG SCAL DIG, V26, P141
   LUBY M, 1996, PSEUDORANDOMNESS CRY
   Marsaglia George., 1991, ANN APPL PROBAB, V1, P462, DOI DOI 10.1214/A0AP/1177005878
   Popov I., 2012, Proceedings of the 2012 20th Euromicro International Conference on Parallel, Distributed and Network-Based Processing (PDP 2012), P536, DOI 10.1109/PDP.2012.58
   Raab M, 1998, LECT NOTES COMPUT SC, V1518, P159
   Sanders P, 2001, SIAM PROC S, P67
   Schindelhauer C., 2005, SPAA 05, P218
   Severence F. L., 2009, SYST MODELING SIMULA
   Stevens M, 2007, LECT NOTES COMPUT SC, V4515, P1
   Stevens M, 2009, LECT NOTES COMPUT SC, V5677, P55, DOI 10.1007/978-3-642-03356-8_4
   Viega J, 2003, 19TH ANNUAL COMPUTER SECURITY APPLICATIONS CONFERENCE, PROCEEDINGS, P129, DOI 10.1109/CSAC.2003.1254318
   Wang T., 2007, INTEGER HASH FUNCTIO
   Wang XY, 2005, LECT NOTES COMPUT SC, V3621, P17
   Weil S. A., 2006, P ACM IEEE C SUP
   Weil SA, 2006, USENIX ASSOCIATION 7TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P307
   Zheng Weimin, 2011, P 9 USENIX C FIL STO
   [No title captured]
NR 43
TC 11
Z9 14
U1 0
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD JUL
PY 2014
VL 10
IS 3
AR 9
DI 10.1145/2632230
PG 35
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA AN3TU
UT WOS:000340512400001
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Abd-El-Malek, M
   Wachs, M
   Cipar, J
   Sanghi, K
   Ganger, GR
   Gibson, GA
   Reiter, MK
AF Abd-El-Malek, Michael
   Wachs, Matthew
   Cipar, James
   Sanghi, Karan
   Ganger, Gregory R.
   Gibson, Garth A.
   Reiter, Michael K.
TI File System Virtual Appliances: Portable File System Implementations
SO ACM TRANSACTIONS ON STORAGE
LA English
DT Article
DE Design; Performance; Operating systems; virtual machines; file systems
AB File system virtual appliances (FSVAs) address the portability headaches that plague file system (FS) developers. By packaging their FS implementation in a virtual machine (VM), separate from the VM that runs user applications, they can avoid the need to port the file system to each operating system (OS) and OS version. A small FS-agnostic proxy, maintained by the core OS developers, connects the FSVA to whatever OS the user chooses. This article describes an FSVA design that maintains FS semantics for unmodified FS implementations and provides desired OS and virtualization features, such as a unified buffer cache and VM migration. Evaluation of prototype FSVA implementations in Linux and NetBSD, using Xen as the virtual machine manager (VMM), demonstrates that the FSVA architecture is efficient, FS-agnostic, and able to insulate file system implementations from OS differences that would otherwise require explicit porting.
C1 [Abd-El-Malek, Michael; Wachs, Matthew; Cipar, James; Ganger, Gregory R.; Gibson, Garth A.] Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.
   [Reiter, Michael K.] Univ N Carolina, Chapel Hill, NC USA.
C3 Carnegie Mellon University; University of North Carolina; University of
   North Carolina Chapel Hill
RP Abd-El-Malek, M (corresponding author), Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.
EM mabdelmalek@gmail.com
OI Gibson, Garth/0000-0002-6656-7080
FU National Science Foundation [CNS-0326453, CCF-0621499]; Department of
   Energy [DE-FC02-06ER25767]; Army Research Office [DAAD19-02-1-0389]
FX This work is sponsored in part by the National Science Foundation, via
   grants CNS-0326453 and CCF-0621499, by the Department of Energy, under
   Award Number DE-FC02-06ER25767, and by the Army Research Office, under
   agreement number DAAD19-02-1-0389. Intel and Network Appliance donated
   hardware donations that enabled this work.
CR [Anonymous], BUG 111656 2 4 20 20
   [Anonymous], P USENIX ANN TECHN C
   [Anonymous], P USENIX ANN TECHN C
   [Anonymous], P 9 WORKSH HOT TOP O
   [Anonymous], 2009, P USENIX ANN TECHN C
   [Anonymous], THESIS STANFORD U
   [Anonymous], XENFS
   [Anonymous], FUSE FIL US
   [Anonymous], P 6 ANN S OP SYST DE
   [Anonymous], FILE SYSTEM VIRUTAL
   [Anonymous], P USENIX ANN TECHN C
   [Anonymous], 2003, ACM SIGOPS OPERATING
   [Anonymous], P IEEE 1 WORKSH MOB
   [Anonymous], RECONSTRUCTING I O
   [Anonymous], IMPROVING DISTRIBUTE
   [Anonymous], P USENIX ANN TECHN C
   [Anonymous], 2007, P 5 USENIX C FIL STO
   [Anonymous], MULTIPROTOCOL DATA A
   [Anonymous], 2002, P FAST 2002 C FIL ST
   [Anonymous], P USENIX ANN TECHN C
   Bershad B. N., 1988, USENIX Association Winter Conference Dallas 1988. Proceedings, P267
   BORDEN TL, 1989, IBM SYST J, V28, P104, DOI 10.1147/sj.281.0104
   Callaghan B., 1989, PROC WINTER 1989 USE, P43
   Carns PH, 2000, USENIX ASSOCIATION PROCEEDINGS OF THE 4TH ANNUAL LINUX SHOWCASE AND CONFERENCE, ATLANTA, P317
   Clark C, 2005, USENIX ASSOCIATION PROCEEDINGS OF THE 2ND SYMPOSIUM ON NETWORKED SYSTEMS DESIGN & IMPLEMENTATION (NSDI '05), P273
   Gingell R. A., 1987, Proceedings of the Summer 1987 USENIX Conference, P81
   Gupta D, 2006, LECT NOTES COMPUT SC, V4290, P342
   Katcher Jeffrey., 1997, POSTMARK NEW FILE SY
   Kleiman S. R., 1986, USENIX Association Summer Conference Proceedings, Atlanta 1986, P238
   Meyer DT, 2008, EUROSYS'08: PROCEEDINGS OF THE EUROSYS 2008 CONFERENCE, P41, DOI 10.1145/1357010.1352598
   Northrop L., 2001, SOFTWARE PRODUCT LIN
   Patterson R. H., 1995, Operating Systems Review, V29, P79, DOI 10.1145/224057.224064
   Pfaff B, 2006, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD SYMPOSIUM ON NETWORKED SYSTEMS DESIGN & IMPLEMENTATION (NSDI 06), P353
   ROSENBLUM M, 1992, ACM T COMPUT SYST, V10, P26, DOI 10.1145/146941.146943
   Rosenblum M., 2004, ACM QUEUE, V2, P34
   Smith JE, 2005, COMPUTER, V38, P32, DOI 10.1109/MC.2005.173
   THEKKATH CA, 1994, SOFTWARE PRACT EXPER, V24, P981, DOI 10.1002/spe.4380241102
   Waldspurger CA, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE FIFTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P181, DOI 10.1145/1060289.1060307
   Webber N., 1993, USENIX Association. Proceedings of the Winter 1993 USENIX Conference, P219
   Weinhold C, 2008, EUROSYS'08: PROCEEDINGS OF THE EUROSYS 2008 CONFERENCE, P81, DOI 10.1145/1357010.1352602
   WELCH B., 2008, P USENIX C FILE STOR, P1
   Yang JF, 2006, P IEEE S SECUR PRIV, P243
   Zadok E, 2000, USENIX ASSOCIATION PROCEEDINGS OF THE 2000 USENIX ANNUAL TECHNICAL CONFERENCE, P55
NR 43
TC 2
Z9 3
U1 0
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1553-3077
EI 1553-3093
J9 ACM T STORAGE
JI ACM Trans. Storage
PD SEP
PY 2012
VL 8
IS 3
AR 9
DI 10.1145/2339118.2339120
PG 26
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 014BR
UT WOS:000309350600002
DA 2024-07-18
ER

EF