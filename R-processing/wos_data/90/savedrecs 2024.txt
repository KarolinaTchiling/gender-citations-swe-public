FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Xu, WF
   Wang, PJ
   Yang, XS
AF Xu, Weifeng
   Wang, Pengjie
   Yang, Xiaosong
TI FrseGAN: Free-style editable facial makeup transfer based on GAN
   combined with transformer
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE generative adversarial networks; makeup transfer; transformer
AB Makeup in real life varies widely and is personalized, presenting a key challenge in makeup transfer. Most previous makeup transfer techniques divide the face into distinct regions for color transfer, frequently neglecting details like eyeshadow and facial contours. Given the successful advancements of Transformers in various visual tasks, we believe that this technology holds large potential in addressing pose, expression, and occlusion differences. To explore this, we propose novel pipeline which combines well-designed Convolutional Neural Network with Transformer to leverage the advantages of both networks for high-quality facial makeup transfer. This enables hierarchical extraction of both local and global facial features, facilitating the encoding of facial attributes into pyramid feature maps. Furthermore, a Low-Frequency Information Fusion Module is proposed to address the problem of large pose and expression variations which exist between the source and reference faces by extracting makeup features from the reference and adapting them to the source. Experiments demonstrate that our method produces makeup faces that are visually more detailed and realistic, yielding superior results.
   Althoughprevious works have achived competitive results, there are still performanceimprovement potential by exploiting both long-distance dependency and localfeature details, to address the missing facial features, bluured faces andincorrect makeup transfer faced by state-of-the-art methods. Therefore, thispaper focuses on proposing a flexible and controllable makeup transfer model. image
C1 [Xu, Weifeng; Wang, Pengjie] Dalian Minzu Univ, Sch Comp Sci, Dalian 116600, Peoples R China.
   [Wang, Pengjie; Yang, Xiaosong] Bournemouth Univ, Natl Ctr Comp Animat, Bournemouth, England.
C3 Dalian Minzu University; Bournemouth University
RP Wang, PJ (corresponding author), Dalian Minzu Univ, Sch Comp Sci, Dalian 116600, Peoples R China.
EM pengjiewang@qq.com
RI Wang, Pengjie/KHE-3288-2024
FU European Union's Horizon 2020 research and innovation programme under
   the Marie Sklstrok;odowska-Curie grant agreement No [900025]; European
   Union's Horizon 2020 research and innovation programme under the Marie
   Sklstrok;odowska-Curie [2023-MS-133]; Natural Science Foundation of
   Liaoning Province
FX Our work is supported by European Union's Horizon 2020 research and
   innovation programme under the Marie Sk & lstrok;odowska-Curie grant
   agreement No 900025; Natural Science Foundation of Liaoning Province,
   Grant/Award Number: 2023-MS-133.
CR Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chang HW, 2018, PROC CVPR IEEE, P40, DOI 10.1109/CVPR.2018.00012
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Deng H, 2021, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR46437.2021.00648
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Gu Q, 2019, IEEE I CONF COMP VIS, P10480, DOI 10.1109/ICCV.2019.01058
   Hassani A, 2023, PROC CVPR IEEE, P6185, DOI 10.1109/CVPR52729.2023.00599
   Hensel M, 2017, ADV NEUR IN, V30
   Huang H., 2023, P IEEE CVF C COMP VI, P22690
   Jiang WT, 2020, PROC CVPR IEEE, P5193, DOI 10.1109/CVPR42600.2020.00524
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Li TT, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P645, DOI 10.1145/3240508.3240618
   Liu SC, 2023, IEEE T CYBERNETICS, V53, P1460, DOI 10.1109/TCYB.2021.3102642
   Lyu YM, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3601, DOI 10.1145/3474085.3475531
   Qiu YW, 2023, IEEE I CONF COMP VIS, P12756, DOI 10.1109/ICCV51070.2023.01176
   Ruoss A, 2023, Arxiv, DOI arXiv:2305.16843
   Sun ZY, 2022, AAAI CONF ARTIF INTE, P2325
   Nguyen T, 2021, PROC CVPR IEEE, P13300, DOI 10.1109/CVPR46437.2021.01310
   Tian R, 2023, PROC CVPR IEEE, P22721, DOI 10.1109/CVPR52729.2023.02176
   Vaswani A, 2017, ADV NEUR IN, V30
   Wan ZY, 2022, IEEE WINT CONF APPL, P3113, DOI 10.1109/WACV51458.2022.00317
   Xia ZF, 2023, Arxiv, DOI arXiv:2309.01430
   Yang CY, 2022, LECT NOTES COMPUT SC, V13676, P737, DOI 10.1007/978-3-031-19787-1_42
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu M., 2022, arXiv
NR 26
TC 1
Z9 1
U1 0
U2 0
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAY
PY 2024
VL 35
IS 3
AR e2235
DI 10.1002/cav.2235
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RU4P5
UT WOS:001230163500001
DA 2024-08-05
ER

PT J
AU Li, BW
   Li, W
   Wang, JQ
   Meng, WL
   Zhang, JG
   Zhang, XP
AF Li, Bowen
   Li, Wei
   Wang, Jingqi
   Meng, Weiliang
   Zhang, Jiguang
   Zhang, Xiaopeng
TI SocialVis: Dynamic social visualization in dense scenes via real-time
   multi-object tracking and proximity graph construction
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE dense pedestrian; detection; multi-object tracking; proximity graph;
   visualization
AB To monitor and assess social dynamics and risks at large gatherings, we propose "SocialVis," a comprehensive monitoring system based on multi-object tracking and graph analysis techniques. Our SocialVis includes a camera detection system that operates in two modes: a real-time mode, which enables participants to track and identify close contacts instantly, and an offline mode that allows for more comprehensive post-event analysis. The dual functionality not only aids in preventing mass gatherings or overcrowding by enabling the issuance of alerts and recommendations to organizers, but also allows for the generation of proximity-based graphs that map participant interactions, thereby enhancing the understanding of social dynamics and identifying potential high-risk areas. It also provides tools for analyzing pedestrian flow statistics and visualizing paths, offering valuable insights into crowd density and interaction patterns. To enhance system performance, we designed the SocialDetect algorithm in conjunction with the BYTE tracking algorithm. This combination is specifically engineered to improve detection accuracy and minimize ID switches among tracked objects, leveraging the strengths of both algorithms. Experiments on both public and real-world datasets validate that our SocialVis outperforms existing methods, showing 1.2%$$ 1.2\% $$ improvement in detection accuracy and 45.4%$$ 45.4\% $$ reduction in ID switches in dense pedestrian scenarios.
   Our SocialVis is a framework for visualizing social dynamics, providing real-time monitoring of individual behavior and social distancing in crowds. Its SocialDetect algorithm, using LSKA attention and Wise-IOUv3 loss, improves accuracy in dense settings with occlusion and scale changes. To simulate interactions among individuals, we employ a graph theory-based approach that utilizes multi-object tracking, combined with SPGA, PFV, and PPV techniques, to provide comprehensive insights into crowd behavior. image
C1 [Li, Bowen; Li, Wei; Meng, Weiliang; Zhang, Jiguang; Zhang, Xiaopeng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.
   [Li, Bowen; Li, Wei; Wang, Jingqi; Meng, Weiliang; Zhang, Jiguang; Zhang, Xiaopeng] Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing, Peoples R China.
   [Wang, Jingqi] Cent China Normal Univ, Natl Engn Res Ctr E Learning, Wuhan, Peoples R China.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Chinese Academy of Sciences; Institute of Automation, CAS; Central
   China Normal University
RP Meng, WL; Zhang, JG (corresponding author), Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing, Peoples R China.
EM weiliang.meng@ia.ac.cn; jiguang.zhang@ia.ac.cn
OI Zhang, Jiguang/0000-0002-8212-1361; meng, wei liang/0000-0002-3221-4981
FU Beijing Natural Science Foundation; National Natural Science Foundation
   of China [U21A20515, 62376271, 62171321, 62365014, 62162044, 52175493];
   Open Project Program of State Key Laboratory of Virtual Reality
   Technology and Systems, Beihang University [VRLAB2023B01];  [L231013]
FX This work was supported by National Natural Science Foundation of China
   (Nos. U21A20515, 62376271, 62171321, 62365014, 62162044, and 52175493),
   Beijing Natural Science Foundation L231013, and the Open Project Program
   of State Key Laboratory of Virtual Reality Technology and Systems,
   Beihang University (No. VRLAB2023B01).
CR Ahmed I, 2021, SUSTAIN CITIES SOC, V65, DOI 10.1016/j.scs.2020.102571
   Alahi A, 2016, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2016.110
   Alhmiedat T, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10192435
   Benfold B., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3457, DOI 10.1109/CVPR.2011.5995667
   Bertozzi M, 1998, IMAGE VISION COMPUT, V16, P585, DOI 10.1016/S0262-8856(97)00093-0
   Bewley A, 2016, IEEE IMAGE PROC, P3464, DOI 10.1109/ICIP.2016.7533003
   Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Chen K., Feature mining for localised crowd counting. Proceedings of British Machine Vision Conference. Volume 1; 2012. p. 3
   Chen L, 2018, IEEE INT CON MULTI, DOI 10.1097/PEC.0000000000001553
   Cota DAM., Monitoring COVID19 prevention measures on CCTV cameras using deep learning. Torino: Politecnico di Torino; 2020
   Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667
   Eades Peter, 1984, Congressus Numerantium, V42, P149
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Hossen A., Social distance monitoring using a lowcost 3d sensor. TechRxiv. 2023
   Jeong J, 2016, INT CONF UBIQ ROBOT, P38, DOI 10.1109/URAI.2016.7734016
   Jocher G., Ultralytics YOLOv8. 2023
   Khandelwal P, 2020, Arxiv, DOI arXiv:2005.05287
   Lau KW, 2024, EXPERT SYST APPL, V236, DOI 10.1016/j.eswa.2023.121352
   Li CY, 2022, Arxiv, DOI [arXiv:2209.02976, DOI 10.48550/ARXIV.2209.02976]
   Liang C, 2022, IEEE T IMAGE PROCESS, V31, P3182, DOI 10.1109/TIP.2022.3165376
   Morikoshi S, 2023, IEEE INT CONF INF VI, P172, DOI 10.1109/IV60283.2023.00038
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rezaei M, 2017, COMPUT IMAGING VIS, V45, P1, DOI 10.1007/978-3-319-50551-0
   Rezaei M, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10217514
   Saleem NH, 2019, IEEE T INTELL TRANSP, V20, P3675, DOI 10.1109/TITS.2018.2879429
   Schindler K., MOT16: a benchmark for multiobject tracking. arXiv preprint arXiv:160300831
   Tong ZJ, 2023, Arxiv, DOI arXiv:2301.10051
   Wang CY, 2022, Arxiv, DOI [arXiv:2207.02696, DOI 10.48550/ARXIV.2207.02696]
   Wojke N, 2017, IEEE IMAGE PROC, P3645, DOI 10.1109/ICIP.2017.8296962
   Yang DF, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21134608
   Yang F, 2016, PROC CVPR IEEE, P2129, DOI 10.1109/CVPR.2016.234
   Zhang SF, 2020, IEEE T MULTIMEDIA, V22, P380, DOI 10.1109/TMM.2019.2929005
   Zhang YF, 2022, LECT NOTES COMPUT SC, V13682, P1, DOI 10.1007/978-3-031-20047-2_1
   Zheng ZH, 2020, AAAI CONF ARTIF INTE, V34, P12993
   Zhichao Lu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14656, DOI 10.1109/CVPR42600.2020.01468
   Zhou BL, 2012, PROC CVPR IEEE, P2871, DOI 10.1109/CVPR.2012.6248013
NR 36
TC 0
Z9 0
U1 2
U2 2
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAY
PY 2024
VL 35
IS 3
AR e2272
DI 10.1002/cav.2272
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RU4M4
UT WOS:001230160400001
DA 2024-08-05
ER

PT J
AU Xiong, W
   Peng, YD
AF Xiong, Wei
   Peng, Yingda
TI Design and development of a mixed reality teaching systems for IV
   cannulation and clinical instruction
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE CATLM; HoloLens2; intravenous cannulation (IV); learning systems; mixed
   reality
ID CONFIRMATORY FACTOR-ANALYSIS; TECHNOLOGY ACCEPTANCE MODEL;
   VIRTUAL-REALITY; AUGMENTED REALITY; MOTIVATION; ENVIRONMENTS; STUDENTS;
   MEDIA; FLOW; FUN
AB Intravenous cannulation (IV) is a common technique used in clinical infusion. This study developed a mixed reality IV cannulation teaching system based on the Hololens2 platform. The paper integrates cognitive-affective theory of learning with media (CATLM) and investigates the cognitive engagement and willingness to use the system from the learners' perspective. Through experimental research on 125 subjects, the variables affecting learners' cognitive engagement and intention to use were determined. On the basis of CATLM, three new mixed reality attributes, immersion, system verisimilitude, and response time, were introduced, and their relationships with cognitive participation and willingness to use were determined. The results show that high immersion of mixed reality technology promotes students' higher cognitive engagement; however, this high immersion does not significantly affect learners' intention to use mixed reality technology for learning. Overall, cognitive and emotional theories are effective in mixed reality environments, and the model has good adaptability. This study provides a reference for the application of mixed reality technology in medical education.
C1 [Xiong, Wei; Peng, Yingda] South China Univ Technol, Sch Design, Guangzhou, Peoples R China.
C3 South China University of Technology
RP Peng, YD (corresponding author), South China Univ Technol, Sch Design, Guangzhou, Peoples R China.
EM 202120156913@mail.scut.edu.cn
CR Agarwal R, 2000, MIS QUART, V24, P665, DOI 10.2307/3250951
   Anastopoulou S, 2011, BRIT J EDUC TECHNOL, V42, P266, DOI 10.1111/j.1467-8535.2009.01017.x
   [Anonymous], 2015, Microsoft HoloLens
   [Anonymous], 2017, J Retail Consum Serv
   Baceviciute S, 2021, COMPUT EDUC, V164, DOI 10.1016/j.compedu.2020.104122
   Barfeld W., 2016, Fundamentals of wearable computers and augmented reality, V2nd ed.
   Boomsma A, 2000, STRUCT EQU MODELING, V7, P461, DOI 10.1207/S15328007SEM0703_6
   Bui T, 2024, MEDICINA-LITHUANIA, V60, DOI 10.3390/medicina60020332
   Bulliard J, 2020, FOREN IMAG, V23, DOI 10.1016/j.fri.2020.200417
   Buttussi F, 2018, IEEE T VIS COMPUT GR, V24, P1063, DOI 10.1109/TVCG.2017.2653117
   Byrne B, 2010, INTERNATIONAL HANDBOOK OF PSYCHOLOGY IN EDUCATION, P3
   Cabero-Almenara J, 2019, HELIYON, V5, DOI 10.1016/j.heliyon.2019.e01597
   Carr Peter J, 2011, Br J Nurs, V20, pS15
   Chih-Mei Wang, 2021, International Journal of Information and Education Technology, V11, P269, DOI 10.18178/ijiet.2021.11.6.1522
   Chittaro L, 2007, COMPUT EDUC, V49, P3, DOI 10.1016/j.compedu.2005.06.002
   Choi B, 2011, COMPUT EDUC, V57, P2382, DOI 10.1016/j.compedu.2011.06.019
   Codd AM, 2011, ANAT SCI EDUC, V4, P119, DOI 10.1002/ase.214
   Condino S, 2018, J HEALTHC ENG, V2018, DOI 10.1155/2018/5435097
   d'Aiello AF, 2023, J MED SYST, V47, DOI 10.1007/s10916-023-01959-8
   Dalgarno B, 2010, BRIT J EDUC TECHNOL, V41, P10, DOI 10.1111/j.1467-8535.2009.01038.x
   DAVIS FD, 1989, MANAGE SCI, V35, P982, DOI 10.1287/mnsc.35.8.982
   DAVIS FD, 1992, J APPL SOC PSYCHOL, V22, P1111, DOI 10.1111/j.1559-1816.1992.tb00945.x
   Di Serio A, 2013, COMPUT EDUC, V68, P586, DOI 10.1016/j.compedu.2012.03.002
   Domagk S, 2010, COMPUT HUM BEHAV, V26, P1024, DOI 10.1016/j.chb.2010.03.003
   Faust F, 2012, WORK, V41, P1164, DOI 10.3233/WOR-2012-0298-1164
   Ford TJ, 2023, FRONT DIGIT HEALTH, V5, DOI 10.3389/fdgth.2023.1146806
   FORNELL C, 1981, J MARKETING RES, V18, P39, DOI 10.2307/3151312
   Garris R., 2002, Simulation & Gaming, V33, P441, DOI 10.1177/1046878102238607
   Gee JP, 2003, WHAT VIDEO GAMES HAVE TO TEACH US ABOUT LEARNING AND LITERACY, P1
   Guembe M, 2017, J HOSP INFECT, V97, P260, DOI 10.1016/j.jhin.2017.07.008
   Hair J. F., 2009, Multivariate data analysis
   Hameed BMZ, 2022, FRONT SURG, V9, DOI 10.3389/fsurg.2022.866946
   Hedberg J.G., 1994, Educational Media International, V31, P214
   Huang HM, 2010, COMPUT EDUC, V55, P1171, DOI 10.1016/j.compedu.2010.05.014
   Huang HW, 2012, COMPUT EDUC, V59, P250, DOI 10.1016/j.compedu.2012.01.015
   Huang W., 2022, J Educ Comput Res
   Huang W, 2022, J EDUC COMPUT RES, V60, P807, DOI 10.1177/07356331211053630
   Huimin J., 2003, J Pract Nurs, V19, P75
   Jackson DL, 2009, PSYCHOL METHODS, V14, P6, DOI 10.1037/a0014694
   Johnson-Glenberg MC, 2021, J COMPUT ASSIST LEAR, V37, P1263, DOI 10.1111/jcal.12567
   Jung EY, 2012, NURS EDUC TODAY, V32, P458, DOI 10.1016/j.nedt.2011.05.012
   Kalyuga S, 2003, EDUC PSYCHOL-US, V38, P23, DOI 10.1207/S15326985EP3801_4
   Koufaris M, 2002, INFORM SYST RES, V13, P205, DOI 10.1287/isre.13.2.205.83
   Leap M., 2018, Magic Leap
   Lee EAL, 2010, COMPUT EDUC, V55, P1424, DOI 10.1016/j.compedu.2010.06.006
   Lele A, 2013, J AMB INTEL HUM COMP, V4, P17, DOI 10.1007/s12652-011-0052-4
   Li WH, 2023, BMC MED EDUC, V23, DOI 10.1186/s12909-023-04610-9
   Lia H, 2018, PROC SPIE, V10576, DOI 10.1117/12.2293934
   Lowry PB, 2013, J ASSOC INF SYST, V14, P617
   Lu S, 2020, J MED SYST, V44, DOI 10.1007/s10916-020-01656-w
   Marks RB, 2005, J MANAG EDUC, V29, P531, DOI 10.1177/1052562904271199
   Mayer RE, 2003, J EDUC PSYCHOL, V95, P806, DOI 10.1037/0022-0663.95.4.806
   McDonald RP, 2002, PSYCHOL METHODS, V7, P64, DOI 10.1037//1082-989X.7.1.64
   McWilliams LA, 2017, NURS RES PRACT, V2017, DOI 10.1155/2017/4685157
   Microsoft and McKinsey & Company's Education Practice, 2018, The class of 2030 and lifeready learning
   MILGRAM P, 1994, IEICE T INF SYST, VE77D, P1321
   Moreno R, 2006, J COMPUT ASSIST LEAR, V22, P149, DOI 10.1111/j.1365-2729.2006.00170.x
   Moreno R, 2004, J EDUC PSYCHOL, V96, P492, DOI 10.1037/0022-0663.96.3.492
   Moreno R, 2001, COGNITION INSTRUCT, V19, P177, DOI 10.1207/S1532690XCI1902_02
   Otero-Varela L, 2023, PLOS ONE, V18, DOI 10.1371/journal.pone.0282698
   Palmer M., 1995, COMMUNICATION AGE VI, P277
   Papalois ZA, 2022, EUR SURG RES, V63, P40, DOI 10.1159/000520386
   Park S, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11167259
   PARKER LE, 1992, J PERS SOC PSYCHOL, V62, P625, DOI 10.1037/0022-3514.62.4.625
   Petersen GB, 2020, BRIT J EDUC TECHNOL, V51, P2098, DOI 10.1111/bjet.12991
   Petersen GB, 2022, COMPUT EDUC, V179, DOI 10.1016/j.compedu.2021.104429
   Pintrich P., 2003, HDB PSYCHOL ED PSYCH, V7, P103, DOI [DOI 10.1002/0471264385.WEI0706, 10.1002/0471264385.wei0706]
   Plass J.L., 2014, Cambridge Handbook of Multimedia Learning, V2nd, P729, DOI [DOI 10.1017/CBO9781139547369.036, 10.1017/CBO9781139547369.036]
   Pranoto H, 2019, PROCEDIA COMPUT SCI, V157, P506, DOI 10.1016/j.procs.2019.09.007
   Ray-Barruel G, 2019, INFECT DIS HEALTH, V24, P152, DOI 10.1016/j.idh.2019.03.001
   Robinson BL, 2020, MED SCI EDUC, V30, P1745, DOI 10.1007/s40670-020-01064-2
   Sajjadi P., 2018, 2018 10th International Conference on Virtual Worlds and Games for Serious Applications (VSGames), P1
   Sandralegar Abiram, 2024, Neurosurg Focus, V56, pE14, DOI 10.3171/2023.10.FOCUS23620
   Schreiber JB, 2006, J EDUC RES, V99, P323, DOI 10.3200/JOER.99.6.323-338
   Sharda R, 2004, J MANAGE INFORM SYST, V20, P31, DOI 10.1080/07421222.2004.11045780
   Sheridan TB., 1992, Presence: Teleoperators and Virtual Environments, V1, P120, DOI [10.1162/pres.1992.1.1.120, DOI 10.1162/PRES.1992.1.1.120]
   Snelling J, 2003, CLIN ANAT, V16, P165, DOI 10.1002/ca.10113
   Stanney K, 1998, INT J HUM-COMPUT INT, V10, P135, DOI 10.1207/s15327590ijhc1002_3
   Stolz LA, 2016, J VASC ACCESS, V17, P366, DOI 10.5301/jva.5000574
   Sun QL, 2023, Symposium Virtual Re, P265, DOI 10.1109/VR55154.2023.00042
   Tamaddon K, 2017, 2017 IEEE VIRTUAL REALITY WORKSHOP ON K-12 EMBODIED LEARNING THROUGH VIRTUAL & AUGMENTED REALITY (KELVAR)
   Thompson B., 2004, EXPLORATORY CONFIRMA, DOI [10.1037/10694-000, DOI 10.1037/10694-000]
   Valles J, 2022, J MED SYST, V46, DOI 10.1007/s10916-022-01891-3
   Venkatesh V, 2000, MANAGE SCI, V46, P186, DOI 10.1287/mnsc.46.2.186.11926
   Vervoorn MT, 2023, JMIR SERIOUS GAMES, V11, DOI 10.2196/41297
   Wang YH, 2017, COMPUT EDUC, V113, P162, DOI 10.1016/j.compedu.2017.04.013
   Wenger E., 1998, Communities of Practice. Learning meaning and identity, DOI DOI 10.1017/CBO9780511803932
   Wilkat M, 2023, J PERS MED, V13, DOI 10.3390/jpm13060922
   Witmer BG, 1998, PRESENCE-TELEOP VIRT, V7, P225, DOI 10.1162/105474698565686
   Wixom BH, 2005, INFORM SYST RES, V16, P85, DOI 10.1287/isre.1050.0042
   Wu G., 2006, Journal of Current Issues and Research in Adverting, V28, P87, DOI 10.1080/10641734.2006.10505193
   Yan XR, 2023, ACTA NEUROCHIR, V165, P3371, DOI 10.1007/s00701-023-05810-4
   Yang HR, 2023, FRONT PSYCHOL, V13, DOI 10.3389/fpsyg.2022.1081372
   Yanping S., 1998, Chinese Nurs J, V7, P37
   Yu JL, 2020, CAN J ANESTH, V67, P715, DOI 10.1007/s12630-020-01570-2
NR 95
TC 0
Z9 0
U1 1
U2 1
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAY
PY 2024
VL 35
IS 3
AR e2288
DI 10.1002/cav.2288
PG 23
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UJ8C8
UT WOS:001247772100001
DA 2024-08-05
ER

PT J
AU Bellenger, D
   Chen, MS
   Xu, ZJ
AF Bellenger, Darren
   Chen, Minsi
   Xu, Zhijie
TI Facial emotion recognition with a reduced feature set for video game and
   metaverse avatars
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE emotion recognition; metaverse; online games; virtual reality
ID SOCIAL-SKILLS; EXPRESSIONS; VALIDATION
AB This paper presents a novel real-time facial feature extraction algorithm, producing a small feature set, suitable for implementing emotion recognition with online game and metaverse avatars. The algorithm aims to reduce data transmission and storage requirements, hurdles in the adoption of emotion recognition in these mediums. The early results presented show a facial emotion recognition accuracy of up to 92% on one benchmark dataset, with an overall accuracy of 77.2% across a wide range of datasets, demonstrating the early promise of the research.
C1 [Bellenger, Darren; Chen, Minsi; Xu, Zhijie] Univ Huddersfield, Dept Comp Sci, Huddersfield, England.
C3 University of Huddersfield
RP Bellenger, D (corresponding author), Univ Huddersfield, Dept Comp Sci, Huddersfield, England.
EM darren.bellenger@hud.ac.uk
OI Chen, Minsi/0000-0001-6628-1421; Bellenger, Darren/0000-0003-0334-5699;
   Xu, Zhijie/0000-0002-0524-5926
FU CGI Group
FX The main author has received part funding from the company he currently
   works for (CGI Group), towards completing a PhD at the University of
   Huddersfield. No other funding has been received.
CR Affectiva, 2023, AFFDEX 2.0: A Real-Time Facial Expression Analysis Toolkit
   Baltrusaitis T, 2016, IEEE WINT CONF APPL
   Bareket R., 2006, Playing it right! Kansas
   Baron-Cohen S, 2009, PHILOS T R SOC B, V364, P3567, DOI 10.1098/rstb.2009.0191
   Barrett LF, 2019, PSYCHOL SCI PUBL INT, V20, P1, DOI 10.1177/1529100619832930
   Beyer J., 1999, Autism and play
   Brodsky S., 2021, Lifewire
   Carlos-Roca L.R., 2018, International Joint Conference on Neural Networks (IJCNN), P1, DOI [10.1109/IJCNN.2018.8489113, DOI 10.1109/IJCNN.2018.8489113]
   Constantine L., 2012, 2012 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops), P697, DOI 10.1109/PerComW.2012.6197603
   Cosker D, 2011, IEEE I CONF COMP VIS, P2296, DOI 10.1109/ICCV.2011.6126510
   Dang VT., 2022, Facial Expression Recognition: A Survey and its Applications. Global IT Research Institute-GiRI
   Deming DJ, 2017, Q J ECON, V132, P1593, DOI 10.1093/qje/qjx022
   DesLauriers M., 2018, Linear Interpolation
   Dionisio JDN, 2013, ACM COMPUT SURV, V45, DOI 10.1145/2480741.2480751
   Duncan D., 2016, Facial emotion recognition in real time, P1
   Dupré D, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0231968
   EDPS TechDispatch, 2021, Facial emotion recognition
   EKMAN P, 1987, J PERS SOC PSYCHOL, V53, P712, DOI 10.1037/0022-3514.53.4.712
   Ekman P., 1978, FACIAL ACTION CODING
   Enox, 2016, OpenCV for Unity
   Enox, 2016, Dlib FaceLandmark detector
   Evenson E., 2014, Powerful phrases for dealing with difficult people: over 325 ready-to-use words and phrases for working with challenging personalities
   Fairweather A., 2014, How to manage difficult people
   Fairweather A., 2007, How to be a motivational manager
   Faita C, 2017, LECT NOTES COMPUT SC, V10325, P144, DOI 10.1007/978-3-319-60928-7_12
   Fove Inc, 2020, Cision PR Newswire
   Gaidhane VH, 2016, SADHANA-ACAD P ENG S, V41, P415, DOI 10.1007/s12046-016-0479-6
   Gibbons S., 2018, Forbes
   Granato M, 2020, MULTIMED TOOLS APPL, V79, P33657, DOI 10.1007/s11042-019-08585-y
   Grishchenko I, 2020, Arxiv, DOI arXiv:2006.10962
   Gupta Vikas., 2018, Face Detection - OpenCV, Dlib and Deep Learning (C++ / Python)
   Hassouneh A., 2020, Informat. Med. Unlocked, V20, DOI [DOI 10.1016/J.IMU.2020.100372, 10.1016/j.imu.2020.100372]
   Heiphetz A., 2010, Training and collaboration with virtual worlds
   Howarth S., 2009, No Matter What
   HTC VIVE, 2021, Bringing your facial expressions to life in VR
   Hughes I., 2022, ITNOW, V64, P22
   Jackson P., 2015, Front Neurosci, V2, P9, DOI [10.3389/fnhum.2015.00112%25/full, DOI 10.3389/FNHUM.2015.00112%/FULL]
   Jones N, 2021, Knowable Magazine
   Kartheek MN, 2021, COMPLEX INTELL SYST, V7, P3303, DOI 10.1007/s40747-021-00526-3
   Khadoudja G., 2011, Int J Tomograph Stat, V7, P17
   King DE, 2009, J MACH LEARN RES, V10, P1755
   Koles B, 2014, ORGAN PSYCHOL REV, V4, P175, DOI 10.1177/2041386613507074
   Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49
   Lewinski P, 2014, J NEUROSCI PSYCHOL E, V7, P227, DOI 10.1037/npe0000028
   Liliana D. Y., 2019, Journal of Physics: Conference Series, V1193, DOI 10.1088/1742-6596/1193/1/012004
   Lou JW, 2020, Arxiv, DOI arXiv:2009.00935
   Lucey P., 2010, 2010 IEEE COMP SOC C, P94, DOI DOI 10.1109/CVPRW.2010.5543262
   Lyons MJ., 1998, The Japanese female facial expression (JAFFE) database
   Ma J., 2021, MATH PROBL ENG, V2021, P1
   Mallick S, 2018, Facemark: facial landmark Detection using OpenCV. Learn OpenCV
   Marin-Morales J., 2018, Sci Rep, V9, P8
   Mazurek MO, 2013, RES AUTISM SPECT DIS, V7, P316, DOI 10.1016/j.rasd.2012.09.008
   McWhertor M, 2012, EverQuest 2's new facial recognition tech lets you role-play a Froglok like never before. Polygon
   Medcalc, 2020, ATAN2 function. Medcalc
   Minaee S, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21093046
   Morris D. Z., 2022, CoinDesk
   Newbutt N., 2014, The Development of Virtual Reality Technologies for People on the Autism Spectrum
   Newbutt N, 2016, J AUTISM DEV DISORD, V46, P3166, DOI 10.1007/s10803-016-2830-5
   NHS, 2013, Nicswell
   Oculus, 2018, Introducing the team behind half dome-Facebook reality
   Olszanowski M, 2015, FRONT PSYCHOL, V5, DOI 10.3389/fpsyg.2014.01516
   Ozel M., 2020, Face the FACS: Lower face cheat sheet
   Pandey S., 2020, Study Tonight
   Perveen N, 2020, VISAPP: PROCEEDINGS OF THE 15TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS, VOL 4: VISAPP, P93, DOI 10.5220/0009099700930102
   Piercy G., 2016, The importance of social skills for the future of work, V16. Waikato, NZ: Human Resources Institute of New Zealand, P32
   Pita P., 2017, VR Times
   Robbaz, 2018, Star citizen-FOIP face tracking #2-Space Delivery
   Salmam F. Z., 2018, INT J EL COMP ENG SY, V8, P52, DOI DOI 10.11591/IJECE.V8I1.PP52-59
   Sanchez J, 2009, A social history of virtual worlds. UploadVR
   Santosh M, 2021, Mater Sci Eng, V1057, P12093, DOI [10.1088/1757-899X/1057/1/0%12093/pdf, DOI 10.1088/1757-899X/1057/1/0%12093/PDF]
   Savas BK., 2018, Real time driver fatigue Detection based on SVM algorithm. Paper presented at: 2018 6th international conference on Control Engineering Information Technology (CEIT)
   Shah V., 2017, Medium
   Siam AI, 2022, COMPUT INTEL NEUROSC, V2022, DOI 10.1155/2022/8032673
   Sicile-Kira C., 2012, A Full Life with Autism: From Learning to Forming Relationships to Achieving Independence
   Steele C.B., 2013, Building collaborative learning environments: The effects of trust and its relationship to learn in g in the 3-D virtual education environment of Second Life
   Stewart S., 2021, What is the best FPS for gaming? Gamingscan
   Stöckli S, 2018, BEHAV RES METHODS, V50, P1446, DOI 10.3758/s13428-017-0996-1
   Suresh V, 2023, Arxiv, DOI [arXiv:2106.15453, DOI 10.48550/ARXIV.2106.15453, 10.48550/arXiv.2106.15453]
   Tautkute I, 2019, FUND INFORM, V168, P269, DOI 10.3233/FI-2019-1832
   Thomaz CE, 2010, IMAGE VISION COMPUT, V28, P902, DOI 10.1016/j.imavis.2009.11.005
   Thorne JM., 2003, Vision, video, and graphics (VVG), Bath, UK, P2003, DOI [10.2312/vvg20031023, DOI 10.2312/VVG20031023]
   Tian YI, 2001, IEEE T PATTERN ANAL, V23, P97, DOI 10.1109/34.908962
   UCSD Computer Vision, 2001, Yale face database
   UMA Steering group, 2021, Unity Multipurpose Avatar (UMA)
   van der Schalk J, 2011, EMOTION, V11, P907, DOI 10.1037/a0023853
   van der Struijk S, 2018, 18TH ACM INTERNATIONAL CONFERENCE ON INTELLIGENT VIRTUAL AGENTS (IVA'18), P159, DOI 10.1145/3267851.3267918
   Velusamy S, 2011, INT CONF ACOUST SPEE, P2028
   VentureBeat, 2023, Researchers find 'inconsistent' benchmarking across 3,867 AI research papers
   Wagner J, 2022, VRChat User Concurrency Hit Nearly 90,000 Last New Year's Eve! New World Notes
   Wang XQ, 2018, COMPUT INTEL NEUROSC, V2018, DOI 10.1155/2018/7208794
   Wenzslaus A., 2019, A Comparative Investigation into the use of Cognitive Services API and Image Processing in Python for Emotion Detection, DOI [10.13140/RG.2.2.28143.87201/1, DOI 10.13140/RG.2.2.28143.87201/1]
   Witt KJ, 2016, INT J ADV COUNS, V38, P218, DOI 10.1007/s10447-016-9269-4
   Zallio M, 2022, TELEMAT INFORM, V75, DOI 10.1016/j.tele.2022.101909
   Zhao GY, 2011, IMAGE VISION COMPUT, V29, P607, DOI 10.1016/j.imavis.2011.07.002
NR 94
TC 0
Z9 0
U1 7
U2 7
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAR
PY 2024
VL 35
IS 2
AR e2230
DI 10.1002/cav.2230
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MQ0L5
UT WOS:001194973100001
OA hybrid, Green Submitted
DA 2024-08-05
ER

PT J
AU Lin, WT
   Zhang, JG
   Meng, WL
   Liu, XL
   Zhang, XP
AF Lin, Weitao
   Zhang, Jiguang
   Meng, Weiliang
   Liu, Xianglong
   Zhang, Xiaopeng
TI HIDE: Hierarchical iterative decoding enhancement for multi-view 3D
   human parameter regression
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE 3D human mesh recovery; body modeling; computer vision; deep learning
AB Parametric human modeling are limited to either single-view frameworks or simple multi-view frameworks, failing to fully leverage the advantages of easily trainable single-view networks and the occlusion-resistant capabilities of multi-view images. The prevalent presence of object occlusion and self-occlusion in real-world scenarios leads to issues of robustness and accuracy in predicting human body parameters. Additionally, many methods overlook the spatial connectivity of human joints in the global estimation of model pose parameters, resulting in cumulative errors in continuous joint parameters.To address these challenges, we propose a flexible and efficient iterative decoding strategy. By extending from single-view images to multi-view video inputs, we achieve local-to-global optimization. We utilize attention mechanisms to capture the rotational dependencies between any node in the human body and all its ancestor nodes, thereby enhancing pose decoding capability. We employ a parameter-level iterative fusion of multi-view image data to achieve flexible integration of global pose information, rapidly obtaining appropriate projection features from different viewpoints, ultimately resulting in precise parameter estimation. Through experiments, we validate the effectiveness of the HIDE method on the Human3.6M and 3DPW datasets, demonstrating significantly improved visualization results compared to previous methods.
C1 [Lin, Weitao; Zhang, Jiguang; Meng, Weiliang; Zhang, Xiaopeng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.
   [Lin, Weitao; Zhang, Jiguang; Meng, Weiliang; Liu, Xianglong; Zhang, Xiaopeng] Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Chinese Academy of Sciences; Institute of Automation, CAS
RP Zhang, JG; Meng, WL (corresponding author), Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing, Peoples R China.
EM jiguang.zhang@ia.ac.cn; weiliang.meng@ia.ac.cn
OI meng, wei liang/0000-0002-3221-4981; Lin, Weitao/0000-0003-1177-9809
FU Beijing Natural Science Foundation; National Natural Science Foundation
   of China [U21A20515, 62376271, 62171321, 62365014, 62162044, 52175493];
   Open Project Program of State Key Laboratory of Virtual Reality
   Technology and Systems, Beihang University [VRLAB2023B01]; Wenzhou
   Business School 2024 Talent launch program [RC202401];  [L231013]
FX This work was supported by National Natural Science Foundation of China
   (Nos. U21A20515, 62376271, 62171321, 62365014, 62162044, and 52175493),
   Beijing Natural Science Foundation L231013, and the Open Project Program
   of State Key Laboratory of Virtual Reality Technology and Systems,
   Beihang University (No. VRLAB2023B01), in part by the Wenzhou Business
   School 2024 Talent launch program (No. RC202401).
CR Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471
   Bogo F., 2016, Keep it SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image
   Cho J., 2022, Crossattention of disentangled modalities for 3d human mesh recovery with transformers. European conference on computer vision
   Gler RizaAlp., 2018, DensePose: Dense Human Pose Estimation In The Wild
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   Huang Y., 2017, Towards accurate markerless human shape and pose estimation over time. 2017 international conference on 3D vision (3DV)
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Jiang XJ, 2022, Arxiv, DOI arXiv:2210.01886
   Joo H., 2021, Exemplar finetuning for 3d human model fitting towards inthewild 3d human pose estimation. 2021 international conference on 3D vision (3DV)
   Kanazawa A, 2018, PROC CVPR IEEE, P7122, DOI 10.1109/CVPR.2018.00744
   Khirodkar R, 2022, PROC CVPR IEEE, P1705, DOI 10.1109/CVPR52688.2022.00176
   Kocabas M., 2021, PARE: part attention regressor for 3D human body estimation. Proceedings of the IEEE/CVF international conference on computer vision
   Kolotouros N, 2019, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2019.00234
   Li J., 2023, NIKI: neural inverse kinematics with invertible neural networks for 3D human pose and shape estimation. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition
   Li J., 2021, Hybrik: a hybrid analyticalneural inverse kinematics solution for 3d human pose and shape estimation. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition
   Li Z., 2021, 3D human pose and shape estimation through collaborative learning and multiview modelfitting. Proceedings of the IEEE/CVF winter conference on applications of computer vision
   Liang J., 2019, Shapeaware human pose and shape reconstruction using multiview images. Proceedings of the IEEE/CVF international conference on computer vision
   Lin K., 2021, Endtoend human pose and mesh reconstruction with transformers. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition
   Lin K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12919, DOI 10.1109/ICCV48922.2021.01270
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin W., 2022, Multiview 3D human physique dataset construction for robust digital human modeling of natural scenes. Proceedings of the 8th international conference on communication and information processing
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Mehta D, 2017, INT CONF 3D VISION, P506, DOI 10.1109/3DV.2017.00064
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Vaswani A, 2017, ADV NEUR IN, V30
   Von Marcard T., 2018, Recovering accurate 3d human pose in the wild using imus and a moving camera. Proceedings of the European conference on computer vision (ECCV)
   Wang Tao, 2021, ADV NEURAL INFORM PR, V34
   Wu S., 2021, Graphbased 3d multiperson pose estimation using multiview images. Proceedings of the IEEE/CVF international conference on computer vision
   Zeng W., 2020, 3d human mesh regression with dense correspondence. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition
   Zhang H., 2021, Pymaf: 3d human pose and shape regression with pyramidal mesh alignment feedback loop. Proceedings of the IEEE/CVF international conference on computer vision
   Zheng C., 2022, A Lightweight Graph Transformer Network for Human Mesh Reconstruction from 2D Human Pose
   Zheng C., 2023, POTTER: pooling attention transformer for efficient human mesh recovery. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition
   Zhu XZ, 2021, Arxiv, DOI [arXiv:2010.04159, 10.48550/arXiv.2010.04159]
NR 33
TC 0
Z9 0
U1 1
U2 1
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAY
PY 2024
VL 35
IS 3
AR e2266
DI 10.1002/cav.2266
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TA1G0
UT WOS:001238436500001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Zhang, Y
   Zhang, YQ
   Chen, LF
   Yin, BC
   Sun, YL
AF Zhang, Yong
   Zhang, Yuqing
   Chen, Lufei
   Yin, Baocai
   Sun, Yongliang
TI Frontal person image generation based on arbitrary-view human images
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE arbitrary-view images; deep learning; frontal person image generation;
   frontal pose estimation; generative adversarial networks
AB Frontal person images contain the richest detailed features of humans, which can effectively assist in behavioral recognition, virtual dress fitting and other applications. While many remarkable networks are devoted to the person image generation task, most of them need accurate target poses as the network inputs. However, the target pose annotation is difficult and time-consuming. In this work, we proposed a first frontal person image generation network based on the proposed anchor pose set and the generative adversarial network. Specifically, our method first classify a rough frontal pose to the input human image based on the proposed anchor pose set, and regress all key points of the rough frontal pose to estimate an accurate frontal pose. Then, we consider the estimated frontal pose as the target pose, and construct a two-stream generator based on the generative adversarial network to update the person's shape and appearance feature in a crossing way and generate a realistic frontal person image. Experiments on the challenging CMU Panoptic dataset show that our method can generate realistic frontal images from arbitrary-view human images.
C1 [Zhang, Yong; Zhang, Yuqing; Chen, Lufei; Yin, Baocai] Beijing Univ Technol, Beijing Inst Artificial Intelligence, Dept Informat Sci, Beijing Key Lab Multimedia & Intelligent Software, Beijing 100124, Peoples R China.
   [Sun, Yongliang] Taiji Co LTD, China Elect Technol Grp, Beijing, Peoples R China.
C3 Beijing University of Technology; China Electronics Technology Group
RP Zhang, Y (corresponding author), Beijing Univ Technol, Beijing Inst Artificial Intelligence, Dept Informat Sci, Beijing Key Lab Multimedia & Intelligent Software, Beijing 100124, Peoples R China.
EM zhangyong2010@bjut.edu.cn
FU National Key Research and Development Program of China
FX National Natural Science Foundation of China, Grant/Award
   Numbers:U21B2038, 61772209, 62072015; National Key Research and
   Development Program of China, Grant/Award Number:2021ZD0111902r No
   Statement Available
CR Albahar B., 2019, Guided imagetoimage translation with bidirectional feature transformation. International conference on computer vision
   Balakrishnan G, 2018, PROC CVPR IEEE, P8340, DOI 10.1109/CVPR.2018.00870
   Benzine A, 2020, PROC CVPR IEEE, P6855, DOI 10.1109/CVPR42600.2020.00689
   Bhunia AK., 2023, Person image synthesis via denoising diffusion model. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chen XL, 2015, Arxiv, DOI arXiv:1504.00325
   Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
   Golda T., 2019, Human pose estimation for realworld crowded scenarios. 2019 16th IEEE international conference on advanced video and signal based surveillance (AVSS)
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Iqbal U, 2017, PROC CVPR IEEE, P4654, DOI 10.1109/CVPR.2017.495
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Joo H, 2019, IEEE T PATTERN ANAL, V41, P190, DOI 10.1109/TPAMI.2017.2782743
   Kendall A, 2018, PROC CVPR IEEE, P7482, DOI 10.1109/CVPR.2018.00781
   Khatun A, 2023, PATTERN RECOGN, V137, DOI 10.1016/j.patcog.2022.109246
   Kocabas M., 2018, Multiposenet: fast multiperson pose estimation using pose residual network. European conference on computer vision
   Kreiss S, 2019, PROC CVPR IEEE, P11969, DOI 10.1109/CVPR.2019.01225
   Lakhal MI., 2019, Pose guided human image synthesis by view disentanglement and enhanced weighting loss. European conference on computer vision
   Lathuilire S., 2020, Attentionbased fusion for multisource human image generation. Winter conference on applications of computer vision
   Liao FJ, 2024, EXPERT SYST APPL, V246, DOI 10.1016/j.eswa.2023.123073
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu W, 2019, IEEE I CONF COMP VIS, P5903, DOI 10.1109/ICCV.2019.00600
   Ma L., 2017, Pose guided person image generation. Proceedings of the 31st international conference on neural information processing systems
   Ma LQ, 2018, PROC CVPR IEEE, P99, DOI 10.1109/CVPR.2018.00018
   Moon G., 2019, Camera distanceaware topdown approach for 3d multiperson pose estimation from a single rgb image. International conference on computer vision
   Moreno-Noguer F, 2017, PROC CVPR IEEE, P1561, DOI 10.1109/CVPR.2017.170
   Newell A, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901343
   Park S., 2016, 3d human pose estimation using convolutional neural networks with 2d pose information. European conference on computer vision
   Peng H, 2020, VISUAL COMPUT, V36, P2227, DOI 10.1007/s00371-020-01908-3
   Qian X., 2018, Posenormalized image generation for person reidentification. European conference on computer vision
   Redmon J, 2018, Arxiv, DOI arXiv:1804.02767
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Siarohin A, 2018, PROC CVPR IEEE, P3408, DOI 10.1109/CVPR.2018.00359
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tang H., 2020, Xinggan for person image generation. CoRR.abs/2007.09278
   Tang H., 2019, Cycle in cycle generative adversarial networks for keypointguided image generation. Proceedings of the 27th ACM international conference on multimedia
   Tang Z., 2023, 3d human pose estimation with spatiotemporal crisscross attention. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition
   Tripathi S., 2023, 3d human pose estimation via intuitive physics. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition
   Vidanpathirana M, 2020, VISUAL COMPUT, V36, P1501, DOI 10.1007/s00371-019-01757-9
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Weng WH, 2021, IEEE ACCESS, V9, P16591, DOI 10.1109/ACCESS.2021.3053408
   Zanfir A., 2018, Deep network for the integrated 3d sensing of multiple people in natural images. Proceedings of the 32nd international conference on neural information processing systems
   Zheng Z., 2017, Unlabeled samples generated by gan improve the person reidentification baseline in vitro. International conference on computer vision
   Zhongjian Q, 2021, NEUROCOMPUTING, V449, P330, DOI 10.1016/j.neucom.2021.03.059
   Zhu J., 2017, Unpaired imagetoimage translation using cycleconsistent adversarial networks. International conference on computer vision
   Zhu Z, 2019, PROC CVPR IEEE, P2342, DOI 10.1109/CVPR.2019.00245
NR 46
TC 0
Z9 0
U1 0
U2 0
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD JUL
PY 2024
VL 35
IS 4
AR e2234
DI 10.1002/cav.2234
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZH8C1
UT WOS:001274487300001
DA 2024-08-05
ER

PT J
AU Huang, ZJ
   Wu, X
AF Huang, Zhangjin
   Wu, Xing
TI PR3D: Precise and realistic 3D face reconstruction from a single image
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE 3D face reconstruction; real-time; semi-supervised; StyleGAN2
AB Reconstructing the three-dimensional (3D) shape and texture of the face from a single image is a significant and challenging task in computer vision and graphics. In recent years, learning-based reconstruction methods have exhibited outstanding performance, but their effectiveness is severely constrained by the scarcity of available training data with 3D annotations. To address this issue, we present the PR3D (Precise and Realistic 3D face reconstruction) method, which consists of high-precision shape reconstruction based on semi-supervised learning and high-fidelity texture reconstruction based on StyleGAN2. In shape reconstruction, we use in-the-wild face images and 3D annotated datasets to train the auxiliary encoder and the identity encoder, encoding the input image into parameters of FLAME (a parametric 3D face model). Simultaneously, a novel semi-supervised hybrid landmark loss is designed to more effectively learn from in-the-wild face images and 3D annotated datasets. Furthermore, to meet the real-time requirements in practical applications, a lightweight shape reconstruction model called fast-PR3D is distilled through teacher-student learning. In texture reconstruction, we propose a texture extraction method based on face reenactment in StyleGAN2 style space, extracting texture from the source and reenacted face images to constitute a facial texture map. Extensive experiments have demonstrated the state-of-the-art performance of our method.
   Although learning-based 3D face reconstruction methods have exhibited outstanding performance, their effectiveness is severely constrained by the scarcity of available training data with 3D annotations. To address this issue, we present the PR3D (Precise and Realistic 3D face reconstruction) method, which consists of high-precision shape reconstruction based on semi-supervised learning and high-fidelity texture reconstruction based on StyleGAN2. Furthermore, to meet the real-time requirements in practical applications, a lightweight shape reconstruction model called fast-PR3D is distilled through teacher-student learning. image
C1 [Huang, Zhangjin; Wu, Xing] Univ Sci & Technol China, Hefei 230026, Peoples R China.
   [Huang, Zhangjin] Anhui Prov Key Lab Software Comp & Commun, Hefei, Peoples R China.
   [Huang, Zhangjin] Deqing Alpha Innovat Inst, Huzhou, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Huang, ZJ (corresponding author), Univ Sci & Technol China, Hefei 230026, Peoples R China.
EM zhuang@ustc.edu.cn
RI Huang, Zhangjin/I-7929-2014
FU National Key Research and Development Program of China [2022YFB3303400,
   2021YFF0500900]; National Key R&D Program of China [202203a05020016];
   Anhui Provincial Major Science and Technology Project [71991464];
   National Natural Science Foundation of China
FX This work was supported in part by the National Key R&D Program of China
   (numbers 2022YFB3303400 and 2021YFF0500900), the Anhui Provincial Major
   Science and Technology Project (number 202203a05020016), and the
   National Natural Science Foundation of China (number 71991464).
CR Bai HR, 2023, PROC CVPR IEEE, P362, DOI 10.1109/CVPR52729.2023.00043
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Blanz V, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P202, DOI 10.1109/AFGR.2002.1004155
   Bounareli Stella, 2023, 2023 IEEE 17th International Conference on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG57933.2023.10042744
   Bulat A, 2017, IEEE I CONF COMP VIS, P1021, DOI 10.1109/ICCV.2017.116
   Cao C, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462012
   Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020
   Chai ZH, 2022, LECT NOTES COMPUT SC, V13668, P74, DOI 10.1007/978-3-031-20074-8_5
   Chen Z, 2022, IEEE T CIRC SYST VID, V32, P8383, DOI 10.1109/TCSVT.2022.3192422
   Danecek R., 2022, P IEEECVF C COMPUTER, P20311
   Deng Yu, 2019, P IEEE C COMP VIS PA, P1
   Dib A, 2021, COMPUT GRAPH FORUM, V40, P153, DOI 10.1111/cgf.142622
   facebookresearch, 2020, PyTorch3D
   Feng Y, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459936
   Feng Y, 2018, LECT NOTES COMPUT SC, V11218, P557, DOI 10.1007/978-3-030-01264-9_33
   Genova K, 2018, PROC CVPR IEEE, P8377, DOI 10.1109/CVPR.2018.00874
   HavenFeng, 2020, Photometricoptimization
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu LW, 2017, ACM T GRAPHIC, V36, DOI [10.1145/3130800.3130887, 10.1145/3130800.31310887]
   Jiankang Deng, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P741, DOI 10.1007/978-3-030-58621-8_43
   Jianzhu Guo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P152, DOI 10.1007/978-3-030-58529-7_10
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Kingma D. P., 2014, arXiv
   Li TY, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130813
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Lugaresi C., 2019, 2019. Third workshop on computer vision for AR/VR at IEEE computer vision and pattern recognition (CVPR)
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Paszke A, 2019, ADV NEUR IN, V32
   Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58
   Rai A, 2023, Arxiv, DOI arXiv:2304.12483
   Ramamoorthi R, 2001, COMP GRAPH, P497, DOI 10.1145/383259.383317
   Romdhani S, 2005, PROC CVPR IEEE, P986
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Sanyal S, 2019, PROC CVPR IEEE, P7755, DOI 10.1109/CVPR.2019.00795
   Tov O, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459838
   Wood E, 2022, LECT NOTES COMPUT SC, V13673, P160, DOI 10.1007/978-3-031-19778-9_10
   Wood E, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3661, DOI 10.1109/ICCV48922.2021.00366
   Zhu XY, 2019, IEEE T PATTERN ANAL, V41, P78, DOI 10.1109/TPAMI.2017.2778152
   Zhu XY, 2016, PROC CVPR IEEE, P146, DOI 10.1109/CVPR.2016.23
   Zhu XY, 2015, PROC CVPR IEEE, P787, DOI 10.1109/CVPR.2015.7298679
   Zielonka W, 2022, LECT NOTES COMPUT SC, V13673, P250, DOI 10.1007/978-3-031-19778-9_15
   zllrunning, 2018, Faceparsing.Pytorch
NR 43
TC 0
Z9 0
U1 1
U2 1
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAY
PY 2024
VL 35
IS 3
AR e2254
DI 10.1002/cav.2254
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SN7N2
UT WOS:001235196700001
DA 2024-08-05
ER

PT J
AU Zhang, YS
   Liu, SG
AF Zhang, Yisheng
   Liu, Shiguang
TI UnderwaterImage2IR: Underwater impulse response generation via dual-path
   pre-trained networks and conditional generative adversarial networks
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE deep learning; GAN; IR generation; pre-trained networks; underwater
   acoustic
AB In the field of acoustic simulation, methods that are widely applied and have been proven to be highly effective rely on accurately capturing the impulse response (IR) and its convolution relationship. This article introduces a novel approach, named as UnderwaterImage2IR, that generates acoustic IRs from underwater images using dual-path pre-trained networks. This technique aims to achieve cross-modal conversion from underwater visual images to acoustic information with high accuracy at a low cost. Our method utilizes deep learning technology by integrating dual-path pre-trained networks and conditional generative adversarial networks conditional generative adversarial networks (CGANs) to generate acoustic IRs that match the observed scenes. One branch of the network focuses on the extraction of spatial features from images, while the other is dedicated to recognizing underwater characteristics. These features are fed into the CGAN network, which is trained to generate acoustic IRs corresponding to the observed scenes, thereby achieving high-accuracy acoustic simulation in an efficient manner. Experimental results, compared with the ground truth and evaluated by human experts, demonstrate the significant advantages of our method in generating underwater acoustic IRs, further proving its potential application in underwater acoustic simulation.
   This paper introduces a novel approach, named as UnderwaterImage2IR, that generates acoustic IRs from underwater images using Dual-Path Pre-trained Networks. This technique aims to achieve cross-modal conversion from underwater visual images to acoustic information with high accuracy at a low cost. image
C1 [Zhang, Yisheng; Liu, Shiguang] Tianjin Univ, Coll Intelligence & Comp, 135 Yaguan Rd, Tianjin 300350, Peoples R China.
C3 Tianjin University
RP Liu, SG (corresponding author), Tianjin Univ, Coll Intelligence & Comp, 135 Yaguan Rd, Tianjin 300350, Peoples R China.
EM lsg@tju.edu.cn
OI Zhang, Yisheng/0009-0007-0482-9392
FU National Natural Science Foundation of China
FX No Statement Available
CR Bailenson J., 2018, Experience on Demand: What Virtual Reality Is, How It Works, and What It Can Do
   DeVries T, 2019, Arxiv, DOI arXiv:1907.08175
   Donahue C, 2019, Arxiv, DOI [arXiv:1802.04208, DOI 10.48550/ARXIV.1802.04208]
   Dong H., 2018, Convolutional generative adversarial networks with binary neurons for poly-phonic music generation. Proceedings of the 19th International Society for Music Information Retrieval Conference
   Eaton J, 2016, IEEE-ACM T AUDIO SPE, V24, P1681, DOI 10.1109/TASLP.2016.2577502
   Engel J, 2019, Arxiv, DOI arXiv:1902.08710
   Gamper H, 2018, INT WORKSH ACOUSTIC, P136, DOI 10.1109/IWAENC.2018.8521241
   Godard C, 2019, IEEE I CONF COMP VIS, P3827, DOI 10.1109/ICCV.2019.00393
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Karras T., 2018, INT C LEARNING REPRE
   Kingma D. P., 2014, arXiv
   Kon H., 2018, Audio engineering society convention 144, P144
   KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694
   Kumar K, 2019, ADV NEUR IN, V32
   KUTTRUFF KH, 1993, J AUDIO ENG SOC, V41, P876
   Li YJ, 2017, PROC CVPR IEEE, P5892, DOI 10.1109/CVPR.2017.624
   Liu SG, 2021, Arxiv, DOI arXiv:2011.05538
   Liu SG, 2021, COMPUT ANIMAT VIRT W, V32, DOI 10.1002/cav.1970
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Mehra R, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2451236.2451245
   Mentzer F., 2020, Proceedings of the 34th International Conference on Neural Information Processing Systems, P11913
   Murphy D. T., 2010, AUDIO ENG SOC CONVEN
   Oord A.v.d., 2016, arXiv, DOI DOI 10.48550/ARXIV.1609.03499
   Pellegrini RS., 2001, Quality assessment of auditory virtual environments. Proceedings of the International Conference on Auditory Displays
   Radford A, 2021, PR MACH LEARN RES, V139
   Raghuvanshi N, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778805
   Ratnarajah A, 2021, Arxiv, DOI arXiv:2010.13219
   Singh N, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P286, DOI 10.1109/ICCV48922.2021.00035
   Välimäki V, 2012, IEEE T AUDIO SPEECH, V20, P1421, DOI 10.1109/TASL.2012.2189567
   Wang K, 2018, COMPUT ANIMAT VIRT W, V29, DOI 10.1002/cav.1835
   Werner S, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11031150
   Wu CY, 2022, PROC CVPR IEEE, P3804, DOI 10.1109/CVPR52688.2022.00379
   Yeh HC, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508420
   Zhou BL, 2014, ADV NEUR IN, V27
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 35
TC 0
Z9 0
U1 0
U2 0
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAY
PY 2024
VL 35
IS 3
AR e2243
DI 10.1002/cav.2243
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RH6J5
UT WOS:001226810700001
DA 2024-08-05
ER

PT J
AU Huang, Y
   Deng, XT
   Zhao, XJ
   Xie, WX
   Yuan, ZY
   Zhao, JH
AF Huang, Yi
   Deng, Xutian
   Zhao, Xujie
   Xie, Wenxuan
   Yuan, Zhiyong
   Zhao, Jianhui
TI Uniform gradient magnetic field and spatial localization method based on
   Maxwell coils for virtual surgery simulation
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE electromagnetic localization; Maxwell coils; positioning algorithm;
   surgery simulation; uniform gradient magnetic field; virtual reality
ID SYSTEM
AB With the development of virtual reality technology, simulation surgery has become a low-risk surgical training method and high-precision positioning of surgical instruments is required in virtual simulation surgery. In this paper we design and validate a novel electromagnetic positioning method based on a uniform gradient magnetic field. We employ Maxwell coils to generate the uniform gradient magnetic field and propose two positioning algorithms based on magnetic field, namely the linear equation positioning algorithm and the magnetic field fingerprint positioning algorithm. After validating the feasibility of proposed positioning system through simulation, we construct a prototype system and conduct practical experiments. The experimental results demonstrate that the positioning system exhibits excellent accuracy and speed in both simulation and real-world applications. The positioning accuracy remains consistent and high, showing no significant variation with changes in the positions of surgical instruments.
   A pair of Maxwell coils generate a uniform gradient magnetic field. The magnetic sensor is fixed on the surgical instrument, and the magnetic induction intensity component received by the sensor is combined with the proposed positioning algorithm to achieve real-time positioning of the surgical instrument and display it in the simulation environment. image
C1 [Huang, Yi; Deng, Xutian; Yuan, Zhiyong; Zhao, Jianhui] Wuhan Univ, Sch Comp Sci, Wuhan 430072, Hubei, Peoples R China.
   [Zhao, Xujie] Tech Univ Munich, Sch Computat Informat & Technol, Munich, Germany.
   [Xie, Wenxuan] Chinese Univ Hong Kong, Sch Mech & Automat Engn, Hong Kong, Peoples R China.
C3 Wuhan University; Technical University of Munich; Chinese University of
   Hong Kong
RP Zhao, JH (corresponding author), Wuhan Univ, Sch Comp Sci, Wuhan 430072, Hubei, Peoples R China.
EM jianhuizhao@whu.edu.cn
FU National Natural Science Foundation of China; Key R&D projects in Hubei
   Province [2023BCB133];  [62372338];  [62073248]
FX This work was supported by National Natural Science Foundation of China
   (Numbers 62372338 and 62073248) and Key R&D projects in Hubei Province
   (Number 2023BCB133).
CR Adel A, 2019, IEEE MAGN LETT, V10, DOI 10.1109/LMAG.2019.2908149
   Chen Dongyao, 2021, MobiCom '21: Proceedings of the 27th Annual International Conference on Mobile Computing and Networking, P269, DOI 10.1145/3447993.3483260
   de Geer AF, 2022, INT J COMPUT ASS RAD, V17, P1343, DOI 10.1007/s11548-022-02610-6
   de Koning SGB, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-84129-5
   Franz AM, 2019, MED PHYS, V46, P15, DOI 10.1002/mp.13280
   Franz AM, 2014, IEEE T MED IMAGING, V33, P1702, DOI 10.1109/TMI.2014.2321777
   Gothesen O, 2020, JBJS ESSENT SURG TEC, V10, DOI 10.2106/JBJS.ST.19.00022
   Herath S., 2022, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, P6604
   Hu C, 2012, IEEE T MAGN, V48, P2211, DOI 10.1109/TMAG.2012.2188537
   Hummel JB, 2005, MED PHYS, V32, P2371, DOI 10.1118/1.1944327
   Li KY, 2022, IEEE ROBOT AUTOM LET, V7, P6878, DOI 10.1109/LRA.2022.3178473
   Lin QZ, 2019, MOBICOM'19: PROCEEDINGS OF THE 25TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, DOI 10.1145/3300061.3300139
   Lombardo L, 2019, IEEE T INSTRUM MEAS, V68, P2396, DOI 10.1109/TIM.2019.2890885
   Qu RR, 2022, ANN THORAC SURG, V113, P1307, DOI 10.1016/j.athoracsur.2021.04.061
   Ragolia MA., 2021, 2021 IEEE international symposium on medical measurements and applications (MeMeA), P1
   Shen S, 2022, APPL MAGN RESON, V53, P895, DOI 10.1007/s00723-022-01470-2
   Su S., 2023, IEEE Trans Industr Inform, V19
   Sun J, 2005, IEEE SENS J, V5, P1127, DOI 10.1109/JSEN.2005.844339
   Tkhorenko MY, 2015, AUTOMAT REM CONTR+, V76, P2033, DOI 10.1134/S0005117915110120
   Tuffaha M, 2022, IEEE T VIS COMPUT GR, V28, P1597, DOI 10.1109/TVCG.2020.3019700
   Vergne C., 2022, IEEE Trans Magn, V59, P1
   Wang YZ, 2016, COMPUT ANIMAT VIRT W, V27, P290, DOI 10.1002/cav.1713
   Xu YX, 2022, IEEE T ROBOT, V38, P2812, DOI 10.1109/TRO.2022.3161766
   Youn J, 2020, IEEE T MED IMAGING, V39, P3855, DOI 10.1109/TMI.2020.3006445
   Yu P, 2020, COMPUT ANIMAT VIRT W, V31, DOI 10.1002/cav.1940
   Zaffanella LE, 1997, IEEE T POWER DELIVER, V12, P443, DOI 10.1109/61.568269
   Zeising S, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3156173
NR 27
TC 0
Z9 0
U1 4
U2 4
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAY
PY 2024
VL 35
IS 3
AR e2247
DI 10.1002/cav.2247
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RF7B2
UT WOS:001226306700001
DA 2024-08-05
ER

PT J
AU Wang, SB
   Li, L
   Wang, J
   Peng, T
   Li, ZW
AF Wang, Shuaibin
   Li, Li
   Wang, Juan
   Peng, Tao
   Li, Zhenwei
TI Highlight mask-guided adaptive residual network for single image
   highlight detection and removal
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE CBAM; multi-scale fusion; specular highlight detection; specular
   highlight removal
ID COLOR
AB Specular highlights detection and removal is a challenging task. Although various methods exist for removing specular highlights, they often fail to effectively preserve the color and texture details of objects after highlight removal due to the high brightness and nonuniform distribution characteristics of highlights. Furthermore, when processing scenes with complex highlight properties, existing methods frequently encounter performance bottlenecks, which restrict their applicability. Therefore, we introduce a highlight mask-guided adaptive residual network (HMGARN). HMGARN comprises three main components: detection-net, adaptive-removal network (AR-Net), and reconstruct-net. Specifically, detection-net can accurately predict highlight mask from a single RGB image. The predicted highlight mask is then inputted into the AR-Net, which adaptively guides the model to remove specular highlights and estimate an image without specular highlights. Subsequently, reconstruct-net is used to progressively refine this result, remove any residual specular highlights, and construct the final high-quality image without specular highlights. We evaluated our method on the public dataset (SHIQ) and confirmed its superiority through comparative experimental results.
   Detecting and removing specular highlights is a complex and challenging task. To address this issue, we propose the highlight mask-guided adaptive residual network (HMGARN), which effectively removes specular highlights while preserving image details through the collaboration of its three main components: detection-net, adaptive-removal network (AR-Net), and reconstruct-net. Experimental results on the public dataset (SHIQ) demonstrate that our method outperforms existing approaches. image
C1 [Wang, Shuaibin; Li, Li; Wang, Juan; Peng, Tao; Li, Zhenwei] Wuhan Text Univ, Sch Comp Sci & Artificial Intelligence, Wuhan, Peoples R China.
   [Li, Li; Peng, Tao] Engn Res Ctr Hubei Prov Clothing Informat, Wuhan, Peoples R China.
C3 Wuhan Textile University
RP Li, L (corresponding author), Wuhan Text Univ, Sch Comp Sci & Artificial Intelligence, Wuhan, Peoples R China.
EM lli@wtu.edu.cn
OI li, li/0000-0002-2027-1145
CR Akashi Y, 2015, LECT NOTES COMPUT SC, V9007, P611, DOI 10.1007/978-3-319-16814-2_40
   Buades A, 2005, MULTISCALE MODEL SIM, V4, P490, DOI 10.1137/040616024
   Cheng BD, 2023, COMPUT VIS IMAGE UND, V236, DOI 10.1016/j.cviu.2023.103819
   Feng W, 2022, OPT LASER ENG, V151, DOI 10.1016/j.optlaseng.2021.106939
   Fu G, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1873, DOI 10.1145/3394171.3413586
   Fu G, 2021, PROC CVPR IEEE, P7748, DOI 10.1109/CVPR46437.2021.00766
   Funke I, 2018, PROC SPIE, V10576, DOI 10.1117/12.2293755
   Guo J, 2018, LECT NOTES COMPUT SC, V11208, P282, DOI 10.1007/978-3-030-01225-0_17
   Hoyer PO, 2004, J MACH LEARN RES, V5, P1457
   Hu GW, 2022, PATTERN RECOGN LETT, V161, P108, DOI 10.1016/j.patrec.2022.06.014
   Jiddi S, 2022, IEEE T VIS COMPUT GR, V28, P1249, DOI 10.1109/TVCG.2020.2976986
   Kim H, 2013, PROC CVPR IEEE, P1460, DOI 10.1109/CVPR.2013.192
   Li RY, 2020, IEEE T MED IMAGING, V39, P328, DOI 10.1109/TMI.2019.2926501
   Minaee S, 2022, IEEE T PATTERN ANAL, V44, P3523, DOI 10.1109/TPAMI.2021.3059968
   Muhammad S, 2020, IMAGE VISION COMPUT, V93, DOI 10.1016/j.imavis.2019.11.001
   Park JB, 2003, IEEE INT CONF ROBOT, P1397
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   SHAFER SA, 1985, COLOR RES APPL, V10, P210, DOI 10.1002/col.5080100409
   Shen HL, 2013, APPL OPTICS, V52, P4483, DOI 10.1364/AO.52.004483
   Shen HL, 2009, APPL OPTICS, V48, P2711, DOI 10.1364/AO.48.002711
   Shen JB, 2011, PROC CVPR IEEE
   Shi J, 2017, PROC CVPR IEEE, P5844, DOI 10.1109/CVPR.2017.619
   Souza ACS, 2018, SIBGRAPI, P56, DOI 10.1109/SIBGRAPI.2018.00014
   Tan RT, 2004, J OPT SOC AM A, V21, P321, DOI 10.1364/JOSAA.21.000321
   Wu SH, 2018, LECT NOTES COMPUT SC, V11208, P193, DOI 10.1007/978-3-030-01225-0_12
   Wu ZQ, 2023, COMPUT VIS MEDIA, V9, P141, DOI 10.1007/s41095-022-0273-9
   Wu ZQ, 2022, IEEE T MULTIMEDIA, V24, P3782, DOI 10.1109/TMM.2021.3107688
   Xue ML, 2021, IEEE T MULTIMEDIA, V23, P2706, DOI 10.1109/TMM.2020.3015037
   Yamamoto T, 2019, ITE TRANS MEDIA TECH, V7, P92, DOI 10.3169/mta.7.92
   Yang QX, 2010, LECT NOTES COMPUT SC, V6314, P87, DOI 10.1007/978-3-642-15561-1_7
   Yi RJ, 2020, AAAI CONF ARTIF INTE, V34, P12685
   Zhang WM, 2019, IEEE T PATTERN ANAL, V41, P611, DOI 10.1109/TPAMI.2018.2803179
   Zou ZX, 2023, P IEEE, V111, P257, DOI 10.1109/JPROC.2023.3238524
NR 33
TC 0
Z9 0
U1 1
U2 1
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAY
PY 2024
VL 35
IS 3
AR e2271
DI 10.1002/cav.2271
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SB1Q2
UT WOS:001231912400001
DA 2024-08-05
ER

PT J
AU Hou, JN
   Zhang, RJ
   Wu, ZQ
   Meng, WL
   Zhang, XP
   Guo, JW
AF Hou, Jianing
   Zhang, Runjie
   Wu, Zhongqi
   Meng, Weiliang
   Zhang, Xiaopeng
   Guo, Jianwei
TI De-NeRF: Ultra-high-definition NeRF with deformable net alignment
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE deformable convolution net; neural radiance fields; voxel-based
   embedding
AB Neural Radiance Field (NeRF) can render complex 3D scenes with viewpoint-dependent effects. However, less work has been devoted to exploring its limitations in high-resolution environments, especially when upscaled to ultra-high resolution (e.g., 4k). Specifically, existing NeRF-based methods face severe limitations in reconstructing high-resolution real scenes, for example, a large number of parameters, misalignment of the input data, and over-smoothing of details. In this paper, we present a novel and effective framework, called De-NeRF, based on NeRF and deformable convolutional network, to achieve high-fidelity view synthesis in ultra-high resolution scenes: (1) marrying the deformable convolution unit which can solve the problem of misaligned input of the high-resolution data. (2) Presenting a density sparse voxel-based approach which can greatly reduce the training time while rendering results with higher accuracy. Compared to existing high-resolution NeRF methods, our approach improves the rendering quality of high-frequency details and achieves better visual effects in 4K high-resolution scenes.
   We present a novel framework, De-NeRF, for achieving high-fidelity view synthesis in ultra-high resolution scenes. The key technical components of De-NeRF includes a hybrid volumetric representation that can significantly speed up the training, and a deformable alignment unit module that can solve the problem of misaligned input of the high-resolution data. image
C1 [Hou, Jianing; Meng, Weiliang; Zhang, Xiaopeng; Guo, Jianwei] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.
   [Hou, Jianing; Meng, Weiliang; Zhang, Xiaopeng; Guo, Jianwei] Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing, Peoples R China.
   [Zhang, Runjie] Univ Calif San Diego, UC San Diego, La Jolla, CA USA.
   [Wu, Zhongqi] Chinese Acad Sci, Inst Sci & Dev, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Chinese Academy of Sciences; Institute of Automation, CAS;
   University of California System; University of California San Diego;
   Chinese Academy of Sciences
RP Guo, JW (corresponding author), Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing, Peoples R China.
EM jianwei.guo@nlpr.ia.ac.cn
OI meng, wei liang/0000-0002-3221-4981
FU National Natural Science Foundation of China
FX No Statement Availabler No Statement Availabler No Statement Availabler
   No Statement Available
CR Barron JT, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5835, DOI 10.1109/ICCV48922.2021.00580
   Blau Y, 2018, PROC CVPR IEEE, P6228, DOI 10.1109/CVPR.2018.00652
   Chen A., 2021, P IEEE CVF INT C COM, P14124
   Chen ZQ, 2019, PROC CVPR IEEE, P5932, DOI 10.1109/CVPR.2019.00609
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Duan Y., 2020, European Conference on Computer Vision, P51, DOI [10.1007/978-3-030-58598-3, DOI 10.1007/978-3-030-58598-3_4]
   Fisher A, 2021, ROBOT AUTON SYST, V142, DOI 10.1016/j.robot.2021.103755
   Goldlücke B, 2014, INT J COMPUT VISION, V106, P172, DOI 10.1007/s11263-013-0654-8
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Jang WB, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12929, DOI 10.1109/ICCV48922.2021.01271
   Jiang YF, 2023, PROC CVPR IEEE, P46, DOI 10.1109/CVPR52729.2023.00013
   Kato H, 2018, PROC CVPR IEEE, P3907, DOI 10.1109/CVPR.2018.00411
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lindenberger P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5967, DOI 10.1109/ICCV48922.2021.00593
   Liu L., 2020, Advances in Neural Information Processing Systems, V33, P15651
   Liu SC, 2019, IEEE I CONF COMP VIS, P7707, DOI 10.1109/ICCV.2019.00780
   Maier R, 2015, 2015 INTERNATIONAL CONFERENCE ON 3D VISION, P536, DOI 10.1109/3DV.2015.66
   Martin-Brualla R, 2021, PROC CVPR IEEE, P7206, DOI 10.1109/CVPR46437.2021.00713
   MAX N, 1995, IEEE T VIS COMPUT GR, V1, P99, DOI 10.1109/2945.468400
   Menon S, 2020, PROC CVPR IEEE, P2434, DOI 10.1109/CVPR42600.2020.00251
   Mescheder L, 2019, PROC CVPR IEEE, P4455, DOI 10.1109/CVPR.2019.00459
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Mildenhall B, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322980
   Müller T, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530127
   Niemeyer M, 2020, PROC CVPR IEEE, P3501, DOI 10.1109/CVPR42600.2020.00356
   Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025
   Park K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5845, DOI 10.1109/ICCV48922.2021.00581
   Peng S., 2020, COMPUTER VISION ECCV
   Pumarola A, 2021, PROC CVPR IEEE, P10313, DOI 10.1109/CVPR46437.2021.01018
   Saito S, 2019, IEEE I CONF COMP VIS, P2304, DOI 10.1109/ICCV.2019.00239
   Sajjadi Mehdi S. M., 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P4501, DOI 10.1109/ICCV.2017.481
   Schönberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445
   Sun C, 2022, PROC CVPR IEEE, P5449, DOI 10.1109/CVPR52688.2022.00538
   Tian YP, 2020, PROC CVPR IEEE, P3357, DOI 10.1109/CVPR42600.2020.00342
   Trevithick Alex, 2021, P IEEE CVF INT C COM, P15182
   Wang C, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P6445, DOI 10.1145/3503161.3547808
   Wang X., 2019, Computer Vision and Pattern Recognition Workshops (CVPRW), Long Beach, CA:IEEE
   Wang ZS, 2022, Arxiv, DOI arXiv:2212.04701
   Yanhong Zeng, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P528, DOI 10.1007/978-3-030-58517-4_31
   Yu A, 2021, PROC CVPR IEEE, P4576, DOI 10.1109/CVPR46437.2021.00455
NR 43
TC 0
Z9 0
U1 1
U2 1
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAY
PY 2024
VL 35
IS 3
AR e2240
DI 10.1002/cav.2240
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SN7J3
UT WOS:001235192800001
DA 2024-08-05
ER

PT J
AU Li, HY
   Yang, M
   Yang, C
   Kang, JL
   Suo, X
   Meng, WL
   Li, Z
   Mao, LJ
   Sheng, B
   Qi, J
AF Li, Hongyu
   Yang, Meng
   Yang, Chao
   Kang, Jianglang
   Suo, Xiang
   Meng, Weiliang
   Li, Zhen
   Mao, Lijuan
   Sheng, Bin
   Qi, Jun
TI Soccer match broadcast video analysis method based on detection and
   tracking
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE field localization; player tracking; soccer ball detection; video
   analysis; visualizations
AB We propose a comprehensive soccer match video analysis pipeline tailored for broadcast footage, which encompasses three pivotal stages: soccer field localization, player tracking, and soccer ball detection. Firstly, we introduce sports camera calibration to seamlessly map soccer field images from match videos onto a standardized two-dimensional soccer field template. This addresses the challenge of consistent analysis across video frames amid continuous camera angle changes. Secondly, given challenges such as occlusions, high-speed movements, and dynamic camera perspectives, obtaining accurate position data for players and the soccer ball is non-trivial. To mitigate this, we curate a large-scale, high-precision soccer ball detection dataset and devise a robust detection model, which achieved the mAP50-95$$ mA{P}_{50-95} $$ of 80.9%. Additionally, we develop a high-speed, efficient, and lightweight tracking model to ensure precise player tracking. Through the integration of these modules, our pipeline focuses on real-time analysis of the current camera lens content during matches, facilitating rapid and accurate computation and analysis while offering intuitive visualizations.
C1 [Li, Hongyu; Yang, Meng; Yang, Chao; Kang, Jianglang] Beijing Forestry Univ, Sch Informat Sci & Technol, Beijing, Peoples R China.
   [Yang, Meng] Natl Forestry & Grassland Adm, Engn Res Ctr Forestry Oriented Intelligent Informa, Beijing, Peoples R China.
   [Suo, Xiang; Li, Zhen; Mao, Lijuan] Shanghai Univ Sport, Sch Athlet Performance, Shanghai, Peoples R China.
   [Meng, Weiliang] Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing, Peoples R China.
   [Meng, Weiliang] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.
   [Sheng, Bin] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai, Peoples R China.
   [Qi, Jun] Xian JiaoTong Liverpool Univ, Dept Comp, Suzhou, Peoples R China.
C3 Beijing Forestry University; Shanghai University of Sport; Chinese
   Academy of Sciences; Institute of Automation, CAS; Chinese Academy of
   Sciences; University of Chinese Academy of Sciences, CAS; Shanghai Jiao
   Tong University; Xi'an Jiaotong-Liverpool University
RP Yang, M (corresponding author), Beijing Forestry Univ, Sch Informat Sci & Technol, Beijing, Peoples R China.; Mao, LJ (corresponding author), Shanghai Univ Sport, Sch Athlet Performance, Shanghai, Peoples R China.; Meng, WL (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.
EM yangmeng@bjfu.edu.cn; weiliang.meng@ia.ac.cn; maolijuan@sus.edu.cn
OI Li, Hongyu/0009-0008-2866-303X; SUO, Xiang/0009-0000-5899-0703; Qi,
   Jun/0000-0002-8761-8318; meng, wei liang/0000-0002-3221-4981; YANG,
   Meng/0000-0001-6439-2873
FU Beijing Natural Science Foundation; National Natural Science Foundation
   of China [62077037, 62376231, U22B2034, 62365014, 62262043, 62171321,
   62301452, 52175493]; Open Project Program of State Key Laboratory of
   Virtual Reality Technology and Systems, Beihang University
   [VRLAB2023B01];  [L231013]
FX This work was supported in part by National Natural Science Foundation
   of China (62077037, 62376231, U22B2034, 62365014, 62262043, 62171321,
   62301452 and 52175493), Beijing Natural Science Foundation L231013, and
   in part by the Open Project Program of State Key Laboratory of Virtual
   Reality Technology and Systems, Beihang University (No. VRLAB2023B01).
CR Aharon N, 2022, Arxiv, DOI [arXiv:2206.14651, DOI 10.48550/ARXIV.2206.14651]
   Akan S, 2023, MULTIMEDIA SYST, V29, P897, DOI 10.1007/s00530-022-01027-0
   Akyon FC., 2022, Slicing aided hyper inference and finetuning for small object detection. 2022 IEEE international conference on image processing (ICIP). IEEE
   Alvarez L., 2014, Homography estimation using one ellipse correspondence and minimal additional information. 2014 IEEE international conference on image processing (ICIP). IEEE
   Baker S, 2004, INT J COMPUT VISION, V56, P221, DOI 10.1023/B:VISI.0000011205.11775.fd
   Cao CQ, 2019, IEEE ACCESS, V7, P106838, DOI 10.1109/ACCESS.2019.2932731
   Chen J., 2019, Sports camera calibration via synthetic data. 2019 IEEE/CVF conference on computer vision and pattern recognition workshops (CVPRW)
   Cheng G, 2023, IEEE T PATTERN ANAL, V45, P13467, DOI 10.1109/TPAMI.2023.3290594
   Cioppa A., 2022, Soccernettracking: multiple object tracking dataset and benchmark in soccer videos. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition
   Cuevas C, 2020, PATTERN RECOGN, V103, DOI 10.1016/j.patcog.2020.107278
   Cui Y., 2023, SportsMOT: a large multiobject tracking dataset in multiple sports scenes. Proceedings of the IEEE/CVF international conference on computer vision (ICCV). The Organization
   D'Orazio T, 2010, PATTERN RECOGN, V43, P2911, DOI 10.1016/j.patcog.2010.03.009
   Dendorfer P., 2020, arXiv
   Di Salvo V, 2007, INT J SPORTS MED, V28, P222, DOI 10.1055/s-2006-924294
   [杜紫薇 Du Ziwei], 2022, [计算机科学, Computer Science], V49, P205
   Homayounfar N, 2017, PROC CVPR IEEE, P4012, DOI 10.1109/CVPR.2017.427
   Kalafatic Zoran, 2022, 2022 45th Jubilee International Convention on Information, Communication and Electronic Technology (MIPRO)., P936, DOI 10.23919/MIPRO55190.2022.9803576
   Komorowski J, 2020, Arxiv, DOI arXiv:1912.05445
   Komorowski J, 2019, Arxiv, DOI arXiv:1902.07304
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Manafifard M, 2017, COMPUT VIS IMAGE UND, V159, P19, DOI 10.1016/j.cviu.2017.02.002
   Meinhardt T., 2022, Trackformer: multiobject tracking with transformers. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition
   Milan Anton, 2016, ARXIV160300831
   Naik BT, 2022, IEEE ACCESS, V10, P32494, DOI 10.1109/ACCESS.2022.3161441
   Qin Qiangqiang, 2023, Journal of Computer Applications, V43, P3579, DOI 10.11772/j.issn.1001-9081.2022111660
   Sharma RA., 2018, Automated top view registration of broadcast football videos. 2018 IEEE winter conference on applications of computer vision (WACV). IEEE
   Sheng B, 2021, IEEE T CYBERNETICS, V51, P1463, DOI 10.1109/TCYB.2020.2988792
   Song Z., 2021, Distractoraware tracker with a domainspecial optimized benchmark for soccer player tracking. Proceedings of the 2021 international conference on multimedia retrieval
   Tarashima S, 2023, Arxiv, DOI arXiv:2311.05237
   Naik BT, 2024, CONNECT SCI, V36, DOI 10.1080/09540091.2023.2291991
   Tong K, 2020, IMAGE VISION COMPUT, V97, DOI 10.1016/j.imavis.2020.103910
   Veeramani B, 2018, BMC BIOINFORMATICS, V19, DOI 10.1186/s12859-018-2267-2
   Wang TY, 2022, COMPUT INTEL NEUROSC, V2022, DOI 10.1155/2022/3540642
   [伍瀚 Wu Han], 2023, [计算机科学, Computer Science], V50, P77
   Xie EZ, 2021, ADV NEUR IN, V34
   Xingyi Zhou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P474, DOI 10.1007/978-3-030-58548-8_28
   Yuan X., 2023, Small object detection via coarsetofine proposal generation and imitation learning. Proceedings of the IEEE/CVF international conference on computer vision
   Zhang YF, 2022, LECT NOTES COMPUT SC, V13682, P1, DOI 10.1007/978-3-031-20047-2_1
NR 38
TC 0
Z9 0
U1 2
U2 2
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAY
PY 2024
VL 35
IS 3
AR e2259
DI 10.1002/cav.2259
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SI2C4
UT WOS:001233750100001
DA 2024-08-05
ER

PT J
AU Marney, B
   Haworth, B
AF Marney, Brendan
   Haworth, Brandon
TI Toward comprehensive Chiroptera modeling: A parametric multiagent model
   for bat behavior
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE Chiroptera; flocking simulation; multiagent behavior modeling
ID ECHOLOCATION
AB Chiroptera behavior is complex and often unseen as bats are nocturnal, small, and elusive animals. Chiroptology has led to significant insights into the behavior and environmental interactions of bats. Biology, ecology, and even digital media often benefit from mathematical models of animals including humans. However, the history of Chiroptera modeling is often limited to specific behaviors, species, or biological functions and relies heavily on classical modeling methodologies that may not fully represent individuals or colonies well. This work proposes a continuous, parametric, multiagent, Chiroptera behavior model that captures the latest research in echolocation, hunting, and energetics of bats. This includes echolocation-based perception (or lack thereof), hunting patterns, roosting behavior, and energy consumption rates. We proposed the integration of these mathematical models in a framework that affords the individual simulation of bats within large-scale colonies. Practitioners can adjust the model to account for different perceptual affordances or patterns among species of bats, or even individuals (such as sickness or injury). We show that our model closely matches results from the literature, affords an animated graphical simulation, and has utility in simulation-based studies.
   A parametric bat model for visualizing and exploring bat behaviours, species affordances, and environment interactions. image
C1 [Marney, Brendan; Haworth, Brandon] Univ Victoria, Dept Comp Sci, Victoria, BC, Canada.
   [Haworth, Brandon] Univ Victoria, Dept Comp Sci, Engn & Comp Sci ECS Bldg, Victoria, BC, Canada.
C3 University of Victoria; University of Victoria
RP Haworth, B (corresponding author), Univ Victoria, Dept Comp Sci, Engn & Comp Sci ECS Bldg, Victoria, BC, Canada.
EM bhaworth@uvic.ca
OI Haworth, Brandon/0000-0001-8134-0047
CR Aihara I, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0068635
   Andrusiak L., 2021, 20162020 BC Bat Action Plan Updated for 2021, P155
   BARAK Y, 1989, J ZOOL, V219, P670, DOI 10.1111/j.1469-7998.1989.tb02608.x
   BARCLAY RMR, 1982, J MAMMAL, V63, P464, DOI 10.2307/1380444
   Beleyur T., 2021, Theoretical and empirical investigations of echolocation in bat groups
   Brigham RM, 2023, CAN J ZOOL, V101, P242, DOI 10.1139/cjz-2022-0113
   Cheng TL, 2021, CONSERV BIOL, V35, P1586, DOI 10.1111/cobi.13739
   Cvikel N, 2015, CURR BIOL, V25, P206, DOI 10.1016/j.cub.2014.11.010
   Dechmann DKN, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0009012
   Dechmann DKN, 2009, P ROY SOC B-BIOL SCI, V276, P2721, DOI 10.1098/rspb.2009.0473
   Gillam EH, 2010, J MAMMAL, V91, P967, DOI 10.1644/09-MAMM-A-302.1
   Goerlitz HR, 2010, CURR BIOL, V20, P1568, DOI 10.1016/j.cub.2010.07.046
   Grunwald JE, 2004, P NATL ACAD SCI USA, V101, P5670, DOI 10.1073/pnas.0308029101
   Guy SJ, 2010, PROCEEDINGS OF THE TWENTY-SIXTH ANNUAL SYMPOSIUM ON COMPUTATIONAL GEOMETRY (SCG'10), P115, DOI 10.1145/1810959.1810981
   He XS, 2014, NEURAL COMPUT APPL, V25, P459, DOI 10.1007/s00521-013-1518-4
   Huang P., 2013, P 12 ACM SIGGRAPH EU, P135, DOI [DOI 10.1145/2485895, DOI 10.1145/2485895.2485911]
   Kolli H., 2007, Study of Interaction Between Mexican Freetailed Bats (Tadarida Brasiliensis) and Moths and Counting Moths in a Real Time Video
   Kunz TH., 2013, ECOLOGY BATS
   Reynolds C., 1999, STEERING BEHAV AUTON, P763
   Reynolds C.W., 1987, P 14 ANN C COMPUTE, P25, DOI [10.1145/37402.37406, DOI 10.1145/37402.37406, 10.1145/37401.37406, DOI 10.1145/37401.37406]
   Stidsholt L, 2023, ELIFE, V12, DOI 10.7554/eLife.84190
   Stoffregen TA, 1995, ECOL PSYCHOL, V7, P181
   Thomas J.A., 2004, Echolocation in Bats and Dolphins
   Tian XD, 2006, BIOINSPIR BIOMIM, V1, pS10, DOI 10.1088/1748-3182/1/4/S02
   Whittle M., 2014, Gait Analysis An Introduction
   WILKINSON GS, 1990, SCI AM, V262, P76, DOI 10.1038/scientificamerican0290-76
   Yang XS, 2010, STUD COMPUT INTELL, V284, P65, DOI 10.1007/978-3-642-12538-6_6
   Zelenka J, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-72999-0
NR 28
TC 0
Z9 0
U1 1
U2 1
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAY
PY 2024
VL 35
IS 3
AR e2251
DI 10.1002/cav.2251
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UD0U0
UT WOS:001246013700001
OA hybrid
DA 2024-08-05
ER

PT J
AU Huang, DJ
   Liu, CM
   Liu, JH
AF Huang, Dongjin
   Liu, Chuanman
   Liu, Jinhua
TI GPSwap: High-resolution face swapping based on StyleGAN prior
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE face swapping; high resolution; StyleGAN prior; training costs
AB Existing high-resolution face-swapping works are still challenges in preserving identity consistency while maintaining high visual quality. We present a novel high-resolution face-swapping method GPSwap, which is based on StyleGAN prior. To better preserves identity consistency, the proposed facial feature recombination network fully leverages the properties of both w space and encoders to decouple identities. Furthermore, we presents the image reconstruction module aligns and blends images in FS space, which further supplements facial details and achieves natural blending. It not only improves image resolution but also optimizes visual quality. Extensive experiments and user studies demonstrate that GPSwap is superior to state-of-the-art high-resolution face-swapping methods in terms of image quality and identity consistency. In addition, GPSwap saves nearly 80% of training costs compared to other high-resolution face-swapping works.
   We present a novel high-resolution face-swapping method GPSwap, which is based on StyleGAN prior. Extensive experiments demonstrate that GPSwap is superior to other high-resolution face-swapping methods in terms of image quality and identity consistency. In addition, GPSwap saves nearly 80% of training costs. image
C1 [Huang, Dongjin; Liu, Chuanman; Liu, Jinhua] Shanghai Univ, Shanghai Film Acad, 149 Yanchang Rd, Shanghai, Peoples R China.
C3 Shanghai University
RP Huang, DJ (corresponding author), Shanghai Univ, Shanghai Film Acad, 149 Yanchang Rd, Shanghai, Peoples R China.
EM djhuang@shu.edu.cn
OI Liu, Chuanman/0009-0001-0246-9561
FU Science and Technology Projects of National Archives Administration of
   China
FX Shanghai Talent Development Funding of China, Grant/Award Number:
   2021016; Science and Technology Projects of National Archives
   Administration of China, Grant/Award Number: 2023-X-036r No Statement
   Available
CR Bulat A, 2017, IEEE I CONF COMP VIS, P1021, DOI 10.1109/ICCV.2017.116
   Chen RW, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2003, DOI 10.1145/3394171.3413630
   Chen S, 2018, LECT NOTES COMPUT SC, V10996, P428, DOI 10.1007/978-3-319-97909-0_46
   Deng JK, 2022, IEEE T PATTERN ANAL, V44, P5962, DOI 10.1109/TPAMI.2021.3087709
   Groshev A, 2022, IEEE ACCESS, V10, P83452, DOI 10.1109/ACCESS.2022.3196668
   Hensel M, 2017, ADV NEUR IN, V30
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T, 2018, Arxiv, DOI [arXiv:1710.10196, DOI 10.48550/ARXIV.1710.10196]
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Li LZ, 2020, Arxiv, DOI arXiv:1912.13457
   Liu KL, 2023, PATTERN RECOGN, V141, DOI 10.1016/j.patcog.2023.109628
   Nitzan Y, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417826
   Rössler A, 2019, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2019.00009
   Rosberg F., 2023, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, Waikoloa, P3454
   Ruiz N, 2018, IEEE COMPUT SOC CONF, P2155, DOI 10.1109/CVPRW.2018.00281
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tishby N, 2015, 2015 IEEE INFORMATION THEORY WORKSHOP (ITW)
   Ulyanov D, 2018, PROC CVPR IEEE, P9446, DOI 10.1109/CVPR.2018.00984
   Vemulapalli R, 2019, PROC CVPR IEEE, P5676, DOI 10.1109/CVPR.2019.00583
   Wang XY, 2019, IEEE I CONF COMP VIS, P6970, DOI 10.1109/ICCV.2019.00707
   Wang Y., 2021, Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, (IJCAI21)
   Wei R, 2023, COMPUT ANIMAT VIRT W, V34, DOI 10.1002/cav.2155
   Wu ZZ, 2021, PROC CVPR IEEE, P12858, DOI 10.1109/CVPR46437.2021.01267
   Xia WH, 2023, IEEE T PATTERN ANAL, V45, P3121, DOI 10.1109/TPAMI.2022.3181070
   Xu Chao, 2022, P IEEECVF C COMPUTER, P7632
   Xu Y., 2022, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, P7642
   Xu ZL, 2022, LECT NOTES COMPUT SC, V13674, P661, DOI 10.1007/978-3-031-19781-9_38
   Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20
   Zhu P., 2021, ACM Trans Graph, V40, P1
   Zhu YH, 2021, PROC CVPR IEEE, P4832, DOI 10.1109/CVPR46437.2021.00480
NR 31
TC 0
Z9 0
U1 0
U2 0
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD JUL
PY 2024
VL 35
IS 4
AR e2238
DI 10.1002/cav.2238
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YQ2X4
UT WOS:001269895000001
DA 2024-08-05
ER

PT J
AU Kang, HYM
   Han, JHY
AF Kang, Hyunmo
   Han, Junghyun
TI Screen-space Streamline Seeding Method for Visualizing Unsteady Flow in
   Augmented Reality
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE augmented reality; flow visualization; streamlines
ID OPACITY OPTIMIZATION; PLACEMENT
AB Streamlines are a popular method of choice in many flow visualization techniques due to their simplicity and intuitiveness. This paper presents a novel streamline seeding method, which is tailored for visualizing unsteady flow in augmented reality (AR). Our method prioritizes visualizing the visible part of the flow field to enhance the flow representation's quality and reduce the computational cost. Being an image-based method, it evenly samples 2D seeds from the screen space. Then, a ray is fired toward each 2D seed, and the on-the-ray point, which has the largest entropy, is selected. It is taken as the 3D seed for a streamline. By advecting such 3D seeds in the velocity field, which is continuously updated in real time, the unsteady flow is visualized more naturally, and the temporal coherence is achieved with no extra efforts. Our method is tested using an AR application for visualizing airflow from a virtual air conditioner. Comparison with the baseline methods shows that our method is suitable for visualizing unsteady flow in AR.
   This paper presents a novel streamline seeding method, which is tailored for visualizing unsteady flow in AR. The method prioritizes visualizing the visible part of the flow field to enhance the flow representation's quality and reduce the computational cost. It is tested using an AR application for visualizing airflow from a virtual air conditioner. image
C1 [Kang, Hyunmo; Han, Junghyun] Korea Univ, Superintelligence Res Ctr, Seoul, South Korea.
   [Han, Junghyun] Korea Univ, Dept Comp Sci & Engn, Seoul, South Korea.
   [Han, Junghyun] Korea Univ, Seoul, South Korea.
C3 Korea University; Korea University; Korea University
RP Han, JHY (corresponding author), Korea Univ, Seoul, South Korea.
EM jhan@korea.ac.kr
FU Ministry of Science and ICT, South Korea [IITP-2024-2020-0-01819];
   Ministry of Science and ICT, Korea, under the ICT Creative Consilience
   Program [IITP-2024-2020-0-01460, 2020-0-00861]; ITRC (Information
   Technology Research Center)
FX This research was supported by the Ministry of Science and ICT, Korea,
   under the ICT Creative Consilience Program (IITP-2024-2020-0-01819),
   ITRC (Information Technology Research Center) Support Program
   (IITP-2024-2020-0-01460) and the grant no. 2020-0-00861.
CR Apple LLC, 2020, Arkit development package
   Chae J., 2022, Virtual air conditioner's airflow simulation and visualization in ar. Proceedings of the 28th ACM symposium on virtual reality software and technology
   Eissele M., 2008, GRAPH INTERFACE, P89
   Günther T, 2017, COMPUT GRAPH FORUM, V36, P153, DOI 10.1111/cgf.13115
   Günther T, 2014, COMPUT GRAPH FORUM, V33, P11, DOI 10.1111/cgf.12357
   Günther T, 2014, COMPUT GRAPH FORUM, V33, P507, DOI 10.1111/cgf.12336
   Günther T, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461930
   Han J, 2020, IEEE T VIS COMPUT GR, V26, P1732, DOI 10.1109/TVCG.2018.2880207
   Heuveline V., 2011, PREPRINT
   Hua WP, 2023, Arxiv, DOI arXiv:2305.08229
   Jobard Bruno., 1997, VISUALIZATION SCI CO, P43
   Kanzler M, 2016, COMPUT GRAPH-UK, V61, P29, DOI 10.1016/j.cag.2016.08.001
   Lee TY., 2011, View point evaluation and streamline filtering for flow visualization. 2011 IEEE Pacific visualization symposium
   Li LY, 2007, IEEE T VIS COMPUT GR, V13, P630, DOI 10.1109/TVCG.2007.1009
   Ma J, 2019, J VISUAL-JAPAN, V22, P1125, DOI 10.1007/s12650-019-00592-3
   Ma J, 2013, PROC SPIE, V8654, DOI 10.1117/12.2001887
   Malkawi AM, 2005, AUTOMAT CONSTR, V14, P71, DOI 10.1016/j.autcon.2004.08.001
   Marchesin S, 2010, IEEE T VIS COMPUT GR, V16, P1578, DOI 10.1109/TVCG.2010.212
   McLoughlin T, 2013, IEEE T VIS COMPUT GR, V19, P1342, DOI 10.1109/TVCG.2012.150
   McLoughlin T, 2010, COMPUT GRAPH FORUM, V29, P1807, DOI 10.1111/j.1467-8659.2010.01650.x
   Nsonga B., 2023, Visualizing similarity of pathline dynamics in 2d flow fields. 2023 IEEE visualization and visual analytics (VIS)
   Rojo IB, 2020, IEEE T VIS COMPUT GR, V26, P3204, DOI 10.1109/TVCG.2019.2915222
   Sane S, 2020, COMPUT GRAPH FORUM, V39, P785, DOI 10.1111/cgf.14036
   SHANNON CE, 1948, BELL SYST TECH J, V27, P379, DOI 10.1002/j.1538-7305.1948.tb01338.x
   Simoudis E., 1996, KDD 96 P 2 INT C KNO, P226
   Spencer B, 2009, COMPUT GRAPH FORUM, V28, P1618, DOI 10.1111/j.1467-8659.2009.01352.x
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   Tao J, 2013, IEEE T VIS COMPUT GR, V19, P393, DOI 10.1109/TVCG.2012.143
   Tian X., 2019, Simulation and visualization of fluid flows around real objects in augmented reality. EuroVis (Short Papers)
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Xu CY, 1997, PROC CVPR IEEE, P66, DOI 10.1109/CVPR.1997.609299
   Xu LJ, 2010, IEEE T VIS COMPUT GR, V16, P1216, DOI 10.1109/TVCG.2010.131
   Yabuki N., 2012, Collaborative visualization of environmental simulation result and sensing data using augmented reality. Cooperative design, visualization, and engineering: 9th international conference, CDVE 2012, Osaka, Japan, September 25, 2012. Proceedings 9
   Zafar A, 2024, IEEE T VIS COMPUT GR, V30, P716, DOI 10.1109/TVCG.2023.3326603
   Zhang W., 2010, A streamline placement method highlighting flow field topology. 2010 international conference on computational intelligence and security
   Zhang W., 2011, Multiresolution streamline placement for 2d flow fields. 2011 seventh international conference on computational intelligence and security
   Zhang WY, 2014, INTEGR COMPUT-AID E, V21, P47, DOI 10.3233/ICA-130447
   Zhang WY, 2013, IEEE T VIS COMPUT GR, V19, P1185, DOI 10.1109/TVCG.2012.169
NR 38
TC 0
Z9 0
U1 0
U2 0
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAY
PY 2024
VL 35
IS 3
AR e2250
DI 10.1002/cav.2250
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SN7K2
UT WOS:001235193700001
DA 2024-08-05
ER

PT J
AU Liu, GT
   Wong, SK
AF Liu, Guan-Ting
   Wong, Sai-Keung
TI Mastering broom-like tools for object transportation animation using
   deep reinforcement learning
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE agents; animation; curriculum learning; object transportation;
   reinforcement learning; tools
AB In this paper, we propose a deep reinforcement-based approach to generate an animation of an agent using a broom-like tool to transport a target object. The tool is attached to the agent. So when the agent moves, the tool moves as well.The challenge is to control the agent to move and use the tool to push the target while avoiding obstacles. We propose a direction sensor to guide the agent's movement direction in environments with static obstacles. Furthermore, different rewards and a curriculum learning are implemented to make the agent efficiently learn skills for manipulating the tool. Experimental results show that the agent can naturally control the tool with different shapes to transport target objects. The result of ablation tests revealed the impacts of the rewards and some state components.
   We propose a deep reinforcement-based approach to generate an animation of an agent using a broom-like tool to transport a target object. We develop various reward terms, a direction sensor, and a curriculum learning to make the agent learn skills for object manipulation and transportation. image
C1 [Liu, Guan-Ting; Wong, Sai-Keung] Natl Yang Ming Chiao Tung Univ, Coll Comp Sci, Hsinchu, Taiwan.
C3 National Yang Ming Chiao Tung University
RP Wong, SK (corresponding author), Natl Yang Ming Chiao Tung Univ, Coll Comp Sci, Hsinchu, Taiwan.
EM cswingo@nycu.edu.tw
FU National Science and Technology Council of the ROC; National Science and
   Technology Council of the R.O.C. [112-2221-E-A49-118]; NSTC
FX This research was supported by the National Science and Technology
   Council of the R.O.C. under grant no. NSTC 112-2221-E-A49-118.
CR Alkilabi MHM., 2015, Cooperative object transport using evolutionary swarm robotics methods. ECAL
   Bin Peng X, 2018, Arxiv, DOI [arXiv:1804.02717, DOI 10.1145/3197517.3201311]
   Chen H., 2019, Transportation strategies for boxmanipulation in crowd simulation. Proceedings of the 32nd international conference on computer animation and social agents
   Chen SC, 2021, COMPUT ANIMAT VIRT W, V32, DOI 10.1002/cav.2017
   Clegg A, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275048
   Devin C., 2017, Learning modular neural network policies for multitask and multirobot transfer. 2017 IEEE international conference on robotics and automation (ICRA). IEEE
   Eoh G, 2021, IEEE ACCESS, V9, P137281, DOI 10.1109/ACCESS.2021.3118109
   Gross R, 2009, INT J BIO-INSPIR COM, V1, P1, DOI 10.1504/IJBIC.2009.022770
   Guo SH, 2017, COMPUT ANIMAT VIRT W, V28, DOI 10.1002/cav.1779
   Haarnoja T, 2018, International Conference on Machine Learning (ICML), P1861
   Li R, 2020, IEEE INT CONF ROBOT, P4051, DOI [10.1109/ICRA40945.2020.9197468, 10.1109/icra40945.2020.9197468]
   Liu LB, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201315
   Luo YS, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392433
   Mason I, 2022, P ACM COMPUT GRAPH, V5, DOI 10.1145/3522618
   Peng XB., 2022, ACM Transactions On Graphics (TOG), V41, P1
   Peng XB, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073602
   Won J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130833
   Wong CC, 2022, COMPUT ANIMAT VIRT W, V33, DOI 10.1002/cav.2081
   Wong SK, 2023, COMPUT ANIMAT VIRT W, V34, DOI 10.1002/cav.2168
   Wong SK, 2018, ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES (I3D 2018), DOI 10.1145/3190834.3190839
   Wong SK, 2012, J INF SCI ENG, V28, P145
   Yang HY, 2019, P ACM COMPUT GRAPH, V2, DOI 10.1145/3320283
   Yao HY, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3550454.3555434
   Zhang YB, 2023, COMPUT GRAPH FORUM, V42, P25, DOI 10.1111/cgf.14741
NR 24
TC 0
Z9 0
U1 0
U2 0
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAY
PY 2024
VL 35
IS 3
AR e2255
DI 10.1002/cav.2255
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UH1Y3
UT WOS:001247086200001
DA 2024-08-05
ER

PT J
AU Zhang, MY
   Yang, JM
   Xian, YW
   Li, W
   Gu, JM
   Meng, WL
   Zhang, JG
   Zhang, XP
AF Zhang, Muyang
   Yang, Jinming
   Xian, Yuewei
   Li, Wei
   Gu, Jiaming
   Meng, Weiliang
   Zhang, Jiguang
   Zhang, Xiaopeng
TI AG-SDM: Aquascape generation based on stable diffusion model with
   low-rank adaptation
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE Aquascaping; ControlNet; diffusion model; LoRA
AB As an amalgamation of landscape design and ichthyology, aquascape endeavors to create visually captivating aquatic environments imbued with artistic allure. Traditional methodologies in aquascape, governed by rigid principles such as composition and color coordination, may inadvertently curtail the aesthetic potential of the landscapes. In this paper, we propose Aquascape Generation based on Stable Diffusion Models (AG-SDM), prioritizing aesthetic principles and color coordination to offer guiding principles for real artists in Aquascape creation. We meticulously curated and annotated three aquascape datasets with varying aspect ratios to accommodate diverse landscape design requirements regarding dimensions and proportions. Leveraging the Fr & eacute;chet Inception Distance (FID) metric, we trained AGFID for quality assessment. Extensive experiments validate that our AG-SDM excels in generating hyper-realistic underwater landscape images, closely resembling real flora, and achieves state-of-the-art performance in aquascape image generation.
   Our AG-SDM employs AI-guided artistic creation techniques in Aquascaping, refining models through LoRA fine-tuning and diffusion methods. We have curated and annotated three diverse datasets, introducing Aquascaping-specific evaluation metrics based on FID for streamlined research. The resulting Aquascaping images boast remarkable fidelity and aesthetics, complemented by our expansion into editable 3D representations, including Mesh, for enhanced creative exploration. image
C1 [Zhang, Muyang; Yang, Jinming; Li, Wei; Gu, Jiaming; Meng, Weiliang; Zhang, Jiguang; Zhang, Xiaopeng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.
   [Zhang, Muyang; Yang, Jinming; Li, Wei; Gu, Jiaming; Meng, Weiliang; Zhang, Jiguang; Zhang, Xiaopeng] Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence, Beijing, Peoples R China.
   [Xian, Yuewei] Natl Univ Def Technol, Coll Aerosp Technol & Engn, Changsha, Peoples R China.
   [Meng, Weiliang; Zhang, Jiguang] Chinese Acad Sci, Inst Automat, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Chinese Academy of Sciences; Institute of Automation, CAS; National
   University of Defense Technology - China; Chinese Academy of Sciences;
   Institute of Automation, CAS
RP Meng, WL; Zhang, JG (corresponding author), Chinese Acad Sci, Inst Automat, Beijing, Peoples R China.
EM weiliang.meng@ia.ac.cn; jiguang.zhang@ia.ac.cn
OI Zhang, Jiguang/0000-0002-8212-1361; meng, wei liang/0000-0002-3221-4981
FU National Natural Science Foundation of China; Beijing Natural Science
   Foundation [JQ23014]; Open Project Program of State Key Laboratory of
   Virtual Reality Technology and Systems, Beihang University
   [VRLAB2023B01];  [62376271];  [U22B2034];  [62172416];  [62365014]; 
   [62162414];  [52175493]
FX This work was supported by National Natural Science Foundation of China
   (Nos. 62376271, U22B2034, 62172416, 62365014, 62162414, and 52175493),
   Beijing Natural Science Foundation No. JQ23014, and the Open Project
   Program of State Key Laboratory of Virtual Reality Technology and
   Systems, Beihang University (number VRLAB2023B01).
CR [Anonymous], 2017, CAN: Creative adversarial networks, generating "art"by learning about styles and deviating from style norms
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Guo Y., 2024, P INT C LEARN REPR I
   Hensel M, 2017, ADV NEUR IN, V30
   Ho J., 2020, Adv. Neural. Inf. Process. Syst, V33, P6840, DOI DOI 10.48550/ARXIV.2006.11239
   Hu EJ., 2021, P 8 INT C LEARN REPR
   Hu L, 2024, Arxiv, DOI arXiv:2311.17117
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Kumari K., 2021, Pharma Innov J, V10, P240
   Ma H., 2024, Phygital Intelligence. CDRF 2023, DOI [10.1007/978-981-99-8405-3_3, DOI 10.1007/978-981-99-8405-3_3]
   McGee RW., 2023, Using ChatGPT and Bing image creator to create images of martial artists: an application of artificial intelligence to create art. SSRN No. 4665226
   Podell D., 2024, 12 INT C LEARNING RE
   Radford A, 2021, PR MACH LEARN RES, V139
   Ramesh A., 2022, Hierarchical text-conditional image generation with clip latents, DOI 10.48550/arXiv.2204.06125
   Ramesh A, 2021, PR MACH LEARN RES, V139
   Ranftl R, 2022, IEEE T PATTERN ANAL, V44, P1623, DOI 10.1109/TPAMI.2020.3019967
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Shilova V, 2023, Arxiv, DOI arXiv:2309.11507
   UENO O, 1988, P NATL ACAD SCI USA, V85, P6733, DOI 10.1073/pnas.85.18.6733
   Wu JJ, 2016, ADV NEUR IN, V29
   Xing JB, 2023, Arxiv, DOI arXiv:2310.12190
   Yin W, 2023, IEEE T PATTERN ANAL, V45, P6480, DOI 10.1109/TPAMI.2022.3209968
   Zhang LM, 2023, IEEE I CONF COMP VIS, P3813, DOI 10.1109/ICCV51070.2023.00355
   Zhao WG, 2024, J PLANT RES, V137, P279, DOI 10.1007/s10265-024-01521-8
NR 25
TC 0
Z9 0
U1 11
U2 11
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAY
PY 2024
VL 35
IS 3
AR e2252
DI 10.1002/cav.2252
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RF7B5
UT WOS:001226307000001
DA 2024-08-05
ER

PT J
AU Wang, LY
   Wu, YQ
   Yang, YL
   Liu, C
   Jin, XG
AF Wang, Luyuan
   Wu, Yiqian
   Yang, Yong-Liang
   Liu, Chen
   Jin, Xiaogang
TI Identity-consistent transfer learning of portraits for digital apparel
   sample display
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE digital apparel; portrait authenticity; StyleGAN; transfer learning;
   uncanny valley effect
AB The rapid development of the online apparel shopping industry demands innovative solutions for high-quality digital apparel sample displays with virtual avatars. However, developing such displays is prohibitively expensive and prone to the well-known "uncanny valley" effect, where a nearly human-looking artifact arouses eeriness and repulsiveness, thus affecting the user experience. To effectively mitigate the "uncanny valley" effect and improve the overall authenticity of digital apparel sample displays, we present a novel photo-realistic portrait generation framework. Our key idea is to employ transfer learning to learn an identity-consistent mapping from the latent space of rendered portraits to that of real portraits. During the inference stage, the input portrait of an avatar can be directly transferred to a realistic portrait by changing its appearance style while maintaining the facial identity. To this end, we collect a new dataset, Daz-Rendered-Faces-HQ (DRFHQ), specifically designed for rendering-style portraits. We leverage this dataset to fine-tune the StyleGAN2-FFHQ generator, using our carefully crafted framework, which helps to preserve the geometric and color features relevant to facial identity. We evaluate our framework using portraits with diverse gender, age, and race variations. Qualitative and quantitative evaluations, along with ablation studies, highlight our method's advantages over state-of-the-art approaches.
   Our study presents a novel photo-realistic portrait generation framework to mitigate the "uncanny valley" effect in digital apparel displays. By employing transfer learning, we map rendered avatars to realistic portraits while maintaining facial identity. Using the new DRFHQ dataset, we fine-tuned the StyleGAN2-FFHQ generator, achieving superior results in authenticity and user experience compared to existing methods. image
C1 [Wang, Luyuan; Wu, Yiqian; Liu, Chen; Jin, Xiaogang] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310058, Peoples R China.
   [Yang, Yong-Liang] Univ Bath, Dept Comp Sci, Bath, England.
C3 Zhejiang University; University of Bath
RP Jin, XG (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310058, Peoples R China.
EM jin@cad.zju.edu.cn
OI Jin, Xiaogang/0000-0001-7339-2920
FU Key Research and Development Program of Zhejiang Province
FX No Statement Availabler No Statement Availabler No Statement Available
CR Abdal R., 2020, P IEEECVF C COMPUTER, P8296
   Abdal R, 2019, IEEE I CONF COMP VIS, P4431, DOI 10.1109/ICCV.2019.00453
   Alaluf Y, 2022, PROC CVPR IEEE, P18490, DOI 10.1109/CVPR52688.2022.01796
   Alaluf Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6691, DOI 10.1109/ICCV48922.2021.00664
   Browzwear, Browzwear Solutions Pte Ltd. 20002024
   Chandran P, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480509
   Chen SY, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459760
   CLO, CLO Virtual Fashion LLC. 2024
   Daz Productions, Daz Productions Inc. 2024
   Debevec P, 2000, COMP GRAPH, P145, DOI 10.1145/344779.344855
   Flickr, Flickr. 2024
   Gal R, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530164
   Garbin Stephan J., 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P220, DOI 10.1007/978-3-030-58604-1_14
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Hensel M, 2017, ADV NEUR IN, V30
   Ho CC, 2017, INT J SOC ROBOT, V9, P129, DOI 10.1007/s12369-016-0380-9
   Huang YG, 2020, Arxiv, DOI [arXiv:2004.00288, 10.48550/ARXIV.2004.00288, DOI 10.48550/ARXIV.2004.00288]
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T, 2021, ADV NEUR IN, V34
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kazemi V, 2014, PROC CVPR IEEE, P1867, DOI 10.1109/CVPR.2014.241
   Lehtinen J., 2020, PROC NEURIPS, V33, P12104
   Liu HY, 2023, PROC CVPR IEEE, P10072, DOI 10.1109/CVPR52729.2023.00971
   Liu MC, 2021, ADV NEUR IN, V34
   Liu PX, 2024, Arxiv, DOI arXiv:2402.00827
   Meng C., 2021, arXiv
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Moore RK, 2012, SCI REP-UK, V2, DOI 10.1038/srep00864
   Mori M., 1970, Energy, V7, P33, DOI DOI 10.1109/MRA.2012.2192811
   Pinkney JNM, 2020, Arxiv, DOI arXiv:2010.05334
   Oliver MM, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0231266
   Optitex, OPTITEX. 19882022
   Pan XG, 2023, PROCEEDINGS OF SIGGRAPH 2023 CONFERENCE PAPERS, SIGGRAPH 2023, DOI 10.1145/3588432.3591500
   Pehlivan H, 2023, PROC CVPR IEEE, P1828, DOI 10.1109/CVPR52729.2023.00182
   Richardson E, 2021, PROC CVPR IEEE, P2287, DOI 10.1109/CVPR46437.2021.00232
   Riviere J, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392464
   Roich D, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3544777
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Sang S, 2022, PROCEEDINGS SIGGRAPH ASIA 2022, DOI 10.1145/3550469.3555402
   Seyama J, 2007, PRESENCE-TELEOP VIRT, V16, P337, DOI 10.1162/pres.16.4.337
   Song GX, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459771
   Sun TC, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417821
   Synthesis AI, Synthesis AI. 2022
   Tov O, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459838
   Wood E, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3661, DOI 10.1109/ICCV48922.2021.00366
   Wu Zongze, 2021, StyleAlign: analysis and applications of aligned StyleGAN models. arXiv preprint arXiv:2110.11323
   Xiong ZY, 2023, IEEE I CONF COMP VIS, P9253, DOI 10.1109/ICCV51070.2023.00852
   Yunjey Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8185, DOI 10.1109/CVPR42600.2020.00821
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang Ziyi, 2022, ADV NEURAL INFORM PR
   Zhu P., 2022, 10th International Conference on Learning Representations, P1
   Zllrunning, faceparsing.PyTorch. 2019
NR 52
TC 0
Z9 0
U1 1
U2 1
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAY 6
PY 2024
VL 35
IS 3
AR e2278
DI 10.1002/cav.2278
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TL3H8
UT WOS:001241373800001
DA 2024-08-05
ER

PT J
AU Feng, JZ
   He, C
   Wang, GR
   Wang, ML
AF Feng, Jingze
   He, Chong
   Wang, Guorui
   Wang, Meili
TI S-LASSIE: Structure and smoothness enhanced learning from sparse image
   ensemble for 3D articulated shape reconstruction
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE bone fusion and smoothing; monocular sparse images; multi-angle
   projection loss; quadruped 3D reconstruction
AB In computer vision, the task of 3D reconstruction from monocular sparse images poses significant challenges, particularly in the field of animal modelling. The diverse morphology of animals, their varied postures, and the variable conditions of image acquisition significantly complicate the task of accurately reconstructing their 3D shape and pose from a monocular image. To address these complexities, we propose S-LASSIE, a novel technique for 3D reconstruction of quadrupeds from monocular sparse images. It requires only 10-30 images of similar breeds for training. To effectively mitigate depth ambiguities inherent in monocular reconstructions, S-LASSIE employs a multi-angle projection loss function. In addition, our approach, which involves fusion and smoothing of bone structures, resolves issues related to disjointed topological structures and uneven connections at junctions, resulting in 3D models with comprehensive topologies and improved visual fidelity. Our extensive experiments on the Pascal-Part and LASSIE datasets demonstrate significant improvements in keypoint transfer, overall 2D IOU and visual quality, with an average keypoint transfer and overall 2D IOU of 59.6% and 86.3%, respectively, which are superior to existing techniques in the field.
   This paper introduces S-LASSIE, a novel method for reconstructing quadrupeds from monocular sparse images. Utilizing a multi-angle projection loss function and a bone fusion smoothing technique, this approach enhances reconstruction results and ensures that the resulting mesh possesses a comprehensive topological structure. image
C1 [Feng, Jingze; He, Chong; Wang, Guorui; Wang, Meili] NorthWest A&F Univ, Coll Informat Engn, Yangling, Shaanxi, Peoples R China.
C3 Northwest A&F University - China
RP Wang, ML (corresponding author), NorthWest A&F Univ, Coll Informat Engn, Yangling, Shaanxi, Peoples R China.
EM wml@nwsuaf.edu.cn
FU National Key Research and Development Program of China; Shaanxi Province
   Key Research and Development Program [2023KXJ-109];  [2022YFD1300200]
FX This work was funded by the National Key Research and Development
   Program of China: 2022YFD1300200 and Shaanxi Province Key Research and
   Development Program: 2023KXJ-109.
CR Amir S, 2022, Arxiv, DOI arXiv:2112.05814
   Badger M., 2020, 3D bird reconstruction: a dataset, model, and shape recovery from a single view. European conference on computer vision, P1
   Biggs B., 2020, Who left the dogs out? 3D animal reconstruction with expectation maximization in the loop. Computer visionECCV 2020: 16th European conference, Glasgow, UK, August 2328, 2020, proceedings, part XI 16, P195
   Chen L, 2014, PROC CVPR IEEE, P1027, DOI 10.1109/CVPR.2014.135
   Hu SX, 2022, FRONT PHYS-LAUSANNE, V10, DOI 10.3389/fphy.2022.839582
   Hung WC, 2019, PROC CVPR IEEE, P869, DOI 10.1109/CVPR.2019.00096
   Jakab T, 2024, Arxiv, DOI arXiv:2304.10535
   Kearney S., 2020, Rgbddog: predicting canine pose from rgbd sensors. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, P8336
   Kulikov V, 2020, PROC CVPR IEEE, P3842, DOI 10.1109/CVPR42600.2020.00390
   Kulkarni N., 2020, Articulationaware canonical surface mapping. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, P452
   Kulkarni N, 2019, IEEE I CONF COMP VIS, P2202, DOI 10.1109/ICCV.2019.00229
   Li Chen., 2021, Adv Neural Inf Process Syst, V34, P11757
   Li C, 2021, Arxiv, DOI arXiv:2106.10102
   Li X., 2020, Selfsupervised singleview 3D reconstruction via semantic consistency. Computer visionECCV 2020: 16th European conference, Glasgow, UK, August 2328, 2020, proceedings, part XIV 16, P677
   Liu S., 2019, Soft rasterizer: a differentiable renderer for imagebased 3D reasoning. Proceedings of the IEEE/CVF international conference on computer vision, P7708
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   MACKIEWICZ A, 1993, COMPUT GEOSCI, V19, P303, DOI 10.1016/0098-3004(93)90090-R
   Paszke A, 2019, ADV NEUR IN, V32
   Rueegg N., 2022, Barc: learning to regress 3D dog shape from images by exploiting breed information. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, P3876
   Rüegg N, 2023, PROC CVPR IEEE, P8867, DOI 10.1109/CVPR52729.2023.00856
   Wu SZ, 2023, PROC CVPR IEEE, P8792, DOI 10.1109/CVPR52729.2023.00849
   Yao CH., 2022, Adv Neural Inf Process Syst, V35, P15296
   Yao CH, 2023, PROC CVPR IEEE, P4853, DOI 10.1109/CVPR52729.2023.00470
   Zhang J., 2021, ADV NEURAL INFORM PR, V34, P29835
   Zuffi S., 2019, ThreeD safari: learning to estimate zebra pose, shape, P5359
   Zuffi S, 2017, PROC CVPR IEEE, P5524, DOI 10.1109/CVPR.2017.586
NR 26
TC 0
Z9 0
U1 1
U2 1
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAY
PY 2024
VL 35
IS 3
AR e2277
DI 10.1002/cav.2277
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SI2B5
UT WOS:001233749200001
DA 2024-08-05
ER

PT J
AU Kim, HY
   Ali, G
   Hwang, JI
AF Kim, Hwang Youn
   Ali, Ghazanfar
   Hwang, Jae-In
TI Enhancing doctor-patient communication in surgical explanations:
   Designing effective facial expressions and gestures for animated
   physician characters
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE computer animation; HCI; healthcare; surgery; virtual agents
ID INFORMATION; EDUCATION; REALITY
AB Paying close attention to facial expressions, gestures, and communication techniques is essential when creating animated physician characters that are realistic and captivating when describing surgical procedures. This paper emphasizes the integration of appropriate emotions, co-speech gestures when medical experts explain the medical procedure, and designing animated characters. We can achieve healthy doctor-patient relationships and improvement of patients' understanding by depicting these components truthfully. We suggest two critical approaches to developing virtual medical experts by incorporating these elements. First, doctors can generate the contents of the surgical procedure with a virtual doctor. Second, patients can listen to the surgical procedure described by the virtual doctor and ask if they have any questions. Our system helps patients by considering their psychology and adding medical professionals' opinions. These improvements ensure the animated virtual agent is comforting, reassuring, and emotionally supportive. Through a user study, we evaluated our hypothesis and gained insight into improvements.
C1 [Kim, Hwang Youn] AI Robot Univ Sci & Technol, Daejeon, South Korea.
   [Ali, Ghazanfar; Hwang, Jae-In] Korea Inst Sci & Technol, Ctr Artificial Intelligence, Seoul, South Korea.
C3 Korea Institute of Science & Technology (KIST)
RP Hwang, JI (corresponding author), Korea Inst Sci & Technol, Ctr Artificial Intelligence, Seoul, South Korea.
EM hji@kist.re.kr
FU Korea Institute of Science and Technology [2E32991]; Korea Institute of
   Science and Technology (KIST) Institutional Program
FX This work was supported by the Korea Institute of Science and Technology
   (KIST) Institutional Program (Project No. 2E32991). Our user study
   received an exemption determination from Korea Institute of Science and
   Technology IRB (KIST-202402-HR-006).
CR Ali G., 2022, Improving Cospeech gesture rulemap generation via wild pose matching with gesture units. SIGGRAPH Asia, DOI [10.1145/3550082, DOI 10.1145/3550082]
   Ali G., 2023, Expanding Multilingual CoSpeech Interaction: The Impact of Enhanced Gesture Units in TexttoGesture Synthesis for Digital Humans, DOI [10.21203/rs.3.rs-3350470/v1, DOI 10.21203/RS.3.RS-3350470/V1]
   Ali G, 2020, COMPUT ANIMAT VIRT W, V31, DOI 10.1002/cav.1944
   [Anonymous], Amazon mechanical turk
   [Anonymous], Clova voice
   [Anonymous], Salsa Lip-Sync
   Bulla C., 2020, Res Appl Web Develop Des, V3, P1
   Celikyilmaz A, 2021, Arxiv, DOI arXiv:2006.14799
   Chheang V., 2024, Towards anatomy education with generative AIbased virtual assistants in immersive virtual reality environments. IEEE
   DIMATTEO MR, 1981, MED CARE, V19, P829, DOI 10.1097/00005650-198108000-00003
   Emmelkamp PMG, 2021, ANNU REV CLIN PSYCHO, V17, P495, DOI 10.1146/annurev-clinpsy-081219-115923
   Faceware Technologies Inc, Faceware Studio
   Gao YF, 2024, Arxiv, DOI [arXiv:2312.10997, 10.48550/arXiv.2312.10997]
   Gerup J, 2020, INT J MED EDUC, V11, DOI 10.5116/ijme.5e01.eb1a
   Holt S, 2023, ACTA ASTRONAUT, V203, P436, DOI 10.1016/j.actaastro.2022.12.016
   Johnsen K, 2005, P IEEE VIRT REAL ANN, P179
   Kurtz S.M., 1997, Teaching and Learning Communication Skills in Medicine, DOI DOI 10.1201/9781315378398
   Larson EB, 2005, JAMA-J AM MED ASSOC, V293, P1100, DOI 10.1001/jama.293.9.1100
   Levinson W, 2010, HEALTH AFFAIR, V29, P1310, DOI 10.1377/hlthaff.2009.0450
   Lewis P., 2020, ADV NEURAL INFORM PR, P9459, DOI DOI 10.48550/ARXIV.2005.11401
   Li YX, 2023, CUREUS J MED SCIENCE, V15, DOI 10.7759/cureus.40895
   Lin C.-Y., 2004, ANN M ASS COMP LING, P74
   Lowe RK, 2003, LEARN INSTR, V13, P157, DOI 10.1016/S0959-4752(02)00018-X
   Menon SS, 2022, CLIN SIMUL NURS, V65, P57, DOI 10.1016/j.ecns.2022.01.007
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Rindfleisch TC, 1997, COMMUN ACM, V40, P92, DOI 10.1145/257874.257896
   Roter D.L., 2006, Doctors talking with patients/Patients talking with doctors
   Ruiz JG, 2009, MED EDUC, V43, P838, DOI 10.1111/j.1365-2923.2009.03429.x
   Sweller J., 2005, The redundancy principle in multimedia learning. Cambridge, DOI [10.1017/CBO9780511816819.011, DOI 10.1017/CBO9780511816819.011]
   Touvron H, 2023, Arxiv, DOI [arXiv:2307.09288, DOI 10.48550/ARXIV.2307.09288]
   Wallraven C, 2014, PSYCHON B REV, V21, P976, DOI 10.3758/s13423-013-0563-4
   Zhang TY, 2020, Arxiv, DOI [arXiv:1904.09675, DOI 10.48550/ARXIV.1904.09675]
   US
NR 33
TC 0
Z9 0
U1 1
U2 1
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAY
PY 2024
VL 35
IS 3
AR e2236
DI 10.1002/cav.2236
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TI2Q2
UT WOS:001240574600001
OA hybrid
DA 2024-08-05
ER

PT J
AU Zhang, JX
   Yang, M
   Li, XM
   Jiang, QO
   Zhang, H
   Meng, WL
AF Zhang, Jiaxiu
   Yang, Meng
   Li, Xiaomin
   Jiang, Qun'ou
   Zhang, Heng
   Meng, Weiliang
TI Two-particle debris flow simulation based on SPH
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE debris flow; GPU acceleration; natural disaster simulation; SPH
ID FLUIDIZED GRANULAR MASSES; ANIMATION; MODEL
AB Debris flow is a highly destructive natural disaster, necessitating accurate simulation and prediction. Existing simulation methods tend to be overly simplified, neglecting the three-dimensional complexity and multiphase fluid interactions, and they also lack comprehensive consideration of soil conditions. We propose a novel two-particle debris flow simulation method based on smoothed particle hydrodynamics (SPH) for enhanced accuracy. Our method employs a sophisticated two-particle model coupling debris flow dynamics with SPH to simulate fluid-solid interaction effectively, which considers various soil factors, dividing terrain into variable and fixed areas, incorporating soil impact factors for realistic simulation. By dynamically updating positions and reconstructing surfaces, and employing GPU and hash lookup acceleration methods, we achieve accurate simulation with significantly efficiency. Experimental results validate the effectiveness of our method across different conditions, making it valuable for debris flow risk assessment in natural disaster management.
C1 [Zhang, Jiaxiu; Yang, Meng; Li, Xiaomin; Zhang, Heng] Beijing Forestry Univ, Sch Informat Sci & Technol, Beijing 100083, Peoples R China.
   [Yang, Meng] Natl Forestry & Grassland Adm, Engn Res Ctr Forestry Oriented Intelligent Informa, Beijing, Peoples R China.
   [Jiang, Qun'ou] Beijing Forestry Univ, Sch Soil & Water Conservat, Beijing, Peoples R China.
   [Meng, Weiliang] Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing, Peoples R China.
   [Meng, Weiliang] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.
C3 Beijing Forestry University; Beijing Forestry University; Chinese
   Academy of Sciences; Institute of Automation, CAS; Chinese Academy of
   Sciences; University of Chinese Academy of Sciences, CAS
RP Yang, M (corresponding author), Beijing Forestry Univ, Sch Informat Sci & Technol, Beijing 100083, Peoples R China.
EM yangmeng@bjfu.edu.cn
OI meng, wei liang/0000-0002-3221-4981; YANG, Meng/0000-0001-6439-2873
FU National Key Research and Development Program of China [2023YFF1304204];
   National Key R&D Program of China [42371291, 32271983, 52175493,
   62172416, 62365014, 62376271, U22B2034]; National Natural Science
   Foundation of China [L231013]; Beijing Natural Science Foundation
   [VRLAB2023B01]; Open Project Program of State Key Laboratory of Virtual
   Reality Technology and Systems, Beihang University
FX This work is supported by National Key R&D Program of China (No.
   2023YFF1304204), National Natural Science Foundation of China (Nos.
   42371291, 32271983, 52175493, 62172416, 62365014, 62376271, and
   U22B2034), Beijing Natural Science Foundation L231013, and the Open
   Project Program of State Key Laboratory of Virtual Reality Technology
   and Systems, Beihang University (No. VRLAB2023B01). The authors thank
   students from Beijing Forestry University, including Yangcheng Xiang,
   and Yi Qian, for their contributions to this paper.
CR Akinci N, 2013, COMPUT ANIMAT VIRT W, V24, P195, DOI 10.1002/cav.1499
   Akinci N, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185558
   Baggio T, 2021, GEOMORPHOLOGY, V381, DOI 10.1016/j.geomorph.2021.107664
   Becker M, 2009, IEEE T VIS COMPUT GR, V15, P493, DOI 10.1109/TVCG.2008.107
   Becker M, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P209
   Blinn James F, 1978, Proceedings of the 5th annual conference on Computer graphics and interactive techniques-SIGGRAPH'78, V12, P286
   CHEN CL, 1988, J HYDRAUL ENG-ASCE, V114, P237, DOI 10.1061/(ASCE)0733-9429(1988)114:3(237)
   Denlinger RP, 2001, J GEOPHYS RES-SOL EA, V106, P553, DOI 10.1029/2000JB900330
   Desbrun M., 1996, Computer Animation and Simulation '96. Proceedings of the Eurographics Workshop, P61
   Foster N., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P181, DOI 10.1145/258734.258838
   Foster N, 2001, COMP GRAPH, P23, DOI 10.1145/383259.383261
   Gao Y, 2022, IEEE INT WORKS MACH, DOI 10.1109/MLSP55214.2022.9943307
   Trujillo-Vela MG, 2020, COMPUT GEOTECH, V125, DOI 10.1016/j.compgeo.2020.103669
   Goldstein H, 1980, Classical mechanics
   Iverson RM, 1997, REV GEOPHYS, V35, P245, DOI 10.1029/97RG00426
   Iverson RM, 2001, J GEOPHYS RES-SOL EA, V106, P537, DOI 10.1029/2000JB900329
   Johnson A.M., 1970, Zeitschrift fur Geomorphol (Annal Geomorphol) Supp, V9, P168
   Klingner BM, 2006, ACM T GRAPHIC, V25, P820, DOI 10.1145/1141911.1141961
   Lee S, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10228189
   Li BL, 2021, ADV CIV ENG, V2021, DOI 10.1155/2021/9098250
   Li Y, 2022, GEOCARTO INT, V37, P5150, DOI 10.1080/10106049.2021.1912194
   Liu C, 2021, LANDSLIDES, V18, P2403, DOI 10.1007/s10346-021-01640-6
   MASTIN GA, 1987, IEEE COMPUT GRAPH, V7, P16, DOI 10.1109/MCG.1987.276961
   Muller M., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P154
   Neto AHF, 2020, ACTA GEOTECH, V15, P2757, DOI 10.1007/s11440-020-00957-1
   Qin SW, 2022, NAT HAZARDS, V114, P2709, DOI 10.1007/s11069-022-05487-5
   REEVES WT, 1983, ACM T GRAPHIC, V2, P91, DOI [10.1145/357318.357320, 10.1145/964967.801167]
   SAVAGE SB, 1989, J FLUID MECH, V199, P177, DOI 10.1017/S0022112089000340
   Serway RA., 2014, Physics for scientists and engineers with modern physics, V9th ed.
   Shao X, 2015, COMPUT GRAPH FORUM, V34, P191, DOI 10.1111/cgf.12467
   Solenthaler B, 2007, COMPUT ANIMAT VIRT W, V18, P69, DOI 10.1002/cav.162
   Stam J, 1999, COMP GRAPH, P121, DOI 10.1145/311535.311548
   Stam J., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P129, DOI 10.1145/218380.218430
   TAKAHASHI T, 1980, J HYDR ENG DIV-ASCE, V106, P381
   Takahashi T., 2002, INT C COMPUTER GRAPH, P266
   Xiong H, 2024, ACTA GEOTECH, V19, P1019, DOI 10.1007/s11440-023-02106-w
   Yokoya N, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2020.3035469
   Zeng QY, 2022, OPEN GEOSCI, V14, P1020, DOI 10.1515/geo-2022-0407
   Zheng HC, 2018, ENG GEOL, V245, P309, DOI 10.1016/j.enggeo.2018.09.003
NR 39
TC 0
Z9 0
U1 2
U2 2
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAY
PY 2024
VL 35
IS 3
AR e2261
DI 10.1002/cav.2261
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SB1Q7
UT WOS:001231912900001
DA 2024-08-05
ER

PT J
AU Zhou, PC
   Li, C
   Zhang, J
   Wang, CB
   Qin, H
   Liu, L
AF Zhou, Peichi
   Li, Chen
   Zhang, Jian
   Wang, Changbo
   Qin, Hong
   Liu, Long
TI A novel transformer-based graph generation model for vectorized road
   design
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE generative model; landscape modeling; non-urban road network
AB Road network design, as an important part of landscape modeling, shows a great significance in automatic driving, video game development, and disaster simulation. To date, this task remains labor-intensive, tedious and time-consuming. Many improved techniques have been proposed during the last two decades. Nevertheless, most of the state-of-the-art methods still encounter problems of intuitiveness, usefulness and/or interactivity. As a rapid deviation from the conventional road design, this paper advocates an improved road modeling framework for automatic and interactive road production driven by geographical maps (including elevation, water, vegetation maps). Our method integrates the capability of flexible image generation models with powerful transformer architecture to afford a vectorized road network. We firstly construct a dataset that includes road graphs, density map and their corresponding geographical maps. Secondly, we develop a density map generation network based on image translation model with an attention mechanism to predict a road density map. The usage of density map facilitates faster convergence and better performance, which also serves as the input for road graph generation. Thirdly, we employ the transformer architecture to evolve density maps to road graphs. Our comprehensive experimental results have verified the efficiency, robustness and applicability of our newly-proposed framework for road design.
   The input to our framework is geographical maps. The density map generation network can generate density maps based on these inputs, and finally the graph generation network can generate road graphs based on the generated density maps. image
C1 [Zhou, Peichi; Zhang, Jian] East China Normal Univ, Sch Software Engn, Shanghai, Peoples R China.
   [Li, Chen; Wang, Changbo; Liu, Long] East China Normal Univ, Sch Comp Sci & Technol, 3663 North Zhongshan Rd, Shanghai, Peoples R China.
   [Qin, Hong] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY USA.
C3 East China Normal University; East China Normal University; State
   University of New York (SUNY) System; State University of New York
   (SUNY) Stony Brook
RP Li, C; Wang, CB (corresponding author), East China Normal Univ, Sch Comp Sci & Technol, 3663 North Zhongshan Rd, Shanghai, Peoples R China.
EM cli@cs.ecnu.edu.cn; cbwang@cs.ecnu.edu.cn
FU National Natural Science Foundation of China [62002121, 62072183];
   Natural Science Foundation of China [IIS-1715985, IIS-1812606]; National
   Science Foundation of USA [22511104600]; Science and Technology
   Commission of Shanghai Municipality
FX This article is partially supported by Natural Science Foundation of
   China under grants 62002121 and 62072183, National Science Foundation of
   USA (IIS-1715985 and IIS-1812606), Science and Technology Commission of
   Shanghai Municipality (No. 22511104600). The authors would like to
   express sincere gratitude to these institutions for their support, as
   well as to all the reviewers for their thoughtful and valuable
   suggestions.
CR Belli D, 2019, Arxiv, DOI arXiv:1910.14388
   Benes J, 2014, COMPUT GRAPH FORUM, V33, P132, DOI 10.1111/cgf.12283
   Chu H., 2019, Neural turtle graphics for modeling city road layouts. Proceedings of the IEEE/CVF international conference on computer vision, P4522
   Chung J., 2014, ARXIV, DOI DOI 10.48550/ARXIV.1412.3555
   Emilien A, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766975
   Emilien A, 2012, VISUAL COMPUT, V28, P809, DOI 10.1007/s00371-012-0699-7
   Fang Z, 2020, Arxiv, DOI [arXiv:2010.04365, 10.48550/arXiv.2010.04365, 10.48550/ARXIV.2010.04365]
   Galin E, 2011, COMPUT GRAPH FORUM, V30, P2021, DOI 10.1111/j.1467-8659.2011.02055.x
   Goodfellow I. J., 2014, ADV NEURAL INFORM PR
   Hartmann S, 2017, COMPUT SCI RES NOTES, V2702, P133
   Huang XW, 2018, PROCEEDINGS OF 2018 IEEE INTERNATIONAL CONFERENCE ON INTEGRATED CIRCUITS, TECHNOLOGIES AND APPLICATIONS (ICTA 2018), P172, DOI 10.1109/CICTA.2018.8706048
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Kim S, 2020, VISUAL COMPUT, V36, P911, DOI 10.1007/s00371-019-01701-x
   Kingma D. P., 2014, arXiv
   Liu MY, 2017, ADV NEUR IN, V30
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Loshchilov I, 2019, Arxiv, DOI [arXiv:1711.05101, 10.48550/arXiv.1711.05101, DOI 10.48550/ARXIV.1711.05101]
   Mattyus G., 2017, Deeproadmapper: extracting road topology from aerial images. Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), P3438
   Nishida G, 2016, COMPUT GRAPH FORUM, V35, P5, DOI 10.1111/cgf.12728
   Parish YIH, 2001, COMP GRAPH, P301, DOI 10.1145/383259.383292
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Vanegas CA, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366187
   Vanegas CA, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618457
   Vaswani A, 2017, ADV NEUR IN, V30
   Weber B, 2009, COMPUT GRAPH FORUM, V28, P481, DOI 10.1111/j.1467-8659.2009.01387.x
   Yang YL, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508405
   You J., 2018, Graphrnn: generating realistic graphs with deep auto-regressive models. International conference on machine learning PMLR, P5708
   Yu Q., 2012, 3rd Annual Conference of the European Association for Computer Graphics,Eurographics 2012-Short Papers, Cagliari, Sardinia, Italy, May 13-18, P53
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 31
TC 0
Z9 0
U1 2
U2 2
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAY
PY 2024
VL 35
IS 3
AR e2267
DI 10.1002/cav.2267
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RF4H0
UT WOS:001226234300001
DA 2024-08-05
ER

PT J
AU Yu, YF
   Qian, JB
   Wang, C
   Dong, YH
   Liu, BS
AF Yu, Yifeng
   Qian, Jiangbo
   Wang, Chong
   Dong, Yihong
   Liu, Baisong
TI Animation line art colorization based on the optical flow method
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE color transfer; optical flow; sketch colorization
AB Coloring an animation sketch sequence is a challenging task in computer vision since the information contained in line sketches is too sparse, and the colors need to be uniform between continuous frames. Many the existing colorization algorithms can only be applied to one image and can be considered color filling algorithms. Such algorithms only provide a color result that fits within a reasonable range and can not be applied to the coloring of frame sequences. This paper proposes an end-to-end two-stage optical flow colorization network to solve the animation frame sequence colorization problem. The first stage of the network finds the direction of the color pixel flow from the detail change between a given reference frame and the next frame of line artwork and then completes the initial coloring process. The second stage of the network performs color correction and clarifies the output of the first stage. Since our algorithm does not directly colorize the image but finds the path of the color change to colorize it, it ensures a consistent color space for the sequence frames after colorization. We conduct experiments on an animation dataset, and the results show that our algorithm is effective. The code is available at .
   Fully automated line sketch colorization algorithm. Optical flow estimation module extracts the flow between the RGB image and the line sketch. Result can be immediately changed without the network retrained. image
C1 [Yu, Yifeng; Qian, Jiangbo; Wang, Chong; Dong, Yihong; Liu, Baisong] Ningbo Univ, Fac Elect Engn & Comp Sci, Ningbo 315211, Zhejiang, Peoples R China.
C3 Ningbo University
RP Qian, JB (corresponding author), Ningbo Univ, Fac Elect Engn & Comp Sci, Ningbo 315211, Zhejiang, Peoples R China.
EM qianjiangbo@nbu.edu.cn
RI Qian, Jiangbo/AAY-9587-2021
OI Qian, Jiangbo/0000-0003-4245-3246
FU National Natural Science Foundation of China [62271274]; China NSF
   [LZ20F020001]; Zhejiang NSF [2023J114]; Ningbo NSF [2023Z059]; Ningbo ST
   Project; K. C. Wong Magna Fund in Ningbo University
FX This work was supported in part by China NSF grant no. 62271274,
   Zhejiang NSF grant no. LZ20F020001, Ningbo NSF grant no. 2023J114,
   Ningbo S&T Project grant no. 2023Z059, and the programs sponsored by K.
   C. Wong Magna Fund in Ningbo University. The authors wish to thank the
   handling editor and anonymous reviewers for their time and constructive
   suggestions to improve the paper.
CR Bao JC, 2018, IEEE ICC
   Bao WB, 2019, PROC CVPR IEEE, P3698, DOI 10.1109/CVPR.2019.00382
   Casey E., 2021, The animation transformer: visual correspondence via segment matching. Proceedings of the IEEE/CVF international conference on computer vision
   Dosovitskiy A., 2016, Advances in Neural Information Processing Systems, P658
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Goroshin R., 2015, Proc 28th Int Conf Neural Inf Process Syst, V28:1234-1242
   Hensman P., 2017, cgan-based manga colorization using a single training image
   Horiuchi T, 2003, IEEE IMAGE PROC, P457
   Huang Z., 2020, arXiv
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Lee J, 2020, IEEE COMPUT SOC CONF, P585, DOI 10.1109/CVPRW50498.2020.00080
   Levin A, 2004, ACM T GRAPHIC, V23, P689, DOI 10.1145/1015706.1015780
   Li HL, 2020, IEEE IC COMP COM NET, DOI 10.1109/icccn49398.2020.9209733
   Li XY, 2022, IEEE T VIS COMPUT GR, V28, P2938, DOI 10.1109/TVCG.2021.3049419
   Lin XX, 2020, COMPUT ANIMAT VIRT W, V31, DOI 10.1002/cav.1917
   Liu ST, 2018, LECT NOTES COMPUT SC, V11215, P404, DOI 10.1007/978-3-030-01252-6_24
   Liu XT, 2022, COMPUT VIS MEDIA, V8, P135, DOI 10.1007/s41095-021-0228-6
   Loshchilov I., 2018, INT C LEARN REPR
   Lucas BD., 1981, An iterative image registration technique with an application to stereo vision
   Lyu L, 2017, 2017 ASIAN CONFERENCE ON ENERGY, POWER AND TRANSPORTATION ELECTRIFICATION (ACEPT)
   Mathieu M, 2016, Arxiv, DOI arXiv:1511.05440
   Mirza M., 2014, ARXIV
   Nazeri Kamyar, 2018, Articulated Motion and Deformable Objects. 10th International Conference, AMDO 2018. Proceedings: LNCS 10945, P85, DOI 10.1007/978-3-319-94544-6_9
   Qu YG, 2006, ACM T GRAPHIC, V25, P1214, DOI 10.1145/1141911.1142017
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shi M, 2020, Arxiv, DOI arXiv:2003.10685
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Siyao L., 2021, Deep animation video interpolation in the wild. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition
   Skora D., 2009, Lazybrush: flexible painting tool for hand-drawn cartoons. Volume 28. Wiley Online Library
   Su JW., 2020, Instance-aware image colorization
   Sun YH, 2017, IEEE ICC
   Teed Zachary, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P402, DOI 10.1007/978-3-030-58536-5_24
   Thasarathan H., 2019, Automatic temporally coherent video colorization. IEEE
   Vondrick C, 2018, LECT NOTES COMPUT SC, V11217, P402, DOI 10.1007/978-3-030-01261-8_24
   Yatziv L, 2006, IEEE T IMAGE PROCESS, V15, P1120, DOI 10.1109/TIP.2005.864231
   Yoo S, 2019, PROC CVPR IEEE, P11275, DOI 10.1109/CVPR.2019.01154
   Zhang B, 2019, PROC CVPR IEEE, P8044, DOI 10.1109/CVPR.2019.00824
   Zhang JS, 2022, COMPUT ANIMAT VIRT W, V33, DOI 10.1002/cav.2032
   Zhang LM, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275090
NR 42
TC 0
Z9 0
U1 3
U2 3
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD JAN
PY 2024
VL 35
IS 1
AR e2229
DI 10.1002/cav.2229
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IX5S4
UT WOS:001169657700001
DA 2024-08-05
ER

PT J
AU Sun, LB
   Wang, YX
   Qin, WH
AF Sun, Libo
   Wang, Yongxiang
   Qin, Wenhu
TI A language-directed virtual human motion generation approach based on
   musculoskeletal models
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE character animation; deep reinforcement learning; language commands;
   musculoskeletal model
AB The development of the systems capable of synthesizing natural and life-like motions for virtual characters has long been a central focus in computer animation. It needs to generate high-quality motions for characters and provide users with a convenient and flexible interface for guiding character motions. In this work, we propose a language-directed virtual human motion generation approach based on musculoskeletal models to achieve interactive and higher-fidelity virtual human motion, which lays the foundation for the development of language-directed controllers in physics-based character animation. First, we construct a simplified model of musculoskeletal dynamics for the virtual character. Subsequently, we propose a hierarchical control framework consisting of a trajectory tracking layer and a muscle control layer, obtaining the optimal control policy for imitating the reference motions through the training. We design a multi-policy aggregation controller based on large language models, which selects the motion policy with the highest similarity to user text commands from the action-caption data pool, facilitating natural language-based control of virtual character motions. Experimental results demonstrate that the proposed approach not only generates high-quality motions highly resembling reference motions but also enables users to effectively guide virtual characters to perform various motions via natural language instructions.
   We propose a language-directed virtual human motion generation approach based on musculoskeletal models to achieve interactive and higher-fidelity virtual human motion. It takes reference motion data, caption and text prompts as inputs, realizing the natural language motion controller through three components: constructing an action-caption data pool, learning the control policies for imitating the motion, and semantic matching selection. image
C1 [Sun, Libo; Wang, Yongxiang; Qin, Wenhu] Southeast Univ, Sch Instrument Sci & Engn, Nanjing 210096, Peoples R China.
C3 Southeast University - China
RP Sun, LB; Qin, WH (corresponding author), Southeast Univ, Sch Instrument Sci & Engn, Nanjing 210096, Peoples R China.
EM sunlibo@seu.edu.cn; qinwenhu@seu.edu.cn
FU Key R&D Program of Jiangsu Province; Jiangsu Modern Agricultural
   Industry Single Technology Research and Development project
   [CX(23)3120]; Advanced Computing and Intelligent Engineering (National
   Level) Laboratory Fund;  [BE2023010-3]
FX This work was supported by the Key R&D Program of Jiangsu Province under
   grant BE2023010-3, Jiangsu Modern Agricultural Industry Single
   Technology Research and Development project under grant CX(23)3120, and
   Advanced Computing and Intelligent Engineering (National Level)
   Laboratory Fund.
CR Agrawal S, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925893
   Ahuja C, 2019, INT CONF 3D VISION, P719, DOI 10.1109/3DV.2019.00084
   Anand AS., A deep reinforcement learning based approach towards generating human walking behavior with a neuromuscular model. 2019 IEEERAS 19th international conference on humanoid robots (humanoids). Piscataway
   Anderson FC, 2001, J BIOMECH ENG-T ASME, V123, P381, DOI 10.1115/1.1392310
   Brown T., 2020, ADV NEURAL INFORM PR, V33, P1877, DOI DOI 10.48550/ARXIV.2005.14165
   Geijtenbeek T, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508399
   Geyer H, 2010, IEEE T NEUR SYS REH, V18, P263, DOI 10.1109/TNSRE.2010.2047592
   Ghosh A., Synthesis of compositional animations from textual descriptions. Proceedings of the IEEE/CVF international conference on computer vision. Piscataway
   Haarnoja T, 2018, PR MACH LEARN RES, V80
   Hill AV, 1938, PROC R SOC SER B-BIO, V126, P136, DOI 10.1098/rspb.1938.0050
   Holden D, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073663
   Juravsky Jordan, 2022, SA '22 Conference Papers: SIGGRAPH Asia 2022 Conference Papers, DOI 10.1145/3550469.3555391
   Lee S, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322972
   Lee SH., 2006, ACM SIGGRAPH 2006 papers
   Lee Y, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661233
   Liu LB, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201315
   Liu LB, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2893476
   Nichol A, 2022, Arxiv, DOI arXiv:2112.10741
   Park Jong-Min, 2022, arXiv
   Peng XB, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201311
   Peng XB, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073602
   Plappert M, 2018, ROBOT AUTON SYST, V109, P13, DOI 10.1016/j.robot.2018.07.006
   Qin WH, 2022, COMPUT ANIMAT VIRT W, V33, DOI 10.1002/cav.2092
   Radford Alec, LEARNING TRANSFERABL
   Ramesh A., 2022, Hierarchical text-conditional image generation with clip latents, DOI 10.48550/arXiv.2204.06125
   Schulman J., 2017, ARXIV
   Sifakis E, 2005, ACM T GRAPHIC, V24, P417, DOI 10.1145/1073204.1073208
   Starke S, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356505
   Sueda S., 2008, ACM SIGGRAPH 2008 papers, P18
   Sun LB, 2024, COMPUT ANIMAT VIRT W, V35, DOI 10.1002/cav.2209
   Tan F., Text2scene: generating compositional scenes from textual descriptions. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. Piscataway
   Tassa Y., Synthesis and stabilization of complex behaviors through online trajectory optimization. 2012 IEEE/RSJ international conference on intelligent robots and systems. Piscataway
   Tevet G, 2022, LECT NOTES COMPUT SC, V13682, P358, DOI 10.1007/978-3-031-20047-2_21
   Touvron H, 2023, Arxiv, DOI [arXiv:2307.09288, DOI 10.48550/ARXIV.2307.09288]
   Wang Y, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3550454.3555490
   Xie Z., 2022, ACM SIGGRAPH 2022 C, P19
   Yin ZQ, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459817
NR 37
TC 0
Z9 0
U1 0
U2 0
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAY
PY 2024
VL 35
IS 3
AR e2257
DI 10.1002/cav.2257
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TA1E3
UT WOS:001238434800001
DA 2024-08-05
ER

PT J
AU Tang, KC
   Zhang, JW
   Shen, YJ
   Li, C
   He, GQ
AF Tang, Kecheng
   Zhang, Jiawen
   Shen, Yuji
   Li, Chen
   He, Gaoqi
TI KDPM: Knowledge-driven dynamic perception model for evacuation scene
   simulation
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE crowd simulation; dynamic perception model; knowledge-driven evacuation;
   stress response
ID CROWD SIMULATION
AB Evacuation scene simulation has become one important approach for public safety decision-making. Although existing research has considered various factors, including social forces, panic emotions, and so forth, there is a lack of consideration of how complex environmental factors affect human psychology and behavior. The main idea of this paper is to model complex evacuation environmental factors from the perspective of knowledge and explore pedestrians' emergency response mechanisms to this knowledge. Thus, a knowledge-driven dynamic perception model (KDPM) for evacuation scene simulation is proposed in this paper. This model combines three modules: knowledge dissemination, dynamic scene perception, and stress response. Both scenario knowledge and hazard source knowledge are extracted and expressed. The improved intelligent agent perception model is designed by adopting position determination. Moreover, a general adaptation syndrome (GAS) model is first presented by introducing a modified stress system model. Experimental results show that the proposed model is closer to the reality of real data sets.
C1 [Tang, Kecheng; Zhang, Jiawen; Shen, Yuji; Li, Chen; He, Gaoqi] East China Normal Univ, Sch Comp Sci & Technol, Shanghai, Peoples R China.
C3 East China Normal University
RP Li, C; He, GQ (corresponding author), East China Normal Univ, Sch Comp Sci & Technol, Shanghai, Peoples R China.
EM cli@cs.ecnu.edu.cn; gqhe@cs.ecnu.edu.cn
OI Tang, Kecheng/0000-0002-7668-9882
FU Natural Science Foundation of Chongqing, China; Fundamental Research
   Funds for the Central Universities; National Natural Science Foundation
   of China [62002121, 62072183]; Shanghai Science and Technology
   Commission [21511100700, 22511104600];  [CSTB2022NSCQ-MSX0552]
FX This work was supported in part by Natural Science Foundation of
   Chongqing, China (No. CSTB2022NSCQ-MSX0552), Fundamental Research Funds
   for the Central Universities, National Natural Science Foundation of
   China (Nos. 62002121 and 62072183), Shanghai Science and Technology
   Commission (Nos. 21511100700 and 22511104600).
CR Cao RF, 2021, SAFETY SCI, V138, DOI 10.1016/j.ssci.2021.105232
   Costa P.T., 1992, Revised NEO Personality Inventory (NEO-PI-R) and NEO Five-Factor Inventory (NEO-FFI) manual
   Durupinar F, 2016, IEEE T VIS COMPUT GR, V22, P2145, DOI 10.1109/TVCG.2015.2501801
   Durupinar F, 2011, IEEE COMPUT GRAPH, V31, P22, DOI 10.1109/MCG.2009.105
   Elsayed P, 2023, J BUILD ENG, V70, DOI 10.1016/j.jobe.2023.106409
   Graham EK, 2012, J GERONTOL B-PSYCHOL, V67, P545, DOI 10.1093/geronb/gbr149
   Gu ZY, 2016, INT J DISAST RISK RE, V18, P1, DOI 10.1016/j.ijdrr.2016.05.008
   Jiang Q., 2014, Stress (stress) system modellingtheory and practice
   Jolliffe D, 2006, J ADOLESCENCE, V29, P589, DOI 10.1016/j.adolescence.2005.08.010
   Kim S., Interactive simulation of dynamic crowd behaviors using general adaptation syndrome theory
   Kobes M, 2010, FIRE SAFETY J, V45, P1, DOI 10.1016/j.firesaf.2009.08.005
   Koh WL., 2008, Modelling and simulation of pedestrian behaviours. 2008 22nd workshop on principles of advanced and distributed simulation
   Li CC, 2022, IEEE T AFFECT COMPUT, V13, P729, DOI 10.1109/TAFFC.2019.2954394
   Liu H, 2018, INFORM SCIENCES, V436, P247, DOI 10.1016/j.ins.2018.01.023
   Liu TT, 2019, SIMUL-T SOC MOD SIM, V95, P65, DOI 10.1177/0037549717753294
   Liu Z, 2018, COMPUT ANIMAT VIRT W, V29, DOI 10.1002/cav.1817
   McEwen BS, 2000, BRAIN RES, V886, P172, DOI 10.1016/S0006-8993(00)02950-4
   MILGRAM S, 1963, J ABNORM PSYCHOL, V67, P371, DOI 10.1037/h0040525
   Ni H., Research on key Technologies of Autonomous Virtual Human
   Ochsner KN, 2005, TRENDS COGN SCI, V9, P242, DOI 10.1016/j.tics.2005.03.010
   Sai T., 2023, J Wuhan Univ Technol (Inf Manag Eng Ed), V45, P336
   Selye H., STRESS LIFE
   Shen Y., 2023, Kdem: a knowledgedriven exploration model for indoor crowd evacuation simulation. Computer graphics international conference
   Sung M., 2005, Fast and accurate goaldirected motion synthesis for crowds. Symposium on computer animation
   Tian ZN, 2020, KNOWL-BASED SYST, V208, DOI 10.1016/j.knosys.2020.106451
   Wiggins J. S., 1996, The fivefactor model of personality: Theoretical perspectives
   [谢秉磊 Xie Binglei], 2024, [中国安全生产科学技术, Journal of Safety Science and Technology], V20, P20
   Xu ML, 2014, J COMPUT SCI TECH-CH, V29, P799, DOI 10.1007/s11390-014-1469-y
   Xu ML, 2021, IEEE T SYST MAN CY-S, V51, P1567, DOI 10.1109/TSMC.2019.2899047
   [杨宏宇 YANG HongYu], 2005, [中国心理卫生杂志, Chinese Mental Health Journal], V19, P762
   Zhang GJ, 2018, IEEE ACCESS, V6, P72581, DOI 10.1109/ACCESS.2018.2882435
   Zhang GJ, 2020, IEEE T AFFECT COMPUT, V11, P708, DOI 10.1109/TAFFC.2018.2836462
   [张淑敏 Zhang Shumin], 2012, [心理科学进展, Advances in Psychological Science], V20, P2061
   Zhou M, 2022, IEEE T INTELL TRANSP, V23, P1492, DOI 10.1109/TITS.2020.3027542
NR 34
TC 0
Z9 0
U1 0
U2 0
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAY
PY 2024
VL 35
IS 3
AR e2279
DI 10.1002/cav.2279
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SN0V1
UT WOS:001235022500001
DA 2024-08-05
ER

PT J
AU Lin, XQ
   Zhang, Y
   Wang, S
   Piao, XL
   Yin, BC
AF Lin, Xuanqi
   Zhang, Yong
   Wang, Shun
   Piao, Xinglin
   Yin, Baocai
TI Multiagent trajectory prediction with global-local scene-enhanced social
   interaction graph network
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE multiagent systems; scene-aware information integration; trajectory
   prediction
AB Trajectory prediction is essential for intelligent autonomous systems like autonomous driving, behavior analysis, and service robotics. Deep learning has emerged as the predominant technique due to its superior modeling capability for trajectory data. However, deep learning-based models face challenges in effectively utilizing scene information and accurately modeling agent interactions, largely due to the complexity and uncertainty of real-world scenarios. To mitigate these challenges, this study presents a novel multiagent trajectory prediction model, termed the global-local scene-enhanced social interaction graph network (GLSESIGN), which incorporates two pivotal strategies: global-local scene information utilization and a social adaptive attention graph network. The model hierarchically learns scene information relevant to multiple intelligent agents, thereby enhancing the understanding of complex scenes. Additionally, it adaptively captures social interactions, improving adaptability to diverse interaction patterns through sparse graph structures. This model not only improves the understanding of complex scenes but also accurately predicts future trajectories of multiple intelligent agents by flexibly modeling intricate interactions. Experimental validation on public datasets substantiates the efficacy of the proposed model. This research offers a novel model to address the complexity and uncertainty in multiagent trajectory prediction, providing more accurate predictive support in practical application scenarios.
   Comparison of scene information learning approaches. image
C1 [Lin, Xuanqi; Zhang, Yong; Wang, Shun; Piao, Xinglin; Yin, Baocai] Beijing Univ Technol, Beijing Artificial Intelligence Inst, Fac Informat Technol, Beijing Key Lab Multimedia & Intelligent Software, Beijing 100124, Peoples R China.
C3 Beijing University of Technology
RP Zhang, Y (corresponding author), Beijing Univ Technol, Beijing Artificial Intelligence Inst, Fac Informat Technol, Beijing Key Lab Multimedia & Intelligent Software, Beijing 100124, Peoples R China.
EM zhangyong2010@bjut.edu.cn
FU National Natural Science Foundation of China; Natural Science Foundation
   of Beijing [4172003]; China Scholarship Council [201806540008]; 
   [62072015];  [U19B2039];  [61632006];  [61876012];  [61902053]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62072015, U19B2039, 61632006, 61876012,
   and 61902053; in part by the Natural Science Foundation of Beijing under
   Grant 4172003; and in part by the China Scholarship Council under Grant
   201806540008.
CR Alahi A, 2016, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2016.110
   Chai YN, 2019, Arxiv, DOI arXiv:1910.05449
   Cheng J, 2023, IEEE I CONF COMP VIS, P8645, DOI 10.1109/ICCV51070.2023.00797
   Cui HG, 2019, IEEE INT CONF ROBOT, P2090, DOI [10.1109/ICRA.2019.8793868, 10.1109/icra.2019.8793868]
   Cunjun Yu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P507, DOI 10.1007/978-3-030-58610-2_30
   Gupta A, 2018, PROC CVPR IEEE, P2255, DOI 10.1109/CVPR.2018.00240
   Hong J, 2019, PROC CVPR IEEE, P8446, DOI 10.1109/CVPR.2019.00865
   Hu Y, 2020, PROC CVPR IEEE, P6318, DOI 10.1109/CVPR42600.2020.00635
   Huang YF, 2019, IEEE I CONF COMP VIS, P6281, DOI 10.1109/ICCV.2019.00637
   Huang ZY, 2023, Arxiv, DOI arXiv:2303.05760
   Jia X., 2022, Conference on Robot Learning, P910
   Junwei Liang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10505, DOI 10.1109/CVPR42600.2020.01052
   Kosaraju Vineet, 2019, Advances in Neural Information Processing Systems
   Li RA, 2023, PATTERN RECOGN LETT, V169, P17, DOI 10.1016/j.patrec.2023.03.006
   Li SJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1920, DOI 10.1109/ICCV48922.2021.00195
   Lisotto M, 2019, IEEE INT CONF COMP V, P2567, DOI 10.1109/ICCVW.2019.00314
   Luo YF, 2022, IEEE ROBOT AUTOM LET, V7, P3499, DOI 10.1109/LRA.2022.3144501
   Lv P, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3250485
   Mangalam K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15213, DOI 10.1109/ICCV48922.2021.01495
   Mohamed Abduallah, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14412, DOI 10.1109/CVPR42600.2020.01443
   Pellegrini S, 2009, IEEE I CONF COMP VIS, P261, DOI 10.1109/ICCV.2009.5459260
   Sadeghian A, 2019, PROC CVPR IEEE, P1349, DOI 10.1109/CVPR.2019.00144
   Saleh F, 2020, Arxiv, DOI arXiv:2004.07482
   Salzmann Tim, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P683, DOI 10.1007/978-3-030-58523-5_40
   Shi LS, 2021, PROC CVPR IEEE, P8990, DOI 10.1109/CVPR46437.2021.00888
   Sun JH, 2020, PROC CVPR IEEE, P657, DOI 10.1109/CVPR42600.2020.00074
   Trautman P, 2010, IEEE INT C INT ROBOT, DOI 10.1109/IROS.2010.5654369
   Vishnu C, 2023, IEEE ROBOT AUTOM LET, V8, P2708, DOI 10.1109/LRA.2023.3258685
   Xu CX, 2022, PROC CVPR IEEE, P6488, DOI 10.1109/CVPR52688.2022.00639
   Xue H, 2018, IEEE WINT CONF APPL, P1186, DOI 10.1109/WACV.2018.00135
   Yu Fan Chen, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P285, DOI 10.1109/ICRA.2017.7989037
   Zhang LD, 2019, Arxiv, DOI arXiv:1907.10233
NR 32
TC 0
Z9 0
U1 0
U2 0
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAY
PY 2024
VL 35
IS 3
AR e2237
DI 10.1002/cav.2237
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SG5W4
UT WOS:001233326700001
DA 2024-08-05
ER

PT J
AU Xiao, ZY
   Yu, F
   Liu, L
   Peng, T
   Hu, XR
   Jiang, MH
AF Xiao, Zhiyong
   Yu, Feng
   Liu, Li
   Peng, Tao
   Hu, Xinrong
   Jiang, Minghua
TI DSANet: A lightweight hybrid network for human action recognition in
   virtual sports
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE activity recognition; attention mechanism; lightweight network;
   multiscale feature; virtual sports
AB Human activity recognition (HAR) has significant potential in virtual sports applications. However, current HAR networks often prioritize high accuracy at the expense of practical application requirements, resulting in networks with large parameter counts and computational complexity. This can pose challenges for real-time and efficient recognition. This paper proposes a hybrid lightweight DSANet network designed to address the challenges of real-time performance and algorithmic complexity. The network utilizes a multi-scale depthwise separable convolutional (Multi-scale DWCNN) module to extract spatial information and a multi-layer Gated Recurrent Unit (Multi-layer GRU) module for temporal feature extraction. It also incorporates an improved channel-space attention module called RCSFA to enhance feature extraction capability. By leveraging channel, spatial, and temporal information, the network achieves a low number of parameters with high accuracy. Experimental evaluations on UCIHAR, WISDM, and PAMAP2 datasets demonstrate that the network not only reduces parameter counts but also achieves accuracy rates of 97.55%, 98.99%, and 98.67%, respectively, compared to state-of-the-art networks. This research provides valuable insights for the virtual sports field and presents a novel network for real-time activity recognition deployment in embedded devices.
   A hybrid lightweight network called DSANet, designed to address the challenges of real-time performance and algorithmic complexity in virtual sports. DSANet incorporates a multi-scale depthwise separable convolutional (Multi-scale DWCNN) module for spatial feature extraction and a multi-layer Gated Recurrent Unit (Multi-layer GRU) module for temporal feature extraction. Additionally, it incorporates an improved channel-space attention module named RCSFA to enhance feature extraction capability. The network achieves high accuracy with a low number of parameters and outperforms state-of-the-art networks on multiple datasets, as demonstrated by experimental evaluations. This paper provides valuable insights for the virtual sports field and presents a novel network suitable for real-time activity recognition deployment on embedded devices. image
C1 [Xiao, Zhiyong; Yu, Feng; Liu, Li; Peng, Tao; Hu, Xinrong; Jiang, Minghua] Wuhan Text Univ, Sch Comp Sci & Artificial Intelligence, Wuhan, Peoples R China.
   [Yu, Feng] Nanyang Technol Univ, Sch Elect & Elect Engn, Nanyang, Singapore.
   [Yu, Feng; Peng, Tao; Hu, Xinrong; Jiang, Minghua] Engn Res Ctr Hubei Prov Clothing Informat, Wuhan, Peoples R China.
C3 Wuhan Textile University; Nanyang Technological University
RP Yu, F (corresponding author), Wuhan Text Univ, Sch Comp Sci & Artificial Intelligence, Wuhan, Peoples R China.
EM yufeng@wtu.edu.cn
RI Yu, Feng/JTD-1798-2023
OI Yu, Feng/0000-0001-8252-5131
FU China Scholarship Council; National Natural Science Foundation of China
   [62202346]; Wuhan Applied Basic Frontier Research Project
   [2022013988065212]; Hubei Science and Technology Project of Safe
   Production Special Fund [SJZX20220908];  [202208420109]
FX This work was supported by National Natural Science Foundation of China
   (No. 62202346), China Scholarship Council (No. 202208420109), Wuhan
   Applied Basic Frontier Research Project (No. 2022013988065212), and
   Hubei Science and Technology Project of Safe Production Special Fund
   (No. SJZX20220908).
CR Al-qaness MAA, 2023, IEEE T IND INFORM, V19, P144, DOI 10.1109/TII.2022.3165875
   Bhattacharya D, 2022, BIOSENSORS-BASEL, V12, DOI 10.3390/bios12060393
   Challa SK, 2022, VISUAL COMPUT, V38, P4095, DOI 10.1007/s00371-021-02283-3
   Cheng X, 2022, IEEE SENS J, V22, P5889, DOI 10.1109/JSEN.2022.3149337
   Dahou A, 2022, MEASUREMENT, V199, DOI 10.1016/j.measurement.2022.111445
   Ding WP, 2023, INFORM SCIENCES, V646, DOI 10.1016/j.ins.2023.119394
   Du CH, 2023, IEEE T MULTIMEDIA, V25, P777, DOI 10.1109/TMM.2022.3152367
   Fu E, 2022, NEUROCOMPUTING, V501, P162, DOI 10.1016/j.neucom.2022.06.014
   Gao WB, 2021, APPL SOFT COMPUT, V111, DOI 10.1016/j.asoc.2021.107728
   Han CL, 2022, EXPERT SYST APPL, V198, DOI 10.1016/j.eswa.2022.116764
   Huang WB, 2023, IEEE T MOBILE COMPUT, V22, P5064, DOI 10.1109/TMC.2022.3174816
   Huang WB, 2021, IEEE J BIOMED HEALTH, V25, P3834, DOI 10.1109/JBHI.2021.3092396
   Huang WB, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3091990
   Ige AO, 2023, APPL SOFT COMPUT, V149, DOI 10.1016/j.asoc.2023.110954
   Khatun MA, 2022, IEEE J TRANSL ENG HE, V10, DOI 10.1109/JTEHM.2022.3177710
   Liang JJ, 2024, IEEE T MOBILE COMPUT, V23, P3259, DOI 10.1109/TMC.2023.3271306
   Liu GX, 2022, IEEE T INTELL TRANSP, V23, P12027, DOI 10.1109/TITS.2021.3109481
   Lu LM, 2022, IEEE ACCESS, V10, P66797, DOI 10.1109/ACCESS.2022.3185112
   Luo F, 2023, IEEE T MOBILE COMPUT, V22, P1356, DOI 10.1109/TMC.2021.3109940
   Meena T, 2023, IEEE SENS J, V23, P21544, DOI 10.1109/JSEN.2023.3301187
   Mim TR, 2023, EXPERT SYST APPL, V216, DOI 10.1016/j.eswa.2022.119419
   Ronald M, 2021, IEEE ACCESS, V9, P68985, DOI 10.1109/ACCESS.2021.3078184
   Sarkar A, 2023, NEURAL COMPUT APPL, V35, P5165, DOI 10.1007/s00521-022-07911-0
   Sekaran SR, 2023, EXPERT SYST APPL, V227, DOI 10.1016/j.eswa.2023.120132
   Singh PK, 2022, ARCH COMPUT METHOD E, V29, P2309, DOI 10.1007/s11831-021-09681-9
   Tang Y, 2023, IEEE T IND ELECTRON, V70, P2106, DOI 10.1109/TIE.2022.3161812
   Teng Q, 2023, IEEE T INSTRUM MEAS, V72, DOI 10.1109/TIM.2023.3240198
   Wang X., 2021, IEEE Trans Instrum Meas, V71, P1
   Wang XY, 2023, APPL SOFT COMPUT, V139, DOI 10.1016/j.asoc.2023.110214
   Wang Y, 2023, IEEE SENS J, V23, P7188, DOI 10.1109/JSEN.2023.3242603
   Xu SG, 2023, IEEE T KNOWL DATA EN, V35, P12497, DOI 10.1109/TKDE.2023.3277839
   Yi MK, 2023, IEEE T CONSUM ELECTR, V69, P657, DOI 10.1109/TCE.2023.3266506
   Yu F., 2024, IEEE Internet Things J, P1, DOI [10.1109/JIOT.2024.3394244, DOI 10.1109/JIOT.2024.3394244]
   Yu F., 2022, IEEE INTERNET THINGS, V10, P6377
   Yu F., 2023, Vis Comput, P1, DOI [10.1007/s00371023032033, DOI 10.1007/S00371023032033]
   Yu F, 2023, IEEE T CONSUM ELECTR, V69, P1101, DOI 10.1109/TCE.2023.3306206
NR 36
TC 0
Z9 0
U1 2
U2 2
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAY
PY 2024
VL 35
IS 3
AR e2274
DI 10.1002/cav.2274
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RU4G7
UT WOS:001230154700001
DA 2024-08-05
ER

PT J
AU Wang, TY
   Liu, SG
AF Wang, Tianyi
   Liu, Shiguang
TI Multi-scale edge aggregation mesh-graph-network for character secondary
   motion
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE secondary motion; self-supervised learning; skinning animation
AB As an enhancement to skinning-based animations, light-weight secondary motion method for 3D characters are widely demanded in many application scenarios. To address the dependence of data-driven methods on ground truth data, we propose a self-supervised training strategy that is free of ground truth data for the first time in this domain. Specifically, we construct a self-supervised training framework by modeling the implicit integration problem with steps as an optimization problem based on physical energy terms. Furthermore, we introduce a multi-scale edge aggregation mesh-graph block (MSEA-MG Block), which significantly enhances the network performance. This enables our model to make vivid predictions of secondary motion for 3D characters with arbitrary structures. Empirical experiments indicate that our method, without requiring ground truth data for model training, achieves comparable or even superior performance quantitatively and qualitatively compared to state-of-the-art data-driven approaches in the field.
   We adopt a self-supervised training framework based on physical loss functions to predict the secondary motion of 3D characters, compared with previous traditional supervised training methods (left), our method (right) is data-free, and has a comparable speed with physical-based methods in the inference stage (1.6 ms/frame). image
C1 [Wang, Tianyi; Liu, Shiguang] Tianjin Univ, Coll Intelligence & Comp, 135 Yaguan Rd, Tianjin 300350, Peoples R China.
C3 Tianjin University
RP Liu, SG (corresponding author), Tianjin Univ, Coll Intelligence & Comp, 135 Yaguan Rd, Tianjin 300350, Peoples R China.
EM lsg@tju.edu.cn
FU National Natural Science Foundation of China
FX No Statement Available
CR Adobe, 2024, Mixamo
   Bailey SW, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392397
   Bailey SW, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201300
   Belbute-Peres FD, 2020, PR MACH LEARN RES, V119
   Benchekroun O, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592404
   Le BH, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925959
   Burke R., 2019, Proceedings of the Workshop on Recommendation in Multi-stakeholder Environments Co-located with the 13th ACM Conference on Recommender Systems (RecSys 2019), Copenhagen, Denmark, September 20, 2019, V2440
   Cao Y, 2022, Arxiv, DOI arXiv:2210.02573
   Casas D, 2018, P ACM COMPUT GRAPH, V1, DOI 10.1145/3203187
   Dalton D, 2023, COMPUT METHOD APPL M, V417, DOI 10.1016/j.cma.2023.116351
   Deng CY, 2020, Arxiv, DOI arXiv:2006.07818
   Grigorev A, 2023, PROC CVPR IEEE, P16965, DOI 10.1109/CVPR52729.2023.01627
   Haeri A, 2021, Arxiv, DOI arXiv:2111.10206
   Hahn F, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185568
   Hahn Fabian, 2013, Proceedings of the 12th ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA'13, P165, DOI [10.1145/2485895.2485918, DOI 10.1145/2485895.2485918]
   Holden D, 2019, PROCEEDINGS SCA 2019: ACM SIGGRAPH/EUROGRAPHICS SYMPOSIUM ON COMPUTER ANIMATION, DOI 10.1145/3309486.3340245
   Huang ZJ, 2023, PROCEEDINGS OF THE 29TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, KDD 2023, P798, DOI 10.1145/3580305.3599362
   Kumar K., 2023, Proceedings of the SC'23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis, P60, DOI [10.1145/3624062.3626082, DOI 10.1145/3624062.3626082]
   Li ZJ, 2022, COMPUT GRAPH-UK, V103, P201, DOI 10.1016/j.cag.2022.02.004
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Mahmood N, 2019, IEEE I CONF COMP VIS, P5441, DOI 10.1109/ICCV.2019.00554
   Mancinelli C, 2019, COMPUT GRAPH-UK, V80, P37, DOI 10.1016/j.cag.2019.03.005
   McAdams A, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964932
   Pfaff T., 2022, arXiv, DOI DOI 10.48550/ARXIV.2210.00612
   Pfaff T., 2021, 9 INT C LEARN REPR I
   Romero C, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459875
   Sanchez-Gonzalez A., 2020, P 37 INT C MACHINE L, P8459, DOI DOI 10.48550/ARXIV.2002.09405
   Santesteban I, 2022, PROC CVPR IEEE, P8130, DOI 10.1109/CVPR52688.2022.00797
   Santesteban I, 2020, COMPUT GRAPH FORUM, V39, P65, DOI 10.1111/cgf.13912
   Shao YD, 2022, LECT NOTES COMPUT SC, V13679, P549, DOI 10.1007/978-3-031-19800-7_32
   Sin FS, 2013, COMPUT GRAPH FORUM, V32, P36, DOI 10.1111/j.1467-8659.2012.03230.x
   Sorkine O, 2007, Proc. Symposium on Geometry Processing, V4, P109, DOI [DOI 10.1145/1281991.1282006, 10.1145/1073204.1073323]
   Wu YH, 2023, P ACM COMPUT GRAPH, V6, DOI 10.1145/3606930
   Zhang JE, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417819
   Zheng ML, 2021, PROC CVPR IEEE, P5928, DOI 10.1109/CVPR46437.2021.00587
NR 35
TC 0
Z9 0
U1 0
U2 0
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAY
PY 2024
VL 35
IS 3
AR e2241
DI 10.1002/cav.2241
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RF4L8
UT WOS:001226239100001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Wang, HP
   Wang, XK
   Xu, YR
   Zhang, YL
   Yao, C
   Guo, Y
   Ban, XJ
AF Wang, Haoping
   Wang, Xiaokun
   Xu, Yanrui
   Zhang, Yalan
   Yao, Chao
   Guo, Yu
   Ban, Xiaojuan
TI Peridynamic-based modeling of elastoplasticity and fracture dynamics
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE elastoplastic simulation; fracture; peridynamic
ID SIMULATION
AB This paper introduces a particle-based framework for simulating the behavior of elastoplastic materials and the formation of fractures, grounded in Peridynamic theory. Traditional approaches, such as the Finite Element Method (FEM) and Smoothed Particle Hydrodynamics (SPH), to modeling elastic materials have primarily relied on discretization techniques and continuous constitutive model. However, accurately capturing fracture and crack development in elastoplastic materials poses significant challenges for these conventional models. Our approach integrates a Peridynamic-based elastic model with a density constraint, enhancing stability and realism. We adopt the Von Mises yield criterion and a bond stretch criterion to simulate plastic deformation and fracture formation, respectively. The proposed method stabilizes the elastic model through a density-based position constraint, while plasticity is modeled using the Von Mises yield criterion within the bond of particle paris. Fracturing and the generation of fine fragments are facilitated by the fracture criterion and the application of complementarity operations to the inter-particle connections. Our experimental results demonstrate the efficacy of our framework in realistically depicting a wide range of material behaviors, including elasticity, plasticity, and fracturing, across various scenarios.
   An experiment on the elasticity, plasticity and cutting of an elastoplastic dough. The dough is made to drop onto a wooden board, and collide with the rolling pin and the metal blade to exhibit the rendering effects of elasticity, plasticity and fracture within our particle framework. image
C1 [Wang, Haoping; Wang, Xiaokun; Xu, Yanrui; Zhang, Yalan; Ban, Xiaojuan] Univ Sci & Technol Beijing, Sch Intelligence Sci & Technol, Beijing, Peoples R China.
   [Xu, Yanrui] Univ Groningen, Fac Sci & Engn, Groningen, Netherlands.
   [Yao, Chao; Guo, Yu] Univ Sci & Technol Beijing, Sch Comp & Commun Engn, Beijing, Peoples R China.
   [Ban, Xiaojuan] Univ Sci & Technol Beijing, Beijing Adv Innovat Ctr Mat Genome Engn, Sch Intelligence Sci & Technol, Beijing, Peoples R China.
   [Ban, Xiaojuan] Univ Sci & Technol Beijing, Key Lab Intelligent Bion Unmanned Syst, Minist Educ, Beijing, Peoples R China.
   [Ban, Xiaojuan] Liaoning Acad Mat, Inst Mat Intelligent Technol, Shenyang, Peoples R China.
C3 University of Science & Technology Beijing; University of Groningen;
   University of Science & Technology Beijing; University of Science &
   Technology Beijing; University of Science & Technology Beijing; Liaoning
   Academy Materials
RP Wang, XK; Ban, XJ (corresponding author), Univ Sci & Technol Beijing, Sch Intelligence Sci & Technol, Beijing, Peoples R China.
EM banxj@ustb.edu.cn; wangxiaokun@ustb.edu.cn
OI Xu, Yanrui/0000-0002-2154-1178; zhang, ya lan/0000-0002-8736-7125
FU National Natural Science Foundation of China [62332017, U22A2022,
   62376025]; National Key Research and Development Program of China
   [2022ZD0118001]; Basic and Applied Basic Research Foundation of
   Guangdong Province [2021A1515012285, 2022A1515110350, 2023A1515030177]
FX National Natural Science Foundation of China, Grant/Award Numbers:
   62332017, U22A2022, 62376025; National Key Research and Development
   Program of China, Grant/Award Number: 2022ZD0118001; Basic and Applied
   Basic Research Foundation of Guangdong Province, Grant/Award Numbers:
   2021A1515012285, 2022A1515110350, 2023A1515030177
CR Akinci N, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185558
   Bargteil AW, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239467
   Becker M., 2007, Weakly compressible SPH for free surface flows
   Chen W, 2018, COMPUT GRAPH FORUM, V37, P112, DOI 10.1111/cgf.13236
   Fan LX, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3522573
   Gissler C, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392431
   He XW, 2018, IEEE T VIS COMPUT GR, V24, P2589, DOI 10.1109/TVCG.2017.2755646
   Hu YM, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201293
   Islam MRI, 2019, INT J MECH SCI, V157, P498, DOI 10.1016/j.ijmecsci.2019.05.003
   Jiang C, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766996
   Joshuah W., 2020, ACM Trans Graph, V39, P31
   Kee MH, 2023, COMPUT GRAPH FORUM, V42, P225, DOI 10.1111/cgf.14756
   Koschier D, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073666
   Kugelstadt T, 2018, COMPUT GRAPH FORUM, V37, P149, DOI 10.1111/cgf.13520
   Kugelstadt T, 2021, P ACM COMPUT GRAPH, V4, DOI 10.1145/3480142
   Levine JA., 2014, A Peridynamic perspective on springmass fracture. Symposium on computer animation
   Liu TT, 2017, ACM T GRAPHIC, V36, DOI 10.1145/2990496
   Lu Z., 2023, IEEE Trans Vis Comput Graph
   Macklin M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461984
   Markus Becker, 2009, Nph, V9, P27, DOI DOI 10.2312/EG/DL/CONF/EG2009/NPH/027-034
   Mller M., 2004, Point based animation of elastic
   Molino N, 2004, ACM T GRAPHIC, V23, P385, DOI 10.1145/1015706.1015734
   MONAGHAN JJ, 1992, ANNU REV ASTRON ASTR, V30, P543, DOI 10.1146/annurev.aa.30.090192.002551
   Muguercia L, 2014, COMPUT GRAPH-UK, V45, P86, DOI 10.1016/j.cag.2014.08.006
   Nocedal J., 1999, Numerical optimization
   O'Brien JF, 1999, COMP GRAPH, P137, DOI 10.1145/311535.311550
   Panc V., 1975, Theories of elastic plates
   Peer A, 2018, COMPUT GRAPH FORUM, V37, P135, DOI 10.1111/cgf.13317
   Sifakis E., 2012, FEM simulation of 3D deformable solids: a practitioner's guide to theory
   Silling SA, 2008, J ELASTICITY, V93, P13, DOI 10.1007/s10659-008-9163-3
   Silling SA, 2007, J ELASTICITY, V88, P151, DOI 10.1007/s10659-007-9125-1
   Silling SA, 2010, J ELASTICITY, V99, P85, DOI 10.1007/s10659-009-9234-0
   Silling SA, 2005, COMPUT STRUCT, V83, P1526, DOI 10.1016/j.compstruc.2004.11.026
   Silling SA, 2000, J MECH PHYS SOLIDS, V48, P175, DOI 10.1016/S0022-5096(99)00029-0
   Silling SA, 2014, Peridynamic model for fatigue cracking
   Stomakhin A, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461948
   Su HZ, 2022, COMPUT GRAPH FORUM, V41, P325, DOI 10.1111/cgf.14477
   Wang TC, 2023, COMPUT GRAPH-UK, V116, P437, DOI 10.1016/j.cag.2023.09.007
   Wang XK, 2024, COMPUT VIS MEDIA, DOI 10.1007/s41095-023-0368-y
   Wolper J, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322949
NR 40
TC 0
Z9 0
U1 0
U2 0
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD JUL
PY 2024
VL 35
IS 4
AR e2242
DI 10.1002/cav.2242
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YU8A1
UT WOS:001271078600001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Ju, E
   Shim, E
   Kim, KY
   Yoon, S
   Choi, MG
AF Ju, Eunjung
   Shim, Eungjune
   Kim, Kwang-yun
   Yoon, Sungjin
   Choi, Myung Geol
TI Fast constrained optimization for cloth simulation parameters from
   static drapes
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE fabrics; neural networks; optimization; parameter estimation; physical
   simulation
ID ALGORITHM
AB We present a cloth simulation parameter estimation method that integrates the flexibility of global optimization with the speed of neural networks. While global optimization allows for varied designs in objective functions and specifying the range of optimization variables, it requires thousands of objective function evaluations. Each evaluation, which involves a cloth simulation, is computationally demanding and impractical time-wise. On the other hand, neural network learning methods offer quick estimation results but face challenges such as the need for data collection, re-training when input data formats change, and difficulties in setting constraints on variable ranges. Our proposed method addresses these issues by replacing the simulation process, typically necessary for objective function evaluations in global optimization, with a neural network for inference. We demonstrate that, once an estimation model is trained, optimization for various objective functions becomes straightforward. Moreover, we illustrate that it is possible to achieve optimization results that reflect the intentions of expert users through visualization of a wide optimization space and the use of range constraints.
   Our optimization model identifies cloth simulation parameters that produce results similar to the given fabric drape. We accelerate the exploration of the optimization space by substituting the repetitive drape simulation process with neural network inference. The middle figure visualizes the optimization space for one experiment, where we explored 3,000 points in about 20 seconds. Thanks to this rapid optimization, we can quickly obtain various results under different boundary conditions. The right figure displays the optimization results for four different boundary conditions, all with respect to the same target drape. image
C1 [Ju, Eunjung; Shim, Eungjune; Kim, Kwang-yun; Yoon, Sungjin] CLO Virtual Fash Inc, Seoul, South Korea.
   [Choi, Myung Geol] Catholic Univ Korea, Digital Media Dept, Bucheon 14662, South Korea.
C3 Catholic University of Korea
RP Choi, MG (corresponding author), Catholic Univ Korea, Digital Media Dept, Bucheon 14662, South Korea.
EM mgchoi@catholic.ac.kr
CR Alizadeh R, 2020, RES ENG DES, V31, P275, DOI 10.1007/s00163-020-00336-7
   Allen K.R., 2022, arXiv
   Alvanon Inc, ALVANON
   Baraff D., 1998, Large steps in cloth simulation. Proceedings of the 25th annual conference on computer graphics and interactive techniques, ACM Press/AddisonWesley Publishing Co
   Bertiche H, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3550454.3555491
   Bhat K.S., 2003, Estimating cloth simulation parameters from video. Proceedings of the 2003 ACM SIGGRAPH/Eurographics symposium on computer animation, Eurographics Association
   Breen DE., 1994, Predicting the drape of woven cloth using interacting particles. Proceedings of the 21st annual conference on computer graphics and interactive techniques, ACM Press/AddisonWesley Publishing Co
   Carrera-Gallissà E, 2017, J TEXT I, V108, P325, DOI 10.1080/00405000.2016.1166804
   Chicco D, 2021, PEERJ COMPUT SCI, DOI 10.7717/peerj-cs.623
   Choi KJ, 2002, ACM T GRAPHIC, V21, P604, DOI 10.1145/566570.566624
   CLO Virtual Fashion Inc, CLO3D
   Cusick G.E., 1965, J. Text. Inst, V56, pT596, DOI DOI 10.1080/19447026508662319
   De Boos A., 1994, SIROFAST FABRIC ASSU
   Leal-Romo FD, 2020, IEEE T ELECTROMAGN C, V62, P2528, DOI 10.1109/TEMC.2020.2973946
   Feng X., 2022, ACM Trans Graph, V41
   Glombikova V, 2014, TEKST KONFEKSIYON, V24, P279
   Gong SW, 2019, IEEE INT CONF COMP V, P4141, DOI 10.1109/ICCVW.2019.00509
   Habib A, 2019, INFORM SCIENCES, V502, P537, DOI 10.1016/j.ins.2019.06.016
   Halsz M., 2006, Sylvie 3D Drape TesterNew system for measuring fabric drape
   Hussain A, 2020, AUTEX RES J, V20, P155, DOI 10.2478/aut-2019-0011
   Ju E, 2022, COMPUT ANIMAT VIRT W, V33, DOI 10.1002/cav.2058
   Ju E, 2020, IEEE ACCESS, V8, P195113, DOI 10.1109/ACCESS.2020.3033765
   KAWABATA S, 1989, J TEXT I, V80, P19, DOI 10.1080/00405008908659184
   Kenkare N, 2008, J TEXT I, V99, P211, DOI 10.1080/00405000701489222
   Kinga D., 2015, A method for stochastic optimization. International conference on learning representations (ICLR). 5. San Diego, California
   Kuijpers S., 2020, The measurement of fabric properties for virtual simulationa critical review. IEEE Standards Association, Industry Connections Report
   Lewin C., 2021, Swish: neural network cloth simulation on madden NFL 21. ACM SIGGRAPH 2021 talks
   Li YF, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3527660
   Liang J., 2019, Adv Neural Inf Process Syst, V32:771780
   Liu F, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1899404.1899408
   Liu SC, 2019, IEEE I CONF COMP VIS, P7707, DOI 10.1109/ICCV.2019.00780
   Matsudaira M, 2003, TEXT RES J, V73, P250, DOI 10.1177/004051750307300309
   Miguel E, 2012, COMPUT GRAPH FORUM, V31, P519, DOI 10.1111/j.1467-8659.2012.03031.x
   Mongus D, 2012, APPL SOFT COMPUT, V12, P266, DOI 10.1016/j.asoc.2011.08.047
   Oh YJ., 2018, Hierarchical cloth simulation using deep neural networks. Proceedings of computer graphics international 2018
   Ravi Nikhila, 2020, Pytorch3d
   Rodriguez-Pardo C, 2023, Arxiv, DOI arXiv:2304.06704
   SwatchOn Inc, 2022, VMOD 3D Library
   Tao J, 2019, AEROSP SCI TECHNOL, V92, P722, DOI 10.1016/j.ast.2019.07.002
   Verschoor M, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392398
   White DA, 2019, COMPUT METHOD APPL M, V346, P1118, DOI 10.1016/j.cma.2018.09.007
   Xiang Y, 1997, PHYS LETT A, V233, P216, DOI 10.1016/S0375-9601(97)00474-X
   Yang S, 2016, Arxiv, DOI arXiv:1608.01250
   Yang S, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3026479
   Zehnder J., 2021, Adv Neural Inf Process Syst, V34, P10368
   Zhu CY, 1997, ACM T MATH SOFTWARE, V23, P550, DOI 10.1145/279232.279236
NR 46
TC 0
Z9 0
U1 1
U2 1
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAY
PY 2024
VL 35
IS 3
AR e2265
DI 10.1002/cav.2265
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UC6S9
UT WOS:001245908000001
DA 2024-08-05
ER

PT J
AU Liu, Y
   Zhao, S
   Cheng, SW
AF Liu, Yang
   Zhao, Song
   Cheng, Shiwei
TI Augmenting collaborative interaction with shared visualization of eye
   movement and gesture in VR
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE collaborative interaction; eye movement; multimodal interaction; virtual
   reality
AB Virtual Reality (VR)-enabled multi-user collaboration has been gradually applied in academic research and industrial applications, but it still has key problems. First, it is often difficult for users to select or manipulate objects in complex three-dimesnional spaces, which greatly affects their operational efficiency. Second, supporting natural communication cues is crucial for cooperation in VR, especially in collaborative tasks, where ambiguous verbal communication cannot effectively assign partners the task of selecting or manipulating objects. To address the above issues, in this paper, we propose a new interaction method, Eye-Gesture Combination Interaction in VR, to enhance the execution of collaborative tasks by sharing the visualization of eye movement and gesture data among partners. We conducted user experiments and showed that using dots to represent eye gaze and virtual hands to represent gestures can help users complete tasks faster than other visualization methods. Finally, we developed a VR multi-user collaborative assembly system. The results of the user study show that sharing gaze points and gestures among users can significantly improve the productivity of collaborating users. Our work can effectively improve the efficiency of multi-user collaborative systems in VR and provide new design guidelines for collaborative systems in VR.
   We propose a collaboration method for sharing gaze points and gestures in VR, which can effectively increase the collaboration efficiency between VR users. image
C1 [Liu, Yang; Zhao, Song; Cheng, Shiwei] Zhejiang Univ Technol, Sch Comp Sci & Technol, Hangzhou, Zhejiang, Peoples R China.
   [Cheng, Shiwei] Zhejiang Univ Technol, 288 Liuhe Rd, Hangzhou, Zhejiang, Peoples R China.
C3 Zhejiang University of Technology; Zhejiang University of Technology
RP Cheng, SW (corresponding author), Zhejiang Univ Technol, 288 Liuhe Rd, Hangzhou, Zhejiang, Peoples R China.
EM 249401866@qq.com
OI Cheng, Shiwei/0000-0003-4716-4179
FU National Natural Science Foundation of China; Zhejiang Provincial
   Natural Science Foundation of China [LR22F020003]; Zhejiang Provincial
   Key Research and Development Program [2023C01045];  [62172368]; 
   [61772468]
FX We thank all the volunteers who participated in the experiments. This
   research work was supported in part by National Natural Science
   Foundation of China under grant numbers 62172368 and 61772468, Zhejiang
   Provincial Natural Science Foundation of China under grant number
   LR22F020003 and Zhejiang Provincial Key Research and Development Program
   under grant number 2023C01045.
CR Bai HD, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376550
   Bailey R., 2012, Proceedings of the symposium on eye tracking research and applications, P75, DOI [10.1145/2168556.2168568, DOI 10.1145/2168556.2168568]
   Caputo FabioMarco., 2015, P 11 BIANN C IT SIGC, P2
   Cutler L. D., 1997, Proceedings 1997 Symposium on Interactive 3D Graphics, P107, DOI 10.1145/253284.253315
   D'Angelo S, 2017, PROCEEDINGS OF THE 2017 ACM SIGCHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'17), P6245, DOI 10.1145/3025453.3025573
   Ichino J, 2024, ACM T COMPUT-HUM INT, V31, DOI 10.1145/3617368
   Jackson R. L., 2000, CVE 2000. Proceedings of the Third International Conference on Collaborative Virtual Environments, P83, DOI 10.1145/351006.351018
   Jing A, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2022), P250, DOI 10.1109/VR51125.2022.00044
   Lee K., 2019, P 2019 CHI C HUM FAC, P1
   Mendes D, 2016, 22ND ACM CONFERENCE ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2016), P261, DOI 10.1145/2993369.2993396
   Mutasim AK., 2021, ACM symposium on eye tracking research and applications, P1
   Nanna MJ, 1998, PSYCHOL METHODS, V3, P55, DOI 10.1037/1082-989X.3.1.55
   pakov O., 2019, P 11 ACM S EYE TRACK, P1
   Pfeuffer K, 2017, SUI'17: PROCEEDINGS OF THE 2017 SYMPOSIUM ON SPATIAL USER INTERACTION, P99, DOI 10.1145/3131277.3132180
   Schjerlund Jonas, 2021, P 2021 CHI C HUM FAC, DOI [10.1145/3411764.3445759, DOI 10.1145/3411764.3445759]
   Steptoe W, 2008, CSCW: 2008 ACM CONFERENCE ON COMPUTER SUPPORTED COOPERATIVE WORK, CONFERENCE PROCEEDINGS, P197
   Veale JF, 2014, LATERALITY, V19, P164, DOI 10.1080/1357650X.2013.783045
   Wang H, 2020, BRIT J NEUROSURG, V34, P512, DOI 10.1080/02688697.2018.1556781
   Wang P, 2023, VIRTUAL REAL-LONDON, V27, P1409, DOI 10.1007/s10055-023-00748-5
   Wang P, 2021, ROBOT CIM-INT MANUF, V72, DOI 10.1016/j.rcim.2020.102071
   Wang P, 2019, INT J ADV MANUF TECH, V102, P1339, DOI 10.1007/s00170-018-03237-1
   Wang Y, 2022, INT J ADV MANUF TECH, V119, P6413, DOI 10.1007/s00170-022-08747-7
   Yu D., 2021, Proceedings of the 2021 CHI conference on human factors in computing systems, P1
   Zhao H, 2018, IEEE T HUM-MACH SYST, V48, P136, DOI 10.1109/THMS.2018.2791562
   Zhao X, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11125754
NR 25
TC 0
Z9 0
U1 0
U2 0
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAY
PY 2024
VL 35
IS 3
AR e2264
DI 10.1002/cav.2264
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SX3F8
UT WOS:001237701600001
DA 2024-08-05
ER

PT J
AU Hu, ZP
   Tang, JL
   Li, LC
   Hou, J
   Xin, HR
   Yu, X
   Bu, JJ
AF Hu, Zhipeng
   Tang, Jilin
   Li, Lincheng
   Hou, Jie
   Xin, Haoran
   Yu, Xin
   Bu, Jiajun
TI MarkerNet: A divide-and-conquer solution to motion capture solving from
   raw markers
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE deep learning; MoCap solving; motion capture; virtual character
   animation
ID DEEPER
AB Marker-based optical motion capture (MoCap) aims to localize 3D human motions from a sequence of input raw markers. It is widely used to produce physical movements for virtual characters in various games such as the role-playing game, the fighting game, and the action-adventure game. However, the conventional MoCap cleaning and solving process is extremely labor-intensive, time-consuming, and usually the most costly part of game animation production. Thus, there is a high demand for automated algorithms to replace costly manual operations and achieve accurate MoCap cleaning and solving in the game industry. In this article, we design a divide-and-conquer-based MoCap solving network, dubbed MarkerNet, to estimate human skeleton motions from sequential raw markers effectively. In a nutshell, our key idea is to decompose the task of direct solving of global motion from all markers into first modeling sub-motions of local parts from the corresponding marker subsets and then aggregating sub-motions into a global one. In this manner, our model can effectively capture local motion patterns w.r.t. different marker subsets, thus producing more accurate results compared to the existing methods. Extensive experiments on both real and synthetic data verify the effectiveness of the proposed method.
   The overall motion of a human body can be decomposed into several sub-motions of different local parts. Thus, we divide all sequential markers into different subsets and learn different local sub-motions from the corresponding marker subsets within local spatio-temporal ranges.image
C1 [Hu, Zhipeng; Bu, Jiajun] Zhejiang Univ, Hangzhou, Zhejiang, Peoples R China.
   [Tang, Jilin; Li, Lincheng; Hou, Jie; Xin, Haoran] NetEase Fuxi AI Lab, Hangzhou 310052, Zhejiang, Peoples R China.
   [Yu, Xin] Univ Queensland, Brisbane, Australia.
C3 Zhejiang University; University of Queensland
RP Li, LC (corresponding author), NetEase Fuxi AI Lab, Hangzhou 310052, Zhejiang, Peoples R China.
EM lilincheng@corp.netease.com
RI li, lincheng/AAR-3978-2020
OI Tang, Jilin/0000-0001-9478-7489; Yu, Xin/0000-0002-0269-5649
CR Ba JL, 2016, ARXIV
   Baumann J., 2011, VRIPHYS, P111, DOI DOI 10.2312/PE/VRIPHYS/VRIPHYS11/111-118
   BESL PJ, 1992, P SOC PHOTO-OPT INS, V1611, P586, DOI 10.1117/12.57955
   Chai JX, 2005, ACM T GRAPHIC, V24, P686, DOI 10.1145/1073204.1073248
   Chen K, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459932
   CMU, CMU GRAPHICS LAB MOT
   Guo C., 2023, P IEEECVF C COMPUTER, p12 858
   He K., 2015, ICCV, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Hendrycks D., 2016, ABS160608415 CORR
   Holden D., 2015, SIGGRAPH Asia 2015 Technical Briefs, SA'15, P1, DOI [DOI 10.1145/2820903.2820918, 10.1145/2820903.2820918]
   Holden D, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201302
   Holden D, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925975
   Hornung A, 2005, P IEEE VIRT REAL ANN, P75
   Ioffe Sergey, 2015, INT C MACHINE LEARNI, V37, P448, DOI DOI 10.48550/ARXIV.1502.03167
   Jia YQ, 2012, PROC CVPR IEEE, P3370, DOI 10.1109/CVPR.2012.6248076
   Kingma D. P., 2014, arXiv
   Kirk AG, 2005, PROC CVPR IEEE, P782
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li Z, 2022, INT CONF 3D VISION, P1, DOI 10.1109/3DV57658.2022.00013
   Liu GD, 2006, VISUAL COMPUT, V22, P721, DOI 10.1007/s00371-006-0080-9
   Liu ST, 2018, LECT NOTES COMPUT SC, V11215, P404, DOI 10.1007/978-3-030-01252-6_24
   Liu Z, 2022, PROC CVPR IEEE, P3192, DOI 10.1109/CVPR52688.2022.00320
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Luo WJ, 2016, ADV NEUR IN, V29
   Paszke A, 2019, ADV NEUR IN, V32
   Pavlakos G, 2019, PROC CVPR IEEE, P10967, DOI 10.1109/CVPR.2019.01123
   Pavllo D., 2019, COMPUTERS GRAPHICS X, V2, DOI DOI 10.1016/J.CAGX.2019.100011
   Pereira MM, 2019, MED SCI EDUC, V29, P431, DOI 10.1007/s40670-019-00706-4
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Robinette K., 2002, Civilian American and European surface anthropometry resource (CAESAR), Final report
   Romero J., 2022, ARXIV
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Shetty K, 2023, PROC CVPR IEEE, P574, DOI 10.1109/CVPR52729.2023.00063
   Shimada S, 2022, LECT NOTES COMPUT SC, V13682, P516, DOI 10.1007/978-3-031-20047-2_30
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C, 2014, Arxiv, DOI arXiv:1312.6199
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tautges J, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1966394.1966397
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Z, 2016, INT CONF SIGN PROCES, P975, DOI 10.1109/ICSP.2016.7877975
   Wei W-L., 2022, P IEEECVF C COMPUTER, p13 211
   Wu ZF, 2019, PATTERN RECOGN, V90, P119, DOI 10.1016/j.patcog.2019.01.006
   Xiao J, 2015, SIGNAL PROCESS, V110, P108, DOI 10.1016/j.sigpro.2014.08.017
   Zhang H, 2019, IEEE T PATTERN ANAL, V41, P1947, DOI 10.1109/TPAMI.2018.2856256
   Zordan V. B., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P245
NR 46
TC 0
Z9 0
U1 1
U2 1
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD JAN
PY 2024
VL 35
IS 1
DI 10.1002/cav.2228
EA JAN 2024
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JN8A9
UT WOS:001142461100001
DA 2024-08-05
ER

PT J
AU Zhao, WQ
   Zhu, JL
   Huang, J
   Li, P
   Sheng, B
AF Zhao, Wenqing
   Zhu, Jianlin
   Huang, Jin
   Li, Ping
   Sheng, Bin
TI GAN-Based Multi-Decomposition Photo Cartoonization
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE attention mechanism; cartoon images; generative adversarial networks;
   style transfer
AB BackgroundCartoon images play a vital role in film production, scientific and educational animation, video games, and other fields, and are one of the key visual expressions of artistic creation. However, since hand-crafted cartoon images often require a great deal of time and effort on the part of professional artists, it is necessary to be able to automatically transform real-world images into different styles of cartoon images. Although cartoon images vary from artist to artist, cartoon images generally have the unique characteristics of being highly simplified and abstract, with clear edges, smooth color shading, and relatively simple textures. However, existing image cartoonization methods tend to create a number of problems when performing style transfer, which mainly include: (1) the resulting generated images do not have obvious cartoon-style textures; and (2) the generated images are prone to structural confusion, color artifacts, and loss of the original image content. Therefore, it is also a great challenge in the field of image cartoonization to be able to make a good balance between style transfer and content keeping.MethodsIn this paper, we propose a GAN-based multi-attention mechanism for image cartoonization to address the above issues. The method combines the residual block used to extract deep network features in the generator with the attention mechanism, and further strengthens the perceptual ability of the generative model to cartoon images through the adaptive feature correction of the attention module to improve the cartoon features of the generated images. At the same time, we also introduce the attention mechanism in the convolution block of the discriminator, which is used to further reduce the image visual quality problem caused by the style transfer process. By introducing the attention mechanism into the generator and discriminator models of the generative adversarial network, our method enables the generated images to have obvious cartoon-style features while effectively improving the image's visual quality.ResultsA large number of quantitative, qualitative, and ablation experiments are conducted to demonstrate the advantages of our method in the field of image cartoonization and the role of each module in the method.
   The architecture is based on multiple decomposition of GAN model. This network model learns the decomposition of the generated image features, texture representation is used to extract the high frequency texture features of the generated image to make it conform to the unique characteristics of the cartoon image and is guided by the discriminator to further enhance the cartoon texture of the generated image. The structure-content representation makes the generated image have sparse color blocks and clear edges by mimicking the global content of the cartoon image, and maintains the structure and content of the original image with the pre-trained network. image
C1 [Zhao, Wenqing; Huang, Jin] Wuhan Text Univ, Sch Comp Sci & Artificial Intelligence, Wuhan, Peoples R China.
   [Zhu, Jianlin] South Cent Minzu Univ, Dept Comp Sci, Wuhan, Peoples R China.
   [Li, Ping] Hong Kong Polytech Univ, Sch Design, Dept Comp, Hong Kong, Peoples R China.
   [Sheng, Bin] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai, Peoples R China.
C3 Wuhan Textile University; South Central Minzu University; Hong Kong
   Polytechnic University; Shanghai Jiao Tong University
RP Huang, J (corresponding author), Wuhan Text Univ, Sch Comp Sci & Artificial Intelligence, Wuhan, Peoples R China.
EM derick0320@foxmail.com
OI Li, Ping/0000-0002-1503-0240; Sheng, Bin/0000-0001-8678-2784
CR Gatys LA, 2015, Arxiv, DOI arXiv:1505.07376
   Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Aly HA, 2005, IEEE T IMAGE PROCESS, V14, P1647, DOI 10.1109/TIP.2005.851684
   [Anonymous], 2016, Learning to generate images of outdoor scenes from attributes and semantic layouts. arXiv preprint arXiv:1612.00215
   Bruna J., 2015, arXiv
   Chen Y, 2018, PROC CVPR IEEE, P9465, DOI 10.1109/CVPR.2018.00986
   Chen Y, 2017, IEEE IMAGE PROC, P2010, DOI 10.1109/ICIP.2017.8296634
   Chen ZH, 2020, IEEE T CIRC SYST VID, V30, P1410, DOI 10.1109/TCSVT.2019.2902937
   Chen ZH, 2020, IEEE T CYBERNETICS, V50, P2152, DOI 10.1109/TCYB.2018.2875983
   Cho W, 2019, PROC CVPR IEEE, P10631, DOI 10.1109/CVPR.2019.01089
   Dumoulin V., 2016, ARXIV PREPRINT ARXIV
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Gatys L., 2016, CoRR, V16, P326, DOI 10.1167/16.12.326
   Gatys LA, 2015, ADV NEUR IN, V28
   Gatys LA, 2017, PROC CVPR IEEE, P3730, DOI 10.1109/CVPR.2017.397
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Hensel M, 2017, ADV NEUR IN, V30
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jin YX, 2021, IEEE T NEUR NET LEAR, V32, P2330, DOI 10.1109/TNNLS.2020.3004634
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Li C., 2018, Twostage sketch colorization. ACM Transactions on Graphics
   Li C, 2016, PROC CVPR IEEE, P2479, DOI 10.1109/CVPR.2016.272
   Li HX, 2021, IEEE T IMAGE PROCESS, V30, P8526, DOI 10.1109/TIP.2021.3117061
   Liu MY, 2017, ADV NEUR IN, V30
   LUQUE Raul R., 2012, The Cel Shading Technique
   Rosin P., 2012, Image and videobased artistic stylisation, V42
   Saito T., 1990, Computer Graphics, V24, P197, DOI 10.1145/97880.97901
   Sheng B, 2020, IEEE T CIRC SYST VID, V30, P955, DOI 10.1109/TCSVT.2019.2901629
   Sheng B, 2020, IEEE T VIS COMPUT GR, V26, P1332, DOI 10.1109/TVCG.2018.2869326
   Shu YZ, 2022, IEEE T VIS COMPUT GR, V28, P3376, DOI 10.1109/TVCG.2021.3067201
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Wang J, 2004, ACM T GRAPHIC, V23, P574, DOI 10.1145/1015706.1015763
   Wen Y, 2021, IEEE T IMAGE PROCESS, V30, P6142, DOI 10.1109/TIP.2021.3092814
   Winnemöller H, 2006, ACM T GRAPHIC, V25, P1221, DOI 10.1145/1141911.1142018
   Xinrui Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8087, DOI 10.1109/CVPR42600.2020.00811
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Zhang BX, 2020, IEEE T VIS COMPUT GR, V26, P2546, DOI 10.1109/TVCG.2019.2894627
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 44
TC 0
Z9 0
U1 2
U2 2
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAY
PY 2024
VL 35
IS 3
AR e2248
DI 10.1002/cav.2248
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RQ7H0
UT WOS:001229186400001
DA 2024-08-05
ER

PT J
AU Zhu, SQ
   Liang, JZ
   Liu, H
AF Zhu, Shuqi
   Liang, Jiuzhen
   Liu, Hao
TI Face attribute translation with multiple feature perceptual
   reconstruction assisted by style translator
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE face attribute translation; generative adversarial networks;
   image-to-image translation; latent space manipulation
AB Improving the accuracy and disentanglement of attribute translation, and maintaining the consistency of face identity have been hot topics in face attribute translation. Recent approaches employ attention mechanisms to enable attribute translation in facial images. However, due to the lack of accuracy in the extraction of style code, the attention mechanism alone is not precise enough for the translation of attributes. To tackle this, we introduce a style translator module, which partitions the style code into attribute-related and unrelated components, enhancing latent space disentanglement for more accurate attribute manipulation. Additionally, many current methods use per-pixel loss functions to preserve face identity. However, this can sacrifice crucial high-level features and textures in the target image. To address this limitation, we propose a multiple-perceptual reconstruction loss to better maintain image fidelity. Extensive qualitative and quantitative experiments in this article demonstrate significant improvements over state-of-the-art methods, validating the effectiveness of our approach.
   We introduce a style translator module, which partitions the style code into attribute-related and unrelated components, enhancing latent space disentanglement for more accurate attribute manipulation. Additionally, we propose a multiple-perceptual reconstruction loss to better maintain image fidelity. image
C1 [Zhu, Shuqi; Liang, Jiuzhen; Liu, Hao] Changzhou Univ, Sch Comp Sci & Artificial Intelligence, Changzhou, Peoples R China.
C3 Changzhou University
RP Liang, JZ (corresponding author), Changzhou Univ, Sch Comp Sci & Artificial Intelligence, Changzhou, Peoples R China.
EM jzliang@cczu.edu.cn
FU The Basic Science (Natural Science) Research Projects of Universities in
   Jiangsu Province
FX We would like to express our sincere gratitude to our laboratory
   colleagues for their hard work and selfless help, whose support enabled
   us to successfully complete this study. Also, we would like to thank the
   reviewers and editors for their invaluable comments, whose guidance made
   our research work more rigorous. Finally, we would like to express our
   sincere gratitude to the institutions that provided us with the
   financial support that enabled us to carry out this important study.
CR Dalva Y, 2023, IEEE T PATTERN ANAL, V45, P14777, DOI 10.1109/TPAMI.2023.3308102
   Durall Lopez R., FacialGAN: style transfer and attribute manipulation on synthetic faces. Proceeding of the 32nd British Machine Vision Conference; 2021. p. 114
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   He K., 2015, ICCV, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   He ZL, 2019, IEEE T IMAGE PROCESS, V28, P5464, DOI 10.1109/TIP.2019.2916751
   Hensel M, 2017, ADV NEUR IN, V30
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karras T, 2018, Arxiv, DOI [arXiv:1710.10196, DOI 10.48550/ARXIV.1710.10196]
   Li X., 2020, PMLR, P5916
   Li XY, 2021, PROC CVPR IEEE, P8635, DOI 10.1109/CVPR46437.2021.00853
   Liu YF, 2023, IEEE T PATTERN ANAL, V45, P14590, DOI 10.1109/TPAMI.2023.3298868
   Shen Y., 2020, P IEEECVF C COMPUTER, P9243
   Torbunov D, 2023, IEEE WINT CONF APPL, P702, DOI 10.1109/WACV56688.2023.00077
   Tu PQ, 2023, NEURAL PROCESS LETT, V55, P537, DOI 10.1007/s11063-022-10896-5
   Wang J., 2023, IEEE Trans. Multimed, V26, P4375
   Xiao TH, 2018, LECT NOTES COMPUT SC, V11214, P172, DOI 10.1007/978-3-030-01249-6_11
   Xie Shaoan, 2022, Advances in Neural Information Processing Systems, V35, P28545
   Yang GX, 2021, PROC CVPR IEEE, P2950, DOI 10.1109/CVPR46437.2021.00297
   Yao X, 2022, arXiv
   Yunjey Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8185, DOI 10.1109/CVPR42600.2020.00821
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 21
TC 0
Z9 0
U1 0
U2 0
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAY
PY 2024
VL 35
IS 3
AR e2273
DI 10.1002/cav.2273
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL0Y5
UT WOS:001234502100001
DA 2024-08-05
ER

PT J
AU Yang, XD
   Lyu, A
   Xian, CH
   Cai, HM
AF Yang, Xudong
   Lyu, Aoran
   Xian, Chuhua
   Cai, Hongmin
TI Microfacet rendering with diffraction compensation
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE BRDF; diffraction; energy compensation; microfacet
AB The traditional microfacet rendering models usually only consider the straight propagation of light and do not take into account the diffraction effect when calculating the radiance of outgoing light. However, ignoring the energy generated by diffraction can lead to darker rendering results when the object's surface has many small details. To address this issue, we introduce a diffraction energy term in the microfacet model to compensate for the energy loss caused by diffraction. Starting from the Fresnel-Kirchhoff diffraction theorem, we combine it with the Cook-Torrance model. By incorporating the computed diffraction radiance into the outgoing radiance of the microfacet, we obtain a diffraction-compensated BRDF (Bidirectional Reflectance Distribution Function) model. Experimental results demonstrate that our proposed method has a significant effect in compensating for outgoing light and produces more realistic rendering results.
   In this paper, we propose a novel diffraction-compensated Bidirectional Reflectance Distribution Function (BRDF) model, which integrates the computed diffracted radiance into the outgoing radiance of the microfacets. This model enables us to consider diffraction effects and improve the accuracy of the rendered appearance. Experimental results demonstrate that out proposed method has a significant effect in compensating for outgoing light and produces more realistic rendering results. image
C1 [Yang, Xudong; Lyu, Aoran; Xian, Chuhua; Cai, Hongmin] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
C3 South China University of Technology
RP Xian, CH (corresponding author), South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
EM chhxian@scut.edu.cn
FU the National Key Research and Development Program of China
   [2022YFE0112200]; National Key Research and Development Program of China
   [U21A20520, 62325204]; National Natural Science Foundation of China
   [202206030009]; Key Area Research and Development Program of Guangzhou
   City
FX This work was supported in part by the National Key Research and
   Development Program of China (2022YFE0112200), the National Natural
   Science Foundation of China (U21A20520, 62325204), the Key Area Research
   and Development Program of Guangzhou City (202206030009).
CR Ashikhmin M., 2000, Journal of Graphics Tools, V5, P25, DOI 10.1080/10867651.2000.10487522
   Beckmann P., SCATTERING ELECTROMA
   Bitterli B., 2016, RENDERING RESOURCES
   Blinn JF., 1978, Simulation of wrinkled surfaces, DOI [10.1145/800248.507101, DOI 10.1145/800248.507101]
   Blinn JF., 1977, Proceedings of the 4th Annual Conference on Computer Graphics and Interactive Techniques
   Burley B., 2012, ACM Siggraph, V2012, P7
   Chai YF, 2020, COMPUT GRAPH-UK, V86, P71, DOI 10.1016/j.cag.2019.08.017
   Collin C., 2020, Graphics Interface 2014. Montreal, QC, P201
   Cook R. L., 1981, Computer Graphics, V15, P307, DOI 10.1145/965161.806819
   Dhillon DS, 2014, COMPUT GRAPH FORUM, V33, P177, DOI 10.1111/cgf.12425
   Harvey JE., 1977, Light-scattering characteristics of optical surfaces, DOI [10.1117/12.964594, DOI 10.1117/12.964594]
   HE XD, 1991, COMP GRAPH, V25, P175, DOI 10.1145/127719.122738
   Heitz E, 2014, COMPUT GRAPH FORUM, V33, P103, DOI 10.1111/cgf.12417
   Heitz E., 2014, J Comput Graph Tech, P32
   Heitz E, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925895
   Hoffman N., Physically-Based Shading Models in Film and Game Production
   Holzschuch N, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073621
   Hosek L, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185591
   Kelemen C., 2001, Eurographics (short presentations)
   Konig A., 1892, Ostwalds Klassiker der exakten Wissenschaften No. 31-33
   Kulla C., 2017, SIGGRAPH Course, Physically Based Shading, V2.
   Löw J, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2077341.2077350
   Ngan A., 2005, EUROGRAPHICS S RENDE
   Oren M., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P239, DOI 10.1145/192161.192213
   SCHLICK C, 1994, COMPUT GRAPH FORUM, V13, pC233, DOI 10.1111/1467-8659.1330233
   Stam J, 1999, COMP GRAPH, P101, DOI 10.1145/311535.311546
   Toisoul A, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3012001
   TORRANCE KE, 1967, J OPT SOC AM, V57, P1105, DOI 10.1364/JOSA.57.001105
   TROWBRIDGE TS, 1975, J OPT SOC AM, V65, P531, DOI 10.1364/JOSA.65.000531
   van Ginneken B, 1998, APPL OPTICS, V37, P130, DOI 10.1364/AO.37.000130
   Vernold CL, 1998, P SOC PHOTO-OPT INS, V3426, P51, DOI 10.1117/12.328477
   Walter B., 2007, P 18 EUR C REND TECH, P195, DOI [10.2312/EGWR/EGSR07/195-206, DOI 10.2312/EGWR/EGSR07/195-206]
   WARD GJ, 1992, COMP GRAPH, V26, P265, DOI 10.1145/142920.134078
   Werner S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130840
   Wilkie A., 2004, An analytical model for skylight polarisation
   Wilkie A., 2013, Predicting Sky Dome Appearance on Earth-like Extrasolar Worlds, DOI [10.1145/2508244.2508263, DOI 10.1145/2508244.2508263]
   WOLFF LB, 1990, IEEE COMPUT GRAPH, V10, P44, DOI 10.1109/38.62695
   Yan LQ, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201351
NR 38
TC 0
Z9 0
U1 1
U2 1
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAY
PY 2024
VL 35
IS 3
AR e2253
DI 10.1002/cav.2253
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RF8E2
UT WOS:001226335700001
DA 2024-08-05
ER

PT J
AU Ye, JN
   Meng, XX
   Guo, DY
   Shang, C
   Mao, HT
   Yang, XB
AF Ye, Jiannan
   Meng, Xiaoxu
   Guo, Daiyun
   Shang, Cheng
   Mao, Haotian
   Yang, Xubo
TI Neural foveated super-resolution for real-time VR rendering
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE foveated rendering; super-resolution; virtual reality
ID QUALITY ASSESSMENT
AB As virtual reality display technologies advance, resolutions and refresh rates continue to approach human perceptual limits, presenting a challenge for real-time rendering algorithms. Neural super-resolution is promising in reducing the computation cost and boosting the visual experience by scaling up low-resolution renderings. However, the added workload of running neural networks cannot be neglected. In this article, we try to alleviate the burden by exploiting the foveated nature of the human visual system, in a way that we upscale the coarse input in a heterogeneous manner instead of uniform super-resolution according to the visual acuity decreasing rapidly from the focal point to the periphery. With the help of dynamic and geometric information (i.e., pixel-wise motion vectors, depth, and camera transformation) available inherently in the real-time rendering content, we propose a neural accumulator to effectively aggregate the amortizedly rendered low-resolution visual information from frame to frame recurrently. By leveraging a partition-assemble scheme, we use a neural super-resolution module to upsample the low-resolution image tiles to different qualities according to their perceptual importance and reconstruct the final output adaptively. Perceptually high-fidelity foveated high-resolution frames are generated in real-time, surpassing the quality of other foveated super-resolution methods.
   We propose a method to improve neural super-resolution for realtime rendering and alleviate the computation burden in VR by exploiting the foveated nature of the human visual system. By leveraging a partition-assemble scheme, our method adaptively upsamples the low-resolution image tiles to different qualities according to their perceptual importance. image
C1 [Ye, Jiannan; Guo, Daiyun; Shang, Cheng; Mao, Haotian; Yang, Xubo] Shanghai Jiao Tong Univ, Sch Software, Digital ART Lab, Shanghai, Peoples R China.
   [Meng, Xiaoxu] Tencent Games Digital Content Technol Ctr, Tencent, CA USA.
C3 Shanghai Jiao Tong University
RP Yang, XB (corresponding author), Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Shanghai, Peoples R China.
EM yangxubo@sjtu.edu.cn
FU National Key Research and Development Program of China [2018YFB1004902]
FX This work was supported by the National Key Research and Development
   Program of China (2018YFB1004902). We thank all the participants for the
   user study and reviewer for their advice.
CR Akeley K., 1993, Computer Graphics Proceedings, P109, DOI 10.1145/166117.166131
   AMD, 2023, AMD FidelityFX Super Resolution AMD
   Briedis KM, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480553
   Caballero J, 2017, PROC CVPR IEEE, P2848, DOI 10.1109/CVPR.2017.304
   Chan K.C., 2022, P IEEE CVF C COMP VI, P5972
   Deng NC, 2022, IEEE T VIS COMPUT GR, V28, P3854, DOI 10.1109/TVCG.2022.3203102
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Fan HM, 2021, COMPUT GRAPH FORUM, V40, P15, DOI 10.1111/cgf.14338
   Guenter B, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366183
   Guo J, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480531
   Haris M, 2019, PROC CVPR IEEE, P3892, DOI 10.1109/CVPR.2019.00402
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Herzog R., 2010, Proceedings of the 2010 ACM SIGGRAPH symposium on Interactive 3D Graphics and Games, P91
   Intel, 2021, Intel ArcXe super sampling
   Isobe T, 2020, Arxiv, DOI arXiv:2008.05765
   Jie Liu, 2020, Proceedings of the 16th European Conference on Computer Vision - ECCV 2020 Workshops. Lecture Notes in Computer Science (LNCS 12537), P41, DOI 10.1007/978-3-030-67070-2_2
   Jimenez J, 2012, COMPUT GRAPH FORUM, V31, P355, DOI 10.1111/j.1467-8659.2012.03014.x
   Jo Y, 2018, PROC CVPR IEEE, P3224, DOI 10.1109/CVPR.2018.00340
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kaplanyan AS, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356557
   Karis B., 2014, ACM Trans. Graph
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kong FY, 2022, IEEE COMPUT SOC CONF, P765, DOI 10.1109/CVPRW56347.2022.00092
   Krajancich B, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592406
   Lai WS, 2019, IEEE T PATTERN ANAL, V41, P2599, DOI 10.1109/TPAMI.2018.2865304
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li Y., 2022, 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P1061
   Li Y., 2023, P IEEECVF C COMPUTER, P1921
   Li ZC, 2011, IMAGE VISION COMPUT, V29, P1, DOI 10.1016/j.imavis.2010.07.001
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu HY, 2022, ARTIF INTELL REV, V55, P5981, DOI 10.1007/s10462-022-10147-y
   Lottes T., 2009, FXAA
   Mantiuk RK, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459831
   Meng X., 2020, Eurographics Symposium on Rendering DL, P13
   Meng XX, 2018, P ACM COMPUT GRAPH, V1, DOI 10.1145/3203199
   Nam H, 2021, IEEE ACCESS, V9, P140042, DOI 10.1109/ACCESS.2021.3119597
   Nehab D, 2007, GRAPHICS HARDWARE 2007: ACM SIGGRAPH / EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P25
   NVIDIA, 2020, Deep learning super sampling (DLSS) technologyNVIDIA
   Patney A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980246
   Reshetov Alexander, 2009, Proceedings of the Conference on High Performance Graphics, P109
   Soundararajan R, 2013, IEEE T CIRC SYST VID, V23, P684, DOI 10.1109/TCSVT.2012.2214933
   Tao X, 2017, IEEE I CONF COMP VIS, P4482, DOI 10.1109/ICCV.2017.479
   Thomas MM, 2022, P ACM COMPUT GRAPH, V5, DOI 10.1145/3543870
   Thomas MM, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417786
   Tian YP, 2020, PROC CVPR IEEE, P3357, DOI 10.1109/CVPR42600.2020.00342
   Tursun OT, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322985
   UnrealEngine, 2022, Screen Percentage with Temporal Upscale in Unreal Engine Unreal Engine 5.0 Documentation
   Vaidyanathan K., 2014, Proceedings of High Performance Graphics, P9
   Wang LD, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5454, DOI 10.1145/3474085.3475673
   Wang XT, 2019, IEEE COMPUT SOC CONF, P1954, DOI 10.1109/CVPRW.2019.00247
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Weier M, 2016, COMPUT GRAPH FORUM, V35, P289, DOI 10.1111/cgf.13026
   Xiao K, 2018, ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES (I3D 2018), DOI 10.1145/3190834.3190850
   Xiao L, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392376
   Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2
   Yang L, 2008, COMPUT GRAPH FORUM, V27, P1183, DOI 10.1111/j.1467-8659.2008.01256.x
   Yang L, 2020, COMPUT GRAPH FORUM, V39, P607, DOI 10.1111/cgf.14018
   Yang L, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618481
   Yang S., 2023, IEEE Trans Vis Comput Graph
   Yang WM, 2019, IEEE T MULTIMEDIA, V21, P3106, DOI 10.1109/TMM.2019.2919431
   Ye JN, 2022, 2022 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2022), P756, DOI 10.1109/VR51125.2022.00097
   Zhang Y., 2023, P IEEECVF C COMPUTER, P1864
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhong Z., 2023, FuseSR: Super resolution for realtime rendering through efficient multiresolution fusion
   Zhou ZW, 2020, IEEE T MED IMAGING, V39, P1856, DOI 10.1109/TMI.2019.2959609
NR 65
TC 0
Z9 0
U1 0
U2 0
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD JUL
PY 2024
VL 35
IS 4
AR e2287
DI 10.1002/cav.2287
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YM3K5
UT WOS:001268864700001
DA 2024-08-05
ER

PT J
AU Li, B
   Liu, SG
AF Li, Bo
   Liu, Shiguang
TI A multi-species material point method with a mixture model
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE fluid-solid coupling; MPM; multiple fluid
AB The material point method (MPM) has attracted more and more attention in computer graphics. It is very successful in simulating both fluid flow and solid deformation, but may fail in simulating multiple fluids and solids coupling. We propose a unified MPM solver for multi-species simulations. Compared to traditional MPM, we extend the degree of freedom on background grid to store information of multiple materials, so that our framework is able to deal with multiple materials well. The proposed method leverages the advantages of MPM as a hybrid method. We introduce the mixture model into the framework, which was the most widely used for grid-based multi-fluid flows. This enables MPM to capture the interaction and relative motion, and animates complex and coupled fluids and solids in a unified manner. A series of experiments are presented to demonstrate effectiveness of our method.
   This paper proposes a unified MPM solver for multi-species simulations. We extend the degree of freedom on background grid to store information of multiple materials, so that our method can greatly deal with multiple materials. Our method enables MPM to capture the interaction and relative motion, and animates complex and coupled fluids and solids in a unified manner. image
C1 [Li, Bo; Liu, Shiguang] Tianjin Univ, Coll Intelligence & Comp, 135 Yaguan Rd, Tianjin 300350, Peoples R China.
C3 Tianjin University
RP Liu, SG (corresponding author), Tianjin Univ, Coll Intelligence & Comp, 135 Yaguan Rd, Tianjin 300350, Peoples R China.
EM lsg@tju.edu.cn
OI li, bo/0009-0002-0905-3822
FU National Natural Science Foundation of China
FX No Statement Available
CR Akinci N, 2013, COMPUT ANIMAT VIRT W, V24, P195, DOI 10.1002/cav.1499
   Akinci N, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185558
   BRACKBILL JU, 1986, J COMPUT PHYS, V65, P314, DOI 10.1016/0021-9991(86)90211-1
   Chen XS, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417809
   Fang Y, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392438
   Fang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322968
   Fei Y, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459678
   Fei Y, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201392
   Fu CY, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130878
   Gao M, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201309
   Gross M., 2005, P 2005 ACM SIGGRAPH, P237, DOI DOI 10.1145/1073368.1073402
   Hu YM, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356506
   Hu YM, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201293
   Jiang CG, 2016, ADVANCES IN ARCHITECTURAL GEOMETRY 2016, P24
   Jiang C, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766996
   Jiang CFF, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073623
   Kolev N.I., 2005, MULTIPHASE FLOW DYNA
   Liu SG, 2011, VISUAL COMPUT, V27, P241, DOI 10.1007/s00371-010-0531-1
   Manninen M, MIXTURE MODEL MULTIP
   Nielsen MB, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461918
   Ren B, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2645703
   Shao X, 2015, COMPUT GRAPH FORUM, V34, P191, DOI 10.1111/cgf.12467
   Stomakhin A, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461948
   Stomakhin A, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601176
   Sun YC, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480541
   Tampubolon AP, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073651
   Tu ZL, 2024, ACM T GRAPHIC, V43, DOI 10.1145/3638047
   Wang XL, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392442
   Wretborn J, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530059
   Xu YH, 2023, PSYCHOL MED, DOI 10.1017/S0033291723001228
   Yan X, 2018, COMPUT GRAPH FORUM, V37, P183, DOI 10.1111/cgf.13523
   Yan X, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925897
   Yeoh G.H., 2019, Computational techniques for multiphase flows
   Zhu YN, 2005, ACM T GRAPHIC, V24, P965, DOI 10.1145/1073204.1073298
NR 34
TC 0
Z9 0
U1 3
U2 3
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAY
PY 2024
VL 35
IS 3
AR e2239
DI 10.1002/cav.2239
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RF8K4
UT WOS:001226341900001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Lee, J
   Kang, H
AF Lee, Jeyoung
   Kang, Hochul
TI PIPformers: Patch based inpainting with vision transformers for
   generalize paintings
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE generative adversarial networks; image inpainting; vision transformer
ID IMAGE; DIFFUSION; MATRIX
AB Image inpainting is a field that has been traditionally attempted in the field of computer vision. After the development of deep learning, image inpainting has been advancing endlessly together with convolutional neural networks and generative adversarial networks. Thereafter, it has been expanded to various fields such as image filing through guiding and image inpainting using various masking. Furthermore, the field termed image out-painting has also been pioneered. Meanwhile, after the recent announcement of the vision transformer, various computer vision problems have been attempted using the vision transformer. In this paper, we are trying to solve the problem of image generalization painting using the vision transformer. This is an attempt to fill images with painting no matter whether the areas where painting is missing are in or out of the images, and without guiding. To that end, the painting problem was defined as a problem to drop images in patch units for easy use in the vision transformer. And we solved the problem with a simple network structure created by slightly modifying the vision transformer to fit the problem. We named this network PIPformers. PIPformers achieved better values than other papers compared to PSNR, RMSE and SSIM.
C1 [Lee, Jeyoung; Kang, Hochul] Catholic Univ Korea, Dept Digital Media, 43 Jibong Ro, Bucheon, Gyeonggi Do, South Korea.
C3 Catholic University of Korea
RP Kang, H (corresponding author), Catholic Univ Korea, Dept Digital Media, 43 Jibong Ro, Bucheon, Gyeonggi Do, South Korea.
EM hckang19@catholic.ac.kr
FU Catholic University of Korea; Research Fund of The Catholic University
   of Korea; Korea Institute of Planning and Evaluation for Technology in
   Food, Agriculture and Forestry (IPET); Korea Smart Farm R&D Foundation -
   Ministry of Agriculture, Food and Rural Affairs (MAFRA); Ministry of
   Science and ICT (MSIT) [421024-04]; Rural Development Administration
   (RDA)
FX This work was supported by the Research Fund of The Catholic University
   of Korea in 2022. And this work was supported by Korea Institute of
   Planning and Evaluation for Technology in Food, Agriculture and Forestry
   (IPET) and Korea Smart Farm R&D Foundation (KoSFarm) through Smart Farm
   Innovation Technology Development Program, funded by Ministry of
   Agriculture, Food and Rural Affairs (MAFRA) and Ministry of Science and
   ICT (MSIT), Rural Development Administration (RDA) (grant number:
   421024-04).
CR Alilou VK, 2017, MULTIMED TOOLS APPL, V76, P7213, DOI 10.1007/s11042-016-3366-6
   Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   Chen M, 2020, PR MACH LEARN RES, V119
   Lu CN, 2021, PROC CVPR IEEE, P843, DOI 10.1109/CVPR46437.2021.00090
   Demir U, 2018, Arxiv, DOI arXiv:1803.07422
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Dumoulin V, 2018, Arxiv, DOI arXiv:1603.07285
   Elharrouss O, 2020, NEURAL PROCESS LETT, V51, P2007, DOI 10.1007/s11063-019-10163-0
   Esser P, 2021, PROC CVPR IEEE, P12868, DOI 10.1109/CVPR46437.2021.01268
   Gardias P, 2020, Arxiv, DOI arXiv:2005.06723
   Geirhos R, 2019, Arxiv, DOI arXiv:1811.12231
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hermann K., 2020, ADV NEURAL INFORM PR, V33, P19000
   Hsu C, 2017, 2017 INTERNATIONAL CONFERENCE ON VISION, IMAGE AND SIGNAL PROCESSING (ICVISP), P76, DOI 10.1109/ICVISP.2017.27
   Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiang Y., 2021, arXiv, DOI DOI 10.48550/ARXIV.2102.07074
   Jin KH, 2015, IEEE T IMAGE PROCESS, V24, P3498, DOI 10.1109/TIP.2015.2446943
   Jo C., 2021, PREPRINT
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Khan S, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3505244
   Li HD, 2017, IEEE T INF FOREN SEC, V12, P3050, DOI 10.1109/TIFS.2017.2730822
   Liu HY, 2019, IEEE I CONF COMP VIS, P4169, DOI 10.1109/ICCV.2019.00427
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Miyato T, 2018, Arxiv, DOI arXiv:1802.05957
   Parmar N, 2018, PR MACH LEARN RES, V80
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Radford A., 2019, OpenAI blog, V1, P9
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Ruzic T, 2015, IEEE T IMAGE PROCESS, V24, P444, DOI 10.1109/TIP.2014.2372479
   Sabini M., 2018, arXiv
   Sridevi G, 2019, CIRC SYST SIGNAL PR, V38, P3802, DOI 10.1007/s00034-019-01029-w
   Van Hoorick B, 2020, Arxiv, DOI [arXiv:1912.10960, 10.48550/arXiv.1912.10960]
   Wang XP, 2021, INT CONF 3D VISION, P106, DOI 10.1109/3DV53792.2021.00021
   Wang Y, 2019, PROC CVPR IEEE, P1399, DOI 10.1109/CVPR.2019.00149
   Yan ZY, 2018, LECT NOTES COMPUT SC, V11218, P3, DOI 10.1007/978-3-030-01264-9_1
   Yang ZX, 2019, IEEE I CONF COMP VIS, P10560, DOI 10.1109/ICCV.2019.01066
   Yao F, 2019, CLUSTER COMPUT, V22, P13683, DOI 10.1007/s10586-018-2068-4
   Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Yu L, 2020, 2020 13TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, BIOMEDICAL ENGINEERING AND INFORMATICS (CISP-BMEI 2020), P110, DOI 10.1109/CISP-BMEI51763.2020.9263639
   Zhang LZ, 2020, IEEE WINT CONF APPL, P3422, DOI [10.1109/wacv45572.2020.9093636, 10.1109/WACV45572.2020.9093636]
   Zhang XF, 2020, IEEE SIGNAL PROC LET, V27, P1590, DOI 10.1109/LSP.2020.3019705
   Zhu XS, 2018, SIGNAL PROCESS-IMAGE, V67, P90, DOI 10.1016/j.image.2018.05.015
NR 46
TC 0
Z9 0
U1 1
U2 1
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAY
PY 2024
VL 35
IS 3
AR e2270
DI 10.1002/cav.2270
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RG3S4
UT WOS:001226480600001
DA 2024-08-05
ER

PT J
AU Cai, JL
   Li, FWB
   Nan, FZ
   Yang, BL
AF Cai, Jianlu
   Li, Frederick W. B.
   Nan, Fangzhe
   Yang, Bailin
TI Multi-style cartoonization: Leveraging multiple datasets with generative
   adversarial networks
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE generative adversarial network; multi-style transfer; photo
   cartoonization
AB Scene cartoonization aims to convert photos into stylized cartoons. While generative adversarial networks (GANs) can generate high-quality images, previous methods focus on individual images or single styles, ignoring relationships between datasets. We propose a novel multi-style scene cartoonization GAN that leverages multiple cartoon datasets jointly. Our main technical contribution is a multi-branch style encoder that disentangles representations to model styles as distributions over entire datasets rather than images. Combined with a multi-task discriminator and perceptual losses optimizing across collections, our model achieves state-of-the-art diverse stylization while preserving semantics. Experiments demonstrate that by learning from inter-dataset relationships, our method translates photos into cartoon images with improved realism and abstraction fidelity compared to prior arts, without iterative re-training for new styles.
   We introduce a multi-style scene cartoonization GAN aiming to enhance the technique of photo-to-cartoon conversion. By amalgamating multiple cartoon datasets and employing innovative encoding methods, our model achieves more realistic and abstract cartoon effects, surpassing previous approaches. By capturing relationships between datasets, we can provide high-quality cartoon images without the need for tedious iterative retraining, marking a subtle but significant advancement in the field. image
C1 [Cai, Jianlu; Nan, Fangzhe; Yang, Bailin] Zhejiang Gongshang Univ, Dept Comp Sci & Technol, Hangzhou, Peoples R China.
   [Li, Frederick W. B.] Univ Durham, Dept Comp Sci, Durham, England.
   [Yang, Bailin] Zhejiang Gongshang Univ, Dept Comp Sci, Hangzhou, Peoples R China.
C3 Zhejiang Gongshang University; Durham University; Zhejiang Gongshang
   University
RP Yang, BL (corresponding author), Zhejiang Gongshang Univ, Dept Comp Sci, Hangzhou, Peoples R China.
EM ybl@zjgsu.edu.cn
OI Cai, jianlu/0009-0004-7290-5308; Li, Frederick W. B./0000-0002-4283-4228
FU Natural Science Foundation of Zhejiang Province [LD24F020003]; Zhejiang
   Provincial Natural Science Foundation of China [62172366]; NSFC
   (National Natural Science Foundation of China)
FX This research was supported by Zhejiang Provincial Natural Science
   Foundation of China under Grant No. LD24F020003. The authors also
   acknowledge the support from the NSFC (National Natural Science
   Foundation of China) under Grant No. 62172366.
CR Ahn N, 2023, PROC CVPR IEEE, P16827, DOI 10.1109/CVPR52729.2023.01614
   Cai Q, 2023, COMPUT ELECTR ENG, V108, DOI 10.1016/j.compeleceng.2023.108723
   Chen J., 2020, INT S INT COMP APPL, P242, DOI DOI 10.1007/978-981-15-5577-0_18
   Chen Y, 2018, PROC CVPR IEEE, P9465, DOI 10.1109/CVPR.2018.00986
   Cheng B., 2023, P IEEECVF INT C COMP, P22736
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Gurney J., 2010, Color and light: a guide for the realist painter, V2
   Hensel M, 2017, ADV NEUR IN, V30
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Jiang YX, 2023, IEEE I CONF COMP VIS, P7323, DOI 10.1109/ICCV51070.2023.00676
   Jung Chanyong, 2022, P IEEE CVF C COMP VI, P18260
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T, 2021, ADV NEUR IN, V34
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kim J, 2020, Arxiv, DOI [arXiv:1907.10830, DOI 10.48550/ARXIV.1907.10830]
   Kingma D. P., 2014, arXiv
   Liu MY, 2017, ADV NEUR IN, V30
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Mirza M., 2014, ARXIV
   Shu YZ, 2022, IEEE T VIS COMPUT GR, V28, P3376, DOI 10.1109/TVCG.2021.3067201
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Taesung Park, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P319, DOI 10.1007/978-3-030-58545-7_19
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Winnemoeller H, 2012, COMPUT GRAPH-UK, V36, P740, DOI 10.1016/j.cag.2012.03.004
   Xinrui Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8087, DOI 10.1109/CVPR42600.2020.00811
   Yunjey Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8185, DOI 10.1109/CVPR42600.2020.00821
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 30
TC 0
Z9 0
U1 1
U2 1
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAY
PY 2024
VL 35
IS 3
AR e2269
DI 10.1002/cav.2269
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RF8D8
UT WOS:001226335300001
OA Green Accepted
DA 2024-08-05
ER

PT J
AU Feng, SM
   Hou, F
   Chen, JL
   Wang, WC
AF Feng, Shiming
   Hou, Fei
   Chen, Jialu
   Wang, Wencheng
TI Extracting roads from satellite images via enhancing road feature
   investigation in learning
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE background feature suppression; feature alignment; road extraction
ID AERIAL
AB It is a hot topic to extract road maps from satellite images. However, it is still very challenging with existing methods to achieve high-quality results, because the regions covered by satellite images are very large and the roads are slender, complex and only take up a small part of a satellite image, making it difficult to distinguish roads from the background in satellite images. In this article, we address this challenge by presenting two modules to more effectively learn road features, and so improving road extraction. The first module exploits the differences between the patches containing roads and the patches containing no road to exclude the background regions as many as possible, by which the small part containing roads can be more specifically investigated for improvement. The second module enhances feature alignment in decoding feature maps by using strip convolution in combination with the attention mechanism. These two modules can be easily integrated into the networks of existing learning methods for improvement. Experimental results show that our modules can help existing methods to achieve high-quality results, superior to the state-of-the-art methods.
   Two modules are developed to improve road feature detection, one for avoiding interferences from background regions, and the other for using strip convolution to enhance feature alignment. These two modules can be easily integrated with existing methods for promoting their effectiveness on road detection, as attested by the experimental results. image
C1 [Feng, Shiming; Hou, Fei; Wang, Wencheng] Chinese Acad Sci, Key Lab Syst Software CAS, Beijing, Peoples R China.
   [Feng, Shiming; Hou, Fei; Wang, Wencheng] Chinese Acad Sci, Inst Software, SKLCS, Beijing, Peoples R China.
   [Feng, Shiming] Beijing Zhongke Arclight Quantum Software Technol, Beijing, Peoples R China.
   [Feng, Shiming; Hou, Fei; Chen, Jialu; Wang, Wencheng] Univ Chinese Acad Sci, Beijing, Peoples R China.
   [Chen, Jialu] Univ Chinese Acad Sci, Hangzhou Inst Adv Study, Hangzhou, Peoples R China.
C3 Chinese Academy of Sciences; Chinese Academy of Sciences; Institute of
   Software, CAS; Chinese Academy of Sciences; University of Chinese
   Academy of Sciences, CAS; Chinese Academy of Sciences; University of
   Chinese Academy of Sciences, CAS
RP Wang, WC (corresponding author), Chinese Acad Sci, Key Lab Syst Software CAS, Beijing, Peoples R China.; Wang, WC (corresponding author), Chinese Acad Sci, Inst Software, SKLCS, Beijing, Peoples R China.
EM whn@ios.ac.cn
OI Feng, Shiming/0009-0009-1745-4512
CR Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bastani F, 2018, PROC CVPR IEEE, P4720, DOI 10.1109/CVPR.2018.00496
   Batra A, 2019, PROC CVPR IEEE, P10377, DOI 10.1109/CVPR.2019.01063
   Chaurasia A, 2017, 2017 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Dai L, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3237561
   Demir I, 2018, IEEE COMPUT SOC CONF, P172, DOI 10.1109/CVPRW.2018.00031
   GRUEN A, 1995, ISPRS J PHOTOGRAMM, V50, P11, DOI 10.1016/0924-2716(95)98233-P
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu JT, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3305031
   Huang SH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P844, DOI 10.1109/ICCV48922.2021.00090
   Huang ZL, 2022, IEEE T PATTERN ANAL, V44, P550, DOI 10.1109/TPAMI.2021.3062772
   Li SF, 2022, ISPRS INT J GEO-INF, V11, DOI 10.3390/ijgi11010009
   Luc P., 2016, Semantic segmentation using adversarial networks. NIPS workshop on adversarial training; 2016
   Máttyus G, 2017, IEEE I CONF COMP VIS, P3458, DOI 10.1109/ICCV.2017.372
   Mei J, 2021, IEEE T IMAGE PROCESS, V30, P8540, DOI 10.1109/TIP.2021.3117076
   Mena JB, 2003, PATTERN RECOGN LETT, V24, P3037, DOI 10.1016/S0167-8655(03)00164-8
   Mnih V., Machine learning for aerial image labeling
   Mosinska A, 2018, PROC CVPR IEEE, P3136, DOI 10.1109/CVPR.2018.00331
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Panboonyuen T, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9070680
   Shi Q, 2018, IEEE ACCESS, V6, P25486, DOI 10.1109/ACCESS.2017.2773142
   Tao JJ, 2023, REMOTE SENS-BASEL, V15, DOI 10.3390/rs15061602
   Tong ZG, 2023, REMOTE SENS-BASEL, V15, DOI 10.3390/rs15081978
   Ünsalan C, 2012, IEEE T GEOSCI REMOTE, V50, P4441, DOI 10.1109/TGRS.2012.2190078
   Van Etten A, 2019, Arxiv, DOI arXiv:1807.01232
   Wang Y, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2021.3050477
   Wegner JD, 2013, PROC CVPR IEEE, P1698, DOI 10.1109/CVPR.2013.222
   Wei YN, 2017, IEEE GEOSCI REMOTE S, V14, P709, DOI 10.1109/LGRS.2017.2672734
   Yong-Qiang Tan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8907, DOI 10.1109/CVPR42600.2020.00893
   Zhou LC, 2018, IEEE COMPUT SOC CONF, P192, DOI 10.1109/CVPRW.2018.00034
NR 31
TC 0
Z9 0
U1 0
U2 0
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAY
PY 2024
VL 35
IS 3
AR e2275
DI 10.1002/cav.2275
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TW7S0
UT WOS:001244367600001
DA 2024-08-05
ER

PT J
AU Bao, YT
   Liu, X
   Qi, Y
   Liu, RJ
   Li, HJ
AF Bao, Yongtang
   Liu, Xiang
   Qi, Yue
   Liu, Ruijun
   Li, Haojie
TI Adaptive information fusion network for multi-modal personality
   recognition
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE adaptation; encoder; multi-modal data; personality recognition
AB Personality recognition is of great significance in deepening the understanding of social relations. While personality recognition methods have made significant strides in recent years, the challenge of heterogeneity between modalities during feature fusion still needs to be solved. This paper introduces an adaptive multi-modal information fusion network (AMIF-Net) capable of concurrently processing video, audio, and text data. First, utilizing the AMIF-Net encoder, we process the extracted audio and video features separately, effectively capturing long-term data relationships. Then, adding adaptive elements in the fusion network can alleviate the problem of heterogeneity between modes. Lastly, we concatenate audio-video and text features into a regression network to obtain Big Five personality trait scores. Furthermore, we introduce a novel loss function to address the problem of training inaccuracies, taking advantage of its unique property of exhibiting a peak at the critical mean. Our tests on the ChaLearn First Impressions V2 multi-modal dataset show partial performance surpassing state-of-the-art networks.
   This paper proposes an adaptive multimodal information fusion network for personality recognition. The design features of each encoder are optimized and merged for downstream tasks. We greatly enhance the functionality of the Transformer component by integrating adaptive attention and automatic learning of cross-modal associations. This not only solves the problem of outliers and gradient vanishing during model training, but also has practical significance for practical applications. image
C1 [Bao, Yongtang; Liu, Xiang; Li, Haojie] Shandong Univ Sci & Technol, Sch Comp Sci & Engn, Qingdao, Peoples R China.
   [Qi, Yue] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
   [Qi, Yue] Beihang Univ, Virtual Real Res Inst, Qingdao Res Inst, Qingdao, Peoples R China.
   [Liu, Ruijun] Beihang Univ, Sch Software, Beijing, Peoples R China.
C3 Shandong University of Science & Technology; Beihang University; Beihang
   University; Beihang University
RP Li, HJ (corresponding author), Shandong Univ Sci & Technol, Sch Comp Sci & Engn, Qingdao, Peoples R China.; Liu, RJ (corresponding author), Beihang Univ, Sch Software, Beijing, Peoples R China.
EM liuruijun@buaa.edu.cn; hjli@sdust.edu.cn
FU National Natural Science Foundation of China; National Science and
   Technology Major Project [2022ZD0119502]; Taishan Scholar Program of
   Shandong Province [tstp20221128]; Beijing Natural Science Foundation;
   Haidian Original Innovation Joint Fund [L222052]; Talented Young
   Teachers Training Program of Shandong University of Science and
   Technology [BJ20231201];  [62072020]
FX This research was funded by the National Science and Technology Major
   Project (2022ZD0119502), the National Natural Science Foundation of
   China (62072020), the Taishan Scholar Program of Shandong Province (No.
   tstp20221128), the Beijing Natural Science Foundation and Haidian
   Original Innovation Joint Fund (L222052), and the Talented Young
   Teachers Training Program of Shandong University of Science and
   Technology (BJ20231201).
CR Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Fu YH, 2024, Arxiv, DOI arXiv:2401.05871
   Gabeur V., 2020, Multimodal transformer for video retrieval. Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, P214
   Gucluturk Y, 2016, LECT NOTES COMPUT SC, V9915, P349, DOI 10.1007/978-3-319-49409-8_28
   Han W., 2023, Speakeraware hierarchical transformer for personality recognition in multiparty dialogues. ICASSP 20232023 IEEE International Conference on Acoustics, P1
   Hayat H, 2019, FRONT ARTIF INTEL AP, V319, P135, DOI 10.3233/FAIA190116
   Iashin V., 2020, A better use of audiovisual cues: dense video captioning with bimodal transformer. CoRR. abs/2005.08271
   Iffath F, 2023, COMPUT ANIMAT VIRT W, V34, DOI 10.1002/cav.2163
   Jacques JCS Jr, 2022, IEEE T AFFECT COMPUT, V13, P75, DOI 10.1109/TAFFC.2019.2930058
   Escalante HJ, 2022, IEEE T AFFECT COMPUT, V13, P894, DOI 10.1109/TAFFC.2020.2973984
   Jaiswal S, 2019, INT CONF AFFECT, DOI 10.1109/acii.2019.8925456
   Li F, 2022, Arxiv, DOI arXiv:2209.04148
   Li YA, 2020, INT J COMPUT VISION, V128, P2763, DOI 10.1007/s11263-020-01309-y
   Liao R., 2024, IEEE Trans Affect Comput, P1, DOI [10.1109/TAFFC.2024.3363710, DOI 10.1109/TAFFC.2024.3363710]
   NORMAN WT, 1963, J ABNORM PSYCHOL, V66, P574, DOI 10.1037/h0040291
   Ponce-López V, 2016, LECT NOTES COMPUT SC, V9915, P400, DOI 10.1007/978-3-319-49409-8_32
   Serrano-Guerrero J, 2024, EGYPT INFORM J, V25, DOI 10.1016/j.eij.2024.100439
   Song SY, 2023, IEEE T AFFECT COMPUT, V14, P178, DOI 10.1109/TAFFC.2021.3064601
   Song SY, 2022, IEEE T AFFECT COMPUT, V13, P829, DOI 10.1109/TAFFC.2020.2970712
   Sun X, 2018, IEEE ICC
   Sun X, 2022, IEEE T IMAGE PROCESS, V31, P2162, DOI 10.1109/TIP.2022.3152049
   Tiwari J., 2023, J Integr Sci Technol, V11, P578
   Wang H, 2022, COMPUT ANIMAT VIRT W, V33, DOI 10.1002/cav.2090
   Wang Y, 2023, PROCEEDINGS OF THE 2023 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, ICMR 2023, P243, DOI 10.1145/3591106.3592243
   Wei XS, 2018, IEEE T AFFECT COMPUT, V9, P303, DOI 10.1109/TAFFC.2017.2762299
   Wen ZY, 2023, INFORM PROCESS MANAG, V60, DOI 10.1016/j.ipm.2023.103422
   Xu P, 2023, IEEE T PATTERN ANAL, V45, P12113, DOI 10.1109/TPAMI.2023.3275156
   Zhang T, 2017, INT J AUTOM COMPUT, V14, P386, DOI 10.1007/s11633-017-1085-8
   Zhu L., 2020, Actbert: learning globallocal videotext representations. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, P8746
   Zhu XQ, 2024, COMPUT ANIMAT VIRT W, V35, DOI 10.1002/cav.2201
NR 30
TC 0
Z9 0
U1 0
U2 0
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAY 6
PY 2024
VL 35
IS 3
AR e2268
DI 10.1002/cav.2268
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TL2P6
UT WOS:001241355600001
DA 2024-08-05
ER

PT J
AU Pan, ZG
   Ren, HY
   Liu, C
   Chen, M
   Mukherjee, M
   Yang, WZ
AF Pan, Zhigeng
   Ren, Hongyi
   Liu, Chang
   Chen, Ming
   Mukherjee, Mithun
   Yang, Wenzhen
TI Design of a lightweight and easy-to-wear hand glove with multi-modal
   tactile perception for digital human
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE AR/VR/XR; data gloves; digital human; embedded system; human-computer
   interaction; metaverse; virtual environment
AB Within the field of human-computer interaction, data gloves play an essential role in establishing a connection between virtual and physical environments for the realization of digital human. To enhance the credibility of human-virtual hand interactions, we aim to develop a system incorporating a data glove-embedded technology. Our proposed system collects a wide range of information (temperature, bending, and pressure of fingers) that arise during natural interactions and afterwards reproduce them within the virtual environment. Furthermore, we implement a novel traversal polling technique to facilitate the streamlined aggregation of multi-channel sensors. This mitigates the hardware complexity of the embedded system. The experimental results indicate that the data glove demonstrates a high degree of precision in acquiring real-time hand interaction information, as well as effectively displaying hand posture in real-time using Unity3D. The data glove's lightweight and compact design facilitates its versatile utilization in virtual reality interactions.
   The gloves proposed in this paper integrate 16 sensors capable of detecting temperature, finger pressure, and bending data of the hand. Additionally, we have designed a small circuit board. The incorporation of a novel traversal polling method for the streamlined collection of multi-channel sensor readings is innovative. The experimental results indicate that the data glove demonstrates a high degree of precision in acquiring real-time hand interaction information, as well as effectively displaying hand posture in real-time using Unity3D. image
C1 [Pan, Zhigeng; Ren, Hongyi; Mukherjee, Mithun] Nanjing Univ Informat Sci & Technol, Sch Artificial Intelligence, Nanjing, Peoples R China.
   [Liu, Chang] Zhejiang Lab, Ctr Spacebased Comp Syst, Hangzhou, Peoples R China.
   [Chen, Ming] Zhejiang Sci Tech Univ, Coll Mech Engn, Hangzhou, Peoples R China.
   [Yang, Wenzhen] Zhejiang Lab, Ctr Intelligent Mfg Comp, Hangzhou, Peoples R China.
C3 Nanjing University of Information Science & Technology; Zhejiang
   Laboratory; Zhejiang Sci-Tech University; Zhejiang Laboratory
RP Yang, WZ (corresponding author), Zhejiang Lab, Ctr Intelligent Mfg Comp, Hangzhou, Peoples R China.
EM ywz@zhejianglab.edu.cn
OI Ren, Hongyi/0000-0002-9015-7575
FU Key Research Project of the Zhejiang Lab; National Key Research and
   Development Program of China [2021YFF0600203]; Youth Fund of the
   Zhejiang Lab [K2023MG0AA11, K2023MG0AA02]; National Science Foundation
   of China [62072150, 61901128]; Postgraduate Research & Practice
   Innovation Program of Jiangsu Province [KYCX23_1374];  [K2022PG1BB01]; 
   [2022MG0AC04]
FX This article is supported by the National Key Research and Development
   Program of China (2021YFF0600203); the Key Research Project of the
   Zhejiang Lab (K2022PG1BB01, 2022MG0AC04); Youth Fund of the Zhejiang Lab
   (K2023MG0AA11, K2023MG0AA02); the National Science Foundation of China
   (62072150 and 61901128); Postgraduate Research & Practice Innovation
   Program of Jiangsu Province (KYCX23_1374).
CR Akcaoglu M, 2022, TECHTRENDS, V66, P931, DOI 10.1007/s11528-022-00782-1
   Bravo E., 2022, Controversies in orthopedic surgery of the upper limb, P265
   Caeiro-Rodríguez M, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21082667
   Connolly J, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22062228
   Delmerico J, 2022, IEEE ROBOT AUTOM MAG, V29, P45, DOI 10.1109/MRA.2021.3138384
   Ferdiansyah T, 2022, INT J CIV ENG, V20, P75, DOI 10.1007/s40999-021-00659-z
   Girvan C, 2018, ETR&D-EDUC TECH RES, V66, P1087, DOI 10.1007/s11423-018-9577-y
   Golf-Papez M, 2022, BUS HORIZONS, V65, P739, DOI 10.1016/j.bushor.2022.07.007
   Hamledari H, 2021, AUTOMAT CONSTR, V132, DOI 10.1016/j.autcon.2021.103926
   Ji BW, 2023, INT J HUM-COMPUT INT, DOI 10.1080/10447318.2023.2212232
   Lin WQ, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22176327
   Lu CH, 2023, ELECTRONICS-SWITZ, V12, DOI 10.3390/electronics12030613
   Mystakidis S., 2022, ENCYCLOPEDIA, V2, P486, DOI [https://doi.org/10.3390/encyclopedia2010031, 10.3390/encyclopedia2010031, DOI 10.3390/ENCYCLOPEDIA2010031]
   Oliveira R, 2019, INT C CONTROL DECISI, P79, DOI [10.1109/CoDIT.2019.8820516, 10.1109/codit.2019.8820516]
   Pellas N, 2021, VIRTUAL REAL-LONDON, V25, P835, DOI 10.1007/s10055-020-00489-9
   Pellas N, 2020, IEEE T LEARN TECHNOL, V13, P748, DOI 10.1109/TLT.2020.3019405
   Safari A, 2021, IET CIRC DEVICE SYST, V15, P485, DOI 10.1049/cds2.12038
   Tailliet F., 2020, US patent, Patent No. 10558609
   Añazco EV, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21041404
   Wang H, 2023, IEEE INTERNET THINGS, V10, P14671, DOI 10.1109/JIOT.2023.3278329
   Xu LD, 2021, IEEE INTERNET THINGS, V8, P10452, DOI 10.1109/JIOT.2021.3060508
NR 21
TC 0
Z9 0
U1 3
U2 3
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAY
PY 2024
VL 35
IS 3
AR e2258
DI 10.1002/cav.2258
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SN7R9
UT WOS:001235201500001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Xu, SB
   Hua, M
   Zhang, JG
   Zhang, ZH
   Zhang, XP
AF Xu, Shibiao
   Hua, Miao
   Zhang, Jiguang
   Zhang, Zhaohui
   Zhang, Xiaopeng
TI Key-point-guided adaptive convolution and instance normalization for
   continuous transitive face reenactment of any person
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE face reenactment; human-centered computing; visualization; visualization
   application domains
ID RECONSTRUCTION
AB Face reenactment technology is widely applied in various applications. However, the reconstruction effects of existing methods are often not quite realistic enough. Thus, this paper proposes a progressive face reenactment method. First, to make full use of the key information, we propose adaptive convolution and instance normalization to encode the key information into all learnable parameters in the network, including the weights of the convolution kernels and the means and variances in the normalization layer. Second, we present continuous transitive facial expression generation according to all the weights of the network generated by the key points, resulting in the continuous change of the image generated by the network. Third, in contrast to classical convolution, we apply the combination of depth- and point-wise convolutions, which can greatly reduce the number of weights and improve the efficiency of training. Finally, we extend the proposed face reenactment method to the face editing application. Comprehensive experiments demonstrate the effectiveness of the proposed method, which can generate a clearer and more realistic face from any person and is more generic and applicable than other methods.
   This work presents a continuous transitive face reenactment algorithm that uses face key points information to gradually reenact faces based on two stages GAN, which contains the key face points transformation module and the facial expression generation module. The process involves transforming key points from the source face and generating corresponding facial expressions on the target face. image
C1 [Xu, Shibiao] Beijing Univ Posts & Telecommun, Sch Artificial Intelligence, Beijing, Peoples R China.
   [Hua, Miao] Beijing Bytedance Technol Co Ltd, Beijing, Peoples R China.
   [Zhang, Jiguang; Zhang, Zhaohui; Zhang, Xiaopeng] Chinese Acad Sci, Inst Automat, Beijing, Peoples R China.
C3 Beijing University of Posts & Telecommunications; Chinese Academy of
   Sciences; Institute of Automation, CAS
RP Zhang, JG (corresponding author), Chinese Acad Sci, Inst Automat, Beijing, Peoples R China.
EM jiguang.zhang@ia.ac.cn
FU Beijing Natural Science Foundation; National Natural Science Foundation
   of China [62271074, 62171321, 62162044, 52175493, 32271983]; Open
   Project Program of State Key Laboratory of Virtual Reality Technology
   and Systems, Beihang University [VRLAB2023B01]; Wenzhou Business School
   2024 Talent launch program [RC202401];  [JQ23014]
FX This work is supported by Beijing Natural Science Foundation no.
   JQ23014, in part by the National Natural Science Foundation of China
   (Nos. 62271074, 62171321, 62162044, 52175493 and 32271983), and in part
   by the Open Project Program of State Key Laboratory of Virtual Reality
   Technology and Systems, Beihang University (No. VRLAB2023B01), and in
   part by the Wenzhou Business School 2024 Talent launch program (No.
   RC202401).
CR Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Chai XY, 2021, IEEE T MULTIMEDIA, V23, P2998, DOI 10.1109/TMM.2021.3068567
   Chen SY, 2023, IEEE T MULTIMEDIA, V25, P3166, DOI 10.1109/TMM.2022.3156820
   Dong HY, 2018, 32 C NEURAL INFORM P
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Guo Xiaojie., 2019, PFLD: A Practical Facial Landmark Detector
   Hsu GS., 2022, Dualgenerator face reenactment. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition Example Organization
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jin X., 2017, CycleGAN faceoff. CoRR
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kingma DP., 3rd international conference on learning representations, ICLR 2015, San Diego, CA, USA, May 79, 2015, conference track proceedings, P2015
   Langner O, 2010, COGNITION EMOTION, V24, P1377, DOI 10.1080/02699930903485076
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Lou JW, 2020, IEEE T MULTIMEDIA, V22, P730, DOI 10.1109/TMM.2019.2933338
   Ma LQ, 2017, ADV NEUR IN, V30
   Ma LM, 2019, ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES (I3D 2019), DOI 10.1145/3306131.3317016
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song LX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P627, DOI 10.1145/3240508.3240612
   Suwajanakorn S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073640
   Thies J, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201350
   Thies J, 2016, PROC CVPR IEEE, P2387, DOI 10.1109/CVPR.2016.262
   Tu XG, 2021, IEEE T MULTIMEDIA, V23, P1160, DOI 10.1109/TMM.2020.2993962
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang W, 2020, IEEE T MULTIMEDIA, V22, P2808, DOI 10.1109/TMM.2019.2963621
   Wang XS, 2023, IEEE T MULTIMEDIA, V25, P6717, DOI 10.1109/TMM.2022.3214100
   Wang XT, 2019, PROC CVPR IEEE, P1692, DOI 10.1109/CVPR.2019.00179
   Wu W., Computer visionECCV 201815th European conference, Munich, Germany, September 814, 2018, proceedings, V2018, P622
   Xu R., 2017, Face transfer with generative adversarial network. CoRR
   Zhang JN, 2020, PROC CVPR IEEE, P5325, DOI 10.1109/CVPR42600.2020.00537
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 32
TC 0
Z9 0
U1 1
U2 1
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAY
PY 2024
VL 35
IS 3
AR e2256
DI 10.1002/cav.2256
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RU5A1
UT WOS:001230174100001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Ye, JM
   Liu, Z
   Liu, TT
   Wu, YH
   Wang, YY
AF Ye, Jianming
   Liu, Zhen
   Liu, Tingting
   Wu, Yanhui
   Wang, Yuanyi
TI Crowd evacuation simulation based on hierarchical agent model and
   physics-based character control
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE agent model; crowd simulation; deep reinforcement learning;
   physics-based character control
AB Crowd evacuation has gained increasing attention in recent years. The agent-based method has shown a superior capability to simulate complex behaviors during crowd evacuation simulation. For agent modeling, most existing methods only consider the decision process but ignore the detailed physical motion. In this article, we propose a hierarchical framework for crowd evacuation simulation, which combines the agent decision model with the agent motion model. In the decision model, we integrate emotional contagion and scene information to determine global path planning and local collision avoidance. In the motion model, we introduce a physics-based character control method and control agent motion using deep reinforcement learning. Based on the decision strategy, the decision model can use a signal to control the agent motion in the motion model. Compared with existing methods, our framework can simulate physical interactions between agents and the environment. The results of the crowd evacuation simulation demonstrate that our framework can simulate crowd evacuation with physical fidelity.
C1 [Ye, Jianming; Liu, Zhen] Ningbo Univ, Fac Informat Sci & Technol, Ningbo, Peoples R China.
   [Liu, Tingting; Wang, Yuanyi] Ningbo Univ, Coll Sci & Technol, Cixi, Peoples R China.
   [Wu, Yanhui] Ningbo Inst Digital Twin, Eastern Inst Technol, Ningbo, Peoples R China.
C3 Ningbo University; Ningbo University
RP Liu, Z (corresponding author), Ningbo Univ, Fac Informat Sci & Technol, Ningbo, Peoples R China.
EM liuzhen@nbu.edu.cn
OI liu, tingting/0000-0002-3469-275X
FU Natural Science Foundation of Zhejiang Province; Ningbo Science
   Technology Plan projects [2022Z077, 2023Z230, 2023Z214];  [LZ23F020005]
FX This work was partially sponsored by the Natural Science Foundation of
   Zhejiang Province (Grant LZ23F020005) and by the Ningbo Science
   Technology Plan projects (Grants 2022Z077, 2023Z230, 2023Z214).
CR Berg JPVD., 2008, Reciprocal velocity obstacles for realtime multiagent navigation. IEEE International Conference on Robotics Automation
   Bergamin K, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356536
   Bollo D, 2018, Inertialization: highperformance animation transitions in gears of war. In Proc. of GDC 2018
   Chraibi M, 2010, PHYS REV E, V82, DOI 10.1103/PhysRevE.82.046111
   Clavet S., 2016, Motion matching and the road to nextgen animation. In Proc of GDC 2016
   Coros S, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1781156
   Ding K., 2015, Learning reducedorder feedback policies for motion skills. Proceedings of the 14th ACM SIGGRAPH / Eurographics symposium on computer animation
   Durupinar F, 2016, IEEE T VIS COMPUT GR, V22, P2145, DOI 10.1109/TVCG.2015.2501801
   Felis ML., 2016, Synthesis of fullbody 3D human gait using optimal control methods. IEEE International Conference on Robotics Automation
   Geijtenbeek T, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508399
   Hassan M., 2023, Synthesizing physical characterscene interactions. ACM SIGGRAPH conference proceedings
   Haworth B., 2020, Deep integration of physical humanoid control and crowd navigation. Proceedings of the 13th ACM SIGGRAPH conference on motion
   Heess N., 2017, ARXIV170702286, P1
   HELBING D, 1995, PHYS REV E, V51, P4282, DOI 10.1103/PhysRevE.51.4282
   Hodgins J.K., 1995, Animating human athletics
   Hu KD, 2023, IEEE T VIS COMPUT GR, V29, P2036, DOI 10.1109/TVCG.2021.3139031
   Kapadia M., 2015, Virtual crowds: steps toward behavioral realism
   Liu L., 2010, ACM SIGGRAPH Papers, P1
   Liu LB, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2893476
   Liu LB, 2015, COMPUT GRAPH FORUM, V34, P415, DOI 10.1111/cgf.12571
   Liu Q., 2022, Phys A: Stat Mech Appl
   Olivier AH., 2011, Comput Anim Virtual Worlds, V22 (5):421433
   Panayiotou A., 2022, CCP: configurable crowd profiles. ACM SIGGRAPH 2022 conference proceedings
   Peng XB, 2021, ACM T GRAPHIC, V40, DOI [10.1145/3450626.3459670, 10.1145/3197517.3201311]
   Peng XB, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201311
   Piccoli B, 2009, CONTINUUM MECH THERM, V21, P85, DOI 10.1007/s00161-009-0100-x
   Schulman J., 2017, ARXIV
   Stuvel SA., 2017, Comput Graph, V23, P1823
   Sun R., 2022, A heterogeneous cellular automata model for crowd evacuation in emergency situations. IEEE 25th international conference on intelligent transportation systems
   Tassa Y, 2012, IEEE INT C INT ROBOT, P4906, DOI 10.1109/IROS.2012.6386025
   Tian ZN, 2020, KNOWL-BASED SYST, V208, DOI 10.1016/j.knosys.2020.106451
   Treuille A, 2006, ACM T GRAPHIC, V25, P1160, DOI 10.1145/1141911.1142008
   Tsai Jason., 2011, ESCAPES EVACUATION S
   Wiggins J. S., 1996, The fivefactor model of personality: Theoretical perspectives
   Wong SK, 2018, ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES (I3D 2018), DOI 10.1145/3190834.3190839
   Wu WH, 2022, IEEE T INTELL TRANSP, V23, P15476, DOI 10.1109/TITS.2022.3140823
   Xu ML, 2021, IEEE T SYST MAN CY-S, V51, P1567, DOI 10.1109/TSMC.2019.2899047
   Xue J., 2022, Crowd evacuation conflicts simulation based cellular automaton integrating game theory. Proceedings of the 18th ACM SIGGRAPH international conference on virtualreality continuum and its applications in industry
   Yao ZZ, 2020, NEUROCOMPUTING, V404, P173, DOI 10.1016/j.neucom.2020.04.141
   Yao ZZ, 2019, NEUROCOMPUTING, V366, P314, DOI 10.1016/j.neucom.2019.08.021
   Ye Y., 2010, Optimal feedback control for character animation using an abstract model. ACM SIGGRAPH papers
   Zhang D, 2023, INT J DIGIT EARTH, V16, P691, DOI 10.1080/17538947.2023.2182376
   Zhang H, 2018, PHYSICA A, V492, P1107, DOI 10.1016/j.physa.2017.11.041
   Zhuo L, 2021, COMPUT ANIMAT VIRT W, V32, DOI 10.1002/cav.1988
NR 44
TC 0
Z9 0
U1 1
U2 1
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAY
PY 2024
VL 35
IS 3
AR e2263
DI 10.1002/cav.2263
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SK4A9
UT WOS:001234322300001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Guo, Z
   Wei, BX
   Cai, QL
   Liu, JY
   Wang, Y
AF Guo, Zhe
   Wei, Bingxin
   Cai, Qinglin
   Liu, Jiayi
   Wang, Yi
TI POST: Prototype-oriented similarity transfer framework for cross-domain
   facial expression recognition
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE bidirectional cross-attention; facial expression recognition; learnable
   category prototypes; similarity transfer
AB Facial expression recognition (FER) is one of the popular research topics in computer vision. Most deep learning expression recognition methods perform well on a single dataset, but may struggle in cross-domain FER applications when applied to different datasets. FER under cross-dataset also suffers from difficulties such as feature distribution deviation and discriminator degradation. To address these issues, we propose a prototype-oriented similarity transfer framework (POST) for cross-domain FER. The bidirectional cross-attention Swin Transformer (BCS Transformer) module is designed to aggregate local facial feature similarities across different domains, enabling the extraction of relevant cross-domain features. The dual learnable category prototypes is designed to represent potential space samples for both source and target domains, ensuring enhanced domain alignment by leveraging both cross-domain and specific domain features. We further introduce the self-training resampling (STR) strategy to enhance similarity transfer. The experimental results with the RAF-DB dataset as the source domain and the CK+, FER2013, JAFFE and SFEW 2.0 datasets as the target domains, show that our approach achieves much higher performance than the state-of-the-art cross-domain FER methods.
   In this paper, we proposed a prototype-oriented similarity transfer framework (POST) for cross-domain facial expression recognition. The bidirectional cross-attention Swin Transformer (BCS Transformer) module is designed to aggregate local facial feature similarities across different domains. The dual learnable category prototypes is designed to represent potential space samples for both source and target domains. The self-training resampling (STR) strategy is further introduced to enhance similarity transfer. image
C1 [Guo, Zhe; Wei, Bingxin; Cai, Qinglin; Liu, Jiayi; Wang, Yi] Northwestern Polytech Univ, Sch Elect & Informat, Xian 710129, Peoples R China.
C3 Northwestern Polytechnical University
RP Guo, Z (corresponding author), Northwestern Polytech Univ, Sch Elect & Informat, Xian 710129, Peoples R China.
EM guozhe@nwpu.edu.cn
FU National Natural Science Foundation of China
FX The authors sincerely appreciate that academic editors and reviewers
   give their helpful comments and constructive suggestions.
CR Chen CF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P347, DOI 10.1109/ICCV48922.2021.00041
   Chen TS, 2022, IEEE T PATTERN ANAL, V44, P9887, DOI 10.1109/TPAMI.2021.3131222
   Dhall A., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P2106, DOI 10.1109/ICCVW.2011.6130508
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Fatras K., 2021, Unbalanced minibatch optimal transport; applications to domain adaptation. International conference on machine learning, P3186
   Goodfellow Ian J., 2013, Neural Information Processing. 20th International Conference, ICONIP 2013. Proceedings: LNCS 8228, P117, DOI 10.1007/978-3-642-42051-1_16
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Gretton A., 2006, Advances in neural information processing systems, V19, P513
   Li HY, 2022, PROC CVPR IEEE, P4156, DOI 10.1109/CVPR52688.2022.00413
   Li S, 2022, IEEE T AFFECT COMPUT, V13, P1195, DOI 10.1109/TAFFC.2020.2981446
   Li S, 2022, IEEE T AFFECT COMPUT, V13, P881, DOI 10.1109/TAFFC.2020.2973158
   Li S, 2018, INT C PATT RECOG, P3092, DOI 10.1109/ICPR.2018.8545284
   Li S, 2017, PROC CVPR IEEE, P2584, DOI 10.1109/CVPR.2017.277
   Liang Y, 2022, arXiv
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Long MS, 2018, ADV NEUR IN, V31
   Lucey P., 2010, 2010 IEEE COMP SOC C, P94, DOI DOI 10.1109/CVPRW.2010.5543262
   Lyons M, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P200, DOI 10.1109/AFGR.1998.670949
   Lyons M. J., 2021, arXiv
   Tanwisuth K, 2021, ADV NEUR IN, V34
   Wang C., 2022, A prototype-oriented contrastive adaption network for cross-domain facial expression recognition. Proceedings of the Asian conference on computer vision, P4194
   Wang W., 2012, Pyramid vision transformer: a versatile backbone for dense prediction without convolutions. Proceedings of the IEEE/CVF international conference on computer vision, P568
   Xu R., 2019, Larger norm more transferable: an adaptive feature norm approach for unsupervised domain adaptation. Proceedings of the IEEE/CVF international conference on computer vision, P142
   Xu TK, 2022, Arxiv, DOI arXiv:2109.06165
   Zeng D., 2022, Face2exp: combating data biases for facial expression recognition. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, P20291
   Zeng J., 2018, Facial expression recognition with inconsistently annotated datasets. Proceedings of the European conference on computer vision (ECCV), P222
   Zhang FF, 2018, PROC CVPR IEEE, P3359, DOI 10.1109/CVPR.2018.00354
   Zhang GQ, 2023, J VIS COMMUN IMAGE R, V95, DOI 10.1016/j.jvcir.2023.103898
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360
NR 29
TC 0
Z9 0
U1 0
U2 0
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAY
PY 2024
VL 35
IS 3
AR e2260
DI 10.1002/cav.2260
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RF6Y0
UT WOS:001226303500001
DA 2024-08-05
ER

PT J
AU Jiang, L
   Xiong, Y
   Wang, QQ
   Chen, T
   Wu, W
   Zhou, Z
AF Jiang, Ling
   Xiong, Yuan
   Wang, Qianqian
   Chen, Tong
   Wu, Wei
   Zhou, Zhong
TI SADNet: Generating immersive virtual reality avatars by real-time
   monocular pose estimation
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE 3D avatar; computer animation; human pose estimation
AB Generating immersive virtual reality avatars is a challenging task in VR/AR applications, which maps physical human body poses to avatars in virtual scenes for an immersive user experience. However, most existing work is time-consuming and limited by datasets, which does not satisfy immersive and real-time requirements of VR systems. In this paper, we aim to generate 3D real-time virtual reality avatars based on a monocular camera to solve these problems. Specifically, we first design a self-attention distillation network (SADNet) for effective human pose estimation, which is guided by a pre-trained teacher. Secondly, we propose a lightweight pose mapping method for human avatars that utilizes the camera model to map 2D poses to 3D avatar keypoints, generating real-time human avatars with pose consistency. Finally, we integrate our framework into a VR system, displaying generated 3D pose-driven avatars on Helmet-Mounted Display devices for an immersive user experience. We evaluate SADNet on two publicly available datasets. Experimental results show that SADNet achieves a state-of-the-art trade-off between speed and accuracy. In addition, we conducted a user experience study on the performance and immersion of virtual reality avatars. Results show that pose-driven 3D human avatars generated by our method are smooth and attractive.
   Generating immersive virtual reality avatars is a challenging task in VR/AR applications, which maps physical human body poses to avatars in virtual scenes for an immersive user experience. However, most existing work is time-consuming and limited by datasets, which does not satisfy immersive and real-time requirements of VR systems. In this paper, we aim to generate 3D real-time virtual reality avatars based on a monocular camera to solve these problems. Specifically, we first design a self-attention distillation network (SADNet) for effective human pose estimation, which is guided by a pre-trained teacher. Secondly, we propose a lightweight pose mapping method for human avatars that utilizes the camera model to map 2D poses to 3D avatar keypoints, generating real-time human avatars with pose consistency. Finally, we integrate our framework into a VR system, displaying generated 3D pose-driven avatars on Helmet-Mounted Display devices for an immersive user experience. We evaluate SADNet on two publicly available datasets. Experimental results show that SADNet achieves a state-of-the-art trade-off between speed and accuracy. In addition, we conducted a user experience study on the performance and immersion of virtual reality avatars. Results show that pose-driven 3D human avatars generated by our method are smooth and attractive. image
C1 [Jiang, Ling; Xiong, Yuan; Wang, Qianqian; Chen, Tong; Wu, Wei; Zhou, Zhong] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
   [Zhou, Zhong] Zhongguancun Lab, Beijing, Peoples R China.
   [Zhou, Zhong] Beihang Univ, POB 6863,37 Xueyuan Rd, Beijing, Peoples R China.
C3 Beihang University; Zhongguancun Laboratory; Beihang University
RP Zhou, Z (corresponding author), Beihang Univ, POB 6863,37 Xueyuan Rd, Beijing, Peoples R China.
EM zz@buaa.edu.cn
OI Xiong, Yuan/0000-0002-7253-4998; Zhou, Zhong/0000-0002-5825-7517
FU Natural Science Foundation of China; Transportation Science and
   Technology Program of Hainan, China [HNJTT-KXC-2024-3-22-02]; 
   [62272018]
FX This work is supported by the Natural Science Foundation of China under
   grant No. 62272018 and Transportation Science and Technology Program of
   Hainan, China under grant no. HNJTT-KXC-2024-3-22-02.
CR Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471
   Cheng B., 2020, Higherhrnet: scaleaware representation learning for bottomup human pose estimation. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition
   Cudeiro D, 2019, PROC CVPR IEEE, P10093, DOI 10.1109/CVPR.2019.01034
   Jiang L., 2023, Vis Comput
   Jiang T, 2023, Arxiv, DOI arXiv:2303.07399
   Kennedy R.S., 1993, Int. J. Aviat. Psy, P203
   Kocabas M, 2020, PROC CVPR IEEE, P5252, DOI 10.1109/CVPR42600.2020.00530
   Kornblith Simon, 2019, SIMILARITY NEURAL NE
   Li W., 2022, Mhformer: multihypothesis transformer for 3d human pose estimation. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition
   Li Y., 2021, Tokenpose: learning keypoint tokens for human pose estimation. Proceedings of the IEEE/CVF. International Conference on Computer Vision
   Li Z, 2021, PROC CVPR IEEE, P14157, DOI 10.1109/CVPR46437.2021.01394
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Live3D, 2023, VTuber
   Luo Z., 2021, Rethinking the heatmap regression for bottomup human pose estimation. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition
   Newell Alejandro., 2017, Advances in neural information processing systems, V9, P30
   Shan W., 2022, Pstmo: pretrained spatial temporal manytoone model for 3d human pose estimation. European conference on computer vision. Springer
   Shao R., 2022, Doublefield: bridging the neural surface and radiance fields for highfidelity human reconstruction and rendering. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition
   Sharma A, 2022, IEEE T NEUR SYS REH, V30, P699, DOI 10.1109/TNSRE.2022.3156884
   Simon D., 2006, Optimal State Estimation: Kalman, H-Infinity, and Nonlinear Approaches, P129
   Smith B, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417768
   Song W., 2022, Realtime expressive avatar animation generation based on monocular videos. 2022 IEEE international symposium on mixed and augmented reality adjunct (ISMARadjunct). IEEE
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Sun X., 2018, Integral human pose regression. Proceedings of the European conference on computer vision (ECCV)
   Tang MT, 2021, INT SYM MIX AUGMENT, P128, DOI 10.1109/ISMAR52148.2021.00027
   Venkataramanan S, 2023, Arxiv, DOI arXiv:2301.02240
   Wang Y., 2022, Lite pose: efficient architecture design for 2d human pose estimation. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition
   Xu YF, 2024, IEEE T PATTERN ANAL, V46, P1212, DOI 10.1109/TPAMI.2023.3330016
   Yang S., 2021, Transpose: Keypoint localization via transformer. Proceedings of the IEEE/CVF international conference on computer vision
   Yang Z., 2023, Effective wholebody pose estimation with twostages distillation. Proceedings of the IEEE/CVF. International Conference on Computer Vision
   Yu T., 2021, Function4d: realtime human volumetric capture from very sparse consumer rgbd sensors. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition
   Yuan YH, 2021, Arxiv, DOI arXiv:2110.09408
   Yukihiko A., 2022, USB Camera Motion Capture ThreeDPoseTracker Description
   Zhang Y., 2021, Lightweight multiperson total motion capture using sparse multiview cameras. Proceedings of the IEEE/CVF International Conference on Computer Vision
   Zhou K., 2019, Hemlets pose: learning partcentric heatmap triplets for accurate 3d human pose estimation. Proceedings of the IEEE/CVF international conference on computer vision
NR 34
TC 0
Z9 0
U1 0
U2 0
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAY
PY 2024
VL 35
IS 3
AR e2233
DI 10.1002/cav.2233
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SI3N8
UT WOS:001233787700001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Qi, FX
   Wang, BJ
   Wang, ML
AF Qi, Feixiang
   Wang, Bojian
   Wang, Meili
TI Graph-based control framework for motion propagation and pattern
   preservation in swarm flight simulations
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE biological behavior; control framework; flight simulation; multi-agent
   systems; topological structure
AB Simulation of swarm motion is a crucial research area in computer graphics and animation, and is widely used in a variety of applications such as biological behavior research, robotic swarm control, and the entertainment industry. In this paper, we address the challenges of preserving structural relations between the individuals in swarm flight simulations by proposing an innovative motion control framework that utilizes a graph-based hierarchy to illustrate patterns within a swarm and allows the swarm to perform flight motions along externally specified paths. In addition, this study designs motion propagation strategies with different focuses for varied application scenarios, analyzes the effects of information transfer latencies on pattern preservation under these strategies, and optimizes the control algorithms at the mathematical level. This study not only establishes a complete set of control methods for group flight simulations, but also has excellent scalability, which can be combined with other techniques in this field to provide new solutions for group behavior simulations.
   This paper introduces a novel motion control framework for swarm flight simulations that employs a graph-based hierarchy to maintain structural relationships within the swarm. It designs varied motion propagation strategies for different applications, examines the impact of information transfer delays on pattern preservation, and mathematically optimizes the control algorithms. The study provides a comprehensive set of control methods with excellent scalability that allows for integration integration with other techniques for enhanced group behavior simulations. image
C1 [Qi, Feixiang; Wang, Bojian; Wang, Meili] Northwest A&F Univ, Coll Informat Engn, 3 Taicheng Rd, Yangling, Shaanxi, Peoples R China.
C3 Northwest A&F University - China
RP Wang, ML (corresponding author), Northwest A&F Univ, Coll Informat Engn, 3 Taicheng Rd, Yangling, Shaanxi, Peoples R China.
EM wml@nwsuaf.edu.cn
OI Wang, Bojian/0009-0009-7955-5618
FU National College Students Innovation and Entrepreneurship Training
   Program;  [2023];  [202310712016]
FX The authors wish to express their gratitude to the reviewers for their
   thorough and detailed feedback, which has significantly aided in
   enhancing the scientific contributions and the overall presentation of
   this paper. This work was financially funded by 2023 National College
   Students Innovation and Entrepreneurship Training Program
   (202310712016).
CR Catmull EE, 1974, COMPUT AIDED GEOM D, P317, DOI DOI 10.1016/B978-0-12-079050-0.50020-5
   Chen Q, 2019, COMPUT ANIMAT VIRT W, V30, DOI 10.1002/cav.1902
   Chung SJ, 2018, IEEE T ROBOT, V34, P837, DOI 10.1109/TRO.2018.2857475
   DEROSE TD, 1988, ACM T GRAPHIC, V7, P1, DOI 10.1145/42188.42265
   Gonzalez LR., 2017, Proceedings of Conference on Computer Graphics Visual Computing, P81
   Gustafson S., 2016, ACM SIGGRAPH 2016 talks, P1
   He L, 2016, Arxiv, DOI arXiv:1602.03623
   Helbing D, 2005, TRANSPORT SCI, V39, P1, DOI 10.1287/trsc.1040.0108
   Henry Joseph., 2012, Proceedings of the 11th ACM SIGGRAPH / Eurographics conference on Computer Animation, EUROSCA'12, P193
   Hughes RL, 2002, TRANSPORT RES B-METH, V36, P507, DOI 10.1016/S0191-2615(01)00015-7
   Kapadia M., 2016, Virtual crowds: steps toward behavioral realism
   Khatib O., 1985, P 1985 IEEE INT C RO, P500, DOI DOI 10.1109/ROBOT.1985.1087247
   LAM NSN, 1983, AM CARTOGRAPHER, V10, P129, DOI 10.1559/152304083783914958
   Li J., 2022, Adv Contin Discrete Models, V2022, P10
   Liu YP, 2020, IEEE T VEH TECHNOL, V69, P11756, DOI 10.1109/TVT.2020.3017162
   Marton ZC, 2009, IEEE INT CONF ROBOT, P2829
   Narain R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618468
   Reynolds C.W., 1987, P 14 ANN C COMPUTE, P25, DOI [10.1145/37402.37406, DOI 10.1145/37402.37406, 10.1145/37401.37406, DOI 10.1145/37401.37406]
   Reynolds CW., 2002, GDC, V1999, P763
   Sakuma T, 2005, COMPUT ANIMAT VIRT W, V16, P343, DOI 10.1002/cav.105
   Strömbom D, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1006523
   Sung M, 2004, COMPUT GRAPH FORUM, V23, P519, DOI 10.1111/j.1467-8659.2004.00783.x
   Mathew CDT, 2019, COMPUT GRAPH FORUM, V38, P455, DOI 10.1111/cgf.13585
   Treuille A, 2006, ACM T GRAPHIC, V25, P1160, DOI 10.1145/1141911.1142008
   Vesentini F, 2024, ROBOT AUTON SYST, V174, DOI 10.1016/j.robot.2024.104645
   Walker P, 2012, IEEE SYS MAN CYBERN, P3009, DOI 10.1109/ICSMC.2012.6378253
   Wang H., 2024, Mach Intell Res, V21, P1
   Wang XJ, 2014, COMPUT GRAPH FORUM, V33, P51, DOI 10.1111/cgf.12277
   Wolfgang K., 2002, Differential geometry: Curves surfaces manifolds
   Yang SW, 2020, GRAPH MODELS, V111, DOI 10.1016/j.gmod.2020.101081
   Yang Y, 2021, IEEE COMMUN SURV TUT, V23, P815, DOI 10.1109/COMST.2021.3059998
   Yu WJ, 2005, PHYS REV E, V72, DOI 10.1103/PhysRevE.72.026112
NR 32
TC 0
Z9 0
U1 0
U2 0
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAY
PY 2024
VL 35
IS 3
AR e2276
DI 10.1002/cav.2276
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SI2B7
UT WOS:001233749400001
DA 2024-08-05
ER

PT J
AU Yang, SP
   Huang, HY
   Huang, YS
   Jin, XG
AF Yang, Sipeng
   Huang, Hongyu
   Huang, Ying Sophie
   Jin, Xiaogang
TI Facial action units detection using temporal context and feature
   reassignment
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE action units; facial AU detection; temporal context modeling
AB Facial action units (AUs) encode the activations of facial muscle groups, playing a crucial role in expression analysis and facial animation. However, current deep learning AU detection methods primarily focus on single-image analysis, which limits the exploitation of rich temporal context for robust outcomes. Moreover, the scale of available datasets remains limited, leading models trained on these datasets to tend to suffer from overfitting issues. This paper proposes a novel AU detection method integrating spatial and temporal data with inter-subject feature reassignment for accurate and robust AU predictions. Our method first extracts regional features from facial images. Then, to effectively capture both the temporal context and identity-independent features, we introduce a temporal feature combination and feature reassignment (TC&FR) module, which transforms single-image features into a cohesive temporal sequence and fuses features across multiple subjects. This transformation encourages the model to utilize identity-independent features and temporal context, thus ensuring robust prediction outcomes. Experimental results demonstrate the enhancements brought by the proposed modules and the state-of-the-art (SOTA) results achieved by our method.
   We introduce a novel automatic detection method for facial action units (AUs) that leverages both spatial and temporal data, enhancing accuracy and robustness in expression analysis and facial animation. Our approach utilizes a Temporal feature Combination and Feature Reassignment (TC&FR) module to transform and fuse features across multiple subjects and temporal sequences. Moreover, by integrating a Regional Attention (RA) encoder and a transformer model, our method refines the extraction and processing of regional features, ensuring more precise identification and analysis of AUs. This integration not only harnesses identity-independent features but also maximizes the temporal context, significantly improving the reliability of AU predictions. image
C1 [Yang, Sipeng; Jin, Xiaogang] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou, Peoples R China.
   [Yang, Sipeng; Huang, Hongyu; Jin, Xiaogang] Zhejiang Univ, Sch Comp Sci & Technol, Hangzhou, Peoples R China.
   [Huang, Ying Sophie] Zhejiang Univ, Sch Management, Hangzhou 310058, Peoples R China.
C3 Zhejiang University; Zhejiang University; Zhejiang University
RP Huang, YS (corresponding author), Zhejiang Univ, Sch Management, Hangzhou 310058, Peoples R China.
EM sophiehuangying@zju.edu.cn
OI Jin, Xiaogang/0000-0001-7339-2920
FU ZJU-Usyd Ignition [188170*194252404/002]; Key R&D Program of Zhejiang
   [2023C01047]
FX This research was supported by the ZJU-Usyd Ignition Grants
   (188170*194252404/002), Key R&D Program of Zhejiang (No. 2023C01047).
CR Alard C, 1998, ASTROPHYS J, V503, P325, DOI 10.1086/305984
   Chang Y., 2022, Knowledge-driven self-supervised representation learning for facial action unit recognition. IEEE conference on computer vision and pattern recognition
   Chu WS, 2017, IEEE INT CONF AUTOMA, P25, DOI 10.1109/FG.2017.13
   Corneanu C., 2018, Deep structure inference network for facial action unit recognition. European conference on computer vision
   Cui Z., 2023, Biomechanics-guided facial action unit detection through force Modeling. IEEE conference on computer vision and pattern recognition
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Donato G, 1999, IEEE T PATTERN ANAL, V21, P974, DOI 10.1109/34.799905
   Ekman P., 1978, Facial action coding system: A technique for the measurement of facial movement
   Fan YC, 2020, COMPUT ANIMAT VIRT W, V31, DOI 10.1002/cav.1946
   Fasel B., 2000, Recognition of asymmetric facial action unit activities and intensities. International conference on pattern recognition. 1. IEEE
   Gudi A., 2015, Deep learning based facs action unit occurrence and intensity estimation. IEEE international conference and workshops on automatic face and gesture Recognition. 6. IEEE
   Han K, 2023, IEEE T PATTERN ANAL, V45, P87, DOI 10.1109/TPAMI.2022.3152247
   He KM, 2022, PROC CVPR IEEE, P15979, DOI 10.1109/CVPR52688.2022.01553
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jacob G.M., 2021, Facial action unit detection with transformers
   Le Guen V, 2019, ADV NEUR IN, V32
   Li YQ, 2015, PATTERN RECOGN, V48, P3417, DOI 10.1016/j.patcog.2015.04.022
   Luo C, 2022, Arxiv, DOI arXiv:2205.01782
   Martinez B, 2019, IEEE T AFFECT COMPUT, V10, P325, DOI 10.1109/TAFFC.2017.2731763
   Mavadati SM, 2013, IEEE T AFFECT COMPUT, V4, P151, DOI 10.1109/T-AFFC.2013.4
   Miller P. W., 1988, NONVERBAL COMMUNICAT
   Kipf TN, 2017, Arxiv, DOI arXiv:1609.02907
   Rawal N, 2022, INT J SOC ROBOT, V14, P1583, DOI 10.1007/s12369-022-00867-0
   Shao ZW, 2022, IEEE T AFFECT COMPUT, V13, P1274, DOI 10.1109/TAFFC.2019.2948635
   Shao ZW, 2021, INT J COMPUT VISION, V129, P321, DOI 10.1007/s11263-020-01378-z
   Song T., 2021, Hybrid message passing with performance-driven structures for facial action unit detection. IEEE conference on computer vision and pattern recognition
   Song TF, 2021, AAAI CONF ARTIF INTE, V35, P5993
   Tang Y., 2021, Piap-df: pixel-interested and anti person-specific facial action unit detection net with discrete feedback learning. IEEE international conference on computer vision
   Tu YH, 2023, CONTACT DERMATITIS, V88, P188, DOI 10.1111/cod.14256
   Valstar MF., 2007, Combined support vector machines and hidden markov models for modeling facial action temporal dynamics. International Workshop on Human-Computer Interaction Springer
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Z., 2023, Spatial-temporal graph-based AU relationship learning for facial action unit detection. IEEE conference on computer vision and pattern recognition
   Zeng M, 2012, COMPUT ANIMAT VIRT W, V23, P167, DOI 10.1002/cav.1455
   Zhang X., 2021, Multi-modal learning for AU detection based on multi-head fused transformers. 2021 16th IEEE international conference on automatic face and gesture recognition IEEE
   Zhang X, 2023, Arxiv, DOI arXiv:2304.00058
   Zhang X, 2014, IMAGE VISION COMPUT, V32, P692, DOI 10.1016/j.imavis.2014.06.002
   Zhao KL, 2016, PROC CVPR IEEE, P3391, DOI 10.1109/CVPR.2016.369
NR 37
TC 0
Z9 0
U1 2
U2 2
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAY
PY 2024
VL 35
IS 3
AR e2246
DI 10.1002/cav.2246
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RF8H7
UT WOS:001226339200001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Zhang, Y
   Yang, B
   Zhu, JL
AF Zhang, Yong
   Yang, Bo
   Zhu, Jianlin
TI A double-layer crowd evacuation simulation method based on deep
   reinforcement learning
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE crowd evacuation simulation; deep reinforcement learning; path planning;
   pedestrian movement modeling
ID MODEL
AB Existing crowd evacuation simulation methods commonly face challenges of low efficiency in path planning and insufficient realism in pedestrian movement during the evacuation process. In this study, we propose a novel crowd evacuation path planning approach based on the learning curve-deep deterministic policy gradient (LC-DDPG) algorithm. The algorithm incorporates dynamic experience pool and a priority experience sampling strategy, enhancing convergence speed and achieving higher average rewards, thus efficiently enabling global path planning. Building upon this foundation, we introduce a double-layer method for crowd evacuation using deep reinforcement learning. Specifically, within each group, individuals are categorized into leaders and followers. At the top layer, we employ the LC-DDPG algorithm to perform global path planning for the leaders. Simultaneously, at the bottom layer, an enhanced social force model guides the followers to avoid obstacles and follow the leaders during evacuation. We implemented a crowd evacuation simulation platform. Experimental results show that our proposed method has high path planning efficiency and can generate more realistic pedestrian trajectories in different scenarios and crowd sizes.
   In this paper, a two-layer crowd evacuation simulation method based on deep reinforcement learning is proposed. The overall framework of the method is divided into two layers: in the global path planning layer, a deep deterministic policy gradient algorithm based on learning curves is proposed, which utilizes the learning curve theory to dynamically change the algorithm's experience pool capacity, and utilizes a prioritized experience sampling strategy to give higher sampling probabilities to experiences with learning value, thus improving the algorithm's learning efficiency; in the layer of the crowd's microscopic behaviors, an improved social force model is applied to the crowd's In the micro-behavioral layer of the crowd, the improved social force model is used to model the micro-motion of the crowd, which guides the crowd to avoid obstacles and reach the safe exit quickly. The experimental results show that the deep reinforcement learning-based two-layer crowd evacuation simulation method proposed in this paper can improve the path planning efficiency. image
C1 [Zhang, Yong; Yang, Bo; Zhu, Jianlin] South Cent Minzu Univ, Coll Comp Sci, Wuhan 430074, Peoples R China.
   [Yang, Bo; Zhu, Jianlin] South Cent Minzu Univ, State Ethn Affairs Commiss, Key Lab Cyber Phys Fus Intelligent Comp, Wuhan, Peoples R China.
C3 South Central Minzu University; South Central Minzu University
RP Yang, B (corresponding author), South Cent Minzu Univ, Coll Comp Sci, Wuhan 430074, Peoples R China.
EM yangbo@mail.scuec.edu.cn
FU National Natural Science Foundation of China; Hubei Provincial Natural
   Science Foundation of China [2022CFB469];  [61976226];  [72104254]
FX The research reported in this paper is financially supported by the
   National Natural Science Foundation of China (61976226, 72104254) and
   the Hubei Provincial Natural Science Foundation of China (No.
   2022CFB469).
CR Blue VJ, 1998, TRANSPORT RES REC, P29, DOI 10.3141/1644-04
   DUANE JT, 1964, IEEE T AEROSP, VAS 2, P563, DOI 10.1109/TA.1964.4319640
   Hong S, 2020, COMPUT-AIDED CHEM EN, V48, P1867, DOI 10.1016/B978-0-12-823377-1.50312-8
   Hou YN, 2017, IEEE SYS MAN CYBERN, P316, DOI 10.1109/SMC.2017.8122622
   Hu JY, 2020, IEEE T VEH TECHNOL, V69, P14413, DOI 10.1109/TVT.2020.3034800
   Hughes RL, 2003, ANNU REV FLUID MECH, V35, P169, DOI 10.1146/annurev.fluid.35.101101.161136
   Kirchner A, 2002, PHYSICA A, V312, P260, DOI 10.1016/S0378-4371(02)00857-9
   Lowe R., 2017, ARXIV
   Ma Y., An artificial intelligencebased approach for simulating pedestrian movement. IEEE Transactions on Intelligent Transportation Systems. 2016;17(11):112
   Mao T., Parallelizing continuum crowds.Proceedings of the 17th ACM symposium on virtual reality software and technology;2010. p.231234
   Shang M., 2021, Comput Simulat, V38, P63
   Shao X., 2024, Sustain Cities Soc
   Sharma J, 2021, IEEE T SYST MAN CY-S, V51, P7363, DOI 10.1109/TSMC.2020.2967936
   Wang F., VPBS: a velocityperceptionbased SFM approach for crowd simulation.2016 international conference on virtual reality and visualization (ICVRV).IEEE;2016. p.317324
   Wu WH, 2022, TSINGHUA SCI TECHNOL, V27, P619, DOI 10.26599/TST.2021.9010023
   Xu D, 2021, T GIS, V25, P1542, DOI 10.1111/tgis.12738
   Yan C, 2020, J INTELL ROBOT SYST, V98, P297, DOI 10.1007/s10846-019-01073-3
   Yao ZZ, 2019, NEUROCOMPUTING, V366, P314, DOI 10.1016/j.neucom.2019.08.021
NR 18
TC 0
Z9 0
U1 10
U2 10
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAY
PY 2024
VL 35
IS 3
AR e2280
DI 10.1002/cav.2280
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SN7I7
UT WOS:001235192200001
DA 2024-08-05
ER

PT J
AU Kumar, L
   Singh, DK
AF Kumar, Lalit
   Singh, Dushyant Kumar
TI Diversified realistic face image generation GAN for human subjects in
   multimedia content creation
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE deep learning; face image generation; Generative Adversarial Network;
   image generation; ResNet-50; VGG-16
AB Face image generation plays an important role in generating innovative and unique multimedia content using the GAN model. With these qualities of the GAN model, they have numerous challenges in the human face image generation. The problems encountered in the generation of facial images are like blurriness in images, incomplete details in the generated facial images, high computational power requirements, and so forth. In this manuscript, we proposed a GAN model that utilizes the composite strength of VGG-16 and ResNet-50's models to overcome those difficulties. It uses VGG-16 to build a discriminator model to discriminate between real and fake images. The generator model utilizes a combination of components from the ResNet-50 and VGG-16 models to enhance the image generation process at each iteration, resulting in the creation of realistic face images. The proposed DRFI GAN (Diversified and Realistic Face Image Generation GAN) model's generator achieves an impressive low FID score of 20.50, which is less than existing state-of-the-art approaches. Furthermore, our findings indicate that the images generated by the DRFI GAN model exhibit 10%-15% greater efficiency and realism with reduced training time compared to existing state-of-the-art methods with lower FID scores.
C1 [Kumar, Lalit; Singh, Dushyant Kumar] MNNIT Allahabad, CSED, Allahabad, India.
C3 National Institute of Technology (NIT System); Motilal Nehru National
   Institute of Technology
RP Kumar, L (corresponding author), MNNIT Allahabad, CSED, Allahabad, India.
EM lalitkmr170@gmail.com
RI Kumar, Lalit/ABX-0228-2022
OI Kumar, Lalit/0000-0002-8441-1696
CR Chang T-Y., Tinygan: distilling biggan for conditional image generation. In: Proceedings of the Asian conference on computer vision; 2020
   Creswell A, 2018, IEEE SIGNAL PROC MAG, V35, P53, DOI 10.1109/MSP.2017.2765202
   Elasri M, 2022, NEURAL PROCESS LETT, V54, P4609, DOI 10.1007/s11063-022-10777-x
   Ethayarajh K, 2019, Arxiv, DOI [arXiv:1909.00512, DOI 10.18653/V1/D19]
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Gui J, 2023, IEEE T KNOWL DATA EN, V35, P3313, DOI 10.1109/TKDE.2021.3130191
   Hensel M, 2017, ADV NEUR IN, V30
   Hori C, 2017, IEEE I CONF COMP VIS, P4203, DOI 10.1109/ICCV.2017.450
   Huang X., 2018, Lecture Notes in Computer Science, V11207
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Kingma DP, 2019, FOUND TRENDS MACH LE, V12, P4, DOI 10.1561/2200000056
   Koonce B., 2021, CONVOLUTIONAL NEURAL, P63
   Kowalski M., 2020, Computer vision-ECCV 2020: 16th European conference, Glasgow, UK, August 23-28, 2020. Proceedings
   Kumar L., 2023, Multimed Tools Appl, V82, P1
   Kumar LMA, 2023, MULTIMED TOOLS APPL, V82, P29823, DOI 10.1007/s11042-023-14977-y
   Kumar L, 2022, CYBERN INF TECHNOL, V22, P68, DOI 10.2478/cait-2022-0028
   Lee HY, 2020, INT J COMPUT VISION, V128, P2402, DOI 10.1007/s11263-019-01284-z
   Liu MC, 2021, ADV NEUR IN, V34
   Ning X, 2023, CONCURR COMP-PRACT E, V35, DOI 10.1002/cpe.6147
   Pagella F, 2020, RHINOLOGY, V58, P516, DOI 10.4193/Rhin20.078
   Pallawi S, 2023, COGN COMPUT SYST, V5, P1, DOI 10.1049/ccs2.12072
   Rusia Mayank Kumar, 2021, International Journal of Information Technology, P2419, DOI 10.1007/s41870-021-00803-x
   Rusia MK, 2023, MULTIMED TOOLS APPL, V82, P1669, DOI 10.1007/s11042-022-13248-6
   Singh DK., 2016, International Journal of Control Theory and Applications, V9, P173
   Taigman Y., Unsupervised cross-domain image generation. ArXiv preprint arXiv:1611.02200; 2016
   TERASVIRTA T, 1994, J AM STAT ASSOC, V89, P208, DOI 10.2307/2291217
   Wang ZW, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3439723
   Yang S, 2015, IEEE I CONF COMP VIS, P3676, DOI 10.1109/ICCV.2015.419
   Zhang L, 2021, FUTURE GENER COMP SY, V122, P98, DOI 10.1016/j.future.2021.03.022
   Zhang ZX, 2020, J VIS COMMUN IMAGE R, V71, DOI 10.1016/j.jvcir.2019.102719
NR 31
TC 0
Z9 0
U1 4
U2 4
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAR
PY 2024
VL 35
IS 2
AR e2232
DI 10.1002/cav.2232
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MT4N8
UT WOS:001195875200001
DA 2024-08-05
ER

PT J
AU Wen, ZH
   Guo, XY
AF Wen, Zehua
   Guo, Xiaoyang
TI Feasibility study of virtual reality in audiovisual environment:
   Assessment of university cafeteria acoustic environment
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE audiovisual environment; cafeteria; noise; questionnaire survey;
   subjective annoyance rating; virtual reality
AB This study primarily focuses on investigating whether virtual reality scenarios can authentically replicate real-life audio-visual environments. The authenticity of audio-visual environments plays a crucial role in both the design and VR fields today. Only when the authenticity of audio-visual interactive experiences is validated as feasible can virtual reality technology demonstrate positive impacts. We assessed the annoyance levels subjectively under different audio-visual conditions: a real cafeteria environment and a simulated cafeteria environment. Participants were tasked with the same activities in both environments. After each experiment, they indicated their levels of annoyance by completing a questionnaire. The results indicated a significant positive correlation between the overall subjective annoyance levels in both experiments and the subjective annoyance levels associated with different behaviors. This suggests that under identical audio conditions, virtual reality scenarios more effectively replicate the real noise environment. Furthermore, we have uncovered that certain objective factors influence the expression of authenticity. Optimizing these factors may potentially further enhance the feasibility of virtual reality technology in audio-visual environments.
   In this paper, we developed an evaluation model for audio-visual interactions in a university cafeteria to construct a mathematical model of human subjective subjective annoyance levels. The results show that there is a significant positive correlation between the overall subjective annoyance level of people and the subjective annoyance level associated with different behaviors in the real cafeteria environment and the virtual cafeteria environment, respectively. Meanwhile, we found that the realism of the audiovisual environment will be enhanced when three factors, including "the realism of the model" "the quality of the sound source" and "the degree of integration between the scene and the sound source", were optimized. image
C1 [Wen, Zehua; Guo, Xiaoyang] Suzhou Univ Sci & Technol, Sch Architecture & Urban Planning, Suzhou, Jiangsu, Peoples R China.
C3 Suzhou University of Science & Technology
RP Guo, XY (corresponding author), Suzhou Univ Sci & Technol, Sch Architecture & Urban Planning, Suzhou, Jiangsu, Peoples R China.
EM 359834709@qq.com
FU The 2023 Graduate Student Practice Innovation Program Fund of Jiangsu
   Province, China [SJCX23_1695]
FX 2023 Graduate Student Practice Innovation Program Fund of Jiangsu
   Province, China, Grant/Award Number: SJCX23_1695
CR [Anonymous], 2023, GB T 42473-2023
   Belojevic G, 2003, Noise Health, V6, P77
   BELOJEVIC G, 1992, INT ARCH OCC ENV HEA, V64, P293, DOI 10.1007/BF00378288
   Berg LP, 2017, VIRTUAL REAL-LONDON, V21, P1, DOI 10.1007/s10055-016-0293-9
   Buckley C., 2016, J Acoust Soc Am, V140, P3292
   Cassidy T., 2013, Environmental psychology: behaviour and experience in context
   Chang CJ, 2022, COMPUT ANIMAT VIRT W, V33, DOI 10.1002/cav.2076
   Chaze F, 2022, J REHABIL ASSIST TER, V9, DOI 10.1177/20556683211072384
   Choi Yoon Jung, 2007, [JOURNAL OF THE KOREAN HOUSING ASSOCIATION, 한국주거학회논문집], V18, P85
   Cui DX, 2023, COMPUT ANIMAT VIRT W, V34, DOI 10.1002/cav.2189
   El-Said OA, 2015, TOUR MANAG PERSPECT, V16, P318, DOI 10.1016/j.tmp.2015.09.006
   Evripidou E, 2023, COMPUT ANIMAT VIRT W, V34, DOI 10.1002/cav.2158
   Fowler MD, 2013, DESIGN STUD, V34, P111, DOI 10.1016/j.destud.2012.06.001
   Gilbert SB, 2016, PRESENCE-TELEOP VIRT, V25, P322, DOI 10.1162/PRES_a_00276
   Heung VCS, 2012, INT J HOSP MANAG, V31, P1167, DOI 10.1016/j.ijhm.2012.02.004
   Hong JY, 2019, BUILD ENVIRON, V149, P1, DOI 10.1016/j.buildenv.2018.12.004
   Iachini T, 2012, APPL COGNITIVE PSYCH, V26, P757, DOI 10.1002/acp.2856
   ISO 22955, 2021, Acoustics-acoustic quality of open office spaces. International Organization for Standardization
   Jo HI, 2022, APPL ACOUST, V186, DOI 10.1016/j.apacoust.2021.108498
   Kang J, 2002, APPL ACOUST, V63, P1315, DOI 10.1016/S0003-682X(02)00045-2
   LaRocco M, 2020, J VIS CULT, V19, P96, DOI 10.1177/1470412920906255
   Luigi M, 2015, ENRGY PROCED, V78, P471, DOI 10.1016/j.egypro.2015.11.703
   Martin V, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12010348
   McMahan RP., Exploring the effects of higher-fidelity display and interaction for virtual reality games (Doctoral dissertation, Virginia Tech)
   Molina E, 2020, 2020 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND VIRTUAL REALITY (AIVR 2020), P222, DOI 10.1109/AIVR50618.2020.00046
   Nakamura L, 2020, J VIS CULT, V19, P47, DOI 10.1177/1470412920906259
   Olaosun A. O., 2009, Research Journal of Medical Sciences, V3, P115
   Silva C, 2021, HUM FACTORS, V63, P1012, DOI 10.1177/0018720820920429
   SImsek KY., 2012, Pamukkale J Sport Sci, V3, P14
   Trivedi H, 2023, COMPUT ANIMAT VIRT W, V34, DOI 10.1002/cav.2169
   Visconti A, 2023, COMPUT ANIMAT VIRT W, V34, DOI 10.1002/cav.2188
   Wang S, 2010, 2010 18TH INTERNATIONAL CONFERENCE ON GEOINFORMATICS
   Wang Y., 2021, Archit Technol, V10, P1183
   Xi C., Evaluation and prediction of acoustic environment in large-scale restaurant space (Doctoral dissertation, Harbin Institute of Technology)
   Xiaolin Z., 2013, Electr Technol, V2, P4
   Xu CY, 2021, ACOUSTICS-BASEL, V3, P11, DOI 10.3390/acoustics3010003
   Yan Xiaozhen, 2011, 2011 Cross Strait Quad-Regional Radio Science and Wireless Technology Conference, P960, DOI 10.1109/CSQRWC.2011.6037116
   Yazli NC, 2022, COMPUT ANIMAT VIRT W, V33, DOI 10.1002/cav.2075
   Zhang B., 2023, Comput Anim Virtual Worlds, V34, DOI [10.1002/cav.2158, DOI 10.1002/CAV.2158]
NR 39
TC 0
Z9 0
U1 1
U2 1
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAR
PY 2024
VL 35
IS 2
AR e2231
DI 10.1002/cav.2231
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MS7N7
UT WOS:001195689200001
DA 2024-08-05
ER

PT J
AU Jeong, H
   Kim, SW
   Lee, J
   Um, K
   Kee, MH
   Han, J
AF Jeong, Heejo
   Kim, Seung-wook
   Lee, JaeHyun
   Um, Kiwon
   Kee, Min Hyung
   Han, JungHyun
TI Momentum-preserving inversion alleviation for elastic material
   simulation
SO COMPUTER ANIMATION AND VIRTUAL WORLDS
LA English
DT Article
DE elasticity; inversion; optimization; physically-based simulation
AB This paper proposes a novel method that enhances the optimization-based elastic body solver. The proposed method tackles the element inversion problem, which is prevalent in the prediction-projection approach for numerical simulation of elastic bodies. At the prediction stage, our method alleviates inversions such that the subsequent projection solver can benefit in stability and efficiency. To prevent excessive suppression of predicted inertial motion when alleviating, we introduce a velocity decomposition method and adapt only the non-rigid motion while preserving the rigid motion, that is, linear and angular momenta. Thanks to the respected inertial motion in the prediction stage, our method produces lively motions while keeping the entire simulation more stable. The experiments demonstrate that our alleviation method successfully stabilizes the simulation and improves the efficiency particularly when large deformations hamper the solver.
   In this paper, we propose a novel method that enhances the prediction-projection approach involved in the optimization-based elastic body solver. At the prediction stage, our method alleviates inversions, allowing the subsequent projection solver to benefit from improved stability and efficiency. image
C1 [Jeong, Heejo; Lee, JaeHyun; Kee, Min Hyung; Han, JungHyun] Korea Univ, Dept Comp Sci & Engn, Seoul, South Korea.
   [Kim, Seung-wook] Hankuk Univ Foreign Studies, Div Comp Engn, Seoul, South Korea.
   [Um, Kiwon] LTCI, Telecom Paris, IP Paris, Paris, France.
C3 Korea University; Hankuk University Foreign Studies; IMT - Institut
   Mines-Telecom; Institut Polytechnique de Paris; Telecom Paris
RP Han, J (corresponding author), Korea Univ, Dept Comp Sci & Engn, Seoul, South Korea.
EM jhan@korea.ac.kr
FU Ministry of Science and ICT, South Korea [IITP-2024-2020-0-01819];
   Ministry of Science and ICT, Korea, under the ICT Creative Consilience
   Program [IITP-2024-2020-0-01460, 2020-0-00861]; ITRC (Information
   Technology Research Center); Hankuk University of Foreign Studies
   Research Fund
FX This research was supported by the Ministry of Science and ICT, Korea,
   under the ICT Creative Consilience Program (IITP-2024-2020-0-01819),
   ITRC (Information Technology Research Center) Support Program
   (IITP-2024-2020-0-01460) and the grant no. 2020-0-00861. This research
   was also supported by Hankuk University of Foreign Studies Research Fund
   (of 2024).
CR Baraff D., 1994, Fast contact force computation for nonpenetrating rigid bodies. Proceedings of the 21st annual conference on computer graphics and interactive techniques
   Bonet J, 2008, NONLINEAR CONTINUUM MECHANICS FOR FINITE ELEMENT ANALYSIS, 2ND EDITION, P1, DOI 10.1017/CBO9780511755446
   Bouaziz S, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601116
   Dinev D, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3153420
   Gao Z., 2009, Semi-automated soft-tissue acquisition and modeling for surgical simulation. 2009 IEEE international conference on automation science and engineering, IEEE
   Gast TF, 2015, IEEE T VIS COMPUT GR, V21, P1103, DOI 10.1109/TVCG.2015.2459687
   Huang QX, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766890
   Irving Geoffrey., 2004, INVERTIBLE FINITE EL
   Kee MH, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459878
   Lee J, 2023, COMPUT ANIMAT VIRT W, V34, DOI 10.1002/cav.2183
   LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116
   Liu TT, 2017, ACM T GRAPHIC, V36, DOI 10.1145/2990496
   Liu TT, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508406
   Macklin M., 2016, Xpbd: position-based simulation of compliant constrained dynamics. Proceedings of the 9th international conference on motion in games
   Macklin M, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461984
   Martin Sebastian., 2011, Example-based elastic materials
   Narain R., 2016, $$ \supseteq $$ projective dynamics: fast simulation of general constitutive models. Symposium on computer animation
   Overby M, 2017, IEEE T VIS COMPUT GR, V23, P2222, DOI 10.1109/TVCG.2017.2730875
   Sin F., 2011, Invertible isotropic hyperelasticity using svd gradients. ACM SIGGRAPH/Eurographics symposium on computer animation (posters)
   Smith Breannan, 2018, ACM Transactions on Graphics, V37, DOI 10.1145/3180491
   Teran Joseph., 2005, ROBUST QUASISTATIC F
   Terzopoulos D., 1987, Elastically deformable models. Proceedings of the 14th annual conference on computer graphics and interactive techniques
NR 22
TC 0
Z9 0
U1 0
U2 0
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1546-4261
EI 1546-427X
J9 COMPUT ANIMAT VIRT W
JI Comput. Animat. Virtual Worlds
PD MAY
PY 2024
VL 35
IS 3
AR e2249
DI 10.1002/cav.2249
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RF3G6
UT WOS:001226207800001
DA 2024-08-05
ER

EF