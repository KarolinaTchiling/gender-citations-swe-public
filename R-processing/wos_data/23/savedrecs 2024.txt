FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Alanazi, T
   Babutain, K
   Muhammad, G
AF Alanazi, Thamer
   Babutain, Khalid
   Muhammad, Ghulam
TI Mitigating human fall injuries: A novel system utilizing 3D 4-stream
   convolutional neural networks and image fusion
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Human fall detection; 3D CNN; Multi-stream CNN; Autoencoders; Support
   vector machines
ID VIDEOS
AB Unintentional human falls, especially in seniors, lead to serious injuries, fatalities, and reduced standard of life. Vision-based fall detection methods have demonstrated their usefulness in timely fall response, helping to lessen such injuries. This paper presents an automated vision-based fall detection system that triggers immediate fall reporting. By incorporating human segmentation and image fusion in the pre-processing stage, the system enhances the accuracy of human action classification, thereby ensuring precise fall alerts. It further employs the innovative 4-stream 3D convolutional neural network (4S-3DCNN) model to learn different but consecutive spatial and temporal features. The system processes video input or live surveillance, segmenting human presence every 32 frames using a fine-tuned deep-learning model and applying a three-level image fusion to accentuate movement differences. This technique produces four pre-processed images, input to the 4S-3DCNN model for classification. Consecutive detection of "Falling" and "Fallen" actions triggers an alert for immediate intervention. The original 4S-3DCNN model is an end-to-end trained deep learning model with a fully connected layer serving as a classifier. The research also evaluates the performance of combining the 4S-3DCNN model with Autoencoders and Support Vector Machines (SVM) networks as classifiers. The SVM classifier demonstrated ideal fall detection performance with 100% accuracy using the MCFD, URFD, and Le2i FDD datasets. The proposed system is vital for detecting and preventing falls and reducing healthcare expenses and productivity losses.
C1 [Alanazi, Thamer; Muhammad, Ghulam] King Saud Univ, Coll Comp & Informat Sci, Dept Comp Engn, Riyadh 11543, Saudi Arabia.
   [Babutain, Khalid] King Saud Univ, Coll Comp & Informat Sci, Dept Comp Sci, Riyadh 11543, Saudi Arabia.
C3 King Saud University; King Saud University
RP Muhammad, G (corresponding author), King Saud Univ, Coll Comp & Informat Sci, Dept Comp Engn, Riyadh 11543, Saudi Arabia.
EM 436106995@student.ksu.edu.sa; ghulam@ksu.edu.sa
FU King Saud University, Riyadh, Saudi Arabia [RSP2024R34]
FX The work was supported by the Researchers Supporting Project number
   (RSP2024R34), King Saud University, Riyadh, Saudi Arabia.
CR Al Jowair H, 2023, IMAGE VISION COMPUT, V137, DOI 10.1016/j.imavis.2023.104767
   Alanazi T, 2023, APPL SCI-BASEL, V13, DOI 10.3390/app13126916
   Alanazi T, 2022, DIAGNOSTICS, V12, DOI 10.3390/diagnostics12123060
   Alaoui A.Y., 2017, Human fall detection using Von Mises distribution and motion vectors of interest points
   Alaoui AY, 2021, J IMAGING, V7, DOI 10.3390/jimaging7070109
   Alaoui AY, 2019, IEEE ACCESS, V7, P154786, DOI 10.1109/ACCESS.2019.2946522
   Alshehri F, 2023, IMAGE VISION COMPUT, V140, DOI 10.1016/j.imavis.2023.104865
   Auvinet C.R.E., 2010, Multiple cameras fall dataset
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Benezeth Y, 2010, INT J SOC ROBOT, V2, P41, DOI 10.1007/s12369-009-0040-4
   Berlin SJ, 2021, J AMB INTEL HUM COMP, DOI 10.1007/s12652-021-03250-5
   Carneiro SA, 2019, INT CONF SYST SIGNAL, P293, DOI [10.1109/iwssip.2019.8787213, 10.1109/IWSSIP.2019.8787213]
   Chamle M, 2016, 2016 INTERNATIONAL CONFERENCE ON INVENTIVE COMPUTATION TECHNOLOGIES (ICICT), VOL 2, P433
   Charfi I, 2013, J ELECTRON IMAGING, V22, DOI 10.1117/1.JEI.22.4.041106
   Chhetri S, 2021, COMPUT INTELL-US, V37, P578, DOI 10.1111/coin.12428
   Fan KB, 2019, MULTIMED TOOLS APPL, V78, P9101, DOI 10.1007/s11042-018-5638-9
   Fan YX, 2017, NEUROCOMPUTING, V260, P43, DOI 10.1016/j.neucom.2017.02.082
   Girshick R, 2016, IEEE T PATTERN ANAL, V38, P142, DOI 10.1109/TPAMI.2015.2437384
   Gonzalez R.C., 2018, Digital Image Processing
   Gruosso M, 2021, MULTIMED TOOLS APPL, V80, P1175, DOI 10.1007/s11042-020-09425-0
   Gu CH, 2018, PROC CVPR IEEE, P6047, DOI 10.1109/CVPR.2018.00633
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Islam MM, 2023, INFORM FUSION, V94, P17, DOI 10.1016/j.inffus.2023.01.015
   Islam MM, 2022, COMPUT BIOL MED, V149, DOI 10.1016/j.compbiomed.2022.106060
   Kong YQ, 2019, J VIS COMMUN IMAGE R, V59, P215, DOI 10.1016/j.jvcir.2019.01.024
   Kwolek B, 2014, COMPUT METH PROG BIO, V117, P489, DOI 10.1016/j.cmpb.2014.09.005
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu N, 2019, IEEE J BIOMED HEALTH, V23, P314, DOI 10.1109/JBHI.2018.2808281
   Min WD, 2018, IEEE ACCESS, V6, P9324, DOI 10.1109/ACCESS.2018.2795239
   Mubashir M, 2013, NEUROCOMPUTING, V100, P144, DOI 10.1016/j.neucom.2011.09.037
   Muhammad G, 2021, INFORM FUSION, V76, P355, DOI 10.1016/j.inffus.2021.06.007
   Muhammad G, 2021, INFORM FUSION, V72, P80, DOI 10.1016/j.inffus.2021.02.013
   Nooruddin S, 2023, INFORM FUSION, V100, DOI 10.1016/j.inffus.2023.101953
   Núñez-Marcos A, 2017, WIREL COMMUN MOB COM, DOI 10.1155/2017/9474806
   Pathak Ajeet Ram, 2018, Procedia Computer Science, V132, P1706, DOI 10.1016/j.procs.2018.05.144
   Peng XJ, 2016, LECT NOTES COMPUT SC, V9908, P744, DOI 10.1007/978-3-319-46493-0_45
   Poonsri A, 2018, PROC INT WORKSH ADV
   San-Segundo R, 2016, IEEE INSTRU MEAS MAG, V19, P27, DOI 10.1109/MIM.2016.7777649
   Shieh WY, 2012, MED ENG PHYS, V34, P954, DOI 10.1016/j.medengphy.2011.10.016
   Shorten C, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0197-0
   Soni PK, 2022, IMAGE VISION COMPUT, V122, DOI 10.1016/j.imavis.2022.104431
   Umer M, 2024, IMAGE VISION COMPUT, V145, DOI 10.1016/j.imavis.2024.104992
   Vieira Leite Guilherme, 2021, Deep Learning Applications. Advances in Intelligent Systems and Computing (AISC 1232), P49, DOI 10.1007/978-981-15-6759-9_3
   Vishnu C, 2021, IEEE SENS J, V21, P17162, DOI 10.1109/JSEN.2021.3082180
   W. H. Organization, 2021, Falls
   Wang SK, 2016, MULTIMED TOOLS APPL, V75, P11603, DOI 10.1007/s11042-015-2698-y
   Zhang ZM, 2019, IEEE ACCESS, V7, P4135, DOI 10.1109/ACCESS.2018.2887144
   Zou S, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10080898
NR 48
TC 0
Z9 0
U1 2
U2 2
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD AUG
PY 2024
VL 148
AR 105153
DI 10.1016/j.imavis.2024.105153
EA JUN 2024
PG 11
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA YD2F4
UT WOS:001266476700001
DA 2024-08-05
ER

PT J
AU Wang, W
   Meng, YZ
   Li, S
   Zhang, CH
AF Wang, Wei
   Meng, Yuanze
   Li, Shun
   Zhang, Chenghong
TI HV-YOLOv8 by HDPconv: Better lightweight detectors for small object
   detection
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE HDPConv; HV-YOLOv8; Small object detection; Lightweighting
AB Accurately identifying and localising small objects within images or videos is a critical challenge in the field of computer vision. It is mostly applied in scenarios that require high real-time performance, such as pedestrian detection and autonomous driving scenarios. These tiny targets generally include small objects at long distances, or objects appearing in low-resolution images, due to which it becomes exceptionally difficult to extract effective feature information. Since YOLOv8 with its large downsampling multiplier leads to deeper feature maps that make it difficult to detect tiny objects, we find that the use of residual structures in the convolution module can enhance the accuracy of small object detection. However, this undoubtedly increases the computational cost, so we lightened the convolution module to make it more suitable for practical applications and named it Halved Deep Pointwise Convolution (HDPConv). A cross-level partial module Variety of View Group Shuffle Cross Stage Partial Network (VOV-GSCSP) is also utilised, using a rational architecture as well as multi-scale information fusion, to ensure that the overall model is lightweight while obtaining rich gradient flows. On this basis, we propose a new network lightweight model HV-YOLOv8. In multiple sets of comparative experiments on two datasets (containing several state-of-the-art solutions as well as classical ones), we demonstrate the superiority of HV-YOLOv8, in particular, the accuracy is improved by 1.4% compared to YOLOv8, while the number of parameters and the amount of computation are drastically reduced.
C1 [Wang, Wei; Meng, Yuanze; Li, Shun; Zhang, Chenghong] Chinese Acad Sci, Shenyang Inst Comp Technol, Donghu St, Shenyang 110168, Peoples R China.
   [Wang, Wei; Meng, Yuanze; Li, Shun; Zhang, Chenghong] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Wang, Wei] Northeastern Univ, Sch Comp Sci & Engn, Shenyang 110169, Peoples R China.
C3 Chinese Academy of Sciences; Chinese Academy of Sciences; University of
   Chinese Academy of Sciences, CAS; Northeastern University - China
RP Meng, YZ (corresponding author), Chinese Acad Sci, Shenyang Inst Comp Technol, Donghu St, Shenyang 110168, Peoples R China.
EM mengyuanze22@mails.ucas.ac.cn
FU The 2022 Shenyang Science and Technology Plan'Jie Bang Gua Shuai ' Key
   Core Technology Tackling Project (Shenyang Bureau of Science and
   Technology) [22-316-1-10]
FX This research was funded by the 2022 Shenyang Science and Technology
   Plan'Jie Bang Gua Shuai ' Key Core Technology Tackling Project
   (22-316-1-10, Shenyang Bureau of Science and Technology) .
CR Bhartiya K., 2019, Swimming Pool and Car Detection
   Bosquet B, 2023, PATTERN RECOGN, V133, DOI 10.1016/j.patcog.2022.108998
   Chen CR, 2019, IEEE INT CONF COMP V, P100, DOI 10.1109/ICCVW.2019.00018
   Chen HT, 2023, Arxiv, DOI arXiv:2305.12972
   Chen JR, 2023, PROC CVPR IEEE, P12021, DOI 10.1109/CVPR52729.2023.01157
   Chen JR, 2022, PROC CVPR IEEE, P12538, DOI 10.1109/CVPR52688.2022.01222
   Cheng G, 2023, IEEE T PATTERN ANAL, V45, P13467, DOI 10.1109/TPAMI.2023.3290594
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Ding XH, 2021, PROC CVPR IEEE, P13728, DOI 10.1109/CVPR46437.2021.01352
   Everingham M., 2012, PASCAL VISUAL OBJECT
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fu JM, 2021, IEEE T GEOSCI REMOTE, V59, P1331, DOI 10.1109/TGRS.2020.3005151
   Graham B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12239, DOI 10.1109/ICCV48922.2021.01204
   Han K, 2020, Arxiv, DOI arXiv:2010.14819
   Han K, 2020, PROC CVPR IEEE, P1577, DOI 10.1109/CVPR42600.2020.00165
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Huang G, 2018, PROC CVPR IEEE, P2752, DOI 10.1109/CVPR.2018.00291
   Huang HJ, 2023, Arxiv, DOI [arXiv:2306.05196, 10.48550/arXiv.2306.05196]
   Jiao JY, 2023, IEEE T MULTIMEDIA, V25, P8906, DOI 10.1109/TMM.2023.3243616
   Jocher G., 2023, Ultralytics YOLO
   Li C., 2022, INT C LEARN REPR, P1
   Li C, 2022, Arxiv, DOI arXiv:2209.07947
   Li D, 2021, PROC CVPR IEEE, P12316, DOI 10.1109/CVPR46437.2021.01214
   Li HL, 2022, Arxiv, DOI [arXiv:2206.02424, DOI 10.48550/ARXIV.2206.02424]
   Liu B., 2023, J. Theory Pract. Eng. Sci., V3, P36, DOI DOI 10.53469/JTPES.2023.03(12).06
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu Z, 2022, PROC CVPR IEEE, P11999, DOI 10.1109/CVPR52688.2022.01170
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lu XC, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3052575
   Najibi M, 2019, IEEE I CONF COMP VIS, P9744, DOI 10.1109/ICCV.2019.00984
   Qi GQ, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14020420
   Qiao SY, 2021, PROC CVPR IEEE, P10208, DOI 10.1109/CVPR46437.2021.01008
   Rao YM, 2022, Arxiv, DOI arXiv:2207.14284
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Singh B, 2018, Arxiv, DOI arXiv:1805.09300
   Singh B, 2022, IEEE T PATTERN ANAL, V44, P3749, DOI 10.1109/TPAMI.2021.3058945
   Steiner A., 2022, Trans. Mach. Learn. Res., V2021
   Tan MX, 2020, Arxiv, DOI arXiv:1905.11946
   Tan MX, 2019, PROC CVPR IEEE, P2815, DOI [arXiv:1807.11626, 10.1109/CVPR.2019.00293]
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Vasu PKA, 2023, PROC CVPR IEEE, P7907, DOI 10.1109/CVPR52729.2023.00764
   Wang A, 2024, Arxiv, DOI [arXiv:2307.09283, DOI 10.48550/ARXIV.2307.09283]
   Wang JQ, 2019, IEEE I CONF COMP VIS, P3007, DOI 10.1109/ICCV.2019.00310
   Woo S, 2018, Arxiv, DOI [arXiv:1807.06521, DOI 10.48550/ARXIV.1807.06521]
   Yuan X, 2023, IEEE I CONF COMP VIS, P6294, DOI 10.1109/ICCV51070.2023.00581
   Zou ZX, 2023, P IEEE, V111, P257, DOI 10.1109/JPROC.2023.3238524
NR 46
TC 0
Z9 0
U1 47
U2 47
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUL
PY 2024
VL 147
AR 105052
DI 10.1016/j.imavis.2024.105052
EA MAY 2024
PG 10
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA TK2U4
UT WOS:001241099300001
DA 2024-08-05
ER

PT J
AU Tian, Q
   Zhao, Y
   Wu, WYC
   Sun, JX
AF Tian, Qing
   Zhao, Yi
   Wu, Wangyuchen
   Sun, Jixin
TI Enhancing open-set domain adaptation through unknown-filtering
   multi-classifier adversarial network
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Open-set domain adaptation; Adversarial learning; Multi-classifiers
AB Domain adaptation is a fundamental research problem that aims to address the domain shift issue during the transfer of a model from a labeled source domain to an unlabeled target domain. In traditional domain adaptation scenarios, it is assumed that the class spaces of both the source and target domains are identical. However, real-world applications often entail situations where the target domain contains private classes that are absent in the source domain. Forcing the alignment of these two domains may result in negative transfer. This specific concern is addressed by the emerging field of Open-set Domain Adaptation (OSDA). Previous OSDA methods attempted to align the known classes between the source and target domains while separating the unknown samples. However, these methods are inadequate in effectively discerning instances from the unknown classes. Therefore, we propose Enhancing Open-Set Domain Adaptation through Unknown-Filtering Multi-Classifier Adversarial Network (UFMCAN), which leverages multiple classifiers including a weighted auxiliary classifier, an open-set recognizer, a primary classifier and a three-way domain discriminator through adversarial learning to effectively filter instances from the unknown classes present in the target domain while concurrently aligning the source and target-known distribution. Experiments on extensive benchmarks (Office-31, Office-Home, VisDA2017 and Office-Mix) demonstrate the superior performance of UFMCAN compared to existing state-of-the-art methods.
C1 [Tian, Qing; Zhao, Yi; Wu, Wangyuchen; Sun, Jixin] Nanjing Univ Informat Sci & Technol, Sch Software, Nanjing, Peoples R China.
   [Tian, Qing] Nanjing Univ Informat Sci & Technol, Wuxi Inst Technol, Wuxi, Peoples R China.
   [Tian, Qing] Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China.
C3 Nanjing University of Information Science & Technology; Wuxi Institute
   of Technology; Nanjing University
RP Tian, Q (corresponding author), Nanjing Univ Informat Sci & Technol, Sch Software, Nanjing, Peoples R China.
EM tianqing@nuist.edu.cn; zhaoyi@nuist.edu.cn; 202212200021@nuist.edu.cn;
   sun_jx@nuist.edu.cn
FU National Natural Science Foundation of China [62176128]; Natural Science
   Foundation of Jiangsu Province [BK20231143]; Open Projects Program of
   State Key Laboratory for Novel Software Technology of Nanjing University
   [KFKT2022B06]; Fundamental Research Funds for the Central Universities
   [NJ2022028]; Priority Academic Program Development of Jiangsu Higher Ed-
   ucation Institutions (PAPD) fund; Qing Lan Project of Jiangsu Province
FX This work was supported by the National Natural Science Foundation of
   China under Grant 62176128, the Natural Science Foundation of Jiangsu
   Province under Grant BK20231143, the Open Projects Program of State Key
   Laboratory for Novel Software Technology of Nanjing University under
   Grant KFKT2022B06, the Fundamental Research Funds for the Central
   Universities No. NJ2022028, the Project Funded by the Priority Academic
   Program Development of Jiangsu Higher Ed- ucation Institutions (PAPD)
   fund, as well as the Qing Lan Project of Jiangsu Province.
CR Bucci S., 2024, P EUR C COMP VIS, P422, DOI [10.1007/978-3-030-58517-4\_25, DOI 10.1007/978-3-030-58517-4]
   Chen Y, 2018, PROC CVPR IEEE, P3339, DOI 10.1109/CVPR.2018.00352
   De Boer PT, 2005, ANN OPER RES, V134, P19, DOI 10.1007/s10479-005-5724-z
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Ganin Y, 2016, J MACH LEARN RES, V17
   Grandvalet Y., 2024, Advances in Neural Information Processing Systems, V17
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hoyer L, 2023, PROC CVPR IEEE, P11721, DOI 10.1109/CVPR52729.2023.01128
   Jang J., 2024, Advances in Neural Information Processing Systems, V35, P16755
   Li GR, 2021, PROC CVPR IEEE, P9752, DOI 10.1109/CVPR46437.2021.00963
   Li J, 2023, IEEE T CIRC SYST VID, V33, P5133, DOI 10.1109/TCSVT.2023.3249200
   Li WY, 2023, PROC CVPR IEEE, P24110, DOI 10.1109/CVPR52729.2023.02309
   Li YY, 2023, KNOWL-BASED SYST, V272, DOI 10.1016/j.knosys.2023.110600
   Liu H, 2019, PROC CVPR IEEE, P2922, DOI 10.1109/CVPR.2019.00304
   Long M., 2024, Advances in Neural Information Processing Systems, V31
   Long MS, 2017, PR MACH LEARN RES, V70
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Luo Y., 2024, INT C MACH LEARN PML, P6468
   Pei ZY, 2018, AAAI CONF ARTIF INTE, P3934
   Peng XC, 2017, Arxiv, DOI arXiv:1710.06924
   Prabhu V., 2024, P IEEE CVF INT C COM, P8558
   Rangwani H., 2024, INT C MACH LEARN PML, P18378
   Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16
   Saito K., 2024, Advances in Neural Information Processing Systems, V33, P16282
   Saito K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8980, DOI 10.1109/ICCV48922.2021.00887
   Saito K, 2018, LECT NOTES COMPUT SC, V11209, P156, DOI 10.1007/978-3-030-01228-1_10
   Shermin T, 2021, IEEE T MULTIMEDIA, V23, P2732, DOI 10.1109/TMM.2020.3016126
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun T, 2022, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR52688.2022.00705
   Tian Q, 2023, ACM T INTEL SYST TEC, V14, DOI 10.1145/3570510
   Tian Q, 2022, IEEE T CIRC SYST VID, V32, P8562, DOI 10.1109/TCSVT.2022.3192135
   Tian Q, 2022, IEEE T CYBERNETICS, V52, P10328, DOI 10.1109/TCYB.2021.3070545
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572
   Wang SS, 2020, KNOWL-BASED SYST, V204, DOI 10.1016/j.knosys.2020.106258
   Wang Y., 2023, Image Vis. Comput.
   Wu AM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9322, DOI 10.1109/ICCV48922.2021.00921
   Xu TK, 2022, Arxiv, DOI arXiv:2109.06165
   Zhao Y, 2023, IMAGE VISION COMPUT, V129, DOI 10.1016/j.imavis.2022.104607
   Zhao YY, 2022, IEEE T CIRC SYST VID, V32, P7019, DOI 10.1109/TCSVT.2022.3179021
   Zhe X, 2023, IMAGE VISION COMPUT, V135, DOI 10.1016/j.imavis.2023.104695
   Zhu JJ, 2023, PROC CVPR IEEE, P3561, DOI 10.1109/CVPR52729.2023.00347
   Zhu YC, 2021, IEEE T NEUR NET LEAR, V32, P1713, DOI 10.1109/TNNLS.2020.2988928
NR 44
TC 1
Z9 1
U1 4
U2 4
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD MAY
PY 2024
VL 145
AR 104993
DI 10.1016/j.imavis.2024.104993
EA MAR 2024
PG 11
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA PQ3W7
UT WOS:001215518700001
DA 2024-08-05
ER

PT J
AU Kong, WH
   Yu, ZP
   Li, H
   Tong, LA
   Zhao, FD
   Li, Y
AF Kong, Weihang
   Yu, Zepeng
   Li, He
   Tong, Liangang
   Zhao, Fengda
   Li, Yang
TI CrowdAlign: Shared-weight dual-level alignment fusion for RGB-T crowd
   counting
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Cross -modal crowd counting; Feature fusion; Feature alignment;
   Dual-level spatial-semantic feature; Low-frequency wavelet filtering
AB The combination of visible and thermal images has been proven to be effective in improving accuracy for crowd counting in illumination-unconstrained scenes. However, the challenging problem of misalignment in RGB-T image pairs has not been extensively explored in this context. This study aims to address the issue of misalignment between RGB and thermal image pairs to enhance the counting accuracy of cross -modal models. Specifically, we propose CrowdAlign, a cross -modal feature alignment fusion network that utilizes a sharedweight strategy for efficient feature extraction. Additionally, CrowdAlign addresses alignment adjustments through two stages: pre -fusion and post -fusion alignment. For pre -fusion feature extraction, we design a duallevel spatial-semantic parallel alignment module, while for post -fusion feature extraction, a low-frequency feature attention filtering module is developed. This two-stage alignment approach enables cross -modal feature alignment without requiring additional supervision. Experiments on the public benchmarks demonstrate that our method is effective under RGB-T misalignment or dark conditions. We hope CrowdAlign will inspire researchers to focus on and explore the issue of misalignment between RGB image and thermal image for cross -modal crowd counting.
C1 [Kong, Weihang; Yu, Zepeng; Li, He; Tong, Liangang; Zhao, Fengda] Yanshan Univ, Sch Informat Sci & Engn, Qinhuangdao 066004, Peoples R China.
   [Kong, Weihang; Li, He] Key Lab Comp Virtual Technol & Syst Integrat Hebei, Qinhuangdao 066004, Peoples R China.
   [Tong, Liangang] Yanshan Univ, Informat Technol Ctr, Engn Res Ctr Rolling Equipment & Complete Technol, Qinhuangdao 066004, Peoples R China.
   [Li, He; Zhao, Fengda] Hebei Key Lab Software Engn, Qinhuangdao 066004, Peoples R China.
   [Li, Yang] SINOPEC, Shengli Oilfield Co, Reservoir Performance Monitoring Ctr, Dongying 257000, Peoples R China.
C3 Yanshan University; Yanshan University; Sinopec
RP Li, H (corresponding author), Yanshan Univ, Sch Informat Sci & Engn, Qinhuangdao 066004, Peoples R China.
EM whkong@ysu.edu.cn; yzp@stumail.ysu.edu.cn; lihe_ysu@163.com;
   tlg@ysu.edu.cn; zfd@ysu.edu.cn
FU National Natural Science Foundation of China [62173290, 62306264];
   Central Government Guided Local Funds for Science and Technology
   Development of China [236Z0303G]; Natural Science Foundation of Hebei
   Province in China [F2022203008, F2024203091]; Natural Science Foundation
   of Xinjiang Uygur Autonomous Region in China [2022D01A59]; Science and
   Technology Project of Hebei Education Department [QN2023189]; Yanshan
   University [2022LGQN007]; Key Laboratory for Software Engineering of
   Hebei Province [KF2302]; Innovation Capability Improvement Plan Project
   of Hebei Province [22567626H]
FX This work was supported partly by the National Natural Science
   Foundation of China (No. 62173290, 62306264) , the Central Government
   Guided Local Funds for Science and Technology Development of China (No.
   236Z0303G) , the Natural Science Foundation of Hebei Province in China
   (No. F2022203008, F2024203091) , the Natural Science Foundation of
   Xinjiang Uygur Autonomous Region in China (No. 2022D01A59) , Science and
   Technology Project of Hebei Education Department (No. QN2023189) ,
   Yanshan University (No. 2022LGQN007) , Key Laboratory for Software
   Engineering of Hebei Province (No. KF2302) and Innovation Capability
   Improvement Plan Project of Hebei Province (No. 22567626H) .
CR Binyu Zhang, 2021, 2021 7th IEEE International Conference on Network Intelligence and Digital Content (IC-NIDC), P117, DOI 10.1109/IC-NIDC54101.2021.9660586
   Cao ZJ, 2020, IMAGE VISION COMPUT, V104, DOI 10.1016/j.imavis.2020.104026
   Chen YH, 2023, IEEE T CIRC SYST VID, V33, P1055, DOI 10.1109/TCSVT.2022.3208714
   Deng-Ping Fan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P275, DOI 10.1007/978-3-030-58610-2_17
   Di Wang J.L., 2022, P 31 INT JOINT C ART, P3508
   Du GD, 2024, IEEE T CIRC SYST VID, V34, P2361, DOI 10.1109/TCSVT.2023.3309647
   Eyiokur FI, 2023, IMAGE VISION COMPUT, V130, DOI 10.1016/j.imavis.2022.104610
   Gu SQ, 2023, IMAGE VISION COMPUT, V131, DOI 10.1016/j.imavis.2023.104631
   Hwang S, 2015, PROC CVPR IEEE, P1037, DOI 10.1109/CVPR.2015.7298706
   Khan MA, 2023, IMAGE VISION COMPUT, V129, DOI 10.1016/j.imavis.2022.104597
   Li H, 2023, EXPERT SYST APPL, V213, DOI 10.1016/j.eswa.2022.119038
   Li H, 2023, IEEE T IND INFORM, V19, P306, DOI 10.1109/TII.2022.3171352
   Li H, 2022, KNOWL-BASED SYST, V257, DOI 10.1016/j.knosys.2022.109944
   Li YH, 2018, PROC CVPR IEEE, P1091, DOI 10.1109/CVPR.2018.00120
   Lin H, 2022, PROC CVPR IEEE, P19596, DOI 10.1109/CVPR52688.2022.01901
   Liu LB, 2021, PROC CVPR IEEE, P4821, DOI 10.1109/CVPR46437.2021.00479
   Liu SJ, 2022, ANIM BIOTECHNOL, V33, P321, DOI 10.1080/10495398.2020.1798974
   Liu YB, 2024, IEEE T MULTIMEDIA, V26, P154, DOI 10.1109/TMM.2023.3262978
   Ma ZH, 2019, IEEE I CONF COMP VIS, P6141, DOI 10.1109/ICCV.2019.00624
   Pan Y, 2023, ENG APPL ARTIF INTEL, V126, DOI 10.1016/j.engappai.2023.106885
   Rong LZ, 2021, IEEE WINT CONF APPL, P3674, DOI 10.1109/WACV48630.2021.00372
   Song QY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3345, DOI 10.1109/ICCV48922.2021.00335
   Sun YM, 2022, IEEE T CIRC SYST VID, V32, P6700, DOI 10.1109/TCSVT.2022.3168279
   Tang HH, 2024, PATTERN RECOGN LETT, V183, P35, DOI 10.1016/j.patrec.2024.04.025
   Tang HH, 2022, IEEE INT SYMP CIRC S, P3299, DOI 10.1109/ISCAS48785.2022.9937583
   Tao Peng, 2021, Computer Vision - ACCV 2020 15th Asian Conference on Computer Vision. Revised Selected Papers. Lecture Notes in Computer Science (LNCS 12647), P497, DOI 10.1007/978-3-030-69544-6_30
   Tu ZZ, 2023, IEEE T MULTIMEDIA, V25, P4163, DOI 10.1109/TMM.2022.3171688
   Tu ZZ, 2022, IEEE T IMAGE PROCESS, V31, P3752, DOI 10.1109/TIP.2022.3176540
   Ullah FUM, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3561971
   Wang B., 2020, ADV NEURAL INF PROCE, P1595, DOI DOI 10.48550/ARXIV.2009.13077
   Wu Z., 2022, P IEEE INT C MULT EX, P1, DOI [10.1109/ICME52920.2022.9859777, DOI 10.1109/ICME52920.2022.9859777]
   Wu Z, 2023, IEEE T CIRC SYST VID, V33, P228, DOI 10.1109/TCSVT.2022.3187194
   Xia YF, 2021, IMAGE VISION COMPUT, V112, DOI 10.1016/j.imavis.2021.104242
   Youwei Pang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P235, DOI 10.1007/978-3-030-58595-2_15
   Yuan MX, 2022, LECT NOTES COMPUT SC, V13669, P509, DOI 10.1007/978-3-031-20077-9_30
   Zhang J., 2020, P IEEE CVF C COMP VI, P8582, DOI DOI 10.1109/CVPR42600.2020.00861
   Zhang L, 2019, IEEE I CONF COMP VIS, P5126, DOI 10.1109/ICCV.2019.00523
   Zhang Q, 2019, PROC CVPR IEEE, P8289, DOI 10.1109/CVPR.2019.00849
   Zhang SH, 2023, IMAGE VISION COMPUT, V129, DOI 10.1016/j.imavis.2022.104592
   Zhang Yang, 2022, Proceedings of the 11th International Conference on Computer Engineering and Networks. Lecture Notes in Electrical Engineering (808), P90, DOI 10.1007/978-981-16-6554-7_10
   Zhou L, 2023, J VIS COMMUN IMAGE R, V90, DOI 10.1016/j.jvcir.2022.103725
   Zhou WJ, 2024, IEEE T INTELL TRANSP, V25, P4156, DOI 10.1109/TITS.2023.3321328
   Zhou WJ, 2022, IEEE T INTELL TRANSP, V23, P24540, DOI 10.1109/TITS.2022.3203385
NR 43
TC 0
Z9 0
U1 1
U2 1
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD AUG
PY 2024
VL 148
AR 105152
DI 10.1016/j.imavis.2024.105152
EA JUN 2024
PG 14
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA XN9D5
UT WOS:001262473100001
DA 2024-08-05
ER

PT J
AU Wang, FP
   Li, J
   Qi, C
   Wang, L
   Wang, P
AF Wang, Fengping
   Li, Jie
   Qi, Chun
   Wang, Lin
   Wang, Pan
TI JGULF: Joint global and unilateral local feature network for
   micro-expression recognition
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE micro-expression recognition; Unilateral local feature; Global feature;
   Feature selection
ID OPTICAL-FLOW; INFORMATION; DATABASE
AB Micro-expression is a subtle facial movement that is fleeting and manifest in localized areas, making it difficult for the human eye to detect and recognize it. Although algorithms that extract facial features from specific regions or the entire face have shown potential, the classification of micro-expressions using features from symmetrical left and right regions can be challenging in the presence of unilateral movements. This can ultimately affect the performance of micro-expression recognition. To address this issue, we propose a network called Joint Global and Unilateral Local Features (JGULF) for micro-expression recognition. Initially, we employ a Convolutional Neural Network (CNN) and an adjusted Vision Transformer (ViT) model to extract global features from micro-expressions. The local feature extraction module is designed based on global features. The facial features are divided into multiple local regions with varying scales. After that, local feature learning and selection are performed to filter out unilateral local features related to micro-expression movements efficiently. Finally, global and local features are combined to classify micro-expressions. Through comprehensive experimental validation, our algorithm achieves state-of-the-art classification performance on the SMIC, CASMEII, and SAMM micro-expression datasets, demonstrating the effectiveness of combining global features and selecting local features.
C1 [Wang, Fengping; Li, Jie; Qi, Chun; Wang, Lin; Wang, Pan] Xi An Jiao Tong Univ, Sch Informat & Commun Engn, Xian, Peoples R China.
C3 Xi'an Jiaotong University
RP Li, J (corresponding author), Xi An Jiao Tong Univ, Sch Informat & Commun Engn, Xian, Peoples R China.
EM jielixjtu@xjtu.edu.cn
FU National Natural Science Foundation of China [61675161, 62275211];
   Shaanxi Province Key Research and Development Program Key Indus- trial
   Innovation Chain (Cluster) -Industrial Field Project [2023-ZDLGY-22]
FX <BOLD>The work was supported by the National Natural Science Foundation
   of China under Grant 61675161 and Grant 62275211, and by the </BOLD>
   Shaanxi Province Key Research and Development Program Key Indus- trial
   Innovation Chain (Cluster) -Industrial Field Project under Grant
   2023-ZDLGY-22.
CR Aouayeb M., 2021, Image Commun., V99
   Chaudhry R, 2009, PROC CVPR IEEE, P1932, DOI 10.1109/CVPRW.2009.5206821
   Davison AK, 2018, IEEE T AFFECT COMPUT, V9, P116, DOI 10.1109/TAFFC.2016.2573832
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   EKMAN P, 1969, PSYCHIATR, V32, P88, DOI 10.1080/00332747.1969.11023575
   Ekman P., 2002, FACIAL ACTION CODING, P77
   Fan G.W., 2023, 2023 2 INT C ART INT, P1
   Fan X.Q., 2023, P IEEE CVF CVPR JUN
   Feng Weijia, 2024, P 3 WORKSH FAC MICR, P1, DOI [10.1145/3607829.3616444, DOI 10.1145/3607829.3616444]
   Gan Y.S., 2018, Image Commun., V47, P129
   Gu Z., 2024, ICASSP 2024 2024 IEE, P8060, DOI [10.1109/ICASSP48485.2024.10446492, DOI 10.1109/ICASSP48485.2024.10446492]
   Guo C.H., 2023, P 3 WORKSH FAC MICR, P8, DOI [10.1145/3607829.3616446, DOI 10.1145/3607829.3616446]
   Guo CY, 2019, IEEE ACCESS, V7, P174517, DOI 10.1109/ACCESS.2019.2942358
   Guo JY, 2022, PROC CVPR IEEE, P12165, DOI 10.1109/CVPR52688.2022.01186
   Happy SL, 2019, IEEE T AFFECT COMPUT, V10, P394, DOI 10.1109/TAFFC.2017.2723386
   Huang XH, 2019, IEEE T AFFECT COMPUT, V10, P32, DOI 10.1109/TAFFC.2017.2713359
   Huang XH, 2016, NEUROCOMPUTING, V175, P564, DOI 10.1016/j.neucom.2015.10.096
   Huang XH, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P1, DOI 10.1109/ICCVW.2015.10
   Jain A, 2022, IEEE COMPUT SOC CONF, P2475, DOI 10.1109/CVPRW56347.2022.00277
   Jiang XX, 2022, INT C PATT RECOG, P1019, DOI 10.1109/ICPR56361.2022.9956540
   Khor HQ, 2019, IEEE IMAGE PROC, P36, DOI [10.1109/icip.2019.8802965, 10.1109/ICIP.2019.8802965]
   Khor HQ, 2018, IEEE INT CONF AUTOMA, P667, DOI 10.1109/FG.2018.00105
   Kumar A., 2021, Comput. Vision and Pattern Recog.
   Kumar A.J.R., 2023, P IEEE CVF CVPR WORK
   Kumar AJR, 2021, IEEE COMPUT SOC CONF, P1511, DOI 10.1109/CVPRW53098.2021.00167
   Lei L., 2024, 2021 IEEE CVF C COMP
   Lei L, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2237, DOI 10.1145/3394171.3413714
   Li X.Y., 2021, Computational Visual Media,
   Li XB, 2013, IEEE INT CONF AUTOMA, DOI 10.1109/FG.2013.6553717
   Li YT, 2021, IEEE T IMAGE PROCESS, V30, P249, DOI 10.1109/TIP.2020.3035042
   Lin W.P., 2023, Appl. Intel.: The Int. Journal of Artificial Intel, DOI [10.1007/s10489-022-0359, DOI 10.1007/S10489-022-0359]
   Liong ST, 2019, IEEE INT CONF AUTOMA, P658, DOI 10.1109/fg.2019.8756567
   Liong ST, 2018, SIGNAL PROCESS-IMAGE, V62, P82, DOI 10.1016/j.image.2017.11.006
   Liu YJ, 2021, IEEE T AFFECT COMPUT, V12, P254, DOI 10.1109/TAFFC.2018.2854166
   Liu YJ, 2016, IEEE T AFFECT COMPUT, V7, P299, DOI 10.1109/TAFFC.2015.2485205
   Mao QR, 2022, IEEE T AFFECT COMPUT, V13, P1998, DOI 10.1109/TAFFC.2022.3197785
   Matsumoto D, 2011, MOTIV EMOTION, V35, P181, DOI 10.1007/s11031-011-9212-2
   Nguyen XB, 2023, PROC CVPR IEEE, P1482, DOI 10.1109/CVPR52729.2023.00149
   Nie X, 2021, NEUROCOMPUTING, V427, P13, DOI 10.1016/j.neucom.2020.10.082
   Pan H, 2023, ENG APPL ARTIF INTEL, V123, DOI 10.1016/j.engappai.2023.106258
   Pan H, 2020, MULTIMED TOOLS APPL, V79, P31451, DOI 10.1007/s11042-020-09475-4
   Pawar SS, 2019, ADV INTELL SYST COMP, V935, P351, DOI 10.1007/978-3-030-19063-7_29
   Polikovsky S., 2009, P IET SEM DIG LOND U, VVolume 2009
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   See J., 2019, Megc 2019-The Second Facial MicroExpressions Grand Challenge, V5
   Shao ZW, 2023, APPL INTELL, V53, P19860, DOI 10.1007/s10489-023-04533-4
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun Z, 2020, J VIS COMMUN IMAGE R, V71, DOI 10.1016/j.jvcir.2020.102862
   Thuseethan S, 2023, INFORM SCIENCES, V630, P341, DOI 10.1016/j.ins.2022.11.113
   Verma M., 2023, P IEEE CVF WACV JAN
   Verma M, 2020, IEEE T IMAGE PROCESS, V29, P1618, DOI 10.1109/TIP.2019.2912358
   Wang G, 2023, MULTIMEDIA SYST, V29, P1967, DOI 10.1007/s00530-023-01080-3
   Wang J., 2024, arXiv, DOI [10.48550/arXiv.2404.12024, DOI 10.48550/ARXIV.2404.12024]
   Wang J., 2021, Graphics and Vis. Comput. Journal., V4
   Wang SJ, 2017, NEUROCOMPUTING, V230, P382, DOI 10.1016/j.neucom.2016.12.034
   Wang SJ, 2016, NEUROCOMPUTING, V214, P218, DOI 10.1016/j.neucom.2016.05.083
   Wang SJ, 2014, NEURAL PROCESS LETT, V39, P25, DOI 10.1007/s11063-013-9288-7
   Wang TH, 2023, PATTERN RECOGN LETT, V167, P122, DOI 10.1016/j.patrec.2023.02.003
   Wang YD, 2015, LECT NOTES COMPUT SC, V9003, P525, DOI 10.1007/978-3-319-16865-4_34
   Wang ZB, 2024, SENSORS-BASEL, V24, DOI 10.3390/s24051574
   Wu HY, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185561
   Xia ZQ, 2020, IEEE T IMAGE PROCESS, V29, P8590, DOI 10.1109/TIP.2020.3018222
   Xia ZQ, 2020, IEEE T MULTIMEDIA, V22, P626, DOI 10.1109/TMM.2019.2931351
   Xie HX, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2871, DOI 10.1145/3394171.3414012
   Yan WJ, 2014, NEUROCOMPUTING, V136, P82, DOI 10.1016/j.neucom.2014.01.029
   Yan WJ, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0086041
   Yan WJ, 2013, J NONVERBAL BEHAV, V37, P217, DOI 10.1007/s10919-013-0159-8
   Yu M, 2019, IEEE ACCESS, V7, P159214, DOI 10.1109/ACCESS.2019.2950339
   Zhai ZJ, 2023, PROC CVPR IEEE, P22086, DOI 10.1109/CVPR52729.2023.02115
   Zhang H, 2024, SIGNAL IMAGE VIDEO P, V18, P3761, DOI 10.1007/s11760-024-03039-x
   Zhang LF, 2022, IEEE T AFFECT COMPUT, V13, P1973, DOI 10.1109/TAFFC.2022.3213509
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
   Zhao X.H., 2021, CHIN C PATT REC COMP
   Zhao Y, 2018, APPL SCI-BASEL, V8, DOI 10.3390/app8101811
   Zhi RC, 2022, PATTERN RECOGN LETT, V163, P25, DOI 10.1016/j.patrec.2022.09.006
   Zhou HL, 2023, MULTIMEDIA SYST, DOI 10.1007/s00530-023-01164-0
   Zhou L, 2022, PATTERN RECOGN, V122, DOI 10.1016/j.patcog.2021.108275
   Zhou L, 2019, IEEE INT CONF AUTOMA, P642
   Zong Y, 2018, IEEE T MULTIMEDIA, V20, P3160, DOI 10.1109/TMM.2018.2820321
NR 80
TC 1
Z9 1
U1 8
U2 8
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUL
PY 2024
VL 147
AR 105091
DI 10.1016/j.imavis.2024.105091
EA MAY 2024
PG 15
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA UC6D4
UT WOS:001245892500001
DA 2024-08-05
ER

PT J
AU Bai, WJ
   Zhang, YZ
   Wang, L
   Liu, W
   Hu, J
   Huang, G
AF Bai, Wenjing
   Zhang, Yunzhou
   Wang, Li
   Liu, Wei
   Hu, Jun
   Huang, Guan
TI SADGFeat: Learning local features with layer spatial attention and
   domain generalization
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Local feature learning; Domain generalization; Spatial attention; Image
   matching; Visual localization
ID SCALE
AB Local feature plays a pivotal role in various robotic tasks, including 3D reconstruction and visual localization. Although deep learning-based local features have proven superior to their traditional counterparts, they still face challenges in practical applications due to matching failures. These challenges primarily stem from inaccuracies in keypoint localization and the limited robustness of descriptors particularly with significant appearance changes. In this study, we introduce a novel method for local feature learning based on layer spatial attention and domain generalization. Firstly, a keypoint extraction strategy driven by layer spatial attention from high-level feature is proposed to enhance the accuracy of keypoint localization, progressing from coarse to fine granularity. Secondly, a new learning paradigm based on domain generalization is developed to extract local features with resilience against variations in illumination. To enrich the domain diversity within the training dataset, a real-time and lossless Fourier transform-based domain augmentation method is introduced. This method seamlessly integrates into the training process, enhancing the model's adaptability to varying domains. Additionally, explicit feature alignment-based representation learning is performed, further reinforcing the extraction of domain-invariant local features. Experimental results on public datasets demonstrate that the proposed method achieves state-of-the-art performance across various downstream tasks reliant on local feature matching, such as image matching, 3D reconstruction, and long-term visual localization.
C1 [Bai, Wenjing; Zhang, Yunzhou; Wang, Li; Liu, Wei] Northeasten Univ, Coll Informat Sci & Engn, 3-11 Wenhua Rd, Shenyang 110000, Liaoning, Peoples R China.
   [Hu, Jun; Huang, Guan] Neusoft Reach Automot Technol Co, 2 Xinxiu St, Shenyang 110170, Liaoning, Peoples R China.
RP Zhang, YZ (corresponding author), Northeasten Univ, Coll Informat Sci & Engn, 3-11 Wenhua Rd, Shenyang 110000, Liaoning, Peoples R China.
EM baiwenjing.love@163.com; zhangyunzhou@mail.neu.edu.cn; lwei@neusoft.com;
   hu.jun@neusoft.com; huang_g@neusoft.com
FU National Natural Science Foundation of China [61973066, 61471110]; Major
   Science and Technology Pro- jects of Liaoning Province
   [2021JH1/2021JH1/10400049]; Funda- mental Research Funds for the Central
   Universities [N2004022]
FX This work was supported by National Natural Science Foundation of China
   (No.61973066, 61471110) , Major Science and Technology Pro- jects of
   Liaoning Province (No.2021JH1/2021JH1/10400049) , Funda- mental Research
   Funds for the Central Universities (N2004022) .
CR Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018
   Balntas V., 2016, BMVC, V1, P3, DOI DOI 10.5244/C.30.119
   Balntas V, 2017, PROC CVPR IEEE, P3852, DOI 10.1109/CVPR.2017.410
   Barroso-Laguna A, 2023, IEEE T PATTERN ANAL, V45, P698, DOI 10.1109/TPAMI.2022.3145820
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Bhowmik A, 2020, PROC CVPR IEEE, P4947, DOI 10.1109/CVPR42600.2020.00500
   Borgwardt KM, 2006, BIOINFORMATICS, V22, pE49, DOI 10.1093/bioinformatics/btl242
   Chen HK, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6281, DOI 10.1109/ICCV48922.2021.00624
   Chen LC, 2017, Arxiv, DOI [arXiv:1706.05587, 10.48550/arXiv.1706.05587, DOI 10.48550/ARXIV.1706.05587]
   Choy CB, 2016, ADV NEUR IN, V29
   DeTone D, 2018, IEEE COMPUT SOC CONF, P337, DOI 10.1109/CVPRW.2018.00060
   Di Febbo P, 2018, IEEE COMPUT SOC CONF, P795, DOI 10.1109/CVPRW.2018.00111
   Dubey A, 2021, PROC CVPR IEEE, P14335, DOI 10.1109/CVPR46437.2021.01411
   Dusmanu M, 2019, PROC CVPR IEEE, P8084, DOI 10.1109/CVPR.2019.00828
   Han XF, 2015, PROC CVPR IEEE, P3279, DOI 10.1109/CVPR.2015.7298948
   Haritosh A., Facenet: A unified embedding for face recognition and clustering
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jin X, 2020, PROC CVPR IEEE, P3140, DOI 10.1109/CVPR42600.2020.00321
   Lee J., 2023, P IEEE CVF C COMP VI, P6144
   Li KH, 2022, PROC CVPR IEEE, P15817, DOI 10.1109/CVPR52688.2022.01538
   Li X., 2020, NeurIPS, V33, P17346
   Li ZC, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3240195
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu F, 1997, J INTELL ROBOT SYST, V18, P249, DOI 10.1023/A:1007957421070
   Luo ZX, 2020, PROC CVPR IEEE, P6588, DOI 10.1109/CVPR42600.2020.00662
   Luo ZX, 2019, PROC CVPR IEEE, P2522, DOI 10.1109/CVPR.2019.00263
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Mishchuk A, 2017, ADV NEUR IN, V30
   Mishkin D, 2018, LECT NOTES COMPUT SC, V11213, P287, DOI 10.1007/978-3-030-01240-3_18
   Noh H, 2017, IEEE I CONF COMP VIS, P3476, DOI 10.1109/ICCV.2017.374
   Piratla V., 2020, INT C MACH LEARN, P7728
   Planamente M, 2022, IEEE WINT CONF APPL, P163, DOI 10.1109/WACV51458.2022.00024
   Qianqian Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P757, DOI 10.1007/978-3-030-58452-8_44
   Rahman MM, 2020, PATTERN RECOGN, V100, DOI 10.1016/j.patcog.2019.107124
   Revaud J, 2019, Arxiv, DOI [arXiv:1906.06195, DOI 10.48550/ARXIV.1906.06195]
   Rocco I, 2022, IEEE T PATTERN ANAL, V44, P1020, DOI 10.1109/TPAMI.2020.3016711
   Rocco L., 2020, COMPUTER VISION ECCV, P605
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Sarlin PE, 2020, PROC CVPR IEEE, P4937, DOI 10.1109/CVPR42600.2020.00499
   Savinov N, 2017, PROC CVPR IEEE, P3929, DOI 10.1109/CVPR.2017.418
   Schönberger JL, 2017, PROC CVPR IEEE, P6959, DOI 10.1109/CVPR.2017.736
   Schönberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445
   Shen TW, 2019, LECT NOTES COMPUT SC, V11361, P415, DOI 10.1007/978-3-030-20887-5_26
   Tang H, 2023, PROCEEDINGS OF THE 31ST ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2023, P1719, DOI 10.1145/3581783.3612221
   Tang H, 2022, PATTERN RECOGN, V130, DOI 10.1016/j.patcog.2022.108792
   Tang H, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P610, DOI 10.1145/3394171.3413884
   Tang L, 2020, IEEE INT CONF ROBOT, P1301, DOI [10.1109/ICRA40945.2020.9196518, 10.1109/icra40945.2020.9196518]
   Thanh PTH, 2024, IMAGE VISION COMPUT, V141, DOI 10.1016/j.imavis.2023.104871
   Tian Q, 2024, IMAGE VISION COMPUT, V145, DOI 10.1016/j.imavis.2024.104993
   Tian YR, 2017, PROC CVPR IEEE, P6128, DOI 10.1109/CVPR.2017.649
   Tyszkiewicz M., 2020, Advances in Neural Information Processing Systems, V33, P14254
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Verdie Y, 2015, PROC CVPR IEEE, P5279, DOI 10.1109/CVPR.2015.7299165
   Wang HB, 2023, IMAGE VISION COMPUT, V138, DOI 10.1016/j.imavis.2023.104796
   Wiles O, 2021, PROC CVPR IEEE, P15915, DOI 10.1109/CVPR46437.2021.01566
   Wilson K, 2014, LECT NOTES COMPUT SC, V8691, P61, DOI 10.1007/978-3-319-10578-9_5
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xu QW, 2021, PROC CVPR IEEE, P14378, DOI 10.1109/CVPR46437.2021.01415
   Yang Y., 2020, 2020 IEEE C COMP VIS, P9008, DOI [10.1109/cvpr42600.2020.00903, DOI 10.1109/CVPR42600.2020.00903]
   Yang YC, 2020, PROC CVPR IEEE, P4084, DOI 10.1109/CVPR42600.2020.00414
   Yao Y, 2019, IEEE I CONF COMP VIS, P753, DOI 10.1109/ICCV.2019.00084
   Yue XY, 2019, IEEE I CONF COMP VIS, P2100, DOI 10.1109/ICCV.2019.00219
   Zhang HY, 2018, Arxiv, DOI arXiv:1710.09412
   Zhang ZC, 2021, INT J COMPUT VISION, V129, P821, DOI 10.1007/s11263-020-01399-8
   Zhou QJ, 2021, PROC CVPR IEEE, P4667, DOI 10.1109/CVPR46437.2021.00464
   Zhu XZ, 2019, IEEE I CONF COMP VIS, P6687, DOI 10.1109/ICCV.2019.00679
NR 66
TC 0
Z9 0
U1 5
U2 5
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUN
PY 2024
VL 146
AR 105033
DI 10.1016/j.imavis.2024.105033
EA APR 2024
PG 14
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA SO1D7
UT WOS:001235292500001
DA 2024-08-05
ER

PT J
AU Sun, KX
   Tao, J
   Zhang, P
   Zhang, J
AF Sun, Kexin
   Tao, Jing
   Zhang, Peng
   Zhang, Jie
TI Appearance flow estimation for online virtual clothing warping via
   optimal feature linear assignment
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Clothing warping; Virtual try-on; Appearance flow; Local context feature
   aggregation; Optimal linear assignment
AB Clothing warping to spatially align source garments with the corresponding body parts is crucial in clothing media tasks such as virtual try-on and pose-guided person generation. Recent pioneering work has utilized flow fields with additional dimensions of freedom to simulate clothing warping flexibly. However, current methods for estimating appearance flow typically rely on calculating just the local cost volume which contains multiple noisy matching points, potentially leading to a mismatch between clothing and body parts. To address this issue, we propose a novel appearance flow estimation network(Warping-Flow) for clothing warping based on optimal features linear assignment. Specially, two remarkable contributions are made to improve feature matching precision. First, a local context feature aggregation module is proposed to enhance the semantic feature distinction of the source cloth and target pose. Second, Warping-Flow estimates a hard attention mask through the cost volume to filter irrelevant features, followed by the optimal linear assignment algorithm to normalize the cost volume to a discrete permutation matrix that explicitly models the most contributing bipartite matches. Experiments conducted on the VITON and VITON-HD datasets demonstrate that Warping-Flow outperforms existing state-of-the-art algorithms, particularly in cases involving complex clothing deformation. Furthermore, Warping-Flow can serve as a plug-in to improve existing garment media technologies.
C1 [Sun, Kexin; Tao, Jing] Donghua Univ, Coll Mech Engn, Shanghai 201620, Peoples R China.
   [Sun, Kexin; Tao, Jing; Zhang, Peng; Zhang, Jie] Donghua Univ, Inst Artificial Intelligence, Shanghai 201620, Peoples R China.
C3 Donghua University; Donghua University
RP Zhang, P (corresponding author), Donghua Univ, Inst Artificial Intelligence, Shanghai 201620, Peoples R China.
EM 2210958@mail.dhu.edu.cn; zhangp88@dhu.edu.cn; mezhangjie@dhu.edu.cn
RI sun, ke/KMX-1265-2024
CR Cao Hu, 2023, Computer Vision - ECCV 2022 Workshops: Proceedings. Lecture Notes in Computer Science (13803), P205, DOI 10.1007/978-3-031-25066-8_9
   Chang Y, 2022, PROCEEDINGS OF THE 2022 INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, ICMR 2022, P313, DOI 10.1145/3512527.3531387
   Chen Y., 2023, P IEEE CVF WINT C AP, P5068
   Choi S, 2021, PROC CVPR IEEE, P14126, DOI 10.1109/CVPR46437.2021.01391
   Dong H, 2019, IEEE I CONF COMP VIS, P9025, DOI 10.1109/ICCV.2019.00912
   Dong Haoye, 2018, NeurIPS, P474
   Du CH, 2023, IEEE T MULTIMEDIA, V25, P777, DOI 10.1109/TMM.2022.3152367
   Fele B, 2022, IEEE WINT CONF APPL, P2203, DOI 10.1109/WACV51458.2022.00226
   Ge YY, 2021, PROC CVPR IEEE, P8481, DOI 10.1109/CVPR46437.2021.00838
   Han XT, 2019, IEEE I CONF COMP VIS, P10470, DOI 10.1109/ICCV.2019.01057
   Han XT, 2018, PROC CVPR IEEE, P7543, DOI 10.1109/CVPR.2018.00787
   Han Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7847, DOI 10.1109/CVPR42600.2020.00787
   He S, 2022, PROC CVPR IEEE, P3460, DOI 10.1109/CVPR52688.2022.00346
   Hu BW, 2022, IEEE T MULTIMEDIA, V24, P1233, DOI 10.1109/TMM.2022.3143712
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jiang Z., 2022, P CVPR, P2343
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Lee SY, 2022, Arxiv, DOI arXiv:2206.14180
   Li YL, 2021, IEEE IMAGE PROC, P1364, DOI 10.1109/ICIP42928.2021.9506397
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu MC, 2021, NEUROCOMPUTING, V460, P345, DOI 10.1016/j.neucom.2021.06.077
   Liu Z, 2022, PROC CVPR IEEE, P11999, DOI 10.1109/CVPR52688.2022.01170
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Mena G, 2018, Arxiv, DOI arXiv:1802.08665
   Minar M.R., 2020, CVPR WORKSH, V3, P10
   Peng Z, 2023, COMPUT VIS MEDIA, V9, P109, DOI 10.1007/s41095-021-0267-z
   Adams RP, 2011, Arxiv, DOI arXiv:1106.1925
   Rocco I, 2017, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2017.12
   Shang Gao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11741, DOI 10.1109/CVPR42600.2020.01176
   Shen SL, 2020, Arxiv, DOI arXiv:2012.12440
   Shohag MSA, 2022, INFORM SCIENCES, V616, P1, DOI 10.1016/j.ins.2022.10.065
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sui X., 2022, P IEEE CVF C COMP VI, P17602
   Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931
   Tak-Wai Hui, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P169, DOI 10.1007/978-3-030-58565-5_11
   Teed Z., 2020, EUR C COMP VIS, P402, DOI DOI 10.1007/978-3-030-58536-524
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang BC, 2018, LECT NOTES COMPUT SC, V11217, P607, DOI 10.1007/978-3-030-01261-8_36
   Wang ZJ, 2022, PROC CVPR IEEE, P7693, DOI 10.1109/CVPR52688.2022.00755
   Xu HF, 2022, PROC CVPR IEEE, P8111, DOI 10.1109/CVPR52688.2022.00795
   Yan KY, 2023, PROC CVPR IEEE, P17194, DOI 10.1109/CVPR52729.2023.01649
   Yu Tianshu, 2020, INT C LEARN REPR
   Zhao SY, 2020, PROC CVPR IEEE, P6277, DOI 10.1109/CVPR42600.2020.00631
NR 45
TC 0
Z9 0
U1 15
U2 15
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD FEB
PY 2024
VL 142
AR 104899
DI 10.1016/j.imavis.2024.104899
EA JAN 2024
PG 14
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA HE3S7
UT WOS:001157784200001
DA 2024-08-05
ER

PT J
AU Zhu, YM
   Zheng, W
   Ma, ZP
AF Zhu, Yueming
   Zheng, Wei
   Ma, Zepeng
TI Superpixel conditional generation adversarial network for CMR artifact
   correction
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Cardiac magnetic resonance imaging; Motion artifact removal; Superpixel
   segmentation; Conditional generative adversarial networks; Superpixel
   pooling
AB Cardiac Magnetic Resonance (CMR) is widely used in diagnosing cardiac diseases for its excellent contrast of cardiovascular features. However, due to the long imaging time of CMR scanning, the patient's respiration, limb shaking, and heart beating will lead to a certain degree of motion artifacts in the image, seriously degrade the image quality and affect the doctor's clinical judgment. This paper proposes a superpixel conditional Generative Adversarial Network (spcGAN) based on a conditional Generative Adversarial Network (cGAN) by applying superpixel to both generator and discriminator parts. In the generator section, a generator network based on superpixel segmentation and pooling is proposed for feature extraction at the superpixel level to enhance the reconstruction of image edge texture and structural details. In the discriminator part, superpixel pooling is used to construct a superpixel discriminator. It is fused with the traditional convolutional discriminator to produce a superpixel-based dual discriminator, which makes the discriminator consider the image's local structure and details. Based on the generator and discriminator structure proposed in this paper, superpixel pooling and edge texturing loss functions are designed for optimization. Adequate ablation experiments and comparison experiments are conducted in terms of experimental results. Three types of objective metrics, Peak Signal-to-Noise Ratio (PSNR), Structural Similarity (SSIM), and Focus Measurement (Tenengrad), were selected as references. The experimental results show that the effect of removing motion artifacts from authentic CMR images on the three datasets is most significant in the dataset produced in this paper. The results obtained from the fusion between the designed generator, discriminator, and loss function are the most obvious. Compared with the existing methods, the spcGAN proposed in this paper performs better.
C1 [Zhu, Yueming; Zheng, Wei] Hebei Univ, Baoding 050224, Peoples R China.
   [Ma, Zepeng] Hebei Univ Hosp, Baoding 071000, Peoples R China.
C3 Hebei University
RP Zheng, W (corresponding author), Hebei Univ, Baoding 050224, Peoples R China.
EM 147685650@qq.com
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Al-Masni MA, 2023, COMPUT BIOL MED, V153, DOI 10.1016/j.compbiomed.2023.106553
   Al-masni MA, 2021, Arxiv, DOI [arXiv:2111.06401, 10.48550/arXiv.2111.06401, DOI 10.48550/ARXIV.2111.06401]
   [Anonymous], 2011, Drug Ther Bull, V49, P141, DOI 10.1136/dtb.2011.02.0073
   Butskova A, 2021, LECT NOTES COMPUT SC, V12928, P83, DOI 10.1007/978-3-030-87602-9_8
   Cai LK, 2023, COMPUT BIOL MED, V164, DOI 10.1016/j.compbiomed.2023.107264
   Chen HY, 2022, PATTERN RECOGN, V130, DOI 10.1016/j.patcog.2022.108827
   Cocker Myra, 2010, Curr Cardiol Rep, V12, P82, DOI 10.1007/s11886-009-0077-x
   Fengting Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13961, DOI 10.1109/CVPR42600.2020.01398
   Ghodrati V, 2021, NMR BIOMED, V34, DOI 10.1002/nbm.4433
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Haut JM, 2019, IEEE T GEOSCI REMOTE, V57, P9277, DOI 10.1109/TGRS.2019.2924818
   HEDLEY M, 1991, IEEE T MED IMAGING, V10, P40, DOI 10.1109/42.75609
   Jampani V, Superpixel Sampling Networks, Patent No. [201816130871, 2019340728A12023-07-21]
   Johnson JD, 2016, RES EDUC POLICY LOCA, P3
   Kingma D. P., 2014, arXiv
   Kupyn O, 2019, IEEE I CONF COMP VIS, P8877, DOI 10.1109/ICCV.2019.00897
   Kupyn O, 2018, PROC CVPR IEEE, P8183, DOI 10.1109/CVPR.2018.00854
   Lauzon ML, 1998, MAGNET RESON MED, V40, P769, DOI 10.1002/mrm.1910400519
   Li XT, 2022, ARTIF INTELL REV, V55, P4809, DOI 10.1007/s10462-021-10121-0
   Lyu Q, 2021, IEEE T MED IMAGING, V40, P2170, DOI 10.1109/TMI.2021.3073381
   Manco L, 2021, PHYS MEDICA, V83, P194, DOI 10.1016/j.ejmp.2021.03.026
   Mirza M., 2014, ARXIV
   Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35
   Pawar K, 2022, NMR BIOMED, V35, DOI 10.1002/nbm.4225
   Raman SV, 2012, J CARDIOVASC MAGN R, V14, DOI 10.1186/1532-429X-14-82
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song Jingqi, 2016, Comput. Therm. Sci., V43, P210
   Tao X, 2018, PROC CVPR IEEE, P8174, DOI 10.1109/CVPR.2018.00853
   Wu Y., 2021, Cold Spring Harbor Lab, DOI [10.1101/2021.04.23.441167, DOI 10.1101/2021.04.23.441167]
   Zhai YY, 2023, COMPUT BIOL MED, V163, DOI 10.1016/j.compbiomed.2023.107239
   Zhang WL, 2019, P INT COMP SOFTW APP, P405, DOI 10.1109/COMPSAC.2019.10240
NR 32
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD SEP
PY 2024
VL 149
AR 105112
DI 10.1016/j.imavis.2024.105112
EA JUL 2024
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA XZ9X0
UT WOS:001265630900001
DA 2024-08-05
ER

PT J
AU Wang, JX
   Zhang, YF
   Bao, FX
   Liu, YT
   Zhang, QY
   Zhang, CM
AF Wang, Jingxin
   Zhang, Yunfeng
   Bao, Fangxun
   Liu, Yuetong
   Zhang, Qiuyue
   Zhang, Caiming
TI Video object segmentation by multi-scale attention using bidirectional
   strategy
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Video object segmentation; Training strategy; Attention mechanism
AB This paper focuses on semi-supervised video object segmentation (VOS). Recently, several Space-Time Memory based networks have effectively improved the performance of VOS. However, most methods predict the target object mask forwardly, which causes error propagation to mislead the future frame segmentation. Moreover, the rich multi-scale information of objects needs to be effectively exploited in videos to extract fine-grained multiscale spatial information. To address these limitations, we present a network with a multi-scale attention module for semi-supervised VOS, which combines a new bidirectional strategy during training. Firstly, we propose the bidirectional strategy in which a backward flow combines the existing standard forward flow. With the strategy, we can rely on the first frame's ground-truth mask to mitigate the problem of error propagation. Secondly, a multi-scale attention module is designed to extracts multi-scale features by different weights and interacts with information between multi-scale channel attention. Especially the multi-scale attention module can effectively extract the fine-grained mask by the network during the bidirectional training. Experimental results show that our network achieves significant segmentation performance compared to state-of-the-art approaches on the YouTube-VOS and DAVIS datasets.
C1 [Wang, Jingxin; Zhang, Yunfeng; Liu, Yuetong] Shandong Univ Finance & Econ, Sch Comp Sci & Technol, Jinan 250014, Peoples R China.
   [Zhang, Qiuyue] Shandong Univ Finance & Econ, Sch Mangement Sci & Engn, Jinan 250014, Peoples R China.
   [Zhang, Yunfeng] Shandong Univ Finance & Econ, Shandong Key Lab Blockchain Finance, Jinan 250014, Peoples R China.
   [Bao, Fangxun] Shandong Univ, Sch Math, Jinan 250100, Peoples R China.
   [Zhang, Caiming] Shandong Univ, Sch Software, Jinan 250101, Peoples R China.
   [Zhang, Caiming] Shandong Coinnovat Ctr Future Intelligent Comp, Yantai 264025, Peoples R China.
C3 Shandong University of Finance & Economics; Shandong University of
   Finance & Economics; Shandong University of Finance & Economics;
   Shandong University; Shandong University
RP Zhang, YF (corresponding author), Shandong Univ Finance & Econ, Sch Comp Sci & Technol, Jinan 250014, Peoples R China.
EM yfzhang@sdufe.edu.cn
FU National Natural Science Foundation of China [61972227]; Natural Science
   Foundation of Shandong Province [ZR2022MF245]; Shandong Provincial
   Natural Science Foundation Key Project [ZR2020KF015]; Shandong
   Provincial Key Research and Development Program (Major Scientific and
   Technological Innovation Project) (Research and application of
   industry-oriented large-scale cloud native Application Architecture
   Support platform) [2020CXGC010110]; Youth Innovation Team in Colleges
   and universities of Shandong Province [2022KJ185]
FX This work was supported by the National Natural Science Foundation of
   China (No.61972227) ; Natural Science Foundation of Shandong Province
   (ZR2022MF245) ; Shandong Provincial Natural Science Foundation Key
   Project (ZR2020KF015) ; Shandong Provincial Key Research and Development
   Program (Major Scientific and Technological Innovation Project)
   (Research and application of industry-oriented large-scale cloud native
   Application Architecture Support platform) (No.2020CXGC010110) ; Youth
   Innovation Team in Colleges and universities of Shandong Province
   (2022KJ185) .
CR Caelles S, 2017, PROC CVPR IEEE, P5320, DOI 10.1109/CVPR.2017.565
   Chen LC, 2017, Arxiv, DOI [arXiv:1706.05587, 10.48550/arXiv.1706.05587, DOI 10.48550/ARXIV.1706.05587]
   Chen WD, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P4416, DOI 10.1145/3503161.3547761
   Chen YH, 2018, PROC CVPR IEEE, P1189, DOI 10.1109/CVPR.2018.00130
   Cheng HK, 2022, LECT NOTES COMPUT SC, V13688, P640, DOI 10.1007/978-3-031-19815-1_37
   Cheng Ho Kei, 2021, Advances in Neural Information Processing Systems, V34, P11781
   Cheng JC, 2018, PROC CVPR IEEE, P7415, DOI 10.1109/CVPR.2018.00774
   Gao BC, 2022, NEUROCOMPUTING, V492, P396, DOI 10.1016/j.neucom.2022.04.042
   Ge CJ, 2024, IEEE T IMAGE PROCESS, V33, P1726, DOI 10.1109/TIP.2023.3251693
   Hu J., 2018, SQUEEZE AND EXCITATI, P7132
   Hu L, 2021, PROC CVPR IEEE, P4142, DOI 10.1109/CVPR46437.2021.00413
   Hu YT, 2018, LECT NOTES COMPUT SC, V11212, P56, DOI 10.1007/978-3-030-01237-3_4
   Johnander J, 2019, PROC CVPR IEEE, P8945, DOI 10.1109/CVPR.2019.00916
   Kingma D. P., 2014, arXiv
   Lan M, 2023, PATTERN RECOGN, V136, DOI 10.1016/j.patcog.2022.109214
   Lan M, 2022, AAAI CONF ARTIF INTE, P1228
   Li XX, 2017, Arxiv, DOI arXiv:1708.00197
   Li Y., 2020, Adv. Neural Inf. Proces. Syst., V33, P1218
   Liang Y., 2020, P 34 INT C NEUR INF, V33, P3430
   Lin FQ, 2020, IMAGE VISION COMPUT, V94, DOI 10.1016/j.imavis.2019.103864
   Lin HJ, 2019, IEEE I CONF COMP VIS, P3948, DOI 10.1109/ICCV.2019.00405
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Luiten J, 2019, LECT NOTES COMPUT SC, V11364, P565, DOI 10.1007/978-3-030-20870-7_35
   Oh SW, 2019, IEEE I CONF COMP VIS, P9225, DOI 10.1109/iccv.2019.00932
   Oh SW, 2019, PROC CVPR IEEE, P5242, DOI 10.1109/CVPR.2019.00539
   Oh SW, 2018, PROC CVPR IEEE, P7376, DOI 10.1109/CVPR.2018.00770
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Perazzi F, 2017, PROC CVPR IEEE, P3491, DOI 10.1109/CVPR.2017.372
   Pont-Tuset J., 2018, The 2017 Davis Challenge on Video Object Segmentation
   Qiao YY, 2023, IEEE T PATTERN ANAL, V45, P8524, DOI 10.1109/TPAMI.2023.3234243
   Seong Hongje, 2020, EUR C COMP VIS, P629, DOI DOI 10.1007/978-3-030-58542-638
   Voigtlaender P, 2019, PROC CVPR IEEE, P9473, DOI 10.1109/CVPR.2019.00971
   Wang HC, 2021, PROC CVPR IEEE, P1296, DOI 10.1109/CVPR46437.2021.00135
   Wang JK, 2023, PROC CVPR IEEE, P2268, DOI 10.1109/CVPR52729.2023.00225
   Wang N, 2019, PROC CVPR IEEE, P1308, DOI 10.1109/CVPR.2019.00140
   Wang XL, 2019, PROC CVPR IEEE, P2561, DOI 10.1109/CVPR.2019.00267
   Wang ZQ, 2019, IEEE I CONF COMP VIS, P3977, DOI 10.1109/ICCV.2019.00408
   Xi Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9381, DOI 10.1109/CVPR42600.2020.00940
   Xiankai Lu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P661, DOI 10.1007/978-3-030-58580-8_39
   Xie HZ, 2021, PROC CVPR IEEE, P1286, DOI 10.1109/CVPR46437.2021.00134
   Xu N, 2018, INT EL DEVICES MEET
   Yang LJ, 2018, PROC CVPR IEEE, P6499, DOI 10.1109/CVPR.2018.00680
   Yang ZX, 2022, IEEE T PATTERN ANAL, V44, P4701, DOI 10.1109/TPAMI.2021.3081597
   Yang ZX, 2019, IEEE INT CONF COMP V, P697, DOI 10.1109/ICCVW.2019.00085
   Yu L, 2022, IMAGE VISION COMPUT, V119, DOI 10.1016/j.imavis.2022.104374
   Yu Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P735, DOI 10.1007/978-3-030-58607-2_43
   Yuan WH, 2020, IEEE INT C INT ROBOT, P10351, DOI 10.1109/IROS45743.2020.9341621
   Zhang H, 2021, Arxiv, DOI arXiv:2105.14447
   Zhang PF, 2020, PROC CVPR IEEE, P1109, DOI 10.1109/CVPR42600.2020.00119
   Zhang YZ, 2020, PROC CVPR IEEE, P6947, DOI 10.1109/CVPR42600.2020.00698
NR 50
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD AUG
PY 2024
VL 148
AR 105136
DI 10.1016/j.imavis.2024.105136
EA JUN 2024
PG 10
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA XD5O6
UT WOS:001259761700001
DA 2024-08-05
ER

PT J
AU Huang, JL
   Huang, GH
   Zhang, XH
   Yuan, XC
   Xie, FF
   Pun, CM
   Zhong, G
AF Huang, Jielun
   Huang, Guoheng
   Zhang, Xuhui
   Yuan, Xiaochen
   Xie, Fenfang
   Pun, Chi-Man
   Zhong, Guo
TI Black-box reversible adversarial examples with invertible neural network
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Image restoration; Adversarial attack; Invertible neural network
AB Reversible Adversarial Example (RAE) has been widely researched for its ability to ensure authorized access while preventing unauthorized recognition. Existing RAE schemes focus on Reversible Data Hiding techniques and white-box attacks. However, white-box attacks might be impractical due to the unknown parameters of the target model. Besides, these methods suffer massive loss during the embedding of perturbations, impacting the RAE's quality. In this paper, we propose I-RAE scheme to generate black-box RAE with minimal loss based on Invertible Neural Network (INN). Specifically, Black-box Attack Flow (BAFlow) is introduced to generate perturbations on a Gaussian distribution that are more easily embeddable. Furthermore, to enhance the embedding capability of RAE, we innovatively treat the embedding of perturbation as an image hiding and propose Perturbation Hiding Network (PHN) to reversibly hide the entire perturbation into the adversarial example. We also implement wavelet high-frequency hiding to reduce the degradation in the visual quality of RAE. Experimental results on the ImageNet and CIFAR-10 datasets demonstrate that I-RAE achieves state-of-the-art blackbox attack ability and visual quality.
C1 [Huang, Jielun; Huang, Guoheng; Zhang, Xuhui] Guangdong Univ Technol, Sch Comp Sci & Technol, Guangzhou 510000, Peoples R China.
   [Yuan, Xiaochen] Macao Polytech Univ, Fac Appl Sci, Macau 999078, Peoples R China.
   [Xie, Fenfang; Zhong, Guo] Guangdong Univ Foreign Studies, Sch Informat Sci & Technol, Guangzhou 510000, Peoples R China.
   [Xie, Fenfang] Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 510000, Peoples R China.
   [Pun, Chi-Man] Univ Macau, Fac Sci & Technol, Macau 999078, Peoples R China.
C3 Guangdong University of Technology; Macao Polytechnic University;
   Guangdong University of Foreign Studies; Sun Yat Sen University;
   University of Macau
RP Huang, GH (corresponding author), Guangdong Univ Technol, Sch Comp Sci & Technol, Guangzhou 510000, Peoples R China.; Zhong, G (corresponding author), Guangdong Univ Foreign Studies, Sch Informat Sci & Technol, Guangzhou 510000, Peoples R China.
EM kevinwong@gdut.edu.cn; yb77410@um.edu.mo
OI Huang, Jielun/0009-0002-4846-6574
FU Key Areas Research and Development Program of Guangzhou [2023B01J0029];
   Science and Technology Research in Key Areas in Foshan [2020001006832];
   National Science Foundation of China [U21A20478, 62102461]; Key -Area
   Research and Development Program of Guangdong Province [2018B010109007,
   2019B010153002]; Science and Technology Projects of Guangzhou
   [202007040006]; Guangdong Provincial Key Laboratory of Cyber-Physical
   System [2020B1212060069]; Guangdong Basic and Applied Basic Research
   Foundation [2024A1515011729]; National Statistical Science Research
   Project of China [2022LY096]
FX This project was partially supported by the Key Areas Research and
   Development Program of Guangzhou under Grant 2023B01J0029, the Science
   and Technology Research in Key Areas in Foshan under Grant
   2020001006832, the National Science Foundation of China under Grant
   U21A20478 and 62102461, the Key -Area Research and Development Program
   of Guangdong Province under Grant 2018B010109007 and 2019B010153002, the
   Science and Technology Projects of Guangzhou under Grant 202007040006,
   the Guangdong Provincial Key Laboratory of Cyber-Physical System under
   Grant 2020B1212060069, the Guangdong Basic and Applied Basic Research
   Foundation under Grant 2024A1515011729, and the National Statistical
   Science Research Project of China (No. 2022LY096) .
CR Bai Y, 2023, PATTERN RECOGN, V133, DOI 10.1016/j.patcog.2022.109037
   Baluja S, 2017, ADV NEUR IN, V30
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Chen L, 2024, MULTIMED TOOLS APPL, V83, P11215, DOI 10.1007/s11042-023-15383-0
   Dinh L., 2017, INT C LEARN REPR
   Dinh L, 2015, Arxiv, DOI [arXiv:1410.8516, 10.48550/arXiv.1410.8516]
   Dolatabadi HM, 2020, ADV NEUR IN, V33
   Ghosh A, 2022, PATTERN RECOGN, V122, DOI 10.1016/j.patcog.2021.108279
   Guo C, 2019, PR MACH LEARN RES, V97
   Haider U, 2023, IMAGE VISION COMPUT, V135, DOI 10.1016/j.imavis.2023.104718
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ho J., 2019, PMLR, V97, P2722
   Hu ZC, 2022, PATTERN RECOGN, V130, DOI 10.1016/j.patcog.2022.108824
   Huynh-Thu Q, 2008, ELECTRON LETT, V44, P800, DOI 10.1049/el:20080522
   Goodfellow IJ, 2015, Arxiv, DOI [arXiv:1412.6572, DOI 10.48550/ARXIV.1412.6572]
   Jing JP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4713, DOI 10.1109/ICCV48922.2021.00469
   Kingma D. P., 2014, arXiv
   Kingma DP, 2018, ADV NEUR IN, V31
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Kurakin A., 2018, Artificial Intelligence Safety and Security, P99, DOI DOI 10.1201/9781351251389-8
   Li YD, 2019, PR MACH LEARN RES, V97
   Liu H, 2023, PATTERN RECOGN, V144, DOI 10.1016/j.patcog.2023.109822
   Liu JY, 2023, PATTERN RECOGN, V134, DOI 10.1016/j.patcog.2022.109048
   Lu SP, 2021, PROC CVPR IEEE, P10811, DOI 10.1109/CVPR46437.2021.01067
   Mahmood A, 2020, IMAGE VISION COMPUT, V93, DOI 10.1016/j.imavis.2019.09.002
   MALLAT SG, 1989, IEEE T PATTERN ANAL, V11, P674, DOI 10.1109/34.192463
   Puteaux P, 2021, J VIS COMMUN IMAGE R, V77, DOI 10.1016/j.jvcir.2021.103085
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Wali A, 2023, DIGIT SIGNAL PROCESS, V141, DOI 10.1016/j.dsp.2023.104187
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wierstra D, 2014, J MACH LEARN RES, V15, P949
   Xiong LZ, 2023, PATTERN RECOGN, V140, DOI 10.1016/j.patcog.2023.109549
   Yin ZX, 2023, PATTERN RECOGN LETT, V166, P1, DOI 10.1016/j.patrec.2022.12.018
   Yin ZX, 2021, Arxiv, DOI arXiv:1911.02360
   Zagoruyko S, 2015, PROC CVPR IEEE, P4353, DOI 10.1109/CVPR.2015.7299064
   Zhang C., 2020, Adv. Neural Inf. Process. Syst., V33, P10223
   Zhu JR, 2018, LECT NOTES COMPUT SC, V11219, P682, DOI 10.1007/978-3-030-01267-0_40
NR 39
TC 0
Z9 0
U1 2
U2 2
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUL
PY 2024
VL 147
AR 105094
DI 10.1016/j.imavis.2024.105094
EA MAY 2024
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA UG0B5
UT WOS:001246777400001
DA 2024-08-05
ER

PT J
AU Chaurasia, D
   Patro, BDK
AF Chaurasia, Divyansh
   Patro, B. D. K.
TI Detection of objects in satellite and aerial imagery using channel and
   spatially attentive YOLO-CSL for surveillance
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Remote sensing; Computer vision; Channel attention; Spatial attention;
   Rotational object detection; Circular smooth label
AB A novel lightweight Rotational Object Detection algorithm is proposed to overcome the shortcomings of conventional computer-vision-aided object detection methods used in Remote Sensing and Surveillance that overlook the variability in size and orientation of objects in satellite and aerial images. This advanced algorithm integrates a branch dedicated to angle prediction and employs the circular smooth label (CSL) method for angle classification. This approach is suitable for scenarios that require detection in rotational boxes. Our work is further distinguished by the introduction of a novel Channel and Spatial Attention (CSA) module, which is seamlessly integrated into the YOLOv5-CSL framework via the C3CS module. This module accentuates pertinent features through both the channel and spatial attention mechanisms. In addition, bicubic interpolation and the GELU activation function were incorporated into the YOLOv5-CSLA model. Our model achieved 57.86 mAP on the challenging DOTA v2 dataset surpassing the second-best method by 0.20 points and simultaneously consuming 11 million fewer parameters and 103 fewer GFLOPs (our model consumes 25 M Params and 54 GFLOPs), justifying its suitability for deployment on a large majority of platforms, as the compute required is a challenge in real-time deployment scenarios.
C1 [Chaurasia, Divyansh; Patro, B. D. K.] Rajkiya Engn Coll Kannauj, Dept Comp Sci & Engn, Aher 209732, Uttar Pradesh, India.
RP Patro, BDK (corresponding author), Rajkiya Engn Coll Kannauj, Dept Comp Sci & Engn, Aher 209732, Uttar Pradesh, India.
EM bdkpatro@reck.ac.in
OI Chaurasia, Divyansh/0000-0003-2714-8219
CR Abdullah Dahlan, 2018, Journal of Physics: Conference Series, V1114, DOI 10.1088/1742-6596/1114/1/012066
   Albawi S, 2017, I C ENG TECHNOL
   Benjamin P., 2023, 2023 JOINT URB REM S, P1, DOI [10.1109/JURSE57346.2023.10144214, DOI 10.1109/JURSE57346.2023.10144214]
   Chaurasia D, 2023, 2023 3 INT C COMP IN, P442, DOI [10.1109/ICCIT58132.2023.10273929, DOI 10.1109/ICCIT58132.2023.10273929]
   Chen K, 2019, PROC CVPR IEEE, P4969, DOI 10.1109/CVPR.2019.00511
   Chen SH, 2021, INT J INTERACT MULTI, V6, P101, DOI 10.9781/ijimai.2021.06.001
   Ding J, 2022, IEEE T PATTERN ANAL, V44, P7778, DOI 10.1109/TPAMI.2021.3117983
   Ding J, 2019, PROC CVPR IEEE, P2844, DOI 10.1109/CVPR.2019.00296
   Gao Yangte, 2023, IEEE Journal on Miniaturization for Air and Space Systems, P93, DOI 10.1109/JMASS.2023.3234076
   Han JM, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3062048
   Han W, 2022, INT J INTERACT MULTI, V7, P23, DOI 10.9781/ijimai.2022.07.003
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Hou LP, 2022, AAAI CONF ARTIF INTE, P923
   Indolia Sakshi, 2018, Procedia Computer Science, V132, P679, DOI 10.1016/j.procs.2018.05.069
   Karthiga M., 2021, 2021 INT C INN COMP, P1, DOI [DOI 10.1109/ICSES52305.2021.9633834, 10.1109/ICSES52305.2021.9633834]
   Li C, 2020, INT CONF WIRE COMMUN, P620, DOI 10.1109/WCSP49889.2020.9299876
   Li WT, 2022, PROC CVPR IEEE, P1819, DOI 10.1109/CVPR52688.2022.00187
   Li ZQ, 2023, IEEE GEOSCI REMOTE S, V20, DOI 10.1109/LGRS.2023.3234267
   Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Ma JQ, 2018, IEEE T MULTIMEDIA, V20, P3111, DOI 10.1109/TMM.2018.2818020
   Mansour Ahmad, 2019, 2019 Ninth International Conference on Intelligent Computing and Information Systems (ICICIS), P86, DOI 10.1109/ICICIS46948.2019.9014842
   Ming Q, 2021, AAAI CONF ARTIF INTE, V35, P2355
   OShea K, 2015, ARXIV
   Pravalika P., 2023, 2023 2nd International Conference on Applied Artificial Intelligence and Computing (ICAAIC), P1123, DOI 10.1109/ICAAIC56838.2023.10140305
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shifeng Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9756, DOI 10.1109/CVPR42600.2020.00978
   Tian Z, 2022, IEEE T PATTERN ANAL, V44, P1922, DOI 10.1109/TPAMI.2020.3032166
   Xia Z., 2022, 2022 4 INT C ROB COM, P142, DOI [10.1109/ICRCV55858.2022.9953228, DOI 10.1109/ICRCV55858.2022.9953228]
   Xie LZ, 2013, GLOB CONGRESS INTELL, P279, DOI 10.1109/GCIS.2013.51
   Xie XX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3500, DOI 10.1109/ICCV48922.2021.00350
   Xu C, 2023, PROC CVPR IEEE, P7318, DOI 10.1109/CVPR52729.2023.00707
   Xu RJ, 2021, FORESTS, V12, DOI 10.3390/f12020217
   Xue Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P677, DOI 10.1007/978-3-030-58598-3_40
   Yang X, 2021, AAAI CONF ARTIF INTE, V35, P3163
   Yang Xue, 2021, PROC ADV NEURAL INF, V34
   Zhang DY, 2023, COMPUT ELECTRON AGR, V211, DOI 10.1016/j.compag.2023.107968
   Zhu MM, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14092034
NR 38
TC 1
Z9 1
U1 4
U2 4
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUL
PY 2024
VL 147
AR 105070
DI 10.1016/j.imavis.2024.105070
EA MAY 2024
PG 15
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA TJ1U7
UT WOS:001240813400001
DA 2024-08-05
ER

PT J
AU Ahmad, I
   Israr, SM
   Ul Islam, Z
AF Ahmad, Ibtihaj
   Israr, Syed Muhammad
   Ul Islam, Zain
TI A three in one bottom-up framework for simultaneous semantic
   segmentation, instance segmentation and classification of multi-organ
   nuclei in digital cancer histology
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Nuclei segmentation; Nuclei classification; Simultaneous segmentation
   and classification; Spatial channel attention; Cancer diagnosis in
   digital histology; Tumor segmentation and classification
ID MODEL
AB Simultaneous segmentation and classification of nuclei in digital histology remains challenging. The highest achieved Panoptic Quality (PQ) remains low due to overlapping nuclei, higher staining and tissue variability, and rough clinical conditions. The generic deep-learning methods usually rely on end-to-end models, which fail to address these problems associated explicitly with digital histology. We resolve these issues using a dual attentionbased model combined with post-processing in a bottom-up fashion. We use three attention decoder heads, which produce semantic segmentation, edge proposals, and classification maps. We use these outputs to apply post-processing, including controlled watershed and pixel grouping, to produce instance segmentation and classification. Our multi-stage approach utilizes edge proposals and semantic segmentations compared to direct segmentation and classification strategies followed by most generic state-of-the-art methods. Due to this, we demonstrate a significant performance improvement in producing high-quality instance segmentation and nuclei classification. We have achieved a 0.841 Dice score for semantic segmentation, 0.713 bPQ scores for instance segmentation, and 0.633 mPQ for nuclei classification. Furthermore, the framework is less complex compared to the state-of-the-art.
C1 [Ahmad, Ibtihaj] Northwestern Polytech Univ, Sch Comp Sci, Xian 710072, Shaanxi, Peoples R China.
   [Israr, Syed Muhammad] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230000, Anhui, Peoples R China.
   [Ul Islam, Zain] Hamad Bin Khalifa Univ, Coll Sci & Engn, ICT Div, Doha 34110, Qatar.
C3 Northwestern Polytechnical University; Chinese Academy of Sciences;
   University of Science & Technology of China, CAS; Qatar Foundation (QF);
   Hamad Bin Khalifa University-Qatar
RP Ahmad, I (corresponding author), Northwestern Polytech Univ, Sch Comp Sci, Xian 710072, Shaanxi, Peoples R China.
EM ibtihajahmadkhan@gmail.com; misrarustc@mail.ustc.edu.cn;
   zain.hbku@gmail.com
RI Islam, Zain Ul/KLD-5805-2024; Ahmad, Ibtihaj/ABH-8062-2020
OI Islam, Zain Ul/0000-0001-9499-3290; Ahmad, Ibtihaj/0000-0002-6628-6967
CR Ahmad I, 2023, COMPUT BIOL MED, V157, DOI 10.1016/j.compbiomed.2023.106748
   Ahmad I, 2023, EXPERT SYST APPL, V213, DOI 10.1016/j.eswa.2022.118945
   Ahmad M.A., 2023, 2023 15 INT C EL COM, DOI [10.1109/ecai58194.2023.10193937, DOI 10.1109/ECAI58194.2023.10193937]
   Alsubaie N, 2018, PROC SPIE, V10581, DOI 10.1117/12.2293316
   Amirkhani A, 2023, IEEE T INTELL TRANSP, V24, P541, DOI 10.1109/TITS.2022.3212921
   Bolya D, 2019, IEEE I CONF COMP VIS, P9156, DOI 10.1109/ICCV.2019.00925
   Chen LC, 2017, Arxiv, DOI arXiv:1712.04837
   Chen XL, 2019, IEEE I CONF COMP VIS, P2061, DOI 10.1109/ICCV.2019.00215
   Dong JS, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102749
   Dong ZW, 2020, Arxiv, DOI [arXiv:2003.09119, 10.48550/arXiv.2003.09119]
   Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667
   Gamper J, 2020, Arxiv, DOI arXiv:2003.10778
   Goceri Evgin, 2017, International Conferences on Computer Graphics, Visualization, Computer, Vision and image Processing 2017 and Big Data Analytics, Data Mining and Computational Intelligence 2017. Proceedings, P300
   Goceri E., 2018, Celal Bayar niversitesi Fen Bilimleri Dergisi, V14, P125, DOI DOI 10.18466/CBAYARFBE.384729
   GOCERI E, 2021, IZMIR KATIP CELEBI U, V6, P91
   Goceri E, 2023, COMPUT BIOL MED, V152, DOI 10.1016/j.compbiomed.2022.106474
   Graham S, 2023, MED IMAGE ANAL, V83, DOI 10.1016/j.media.2022.102685
   Graham S, 2019, MED IMAGE ANAL, V58, DOI 10.1016/j.media.2019.101563
   He KM, 2018, Arxiv, DOI [arXiv:1703.06870, DOI 10.48550/ARXIV.1703.06870]
   Kainz P, 2015, LECT NOTES COMPUT SC, V9351, P276, DOI 10.1007/978-3-319-24574-4_33
   Kashyap R, 2022, PATTERN RECOGN LETT, V159, P157, DOI 10.1016/j.patrec.2022.04.037
   Khosravian A, 2021, EXPERT SYST APPL, V183, DOI 10.1016/j.eswa.2021.115417
   Kumar N, 2017, IEEE T MED IMAGING, V36, P1550, DOI 10.1109/TMI.2017.2677499
   Lal S, 2021, COMPUT BIOL MED, V128, DOI 10.1016/j.compbiomed.2020.104075
   Lan S., 2020, P IEEE CVF C COMP VI, P10397, DOI [10.1109/cvpr42600.2020.01041, DOI 10.1109/CVPR42600.2020.01041]
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Lee YW, 2020, Arxiv, DOI arXiv:1911.06667
   Liu JF, 2018, CELL, V173, P400, DOI 10.1016/j.cell.2018.02.052
   Liu Y, 2017, Arxiv, DOI [arXiv:1703.02442, DOI 10.48550/ARXIV.1703.02442]
   Lu C, 2018, LAB INVEST, V98, P1438, DOI 10.1038/s41374-018-0095-7
   Naylor P, 2019, IEEE T MED IMAGING, V38, P448, DOI 10.1109/TMI.2018.2865709
   Vu QD, 2019, FRONT BIOENG BIOTECH, V7, DOI 10.3389/fbioe.2019.00053
   Raza SEA, 2019, MED IMAGE ANAL, V52, P160, DOI 10.1016/j.media.2018.12.003
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Swerdlow Mark, 2023, Comput Math Methods Med, V2023, P3858997, DOI 10.1155/2023/3858997
   Xie EZ, 2020, Arxiv, DOI arXiv:1909.13226
   Xing Fuyong, 2016, IEEE Rev Biomed Eng, V9, P234, DOI 10.1109/RBME.2016.2515127
   Yang Z, 2019, IEEE I CONF COMP VIS, P9656, DOI 10.1109/ICCV.2019.00975
   Yao K., 2024, Comput. Intell., V8, P802
   Zhao BC, 2020, MED IMAGE ANAL, V65, DOI 10.1016/j.media.2020.101786
   Zhou YN, 2019, LECT NOTES COMPUT SC, V11492, P682, DOI 10.1007/978-3-030-20351-1_53
NR 41
TC 0
Z9 0
U1 2
U2 2
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUN
PY 2024
VL 146
AR 105047
DI 10.1016/j.imavis.2024.105047
EA APR 2024
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA SR5P1
UT WOS:001236193400001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Yuan, TB
   Yu, YY
   Wang, XL
AF Yuan, Tiebiao
   Yu, Yangyang
   Wang, Xiaolong
TI Semantic segmentation of large-scale point clouds by integrating
   attention mechanisms and transformer models
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Point cloud semantic segmentation; Large-scale point cloud; Transformer;
   Slot attention; Loss function
ID NETWORK; AGGREGATION; NET
AB In the current field of point cloud processing, semantic segmentation of large-scale point clouds remains a challenging problem. Traditional methods often underperform when faced with the complexity and density variations present in large-scale point cloud data. This study introduces an innovative model designed for semantic segmentation of large-scale point clouds. The model integrates a CNN -Transformer -based Context Aggregation Module with a Slot Attention mechanism to enhance the understanding of entity relationships and improve semantic segmentation performance. Furthermore, by flexibly adjusting the weight parameters within the loss function, we have optimized the training process, increasing its flexibility. In our experiments, we compared the proposed model with 55 classical and contemporary point cloud semantic segmentation models, evaluating semantic segmentation performance, computational resource consumption, inference time, and the number of model parameters. The results show a significant improvement in performance on the S3DIS and Semantic3D datasets, specifically achieving an mIoU of 71.53% and an OA of 93.92%. Ablation studies were conducted to further ascertain the contribution of each module to the overall performance of the model. This study presents an innovative approach to overcoming the challenges of large-scale point cloud semantic segmentation, offering significant contributions to the advancement of point cloud processing.
C1 [Yuan, Tiebiao; Yu, Yangyang] Tianjin Renai Coll, Tianjin 301636, Peoples R China.
   [Yu, Yangyang] Tianjin Univ, State Key Lab Engines, Tianjin 300354, Peoples R China.
   [Wang, Xiaolong] Jiachuan Digital Technol Tianjin Co Ltd, Data Ctr, Tianjin 300392, Peoples R China.
C3 Tianjin University
RP Yu, YY (corresponding author), Tianjin Renai Coll, Tianjin 301636, Peoples R China.; Yu, YY (corresponding author), Tianjin Univ, State Key Lab Engines, Tianjin 300354, Peoples R China.
EM yuantb@tjrac.edu.cn
FU Tianjin Natural Science Foundation [23JCQNJC00990]; National Natural
   Science Foundation of China [U2031142]
FX <BOLD>Funding</BOLD> This work was supported by the Tianjin Natural
   Science Foundation (NO.23JCQNJC00990) and the National Natural Science
   Foundation of China (No. U2031142) .
CR Alnaggar YA, 2021, IEEE WINT CONF APPL, P1799, DOI 10.1109/WACV48630.2021.00184
   Alonso I, 2020, IEEE ROBOT AUTOM LET, V5, P5432, DOI 10.1109/LRA.2020.3007440
   Phan AV, 2018, NEURAL NETWORKS, V108, P533, DOI 10.1016/j.neunet.2018.09.001
   Axelsson M, 2021, IEEE COMPUT SOC CONF, P4309, DOI 10.1109/CVPRW53098.2021.00487
   Chen JJ, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3168555
   Chenfeng Xu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P1, DOI 10.1007/978-3-030-58604-1_1
   Cheng H.-X., 2022, 2022 IEEE INT C MULT, P01
   Cheng R, 2021, PROC CVPR IEEE, P12542, DOI 10.1109/CVPR46437.2021.01236
   Contreras J, 2019, INT GEOSCI REMOTE SE, P5236, DOI [10.1109/IGARSS.2019.8899303, 10.1109/igarss.2019.8899303]
   Cortinhal Tiago, 2020, Advances in Visual Computing. 15th International Symposium, ISVC 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12510), P207, DOI 10.1007/978-3-030-64559-5_16
   Cui YD, 2022, IEEE T INTELL TRANSP, V23, P722, DOI 10.1109/TITS.2020.3023541
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Fang X., 2023, INT C COMP VIS APPL, P114
   Firintepe A, 2021, J IMAGING, V7, DOI 10.3390/jimaging7050080
   Gerdzhev M, 2021, IEEE INT CONF ROBOT, P9543, DOI 10.1109/ICRA48506.2021.9562041
   Hackel T., 2017, ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, VI V-1/W1, P91
   Huan Lei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11608, DOI 10.1109/CVPR42600.2020.01163
   Ibrahim M, 2023, IEEE T INTELL TRANSP, V24, P5456, DOI 10.1109/TITS.2023.3243643
   Ibrahim M, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13183621
   Jinxian Liu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P187, DOI 10.1007/978-3-030-58542-6_12
   Jovanov L, 2023, APPL OPTICS, V62, pF8, DOI 10.1364/AO.482535
   Jurgenson N., 2021, User- 607cde9d4c775-0497f57189
   Kaba Sekou-Oumar, 2023, INT C MACH LEARN, P15546
   Kochanov D, 2020, Arxiv, DOI arXiv:2007.12668
   Lee MS, 2023, IEEE WINT CONF APPL, P582, DOI 10.1109/WACV56688.2023.00065
   Leung Y.-T., 2023, Int. Arch. Photogramm. Remote. Sens. Spat. Inf. Sci., V48, P227
   Li DW, 2021, IEEE T CIRC SYST VID, V31, P2175, DOI 10.1109/TCSVT.2020.3023051
   Li HJ, 2023, Arxiv, DOI arXiv:2303.17815
   Li JN, 2023, PROC CVPR IEEE, P9425, DOI 10.1109/CVPR52729.2023.00909
   Li L, 2023, Arxiv, DOI arXiv:2309.13472
   Li YY, 2018, ADV NEUR IN, V31
   Li Y, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3162582
   Liu C, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3182776
   Liu H, 2021, IEEE T MULTIMEDIA, V23, P2045, DOI 10.1109/TMM.2020.3007331
   Liu KC, 2023, IEEE T CYBERNETICS, V53, P553, DOI 10.1109/TCYB.2022.3159815
   Liu LZ, 2023, IEEE I CONF COMP VIS, P18367, DOI 10.1109/ICCV51070.2023.01688
   Locatello F., 2020, NEURIPS, P11525
   Ma YN, 2020, IEEE WINT CONF APPL, P2920, DOI [10.1109/wacv45572.2020.9093411, 10.1109/WACV45572.2020.9093411]
   Martinovic I, 2023, Arxiv, DOI arXiv:2305.00773
   Milioto A, 2019, IEEE INT C INT ROBOT, P4213, DOI 10.1109/IROS40897.2019.8967762
   Monica R, 2020, IEEE ROBOT AUTOM LET, V5, P4695, DOI 10.1109/LRA.2020.3003883
   Morbidoni C, 2020, ACM J COMPUT CULT HE, V13, DOI 10.1145/3409262
   Park J, 2023, EXPERT SYST APPL, V212, DOI 10.1016/j.eswa.2022.118815
   Park J, 2023, PROC CVPR IEEE, P21814, DOI 10.1109/CVPR52729.2023.02089
   Qi CR, 2017, ADV NEUR IN, V30
   Qian YQ, 2023, IEEE T INTELL VEHICL, V8, P1597, DOI 10.1109/TIV.2022.3187008
   Qingyong Hu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11105, DOI 10.1109/CVPR42600.2020.01112
   Qiu S, 2021, PROC CVPR IEEE, P1757, DOI 10.1109/CVPR46437.2021.00180
   Quan TM, 2021, FRONT COMP SCI-SWITZ, V3, DOI 10.3389/fcomp.2021.613981
   Riz L, 2023, PROC CVPR IEEE, P9393, DOI 10.1109/CVPR52729.2023.00906
   Rosu RA, 2020, ROBOTICS: SCIENCE AND SYSTEMS XVI
   Shuai H, 2021, IEEE T IMAGE PROCESS, V30, P4973, DOI 10.1109/TIP.2021.3073660
   Tang LY, 2022, PROC CVPR IEEE, P8479, DOI 10.1109/CVPR52688.2022.00830
   Thabet A, 2020, IEEE COMPUT SOC CONF, P4048, DOI 10.1109/CVPRW50498.2020.00477
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Tong He, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P564, DOI 10.1007/978-3-030-58523-5_33
   Wan J, 2023, INT J GEOGR INF SCI, V37, P138, DOI 10.1080/13658816.2022.2111572
   Wang J, 2023, IEEE I CONF COMP VIS, P14284, DOI 10.1109/ICCV51070.2023.01318
   Wang ZY, 2022, PROC CVPR IEEE, P11809, DOI 10.1109/CVPR52688.2022.01152
   Warsaw JK, 2021, IEEE WINT CONF APPL, P1789, DOI 10.1109/WACV48630.2021.00183
   Wen CC, 2021, ISPRS J PHOTOGRAMM, V173, P181, DOI 10.1016/j.isprsjprs.2021.01.007
   Wu BC, 2019, IEEE INT CONF ROBOT, P4376, DOI [10.1109/ICRA.2019.8793495, 10.1109/icra.2019.8793495]
   Wu BC, 2018, IEEE INT CONF ROBOT, P1887
   Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985
   Wysocki O, 2022, ISPRS ANN PHOTO REM, V10-4, P289, DOI 10.5194/isprs-annals-X-4-W2-2022-289-2022
   Xiao AR, 2021, ISPRS J PHOTOGRAMM, V176, P237, DOI 10.1016/j.isprsjprs.2021.04.011
   Xie SN, 2018, PROC CVPR IEEE, P4606, DOI 10.1109/CVPR.2018.00484
   Xu JY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16004, DOI 10.1109/ICCV48922.2021.01572
   Xu QG, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15426, DOI 10.1109/ICCV48922.2021.01516
   Xu YT, 2023, Arxiv, DOI arXiv:2309.11228
   Xu YF, 2018, LECT NOTES COMPUT SC, V11212, P90, DOI 10.1007/978-3-030-01237-3_6
   Yan X, 2021, AAAI CONF ARTIF INTE, V35, P3101
   Yan X, 2020, PROC CVPR IEEE, P5588, DOI 10.1109/CVPR42600.2020.00563
   Yang S, 2023, REMOTE SENS-BASEL, V15, DOI 10.3390/rs15030548
   Yang YT, 2023, INT J APPL EARTH OBS, V119, DOI 10.1016/j.jag.2023.103322
   Yang Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9598, DOI 10.1109/CVPR42600.2020.00962
   Yao LL, 2020, LECT NOTES COMPUT SC, V12436, P474, DOI 10.1007/978-3-030-59861-7_48
   Yin FK, 2023, IEEE T CIRC SYST VID, V33, P4083, DOI 10.1109/TCSVT.2023.3239541
   Yoo Sunghwan, 2023, P IEEE CVF C COMP VI, P6576
   Yu J., 2023, AUTONOMOUS SYSTEMS S, P6
   Yuan MZ, 2023, IEEE I CONF COMP VIS, P17648, DOI 10.1109/ICCV51070.2023.01622
   Zhang ZX, 2020, COMPUT ANIMAT VIRT W, V31, DOI 10.1002/cav.1948
   Zhang ZY, 2019, IEEE I CONF COMP VIS, P1607, DOI 10.1109/ICCV.2019.00169
   Zhao HS, 2019, PROC CVPR IEEE, P5550, DOI 10.1109/CVPR.2019.00571
   Zheng Y, 2022, IEEE T IMAGE PROCESS, V31, P6002, DOI 10.1109/TIP.2022.3205208
   Zhou H, 2020, Arxiv, DOI arXiv:2008.01550
   Zou Q, 2022, IEEE T INTELL TRANSP, V23, P6907, DOI 10.1109/TITS.2021.3063477
NR 87
TC 0
Z9 0
U1 12
U2 12
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUN
PY 2024
VL 146
AR 105019
DI 10.1016/j.imavis.2024.105019
EA APR 2024
PG 18
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA SA0R3
UT WOS:001231627500001
DA 2024-08-05
ER

PT J
AU Sun, LF
   Li, NN
   Zhao, GF
   Wang, G
AF Sun, Linfang
   Li, Ningning
   Zhao, Guangfeng
   Wang, Gang
TI A three-dimensional human motion pose recognition algorithm based on
   graph convolutional networks
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Human motion; Pose recognition; Graph convolutional networks;
   Transformer
AB In the task of three-dimensional human motion posture recognition, there are problems such as target loss, inaccurate target positioning, and high computational complexity. This article designs a recognition evaluation algorithm to address these issues. Design a LiteHRNet model for extracting skeleton sequences from action videos, and propose a graph convolutional structure that combines residual networks and attention mechanisms. This network can effectively enhance the expression ability of node key features. Introducing second-order velocity information and spatial position information of joint points to improve positioning accuracy. Improve the TCN and Transformer network models to simultaneously extract local and long-term features throughout the entire model, and more accurately model the temporal correlation between nodes in the entire action sequence. The fusion of Transformer networks can reduce the computational complexity of the model while ensuring its accuracy. The experiment shows that the model has good evaluation performance on multiple datasets.
C1 [Sun, Linfang] Shandong Sport Univ, Coll Sports & Art, Jinan 250102, Peoples R China.
   [Li, Ningning] Shandong Sport Univ, Sch Sport Commun & Informat Technol, Jinan 205102, Peoples R China.
   [Zhao, Guangfeng] Shandong Sport Univ, Coll Sports & Hlth, Jinan 205102, Shandong, Peoples R China.
   [Wang, Gang] NingboTech Univ, Sch Comp & Data Engn, Ningbo 315100, Peoples R China.
   [Wang, Gang] Imperial Coll London, Dept Bioengn, London SW7 2AZ, England.
C3 Shandong Sport University; Shandong Sport University; Shandong Sport
   University; NingboTech University; Imperial College London
RP Zhao, GF (corresponding author), Shandong Sport Univ, Coll Sports & Hlth, Jinan 205102, Shandong, Peoples R China.
EM zhaoguangfeng@sdpei.edu.cn; gang.wang@imperial.ac.uk
CR Angelini F., 2020, Novel Methods for Posture-Based Human Action Recognition and Activity Anomalydetection
   Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Bowen Cheng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12472, DOI 10.1109/CVPR42600.2020.01249
   Carreira J, 2016, PROC CVPR IEEE, P4733, DOI 10.1109/CVPR.2016.512
   Gao Z., 2023, J. Xinxiang Univ, V40, P26
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Han Guijin, 2018, Computer Engineering and Applications, V54, P198, DOI 10.3778/j.issn.1002-8331.1709-0045
   Kim TS, 2017, IEEE COMPUT SOC CONF, P1623, DOI 10.1109/CVPRW.2017.207
   Li MS, 2019, PROC CVPR IEEE, P3590, DOI 10.1109/CVPR.2019.00371
   Liu S., 2023, Fujian Agricult. Machin, V4, P19
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Redmon J., 2018, CoRR
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Sheng Y., 2022, Modern Inform. Technol, V16, P87
   Shi L, 2019, Arxiv, DOI [arXiv:1805.07694, DOI 10.48550/ARXIV.1805.07694]
   Shi L, 2019, PROC CVPR IEEE, P12018, DOI 10.1109/CVPR.2019.01230
   Tang YS, 2018, PROC CVPR IEEE, P5323, DOI 10.1109/CVPR.2018.00558
   Wang JD, 2021, IEEE T PATTERN ANAL, V43, P3349, DOI 10.1109/TPAMI.2020.2983686
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Yan SJ, 2018, Arxiv, DOI [arXiv:1801.07455, 10.48550/arXiv.1801.07455, 10.1609/aaai.v32i1.12328, 10.48550/ARXIV.1801.07455, DOI 10.1609/AAAI.V32I1.12328]
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Zhang PF, 2020, PROC CVPR IEEE, P1109, DOI 10.1109/CVPR42600.2020.00119
   Zhang PF, 2017, IEEE I CONF COMP VIS, P2136, DOI [10.1109/ICCV.2017.233, 10.1109/ICCV.2017.231]
   Zhao J., 2023, Sci. Innovat, V17, DOI [10.15913/j.cnki.kjycx.2023.17.002,6-10+14, DOI 10.15913/J.CNKI.KJYCX.2023.17.002,6-10+14]
   Zheng Y., 2023, J. Sens. Technol, V36, P462
   Zhou X., 2019, Objects as Points, P1, DOI [10.48550/arXiv.1904.07850, DOI 10.48550/ARXIV.1904.07850]
NR 30
TC 0
Z9 0
U1 10
U2 10
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUN
PY 2024
VL 146
AR 105009
DI 10.1016/j.imavis.2024.105009
EA APR 2024
PG 10
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA RL2A9
UT WOS:001227743100001
DA 2024-08-05
ER

PT J
AU Jin, Y
   Tian, XY
   Zhang, Z
   Liu, P
   Tang, XL
AF Jin, Ye
   Tian, Xiaoyan
   Zhang, Zhao
   Liu, Peng
   Tang, Xianglong
TI C2F: An effective coarse-to-fine network for video summarization
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Video summarization; Coarse -to -fine network; Multiscale
   representation; Local adaptive loss
AB The objective of video summarization is to develop a concise and condensed summary that accurately captures the original video content. The methods currently used to summarize supervised videos and consider the task a sequence-to-sequence problem. However, modeling the order of long videos presents three challenges: (1) capturing both local and global relationships simultaneously is challenging; (2) the boundaries of video highlight segments are often incorrectly located, indicating that semantic integrity is incomplete; (3) efficient relation computing is difficult to do well. We design a novel coarse-to-fine network (C2F) for video summarization adapted to the multi-level semantic video structure, thus addressing these limitations. The multiscale representation scheme initially captures different scales of temporal relationships for the coarse classification results; Meanwhile, the action-wise proposal module is intended to provide the fine prediction of importance scores and regress the temporal locations of key-frames. In addition, a loss function is proposed to identify local differences among frames and analyze combinations of various loss functions. Extensive experimental results on two benchmark datasets have demonstrated that the proposed C2F achieves significant performance compared with state-of-the-art methods, and performs satisfactorily in efficient relation computing. For example, on the TVSum dataset, we improve the F-score from 69.4% to 72.8% by 3.4%. Furthermore, C2F includes 4.7 M parameters, accounting for only 10.7% of the parameters used in the SASUM model.
C1 [Jin, Ye; Tian, Xiaoyan; Liu, Peng; Tang, Xianglong] Harbin Inst Technol, Fac Comp, Harbin 150001, Peoples R China.
   [Zhang, Zhao] Harbin Inst Technol, Sch Instrument Sci & Engn, Harbin 150001, Peoples R China.
C3 Harbin Institute of Technology; Harbin Institute of Technology
RP Tian, XY (corresponding author), Harbin Inst Technol, Fac Comp, Harbin 150001, Peoples R China.
EM jinye@hit.edu.cn; tianxy@stu.hit.edu.cn; zhangzhao@stu.hit.edu.cn;
   pengliu@hit.edu.cn; tangxl@hit.edu.cn
RI Tian, Xiaoyan/HGD-8506-2022
OI Tian, Xiaoyan/0000-0001-9278-1032
FU National Natural Science Foundation of China [51935005]; Basic
   Scientific Research Project [JCKY20200603C010]; Natural Science
   Foundation of Heilongjiang Province of China [LH2021F023]
FX This work was supported by the National Natural Science Foundation of
   China (Grant Number: 51935005) , Basic Scientific Research Project
   (Grant Number: JCKY20200603C010) , and Natural Science Foundation of
   Heilongjiang Province of China (Grant Number: LH2021F023) .
CR Apostolidis E, 2021, P IEEE, V109, P1838, DOI 10.1109/JPROC.2021.3117472
   Aziere N, 2022, IMAGE VISION COMPUT, V128, DOI 10.1016/j.imavis.2022.104567
   Cao Z., 2007, P 24 INT C MACH LEAR, P129
   Chen YS, 2022, APPL INTELL, V52, P17864, DOI 10.1007/s10489-022-03451-1
   Cong Y, 2012, IEEE T MULTIMEDIA, V14, P66, DOI 10.1109/TMM.2011.2166951
   Das SK, 2023, IMAGE VISION COMPUT, V138, DOI 10.1016/j.imavis.2023.104809
   Du GT, 2020, J IMAGING SCI TECHN, V64, DOI 10.2352/J.ImagingSci.Technol.2020.64.2.020508
   de Avila SEF, 2011, PATTERN RECOGN LETT, V32, P56, DOI 10.1016/j.patrec.2010.08.004
   Galiyawala H, 2019, IMAGE VISION COMPUT, V92, DOI 10.1016/j.imavis.2019.10.002
   Gandapur MQ, 2022, IMAGE VISION COMPUT, V123, DOI 10.1016/j.imavis.2022.104467
   Gaviao W, 2007, IMAGE VISION COMPUT, V25, P70, DOI 10.1016/j.imavis.2006.01.003
   Gygli M, 2014, LECT NOTES COMPUT SC, V8695, P505, DOI 10.1007/978-3-319-10584-0_33
   He XF, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2296, DOI 10.1145/3343031.3351056
   Hsu TC, 2023, IEEE T IMAGE PROCESS, V32, P3013, DOI 10.1109/TIP.2023.3275069
   Ji Z, 2020, NEUROCOMPUTING, V405, P200, DOI 10.1016/j.neucom.2020.04.132
   Ji Z, 2021, IEEE T NEUR NET LEAR, V32, P1765, DOI 10.1109/TNNLS.2020.2991083
   Ji Z, 2020, IEEE T CIRC SYST VID, V30, P1709, DOI 10.1109/TCSVT.2019.2904996
   Kuanar SK, 2013, J VIS COMMUN IMAGE R, V24, P1212, DOI 10.1016/j.jvcir.2013.08.003
   Li P, 2021, PATTERN RECOGN, V111, DOI 10.1016/j.patcog.2020.107677
   Li S., 2023, ICASSP 2023 2023 IEE, P1
   Li XL, 2018, IEEE T CYBERNETICS, V48, P1923, DOI 10.1109/TCYB.2017.2718579
   Liang GQ, 2022, PATTERN RECOGN, V131, DOI 10.1016/j.patcog.2022.108840
   Liang GQ, 2022, NEUROCOMPUTING, V467, P1, DOI 10.1016/j.neucom.2021.09.015
   Lin JX, 2022, COMPUT ELECTR ENG, V97, DOI 10.1016/j.compeleceng.2021.107618
   Lin TW, 2018, LECT NOTES COMPUT SC, V11208, P3, DOI 10.1007/978-3-030-01225-0_1
   Liu Y, 2022, PROC CVPR IEEE, P3032, DOI 10.1109/CVPR52688.2022.00305
   Mahasseni B, 2017, PROC CVPR IEEE, P2982, DOI 10.1109/CVPR.2017.318
   Narasimhan M, 2021, 35 C NEURAL INFORM P, V34
   Park J, 2019, IEEE INT CONF COMP V, P1545, DOI 10.1109/ICCVW.2019.00193
   Potapov D, 2014, LECT NOTES COMPUT SC, V8694, P540, DOI 10.1007/978-3-319-10599-4_35
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rochan M, 2019, PROC CVPR IEEE, P7894, DOI 10.1109/CVPR.2019.00809
   Rochan M, 2018, LECT NOTES COMPUT SC, V11216, P358, DOI 10.1007/978-3-030-01258-8_22
   Song YL, 2015, PROC CVPR IEEE, P5179, DOI 10.1109/CVPR.2015.7299154
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tian XY, 2023, PATTERN ANAL APPL, V26, P1375, DOI 10.1007/s10044-023-01166-8
   Tian XY, 2023, MULTIMEDIA SYST, V29, P615, DOI 10.1007/s00530-022-00998-4
   Vaswani A, 2017, ADV NEUR IN, V30
   Wei HW, 2018, AAAI CONF ARTIF INTE, P216
   Xiao SW, 2020, IEEE T IMAGE PROCESS, V29, P5889, DOI 10.1109/TIP.2020.2985868
   Yunjae Jung, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P167, DOI 10.1007/978-3-030-58595-2_11
   Zhang K, 2016, LECT NOTES COMPUT SC, V9911, P766, DOI 10.1007/978-3-319-46478-7_47
   Zhang Y., 2023, IEEE Trans. Circuits Syst. Video Technol.
   Zhang Z, 2023, IEEE T INSTRUM MEAS, V72, DOI 10.1109/TIM.2023.3282301
   Zhao B, 2023, IEEE T NEUR NET LEAR, V34, P5181, DOI 10.1109/TNNLS.2021.3119969
   Zhao B, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P863, DOI 10.1145/3123266.3123328
   Zhao ZP, 2023, IMAGE VISION COMPUT, V136, DOI 10.1016/j.imavis.2023.104715
   Zhou KY, 2018, AAAI CONF ARTIF INTE, P7582
   Zhu WC, 2022, IEEE T IMAGE PROCESS, V31, P3017, DOI 10.1109/TIP.2022.3163855
   Zhu WC, 2022, PATTERN RECOGN, V122, DOI 10.1016/j.patcog.2021.108312
   Zhu WC, 2021, IEEE T IMAGE PROCESS, V30, P948, DOI 10.1109/TIP.2020.3039886
NR 51
TC 1
Z9 1
U1 1
U2 1
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD APR
PY 2024
VL 144
AR 104962
DI 10.1016/j.imavis.2024.104962
EA MAR 2024
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA NK0I6
UT WOS:001200226200001
DA 2024-08-05
ER

PT J
AU Islam, MK
   Rahman, MM
   Ali, MS
   Mahim, SM
   Miah, MS
AF Islam, Md Khairul
   Rahman, Md Mahbubur
   Ali, Md Shahin
   Mahim, S. M.
   Miah, Md Sipon
TI Enhancing lung abnormalities diagnosis using hybrid DCNN-ViT-GRU model
   with explainable AI: A deep learning approach
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Lung cancer; COVID-19; Pre-processing; Feature extraction; DCNN-ViT-GRU;
   Explainable AI
ID CLASSIFICATION
AB In this study, we propose a novel approach called DCNN-ViT-GRU, which combines deep Convolutional Neural Networks (CNNs) with Gated Recurrent Units (GRUs) and the Vision Transformer (ViT) model for the accurate detection and classification of lung abnormalities. By leveraging the strengths of both CNNs and the ViT model, our architecture automatically extracts meaningful features from lung images, leading to improved diagnostic capabilities. The DCNN-ViT-GRU model utilizes a combination of deep CNN and GRU layers, allowing it to effectively capture local and global patterns. This comprehensive feature representation enhances the model's ability to identify various abnormalities in lung images, including lung cancer, COVID-19, and pneumonia. To further enhance the interpretability and transparency of our model, we integrate Explainable Artificial Intelligence (XAI) techniques, including LIME and SHAP. This integration provides valuable insights into the decisionmaking process of the DCNN-ViT-GRU model, enabling clinicians to understand and validate the predictions made by the model. We evaluated the performance of our proposed approach on diverse datasets containing cases of lung abnormalities. Through cross-validation, our DCNN-ViT-GRU model achieved impressive weighted mean accuracy of 99% and 99.86% for two distinct datasets, demonstrating its superior performance. Furthermore, in hold-out validation on separate datasets, the model achieved accuracies of 99.09% and 99.87%, respectively. Integrating the XAI techniques enhances the interpretability of the DCNN-ViT-GRU model and provides clinicians with valuable insights regarding the factors contributing to the diagnosis of lung abnormalities. This approach presents a promising solution for accurate and interpretable lung abnormalities detection and classification, with potential implications for improved patient care and treatment planning.
C1 [Islam, Md Khairul; Ali, Md Shahin; Mahim, S. M.] Islamic Univ, Dept Biomed Engn, Kushtia 7003, Bangladesh.
   [Rahman, Md Mahbubur; Miah, Md Sipon] Islamic Univ, Dept Informat & Commun Technol, Kushtia 7003, Bangladesh.
   [Miah, Md Sipon] Univ Carlos III Madrid, Dept Signal Theory & Commun, Madrid 28911, Spain.
C3 Islamic University; Islamic University; Universidad Carlos III de Madrid
RP Islam, MK (corresponding author), Islamic Univ, Dept Biomed Engn, Kushtia 7003, Bangladesh.; Rahman, MM (corresponding author), Islamic Univ, Dept Informat & Commun Technol, Kushtia 7003, Bangladesh.
EM khairul.ice06@gmail.com; mrahman@ict.iu.ac.bd
RI Rahman, Mahbubur/AAX-4756-2020; Ali, Md Shahin/AEV-7853-2022; Miah,
   Sipon/AAK-5397-2020; Islam, Md Khairul/AGF-7094-2022
OI Rahman, Mahbubur/0000-0001-6994-2598; Ali, Md
   Shahin/0000-0003-2564-8746; Miah, Sipon/0000-0002-6986-1517; Islam, Md
   Khairul/0000-0002-6973-1536
FU Bio-Imaging Research Lab, Department of Biomedical Engineering, Islamic
   University [Kushtia-7003]
FX We would like to acknowledge the support provided by the Bio- Imaging
   Research Lab, Department of Biomedical Engineering, Islamic University,
   Kushtia-7003, Bangladesh, in carrying out our research successfully.
CR Abdullah TAA, 2023, PROCESSES, V11, DOI 10.3390/pr11020595
   Ahmed M. S., 2023, 2023 INT C ADV TECHN, P1, DOI [10.1109/ICONAT57137.2023.10080480, DOI 10.1109/ICONAT57137.2023.10080480]
   Ahsan MM, 2023, EXPERT SYST APPL, V216, DOI 10.1016/j.eswa.2022.119483
   Akinbi A., 2023, Password-sniffing acoustic keylogger using machine learning
   Al-Badri AH, 2022, IEEE ACCESS, V10, P90940, DOI 10.1109/ACCESS.2022.3200603
   AL-Huseiny M.S., 2021, Indonesian Journal of Electrical Engineering and Computer Science, V22, P1078, DOI DOI 10.11591/IJEECS.V22.I2.PP1078-1086
   Al-Yasriy H.F., 2020, IOP Conference Series: Materials Science and Engineering
   Alam MS, 2019, BIG DATA COGN COMPUT, V3, DOI 10.3390/bdcc3020027
   Ali Md Shahin, 2021, 2021 1st International Conference on Artificial Intelligence and Data Analytics (CAIDA), P1, DOI 10.1109/CAIDA51941.2021.9425212
   Ali MS, 2023, BIOMED RES INT, V2023, DOI 10.1155/2023/8583210
   Ali MS, 2021, MACH LEARN APPL, V5, DOI 10.1016/j.mlwa.2021.100036
   Alyasriy Hamdalla, 2021, Mendeley Data, VV2
   Antoniadi AM, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11115088
   Apostolopoulos ID, 2022, REPORTS-BASEL, V5, DOI 10.3390/reports5020020
   Arafa A, 2022, J KING SAUD UNIV-COM, V34, P5059, DOI 10.1016/j.jksuci.2022.06.005
   Asuntha A, 2020, MULTIMED TOOLS APPL, V79, P7731, DOI 10.1007/s11042-019-08394-3
   Ayalew AM, 2022, BIOMED SIGNAL PROCES, V74, DOI 10.1016/j.bspc.2022.103530
   Baek JW, 2023, CMC-COMPUT MATER CON, V74, P6573, DOI 10.32604/cmc.2023.035246
   Berner ES, 2008, AM J MED, V121, P2, DOI 10.1016/j.amjmed.2008.01.001
   Bhandari M, 2022, COMPUT BIOL MED, V150, DOI 10.1016/j.compbiomed.2022.106156
   Chen A, 2020, RADIOGRAPHICS, V40, P28, DOI 10.1148/rg.2020190099
   Choe J, 2022, RADIOLOGY, V302, P187, DOI 10.1148/radiol.2021204164
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Das A., 2020, arXiv, DOI DOI 10.48550/ARXIV.2006.11371
   Deperlioglu O, 2022, FUTURE GENER COMP SY, V129, P152, DOI 10.1016/j.future.2021.11.018
   Dorj UO, 2018, MULTIMED TOOLS APPL, V77, P9909, DOI 10.1007/s11042-018-5714-1
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Duranta D., 2023, Machine Learn Appl
   Dwivedi K, 2023, COMPUT BIOL MED, V153, DOI 10.1016/j.compbiomed.2023.106544
   Farda NA, 2021, INJURY, V52, P616, DOI 10.1016/j.injury.2020.09.010
   Främling K, 2021, LECT NOTES ARTIF INT, V12688, P39, DOI 10.1007/978-3-030-82017-6_3
   Gilpin LH, 2018, PR INT CONF DATA SC, P80, DOI 10.1109/DSAA.2018.00018
   Gumma L. N., 2022, SN Computer Science, V3, P1, DOI DOI 10.1007/S42979-021-00887-Z
   Hasan MI, 2022, J HEALTHC ENG, V2022, DOI 10.1155/2022/5269913
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hossain M.M., 2023, Inform. Med. Unlocked, V42
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang LW, 2022, ULTRASONICS, V121, DOI 10.1016/j.ultras.2022.106685
   Islam Md Khairul, 2021, 2021 1st International Conference on Artificial Intelligence and Data Analytics (CAIDA), P48, DOI 10.1109/CAIDA51941.2021.9425117
   Islam M.K., 2023, Machine Learn Appl, V14
   Islam M. K., 2020, International Journal of Scientific Engineering Research, V11, P6
   Islam MK, 2021, MACH LEARN APPL, V5, DOI 10.1016/j.mlwa.2021.100044
   Jena SR, 2021, NEURAL COMPUT APPL, V33, P15601, DOI 10.1007/s00521-021-06182-5
   Jiang ZC, 2021, COMPUT INTEL NEUROSC, V2021, DOI 10.1155/2021/7529893
   Kareem H.F., 2021, Indonesian J. Electr. Eng. Comput. Sci., V21, P1731, DOI DOI 10.11591/IJEECS.V21.I3.PP1731-1738
   Keane MT, 2020, LECT NOTES ARTIF INT, V12311, P163, DOI 10.1007/978-3-030-58342-2_11
   Lee T.F., 2023, Epidemiol Health
   Mary Shyni H, 2022, Comput Methods Programs Biomed Update, V2, P100054, DOI 10.1016/j.cmpbup.2022.100054
   Mohamed T.I.A., 2022, Ph.D. thesis
   Monshi MMA, 2021, COMPUT BIOL MED, V133, DOI 10.1016/j.compbiomed.2021.104375
   Nagaraj V., 2022, P INT C SUST COMP DA, P1472, DOI DOI 10.1109/ICSCDS53736.2022.9760847
   Naseer I, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22124426
   Parvaiz A, 2023, ENG APPL ARTIF INTEL, V122, DOI 10.1016/j.engappai.2023.106126
   Pian WJ, 2021, INFORM PROCESS MANAG, V58, DOI 10.1016/j.ipm.2021.102713
   Pölsterl S, 2016, ARTIF INTELL MED, V72, P1, DOI 10.1016/j.artmed.2016.07.004
   Rajalakshmi V., 2022, P 2 INT C ART INT AD, P407
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Soda P, 2021, MED IMAGE ANAL, V74, DOI 10.1016/j.media.2021.102216
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tian SF, 2020, J THORAC ONCOL, V15, P700, DOI 10.1016/j.jtho.2020.02.010
   van der Velden BHM, 2022, MED IMAGE ANAL, V79, DOI 10.1016/j.media.2022.102470
   Vishwakarma R, 2020, IEEE INT CONF BIG DA, P5609, DOI 10.1109/BigData50022.2020.9377902
   Wang LB, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3186634
   Wu CK, 2022, CLUSTER COMPUT, V25, P2715, DOI 10.1007/s10586-021-03439-5
   Yao T, 2023, IEEE T PATTERN ANAL, V45, P10870, DOI 10.1109/TPAMI.2023.3268446
   Yu J, 2020, J HEALTHC ENG, V2020, DOI 10.1155/2020/1051394
NR 66
TC 2
Z9 2
U1 4
U2 4
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD FEB
PY 2024
VL 142
AR 104918
DI 10.1016/j.imavis.2024.104918
EA JAN 2024
PG 18
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA JJ0T4
UT WOS:001172687000001
DA 2024-08-05
ER

PT J
AU Bigioi, D
   Basak, S
   Stypulkowski, M
   Zieba, M
   Jordan, H
   Mcdonnell, R
   Corcoran, P
AF Bigioi, Dan
   Basak, Shubhajit
   Stypulkowski, Michal
   Zieba, Maciej
   Jordan, Hugh
   Mcdonnell, Rachel
   Corcoran, Peter
TI Speech driven video editing via an audio-conditioned diffusion model
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Video editing; Talking head generation; Generative AI; Diffusion models;
   Dubbing
AB Taking inspiration from recent developments in visual generative tasks using diffusion models, we propose a method for end-to-end speech-driven video editing using a denoising diffusion model. Given a video of a talking person, and a separate auditory speech recording, the lip and jaw motions are re-synchronised without relying on intermediate structural representations such as facial landmarks or a 3D face model. We show this is possible by conditioning a denoising diffusion model on audio mel spectral features to generate synchronised facial motion. Proof of concept results are demonstrated on both single -speaker and multi-speaker video editing, providing a baseline model on the CREMA-D audiovisual data set. To the best of our knowledge, this is the first work to demonstrate and validate the feasibility of applying end-to-end denoising diffusion models to the task of audiodriven video editing. All code, datasets, and models used as part of this work are made publicly available here: https://danbigioi.github.io/DiffusionVideoEditing/.
C1 [Bigioi, Dan; Basak, Shubhajit; Corcoran, Peter] Univ Galway, Galway, Ireland.
   [Stypulkowski, Michal] Univ Wroclaw, Wroclaw, Poland.
   [Zieba, Maciej] Wroclaw Univ Sci & Technol, Wroclaw, Poland.
   [Zieba, Maciej] Tooploox, Wroclaw, Poland.
   [Jordan, Hugh; Mcdonnell, Rachel] Trinity Coll Dublin, Dublin, Ireland.
C3 Ollscoil na Gaillimhe-University of Galway; University of Wroclaw;
   Wroclaw University of Science & Technology; Trinity College Dublin
RP Bigioi, D (corresponding author), Univ Galway, Galway, Ireland.
EM d.bigioi1@universityofgalway.ie; s.basak1@universityofgalway.ie;
   michal.stypulkowski@cs.uni.wroc.pl; maciej.zieba@pwr.edu.pl;
   jordanhu@tcd.ie; ramcdonn@tcd.ie; peter.corcoran@universityofgalway.ie
FU Science Foundation Ireland Centre for Research Training in
   Digitally-Enhanced Reality (d-real) [18/CRT/6224]; ADAPT Centre
   [13/RC/2106]
FX <STRONG> </STRONG>This work has the financial support of the Science
   Foundation Ireland Centre for Research Training in Digitally-Enhanced
   Reality (d-real) under Grant No. 18/CRT/6224, and the ADAPT Centre
   (Grant 13/RC/2106) .
CR Amodei D, 2016, PR MACH LEARN RES, V48
   Avrahami O, 2022, PROC CVPR IEEE, P18187, DOI 10.1109/CVPR52688.2022.01767
   Baevski A, 2020, Advances in neural information processing systems, V33, P12449, DOI 10.5555/3495724.3496768
   Batzolis G, 2021, Arxiv, DOI arXiv:2111.13606
   Biswas S., 2021, P 12 IND C COMP VIS, P1
   Cao HW, 2014, IEEE T AFFECT COMPUT, V5, P377, DOI 10.1109/TAFFC.2014.2336244
   Chen LL, 2019, PROC CVPR IEEE, P7824, DOI 10.1109/CVPR.2019.00802
   Chen LL, 2018, LECT NOTES COMPUT SC, V11211, P538, DOI 10.1007/978-3-030-01234-2_32
   Chen NX, 2020, Arxiv, DOI arXiv:2009.00713
   Chen S., 2022, arXiv
   Chung JS, 2017, LECT NOTES COMPUT SC, V10117, P251, DOI 10.1007/978-3-319-54427-4_19
   Chung JS, 2017, PROC CVPR IEEE, P3444, DOI 10.1109/CVPR.2017.367
   Cooke M, 2006, J ACOUST SOC AM, V120, P2421, DOI 10.1121/1.2229005
   Cudeiro D, 2019, PROC CVPR IEEE, P10093, DOI 10.1109/CVPR.2019.01034
   Dhariwal P, 2021, ADV NEUR IN, V34
   Eskimez SE, 2020, INT CONF ACOUST SPEE, P1948, DOI [10.1109/ICASSP40776.2020.9054103, 10.1109/icassp40776.2020.9054103]
   Eskimez SE, 2018, LECT NOTES COMPUT SC, V10891, P372, DOI 10.1007/978-3-319-93764-9_35
   Fan W.C., 2023, P AAAI C ART INT, V37
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Gu SY, 2022, PROC CVPR IEEE, P10686, DOI 10.1109/CVPR52688.2022.01043
   Harvey W, 2022, ADV NEURAL INF PROCE, P27953
   Hensel M, 2017, ADV NEUR IN, V30
   Ho J., 2020, Adv. Neural. Inf. Process. Syst, V33, P6840, DOI DOI 10.48550/ARXIV.2006.11239
   Ho JAT, 2022, Arxiv, DOI [arXiv:2204.03458, DOI 10.48550/ARXIV.2204.03458,ARXIV]
   Ho J, 2022, J MACH LEARN RES, V23, P1
   Huang RJ, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P2595, DOI 10.1145/3503161.3547855
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jamaludin A, 2019, INT J COMPUT VISION, V127, P1767, DOI 10.1007/s11263-019-01150-y
   Ji X., 2022, ACM SIGGRAPH 2022 C, P1
   Ji XY, 2021, PROC CVPR IEEE, P14075, DOI 10.1109/CVPR46437.2021.01386
   Karras T, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073658
   Kim Sungwon, 2022, Guided-TTS 2: A Diffusion Model for High-quality Adaptive Text-to-Speech with Untranscribed Data
   Kong Z., 2020, arXiv, DOI DOI 10.48550/ARXIV.2009.09761
   Kumar N, 2020, IEEE COMPUT SOC CONF, P3334, DOI 10.1109/CVPRW50498.2020.00393
   Lahiri A, 2021, PROC CVPR IEEE, P2754, DOI 10.1109/CVPR46437.2021.00278
   Lele Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P35, DOI 10.1007/978-3-030-58545-7_3
   Levkovitch A, 2022, Arxiv, DOI arXiv:2206.02246
   Liu CX, 2022, PROC CVPR IEEE, P5677, DOI 10.1109/CVPR52688.2022.00560
   Lu YX, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480484
   Lugaresi C, 2019, Arxiv, DOI arXiv:1906.08172
   Lugmayr A, 2022, PROC CVPR IEEE, P11451, DOI 10.1109/CVPR52688.2022.01117
   Meng C., 2021, arXiv
   Mittal G, 2020, IEEE WINT CONF APPL, P3279, DOI [10.1109/WACV45572.2020.9093527, 10.1109/wacv45572.2020.9093527]
   Nagrani A, 2018, Arxiv, DOI [arXiv:1706.08612, DOI 10.21437/INTERSPEECH.2017-950]
   Narvekar ND, 2011, IEEE T IMAGE PROCESS, V20, P2678, DOI 10.1109/TIP.2011.2131660
   Nichol A, 2021, PR MACH LEARN RES, V139
   Nichol A, 2022, PR MACH LEARN RES
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   Popov V, 2021, PR MACH LEARN RES, V139
   Prajwal KR, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P484, DOI 10.1145/3394171.3413532
   Preechakul K, 2022, PROC CVPR IEEE, P10609, DOI 10.1109/CVPR52688.2022.01036
   Radford A, 2023, P 40 INT C MACH LEAR, P28492, DOI DOI 10.5555/3618408.3619590
   Ramesh A., 2022, Hierarchical text-conditional image generation with clip latents, DOI 10.48550/arXiv.2204.06125
   Richard A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1153, DOI 10.1109/ICCV48922.2021.00121
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Ruiz N, 2023, PROC CVPR IEEE, P22500, DOI 10.1109/CVPR52729.2023.02155
   Saharia C., 2022, ADV NEURAL INF PROCE, V35, P36479
   Saharia C., 2022, ACM SIGGRAPH 2022 C, P1
   Saharia C, 2023, IEEE T PATTERN ANAL, V45, P4713, DOI 10.1109/TPAMI.2022.3204461
   Shen S, 2023, PROC CVPR IEEE, P1982, DOI 10.1109/CVPR52729.2023.00197
   Sohl-Dickstein J, 2015, PR MACH LEARN RES, V37, P2256
   Song LS, 2022, IEEE T INF FOREN SEC, V17, P585, DOI 10.1109/TIFS.2022.3146783
   Song LC, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P478, DOI 10.1145/3474085.3475196
   Song Y, 2019, ADV NEUR IN, V32
   Song Y, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P919
   Stypulkowski M., 2024, P IEEECVF WINTER C A, P5091
   Suwajanakorn S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073640
   Tae J, 2022, Arxiv, DOI arXiv:2110.02584
   Taylor S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073699
   Thies Justus, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P716, DOI 10.1007/978-3-030-58517-4_42
   Vougioukas K., 2018, arXiv
   Vougioukas K, 2020, INT J COMPUT VISION, V128, P1398, DOI 10.1007/s11263-019-01251-8
   Wang Suzhen, 2021, arXiv
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wen X, 2020, IEEE T VIS COMPUT GR, V26, P3457, DOI 10.1109/TVCG.2020.3023573
   Wu HZ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1478, DOI 10.1145/3474085.3475280
   Xiao Z., 2021, arXiv
   Yang Dongchao, 2023, IEEE/ACM Transactions on Audio, Speech, and Language Processing
   Yang L, 2024, ACM COMPUT SURV, V56, DOI 10.1145/3626235
   Yang RH, 2022, Arxiv, DOI arXiv:2203.09481
   Yi R, 2020, Arxiv, DOI arXiv:2002.10137
   Zhang CX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3847, DOI 10.1109/ICCV48922.2021.00384
   Zhang MY, 2022, Arxiv, DOI arXiv:2208.15001
   Zhang ZM, 2021, PROC CVPR IEEE, P3660, DOI 10.1109/CVPR46437.2021.00366
   Zhao RQ, 2021, IEEE INT CONF COMP V, P1991, DOI 10.1109/ICCVW54120.2021.00226
   Zhou H, 2021, PROC CVPR IEEE, P4174, DOI 10.1109/CVPR46437.2021.00416
   Zhou H, 2019, AAAI CONF ARTIF INTE, P9299
   Zhou Y, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417774
   Zhu H, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2362
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 91
TC 0
Z9 0
U1 6
U2 6
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD FEB
PY 2024
VL 142
AR 104911
DI 10.1016/j.imavis.2024.104911
EA JAN 2024
PG 10
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA JQ2U8
UT WOS:001174573800001
OA hybrid, Green Submitted
DA 2024-08-05
ER

PT J
AU Liu, C
   Li, HQ
   He, X
   Yang, GZ
   Li, ZY
AF Liu, Chang
   Li, Haoqi
   He, Xuan
   Yang, Guanzhong
   Li, Zhiyong
TI Attribute discrimination combined with selected sample dropout for
   unsupervised domain adaptive person re-identification
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Person re -identification; Unsupervised learning; Domain adaptation;
   Pseudo label noise; Gender attribute recognition
ID UNCERTAINTY
AB The popular clustering-based Unsupervised Domain Adaptive (UDA) person re-identification (re-ID) does not require additional annotation. However, owing to unsatisfactory feature embedding and imperfect clustering, most existing clustering-based methods suffer from the noise of pseudo labels in the target domain, which will lead to a serious performance degradation. To reduce the negative impact of noisy pseudo labels on training, we put forward an approach named selected sample dropout (SSD) in the training stage, which defines a criterion for evaluating the noise level of pseudo labels. SSD would mine and discard samples with noisy pseudo labels before training, and then all the remaining samples are fed into the network to train. On top of a strong baseline, SSD is proved to be effective. We call the baseline of adding SSD as the selected sample dropout-enhanced teacherstudent network (SSD-TSNet). In addition, considering the robustness of pedestrian gender, we use it as auxiliary information in the SSD-TSNet test stage. Specifically, we propose a pedestrian gender attribute discriminator (GAD) to predict gender labels. Based on predictive gender labels, SSD-TSNet could retrieve one person among other persons with the same gender as the person, thus narrowing the search space of re-ID. The proposed SSDTSNet and GAD are integrated into one framework, and extensive experiments on four widely used UDA benchmark protocols demonstrate its competitive performance. Specifically, our method outperforms the baseline by 11.6% mAP on the Duke-to-Market task, while surpassing the state-of-the-art method by 0.5% mAP on the Market-to-Duke task.
C1 [Liu, Chang; Li, Haoqi; He, Xuan; Yang, Guanzhong; Li, Zhiyong] Hunan Univ, Coll Comp Sci & Elect Engn, Changsha, Hunan, Peoples R China.
C3 Hunan University
RP Li, ZY (corresponding author), Hunan Univ, Coll Comp Sci & Elect Engn, Changsha, Hunan, Peoples R China.
EM liuxiaochang@hnu.edu.cn; haoqili@hnu.edu.cn; gzyang@hnu.edu.cn;
   zhiyong.li@hnu.edu.cn
FU National Natural Science Foundation of China [U21A20518, U23A20341]
FX This work was partially supported by National Natural Science Foundation
   of China (No. U21A20518, No. U23A20341) .
CR Bai ZC, 2021, PROC CVPR IEEE, P12909, DOI 10.1109/CVPR46437.2021.01272
   Cheng DQ, 2022, IMAGE VISION COMPUT, V124, DOI 10.1016/j.imavis.2022.104493
   Cheng H, 2021, Arxiv, DOI arXiv:2010.02347
   Dai YX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11844, DOI 10.1109/ICCV48922.2021.01165
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng WJ, 2018, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.2018.00110
   Dong XY, 2019, IEEE I CONF COMP VIS, P783, DOI 10.1109/ICCV.2019.00087
   Dong XY, 2019, IEEE T PATTERN ANAL, V41, P1641, DOI 10.1109/TPAMI.2018.2844853
   Fan HH, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3243316
   Fang Zhao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P526, DOI 10.1007/978-3-030-58621-8_31
   Fu Y, 2019, IEEE I CONF COMP VIS, P6111, DOI 10.1109/ICCV.2019.00621
   Ge Y., 2020, ICLR
   Ge Y., 2020, Advances in Neural Information Processing Systems (NeurlPS), V33, p11 309
   Ge YX, 2024, IEEE T NEUR NET LEAR, V35, P258, DOI 10.1109/TNNLS.2022.3173489
   Ghosh A, 2017, AAAI CONF ARTIF INTE, P1919
   Guangyi Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P643, DOI 10.1007/978-3-030-58598-3_38
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   Han J, 2022, AAAI CONF ARTIF INTE, P790
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Hu ZD, 2022, AAAI CONF ARTIF INTE, P980
   Huang YR, 2024, Arxiv, DOI arXiv:1905.10529
   Jia J, 2021, Arxiv, DOI arXiv:2107.03576
   Jianing Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P483, DOI 10.1007/978-3-030-58586-0_29
   Lee KH, 2018, PROC CVPR IEEE, P5447, DOI 10.1109/CVPR.2018.00571
   Li QZ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P833
   Li SH, 2022, IEEE T CIRC SYST VID, V32, P3825, DOI 10.1109/TCSVT.2021.3118060
   Li YY, 2021, IEEE T IMAGE PROCESS, V30, P7952, DOI 10.1109/TIP.2021.3112039
   Li ZY, 2022, AAAI CONF ARTIF INTE, P1527
   Lin P., 2021, arXiv
   Lin YT, 2020, IEEE T IMAGE PROCESS, V29, P5481, DOI 10.1109/TIP.2020.2982826
   Lin YT, 2019, PATTERN RECOGN, V95, P151, DOI 10.1016/j.patcog.2019.06.006
   Liu L., 2021, arXiv
   Ning X, 2021, IEEE T CIRC SYST VID, V31, P3391, DOI 10.1109/TCSVT.2020.3043026
   Pang ZQ, 2022, IEEE T CIRC SYST VID, V32, P3164, DOI 10.1109/TCSVT.2021.3103753
   Qin ZQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P763, DOI 10.1109/ICCV48922.2021.00082
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Roxo T, 2022, IEEE ACCESS, V10, P28122, DOI 10.1109/ACCESS.2022.3157857
   Shu J, 2019, ADV NEUR IN, V32
   Song XL, 2022, INT J MACH LEARN CYB, V13, P255, DOI 10.1007/s13042-021-01399-1
   Tan ZC, 2019, IEEE T IMAGE PROCESS, V28, P6126, DOI 10.1109/TIP.2019.2919199
   Tang C., 2023, Multimed. Tools Appl., P1
   Wang D, 2020, PROC CVPR IEEE, P3950, DOI 10.1109/CVPR42600.2020.00401
   Wang GA, 2020, PROC CVPR IEEE, P6448, DOI 10.1109/CVPR42600.2020.00648
   Wang JY, 2018, PROC CVPR IEEE, P2275, DOI 10.1109/CVPR.2018.00242
   Wang PF, 2023, IEEE T MULTIMEDIA, V25, P2624, DOI 10.1109/TMM.2022.3149629
   Wang X, 2022, PATTERN RECOGN, V121, DOI 10.1016/j.patcog.2021.108220
   Wang XB, 2019, IEEE I CONF COMP VIS, P9357, DOI 10.1109/ICCV.2019.00945
   Wang X, 2020, PROC CVPR IEEE, P6387, DOI 10.1109/CVPR42600.2020.00642
   Wang YS, 2019, IEEE I CONF COMP VIS, P322, DOI 10.1109/ICCV.2019.00041
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wu L, 2020, IEEE T CIRC SYST VID, V30, P2081, DOI 10.1109/TCSVT.2019.2909549
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Ye M, 2018, LECT NOTES COMPUT SC, V11211, P176, DOI 10.1007/978-3-030-01234-2_11
   Yunpeng Zhai, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9018, DOI 10.1109/CVPR42600.2020.00904
   Zhai Y., 2020, COMPUTER VISION ECCV, P594, DOI DOI 10.1007/978-3-030-58571-6_35
   Zhang X, 2021, PROC CVPR IEEE, P3435, DOI 10.1109/CVPR46437.2021.00344
   Zhang XY, 2019, IEEE I CONF COMP VIS, P8221, DOI 10.1109/ICCV.2019.00831
   Zhang ZL, 2018, ADV NEUR IN, V31
   Zhang Z, 2022, IEEE T CIRC SYST VID, V32, P1160, DOI 10.1109/TCSVT.2021.3074745
   Zhao X, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3177
   Zheng DY, 2022, PATTERN RECOGN, V127, DOI 10.1016/j.patcog.2022.108615
   Zheng KC, 2021, PROC CVPR IEEE, P5306, DOI 10.1109/CVPR46437.2021.00527
   Zheng KC, 2021, AAAI CONF ARTIF INTE, V35, P3538
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2021, INT J COMPUT VISION, V129, P1106, DOI 10.1007/s11263-020-01395-y
   Zhong Z, 2018, LECT NOTES COMPUT SC, V11217, P176, DOI 10.1007/978-3-030-01261-8_11
   Zhong Z, 2019, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2019.00069
NR 68
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUN
PY 2024
VL 146
AR 105038
DI 10.1016/j.imavis.2024.105038
EA MAY 2024
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA SZ2O2
UT WOS:001238208600001
DA 2024-08-05
ER

PT J
AU Wang, WD
   Li, Z
   Liu, SW
   Zhang, L
   Yang, J
   Wang, Y
AF Wang, Weidong
   Li, Zhi
   Liu, Shuaiwei
   Zhang, Li
   Yang, Jin
   Wang, Yi
TI Feature decoupling and interaction network for defending against
   adversarial examples
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Deep neural networks; Adversarial examples; Adversarial defense; Feature
   decoupling-interaction
AB Recently, it was found that deep neural networks (DNNs) are susceptible to adversarial input perturbations. Most defense strategies adopt the denoising method based on preprocessing, which mitigates the impacts of adversarial perturbations on DNNs by learning the distributions of nonadversarial datasets and projecting adversarial inputs into the learned nonadversarial manifolds. However, existing defense strategies commonly focus on reconstructing clean images while ignoring the role of adversarial perturbations, which results in the reconstructed images failing to achieve the visual quality and classification accuracy of the original clean images, and the induced adversarial robustness improvement is limited. This paper proposes a feature decoupling-interaction network (FDIN), which introduces the concepts of clean features and adversarial features to separate the two kinds of features from the input adversarial examples (AEs) in a feature decoupling-interaction manner. The clean features are used to reconstruct the input image so that it is infinitely close to the original clean image, and the adversarial features are used to reconstruct the adversarial perturbations. Adversarial perturbations are removed from the adversarial examples across multiple cross cycles to improve further the reconstructed image's visual quality and classification accuracy. The features of the original clean image are used as prior knowledge to guide the network to learn the clean features of the adversarial examples and improve the classification accuracy of the model on the clean examples. In addition, a classification loss function based on the Carlini & Wagner (CW) attack algorithm is used instead of the conventional cross-entropy loss function to improve the adversarial robustness of the FDIN. The experimental results show that the proposed method achieves better defense performance than the current state-of-the-art methods on both standard tests and various attack tests and even exceeds the test accuracy of the target classifier on the original test set.
C1 [Wang, Weidong; Li, Zhi; Liu, Shuaiwei; Zhang, Li; Yang, Jin; Wang, Yi] Guizhou Univ, Coll Comp Sci & Technol, State Key Lab Publ Big Data, Guiyang, Peoples R China.
C3 Guizhou University
RP Li, Z (corresponding author), Guizhou Univ, Coll Comp Sci & Technol, State Key Lab Publ Big Data, Guiyang, Peoples R China.
EM gs.wdwang21@gzu.edu.cn; zhili@gzu.edu.cn; gs.yw21@gzu.edu.cn
FU National Natural Science Foundation of China [62062023]
FX This work is funded by the National Natural Science Foundation of China
   (No. 62062023) .
CR Andriushchenko Maksym, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P484, DOI 10.1007/978-3-030-58592-1_29
   Brendel W., 2018, Decision-based adversarial attacks: Reliable attacks against black-box machine learning models
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Croce F, 2020, PR MACH LEARN RES, V119
   Croce F, 2020, PR MACH LEARN RES, V119
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong YP, 2018, PROC CVPR IEEE, P9185, DOI 10.1109/CVPR.2018.00957
   Feinman R, 2017, Arxiv, DOI arXiv:1703.00410
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Metzen JH, 2017, Arxiv, DOI arXiv:1702.04267
   Hendrycks Dan, 2016, arXiv
   Hou XX, 2020, IMAGE VISION COMPUT, V99, DOI 10.1016/j.imavis.2020.103926
   Hu QM, 2021, ADV NEUR IN
   Goodfellow IJ, 2015, Arxiv, DOI [arXiv:1412.6572, DOI 10.48550/ARXIV.1412.6572]
   Jin GQ, 2019, INT CONF ACOUST SPEE, P3842, DOI [10.1109/icassp.2019.8683044, 10.1109/ICASSP.2019.8683044]
   Krizhevsky A, 2009, CIFAR-10 dataset
   Laidlaw Cassidy, 2020, INT C LEARN REPR
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li X, 2020, COMM COM INF SC, V1168, P191, DOI 10.1007/978-3-030-43887-6_15
   Liang Q, 2022, IMAGE VISION COMPUT, V120, DOI 10.1016/j.imavis.2021.104370
   Liao FZ, 2018, PROC CVPR IEEE, P1778, DOI 10.1109/CVPR.2018.00191
   Liu BY, 2022, IMAGE VISION COMPUT, V123, DOI 10.1016/j.imavis.2022.104469
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Madry A, 2019, Arxiv, DOI arXiv:1706.06083
   Moosavi-Dezfooli SM, 2017, PROC CVPR IEEE, P86, DOI 10.1109/CVPR.2017.17
   Naseer M, 2020, PROC CVPR IEEE, P259, DOI 10.1109/CVPR42600.2020.00034
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   Pang TY, 2018, Arxiv, DOI arXiv:1706.00633
   Papernot N, 2016, Arxiv, DOI [arXiv:1605.07277, 10.48550/arXiv.1605.07277]
   Papernot N, 2017, PROCEEDINGS OF THE 2017 ACM ASIA CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (ASIA CCS'17), P506, DOI 10.1145/3052973.3053009
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rony J, 2019, PROC CVPR IEEE, P4317, DOI 10.1109/CVPR.2019.00445
   Samangouei Pouya, 2018, INT C LEARN REPR
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh A, 2020, IMAGE VISION COMPUT, V104, DOI 10.1016/j.imavis.2020.104017
   Sriramanan G, 2020, Advances in neural information processing systems (NeurIPS)
   Sun B, 2019, PROC CVPR IEEE, P11439, DOI 10.1109/CVPR.2019.01171
   Sun P, 2020, IMAGE VISION COMPUT, V103, DOI 10.1016/j.imavis.2020.104036
   Sun YD, 2021, IMAGE VISION COMPUT, V116, DOI 10.1016/j.imavis.2021.104318
   Szegedy C, 2014, Arxiv, DOI arXiv:1312.6199
   Tian JY, 2021, AAAI CONF ARTIF INTE, V35, P9877
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vinyals O, 2016, 30 C NEURAL INFORM P, V29
   Wang XS, 2021, PROC CVPR IEEE, P1924, DOI 10.1109/CVPR46437.2021.00196
   Wu K., 2020, P 37 INT C MACH LEAR, P10377
   Wu T., 2020, INT C LEARN REPR
   Xiao Chaowei, 2018, 6 INT C LEARN REPR I
   Xu WL, 2017, Arxiv, DOI arXiv:1704.01155
   Yang J, 2023, NEURAL COMPUT APPL, V35, P18623, DOI 10.1007/s00521-023-08688-6
   Yang KW, 2021, ADV NEUR IN, V34
   Yao ZW, 2019, PROC CVPR IEEE, P11342, DOI 10.1109/CVPR.2019.01161
   Yucel MK, 2022, IMAGE VISION COMPUT, V119, DOI 10.1016/j.imavis.2022.104392
   Zagoruyko S., 2016, Wide residual networks, DOI DOI 10.5244/C.30.87
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhou DW, 2021, PR MACH LEARN RES, V139
   Zhou Dawei, 2023, INT C MACH LEARN PML, P42724
   Zhou Dawei, 2023, ICML 2023
   Zhou Dawei, 2021, arXiv
   Zhou DW, 2022, 39 INT C MACHINE LEA
NR 60
TC 0
Z9 0
U1 3
U2 3
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD APR
PY 2024
VL 144
AR 104931
DI 10.1016/j.imavis.2024.104931
EA FEB 2024
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA OA0V5
UT WOS:001204429000001
DA 2024-08-05
ER

PT J
AU Zhuang, XY
   Wei, D
   Liang, DY
   Jiang, L
AF Zhuang, Xuyao
   Wei, Dan
   Liang, Danyang
   Jiang, Lei
TI Feature attention fusion network for occluded person re-identification
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Feature attention fusion network; Occluded person re-identification;
   Partial person re-identification; Attention mechanism
AB Occluded Person re -identification (ReID) is a person retrieval task which aims to match occluded person images with the holistic image. In this paper, we propose a novel framework by using person key -points estimation and attention mechanism based on occluded person Re -identification, which is used to get discriminative features and robust alignment. We use a CNN backbone and a key -points estimation model to extract semantic local features and global features. In this process, the features extracted by the backbone network contain a lot of noise. Therefore, the Feature Attention Module (FAM) was built and applied to the backbone network to enable the network to better extract foreground information. Since the currently used baseline does not achieve very good results in occlusion scenarios, the authors considered using ConvNeXt instead of Resnet. In addition, the network adds the attention of the spatial and channel after the last convolution block, and gets the person feature with little background information. Most multi -level feature aggregation methods treat feature maps on different levels equally and use simple local operations for feature fusion, which neglects the long-distance connection among feature maps. FAM uses attention feature as query to perform second -order information propagation from the source feature map. The attention feature is computed based on the compatibility of the source feature map with the attention feature map. The feature attention module connects the attention features with the features at all levels, so as to make better use of the relationship among the features, and greatly reduce the influence of the background noise of the picture. Our method achieves 55.9% and 79.1% Rank -1 scores on the datasets of Occluded -Duke and Occluded-REID. Meanwhile, our proposed framework has 85.1% of the Rank -1 scores on partial-REID, and is superior to other methods on Partial-iLIDS datasets. Finally, our method is close to the most advanced method in Holistic Datasets.
C1 [Zhuang, Xuyao; Wei, Dan; Liang, Danyang; Jiang, Lei] Shanghai Univ Engn Sci, Shanghai, Peoples R China.
C3 Shanghai University of Engineering Science
RP Wei, D (corresponding author), Shanghai Univ Engn Sci, Shanghai, Peoples R China.
EM weiweidandan@163.com
FU National Natural Science Foundation of China [62101314]
FX This work is supported by National Natural Science Foundation of China
   (No. 62101314) .
CR Cao Y, 2023, IEEE T PATTERN ANAL, V45, P6881, DOI 10.1109/TPAMI.2020.3047209
   Chen PX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11813, DOI 10.1109/ICCV48922.2021.01162
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Fan X, 2019, LECT NOTES COMPUT SC, V11362, P19, DOI 10.1007/978-3-030-20890-5_2
   Ge YX, 2018, ADV NEUR IN, V31
   Gong S, 2014, ADV COMPUT VIS PATT, P1, DOI 10.1007/978-1-4471-6296-4
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   He LX, 2018, Arxiv, DOI arXiv:1810.07399
   He LX, 2018, PROC CVPR IEEE, P7073, DOI 10.1109/CVPR.2018.00739
   He Lingxiao, 2019, Computer Vision and Pattern Recognition
   He ST, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14993, DOI 10.1109/ICCV48922.2021.01474
   He TY, 2021, PROC CVPR IEEE, P9101, DOI 10.1109/CVPR46437.2021.00899
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Hu XQ, 2021, PATTERN RECOGN, V111, DOI 10.1016/j.patcog.2020.107688
   Huang HJ, 2018, PROC CVPR IEEE, P5098, DOI 10.1109/CVPR.2018.00535
   Iodice S, 2019, LECT NOTES COMPUT SC, V11366, P101, DOI 10.1007/978-3-030-20876-9_7
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Jia MX, 2021, AAAI CONF ARTIF INTE, V35, P1673
   Kalayeh MM, 2018, PROC CVPR IEEE, P1062, DOI 10.1109/CVPR.2018.00117
   Köstinger M, 2012, PROC CVPR IEEE, P2288, DOI 10.1109/CVPR.2012.6247939
   Li J., 2018, ARXIV181200324
   Li YL, 2021, PROC CVPR IEEE, P2897, DOI 10.1109/CVPR46437.2021.00292
   Liao SC, 2015, IEEE I CONF COMP VIS, P3685, DOI 10.1109/ICCV.2015.420
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu Z, 2022, PROC CVPR IEEE, P11966, DOI 10.1109/CVPR52688.2022.01167
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2905, DOI 10.1109/TMM.2020.2965491
   Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190
   Ma BP, 2014, IMAGE VISION COMPUT, V32, P379, DOI 10.1016/j.imavis.2014.04.002
   Ma ZX, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1487, DOI 10.1145/3474085.3475283
   Miao Yu Wu Jiaxu, 2019, P INT C COMP VIS
   Qi L, 2019, Arxiv, DOI arXiv:1804.03864
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Sarfraz MS, 2018, PROC CVPR IEEE, P420, DOI 10.1109/CVPR.2018.00051
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Song CF, 2018, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2018.00129
   Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427
   Suh Y, 2018, LECT NOTES COMPUT SC, V11218, P418, DOI 10.1007/978-3-030-01264-9_25
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Sun YF, 2019, PROC CVPR IEEE, P393, DOI 10.1109/CVPR.2019.00048
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Wang GA, 2020, PROC CVPR IEEE, P6448, DOI 10.1109/CVPR42600.2020.00648
   Wang T, 2022, AAAI CONF ARTIF INTE, P2540
   Wang ZK, 2022, PROC CVPR IEEE, P4744, DOI 10.1109/CVPR52688.2022.00471
   Wei D, 2023, VISUAL COMPUT, V39, P501, DOI 10.1007/s00371-021-02344-7
   Wei D, 2021, IEEE ACCESS, V9, P34845, DOI 10.1109/ACCESS.2021.3062967
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Yang JR, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11865, DOI 10.1109/ICCV48922.2021.01167
   Yang Y, 2014, LECT NOTES COMPUT SC, V8689, P536, DOI 10.1007/978-3-319-10590-1_35
   Zhao LM, 2017, IEEE I CONF COMP VIS, P3239, DOI 10.1109/ICCV.2017.349
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng WS, 2015, IEEE I CONF COMP VIS, P4678, DOI 10.1109/ICCV.2015.531
   Zheng WS, 2013, IEEE T PATTERN ANAL, V35, P653, DOI 10.1109/TPAMI.2012.138
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhu K, 2022, LECT NOTES COMPUT SC, V13674, P198, DOI 10.1007/978-3-031-19781-9_12
   Zhuo JX, 2018, IEEE INT CON MULTI
NR 58
TC 1
Z9 1
U1 5
U2 5
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD MAR
PY 2024
VL 143
AR 104921
DI 10.1016/j.imavis.2024.104921
EA FEB 2024
PG 9
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA KM2G4
UT WOS:001180311800001
DA 2024-08-05
ER

PT J
AU Wang, XP
   Ruan, T
   Xu, J
   Guo, XN
   Li, JH
   Yan, FH
   Zhao, GZ
   Wang, CY
AF Wang, Xueping
   Ruan, Tao
   Xu, Jun
   Guo, Xueni
   Li, Jiahe
   Yan, Feihu
   Zhao, Guangzhe
   Wang, Caiyong
TI Expression-aware neural radiance fields for high-fidelity talking
   portrait synthesis
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Talking portrait synthesis; Expression -aware neural radiance fields;
   Attention mechanism
ID OF-THE-ART
AB Neural Radiance Fields (NeRF) have attracted increasing interest in 3D talking portrait synthesis, which is a crucial problem in the field of digital humans and the metaverse. The synthesis of high-fidelity talking portraits remains a challenging task due to the intricacies of capturing and reproducing subtle facial expressions. In this paper, we propose an innovative approach termed Expression-Aware Neural Radiance Fields (EA-NeRF) for the talking portraits synthesis with remarkable realism and expressiveness. Our method leverages the power of NeRF to model complex scene appearance and illumination, while incorporating expression-awareness to accurately capture and reproduce nuanced facial dynamics. Specifically, we introduce a novel Expression-Aware Module (EAM) that enables our model to seamlessly blend between different facial expressions, yielding convincing and natural transitions during synthesis. Moreover, we present a Local-Global Attention Module (LGAM) that dynamically focuses on salient regions of the face, allowing the model to allocate more resources to areas exhibiting significant expression changes. This attention-guided synthesis process enables our model to generate talking portraits with unparalleled realism and expressiveness, accurately preserving fine-grained details and subtle nuances of facial dynamics. Both qualitative and quantitative experimental results demonstrate the effectiveness of our proposed method in generating talking portraits with superior fidelity and expressiveness compared to existing methods.
C1 [Wang, Xueping; Xu, Jun; Guo, Xueni; Yan, Feihu; Zhao, Guangzhe; Wang, Caiyong] Beijing Univ Civil Engn & Architecture, Sch Elect & Informat Engn, Beijing 100044, Peoples R China.
   [Ruan, Tao] China Patent Informat Ctr, Beijing 102200, Peoples R China.
   [Li, Jiahe] Beihang Univ, Sch Comp Sci & Engn, Beijing 100191, Peoples R China.
C3 Beijing University of Civil Engineering & Architecture; Beihang
   University
RP Yan, FH (corresponding author), Beijing Univ Civil Engn & Architecture, Sch Elect & Informat Engn, Beijing 100044, Peoples R China.
EM yanfeihu@bucea.edu.cn
FU National Natural Science Foundation of China [62176018, 62106015];
   Beijing University of Civil Engineering and Architecture Research
   Capacity Promotion Program for Young Scholars [X23026]; Beijing Natural
   Science Foundation [4242018]
FX This work was supported by the National Natural Science Foundation of
   China (No. 62176018, No. 62106015), Beijing University of Civil
   Engineering and Architecture Research Capacity Promotion Program for
   Young Scholars (X23026) , and Beijing Natural Science Foundation (No.
   4242018).
CR Baltrusaitis T, 2015, IEEE INT CONF AUTOMA
   Baltrusaitis T, 2018, IEEE INT CONF AUTOMA, P59, DOI 10.1109/FG.2018.00019
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Chatziagapi A., 2023, P IEEE INT C AUT FAC, P1
   Chen LL, 2019, PROC CVPR IEEE, P7824, DOI 10.1109/CVPR.2019.00802
   Chen LL, 2018, LECT NOTES COMPUT SC, V11211, P538, DOI 10.1007/978-3-030-01234-2_32
   Chen YS, 2023, Arxiv, DOI arXiv:2310.05720
   Chung JS, 2017, LECT NOTES COMPUT SC, V10117, P251, DOI 10.1007/978-3-319-54427-4_19
   Chung JS, 2017, LECT NOTES COMPUT SC, V10112, P87, DOI 10.1007/978-3-319-54184-6_6
   Cudeiro D, 2019, PROC CVPR IEEE, P10093, DOI 10.1109/CVPR.2019.01034
   Danecek R, 2022, PROC CVPR IEEE, P20279, DOI 10.1109/CVPR52688.2022.01967
   Das Dipanjan, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P408, DOI 10.1007/978-3-030-58577-8_25
   Deng Y, 2019, IEEE COMPUT SOC CONF, P285, DOI 10.1109/CVPRW.2019.00038
   Ding CX, 2016, ACM T INTEL SYST TEC, V7, DOI 10.1145/2845089
   Ekman P., 1978, Environ. Psychol. Nonverbal Behav.
   Eskimez SE, 2021, IEEE T MULTIMEDIA, V24, P3480, DOI 10.1109/TMM.2021.3099900
   Fan YR, 2022, P ACM COMPUT GRAPH, V5, DOI 10.1145/3522615
   Feng Y, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459936
   Gafni G, 2021, PROC CVPR IEEE, P8645, DOI 10.1109/CVPR46437.2021.00854
   Gu KX, 2020, AAAI CONF ARTIF INTE, V34, P10861
   Guo YD, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5764, DOI 10.1109/ICCV48922.2021.00573
   Hensel M, 2017, ADV NEUR IN, V30
   Jo Y, 2020, IEEE COMPUT SOC CONF, P1705, DOI 10.1109/CVPRW50498.2020.00220
   Lele Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P35, DOI 10.1007/978-3-030-58545-7_3
   Li J., 2023, P IEEECVF INT C COMP, P7568
   Liao SC, 2013, IEEE T PATTERN ANAL, V35, P1193, DOI 10.1109/TPAMI.2012.191
   Liu X, 2022, LECT NOTES COMPUT SC, V13697, P106, DOI 10.1007/978-3-031-19836-6_7
   Lu YX, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480484
   Luo JX, 2019, J NEUROSCI, V39, P2664, DOI 10.1523/JNEUROSCI.2112-18.2019
   Luong T., 2015, P 2015 C EMPIRICAL M, P1412, DOI DOI 10.18653/V1/D15-1166
   Meshry Moustafa, 2021, P IEEE CVF INT C COM, P13829
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Müller T, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530127
   Peng Z., 2024, P IEEE CVF INT C COM
   Ploumpis S, 2021, IEEE T PATTERN ANAL, V43, P4142, DOI 10.1109/TPAMI.2020.2991150
   Prajwal KR, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P484, DOI 10.1145/3394171.3413532
   Pumarola A, 2018, LECT NOTES COMPUT SC, V11214, P835, DOI 10.1007/978-3-030-01249-6_50
   Shen S, 2022, LECT NOTES COMPUT SC, V13672, P666, DOI [10.1007/978-3-031-19775-8_39, 10.1007/978-3-031-19775-839]
   Spillmann L., 2023, J. Vis., V23, P1
   Tang J., 2022, arXiv
   Tewari A, 2020, COMPUT GRAPH FORUM, V39, P701, DOI 10.1111/cgf.14022
   Tretschk E, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12939, DOI 10.1109/ICCV48922.2021.01272
   Savchenko AV, 2022, SOFTW IMPACTS, V14, DOI 10.1016/j.simpa.2022.100433
   Wang K, 2020, IEEE T IMAGE PROCESS, V29, P4057, DOI 10.1109/TIP.2019.2956143
   Yao S., 2022, arXiv
   Ye Z., 2023, P INT C LEARN REPR, P1
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang X., 2020, P IEEE CVF C COMP VI, P12335
   Zhi RC, 2020, VISUAL COMPUT, V36, P1067, DOI 10.1007/s00371-019-01707-5
   Zhou H, 2021, PROC CVPR IEEE, P4174, DOI 10.1109/CVPR46437.2021.00416
   Zhou Y, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417774
   Zhu H, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2362
   Zollhöfer M, 2018, COMPUT GRAPH FORUM, V37, P523, DOI 10.1111/cgf.13382
NR 53
TC 0
Z9 0
U1 2
U2 2
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUL
PY 2024
VL 147
AR 105075
DI 10.1016/j.imavis.2024.105075
EA MAY 2024
PG 10
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA UP4R5
UT WOS:001249253200001
DA 2024-08-05
ER

PT J
AU Abdullah, MT
   Rahman, S
   Rahman, S
   Islam, MF
AF Abdullah, Md Tahmeed
   Rahman, Sejuti
   Rahman, Shafin
   Islam, Md Fokhrul
TI VAE-GAN3D: Leveraging image-based semantics for 3D zero-shot recognition
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Zero-shot learning; 3D object recognition; Deep learning; Generative
   adversarial network; Variational autoencoder
ID CONVOLUTIONAL NEURAL-NETWORKS
AB The current state of 3D zero-shot recognition falls short in performance when compared to its counterpart in 2D images. A major challenge is the absence of a robust feature extractor for 3D point cloud data. To overcome this challenge, a large dataset needs to be collected, processed, and fed into a deep-learning model capable of generating distinguishable features after training. To this end, we propose VAE-GAN3D, a model that uses a combination of a variational autoencoder (VAE) and a generative adversarial network (GAN) to supplement the small amount of data available for training with synthetic examples of classes. This allows for the underlying data distribution of large and complex data to be learned by the deep VAE network. When combined with a GAN, this network generates synthetic features that exhibit consistency in both seen and unseen classes. Furthermore, we notice that for tasks involving a small domain of classes, the existing text features do not contribute significantly to zero-shot learning. Therefore, we introduce image representation-based semantic features of classes, which improve the performance of zero-shot recognition for 3D objects. We assess the proposed model using three different datasets and present a technique for partitioning the RGB-D object dataset, which contains real -world objects, into seen and unseen classes. Our approach shows promising results in addressing the challenges of 3D zero-shot recognition and presents a novel solution for improving the accuracy of 3D point cloud recognition.
C1 [Abdullah, Md Tahmeed; Rahman, Sejuti; Islam, Md Fokhrul] Univ Dhaka, Dept Robot & Mechatron Engn, Dhaka 1000, Bangladesh.
   [Rahman, Shafin] North South Univ, Dept Elect & Comp Engn, Dhaka 1229, Bangladesh.
C3 University of Dhaka; North South University (NSU)
RP Rahman, S (corresponding author), Univ Dhaka, Dept Robot & Mechatron Engn, Dhaka 1000, Bangladesh.
EM sejuti.rahman@du.ac.bd
FU Bangladesh University Grants Commission's innovation grant; North South
   University [CTRG-23- SEPS-20]
FX This research received partial funding from Bangladesh University Grants
   Commission's innovation grant for the fiscal year 2022-2023,
   complemented by the Conference Travel and Research Grants (CTRG) for
   2023-2024 from North South University, under Grant ID: CTRG-23- SEPS-20.
CR Akata Z, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2487986
   Al-Halah Z, 2016, PROC CVPR IEEE, P5975, DOI 10.1109/CVPR.2016.643
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Bhattacharjee S, 2019, IEEE IMAGE PROC, P3646, DOI [10.1109/icip.2019.8803562, 10.1109/ICIP.2019.8803562]
   Bojanowski Piotr, 2017, T ASSOC COMPUT LING, V5, P135, DOI [10.48550/arXiv.1607.04606, DOI 10.1162/TACLA00051]
   Chan ER, 2022, PROC CVPR IEEE, P16102, DOI 10.1109/CVPR52688.2022.01565
   Changpinyo S, 2017, IEEE I CONF COMP VIS, P3496, DOI 10.1109/ICCV.2017.376
   Cheraghian A., 2019, 30 BRIT MACH VIS C B
   Cheraghian A, 2022, INT J COMPUT VISION, V130, P2364, DOI 10.1007/s11263-022-01650-4
   Cheraghian A, 2020, IEEE WINT CONF APPL, P912, DOI 10.1109/WACV45572.2020.9093545
   Cheraghian A, 2019, PROCEEDINGS OF MVA 2019 16TH INTERNATIONAL CONFERENCE ON MACHINE VISION APPLICATIONS (MVA), DOI [10.23919/mva.2019.8758063, 10.23919/MVA.2019.8758063]
   Choe J, 2022, Arxiv, DOI arXiv:2111.11187
   Cui YM, 2021, NEUROCOMPUTING, V432, P300, DOI 10.1016/j.neucom.2020.12.067
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Feng YF, 2018, PROC CVPR IEEE, P264, DOI 10.1109/CVPR.2018.00035
   Gu Xiuye, 2022, INT C LEARN REPR ICL
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hickmon J., 2024, AAAI C ART INT AAAI, P23747
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang H, 2019, PROC CVPR IEEE, P801, DOI 10.1109/CVPR.2019.00089
   Gulrajani I, 2017, ADV NEUR IN, V30
   Koger B, 2023, J ANIM ECOL, V92, P1357, DOI 10.1111/1365-2656.13904
   Lai K, 2011, IEEE INT CONF ROBOT, P1817
   Larsen ABL, 2016, PR MACH LEARN RES, V48
   Li JJ, 2019, PROC CVPR IEEE, P7394, DOI 10.1109/CVPR.2019.00758
   Li RH, 2020, PROC CVPR IEEE, P6377, DOI 10.1109/CVPR42600.2020.00641
   Li XZ, 2022, IEEE T VIS COMPUT GR, V28, P4503, DOI 10.1109/TVCG.2021.3092570
   Li XP, 2021, NEURAL COMPUT APPL, V33, P5313, DOI 10.1007/s00521-020-05322-7
   Li YW, 2022, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR52688.2022.00119
   Li Y, 2023, IEEE T MULTIMEDIA, V25, P1600, DOI 10.1109/TMM.2021.3139211
   Liu RS, 2023, IEEE I CONF COMP VIS, P9264, DOI 10.1109/ICCV51070.2023.00853
   Ma Xu, 2022, INT C LEARN REPR ICL
   Mahmoud A, 2023, IEEE WINT CONF APPL, P663, DOI 10.1109/WACV56688.2023.00073
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Meyer GP, 2019, PROC CVPR IEEE, P12669, DOI 10.1109/CVPR.2019.01296
   Michele B, 2021, INT CONF 3D VISION, P992, DOI 10.1109/3DV53792.2021.00107
   Mikolov T., 2013, ARXIV, DOI DOI 10.48550/ARXIV.1301.3781
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P1, DOI DOI 10.48550/ARXIV.1310.4546
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Palatucci M., 2009, Zero-shot learning with semantic output codes, P22
   Pennington J., 2014, GLOVE GLOBAL VECTORS, DOI DOI 10.3115/V1/D14-1162
   Qi CR, 2017, ADV NEUR IN, V30
   Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609
   Rahman S, 2018, IEEE T IMAGE PROCESS, V27, P5652, DOI 10.1109/TIP.2018.2861573
   Schönfeld E, 2019, PROC CVPR IEEE, P8239, DOI 10.1109/CVPR.2019.00844
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Tolstikhin I, 2021, ADV NEUR IN, V34
   Tombari F., 2010, P ACM WORKSH 3D OBJ, P57, DOI 10.1145/1877808.1877821
   Uy MA, 2019, IEEE I CONF COMP VIS, P1588, DOI 10.1109/ICCV.2019.00167
   Wang C., 2017, PROC BRIT MACH VIS C, P1
   Wang Q, 2017, LECT NOTES ARTIF INT, V10534, P87, DOI 10.1007/978-3-319-71249-9_6
   Wang W, 2019, ACM T INTEL SYST TEC, V10, DOI 10.1145/3293318
   Wang WH, 2023, IEEE INT CONF ROBOT, P5184, DOI 10.1109/ICRA48891.2023.10160266
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xian YQ, 2019, PROC CVPR IEEE, P10267, DOI 10.1109/CVPR.2019.01052
   Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581
   Yu T, 2018, PROC CVPR IEEE, P186, DOI 10.1109/CVPR.2018.00027
   Zhang L, 2017, PROC CVPR IEEE, P3010, DOI 10.1109/CVPR.2017.321
   Zhang RR, 2022, PROC CVPR IEEE, P8542, DOI 10.1109/CVPR52688.2022.00836
   Zhu YZ, 2018, PROC CVPR IEEE, P1004, DOI 10.1109/CVPR.2018.00111
NR 64
TC 1
Z9 1
U1 1
U2 1
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUL
PY 2024
VL 147
AR 105049
DI 10.1016/j.imavis.2024.105049
EA MAY 2024
PG 10
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA TD3B8
UT WOS:001239272700001
DA 2024-08-05
ER

PT J
AU Fernandez-Beltran, R
   Guzmán-Ponce, A
   Fernandez, R
   Kang, J
   García-Mateos, G
AF Fernandez-Beltran, Ruben
   Guzman-Ponce, Angelica
   Fernandez, Rafael
   Kang, Jian
   Garcia-Mateos, Gines
TI Shadow detection using a cross-attentional dual-decoder network with
   self-supervised image reconstruction features
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Shadow detection; Semantic segmentation; Convolutional neural networks;
   Cross-attention; Dual-decoder
ID AWARE; CLOUD
AB Shadow detection is a challenging problem in computer vision due to the high variability in lighting conditions, object shapes, and scene layouts. Despite the positive results achieved by some existing technologies, the problem becomes particularly challenging with complex and heterogeneous images where shadow-casting objects coexist and shadows can have different depths, scales, and morphologies. As a result, more advanced and accurate solutions are still needed to deal with this type of complexities. To address these challenges, this paper proposes a novel deep learning model, called the Cross-Attentional Dual Decoder Network (CADDN), to improve shadow detection by using fine-grained image reconstruction features. Unlike other existing methods, the CADDN uses an innovative encoder-decoder architecture with two decoder segments that work together to reconstruct the input images and their corresponding shadow masks. In this way, the features used to reconstruct the original input image can be used to support the shadow detection process itself. The proposed model also incorporates a crossattention mechanism to weight the most relevant features for detecting shadows and skip connections with noise to improve the quality of the transferred features. The experimental results, including several benchmark image datasets and state-of-the-art detection methods, demonstrate the suitability of the presented approach for detecting shadows in computer vision applications.
C1 [Fernandez-Beltran, Ruben; Garcia-Mateos, Gines] Univ Murcia, Dept Comp Sci & Syst, E-30100 Murcia, Spain.
   [Guzman-Ponce, Angelica; Fernandez, Rafael] Univ Jaume 1, Inst New Imaging Technol, Castellon De La Plana 12071, Spain.
   [Kang, Jian] Soochow Univ, Sch Elect & Informat Engn, Suzhou 215006, Peoples R China.
C3 University of Murcia; Universitat Jaume I; Soochow University - China
RP Fernandez-Beltran, R (corresponding author), Univ Murcia, Dept Comp Sci & Syst, E-30100 Murcia, Spain.
EM rufernan@um.es
RI Fernandez-Beltran, Ruben/GLT-5907-2022; Garcia-Mateos, G./G-7779-2015;
   kang, jian/V-3055-2019
OI Fernandez-Beltran, Ruben/0000-0003-1374-8416; Garcia-Mateos,
   G./0000-0003-2521-4454; kang, jian/0000-0001-6284-3044
FU Science and Technology Agency of the Region of Murcia (Fundaci<acute
   accent>on Se<acute accent>neca, Action Plan) [2022, 22130/PI/22];
   National Natural Science Foundation of China [62101371]
FX This work was supported by the Postdoctoral Margarita Salas Fellowship
   MGS/2021/23 (UP2021-021) from the European Union NextGenerationEU funds,
   the Science and Technology Agency of the Region of Murcia (Fundacion
   Seneca, Action Plan 2022) by grant 22130/PI/22, and the National Natural
   Science Foundation of China under Grant 62101371.r NextGenerationEU
   funds, the Science and Technology Agency of the Region of Murcia
   (Fundaci<acute accent>on Se<acute accent>neca, Action Plan 2022) by
   grant 22130/PI/22, and the National Natural Science Foundation of China
   under Grant 62101371.
CR Ahn WJ, 2023, NEUROCOMPUTING, V552, DOI 10.1016/j.neucom.2023.126559
   Al-Najdawi N, 2012, PATTERN RECOGN LETT, V33, P752, DOI 10.1016/j.patrec.2011.12.013
   Bansal Naman, 2019, Proceedings of 2nd International Conference on Communication, Computing and Networking. ICCCN 2018. Lecture Notes in Networks and Systems (LNNS 46), P375, DOI 10.1007/978-981-13-1217-5_37
   Brunet D, 2012, IEEE T IMAGE PROCESS, V21, P1488, DOI 10.1109/TIP.2011.2173206
   Chaurasia A, 2017, 2017 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)
   Chen JY, 2023, IEEE INT CON MULTI, P150, DOI 10.1109/ICME55011.2023.00034
   Chen L.-C., 2018, ECCV, P801, DOI [DOI 10.1007/978-3-030-01234-249, 10.1007/978-3-030-01234-2_49]
   Chen L-C, 2017, ARXIV PREPRINT ARXIV, DOI DOI 10.48550/ARXIV.1706.05587
   Cong Runmin, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P1202, DOI 10.1145/3581783.3612482
   Fan TL, 2020, IEEE ACCESS, V8, P179656, DOI 10.1109/ACCESS.2020.3025372
   Fan ZY, 2023, IEEE J-STARS, V16, P2094, DOI 10.1109/JSTARS.2023.3238720
   Fang XY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2927, DOI [10.1145/3474085.3475199, 10.114510.1145/3474085.3475199]
   Feng JF, 2023, INT J REMOTE SENS, V44, P5473, DOI 10.1080/01431161.2023.2249603
   Fernandez-Beltran R, 2020, IEEE GEOSCI REMOTE S, V17, P2120, DOI 10.1109/LGRS.2019.2963114
   Guo LQ, 2023, PROC CVPR IEEE, P14049, DOI 10.1109/CVPR52729.2023.01350
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heidari M, 2023, IEEE WINT CONF APPL, P6191, DOI 10.1109/WACV56688.2023.00614
   Hieu Le, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P264, DOI 10.1007/978-3-030-58621-8_16
   Hosseinzadeh S, 2018, IEEE INT C INT ROBOT, P3124, DOI 10.1109/IROS.2018.8594050
   Hu XW, 2021, IEEE T IMAGE PROCESS, V30, P1925, DOI 10.1109/TIP.2021.3049331
   Hu XW, 2020, IEEE T PATTERN ANAL, V42, P2795, DOI 10.1109/TPAMI.2019.2919616
   Huang X, 2011, IEEE I CONF COMP VIS, P898, DOI 10.1109/ICCV.2011.6126331
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Huerta I, 2015, IMAGE VISION COMPUT, V41, P42, DOI 10.1016/j.imavis.2015.06.003
   Jadon S, 2020, 2020 IEEE CONFERENCE ON COMPUTATIONAL INTELLIGENCE IN BIOINFORMATICS AND COMPUTATIONAL BIOLOGY (CIBCB), P115, DOI 10.1109/cibcb48159.2020.9277638
   Jiao LB, 2023, REMOTE SENS-BASEL, V15, DOI 10.3390/rs15040906
   Jiao LB, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12122001
   Jie LP, 2023, IEEE T CIRC SYST VID, V33, P7819, DOI 10.1109/TCSVT.2023.3283416
   Jie LP, 2023, Arxiv, DOI arXiv:2305.11513
   Khan SH, 2016, IEEE T PATTERN ANAL, V38, P431, DOI 10.1109/TPAMI.2015.2462355
   Kumar A, 2023, OPTIK, V273, DOI 10.1016/j.ijleo.2023.170513
   Lalonde JF, 2012, INT J COMPUT VISION, V98, P123, DOI 10.1007/s11263-011-0501-8
   Lalonde JF, 2010, LECT NOTES COMPUT SC, V6312, P322, DOI 10.1007/978-3-642-15552-9_24
   Li H., 2018, ARXIV180510180, P1, DOI DOI 10.48550/ARXIV.1805.10180
   Li ZW, 2022, ISPRS J PHOTOGRAMM, V188, P89, DOI 10.1016/j.isprsjprs.2022.03.020
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu DY, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2021.3100294
   Liu LH, 2023, PROC CVPR IEEE, P10449, DOI 10.1109/CVPR52729.2023.01007
   Liu Y., 2023, IEEE Transactions on Neural Networks and Learning Systems
   Luo S, 2020, ISPRS J PHOTOGRAMM, V167, P443, DOI 10.1016/j.isprsjprs.2020.07.016
   Maggiori E, 2017, INT GEOSCI REMOTE SE, P3226, DOI 10.1109/IGARSS.2017.8127684
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sanin A, 2012, PATTERN RECOGN, V45, P1684, DOI 10.1016/j.patcog.2011.10.001
   Seferbekov S, 2018, IEEE COMPUT SOC CONF, P272, DOI 10.1109/CVPRW.2018.00051
   Sudre CH, 2017, LECT NOTES COMPUT SC, V10553, P240, DOI 10.1007/978-3-319-67558-9_28
   Valanarasu JMJ, 2023, IEEE WINT CONF APPL, P1705, DOI 10.1109/WACV56688.2023.00175
   Vincente TFY, 2016, LECT NOTES COMPUT SC, V9910, P816, DOI 10.1007/978-3-319-46466-4_49
   Vinuesa R, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-019-14108-y
   Wang TY, 2023, IEEE T PATTERN ANAL, V45, P3259, DOI 10.1109/TPAMI.2022.3185628
   Wang TY, 2020, PROC CVPR IEEE, P1877, DOI 10.1109/CVPR42600.2020.00195
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu W, 2023, KNOWL-BASED SYST, V273, DOI 10.1016/j.knosys.2023.110614
   Wu W, 2022, J VIS COMMUN IMAGE R, V82, DOI 10.1016/j.jvcir.2021.103397
   Wu W, 2022, COMPUT VIS IMAGE UND, V216, DOI 10.1016/j.cviu.2021.103341
   Xie E., 2021, ADV NEURAL INF PROCE, V34, P12077
   Xie YK, 2022, ISPRS J PHOTOGRAMM, V193, P29, DOI 10.1016/j.isprsjprs.2022.09.004
   Xu YM, 2024, PATTERN RECOGN, V146, DOI 10.1016/j.patcog.2023.109969
   Yang L, 2022, COMPUT ELECTRON AGR, V199, DOI 10.1016/j.compag.2022.107123
   Yucel MK, 2023, IEEE WINT CONF APPL, P4914, DOI 10.1109/WACV56688.2023.00490
   Zhai GT, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2757-1
   Zhang C, 2023, REMOTE SENS-BASEL, V15, DOI 10.3390/rs15061664
   Zhang J, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3282967
   Zhang Lan., 2023, ACM Transactions on Sensor Networks, V19, P1
   Zhang WM, 2019, IEEE T PATTERN ANAL, V41, P611, DOI 10.1109/TPAMI.2018.2803179
   Zhang X., 2023, 2023 INT JOINT C NEU, P1
   Zhang XZ, 2023, J APPL REMOTE SENS, V17, DOI 10.1117/1.JRS.17.016506
   Zhang ZG, 2023, REMOTE SENS-BASEL, V15, DOI 10.3390/rs15071859
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhou GD, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3241331
   Zhou HL, 2023, J KING SAUD UNIV-COM, V35, DOI 10.1016/j.jksuci.2023.101766
   Zhou TT, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13040699
   Zhou ZW, 2018, LECT NOTES COMPUT SC, V11045, P3, DOI 10.1007/978-3-030-00889-5_1
   Zhu JJ, 2010, PROC CVPR IEEE, P223, DOI 10.1109/CVPR.2010.5540209
   Zhu L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4682, DOI 10.1109/ICCV48922.2021.00466
   Zhu L, 2018, LECT NOTES COMPUT SC, V11210, P122, DOI 10.1007/978-3-030-01231-1_8
   Zhu YR, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P6717, DOI 10.1145/3503161.3547904
NR 76
TC 0
Z9 0
U1 4
U2 4
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD MAR
PY 2024
VL 143
AR 104922
DI 10.1016/j.imavis.2024.104922
EA FEB 2024
PG 16
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA KO6X3
UT WOS:001180958500001
OA hybrid
DA 2024-08-05
ER

PT J
AU Koch, B
   Grbic, R
AF Koch, Brando
   Grbic, Ratko
TI One-shot lip-based biometric authentication: Extending behavioral
   features with authentication phrase information
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Lip-based biometric authentication; Siamese neural network;
   Hard-negative mining; Presentation attack detection; One-shot learning;
   GRID dataset
ID PERSON AUTHENTICATION; FACE; IDENTIFICATION; SPEECH; MOTION
AB Lip -based biometric authentication (LBBA) is an authentication method based on a person's lip movements during speech in the form of video data. LBBA can utilize both physical and behavioral characteristics of lip movements without requiring any additional sensory equipment apart from an RGB camera. Current approaches employ deep siamese neural networks trained with one-shot learning to generate embedding vectors from lip movement features. However, most of these approaches don't discriminate against speech content which makes them vulnerable to video replay attacks. Moreover, there is a lack of comprehensive analysis regarding the impact of distinct lip characteristics or difficult dataset phrases with significant word overlap on the performance of authentication in one-shot approaches. To address this, we introduce the GRID-CCP dataset and train a siamese neural network using 3D convolutions and recurrent neural network layers to additionally discriminate against speech content. For loss calculation, we propose a custom triplet loss function for efficient and customizable batch -wise hard -negative mining. Our experimental results, using an open -set protocol, demonstrate a False Acceptance Rate (FAR) of 3.2% and a False Rejection Rate (FRR) of 3.8% on the test set of the GRID-CCP dataset. Finally, we conduct an analysis to assess the influence and discriminative power of behavioral and physical features in LBBA.
C1 [Koch, Brando; Grbic, Ratko] Fac Elect Engn Comp Sci & Informat Technol Osijek, Kneza Trpimira 2B, HR-31000 Osijek, Croatia.
C3 University of JJ Strossmayer Osijek
RP Grbic, R (corresponding author), Fac Elect Engn Comp Sci & Informat Technol Osijek, Kneza Trpimira 2B, HR-31000 Osijek, Croatia.
EM ratko.grbic@ferit.hr
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   [Anonymous], 2004, P 10 AUSTR INT C SPE
   Benedikt L, 2010, IEEE T SYST MAN CY A, V40, P449, DOI 10.1109/TSMCA.2010.2041656
   Bigun J, 2004, CIHSPS 2004: PROCEEDINGS OF THE 2004 IEEE INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE FOR HOMELAND SECURITY AND PERSONAL SAFETY, P104, DOI 10.1109/CIHSPS.2004.1360218
   Cao HW, 2014, IEEE T AFFECT COMPUT, V5, P377, DOI 10.1109/TAFFC.2014.2336244
   Cao ZM, 2010, PROC CVPR IEEE, P2707, DOI 10.1109/CVPR.2010.5539992
   Çetingül HE, 2006, IEEE T IMAGE PROCESS, V15, P2879, DOI 10.1109/TIP.2006.877528
   Cheng F, 2018, PATTERN RECOGN, V83, P340, DOI 10.1016/j.patcog.2018.06.005
   Chetty G, 2009, INT J BIOMETRICS, V1, P463, DOI 10.1504/IJBM.2009.027306
   Chingovska I., 2012, 2012 BIOSIG P INT C, P1
   Chung J S., 2017, BRIT MACHINE VISION, P155, DOI 10.5244/C.31.155
   Cooke M, 2006, J ACOUST SOC AM, V120, P2421, DOI 10.1121/1.2229005
   Costa-Pazo A, 2016, LECT NOTE INFORM, VP-260
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dosovitskiy A., 2020, arXiv, DOI DOI 10.48550/ARXIV
   Faraj MI, 2006, INT C PATT RECOG, P1059
   Faraj MI, 2007, PATTERN RECOGN LETT, V28, P1368, DOI 10.1016/j.patrec.2007.02.017
   Haliassos A, 2021, PROC CVPR IEEE, P5037, DOI 10.1109/CVPR46437.2021.00500
   Huang G.B., 2008, WORKSH FAC REAL LIF
   Jain AK, 2006, IEEE T INF FOREN SEC, V1, P125, DOI 10.1109/TIFS.2006.873653
   Jee H., 2008, Int. J. Comput. Inform. Eng., V2, P2142
   Liu CJ, 2002, IEEE T IMAGE PROCESS, V11, P467, DOI 10.1109/TIP.2002.999679
   Liu M., 2021, arXiv
   Lucey S, 2003, LECT NOTES COMPUT SC, V2688, P260
   Luettin J, 1996, ICSLP 96 - FOURTH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING, PROCEEDINGS, VOLS 1-4, P62, DOI 10.1109/ICSLP.1996.607030
   Assael YM, 2016, Arxiv, DOI arXiv:1611.01599
   Messer K., 2000, PROC AUDIOAND VIDEO
   Movellan J. R., 1995, Advances in Neural Information Processing Systems 7, P851
   Pan G, 2007, IEEE I CONF COMP VIS, P1879, DOI 10.1109/ICCV.2007.4409068
   Phillips PJ, 1999, ADV NEUR IN, V11, P803
   Pigeon S, 1997, LECT NOTES COMPUT SC, V1206, P403, DOI 10.1007/BFb0016021
   Ramachandra R, 2017, ACM COMPUT SURV, V50, DOI 10.1145/3038924
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shillingford B, 2018, Arxiv, DOI arXiv:1807.05162
   Singh P., 2012, 2012 5th IAPR International Conference on Biometrics (ICB), P472, DOI 10.1109/ICB.2012.6199795
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   TSUCHIHASHI Y, 1974, FORENSIC SCI, V3, P233, DOI 10.1016/0300-9432(74)90034-X
   TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71
   Wand M, 2016, INT CONF ACOUST SPEE, P6115, DOI 10.1109/ICASSP.2016.7472852
   Wang Liting, 2009, Tsinghua Science and Technology, V14, P685, DOI 10.1016/S1007-0214(09)70135-X
   Wang SL, 2012, PATTERN RECOGN, V45, P3328, DOI 10.1016/j.patcog.2012.02.016
   Wright C, 2020, EURASIP J INF SECUR, V2020, DOI 10.1186/s13635-020-0102-6
   Wright C, 2020, LECT NOTES COMPUT SC, V11844, P405, DOI 10.1007/978-3-030-33720-9_31
   Yu ZT, 2022, Arxiv, DOI arXiv:2106.14948
   Zakeri A., 2021, 2021 7 INT C SIGNAL, P1, DOI [10.1109/ICSPIS54653.2021.9729394, DOI 10.1109/ICSPIS54653.2021.9729394]
NR 45
TC 0
Z9 0
U1 1
U2 1
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD FEB
PY 2024
VL 142
AR 104900
DI 10.1016/j.imavis.2024.104900
EA JAN 2024
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA IW9W5
UT WOS:001169504000001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Li, J
   Wei, XM
AF Li, Jing
   Wei, Xiaomeng
TI Research on efficient detection network method for remote sensing images
   based on self attention mechanism
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Computer vision; Remote sensing images; Image detection; Faster R-CNN;
   Self attention mechanism; End-to-end
AB Remote sensing images are widely used in aerial drones, satellites, and other fields. However, traditional object detection methods face challenges of low efficiency and accuracy due to the diverse scales, numerous target types, and complex backgrounds of remote sensing images. To address these issues, this paper proposes a remote sensing image detection network based on self-attention, which adaptively learns the relative importance of pixels to achieve more precise and efficient object detection in remote sensing images, meeting the demands of large-scale remote sensing image processing. Firstly, through end-to-end training, the object detection network is optimized as a whole to directly output detection results from the input remote sensing images, eliminating the need for additional intermediate steps. This not only reduces the impact of information loss and inconsistency but also simplifies the entire detection process. Secondly, we integrate the Faster R-CNN architecture as the foundation, combining region extraction and object classification into a unified process. Lastly, we embed selfattention mechanisms at different levels of the Faster R-CNN to progressively extract multi-scale and multilevel feature information, enhancing the network's ability to learn the correlation of information from different positions in the image, automatically capturing the relationships between objects, and improving the accuracy of object detection. This significantly reduces redundant computation, making it more efficient for large-scale remote sensing image processing. Experimental verification demonstrates that this approach outperforms traditional methods in terms of detection accuracy and efficiency, better addressing the particularities of remote sensing images, and providing an efficient and precise solution for aerial drone and satellite image processing. Remote sensing image detection has become one of the research hotspots in the field of remote sensing, holding significant theoretical significance and practical application value.
C1 [Li, Jing] Henan Univ Sci & Technol, Informat Engn Coll, Luoyang 471023, Peoples R China.
   [Li, Jing; Wei, Xiaomeng] Henan Mech & Elect Vocat Coll, Sch Informat Enginering, Xinzheng 451192, Peoples R China.
   [Li, Jing; Wei, Xiaomeng] Henan Mech & Elect Vocat Coll, Sch Internet Things, Xinzheng 451192, Peoples R China.
   [Li, Jing] Henan Univ Sci & Technol, Henan Int Joint Lab Cyberspace Secur Applicat, Luoyang 471023, Peoples R China.
   [Li, Jing] Henan Univ Sci Technol, Coll Informat Enginering, Luoyang 471000, Peoples R China.
C3 Henan University of Science & Technology; Henan University of Science &
   Technology; Henan University of Science & Technology
RP Li, J (corresponding author), Henan Univ Sci Technol, Coll Informat Enginering, Luoyang 471000, Peoples R China.
EM mandyjing2015@hotmail.com
FU National Science Foundation
FX * This document is the results of the research project funded by the
   National Science Foundation. The second title footnote which is a longer
   text matter to fill through the whole text width and overflow into
   another line in the footnotes area of the first page.
CR Bao WX, 2023, COMPUT ELECTRON AGR, V205, DOI 10.1016/j.compag.2023.107637
   Dai X., 2023, IEEE Trans. Geosci. Remote Sens
   Dang FY, 2023, COMPUT ELECTRON AGR, V205, DOI 10.1016/j.compag.2023.107655
   Diwan T, 2023, MULTIMED TOOLS APPL, V82, P9243, DOI 10.1007/s11042-022-13644-y
   Gao K., 2023, IEEE Trans. Geosci. Remote Sens.
   Gao LA, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3242987
   Ghosh R, 2024, MULTIMED TOOLS APPL, V83, P7135, DOI 10.1007/s11042-023-15633-1
   Hong JC, 2023, APPL THERM ENG, V226, DOI 10.1016/j.applthermaleng.2023.120304
   Hu XD, 2023, IEEE GEOSCI REMOTE S, V20, DOI 10.1109/LGRS.2023.3235117
   Jin J., 2023, IEEEGeosci. Remote Sens. Lett., V20, P1
   Jin JH, 2023, IEEE GEOSCI REMOTE S, V20, DOI 10.1109/LGRS.2023.3234257
   Li WL, 2023, LANDSLIDES, V20, P1, DOI 10.1007/s10346-022-01960-1
   Liu ZG, 2023, IEEE ACCESS, V11, P1742, DOI 10.1109/ACCESS.2023.3233964
   Lu YW, 2023, PROC CVPR IEEE, P18063, DOI 10.1109/CVPR52729.2023.01732
   Qian HM, 2023, J REAL-TIME IMAGE PR, V20, DOI 10.1007/s11554-023-01258-y
   Song CY, 2023, J INTEGR AGR, V22, P1671, DOI 10.1016/j.jia.2022.09.021
   Thepade S.D., 2023, 2023 2 INT C INNOVAT, P1
   Van Etten A, 2019, Arxiv, DOI arXiv:1807.01232
   Wang Z, 2023, Expert Syst Appl
   Wen GQ, 2023, APPL INTELL, V53, P1586, DOI 10.1007/s10489-022-03549-6
   Xia GS, 2018, PROC CVPR IEEE, P3974, DOI 10.1109/CVPR.2018.00418
   Zhang S, 2023, ISA T, V133, P369, DOI 10.1016/j.isatra.2022.06.035
   Zhang TY, 2023, ISPRS J PHOTOGRAMM, V195, P353, DOI 10.1016/j.isprsjprs.2022.12.004
   Zhang XF, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3245674
   Zhu SY, 2023, IEEE GEOSCI REMOTE S, V20, DOI 10.1109/LGRS.2023.3238553
   Zhu WG, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3239013
NR 26
TC 0
Z9 0
U1 16
U2 16
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD FEB
PY 2024
VL 142
AR 104884
DI 10.1016/j.imavis.2023.104884
EA JAN 2024
PG 14
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA IH1V9
UT WOS:001165355200001
DA 2024-08-05
ER

PT J
AU Liu, JP
   Zheng, KY
   Liu, XY
   Xu, PF
   Zhou, Y
AF Liu, Jinping
   Zheng, Kunyi
   Liu, Xianyi
   Xu, Pengfei
   Zhou, Ying
TI SDSDet: A real-time object detector for small, dense, multi-scale remote
   sensing objects
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Remote sensing images; Lightweight network; Small dense and multi-scale
   object detection; Gradient confusion; Spatial artifacts; Dual-path
   learning network
AB Object detection in remote sensing images (RSIs) plays a crucial role in aerial and satellite image analysis. Existing methods lack the capability to effectively detect small and multi -scale objects in RSIs. Consequently, achieving an optimal trade-off between speed and accuracy remains unattainable. Extensive investigation reveals that state-of-the-art detectors have largely overlooked two critical aspects: Spatial artifacts from convolution operations and gradient confusion caused by neighboring levels in the Feature Pyramid Network. To address the first problem, we propose adopting a non -reorganized patch -embedding layer in the downsampling stage and a dual -path learning network (DPLNet) as the backbone, which can effectively mitigate the adverse effects of the edge pixel feature bias in feature maps. Additionally, using DPLNet as the backbone network can minimize costs while learning the intrinsic feature information of objects in RSI. For the second aspect, we propose a neighborerasing module with only one gradient flow (OGF-NEM). This module utilizes deep features to erase large objects to highlight small objects in shallow features and changes the backpropagation path to prevent the backflow of unreasonable gradients and the erosion of information from neighbor scales. Thus, a novel detector, called SDSDet, is proposed, which achieves excellent performance for small, dense, and multi -scale objects in RSIs. We have conducted exhaustive experiments on DOTA and MS COCO datasets. Specifically, the SDSDet achieves 42.8% AP on DOTA and 33.3% AP on MS COCO, together with nearly 4.87 M model size and 95 FPS.
C1 [Liu, Jinping; Zheng, Kunyi; Liu, Xianyi; Xu, Pengfei] Hunan Normal Univ, Coll Informat Sci & Engn, Changsha 410081, Hunan, Peoples R China.
   [Liu, Jinping] Hunan Normal Univ, Key Lab Comp & Stochast Math, Minist Educ, Changsha 410081, Hunan, Peoples R China.
   [Zhou, Ying] Hunan Childrens Hosp, Data & Informat Management Ctr, Changsha 410007, Hunan, Peoples R China.
C3 Hunan Normal University; Hunan Normal University
RP Zhou, Y (corresponding author), Hunan Childrens Hosp, Data & Informat Management Ctr, Changsha 410007, Hunan, Peoples R China.
EM ljp202518@163.com; painlove@hunnu.edu.cn; liuxianyi@hunnu.edu.cn;
   xupf@hunnu.edu.cn; yingzhou_hch@163.com
RI liu, jinping/AAM-7723-2021
OI liu, jinping/0000-0002-8669-882X
FU National Natural Science Foundation of China [62371187]
FX This work was supported by the National Natural Science Foundation of
   China under Grant No. 62371187.
CR Alsallakh B., 2021, INT C LEARNING REPRE
   [Anonymous], 2015, arXiv: Computer Vision and Pattern Recognition
   [Anonymous], 2015, arXiv: Learning
   Bochkovskiy A., 2020, ARXIV, DOI [10.48550/ARXIV.2004.10934, DOI 10.48550/ARXIV.2004.10934]
   BOX GEP, 1964, J ROY STAT SOC B, V26, P211, DOI 10.1111/j.2517-6161.1964.tb00553.x
   Cai Z., 2019, arXiv
   Chen YP, 2022, PROC CVPR IEEE, P5260, DOI 10.1109/CVPR52688.2022.00520
   Cui C, 2021, Arxiv, DOI [arXiv:2109.15099, 10.48550/arXiv.2109.15099]
   Dai JF, 2016, ADV NEUR IN, V29
   Dosovitskiy A., 2020, arXiv: Computer Vision and Pattern Recognition
   Elfwing S, 2018, NEURAL NETWORKS, V107, P3, DOI 10.1016/j.neunet.2017.12.012
   Ge Z., 2021, arXiv: Computer Vision and Pattern Recognition,
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2016, IEEE T PATTERN ANAL, V38, P142, DOI 10.1109/TPAMI.2015.2437384
   Guo B., 2021, 2021 IEEECVF INT C
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Howard A. G., 2017, ARXIV, DOI DOI 10.48550/ARXIV.1704.04861
   Jocher G., 2020, Zenodo
   Karim S, 2021, MULTIMED TOOLS APPL, V80, P4507, DOI 10.1007/s11042-020-09959-3
   Karim S, 2019, MULTIMED TOOLS APPL, V78, P32565, DOI 10.1007/s11042-019-08033-x
   Karim S, 2017, 2017 17TH IEEE INTERNATIONAL CONFERENCE ON COMMUNICATION TECHNOLOGY (ICCT 2017), P1725, DOI 10.1109/ICCT.2017.8359925
   Law H, 2020, INT J COMPUT VISION, V128, P642, DOI 10.1007/s11263-019-01204-1
   Li Y., 2020, Ser. Computer Vision and Pattern Recognition
   Liao JJ, 2021, IEEE J-STARS, V14, P11204, DOI 10.1109/JSTARS.2021.3122152
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu Z, 2022, PROC CVPR IEEE, P11999, DOI 10.1109/CVPR52688.2022.01170
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Ma X., 2021, Alpha-Iou: A Family of Power Intersection over Union Losses for Bounding Box Regression
   RangiLyu, 2021, NanoDet-Plus: Super fast and high accuracy lightweight anchorfree object detection model
   Redmon J., 2018, YOLOv 3: An Incremental ImprovementC
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Sermanet P., 2013, arXiv: Computer Vision and Pattern Recognition
   Tan M., 2020, Ser. Computer Vision and Pattern Recognition
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Trockman A., 2022, PATCHES ARE ALL YOU
   Ultralytics, 2018, Yolov3
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang C.-Y., 2020, arXiv: Computer Vision and Pattern Recognition
   Wang CY, 2020, IEEE COMPUT SOC CONF, P1571, DOI 10.1109/CVPRW50498.2020.00203
   Wang LG, 2024, GEOSCI DATA J, V11, P237, DOI 10.1002/gdj3.162
   Wu S., 2019, Ser. Neural Information Processing Systems
   Xia GS, 2019, Arxiv, DOI arXiv:1711.10398
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yang J., 2021, arXiv: Computer Vision and Pattern Recognition
   Yang X., 2020, arXiv: Computer Vision and Pattern Recognition
   Yang X, 2019, IEEE I CONF COMP VIS, P8231, DOI 10.1109/ICCV.2019.00832
   Yu J., 2016, ACM Multimedia, Ser. ACM Multimedia
   Yuan L., 2021, arXiv: Computer Vision and Pattern Recognition
   Zhang H., 2022, Dino: Detr with improved denoising anchor boxes for end-to-end object detection
   Zhang JY, 2019, IEEE INT CONF COMP V, P1, DOI 10.1109/ICCVW.2019.00007
   Zhang S., 2020, P IEEE CVF C COMP VI, P9759
   Zhang Y.-F., 2021, arXiv: Computer Vision and Pattern Recognition
   Zheng ZH, 2020, AAAI CONF ARTIF INTE, V34, P12993
NR 58
TC 2
Z9 2
U1 7
U2 7
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD FEB
PY 2024
VL 142
AR 104898
DI 10.1016/j.imavis.2024.104898
EA JAN 2024
PG 11
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA HR3V3
UT WOS:001161202700001
DA 2024-08-05
ER

PT J
AU Chen, JL
   Li, GY
   Zhang, ZJ
   Zeng, D
AF Chen, Jianlin
   Li, Gongyang
   Zhang, Zhijiang
   Zeng, Dan
TI EFDCNet: Encoding fusion and decoding correction network for RGB-D
   indoor semantic segmentation
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE RGB-D indoor semantic segmentation; Encoding fusion; Decoding correction
AB Semantic segmentation is a crucial task in vision measurement systems that involves understanding and segmenting different objects and regions within an image. Over the years, numerous RGB-D semantic segmentation methods have been developed, leveraging the encoder -decoder architecture to achieve outstanding performance. However, existing methods have two main problems that constrain further performance improvement. Firstly, in the encoding stage, existing methods have a weak ability to fuse cross -modal information, and low -quality depth maps can easily lead to poor feature representation. Secondly, in the decoding stage, the upsampling of highlevel semantic information may cause the loss of contextual information, and low-level features from the encoder may bring noises to the decoder through skip connections. To solve these issues, we propose a novel Encoding Fusion and Decoding Correction Network (EFDCNet) for RGB-D indoor semantic segmentation. First, in the encoding stage of EFDCNet, we focus on extracting valuable information from low -quality depth maps, and employ a channel -wise filter to select informative depth features. Additionally, we establish the global dependencies between RGB and depth features via the self -attention mechanism to enhance the cross -modal feature interactions, extracting discriminant and powerful features. Then, in the decoding stage of EFDCNet, we use the highest -level information as semantic guidance to compensate for the upsampling information and filter out noise from the low-level encoder features propagated through the skip connections to the decoder. Extensive experiments conducted on two widely -used RGB-D indoor semantic segmentation datasets demonstrate that the proposed EFDCNet surpasses the performance of relevant state-of-the-art methods. The code is available at https://github.com/ Mark9010/EFDCNet
C1 [Chen, Jianlin; Li, Gongyang; Zhang, Zhijiang; Zeng, Dan] Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Shanghai 200444, Peoples R China.
   [Chen, Jianlin; Li, Gongyang; Zhang, Zhijiang; Zeng, Dan] Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
C3 Shanghai University; Shanghai University
RP Li, GY; Zhang, ZJ (corresponding author), Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Shanghai 200444, Peoples R China.
EM chen1026@shu.edu.cn; ligongyang@shu.edu.cn; zjzhang@staff.shu.edu.cn;
   dzeng@shu.edu.cn
RI Li, Gongyang/IXD-9078-2023
CR Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bottou Leon, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P421, DOI 10.1007/978-3-642-35289-8_25
   Caglayan A, 2022, IMAGE VISION COMPUT, V122, DOI 10.1016/j.imavis.2022.104453
   Cao JM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7068, DOI 10.1109/ICCV48922.2021.00700
   Cao JM, 2021, NEUROCOMPUTING, V462, P568, DOI 10.1016/j.neucom.2021.08.009
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen LZ, 2021, IEEE T IMAGE PROCESS, V30, P2313, DOI 10.1109/TIP.2021.3049332
   Cheng YH, 2017, PROC CVPR IEEE, P1475, DOI 10.1109/CVPR.2017.161
   Feng D, 2021, IEEE T INTELL TRANSP, V22, P1341, DOI 10.1109/TITS.2020.2972974
   Fu J, 2019, IEEE I CONF COMP VIS, P6747, DOI 10.1109/ICCV.2019.00685
   Gongyang Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P665, DOI 10.1007/978-3-030-58520-4_39
   Hazirbas C, 2017, LECT NOTES COMPUT SC, V10111, P213, DOI 10.1007/978-3-319-54181-5_14
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu XX, 2019, IEEE IMAGE PROC, P1440, DOI [10.1109/icip.2019.8803025, 10.1109/ICIP.2019.8803025]
   Ji W, 2021, PROC CVPR IEEE, P9466, DOI 10.1109/CVPR46437.2021.00935
   Lewandowski B, 2020, IEEE ROMAN, P363, DOI 10.1109/RO-MAN47096.2020.9223568
   Li GY, 2023, IEEE T IMAGE PROCESS, V32, P5257, DOI 10.1109/TIP.2023.3314285
   Li GY, 2023, IEEE T CIRC SYST VID, V33, P1223, DOI 10.1109/TCSVT.2022.3208833
   Li GY, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3235717
   Li GY, 2023, IEEE T CYBERNETICS, V53, P526, DOI 10.1109/TCYB.2022.3162945
   Li GY, 2021, IEEE T IMAGE PROCESS, V30, P3528, DOI 10.1109/TIP.2021.3062689
   Li GY, 2020, IEEE T IMAGE PROCESS, V29, P4873, DOI 10.1109/TIP.2020.2976689
   Li JY, 2022, IEEE T IMAGE PROCESS, V31, P3211, DOI 10.1109/TIP.2022.3166673
   Lin D, 2020, IEEE T PATTERN ANAL, V42, P2642, DOI 10.1109/TPAMI.2019.2923513
   Lin D, 2020, IEEE T CYBERNETICS, V50, P1120, DOI 10.1109/TCYB.2018.2885062
   Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Park SJ, 2017, IEEE I CONF COMP VIS, P4990, DOI 10.1109/ICCV.2017.533
   Paszke A, 2019, ADV NEUR IN, V32
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Seichter D, 2021, IEEE INT CONF ROBOT, P13525, DOI 10.1109/ICRA48506.2021.9561675
   Shaikh MB, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21124246
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655
   Tian Z, 2019, PROC CVPR IEEE, P3121, DOI 10.1109/CVPR.2019.00324
   Tsai TH, 2023, NEUROCOMPUTING, V532, P33, DOI 10.1016/j.neucom.2023.02.025
   Wang JQ, 2019, IEEE I CONF COMP VIS, P3007, DOI 10.1109/ICCV.2019.00310
   Wang JH, 2016, LECT NOTES COMPUT SC, V9909, P664, DOI 10.1007/978-3-319-46454-1_40
   Wang KK, 2014, IEEE T IMAGE PROCESS, V23, P4893, DOI 10.1109/TIP.2014.2352851
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang YK, 2023, IEEE T PATTERN ANAL, V45, P5481, DOI 10.1109/TPAMI.2022.3211086
   Wei WY, 2023, IMAGE VISION COMPUT, V138, DOI 10.1016/j.imavis.2023.104792
   Wu P, 2022, IEEE SENS J, V22, P24161, DOI 10.1109/JSEN.2022.3218601
   Wu Z., 2020, AS C COMP VIS ACCV, P388
   Wu ZW, 2022, Arxiv, DOI arXiv:2206.03939
   Xiao Y, 2019, IMAGE VISION COMPUT, V88, P67, DOI 10.1016/j.imavis.2019.05.003
   Xiaokang Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P561, DOI 10.1007/978-3-030-58621-8_33
   Yan XC, 2021, DISPLAYS, V70, DOI 10.1016/j.displa.2021.102082
   Yao CL, 2022, IMAGE VISION COMPUT, V117, DOI 10.1016/j.imavis.2021.104351
   Zhang GD, 2021, IEEE SIGNAL PROC LET, V28, P658, DOI 10.1109/LSP.2021.3066071
   Zhang H, 2018, PROC CVPR IEEE, P7151, DOI 10.1109/CVPR.2018.00747
   Zhang YF, 2021, IMAGE VISION COMPUT, V105, DOI 10.1016/j.imavis.2020.104042
   Zhang ZL, 2018, LECT NOTES COMPUT SC, V11214, P273, DOI 10.1007/978-3-030-01249-6_17
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao QK, 2023, NEUROCOMPUTING, V548, DOI 10.1016/j.neucom.2023.126389
   Zhou F, 2022, NEUROCOMPUTING, V492, P464, DOI 10.1016/j.neucom.2022.04.025
   Zhou H, 2022, PATTERN RECOGN, V124, DOI 10.1016/j.patcog.2021.108468
   Zhou WJ, 2024, IEEE T MULTIMEDIA, V26, P4564, DOI 10.1109/TMM.2023.3323890
   Zhou WJ, 2023, INFORM FUSION, V94, P32, DOI 10.1016/j.inffus.2023.01.016
   Zhou WJ, 2023, IEEE T MULTIMEDIA, V25, P3483, DOI 10.1109/TMM.2022.3161852
   Zhou XF, 2020, IMAGE VISION COMPUT, V95, DOI 10.1016/j.imavis.2020.103888
   Zhu LZ, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22218520
   Zhu ML, 2020, SCI ADV, V6, DOI 10.1126/sciadv.aaz8693
NR 66
TC 0
Z9 0
U1 14
U2 14
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD FEB
PY 2024
VL 142
AR 104892
DI 10.1016/j.imavis.2023.104892
EA JAN 2024
PG 11
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA GR6K4
UT WOS:001154437000001
DA 2024-08-05
ER

PT J
AU Zou, CZ
   Wang, ZY
AF Zou, Changzhong
   Wang, Ziyuan
TI A semi-parallel CNN-transformer fusion network for semantic change
   detection
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Fusion semantic change detection network; (FSCD); Transformer;
   Convolutional neural network (CNN); Siamese
ID UNSUPERVISED CHANGE DETECTION; IMAGE
AB Semantic change detection (SCD) can recognize the region and the type of changes in remote sensing images. Existing methods are either based on transformer or convolutional neural network (CNN), but due to the size of various ground objects is different, it is necessary to have global modeling ability and local information extraction ability at the same time. Therefore, in this paper we propose a fusion semantic change detection network (FSCD) with both global modeling ability and local information extraction ability by fusing transformer and CNN. A semi-parallel fusion block has also been proposed to construct FSCD. It can not only have global and local features in parallel, but also fuse them as deeply as serial. To better adaptively decide which mechanism is applied to which pixel, we design a self-attention and convolution selection module (ACSM). ACSM is a selfattention mechanism used to selectively combine transformer and CNN. Specifically, the importance of each mechanism is automatically obtained by learning. According to the importance, the mechanism suitable for a pixel is selected, which is better than using either mechanism alone. We evaluate the proposed FSCD on two datasets, and the proposed network has a significant improvement compared with the state-of-the-art network.
C1 [Zou, Changzhong; Wang, Ziyuan] Fuzhou Univ, Coll Comp & Data Sci, Fuzhou 350000, Peoples R China.
C3 Fuzhou University
RP Zou, CZ; Wang, ZY (corresponding author), Fuzhou Univ, Coll Comp & Data Sci, Fuzhou 350000, Peoples R China.
EM chzhzou@fzu.edu.cn; 211027188@fzu.edu.cn
FU National Natural Science Founda- tion of China [62076065]; Natural
   Science Foundation of Fujian Province [2021J01611]; Development Program
   of Fuzhou University [GXRC-21007, XRC-18080]
FX This paper was supported by the National Natural Science Founda- tion of
   China, grant No. 62076065, the Natural Science Foundation of Fujian
   Province, grant No. 2021J01611, and the Development Program of Fuzhou
   University, Grant No. GXRC-21007 and XRC-18080.
CR Bandara WGC, 2022, INT GEOSCI REMOTE SE, P207, DOI 10.1109/IGARSS46834.2022.9883686
   Bovolo F, 2007, IEEE T GEOSCI REMOTE, V45, P218, DOI 10.1109/TGRS.2006.885408
   Bruzzone L, 2000, IEEE T GEOSCI REMOTE, V38, P1171, DOI 10.1109/36.843009
   Bruzzone L, 2002, IEEE T IMAGE PROCESS, V11, P452, DOI 10.1109/TIP.2002.999678
   Carlotto MJ, 1997, IEEE T IMAGE PROCESS, V6, P189, DOI 10.1109/83.552106
   Chatelain F, 2007, IEEE T IMAGE PROCESS, V16, P1796, DOI 10.1109/TIP.2007.896651
   Chen H, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3095166
   Chen H, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12101662
   Chen Q, 2022, PROC CVPR IEEE, P5239, DOI 10.1109/CVPR52688.2022.00518
   Cui FZ, 2023, INT J APPL EARTH OBS, V118, DOI 10.1016/j.jag.2023.103294
   Daudt RC, 2019, COMPUT VIS IMAGE UND, V187, DOI 10.1016/j.cviu.2019.07.003
   Daudt RC, 2018, IEEE IMAGE PROC, P4063, DOI 10.1109/ICIP.2018.8451652
   Ding L, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3154390
   Lanza A, 2011, IEEE T PATTERN ANAL, V33, P1894, DOI 10.1109/TPAMI.2011.42
   Lee S.H., 2021, arXiv, DOI DOI 10.48550/ARXIV.2112.13492
   Li XH, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2021.3098774
   Liao MH, 2023, IEEE T PATTERN ANAL, V45, P919, DOI 10.1109/TPAMI.2022.3155612
   Liu M., 2021 IEEE INT GEOSC, P6159
   Liu SC, 1998, IEEE T IMAGE PROCESS, V7, P1258, DOI 10.1109/83.709658
   Liu Y, 2021, IEEE GEOSCI REMOTE S, V18, P811, DOI 10.1109/LGRS.2020.2988032
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Mou LC, 2019, IEEE T GEOSCI REMOTE, V57, P924, DOI 10.1109/TGRS.2018.2863224
   Nemmour H, 2010, J INDIAN SOC REMOT, V38, P585, DOI 10.1007/s12524-011-0060-z
   Peng DF, 2021, INT J APPL EARTH OBS, V103, DOI 10.1016/j.jag.2021.102465
   Peng DF, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11111382
   Peng XL, 2021, IEEE T GEOSCI REMOTE, V59, P7296, DOI 10.1109/TGRS.2020.3033009
   Prendes J, 2015, IEEE T IMAGE PROCESS, V24, P799, DOI 10.1109/TIP.2014.2387013
   Robin A, 2010, IEEE T PATTERN ANAL, V32, P1977, DOI 10.1109/TPAMI.2010.37
   Sachdeva R, 2023, Arxiv, DOI arXiv:2308.10417
   Shi N, 2022, IEEE J-STARS, V15, P4897, DOI 10.1109/JSTARS.2022.3176858
   Si C., 2022, arXiv, DOI DOI 10.48550/ARXIV.2205.12956
   Song A, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10111827
   Wang W, 2022, IEEE J-STARS, V15, P6817, DOI 10.1109/JSTARS.2022.3198517
   Wang WH, 2022, COMPUT VIS MEDIA, V8, P415, DOI 10.1007/s41095-022-0274-8
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wang X, 2020, IEEE J-STARS, V13, P6260, DOI 10.1109/JSTARS.2020.3029460
   Wang X, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10020276
   Xia H, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3171067
   Yang KP, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3113912
   Yuan PL, 2022, INT J DIGIT EARTH, V15, P1506, DOI 10.1080/17538947.2022.2111470
   Zanetti M, 2015, IEEE T IMAGE PROCESS, V24, P5004, DOI 10.1109/TIP.2015.2474710
   Zhan Y, 2017, IEEE GEOSCI REMOTE S, V14, P1845, DOI 10.1109/LGRS.2017.2738149
   Zhang CX, 2020, ISPRS J PHOTOGRAMM, V166, P183, DOI 10.1016/j.isprsjprs.2020.06.003
   Zhang C, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3160007
   Zhang MY, 2019, IEEE GEOSCI REMOTE S, V16, P266, DOI 10.1109/LGRS.2018.2869608
   Zhang M, 2020, IEEE T GEOSCI REMOTE, V58, P7232, DOI 10.1109/TGRS.2020.2981051
   Zhang WX, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11030240
   Zhao MQ, 2022, IEEE J-STARS, V15, P2563, DOI 10.1109/JSTARS.2022.3159528
   Zhao WZ, 2020, IEEE T GEOSCI REMOTE, V58, P2720, DOI 10.1109/TGRS.2019.2953879
   Zheng Z, 2022, ISPRS J PHOTOGRAMM, V183, P228, DOI 10.1016/j.isprsjprs.2021.10.015
NR 50
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD SEP
PY 2024
VL 149
AR 105157
DI 10.1016/j.imavis.2024.105157
EA JUL 2024
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA YW7Y6
UT WOS:001271600200001
DA 2024-08-05
ER

PT J
AU Zhang, SQ
   Zhang, L
   Liu, ZY
AF Zhang, Siqi
   Zhang, Lu
   Liu, Zhiyong
TI Active domain adaptation for semantic segmentation via dynamically
   balancing domainness and uncertainty
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Active learning; Domain adaptation; Semantic segmentation; Transfer
   learning
AB Active domain adaptation aims to enhance model adaptation performance by annotating a limited number of informative unlabeled target data. Traditional active learning strategies for semantic segmentation often neglect the presence of domain shifts, resulting in suboptimal results in domain adaptation scenarios. In this paper, we present a novel active domain adaptation approach for semantic segmentation that maximizes segmentation performance under domain shifts with a limited number of queried target labels. To recognize the most valuable samples for labeling, we introduce a new acquisition strategy. This strategy leverages a target domainness map to identify the most informative samples for reducing the domain gap and employs region-aware prediction uncertainty to explore ambiguous samples. Meanwhile, to optimize the efficiency of the acquisition strategy, we dynamically adjust the balance between prediction uncertainty and target domainness over the selection rounds. To further bolster adaptation performance, a smooth loss function is employed for the target data, which promotes consistency in local predictions. Extensive experiments on two benchmarks, GTAV -* Cityscapes and SYNTHIA -* Cityscapes, demonstrate that our method surpasses existing active domain adaptation methods for semantic segmentation. Moreover, it achieves comparable results to supervised performance with only 5% annotations in the target domain, validating the effectiveness of our method.
C1 [Zhang, Siqi; Zhang, Lu; Liu, Zhiyong] Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing, Peoples R China.
   [Zhang, Siqi; Liu, Zhiyong] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.
   [Liu, Zhiyong] Nanjing Artificial Intelligence Res IA, Nanjing, Jiangsu, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS
RP Liu, ZY (corresponding author), Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing, Peoples R China.
EM zhiyong.liu@ia.ac.cn
FU National Key Research and Development Plan of China [2020AAA0108902];
   NSFC [62206288]
FX This work was supported in part by the National Key Research and
   Development Plan of China under Grant 2020AAA0108902; in part by the
   NSFC under Grant 62206288.
CR [Anonymous], 2015, INT C LEARN REPR
   Bai LB, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3198972
   Cai LL, 2021, PROC CVPR IEEE, P10983, DOI 10.1109/CVPR46437.2021.01084
   Chen L.-C., 2018, P EUR C COMP VIS ECC, P801, DOI DOI 10.1007/978-3-030-01234-2_49
   Chen YH, 2018, PROC CVPR IEEE, P7892, DOI 10.1109/CVPR.2018.00823
   Cheng DQ, 2022, IMAGE VISION COMPUT, V124, DOI 10.1016/j.imavis.2022.104493
   Cohn DA, 1996, J ARTIF INTELL RES, V4, P129, DOI 10.1613/jair.295
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Deheeger Francois, 2021, P INT C LEARN REPR
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Feng HT, 2023, IMAGE VISION COMPUT, V140, DOI 10.1016/j.imavis.2023.104856
   Fu B, 2021, PROC CVPR IEEE, P7268, DOI 10.1109/CVPR46437.2021.00719
   Gal Y, 2017, PR MACH LEARN RES, V70
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hoffman J, 2016, Arxiv, DOI arXiv:1612.02649
   Hu ZY, 2022, LECT NOTES COMPUT SC, V13687, P248, DOI 10.1007/978-3-031-19812-0_15
   Huang DJ, 2023, PROC CVPR IEEE, P7651, DOI 10.1109/CVPR52729.2023.00739
   Inkyu Shin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P532, DOI 10.1007/978-3-030-58601-0_32
   Iqbal J, 2022, IMAGE VISION COMPUT, V124, DOI 10.1016/j.imavis.2022.104504
   Jain SD, 2016, PROC CVPR IEEE, P2864, DOI 10.1109/CVPR.2016.313
   Ke Mei, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P415, DOI 10.1007/978-3-030-58574-7_25
   Kirsch A, 2019, ADV NEUR IN, V32
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694
   Lai X, 2022, LECT NOTES COMPUT SC, V13693, P369, DOI 10.1007/978-3-031-19827-4_22
   Li RH, 2022, PROC CVPR IEEE, P11583, DOI 10.1109/CVPR52688.2022.01130
   Li YA, 2023, IMAGE VISION COMPUT, V137, DOI 10.1016/j.imavis.2023.104755
   Liu YH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8781, DOI 10.1109/ICCV48922.2021.00868
   Luo YW, 2022, IEEE T PATTERN ANAL, V44, P3940, DOI 10.1109/TPAMI.2021.3064379
   Lv FM, 2021, IEEE T CIRC SYST VID, V31, P3493, DOI 10.1109/TCSVT.2020.3040343
   Ning MN, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9092, DOI 10.1109/ICCV48922.2021.00898
   Ovadia Y, 2019, ADV NEUR IN, V32
   Panda Shivam K., 2023, P IEEE CVF C COMP VI, P6271
   Paszke A, 2019, ADV NEUR IN, V32
   Prabhu V, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8485, DOI 10.1109/ICCV48922.2021.00839
   Rangnekar A, 2023, IEEE WINT CONF APPL, P5955, DOI 10.1109/WACV56688.2023.00591
   Richter SR, 2016, LECT NOTES COMPUT SC, V9906, P102, DOI 10.1007/978-3-319-46475-6_7
   Ros G, 2016, PROC CVPR IEEE, P3234, DOI 10.1109/CVPR.2016.352
   Shin G, 2021, IEEE INT CONF COMP V, P1687, DOI 10.1109/ICCVW54120.2021.00194
   Shin I, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8568, DOI 10.1109/ICCV48922.2021.00847
   Siddiqui Yawar, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9430, DOI 10.1109/CVPR42600.2020.00945
   Su JC, 2020, IEEE WINT CONF APPL, P728, DOI [10.1109/wacv45572.2020.9093390, 10.1109/WACV45572.2020.9093390]
   Teichmann M, 2018, IEEE INT VEH SYM, P1013, DOI 10.1109/IVS.2018.8500504
   Tsai YH, 2018, PROC CVPR IEEE, P7472, DOI 10.1109/CVPR.2018.00780
   Vu TH, 2019, PROC CVPR IEEE, P2512, DOI 10.1109/CVPR.2019.00262
   Wu TH, 2022, LECT NOTES COMPUT SC, V13689, P449, DOI 10.1007/978-3-031-19818-2_26
   Wu TH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15490, DOI 10.1109/ICCV48922.2021.01522
   Xie BH, 2023, IEEE T PATTERN ANAL, V45, P9004, DOI 10.1109/TPAMI.2023.3237740
   Xie BH, 2022, PROC CVPR IEEE, P8058, DOI 10.1109/CVPR52688.2022.00790
   Xie BH, 2022, AAAI CONF ARTIF INTE, P8708
   Xie C, 2021, IEEE T ROBOT, V37, P1343, DOI 10.1109/TRO.2021.3060341
   Xie Mixue, 2023, INT C LEARN REPR ICL
   You FM, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P1866, DOI 10.1145/3503161.3548079
   Zhang J, 2018, PROC CVPR IEEE, P8156, DOI 10.1109/CVPR.2018.00851
   Zhang L, 2023, IEEE INT CONF ROBOT, P4945, DOI 10.1109/ICRA48891.2023.10160742
   Zhang P, 2021, PROC CVPR IEEE, P12409, DOI 10.1109/CVPR46437.2021.01223
   Zou Y, 2019, IEEE I CONF COMP VIS, P5981, DOI 10.1109/ICCV.2019.00608
NR 57
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD AUG
PY 2024
VL 148
AR 105132
DI 10.1016/j.imavis.2024.105132
EA JUN 2024
PG 11
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA XD7A0
UT WOS:001259799200001
DA 2024-08-05
ER

PT J
AU Cheng, C
   Xu, HH
AF Cheng, Chen
   Xu, Huahu
TI A 3D motion image recognition model based on 3D CNN-GRU model and
   attention mechanism
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Graph convolutional networks; Cross -graph convolution operation;
   Residual connections; Efficiency optimization; Transfer learning; 3D CNN
AB Moving image recognition has become a well-explored problem in computer vision. However, it is difficult for the traditional convolutional neural network (CNN) model to effectively capture timing information in motion. For better use of video sequence features and to improve the accuracy of action recognition, Therefore, this paper proposes a Three-dimensional CNN (3DCNN) model based on Gated Recurrent Unit (GRU) with an attention mechanism. The model leverages 3DCNN for deep feature extraction from video frames, employs GRU to capture the temporal dynamics of feature sequences and incorporates an attention mechanism to emphasize key frames, which improves moving image recognition. Demonstrating superior accuracy in'Cross-Subject' and'Cross-View' evaluations, our model surpasses standard benchmarks with accuracies of 83.2% and 87.3% respectively.
C1 [Cheng, Chen; Xu, Huahu] Shanghai Univ, Sch Comp Engn & Sci, Shanghai 200444, Peoples R China.
C3 Shanghai University
RP Xu, HH (corresponding author), Shanghai Univ, Sch Comp Engn & Sci, Shanghai 200444, Peoples R China.
EM chengch88@shu.edu.cn; huahuxu@staff.shu.edu.cn
RI Chen, Cheng/KHD-2838-2024
OI Chen, Cheng/0009-0009-8134-983X
CR Ai L, 2021, MACH LEARN, V110, P695, DOI 10.1007/s10994-020-05941-0
   Ailing Zeng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P507, DOI 10.1007/978-3-030-58568-6_30
   Aung N, 2023, Arxiv, DOI arXiv:2308.12761
   Awan U, 2021, TECHNOL FORECAST SOC, V168, DOI 10.1016/j.techfore.2021.120766
   Bétard F, 2019, ENVIRON MANAGE, V63, P822, DOI 10.1007/s00267-019-01168-5
   Cai YJ, 2019, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2019.00236
   Chatterjee M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1184, DOI 10.1109/ICCV48922.2021.00124
   Du ZX, 2023, IMAGE VISION COMPUT, V137, DOI 10.1016/j.imavis.2023.104789
   Dubey S, 2023, MULTIMEDIA SYST, V29, P167, DOI 10.1007/s00530-022-00980-0
   Fu J, 2022, IEEE T CIRC SYST VID, V32, P5213, DOI 10.1109/TCSVT.2021.3137023
   Gao LL, 2022, IEEE T MULTIMEDIA, V24, P4493, DOI 10.1109/TMM.2021.3119177
   Gul S, 2021, EXPERT SYST APPL, V179, DOI 10.1016/j.eswa.2021.115057
   Hedlin E., 2022, 2022 19 C ROB VIS CR, P1
   Hossain M.R.I., 2017, P EUR C COMP VIS ECC, P68
   Host K, 2022, HELIYON, V8, DOI 10.1016/j.heliyon.2022.e09633
   Hu ZF, 2022, ENG LET, V30
   Kanazawa A, 2018, PROC CVPR IEEE, P7122, DOI 10.1109/CVPR.2018.00744
   Kumawat S, 2022, IEEE T PATTERN ANAL, V44, P4839, DOI 10.1109/TPAMI.2021.3076522
   Lai QX, 2021, IEEE T MULTIMEDIA, V23, P2086, DOI 10.1109/TMM.2020.3007321
   Lee E.A., 2006, Em, V2, P1
   Lee K, 2018, LECT NOTES COMPUT SC, V11211, P123, DOI 10.1007/978-3-030-01234-2_8
   Li D, 2023, APPL SOFT COMPUT, V144, DOI 10.1016/j.asoc.2023.110487
   Liang J., 2022, Adv. Neural Inf. Proces. Syst., V35, P14475
   Liu CC, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3115574
   Luo HL, 2022, J ELECTRON IMAGING, V31, DOI 10.1117/1.JEI.31.4.043007
   Mallick R, 2022, IEEE MULTIMEDIA, V29, P7, DOI 10.1109/MMUL.2022.3147381
   O'Mahony N, 2020, ADV INTELL SYST COMP, V943, P128, DOI 10.1007/978-3-030-17795-9_10
   Punnakkal AR, 2021, PROC CVPR IEEE, P722, DOI 10.1109/CVPR46437.2021.00078
   Qi Xiangyuan, 2023, 2023 IEEE 18th Conference on Industrial Electronics and Applications (ICIEA), P1477, DOI 10.1109/ICIEA58696.2023.10241867
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Selva J, 2023, IEEE T PATTERN ANAL, V45, P12922, DOI 10.1109/TPAMI.2023.3243465
   Singh P., 2023, J. Inform. Assurance Secur., V18
   Singh SP, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20185097
   Sun CJ, 2023, MECH SYST SIGNAL PR, V190, DOI 10.1016/j.ymssp.2023.110141
   Tajbakhsh N, 2020, MED IMAGE ANAL, V63, DOI 10.1016/j.media.2020.101693
   Teng QY, 2022, MULTIMEDIA SYST, V28, P2335, DOI 10.1007/s00530-022-00960-4
   Tsai YHH, 2019, PROC CVPR IEEE, P10416, DOI 10.1109/CVPR.2019.01067
   Ullah W, 2021, MULTIMED TOOLS APPL, V80, P16979, DOI 10.1007/s11042-020-09406-3
   Varshney N., 2021, Multimed. Tools Appl., P1
   Wu LY, 2023, PATTERN RECOGN, V136, DOI 10.1016/j.patcog.2022.109231
   Yang H, 2020, IEEE T IMAGE PROCESS, V29, P5783, DOI 10.1109/TIP.2020.2984904
   Ying W, 2023, J APPL SCI ENG, V26, P357, DOI 10.6180/jase.202303_26(3).0007
   Zhang B, 2020, REMOTE SENS ENVIRON, V247, DOI 10.1016/j.rse.2020.111938
   Zhang L, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14184441
   Zhang T, 2019, IEEE T CYBERNETICS, V49, P839, DOI 10.1109/TCYB.2017.2788081
   Zhu JG, 2019, NEUROCOMPUTING, V370, P109, DOI 10.1016/j.neucom.2019.08.043
NR 46
TC 0
Z9 0
U1 2
U2 2
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUN
PY 2024
VL 146
AR 104991
DI 10.1016/j.imavis.2024.104991
EA APR 2024
PG 11
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA RJ6Q7
UT WOS:001227340100001
DA 2024-08-05
ER

PT J
AU Li, BF
   Ruan, HH
   Li, XW
   Wang, KP
AF Li, Bingfeng
   Ruan, Haohao
   Li, Xinwei
   Wang, Keping
TI Feature disparity learning for weakly supervised object localization
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Weakly supervised learning; Object localization; Adversarial erasing;
   Spatial attention; Similarity measure
AB Weakly supervised object localization (WSOL) aims to localize objects with only image-level labels. As a common WSOL method, adversarial erasing always masks the most discriminative region in the feature space to compel the network to localize more regions of the object. However, with the discriminative region vanishing, the localizer is confused when distinguishing the regions of object from the background. In this paper, we propose a new feature disparity learning (FDL), which encourages the network to learn more distinctive features from the object region with similarity measurement after feature enhancement. Specifically, we first introduce a Spatial Vector Cross Attention (SVCA) module. This module enhances responses in less discriminative region of erased feature maps by reintegrating the spatial distribution of features through the capture of interdependencies among spatial vectors on each channel. Furthermore, we propose a feature complementarity loss to measure the similarity between unerased features and erased features, guiding the network to learn feature disparities caused by adversarial erasing for improved localization and classification. Several experimental studies demonstrate a significant increase in localization performance over the existing state -of -the -art erasing methods on the CUB 200 - 2011 and ILSVRC 2016 datasets.
C1 [Li, Bingfeng; Ruan, Haohao; Li, Xinwei; Wang, Keping] Henan Polytech Univ, Sch Elect Engn & Automat, Jiaozuo 454003, Peoples R China.
   [Li, Bingfeng; Li, Xinwei; Wang, Keping] Henan Key Lab Intelligent Detect & Control Coal Mi, Jiaozuo 454003, Peoples R China.
C3 Henan Polytechnic University
RP Ruan, HH (corresponding author), Henan Polytech Univ, Sch Elect Engn & Automat, Jiaozuo 454003, Peoples R China.
EM 212207020023@home.hpu.edu.cn
RI Wang, Keping/KVA-9146-2024
CR Babar S, 2021, IEEE WINT CONF APPL, P1009, DOI 10.1109/WACV48630.2021.00105
   Choe J, 2019, PROC CVPR IEEE, P2214, DOI 10.1109/CVPR.2019.00232
   Guo GY, 2021, PROC CVPR IEEE, P7399, DOI 10.1109/CVPR46437.2021.00732
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hwang D, 2023, PATTERN RECOGN LETT, V169, P1, DOI 10.1016/j.patrec.2023.03.018
   Jinjie Mai, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8763, DOI 10.1109/CVPR42600.2020.00879
   Jungbeom Lee, 2021, 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), P4070, DOI 10.1109/CVPR46437.2021.00406
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Kim E, 2022, PROC CVPR IEEE, P14238, DOI 10.1109/CVPR52688.2022.01386
   Koo B, 2023, IMAGE VISION COMPUT, V129, DOI 10.1016/j.imavis.2022.104598
   Koo B, 2021, MULTIDIM SYST SIGN P, V32, P1185, DOI 10.1007/s11045-021-00778-9
   Lee Jungbeom, 2021, Advances in Neural Information Processing Systems, V34
   Lee Wonyoung, 2020, P AS C COMP VIS
   Li J, 2022, Adv Neural Inf Process Syst, V35, P16037
   Li JL, 2023, IEEE T MULTIMEDIA, V25, P1686, DOI 10.1109/TMM.2022.3152388
   Lin M, 2014, Arxiv, DOI arXiv:1312.4400
   Murtaza S, 2023, IMAGE VISION COMPUT, V140, DOI 10.1016/j.imavis.2023.104838
   Pan XJ, 2021, PROC CVPR IEEE, P11637, DOI 10.1109/CVPR46437.2021.01147
   Peyre J, 2017, IEEE I CONF COMP VIS, P5189, DOI 10.1109/ICCV.2017.554
   Qin ZQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P763, DOI 10.1109/ICCV48922.2021.00082
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rochan M, 2016, IMAGE VISION COMPUT, V56, P1, DOI 10.1016/j.imavis.2016.08.015
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shao FF, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3321, DOI 10.1145/3474085.3475485
   Shieh JL, 2023, PATTERN RECOGN LETT, V170, P56, DOI 10.1016/j.patrec.2023.04.017
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh Krishna Kumar, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P3544, DOI 10.1109/ICCV.2017.381
   Sutskever I., 2013, P 30 INT C MACH LEAR, P1147
   Wah C., 2011, Tech. Rep. CNS-TR-2011-001
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/ISQED48828.2020.9137057, 10.1109/isqed48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wei XS, 2019, PATTERN RECOGN, V88, P113, DOI 10.1016/j.patcog.2018.10.022
   Wei YC, 2017, PROC CVPR IEEE, P6488, DOI 10.1109/CVPR.2017.687
   Weizeng Lu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P481, DOI 10.1007/978-3-030-58574-7_29
   Wonho Bae, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P618, DOI 10.1007/978-3-030-58555-6_37
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xie JH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P132, DOI 10.1109/ICCV48922.2021.00020
   Xie XY, 2023, ISA T, V132, P39, DOI 10.1016/j.isatra.2022.08.003
   Xu JL, 2022, PROC CVPR IEEE, P9427, DOI 10.1109/CVPR52688.2022.00922
   Xu L, 2022, PROC CVPR IEEE, P4300, DOI 10.1109/CVPR52688.2022.00427
   Xue HL, 2019, IEEE I CONF COMP VIS, P6588, DOI 10.1109/ICCV.2019.00669
   Yang LX, 2021, PR MACH LEARN RES, V139
   Yao Yuan, 2022, IEEE Transactions on Neural Networks and Learning Systems
   Yin JH, 2021, INT C PATT RECOG, P4229, DOI 10.1109/ICPR48806.2021.9412181
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Zhang C.-L., 2020, P IEEE CVF C COMP VI, P13460
   Zhang DW, 2022, IEEE T PATTERN ANAL, V44, P5866, DOI 10.1109/TPAMI.2021.3074313
   Zhang X, 2021, AUTOPHAGY, V17, P1519, DOI 10.1080/15548627.2020.1840796
   Zhang XL, 2018, PROC CVPR IEEE, P1325, DOI 10.1109/CVPR.2018.00144
   Zhang XL, 2018, LECT NOTES COMPUT SC, V11216, P610, DOI 10.1007/978-3-030-01258-8_37
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhou ZH, 2018, NATL SCI REV, V5, P44, DOI 10.1093/nsr/nwx106
   Zhu K, 2023, 2023 IEEE 9TH INTL CONFERENCE ON BIG DATA SECURITY ON CLOUD, BIGDATASECURITY, IEEE INTL CONFERENCE ON HIGH PERFORMANCE AND SMART COMPUTING, HPSC AND IEEE INTL CONFERENCE ON INTELLIGENT DATA AND SECURITY, IDS, P10, DOI 10.1109/BigDataSecurity-HPSC-IDS58521.2023.00013
   Zhu LH, 2023, Arxiv, DOI arXiv:2304.01184
NR 54
TC 0
Z9 0
U1 1
U2 1
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD MAY
PY 2024
VL 145
AR 104986
DI 10.1016/j.imavis.2024.104986
EA MAR 2024
PG 11
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA QU8R3
UT WOS:001223478900001
DA 2024-08-05
ER

PT J
AU Wang, M
   Wang, Y
   Liu, HP
AF Wang, Meng
   Wang, Yang
   Liu, Haipeng
TI Explicit knowledge transfer of graph-based correlation distillation and
   diversity data hallucination for few-shot object detection
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Few -shot object detection; Graph convolutional network; Knowledge
   distillation; Data hallucination
AB The performance of few-shot object detection has seen marked improvement through fine-tuning paradigms. However, existing methods often depend on shared parameters to implicitly transfer knowledge without explicit induction. This results in novel-class representations that are easily confused with similar base classes and poorly suited to diverse patterns of variation in the truth distribution. In view of this, the present paper focuses on mining transferable base-class knowledge, which is further subdivided into inter-class correlation and intra-class diversity. First, we design a graph to dynamically capture the relationship between base and novel class representations, and then introduce distillation techniques to tackle the shortage of correlation knowledge in fewshot labels. Furthermore, an efficient diversity knowledge transfer module based on the data hallucination is proposed, which can adaptively disentangle class-independent variation patterns from base-class features and generate additional trainable hallucinated instances for novel classes. Experiments on VOC and COCO datasets confirmed that our proposed method effectively reduces the reliance on novel-class samples and demonstrates superior performance compared to other state-of-the-art baseline methods.
C1 [Wang, Meng; Wang, Yang; Liu, Haipeng] Kunming Univ Sci & Technol, Fac Informat Engn & Automat, Kunming 650500, Yunnan, Peoples R China.
C3 Kunming University of Science & Technology
RP Wang, Y (corresponding author), Kunming Univ Sci & Technol, Fac Informat Engn & Automat, Kunming 650500, Yunnan, Peoples R China.
EM wangmeng@kmust.edu.cn; yangwang040128@gmail.com
FU National Natural Science Foundation of China [62062048]; Yunnan
   Provincial Science and Technology Plan Project [202201AT070113]; Faculty
   of Information Engineering and Automation, Kunming University of Sci-
   ence and Technology
FX The work is supported by National Natural Science Foundation of China
   (62062048) and Yunnan Provincial Science and Technology Plan Project
   (202201AT070113) . This work is also supported by Faculty of Information
   Engineering and Automation, Kunming University of Sci- ence and
   Technology.
CR Cao JZ, 2023, IMAGE VISION COMPUT, V137, DOI 10.1016/j.imavis.2023.104757
   Cao YH, 2021, ADV NEUR IN, V34
   Chen D, 2012, LECT NOTES COMPUT SC, V7574, P566, DOI 10.1007/978-3-642-33712-3_41
   Chen MT, 2020, AAAI CONF ARTIF INTE, V34, P10559
   Chen RQ, 2020, AAAI CONF ARTIF INTE, V34, P10575
   Chen TS, 2022, IEEE T PATTERN ANAL, V44, P1371, DOI 10.1109/TPAMI.2020.3025814
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Duan YQ, 2019, PROC CVPR IEEE, P3410, DOI 10.1109/CVPR.2019.00353
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fan ZB, 2021, PROC CVPR IEEE, P4525, DOI 10.1109/CVPR46437.2021.00450
   Furlanello T, 2018, PR MACH LEARN RES, V80
   Gidaris S, 2018, PROC CVPR IEEE, P4367, DOI 10.1109/CVPR.2018.00459
   Guirguis K, 2022, IEEE COMPUT SOC CONF, P4048, DOI 10.1109/CVPRW56347.2022.00449
   Han GX, 2022, PROC CVPR IEEE, P5311, DOI 10.1109/CVPR52688.2022.00525
   Han GX, 2022, AAAI CONF ARTIF INTE, P780
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Hu HZ, 2021, PROC CVPR IEEE, P10180, DOI 10.1109/CVPR46437.2021.01005
   Gulrajani I, 2017, ADV NEUR IN, V30
   Jiaxi Wu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P456, DOI 10.1007/978-3-030-58517-4_27
   Kai Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13467, DOI 10.1109/CVPR42600.2020.01348
   Kampffmeyer M, 2019, PROC CVPR IEEE, P11479, DOI 10.1109/CVPR.2019.01175
   Kang BY, 2019, IEEE I CONF COMP VIS, P8419, DOI 10.1109/ICCV.2019.00851
   Kaul P, 2022, PROC CVPR IEEE, P14217, DOI 10.1109/CVPR52688.2022.01384
   Khandelwal S, 2021, PROC CVPR IEEE, P5947, DOI 10.1109/CVPR46437.2021.00589
   Li AX, 2021, PROC CVPR IEEE, P3093, DOI 10.1109/CVPR46437.2021.00311
   Li BH, 2021, PROC CVPR IEEE, P7359, DOI 10.1109/CVPR46437.2021.00728
   Li BW, 2022, LECT NOTES COMPUT SC, V13699, P427, DOI 10.1007/978-3-031-19842-7_25
   Lin CC, 2021, IEEE T IMAGE PROCESS, V30, P9245, DOI 10.1109/TIP.2021.3124322
   Lin CC, 2019, IEEE IMAGE PROC, P3302, DOI [10.1109/icip.2019.8803420, 10.1109/ICIP.2019.8803420]
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu L, 2020, INT J COMPUT VISION, V128, P261, DOI 10.1007/s11263-019-01247-4
   Liu WJ, 2022, Arxiv, DOI arXiv:2108.02235
   Liu ZW, 2019, PROC CVPR IEEE, P2532, DOI 10.1109/CVPR.2019.00264
   Lu Y, 2023, IEEE T CYBERNETICS, V53, P514, DOI 10.1109/TCYB.2022.3149825
   Marino K, 2017, PROC CVPR IEEE, P20, DOI 10.1109/CVPR.2017.10
   Kipf TN, 2017, Arxiv, DOI arXiv:1609.02907
   Pei WJ, 2022, LECT NOTES COMPUT SC, V13670, P283, DOI 10.1007/978-3-031-20080-9_17
   Qiao LM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8661, DOI 10.1109/ICCV48922.2021.00856
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Schwartz E, 2018, ADV NEUR IN, V31
   Sun B, 2021, PROC CVPR IEEE, P7348, DOI 10.1109/CVPR46437.2021.00727
   Upchurch P, 2017, PROC CVPR IEEE, P6090, DOI 10.1109/CVPR.2017.645
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vu AKN, 2022, IMAGE VISION COMPUT, V120, DOI 10.1016/j.imavis.2022.104398
   Wang X., 2020, P INT C MACH LEARN, P9919
   Wang XH, 2023, IEEE T PATTERN ANAL, V45, P6605, DOI 10.1109/TPAMI.2020.3015894
   Wang XH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8148, DOI 10.1109/ICCV48922.2021.00806
   Wang YX, 2018, PROC CVPR IEEE, P7278, DOI 10.1109/CVPR.2018.00760
   Wu AM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9547, DOI 10.1109/ICCV48922.2021.00943
   Xiao Y, 2023, IEEE T PATTERN ANAL, V45, P3090, DOI 10.1109/TPAMI.2022.3174072
   Yan XP, 2019, IEEE I CONF COMP VIS, P9576, DOI 10.1109/ICCV.2019.00967
   Yang S, 2021, Arxiv, DOI [arXiv:2101.06395, DOI 10.48550/ARXIV.2101.06395]
   Yang Y, 2021, FRONT INFORM TECH EL, V22, P1551, DOI 10.1631/FITEE.2100463
   Yang Z, 2023, IEEE T IMAGE PROCESS, V32, P321, DOI 10.1109/TIP.2022.3228162
   Zhang S, 2022, PROC CVPR IEEE, P19185, DOI 10.1109/CVPR52688.2022.01861
   Zhang WL, 2021, PROC CVPR IEEE, P13003, DOI 10.1109/CVPR46437.2021.01281
   Zhao ZY, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P6831, DOI 10.1145/3503161.3548062
   Zhu CC, 2021, PROC CVPR IEEE, P8778, DOI 10.1109/CVPR46437.2021.00867
   Zhu XZ, 2021, Arxiv, DOI [arXiv:2010.04159, 10.48550/arXiv.2010.04159]
NR 61
TC 0
Z9 0
U1 4
U2 4
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD MAR
PY 2024
VL 143
AR 104958
DI 10.1016/j.imavis.2024.104958
EA FEB 2024
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA QV0Y9
UT WOS:001223539100001
DA 2024-08-05
ER

PT J
AU Lu, SQ
   Guan, FX
   Lai, HT
AF Lu, Siqi
   Guan, Fengxu
   Lai, Haitao
TI Underwater image enhancement based on global features and prior
   distribution guided
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Underwater image enhancement; Global features; Generalization
   capabilities; Condition variational auto -encoder
AB Underwater images often suffer from substantial image blur and color distortion due to the variability of water conditions and the physical location of optical equipment, which significantly impacts the underwater intelligent system's environmental perception. Standard methods exhibit limited generalization capabilities, leading to considerable performance fluctuations when handling images with uncontrolled degradation. In this research, we leverage global features and the prior distribution of ground truth images to guide our enhancement model, introducing a novel conditional Variational Auto-Encoder-based model, named UWG-VAE, to address these challenges. UWG-VAE enhances model controllability by incorporating prior distribution information and classes of degraded styles into the decoder of the enhancement model. We assess the performance of UWG-VAE in underwater image enhancement tasks across four challenging real underwater image datasets, comparing it to state-of-the-art models. UWG-VAE demonstrates a substantial enhancement in visual quality, with notable improvements in UIQM, UCIQE, and URanker evaluation metrics when compared to existing state-of-the-art models.
C1 [Lu, Siqi; Guan, Fengxu; Lai, Haitao] Harbin Engn Univ, Coll Intelligent Syst Sci & Engn, Harbin 15001, Heilongjiang, Peoples R China.
C3 Harbin Engineering University
RP Guan, FX (corresponding author), Harbin Engn Univ, Coll Intelligent Syst Sci & Engn, Harbin 15001, Heilongjiang, Peoples R China.
EM guanfengxu@hrbeu.edu.cn
CR Akkaynak D, 2019, PROC CVPR IEEE, P1682, DOI 10.1109/CVPR.2019.00178
   Berman D, 2021, IEEE T PATTERN ANAL, V43, P2822, DOI 10.1109/TPAMI.2020.2977624
   Chen TY, 2022, IMAGE VISION COMPUT, V117, DOI 10.1016/j.imavis.2021.104340
   Ding Xueyan, 2021, Image Commun., V98, P1
   Drews P, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P825, DOI 10.1109/ICCVW.2013.113
   Fu ZQ, 2022, LECT NOTES COMPUT SC, V13678, P465, DOI 10.1007/978-3-031-19797-0_27
   Galdran A, 2015, J VIS COMMUN IMAGE R, V26, P132, DOI 10.1016/j.jvcir.2014.11.006
   Gao SB, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2919947
   Guo Chunle, 2023, Underwater ranker: Learn which is better and how to be better, V37, P702
   Guo YC, 2020, IEEE J OCEANIC ENG, V45, P862, DOI 10.1109/JOE.2019.2911447
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Hong DF, 2021, IEEE T GEOSCI REMOTE, V59, P5966, DOI 10.1109/TGRS.2020.3015157
   Hong DF, 2021, IEEE T GEOSCI REMOTE, V59, P4340, DOI 10.1109/TGRS.2020.3016820
   Hong DF, 2019, IEEE T IMAGE PROCESS, V28, P1923, DOI 10.1109/TIP.2018.2878958
   Hou GJ, 2020, IEEE ACCESS, V8, P122078, DOI 10.1109/ACCESS.2020.3006359
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang J, 2019, LECT NOTES COMPUT SC, V11133, P230, DOI 10.1007/978-3-030-11021-5_15
   Huang SR, 2023, PROC CVPR IEEE, P18145, DOI 10.1109/CVPR52729.2023.01740
   Ignatov A, 2017, IEEE I CONF COMP VIS, P3297, DOI 10.1109/ICCV.2017.355
   Islam MJ, 2020, IEEE ROBOT AUTOM LET, V5, P3227, DOI 10.1109/LRA.2020.2974710
   Kocak DM, 2008, MAR TECHNOL SOC J, V42, P52, DOI 10.4031/002533208786861209
   Lai YT, 2022, FRONT MAR SCI, V9, DOI 10.3389/fmars.2022.1047053
   Li CY, 2016, IEEE T IMAGE PROCESS, V26, P5664, DOI 10.1109/TIP.2016.2612882
   Li CY, 2020, IEEE T IMAGE PROCESS, V29, P4376, DOI 10.1109/TIP.2019.2955241
   Li FC, 2022, MATH PROBL ENG, V2022, DOI 10.1155/2022/8330985
   Li H., 2019, arXiv
   Li J, 2018, IEEE ROBOT AUTOM LET, V3, P387, DOI 10.1109/LRA.2017.2730363
   Li ZC, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3240195
   Li ZX, 2023, IMAGE VISION COMPUT, V140, DOI 10.1016/j.imavis.2023.104864
   Liu RS, 2020, IEEE T CIRC SYST VID, V30, P4861, DOI 10.1109/TCSVT.2019.2963772
   Liu X., 2020, IEEE Geosci. Remote Sens. Lett., V1
   LIU YC, 1995, IEEE T CONSUM ELECTR, V41, P460, DOI 10.1109/30.468045
   Mandal D., 2012, 2012 IEEE Conference on Technologies for Practical Robot Applications (TePRA), P145, DOI 10.1109/TePRA.2012.6215669
   Naik Ankita Rajaram, 2021, AAAI Conf. Artificial Intellig., V2
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   Qi Q, 2022, IEEE T CIRC SYST VID, V32, P1133, DOI 10.1109/TCSVT.2021.3074197
   Rahman Z, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL III, P1003, DOI 10.1109/ICIP.1996.560995
   Sohn K, 2015, ADV NEUR IN, V28
   Song W, 2018, LECT NOTES COMPUT SC, V11164, P678, DOI 10.1007/978-3-030-00776-8_62
   Tang H, 2022, PATTERN RECOGN, V130, DOI 10.1016/j.patcog.2022.108792
   Vaswani A, 2023, Arxiv, DOI arXiv:1706.03762
   Wang JH, 2019, IEEE ACCESS, V7, P145199, DOI 10.1109/ACCESS.2019.2945576
   Wang J, 2020, IEEE ACCESS, V8, P130719, DOI 10.1109/ACCESS.2020.3003351
   Wu JJ, 2022, SIGNAL PROCESS-IMAGE, V109, DOI 10.1016/j.image.2022.116855
   Wu X, 2023, IEEE T IMAGE PROCESS, V32, P364, DOI 10.1109/TIP.2022.3228497
   Wu X, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3124913
   Yan SL, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3310118
   Yang M, 2015, IEEE T IMAGE PROCESS, V24, P6062, DOI 10.1109/TIP.2015.2491020
   Zha ZC, 2023, IEEE T CIRC SYST VID, V33, P3947, DOI 10.1109/TCSVT.2023.3236636
   Zhang W., 2021, Image Commun., V90, P1
   Zhu QX, 2023, DISPLAYS, V79, DOI 10.1016/j.displa.2023.102468
   Zuiderveld K., 1994, Graphics gems, DOI 10.1016/b978-0-12-336156-1.50061-6
NR 52
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD AUG
PY 2024
VL 148
AR 105101
DI 10.1016/j.imavis.2024.105101
EA JUN 2024
PG 15
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA WA9E0
UT WOS:001252256600001
DA 2024-08-05
ER

PT J
AU Kang, M
   Ting, CM
   Ting, FF
   Phan, RCW
AF Kang, Ming
   Ting, Chee-Ming
   Ting, Fung Fung
   Phan, Raphael C. -W.
TI ASF-YOLO: A novel YOLO model with attentional scale sequence fusion for
   cell instance segmentation
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Medical image analysis; Small object segmentation; You only look once
   (YOLO); Sequence feature fusion; Attention mechanism
AB We propose a novel Attentional Scale Sequence Fusion based You Only Look Once (YOLO) framework (ASFYOLO) which combines spatial and scale features for accurate and fast cell instance segmentation. Built on the YOLO segmentation framework, we employ the Scale Sequence Feature Fusion (SSFF) module to enhance the multiscale information extraction capability of the network, and the Triple Feature Encoder (TFE) module to fuse feature maps of different scales to increase detailed information. We further introduce a Channel and Position Attention Mechanism (CPAM) to integrate both the SSFF and TFE modules, which focus on informative channels and spatial position-related small objects for improved detection and segmentation performance. Experimental validations on two cell datasets show remarkable segmentation accuracy and speed of the proposed ASF-YOLO model. It achieves a box mAP of 0.91, mask mAP of 0.887, and an inference speed of 47.3 FPS on the 2018 Data Science Bowl dataset, outperforming the state-of-the-art methods. The source code is available at https://github. com/mkang315/ASF-YOLO.
C1 [Kang, Ming; Ting, Chee-Ming; Ting, Fung Fung; Phan, Raphael C. -W.] Monash Univ, Sch Informat Technol, Malaysia Campus, Subang Jaya 47500, Malaysia.
C3 Monash University; Monash University Malaysia
RP Ting, CM (corresponding author), Monash Univ, Sch Informat Technol, Malaysia Campus, Subang Jaya 47500, Malaysia.
EM ting.cheeming@monash.edu
FU Monash University Malaysia; Ministry of Higher Education, Malaysia under
   Fundamental Research Grant Scheme [FRGS/1/2023/ICT02/MUSM/02/1]
FX This work was supported by the Monash University Malaysia and the
   Ministry of Higher Education, Malaysia under Fundamental Research Grant
   Scheme FRGS/1/2023/ICT02/MUSM/02/1.
CR Bai B., 2022, NEURIPS 2022 WEAKLY
   Bancher B., 2024, P MICCAI WORKSH COMP, P20
   Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Bodla N, 2017, IEEE I CONF COMP VIS, P5562, DOI 10.1109/ICCV.2017.593
   Bolya D, 2019, IEEE I CONF COMP VIS, P9156, DOI 10.1109/ICCV.2019.00925
   Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644
   Cao XH, 2023, IEEE ACCESS, V11, P111079, DOI 10.1109/ACCESS.2023.3322143
   CBI, 2008, Breast Cancer Cell
   Cheng ZM, 2020, IEEE ACCESS, V8, P158679, DOI 10.1109/ACCESS.2020.3020393
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Elfwing S, 2018, NEURAL NETWORKS, V107, P3, DOI 10.1016/j.neunet.2017.12.012
   Fujita S., 2020, P ACCV WORKSH, P58, DOI DOI 10.1007/978-3-030-69756-3_5
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Goodman A., 2018 Data Science Bowl
   Guo XY, 2024, COMPUT BIOL MED, V169, DOI 10.1016/j.compbiomed.2023.107879
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Hollandi R, 2022, TRENDS CELL BIOL, V32, P295, DOI 10.1016/j.tcb.2021.12.004
   Hou QB, 2021, PROC CVPR IEEE, P13708, DOI 10.1109/CVPR46437.2021.01350
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hua J, 2021, MATHEMATICS-BASEL, V9, DOI 10.3390/math9151766
   Jocher G., 2023, YOLO by Ultralytics
   Jocher G., 2022, YOLO by ultralytics
   Jung Hwejin, 2019, BMC Biomed Eng, V1, P24, DOI 10.1186/s42490-019-0026-8
   Konopczynski Tomasz, 2020, Artificial Intelligence and Soft Computing. 19th International Conference, ICAISC 2020. Proceedings. Lecture Notes in Artificial Intelligence Subseries of Lecture Notes in Computer Science (LNAI 12415), P626, DOI 10.1007/978-3-030-61401-0_58
   Lawal OM, 2023, PLOS ONE, V18, DOI 10.1371/journal.pone.0282297
   Liansheng Wang, 2020, Pattern Recognition Letters, V135, P244, DOI 10.1016/j.patrec.2020.04.008
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Lindeberg T., 1994, The Kluwer International Series in Engineering and Computer Science
   Liu HL, 2023, MACH INTELL RES, V20, P716, DOI 10.1007/s11633-022-1379-3
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mahbod A, 2022, FRONT MED-LAUSANNE, V9, DOI 10.3389/fmed.2022.978146
   Mohamed E, 2021, Arxiv, DOI [arXiv:2102.06777, DOI 10.48550/ARXIV.2102.06777]
   Neubeck A, 2006, INT C PATT RECOG, P850, DOI 10.1109/icpr.2006.479
   Olivier R, 2012, INT J ADV COMPUT SC, V3, P25
   OpenMMLab, 2022, about Us
   Park HJ, 2023, SENSORS-BASEL, V23, DOI 10.3390/s23094432
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shang ZN, 2022, BIOSYST ENG, V215, P156, DOI 10.1016/j.biosystemseng.2022.01.005
   Johnson JW, 2018, Arxiv, DOI arXiv:1805.00500
   Wang CY, 2023, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR52729.2023.00721
   Wang CY, 2020, IEEE COMPUT SOC CONF, P1571, DOI 10.1109/CVPRW50498.2020.00203
   Wang J, 2023, IET IMAGE PROCESS, V17, P2284, DOI 10.1049/ipr2.12792
   Wang X., 2020, INT C NEURAL INF PRO, V33, P17721
   Wang Yang, 2022, 2022 IEEE 22nd International Conference on Communication Technology (ICCT), P1646, DOI 10.1109/ICCT56141.2022.10073387
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu PS, 2022, IMAGE VISION COMPUT, V117, DOI 10.1016/j.imavis.2021.104341
   Xinlong Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P649, DOI 10.1007/978-3-030-58523-5_38
   Yang W., 2024, PREPRINT, DOI DOI 10.21203/RS.3.RS-3199595/V1
   Yasir M, 2023, FRONT MAR SCI, V10, DOI 10.3389/fmars.2023.1113669
   Yi JR, 2019, MED IMAGE ANAL, V55, P228, DOI 10.1016/j.media.2019.05.004
   Zhang YF, 2022, NEUROCOMPUTING, V506, P146, DOI 10.1016/j.neucom.2022.07.042
   Zheng ZH, 2020, AAAI CONF ARTIF INTE, V34, P12993
NR 55
TC 2
Z9 2
U1 29
U2 29
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUL
PY 2024
VL 147
AR 105057
DI 10.1016/j.imavis.2024.105057
EA MAY 2024
PG 9
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA TI3K3
UT WOS:001240594700001
OA hybrid, Green Submitted
DA 2024-08-05
ER

PT J
AU Lukic, T
   Balázs, P
AF Lukic, Tibor
   Balazs, Peter
TI Moment preserving tomographic image reconstruction model
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Tomography; Image moments; Shape descriptors; Gradient based
   optimization
ID DISCRETE TOMOGRAPHY; FAST COMPUTATION; OPTIMIZATION; ORIENTATION;
   INVARIANT; ALGORITHM
AB Shape descriptors provide valuable prior information in many tomographic image reconstruction methods. Such descriptors include, among others, centroid, circularity, orientation, and elongation. Shape descriptor measures are often analytically expressed as a composition of certain geometric moments. Building upon this fact, this paper suggests preserving the values of a specific geometric moment in the reconstruction process, instead of preserving entire descriptors, as it has been suggested so far. Reconstructions from two natural projection directions (vertical and horizontal) are considered with special attention. The provided theoretical analysis demonstrates that preserving the value of a specific geometric moment, provided as prior information for the reconstruction process, simultaneously ensures the preservation of the true measures of all four abovementioned descriptors. Based on this result, a novel regularized energy minimization reconstruction model is proposed. The minimization task of the new model is solved using gradient-based optimization algorithm. Performance evaluation of the proposed method is supported by experimental results obtained through comparisons with other well-known reconstruction methods.
C1 [Lukic, Tibor] Univ Novi Sad, Fac Tech Sci, Novi Sad, Serbia.
   [Balazs, Peter] Univ Szeged, Dept Image Proc & Comp Graph, H-6720 Szeged, Hungary.
C3 University of Novi Sad; Szeged University
RP Lukic, T (corresponding author), Univ Novi Sad, Fac Tech Sci, Novi Sad, Serbia.
EM tibor@uns.ac.rs; pbalazs@inf.u-szeged.hu
FU Ministry of Science, Technological Development and Innovation [451-
   03-65/2024-03/200156]; Faculty of Technical Sciences, University of Novi
   Sad [01-3394/1]; National Research, Development and Innovation Fund
FX The research work of T. Luki c has been supported by the Ministry of
   Science, Technological Development and Innovation (Contract No. 451-
   03-65/2024-03/200156) and the Faculty of Technical Sciences, University
   of Novi Sad through project "Scientific and Artistic Research Work of
   Researchers in Teaching and Associate Positions at the Faculty of
   Technical Sciences, University of Novi Sad" (No. 01-3394/1). He also
   acknowledges to the Domus Hungarica Scientiarum et Artium program of the
   Hungarian Academy of Sciences. The work of Peter Balazs was supported by
   project no. TKP2021-NVA-09 that has been implemented with the support
   provided by the Ministry of Culture and Innovation of Hungary from the
   National Research, Development and Innovation Fund, financed under the
   TKP2021-NVA funding scheme.
CR Batenburg KJ, 2007, IEEE IMAGE PROC, P1829
   Batenburg KJ, 2011, IEEE T IMAGE PROCESS, V20, P2542, DOI 10.1109/TIP.2011.2131661
   Birgin EG, 2001, ACM T MATH SOFTWARE, V27, P340, DOI 10.1145/502800.502803
   Carmignato S, 2018, Industrial X-Ray Computed Tomography
   Daoui A, 2022, CIRC SYST SIGNAL PR, V41, P166, DOI 10.1007/s00034-021-01764-z
   Daoui A, 2020, CIRC SYST SIGNAL PR, V39, P4552, DOI 10.1007/s00034-020-01384-z
   El Ogri O, 2021, MULTIDIM SYST SIGN P, V32, P431, DOI 10.1007/s11045-020-00745-w
   El Ogri O, 2020, MULTIMED TOOLS APPL, V79, P23261, DOI 10.1007/s11042-020-09084-1
   El Ogri O, 2019, PROCEDIA COMPUT SCI, V148, P428, DOI 10.1016/j.procs.2019.01.055
   El Ogri O, 2021, SIGNAL PROCESS-IMAGE, V98, DOI 10.1016/j.image.2021.116410
   Grob D, 2019, EUR RADIOL, V29, P1408, DOI 10.1007/s00330-018-5740-4
   Herman GT, 2009, ADV PATTERN RECOGNIT, P1, DOI 10.1007/978-1-84628-723-7
   HU M, 1962, IRE T INFORM THEOR, V8, P179, DOI 10.1109/tit.1962.1057692
   Jahid T, 2017, PROCEEDINGS OF 2017 INTERNATIONAL CONFERENCE ON ELECTRICAL AND INFORMATION TECHNOLOGIES (ICEIT 2017)
   Jin LH, 2020, J MATH IMAGING VIS, V62, P505, DOI 10.1007/s10851-019-00942-8
   Karmouni H, 2020, MULTIMED TOOLS APPL, V79, P29121, DOI 10.1007/s11042-020-09351-1
   Karmouni H, 2021, CIRC SYST SIGNAL PR, V40, P3782, DOI 10.1007/s00034-020-01646-w
   Karmouni H, 2019, MULTIMED TOOLS APPL, V78, P31245, DOI 10.1007/s11042-019-07961-y
   Karmouni H, 2017, 2017 INTELLIGENT SYSTEMS AND COMPUTER VISION (ISCV)
   Klette R, 2006, LECT NOTES COMPUT SC, V4245, P367
   Lindblad J, 2007, PROCEEDINGS OF THE 5TH INTERNATIONAL SYMPOSIUM ON IMAGE AND SIGNAL PROCESSING AND ANALYSIS, P373
   Liu XL, 2015, VISUAL COMPUT, V31, P1431, DOI 10.1007/s00371-014-1024-4
   Lukic T, 2008, LECT NOTES COMPUT SC, V5096, P476, DOI 10.1007/978-3-540-69321-5_48
   Lukic T, 2022, VISUAL COMPUT, V38, P695, DOI 10.1007/s00371-020-02044-8
   Lukic T, 2020, PHYS SCRIPTA, V95, DOI 10.1088/1402-4896/abb633
   Lukic T, 2019, PHYS SCRIPTA, V94, DOI 10.1088/1402-4896/aafbcb
   Lukic T, 2016, PATTERN RECOGN LETT, V79, P18, DOI 10.1016/j.patrec.2016.04.010
   Lukic T, 2014, PATTERN RECOGN LETT, V49, P11, DOI 10.1016/j.patrec.2014.05.014
   Lukic T, 2014, INVERSE PROBL, V30, DOI 10.1088/0266-5611/30/9/095007
   Lukic T, 2011, INVERSE PROBL, V27, DOI 10.1088/0266-5611/27/8/085010
   Lukic T, 2010, STUD COMPUT INTELL, V313, P263
   Prause GPM, 1996, IEEE T MED IMAGING, V15, P532, DOI 10.1109/42.511756
   Qi SR, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3479428
   Sayyouri M, 2015, CIRC SYST SIGNAL PR, V34, P875, DOI 10.1007/s00034-014-9881-7
   Scarfe W.C., 2018, Maxillofacial Cone Beam Computed Tomography: Principles, Techniques and Clinical Applications
   Schüle T, 2005, DISCRETE APPL MATH, V151, P229, DOI 10.1016/j.dam.2005.02.028
   Sonka M., 2014, Image processing, analysis and machine vision
   Steingruber IE, 2003, AM J ROENTGENOL, V181, P99, DOI 10.2214/ajr.181.1.1810099
   Tahiri MA, 2022, INT C INT SYST COMP, P1, DOI [10.1109/ISCV54655.2022.9806106, DOI 10.1109/ISCV54655.2022.9806106]
   Tao PD, 1998, SIAM J OPTIMIZ, V8, P476, DOI 10.1137/S1052623494274313
   Yamni M, 2021, CIRC SYST SIGNAL PR, V40, P6193, DOI 10.1007/s00034-021-01763-0
   Yamni M, 2021, MULTIMED TOOLS APPL, V80, P26683, DOI 10.1007/s11042-020-10311-y
   Yamni M, 2019, PROCEDIA COMPUT SCI, V148, P418, DOI 10.1016/j.procs.2019.01.054
   Zunic J, 2006, PATTERN RECOGN, V39, P856, DOI 10.1016/j.patcog.2005.11.010
   Zunic J., 2012, Zbornik Radova, V23, P5
   Zunic J, 2017, PATTERN RECOGN, V69, P141, DOI 10.1016/j.patcog.2017.04.009
   Zunic J, 2010, PATTERN RECOGN, V43, P47, DOI 10.1016/j.patcog.2009.06.017
NR 47
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUN
PY 2024
VL 146
AR 105036
DI 10.1016/j.imavis.2024.105036
EA APR 2024
PG 8
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA SQ3K3
UT WOS:001235876000001
DA 2024-08-05
ER

PT J
AU Bisogni, C
   Cascone, L
   Nappi, M
   Pero, C
AF Bisogni, Carmen
   Cascone, Lucia
   Nappi, Michele
   Pero, Chiara
TI POSER: POsed vs Spontaneous Emotion Recognition using fractal encoding
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Emotion recognition; Face recognition; Partitioned iterated function
   systems; Fractal encoding; Machine learning; Spontaneous emotions
ID FACIAL EXPRESSION RECOGNITION; FACE; NETWORK
AB Emotion recognition from facial expressions is a fundamental human ability that can be harnessed and transferred to machines. The ability to differentiate between spontaneous and posed emotions holds significant importance in various domains, including behavioral biometrics, forensics, and security. This paper introduces a novel method, called POsed vs Spontaneous Emotion Recognition (POSER), which leverages a modified version of the Partitioned Iterated Functions System (PIFS) to obtain a Fractal Encoding. This encoding is used for the first time as facial features to train a machine learning approach for the classification of emotions as either spontaneous or posed. Furthermore, by adapting the original architecture, we demonstrate the effectiveness of these features in distinguishing seven different emotions in controlled as well as wild environments, within a framework referred to as POSER-EMO. Experimental results are presented on the SPOS and DISFA+ datasets for the first classification problem, where POSER outperforms the state of the art, and on the CK+ and SFEW datasets for the second classification problem.
C1 [Bisogni, Carmen; Cascone, Lucia; Nappi, Michele; Pero, Chiara] Univ Salerno, Via Giovanni Paolo 2132, I-84084 Salerno, Fisciano, Italy.
C3 University of Salerno
RP Bisogni, C (corresponding author), Univ Salerno, Via Giovanni Paolo 2132, I-84084 Salerno, Fisciano, Italy.
EM cbisogni@unisa.it
FU Information Disorder Awareness (IDA); European Union-NextGenerationEU
   [PE00000014]
FX This work was partially supported by the project Information Disorder
   Awareness (IDA) included in the Spoke 2-Misinformation and Fakes of the
   Research and Innovation Program PE00000014, "SEcurity and RIghts in the
   CyberSpace (SERICS) ", under the National Recovery and Resilience Plan,
   Mission 4 "Education and Research"-Component 2 "From Research to
   Enterprise"-Investment 1.3, funded by the European
   Union-NextGenerationEU.
CR Abdullahi SM, 2020, IEEE T INF FOREN SEC, V15, P2587, DOI 10.1109/TIFS.2020.2971142
   Al-Saidi NMG, 2019, INT J INNOV COMPUT I, V15, P1441, DOI 10.24507/ijicic.15.04.1441
   Bajahzar A, 2019, INT J ADV COMPUT SC, V10, P103
   Ben Tanfous A, 2020, IEEE T PATTERN ANAL, V42, P2594, DOI 10.1109/TPAMI.2019.2932979
   Bilotti U, 2023, LECT NOTES COMPUT SC, V14233, P196, DOI 10.1007/978-3-031-43148-7_17
   Bisogni Carmen, 2022, IEEE Transactions on Biometrics, Behavior, and Identity Science, V4, P173, DOI 10.1109/TBIOM.2021.3122307
   Bisogni C., 2023, 2023 24 INT C CONTR, P281, DOI [10.1109/CSCS59211.2023.00051, DOI 10.1109/CSCS59211.2023.00051]
   Bisogni C, 2021, INT C PATT RECOG, P1725, DOI 10.1109/ICPR48806.2021.9413227
   Bisogni C, 2021, IEEE T IMAGE PROCESS, V30, P3192, DOI 10.1109/TIP.2021.3059409
   Boukhriss H., 2021, springer international publishing, Cham, P41, DOI [10.1007/978-3-030-81982-8_3, DOI 10.1007/978-3-030-81982-8_3]
   Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020
   Conson M, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00382
   Dhall A., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P2106, DOI 10.1109/ICCVW.2011.6130508
   Distasi R., 2005, A range/domain approximation error-based approach for fractal image compression
   King DE, 2015, Arxiv, DOI arXiv:1502.00046
   Fu YJ, 2020, IEEE T IMAGE PROCESS, V29, P6535, DOI 10.1109/TIP.2020.2991510
   Gan CQ, 2022, IMAGE VISION COMPUT, V117, DOI 10.1016/j.imavis.2021.104342
   Jacquin A, 1989, A Fractal of Iterated Markov Operators with Applications to Digital Image Coding
   Jung H, 2015, IEEE I CONF COMP VIS, P2983, DOI 10.1109/ICCV.2015.341
   Kacem A, 2017, IEEE I CONF COMP VIS, P3199, DOI 10.1109/ICCV.2017.345
   Kar NB, 2022, IMAGE VISION COMPUT, V123, DOI 10.1016/j.imavis.2022.104445
   Karnati M, 2022, IEEE T AFFECT COMPUT, V13, P2058, DOI 10.1109/TAFFC.2022.3208309
   Kouzani AZ, 1997, IEEE SYS MAN CYBERN, P1609, DOI 10.1109/ICSMC.1997.638231
   Li S, 2019, IEEE T IMAGE PROCESS, V28, P356, DOI 10.1109/TIP.2018.2868382
   Li Y, 2019, IEEE T IMAGE PROCESS, V28, P2439, DOI 10.1109/TIP.2018.2886767
   Lucey Patrick, 2010, Proceedings of the 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), DOI 10.1109/CVPRW.2010.5543262
   Maji S, 2009, PROC CVPR IEEE, P1038, DOI 10.1109/CVPRW.2009.5206693
   Mavadati M, 2016, IEEE COMPUT SOC CONF, P1452, DOI 10.1109/CVPRW.2016.182
   Mavadati SM, 2013, IEEE T AFFECT COMPUT, V4, P151, DOI 10.1109/T-AFFC.2013.4
   Ozturk A., 2011, Inform. Syst. Sci., V36, P159
   Park S, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20041199
   Pfister T, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS)
   Poux D, 2022, IEEE T IMAGE PROCESS, V31, P446, DOI 10.1109/TIP.2021.3129120
   Racoviteanu A, 2019, 2019 INTERNATIONAL SYMPOSIUM ON SIGNALS, CIRCUITS AND SYSTEMS (ISSCS 2019), DOI 10.1109/isscs.2019.8801755
   Sowden S, 2021, EMOTION, V21, P1041, DOI 10.1037/emo0000835
   Stanley JT, 2008, PSYCHOL AGING, V23, P24, DOI 10.1037/0882-7974.23.1.24
   Sun B, 2022, IEEE T AFFECT COMPUT, V13, P1037, DOI 10.1109/TAFFC.2020.2986962
   Tang Y, 2021, IEEE T IMAGE PROCESS, V30, P444, DOI 10.1109/TIP.2020.3037467
   Tang ZJ, 2017, SIGNAL IMAGE VIDEO P, V11, P1221, DOI 10.1007/s11760-017-1078-7
   Tong XY, 2022, IMAGE VISION COMPUT, V120, DOI 10.1016/j.imavis.2022.104399
   Umer S, 2022, J AMB INTEL HUM COMP, V13, P721, DOI 10.1007/s12652-020-02845-8
   Wang K, 2020, IEEE T IMAGE PROCESS, V29, P4057, DOI 10.1109/TIP.2019.2956143
   Wang SF, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3391290
   Wang SF, 2020, IEEE T PATTERN ANAL, V42, P2082, DOI 10.1109/TPAMI.2019.2911937
   Wang ZN, 2021, PATTERN RECOGN, V112, DOI 10.1016/j.patcog.2020.107694
   Wegrzyn M, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0177239
   Xie SY, 2019, PATTERN RECOGN, V92, P177, DOI 10.1016/j.patcog.2019.03.019
   Xie WC, 2021, PATTERN RECOGN, V111, DOI 10.1016/j.patcog.2020.107701
   Xie WC, 2019, PATTERN RECOGN, V96, DOI 10.1016/j.patcog.2019.106966
   Yang JJ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P469, DOI 10.1145/3123266.3123350
   Zhang FF, 2020, IEEE T IMAGE PROCESS, V29, P6574, DOI 10.1109/TIP.2020.2991549
   Zhang FF, 2020, IEEE T IMAGE PROCESS, V29, P4445, DOI 10.1109/TIP.2020.2972114
NR 52
TC 0
Z9 0
U1 3
U2 3
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD APR
PY 2024
VL 144
AR 104952
DI 10.1016/j.imavis.2024.104952
EA FEB 2024
PG 10
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA OF2E4
UT WOS:001205777500001
OA hybrid
DA 2024-08-05
ER

PT J
AU Liu, SS
   Zhao, DX
   Sun, ZB
   Chen, YK
AF Liu, Shuaishi
   Zhao, Dongxu
   Sun, Zhongbo
   Chen, Yuekun
TI BPMB: BayesCNNs with perturbed multi-branch structure for robust facial
   expression recognition
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Facial expression recognition; Uncertainty; Perturbed multi-branch
   structure; Bayesian convolutional neural network
ID REPRESENTATION; ATTENTION; NETWORK
AB Wild Facial Expression Recognition (FER) task has been a long-standing challenge due to the various forms of uncertainty exist in expression data. When expression data is fed into a convolutional neural network (CNN), the model's estimated parameters also become uncertain. This uncertainty gives rise to concerns regarding the reliability of the recognition results. To quantify these uncertainties and achieve robust performance in the presence of noisy data, this paper introduces a novel model for Wild Facial Expression Recognition: the Bayesian Convolutional Neural Network with Perturbed Multi-Branch Structure (BPMB). This model aims to address uncertainty issues, enabling the network's decisions to become more deterministic with increasing training accuracy. Specifically, BPMB incorporates variational inference (VI) to introduce a probability distribution for the weights. A variational approximation to the true posterior is derived using Bayes by Backprop, involving two convolution operations: one for classification and another for quantifying uncertainty. Furthermore, an exploration is conducted into a lightweight multi-branch structure that leverages Dropout as a random generator to introduce perturbations during the training process, enhancing the model's robustness while extracting deep features. Extensive experiments validate the superiority of the proposed BPMB algorithm over the majority of existing mainstream algorithms on three widely utilized wild datasets.
C1 [Liu, Shuaishi; Zhao, Dongxu; Sun, Zhongbo; Chen, Yuekun] Changchun Univ Technol, Dept Control Engn, Changchun 130012, Peoples R China.
C3 Changchun University of Technology
RP Liu, SS (corresponding author), Changchun Univ Technol, Dept Control Engn, Changchun 130012, Peoples R China.
EM liushuaishi@ccut.edu.cn
FU Project of National Natural Science Foundation of China [62106023]
FX * This research was funded by the Project of National Natural Science
   Foundation of China under Grant No. 62106023.
CR Blundell C, 2015, PR MACH LEARN RES, V37, P1613
   Chang J, 2020, PROC CVPR IEEE, P5709, DOI 10.1109/CVPR42600.2020.00575
   Chen Y., 2019, 2019 IEEE VIS COMM I, P1
   Chen YP, 2017, ADV NEUR IN, V30
   Farzaneh AH, 2021, IEEE WINT CONF APPL, P2401, DOI 10.1109/WACV48630.2021.00245
   Gera D, 2021, PATTERN RECOGN LETT, V145, P58, DOI 10.1016/j.patrec.2021.01.029
   Goodfellow IJ, 2015, NEURAL NETWORKS, V64, P59, DOI 10.1016/j.neunet.2014.09.005
   Hasani B, 2022, IEEE T AFFECT COMPUT, V13, P1023, DOI 10.1109/TAFFC.2020.2986440
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Houthooft R., 2016, arXiv, DOI DOI 10.48550/ARXIV.1605.09674
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jabbooree AI, 2023, IMAGE VISION COMPUT, V134, DOI 10.1016/j.imavis.2023.104677
   Kendall Alex, 2017, C NEUR INF PROC SYST
   Khan S, 2019, PROC CVPR IEEE, P103, DOI 10.1109/CVPR.2019.00019
   Kingma D.P., 2015, C NEUR INF PROC SYST
   Li HY, 2021, IEEE T IMAGE PROCESS, V30, P2016, DOI 10.1109/TIP.2021.3049955
   Li S, 2019, IEEE T IMAGE PROCESS, V28, P356, DOI 10.1109/TIP.2018.2868382
   Lipton Z, 2018, AAAI CONF ARTIF INTE, P5237
   Ma FY, 2023, IEEE T AFFECT COMPUT, V14, P1236, DOI 10.1109/TAFFC.2021.3122146
   MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI 10.1162/neco.1992.4.3.448
   Fernandez PDM, 2019, IEEE COMPUT SOC CONF, P837, DOI 10.1109/CVPRW.2019.00112
   Minaee S., 2019, Sensors (Basel, Switzerland), V21
   Mirikitani DT, 2010, IEEE T NEURAL NETWOR, V21, P262, DOI 10.1109/TNN.2009.2036174
   Mollahosseini A, 2019, IEEE T AFFECT COMPUT, V10, P18, DOI 10.1109/TAFFC.2017.2740923
   Nguyen A., 2022, AAAI C ART INT
   Park J, 2018, Arxiv, DOI [arXiv:1807.06514, 10.48550/arXiv.1807.06514]
   Psaroudakis A, 2022, IEEE COMPUT SOC CONF, P2366, DOI 10.1109/CVPRW56347.2022.00264
   Pu T, 2021, IEEE INT CONF ROBOT, P11154, DOI 10.1109/ICRA48506.2021.9561252
   Shabbir N, 2023, IMAGE VISION COMPUT, V137, DOI 10.1016/j.imavis.2023.104770
   Shao J, 2019, NEUROCOMPUTING, V355, P82, DOI 10.1016/j.neucom.2019.05.005
   She JH, 2021, PROC CVPR IEEE, P6244, DOI 10.1109/CVPR46437.2021.00618
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vo TH, 2020, IEEE ACCESS, V8, P131988, DOI 10.1109/ACCESS.2020.3010018
   Vulpe-Grigorasi A, 2021, INT SYMP ADV TOP, DOI 10.1109/ATEE52255.2021.9425073
   Wang K, 2020, PROC CVPR IEEE, P6896, DOI 10.1109/CVPR42600.2020.00693
   Wang K, 2020, IEEE T IMAGE PROCESS, V29, P4057, DOI 10.1109/TIP.2019.2956143
   Xu R, 2023, IMAGE VISION COMPUT, V139, DOI 10.1016/j.imavis.2023.104824
   Zafar U, 2019, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-019-0406-y
   Zeng D, 2022, PROC CVPR IEEE, P20259, DOI 10.1109/CVPR52688.2022.01965
   Zhang Y., 2021, C NEUR INF PROC SYST
   Zhang YH, 2021, ADV NEUR IN, V34
   Zhao ZQ, 2021, AAAI CONF ARTIF INTE, V35, P3510
   Zhao ZQ, 2021, IEEE T IMAGE PROCESS, V30, P6544, DOI 10.1109/TIP.2021.3093397
NR 44
TC 1
Z9 1
U1 4
U2 4
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD MAR
PY 2024
VL 143
AR 104960
DI 10.1016/j.imavis.2024.104960
EA FEB 2024
PG 11
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA NF5N9
UT WOS:001199052600001
DA 2024-08-05
ER

PT J
AU Jing, LH
   Wang, B
AF Jing, Lianghu
   Wang, Bo
TI EMNet: Edge-guided multi-level network for salient object detection in
   low-light images
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Low -light images; Salient object detection; Multi -layer feature
   fusion; Edge feature highlight
AB Salient object detection (SOD) has achieved remarkable performance in well -lit scenes. However, when generalized to low -light scenes, the performance of SOD shows significant decrease owing to more challenging conditions such as weak brightness, low contrast, and poor signal-to-noise ratio. To address this issue, we propose a novel edge -guided and multi -level network (EMNet) for SOD in low light images, which learns robust multiscale region features by optimizing the boundaries of salient objects and employing a multi -stage cascaded strategy. To be more specific, the proposed Edge Feature Highlight (EFH) module can establish mapping relationships at different scales and fuse the outputs obtained from pairs of different branches for extracting accurate boundary information. Secondly, Multi -layer Feature Fusion (MFF) module is proposed for combining multi -scale deep features with salient edge cues, using stepwise fusion for effective integration of deep features. Finally, we employ a coarse -to -fine way for iterative prediction to generate high -quality saliency maps. We conducted comprehensive experiments on LLI dataset, and the results demonstrate that the proposed method achieves the best performance in terms of five evaluation metrics, where an average improvement of max F beta, omega F beta, Em, Sm, and MAE outperforms state-of-the-art method by 7.20%, 12.06%, 4.62%, 5.00%, and 36.34%, respectively.
C1 [Jing, Lianghu; Wang, Bo] Ningxia Univ, Sch Elect & Elect Engn, Yinchuan 750021, Peoples R China.
C3 Ningxia University
RP Wang, B (corresponding author), Ningxia Univ, Sch Elect & Elect Engn, Yinchuan 750021, Peoples R China.
EM tjuwb@nxu.edu.cn
FU Natural Science Foundation of Ningxia Province [2023AAC03023]
FX This work was supported by the Natural Science Foundation of Ningxia
   Province (No. 2023AAC03023) .
CR Abdullahi S. B., 2023, Intell. Syst. Appl., V19
   Abdullahi SB, 2023, BRAIN SCI, V13, DOI 10.3390/brainsci13040555
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Aggarwal A.K., International journal of engineering sciences & research technology a hybrid approach to gps improvement in urban canyons
   Aggarwal A.K., 2023, WSEAS T BIOL BIOMEDI
   Banitalebi-Dehkordi A., 2023, IEEE Access
   Borji A., 2012, 2012 IEEE COMPUTER S, P23
   Cheng MM, 2021, INT J COMPUT VISION, V129, P2622, DOI [10.1007/s11263-021-01490-8, 10.1109/ICCV.2017.487]
   Cheng MM, 2017, J COMPUT SCI TECH-CH, V32, P110, DOI 10.1007/s11390-017-1681-7
   Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401
   Deng ZJ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P684
   Enze Xie, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P696, DOI 10.1007/978-3-030-58601-0_41
   Fan DP, 2018, Arxiv, DOI arXiv:1805.10421
   Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754
   Fang YM, 2023, PATTERN RECOGN, V134, DOI 10.1016/j.patcog.2022.109099
   He JF, 2012, PROC CVPR IEEE, P3005, DOI 10.1109/CVPR.2012.6248030
   He SF, 2015, INT J COMPUT VISION, V115, P330, DOI 10.1007/s11263-015-0822-0
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Jun Wei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13022, DOI 10.1109/CVPR42600.2020.01304
   Yun YK, 2022, Arxiv, DOI arXiv:2205.11283
   Ke YY, 2022, IEEE WINT CONF APPL, P1360, DOI 10.1109/WACV51458.2022.00143
   Khosravian A, 2023, IET IMAGE PROCESS, V17, P1253, DOI 10.1049/ipr2.12710
   Khosravian A, 2022, P I MECH ENG D-J AUT, V236, P1849, DOI 10.1177/09544070211042961
   Kingma D. P., 2014, arXiv
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Liu J, 2022, DIGIT SIGNAL PROCESS, V126, DOI 10.1016/j.dsp.2022.103425
   Liu Y, 2021, IEEE T IMAGE PROCESS, V30, P3804, DOI 10.1109/TIP.2021.3065239
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Ma Yu-Fei, 2002, P 10 ACM INT C MULT, P533
   Maini Surita, 2018, Int. J. Innov. Eng. Technol, V10, P199
   Margolin R, 2014, PROC CVPR IEEE, P248, DOI 10.1109/CVPR.2014.39
   Movahedi V., 2010, 2010 IEEE COMP SOC C, P49
   Mu N., 2017, P 9 INT C MACHINE LE, P314
   Mu N, 2018, LECT NOTES COMPUT SC, V11166, P35, DOI 10.1007/978-3-030-00764-5_4
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Ren SC, 2022, PROC CVPR IEEE, P10843, DOI 10.1109/CVPR52688.2022.01058
   Sun YJ, 2022, Arxiv, DOI arXiv:2207.00794
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404
   Wang WG, 2022, IEEE T PATTERN ANAL, V44, P3239, DOI 10.1109/TPAMI.2021.3051099
   Wei J, 2020, AAAI CONF ARTIF INTE, V34, P12321
   Wu Z, 2019, IEEE I CONF COMP VIS, P7263, DOI 10.1109/ICCV.2019.00736
   Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403
   Xiao J, 2023, IEEE Access
   Xu X, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3414839
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang B, 2019, ADV NEUR IN, V32
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yao ZJ, 2023, EXPERT SYST APPL, V213, DOI 10.1016/j.eswa.2022.118973
   Yu FS, 2016, Arxiv, DOI arXiv:1511.07122
   Yue HH, 2022, KNOWL-BASED SYST, V257, DOI 10.1016/j.knosys.2022.109938
   Zhang PP, 2021, IEEE T IMAGE PROCESS, V30, P3204, DOI 10.1109/TIP.2020.3045624
   Zhang Yu, 2022, 2022 IEEE International Conference on Systems, Man, and Cybernetics (SMC), P3119, DOI 10.1109/SMC53654.2022.9945579
   Zhang ZA, 2023, J VIS COMMUN IMAGE R, V95, DOI 10.1016/j.jvcir.2023.103862
   Zhao X., 2020, PROC 16 EUR C COMPU, P35, DOI 10.1007/ 978-3-030-58536-5_3
   Zhou H., 2020, P IEEE CVF C COMP VI, P9138, DOI 10.1109/CVPR42600.2020.00916
   Zhu JY, 2015, IEEE T PATTERN ANAL, V37, P862, DOI 10.1109/TPAMI.2014.2353617
   Zhu L, 2023, PROC CVPR IEEE, P10323, DOI 10.1109/CVPR52729.2023.00995
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
NR 63
TC 0
Z9 0
U1 2
U2 2
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD MAR
PY 2024
VL 143
AR 104933
DI 10.1016/j.imavis.2024.104933
EA FEB 2024
PG 11
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA NR3B5
UT WOS:001202130200001
DA 2024-08-05
ER

PT J
AU Guo, JT
   Du, HS
   Hao, XX
   Zhang, MH
AF Guo, Jiangtao
   Du, Haishun
   Hao, Xinxin
   Zhang, Minghao
TI IGIE-net: Cross-modality person re-identification via intermediate
   modality image generation and discriminative information enhancement
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Cross-modality person re-identification; Intermediate modality image
   generation; Discriminative information enhancement; Modality-specific
   information; Modality-shared information
ID GAN
AB Given an RGB image (or an IR image) of a pedestrian, the task of cross-modality person re-identification aims to retrieve images of a specific pedestrian from an IR (or RGB) gallery. However, the huge modality discrepancy between RGB and IR images significantly affects the performance of cross-modality person re-identification. For the purpose of reducing the impact of modality discrepancy and extracting more discriminative pedestrian features, a new cross-modality person re-identification network is proposed by us, which is based on intermediate modality image generation and discriminative information enhancement (IGIE-Net). Specifically, we design an intermediate modality image generation module (IMIGM) to effectively weaken the impact of modality discrepancy, which first extracts the modality-specific and modality-shared information contained in RGB and IR images separately, and then generates intermediate RGB and intermediate IR images by adaptively fusing original RGB and original IR images with their corresponding modality-specific and modality-shared information separately. In addition, we also design a discriminative information enhancement module (DIEM) to obtain more discriminative pedestrian representations by enhancing the discriminative information contained in the deep features of pedestrians. Extensive experiments on the publicly available SYSU-MM01 and RegDB datasets indicate that the performance of IGIE-Net reaches the current advanced level.
C1 [Guo, Jiangtao; Du, Haishun; Hao, Xinxin; Zhang, Minghao] Henan Univ, Sch Artificial Intelligence, Zhengzhou 450046, Peoples R China.
   [Guo, Jiangtao; Du, Haishun] Int Joint Lab Cooperat Vehicular Networks Henan, Zhengzhou 450046, Peoples R China.
C3 Henan University
RP Du, HS (corresponding author), Henan Univ, Sch Artificial Intelligence, Zhengzhou 450046, Peoples R China.; Du, HS (corresponding author), Int Joint Lab Cooperat Vehicular Networks Henan, Zhengzhou 450046, Peoples R China.
EM jddhs@vip.henu.edu.cn
FU Science and Technology Development Plan Project of Henan Province, China
   [222102110135]; Natural Science Foundation of Henan Province, China
   [202300410093]
FX <BOLD>Acknowledgements</BOLD> This work is supported in part by the
   Science and Technology Development Plan Project of Henan Province, China
   (No. 222102110135) and the Natural Science Foundation of Henan Province,
   China (No. 202300410093) .
CR Bai S, 2019, PROC CVPR IEEE, P740, DOI 10.1109/CVPR.2019.00083
   Basaran E, 2020, SIGNAL PROCESS-IMAGE, V87, DOI 10.1016/j.image.2020.115933
   Chen CQ, 2022, IEEE T IMAGE PROCESS, V31, P2352, DOI 10.1109/TIP.2022.3141868
   Chen YHS, 2021, PROC CVPR IEEE, P587, DOI 10.1109/CVPR46437.2021.00065
   Chen YY, 2022, IMAGE VISION COMPUT, V123, DOI 10.1016/j.imavis.2022.104462
   Dai PY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P677
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fan X, 2020, Arxiv, DOI [arXiv:2003.00213, DOI 10.48550/ARXIV.2003.00213]
   Feng ZX, 2020, IEEE T IMAGE PROCESS, V29, P579, DOI 10.1109/TIP.2019.2928126
   Fu CY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11803, DOI 10.1109/ICCV48922.2021.01161
   Hao X, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16383, DOI 10.1109/ICCV48922.2021.01609
   Hao Y, 2019, AAAI CONF ARTIF INTE, P8385
   Huang NC, 2022, PATTERN RECOGN, V128, DOI 10.1016/j.patcog.2022.108653
   Jia MX, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1026
   Kalayeh MM, 2018, PROC CVPR IEEE, P1062, DOI 10.1109/CVPR.2018.00117
   Kuan Zhu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P346, DOI 10.1007/978-3-030-58580-8_21
   Li DG, 2020, AAAI CONF ARTIF INTE, V34, P4610
   Li KF, 2022, KNOWL-BASED SYST, V252, DOI 10.1016/j.knosys.2022.109337
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Li YJ, 2019, IEEE I CONF COMP VIS, P7918, DOI 10.1109/ICCV.2019.00801
   Li Yunshang, 2022, Image Commun., V109
   Liu HJ, 2021, IEEE T MULTIMEDIA, V23, P4414, DOI 10.1109/TMM.2020.3042080
   Liu HJ, 2020, NEUROCOMPUTING, V398, P11, DOI 10.1016/j.neucom.2020.01.089
   Liu JL, 2019, IEEE INT CONF MULTI, P531, DOI 10.1109/ICMEW.2019.00097
   Mang Ye, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P229, DOI 10.1007/978-3-030-58520-4_14
   Miao JX, 2019, IEEE I CONF COMP VIS, P542, DOI 10.1109/ICCV.2019.00063
   Moon H, 2001, PERCEPTION, V30, P303, DOI 10.1068/p2896
   Nguyen DT, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17030605
   Park H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12026, DOI 10.1109/ICCV48922.2021.01183
   Pei JB, 2019, IEEE GLOB CONF SIG, DOI 10.1109/globalsip45357.2019.8969471
   Shang Gao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11741, DOI 10.1109/CVPR42600.2020.01176
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Tay CP, 2019, PROC CVPR IEEE, P7127, DOI 10.1109/CVPR.2019.00730
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang GA, 2019, IEEE I CONF COMP VIS, P3622, DOI 10.1109/ICCV.2019.00372
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang Jiabao, 2020, Image Commun., V80
   Wang YQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12006, DOI 10.1109/ICCV48922.2021.01181
   Wang YJ, 2021, SIGNAL PROCESS-IMAGE, V94, DOI 10.1016/j.image.2021.116197
   Wang ZX, 2019, PROC CVPR IEEE, P618, DOI 10.1109/CVPR.2019.00071
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wei ZY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P225, DOI 10.1109/ICCV48922.2021.00029
   Wu AC, 2017, IEEE I CONF COMP VIS, P5390, DOI 10.1109/ICCV.2017.575
   Wu Q, 2021, PROC CVPR IEEE, P4328, DOI 10.1109/CVPR46437.2021.00431
   Xiang XZ, 2019, IEEE SENS J, V19, P11706, DOI 10.1109/JSEN.2019.2936916
   Xu XH, 2022, KNOWL-BASED SYST, V257, DOI 10.1016/j.knosys.2022.109883
   Yan Lu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13376, DOI 10.1109/CVPR42600.2020.01339
   Ye M, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1092
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Ye M, 2021, IEEE T INF FOREN SEC, V16, P728, DOI 10.1109/TIFS.2020.3001665
   Ye M, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P347, DOI 10.1145/3343031.3351043
   Ye M, 2018, AAAI CONF ARTIF INTE, P7501
   Yi D, 2014, INT C PATT RECOG, P34, DOI 10.1109/ICPR.2014.16
   Zhu YX, 2020, NEUROCOMPUTING, V386, P97, DOI 10.1016/j.neucom.2019.12.100
   Zhan FN, 2021, INT C PATT RECOG, P6889, DOI 10.1109/ICPR48806.2021.9412395465
   Zhang Ji, 2022, Image Commun., V107
   Zhang P, 2021, IMAGE VISION COMPUT, V108, DOI 10.1016/j.imavis.2021.104118
   Zhang SZ, 2021, IEEE T IMAGE PROCESS, V30, P8861, DOI 10.1109/TIP.2021.3120881
   Zhang ZY, 2021, PATTERN RECOGN LETT, V150, P155, DOI 10.1016/j.patrec.2021.07.006
   Zhao YB, 2019, IET IMAGE PROCESS, V13, P2897, DOI 10.1049/iet-ipr.2019.0699
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng XT, 2022, IEEE T IMAGE PROCESS, V31, P6951, DOI 10.1109/TIP.2022.3217697
   Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI 10.1109/CVPR.2019.00224
   Zhu XK, 2018, IEEE T INF FOREN SEC, V13, P717, DOI 10.1109/TIFS.2017.2765524
NR 64
TC 0
Z9 0
U1 3
U2 3
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUL
PY 2024
VL 147
AR 105066
DI 10.1016/j.imavis.2024.105066
EA MAY 2024
PG 11
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA TO8T1
UT WOS:001242301200001
DA 2024-08-05
ER

PT J
AU Wang, ZB
   Wang, WM
   Li, NN
   Zhang, SY
   Chen, Q
   Jiang, Z
AF Wang, Zhibing
   Wang, Wenmin
   Li, Nannan
   Zhang, Shenyong
   Chen, Qi
   Jiang, Zhe
TI Multimodal parallel attention network for medical image segmentation
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Multimodal parallel attention; Feature parallel; Spatial parallel;
   Channel parallel; Medical image segmentation
ID U-NET; ARCHITECTURE
AB Medical image segmentation is a crucial aspect of medical image processing, and has been widely used in the detection and clinical diagnosis for brain, lung, liver, heart and other diseases. In this paper, we propose a novel multimodal parallel attention network, called MPA-Net, for medical image segmentation. MPA-Net is divided into two parts. The first part extracts more high -dimensional features by improved network structure, which contains the skip connection, the output of the multimodal parallel attention and the output of the previous upsampling layer. The second part incorporates a multimodal parallel attention mechanism, encompassing feature parallel attention, spatial parallel attention and channel parallel attention. This mechanism facilitates the effective fusion of high -dimensional and low -dimensional features, leading to enhanced context information. Experimental results on Kagglelung dataset, Liver dataset, Cell dataset, Drive dataset and Kvasir-SEG dataset show that MPA-Net has achieved better segmentation performance than that of other baseline methods, on lung, liver, cell contour, retinal vessel and polyps.
C1 [Wang, Zhibing; Wang, Wenmin; Li, Nannan; Zhang, Shenyong; Chen, Qi; Jiang, Zhe] Macau Univ Sci & Technol, Sch Comp Sci & Engn, Taipa, Macau, Peoples R China.
   [Zhang, Shenyong] Beijing Inst Technol, Sch Comp Technol, 6 Jinfeng Rd, Zhuhai 519088, Guangdong, Peoples R China.
C3 Macau University of Science & Technology; Beijing Institute of
   Technology
RP Wang, WM (corresponding author), Macau Univ Sci & Technol, Sch Comp Sci & Engn, Taipa, Macau, Peoples R China.
EM wmwang@must.edu.mo
CR Bilic P, 2023, MED IMAGE ANAL, V84, DOI 10.1016/j.media.2022.102680
   Cao Hu, 2023, Computer Vision - ECCV 2022 Workshops: Proceedings. Lecture Notes in Computer Science (13803), P205, DOI 10.1007/978-3-031-25066-8_9
   Cardona A, 2010, PLOS BIOL, V8, DOI 10.1371/journal.pbio.1000502
   Chen GP, 2023, IEEE T MED IMAGING, V42, P1289, DOI 10.1109/TMI.2022.3226268
   Chen J., 2021, TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation
   Cicek Ozgun, 2016, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016. 19th International Conference. Proceedings: LNCS 9901, P424, DOI 10.1007/978-3-319-46723-8_49
   Drozdzal M, 2016, LECT NOTES COMPUT SC, V10008, P179, DOI 10.1007/978-3-319-46976-8_19
   Fan X., 2024, Comput Biol Med, V172
   Fang CW, 2023, IEEE T MED IMAGING, V42, P1720, DOI 10.1109/TMI.2023.3237183
   Fitzgerald K, 2023, Arxiv, DOI [arXiv:2302.01027, DOI 10.48550/ARXIV.2302.01027]
   Gu ZW, 2019, IEEE T MED IMAGING, V38, P2281, DOI 10.1109/TMI.2019.2903562
   Hassan L, 2021, INT J INTERACT MULTI, V6, P35, DOI 10.9781/ijimai.2020.10.004
   Hatamizadeh A, 2022, IEEE WINT CONF APPL, P1748, DOI 10.1109/WACV51458.2022.00181
   Huang XH, 2023, IEEE T MED IMAGING, V42, P1484, DOI 10.1109/TMI.2022.3230943
   Ibtehaz N, 2020, NEURAL NETWORKS, V121, P74, DOI 10.1016/j.neunet.2019.08.025
   Jha D, 2021, IEEE J BIOMED HEALTH, V25, P2029, DOI 10.1109/JBHI.2021.3049304
   Jha D, 2020, COMP MED SY, P558, DOI 10.1109/CBMS49503.2020.00111
   Jha D, 2020, LECT NOTES COMPUT SC, V11962, P451, DOI 10.1007/978-3-030-37734-2_37
   Jha D, 2019, IEEE INT SYM MULTIM, P225, DOI 10.1109/ISM46123.2019.00049
   Jiang YK, 2023, Arxiv, DOI arXiv:2302.05615
   Kamnitsas K, 2017, MED IMAGE ANAL, V36, P61, DOI 10.1016/j.media.2016.10.004
   Li ZX, 2023, COMPUT BIOL MED, V158, DOI 10.1016/j.compbiomed.2023.106834
   Liskowski P, 2016, IEEE T MED IMAGING, V35, P2369, DOI 10.1109/TMI.2016.2546227
   Liu C, 2023, COMPUT ELECTRON AGR, V213, DOI 10.1016/j.compag.2023.108186
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Luo SJ, 2023, COMPUT MED IMAG GRAP, V103, DOI 10.1016/j.compmedimag.2022.102159
   Mader K.S., Kagglelung dataset
   Maji D, 2022, BIOMED SIGNAL PROCES, V71, DOI 10.1016/j.bspc.2021.103077
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Myronenko A, 2019, LECT NOTES COMPUT SC, V11384, P311, DOI 10.1007/978-3-030-11726-9_28
   Ni ZL, 2019, LECT NOTES COMPUT SC, V11954, P139, DOI 10.1007/978-3-030-36711-4_13
   Peng C, 2022, PROC CVPR IEEE, P20709, DOI 10.1109/CVPR52688.2022.02008
   Rasti R, 2023, IEEE T MED IMAGING, V42, P1413, DOI 10.1109/TMI.2022.3228285
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Roth HR, 2018, COMPUT MED IMAG GRAP, V66, P90, DOI 10.1016/j.compmedimag.2018.03.001
   Shin SY, 2023, COMPUT MED IMAG GRAP, V108, DOI 10.1016/j.compmedimag.2023.102259
   Sinha A, 2021, IEEE J BIOMED HEALTH, V25, P121, DOI 10.1109/JBHI.2020.2986926
   Staal J, 2004, IEEE T MED IMAGING, V23, P501, DOI 10.1109/TMI.2004.825627
   Taleb A, 2020, ADV NEUR IN, V33
   Tang H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3898, DOI 10.1109/ICCV48922.2021.00389
   Tang YC, 2022, PROC CVPR IEEE, P20698, DOI 10.1109/CVPR52688.2022.02007
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Ruxin, 2022, Med Image Anal, V78, P102395, DOI 10.1016/j.media.2022.102395
   Wang WX, 2021, LECT NOTES COMPUT SC, V12901, P109, DOI 10.1007/978-3-030-87193-2_11
   Wu YK, 2023, NONLINEAR DYNAM, V111, P2161, DOI [10.1007/s11071-022-07935-0, 10.1007/s00521-022-07859-1]
   Xian JL, 2023, IEEE T MED IMAGING, V42, P1774, DOI 10.1109/TMI.2023.3238114
   Xie YT, 2021, LECT NOTES COMPUT SC, V12903, P171, DOI 10.1007/978-3-030-87199-4_16
   You CY, 2022, IEEE T MED IMAGING, V41, P2228, DOI 10.1109/TMI.2022.3161829
   Yuan FN, 2023, PATTERN RECOGN, V136, DOI 10.1016/j.patcog.2022.109228
   Zhang C, 2023, INT J INTERACT MULTI, V8, P69, DOI 10.9781/ijimai.2023.01.009
   Zhang YC, 2023, ARTIF INTELL MED, V138, DOI 10.1016/j.artmed.2022.102476
   Zhang Z, 2020, COMPUT METH PROG BIO, V192, DOI 10.1016/j.cmpb.2020.105395
   Zhao A, 2019, PROC CVPR IEEE, P8535, DOI 10.1109/CVPR.2019.00874
   Zhao XQ, 2023, Arxiv, DOI arXiv:2303.10894
   Zhou XR, 2017, MED PHYS, V44, P5221, DOI 10.1002/mp.12480
   Zhou ZW, 2020, IEEE T MED IMAGING, V39, P1856, DOI 10.1109/TMI.2019.2959609
   Zhu Q, 2023, IEEE T MED IMAGING, V42, P1472, DOI 10.1109/TMI.2022.3230750
   Zhu YM, 2023, IEEE T MED IMAGING, V42, P1278, DOI 10.1109/TMI.2022.3226226
NR 58
TC 0
Z9 0
U1 9
U2 9
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUL
PY 2024
VL 147
AR 105069
DI 10.1016/j.imavis.2024.105069
EA MAY 2024
PG 11
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA TL8A9
UT WOS:001241497000001
DA 2024-08-05
ER

PT J
AU Ferdous, SN
   Li, X
AF Ferdous, Syeda Nyma
   Li, Xin
TI Robust ensemble person reidentification via orthogonal fusion with
   occlusion handling
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Ensemble learning; Orthogonal fusion with occlusion handling; (OFOH);
   Masked autoencoder (MAE); Person re -id
ID NETWORK; TRACKING; SAMPLES
AB Occlusion remains one of the major challenges in person reidentification (ReID) due to the diversity of poses and the variation of appearances. Developing novel architectures to improve the robustness of occlusion-aware person Re-ID requires new insights, especially on low-resolution edge cameras. We propose a deep ensemble model that harnesses both CNN and Transformer architectures to generate robust feature representations. To achieve robust Re-ID without manually labeling occluded regions, we propose to take an ensemble learningbased approach derived from the analogy between arbitrarily shaped occluded regions and robust feature representation. Using the orthogonality principle, our developed deep CNN model uses masked autoencoder (MAE) and global-local feature fusion for robust person identification. Furthermore, we present a part occlusion-aware transformer capable of learning feature space that is robust to occluded regions. Experimental results are reported on several Re-ID datasets to show the effectiveness of our developed ensemble model named orthogonal fusion with occlusion handling (OFOH). Compared to competing methods, the proposed OFOH approach has achieved competent rank-1 and mAP performance.
C1 [Ferdous, Syeda Nyma] West Virginia Univ, Lane Dept Comp Sci & Elect Engn, Morgantown, WV 26506 USA.
   [Li, Xin] SUNY Albany, Dept Comp Sci, Albany, NY 12222 USA.
C3 West Virginia University; State University of New York (SUNY) System;
   State University of New York (SUNY) Albany
RP Li, X (corresponding author), SUNY Albany, Dept Comp Sci, Albany, NY 12222 USA.
EM xin.li@ieee.org
FU NSF Award [CCSS-2348046]
FX Xin Li ' s work was partially supported by NSF Award CCSS-2348046.
CR Aich A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P152, DOI 10.1109/ICCV48922.2021.00022
   Chen A, 2022, BIOCYBERN BIOMED ENG, V42, P204, DOI 10.1016/j.bbe.2021.12.010
   Chen HY, 2022, PATTERN RECOGN, V130, DOI 10.1016/j.patcog.2022.108827
   Chen HY, 2022, COMPUT BIOL MED, V143, DOI 10.1016/j.compbiomed.2022.105265
   Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145
   Chen YT, 2015, PROC CVPR IEEE, P3470, DOI 10.1109/CVPR.2015.7298969
   Chen YF, 2020, PROC CVPR IEEE, P3792, DOI 10.1109/CVPR42600.2020.00385
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Fan ZZ, 2023, COMPUT BIOL MED, V162, DOI 10.1016/j.compbiomed.2023.107070
   Ferdous SN, 2022, IEEE IMAGE PROC, P2381, DOI 10.1109/ICIP46576.2022.9898013
   Ferdous SN, 2019, PROC SPIE, V11006, DOI 10.1117/12.2519045
   He KM, 2022, PROC CVPR IEEE, P15979, DOI 10.1109/CVPR52688.2022.01553
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He LX, 2019, IEEE I CONF COMP VIS, P8449, DOI 10.1109/ICCV.2019.00854
   He ST, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14993, DOI 10.1109/ICCV48922.2021.01474
   Hou RB, 2019, PROC CVPR IEEE, P9309, DOI 10.1109/CVPR.2019.00954
   Huang HJ, 2018, PROC CVPR IEEE, P5098, DOI 10.1109/CVPR.2018.00535
   Huang NC, 2023, PATTERN RECOGN, V135, DOI 10.1016/j.patcog.2022.109145
   Ilya L., 2019, Proceedings of ICLR
   Jiang KZ, 2020, IEEE T IMAGE PROCESS, V29, P8549, DOI 10.1109/TIP.2020.3016869
   Khatun A, 2023, PATTERN RECOGN, V137, DOI 10.1016/j.patcog.2022.109246
   Kim M, 2023, IEEE WINT CONF APPL, P1603, DOI 10.1109/WACV56688.2023.00165
   Kiran M, 2023, Arxiv, DOI arXiv:2104.06524
   Kulwa F, 2022, ENVIRON SCI POLLUT R, V29, P51909, DOI 10.1007/s11356-022-18849-0
   Lee S.H., 2021, arXiv, DOI DOI 10.48550/ARXIV.2112.13492
   Leng QM, 2020, IEEE T CIRC SYST VID, V30, P1092, DOI 10.1109/TCSVT.2019.2898940
   Li PK, 2022, IEEE T PATTERN ANAL, V44, P3260, DOI 10.1109/TPAMI.2020.3048039
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Li XT, 2022, ARTIF INTELL REV, V55, P4809, DOI 10.1007/s10462-021-10121-0
   Li YL, 2021, PROC CVPR IEEE, P2897, DOI 10.1109/CVPR46437.2021.00292
   Liang XD, 2019, IEEE T PATTERN ANAL, V41, P871, DOI 10.1109/TPAMI.2018.2820063
   Liu WL, 2022, PATTERN RECOGN, V130, DOI [10.1016/j.patcog.2020.108829, 10.1016/j.patcog.2022.108829]
   Liu X, 2023, IEEE T KNOWL DATA EN, V35, P857, DOI 10.1109/TKDE.2021.3090866
   Liu XH, 2021, PROC CVPR IEEE, P13329, DOI 10.1109/CVPR46437.2021.01313
   Martins AFT, 2016, PR MACH LEARN RES, V48
   McLaughlin N, 2016, PROC CVPR IEEE, P1325, DOI 10.1109/CVPR.2016.148
   Miao JX, 2019, IEEE I CONF COMP VIS, P542, DOI 10.1109/ICCV.2019.00063
   Misra I, 2020, PROC CVPR IEEE, P6706, DOI 10.1109/CVPR42600.2020.00674
   Ni H, 2022, PROC CVPR IEEE, P2477, DOI 10.1109/CVPR52688.2022.00252
   Nie QQ, 2023, COMPUT BIOL MED, V167, DOI 10.1016/j.compbiomed.2023.107620
   Niu ZY, 2021, NEUROCOMPUTING, V452, P48, DOI 10.1016/j.neucom.2021.03.091
   Ouyang WL, 2012, PROC CVPR IEEE, P3258, DOI 10.1109/CVPR.2012.6248062
   Polikar R, 2012, ENSEMBLE MACHINE LEARNING: METHODS AND APPLICATIONS, P1, DOI 10.1007/978-1-4419-9326-7_1
   Qiu HB, 2022, IEEE T PATTERN ANAL, V44, P6939, DOI 10.1109/TPAMI.2021.3098962
   Rahaman MM, 2021, COMPUT BIOL MED, V136, DOI 10.1016/j.compbiomed.2021.104649
   Rahaman MM, 2020, J X-RAY SCI TECHNOL, V28, P821, DOI 10.3233/XST-200715
   Rao Kanishka, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11154, DOI 10.1109/CVPR42600.2020.01117
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Sanchez FC, 1996, ANAL CHEM, V68, P79, DOI 10.1021/ac950496g
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shang Gao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11741, DOI 10.1109/CVPR42600.2020.01176
   Shu G, 2012, PROC CVPR IEEE, P1815, DOI 10.1109/CVPR.2012.6247879
   Somers V, 2023, IEEE WINT CONF APPL, P1613, DOI 10.1109/WACV56688.2023.00166
   Song LX, 2019, IEEE I CONF COMP VIS, P773, DOI 10.1109/ICCV.2019.00086
   Stadler D, 2021, PROC CVPR IEEE, P10953, DOI 10.1109/CVPR46437.2021.01081
   Sun J, 2005, PROC CVPR IEEE, P399
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Tan M., 2017, 2017 IEEE INT C COMP, P618
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang GA, 2020, PROC CVPR IEEE, P6448, DOI 10.1109/CVPR42600.2020.00648
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang JB, 2019, Arxiv, DOI arXiv:1901.05798
   Wang PF, 2023, IEEE T MULTIMEDIA, V25, P3154, DOI 10.1109/TMM.2022.3156282
   Wang T, 2022, AAAI CONF ARTIF INTE, P2540
   Wang XY, 2009, IEEE I CONF COMP VIS, P32, DOI 10.1109/iccv.2009.5459207
   Wang Z., 2020, Advances in Neural Information Processing Systems, V33, P19099
   Wang ZK, 2022, PROC CVPR IEEE, P4744, DOI 10.1109/CVPR52688.2022.00471
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wu GL, 2020, AAAI CONF ARTIF INTE, V34, P12362
   Wu JL, 2023, NEUROCOMPUTING, V518, P155, DOI 10.1016/j.neucom.2022.11.009
   Xu HH, 2022, Arxiv, DOI [arXiv:2206.04846, 10.48550/arXiv.2206.04846]
   Xuan SY, 2021, PROC CVPR IEEE, P11921, DOI 10.1109/CVPR46437.2021.01175
   Yang M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11752, DOI 10.1109/ICCV48922.2021.01156
   Yang T, 2005, PROC CVPR IEEE, P970
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Zhang JH, 2023, ARTIF INTELL REV, V56, P1013, DOI 10.1007/s10462-022-10192-7
   Zhang JH, 2021, PATTERN RECOGN, V115, DOI 10.1016/j.patcog.2021.107885
   Zhang SZ, 2021, IEEE T MULTIMEDIA, V23, P281, DOI 10.1109/TMM.2020.2977528
   Zhang XK, 2021, IEEE T CIRC SYST VID, V31, P2764, DOI 10.1109/TCSVT.2020.3033165
   Zhang Y, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3542820
   Zheng KC, 2021, PROC CVPR IEEE, P5306, DOI 10.1109/CVPR46437.2021.00527
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng WS, 2015, IEEE I CONF COMP VIS, P4678, DOI 10.1109/ICCV.2015.531
   Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI 10.1109/CVPR.2019.00224
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2019, IEEE T IMAGE PROCESS, V28, P1176, DOI 10.1109/TIP.2018.2874313
   Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380
   Zhuo JX, 2018, IEEE INT CON MULTI
NR 89
TC 0
Z9 0
U1 3
U2 3
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUN
PY 2024
VL 146
AR 105010
DI 10.1016/j.imavis.2024.105010
EA APR 2024
PG 11
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA QY7A3
UT WOS:001224482700001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Xue, JY
   Wang, ZJ
   Dong, GNN
   Zhu, AC
AF Xue, Jingyi
   Wang, Zijie
   Dong, Guan-Nan
   Zhu, Aichun
TI EESSO: Exploiting Extreme and Smooth Signals via Omni-frequency learning
   for Text-based Person Retrieval
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Text -based person retrieval; Person re -identification; Cross -modal
   retrieval; Multi -branch; Frequency; Mutual learning
ID NETWORK
AB Most of the existing methods manage to tackle the problem of text-based person retrieval from the spatial-wise perspective. In this paper, we manage to address the problem of this task from a novel perspective, namely, the frequency-wise perspective. To this end, we propose to Exploit Extreme and Smooth Signals via Omni-frequency learning (EESSO) through a jointly optimized multi-stream architecture. It consists of a Spatial Information Stream (SIS), an Extreme Signal Stream (ESS) and a Smooth Signal Stream (SSS). EESSO aims to excavate the complementary effect between spatial-wise and frequency-wise features, so as to achieve a superior performance. A novel Uncertainty-Guided Mutual Learning Mechanism (UG-MLM) is utilized during training, which not only enables the three streams to communicate with and learn from each other, but also models the data-related heteroscedastic uncertainty as a weight for knowledge transference, and hence enables each stream to adaptively allocate knowledge from the other two streams. A large number of experiments are carried out on the widely-used CUHK-PEDES, RSTPReid and ICFG-PEDES datasets to verify the effectiveness of EESSO. Through the experimental results, it may not be hard to find that EESSO has achieved the state-of-the-art performance in supervised, weakly supervised and cross-domain text-based person retrieval settings.
C1 [Xue, Jingyi; Wang, Zijie; Dong, Guan-Nan; Zhu, Aichun] Nanjing Tech Univ, Sch Comp Sci & Technol, Nanjing, Peoples R China.
C3 Nanjing Tech University
RP Zhu, AC (corresponding author), Nanjing Tech Univ, Sch Comp Sci & Technol, Nanjing, Peoples R China.
EM aichun.zhu@njtech.edu.cn
FU National Natural Science Foundation of China [62101245]; Postgraduate
   Research & Practice Innovation Program of Jiangsu Province, China
   [KYCX23_1452]; Future Network Scientific Research Fund Project
   [FNSRFP-2021-YB-21]
FX This work is partially supported by the National Natural Science
   Foundation of China (Grant No. 62101245) , and Postgraduate Research &
   Practice Innovation Program of Jiangsu Province, China (Grant No.
   KYCX23_1452) and Future Network Scientific Research Fund Project (Grant
   No. FNSRFP-2021-YB-21) .
CR Aggarwal S, 2020, IEEE WINT CONF APPL, P2606, DOI [10.1109/wacv45572.2020.9093640, 10.1109/WACV45572.2020.9093640]
   Chen DP, 2018, LECT NOTES COMPUT SC, V11220, P56, DOI 10.1007/978-3-030-01270-0_4
   Chen TL, 2018, IEEE WINT CONF APPL, P1879, DOI 10.1109/WACV.2018.00208
   Cheng DQ, 2022, IMAGE VISION COMPUT, V124, DOI 10.1016/j.imavis.2022.104493
   Deng WJ, 2018, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.2018.00110
   Ding ZF, 2021, Arxiv, DOI arXiv:2107.12666
   Farooq A, 2022, Arxiv, DOI arXiv:2101.08238
   Gao CY, 2021, Arxiv, DOI arXiv:2101.03036
   Ge Y., 2020, Advances in neural information processing systems, V33, P11309
   Ge YX, 2020, Arxiv, DOI [arXiv:2001.01526, 10.48550/arXiv.2001.01526]
   Gomez R, 2019, MULTIMODAL SCENE UNDERSTANDING: ALGORITHMS, APPLICATIONS AND DEEP LEARNING, P279, DOI 10.1016/B978-0-12-817358-9.00015-9
   Gray D., 2007, PROC IEEE INT WORKS, P1
   Hao X, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16383, DOI 10.1109/ICCV48922.2021.01609
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou RB, 2019, PROC CVPR IEEE, P9309, DOI 10.1109/CVPR.2019.00954
   Jing Y, 2020, AAAI CONF ARTIF INTE, V34, P11189
   Jing Ya, 2020, P IEEE CVF C COMP VI, P10678
   Kendall A., 2017, Adv Neural Inf Process Syst, V30
   Kendall A, 2018, PROC CVPR IEEE, P7482, DOI 10.1109/CVPR.2018.00781
   Kingma D., 2015, P INT C LEARN REPR S, P1, DOI DOI 10.1002/9781118900772.ETRDS0277
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Li S, 2017, PROC CVPR IEEE, P5187, DOI 10.1109/CVPR.2017.551
   Li W., 2012, ACCV, P31, DOI DOI 10.1007/978-3-642-37331-23
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Liu JW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P665, DOI 10.1145/3343031.3350991
   Niu K, 2020, IEEE T IMAGE PROCESS, V29, P5542, DOI 10.1109/TIP.2020.2984883
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sarafianos N, 2019, IEEE I CONF COMP VIS, P5813, DOI 10.1109/ICCV.2019.00591
   Shao Zhiyin, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P5566, DOI 10.1145/3503161.3548028
   Shu XJ, 2022, Arxiv, DOI arXiv:2208.08608
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Chengji, 2021, IJCAI
   Wang ZJ, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P5314, DOI 10.1145/3503161.3548057
   Wang ZJ, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P1984, DOI 10.1145/3503161.3548166
   Wang ZJ, 2021, LECT NOTES COMPUT SC, V13020, P462, DOI 10.1007/978-3-030-88007-1_38
   Wang ZJ, 2020, J ELECTRON IMAGING, V29, DOI 10.1117/1.JEI.29.4.043028
   Wu YS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1604, DOI 10.1109/ICCV48922.2021.00165
   Xia B, 2019, IEEE I CONF COMP VIS, P3759, DOI 10.1109/ICCV.2019.00386
   Xiao T., 2016, arXiv
   Xu ZQJ, 2024, Arxiv, DOI arXiv:1901.06523
   Yi D, 2014, INT C PATT RECOG, P34, DOI 10.1109/ICPR.2014.16
   Yuan Y, 2020, NEUROCOMPUTING, V378, P387, DOI 10.1016/j.neucom.2019.10.083
   Zhang Y, 2018, LECT NOTES COMPUT SC, V11205, P707, DOI 10.1007/978-3-030-01246-5_42
   Zhao SZ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11375, DOI 10.1109/ICCV48922.2021.01120
   Zhe Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P402, DOI 10.1007/978-3-030-58610-2_24
   Zheng KC, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3441, DOI 10.1145/3394171.3413864
   Zheng L, 2015, Arxiv, DOI arXiv:1502.02171
   Zheng ZD, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3383184
   Zhong Z, 2019, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2019.00069
   Zhu AC, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P209, DOI 10.1145/3474085.3475369
NR 52
TC 0
Z9 0
U1 1
U2 1
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD FEB
PY 2024
VL 142
AR 104912
DI 10.1016/j.imavis.2024.104912
EA JAN 2024
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA JN9R7
UT WOS:001173970000001
DA 2024-08-05
ER

PT J
AU Huang, JD
   Du, JX
   Zhang, HB
   Liu, HJ
AF Huang, Jing-Dong
   Du, Ji-Xiang
   Zhang, Hong-Bo
   Liu, Huai-Jin
TI Semantics feature sampling for point-based 3D object detection
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE 3D object detection; Point clouds; RoI pooling; Sampling method
AB Currently, 3D object detection is a research hotspot in the field of computer vision. In this paper, we have observed that the commonly used set abstraction module retains excessive irrelevant background information during downsampling, impacting object detection precision. To address this, we propose a mixed sampling method. During point feature extraction, we integrate semantic features into the sampling process, guiding the set abstraction module to sample foreground points. In order to leverage the high-quality 3D proposals generated in the first stage, we have developed a virtual point pooling module aimed at acquiring the features of these proposals. This module facilitates the capture of more comprehensive and resilient ROI features. Experimental results on the KITTI test set show a 3.51% higher Average Precision (AP) compared to the PointRCNN baseline, particularly for moderately challenging car classes, highlighting the effectiveness of our approach.
C1 [Huang, Jing-Dong; Du, Ji-Xiang; Zhang, Hong-Bo; Liu, Huai-Jin] Huaqiao Univ, Dept Comp Sci & Technol, Xiamen 361000, Peoples R China.
   [Huang, Jing-Dong; Du, Ji-Xiang; Zhang, Hong-Bo; Liu, Huai-Jin] Huaqiao Univ, Fujian Key Lab Big Data Intelligence & Secur, Xiamen 361000, Peoples R China.
   [Huang, Jing-Dong; Du, Ji-Xiang; Zhang, Hong-Bo; Liu, Huai-Jin] Huaqiao Univ, Xiamen Key Lab Comp Vis & Pattern Recognit, Xiamen 361000, Peoples R China.
C3 Huaqiao University; Huaqiao University; Huaqiao University
RP Du, JX (corresponding author), Huaqiao Univ, Dept Comp Sci & Technol, Xiamen 361000, Peoples R China.; Du, JX (corresponding author), Huaqiao Univ, Fujian Key Lab Big Data Intelligence & Secur, Xiamen 361000, Peoples R China.; Du, JX (corresponding author), Huaqiao Univ, Xiamen Key Lab Comp Vis & Pattern Recognit, Xiamen 361000, Peoples R China.
EM 21013083007@stu.hqu.edu.cn; jxdu@hqu.edu.cn; zhanghongbo@hqu.edu.cn
CR Ali W., 2018, P EUROPEAN C COMPUTE
   Barrera Alejandro, 2020, 2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC), DOI 10.1109/ITSC45102.2020.9294293
   Chen XZ, 2017, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2017.691
   Chen YL, 2019, IEEE I CONF COMP VIS, P9774, DOI [10.1109/iccv.2019.00987, 10.1109/ICCV.2019.00987]
   Chen Z., 2022, P 31 INT JOINT C ART, P827
   Chenhang He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11870, DOI 10.1109/CVPR42600.2020.01189
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dovrat O, 2019, PROC CVPR IEEE, P2755, DOI 10.1109/CVPR.2019.00287
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Graham B., 2015, P BRIT MACH VIS C 20
   Graham B, 2017, Arxiv, DOI arXiv:1706.01307
   Hu Q., 2020, P IEEECVF C COMPUTER, P11108, DOI [DOI 10.1109/CVPR42600.2020.01112, 10.1109/CVPR42600.2020.01112]
   Lang AH, 2019, PROC CVPR IEEE, P12689, DOI 10.1109/CVPR.2019.01298
   Li ZC, 2021, PROC CVPR IEEE, P7542, DOI 10.1109/CVPR46437.2021.00746
   Liang M, 2019, PROC CVPR IEEE, P7337, DOI 10.1109/CVPR.2019.00752
   Liu HJ, 2023, ENG APPL ARTIF INTEL, V123, DOI 10.1016/j.engappai.2023.106227
   Mao JG, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2703, DOI 10.1109/ICCV48922.2021.00272
   Pan XR, 2021, PROC CVPR IEEE, P7459, DOI 10.1109/CVPR46437.2021.00738
   Qi CR, 2017, ADV NEUR IN, V30
   Qi CR, 2019, IEEE I CONF COMP VIS, P9276, DOI 10.1109/ICCV.2019.00937
   Qi CR, 2018, PROC CVPR IEEE, P918, DOI 10.1109/CVPR.2018.00102
   Sheng HL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2723, DOI 10.1109/ICCV48922.2021.00274
   Shi SS, 2019, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2019.00086
   Shi SS, 2021, IEEE T PATTERN ANAL, V43, P2647, DOI 10.1109/TPAMI.2020.2977026
   Shi WJ, 2020, PROC CVPR IEEE, P1708, DOI 10.1109/CVPR42600.2020.00178
   Tengteng Huang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P35, DOI 10.1007/978-3-030-58555-6_3
   Vora S, 2020, PROC CVPR IEEE, P4603, DOI 10.1109/CVPR42600.2020.00466
   Xie L, 2020, AAAI CONF ARTIF INTE, V34, P12460
   Yan Y, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18103337
   Yang HH, 2023, PROC CVPR IEEE, P9403, DOI 10.1109/CVPR52729.2023.00907
   Yu J., 2023, P IEEE CVF C COMP VI, P5784
   Yu S, 2023, PROC CVPR IEEE, P19456, DOI 10.1109/CVPR52729.2023.01864
   Yussif SB, 2023, PROCEEDINGS OF THE 31ST ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2023, P27, DOI 10.1145/3581783.3612280
   Zetong Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11037, DOI 10.1109/CVPR42600.2020.01105
   Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472
NR 35
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD SEP
PY 2024
VL 149
AR 105180
DI 10.1016/j.imavis.2024.105180
PG 9
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA ZZ3X7
UT WOS:001279079700001
DA 2024-08-05
ER

PT J
AU Xiao, YW
   Liu, XM
   Zhu, AS
   Huang, J
AF Xiao, Yewei
   Liu, Xuanming
   Zhu, Aosu
   Huang, Jian
TI Relational-branchformer: Novel framework for audio-visual speech
   recognition
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Audio-visual speech recognition; Branchformer; Relational; CTC; Gated
   interlayer collaboration
AB This study embraced the state-of-the-art Branchformer series architecture within the realm of automatic speech recognition, supplanting the widely utilized Conformer architecture. This substitution offers an innovative remedy tailored to audio-visual speech recognition tasks. Building upon the Branchformer architecture, enhancements were made, culminating in the proposal of the Relational-Branchformer (R-Branchformer). The convolutional attention relation module was innovatively incorporated to augment the connectivity between the local and global branches by meticulously considering their interrelations and interplays. Consequently, this module facilitates the mutual embedding of local and global contextual information, ultimately leading to a substantial enhancement in model performance. Our model was grounded in the utilization of the connectionist temporal classification (CTC) loss, wherein intermediate CTC losses were incorporated between blocks. Moreover, through the reference and enhancement of the gated interlayer collaboration module, which superseded the inter CTC module, the conditional independence assumption intrinsic to the CTC model was effectively relaxed. As a consequence, this augmentation markedly bolstered the overall performance of our model. Furthermore, the audio-visual output enhancement module was proposed, which adeptly assimilates information from both audio and visual modalities to enrich the representation of audio-visual information. Consequently, the R-Branchformer model achieved remarkable word error rates of 1.7% and 1.5% on the LRS2 and LRS3 test sets, respectively, exemplifying its state-of-the-art performance in audio-visual speech recognition tasks.
C1 [Xiao, Yewei; Liu, Xuanming; Zhu, Aosu; Huang, Jian] Xiangtan Univ, Sch Informat Engn, Xiangtan 411105, Peoples R China.
   [Xiao, Yewei; Liu, Xuanming; Zhu, Aosu; Huang, Jian] Xiangtan Univ, Key Lab Intelligent Comp & Informat Proc, Minist Educ, Xiangtan 411105, Peoples R China.
C3 Xiangtan University; Xiangtan University
RP Liu, XM (corresponding author), Xiangtan Univ, Xiangtan, Peoples R China.
EM 202121623007@smail.xtu.edu.cn
FU Joint Fund for Regional Innovation and Development of NSFC [U19A2083];
   Science and Technology Research and Major Achievements Trans- formation
   Project of Strategic Emerging Industries in Hunan Province [2019 GK4007]
FX This research was funded by the Joint Fund for Regional Innovation and
   Development of NSFC under Grant U19A2083, and in part by the Science and
   Technology Research and Major Achievements Trans- formation Project of
   Strategic Emerging Industries in Hunan Province under Grant 2019 GK4007.
CR Afouras T, 2022, IEEE T PATTERN ANAL, V44, P8717, DOI 10.1109/TPAMI.2018.2889052
   Afouras T, 2018, Arxiv, DOI arXiv:1809.00496
   Afouras T, 2020, INT CONF ACOUST SPEE, P2143, DOI [10.1109/icassp40776.2020.9054253, 10.1109/ICASSP40776.2020.9054253]
   Bo Xu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14421, DOI 10.1109/CVPR42600.2020.01444
   Bulat A, 2017, IEEE I CONF COMP VIS, P1021, DOI 10.1109/ICCV.2017.116
   Burchi M, 2023, IEEE WINT CONF APPL, P2257, DOI 10.1109/WACV56688.2023.00229
   Burchi M, 2021, 2021 IEEE AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING WORKSHOP (ASRU), P8, DOI 10.1109/ASRU51503.2021.9687874
   Chen C, 2022, INT CONF ACOUST SPEE, P3688, DOI 10.1109/ICASSP43922.2022.9746668
   Chung JS, 2017, PROC CVPR IEEE, P3444, DOI 10.1109/CVPR.2017.367
   Chung JS, 2017, LECT NOTES COMPUT SC, V10112, P87, DOI 10.1007/978-3-319-54184-6_6
   Dauphin YN, 2017, PR MACH LEARN RES, V70
   Deng JK, 2020, PROC CVPR IEEE, P5202, DOI 10.1109/CVPR42600.2020.00525
   Gulati A., Convolution-augmented transformer for speech recognitionJ
   Han W, 2020, Arxiv, DOI arXiv:2005.03191
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hsu W.-N., 2022, Adv. Neural Inf. Proces. Syst., V35, P21157
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Kim K, 2022, IEEE W SP LANG TECH, P84, DOI 10.1109/SLT54892.2023.10022656
   Kingma D. P., 2014, arXiv
   Kriman S, 2020, INT CONF ACOUST SPEE, P6124, DOI [10.1109/ICASSP40776.2020.9053889, 10.1109/icassp40776.2020.9053889]
   Kudo T, 2018, Arxiv, DOI [arXiv:1808.06226, 10.48550/arXiv.1808.06226]
   Li J, 2019, INTERSPEECH, P71, DOI 10.21437/Interspeech.2019-1819
   Lu YP, 2019, Arxiv, DOI [arXiv:1906.02762, 10.48550/arxiv.1906.02762]
   Assael YM, 2016, Arxiv, DOI arXiv:1611.01599
   Ma PC, 2022, NAT MACH INTELL, V4, P930, DOI 10.1038/s42256-022-00550-z
   Ma PC, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P7613, DOI 10.1109/ICASSP39728.2021.9414567
   Makino T, 2019, 2019 IEEE AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING WORKSHOP (ASRU 2019), P905, DOI [10.1109/asru46091.2019.9004036, 10.1109/ASRU46091.2019.9004036]
   Mang Q, 2020, INT CONF ACOUST SPEE, P7829, DOI [10.1109/ICASSP40776.2020.9053896, 10.1109/icassp40776.2020.9053896]
   Moritz N, 2020, INT CONF ACOUST SPEE, P6074, DOI [10.1109/icassp40776.2020.9054476, 10.1109/ICASSP40776.2020.9054476]
   Nozaki J, 2021, Arxiv, DOI arXiv:2104.02724
   Park DS, 2020, INT CONF ACOUST SPEE, P6879, DOI [10.1109/ICASSP40776.2020.9053205, 10.1109/icassp40776.2020.9053205]
   Peng Yifan, 2022, PMLR, P17627
   Petridis S, 2018, IEEE W SP LANG TECH, P513, DOI 10.1109/SLT.2018.8639643
   Prajwal KR, 2022, PROC CVPR IEEE, P5152, DOI 10.1109/CVPR52688.2022.00510
   Ramachandran P, 2017, Arxiv, DOI arXiv:1710.05941
   Sakuma J., 2021, MLP-Based Architecture with Variable Length Input for Automatic Speech Recognition
   Serdyuk D, 2021, 2021 IEEE AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING WORKSHOP (ASRU), P796, DOI 10.1109/ASRU51503.2021.9688191
   Shi BW, 2022, Arxiv, DOI arXiv:2201.01763
   Shillingford B., 2018, Large-scale visual speech recognition
   VARGA A, 1993, SPEECH COMMUN, V12, P247, DOI 10.1016/0167-6393(93)90095-3
   Vaswani A., 2017, NEURIPS
   Wang X., 2022, arXiv
   Yang Y., 2023, ICASSP 2023, P1
   Yu JW, 2020, INT CONF ACOUST SPEE, P6984, DOI [10.1109/ICASSP40776.2020.9054127, 10.1109/icassp40776.2020.9054127]
   Zeyer A, 2018, INTERSPEECH, P7
   Zhang XX, 2019, IEEE I CONF COMP VIS, P713, DOI 10.1109/ICCV.2019.00080
   Zhao Y, 2020, AAAI CONF ARTIF INTE, V34, P6917
NR 47
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD SEP
PY 2024
VL 149
AR 105182
DI 10.1016/j.imavis.2024.105182
EA JUL 2024
PG 13
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA YW6P7
UT WOS:001271565000001
DA 2024-08-05
ER

PT J
AU Wang, LL
   Zhou, L
   Liang, PD
   Wang, K
   Ge, LZ
AF Wang, Liangliang
   Zhou, Lei
   Liang, Peidong
   Wang, Ke
   Ge, Lianzheng
TI SMTCNN - A global spatio-temporal texture convolutional neural network
   for 3D dynamic texture recognition
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Dynamic texture recognition; SMTCNN; Spatio-temporal semantic
   representation; Deep neural networks
ID PATTERNS
AB Dynamic textures (DT) are typically 3D videos of physical processes showing statistical regularity but have indeterminate spatial and temporal extent. Existing DT recognition methods usually neglect the global spatiotemporal relationships of DT which reflect the statistical regularities. In this paper, a spatio-temporal texture convolutional neural network (SMTCNN) is proposed for global semantic DT representation. Specifically, SMTCNN describes DT features by learning DT' temporal motion as well as the sources of the motions and the scenarios where the motion is happening, and accordingly, a motion net and a source net are formulated. In particular, a novel module consisting of expansion and concatenation implementations on deep features is presented, with an arbitrary 2D backbone as input, followed by a new 1D CNN including 4 convolutional, 2 pooling and 2 fully -connected layers to represent the 2D tensors in space-time, by transforming DT descriptors from discrete "words" to global "textures". A number of comparative experiments on three DT dataset - UCLA, DynTex and DynTex ++ are conducted to demonstrate our approach.
C1 [Wang, Liangliang; Zhou, Lei] Wuhan Univ Technol, Sch Comp Sci & Artificial Intelligence, Wuhan 430070, Peoples R China.
   [Wang, Liangliang] Wuhan Univ Technol, Chongqing Res Inst, Chongqing 401120, Peoples R China.
   [Liang, Peidong] Fujian Quanzhou Inst Adv Mfg Technol, Quanzhou 362008, Peoples R China.
   [Liang, Peidong] Fujian Key Lab Intelligent Operat & Maintenance Ro, Quanzhou 362008, Peoples R China.
   [Wang, Ke; Ge, Lianzheng] Harbin Inst Technol, State Key Lab Robot & Syst, Harbin 150001, Peoples R China.
C3 Wuhan University of Technology; Wuhan University of Technology; Harbin
   Institute of Technology
RP Liang, PD (corresponding author), Fujian Quanzhou Inst Adv Mfg Technol, Quanzhou 362008, Peoples R China.
EM lpd0004@hitqz.com
CR Andrearczyk V, 2018, PATTERN RECOGN, V76, P36, DOI 10.1016/j.patcog.2017.10.030
   Arashloo SR, 2017, J VIS COMMUN IMAGE R, V43, P89, DOI 10.1016/j.jvcir.2016.12.015
   Chen J, 2022, IEEE T IMAGE PROCESS, V31, P2661, DOI 10.1109/TIP.2022.3160070
   Chen J, 2021, IMAGE VISION COMPUT, V112, DOI 10.1016/j.imavis.2021.104214
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Ghanem B, 2010, LECT NOTES COMPUT SC, V6312, P223
   Gong XL, 2022, DIGIT SIGNAL PROCESS, V123, DOI 10.1016/j.dsp.2022.103454
   Ha M., 2024, J. Comp. Theories Appl, V2, P39
   Hadji I, 2018, LECT NOTES COMPUT SC, V11218, P334, DOI 10.1007/978-3-030-01264-9_20
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hong S, 2018, NEUROCOMPUTING, V273, P611, DOI 10.1016/j.neucom.2017.08.046
   Hou R, 2017, IEEE I CONF COMP VIS, P5823, DOI 10.1109/ICCV.2017.620
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Karen S., 2014, Proc. NIPS, P568, DOI DOI 10.1002/14651858.CD001941.PUB3
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Luo J, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3065417
   Nguyen T. P., 2021, Image Commun., V98
   Ozcelik Y.B., 2023, 2023 14 INT C EL EL, P1
   Qi XB, 2016, NEUROCOMPUTING, V171, P1230, DOI 10.1016/j.neucom.2015.07.071
   Quan YH, 2017, COMPUT VIS IMAGE UND, V165, P85, DOI 10.1016/j.cviu.2017.10.008
   Quan YH, 2016, PROC CVPR IEEE, P308, DOI 10.1109/CVPR.2016.40
   Quan YH, 2015, IEEE I CONF COMP VIS, P73, DOI 10.1109/ICCV.2015.17
   Rivera AR, 2015, IEEE T PATTERN ANAL, V37, P2146, DOI 10.1109/TPAMI.2015.2392774
   Ribas LC, 2022, APPL SOFT COMPUT, V114, DOI 10.1016/j.asoc.2021.108035
   Sezer A, 2021, SOLDER SURF MT TECH, V33, P291, DOI 10.1108/SSMT-04-2021-0013
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun DQ, 2010, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2010.5539939
   Sun L, 2015, IEEE I CONF COMP VIS, P4597, DOI 10.1109/ICCV.2015.522
   Nguyen TT, 2017, INT CONF IMAG PROC
   Nguyen TT, 2020, PATTERN RECOGN LETT, V135, P180, DOI 10.1016/j.patrec.2020.04.007
   Nguyen TT, 2020, COMPUT VIS IMAGE UND, V194, DOI 10.1016/j.cviu.2019.102882
   Nguyen TT, 2018, LECT NOTES COMPUT SC, V11182, P74, DOI 10.1007/978-3-030-01449-0_7
   Tiwari D, 2017, COMPUT ELECTR ENG, V62, P485, DOI 10.1016/j.compeleceng.2016.11.008
   Wang LM, 2021, PROC CVPR IEEE, P1895, DOI 10.1109/CVPR46437.2021.00193
   Xiong ZG, 2022, J SIGNAL PROCESS SYS, V94, P1129, DOI 10.1007/s11265-021-01737-0
   Xu Y, 2011, IEEE I CONF COMP VIS, P1219, DOI 10.1109/ICCV.2011.6126372
   Yag I, 2022, BIOLOGY-BASEL, V11, DOI 10.3390/biology11121732
   Yang XK, 2005, SIGNAL PROCESS-IMAGE, V20, P662, DOI 10.1016/j.image.2005.04.001
   Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512
   Zhang H, 2023, DISPLAYS, V79, DOI 10.1016/j.displa.2023.102456
   Zhang JW, 2022, PROC CVPR IEEE, P12991, DOI 10.1109/CVPR52688.2022.01266
   Zhang K., 2021, 2021 INT S SIGN CIRC, P1
   Zhang PC, 2022, DISPLAYS, V72, DOI 10.1016/j.displa.2022.102160
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
   Zhao XC, 2019, IEEE T MULTIMEDIA, V21, P1694, DOI 10.1109/TMM.2018.2890362
   Zhao XC, 2018, IEEE T MULTIMEDIA, V20, P552, DOI 10.1109/TMM.2017.2750415
   Zhou K, 2021, IMAGE VISION COMPUT, V108, DOI 10.1016/j.imavis.2021.104120
NR 47
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD AUG
PY 2024
VL 148
AR 105145
DI 10.1016/j.imavis.2024.105145
EA JUN 2024
PG 9
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA XH7A6
UT WOS:001260845200001
DA 2024-08-05
ER

PT J
AU Gao, LN
   Nie, RC
   Cao, JD
   Zhang, GC
AF Gao, Lingna
   Nie, Rencan
   Cao, Jinde
   Zhang, Gucheng
TI DFG-HCEN: A distinctive-feature guided and hierarchical channel enhanced
   network-based infrared and visible image fusion
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Infrared and visible image fusion; Unsupervised learning; Feature
   guided-based hierarchical channel; enhanced module; A hybrid loss
ID PERFORMANCE; ARCHITECTURE
AB In this paper, we propose an unsupervised learning approach for the task of infrared and visible image fusion. This approach is called a distinctive-feature guided and hierarchical channel enhanced network-based infrared and visible image Fusion (DFG-HCEN). Instead of using complex fusion rules, DFG-HCEN uses multi-level fusion to achieve fusion results, effectively avoiding information loss during feature extraction. To improve the fusion effect, we designed a distinctive-feature guided module that strengthens the relationship between modules. Moreover, the proposed hierarchical channel enhanced and distinctive-Feature guided module aims to facilitate the fusion framework in efficiently integrating the multilevel complementary features of the source pictures. In addition, we incorporate a hybrid loss method for unsupervised training of the provided DFG-HCEN. The fidelity loss is used to constrain the pixel similarity between the fused result and source images. The application of luminance regularization loss has been shown to be an efficient method for addressing the problem of luminance degradation in fused images. We conducted extensive experiments, including visual examination and quantitative analysis, comparing DFG-HCEN with thirteen other state-of-the-art fusion techniques. The results demonstrate the superiority of DFG-HCEN. Moreover, the extended object detection experiments validate the ability of DFG-HCEN to fully support downstream tasks.
C1 [Gao, Lingna; Nie, Rencan; Zhang, Gucheng] Yunnan Univ, Sch Informat Sci & Engn, Kunming 650500, Peoples R China.
   [Cao, Jinde] Southeast Univ, Sch Math, Nanjing 211189, Peoples R China.
   [Cao, Jinde] Purple Mt Labs, Nanjing 211111, Peoples R China.
   [Cao, Jinde] Ahlia Univ, Manama, Bahrain.
C3 Yunnan University; Southeast University - China; Ahlia University
   Bahrain
RP Nie, RC (corresponding author), Yunnan Univ, Sch Informat Sci & Engn, Kunming 650500, Peoples R China.
EM rcnie@ynu.edu.cn
RI Cao, Jinde/D-1482-2012
OI Cao, Jinde/0000-0003-3133-7119
FU National Natural Science Foundation of China [61966037, 61833005];
   National Key Research and Development Project of China [2020YFA0714301];
   China Postdoctoral Science Foundation [2017M621586]; Key Project of
   Yunnan Basic Research Program [202301AS070025]; Yunnan Provincial
   Department of Education Science Foundation [2024Y031]; Graduate Research
   Innovation program of Yunnan University [KC-23233941]
FX This work was supported by National Natural Science Foundation of China
   under Grants 61966037 and 61833005, National Key Research and
   Development Project of China under Grant 2020YFA0714301, and China
   Postdoctoral Science Foundation under Grant 2017M621586, and the Key
   Project of Yunnan Basic Research Program under grant 202301AS070025, and
   Yunnan Provincial Department of Education Science Foundation under Grant
   2024Y031, and Graduate Research Innovation program of Yunnan University
   KC-23233941.
CR Bhatnagar G, 2013, IEEE T MULTIMEDIA, V15, P1014, DOI 10.1109/TMM.2013.2244870
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Easley G, 2008, APPL COMPUT HARMON A, V25, P25, DOI 10.1016/j.acha.2007.09.003
   Garcia-Garcia A, 2017, Arxiv, DOI arXiv:1704.06857
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Han Y, 2013, INFORM FUSION, V14, P127, DOI 10.1016/j.inffus.2011.08.002
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jian LH, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3022438
   John Vijay, 2021, Pattern Recognition. ICPR International Workshops and Challenges. Proceedings. Lecture Notes in Computer Science (LNCS 12666), P277, DOI 10.1007/978-3-030-68780-9_24
   Kingma D. P., 2014, arXiv
   Kumar Srivastava R., 2015, arXiv
   Lewis JJ, 2007, INFORM FUSION, V8, P119, DOI 10.1016/j.inffus.2005.09.006
   Li GF, 2021, INFORM FUSION, V71, P109, DOI 10.1016/j.inffus.2021.02.008
   Li H, 2022, Arxiv, DOI arXiv:1804.08992
   Li H, 2020, IEEE T INSTRUM MEAS, V69, P9645, DOI 10.1109/TIM.2020.3005230
   Li H, 2020, IEEE T IMAGE PROCESS, V29, P4733, DOI 10.1109/TIP.2020.2975984
   Li H, 2019, INFRARED PHYS TECHN, V102, DOI 10.1016/j.infrared.2019.103039
   Li H, 2018, INT C PATT RECOG, P2705, DOI 10.1109/ICPR.2018.8546006
   Li H, 2019, IEEE T IMAGE PROCESS, V28, P2614, DOI 10.1109/TIP.2018.2887342
   Li J, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3029360
   Li SS, 2008, 2008 INTERNATIONAL CONFERENCE ON AUDIO, LANGUAGE AND IMAGE PROCESSING, VOLS 1 AND 2, PROCEEDINGS, P167, DOI 10.1109/ICALIP.2008.4589989
   Liu JY, 2022, IEEE T CIRC SYST VID, V32, P105, DOI 10.1109/TCSVT.2021.3056725
   Liu JY, 2021, IEEE SIGNAL PROC LET, V28, P1818, DOI 10.1109/LSP.2021.3109818
   Liu Y, 2017, INFORM FUSION, V36, P191, DOI 10.1016/j.inffus.2016.12.001
   Ma JY, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3038013
   Ma JY, 2022, IEEE-CAA J AUTOMATIC, V9, P1200, DOI 10.1109/JAS.2022.105686
   Ma JY, 2020, IEEE T IMAGE PROCESS, V29, P4980, DOI 10.1109/TIP.2020.2977573
   Ma JY, 2019, INFORM FUSION, V48, P11, DOI 10.1016/j.inffus.2018.09.004
   Ma JY, 2016, INFORM FUSION, V31, P100, DOI 10.1016/j.inffus.2016.02.001
   Nencini F, 2007, INFORM FUSION, V8, P143, DOI 10.1016/j.inffus.2006.02.001
   Pang HC, 2012, 2012 5TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING (CISP), P543, DOI 10.1109/CISP.2012.6469884
   Prabhakar KR, 2017, IEEE I CONF COMP VIS, P4724, DOI 10.1109/ICCV.2017.505
   Qu GH, 2002, ELECTRON LETT, V38, P313, DOI 10.1049/el:20020212
   Raza A, 2021, IEEE J-STARS, V14, P3426, DOI 10.1109/JSTARS.2021.3065121
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sahu Akanksha, 2014, 2014 International Conference on Medical Imaging, m-Health and Emerging Communication Systems (MedCom), P448, DOI 10.1109/MedCom.2014.7006050
   Saini R, 2020, IEEE WINT CONF APPL, P1616, DOI [10.1109/wacv45572.2020.9093341, 10.1109/WACV45572.2020.9093341]
   Shopovska I, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19173727
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sirer Emin Gun, 2001, Cliquenet: A self-organizing, scalable, peer-to-peer anonymous communication substrate
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tang H, 2018, INFORM SCIENCES, V433, P125, DOI 10.1016/j.ins.2017.12.043
   Tang LF, 2022, IEEE-CAA J AUTOMATIC, V9, P2121, DOI 10.1109/JAS.2022.106082
   Tang LF, 2022, INFORM FUSION, V83, P79, DOI 10.1016/j.inffus.2022.03.007
   Toet A., 1990, Machine Vision and Applications, V3, P1, DOI 10.1007/BF01211447
   Uçar A, 2017, SIMUL-T SOC MOD SIM, V93, P759, DOI 10.1177/0037549717709932
   Wang XS, 2020, Arxiv, DOI arXiv:1904.07793
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xie HS, 2023, INFORM FUSION, V98, DOI 10.1016/j.inffus.2023.101835
   Xu H, 2021, IEEE T COMPUT IMAG, V7, P824, DOI 10.1109/TCI.2021.3100986
   Xu H, 2022, IEEE T PATTERN ANAL, V44, P502, DOI 10.1109/TPAMI.2020.3012548
   Xydeas CS, 2000, ELECTRON LETT, V36, P308, DOI 10.1049/el:20000267
   Yan X, 2018, Arxiv, DOI arXiv:1806.07272
   Zhang XC, 2020, INFORM FUSION, V63, P166, DOI 10.1016/j.inffus.2020.05.002
   Zhang Y, 2020, INFORM FUSION, V54, P99, DOI 10.1016/j.inffus.2019.07.011
   Zhao C, 2021, NEURAL COMPUT APPL, V33, P6595, DOI 10.1007/s00521-020-05421-5
   Zhou ZW, 2018, LECT NOTES COMPUT SC, V11045, P3, DOI 10.1007/978-3-030-00889-5_1
   Zhu DP, 2022, IEEE SENS J, V22, P8808, DOI 10.1109/JSEN.2022.3161733
   Zhu JH, 2021, CONCURR COMP-PRACT E, V33, DOI 10.1002/cpe.6155
NR 62
TC 0
Z9 0
U1 1
U2 1
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD AUG
PY 2024
VL 148
AR 105115
DI 10.1016/j.imavis.2024.105115
EA JUN 2024
PG 13
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA WV5O5
UT WOS:001257663100001
DA 2024-08-05
ER

PT J
AU Ma, YD
   Lan, XB
AF Ma, Yingdong
   Lan, Xiaobin
TI Semantic segmentation using cross-stage feature reweighting and
   efficient self-attention
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Semantic segmentation; Convolutional neural networks; Transformer;
   Feature fusion and reweighting
ID NETWORK
AB Recently, vision transformers have demonstrated strong performance in various computer vision tasks. The success of ViTs can be attribute to the ability of capturing long-range dependencies. However, transformer-based approaches often yield segmentation maps with incomplete object structures because of restricted cross-stage information propagation and lack of low-level details. To address these problems, we introduce a CNNtransformer semantic segmentation architecture which adopts a CNN backbone for multi-level feature extraction and a transformer encoder that focuses on global perception learning. Transformer embeddings of all stages are integrated to compute feature weights for dynamic cross-stage feature reweighting. As a result, high-level semantic context and low-level spatial details can be embedded into each stage to preserve multi-level information. An efficient attention-based feature fusion mechanism is developed to combine reweighted transformer embeddings with CNN features to generate segmentation maps with more complete object structure. Different from regular self-attention that has quadratic computational complexity, our efficient self-attention method achieves similar performance with linear complexity. Experimental results on ADE20K and Cityscapes datasets show that the proposed segmentation approach demonstrates superior performance against most state-of-the-art networks.
C1 [Ma, Yingdong; Lan, Xiaobin] Inner Mongolia Univ, Coll Comp Sci, 235 West Daxue Rd, Hohhot, Peoples R China.
C3 Inner Mongolia University
RP Ma, YD (corresponding author), Inner Mongolia Univ, Coll Comp Sci, 235 West Daxue Rd, Hohhot, Peoples R China.
EM csmyd@imu.edu.cn
CR Cao Y, 2019, IEEE ICC
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Changqian Yu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12413, DOI 10.1109/CVPR42600.2020.01243
   Chen L.-C., 2018, ECCV, P801, DOI [DOI 10.1007/978-3-030-01234-249, 10.1007/978-3-030-01234-2_49]
   Chen LC, 2017, Arxiv, DOI [arXiv:1706.05587, 10.48550/arXiv.1706.05587, DOI 10.48550/ARXIV.1706.05587]
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Cheng B, 2021, ADV NEUR IN, V34
   Cheng BW, 2022, PROC CVPR IEEE, P1280, DOI 10.1109/CVPR52688.2022.00135
   Chu X., 2023, P ICLR
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong XY, 2022, PROC CVPR IEEE, P12114, DOI 10.1109/CVPR52688.2022.01181
   Dosovitskiy A., 2021, ICLR
   Elhanashi A, 2022, PROC SPIE, V12102, DOI 10.1117/12.2618762
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Gan Y, 2021, INT C PATT RECOG, P795, DOI 10.1109/ICPR48806.2021.9412639
   Guo M.-H., 2022, P ADV NEURAL INFORM, P1
   Han K, 2021, ADV NEUR IN
   He JJ, 2019, IEEE I CONF COMP VIS, P3561, DOI 10.1109/ICCV.2019.00366
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heo B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11916, DOI 10.1109/ICCV48922.2021.01172
   Hou Q., 2022, arXiv
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Yun YK, 2022, Arxiv, DOI arXiv:2205.11283
   Kirillov A, 2019, PROC CVPR IEEE, P6392, DOI 10.1109/CVPR.2019.00656
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Li YY, 2023, IEEE I CONF COMP VIS, P16843, DOI 10.1109/ICCV51070.2023.01549
   Li ZC, 2022, IEEE T PATTERN ANAL, V44, P9904, DOI 10.1109/TPAMI.2021.3132068
   Liang TT, 2022, IEEE T IMAGE PROCESS, V31, P6893, DOI 10.1109/TIP.2022.3216771
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu Shiwei, 2023, INT C LEARN REPR ICL
   Liu SA, 2022, PROC CVPR IEEE, P16815, DOI 10.1109/CVPR52688.2022.01633
   Liu Y, 2024, Arxiv, DOI [arXiv:2106.03180, DOI 10.48550/ARXIV.2106.03180]
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Ma LF, 2023, IEEE T MULTIMEDIA, V25, P2774, DOI 10.1109/TMM.2022.3151145
   Ma YD, 2023, MACH VISION APPL, V34, DOI 10.1007/s00138-023-01402-5
   Mehta S., 2023, Transactions on Machine Learning Research
   Mingmin Zhen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13663, DOI 10.1109/CVPR42600.2020.01368
   Peng ZL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P357, DOI 10.1109/ICCV48922.2021.00042
   Ranftl R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12159, DOI 10.1109/ICCV48922.2021.01196
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683
   Strudel R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7242, DOI 10.1109/ICCV48922.2021.00717
   Tan MX, 2019, PR MACH LEARN RES, V97
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang WH, 2022, COMPUT VIS MEDIA, V8, P415, DOI 10.1007/s41095-022-0274-8
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Welling M., 2016, ICLR, P1, DOI DOI 10.48550/ARXIV.1609.02907
   Xiao TT, 2018, LECT NOTES COMPUT SC, V11209, P432, DOI 10.1007/978-3-030-01228-1_26
   Xie CX, 2022, PROC CVPR IEEE, P11707, DOI 10.1109/CVPR52688.2022.01142
   Xie EZ, 2021, ADV NEUR IN, V34
   Xier Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P617, DOI 10.1007/978-3-030-58583-9_37
   Xu Yufei, 2021, Advances in neural information processing systems, DOI DOI 10.48550/ARXIV.2106.03348
   Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20
   Yu Q., 2021, P ADV NEUR INF PROC, V34, P12992
   Yuan Y., 2021, P C NEUR INF PROC SY, P7281
   Yuan YH, 2021, Arxiv, DOI [arXiv:1909.11065, DOI 10.48550/ARXIV.1909.11065]
   Zhang F, 2019, IEEE I CONF COMP VIS, P6797, DOI 10.1109/ICCV.2019.00690
   Zhang H, 2022, IEEE COMPUT SOC CONF, P2735, DOI 10.1109/CVPRW56347.2022.00309
   Zhang H, 2018, PROC CVPR IEEE, P7151, DOI 10.1109/CVPR.2018.00747
   Zhang WQ, 2022, PROC CVPR IEEE, P12073, DOI 10.1109/CVPR52688.2022.01177
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11213, P270, DOI 10.1007/978-3-030-01240-3_17
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhou BL, 2017, PROC CVPR IEEE, P5122, DOI 10.1109/CVPR.2017.544
   Zhu FR, 2021, IEEE INT CONF COMP V, P2667, DOI 10.1109/ICCVW54120.2021.00301
   Zhu LY, 2021, PROC CVPR IEEE, P12532, DOI 10.1109/CVPR46437.2021.01235
   Zhu L, 2023, PROC CVPR IEEE, P10323, DOI 10.1109/CVPR52729.2023.00995
NR 71
TC 0
Z9 0
U1 3
U2 3
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD MAY
PY 2024
VL 145
DI 10.1016/j.imavis.2024.104996
EA MAR 2024
PG 11
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA QI6H7
UT WOS:001220281500001
DA 2024-08-05
ER

PT J
AU Yan, YC
   Jiang, T
   Li, XF
   Sun, LP
   Zhu, JJ
   Lin, JX
AF Yan, Yicheng
   Jiang, Tong
   Li, Xianfeng
   Sun, Lianpeng
   Zhu, Jinjun
   Lin, Jianxin
TI Model-agnostic progressive saliency map generation for object detector
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Saliency map; Interpretability; Model -agnostic; Black -box; Object
   detector
ID BLACK-BOX
AB With the widespread adoption of object detection models across various industries, the interpretability of these detectors has become an important research topic. The interpretability of a detector helps humans understand which areas significantly contribute to the model's decision. Furthermore, the interpretability enhances the credibility of detectors and helps identify their strengths and weaknesses. Due to the ability to provide intuitive explanations of models, the saliency map has been widely employed in the field of interpreting deep models. Model-agnostic interpretability methods are more general approaches as they treat the model as a black box without considering its internal complexity structure. However, existing model-agnostic interpretability methods often introduce "noise" into saliency maps by applying random masking and fixed masking granularity. This noise reduces the quality and interpretability of the generated saliency maps. To address this challenge and obtain more interpretable saliency maps for object detection models, this paper proposes a model-agnostic progressive saliency map generation method based on a hierarchical framework called MAPSM. In MAPSM, an adaptive masking partition mechanism is introduced to adapt the masking granularity to different object sizes. Additionally, MAPSM employs a saliency-driven mask generation strategy to effectively reduce the "noise". Utilizing a hierarchical framework, MAPSM progressively discovers and refines the saliency areas of objects, resulting in more interpretable saliency maps. To evaluate the quality of the saliency maps generated by MAPSM, we compare it with other methods in multiple metrics. Experimental results demonstrate that our method produces saliency maps with better quality and interpretability.
C1 [Yan, Yicheng; Jiang, Tong; Li, Xianfeng] Macau Univ Sci & Technol, Sch Comp Sci & Engn, Macau 999078, Peoples R China.
   [Sun, Lianpeng; Zhu, Jinjun] Sun Yat Sen Univ, Sch Environm Sci & Engn, Guangzhou 510006, Peoples R China.
   [Lin, Jianxin] Guangdong AIKE Environm Sci & Tech Co Ltd, Zhongshan 528451, Peoples R China.
C3 Macau University of Science & Technology; Sun Yat Sen University
RP Li, XF (corresponding author), Macau Univ Sci & Technol, Sch Comp Sci & Engn, Macau 999078, Peoples R China.
EM xifli@must.edu.mo
RI Li, Xianfeng/ABF-3469-2021
OI Li, Xianfeng/0000-0002-2473-651X
FU Science and Technology Development Fund of Macau [0079/2019/AMJ];
   National Key Research and Development Program of China [2019YFE0111400]
FX This work is supported in part by the Science and Technology Development
   Fund of Macau under Grant 0079/2019/AMJ, and in part by the National Key
   Research and Development Program of China under Grant 2019YFE0111400.
CR Adadi A, 2018, IEEE ACCESS, V6, P52138, DOI 10.1109/ACCESS.2018.2870052
   Ahmad HM, 2022, J MANUF SYST, V64, P181, DOI 10.1016/j.jmsy.2022.06.011
   Anjomshoae S, 2021, IMAGE VISION COMPUT, V116, DOI 10.1016/j.imavis.2021.104310
   Bach S, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0130140
   Bai X, 2021, PATTERN RECOGN, V120, DOI 10.1016/j.patcog.2021.108102
   Arrieta AB, 2020, INFORM FUSION, V58, P82, DOI 10.1016/j.inffus.2019.12.012
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Chug A, 2023, SOFT COMPUT, V27, P13613, DOI 10.1007/s00500-022-07177-7
   Cooper J, 2022, PATTERN RECOGN, V129, DOI 10.1016/j.patcog.2022.108743
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fong RC, 2017, IEEE I CONF COMP VIS, P3449, DOI 10.1109/ICCV.2017.371
   Fu KR, 2022, IEEE T PATTERN ANAL, V44, P5541, DOI 10.1109/TPAMI.2021.3073689
   Gaur L, 2023, MULTIMEDIA SYST, V29, P1729, DOI 10.1007/s00530-021-00794-6
   Han K, 2023, IEEE T PATTERN ANAL, V45, P87, DOI 10.1109/TPAMI.2022.3152247
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KL, 2023, INTEL MED, V3, P59, DOI 10.1016/j.imed.2022.07.002
   Hossain M.U., 2022, Intell. Syst. Appl, V14, P200075, DOI [10.1016/j.iswa.2022.200075, DOI 10.1016/J.ISWA.2022.200075]
   Jiang PT, 2021, IEEE T IMAGE PROCESS, V30, P5875, DOI 10.1109/TIP.2021.3089943
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Özcelik YB, 2023, FRACTAL FRACT, V7, DOI 10.3390/fractalfract7080598
   Paszke A., 2017, NIPS-W
   Petsiuk V, 2018, Arxiv, DOI [arXiv:1806.07421, 10.48550/arXiv.1806.07421]
   Petsiuk V, 2021, PROC CVPR IEEE, P11438, DOI 10.1109/CVPR46437.2021.01128
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Sezer A, 2021, SOLDER SURF MT TECH, V33, P291, DOI 10.1108/SSMT-04-2021-0013
   Sundararajan M, 2017, PR MACH LEARN RES, V70
   Tjoa E, 2021, IEEE T NEUR NET LEAR, V32, P4793, DOI 10.1109/TNNLS.2020.3027314
   Springenberg JT, 2015, Arxiv, DOI [arXiv:1412.6806, 10.48550/arXiv.1412.6806]
   Truong V, 2023, Arxiv, DOI arXiv:2306.02744
   Usamentiaga R, 2022, IEEE T IND APPL, V58, P4203, DOI 10.1109/TIA.2022.3151560
   Wang H, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3196954
   Wang HJ, 2020, Cambria Sinophone Wo, P111, DOI 10.1109/CVPRW50498.2020.00020
   Wang WG, 2022, IEEE T PATTERN ANAL, V44, P3239, DOI 10.1109/TPAMI.2021.3051099
   Yag I, 2022, BIOLOGY-BASEL, V11, DOI 10.3390/biology11121732
   Yan YC, 2022, IEEE ACCESS, V10, P98268, DOI 10.1109/ACCESS.2022.3206379
   Yang Q, 2021, INT C PATT RECOG, P1376, DOI 10.1109/ICPR48806.2021.9413046
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zou ZX, 2023, P IEEE, V111, P257, DOI 10.1109/JPROC.2023.3238524
NR 41
TC 0
Z9 0
U1 1
U2 1
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD MAY
PY 2024
VL 145
AR 104988
DI 10.1016/j.imavis.2024.104988
EA MAR 2024
PG 13
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA QR2C3
UT WOS:001222521900001
DA 2024-08-05
ER

PT J
AU Hadikhani, P
   Lai, DTC
   Ong, WH
AF Hadikhani, Parham
   Lai, Daphne Teck Ching
   Ong, Wee -Hong
TI Flexible multi-objective particle swarm optimization clustering with
   game theory to address human activity discovery fully unsupervised
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Human activity discovery; Unsupervised learning; Clustering; Feature
   extraction; Incremental manner; Multi -objectives optimization;
   Dimension reduction; Skeleton sequence
ID ACTION RECOGNITION; DATA SET; NUMBER; REPRESENTATIONS; NETWORKS
AB Human activity recognition is a crucial field of study, but current approaches often require ground truth labels, which are not always available. We propose a new method called the Flexible Multi-Objective Particle swarm optimization clustering method based on Game theory (FMOPG), which can identify human activities without any supervision. Unlike traditional clustering methods that require an estimate of the number of clusters and are often inaccurate, FMOPG handles varying cluster numbers with an incremental technique, selecting clusters with good connectivity and separation. We enhance Particle Swarm Optimization (PSO) with mean-shift vectors for faster convergence and better handling of non-spherical clusters. Employing multi-objective optimization and Gaussian mutation, FMOPG provides a range of optimal solutions. We map the optimization problem to game theory to select the best solution based on different criteria. A smart grid-based method is proposed for population initialization, reducing variance and improving reliability. FMOPG outperforms state-of-the-art methods, improving clustering accuracy by 3.65%. Moreover, the incremental technique has improved clustering time by 71.18%.
C1 [Hadikhani, Parham; Lai, Daphne Teck Ching; Ong, Wee -Hong] Univ Brunei Darussalam, Sch Digital Sci, Gadong, Brunei.
C3 University Brunei Darussalam
RP Lai, DTC (corresponding author), Univ Brunei Darussalam, Sch Digital Sci, Gadong, Brunei.
EM 20h8561@ubd.edu.bn; daphne.lai@ubd.edu.bn; weehong.ong@ubd.edu.bn
FU Universiti Brunei Darussalam [UBD/RSCH/1.11/FICBF (b) /2019/001]
FX This work was supported by Grant UBD/RSCH/1.11/FICBF (b) /2019/001 from
   Universiti Brunei Darussalam.
CR Abubaker A, 2015, PLOS ONE, V10, DOI [10.1371/journal.pone.0130995, 10.1371/journal.pone.0135641]
   Agarwal P., 2021, Artif. Intell. Sustain. Ind, V4, P169
   Agarwal P, 2021, SOFT COMPUT, V25, P10237, DOI 10.1007/s00500-021-05973-1
   Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027
   Bajer D, 2016, EXPERT SYST APPL, V60, P294, DOI 10.1016/j.eswa.2016.05.009
   Boualia SN, 2021, INFORMATICS-BASEL, V8, DOI 10.3390/informatics8010002
   Calinski Tadeusz, 1974, Communications in Statistics, V3, P1, DOI [10.1080/03610927408827101, DOI 10.1080/03610927408827101]
   Cheng DZ, 2023, IEEE J BIOMED HEALTH, V27, P3900, DOI 10.1109/JBHI.2023.3275438
   Cippitelli E, 2016, COMPUT INTEL NEUROSC, V2016, DOI 10.1155/2016/4351435
   DAVIES DL, 1979, IEEE T PATTERN ANAL, V1, P224, DOI 10.1109/TPAMI.1979.4766909
   Dunn J.C., 1973, Journal of Cybernetics, DOI 10.1080/01969727308546046
   Eldib M, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20092513
   Ercolano G, 2021, INTEL SERV ROBOT, V14, P175, DOI 10.1007/s11370-021-00358-7
   FORGY EW, 1965, BIOMETRICS, V21, P768
   Fujita A, 2014, COMPUT STAT DATA AN, V73, P27, DOI 10.1016/j.csda.2013.11.012
   Gaglio S, 2015, IEEE T HUM-MACH SYST, V45, P586, DOI 10.1109/THMS.2014.2377111
   Gao X, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P601, DOI 10.1145/3343031.3351170
   Guo M, 2018, LECT NOTES COMPUT SC, V11205, P673, DOI 10.1007/978-3-030-01246-5_40
   Hadikhani P., 2022, Human Activity Discovery with Automatic Multi-Objective Particle Swarm Optimization Clustering with Gaussian Mutation and Game Theory
   Hadikhani P, 2023, IMAGE VISION COMPUT, V136, DOI 10.1016/j.imavis.2023.104712
   Hadikhani P, 2022, PROCEEDINGS OF THE 2022 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE COMPANION, GECCO 2022, P487, DOI 10.1145/3520304.3528885
   Hadikhani P, 2022, Arxiv, DOI arXiv:2201.05314
   Hadikhani P, 2020, WIREL NETW, V26, P507, DOI 10.1007/s11276-019-02157-6
   Hartigan J.A., 1975, Clustering Algorithms
   Hossain T., 2021, Vision, Sensing and Analytics: Integrative Approaches, V207, P125, DOI 10.1007/978-3-030-75490-7_5
   Hou YH, 2018, IEEE T CIRC SYST VID, V28, P807, DOI 10.1109/TCSVT.2016.2628339
   Huang KJ, 2018, AAAI CONF ARTIF INTE, P3263
   Huang WB, 2023, IEEE T MOBILE COMPUT, V22, P5064, DOI 10.1109/TMC.2022.3174816
   Koniusz P, 2022, IEEE T PATTERN ANAL, V44, P648, DOI 10.1109/TPAMI.2021.3107160
   KRZANOWSKI WJ, 1988, BIOMETRICS, V44, P23, DOI 10.2307/2531893
   Kun Su, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9628, DOI 10.1109/CVPR42600.2020.00965
   Lee I, 2021, IEEE T MULTIMEDIA, V23, P415, DOI 10.1109/TMM.2020.2978637
   Leotta F, 2020, J AMB INTEL HUM COMP, V11, P1997, DOI 10.1007/s12652-019-01211-7
   Li B, 2019, AAAI CONF ARTIF INTE, P8561
   Li TJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13414, DOI 10.1109/ICCV48922.2021.01318
   Liao S., 2021, arXiv
   Lin LL, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2490, DOI 10.1145/3394171.3413548
   Lin YJ, 2021, PROC CVPR IEEE, P11169, DOI 10.1109/CVPR46437.2021.01102
   Liu GY, 2019, IEEE INT C INT ROBOT, P258, DOI [10.1109/IROS40897.2019.8967570, 10.1109/iros40897.2019.8967570]
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P2684, DOI 10.1109/TPAMI.2019.2916873
   Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50
   Liu MY, 2017, IEEE IMAGE PROC, P3670, DOI 10.1109/ICIP.2017.8296967
   Liu MY, 2017, PATTERN RECOGN, V68, P346, DOI 10.1016/j.patcog.2017.02.030
   Matake N, 2007, GECCO 2007: GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, VOL 1 AND 2, P861
   McConville R, 2021, INT C PATT RECOG, P5145, DOI 10.1109/ICPR48806.2021.9413131
   Miao SY, 2022, IEEE T CIRC SYST VID, V32, P4893, DOI 10.1109/TCSVT.2021.3124562
   Mohammadzade H, 2020, J VIS COMMUN IMAGE R, V66, DOI 10.1016/j.jvcir.2019.102691
   Niebles JC, 2008, INT J COMPUT VISION, V79, P299, DOI 10.1007/s11263-007-0122-4
   Nikpour B, 2021, IEEE SYS MAN CYBERN, P1056, DOI 10.1109/SMC52423.2021.9659047
   Ong W.-H., 2015, IEEJ Trans. Elect. Inform. Syst., V135, P1136
   Paoletti G, 2021, INT C PATT RECOG, P6035, DOI 10.1109/ICPR48806.2021.9412060
   Peng B, 2020, IEEE T IND INFORM, V16, P555, DOI 10.1109/TII.2019.2937514
   Perez M, 2022, PATTERN RECOGN, V122, DOI 10.1016/j.patcog.2021.108360
   Qu H., 2022, Advances in Intelligent Systems Research and Innovation, P87
   ROUSSEEUW PJ, 1987, J COMPUT APPL MATH, V20, P53, DOI 10.1016/0377-0427(87)90125-7
   Seddik B, 2017, IET COMPUT VIS, V11, P530, DOI 10.1049/iet-cvi.2016.0326
   Seidenari L, 2013, IEEE COMPUT SOC CONF, P479, DOI 10.1109/CVPRW.2013.77
   Sempena S., 2011, P 2011 INT C EL ENG, P1
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Shen XP, 2022, J VIS COMMUN IMAGE R, V82, DOI 10.1016/j.jvcir.2021.103386
   Shu XB, 2022, IEEE T PATTERN ANAL, V44, P3300, DOI 10.1109/TPAMI.2021.3050918
   Soomro K, 2017, IEEE I CONF COMP VIS, P696, DOI 10.1109/ICCV.2017.82
   Sugar CA, 2003, J AM STAT ASSOC, V98, P750, DOI 10.1198/016214503000000666
   Sung JY, 2012, IEEE INT CONF ROBOT, P842, DOI 10.1109/ICRA.2012.6224591
   Tang YS, 2020, IEEE T CIRC SYST VID, V30, P2872, DOI 10.1109/TCSVT.2020.2973301
   Thorndike RL, 1953, PSYCHOMETRIKA, DOI DOI 10.1007/BF02289263
   Tibshirani R, 2001, J ROY STAT SOC B, V63, P411, DOI 10.1111/1467-9868.00293
   van der Merwe D, 2003, IEEE C EVOL COMPUTAT, P215, DOI 10.1109/CEC.2003.1299577
   Van Gansbeke Wouter, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P268, DOI 10.1007/978-3-030-58607-2_16
   Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813
   Wang P, 2022, IEEE T IMAGE PROCESS, V31, P6224, DOI 10.1109/TIP.2022.3207577
   Wang T, 2022, IEEE T CIRC SYST VID, V32, P210, DOI 10.1109/TCSVT.2021.3057469
   Wen YH, 2019, AAAI CONF ARTIF INTE, P8989
   Wu D, 2014, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2014.98
   Xia L., 2012, P IEEE COMP SOC C CO, P20, DOI DOI 10.1109/CVPRW.2012.6239233
   Xu SG, 2023, IEEE T KNOWL DATA EN, V35, P12497, DOI 10.1109/TKDE.2023.3277839
   Yadav SK, 2021, KNOWL-BASED SYST, V223, DOI 10.1016/j.knosys.2021.106970
   Yang B, 2017, PR MACH LEARN RES, V70
   Yang SY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13403, DOI 10.1109/ICCV48922.2021.01317
   Zhang MC, 2023, SENSORS-BASEL, V23, DOI 10.3390/s23239529
   Zhang PF, 2017, IEEE I CONF COMP VIS, P2136, DOI [10.1109/ICCV.2017.233, 10.1109/ICCV.2017.231]
   Zheng NG, 2018, AAAI CONF ARTIF INTE, P2644
   Zhou LJ, 2020, IEEE T CIRC SYST VID, V30, P457, DOI 10.1109/TCSVT.2019.2890829
   Zhu YS, 2023, IEEE I CONF COMP VIS, P13867, DOI 10.1109/ICCV51070.2023.01279
   Zhu Y, 2014, IMAGE VISION COMPUT, V32, P453, DOI 10.1016/j.imavis.2014.04.005
NR 85
TC 0
Z9 0
U1 1
U2 1
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD MAY
PY 2024
VL 145
AR 104985
DI 10.1016/j.imavis.2024.104985
EA MAR 2024
PG 14
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA QI1J1
UT WOS:001220152600001
OA hybrid, Green Submitted
DA 2024-08-05
ER

PT J
AU Altamimi, A
   Alrowais, F
   Karamti, H
   Umer, M
   Cascone, L
   Ashraf, I
AF Altamimi, Abdulaziz
   Alrowais, Fadwa
   Karamti, Hanen
   Umer, Muhammad
   Cascone, Lucia
   Ashraf, Imran
TI An improved skin lesion detection solution using multi-step
   preprocessing features and NASNet transfer learning model
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Computer vision application; Computer-aided diagnosis; Skin lesion;
   Dermatology pigmented lesion classification; Multi-step image
   processing; NASNet model; Transfer learning
ID SURFACE MICROSCOPY; MELANOMA; IMAGES; DIAGNOSIS; DERMOSCOPY; AREAS
AB Computer -aided diagnosis has shown its potential for accurate detection of various diseases like skin lesion. Skin lesion has been recognized as a challenging task since manual identification through visual analysis of images can be inefficient, tedious, and error -prone. Although automatic diagnosis approaches are used to overcome this challenge, it is crucial to address problems such as variations in the size of images, presence of hairs in images, unsatisfactory schemes of colors, ruler markers, low -contrast, and differences in dimensions of lesions, and gel bubbles. Researchers in the field of dermatology pigmented lesion classification have proposed different methodologies to confront this issue. Specifically, they have focused on the binary classification problem of distinguishing Melanocytic lesions from normal ones. In this research, the dataset "MNIST HAM10000" is utilized, published by International Skin Image Collaboration, and contains data about 07 different skin cancer types. Moreover, in this research, we have focused on image preprocessing and skin lesion detection with NASNet model. Experimental reuslts demonstrated the superiority of the proposed model, which achieves an accuracy of 99.85%. This accomplishment has been made possible with the utilization of data augmentation techniques and multi -step image processing methods with the proposed NasNET model.
C1 [Altamimi, Abdulaziz] Univ Hafr Al Batin, Dept Comp Sci & Engn, Hafar al Batin 39524, Saudi Arabia.
   [Alrowais, Fadwa; Karamti, Hanen] Princess Nourah bint Abdulrahman Univ, Coll Comp & Informat Sci, Dept Comp Sci, POB 84428, Riyadh 11671, Saudi Arabia.
   [Umer, Muhammad] Islamia Univ Bahawalpur, Dept Comp Sci & Informat Technol, Bahawalpur, Pakistan.
   [Cascone, Lucia] Univ Salerno, Dept Comp Sci, Fisciano, Italy.
   [Ashraf, Imran] Yeungnam Univ, Dept Informat & Commun Engn, Gyongsan 38544, South Korea.
C3 Hafr Albatin University; Princess Nourah bint Abdulrahman University;
   Islamia University of Bahawalpur; University of Salerno; Yeungnam
   University
RP Ashraf, I (corresponding author), Yeungnam Univ, Dept Informat & Commun Engn, Gyongsan 38544, South Korea.
EM Dr.Altamimi@uhb.edu.sa; fmalrowais@pnu.edu.sa; hmkaramti@pnu.edu.sa;
   umer.sabir@iub.edu.pk; lcascone@unisa.it; imranashraf@ynu.ac.kr
RI Altamimi, Abdulaziz/KCZ-1113-2024
OI Altamimi, Abdulaziz/0000-0002-2990-8341
FU Princess Nourah Abdulrahman University, Riyadh, Saudi Arabia
   [PNURSP2024R77]
FX <B>Funding</B> Princess Nourah bint Abdulrahman University Researchers
   porting Project number (PNURSP2024R77) , Princess Nourah Abdulrahman
   University, Riyadh, Saudi Arabia.
CR Adegun A, 2021, ARTIF INTELL REV, V54, P811, DOI 10.1007/s10462-020-09865-y
   Akram Tallha, 2024, Journal of Ambient Intelligence and Humanized Computing, V15, P1083, DOI 10.1007/s12652-018-1051-5
   Alexandrov LB, 2020, NATURE, V578, P94, DOI 10.1038/s41586-020-1943-3
   Roldán FA, 2014, ACTAS DERMO-SIFILOGR, V105, P891, DOI 10.1016/j.ad.2013.11.015
   Ali ARA, 2012, PROC SPIE, V8318, DOI 10.1117/12.912389
   Alturki N, 2023, CANCERS, V15, DOI 10.3390/cancers15061767
   Argenziano G, 2001, LANCET ONCOL, V2, P443, DOI 10.1016/S1470-2045(00)00422-8
   Bachert SE, 2020, DIAGNOSTICS, V10, DOI 10.3390/diagnostics10020102
   Bergeron S, 2021, OCUL ONCOL PATHOL, V7, P149, DOI 10.1159/000511188
   Bissoto A., 2018, Deep-learning ensembles for skin-lesion segmentation, analysis, classification: RECOD titans at ISIC challenge 2018
   Codella Noel, 2015, Machine Learning in Medical Imaging. 6th International Workshop, MLMI 2015, held in conjunction with MICCAI 2015. Proceedings: LNCS 9352, P118, DOI 10.1007/978-3-319-24888-2_15
   Codella NCF, 2017, IBM J RES DEV, V61, DOI 10.1147/JRD.2017.2708299
   Codella N, 2019, Arxiv, DOI [arXiv:1902.03368, 10.48550/arXiv.1902.03368]
   Dhivyaa CR, 2020, J AMB INTEL HUM COMP, DOI 10.1007/s12652-020-02675-8
   Dunker S, 2021, NEW PHYTOL, V229, P593, DOI 10.1111/nph.16882
   Farahani A, 2021, NEURAL COMPUT APPL, V33, P6307, DOI 10.1007/s00521-020-05396-3
   Faziloglu Y, 2003, SKIN RES TECHNOL, V9, P147, DOI 10.1034/j.1600-0846.2003.00030.x
   Feit NE, 2004, BRIT J DERMATOL, V150, P706, DOI 10.1111/j.0007-0963.2004.05892.x
   Feng J, 2013, METABOLITES, V3, P1011, DOI 10.3390/metabo3041011
   Javed R, 2019, NETW MODEL ANAL HLTH, V9, DOI 10.1007/s13721-019-0209-1
   Jha D, 2020, Arxiv, DOI arXiv:2006.04868
   Juna A, 2022, WATER-SUI, V14, DOI 10.3390/w14172592
   Lopez AR, 2017, 2017 13TH IASTED INTERNATIONAL CONFERENCE ON BIOMEDICAL ENGINEERING (BIOMED), P49, DOI 10.2316/P.2017.852-053
   Lozano A, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-66926-6
   Mujahid M, 2022, DIAGNOSTICS, V12, DOI 10.3390/diagnostics12051280
   Nehal KS, 2002, MELANOMA RES, V12, P161, DOI 10.1097/00008390-200204000-00009
   Nisar Humaira, 2020, 2020 IEEE Conference on Open Systems (ICOS), P25, DOI 10.1109/ICOS50156.2020.9293657
   Panjehpour M, 2002, LASER SURG MED, V31, P367, DOI 10.1002/lsm.10125
   Pellacani G, 2004, MELANOMA RES, V14, P125, DOI 10.1097/00008390-200404000-00008
   Pellacani G, 2002, CLIN DERMATOL, V20, P222, DOI 10.1016/S0738-081X(02)00231-6
   Pour MP, 2020, EXPERT SYST APPL, V144, DOI 10.1016/j.eswa.2019.113129
   Razmjooy N, 2020, CURR MED IMAGING, V16, P781, DOI 10.2174/1573405616666200129095242
   Recalcati S, 2020, J EUR ACAD DERMATOL, V34, pE346, DOI 10.1111/jdv.16533
   Rey-Barroso L, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18051441
   Rohrbach DJ, 2014, ACAD RADIOL, V21, P263, DOI 10.1016/j.acra.2013.11.013
   Rosdi BA, 2011, SENSORS-BASEL, V11, P11357, DOI 10.3390/s111211357
   Ruini C., 2020, Optical Coherence Tomography for Patch Test Grading: A Prospective Study on its Use for Noninvasive Diagnosis of Allergic Contact Dermatitis
   Ruini C, 2021, SKIN RES TECHNOL, V27, P340, DOI 10.1111/srt.12949
   Sadeghi M, 2011, COMPUT MED IMAG GRAP, V35, P137, DOI 10.1016/j.compmedimag.2010.07.002
   Salim F, 2023, ELECTRONICS-SWITZ, V12, DOI 10.3390/electronics12143132
   Sforza G, 2012, IEEE T INSTRUM MEAS, V61, P1839, DOI 10.1109/TIM.2012.2192349
   Siegel RL, 2017, CA-CANCER J CLIN, V67, P7, DOI [10.3322/caac.21551, 10.3322/caac.20006, 10.3322/caac.21654, 10.3322/caac.21254, 10.3322/caac.21387, 10.3322/caac.20073, 10.3322/caac.21601, 10.3322/caac.21332]
   Stoecker WV, 2005, SKIN RES TECHNOL, V11, P179, DOI 10.1111/j.1600-0846.2005.00117.x
   Tarver T, 2012, J CONS HLTH INTERNET, V16, P366, DOI 10.1080/15398285.2012.701177
   Torre LA, 2018, CA-CANCER J CLIN, V68, P284, DOI 10.3322/caac.21456
   Tsang S.H., 2021, Review: Nasnet-Neural Architecture Search Network (Image Classification)
   Vasconcelos CN, 2020, PATTERN RECOGN LETT, V139, P95, DOI 10.1016/j.patrec.2017.11.005
   Wang L, 2019, WORLD J SURG ONCOL, V17, DOI 10.1186/s12957-019-1558-z
   Wang ZY, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9224890
   WHITE R, 1991, DERMATOL CLIN, V9, P695
   Sikkandar MY, 2021, J AMB INTEL HUM COMP, V12, P3245, DOI 10.1007/s12652-020-02537-3
   Yuan YD, 2017, Arxiv, DOI arXiv:1703.05165
   Zhen SH, 2020, FRONT ONCOL, V10, DOI 10.3389/fonc.2020.00680
   Zulfiqar F, 2023, BIOMED SIGNAL PROCES, V84, DOI 10.1016/j.bspc.2023.104777
NR 54
TC 1
Z9 1
U1 6
U2 6
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD APR
PY 2024
VL 144
AR 104969
DI 10.1016/j.imavis.2024.104969
EA MAR 2024
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA OX3K2
UT WOS:001210535400001
DA 2024-08-05
ER

PT J
AU Huang, X
   Zhan, YW
AF Huang, Xi
   Zhan, Yinwei
TI Multi-object tracking with adaptive measurement noise and information
   fusion
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Multi -object tracking; Data association; Kalman filter; Camera -motion
   -compensation
AB Multi -object tracking (MOT) is a challenging task in computer vision that aims to estimate the trajectories of multiple objects in a video sequence. Observation -Centric SORT (OCSORT) is a pure motion -based MOT algorithm that uses the Kalman filter as the motion model and three observation -centric techniques: Re -Update, Momentum and Recovery, to enhance the data association. However, OCSORT is limited by camera motion error, constant measurement noise and lack of appearance information. In this paper, we propose three methods to address these limitations and improve the performance of OCSORT. First, we use Enhanced Correlation Coefficient Maximization (ECC) to compensate for the camera motion between adjacent frames. Second, we adjust the measurement noise scale for the Kalman filter according to the detection confidence. Third, we introduce a deep visual feature model to extract appearance information and propose a method to effectively use both motion and appearance information. The proposed method first filters out the inappropriate appearance information based on motion information and then combines the filtered appearance information with the motion information by minimization. We evaluate our algorithm on three MOT benchmarks: MOT17, MOT20 and DanceTrack. The results show that our algorithm achieves state-of-the-art performance on all datasets, especially on DanceTrack, where the objects have highly nonlinear motion and frequent occlusion. Compared to OCSORT, our algorithm improves Higher Order Tracking Accuracy (HOTA) by 1.1%, 0.8%, and 3.5%, and ID F1 Score (IDF1) by 1.7%, 1.9%, and 4.3% on MOT17, MOT20 and DanceTrack, respectively.
C1 [Huang, Xi; Zhan, Yinwei] Guangdong Univ Technol, Sch Comp Sci & Technol, Guangzhou Higher Educ Megactr, 100 Waihuanxi Rd, Guangzhou 510006, Guangdong, Peoples R China.
C3 Guangdong University of Technology
RP Zhan, YW (corresponding author), Guangdong Univ Technol, Sch Comp Sci & Technol, Guangzhou Higher Educ Megactr, 100 Waihuanxi Rd, Guangzhou 510006, Guangdong, Peoples R China.
EM ywzhan@gdut.edu.cn
FU National Natural Science Foundation of China (NSFC) [62272108]
FX <B>Acknowledgement</B> This work was sponsored by National Natural
   Science Foundation of China (NSFC) Grant No.62272108.
CR Aharon N, 2022, Arxiv, DOI [arXiv:2206.14651, DOI 10.48550/ARXIV.2206.14651]
   Bergmann P, 2019, IEEE I CONF COMP VIS, P941, DOI 10.1109/ICCV.2019.00103
   Bernardin K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/246309
   Bewley A, 2016, IEEE IMAGE PROC, P3464, DOI 10.1109/ICIP.2016.7533003
   Bradski G, 2000, DR DOBBS J, V25, P120
   Cai JR, 2022, PROC CVPR IEEE, P8080, DOI 10.1109/CVPR52688.2022.00792
   Cao JK, 2023, PROC CVPR IEEE, P9686, DOI 10.1109/CVPR52729.2023.00934
   Chu P, 2023, IEEE WINT CONF APPL, P4859, DOI 10.1109/WACV56688.2023.00485
   Dendorfer P., 2020, arXiv
   Du YH, 2023, IEEE T MULTIMEDIA, V25, P8725, DOI 10.1109/TMM.2023.3240881
   Du YH, 2021, IEEE INT CONF COMP V, P2809, DOI 10.1109/ICCVW54120.2021.00315
   Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667
   Evangelidis GD, 2008, IEEE T PATTERN ANAL, V30, P1858, DOI 10.1109/TPAMI.2008.113
   Fang K, 2018, IEEE WINT CONF APPL, P466, DOI 10.1109/WACV.2018.00057
   Ge Z, 2021, Arxiv, DOI arXiv:2107.08430
   Han SD, 2022, NEUROCOMPUTING, V476, P75, DOI 10.1016/j.neucom.2021.12.104
   He JW, 2021, PROC CVPR IEEE, P5295, DOI 10.1109/CVPR46437.2021.00526
   He ZY, 2016, IEEE T IMAGE PROCESS, V25, P3698, DOI 10.1109/TIP.2016.2570553
   Kalman R.E., 1960, Bol. soc. mat. mexicana, V5, P102
   Khurana Tarasha, 2021, ICCV, P3174
   Kuhn HW, 2005, NAV RES LOG, V52, P7, DOI 10.1002/nav.20053
   Li W., 2021, arXiv
   Liang C, 2022, IEEE T IMAGE PROCESS, V31, P3182, DOI 10.1109/TIP.2022.3165376
   Luiten J, 2021, INT J COMPUT VISION, V129, P548, DOI 10.1007/s11263-020-01375-2
   Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190
   Mandel T, 2023, PATTERN RECOGN, V135, DOI 10.1016/j.patcog.2022.109107
   Meinhardt T, 2022, PROC CVPR IEEE, P8834, DOI 10.1109/CVPR52688.2022.00864
   Milan A, 2016, Arxiv, DOI arXiv:1603.00831
   Pang JM, 2021, PROC CVPR IEEE, P164, DOI 10.1109/CVPR46437.2021.00023
   Psarakis EZ, 2005, IEEE I CONF COMP VIS, P907
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Stadler D, 2022, IEEE WINT CONF APPL, P133, DOI 10.1109/WACVW54805.2022.00019
   Sun PZ, 2022, PROC CVPR IEEE, P20961, DOI 10.1109/CVPR52688.2022.02032
   Sun PZ, 2021, Arxiv, DOI arXiv:2012.15460
   Sun ZH, 2021, IEEE T CIRC SYST VID, V31, P1819, DOI 10.1109/TCSVT.2020.3009717
   Tokmakov P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10840, DOI 10.1109/ICCV48922.2021.01068
   Vaswani A., 2024, Adv. Neural Inf. Proces. Syst., P30
   Wang J, 2024, IEEE T MULTIMEDIA, V26, P326, DOI 10.1109/TMM.2023.3264851
   Wang S, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13199, DOI 10.1109/ICCV48922.2021.01297
   Wang YX, 2021, IEEE INT CONF ROBOT, P13708, DOI 10.1109/ICRA48506.2021.9561110
   Wang YY, 2023, KNOWL-BASED SYST, V265, DOI 10.1016/j.knosys.2023.110380
   Wojke N, 2017, IEEE IMAGE PROC, P3645, DOI 10.1109/ICIP.2017.8296962
   Wu JL, 2021, PROC CVPR IEEE, P12347, DOI 10.1109/CVPR46437.2021.01217
   Xingyi Zhou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P474, DOI 10.1007/978-3-030-58548-8_28
   Xu Y, 2021, ARXIV
   Yan B, 2022, LECT NOTES COMPUT SC, V13681, P733, DOI 10.1007/978-3-031-19803-8_43
   Zeng FG, 2022, LECT NOTES COMPUT SC, V13687, P659, DOI 10.1007/978-3-031-19812-0_38
   Zhang H, 2022, IEEE COMPUT SOC CONF, P2735, DOI 10.1109/CVPRW56347.2022.00309
   Zhang YF, 2022, LECT NOTES COMPUT SC, V13682, P1, DOI 10.1007/978-3-031-20047-2_1
   Zhang YF, 2021, INT J COMPUT VISION, V129, P3069, DOI 10.1007/s11263-021-01513-4
   Zhongdao Wang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P107, DOI 10.1007/978-3-030-58621-8_7
   Zhou XY, 2022, PROC CVPR IEEE, P8761, DOI 10.1109/CVPR52688.2022.00857
NR 52
TC 0
Z9 0
U1 4
U2 4
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD APR
PY 2024
VL 144
AR 104964
DI 10.1016/j.imavis.2024.104964
EA MAR 2024
PG 10
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA NK4D6
UT WOS:001200325600001
DA 2024-08-05
ER

PT J
AU Ciranni, M
   Murino, V
   Odone, F
   Pastore, VP
AF Ciranni, Massimiliano
   Murino, Vittorio
   Odone, Francesca
   Pastore, Vito Paolo
TI Computer vision and deep learning meet plankton: Milestones and future
   directions
SO IMAGE AND VISION COMPUTING
LA English
DT Review
DE Plankton image analysis; Deep learning; Computer vision; Image
   classification; Object detection; Anomaly detection; Transfer learning
ID SILHOUETTE PHOTOGRAPHY; CLASSIFICATION; PHYTOPLANKTON; SYSTEM;
   RECOGNITION; ABUNDANCE; IMAGES; POWER
AB Planktonic organisms play a pivotal role within aquatic ecosystems, serving as the foundation of the aquatic food chain while also playing a critical role in climate regulation and the production of oxygen. In recent years, the advent of automated systems for capturing in-situ images has led to a huge influx of plankton images, making manual classification impractical. This, at the same time, has opened up opportunities for the application of machine learning and deep learning solutions. This paper undertakes an extensive analysis of the broad range of computer vision techniques and methodologies that have emerged to facilitate the automatic analysis of small- to large-scale datasets containing plankton images. By focusing on different computer vision tasks, we present findings and limitations in order to offer a comprehensive overview of the current state-of-the-art, while also pinpointing the open challenges that demand further research and attention.
C1 [Ciranni, Massimiliano; Odone, Francesca; Pastore, Vito Paolo] Univ Genoa, MaLGa, Genoa, Italy.
   [Ciranni, Massimiliano; Murino, Vittorio; Odone, Francesca; Pastore, Vito Paolo] Univ Genoa, DIBRIS, Genoa, Italy.
   [Murino, Vittorio] Univ Verona, Verona, Italy.
C3 University of Genoa; University of Genoa; University of Verona
RP Pastore, VP (corresponding author), Univ Genoa, MaLGa, Genoa, Italy.; Pastore, VP (corresponding author), Univ Genoa, DIBRIS, Genoa, Italy.
EM Vito.Paolo.Pastore@unige.it
OI PASTORE, VITO PAOLO/0000-0002-5827-5571; Ciranni,
   Massimiliano/0009-0001-3728-9640
CR Alexander S, 2000, P SOC PHOTO-OPT INS, V4076, P111, DOI 10.1117/12.397939
   Alfano PD, 2024, IMAGE VISION COMPUT, V142, DOI 10.1016/j.imavis.2023.104894
   Alfano PD, 2022, INT C PATT RECOG, P1314, DOI 10.1109/ICPR56361.2022.9956360
   Bank D., 2023, Machine Learning for Data Science Handbook: Data Mining and Knowledge Discovery Handbook, P353, DOI [10.1007/978-3-031-24628-916, DOI 10.1007/978-3-031-24628-916]
   Bergum S, 2020, OCEANS-IEEE, DOI 10.1109/IEEECONF38699.2020.9389377
   BEZDEK JC, 1984, COMPUT GEOSCI, V10, P191, DOI 10.1016/0098-3004(84)90020-7
   Blackburn N, 1998, APPL ENVIRON MICROB, V64, P3246
   Blaschko MB, 2005, WACV 2005: SEVENTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P79
   Boyce DG, 2010, NATURE, V466, P591, DOI 10.1038/nature09268
   Brierley AS, 2017, CURR BIOL, V27, pR478, DOI 10.1016/j.cub.2017.02.045
   Bucak SS, 2014, IEEE T PATTERN ANAL, V36, P1354, DOI 10.1109/TPAMI.2013.212
   Caron M, 2018, LECT NOTES COMPUT SC, V11218, P139, DOI 10.1007/978-3-030-01264-9_9
   Chavez FP, 2011, ANNU REV MAR SCI, V3, P227, DOI 10.1146/annurev.marine.010908.163917
   Chen T, 2021, ONCOGENE, V40, P2756, DOI 10.1038/s41388-021-01739-z
   Chen W, 2023, IEEE T PATTERN ANAL, V45, P7270, DOI 10.1109/TPAMI.2022.3218591
   Chen YQ, 2002, BIOCHEM SYST ECOL, V30, P15, DOI 10.1016/S0305-1978(01)00054-0
   Cheng Kaichang, 2019, Figshare, DOI 10.6084/m9.figshare.8146283.v3
   Cheng KC, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0219570
   Ciranni M, 2024, FRONT MAR SCI, V10, DOI 10.3389/fmars.2023.1283265
   Comaniciu D., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1197, DOI 10.1109/ICCV.1999.790416
   Cowen R.K., 2015, Planktonset 1.0: Plankton imagery data collected from f.g. walton smith in straits of florida from 2014-06-03 to 2014-06-06 and used in the 2015 national data science bowl (ncei accession 0127422), DOI [10.7289/V5D21VJD.URL, DOI 10.7289/V5D21VJD.URL]
   Cowen RK, 2008, LIMNOL OCEANOGR-METH, V6, P126, DOI 10.4319/lom.2008.6.126
   Cowen Robert K, 2015, NCEI
   Cui J, 2018, 2018 OCEANS MTSIEEE, P15
   Culverhouse P. F., 2003, International Conference on Visual Information Engineering (VIE 2003) (IEE Conf. Publ.No.495), P177, DOI 10.1049/cp:20030516
   Culverhouse PF, 1996, MAR ECOL PROG SER, V139, P281, DOI 10.3354/meps139281
   CULVERHOUSE PF, 1994, MAR ECOL PROG SER, V107, P273, DOI 10.3354/meps107273
   Dai J., 2016, OCEANS 2016 SHANGHAI, P1, DOI [DOI 10.1109/OCEANSAP.2016.7485680, 10.1109/OCEANSAP. 2016.7485680]
   Dai JL, 2017, LECT NOTES COMPUT SC, V10118, P102, DOI 10.1007/978-3-319-54526-4_8
   Davis C.S., 1992, Advances in Limnology, V36, P67
   Ding LJ, 2001, PATTERN RECOGN, V34, P721, DOI 10.1016/S0031-3203(00)00023-6
   Dony R., 2001, The Transform and Data Compression Handbook, V1, P29
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Edgerton HE, 1955, J SOC MOTION PICT T, V64, P345
   Elineau Amanda, 2018, SEANOE, DOI 10.17882/55741
   Ellen JS, 2019, LIMNOL OCEANOGR-METH, V17, P439, DOI 10.1002/lom3.10324
   Falkowski P, 2012, NATURE, V483, pS17, DOI 10.1038/483S17a
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Field CB, 1998, SCIENCE, V281, P237, DOI 10.1126/science.281.5374.237
   Froese R., 1990, ICES CM Documents, P1
   Gilad-Bachrach Ran, 2004, P 21 INT C MACH LEAR, P43, DOI [10.1145/1015330.1015352, DOI 10.1145/1015330.1015352]
   González P, 2017, LIMNOL OCEANOGR-METH, V15, P221, DOI 10.1002/lom3.10151
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Gorsky G, 2000, ESTUAR COAST SHELF S, V50, P121, DOI 10.1006/ecss.1999.0539
   Gorsky G., 2003, GLOBEC Int. Newslett., V9
   Gorsky G, 2010, J PLANKTON RES, V32, P285, DOI 10.1093/plankt/fbp124
   Grosjean P, 2004, ICES J MAR SCI, V61, P518, DOI 10.1016/j.icesjms.2004.03.012
   Guo BY, 2021, LIMNOL OCEANOGR-METH, V19, P21, DOI 10.1002/lom3.10402
   Guo J, 2023, IET COMPUT VIS, V17, P111, DOI 10.1049/cvi2.12137
   HARALICK RM, 1987, IEEE T PATTERN ANAL, V9, P532, DOI 10.1109/TPAMI.1987.4767941
   Hays GC, 2005, TRENDS ECOL EVOL, V20, P337, DOI 10.1016/j.tree.2005.03.004
   He KM, 2015, Arxiv, DOI [arXiv:1512.03385, DOI 10.48550/ARXIV.1512.03385]
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Hearst MA, 1998, IEEE INTELL SYST APP, V13, P18, DOI 10.1109/5254.708428
   Hendrycks D, 2018, P INT C LEARN REPR
   Hestness J, 2019, PROCEEDINGS OF THE 24TH SYMPOSIUM ON PRINCIPLES AND PRACTICE OF PARALLEL PROGRAMMING (PPOPP '19), P1, DOI 10.1145/3293883.3295710
   Ho TK, 1998, IEEE T PATTERN ANAL, V20, P832, DOI 10.1109/34.709601
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu Q, 2006, MAR ECOL PROG SER, V306, P51, DOI 10.3354/meps306051
   Hu Q, 2005, MAR ECOL PROG SER, V295, P21, DOI 10.3354/meps295021
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   KAGGLE, 2014, National Data Science Bowl
   Kang D, 2022, PROC CVPR IEEE, P9969, DOI 10.1109/CVPR52688.2022.00974
   Karlusich JJP, 2022, FRONT MAR SCI, V9, DOI 10.3389/fmars.2022.878803
   KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570
   Kerr T, 2020, IEEE ACCESS, V8, P170013, DOI 10.1109/ACCESS.2020.3022242
   Khosla P., 2020, Adv. Neural Inf. Process. Syst, P18661, DOI DOI 10.48550/ARXIV.2004.11362
   Koch G., 2015, ICML DEEP LEARN WORK, V2, P1
   Kohonen T., 1995, Learning vector quantization: self-organizing maps, P175, DOI DOI 10.1007/978-3-642-97610-0_6
   Krizhevsky A, 2009, CIFAR-10 dataset
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694
   Kuzminykh Denis, 2018, P MACHINE LEARNING R, V95, P438
   Kyathanahally SP, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-21910-0
   Kyathanahally SP, 2021, FRONT MICROBIOL, V12, DOI 10.3389/fmicb.2021.746297
   Lee H, 2016, IEEE IMAGE PROC, P3713, DOI 10.1109/ICIP.2016.7533053
   Li D, 2019, IEEE I CONF COMP VIS, P1446, DOI 10.1109/ICCV.2019.00153
   Li Q, 2020, ICES J MAR SCI, V77, P1427, DOI 10.1093/icesjms/fsz171
   Li X, 2016, OCEANS 2016 MTS/IEEE MONTEREY, DOI 10.1109/OCEANS.2016.7761223
   Li YL, 2020, IEEE IC COMP COM NET, DOI 10.1109/icccn49398.2020.9209626
   Li ZF, 2014, IEEE J OCEANIC ENG, V39, P695, DOI 10.1109/JOE.2013.2280035
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Lombard F, 2019, FRONT MAR SCI, V6, DOI 10.3389/fmars.2019.00196
   Lumini A, 2019, ECOL INFORM, V51, P33, DOI 10.1016/j.ecoinf.2019.02.007
   Luo T, 2005, J MACH LEARN RES, V6, P589
   Luo T, 2004, IEEE T SYST MAN CY B, V34, P1753, DOI 10.1109/TSMCB.2004.830340
   Luo T, 2003, IEEE SYS MAN CYBERN, P888
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Maracani A, 2023, SCI REP-UK, V13, DOI 10.1038/s41598-023-37627-7
   Matas J, 2004, IMAGE VISION COMPUT, V22, P761, DOI 10.1016/j.imavis.2004.02.006
   McInnes L, 2017, INT CONF DAT MIN WOR, P33, DOI 10.1109/ICDMW.2017.12
   Merz E, 2021, WATER RES, V203, DOI 10.1016/j.watres.2021.117524
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Nishizawa S., 1954, Bulletin of the Faculty of Fisheries Hokkaido, V5, P36
   Olson RJ, 2007, LIMNOL OCEANOGR-METH, V5, P195, DOI 10.4319/lom.2007.5.195
   Orenstein EC, 2020, LIMNOL OCEANOGR-METH, V18, P739, DOI 10.1002/lom3.10399
   Orenstein EC, 2020, LIMNOL OCEANOGR-METH, V18, P681, DOI 10.1002/lom3.10394
   Orenstein EC, 2017, IEEE WINT CONF APPL, P1082, DOI 10.1109/WACV.2017.125
   ORTNER PB, 1979, NATURE, V277, P50, DOI 10.1038/277050a0
   ORTNER PB, 1981, DEEP-SEA RES, V28, P1569, DOI 10.1016/0198-0149(81)90098-4
   Ouyang P, 2016, 2016 IEEE INFORMATION TECHNOLOGY, NETWORKING, ELECTRONIC AND AUTOMATION CONTROL CONFERENCE (ITNEC), P132, DOI 10.1109/ITNEC.2016.7560334
   Panaiotis T, 2022, FRONT MAR SCI, V9, DOI 10.3389/fmars.2022.870005
   Pastore VP, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-68662-3
   Pastore VP, 2019, PROC SPIE, V10881, DOI 10.1117/12.2511065
   Pastore VP, 2023, IMAGE VISION COMPUT, V137, DOI 10.1016/j.imavis.2023.104764
   Pastore VP, 2022, LECT NOTES COMPUT SC, V13232, P599, DOI 10.1007/978-3-031-06430-2_50
   Polikar R, 2012, ENSEMBLE MACHINE LEARNING: METHODS AND APPLICATIONS, P1, DOI 10.1007/978-1-4419-9326-7_1
   Pu YC, 2021, IEEE INT CONF COMP V, P3654, DOI 10.1109/ICCVW54120.2021.00409
   Qi H, 2018, PROC CVPR IEEE, P5822, DOI 10.1109/CVPR.2018.00610
   Redmon J., 2018, CoRR
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Richardson AJ, 2004, SCIENCE, V305, P1609, DOI 10.1126/science.1100958
   Ridnik Tal, 2021, P NEUR INF PROC SYST, V1
   Rivas-Villar D, 2021, COMPUT METH PROG BIO, V200, DOI 10.1016/j.cmpb.2020.105923
   Salvesen E, 2022, PROC SPIE, V12084, DOI 10.1117/12.2622489
   Salvesen E, 2020, OCEANS-IEEE, DOI 10.1109/IEEECONF38699.2020.9389188
   Samson S, 2001, IEEE J OCEANIC ENG, V26, P671, DOI 10.1109/48.972110
   Schroder R., 1961, Untersuchungen Uber Die Planktonverteilung, Mit Hilfe Der Unterwasser-Fernsehanlage und Des Echographen
   Schroder S.-M., 2018, GERM C PATT REC SPRI, P391, DOI 10.1007/978-3-030-12939-2_27
   Schröder SM, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20113060
   Schubert E, 2017, ACM T DATABASE SYST, V42, DOI 10.1145/3068335
   Settles B., 2009, ACTIVE LEARNING LIT
   Sieracki CK, 1998, MAR ECOL PROG SER, V168, P285, DOI 10.3354/meps168285
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Snell J, 2017, ADV NEUR IN, V30
   Sosik HM, 2007, LIMNOL OCEANOGR-METH, V5, P204, DOI 10.4319/lom.2007.5.204
   Sosik P.E.E., 2015, WHOI-Plankton, annotated plankton images-data set for developing and evaluating classification methods, DOI [10.1575/1912/7341, DOI 10.1575/1912/7341]
   SOURNIA A, 1991, J PLANKTON RES, V13, P1093, DOI 10.1093/plankt/13.5.1093
   Strehl A, 2002, EIGHTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-02)/FOURTEENTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-02), PROCEEDINGS, P93, DOI 10.1162/153244303321897735
   Szegedy C, 2014, Arxiv, DOI arXiv:1409.4842
   Tang XO, 2006, IEEE J OCEANIC ENG, V31, P728, DOI 10.1109/JOE.2004.836995
   Tang XO, 1998, ARTIF INTELL REV, V12, P177, DOI 10.1023/A:1006517211724
   Taylor AH, 2002, NATURE, V416, P629, DOI 10.1038/416629a
   Teigen AL, 2020, OCEANS-IEEE, DOI 10.1109/IEEECONF38699.2020.9388998
   Theiler J, 2003, P SOC PHOTO-OPT INS, V5093, P230, DOI 10.1117/12.487069
   Tseng Hung-Yu, 2019, INT C LEARN REPR
   van den Oord A., 2014, Classifying plankton with deep neural networks-benanne.github. io
   Vinyals O, 2016, 30 C NEURAL INFORM P, V29
   Walker JL, 2021, IEEE INT CONF COMP V, P3665, DOI 10.1109/ICCVW54120.2021.00410
   Wang C, 2017, IEEE IMAGE PROC, P855, DOI 10.1109/ICIP.2017.8296402
   Wang CW, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351100
   Wang YQ, 2020, ACM COMPUT SURV, V53, DOI 10.1145/3386252
   Wertheimer D, 2019, PROC CVPR IEEE, P6551, DOI 10.1109/CVPR.2019.00672
   Wong TT, 2015, PATTERN RECOGN, V48, P2839, DOI 10.1016/j.patcog.2015.03.009
   Xu XY, 2011, PATTERN RECOGN LETT, V32, P956, DOI 10.1016/j.patrec.2011.01.021
   Yang XL, 2023, IEEE T KNOWL DATA EN, V35, P8934, DOI 10.1109/TKDE.2022.3220219
   Yang ZY, 2022, ICES J MAR SCI, V79, P2643, DOI 10.1093/icesjms/fsac198
   Zhang Si, 2019, Comput Soc Netw, V6, P11, DOI 10.1186/s40649-019-0069-y
   Zhao F, 2010, NEUROCOMPUTING, V73, P1853, DOI 10.1016/j.neucom.2009.12.033
   Zheng HY, 2017, BMC BIOINFORMATICS, V18, DOI 10.1186/s12859-017-1954-8
NR 155
TC 0
Z9 0
U1 11
U2 11
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD MAR
PY 2024
VL 143
AR 104934
DI 10.1016/j.imavis.2024.104934
EA FEB 2024
PG 21
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA NE2W6
UT WOS:001198722100001
OA hybrid
DA 2024-08-05
ER

PT J
AU Tian, J
   Sun, D
   Gao, QW
   Lu, YX
   Bao, MX
   Zhu, D
   Zhao, DW
AF Tian, Jia
   Sun, Dong
   Gao, Qingwei
   Lu, Yixiang
   Bao, Muxi
   Zhu, De
   Zhao, Dawei
TI A novel infrared and visible image fusion algorithm based on global
   information-enhanced attention network
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Image fusion; Attention mechanism; Global information; Feature
   enhancement
AB The fusion of infrared and visible images aims to extract and fuse thermal target information and texture details to the fullest extent possible, enhancing the visual understanding capabilities of images for both humans and computers in complex scenes. However, existing methods have difficulties in preserving the comprehensiveness of source image feature information and enhancing the saliency of image texture information. Therefore, we put forward a novel infrared and visible image fusion algorithm based on global information-enhanced attention network (GIEA). Specifically, we develop an attention-guided Transformer module (AGTM) to make sure the fused images have enough global information. This module combines the convolutional neural network and Transformer to perform adequate feature extraction from shallow to deep layers, and utilize the attention network for multi-level feature-guided learning. Then, we build the contrast enhancement module (CENM), which enhances the feature representation and contrast of the image so that the fused image contains significant texture information. Furthermore, our network is driven to fully preserve the texture and structure details of the source images with a loss function that consists of content loss and total variance loss. Numerous experiments demonstrate that our fusion approach outperforms other fusion approaches in both subjective and objective assessments.
C1 [Tian, Jia; Sun, Dong; Gao, Qingwei; Lu, Yixiang; Bao, Muxi; Zhu, De; Zhao, Dawei] Anhui Univ, Hefei 230601, Peoples R China.
C3 Anhui University
RP Sun, D (corresponding author), Anhui Univ, Hefei 230601, Peoples R China.
EM z22301108@stu.ahu.edu.cn; sundong@ahu.edu.cn; qingweigao@ahu.edu.cn;
   lyxahu@ahu.edu.cn; 3086636249@qq.com; zhude@ahu.edu.cn;
   daweizhao@ahu.edu.cn
FU National Natural Science Foundation of China [62071001]; Nature Science
   Foundation of Anhui [2308085QF224]; China Postdoctoral Sci- ence
   Foundation [2023M730009]
FX This work is supported by the National Natural Science Foundation of
   China (No. 62071001) , the Nature Science Foundation of Anhui (No.
   2308085QF224) , and is also supported by the China Postdoctoral Sci-
   ence Foundation (No. 2023M730009) .
CR Awad M, 2020, IEEE T COMPUT IMAG, V6, P408, DOI 10.1109/TCI.2019.2956873
   Chen J, 2020, INFORM SCIENCES, V508, P64, DOI 10.1016/j.ins.2019.08.066
   Cui GM, 2015, OPT COMMUN, V341, P199, DOI 10.1016/j.optcom.2014.12.032
   Cvejic N, 2007, IEEE SENS J, V7, P743, DOI 10.1109/JSEN.2007.894926
   Eskicioglu AM, 1995, IEEE T COMMUN, V43, P2959, DOI 10.1109/26.477498
   Haibo Zhao, 2021, 2021 International Conference on Information Technology and Biomedical Engineering (ICITBE), P71, DOI 10.1109/ICITBE54178.2021.00025
   Han Y, 2013, INFORM FUSION, V14, P127, DOI 10.1016/j.inffus.2011.08.002
   Hu HM, 2017, IEEE T MULTIMEDIA, V19, P2706, DOI 10.1109/TMM.2017.2711422
   Jha A, 2023, IEEE WINT CONF APPL, P6343, DOI 10.1109/WACV56688.2023.00629
   Jian LH, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3022438
   Li HF, 2024, INT J COMPUT VISION, V132, P1625, DOI 10.1007/s11263-023-01948-x
   Li H, 2024, INFORM FUSION, V103, DOI 10.1016/j.inffus.2023.102147
   Li H, 2023, IEEE T PATTERN ANAL, V45, P11040, DOI 10.1109/TPAMI.2023.3268209
   Li H, 2020, IEEE T IMAGE PROCESS, V29, P4733, DOI 10.1109/TIP.2020.2975984
   Li H, 2019, IEEE T IMAGE PROCESS, V28, P2614, DOI 10.1109/TIP.2018.2887342
   Li YH, 2023, DIGIT SIGNAL PROCESS, V134, DOI 10.1016/j.dsp.2023.103910
   Liu JY, 2024, INT J COMPUT VISION, V132, P1748, DOI 10.1007/s11263-023-01952-1
   Liu JY, 2022, PROC CVPR IEEE, P5792, DOI 10.1109/CVPR52688.2022.00571
   Liu JY, 2022, IEEE T CIRC SYST VID, V32, P105, DOI 10.1109/TCSVT.2021.3056725
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Long YZ, 2021, INFORM FUSION, V69, P128, DOI 10.1016/j.inffus.2020.11.009
   Ma JY, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3038013
   Ma JY, 2022, IEEE-CAA J AUTOMATIC, V9, P1200, DOI 10.1109/JAS.2022.105686
   Ma JY, 2020, IEEE T IMAGE PROCESS, V29, P4980, DOI 10.1109/TIP.2020.2977573
   Ma JY, 2020, INFORM FUSION, V54, P85, DOI 10.1016/j.inffus.2019.07.005
   Ma JY, 2019, INFORM FUSION, V48, P11, DOI 10.1016/j.inffus.2018.09.004
   Ma JY, 2016, INFORM FUSION, V31, P100, DOI 10.1016/j.inffus.2016.02.001
   Ma JL, 2017, INFRARED PHYS TECHN, V82, P8, DOI 10.1016/j.infrared.2017.02.005
   Rao Dongyu, 2023, IEEE Trans Image Process, VPP, DOI 10.1109/TIP.2023.3273451
   Rao YJ, 1997, MEAS SCI TECHNOL, V8, P355, DOI 10.1088/0957-0233/8/4/002
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Roberts JW, 2008, J APPL REMOTE SENS, V2, DOI 10.1117/1.2945910
   Sharma V., 2017, COL IM C SOC IM SCI, V2017, P330
   Tang HJ, 2022, INFRARED PHYS TECHN, V127, DOI 10.1016/j.infrared.2022.104435
   Tang LF, 2023, INFORM FUSION, V91, P477, DOI 10.1016/j.inffus.2022.10.034
   Tang LF, 2022, INFORM FUSION, V83, P79, DOI 10.1016/j.inffus.2022.03.007
   Tang LF, 2022, INFORM FUSION, V82, P28, DOI 10.1016/j.inffus.2021.12.004
   Tang W, 2023, IEEE T CIRC SYST VID, V33, P3159, DOI 10.1109/TCSVT.2023.3234340
   Tang W, 2023, IEEE T MULTIMEDIA, V25, P5413, DOI 10.1109/TMM.2022.3192661
   Toet Alexander, 2014, Figshare
   Vibashan VS, 2022, IEEE IMAGE PROC, P3566, DOI 10.1109/ICIP46576.2022.9897280
   Wang D, 2023, INFORM FUSION, V98, DOI 10.1016/j.inffus.2023.101828
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Xu H, 2021, IEEE T COMPUT IMAG, V7, P824, DOI 10.1109/TCI.2021.3100986
   Xu H, 2022, IEEE T PATTERN ANAL, V44, P502, DOI 10.1109/TPAMI.2020.3012548
   Yang B, 2016, INT J WAVELETS MULTI, V14, DOI 10.1142/S0219691316500247
   Zhang H, 2020, AAAI CONF ARTIF INTE, V34, P12797
   Zhang ZJ, 2019, LECT NOTES COMPUT SC, V11764, P442, DOI 10.1007/978-3-030-32239-7_49
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhao ZX, 2023, PROC CVPR IEEE, P5906, DOI 10.1109/CVPR52729.2023.00572
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
NR 51
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD SEP
PY 2024
VL 149
AR 105161
DI 10.1016/j.imavis.2024.105161
EA JUL 2024
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA YH7G4
UT WOS:001267657700001
DA 2024-08-05
ER

PT J
AU Jin, HL
   Li, HY
AF Jin, Hailong
   Li, Huiying
TI An enhanced approach for few-shot segmentation via smooth downsampling
   mask and label smoothing loss
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Few-shot segmentation; Segmentation; Few-shot learning; Deep learning
ID AGGREGATION
AB Few-shot semantic segmentation aims to segment new categories with only a small number of annotated images. Previous methods mainly focused on exploiting the pixel-level correlation between the support image and the query image, combined with attention-based methods, resulting in significant advancements. In this paper, we introduce a new perspective to enhance few-shot segmentation. We identify that utilizing the bilinear interpolation method to downsample the mask leads to the loss of fine-grained information from the target features. To address this issue, we propose a Smooth Downsampling Mask (SDM) method. The SDM method is designed to retain more effective target semantic features by employing a cascaded downsampling approach with a smooth kernel for mask processing. Additionally, we propose a label smoothing loss to further enhance the performance, which provides direct guidance for low-resolution feature map optimization. Both methods can be used as plugand -play modules for existing methods. Notably, our proposed method does not involve additional learnable parameters and is computationally efficient, thus achieving painless gains. To validate the effectiveness of our method, we take three publicly available models as baselines and conduct extensive experiments on three public benchmarks PASCAL -5 i , COCO-20 i and FSS-1000, and achieve considerable improvement.
C1 [Jin, Hailong; Li, Huiying] Jilin Univ, Coll Comp Sci & Technol, Changchun, Jilin, Peoples R China.
   [Jin, Hailong; Li, Huiying] Jilin Univ, Key Lab Symbol Computat & Knowledge Engn, Changchun 130012, Peoples R China.
C3 Jilin University; Jilin University
RP Li, HY (corresponding author), Jilin Univ, Coll Comp Sci & Technol, Changchun, Jilin, Peoples R China.
EM lihuiying@jlu.edu.cn
FU JiLin Scientific and Technological Development Program, China
   [20230201089GX, 20210201138GX]
FX This study was supported by JiLin Scientific and Technological
   Development Program, China (Grant No. 20230201089GX and 20210201138GX) .
CR Chen L.-C., 2018, P EUR C COMP VIS ECC, P801, DOI DOI 10.1007/978-3-030-01234-2_49
   Chen XK, 2021, PROC CVPR IEEE, P2613, DOI 10.1109/CVPR46437.2021.00264
   Cheng G, 2023, IEEE T PATTERN ANAL, V45, P4650, DOI 10.1109/TPAMI.2022.3193587
   Cheng H, 2023, IEEE I CONF COMP VIS, P11780, DOI 10.1109/ICCV51070.2023.01085
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding HH, 2023, PATTERN RECOGN, V133, DOI 10.1016/j.patcog.2022.109018
   Dong N., 2018, BMVC, V3, P1
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fan Q, 2022, LECT NOTES COMPUT SC, V13679, P701, DOI 10.1007/978-3-031-19800-7_41
   Fan Z., 2020, IEEE C COMPUT VIS PA, P9172, DOI [10.1109/cvpr42600.2020.00919, DOI 10.1109/CVPR42600.2020.00919]
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Han MY, 2024, IEEE T MULTIMEDIA, V26, P2408, DOI 10.1109/TMM.2023.3295731
   Hariharan B, 2014, LECT NOTES COMPUT SC, V8695, P297, DOI 10.1007/978-3-319-10584-0_20
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Hong S, 2022, LECT NOTES COMPUT SC, V13689, P108, DOI 10.1007/978-3-031-19818-2_7
   Hu T, 2019, AAAI CONF ARTIF INTE, P8441
   Jin C, 2022, Arxiv, DOI [arXiv:2109.11071, DOI 10.48550/ARXIV.2109.11071, 10.48550/arXiv.2109.11071]
   Nguyen K, 2019, IEEE I CONF COMP VIS, P622, DOI 10.1109/ICCV.2019.00071
   Lai X, 2021, PROC CVPR IEEE, P1205, DOI 10.1109/CVPR46437.2021.00126
   Lang C., 2022, INT JOINT C ART INT, P1024
   Lang CB, 2023, IEEE T IMAGE PROCESS, V32, P5353, DOI 10.1109/TIP.2023.3315555
   Lang CB, 2023, IEEE T PATTERN ANAL, V45, P10669, DOI 10.1109/TPAMI.2023.3265865
   Li DQ, 2021, PROC CVPR IEEE, P8296, DOI 10.1109/CVPR46437.2021.00820
   Li X, 2020, PROC CVPR IEEE, P2866, DOI 10.1109/CVPR42600.2020.00294
   Li ZC, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3240195
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu HF, 2023, IEEE T MULTIMEDIA, V25, P8580, DOI 10.1109/TMM.2023.3238521
   Liu J, 2022, PROC CVPR IEEE, P11543, DOI 10.1109/CVPR52688.2022.01126
   Liu N., 2022, Advances in neural information processing systems, V35, P38020
   Liu SA, 2023, PROC CVPR IEEE, P11319, DOI 10.1109/CVPR52729.2023.01089
   Liu YW, 2022, PROC CVPR IEEE, P11563, DOI 10.1109/CVPR52688.2022.01128
   Liu YY, 2022, PROC CVPR IEEE, P4248, DOI 10.1109/CVPR52688.2022.00422
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lu ZH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8721, DOI 10.1109/ICCV48922.2021.00862
   Marin D, 2019, IEEE I CONF COMP VIS, P2131, DOI 10.1109/ICCV.2019.00222
   Min H, 2023, PATTERN RECOGN, V137, DOI 10.1016/j.patcog.2022.109291
   Min J, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6921, DOI 10.1109/ICCV48922.2021.00686
   Moon S, 2022, LECT NOTES COMPUT SC, V13680, P506, DOI 10.1007/978-3-031-20044-1_29
   Okazawa A, 2022, LECT NOTES COMPUT SC, V13689, P362, DOI 10.1007/978-3-031-19818-2_21
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Shaban A, 2017, Arxiv, DOI [arXiv:1709.03410, DOI 10.48550/ARXIV.1709.03410]
   Shi XY, 2022, LECT NOTES COMPUT SC, V13680, P151, DOI 10.1007/978-3-031-20044-1_9
   Siam M, 2019, IEEE I CONF COMP VIS, P5248, DOI 10.1109/ICCV.2019.00535
   Snell J, 2017, ADV NEUR IN, V30
   Song YS, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3582688
   Strudel R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7242, DOI 10.1109/ICCV48922.2021.00717
   Tang H, 2023, PROCEEDINGS OF THE 31ST ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2023, P1719, DOI 10.1145/3581783.3612221
   Tang H, 2022, PATTERN RECOGN, V130, DOI 10.1016/j.patcog.2022.108792
   Tang H, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P610, DOI 10.1145/3394171.3413884
   Tian ZH, 2020, IEEE INTERNET THINGS, V7, P3901, DOI [10.1109/JIOT.2019.2951620, 10.1109/TPAMI.2020.3013717]
   Tolstikhin I, 2021, ADV NEUR IN, V34
   Vinyals O., 2016, Advances in neural information processing systems, V29
   Wang CY, 2023, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR52729.2023.00721
   Wang KX, 2019, IEEE I CONF COMP VIS, P9196, DOI 10.1109/ICCV.2019.00929
   Wang Y, 2022, LECT NOTES COMPUT SC, V13689, P36, DOI 10.1007/978-3-031-19818-2_3
   Zha ZC, 2023, IEEE T CIRC SYST VID, V33, P3947, DOI 10.1109/TCSVT.2023.3236636
   Zhang J.W., Advances in Neural Information Processing Systems, V35, P6575
   Zhang L., 2023, IEEE Transactions on Circuits and Systems for Video Technology
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhong X, 2023, IEEE T MULTIMEDIA, V25, P1979, DOI 10.1109/TMM.2022.3141886
   Zhu X.J., 2005, SEMISUPERVISED LEARN
NR 62
TC 0
Z9 0
U1 1
U2 1
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD AUG
PY 2024
VL 148
AR 105113
DI 10.1016/j.imavis.2024.105113
EA JUN 2024
PG 10
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA WY1X9
UT WOS:001258354200001
DA 2024-08-05
ER

PT J
AU Nemani, P
   Vadali, VSS
   Medi, PR
   Marisetty, A
   Vollala, S
   Kumar, S
AF Nemani, Praneeth
   Vadali, Venkata Surya Sundar
   Medi, Prathistith Raj
   Marisetty, Ashish
   Vollala, Satyanarayana
   Kumar, Santosh
TI Cross-modal hybrid architectures for gastrointestinal tract image
   analysis: A systematic review and futuristic applications
SO IMAGE AND VISION COMPUTING
LA English
DT Review
DE Segmentation; CNNs; Transformers; Generative AI; Hybrid architectures;
   Dataset; GI-Tract
ID ENDOSCOPIC RESECTION; FEATURE-EXTRACTION; U-NET; SEGMENTATION; DEEP;
   CHALLENGES; POLYPS; NETWORKS
AB This review paper presents an in-depth exploration of gastrointestinal (GI) tract image analysis, particularly emphasizing organ and polyp segmentation. It addresses the inherent challenges posed by the GI tract's complex anatomy and diverse pathologies, which complicate accurate image analysis. Central to this review is the examination of hybrid computational models that integrate convolutional neural networks (CNNs) and Transformers. This synergy enhances the accuracy of segmenting intricate structures in GI tract imaging, marking a significant advancement in the field. A notable contribution of this review is the systematic categorization and analysis of the latest methodologies in organ and polyp segmentation. It provides a comprehensive overview of various techniques, highlighting their strengths and limitations in addressing the specifications of GI tract imaging. This survey serves as a valuable reference for researchers, outlining current practices and offering insights for future innovations. The review also underscores the critical role of extensive and diverse datasets in advancing GI tract image analysis. It stresses the need for high-quality datasets to effectively train and evaluate emerging models, addressing the broad spectrum of GI tract conditions. Moreover, the review delves into the burgeoning area of Generative AI, exploring its potential to enrich datasets and enhance segmentation models. Future developments in GI tract segmentation will focus on refining hybrid CNN-Transformer models and creating larger, more diverse datasets for better model training. Specialized focus on specific segmentation challenges, like polyp and organ segmentation, is anticipated. The field will explore Generative AI applications for innovative segmentation approaches. Collaborative efforts between technologists and clinicians will enhance practical clinical integration and applicability.
C1 [Nemani, Praneeth] Univ Colorado Boulder, Coll Engn & Appl Sci, Boulder, CO 80309 USA.
   [Vadali, Venkata Surya Sundar; Vollala, Satyanarayana; Kumar, Santosh] IIIT Naya Raipur, Dept Comp Sci & Engn, Uparwara, India.
   [Medi, Prathistith Raj; Marisetty, Ashish] IIIT Naya Raipur, Dept Data Sci & Artificial Intelligence, Uparwara, India.
C3 University of Colorado System; University of Colorado Boulder
RP Nemani, P (corresponding author), Univ Colorado Boulder, Coll Engn & Appl Sci, Boulder, CO 80309 USA.
EM praneeth.nemani@colorado.edu
CR Abbas Q., 2022, J. Intell. Fuzzy Syst., P1
   Abraham E, 2023, BIOMED SIGNAL PROCES, V84, DOI 10.1016/j.bspc.2023.104961
   Al Jowair H, 2023, IMAGE VISION COMPUT, V137, DOI 10.1016/j.imavis.2023.104767
   Alam MJ, 2023, COMPUT BIOL MED, V160, DOI 10.1016/j.compbiomed.2023.106945
   Albahri OS, 2020, J INFECT PUBLIC HEAL, V13, P1381, DOI 10.1016/j.jiph.2020.06.028
   Ahmed AMAA, 2020, Arxiv, DOI arXiv:2012.06771
   Ali H, 2020, ARTIF INTELL REV, V53, P2635, DOI 10.1007/s10462-019-09743-2
   Ali S, 2021, Arxiv, DOI [arXiv:2106.04463, DOI 10.48550/ARXIV.2106.04463]
   Altaf F, 2019, IEEE ACCESS, V7, P99540, DOI 10.1109/ACCESS.2019.2929365
   Aminabee S., 2024, Emerging Technologies for Health Literacy and Medical Practice, P240
   Arnold M, 2010, EURASIP J IMAGE VIDE, DOI 10.1155/2010/814319
   Aronowitz RA, 2009, MILBANK Q, V87, P417, DOI 10.1111/j.1468-0009.2009.00563.x
   Banik D., 2022, Artificial Intelligence on Medical Data, P397
   Bhattamisra SK, 2023, BIG DATA COGN COMPUT, V7, DOI 10.3390/bdcc7010010
   Biswas R, 2023, Arxiv, DOI arXiv:2308.06623
   Braga Miguel, 2023, Radiol Case Rep, V18, P1181, DOI 10.1016/j.radcr.2022.12.003
   Bulut B, 2022, 2022 INTERNATIONAL CONFERENCE ON DECISION AID SCIENCES AND APPLICATIONS (DASA), P1149, DOI 10.1109/DASA54658.2022.9765101
   Buzug TM, 2011, SPRINGER HANDBOOK OF MEDICAL TECHNOLOGY, P311
   Caceres M, 2006, ANN THORAC SURG, V81, P393, DOI 10.1016/j.athoracsur.2005.05.106
   Chen FY, 2023, Arxiv, DOI arXiv:2306.10773
   Chowdhary CL, 2020, PROCEDIA COMPUT SCI, V167, P26, DOI 10.1016/j.procs.2020.03.179
   Cootes TF, 2001, PROC SPIE, V4322, P236, DOI 10.1117/12.431093
   Creswell A, 2018, IEEE SIGNAL PROC MAG, V35, P53, DOI 10.1109/MSP.2017.2765202
   Deeba F, 2016, IEEE IJCNN, P4650, DOI 10.1109/IJCNN.2016.7727810
   Diakogiannis FI, 2020, ISPRS J PHOTOGRAMM, V162, P94, DOI 10.1016/j.isprsjprs.2020.01.013
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Duncan JS, 2000, IEEE T PATTERN ANAL, V22, P85, DOI 10.1109/34.824822
   Fagereng JA, 2022, COMP MED SY, P66, DOI 10.1109/CBMS55023.2022.00019
   Fu J, 2019, IEEE I CONF COMP VIS, P6747, DOI 10.1109/ICCV.2019.00685
   Gangrade S., 2023, 2023 5 INT C EL COMP, P1
   Georgakopoulos SV, 2016, IEEE CONF IMAGING SY, P510, DOI 10.1109/IST.2016.7738279
   Gibson Eli, 2017, Medical Image Computing and Computer Assisted Intervention  MICCAI 2017. 20th International Conference. Proceedings: LNCS 10433, P728, DOI 10.1007/978-3-319-66182-7_83
   Gómez P, 2019, MED BIOL ENG COMPUT, V57, P1451, DOI 10.1007/s11517-019-01965-4
   Gould S., 2009, Adv. Neural Inf. Proces. Syst., V22
   Guggari S., 2022, INT C APPL MACH LEAR, P131
   Gupta D, 2017, BIOMED SIGNAL PROCES, V31, P116, DOI 10.1016/j.bspc.2016.06.012
   Gupta S, 2023, 2023 3 INT C SEC CYB, P501, DOI 10.1109/ICSCCC58608.2023.10176721
   Hai Vu, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2468, DOI 10.1109/ICPR.2010.604
   Hicks S.A., 2021, Lecture Notes in Computer Science, P263
   Hong LTT, 2021, IEEE ACCESS, V9, P156987, DOI 10.1109/ACCESS.2021.3129480
   Hu KL, 2023, COMPUT BIOL MED, V160, DOI 10.1016/j.compbiomed.2023.107028
   Huang C.-H., 2021, arXiv, DOI DOI 10.48550/ARXIV.2101.07172
   Iddan G, 2000, NATURE, V405, P417, DOI 10.1038/35013140
   Iishi H, 1996, GASTROINTEST ENDOSC, V44, P594, DOI 10.1016/S0016-5107(96)70015-9
   Jha D, 2021, IEEE J BIOMED HEALTH, V25, P2029, DOI 10.1109/JBHI.2021.3049304
   Jha D, 2020, COMP MED SY, P558, DOI 10.1109/CBMS49503.2020.00111
   Jha D, 2020, LECT NOTES COMPUT SC, V11962, P451, DOI 10.1007/978-3-030-37734-2_37
   Jha D, 2019, IEEE INT SYM MULTIM, P225, DOI 10.1109/ISM46123.2019.00049
   Jia X, 2017, I S BIOMED IMAGING, P179, DOI 10.1109/ISBI.2017.7950496
   Jia X, 2016, IEEE ENG MED BIO, P639, DOI 10.1109/EMBC.2016.7590783
   Kingma DP, 2019, FOUND TRENDS MACH LE, V12, P4, DOI 10.1561/2200000056
   Kouklakis G, 2009, SURG ENDOSC, V23, P2732, DOI 10.1007/s00464-009-0478-3
   Tomar NK, 2022, Arxiv, DOI arXiv:2206.08985
   Lafraxo S, 2023, LIFE-BASEL, V13, DOI 10.3390/life13030719
   Lewis J, 2023, SCI REP-UK, V13, DOI 10.1038/s41598-023-28530-2
   Li ZY, 2018, MED IMAGE ANAL, V43, P66, DOI 10.1016/j.media.2017.09.007
   Liedlgruber Michael, 2011, IEEE Rev Biomed Eng, V4, P73, DOI 10.1109/RBME.2011.2175445
   Liu FJ, 2022, COMPUT BIOL MED, V151, DOI 10.1016/j.compbiomed.2022.106304
   Lu L., 2017, Advances in Computer Vision and Pattern Recognition, V10, P978
   Machácek R, 2023, Arxiv, DOI arXiv:2304.05233
   Mahmood T., 2023, IEEE Access
   Maier A, 2022, PROG BIOMED ENG, V4, DOI 10.1088/2516-1091/ac5b13
   Mau T.-H.N., 2023, arXiv
   Meester RGS, 2020, GASTROENTEROLOGY, V159, P105, DOI 10.1053/j.gastro.2020.03.025
   Melson J, 2021, GASTROINTEST ENDOSC, V93, P784, DOI 10.1016/j.gie.2020.12.001
   Mohapatra S, 2022, GASTROENTEROL INSIGH, V13, P264, DOI 10.3390/gastroent13030027
   Musa P., 2018, 2018 3 INT C INF COM, P1, DOI [10.1109/IAC.2018.8780492, DOI 10.1109/IAC.2018.8780492]
   Nanni L, 2023, SENSORS-BASEL, V23, DOI 10.3390/s23104688
   Nemani Praneeth, 2022, 2022 26th International Computer Science and Engineering Conference (ICSEC), P7, DOI 10.1109/ICSEC56337.2022.10049343
   Ng MY, 2023, JAMA NETW OPEN, V6, DOI 10.1001/jamanetworkopen.2023.45892
   Duc NT, 2022, IEEE ACCESS, V10, P80575, DOI 10.1109/ACCESS.2022.3195241
   Nguyen-Mau TH, 2023, LECT NOTES COMPUT SC, V13834, P240, DOI 10.1007/978-3-031-27818-1_20
   Nia N. G., 2023, Discov. Artif. Intell., V3, DOI DOI 10.1007/S44163-023-00049-5
   Pacal I, 2020, COMPUT BIOL MED, V126, DOI 10.1016/j.compbiomed.2020.104003
   Pandey P, 2023, IEEE T MED IMAGING, V42, P2490, DOI 10.1109/TMI.2023.3258069
   Park KB, 2022, J COMPUT DES ENG, V9, P616, DOI 10.1093/jcde/qwac018
   Pei MQ, 2017, KNOWL-BASED SYST, V121, P163, DOI 10.1016/j.knosys.2017.01.023
   Pishva AK, 2023, COMP MED SY, P47, DOI 10.1109/CBMS58004.2023.00190
   Pons JP, 2007, LECT NOTES COMPUT SC, V4584, P198
   Prasath VBS, 2017, J IMAGING, V3, DOI 10.3390/jimaging3010001
   Prasath VBS, 2016, ADV INTELL SYST, V410, P95, DOI 10.1007/978-81-322-2734-2_10
   Qian Zhao, 2010, 2010 IEEE International Conference on Robotics and Biomimetics (ROBIO), P442, DOI 10.1109/ROBIO.2010.5723367
   Ramzan M, 2022, J PERS MED, V12, DOI 10.3390/jpm12091459
   Rastogi P, 2018, 2018 FIFTH INTERNATIONAL CONFERENCE ON PARALLEL, DISTRIBUTED AND GRID COMPUTING (IEEE PDGC), P60, DOI 10.1109/PDGC.2018.8745750
   Raymann J, 2023, DIAGNOSTICS, V13, DOI 10.3390/diagnostics13010123
   Razzak MI, 2018, L N COMPUT VIS BIOME, V26, P323, DOI 10.1007/978-3-319-65981-7_12
   Ribeiro E, 2016, COMPUT MATH METHOD M, V2016, DOI 10.1155/2016/6584725
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sabarinathan, 2023, Machine Vision and Augmented Intelligence: Select Proceedings of MAI 2022. Lecture Notes in Electrical Engineering (1007), P203, DOI 10.1007/978-981-99-0189-0_13
   Safarov S, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21041441
   Seguí S, 2012, IEEE T INF TECHNOL B, V16, P1341, DOI 10.1109/TITB.2012.2221472
   Shaikh S.S., 2023, J. Univ. Med. Dental College, V14, P590
   Shamaev D, 2023, LECT NOTE NETW SYST, V597, P519, DOI 10.1007/978-3-031-21438-7_41
   Sharma Neha, 2023, 2023 International Conference in Advances in Power, Signal, and Information Technology (APSIT), P226, DOI 10.1109/APSIT58554.2023.10201739
   Sharma Neha, 2022, 2022 6th International Conference on Electronics, Communication and Aerospace Technology, P1493, DOI 10.1109/ICECA55336.2022.10009547
   Sharma N, 2023, DIAGNOSTICS, V13, DOI 10.3390/diagnostics13142399
   Siddique N, 2021, IEEE ACCESS, V9, P82031, DOI 10.1109/ACCESS.2021.3086020
   Singh N, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3096266
   Su R, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.586197
   Surya Prasath V., 2015, Computational Intelligence in Medical Informatics, P73
   Tajbakhsh N, 2021, IEEE T MED IMAGING, V40, P2526, DOI [10.1109/TMI.2021.3089292, 10.1109/tmi.2021.3089292]
   Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P1299, DOI 10.1109/TMI.2016.2535302
   Tang SG, 2023, COMPUT BIOL MED, V157, DOI 10.1016/j.compbiomed.2023.106723
   Tomar Nikhil Kumar, 2021, Pattern Recognition. ICPR International Workshops and Challenges. Proceedings. Lecture Notes in Computer Science (LNCS 12668), P307, DOI 10.1007/978-3-030-68793-9_23
   Valvano G, 2021, IEEE T MED IMAGING, V40, P1990, DOI 10.1109/TMI.2021.3069634
   Vázquez D, 2017, J HEALTHC ENG, V2017, DOI 10.1155/2017/4037190
   Sang DV, 2022, Arxiv, DOI [arXiv:2105.00402, DOI 10.48550/ARXIV.2105.00402]
   WALSH RM, 1992, GASTROINTEST ENDOSC, V38, P303
   Wang J., 2022, Boundary Aware Pyramid Transformer for Polyp Segmentation
   Wang KQ, 2023, COMPUT BIOL MED, V155, DOI 10.1016/j.compbiomed.2023.106704
   Wang SQ, 2020, IEEE ACCESS, V8, P184841, DOI 10.1109/ACCESS.2020.3029857
   Wu HS, 2023, IEEE T CYBERNETICS, V53, P2610, DOI 10.1109/TCYB.2022.3162873
   Xu X., 2023, IEEE Transactions on Medical Imaging
   Yang L, 2023, COMPUT BIOL MED, V164, DOI 10.1016/j.compbiomed.2023.107301
   Yang TW, 2021, DIAGNOSTICS, V11, DOI 10.3390/diagnostics11050790
   Yeung M, 2021, COMPUT BIOL MED, V137, DOI 10.1016/j.compbiomed.2021.104815
   Yousef R, 2022, MULTIMEDIA SYST, V28, P881, DOI 10.1007/s00530-021-00884-5
   Zhang H, 2022, IEEE COMPUT SOC CONF, P2735, DOI 10.1109/CVPRW56347.2022.00309
   Zhang WC, 2022, COMPUT BIOL MED, V150, DOI 10.1016/j.compbiomed.2022.106173
   Zhang YF, 2020, IEEE T IMAGE PROCESS, V29, P7834, DOI 10.1109/TIP.2020.3006377
   Zhang YY, 2023, BIOMED SIGNAL PROCES, V86, DOI 10.1016/j.bspc.2023.105133
   Zhao F., 2013, Ann. BMVA, V2013, P1, DOI [10.1155/2013/325903, DOI 10.1155/2013/325903]
   Zheng PP, 2018, MED RES REV, V38, P325, DOI 10.1002/med.21463
   Zhi Z, 2021, TECHNOL HEALTH CARE, V29, P363, DOI 10.3233/THC-202638
   Zhou ZL, 2017, IEEE T INF FOREN SEC, V12, P48, DOI 10.1109/TIFS.2016.2601065
   Zhu RS, 2015, 2015 8TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING (CISP), P372, DOI 10.1109/CISP.2015.7407907
NR 126
TC 0
Z9 0
U1 1
U2 1
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD AUG
PY 2024
VL 148
AR 105068
DI 10.1016/j.imavis.2024.105068
EA JUN 2024
PG 14
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA UY3C8
UT WOS:001251575500001
DA 2024-08-05
ER

PT J
AU Imran, M
   Akram, MU
   Tiwana, MI
   Salam, AA
   Greco, D
AF Imran, Muhammad
   Akram, Muhammad Usman
   Tiwana, Mohsin Islam
   Salam, Anum Abdul
   Greco, Danilo
TI Two-dimensional hybrid incremental learning (2DHIL) framework for
   semantic segmentation of skin tissues
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE AI; Incremental semantic segmentation; Incremental learning; Continual
   learning; Knowledge distillation; Mutual distillation loss; Transformer;
   Segformer; Skin cancer; Non-melanoma skin cancer; Computational
   histopathology
AB This study aims to enhance the robustness and generalization capability of a deep learning transformer model used for segmenting skin carcinomas and tissues through the introduction of incremental learning. Deep learning AI models demonstrate their claimed performance only for tasks and data types for which they are specifically trained. Their performance is severely challenged for the test cases which are not similar to training data thus questioning their robustness and ability to generalize. Moreover, these models require an enormous amount of annotated data for training to achieve desired performance. The availability of large annotated data, particularly for medical applications, is itself a challenge. Despite efforts to alleviate this limitation through techniques like data augmentation, transfer learning, and few-shot training, the challenge persists. To address this, we propose refining the models incrementally as new classes are discovered and more data becomes available, emulating the human learning process. However, deep learning models face the challenge of catastrophic forgetting during incremental training. Therefore, we introduce a two-dimensional hybrid incremental learning framework for segmenting non-melanoma skin cancers and tissues from histopathology images. Our approach involves progressively adding new classes and introducing data of varying specifications to introduce adaptability in the models. We also employ a combination of loss functions to facilitate new learning and mitigate catastrophic forgetting. Our extended experiments demonstrate significant improvements, with an F1 score reaching 91.78, mIoU of 93.00, and an average accuracy of 95%. These findings highlight the effectiveness of our incremental learning strategy in enhancing the robustness and generalization of deep learning segmentation models while mitigating catastrophic forgetting.
C1 [Imran, Muhammad; Tiwana, Mohsin Islam] Natl Univ Sci & Technol, Dept Mechatron Engn, Islamabad, Pakistan.
   [Akram, Muhammad Usman; Salam, Anum Abdul] Natl Univ Sci & Technol, Dept Comp Software Engn, Islamabad, Pakistan.
   [Greco, Danilo] Politecn Milan, Dept Management Econ & Ind Engn, Via Lambruschini 24-b, I-20156 Milan, Italy.
C3 National University of Sciences & Technology - Pakistan; National
   University of Sciences & Technology - Pakistan; Polytechnic University
   of Milan
RP Akram, MU (corresponding author), Natl Univ Sci & Technol, Dept Comp Software Engn, Islamabad, Pakistan.
EM usmakram@gmail.com
OI Greco, Danilo/0000-0002-0011-7001
CR Aljundi R, 2018, LECT NOTES COMPUT SC, V11207, P144, DOI 10.1007/978-3-030-01219-9_9
   Aljundi R, 2017, PROC CVPR IEEE, P7120, DOI 10.1109/CVPR.2017.753
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Belouadah E, 2019, IEEE I CONF COMP VIS, P583, DOI 10.1109/ICCV.2019.00067
   Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734
   Castro FM, 2018, LECT NOTES COMPUT SC, V11216, P241, DOI 10.1007/978-3-030-01258-8_15
   Cermelli Fabio, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9230, DOI 10.1109/CVPR42600.2020.00925
   Cermelli F, 2022, IEEE T PATTERN ANAL, V44, P10099, DOI 10.1109/TPAMI.2021.3133954
   Cha Sungmin, 2021, NeurIPS, V34, P10919
   Chaudhry A, 2018, LECT NOTES COMPUT SC, V11215, P556, DOI 10.1007/978-3-030-01252-6_33
   Chen J., 2021, TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation
   Chen L.-C., 2018, P EUR C COMP VIS ECC, P801, DOI DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2017, Arxiv, DOI [arXiv:1706.05587, 10.48550/arXiv.1706.05587, DOI 10.48550/ARXIV.1706.05587]
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Cheng B, 2021, ADV NEUR IN, V34
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dhar P, 2019, PROC CVPR IEEE, P5133, DOI 10.1109/CVPR.2019.00528
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Douillard A, 2021, PROC CVPR IEEE, P4039, DOI 10.1109/CVPR46437.2021.00403
   French RM, 1999, TRENDS COGN SCI, V3, P128, DOI 10.1016/S1364-6613(99)01294-2
   Goswami D, 2023, IEEE WINT CONF APPL, P3194, DOI 10.1109/WACV56688.2023.00321
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He WJ, 2021, INFORM FUSION, V73, P157, DOI 10.1016/j.inffus.2021.02.017
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Jung HC, 2016, Arxiv, DOI arXiv:1607.00122
   Kalb T., 2022, P AS C COMP VIS, P56
   Khan AM, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22041667
   Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114
   Li K, 2023, IEEE T MED IMAGING, V42, P570, DOI 10.1109/TMI.2022.3211195
   Li ZZ, 2018, IEEE T PATTERN ANAL, V40, P2935, DOI 10.1109/TPAMI.2017.2773081
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lopez-Paz D, 2017, ADV NEUR IN, V30
   Maracani A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7006, DOI 10.1109/ICCV48922.2021.00694
   McCloskey M., 1989, Psychology of learning and motivation, V24, P165
   Michieli U, 2021, PROC CVPR IEEE, P1114, DOI 10.1109/CVPR46437.2021.00117
   Michieli U, 2021, COMPUT VIS IMAGE UND, V205, DOI 10.1016/j.cviu.2021.103167
   Michieli U, 2019, IEEE INT CONF COMP V, P3205, DOI 10.1109/ICCVW.2019.00400
   Mittal S, 2021, IEEE COMPUT SOC CONF, P3508, DOI 10.1109/CVPRW53098.2021.00390
   Oh Y., 2022, Adv. Neural Inform. Process. Syst., V35, P14516
   Paszke A, 2016, Arxiv, DOI arXiv:1606.02147
   Qiu YQ, 2023, PATTERN RECOGN, V138, DOI 10.1016/j.patcog.2023.109383
   Rebuffi SA, 2017, PROC CVPR IEEE, P5533, DOI 10.1109/CVPR.2017.587
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shang C, 2023, PROC CVPR IEEE, P7214, DOI 10.1109/CVPR52729.2023.00697
   Sirshar M, 2021, COMPUT BIOL MED, V134, DOI 10.1016/j.compbiomed.2021.104435
   Smith J, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9354, DOI 10.1109/ICCV48922.2021.00924
   Strudel R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7242, DOI 10.1109/ICCV48922.2021.00717
   Tan MX, 2019, PR MACH LEARN RES, V97
   Thomas SM, 2021, DATA BRIEF, V39, DOI 10.1016/j.dib.2021.107587
   Thomas SM, 2021, MED IMAGE ANAL, V68, DOI 10.1016/j.media.2020.101915
   Tian Y., 2020, Contrastive representation distillation
   van de Ven GM, 2022, NAT MACH INTELL, V4, P1185, DOI 10.1038/s42256-022-00568-3
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Xie EZ, 2021, ADV NEUR IN, V34
   Yan SP, 2021, PROC CVPR IEEE, P3013, DOI 10.1109/CVPR46437.2021.00303
   Yang Y, 2022, PATTERN RECOGN, V122, DOI 10.1016/j.patcog.2021.108260
   Yang YZ, 2022, I C CONT AUTOMAT ROB, P299, DOI 10.1109/ICARCV57592.2022.10004288
   Yu L, 2023, IEEE T NEUR NET LEAR, V34, P9116, DOI 10.1109/TNNLS.2022.3155746
   Zhang CB, 2022, PROC CVPR IEEE, P7043, DOI 10.1109/CVPR52688.2022.00692
   Zhao DP, 2023, IEEE T PATTERN ANAL, V45, P11932, DOI 10.1109/TPAMI.2023.3273574
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhuang C, 2022, PATTERN RECOGN, V132, DOI 10.1016/j.patcog.2022.108907
NR 64
TC 0
Z9 0
U1 1
U2 1
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD AUG
PY 2024
VL 148
AR 105098
DI 10.1016/j.imavis.2024.105098
EA JUN 2024
PG 13
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA UU6Y1
UT WOS:001250626300001
DA 2024-08-05
ER

PT J
AU Xu, J
   Liu, B
   Xiao, YS
   Cao, F
   He, JH
AF Xu, Jian
   Liu, Bo
   Xiao, Yanshan
   Cao, Fan
   He, Jinghui
TI A multitask tensor-based relation network for cloth-changing person
   re-identification
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Cloth -changing person re -identification multi; task learning; Meta
   learning tensor feature; Relation network
ID NEURAL-NETWORK; ATTENTION
AB The Cloth-changing person re-identification (CC-ReID) is more challenging than person re-identification (ReID) because of the unreliability of cloth-relevant features. Existing CC-ReID methods output the unique latent vector feature for the same pedestrian image. However, on the one hand, the latent vector feature generated by the pooling layer will lose more spatial information. On the other hand, the features that they focus on are the same for the same pedestrian image under different comparison pedestrian images, which does not consider the pedestrian pair relation information. For this, we propose a Multitask Tensor-based Relation Network (MTTRN) for CC-ReID. In MTTRN, we utilize the augmented cloth-changing identity images, human parsing images and head images to guide the model learning more fine-grained cloth-irrelevant feature cues. We propose a novel channel subspace generator and use the tensor mode-n product to generate the subspace tensor features instead of the vector feature generated by the pooling layer, which can retain more spatial feature information. Furthermore, we assume that the model will focus on different features for the same pedestrian image under different comparison pedestrian images in the same feature subspace or the same pedestrian image pair in different feature subspaces, in which the tensor feature pair relation is considered fully to mine more robust features. Extensive experiments show that our method achieves the state-of-the-art or competitive performance on three CC-ReID benchmark datasets and demonstrate the robustness of our model.
C1 [Xu, Jian; Liu, Bo; Cao, Fan] Guangdong Univ Technol, Sch Automat, Guangzhou, Peoples R China.
   [Xiao, Yanshan] Guangdong Univ Technol, Sch Comp Sci, Guangzhou, Peoples R China.
   [He, Jinghui] Guangdong Univ Technol, Sch Electromech Engn, Guangzhou, Peoples R China.
C3 Guangdong University of Technology; Guangdong University of Technology;
   Guangdong University of Technology
RP Liu, B (corresponding author), Guangdong Univ Technol, Sch Automat, Guangzhou, Peoples R China.
EM jianxu9603@gmail.com; csboliu@163.com
OI Cao, Fan/0000-0003-1685-330X
FU Natural Science Foundation of China [62076074, 61876044, 61672169];
   Guangdong Basic and Applied Basic Research Foundation [2020A1515010670,
   2020A1515011501]; Science and Technology Planning Project of Guangzhou
   [202002030141]
FX The authors would like to thank the reviewers for their very useful
   comments and suggestions. This work was supported in part by the Natural
   Science Foundation of China under Grant 62076074, 61876044 and 61672169,
   in part by Guangdong Basic and Applied Basic Research Foundation Grant
   2020A1515010670 and 2020A1515011501, in part by the Science and
   Technology Planning Project of Guangzhou under Grant 202002030141.
CR Bansal V, 2022, IEEE WINT CONF APPL, P602, DOI 10.1109/WACVW54805.2022.00066
   Cao BK, 2014, IEEE DATA MINING, P40, DOI 10.1109/ICDM.2014.26
   Chen WH, 2017, AAAI CONF ARTIF INTE, P3988
   Chen XJ, 2014, PROC CVPR IEEE, P1979, DOI 10.1109/CVPR.2014.254
   Cui ZY, 2023, IEEE T CIRC SYST VID, V33, P4415, DOI 10.1109/TCSVT.2023.3241988
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Duan MX, 2021, ACM T INTEL SYST TEC, V12, DOI 10.1145/3418285
   Finn C, 2017, PR MACH LEARN RES, V70
   Gao Z, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P3703, DOI 10.1145/3503161.3547884
   Gu XQ, 2022, PROC CVPR IEEE, P1050, DOI 10.1109/CVPR52688.2022.00113
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hong PX, 2021, PROC CVPR IEEE, P10508, DOI 10.1109/CVPR46437.2021.01037
   Hospedales T, 2022, IEEE T PATTERN ANAL, V44, P5149, DOI 10.1109/TPAMI.2021.3079209
   Huang Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11875, DOI 10.1109/ICCV48922.2021.01168
   Jia XM, 2022, IEEE T IMAGE PROCESS, V31, P4227, DOI 10.1109/TIP.2022.3183469
   Jin X, 2022, PROC CVPR IEEE, P14258, DOI 10.1109/CVPR52688.2022.01388
   Lazarou M, 2022, IEEE WINT CONF APPL, P2050, DOI 10.1109/WACV51458.2022.00211
   Li PK, 2022, IEEE T PATTERN ANAL, V44, P3260, DOI 10.1109/TPAMI.2020.3048039
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Li X., 2022, PATT REC COMP VIS 1, P527
   Li YJ, 2021, IEEE WINT CONF APPL, P2431, DOI 10.1109/WACV48630.2021.00248
   Nichol A, 2018, Arxiv, DOI arXiv:1803.02999
   Panagakis Y, 2021, P IEEE, V109, P863, DOI 10.1109/JPROC.2021.3074329
   Pandey SK, 2022, BIOMED SIGNAL PROCES, V71, DOI 10.1016/j.bspc.2021.103173
   Qian Xuelin, 2020, P AS C COMP VIS
   Ravi S., 2017, INT C LEARN REPR
   Rusu A.A., 2019, ICLR
   Shi W, 2022, IMAGE VISION COMPUT, V117, DOI 10.1016/j.imavis.2021.104335
   Sun WZ, 2021, IEEE J-STSP, V15, P603, DOI 10.1109/JSTSP.2020.3038227
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Tjandra A, 2018, IEEE IJCNN
   Wan FB, 2020, IEEE COMPUT SOC CONF, P3620, DOI 10.1109/CVPRW50498.2020.00423
   Wang Q, 2023, LECT NOTES COMPUT SC, V13845, P351, DOI 10.1007/978-3-031-26348-4_21
   Wu AC, 2017, IEEE T IMAGE PROCESS, V26, P2588, DOI 10.1109/TIP.2017.2675201
   Wu JB, 2022, IEEE IMAGE PROC, P1016, DOI 10.1109/ICIP46576.2022.9897243
   Xiao Y., 2022, P 6 INT C COMP SCI A, P1
   Xie QK, 2023, IEEE T MULTIMEDIA, V25, P6384, DOI 10.1109/TMM.2022.3207949
   Xu Wanlu, 2021, PROC 13 INT JOINT C, P1201, DOI DOI 10.24963/IJCAI
   Xue J, 2018, IEEE COMPUT SOC CONF, P2193, DOI 10.1109/CVPRW.2018.00285
   Yang QZ, 2021, IEEE T PATTERN ANAL, V43, P2029, DOI 10.1109/TPAMI.2019.2960509
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Yu H., 2022, PATT REC COMP VIS 1, P29
   Yu SJ, 2020, PROC CVPR IEEE, P3397, DOI 10.1109/CVPR42600.2020.00346
   Zhang RJ, 2023, PATTERN RECOGN, V134, DOI 10.1016/j.patcog.2022.109070
   Zhang YL, 2021, INFORM SCIENCES, V568, P133, DOI 10.1016/j.ins.2021.03.048
   Zhang ZZ, 2020, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR42600.2020.00325
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   ZHU K, 2020, ECCV, DOI DOI 10.1007/978-3-030-58580-8_21
NR 51
TC 0
Z9 0
U1 3
U2 3
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUL
PY 2024
VL 147
AR 105090
DI 10.1016/j.imavis.2024.105090
EA MAY 2024
PG 14
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA UC9R3
UT WOS:001245984800001
DA 2024-08-05
ER

PT J
AU Li, BC
   Wang, YF
   Wang, LJ
   Lu, HC
AF Li, Bocen
   Wang, Yifan
   Wang, Lijun
   Lu, Huchuan
TI CSRNet: Focusing on critical points for depth completion
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Depth completion; Mining L1 loss; Minimum error curve;
   Standard-point-enhancing learning paradigm
ID NETWORK
AB Depth completion is an effective method for generating dense depth maps from sparse ones. In recent studies, the majority of points, which we call standard points, often exhibit sub -optimal performance. This issue arises from the need to fit only very few points, termed as challenging points, which consist of noises and regions with discontinuous depth in the ground -truth. On the other hand, traditional evaluations can not recognize this situation, and are dominated by these limited challenging points, whose performance improvements may not significantly benefit related tasks. In contrast, standard points, which are critical for these tasks, are not effectively measured. This discrepancy highlights the need for a more targeted approach and evaluation method for depth completion. In order to solve the above problems, we propose a standard -point -enhancing learning paradigm. This paradigm aims to improve the performance on standard points, which consists of a Cascaded Segmentation -to -Regression Networks (CSRNet) and a Mining L1 loss. CSRNet includes two branches: DSNet and DRNet. DSNet uses segmentation to generate a coarse depth map, providing challenging -point -insensitive information. DRNet adopts a coarse -to -fine approach to learn the residual depth map between the coarse depth map and the ground -truth depth map. In addition, our Mining L1 loss leverages the segmentation results to filter out potential challenging points. This approach allows the network to concentrate more effectively on standard points. Lastly, we introduce the Minimum Error (ME) Curves as a new way to measure the performance of predicted depth maps in a flexible and comprehensive manner, irrespective of whether the points are standard or challenging. Experimental results on the KITTI and NYUDv2 datasets show that our approach significantly improves accuracy on the majority of points.
C1 [Li, Bocen; Wang, Yifan; Wang, Lijun; Lu, Huchuan] Dalian Univ Technol, Linggong Rd, Dalian 116024, Liaoning, Peoples R China.
C3 Dalian University of Technology
RP Wang, LJ (corresponding author), Dalian Univ Technol, Linggong Rd, Dalian 116024, Liaoning, Peoples R China.
EM ljwang@dlut.edu.cn
FU National Natural Science Foundation of China [U23A20386, 62276045,
   62293540, 62293542]; Dalian Sci-ence and Technology Talent Innovation
   Support Plan [2022RY17]
FX This work is supported by the National Natural Science Foundation of
   China (U23A20386, 62276045, 62293540, 62293542) , Dalian Sci-ence and
   Technology Talent Innovation Support Plan (2022RY17) .
CR Bhat SF, 2021, PROC CVPR IEEE, P4008, DOI 10.1109/CVPR46437.2021.00400
   Chen WQ, 2022, J FIELD ROBOT, V39, P117, DOI 10.1002/rob.22040
   Cheng XJ, 2020, IEEE T PATTERN ANAL, V42, P2361, DOI 10.1109/TPAMI.2019.2947374
   Cheng XJ, 2020, AAAI CONF ARTIF INTE, V34, P10615
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Deng KL, 2022, PROC CVPR IEEE, P12872, DOI 10.1109/CVPR52688.2022.01254
   Fu H, 2018, PROC CVPR IEEE, P2002, DOI 10.1109/CVPR.2018.00214
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Gu JQ, 2021, IEEE ROBOT AUTOM LET, V6, P1808, DOI 10.1109/LRA.2021.3060396
   Hambarde P, 2020, IEEE T COMPUT IMAG, V6, P806, DOI 10.1109/TCI.2020.2981761
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu M, 2021, IEEE INT CONF ROBOT, P13656, DOI 10.1109/ICRA48506.2021.9561035
   Imran Saif, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12438, DOI 10.1109/CVPR.2019.01273
   Imran S, 2021, PROC CVPR IEEE, P2583, DOI 10.1109/CVPR46437.2021.00261
   Jin WD, 2021, IEEE T IMAGE PROCESS, V30, P3376, DOI 10.1109/TIP.2021.3060167
   Jinsun Park, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P120, DOI 10.1007/978-3-030-58601-0_8
   Lee BU, 2021, PROC CVPR IEEE, P13911, DOI 10.1109/CVPR46437.2021.01370
   Li A, 2020, IEEE WINT CONF APPL, P32, DOI 10.1109/WACV45572.2020.9093407
   Li YH, 2023, AAAI CONF ARTIF INTE, P1477
   Lin YK, 2022, Arxiv, DOI arXiv:2202.09769
   Liu LN, 2021, AAAI CONF ARTIF INTE, V35, P2136
   Liu SF, 2017, ADV NEUR IN, V30
   Liu Z, 2022, PROC CVPR IEEE, P11966, DOI 10.1109/CVPR52688.2022.01167
   Loshchilov I, 2019, Arxiv, DOI [arXiv:1711.05101, 10.48550/arXiv.1711.05101, DOI 10.48550/ARXIV.1711.05101]
   Nazir D, 2022, IEEE ACCESS, V10, P120781, DOI 10.1109/ACCESS.2022.3214316
   Philion Jonah, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P194, DOI 10.1007/978-3-030-58568-6_12
   Qiu JX, 2019, PROC CVPR IEEE, P3308, DOI 10.1109/CVPR.2019.00343
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Tang J, 2021, IEEE T IMAGE PROCESS, V30, P1116, DOI 10.1109/TIP.2020.3040528
   Tateno K, 2017, PROC CVPR IEEE, P6565, DOI 10.1109/CVPR.2017.695
   Nguyen TN, 2018, IEEE ACCESS, V6, P38106, DOI 10.1109/ACCESS.2018.2854262
   Wong A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12727, DOI 10.1109/ICCV48922.2021.01251
   Xu ZY, 2020, IEEE IMAGE PROC, P913, DOI [10.1109/ICIP40778.2020.9191138, 10.1109/icip40778.2020.9191138]
   Yan Z., 2022, arXiv
   Yao Y, 2020, IEEE ROBOT AUTOM LET, V5, P5128, DOI 10.1109/LRA.2020.3005890
   Zhang HW, 2022, IEEE INT C INT ROBOT, P4865, DOI 10.1109/IROS47612.2022.9982280
   Zhao SS, 2021, IEEE T IMAGE PROCESS, V30, P5264, DOI 10.1109/TIP.2021.3079821
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
NR 38
TC 0
Z9 0
U1 1
U2 1
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUL
PY 2024
VL 147
AR 105051
DI 10.1016/j.imavis.2024.105051
EA MAY 2024
PG 10
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA TM7X8
UT WOS:001241757500001
DA 2024-08-05
ER

PT J
AU Hu, KD
   Xie, ZX
   Hu, QH
AF Hu, Kaidi
   Xie, Zongxia
   Hu, Qinghua
TI Lightweight convolutional neural networks with context broadcast
   transformer for real-time semantic segmentation
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Lightweight neural network; Vision transformer; Real-time semantic
   segmentation; Multi -scale fusion; Attention mechanism
ID FUSION NETWORK
AB With the increasing application of embedded mobile devices in various fields, lightweight real-time semantic segmentation systems have attracted more and more attention. Many current methods have successfully reduced the model's parameters, but they have led to low model accuracy, diminishing their practical value. In recent years, the Transformer architecture has achieved good results in many tasks, effectively capturing long-range dependencies and enhancing accuracy. However, the Transformer is not adept at extracting local features, and the model's computational cost is generally too high, hindering real-time inference implementation. We propose a lightweight semantic segmentation network called LCBFormer-Net, which embeds Transformer units between asymmetric encoders and decoders to fully leverage their advantages. On the encoder side, we design the Lightweight Multi-Fusion Unit (LMFU) and Partition Grouping Shuffle Channel Attention (PGSCA). The former fully utilizes input features, merging information multiple times through multiple branches and employing depthwise convolutions with dilation rate to further obtain sufficient features. The latter includes a lightweight grouped channel attention, better guide feature extraction. The Lightweight Context Broadcast Transformer (LCB Transformer) is the Transformer unit we designed, with a lightweight structure that significantly reduces GPU memory consumption. It also improves self-attention and feed-forward networks, enhancing the model's robustness. The decoder includes the Multi-scale Semantic Information Attention Fusion (MSIAF) module, guiding the fusion of features at three different scales and employing a hybrid attention mechanism with both channel and spatial attention to guide feature extraction. LCBFormer-Net achieves good segmentation results with a parameter count of 0.88 M on multiple challenging datasets with diverse scenes.
C1 [Hu, Kaidi; Xie, Zongxia; Hu, Qinghua] Tianjin Univ, Coll Intelligence & Comp, Tianjin 300350, Peoples R China.
   [Hu, Kaidi; Xie, Zongxia; Hu, Qinghua] Minist Educ, Engn Res Ctr Urban Intelligence & Digital Governan, Tianjin 300350, Peoples R China.
C3 Tianjin University
RP Hu, QH (corresponding author), Tianjin Univ, Coll Intelligence & Comp, Tianjin 300350, Peoples R China.
EM kaidihu@tju.edu.cn; huqinghua@tju.edu.cn
FU National Natural Science Foundation of China [61925602, U23B2049,
   62376194]
FX This research was funded by National Natural Science Foundation of China
   under Grants 61925602, U23B2049, 62376194.
CR [Anonymous], 2020, ARXIV
   Taghanaki SA, 2021, ARTIF INTELL REV, V54, P137, DOI 10.1007/s10462-020-09854-1
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Brostow GJ, 2009, PATTERN RECOGN LETT, V30, P88, DOI 10.1016/j.patrec.2008.04.005
   Chaurasia A, 2017, 2017 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)
   Chen LC, 2017, Arxiv, DOI [arXiv:1706.05587, 10.48550/arXiv.1706.05587, DOI 10.48550/ARXIV.1706.05587]
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Cheng JR, 2022, INT J INTELL SYST, V37, P5617, DOI 10.1002/int.22804
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dai YP, 2022, INT J CONTROL AUTOM, V20, P2702, DOI 10.1007/s12555-021-0430-4
   Dong GS, 2021, IEEE T INTELL TRANSP, V22, P3258, DOI 10.1109/TITS.2020.2980426
   Dong YS, 2023, COMPLEX INTELL SYST, V9, P6177, DOI 10.1007/s40747-023-01063-x
   Elhassan MAM, 2021, EXPERT SYST APPL, V183, DOI 10.1016/j.eswa.2021.115090
   Fan JQ, 2024, IEEE T INTELL TRANSP, V25, P1586, DOI 10.1109/TITS.2023.3313982
   Fan JQ, 2023, IEEE T INTELL VEHICL, V8, P756, DOI 10.1109/TIV.2022.3176860
   Fan ZY, 2023, APPL SCI-BASEL, V13, DOI 10.3390/app13031493
   Feng D, 2021, IEEE T INTELL TRANSP, V22, P1341, DOI 10.1109/TITS.2020.2972974
   Gao GW, 2023, IEEE T MULTIMEDIA, V25, P3273, DOI 10.1109/TMM.2022.3157995
   Gao GW, 2022, IEEE T INTELL TRANSP, V23, P25489, DOI 10.1109/TITS.2021.3098355
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Gould S, 2009, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2009.5459211
   Hao XC, 2021, IMAGE VISION COMPUT, V114, DOI 10.1016/j.imavis.2021.104269
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu KD, 2024, COMPUT GRAPH-UK, V118, P220, DOI 10.1016/j.cag.2023.12.015
   Hu XG, 2023, IMAGE VISION COMPUT, V139, DOI 10.1016/j.imavis.2023.104823
   Hu XG, 2022, APPL INTELL, V52, P580, DOI 10.1007/s10489-021-02446-8
   Hyeon-Woo N., 2023, P IEEECVF INT C COMP, P5807
   Jégou S, 2017, IEEE COMPUT SOC CONF, P1175, DOI 10.1109/CVPRW.2017.156
   Kreso I, 2016, LECT NOTES COMPUT SC, V9796, P64, DOI 10.1007/978-3-319-45886-1_6
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li BN, 2023, PROC CVPR IEEE, P22700, DOI 10.1109/CVPR52729.2023.02174
   Li G., 2019, P BRIT MACH VIS C BM, P259
   Li HC, 2019, PROC CVPR IEEE, P9514, DOI 10.1109/CVPR.2019.00975
   Li YQ, 2022, NEURAL PROCESS LETT, V54, P4647, DOI 10.1007/s11063-022-10740-w
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu K, 2023, SENSORS-BASEL, V23, DOI 10.3390/s23146382
   Lu MX, 2022, IEEE T INTELL TRANSP, V23, P3522, DOI 10.1109/TITS.2020.3037727
   Lu ZS, 2022, IEEE COMPUT SOC CONF, P456, DOI 10.1109/CVPRW56347.2022.00061
   Lv QX, 2022, IEEE T INTELL TRANSP, V23, P4432, DOI 10.1109/TITS.2020.3044672
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Mazhar S, 2023, ENG APPL ARTIF INTEL, V126, DOI 10.1016/j.engappai.2023.107086
   Mehta S, 2018, LECT NOTES COMPUT SC, V11214, P561, DOI 10.1007/978-3-030-01249-6_34
   Neupane B, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13040808
   Orsic M, 2019, PROC CVPR IEEE, P12599, DOI 10.1109/CVPR.2019.01289
   Poudel R. P. K., 2019, ARXIV190204502, P289
   Romera E, 2018, IEEE T INTELL TRANSP, V19, P263, DOI 10.1109/TITS.2017.2750080
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Shen SY, 2023, COMPLEX INTELL SYST, V9, P5975, DOI 10.1007/s40747-023-01054-y
   Shi M, 2023, IEEE T NEUR NET LEAR, V34, P3205, DOI 10.1109/TNNLS.2022.3176493
   Shim JH, 2023, AAAI CONF ARTIF INTE, P2263
   Singha T, 2023, COMPUT VIS IMAGE UND, V235, DOI 10.1016/j.cviu.2023.103795
   Singha T, 2023, PATTERN RECOGN, V140, DOI 10.1016/j.patcog.2023.109557
   Tang XY, 2021, INFORM SCIENCES, V565, P326, DOI 10.1016/j.ins.2021.02.004
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang JW, 2020, APPL INTELL, V50, P1045, DOI 10.1007/s10489-019-01587-1
   Wang LB, 2022, ISPRS J PHOTOGRAMM, V190, P196, DOI 10.1016/j.isprsjprs.2022.06.008
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/ISQED48828.2020.9137057, 10.1109/isqed48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang Y, 2019, IEEE IMAGE PROC, P1860, DOI [10.1109/icip.2019.8803154, 10.1109/ICIP.2019.8803154]
   Weng X, 2022, IEEE T INTELL TRANSP, V23, P17224, DOI 10.1109/TITS.2022.3150350
   Woo S., 2018, BRIT MACHINE VISION, V2018, P147, DOI DOI 10.48550/ARXIV.1807.06514
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu Y, 2022, APPL INTELL, V52, P3319, DOI 10.1007/s10489-021-02603-z
   Xia X., 2023, CHIN C PATT REC COMP, P372
   Xiangtai Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P775, DOI 10.1007/978-3-030-58452-8_45
   Xie E., 2021, ADV NEURAL INF PROCE, V34, P12077
   Xu GA, 2023, IEEE T INTELL TRANSP, V24, P15897, DOI 10.1109/TITS.2023.3248089
   Yan XC, 2021, DISPLAYS, V70, DOI 10.1016/j.displa.2021.102082
   Yi QM, 2023, NEURAL PROCESS LETT, V55, P6425, DOI 10.1007/s11063-023-11145-z
   Yu CQ, 2021, INT J COMPUT VISION, V129, P3051, DOI 10.1007/s11263-021-01515-2
   Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20
   Yu Wang, 2019, Pattern Recognition and Computer Vision. Second Chinese Conference, PRCV 2019. Proceedings. Lecture Notes in Computer Science (LNCS 11858), P41, DOI 10.1007/978-3-030-31723-2_4
   Zhang QL, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P2235, DOI 10.1109/ICASSP39728.2021.9414568
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11207, P418, DOI 10.1007/978-3-030-01219-9_25
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhou Q., 2024, IEEE Trans Multimed, P1
   Zhou Q, 2024, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2024.3376563
   Zhou Q, 2020, APPL SOFT COMPUT, V96, DOI 10.1016/j.asoc.2020.106682
NR 82
TC 0
Z9 0
U1 1
U2 1
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUN
PY 2024
VL 146
AR 105053
DI 10.1016/j.imavis.2024.105053
EA MAY 2024
PG 14
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA SY8A2
UT WOS:001238090400001
DA 2024-08-05
ER

PT J
AU Kang, M
   Kang, M
   Lee, SW
   Kim, S
AF Kang, Minsoo
   Kang, Minkoo
   Lee, Seong-Whan
   Kim, Suhyun
TI Mixup Mask Adaptation: Bridging the gap between input saliency and
   representations via attention mechanism in feature mixup
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Regularization; Data augmentation; Mixup
ID NETWORKS
AB The inherent complexity and extensive architecture of deep neural networks often lead to overfitting, compromising their ability to generalize to new, unseen data. One of the regularization techniques, data augmentation, is now considered vital to alleviate this, and mixup, which blends pairs of images and labels, has proven effective in enhancing model generalization. Recently, incorporating saliency in mixups has shown performance gains by retaining salient regions in mixed results. While these methods have become mainstream at the input level, their applications at the feature level remain under-explored. Our observations indicate that outcomes from naive applications of input saliency-based methods did not consistently lead to enhancements in performance. In this paper, we attribute these observations primarily to two challenges: 'Hard Boundary Issue' and 'Saliency Mismatch.' The Hard Boundary Issue describes a situation where masks with distinct, sharp edges work well at the input level, but lead to unintended distortions in the deeper layers. The Saliency Mismatch points to the disparity between saliency masks generated from input images and the saliency of feature maps. To tackle these challenges, we present a novel method called 'attention-based mixup mask adaptation' (MMA). This approach employs an attention mechanism to effectively adapt mixup masks, which are designed to maximize saliency at the input level, for feature augmentation purposes. We reduce the Saliency Mismatch problem by incorporating the spatial significance of the feature map into the mixup mask. Additionally, we address the Hard Boundary Issue by applying softmax to smoothen the adjusted mixup mask. Through comprehensive experiments, we validate our observations and confirm the effectiveness of applying MMA to saliency-aware mixup approaches at the feature level, as evidenced by the performance improvements on multiple benchmarks and the robustness improvements against corruption and deformation.
C1 [Kang, Minsoo; Kang, Minkoo; Kim, Suhyun] Korea Inst Sci & Technol, Ctr Artificial Intelligence, Hwarang Ro 14-5, Seoul 02792, South Korea.
   [Kang, Minsoo; Lee, Seong-Whan] Korea Univ, Dept Artificial Intelligence, Anam Ro 145, Seoul 02841, South Korea.
C3 Korea Institute of Science & Technology (KIST); Korea University
RP Kim, S (corresponding author), Korea Inst Sci & Technol, Ctr Artificial Intelligence, Hwarang Ro 14-5, Seoul 02792, South Korea.; Lee, SW (corresponding author), Korea Univ, Dept Artificial Intelligence, Anam Ro 145, Seoul 02841, South Korea.
EM minsookang@korea.ac.kr; mit240083@gmail.com; sw.lee@korea.ac.kr;
   dr.suhyun.kim@gmail.com
FU Institute of Information & communi-cations Technology Planning &
   Evaluation (IITP) - Korea Government (MSIT) [2021-0-00456]
FX <B>Acknowledgement</B> This work was supported by Institute of
   Information & communi-cations Technology Planning & Evaluation (IITP)
   grant funded by the Korea Government (MSIT) (No. 2021-0-00456,
   Development of Ultra-high Speech Quality Technology for Remote
   Multi-speaker Conference System) .
CR An J, 2022, PLOS ONE, V17, DOI 10.1371/journal.pone.0274767
   Bang JS, 2022, IEEE T NEUR NET LEAR, V33, P3038, DOI 10.1109/TNNLS.2020.3048385
   Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89
   Chrabaszcz P, 2017, Arxiv, DOI arXiv:1707.08819
   Dabouei A, 2021, PROC CVPR IEEE, P13789, DOI 10.1109/CVPR46437.2021.01358
   Guo HY, 2019, AAAI CONF ARTIF INTE, P3714
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hendrycks D, 2018, P INT C LEARN REPR
   Huang SL, 2021, AAAI CONF ARTIF INTE, V35, P1628
   Kang M., 2024, P AAAI C ART INT, V38, P2705
   Kang M., 2023, 2023 P AAAI C ART IN, V37, P1096, DOI DOI 10.1609/AAAI.V37I1.25191
   Kim J., 2021, 9 INT C LEARN REPR I
   Kim JH, 2020, PR MACH LEARN RES, V119
   Kim Y, 2022, AAAI CONF ARTIF INTE, P1201
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Krizhevsky A., 2012, Learning multiple layers of features from tiny images, DOI DOI 10.1145/3079856.3080246
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lad BV, 2023, IMAGE VISION COMPUT, V137, DOI 10.1016/j.imavis.2023.104748
   Li BY, 2021, PROC CVPR IEEE, P12378, DOI 10.1109/CVPR46437.2021.01220
   Li SY, 2023, Arxiv, DOI arXiv:2209.04851
   Liu KL, 2020, IMAGE VISION COMPUT, V104, DOI 10.1016/j.imavis.2020.104005
   Maji S, 2013, Arxiv, DOI arXiv:1306.5151
   Ng A.Y., 2004, ICML, P615, DOI 10.1145/1015330.1015435
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Seo JW, 2021, NEURAL NETWORKS, V138, P140, DOI 10.1016/j.neunet.2021.02.007
   Simonyan K, 2014, Arxiv, DOI [arXiv:1312.6034, DOI 10.48550/ARXIV.1312.6034]
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Vaswani A, 2017, ADV NEUR IN, V30
   Venkataramanan S, 2022, PROC CVPR IEEE, P19152, DOI 10.1109/CVPR52688.2022.01858
   Verma V, 2019, PR MACH LEARN RES, V97
   Wah C., 2011, Tech. Rep. CNS-TR-2011-001
   Wang J, 2017, Convolutional Neural Networks Vis. Recognit, V11, P1, DOI DOI 10.48550/ARXIV.1712.04621
   Wang Y, 2023, IMAGE VISION COMPUT, V132, DOI 10.1016/j.imavis.2023.104647
   Wightman R., 2021, Comput Vis Pattern Recognit
   Yang L., 2022, Adv. Neural Inf. Process. Syst., V35, P8427
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Zhang H., 2018, INT C LEARNING REPRE
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
NR 43
TC 0
Z9 0
U1 1
U2 1
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUN
PY 2024
VL 146
AR 105013
DI 10.1016/j.imavis.2024.105013
EA APR 2024
PG 9
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA SA6U2
UT WOS:001231786400001
OA hybrid
DA 2024-08-05
ER

PT J
AU Wang, MJ
   Zhang, KK
   Wei, HA
   Chen, WL
   Zhao, TS
AF Wang, Mingjie
   Zhang, Keke
   Wei, Hongan
   Chen, Weiling
   Zhao, Tiesong
TI Underwater image quality optimization: Researches, challenges, and
   future trends
SO IMAGE AND VISION COMPUTING
LA English
DT Review
DE Underwater image enhancement; Underwater image restoration; Image
   quality assessment; Underwater image datasets
ID ENHANCEMENT; COLOR; RESTORATION; VISIBILITY; RETINEX; SPACE; MODEL
AB Underwater images serve as crucial mediums for conveying marine information. Nevertheless, due to the inherent complexity of the underwater environment, underwater images often suffer from various quality degradation phenomena such as color deviation, low contrast, and non-uniform illumination. These degraded underwater images fail to meet the requirements of underwater computer vision applications. Consequently, effective quality optimization of underwater images is of paramount research and analytical value. Based on whether they rely on underwater physical imaging models, underwater image quality optimization techniques can be categorized into underwater image enhancement and underwater image restoration methods. This paper provides a comprehensive review of underwater image enhancement and restoration algorithms, accompanied by a brief introduction to underwater imaging model. Then, we systematically analyze publicly available underwater image datasets and commonly-used quality assessment methodologies. Furthermore, extensive experimental comparisons are carried out to assess the performance of underwater image optimization algorithms and their practical impact on high-level vision tasks. Finally, the challenges and future development trends in this field are discussed. We hope that the efforts made in this paper will provide valuable references for future research and contribute to the innovative advancement of underwater image optimization.
C1 [Wang, Mingjie; Zhang, Keke; Wei, Hongan; Chen, Weiling; Zhao, Tiesong] Fuzhou Univ, Key Lab Intelligent Metro Univ Fujian, Fuzhou 350108, Peoples R China.
C3 Fuzhou University
RP Chen, WL (corresponding author), Fuzhou Univ, Key Lab Intelligent Metro Univ Fujian, Fuzhou 350108, Peoples R China.
EM weiling.chen@fzu.edu.cn
OI Wang, Mingjie/0009-0007-4627-1788
FU National Natural Science Foundation of China [62171134]; Natural Science
   Foundation of Fujian Province, China [2022J02015, 2022J05117]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62171134 and in part by Natural Science
   Foundation of Fujian Province, China under Grant 2022J02015 and
   2022J05117.
CR Agaian SS, 2001, IEEE T IMAGE PROCESS, V10, P367, DOI 10.1109/83.908502
   Akkaynak D, 2019, PROC CVPR IEEE, P1682, DOI 10.1109/CVPR.2019.00178
   Ancuti CO, 2018, IEEE T IMAGE PROCESS, V27, P379, DOI 10.1109/TIP.2017.2759252
   Ancuti C, 2012, PROC CVPR IEEE, P81, DOI 10.1109/CVPR.2012.6247661
   [Anonymous], 2012, Recommendation ITU-R BT 500 (13)
   Anwar S, 2020, SIGNAL PROCESS-IMAGE, V89, DOI 10.1016/j.image.2020.115978
   Berman D, 2021, IEEE T PATTERN ANAL, V43, P2822, DOI 10.1109/TPAMI.2020.2977624
   Cai XW, 2024, IEEE J OCEANIC ENG, V49, P226, DOI 10.1109/JOE.2023.3245760
   Carlevaris-Bianco N, 2010, OCEANS-IEEE
   Chai S, 2022, INT CONF ACOUST SPEE, P2774, DOI 10.1109/ICASSP43922.2022.9746292
   Chang L, 2023, ISPRS J PHOTOGRAMM, V196, P415, DOI 10.1016/j.isprsjprs.2023.01.007
   Chen XY, 2019, IEEE T IND ELECTRON, V66, P9350, DOI 10.1109/TIE.2019.2893840
   Chiang JY, 2012, IEEE T IMAGE PROCESS, V21, P1756, DOI 10.1109/TIP.2011.2179666
   Cutter G, 2015, 2015 IEEE WINTER APPLICATIONS AND COMPUTER VISION WORKSHOPS (WACVW), P57, DOI 10.1109/WACVW.2015.11
   Drews P, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P825, DOI 10.1109/ICCVW.2013.113
   Dudhane A, 2020, IEEE SIGNAL PROC LET, V27, P675, DOI 10.1109/LSP.2020.2988590
   Dwivedi P, 2023, IMAGE VISION COMPUT, V136, DOI 10.1016/j.imavis.2023.104747
   Ebner M., 2007, Color Constancy, V7
   Ebrahimi Moghadam A., 2015, Majlesi J. Electr. Eng., V9
   Fabbri C, 2018, IEEE INT CONF ROBOT, P7159
   Fazal Shaad, 2021, 2021 7th International Conference on Signal Processing and Communication (ICSC), P261, DOI 10.1109/ICSC53193.2021.9673286
   Fu ZQ, 2022, LECT NOTES COMPUT SC, V13678, P465, DOI 10.1007/978-3-031-19797-0_27
   Fu ZQ, 2022, AAAI CONF ARTIF INTE, P643
   Galdran A, 2015, J VIS COMMUN IMAGE R, V26, P132, DOI 10.1016/j.jvcir.2014.11.006
   Guo YC, 2020, IEEE J OCEANIC ENG, V45, P862, DOI 10.1109/JOE.2019.2911447
   Han M, 2020, IEEE T SYST MAN CY-S, V50, P1820, DOI 10.1109/TSMC.2017.2788902
   Hancheng Zhu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14131, DOI 10.1109/CVPR42600.2020.01415
   Hao JY, 2023, IEEE SIGNAL PROC LET, V30, P120, DOI 10.1109/LSP.2022.3232035
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Henke B, 2013, INT SYMP IMAGE SIG, P20
   Hong Lin, 2023, IEEE Trans Image Process, VPP, DOI 10.1109/TIP.2023.3266163
   Huang DM, 2018, LECT NOTES COMPUT SC, V10704, P453, DOI 10.1007/978-3-319-73603-7_37
   HUMMEL R, 1977, COMPUT VISION GRAPH, V6, P184, DOI 10.1016/S0146-664X(77)80011-7
   Hung-Yu Yang, 2011, Proceedings of the 2011 2nd International Conference on Innovations in Bio-Inspired Computing and Applications (IBICA 2011), P17, DOI 10.1109/IBICA.2011.9
   Iqbal M, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2020.3021134
   Islam MJ, 2022, ROBOT SCI SYS
   Islam MJ, 2020, IEEE ROBOT AUTOM LET, V5, P3227, DOI 10.1109/LRA.2020.2974710
   Islam MJ, 2020, Arxiv, DOI arXiv:2002.01155
   Jian MW, 2021, SIGNAL PROCESS-IMAGE, V91, DOI 10.1016/j.image.2020.116088
   Jiang LH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4259, DOI 10.1145/3474085.3475563
   Jiang NF, 2022, IEEE T MULTIMEDIA, V24, DOI 10.1109/TMM.2021.3115442
   Jiang QP, 2022, IEEE T CIRC SYST VID, V32, P5959, DOI 10.1109/TCSVT.2022.3164918
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272
   Ke JJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5128, DOI 10.1109/ICCV48922.2021.00510
   LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108
   Lee S, 2016, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-016-0104-y
   Li BY, 2021, INT J COMPUT VISION, V129, P1754, DOI 10.1007/s11263-021-01431-5
   Li CY, 2022, IEEE IMAGE PROC, P4148, DOI 10.1109/ICIP46576.2022.9897840
   Li CY, 2021, IEEE T IMAGE PROCESS, V30, P4985, DOI 10.1109/TIP.2021.3076367
   Li CY, 2020, IEEE T IMAGE PROCESS, V29, P4376, DOI 10.1109/TIP.2019.2955241
   Li CY, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107038
   Li J, 2018, IEEE ROBOT AUTOM LET, V3, P387, DOI 10.1109/LRA.2017.2730363
   Liang Z, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3227548
   Liang Z, 2022, IEEE T CIRC SYST VID, V32, P4879, DOI 10.1109/TCSVT.2021.3114230
   Liu Chao, 2010, 2010 2nd International Conference on Computer Engineering and Technology (ICCET), P35, DOI 10.1109/ICCET.2010.5485339
   Liu CW, 2021, IEEE INT CONF MULTI, DOI 10.1109/ICMEW53276.2021.9455997
   Liu K, 2021, MULTIMED TOOLS APPL, V80, P19421, DOI 10.1007/s11042-021-10740-3
   Liu RS, 2020, IEEE T CIRC SYST VID, V30, P4861, DOI 10.1109/TCSVT.2019.2963772
   Liu W, 2018, PROC SPIE, V10850, DOI 10.1117/12.2505587
   LIU YC, 1995, IEEE T CONSUM ELECTR, V41, P460, DOI 10.1109/30.468045
   Lu JX, 2021, IEEE J OCEANIC ENG, V46, P1228, DOI 10.1109/JOE.2021.3077692
   Ma JP, 2021, IEEE T IMAGE PROCESS, V30, P3650, DOI 10.1109/TIP.2021.3064195
   MCLAREN K, 1976, J SOC DYERS COLOUR, V92, P338
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Muniraj M, 2021, NEUROCOMPUTING, V460, P211, DOI 10.1016/j.neucom.2021.07.003
   Naik A, 2021, AAAI CONF ARTIF INTE, V35, P15853
   Panetta K, 2016, IEEE J OCEANIC ENG, V41, P541, DOI 10.1109/JOE.2015.2469915
   Parihar A.S., P 2020 5 INT C COMM, P766, DOI [10.1109/ICCES48766.2020.9138037, DOI 10.1109/ICCES48766.2020.9138037]
   Peng LT, 2023, IEEE T IMAGE PROCESS, V32, P3066, DOI 10.1109/TIP.2023.3276332
   Peng YT, 2018, IEEE T IMAGE PROCESS, V27, P2856, DOI 10.1109/TIP.2018.2813092
   Peng YT, 2017, IEEE T IMAGE PROCESS, V26, P1579, DOI 10.1109/TIP.2017.2663846
   Perez J, 2017, LECT NOTES COMPUT SC, V10338, P183, DOI 10.1007/978-3-319-59773-7_19
   Qi Q, 2022, IEEE T IMAGE PROCESS, V31, P6816, DOI 10.1109/TIP.2022.3216208
   Qiao NZ, 2023, IEEE T CIRC SYST VID, V33, P5391, DOI 10.1109/TCSVT.2023.3253898
   Raveendran S, 2021, ARTIF INTELL REV, V54, P5413, DOI 10.1007/s10462-021-10025-z
   Roser M, 2014, IEEE INT CONF ROBOT, P3840, DOI 10.1109/ICRA.2014.6907416
   Schechner YY, 2005, IEEE J OCEANIC ENG, V30, P570, DOI 10.1109/JOE.2005.850871
   Sharma S, 2022, ENG SCI TECHNOL, V32, DOI 10.1016/j.jestch.2021.09.005
   Song W, 2020, IEEE T BROADCAST, V66, P153, DOI 10.1109/TBC.2019.2960942
   Song W, 2018, LECT NOTES COMPUT SC, V11164, P678, DOI 10.1007/978-3-030-00776-8_62
   Su SL, 2020, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR42600.2020.00372
   Treibitz T, 2009, IEEE T PATTERN ANAL, V31, P385, DOI 10.1109/TPAMI.2008.85
   Van de Weijer J, 2007, IEEE T IMAGE PROCESS, V16, P2207, DOI 10.1109/TIP.2007.901808
   Vasamsetti S, 2017, OCEAN ENG, V141, P88, DOI 10.1016/j.oceaneng.2017.06.012
   Wang SQ, 2015, IEEE SIGNAL PROC LET, V22, P2387, DOI 10.1109/LSP.2015.2487369
   Wang Y, 2018, COMPUT ELECTR ENG, V70, P904, DOI 10.1016/j.compeleceng.2017.12.006
   Wang Y, 2017, IEEE IMAGE PROC, P1382, DOI 10.1109/ICIP.2017.8296508
   Wang YD, 2021, SIGNAL PROCESS-IMAGE, V96, DOI 10.1016/j.image.2021.116250
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZY, 2023, IEEE T IMAGE PROCESS, V32, P1442, DOI 10.1109/TIP.2023.3244647
   Wang ZY, 2023, IEEE T CIRC SYST VID, V33, P1123, DOI 10.1109/TCSVT.2022.3212788
   Wu QB, 2020, IEEE T CIRC SYST VID, V30, P3883, DOI 10.1109/TCSVT.2020.2972566
   Xie J, 2022, IEEE T CIRC SYST VID, V32, P3514, DOI 10.1109/TCSVT.2021.3115791
   Xu HP, 2023, INT J MACH LEARN CYB, V14, P725, DOI 10.1007/s13042-022-01659-8
   Yan HF, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2022.3229439
   Yan SZ, 2023, IEEE T IMAGE PROCESS, V32, P5004, DOI 10.1109/TIP.2023.3309408
   Yang M, 2015, IEEE T IMAGE PROCESS, V24, P6062, DOI 10.1109/TIP.2015.2491020
   Zhang CX, 2018, PROTEINS, V86, P136, DOI 10.1002/prot.25414
   Zhang DH, 2023, EXPERT SYST APPL, V231, DOI 10.1016/j.eswa.2023.120842
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang WD, 2022, IEEE T IMAGE PROCESS, V31, P3997, DOI 10.1109/TIP.2022.3177129
   Zhang WD, 2024, IEEE T CIRC SYST VID, V34, P2469, DOI 10.1109/TCSVT.2023.3299314
   Zhang WD, 2023, IEEE SIGNAL PROC LET, V30, P229, DOI 10.1109/LSP.2023.3255005
   Zhao XW, 2015, OCEAN ENG, V94, P163, DOI 10.1016/j.oceaneng.2014.11.036
   Zheng YN, 2022, IEEE T IMAGE PROCESS, V31, P5456, DOI 10.1109/TIP.2022.3196815
   Zhou JC, 2023, INT J COMPUT VISION, DOI 10.1007/s11263-023-01853-3
   Zhou JC, 2023, IEEE J OCEANIC ENG, V48, P1322, DOI 10.1109/JOE.2023.3275615
   Zhou JC, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3293912
   Zhou JC, 2023, ENG APPL ARTIF INTEL, V121, DOI 10.1016/j.engappai.2023.105946
   Zhou JC, 2022, ENG APPL ARTIF INTEL, V111, DOI 10.1016/j.engappai.2022.104785
   Zhou JC, 2022, COMPUT ELECTR ENG, V100, DOI 10.1016/j.compeleceng.2022.107898
   Zhou JC, 2022, APPL INTELL, V52, P16435, DOI 10.1007/s10489-022-03275-z
   Zhou JC, 2021, OPT EXPRESS, V29, P29864, DOI 10.1364/OE.427839
   Zhou JC, 2021, OPT EXPRESS, V29, P28228, DOI 10.1364/OE.432900
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhuang PX, 2022, IEEE T IMAGE PROCESS, V31, P5442, DOI 10.1109/TIP.2022.3196546
   Zuiderveld K., 1994, Graphics gems, DOI 10.1016/b978-0-12-336156-1.50061-6
NR 118
TC 0
Z9 0
U1 28
U2 28
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUN
PY 2024
VL 146
AR 104995
DI 10.1016/j.imavis.2024.104995
EA APR 2024
PG 21
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA QX8B4
UT WOS:001224247300001
DA 2024-08-05
ER

PT J
AU Cai, SD
   Wakaki, R
   Nobuhara, S
   Nishino, K
AF Cai, Sudong
   Wakaki, Ryosuke
   Nobuhara, Shohei
   Nishino, Ko
TI RGB road scene material segmentation
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE RGB road scene material segmentation; Self-attention mechanism;
   Multi-level feature fusion
AB We introduce RGB road scene material segmentation, i.e., per-pixel segmentation of materials in real-world driving views with pure RGB images, as a novel computer vision task by building a benchmark dataset and by deriving a new method. Our dataset, KITTI-Materials, is based on the well-established KITTI dataset and consists of 1000 frames covering 24 different road scenes of urban/suburban landscapes, carefully annotated with one of 20 material categories for every pixel. It is the first dataset for RGB material segmentation in real driving scenes. Through careful analysis of KITTI-Materials, we identify the extraction and fusion of texture and image context as the key to accurate modeling of road scene material appearance. For this, we introduce Road scene Material Segmentation Network (RMSNet) as a baseline method for this challenging task. RMSNet encodes multi-scale hierarchical features with efficient Transformer layers. We construct the decoder of RMSNet based on a novel efficient self-attention model, which we refer to as SAMixer which adaptively fuses texture and context cues across multiple feature levels. Extensive experiments on KITTI-Materials validate the effectiveness of our RMSNet. We believe our work lays a solid foundation for further studies on RGB road scene material segmentation.
C1 [Cai, Sudong; Wakaki, Ryosuke; Nobuhara, Shohei; Nishino, Ko] Kyoto Univ, Grad Sch Informat, Yoshida Honmachi,Sakyo Ku, Kyoto 6068501, Japan.
C3 Kyoto University
RP Cai, SD (corresponding author), Kyoto Univ, Grad Sch Informat, Yoshida Honmachi,Sakyo Ku, Kyoto 6068501, Japan.
EM scai@vision.ist.i.kyoto-u.ac.jp
FU JSPS [20H05951, 21H04893]; JST [JPMJCR20G7]; SenseTime Japan
FX This work was in part supported by JSPS 20H05951, 21H04893, JST
   JPMJCR20G7, and SenseTime Japan.
CR Bell S, 2015, PROC CVPR IEEE, P3479, DOI 10.1109/CVPR.2015.7298970
   Brown M, 2018, PROC SPIE, V10636, DOI 10.1117/12.2304403
   Cai Sudong, 2023, Computer Vision - ACCV 2022: 16th Asian Conference on Computer Vision, Proceedings. Lecture Notes in Computer Science (13842), P256, DOI 10.1007/978-3-031-26284-5_16
   Chen LC, 2017, Arxiv, DOI [arXiv:1706.05587, 10.48550/arXiv.1706.05587, DOI 10.48550/ARXIV.1706.05587]
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen LC, 2016, PROC CVPR IEEE, P3640, DOI 10.1109/CVPR.2016.396
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Demir I, 2018, IEEE COMPUT SOC CONF, P172, DOI 10.1109/CVPRW.2018.00031
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding MY, 2022, LECT NOTES COMPUT SC, V13684, P74, DOI 10.1007/978-3-031-20053-3_5
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Graham B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12239, DOI 10.1109/ICCV48922.2021.01204
   Gu JQ, 2022, PROC CVPR IEEE, P12084, DOI 10.1109/CVPR52688.2022.01178
   Han D, 2017, PROC CVPR IEEE, P6307, DOI 10.1109/CVPR.2017.668
   Han K, 2021, ADV NEUR IN
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hendrycks D, 2020, Arxiv, DOI arXiv:1606.08415
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Huiyu Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P108, DOI 10.1007/978-3-030-58548-8_7
   Krahenbuhl P., 2013, P 30 INT C MACH LEAR, P513
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Liang YP, 2022, PROC CVPR IEEE, P19768, DOI 10.1109/CVPR52688.2022.01918
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu Z, 2022, PROC CVPR IEEE, P11966, DOI 10.1109/CVPR52688.2022.01167
   Mingxing Tan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10778, DOI 10.1109/CVPR42600.2020.01079
   Neuhold G, 2017, IEEE I CONF COMP VIS, P5000, DOI 10.1109/ICCV.2017.534
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Purri M, 2019, Arxiv, DOI [arXiv:1904.08537, DOI arXiv:1904.08537.null]
   Ramachandran P, 2019, ADV NEUR IN, V32
   Reda FA, 2018, LECT NOTES COMPUT SC, V11211, P747, DOI 10.1007/978-3-030-01234-2_44
   Schwartz G, 2017, Arxiv, DOI arXiv:1611.09394
   Schwartz G, 2020, IEEE T PATTERN ANAL, V42, P1981, DOI 10.1109/TPAMI.2019.2907850
   Schwartz G, 2015, PROC CVPR IEEE, P3565, DOI 10.1109/CVPR.2015.7298979
   Schwartz G, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P883, DOI 10.1109/ICCVW.2013.121
   Sungha Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9370, DOI 10.1109/CVPR42600.2020.00939
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wu HP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P22, DOI 10.1109/ICCV48922.2021.00009
   Xiao TT, 2018, LECT NOTES COMPUT SC, V11209, P432, DOI 10.1007/978-3-030-01228-1_26
   Xie E., 2021, ADV NEURAL INF PROCE, V34, P12077
   Xue J, 2022, IEEE T PATTERN ANAL, V44, P1205, DOI 10.1109/TPAMI.2020.3025121
   Xue J, 2020, INT GEOSCI REMOTE SE, P6746, DOI 10.1109/IGARSS39084.2020.9324625
   Xue J, 2017, PROC CVPR IEEE, P6940, DOI 10.1109/CVPR.2017.734
   Yang TJ, 2019, Arxiv, DOI arXiv:1902.05093
   Yu WH, 2022, PROC CVPR IEEE, P10809, DOI 10.1109/CVPR52688.2022.01055
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhang H, 2019, PROC CVPR IEEE, P548, DOI 10.1109/CVPR.2019.00064
   Zhang H, 2018, PROC CVPR IEEE, P7151, DOI 10.1109/CVPR.2018.00747
   Zhang H, 2017, PROC CVPR IEEE, P2896, DOI 10.1109/CVPR.2017.309
   Zhang H, 2015, PROC CVPR IEEE, P3071, DOI 10.1109/CVPR.2015.7298926
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhou JK, 2021, PROC CVPR IEEE, P6643, DOI 10.1109/CVPR46437.2021.00658
   Zhu Xizhou, 2021, INT C LEARN REPR ICL
   Zhu Y, 2019, PROC CVPR IEEE, P8848, DOI 10.1109/CVPR.2019.00906
NR 61
TC 1
Z9 1
U1 1
U2 1
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD MAY
PY 2024
VL 145
AR 104970
DI 10.1016/j.imavis.2024.104970
EA MAR 2024
PG 19
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA PQ2Z3
UT WOS:001215495300001
DA 2024-08-05
ER

PT J
AU Yar, H
   Khan, ZA
   Rida, I
   Ullah, W
   Kim, MJ
   Baik, SW
AF Yar, Hikmat
   Khan, Zulfiqar Ahmad
   Rida, Imad
   Ullah, Waseem
   Kim, Min Je
   Baik, Sung Wook
TI An efficient deep learning architecture for effective fire detection in
   smart surveillance
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Aerial view; Attention mechanism; Convolutional neural network;
   Environment monitoring; Fire detection; Remote sensing
ID CONVOLUTIONAL NEURAL-NETWORKS; COLOR; SHAPE
AB The threat of fire is pervasive, poses significant risks to the environment, and may include potential fatalities, property devastation, and socioeconomic disruption. Successfully mitigating these risks relies on the prompt identification of fires, a process in which soft computing methodologies play a pivotal role. Although, these fire detection methodologies neglected to explore the relationships among fire-indicative features, which are important to enable a model to learn more representative and robust features in remote sensing scenarios. In the context of small fire detection from aerial view using satellite imagery or unmanned arial vehicle (UAVs) presents challenges to capture rich spatial detail, hinder the model ability for accurate fire scene classification. Furthermore, it is significant to manage model complexity effectively to facilitate deployment on UAVs for fast and accurate responses in an emergency situation. To cope with these challenges, we propose an advanced model integrated a modified soft attention mechanism (MSAM) and a 3D convolution operation with a MobileNet architecture to overcome obstacles related to optimising features and controlling model complexity. The MSAM enabling the model to selectively emphasise essential features during the training process which acts as a selective filter. This adaptive attention mechanism enhances sensitivity and allowing the model to prioritise relevant patterns for accurate fire detection. Concurrently, the integration of a 3D convolutional operation extends the model spatial awareness, to capture intricate details across multiple scales, and particularly in small regions observed from aerial viewpoints. Benchmark evaluations of the proposed model over the FD, DFAN, and ADSF datasets reveal superior performance with enhanced accuracy (ACR) compared to existing methods. Our model surpassed the state-of-the-art models with an average ACR improvement of 0.54%, 2.64%, and 1.20% on the FD, ADSF, and DFAN datasets, respectively. Furthermore, the use of an explainable artificial intelligence (XAI) technique enhances the validation of the model visual emphasis on critical regions of the image, providing valuable insights into its decision-making process.
C1 [Yar, Hikmat; Khan, Zulfiqar Ahmad; Ullah, Waseem; Kim, Min Je; Baik, Sung Wook] Sejong Univ, Seoul 143747, South Korea.
   [Rida, Imad] Univ Technol Compiegne, Ctr Rech Royallieu, Lab Biomecan & Bioingn UMR7338, Compiegne, France.
C3 Sejong University; Universite de Technologie de Compiegne
RP Baik, SW (corresponding author), Sejong Univ, Seoul 143747, South Korea.
EM sbaik@sejong.ac.kr
OI gim, minje/0009-0007-8839-5260
FU National Research Foundation of Korea (NRF) - Korea government (MSIT)
   [2023R1A2C1005788]
FX This work was supported by National Research Foundation of Korea (NRF)
   grant funded by the Korea government (MSIT) , Grant/Award Number:
   (2023R1A2C1005788) .
CR Ali N, 2023, J MATER ENG PERFORM, DOI 10.1007/s11665-023-08463-7
   Avenash R, 2019, VISAPP: PROCEEDINGS OF THE 14TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS, VOL 4, P413, DOI 10.5220/0007469604130420
   Borges PVK, 2010, IEEE T CIRC SYST VID, V20, P721, DOI 10.1109/TCSVT.2010.2045813
   Cao XH, 2024, J BUILD ENG, V84, DOI 10.1016/j.jobe.2024.108686
   Çelik T, 2007, INT CONF ACOUST SPEE, P1205
   Chen J, 2010, BUILD ENVIRON, V45, P1113, DOI 10.1016/j.buildenv.2009.10.017
   Chen TH, 2004, IEEE IMAGE PROC, P1707
   Chino DYT, 2015, SIBGRAPI, P95, DOI 10.1109/SIBGRAPI.2015.19
   Csapaiova N., 2021, Transp. Res. Procedia, V55, P1704
   Dalal S, 2024, ENVIRON SCI POLLUT R, DOI 10.1007/s11356-024-32023-8
   Pereira GHD, 2021, ISPRS J PHOTOGRAMM, V178, P171, DOI 10.1016/j.isprsjprs.2021.06.002
   Deng Z, 2022, IET IMAGE PROCESS, V16, P2338, DOI 10.1049/ipr2.12491
   Dilshad N., 2023, Comput. Syst. Sci. Eng., V46, P749, DOI [10.32604/csse.2023.034475, DOI 10.32604/CSSE.2023.034475]
   Dimitropoulos K, 2015, IEEE T CIRC SYST VID, V25, P339, DOI 10.1109/TCSVT.2014.2339592
   Dou Z, 2024, FIRE TECHNOL, V60, P135, DOI 10.1007/s10694-023-01492-7
   Douglas H, 2023, CURR ISS CRIM JUSTIC, V35, P27, DOI 10.1080/10345329.2022.2095794
   Elfwing S, 2018, NEURAL NETWORKS, V107, P3, DOI 10.1016/j.neunet.2017.12.012
   Feng J, 2024, EXPERT SYST APPL, V240, DOI 10.1016/j.eswa.2023.122494
   Filkov AI, 2020, J SAF SCI RESIL, V1, P44, DOI 10.1016/j.jnlssr.2020.06.009
   Foggia P, 2015, IEEE T CIRC SYST VID, V25, P1545, DOI 10.1109/TCSVT.2015.2392531
   Gubbi J, 2009, FIRE SAFETY J, V44, P1110, DOI 10.1016/j.firesaf.2009.08.003
   Ha C., 2012, 2012 6 INT C COMPL I
   Harkat H, 2023, EXPERT SYST APPL, V212, DOI 10.1016/j.eswa.2022.118594
   Hennighausen H., 2023, Catastrophic fires, human displacement, and real estate prices in California
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang LD, 2022, ENG APPL ARTIF INTEL, V110, DOI 10.1016/j.engappai.2022.104737
   Jan H., 2020, 2020 1 INT C SMART S
   Khan A., 2024, P IEEE CVF WINT C AP
   Khan H, 2022, AGRICULTURE-BASEL, V12, DOI 10.3390/agriculture12081226
   Khan M., 2024, An Efficient Violence Detection Approach for Smart Cities Surveillance System
   Khan M., 2023, Intelligent Multimedia Signal Processing for Smart Ecosystems, P307
   Khan M, 2024, EXPERT SYST APPL, V245, DOI 10.1016/j.eswa.2023.122946
   Khan R, 2022, IET COMMUN, V16, P497, DOI 10.1049/cmu2.12269
   Khan T, 2023, NEURAL COMPUT APPL, DOI 10.1007/s00521-023-09298-y
   Khan ZA, 2024, APPL ENERG, V356, DOI 10.1016/j.apenergy.2023.122339
   Khan ZA, 2024, IEEE T IND INFORM, V20, P5750, DOI 10.1109/TII.2023.3335453
   Khan ZA, 2022, ENG APPL ARTIF INTEL, V116, DOI 10.1016/j.engappai.2022.105403
   Kim B, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9142862
   l'Ell e clignera, 2021, 2020= 2020 fire statistical yearbook
   Lee CY, 2012, INT J INNOV COMPUT I, V8, P4749
   Li SB, 2020, IEEE T IMAGE PROCESS, V29, P8467, DOI 10.1109/TIP.2020.3016431
   Mahaveerakannan R, 2023, COMPUT COMMUN, V211, P37, DOI 10.1016/j.comcom.2023.08.020
   Majid S, 2022, EXPERT SYST APPL, V189, DOI 10.1016/j.eswa.2021.116114
   Malebary SJ, 2023, SENSORS-BASEL, V23, DOI 10.3390/s23229043
   Mueller M, 2013, IEEE T IMAGE PROCESS, V22, P2786, DOI 10.1109/TIP.2013.2258353
   Muhammad K, 2022, IEEE T INTELL TRANSP, V23, P22694, DOI 10.1109/TITS.2022.3207665
   Muhammad K, 2023, IEEE T INTELL TRANSP, V24, P13141, DOI 10.1109/TITS.2022.3203868
   Muhammad K, 2019, IEEE T IND INFORM, V15, P3113, DOI 10.1109/TII.2019.2897594
   Muhammad K, 2019, IEEE T SYST MAN CY-S, V49, P1419, DOI 10.1109/TSMC.2018.2830099
   Muhammad K, 2018, IEEE ACCESS, V6, P18174, DOI 10.1109/ACCESS.2018.2812835
   Muhammad K, 2018, NEUROCOMPUTING, V288, P30, DOI 10.1016/j.neucom.2017.04.083
   Munsif M, 2022, EUR W VIS INF PROCES, DOI 10.1109/EUVIP53989.2022.9922799
   Mustaqeem K, 2023, KNOWL-BASED SYST, V270, DOI 10.1016/j.knosys.2023.110525
   Prema CE, 2016, FIRE TECHNOL, V52, P1319, DOI 10.1007/s10694-016-0580-8
   Sajjad M, 2021, IEEE T INTELL TRANSP, V22, P1718, DOI 10.1109/TITS.2020.2980855
   Shahid M, 2021, PROCEEDINGS OF THE 2021 INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR '21), P627, DOI 10.1145/3460426.3463665
   Sharma J, 2017, COMM COM INF SC, V744, P183, DOI 10.1007/978-3-319-65172-9_16
   Sun XF, 2021, J FORESTRY RES, V32, P1921, DOI 10.1007/s11676-020-01230-7
   Tan MX, 2019, PROC CVPR IEEE, P2815, DOI [arXiv:1807.11626, 10.1109/CVPR.2019.00293]
   Ullah H, 2021, IEEE T IMAGE PROCESS, V30, P8968, DOI 10.1109/TIP.2021.3116790
   Yan ZJ, 2023, FORESTS, V14, DOI 10.3390/f14010052
   Yang TY, 2018, LECT NOTES COMPUT SC, V11213, P153, DOI 10.1007/978-3-030-01240-3_10
   Yar H, 2023, ISPRS J PHOTOGRAMM, V206, P335, DOI 10.1016/j.isprsjprs.2023.10.019
   Yar H, 2023, EXPERT SYST APPL, V231, DOI 10.1016/j.eswa.2023.120465
   Yar H, 2022, IEEE T IMAGE PROCESS, V31, P6331, DOI 10.1109/TIP.2022.3207006
   Yar Hikmat, 2021, [The Journal of Korean Institute of Next Generation Computing, 한국차세대컴퓨팅학회 논문지], V17, P21
   Yar H, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21144932
   Yuan FN, 2012, PATTERN RECOGN, V45, P4326, DOI 10.1016/j.patcog.2012.06.008
   Zhang DY, 2009, FIRST IITA INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P290, DOI 10.1109/JCAI.2009.79
NR 69
TC 2
Z9 2
U1 2
U2 2
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD MAY
PY 2024
VL 145
AR 104989
DI 10.1016/j.imavis.2024.104989
EA MAR 2024
PG 13
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA PR9Q5
UT WOS:001215932000001
DA 2024-08-05
ER

PT J
AU Tian, HY
   Zhang, L
   Li, SJ
   Yao, M
   Pan, G
AF Tian, Huiyuan
   Zhang, Li
   Li, Shijian
   Yao, Min
   Pan, Gang
TI Multi-depth branch network for efficient image super-resolution
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Efficient super -resolution; Multi -depth branch network; Feature map
   visualization; Fourier spectral analysis; Feature fusion
AB A longstanding challenge in Super-Resolution (SR) is how to efficiently enhance high-frequency details in Low-Resolution (LR) images while maintaining semantic coherence. This is particularly crucial in practical applications where SR models are often deployed on low-power devices. To address this issue, we propose an innovative asymmetric SR architecture featuring Multi-Depth Branch Module (MDBM). These MDBMs contain branches of different depths, designed to capture high- and low-frequency information simultaneously and efficiently. The hierarchical structure of MDBM allows the deeper branch to gradually accumulate fine-grained local details under the contextual guidance of the shallower branch. We visualize this process using feature maps, and further demonstrate the rationality and effectiveness of this design using proposed novel Fourier spectral analysis methods. Moreover, our model exhibits more significant spectral differentiation between branches than existing branch networks. This suggests that MDBM reduces feature redundancy and offers a more effective method for integrating high- and low-frequency information. Extensive qualitative and quantitative evaluations on various datasets show that our model can generate structurally consistent and visually realistic HR images. It achieves state-of-the-art (SOTA) results at a very fast inference speed. Our code is available at https://github.com/thy96 0112/MDBN.
C1 [Tian, Huiyuan; Zhang, Li; Li, Shijian; Yao, Min; Pan, Gang] Zhejiang Univ, Coll Comp Sci & Technol, 38 Zheda Rd, Hangzhou 310027, Zhejiang, Peoples R China.
   [Zhang, Li] Zhejiang Univ, Adv Technol Res Inst, 38 Zheda Rd, Hangzhou 310027, Peoples R China.
C3 Zhejiang University; Zhejiang University
RP Li, SJ (corresponding author), Zhejiang Univ, Coll Comp Sci & Technol, 38 Zheda Rd, Hangzhou 310027, Zhejiang, Peoples R China.
EM tianhuiyuan@zju.edu.cn; zhangli85@zju.edu.cn; shijianli@zju.edu.cn;
   myao@zju.edu.cn; gpan@zju.edu.cn
FU Natural Science Foundation of China [61925603]
FX The authors gratefully acknowledge the financial support of the Natural
   Science Foundation of China (Grant 61925603) .
CR Ahn N, 2018, LECT NOTES COMPUT SC, V11214, P256, DOI 10.1007/978-3-030-01249-6_16
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Chen XY, 2023, PROC CVPR IEEE, P22367, DOI 10.1109/CVPR52729.2023.02142
   Chen YP, 2017, ADV NEUR IN, V30
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Gao DD, 2023, EXPERT SYST APPL, V213, DOI 10.1016/j.eswa.2022.118898
   Gao SC, 2023, PROC CVPR IEEE, P10021, DOI 10.1109/CVPR52729.2023.00966
   Han N, 2022, DISPLAYS, V73, DOI 10.1016/j.displa.2022.102192
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hendrycks D, 2020, Arxiv, DOI arXiv:1606.08415
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Hui Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2024, DOI 10.1145/3343031.3351084
   Kim J, 2016, PROC CVPR IEEE, P1646, DOI 10.1109/CVPR.2016.182
   Kingma D. P., 2014, arXiv
   Kong FY, 2022, IEEE COMPUT SOC CONF, P765, DOI 10.1109/CVPRW56347.2022.00092
   Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li HY, 2022, NEUROCOMPUTING, V479, P47, DOI 10.1016/j.neucom.2022.01.029
   Li W., 2020, Advances in Neural Information Processing Systems, V33, P20343
   Li ZY, 2022, IEEE COMPUT SOC CONF, P832, DOI 10.1109/CVPRW56347.2022.00099
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu JQ, 2023, PROCESSES, V11, DOI 10.3390/pr11061666
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Loshchilov I, 2017, Sgdr: Stochastic gradient descent with warm restarts, DOI [10.48550/arXiv.1608.03983, DOI 10.48550/ARXIV.1608.03983]
   Lu ZS, 2022, IEEE COMPUT SOC CONF, P456, DOI 10.1109/CVPRW56347.2022.00061
   Matsui Y, 2017, MULTIMED TOOLS APPL, V76, P21811, DOI 10.1007/s11042-016-4020-z
   Michelini PN, 2022, IEEE WINT CONF APPL, P4019, DOI 10.1109/WACV51458.2022.00407
   Moser BB, 2023, IEEE T PATTERN ANAL, V45, P9862, DOI 10.1109/TPAMI.2023.3243794
   Saharia C, 2023, IEEE T PATTERN ANAL, V45, P4713, DOI 10.1109/TPAMI.2022.3204461
   Shamsolmoali P, 2019, IMAGE VISION COMPUT, V88, P9, DOI 10.1016/j.imavis.2019.03.006
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Sun L., 2023, P IEEE CVF INT C COM, P13190
   Sun L., 2022, Advances in Neural Information Processing Systems, V35, P17314
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tian HY, 2023, COMPUT VIS MEDIA, V9, P827, DOI 10.1007/s41095-022-0331-3
   Timofte R., 2017, P IEEE C COMPUTER VI, P114
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang JH, 2023, IMAGE VISION COMPUT, V137, DOI 10.1016/j.imavis.2023.104766
   Wang LG, 2021, PROC CVPR IEEE, P4915, DOI 10.1109/CVPR46437.2021.00488
   WANG X., 2022, BASICSR OPEN SOURCE
   Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002
   Wang XT, 2021, IEEE INT CONF COMP V, P1905, DOI 10.1109/ICCVW54120.2021.00217
   Wang Xuehui, 2020, P AS C COMP VIS
   Wang ZH, 2021, IEEE T PATTERN ANAL, V43, P3365, DOI 10.1109/TPAMI.2020.2982166
   Wu HJ, 2023, IMAGE VISION COMPUT, V140, DOI 10.1016/j.imavis.2023.104857
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xu BA, 2023, Arxiv, DOI arXiv:2311.15232
   Zeyde R., 2012, INT C CURV SURF, P711
   Zhang HY, 2019, PR MACH LEARN RES, V89
   Zhang XD, 2022, LECT NOTES COMPUT SC, V13677, P649, DOI 10.1007/978-3-031-19790-1_39
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhao H., 2020, EUROPEAN C COMPUTER, P56, DOI DOI 10.1007/978-3-030-67070-23
   Zhou YP, 2023, Arxiv, DOI arXiv:2303.09735
   Zou WB, 2022, IEEE COMPUT SOC CONF, P929, DOI 10.1109/CVPRW56347.2022.00107
NR 61
TC 0
Z9 0
U1 12
U2 12
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD APR
PY 2024
VL 144
AR 104949
DI 10.1016/j.imavis.2024.104949
EA FEB 2024
PG 10
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA NX1B6
UT WOS:001203650600001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Wang, YB
AF Wang, Yubo
TI A Point-2s reinforcement learning biomimetic model for estimating and
   analyzing human 3D motion posture
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE 3D human activity; Motion posture estimation; VIBE; PointNet plus plus;
   Point-2 s reinforcement learning; Biomimetic model
ID HUMAN POSE ESTIMATION; RECOGNITION; NETWORKS
AB With the rapid progress of computer vision and artificial intelligence, the accuracy of estimating and analyzing human body movements and postures has always been a highly focused research field. However, current methods still have some shortcomings in accurately estimating the pose of 3D human movements. This study aims to propose an effective method to accurately estimate the motion posture of 3D human activities using new technologies of deep learning and neural networks. Firstly, based on previous research, this paper analyzes the problems and challenges of insufficient accuracy in current 3D human motion pose estimation methods, limited capture of 3D spatial information in deep video data, and inability to capture subtle motion details. Then, in response to the problem of disorder in point clouds, an innovative SMPL human Point-2 s reinforcement learning framework was constructed using the VIBE network to estimate the pose of RGB in the NTU dataset. 24 Point-2 s biomimetic algorithm joints were sampled from a distance using the FPS of PointNet++in SMPL, and an index was established based on relative positions to ensure that other joint points are in the same position. Finally, these joint points were input into the 2 s-AGCN network to construct a complete Point-2 s model. The research results indicate that the proposed Point-2 s model has achieved good results in accurately estimating the motion posture of 3D human activities, and effectively solves the problem of disorder in point clouds converted from deep videos. Compared to traditional methods, this research model has significantly improved accuracy and stability. The practical application of this method will provide useful references and guidance for research and method development in related fields.
C1 [Wang, Yubo] Northeast Univ, Coll Informat Sci & Engn, Shenyang 110819, Peoples R China.
C3 Northeastern University - China
RP Wang, YB (corresponding author), Northeast Univ, Coll Informat Sci & Engn, Shenyang 110819, Peoples R China.
EM wangyb369@outlook.com
CR Altaf S, 2023, ELECTRONICS-SWITZ, V12, DOI 10.3390/electronics12020374
   Bai GH, 2022, IMAGE VISION COMPUT, V123, DOI 10.1016/j.imavis.2022.104452
   Ben Gamra M, 2021, IMAGE VISION COMPUT, V114, DOI 10.1016/j.imavis.2021.104282
   Difini GM, 2021, PROCEEDINGS OF THE 27TH BRAZILIAN SYMPOSIUM ON MULTIMEDIA AND THE WEB (WEBMEDIA '21), P189, DOI 10.1145/3470482.3479633
   Guan SN, 2022, NEUROCOMPUTING, V514, P256, DOI 10.1016/j.neucom.2022.10.016
   Pham HH, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20071825
   Jaouedi N, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20174944
   Li Y, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207296
   Lin JC, 2023, CMES-COMP MODEL ENG, V134, P1621, DOI 10.32604/cmes.2022.020857
   Lo Presti L, 2015, IMAGE VISION COMPUT, V44, P29, DOI 10.1016/j.imavis.2015.09.007
   Luo DL, 2021, MULTIMED TOOLS APPL, V80, P27223, DOI 10.1007/s11042-021-10982-1
   Luo YM, 2022, IMAGE VISION COMPUT, V119, DOI 10.1016/j.imavis.2022.104390
   Luvizon D, 2019, Machine Learning for Human Action Recognition and Pose Estimation Based on 3D Information
   Malik Z, 2022, MACH VISION APPL, V33, DOI 10.1007/s00138-022-01291-0
   Muhammad ZUD, 2022, DIGIT SIGNAL PROCESS, V128, DOI 10.1016/j.dsp.2022.103628
   Nale R., 2021, 2021 INT S AS CONTR, P197, DOI [10.1109/iria53009.2021.9588719, DOI 10.1109/IRIA53009.2021.9588719]
   Nguyen NH, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10186188
   Park S, 2018, KSII T INTERNET INF, V12, P800, DOI 10.3837/tiis.2018.02.015
   Salimi M, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22124544
   Saroja M.N., 2021, 2021 INT C ADV EL EL, P1
   Singh A, 2019, PROCEEDINGS 2019 AMITY INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AICAI), P946, DOI [10.1109/aicai.2019.8701267, 10.1109/AICAI.2019.8701267]
   Song LC, 2021, J VIS COMMUN IMAGE R, V76, DOI 10.1016/j.jvcir.2021.103055
   Ullah HA, 2021, IEEE ACCESS, V9, P126366, DOI 10.1109/ACCESS.2021.3110610
   Le VH, 2023, MULTIMED TOOLS APPL, V82, P20771, DOI 10.1007/s11042-022-13921-w
   Wandt Bastian, 2021, 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), P13289, DOI 10.1109/CVPR46437.2021.01309
   Wang C., 2023, IEEE Transactions on Circuits and Systems for Video Technology
   Yu ZY, 2024, KNOWL-BASED SYST, V283, DOI 10.1016/j.knosys.2023.111200
   Zhang ZQ, 2021, IMAGE VISION COMPUT, V111, DOI 10.1016/j.imavis.2021.104198
   Zheng C, 2024, ACM COMPUT SURV, V56, DOI 10.1145/3603618
   Zhou SB, 2023, ELECTRONICS-SWITZ, V12, DOI 10.3390/electronics12071711
   Zou ZM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11457, DOI 10.1109/ICCV48922.2021.01128
NR 31
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD APR
PY 2024
VL 144
AR 104927
DI 10.1016/j.imavis.2024.104927
EA FEB 2024
PG 10
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA NZ9O6
UT WOS:001204395300001
DA 2024-08-05
ER

PT J
AU Hémon, C
   Texier, B
   Chourak, H
   Simon, A
   Bessières, I
   de Crevoisier, R
   Castelli, J
   Lafond, C
   Barateau, A
   Nunes, JC
AF Hemon, Cedric
   Texier, Blanche
   Chourak, Hilda
   Simon, Antoine
   Bessieres, Igor
   de Crevoisier, Renaud
   Castelli, Joel
   Lafond, Caroline
   Barateau, Anais
   Nunes, Jean-Claude
TI Indirect deformable image registration using synthetic image generated
   by unsupervised deep learning
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Multimodal image registration; Synthetic-CT; MRI; CBCT; Radiotherapy;
   Unsupervised generation
ID RADIOTHERAPY; FRAMEWORK
AB Background and purpose: 3D image registration is now common in many medical domains. Multimodal registration implies the use of different imaging modalities, which results in lower accuracy compared to monomodal registration. The aim of this study was to propose a novel approach for deformable image registration (DIR) that incorporates an unsupervised deep learning (DL)-based generation step. The objective was to reduce the challenge of multimodal registration to monomodal registration. Material and methods: Two datasets from prostate radiotherapy patients were used to evaluate the proposed method. The first dataset consisted of Computed Tomography (CT)/ Cone Beam Computed Tomography (CBCT) pairs from 23 patients using different CBCT devices. The second dataset included Magnetic Resonance Imaging (MRI)/CT pairs from two different care centers, utilizing different MRI devices (0.35 T MRIdian MR-Linac, 1.5 T GE lightspeed MRI). Following a preprocessing step essential for ensuring DL synthesis accuracy and standardizing the database, synthetic CTs ( sCT reg ) were generated using an unsupervised conditional Generative Adversarial Network (cGAN). The generated sCTs from CBCT or MRI were then utilized for deformable registration with CT scans. This registration method was compared to three standard methods: rigid registration, Elastix registration based on BSplines, and VoxelMorph-based registration (applied exclusively to CBCT/CT). The endpoints of comparison were the dice coefficients calculated between delineated structures for both datasets. Results: For both datasets, intermediary sCT generation provided the highest dice coefficients. Dices reached 0.85, 0.85 and 0.75 for the prostate, bladder and rectum for the dataset 1 and 0.90, 0.95 and 0.87 respectively for the dataset 2. When the sCT were not used, dices reached 0.66, 0.78, 0.66 for the dataset 1 and 0.93, 0.87 and 0.84 for the dataset 2. Furthermore, the evaluation of the impact of registration on sCT generation showed that lower Mean Absolute Errors were obtained when the registration was conducted with a sCT. Conclusions: Using unsupervised deep learning to synthesize intermediate sCT has led to improved registration accuracy in radiotherapy applications employing two distinct imaging modalities.
C1 [Hemon, Cedric; Texier, Blanche; Chourak, Hilda; Simon, Antoine; de Crevoisier, Renaud; Castelli, Joel; Lafond, Caroline; Barateau, Anais; Nunes, Jean-Claude] Univ Rennes, CLCC Eugene Marquis, INSERM, LTSI UMR 1099, F-35000 Rennes, France.
   [Bessieres, Igor] Ctr Georges Francois Leclerc, Dijon, France.
C3 Institut National de la Sante et de la Recherche Medicale (Inserm);
   Universite de Rennes; UNICANCER; Centre Eugene Marquis; UNICANCER;
   Centre Georges-Francois Leclerc
RP Hémon, C; Nunes, JC (corresponding author), Univ Rennes, CLCC Eugene Marquis, INSERM, LTSI UMR 1099, F-35000 Rennes, France.
EM cedric.hemon@univ-rennes.fr
RI Nunes, Jean-Claude/O-7431-2017
OI Nunes, Jean-Claude/0000-0001-6560-1518
FU Elekta AB; University of Rennes; CominLabs; CEMMTAUR project
FX This research was supported by a PhD scholarship Grant from Elekta AB
   and a PhD scholarship Grant from University of Rennes. The present work
   was also funded by CominLabs with the CEMMTAUR project 2022.
CR Balakrishnan G, 2019, IEEE T MED IMAGING, V38, P1788, DOI 10.1109/TMI.2019.2897538
   Boulanger M, 2021, PHYS MEDICA, V89, P265, DOI 10.1016/j.ejmp.2021.07.027
   Brock KK, 2017, MED PHYS, V44, pE43, DOI 10.1002/mp.12256
   Chen JH, 2023, PHYS MED BIOL, V68, DOI 10.1088/1361-6560/acba74
   Chen T, 2020, Arxiv, DOI [arXiv:2002.05709, DOI 10.48550/ARXIV.2002.05709]
   Chen WY, 2022, Arxiv, DOI arXiv:2112.09747
   Chourak H, 2022, FRONT ONCOL, V12, DOI 10.3389/fonc.2022.968689
   Deng LW, 2023, CMC-COMPUT MATER CON, V77, P2271, DOI 10.32604/cmc.2023.039062
   Dossun C, 2022, PHYS MEDICA, V101, P137, DOI 10.1016/j.ejmp.2022.08.011
   Florkow MC, 2019, PROC SPIE, V10949, DOI 10.1117/12.2512747
   Gao ZY, 2008, IMAGE VISION COMPUT, V26, P164, DOI 10.1016/j.imavis.2006.08.002
   Glide-Hurst C., 2023, COMPUTER VISION ECCV, P527, DOI [10.1007/978-3-031-25066-8_30, DOI 10.1007/978-3-031-25066-8_30]
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Guan H, 2022, IEEE T BIO-MED ENG, V69, P1173, DOI [10.1109/TBME.2021.3117407, 10.1145/3476779.3476780]
   Han R, 2022, PHYS MED BIOL, V67, DOI 10.1088/1361-6560/ac72ef
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hemon C., 2023, SYNTHRAD2023 CHALLEN
   Hemon C, 2023, J APPL CLIN MED PHYS, V24, DOI 10.1002/acm2.13991
   Hong DF, 2024, IEEE T PATTERN ANAL, V46, P5227, DOI 10.1109/TPAMI.2024.3362475
   Huang JL, 2022, IEEE T MULTIMEDIA, V24, P1435, DOI 10.1109/TMM.2021.3065230
   Huijben E.M.C., 2024, Generating Synthetic Computed Tomography for Radiotherapy: SynthRAD2023 Challenge Report
   Hussein M, 2021, BRIT J RADIOL, V94, DOI 10.1259/bjr.20210001
   Ishida T, 2021, J RADIAT RES, V62, P1076, DOI 10.1093/jrr/rrab078
   Jiangtao W., 2021, 2021 IEEE INT C MED, DOI [10.1109/icmipe53131.2021.9698888, DOI 10.1109/ICMIPE53131.2021.9698888]
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kazerouni A, 2023, Arxiv, DOI arXiv:2211.07804
   Klein S, 2010, IEEE T MED IMAGING, V29, P196, DOI 10.1109/TMI.2009.2035616
   Kolenbrander ID, 2024, MED PHYS, V51, P2367, DOI 10.1002/mp.17000
   Li YH, 2017, Arxiv, DOI arXiv:1701.01036
   Liu XL, 2019, MED BIOL ENG COMPUT, V57, P1037, DOI 10.1007/s11517-018-1924-y
   Liu YX, 2023, IEEE J BIOMED HEALTH, V27, P3455, DOI 10.1109/JBHI.2023.3270199
   Liu Z., 2022, arXiv, DOI DOI 10.48550/ARXIV.2201.03545
   Maes F, 2003, P IEEE, V91, P1699, DOI 10.1109/JPROC.2003.817864
   Nenoff L, 2023, PHYS MED BIOL, V68, DOI 10.1088/1361-6560/ad0d8a
   Oh SJ, 2017, RADIAT ONCOL J, V35, P101, DOI 10.3857/roj.2017.00325
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   Rigaud B, 2019, ACTA ONCOL, DOI 10.1080/0284186X.2019.1620331
   Ronneberger O, 2015, Arxiv, DOI arXiv:1505.04597
   Saad MM, 2024, ARTIF INTELL REV, V57, DOI 10.1007/s10462-023-10624-y
   Schaly B, 2004, PHYS MED BIOL, V49, P791, DOI 10.1088/0031-9155/49/5/010
   Sengupta D, 2022, NEUROCOMPUTING, V486, P174, DOI 10.1016/j.neucom.2021.11.023
   Spadea MF, 2021, MED PHYS, V48, P6537, DOI 10.1002/mp.15150
   Tahri S, 2023, FRONT ONCOL, V13, DOI 10.3389/fonc.2023.1279750
   Tang ZY, 2019, IEEE T IMAGE PROCESS, V28, P2293, DOI 10.1109/TIP.2018.2884563
   Texier B, 2023, PHYS IMAG RADIAT ONC, V28, DOI 10.1016/j.phro.2023.100511
   Thörnqvist S, 2010, ACTA ONCOL, V49, P1023, DOI 10.3109/0284186X.2010.503662
   Tustison NJ, 2010, IEEE T MED IMAGING, V29, P1310, DOI 10.1109/TMI.2010.2046908
   Wang D, 2022, Arxiv, DOI [arXiv:2205.11876, DOI 10.48550/ARXIV.2205.11876]
   Wasserthal J, 2023, RADIOL-ARTIF INTELL, V5, DOI 10.1148/ryai.230024
   Wu GR, 2013, LECT NOTES COMPUT SC, V8150, P649, DOI 10.1007/978-3-642-40763-5_80
   Yang H, 2020, COMPUT MATH METHOD M, V2020, DOI 10.1155/2020/2684851
   Zhou LY, 2023, PHYS MED BIOL, V68, DOI 10.1088/1361-6560/ad0ddc
   Zou J, 2022, FRONT ONCOL, V12, DOI 10.3389/fonc.2022.1047215
NR 53
TC 0
Z9 0
U1 1
U2 1
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD AUG
PY 2024
VL 148
AR 105143
DI 10.1016/j.imavis.2024.105143
EA JUN 2024
PG 9
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA XG2N6
UT WOS:001260466200001
OA hybrid
DA 2024-08-05
ER

PT J
AU Berghouse, M
   Bebis, G
   Tavakkoli, A
AF Berghouse, Marc
   Bebis, George
   Tavakkoli, Alireza
TI Exploring the influence of attention for whole-image mammogram
   classification
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Mammogram classification; Attention deep; Learning
AB Attention is an important component of modern Convolutional Neural Networks (CNNs) that has been shown to improve baseline model performance for a wide variety of tasks. Attention has shown specific promise in the classification and segmentation of mammograms, but we have a limited understanding of why attention improves performance in these domains. In this paper, we present a robust comparison of different combinations of baseline models and attention methods at two resolutions for whole mammogram classification of masses and calcifications. We find that attention generally helps to improve baseline model performance. However, the extent of improvement is governed by a combination of model architecture and the statistical characteristics of the data. Specifically, we show that high amounts of pooling and model complexity may result in decreased performance for data with high variability. To better understand the effect of attention on mammogram classification, we used LayerCAM, a hierarchical Class Activation Map (CAM) approach, to visualize where the network pays attention in the input image. This research provides statistical evidence that attention can improve the correlation between model performance and LayerCAM activation in the region of interest (ROI). However, these correlations are weak and variable, indicating that improvements in model performance due to attention are not necessarily caused by increased model activation near the ROI. Overall, our work provides novel insights to help guide future efforts in incorporating attention-based mechanisms for mammogram classification.
C1 [Berghouse, Marc] Univ Nevada, GPHS, Reno, NV 89597 USA.
   [Bebis, George; Tavakkoli, Alireza] Univ Nevada, Dept Comp Sci & Engn, Reno, NV 89597 USA.
C3 Nevada System of Higher Education (NSHE); University of Nevada Reno;
   Nevada System of Higher Education (NSHE); University of Nevada Reno
RP Berghouse, M (corresponding author), Univ Nevada, GPHS, Reno, NV 89597 USA.
EM mberghouse@nevada.unr.edu; bebis@unr.edu
CR Abdel-Nasser M, 2016, INT J OPT, V2016, DOI 10.1155/2016/1370259
   Adedigba AP, 2022, BIOENGINEERING-BASEL, V9, DOI 10.3390/bioengineering9040161
   Akiba T, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2623, DOI 10.1145/3292500.3330701
   Altan G., 2020, Int. J. Intell. Syst. Appl. Eng., V9, P171, DOI DOI 10.18201/IJISAE.2020466308
   Anaya-Isaza A., 2021, Informatics in medicine unlocked, V26, DOI [10.1016/j.imu.2021.100723, DOI 10.1016/J.IMU.2021.100723]
   Azad Reza, 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12535), P251, DOI 10.1007/978-3-030-66415-2_16
   Baccouche A., 2021, Comp. Mater. Continua., V69
   Berghouse M, 2023, LECT NOTES COMPUT SC, V14361, P30, DOI 10.1007/978-3-031-47969-4_3
   Bergstra J., 2011, Advances in Neural Information Processing Systems, P2546
   Casper S, 2021, AAAI CONF ARTIF INTE, V35, P6921
   Chen Y., 2019, Lecture Notes in Electrical Engineering., V536
   Chu XX, 2021, ADV NEUR IN
   Datta SK, 2021, LECT NOTES COMPUT SC, V12929, P13, DOI 10.1007/978-3-030-87444-5_2
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding MY, 2022, LECT NOTES COMPUT SC, V13684, P74, DOI 10.1007/978-3-031-20053-3_5
   Doimo D., 2020, Adv. Neural Inf. Proces. Syst., V35, P19659
   Dong N., 2023, IEEE T CIRCUITS SYST
   Dong N, 2024, INFORM FUSION, V104, DOI 10.1016/j.inffus.2023.102201
   Falconi L.G., 2020, Advances in Science, Technology and Engineering Systems, V5, P154, DOI DOI 10.25046/AJ050220
   Goncalves T, 2022, IEEE ACCESS, V10, P98909, DOI 10.1109/ACCESS.2022.3206449
   Guo MH, 2022, COMPUT VIS MEDIA, V8, P331, DOI 10.1007/s41095-022-0271-y
   Hassan NM, 2022, MULTIMED TOOLS APPL, V81, P20043, DOI 10.1007/s11042-022-12332-1
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Jiang PT, 2021, IEEE T IMAGE PROCESS, V30, P5875, DOI 10.1109/TIP.2021.3089943
   Kebria P.M., 2018, Neural Information Processing. ICONIP 2018., V11301, DOI [10.1007/978-3-030-04167-016, DOI 10.1007/978-3-030-04167-016]
   Lee RS, 2017, SCI DATA, V4, DOI 10.1038/sdata.2017.177
   Li ZH, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12189016
   Lou Q, 2022, COMPUT BIOL MED, V150, DOI 10.1016/j.compbiomed.2022.106082
   Mao N, 2023, BRIT J CANCER, V128, P793, DOI 10.1038/s41416-022-02092-y
   Moreira IC, 2012, ACAD RADIOL, V19, P236, DOI 10.1016/j.acra.2011.09.014
   Niu J, 2021, MED PHYS, V48, P3878, DOI 10.1002/mp.14942
   Radosavovic Ilija, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10425, DOI 10.1109/CVPR42600.2020.01044
   Rice L., 2020, PMLR, P8093
   Roy AG, 2019, IEEE T MED IMAGING, V38, P540, DOI 10.1109/TMI.2018.2867261
   Saunders RS, 2007, MED PHYS, V34, P3971, DOI 10.1118/1.2776253
   Shen L, 2017, Arxiv, DOI arXiv:1711.05775
   Shen L, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-48995-4
   Sinha A, 2021, IEEE J BIOMED HEALTH, V25, P121, DOI 10.1109/JBHI.2020.2986926
   Sun H, 2020, PHYS MED BIOL, V65, DOI 10.1088/1361-6560/ab5745
   Sun S., 2016, Proc. AAAI Conf. Artif. Intell.., V30
   Tang H, 2022, PATTERN RECOGN, V130, DOI 10.1016/j.patcog.2022.108792
   Vuckovic J, 2020, Arxiv, DOI arXiv:2007.02876
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/ISQED48828.2020.9137057, 10.1109/isqed48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wei T, 2022, MED IMAGE ANAL, V82, DOI 10.1016/j.media.2022.102618
   Wiegreffe S., 2019, arXiv
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xu CB, 2022, BIOMED SIGNAL PROCES, V71, DOI 10.1016/j.bspc.2021.103178
   Xu CB, 2021, BIOMED SIGNAL PROCES, V68, DOI 10.1016/j.bspc.2021.102730
   Yu SQ, 2017, NEUROCOMPUTING, V219, P88, DOI 10.1016/j.neucom.2016.09.010
   Zagoruyko S, 2017, Arxiv, DOI [arXiv:1612.03928, DOI 10.48550/ARXIV.1612.03928]
   Zha ZC, 2023, IEEE T CIRC SYST VID, V33, P3947, DOI 10.1109/TCSVT.2023.3236636
   Zhang KH, 2021, ENG APPL ARTIF INTEL, V102, DOI 10.1016/j.engappai.2021.104242
   Zhao WW, 2021, BIOMED SIGNAL PROCES, V70, DOI 10.1016/j.bspc.2021.103073
   Zhao XR, 2020, INT CONF ACOUST SPEE, P1050, DOI [10.1109/ICASSP40776.2020.9054612, 10.1109/icassp40776.2020.9054612]
NR 57
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUL
PY 2024
VL 147
AR 105062
DI 10.1016/j.imavis.2024.105062
EA MAY 2024
PG 15
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA YA8L2
UT WOS:001265854200001
DA 2024-08-05
ER

PT J
AU Das, D
   Nayak, DR
   Pachori, RB
AF Das, Dipankar
   Nayak, Deepak Ranjan
   Pachori, Ram Bilas
TI AES-Net: An adapter and enhanced self-attention guided network for
   multi-stage glaucoma classification using fundus images
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Multi-stage glaucoma classification; Fundus image; Spatial -adapter
   module; Enhanced self -attention module (ESAM); AES-Net
ID DIAGNOSIS; FEATURES
AB Glaucoma is a progressive eye condition that can lead to permanent vision loss. Therefore, on-time detection of glaucoma is critical for making an effective treatment plan. In recent years, enormous attempts have been made to develop automated glaucoma classification systems using CNNs through images. In contrast, limited methods have been proposed for diagnosing different glaucoma stages. It is mainly owing to the lack of large publicly available labeled datasets. Also, fundus images exhibit a high inter-stage resemblance, redundant features and minute size variations of lesions, making the conventional CNNs difficult to classify multiple stages of glaucoma accurately. To address these challenges, this paper proposes a novel adapter and enhanced self-attention based CNN framework named AES-Net for effective classification of glaucoma stages. In particular, we propose a spatial adapter module on top of the backbone network for learning better feature representations and an enhanced selfattention module (ESAM) to capture global feature correlations among the relevant channels and spatial positions. The ESAM assists in capturing stage-specific and detailed-lesion features from the fundus images. Extensive experiments on two multi-stage glaucoma datasets indicate that our AES-Net surpasses CNN-based existing approaches. The Grad-CAM++ visualization maps further confirm the effectiveness of our AES-Net.
C1 [Das, Dipankar; Nayak, Deepak Ranjan] Malaviya Natl Inst Technol Jaipur, Dept Comp Sceince & Engn, Jaipur, India.
   [Pachori, Ram Bilas] Indian Inst Technol Indore, Dept Elect Engn, Indore, India.
C3 National Institute of Technology (NIT System); Malaviya National
   Institute of Technology Jaipur; Indian Institute of Technology System
   (IIT System); Indian Institute of Technology (IIT) - Indore
RP Nayak, DR (corresponding author), Malaviya Natl Inst Technol Jaipur, Dept Comp Sceince & Engn, Jaipur, India.
EM 2022rcp9028@mnit.ac.in; drnayak.cse@mnit.ac.in; pachori@iiti.ac.in
FU Science and Engineering Research Board (SERB) , Department of Science
   and Technology [SRG/2020/001460]
FX This work is supported by the Science and Engineering Research Board
   (SERB) , Department of Science and Technology, Govt. of India under
   project No. SRG/2020/001460.
CR Acharya UR, 2015, BIOMED SIGNAL PROCES, V15, P18, DOI 10.1016/j.bspc.2014.09.004
   Ahn JM, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0207982
   Bajwa MN, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207664
   Cao Y, 2019, IEEE ICC
   Chai YD, 2018, KNOWL-BASED SYST, V161, P147, DOI 10.1016/j.knosys.2018.07.043
   Chattopadhay A, 2018, IEEE WINT CONF APPL, P839, DOI 10.1109/WACV.2018.00097
   Chen XY, 2015, IEEE ENG MED BIO, P715, DOI 10.1109/EMBC.2015.7318462
   Cheng J, 2015, IEEE T BIO-MED ENG, V62, P1395, DOI 10.1109/TBME.2015.2389234
   Cheng J, 2013, IEEE T MED IMAGING, V32, P1019, DOI 10.1109/TMI.2013.2247770
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Das D, 2023, IEEE IMAGE PROC, P3454, DOI 10.1109/ICIP49359.2023.10222689
   Das D, 2023, IEEE T INSTRUM MEAS, V72, DOI 10.1109/TIM.2023.3322499
   Dua S, 2012, IEEE T INF TECHNOL B, V16, P80, DOI 10.1109/TITB.2011.2176540
   Fu HZ, 2018, IEEE T MED IMAGING, V37, P2493, DOI 10.1109/TMI.2018.2837012
   Fumero F, 2011, COMP MED SY
   Howard AG, 2017, Arxiv, DOI [arXiv:1704.04861, 10.48550/arXiv.1704.04861]
   He AL, 2021, IEEE T MED IMAGING, V40, P143, DOI 10.1109/TMI.2020.3023463
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hervella AS, 2022, APPL SOFT COMPUT, V116, DOI 10.1016/j.asoc.2021.108347
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Joshi GD, 2011, IEEE T MED IMAGING, V30, P1192, DOI 10.1109/TMI.2011.2106509
   Juneja M, 2022, COMPUT ELECTR ENG, V101, DOI 10.1016/j.compeleceng.2022.108009
   Kausu TR, 2018, BIOCYBERN BIOMED ENG, V38, P329, DOI 10.1016/j.bbe.2018.02.003
   Kingma D. P., 2014, arXiv
   Li AN, 2016, IEEE ENG MED BIO, P1328, DOI 10.1109/EMBC.2016.7590952
   Li W, 2023, ENG APPL ARTIF INTEL, V126, DOI 10.1016/j.engappai.2023.107123
   Maheshwari S, 2017, IEEE J BIOMED HEALTH, V21, P803, DOI 10.1109/JBHI.2016.2544961
   Misra D, 2021, IEEE WINT CONF APPL, P3138, DOI 10.1109/WACV48630.2021.00318
   Mookiah MRK, 2012, KNOWL-BASED SYST, V33, P73, DOI 10.1016/j.knosys.2012.02.010
   Nayak DR, 2021, BIOMED SIGNAL PROCES, V67, DOI 10.1016/j.bspc.2021.102559
   Özcelik YB, 2023, FRACTAL FRACT, V7, DOI 10.3390/fractalfract7080598
   Ozcelik Yusuf Bahri, 2023, P CANK INT C SCI RES, P10
   Pal A, 2018, IEEE IMAGE PROC, P2775, DOI 10.1109/ICIP.2018.8451029
   Pan JT, 2022, ADV NEUR IN
   Parashar D, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3071223
   Parashar D, 2020, IEEE SENS J, V20, P12885, DOI 10.1109/JSEN.2020.3001972
   Park J, 2018, Arxiv, DOI [arXiv:1807.06514, 10.48550/arXiv.1807.06514]
   Phasuk S, 2019, IEEE ENG MED BIO, P904, DOI [10.1109/EMBC.2019.8857136, 10.1109/embc.2019.8857136]
   Quigley HA, 2006, BRIT J OPHTHALMOL, V90, P262, DOI 10.1136/bjo.2005.081224
   Raghavendra U, 2018, INFORM SCIENCES, V441, P41, DOI 10.1016/j.ins.2018.01.051
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sunanthini V, 2022, J HEALTHC ENG, V2022, DOI 10.1155/2022/7873300
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tang H, 2022, PATTERN RECOGN, V130, DOI 10.1016/j.patcog.2022.108792
   Tang H, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P610, DOI 10.1145/3394171.3413884
   Tham YC, 2014, OPHTHALMOLOGY, V121, P2081, DOI 10.1016/j.ophtha.2014.05.013
   Tian H, 2022, C IND ELECT APPL, P498, DOI 10.1109/ICIEA54703.2022.10005946
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Velpula VK, 2023, FRONT PHYSIOL, V14, DOI 10.3389/fphys.2023.1175881
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Yan SL, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3310118
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhu JW, 2022, IMAGE VISION COMPUT, V124, DOI 10.1016/j.imavis.2022.104507
   Zhu QX, 2023, APPL INTELL, V53, P23049, DOI 10.1007/s10489-023-04734-x
NR 56
TC 0
Z9 0
U1 2
U2 2
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUN
PY 2024
VL 146
AR 105042
DI 10.1016/j.imavis.2024.105042
EA APR 2024
PG 11
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA SU9I0
UT WOS:001237073500001
DA 2024-08-05
ER

PT J
AU Dar, MF
   Ganivada, A
AF Dar, Mohsin Furkh
   Ganivada, Avatharam
TI Deep learning and genetic algorithm-based ensemble model for feature
   selection and classification of breast ultrasound images
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Deep learning; Feature selection; Ultrasound imaging; Breast cancer;
   Genetic algorithm
AB Feature extraction and selection are important techniques in the classification of medical images. Extraction of key features and selection of relevant features are the preliminary processes that are essential for identifying the shape of an object or diagnosis of a tumor in images. In this study, we conduct a thorough comparison of deep neural networks' performance. The comparison of the networks infers MobileNet as optimal for feature extraction from medical images, where it has minimal parameters and high validation accuracy. For feature selection, we employ the Genetic Algorithm (GA) because of its proficiency in handling high-dimensional and complex feature space. GA's iterative process aligns well with the unique characteristics of breast ultrasound (BUS) images, which enhances its efficacy in selecting salient features of BUS images. An ensemble model, capitalizing on the collective decision-making capabilities of multiple classifiers based on a weighted voting scheme for classification, is proposed. Empirical evaluations are conducted using two publicly available BUS image datasets, BUSI and UDIAT. The proposed model demonstrates a notable improvement of approximately 4% and 9% in accuracy for the BUSI and UDIAT, respectively. The improved diagnostic accuracy in breast abnormality identification allows for early abnormality diagnosis. This improves treatment outcomes for breast cancer patients and highlights the practical value of the proposed method for improving BUS image categorization.
C1 [Dar, Mohsin Furkh; Ganivada, Avatharam] Univ Hyderabad, Sch Comp & Informat Sci, Artificial Intelligence Lab, Hyderabad 500046, India.
C3 University of Hyderabad
RP Ganivada, A (corresponding author), Univ Hyderabad, Sch Comp & Informat Sci, Artificial Intelligence Lab, Hyderabad 500046, India.
EM avatharg@uohyd.ac.in
CR Ahila A, 2022, FRONT ONCOL, V12, DOI 10.3389/fonc.2022.834028
   Al-Dhabyani W, 2020, DATA BRIEF, V28, DOI 10.1016/j.dib.2019.104863
   Balaha HM, 2022, NEURAL COMPUT APPL, V34, P8671, DOI 10.1007/s00521-021-06851-5
   Byra M, 2021, BIOMED SIGNAL PROCES, V69, DOI 10.1016/j.bspc.2021.102828
   Carvalho ED, 2021, COMPUT BIOL MED, V136, DOI 10.1016/j.compbiomed.2021.104744
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Daoud MI, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20236838
   Dar MF, 2023, NEURAL PROCESS LETT, V55, P10439, DOI 10.1007/s11063-023-11333-x
   Deb SD, 2023, BIOMED SIGNAL PROCES, V85, DOI 10.1016/j.bspc.2023.104871
   Di XH, 2022, COMPUT METH PROG BIO, V215, DOI 10.1016/j.cmpb.2021.106612
   Du J, 2023, IEEE T EM TOP COMP I, V7, P845, DOI 10.1109/TETCI.2022.3199733
   Goldberg DE., 1989, GENETIC ALGORITHMS S, DOI DOI 10.1109/ICETEEEM.2012.6494460
   Haq MA, 2023, FRACTALS, V31, DOI 10.1142/S0218348X23401023
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   Howard A.G., 2017, Comput. Vis. Pattern Recognit
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jiménez-Gaona Y, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10228298
   Joshi RC, 2022, MULTIMED TOOLS APPL, V81, P13691, DOI 10.1007/s11042-021-11240-0
   Kingma D. P., 2014, arXiv
   Luo YZ, 2023, PATTERN RECOGN, V143, DOI 10.1016/j.patcog.2023.109776
   Mishra AK, 2021, EXPERT SYST, V38, DOI 10.1111/exsy.12713
   Mishra AK, 2022, MULTIMED TOOLS APPL, V81, P37627, DOI 10.1007/s11042-022-13498-4
   Mo YH, 2023, IEEE T MED IMAGING, V42, P1696, DOI 10.1109/TMI.2023.3236011
   Moon WK, 2020, COMPUT METH PROG BIO, V190, DOI 10.1016/j.cmpb.2020.105361
   Muduli D, 2022, BIOMED SIGNAL PROCES, V71, DOI 10.1016/j.bspc.2021.102825
   Munshi RM, 2024, IMAGE VISION COMPUT, V142, DOI 10.1016/j.imavis.2024.104910
   OpenCV, 2015, Open Source Computer Vision Library
   Pramanik R, 2023, EXPERT SYST APPL, V219, DOI 10.1016/j.eswa.2023.119643
   Qu XL, 2022, MED PHYS, V49, P5787, DOI 10.1002/mp.15852
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Saba T, 2022, MICROSC RES TECHNIQ, V85, P1444, DOI 10.1002/jemt.24008
   Sahu A, 2024, BIOMED SIGNAL PROCES, V87, DOI 10.1016/j.bspc.2023.105377
   Shaban WM, 2023, NEURAL COMPUT APPL, V35, P6831, DOI 10.1007/s00521-022-08062-y
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh VK, 2024, NEURAL COMPUT APPL, DOI 10.1007/s00521-023-09363-6
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tasnim J, 2024, PHYS MED BIOL, V69, DOI 10.1088/1361-6560/ad1319
   van der Walt S, 2014, PEERJ, V2, DOI 10.7717/peerj.453
   Wang Y, 2020, ULTRASOUND MED BIOL, V46, P1119, DOI 10.1016/j.ultrasmedbio.2020.01.001
   Wang Y, 2021, IEEE ACCESS, V9, P54310, DOI 10.1109/ACCESS.2021.3071301
   Wei MW, 2020, COMPUT MATH METHOD M, V2020, DOI 10.1155/2020/5894010
   Xie J, 2020, PHYS MED BIOL, V65, DOI 10.1088/1361-6560/abc5c7
   Xing J, 2021, IEEE J BIOMED HEALTH, V25, P2058, DOI 10.1109/JBHI.2020.3034804
   Yap MH, 2018, IEEE J BIOMED HEALTH, V22, P1218, DOI 10.1109/JBHI.2017.2731873
   Zhang J, 2023, IET IMAGE PROCESS, V17, P3789, DOI 10.1049/ipr2.12897
   Zhenyuan Ning, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P171, DOI 10.1007/978-3-030-59725-2_17
NR 46
TC 0
Z9 0
U1 4
U2 4
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUN
PY 2024
VL 146
AR 105018
DI 10.1016/j.imavis.2024.105018
EA APR 2024
PG 15
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA RT0Q0
UT WOS:001229797400001
DA 2024-08-05
ER

PT J
AU Du, ZX
   Wang, Q
AF Du, Zexing
   Wang, Qing
TI Exploring global context and position-aware representation for group
   activity recognition
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Group activity recognition; Spatio-temporal representation; Position
   -aware representation
ID PERSON REIDENTIFICATION
AB This paper explores the context and position information in the scene for group activity understanding. Firstly, previous group activity recognition methods strive to reason on individual features without considering the information in the scene. Besides correlations among actors, we argue that integrating the scene context simultaneously can afford us more useful and supplementary cues. Therefore, we propose a new network, termed Contextual Transformer Network (CTN), to incorporate global contextual information into individual representations. In addition, the position of individuals also plays a vital role in group activity understanding. Unlike previous methods that explore correlations among individuals semantically, we propose Clustered Position Embedding (CPE) to integrate the spatial structure of actors and produce position-aware representations. Experimental results on two widely used datasets for sports video and social activity (i.e., Volleyball and Collective Activity datasets) show that the proposed method outperforms state-of-the-art approaches. Especially, when using ResNet-18 as the backbone, our method achieves 93.6/93.9% MCA/MPCA on the Volleyball dataset and 95.4/96.3% MCA/MPCA on the Collective Activity dataset.
C1 [Du, Zexing; Wang, Qing] Northwestern Polytech Univ, Sch Comp Sci, You Yi Xi Rd 127, Xian 710072, Shaanxi, Peoples R China.
C3 Northwestern Polytechnical University
RP Wang, Q (corresponding author), Northwestern Polytech Univ, Sch Comp Sci, You Yi Xi Rd 127, Xian 710072, Shaanxi, Peoples R China.
EM duzexing@mail.nwpu.edu.cn; qwang@nwpu.edu.cn
FU NSFC [62031023]
FX This work was supported by NSFC under Grant 62031023.
CR Amer MR, 2012, LECT NOTES COMPUT SC, V7575, P187, DOI 10.1007/978-3-642-33765-9_14
   Amer MR, 2014, LECT NOTES COMPUT SC, V8694, P572, DOI 10.1007/978-3-319-10599-4_37
   Arnab A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6816, DOI 10.1109/ICCV48922.2021.00676
   Azar SM, 2019, PROC CVPR IEEE, P7884, DOI 10.1109/CVPR.2019.00808
   Bagautdinov T, 2017, PROC CVPR IEEE, P3425, DOI 10.1109/CVPR.2017.365
   Bottou L., 1995, Advances in Neural Information Processing Systems 7, P585
   Chen J, 2021, IMAGE VISION COMPUT, V112, DOI 10.1016/j.imavis.2021.104214
   Choi W., 2009, P IEEE INT C COMPUTE, P1282
   Choi W, 2014, IEEE T PATTERN ANAL, V36, P1242, DOI 10.1109/TPAMI.2013.220
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Du ZX, 2023, IEEE T CIRC SYST VID, V33, P5076, DOI 10.1109/TCSVT.2023.3249906
   Ehsanpour Mahsa, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P177, DOI 10.1007/978-3-030-58545-7_11
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Han M., 2022, P IEEE C COMPUTER VI, P2990
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu GY, 2020, PROC CVPR IEEE, P977, DOI 10.1109/CVPR42600.2020.00106
   Ibrahim MS, 2016, PROC CVPR IEEE, P1971, DOI 10.1109/CVPR.2016.217
   Kim D, 2022, PROC CVPR IEEE, P20051, DOI 10.1109/CVPR52688.2022.01945
   Lan T, 2012, PROC CVPR IEEE, P1354, DOI 10.1109/CVPR.2012.6247821
   Li Shuaicheng, 2021, P IEEE CVF INT C COM, P13668
   Li W., 2022, P INT JOINT C ARTIFI, P1102, DOI 10.24963/ijcai.2022/154
   Li W, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P2051, DOI 10.1145/3503161.3547825
   Liu TY, 2022, IEEE T IMAGE PROCESS, V31, P4240, DOI 10.1109/TIP.2022.3181811
   Pei DX, 2023, IEEE T CIRC SYST VID, V33, P7803, DOI 10.1109/TCSVT.2023.3283282
   Poulose A, 2022, COMPUT INTEL NEUROSC, V2022, DOI 10.1155/2022/1808990
   Pramono Rizard Renanda Adhi, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P71, DOI 10.1007/978-3-030-58452-8_5
   Qi MS, 2018, LECT NOTES COMPUT SC, V11214, P104, DOI 10.1007/978-3-030-01249-6_7
   Rodriguez M, 2016, IMAGE VISION COMPUT, V48-49, P26, DOI 10.1016/j.imavis.2015.12.006
   Rui Yan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P208, DOI 10.1007/978-3-030-58598-3_13
   Shu TM, 2017, PROC CVPR IEEE, P4255, DOI 10.1109/CVPR.2017.453
   Shu TM, 2015, PROC CVPR IEEE, P4576, DOI 10.1109/CVPR.2015.7299088
   Shu XB, 2021, IEEE T PATTERN ANAL, V43, P1110, DOI 10.1109/TPAMI.2019.2942030
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tamura M, 2022, LECT NOTES COMPUT SC, V13664, P19, DOI 10.1007/978-3-031-19772-7_2
   Tang YS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1283, DOI 10.1145/3240508.3240576
   Tarashima S., 2021, BRIT MACH VIS C
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A., 2017, Advances in neural information processing systems, P5998
   Wang MS, 2017, PROC CVPR IEEE, P7408, DOI 10.1109/CVPR.2017.783
   Wu BC, 2020, Arxiv, DOI [arXiv:2006.03677, DOI 10.48550/ARXIV.2006.03677]
   Wu JC, 2019, PROC CVPR IEEE, P9956, DOI 10.1109/CVPR.2019.01020
   Wu Y, 2019, IEEE T IMAGE PROCESS, V28, P2872, DOI 10.1109/TIP.2019.2891895
   Yan R, 2022, IEEE T NEUR NET LEAR, V33, P7574, DOI 10.1109/TNNLS.2021.3085567
   Yan R, 2023, IEEE T PATTERN ANAL, V45, P6955, DOI 10.1109/TPAMI.2020.3034233
   Yan R, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1292, DOI 10.1145/3240508.3240572
   Yuan HJ, 2021, AAAI CONF ARTIF INTE, V35, P3261
   Yuan Hangjie, 2021, P IEEE CVF INT C COM, P7476
   Zalluhoglu C, 2020, IMAGE VISION COMPUT, V94, DOI 10.1016/j.imavis.2020.103870
   Zhang P., 2019, IEEE Trans. Image Process., V29, P29
   Zhou WT, 2024, IEEE T MULTIMEDIA, V26, P353, DOI 10.1109/TMM.2023.3265280
   Zhu XL, 2023, IEEE T CIRC SYST VID, V33, P3383, DOI 10.1109/TCSVT.2022.3233069
NR 52
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD SEP
PY 2024
VL 149
AR 105181
DI 10.1016/j.imavis.2024.105181
EA JUL 2024
PG 10
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA ZF0L3
UT WOS:001273762700001
DA 2024-08-05
ER

PT J
AU Patel, AN
   Murugan, R
   Maddikunta, PKR
   Yenduri, G
   Jhaveri, RH
   Zhu, YD
   Gadekallu, TR
AF Patel, Aryan Nikul
   Murugan, Ramalingam
   Maddikunta, Praveen Kumar Reddy
   Yenduri, Gokul
   Jhaveri, Rutvij H.
   Zhu, Yaodong
   Gadekallu, Thippa Reddy
TI AI-powered trustable and explainable fall detection system using
   transfer learning
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Artificial intelligence; Explainable artificial intelligence; Transfer
   learning; Deep neural networks; Fall detection
ID WEARABLE SENSORS; RECOGNITION; MACHINE
AB Accidental falls pose a significant public health challenge, especially among vulnerable populations. To address this issue, comprehensive research on fall detection and rescue systems is essential. Vision-based technologies, with their promising potential, offer an effective means to detect falls. This research paper presents a cuttingedge fall detection methodology aimed at enhancing individual safety and well-being. The proposed methodology utilizes deep neural networks, leveraging their capabilities to drive advancements in fall detection. To overcome data limitations and computational efficiency concerns, this study employ transfer learning by finetuning pre-trained models on large-scale image datasets for fall detection. This approach significantly enhances model performance, enabling better generalization and accuracy, especially in real-time applications with constrained resources. Notably, the methodology achieved an impressive test accuracy of 98.15%. Additionally, the incorporation of Explainable Artificial Intelligence (XAI) techniques is used to ensure transparent and trustworthy decision-making in fall detection using deep learning models, especially in critical healthcare contexts for vulnerable individuals. XAI provides valuable insights into complex model architectures and parameters, enabling a deeper understanding of fall identification patterns. To evaluate the effectiveness of this approach, a rigorous experimentation was conducted using a diverse dataset containing real-world fall and nonfall scenarios. The results demonstrate substantial improvements in both accuracy and interpretability, confirming the superiority of this method over conventional fall detection approaches.
C1 [Patel, Aryan Nikul] Vellore Inst Technol, Sch Comp Sci & Engn, Vellore, India.
   [Murugan, Ramalingam; Maddikunta, Praveen Kumar Reddy] Vellore Inst Technol, Sch Comp Sci Engn & Informat Syst, Vellore, India.
   [Yenduri, Gokul] VIT AP Univ, Sch Comp Sci & Engn, Amaravati 522237, Andhra Pradesh, India.
   [Jhaveri, Rutvij H.] Pandit Deendayal Energy Univ, Sch Technol, Gandhinagar, Gujarat, India.
   [Zhu, Yaodong] Jiaxing Univ, Sch Informat Sci & Engn, Jiaxing 314001, Peoples R China.
   [Gadekallu, Thippa Reddy] Zhejiang A&F Univ, Coll Math & Comp Sci, Hangzhou 311300, Peoples R China.
   [Gadekallu, Thippa Reddy] Lovely Profess Univ, Div Res & Dev, Phagwara, India.
   [Gadekallu, Thippa Reddy] Chitkara Univ, Inst Engn & Technol, Ctr Res Impact & Outcome, Rajpura 140401, Punjab, India.
C3 Vellore Institute of Technology (VIT); VIT Vellore; Vellore Institute of
   Technology (VIT); VIT Vellore; VIT-AP University; Pandit Deendayal
   Energy University; Jiaxing University; Zhejiang A&F University; Lovely
   Professional University; Chitkara University, Punjab
RP Maddikunta, PKR (corresponding author), Vellore Inst Technol, Sch Comp Sci Engn & Informat Syst, Vellore, India.
EM aryannikul.patel2020@vitstudent.ac.in; ramalingam.m@vit.ac.in;
   praveenkumarreddy@vit.ac.in; rutvij.jhaveri@sot.pdpu.ac.in;
   thippareddy@ieee.org
RI Gadekallu, Thippa Reddy/T-4254-2019
OI Gadekallu, Thippa Reddy/0000-0003-0097-801X
CR Abbate S, 2012, PERVASIVE MOB COMPUT, V8, P883, DOI 10.1016/j.pmcj.2012.08.003
   Adadi A, 2018, IEEE ACCESS, V6, P52138, DOI 10.1109/ACCESS.2018.2870052
   Alam E, 2022, COMPUT BIOL MED, V146, DOI 10.1016/j.compbiomed.2022.105626
   Amparore E, 2021, PEERJ COMPUT SCI, DOI 10.7717/peerj-cs.479
   Amsaprabhaa M, 2023, EXPERT SYST APPL, V212, DOI 10.1016/j.eswa.2022.118681
   Antoniadi AM, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11115088
   Balemans D, 2020, INTERNET THINGS-NETH, V11, DOI 10.1016/j.iot.2020.100231
   Arrieta AB, 2020, INFORM FUSION, V58, P82, DOI 10.1016/j.inffus.2019.12.012
   Bhardwaj C, 2021, INT J IMAG SYST TECH, V31, P592, DOI 10.1002/ima.22510
   Bhavani KD, 2023, MULTIMED TOOLS APPL, DOI 10.1007/s11042-023-16476-6
   Butz R, 2022, ARTIF INTELL MED, V134, DOI 10.1016/j.artmed.2022.102438
   Cadrin-Chênevert A, 2022, RADIOL-ARTIF INTELL, V4, DOI 10.1148/ryai.220126
   Campana MG, 2023, PERVASIVE MOB COMPUT, V89, DOI 10.1016/j.pmcj.2023.101754
   Casilari E, 2020, SYMMETRY-BASEL, V12, DOI 10.3390/sym12040649
   Castro-Zunti R, 2021, COMPUT MED IMAG GRAP, V91, DOI 10.1016/j.compmedimag.2021.101937
   Chang WJ, 2021, IEEE ACCESS, V9, P129965, DOI 10.1109/ACCESS.2021.3113824
   Chen J, 2005, P ANN INT IEEE EMBS, P3551, DOI 10.1109/IEMBS.2005.1617246
   Cuevas-Trisan R, 2019, CLIN GERIATR MED, V35, P173, DOI 10.1016/j.cger.2019.01.008
   de Miguel K, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17122864
   Feng Y, 2021, KNOWL-BASED SYST, V217, DOI 10.1016/j.knosys.2021.106829
   Himeur Y, 2023, ENG APPL ARTIF INTEL, V119, DOI 10.1016/j.engappai.2022.105698
   Huang HH, 2024, IEEE J BIOMED HEALTH, V28, P2428, DOI 10.1109/JBHI.2024.3363081
   Hussain A, 2024, INFORM FUSION, V106, DOI 10.1016/j.inffus.2023.102211
   Igual R, 2013, BIOMED ENG ONLINE, V12, DOI 10.1186/1475-925X-12-66
   Inturi AR, 2023, ARAB J SCI ENG, V48, P1143, DOI 10.1007/s13369-022-06684-x
   Kalita I, 2023, MULTIMED TOOLS APPL, V82, P18409, DOI 10.1007/s11042-022-13946-1
   Kandagatla U.K., 2023, Fall Detection Dataset
   Ke ZM, 2021, IEEE T INTELL TRANSP, V22, P4684, DOI 10.1109/TITS.2020.2990598
   Kim JK, 2022, PROCEEDINGS OF THE 2022 THE 28TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, ACM MOBICOM 2022, P823, DOI 10.1145/3495243.3558250
   Kim JK, 2023, EXPERT SYST APPL, V234, DOI 10.1016/j.eswa.2023.121034
   Kordík P, 2018, MACH LEARN, V107, P177, DOI 10.1007/s10994-017-5682-0
   Lee E, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12136761
   Li K, 2022, KNOWL-BASED SYST, V254, DOI 10.1016/j.knosys.2022.109537
   Liang HW, 2024, J NEUROENG REHABIL, V21, DOI 10.1186/s12984-024-01310-3
   Lin C., 2019, Periodica Polytechnica Transp. Eng, V47, P242, DOI [10.3311/PPtr.11480, DOI 10.3311/PPTR.11480]
   Liu BF, 2021, ROBOT CIM-INT MANUF, V70, DOI 10.1016/j.rcim.2021.102128
   Lu J, 2015, KNOWL-BASED SYST, V80, P14, DOI 10.1016/j.knosys.2015.01.010
   Lustrek M, 2015, IEEE PERVAS COMPUT, V14, P72, DOI 10.1109/MPRV.2015.84
   Manakitsa N, 2024, TECHNOLOGIES, V12, DOI 10.3390/technologies12020015
   Mankodiya H, 2022, MATHEMATICS-BASEL, V10, DOI 10.3390/math10121990
   Maray N, 2023, SENSORS-BASEL, V23, DOI 10.3390/s23031105
   Mozaffari N, 2019, INTERNET THINGS-NETH, V8, DOI 10.1016/j.iot.2019.100124
   Mubashir M, 2013, NEUROCOMPUTING, V100, P144, DOI 10.1016/j.neucom.2011.09.037
   Neyshabur B., 2020, Advances in neural information processing systems, V33, P512
   Nimmi K., 2023, J. Ambient. Intell. Humaniz. Comput., P1
   Núñez-Marcos A, 2024, ENG APPL ARTIF INTEL, V132, DOI 10.1016/j.engappai.2024.107937
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Parashar A, 2023, PATTERN RECOGN LETT, V172, P65, DOI 10.1016/j.patrec.2023.05.021
   Plumb G., 2024, Adv. Neural Inf. Proces. Syst., V31
   Qi HA, 2024, J MANAGE ENG, V40, DOI 10.1061/JMENEA.MEENG-5485
   Qi WB, 2023, IEEE T GREEN COMMUN, V7, P393, DOI 10.1109/TGCN.2022.3233825
   Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Saarela M, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12199545
   Sadreazami H, 2019, IEEE INT SYM MED MEA
   Saleh M, 2019, IEEE SENS J, V19, P3156, DOI 10.1109/JSEN.2019.2891128
   Shah Shrishti, 2023, Journal of Electrical Systems and Information Technology, DOI 10.1186/s43067-023-00123-z
   Shenavarmasouleh F., 2021, ADV COMPUTER VISION, V2021, P307
   Singh A, 2020, IEEE SENS J, V20, P6889, DOI 10.1109/JSEN.2020.2976554
   Situ Z, 2023, DEV BUILT ENVIRON, V15, DOI 10.1016/j.dibe.2023.100191
   Srivastava G., 2024, arXiv
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tan CQ, 2018, LECT NOTES COMPUT SC, V11141, P270, DOI 10.1007/978-3-030-01424-7_27
   Thakur N, 2021, J SENS ACTUAT NETW, V10, DOI 10.3390/jsan10030039
   Torrey L., 2010, Handbook of research on machine learning applications and trends: algorithms, methods, and techniques, P242, DOI DOI 10.4018/978-1-60566-7669.CH011
   Vale D., 2022, AI and Ethics, V2, P815, DOI [10.1007/s43681-022-00142-y, DOI 10.1007/S43681-022-00142-Y]
   Vallabh P, 2018, J AMB INTEL HUM COMP, V9, P1809, DOI 10.1007/s12652-017-0592-3
   van der Velden BHM, 2022, MED IMAGE ANAL, V79, DOI 10.1016/j.media.2022.102470
   Verma S., 2023, Multimed. Tools Appl., P1
   Vrbancic G, 2020, IEEE ACCESS, V8, P196197, DOI 10.1109/ACCESS.2020.3034343
   Wang XY, 2020, FRONT ROBOT AI, V7, DOI 10.3389/frobt.2020.00071
   Weber L, 2023, INFORM FUSION, V92, P154, DOI 10.1016/j.inffus.2022.11.013
   Weiss Karl, 2016, Journal of Big Data, V3, DOI 10.1186/s40537-016-0043-6
   Wozniak Marcin, 2022, DroneCom '22: Proceedings of the 5th International ACM Mobicom Workshop on Drone Assisted Wireless Communications for 5G and Beyond, P121, DOI 10.1145/3555661.3560875
   Xu MR, 2022, IEEE WIREL COMMUN, V29, P132, DOI 10.1109/MWC.004.2100542
   Yu XQ, 2023, IEEE J BIOMED HEALTH, V27, P2197, DOI 10.1109/JBHI.2022.3228598
   Zafar MR, 2021, MACH LEARN KNOW EXTR, V3, P525, DOI 10.3390/make3030027
   Zhang L., 2024, IEEE Transactions on Neural Networks and Learning Systems
   Zhang SY, 2023, PROCEEDINGS OF THE 29TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, KDD 2023, P4842, DOI 10.1145/3580305.3599801
   Zhao S, 2024, IEEE T IND ELECTRON, V71, P7498, DOI 10.1109/TIE.2023.3310041
   Zhong HY, 2022, NEUROCOMPUTING, V501, P765, DOI 10.1016/j.neucom.2022.06.066
   Zhuang FZ, 2021, P IEEE, V109, P43, DOI 10.1109/JPROC.2020.3004555
   Zitouni M., 2019, J. Sens. Technol., V9, P71, DOI [10.4236/jst.2019.94007, DOI 10.4236/JST.2019.94007]
   Zolanvari M, 2023, IEEE INTERNET THINGS, V10, P2967, DOI 10.1109/JIOT.2021.3122019
NR 84
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD SEP
PY 2024
VL 149
AR 105164
DI 10.1016/j.imavis.2024.105164
EA JUL 2024
PG 19
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA YG1P8
UT WOS:001267245500001
DA 2024-08-05
ER

PT J
AU Putra, BHH
   Jeong, C
AF Putra, Bahy Helmi Hartoyo
   Jeong, Cheol
TI Video captioning based on dual learning via multiple reconstruction
   blocks
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Dual learning; Reconstruction network; Video captioning
ID IMAGE
AB In the context of video captioning, a conventional dual learning scheme involves two tasks: a primal task, which translates frame features into natural language captions, and a dual task, which reconstructs frame features from the generated captions. The dual task serves as a regularization mechanism for the primal task, providing feedback that helps to improve the accuracy of the generated captions. In prior research, it has been demonstrated that the inclusion of dual learning regularization into the architecture of a video captioning model can substantially improve performance. However, it remains an open question whether the performance of such a model can be further enhanced through the incorporation of additional regularizers. In this study, we investigate the use of multiple blocks of primal and dual tasks as additional regularizers in the model. Our experiments on benchmark datasets show that the appropriate number of additional regularizers can further improve the quality of the video captioning model and achieve state-of-the-art results.
C1 [Putra, Bahy Helmi Hartoyo; Jeong, Cheol] Sejong Univ, Dept Intelligent Mechatron Engn, Seoul, South Korea.
   Sejong Univ, Dept Convergence Engn Intelligent Drone, Seoul, South Korea.
C3 Sejong University; Sejong University
RP Jeong, C (corresponding author), Sejong Univ, Dept Intelligent Mechatron Engn, Seoul, South Korea.
EM bahy@sju.ac.kr; cheol.jeong@ieee.org
FU Institute of Information & Communications Technology Planning &
   Evaluation (IITP) grants - Korea government (MSIT) [RS-2022-00156345,
   2021-0-02067]; Korea Planning & Evaluation Institute of Industrial
   Technology (KEIT) grant - Ministry of Trade, Industry Energy (MOTIE)
   [20023583]
FX This work was supported by Institute of Information & Communications
   Technology Planning & Evaluation (IITP) grants funded by the Korea
   government (MSIT) (RS-2022-00156345 and No. 2021-0-02067) and Korea
   Planning & Evaluation Institute of Industrial Technology (KEIT) grant
   funded by the Ministry of Trade, Industry & Energy (MOTIE) (20023583) .
CR Amirian S, 2020, IEEE ACCESS, V8, P218386, DOI 10.1109/ACCESS.2020.3042484
   Bai Y, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3556, DOI 10.1145/3474085.3475519
   Banerjee S., 2005, P ACL WORKSHOP INTRI, P65, DOI DOI 10.3115/1626355.1626389
   Boxiao Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10867, DOI 10.1109/CVPR42600.2020.01088
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Changpinyo S, 2021, PROC CVPR IEEE, P3557, DOI 10.1109/CVPR46437.2021.00356
   Chen David, 2011, ACL
   Chen HR, 2022, COMPUT VIS IMAGE UND, V225, DOI 10.1016/j.cviu.2022.103581
   Chen HR, 2020, FRONT ARTIF INTEL AP, V325, P1079, DOI 10.3233/FAIA200204
   Chen HR, 2020, FRONT ROBOT AI, V7, DOI 10.3389/frobt.2020.475767
   Chen JW, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3539225
   Chen S., 2020, ECCV 2020
   Chen SX, 2019, AAAI CONF ARTIF INTE, P8191
   Chen SH, 2023, Arxiv, DOI arXiv:2304.08345
   Deb T., 2022, WACV 2022, P4070
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Donahue J, 2017, IEEE T PATTERN ANAL, V39, P677, DOI 10.1109/TPAMI.2016.2599174
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Freitag Markus, 2017, P 1 WORKSHOP NEURAL, P56, DOI [10.18653/v1/W17-3207, DOI 10.18653/V1/W17-3207]
   Gao LL, 2022, IEEE T IMAGE PROCESS, V31, P202, DOI 10.1109/TIP.2021.3120867
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   He D, 2016, ADV NEUR IN, V29
   He KM, 2015, Arxiv, DOI [arXiv:1512.03385, DOI 10.48550/ARXIV.1512.03385]
   Hu JT, 2022, IMAGE VISION COMPUT, V128, DOI 10.1016/j.imavis.2022.104575
   Hu XW, 2022, PROC CVPR IEEE, P17959, DOI 10.1109/CVPR52688.2022.01745
   Huang Q., 2018, 2018 NIPS
   Ji WT, 2022, APPL SOFT COMPUT, V117, DOI 10.1016/j.asoc.2021.108332
   Ji WT, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3446792
   Jing S., 2023, IEEE Trans. Multimed., P1
   Karpathy A, 2014, ADV NEUR IN, V27
   Li L, 2022, IEEE T IMAGE PROCESS, V31, P2726, DOI 10.1109/TIP.2022.3158546
   Li ZX, 2023, IMAGE VISION COMPUT, V129, DOI 10.1016/j.imavis.2022.104591
   Lin C.-Y., 2004, ANN M ASS COMP LING, P74
   Lin K, 2021, AAAI CONF ARTIF INTE, V35, P2047
   Lin K, 2022, PROC CVPR IEEE, P17928, DOI 10.1109/CVPR52688.2022.01742
   Luo HS, 2022, NEUROCOMPUTING, V508, P293, DOI 10.1016/j.neucom.2022.07.028
   Luo HS, 2020, Arxiv, DOI arXiv:2002.06353
   Miech A, 2019, IEEE I CONF COMP VIS, P2630, DOI 10.1109/ICCV.2019.00272
   Niu T.-Z., 2023, ACM Trans. Multimed. Comput. Commun. Appl., V19, P20
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Perez-Martin J, 2021, IEEE WINT CONF APPL, P3038, DOI 10.1109/WACV48630.2021.00308
   Perez-Martin J, 2021, INT C PATT RECOG, P5767, DOI 10.1109/ICPR48806.2021.9412898
   Qin Tao, 2020, DUAL LEARNING
   Radford A, 2021, PR MACH LEARN RES, V139
   Seo PH, 2022, PROC CVPR IEEE, P17938, DOI 10.1109/CVPR52688.2022.01743
   Shen ZQ, 2017, PROC CVPR IEEE, P5159, DOI 10.1109/CVPR.2017.548
   Shi XX, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P818, DOI 10.1145/3343031.3351060
   Shi YY, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3546828
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Tang M., 2021, arXiv
   Tang MK, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4858, DOI 10.1145/3474085.3479207
   Tian Y., 2019, 2019 CVPRW
   Tu ZP, 2017, AAAI CONF ARTIF INTE, P3097
   Vaidya J, 2022, IEEE WINT CONF APPL, P2442, DOI 10.1109/WACV51458.2022.00250
   Vaswani A, 2017, ADV NEUR IN, V30
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Wang BR, 2018, PROC CVPR IEEE, P7622, DOI 10.1109/CVPR.2018.00795
   Wang J., 2022, Transactions on Machine Learning Research
   Wang X, 2019, IEEE I CONF COMP VIS, P4580, DOI 10.1109/ICCV.2019.00468
   Wang ZA, 2022, IMAGE VISION COMPUT, V128, DOI 10.1016/j.imavis.2022.104570
   Xia YC, 2017, PR MACH LEARN RES, V70
   Xu HY, 2023, Arxiv, DOI arXiv:2302.00402
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Yan L., 2022, P 31 INT JOINT C ART, P2769, DOI [10.24963/ijcai.2022/384, DOI 10.24963/IJCAI.2022/384]
   Yan S, 2022, Arxiv, DOI arXiv:2212.04979
   Yang B, 2021, AAAI CONF ARTIF INTE, V35, P3119
   Yang M, 2019, IEEE T MULTIMEDIA, V21, P1047, DOI 10.1109/TMM.2018.2869276
   Yao ZY, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P2203, DOI 10.1145/3308558.3313632
   Ye HH, 2022, PROC CVPR IEEE, P17918, DOI 10.1109/CVPR52688.2022.01741
   Zeng P., IEEE Trans. Neural Netw. Learn.
   Zhang Haonan, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P4778, DOI 10.1145/3581783.3611714
   Zhang W, 2020, IEEE T PATTERN ANAL, V42, P3088, DOI 10.1109/TPAMI.2019.2920899
   Zhang ZQ, 2021, PROC CVPR IEEE, P9832, DOI 10.1109/CVPR46437.2021.00971
   Zhao W, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P29, DOI 10.1145/3132847.3132920
   Zheng Q., 2020, 2020 CVPR, P13093, DOI 10.1109/CVPR42600.2020.01311
   Zhou LW, 2018, AAAI CONF ARTIF INTE, P7590
   Ziqi Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13275, DOI 10.1109/CVPR42600.2020.01329
NR 79
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD AUG
PY 2024
VL 148
AR 105119
DI 10.1016/j.imavis.2024.105119
EA JUN 2024
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA XH3R1
UT WOS:001260757400001
DA 2024-08-05
ER

PT J
AU Xu, SC
   Chen, XX
   Zheng, YH
   Zhou, GY
   Chen, YR
   Zha, HB
   Zhao, H
AF Xu, Shaocong
   Chen, Xiaoxue
   Zheng, Yuhang
   Zhou, Guyue
   Chen, Yurong
   Zha, Hongbin
   Zhao, Hao
TI ECT: Fine-grained edge detection with learned cause tokens
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Edge detection; Edge cause; Fine-grained edge detection; Multi-task
   learning
AB In this study, we tackle the challenging fine-grained edge detection task, which refers to predicting specific edges caused by reflectance, illumination, normal, and depth changes, respectively. Prior methods exploit multi-scale convolutional networks, which are limited in three aspects: (1) Convolutions are local operators while identifying the cause of edge formation requires looking at far away pixels. (2) Priors specific to edge cause are fixed in prediction heads. (3) Using separate networks for generic and fine-grained edge detection, and the constraint between them may be violated. To address these three issues, we propose a two-stage transformer-based network sequentially predicting generic edges and fine-grained edges, which has a global receptive field thanks to the attention mechanism. The prior knowledge of edge causes is formulated as four learnable cause tokens in a cause-aware decoder design. Furthermore, to encourage the consistency between generic edges and fine-grained edges, an edge aggregation and alignment loss is exploited. We evaluate our method on the public benchmark BSDS-RIND and several newly derived benchmarks, and achieve new state-of-the-art results. Our code, data, and models are publicly available at https://github.com/Daniellli/ECT.git.
C1 [Xu, Shaocong] Xiamen Univ, Sch Informat, Xiamen 361005, Peoples R China.
   [Xu, Shaocong; Chen, Xiaoxue; Zhao, Hao] Tsinghua Univ, Inst AI Ind Res AIR, Beijing 100084, Peoples R China.
   [Chen, Xiaoxue] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
   [Zheng, Yuhang; Zhou, Guyue] Beihang Univ, Sch Mech Engn & Automat, Beijing 100084, Peoples R China.
   [Zha, Hongbin] Peking Univ, Sch Elect Engn & Comp Sci, Beijing 100084, Peoples R China.
   [Chen, Yurong] Intel Labs, Beijing 100026, Peoples R China.
C3 Xiamen University; Tsinghua University; Tsinghua University; Beihang
   University; Peking University; Intel Corporation
RP Zhao, H (corresponding author), Tsinghua Univ, Inst AI Ind Res AIR, Beijing 100084, Peoples R China.
EM xushaocong@stu.xmu.edu.cn; chenxiaoxue@air.tsinghua.edu.cn;
   zyh_021@buaa.edu.cn; zhouguyue@air.tsinghua.edu.cn;
   yurong.chen@intel.com; zha@cis.pku.edu.cn; zhaohao@air.tsinghua.edu.cn
CR Acuna D, 2019, PROC CVPR IEEE, P11067, DOI 10.1109/CVPR.2019.01133
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Bell S, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601206
   Benbihi A, 2020, IEEE INT CONF ROBOT, P3032, DOI [10.1109/ICRA40945.2020.9197529, 10.1109/icra40945.2020.9197529]
   Bertasius G, 2015, IEEE I CONF COMP VIS, P504, DOI 10.1109/ICCV.2015.65
   Bertasius G, 2015, PROC CVPR IEEE, P4380, DOI 10.1109/CVPR.2015.7299067
   Borse S, 2021, PROC CVPR IEEE, P5897, DOI 10.1109/CVPR46437.2021.00584
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Carion N., 2020, EUR C COMP VIS, P213
   Chen XX, 2022, PROC CVPR IEEE, P19617, DOI 10.1109/CVPR52688.2022.01903
   Chen XX, 2022, IEEE ROBOT AUTOM LET, V7, P2519, DOI 10.1109/LRA.2022.3143224
   Chen ZH, 2020, PROC CVPR IEEE, P5610, DOI 10.1109/CVPR42600.2020.00565
   Cheng B, 2021, ADV NEUR IN, V34
   Deng RX, 2018, LECT NOTES COMPUT SC, V11210, P570, DOI 10.1007/978-3-030-01231-1_35
   Dollar P., 2006, 2006 IEEE COMPUTER S, V2, P1964, DOI DOI 10.1109/CVPR.2006.298
   Dosovitskiy A., 2020, INT C LEARNING REPRE
   Gijsenij A, 2009, IEEE IMAGE PROC, P693, DOI 10.1109/ICIP.2009.5414089
   Gupta S, 2013, PROC CVPR IEEE, P564, DOI 10.1109/CVPR.2013.79
   He JZ, 2019, PROC CVPR IEEE, P3823, DOI 10.1109/CVPR.2019.00395
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hedau V, 2009, IEEE I CONF COMP VIS, P1849, DOI 10.1109/ICCV.2009.5459411
   Herb M, 2021, IEEE INT C INT ROBOT, P1124, DOI 10.1109/IROS51168.2021.9636517
   Hoiem D, 2011, INT J COMPUT VISION, V91, P328, DOI 10.1007/s11263-010-0400-4
   Hu Y, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P782
   Kelm AP, 2019, LECT NOTES COMPUT SC, V11678, P246, DOI 10.1007/978-3-030-29888-3_20
   Kittler J., 1983, Image and Vision Computing, V1, P37, DOI [DOI 10.1016/0262-8856(83)90006-9, 10.1016/0262-8856(83)90006-9]
   Kokkinos Iasonas, 2016, 4 INT C LEARNING REP
   Lim JJ, 2013, PROC CVPR IEEE, P3158, DOI 10.1109/CVPR.2013.406
   Liu Y, 2019, IEEE T PATTERN ANAL, V41, P1939, DOI 10.1109/TPAMI.2018.2878849
   Lu R, 2019, IEEE I CONF COMP VIS, P10342, DOI 10.1109/ICCV.2019.01044
   Maninis KK, 2016, LECT NOTES COMPUT SC, V9905, P580, DOI 10.1007/978-3-319-46448-0_35
   Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   Pu MY, 2022, PROC CVPR IEEE, P1392, DOI 10.1109/CVPR52688.2022.00146
   Pu MY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6859, DOI 10.1109/ICCV48922.2021.00680
   Qiu KJ, 2017, IEEE ROBOT AUTOM LET, V2, P1256, DOI 10.1109/LRA.2017.2660063
   Ramamonjisoa Y., 2020, P IEEE CVF C COMP VI, P14648
   Schwing AG, 2013, IEEE I CONF COMP VIS, P353, DOI 10.1109/ICCV.2013.51
   Shen L, 2015, PROC CVPR IEEE, P2067, DOI 10.1109/CVPR.2015.7298818
   Shen W, 2015, PROC CVPR IEEE, P3982, DOI 10.1109/CVPR.2015.7299024
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Soria X, 2020, IEEE WINT CONF APPL, P1912, DOI 10.1109/WACV45572.2020.9093290
   Strudel R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7242, DOI 10.1109/ICCV48922.2021.00717
   Vaswani A, 2017, ADV NEUR IN, V30
   Vincente TFY, 2016, LECT NOTES COMPUT SC, V9910, P816, DOI 10.1007/978-3-319-46466-4_49
   Wang C., 2020, Transactions on Pattern Analysis and Machine Intelligence, V44, P2641
   Wang JF, 2018, PROC CVPR IEEE, P1788, DOI 10.1109/CVPR.2018.00192
   Wang K, 2022, IEEE ROBOT AUTOM LET, V7, P976, DOI 10.1109/LRA.2021.3136307
   Wen YL, 2020, IEEE ROBOT AUTOM LET, V5, P4931, DOI 10.1109/LRA.2020.3005121
   Wu Q, 2012, IEEE INT CONF ROBOT, P2177, DOI 10.1109/ICRA.2012.6224561
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1007/s11263-017-1004-z, 10.1109/ICCV.2015.164]
   Xu D, 2017, ADV NEUR IN, V30
   Xu ZH, 2021, IEEE ROBOT AUTOM LET, V6, P7248, DOI 10.1109/LRA.2021.3097512
   Yang F, 2020, IEEE T INTELL TRANSP, V21, P1525, DOI 10.1109/TITS.2019.2910595
   Yu X, 2018, IEEE INT C INT ROBOT, P3196, DOI 10.1109/IROS.2018.8594358
   Yu ZD, 2018, LECT NOTES COMPUT SC, V11207, P400, DOI 10.1007/978-3-030-01219-9_24
   Yu ZD, 2017, PROC CVPR IEEE, P1761, DOI 10.1109/CVPR.2017.191
   Zhang WW, 2021, ADV NEUR IN, V34
   Zhao H, 2020, INT J COMPUT VISION, V128, P1076, DOI 10.1007/s11263-019-01263-4
   Zhao H, 2017, PROC CVPR IEEE, P870, DOI 10.1109/CVPR.2017.99
NR 60
TC 0
Z9 0
U1 3
U2 3
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD MAR
PY 2024
VL 143
AR 104947
DI 10.1016/j.imavis.2024.104947
EA FEB 2024
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA NR0L4
UT WOS:001202062100001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Ma, ZC
   Liu, ZX
   Wang, K
   Lian, SG
AF Ma, Zhicheng
   Liu, Zhaoxiang
   Wang, Kai
   Lian, Shiguo
TI Hybrid attention transformer with re-parameterized large kernel
   convolution for image super-resolution
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Image super -resolution; Transformer; Hybrid attention; Large kernel
   convolution; Re -parameterization
AB Single image super-resolution is a well-established low-level vision task that aims to reconstruct high-resolution images from low-resolution images. Methods based on Transformer have shown remarkable success and achieved outstanding performance in SISR tasks. While Transformer effectively models global information, it is less effective at capturing high frequencies such as stripes that primarily provide local information. Additionally, it has the potential to further enhance the capture of global information. To tackle this, we propose a novel Large Kernel Hybrid Attention Transformer using re-parameterization. It combines different kernel sizes and different steps re-parameterized convolution layers with Transformer to effectively capture global and local information to learn comprehensive features with low-frequency and high-frequency information. Moreover, in order to solve the problem of using batch normalization layer to introduce artifacts in SISR, we propose a new training strategy which is fusing convolution layer and batch normalization layer after certain training epochs. This strategy can enjoy the acceleration convergence effect of batch normalization layer in training and effectively eliminate the problem of artifacts in the inference stage. For re-parameterization of multiple parallel branch convolution layers, adopting this strategy can further reduce the amount of calculation of training. By coupling these core improvements, our LKHAT achieves state-of-the-art performance for single image super-resolution task.
C1 [Ma, Zhicheng; Liu, Zhaoxiang; Wang, Kai; Lian, Shiguo] China Unicom, AI Innovat Ctr, Beijing 100013, Peoples R China.
   [Ma, Zhicheng; Liu, Zhaoxiang; Wang, Kai; Lian, Shiguo] China Unicom, Unicom Digital Technol, Beijing 100013, Peoples R China.
C3 China United Network Communications Limited; China United Network
   Communications Limited
RP Liu, ZX; Lian, SG (corresponding author), China Unicom, AI Innovat Ctr, Beijing 100013, Peoples R China.
EM liuzx178@chinaunicom.cn; liansg@chinaunicom.cn
CR Ali A., 2021, C NEUR INF PROC SYST, V34, P20014
   Anasosalu Vasu P.K., 2022, ARXIV
   Ben Niu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P191, DOI 10.1007/978-3-030-58610-2_12
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen XY, 2023, PROC CVPR IEEE, P22367, DOI 10.1109/CVPR52729.2023.02142
   Chen Z., 2022, Advances in Neural Information Processing Systems, V35, P25478, DOI DOI 10.48550/ARXIV.2211.13654
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Ding XH, 2024, Arxiv, DOI arXiv:2311.15599
   Ding XH, 2022, PROC CVPR IEEE, P11953, DOI 10.1109/CVPR52688.2022.01166
   Ding XH, 2021, PROC CVPR IEEE, P13728, DOI 10.1109/CVPR46437.2021.01352
   Ding XH, 2019, IEEE I CONF COMP VIS, P1911, DOI 10.1109/ICCV.2019.00200
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dosovitskiy A., ARXIV
   Esser P, 2021, PROC CVPR IEEE, P12868, DOI 10.1109/CVPR46437.2021.01268
   Fan YC, 2017, IEEE COMPUT SOC CONF, P1157, DOI 10.1109/CVPRW.2017.154
   Fedus W, 2022, J MACH LEARN RES, V23
   Howard AG, 2017, Arxiv, DOI [arXiv:1704.04861, 10.48550/arXiv.1704.04861]
   Gao DD, 2023, EXPERT SYST APPL, V213, DOI 10.1016/j.eswa.2022.118898
   Gu JJ, 2021, PROC CVPR IEEE, P9195, DOI 10.1109/CVPR46437.2021.00908
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Kong FY, 2022, IEEE COMPUT SOC CONF, P765, DOI 10.1109/CVPRW56347.2022.00092
   Li KC, 2023, IEEE T PATTERN ANAL, V45, P12581, DOI 10.1109/TPAMI.2023.3282631
   Li WB, 2022, Arxiv, DOI arXiv:2112.10175
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu BZ, 2024, IEEE SIGNAL PROC LET, V31, P1575, DOI 10.1109/LSP.2024.3410017
   Liu SW, 2022, Arxiv, DOI [arXiv:2207.03620, DOI 10.48550/ARXIV.2207.03620, 10.48550/arXiv.2207.03620]
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Matsui Y, 2017, MULTIMED TOOLS APPL, V76, P21811, DOI 10.1007/s11042-016-4020-z
   Mei YQ, 2021, PROC CVPR IEEE, P3516, DOI 10.1109/CVPR46437.2021.00352
   Shamsolmoali P, 2019, IMAGE VISION COMPUT, V88, P9, DOI 10.1016/j.imavis.2019.03.006
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Su JN, 2024, IEEE T IMAGE PROCESS, V33, P610, DOI 10.1109/TIP.2023.3348293
   Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149
   Wang PC, 2022, LECT NOTES COMPUT SC, V13684, P285, DOI 10.1007/978-3-031-20053-3_17
   Wang WH, 2022, COMPUT VIS MEDIA, V8, P415, DOI 10.1007/s41095-022-0274-8
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   WANG X., 2022, BASICSR OPEN SOURCE
   Wang XT, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P2556, DOI 10.1145/3503161.3547915
   Wu HP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P22, DOI 10.1109/ICCV48922.2021.00009
   Xiao T, 2021, ADV NEUR IN, V34
   Yu FS, 2016, Arxiv, DOI arXiv:1511.07122
   Zeyde R., 2012, INT C CURV SURF, P711
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhou D., 2021, arXiv
   Zhou DQ, 2021, Arxiv, DOI arXiv:2103.11886
   Zhou S., 2020, Advances in Neural Information Processing Systems, V33, P3499
   Zhou XQ, 2024, IEEE T MULTIMEDIA, V26, P6475, DOI 10.1109/TMM.2024.3352400
NR 53
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD SEP
PY 2024
VL 149
AR 105162
DI 10.1016/j.imavis.2024.105162
EA JUL 2024
PG 9
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA YH7B3
UT WOS:001267652300001
DA 2024-08-05
ER

PT J
AU Wang, HY
   Song, KN
   Jiang, X
   He, ZQ
AF Wang, Hengyou
   Song, Kani
   Jiang, Xiang
   He, Zhiquan
TI ragBERT: Relationship-aligned and grammar-wise BERT model for image
   captioning
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Image captioning; Relationship tags; Grammar; BERT
AB Image captioning has become one of the most popular research problems in the field of artificial intelligence. Although many studies have achieved excellent results, there still are some challenges, for example, cross-modal feature alignment lacks explicit guidance, and model-generated sentences contain grammatical errors. In this paper, we propose a relationship-aligned and grammar-wise BERT model, which integrates a relationship exploration module and a grammar enhancement module into the BERT-based model. Specifically, in the relationship exploration module, to explore relationship tags as anchors to guide semantic alignment, we design a network to calculate the cosine similarity between visual features and word vector information. We construct the grammar enhancement module similarly to the BERT. That means we use two BERT modules in our framework. The first is the main frame for generating captions, and the second is the auxiliary model to determine whether the syntax of the generated caption is correct. To validate the performance of our proposed model, we conduct abundant experiments on the MSCOCO dataset, Flickr30k dataset, and Flickr8k dataset. Experimental results show that our proposed method performs better than state-of-the-art approaches.
C1 [Wang, Hengyou; Song, Kani] Beijing Univ Civil Engn & Architecture, Sch Sci, Beijing 100044, Peoples R China.
   [Jiang, Xiang] Jiangsu Normal Univ, Sch Comp Sci & Technol, Xuzhou 221116, Peoples R China.
   [He, Zhiquan] Shenzhen Univ, Guangdong Multimedia Informat Serv Engn Technol, Shenzhen 518060, Peoples R China.
C3 Beijing University of Civil Engineering & Architecture; Jiangsu Normal
   University; Shenzhen University
RP Wang, HY (corresponding author), Beijing Univ Civil Engn & Architecture, Sch Sci, Beijing 100044, Peoples R China.
EM wanghengyou@bucea.edu.cn
FU National Natural Science Foundation of China [62072024, 61971290];
   Outstanding Youth Program of Beijing University of Civil Engineering and
   Architecture [JDJQ20220805]; BUCEA Post Graduate Innovation Project
FX This work was supported in part by the National Natural Science
   Foundation of China (Nos. 62072024, 61971290), the outstanding Youth
   Program of Beijing University of Civil Engineering and Architecture
   (No.JDJQ20220805), the BUCEA Post Graduate Innovation Project.
CR Alahmadi Rehab, 2022, P IEEE CVF WINT C AP, P1025
   Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24
   Banerjee S., 2005, ACL WORKSH INTR EXTR, P65
   Dai B, 2017, PROC CVPR IEEE, P3298, DOI 10.1109/CVPR.2017.352
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Ding ST, 2020, NEUROCOMPUTING, V398, P520, DOI 10.1016/j.neucom.2019.04.095
   Gan C, 2017, PROC CVPR IEEE, P955, DOI 10.1109/CVPR.2017.108
   Guo LT, 2019, PROC CVPR IEEE, P4199, DOI 10.1109/CVPR.2019.00433
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Hodosh M, 2013, J ARTIF INTELL RES, V47, P853, DOI 10.1613/jair.3994
   Huang L, 2019, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2019.00473
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Kulkarni G, 2013, IEEE T PATTERN ANAL, V35, P2891, DOI 10.1109/TPAMI.2012.162
   Li X., 2020, Lecture Notes in Computer Science, P121, DOI DOI 10.1007/978-3-030-58577-8_8
   Li YK, 2017, IEEE I CONF COMP VIS, P1270, DOI 10.1109/ICCV.2017.142
   Lin C.-Y., 2004, ANN M ASS COMP LING, P74
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lu CW, 2016, LECT NOTES COMPUT SC, V9905, P852, DOI 10.1007/978-3-319-46448-0_51
   Mathews A, 2016, AAAI CONF ARTIF INTE, P3574
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Plummer BA, 2015, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2015.303
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Shizhe Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9959, DOI 10.1109/CVPR42600.2020.00998
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sutskever I, 2014, ADV NEUR IN, V27
   Vaswani A, 2017, ADV NEUR IN, V30
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang CZ, 2022, APPL INTELL, V52, P6575, DOI 10.1007/s10489-021-02734-3
   Wang SJ, 2021, PROC CVPR IEEE, P14045, DOI 10.1109/CVPR46437.2021.01383
   Wu Q, 2018, IEEE T PATTERN ANAL, V40, P1367, DOI 10.1109/TPAMI.2017.2708709
   Xiao XY, 2019, IEEE T MULTIMEDIA, V21, P2942, DOI 10.1109/TMM.2019.2915033
   Xu DF, 2017, PROC CVPR IEEE, P3097, DOI 10.1109/CVPR.2017.330
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yan CG, 2022, IEEE T CIRC SYST VID, V32, P43, DOI 10.1109/TCSVT.2021.3067449
   Yang LY, 2021, IEEE T MULTIMEDIA, V23, P835, DOI 10.1109/TMM.2020.2990074
   Yang X, 2019, PROC CVPR IEEE, P10677, DOI 10.1109/CVPR.2019.01094
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Zhang J, 2021, IEEE T MULTIMEDIA, V23, P92, DOI 10.1109/TMM.2020.2976552
   Zhang PC, 2021, PROC CVPR IEEE, P5575, DOI 10.1109/CVPR46437.2021.00553
   Zhang XY, 2021, PROC CVPR IEEE, P15460, DOI 10.1109/CVPR46437.2021.01521
   Zhang ZJ, 2021, IMAGE VISION COMPUT, V109, DOI 10.1016/j.imavis.2021.104146
   Zhou LW, 2020, AAAI CONF ARTIF INTE, V34, P13041
   Zhu XX, 2018, NEUROCOMPUTING, V319, P55, DOI 10.1016/j.neucom.2018.08.069
NR 47
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD AUG
PY 2024
VL 148
AR 105105
DI 10.1016/j.imavis.2024.105105
EA JUN 2024
PG 10
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA UZ5W1
UT WOS:001251909400001
DA 2024-08-05
ER

PT J
AU Guo, Z
   Zhang, PZ
   Liang, P
AF Guo, Zhen
   Zhang, Pengzhou
   Liang, Peng
TI SAKD: Sparse attention knowledge distillation
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Knowledge distillation; Attention mechanisms; Sparse attention
   mechanisms
AB Deep learning techniques have gained significant interest due to their success in large model scenarios. However, large models often require massive computational resources, which can challenge end devices with limited storage capabilities. Transferring knowledge from big to small models and achieving similar results with limited resources requires further research. Knowledge distillation techniques, which involve using teacher-student models to migrate large model capabilities to small models, have been widely used in model compression and knowledge transfer. In this paper, a novel knowledge distillation approach is proposed, which utilizes the sparse attention mechanism (SAKD). SAKD computes attention using student features as queries and teacher features as key values and performs sparse attention values by random deactivation. Then, this sparse attention value is used to reweight the feature distance of each teacher-student feature pair to avoid negative transfer. Comprehensive experiments demonstrate the effectiveness and generality of our approach. Moreover, our SAKD method outperforms previous state-of-the-art methods on image classification tasks.
C1 [Guo, Zhen; Zhang, Pengzhou] Commun Univ China, State Key Lab Media Convergence & Commun, Dingfuzhuang East St 1, Beijing 100024, Peoples R China.
   [Guo, Zhen; Liang, Peng] China Unicom Smart City Res Inst, Shoutinanlu 9, Beijing 100024, Peoples R China.
C3 Communication University of China
RP Guo, Z; Zhang, PZ (corresponding author), Commun Univ China, State Key Lab Media Convergence & Commun, Dingfuzhuang East St 1, Beijing 100024, Peoples R China.
EM cathy.guozhen@cuc.edu.cn; zhangpengzhou@cuc.edu.cn;
   liangp@chinaunicom.cn
FU National Key R & D Program of China [2020AAA0108700]; Fundamental
   Research Funds for the Cen- tral Universities [CUC21GZ014]
FX This work is supported by the National Key R & D Program of China
   (2020AAA0108700) and the Fundamental Research Funds for the Cen- tral
   Universities (CUC21GZ014) .
CR Ahn S, 2019, PROC CVPR IEEE, P9155, DOI 10.1109/CVPR.2019.00938
   Chattopadhyay A., 2018, Grad-Cam++: Improved Visual Explanations for Deep Convolutional Networks
   Chen DF, 2021, AAAI CONF ARTIF INTE, V35, P7028
   Chen K., 2022, CVPRW
   Chen Pengguang, 2021, CVPR
   Dong PJ, 2023, PROC CVPR IEEE, P11898, DOI 10.1109/CVPR52729.2023.01145
   Dong Peijie, 2023, ICASSP
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Fan HQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6804, DOI 10.1109/ICCV48922.2021.00675
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Gou JP, 2023, INT J COMPUT VISION, V131, P1857, DOI 10.1007/s11263-023-01792-z
   Gou JP, 2024, ACM T MULTIM COMPUT, V20, DOI 10.1145/3568679
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Han K, 2021, ADV NEUR IN
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heo B, 2019, IEEE I CONF COMP VIS, P1921, DOI 10.1109/ICCV.2019.00201
   Heo B, 2019, AAAI CONF ARTIF INTE, P3779
   Hinton G., 2015, NEURIPS DEEP LEARN R, P9
   Hu YM, 2021, PATTERN RECOGN, V118, DOI 10.1016/j.patcog.2021.108025
   Huang Z., 2019, ICLR
   Kim J, 2018, ADV NEUR IN, V31
   Li L., 2024, Kd-Zero: Evolving Knowledge Distiller for any Teacher-Student Pairs
   Li L., 2022, Shadow Knowledge Distillation: Bridging Offline and Online Knowledge Transfer
   Liu X., 2023, ICLR
   Liu Xiaolong, 2023, ICLR
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lu L., 2024, Uniads: Universal ArchitectureDistiller Search for Distillation Gap
   Park W, 2019, PROC CVPR IEEE, P3962, DOI 10.1109/CVPR.2019.00409
   Passalis N, 2018, LECT NOTES COMPUT SC, V11215, P283, DOI 10.1007/978-3-030-01252-6_17
   Peng BY, 2019, IEEE I CONF COMP VIS, P5006, DOI 10.1109/ICCV.2019.00511
   Qin J, 2022, AAAI CONF ARTIF INTE, P2117
   Romero A., 2015, ICLR, P1
   Rong Yu, 2020, INT C LEARN REPR
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Tian Y., 2020, ICLR
   Tung F, 2019, IEEE I CONF COMP VIS, P1365, DOI 10.1109/ICCV.2019.00145
   Vaswani A, 2017, ADV NEUR IN, V30
   Wan M., 2013, PROC INT C MACH LEA, P1058
   Wang L, 2022, IEEE T PATTERN ANAL, V44, P3048, DOI 10.1109/TPAMI.2021.3055564
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wang YQ, 2021, PROC CVPR IEEE, P8737, DOI 10.1109/CVPR46437.2021.00863
   Wu HP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P22, DOI 10.1109/ICCV48922.2021.00009
   Yang Jing, 2021, ICLR2021
   Yang Z., 2022, ECCV
   Yim J, 2017, PROC CVPR IEEE, P7130, DOI 10.1109/CVPR.2017.754
   Yuan L, 2023, IEEE T PATTERN ANAL, V45, P6575, DOI 10.1109/TPAMI.2022.3206108
   Yuan L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P538, DOI 10.1109/ICCV48922.2021.00060
   Zagoruyko S., 2017, ICLR, DOI DOI 10.1016/J.CVIU.2019.07.006.ARXIV:1612.0
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhu C., 2024, Saswot: Real-Time Semantic Segmentation Architecture Search without Training
   Zimian Wei Z., 2024, Auto-Prox: Training-Free Vision Transformer Architecture Search Via Automatic Proxy Discovery
NR 51
TC 1
Z9 1
U1 5
U2 5
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUN
PY 2024
VL 146
AR 105020
DI 10.1016/j.imavis.2024.105020
EA APR 2024
PG 8
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA RY0A9
UT WOS:001231089200001
DA 2024-08-05
ER

PT J
AU Moutik, O
   Sekkat, H
   Tchakoucht, TA
   El Kari, B
   Alaoui, AE
AF Moutik, Oumaima
   Sekkat, Hiba
   Tchakoucht, Taha Ait
   El Kari, Badr
   Alaoui, Ahmed El Hilali
TI A puzzle questions form training for self-supervised skeleton-based
   action recognition
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Skeleton -based action recognition; Self -supervised learning; Solving
   pretext task; Self -supervised skeleton -based action; recognition
ID LSTM
AB This paper proposed a novel pretext task to address the skeleton-based video representation for self-supervised action recognition tasks. Instead of exploiting only the whole body, various levels of the skeleton structure (e.g., upper body, lower body, left arm, left leg, right arm, right leg, and torso) are employed to extract essential coarser-grained characteristics. This involves computing statistical representations like motion, orientation, trajectory, and magnitude shift from unlabeled skeleton configurations. Then a learning model is built and trained to yield these statistical representations given the sequence configuration as the input. Our approach is question-driven, where each question acts as a puzzle piece contributing to a deeper understanding of the skeleton joint configuration. It's inspired by the ability of the cognitive system observed in individuals to hypothesize unseen actions. This is accomplished by posing pertinent questions and envisioning plausible scenarios to recognize the actions taking place. The answers to these devised questions are derived from the statistical representation of skeleton configurations. To this end, we made 44 questions designed to encompass the broadest overview to the finest detail. Our experiments on the NTU RGB-D, NW-UCLA, and PKU-MMD datasets demonstrate outstanding results in action recognition, proving the superiority of our approach in learning discriminative characteristics.
C1 [Moutik, Oumaima; Sekkat, Hiba; Tchakoucht, Taha Ait; El Kari, Badr; Alaoui, Ahmed El Hilali] Euromed Univ Fes, UEMF, Fes, Morocco.
RP Moutik, O (corresponding author), Euromed Univ Fes, UEMF, Fes, Morocco.
EM o.moutik@ueuromed.org
CR Alzubaidi L, 2021, J BIG DATA-GER, V8, DOI 10.1186/s40537-021-00444-8
   Amin J, 2023, IMAGE VISION COMPUT, V135, DOI 10.1016/j.imavis.2023.104710
   Arnab A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6816, DOI 10.1109/ICCV48922.2021.00676
   Beddiar DR, 2020, MULTIMED TOOLS APPL, V79, P30509, DOI 10.1007/s11042-020-09004-3
   Chen J, 2021, IMAGE VISION COMPUT, V112, DOI 10.1016/j.imavis.2021.104214
   Cheng K, 2020, PROC CVPR IEEE, P180, DOI 10.1109/CVPR42600.2020.00026
   Chenyang Si, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P35, DOI 10.1007/978-3-030-58571-6_3
   Chi HG, 2022, PROC CVPR IEEE, P20154, DOI 10.1109/CVPR52688.2022.01955
   Della Villa F, 2020, BRIT J SPORT MED, V54, P1423, DOI 10.1136/bjsports-2019-101247
   Divjak D, 2020, COGN LINGUIST, V31, P37, DOI 10.1515/cog-2018-0103
   Dong Jianfeng, 2023, P AAAI C ART INT, V37, P525
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Goyal P, 2019, IEEE I CONF COMP VIS, P6400, DOI 10.1109/ICCV.2019.00649
   Guo TY, 2022, AAAI CONF ARTIF INTE, P762
   Huang SC, 2023, NPJ DIGIT MED, V6, DOI 10.1038/s41746-023-00811-0
   Jin ZH, 2024, IEEE T CIRC SYST VID, V34, P274, DOI 10.1109/TCSVT.2023.3284493
   Kim B, 2022, LECT NOTES COMPUT SC, V13664, P209, DOI 10.1007/978-3-031-19772-7_13
   Kun Su, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9628, DOI 10.1109/CVPR42600.2020.00965
   Leroux S, 2022, IEEE WINT CONF APPL, P3027, DOI 10.1109/WACV51458.2022.00308
   Li D, 2023, IMAGE VISION COMPUT, V135, DOI 10.1016/j.imavis.2023.104689
   Li LG, 2021, PROC CVPR IEEE, P4739, DOI 10.1109/CVPR46437.2021.00471
   Li PZ, 2021, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR46437.2021.00560
   Lin LL, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2490, DOI 10.1145/3394171.3413548
   Liu JY, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3365212
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P2684, DOI 10.1109/TPAMI.2019.2916873
   Liu MY, 2017, PATTERN RECOGN, V68, P346, DOI 10.1016/j.patcog.2017.02.030
   Men Q, 2023, NEUROCOMPUTING, V537, P198, DOI 10.1016/j.neucom.2023.03.070
   Mohamed A, 2022, IEEE J-STSP, V16, P1179, DOI 10.1109/JSTSP.2022.3207050
   Muhammad K, 2021, FUTURE GENER COMP SY, V125, P820, DOI 10.1016/j.future.2021.06.045
   Peng K., 2024, P AAAI C ART INT, V38, P4487
   Qin ZY, 2024, IEEE T NEUR NET LEAR, V35, P4783, DOI 10.1109/TNNLS.2022.3201518
   Rao HC, 2021, INFORM SCIENCES, V569, P90, DOI 10.1016/j.ins.2021.04.023
   Schiappa MC, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3577925
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Si CY, 2019, PROC CVPR IEEE, P1227, DOI 10.1109/CVPR.2019.00132
   Su YK, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13308, DOI 10.1109/ICCV48922.2021.01308
   Sun N, 2021, IMAGE VISION COMPUT, V109, DOI 10.1016/j.imavis.2021.104141
   Thien HT, 2020, INFORM SCIENCES, V513, P112, DOI 10.1016/j.ins.2019.10.047
   Thoker FM, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1655, DOI 10.1145/3474085.3475307
   Wang J., 2020, ECCV, P504
   Wang J, 2014, PROC CVPR IEEE, P2649, DOI 10.1109/CVPR.2014.339
   Wang J, 2014, IEEE T PATTERN ANAL, V36, P914, DOI 10.1109/TPAMI.2013.198
   Wang P, 2022, IEEE T IMAGE PROCESS, V31, P6224, DOI 10.1109/TIP.2022.3207577
   Wang PC, 2018, KNOWL-BASED SYST, V158, P43, DOI 10.1016/j.knosys.2018.05.029
   Wang Qingtian, 2023, 2023 IEEE International Conference on Big Data (BigData), P936, DOI 10.1109/BigData59044.2023.10386970
   Xu SH, 2023, IEEE T MULTIMEDIA, V25, P624, DOI 10.1109/TMM.2021.3129616
   Xu SA, 2021, IEEE INTERNET THINGS, V8, P15990, DOI 10.1109/JIOT.2020.3042986
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yang SY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13403, DOI 10.1109/ICCV48922.2021.01317
   Yang XM, 2023, IMAGE VISION COMPUT, V137, DOI 10.1016/j.imavis.2023.104765
   You W, 2022, IEEE ACCESS, V10, P36385, DOI 10.1109/ACCESS.2022.3165040
   Yu JL, 2024, IEEE T KNOWL DATA EN, V36, P335, DOI 10.1109/TKDE.2023.3282907
   Zhai XH, 2019, IEEE I CONF COMP VIS, P1476, DOI 10.1109/ICCV.2019.00156
   Zhang PF, 2019, IEEE T PATTERN ANAL, V41, P1963, DOI 10.1109/TPAMI.2019.2896631
   Zheng NG, 2018, AAAI CONF ARTIF INTE, P2644
   Zhu YS, 2023, IEEE I CONF COMP VIS, P13867, DOI 10.1109/ICCV51070.2023.01279
NR 56
TC 0
Z9 0
U1 2
U2 2
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD AUG
PY 2024
VL 148
AR 105137
DI 10.1016/j.imavis.2024.105137
EA JUN 2024
PG 11
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA XD7H7
UT WOS:001259806900001
DA 2024-08-05
ER

PT J
AU Luo, LG
   Yi, BS
   Wang, ZY
   He, Z
   Zhu, C
AF Luo, Laigan
   Yi, Benshun
   Wang, Zhongyuan
   He, Zheng
   Zhu, Chao
TI Bidirectional scale-aware upsampling network for arbitrary-scale video
   super-resolution
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Video super -resolution; Arbitrary -scale factor; Bidirectional module;
   Upsampling module
ID IMAGE SUPERRESOLUTION
AB The performance of video super-resolution (VSR) has significantly improved. However, the current methods only focus on a single scale factor, treating the VSR of different scale factors independently and disregarding video super-resolution of arbitrary-scale factors. To address this issue, we propose a model, the Bidirectional ScaleAware Upsampling Network for Arbitrary-Scale Video Super-Resolution, which eliminates the need for multiple models for various scale factors. We design a Bidirectional Scale-Aware Upsampling module in the proposed model, consisting of a Bidirectional Scale-Aware Module (BSAM) and a Spatial Pyramid Upsampling section. The BSAM extracts feature for various scale factors and allows feature information of different scales to interact bidirectionally. Additionally, we propose a Spatial Pyramid Loss that optimizes the network based on upsampling and maps the results of different scales to a unified spatial set to find the arbitrary-scale factor's loss. Along with this, we introduce an Explicit Feature Pyramid module, which uses Spatial Pyramid Upsampling to learn arbitrary-scale factor details explicitly. Finally, we demonstrate the extensibility of the model through a VSR algorithm integration with the Bidirectional Scale-Aware Upsampling, ensuring high-resolution results of arbitrary-scale factors without affecting the performance. Our comprehensive experiments on public benchmarks show promising results for video super-resolution of arbitrary-scale factors.
C1 [Luo, Laigan; Yi, Benshun; Zhu, Chao] Wuhan Univ, Elect Informat Sch, Wuhan 430072, Peoples R China.
   [Wang, Zhongyuan; He, Zheng] Wuhan Univ, Natl Engn Res Ctr Multimedia Software, Sch Comp Sci, Wuhan 430072, Peoples R China.
C3 Wuhan University; Wuhan University
RP Yi, BS (corresponding author), Wuhan Univ, Elect Informat Sch, Wuhan 430072, Peoples R China.; Wang, ZY (corresponding author), Wuhan Univ, Natl Engn Res Ctr Multimedia Software, Sch Comp Sci, Wuhan 430072, Peoples R China.
EM lgtristan5119@gmail.com; luolaigan@whu.edu.cn
FU National Natural Science Foundation of China [62071339, 62371350,
   U1903214]; Natural Science Foundation of Hubei Province [2021CFB464]
FX This research was funded by National Natural Science Foundation of China
   (62071339, 62371350, U1903214) and Natural Science Foundation of Hubei
   Province (2021CFB464) .
CR Behjati P, 2021, IEEE WINT CONF APPL, P2693, DOI 10.1109/WACV48630.2021.00274
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Caballero J, 2017, PROC CVPR IEEE, P2848, DOI 10.1109/CVPR.2017.304
   Chan KCK, 2022, PROC CVPR IEEE, P5962, DOI 10.1109/CVPR52688.2022.00588
   Chan KCK, 2021, PROC CVPR IEEE, P4945, DOI 10.1109/CVPR46437.2021.00491
   Chen HW, 2023, PROC CVPR IEEE, P18257, DOI 10.1109/CVPR52729.2023.01751
   Chen YB, 2021, PROC CVPR IEEE, P8624, DOI 10.1109/CVPR46437.2021.00852
   Chen YJ, 2017, IEEE T PATTERN ANAL, V39, P1256, DOI 10.1109/TPAMI.2016.2596743
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Guo J, 2017, AAAI CONF ARTIF INTE, P4053
   Haris M, 2020, PROC CVPR IEEE, P2856, DOI 10.1109/CVPR42600.2020.00293
   Hu XC, 2019, PROC CVPR IEEE, P1575, DOI 10.1109/CVPR.2019.00167
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Huang Y, 2015, ADV NEUR IN, V28
   Huang Y, 2018, IEEE T PATTERN ANAL, V40, P1015, DOI 10.1109/TPAMI.2017.2701380
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Isobe Takashi, 2022, P IEEECVF C COMPUTER, P17411
   Jiang XR, 2021, NEURAL NETWORKS, V144, P21, DOI 10.1016/j.neunet.2021.08.002
   Jo Y, 2018, PROC CVPR IEEE, P3224, DOI 10.1109/CVPR.2018.00340
   Kappeler A, 2016, IEEE T COMPUT IMAG, V2, P109, DOI 10.1109/TCI.2016.2532323
   Kim SY, 2019, IEEE IMAGE PROC, P2831, DOI [10.1109/ICIP.2019.8803297, 10.1109/icip.2019.8803297]
   Kingma D. P., 2014, arXiv
   Lee J, 2022, PROC CVPR IEEE, P1928, DOI 10.1109/CVPR52688.2022.00197
   Li DY, 2017, IEEE T COMPUT IMAG, V3, P749, DOI 10.1109/TCI.2017.2671360
   Li Wenbo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P335, DOI 10.1007/978-3-030-58607-2_20
   Liao RJ, 2015, IEEE I CONF COMP VIS, P531, DOI 10.1109/ICCV.2015.68
   Liu HY, 2022, ARTIF INTELL REV, V55, P5981, DOI 10.1007/s10462-022-10147-y
   Liu H, 2020, NEURAL NETWORKS, V132, P84, DOI 10.1016/j.neunet.2020.08.008
   Liu J, 2020, PROC CVPR IEEE, P2356, DOI 10.1109/CVPR42600.2020.00243
   Liu Y.T., 2021, arXiv
   Lucas A, 2019, IEEE T IMAGE PROCESS, V28, P3312, DOI 10.1109/TIP.2019.2895768
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Nah S, 2019, IEEE COMPUT SOC CONF, P1996, DOI 10.1109/CVPRW.2019.00251
   Osendorfer C, 2014, LECT NOTES COMPUT SC, V8836, P250, DOI 10.1007/978-3-319-12643-2_31
   Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291
   Sajjadi MSM, 2018, PROC CVPR IEEE, P6626, DOI 10.1109/CVPR.2018.00693
   Shamsolmoali P, 2019, IMAGE VISION COMPUT, V88, P9, DOI 10.1016/j.imavis.2019.03.006
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Sun L, 2021, IEEE-CAA J AUTOMATIC, V8, P1271, DOI 10.1109/JAS.2021.1004009
   Tao X, 2017, IEEE I CONF COMP VIS, P4482, DOI 10.1109/ICCV.2017.479
   Tian YP, 2020, PROC CVPR IEEE, P3357, DOI 10.1109/CVPR42600.2020.00342
   Wang LG, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4781, DOI 10.1109/ICCV48922.2021.00476
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang XT, 2019, IEEE COMPUT SOC CONF, P1954, DOI 10.1109/CVPRW.2019.00247
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZH, 2021, IEEE T PATTERN ANAL, V43, P3365, DOI 10.1109/TPAMI.2020.2982166
   Wang ZY, 2019, IEEE T IMAGE PROCESS, V28, P2530, DOI 10.1109/TIP.2018.2887017
   Wu H., 2021, arXiv
   Xu XQ, 2022, Arxiv, DOI arXiv:2103.12716
   Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2
   Yao XX, 2019, IMAGE VISION COMPUT, V82, P39, DOI 10.1016/j.imavis.2019.02.002
   Yi P, 2022, IEEE T PATTERN ANAL, V44, P2264, DOI 10.1109/TPAMI.2020.3042298
   Yi P, 2019, IEEE I CONF COMP VIS, P3106, DOI 10.1109/ICCV.2019.00320
   Yi P, 2020, IEEE T CIRC SYST VID, V30, P2503, DOI 10.1109/TCSVT.2019.2925844
   Zeyde R., 2012, INT C CURV SURF, P711
NR 58
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD AUG
PY 2024
VL 148
AR 105116
DI 10.1016/j.imavis.2024.105116
EA JUN 2024
PG 13
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA XD0Z8
UT WOS:001259642600001
DA 2024-08-05
ER

PT J
AU Zhang, X
   Lin, C
   Li, FZ
   Cao, YJ
   Li, YJ
AF Zhang, Xiao
   Lin, Chuan
   Li, Fuzhang
   Cao, Yijun
   Li, Yongjie
TI LVP-net: A deep network of learning visual pathway for edge detection
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Edge detection; Enhancer; Color-opponency; Visual pathway; Convolutional
   neural networks
ID COLOR; BOUNDARIES; CONNECTIONS
AB Deep learning-based edge detectors typically consist of the encoder and the decoder. To integrate multi-scale features into a global edge map effectively, researchers utilize classification networks such as VGG16 as the encoder and focus on the decoder architecture. In contrast to existing approaches, we propose a novel deep network for edge detection called learning-visual-pathway network (LVP-Net), in which an enhancer-encoderdecoder architecture is designed inspired by the biological visual pathway: the retina/lateral geniculate nucleus-*the primary visual cortex (V1) -* V2 -* V4 -* the inferior temporal cortex (IT). To simulate the visual mechanisms along this pathway, we design a feature enhancer network (FENet) that boosts the feature representation capability of the encoder. FENet is combined with VGG16 based on the hierarchical structure of the pathway. Furthermore, inspired by the integration ability of multiple features in IT, we introduce a feedforward fusion module (FFM). Finally, we evaluate LVP-Net on three benchmark datasets, i.e., BSDS500, NYUDv2, and Multicue. Experimental results demonstrate that our method achieves competitive performance compared with most state-of-the-art approaches.
C1 [Zhang, Xiao; Lin, Chuan; Li, Fuzhang] Guangxi Univ Sci & Technol, Sch Automat, Liuzhou 545006, Guangxi, Peoples R China.
   [Cao, Yijun; Li, Yongjie] Univ Elect Sci & Technol China, Sch Life Sci & Technol, Chengdu 610054, Sichuan, Peoples R China.
C3 Guangxi University of Science & Technology; University of Electronic
   Science & Technology of China
RP Lin, C (corresponding author), Guangxi Univ Sci & Technol, Sch Automat, Liuzhou 545006, Guangxi, Peoples R China.
EM chuanlin@gxust.edu.cn
RI Xiao, Zhang/HNR-5959-2023
OI Xiao, Zhang/0000-0002-8627-8921
FU National Natural Science Foundation of China [62266006, 61866002];
   Guangxi Natural Science Foundation [2020GXNSFDA297006,
   2018GXNSFAA138122, 2015GXNSFAA139293]; Project of the Key Laboratory of
   AI and Information Processing (Hechi University); Education Department
   of Guangxi Zhuang Autonomous Region [2022GXZDSY013]
FX The authors appreciate the anonymous reviewers for their helpful and
   constructive comments on an earlier draft of this paper. This work is
   supported by the National Natural Science Foundation of China (Grant No.
   62266006) , Guangxi Natural Science Foundation (Grant No.
   2020GXNSFDA297006) , National Natural Science Foundation of China (Grant
   No. 61866002) , Project of the Key Laboratory of AI and Information
   Processing (Hechi University) , Education Department of Guangxi Zhuang
   Autonomous Region (Grant No. 2022GXZDSY013) , Guangxi Natural Science
   Foundation (Grant No. 2018GXNSFAA138122, Grant No. 2015GXNSFAA139293) .
CR Akbarinia A, 2018, INT J COMPUT VISION, V126, P1367, DOI 10.1007/s11263-017-1035-5
   Angelucci A, 2006, PROG BRAIN RES, V154, P93, DOI 10.1016/S0079-6123(06)54005-1
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Bertasius G, 2015, PROC CVPR IEEE, P4380, DOI 10.1109/CVPR.2015.7299067
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Cao YJ, 2021, IEEE T MULTIMEDIA, V23, P761, DOI 10.1109/TMM.2020.2987685
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen XW, 2021, IMAGE VISION COMPUT, V110, DOI 10.1016/j.imavis.2021.104166
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Conway BR, 2010, J NEUROSCI, V30, P14955, DOI 10.1523/JNEUROSCI.4348-10.2010
   Dacey DM, 1996, P NATL ACAD SCI USA, V93, P582, DOI 10.1073/pnas.93.2.582
   Deng RX, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4435, DOI 10.1145/3474085.3475593
   Deng RX, 2018, LECT NOTES COMPUT SC, V11210, P570, DOI 10.1007/978-3-030-01231-1_35
   DISTLER C, 1993, J COMP NEUROL, V334, P125, DOI 10.1002/cne.903340111
   Duda R.O., 2000, Pattern Classification and Scene Analysis
   Fang T, 2021, MULTIMED TOOLS APPL, V80, P1611, DOI 10.1007/s11042-020-09800-x
   Fukushima K., 1982, Competition and cooperation in neural nets, P267, DOI DOI 10.1007/978-3-642-46466-9_18
   Gai Zhenbiao, 2023, Image and Vision Computing, DOI 10.1016/j.imavis.2023.104670
   Gao LL, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P594
   Grigorescu C, 2003, IEEE T IMAGE PROCESS, V12, P729, DOI 10.1109/TIP.2003.814250
   Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23
   Hallman S, 2015, PROC CVPR IEEE, P1732, DOI 10.1109/CVPR.2015.7298782
   He JZ, 2019, PROC CVPR IEEE, P3823, DOI 10.1109/CVPR.2019.00395
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   Jones HE, 2001, J NEUROPHYSIOL, V86, P2011, DOI 10.1152/jn.2001.86.4.2011
   Krüger N, 2013, IEEE T PATTERN ANAL, V35, P1847, DOI 10.1109/TPAMI.2012.272
   Liu Y, 2019, IEEE T PATTERN ANAL, V41, P1939, DOI 10.1109/TPAMI.2018.2878849
   Maninis KK, 2018, IEEE T PATTERN ANAL, V40, P819, DOI 10.1109/TPAMI.2017.2700300
   Markov NT, 2014, J COMP NEUROL, V522, P225, DOI 10.1002/cne.23458
   Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918
   Mély DA, 2016, VISION RES, V120, P93, DOI 10.1016/j.visres.2015.11.007
   Mottaghi R, 2014, PROC CVPR IEEE, P891, DOI 10.1109/CVPR.2014.119
   Osman I, 2021, IMAGE VISION COMPUT, V113, DOI 10.1016/j.imavis.2021.104248
   Prewitt J.M., 1970, Pict. Process. Psychopictorics, V10, P15
   Pu MY, 2022, PROC CVPR IEEE, P1392, DOI 10.1109/CVPR52688.2022.00146
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Schluppeck Denis, 2002, J Vis, V2, P480, DOI 10.1167/2.6.5
   Shen W, 2015, PROC CVPR IEEE, P3982, DOI 10.1109/CVPR.2015.7299024
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Solomon SG, 2007, NAT REV NEUROSCI, V8, P276, DOI 10.1038/nrn2094
   Soria X, 2020, IEEE WINT CONF APPL, P1912, DOI 10.1109/WACV45572.2020.9093290
   Tang QL, 2020, IEEE T IMAGE PROCESS, V29, P1192, DOI 10.1109/TIP.2019.2940690
   Ungerleider Leslie G., 1994, Current Opinion in Neurobiology, V4, P157, DOI 10.1016/0959-4388(94)90066-3
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   WIESEL TN, 1966, J NEUROPHYSIOL, V29, P1115, DOI 10.1152/jn.1966.29.6.1115
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1007/s11263-017-1004-z, 10.1109/ICCV.2015.164]
   Xuan WJ, 2022, NEURAL NETWORKS, V145, P248, DOI 10.1016/j.neunet.2021.10.022
   Yang KF, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2425538
   Zhang Q, 2021, PATTERN RECOGN, V110, DOI 10.1016/j.patcog.2020.107657
   Zhang X, 2023, NEURAL PROCESS LETT, V55, P4889, DOI 10.1007/s11063-022-11070-7
NR 52
TC 0
Z9 0
U1 1
U2 1
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUL
PY 2024
VL 147
AR 105078
DI 10.1016/j.imavis.2024.105078
EA MAY 2024
PG 13
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA UJ8C4
UT WOS:001247771700001
DA 2024-08-05
ER

PT J
AU Özbilgin, F
   Kurnaz,Ç
   Aydin, E
AF Ozbilgin, Ferdi
   Kurnaz, Cetin
   Aydin, Ertan
TI Non-invasive coronary artery disease identification through the iris and
   bio-demographic health profile features using stacking learning
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Coronary artery disease iris, image processing; Bio-demographic data;
   Stacking machine learning
ID CARDIOVASCULAR RISK; HEART-DISEASE; DIAGNOSIS; RECOGNITION; MARKERS;
   STRESS; CLAHE
AB This study proposes a non-invasive method for predicting Coronary Artery Disease (CAD) using iris analysis, patient data, and Machine Learning (ML), primarily with iris images. It involved 281 participants, comprising 155 CAD patients and 126 non -patient controls, with eye images and biodemographic data collected at a Cardiology outpatient clinic. The study explored three scenarios: Scenario -I focused on biodemographic data, Scenario -II on iris features, and Scenario -III combined iris images and data. Iris processing included location determination, normalization, and heart region selection, with image enhancement via adaptive histogram equalization. Feature extraction through a 2 -level wavelet transform generated 272 attributes, including statistical, Gray Level Co -occurrence Matrix, and Gray Level Run Length Matrix features for eight subcomponents. Correlation -based selection identified the best features, and classification employed ML techniques and incorporated stacking learning to enhance the results. Scenario -I achieved the highest accuracy at 83.57% among all evaluated algorithms. In Scenario -II, the proposed algorithm consistently outperformed others, achieving 94.88% accuracy and strong performance in other metrics, highlighting its effectiveness. In Scenario -III, the algorithm maintained superiority with 96.07% accuracy, specificity, recall, and area under the curve values. The proposed algorithm consistently outperforms other methods across scenarios, indicating its potential for CAD diagnosis, making it a promising choice for future CAD systems. The proposed algorithm presents a novel approach to the preliminary diagnosis of CAD, eliminating the necessity for electrocardiography, echocardiography, or effort tests. It also enables seamless integration into telemedicine systems, allowing for tele -diagnosis to conduct preliminary assessments before routine clinical practice.
C1 [Ozbilgin, Ferdi] Giresun Univ, Dept Elect & Elect Engn, Giresun, Turkiye.
   [Kurnaz, Cetin] Ondokuz Mayis Univ, Dept Elect & Elect Engn, Samsun, Turkiye.
   [Aydin, Ertan] Giresun Univ, Fac Med, Dept Cardiol, Giresun, Turkiye.
C3 Giresun University; Ondokuz Mayis University; Giresun University
RP Özbilgin, F (corresponding author), Giresun Univ, Dept Elect & Elect Engn, Giresun, Turkiye.
EM ferdi.ozbilgin@giresun.edu.tr; ckurnaz@omu.edu.tr
RI Kurnaz, Cetin/S-3469-2016
OI Kurnaz, Cetin/0000-0003-3436-899X
CR Adlung L, 2021, MED-CAMBRIDGE, V2, P642, DOI 10.1016/j.medj.2021.04.006
   Aksoy S, 2001, PATTERN RECOGN LETT, V22, P563, DOI 10.1016/S0167-8655(00)00112-4
   Alizadehsani R, 2018, COMPUT METH PROG BIO, V162, P119, DOI 10.1016/j.cmpb.2018.05.009
   Alizadehsani R, 2013, COMPUT METH PROG BIO, V111, P52, DOI 10.1016/j.cmpb.2013.03.004
   Alzubaidi L, 2023, J BIG DATA-GER, V10, DOI 10.1186/s40537-023-00727-2
   Anand A., 2018, 2018 IEEE INT S SIGN
   Bachmann JM, 2012, CIRCULATION, V125, P3092, DOI 10.1161/CIRCULATIONAHA.111.065490
   Bansal A, 2015, INT J DIABETES DEV C, V35, P432, DOI 10.1007/s13410-015-0296-1
   Benesty J, 2009, SPRINGER TOP SIGN PR, V2, P37, DOI 10.1007/978-3-642-00296-0_5
   Benjamin, 2020, CIRCULATION, V141, pE33, DOI 10.1161/CIR.0000000000000746
   Benjamin EJ, 2019, CIRCULATION, V139, pE56, DOI [10.1161/CIR.0000000000000659, 10.1161/CIR.0000000000000746]
   Berrar D., 2018, Ency. Bioinform. Comput. Biol.: ABC Bioinform, P403, DOI DOI 10.1016/B978-0-12-809633-8.20473-1
   Brown J C., 2020, Risk factors for coronary artery disease
   Cai H, 2019, SIGNAL PROCESS-IMAGE, V71, P88, DOI 10.1016/j.image.2018.11.003
   Canbek G, 2021, NEURAL COMPUT APPL, V33, P14623, DOI 10.1007/s00521-021-06103-6
   Chand R., 2019, Essential Radiology Review, P263
   Chandola T, 2008, EUR HEART J, V29, P640, DOI 10.1093/eurheartj/ehm584
   Chowdhury M, 2020, CURR TREAT OPT CARD, V22, DOI 10.1007/s11936-020-0803-7
   Clausi DA, 2002, CAN J REMOTE SENS, V28, P45, DOI 10.5589/m02-004
   DAUGMAN JG, 1993, IEEE T PATTERN ANAL, V15, P1148, DOI 10.1109/34.244676
   Daugman J, 2009, ESSENTIAL GUIDE TO IMAGE PROCESSING, 2ND EDITION, P715, DOI 10.1016/B978-0-12-374457-9.00025-1
   Dhar S, 2021, ACM T INTERNET THING, V2, DOI 10.1145/3450494
   Diwakar M, 2021, MATER TODAY-PROC, V37, P3411, DOI 10.1016/j.matpr.2020.09.278
   Esteves RB, 2021, EUR J INTEGR MED, V43, DOI 10.1016/j.eujim.2021.101311
   Fausett L., 1994, Fundamentals of neural networks: architectures, algorithms, and applications
   Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010
   Fu QQ, 2019, CLUSTER COMPUT, V22, P12609, DOI 10.1007/s10586-017-1692-8
   FUSTER V, 1990, CIRCULATION, V82, P47
   Gorgel E.B., 2007, Menopoz doneminde kadin
   Gunawan VA, 2022, INT J ADV COMPUT SC, V13, P639
   Gupta A, 2022, APPL INTELL, V52, P2436, DOI 10.1007/s10489-021-02467-3
   Gupta A, 2021, 35TH INTERNATIONAL CONFERENCE ON INFORMATION NETWORKING (ICOIN 2021), P818, DOI 10.1109/ICOIN50884.2021.9333884
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   Hasegawa K, 2019, EUR CARDIOL REV, V14, P60, DOI 10.15420/ecr.2019.4.2
   Joloudari JH, 2022, FRONT CARDIOVASC MED, V8, DOI 10.3389/fcvm.2021.760178
   Hess A.S., 2019, Pain: A Review Guide, P135
   Hinchliffe Robert J, 2020, Diabetes Metab Res Rev, V36 Suppl 1, pe3276, DOI 10.1002/dmrr.3276
   Ikonomidis I, 2008, ATHEROSCLEROSIS, V199, P3, DOI 10.1016/j.atherosclerosis.2008.02.019
   Imran, 2021, SUSTAINABILITY-BASEL, V13, DOI 10.3390/su131810057
   Jaarsma C, 2013, CURR CARDIOVASC IMAG, V6, P117, DOI 10.1007/s12410-012-9185-x
   Jensen B., 2012, Iridology Simplified
   Kannel WB, 2004, AM HEART J, V148, P16, DOI 10.1016/j.ahj.2003.10.022
   Keil U., 2000, Basic Research in Cardiology, V95, DOI 10.1007/s003950070010
   Kiliç Ü, 2018, 2018 INNOVATIONS IN INTELLIGENT SYSTEMS AND APPLICATIONS CONFERENCE (ASYU), P60
   Kimura A, 2009, CIRC J, V73, P1016, DOI 10.1253/circj.CJ-09-0263
   Kullo IJ, 2010, NAT REV CARDIOL, V7, P309, DOI 10.1038/nrcardio.2010.53
   Kumar S, 2021, MULTIMED TOOLS APPL, V80, P15487, DOI 10.1007/s11042-020-10322-9
   Li DL, 2021, AM J MED, V134, P968, DOI 10.1016/j.amjmed.2021.03.011
   Li Q, 2019, PATTERN RECOGN LETT, V128, P107, DOI 10.1016/j.patrec.2019.08.024
   Liao YH, 2002, COMPUT SECUR, V21, P439, DOI 10.1016/S0167-4048(02)00514-X
   Liao YW, 2021, IEEE IJCNN, DOI 10.1109/IJCNN52387.2021.9533531
   Malakar AK, 2019, J CELL PHYSIOL, V234, P16812, DOI 10.1002/jcp.28350
   Malekian A., 2021, Advances in streamflow forecasting, P115, DOI DOI 10.1016/B978-0-12-820673-7.00003-2
   Mienye ID, 2022, IEEE ACCESS, V10, P99129, DOI 10.1109/ACCESS.2022.3207287
   Mozaffarian, 2015, CIRCULATION, V131, pE535, DOI 10.1161/CIR.0000000000000219
   Ndumele CE, 2016, J AM HEART ASSOC, V5, DOI 10.1161/JAHA.116.003921
   Nguyen DK, 2021, INT J ENV RES PUB HE, V18, DOI 10.3390/ijerph182010811
   Nordhausen K., 2013, Ensemble Methods: Foundations and Algorithms by Zhi-Hua Zhou
   Nordlund D, 2016, CIRC-CARDIOVASC IMAG, V9, DOI 10.1161/CIRCIMAGING.115.004376
   Nowbar AN, 2019, CIRC-CARDIOVASC QUAL, V12, DOI 10.1161/CIRCOUTCOMES.118.005375
   Ozbilgin F., 2023, Karadeniz Fen Bilimleri Dergisi, V13, P665
   Özbilgin F, 2023, DIAGNOSTICS, V13, DOI 10.3390/diagnostics13061081
   Persic V, 2018, MED HYPOTHESES, V115, P72, DOI 10.1016/j.mehy.2018.04.001
   Poonguzhali N, 2017, ADV CIV IND ENG BOOK, P118, DOI 10.4018/978-1-5225-2423-6.ch005
   Pyxaras SA, 2018, J NUCL CARDIOL, V25, P860, DOI 10.1007/s12350-017-1050-5
   Rácz A, 2019, MOLECULES, V24, DOI 10.3390/molecules24152811
   Raudys S., 2001, Statistical and Neural Classifiers: An Integrated Approach to Design
   Rehm J, 2003, J CARDIOVASC RISK, V10, P15, DOI 10.1097/00043798-200302000-00004
   Reza AM, 2004, J VLSI SIG PROC SYST, V38, P35, DOI 10.1023/B:VLSI.0000028532.53893.82
   Rhee EJ, 2020, ENDOCRINOL METAB, V35, P85, DOI 10.3803/EnM.2020.35.1.85
   Rish I, 2001, Proceedings of the IJCAI Workshop on Empirical Methods in Arti cial Intelligence, P41
   Rizwan A, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11104657
   Sabic E, 2021, AI SOC, V36, P149, DOI 10.1007/s00146-020-00985-1
   Samant P, 2018, COMPUT METH PROG BIO, V157, P121, DOI 10.1016/j.cmpb.2018.01.004
   Sanchis-Gomar F, 2016, ANN TRANSL MED, V4, DOI 10.21037/atm.2016.06.33
   Semmlow J, 2007, ANNU REV BIOMED ENG, V9, P449, DOI 10.1146/annurev.bioeng.9.060906.151840
   Shang Nong., 1996, Ionosphere, V2, P351
   Shaw LJ, 2017, CIRCULATION, V135, DOI 10.1161/CIRCULATIONAHA.117.028637
   Silva AC, 2020, INT J CARDIOL, V298, P32, DOI 10.1016/j.ijcard.2019.07.066
   Soh LK, 1999, IEEE T GEOSCI REMOTE, V37, P780, DOI 10.1109/36.752194
   Sridhar C, 2021, J AMB INTEL HUM COMP, V12, P3227, DOI 10.1007/s12652-020-02536-4
   Tao XH, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21030776
   Tayebati S, 2023, Arxiv, DOI arXiv:2307.01872
   Toth P.P., 2021, Therapeut. Lipidol., P11, DOI [10.1007/978-3-030-56514-5_2, DOI 10.1007/978-3-030-56514-5_2]
   Upasani N., 2023, J. Theor. Appl. Inf. Technol., V101
   Willemsen FJ, 2021, PROCEEDINGS OF PERFORMANCE MODELING, BENCHMARKING AND SIMULATION OF HIGH PERFORMANCE COMPUTER SYSTEMS (PMBS 2021), P106, DOI 10.1109/PMBS54543.2021.00017
   Yusuf S, 2004, LANCET, V364, P937, DOI 10.1016/S0140-6736(04)17018-9
   Zeng J, 2020, IEEE ACCESS, V8, P136141, DOI 10.1109/ACCESS.2020.3011331
   Zhang JX, 2024, Arxiv, DOI arXiv:2101.07077
   Zhou Z.H., 2012, Ensemble methods: foundations and algorithms
   Zimarino M, 2018, J CARDIOVASC MED, V19, pE133, DOI 10.2459/JCM.0000000000000591
NR 91
TC 0
Z9 0
U1 1
U2 1
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUN
PY 2024
VL 146
AR 105046
DI 10.1016/j.imavis.2024.105046
EA APR 2024
PG 15
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA QM5Z7
UT WOS:001221316800001
DA 2024-08-05
ER

PT J
AU Mokari, M
   Sadeghi, KH
AF Mokari, Mozhgan
   Sadeghi, Khosrow Haj
TI Enhancing temporal action localization in an end-to-end network through
   estimation error incorporation
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Temporal action localization; Activity; Classification; Activity
   proposal; Action recognition
AB Temporal action localization presents a significant challenge in computer vision, as the development of an efficient method for this task remains elusive. The objective is to identify human activities within untrimmed videos, determining when and which actions occur in each video. While using trimmed videos could potentially resolve the localization problem and enhance classification accuracy, it is impractical for real-world applications as the trimming process itself requires human intervention. This highlights the importance of temporal localization. Due to the availability of several successful approaches for action recognition in trimmed video, conventional multi-stage methods for untrimmed video, commonly employ a network to generate activity proposals, followed by a separate network for classification. These disjoint networks are optimized individually and thus usually vary from the global optimum, leading to less precise candidate action proposals. To address this challenge, we propose a novel end-to-end neural network that utilizes error estimation for precise action localization and recognition in untrimmed videos. The proposed method performs the localization and classification of action instances simultaneously, thereby optimizing the corresponding networks concurrently. To increase the precision of the action proposal boundaries, the Regression module is innovatively utilized as part of the proposed end-toend network, along with the Evaluation and Classification modules. This module estimates the potential error in proposal time boundaries and enhances the result accuracy. We have conducted experiments on THUMOS 14 and ActivityNet-1.3, which are considered the most challenging datasets for temporal action localization. The novel, yet fairly simple, proposed network achieves remarkable performance improvement compared to the other stateof-the-art methods. This improvement, which is more pronounced in the cases of high temporal intersection with ground truth, is accomplished without requiring extra data or complicated architecture. By incorporating error estimation, we achieved improvement in mean Average Precision (mAP). The proposed approach particularly shines for the localization of challenging activities in the complex and diverse dataset ActivityNet-1.3. For instance, for the "drinking coffee" activity, the mean Average Precision (mAP) was enhanced fivefold compared to the best-reported results.
C1 [Mokari, Mozhgan; Sadeghi, Khosrow Haj] Sharif Univ Technol, Dept Elect Engn, Tehran, Iran.
C3 Sharif University of Technology
RP Sadeghi, KH (corresponding author), Sharif Univ Technol, Dept Elect Engn, Tehran, Iran.
EM mozhgan.mokari@ee.sharif.edu; ksadeghi@sharif.edu
CR Aggarwal A.K., 2021, Autonomous Driving and Advanced Driver-Assistance Systems (ADAS), P420
   Alwassel H, 2021, IEEE INT CONF COMP V, P3166, DOI 10.1109/ICCVW54120.2021.00356
   Alwassel H, 2018, LECT NOTES COMPUT SC, V11207, P264, DOI 10.1007/978-3-030-01219-9_16
   [Anonymous], 2021, The AI Index Report, Measuring Trends in Artificial Intelligence
   Bodla N, 2017, IEEE I CONF COMP VIS, P5562, DOI 10.1109/ICCV.2017.593
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Chao YW, 2018, PROC CVPR IEEE, P1130, DOI 10.1109/CVPR.2018.00124
   Chen YS, 2021, IMAGE VISION COMPUT, V109, DOI 10.1016/j.imavis.2021.104144
   Gao JY, 2018, LECT NOTES COMPUT SC, V11206, P70, DOI 10.1007/978-3-030-01216-8_5
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Jiang Y.-G., 2014, THUMOS challenge: Action recognition with a large number of classes
   Vo K, 2023, INT J COMPUT VISION, V131, P302, DOI 10.1007/s11263-022-01702-9
   Kingma D. P., 2014, arXiv
   Kong WJ, 2019, INT CONF ACOUST SPEE, P1647, DOI 10.1109/ICASSP.2019.8682466
   Kumar A., 2014, Seisan Kenkyu, V66, P101
   Kumar Ashwani, 2014, Rapport technique
   Lin TW, 2019, IEEE I CONF COMP VIS, P3888, DOI 10.1109/ICCV.2019.00399
   Lin TW, 2018, LECT NOTES COMPUT SC, V11208, P3, DOI 10.1007/978-3-030-01225-0_1
   Lin TW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P988, DOI 10.1145/3123266.3123343
   Liu QY, 2020, AAAI CONF ARTIF INTE, V34, P11612
   Liu XL, 2022, PROC CVPR IEEE, P19978, DOI 10.1109/CVPR52688.2022.01938
   Liu Y, 2019, PROC CVPR IEEE, P3599, DOI [10.1109/CVPR.2019.00372, 10.1109/CVPR.2019.00726]
   Long FC, 2019, PROC CVPR IEEE, P344, DOI 10.1109/CVPR.2019.00043
   Qin X, 2022, NEUROCOMPUTING, V510, P48, DOI 10.1016/j.neucom.2022.08.040
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shang JH, 2023, IMAGE VISION COMPUT, V129, DOI 10.1016/j.imavis.2022.104589
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Tang YP, 2023, INFORM PROCESS MANAG, V60, DOI 10.1016/j.ipm.2022.103141
   Vaudaux-Ruth G, 2021, IEEE WINT CONF APPL, P1268, DOI 10.1109/WACV48630.2021.00131
   Wang L, 2021, NEUROCOMPUTING, V434, P211, DOI 10.1016/j.neucom.2020.12.126
   Wang LM, 2019, IEEE T PATTERN ANAL, V41, P2740, DOI 10.1109/TPAMI.2018.2868668
   Wang LN, 2023, NEUROCOMPUTING, V538, DOI 10.1016/j.neucom.2023.01.045
   Xia K, 2022, PATTERN RECOGN, V129, DOI 10.1016/j.patcog.2022.108725
   Xiang Wang, 2020, Pattern Recognition and Computer Vision. Third Chinese Conference, PRCV 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12306), P41, DOI 10.1007/978-3-030-60639-8_4
   Xu HJ, 2017, IEEE I CONF COMP VIS, P5794, DOI 10.1109/ICCV.2017.617
   Yang L, 2020, IEEE T IMAGE PROCESS, V29, P8535, DOI 10.1109/TIP.2020.3016486
   Zhang D., 2021, The AI index 2021 annual report, P58
   Zhang W, 2021, NEUROCOMPUTING, V444, P16, DOI 10.1016/j.neucom.2021.02.085
NR 38
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD MAY
PY 2024
VL 145
AR 104994
DI 10.1016/j.imavis.2024.104994
EA APR 2024
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA QW7A3
UT WOS:001223959900001
DA 2024-08-05
ER

PT J
AU Lu, YY
   Zhu, YH
   Feng, H
   Liu, Y
AF Lu, Yuanyuan
   Zhu, Yanhui
   Feng, Hao
   Liu, Yang
TI Remote sensing scene classification using multi-domain sematic
   high-order network
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Remote sensing; Scene classification; Convolutional neural networks;
   Deep semantic feature; Second-order
ID FUSION; ATTENTION; IMAGES
AB Recently, convolutional neural networks (CNNs), which obtain powerful deep features in an end-to-end manner, have achieved powerful performance in remote sensing scene classification. However, the average or maximum pooling operations defined in the spatial domain and coarser-resolution features with high levels cannot extract reliable features and clear boundaries for small-scale targets in remote sensing scene imagery. This paper attempts to address these problems and proposes a multi-domain sematic high-order network for scene classification, named MSHNet. First, wavelet-spatial and detachable pooling blocks defined in the wavelet and spatial domains are inserted at the end of the convolutional block to learn the features in a more structural fusion manner. Second, multi-scale and multi-resolution semantic embedding modules are proposed to take full advantage of the complementary information and effectively maintain the spatial structures of learned deep features. Third, we employ a factorized bilinear coding approach to obtain compact and discriminative secondorder features. MSHNet is thoroughly evaluated on two publicly available benchmarks, i.e., AID (Aerial Image Dataset) and NWPU-RESISC45 (Northwestern Polytechnical University-Remote Sensing Image Scene Classification 45). The extensive results illustrate that our MSHNet is competitive with other related multi-scale deep neural networks.
C1 [Lu, Yuanyuan; Feng, Hao; Liu, Yang] Wuhan Coll, Sch Informat Engn, Wuhan 430212, Peoples R China.
   [Lu, Yuanyuan] Cent China Normal Univ, Fac Artificial Intelligence Educ, Wuhan 430079, Peoples R China.
   [Zhu, Yanhui] Hunan Geol Explorat Inst China Met Geol Bur, Changsha 410001, Peoples R China.
C3 Wuhan College; Central China Normal University
RP Lu, YY (corresponding author), Wuhan Coll, Sch Informat Engn, Wuhan 430212, Peoples R China.
EM yuanyuan.lu@mails.ccnu.edu.cn
RI Zhu, Yanhui/GVU-5794-2022
FU National Natural Science Foundation of China [6207020477]; The 2023
   Wuhan College Research Fund Program Class A Project [JJA202303]; Hubei
   Provincial Education Science Planning Key Project [2022GA108]
FX This research was supported by the National Natural Science Foundation
   of China (No. 6207020477) , 2023 Wuhan College Research Fund Program
   Class A Project (No. JJA202303) , and Hubei Provincial Education Science
   Planning Key Project (No. 2022GA108) .
CR Cao R, 2021, IEEE GEOSCI REMOTE S, V18, P43, DOI 10.1109/LGRS.2020.2968550
   Chen JS, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3255211
   Chen XM, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2022.3150801
   Cheng G, 2017, P IEEE, V105, P1865, DOI 10.1109/JPROC.2017.2675998
   Cheng G, 2013, INT J REMOTE SENS, V34, P45, DOI 10.1080/01431161.2012.705443
   Dai YM, 2021, IEEE WINT CONF APPL, P3559, DOI 10.1109/WACV48630.2021.00360
   Deng PF, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2020.3016769
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   Gao Z, 2020, AAAI CONF ARTIF INTE, V34, P3954
   Gosztolya G, 2022, EXPERT SYST APPL, V205, DOI 10.1016/j.eswa.2022.117613
   He KM, 2015, IEEE T PATTERN ANAL, V37, P1904, DOI 10.1109/TPAMI.2015.2389824
   He NJ, 2018, IEEE T GEOSCI REMOTE, V56, P6899, DOI 10.1109/TGRS.2018.2845668
   Hou YE, 2023, IEEE GEOSCI REMOTE S, V20, DOI 10.1109/LGRS.2023.3304645
   Huang XY, 2023, REMOTE SENS-BASEL, V15, DOI 10.3390/rs15143645
   Huang YF, 2022, IEEE IMAGE PROC, P3311, DOI 10.1109/ICIP46576.2022.9897710
   Huang ZL, 2023, IEEE T PATTERN ANAL, V45, P6896, DOI 10.1109/TPAMI.2020.3007032
   Karim S, 2019, MULTIMED TOOLS APPL, V78, P32565, DOI 10.1007/s11042-019-08033-x
   Khan SD, 2023, REMOTE SENS-BASEL, V15, DOI 10.3390/rs15133408
   Laghari AA, 2023, SCI REP-UK, V13, DOI 10.1038/s41598-023-40343-x
   Li EZ, 2017, IEEE T GEOSCI REMOTE, V55, P5653, DOI 10.1109/TGRS.2017.2711275
   Li F., 2022, IEEE Geosci. Remote Sens. Lett, V20, P1
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Li XH, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2021.3098774
   Li ZX, 2022, IEEE IMAGE PROC, P1971, DOI 10.1109/ICIP46576.2022.9897888
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Lu XQ, 2019, IEEE T GEOSCI REMOTE, V57, P7894, DOI 10.1109/TGRS.2019.2917161
   Luo B, 2013, IEEE J-STARS, V6, P1899, DOI 10.1109/JSTARS.2012.2228254
   Miao W, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3244565
   NASRABADI NM, 1988, IEEE T COMMUN, V36, P957, DOI 10.1109/26.3776
   Ni K, 2020, INT J REMOTE SENS, V41, P1415, DOI 10.1080/01431161.2019.1667551
   Niu B, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3217180
   Ren JF, 2015, PATTERN RECOGN, V48, P3180, DOI 10.1016/j.patcog.2015.02.001
   Shi CP, 2020, IEEE J-STARS, V13, P5194, DOI 10.1109/JSTARS.2020.3018307
   Silva FB, 2018, PATTERN RECOGN, V74, P266, DOI 10.1016/j.patcog.2017.09.018
   Sun H, 2020, IEEE T GEOSCI REMOTE, V58, P82, DOI 10.1109/TGRS.2019.2931801
   Tang X, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3194505
   Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018
   Wang LG, 2024, GEOSCI DATA J, V11, P237, DOI 10.1002/gdj3.162
   Wang WH, 2022, COMPUT VIS MEDIA, V8, P415, DOI 10.1007/s41095-022-0274-8
   Wang X, 2021, IEEE T GEOSCI REMOTE, V59, P7918, DOI 10.1109/TGRS.2020.3044655
   Xia GS, 2017, IEEE T GEOSCI REMOTE, V55, P3965, DOI 10.1109/TGRS.2017.2685945
   Xu CJ, 2023, REMOTE SENS-BASEL, V15, DOI 10.3390/rs15040914
   Xu JY, 2022, IEEE SIGNAL PROC LET, V29, P1202, DOI 10.1109/LSP.2022.3175096
   Xu QS, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3235988
   Yang Y, 2008, IEEE IMAGE PROC, P1852, DOI 10.1109/ICIP.2008.4712139
   Yu DH, 2020, IEEE J-STARS, V13, P6372, DOI 10.1109/JSTARS.2020.3030257
   Yuan JW, 2023, REMOTE SENS-BASEL, V15, DOI 10.3390/rs15030810
   Yuan TH, 2023, IEEE GEOSCI REMOTE S, V20, DOI 10.1109/LGRS.2023.3282310
   Zhang BQ, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3200056
   Zhang GK, 2021, IEEE J-STARS, V14, P9530, DOI 10.1109/JSTARS.2021.3109661
   Zhang W, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11050494
   Zhang YP, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3235881
   Zhao K, 2023, REMOTE SENS-BASEL, V15, DOI 10.3390/rs15061546
   Zhao MF, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3265346
   Zhu S, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3056624
NR 55
TC 0
Z9 0
U1 9
U2 9
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD MAR
PY 2024
VL 143
AR 104948
DI 10.1016/j.imavis.2024.104948
EA FEB 2024
PG 11
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA LQ0X8
UT WOS:001188161700001
DA 2024-08-05
ER

PT J
AU Asheghi, B
   Salehpour, P
   Khiavi, AM
   Hashemzadeh, M
   Monajemi, A
AF Asheghi, Bahareh
   Salehpour, Pedram
   Khiavi, Abdolhamid Moallemi
   Hashemzadeh, Mahdi
   Monajemi, Amirhassan
TI DASOD: Detail-aware salient object detection
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Conditional variational auto-encoder; Uncertainty quantification;
   Refinement network; Saliency detection; Salient object detection; SOD
ID UNCERTAINTY QUANTIFICATION; NETWORK
AB Salient object detection (SOD) is a challenging task in computer vision. Current SOD approaches have made significant progress, but they fail in challenging scenarios. This paper categorizes the existing challenges in SOD into four groups: images with complex backgrounds, low contrast, transparent objects, and occluded objects. Then, the Detail-Aware Salient Object Detection (DASOD) method is proposed to address these challenging scenarios. To the best of our knowledge, DASOD is the first method that considers mentioned challenging situations together and detects salient objects in images through camouflaged object detection (COD). DASOD has two main stages: 1) pseudo-mask generation and 2) refinement. It first generates a pseudo-mask using the body label and super-resolution technique, then refines the pseudo-mask with the detail map produced by the pseudoedge generator to detect salient objects with clear boundaries in the pseudo-mask refinement module. This module quantifies uncertainty using the conditional normalizing flows (cFlow) based conditional variational auto-encoder (cVAE) to generate reliable results. Extensive experiments are conducted on six datasets, and the performance of DASOD is compared with 18 state-of-the-art methods. The results demonstrate that DASOD outperforms its competitors and can accurately detect the salient objects when the image background is cluttered and the contrast between foreground and background is low. Also, it effectively detects the transparent and occluded objects in images. It achieves MAE rates of 0.052, 0.033, 0.027, 0.024, 0.059, and 0.088 on DUTOMRON, DUTS-TE, ECSSD, HKU-IS, PASCAL-S, and SCAS datasets, respectively. All the implementation source codes and results are available at: https://github.com/BaharehAsheghi/DASOD.
C1 [Asheghi, Bahareh; Salehpour, Pedram; Khiavi, Abdolhamid Moallemi] Univ Tabriz, Fac Elect & Comp Engn, Dept Comp Engn, Tabriz, Iran.
   [Hashemzadeh, Mahdi] Azarbaijan Shahid Madani Univ, Fac Informat Technol & Comp Engn, Tabriz, Iran.
   [Hashemzadeh, Mahdi] Azarbaijan Shahid Madani Univ, Artificial Intelligence & Machine Learning Res Lab, Tabriz, Iran.
   [Monajemi, Amirhassan] Natl Univ Singapore, Sch Comp, Dept Comp Sci, Singapore, Singapore.
C3 University of Tabriz; Azarbaijan Shahid Madani University; Azarbaijan
   Shahid Madani University; National University of Singapore
RP Asheghi, B; Salehpour, P (corresponding author), Univ Tabriz, BistNoh Bahman Bulvar, Tabriz 5166616471, Iran.
EM bahareh.asheghi@tabrizu.ac.ir; psalehpoor@tabrizu.ac.ir;
   moallemi@tabrizu.ac.ir; hashemzadeh@azaruniv.ac.ir; amir@comp.nus.edu.sg
CR Abdar M, 2021, INFORM FUSION, V76, P243, DOI 10.1016/j.inffus.2021.05.008
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Asheghi B, 2022, SIGNAL PROCESS, V195, DOI 10.1016/j.sigpro.2022.108496
   Chen SH, 2018, LECT NOTES COMPUT SC, V11213, P236, DOI 10.1007/978-3-030-01240-3_15
   Chen ZY, 2020, AAAI CONF ARTIF INTE, V34, P10599
   Cheng MM, 2021, INT J COMPUT VISION, V129, P2622, DOI [10.1007/s11263-021-01490-8, 10.1109/ICCV.2017.487]
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Craye C, 2016, IEEE INT CONF ROBOT, P2303, DOI 10.1109/ICRA.2016.7487379
   Fan DP, 2022, IEEE T PATTERN ANAL, V44, P6024, DOI 10.1109/TPAMI.2021.3085766
   Fan DP, 2020, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR42600.2020.00285
   Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698
   Farajzadeh N, 2023, EXPERT SYST APPL, V224, DOI 10.1016/j.eswa.2023.119963
   Farajzadeh N, 2023, MED ENG PHYS, V113, DOI 10.1016/j.medengphy.2023.103957
   Farajzadeh N, 2022, ENTERTAIN COMPUT, V43, DOI 10.1016/j.entcom.2022.100518
   Feng MY, 2019, PROC CVPR IEEE, P1623, DOI 10.1109/CVPR.2019.00172
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   Gao Y, 2013, IEEE T IMAGE PROCESS, V22, P363, DOI 10.1109/TIP.2012.2202676
   Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699
   Gu ZW, 2019, IEEE T MED IMAGING, V38, P2281, DOI 10.1109/TMI.2019.2903562
   Hashemzadeh M, 2019, SIGNAL PROCESS, V155, P233, DOI 10.1016/j.sigpro.2018.09.037
   He CM, 2023, PROC CVPR IEEE, P22046, DOI 10.1109/CVPR52729.2023.02111
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hong S, 2015, PR MACH LEARN RES, V37, P597
   Hou QB, 2019, IEEE T PATTERN ANAL, V41, P815, DOI 10.1109/TPAMI.2018.2815688
   Hoyer L., 2019, ADV NEUR IN, V32
   Huang Z, 2023, PROC CVPR IEEE, P5557, DOI 10.1109/CVPR52729.2023.00538
   Hui SX, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3570507
   Ishida T., 2020, P 37 INT C MACH LEAR, P4604
   Jabbooree AI, 2023, IMAGE VISION COMPUT, V134, DOI 10.1016/j.imavis.2023.104677
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Jun Wei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13022, DOI 10.1109/CVPR42600.2020.01304
   Kajiura Nobukatsu, 2021, MMAsia '21: ACM Multimedia Asia, DOI 10.1145/3469877.3490587
   Ke YY, 2022, IEEE WINT CONF APPL, P1360, DOI 10.1109/WACV51458.2022.00143
   Kingma D. P., 2014, arXiv
   Kohl SAA, 2018, ADV NEUR IN, V31
   Kong YQ, 2021, PATTERN RECOGN, V114, DOI 10.1016/j.patcog.2021.107867
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li HY, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2440174
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Liu JJ, 2023, IEEE T PATTERN ANAL, V45, P887, DOI 10.1109/TPAMI.2021.3140168
   Liu JJ, 2021, IEEE T IMAGE PROCESS, V30, P9030, DOI 10.1109/TIP.2021.3122093
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Liu JJ, 2020, IEEE T IMAGE PROCESS, V29, P8652, DOI 10.1109/TIP.2020.3017352
   Liu J, 2022, DIGIT SIGNAL PROCESS, V126, DOI 10.1016/j.dsp.2022.103425
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80
   Liu RS, 2014, PROC CVPR IEEE, P3866, DOI 10.1109/CVPR.2014.494
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu Y, 2020, AAAI CONF ARTIF INTE, V34, P5005
   Luo ZM, 2017, PROC CVPR IEEE, P6593, DOI 10.1109/CVPR.2017.698
   Lv YQ, 2023, IEEE T CIRC SYST VID, V33, P3462, DOI 10.1109/TCSVT.2023.3234578
   Margolin R, 2014, PROC CVPR IEEE, P248, DOI 10.1109/CVPR.2014.39
   Mei HY, 2022, IEEE T CIRC SYST VID, V32, P1378, DOI 10.1109/TCSVT.2021.3069848
   Oskouei SH, 2023, KNOWL INF SYST, V65, P3753, DOI 10.1007/s10115-023-01897-4
   Özcelik YB, 2023, FRACTAL FRACT, V7, DOI 10.3390/fractalfract7080598
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Qiu Y, 2020, NEUROCOMPUTING, V388, P124, DOI 10.1016/j.neucom.2019.12.123
   Ren ZX, 2014, IEEE T CIRC SYST VID, V24, P769, DOI 10.1109/TCSVT.2013.2280096
   Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Selvan R, 2020, LECT NOTES COMPUT SC, V12436, P80, DOI 10.1007/978-3-030-59861-7_9
   Sezer A, 2021, SOLDER SURF MT TECH, V33, P291, DOI 10.1108/SSMT-04-2021-0013
   Shi JP, 2016, IEEE T PATTERN ANAL, V38, P717, DOI 10.1109/TPAMI.2015.2465960
   Siris A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4136, DOI 10.1109/ICCV48922.2021.00412
   Ullah I, 2021, MULTIMED TOOLS APPL, V80, P7145, DOI 10.1007/s11042-020-10111-4
   Ullah I, 2020, MULTIMED TOOLS APPL, V79, P34605, DOI 10.1007/s11042-020-08849-y
   Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404
   Wang WG, 2022, IEEE T PATTERN ANAL, V44, P3239, DOI 10.1109/TPAMI.2021.3051099
   Wang WG, 2019, PROC CVPR IEEE, P5961, DOI 10.1109/CVPR.2019.00612
   Wang WG, 2018, PROC CVPR IEEE, P1711, DOI 10.1109/CVPR.2018.00184
   Wei J, 2020, AAAI CONF ARTIF INTE, V34, P12321
   Wei YC, 2017, IEEE T PATTERN ANAL, V39, P2314, DOI 10.1109/TPAMI.2016.2636150
   Wu YH, 2022, IEEE T IMAGE PROCESS, V31, P3125, DOI 10.1109/TIP.2022.3164550
   Wu Z, 2021, IEEE T IMAGE PROCESS, V30, P6226, DOI 10.1109/TIP.2021.3093380
   Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1007/s11263-017-1004-z, 10.1109/ICCV.2015.164]
   Yag I, 2022, BIOLOGY-BASEL, V11, DOI 10.3390/biology11121732
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yang C, 2013, IEEE SIGNAL PROC LET, V20, P637, DOI 10.1109/LSP.2013.2260737
   Yao ZJ, 2023, EXPERT SYST APPL, V213, DOI 10.1016/j.eswa.2022.118973
   Youwei Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9410, DOI 10.1109/CVPR42600.2020.00943
   Zhang DW, 2019, INT J COMPUT VISION, V127, P363, DOI 10.1007/s11263-018-1112-4
   Zhang L, 2018, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2018.00187
   Zhang PP, 2017, IEEE I CONF COMP VIS, P202, DOI 10.1109/ICCV.2017.31
   Zhang PP, 2017, IEEE I CONF COMP VIS, P212, DOI 10.1109/ICCV.2017.32
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhao T, 2019, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR.2019.00320
   Zhao X., 2020, COMPUT VIS ECCV
   Zhou H., 2020, P IEEE CVF C COMP VI, P9138, DOI 10.1109/CVPR42600.2020.00916
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
   Zhuge M., 2022, IEEE TPAMI
NR 93
TC 0
Z9 0
U1 3
U2 3
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD AUG
PY 2024
VL 148
AR 105154
DI 10.1016/j.imavis.2024.105154
EA JUN 2024
PG 20
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA XK2P8
UT WOS:001261516700001
DA 2024-08-05
ER

PT J
AU Gao, HJ
   Feng, MF
AF Gao, Haijun
   Feng, Minfu
TI Blind deblurring text images via Beltrami regularization
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE 65 L12; 65 L20; 65 L70 Beltrami model; Blind deblurring; Text images; L0
ID SINGLE IMAGE
AB This article proposes a blind image deblurring model based on Beltrami regularization. The existence and uniqueness of the Beltrami model are proved, and we perform a theoretical analysis of the convergence and error estimation of the algorithm for solving the Beltrami regularization model. Meanwhile, we apply a half -quadratic splitting algorithm to solve it and obtain the blur kernel of the blurred image. Furthermore, we get a clean image through some non -blind deblurring algorithms. Finally, we compare this with the state -of -art methods, and our algorithm has specific performance improvements and advantages in the text images. (c) 2024 Elsevier Ltd. All rights reserved.
C1 [Gao, Haijun; Feng, Minfu] Sichuan Univ, Sch Math, 24 South Sect 1,Yihuan Rd, Chengdu 610065, Peoples R China.
C3 Sichuan University
RP Feng, MF (corresponding author), Sichuan Univ, Sch Math, 24 South Sect 1,Yihuan Rd, Chengdu 610065, Peoples R China.
EM fmf@scu.edu.cn
FU National Natural Science Founda- tion of China [11971337]
FX This research is supported by the National Natural Science Founda- tion
   of China (No.11971337) .
CR ACAR R, 1994, INVERSE PROBL, V10, P1217, DOI 10.1088/0266-5611/10/6/003
   Anwar S, 2019, IEEE T PATTERN ANAL, V41, P2112, DOI 10.1109/TPAMI.2018.2855177
   Ates HF, 2023, COMPUT VIS IMAGE UND, V233, DOI 10.1016/j.cviu.2023.103718
   Bai YC, 2020, IEEE T CIRC SYST VID, V30, P2033, DOI 10.1109/TCSVT.2019.2919159
   Ben-Loghfyry A, 2023, J APPL MATH COMPUT, V69, P1431, DOI 10.1007/s12190-022-01798-9
   Chan TF, 2005, IMAGE PROCESSING AND ANALYSIS, P1, DOI 10.1137/1.9780898717877
   Chan TF, 1998, IEEE T IMAGE PROCESS, V7, P370, DOI 10.1109/83.661187
   Chen L, 2019, PROC CVPR IEEE, P1742, DOI 10.1109/CVPR.2019.00184
   Cho S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618491
   Dwivedi P, 2023, IMAGE VISION COMPUT, V136, DOI 10.1016/j.imavis.2023.104747
   Feng X, 2023, CIRC SYST SIGNAL PR, V42, P5478, DOI 10.1007/s00034-023-02365-8
   Gong D, 2016, PROC CVPR IEEE, P1827, DOI 10.1109/CVPR.2016.202
   Gupta A, 2010, LECT NOTES COMPUT SC, V6311, P171, DOI 10.1007/978-3-642-15549-9_13
   Hu Z, 2014, PROC CVPR IEEE, P2893, DOI 10.1109/CVPR.2014.370
   Hu Z, 2014, PROC CVPR IEEE, P3382, DOI 10.1109/CVPR.2014.432
   Huo D, 2023, Arxiv, DOI arXiv:2202.00179
   Joshi N, 2008, PROC CVPR IEEE, P3823
   Koh J, 2021, COMPUT VIS IMAGE UND, V203, DOI 10.1016/j.cviu.2020.103134
   Krishnan D, 2011, PROC CVPR IEEE, P233, DOI 10.1109/CVPR.2011.5995521
   Levin A., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2657, DOI 10.1109/CVPR.2011.5995308
   Levin A, 2011, IEEE T PATTERN ANAL, V33, P2354, DOI 10.1109/TPAMI.2011.148
   Li LRH, 2018, PROC CVPR IEEE, P6616, DOI 10.1109/CVPR.2018.00692
   Liang Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P631, DOI 10.1007/978-3-030-58595-2_38
   Liao XR, 2021, NONLINEAR DYNAM, V103, P1999, DOI 10.1007/s11071-020-06136-x
   Lin TC, 2018, IEEE T IMAGE PROCESS, V27, P2762, DOI 10.1109/TIP.2018.2811048
   Liu J, 2022, J VIS COMMUN IMAGE R, V89, DOI 10.1016/j.jvcir.2022.103645
   Liu J, 2021, IEEE T PATTERN ANAL, V43, P1041, DOI 10.1109/TPAMI.2019.2941472
   Money JH, 2008, IMAGE VISION COMPUT, V26, P302, DOI 10.1016/j.imavis.2007.06.005
   Pan JS, 2018, IEEE T PATTERN ANAL, V40, P2315, DOI 10.1109/TPAMI.2017.2753804
   Pan JS, 2017, IEEE T PATTERN ANAL, V39, P342, DOI 10.1109/TPAMI.2016.2551244
   Pan JS, 2014, PROC CVPR IEEE, P2901, DOI 10.1109/CVPR.2014.371
   Shan Q, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360672
   Sun J, 2015, PROC CVPR IEEE, P769, DOI 10.1109/CVPR.2015.7298677
   Sun LB, 2013, IEEE INT CONF COMPUT
   Wen F, 2021, IEEE T CIRC SYST VID, V31, P2923, DOI 10.1109/TCSVT.2020.3034137
   Xu L, 2013, PROC CVPR IEEE, P1107, DOI 10.1109/CVPR.2013.147
   Xu L, 2010, LECT NOTES COMPUT SC, V6311, P157
   Xu Y, 2021, COMPUT VIS IMAGE UND, V205, DOI 10.1016/j.cviu.2021.103169
   Yan YY, 2017, PROC CVPR IEEE, P6978, DOI 10.1109/CVPR.2017.738
   Zhang JP, 2015, SIAM J IMAGING SCI, V8, P2487, DOI 10.1137/14097121X
   Zheng SC, 2013, IEEE I CONF COMP VIS, P1465, DOI 10.1109/ICCV.2013.185
   ZOSSO D., 2014, Comput. Vis. Image Underst., P14
NR 42
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUL
PY 2024
VL 147
AR 105080
DI 10.1016/j.imavis.2024.105080
EA JUN 2024
PG 14
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA UY6H0
UT WOS:001251658400001
DA 2024-08-05
ER

PT J
AU Gong, S
   Teng, Z
   Li, R
   Fan, J
   Zhang, BP
   Fan, JP
AF Gong, Shuang
   Teng, Zhu
   Li, Rui
   Fan, Jack
   Zhang, Baopeng
   Fan, Jianping
TI MINet: Modality interaction network for unified multi-modal tracking
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Modality interaction; Memory query; Multi -modal tracking
ID ROBUST
AB While RGB-based trackers have made impressive progress, they still falter in complex scenarios, necessitating the exploration of multi-modal tracking strategies that leverage auxiliary modalities. However, most existing methods lack sufficient exploration and interaction of complementary information within and between modalities. To address this, we propose a Modality Interaction Network (MINet), a unified framework for multi-modal tracking. It consists of a Modality Representation Module (MRM) and a Memory Query Module (MQM). MRM enforces communications between different modalities by a designed Modality Interaction module (MIM) and fuses multi-modal information by a Modality Fuse Module (MFM) to generate more discriminative representation. MQM maintains historical multi-modal information and builds long-range dependencies between current and historical targets for tracking, which enhances the tracking performance, especially when targets undergo significant deformation and occlusions. To verify efficiency across different multi-modal tracking paradigms, we conduct extensive experiments, including RGB-D, RGB-T, and RGB-E. The experimental results demonstrate that in these multi-modal tracking tasks, the proposed MINet achieves outstanding performance compared to state-ofthe-art trackers. Specifically, it outperforms them by 1% in RGB-D, 1.2% in RGB-T, and 1% in RGB-E tracking performance, respectively.
C1 [Gong, Shuang; Teng, Zhu; Li, Rui; Zhang, Baopeng] Beijing Jiaotong Univ, Sch Comp & Informat Technol, Beijing 100044, Peoples R China.
   [Fan, Jack] Yoomi Hlth Inc, New York, NY USA.
   [Fan, Jianping] Lenovo Res, AI Lab, Beijing 100085, Peoples R China.
C3 Beijing Jiaotong University; Legend Holdings; Lenovo
RP Teng, Z (corresponding author), Beijing Jiaotong Univ, Sch Comp & Informat Technol, Beijing 100044, Peoples R China.
EM 21120351@bjtu.edu.cn; zteng@bjtu.edu.cn; rui.li@bjtu.edu.cn;
   fan23j@live.unc.edu; bpzhang@bjtu.edu.cn; jfan1@lenovo.com
FU Fundamental Research Funds for the Central Universities of China
   [2024JBMC008]
FX This work was supported by the Fundamental Research Funds for the
   Central Universities of China (2024JBMC008) .
CR Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bhat G, 2019, IEEE I CONF COMP VIS, P6181, DOI 10.1109/ICCV.2019.00628
   Chen X, 2021, PROC CVPR IEEE, P8122, DOI 10.1109/CVPR46437.2021.00803
   Chenglong Li, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P222, DOI 10.1007/978-3-030-58542-6_14
   Cui YT, 2022, PROC CVPR IEEE, P13598, DOI 10.1109/CVPR52688.2022.01324
   Danelljan M, 2020, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR42600.2020.00721
   Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Fu ZH, 2021, PROC CVPR IEEE, P13769, DOI 10.1109/CVPR46437.2021.01356
   Gao Y, 2019, IEEE INT CONF COMP V, P91, DOI 10.1109/ICCVW.2019.00017
   He KJ, 2023, Arxiv, DOI arXiv:2302.13840
   Kristan M., 2019, P IEEE CVF INT C COM, P0
   Kristan M., 2020, P COMP VIS ECCV 20 5, P547, DOI 10.1007/978-3-030-68238-5_39
   Kristan M., 2022, EUROPEAN C COMPUTER, P431
   Kristan M, 2021, IEEE INT CONF COMP V, P2711, DOI 10.1109/ICCVW54120.2021.00305
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li CL, 2022, IEEE T IMAGE PROCESS, V31, P392, DOI 10.1109/TIP.2021.3130533
   Li CL, 2019, PATTERN RECOGN, V96, DOI 10.1016/j.patcog.2019.106977
   Li CL, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1856, DOI 10.1145/3123266.3123289
   Liu HJ, 2016, IEEE INT SYMP CIRC S, P2511, DOI 10.1109/ISCAS.2016.7539103
   Mueller F, 2017, IEEE I CONF COMP VIS, P1163, DOI 10.1109/ICCV.2017.131
   Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465
   Qian YL, 2021, INT C PATT RECOG, P7825, DOI 10.1109/ICPR48806.2021.9412984
   Song YB, 2018, PROC CVPR IEEE, P8990, DOI 10.1109/CVPR.2018.00937
   Voigtlaender P, 2020, PROC CVPR IEEE, P6577, DOI 10.1109/CVPR42600.2020.00661
   Wang CQ, 2020, PROC CVPR IEEE, P7062, DOI 10.1109/CVPR42600.2020.00709
   Wang X, 2023, IEEE Transactions on Cybernetics, DOI DOI 10.48550/ARXIV.2108.05015
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Xiao Y, 2022, AAAI CONF ARTIF INTE, P2831
   Xu YD, 2020, AAAI CONF ARTIF INTE, V34, P12549
   Yan B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10428, DOI 10.1109/ICCV48922.2021.01028
   Yan S, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10705, DOI 10.1109/ICCV48922.2021.01055
   Yang JY, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P3492, DOI 10.1145/3503161.3547851
   Yang TY, 2018, LECT NOTES COMPUT SC, V11213, P153, DOI 10.1007/978-3-030-01240-3_10
   Ye BT, 2022, LECT NOTES COMPUT SC, V13682, P341, DOI 10.1007/978-3-031-20047-2_20
   Yu YC, 2020, PROC CVPR IEEE, P6727, DOI 10.1109/CVPR42600.2020.00676
   Zhang H, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20020393
   Zhang JQ, 2022, PROC CVPR IEEE, P8791, DOI 10.1109/CVPR52688.2022.00860
   Zhang JQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13023, DOI 10.1109/ICCV48922.2021.01280
   Zhang PY, 2022, PROC CVPR IEEE, P8876, DOI 10.1109/CVPR52688.2022.00868
   Zhang PY, 2021, IEEE T IMAGE PROCESS, V30, P3335, DOI 10.1109/TIP.2021.3060862
   Zhang TL, 2022, IEEE T CIRC SYST VID, V32, P1403, DOI 10.1109/TCSVT.2021.3072207
   Zhang XC, 2020, INFORM FUSION, V63, P166, DOI 10.1016/j.inffus.2020.05.002
   Zhao C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6444, DOI 10.1109/ICCV48922.2021.00640
   Zhu JW, 2023, PROC CVPR IEEE, P9516, DOI 10.1109/CVPR52729.2023.00918
   Zhu X.-F., 2023, P AAAI C ART INT, V37, P3870
   Zhu YB, 2021, IEEE T INTELL VEHICL, V6, P121, DOI 10.1109/TIV.2020.2980735
NR 48
TC 0
Z9 0
U1 2
U2 2
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD AUG
PY 2024
VL 148
AR 105071
DI 10.1016/j.imavis.2024.105071
EA JUN 2024
PG 10
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA WZ0R4
UT WOS:001258583100001
DA 2024-08-05
ER

PT J
AU Xie, CY
   Liu, Q
   Chen, BJ
   Hao, ZQ
AF Xie, Chenyang
   Liu, Qiang
   Chen, Baojia
   Hao, Zhiqiang
TI Evaluation and analysis of feature point detection methods based on
   vSLAM systems
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Feature point detection; vSLAM; Complex datasets; Performance evaluation
ID ROBUST; VERSATILE
AB Feature point detection is identified as a significant issue in the field of computer vision. Quantitative conclusions have been established regarding the performance evaluation of feature point detection using manually annotated datasets. However, these datasets, obtained through affine transformations with preset parameters on images, exhibit limitations in terms of variety, quantity, and challenges, differing from real-world application scenarios. In actual scenes, Vision Simultaneous Localization and Mapping (vSLAM) systems are extensively applied, and the precision of their localization and mapping is directly linked to the performance of feature points. To profoundly understand the performance of feature point detection in practical applications, vSLAM systems are chosen for evaluation in this study. More diverse and challenging datasets are utilized, encompassing real datasets covering variations in lighting, rotation, occlusion, and camera angle changes, as well as synthetic datasets reflecting complex conditions such as time, season, motion patterns, and environmental textures. Based on the evaluation results, the applicability of various feature point detection methods in different environments is thoroughly discussed, and the underlying principles are analyzed (Table 13). The conclusions drawn from this research provide references for the development of new feature point detection methods, the selection of such methods in vSLAM systems, and other related studies in the field of computer vision.
C1 [Xie, Chenyang; Liu, Qiang; Chen, Baojia] China Three Gorges Univ, Coll Mech & Power Engn, 8 Daxue Rd, Yichang 443000, Hubei, Peoples R China.
   [Hao, Zhiqiang] Wuhan Univ Sci & Technol, Sch Met & Ecol Engn, 947 Heping Ave, Wuhan 430000, Hubei, Peoples R China.
C3 China Three Gorges University; Wuhan University of Science & Technology
RP Xie, CY (corresponding author), China Three Gorges Univ, Coll Mech & Power Engn, 8 Daxue Rd, Yichang 443000, Hubei, Peoples R China.
EM 1340856660@qq.com
FU Excellent Young and Middle-aged Talent Program for Scientific and
   Technological Research of Hubei Provincial Department of Education
   [Q20221204]; Key Laboratory of Metallurgical Equipment and Control
   Technology, Ministry of Education, Wuhan University of Science and
   Technology [MECOF2022B02]
FX This research was sponsored by the Excellent Young and Middle-aged
   Talent Program for Scientific and Technological Research of Hubei
   Provincial Department of Education (Q20221204) . This research was
   sponsored by the Key Laboratory of Metallurgical Equipment and Control
   Technology, Ministry of Education, Wuhan University of Science and
   Technology (MECOF2022B02) .
CR Alcantarilla PF, 2012, LECT NOTES COMPUT SC, V7577, P214, DOI 10.1007/978-3-642-33783-3_16
   Amirkhani A, 2021, IEEE ACCESS, V9, P119049, DOI 10.1109/ACCESS.2021.3107841
   Balntas V, 2017, PROC CVPR IEEE, P3852, DOI 10.1109/CVPR.2017.410
   Barandiaran I, 2013, CYBERNET SYST, V44, P98, DOI 10.1080/01969722.2013.762232
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Burri M, 2016, INT J ROBOT RES, V35, P1157, DOI 10.1177/0278364915620033
   Calonder M, 2010, LECT NOTES COMPUT SC, V6314, P778, DOI 10.1007/978-3-642-15561-1_56
   Campos C, 2021, IEEE T ROBOT, V37, P1874, DOI 10.1109/TRO.2021.3075644
   [曹力科 Cao Like], 2021, [机器人, Robot], V43, P193
   Ceng LC, 2020, OPTIMIZATION, V69, P357, DOI 10.1080/02331934.2019.1625354
   Criado R, 2012, J COMPUT APPL MATH, V236, P2975, DOI 10.1016/j.cam.2011.05.026
   Davison AJ, 2007, IEEE T PATTERN ANAL, V29, P1052, DOI 10.1109/TPAMI.2007.1049
   DeTone D, 2018, IEEE COMPUT SOC CONF, P337, DOI 10.1109/CVPRW.2018.00060
   Dusmanu M, 2019, PROC CVPR IEEE, P8084, DOI 10.1109/CVPR.2019.00828
   Farrukh FUD, 2022, ELECTRONICS-SWITZ, V11, DOI 10.3390/electronics11244168
   Gao X., 2017, Lectures on Visual SLAM: From Theory to PracticeM
   [高兴波 Gao Xingbo], 2021, [机器人, Robot], V43, P733
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Girdhar R, 2018, PROC CVPR IEEE, P350, DOI 10.1109/CVPR.2018.00044
   Harris C, 1988, A combined corner and edge detector, P147, DOI [10.5244/C.2.23.23.1-23.6, DOI 10.5244/C.2.23]
   He LX, 2018, PROC CVPR IEEE, P7054, DOI 10.1109/CVPR.2018.00737
   Huang H, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS (ROBIO), P1619, DOI 10.1109/ROBIO.2016.7866559
   Jing JF, 2023, IEEE T PATTERN ANAL, V45, P4694, DOI 10.1109/TPAMI.2022.3201185
   Khosravian A, 2022, P I MECH ENG D-J AUT, V236, P1849, DOI 10.1177/09544070211042961
   Klein G., 2017, 2007 6 IEEE ACM INT, P225
   [李云天 Li Yuntian], 2021, [控制与决策, Control and Decision], V36, P513
   Lindeberg T, 2015, J MATH IMAGING VIS, V52, P3, DOI 10.1007/s10851-014-0541-0
   Lowe D. G., 1999, Computer vision, V2, P1150, DOI [10.1109/ICCV.1999.790410, DOI 10.1109/ICCV.1999.790410]
   Lowry S., 2015, IEEE Trans. Robot., V31, P1147
   Ma JY, 2018, IEEE T GEOSCI REMOTE, V56, P4435, DOI 10.1109/TGRS.2018.2820040
   Miao S., 2021, Comput. Meas. Control, V29, P1
   Montemerlo M, 2003, IEEE INT CONF ROBOT, P1985, DOI 10.1109/ROBOT.2003.1241885
   Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103
   Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671
   Murphy K, 2001, STAT ENG IN, P499
   Özuysal M, 2010, IEEE T PATTERN ANAL, V32, P448, DOI 10.1109/TPAMI.2009.23
   Pire T, 2017, ROBOT AUTON SYST, V93, P27, DOI 10.1016/j.robot.2017.03.019
   Qin T, 2018, IEEE T ROBOT, V34, P1004, DOI 10.1109/TRO.2018.2853729
   Revaud J., 2019, 33 C NEUR INF PROC S, P1
   Rosten E, 2006, LECT NOTES COMPUT SC, V3951, P430, DOI 10.1007/11744023_34
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Savinov N, 2017, PROC CVPR IEEE, P3929, DOI 10.1109/CVPR.2017.418
   Shah S., 2018, FIELD SERVICE ROBOTI, P621, DOI [10.1007/978-3-319-67361-5_40, DOI 10.1007/978-3-319-67361-5_40]
   SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794
   Smith R., 1990, Autonomous Robot Vehicles, P167
   Sun Z, 2018, PROC CVPR IEEE, P7957, DOI 10.1109/CVPR.2018.00830
   Usenko V, 2020, IEEE ROBOT AUTOM LET, V5, P422, DOI 10.1109/LRA.2019.2961227
   Verdie Y, 2015, PROC CVPR IEEE, P5279, DOI 10.1109/CVPR.2015.7299165
   Wang WS, 2020, IEEE INT C INT ROBOT, P4909, DOI 10.1109/IROS45743.2020.9341801
   Yi KM, 2016, LECT NOTES COMPUT SC, V9910, P467, DOI 10.1007/978-3-319-46466-4_28
   Zeng Q.H., 2022, J. Nanjing Univ. Aeronaut. Astronaut., P54
   Zheng S, 2016, PROC CVPR IEEE, P4480, DOI 10.1109/CVPR.2016.485
   Zou DP, 2019, IEEE T ROBOT, V35, P999, DOI 10.1109/TRO.2019.2915140
NR 53
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUN
PY 2024
VL 146
AR 105015
DI 10.1016/j.imavis.2024.105015
EA MAY 2024
PG 18
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA YE7Q3
UT WOS:001266878400001
DA 2024-08-05
ER

PT J
AU Hwang, J
   Kim, BG
   Kim, T
   Oh, H
   Kang, JW
AF Hwang, Juheon
   Kim, Byung-gyu
   Kim, Taewan
   Oh, Heeseok
   Kang, Jiwoo
TI EMOVA: Emotion-driven neural volumetric avatar
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Face reconstruction; Multimodal analysis; Novel view synthesis; Neural
   rendering; Dynamic scenes
ID FACIAL EXPRESSION RECOGNITION; MODEL; FACE
AB 3D facial reconstruction is essential for metaverse applications. Traditional mesh-based methods have difficulty rendering photorealistic faces and complex objects. Recent advancements in Neural Radiance Fields (NeRFs) have excelled in representing complex objects. However, they struggle with capturing subtle facial differences, particularly around the eyes and mouth, due to their reliance on RGB value comparisons. To address this, we propose an EMOtion-driven Volumetric Avatar (EMOVA) that utilizes emotional stimuli from images and voices to enhance facial precision. Visual emotional features ensure that the reconstructed face aligns with the input emotion, while auditory features enhance facial details from various viewpoints. Through an attention-based fusion of these features, EMOVA accurately captures and reconstructs faces, even in self-occlusion. EMOVA outperforms the state-of-the-art methods by more than 5.93% in terms of LPIPS for face reconstruction.
C1 [Hwang, Juheon] Yonsei Univ, Dept Elect & Elect Engn, Yonsei Ro 50, Seoul 03722, South Korea.
   [Kim, Byung-gyu; Kang, Jiwoo] Sookmyung Womens Univ, Div Artificial Intelligence Engn, Cheongpa ro 100, Seoul 04310, South Korea.
   [Kim, Taewan] Dongduk Womens Univ, Hwarang Ro 13 Gil 60, Seoul 04310, South Korea.
   [Oh, Heeseok] Hansung Univ, Dept Appl AI, Samseongyo Ro 16 Gil 116, Seoul 02748, South Korea.
C3 Yonsei University; Sookmyung Women's University; Dongduk Women's
   University; Hansung University
RP Kang, JW (corresponding author), Sookmyung Womens Univ, Div Artificial Intelligence Engn, Cheongpa ro 100, Seoul 04310, South Korea.
EM jwkang@sookmyung.ac.kr
OI kim, taewan/0000-0003-3319-7797
FU Institute of Information & communications Technology Planning &
   Evaluation (IITP) grant - Korea government (MSIT);  [RS-2023-00229451]
FX This work was supported by Institute of Information & communications
   Technology Planning & Evaluation (IITP) grant funded by the Korea
   government (MSIT) (No. RS-2023-00229451, Interoperable Digital Human
   (Avatar) Interlocking Technology Between Heterogeneous Platforms) .
CR Athar S, 2022, PROC CVPR IEEE, P20332, DOI 10.1109/CVPR52688.2022.01972
   Benitez-Quiroz CF, 2016, PROC CVPR IEEE, P5562, DOI 10.1109/CVPR.2016.600
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Booth J, 2016, PROC CVPR IEEE, P5543, DOI 10.1109/CVPR.2016.598
   Cao A, 2023, PROC CVPR IEEE, P130, DOI 10.1109/CVPR52729.2023.00021
   Cao C, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766943
   Cao HW, 2014, IEEE T AFFECT COMPUT, V5, P377, DOI 10.1109/TAFFC.2014.2336244
   Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020
   Chan ER, 2022, PROC CVPR IEEE, P16102, DOI 10.1109/CVPR52688.2022.01565
   Chen MY, 2018, IEEE SIGNAL PROC LET, V25, P1440, DOI 10.1109/LSP.2018.2860246
   Danecek R, 2022, PROC CVPR IEEE, P20279, DOI 10.1109/CVPR52688.2022.01967
   Feng X., 2005, Pattern Recognition and Image Analysis, V15, P546
   Feng Y, 2018, LECT NOTES COMPUT SC, V11218, P557, DOI 10.1007/978-3-030-01264-9_33
   Fridovich-Keil S, 2023, PROC CVPR IEEE, P12479, DOI 10.1109/CVPR52729.2023.01201
   Gafni G, 2021, PROC CVPR IEEE, P8645, DOI 10.1109/CVPR46437.2021.00854
   Garrido P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2890493
   Ghosh A, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024163
   Gong Y, 2021, Arxiv, DOI arXiv:2104.01778
   Jack RE, 2015, CURR BIOL, V25, pR621, DOI 10.1016/j.cub.2015.05.052
   Jain A.K., 2011, Handbook of Face Recognition, V1
   Kemelmacher-Shlizerman I, 2011, IEEE T PATTERN ANAL, V33, P394, DOI 10.1109/TPAMI.2010.63
   Keren G, 2016, IEEE IJCNN, P3412, DOI 10.1109/IJCNN.2016.7727636
   Klaudiny M, 2012, SECOND JOINT 3DIM/3DPVT CONFERENCE: 3D IMAGING, MODELING, PROCESSING, VISUALIZATION & TRANSMISSION (3DIMPVT 2012), P17, DOI 10.1109/3DIMPVT.2012.67
   Kossaifi J, 2021, IEEE T PATTERN ANAL, V43, P1022, DOI 10.1109/TPAMI.2019.2944808
   Kossaifi J, 2017, IMAGE VISION COMPUT, V65, P23, DOI 10.1016/j.imavis.2017.02.001
   Lei BW, 2023, PROC CVPR IEEE, P394, DOI 10.1109/CVPR52729.2023.00046
   Li S, 2022, IEEE T AFFECT COMPUT, V13, P1195, DOI 10.1109/TAFFC.2020.2981446
   Li TY, 2022, PROC CVPR IEEE, P5511, DOI 10.1109/CVPR52688.2022.00544
   Li TY, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130813
   Li YC, 2019, INTERSPEECH, P2803, DOI 10.21437/Interspeech.2019-2594
   Li ZQ, 2021, PROC CVPR IEEE, P6494, DOI 10.1109/CVPR46437.2021.00643
   Lombardi S, 2021, ACM T GRAPHIC, V40, DOI [10.1145/3476576.3476608, 10.1145/3450626.3459863]
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Mollahosseini A, 2019, IEEE T AFFECT COMPUT, V10, P18, DOI 10.1109/TAFFC.2017.2740923
   Morales A, 2021, COMPUT SCI REV, V40, DOI 10.1016/j.cosrev.2021.100400
   Nagrani A, 2018, Arxiv, DOI [arXiv:1706.08612, DOI 10.21437/INTERSPEECH.2017-950]
   Nagrani A, 2018, LECT NOTES COMPUT SC, V11217, P73, DOI 10.1007/978-3-030-01261-8_5
   Nagrani A, 2018, PROC CVPR IEEE, P8427, DOI 10.1109/CVPR.2018.00879
   Nawaz S, 2021, IEEE COMPUT SOC CONF, P1682, DOI 10.1109/CVPRW53098.2021.00184
   Nawaz S, 2019, 2019 DIGITAL IMAGE COMPUTING: TECHNIQUES AND APPLICATIONS (DICTA), P83, DOI 10.1109/dicta47822.2019.8945863
   Nonis F, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9183904
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   Pantic M, 2000, IMAGE VISION COMPUT, V18, P881, DOI 10.1016/S0262-8856(00)00034-2
   Park K, 2021, Arxiv, DOI arXiv:2106.13228
   Park K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5845, DOI 10.1109/ICCV48922.2021.00581
   Ploumpis S, 2021, IEEE T PATTERN ANAL, V43, P4142, DOI 10.1109/TPAMI.2020.2991150
   Pumarola Albert, 2021, 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), P10313, DOI 10.1109/CVPR46437.2021.01018
   Saeed MS, 2022, INT CONF ACOUST SPEE, P7057, DOI 10.1109/ICASSP43922.2022.9747704
   Sandbach G, 2012, IMAGE VISION COMPUT, V30, P683, DOI 10.1016/j.imavis.2012.06.005
   Shan CF, 2009, IMAGE VISION COMPUT, V27, P803, DOI 10.1016/j.imavis.2008.08.005
   Sharma S, 2022, ARCH COMPUT METHOD E, V29, P3475, DOI 10.1007/s11831-021-09705-4
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tian YI, 2001, IEEE T PATTERN ANAL, V23, P97, DOI 10.1109/34.908962
   Trigeorgis G, 2016, INT CONF ACOUST SPEE, P5200, DOI 10.1109/ICASSP.2016.7472669
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wen PS, 2021, PROC CVPR IEEE, P16342, DOI 10.1109/CVPR46437.2021.01608
   Wen YD, 2018, Arxiv, DOI arXiv:1807.04836
   Wen Z, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1343, DOI 10.1109/ICCV.2003.1238646
   Wuu CH, 2023, Arxiv, DOI arXiv:2207.11243
   Xian WQ, 2021, PROC CVPR IEEE, P9416, DOI 10.1109/CVPR46437.2021.00930
   Yang SW, 2021, Arxiv, DOI arXiv:2105.01051
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
NR 62
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUN
PY 2024
VL 146
AR 105043
DI 10.1016/j.imavis.2024.105043
EA APR 2024
PG 11
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA ST5O7
UT WOS:001236713400001
DA 2024-08-05
ER

PT J
AU Zhou, J
   Yang, DG
   Song, TT
   Ye, YC
   Zhang, X
   Song, YZ
AF Zhou, Jie
   Yang, Degang
   Song, Tingting
   Ye, Yichen
   Zhang, Xin
   Song, Yingze
TI Improved YOLOv7 models based on modulated deformable convolution and
   swin transformer for object detection in fisheye images
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Fisheye image; YOLOv7; Modulated deformable convolution; Swin
   transformer; Object detection
ID VISUAL-PERCEPTION; NETWORK
AB Thanks to the wide view field, the fisheye camera can get much more visual information. Thus, it is widely used in the field of computer vision. However, projection is often required for fisheye images to be used for object detection. Meanwhile, the projection will lead to distortion in fisheye images, and the discontinuous image edges will make the objects incomplete. Fisheye images are characterized by objects that are large near and small far. These problems are still challenges for the existing advanced object detector YOLOv7. Therefore, in this paper, we propose an improved YOLOv7 model. First, Modulated Deformable Convolution is introduced into the YOLOv7 model to automatically adapt to distortion changes of distorted objects in fisheye images. It not only adjusts the sampling position of the convolutional kernel but also further extends the deformation range. The improved model can efficiently extract features of distorted and edge -discontinuous objects. In addition, fisheye images are characterized by objects close to the fisheye lens being large, while objects farther away from the fisheye lens will be smaller. To further optimize the detection performance of small objects in fisheye images, Swin Transformer is also introduced into the YOLOv7 model, and Swin Transformer Block with Window Multihead Self -Attention (W-MSA) Effectively enhances Network Local Perception. Finally, our proposed model achieves up to 2.4% improvement in mAP compared to the original YOLOv7 model on the ERP-360 dataset. Also, the proposed model achieves the best results compared to other state-of-the-art object detection methods for equirectangular projection images. On the VOC-360 dataset, our proposed model improves the mAP by up to 5.9% compared to the original YOLOv7 model. The experimental results show that the proposed models achieve good results for object detection in both fisheye images and equirectangular projection images. The ERP-360 dataset, source code and pre -trained models for related tasks can be found at https://github.com/xiaoxi aomichong/ERP-360dataset.
C1 [Zhou, Jie; Yang, Degang; Song, Tingting; Zhang, Xin; Song, Yingze] Chongqing Normal Univ, Coll Comp & Informat Sci, Chongqing 401331, Peoples R China.
   [Yang, Degang] Chongqing Engn Res Ctr Educ Big Data Intelligent P, Chongqing 401331, Peoples R China.
   [Ye, Yichen] Southwest Univ, Coll Elect & Informat Engn, Chongqing 400715, Peoples R China.
C3 Chongqing Normal University; Southwest University - China
RP Song, TT (corresponding author), Chongqing Normal Univ, Coll Comp & Informat Sci, Chongqing 401331, Peoples R China.
EM ttsong@cqnu.edu.cn
FU Natural Science Foundation Chongqing [CSTB2022NSCQ-MSX1200]; Science and
   Technology Research Program of Chongqing Municipal Education Commission
   [KJQN202200537, KJZD-M202300502]; Chongqing Normal University [21XLB035]
FX This research was supported in part by Natural Science Foundation
   Chongqing (CSTB2022NSCQ-MSX1200) , in part by the Science and Technology
   Research Program of Chongqing Municipal Education Commission
   (KJQN202200537 and KJZD-M202300502) , in part Chongqing Normal
   University Ph.D. Start-up Fund (21XLB035) .
CR Arsenali B, 2019, IEEE INT CONF COMP V, P2373, DOI 10.1109/ICCVW.2019.00291
   Barmpoutis P, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12193177
   Benseddik HE, 2020, INT J ROBOT RES, V39, P1037, DOI 10.1177/0278364920915248
   Bertel T, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417770
   Billings G, 2020, IEEE ROBOT AUTOM LET, V5, P4241, DOI 10.1109/LRA.2020.2994036
   Bo-Hong Lin, 2020, 2020 International Conference on Pervasive Artificial Intelligence (ICPAI), P194, DOI 10.1109/ICPAI51961.2020.00043
   Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Cao X, 2023, SENSORS-BASEL, V23, DOI 10.3390/s23073634
   Chen PY, 2019, IEEE IMAGE PROC, P2956, DOI [10.1109/ICIP.2019.8803719, 10.1109/icip.2019.8803719]
   Chiang SH, 2021, IMAGE VISION COMPUT, V105, DOI 10.1016/j.imavis.2020.104069
   Coors B, 2018, LECT NOTES COMPUT SC, V11213, P525, DOI 10.1007/978-3-030-01240-3_32
   Cruz-Mota J, 2012, INT J COMPUT VISION, V98, P217, DOI 10.1007/s11263-011-0505-4
   Cui ZP, 2019, IEEE INT CONF ROBOT, P6087, DOI 10.1109/icra.2019.8793884
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Delibasis KK, 2016, INTEGR COMPUT-AID E, V23, P185, DOI 10.3233/ICA-160511
   Delibasis KK, 2018, J IMAGING, V4, DOI 10.3390/jimaging4060073
   Demonceaux C, 2011, IMAGE VISION COMPUT, V29, P840, DOI 10.1016/j.imavis.2011.09.007
   Fu JL, 2019, DATA BRIEF, V27, DOI 10.1016/j.dib.2019.104752
   Gao WL, 2020, J FIELD ROBOT, V37, P497, DOI 10.1002/rob.21946
   Ge Z, 2021, Arxiv, DOI arXiv:2107.08430
   Glenn J., YOLOv8
   Glenn J., Yolov5
   Häne C, 2017, IMAGE VISION COMPUT, V68, P14, DOI 10.1016/j.imavis.2017.07.003
   Huang MK, 2023, IMAGE VISION COMPUT, V129, DOI 10.1016/j.imavis.2022.104590
   Huo D, 2023, 2023 18TH INTERNATIONAL CONFERENCE ON MACHINE VISION AND APPLICATIONS, MVA, DOI 10.23919/MVA57639.2023.10216093
   Kim S, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12052403
   Kumar VR, 2021, IEEE ROBOT AUTOM LET, V6, P2830, DOI 10.1109/LRA.2021.3062324
   Lee Y, 2019, PROC CVPR IEEE, P9173, DOI 10.1109/CVPR.2019.00940
   Li CY, 2022, Arxiv, DOI [arXiv:2209.02976, DOI 10.48550/ARXIV.2209.02976]
   Li M, 2021, IMAGE VISION COMPUT, V114, DOI 10.1016/j.imavis.2021.104264
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Luo C, 2021, COMPUT-AIDED CIV INF, V36, P1585, DOI 10.1111/mice.12686
   Redmon J., 2018, CoRR
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Roxas M, 2020, IEEE ROBOT AUTOM LET, V5, P1303, DOI 10.1109/LRA.2020.2967657
   Su YC, 2017, ADV NEUR IN, V30
   Su YC, 2019, PROC CVPR IEEE, P9434, DOI 10.1109/CVPR.2019.00967
   Sun Y, 2019, ENG COMPUTATION, V36, P2403, DOI 10.1108/EC-09-2018-0431
   Wang CY, 2023, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR52729.2023.00721
   Yang L, 2020, COMPUT VIS IMAGE UND, V196, DOI 10.1016/j.cviu.2020.102968
   Yang ST, 2019, PROC CVPR IEEE, P3358, DOI 10.1109/CVPR.2019.00348
   Zhang X, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app122312398
   Zhou YM, 2020, IEEE J-STSP, V14, P118, DOI 10.1109/JSTSP.2019.2957952
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
NR 45
TC 0
Z9 0
U1 1
U2 1
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD APR
PY 2024
VL 144
AR 104966
DI 10.1016/j.imavis.2024.104966
EA MAR 2024
PG 13
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA QC4V2
UT WOS:001218676000001
DA 2024-08-05
ER

PT J
AU Zhang, X
   Song, YZ
   Song, TT
   Yang, DG
   Ye, YC
   Zhou, J
   Zhang, LM
AF Zhang, Xin
   Song, Yingze
   Song, Tingting
   Yang, Degang
   Ye, Yichen
   Zhou, Jie
   Zhang, Liming
TI LDConv: Linear deformable convolution for improving convolutional neural
   networks
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Novel convolutional operation; Arbitrary sampled shapes; Arbitrary
   number of parameters; Object detection
AB Neural networks based on convolutional operations have achieved remarkable results in the field of deep learning, but there are two inherent flaws in standard convolutional operations. On the one hand, the convolution operation is confined to a local window, so it cannot capture information from other locations, and its sampled shapes is fixed. On the other hand, the size of the convolutional kernel is fixed to k x k, which is a fixed square shape, and the number of parameters tends to grow squarely with size. Although Deformable Convolution (Deformable Conv) address the problem of fixed sampling of standard convolutions, the number of parameters also tends to grow in a squared manner, and Deformable Conv do not explore the effect of different initial sample shapes on network performance. In response to the above questions, the Linear Deformable Convolution (LDConv) is explored in this work, which gives the convolution kernel an arbitrary number of parameters and arbitrary sampled shapes to provide richer options for the trade-off between network overhead and performance. In LDConv, a novel coordinate generation algorithm is defined to generate different initial sampled positions for convolutional kernels of arbitrary size. To adapt to changing targets, offsets are introduced to adjust the shape of the samples at each position. LDConv corrects the growth trend of the number of parameters for standard convolution and Deformable Conv to a linear growth. Compared to Deformable Conv, LDConv provides richer choices and can be equivalent to deformable convolution when the number of parameters of LDConv is set to the square of K. Differently, this paper also explores the effect of neural networks by using LDConv with the same size and different initial sampling shapes. LDConv completes the process of efficient feature extraction by irregular convolutional operations and brings more exploration options for convolutional sampled shapes. Object detection experiments on representative datasets COCO2017, VOC 7 + 12, and VisDrone-DET2021 fully demonstrate the advantages of LDConv. LDConv is a plug-and-play convolutional operation that can replace the convolutional operation to improve network performance. The code for the relevant tasks can be found at https://github. com/CV-ZhangXin/LDConv. .
C1 [Zhang, Xin; Song, Yingze; Song, Tingting; Yang, Degang; Zhou, Jie; Zhang, Liming] Chongqing Normal Univ, Coll Comp & Informat Sci, Chongqing 401331, Peoples R China.
   [Yang, Degang] Chongqing Engn Res Ctr Educ Big Data Intelligent P, Chongqing 401331, Peoples R China.
   [Ye, Yichen] Southwest Univ, Coll Elect & Informat Engn, Chongqing 400715, Peoples R China.
C3 Chongqing Normal University; Southwest University - China
RP Song, TT (corresponding author), Chongqing Normal Univ, Coll Comp & Informat Sci, Chongqing 401331, Peoples R China.
EM ttsong@cqnu.edu.cn
FU Natural Science Foundation of Chongqing [CSTB2022NSCQ-MSX1200]; Science
   and Technology Research Program of Chongqing Municipal Education
   Commission [KJQN202200537, KJZD-M202300502]; Chongqing Normal University
   Ph.D. Start-up Fund [21XLB035]
FX This research was supported in part by Natural Science Foundation of
   Chongqing (CSTB2022NSCQ-MSX1200) , in part by the Science and Technology
   Research Program of Chongqing Municipal Education Commission
   (KJQN202200537 and KJZD-M202300502) , in part by Chongqing Normal
   University Ph.D. Start-up Fund (21XLB035) .
CR Abbasi M, 2021, COMPUT COMMUN, V170, P19, DOI 10.1016/j.comcom.2021.01.021
   An HW, 2019, J AMB INTEL HUM COMP, DOI 10.1007/s12652-019-01521-w
   Ball‚ J, 2018, Arxiv, DOI arXiv:1802.01436
   Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Chang CM, 2022, MEAS CONTROL-UK, V55, P567, DOI 10.1177/00202940221115199
   Chen JR, 2023, PROC CVPR IEEE, P12021, DOI 10.1109/CVPR52729.2023.01157
   Coors B, 2018, LECT NOTES COMPUT SC, V11213, P525, DOI 10.1007/978-3-030-01240-3_32
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Dumas T, 2020, IEEE T IMAGE PROCESS, V29, P679, DOI 10.1109/TIP.2019.2934565
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Glenn J., 2020, Yolov5 release v6.1
   Glenn J., 2023, Ultralytics yolov8
   Hassani I.K., 2023, INT C LEARN REPR ICL
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou QB, 2021, PROC CVPR IEEE, P13708, DOI 10.1109/CVPR46437.2021.01350
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang SH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P844, DOI 10.1109/ICCV48922.2021.00090
   Li DW, 2022, J VIS COMMUN IMAGE R, V87, DOI 10.1016/j.jvcir.2022.103573
   Li D, 2021, PROC CVPR IEEE, P12316, DOI 10.1109/CVPR46437.2021.01214
   Li HL, 2022, Arxiv, DOI [arXiv:2206.02424, DOI 10.48550/ARXIV.2206.02424]
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Pintea SL, 2021, IEEE T IMAGE PROCESS, V30, P8342, DOI 10.1109/TIP.2021.3115001
   Qi YL, 2023, IEEE I CONF COMP VIS, P6047, DOI 10.1109/ICCV51070.2023.00558
   Qiao SY, 2021, PROC CVPR IEEE, P10208, DOI 10.1109/CVPR46437.2021.01008
   Qin JH, 2020, ECOL INFORM, V58, DOI 10.1016/j.ecoinf.2020.101093
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Romero D.W., 2022, INT C LEARN REPR ICL
   Song TT, 2023, IMAGE VISION COMPUT, V140, DOI 10.1016/j.imavis.2023.104855
   Sunkara R, 2023, LECT NOTES ARTIF INT, V13715, P443, DOI 10.1007/978-3-031-26409-2_27
   Tan M., 2019, P BRIT MACH VIS C BM
   Wang CY, 2023, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR52729.2023.00721
   Wang X, 2023, IMAGE VISION COMPUT, V135, DOI 10.1016/j.imavis.2023.104697
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xie YT, 2021, LECT NOTES COMPUT SC, V12903, P171, DOI 10.1007/978-3-030-87199-4_16
   Yang EQ, 2023, ENG APPL ARTIF INTEL, V125, DOI 10.1016/j.engappai.2023.106729
   Yang WJ, 2023, COMPUT ELECTRON AGR, V211, DOI 10.1016/j.compag.2023.108006
   Yinpeng Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11027, DOI 10.1109/CVPR42600.2020.01104
   Zhang X, 2024, Arxiv, DOI [arXiv:2304.03198, DOI 10.48550/ARXIV.2304.03198]
   Zhao Q, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1198
   Zhao SL, 2022, COMPUT ELECTRON AGR, V198, DOI 10.1016/j.compag.2022.107098
   Zhao Y., arXiv
   Zhu PF, 2022, IEEE T PATTERN ANAL, V44, P7380, DOI 10.1109/TPAMI.2021.3119563
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
NR 43
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD SEP
PY 2024
VL 149
AR 105190
DI 10.1016/j.imavis.2024.105190
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA A0B5Y
UT WOS:001279281000001
DA 2024-08-05
ER

PT J
AU Imran, M
   Akram, MU
   Tiwana, MI
   Salam, AA
   Hassan, T
   Greco, D
AF Imran, Muhammad
   Akram, Muhammad Usman
   Tiwana, Mohsin Islam
   Salam, Anum Abdul
   Hassan, Taimur
   Greco, Danilo
TI Two-dimensional hybrid incremental learning (2DHIL) framework for
   semantic segmentation of skin tissues
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE AI; Incremental semantic segmentation; Incremental learning; Continual
   learning; Knowledge distillation; Mutual distillation loss; Transformer;
   Segformer; Skin cancer; Non-melanoma skin cancer; Computational
   histopathology
AB This study aims to enhance the robustness and generalization capability of a deep learning transformer model used for segmenting skin carcinomas and tissues through the introduction of incremental learning. Deep learning AI models demonstrate their claimed performance only for tasks and data types for which they are specifically trained. Their performance is severely challenged for the test cases which are not similar to training data thus questioning their robustness and ability to generalize. Moreover, these models require an enormous amount of annotated data for training to achieve desired performance. The availability of large annotated data, particularly for medical applications, is itself a challenge. Despite efforts to alleviate this limitation through techniques like data augmentation, transfer learning, and few-shot training, the challenge persists. To address this, we propose refining the models incrementally as new classes are discovered and more data becomes available, emulating the human learning process. However, deep learning models face the challenge of catastrophic forgetting during incremental training. Therefore, we introduce a two-dimensional hybrid incremental learning framework for segmenting non-melanoma skin cancers and tissues from histopathology images. Our approach involves progressively adding new classes and introducing data of varying specifications to introduce adaptability in the models. We also employ a combination of loss functions to facilitate new learning and mitigate catastrophic forgetting. Our extended experiments demonstrate significant improvements, with an F1 score reaching 91.78, mIoU of 93.00, and an average accuracy of 95%. These findings highlight the effectiveness of our incremental learning strategy in enhancing the robustness and generalization of deep learning segmentation models while mitigating catastrophic forgetting.
C1 [Imran, Muhammad; Tiwana, Mohsin Islam] Natl Univ Sci & Technol, Dept Mechatron Engn, Islamabad, Pakistan.
   [Akram, Muhammad Usman; Salam, Anum Abdul] Natl Univ Sci & Technol, Dept Comp & Software Engn, Islamabad, Pakistan.
   [Hassan, Taimur] Abu Dhabi Univ, Dept Elect & Comp Engn, Abu Dhabi, U Arab Emirates.
   [Greco, Danilo] Politecn Milan, Dept Management Econ & Ind Engn, Via Lambruschini 24-b, I-20156 Milan, Italy.
C3 National University of Sciences & Technology - Pakistan; National
   University of Sciences & Technology - Pakistan; Abu Dhabi University;
   Polytechnic University of Milan
RP Akram, MU (corresponding author), Natl Univ Sci & Technol, Dept Comp & Software Engn, Islamabad, Pakistan.
EM usmakram@gmail.com
CR Afza F, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22030799
   Aljundi R, 2018, LECT NOTES COMPUT SC, V11207, P144, DOI 10.1007/978-3-030-01219-9_9
   Aljundi R, 2017, PROC CVPR IEEE, P7120, DOI 10.1109/CVPR.2017.753
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Belouadah E, 2019, IEEE I CONF COMP VIS, P583, DOI 10.1109/ICCV.2019.00067
   Bibi A, 2022, CMC-COMPUT MATER CON, V71, P2477, DOI 10.32604/cmc.2022.018917
   Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734
   Castro FM, 2018, LECT NOTES COMPUT SC, V11216, P241, DOI 10.1007/978-3-030-01258-8_15
   Cermelli Fabio, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9230, DOI 10.1109/CVPR42600.2020.00925
   Cermelli F, 2022, IEEE T PATTERN ANAL, V44, P10099, DOI 10.1109/TPAMI.2021.3133954
   Cha Sungmin, 2021, NeurIPS, V34, P10919
   Chaudhry A, 2018, LECT NOTES COMPUT SC, V11215, P556, DOI 10.1007/978-3-030-01252-6_33
   Chen J., 2021, TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation
   Chen L.-C., 2018, P EUR C COMP VIS ECC, P801, DOI DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2017, Arxiv, DOI [arXiv:1706.05587, 10.48550/arXiv.1706.05587, DOI 10.48550/ARXIV.1706.05587]
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Cheng B, 2021, ADV NEUR IN, V34
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dhar P, 2019, PROC CVPR IEEE, P5133, DOI 10.1109/CVPR.2019.00528
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Douillard A, 2021, PROC CVPR IEEE, P4039, DOI 10.1109/CVPR46437.2021.00403
   French RM, 1999, TRENDS COGN SCI, V3, P128, DOI 10.1016/S1364-6613(99)01294-2
   Goswami D, 2023, IEEE WINT CONF APPL, P3194, DOI 10.1109/WACV56688.2023.00321
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He WJ, 2021, INFORM FUSION, V73, P157, DOI 10.1016/j.inffus.2021.02.017
   Hesamian MH, 2019, J DIGIT IMAGING, V32, P582, DOI 10.1007/s10278-019-00227-x
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Jung HC, 2016, Arxiv, DOI arXiv:1607.00122
   Kalb T., 2022, P AS C COMP VIS, P56
   Khan AM, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22041667
   Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114
   Kleczek P, 2020, COMPUT MED IMAG GRAP, V79, DOI 10.1016/j.compmedimag.2019.101686
   Li K, 2023, IEEE T MED IMAGING, V42, P570, DOI 10.1109/TMI.2022.3211195
   Li ZZ, 2018, IEEE T PATTERN ANAL, V40, P2935, DOI 10.1109/TPAMI.2017.2773081
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lopez-Paz D, 2017, ADV NEUR IN, V30
   Maracani A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7006, DOI 10.1109/ICCV48922.2021.00694
   McCloskey M., 1989, Psychology of learning and motivation, V24, P165
   Michieli U, 2021, PROC CVPR IEEE, P1114, DOI 10.1109/CVPR46437.2021.00117
   Michieli U, 2021, COMPUT VIS IMAGE UND, V205, DOI 10.1016/j.cviu.2021.103167
   Michieli U, 2019, IEEE INT CONF COMP V, P3205, DOI 10.1109/ICCVW.2019.00400
   Mittal S, 2021, IEEE COMPUT SOC CONF, P3508, DOI 10.1109/CVPRW53098.2021.00390
   Oh Y., 2022, Adv. Neural Inform. Process. Syst., V35, P14516
   Öztürk S, 2020, J DIGIT IMAGING, V33, P958, DOI 10.1007/s10278-020-00343-z
   Paszke A, 2016, Arxiv, DOI arXiv:1606.02147
   Pereira PMM, 2020, BIOMED SIGNAL PROCES, V59, DOI 10.1016/j.bspc.2020.101924
   Qiu YQ, 2023, PATTERN RECOGN, V138, DOI 10.1016/j.patcog.2023.109383
   Rebuffi SA, 2017, PROC CVPR IEEE, P5533, DOI 10.1109/CVPR.2017.587
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shamsul Arifin M., 2012, 2012 International Conference on Machine Learning and Cybernetics (ICMLC 2012). Proceedings, P1675, DOI 10.1109/ICMLC.2012.6359626
   Shang C, 2023, PROC CVPR IEEE, P7214, DOI 10.1109/CVPR52729.2023.00697
   Sirshar M, 2021, COMPUT BIOL MED, V134, DOI 10.1016/j.compbiomed.2021.104435
   Smith J, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9354, DOI 10.1109/ICCV48922.2021.00924
   Strudel R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7242, DOI 10.1109/ICCV48922.2021.00717
   Tan MX, 2019, PR MACH LEARN RES, V97
   Thomas SM, 2021, DATA BRIEF, V39, DOI 10.1016/j.dib.2021.107587
   Thomas SM, 2021, MED IMAGE ANAL, V68, DOI 10.1016/j.media.2020.101915
   Tian Y., 2020, Contrastive representation distillation
   van de Ven GM, 2022, NAT MACH INTELL, V4, P1185, DOI 10.1038/s42256-022-00568-3
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wu HS, 2022, MED IMAGE ANAL, V76, DOI 10.1016/j.media.2021.102327
   Wu HS, 2021, IEEE T MED IMAGING, V40, P357, DOI 10.1109/TMI.2020.3027341
   Xie EZ, 2021, ADV NEUR IN, V34
   Yan SP, 2021, PROC CVPR IEEE, P3013, DOI 10.1109/CVPR46437.2021.00303
   Yang Y, 2022, PATTERN RECOGN, V122, DOI 10.1016/j.patcog.2021.108260
   Yang YZ, 2022, I C CONT AUTOMAT ROB, P299, DOI 10.1109/ICARCV57592.2022.10004288
   Yu L, 2023, IEEE T NEUR NET LEAR, V34, P9116, DOI 10.1109/TNNLS.2022.3155746
   Zhang CB, 2022, PROC CVPR IEEE, P7043, DOI 10.1109/CVPR52688.2022.00692
   Zhao DP, 2023, IEEE T PATTERN ANAL, V45, P11932, DOI 10.1109/TPAMI.2023.3273574
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhuang C, 2022, PATTERN RECOGN, V132, DOI 10.1016/j.patcog.2022.108907
NR 73
TC 0
Z9 0
U1 1
U2 1
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD AUG
PY 2024
VL 148
AR 105147
DI 10.1016/j.imavis.2024.105147
EA JUL 2024
PG 14
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA YW2S1
UT WOS:001271462100001
DA 2024-08-05
ER

PT J
AU Paulraj, S
   Vairavasundaram, S
AF Paulraj, Shalmiya
   Vairavasundaram, Subramaniyaswamy
TI M<SUP>2</SUP>VAD: Multiview multimodality transformer-based weakly
   supervised video anomaly detection
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Intelligent video surveillance; Multiview; Multimodality; Space -time
   transformer; SpectFormer
AB Video Anomaly Detection (VAD) under a weakly supervised setting involves operating with limited video-level annotations. The practical significance of this work plays a crucial role in applications related to surveillance and security. A commonly adopted strategy within existing Weakly Supervised VAD (WS-VAD) methods involves utilizing various modalities as inputs such as audio and video. This integration is motivated by the ability of these modalities to provide abundant discriminative capabilities in addressing the complexities of diverse real-world scenarios. However, the integration of multimodal data poses challenges like misalignment of temporal features and the need for efficient fusion strategies. In response to these challenges, a transformer-based deep architecture based on Multiview and Multimodality WS-VAD named M2VAD is proposed. The proposed M2VAD utilizes different views of input with varying durations or resolutions for efficient processing. The Space-Time transformer (ST Transformer) is employed for extracting visual features, while the SpectFormer is utilized for extracting the audio features. To interact and synchronize features from various modalities, a novel CrossModality Synchronization (CMS) module is proposed to ensure the coherent representation for strong detection of anomalies. To enhance the overall robustness of the proposed M2VAD, the anomaly detection module with optimization techniques is proposed to generate anomaly scores. Experimental studies conducted on benchmark datasets demonstrate the superior performance of the M2VAD, underscoring its resilience and generalization across various views and modalities. Code is available at https://github.com/Shalmiyapaulraj78/ Multiview-Multimodality-VAD.git.
C1 [Paulraj, Shalmiya] SASTRA Deemed Univ, Sch Comp, Thanjavur, India.
   [Vairavasundaram, Subramaniyaswamy] Vellore Inst Technol, Sch Comp Sci & Engn, Vellore, India.
C3 Shanmugha Arts, Science, Technology & Research Academy (SASTRA); Vellore
   Institute of Technology (VIT); VIT Vellore
RP Vairavasundaram, S (corresponding author), Vellore Inst Technol, Sch Comp Sci & Engn, Vellore, India.
EM shalmiya@cse.sastra.ac.in; subramaniyaswamy.v@vit.ac.in
CR Bao P., 2023, P AAAI C ART INT, V37, P215
   Bertasius G, 2021, PR MACH LEARN RES, V139
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chang SN, 2022, IEEE T MULTIMEDIA, V24, P4067, DOI 10.1109/TMM.2021.3112814
   Cui YT, 2022, PROC CVPR IEEE, P13598, DOI 10.1109/CVPR52688.2022.01324
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Fan HQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6804, DOI 10.1109/ICCV48922.2021.00675
   Fan YX, 2020, COMPUT VIS IMAGE UND, V195, DOI 10.1016/j.cviu.2020.102920
   Fang ZW, 2021, IEEE T MULTIMEDIA, V23, P4106, DOI 10.1109/TMM.2020.3037538
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Feichtenhofer C, 2017, PROC CVPR IEEE, P7445, DOI 10.1109/CVPR.2017.787
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Feng JC, 2021, PROC CVPR IEEE, P14004, DOI 10.1109/CVPR46437.2021.01379
   Gong Y, 2021, Arxiv, DOI arXiv:2104.01778
   Hasan M, 2016, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2016.86
   Huang C, 2024, IEEE T CYBERNETICS, V54, P3197, DOI 10.1109/TCYB.2022.3227044
   Kumari P, 2022, IEEE ACCESS, V10, P36188, DOI 10.1109/ACCESS.2022.3164439
   Lea C, 2016, LECT NOTES COMPUT SC, V9915, P47, DOI 10.1007/978-3-319-49409-8_7
   Li S, 2022, AAAI CONF ARTIF INTE, P1395
   Liu TS, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3263966
   Liu YC, 2023, Arxiv, DOI arXiv:2305.00104
   Luo WX, 2017, IEEE I CONF COMP VIS, P341, DOI 10.1109/ICCV.2017.45
   Lv H, 2021, PROC CVPR IEEE, P15420, DOI 10.1109/CVPR46437.2021.01517
   Lv H, 2021, IEEE T IMAGE PROCESS, V30, P4505, DOI 10.1109/TIP.2021.3072863
   Peng KY, 2023, IEEE T MULTIMEDIA, V25, P1489, DOI 10.1109/TMM.2023.3235300
   Peng Wu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P322, DOI 10.1007/978-3-030-58577-8_20
   Peng XG, 2024, Arxiv, DOI arXiv:2305.18797
   Sultani W, 2018, PROC CVPR IEEE, P6479, DOI 10.1109/CVPR.2018.00678
   Tian Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4955, DOI 10.1109/ICCV48922.2021.00493
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang YK, 2022, PROC CVPR IEEE, P12176, DOI 10.1109/CVPR52688.2022.01187
   Wei DL, 2022, IEEE SIGNAL PROC LET, V29, P2178, DOI 10.1109/LSP.2022.3216500
   Wu P, 2021, IEEE T IMAGE PROCESS, V30, P3513, DOI 10.1109/TIP.2021.3062192
   Xu HM, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3893, DOI 10.1145/3394171.3413581
   Yu SH, 2021, IEEE SIGNAL PROC LET, V28, P2137, DOI 10.1109/LSP.2021.3117737
   Zhang DS, 2022, IEEE SIGNAL PROC LET, V29, P1197, DOI 10.1109/LSP.2022.3175092
   Zhang JM, 2023, IEEE T INTELL TRANSP, V24, P14679, DOI 10.1109/TITS.2023.3300537
   Zhong JX, 2019, PROC CVPR IEEE, P1237, DOI 10.1109/CVPR.2019.00133
   Zhou JT, 2020, IEEE T CIRC SYST VID, V30, P4639, DOI 10.1109/TCSVT.2019.2962229
   Zhu Y, 2019, Arxiv, DOI arXiv:1907.10211
NR 40
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD SEP
PY 2024
VL 149
AR 105139
DI 10.1016/j.imavis.2024.105139
EA JUL 2024
PG 13
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA YA1V0
UT WOS:001265681300001
DA 2024-08-05
ER

PT J
AU Ji, RL
   Tekalp, AM
AF Ji, Ronglei
   Tekalp, A. Murat
TI A new multi-picture architecture for learned video deinterlacing and
   demosaicing with parallel deformable convolution and self-attention
   blocks
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Deep learning; Deinterlacing; Demosaicing; Modified deformable
   convolution; Efficient self-attention
ID IMAGE DEMOSAICKING; NETWORK; INTERPOLATION
AB Despite the fact real-world video deinterlacing and demosaicing are well-suited to supervised learning from synthetically degraded data because the degradation models are known and fixed, learned video deinterlacing and demosaicing have received much less attention compared to denoising and super-resolution tasks. We propose a new multi-picture architecture for video deinterlacing or demosaicing by aligning multiple supporting pictures with missing data to a reference picture to be reconstructed, benefiting from both local and global spatio-temporal correlations in the feature space using modified deformable convolution blocks and a novel residual efficient top -k self-attention (kSA) block, respectively. Separate reconstruction blocks are used to estimate different types of missing data. Our extensive experimental results, on synthetic or real-world datasets, demonstrate that the proposed novel architecture provides superior results that significantly exceed the state-ofthe-art for both tasks in terms of PSNR, SSIM, and perceptual quality. Ablation studies are provided to justify and show the benefit of each novel modification made to the deformable convolution and residual efficient kSA blocks.
C1 [Ji, Ronglei; Tekalp, A. Murat] Koc Univ, Dept Elect & Elect Engn, Istanbul 34450, Turkiye.
C3 Koc University
RP Tekalp, AM (corresponding author), Koc Univ, Dept Elect & Elect Engn, Istanbul 34450, Turkiye.
EM rji19@ku.edu.tr; mtekalp@ku.edu.tr
RI Ji, R/KFA-2521-2024
FU TUBITAK 2247-A Award [120C156]; Fung Foundation; KUIS AI Center -
   Turkish Is Bank
FX This work is supported by TUBITAK 2247-A Award No. 120C156 and KUIS AI
   Center funded by Turkish Is Bank. AMT acknowledges Turkish Academy of
   Sciences. RJ acknowledges support from Fung Foundation.
CR Bello I, 2019, IEEE I CONF COMP VIS, P3285, DOI 10.1109/ICCV.2019.00338
   Bernasconi M., 2020, SMPTE ANN TECHN C EX
   Bolya D, 2023, Arxiv, DOI arXiv:2210.09461
   Brar Dilpreet Singh, 2024, Sustainable Food Technology, V2, P373, DOI 10.1039/d3fb00170a
   Brar D.S., 2024, Food Human., V2
   Caballero J, 2017, PROC CVPR IEEE, P2848, DOI 10.1109/CVPR.2017.304
   Chen TY, 2022, IMAGE VISION COMPUT, V117, DOI 10.1016/j.imavis.2021.104340
   Cheng XH, 2022, IEEE T PATTERN ANAL, V44, P7029, DOI 10.1109/TPAMI.2021.3100714
   Çogalan U, 2020, IEEE T IMAGE PROCESS, V29, P7511, DOI 10.1109/TIP.2020.3004014
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Dewil V., 2023, P IEEE CVF WINT C AP, P5108
   Ehret T, 2019, IEEE I CONF COMP VIS, P8867, DOI 10.1109/ICCV.2019.00896
   Feng K, 2021, IEEE T COMPUT IMAG, V7, P864, DOI 10.1109/TCI.2021.3102052
   GAO Z., 2023, ARXIV
   Ji RL, 2022, IEEE IMAGE PROC, P901, DOI 10.1109/ICIP46576.2022.9897353
   Ji RL, 2021, IEEE I C VI COM I PR, DOI 10.1109/VCIP53242.2021.9675408
   Jo Y, 2018, PROC CVPR IEEE, P3224, DOI 10.1109/CVPR.2018.00340
   Kim W, 2007, IEEE T CONSUM ELECTR, V53, P1036, DOI 10.1109/TCE.2007.4341583
   Kokkinos F, 2019, PROC CVPR IEEE, P5922, DOI 10.1109/CVPR.2019.00608
   Kumari T., 2021, INT C COMP COMM GREE, P1
   Kwon H, 2003, IEEE T CONSUM ELECTR, V49, P198, DOI 10.1109/TCE.2003.1205477
   Li Y, 2023, PROC CVPR IEEE, P18919, DOI 10.1109/CVPR52729.2023.01814
   Li ZX, 2023, IMAGE VISION COMPUT, V140, DOI 10.1016/j.imavis.2023.104864
   Liu Y., 2021, IEEE INT C MULT EXP, P1
   Losson O, 2010, ADV IMAG ELECT PHYS, V162, P173, DOI 10.1016/S1076-5670(10)62005-8
   MSU, 2022, Msu Deinterlacer Benchmark
   Nah S, 2019, IEEE COMPUT SOC CONF, P1996, DOI 10.1109/CVPRW.2019.00251
   Sajjadi MSM, 2018, PROC CVPR IEEE, P6626, DOI 10.1109/CVPR.2018.00693
   Sharif SMA, 2021, IEEE COMPUT SOC CONF, P233, DOI 10.1109/CVPRW53098.2021.00032
   Shechtman E, 2005, IEEE T PATTERN ANAL, V27, P531, DOI 10.1109/TPAMI.2005.85
   Shen ZR, 2021, IEEE WINT CONF APPL, P3530, DOI 10.1109/WACV48630.2021.00357
   Song C., 2023, INT C NEUR INF PROC, P357
   Soomro K, 2012, Arxiv, DOI arXiv:1212.0402
   Tan DS, 2018, IEEE T IMAGE PROCESS, V27, P2408, DOI 10.1109/TIP.2018.2803341
   Tay Y, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3530811
   Tian YP, 2020, PROC CVPR IEEE, P3357, DOI 10.1109/CVPR42600.2020.00342
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang PC, 2022, LECT NOTES COMPUT SC, V13684, P285, DOI 10.1007/978-3-031-20053-3_17
   Wang XT, 2019, IEEE COMPUT SOC CONF, P1954, DOI 10.1109/CVPRW.2019.00247
   Wang Y, 2021, IEEE T CIRC SYST VID, V31, P3725, DOI 10.1109/TCSVT.2020.3040082
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xia B, 2022, AAAI CONF ARTIF INTE, P2759
   Xu X, 2020, IEEE T COMPUT IMAG, V6, P968, DOI 10.1109/TCI.2020.2999819
   Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2
   Yilmaz MA, 2021, IEEE IMAGE PROC, P1944, DOI 10.1109/ICIP42928.2021.9506210
   Ying XY, 2020, IEEE SIGNAL PROC LET, V27, P1500, DOI 10.1109/LSP.2020.3013518
   Yoo H, 2002, IEEE T CONSUM ELECTR, V48, P954, DOI 10.1109/TCE.2003.1196426
   Youku, 2019, Youku Video Enhancement and Super Resolution Dataset
   Zhang J, 2022, IMAGE VISION COMPUT, V124, DOI 10.1016/j.imavis.2022.104513
   Zhang K, 2022, IEEE T PATTERN ANAL, V44, P6360, DOI 10.1109/TPAMI.2021.3088914
   Zhang T, 2022, AAAI CONF ARTIF INTE, P3326
   Zhao Y, 2022, IEEE T IMAGE PROCESS, V31, P6282, DOI 10.1109/TIP.2022.3207003
   Zhao Y, 2022, IEEE T CIRC SYST VID, V32, P4872, DOI 10.1109/TCSVT.2021.3112548
   Zhu HC, 2017, Arxiv, DOI arXiv:1708.00187
   Zhu JW, 2022, IMAGE VISION COMPUT, V124, DOI 10.1016/j.imavis.2022.104507
NR 55
TC 0
Z9 0
U1 3
U2 3
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUN
PY 2024
VL 146
AR 105023
DI 10.1016/j.imavis.2024.105023
EA APR 2024
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA SC4C8
UT WOS:001232237000001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Boruah, M
   Das, R
AF Boruah, Minakshi
   Das, Ranjita
TI MLCapsNet plus : A multi-capsule network for the identification of the
   HIV ISs along important sequence positions
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Capsule network; Convolutional neural network; Deep neural network; DNA
   sequence; HIV; Integration sites
ID DNA INTEGRATION; SELECTION
AB The most studied sub-category of the retrovirus is the human immunodeficiency virus (HIV), which is a type of virus of the Retroviridae family. The HIV integration site (HIV IS)/ integration sites (HIV ISs) denote a crucial entity in the entire process of infection and its rebound if there is an interruption in therapy. It determines the steps involved in the formation of latent viral reserve. This work proposes a very deep neural network framework, where each of the layers of the multi-layered network is itself a neural network (NN). The attention mechanism is used for the extraction of the importance of positions in terms of an attention map. This framework will use the Capsule Network along with the attention mechanism to increase the explainability about the presence of local features. Convolutional neural networks (CNNs), which are specialized for image-based recognition and classification, have many drawbacks with one of these being its invariance to translation. This drawback has been overcome by the Capsule Networks. The proposed model also identifies the HIV ISs achieving a performance better than the State-of-the-art methods. Support Vector Machine (SVM), Random Forest (RF) and Logistic Regression (LR) classifiers have been used. Only two state-of-the-art methods are present in the literature that achieve these two goals and a comparison of this work with those two works has been provided here. This work performs way better than the state-of-the-art work in detecting the HIV IS. The comparison is presented here in terms of AUC-ROC, AUC-PR, accuracy, confusion matrix, and F beta score.
C1 [Boruah, Minakshi; Das, Ranjita] Natl Inst Technol Mizoram, Comp Sci & Engn, Aizawl 796012, Mizoram, India.
   [Das, Ranjita] Natl Inst Technol Agartala, Comp Sci & Engn, Agartala 799046, Tripura, India.
   [Boruah, Minakshi] Natl Inst Technol Mizoram, Comp Sci & Engn Dept, Aizawl 796012, Mizoram, India.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Mizoram; National Institute of Technology (NIT System);
   National Institute of Technology Agartala; National Institute of
   Technology (NIT System); National Institute of Technology Mizoram
RP Boruah, M (corresponding author), Natl Inst Technol Mizoram, Comp Sci & Engn Dept, Aizawl 796012, Mizoram, India.
EM minakshiboruahassam@gmail.com
CR Alipanahi B, 2015, NAT BIOTECHNOL, V33, P831, DOI 10.1038/nbt.3300
   Bahdanau D, 2016, Arxiv, DOI arXiv:1409.0473
   Bengio Y, 2012, Arxiv, DOI arXiv:1206.5533
   Berry C, 2006, PLOS COMPUT BIOL, V2, P1450, DOI 10.1371/journal.pcbi.0020157
   Berry CC, 2014, BIOINFORMATICS, V30, P1493, DOI 10.1093/bioinformatics/btu035
   Boruah M, 2023, NEURAL COMPUT APPL, V35, P17113, DOI 10.1007/s00521-023-08585-y
   Boruah Minakshi, 2022, INT C FRONT INT COMP, P477
   Boruah Minakshi, 2021, 2021 6 IEEE INT C RE, V6, P1
   Boruah Minakshi, 2018, Journal of Advances in Computer Engineering and Technology, V4, P219
   Boruah Minakshi, 2022, 2022 IEEE INT IOT EL, P1
   Calì C, 2015, RIC MAT, V64, P391, DOI 10.1007/s11587-015-0246-8
   Chowdhury M.S., 2024, Environmental Challenges, V14
   Davis J, 2006, Proceedings of the 23rd international conference on Machine learning, P233, DOI [10.1145/1143844.1143874, DOI 10.1145/1143844.1143874]
   Debyser Z, 2019, VIRUSES-BASEL, V11, DOI 10.3390/v11010012
   Goceri Evgin, 2020, 2020 IEEE 4th International Conference on Image Processing, Applications and Systems (IPAS), P138, DOI 10.1109/IPAS50080.2020.9334956
   Goceri E., 2021, INT C COMP GRAPH VIS, DOI DOI 10.33965/MCCSIS2021_202107L007
   Goceri E, 2020, 14 INT C COMP GRAPH, P1
   GOCERI E, 2021, IZMIR KATIP CELEBI U, V6, P91
   Goceri E, 2024, EXPERT SYST APPL, V241, DOI 10.1016/j.eswa.2023.122672
   Goceri E, 2023, BIOMED SIGNAL PROCES, V85, DOI 10.1016/j.bspc.2023.104949
   Goceri E, 2023, COMPUT BIOL MED, V152, DOI 10.1016/j.compbiomed.2022.106474
   Goceri E, 2021, 2021 44TH INTERNATIONAL CONFERENCE ON TELECOMMUNICATIONS AND SIGNAL PROCESSING (TSP), P48, DOI 10.1109/TSP52935.2021.9522605
   Göçeri E, 2020, INT CONF IMAG PROC, DOI 10.1109/ipta50016.2020.9286706
   Goceri Evgin, 2024, J. Imaging Inform. Med., P1
   Goceri Evgin, 2021, 15 INT C COMPUTER GR, P53
   Hinton GE, 2011, LECT NOTES COMPUT SC, V6791, P44, DOI 10.1007/978-3-642-21735-7_6
   Hinton Geoffrey E., 2014, SubReddit-AMA
   Hu HL, 2019, BIOINFORMATICS, V35, P1660, DOI 10.1093/bioinformatics/bty842
   Huang AS, 2021, J EXP MED, V218, DOI 10.1084/jem.20211427
   Idlahcen F, 2024, ARTIF INTELL REV, V57, DOI 10.1007/s10462-023-10666-2
   Karsoliya S., 2012, Int. J. Eng. Trends Technol, V3, P714
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Makrodimitris S, 2020, GENES-BASEL, V11, DOI 10.3390/genes11111264
   Maldarelli F, 2014, SCIENCE, V345, P179, DOI 10.1126/science.1254194
   Maldarelli F, 2016, J CLIN INVEST, V126, P438, DOI 10.1172/JCI80564
   Manshahia Mukhdeep Singh, 2022, Handbook of Intelligent Computing and Optimization for Sustainable Development
   Norel R, 2011, MOL SYST BIOL, V7, DOI 10.1038/msb.2011.70
   Olshen AB, 2004, BIOSTATISTICS, V5, P557, DOI 10.1093/biostatistics/kxh008
   Patro SC, 2019, P NATL ACAD SCI USA, V116, P25891, DOI 10.1073/pnas.1910334116
   Peipei Zhang, 2019, Journal of Physics: Conference Series, V1302, DOI 10.1088/1742-6596/1302/2/022017
   Raut Purva, 2020, Advanced computing technologies and applications. Algorithms for intelligent systems, P513
   Sabour S, 2017, ADV NEUR IN, V30
   Santoni FA, 2010, PLOS COMPUT BIOL, V6, DOI 10.1371/journal.pcbi.1001008
   Shen ZW, 2021, NEURAL NETWORKS, V141, P160, DOI 10.1016/j.neunet.2021.04.011
   Shukla A, 2020, VIRUSES-BASEL, V12, DOI 10.3390/v12050555
   Spyrakis F, 2004, J AM CHEM SOC, V126, P11764, DOI 10.1021/ja0465754
   Uzair M., 2020, 2020 IEEE 23 INT MUL, DOI [DOI 10.1109/INMIC50486.2020.9318195, 10.1109/inmic50486.2020.9318195, 10.1109/INMIC50486.2020.9318195]
   Wagner TA, 2014, SCIENCE, V345, P570, DOI 10.1126/science.1256304
   Wang GP, 2007, GENOME RES, V17, P1186, DOI 10.1101/gr.6286907
   Wong JK, 1997, SCIENCE, V278, P1291, DOI 10.1126/science.278.5341.1291
   Younis MA, 2020, ADV THER-GERMANY, V3, DOI 10.1002/adtp.202000087
   Zhou YF, 2024, INT J GREEN ENERGY, V21, P376, DOI 10.1080/15435075.2023.2196328
NR 52
TC 0
Z9 0
U1 1
U2 1
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD MAY
PY 2024
VL 145
AR 104990
DI 10.1016/j.imavis.2024.104990
EA MAR 2024
PG 9
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA QZ1R0
UT WOS:001224604300001
DA 2024-08-05
ER

PT J
AU Pang, XY
   Zheng, YL
   Nie, XS
   Yin, YL
   Li, X
AF Pang, Xiyu
   Zheng, Yanli
   Nie, Xiushan
   Yin, Yilong
   Li, Xi
TI Multi-axis interactive multidimensional attention network for vehicle
   re-identification
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Vehicle re -identification; Attention mechanism; Multi -axis interactive
ID PERSON REIDENTIFICATION
AB Learning fine-grained discriminative information is essential to address the challenges of small inter-class differences and large intra-class differences in vehicle re-identification (Re-ID). Attentional mechanism is often used to capture important global information in images rather than fine-grained discriminative information. Studies have shown that the multi-axis interaction of information can enhance the feature representation ability of networks. This paper explores how to use the multi-axis interaction of information to facilitate more effective learning of attention and how to capture important detailed information in local regions. We propose a multi-axis interactive multidimensional attention network (MIMA-Net) for vehicle Re-ID. The network allows information to interact on multiple axes and calibrates the weight distribution of features from multiple dimensions to learn subtle discriminative information in vehicle parts/regions. The window-channel attention module (W-CAM) in MIMA-Net facilitates the learning of channel attention by interacting first across locations and then across channels, while the channel group-spatial attention module (CG-SAM) facilitates the learning of spatial attention by interacting first across channels and then across locations. These two modules perform window partitioning in a priori manner and channel semantic aggregation in an adaptive manner to learn discriminative semantic features in parts, respectively. These two approaches complement each other to strengthen the feature representation ability of MIMA-Net. Extensive experiments on three large public datasets, VeRi-776, VehicleID, and VERI-Wild, verify the effectiveness of our MIMA-Net and show that our method achieves state-of-the-art performance.
C1 [Pang, Xiyu; Yin, Yilong] Shandong Univ, Sch Software, 1500 Shunhua Rd, Jinan 250101, Shan Dong, Peoples R China.
   [Pang, Xiyu; Zheng, Yanli; Li, Xi] Shandong Jiaotong Univ, Sch Informat Sci & Elect Engn, 5001 Haitang Rd, Jinan 250357, Shan Dong, Peoples R China.
   [Nie, Xiushan] Shandong Jianzhu Univ, Sch Comp Sci & Technol, 1000 Fengming Rd, Jinan 250101, Shan Dong, Peoples R China.
C3 Shandong University; Shandong Jiaotong University; Shandong Jianzhu
   University
RP Yin, YL (corresponding author), Shandong Univ, Sch Software, 1500 Shunhua Rd, Jinan 250101, Shan Dong, Peoples R China.
EM ylyin@sdu.edu.cn
FU National Natural Science Foundation of China [62176139, 61876098]; Major
   Basic Research Project of the Natural Science Foundation of Shandong
   Province [ZR2021ZD15]
FX This work is jointly supported by the National Natural Science
   Foundation of China (62176139 and 61876098) , and by the Major Basic
   Research Project of the Natural Science Foundation of Shandong Province
   (ZR2021ZD15) .
CR Almeida Eurico, 2023, 2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC), P4690, DOI 10.1109/ITSC57777.2023.10422175
   Asher Trockman J., 2023, Trans. Mach. Learn. Res., V2023
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen TL, 2019, IEEE I CONF COMP VIS, P8350, DOI 10.1109/ICCV.2019.00844
   Chen XY, 2023, IEEE T COMMUN, V71, P2475, DOI [10.1109/TCOMM.2023.3244954, 10.1109/TIM.2023.3295011]
   Chen YB, 2022, J VIS COMMUN IMAGE R, V83, DOI 10.1016/j.jvcir.2021.103432
   Chen Y, 2022, IMAGE VISION COMPUT, V128, DOI 10.1016/j.imavis.2022.104587
   Chen YC, 2023, IEEE T MULTIMEDIA, V25, P9479, DOI 10.1109/TMM.2023.3253391
   Cheng YT, 2020, INT CONF ACOUST SPEE, P1928, DOI [10.1109/icassp40776.2020.9053328, 10.1109/ICASSP40776.2020.9053328]
   Ding MY, 2022, LECT NOTES COMPUT SC, V13684, P74, DOI 10.1007/978-3-031-20053-3_5
   Dosovitskiy A., 2021, ICLR
   Ghosh A, 2023, IEEE WINT CONF APPL, P4829, DOI 10.1109/WACV56688.2023.00482
   Gu JY, 2023, PROC CVPR IEEE, P19243, DOI 10.1109/CVPR52729.2023.01844
   Guo MH, 2023, COMPUT VIS MEDIA, V9, P733, DOI 10.1007/s41095-023-0364-2
   He B, 2019, PROC CVPR IEEE, P3992, DOI 10.1109/CVPR.2019.00412
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He ST, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14993, DOI 10.1109/ICCV48922.2021.01474
   He ZJ, 2023, IEEE T VEH TECHNOL, V72, P4357, DOI 10.1109/TVT.2022.3228127
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Hou QB, 2021, PROC CVPR IEEE, P13708, DOI 10.1109/CVPR46437.2021.01350
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu ZJ, 2023, APPL INTELL, V53, P2576, DOI 10.1007/s10489-022-03192-1
   Huang FX, 2023, KNOWL-BASED SYST, V270, DOI 10.1016/j.knosys.2023.110526
   Huang MY, 2023, IEEE T IMAGE PROCESS, V32, P1568, DOI 10.1109/TIP.2023.3247159
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Huynh Su V., 2021, arXiv
   Khorramshahi Pirazh, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P369, DOI 10.1007/978-3-030-58568-6_22
   Khorramshahi P, 2019, IEEE I CONF COMP VIS, P6131, DOI 10.1109/ICCV.2019.00623
   Kim S, 2023, IEEE SIGNAL PROC LET, V30, P65, DOI 10.1109/LSP.2023.3240596
   Kumar R., 2019, IJCNN
   Lai SQ, 2021, IEEE INT CONF COMP V, P4133, DOI 10.1109/ICCVW54120.2021.00461
   Lee SR, 2023, J COMPUT DES ENG, V10, P488, DOI 10.1093/jcde/qwad014
   Li K, 2022, IEEE T NEUR NET LEAR, V33, P826, DOI 10.1109/TNNLS.2020.3029299
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Li YD, 2022, IEEE T INTELL TRANSP, V23, P1381, DOI 10.1109/TITS.2020.3025387
   Li ZW, 2023, IEEE T INTELL VEHICL, V8, P4644, DOI 10.1109/TIV.2023.3292513
   Lifang Du, 2021, ICSAI, P1
   Liu Chenghuan, 2019, IEEE IJCNN, P1, DOI DOI 10.1109/ijcnn.2019.8852350
   Liu HY, 2016, PROC CVPR IEEE, P2167, DOI 10.1109/CVPR.2016.238
   Liu K, 2020, IEEE COMPUT SOC CONF, P2494, DOI 10.1109/CVPRW50498.2020.00300
   Liu XB, 2018, IEEE INT CON MULTI
   Liu XC, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P907, DOI 10.1145/3394171.3413578
   Liu XC, 2016, IEEE INT CON MULTI
   Liu YJ, 2023, IMAGE VISION COMPUT, V140, DOI 10.1016/j.imavis.2023.104844
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Loshchilov I., 2018, INT C LEARN REPR
   Lou YH, 2019, PROC CVPR IEEE, P3230, DOI 10.1109/CVPR.2019.00335
   Lou YH, 2019, IEEE T IMAGE PROCESS, V28, P3794, DOI 10.1109/TIP.2019.2902112
   Lu YH, 2023, IMAGE VISION COMPUT, V131, DOI 10.1016/j.imavis.2023.104633
   Luo H, 2019, PATTERN RECOGN, V94, P53, DOI 10.1016/j.patcog.2019.05.028
   Meng DC, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P619, DOI 10.1145/3394171.3413573
   Meng DC, 2020, PROC CVPR IEEE, P7101, DOI 10.1109/CVPR42600.2020.00713
   Miao Y., 2022, arXiv
   Patel Y, 2022, PROC CVPR IEEE, P7492, DOI 10.1109/CVPR52688.2022.00735
   Qian JJ, 2019, Arxiv, DOI arXiv:1910.05549
   Qian JC, 2023, IEEE T VEH TECHNOL, V72, P11156, DOI 10.1109/TVT.2023.3262983
   Qin WC, 2022, IMAGE VISION COMPUT, V126, DOI 10.1016/j.imavis.2022.104551
   Quispe R, 2021, NEUROCOMPUTING, V465, P84, DOI 10.1016/j.neucom.2021.08.126
   Song LP, 2022, SIGNAL IMAGE VIDEO P, V16, P807, DOI 10.1007/s11760-021-02021-1
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tang Z, 2020, Arxiv, DOI arXiv:2005.00673
   Teng SZ, 2021, IEEE T CIRC SYST VID, V31, P816, DOI 10.1109/TCSVT.2020.2980283
   Teng SZ, 2021, INT J COMPUT VISION, V129, P719, DOI 10.1007/s11263-020-01402-2
   Tolstikhin I, 2021, ADV NEUR IN, V34
   Touvron H, 2023, IEEE T PATTERN ANAL, V45, P5314, DOI 10.1109/TPAMI.2022.3206148
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Tu JZ, 2022, PATTERN RECOGN, V131, DOI 10.1016/j.patcog.2022.108887
   Tumrani S, 2023, MULTIMEDIA SYST, V29, P1853, DOI 10.1007/s00530-023-01077-y
   Wan L, 2023, IEEE T INF FOREN SEC, V18, P3044, DOI 10.1109/TIFS.2023.3273911
   Wang PF, 2023, IEEE T MULTIMEDIA, V25, P3154, DOI 10.1109/TMM.2022.3156282
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/ISQED48828.2020.9137057, 10.1109/isqed48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang T, 2022, AAAI CONF ARTIF INTE, P2540
   Wang WH, 2022, COMPUT VIS MEDIA, V8, P415, DOI 10.1007/s41095-022-0274-8
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wang ZD, 2017, IEEE I CONF COMP VIS, P379, DOI 10.1109/ICCV.2017.49
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xi JL, 2023, PATTERN RECOGN, V134, DOI 10.1016/j.patcog.2022.109068
   Zhang C, 2022, APPL INTELL, V52, P14799, DOI 10.1007/s10489-022-03349-y
   Zhang JH, 2022, NEURAL COMPUT APPL, V34, P2953, DOI 10.1007/s00521-021-06559-6
   Zhang ZZ, 2020, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR42600.2020.00325
   Zheng ZD, 2020, IEEE COMPUT SOC CONF, P2550, DOI 10.1109/CVPRW50498.2020.00307
   Zhu JQ, 2020, IEEE T INTELL TRANSP, V21, P410, DOI 10.1109/TITS.2019.2901312
   Zhu WQ, 2023, PATTERN RECOGN, V137, DOI 10.1016/j.patcog.2022.109258
   Zhuge CR, 2020, IEEE COMPUT SOC CONF, P2632, DOI 10.1109/CVPRW50498.2020.00317
NR 84
TC 0
Z9 0
U1 2
U2 2
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD APR
PY 2024
VL 144
AR 104972
DI 10.1016/j.imavis.2024.104972
EA MAR 2024
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA OO5M0
UT WOS:001208228600001
DA 2024-08-05
ER

PT J
AU Song, LF
   Li, F
   Wang, Y
   Liu, Y
   Wang, YH
   Xiang, SM
AF Song, Lifei
   Li, Fei
   Wang, Ying
   Liu, Yu
   Wang, Yuanhua
   Xiang, Shiming
TI Image captioning: Semantic selection unit with stacked residual
   attention
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Image captioning; Semantic attributes; Semantic selection unit;
   Transformer; Stacked residual attention
ID TRANSFORMER
AB Semantic information and attention mechanism play important roles in the task of image captioning. Semantic information can strengthen the relationship between images and languages, while attention operation can steer the relevant regions spatially in the image. However, in most current works, semantic attributes are always confined to be learned from pairs of images and sentences, which ignore to fully utilize more semantic attributes and the structure information of sentences, thus limit the variety of sentences to be generated. Meanwhile, current attention models usually lack the ability to learn the positional information in an explicit way during attention generation, and have the problem of vanishing gradient in the training process. This paper proposes a Semantic Selection Unit (SSU) and a Stacked Residual Attention (SRA) to remedy these drawbacks. Specifically, the SSU is designed to capture selectively semantic information from expanding attributes or guidance sentences. With the help of expanding vocabulary and the structure information in sentences, the SSU can improve the quality of the generated sentences. The SRA is constructed to solve the problem of positional information missing and vanishing gradient problem during attention generation. Architecturally, the SSU and SRA work together in a jointed framework with end -to -end learning for image captioning. Extensive experiments have been conducted on the public dataset of the MS COCO, achieving 139.7 CIDEr score on the test set.
C1 [Song, Lifei; Wang, Ying; Xiang, Shiming] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
   [Song, Lifei; Wang, Ying; Xiang, Shiming] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Li, Fei] China Tower Corp Ltd, Beijing 100029, Peoples R China.
   [Liu, Yu; Wang, Yuanhua] Beijing Inst Tracking & Telecommun Technol, Beijing 100094, Peoples R China.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Chinese Academy of Sciences; Institute of Automation, CAS
RP Wang, Y (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
EM songlifei2018@ia.ac.cn; lifei123457@chinatowercom.cn;
   ying.wang@ia.ac.cn; liuyu2001@tsinghua.org.cn; 13717889715@139.com;
   smxiang@nlpr.ia.ac.cn
FU National Key Research and Development Program of China [2018AAA0100400];
   National Natural Science Foundation of China [62076242]
FX This research was supported by the National Key Research and Development
   Program of China under Grant No. 2018AAA0100400, and the National
   Natural Science Foundation of China under Grant 62076242.
CR Ali K, 2014, PROC CVPR IEEE, P2433, DOI 10.1109/CVPR.2014.312
   An D, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5101, DOI 10.1145/3474085.3475282
   Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387
   Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24
   Banerjee S., 2005, P ACL WORKSHOP INTRI, P65, DOI DOI 10.3115/1626355.1626389
   Brumby DP, 2008, HUM-COMPUT INTERACT, V23, P1, DOI 10.1080/07370020701851078
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Chen TY, 2022, IMAGE VISION COMPUT, V117, DOI 10.1016/j.imavis.2021.104340
   Chen X., Preprints
   Cornia Marcella, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10575, DOI 10.1109/CVPR42600.2020.01059
   Deshpande A., Preprints
   Fan ZH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6514
   Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754
   Fu K, 2017, IEEE T PATTERN ANAL, V39, P2321, DOI 10.1109/TPAMI.2016.2642953
   Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu X., PROC CVPR IEEE
   Huang J, 1997, PROC CVPR IEEE, P762, DOI 10.1109/CVPR.1997.609412
   Huang L, 2019, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2019.00473
   Ji JY, 2021, AAAI CONF ARTIF INTE, V35, P1655
   Jiang WH, 2018, LECT NOTES COMPUT SC, V11206, P510, DOI 10.1007/978-3-030-01216-8_31
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kulkarni G, 2013, IEEE T PATTERN ANAL, V35, P2891, DOI 10.1109/TPAMI.2012.162
   Li G, 2019, IEEE I CONF COMP VIS, P8927, DOI 10.1109/ICCV.2019.00902
   Li J., Preprints
   Li JN, 2022, PR MACH LEARN RES
   Li YA, 2022, PROC CVPR IEEE, P17969, DOI 10.1109/CVPR52688.2022.01746
   Li YH, 2019, PROC CVPR IEEE, P12489, DOI 10.1109/CVPR.2019.01278
   Li ZX, 2023, IMAGE VISION COMPUT, V129, DOI 10.1016/j.imavis.2022.104591
   Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345
   Luo YP, 2021, AAAI CONF ARTIF INTE, V35, P2286
   Mansimov Elman, 2016, ICLR
   Mun J, 2017, AAAI CONF ARTIF INTE, P4233
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Qin Y, 2019, PROC CVPR IEEE, P8359, DOI 10.1109/CVPR.2019.00856
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Tan Y. H., Preprints
   Nguyen VQ, 2022, LECT NOTES COMPUT SC, V13696, P167, DOI 10.1007/978-3-031-20059-5_10
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Veit A., Preprints
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang P, 2022, 39 INT C MACHINE LEA
   Wang WX, 2019, AAAI CONF ARTIF INTE, P8957
   Wang Z., 2022, ICLR VIRTUAL EVENT
   Xian TT, 2022, IEEE T CIRC SYST VID, V32, P5762, DOI 10.1109/TCSVT.2022.3155795
   Xian TT, 2022, NEURAL NETWORKS, V148, P129, DOI 10.1016/j.neunet.2022.01.011
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yao T, 2017, IEEE I CONF COMP VIS, P4904, DOI 10.1109/ICCV.2017.524
   Yingwei Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10968, DOI 10.1109/CVPR42600.2020.01098
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Zhang PC, 2021, PROC CVPR IEEE, P5575, DOI 10.1109/CVPR46437.2021.00553
   Zhang XY, 2021, PROC CVPR IEEE, P15460, DOI 10.1109/CVPR46437.2021.01521
   Zhou L., Preprints
NR 57
TC 0
Z9 0
U1 9
U2 9
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD APR
PY 2024
VL 144
AR 104965
DI 10.1016/j.imavis.2024.104965
EA MAR 2024
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA NR2G9
UT WOS:001202109600001
DA 2024-08-05
ER

PT J
AU Shaik, NS
   Cherukuri, TK
AF Shaik, Nagur Shareef
   Cherukuri, Teja Krishna
TI Gated contextual transformer network for multi-modal retinal image
   clinical description generation
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Clinical description generation; Expert-defined clinical keywords; Gated
   contextual attention; Multi -modal retinal images; Transformer network;
   Visual explanation
AB Generating semantically meaningful and coherent clinical description for the diagnosis of retinal images has been a challenging task for both Computer Vision and Natural Language Processing domains. This is mainly due to the fact that the clinical descriptions generated by the language model are completely dependent on the type of retinal image representations learned by the vision model. This work investigates and proposes a unified approach to integrate multi -modal retinal image visual representations with corresponding clinical keyword embeddings which can aid the language model to learn the clinical semantics and generate lengthy, coherent clinical descriptions accurately. Our proposed approach, named the Gated Contextual Transformer Network, comprises two attention-based encoders that learn semantically discriminative attention-based representations from retinal images and clinical keywords, along with a Transformer Network for clinical description generation. The first encoder leverages a pre-trained Convolutional Neural Network (VGG19) and a Gated Contextual Attention module to learn discriminative attention-based representations from the multi -modal retinal images. The second encoder incorporates an Embedding layer and an Attention module to learn attention-based clinical keyword embeddings. The Transformer network consists of a fusion encoder that attentively integrates retinal image visual features with clinical keyword embeddings and a decoder that is responsible for generating semantically meaningful and coherent clinical descriptions. Our experimental studies on the benchmark DeepEyeNet dataset demonstrate that the proposed approach successfully generates clinical descriptions from multimodal retinal images, meeting the standards of ophthalmologists. To support our claim, we provide qualitative and quantitative evaluations of the proposed approach. This includes reporting BLUE, CIDEr, and ROUGE scores for the predicted descriptions, as well as employing Visual Explanation for Clinical Description Generation.
C1 [Shaik, Nagur Shareef; Cherukuri, Teja Krishna] Georgia State Univ, Atlanta, GA 30302 USA.
C3 University System of Georgia; Georgia State University
RP Shaik, NS (corresponding author), Georgia State Univ, Atlanta, GA 30302 USA.
EM shaiknagurshareef6@gmail.com
RI Cherukuri, Teja Krishna/KEJ-6069-2024; Shaik, Nagur
   Shareef/ABG-8568-2021
OI Cherukuri, Teja Krishna/0000-0002-4952-728X; Shaik, Nagur
   Shareef/0000-0002-2432-7409
CR Beddiar DR, 2023, ARTIF INTELL REV, V56, P4019, DOI 10.1007/s10462-022-10270-w
   Cao Y, 2023, IEEE T PATTERN ANAL, V45, P6881, DOI 10.1109/TPAMI.2020.3047209
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Herdade S, 2019, ADV NEUR IN, V32
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang J.H., 2022, P IEEE CVF WINT C AP, P1606
   Huang JH, 2021, IEEE IMAGE PROC, P3762, DOI 10.1109/ICIP42928.2021.9506803
   Huang JH, 2021, PROCEEDINGS OF THE 2021 INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR '21), P645, DOI 10.1145/3460426.3463667
   Huang JH, 2021, IEEE WINT CONF APPL, P2441, DOI 10.1109/WACV48630.2021.00249
   Kamal Abrar Hasin, 2020, 2020 International Conference on Decision Aid Sciences and Application (DASA), P822, DOI 10.1109/DASA51403.2020.9317108
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kuo CW, 2022, PROC CVPR IEEE, P17948, DOI 10.1109/CVPR52688.2022.01744
   Lin C.-Y., 2004, ANN M ASS COMP LING, P74
   Misra I, 2020, PROC CVPR IEEE, P6706, DOI 10.1109/CVPR42600.2020.00674
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Park H, 2021, IEEE ACCESS, V9, P150560, DOI 10.1109/ACCESS.2021.3124564
   Pizzarello L, 2004, ARCH OPHTHALMOL-CHIC, V122, P615, DOI 10.1001/archopht.122.4.615
   Shaik NS, 2022, APPL INTELL, V52, P15105, DOI 10.1007/s10489-021-03043-5
   Shaik NS, 2022, COMPUT BIOL MED, V141, DOI 10.1016/j.compbiomed.2021.105127
   Shaik NS, 2021, MACH VISION APPL, V32, DOI 10.1007/s00138-021-01253-y
   Stefanini M, 2023, IEEE T PATTERN ANAL, V45, P539, DOI 10.1109/TPAMI.2022.3148210
   Tanti M, 2018, NAT LANG ENG, V24, P467, DOI 10.1017/S1351324918000098
   Vaswani A, 2017, ADV NEUR IN, V30
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Weiss Karl, 2016, Journal of Big Data, V3, DOI 10.1186/s40537-016-0043-6
   Wu TW, 2023, IEEE WINT CONF APPL, P1859, DOI 10.1109/WACV56688.2023.00190
   Yin CC, 2019, IEEE DATA MINING, P728, DOI 10.1109/ICDM.2019.00083
   Zhai XH, 2019, IEEE I CONF COMP VIS, P1476, DOI 10.1109/ICCV.2019.00156
   Zhang Y, 2021, PATTERN RECOGN LETT, V143, P43, DOI 10.1016/j.patrec.2020.12.020
NR 31
TC 0
Z9 0
U1 1
U2 1
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD MAR
PY 2024
VL 143
AR 104946
DI 10.1016/j.imavis.2024.104946
EA FEB 2024
PG 11
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA LP9N5
UT WOS:001188124600001
DA 2024-08-05
ER

PT J
AU Niu, YC
   Yin, JQ
AF Niu, Yingchun
   Yin, Jianqin
TI Weakly supervised point cloud semantic segmentation with the fusion of
   heterogeneous network features
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Weakly supervised; Point cloud; Artifical intelligence; 3D computer
   vision
AB Weakly supervised point cloud segmentation has emerged as a prominent research area to address the problem of manual annotation costs. A crucial challenge in weakly supervised point cloud segmentation is the implicit augmentation of the total amount of supervision signals. In this article, we propose a novel method that utilizes the fusion of features from different networks to enhance the supervision signals. Specifically, we utilize a deep Encoder-Decoder network to capture high-level semantic features of labeled points, while a shallow Encoder network captures multi-scale detail features of labeled data. By combining these two heterogeneous networks, we acquire richer feature representations that implicitly enhance the supervision signal.Furthermore, we introduce scene-level and instance-level contrast to enhance feature representations in both coarse-grained and finegrained manners, thus further boosting the supervisory signal. To validate the effectiveness of our approach, we conducted experiments on the large-scale indoor scene dataset, S3DIS, and the outdoor datasets, Toronto3D and Semantic3D, achieving convincing results.
C1 [Niu, Yingchun; Yin, Jianqin] Beijing Univ Posts & Telecommun, Beijing 100876, Peoples R China.
C3 Beijing University of Posts & Telecommunications
RP Yin, JQ (corresponding author), Beijing Univ Posts & Telecommun, Beijing 100876, Peoples R China.
EM jqyin@bupt.edu.cn
FU National Natural Science Foundation of China [62173045, 62273054];
   Fundamental Research Funds for the Central Universities [2020XD-A04-3];
   Natural Science Foundation of Hainan Prov- ince [622RC675]
FX This work was supported partly by the National Natural Science
   Foundation of China (Grant No. 62173045, 62273054) , partly by the
   Fundamental Research Funds for the Central Universities (Grant No.
   2020XD-A04-3) , and the Natural Science Foundation of Hainan Prov- ince
   (Grant No. 622RC675) .
CR Nguyen A, 2013, PROCEEDINGS OF THE 2013 6TH IEEE CONFERENCE ON ROBOTICS, AUTOMATION AND MECHATRONICS (RAM), P225, DOI 10.1109/RAM.2013.6758588
   Armeni I., 2017, arXiv
   Blanc T, 2020, NAT METHODS, V17, P1100, DOI 10.1038/s41592-020-0946-1
   Chen H, 2023, PATTERN RECOGN, V137, DOI 10.1016/j.patcog.2023.109307
   Chen XZ, 2017, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2017.691
   Chen YL, 2020, Arxiv, DOI arXiv:2008.06374
   Choy C, 2019, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2019.00319
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Hackel T, 2017, Arxiv, DOI arXiv:1704.03847
   Hu Q., 2021, EUROPEAN C COMPUTER
   Huang SS, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3453485
   Jaritz M, 2019, IEEE INT CONF COMP V, P3995, DOI 10.1109/ICCVW.2019.00494
   Kim S, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P528, DOI 10.1109/ICCV48922.2021.00059
   Lei H, 2021, IEEE T PATTERN ANAL, V43, P3664, DOI 10.1109/TPAMI.2020.2983410
   Li MT, 2022, PROC CVPR IEEE, P14910, DOI 10.1109/CVPR52688.2022.01451
   Li RH, 2020, PROC CVPR IEEE, P6377, DOI 10.1109/CVPR42600.2020.00641
   Li Y, 2021, IEEE T NEUR NET LEAR, V32, P3412, DOI 10.1109/TNNLS.2020.3015992
   Lin YQ, 2020, PROC CVPR IEEE, P4292, DOI 10.1109/CVPR42600.2020.00435
   Liu LZ, 2023, Arxiv, DOI arXiv:2307.10316
   Liu ZZ, 2021, PROC CVPR IEEE, P1726, DOI 10.1109/CVPR46437.2021.00177
   Mei G., 2022, BRIT MACHINE VISION
   Mildenhall B, 2020, Arxiv, DOI [arXiv:2003.08934, 10.1145/3503250, DOI 10.1145/3503250]
   Qi CR, 2017, ADV NEUR IN, V30
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Qingyong Hu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11105, DOI 10.1109/CVPR42600.2020.01112
   Rusu RB, 2009, IEEE INT CONF ROBOT, P1848
   Shin I, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8568, DOI 10.1109/ICCV48922.2021.00847
   Tan WK, 2020, IEEE COMPUT SOC CONF, P797, DOI 10.1109/CVPRW50498.2020.00109
   Tatarchenko M, 2018, PROC CVPR IEEE, P3887, DOI 10.1109/CVPR.2018.00409
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Wang LH, 2022, IEEE COMPUT SOC CONF, P1646, DOI 10.1109/CVPRW56347.2022.00171
   Wei JC, 2024, Arxiv, DOI arXiv:2107.11267
   Wei JC, 2020, PROC CVPR IEEE, P4383, DOI 10.1109/CVPR42600.2020.00444
   Wu AT, 2023, Arxiv, DOI arXiv:2301.10732
   Wu X., 2022, arXiv
   Wu YS, 2023, Arxiv, DOI arXiv:2202.10705
   Xun Xu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13703, DOI 10.1109/CVPR42600.2020.01372
   Yosinski J, 2014, ADV NEUR IN, V27
   Zhang YC, 2022, Arxiv, DOI arXiv:2212.04744
   Zhang YC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15500, DOI 10.1109/ICCV48922.2021.01523
   Zhang ZH, 2023, PROC CVPR IEEE, P17619, DOI 10.1109/CVPR52729.2023.01690
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
NR 42
TC 2
Z9 2
U1 9
U2 9
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD FEB
PY 2024
VL 142
AR 104916
DI 10.1016/j.imavis.2024.104916
EA JAN 2024
PG 13
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA JJ7U6
UT WOS:001172872700001
DA 2024-08-05
ER

PT J
AU Ghadai, C
   Patra, D
   Okade, M
AF Ghadai, Chakrapani
   Patra, Dipti
   Okade, Manish
TI A novel facial expression recognition model based on harnessing
   complementary features in multi-scale network with attention fusion
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Facial expression recognition; Muti-scale; Attention network; Feature
   complementation; Feature fusion
ID NEURAL-NETWORK
AB This paper presents a novel method for facial expression recognition using the proposed feature complementation and multi-scale attention model with attention fusion (FCMSA-AF). The proposed model consists of four main components: the shallow feature extractor module, parallel structured two-branch multi-scale attention module (MSA), feature complementing module (FCM), and attention fusion and classification module. The MSA module contains multi-scale attention modules in a cascaded fashion in two paths to learn diverse features. The upper and lower paths use left and right multi-scale blocks to extract and aggregate the features at different receptive fields. The attention networks in MSA focus on salient local regions to extract features at granular levels. The FCM uses the correlation between the feature maps in two paths to make the multi-scale attention features complementary to each other. Finally, the complementary features are fused through an attention network to form an informative holistic feature which includes subtle, visually varying regions in similar classes. Hence, complementary and informative features are used in classification to minimize information loss and capture the discriminating finer aspects of facial expression recognition. Experimental evaluation of the proposed model carried out on AffectNet and CK+ datasets achieve accuracies of 64.59% and 98.98%, respectively, outperforming some of the state-of-the-art methods.
C1 [Ghadai, Chakrapani; Patra, Dipti] Natl Inst Technol, Dept Elect Engn, Rourkela 769008, Odisha, India.
   [Okade, Manish] Natl Inst Technol, Elect & Commun Engn, Rourkela 769008, Odisha, India.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Rourkela; National Institute of Technology (NIT System);
   National Institute of Technology Rourkela
RP Patra, D (corresponding author), Natl Inst Technol, Dept Elect Engn, Rourkela 769008, Odisha, India.
EM dpatra@nitrkl.ac.in
CR Afshar S, 2016, IEEE COMPUT SOC CONF, P1517, DOI 10.1109/CVPRW.2016.189
   Altameem T, 2020, IMAGE VISION COMPUT, V103, DOI 10.1016/j.imavis.2020.104044
   de Montis IA, 2013, SALUD MENT, V36, P95, DOI 10.17711/SM.0185-3325.2013.011
   Arnaud E, 2023, IEEE T AFFECT COMPUT, V14, P2336, DOI 10.1109/TAFFC.2022.3144439
   Bisogni C, 2022, IEEE T IND INFORM, V18, P5619, DOI 10.1109/TII.2022.3141400
   Chattopadhay A, 2018, IEEE WINT CONF APPL, P839, DOI 10.1109/WACV.2018.00097
   Chattopadhyay J., 2020, New Trends in Computational Vision and Bio-inspired Computing, P1181
   Chen CF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P347, DOI 10.1109/ICCV48922.2021.00041
   Chen XJ, 2020, IEEE ACCESS, V8, P2772, DOI 10.1109/ACCESS.2019.2960769
   Corbetta M, 2002, NAT REV NEUROSCI, V3, P201, DOI 10.1038/nrn755
   Cugu I, 2019, INT CONF IMAG PROC, DOI 10.1109/ipta.2019.8936114
   Cui ZY, 2019, IEEE T GEOSCI REMOTE, V57, P8983, DOI 10.1109/TGRS.2019.2923988
   Dong YN, 2022, IEEE T IMAGE PROCESS, V31, P1559, DOI 10.1109/TIP.2022.3144017
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hua WT, 2019, IEEE ACCESS, V7, P24321, DOI 10.1109/ACCESS.2019.2900231
   Huo H, 2023, MULTIMED TOOLS APPL, V82, P18635, DOI 10.1007/s11042-022-14066-6
   Jain DK, 2023, IMAGE VISION COMPUT, V133, DOI 10.1016/j.imavis.2023.104659
   Jain DK, 2019, PATTERN RECOGN LETT, V120, P69, DOI 10.1016/j.patrec.2019.01.008
   Karnati M, 2022, IEEE T AFFECT COMPUT, V13, P2058, DOI 10.1109/TAFFC.2022.3208309
   Li YJ, 2022, IEEE T CIRC SYST VID, V32, P3178, DOI 10.1109/TCSVT.2021.3103760
   Li Y, 2019, IEEE T IMAGE PROCESS, V28, P2439, DOI 10.1109/TIP.2018.2886767
   Liu C, 2023, INFORM SCIENCES, V619, P781, DOI 10.1016/j.ins.2022.11.068
   Liu C, 2021, IEEE ACCESS, V9, P18876, DOI 10.1109/ACCESS.2021.3054332
   Liu HW, 2022, IEEE T CIRC SYST VID, V32, P6253, DOI 10.1109/TCSVT.2022.3165321
   Lucey P., 2010, 2010 IEEE COMP SOC C, P94, DOI DOI 10.1109/CVPRW.2010.5543262
   Luo ZM, 2018, INT C PATT RECOG, P3132, DOI 10.1109/ICPR.2018.8545847
   Mao JW, 2023, Arxiv, DOI [arXiv:2301.12149, DOI 10.48550/ARXIV.2301.12149]
   Minaee S, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21093046
   Mollahosseini A, 2019, IEEE T AFFECT COMPUT, V10, P18, DOI 10.1109/TAFFC.2017.2740923
   Riaz MN, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20041087
   Shao J, 2019, NEUROCOMPUTING, V355, P82, DOI 10.1016/j.neucom.2019.05.005
   She JH, 2021, PROC CVPR IEEE, P6244, DOI 10.1109/CVPR46437.2021.00618
   Song JW, 2021, IEEE IJCNN, DOI 10.1109/IJCNN52387.2021.9534004
   Su C, 2023, PATTERN ANAL APPL, V26, P543, DOI 10.1007/s10044-022-01124-w
   Sun B, 2016, J ELECTRON IMAGING, V25, DOI 10.1117/1.JEI.25.6.061407
   Sun Z, 2023, PATTERN RECOGN, V135, DOI 10.1016/j.patcog.2022.109157
   Tao AD, 2020, Arxiv, DOI [arXiv:2005.10821, DOI 10.48550/ARXIV.2005.10821]
   Wang Y, 2024, Arxiv, DOI arXiv:2209.14145
   Zhao XP, 2020, KSII T INTERNET INF, V14, P4426, DOI 10.3837/tiis.2020.11.010
   Zhao ZQ, 2021, IEEE T IMAGE PROCESS, V30, P6544, DOI 10.1109/TIP.2021.3093397
NR 42
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD SEP
PY 2024
VL 149
AR 105183
DI 10.1016/j.imavis.2024.105183
PG 9
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA ZM1T2
UT WOS:001275631300001
DA 2024-08-05
ER

PT J
AU Lee, SL
   Kang, M
   Hou, JU
AF Lee, Seung-Lee
   Kang, Minjae
   Hou, Jong-Uk
TI Localization of diffusion model-based inpainting through the inter-intra
   similarity of frequency features
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Image forensics; Image inpainting; Generative model; Diffusion model;
   Localization
ID CAMERA IDENTIFICATION
AB Recently, the enhanced abilities of diffusion models have led to more realistic inpainting results, which raises the potential for criminal activity through image forgery. In this study, we explore the detection of inpainted images generated by a diffusion model. We propose a method for inpainting localization using an inter-intra similarity (IIS) module based on image frequency features. The proposed IIS module learns the inter-patch relationship through the learnable frequency filter and subsequently covers the intra-patch relationship through the selfsimilarity operation. We provide the Diffusion Model Inpainting Dataset (DMID), a benchmark dataset comprising inpainted images using four different diffusion models and three types of masks. Additionally, a test dataset that includes three sampling steps is provided. We validated the effectiveness of our proposed approach by conducting comparative tests with existing forgery detectors using our dataset and testing the robustness of JPEG compression. Additionally, we tested our proposed method on datasets with different sampling step sizes. Our work provides a starting point for research on the detection of inpainting-based forgery using diffusion models. Additionally, by openly releasing the dataset, we offer an opportunity to advance future in-depth research related to forensics.
C1 [Lee, Seung-Lee; Kang, Minjae; Hou, Jong-Uk] Hallym Univ, Media Sch, Chunchon 24252, Kangwon Do, South Korea.
C3 Hallym University
RP Hou, JU (corresponding author), Hallym Univ, Div Software, Chunchon, South Korea.
EM juhou@hallym.ac.kr
FU National Research Foundation (NRF) of Korea - Korean Government (MSIT)
   [NRF- 2022R1A4A1033600]
FX This work was supported in part by the National Research Foundation
   (NRF) of Korea funded by the Korean Government (MSIT) (NRF-
   2022R1A4A1033600).
CR Al-Qershi OM, 2013, FORENSIC SCI INT, V231, P284, DOI 10.1016/j.forsciint.2013.05.027
   antaru D. T, 2024, P IEEECVF WINTER C A, P6258
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bammey Q, 2024, IEEE OPEN J SIGNAL P, V5, P1, DOI 10.1109/OJSP.2023.3337714
   Bayar B, 2017, IH&MMSEC'17: PROCEEDINGS OF THE 2017 ACM WORKSHOP ON INFORMATION HIDING AND MULTIMEDIA SECURITY, P147, DOI 10.1145/3082031.3083249
   Bayram S, 2005, IEEE IMAGE PROC, P2793
   Bi XL, 2019, IEEE COMPUT SOC CONF, P30, DOI 10.1109/CVPRW.2019.00010
   Chandrasegaran Keshigeyan, 2021, P IEEE CVF C COMP VI, P7200
   Chaurasia A, 2017, 2017 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)
   Chen BJ, 2021, IEEE T MULTIMEDIA, V23, P3506, DOI 10.1109/TMM.2020.3026868
   Chen L.-C., 2018, P EUR C COMP VIS ECC, P801, DOI DOI 10.1007/978-3-030-01234-2_49
   Chen YQ, 2021, AAAI CONF ARTIF INTE, V35, P1105
   Choi M, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P6860, DOI 10.1145/3503161.3548233
   Corvi Riccardo, 2023, 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P973, DOI 10.1109/CVPRW59228.2023.00104
   Cozzolino D, 2014, IEEE IMAGE PROC, P5312, DOI 10.1109/ICIP.2014.7026075
   Demir U, 2018, Arxiv, DOI arXiv:1803.07422
   Dhariwal P, 2021, ADV NEUR IN, V34
   Dong CB, 2023, IEEE T PATTERN ANAL, V45, P3539, DOI 10.1109/TPAMI.2022.3180556
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Durall R., 2020, P IEEE CVF C COMP VI, P7890
   Fan TL, 2020, IEEE ACCESS, V8, P179656, DOI 10.1109/ACCESS.2020.3025372
   Hao J, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15035, DOI 10.1109/ICCV48922.2021.01478
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He ZW, 2012, PATTERN RECOGN, V45, P4292, DOI 10.1016/j.patcog.2012.05.014
   Ho J., 2020, Adv. Neural. Inf. Process. Syst, V33, P6840, DOI DOI 10.48550/ARXIV.2006.11239
   Isola P., 2017, P IEEE C COMP VIS PA, P1125, DOI [DOI 10.1109/CVPR.2017.632, 10.1109/CVPR.2017.632]
   Jadon S, 2020, 2020 IEEE CONFERENCE ON COMPUTATIONAL INTELLIGENCE IN BIOINFORMATICS AND COMPUTATIONAL BIOLOGY (CIBCB), P115, DOI 10.1109/cibcb48159.2020.9277638
   Kim G, 2022, PROC CVPR IEEE, P2416, DOI 10.1109/CVPR52688.2022.00246
   Lam M., 2022, ICLR 2022, V10
   Li HC, 2018, Arxiv, DOI [arXiv:1805.10180, 10.48550/arXiv.1805.10180]
   Li HD, 2019, IEEE I CONF COMP VIS, P8300, DOI 10.1109/ICCV.2019.00839
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Lin X, 2023, PATTERN RECOGN, V133, DOI 10.1016/j.patcog.2022.109026
   Liu GL, 2018, LECT NOTES COMPUT SC, V11215, P89, DOI 10.1007/978-3-030-01252-6_6
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lugmayr A, 2022, PROC CVPR IEEE, P11451, DOI 10.1109/CVPR52688.2022.01117
   Lukás J, 2006, IEEE T INF FOREN SEC, V1, P205, DOI 10.1109/TIFS.2006.873602
   Luo X., 2023, P IEEE CVF INT C COM, P13243
   Luo YC, 2021, PROC CVPR IEEE, P16312, DOI 10.1109/CVPR46437.2021.01605
   Lyu S., 2012, Digital Image Forensics: There is More to a Picture Than Meets the Eye, P239
   Ma YD, 2004, PROCEEDINGS OF THE 2004 INTERNATIONAL SYMPOSIUM ON INTELLIGENT MULTIMEDIA, VIDEO AND SPEECH PROCESSING, P743
   Marra F, 2017, IEEE T INF FOREN SEC, V12, P2197, DOI 10.1109/TIFS.2017.2701335
   Nichol A, 2022, Arxiv, DOI arXiv:2112.10741
   Niloy FF, 2023, IEEE WINT CONF APPL, P4631, DOI 10.1109/WACV56688.2023.00462
   Novozámsky A, 2020, IEEE WINT CONF APPL, P71, DOI [10.1109/WACVW50321.2020.9096940, 10.1109/wacvw50321.2020.9096940]
   Oktay O, 2018, Arxiv, DOI [arXiv:1804.03999, DOI 10.48550/ARXIV.1804.03999]
   Park S., 2024, P IEEE CVF WINT C AP, P4675
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Paulus J., 2010, Proceedings of the 11th International Society for Music Information Retrieval Conference ISMIR, P625
   Rao Yongming, 2021, Advances in neural information processing systems, V34
   Rao Y, 2016, IEEE INT WORKS INFOR
   Ricker J, 2024, Arxiv, DOI arXiv:2210.14571
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Saharia C., 2022, ACM SIGGRAPH 2022 C, P1
   Schwarz K, 2021, ADV NEUR IN, V34
   Song Y, 2021, Arxiv, DOI [arXiv:2011.13456, DOI 10.48550/ARXIV.2011.13456]
   Sudre CH, 2017, LECT NOTES COMPUT SC, V10553, P240, DOI 10.1007/978-3-319-67558-9_28
   Suvorov R, 2022, IEEE WINT CONF APPL, P3172, DOI 10.1109/WACV51458.2022.00323
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Tralic Dijana, 2013, Proceedings of the 2013 55th International Symposium. ELMAR-2013, P49
   Wang Q, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P1810
   Wang XY, 2021, IETE TECH REV, V38, P149, DOI 10.1080/02564602.2020.1782274
   Wu HW, 2021, IEEE IMAGE PROC, P3867, DOI 10.1109/ICIP42928.2021.9506778
   Wu HW, 2022, IEEE T CIRC SYST VID, V32, P1172, DOI 10.1109/TCSVT.2021.3075039
   Yang C, 2017, PROC CVPR IEEE, P4076, DOI 10.1109/CVPR.2017.434
   Zhang ZX, 2018, IEEE GEOSCI REMOTE S, V15, P749, DOI 10.1109/LGRS.2018.2802944
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhou ZW, 2018, LECT NOTES COMPUT SC, V11045, P3, DOI 10.1007/978-3-030-00889-5_1
NR 70
TC 0
Z9 0
U1 2
U2 2
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD AUG
PY 2024
VL 148
AR 105138
DI 10.1016/j.imavis.2024.105138
EA JUN 2024
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA XL9V7
UT WOS:001261966100001
DA 2024-08-05
ER

PT J
AU Qiu, JH
   Liu, WH
   Lin, CC
   Li, JJ
   Yu, HP
   Boumaraf, S
AF Qiu, Jianhua
   Liu, Weihua
   Lin, Chaochao
   Li, Jiaojiao
   Yu, Haoping
   Boumaraf, Said
TI Occlusion-aware deep convolutional neural network via homogeneous
   Tanh-transforms for face parsing
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Face parsing; Face occlusion; Convolutional neural networks
AB Face parsing infers a pixel-wise label map for each semantic facial component. Previous methods generally work well for uncovered faces, however, they overlook facial occlusion and ignore some contextual areas outside a single face, especially when facial occlusion has become a common situation during the COVID-19 epidemic. Inspired by the lighting phenomena in everyday life, where illumination from four distinct lamps provides a more uniform distribution than a single central light source, we propose a novel homogeneous tanh-transform for image preprocessing, which is made up of four tanh-transforms. These transforms fuse the central vision and the peripheral vision together. Our proposed method addresses the dilemma of face parsing under occlusion and compresses more information from the surrounding context. Based on homogeneous tanh-transforms, we propose an occlusion-aware convolutional neural network for occluded face parsing. It combines information in both Tanh-polar space and Tanh-Cartesian space, capable of enhancing receptive fields. Furthermore, we introduce an occlusion-aware loss to focus on the boundaries of occluded regions. The network is simple, flexible, and can be trained end-to-end. To facilitate future research of occluded face parsing, we also contribute a new cleaned face parsing dataset. This dataset is manually purified from several academic or industrial datasets, including CelebAMask-HQ, Short-video Face Parsing, and the Helen dataset, and will be made public. Experiments demonstrate that our method surpasses state-of-the-art methods in face parsing under occlusion.
C1 [Qiu, Jianhua] Hunan Univ, 116 Lushan South Rd, Changsha 410082, Hunan, Peoples R China.
   [Liu, Weihua; Lin, Chaochao; Li, Jiaojiao] AthenaEyesCO LTD, Bldg 14,Zhongdian Software Pk,39 Jianshan Rd, Changsha 410205, Hunan, Peoples R China.
   [Yu, Haoping] Johns Hopkins Univ, 3400 N Charles St, Baltimore, MD 21218 USA.
   [Boumaraf, Said] Khalifa Univ Sci & Technol, Abu Dhabi 127788, U Arab Emirates.
C3 Hunan University; Johns Hopkins University; Khalifa University of
   Science & Technology
RP Liu, WH (corresponding author), AthenaEyesCO LTD, Bldg 14,Zhongdian Software Pk,39 Jianshan Rd, Changsha 410205, Hunan, Peoples R China.
EM qiujianhua@hnu.edu.cn; liuweihua@a-eye.cn; linchaochao@a-eye.cn;
   lijiaojiao@a-eye.cn; hyu90@jh.edu; said.boumaraf@ku.ac.ae
FU Changsha Major Science and Technology Special Project [hk2003001]
FX This work was supported in part by the Changsha Major Science and
   Technology Special Project under Grant NO. hk2003001.
CR Albalas F, 2022, IEEE ACCESS, V10, P35162, DOI 10.1109/ACCESS.2022.3163565
   Bitouk D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360638
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen YT, 2015, PROC CVPR IEEE, P3470, DOI 10.1109/CVPR.2015.7298969
   De Clerck HJ, 2015, AM J ORTHOD DENTOFAC, V148, P37, DOI 10.1016/j.ajodo.2015.04.017
   Fu HZ, 2018, IEEE T MED IMAGING, V37, P1597, DOI 10.1109/TMI.2018.2791488
   Gao TS, 2011, PROC CVPR IEEE, P1361, DOI 10.1109/CVPR.2011.5995623
   Ghiasi G, 2014, PROC CVPR IEEE, P2401, DOI 10.1109/CVPR.2014.308
   Han S, 2023, 2023 20TH INTERNATIONAL CONFERENCE ON UBIQUITOUS ROBOTS, UR, P13, DOI 10.1109/UR57808.2023.10202537
   Hariharan B, 2014, LECT NOTES COMPUT SC, V8695, P297, DOI 10.1007/978-3-319-10584-0_20
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang PL, 2023, IEEE T NEUR NET LEAR, V34, P1439, DOI 10.1109/TNNLS.2021.3105386
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jiang RQ, 2019, IEEE IMAGE PROC, P355, DOI [10.1109/ICIP.2019.8802940, 10.1109/icip.2019.8802940]
   Ke L, 2021, PROC CVPR IEEE, P4018, DOI 10.1109/CVPR46437.2021.00401
   Kim BS, 2018, ELECTRON LETT, V54, P1321, DOI 10.1049/el.2018.5051
   Kim H, 2022, MEASUREMENT, V191, DOI 10.1016/j.measurement.2022.110807
   Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lee CH, 2020, PROC CVPR IEEE, P5548, DOI 10.1109/CVPR42600.2020.00559
   Li L, 2023, COMPUT GRAPH-UK, V116, P185, DOI 10.1016/j.cag.2023.08.003
   Lin JP, 2019, PROC CVPR IEEE, P5637, DOI 10.1109/CVPR.2019.00580
   Lin YM, 2021, IMAGE VISION COMPUT, V112, DOI 10.1016/j.imavis.2021.104190
   Liu SF, 2017, Arxiv, DOI arXiv:1708.01936
   Liu SF, 2015, PROC CVPR IEEE, P3451, DOI 10.1109/CVPR.2015.7298967
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Luo P, 2012, PROC CVPR IEEE, P2480, DOI 10.1109/CVPR.2012.6247963
   maadaa.ai, 2021, Short-video face parsing challenge cvpr 2021
   Masi I, 2020, PROC CVPR IEEE, P5507, DOI 10.1109/CVPR42600.2020.00555
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178
   Salehinejad H, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P3016, DOI 10.1109/ICASSP.2018.8462241
   Selfa T, 2005, AGR HUM VALUES, V22, P451, DOI 10.1007/s10460-005-3401-0
   Smith BM, 2013, PROC CVPR IEEE, P3484, DOI 10.1109/CVPR.2013.447
   Tai KS, 2019, PR MACH LEARN RES, V97
   Wang CY, 2020, FRONT PSYCHIATRY, V11, DOI 10.3389/fpsyt.2020.569981
   Warrell J, 2009, IEEE IMAGE PROC, P2481, DOI 10.1109/ICIP.2009.5413918
   Yin Z, 2021, COGN NEURODYNAMICS, V15, P169, DOI 10.1007/s11571-020-09615-4
   Zhao W, 2003, ACM COMPUT SURV, V35, P399, DOI 10.1145/954339.954342
   Zheng WB, 2020, NEUROCOMPUTING, V376, P25, DOI 10.1016/j.neucom.2019.09.045
   Zhou L, 2017, Arxiv, DOI arXiv:1708.03736
   Zhou YS, 2015, LECT NOTES COMPUT SC, V9377, P222, DOI 10.1007/978-3-319-25393-0_25
NR 42
TC 0
Z9 0
U1 1
U2 1
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD AUG
PY 2024
VL 148
AR 105120
DI 10.1016/j.imavis.2024.105120
EA JUN 2024
PG 10
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA WZ8V1
UT WOS:001258798500001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Li, QT
   Ma, Y
   Huang, J
   Zhang, C
   Cai, Z
AF Li, Qintong
   Ma, Yong
   Huang, Jun
   Zhang, Can
   Cai, Zhao
TI LELD: Learn enhancement by learning degradation
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Low -light image enhancement; Degradation network; Generative
   adversarial network; Retinex theory
ID HISTOGRAM EQUALIZATION; IMAGE
AB Enhancing low-light images improves both the visibility and quality of the images. Existing methods primarily focus on the enhancement process and heavily rely on the supervised learning strategy, where low/normal-light image pairs are used as the training dataset. In this paper, we propose a novel method called Learn Enhancement by Learning Degradation (LELD) to achieve efficient light adjustment and scene fidelity. We use a carefully designed degradation network (DNet) to guide the enhancement network (ENet). Specifically, the role of DNet is transforming normal-light images into low-light images. For better generalization ability, we employ an unsupervised learning strategy and a generative adversarial network framework. The training is totally dependent on unpaired datasets. Inspired by Retinex theory, we propose a fidelity loss to maintain color and detail during the degradation process. The ENet exhibits a straightforward architecture and achieves efficient enhancement. Experimental results demonstrate the advantages of our method over state-of-the-art methods in terms of visual quality and enhancement efficiency.
C1 [Li, Qintong; Ma, Yong; Huang, Jun; Zhang, Can; Cai, Zhao] Wuhan Univ, Wuhan 430072, Peoples R China.
C3 Wuhan University
RP Ma, Y (corresponding author), Wuhan Univ, Wuhan 430072, Peoples R China.
EM liqintong@whu.edu.cn; mayong@whu.edu.cn; junhwong@whu.edu.cn;
   zhangcan@whu.edu.cn; zhao_c_2020@whu.edu.cn
RI cai, zhao/KFQ-0819-2024
OI cai, zhao/0009-0002-0794-8075
FU National Natural Science Foundation of China [62075169, U23B2050];
   Industry -University-Research Cooperation Program of Zhuhai
   [2220004002828]
FX This work was supported by the National Natural Science Foundation of
   China (No. 62075169 and U23B2050) and the Industry -University-Research
   Cooperation Program of Zhuhai (No. 2220004002828) .
CR Cai JR, 2018, IEEE T IMAGE PROCESS, V27, P2049, DOI 10.1109/TIP.2018.2794218
   Chen C, 2018, PROC CVPR IEEE, P3291, DOI 10.1109/CVPR.2018.00347
   Dang-Nguyen D.-T., 2015, P 6 ACM MULT SYST C, P219
   Economopoulos TL, 2010, IMAGE VISION COMPUT, V28, P45, DOI 10.1016/j.imavis.2009.04.011
   Fu ZQ, 2023, PROC CVPR IEEE, P22252, DOI 10.1109/CVPR52729.2023.02131
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Guo YB, 2023, NEURAL NETWORKS, V165, P491, DOI 10.1016/j.neunet.2023.05.052
   Guo YB, 2024, IEEE T NEUR NET LEAR, V35, P8241, DOI 10.1109/TNNLS.2022.3226301
   Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462
   Kang B, 2011, IMAGE VISION COMPUT, V29, P557, DOI 10.1016/j.imavis.2011.06.001
   LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li CY, 2022, IEEE T PATTERN ANAL, V44, P9396, DOI 10.1109/TPAMI.2021.3126387
   Li JJ, 2021, IEEE T CIRC SYST VID, V31, P4227, DOI 10.1109/TCSVT.2021.3049940
   Li MD, 2018, IEEE T IMAGE PROCESS, V27, P2828, DOI 10.1109/TIP.2018.2810539
   Li WH, 2023, PATTERN RECOGN, V136, DOI 10.1016/j.patcog.2022.109234
   Liu JY, 2021, INT J COMPUT VISION, V129, P1153, DOI 10.1007/s11263-020-01418-8
   Loh YP, 2019, COMPUT VIS IMAGE UND, V178, P30, DOI 10.1016/j.cviu.2018.10.010
   Ma L, 2022, PROC CVPR IEEE, P5627, DOI 10.1109/CVPR52688.2022.00555
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Ren WQ, 2019, IEEE T IMAGE PROCESS, V28, P4364, DOI 10.1109/TIP.2019.2910412
   Sekeroglu B, 2023, IMAGE VISION COMPUT, V138, DOI 10.1016/j.imavis.2023.104810
   Shin J, 2020, IEEE T MULTIMEDIA, V22, P30, DOI 10.1109/TMM.2019.2922127
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wang WC, 2020, IEEE ACCESS, V8, P87884, DOI 10.1109/ACCESS.2020.2992749
   Wang X., 2023, IEEE Transactions on Neural Networks and Learning Systems
   Wang Y, 1999, IEEE T CONSUM ELECTR, V45, P68, DOI 10.1109/30.754419
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei Cui, 2018, 2018 Photonics North (PN), DOI 10.1109/PN.2018.8438843
   Wu Kejun, 2023, IEEE Transactions on Multimedia, P1
   Yadav G, 2023, IMAGE VISION COMPUT, V135, DOI 10.1016/j.imavis.2023.104693
   Yang SL, 2023, IEEE T COMPUT IMAG, V9, P29, DOI 10.1109/TCI.2023.3240087
   Yang WH, 2021, IEEE T IMAGE PROCESS, V30, P2072, DOI 10.1109/TIP.2021.3050850
   Zhang YH, 2021, INT J COMPUT VISION, V129, P1013, DOI 10.1007/s11263-020-01407-x
   Zhang YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1632, DOI 10.1145/3343031.3350926
   Zhao ZJ, 2022, IEEE T CIRC SYST VID, V32, P1076, DOI 10.1109/TCSVT.2021.3073371
NR 39
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD AUG
PY 2024
VL 148
AR 105102
DI 10.1016/j.imavis.2024.105102
EA JUN 2024
PG 10
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA WG1H1
UT WOS:001253619500001
DA 2024-08-05
ER

PT J
AU Shin, AH
   Lee, JH
   Hwang, J
   Kim, Y
   Park, GM
AF Shin, Ah-Hyung
   Lee, Jae-Ho
   Hwang, Jiwon
   Kim, Yoonhyung
   Park, Gyeong-Moon
TI Wav2NeRF: Audio-driven realistic talking head generation via
   wavelet-based NeRF
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Talking head generation; Neural radiance fields; Cross -modal
   generation; Audio-visual; Wavelet transform
AB Talking head generation is an essential task in various real -world applications such as film making and virtual reality. To this end, recent works focus on the NeRF-based methods that can capture the 3D structural information of faces and generate more natural and vivid talking videos. However, the existing NeRF-based methods fail to accurately generate the audio -synced videos. In this paper, we point out that the previous methods do not consider the audio-visual representations explicitly, which is crucial for precise lip synchronization. Moreover, the existing methods struggle to generate high -frequency details, making the generation results unnatural. To overcome these problems, we propose a novel audio -synced and high-fidelity NeRF-based talking head generation framework, named Wav2NeRF, which learns audio-visual cross -modality representations and employs the wavelet transform for better visual quality. In precise, we adopt a 2D CNN -based neural rendering decoder to a NeRF-based encoder for fast generation of the whole image to employ a new multi -level SyncNet loss for accurate lip synchronization. We also propose a novel cross -attention module to effectively fuse the image and the audio representation. In addition, we integrate the wavelet transform into our framework by proposing the wavelet loss function to enhance high -frequency details. We demonstrate that the proposed method renders realistic and audio -synced talking head videos and shows outstanding performances on average in 4 representative metrics, including PSNR (+4.7%), SSIM (+ 2.2%), LMD (+51.3%), and SyncNet Confidence (+ 154.7%) compared to the NeRF-based current state-of-the-art methods.
C1 [Shin, Ah-Hyung; Lee, Jae-Ho; Hwang, Jiwon; Park, Gyeong-Moon] Kyung Hee Univ, Yongin, South Korea.
   [Kim, Yoonhyung] Elect & Telecommun Res Inst ETRI, Daejeon, South Korea.
C3 Kyung Hee University; Electronics & Telecommunications Research
   Institute - Korea (ETRI)
RP Park, GM (corresponding author), Kyung Hee Univ, Yongin, South Korea.
EM gmpark@khu.ac.kr
FU Institute of Information & Communications Technology Planning &
   Evaluation (IITP) through the Korea Government (MSIT) , Development of
   Semi-Supervised Learning Language Intelligence Technology and Korean
   Tutoring Service for Foreigners [2019-0-00004]
FX This work was supported by the Institute of Information & Communications
   Technology Planning & Evaluation (IITP) Grant through the Korea
   Government (MSIT) , Development of Semi-Supervised Learning Language
   Intelligence Technology and Korean Tutoring Service for Foreigners,
   under Grant 2019-0-00004.
CR Amodei D, 2016, PR MACH LEARN RES, V48
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Bulat A, 2017, IEEE I CONF COMP VIS, P1021, DOI 10.1109/ICCV.2017.116
   Chen LL, 2019, PROC CVPR IEEE, P7824, DOI 10.1109/CVPR.2019.00802
   Chen LL, 2018, LECT NOTES COMPUT SC, V11211, P538, DOI 10.1007/978-3-030-01234-2_32
   Chung JS, 2017, LECT NOTES COMPUT SC, V10117, P251, DOI 10.1007/978-3-319-54427-4_19
   Gal R, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459836
   Gao J, 2020, NEURAL COMPUT, V32, P829, DOI 10.1162/neco_a_01273
   Gao X, 2016, IEEE IMAGE PROC, P1439, DOI 10.1109/ICIP.2016.7532596
   Guo YD, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5764, DOI 10.1109/ICCV48922.2021.00573
   Haar A, 1910, MATH ANN, V69, P331, DOI 10.1007/BF01456326
   Hong FT, 2022, PROC CVPR IEEE, P3387, DOI 10.1109/CVPR52688.2022.00339
   Hong Y, 2022, PROC CVPR IEEE, P20342, DOI 10.1109/CVPR52688.2022.01973
   Hori C, 2017, IEEE I CONF COMP VIS, P4203, DOI 10.1109/ICCV.2017.450
   Huang HB, 2017, IEEE I CONF COMP VIS, P1698, DOI 10.1109/ICCV.2017.187
   Hui KH, 2022, PROCEEDINGS SIGGRAPH ASIA 2022, DOI 10.1145/3550469.3555394
   Ji XY, 2021, PROC CVPR IEEE, P14075, DOI 10.1109/CVPR46437.2021.01386
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kingma D. P., 2014, arXiv
   Lee CH, 2020, PROC CVPR IEEE, P5548, DOI 10.1109/CVPR42600.2020.00559
   Lele Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P35, DOI 10.1007/978-3-030-58545-7_3
   Lin Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P86, DOI 10.1007/978-3-030-58601-0_6
   Liu PJ, 2019, IEEE ACCESS, V7, P74973, DOI 10.1109/ACCESS.2019.2921451
   Liu X, 2022, LECT NOTES COMPUT SC, V13697, P106, DOI 10.1007/978-3-031-19836-6_7
   Lu YX, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480484
   Tran L, 2018, PROC CVPR IEEE, P7346, DOI 10.1109/CVPR.2018.00767
   Meshry M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13809, DOI 10.1109/ICCV48922.2021.01357
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Moon S-J, 2022, ARXIV
   Paszke A, 2019, ADV NEUR IN, V32
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Prajwal KR, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P484, DOI 10.1145/3394171.3413532
   Richardson E, 2017, PROC CVPR IEEE, P5553, DOI 10.1109/CVPR.2017.589
   Shao RZ, 2022, PROC CVPR IEEE, P15851, DOI 10.1109/CVPR52688.2022.01541
   Shen S, 2022, LECT NOTES COMPUT SC, V13672, P666, DOI [10.1007/978-3-031-19775-8_39, 10.1007/978-3-031-19775-839]
   Stankovic RS, 2003, COMPUT ELECTR ENG, V29, P25, DOI 10.1016/S0045-7906(01)00011-8
   Sun JX, 2022, PROC CVPR IEEE, P7662, DOI 10.1109/CVPR52688.2022.00752
   Suwajanakorn S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073640
   Tancik M., 2020, ADV NEURAL INFORM PR, V33, P7537, DOI DOI 10.48550/ARXIV.2006.10739
   Taylor S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073699
   Thies Justus, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P716, DOI 10.1007/978-3-030-58517-4_42
   Tretschk E, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12939, DOI 10.1109/ICCV48922.2021.01272
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang J., 2020, COMPUTER VISION ECCV, P405
   Wang Peng, 2021, NeurIPS
   Wang TC, 2021, PROC CVPR IEEE, P10034, DOI 10.1109/CVPR46437.2021.00991
   Wang Y, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3408317
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZY, 2021, PROC CVPR IEEE, P5700, DOI 10.1109/CVPR46437.2021.00565
   Wiles O, 2018, LECT NOTES COMPUT SC, V11217, P690, DOI 10.1007/978-3-030-01261-8_41
   Xu TH, 2022, PROC CVPR IEEE, P15862, DOI 10.1109/CVPR52688.2022.01542
   Yoo J, 2019, IEEE I CONF COMP VIS, P9035, DOI 10.1109/ICCV.2019.00913
   Zakharov E, 2019, IEEE I CONF COMP VIS, P9458, DOI 10.1109/ICCV.2019.00955
   Zhang LF, 2022, PROC CVPR IEEE, P12454, DOI 10.1109/CVPR52688.2022.01214
   Zhou H, 2021, PROC CVPR IEEE, P4174, DOI 10.1109/CVPR46437.2021.00416
   Zhou H, 2019, AAAI CONF ARTIF INTE, P9299
   Zhou Y, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417774
NR 57
TC 0
Z9 0
U1 2
U2 2
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD AUG
PY 2024
VL 148
AR 105104
DI 10.1016/j.imavis.2024.105104
EA JUN 2024
PG 14
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA XT0T2
UT WOS:001263818300001
OA hybrid
DA 2024-08-05
ER

PT J
AU Panigrahi, U
   Sahoo, PK
   Panda, MK
   Panda, G
AF Panigrahi, Upasana
   Sahoo, Prabodh Kumar
   Panda, Manoj Kumar
   Panda, Ganapati
TI A ResNet-101 deep learning framework induced transfer learning strategy
   for moving object detection
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Background subtraction; Deep learning architecture; Transfer learning;
   Feature pooling framework; Contrast normalization
ID BACKGROUND SUBTRACTION; SEGMENTATION; NETWORK; SURVEILLANCE; MOTION
AB Background subtraction is a crucial stage in many visual surveillance systems. The prime objective of any such system is to detect moving objects such that the system could be utilized to face many real-time challenges. In the last few decades, various methods have been developed to detect moving objects. However, the performance of many existing methods needs further improvement for slow, moderate, and fast-moving object detection in videos simultaneously and also for unseen video setups. In this article, a noteworthy effort is made to detect moving objects in complex videos by harnessing the potential of an encoder-decoder-type deep framework, employing a customized ResNet-101 model along with a feature pooling framework (FPF). The proposed algorithm has four-fold innovations including: A pre-trained modified ResNet-101 network with a transfer learning technique is proposed as an encoder to learn the challenging video scene adequately. The proposed encoder network employs a total of twenty three numbers of layers with skip connections making the model less complex. In between the encoder and decoder framework, the FPF module is used that combines a max-pooling layer, a convolutional layer, and multiple convolutional layers with varying sampling rates. This FPM module can preserve multi-scale and multi-dimensional features across different levels accurately. A decoder architecture consisting of stacked convolution layers is implemented to transform the features into image space efficiently. The efficiency of the proposed scheme is corroborated using subjective and objective analysis. The efficiency of the developed model is highlighted through a comparison with thirty-three existing methods, effectively illustrating its superior efficacy.
C1 [Panigrahi, Upasana; Panda, Ganapati] CV Raman Global Univ, Dept Elect & Commun Engn, Bhubaneswar 752054, Odisha, India.
   [Sahoo, Prabodh Kumar] Parul Univ, Parul Inst Technol, Dept Mechatron Engn, Vadodara 391760, Gujarat, India.
   [Panda, Manoj Kumar] GIET Univ, Dept Elect & Commun Engn, Rayagada 765022, Orissa, India.
C3 Parul University; GIET University
RP Sahoo, PK (corresponding author), Parul Univ, Parul Inst Technol, Dept Mechatron Engn, Vadodara 391760, Gujarat, India.
EM sahooprabodhkumar@gmail.com; manojkumarpanda@giet.edu
RI Panda, Manoj Kumar/HVU-4540-2023
OI Panda, Manoj Kumar/0009-0002-5021-2741
CR Abdullahi SB, 2023, BRAIN SCI, V13, DOI 10.3390/brainsci13040555
   Allebosch G, 2016, COMM COM INF SC, V598, P433, DOI 10.1007/978-3-319-29971-6_23
   An YQ, 2023, PROC CVPR IEEE, P6355, DOI 10.1109/CVPR52729.2023.00615
   Babaee M, 2018, PATTERN RECOGN, V76, P635, DOI 10.1016/j.patcog.2017.09.040
   Barnich O, 2011, IEEE T IMAGE PROCESS, V20, P1709, DOI 10.1109/TIP.2010.2101613
   Bianco S, 2017, IEEE T EVOLUT COMPUT, V21, P914, DOI 10.1109/TEVC.2017.2694160
   Bouwmans T, 2019, NEURAL NETWORKS, V117, P8, DOI 10.1016/j.neunet.2019.04.024
   Bouwmans T, 2014, COMPUT SCI REV, V11-12, P31, DOI 10.1016/j.cosrev.2014.04.001
   Braham M, 2017, IEEE IMAGE PROC, P4552, DOI 10.1109/ICIP.2017.8297144
   Choudhury SK, 2016, IEEE ACCESS, V4, P6133, DOI 10.1109/ACCESS.2016.2608847
   Cioppa A, 2020, IEEE IMAGE PROC, P3214, DOI [10.1109/ICIP40778.2020.9190838, 10.1109/icip40778.2020.9190838]
   Cucchiara R, 2003, IEEE T PATTERN ANAL, V25, P1337, DOI 10.1109/TPAMI.2003.1233909
   De Gregorio Massimo, 2017, P EUR S ART NEUR NET
   Dollár P, 2012, IEEE T PATTERN ANAL, V34, P743, DOI 10.1109/TPAMI.2011.155
   Dou JF, 2017, SIGNAL IMAGE VIDEO P, V11, P407, DOI 10.1007/s11760-016-0975-5
   DUNCAN JH, 1992, IEEE T PATTERN ANAL, V14, P346, DOI 10.1109/34.120329
   Fan L, 2021, EXPERT SYST APPL, V170, DOI 10.1016/j.eswa.2020.114544
   Fisher RB, 2016, INTEL SYST REF LIBR, V104, P1, DOI 10.1007/978-3-319-30208-9
   Gracewell J, 2020, MULTIMED TOOLS APPL, V79, P4639, DOI 10.1007/s11042-019-7411-0
   Guo JJ, 2017, IOP CONF SER-MAT SCI, V242, DOI 10.1088/1757-899X/242/1/012115
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hsieh JW, 2006, IEEE T INTELL TRANSP, V7, P175, DOI 10.1109/TITS.2006.874722
   Hu WM, 2004, IEEE T SYST MAN CY C, V34, P334, DOI 10.1109/TSMCC.2004.829274
   Hu ZH, 2018, IEEE ACCESS, V6, P43450, DOI 10.1109/ACCESS.2018.2861223
   Huang JJ, 2019, CHIN CONT DECIS CONF, P5272, DOI [10.1109/CCDC.2019.8833206, 10.1109/ccdc.2019.8833206]
   Huang XS, 2012, AASRI PROC, V1, P492, DOI 10.1016/j.aasri.2012.06.077
   Isik S, 2019, IET COMPUT VIS, V13, P719, DOI 10.1049/iet-cvi.2018.5642
   Isik S, 2018, J ELECTRON IMAGING, V27, DOI 10.1117/1.JEI.27.2.023002
   Jiang SQ, 2018, IEEE T CIRC SYST VID, V28, P2105, DOI 10.1109/TCSVT.2017.2711659
   KaewTraKulPong P, 2002, VIDEO-BASED SURVEILLANCE SYSTEMS: COMPUTER VISION AND DISTRIBUTED PROCESSING, P135
   Kalsotra R, 2019, IEEE ACCESS, V7, P59143, DOI 10.1109/ACCESS.2019.2914961
   Kanungo P, 2017, 2 INT C MAN MACHINE
   Kebir Abdeldjalil, 2022, Int. J. Electric. Comput. Eng., V12
   Law H, 2020, INT J COMPUT VISION, V128, P642, DOI 10.1007/s11263-019-01204-1
   Lee SH, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11050621
   Lin T.-Y., 2017, PROC CVPR IEEE, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu WB, 2017, NEUROCOMPUTING, V234, P11, DOI 10.1016/j.neucom.2016.12.038
   López-Rubio E, 2018, INT J NEURAL SYST, V28, DOI 10.1142/S0129065717500563
   Mandal M, 2022, IEEE T INTELL TRANSP, V23, P2031, DOI 10.1109/TITS.2020.3030801
   Martins I, 2017, LECT NOTES COMPUT SC, V10255, P50, DOI 10.1007/978-3-319-58838-4_6
   Mondejar-Guerra V., 2019, British Machine Vision Conference, P266
   Montgomery Christopher, 2004, Xiph.org video test media
   Panda MK, 2022, 2022 18TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS 2022), DOI 10.1109/AVSS56176.2022.9959141
   Panda MK, 2022, COMPUT VIS IMAGE UND, V222, DOI 10.1016/j.cviu.2022.103501
   Panda Manoj Kumar, 2023, IEEE Trans. Artif. Intell., P1
   Poppe R, 2010, IMAGE VISION COMPUT, V28, P976, DOI 10.1016/j.imavis.2009.11.014
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Reisslein M., 2000, Yuv video sequences
   Ren Shaoqing, 2016, Faster r-cnn: Towards realtime object detection with region proposal networks
   Rout DK, 2018, EXPERT SYST APPL, V97, P117, DOI 10.1016/j.eswa.2017.12.009
   Sahoo PK, 2014, 2014 INTERNATIONAL CONFERENCE ON HIGH PERFORMANCE COMPUTING AND APPLICATIONS (ICHPCA)
   Sahoo PK, 2018, SIGNAL IMAGE VIDEO P, V12, P1265, DOI 10.1007/s11760-018-1278-9
   Sahoo PK, 2022, J KING SAUD UNIV-COM, V34, P5296, DOI 10.1016/j.jksuci.2020.12.019
   Sajid H, 2017, IEEE T IMAGE PROCESS, V26, P3249, DOI 10.1109/TIP.2017.2695882
   Sauvalle B, 2023, IEEE WINT CONF APPL, P3243, DOI 10.1109/WACV56688.2023.00326
   Savas MF, 2018, OPTIK, V168, P605, DOI 10.1016/j.ijleo.2018.04.047
   Sengar SS, 2017, SIGNAL IMAGE VIDEO P, V11, P1357, DOI 10.1007/s11760-017-1093-8
   Shin HC, 2016, IEEE T MED IMAGING, V35, P1285, DOI 10.1109/TMI.2016.2528162
   Spagnolo P, 2006, IMAGE VISION COMPUT, V24, P411, DOI 10.1016/j.imavis.2006.01.001
   St-Charles PL, 2015, IEEE WINT CONF APPL, P990, DOI 10.1109/WACV.2015.137
   Subudhi BN, 2023, IEEE T COMPUT SOC SY, V10, P1314, DOI 10.1109/TCSS.2021.3137306
   Subudhi BN, 2008, TENCON IEEE REGION, P1317
   Tezcan MO, 2020, IEEE WINT CONF APPL, P2763, DOI [10.1109/wacv45572.2020.9093464, 10.1109/WACV45572.2020.9093464]
   Tezcan MO, 2021, IEEE ACCESS, V9, P53849, DOI 10.1109/ACCESS.2021.3071163
   Toyama K., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P255, DOI 10.1109/ICCV.1999.791228
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wang KF, 2018, IEEE ACCESS, V6, P15505, DOI 10.1109/ACCESS.2018.2812880
   Wang Y, 2017, PATTERN RECOGN LETT, V96, P66, DOI 10.1016/j.patrec.2016.09.014
   Wang Y, 2014, IEEE COMPUT SOC CONF, P393, DOI 10.1109/CVPRW.2014.126
   Xia HY, 2016, SIGNAL IMAGE VIDEO P, V10, P343, DOI 10.1007/s11760-014-0747-z
   Xu P, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P107, DOI 10.1145/2647868.2654914
   Yang YZ, 2022, IEEE T CIRC SYST VID, V32, P2145, DOI 10.1109/TCSVT.2021.3088130
   Zhang Q, 2021, IEEE T CIRC SYST VID, V31, P1804, DOI 10.1109/TCSVT.2020.3014663
   Zhang T, 2010, CHIN CONT DECIS CONF, P2375, DOI 10.1109/CCDC.2010.5498797
   Zheng WB, 2020, NEUROCOMPUTING, V394, P178, DOI 10.1016/j.neucom.2019.04.088
   Zhou XY, 2019, Arxiv, DOI arXiv:1904.07850
   Zhu BH, 2020, IEEE T INFORM THEORY, V66, P7155, DOI 10.1109/TIT.2020.2983698
   Zhu ZJ, 2012, AEU-INT J ELECTRON C, V66, P249, DOI 10.1016/j.aeue.2011.07.009
NR 79
TC 0
Z9 0
U1 4
U2 4
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUN
PY 2024
VL 146
AR 105021
DI 10.1016/j.imavis.2024.105021
EA APR 2024
PG 14
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA RU9S2
UT WOS:001230297600001
DA 2024-08-05
ER

PT J
AU Xiao, YQ
   Wu, YJ
AF Xiao, Yuqi
   Wu, Yongjun
TI Robust visual tracking via modified Harris hawks optimization
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Frame scale adaptive adjustment; Harris hawks optimization; Generative
   visual tracker; Computer vision technology
ID CORRELATION FILTERS
AB Due to its outstanding efficiency and high precision, Harris hawks optimization (HHO for short) is suitable for solving the problem of visual target tracking under conditions of occlusion, deformation, rotation and in other complicated tracking scenes. A visual target tracker based on HHO is proposed in this study. To further promote the efficiency and stability of the standard HHO method and reduce the probability of the iteration falling into local optima and algorithm prematurity, in this study we propose an improved method called Super-HHO and apply it to visual target tracking. Compared with standard HHO, Super-HHO is superior due to its parameter optimization and updating strategy. We first optimize the random parameters of HHO via chaos theory to avoid frequent repeated exploration of the feasible region. Next, we design a nonlinear renewal strategy for the escape energy, which solves the problem in traditional HHO in which the fixed escape energy cannot accurately reflect the real hunting process of Harris hawks. Mutation strategies are also designed for the locations of the prey and the hunters to improve the optimization ability and eliminate the risk of falling into local extremes. In addition, a frame scale adjustment method model is developed to address the the issue in which the use of a size-fixed tracking frame makes it easy to include too many invalid features, which reduces the efficiency. Finally, we use the OTB2015, and VOT2018 tracking evaluation datasets, which contain hundreds of visual sequences and more than 10 complex interference scenes to conduct a qualitative analysis, a quantitative analysis and a statistical analysis of ours and other classic trackers, and to effectively test and compare the success ratio, precision and stability of each tracker. The proposed method was also compared with other classic trackers using classic large-scale benchmarks such as LaSOT and TrackingNet. Experimental data prove that ours performs well in terms of robustness, precision and efficiency.
C1 [Xiao, Yuqi] West Anhui Univ, Sch Mech & Vehicle Engn, Luan 237012, Anhui, Peoples R China.
   [Wu, Yongjun] Chongqing Jiaotong Univ, Sch Traff & Transportat, Chongqing 400074, Peoples R China.
C3 West Anhui University; Chongqing Jiaotong University
RP Xiao, YQ (corresponding author), West Anhui Univ, Sch Mech & Vehicle Engn, Luan 237012, Anhui, Peoples R China.
EM csuxyq@163.com
FU The 2022 High Level Talent Research Launch Project for West Anhui
   University [00701092350]
FX This study was funded by the 2022 High Level Talent Research Launch
   Project for West Anhui University, with the number 00701092350.
CR Abbaspour M, 2022, IMAGE VISION COMPUT, V127, DOI 10.1016/j.imavis.2022.104553
   Abdelpakey MH, 2022, IMAGE VISION COMPUT, V127, DOI 10.1016/j.imavis.2022.104550
   Azar NA, 2021, J CONTAM HYDROL, V240, DOI 10.1016/j.jconhyd.2021.103781
   Ban YT, 2021, IEEE T PATTERN ANAL, V43, P1761, DOI 10.1109/TPAMI.2019.2953020
   Cui YT, 2022, PROC CVPR IEEE, P13598, DOI 10.1109/CVPR52688.2022.01324
   Danelljan M., 2014, BRIT MACH VIS C NOTT, DOI DOI 10.5244/C.28.65
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2016, PROC CVPR IEEE, P1430, DOI 10.1109/CVPR.2016.159
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Dong XP, 2021, IEEE T PATTERN ANAL, V43, P1515, DOI 10.1109/TPAMI.2019.2956703
   Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1144, DOI [10.1109/ICCV.2017.129, 10.1109/ICCV.2017.128]
   Heidari AA, 2019, FUTURE GENER COMP SY, V97, P849, DOI 10.1016/j.future.2019.02.028
   Hong ZB, 2015, PROC CVPR IEEE, P749, DOI 10.1109/CVPR.2015.7298675
   Houssein EH, 2020, IEEE ACCESS, V8, P19381, DOI 10.1109/ACCESS.2020.2968981
   Huang ZH, 2022, VISUAL COMPUT, V38, P2739, DOI 10.1007/s00371-021-02150-1
   Kristan M, 2019, LECT NOTES COMPUT SC, V11129, P3, DOI 10.1007/978-3-030-11009-3_1
   Kwon J, 2017, IEEE T PATTERN ANAL, V39, P18, DOI 10.1109/TPAMI.2016.2537330
   Lan XY, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3430257
   Lei YW, 2021, IEEE T PATTERN ANAL, V43, P4505, DOI 10.1109/TPAMI.2021.3068154
   Li F, 2018, PROC CVPR IEEE, P4904, DOI 10.1109/CVPR.2018.00515
   Li Y, 2015, LECT NOTES COMPUT SC, V8926, P254, DOI 10.1007/978-3-319-16181-5_18
   Li YH, 2022, Arxiv, DOI arXiv:1902.02804
   Liu S, 2023, INFORM FUSION, V96, P281, DOI 10.1016/j.inffus.2023.02.005
   Luo YM, 2022, IMAGE VISION COMPUT, V119, DOI 10.1016/j.imavis.2022.104390
   Ma C, 2019, IEEE T PATTERN ANAL, V41, P2709, DOI [10.1109/INTMAG.2018.8508195, 10.1109/TPAMI.2018.2865311]
   Moayedi H, 2020, MEASUREMENT, V152, DOI 10.1016/j.measurement.2019.107389
   Müller M, 2018, LECT NOTES COMPUT SC, V11205, P310, DOI 10.1007/978-3-030-01246-5_19
   Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465
   Naskar PK, 2020, NONLINEAR DYNAM, V100, P2877, DOI 10.1007/s11071-020-05625-3
   Shahbazi M, 2022, IMAGE VISION COMPUT, V126, DOI 10.1016/j.imavis.2022.104533
   Tong K, 2022, IMAGE VISION COMPUT, V123, DOI 10.1016/j.imavis.2022.104471
   Viswanathan GM, 1996, NATURE, V381, P413, DOI 10.1038/381413a0
   Wang MM, 2017, PROC CVPR IEEE, P4800, DOI 10.1109/CVPR.2017.510
   Wang N, 2018, PROC CVPR IEEE, P4844, DOI 10.1109/CVPR.2018.00509
   Wu JJ, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3497746
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Xu L, 2021, IEEE IMAGE PROC, P664, DOI 10.1109/ICIP42928.2021.9506055
   Yu MX, 2022, IMAGE VISION COMPUT, V126, DOI 10.1016/j.imavis.2022.104546
   Yuan D, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3486678
   Zeng YL, 2022, APPL INTELL, V52, P4973, DOI 10.1007/s10489-021-02651-5
   Zha YF, 2022, IET COMPUT VIS, V16, P317, DOI 10.1049/cvi2.12090
   Zhang JM, 2023, IEEE SIGNAL PROC LET, V30, P11, DOI 10.1109/LSP.2023.3238277
   Zhao F, 2021, IEEE ACCESS, V9, P73544, DOI 10.1109/ACCESS.2021.3080308
NR 44
TC 0
Z9 1
U1 4
U2 4
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD APR
PY 2024
VL 144
AR 104959
DI 10.1016/j.imavis.2024.104959
EA MAR 2024
PG 9
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA PZ2N1
UT WOS:001217835500001
DA 2024-08-05
ER

PT J
AU Huang, J
   Chen, Z
   Ma, Y
   Fan, F
   Tang, LF
   Xiang, XY
AF Huang, Jun
   Chen, Ziang
   Ma, Yong
   Fan, Fan
   Tang, Linfeng
   Xiang, Xinyu
TI PTET: A progressive token exchanging transformer for infrared and
   visible image fusion
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Image fusion; Transformer; Token exchanging; Infrared image; Visible
   image
ID NETWORK; PERFORMANCE; NEST
AB Integrating complementary information from different modalities is one of the key challenges in image fusion. Most of the existing deep learning -based methods still rely on a one-off fusion layer to integrate the features extracted from two modalities into one. Such an information interaction pattern only considers significant feature integration but neglects the removal of hazardous information that is widely present in the source images. To overcome these limitations, we propose a progressive token exchanging Transformer for infrared and visible image fusion, named PTET. Different from the one-time fusion layer, we devise a progressive token exchange strategy to gradually transfer features from source images and remove harmful information simultaneously. A predictor is utilized to assess the saliency of Transformer tokens from both modalities. Afterwards, an exchanger is designed to perform beneficial token transfer and insignificant token elimination. Through the cascading layers, our network enhances the feature of fusion branch in a progressive manner. Innovative exchange loss and rank loss are introduced to constrain the fusion network. Extensive experiments on MSRS and LLVIP datasets demonstrate the superiority of our PTET compared to nine state-of-the-art alternatives. Visualization of token exchanging strategy and ablation study reveals the effectiveness of our designs.
C1 [Huang, Jun; Chen, Ziang; Ma, Yong; Fan, Fan; Tang, Linfeng; Xiang, Xinyu] Wuhan Univ, Elect Informat Sch, Wuhan 430072, Peoples R China.
C3 Wuhan University
RP Fan, F (corresponding author), Wuhan Univ, Elect Informat Sch, Wuhan 430072, Peoples R China.
EM junhwong@whu.edu.cn; frostcza@whu.edu.cn; mayong@whu.edu.cn;
   fanfan@whu.edu.cn; xiangxinyu@whu.edu.cn
FU National Natural Science Foundation of China [62075169, 62003247,
   62061160370]; Key Research and Development Program of Hubei Province
   [2021BBA235]
FX This work was supported by the National Natural Science Foundation of
   China (No. 62075169, 62003247 and 62061160370) and the Key Research and
   Development Program 2021BBA235 of Hubei Province.
CR Aslantas V, 2015, AEU-INT J ELECTRON C, V69, P160, DOI 10.1016/j.aeue.2015.09.004
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Chen J, 2023, NEUROCOMPUTING, V527, P71, DOI 10.1016/j.neucom.2023.01.033
   Cordonnier J. -B., 2020, IBER CONF INF SYST, DOI DOI 10.23919/cisti49556.2020.9141108
   Deshmukh M., 2010, Int. J. Image Process., V4, P484
   Di Wang J.L., 2022, P 31 INT JOINT C ART, P3508
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Fang JM, 2022, PROC CVPR IEEE, P12053, DOI 10.1109/CVPR52688.2022.01175
   Han Y, 2013, INFORM FUSION, V14, P127, DOI 10.1016/j.inffus.2011.08.002
   He ST, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14993, DOI 10.1109/ICCV48922.2021.01474
   Jia XY, 2021, IEEE INT CONF COMP V, P3489, DOI 10.1109/ICCVW54120.2021.00389
   Li H, 2023, IEEE T PATTERN ANAL, V45, P11040, DOI 10.1109/TPAMI.2023.3268209
   Li H, 2021, INFORM FUSION, V73, P72, DOI 10.1016/j.inffus.2021.02.023
   Li H, 2020, IEEE T INSTRUM MEAS, V69, P9645, DOI 10.1109/TIM.2020.3005230
   Li H, 2018, INT C PATT RECOG, P2705, DOI 10.1109/ICPR.2018.8546006
   Li H, 2019, IEEE T IMAGE PROCESS, V28, P2614, DOI 10.1109/TIP.2018.2887342
   Li JP, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3164136
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lin Liting, 2022, ADV NEUR IN
   Liu JY, 2022, PROC CVPR IEEE, P5792, DOI 10.1109/CVPR52688.2022.00571
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Ma JY, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3038013
   Ma JY, 2022, IEEE-CAA J AUTOMATIC, V9, P1200, DOI 10.1109/JAS.2022.105686
   Ma JY, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3075747
   Ma JY, 2020, IEEE T IMAGE PROCESS, V29, P4980, DOI 10.1109/TIP.2020.2977573
   Ma JY, 2019, INFORM FUSION, V48, P11, DOI 10.1016/j.inffus.2018.09.004
   Ma JY, 2019, INFORM FUSION, V45, P153, DOI 10.1016/j.inffus.2018.02.004
   Prabhakar KR, 2017, IEEE I CONF COMP VIS, P4724, DOI 10.1109/ICCV.2017.505
   Qu GH, 2002, ELECTRON LETT, V38, P313, DOI 10.1049/el:20020212
   Qu LH, 2022, AAAI CONF ARTIF INTE, P2126
   Rao Dongyu, 2023, IEEE Trans Image Process, VPP, DOI 10.1109/TIP.2023.3273451
   Roberts JW, 2008, J APPL REMOTE SENS, V2, DOI 10.1117/1.2945910
   Shen DH, 2021, IMAGE VISION COMPUT, V105, DOI 10.1016/j.imavis.2020.104047
   Shen DH, 2020, IMAGE VISION COMPUT, V104, DOI 10.1016/j.imavis.2020.104037
   Sun YM, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P4003, DOI 10.1145/3503161.3547902
   Sun ZQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3591, DOI 10.1109/ICCV48922.2021.00359
   Tang LF, 2022, IEEE-CAA J AUTOMATIC, V9, P2121, DOI 10.1109/JAS.2022.106082
   Tang LF, 2022, INFORM FUSION, V82, P28, DOI 10.1016/j.inffus.2021.12.004
   Tang Wei, 2022, IEEE Transactions on Multimedia
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang M, 2019, IMAGE VISION COMPUT, V86, P1, DOI 10.1016/j.imavis.2019.02.011
   Wang WH, 2022, COMPUT VIS MEDIA, V8, P415, DOI 10.1007/s41095-022-0274-8
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wang YK, 2022, PROC CVPR IEEE, P12176, DOI 10.1109/CVPR52688.2022.01187
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xie E., 2021, ADV NEURAL INF PROCE, V34, P12077
   Xu H, 2023, IEEE T PATTERN ANAL, V45, P12148, DOI 10.1109/TPAMI.2023.3283682
   Xu H, 2022, IEEE T PATTERN ANAL, V44, P502, DOI 10.1109/TPAMI.2020.3012548
   Yang FZ, 2020, PROC CVPR IEEE, P5790, DOI 10.1109/CVPR42600.2020.00583
   Zhang H, 2021, INFORM FUSION, V76, P323, DOI 10.1016/j.inffus.2021.06.008
   Zhang H, 2021, INT J COMPUT VISION, V129, P2761, DOI 10.1007/s11263-021-01501-8
   Zhang YF, 2021, IMAGE VISION COMPUT, V105, DOI 10.1016/j.imavis.2020.104042
   Zhang Y, 2020, INFORM FUSION, V54, P99, DOI 10.1016/j.inffus.2019.07.011
   Zhao ZX, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P970
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhu X., 2020, INT C LEARN REPR
NR 56
TC 0
Z9 0
U1 11
U2 11
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD APR
PY 2024
VL 144
AR 104957
DI 10.1016/j.imavis.2024.104957
EA MAR 2024
PG 16
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA NR2Q6
UT WOS:001202119300001
DA 2024-08-05
ER

PT J
AU Liu, ZG
   Lu, BS
   Wu, Y
   Gao, CL
AF Liu, Zhigang
   Lu, Bingshuo
   Wu, Yin
   Gao, Chunlei
TI Multi-view daily action recognition based on Hooke balanced matrix and
   broad learning system
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Feature fusion; Broad learning system; Multi -layer methods; Multi -view
   clustering; Action recognition
ID REPRESENTATION; FRAMEWORK; NETWORK; FUSION
AB Daily action recognition is a challenging task in computer vision, and so the multi-layer methods are proposed recently. However, the feature concatenation strategy in multi-view clustering can be regarded as equal-scale feature fusion and ignores the information difference between views. To deal with this problem, we firstly propose the multi-view feature fusion strategy, which constructs Hooke balanced matrix to complete multi-view unsupervised clustering in preparation for more discriminative motion atoms. Secondly, we build the coverage detection network model based on the broad learning system (BLS) to mine the relationship between the features and labels of motion atoms and obtain more accurate labels of motion atoms. Finally, the experimental results based on the WVU dataset, the NTU RGB-D 120 dataset and the N-UCLA dataset show that the proposed UVS-H-BLS method has state-of-the-art performance, compared with the classic methods such as iDT, MoFAP, JLMF, FGCN, MVMLR, and UVS.
C1 [Liu, Zhigang; Lu, Bingshuo; Wu, Yin; Gao, Chunlei] Northeastern Univ Qinhuangdao, Sch Comp & Commun Engn, Qinhuangdao 066004, Peoples R China.
   [Liu, Zhigang; Lu, Bingshuo; Wu, Yin; Gao, Chunlei] Northeastern Univ, Hebei Key Lab Marine Percept Network & Data Proc, Qinhuangdao 066004, Peoples R China.
C3 Northeastern University - China; Northeastern University - China
RP Liu, ZG (corresponding author), Northeastern Univ Qinhuangdao, Sch Comp & Commun Engn, Qinhuangdao 066004, Peoples R China.; Liu, ZG (corresponding author), Northeastern Univ, Hebei Key Lab Marine Percept Network & Data Proc, Qinhuangdao 066004, Peoples R China.
EM zliu@neuq.edu.cn
FU National Natural Science Foundation of China [61973069, 62306068]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61973069 and Grant 62306068.
CR Berkhin P, 2006, GROUPING MULTIDIMENSIONAL DATA: RECENT ADVANCES IN CLUSTERING, P25
   Cai XS, 2023, INFORM FUSION, V91, P364, DOI 10.1016/j.inffus.2022.10.020
   Chen C. L. Philip, 2018, IEEE Transactions on Neural Networks and Learning Systems, V29, P10, DOI 10.1109/TNNLS.2017.2716952
   Chen YX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13339, DOI 10.1109/ICCV48922.2021.01311
   Dhiman C, 2019, 2019 IEEE FIFTH INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM 2019), P225, DOI [10.1109/BigMM.2019.00-21, 10.1109/BigMM.2019.00041]
   Dhiman C, 2020, IEEE T IMAGE PROCESS, V29, P3835, DOI 10.1109/TIP.2020.2965299
   Dhiman C, 2019, IEEE SENS J, V19, P5195, DOI 10.1109/JSEN.2019.2903645
   Gammulle H, 2020, PATTERN RECOGN LETT, V131, P442, DOI 10.1016/j.patrec.2020.01.023
   Gao Z, 2019, IEEE INTERNET THINGS, V6, P9280, DOI 10.1109/JIOT.2019.2911669
   Gong XR, 2022, IEEE T CYBERNETICS, V52, P8922, DOI 10.1109/TCYB.2021.3061094
   He Ziqiang, 2023, IEEE Transactions on Artificial Intelligence
   Hou YX, 2022, ACS NANO, V16, P8358, DOI 10.1021/acsnano.2c02609
   Li GZ, 2021, IEEE T IMAGE PROCESS, V30, P9332, DOI 10.1109/TIP.2021.3124671
   Li M., 2023, IEEE T NEURAL NETWOR, P1
   Li XF, 2023, PATTERN RECOGN, V134, DOI 10.1016/j.patcog.2022.109083
   Li Z, 2022, LECT NOTES COMPUT SC, V13670, P567, DOI 10.1007/978-3-031-20080-9_33
   Lillo I, 2017, IMAGE VISION COMPUT, V59, P63, DOI 10.1016/j.imavis.2016.11.004
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P2684, DOI 10.1109/TPAMI.2019.2916873
   Liu YN, 2023, IEEE T VIS COMPUT GR, V29, P2575, DOI 10.1109/TVCG.2023.3247075
   Liu Y, 2019, IEEE T CIRC SYST VID, V29, P2416, DOI 10.1109/TCSVT.2018.2868123
   Liu Z, 2022, KNOWL-BASED SYST, V255, DOI 10.1016/j.knosys.2022.109741
   Liu ZG, 2023, IMAGE VISION COMPUT, V134, DOI 10.1016/j.imavis.2023.104687
   Liu ZG, 2021, IMAGE VISION COMPUT, V116, DOI 10.1016/j.imavis.2021.104333
   Men Q, 2023, NEUROCOMPUTING, V537, P198, DOI 10.1016/j.neucom.2023.03.070
   Tu NA, 2019, IEEE T CIRC SYST VID, V29, P800, DOI 10.1109/TCSVT.2018.2816960
   Peng S, 2023, Comput. Intell. Neurosci., V2023
   Qiu S, 2022, INFORM FUSION, V80, P241, DOI 10.1016/j.inffus.2021.11.006
   Santos L, 2015, PATTERN RECOGN, V48, P568, DOI 10.1016/j.patcog.2014.08.015
   Shah K, 2023, IEEE WINT CONF APPL, P3370, DOI 10.1109/WACV56688.2023.00338
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Shao D, 2020, PROC CVPR IEEE, P2613, DOI 10.1109/CVPR42600.2020.00269
   Ulhaq A, 2018, IEEE T IMAGE PROCESS, V27, P1230, DOI 10.1109/TIP.2017.2765821
   Wang CX, 2019, J PHYS CONF SER, V1176, DOI 10.1088/1742-6596/1176/6/062015
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang J, 2016, IEEE T CIRC SYST VID, V26, P1461, DOI 10.1109/TCSVT.2014.2382984
   Wang LM, 2016, INT J COMPUT VISION, V119, P254, DOI 10.1007/s11263-015-0859-0
   Wang LM, 2013, IEEE I CONF COMP VIS, P2680, DOI 10.1109/ICCV.2013.333
   Wang QY, 2018, IEEE ACCESS, V6, P20174, DOI 10.1109/ACCESS.2018.2791578
   Wang RS, 2020, CHIN CONT DECIS CONF, P4858, DOI 10.1109/CCDC49329.2020.9164815
   Wang TW, 2019, J VIS COMMUN IMAGE R, V61, P315, DOI 10.1016/j.jvcir.2019.04.001
   Wei P, 2019, IEEE T MULTIMEDIA, V21, P2195, DOI 10.1109/TMM.2019.2897902
   Wu LY, 2023, PATTERN RECOGN, V136, DOI 10.1016/j.patcog.2022.109231
   Yang H, 2022, IEEE T IMAGE PROCESS, V31, P164, DOI 10.1109/TIP.2021.3129117
   Yu X, 2021, INFORM SCIENCES, V568, P350, DOI 10.1016/j.ins.2021.03.059
   Zheng JJ, 2016, IEEE T IMAGE PROCESS, V25, P2542, DOI 10.1109/TIP.2016.2548242
   Zhu LC, 2022, IEEE T MULTIMEDIA, V24, P668, DOI 10.1109/TMM.2021.3057503
   Zhu YS, 2023, IEEE T IMAGE PROCESS, V32, P496, DOI 10.1109/TIP.2022.3230249
NR 47
TC 0
Z9 0
U1 4
U2 4
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD MAR
PY 2024
VL 143
AR 104919
DI 10.1016/j.imavis.2024.104919
EA FEB 2024
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA KC1T3
UT WOS:001177676700001
DA 2024-08-05
ER

PT J
AU Jia, ZH
   Wang, B
   Chen, CH
AF Jia, Zhihao
   Wang, Bing
   Chen, Changhao
TI Drone-NeRF: Efficient NeRF based 3D scene reconstruction for large-scale
   drone survey
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Scene reconstruction; Neural radiance fields; UAV
ID STEREO; SHAPE
AB Neural rendering has garnered substantial attention owing to its capacity for creating realistic 3D scenes. However, its applicability to extensive scenes remains challenging, with limitations in effectiveness. In this work, we propose the Drone-NeRF framework to enhance the efficient reconstruction of unbounded large-scale scenes suited for drone oblique photography using Neural Radiance Fields (NeRF). Our approach involves dividing the scene into uniform sub-blocks based on camera position and depth visibility. Sub-scenes are trained in parallel using NeRF, then merged for a complete scene. We refine the model by optimizing camera poses and guiding NeRF with a uniform sampler. Integrating chosen samples enhances accuracy. A hash-coded fusion MLP accelerates density representation, yielding RGB and Depth outputs. Our framework accounts for sub-scene constraints, reduces parallel-training noise, handles shadow occlusion, and merges sub-regions for a polished rendering result. Moreover, our framework can be enhanced through the integration of semantic scene division, ensuring consistent allocation of identical objects to the same sub-block for improved object integrity and rendering performance. This Drone-NeRF framework demonstrates promising capabilities in addressing challenges related to scene complexity, rendering efficiency, and accuracy in drone-obtained imagery.
C1 [Jia, Zhihao] Univ Southampton, Geog & Environm, Southampton SO17 1BJ, Hants, England.
   [Wang, Bing] Hong Kong Polytech Univ, Dept Aeronaut & Aviat Engn, Hong Kong, Peoples R China.
   [Chen, Changhao] Natl Univ Def Technol, Coll Intelligence Sci & Technol, Changsha, Peoples R China.
C3 University of Southampton; Hong Kong Polytechnic University; National
   University of Defense Technology - China
RP Chen, CH (corresponding author), Natl Univ Def Technol, Coll Intelligence Sci & Technol, Changsha, Peoples R China.
EM zj3g20@soton.ac.uk; bingwang@polyu.edu.hk; changhao.chen66@outlook.com
RI Jia, Zhihao/GXH-0735-2022
OI Jia, Zhihao/0000-0002-3462-0246
FU National Natural Science Foundation of China (NFSC) [62103427,
   42301520]; Young Elite Scientist Sponsorship Program by CAST
   [YESS20220181]
FX This work was in part supported by National Natural Science Foundation
   of China (NFSC) under the Grant Number of 62103427 and 42301520.
   Changhao Chen is sponsored by the Young Elite Scientist Sponsorship
   Program by CAST (No. YESS20220181).
CR Athar S, 2022, PROC CVPR IEEE, P20332, DOI 10.1109/CVPR52688.2022.01972
   Barron JT, 2022, PROC CVPR IEEE, P5460, DOI 10.1109/CVPR52688.2022.00539
   Barron JT, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5835, DOI 10.1109/ICCV48922.2021.00580
   Cai SQ, 2022, PROC CVPR IEEE, P3971, DOI 10.1109/CVPR52688.2022.00395
   Carr JC, 2001, COMP GRAPH, P67, DOI 10.1145/383259.383266
   Hao YN, 2022, J MECH ENG SCI, V16, P9142, DOI 10.15282/jmes.16.3.2022.15.0724
   Vu HH, 2012, IEEE T PATTERN ANAL, V34, P889, DOI 10.1109/TPAMI.2011.172
   HORN BKP, 1986, COMPUT VISION GRAPH, V33, P174, DOI 10.1016/0734-189X(86)90114-3
   Huang X, 2022, PROC CVPR IEEE, P18377, DOI 10.1109/CVPR52688.2022.01785
   Inzerillo L, 2018, AUTOMAT CONSTR, V96, P457, DOI 10.1016/j.autcon.2018.10.010
   Jiang WG, 2020, AUTOMAT CONSTR, V113, DOI 10.1016/j.autcon.2020.103137
   Jin HL, 2005, INT J COMPUT VISION, V63, P175, DOI 10.1007/s11263-005-6876-7
   Jun-Seong K, 2022, LECT NOTES COMPUT SC, V13692, P384, DOI 10.1007/978-3-031-19824-3_23
   Kingma D. P., 2014, arXiv
   Krishnan A, 2023, IMAGE VISION COMPUT, V137, DOI 10.1016/j.imavis.2023.104793
   Li RL, 2022, Arxiv, DOI arXiv:2210.04847
   Lin CH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5721, DOI 10.1109/ICCV48922.2021.00569
   Liu Steven, 2021, P IEEECVF INT C COMP, P5773
   Mari R, 2022, IEEE COMPUT SOC CONF, P1310, DOI 10.1109/CVPRW56347.2022.00137
   Martin-Brualla R, 2021, PROC CVPR IEEE, P7206, DOI 10.1109/CVPR46437.2021.00713
   Mildenhall B, 2022, PROC CVPR IEEE, P16169, DOI 10.1109/CVPR52688.2022.01571
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Müller T, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530127
   Noonan J, 2021, IMAGE VISION COMPUT, V109, DOI 10.1016/j.imavis.2021.104148
   Ost J, 2021, PROC CVPR IEEE, P2855, DOI 10.1109/CVPR46437.2021.00288
   Rematas K, 2022, PROC CVPR IEEE, P12922, DOI 10.1109/CVPR52688.2022.01259
   Seitz S.M., 2006, P 2006 IEEE COMP SOC
   Shang ZX, 2018, CONSTRUCTION RESEARCH CONGRESS 2018: CONSTRUCTION INFORMATION TECHNOLOGY, P305
   Shao RZ, 2022, PROC CVPR IEEE, P15851, DOI 10.1109/CVPR52688.2022.01541
   Shen SH, 2013, IEEE T IMAGE PROCESS, V22, P1901, DOI 10.1109/TIP.2013.2237921
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Song BY, 2022, IMAGE VISION COMPUT, V124, DOI 10.1016/j.imavis.2022.104511
   Tancik M, 2022, PROC CVPR IEEE, P8238, DOI 10.1109/CVPR52688.2022.00807
   Turki H, 2022, PROC CVPR IEEE, P12912, DOI 10.1109/CVPR52688.2022.01258
   Weng CY, 2022, PROC CVPR IEEE, P16189, DOI 10.1109/CVPR52688.2022.01573
   Woodham R. J., 1978, Proceedings of the Society of Photo-Optical Instrumentation Engineers, vol.155. Image Understanding Systems and Industrial Applications, P136
   Xi JH, 2022, PROC CVPR IEEE, P8585, DOI 10.1109/CVPR52688.2022.00840
   Xiangli Yuanbo, 2021, arXiv
   Xu HN, 2019, PATTERN RECOGN LETT, V128, P505, DOI 10.1016/j.patrec.2019.10.020
   Xu TH, 2022, PROC CVPR IEEE, P15862, DOI 10.1109/CVPR52688.2022.01542
   Xu YH, 2022, PROC CVPR IEEE, P1736, DOI 10.1109/CVPR52688.2022.00179
   Yang BB, 2022, LECT NOTES COMPUT SC, V13676, P597, DOI 10.1007/978-3-031-19787-1_34
   Yang BB, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13759, DOI 10.1109/ICCV48922.2021.01352
   Yang XR, 2022, INT SYM MIX AUGMENT, P499, DOI 10.1109/ISMAR55827.2022.00066
   Yu A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5732, DOI 10.1109/ICCV48922.2021.00570
   Zhang K, 2020, Arxiv, DOI arXiv:2010.07492
   Zhao SZ, 2021, AUTOMAT CONSTR, V130, DOI 10.1016/j.autcon.2021.103832
   Zhao X, 2023, Arxiv, DOI [arXiv:2306.12156, DOI 10.48550/ARXIV.2306.12156]
   Zhu ZH, 2022, PROC CVPR IEEE, P12776, DOI 10.1109/CVPR52688.2022.01245
NR 49
TC 0
Z9 0
U1 20
U2 20
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD MAR
PY 2024
VL 143
AR 104920
DI 10.1016/j.imavis.2024.104920
EA JAN 2024
PG 17
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA JP2K3
UT WOS:001174302200001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Zha, YF
   Guo, X
   Li, F
   Li, HF
AF Zha, Yufei
   Guo, Xiao
   Li, Fan
   Li, Hangfei
TI Enhancing small object tracking with reversible rescaling networks
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Small object tracking; Reversible network; Image generation
AB In the rapidly evolving domain of visual tracking, the accurate identification and tracking of small objects pose significant challenges due to their minimal pixel presence and detail deficiency. In the field of small object tracking, utilizing deep features derived from models trained on conventionally sized objects frequently leads to a substantial reduction in the fidelity of the representation. This discrepancy often undermines the effectiveness of tracking small objects. To address this challenge, we introduce the Reversible Small Object Tracker (RSTrack). RSTrack innovatively integrates two reversible neural networks into the tracking architecture, specifically tailored for the nuanced representation of small objects. Drawing upon the reversible networks' unique attribute of lossless information propagation, the reversible extraction network in RSTrack is adept at retaining a more granular level of detail pertaining to small objects. In addition, the reversible scaling network can suppress the background and highlight the small objects by enhancing the details of the image, effectively mitigating the issue of diminished object-background discriminability that arises from previously inadequate representational methods. Extensive experiments on four popular small object tracking benchmarks demonstrate our method achieves new state-of-the-art performances.
C1 [Zha, Yufei; Guo, Xiao; Li, Fan; Li, Hangfei] Northwestern Polytech Univ, ASGO, Sch Comp Sci, Dongxiang Rd, Xian 710129, Shanxi, Peoples R China.
C3 Northwestern Polytechnical University
RP Guo, X (corresponding author), Northwestern Polytech Univ, ASGO, Sch Comp Sci, Dongxiang Rd, Xian 710129, Shanxi, Peoples R China.
EM xiaog@mail.nwpu.edu.cn
FU National Natural Science Foundation of China [U19B2037, 62271239,
   61971352]
FX This work is supported in part by National Natural Science Foundation of
   China (U19B2037,62271239,61971352).
CR Bhat G, 2019, IEEE I CONF COMP VIS, P6181, DOI 10.1109/ICCV.2019.00628
   Cui YT, 2022, PROC CVPR IEEE, P13598, DOI 10.1109/CVPR52688.2022.01324
   Danelljan M, 2020, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR42600.2020.00721
   Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Dinh L., 2015, ICLR WORKSH
   Dinh L., 2017, INT C LEARN REPR
   Gomez A.N., 2017, Advances in Neural Information Processing Systems, V30, P6122
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li SW, 2021, Arxiv, DOI arXiv:2104.03114
   Li X, 2019, PROC CVPR IEEE, P1369, DOI 10.1109/CVPR.2019.00146
   Li Y, 2019, AAAI CONF ARTIF INTE, P8666
   Li YM, 2020, IEEE INT CONF ROBOT, P193, DOI 10.1109/icra40945.2020.9196943
   Liu CL, 2020, IEEE T IMAGE PROCESS, V29, P1738, DOI 10.1109/TIP.2019.2940477
   Liu Y, 2021, PROC CVPR IEEE, P13360, DOI 10.1109/CVPR46437.2021.01316
   Lu SP, 2021, PROC CVPR IEEE, P10811, DOI 10.1109/CVPR46437.2021.01067
   Marvasti-Zadeh S.M., 2020, AS C COMP VIS, P195
   Mingqing Xiao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P126, DOI 10.1007/978-3-030-58452-8_8
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465
   Simonyan K., 2014, C TRACK P
   Song HH, 2022, PATTERN RECOGN, V124, DOI 10.1016/j.patcog.2021.108475
   Wang X, 2022, IEEE T NEUR NET LEAR, V33, P6931, DOI 10.1109/TNNLS.2021.3083933
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Yao LL, 2023, Arxiv, DOI arXiv:2303.04378
   Ye BT, 2022, LECT NOTES COMPUT SC, V13682, P341, DOI 10.1007/978-3-031-20047-2_20
   Yiming Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11920, DOI 10.1109/CVPR42600.2020.01194
   Yin Q, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3130436
   Zaveri MA, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P1539, DOI 10.1109/ICME.2004.1394540
   Zeng D, 2023, IEEE INT CON MULTI, P1349, DOI 10.1109/ICME55011.2023.00234
   Zhu YB, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3239529
   Zhu YB, 2022, Arxiv, DOI arXiv:2202.05659
NR 33
TC 0
Z9 0
U1 1
U2 1
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD AUG
PY 2024
VL 148
AR 105131
DI 10.1016/j.imavis.2024.105131
EA JUN 2024
PG 11
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA XD7B1
UT WOS:001259800300001
DA 2024-08-05
ER

PT J
AU Zou, ZL
   Chen, Y
AF Zou, Zilin
   Chen, Ying
TI Modality interactive attention for cross-modality person
   re-identification
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Person re-identification; Cross-modality; Modality interactive;
   Attention mechanism
ID NETWORKS
AB The visible-infrared person re-identification (VI-ReID) task is challenging in image retrievals because of the modality gaps between visible and infrared images. Different from the most existing methods which either strive to capture modality invariant features or bridge the two modalities via modal data compensation, the proposed network introduces a modality interactive attention(MIA) module, which aims to establish an interactive relation between modality-shared (MSH) features and modality-specific (MSP) features to narrow down the gap. The attention-driven module explores the relevance score between MSH and MSP features, and takes the score as the modality bridge to fuse the two features, thus introducing the specific feature into the shared one. The subnetworks for extracting the MSP and MSH features are also introduced. Extensive experiments on benchmark datasets show that the proposed VI-ReID method outperforms other state-of-the-art methods. On the large-scale SYSU-MM01 dataset, the proposed method can achieve 83.56% and 85.67% in Rank-1 accuracy and mAP, which is 3.26% and 2.37% higher than the baseline.
C1 [Zou, Zilin; Chen, Ying] Jiangnan Univ, Minist Educ, Key Lab Adv Proc Control Light Ind, Wuxi 214122, Peoples R China.
C3 Jiangnan University
RP Chen, Y (corresponding author), Jiangnan Univ, Minist Educ, Key Lab Adv Proc Control Light Ind, Wuxi 214122, Peoples R China.
EM 6221905014@stu.jiangnan.edu.cn; chenying@jiangnan.edu.cn
FU National Natural Science Foundation of China [62173160]
FX This work is supported by the National Natural Science Foundation of
   China (grant no. 62173160) .
CR Chaudhuri A, 2022, Arxiv, DOI arXiv:2210.10486
   Chen CQ, 2022, IEEE T IMAGE PROCESS, V31, P2352, DOI 10.1109/TIP.2022.3141868
   Dai PY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P677
   Deng SH, 2022, PROC CVPR IEEE, P8438, DOI 10.1109/CVPR52688.2022.00826
   Hao X, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16383, DOI 10.1109/ICCV48922.2021.01609
   He ST, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14993, DOI 10.1109/ICCV48922.2021.01474
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang NC, 2023, PATTERN RECOGN, V135, DOI 10.1016/j.patcog.2022.109145
   Jeong MS, 2022, MATHEMATICS-BASEL, V10, DOI 10.3390/math10193503
   Kai Jungling, 2010, Proceedings 7th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS 2010), P448, DOI 10.1109/AVSS.2010.75
   Lee J, 2020, IEEE T IMAGE PROCESS, V29, P6977, DOI 10.1109/TIP.2020.2996086
   Li R, 2021, APPL INTELL, V51, P1479, DOI 10.1007/s10489-020-01880-4
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Liang XT, 2022, IEEE IMAGE PROC, P2651, DOI 10.1109/ICIP46576.2022.9897515
   Liu JL, 2022, PROC CVPR IEEE, P19344, DOI 10.1109/CVPR52688.2022.01876
   Liu Q, 2022, APPL INTELL, V52, P547, DOI 10.1007/s10489-021-02390-7
   Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190
   Mang Ye, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P229, DOI 10.1007/978-3-030-58520-4_14
   Nguyen DT, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17030605
   Seokeon Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10254, DOI 10.1109/CVPR42600.2020.01027
   Si TZ, 2022, PATTERN RECOGN, V124, DOI 10.1016/j.patcog.2021.108462
   Su P., 2022, Computer Vision-ACCV 2023, P543
   Sun J, 2021, PATTERN RECOGN, V116, DOI 10.1016/j.patcog.2021.107937
   Tian XD, 2021, PROC CVPR IEEE, P1522, DOI 10.1109/CVPR46437.2021.00157
   Wan L, 2023, PATTERN RECOGN, V135, DOI 10.1016/j.patcog.2022.109150
   Wang GA, 2019, IEEE I CONF COMP VIS, P3622, DOI 10.1109/ICCV.2019.00372
   Wang J, 2021, IEEE IMAGE PROC, P1044, DOI 10.1109/ICIP42928.2021.9506424
   Wu AC, 2017, IEEE I CONF COMP VIS, P5390, DOI 10.1109/ICCV.2017.575
   Wu Q, 2021, PROC CVPR IEEE, P4328, DOI 10.1109/CVPR46437.2021.00431
   Yan C, 2022, IEEE T MULTIMEDIA, V24, P1665, DOI 10.1109/TMM.2021.3069562
   Yan Lu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13376, DOI 10.1109/CVPR42600.2020.01339
   Yang MX, 2022, PROC CVPR IEEE, P14288, DOI 10.1109/CVPR52688.2022.01391
   Yang X, 2019, IEEE T NEUR NET LEAR, V30, P2987, DOI [10.1109/TNNLS.2018.2861991, 10.1109/TNNLS.2018.2790479]
   Ye M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13547, DOI 10.1109/ICCV48922.2021.01331
   Ye M, 2022, IEEE T INF FOREN SEC, V17, P386, DOI 10.1109/TIFS.2021.3139224
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Yu H, 2023, PROC CVPR IEEE, P3541, DOI 10.1109/CVPR52729.2023.00345
   Yu HX, 2020, IEEE T PATTERN ANAL, V42, P956, DOI 10.1109/TPAMI.2018.2886878
   Zhang CY, 2019, NEUROCOMPUTING, V340, P259, DOI 10.1016/j.neucom.2019.01.093
   Zhang J, 2019, IEEE ACCESS, V7, P95496, DOI 10.1109/ACCESS.2019.2929854
   Zhang Q, 2022, PROC CVPR IEEE, P7339, DOI 10.1109/CVPR52688.2022.00720
   Zhang YY, 2022, LECT NOTES COMPUT SC, V13674, P462, DOI 10.1007/978-3-031-19781-9_27
   Zhang YK, 2023, PROC CVPR IEEE, P2153, DOI 10.1109/CVPR52729.2023.00214
   Zheng ZD, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3159171
NR 45
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD AUG
PY 2024
VL 148
AR 105128
DI 10.1016/j.imavis.2024.105128
EA JUN 2024
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA WY9F4
UT WOS:001258544700001
DA 2024-08-05
ER

PT J
AU Adhikari, A
   Lee, SW
AF Adhikari, Astha
   Lee, Sang-Woong
TI AM-BQA: Enhancing blind image quality assessment using attention
   retractable features and multi-dimensional learning
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Blind image quality assessment; No -reference image quality assessment;
   Multi -scale feature; Progressive multi -task learning
AB In the realm of no-reference image quality assessment (NR-IQA), acquiring pristine source content for reference is often unattainable. This absence of reference presents challenges in accurately estimating perceptual scores due to the diversity and complexity of distortion patterns. To tackle these challenges, we introduce a novel approach named AM-BQA: Enhancing Blind Image Quality Assessment using Attention Retractable Features and Multi-Dimensional Learning, designed to capture and analyze complex patterns. Our method involves several steps. Firstly, we extract crucial and intricate features using a vision transformer. Next, we employ a multi-head transpose attention block with a dual key, incorporating overlap convolution patches and transpose attention into these extracted features. Finally, the attention maps generated by this process pass through an attention retractable block and a weighted multi-head layer to calculate the final quality score. By employing this architecture, we enhance both global and local interactions between complex patches. To validate the effectiveness of our approach, we assess it on four standard datasets (LIVE, TID2013, CSIQ, and KADID-10 K), including both synthetic datasets. Additionally, we conduct experiments on authentic datasets and demonstrate that our model achieves state-of-the-art performance across multiple datasets. The source code and pretrained models are available on this GitHub repository: https://github.com/adhikariastha5/AM-BQA.
C1 [Adhikari, Astha; Lee, Sang-Woong] Gachon Univ, Sch Comp, Seongnam, South Korea.
C3 Gachon University
RP Lee, SW (corresponding author), Gachon Univ, Sch Comp, Seongnam, South Korea.
EM adhikariastha2019@gmail.com; slee@gachon.ac.kr
FU Gachon University [GCU-202109990001]; National Research Foundation of
   Korea (NRF) - Korea Government (MSIT) [RS-2023 -00250978]
FX This work was supported by the Gachon University research fund of 2021
   (GCU-202109990001) and the National Research Foundation of Korea (NRF)
   grant funded by the Korea Government (MSIT) (No. RS-2023 -00250978) .
CR Bianco S, 2018, SIGNAL IMAGE VIDEO P, V12, P355, DOI 10.1007/s11760-017-1166-8
   Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Cao MD, 2023, IEEE T CIRC SYST VID, V33, P160, DOI 10.1109/TCSVT.2022.3201045
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Fang YM, 2020, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR42600.2020.00373
   Ghadiyaram D, 2016, IEEE T IMAGE PROCESS, V25, P372, DOI 10.1109/TIP.2015.2500021
   Golestaneh SA, 2022, IEEE WINT CONF APPL, P3989, DOI 10.1109/WACV51458.2022.00404
   Hancheng Zhu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14131, DOI 10.1109/CVPR42600.2020.01415
   Hosu V, 2020, IEEE T IMAGE PROCESS, V29, P4041, DOI 10.1109/TIP.2020.2967829
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Ke JJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5128, DOI 10.1109/ICCV48922.2021.00510
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Lin HH, 2019, INT WORK QUAL MULTIM
   Lin KY, 2018, PROC CVPR IEEE, P732, DOI 10.1109/CVPR.2018.00083
   Liu D, 2017, IEEE I CONF COMP VIS, P2526, DOI 10.1109/ICCV.2017.274
   Liu Jianzhao, 2022, IEEE Transactions on Multimedia
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lu Y, 2022, LECT NOTES COMPUT SC, V13431, P644, DOI 10.1007/978-3-031-16431-6_61
   Ma KD, 2018, IEEE T IMAGE PROCESS, V27, P1202, DOI 10.1109/TIP.2017.2774045
   Madhusudana PC, 2022, IEEE T IMAGE PROCESS, V31, P4149, DOI 10.1109/TIP.2022.3181496
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009
   Saha A, 2023, PROC CVPR IEEE, P5846, DOI 10.1109/CVPR52729.2023.00566
   Su SL, 2020, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR42600.2020.00372
   Sun SM, 2023, IEEE T MULTIMEDIA, V25, P2912, DOI 10.1109/TMM.2022.3152942
   Talebi H, 2018, IEEE T IMAGE PROCESS, V27, P3998, DOI 10.1109/TIP.2018.2831899
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Zhou, 2006, PhD thesis
   Willmott CJ, 2005, CLIMATE RES, V30, P79, DOI 10.3354/cr030079
   Xia WH, 2021, IEEE T CIRC SYST VID, V31, P1332, DOI 10.1109/TCSVT.2020.3002662
   Yang S, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1383, DOI 10.1145/3343031.3350990
   Yang SD, 2022, IEEE COMPUT SOC CONF, P1190, DOI 10.1109/CVPRW56347.2022.00126
   Ying ZQ, 2020, PROC CVPR IEEE, P3572, DOI 10.1109/CVPR42600.2020.00363
   You JY, 2021, IEEE IMAGE PROC, P1389, DOI 10.1109/ICIP42928.2021.9506075
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zhang JL, 2022, Arxiv, DOI arXiv:2210.01427
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang WX, 2023, PROC CVPR IEEE, P14071, DOI 10.1109/CVPR52729.2023.01352
   Zhang WX, 2023, IEEE T PATTERN ANAL, V45, P2864, DOI 10.1109/TPAMI.2022.3178874
   Zhang WX, 2020, IEEE T CIRC SYST VID, V30, P36, DOI 10.1109/TCSVT.2018.2886771
NR 42
TC 0
Z9 0
U1 3
U2 3
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUL
PY 2024
VL 147
AR 105076
DI 10.1016/j.imavis.2024.105076
EA MAY 2024
PG 10
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA UO5Y1
UT WOS:001249024400001
DA 2024-08-05
ER

PT J
AU Zhang, SL
   Tao, ZY
   Lin, S
AF Zhang, Shengli
   Tao, Zhiyong
   Lin, Sen
TI WaveletFormerNet: A Transformer-based wavelet network for real-world
   non-homogeneous and dense fog removal
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Image dehazing; Convolution neural network; Wavelet transform; Swin
   transformer; Fog removal
ID IMAGE; GAN
AB Although deep convolutional neural networks have achieved remarkable success in removing synthetic fog, it is essential to be able to process images taken in complex foggy conditions, such as dense or non-homogeneous fog, in the real world. However, the haze distribution in the real world is complex, and downsampling can lead to color distortion or loss of detail in the output results as the resolution of a feature map or image resolution decreases. Moreover, the over-stacking of convolutional blocks might increase the model complexity. In addition to the challenges of obtaining sufficient training data, overfitting can also arise in deep learning techniques for foggy image processing, which can limit the generalization abilities of the model, posing challenges for its practical applications in real-world scenarios. Considering these issues, this paper proposes a Transformer-based wavelet network (WaveletFormerNet) for real-world foggy image recovery. We embed the discrete wavelet transform into the Vision Transformer by proposing the WaveletFormer and IWaveletFormer blocks, aiming to alleviate texture detail loss and color distortion in the image due to downsampling. We introduce parallel convolution in the Transformer block, which allows for the capture of multi-frequency information in a lightweight mechanism. Such a structure reduces computational expenses and improves the effectiveness of the network. Additionally, we have implemented a feature aggregation module (FAM) to maintain image resolution and enhance the feature extraction capacity of our model, further contributing to its impressive performance in real-world foggy image recovery tasks. Through extensive experiments on real-world fog datasets, we have demonstrated that our WaveletFormerNet achieves superior performance compared to state-of-the-art methods, as shown through quantitative and qualitative evaluations of minor model complexity. Additionally, our satisfactory results on real-world dust removal and application tests showcase the superior generalization ability and improved performance of WaveletFormerNet in computer vision-related applications compared to existing stateof-the-art methods, further confirming our proposed approach's effectiveness and robustness. Our code is available at https://github.com/shengli666666/WaveletFormerNet.
C1 [Zhang, Shengli; Tao, Zhiyong] Liaoning Tech Univ, Sch Elect & Informat Engn, Huludao, Liaoning, Peoples R China.
   [Lin, Sen] Shenyang Ligong Univ, Sch Automat & Elect Engn, Shenyang, Liaoning, Peoples R China.
C3 Liaoning Technical University; Shenyang Ligong University
RP Tao, ZY (corresponding author), Liaoning Tech Univ, Sch Elect & Informat Engn, Huludao, Liaoning, Peoples R China.
EM taozhiyong@lntu.edu.cn
RI Tao, Zhiyong/ABC-6665-2020; Zhang, Shengli/KBP-8046-2024
OI Tao, Zhiyong/0000-0001-6743-3482; Zhang, Shengli/0009-0002-4292-4386
FU Applied Basic Research Project of Department of Science & Technology of
   Liaoning province [2022JH2/101300274]; Educational Department of
   Liaoning Province [LJKMZ20220679]
FX This work was partly supported by the Applied Basic Research Project of
   Department of Science & Technology of Liaoning province under Grant
   2022JH2/101300274 and partly by the Educational Department of Liaoning
   Province under Grant No. LJKMZ20220679.
CR Ali U, 2023, PATTERN RECOGN, V140, DOI 10.1016/j.patcog.2023.109522
   Ancuti CO, 2020, IEEE COMPUT SOC CONF, P2029, DOI 10.1109/CVPRW50498.2020.00253
   Ancuti CO, 2019, IEEE IMAGE PROC, P1014, DOI [10.1109/ICIP.2019.8803046, 10.1109/icip.2019.8803046]
   Ancuti CO, 2018, IEEE COMPUT SOC CONF, P867, DOI 10.1109/CVPRW.2018.00119
   Ancuti C, 2018, LECT NOTES COMPUT SC, V11182, P620, DOI 10.1007/978-3-030-01449-0_52
   Appina Balasubramanyam, 2020, 2020 INT C SIGN PROC, P1
   Buckel Peter, 2023, 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P1140, DOI 10.1109/CVPRW59228.2023.00121
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chen DD, 2019, IEEE WINT CONF APPL, P1375, DOI 10.1109/WACV.2019.00151
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Choi LK, 2015, IEEE T IMAGE PROCESS, V24, P3888, DOI 10.1109/TIP.2015.2456502
   Claypoole RL, 1998, INT CONF ACOUST SPEE, P1513, DOI 10.1109/ICASSP.1998.681737
   Das SD, 2020, IEEE COMPUT SOC CONF, P1994, DOI 10.1109/CVPRW50498.2020.00249
   Dong H, 2020, PROC CVPR IEEE, P2154, DOI 10.1109/CVPR42600.2020.00223
   Dong Y, 2020, AAAI CONF ARTIF INTE, V34, P10729
   Dosovitskiy A., 2021, ICLR
   Fattal R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360671
   Fu MH, 2021, IEEE COMPUT SOC CONF, P203, DOI 10.1109/CVPRW53098.2021.00029
   Guo CL, 2022, PROC CVPR IEEE, P5802, DOI 10.1109/CVPR52688.2022.00572
   Guo TT, 2017, IEEE COMPUT SOC CONF, P1100, DOI 10.1109/CVPRW.2017.148
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He T, 2019, PROC CVPR IEEE, P558, DOI 10.1109/CVPR.2019.00065
   Jiang K, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10111700
   Kumari A, 2024, SIGNAL PROCESS, V215, DOI 10.1016/j.sigpro.2023.109289
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li ZG, 2022, IEEE T IMAGE PROCESS, V31, P6213, DOI 10.1109/TIP.2022.3207571
   Liu PJ, 2018, IEEE COMPUT SOC CONF, P886, DOI 10.1109/CVPRW.2018.00121
   Liu SB, 2022, IEEE ROBOT AUTOM LET, V7, P5326, DOI 10.1109/LRA.2022.3156176
   Liu W, 2020, IEEE COMPUT SOC CONF, P1742, DOI 10.1109/CVPRW50498.2020.00224
   Liu XH, 2019, IEEE I CONF COMP VIS, P7313, DOI 10.1109/ICCV.2019.00741
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   McCartney E. J., 1976, Optics of the atmosphere. Scattering by molecules and particles
   Mehta A, 2021, IEEE WINT CONF APPL, P413, DOI 10.1109/WACV48630.2021.00046
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Narasimhan SG, 2002, INT J COMPUT VISION, V48, P233, DOI 10.1023/A:1016328200723
   Petit O, 2021, LECT NOTES COMPUT SC, V12966, P267, DOI 10.1007/978-3-030-87589-3_28
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Rasti P, 2016, LECT NOTES COMPUT SC, V9756, P175, DOI 10.1007/978-3-319-41778-3_18
   Ren WQ, 2018, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2018.00343
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Ren Wenqi, 2016, P IEEE C COMP VIS PA, P1
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shen HF, 2020, IEEE T GEOSCI REMOTE, V58, P6168, DOI 10.1109/TGRS.2020.2974807
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh Ayush, 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12538), P166, DOI 10.1007/978-3-030-66823-5_10
   Song YD, 2023, IEEE T IMAGE PROCESS, V32, P1927, DOI 10.1109/TIP.2023.3256763
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang PY, 2022, IEEE T CIRC SYST VID, V32, P2760, DOI 10.1109/TCSVT.2021.3097713
   Wang ZY, 2019, IEEE T IMAGE PROCESS, V28, P2530, DOI 10.1109/TIP.2018.2887017
   Wu HY, 2021, PROC CVPR IEEE, P10546, DOI 10.1109/CVPR46437.2021.01041
   Yang HH, 2020, INT CONF ACOUST SPEE, P2628, DOI [10.1109/ICASSP40776.2020.9053920, 10.1109/icassp40776.2020.9053920]
   Yang HH, 2019, IEEE IMAGE PROC, P2736, DOI [10.1109/icip.2019.8803391, 10.1109/ICIP.2019.8803391]
   Zhang GQ, 2024, IEEE T CIRC SYST VID, V34, P60, DOI 10.1109/TCSVT.2023.3274366
   Zhao H, 2017, IEEE T COMPUT IMAG, V3, P47, DOI 10.1109/TCI.2016.2644865
   Zheng Y, 2023, PROC CVPR IEEE, P5785, DOI 10.1109/CVPR52729.2023.00560
   Zhou Chu, 2021, Adv. Neural Inf. Process. Syst, V34, P11487
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
   Zou WB, 2021, IEEE INT CONF COMP V, P1895, DOI 10.1109/ICCVW54120.2021.00216
NR 61
TC 0
Z9 0
U1 3
U2 3
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUN
PY 2024
VL 146
AR 105014
DI 10.1016/j.imavis.2024.105014
EA APR 2024
PG 16
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA RS2U3
UT WOS:001229593100001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Wang, ZR
   Yi, JJ
   Ding, HK
   Zeng, F
   Mu, JZ
   Wu, B
AF Wang, Zhuoran
   Yi, Jianjun
   Ding, Hongkai
   Zeng, Fei
   Mu, Jinzhen
   Wu, Bin
TI Nonlinear circumference-based robust ellipse detection in low-SNR images
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Ellipse detection; Nonlinear circumference; Hough transform; Low-SNR
   images
ID RANDOMIZED HOUGH TRANSFORM
AB This study aims to present an effective method to detect ellipses on images with low signal-to-noise ratios (SNR). Firstly, we analyze four major interferences caused by low SNR images. A nonlinear circumference-based ellipse detection method is proposed, which uses a nonlinear circumference to vote on the parameter space constructed from input images and a spatial hierarchical search strategy to find the optimum ellipse parameters. The nonlinear circumference is designed to describe the possibility that selected edge points will generate a given ellipse. Experiments on six low SNR datasets show that our method outperforms five existing methods in terms of recall, precision and F-measure. Furthermore, a definition based on the nonlinear circumference is proposed to quantify SNR of synthetic images with ellipses. Experimental results demonstrate that our method can detect ellipses on images close to 0.2 dB while state-of-the-art methods can almost only reach 1.2 dB.
C1 [Wang, Zhuoran; Yi, Jianjun; Ding, Hongkai; Zeng, Fei] East China Univ Sci & Technol, Dept Mech Engn, Shanghai 200237, Peoples R China.
   [Mu, Jinzhen] Shanghai Aerosp Control Technol Inst, Shanghai 201109, Peoples R China.
   [Mu, Jinzhen] Shanghai Key Lab Aerosp Intelligent Control Techno, Shanghai 201109, Peoples R China.
   [Wu, Bin] Aerosp Syst Engn Shanghai, 3888 Yuanjiang Rd,Minhang Dist, Shanghai 201109, Peoples R China.
C3 East China University of Science & Technology
RP Yi, JJ (corresponding author), East China Univ Sci & Technol, Dept Mech Engn, Shanghai 200237, Peoples R China.
EM jjyi@ecust.edu.cn
CR Arellano C, 2016, PATTERN RECOGN, V58, P12, DOI 10.1016/j.patcog.2016.01.017
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chia AYS, 2007, IEEE IMAGE PROC, P2585
   Chia AYS, 2011, IEEE T IMAGE PROCESS, V20, P1991, DOI 10.1109/TIP.2010.2099127
   Cooke T., 2010, Proceedings 2010 International Conference on Digital Image Computing: Techniques and Applications (DICTA 2010), P575, DOI 10.1109/DICTA.2010.102
   Da Xu RY, 2010, IEEE T IMAGE PROCESS, V19, P1673, DOI 10.1109/TIP.2010.2045071
   DUDA RO, 1972, COMMUN ACM, V15, P11, DOI 10.1145/361237.361242
   Fornaciari M, 2014, PATTERN RECOGN, V47, P3693, DOI 10.1016/j.patcog.2014.05.012
   Garcés Y, 2016, SCI REP-UK, V6, DOI 10.1038/srep36505
   Jabbar A, 2019, LECT NOTES COMPUT SC, V11754, P319, DOI 10.1007/978-3-030-34995-0_29
   Jia Q, 2017, IEEE T IMAGE PROCESS, V26, P3665, DOI 10.1109/TIP.2017.2704660
   Kanatani K., 2004, INT J IMAGE GRAPHICS, V4, P35
   Kim E, 2002, P IEEE INF TECHN APP
   KIRYATI N, 1991, PATTERN RECOGN, V24, P303, DOI 10.1016/0031-3203(91)90073-E
   Lei YW, 1999, PATTERN RECOGN LETT, V20, P41, DOI 10.1016/S0167-8655(98)00127-5
   Li GH, 2021, VISUAL COMPUT, V37, P433, DOI 10.1007/s00371-020-01812-w
   Li ZX, 2019, J FIELD ROBOT, V36, P34, DOI 10.1002/rob.21815
   Liu CC, 2022, MACH VISION APPL, V33, DOI 10.1007/s00138-022-01319-5
   Liu ZY, 2009, PATTERN RECOGN, V42, P2421, DOI 10.1016/j.patcog.2009.01.028
   Lu CS, 2020, IEEE T IMAGE PROCESS, V29, P768, DOI 10.1109/TIP.2019.2934352
   Lu TT, 2015, IET COMPUT VIS, V9, P914, DOI 10.1049/iet-cvi.2014.0347
   Lu W, 2008, PATTERN RECOGN, V41, P1268, DOI 10.1016/j.patcog.2007.09.006
   McLaughlin RA, 1998, IEEE T PATTERN ANAL, V20, P396, DOI 10.1109/34.677267
   McLaughlin RA, 1998, PATTERN RECOGN LETT, V19, P299, DOI 10.1016/S0167-8655(98)00010-5
   Meng C, 2020, IEEE T IMAGE PROCESS, V29, P4406, DOI 10.1109/TIP.2020.2967601
   Meng C, 2018, IEEE T AERO ELEC SYS, V54, P3084, DOI 10.1109/TAES.2018.2843578
   Meng C, 2015, 2015 INTERNATIONAL CONFERENCE ON DIGITAL IMAGE COMPUTING: TECHNIQUES AND APPLICATIONS (DICTA), P519
   Prasad D.K., 2012, Methods for Ellipse Detection from Edge Maps of Real Images
   Prasad DK, 2012, PATTERN RECOGN, V45, P3204, DOI 10.1016/j.patcog.2012.02.014
   Tang Y, 2011, IEEE IMAGE PROC, P1045, DOI 10.1109/ICIP.2011.6115603
   Teutsch C., 2006, Int. Soc. Optics Photon., V6070, P171
   Xie YH, 2002, INT C PATT RECOG, P957, DOI 10.1109/ICPR.2002.1048464
   Yin PY, 1999, PATTERN RECOGN LETT, V20, P731, DOI 10.1016/S0167-8655(99)00037-9
   Zafari S, 2015, IEEE T IMAGE PROCESS, V24, P5942, DOI 10.1109/TIP.2015.2492828
   Zhang SC, 2005, PATTERN RECOGN, V38, P273, DOI 10.1016/j.patcog.2004.03.014
NR 35
TC 0
Z9 0
U1 2
U2 2
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD APR
PY 2024
VL 144
AR 104968
DI 10.1016/j.imavis.2024.104968
EA MAR 2024
PG 18
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA QK2P5
UT WOS:001220705600001
DA 2024-08-05
ER

PT J
AU Wang, AZ
   Ren, CH
   Zhao, S
   Mu, SB
AF Wang, Anzhi
   Ren, Chunhong
   Zhao, Shuang
   Mu, Shibiao
TI Attention guided multi-level feature aggregation network for camouflaged
   object detection
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Camouflaged object detection; Attention; Feature aggregation; Edge
   context
AB Camouflaged object detection (COD) aims to identify objects that are visually blended into their highly similar surroundings, which is an extremely complex and challenging visual task in real -world scenarios, and has recently attracted increasing research interest in the field of computer vision due to its valuable applications. The existing deep learning based methods of COD have the following problems: 1) the ambiguous boundary of the camouflaged objects in prediction map, 2) an inaccurate detection of the camouflaged object with accurate and complete structure details. To this end, an attention guided multi-level feature aggregation network is proposed for this task, which is based on three key designs. First, by embedding spatial pyramid attention (SPA) in ResNetlike backbone network, better multi-level features are extracted to identify the camouflaged objects with complete internal details. Second, an edge context module is designed to make full use of edge information, which highlights camouflaged object structure and generates accurate edge localization of COD. Third, the feature aggregation module based on local attention is used to fuse these enhanced multi-level features and edge context cues, which consists of two major components: the iterative Attentional Feature Fusion (iAFF) module and the Dual-branch Global Context Module (DGCM). Compared with the existing 16 state -of -the -art methods, extensive experiments on four widely-used benchmark datasets under four authoritative evaluation metrics illustrate that the proposed method is very beneficial to the COD task.
C1 [Wang, Anzhi; Ren, Chunhong; Zhao, Shuang] Guizhou Normal Univ, Sch Big Data & Comp Sci, Huaxi Univ Town, Guiyang 550025, Guizhou, Peoples R China.
   [Mu, Shibiao] Yiwu Ind & Commercial Coll, Sch Mechatron & IT, 2 Xueyuan Rd, Yiwu 322000, Zhejiang, Peoples R China.
C3 Guizhou Normal University
RP Wang, AZ (corresponding author), Guizhou Normal Univ, Sch Big Data & Comp Sci, Huaxi Univ Town, Guiyang 550025, Guizhou, Peoples R China.
EM andyscu@163.com
FU Foundation for Innovative Research Groups of the National Natural
   Science Foundation of China [62162013]; Academic XinMiao Fund project of
   Guizhou Normal University [[2022] 30]; Zhejiang Provincial Philosophy
   and Social Sciences Planning Project [23NDJC371YB]
FX Anzhi wang reports financial support was provided by Foundation for
   Innovative Research Groups of the National Natural Science Foundation of
   China (No.62162013) and Academic XinMiao Fund project of Guizhou Normal
   University (Grant No: [2022] 30) . Shibiao Mu reports financial support
   was provided by the Zhejiang Provincial Philosophy and Social Sciences
   Planning Project (23NDJC371YB) .
CR Bi HB, 2022, IEEE T CIRC SYST VID, V32, P5708, DOI 10.1109/TCSVT.2021.3124952
   Chen G, 2022, IEEE T CIRC SYST VID, V32, P6981, DOI 10.1109/TCSVT.2022.3178173
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Cheng XL, 2022, PROC CVPR IEEE, P13854, DOI 10.1109/CVPR52688.2022.01349
   Cui Y., 2023, Computer Vision-ACCV 2022. ACCV 2022, V13842
   Dai YM, 2021, IEEE WINT CONF APPL, P3559, DOI 10.1109/WACV48630.2021.00360
   Fan D.P., 2020, Medical Image Computing and Computer Assisted Intervention-MICCAI 2020. MICCAI 2020, V12266
   Fan DP, 2022, IEEE T PATTERN ANAL, V44, P6024, DOI 10.1109/TPAMI.2021.3085766
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2020, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR42600.2020.00285
   Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698
   Gao S.H., 2020, Computer Vision-ECCV 2020. ECCV 2020, V12351
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   Guo JD, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102906
   Ji GP, 2022, PATTERN RECOGN, V123, DOI 10.1016/j.patcog.2021.108414
   Jia Q, 2022, PROC CVPR IEEE, P4703, DOI 10.1109/CVPR52688.2022.00467
   Ke ZH, 2022, AAAI CONF ARTIF INTE, P1140
   Li AX, 2021, PROC CVPR IEEE, P10066, DOI 10.1109/CVPR46437.2021.00994
   Li S, 2018, IEEE T IMAGE PROCESS, V27, P3918, DOI 10.1109/TIP.2018.2828329
   Liang J, 2022, IEEE IMAGE PROC, P1116, DOI 10.1109/ICIP46576.2022.9897210
   Lijian Mao, 2019, 2019 12th International Symposium on Computational Intelligence and Design (ISCID). Proceedings, P94, DOI 10.1109/ISCID.2019.00028
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Liu ZY, 2022, INT C PATT RECOG, P140, DOI 10.1109/ICPR56361.2022.9956724
   Luo ZM, 2017, PROC CVPR IEEE, P6593, DOI 10.1109/CVPR.2017.698
   Lv YQ, 2021, PROC CVPR IEEE, P11586, DOI 10.1109/CVPR46437.2021.01142
   Margolin R, 2014, PROC CVPR IEEE, P248, DOI 10.1109/CVPR.2014.39
   Mei HY, 2021, PROC CVPR IEEE, P8768, DOI 10.1109/CVPR46437.2021.00866
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Pang YW, 2022, PROC CVPR IEEE, P2150, DOI 10.1109/CVPR52688.2022.00220
   Patel K, 2022, INT C PATT RECOG, P1141, DOI 10.1109/ICPR56361.2022.9956379
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Ren JJ, 2023, IEEE T CIRC SYST VID, V33, P1157, DOI 10.1109/TCSVT.2021.3126591
   Le TN, 2022, IEEE T IMAGE PROCESS, V31, P287, DOI 10.1109/TIP.2021.3130490
   Le TN, 2019, COMPUT VIS IMAGE UND, V184, P45, DOI 10.1016/j.cviu.2019.04.006
   Wang AZ, 2021, IEEE SIGNAL PROC LET, V28, P46, DOI 10.1109/LSP.2020.3044544
   Wei J, 2020, AAAI CONF ARTIF INTE, V34, P12321
   Xu XQ, 2021, IMAGE VISION COMPUT, V114, DOI 10.1016/j.imavis.2021.104283
   Yan JN, 2021, IEEE ACCESS, V9, P43290, DOI 10.1109/ACCESS.2021.3064443
   Yang F, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4126, DOI 10.1109/ICCV48922.2021.00411
   Youwei Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9410, DOI 10.1109/CVPR42600.2020.00943
   Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20
   Zhai Q, 2021, PROC CVPR IEEE, P12992, DOI 10.1109/CVPR46437.2021.01280
   Zhang X, 2017, IEEE T CIRC SYST VID, V27, P2001, DOI 10.1109/TCSVT.2016.2555719
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhao ZQ, 2019, IEEE T NEUR NET LEAR, V30, P3212, DOI 10.1109/TNNLS.2018.2876865
   Zhou H., 2020, P IEEE CVF C COMP VI, P9138, DOI 10.1109/CVPR42600.2020.00916
   Zhou T, 2023, PATTERN RECOGN, V140, DOI 10.1016/j.patcog.2023.109555
   Zhou T, 2022, IEEE T IMAGE PROCESS, V31, P7036, DOI 10.1109/TIP.2022.3217695
   Zhu JC, 2021, AAAI CONF ARTIF INTE, V35, P3599
   Zhuge MC, 2022, PATTERN RECOGN, V127, DOI 10.1016/j.patcog.2022.108644
NR 50
TC 0
Z9 0
U1 3
U2 3
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD APR
PY 2024
VL 144
AR 104953
DI 10.1016/j.imavis.2024.104953
EA FEB 2024
PG 9
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA OB6D4
UT WOS:001204830500001
DA 2024-08-05
ER

PT J
AU Venugopal, V
   Nath, MK
   Joseph, J
   Das, MV
AF Venugopal, Vipin
   Nath, Malaya Kumar
   Joseph, Justin
   Das, M. Vipin
TI A deep learning-based illumination transform for devignetting
   photographs of dermatological lesions
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Convolutional neural network; Counter exponential transform; Deep
   learning; Illumination correction; Skin lesions
ID PIGMENTED SKIN-LESIONS; IMAGE-ENHANCEMENT; SEGMENTATION; SYSTEM
AB Photographs of skin lesions taken with standard digital cameras (macroscopic images) have gained wide acceptance in dermatology. However, uneven background lighting caused by nonstandard image acquisition negatively impacts lesion segmentation and diagnosis. To address this, we propose an automated illumination equalization method based on a counter exponential transform (IECET). A modified residual network (ResNet) regressor is used to automate the selection of the operational parameter of the IECET. The regressor is designed by modifying the final fully-connected layer of the baseline ResNet-50 model. The modified fully-connected layer is coupled to a regression layer in the modified ResNet regressor. A prior knowledge base is created to train the modified ResNet regressor. For this, a set of corrupted images are generated by simulating uneven background illumination on pristine images. The knowledge base is created by including pairs of value components obtained from the HSV color space version of the corrupted macroscopic images and ideal operational parameter values that maximize the peak signal-to-noise ratio (PSNR) between the pristine images and the IECET outputs. We evaluated segmentation accuracies of the deep threshold prediction network (DTP-Net), DeepLabV3+, fully convolutional network (FCN), and U-Net on the corrupted macroscopic images and output images of the IECET. The DTP-Net, DeepLabV3+, FCN, and U-Net exhibited Dice similarity coefficient (DSC) of 0.71 +/- 0.26, 0.85 +/- 0.15, 0.75 +/- 0.22, and 0.66 +/- 0.28 on corrupted images and 0.81 +/- 0.17, 0.87 +/- 0.12, 0.79 +/- 0.18, and 0.79 +/- 0.15, on the outputs of the IECET. Increase in DSC proves the ability of the IECET to improve the performance of deep learning models used to segment skin lesions on macroscopic images.
C1 [Venugopal, Vipin; Nath, Malaya Kumar] Natl Inst Technol Puducherry, Dept Elect & Commun Engn, Karaikal 609609, Pondicherry, India.
   [Joseph, Justin] Indian Inst Sci, Ctr Brain Res, Bangalore 560012, Karnataka, India.
   [Das, M. Vipin] Kerala Hlth Serv, Dept Dermatol, Trivandrum 695035, Kerala, India.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Puducherry; Indian Institute of Science (IISC) - Bangalore
RP Venugopal, V (corresponding author), Natl Inst Technol Puducherry, Dept Elect & Commun Engn, Karaikal 609609, Pondicherry, India.
EM vipinscms@gmail.com; malaya.nath@nitpy.ac.in; justinjoseph@iisc.ac.in;
   vipindasm144@gmail.com
RI Venugopal, Vipin/AAY-6940-2021
OI Venugopal, Vipin/0000-0002-7943-8923
CR Abbas Q, 2011, COMPUT METH PROG BIO, V104, pE1, DOI 10.1016/j.cmpb.2010.06.016
   Aja-Fernández S, 2015, KNOWL-BASED SYST, V83, P1, DOI 10.1016/j.knosys.2015.02.029
   Alcón JF, 2009, IEEE J-STSP, V3, P14, DOI 10.1109/JSTSP.2008.2011156
   Amelard R, 2015, IEEE T BIO-MED ENG, V62, P820, DOI 10.1109/TBME.2014.2365518
   Ansar SA, 2023, SCI REP-UK, V13, DOI 10.1038/s41598-023-32850-8
   Arantes RB, 2022, IMAGE VISION COMPUT, V117, DOI 10.1016/j.imavis.2021.104338
   Ardizzone Edoardo, 2006, J Clin Monit Comput, V20, P391, DOI 10.1007/s10877-006-9040-1
   ARMSTRONG JS, 1992, INT J FORECASTING, V8, P69, DOI 10.1016/0169-2070(92)90008-W
   Barata C, 2015, IEEE J BIOMED HEALTH, V19, P1146, DOI 10.1109/JBHI.2014.2336473
   Bertels J, 2019, LECT NOTES COMPUT SC, V11765, P92, DOI 10.1007/978-3-030-32245-8_11
   Birkenfeld JS, 2020, COMPUT METH PROG BIO, V195, DOI 10.1016/j.cmpb.2020.105631
   Bloice MD, 2019, BIOINFORMATICS, V35, P4522, DOI 10.1093/bioinformatics/btz259
   Cao XT, 2020, IEEE ACCESS, V8, P109989, DOI 10.1109/ACCESS.2020.3002593
   Caruana R, 2001, ADV NEUR IN, V13, P402
   Cavalcanti PG, 2013, EXPERT SYST APPL, V40, P4054, DOI 10.1016/j.eswa.2013.01.002
   Cavalcanti PG, 2011, COMPUT MED IMAG GRAP, V35, P481, DOI 10.1016/j.compmedimag.2011.02.007
   Chen YQ, 2023, SCI REP-UK, V13, DOI 10.1038/s41598-023-46693-w
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dwivedi P, 2023, IMAGE VISION COMPUT, V136, DOI 10.1016/j.imavis.2023.104747
   Fan JY, 2023, J VIS COMMUN IMAGE R, V97, DOI 10.1016/j.jvcir.2023.103978
   Gautam D, 2018, INT J NUMER METH BIO, V34, DOI 10.1002/cnm.2953
   Giotis I, 2015, EXPERT SYST APPL, V42, P6578, DOI 10.1016/j.eswa.2015.04.034
   Glaister J, 2013, IEEE T BIO-MED ENG, V60, P1873, DOI 10.1109/TBME.2013.2244596
   Goceri E, 2023, INT J IMAG SYST TECH, V33, P1727, DOI 10.1002/ima.22890
   Goyal M, 2020, IEEE ACCESS, V8, P4171, DOI 10.1109/ACCESS.2019.2960504
   Grignaffini F, 2022, ALGORITHMS, V15, DOI 10.3390/a15110438
   Haq MA, 2023, FRACTALS, V31, DOI 10.1142/S0218348X23401023
   Haq MA, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-16665-7
   Haq MA, 2022, CMC-COMPUT MATER CON, V71, P2363, DOI 10.32604/cmc.2022.023059
   Haq MA, 2022, CMC-COMPUT MATER CON, V71, P1769, DOI 10.32604/cmc.2022.018708
   Haq MA, 2022, CMC-COMPUT MATER CON, V71, P1729, DOI 10.32604/cmc.2022.020938
   Haq MA, 2022, CMC-COMPUT MATER CON, V70, P4599, DOI 10.32604/cmc.2022.020495
   Hasan MK, 2023, COMPUT BIOL MED, V155, DOI 10.1016/j.compbiomed.2023.106624
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Kavin Kumar K., 2023, Comput. Syst. Sci. Eng., V46, P1845, DOI [10.32604/csse.2023.033927, DOI 10.32604/CSSE.2023.033927, 10]
   Kaymak R, 2020, EXPERT SYST APPL, V161, DOI 10.1016/j.eswa.2020.113742
   Keerthana D., 2023, Biomedical Engineering Advances, V5, DOI DOI 10.1016/J.BEA.2022.100069
   Kornblith S, 2019, PROC CVPR IEEE, P2656, DOI 10.1109/CVPR.2019.00277
   Lee Dohyoung., 2014, Advances in Low-Level Color Image Process-ing, chapter A Taxonomy of Color Constancy and Invariance Algorithm, P55, DOI [DOI 10.1007/978-94-007-7584-8_3, 10.1007/978-94-007-7584-8_3]
   Lee HS, 2020, SYMMETRY-BASEL, V12, DOI 10.3390/sym12081220
   Lee JRH, 2022, BMC MED IMAGING, V22, DOI 10.1186/s12880-022-00871-w
   Li XH, 2020, IMAGE VISION COMPUT, V93, DOI 10.1016/j.imavis.2019.103853
   Lin BS, 2017, 2017 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (SSCI), P117
   MacLellan AN, 2021, J AM ACAD DERMATOL, V85, P353, DOI 10.1016/j.jaad.2020.04.019
   Mahbod A, 2020, COMPUT METH PROG BIO, V193, DOI 10.1016/j.cmpb.2020.105475
   Murphy KP, 2012, MACHINE LEARNING: A PROBABILISTIC PERSPECTIVE, P1
   Oliveira RB, 2016, EXPERT SYST APPL, V61, P53, DOI 10.1016/j.eswa.2016.05.017
   Rizzi A, 2003, PATTERN RECOGN LETT, V24, P1663, DOI 10.1016/S0167-8655(02)00323-9
   Saleem A, 2019, J MED IMAGING, V6, DOI 10.1117/1.JMI.6.3.034501
   Saleem RM, 2023, IEEE ACCESS, V11, P85900, DOI 10.1109/ACCESS.2023.3301504
   Salvi M, 2022, COMPUT METH PROG BIO, V225, DOI 10.1016/j.cmpb.2022.107040
   Sathish S, 2023, VISUAL COMPUT, V39, P693, DOI 10.1007/s00371-021-02368-z
   Schaefer G, 2011, COMPUT MED IMAG GRAP, V35, P99, DOI 10.1016/j.compmedimag.2010.08.004
   Shahsavari Ali, 2021, Informatics in Medicine Unlocked, V24, DOI 10.1016/j.imu.2021.100628
   Shamsudeen Fousia M., 2019, Informatics in Medicine Unlocked, V14, P82, DOI 10.1016/j.imu.2018.10.001
   Shorten C, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0197-0
   Smith AR., 1978, COLOR GAMUT TRANSFOR, V12, P12, DOI [10.1145/965139.807361, DOI 10.1145/965139.807361]
   Tajeddin NZ, 2018, COMPUT METH PROG BIO, V163, P143, DOI 10.1016/j.cmpb.2018.05.005
   Torres-Velazquez M, 2021, IEEE T RADIAT PLASMA, V5, P137, DOI [10.1109/TRPMS.2020.3030611, 10.1109/trpms.2020.3030611]
   Venugopal V., 2023, Decis Anal J, V8, DOI [10.1016/j.dajour.2023.100278, DOI 10.1016/J.DAJOUR.2023.100278]
   Venugopal V, 2022, COMPUT BIOL MED, V148, DOI 10.1016/j.compbiomed.2022.105852
   Venugopal V, 2022, COMPUT METH PROG BIO, V222, DOI 10.1016/j.cmpb.2022.106935
   Vision I. P. Lab, 2023, University of waterloo skin cancer database
   Wang WC, 2019, INFORM SCIENCES, V496, P25, DOI 10.1016/j.ins.2019.05.015
   Xie FY, 2016, IEEE T BIO-MED ENG, V63, P1248, DOI 10.1109/TBME.2015.2493580
   Xu L, 1999, IMAGE VISION COMPUT, V17, P65, DOI 10.1016/S0262-8856(98)00091-2
   Yousef R, 2023, DIAGNOSTICS, V13, DOI 10.3390/diagnostics13091624
   Zhang L, 2019, IET IMAGE PROCESS, V13, P2647, DOI 10.1049/iet-ipr.2018.5840
   Zhang WD, 2022, IEEE T IMAGE PROCESS, V31, P3997, DOI 10.1109/TIP.2022.3177129
   Zhang WD, 2022, IEEE J OCEANIC ENG, V47, P718, DOI 10.1109/JOE.2022.3140563
   Zhuang PX, 2021, ENG APPL ARTIF INTEL, V101, DOI 10.1016/j.engappai.2021.104171
NR 71
TC 1
Z9 1
U1 2
U2 2
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD FEB
PY 2024
VL 142
AR 104909
DI 10.1016/j.imavis.2024.104909
EA JAN 2024
PG 20
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA JJ9N0
UT WOS:001172918000001
DA 2024-08-05
ER

PT J
AU Luo, F
   Ma, JX
   Ho, G
AF Luo, Fang
   Ma, Jiaxing
   Ho, G. T. S.
TI An instance-level data balancing method for object detection via
   contextual information alignment
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Object detection; Data imbalance; Instance augmentation; Context
   alignment; Dynamic data balancing mechanism
AB The imbalance issues in object detection training data, such as in categories, scales, and spatial distribution, result in detection models failing to effectively fit unbalanced data. Our method aims to mitigate the performance disparity caused by data imbalance from the perspective of instance-level augmentation. Firstly, we designed a dynamic data balancing mechanism (DDBM) to develop category expansion rate, scale ratio rules, and spatial distribution indicators to alleviate data imbalance. Then, based on the pixel-level fine-grained context, a finegrained local object instance augmentation (FLOIA) method is designed to selectively copy the object instance according to the background Mosaic degree. In addition, based on the coarse-grained global context and dynamic balancing mechanism, we proposes a coarse-grained global object instance augmentation (CGOIA) method to establish an object-background association, ensure the alignment of the context information of the object instance and alleviate the data imbalance. We train the proposed instance augmentation-treated datasets on various models, resulting in improved balance across different categories and scales. Additionally, visual analysis validates that this approach balances spatial distributions while conforming to contextual information. Furthermore, this method proves advantageous for training with small-sample datasets.
C1 [Luo, Fang; Ma, Jiaxing] Wuhan Univ Technol, Wuhan 430000, Peoples R China.
   [Ho, G. T. S.] Hang Seng Univ Hong Kong, Hong Kong 999077, Peoples R China.
C3 Wuhan University of Technology; Hang Seng University of Hong Kong
RP Ho, G (corresponding author), Hang Seng Univ Hong Kong, Hong Kong 999077, Peoples R China.
EM georgeho@hsu.edu.hk
FU Guangdong -Macao Innovation Technology Cooperation Funding Project,
   China [2021A0505080008]
FX This work was supported by the Guangdong -Macao Innovation Technology
   Cooperation Funding Project, China (No.2021A0505080008) .
CR Ai Qing-Lin, 2023, Journal of Zhejiang University (Engineering Science), P1933, DOI 10.3785/j.issn.1008-973X.2023.10.003
   Bosquet B, 2023, PATTERN RECOGN, V133, DOI 10.1016/j.patcog.2022.108998
   Chen CR, 2019, IEEE INT CONF COMP V, P100, DOI 10.1109/ICCVW.2019.00018
   Chen G, 2022, IEEE T SYST MAN CY-S, V52, P936, DOI 10.1109/TSMC.2020.3005231
   Chen J, 2020, THIRD INTERNATIONAL CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2020), P285, DOI 10.1109/MIPR49039.2020.00066
   Chen NY, 2023, J SUPERCOMPUT, V79, P10117, DOI 10.1007/s11227-023-05065-x
   Chen Xue-Yun, 2021, Journal of Zhejiang University (Engineering Science), V55, P1772, DOI 10.3785/j.issn.1008-973X.2021.09.019
   Dvornik N, 2021, IEEE T PATTERN ANAL, V43, P2014, DOI 10.1109/TPAMI.2019.2961896
   Ge Z, 2021, Arxiv, DOI arXiv:2107.08430
   Ghiasi G, 2021, PROC CVPR IEEE, P2917, DOI 10.1109/CVPR46437.2021.00294
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hong S, 2019, IEEE INT CONF COMP V, P127, DOI 10.1109/ICCVW.2019.00021
   Kisantal M, 2019, Arxiv, DOI arXiv:1902.07296
   Krawczyk B, 2016, PROG ARTIF INTELL, V5, P221, DOI 10.1007/s13748-016-0094-0
   Lim JS, 2021, 3RD INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE IN INFORMATION AND COMMUNICATION (IEEE ICAIIC 2021), P181, DOI 10.1109/ICAIIC51459.2021.9415217
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Oksuz K, 2021, IEEE T PATTERN ANAL, V43, P3388, DOI 10.1109/TPAMI.2020.2981890
   [潘晓英 Pan Xiaoying], 2023, [中国图象图形学报, Journal of Image and Graphics], V28, P2587
   Redmon J., 2018, CoRR
   [任宁 Ren Ning], 2022, [计算机科学与探索, Journal of Frontiers of Computer Science & Technology], V16, P1933
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Tong K, 2022, IMAGE VISION COMPUT, V123, DOI 10.1016/j.imavis.2022.104471
   Wang H, 2019, Arxiv, DOI arXiv:1906.00358
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Weiyu Xiong, 2021, 2021 7th IEEE International Conference on Network Intelligence and Digital Content (IC-NIDC), P128, DOI 10.1109/IC-NIDC54101.2021.9660458
   Zhang HY, 2023, PATTERN RECOGN, V143, DOI 10.1016/j.patcog.2023.109801
   Zhang J, 2021, INT J COMPUT INT SYS, V14, P1871, DOI 10.2991/ijcis.d.210622.003
   [郑晨斌 Zheng Chenbin], 2020, [浙江大学学报. 工学版, Journal of Zhejiang University. Engineering Science], V54, P529
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zoph Barret, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P566, DOI 10.1007/978-3-030-58583-9_34
NR 31
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD SEP
PY 2024
VL 149
AR 105155
DI 10.1016/j.imavis.2024.105155
EA JUL 2024
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA YW5C2
UT WOS:001271525000001
DA 2024-08-05
ER

PT J
AU Liao, HX
   Li, XS
   Qin, X
   Wang, WJ
   He, GD
   Huang, HJ
   Guo, X
   Chun, X
   Zhang, JY
   Fu, YQ
   Qin, ZY
AF Liao, Huixian
   Li, Xiaosen
   Qin, Xiao
   Wang, Wenji
   He, Guodui
   Huang, Haojie
   Guo, Xu
   Chun, Xin
   Zhang, Jinyong
   Fu, Yunqin
   Qin, Zhengyou
TI EPSViTs: A hybrid architecture for image classification based on
   parameter-shared multi-head self-attention
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Image classification; Multi -head self -attention; Parameter -shared;
   Hybrid architecture
AB Vision transformers have been successfully applied to image recognition tasks due to their ability to capture longrange dependencies within an image. However, they still suffer from weak local feature extraction, easy loss of channel interaction information in one-dimensional multi-head self-attention modeling, and large number of parameters. This paper proposes a lightweight image classification hybrid architecture named EPSViTs (Efficient Parameter Shared Transformer, EPSViTs). Firstly, a new local feature extraction module is designed to effectively enhance the expression of local features. Secondly, using the parameter sharing approach, a lightweight multihead self-attention module based on information interaction is designed, which can globally model the image from both spatial and channel dimensions, and mine the potential correlation of the image in space and channel. Extensive experiments are conducted on three public datasets, a subset of ImageNet, Cifar100 and APTOS2019, a private dataset Mushroom66, and the results show that the hybrid architecture EPSViTs proposed in this paper based on parameter sharing for multi-head self-attentive image classification has obvious advantages, especially on a subset of ImageNet to reach 89.18%, which is a 3.8% improvement compared to Edgevits_xxs, verifying the effectiveness of the model.
C1 [Liao, Huixian; Qin, Xiao; Wang, Wenji; Huang, Haojie; Guo, Xu; Chun, Xin; Zhang, Jinyong; Fu, Yunqin; Qin, Zhengyou] Nanning Normal Univ, Guangxi Key Lab Human Machine Interact & Intellige, Nanning 530100, Guangxi, Peoples R China.
   [Li, Xiaosen] Guangxi Minzu Univ, Sch Artificial Intelligence, Nanning 530006, Peoples R China.
   [Qin, Xiao] Nanning Normal Univ, Ctr Appl Math Guangxi, Nanning 530100, Guangxi, Peoples R China.
   [He, Guodui] Guangxi Tech Serv Co, China Commun Serv Corp Ltd, Nanning 530013, Peoples R China.
   [Qin, Xiao] Guangxi Collaborat Innovat Ctr Multisource Informa, Nanning, Peoples R China.
C3 Nanning Normal University; Guangxi Minzu University; Nanning Normal
   University
RP Qin, X (corresponding author), Nanning Normal Univ, Guangxi Key Lab Human Machine Interact & Intellige, Nanning 530100, Guangxi, Peoples R China.
EM 7670172@qq.com
FU STI2030-Major Projects [2021ZD0201900]; Guangxi Key RD Project
   [AA22068057]; BAGUI Scholar Program of Guangxi Zhuang Autonomous Region
   of China [201979]; Guangxi University Young and Middle-aged Teachers'
   Basic Scientific Research Ability Improvement Project [2022KY0378]
FX This work was supported by STI2030-Major Projects2021ZD0201900, Guangxi
   Key R&D Project (Grant no. AA22068057). BAGUI Scholar Program of Guangxi
   Zhuang Autonomous Region of China (201979), Guangxi University Young and
   Middle-aged Teachers' Basic Scientific Research Ability Improvement
   Project (2022KY0378).
CR Biasi LD, 2022, IEEE J BIOMED HEALTH, V26, P962, DOI 10.1109/JBHI.2021.3113609
   Chen JR, 2023, PROC CVPR IEEE, P12021, DOI 10.1109/CVPR52729.2023.01157
   Chu X., 2021, arXiv
   Ding XH, 2022, PROC CVPR IEEE, P568, DOI 10.1109/CVPR52688.2022.00066
   Ding XH, 2021, PROC CVPR IEEE, P13728, DOI 10.1109/CVPR46437.2021.01352
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Graham B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12239, DOI 10.1109/ICCV48922.2021.01204
   Guo JY, 2022, PROC CVPR IEEE, P12165, DOI 10.1109/CVPR52688.2022.01186
   Hacene GB, 2021, INT C PATT RECOG, P4054, DOI 10.1109/ICPR48806.2021.9412859
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Huang XH, 2023, IEEE T MED IMAGING, V42, P1484, DOI 10.1109/TMI.2022.3230943
   Krizhevsky A., 2012, Learning multiple layers of features from tiny images, DOI DOI 10.1145/3079856.3080246
   Lee Y, 2022, PROC CVPR IEEE, P7277, DOI 10.1109/CVPR52688.2022.00714
   Li BN, 2023, PROC CVPR IEEE, P22700, DOI 10.1109/CVPR52729.2023.02174
   Li KC, 2023, IEEE T PATTERN ANAL, V45, P12581, DOI 10.1109/TPAMI.2023.3282631
   Li XS, 2023, COMPUT BIOL MED, V167, DOI 10.1016/j.compbiomed.2023.107596
   Li YW, 2021, Arxiv, DOI arXiv:2104.05707
   Liu XY, 2023, PROC CVPR IEEE, P14420, DOI 10.1109/CVPR52729.2023.01386
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Ma NN, 2018, Arxiv, DOI [arXiv:1807.11164, DOI 10.48550/ARXIV.1807.11164]
   Mehta S, 2022, Arxiv, DOI arXiv:2110.02178
   Pan J., 2022, arXiv
   Rao YM, 2022, Arxiv, DOI arXiv:2207.14284
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shaker A, 2024, Arxiv, DOI [arXiv:2212.04497, DOI 10.48550/ARXIV.2212.04497]
   Touvron H, 2021, Arxiv, DOI [arXiv:2012.12877, 10.48550/arXiv.2012.12877]
   Wang H, 2023, PROC CVPR IEEE, P22378, DOI 10.1109/CVPR52729.2023.02143
   Wang WH, 2022, COMPUT VIS MEDIA, V8, P415, DOI 10.1007/s41095-022-0274-8
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
NR 30
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD SEP
PY 2024
VL 149
AR 105130
DI 10.1016/j.imavis.2024.105130
EA JUL 2024
PG 11
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA YO8Q6
UT WOS:001269523100001
DA 2024-08-05
ER

PT J
AU Choi, S
   Choi, D
   Kim, D
AF Choi, Sangwon
   Choi, Daejune
   Kim, Duksu
TI TIE-KD: Teacher-independent and explainable knowledge distillation for
   monocular depth estimation
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Lightweight; Knowledge distillation; Explainable feature map; Depth
   estimation
AB Monocular depth estimation (MDE) is essential for numerous applications yet is impeded by the substantial computational demands of accurate deep learning models. To mitigate this, we introduce a novel TeacherIndependent Explainable Knowledge Distillation (TIE-KD) framework that streamlines the knowledge transfer from complex teacher models to compact student networks, eliminating the need for architectural similarity. The cornerstone of TIE-KD is the Depth Probability Map (DPM), an explainable feature map that interprets the teacher's output, enabling feature-based knowledge distillation solely from the teacher's response. This approach allows for efficient student learning, leveraging the strengths of feature-based distillation. Extensive evaluation of the KITTI dataset indicates that TIE-KD not only outperforms conventional response-based KD methods but also demonstrates consistent efficacy across diverse teacher and student architectures. The robustness and adaptability of TIE-KD underscore its potential for applications requiring efficient and interpretable models, affirming its practicality for real-world deployment. The code and pre-trained models related to this research are publicly available.1
C1 [Choi, Sangwon; Choi, Daejune; Kim, Duksu] Korea Univ Technol & Educ, KOREATECH, Cheonan 31253, South Korea.
C3 Korea University of Technology & Education
RP Kim, D (corresponding author), Korea Univ Technol & Educ, KOREATECH, Cheonan 31253, South Korea.
EM bluekds@koreatech.ac.kr
FU National Research Foundation of Korea (NRF) through the Ministry of
   Education [2021R1I1A3048263]; Regional Innovation Strategy (RIS)
   [2021RIS-004]
FX This work was supported by the National Research Foundation of Korea
   (NRF) through the Ministry of Education as part of the "Basic Science
   Research Program" under grant 2021R1I1A3048263 (High Performance CGH
   Algorithms for Ultra-High Resolution Hologram Generation, 50%) and the
   "Regional Innovation Strategy (RIS) " under grant 2021RIS-004 (50%) .
CR Ba LJ, 2014, ADV NEUR IN, V27
   Bhat SF, 2021, PROC CVPR IEEE, P4008, DOI 10.1109/CVPR46437.2021.00400
   Chen DF, 2021, AAAI CONF ARTIF INTE, V35, P7028
   Díaz R, 2019, PROC CVPR IEEE, P4733, DOI 10.1109/CVPR.2019.00487
   Dudek G., 2010, Computational principles of mobile robotics, V2nd ed.
   Eigen D, 2014, ADV NEUR IN, V27
   Fu H, 2018, PROC CVPR IEEE, P2002, DOI 10.1109/CVPR.2018.00214
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699
   Gou JP, 2021, INT J COMPUT VISION, V129, P1789, DOI 10.1007/s11263-021-01453-z
   Han S, 2015, ADV NEUR IN, V28
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heo B, 2019, AAAI CONF ARTIF INTE, P3779
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Hu Junjie, 2023, Knowledge Science, Engineering and Management: 16th International Conference, KSEM 2023, Proceedings. Lecture Notes in Computer Science, Lecture Notes in Artificial Intelligence (14117), P27, DOI 10.1007/978-3-031-40283-8_3
   Johnston A, 2020, PROC CVPR IEEE, P4755, DOI 10.1109/CVPR42600.2020.00481
   Kim JH, 2018, ADV NEUR IN, V31
   Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32
   Lee J.H., 2019, arXiv
   Li QQ, 2017, PROC CVPR IEEE, P7341, DOI 10.1109/CVPR.2017.776
   Li ZY, 2023, MACH INTELL RES, V20, P837, DOI 10.1007/s11633-023-1458-0
   Li Zhenyu, 2022, arXiv
   Liebel L, 2019, IEEE INT C INTELL TR, P1440, DOI [10.1109/ITSC.2019.8917177, 10.1109/itsc.2019.8917177]
   Liu LN, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12717, DOI 10.1109/ICCV48922.2021.01250
   Liu YF, 2019, PROC CVPR IEEE, P2599, DOI 10.1109/CVPR.2019.00271
   Mirzadeh SI, 2020, AAAI CONF ARTIF INTE, V34, P5191
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Phan MH, 2021, INT C PATT RECOG, P3620, DOI 10.1109/ICPR48806.2021.9412477
   Pilzer A, 2019, PROC CVPR IEEE, P9760, DOI 10.1109/CVPR.2019.01000
   Ranftl R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12159, DOI 10.1109/ICCV48922.2021.01196
   Romero A, 2015, Arxiv, DOI arXiv:1412.6550
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Saputra MRU, 2019, IEEE I CONF COMP VIS, P263, DOI 10.1109/ICCV.2019.00035
   Song K., 2022, arXiv
   Wang K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16035, DOI 10.1109/ICCV48922.2021.01575
   Wang YR, 2021, IEEE COMPUT SOC CONF, P2457, DOI 10.1109/CVPRW53098.2021.00278
   Wang YH, 2023, Arxiv, DOI arXiv:2309.00526
   Wu JX, 2016, PROC CVPR IEEE, P4820, DOI 10.1109/CVPR.2016.521
   Yan Z., 2023, P AAAI C ARTIFICIAL, P3109
   Yan ZQ, 2023, Arxiv, DOI arXiv:2306.14538
   Yan ZQ, 2022, LECT NOTES COMPUT SC, V13687, P214, DOI 10.1007/978-3-031-19812-0_13
   Yu XY, 2017, PROC CVPR IEEE, P67, DOI 10.1109/CVPR.2017.15
   Yuan L, 2020, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR42600.2020.00396
   Zagoruyko S, 2017, Arxiv, DOI [arXiv:1612.03928, DOI 10.48550/ARXIV.1612.03928]
   Zhai SF, 2016, ADV NEUR IN, V29
   Zhao BR, 2022, PROC CVPR IEEE, P11943, DOI 10.1109/CVPR52688.2022.01165
   Zhao JW, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P163, DOI 10.1109/ICCV48922.2021.00023
   Zheng YP, 2023, IEEE INT CONF ROBOT, P4916, DOI 10.1109/ICRA48891.2023.10160708
NR 49
TC 0
Z9 0
U1 2
U2 2
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD AUG
PY 2024
VL 148
AR 105110
DI 10.1016/j.imavis.2024.105110
EA JUN 2024
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA XA3X5
UT WOS:001258932700001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Li, Q
   Fu, R
   Tang, FL
AF Li, Qian
   Fu, Rao
   Tang, Fulin
TI Depth assisted novel view synthesis using few images
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Neural radiance fields; View synthesis; Image warping
AB In this paper, we introduce a novel approach to improve the performance of Neural Radiance Fields (NeRF) from limited input views. NeRF has exhibited impressive capabilities in producing photo-realistic renderings when trained on dense input views, but its performance degrades as the number of training views decreases. Our key insight is that the original NeRF lacks geometric regularization and appearance information due to limited inputs, resulting in an over-fitting issue. To address this challenge, we present a novel method: first, a global sampling method with geometric regularization is employed by utilizing warped images as additional pseudoviews, which optimizes the multi-view consistency during the training. Second, we introduce a local patch sampling technique with perceptual regularization to ensure pixel correspondence in appearance. Furthermore, we incorporate depth information for explicit geometry regularization. We evaluate our method on the DTU dataset and LLFF dataset from a different number of inputs. Extensive evaluations demonstrate that our approach outperforms existing benchmarks across various metrics, achieving state-of-the-art results.
C1 [Li, Qian; Fu, Rao] Inria, Le Chesnay Rocquencourt, France.
   [Tang, Fulin] Chinese Acad Sci, Inst Automat, Beijing, Peoples R China.
C3 Inria; Chinese Academy of Sciences; Institute of Automation, CAS
RP Fu, R (corresponding author), Inria, Le Chesnay Rocquencourt, France.
EM rao.fu@inria.fr
CR Barron JT, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5835, DOI 10.1109/ICCV48922.2021.00580
   Cao A, 2022, PROC CVPR IEEE, P15692, DOI 10.1109/CVPR52688.2022.01526
   Chan SC, 2007, IEEE SIGNAL PROC MAG, V24, P22, DOI 10.1109/MSP.2007.905702
   Chen AP, 2022, Arxiv, DOI arXiv:2203.09517
   Chen AP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14104, DOI 10.1109/ICCV48922.2021.01386
   Chen D, 2022, LECT NOTES COMPUT SC, V13677, P322, DOI 10.1007/978-3-031-19790-1_20
   Chibane J, 2021, PROC CVPR IEEE, P7907, DOI 10.1109/CVPR46437.2021.00782
   Deng KL, 2022, PROC CVPR IEEE, P12872, DOI 10.1109/CVPR52688.2022.01254
   Fehn C, 2004, PROC SPIE, V5291, P93, DOI 10.1117/12.524762
   Fisher A, 2021, ROBOT AUTON SYST, V142, DOI 10.1016/j.robot.2021.103755
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Jain A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5865, DOI 10.1109/ICCV48922.2021.00583
   Jensen R, 2014, PROC CVPR IEEE, P406, DOI 10.1109/CVPR.2014.59
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kim M, 2022, PROC CVPR IEEE, P12902, DOI 10.1109/CVPR52688.2022.01257
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Liu B., 2021, arXiv
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Mildenhall B, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322980
   Müller T, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530127
   Niemeyer M, 2022, PROC CVPR IEEE, P5470, DOI 10.1109/CVPR52688.2022.00540
   Olszewski K, 2019, IEEE I CONF COMP VIS, P7647, DOI 10.1109/ICCV.2019.00774
   Radford A, 2021, PR MACH LEARN RES, V139
   Riegler Gernot, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P623, DOI 10.1007/978-3-030-58529-7_37
   Roessle B, 2022, PROC CVPR IEEE, P12882, DOI 10.1109/CVPR52688.2022.01255
   Wang GC, 2023, Arxiv, DOI arXiv:2303.16196
   Wang QQ, 2021, PROC CVPR IEEE, P4688, DOI 10.1109/CVPR46437.2021.00466
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yao Y, 2018, LECT NOTES COMPUT SC, V11212, P785, DOI 10.1007/978-3-030-01237-3_47
   Yu A, 2021, PROC CVPR IEEE, P4576, DOI 10.1109/CVPR46437.2021.00455
   Yu Z., 2022, Advances in neural information processing systems, V35, P25018
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
NR 33
TC 0
Z9 0
U1 5
U2 5
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUL
PY 2024
VL 147
AR 105079
DI 10.1016/j.imavis.2024.105079
EA MAY 2024
PG 9
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA TX5B6
UT WOS:001244560700001
OA hybrid
DA 2024-08-05
ER

PT J
AU Di, JR
   Hu, ZP
   Bi, S
   Zhang, HH
   Wang, YL
   Sun, Z
AF Di, Jirui
   Hu, Zhengping
   Bi, Shuai
   Zhang, Hehao
   Wang, Yulu
   Sun, Zhe
TI Temporal refinement network: Combining dynamic convolution and
   multi-scale information for fine-grained action recognition
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Fine-grained action recognition; Temporal refinement block; Temporal
   pyramidal network
AB Fine-grained action recognition is challenging due to the nearly identical context, limited background information, and less distinct inter-class differences compared to coarse-grained actions.Effectively capturing spatiotemporal information is crucial for fine-grained action recognition models. To address the limitations of coarsegrained models in describing spatio-temporal context, we propose a Temporal Refinement Block (TRB) as an efficient component for fine-grained action recognition. The TRB enables our model to effectively model underlying semantics and global dependencies by generating spatial-temporal kernels of different scales and performing fully connected operations within the temporal dimension. Our experiments demonstrate the effectiveness of TRB in learning latent semantics and global dependencies. To further enhance the framework's performance, we incorporate an enhanced spatio-temporal pyramidal network (TPN) that collects beat information and utilizes dilated convolutions to boost multi-scale features.We refer to the proposed framework as the Temporal Refinement Network, abbreviated as TRN.Our TRN achieves competitive performance on the FineGym and Diving48 benchmarks.
C1 [Di, Jirui; Hu, Zhengping; Bi, Shuai; Zhang, Hehao; Wang, Yulu; Sun, Zhe] Yanshan Univ, Dept Informat Sci & Engn, Qinhuangdao 066000, Hebei, Peoples R China.
C3 Yanshan University
RP Hu, ZP (corresponding author), Yanshan Univ, Dept Informat Sci & Engn, Qinhuangdao 066000, Hebei, Peoples R China.
EM djr@stumail.ysu.edu.cn; hzp_ysu@163.com; zhanghh@stumail.ysu.edu.cn;
   wangyulu@stumail.ysu.edu.cn
FU National Natural Science Foundation of China [61771420, 62001413];
   Natural Science Foundation of Hebei Province [F2020203064]; Science and
   Technology Project of Hebei Education Department [BJK2023117]
FX This work is supported by the National Natural Science Foundation of
   China under Grants 61771420 and 62001413, the Natural Science Foundation
   of Hebei Province under Grants F2020203064, and the Science and
   Technology Project of Hebei Education Department under Grants
   BJK2023117.
CR Bertasius G, 2021, PR MACH LEARN RES, V139
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Diba A, 2019, IEEE I CONF COMP VIS, P6191, DOI 10.1109/ICCV.2019.00629
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Fan Ma, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P420, DOI 10.1007/978-3-030-58548-8_25
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Jia X, 2016, NEURIPS, P667
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   Kim M., 2021, Adv. Neural Inf. Proces. Syst., V34, P8046
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Li Y, 2020, PROC CVPR IEEE, P906, DOI 10.1109/CVPR42600.2020.00099
   Li YW, 2018, LECT NOTES COMPUT SC, V11210, P520, DOI 10.1007/978-3-030-01231-1_32
   Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu ZY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13688, DOI 10.1109/ICCV48922.2021.01345
   Liu ZY, 2020, AAAI CONF ARTIF INTE, V34, P11669
   Luo CX, 2019, IEEE I CONF COMP VIS, P5511, DOI 10.1109/ICCV.2019.00561
   Ma F, 2022, PROC CVPR IEEE, P8771, DOI 10.1109/CVPR52688.2022.00858
   Ma F, 2022, INT J COMPUT VISION, V130, P1244, DOI 10.1007/s11263-022-01600-0
   Shao D, 2020, PROC CVPR IEEE, P2613, DOI 10.1109/CVPR42600.2020.00269
   Simonyan K, 2014, ADV NEUR IN, V27
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Soomro K, 2012, Arxiv, DOI arXiv:1212.0402
   Sun Baoli, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P5070, DOI 10.1145/3581783.3612206
   Tian Y, 2022, INT J COMPUT VISION, V130, P2453, DOI 10.1007/s11263-022-01661-1
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   Wang J, 2022, PROC CVPR IEEE, P14033, DOI 10.1109/CVPR52688.2022.01366
   Wang LM, 2019, IEEE T PATTERN ANAL, V41, P2740, DOI 10.1109/TPAMI.2018.2868668
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Xiang WM, 2022, LECT NOTES COMPUT SC, V13663, P627, DOI 10.1007/978-3-031-20062-5_36
   Yang B, 2019, ADV NEUR IN, V32
   Yang CY, 2020, PROC CVPR IEEE, P588, DOI 10.1109/CVPR42600.2020.00067
   Yang Y, 2021, FRONT INFORM TECH EL, V22, P1551, DOI 10.1631/FITEE.2100463
   Yinpeng Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11027, DOI 10.1109/CVPR42600.2020.01104
   Zhao Y, 2023, FRICTION, V11, P1253, DOI 10.1007/s40544-022-0658-x
   Zhou BL, 2018, LECT NOTES COMPUT SC, V11205, P831, DOI 10.1007/978-3-030-01246-5_49
   Zhou YC, 2023, PATTERN RECOGN, V133, DOI 10.1016/j.patcog.2022.108970
   Zhuo Su, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P138, DOI 10.1007/978-3-030-58539-6_9
NR 39
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUL
PY 2024
VL 147
AR 105058
DI 10.1016/j.imavis.2024.105058
EA MAY 2024
PG 8
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA TI7A3
UT WOS:001240689000001
DA 2024-08-05
ER

PT J
AU Liu, Y
   Xue, JH
   Li, DX
   Zhang, WD
   Chiew, TK
   Xu, ZJ
AF Liu, Ying
   Xue, Jiahao
   Li, Daxiang
   Zhang, Weidong
   Chiew, Tuan Kiang
   Xu, Zhijie
TI Image recognition based on lightweight convolutional neural network:
   Recent advances
SO IMAGE AND VISION COMPUTING
LA English
DT Review
DE Image recognition; Lightweight network; Model compression; Optimization
   of lightweight network; Transformer
AB Image recognition is an important task in computer vision with broad applications. In recent years, with the advent of deep learning, lightweight convolutional neural network (CNN) has brought new opportunities for image recognition, which allows high-performance recognition algorithms to run on resource-constrained devices with strong representation and generalization capabilities. This paper first presents an overview of several classical lightweight CNN models. Then, a comprehensive review is provided on recent image recognition techniques using lightweight CNN. According to the strategies applied to optimize image recognition performance, existing methods are classified into three categories: (1) model compression, (2) optimization of lightweight network, and (3) combining Transformer with lightweight network. In addition, some representative methods are tested on three commonly used datasets for performance comparison. Finally, technical challenges and future research trends in this field are discussed.
C1 [Liu, Ying; Xue, Jiahao; Li, Daxiang; Zhang, Weidong] Xian Univ Posts & Telecommun, Ctr Image & Informat Proc, Xian 710121, Shaanxi, Peoples R China.
   [Chiew, Tuan Kiang] Rekindle Pte Ltd, Singapore, Singapore.
   [Xu, Zhijie] Univ Huddersfield, Sch Engn & Comp Sci, Huddersfield, England.
C3 Xi'an University of Posts & Telecommunications; University of
   Huddersfield
RP Liu, Y (corresponding author), Xian Univ Posts & Telecommun, Ctr Image & Informat Proc, Xian 710121, Shaanxi, Peoples R China.
EM liuying_ciip@163.com
CR Anh-Huy Phan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P522, DOI 10.1007/978-3-030-58526-6_31
   Arel I, 2010, IEEE COMPUT INTELL M, V5, P13, DOI 10.1109/MCI.2010.938364
   Bello I, 2019, IEEE I CONF COMP VIS, P3285, DOI 10.1109/ICCV.2019.00338
   Biswas S, 2024, IEEE INTERNET THINGS, V11, P8288, DOI 10.1109/JIOT.2023.3317878
   Bulat A, 2019, Arxiv, DOI arXiv:1909.13863
   Chen FH, 2024, ARCH COMPUT METHOD E, V31, P1915, DOI 10.1007/s11831-023-10032-z
   Chen J.Y., 2023, J. Software, P1
   Chen PG, 2021, PROC CVPR IEEE, P5006, DOI 10.1109/CVPR46437.2021.00497
   Chen YP, 2022, PROC CVPR IEEE, P5260, DOI 10.1109/CVPR52688.2022.00520
   Chen YP, 2019, IEEE I CONF COMP VIS, P3434, DOI 10.1109/ICCV.2019.00353
   Chen Z, 2020, NAT MACH INTELL, V2, DOI 10.1038/s42256-020-00265-z
   Cheng XP, 2024, ENG APPL ARTIF INTEL, V127, DOI 10.1016/j.engappai.2023.107288
   Chin TW, 2020, PROC CVPR IEEE, P1515, DOI 10.1109/CVPR42600.2020.00159
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Choudhary T, 2020, ARTIF INTELL REV, V53, P5113, DOI 10.1007/s10462-020-09816-7
   Dai XL, 2021, PROC CVPR IEEE, P16271, DOI 10.1109/CVPR46437.2021.01601
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding RZ, 2019, PROC CVPR IEEE, P11400, DOI 10.1109/CVPR.2019.01167
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Dou H, 2023, J. Software, P1
   Fan FL, 2021, IEEE T RADIAT PLASMA, V5, P741, DOI 10.1109/TRPMS.2021.3066428
   Fang GF, 2023, PROC CVPR IEEE, P16091, DOI 10.1109/CVPR52729.2023.01544
   Frankle J., 2018, arXiv, DOI DOI 10.48550/ARXIV.1803.03635
   Howard AG, 2017, Arxiv, DOI [arXiv:1704.04861, 10.48550/arXiv.1704.04861]
   Gang H, 2023, J BIG DATA-GER, V10, DOI 10.1186/s40537-023-00795-4
   Gao HY, 2021, IEEE T PATTERN ANAL, V43, P2570, DOI 10.1109/TPAMI.2020.2975796
   Ghimire D, 2023, IMAGE VISION COMPUT, V136, DOI 10.1016/j.imavis.2023.104745
   Ghimire D, 2022, ELECTRONICS-SWITZ, V11, DOI 10.3390/electronics11060945
   Guo JY, 2022, PROC CVPR IEEE, P12165, DOI 10.1109/CVPR52688.2022.01186
   Guo SP, 2020, PROC CVPR IEEE, P1536, DOI 10.1109/CVPR42600.2020.00161
   Hafiz A.M., 2023, J. Mobile Multimedia, V19
   Han K, 2020, PROC CVPR IEEE, P1577, DOI 10.1109/CVPR42600.2020.00165
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heo B, 2019, IEEE I CONF COMP VIS, P1921, DOI 10.1109/ICCV.2019.00201
   Heo B, 2019, AAAI CONF ARTIF INTE, P3779
   Hou ZJ, 2022, PROC CVPR IEEE, P12277, DOI 10.1109/CVPR52688.2022.01197
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Hu J, 2022, AAAI CONF ARTIF INTE, P942
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu P, 2021, AAAI CONF ARTIF INTE, V35, P7780
   Huang JH, 2023, IEEE T EVOLUT COMPUT, V27, P1298, DOI 10.1109/TEVC.2022.3217290
   Jia S, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3087186
   Jin Y, 2023, PROC CVPR IEEE, P24276, DOI 10.1109/CVPR52729.2023.02325
   Joo D, 2021, AAAI CONF ARTIF INTE, V35, P8021
   Kim YD, 2016, Arxiv, DOI [arXiv:1511.06530, 10.48550/arXiv.1511.06530]
   Krizhevsky A., 2012, Learning multiple layers of features from tiny images, DOI DOI 10.1145/3079856.3080246
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Li CX, 2022, LECT NOTES COMPUT SC, V13671, P19, DOI 10.1007/978-3-031-20083-0_2
   Li H, 2017, Arxiv, DOI arXiv:1608.08710
   Li HC, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2021.3064349
   Li JF, 2023, PROC CVPR IEEE, P6153, DOI 10.1109/CVPR52729.2023.00596
   Li LX, 2020, IEEE ACCESS, V8, P139110, DOI 10.1109/ACCESS.2020.3011028
   Li YS, 2020, Arxiv, DOI arXiv:2011.12289
   Li YY, 2019, IMAGE VISION COMPUT, V92, DOI 10.1016/j.imavis.2019.10.005
   Liang Y, 2023, IEEE T NEUR NET LEAR, V34, P6069, DOI 10.1109/TNNLS.2021.3133127
   LIEBENWEIN L, 2021, Advances in Neural Information Processing Systems, V34, P5328
   Lin MB, 2020, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR42600.2020.00160
   Lin XF, 2017, ADV NEUR IN, V30
   Liu J, 2024, Arxiv, DOI arXiv:2401.06426
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Mehta S, 2022, Arxiv, DOI arXiv:2110.02178
   Mehta S, 2019, PROC CVPR IEEE, P9182, DOI 10.1109/CVPR.2019.00941
   Mehta S, 2018, LECT NOTES COMPUT SC, V11214, P561, DOI 10.1007/978-3-030-01249-6_34
   Iandola FN, 2016, Arxiv, DOI [arXiv:1602.07360, 10.48550/arXiv.1602.07360]
   Niu T, 2023, IMAGE VISION COMPUT, V136, DOI 10.1016/j.imavis.2023.104743
   Park J, 2022, LECT NOTES COMPUT SC, V13671, P120, DOI 10.1007/978-3-031-20083-0_8
   Park W, 2019, PROC CVPR IEEE, P3962, DOI 10.1109/CVPR.2019.00409
   Parmar N, 2018, PR MACH LEARN RES, V80
   Passalis N, 2018, LECT NOTES COMPUT SC, V11215, P283, DOI 10.1007/978-3-030-01252-6_17
   Peng C, 2024, IEEE T NEUR NET LEAR, V35, P2805, DOI 10.1109/TNNLS.2022.3192169
   Peng P, 2023, IEEE T IMAGE PROCESS, V32, P2438, DOI 10.1109/TIP.2023.3268562
   Pentsos V., 2023, Micromachines, V14
   Poyser M, 2024, PATTERN RECOGN, V147, DOI 10.1016/j.patcog.2023.110052
   Qin HT, 2023, INT J COMPUT VISION, V131, P26, DOI 10.1007/s11263-022-01687-5
   Qin HT, 2020, PROC CVPR IEEE, P2247, DOI 10.1109/CVPR42600.2020.00232
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Ren PZ, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3447582
   Romero A, 2015, Arxiv, DOI arXiv:1412.6550
   Ruan XF, 2021, AAAI CONF ARTIF INTE, V35, P2495
   Saha R., 2023, Advances in Neural Information Processing Systems, V36
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Shen H, 2024, INFORM SCIENCES, V660, DOI 10.1016/j.ins.2024.120131
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun B, 2023, IEEE T NEUR NET LEAR, V34, P4440, DOI 10.1109/TNNLS.2021.3117685
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tan MX, 2019, PROC CVPR IEEE, P2815, DOI [arXiv:1807.11626, 10.1109/CVPR.2019.00293]
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tan ST, 2023, FORESTS, V14, DOI 10.3390/f14010053
   Tang Y., 2022, Adv. Neural Inf. Process. Syst, V35, P9969
   Thwal CM, 2024, NEURAL NETWORKS, V170, P635, DOI 10.1016/j.neunet.2023.11.044
   Tian Y., 2020, Contrastive representation distillation
   Tian YH, 2020, IEEE ACCESS, V8, P125731, DOI 10.1109/ACCESS.2020.3006097
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Tung F, 2019, IEEE I CONF COMP VIS, P1365, DOI 10.1109/ICCV.2019.00145
   Vaswani A, 2017, ADV NEUR IN, V30
   Wan Alvin, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12962, DOI 10.1109/CVPR42600.2020.01298
   Wang RJ, 2018, ADV NEUR IN, V31
   Wang S, 2023, NEUROCOMPUTING, V541, DOI 10.1016/j.neucom.2023.126269
   Wang XD, 2021, AAAI CONF ARTIF INTE, V35, P10227
   Wang ZW, 2019, PROC CVPR IEEE, P568, DOI 10.1109/CVPR.2019.00066
   Wimmer P, 2022, PROC CVPR IEEE, P12517, DOI 10.1109/CVPR52688.2022.01220
   Wu BC, 2019, PROC CVPR IEEE, P10726, DOI 10.1109/CVPR.2019.01099
   Yadav N., 2022, arXiv
   Yang SK, 2023, KNOWL-BASED SYST, V278, DOI 10.1016/j.knosys.2023.110868
   Ye M., 2020, P INT C MACH LEARN, P10820
   Yin M, 2022, AAAI CONF ARTIF INTE, P8874
   Yu FS, 2016, Arxiv, DOI arXiv:1511.07122
   [张珂 Zhang Ke], 2021, [中国图象图形学报, Journal of Image and Graphics], V26, P2305
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhao B., 2023, P IEEE CVF INT C COM, P6189
   Zhao BR, 2022, PROC CVPR IEEE, P11943, DOI 10.1109/CVPR52688.2022.01165
   Zhao K, 2021, AAAI CONF ARTIF INTE, V35, P3483
   Zhao ZP, 2023, IMAGE VISION COMPUT, V136, DOI 10.1016/j.imavis.2023.104715
   Zheng M., 2022, SPIE, V12168, P333
   Zhong JC, 2023, IEEE T NEUR NET LEAR, V34, P9528, DOI 10.1109/TNNLS.2022.3151138
   Zhou JY, 2023, J SUPERCOMPUT, V79, P7228, DOI 10.1007/s11227-022-04963-w
   Zhou Y, 2020, PROCEEDINGS OF 2020 IEEE 5TH INFORMATION TECHNOLOGY AND MECHATRONICS ENGINEERING CONFERENCE (ITOEC 2020), P1713, DOI [10.1109/itoec49072.2020.9141847, 10.1109/ITOEC49072.2020.9141847]
   Zhu SL, 2019, PROC CVPR IEEE, P4918, DOI 10.1109/CVPR.2019.00506
   Zoph B, 2017, Arxiv, DOI arXiv:1611.01578
NR 121
TC 0
Z9 0
U1 32
U2 32
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUN
PY 2024
VL 146
AR 105037
DI 10.1016/j.imavis.2024.105037
EA MAY 2024
PG 15
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA TB9V3
UT WOS:001238924500001
OA hybrid
DA 2024-08-05
ER

PT J
AU Ye, S
   Yu, SJ
   Wang, Y
   You, XG
AF Ye, Shuo
   Yu, Shujian
   Wang, Yu
   You, Xinge
TI R2-trans: Fine-grained visual categorization with redundancy reduction
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Fine-grained visual categorization; Batch -based dynamic mask;
   Information bottleneck
ID NETWORK
AB Fine-grained visual categorization (FGVC) aims to discriminate similar subcategories, whose main challenge is the large intraclass diversities and subtle inter-class differences. Existing FGVC methods usually select discriminant regions found by a trained model, which is prone to neglect other potential discriminant information. On the other hand, the massive interactions between the sequence of image patches in ViT make the resulting class token contain lots of redundant information, which may also impact FGVC performance. In this paper, we present a novel approach for FGVC, which can simultaneously make use of partial yet sufficient discriminative information in environmental cues and also compress the redundant information in class-token with respect to the target. Specifically, our model calculates the ratio of high-weight regions in a batch, adaptively adjusts the masking threshold, and achieves moderate extraction of background information in the input space. Moreover, we also use the Information Bottleneck (IB) approach to guide our network to learn a minimum sufficient representations in the feature space. Experimental results on three widely-used benchmark datasets verify that our approach can achieve better performance than other state-of-the-art approaches and baseline models. The code of our model is available at: https://github.com/SYe-hub/R-2-Trans.
C1 [Ye, Shuo; Wang, Yu; You, Xinge] Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China.
   [Yu, Shujian] Vrije Univ Amsterdam, Dept Comp Sci, Amsterdam, Netherlands.
   [Yu, Shujian] UiT Arctic Univ Norway, Machine Learning Grp, Tromso, Norway.
C3 Huazhong University of Science & Technology; Vrije Universiteit
   Amsterdam; UiT The Arctic University of Tromso
RP You, XG (corresponding author), Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China.; Yu, SJ (corresponding author), Vrije Univ Amsterdam, Dept Comp Sci, Amsterdam, Netherlands.; Yu, SJ (corresponding author), UiT Arctic Univ Norway, Machine Learning Grp, Tromso, Norway.
EM yusj9011@gmail.com; youxg@mail.hust.edu.cn
FU National Key R & D Program of China [2022YFC3301000]; Fundamental
   Research Funds for the Central Universities [HUST: 2023JYCXJJ031]
FX This work was supported in part by the National Key R & D Program of
   China 2022YFC3301000, in part by the Fundamental Research Funds for the
   Central Universities, HUST: 2023JYCXJJ031.
CR Achille A, 2018, J MACH LEARN RES, V19
   Ahuja K., 2023, Advances in Neural Information Processing Systems, P34
   Alemi A. A., 2017, INT C LEARNING REPRE
   Amjad RA, 2020, IEEE T PATTERN ANAL, V42, P2225, DOI 10.1109/TPAMI.2019.2909031
   Bang S, 2021, AAAI CONF ARTIF INTE, V35, P11396
   Belghazi MI, 2018, PR MACH LEARN RES, V80
   Chang DL, 2020, IEEE T IMAGE PROCESS, V29, P4683, DOI 10.1109/TIP.2020.2973812
   Cui Y, 2017, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR.2017.325
   Ding Y, 2019, IEEE I CONF COMP VIS, P6598, DOI 10.1109/ICCV.2019.00670
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Fu JL, 2017, PROC CVPR IEEE, P4476, DOI 10.1109/CVPR.2017.476
   Giraldo LGS, 2015, IEEE T INFORM THEORY, V61, P535, DOI 10.1109/TIT.2014.2370058
   Guo C, 2022, KNOWL-BASED SYST, V235, DOI 10.1016/j.knosys.2021.107651
   He J, 2022, AAAI CONF ARTIF INTE, P852
   Hu YQ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4239, DOI 10.1145/3474085.3475561
   Khosla Aditya E.A., 2011, PROC CVPR WORKSHOP F, V2
   Kim J, 2021, Arxiv, DOI arXiv:2103.12300
   Kim S, 2022, 39 INT C MACHINE LEA
   Kolchinsky A, 2019, ENTROPY-SWITZ, V21, DOI 10.3390/e21121181
   Lai QX, 2021, Arxiv, DOI arXiv:2108.03418
   Li W, 2023, ENG APPL ARTIF INTEL, V126, DOI 10.1016/j.engappai.2023.107123
   Li ZQ, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3595921
   Li ZX, 2023, IMAGE VISION COMPUT, V129, DOI 10.1016/j.imavis.2022.104591
   Liang Y., 2023, IEEE Transactions on Neural Networks and Learning Systems
   Lin TY, 2018, IEEE T PATTERN ANAL, V40, P1309, DOI 10.1109/TPAMI.2017.2723400
   Liu CB, 2020, IEEE T MULTIMEDIA, V22, P1785, DOI 10.1109/TMM.2019.2954747
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Luo W, 2020, IEEE SIGNAL PROC LET, V27, P1545, DOI 10.1109/LSP.2020.3020227
   Luo W, 2019, IEEE I CONF COMP VIS, P8241, DOI 10.1109/ICCV.2019.00833
   Miao Z, 2021, IEEE SIGNAL PROC LET, V28, P1983, DOI 10.1109/LSP.2021.3114622
   Min SB, 2020, IEEE T IMAGE PROCESS, V29, P4996, DOI 10.1109/TIP.2020.2977457
   Okamoto N, 2022, LECT NOTES COMPUT SC, V13671, P502, DOI [10.1007/978-3-031-20083-0_30, 10.1007/978-3-031-20083-030]
   Sadeghi K, 2020, IEEE T EM TOP COMP I, V4, P450, DOI [10.1109/TETCI.2020.2968933, 10.1109/tetci.2020.2968933]
   Saxe AM, 2019, J STAT MECH-THEORY E, V2019, DOI 10.1088/1742-5468/ab3985
   Shamir O, 2010, THEOR COMPUT SCI, V411, P2696, DOI 10.1016/j.tcs.2010.04.006
   Tian YR, 2019, PROC CVPR IEEE, P11008, DOI 10.1109/CVPR.2019.01127
   Tishby N, 2015, 2015 IEEE INFORMATION THEORY WORKSHOP (ITW)
   Tishby Naftali, 1999, ALL C COMM CONTR COM
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Van Horn G, 2015, PROC CVPR IEEE, P595, DOI 10.1109/CVPR.2015.7298658
   Wah C., 2023, The Caltech-ucsd birds- 200-2011 dataset
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang J, 2022, Arxiv, DOI arXiv:2107.02341
   Wei XS, 2022, IEEE T PATTERN ANAL, V44, P8927, DOI 10.1109/TPAMI.2021.3126648
   Xu Q, 2023, IEEE T MULTIMEDIA, V25, P9015, DOI 10.1109/TMM.2023.3244340
   Yang XH, 2022, PROC CVPR IEEE, P7389, DOI 10.1109/CVPR52688.2022.00725
   Yang Z, 2018, LECT NOTES COMPUT SC, V11218, P438, DOI 10.1007/978-3-030-01264-9_26
   Ye S., 2023, IEEE Transactions on Neural Networks and Learning Systems
   Ye S, 2023, Arxiv, DOI arXiv:2306.02346
   Yi JF, 2022, IEEE T EM TOP COMP I, V6, P1302, DOI 10.1109/TETCI.2022.3160702
   Yu X, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P3160, DOI 10.1109/ICASSP39728.2021.9414151
   Zhang J, 2022, IMAGE VISION COMPUT, V124, DOI 10.1016/j.imavis.2022.104513
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54
   Zhang XP, 2017, IEEE T MULTIMEDIA, V19, P2736, DOI 10.1109/TMM.2017.2710803
   Zhang YB, 2020, IEEE T MULTIMEDIA, V22, P1345, DOI 10.1109/TMM.2019.2939747
   Zhang Y, 2022, INT CONF ACOUST SPEE, P3234, DOI 10.1109/ICASSP43922.2022.9747591
   Zhao B, 2017, IEEE T MULTIMEDIA, V19, P1245, DOI 10.1109/TMM.2017.2648498
   Zhao YF, 2021, IEEE T IMAGE PROCESS, V30, P9470, DOI 10.1109/TIP.2021.3126490
   Zheng HL, 2017, IEEE I CONF COMP VIS, P5219, DOI 10.1109/ICCV.2017.557
   Zheng XT, 2021, IEEE T MULTIMEDIA, V23, P1187, DOI 10.1109/TMM.2020.2993960
   Zhmoginov A, 2021, LECT NOTES ARTIF INT, V12459, P531, DOI 10.1007/978-3-030-67664-3_32
   Zhu HW, 2022, PROC CVPR IEEE, P4682, DOI 10.1109/CVPR52688.2022.00465
   Zhu Lanyun, 2023, 2023 IEEE/CVF International Conference on Computer Vision (ICCV), P1621, DOI 10.1109/ICCV51070.2023.00156
   Zhu Q., 2023, Appl. Intell., P1
   Zhuang PQ, 2020, AAAI CONF ARTIF INTE, V34, P13130
NR 66
TC 1
Z9 1
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD MAR
PY 2024
VL 143
AR 104923
DI 10.1016/j.imavis.2024.104923
EA FEB 2024
PG 10
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA KZ5H1
UT WOS:001183799100001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Du, WC
   Chen, H
   Zhang, Y
   Yang, HY
AF Du, Wenchao
   Chen, Hu
   Zhang, Yi
   Yang, Hongyu
TI Hierarchical disentangled representation for image denoising and beyond
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Invertible image denoising; Bijective transformation; Disentangling
   learning; Hierarchical representation
ID LOW-DOSE CT; FRAMEWORK
AB Image denoising is a typical ill-posed problem due to complex degradation. Leading methods based on normalizing flows have tried to solve this problem with an invertible transformation instead of a deterministic mapping. However, it is difficult to construct feasible bijective mapping to remove spatial-variant noise while recovering fine texture and structure details due to latent ambiguity in inverse problems. Inspired by a common observation that noise tends to appear in the high-frequency part of the image, we propose a fully invertible denoising method that injects the idea of disentangled learning into a general invertible architecture to split noise from the high-frequency part. More specifically, we decompose the noisy image into clean low-frequency and hybrid high-frequency parts with an invertible transformation and then disentangle case-specific noise and high-frequency components in the latent space. In this way, denoising is made tractable by inversely merging noiseless low and high-frequency parts. Furthermore, we construct a flexible hierarchical disentangling framework, which aims to decompose most of the low-frequency image information while disentangling noise from the high-frequency part in a coarse-to-fine manner. Extensive experiments on real image denoising, JPEG compressed artifact removal, and medical low-dose CT image restoration have demonstrated that the proposed method achieves competitive performance on both quantitative metrics and visual quality, with significantly less computational cost.
C1 [Du, Wenchao; Chen, Hu; Yang, Hongyu] Sichuan Univ, Coll Comp Sci, Chengdu 610065, Sichuan, Peoples R China.
   [Zhang, Yi] Sichuan Univ, Coll Cyber Sci & Engn, Chengdu 610065, Peoples R China.
C3 Sichuan University; Sichuan University
RP Du, WC (corresponding author), Sichuan Univ, Coll Comp Sci, Chengdu 610065, Sichuan, Peoples R China.
EM wenchaodu.scu@gmail.com; huchen@scu.edu.cn; yzhang@scu.edu.cn;
   yanghongyu@scu.edu.cn
FU Natural Science Foundation of Sichuan Province of China [2023NSFSC1403];
   National Natural Science Foundation of China [62301345]; China
   Postdoctoral Sci- ence Foundation [2023M732426]; Sichuan University
   Postdoctoral R & D Start -up Fund [2023SCU12090]; Sichuan University
   Post- doctoral Interdisciplinary Innovation Fund [JCXK2233]
FX This work was supported by the Natural Science Foundation of Sichuan
   Province of China (No. 2023NSFSC1403) , National Natural Science
   Foundation of China (No. 62301345) , China Postdoctoral Sci- ence
   Foundation (No. 2023M732426) , Sichuan University Postdoctoral R & D
   Start -up Fund (No. 2023SCU12090) and Sichuan University Post- doctoral
   Interdisciplinary Innovation Fund (No. JCXK2233) .
CR Abdelhamed A, 2019, IEEE I CONF COMP VIS, P3165, DOI 10.1109/ICCV.2019.00326
   Abdelhamed A, 2018, PROC CVPR IEEE, P1692, DOI 10.1109/CVPR.2018.00182
   Anaya J, 2018, J VIS COMMUN IMAGE R, V51, P144, DOI 10.1016/j.jvcir.2018.01.012
   Anwar S, 2019, IEEE I CONF COMP VIS, P3155, DOI 10.1109/ICCV.2019.00325
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Ardizzone Lynton, 2018, INT C LEARN REPR
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Burger HC, 2012, PROC CVPR IEEE, P2392, DOI 10.1109/CVPR.2012.6247952
   Chen F, 2015, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2015.76
   Chen H, 2017, IEEE T MED IMAGING, V36, P2524, DOI 10.1109/TMI.2017.2715284
   Chen YJ, 2017, IEEE T PATTERN ANAL, V39, P1256, DOI 10.1109/TPAMI.2016.2596743
   Chen ZH, 2023, LECT NOTES COMPUT SC, V14229, P355, DOI 10.1007/978-3-031-43999-5_34
   Cheng S, 2021, PROC CVPR IEEE, P4894, DOI 10.1109/CVPR46437.2021.00486
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Deng J, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P3099, DOI 10.1145/2556288.2557011
   Dinh L., 2015, ICLR WORKSH
   Dinh L., 2017, INT C LEARN REPR
   Dong C, 2015, IEEE I CONF COMP VIS, P576, DOI 10.1109/ICCV.2015.73
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1618, DOI 10.1109/TIP.2012.2235847
   Dong WS, 2011, PROC CVPR IEEE, P457, DOI 10.1109/CVPR.2011.5995478
   Fan QN, 2021, IEEE T PATTERN ANAL, V43, P33, DOI 10.1109/TPAMI.2019.2925793
   Foi A, 2007, IEEE T IMAGE PROCESS, V16, P1395, DOI 10.1109/TIP.2007.891788
   Fu XY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4066, DOI 10.1109/ICCV48922.2021.00405
   Fu XY, 2019, IEEE I CONF COMP VIS, P2501, DOI 10.1109/ICCV.2019.00259
   Grathwohl Will, 2019, INT C LEARN REPR
   Gu SH, 2014, PROC CVPR IEEE, P2862, DOI 10.1109/CVPR.2014.366
   Guo LQ, 2024, IEEE T CIRC SYST VID, V34, P6105, DOI 10.1109/TCSVT.2023.3345667
   Guo S, 2019, PROC CVPR IEEE, P1712, DOI 10.1109/CVPR.2019.00181
   HR S., 2005, Live Image Quality Assessment Database Release 2
   Huang ZH, 2018, IET IMAGE PROCESS, V12, P254, DOI 10.1049/iet-ipr.2017.0518
   Huang ZH, 2018, BIOMED SIGNAL PROCES, V40, P131, DOI 10.1016/j.bspc.2017.09.019
   Jain V., 2008, NIPS
   Kaur L., 2002, ICVGIP
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kim Y, 2020, PROC CVPR IEEE, P3479, DOI 10.1109/CVPR42600.2020.00354
   Kingma D.P., 2018, Advances in neural information processing systems
   Kingma D.P., 2016, Advances in neural information processing systems, V29
   Lefkimmiatis S, 2018, PROC CVPR IEEE, P3204, DOI 10.1109/CVPR.2018.00338
   Li Y, 2014, LECT NOTES COMPUT SC, V8690, P174, DOI 10.1007/978-3-319-10605-2_12
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lienhart R, 2002, IEEE IMAGE PROC, P900
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu D, 2018, ADV NEUR IN, V31
   Liu PJ, 2018, IEEE COMPUT SOC CONF, P886, DOI 10.1109/CVPRW.2018.00121
   Liu W, 2020, IEEE COMPUT SOC CONF, P1742, DOI 10.1109/CVPRW50498.2020.00224
   Liu Y., 2021, Sensors (Basel, Switzerland), V22
   Liu Y, 2021, PROC CVPR IEEE, P13360, DOI 10.1109/CVPR46437.2021.01316
   Liu Y, 2020, IEEE COMPUT SOC CONF, P2140, DOI 10.1109/CVPRW50498.2020.00262
   Lugmayr Andreas, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P715, DOI 10.1007/978-3-030-58558-7_42
   Luthra A., 2021, arXiv
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P1004, DOI 10.1109/TIP.2016.2631888
   Mairal J, 2009, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2009.5459452
   Mao XJ, 2016, ADV NEUR IN, V29
   McCollough CH, 2017, MED PHYS, V44, pe339, DOI 10.1002/mp.12345
   Mingqing Xiao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P126, DOI 10.1007/978-3-030-58452-8_8
   Mitchell D.P., 1988, P 15 ANN C COMP GRAP
   Plötz T, 2017, PROC CVPR IEEE, P2750, DOI 10.1109/CVPR.2017.294
   Qian YQ, 2022, OPTIK, V261, DOI 10.1016/j.ijleo.2022.169089
   Roth S, 2005, PROC CVPR IEEE, P860
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Song Q, 2020, IEEE T IMAGE PROCESS, V29, P7399, DOI 10.1109/TIP.2020.3002452
   Sukthanker RS, 2022, PROC CVPR IEEE, P11224, DOI 10.1109/CVPR52688.2022.01095
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Tu ZZ, 2022, PROC CVPR IEEE, P5759, DOI 10.1109/CVPR52688.2022.00568
   van den Berg R, 2018, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P393
   Wang DY, 2023, PHYS MED BIOL, V68, DOI 10.1088/1361-6560/acc000
   Wang XT, 2019, LECT NOTES COMPUT SC, V11133, P63, DOI 10.1007/978-3-030-11021-5_5
   Wang ZD, 2022, PROC CVPR IEEE, P17662, DOI 10.1109/CVPR52688.2022.01716
   Wenchao Du, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14471, DOI 10.1109/CVPR42600.2020.01449
   Wu X., 2020, Unpaired Learning of Deep Image Denoising
   Xu J, 2015, IEEE I CONF COMP VIS, P244, DOI 10.1109/ICCV.2015.36
   Yang QS, 2018, IEEE T MED IMAGING, V37, P1348, DOI 10.1109/TMI.2018.2827462
   Yim C, 2011, IEEE T IMAGE PROCESS, V20, P88, DOI 10.1109/TIP.2010.2061859
   Yue ZS, 2019, ADV NEUR IN, V32
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zamir SW, 2021, PROC CVPR IEEE, P14816, DOI 10.1109/CVPR46437.2021.01458
   Zamir SW, 2020, PROC CVPR IEEE, P2693, DOI 10.1109/CVPR42600.2020.00277
   Zeyde R., Revised Selected Papers, V7, P711
   Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang Y., 2019, INT C LEARN REPR
   Zongsheng Yue, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P41, DOI 10.1007/978-3-030-58607-2_3
   Zoran D, 2011, IEEE I CONF COMP VIS, P479, DOI 10.1109/ICCV.2011.6126278
NR 83
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD SEP
PY 2024
VL 149
AR 105165
DI 10.1016/j.imavis.2024.105165
EA JUL 2024
PG 13
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA ZD4R8
UT WOS:001273347900001
DA 2024-08-05
ER

PT J
AU Polsinelli, M
   Li, HB
   Mignosi, F
   Zhang, L
   Placidi, G
AF Polsinelli, Matteo
   Li, Hongwei Bran
   Mignosi, Filippo
   Zhang, Li
   Placidi, Giuseppe
TI Siamese network to assess scanner-related contrast variability in MRI
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE MRI; MRI pre-processing; Deep learning; Siamese network; Explainable AI
ID NORMALIZATION TECHNIQUES
AB Magnetic Resonance Imaging (MRI) stands as a noninvasive tool for diagnosing and monitoring various diseases. The flexibility of MRI configuration parameters allows for adaptable imaging sequences, and at the same time poses challenges in terms of reproducibility, as variability in imaging sequences leads to significant differences in image contrast. This is one of the major causes that compromise the reliability of deep learning methods. Since the majority of the literature is focused on documenting the effects of this issue rather than delving into its underlying causes, this work follows a different approach. A Siamese Neural Network (SNN) has been trained to identify the scanner that acquired the input image. Experimental results include the use of Euclidean Distance (ED) and machine learning algorithms trained and tested using the feature vectors generated with the SNN. The results have shown that the proposed method is capable of distinguishing the scanner used for the acquisition with high accuracy. For a comprehensive interpretation of the results, the feature vectors have been dimensionality reduced and visualized with a 3D plot. Finally, the proposed method is sensitive to MR image contrast variability and could be used to detect data-related inconsistencies and provide a mechanism to make users aware of potential issues.
C1 [Polsinelli, Matteo] Univ Salerno, Dept Management & Innovat Syst, Fisciano, Italy.
   [Li, Hongwei Bran] Harvard Med Sch, Athinoula A Martinos Ctr Biomed Imaging, Boston, MA USA.
   [Mignosi, Filippo] Univ Aquila, Dept Informat Engn Comp Sci & Math, Laquila, Italy.
   [Zhang, Li] Peoples Liberat Army Air Force Engn Univ, Xian, Peoples R China.
   [Placidi, Giuseppe] Univ Aquila, Dept MeSVA, Lab A2VI, Laquila, Italy.
C3 University of Salerno; Harvard University; Harvard Medical School;
   University of L'Aquila; University of L'Aquila
RP Polsinelli, M (corresponding author), Univ Salerno, Dept Management & Innovat Syst, Fisciano, Italy.
EM mpolsinelli@unisa.it
RI Placidi, Giuseppe/R-4065-2019
OI Placidi, Giuseppe/0000-0002-4790-4029
CR Anowar F, 2021, COMPUT SCI REV, V40, DOI 10.1016/j.cosrev.2021.100378
   Bernstein MA., 2004, HDB MRI PULSE SEQUEN
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324
   Chu F, 2005, STUD FUZZ SOFT COMP, V177, P343
   Cohen JP, 2018, LECT NOTES COMPUT SC, V11070, P529, DOI 10.1007/978-3-030-00928-1_60
   De Marco F, 2022, IEEE INT CONF INF VI, P393, DOI 10.1109/IV56949.2022.00071
   De Marco F, 2022, PLOS ONE, V17, DOI 10.1371/journal.pone.0268555
   Dewey BE, 2019, MAGN RESON IMAGING, V64, P160, DOI 10.1016/j.mri.2019.05.041
   Di Biasi L, 2023, BMC BIOINFORMATICS, V24, DOI 10.1186/s12859-023-05516-5
   Fortin JP, 2017, NEUROIMAGE, V161, P149, DOI 10.1016/j.neuroimage.2017.08.047
   Glocker B, 2019, Arxiv, DOI arXiv:1910.04597
   Gonzalez R. C., Digital Image Processing
   Guan H, 2021, MED IMAGE ANAL, V71, DOI 10.1016/j.media.2021.102076
   Hawco C, 2022, SCI DATA, V9, DOI 10.1038/s41597-022-01386-3
   Hearst MA, 1998, IEEE INTELL SYST APP, V13, P18, DOI 10.1109/5254.708428
   Hosmer DW, 2013, WILEY SER PROBAB ST, P89
   Hu FL, 2023, NEUROIMAGE, V274, DOI 10.1016/j.neuroimage.2023.120125
   Isaksson LJ, 2020, PHYS MEDICA, V71, P7, DOI 10.1016/j.ejmp.2020.02.007
   Kouw WM, 2019, I S BIOMED IMAGING, P364, DOI [10.1109/isbi.2019.8759281, 10.1109/ISBI.2019.8759281]
   Liu MT, 2021, LECT NOTES COMPUT SC, V12903, P313, DOI 10.1007/978-3-030-87199-4_30
   Loh WY, 2011, WIRES DATA MIN KNOWL, V1, P14, DOI 10.1002/widm.8
   McInnes L, 2020, Arxiv, DOI [arXiv:1802.03426, 10.21105/joss.00861]
   Mi HL, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-76989-0
   Mucherino A, 2009, SPRINGER SER OPTIM A, V34, P83, DOI 10.1007/978-0-387-88615-2_4
   Naik GR, 2011, INFORM-J COMPUT INFO, V35, P63
   Placidi G, 2012, MRI: ESSENTIALS FOR INNOVATIVE TECHNOLOGIES, P1, DOI 10.1201/b11868
   Placidi G., 2021, Proceedings, V1, P255, DOI [10.1007/978-3-030-88163-4_23, DOI 10.1007/978-3-030-88163-4_23]
   Placidi G, 2020, ICPRAM: PROCEEDINGS OF THE 9TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION APPLICATIONS AND METHODS, P570, DOI 10.5220/0009150705700577
   Placidi G, 2019, LECT NOTES COMPUT SC, V11752, P367, DOI 10.1007/978-3-030-30645-8_34
   Polsinelli M, 2023, COMP MED SY, P535, DOI 10.1109/CBMS58004.2023.00275
   PyTorch, 2024, ABOUT US
   Sarker Iqbal H, 2021, SN Comput Sci, V2, P160, DOI 10.1007/s42979-021-00592-x
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Scikit-Learn, 2024, about us
   Shinohara RT, 2014, NEUROIMAGE-CLIN, V6, P9, DOI 10.1016/j.nicl.2014.08.008
   Singh A, 2016, PROCEEDINGS OF THE 10TH INDIACOM - 2016 3RD INTERNATIONAL CONFERENCE ON COMPUTING FOR SUSTAINABLE GLOBAL DEVELOPMENT, P1310
   Tan MX, 2019, PR MACH LEARN RES, V97
   WOLD S, 1987, CHEMOMETR INTELL LAB, V2, P37, DOI 10.1016/0169-7439(87)80084-9
   Yan WJ, 2020, RADIOL-ARTIF INTELL, V2, DOI 10.1148/ryai.2020190195
   Zebari R.R., 2020, A Comprehensive Review of Dimensionality Reduction Techniques for Feature Selection and Feature Extraction, P56, DOI [10.38094/jastt1224, DOI 10.38094/JASTT1224]
NR 41
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD MAY
PY 2024
VL 145
AR 104997
DI 10.1016/j.imavis.2024.104997
EA APR 2024
PG 9
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA RC0P9
UT WOS:001225358900001
OA hybrid
DA 2024-08-05
ER

PT J
AU Anand, A
   Bedi, J
   Aggarwal, A
   Khan, MA
   Rida, I
AF Anand, Ashima
   Bedi, Jatin
   Aggarwal, Ashutosh
   Khan, Muhammad Attique
   Rida, Imad
TI Authenticating and securing healthcare records: A deep learning-based
   zero watermarking approach
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Visible watermarking; Zero watermarking; Deep learning; Security;
   Authentication; Robust
AB Security in medical records is critical to patient privacy and confidentiality. Digital Patient Records (DPR) hold sensitive information that can reveal a patient's health status and history. Their unauthorized access or exposure can lead to severe consequences, including identity theft, discrimination, and medical malpractice. Therefore, ensuring proper security measures is critical in protecting DPR and other medical records from breaches or unauthorized access. In this regard, a robust deep learning-based zero-watermarking approach is presented for authenticating and securing healthcare records. The carrier image is initially visibly marked with the hospital logo to identify ownership and prevent illegal duplication and forgery. The image mark is scrambled by applying the step space-filling curve method for improved security. In the final phase, Alexnet is used to extract the features of visibly marked carrier image. Further, NSST and SVD-based zero watermarking is implemented to conceal the scrambled mark within the features of visibly marked carrier images. It is essential for copyright protection since it establishes ownership while preventing the unauthorized use or dissemination of valuable medical research, images, and reports. The proposed framework has exhibited superior versatility, robustness, and imperceptibility compared to existing techniques with a maximum improvement of 47%.
C1 [Anand, Ashima; Bedi, Jatin; Aggarwal, Ashutosh] Thapar Inst Engn & Technol, CSED, Patiala, Punjab, India.
   [Khan, Muhammad Attique] Lebanese Amer Univ, Dept Comp Sci & Math, Beirut, Lebanon.
   [Rida, Imad] Univ Technol Compiegne, Ctr Rech Royallieu, Lab Biomecan & Bioingn UMR 7338, Compiegne, France.
C3 Thapar Institute of Engineering & Technology; Lebanese American
   University; Universite de Technologie de Compiegne; Centre National de
   la Recherche Scientifique (CNRS); CNRS - Institute for Engineering &
   Systems Sciences (INSIS)
RP Bedi, J (corresponding author), Thapar Inst Engn & Technol, CSED, Patiala, Punjab, India.
EM ashima.anand@thapar.edu; jatin.bedi@thapar.edu;
   ashutosh.aggarwal@thapar.edu; attique.khan@ieee.org; imad.rida@utc.fr
RI Khan, Dr. Muhammad Attique/AAX-2644-2021
OI Khan, Dr. Muhammad Attique/0000-0002-6347-4890
CR Al-Dabbas H. M., 2023, Iraqi J. Sci., V64, P4169, DOI 10.24996/ijs.2023.64.8.37
   Anand A, 2023, IEEE T IND INFORM, V19, P849, DOI 10.1109/TII.2022.3172622
   Anand A, 2020, COMPUT COMMUN, V152, P72, DOI 10.1016/j.comcom.2020.01.038
   Awwad AMA, 2024, MULTIMED TOOLS APPL, DOI 10.1007/s11042-023-18024-8
   Cao F, 2024, EXPERT SYST APPL, V238, DOI 10.1016/j.eswa.2023.122062
   Chen YM, 2023, PROC CVPR IEEE, P8123, DOI 10.1109/CVPR52729.2023.00785
   Chunhua Dong, 2012, Proceedings of the 2012 IEEE International Conference on Computer Science and Automation Engineering (CSAE 2012), P22, DOI 10.1109/CSAE.2012.6272540
   Cox I., 2007, Morgan Kaufmann google schola, V2, P893
   Dong F., 2024, Deep Learning for Multimedia Processing Applications: Volume One: Image Security and Intelligent Systems for Multimedia Processing, V1, P45
   Dong FC, 2023, ELECTRONICS-SWITZ, V12, DOI 10.3390/electronics12163444
   Eldem H, 2023, ENG SCI TECHNOL, V45, DOI 10.1016/j.jestch.2023.101490
   Fan Y, 2023, CMC-COMPUT MATER CON, V74, P1279, DOI 10.32604/cmc.2023.031445
   Fang YX, 2022, MULTIMED TOOLS APPL, V81, P16863, DOI 10.1007/s11042-022-12592-x
   Franzen R., 2024, Kodak Lossless True Color Image Suite
   Han Baoru, 2023, IEEE J Biomed Health Inform, VPP, DOI 10.1109/JBHI.2023.3257340
   Huang TY, 2022, MATHEMATICS-BASEL, V10, DOI 10.3390/math10071154
   ibm, 2020, Security, Cost of a Data Breach Report 2020
   Image Processing Place, 2024, Image Databases
   Lang J, 2023, MULTIMED TOOLS APPL, V82, P4551, DOI 10.1007/s11042-022-13601-9
   Li DK, 2023, ELECTRONICS-SWITZ, V12, DOI 10.3390/electronics12071554
   Magdy M, 2022, IEEE ACCESS, V10, P38821, DOI 10.1109/ACCESS.2022.3165813
   Mahto DK, 2022, SOFT COMPUT, V26, P8105, DOI 10.1007/s00500-022-07155-z
   Nawaz SA, 2024, INFORM SCIENCES, V653, DOI 10.1016/j.ins.2023.119810
   Nawaz SA, 2023, COMPUT ELECTR ENG, V112, DOI 10.1016/j.compeleceng.2023.108985
   Raza K, 2021, CURR MED IMAGING, V17, P1059, DOI 10.2174/1573405617666210127154257
   Roustaei H, 2023, DATA BRIEF, V48, DOI 10.1016/j.dib.2023.109236
   Shen YX, 2024, EXPERT SYST APPL, V241, DOI 10.1016/j.eswa.2023.122547
   Sheng MS, 2023, CMC-COMPUT MATER CON, V75, P293, DOI 10.32604/cmc.2023.036438
   Shi H, 2023, MULTIMED TOOLS APPL, V82, P36507, DOI 10.1007/s11042-023-15074-w
   Singh AK, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3422816
   Singh NK, 2022, EXPERT SYST APPL, V199, DOI 10.1016/j.eswa.2022.116968
   Thakur S., 2018, Cryptographic and Information Security Approaches for Images and Videos, P467
   Wenham C, 2021, Feminist Global Health Security, DOI 10.1093/oso/9780197556931.001.0001
   Wu DY, 2023, J KING SAUD UNIV-COM, V35, DOI 10.1016/j.jksuci.2023.101708
   Yang JH, 2022, MULTIMED TOOLS APPL, V81, P20127, DOI 10.1007/s11042-022-12115-8
   Zhang WX, 2023, CMC-COMPUT MATER CON, V75, P565, DOI 10.32604/cmc.2023.036317
NR 36
TC 2
Z9 2
U1 10
U2 10
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD MAY
PY 2024
VL 145
AR 104975
DI 10.1016/j.imavis.2024.104975
EA MAR 2024
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA OO3D7
UT WOS:001208167700001
DA 2024-08-05
ER

PT J
AU Qu, HC
   Wang, XA
   Wang, Y
   Chen, Y
AF Qu, Haicheng
   Wang, Xiaona
   Wang, Ying
   Chen, Yao
TI Multi-branch residual image semantic segmentation combined with inverse
   weight gated-control
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Image semantic segmentation; Deep multi -branch residuals; Attention;
   Inverse weight gated -control; Contextual information
AB The loss of pixel-level information in the multi-class segmentation task based on the U-net model results in unclear boundaries and low semantic segmentation accuracy. Aiming at this, a deep multi-branch residual Unet (IWG-MRUN) with fused inverse weight gated-control is proposed to improve the quality of image semantic segmentation. Specifically, we first introduce a deep multi-branch residual module, which used parallel convolution mode to capture the contextual feature to extract the detailed features of the input image at a deeper level. Then, we adopt an inverse weight gated-control module to enhance the diversity of up-sampling information by counterclockwise transmitting attention horizontally to improve the restoration accuracy of upsampled image pixels. Finally, to obtain finer granularity features from low spatial resolution images, we adopt the different receptive field pyramid attention mechanisms at the highest level of the U-shaped encoder to capture high-level context information at different scales, thereby improving the accuracy of semantic segmentation. The experimental results show that the segmentation accuracy of the proposed algorithm reaches 91.80% and the CCE loss is reduced to 0.21. When compared to the Unet, BiSeNet, DeeplabV3+ and U-net + BLR model, the pixel accuracy of semantic segmentation is improved by 15.0%, 1.98%, 0.9% and 6.5%, respectively. The semantic segmentation model proposed in this paper provides an end-to-end semantic segmentation capability with the enriched finer granularity features of the target boundary and realizes the accurate segmentation of the objects in different categories.
C1 [Qu, Haicheng; Wang, Xiaona; Wang, Ying] Liaoning Tech Univ, Sch Software, Huludao 125105, Peoples R China.
   [Chen, Yao] Natl Univ Singapore, Sch Comp, Singapore 119077, Singapore.
C3 Liaoning Technical University; National University of Singapore
RP Qu, HC (corresponding author), Liaoning Tech Univ, Sch Software, Huludao 125105, Peoples R China.
EM quhaicheng@lntu.edu.cn
FU National Natural Science Foundation of China [42271409]; Scientific
   Research Foundation of the Higher Education Institutions of Liaoning
   Province [LJKMZ20220699]
FX This paper was supported by the National Natural Science Foundation of
   China (Grant No.42271409) and the Scientific Research Foundation of the
   Higher Education Institutions of Liaoning Province (Grant
   No.LJKMZ20220699) .
CR Taghanaki SA, 2021, ARTIF INTELL REV, V54, P137, DOI 10.1007/s10462-020-09854-1
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Brostow GJ, 2009, PATTERN RECOGN LETT, V30, P88, DOI 10.1016/j.patrec.2008.04.005
   Chen L.C., 2014, ARXIV PREPRINT ARXIV, V6, P357, DOI DOI 10.48550/ARXIV.1412.7062
   Chen L.-C., 2018, ECCV, P801, DOI [DOI 10.1007/978-3-030-01234-249, 10.1007/978-3-030-01234-2_49]
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Fengting Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13961, DOI 10.1109/CVPR42600.2020.01398
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang Y, 2021, NEUROCOMPUTING, V443, P26, DOI 10.1016/j.neucom.2021.02.091
   Kendall A., 2017, P BRIT MACH VIS C BM, V57, P1
   Koul A, 2020, 2020 3RD INTERNATIONAL CONFERENCE ON INFORMATION AND COMPUTER TECHNOLOGIES (ICICT 2020), P148, DOI 10.1109/ICICT50521.2020.00031
   Li HC, 2018, Arxiv, DOI [arXiv:1805.10180, 10.48550/arXiv.1805.10180]
   [刘青 Liu Qing], 2023, [激光与红外, Laser and Infrared], V53, P1288
   Manoila C.P., 2022, 2022 E HLTH BIOENG C, P01
   Peng CL, 2020, PATTERN RECOGN, V107, DOI 10.1016/j.patcog.2020.107498
   [青晨 Qing Chen], 2020, [中国图象图形学报, Journal of Image and Graphics], V25, P1069
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Seo H, 2020, MED PHYS, V47, pE148, DOI 10.1002/mp.13649
   Wang X, 2021, CHIN J LIQ CRYST DIS, V36, P475, DOI 10.37188/CJLCD.2020-0116
   Wang X, 2020, NEUROCOMPUTING, V381, P20, DOI 10.1016/j.neucom.2019.11.019
   Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20
   Yu CQ, 2018, PROC CVPR IEEE, P1857, DOI 10.1109/CVPR.2018.00199
   Yuan Y, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3321512
   Zhang JM, 2022, PROC CVPR IEEE, P16896, DOI 10.1109/CVPR52688.2022.01641
   Zhang M, 2020, ARTIF INTELL REV, V53, P4259, DOI 10.1007/s10462-019-09792-7
   Zhang ZP, 2019, IEEE ACM T COMPUT BI, V16, P407, DOI 10.1109/TCBB.2017.2704587
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11213, P270, DOI 10.1007/978-3-030-01240-3_17
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11207, P418, DOI 10.1007/978-3-030-01219-9_25
NR 29
TC 0
Z9 0
U1 8
U2 8
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD MAR
PY 2024
VL 143
AR 104932
DI 10.1016/j.imavis.2024.104932
EA FEB 2024
PG 11
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA LQ7K1
UT WOS:001188331100001
DA 2024-08-05
ER

PT J
AU Xu, M
   Lee, J
   Yoon, S
   Kim, H
   Park, DS
AF Xu, Mingle
   Lee, Jaehwan
   Yoon, Sook
   Kim, Hyongsuk
   Park, Dong Sun
TI Variation-aware semantic image synthesis
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Semantic image synthesis; Image variations; Generative adversarial
   networks; Conditional normalization
AB Semantic image synthesis (SIS) aims to produce photorealistic images aligning to given conditional semantic layout and has witnessed a significant improvement in recent years. Although the diversity in image-level has been discussed heavily, class-level mode collapse widely exists in current algorithms. Therefore, we declare a new requirement for SIS to achieve more photorealistic images, variation-aware, which consists of inter- and intra-class variation. The inter-class variation is the diversity between different semantic classes while the intraclass variation stresses the diversity inside one class. Through analysis, we find that current algorithms elusively embrace the inter-class variation but the intra-class variation is still not enough. Further, we introduce two simple methods to achieve variation-aware semantic image synthesis (VASIS) with a higher intra-class variation, semantic noise and position code. We combine our method with several state-of-the-art algorithms and the experimental result shows that our models generate more natural images and achieve slightly better FIDs and/or mIoUs than the counterparts. Our codes and models will be publicly available.
C1 [Xu, Mingle; Lee, Jaehwan; Kim, Hyongsuk; Park, Dong Sun] Jeonbuk Natl Univ, Core Res Inst Intelligent Robots, Dept Elect Engn, Jeonju, South Korea.
   [Yoon, Sook] Mokpo Natl Univ, Dept Comp Engn, Mokpo, South Korea.
C3 Jeonbuk National University; Mokpo National University
RP Yoon, S (corresponding author), Mokpo Natl Univ, Dept Comp Engn, Mokpo, South Korea.
EM syoon@mokpo.ac.kr; dspark@jbnu.ac.kr
FU National Research Foundation of Korea (NRF) - Ministry of Education
   [2019R1A6A1A09031717]; National Research Foundation of Korea (NRF) -
   Ministry of Science and ICT (MSIT) [2020R1A2C2013060]; Korea Institute
   of Planning and Evaluation for Technology in Food, Agriculture and
   Forestry (IPET); Korea Smart Farm R & D Foundation (KosFarm) - Ministry
   of Agriculture, Food and Rural Affairs (MAFRA); Ministry of Science and
   ICT (MSIT) , Rural Development Administration (RDA) [1545027423]
FX This research was partly supported by the Basic Science Research Program
   through the National Research Foundation of Korea (NRF) funded by the
   Ministry of Education (No.2019R1A6A1A09031717) , supported by the
   National Research Foundation of Korea (NRF) grant funded by the Ministry
   of Science and ICT (MSIT) (No. 2020R1A2C2013060) , and supported by
   Korea Institute of Planning and Evaluation for Technology in Food,
   Agriculture and Forestry (IPET) and Korea Smart Farm R & D Foundation
   (KosFarm) through Smart Farm Innovation Technology Development Program,
   funded by Ministry of Agriculture, Food and Rural Affairs (MAFRA) and
   Ministry of Science and ICT (MSIT) , Rural Development Administration
   (RDA) (1545027423) .
CR Ba JL, 2016, ARXIV
   Brock A., 2018, P INT C LEARN REPR
   Caesar H, 2018, PROC CVPR IEEE, P1209, DOI 10.1109/CVPR.2018.00132
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen X, 2016, ADV NEUR IN, V29
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Devlin J, 2018, ARXIV
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Gehring J, 2017, PR MACH LEARN RES, V70
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Li YH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14398, DOI 10.1109/ICCV48922.2021.01415
   Liu R, 2018, ADV NEUR IN, V31
   Liu XH, 2019, ADV NEUR IN, V32
   Lv ZY, 2022, PROC CVPR IEEE, P11204, DOI 10.1109/CVPR52688.2022.01093
   Mao Q, 2019, PROC CVPR IEEE, P1429, DOI 10.1109/CVPR.2019.00152
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Radford A, 2016, Arxiv, DOI [arXiv:1511.06434, DOI 10.48550/ARXIV.1511.06434]
   Salimans T, 2016, ADV NEUR IN, V29
   Schonfeld E., 2020, INT C LEARN REPR
   Shaham TR, 2021, PROC CVPR IEEE, P14877, DOI 10.1109/CVPR46437.2021.01464
   Shaw P, 2018, Arxiv, DOI [arXiv:1803.02155, 10.48550/arXiv.1803.02155]
   Sushko V, 2022, INT J COMPUT VISION, V130, P2903, DOI 10.1007/s11263-022-01673-x
   Tan ZT, 2023, IEEE T PATTERN ANAL, V45, P6247, DOI 10.1109/TPAMI.2022.3210085
   Tan ZT, 2021, PROC CVPR IEEE, P7958, DOI 10.1109/CVPR46437.2021.00787
   Tan ZT, 2022, IEEE T PATTERN ANAL, V44, P4852, DOI 10.1109/TPAMI.2021.3076487
   Tang H, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1994, DOI 10.1145/3394171.3416270
   Tang Hao, 2020, P IEEE CVF C COMP VI, P7867
   Ulyanov D, 2017, Arxiv, DOI arXiv:1607.08022
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wu K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10013, DOI 10.1109/ICCV48922.2021.00988
   Wu YX, 2020, INT J COMPUT VISION, V128, P742, DOI [10.1007/s11263-019-01198-w, 10.1109/CSTIC.2018.8369274]
   Xu M., 2022, IEEE Access
   Xu M, 2022, Arxiv, DOI arXiv:2205.01491
   Yu F, 2017, PROC CVPR IEEE, P636, DOI 10.1109/CVPR.2017.75
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhou BL, 2019, INT J COMPUT VISION, V127, P302, DOI 10.1007/s11263-018-1140-0
   Zhou BL, 2017, PROC CVPR IEEE, P5122, DOI 10.1109/CVPR.2017.544
   Zhu JY, 2017, ADV NEUR IN, V30
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu PH, 2020, PROC CVPR IEEE, P5103, DOI 10.1109/CVPR42600.2020.00515
   Zhu Z, 2020, PROC CVPR IEEE, P5466, DOI 10.1109/CVPR42600.2020.00551
NR 51
TC 0
Z9 0
U1 1
U2 1
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD FEB
PY 2024
VL 142
AR 104914
DI 10.1016/j.imavis.2024.104914
EA JAN 2024
PG 9
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA JJ0D4
UT WOS:001172670900001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Shao, MW
   Peng, ZL
AF Shao, Mingwen
   Peng, Zilu
TI Distance metric-based learning for long-tail object detection
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Deep convolutional neural network; Object detection; Long -tail
   distribution; Metric learning; Feature extraction
ID SMOTE
AB Despite the recent success of general object detection, almost all models perform unsatisfactorily on long-tail datasets. The main cause of performance degradation is the imbalance in the number of positive samples between categories. The traditional approaches can lead to distortion of the classification feature space, which in turn can seriously affect the classification ability of the network. To address the above issues, we propose a novel distance metric-based learning approach for long-tail object detection (LTDL) in this paper. Specifically, we directly use the feature space as the optimization target, thus allowing clearer decision boundaries between classes. In order to optimize the decision boundary, we adjust the intra-class and inter-class distances by Margin Module (MAM). Meanwhile, to further exploit the information provided by the dataset, we introduce supervised information of labels for distance weighting using the Semantic Module (SEM). In addition, to protect the learning of tail samples and optimize the classifier, we propose a Distance-based Equilibrium Loss (DEL). Extensive experiments conducted on the LVIS benchmark have demonstrated the strength of our proposed approach. The experimental results show that our method improves the baseline by 2.9% AP. And our best model can outperform almost all other representative methods.
C1 [Shao, Mingwen] Quanzhou Vocat & Tech Univ, Natl Sci Digital Ind Coll, Jinjiang 362000, Peoples R China.
   [Shao, Mingwen; Peng, Zilu] China Univ Petr, Sch Comp Sci & Technol, Qingdao 266580, Peoples R China.
C3 China University of Petroleum
RP Peng, ZL (corresponding author), China Univ Petr, Sch Comp Sci & Technol, Qingdao 266580, Peoples R China.
EM 709117316@qq.com
FU National Key Research and development Program of China [2021YFA1000102];
   National Natural Science Foundation of China [62376285, 62272375,
   61673396]; Natural Science Foundation of Shandong Province, China
   [ZR2022MF260]
FX <BOLD>Acknowledgements</BOLD> This work was supported by National Key
   Research and development Program of China (2021YFA1000102) , the grants
   from the National Natural Science Foundation of China (Nos. 62376285,
   62272375, 61673396) , Natural Science Foundation of Shandong Province,
   China (No. ZR2022MF260) .
CR Alexandridis KP, 2022, LECT NOTES COMPUT SC, V13670, P353, DOI 10.1007/978-3-031-20080-9_21
   Boyan Zhou, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9716, DOI 10.1109/CVPR42600.2020.00974
   Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644
   Cao KD, 2019, ADV NEUR IN, V32
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953
   Chen K, 2019, Arxiv, DOI arXiv:1906.07155
   Cui Y, 2019, PROC CVPR IEEE, P9260, DOI 10.1109/CVPR.2019.00949
   Drummond Chris, 2003, Workshop on learning from imbalanced datasets II, International Conference on Machine Learning, V11, P1
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fan BB, 2022, INT J MACH LEARN CYB, V13, P2189, DOI 10.1007/s13042-022-01514-w
   Feng CJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3397, DOI 10.1109/ICCV48922.2021.00340
   Gupta A, 2019, PROC CVPR IEEE, P5351, DOI 10.1109/CVPR.2019.00550
   Han H, 2005, LECT NOTES COMPUT SC, V3644, P878, DOI 10.1007/11538059_91
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Jiang CM, 2022, LECT NOTES COMPUT SC, V13670, P158, DOI 10.1007/978-3-031-20080-9_10
   Jingru Tan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11659, DOI 10.1109/CVPR42600.2020.01168
   Kang Bingyi, 2019, arXiv
   Kang Kim, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P355, DOI 10.1007/978-3-030-58595-2_22
   Law H, 2020, INT J COMPUT VISION, V128, P642, DOI 10.1007/s11263-019-01204-1
   Li B, 2022, PROC CVPR IEEE, P6980, DOI 10.1109/CVPR52688.2022.00686
   Li X, 2021, PROC CVPR IEEE, P11627, DOI 10.1109/CVPR46437.2021.01146
   Li YH, 2022, SIGNAL IMAGE VIDEO P, V16, P705, DOI 10.1007/s11760-021-02010-4
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Mahajan D, 2018, LECT NOTES COMPUT SC, V11206, P185, DOI 10.1007/978-3-030-01216-8_12
   Minderer M, 2022, LECT NOTES COMPUT SC, V13670, P728, DOI 10.1007/978-3-031-20080-9_42
   Pan T.-Y., 2021, P INT C ADV NEUR INF, V34, P2529
   Redmon J., 2018, CoRR
   Reed WJ, 2001, ECON LETT, V74, P15, DOI 10.1016/S0165-1765(01)00524-9
   Ren J., 2020, ADV NEURAL INFORM PR, V33, P4175, DOI DOI 10.48550/ARXIV.2007.10740
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shen L, 2016, LECT NOTES COMPUT SC, V9911, P467, DOI 10.1007/978-3-319-46478-7_29
   Tan JR, 2021, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR46437.2021.00173
   Tao Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P728, DOI 10.1007/978-3-030-58568-6_43
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang JQ, 2021, PROC CVPR IEEE, P9690, DOI 10.1109/CVPR46437.2021.00957
   Wang LG, 2024, GEOSCI DATA J, V11, P237, DOI 10.1002/gdj3.162
   Wang T, 2022, PROC CVPR IEEE, P6970, DOI 10.1109/CVPR52688.2022.00685
   Wang T, 2021, PROC CVPR IEEE, P3102, DOI 10.1109/CVPR46437.2021.00312
   Wu JL, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1570, DOI 10.1145/3394171.3413970
   Xu ZZ, 2023, PROC CVPR IEEE, P15793, DOI 10.1109/CVPR52729.2023.01516
   Yu Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10988, DOI 10.1109/CVPR42600.2020.01100
   Yu WP, 2021, IEEE WINT CONF APPL, P3257, DOI 10.1109/WACV48630.2021.00330
   Zhang HY, 2021, PROC CVPR IEEE, P8510, DOI 10.1109/CVPR46437.2021.00841
   Zhang S., 2020, P IEEE CVF C COMP VI, P9759
   Zhou XY, 2019, Arxiv, DOI arXiv:1904.07850
NR 51
TC 0
Z9 0
U1 5
U2 5
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD FEB
PY 2024
VL 142
AR 104888
DI 10.1016/j.imavis.2023.104888
EA JAN 2024
PG 9
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA GN4P3
UT WOS:001153338500001
DA 2024-08-05
ER

PT J
AU Fayyad, J
   Gupta, K
   Mahdian, N
   Gruyer, D
   Najjaran, H
AF Fayyad, Jamil
   Gupta, Kashish
   Mahdian, Navid
   Gruyer, Dominique
   Najjaran, Homayoun
TI Exploiting classifier inter-level features for efficient
   out-of-distribution detection
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Out -of -distribution detection; Deep learning -based classification;
   Machine learning; Feature exploitation; Intermediate feature extraction
AB Deep learning approaches have achieved state-of-the-art performance in a wide range of applications. Most often, however, it is falsely assumed that samples at inference follow a similar distribution as the training data. This assumption impairs models' ability to handle Out-of-Distribution (OOD) data during deployment. While several OOD detection approaches mostly focus on outputs of the last layer, we propose a novel mechanism that exploits features extracted from intermediate layers of a deep classifier. Specifically, we train an off-the-shelf auxiliary network using features of early layers to learn distinctive representations that improve OOD detection. The proposed network can be appended to any classification model without imposing any modification to its original architecture. Additionally, the mechanism does not require access to OOD data during training. We evaluate the performance of the mechanism on a variety of backbone architectures and datasets for near-OOD and far-OOD scenarios. The results demonstrate improvements in OOD detection compared to other state-of-the-art approaches. In particular, our proposed mechanism improves AUROC by 14.2% and 8.3% in comparison to the strong OOD baseline method, and by 3.2% and 3.9% in comparison to the second-best performing approach, on CIFAR-10 and CIFAR-100 datasets respectively.
C1 [Fayyad, Jamil] Univ British Columbia, Sch Engn, 3333 Univ Way, Kelowna, BC V1V 1V7, Canada.
   [Gupta, Kashish; Mahdian, Navid; Najjaran, Homayoun] Univ Victoria, Fac Engn & Comp Sci, 3800 Finnerty Rd, Victoria, BC V8P 5C2, Canada.
   [Gruyer, Dominique] Univ Gustave Eiffel, PICS L COSYS, IFSTTAR, 25 Marronniers, F-78000 Champs Sur Marne, France.
C3 University of British Columbia; University of Victoria; Universite
   Gustave-Eiffel
RP Najjaran, H (corresponding author), Univ Victoria, Fac Engn & Comp Sci, 3800 Finnerty Rd, Victoria, BC V8P 5C2, Canada.
EM jfayyad@mail.ubc.ca; kashishg@uvic.ca; navidmahdian@uvic.ca;
   dominique.gruyer@univ-eiffel.fr; najjaran@uvic.ca
FU MITACS [16254]; ROSEN Group
FX The authors would like to acknowledge the financial support received by
   Jamil Fayyad from MITACS under the Accelerate Program Internship No.
   16254 and ROSEN Group.
CR Amini Alexander, 2020, ADV NEURAL INFORM PR, V33, P14927, DOI DOI 10.5555/3495724.3496975
   Bevandic P, 2022, IMAGE VISION COMPUT, V124, DOI 10.1016/j.imavis.2022.104490
   Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773
   DeVries T, 2018, Arxiv, DOI arXiv:1802.04865
   Fayyad J., 2023, Ph.D. thesis
   Fayyad J, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20154220
   Fei Geli, 2016, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, P506
   Furui S, 2012, IEEE SIGNAL PROC MAG, V29, P16, DOI 10.1109/MSP.2012.2209906
   Gal Y, 2016, PR MACH LEARN RES, V48
   GAMERMAN D, 2006, Markov chain Monte Carlo: stochastic simulation for Bayesian inference
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Geng CX, 2021, IEEE T PATTERN ANAL, V43, P3614, DOI 10.1109/TPAMI.2020.2981604
   Guo JY, 2020, IEEE T INTELL TRANSP, V21, P3135, DOI 10.1109/TITS.2019.2926042
   Guo Z, 2022, Arxiv, DOI [arXiv:2206.05675, 10.48550/arXiv.2206.05675]
   Hendrycks D, 2022, PR MACH LEARN RES
   Hendrycks D, 2018, Arxiv, DOI arXiv:1610.02136
   Hüllermeier E, 2021, MACH LEARN, V110, P457, DOI 10.1007/s10994-021-05946-3
   Kaiyang Zhou, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P561, DOI 10.1007/978-3-030-58517-4_33
   Kendall A, 2017, 31 ANN C NEURAL INFO, V30
   Krizhevsky A., 2009, Learning multiple layers of features from tiny images
   Lakshminarayanan B, 2017, ADV NEUR IN, V30
   Lee KM, 2018, Arxiv, DOI arXiv:1711.09325
   Lee K, 2018, ADV NEUR IN, V31
   Li D, 2018, AAAI CONF ARTIF INTE, P3490
   Liang S., 2018, ICLR
   Luo ZM, 2018, IEEE T IMAGE PROCESS, V27, P5129, DOI 10.1109/TIP.2018.2848705
   Macêdo D, 2022, Arxiv, DOI arXiv:2205.05874
   Macêdo D, 2022, IEEE T NEUR NET LEAR, V33, P2350, DOI 10.1109/TNNLS.2021.3112897
   Macedo David, 2021, arXiv
   Malinin A, 2018, ADV NEUR IN, V31
   Mena J, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3477140
   Osawa K, 2019, ADV NEUR IN, V32
   Paschali M, 2018, LECT NOTES COMPUT SC, V11070, P493, DOI 10.1007/978-3-030-00928-1_56
   Ran XM, 2022, NEURAL NETWORKS, V145, P199, DOI 10.1016/j.neunet.2021.10.020
   Sastry C. S., 2020, PMLR, P8491
   Schwaiger A., 2020, AISAFETY IJCAI
   Sensoy M, 2018, Arxiv, DOI arXiv:1806.01768
   Sun YY, 2022, LECT NOTES COMPUT SC, V13684, P691, DOI 10.1007/978-3-031-20053-3_40
   Sun YY, 2021, ADV NEUR IN, V34
   Tack J., 2020, Advances in neural information processing systems, P11839
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Wang K, 2011, IEEE I CONF COMP VIS, P1457, DOI 10.1109/ICCV.2011.6126402
   Wang L, 2022, IMAGE VISION COMPUT, V126, DOI 10.1016/j.imavis.2022.104548
   Wason R, 2018, COGN SYST RES, V52, P701, DOI 10.1016/j.cogsys.2018.08.023
   Xiao H, 2017, Arxiv, DOI [arXiv:1708.07747, DOI 10.48550/ARXIV.1708.07747]
   Yang JK, 2024, Arxiv, DOI [arXiv:2110.11334, DOI 10.48550/ARXIV.2110.11334, 10.48550/arXiv.2110.11334]
   Yang Jingkang, 2022, arXiv
   Yen-Chang Hsu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10948, DOI 10.1109/CVPR42600.2020.01096
   Yosinski J, 2014, ADV NEUR IN, V27
   Yu Q, 2019, IEEE I CONF COMP VIS, P9517, DOI 10.1109/ICCV.2019.00961
   Zhihe Lu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9108, DOI 10.1109/CVPR42600.2020.00913
   Ziwei Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12403, DOI 10.1109/CVPR42600.2020.01242
NR 52
TC 1
Z9 1
U1 2
U2 2
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD FEB
PY 2024
VL 142
AR 104897
DI 10.1016/j.imavis.2023.104897
EA JAN 2024
PG 7
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA GX6J3
UT WOS:001156008200001
DA 2024-08-05
ER

PT J
AU Variyar, VVS
   Sowmya, V
   Sivanpillai, R
   Brown, GK
AF Variyar, V. V. Sajith
   Sowmya, V.
   Sivanpillai, Ramesh
   Brown, Gregory K.
TI A multi-branch dual attention segmentation network for epiphyte drone
   images
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE UAV image segmentation; Multi-branch network; Dual attention; Low
   samples; Mixed quality
ID NET
AB Acquiring images of epiphytes growing on trees using Unmanned Aerial Vehicles (UAVs) enables botanists to efficiently collect data on these important plant species. Despite the advantages offered by UAVs, challenges such as complex backgrounds, uneven lighting inside the tree canopy, and accessibility issues hinder the acquisition of quality images, resulting in acquiring images datasets of heterogenous quality. AI/Deep Learning algorithms can be used to segment target plants in these images for selecting sampling locations. Existing DL models require large volume of data for training, and they tend to prioritize local features over global ones, impacting segmentation accuracy, particularly on smaller, heterogeneous quality image datasets. To overcome these limitations, we propose a multi-branch dual attention segmentation network designed to effectively handle small datasets with heterogeneous quality. The proposed network incorporates dedicated branches for extracting both global and local features, utilizing spatial and channel attention mechanisms to focus on important regions. Through a fusion process and a decoder with crossed fusion technique, this network effectively combines and enhances features from multiple branches, resulting in improved segmentation performance. Output obtained from the trained model demonstrated major improvements in predicting the boundary regions and class labels, even in close-range, low-light, and zoomed/cropped images. The average Intersection over Union (IoU) scores of the trained model was 5% higher for images acquired close range, 48% higher for images in low-light conditions, and 68% higher for zoomed/cropped images when compared to those obtained from TransUnet, a state-of-the-art vision transformer model trained on epiphyte dataset. The proposed network can be used for segmenting epiphytes in images of heterogeneous quality as well as identifying targets in images acquired in domains such as agriculture and forestry.
C1 [Variyar, V. V. Sajith; Sowmya, V.] Amrita Vishwa Vidyapeetham, Amrita Sch Artificial Intelligence, Coimbatore 641112, India.
   [Sivanpillai, Ramesh] Univ Wyoming, Wyoming GIS Ctr, Sch Comp, Laramie, WY 82071 USA.
   [Brown, Gregory K.] Univ Wyoming, Dept Bot, Laramie, WY 82071 USA.
C3 Amrita Vishwa Vidyapeetham; Amrita Vishwa Vidyapeetham Coimbatore;
   University of Wyoming; University of Wyoming
RP Sowmya, V (corresponding author), Amrita Vishwa Vidyapeetham, Amrita Sch Artificial Intelligence, Coimbatore 641112, India.
EM vv_sajithvariyar@cb.amrita.edu; v_sowmya@cb.amrita.edu; sivan@uwyo.edu;
   gkbrown@uwyo.edu
FU Amrita School of Artificial Intelligence, Coimbatore
FX The authors would like to thank Professor K.P Soman, Dean, Amrita School
   of Artificial Intelligence, Coimbatore, Amrita Vishwa Vidyapee- tham,
   India for all the support and guidance, and our students Shashank
   Anivilla, Anjana K. Menon, and Rahesh Ravi for labelling the epiphytes
   in the UAV photos used in this study.
CR Bai Y, 2023, ELECTRONICS-SWITZ, V12, DOI 10.3390/electronics12030674
   Chen J., 2021, TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation
   Chen MH, 2023, PATTERN RECOGN, V135, DOI 10.1016/j.patcog.2022.109168
   Cheng Y, 2023, INT J APPL EARTH OBS, V124, DOI 10.1016/j.jag.2023.103514
   Chowdhury Tashnim, 2021, 2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS, P2325, DOI 10.1109/IGARSS47720.2021.9553712
   Dosovitskiy A., 2020, arXiv, DOI DOI 10.48550/ARXIV
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Girisha S, 2021, IEEE J-STARS, V14, P4115, DOI 10.1109/JSTARS.2021.3069909
   Hao Zhang, 2019, Pattern Recognition and Computer Vision. Second Chinese Conference, PRCV 2019. Proceedings. Lecture Notes in Computer Science (LNCS 11857), P611, DOI 10.1007/978-3-030-31654-9_52
   Huan H, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2021.3137522
   Huang HZ, 2023, FORESTS, V14, DOI 10.3390/f14030549
   James K, 2020, METHODS ECOL EVOL, V11, P1509, DOI 10.1111/2041-210X.13473
   Jin SZ, 2023, SCI REP-UK, V13, DOI 10.1038/s41598-023-33357-y
   Liu GQ, 2022, J APPL REMOTE SENS, V16, DOI 10.1117/1.JRS.16.034511
   Liu YJ, 2020, IEEE ACCESS, V8, P145740, DOI 10.1109/ACCESS.2020.3014910
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lv LJ, 2023, REMOTE SENS-BASEL, V15, DOI 10.3390/rs15184420
   Meng JY, 2023, Arxiv, DOI [arXiv:2306.14097, 10.48550/arXiv.2306.14097, DOI 10.48550/ARXIV.2306.14097]
   Millner N., 2023, Global Social Challenges J., V2, P2, DOI [10.1332/TIOK6806, DOI 10.1332/TIOK6806]
   Mou LC, 2020, IEEE T GEOSCI REMOTE, V58, P7557, DOI 10.1109/TGRS.2020.2979552
   Narisetti N, 2022, FRONT PLANT SCI, V13, DOI 10.3389/fpls.2022.906410
   Nduku L, 2023, GEOMATICS-BASEL, V3, P115, DOI 10.3390/geomatics3010006
   Osco LP, 2021, INT J APPL EARTH OBS, V102, DOI 10.1016/j.jag.2021.102456
   Qiu LY, 2023, REMOTE SENS-BASEL, V15, DOI 10.3390/rs15010231
   Ren K, 2022, ASIAPAC SIGN INFO PR, P520, DOI 10.23919/APSIPAASC55919.2022.9980160
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Saire D, 2022, IEEE ACCESS, V10, P77323, DOI 10.1109/ACCESS.2022.3192605
   Shi X, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2022.3197319
   Siddique N, 2021, IEEE ACCESS, V9, P82031, DOI 10.1109/ACCESS.2021.3086020
   Sivanpillai G.K., 2019, Applications of Small Unmanned Aircraft Systems, V1, P269, DOI [10.1201/9780429244117, DOI 10.1201/9780429244117]
   Steenvoorden J, 2023, INT J APPL EARTH OBS, V117, DOI 10.1016/j.jag.2023.103220
   Su ZB, 2022, COMPUT ELECTRON AGR, V196, DOI 10.1016/j.compag.2022.106873
   Tan YH, 2020, NEUROCOMPUTING, V396, P358, DOI 10.1016/j.neucom.2018.09.106
   Variyar VVS, 2023, IEEE ACCESS, V11, P47040, DOI 10.1109/ACCESS.2023.3275748
   Wang YH, 2023, REMOTE SENS-BASEL, V15, DOI 10.3390/rs15061593
   Wei R, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14102443
   Yang KL, 2020, AGRONOMY-BASEL, V10, DOI 10.3390/agronomy10111721
   Yfantis EA, 2019, 2019 IEEE 9TH ANNUAL COMPUTING AND COMMUNICATION WORKSHOP AND CONFERENCE (CCWC), P409, DOI 10.1109/CCWC.2019.8666471
   Yu CQ, 2018, PROC CVPR IEEE, P1857, DOI 10.1109/CVPR.2018.00199
   Zhang K, 2023, REMOTE SENS-BASEL, V15, DOI 10.3390/rs15184572
   Zhang L, 2023, DRONES-BASEL, V7, DOI 10.3390/drones7070456
   Zhang TX, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13193892
   Zhang W, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12111760
   Zhang ZH, 2023, ELECTRONICS-SWITZ, V12, DOI 10.3390/electronics12143113
NR 45
TC 0
Z9 0
U1 3
U2 3
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD AUG
PY 2024
VL 148
AR 105099
DI 10.1016/j.imavis.2024.105099
EA JUN 2024
PG 15
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA WA0T1
UT WOS:001252037300001
DA 2024-08-05
ER

PT J
AU Illakiya, T
   Karthik, R
AF Illakiya, T.
   Karthik, R.
CA Alzheimer's Dis Neuroimaging
TI A deep feature fusion network with global context and cross-dimensional
   dependencies for classification of mild cognitive impairment from brain
   MRI
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Mild cognitive impairment; Magnetic resonance imaging; Deep learning;
   Convolutional neural network; Classification: Alzheimer's disease
ID ALZHEIMERS-DISEASE
AB Background and objectives: The accurate identification of people with Mild Cognitive Impairment (MCI) who may develop Alzheimer's disease (AD) holds significant importance in facilitating timely intervention and treatment. However, current classification methods have yet to be effective due to the subtle nature of the features involved. This research aims to improve the performance of the MCI classification by enhancing the feature representation in brain MRI. Methods: We propose an integrated model that combines Group Shuffle Depth -wise Convolution (GSDW), Global Context Network (GCN), Hybrid Multi -Focus Attention Block (HMAB), and EfficientNet-B0 architecture for MCI classification. This model extracts fine-grained features, contextual information, and long-range dependencies and aggregates low-level details with high-level context information to learn discriminative features. Results: Our comprehensive evaluation demonstrates significant improvements in the classification performance for both progressive MCI (pMCI) and stable MCI (sMCI), surpassing existing approaches. The proposed model achieved a notable accuracy of 77.2%. Conclusions: This study introduces a novel feature fusion technique that combines global contextual representations and cross -dimensional dependencies to enhance classification results. These findings highlight the potential of the proposed framework for early identification and intervention in individuals at risk of cognitive decline.
C1 [Illakiya, T.] Vellore Inst Technol, Sch Comp Sci & Engn, Chennai, India.
   [Karthik, R.] Vellore Inst Technol, CCPS, Chennai, India.
C3 Vellore Institute of Technology (VIT); VIT Chennai; Vellore Institute of
   Technology (VIT); VIT Chennai
RP Karthik, R (corresponding author), Vellore Inst Technol, CCPS, Chennai, India.
EM illakiya.t2020@vitstudent.ac.in; r.karthik@vit.ac.in
RI T, Illakiya/AEQ-7709-2022
OI T, Illakiya/0000-0001-5890-529X
FU Alzheimer's Disease Neuroimaging Initiative (ADNI) (National Institutes
   of Health) [U01 AG024904]; DOD ADNI (Department of Defense)
   [W81XWH-12-2-0012]; National Institute on Aging; National Institute of
   Biomedical Imaging and Bioengineering; ADNI Clinical sites in Canada
FX Data collection and sharing for this project was funded by the
   Alzheimer's Disease Neuroimaging Initiative (ADNI) (National Institutes
   of Health Grant U01 AG024904) and DOD ADNI (Department of Defense award
   number W81XWH-12-2-0012) . ADNI is funded by the National Institute on
   Aging, the National Institute of Biomedical Imaging and Bioengineering,
   and through generous contributions from the following: AbbVie,
   Alzheimer's Association; Alzheimer's Drug Discovery Foundation; Araclon
   Biotech; BioClinica, Inc.; Biogen; Bristol-Myers Squibb Company;
   CereSpir, Inc.; Cogstate; Eisai Inc.; Elan Pharmaceuticals, Inc.; Eli
   Lilly and Company; EuroImmun; F. Hoffmann -La Roche Ltd. and its
   affiliated Company Genentech, Inc.; Fujirebio; GE Healthcare; IXICO
   Ltd.; Janssen Alzheimer Immunotherapy Research & Development, LLC.;
   Johnson & Johnson Pharmaceutical Research & Development LLC.; Lumosity;
   Lundbeck; Merck & Co., Inc.; Meso Scale Diagnostics, LLC.; NeuroRx
   Research; Neurotrack Technologies; Novartis Pharma- ceuticals
   Corporation; Pfizer Inc.; Piramal Imaging; Servier; Takeda
   Pharmaceutical Company; and Transition Therapeutics. The Canadian
   Institutes of Health Research is providing funds to support ADNI
   Clinical sites in Canada. Private sector contributions are facilitated
   by the Foundation for the National Institutes of Health ( www.fnih.org)
   . The grantee organization is the Northern California Institute for
   Research and Education, and the study is coordinated by the Alzheimer's
   Therapeutic Research Institute at the University of Southern California.
   ADNI data are disseminated by the Laboratory for Neuro Imaging at the
   University of Southern California.
CR Abdelaziz M, 2021, J BIOMED INFORM, V121, DOI 10.1016/j.jbi.2021.103863
   [Anonymous], 2021, ALZHEIMERS DEMENT, V17, P327, DOI 10.1002/alz.12328
   Bae JB, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-79243-9
   Basheera S, 2020, COMPUT MED IMAG GRAP, V81, DOI [10.1016/j.compmedimg.2020.101713, 10.1016/j.compmedimag.2020.101713]
   Beltrán JF, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0235663
   Cao Y, 2023, IEEE T PATTERN ANAL, V45, P6881, DOI 10.1109/TPAMI.2020.3047209
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Cheng B, 2019, BRAIN IMAGING BEHAV, V13, P138, DOI 10.1007/s11682-018-9846-8
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Cui RX, 2019, IEEE J BIOMED HEALTH, V23, P2099, DOI 10.1109/JBHI.2018.2882392
   Fathi S, 2022, COMPUT BIOL MED, V146, DOI 10.1016/j.compbiomed.2022.105634
   Feng CY, 2019, IEEE ACCESS, V7, P63605, DOI 10.1109/ACCESS.2019.2913847
   Gao F, 2020, NEUROIMAGE-CLIN, V27, DOI 10.1016/j.nicl.2020.102290
   Han RZ, 2022, APPL SOFT COMPUT, V120, DOI 10.1016/j.asoc.2022.108660
   Hazarika RA, 2022, J KING SAUD UNIV-COM, V34, P8576, DOI 10.1016/j.jksuci.2021.09.003
   Huang YC, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00509
   Illakiya T, 2023, BIOENGINEERING-BASEL, V10, DOI 10.3390/bioengineering10060714
   Illakiya T, 2023, NEUROINFORMATICS, V21, P339, DOI 10.1007/s12021-023-09625-7
   Khan TK, 2016, BIOMARKERS IN ALZHEIMER'S DISEASE, P27, DOI 10.1016/B978-0-12-804832-0.00002-X
   Li AJ, 2021, BRAIN IMAGING BEHAV, V15, P2330, DOI 10.1007/s11682-020-00427-y
   Li F, 2019, J NEUROSCI METH, V323, P108, DOI 10.1016/j.jneumeth.2019.05.006
   Luo M, 2023, COMPUT BIOL MED, V156, DOI 10.1016/j.compbiomed.2023.106700
   Mahendran N, 2022, COMPUT BIOL MED, V141, DOI 10.1016/j.compbiomed.2021.105056
   Misra D, 2021, IEEE WINT CONF APPL, P3138, DOI 10.1109/WACV48630.2021.00318
   Oh K, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-54548-6
   Pan D, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00259
   Shorten C, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0197-0
   Tan MX, 2019, PR MACH LEARN RES, V97
   Thayumanasamy I, 2022, TRAIT SIGNAL, V39, P1961, DOI 10.18280/ts.390608
   Uysal G, 2020, J NEUROSCI METH, V337, DOI 10.1016/j.jneumeth.2020.108669
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Weber CJ, 2021, ALZH DEMENT-TRCI, V7, DOI 10.1002/trc2.12226
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhang X, 2022, IEEE J BIOMED HEALTH, V26, P5289, DOI 10.1109/JBHI.2021.3066832
   Zhu XZ, 2019, IEEE I CONF COMP VIS, P6687, DOI 10.1109/ICCV.2019.00679
NR 36
TC 4
Z9 4
U1 5
U2 5
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD APR
PY 2024
VL 144
AR 104967
DI 10.1016/j.imavis.2024.104967
EA FEB 2024
PG 14
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA NZ3J4
UT WOS:001204234100001
DA 2024-08-05
ER

PT J
AU Li, B
   Zhu, JS
   Dai, LL
   Jing, H
   Huang, ZZ
AF Li, Bo
   Zhu, Jiansheng
   Dai, Linlin
   Jing, Hui
   Huang, Zhizheng
TI The impact of introducing textual semantics on item instance retrieval
   with highly similar appearance: An empirical study
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Image instance retrieval; Multi -modal fusion; Deep-learning; Neural
   networks; Cross -modal embedding
ID IMAGE; REPRESENTATION
AB Feature representation plays an important role in image instance retrieval (IIR). In practical applications, we find that items of different categories but highly similar in appearance are easy to become the objects of incorrect retrieval. We analyze that extracting features from the appearance dimension alone may cause objects with similar appearance to have smaller similar distances in feature space. But the appearance is not the only factor that determines whether the item is the same, and the difference in the shooting angle will also amplify the appearance difference of the same item in the image. In this paper, through detailed empirical study, we verify a conjecture that by introducing text semantics and fusing it with appearance features, the similarity distance of falsely retrieved objects in feature space can be corrected, thus improving the retrieval effectiveness of image instance retrieval tasks in highly similar appearance data. We introduce textual semantics for image instances based on the image-text cross-modal model. Specifically, we enhance the proportion of appearance similar items based on three open-source datasets (Products-10 k, RP2k and Stanford products) of item instances, and add multi-angle image samples of the same item to enlarge the difference of the same item. Subsequently, we have embarked on baseline experiments for appearance features and textual features from the perspectives of shooting angle similarity and visual character similarity, to explore the advantages of multiple strategies for fusing textual semantics with appearance features. Then, we examine the effect of our method on fine-grained item instance retrieval methods with state-of-the-art. Resultantly, taking mean Average Precision (mAP) as the quantitative metric and averaging experimental results, our method has an obvious improvement over the appearance and textual baselines, where the improvement of appearance feature baselines is generally more obvious than that of textual feature baselines (e.g., in our expanded RP2k dataset, from the perspective of shooting angle similarity, the mAP of the appearance feature baseline is nearly 19.62, the textual feature baseline is 32.45, our method is 43.19. From perspective of visual character similarity, the values are 27.14, 43.59, 54.76, respectively). Moreover, our methods outperform the state-of-the-art fine-grained item instance retrieval methods with improvements of nearly 13.05% and 22.49% on expanded Products-10 k and RP2k, respectively.
C1 [Li, Bo] China Acad Railway Sci, Postgrad Dept, Beijing 100081, Peoples R China.
   [Zhu, Jiansheng] China Railway, Dept Sci Technol & Informat, Beijing 100844, Peoples R China.
   [Dai, Linlin; Jing, Hui; Huang, Zhizheng] China Acad Railway Sci Corp Ltd, Inst Comp Technol, Beijing 100081, Peoples R China.
RP Huang, ZZ (corresponding author), China Acad Railway Sci Corp Ltd, Inst Comp Technol, Beijing 100081, Peoples R China.
EM h4444433333@163.com
RI Jing, Hui/AAT-4264-2021
FU Foundation of China Academy of Railway Sciences [2023YJ132]
FX We would like to thank all the reviewers and editors for their com-
   ments and opinions, which improved the quality of this paper. This work
   was supported by Foundation of China Academy of Railway Sciences under
   Grants NO.2023YJ132.
CR Bai C, 2021, IEEE T MULTIMEDIA, V23, P2199, DOI 10.1109/TMM.2021.3065578
   Bai YL, 2020, Arxiv, DOI arXiv:2008.10545
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Bingyi Cao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P726, DOI 10.1007/978-3-030-58565-5_43
   Chen RN, 2023, Arxiv, DOI arXiv:2301.04926
   Chen W, 2023, IEEE T PATTERN ANAL, V45, P7270, DOI 10.1109/TPAMI.2022.3218591
   Chen WT, 2023, PROC CVPR IEEE, P23581, DOI 10.1109/CVPR52729.2023.02258
   Chen YH, 2023, PROC CVPR IEEE, P22648, DOI 10.1109/CVPR52729.2023.02169
   Do TN, 2020, ACTA GEOTECH, V15, P1707, DOI 10.1007/s11440-019-00886-8
   Dosovitskiy A., ARXIV
   El-Nouby A, 2021, Arxiv, DOI [arXiv:2102.05644, DOI 10.48550/ARXIV.2102.05644]
   Fukui A., 2016, arXiv
   Gong YC, 2014, LECT NOTES COMPUT SC, V8695, P392, DOI 10.1007/978-3-319-10584-0_26
   Guo WZ, 2019, IEEE ACCESS, V7, P63373, DOI 10.1109/ACCESS.2019.2916887
   Li LH, 2019, Arxiv, DOI [arXiv:1908.03557, DOI 10.48550/ARXIV.1908.03557]
   Hu HD, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P359, DOI 10.1145/3219819.3219843
   Kalantidis Yannis, 2016, Computer Vision - ECCV 2016. 14th European Conference: Workshops. Proceedings: LNCS 9913, P685, DOI 10.1007/978-3-319-46604-0_48
   Kiela D, 2018, AAAI CONF ARTIF INTE, P5198
   Kim J., 2018, BMVC
   Li G, 2020, AAAI CONF ARTIF INTE, V34, P11336
   Li JN, 2022, PR MACH LEARN RES
   Li S, 2021, KDD '21: PROCEEDINGS OF THE 27TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P3181, DOI 10.1145/3447548.3467101
   Lou YH, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1128, DOI 10.1145/3240508.3240602
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luo YP, 2021, AAAI CONF ARTIF INTE, V35, P2286
   Movshovitz-Attias Y, 2017, IEEE I CONF COMP VIS, P360, DOI 10.1109/ICCV.2017.47
   Ng Tony, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P253, DOI 10.1007/978-3-030-58595-2_16
   Parekh V., 2021, P IEEECVF C COMPUTER, P3973
   Radford A, 2021, PR MACH LEARN RES, V139
   Revaud J, 2019, IEEE I CONF COMP VIS, P5106, DOI 10.1109/ICCV.2019.00521
   Sarmiento JA, 2020, Arxiv, DOI arXiv:2009.01053
   Shankar D, 2017, Arxiv, DOI arXiv:1703.02344
   Shoib AM, 2023, Arxiv, DOI arXiv:2305.07540
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Sohn K, 2016, ADV NEUR IN, V29
   Song CH, 2023, IEEE WINT CONF APPL, P107, DOI 10.1109/WACV56688.2023.00019
   Tan FW, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12085, DOI 10.1109/ICCV48922.2021.01189
   Vaezi Joze Hamid Reza, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13286, DOI 10.1109/CVPR42600.2020.01330
   Vaswani A, 2017, ADV NEUR IN, V30
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang Q, 2019, NEUROCOMPUTING, V363, P17, DOI 10.1016/j.neucom.2019.08.025
   Wang X, 2019, PROC CVPR IEEE, P5017, DOI 10.1109/CVPR.2019.00516
   Wang Y., 2020, P NEURIPS, V33, P4835
   Wei SK, 2019, IEEE T IMAGE PROCESS, V28, P4580, DOI 10.1109/TIP.2019.2913513
   Wei XS, 2017, IEEE T IMAGE PROCESS, V26, P2868, DOI 10.1109/TIP.2017.2688133
   Wu D, 2016, IEEE T PATTERN ANAL, V38, P1583, DOI 10.1109/TPAMI.2016.2537340
   Wu H, 2022, AAAI CONF ARTIF INTE, P2703
   Xu J, 2018, AAAI CONF ARTIF INTE, P7436
   Xu JR, 2023, PROC CVPR IEEE, P2955, DOI 10.1109/CVPR52729.2023.00289
   Yang F, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2101, DOI 10.1145/3097983.3098162
   Yang M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11752, DOI 10.1109/ICCV48922.2021.01156
   Yang XC, 2021, IEEE T MULTIMEDIA, V23, P4014, DOI 10.1109/TMM.2020.3035277
   Zadeh A, 2017, Arxiv, DOI arXiv:1707.07250
   Zhai A, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2412, DOI 10.1145/3292500.3330739
   Zhang YH, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P993, DOI 10.1145/3219819.3219820
   Zhang YF, 2021, IMAGE VISION COMPUT, V105, DOI 10.1016/j.imavis.2020.104042
   Zheng L, 2018, IEEE T PATTERN ANAL, V40, P1224, DOI 10.1109/TPAMI.2017.2709749
NR 57
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD MAR
PY 2024
VL 143
AR 104925
DI 10.1016/j.imavis.2024.104925
EA FEB 2024
PG 14
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA JX9N0
UT WOS:001176577200001
DA 2024-08-05
ER

PT J
AU Li, M
   Zhu, ZF
   Li, KF
   Zhou, LH
   Zhao, Z
   Pei, HL
AF Li, Meng
   Zhu, Zhenfang
   Li, Kefeng
   Zhou, Lihua
   Zhao, Zhen
   Pei, Hongli
TI Joint training strategy of unimodal and multimodal for multimodal
   sentiment analysis
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Multimodal sentiment analysis; Multimodal fusion; Multimodal interaction
AB With the explosive growth of social media video content, research on multimodal sentiment analysis (MSA) has attracted considerable attention recently. Despite significant progress in MSA, there remains challenges: current research mostly focuses on learning either unimodal features or aspects of multimodal interactions, neglecting the importance of simultaneously considering both unimodal features and intermodal interactions. To address the aforementioned challenges, this paper proposes a fusion strategy called Joint Training of Unimodal and Multimodal (JTUM). Specifically, this strategy combines unimodal label generation module with cross-modal transformer. The unimodal label generation module aims to generate more distinctive labels for each unimodal input, facilitating more effective learning of unimodal representations. Meanwhile, cross-modal transformer is designed to treat each modality as a target modality and optimize it using other modalities as source modalities, thereby learning the interactions between each pair of modalities. By jointly training unimodal and multimodal tasks, our model can focus on individual modality features while learning the interactions between modalities. Finally, to better capture temporal information and make predictions, we also added self-attention transformer as sequence models. Experimental results on the CMU-MOSI and CMU-MOSEI datasets demonstrate that JTUM outperforms current main methods.
C1 [Li, Meng; Zhu, Zhenfang; Li, Kefeng; Zhou, Lihua; Zhao, Zhen; Pei, Hongli] Shandong Jiaotong Univ, Sch Informat Sci & Elect Engn, 5001 Haitang Rd, Jinan 250357, Shan Dong, Peoples R China.
C3 Shandong Jiaotong University
RP Zhu, ZF (corresponding author), Shandong Jiaotong Univ, Sch Informat Sci & Elect Engn, 5001 Haitang Rd, Jinan 250357, Shan Dong, Peoples R China.
EM zhuzf@sdjtu.edu.cn
FU National Social Science Foundation [19BYY076]
FX <B>Acknowledgments</B> The work in this paper id supported by the
   National Social Science Foundation (19BYY076) . During the research
   process, our work received a lot of support and assistance. Thanks to
   Professor Zhu Zhenfang for his careful guidance, the members of our NLP
   group for their support and effective suggestions, and finally to all
   the authors who helped us with our research.
CR [Anonymous], 2015, P C EMP METH NAT LAN, DOI [10.18653/v1/D15-1167, DOI 10.18653/V1/D15-1167]
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Chen M., 2017, P 19 ACM INT C MULT, P163, DOI [10.1145/3136755.3136801, DOI 10.1145/3136755.3136801]
   Chen T, 2014, Arxiv, DOI arXiv:1410.8586
   Cheng JY, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P2447
   Degottex G, 2014, INT CONF ACOUST SPEE, DOI 10.1109/ICASSP.2014.6853739
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Ekman P., 1997, WHAT FACE REVEALS BA
   Gandhi A, 2023, INFORM FUSION, V91, P424, DOI 10.1016/j.inffus.2022.09.025
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Han W, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P9180
   Haq S.-u., 2013, P 51 ANN M ASS COMP, V1, P973
   Hazarika D, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1122, DOI 10.1145/3394171.3413678
   He Yu, 2022, P 3 INT MULT SENT AN, P61
   Hou M, 2019, ADV NEUR IN, V32
   Hu J, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3505, DOI 10.1145/3394171.3413711
   Huang Y., 2021, Advances in Neural Information Processing Systems, V14, P10944
   Tsai YHH, 2019, Arxiv, DOI arXiv:1806.06176
   Kalchbrenner N, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P655, DOI 10.3115/v1/p14-1062
   Kaur R., 2022, Research anthology on implementing sentiment analysis across multiple disciplines (2022), P1846, DOI DOI 10.4018/978-1-6684-6303-1.CH098
   Kaushik L, 2013, INT CONF ACOUST SPEE, P8485, DOI 10.1109/ICASSP.2013.6639321
   Kim Y., 2014, C EMP METH NAT LANG, P1746, DOI DOI 10.3115/V1/D14-1181
   Ko D, 2023, PROC CVPR IEEE, P20105, DOI 10.1109/CVPR52729.2023.01925
   Liu Z, 2018, Arxiv, DOI [arXiv:1806.00064, 10.48550/arXiv.1806.00064]
   Lu JS, 2016, ADV NEUR IN, V29
   Maas A., 2011, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, P142
   Mai Sijie, 2023, IEEE Transactions on Multimedia, P4121, DOI 10.1109/TMM.2022.3171679
   Mai SJ, 2023, IEEE T AFFECT COMPUT, V14, P2276, DOI 10.1109/TAFFC.2022.3172360
   Mohammad SM, 2013, COMPUT INTELL-US, V29, P436, DOI 10.1111/j.1467-8640.2012.00460.x
   Morency L.-P., 2011, P 13 INT C MULT INT, P169, DOI [DOI 10.1145/2070481.2070509, 10.1145/2070481.2070509]
   Naseem U, 2020, FUTURE GENER COMP SY, V113, P58, DOI 10.1016/j.future.2020.06.050
   Ngiam A., 2011, IEEE INT C MACH LEAR, P689, DOI DOI 10.5555/3104482.3104569
   Nojavanasghari B, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P284, DOI 10.1145/2993148.2993176
   Nwe TL, 2003, SPEECH COMMUN, V41, P603, DOI 10.1016/S0167-6393(03)00099-2
   Pang B, 2002, PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING, P79, DOI 10.3115/1118693.1118704
   Pang Bo, 2004, arXiv
   Poria S, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P873, DOI 10.18653/v1/P17-1081
   Rahman W, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P2359, DOI 10.18653/v1/2020.acl-main.214
   Rajagopalan SS, 2016, LECT NOTES COMPUT SC, V9911, P338, DOI 10.1007/978-3-319-46478-7_21
   Sadegh M., 2012, International Journal of Computers Technology, V2, P171
   Schuller B, 2009, 2009 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION & UNDERSTANDING (ASRU 2009), P552, DOI 10.1109/ASRU.2009.5372886
   Snoek C. G. M., 2005, 13th Annual ACM International Conference on Multimedia, P399, DOI 10.1145/1101149.1101236
   Soleymani M, 2017, IMAGE VISION COMPUT, V65, P3, DOI 10.1016/j.imavis.2017.08.003
   Sun C, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P380
   Sun LC, 2024, IEEE T AFFECT COMPUT, V15, P309, DOI 10.1109/TAFFC.2023.3274829
   Sun T, 2023, IEEE T KNOWL DATA EN, V35, P12605, DOI 10.1109/TKDE.2023.3270940
   Sun Y, 2023, IEEE T AFFECT COMPUT, V14, P2209, DOI 10.1109/TAFFC.2022.3178231
   Taboada M, 2011, COMPUT LINGUIST, V37, P267, DOI 10.1162/COLI_a_00049
   Tsai YHH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6558, DOI 10.18653/v1/p19-1656
   Turney Peter D., 2002, arXiv
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang HH, 2017, IEEE INT CON MULTI, P949, DOI 10.1109/ICME.2017.8019301
   Wang Y., 2016, P 2016 C EMP METH NA, P606, DOI 10.18653/v1/D16-1058
   Wang YS, 2019, AAAI CONF ARTIF INTE, P7216
   Xu H, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P2324
   Xu N, 2019, AAAI CONF ARTIF INTE, P371
   Yang JD, 2023, PROCEEDINGS OF THE 61ST ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, ACL 2023, VOL 1, P7617
   You QZ, 2017, AAAI CONF ARTIF INTE, P231
   Yu WM, 2021, AAAI CONF ARTIF INTE, V35, P10790
   Yuan Jianbo, 2013, P 2 INT WORKSH ISS S, P1
   Zadeh A, 2017, Arxiv, DOI arXiv:1707.07250
   Zadeh A, 2016, Arxiv, DOI arXiv:1606.06259
   Zadeh A, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2236
   Zadeh A, 2018, AAAI CONF ARTIF INTE, P5642
   Zadeh A, 2018, AAAI CONF ARTIF INTE, P5634
   Zhu ZF, 2023, INFORM PROCESS MANAG, V60, DOI 10.1016/j.ipm.2022.103223
NR 66
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD SEP
PY 2024
VL 149
AR 105172
DI 10.1016/j.imavis.2024.105172
PG 10
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA ZK4S5
UT WOS:001275185700001
DA 2024-08-05
ER

PT J
AU He, MF
   Yang, ZY
   Zhang, GB
   Long, Y
   Song, HB
AF He, Mengfei
   Yang, Zhiyou
   Zhang, Guangben
   Long, Yan
   Song, Huaibo
TI IIMT-net: Poly-1 weights balanced multi-task network for semantic
   segmentation and depth estimation using interactive information
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Multi -task learning; Scene understanding; Semantic segmentation; Depth
   estimation; Vision transformer
AB Semantic segmentation and depth estimation are two basic researchable problems in computer vision. In common, we explore the two tasks separately. However, in some scenes, such as autonomous driving, they need be done at the same time. Meanwhile, there exists interconnected information between two tasks, which can jointly promote the performances of them. Thus, we explore the two tasks based on multi -task learning to jointly train the tasks and gain predictions together. In this paper, we build Interactive Information Multi -Task Network (IIMT-Net) incorporating the information interactive modules, trained with proposed task -balancing strategy. To be specific, we construct the principal part of encoder and decoder based on Transformer to well capture the global information. For better utilization of the task interaction between two tasks, we also add information fusion modules in two sub -decoders. In addition, the task -balancing strategy, Poly -1 weights, is designed as the balance among samples with different degrees of difficulty to ensure the network won't be biased towards any task severely. The proposed approach's exceptional performance has been extensively showcased through experimental results on the NYU Depth V2 dataset, the Cityscapes dataset, and the SUN RGB-D dataset. Our model can complete the predictions of semantic segmentation task and depth estimation task together and obtain mIoU values of 46.66% on the NYU Depth V2 dataset, 66.37% on the Cityscapes dataset, and 49.89% on the SUN RGB-D dataset, respectively with rmse values of 0.648, 6.630 and 0.401 for depth estimation task, which outperform most existing methods in multi -task learning.
C1 [He, Mengfei; Yang, Zhiyou; Zhang, Guangben; Long, Yan; Song, Huaibo] Northwest A&F Univ, Coll Mech & Elect Engn, Yangling 712100, Shaanxi, Peoples R China.
   [He, Mengfei; Yang, Zhiyou; Zhang, Guangben; Long, Yan; Song, Huaibo] Minist Agr & Rural Affairs, Key Lab Agr Internet Things, Yangling 712100, Shaanxi, Peoples R China.
   [He, Mengfei; Yang, Zhiyou; Zhang, Guangben; Long, Yan; Song, Huaibo] Shaanxi Key Lab Agr Informat Percept & Intelligent, Yangling 712100, Shaanxi, Peoples R China.
C3 Northwest A&F University - China; Ministry of Agriculture & Rural
   Affairs
RP Long, Y (corresponding author), Northwest A&F Univ, Coll Mech & Elect Engn, Yangling 712100, Shaanxi, Peoples R China.; Long, Y (corresponding author), Minist Agr & Rural Affairs, Key Lab Agr Internet Things, Yangling 712100, Shaanxi, Peoples R China.; Long, Y (corresponding author), Shaanxi Key Lab Agr Informat Percept & Intelligent, Yangling 712100, Shaanxi, Peoples R China.
EM longyan@nwsuaf.edu.cn
FU Key Research and Development Program of Shaanxi, China [2020NY-144]
FX This work was supported by the Key Research and Development Program of
   Shaanxi, China (Program No. 2020NY-144) . The authors appreciate the
   funding organization for their financial supports. The authors would
   also like to thank the helpful comments and suggestions provided by all
   the authors cited in this article and the anonymous reviewers.
CR Agarwal A, 2023, IEEE WINT CONF APPL, P5850, DOI 10.1109/WACV56688.2023.00581
   Alenzi Z, 2022, ELECTRONICS-SWITZ, V11, DOI 10.3390/electronics11132063
   Araki R, 2022, ADV ROBOTICS, V36, P373, DOI 10.1080/01691864.2022.2043183
   Bhat SF, 2021, PROC CVPR IEEE, P4008, DOI 10.1109/CVPR46437.2021.00400
   Borse S, 2021, PROC CVPR IEEE, P5897, DOI 10.1109/CVPR46437.2021.00584
   Cao JM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7068, DOI 10.1109/ICCV48922.2021.00700
   Changqian Yu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12413, DOI 10.1109/CVPR42600.2020.01243
   Chen JY, 2022, PATTERN RECOGN, V129, DOI 10.1016/j.patcog.2022.108753
   Chen MH, 2023, PATTERN RECOGN, V135, DOI 10.1016/j.patcog.2022.109168
   Chennupati S, 2019, IEEE COMPUT SOC CONF, P1200, DOI 10.1109/CVPRW.2019.00159
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Eigen D., 2014, In: Neural Information Processing Systems, P2366, DOI DOI 10.5555/2969033.2969091
   Fiedler MA, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21175918
   Fu H, 2018, PROC CVPR IEEE, P2002, DOI 10.1109/CVPR.2018.00214
   Gao Tianxiao, 2022, Proceedings of SPIE, V12285, DOI 10.1117/12.2637183
   Gao TX, 2022, APPL INTELL, V52, P18167, DOI 10.1007/s10489-022-03401-x
   Guo M, 2018, LECT NOTES COMPUT SC, V11220, P282, DOI 10.1007/978-3-030-01270-0_17
   He L, 2021, NEUROCOMPUTING, V440, P251, DOI 10.1016/j.neucom.2021.01.126
   Hoyer L, 2021, PROC CVPR IEEE, P11125, DOI 10.1109/CVPR46437.2021.01098
   Ibrahem H, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22010337
   Jun J, 2022, LECT NOTES COMPUT SC, V13662, P18, DOI 10.1007/978-3-031-20086-1_2
   Kawakami R, 2019, IEEE IMAGE PROC, P3636, DOI [10.1109/ICIP.2019.8803687, 10.1109/icip.2019.8803687]
   Kendall A, 2018, PROC CVPR IEEE, P7482, DOI 10.1109/CVPR.2018.00781
   Kim S, 2022, IEEE IMAGE PROC, P2311, DOI 10.1109/ICIP46576.2022.9897871
   Kingma D. P., 2014, arXiv
   Lee DH, 2021, APPL INTELL, V51, P237, DOI 10.1007/s10489-020-01827-9
   Leng Z., 2022, P 10 INT C LEARN REP
   Lin BJ, 2022, Arxiv, DOI arXiv:2111.10603
   Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Lin X, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19081795
   Liu CL, 2023, IEEE T VIS COMPUT GR, V29, P2132, DOI 10.1109/TVCG.2022.3141943
   Liu JJ, 2020, IEEE T IMAGE PROCESS, V29, P8652, DOI 10.1109/TIP.2020.3017352
   Liu J, 2018, IEEE T NEUR NET LEAR, V29, P5655, DOI 10.1109/TNNLS.2017.2787781
   Liu SK, 2019, PROC CVPR IEEE, P1871, DOI 10.1109/CVPR.2019.00197
   Liu XD, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P4487
   Lopes I, 2023, IEEE WINT CONF APPL, P2328, DOI 10.1109/WACV56688.2023.00236
   Lyu K, 2022, SSRN
   Ming Y, 2021, NEUROCOMPUTING, V438, P14, DOI 10.1016/j.neucom.2020.12.089
   Nakamura ATM, 2021, ENG APPL ARTIF INTEL, V100, DOI 10.1016/j.engappai.2021.104205
   Nekrasov V., 2018, P 29 BRIT MACH VIS C
   Paszke A, 2019, ADV NEUR IN, V32
   Pei Z., 2023, Advances in Neural Information Processing Systems
   Shao S., 2023, P IEEECVF INT C COMP, P7931
   Shao SW, 2024, IEEE T MULTIMEDIA, V26, P3341, DOI 10.1109/TMM.2023.3310259
   Shengjie Zhu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13113, DOI 10.1109/CVPR42600.2020.01313
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Song M, 2021, IEEE T CIRC SYST VID, V31, P4381, DOI 10.1109/TCSVT.2021.3049869
   Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655
   Song TJ, 2022, IEEE T INTELL TRANSP, V23, P16318, DOI 10.1109/TITS.2022.3149789
   Strudel R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7242, DOI 10.1109/ICCV48922.2021.00717
   Vandenhende Simon, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P527, DOI 10.1007/978-3-030-58548-8_31
   Vandenhende S, 2022, IEEE T PATTERN ANAL, V44, P3614, DOI 10.1109/TPAMI.2021.3054719
   Wang SF, 2021, PATTERN RECOGN, V114, DOI 10.1016/j.patcog.2021.107837
   Wang WH, 2023, PROC CVPR IEEE, P14408, DOI 10.1109/CVPR52729.2023.01385
   Wu BY, 2022, IMAGE VISION COMPUT, V125, DOI 10.1016/j.imavis.2022.104520
   Wu D, 2022, MACH INTELL RES, V19, P550, DOI 10.1007/s11633-022-1339-y
   Xi YL, 2023, INT J COMPUT VISION, V131, P2977, DOI 10.1007/s11263-023-01835-5
   Xu D, 2018, PROC CVPR IEEE, P675, DOI 10.1109/CVPR.2018.00077
   Yang C., 2023, P 11 INT C LEARN REP, DOI [10.48550/arXiv.2210.01820, DOI 10.48550/ARXIV.2210.01820]
   Zamir A, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P6241
   Zhang JM, 2023, Arxiv, DOI arXiv:2203.04838
   Zhang JN, 2023, IEEE T NEUR NET LEAR, V34, P2710, DOI 10.1109/TNNLS.2021.3107362
   Zhang YF, 2021, IMAGE VISION COMPUT, V105, DOI 10.1016/j.imavis.2020.104042
   Zhang YM, 2022, IEEE IMAGE PROC, P1316, DOI 10.1109/ICIP46576.2022.9897517
   Zhang ZY, 2019, PROC CVPR IEEE, P4101, DOI 10.1109/CVPR.2019.00423
   Zhang ZY, 2020, IEEE T PATTERN ANAL, V42, P2608, DOI 10.1109/TPAMI.2019.2926728
   Zhang ZY, 2018, PATTERN RECOGN, V83, P430, DOI 10.1016/j.patcog.2018.05.016
   Zhang ZL, 2018, ADV NEUR IN, V31
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
NR 70
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD AUG
PY 2024
VL 148
AR 105109
DI 10.1016/j.imavis.2024.105109
EA JUN 2024
PG 15
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA WS8S3
UT WOS:001256960700001
DA 2024-08-05
ER

PT J
AU Andrade, ED
   Guérin, J
   Viterbo, J
   Sampaio, IGB
AF Andrade, Eduardo de O.
   Guerin, Joris
   Viterbo, Jose
   Sampaio, Igor Garcia Ballhausen
TI Adversarial attacks and defenses in person search: A systematic mapping
   study and taxonomy
SO IMAGE AND VISION COMPUTING
LA English
DT Review
DE Person search; Person re -identification; Object detection; Adversarial
   attacks; Adversarial defenses
ID REIDENTIFICATION
AB Person Search aims at retrieving a specific individual (the query) within a collection of whole scene images from diverse, non -overlapping cameras. It has the potential to play a pivotal role in various public safety applications like suspect searching and identifying abandoned luggage owners. Person Search encompasses two Computer Vision challenges: 1. Object Detection, which entails localizing humans in whole scene images, and 2. Person ReIdentification, where the query image is compared with images of detected individuals to establish identification. The critical nature of Person Search underscores the imperative to safeguard it against security threats, such as adversarial attacks, which can result in non-detection or misidentification. While adversarial attacks and defense mechanisms have been extensively studied for both Object Detection and Person Re -Identification, there is a noticeable gap in research concerning Person Search. This work presents a comprehensive Systematic Mapping Study and taxonomy of adversarial attacks and defenses in Person Search, utilizing Parsifal and ChatGPT 4 for indepth analysis. We highlight the persistent challenges associated with Person Search and discuss prospects for future advancements in addressing its vulnerabilities.
C1 [Andrade, Eduardo de O.; Viterbo, Jose; Sampaio, Igor Garcia Ballhausen] Fluminense Fed Univ, Inst Comp, Ave Gal Milton Tavares Souza,S-n Sao Domingos, BR-24210310 Niteroi, RJ, Brazil.
   [Guerin, Joris] Univ Montpellier, Espace Dev, IRD, 500 Rue Jean Francois Breton, F-34090 Montpellier, France.
C3 Universidade Federal Fluminense; Universite de Montpellier; Institut de
   Recherche pour le Developpement (IRD)
RP Andrade, ED (corresponding author), Fluminense Fed Univ, Inst Comp, Ave Gal Milton Tavares Souza,S-n Sao Domingos, BR-24210310 Niteroi, RJ, Brazil.
EM eandrade@ic.uff.br; joris.guerin@ird.fr; viterbo@ic.uff.br;
   igorgarcia@id.uff.br
FU Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior (CAPES);
   Conselho Nacional de Desenvolvimento Cientifico e Tecnologico (CNPq);
   PrimeUp Solucoes de TI LTDA
FX The Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior (CAPES),
   Conselho Nacional de Desenvolvimento Cientifico e Tecnologico (CNPq),
   and PrimeUp Solucoes de TI LTDA financed part of this work.
CR Achiam OJ, 2023, Arxiv, DOI arXiv:2303.08774
   Akhtar N, 2021, IEEE ACCESS, V9, P155161, DOI 10.1109/ACCESS.2021.3127960
   Alshamaila Y, 2023, INT J DISAST RISK RE, V85, DOI 10.1016/j.ijdrr.2023.103521
   Alshantti A, 2024, Arxiv, DOI arXiv:2404.00696
   Andrade E.D.O., 2023, arXiv
   Andrade ED, 2022, COMPUT INTELL-US, V38, P1802, DOI 10.1111/coin.12543
   Bai S, 2021, IEEE T PATTERN ANAL, V43, P2119, DOI 10.1109/TPAMI.2020.3031625
   Carrera-Rivera Angela, 2022, MethodsX, V9, P101895, DOI 10.1016/j.mex.2022.101895
   Chakraborty A, 2021, CAAI T INTELL TECHNO, V6, P25, DOI 10.1049/cit2.12028
   Chang YP, 2023, Arxiv, DOI [arXiv:2307.03109, DOI 10.1145/3641289]
   Chen ST, 2019, LECT NOTES ARTIF INT, V11051, P52, DOI 10.1007/978-3-030-10925-7_4
   Cooke A, 2012, QUAL HEALTH RES, V22, P1435, DOI 10.1177/1049732312452938
   Gao J, 2018, 2018 IEEE SYMPOSIUM ON SECURITY AND PRIVACY WORKSHOPS (SPW 2018), P50, DOI 10.1109/SPW.2018.00016
   Gill S.S., 2023, Internet of Things and Cyber-Physical Systems, V3, P262, DOI DOI 10.1016/J.IOTCPS.2023.05.004
   Gong Y., 2021, ARXIV
   Gong YP, 2022, IEEE COMPUT SOC CONF, P4312, DOI 10.1109/CVPRW56347.2022.00477
   Ho GTS, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19081796
   Hu S., 2022, arXiv
   Hu S., 2019, Adv. Neural Inf. Proces. Syst., V32
   Huang JS, 2023, AM J CANCER RES, V13, P1148
   Hussain S, 2021, IEEE WINT CONF APPL, P3347, DOI 10.1109/WACV48630.2021.00339
   Islam K, 2020, IMAGE VISION COMPUT, V101, DOI 10.1016/j.imavis.2020.103970
   Goodfellow IJ, 2015, Arxiv, DOI [arXiv:1412.6572, DOI 10.48550/ARXIV.1412.6572]
   Ji N, 2021, Arxiv, DOI arXiv:2103.08860
   Kaidi Xu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P665, DOI 10.1007/978-3-030-58558-7_39
   Khan PW, 2020, ELECTRONICS-SWITZ, V9, DOI 10.3390/electronics9030484
   Kim K, 2021, J INF PROCESS SYST, V17, P571, DOI 10.3745/JIPS.03.0161
   Kitchenham B, 2010, INFORM SOFTWARE TECH, V52, P792, DOI 10.1016/j.infsof.2010.03.006
   Kitchin R, 2019, J URBAN TECHNOL, V26, P47, DOI 10.1080/10630732.2017.1408002
   Li Y, 2022, AM STAT, V76, P329, DOI 10.1080/00031305.2021.2006781
   Li YZ, 2019, Arxiv, DOI arXiv:1906.09288
   Liang B., 2021, arXiv
   Liu X, 2019, Arxiv, DOI arXiv:1806.02299
   Lyu HR, 2021, CHINESE J ELECTRON, V30, P406, DOI 10.1049/cje.2021.03.003
   Marchezan L., 2019, 2019 ACM IEEE INT S, P1
   Mi JX, 2023, NEUROCOMPUTING, V519, P114, DOI 10.1016/j.neucom.2022.10.046
   Ming ZQ, 2022, IMAGE VISION COMPUT, V119, DOI 10.1016/j.imavis.2022.104394
   Minh D, 2022, ARTIF INTELL REV, V55, P3503, DOI 10.1007/s10462-021-10088-y
   Moher D, 2015, SYST REV-LONDON, V4, DOI [10.1186/2046-4053-4-1, 10.1371/journal.pmed.1000097, 10.1136/bmj.i4086, 10.1016/j.ijsu.2010.02.007, 10.1136/bmj.b2535, 10.1016/j.ijsu.2010.07.299, 10.1136/bmj.b2700]
   Mourao E, 2020, INFORM SOFTWARE TECH, V123, DOI 10.1016/j.infsof.2020.106294
   Pietron Marcin, 2024, arXiv
   Pitropakis N, 2019, COMPUT SCI REV, V34, DOI 10.1016/j.cosrev.2019.100199
   Qin YX, 2023, COMPUT SECUR, V134, DOI 10.1016/j.cose.2023.103460
   Qiu SL, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9050909
   Reiss MV, 2023, Arxiv, DOI [arXiv:2304.11085, DOI 10.48550/ARXIV.2304.11085, 10.48550/arXiv.2304.11085]
   Serban A, 2020, ACM COMPUT SURV, V53, DOI 10.1145/3398394
   Smite D, 2014, EMPIR SOFTW ENG, V19, P105, DOI 10.1007/s10664-012-9217-9
   Sommer L., 2021, SPIE, V11729, P207
   Sumari FO, 2020, PATTERN RECOGN LETT, V138, P513, DOI 10.1016/j.patrec.2020.08.023
   Szegedy C, 2014, Arxiv, DOI arXiv:1312.6199
   Wang S, 2023, Arxiv, DOI [arXiv:2302.03495, DOI 10.48550/ARXIV.2302.03495]
   Wang Z, 2022, WORLD WIDE WEB, V25, P1725, DOI 10.1007/s11280-022-01058-7
   Wei H, 2023, Arxiv, DOI arXiv:2209.15179
   Wei WY, 2022, J VIS COMMUN IMAGE R, V82, DOI 10.1016/j.jvcir.2021.103418
   Wohlin C., 2014, P 18 INT C EV ASS SO, P1, DOI [DOI 10.1145/2601248.2601268, 10.1145/2601248.2601268]
   Xiang C, 2021, CCS '21: PROCEEDINGS OF THE 2021 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P3177, DOI 10.1145/3460120.3484757
   Xie CH, 2017, IEEE I CONF COMP VIS, P1378, DOI 10.1109/ICCV.2017.153
   Xu H, 2020, INT J AUTOM COMPUT, V17, P151, DOI 10.1007/s11633-019-1211-x
   Xu YL, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P937, DOI 10.1145/2647868.2654965
   Yang D, 2023, Arxiv, DOI arXiv:2305.10929
   Yang J, 2023, NPJ DIGIT MED, V6, DOI 10.1038/s41746-023-00805-y
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Zaidi SSA, 2022, DIGIT SIGNAL PROCESS, V126, DOI 10.1016/j.dsp.2022.103514
   Zhao Y, 2019, PROCEEDINGS OF THE 2019 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (CCS'19), P1989, DOI 10.1145/3319535.3354259
   Zheng Y, 2020, IEEE ACCESS, V8, P183891, DOI 10.1109/ACCESS.2020.3024149
NR 65
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD AUG
PY 2024
VL 148
AR 105096
DI 10.1016/j.imavis.2024.105096
EA JUN 2024
PG 14
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA UY7H7
UT WOS:001251685100001
DA 2024-08-05
ER

PT J
AU Ilamathi, M
   Ramakrishnan, S
   Babusankar, RK
AF Ilamathi, M.
   Ramakrishnan, Sabitha
   Babusankar, Rakhul Kumar
TI Proactive hybrid learning framework for real-time multi-vehicle
   detection in unregulated traffic environments
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Transformer encoder; SS-BiFPN; Optimal transport algorithm; Vehicle
   detection; Unregulated traffic; Active learning
ID VEHICLE DETECTION; VISION
AB Reliable multi -vehicle detection in unregulated traffic environments is a crucial computer vision task in the development of Intelligent Transportation Systems (ITS). Despite the promising potential of Deep Learning (DL) methods for vehicle detection, the presence of uncertainties such as varying vehicle shapes and sizes, intricate background clutter, and unpredictable vehicle flow contribute to the chaotic unregulated traffic. For real-time vehicle detection in an unregulated traffic environment this paper proposes the Hybrid Learning Multi -Vehicle Detection framework (HL-MVD) based on the C onvolutional M ulti -head A ttention T ransformer D etector (CMATDet). The primary objective of this study is to generate a dataset containing highly informative video frames using a Pool -based Active Learning Strategy (PALS). Additionally, transfer learning will be implemented to train CMATDet to achieve improved accuracy and reduced detection latency. The proposed approach restructures the baseline YOLOv5x and incorporates a Multi -head Attention transformer encoder to effectively extract global features and a Scale Specific Bidirectional Feature Pyramid Network (SS-BiFPN) to facilitate multiscale feature representation. Simplified Optimal Transport Algorithm with top -q approximation technique (SimOTA) is utilized for label assignment approach. Heatmap analysis demonstrated the suitability of the newly generated dataset "AU-INV-P-PALS " for detecting specific Indian native vehicles. The performance of the proposed framework is evaluated on our custom -developed AU-INV-P-PALS vehicle dataset and the IITM-HeTra Dataset 1. In comparison with contemporary detection models, the proposed HL-MVD framework resulted with higher mAP scores (91.1% on mAP@0.5 and 78.3% on mAP@0.5:0.95) for AU-INV-P-PALS. The proposed model demonstrated lower inference latency (8.1 ms), higher precision score (82.7% for IoU = 0.5), and higher recall score (90.8% for IoU = 0.5) than recent deep learning -based detection models in the literature. The top -q approximation technique in the detection head results in a reduced false -positive rate compared to conventional models. Finally, the performance of the proposed framework is tested on CCTV traffic footage captured on city roads in Chennai, India.
C1 [Ilamathi, M.; Ramakrishnan, Sabitha; Babusankar, Rakhul Kumar] Anna Univ, Madras Inst Technol, Dept Instrumentat Engn, Chennai 600044, India.
C3 Anna University; Madras Institute of Technology; Anna University Chennai
RP Ilamathi, M (corresponding author), Anna Univ, Madras Inst Technol, Dept Instrumentat Engn, Chennai 600044, India.
EM ilam.2210@gmail.com
FU Centre for Research, Anna University, Chennai, India
FX We are sincerely obliged to the Centre for Research, Anna University,
   Chennai, India, for supporting this work by providing us with Anna
   Central Research Fellowship. We are also grateful to the Greater Chennai
   Police for their support throughout the data collection process.
CR Amjoud AB, 2023, IEEE ACCESS, V11, P35479, DOI 10.1109/ACCESS.2023.3266093
   Bochkovskiy A., 2020, ARXIV, DOI [10.48550/ARXIV.2004.10934, DOI 10.48550/ARXIV.2004.10934]
   Bommes M, 2016, TRANSP RES PROC, V14, P4495, DOI 10.1016/j.trpro.2016.05.372
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Carranza-García M, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13010089
   Chen PX, 2022, LECT NOTES COMPUT SC, V13670, P70, DOI 10.1007/978-3-031-20080-9_5
   Chen Z, 2021, Arxiv
   Cuturi M., 2013, Ad-vances in Neural Information Processing Systems, V26, P1
   Deshmukh P, 2023, EXPERT SYST APPL, V213, DOI 10.1016/j.eswa.2022.118992
   Deshmukh P, 2020, ADV INTELL SYST COMP, V1040, P457, DOI 10.1007/978-981-15-1451-7_49
   Dewi C, 2022, BIG DATA COGN COMPUT, V6, DOI 10.3390/bdcc6040149
   Dong C, 2023, PATTERN RECOGN, V137, DOI 10.1016/j.patcog.2022.109256
   Emam Z., 2021, STATE DATA COMPUTER
   Ge Z, 2021, PROC CVPR IEEE, P303, DOI 10.1109/CVPR46437.2021.00037
   Han GX, 2022, PROC CVPR IEEE, P5311, DOI 10.1109/CVPR52688.2022.00525
   Han K, 2023, IEEE T PATTERN ANAL, V45, P87, DOI 10.1109/TPAMI.2022.3152247
   Hassaballah M, 2020, PATTERN ANAL APPL, V23, P1505, DOI 10.1007/s10044-020-00874-9
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Hengduo Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10585, DOI 10.1109/CVPR42600.2020.01060
   Islam F., 2023, DEF COMMER SENS
   Jiang PY, 2022, PROCEDIA COMPUT SCI, V199, P1066, DOI 10.1016/j.procs.2022.01.135
   Jocher G., 2023, Ultralytics YOLOv8
   Jocher Glenn, 2022, Zenodo, DOI 10.5281/ZENODO.3908559
   Kingma D. P., 2014, arXiv
   Kumar A, 2020, EURASIP J WIREL COMM, V2020, DOI 10.1186/s13638-020-01826-x
   Lin T.-Y., 2017, PROC CVPR IEEE, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu ZM, 2018, MATH PROBL ENG, V2018, DOI 10.1155/2018/3518959
   Lou Liangshan, 2023, Procedia Computer Science, P397, DOI 10.1016/j.procs.2023.08.178
   Mingxing Tan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10778, DOI 10.1109/CVPR42600.2020.01079
   Mittal D, 2018, INT CONF COMMUN SYST, P589, DOI 10.1109/COMSNETS.2018.8328279
   Park K., 2007, INT C IM PROC COMP V
   Ra M, 2018, EXPERT SYST APPL, V101, P116, DOI 10.1016/j.eswa.2018.02.005
   Rayhan F, 2021, PATTERN ANAL APPL, V24, P1757, DOI 10.1007/s10044-021-01004-9
   Redmon J, 2018, Arxiv, DOI arXiv:1804.02767
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1476, DOI 10.1109/TPAMI.2016.2601099
   Rodrawangpai B, 2022, MACH LEARN APPL, V10, DOI 10.1016/j.mlwa.2022.100403
   Roy S., 2018, BR MACH VIS C
   Ruder S, 2016, ARXIV
   Settles B., 2012, ACTIVE LEARNING, DOI DOI 10.2200/S00429ED1V01Y201207AIM018
   Shah Junayed M., 2021, 2021 INT C INN INT S, P1, DOI [10.1109/INISTA52262.2021.9548650, DOI 10.1109/INISTA52262.2021.9548650]
   Sivaraman S, 2013, IEEE T INTELL TRANSP, V14, P1773, DOI 10.1109/TITS.2013.2266661
   Song HS, 2019, EUR TRANSP RES REV, V11, DOI 10.1186/s12544-019-0390-4
   Sun P, 2021, PATTERN ANAL APPL, V24, P1357, DOI 10.1007/s10044-021-00993-x
   Terven J, 2023, MACH LEARN KNOW EXTR, V5, P1680, DOI 10.3390/make5040083
   Tian Z., 2019, arXiv
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang CY, 2022, Arxiv, DOI [arXiv:2207.02696, DOI 10.48550/ARXIV.2207.02696]
   Wang JQ, 2019, PROC CVPR IEEE, P2960, DOI 10.1109/CVPR.2019.00308
   Wang P, 2022, PROC CVPR IEEE, P9357, DOI 10.1109/CVPR52688.2022.00915
   Wong TT, 2020, IEEE T KNOWL DATA EN, V32, P1586, DOI 10.1109/TKDE.2019.2912815
   Yang T, 2018, ARXIV
   Yang Z, 2018, IMAGE VISION COMPUT, V69, P143, DOI 10.1016/j.imavis.2017.09.008
   Zhang W, 2010, DIGIT SIGNAL PROCESS, V20, P793, DOI 10.1016/j.dsp.2009.10.006
   Zhao Zhong-Qiu, 2019, IEEE Trans Neural Netw Learn Syst, V30, P3212, DOI 10.1109/TNNLS.2018.2876865
   Zhu FZ, 2023, EGYPT J REMOTE SENS, V26, P351, DOI 10.1016/j.ejrs.2023.04.003
NR 59
TC 0
Z9 0
U1 2
U2 2
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUL
PY 2024
VL 147
AR 105081
DI 10.1016/j.imavis.2024.105081
EA MAY 2024
PG 16
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA XU5W6
UT WOS:001264213700001
DA 2024-08-05
ER

PT J
AU Zhang, SJ
   Liu, T
   Li, ZY
   Sun, Y
AF Zhang, Sijia
   Liu, Ting
   Li, Zhuoyuan
   Sun, Yi
TI Arbitrary 3D stylization of radiance fields
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Novel view synthesis; Neural radiance fields; Style transfer
AB 3D Stylization that creates stylized multi-view images is quite challenging, as it requires not only generating images which align with the desired style but also maintaining consistency across different perspectives. Most previous image style transfer methods focus on the 2D image domain and stylize each view independently, suffering from multi-view inconsistency. To tackle this challenging problem, we build on the neural radiance fields (NeRF) to stylize each 3D scene, as NeRF inherently ensures consistency across multiple perspectives, and has two sub-networks of geometry and appearance where appearance stylization cannot change the geometry. To enable arbitrary style transfer and more explicit and precise style adjustment, we introduce the CLIP model, which allows for style transfer based on either a text prompt or an arbitrary style image. We employ an ensemble of loss functions, of which CLIP loss ensures the similarity between the shared latent embeddings and generated style images, and Mask Loss is to constrain the 3D geometry to avoid non-smooth surface of NeRF. Experimental results demonstrate the effectiveness of our arbitrary 3D stylization generalized across diverse datasets. The proposed method outperforms most image-based and text-based 3D stylization models in terms of style transfer quality, producing pleasing images.
C1 [Zhang, Sijia; Liu, Ting; Li, Zhuoyuan; Sun, Yi] Dalian Univ Technol, Dalian, Peoples R China.
C3 Dalian University of Technology
RP Li, ZY (corresponding author), Dalian Univ Technol, Dalian, Peoples R China.
EM zsj0408@mail.dult.edu.cn; TingLiu2017@mail.dlut.edu.cn;
   lslwfly@outlook.com; lslwf@dlut.edu.cn
CR An J, 2021, PROC CVPR IEEE, P862, DOI 10.1109/CVPR46437.2021.00092
   Chen JF, 2023, PROCEEDINGS OF THE THIRTY-SECOND INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, IJCAI 2023, P5788
   Chen YS, 2022, Arxiv, DOI [arXiv:2208.07059, DOI 10.48550/ARXIV.2208.07059, 10.48550/arXiv.2208.07059]
   Chiang PZ, 2022, IEEE WINT CONF APPL, P215, DOI 10.1109/WACV51458.2022.00029
   Chiu Tai-Yin., 2020, European Conference on Computer Vision, P169
   Fan ZW, 2022, LECT NOTES COMPUT SC, V13675, P636, DOI 10.1007/978-3-031-19784-0_37
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Godi M., 2019, Texel-Att: Representing and Classifying Element-Based Textures by Attributes
   Ha D, 2016, Arxiv, DOI arXiv:1609.09106
   Höllein L, 2022, PROC CVPR IEEE, P6188, DOI 10.1109/CVPR52688.2022.00610
   Huang H.-P., 2021, IEEE CVF INT C COMP, P13869
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Huang YH, 2022, PROC CVPR IEEE, P18321, DOI 10.1109/CVPR52688.2022.01780
   Li WZ, 2023, Arxiv, DOI arXiv:2308.12452
   Li XT, 2018, Arxiv, DOI [arXiv:1808.04537, DOI 10.48550/ARXIV.1808.04537]
   Li YJ, 2017, ADV NEUR IN, V30
   Liu SH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6629, DOI 10.1109/ICCV48922.2021.00658
   Maxwell J.C., 1873, A Treatise on Electricity and Magnetism, DOI DOI 10.1017/CBO9780511709333
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Mildenhall B, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322980
   Nguyen-Phuoc Thu, 2022, arXiv
   Nichol K., 2016, Kiri Nichol, V5
   Park DY, 2019, PROC CVPR IEEE, P5873, DOI 10.1109/CVPR.2019.00603
   Park JH, 2019, IMAGE VISION COMPUT, V87, P13, DOI 10.1016/j.imavis.2019.04.001
   Qiao YX, 2021, IEEE T IMAGE PROCESS, V30, P3154, DOI 10.1109/TIP.2021.3058566
   Radford A, 2021, PR MACH LEARN RES, V139
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang C, 2023, IEEE Transactions on Visualization and Computer Graphics
   Wang C, 2022, PROC CVPR IEEE, P3825, DOI 10.1109/CVPR52688.2022.00381
   Wang Y, 2023, COMPUT GRAPH-UK, V116, P102, DOI 10.1016/j.cag.2023.08.009
   Xide Xia, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P327, DOI 10.1007/978-3-030-58598-3_20
   Yu Yue, 2024, Image and Vision Computing, V143, DOI 10.1016/j.imavis.2024.104956
   Zhang K, 2022, LECT NOTES COMPUT SC, V13691, P717, DOI 10.1007/978-3-031-19821-2_41
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhao SY, 2020, PROC CVPR IEEE, P6277, DOI 10.1109/CVPR42600.2020.00631
NR 35
TC 0
Z9 0
U1 2
U2 2
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD MAY
PY 2024
VL 145
AR 104971
DI 10.1016/j.imavis.2024.104971
EA MAR 2024
PG 11
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA QF3E2
UT WOS:001219416000001
DA 2024-08-05
ER

PT J
AU Bisogni, C
   Nappi, M
   Tortora, G
   Del Bimbo, A
AF Bisogni, Carmen
   Nappi, Michele
   Tortora, Genoveffa
   Del Bimbo, Alberto
TI Gaze analysis: A survey on its applications
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Gaze analysis gaze; Applications; Human gaze; Biometrics; HCI; VR; AR;
   Healthcare
ID EYE-TRACKING; LOCALIZATION; CALIBRATION; ATTENTION; PATTERNS
AB The examination of ocular movements has a wide range of applications due to the current developments in sensors that are now able to collect this biometric. This type of investigation is known as "gaze analysis". The gaze has successfully examined a subject's physical and mental status in the past. As a result, over the last few decades, a large and diverse amount of literature on this subject has been generated and presented. The aim of this study is to collect and debate current gaze analysis methods based on their application field. Due to the context-specific needs for performance and efficiency, the eye movements under research are frequently evaluated from completely distinct perspectives. As a result, a collection of data, methods, and discussions ranging from the medical community to virtual and augmented reality, as well as human computer interface and remote learning, has been produced. In addition to providing a peek of novel observation on the issue of gaze analysis, the gaps between and within areas are also discussed to provide points for researchers to pursue.
C1 [Bisogni, Carmen; Nappi, Michele; Tortora, Genoveffa] Univ Salerno, Via Giovanni Paolo II 132, I-84084 Fisciano, Italy.
   [Del Bimbo, Alberto] Univ Firenze, Via Santa Marta 3, I-50139 Florence, Italy.
C3 University of Salerno; University of Florence
RP Bisogni, C (corresponding author), Univ Salerno, Via Giovanni Paolo II 132, I-84084 Fisciano, Italy.
EM cbisogni@unisa.it; mnappi@unisa.it; tortora@unisa.it;
   alberto.delbimbo@unifi.it
FU Project Information Disorder Awareness (IDA) included in the Spoke
   2-Misinformation and Fakes of the Research and Innovation Program
   [PE00000014]; SEcurity and RIghts in the Cyber Space (SERICS) ", under
   the National Recovery and Resilience Plan, Mission 4 "Education and
   Research"-Component 2 "From Research to Enterprise"-Investment 1.3;
   European Union-NextGenerationEU
FX This work was partially supported by the Project Information Disorder
   Awareness (IDA) included in the Spoke 2-Misinformation and Fakes of the
   Research and Innovation Program PE00000014, "SEcurity and RIghts in the
   Cyber Space (SERICS) ", under the National Recovery and Resilience Plan,
   Mission 4 "Education and Research"-Component 2 "From Research to
   Enterprise"-Investment 1.3, funded by the European
   Union-NextGenerationEU.
CR Abdelrahman A.A., 2022, 2023 8 INT C FRONT S, P98
   Adhanom IB, 2023, VIRTUAL REAL-LONDON, V27, P1481, DOI 10.1007/s10055-022-00738-z
   Ahmed M, 2021, MULTIMEDIA SYST, V27, P429, DOI 10.1007/s00530-020-00744-8
   Ahmed M, 2019, IMAGE VISION COMPUT, V88, P52, DOI 10.1016/j.imavis.2019.05.002
   Akinyelu AA, 2020, IEEE ACCESS, V8, P142581, DOI 10.1109/ACCESS.2020.3013540
   [Anonymous], Those graphs has been designed using images from flaticon.com
   Anzalone SM, 2019, PATTERN RECOGN LETT, V118, P42, DOI 10.1016/j.patrec.2018.03.007
   Asadi A, 2023, HUM MOVEMENT SCI, V87, DOI 10.1016/j.humov.2022.103038
   Bek J, 2020, J NEUROSCI METH, V331, DOI 10.1016/j.jneumeth.2019.108524
   Benn DE, 1997, LECT NOTES COMPUT SC, V1206, P3
   Best V, 2023, TRENDS HEAR, V27, DOI 10.1177/23312165231152356
   Blattgerste J, 2018, COMMUNICATION BY GAZE INTERACTION (COGAIN 2018), DOI 10.1145/3206343.3206349
   Bonazzi P, 2023, IEEE SENSOR, DOI 10.1109/SENSORS56945.2023.10325167
   Boyd K, 2021, HEALTH EXPECT, V24, P1207, DOI 10.1111/hex.13251
   Campbell Anna, 2017, J Gerontol B Psychol Sci Soc Sci, V72, P633, DOI 10.1093/geronb/gbv114
   Casanova A, 2021, PATTERN RECOGN LETT, V148, P114, DOI 10.1016/j.patrec.2021.05.006
   Castner N, 2016, MCPMD'18: PROCEEDINGS OF THE WORKSHOP ON MODELING COGNITIVE PROCESSES FROM MULTIMODAL DATA, DOI 10.1145/3279810.3279845
   Cazzato D, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20133739
   Chang KM, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19071612
   Chatelain P, 2020, IEEE T CYBERNETICS, V50, P153, DOI 10.1109/TCYB.2018.2866274
   Cheng Y., 2021, arXiv
   Cheng YH, 2022, INT C PATT RECOG, P3341, DOI 10.1109/ICPR56361.2022.9956687
   Chettaoui N, 2023, EDUC INF TECHNOL, V28, P833, DOI 10.1007/s10639-022-11163-9
   Cimmino L, 2021, PATTERN RECOGN LETT, V151, P252, DOI 10.1016/j.patrec.2021.09.010
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Deniel J, 2023, THEOR ISS ERGON SCI, V24, P54, DOI 10.1080/1463922X.2022.2036861
   Dixit A., 2018, 2018 9 INT C COMP CO, P1, DOI [10.1109/ICCCNT.2018.8493740, DOI 10.1109/ICCCNT.2018.8493740]
   Duan H, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3337066
   Dubois B, 2016, J ALZHEIMERS DIS, V49, P617, DOI 10.3233/JAD-150692
   Erickson A, 2020, IEEE T VIS COMPUT GR, V26, P1934, DOI 10.1109/TVCG.2020.2973054
   Fabiano D, 2020, PATTERN RECOGN LETT, V135, P204, DOI 10.1016/j.patrec.2020.04.028
   Fowler TJ., 2003, CLIN NEUROL JAPAN, V3rd
   Fuhl W, 2016, MCPMD'18: PROCEEDINGS OF THE WORKSHOP ON MODELING COGNITIVE PROCESSES FROM MULTIMODAL DATA, DOI 10.1145/3279810.3279844
   Fuhl W, 2016, MCPMD'18: PROCEEDINGS OF THE WORKSHOP ON MODELING COGNITIVE PROCESSES FROM MULTIMODAL DATA, DOI 10.1145/3279810.3279843
   Gahyun Sung, 2021, Proceedings of the ACM on Human-Computer Interaction, V5, DOI 10.1145/3449208
   Gatoula P, 2021, COMP MED SY, P189, DOI 10.1109/CBMS52027.2021.00070
   George C, 2020, LECT NOTES COMPUT SC, V12242, P61, DOI 10.1007/978-3-030-58465-8_5
   Ghosh S., 2021, arXiv, DOI DOI 10.48550/ARXIV.2108.05479
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Gotardi GC, 2022, ERGONOMICS, V65, P1302, DOI 10.1080/00140139.2022.2028901
   Guan YR, 2023, IEEE SIGNAL PROC LET, V30, P1687, DOI 10.1109/LSP.2023.3332569
   Hapsari R. K., 2020, Journal of Physics: Conference Series, V1477, DOI 10.1088/1742-6596/1477/2/022037
   Hariri W, 2022, SIGNAL IMAGE VIDEO P, V16, P605, DOI 10.1007/s11760-021-02050-w
   Hodgson TL, 2024, INT J LANG COMM DIS, V59, P715, DOI 10.1111/1460-6984.12960
   Hooge ITC, 2023, BEHAV RES METHODS, V55, P4128, DOI 10.3758/s13428-022-02010-3
   Hsu CF, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3311784
   Hu ZX, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3160534
   Hu ZM, 2020, IEEE T VIS COMPUT GR, V26, P1902, DOI 10.1109/TVCG.2020.2973473
   Hu ZX, 2022, IEEE T IND ELECTRON, V69, P1800, DOI 10.1109/TIE.2021.3057033
   Huang L., 2022, A Study of&Nbsp; the&Nbsp;Challenges of&Nbsp;Eye Tracking Systems and&Nbsp;Gaze Interaction for&Nbsp;Individuals with&Nbsp;Motor Disabilities, P396, DOI [10.1007/978-3-031-17902-0_28, DOI 10.1007/978-3-031-17902-0_28]
   Huang LJ, 2020, PATTERN RECOGN LETT, V138, P608, DOI 10.1016/j.patrec.2020.09.017
   Huang MX, 2018, ACM T INTEL SYST TEC, V9, DOI 10.1145/3156682
   Ian H.Witten., 2011, Data Mining: PracticalMachine Learning Tools and Techniques, VThird
   ILLINGWORTH J, 1987, IEEE T PATTERN ANAL, V9, P690, DOI 10.1109/TPAMI.1987.4767964
   Insch PM, 2017, BRAIN COGNITION, V116, P47, DOI 10.1016/j.bandc.2017.03.004
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Jan F, 2018, MULTIMED TOOLS APPL, V77, P1041, DOI 10.1007/s11042-016-4334-x
   Jayanthi J, 2021, J AMB INTEL HUM COMP, V12, P3271, DOI 10.1007/s12652-020-02172-y
   Kar Pragma, 2020, Proceedings of the ACM on Human-Computer Interaction, V4, DOI 10.1145/3394974
   Karim F, 2019, NEURAL NETWORKS, V116, P237, DOI 10.1016/j.neunet.2019.04.014
   Kasprowski P, 2016, SMART INNOV SYST TEC, V57, P83, DOI 10.1007/978-3-319-39627-9_8
   Katsini C, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376840
   Kellnhofer P, 2019, IEEE I CONF COMP VIS, P6911, DOI 10.1109/ICCV.2019.00701
   Khan MQ, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19245540
   Kok EM, 2023, COGNITIVE SCI, V47, DOI 10.1111/cogs.13247
   Konrad R, 2019, SIGGRAPH '19 -ACM SIGGRAPH 2019 TALKS, DOI 10.1145/3306307.3328201
   Krafka K, 2016, PROC CVPR IEEE, P2176, DOI 10.1109/CVPR.2016.239
   Krajancich B, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417820
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kumar JA, 2023, EDUC INF TECHNOL, V28, P7877, DOI 10.1007/s10639-022-11504-8
   Kutt Grete Helena, 2020, Proceedings of the ACM on Human-Computer Interaction, V4, DOI 10.1145/3415207
   Lathuilière S, 2019, PATTERN RECOGN LETT, V118, P61, DOI 10.1016/j.patrec.2018.05.023
   Lee I., 2022, P AS C COMP VIS, P3379
   Lenoble Q, 2018, CORTEX, V107, P4, DOI 10.1016/j.cortex.2018.06.002
   Lévêque L, 2021, EUR SIGNAL PR CONF, P1249, DOI 10.23919/Eusipco47968.2020.9287678
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu CH, 2023, J CLIN NEUROSCI, V117, P173, DOI 10.1016/j.jocn.2023.10.004
   Liu JH, 2022, PATTERN RECOGN, V132, DOI 10.1016/j.patcog.2022.108944
   Lucio DR, 2019, SIBGRAPI, P178, DOI 10.1109/SIBGRAPI.2019.00032
   Lystbaek Mathias N., 2022, Proceedings of the ACM on Human-Computer Interaction, V6, DOI 10.1145/3530886
   Lystbaek Mathias N., 2022, Proceedings of the ACM on Human-Computer Interaction, V6, DOI 10.1145/3530882
   Madhusanka B., 2022, Predictive Modeling in Biomedical Data Mining and Analysis, Advanced Studies in Complex Systems: Theory and Applications, P137, DOI [10.1016/B978-0-323-99864-2.00016-0, DOI 10.1016/B978-0-323-99864-2.00016-0]
   Mahanama B, 2022, FRONT COMP SCI-SWITZ, V3, DOI 10.3389/fcomp.2021.733531
   Mao RZ, 2021, IEEE T HUM-MACH SYST, V51, P87, DOI 10.1109/THMS.2021.3053196
   Mariam K, 2022, IEEE J BIOMED HEALTH, V26, P3025, DOI 10.1109/JBHI.2022.3148944
   Matsuda Akira, 2021, Proceedings of the ACM on Human-Computer Interaction, V5, DOI 10.1145/3461726
   McMillan Donald, 2019, Proceedings of the ACM on Human-Computer Interaction, V3, DOI 10.1145/3359278
   Mengoudi K, 2020, IEEE J BIOMED HEALTH, V24, P3066, DOI 10.1109/JBHI.2020.3004686
   Mesfin G, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3303080
   Microsoft, Eye tracking overview-mixed reality.
   Min-Allah N, 2021, MULTIMEDIA SYST, V27, P753, DOI 10.1007/s00530-021-00806-5
   Modi N, 2023, INT J HUM-COMPUT INT, V39, P721, DOI 10.1080/10447318.2022.2047318
   N. I. of Health, NIMH Data Archive nda home page
   Iandola FN, 2016, Arxiv, DOI [arXiv:1602.07360, 10.48550/arXiv.1602.07360]
   Niehorster DC, 2020, BEHAV RES METHODS, V52, P2515, DOI 10.3758/s13428-020-01400-9
   Niehorster DC, 2020, BEHAV RES METHODS, V52, P1140, DOI 10.3758/s13428-019-01307-0
   Nijholt Anton, 2022, Proceedings of International Conference on Industrial Instrumentation and Control: ICI2C 2021. Lecture Notes in Electrical Engineering (815), P1, DOI 10.1007/978-981-16-7011-4_1
   Nonaka S, 2022, PROC CVPR IEEE, P2182, DOI 10.1109/CVPR52688.2022.00223
   Oki T, 2019, IEEE SYS MAN CYBERN, P1062, DOI 10.1109/SMC.2019.8914443
   Pathirana P, 2022, EXPERT SYST APPL, V199, DOI 10.1016/j.eswa.2022.116894
   Penedo T, 2018, EXP BRAIN RES, V236, P3319, DOI 10.1007/s00221-018-5385-1
   Perez A., 2020, Alzheimers Dement., V16, DOI [10.1002/alz.043869, DOI 10.1002/ALZ.043869]
   Pershin I, 2022, IEEE J BIOMED HEALTH, V26, P4541, DOI 10.1109/JBHI.2022.3183299
   Pfeuffer K, 2021, COMPUT GRAPH-UK, V95, P1, DOI 10.1016/j.cag.2021.01.001
   Plopski A, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3491207
   Porcu S, 2020, IEEE T NETW SERV MAN, V17, P2702, DOI 10.1109/TNSM.2020.3018303
   Purves D., 2001, NEUROSCIENCE
   Qinjie Ju, 2019, Proceedings of the ACM on Human-Computer Interaction, V3, DOI 10.1145/3300962
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ringhand M, 2022, TRANSPORT RES F-TRAF, V91, P116, DOI 10.1016/j.trf.2022.09.010
   Robin Mahmudul Hasan, 2020, BDIOT 2020: Proceedings of the 2020 4th International Conference on Big Data and Internet of Things, P24, DOI 10.1145/3421537.3421553
   Rodger H, 2023, J EXP CHILD PSYCHOL, V229, DOI 10.1016/j.jecp.2022.105622
   Rozado D, 2017, ACM T ACCESS COMPUT, V10, DOI 10.1145/3075301
   Sabab SA, 2022, IEEE ACCESS, V10, P70779, DOI 10.1109/ACCESS.2022.3187969
   Santini T, 2016, 2016 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS (ETRA 2016), P163, DOI 10.1145/2857491.2857512
   Senarath S, 2022, IEEE ACCESS, V10, P64904, DOI 10.1109/ACCESS.2022.3183357
   Sharma VK, 2022, ACM T ACCESS COMPUT, V15, DOI 10.1145/3530822
   Shi R., 2023, PROC ACM HUM COMPUT, DOI [10.1145/3591129, DOI 10.1145/3591129]
   Sidenmark L., 2021, P 2021 CHI C HUM FAC, DOI [10.1145/3411764, DOI 10.1145/3411764]
   Sidenmark L, 2020, ACM T COMPUT-HUM INT, V27, DOI 10.1145/3361218
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh J, 2019, HELIYON, V5, DOI 10.1016/j.heliyon.2019.e03033
   Singhal Prateek, 2022, Proceedings of Second Doctoral Symposium on Computational Intelligence: DoSCI 2021. Advances in Intelligent Systems and Computing (1374), P103, DOI 10.1007/978-981-16-3346-1_9
   Song H, 2017, IEEE T VIS COMPUT GR, V23, P311, DOI 10.1109/TVCG.2016.2598796
   Song H, 2014, IEEE T VIS COMPUT GR, V20, P726, DOI 10.1109/TVCG.2013.271
   Spatola N, 2021, ACM T HUM-ROBOT INTE, V10, DOI 10.1145/3459994
   Spiller M, 2021, ACM T INTERACT INTEL, V11, DOI 10.1145/3446638
   Steinhauser J, 2019, APPETITE, V141, DOI 10.1016/j.appet.2019.104337
   Stuart N, 2023, J AUTISM DEV DISORD, V53, P1884, DOI 10.1007/s10803-022-05443-z
   Sun QC, 2018, ACCIDENT ANAL PREV, V113, P85, DOI 10.1016/j.aap.2018.01.019
   Sutskever I, 2014, ADV NEUR IN, V27
   Syed R, 2020, WEB CONFERENCE 2020: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2020), P1693, DOI 10.1145/3366423.3380240
   Thirunarayanan I, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3078836
   Tolosa E, 2021, LANCET NEUROL, V20, P385, DOI 10.1016/S1474-4422(21)00030-2
   Trefzger M., 2021, INFORMATIK 2020, P1163, DOI [10.18420/inf2020_108, DOI 10.18420/INF2020_108]
   Tsuchiya K, 2023, Bloomsbury Stud Lang, P1, DOI 10.5040/9781350298507
   Tupikovskaja-Omovie Z, 2022, INT J FASH DES TECHN, V15, P178, DOI 10.1080/17543266.2021.1980614
   van't Hof M, 2021, AUTISM, V25, P862, DOI 10.1177/1362361320971107
   Veerabhadrappa R, 2022, ANN IEEE SYST CONF, DOI 10.1109/SysCon53536.2022.9773865
   Vehlen A, 2023, J PSYCHIATR RES, V159, P50, DOI 10.1016/j.jpsychires.2023.01.016
   Vora S, 2018, IEEE T INTELL VEHICL, V3, P254, DOI 10.1109/TIV.2018.2843120
   Wang K, 2019, PROC CVPR IEEE, P9823, DOI 10.1109/CVPR.2019.01006
   Wang S, 2022, IEEE T MED IMAGING, V41, P1688, DOI 10.1109/TMI.2022.3146973
   Wang YH, 2021, IEEE ACCESS, V9, P137991, DOI 10.1109/ACCESS.2021.3117780
   Williams EH, 2023, SCI REP-UK, V13, DOI 10.1038/s41598-023-41900-0
   Wu SW, 2019, ICMI'19: PROCEEDINGS OF THE 2019 INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P40, DOI 10.1145/3340555.3353739
   Xiao LM, 2023, COMPUT ELECTR ENG, V107, DOI 10.1016/j.compeleceng.2023.108625
   Yasui Y, 2019, J PROSTHODONT RES, V63, P210, DOI 10.1016/j.jpor.2018.11.011
   Zemblys R, 2019, BEHAV RES METHODS, V51, P840, DOI 10.3758/s13428-018-1133-5
   Zeng D, 2021, IET BIOMETRICS, V10, P581, DOI 10.1049/bme2.12029
   Zhang RH, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4951, DOI 10.24963/ijcai.2020/689
   Zhang XC, 2015, PROC CVPR IEEE, P4511, DOI 10.1109/CVPR.2015.7299081
NR 154
TC 1
Z9 1
U1 1
U2 1
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD APR
PY 2024
VL 144
AR 104961
DI 10.1016/j.imavis.2024.104961
EA MAR 2024
PG 25
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA PZ1Y7
UT WOS:001217821100001
OA hybrid
DA 2024-08-05
ER

PT J
AU Zeng, XH
   Guo, JQ
   Wei, YF
   Zhuo, Y
AF Zeng, Xianhua
   Guo, Jueqiu
   Wei, Yifan
   Zhuo, Yang
TI Deep hybrid manifold for image set classification
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE SPD manifold; Grassmann manifold; Visual classification; Hybrid
   manifold; Neural network
ID GEOMETRY
AB The exponential growth of the data volume of image sets, which contain more information than a single image, has attracted increasing attention from researchers. Image set data are often described as covariance matrices or linear subspaces, and the unique geometries they span are symmetric positive definite (SPD) manifolds and Grassmann manifolds, respectively. Image set data are often described as covariance matrices or linear subspaces, and the distinctive geometries they span are symmetric positive definite (SPD) manifold and Grassmann manifold, respectively. However, most studies focus on a single manifold and ignore the useful information of the another manifold. Based on this, we propose a new Deep Hybrid Manifold Network (DHMNet). The DHMNet consists of backbone network, stackable Hybrid Manifold AutoEncoder (HMAE) and,Maximum Fusion Module (MFM). The image set data is modeled through SPD manifold and Grassmann manifold. The modeled data is input into the backbone network composed of SPDNet and GrNet for initial feature extraction, and the output manifold data are input into HMAEs. The HMAE effectively extracts and hybridizes complementary information from different manifolds and has the ability to generate deep representations with rich structural semantic information. For the three image datasets used, DHMNet with two HMAEs improves the classification accuracy by 3.83-5.76% over the classical SPDNet, and even reaches the best when compared to other models, with the best performance on the First Person Hand Action (FPHA) dataset for skeleton -based hand action recognition.
C1 [Zeng, Xianhua; Guo, Jueqiu; Wei, Yifan; Zhuo, Yang] Chongqing Univ Posts & Telecommun, Sch Comp Sci & Technol, Sch Artificial Intelligence, Chongqing 400065, Peoples R China.
   [Zeng, Xianhua] U Posts & Telecommun, Sch Comp Sci & Technol, Sch Artificial Intelligence, Chongqing 400065, Peoples R China.
C3 Chongqing University of Posts & Telecommunications
RP Zeng, XH (corresponding author), U Posts & Telecommun, Sch Comp Sci & Technol, Sch Artificial Intelligence, Chongqing 400065, Peoples R China.
EM zengxh@cqupt.edu.cn
FU National Natural Science Foundation of China [62076044]; Chongqing
   Talent Plan Project [cstc2022ycjh-bgzxm0160]
FX The authors would like to thank the anonymous reviewers for their help.
   This work was supported by the National Natural Science Foundation of
   China [No. 62076044] and Chongqing Talent Plan Project [No.
   cstc2022ycjh-bgzxm0160] .
CR Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1
   Arsigny V, 2007, SIAM J MATRIX ANAL A, V29, P328, DOI 10.1137/050637996
   Baudat G, 2000, NEURAL COMPUT, V12, P2385, DOI 10.1162/089976600300014980
   Bouza JJ, 2021, LECT NOTES COMPUT SC, V12729, P304, DOI 10.1007/978-3-030-78191-0_24
   Brooks D, 2019, ADV NEUR IN, V32
   Chakraborty R, 2022, IEEE T PATTERN ANAL, V44, P799, DOI 10.1109/TPAMI.2020.3003846
   Chen ZH, 2023, IEEE T BIG DATA, V9, P75, DOI 10.1109/TBDATA.2021.3113084
   Dhall A., 2014, P 16 INT C MULT INT, P461, DOI 10.1145/2663204.2666275
   Edelman A, 1998, SIAM J MATRIX ANAL A, V20, P303, DOI 10.1137/S0895479895290954
   Gao Z, 2020, IEEE T NEUR NET LEAR, V31, P3230, DOI 10.1109/TNNLS.2019.2939177
   Garcia-Hernando G, 2018, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2018.00050
   Harandi M, 2018, IEEE T PATTERN ANAL, V40, P48, DOI 10.1109/TPAMI.2017.2655048
   Harandi M, 2017, PR MACH LEARN RES, V70
   Harandi MT, 2012, LECT NOTES COMPUT SC, V7573, P216, DOI 10.1007/978-3-642-33709-3_16
   Hu JF, 2015, PROC CVPR IEEE, P5344, DOI 10.1109/CVPR.2015.7299172
   Huang ZW, 2018, AAAI CONF ARTIF INTE, P3279
   Huang ZW, 2017, AAAI CONF ARTIF INTE, P2036
   Huang ZW, 2015, PR MACH LEARN RES, V37, P720
   Huang ZW, 2015, PROC CVPR IEEE, P140, DOI 10.1109/CVPR.2015.7298609
   Huang ZW, 2015, PATTERN RECOGN, V48, P3113, DOI 10.1016/j.patcog.2015.03.011
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Nguyen XS, 2019, PROC CVPR IEEE, P12028, DOI 10.1109/CVPR.2019.01231
   Rahmani H, 2016, PROC CVPR IEEE, P1506, DOI 10.1109/CVPR.2016.167
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Turaga P, 2011, IEEE T PATTERN ANAL, V33, P2273, DOI 10.1109/TPAMI.2011.52
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang R., 2022, ASIAN C COMPUTER VIS, P646, DOI [10.1007/978-3-031-26351-4_39, DOI 10.1007/978-3-031-26351-4_39]
   Wang R, 2023, NEURAL NETWORKS, V161, P382, DOI 10.1016/j.neunet.2022.11.030
   Wang R, 2022, IEEE T COGN DEV SYST, V14, P957, DOI 10.1109/TCDS.2021.3086814
   Wang R, 2022, NEURAL NETWORKS, V151, P94, DOI 10.1016/j.neunet.2022.03.012
   Wang R, 2022, IEEE T NEUR NET LEAR, V33, P2208, DOI 10.1109/TNNLS.2020.3044176
   Wang R, 2021, IEEE T MULTIMEDIA, V23, P228, DOI 10.1109/TMM.2020.2981189
   Wang R, 2022, IEEE T BIG DATA, V8, P753, DOI 10.1109/TBDATA.2020.2982146
   Wang R, 2018, INT C PATT RECOG, P627, DOI 10.1109/ICPR.2018.8546030
   Wang RP, 2012, PROC CVPR IEEE, P2496, DOI 10.1109/CVPR.2012.6247965
   Zhang T, 2020, IEEE T MULTIMEDIA, V22, P2926, DOI 10.1109/TMM.2020.2966878
   Zhou LP, 2017, PROC CVPR IEEE, P7111, DOI 10.1109/CVPR.2017.752
NR 39
TC 0
Z9 0
U1 1
U2 1
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD MAR
PY 2024
VL 143
AR 104935
DI 10.1016/j.imavis.2024.104935
EA FEB 2024
PG 15
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA NR8N3
UT WOS:001202272600001
DA 2024-08-05
ER

PT J
AU Cai, AP
   Chen, LT
   Chen, YQ
   He, ZY
   Tao, SQ
   Zhou, C
AF Cai, Anping
   Chen, Leiting
   Chen, Yongqi
   He, Ziyu
   Tao, Shuqing
   Zhou, Chuan
TI Adaptive attribute distribution similarity for few-shot learning
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Similarity; Few-shot learning; Attribute separation; Attribute
   distribution similarity
AB Modern deep learning has many drawbacks, including a heavy reliance on labeled data. One of the key strategies for solving this problem is few-shot learning (FSL). With just a few labeled samples, FSL seeks to identify previously unknown classes. The majority of works that have been published thus far focus on comparing the features of query samples and support classes, which do not fully utilize the training set 's data and do not help sustain performance improvement. In our study, we compute a new attribute distribution similarity between support classes and a query sample of novel classes using attribute information on the training set. We suggest a fresh approach to three phases to accomplish our objective: 1) A attribute provider harnesses the visual features of the training set to construct attributes. 2) Choosing appropriate attributes for novel classes and enriching attributes to determine how similar novel classes and attributes are to one another. 3) To help with classification, attribute distribution similarity is computed for the first time by creating new correlations between the support classes and the query samples, which increases the accuracy of picture classification. Be aware that our solution won 't make the initial network settings larger. Experiments on inductive FSL tasks demonstrate the usefulness and practicality of our strategy. Specifically, Our method has achieved the highest performance in the 5-way 1shot task settings on the tiered-ImageNet and CUB 200 -2011 datasets, with impressive results of 73 .22% and 82 .34% respectively.
C1 [Cai, Anping; Chen, Leiting; Chen, Yongqi; Tao, Shuqing; Zhou, Chuan] Univ Elect Sci & Technol China, Sch Informat & Software Engn, Chengdu 610054, Peoples R China.
   [Cai, Anping; Chen, Leiting; Chen, Yongqi; Tao, Shuqing; Zhou, Chuan] Univ Elect Sci & Technol China, Digital Media Technol Key Lab Sichuan Prov, Chengdu 610054, Peoples R China.
   [He, Ziyu] Ludong Univ China, Yantai, Peoples R China.
C3 University of Electronic Science & Technology of China; University of
   Electronic Science & Technology of China
RP Zhou, C (corresponding author), Univ Elect Sci & Technol China, Sch Informat & Software Engn, Chengdu 610054, Peoples R China.; Zhou, C (corresponding author), Univ Elect Sci & Technol China, Digital Media Technol Key Lab Sichuan Prov, Chengdu 610054, Peoples R China.
EM zhouchuan@uestc.edu.cn
FU Natural Science Foundation of Sichuan, China [2023NSFSC0468,
   2023NSFSC0031]
FX <B>Acknowledgments</B> This work was supported by the Natural Science
   Foundation of Sichuan, China (No. 2023NSFSC0468, No. 2023NSFSC0031) .
CR Afrasiyabi Arman, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P18, DOI 10.1007/978-3-030-58558-7_2
   Bin Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P438, DOI 10.1007/978-3-030-58548-8_26
   Chen W., 2019, INT C LEARNING REPRE
   Chen WT, 2023, PROC CVPR IEEE, P23581, DOI 10.1109/CVPR52729.2023.02258
   Chen YB, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9042, DOI 10.1109/ICCV48922.2021.00893
   Chen ZT, 2019, IEEE T IMAGE PROCESS, V28, P4594, DOI 10.1109/TIP.2019.2910052
   Cheng J, 2022, IEEE T IMAGE PROCESS, V31, P1587, DOI 10.1109/TIP.2022.3143692
   Chi Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12200, DOI 10.1109/CVPR42600.2020.01222
   Dhillon G., 2020, ICLR, P1
   Finn C, 2017, PR MACH LEARN RES, V70
   Gidaris S, 2018, PROC CVPR IEEE, P4367, DOI 10.1109/CVPR.2018.00459
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hiller M., 2022, Adv. Neural Inf. Proces. Syst., V35, P4059
   Holkar A, 2022, IMAGE VISION COMPUT, V120, DOI 10.1016/j.imavis.2022.104420
   Hou RB, 2019, ADV NEUR IN, V32
   Hu P, 2019, Arxiv, DOI arXiv:1906.04833
   Huang HW, 2021, PATTERN RECOGN, V116, DOI 10.1016/j.patcog.2021.107935
   Huang ST, 2021, AAAI CONF ARTIF INTE, V35, P7840
   Khan M.H., 2024, arXiv
   Li HY, 2019, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2019.00009
   Li P., 2021, Regularising Knowledge Transfer by meta Functional Learning
   Li WB, 2019, PROC CVPR IEEE, P7253, DOI 10.1109/CVPR.2019.00743
   Lin H, 2023, PROC CVPR IEEE, P19649, DOI 10.1109/CVPR52729.2023.01882
   Lyu Q., 2023, P AAAI C ART INT, V37, P9011
   Ma F, 2023, Arxiv, DOI arXiv:2301.07463
   Ma F, 2022, INT J COMPUT VISION, V130, P1244, DOI 10.1007/s11263-022-01600-0
   Ma F, 2022, IEEE T NEUR NET LEAR, V33, P6275, DOI 10.1109/TNNLS.2021.3073248
   Meng Y., 2023, INT C MACHINE LEARNI, P24457
   Paszke A., 2024, Advances in Neural Information Processing Systems, V32
   Peng ZM, 2019, IEEE I CONF COMP VIS, P441, DOI 10.1109/ICCV.2019.00053
   Qu M., 2020, INT C MACH LEARN, V119, P7867, DOI DOI 10.48550/ARXIV.2007.02387
   Schwartz E, 2022, PATTERN RECOGN LETT, V160, P142, DOI 10.1016/j.patrec.2022.06.012
   Snell J, 2017, ADV NEUR IN, V30
   Song YS, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3582688
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Tokmakov P, 2019, IEEE I CONF COMP VIS, P6381, DOI 10.1109/ICCV.2019.00647
   Vinyals O., 2016, Advances in neural information processing systems, V29
   Wang CF, 2021, NEUROCOMPUTING, V466, P16, DOI 10.1016/j.neucom.2021.09.016
   Wang HX, 2022, PROC CVPR IEEE, P9787, DOI 10.1109/CVPR52688.2022.00957
   Wang ZY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1524, DOI 10.1145/3394171.3413946
   Wertheimer D, 2021, PROC CVPR IEEE, P8008, DOI 10.1109/CVPR46437.2021.00792
   Xing C., 2019, ADV NEURAL INFORM PR, P4847
   Xu W., 2020, INT C LEARN REPR
   Xu WJ, 2022, INT J COMPUT VISION, V130, P1735, DOI 10.1007/s11263-022-01613-9
   Xue WQ, 2020, AAAI CONF ARTIF INTE, V34, P6558
   Yang FY, 2022, IEEE WINT CONF APPL, P1586, DOI 10.1109/WACV51458.2022.00165
   Yang Shuo, 2021, INT C LEARN REPR, P1
   Yang Y, 2021, FRONT INFORM TECH EL, V22, P1551, DOI 10.1631/FITEE.2100463
   Yonglong Tian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P266, DOI 10.1007/978-3-030-58568-6_16
   Yu Z, 2020, P IEEE CVF C COMP VI, P12856, DOI DOI 10.1109/CVPR42600.2020.01287
   Zhang BQ, 2021, PROC CVPR IEEE, P3753, DOI 10.1109/CVPR46437.2021.00375
   Zhou P., 2021, Uncertainty in artificial intelligence, P23
   Zou YX, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P156, DOI 10.1145/3394171.3413849
NR 53
TC 0
Z9 0
U1 2
U2 2
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD AUG
PY 2024
VL 148
AR 105118
DI 10.1016/j.imavis.2024.105118
EA JUN 2024
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA XF5B9
UT WOS:001260270800001
DA 2024-08-05
ER

PT J
AU Du, CJ
   Li, ZY
   Zhao, HJ
   He, SJ
   Yu, L
AF Du, Congju
   Li, Zhenyu
   Zhao, Huijuan
   He, Shuangjiang
   Yu, Li
TI Heterogeneous heatmap distillation framework based on unbiased alignment
   for lightweight human pose estimation
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Human pose estimation; Heterogeneous heatmap distillation; Unbiased
   heatmap alignment; Integral pose regression
AB The growing demand for mobile devices has generated interest in lightweight human pose estimation. Currently, lightweight estimation generally uses heatmap-based methods, which has demonstrated exceptional performance. However, their use of non-differentiable post-processing imposes considerable inference latencies. Conversely, integral-based approaches expedite the inference process by employing a soft-argmax operation but compromise in accuracy. Integrating explicit heatmap knowledge learned using the heatmap-based method into the implicit heatmap generated by the integral-based method, thereby combining the best of both worlds, offers a promising avenue. However, owing to the disparities in supervision and inference processes, the explicit and implicit heatmaps are heterogeneous. Consequently, direct transfer of knowledge presents difficulties in ensuring consistencies in heat value and location. In this paper, we propose a novel Heterogeneous Heatmap Distillation (HHD) framework that effectively tackles these challenges. The framework seamlessly integrates explicit heatmap knowledge that contains high-precision localization information into implicit heatmaps. The framework revolves around an unbiased heatmap alignment scheme encompassing two steps: heterogeneous heatmap normalization and unbiased cropping. Heterogeneous heatmap normalization separately normalizes the output feature maps of both the teacher and student models, alleviating potential heat value bias during the knowledge transfer. Unbiased cropping applies closed-form computation on the normalized teacher and student heatmap to eliminate location bias. Additionally, mirror expansion is implemented to handle potential cases wherein the cropped region extends beyond the image boundary. Extensive experiments demonstrate the efficiency and effectiveness of our methods on the MSCOCO and MPII datasets compared to other integral-based lightweight networks. Our source codes and pre-trained models are available at https://github.com/ducongju/HHD.
C1 [Du, Congju; Li, Zhenyu; Zhao, Huijuan; He, Shuangjiang; Yu, Li] Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China.
C3 Huazhong University of Science & Technology
RP Yu, L (corresponding author), Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China.
EM hustlyu@hust.edu.cn
FU National Natural Science Foundation of China [62271220]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62271220. The computation is completed
   in the HPC Platform of Huazhong University of Science and Technology.
CR Afza F, 2021, IMAGE VISION COMPUT, V106, DOI 10.1016/j.imavis.2020.104090
   Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471
   Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   Carreira J, 2016, PROC CVPR IEEE, P4733, DOI 10.1109/CVPR.2016.512
   Fang HS, 2023, IEEE T PATTERN ANAL, V45, P7157, DOI 10.1109/TPAMI.2022.3222784
   Gu KR, 2023, IEEE T PATTERN ANAL, V45, P10687, DOI 10.1109/TPAMI.2023.3264742
   Herath S, 2017, IMAGE VISION COMPUT, V60, P4, DOI 10.1016/j.imavis.2017.01.010
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Li J., 2021, Localization with Sampling-Argmax, V34, P27236
   Li K, 2021, PROC CVPR IEEE, P1944, DOI 10.1109/CVPR46437.2021.00198
   Li YJ, 2022, LECT NOTES COMPUT SC, V13666, P89, DOI 10.1007/978-3-031-20068-7_6
   Li Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11720, DOI 10.1109/ICCV48922.2021.01153
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Luo YM, 2022, IMAGE VISION COMPUT, V119, DOI 10.1016/j.imavis.2022.104390
   Luvizon DC, 2019, COMPUT GRAPH-UK, V85, P15, DOI 10.1016/j.cag.2019.09.002
   M. Contributors, 2020, OpenMMLab Pose Estimation Toolbox and Benchmark
   Ma LQ, 2017, ADV NEUR IN, V30
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Ming ZQ, 2022, IMAGE VISION COMPUT, V119, DOI 10.1016/j.imavis.2022.104394
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Nibali A, 2018, Arxiv, DOI [arXiv:1801.07372, 10.48550/arXiv.1801.07372]
   Nie XC, 2019, IEEE I CONF COMP VIS, P6950, DOI 10.1109/ICCV.2019.00705
   Quispe R, 2019, IMAGE VISION COMPUT, V92, DOI 10.1016/j.imavis.2019.07.009
   Ren S., 2016, Faster r-cnn: Towards real-time object detection with region proposal networks
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Sun X, 2018, LECT NOTES COMPUT SC, V11210, P536, DOI 10.1007/978-3-030-01231-1_33
   Tian Z, 2019, Arxiv, DOI arXiv:1911.07451
   Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214
   Wang JD, 2021, IEEE T PATTERN ANAL, V43, P3349, DOI 10.1109/TPAMI.2020.2983686
   Wang YH, 2022, PROC CVPR IEEE, P13116, DOI 10.1109/CVPR52688.2022.01278
   Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Ye SH, 2023, PROC CVPR IEEE, P2163, DOI 10.1109/CVPR52729.2023.00215
   Yu CQ, 2021, PROC CVPR IEEE, P10435, DOI 10.1109/CVPR46437.2021.01030
   Zhang F, 2019, PROC CVPR IEEE, P3512, DOI 10.1109/CVPR.2019.00363
   Zhang L, 2023, Arxiv, DOI [arXiv:2302.05543, 10.48550/ARXIV.2302.05543]
   Zhou XY, 2019, Arxiv, DOI arXiv:1904.07850
NR 42
TC 0
Z9 0
U1 3
U2 3
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUN
PY 2024
VL 146
AR 105041
DI 10.1016/j.imavis.2024.105041
EA MAY 2024
PG 11
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA SY3O0
UT WOS:001237973500001
DA 2024-08-05
ER

PT J
AU Zhang, JH
   Li, SB
   Zhang, XX
   Huang, ZC
   Miao, H
AF Zhang, Jinhu
   Li, Shaobo
   Zhang, Xingxing
   Huang, Zichen
   Miao, Hui
TI Transductive semantic decoupling double variational inference for
   few-shot classification
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Few -shot; Variational inference; meta -learning; Latent embedding
AB In recent years, within the rapidly evolving landscape of deep learning technology, few -shot learning, particularly in few -shot classification, has emerged as an enticing frontier. Despite the notable achievements of deep learning in handling extensive datasets, the task of image classification remains highly demanding when faced with a limited number of annotated samples. To address this challenge, we introduce the Transductive Semantic Decoupling Double Variational Inference (TSDVI), a novel framework that employs two iteratively interacting variational networks to disentangle image information and model distributions. This approach greatly improves the model ' s ability to discern inter -class differences, hence enabling more effective separation of features across distinct categories. Our TSDVI approach has been extensively validated through experiments, which have shown significant performance gains. These experiments were conducted on many widely -used datasets such as miniImagenet, tiered-Imagenet, CIFAR-FS, and FC100. Particularly noteworthy is the outstanding performance gain of up to 30% on the 1 -shot task within the FC100 dataset. These practical results strongly emphasize the effectiveness of the TSDVI model and its promise in few -shot classification. Code is available at: https://github. com/zjh1015/tsdvi.
C1 [Zhang, Jinhu; Li, Shaobo; Zhang, Xingxing; Huang, Zichen] Guizhou Univ, State Key Lab Publ Big Data, Guiyang 550025, Peoples R China.
   [Miao, Hui] Baishan Cloud Technol Co, Guiyang 550025, Peoples R China.
C3 Guizhou University
RP Li, SB (corresponding author), Guizhou Univ, State Key Lab Publ Big Data, Guiyang 550025, Peoples R China.
EM lishaobo@gzu.edu.cn; mel@baishan.com
FU National Natural Science Foundation program of China [52275480];
   National important project of China [2020YFB1713300]; Guizhou Provincial
   Department of Science and Technology Project [[2023] 002]
FX The authors would like to thank the anonymous reviewers for their
   insightful comments and suggestions to significantly improve the quality
   of this paper.This work was supported by National Natural Science
   Foundation program of China [No.52275480] ; the National important
   project of China [No.2020YFB1713300] ; and the Guizhou Provincial
   Department of Science and Technology Project [No.QKHZYD [2023] 002] .
CR Afrasiyabi Arman, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P18, DOI 10.1007/978-3-030-58558-7_2
   Agarwal M., 2021, P 25 C NEUR INF PROC, V34, P20447
   Andrychowicz M, 2016, ADV NEUR IN, V29
   Arnold SMR, 2020, Arxiv, DOI arXiv:2008.12284
   Baik S, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9445, DOI 10.1109/ICCV48922.2021.00933
   Bateni P, 2022, IEEE WINT CONF APPL, P1597, DOI 10.1109/WACV51458.2022.00166
   Bertinetto L., 2019, INT C LEARN REPR ICL, DOI [10.48550/arXiv.1805.08136, DOI 10.48550/ARXIV.1805.08136]
   Boudiaf Malik, 2020, ADV NEURAL INFORM PR, P2445, DOI DOI 10.5555/3495724.3495930
   Cao JZ, 2023, IMAGE VISION COMPUT, V137, DOI 10.1016/j.imavis.2023.104757
   Chen WY, 2020, Arxiv, DOI arXiv:1904.04232
   Cui ZY, 2022, IMAGE VISION COMPUT, V128, DOI 10.1016/j.imavis.2022.104574
   Dong CAQ, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P716
   Ferrante M, 2023, IMAGE VISION COMPUT, V137, DOI 10.1016/j.imavis.2023.104746
   Finn C, 2018, ADV NEUR IN, V31
   Finn C, 2017, PR MACH LEARN RES, V70
   Guo CA, 2017, PR MACH LEARN RES, V70
   Han-Jia Ye, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8805, DOI 10.1109/CVPR42600.2020.00883
   Hiller M., 2022, P NEURIPS, V35, P3582, DOI 10.48550/arXiv.2206.07267
   Hu Y., 2023, PMLR, P5899, DOI DOI 10.48550/ARXIV.2209.08527
   Hu YQ, 2022, ALGORITHMS, V15, DOI 10.3390/a15050147
   Hu YQ, 2021, LECT NOTES COMPUT SC, V12892, P487, DOI 10.1007/978-3-030-86340-1_39
   Joy T, 2021, Arxiv, DOI [arXiv:2006.10102, 10.48550/arXiv.2006.10102]
   Kim J, 2020, COMPUTER VISION ECCV, P599, DOI DOI 10.1007/978-3-030-58452-8_35
   Kingma D. P., 2014, arXiv
   Kingma D. P., 2014, P 2 INT C LEARNING R, P1
   Lake B. M., 2013, Advances in neural information processing systems, P2526, DOI DOI 10.5555/2999792.2999894
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee K, 2019, PROC CVPR IEEE, P10649, DOI 10.1109/CVPR.2019.01091
   Li HY, 2019, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2019.00009
   Li JJ, 2021, AAAI CONF ARTIF INTE, V35, P8401
   Lichtenstein Moshe, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P522, DOI 10.1007/978-3-030-58571-6_31
   Luo QX, 2021, IEEE WINT CONF APPL, P3962, DOI 10.1109/WACV48630.2021.00401
   Luo XH, 2022, IMAGE VISION COMPUT, V124, DOI 10.1016/j.imavis.2022.104491
   Ma JW, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10553, DOI 10.1109/ICCV48922.2021.01040
   Mangla P, 2020, IEEE WINT CONF APPL, P2207, DOI [10.1109/wacv45572.2020.9093338, 10.1109/WACV45572.2020.9093338]
   McInnes L, 2018, J OPEN SOURCE SOFTWA, V3, P861, DOI [DOI 10.21105/JOSS.00861, 10.21105/joss.00861, DOI 10.21105/joss.00861]
   Mishra N, 2018, Arxiv, DOI arXiv:1707.03141
   Nguyen C, 2020, IEEE WINT CONF APPL, P3079, DOI 10.1109/WACV45572.2020.9093536
   Oh J, 2021, Arxiv, DOI arXiv:2008.08882
   Oreshkin BN, 2018, ADV NEUR IN, V31
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   Ravi S., 2017, INT C LEARN REPR
   Ren MY, 2018, Arxiv, DOI arXiv:1803.00676
   Requeima J, 2019, ADV NEUR IN, V32
   Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278
   Rusu AA, ARXIV
   Santoro A, 2016, PR MACH LEARN RES, V48
   Shalam D., 2022, arXiv, DOI 10.48550/arXiv.2204.03065
   Shen X., 2021, P ADV NEUR INF PROC, V34, P25932
   Simon Christian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P556, DOI 10.1007/978-3-030-58598-3_33
   Singh A, 2022, Arxiv, DOI [arXiv:2208.10559, 10.48550/arXiv.2208.10559]
   Snell J, 2017, ADV NEUR IN, V30
   Sohn K, 2015, ADV NEUR IN, V28
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Vaswani A, 2017, ADV NEUR IN, V30
   Vinyals O., 2016, Advances in neural information processing systems, V29
   Von Oswald J., 2021, P INT C ADV NEUR INF, P5250, DOI DOI 10.48550/ARXIV.2110.14402
   Wang HC, 2021, IEEE WINT CONF APPL, P525, DOI 10.1109/WACV48630.2021.00057
   Wertheimer D, 2021, PROC CVPR IEEE, P8008, DOI 10.1109/CVPR46437.2021.00792
   Wu JM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8413, DOI 10.1109/ICCV48922.2021.00832
   Xu H., 2021, P 2021 SIAM INT C DA, P540, DOI 10.1137/1.9781611976700.61
   Xu J, 2023, IEEE T CIRC SYST VID, V33, P269, DOI 10.1109/TCSVT.2022.3199496
   Xu JY, 2022, PROC CVPR IEEE, P8993, DOI 10.1109/CVPR52688.2022.00880
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Zhang C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9415, DOI 10.1109/ICCV48922.2021.00930
   Zhang J, 2019, IEEE I CONF COMP VIS, P1685, DOI 10.1109/ICCV.2019.00177
   Zhang XT, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P631, DOI 10.1109/ICCV48922.2021.00069
   Zhang Y, 2024, IEEE T NEUR NET LEAR, V35, P9455, DOI 10.1109/TNNLS.2022.3233553
   Zhao D., 2020, 4 WORKSH MET NEURIPS, DOI [10.3929/ethz-b-000465883, DOI 10.3929/ETHZ-B-000465883]
   Zhou ZQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8382, DOI 10.1109/ICCV48922.2021.00829
   Ziko I., 2020, INT C MACH LEARN, P11660
NR 71
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUN
PY 2024
VL 146
AR 105034
DI 10.1016/j.imavis.2024.105034
EA APR 2024
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA SQ3H8
UT WOS:001235873500001
DA 2024-08-05
ER

PT J
AU Davila, A
   Colan, J
   Hasegawa, Y
AF Davila, Ana
   Colan, Jacinto
   Hasegawa, Yasuhisa
TI Comparison of fine-tuning strategies for transfer learning in medical
   image classification
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Medical image analysis; Fine-tuning; Transfer learning; Convolutional
   neural network; Image classification
AB In the context of medical imaging and machine learning, one of the most pressing challenges is the effective adaptation of pre-trained models to specialized medical contexts. Despite the availability of advanced pre-trained models, their direct application to the highly specialized and diverse field of medical imaging often falls short due to the unique characteristics of medical data. This study provides a comprehensive analysis on the performance of various fine-tuning methods applied to pre-trained models across a spectrum of medical imaging domains, including X-ray, MRI, Histology, Dermoscopy, and Endoscopic surgery. We evaluated eight fine-tuning strategies, including standard techniques such as fine-tuning all layers or fine-tuning only the classifier layers, alongside methods such as gradually unfreezing layers, regularization based fine-tuning and adaptive learning rates. We selected three well-established CNN architectures (ResNet-50, DenseNet-121, and VGG-19) to cover a range of learning and feature extraction scenarios. Although our results indicate that the efficacy of these finetuning methods significantly varies depending on both the architecture and the medical imaging type, strategies such as combining Linear Probing with Full Fine-tuning resulted in notable improvements in over 50% of the evaluated cases, demonstrating general effectiveness across medical domains. Moreover, Auto-RGN, which dynamically adjusts learning rates, led to performance enhancements of up to 11% for specific modalities. Additionally, the DenseNet architecture showed more pronounced benefits from alternative fine-tuning approaches compared to traditional full fine-tuning. This work not only provides valuable insights for optimizing pre-trained models in medical image analysis but also suggests the potential for future research into more advanced architectures and fine-tuning methods.
C1 [Davila, Ana; Hasegawa, Yasuhisa] Nagoya Univ, Inst Innovat Future Soc, Furo Cho,Chikusa Ku, Nagoya, Aichi 4648601, Japan.
   [Colan, Jacinto] Nagoya Univ, Dept Micronano Mech Sci & Engn, Furo Cho,Chikusa Ku, Nagoya, Aichi 4648603, Japan.
C3 Nagoya University; Nagoya University
RP Davila, A (corresponding author), Nagoya Univ, Inst Innovat Future Soc, Furo Cho,Chikusa Ku, Nagoya, Aichi 4648601, Japan.
EM davila.ana@robo.mein.nagoya-u.ac.jp
RI Colan Zaita, Jacinto Enrique/AAO-8150-2020
OI Colan Zaita, Jacinto Enrique/0000-0002-8833-2215; davila,
   ana/0000-0002-2076-6842
FU Japan Science and Tech-nology Agency (JST) CREST including AIP Challenge
   Program [JPMJCR20D5]; Japan Society for the Promotion of Science (JSPS)
   [22K14221]
FX This work was supported in part by the Japan Science and Tech-nology
   Agency (JST) CREST including AIP Challenge Program under Grant
   JPMJCR20D5, and in part by the Japan Society for the Promotion of
   Science (JSPS) Grants-in-Aid for Scientific Research (KAKENHI) under
   Grant 22K14221.
CR Ahuja S, 2021, APPL INTELL, V51, P571, DOI 10.1007/s10489-020-01826-w
   Apostolopoulos ID, 2020, PHYS ENG SCI MED, V43, P635, DOI 10.1007/s13246-020-00865-4
   Aresta G, 2019, MED IMAGE ANAL, V56, P122, DOI 10.1016/j.media.2019.05.010
   Arjovsky M, 2020, Arxiv, DOI [arXiv:1907.02893, DOI 10.48550/ARXIV.1907.02893]
   Basaia S, 2019, NEUROIMAGE-CLIN, V21, DOI 10.1016/j.nicl.2018.101645
   Bayramoglu N, 2016, LECT NOTES COMPUT SC, V9915, P532, DOI 10.1007/978-3-319-49409-8_46
   Bhojanapalli S, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10211, DOI 10.1109/ICCV48922.2021.01007
   Bustos A, 2020, MED IMAGE ANAL, V66, DOI 10.1016/j.media.2020.101797
   Cai ZP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8261, DOI 10.1109/ICCV48922.2021.00817
   Cassidy B, 2022, MED IMAGE ANAL, V75, DOI 10.1016/j.media.2021.102305
   Cheng J, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0140381
   Cheng Jun, 2024, T1-weighted CE-MRI dataset
   Chouhan V, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10020559
   Colan J, 2023, IEEE ACCESS, V11, P6092, DOI 10.1109/ACCESS.2023.3236821
   Cubuk ED, 2019, PROC CVPR IEEE, P113, DOI 10.1109/CVPR.2019.00020
   Cui Y, 2018, PROC CVPR IEEE, P4109, DOI 10.1109/CVPR.2018.00432
   Davila A., 2023, 2023 INT S MICRONANO
   Davila A, 2022, BIOINFORM ADV, V2, DOI 10.1093/bioadv/vbac015
   Eitel F, 2019, NEUROIMAGE-CLIN, V24, DOI 10.1016/j.nicl.2019.102003
   Ferreira CA, 2018, LECT NOTES COMPUT SC, V10882, P763, DOI 10.1007/978-3-319-93000-8_86
   Fozilov K, 2023, SENSORS-BASEL, V23, DOI 10.3390/s23249865
   Guo YH, 2019, PROC CVPR IEEE, P4800, DOI 10.1109/CVPR.2019.00494
   Hasan Md Kamrul, 2022, Informatics in Medicine Unlocked, DOI 10.1016/j.imu.2021.100819
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Howard J, 2018, Arxiv, DOI [arXiv:1801.06146, DOI 10.48550/ARXIV.1801.06146]
   Hu MZ, 2023, J APPL CLIN MED PHYS, V24, DOI 10.1002/acm2.13898
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Hussain M, 2019, ADV INTELL SYST, V840, P191, DOI 10.1007/978-3-319-97982-3_16
   Irvin J, 2019, AAAI CONF ARTIF INTE, P590
   Jaafari J, 2021, J BIG DATA-GER, V8, DOI 10.1186/s40537-021-00509-8
   Jonsson BA, 2019, NAT COMMUN, V10, DOI 10.1038/s41467-019-13163-9
   Kermany DS, 2018, CELL, V172, P1122, DOI 10.1016/j.cell.2018.02.010
   Kim HE, 2022, BMC MED IMAGING, V22, DOI 10.1186/s12880-022-00793-7
   Kim YJ, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-83199-9
   Kirichenko Polina, 2022, arXiv
   Kora P, 2022, BIOCYBERN BIOMED ENG, V42, P79, DOI 10.1016/j.bbe.2021.11.004
   Koskinen J, 2022, COMPUT BIOL MED, V141, DOI 10.1016/j.compbiomed.2021.105121
   Kumar A., 2022, arXiv
   Lavanchy JL, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-84295-6
   Lee YH, 2023, Arxiv, DOI arXiv:2210.11466
   Li XH, 2018, PR MACH LEARN RES, V80
   Liang GB, 2020, COMPUT METH PROG BIO, V187, DOI 10.1016/j.cmpb.2019.06.023
   Litjens G, 2017, MED IMAGE ANAL, V42, P60, DOI 10.1016/j.media.2017.07.005
   Liu XQ, 2020, NEUROCOMPUTING, V392, P253, DOI 10.1016/j.neucom.2018.10.100
   Mahbod A, 2020, COMPUT METH PROG BIO, V193, DOI 10.1016/j.cmpb.2020.105475
   Mahbod A, 2019, COMPUT MED IMAG GRAP, V71, P19, DOI 10.1016/j.compmedimag.2018.10.007
   Manokaran J, 2021, J MED IMAGING, V8, DOI 10.1117/1.JMI.8.S1.017503
   Mukherjee S, 2020, Arxiv, DOI arXiv:1910.01769
   Mukhlif AA, 2023, SENSORS-BASEL, V23, DOI 10.3390/s23020570
   Nagae S, 2022, NEUROCOMPUTING, V469, P151, DOI 10.1016/j.neucom.2021.10.051
   Naser MA, 2020, COMPUT BIOL MED, V121, DOI 10.1016/j.compbiomed.2020.103758
   Nawaz W, 2018, LECT NOTES COMPUT SC, V10882, P869, DOI 10.1007/978-3-319-93000-8_99
   Nogueira K, 2017, PATTERN RECOGN, V61, P539, DOI 10.1016/j.patcog.2016.07.001
   Nwoye CI, 2023, MED IMAGE ANAL, V86, DOI 10.1016/j.media.2023.102803
   Oh K, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-54548-6
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Patrini I, 2020, MED BIOL ENG COMPUT, V58, P1225, DOI 10.1007/s11517-020-02127-7
   Peng L, 2024, Arxiv, DOI arXiv:2106.05152
   Peters J, 2016, J ROY STAT SOC B, V78, P947, DOI 10.1111/rssb.12167
   Quiñonero-Candela J, 2009, NEURAL INF PROCESS S, pXI
   Radford A, 2021, PR MACH LEARN RES, V139
   Rajpurkar P, 2018, Arxiv, DOI [arXiv:1712.06957, DOI 10.48550/ARXIV.1712.06957, 10.48550/arxiv.1712.06957]
   Rajpurkar P, 2017, Arxiv, DOI arXiv:1711.05225
   Recht B, 2019, PR MACH LEARN RES, V97
   Ro Y, 2021, AAAI CONF ARTIF INTE, V35, P2486
   Romero M, 2020, MED PHYS, V47, P6246, DOI 10.1002/mp.14507
   Rosenfeld E, 2022, Arxiv, DOI [arXiv:2202.06856, 10.48550/arXiv.2202.06856]
   Rotemberg V, 2021, SCI DATA, V8, DOI 10.1038/s41597-021-00815-z
   Maghdid HS, 2020, Arxiv, DOI arXiv:2004.00038
   Sanford TH, 2020, AM J ROENTGENOL, V215, P1403, DOI 10.2214/AJR.19.22347
   Shen DG, 2017, ANNU REV BIOMED ENG, V19, P221, DOI [10.1146/annurev-bioeng-071516-044442, 10.1146/annurev-bioeng-071516044442]
   Shen ZQ, 2021, AAAI CONF ARTIF INTE, V35, P9594
   Shi ZH, 2019, MULTIMED TOOLS APPL, V78, P1017, DOI 10.1007/s11042-018-6082-6
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Spolaôr N, 2024, MULTIMED TOOLS APPL, V83, P27305, DOI 10.1007/s11042-023-16529-w
   Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P1299, DOI 10.1109/TMI.2016.2535302
   Taori Rohan, 2020, Advances in Neural Information Processing Systems
   Vesal S, 2018, LECT NOTES COMPUT SC, V10882, P812, DOI 10.1007/978-3-319-93000-8_92
   Vrbancic G, 2020, IEEE ACCESS, V8, P196197, DOI 10.1109/ACCESS.2020.3034343
   Wang JD, 2018, PROCEEDINGS OF THE 3RD INTERNATIONAL CONFERENCE ON CROWD SCIENCE AND ENGINEERING (ICCSE 2018), DOI 10.1145/3265689.3265705
   Wang LT, 2023, MED IMAGE ANAL, V85, DOI 10.1016/j.media.2023.102746
   Xu ZC, 2022, CHEMBIOCHEM, V23, DOI 10.1002/cbic.202200303
   Yadav SS, 2019, J BIG DATA-GER, V6, DOI [10.12921/jas.v6i1.14911, 10.1186/s40537-019-0276-2]
   Yamada Y, 2023, Control Robotics Eng, P260, DOI 10.1109/ICCRE57112.2023.10155581
   Yuan ZN, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3020, DOI 10.1109/ICCV48922.2021.00303
   Zhang DD, 2020, IEEE ROBOT AUTOM LET, V5, P4148, DOI 10.1109/LRA.2020.2989075
   Zhuang FZ, 2021, P IEEE, V109, P43, DOI 10.1109/JPROC.2020.3004555
NR 87
TC 1
Z9 1
U1 5
U2 5
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUN
PY 2024
VL 146
AR 105012
DI 10.1016/j.imavis.2024.105012
EA APR 2024
PG 17
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA RC9X5
UT WOS:001225600500001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Ge, YL
   Ren, JC
   Zhang, Q
   He, M
   Bi, HB
   Zhang, C
AF Ge, Yanliang
   Ren, Junchao
   Zhang, Qiao
   He, Min
   Bi, Hongbo
   Zhang, Cong
TI Camouflaged object detection via cross-level refinement and interaction
   network
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Camouflaged object detection; Semantic amplification; Cross -level
   refinement; Semantic -texture interaction
ID NET
AB The purpose of camouflaged object detection (COD) focuses on detecting objects that seamlessly blend into their surroundings. Camouflaged objects pose a substantial challenge in the realm of computer vision due to various factors, including occlusion, limited illumination, and diminutive dimensions. In this paper, we propose a crosslevel refinement and interaction network (CRI-Net) to capture camouflaged objects. Specifically, we advance the concept of a semantic amplification module (SAM), which simulates human visual processes through multi -scale parallel convolution in a way of progressive aggregation with the aim of obtaining rich semantic information. Subsequently, we propose a cross -level refinement unit (CRU), which focuses on multivariate information at different levels in an attention -induced manner to facilitate the fusion refinement of features between levels and the exploration of feature similarity. Finally, we design a semantic -texture interaction module (SIM) to facilitate the interaction between high-level semantics and low-level textures while mining rich fine-grained spatial information to improve the integrity of camouflaged objects. By conducting comprehensive experiments on four benchmark camouflaged datasets, our CRI-Net demonstrates significantly superior performance compared to 20 cutting -edge competing methods.
C1 [Ge, Yanliang; Ren, Junchao; Zhang, Qiao; Bi, Hongbo; Zhang, Cong] Northeast Petr Univ, Dept Elect Informat Engn, Daqing 163318, Heilongjiang, Peoples R China.
   [He, Min] China Mobile Commun Grp Heilongjiang Co Ltd, Daqing Branch, Daqing 163318, Heilongjiang, Peoples R China.
C3 Northeast Petroleum University
RP Bi, HB; Zhang, C (corresponding author), Northeast Petr Univ, Dept Elect Informat Engn, Daqing 163318, Heilongjiang, Peoples R China.
EM bhbdq@126.com; congzhang98@126.com
FU Heilongjiang Province Natural Science Foundation [LH2022F005]; Young Top
   Talents Fund in the School of Electrical Information Engineering of
   Northeast Petroleum University [DYDQQB202204]
FX <BOLD>Acknowledgements</BOLD> This paper was supported by Heilongjiang
   Province Natural Science Foundation (No. LH2022F005) and Young Top
   Talents Fund in the School of Electrical Information Engineering of
   Northeast Petroleum University (No. DYDQQB202204) .
CR Angtian Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12642, DOI 10.1109/CVPR42600.2020.01266
   Ba L.J., 2016, arXiv, DOI DOI 10.48550/ARXIV.1607.06450
   Bao YQ, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3083561
   Bhattacharyya P, 2021, IEEE INT CONF COMP V, P3022, DOI 10.1109/ICCVW54120.2021.00337
   Chen LC, 2017, Arxiv, DOI [arXiv:1706.05587, 10.48550/arXiv.1706.05587, DOI 10.48550/ARXIV.1706.05587]
   Chen TY, 2022, KNOWL-BASED SYST, V248, DOI 10.1016/j.knosys.2022.108901
   Chen WC, 2022, MACH INTELL RES, V19, P153, DOI 10.1007/s11633-022-1321-8
   Chen Z, 2018, LECT NOTES COMPUT SC, V11212, P74, DOI 10.1007/978-3-030-01237-3_5
   Cheng MM, 2021, INT J COMPUT VISION, V129, P2622, DOI [10.1007/s11263-021-01490-8, 10.1109/ICCV.2017.487]
   Cong Runmin, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P1179, DOI 10.1145/3581783.3612083
   Deng-Ping Fan, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P263, DOI 10.1007/978-3-030-59725-2_26
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Fan DP, 2018, Arxiv, DOI arXiv:1805.10421
   Fan DP, 2022, IEEE T PATTERN ANAL, V44, P6024, DOI 10.1109/TPAMI.2021.3085766
   Fan DP, 2020, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR42600.2020.00285
   Fan DP, 2020, IEEE T MED IMAGING, V39, P2626, DOI 10.1109/TMI.2020.2996645
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   Ge YL, 2023, IEEE T CIRC SYST VID, V33, P2600, DOI 10.1109/TCSVT.2022.3225865
   Ge YL, 2023, COMPUT VIS IMAGE UND, V227, DOI 10.1016/j.cviu.2022.103611
   He R., 2023, AAAI, V37, P781
   He Y, 2020, IEEE T INSTRUM MEAS, V69, P1493, DOI 10.1109/TIM.2019.2915404
   Hu HZ, 2021, PROC CVPR IEEE, P10180, DOI 10.1109/CVPR46437.2021.01005
   Hu XW, 2018, AAAI CONF ARTIF INTE, P6943
   Huerta I, 2007, LECT NOTES COMPUT SC, V4478, P475
   Ji GP, 2022, MACH INTELL RES, V19, P531, DOI 10.1007/s11633-022-1371-y
   Ji GP, 2022, PATTERN RECOGN, V123, DOI 10.1016/j.patcog.2021.108414
   Ji GP, 2021, LECT NOTES COMPUT SC, V12901, P142, DOI 10.1007/978-3-030-87193-2_14
   Jia Q, 2022, PROC CVPR IEEE, P4703, DOI 10.1109/CVPR52688.2022.00467
   Kavitha C., 2011, Int J Eng Sci Technol (IJEST), V3, P1060
   Kingma D. P., 2014, arXiv
   Kumar A, 2008, IEEE T IND ELECTRON, V55, P348, DOI 10.1109/TIE.1930.896476
   Li P, 2022, IEEE T IMAGE PROCESS, V31, P6396, DOI 10.1109/TIP.2022.3189828
   Lin W., 2023, P IEEE CVF INT C COM, P6015
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Liu ST, 2018, LECT NOTES COMPUT SC, V11215, P404, DOI 10.1007/978-3-030-01252-6_24
   Liu Y, 2023, IEEE T CIRC SYST VID, V33, P4934, DOI 10.1109/TCSVT.2023.3245883
   Lv YQ, 2021, PROC CVPR IEEE, P11586, DOI 10.1109/CVPR46437.2021.01142
   Margolin R, 2014, PROC CVPR IEEE, P248, DOI 10.1109/CVPR.2014.39
   Mei HY, 2021, PROC CVPR IEEE, P8768, DOI 10.1109/CVPR46437.2021.00866
   Paszke A, 2019, ADV NEUR IN, V32
   Pei JL, 2022, LECT NOTES COMPUT SC, V13678, P19, DOI 10.1007/978-3-031-19797-0_2
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Rustia DJA, 2020, J ASIA-PAC ENTOMOL, V23, P17, DOI 10.1016/j.aspen.2019.11.006
   Shi CJ, 2023, APPL INTELL, V53, P22429, DOI 10.1007/s10489-023-04645-x
   Shi Y, 2016, IEEE T INTELL TRANSP, V17, P3434, DOI 10.1109/TITS.2016.2552248
   Skurowski P., 2018, Unpubl Manuscr, V2, P7
   Song Z, 2023, IEEE T IMAGE PROCESS, V32, P2267, DOI 10.1109/TIP.2023.3266659
   Stevens M, 2009, PHILOS T R SOC B, V364, P423, DOI 10.1098/rstb.2008.0217
   Sun Y, 2021, arXiv, DOI DOI 10.24963/IJCAI.2021/142
   Sun YJ, 2022, Arxiv, DOI arXiv:2207.00794
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Le TN, 2019, COMPUT VIS IMAGE UND, V184, P45, DOI 10.1016/j.cviu.2019.04.006
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang K, 2022, IEEE T IND ELECTRON, V69, P5364, DOI 10.1109/TIE.2021.3078379
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wei J, 2020, AAAI CONF ARTIF INTE, V34, P12321
   Wu YH, 2021, IEEE T IMAGE PROCESS, V30, P3113, DOI 10.1109/TIP.2021.3058783
   Wu Z, 2019, IEEE I CONF COMP VIS, P7263, DOI 10.1109/ICCV.2019.00736
   Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403
   Xue JR, 2018, INT J AUTOM COMPUT, V15, P249, DOI 10.1007/s11633-018-1126-y
   Yang F, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4126, DOI 10.1109/ICCV48922.2021.00411
   Yin JQ, 2011, PROCEDIA ENGINEER, V15, DOI 10.1016/j.proeng.2011.08.412
   Youwei Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9410, DOI 10.1109/CVPR42600.2020.00943
   Zhai Q, 2021, PROC CVPR IEEE, P12992, DOI 10.1109/CVPR46437.2021.01280
   Zhang L, 2023, IEEE T CYBERNETICS, V53, P1765, DOI 10.1109/TCYB.2021.3126831
   Zhang PP, 2017, IEEE I CONF COMP VIS, P202, DOI 10.1109/ICCV.2017.31
   Zhang Q, 2023, COMPUT VIS IMAGE UND, V233, DOI 10.1016/j.cviu.2023.103719
   Zhang Q, 2023, VISUAL COMPUT, V39, P4593, DOI 10.1007/s00371-022-02611-1
   Zhang XQ, 2021, VISUAL COMPUT, V37, P1089, DOI 10.1007/s00371-020-01854-0
   Zhang Yuhang, 2023, IEEE Trans. Image Process.
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhou T, 2022, IEEE T IMAGE PROCESS, V31, P7036, DOI 10.1109/TIP.2022.3217695
   Zhou ZW, 2020, IEEE T MED IMAGING, V39, P1856, DOI 10.1109/TMI.2019.2959609
   Zhu HW, 2022, AAAI CONF ARTIF INTE, P3608
   Zhuge MC, 2022, PATTERN RECOGN, V127, DOI 10.1016/j.patcog.2022.108644
NR 77
TC 0
Z9 0
U1 3
U2 3
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD APR
PY 2024
VL 144
AR 104973
DI 10.1016/j.imavis.2024.104973
EA MAR 2024
PG 14
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA ON4Q3
UT WOS:001207945200001
DA 2024-08-05
ER

PT J
AU Munshi, RM
   Cascone, L
   Alturki, N
   Saidani, O
   Alshardan, A
   Umer, M
AF Munshi, Raafat M.
   Cascone, Lucia
   Alturki, Nazik
   Saidani, Oumaima
   Alshardan, Amal
   Umer, Muhammad
TI A novel approach for breast cancer detection using optimized ensemble
   learning framework and XAI
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Breast cancer detection; Image processing; Healthcare; Transfer
   learning; Ensemble learning; Deep convoluted features
AB Breast cancer (BC) is a common and highly lethal ailment. It stands as the second leading contributor to cancerrelated deaths in women worldwide. The timely identification of this condition is of utmost importance in mitigating mortality rates. This research paper presents a novel framework for the precise identification of BC, utilising a combination of image and numerical data features with explainable Artificial Intelligence (XAI). The utilisation of the U-NET transfer learning model is employed for image-based prediction. Additionally, an ensemble model is constructed by integrating characteristics from a customised convolutional neural network (CNN) model with an ensemble comprising random forest (RF) and support vector machine (SVM). The experiments aim to evaluate the influence of original features compared to convoluted features. A comparative analysis is carried out to assess the efficacy of various classifiers in accurately detecting BC, utilising the Wisconsin dataset. The model under consideration exhibits promising capabilities in enhancing BC diagnosis, with a remarkable accuracy rate of 99.99%. The present study contributes to the advancement of BC diagnosis by introducing a novel strategy based on machine learning and discussing the interpretation of the variables using XAI. The primary objective of this approach is to get a notable level of precision, hence facilitating the early and reliable identification of BC. Ultimately, the implementation of this approach is expected to enhance patient outcomes.
C1 [Munshi, Raafat M.] King Abdulaziz Univ, Fac Appl Med Sci, Dept Med Lab Technol MLT, Rabigh, Saudi Arabia.
   [Cascone, Lucia] Univ Salerno, Dept Comp Sci, Fisciano, Italy.
   [Alturki, Nazik; Saidani, Oumaima; Alshardan, Amal] Princess Nourah Bint Abdulrahman Univ, Coll Comp & Informat Sci, Dept Informat Syst, POB 84428, Riyadh 11671, Saudi Arabia.
   [Umer, Muhammad] Islamia Univ Bahawalpur, Dept Comp Sci & Informat Technol, Bahawalpur 63100, Pakistan.
C3 King Abdulaziz University; University of Salerno; Princess Nourah bint
   Abdulrahman University; Islamia University of Bahawalpur
RP Umer, M (corresponding author), Islamia Univ Bahawalpur, Dept Comp Sci & Informat Technol, Bahawalpur 63100, Pakistan.
EM rmonshi@kau.edu.sa; lcascone@unisa.it; namalturki@pnu.edu.sa;
   ocsaidani@pnu.edu.sa; amalshardan@pnu.edu.sa; umersabir1996@gmail.com
RI Alshardan, Amal/KVZ-0940-2024
OI Alshardan, Amal/0000-0003-3523-9102
FU Institutional Fund Projects [IFPIP: 1328-415-1443]; Princess Nourah bint
   Abdulrahman University, Riyadh, Saudi Arabia [PNURSP2024R333]; Ministry
   of Education; King Abdulaziz University, DSR. Jeddah, Saudi Arabia
FX This research work was funded by Institutional Fund Projects under grant
   no. (IFPIP: 1328-415-1443) and by Princess Nourah bint Abdul rahman
   University Researchers Supporting Project number (PNURSP2024R333) ,
   Princess Nourah bint Abdulrahman University, Riyadh, Saudi Arabia. The
   authors gratefully acknowledge the technical and financial support
   provided by the Ministry of Education and King Abdulaziz University,
   DSR. Jeddah, Saudi Arabia.
CR Ahmad MA, 2018, IEEE INT CONF HEALT, P447, DOI [10.1109/ICHI.2018.00095, 10.1145/3233547.3233667]
   Ak MF, 2020, HEALTHCARE-BASEL, V8, DOI 10.3390/healthcare8020111
   Akbulut Sami, 2022, Med. Bull. Haseki/Haseki Tip Bulteni, V60
   Alanazi SA, 2021, J HEALTHC ENG, V2021, DOI 10.1155/2021/5528622
   Aljuaid H, 2022, COMPUT METH PROG BIO, V223, DOI 10.1016/j.cmpb.2022.106951
   American Cancer Society American Cancer Society, 2022, Breast Cancer
   Ankit Khushal Barai, Uci machine learning repository
   Ashraf I, 2022, ELECTRONICS-SWITZ, V11, DOI 10.3390/electronics11040667
   Besharati E, 2019, J AMB INTEL HUM COMP, V10, P3669, DOI 10.1007/s12652-018-1093-8
   Biau G, 2016, TEST-SPAIN, V25, P197, DOI 10.1007/s11749-016-0481-7
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Cascone L, 2023, BIG DATA RES, V31, DOI 10.1016/j.bdr.2022.100360
   Chaudhury Amrita Ray, 2011, 2011 INT C IMAGE INF, P1
   Chekkoury A, 2012, PROC SPIE, V8315, DOI 10.1117/12.911643
   Chen SH, 2021, EUR J GYNAECOL ONCOL, V42, P554, DOI 10.31083/j.ejgo.2021.03.2416
   Dhahri H, 2019, J HEALTHC ENG, V2019, DOI 10.1155/2019/4253641
   Dubey AK, 2016, INT J COMPUT ASS RAD, V11, P2033, DOI 10.1007/s11548-016-1437-9
   Ghosh P., Breast Cancer Wisconsin (Diagnostic) Prediction
   Hameed A, 2021, J AMB INTEL HUM COMP, DOI 10.1007/s12652-021-03485-2
   Hou YW, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21062153
   Idrees Muhammad, 2022, Sensors Int., V3
   Jemal A, 2011, CA-CANCER J CLIN, V61, P134, DOI [10.3322/caac.20107, 10.3322/caac.21492, 10.3322/caac.20115]
   Juna A, 2022, WATER-SUI, V14, DOI 10.3390/w14172592
   Karamti H., 2023, Cancer Biomarkers, P1
   Lundberg SM, 2017, ADV NEUR IN, V30
   Majeed R, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10232926
   Mangukiya M., 2022, International Journal for Research in Applied Science and Engineering Technology, V10, P141, DOI DOI 10.22214/IJRASET.2022.40204
   Manzoor M, 2021, IEEE ACCESS, V9, P128359, DOI 10.1109/ACCESS.2021.3112546
   Masciari S, 2007, J MED GENET, V44, P726, DOI 10.1136/jmg.2007.051268
   Murphy A., 2021, N AM FUZZ INF PROC S, P302
   Obaid O. I., 2018, Int. J. Eng. Technol., V7, P160, DOI DOI 10.14419/IJET.V7I4.36.23737
   Pandian A.P., 2019, Journal of Artificial Intelligence, V1, P37
   Pradesh A., 2011, INDIAN J COMPUT SCI, V2, P756
   RAJANI K., Breast Cancer Survival Dataset
   Robertson FM, 2010, CA-CANCER J CLIN, V60, P351, DOI 10.3322/caac.20082
   Rodriguez-Sampaio M, 2022, LECT NOTES COMPUT SC, V13258, P557, DOI 10.1007/978-3-031-06242-1_55
   Rupapara Vaibhav, 2023, Intell. Automat. Soft Comput., V36
   Rustam F, 2022, DIAGNOSTICS, V12, DOI 10.3390/diagnostics12061474
   Sachdeva Ravi Kumar, 2022, International Journal of Software Innovation, V10, DOI 10.4018/IJSI.301221
   Sarwat S, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22134834
   Silva-Aravena F, 2023, CANCERS, V15, DOI 10.3390/cancers15092443
   Singh Sonam Jawahar, 2023, Data Management, Analytics and Innovation: Proceedings of ICDMAI 2022. Lecture Notes on Data Engineering and Communications Technologies (137), P121, DOI 10.1007/978-981-19-2600-6_9
   Srinivasu PN, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21082852
   Suh YJ, 2020, J PERS MED, V10, DOI 10.3390/jpm10040211
   Sun YS, 2017, INT J BIOL SCI, V13, P1387, DOI 10.7150/ijbs.21635
   Tang L, 2018, ONCOTARGETS THER, V11, P1055, DOI 10.2147/OTT.S149428
   UCI Repository, Uci machine learning repository
   Umer M, 2022, CANCERS, V14, DOI 10.3390/cancers14236015
   Umer M, 2022, PATTERN RECOGN LETT, V164, P224, DOI 10.1016/j.patrec.2022.11.012
   Umer M, 2021, PATTERN RECOGN LETT, V150, P250, DOI 10.1016/j.patrec.2021.07.009
   Wang XM, 2022, ELECTRONICS-SWITZ, V11, DOI 10.3390/electronics11172767
   Wang ZQ, 2019, IEEE ACCESS, V7, P105146, DOI 10.1109/ACCESS.2019.2892795
   WHO. World Health Organization, 2021, Breast Cancer
   WHO. World Health Organization, 2022, Cancer: Key Facts
   WHO. World Health Organization, 2022, Breast Cancer
   Xie XL, 2021, FRONT ONCOL, V11, DOI 10.3389/fonc.2021.763527
   Yadav SS, 2022, MULTIMED TOOLS APPL, V81, P13139, DOI 10.1007/s11042-020-09600-3
   Zheng J, 2020, IEEE ACCESS, V8, P96946, DOI 10.1109/ACCESS.2020.2993536
NR 58
TC 4
Z9 4
U1 1
U2 1
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD FEB
PY 2024
VL 142
AR 104910
DI 10.1016/j.imavis.2024.104910
EA JAN 2024
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA JJ9I8
UT WOS:001172913800001
DA 2024-08-05
ER

PT J
AU Lv, C
   Han, CG
   Lang, JC
   Jiang, H
   Cheng, DQ
   Qian, JS
AF Lv, Chen
   Han, Chenggong
   Lang, Jochen
   Jiang, He
   Cheng, Deqiang
   Qian, Jiansheng
TI GDM-depth: Leveraging global dependency modelling for self-supervised
   indoor depth estimation
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Indoor depth estimation; Self -supervision; Global dependency modelling;
   Tree filter; Transformer
AB Self-supervised depth estimation algorithms eschew depth ground truth and employ the convolutional U-Net with a fixed receptive field which confines its focus primarily to nearby spatial distances. These factors obscure adequate supervision during image reconstruction, consequently hindering accurate depth estimation, particularly in complex indoor scenes. The pure transformer framework can perform global modelling to provide more semantic information. However, the cost is significant. To tackle these challenges, we introduce GDM-Depth, which utilizes global dependency modelling to offer more precise depth guidance from the network itself. Initially, we propose integrating learnable tree filters with unary terms, leveraging the structural properties of spanning trees to facilitate efficient long-range interactions. Subsequently, instead of replacing the convolutional framework entirely, we employ the transformer to design a scale-aware global feature extractor, establishing global relationships among local features at various scales, achieving both efficiency and cost-effectiveness. Furthermore, inter-class disparities between depth global and local features are observed. To address this issue, we introduce the global feature injector to further enhance the representation. GDM-Depth's effectiveness is demonstrated on the NYUv2, ScanNet, and InteriorNet depth datasets, achieving impressive test set performances of 87.2%, 83.1%, and 76.1% in key indicators delta < 0.125, respectively.
C1 [Lv, Chen; Han, Chenggong; Jiang, He; Cheng, Deqiang; Qian, Jiansheng] China Univ Min & Technol, Sch Informat & Control Engn, Xuzhou 221116, Peoples R China.
   [Lang, Jochen] Univ Ottawa, Sch Elect Engn & Comp Sci, Ottawa, ON K1N6N5, Canada.
   [Jiang, He] Minist Educ, Key Lab Syst Control & Informat Proc, Shanghai, Peoples R China.
C3 China University of Mining & Technology; University of Ottawa
RP Jiang, H; Cheng, DQ (corresponding author), China Univ Min & Technol, Sch Informat & Control Engn, Xuzhou 221116, Peoples R China.
EM chenlv@cumt.edu.cn; hanchenggong@cumt.edu.cn; jlang@uOttawa.ca;
   chenlv@cumt.edu.cn; hanchenggong@cumt.edu.cn; qianjsh@cumt.edu.cn
FU National Natural Science Foundation of China [52204177, 52304182]; Key
   Laboratory of System Control and Information Pro- cessing of the
   Ministry of Education [SCIP200240105]
FX The authors gratefully acknowledge the financial supports by the
   National Natural Science Foundation of China (Grant No: 52204177,
   52304182), Key Laboratory of System Control and Information Processing
   of the Ministry of Education (Grant No: SCIP200240105) .
CR Bian J.W., 2020, arXiv
   Chang YK, 2021, IEEE T CYBERNETICS, V51, P5836, DOI 10.1109/TCYB.2019.2959381
   Chen LC, 2017, Arxiv, DOI [arXiv:1706.05587, 10.48550/arXiv.1706.05587, DOI 10.48550/ARXIV.1706.05587]
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen YH, 2019, IEEE I CONF COMP VIS, P7062, DOI 10.1109/ICCV.2019.00716
   Cheng A., 2024, GAM-Depth: Self-Supervised Indoor Depth Estimation Leveraging a Gradient-Aware Mask and Semantic Constraints
   Cheng ZY, 2021, IEEE SENS J, V21, P26912, DOI 10.1109/JSEN.2021.3120753
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Eigen D, 2014, ADV NEUR IN, V27
   Fu H, 2018, PROC CVPR IEEE, P2002, DOI 10.1109/CVPR.2018.00214
   Garg R, 2016, LECT NOTES COMPUT SC, V9912, P740, DOI 10.1007/978-3-319-46484-8_45
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Godard C, 2019, IEEE I CONF COMP VIS, P3827, DOI 10.1109/ICCV.2019.00393
   Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699
   Graham B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12239, DOI 10.1109/ICCV48922.2021.01204
   Gross J.L., 2018, Graph Theory and its Applications, V3rd, DOI [DOI 10.1201/9780429425134, 10.1201/9780429425134]
   Guizilini V, 2022, PROC CVPR IEEE, P160, DOI 10.1109/CVPR52688.2022.00026
   He KM, 2015, IEEE T PATTERN ANAL, V37, P1904, DOI 10.1109/TPAMI.2015.2389824
   Hu JJ, 2019, IEEE WINT CONF APPL, P1043, DOI 10.1109/WACV.2019.00116
   Huang Zilong, 2021, Shuffle Transformer: Rethinking Spatial Shuffle for Vision Transformer
   Ji P., 2021, MonoIndoor: Towards Good Practice of Self-Supervised Monocular Depth Estimation for Indoor Environments,
   Ji P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12767, DOI 10.1109/ICCV48922.2021.01255
   Jung H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12622, DOI 10.1109/ICCV48922.2021.01241
   Karsch K, 2014, IEEE T PATTERN ANAL, V36, P2144, DOI 10.1109/TPAMI.2014.2316835
   Kingma D. P., 2014, arXiv
   Ladicky L, 2014, PROC CVPR IEEE, P89, DOI 10.1109/CVPR.2014.19
   Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32
   Li B, 2015, PROC CVPR IEEE, P1119, DOI 10.1109/CVPR.2015.7298715
   Li BY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12643, DOI 10.1109/ICCV48922.2021.01243
   Li RZ, 2023, IEEE T CIRC SYST VID, V33, P830, DOI 10.1109/TCSVT.2022.3207105
   Li S. Z., 1994, Computer Vision - ECCV '94. Third European Conference on Computer Vision. Proceedings. Vol.II, P361, DOI 10.1007/BFb0028368
   Li WW, 2020, INT J GEOGR INF SCI, V34, P637, DOI 10.1080/13658816.2018.1542697
   Li ZQ, 2019, PROC CVPR IEEE, P4516, DOI 10.1109/CVPR.2019.00465
   Li ZQ, 2018, PROC CVPR IEEE, P2041, DOI 10.1109/CVPR.2018.00218
   Liu FY, 2016, IEEE T PATTERN ANAL, V38, P2024, DOI 10.1109/TPAMI.2015.2505283
   Liu YB, 2023, IMAGE VISION COMPUT, V136, DOI 10.1016/j.imavis.2023.104723
   Luo WJ, 2017, Arxiv, DOI [arXiv:1701.04128, DOI 10.48550/ARXIV.1701.04128]
   Meng XY, 2022, IEEE T CIRC SYST VID, V32, P4841, DOI 10.1109/TCSVT.2021.3128505
   Polasek T, 2023, COMPUT GRAPH-UK, V111, P180, DOI 10.1016/j.cag.2023.02.003
   Ranftl R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12159, DOI 10.1109/ICCV48922.2021.01196
   Ranjan A, 2019, PROC CVPR IEEE, P12232, DOI 10.1109/CVPR.2019.01252
   Ren Z., 2023, ICASSP 2023 2023 IEE, P1, DOI [10.1109/ICASSP49357.2023.10094895, DOI 10.1109/ICASSP49357.2023.10094895]
   Ronneberger O, 2015, Arxiv, DOI arXiv:1505.04597
   Saxena A, 2009, IEEE T PATTERN ANAL, V31, P824, DOI 10.1109/TPAMI.2008.132
   Silberman N., 2012, Computer Vision-ECCV 2012-12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part V, volume 7576 of Lecture Notes in Computer Science, DOI [DOI 10.1007/978-3-642-33715-4_54, 10.1007/978-3-642-33715-454, 10.1007/978-3-642-33715-4_54]
   Song L., 2019, NIPS, V32
   Song L., 2020, Advances in Neural Information Processing Systems, P3991
   Su W, 2022, IMAGE VISION COMPUT, V124, DOI 10.1016/j.imavis.2022.104487
   Sun LB, 2023, Arxiv, DOI arXiv:2211.03660
   Teed Z, 2020, Arxiv, DOI arXiv:1812.04605
   Ummenhofer B, 2017, PROC CVPR IEEE, P5622, DOI 10.1109/CVPR.2017.596
   Vaswani A., 2017, Advances in neural information processing systems, P5998
   Wang LJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12707, DOI 10.1109/ICCV48922.2021.01249
   Wang P, 2015, PROC CVPR IEEE, P2800, DOI 10.1109/CVPR.2015.7298897
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Watson J, 2021, PROC CVPR IEEE, P1164, DOI 10.1109/CVPR46437.2021.00122
   Wei Y, 2022, IEEE T CIRC SYST VID, V32, P3839, DOI 10.1109/TCSVT.2021.3118681
   Wu CY, 2022, PROC CVPR IEEE, P3804, DOI 10.1109/CVPR52688.2022.00379
   Xu D, 2017, PROC CVPR IEEE, P161, DOI 10.1109/CVPR.2017.25
   Yang QX, 2015, IEEE T PATTERN ANAL, V37, P834, DOI 10.1109/TPAMI.2014.2353642
   Yin W, 2019, IEEE I CONF COMP VIS, P5683, DOI 10.1109/ICCV.2019.00578
   Yu FS, 2016, Arxiv, DOI arXiv:1511.07122
   Yu ZH, 2020, Arxiv, DOI arXiv:2007.07696
   Yuan K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P559, DOI 10.1109/ICCV48922.2021.00062
   Zhao C., 2023, P IEEE CVF INT C COM, P16209
   Zhao CQ, 2022, INT CONF 3D VISION, P668, DOI 10.1109/3DV57658.2022.00077
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao W, 2021, Arxiv, DOI arXiv:2004.01314
   Zhao YP, 2022, Arxiv, DOI arXiv:2207.07268
   Zhou JS, 2019, IEEE I CONF COMP VIS, P8617, DOI 10.1109/ICCV.2019.00871
   Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
NR 75
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD SEP
PY 2024
VL 149
AR 105160
DI 10.1016/j.imavis.2024.105160
EA JUL 2024
PG 15
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA ZA8J3
UT WOS:001272658100001
DA 2024-08-05
ER

PT J
AU Ghari, B
   Tourani, A
   Shahbahrami, A
   Gaydadjiev, G
AF Ghari, Bahareh
   Tourani, Ali
   Shahbahrami, Asadollah
   Gaydadjiev, Georgi
TI Pedestrian detection in low-light conditions: A comprehensive survey
SO IMAGE AND VISION COMPUTING
LA English
DT Review
DE Pedestrian detection; Object detection; Computer vision; Autonomous
   vehicles
ID FASTER R-CNN; DEEP NEURAL-NETWORKS; ATTENTION NETWORK; FEATURE FUSION;
   NIGHT; FRAMEWORK; TIME
AB Pedestrian detection remains a critical problem in various domains, such as computer vision, surveillance, and autonomous driving. In particular, accurate and instant detection of pedestrians in low-light conditions and reduced visibility is of utmost importance for autonomous vehicles to prevent accidents and save lives. This paper aims to comprehensively survey various pedestrian detection approaches, baselines, and datasets that specifically target low-light conditions. The survey discusses the challenges faced in detecting pedestrians at night and explores state-of-the-art methodologies proposed in recent years to address this issue. These methodologies encompass a diverse range, including deep learning-based, feature-based, and hybrid approaches, which have shown promising results in enhancing pedestrian detection performance under challenging lighting conditions. Furthermore, the paper highlights current research directions in the field and identifies potential solutions that merit further investigation by researchers. By thoroughly examining pedestrian detection techniques in low-light conditions, this survey seeks to contribute to the advancement of safer and more reliable autonomous driving systems and other applications related to pedestrian safety. Accordingly, most of the current approaches in the field use deep learning-based image fusion methodologies (i.e., early, halfway, and late fusion) for accurate and reliable pedestrian detection. Moreover, the majority of the works in the field (approximately 48%) have been evaluated on the KAIST dataset, while the real-world video feeds recorded by authors have been used in less than 6 % of the works.
C1 [Ghari, Bahareh; Shahbahrami, Asadollah] Guilan Univ, Dept Comp Engn, Rasht, Iran.
   [Tourani, Ali] Univ Luxembourg, Interdisciplinary Ctr Secur Reliabil & Trust SnT, Luxembourg City, Luxembourg.
   [Gaydadjiev, Georgi] Delft Univ Technol, Dept Quantum & Comp Engn, Delft, Netherlands.
C3 University of Guilan; University of Luxembourg; Delft University of
   Technology
RP Tourani, A (corresponding author), Univ Luxembourg, Interdisciplinary Ctr Secur Reliabil & Trust SnT, Luxembourg City, Luxembourg.
EM ali.tourani@uni.lu
CR Ahmed Zahid, 2019, 2019 International Conference on Communication and Signal Processing (ICCSP), P0971, DOI 10.1109/ICCSP.2019.8697978
   Farooq MA, 2022, Arxiv, DOI arXiv:2201.01661
   Altay F, 2022, IEEE SENS J, V22, P11489, DOI 10.1109/JSEN.2022.3172386
   [Anonymous], 2021, FLIR Thermal Dataset for Algorithm Training
   BAE KIM JONG, 2015, [KIPS Transactions on Software and Data Engineering, 정보처리학회논문지. 소프트웨어 및 데이터 공학], V4, P201
   Bao C, 2023, SENSORS-BASEL, V23, DOI 10.3390/s23062934
   Barba-Guaman L, 2020, ELECTRONICS-SWITZ, V9, DOI 10.3390/electronics9040589
   Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314
   Ben Khalifa A, 2020, COGN SYST RES, V60, P77, DOI 10.1016/j.cogsys.2019.12.003
   Bishop CM., 2006, PATTERN RECOGN
   Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Boukerche A, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3460770
   Cai YF, 2017, IEEE ACCESS, V5, P5013, DOI 10.1109/ACCESS.2017.2695721
   Cao YP, 2022, INFORM FUSION, V88, P1, DOI 10.1016/j.inffus.2022.06.008
   Cao YP, 2019, ISPRS J PHOTOGRAMM, V150, P70, DOI 10.1016/j.isprsjprs.2019.02.005
   Cao YP, 2019, INFORM FUSION, V46, P206, DOI 10.1016/j.inffus.2018.06.005
   Cao Y, 2023, IEEE COMPUT SOC CONF, P403, DOI 10.1109/CVPRW59228.2023.00046
   Cao ZW, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21124184
   Cao ZW, 2019, IEEE ACCESS, V7, P135023, DOI 10.1109/ACCESS.2019.2932749
   Chan Hung-Tse, 2023, 2023 IEEE 3rd International Conference on Electronic Communications, Internet of Things and Big Data (ICEIB), P313, DOI 10.1109/ICEIB57887.2023.10170473
   Chen L, 2021, IEEE T INTELL TRANSP, V22, P3234, DOI 10.1109/TITS.2020.2993926
   Chen Wei, 2023, Array
   Chen X, 2022, ELECTRONICS-SWITZ, V11, DOI 10.3390/electronics11010001
   Chen YT, 2022, LECT NOTES COMPUT SC, V13669, P139, DOI 10.1007/978-3-031-20077-9_9
   Chen YJ, 2023, APPL SCI-BASEL, V13, DOI 10.3390/app131810225
   Chen YF, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10030809
   Chen YY, 2019, I S INTELL SIG PROC, DOI 10.1109/ispacs48206.2019.8986298
   Choi H, 2016, INT C PATT RECOG, P621, DOI 10.1109/ICPR.2016.7899703
   Crawshaw Michael, 2020, Multi-task learning with deep neural networks: A survey
   Cui CH, 2023, Arxiv, DOI arXiv:2305.12845
   Dai JF, 2016, ADV NEUR IN, V29
   Dai XB, 2021, INFRARED PHYS TECHN, V115, DOI 10.1016/j.infrared.2021.103694
   Dai XB, 2019, INFRARED PHYS TECHN, V97, P25, DOI 10.1016/j.infrared.2018.11.028
   Dangle Anagha, 2023, Procedia Computer Science, P2091, DOI 10.1016/j.procs.2023.01.185
   Das A, 2023, Arxiv, DOI arXiv:2302.12589
   Dasgupta K, 2022, IEEE T INTELL TRANSP, V23, P15940, DOI 10.1109/TITS.2022.3146575
   Davis JW, 2005, WACV 2005: SEVENTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P364
   Deng Qing, 2021, 2021 IEEE 24 INT C I, P1
   Deng ZJ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P684
   Devaguptapu C, 2019, IEEE COMPUT SOC CONF, P1029, DOI 10.1109/CVPRW.2019.00135
   Ding L, 2021, KNOWL-BASED SYST, V227, DOI 10.1016/j.knosys.2021.106990
   Ding L, 2020, SIGNAL PROCESS-IMAGE, V82, DOI 10.1016/j.image.2019.115764
   Dollár P, 2014, IEEE T PATTERN ANAL, V36, P1532, DOI 10.1109/TPAMI.2014.2300479
   Dollár P, 2012, IEEE T PATTERN ANAL, V34, P743, DOI 10.1109/TPAMI.2011.155
   Fan Q., 2021, arXiv
   Fritz K, 2019, PROC SPIE, V10988, DOI 10.1117/12.2520705
   Fu L, 2021, INFRARED PHYS TECHN, V116, DOI 10.1016/j.infrared.2021.103770
   Galarza-Bravo MA, 2018, LECT NOTES ARTIF INT, V10985, P335, DOI 10.1007/978-3-319-97589-4_28
   Gawande Ujwalla, 2020, Recent Trends in Computational Intelligence, P1
   Gebhardt E, 2018, 2018 15TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), P37
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Geng SQ, 2020, Arxiv, DOI arXiv:2012.11185
   Ghari Bahareh, 2022, P 2022 8 IR C SIGN P, P1
   Ghose D, 2019, IEEE COMPUT SOC CONF, P988, DOI 10.1109/CVPRW.2019.00130
   González A, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16060820
   GROSS L, 1975, AM J MATH, V97, P1061, DOI 10.2307/2373688
   Guan DY, 2019, IEEE COMPUT SOC CONF, P434, DOI 10.1109/CVPRW.2019.00057
   Guan DY, 2019, INFORM FUSION, V50, P148, DOI 10.1016/j.inffus.2018.11.017
   Guo TT, 2019, IEEE IMAGE PROC, P1660, DOI [10.1109/ICIP.2019.8803104, 10.1109/icip.2019.8803104]
   Ha Q, 2017, IEEE INT C INT ROBOT, P5108, DOI 10.1109/IROS.2017.8206396
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Hnewa Mazin, 2023, ICASSP 2023 2023 IEE, P1
   Hou YL, 2018, INFRARED PHYS TECHN, V94, P69, DOI 10.1016/j.infrared.2018.08.029
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Hsia CH, 2023, ELECTRONICS-SWITZ, V12, DOI 10.3390/electronics12102312
   Hu Ruizhe, 2022, IEEE ACCESS, P1
   Hung Goon Li, 2020, SN Comp. Sci., V1, P1
   Hwang S, 2015, PROC CVPR IEEE, P1037, DOI 10.1109/CVPR.2015.7298706
   Iftikhar S, 2022, ELECTRONICS-SWITZ, V11, DOI 10.3390/electronics11213551
   Jeong M, 2017, IEEE T CIRC SYST VID, V27, P1368, DOI 10.1109/TCSVT.2016.2539684
   Jia XY, 2021, IEEE INT CONF COMP V, P3489, DOI 10.1109/ICCVW54120.2021.00389
   Jiang QY, 2022, IEEE ACCESS, V10, P53797, DOI 10.1109/ACCESS.2022.3175303
   Jiang Y, 2019, IEEE ACCESS, V7, P118310, DOI 10.1109/ACCESS.2019.2936454
   Jinda Hu, 2020, 2020 IEEE 5th International Conference on Image, Vision and Computing (ICIVC), P1, DOI 10.1109/ICIVC50857.2020.9177438
   Joseph RK, 2016, CRIT POL ECON S ASIA, P1
   Kailai Zhou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P787, DOI 10.1007/978-3-030-58523-5_46
   Kalita Rumi, 2020, 2020 IEEE 17 IND COU, P1
   Karasawa T, 2017, PROCEEDINGS OF THE THEMATIC WORKSHOPS OF ACM MULTIMEDIA 2017 (THEMATIC WORKSHOPS'17), P35, DOI 10.1145/3126686.3126727
   Karol Piniarski, 2015, CMST, V21, P141
   Khalid B, 2019, 2019 2ND INTERNATIONAL CONFERENCE ON COMMUNICATION, COMPUTING AND DIGITAL SYSTEMS (C-CODE), P143, DOI [10.1109/C-CODE.2019.8680991, 10.1109/c-code.2019.8680991]
   Kieu M, 2019, LECT NOTES COMPUT SC, V11752, P203, DOI 10.1007/978-3-030-30645-8_19
   Kieu M, 2021, INT C PATT RECOG, P8804, DOI 10.1109/ICPR48806.2021.9412764
   Kim J, 2018, IEEE INT VEH SYM, P1620, DOI 10.1109/IVS.2018.8500711
   Kim JH, 2018, EXPERT SYST APPL, V114, P15, DOI 10.1016/j.eswa.2018.07.020
   Kim J, 2019, 2019 1ST INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE IN INFORMATION AND COMMUNICATION (ICAIIC 2019), P463, DOI [10.1109/ICAIIC.2019.8669070, 10.1109/icaiic.2019.8669070]
   Kim JU, 2022, AAAI CONF ARTIF INTE, P1157
   Kim JU, 2022, IEEE T CIRC SYST VID, V32, P1510, DOI 10.1109/TCSVT.2021.3076466
   Kim Jung Uk, 2021, P IEEECVF INT C COMP, P3050, DOI DOI 10.1109/ICCV48922.2021.00304
   Kim M, 2019, IEEE IMAGE PROC, P1650, DOI [10.1109/icip.2019.8803098, 10.1109/ICIP.2019.8803098]
   Kim S, 2019, IEEE ACCESS, V7, P12415, DOI 10.1109/ACCESS.2019.2892425
   Kim T, 2024, Arxiv, DOI arXiv:2403.15209
   Kim T, 2024, Arxiv, DOI arXiv:2403.01300
   Kim T, 2018, PATTERN RECOGN, V79, P44, DOI 10.1016/j.patcog.2018.01.029
   Koenig D, 2017, IEEE COMPUT SOC CONF, P243, DOI 10.1109/CVPRW.2017.36
   Kristo M, 2020, IEEE ACCESS, V8, P125459, DOI 10.1109/ACCESS.2020.3007481
   Lee S, 2024, SENSORS-BASEL, V24, DOI 10.3390/s24041168
   Lei Pang, 2019, 2019 IEEE International Conference on Robotics and Biomimetics (ROBIO), P2902, DOI 10.1109/ROBIO49542.2019.8961523
   Li CY, 2018, Arxiv, DOI arXiv:1808.04818
   Li CY, 2019, PATTERN RECOGN, V85, P161, DOI 10.1016/j.patcog.2018.08.005
   Li F, 2022, IEEE ACCESS, V10, P19937, DOI 10.1109/ACCESS.2022.3150988
   Li G, 2021, INT C PATT RECOG, P9180, DOI 10.1109/ICPR48806.2021.9412889
   Li GF, 2022, OPT LASER TECHNOL, V156, DOI 10.1016/j.optlastec.2022.108466
   Li GF, 2020, IEEE T IND ELECTRON, V67, P8889, DOI 10.1109/TIE.2019.2945295
   Li H, 2019, IEEE T IMAGE PROCESS, V28, P2614, DOI 10.1109/TIP.2018.2887342
   Li P, 2023, Arxiv, DOI arXiv:2306.10364
   Li SS, 2021, IEEE ACCESS, V9, P141861, DOI 10.1109/ACCESS.2021.3120870
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu JJ, 2016, Arxiv, DOI arXiv:1611.02644
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Liu TS, 2022, IEEE T CIRC SYST VID, V32, P315, DOI 10.1109/TCSVT.2021.3060162
   Liu W, 2019, PROC CVPR IEEE, P5182, DOI 10.1109/CVPR.2019.00533
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Luo FY, 2022, IEEE T INTELL TRANSP, V23, P15808, DOI 10.1109/TITS.2022.3145476
   Lyu CJ, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22124416
   Lyu CJ, 2021, 2021 5TH INTERNATIONAL CONFERENCE ON INNOVATION IN ARTIFICIAL INTELLIGENCE (ICIAI 2021), P158, DOI 10.1145/3461353.3461369
   Ma Y, 2016, NEUROCOMPUTING, V202, P12, DOI 10.1016/j.neucom.2016.03.009
   Mao X., 2017, arXiv, DOI DOI 10.48550/ARXIV.1611.04076
   Marnissi MA, 2023, IEEE COMPUT SOC CONF, P817, DOI 10.1109/CVPRW59228.2023.00089
   Marnissi MA, 2021, INT C PATT RECOG, P6509, DOI 10.1109/ICPR48806.2021.9412331
   Marnissi MA, 2022, VISIGRAPP, P275, DOI 10.5220/0010913000003124
   Marnissi MA, 2022, PATTERN RECOGN LETT, V153, P222, DOI 10.1016/j.patrec.2021.11.024
   Montenegro Bryan, 2022, Ingenius. Rev. Ciencia Tecnol., V27, P85
   Munir F, 2021, IEEE INT C INT ROBOT, P206, DOI 10.1109/IROS51168.2021.9636353
   Kieu M, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3418213
   My Kieu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P546, DOI 10.1007/978-3-030-58542-6_33
   Narayanan A, 2021, 2021 SIXTH INTERNATIONAL CONFERENCE ON WIRELESS COMMUNICATIONS, SIGNAL PROCESSING AND NETWORKING (WISPNET), P431, DOI [10.1109/WISPNET51692.2021.9419443, 10.1109/WiSPNET51692.2021.9419443]
   Nataprawira J, 2021, IEEE ICCE, DOI 10.1109/ICCE50685.2021.9427627
   Nataprawira J, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21072536
   Neumann L, 2019, LECT NOTES COMPUT SC, V11361, P691, DOI 10.1007/978-3-030-20887-5_43
   Nikolov Ivan Adriyanov, 2024, 19 INT JOINT C COMP, P829
   Nowosielski A, 2020, IEEE SENS J, V20, P9293, DOI 10.1109/JSEN.2020.2986855
   Oksuz K, 2021, IEEE T PATTERN ANAL, V43, P3388, DOI 10.1109/TPAMI.2020.2981890
   Olmeda D, 2013, INTEGR COMPUT-AID E, V20, P347, DOI 10.3233/ICA-130441
   Oltean G, 2019, INT SYM DES TECH ELE, P264, DOI 10.1109/siitme47687.2019.8990686
   Oluyide OM, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22051728
   Pang YX, 2022, IEEE T MULTIMEDIA, V24, P3859, DOI 10.1109/TMM.2021.3109419
   Park K, 2018, PATTERN RECOGN, V80, P143, DOI 10.1016/j.patcog.2018.03.007
   Patel H, 2022, IEEE COMPUT SOC CONF, P378, DOI 10.1109/CVPRW56347.2022.00053
   Pei DS, 2020, INFRARED PHYS TECHN, V105, DOI 10.1016/j.infrared.2019.103178
   Peng PR, 2023, REMOTE SENS-BASEL, V15, DOI 10.3390/rs15082041
   Ravi Yadav, 2020, Irish Machine Vision Image Proc.
   Redmon J., 2018, CoRR
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Renu Chebrolu Koti Naga, 2019, 2019 International Conference on Communication and Signal Processing (ICCSP), P0838, DOI 10.1109/ICCSP.2019.8698101
   Roszyk K, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22031082
   Sha MZ, 2022, AD HOC NETW, V128, DOI 10.1016/j.adhoc.2022.102784
   Shahzad Ali Raza, 2021, 2021 INT C ROB AUT I, P1
   Shaikh ZA, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22228637
   Socarras Yainuvis, 2013, ICCV WORKSH 3
   Song FZ, 2023, BIOMIMETICS-BASEL, V8, DOI 10.3390/biomimetics8060480
   Song XR, 2021, ALEX ENG J, V60, P73, DOI 10.1016/j.aej.2020.05.035
   Sun Yue, 2021, Am. J. Optics Photon., V9, P32
   Tang LF, 2023, INFORM FUSION, V91, P477, DOI 10.1016/j.inffus.2022.10.034
   Tang LF, 2022, INFORM FUSION, V83, P79, DOI 10.1016/j.inffus.2022.03.007
   Torabi A, 2012, COMPUT VIS IMAGE UND, V116, P210, DOI 10.1016/j.cviu.2011.10.006
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Tumas P, 2020, IEEE ACCESS, V8, P62775, DOI 10.1109/ACCESS.2020.2982539
   Tumas P, 2018, 2018 OPEN CONFERENCE OF ELECTRICAL, ELECTRONIC AND INFORMATION SCIENCES (ESTREAM)
   Vandersteegen M, 2018, LECT NOTES COMPUT SC, V10882, P419, DOI 10.1007/978-3-319-93000-8_47
   Velickovic P, 2018, INT C LEARN REPR, DOI DOI 10.48550/ARXIV.1710.10903
   Vs V, 2022, IEEE WINT CONF APPL, P3697, DOI 10.1109/WACV51458.2022.00375
   Wanchaitanawong N, 2021, PROCEEDINGS OF 17TH INTERNATIONAL CONFERENCE ON MACHINE VISION APPLICATIONS (MVA 2021), DOI 10.23919/MVA51890.2021.9511366
   Wang CY, 2023, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR52729.2023.00721
   Wang H, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3196954
   Wang QW, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14092020
   Wang WG, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12041799
   Wang YZ, 2021, NEUROCOMPUTING, V462, P282, DOI 10.1016/j.neucom.2021.07.096
   Wei Li, 2021, 2021 IEEE 5th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC), P1052, DOI 10.1109/IAEAC50856.2021.9390896
   Wolpert A, 2020, Arxiv, DOI arXiv:2008.08418
   Wu Z, 2014, IEEE COMPUT SOC CONF, P201, DOI 10.1109/CVPRW.2014.39
   Xie Qian, 2024, P IEEE CVF WINT C AP, P655
   Xing YH, 2023, Arxiv, DOI arXiv:2302.00290
   Xu D, 2017, PROC CVPR IEEE, P4236, DOI 10.1109/CVPR.2017.451
   Xu ZW, 2021, IEEE T INTELL TRANSP, V22, P6395, DOI 10.1109/TITS.2020.2991848
   Xu ZW, 2019, INFRARED PHYS TECHN, V96, P199, DOI 10.1016/j.infrared.2018.11.007
   Yang SH, 2023, REMOTE SENS-BASEL, V15, DOI 10.3390/rs15030663
   Yang XX, 2022, 2022 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA 2022), P2920, DOI 10.1109/ICRA46639.2022.9811999
   Yang Y, 2023, FRONT PHYS-LAUSANNE, V11, DOI 10.3389/fphy.2023.1121311
   Yi KF, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app122312476
   Yu JH, 2018, Arxiv, DOI arXiv:1808.08718
   Yu Song, 2020, 2020 International Conference on Intelligent Transportation, Big Data & Smart City (ICITBS). Proceedings, P516, DOI 10.1109/ICITBS49701.2020.00112
   Yue J, 2023, IEEE T IMAGE PROCESS, V32, P5705, DOI 10.1109/TIP.2023.3322046
   Yun JS, 2022, MATHEMATICS-BASEL, V10, DOI 10.3390/math10213966
   Zhang H, 2021, INFORM FUSION, V76, P323, DOI 10.1016/j.inffus.2021.06.008
   Zhang H, 2022, IEEE WINT CONF APPL, P3331, DOI 10.1109/WACV51458.2022.00339
   Zhang H, 2021, IEEE WINT CONF APPL, P72, DOI 10.1109/WACV48630.2021.00012
   Zhang H, 2020, IEEE IMAGE PROC, P276, DOI [10.1109/ICIP40778.2020.9191080, 10.1109/icip40778.2020.9191080]
   Zhang L, 2019, IEEE I CONF COMP VIS, P5126, DOI 10.1109/ICCV.2019.00523
   Zhang L, 2019, INFORM FUSION, V50, P20, DOI 10.1016/j.inffus.2018.09.015
   Zhang SS, 2017, PROC CVPR IEEE, P4457, DOI 10.1109/CVPR.2017.474
   Zhang YT, 2020, IEEE ACCESS, V8, P165071, DOI 10.1109/ACCESS.2020.3022623
   Zhao YF, 2019, ASIAPAC SIGN INFO PR, P2025, DOI 10.1109/APSIPAASC47483.2019.9023228
   Zhao Zhe, 2022, CCRIS'22: Proceedings of the 2022 3rd International Conference on Control, Robotics and Intelligent System, P7, DOI 10.1145/3562007.3562009
   Zhao Zixiang, 2023, P IEEECVF INT C COMP, P8082
   Zheng Y, 2019, Arxiv, DOI arXiv:1903.06999
   Zheng ZH, 2020, AAAI CONF ARTIF INTE, V34, P12993
   Zhou DM, 2020, INFRARED PHYS TECHN, V105, DOI 10.1016/j.infrared.2020.103236
   Zhou XY, 2019, Arxiv, DOI arXiv:1904.07850
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhuang YF, 2022, IEEE T NETW SCI ENG, V9, P1282, DOI 10.1109/TNSE.2021.3139335
   Zou Meiyuan, 2022, 2022 IEEE International Conference on Real-time Computing and Robotics (RCAR), P255, DOI 10.1109/RCAR54675.2022.9872286
   Zuo X, 2023, IET COMPUT VIS, V17, P726, DOI 10.1049/cvi2.12159
NR 203
TC 0
Z9 0
U1 1
U2 1
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD AUG
PY 2024
VL 148
AR 105106
DI 10.1016/j.imavis.2024.105106
EA JUN 2024
PG 22
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA WR5G7
UT WOS:001256607600001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Liu, YJ
   Zhang, XC
   Hao, QQ
   Luo, Y
   Su, JH
   Cai, GR
AF Liu, Yujun
   Zhang, Xiangchen
   Hao, Qiaoqiao
   Luo, Yang
   Su, Jinhe
   Cai, Guorong
TI Giving loss a personal course: Universal loss reweighting to improve
   stereo matching via uncertainty guidance
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Stereo matching; Disparity refinement; Uncertainty; Loss reweighting
ID NETWORK
AB Although learning-based stereo matching methods have achieved remarkable performance, accurately recovering disparity maps for boundary areas (e.g., thin structures) remains an intractable issue. Existing stereo pipelines usually employ the standard L1 loss function, averaging pixel-wise losses equally despite variable difficulty. This mechanism inevitably overwhelms thin structures due to their limited proportion. In this paper, we propose reweighting the L1 loss to focus on pixels of varying hardness. First, we explicitly model the uncertainty of each pixel to gauge the confidence in its prediction. By aggregating the volume, uncertainty is obtained effortlessly. Second, uncertainty is mapped to weights, which reweight the L1 loss accordingly. The core of our approach lies in leveraging uncertainty to personalize the loss map adjustment, progressively optimizing challenging regions during training. Notably, our method requires no extra parameters or inference computations. Finally, we introduce the Boundary Pixel Error (BPE), a novel metric targeting boundary quality. Extensive experiments with the SceneFlow, KITTI 2012, and KITTI 2015 datasets demonstrate the effectiveness and universality of our elegant framework, seamlessly integrating it into existing models as a plug-and-play component, leading to substantial performance improvements.
C1 [Liu, Yujun; Zhang, Xiangchen; Hao, Qiaoqiao; Luo, Yang; Su, Jinhe; Cai, Guorong] Jimei Univ, Sch Comp Engn, Xiamen 361021, Peoples R China.
C3 Jimei University
RP Su, JH; Cai, GR (corresponding author), Jimei Univ, Sch Comp Engn, Xiamen 361021, Peoples R China.
EM sujh@jmu.edu.cn; guorongcai.jmu@gmail.com
RI luoyang, 罗阳/KIE-1093-2024
FU National Natural Science Foundation of China [42371457]; Natural Science
   Foundation of Xiamen, China [3502Z202373036]; Key Project of Natural
   Science Foundation of Fujian Province, China [2022J02045]; Natural
   Science Foundation of Fujian Province, China [2022J01337, 2022J01819,
   2023J01801, 2023J01799, 2022J05157, 2022J011394]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 42371457; in part by the Natural Science
   Foundation of Xiamen, China, under Grant 3502Z202373036; in part by the
   Key Project of Natural Science Foundation of Fujian Province, China,
   under Grant 2022J02045; in part by the Natural Science Foundation of
   Fujian Province, China, under Grant 2022J01337, Grant 2022J01819, Grant
   2023J01801, Grant 2023J01799, Grant 2022J05157, and Grant 2022J011394.
CR Arpit D, 2017, PR MACH LEARN RES, V70
   Bangunharcana A, 2021, IEEE INT C INT ROBOT, P3542, DOI 10.1109/IROS51168.2021.9635909
   Chang JR, 2018, PROC CVPR IEEE, P5410, DOI 10.1109/CVPR.2018.00567
   Chen CR, 2019, IEEE I CONF COMP VIS, P8996, DOI 10.1109/ICCV.2019.00909
   Chen JR, 2023, PROC CVPR IEEE, P12021, DOI 10.1109/CVPR52729.2023.01157
   Chen SL, 2023, IMAGE VISION COMPUT, V130, DOI 10.1016/j.imavis.2022.104614
   Cheng S, 2020, PROC CVPR IEEE, P2521, DOI 10.1109/CVPR42600.2020.00260
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Fu K, 2019, IMAGE VISION COMPUT, V85, P36, DOI 10.1016/j.imavis.2019.02.007
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Guo XY, 2019, PROC CVPR IEEE, P3268, DOI 10.1109/CVPR.2019.00339
   Häne C, 2017, IMAGE VISION COMPUT, V68, P14, DOI 10.1016/j.imavis.2017.07.003
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hirschmüller H, 2005, PROC CVPR IEEE, P807, DOI 10.1109/cvpr.2005.56
   Jing JP, 2023, Arxiv, DOI arXiv:2307.14071
   Kalia M, 2019, IEEE INT CONF ROBOT, P8291, DOI 10.1109/icra.2019.8793610
   Kendall A, 2017, IEEE I CONF COMP VIS, P66, DOI 10.1109/ICCV.2017.17
   Li ZS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6177, DOI 10.1109/ICCV48922.2021.00614
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu BY, 2022, AAAI CONF ARTIF INTE, P1647
   Lou JM, 2023, Arxiv, DOI arXiv:2308.00728
   Mao YM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6291, DOI 10.1109/ICCV48922.2021.00625
   Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438
   Menze M, 2015, PROC CVPR IEEE, P3061, DOI 10.1109/CVPR.2015.7298925
   Michael M, 2013, IEEE INT VEH SYM, P1197, DOI 10.1109/IVS.2013.6629629
   Morley MS, 2001, ADV ENG SOFTW, V32, P467, DOI 10.1016/S0965-9978(00)00107-1
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Pang JH, 2017, IEEE INT CONF COMP V, P878, DOI 10.1109/ICCVW.2017.108
   Shen ZL, 2023, Arxiv, DOI arXiv:2307.16509
   Shen ZL, 2021, PROC CVPR IEEE, P13901, DOI 10.1109/CVPR46437.2021.01369
   Shuya Chen, 2021, Computer Vision - ACCV 2020. 15th Asian Conference on Computer Vision. Revised Selected Papers. Lecture Notes in Computer Science (LNCS 12622), P106, DOI 10.1007/978-3-030-69525-5_7
   Xiao X, 2023, Arxiv, DOI [arXiv:2305.08462, 10.48550/arXiv.2305.08462, DOI 10.48550/ARXIV.2305.08462]
   Xu GW, 2023, PROC CVPR IEEE, P21919, DOI 10.1109/CVPR52729.2023.02099
   Xu HF, 2020, PROC CVPR IEEE, P1956, DOI 10.1109/CVPR42600.2020.00203
   Xu P, 2024, Arxiv, DOI arXiv:2306.15612
   Xue YB, 2022, IMAGE VISION COMPUT, V124, DOI 10.1016/j.imavis.2022.104510
   Yang XW, 2022, IMAGE VISION COMPUT, V117, DOI 10.1016/j.imavis.2021.104336
   Zhang YR, 2021, IMAGE VISION COMPUT, V106, DOI 10.1016/j.imavis.2020.104088
   Zhang YM, 2020, AAAI CONF ARTIF INTE, V34, P12926
   Zhao HL, 2023, LECT NOTES COMPUT SC, V13841, P3, DOI 10.1007/978-3-031-26319-4_1
   Zhao HL, 2023, PROC CVPR IEEE, P1327, DOI 10.1109/CVPR52729.2023.00134
   Zhou C, 2017, IEEE COMPUT SOC CONF, P318, DOI 10.1109/CVPRW.2017.45
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
NR 44
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUL
PY 2024
VL 147
AR 105077
DI 10.1016/j.imavis.2024.105077
EA MAY 2024
PG 9
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA UH2W8
UT WOS:001247110700001
DA 2024-08-05
ER

PT J
AU Wu, Q
   Song, TT
   Fan, SN
   Chen, ZD
   Jin, KL
   Zhou, HJ
AF Wu, Qin
   Song, Tingting
   Fan, Shengnan
   Chen, Zeda
   Jin, Kelei
   Zhou, Haojie
TI Feature alignment via mutual mapping for few-shot fine-grained visual
   classification
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Mutual mapping; Few-shot learning; Feature alignment;
   Self-reconstruction; Fine-grained visual classification
ID NETWORKS
AB Few-shot fine-grained visual classification aims to identify fine-grained concepts with very few samples, which is widely used in many fields, such as the classification of different species of birds in biological research, and the identification of car models in traffic monitoring. Compared with the common few-shot classification task, it encounters difficulties due to significant variations within each class and small gaps between different categories. To address such problems, previous studies primarily project support samples into the space of query samples and employ metric learning to classify query images into their respective categories. However, we observe that such methods are not effective in resolving inter-class variations. To overcome this limitation, we propose a new feature alignment method based on mutual mapping, which simultaneously considers the discriminative features of new samples and classes. Specifically, besides projecting support samples into the space of query samples for reducing intra-class variations, we also project query samples into the space of support samples to increase interclass variations. Furthermore, a direct position self-reconstruction module is proposed to utilize the location information of objects and obtain more discriminative features. Extensive experiments on four fine-grained benchmarks demonstrate that our approach is competitive when compared with other state-of-the-art methods, in both 1-shot and 5-shot settings. In the case of 5-shot, our method achieved the best performance on all four datasets, with 92.11%, 85.31%, 96.09%, and 94.64% accuracies on CUB-200-2011, Stanford Dogs, Stanford Cars, and Aircaft, respectively.
C1 [Wu, Qin; Song, Tingting; Fan, Shengnan; Chen, Zeda; Jin, Kelei; Zhou, Haojie] Jiangnan Univ, Sch Artificial Intelligence & Comp Sci, Wuxi 214122, Peoples R China.
C3 Jiangnan University
RP Zhou, HJ (corresponding author), Jiangnan Univ, Sch Artificial Intelligence & Comp Sci, Wuxi 214122, Peoples R China.
EM zhouhaojie@jiangnan.edu.cn
CR Afrasiyabi A, 2022, PROC CVPR IEEE, P9004, DOI 10.1109/CVPR52688.2022.00881
   Andrychowicz M, 2016, ADV NEUR IN, V29
   Baek J., 2020, P 37 INT C MACH LEAR, DOI [10.5555/3524938.3525634.JMLR.org, DOI 10.5555/3524938.3525634.JMLR.ORG]
   Brown T., 2020, ADV NEURAL INFORM PR, V33, P1877, DOI DOI 10.48550/ARXIV.2005.14165
   Cao SY, 2022, INT J MACH LEARN CYB, V13, P2273, DOI 10.1007/s13042-022-01522-w
   Chen Y, 2019, PROC CVPR IEEE, P5152, DOI 10.1109/CVPR.2019.00530
   Chu XX, 2021, Arxiv, DOI arXiv:2102.10882
   Devlin J, 2018, arXiv, DOI [10.18653/v1/N19-1423, DOI 10.18653/V1/N19-1423]
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Dubey A, 2018, LECT NOTES COMPUT SC, V11216, P71, DOI 10.1007/978-3-030-01258-8_5
   Fan Zhang, 2021, MultiMedia Modeling. 27th International Conference, MMM 2021. Proceedings. Lecture Notes in Computer Science (LNCS 12572), P136, DOI 10.1007/978-3-030-67832-6_12
   Finn C, 2017, PR MACH LEARN RES, V70
   Fort S, 2017, Arxiv, DOI arXiv:1708.02735
   Guo YR, 2022, IEEE T IMAGE PROCESS, V31, P4543, DOI 10.1109/TIP.2022.3184813
   Han-Jia Ye, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8805, DOI 10.1109/CVPR42600.2020.00883
   Hao FS, 2022, IEEE T CIRC SYST VID, V32, P4351, DOI 10.1109/TCSVT.2021.3132912
   Hariharan B, 2017, IEEE I CONF COMP VIS, P3037, DOI 10.1109/ICCV.2017.328
   Hassani A, 2022, Arxiv, DOI [arXiv:2104.05704, 10.48550/arXiv.2104.05704]
   Hong DF, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3324497
   Hong DF, 2019, IEEE T IMAGE PROCESS, V28, P1923, DOI 10.1109/TIP.2018.2878958
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang HW, 2021, PATTERN RECOGN, V116, DOI 10.1016/j.patcog.2021.107935
   Huang HX, 2019, IEEE INT CON MULTI, P91, DOI 10.1109/ICME.2019.00024
   Ji Z, 2020, PATTERN RECOGN LETT, V140, P81, DOI 10.1016/j.patrec.2020.07.015
   Jiang X, 2023, Arxiv, DOI [arXiv:2309.08912, 10.48550/ARXIV.2309.08912, DOI 10.48550/ARXIV.2309.08912]
   Kang D, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8802, DOI 10.1109/ICCV48922.2021.00870
   Khosla A., 2011, CVPR WORKSH, V2
   Kim B, 2022, Arxiv, DOI [arXiv:2206.13691, 10.48550/ARXIV.2206.13691, DOI 10.48550/ARXIV.2206.13691]
   Koch G., 2015, ICML DEEP LEARN WORK, V2, P1
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Lee K, 2019, PROC CVPR IEEE, P10649, DOI 10.1109/CVPR.2019.01091
   Lee S, 2022, PROC CVPR IEEE, P5321, DOI 10.1109/CVPR52688.2022.00526
   Li XX, 2021, IEEE T IMAGE PROCESS, V30, P1318, DOI 10.1109/TIP.2020.3043128
   Li ZC, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3240195
   Liu H, 2022, PROCEEDINGS OF THE 28TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, KDD 2022, P1079, DOI 10.1145/3534678.3539340
   Liu KJ, 2022, IEEE T IMAGE PROCESS, V31, P5570, DOI 10.1109/TIP.2022.3197931
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Maji S, 2013, Arxiv, DOI arXiv:1306.5151
   Munjal B, 2023, PATTERN RECOGN, V133, DOI 10.1016/j.patcog.2022.109049
   Pan YW, 2019, PROC CVPR IEEE, P2234, DOI 10.1109/CVPR.2019.00234
   Qi Y, 2022, LECT NOTES COMPUT SC, V13631, P606, DOI 10.1007/978-3-031-20868-3_45
   Ren MY, 2018, Arxiv, DOI arXiv:1803.00676
   Rong Y, 2023, Arxiv, DOI arXiv:2304.13287
   Rusu AA, ARXIV
   Shaw P, 2018, Arxiv, DOI [arXiv:1803.02155, 10.48550/arXiv.1803.02155]
   Snell J, 2017, ADV NEUR IN, V30
   Subedi B, 2022, MATH PROBL ENG, V2022, DOI 10.1155/2022/9710667
   Subramanyam R, 2023, IEEE WINT CONF APPL, P2478, DOI 10.1109/WACV56688.2023.00251
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Tang H, 2022, PATTERN RECOGN, V130, DOI 10.1016/j.patcog.2022.108792
   Tang H, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P610, DOI 10.1145/3394171.3413884
   Trosten DJ, 2023, PROC CVPR IEEE, P7527, DOI 10.1109/CVPR52729.2023.00727
   Vaswani A, 2017, ADV NEUR IN, V30
   Vinyals O., 2016, Advances in neural information processing systems, V29
   Wah C., 2011, Tech. Rep. CNS-TR-2011-001
   Wang SM, 2023, IMAGE VISION COMPUT, V136, DOI 10.1016/j.imavis.2023.104736
   Wang YX, 2018, PROC CVPR IEEE, P7278, DOI 10.1109/CVPR.2018.00760
   Wei XS, 2019, IEEE T IMAGE PROCESS, V28, P6116, DOI 10.1109/TIP.2019.2924811
   Wertheimer D, 2021, PROC CVPR IEEE, P8008, DOI 10.1109/CVPR46437.2021.00792
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xu CM, 2021, PROC CVPR IEEE, P5178, DOI 10.1109/CVPR46437.2021.00514
   Xu SL, 2022, AAAI CONF ARTIF INTE, P2911
   Yang S, 2022, IEEE T PATTERN ANAL, V44, P9830, DOI 10.1109/TPAMI.2021.3132021
   Yikai Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12833, DOI 10.1109/CVPR42600.2020.01285
   Yu TY, 2022, AAAI CONF ARTIF INTE, P3179
   Zha ZC, 2023, IEEE T CIRC SYST VID, V33, P3947, DOI 10.1109/TCSVT.2023.3236636
   Zhang B, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P2135, DOI 10.1145/3503161.3547961
   Zhang M, 2022, LECT NOTES COMPUT SC, V13680, P453, DOI 10.1007/978-3-031-20044-1_26
   Zhang XC, 2021, IEEE SYS MAN CYBERN, P2102, DOI 10.1109/SMC52423.2021.9659285
   Zheng HL, 2019, PROC CVPR IEEE, P5007, DOI 10.1109/CVPR.2019.00515
   Zheng HL, 2017, IEEE I CONF COMP VIS, P5219, DOI 10.1109/ICCV.2017.557
   Zheng YY, 2023, KNOWL-BASED SYST, V277, DOI 10.1016/j.knosys.2023.110798
NR 72
TC 0
Z9 0
U1 5
U2 5
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUL
PY 2024
VL 147
AR 105032
DI 10.1016/j.imavis.2024.105032
EA MAY 2024
PG 13
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA TF9N3
UT WOS:001239969300001
DA 2024-08-05
ER

PT J
AU Zheng, XL
   Wang, HJ
   Shang, Y
   Chen, G
   Zou, SH
   Yuan, QB
AF Zheng, Xiuling
   Wang, Huijuan
   Shang, Yu
   Chen, Gang
   Zou, Suhua
   Yuan, Quanbo
TI Starting from the structure: A review of small object detection based on
   deep learning
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Small object detection; Data augmentation; Feature extraction; Feature
   fusion; Unsupervised; Transfer learning; Anchor -free
ID FEATURE PYRAMID NETWORK
AB Object detection, as one of the most fundamental and essential tasks in the field of computer vision, has been the focus of unremitting efforts by researchers, who are committed to modifying the neural network structure in order to improve the accuracy of object detection and expedite task execution. As the application scope continues to expand, small object detection has gradually emerged as a crucial branch in the field of object detection. In this paper, the development history of object detection algorithms is introduced, the concept of small objects is introduced, and the current problems and challenges faced by small object detection are outlined. In this paper, the network structure is disassembled from a macroscopic point of view, and improved algorithms such as enhanced data augmentation, improved feature extraction, superior feature fusion, and refined loss functions are described in detail. Furthermore, the paper explores a series of emerging and improved algorithms for small object detection. It encompasses the introduction of advanced strategies such as unsupervised learning, end-to-end training, density cropping, transfer learning, and anchor-free approaches. The paper provides a comprehensive list of commonly used general-purpose datasets and domain-specific datasets for small object detection tasks, offering performance comparisons of the mentioned improved algorithms. In conclusion, the paper summarizes and provides an outlook on current small object detection algorithms, furnishing the reader with a thorough understanding of the field and insights into future directions.
C1 [Zheng, Xiuling; Wang, Huijuan; Shang, Yu; Chen, Gang; Zou, Suhua; Yuan, Quanbo] North China Inst Aerosp Engn, Sch Comp, Langfang 065000, Peoples R China.
   [Yuan, Quanbo] Tianjin Univ, Coll Intelligence & Comp, Tianjin 300072, Peoples R China.
C3 North China Institute of Aerospace Engineering; Tianjin University
RP Wang, HJ; Chen, G (corresponding author), North China Inst Aerosp Engn, Sch Comp, Langfang 065000, Peoples R China.
EM wanghj323@126.com; lfcg66@163.com
FU Fund Project of Central Government Guided Local Science and Technology
   Development [226Z0302G]; Special Project of Langfang Key Research and
   Development [2023011005B]
FX <STRONG>This work was supported by the Fund Project of Central
   Government Guided Local Science and Technology Development under Grant
   No. 226Z0302G and the Special Project of Langfang Key Research and
   Development under Grant No. 2023011005B. </STRONG>
CR Akyon FC, 2022, IEEE IMAGE PROC, P966, DOI 10.1109/ICIP46576.2022.9897990
   Bai YC, 2018, LECT NOTES COMPUT SC, V11217, P210, DOI 10.1007/978-3-030-01261-8_13
   Bosquet B, 2023, PATTERN RECOGN, V133, DOI 10.1016/j.patcog.2022.108998
   Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644
   Cao Yefan, 2022, 2022 7th International Conference on Image, Vision and Computing (ICIVC), P100, DOI 10.1109/ICIVC55077.2022.9886277
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen CY, 2017, LECT NOTES COMPUT SC, V10115, P214, DOI 10.1007/978-3-319-54193-8_14
   Chen JY, 2022, MULTIMED TOOLS APPL, V81, P12093, DOI 10.1007/s11042-021-10833-z
   Chen PY, 2021, IEEE T IMAGE PROCESS, V30, P9099, DOI 10.1109/TIP.2021.3118953
   Chen Y, 2018, EURASIP J WIREL COMM, DOI 10.1186/s13638-018-1133-2
   Chen Yukang, 2020, arXiv
   Cui LS, 2022, IEEE T CYBERNETICS, V52, P2300, DOI 10.1109/TCYB.2020.3004636
   Deng CF, 2022, IEEE T MULTIMEDIA, V24, P1968, DOI 10.1109/TMM.2021.3074273
   Ding X, 2022, 2022 INT C COMP ENG, P735, DOI [10.1109/ICCEAI55464.2022.00155, DOI 10.1109/ICCEAI55464.2022.00155]
   Du DW, 2019, IEEE INT CONF COMP V, P199, DOI 10.1109/ICCVW.2019.00029
   Duan KW, 2024, IEEE T PATTERN ANAL, V46, P3509, DOI 10.1109/TPAMI.2023.3342120
   Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fang YX, 2023, PROC CVPR IEEE, P19358, DOI 10.1109/CVPR52729.2023.01855
   Ge Z, 2021, Arxiv, DOI arXiv:2107.08430
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gong YQ, 2021, IEEE WINT CONF APPL, P1159, DOI 10.1109/WACV48630.2021.00120
   Guo XH, 2023, Arxiv, DOI arXiv:2303.14977
   Han K, 2023, IEEE T PATTERN ANAL, V45, P87, DOI 10.1109/TPAMI.2022.3152247
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Ji SJ, 2023, COMPUT ELECTR ENG, V105, DOI 10.1016/j.compeleceng.2022.108490
   Jianlu Fu, 2021, 2021 IEEE 4th Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC), P1261, DOI 10.1109/IMCEC51613.2021.9482158
   Kisantal M, 2019, Arxiv, DOI arXiv:1902.07296
   Kondo Y, 2023, 2023 18TH INTERNATIONAL CONFERENCE ON MACHINE VISION AND APPLICATIONS, MVA, DOI 10.23919/MVA57639.2023.10215935
   Lai HQ, 2023, SENSORS-BASEL, V23, DOI 10.3390/s23115307
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Li Bingfeng, 2023, J. Comp. Aid. Design Comp. Graph., V35, P525
   Li CJ, 2021, PATTERN RECOGN LETT, V145, P127, DOI 10.1016/j.patrec.2021.02.003
   Li JA, 2017, PROC CVPR IEEE, P1951, DOI 10.1109/CVPR.2017.211
   Li Xuewei, 2023, Appl. Res. Comp., V40, P607
   Li Z, 2021, IEEE T IMAGE PROCESS, V30, P4587, DOI 10.1109/TIP.2021.3072811
   Liang Xin, 2022, Mod. Comp., V28, P9
   Lim JS, 2021, 3RD INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE IN INFORMATION AND COMMUNICATION (IEEE ICAIIC 2021), P181, DOI 10.1109/ICAIIC51459.2021.9415217
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu MS, 2023, APPL INTELL, V53, P18171, DOI 10.1007/s10489-023-04456-0
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu S, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3152317
   Liu ST, 2019, Arxiv, DOI arXiv:1911.09516
   Liu ST, 2018, LECT NOTES COMPUT SC, V11215, P404, DOI 10.1007/978-3-030-01252-6_24
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu YC, 2022, PROC CVPR IEEE, P9809, DOI 10.1109/CVPR52688.2022.00959
   [刘颖 Liu Ying], 2020, [电子学报, Acta Electronica Sinica], V48, P590
   Liu Yuhong, 2023, J. Comp. Eng. Appl., V59
   Liu ZG, 2023, IEEE ACCESS, V11, P1742, DOI 10.1109/ACCESS.2023.3233964
   Mahaur B, 2023, PATTERN RECOGN LETT, V168, P115, DOI 10.1016/j.patrec.2023.03.009
   Meethal Akhil, 2023, P IEEECVF C COMPUTER, P2045
   Mehta S, 2022, Arxiv, DOI arXiv:2110.02178
   Meng Guo, 2021, Progress, V58, P213
   Mirzaei B, 2023, SENSORS-BASEL, V23, DOI 10.3390/s23156887
   Najibi M, 2017, IEEE I CONF COMP VIS, P4885, DOI 10.1109/ICCV.2017.522
   Niu Weihua, 2023, Chinese Journal of Sensors and Actuators, P36, DOI 10.3969/j.issn.1004-1699.2023.01.006
   Ouyang HD, 2022, Arxiv, DOI arXiv:2211.06588
   Park HJ, 2023, SENSORS-BASEL, V23, DOI 10.3390/s23094432
   Qi GQ, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14020420
   Qu Hongquan Xie, 2022, Progress, V59, P364
   Quan Y, 2023, IEEE Trans Image Process
   Rabbi J, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12091432
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Rui C, 2021, Arxiv, DOI arXiv:2109.01800
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shi TJ, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14215488
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun W, 2022, APPL INTELL, V52, P8448, DOI 10.1007/s10489-021-02893-3
   Tan R., 2020, P IEEE CVF C COMP VI, DOI [10.1109/CVPR42600.2020.01079, DOI 10.1109/CVPR42600.2020.01079]
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Tong K, 2023, J VIS COMMUN IMAGE R, V93, DOI 10.1016/j.jvcir.2023.103830
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Wahyudi Dwi, 2022, 2022 14 INT C INF TE, P314, DOI [10.1109/ICITEE56407.2022.9954101, DOI 10.1109/ICITEE56407.2022.9954101]
   Wang G, 2023, SENSORS-BASEL, V23, DOI 10.3390/s23167190
   Wang JW, 2022, Arxiv, DOI arXiv:2110.13389
   Wang JW, 2021, INT C PATT RECOG, P3791, DOI 10.1109/ICPR48806.2021.9413340
   Wang KX, 2019, IEEE I CONF COMP VIS, P9196, DOI 10.1109/ICCV.2019.00929
   Wang Nengwen, 2023, ICCAI '23: Proceedings of the 2023 9th International Conference on Computing and Artificial Intelligence, P13, DOI 10.1145/3594315.3594318
   Wang XW, 2022, COMPUT ELECTRON AGR, V198, DOI 10.1016/j.compag.2022.107035
   Wang Y, 2022, EXPERT SYST APPL, V197, DOI 10.1016/j.eswa.2022.116793
   Wei Jialong, 2023, 2023 INT C APPL INT, P1
   Weiss Karl, 2016, Journal of Big Data, V3, DOI 10.1186/s40537-016-0043-6
   Wolpert A, 2020, Arxiv, DOI arXiv:2008.08418
   Xia GS, 2018, PROC CVPR IEEE, P3974, DOI 10.1109/CVPR.2018.00418
   Xie EZ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8372, DOI 10.1109/ICCV48922.2021.00828
   Xu FQ, 2022, NEURAL COMPUT APPL, V34, P14881, DOI 10.1007/s00521-022-07264-8
   Xu XK, 2023, REMOTE SENS-BASEL, V15, DOI 10.3390/rs15143525
   Xue ZJ, 2020, CHIN CONTR CONF, P7212, DOI [10.23919/ccc50068.2020.9189352, 10.23919/CCC50068.2020.9189352]
   Yang CHY, 2022, PROC CVPR IEEE, P13658, DOI 10.1109/CVPR52688.2022.01330
   Yang L, 2023, IEEE T INTELL TRANSP, V24, P7717, DOI 10.1109/TITS.2022.3193909
   Yang S, 2016, PROC CVPR IEEE, P5525, DOI 10.1109/CVPR.2016.596
   Yang X, 2023, IEEE T PATTERN ANAL, V45, P2384, DOI 10.1109/TPAMI.2022.3166956
   Yankang Chen, 2024, Comp. Eng. Appl., P1
   Yu XH, 2020, IEEE WINT CONF APPL, P1246, DOI 10.1109/WACV45572.2020.9093394
   Yuan X., 2023, P IEEE CVF INT C COM, P6317
   Yuying Guo, 2024, Electron. Opt. Control., P1
   Zeng NY, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3153997
   Zhang H., 2022, P AS C COMP VIS, P1161, DOI DOI 10.48550/ARXIV.2105.14447
   Zhang HY, 2021, PROC CVPR IEEE, P8510, DOI 10.1109/CVPR46437.2021.00841
   Zhang HY, 2023, PATTERN RECOGN, V143, DOI 10.1016/j.patcog.2023.109801
   Zhang YF, 2022, NEUROCOMPUTING, V506, P146, DOI 10.1016/j.neucom.2022.07.042
   Zhao QJ, 2019, AAAI CONF ARTIF INTE, P9259
   Zheng QY, 2021, IMAGE VISION COMPUT, V108, DOI 10.1016/j.imavis.2021.104128
   Zheng ZH, 2020, AAAI CONF ARTIF INTE, V34, P12993
   Zhipeng Liu, 2021, 2021 20th International Symposium on Distributed Computing and Applications for Business Engineering and Science (DCABES), P96, DOI 10.1109/DCABES52998.2021.00031
   Zhu R, 2019, PROC CVPR IEEE, P2263, DOI 10.1109/CVPR.2019.00237
   Zhu XZ, 2021, Arxiv, DOI [arXiv:2010.04159, 10.48550/arXiv.2010.04159]
   Zhu Z, 2016, PROC CVPR IEEE, P2110, DOI 10.1109/CVPR.2016.232
   Zong ZF, 2023, IEEE I CONF COMP VIS, P6725, DOI 10.1109/ICCV51070.2023.00621
NR 113
TC 0
Z9 0
U1 4
U2 4
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUN
PY 2024
VL 146
AR 105054
DI 10.1016/j.imavis.2024.105054
EA MAY 2024
PG 20
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA SZ4T4
UT WOS:001238265800001
DA 2024-08-05
ER

PT J
AU Wu, S
   Zhang, GJ
   Liu, XF
AF Wu, Shuang
   Zhang, Guangjian
   Liu, Xuefeng
TI SwinSOD: Salient object detection using swin-transformer
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Salient object detection; Swin-transformer; Poly loss; Feature fusion
ID NETWORK
AB The Transformer structure has achieved excellent performance in a wide range of applications in computer vision, and Swin-Transformer also shows strong feature representation capabilities. On this basis, we proposed a fusion model SwinSOD for RGB salient object detection. This model used a Swin-Transformer as the encoder to extract hierarchical features, was driven by a multi-head attention mechanism to bridge the gap between hierarchical features, progressively fused adjacent layer feature information under the guidance of global information, and refined the boundaries of saliency objects through the feedback information. Specifically, the SwinTransformer encoder extracted multi-level features and then recalibrated the channels to optimize intra-layer channel features. The feature fusion module realized feature fusion between each layer under the guidance of global information. In order to clarify the fuzzy boundaries, the second stage feature fusion achieved edge refinement under the guidance of feedback information. The proposed model outperforms state-of-the-art models on five popular SOD datasets, demonstrating the advanced performance of this network. Code released: https ://github.com/user-wu/SwinSOD.
C1 [Wu, Shuang] Chongqing Metropolitan Coll Sci & Technol, Sch Artificial Intelligence & Big Data, Chongqing 402167, Peoples R China.
   [Wu, Shuang; Zhang, Guangjian; Liu, Xuefeng] Chongqing Univ Technol, Sch Artificial Intelligence, Chongqing 400054, Peoples R China.
   [Wu, Shuang] Futong Technol, Genesis AI Lab, Chengdu 610054, Peoples R China.
C3 Chongqing University of Technology
RP Wu, S (corresponding author), Chongqing Metropolitan Coll Sci & Technol, Sch Artificial Intelligence & Big Data, Chongqing 402167, Peoples R China.; Wu, S (corresponding author), Chongqing Univ Technol, Sch Artificial Intelligence, Chongqing 400054, Peoples R China.; Wu, S (corresponding author), Futong Technol, Genesis AI Lab, Chengdu 610054, Peoples R China.
EM wushuang@2020.cqut.edu.cn
RI Wu, Shuang/GLU-2681-2022
OI Wu, Shuang/0000-0001-9245-6037
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Chen SH, 2018, LECT NOTES COMPUT SC, V11213, P236, DOI 10.1007/978-3-030-01240-3_15
   Chen TS, 2016, IEEE T NEUR NET LEAR, V27, P1135, DOI 10.1109/TNNLS.2015.2506664
   Chen ZY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2899, DOI 10.1145/3474085.3475467
   Chen ZY, 2020, AAAI CONF ARTIF INTE, V34, P10599
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Deng ZJ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P684
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698
   Fang H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16311, DOI 10.1109/ICCV48922.2021.01602
   Feng MY, 2019, PROC CVPR IEEE, P1623, DOI 10.1109/CVPR.2019.00172
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jiang ZL, 2013, PROC CVPR IEEE, P2043, DOI 10.1109/CVPR.2013.266
   Jun Wei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13022, DOI 10.1109/CVPR42600.2020.01304
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee MS, 2022, AAAI CONF ARTIF INTE, P12993
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Li Z., 2023, IEEE Transactions on Neural Networks and Learning Systems
   Li ZC, 2022, IEEE T PATTERN ANAL, V44, P9904, DOI 10.1109/TPAMI.2021.3132068
   Lin WD, 2021, Arxiv, DOI arXiv:2104.14984
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu ZY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4481, DOI 10.1145/3474085.3475601
   Liu ZY, 2022, IEEE T CIRC SYST VID, V32, P4486, DOI 10.1109/TCSVT.2021.3127149
   Luo ZY, 2024, Arxiv, DOI arXiv:2311.15011
   Ma MC, 2023, IEEE T IMAGE PROCESS, V32, P1026, DOI 10.1109/TIP.2022.3232209
   Ma MC, 2021, AAAI CONF ARTIF INTE, V35, P2311
   Margolin R, 2014, PROC CVPR IEEE, P248, DOI 10.1109/CVPR.2014.39
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Qin XB, 2020, PATTERN RECOGN, V106, DOI 10.1016/j.patcog.2020.107404
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Shi JP, 2016, IEEE T PATTERN ANAL, V38, P717, DOI 10.1109/TPAMI.2015.2465960
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404
   Wang LJ, 2015, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR.2015.7298938
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wang Y, 2023, PROC CVPR IEEE, P10031, DOI 10.1109/CVPR52729.2023.00967
   Wei J, 2020, AAAI CONF ARTIF INTE, V34, P12321
   Wu HP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P22, DOI 10.1109/ICCV48922.2021.00009
   Wu Z., 2023, P AAAI C ART INT, P2883
   Wu ZY, 2022, IEEE T IMAGE PROCESS, V31, P6649, DOI 10.1109/TIP.2022.3214332
   Xie CX, 2022, PROC CVPR IEEE, P11707, DOI 10.1109/CVPR52688.2022.01142
   Xu BW, 2021, AAAI CONF ARTIF INTE, V35, P3004
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Youwei Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9410, DOI 10.1109/CVPR42600.2020.00943
   Yuan L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P538, DOI 10.1109/ICCV48922.2021.00060
   Yun YK, 2024, IEEE T MULTIMEDIA, V26, P4667, DOI 10.1109/TMM.2023.3325731
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhao T, 2019, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR.2019.00320
   Zhao X., 2020, PROC 16 EUR C COMPU, P35, DOI 10.1007/ 978-3-030-58536-5_3
NR 58
TC 0
Z9 0
U1 2
U2 2
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUN
PY 2024
VL 146
AR 105039
DI 10.1016/j.imavis.2024.105039
EA APR 2024
PG 10
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA SO3A1
UT WOS:001235341100001
DA 2024-08-05
ER

PT J
AU Yang, Y
   Sun, Y
   Gao, W
   Wang, XY
   Zeng, LL
AF Yang, Yang
   Sun, Yue
   Gao, Wei
   Wang, Xinyu
   Zeng, Lanling
TI Bilateral regularized optimization model for edge-preserving image
   smoothing
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Edge -preserving; Image smoothing; Bilateral regularization; Fixed point
   iteration
ID DECOMPOSITION
AB Edge -preserving image smoothing is vital in the field of image processing and computational photography. The state-of-the-art filters based on optimization models have achieved promising performance. However, most of them fail to consider the spatial support in the regularization term, thus limiting the edge -preserving capabilities. In this paper, inspired by the bilateral filter, which consists of a range kernel and a spatial kernel. we propose to leverage bilateral kernel as a penalty function, and embed it into an optimization model for edge -preserving image smoothing. Furthermore, we propose to incorporate an edge -aware weighted scheme in the data term design, which further improves the edge -preserving capability. The bilateral function is non -convex and can be non -trivial to solve. In this paper, we propose a novel iterative solution based on fixed point iteration, where the main burden in each iteration is a bilateral filtering process. We have conducted extensive experiments to evaluate the proposed filter. Experiment results indicate that our filter benefits a variety of image processing tasks. Moreover, we propose an efficient approximation of the proposed filter, which is able to significantly accelerate the filtering process with neglectable sacrifice of smoothing quality.
C1 [Yang, Yang; Sun, Yue; Gao, Wei; Wang, Xinyu; Zeng, Lanling] Jiangsu Univ, Dept Comp Sci, Xuefu Rd 301, Zhenjiang 212013, Jiangsu, Peoples R China.
C3 Jiangsu University
RP Yang, Y (corresponding author), Jiangsu Univ, Dept Comp Sci, Xuefu Rd 301, Zhenjiang 212013, Jiangsu, Peoples R China.
EM yyoung@ujs.edu.cn
FU NSFC [62171205, 61402205]; Jiangsu University [13JDG085]
FX This paper was funded by NSFC under Grant No. 62171205, 61402205,
   Jiangsu University under Grant No. 13JDG085.
CR Badri H, 2015, IEEE T VIS COMPUT GR, V21, P743, DOI 10.1109/TVCG.2015.2396064
   Barash D, 2004, IMAGE VISION COMPUT, V22, P73, DOI 10.1016/j.imavis.2003.08.005
   Bi S, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766946
   Chaudhury KN, 2016, IEEE T IMAGE PROCESS, V25, P2519, DOI 10.1109/TIP.2016.2548363
   Chen QF, 2017, IEEE I CONF COMP VIS, P2516, DOI 10.1109/ICCV.2017.273
   Criminisi A, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1857907.1857910
   Durand F, 2002, ACM T GRAPHIC, V21, P257, DOI 10.1145/566570.566574
   Fan QN, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275081
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Feng YD, 2022, IEEE T NEUR NET LEAR, V33, P7223, DOI 10.1109/TNNLS.2021.3084473
   Gastal ESL, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964964
   Gu B, 2013, IEEE T IMAGE PROCESS, V22, P70, DOI 10.1109/TIP.2012.2214047
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Ham B, 2018, IEEE T PATTERN ANAL, V40, P192, DOI 10.1109/TPAMI.2017.2669034
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Kang DJ, 2001, IMAGE VISION COMPUT, V19, P369, DOI 10.1016/S0262-8856(00)00085-8
   Lanza A, 2015, SIAM J SCI COMPUT, V37, pS30, DOI 10.1137/140967982
   Li SH, 2023, IMAGE VISION COMPUT, V135, DOI 10.1016/j.imavis.2023.104709
   Li YJ, 2019, IEEE T PATTERN ANAL, V41, P1909, DOI 10.1109/TPAMI.2018.2890623
   Li YJ, 2016, LECT NOTES COMPUT SC, V9908, P154, DOI 10.1007/978-3-319-46493-0_10
   Li ZG, 2018, IEEE T IMAGE PROCESS, V27, P442, DOI 10.1109/TIP.2017.2750418
   Li ZG, 2015, IEEE T IMAGE PROCESS, V24, P5432, DOI 10.1109/TIP.2015.2482903
   Liu W, 2018, Arxiv, DOI arXiv:1812.07122
   Liu W, 2022, IEEE T PATTERN ANAL, V44, P6631, DOI 10.1109/TPAMI.2021.3097891
   Liu W, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3388887
   Liu W, 2020, IEEE T CIRC SYST VID, V30, P23, DOI 10.1109/TCSVT.2018.2890202
   Min DB, 2014, IEEE T IMAGE PROCESS, V23, P5638, DOI 10.1109/TIP.2014.2366600
   Paris S, 2015, COMMUN ACM, V58, P81, DOI 10.1145/2723694
   Shen C., 2012, SIGGRAPH Asia 2012 Technical Briefs, V6
   Sun ZG, 2020, IEEE T IMAGE PROCESS, V29, P500, DOI 10.1109/TIP.2019.2928631
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Wu HK, 2018, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2018.00197
   Xu J, 2021, IEEE T MULTIMEDIA, V23, P4065, DOI 10.1109/TMM.2020.3037535
   Xu L, 2015, PR MACH LEARN RES, V37, P1669
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Yang QX, 2012, LECT NOTES COMPUT SC, V7572, P399, DOI 10.1007/978-3-642-33718-5_29
   Yang Y, 2023, IEEE T MULTIMEDIA, V25, P4148, DOI 10.1109/TMM.2022.3171686
   Yang Y, 2022, IEEE T CIRC SYST VID, V32, P4150, DOI 10.1109/TCSVT.2021.3124291
   Yeganeh H, 2013, IEEE T IMAGE PROCESS, V22, P657, DOI 10.1109/TIP.2012.2221725
   Yin H, 2019, PROC CVPR IEEE, P8750, DOI 10.1109/CVPR.2019.00896
   Zhu FD, 2019, IEEE T IMAGE PROCESS, V28, P3556, DOI 10.1109/TIP.2019.2908778
NR 41
TC 0
Z9 0
U1 2
U2 2
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUN
PY 2024
VL 146
AR 105031
DI 10.1016/j.imavis.2024.105031
EA APR 2024
PG 10
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA SM4C1
UT WOS:001234846200001
DA 2024-08-05
ER

PT J
AU Khojaste-Sarakhsi, M
   Haghighi, SS
   Ghomi, SMTF
   Marchiori, E
AF Khojaste-Sarakhsi, M.
   Haghighi, Seyedhamidreza Shahabi
   Ghomi, S. M. T. Fatemi
   Marchiori, Elena
TI A 3D multi-scale CycleGAN framework for generating synthetic PETs from
   MRIs for Alzheimer 's disease diagnosis
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Cycle GAN; Multi-scale GAN; 3D image-to-image translation; Image
   synthesis; Alzheimer's Disease diagnosis
ID IMAGE QUALITY; NEURAL-NETWORKS; CT
AB This paper proposes a novel framework for generating synthesized PET images from MRIs to fill in missing PETs and help with Alzheimer ' s disease (AD) diagnosis. This framework employs a 3D multi-scale image -to-image CycleGAN architecture for the end -to -end translation of MRI and PET domains together. A hybrid loss function is also proposed to enforce structural similarity while preserving voxel-wise similarity and avoiding blurry images. As shown by the quantitative and visual assessment of the synthesized PETs, this framework is superior to the state -of -the -art. Moreover, using these synthesized PETs helps improve the ternary classification of AD subjects (AD vs. MCI vs. NC). Specifically, assuming an extreme case where none of the subjects has a PET, feeding the classifier with MRIs and their corresponding synthetic PETs results in a more accurate diagnosis than feeding it with just available MRIs. Accordingly, the proposed framework can help improve AD diagnosis, which is the final goal of the current study. Ablation investigation of the proposed multi-scale framework as well as the proposed loss function, is also conducted to study their contribution to the quality of synthesized PETs. Furthermore, other factors, such as stopping criteria, the type of normalization layer, the activation function, and dropouts, are examined, concluding that the appropriate use of these factors can significantly improve the quality of synthesized PETs.
C1 [Khojaste-Sarakhsi, M.; Haghighi, Seyedhamidreza Shahabi; Ghomi, S. M. T. Fatemi] Amirkabir Univ Technol, Dept Ind Engn, Tehran, Iran.
   [Khojaste-Sarakhsi, M.; Marchiori, Elena] Radboud Univ Nijmegen, Inst Comp & Informat Sci, Nijmegen, Netherlands.
C3 Amirkabir University of Technology; Radboud University Nijmegen
RP Haghighi, SS (corresponding author), Amirkabir Univ Technol, Dept Ind Engn, Tehran, Iran.
EM shahabi@aut.ac.ir
CR Alberdi A, 2016, ARTIF INTELL MED, V71, P1, DOI 10.1016/j.artmed.2016.06.003
   Albishri AA, 2019, IEEE INT C BIOINFORM, P1416, DOI 10.1109/BIBM47256.2019.8983266
   Alzheimer's Assoc, 2018, ALZHEIMERS DEMENT, V14, P367, DOI 10.1016/j.jalz.2018.02.001
   Barrett HH, 2015, PHYS MED BIOL, V60, pR1, DOI 10.1088/0031-9155/60/2/R1
   Bazangani F, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22124640
   Ben-Cohen A, 2019, ENG APPL ARTIF INTEL, V78, P186, DOI 10.1016/j.engappai.2018.11.013
   Bi L, 2017, LECT NOTES COMPUT SC, V10555, P43, DOI 10.1007/978-3-319-67564-0_5
   Cheema MN, 2021, IEEE T IND INFORM, V17, P7991, DOI 10.1109/TII.2021.3064369
   Chen K T, 2023, AJNR Am J Neuroradiol, V44, P1012, DOI 10.3174/ajnr.A7961
   Chen SJ, 2023, BIOMED SIGNAL PROCES, V86, DOI 10.1016/j.bspc.2023.105197
   Chen YL, 2019, FRONT GENET, V10, DOI 10.3389/fgene.2019.01110
   Chen Y, 2024, BIOMED SIGNAL PROCES, V92, DOI 10.1016/j.bspc.2024.106100
   Chow LS, 2016, BIOMED SIGNAL PROCES, V27, P145, DOI 10.1016/j.bspc.2016.02.006
   Cicek Ozgun, 2016, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016. 19th International Conference. Proceedings: LNCS 9901, P424, DOI 10.1007/978-3-319-46723-8_49
   Cohen JP, 2018, LECT NOTES COMPUT SC, V11070, P529, DOI 10.1007/978-3-030-00928-1_60
   Dayarathna Kh. Sanuwani, 2024, Review analysis, P2
   Demsar J, 2006, J MACH LEARN RES, V7, P1
   Devi S. Gayathri, 2011, Proceedings of the International Conference on Sustainable Energy and Intelligent Systems (SEISCON 2011), P844, DOI 10.1049/cp.2011.0483
   Emami H, 2018, MED PHYS, V45, P3627, DOI 10.1002/mp.13047
   Farina FR, 2020, NEUROIMAGE, V215, DOI 10.1016/j.neuroimage.2020.116795
   Ferreira A, 2024, MED IMAGE ANAL, V93, DOI 10.1016/j.media.2024.103100
   Franco-Barranco Daniel, 2022, Comput. Methods Prog. Biomed., V222, P7
   Frid-Adar M, 2018, NEUROCOMPUTING, V321, P321, DOI 10.1016/j.neucom.2018.09.013
   Gao S., 2022, International Journal of Cognitive Computing in Engineering, V3, P1, DOI [DOI 10.1016/J.IJCCE.2021.12.002, 10.1016/j.ijcce.2021.12.002]
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   Hu SY, 2022, IEEE T MED IMAGING, V41, P145, DOI 10.1109/TMI.2021.3107013
   Huang B, 2020, COMPUT MATH METHOD M, V2020, DOI 10.1155/2020/8279342
   Huang W, 2022, INFORM SCIENCES, V609, P691, DOI 10.1016/j.ins.2022.07.091
   Huh J, 2023, MED IMAGE ANAL, V83, DOI 10.1016/j.media.2022.102651
   Hussein R, 2024, MED IMAGE ANAL, V93, DOI 10.1016/j.media.2023.103072
   Islam Jyoti, 2020, Brain Inform, V7, P3, DOI 10.1186/s40708-020-00104-2
   Isola Phillip, 2016, Image-to-Image Translation with Conditional Adversarial Networks, P11
   Jeong JJ, 2022, J DIGIT IMAGING, V35, P137, DOI 10.1007/s10278-021-00556-w
   Jiang J, 2018, LECT NOTES COMPUT SC, V11071, P777, DOI 10.1007/978-3-030-00934-2_86
   Jin Y, 2024, LECT NOTES COMPUT SC, V14348, P94, DOI 10.1007/978-3-031-45673-2_10
   Jung E, 2023, PATTERN RECOGN, V133, DOI 10.1016/j.patcog.2022.109061
   Junqing Liu, 2012, 2012 5th International Conference on BioMedical Engineering and Informatics (BMEI), P131, DOI 10.1109/BMEI.2012.6512971
   Kang SK, 2021, NEUROIMAGE, V232, DOI 10.1016/j.neuroimage.2021.117890
   Kazeminia S, 2020, ARTIF INTELL MED, V109, DOI 10.1016/j.artmed.2020.101938
   Kelkar VA, 2023, IEEE T MED IMAGING, V42, P1799, DOI 10.1109/TMI.2023.3241454
   Khojaste-Sarakhsi M., 2022, Deep learning for Alzheimer's disease diagnosis: A survey 8
   Kong F, 2023, WORLD WIDE WEB, V26, P1073, DOI 10.1007/s11280-022-01066-7
   Lin WY, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.646013
   Liu FY, 2023, BIOMED SIGNAL PROCES, V80, DOI 10.1016/j.bspc.2022.104400
   Liu YB, 2022, MED IMAGE ANAL, V75, DOI 10.1016/j.media.2021.102266
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Luo JM, 2023, IMAGE VISION COMPUT, V137, DOI 10.1016/j.imavis.2023.104761
   Martinez-Murcia FJ, 2020, IEEE J BIOMED HEALTH, V24, P17, DOI 10.1109/JBHI.2019.2914970
   McNaughton Jake, 2023, Machine Learning for Medical Image Translation: A Systematic Review, V9
   Mihelcic M, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0187364
   Mirza M., 2014, ARXIV
   Mutasa S, 2020, CLIN IMAG, V65, P96, DOI 10.1016/j.clinimag.2020.04.025
   Nie Dong, 2017, Med Image Comput Comput Assist Interv, V10435, P417, DOI 10.1007/978-3-319-66179-7_48
   Oulbacha R, 2020, I S BIOMED IMAGING, P1784, DOI [10.1109/isbi45749.2020.9098421, 10.1109/ISBI45749.2020.9098421]
   Pan YS, 2018, LECT NOTES COMPUT SC, V11072, P455, DOI 10.1007/978-3-030-00931-1_52
   Rallabandi VPS, 2023, BIOMED SIGNAL PROCES, V80, DOI 10.1016/j.bspc.2022.104312
   Rebouças PP, 2019, APPL SOFT COMPUT, V76, P649, DOI 10.1016/j.asoc.2018.10.057
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sikka A, 2018, LECT NOTES COMPUT SC, V11037, P80, DOI 10.1007/978-3-030-00536-8_9
   Sikka Apoorva, 2021, MRI to PET CrossModality Translation using Globally and Locally Aware GAN (GLA-GAN) for MultiModal Diagnosis of Alzheimer's Disease, V8
   Snell J, 2017, IEEE IMAGE PROC, P4277, DOI 10.1109/ICIP.2017.8297089
   Spasov S, 2019, NEUROIMAGE, V189, P276, DOI 10.1016/j.neuroimage.2019.01.031
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Tang Anda, 2022, A Survey for Sparse Regularization Based Compression Methods, V8
   Tharwat A, 2021, APPL COMPUT INFORM, V17, P168, DOI 10.1016/j.aci.2018.08.003
   Tran ST, 2021, HEALTHCARE-BASEL, V9, DOI 10.3390/healthcare9010054
   Treder MS, 2022, J NEUROSCI METH, V374, DOI 10.1016/j.jneumeth.2022.109579
   Tu Y, 2024, BIOMED SIGNAL PROCES, V89, DOI 10.1016/j.bspc.2023.105709
   Uzunova H, 2019, LECT NOTES COMPUT SC, V11769, P112, DOI 10.1007/978-3-030-32226-7_13
   Vega F, 2024, J MAGN RESON IMAGING, V59, P1021, DOI 10.1002/jmri.29070
   Venkat T, 2014, INT J COMPUT SCI NET, V14, P56
   Venkataramanan AK, 2021, IEEE ACCESS, V9, P28872, DOI 10.1109/ACCESS.2021.3056504
   Wang CH, 2024, MED IMAGE ANAL, V91, DOI 10.1016/j.media.2023.103032
   Wang Y, 2024, MED IMAGE ANAL, V91, DOI 10.1016/j.media.2023.102983
   Wang Y, 2019, IEEE T MED IMAGING, V38, P1328, DOI 10.1109/TMI.2018.2884053
   Wang Y, 2018, LECT NOTES COMPUT SC, V11070, P329, DOI 10.1007/978-3-030-00928-1_38
   Wang Y, 2018, NEUROIMAGE, V174, P550, DOI 10.1016/j.neuroimage.2018.03.045
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wei W, 2018, LECT NOTES COMPUT SC, V11072, P514, DOI 10.1007/978-3-030-00931-1_59
   Yamashita R, 2018, INSIGHTS IMAGING, V9, P611, DOI 10.1007/s13244-018-0639-9
   You CY, 2020, IEEE T MED IMAGING, V39, P188, DOI 10.1109/TMI.2019.2922960
   You CY, 2018, IEEE ACCESS, V6, P41839, DOI 10.1109/ACCESS.2018.2858196
   Zhan Bo, 2022, Knowl.-Based Syst., V252, P9
   Zhang DQ, 2012, NEUROIMAGE, V59, P895, DOI 10.1016/j.neuroimage.2011.09.069
   Zhang J, 2022, COMPUT METH PROG BIO, V217, DOI 10.1016/j.cmpb.2022.106676
   Zhang Mengyi, 2024, Biomed. Sign. Process. Control, V89, P3
   Zhang XH, 2021, J MED IMAGING, V8, DOI 10.1117/1.JMI.8.6.065501
   Zhang YD, 2018, J COMPUT SCI-NETH, V28, P1, DOI 10.1016/j.jocs.2018.07.003
   Zhou TX, 2019, ARRAY-NY, V3-4, DOI 10.1016/j.array.2019.100004
   Zhou X, 2021, ALZHEIMERS RES THER, V13, DOI 10.1186/s13195-021-00797-5
   Zhou XY, 2020, LECT NOTES COMPUT SC, V11977, P101, DOI 10.1007/978-3-030-37969-8_13
   Zhou ZW, 2020, IEEE T MED IMAGING, V39, P1856, DOI 10.1109/TMI.2019.2959609
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu Jun-Yan, 2017, Toward Multimodal Image-to-Image Translation, V11
   Zotova D, 2021, LECT NOTES COMPUT SC, V12965, P142, DOI 10.1007/978-3-030-87592-3_14
NR 96
TC 0
Z9 0
U1 5
U2 5
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUN
PY 2024
VL 146
AR 105017
DI 10.1016/j.imavis.2024.105017
EA APR 2024
PG 17
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA RX1O3
UT WOS:001230868000001
DA 2024-08-05
ER

PT J
AU Bai, C
   Han, XJ
AF Bai, Can
   Han, Xianjun
TI MRFormer: Multiscale retractable transformer for medical image
   progressive denoising via noise level estimation
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Medical image processing; Noise level estimation; Progressive denoising;
   Denoising model
AB Clear medical images are important for auxiliary diagnoses, but the images generated by various medical devices inevitably contain considerable noise. Although various models have been proposed for denoising, these methods ignore the fact that different types of medical images have different noise levels, which leads to unsatisfactory test results. In addition, collecting many medical images for training denoising models consumes many material resources. To address these issues, we formulate a progressive denoising architecture that contains preliminary and profound denoising. First, we construct a noise level estimation network to estimate the noise level via selfsupervised learning and perform preliminary denoising with a dilated blind-spot network. Second, with the learned noise distribution, we synthesize noisy natural images to construct clean-noisy natural image pairs. Finally, we design a novel medical image denoising model for profound denoising by training these pairs. The proposed three-stage learning scheme and progressive denoising architecture not only solve the problem that the denoising model only adapts to a single noise level but also alleviate the lack of medical image pairs. Moreover, we integrate dense attention and sparse attention to constitute the retractable transformer module in the profound denoising model, which reconciles a wider receptive field and enhances the representation ability of the transformer, s allowing the denoising model to obtain retractable attention on the input feature and capture more local and global receptive fields simultaneously. The results of qualitative and quantitative experiments demonstrate the effectiveness of our method in removing noise at various levels.
C1 [Bai, Can] Anhui Univ Chinese Med, Coll Acupuncture & Massage, Hefei, Peoples R China.
   [Han, Xianjun] Anhui Univ, Sch Comp Sci & Technol, Hefei, Peoples R China.
C3 Anhui University of Chinese Medicine; Anhui University
RP Han, XJ (corresponding author), Anhui Univ, Sch Comp Sci & Technol, Hefei, Peoples R China.
EM BaiCan@Ahtcm.edu.cn; hxj@ahu.edu.cn
FU National Natural Science Foundation of China [62106005]
FX This work was supported by the National Natural Science Foundation of
   China (62106005) .
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Batson J, 2019, PR MACH LEARN RES, V97
   Bhadauria HS, 2013, COMPUT ELECTR ENG, V39, P1451, DOI 10.1016/j.compeleceng.2012.04.003
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Chen HY, 2023, Arxiv, DOI arXiv:2303.13132
   Chen LY, 2021, IEEE COMPUT SOC CONF, P182, DOI 10.1109/CVPRW53098.2021.00027
   Chung H, 2023, IEEE T MED IMAGING, V42, P922, DOI 10.1109/TMI.2022.3220681
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Dong XY, 2022, PROC CVPR IEEE, P12114, DOI 10.1109/CVPR52688.2022.01181
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Fan CM, 2022, Arxiv, DOI arXiv:2202.14009
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Guo S, 2019, PROC CVPR IEEE, P1712, DOI 10.1109/CVPR.2019.00181
   He HY, 2024, Arxiv, DOI arXiv:2111.11802
   Ho JAT, 2019, Arxiv, DOI arXiv:1912.12180
   Huang T, 2021, PROC CVPR IEEE, P14776, DOI 10.1109/CVPR46437.2021.01454
   Jiang YF, 2022, LECT NOTES COMPUT SC, V13678, P429, DOI 10.1007/978-3-031-19797-0_25
   Kang E, 2019, MED PHYS, V46, P550, DOI 10.1002/mp.13284
   Krull A, 2019, PROC CVPR IEEE, P2124, DOI 10.1109/CVPR.2019.00223
   Lee H, 2020, IEEE ACCESS, V8, P34686, DOI 10.1109/ACCESS.2020.2974001
   Lehtinen J, 2018, PR MACH LEARN RES, V80
   Li J., 2023, P IEEE CVF C COMPUTE
   Li M, 2020, IEEE T MED IMAGING, V39, P2289, DOI 10.1109/TMI.2020.2968472
   Li MY, 2023, Arxiv, DOI arXiv:2304.00844
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Liu KL, 2023, Arxiv, DOI arXiv:2304.06346
   Liu Z, 2022, PROC CVPR IEEE, P3192, DOI 10.1109/CVPR52688.2022.00320
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Luthra A., 2021, arXiv
   Mazandarani Farzan Niknejad, 2022, Annu Int Conf IEEE Eng Med Biol Soc, V2022, P3834, DOI 10.1109/EMBC48229.2022.9871380
   Moran Nick, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12061, DOI 10.1109/CVPR42600.2020.01208
   Pang TY, 2021, PROC CVPR IEEE, P2043, DOI 10.1109/CVPR46437.2021.00208
   Quan YH, 2020, PROC CVPR IEEE, P1887, DOI 10.1109/CVPR42600.2020.00196
   Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P1299, DOI 10.1109/TMI.2016.2535302
   Tang C, 2019, COMPUT MATH METHOD M, V2019, DOI 10.1155/2019/8639825
   Tian CW, 2020, NEURAL NETWORKS, V124, P117, DOI 10.1016/j.neunet.2019.12.024
   Tian CW, 2019, CAAI T INTELL TECHNO, V4, P17, DOI 10.1049/trit.2018.1054
   van den Oord A, 2016, PR MACH LEARN RES, V48
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wang ZD, 2022, PROC CVPR IEEE, P17662, DOI 10.1109/CVPR52688.2022.01716
   Xia Z., 2022, P IEEE CVF C COMP VI, P4794, DOI 10.1109/cvpr52688.2022.00475
   Xiaohe Wu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P352, DOI 10.1007/978-3-030-58548-8_21
   Xie M., 2023, arXiv
   Xing W., 2022, arXiv
   Yang C, 2023, Arxiv, DOI arXiv:2305.04457
   Yuan L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P538, DOI 10.1109/ICCV48922.2021.00060
   Zhang D, 2023, Arxiv, DOI arXiv:2304.01598
   Zhang J., 2023, arXiv
   Zhang J., 2022, Accurate Image Restoration with Attention Retractable Transformer
   Zhang K, 2022, IEEE T PATTERN ANAL, V44, P6360, DOI 10.1109/TPAMI.2021.3088914
   Zhang K, 2023, Arxiv, DOI arXiv:2203.13278
   Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891
   Zhang LP, 2022, MED PHYS, V49, P343, DOI 10.1002/mp.15368
   Zhang ZC, 2021, LECT NOTES COMPUT SC, V12906, P55, DOI 10.1007/978-3-030-87231-1_6
NR 54
TC 0
Z9 0
U1 6
U2 6
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD APR
PY 2024
VL 144
AR 104974
DI 10.1016/j.imavis.2024.104974
EA MAR 2024
PG 20
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA OJ2H1
UT WOS:001206832400001
DA 2024-08-05
ER

PT J
AU Han, XJ
   Li, TT
   Bai, C
   Yang, HY
AF Han, Xianjun
   Li, Tiantian
   Bai, Can
   Yang, Hongyu
TI Integrating prior knowledge into a bibranch pyramid network for medical
   image segmentation
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Image pyramid; Medical image segmentation; Prior knowledge; Medical
   image processing
ID TRANSFORMER
AB Medical image segmentation is crucial for obtaining accurate diagnoses, and while convolutional neural network (CNN) -based methods have made strides in recent years, they struggle with modeling long-range dependencies. Transformer -based methods improve this task but require more computational resources. The segment anything model (SAM) can generate pixel -level segmentation results for natural images using sparse manual prompts, but it performs poorly on low -contrast, noisy ultrasound images. To address this issue, we propose a new medical image segmentation network architecture that integrates transformer components, CNN modules, and an SAM encoder into a unified framework. This allows us to simultaneously capture both long-range dependencies and local features. Additionally, we incorporate the image features extracted from the SAM model as prior knowledge to achieve further improved segmentation accuracy with limited training data. To reduce the imposed computational stress, we employ an axial attention mechanism to approximate a transformer's effects by expanding the receptive field. Instead of replacing the transformer components with lightweight attention modules, our model is divided into a global branch and a local branch. The global branch extracts context features with the transformer components, while the local branch processes patch tokens with the axial attention mechanism. We also construct an image pyramid to excavate internal statistics and multiscale representations to obtain more accurate segmentation regions. This bibranch pyramid transformer (Bi-BPT) architecture is effective and robust for medical image segmentation, surpassing other related segmentation network architectures. The experimental results obtained on various medical image datasets demonstrate its effectiveness.
C1 [Han, Xianjun; Li, Tiantian] Anhui Univ, Sch Comp Sci & Technol, Hefei, Peoples R China.
   [Bai, Can] Anhui Univ Chinese Med, Coll Acupuncture & Massage, Hefei, Peoples R China.
   [Yang, Hongyu] Sichuan Univ, Coll Comp Sci, Chengdu, Peoples R China.
C3 Anhui University; Anhui University of Chinese Medicine; Sichuan
   University
RP Bai, C (corresponding author), Anhui Univ Chinese Med, Coll Acupuncture & Massage, Hefei, Peoples R China.
EM hxj@ahu.edu.cn; E22201091@stu.ahu.edu.cn; BaiCan@Ahtcm.edu.cn;
   yanghongyu@scu.edu.cn
FU National Natural Science Foundation of China [62106005]
FX The authors declare the following financial interests/personal re-
   lationships which may be considered as potential competing interests:
   Xianjun Han reports financial support was provided by National Natural
   Science Foundation of China (62106005) .
CR Antonelli M, 2021, Arxiv, DOI arXiv:2106.05735
   Azad R., 2023, arXiv
   Badrinarayanan V, 2016, Arxiv, DOI [arXiv:1511.00561, DOI 10.1109/TPAMI.2016.2644615]
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen J., 2021, TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation
   Chen L.-C., 2018, ECCV, P801, DOI [DOI 10.1007/978-3-030-01234-249, 10.1007/978-3-030-01234-2_49]
   Chen LC, 2017, Arxiv, DOI [arXiv:1706.05587, 10.48550/arXiv.1706.05587, DOI 10.48550/ARXIV.1706.05587]
   Codella NCF, 2018, I S BIOMED IMAGING, P168, DOI 10.1109/ISBI.2018.8363547
   Dalmaz O, 2022, Arxiv, DOI [arXiv:2106.16031, 10.48550/arXiv.2106.16031]
   Deng RN, 2023, Arxiv, DOI arXiv:2304.04155
   Dey D, 2008, IEEE T DIELECT EL IN, V15, P1297, DOI 10.1109/TDEI.2008.4656237
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Feng SL, 2020, IEEE T MED IMAGING, V39, P3008, DOI 10.1109/TMI.2020.2983721
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Gao YH, 2021, LECT NOTES COMPUT SC, V12903, P61, DOI 10.1007/978-3-030-87199-4_6
   Ghiasi G, 2016, LECT NOTES COMPUT SC, V9907, P519, DOI 10.1007/978-3-319-46487-9_32
   He S, 2023, Arxiv, DOI [arXiv:2304.09324, DOI 10.48550/ARXIV.2304.09324]
   Ho JAT, 2019, Arxiv, DOI arXiv:1912.12180
   Hu CF, 2023, Arxiv, DOI arXiv:2304.08506
   Huang Y., 2022, Channelized Axial Attention-Considering Channel Relation within Spatial Attention for Semantic Segmentation
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Huiyu Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P108, DOI 10.1007/978-3-030-58548-8_7
   Ji YF, 2021, LECT NOTES COMPUT SC, V12901, P326, DOI 10.1007/978-3-030-87193-2_31
   Jiao LC, 2019, IEEE ACCESS, V7, P128837, DOI 10.1109/ACCESS.2019.2939201
   Kingma D. P., 2014, arXiv
   Kirillov A, 2023, Arxiv, DOI arXiv:2304.02643
   Kumar N, 2020, IEEE T MED IMAGING, V39, P1380, DOI 10.1109/TMI.2019.2947628
   Kumar N, 2017, IEEE T MED IMAGING, V36, P1550, DOI 10.1109/TMI.2017.2677499
   Li JY, 2022, PHYS MED BIOL, V67, DOI 10.1088/1361-6560/ac628a
   Li XM, 2018, IEEE T MED IMAGING, V37, P2663, DOI 10.1109/TMI.2018.2845918
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu YH, 2024, Arxiv, DOI arXiv:2304.05622
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu ZM, 2020, IEEE COMPUT SOC CONF, P4422, DOI 10.1109/CVPRW50498.2020.00521
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Ma J, 2024, Arxiv, DOI [arXiv:2304.12306, DOI 10.48550/ARXIV.2304.12306]
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Patil A, 2021, I S BIOMED IMAGING, P1563, DOI 10.1109/ISBI48211.2021.9434121
   Rahman T, 2021, COMPUT BIOL MED, V132, DOI 10.1016/j.compbiomed.2021.104319
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Roy S, 2023, Arxiv, DOI arXiv:2304.05396
   Sirinukunwattana K, 2017, MED IMAGE ANAL, V35, P489, DOI 10.1016/j.media.2016.08.008
   Srivastava A, 2022, Arxiv, DOI arXiv:2105.07451
   Tolstikhin I, 2021, ADV NEUR IN, V34
   Valanarasu JMJ, 2021, LECT NOTES COMPUT SC, V12901, P36, DOI 10.1007/978-3-030-87193-2_4
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Xiao X, 2018, 2018 NINTH INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY IN MEDICINE AND EDUCATION (ITME 2018), P327, DOI 10.1109/ITME.2018.00080
   Xie YT, 2021, LECT NOTES COMPUT SC, V12903, P171, DOI 10.1007/978-3-030-87199-4_16
   Xu GP, 2024, LECT NOTES COMPUT SC, V14432, P42, DOI 10.1007/978-981-99-8543-2_4
   Xu KL, 2016, Arxiv, DOI [arXiv:1502.03044, DOI 10.48550/ARXIV.1502.03044]
   Xu ZY, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13010071
   You CY, 2022, Arxiv, DOI arXiv:2201.10737
   Zhang K, 2023, Arxiv, DOI arXiv:2203.13278
   Zhang KD, 2023, Arxiv, DOI arXiv:2304.13785
   Zhang YL, 2021, LECT NOTES COMPUT SC, V12901, P99, DOI 10.1007/978-3-030-87193-2_10
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhou H.-Y., 2021, arXiv
   Zhou ZW, 2020, IEEE T MED IMAGING, V39, P1856, DOI 10.1109/TMI.2019.2959609
   Zhou ZW, 2018, LECT NOTES COMPUT SC, V11045, P3, DOI 10.1007/978-3-030-00889-5_1
   Zhu XZ, 2019, IEEE I CONF COMP VIS, P6687, DOI 10.1109/ICCV.2019.00679
   Zhu Z, 2019, IEEE I CONF COMP VIS, P593, DOI 10.1109/ICCV.2019.00068
NR 62
TC 0
Z9 0
U1 14
U2 14
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD MAR
PY 2024
VL 143
AR 104945
DI 10.1016/j.imavis.2024.104945
EA FEB 2024
PG 15
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA NQ3X4
UT WOS:001201891000001
DA 2024-08-05
ER

PT J
AU Wang, YY
   Mao, J
   Zou, C
   Kong, XY
AF Wang, Yunyun
   Mao, Jian
   Zou, Cong
   Kong, Xinyang
TI Universal domain adaptation from multiple black-box sources
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Unsupervised domain adaptation; Universal domain adaptation; Multiple
   black-box sources; Domain attention; Class attention
AB Black-box domain adaptation (B2DA) is a practical unsupervised domain adaptation (UDA) setting, in which only an interface to source model is available due to privacy or security concerns. It aims to transfer source knowledge with no source data or model, but only queries for target samples from the source interface. Previous B2DA methods commonly assume a single source interface and shared label space across domains. Whereas in real tasks, there are usually multiple source interfaces with label shift from the target domain, how to effectively combine them without target ground-truth has not been fully investigated yet so far. We term such setting as Universal Domain Adaptation from Multiple Black-box sources (Uni-MBDA), and propose a new Universal multisource Black-Box method (Um2B) through Dual-Attention on both source domains and classes. In Um2B, adaptive domain attention is first introduced to seek the optimal combination of source interfaces, whose performance is no worse than the single best one. Second, to address the label shift across domains, adaptive class attention is adopted in adaptation to suppress the private source classes, then automatically identify target unknowns and separate them apart from the known classes. Empirical results over benchmark datasets show the effectiveness of Um2B, and in most cases, it performs no worse than the best single-source.
C1 [Wang, Yunyun; Mao, Jian; Zou, Cong; Kong, Xinyang] Nanjing Univ Posts & Telecommun, Sch Comp Sci & Engn, Nanjing 210046, Peoples R China.
   [Wang, Yunyun; Mao, Jian; Zou, Cong; Kong, Xinyang] Jiangsu Key Lab Big Data Secur & Intelligent Proc, Nanjing 210046, Peoples R China.
C3 Nanjing University of Posts & Telecommunications
RP Wang, YY (corresponding author), Nanjing Univ Posts & Telecommun, Sch Comp Sci & Engn, Nanjing 210046, Peoples R China.
EM wangyunyun@njupt.edu.cn
CR Ahmed SM, 2021, PROC CVPR IEEE, P10098, DOI 10.1109/CVPR46437.2021.00997
   Bo Fu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P567, DOI 10.1007/978-3-030-58555-6_34
   Cai ZY, 2022, KNOWL-BASED SYST, V254, DOI 10.1016/j.knosys.2022.109632
   Chen WJ, 2021, Arxiv, DOI arXiv:2102.11614
   Chu T, 2022, AAAI CONF ARTIF INTE, P472
   Deng B, 2021, Arxiv, DOI arXiv:2104.04665
   Fang YQ, 2023, Arxiv, DOI arXiv:2301.00265
   Ganin Y, 2016, J MACH LEARN RES, V17
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Guo J, 2018, Arxiv, DOI arXiv:1809.02256
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ishii M., 2021, Source-free domain adaptation via distributional alignment by matching batch normalization statistics
   Karim N, 2023, PROC CVPR IEEE, P24120, DOI 10.1109/CVPR52729.2023.02310
   Lee CY, 2019, PROC CVPR IEEE, P10277, DOI 10.1109/CVPR.2019.01053
   Li R., 2020, P IEEECVF C COMPUTER
   Li Rui, 2020, P IEEE CVF C COMP VI, P9641
   Liang J., 2020, International Conference on Machine Learning, P6028
   Liang J., 2021, arXiv
   Liang J, 2023, Arxiv, DOI arXiv:2303.15361
   Liang J, 2022, IEEE T PATTERN ANAL, V44, P8602, DOI 10.1109/TPAMI.2021.3103390
   Liu Hong, 2021, arXiv
   Long MS, 2017, PR MACH LEARN RES, V70
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Long MS, 2013, IEEE I CONF COMP VIS, P2200, DOI 10.1109/ICCV.2013.274
   Saito K., Adv. Neural Inf. Proces. Syst., V33
   Saito K, 2018, PROC CVPR IEEE, P3723, DOI 10.1109/CVPR.2018.00392
   Saito K, 2018, LECT NOTES COMPUT SC, V11209, P156, DOI 10.1007/978-3-030-01228-1_10
   Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35
   Wu, 2021, ARXIV
   Wu B, 2021, Arxiv, DOI arXiv:2010.12751
   Xie JY, 2016, PR MACH LEARN RES, V48
   Yang SQ, 2023, Arxiv, DOI arXiv:2010.12427
   Yin YM, 2022, PATTERN RECOGN, V121, DOI 10.1016/j.patcog.2021.108238
   You KC, 2019, PROC CVPR IEEE, P2715, DOI 10.1109/CVPR.2019.00283
   Zari O., 2021, NEURIPS PRIML WORKSH
   Zhang HJ, 2021, Arxiv, DOI arXiv:2101.02839
   Zhang JY, 2023, Arxiv, DOI arXiv:2308.13236
   Zhu YC, 2019, AAAI CONF ARTIF INTE, P5989
NR 38
TC 1
Z9 1
U1 4
U2 4
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD FEB
PY 2024
VL 142
AR 104896
DI 10.1016/j.imavis.2023.104896
EA JAN 2024
PG 9
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA GN4K2
UT WOS:001153333400001
DA 2024-08-05
ER

PT J
AU Su, K
   Tomioka, Y
   Zhao, QF
   Liu, Y
AF Su, Kai
   Tomioka, Yoichi
   Zhao, Qiangfu
   Liu, Yong
TI YOLIC: An efficient method for object localization and classification on
   edge devices
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Object localization and classification; Cell-wise segmentation;
   Real-time detection; Tiny AI
ID TRACKING
AB In the realm of Tiny AI, we introduce "You Only Look at Interested Cells " (YOLIC), an efficient method for object localization and classification on edge devices. Through seamlessly blending the strengths of semantic segmentation and object detection, YOLIC provides improved computational efficiency and precision compared to traditional methods. By adopting Cells of Interest for classification instead of individual pixels, YOLIC encapsulates relevant information, reduces computational load, and enables rough object shape inference. Importantly, the need for bounding box regression is obviated, as YOLIC capitalizes on the predetermined cell configuration that provides information about potential object location, size, and shape. To tackle the issue of single -label classification limitations, a multi -label classification approach is applied to each cell for effectively recognizing overlapping or closely situated objects. This paper presents extensive experiments on multiple datasets to demonstrate that YOLIC achieves detection performance comparable to the state-of-the-art YOLO algorithms while surpassing in speed, exceeding 30fps on a Raspberry Pi 4B CPU.
C1 [Su, Kai; Tomioka, Yoichi; Zhao, Qiangfu; Liu, Yong] Univ Aizu, Grad Sch Comp Sci & Engn, Aizu Wakamatsu, Japan.
C3 University of Aizu
RP Su, K (corresponding author), Univ Aizu, Grad Sch Comp Sci & Engn, Aizu Wakamatsu, Japan.
EM d8232114@u-aizu.ac.jp
CR [Anonymous], 2024, Google Recaptcha
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Chang XP, 2021, IEEE T NEUR NET LEAR, V32, P5323, DOI 10.1109/TNNLS.2021.3056383
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Ge Z, 2021, Arxiv, DOI arXiv:2107.08430
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Gong T, 2019, NEUROCOMPUTING, V370, P174, DOI 10.1016/j.neucom.2019.08.089
   Guo SY, 2022, IEEE INTERNET THINGS, V9, P20382, DOI 10.1109/JIOT.2022.3173685
   He YH, 2017, IEEE I CONF COMP VIS, P1398, DOI 10.1109/ICCV.2017.155
   Li RD, 2019, PROC CVPR IEEE, P2805, DOI 10.1109/CVPR.2019.00292
   Lin Y, 2020, IEEE T VEH TECHNOL, V69, P5703, DOI 10.1109/TVT.2020.2983143
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu X, 2022, IEEE T COMPUT SOC SY, V9, P252, DOI 10.1109/TCSS.2021.3059318
   Liu Z, 2019, Arxiv, DOI [arXiv:1810.05270, DOI 10.48550/ARXIV.1810.05270]
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Puchtler Pascal, 2020, KI 2020: Advances in Artificial Intelligence. 43rd German Conference on AI. Proceedings. Lecture Notes in Artificial Intelligence Subseries of Lecture Notes in Computer Science (LNAI 12325), P320, DOI 10.1007/978-3-030-58285-2_29
   Qi JT, 2022, COMPUT ELECTRON AGR, V194, DOI 10.1016/j.compag.2022.106780
   RangiLyu, 2021, NanoDet-Plus: Super fast and high accuracy lightweight anchorfree object detection model
   Redmon J., 2018, CoRR
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Ruiz-Beltrán CA, 2023, ELECTRONICS-SWITZ, V12, DOI 10.3390/electronics12224713
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Su K., 2023, Cell Designer Tool
   Su K., 2023, Image Annotation Tool
   Su K, 2020, INT CONF AWARE SCI, DOI 10.1109/ICAST51195.2020.9319469
   Su K, 2020, 2020 IEEE INTL CONF ON DEPENDABLE, AUTONOMIC AND SECURE COMPUTING, INTL CONF ON PERVASIVE INTELLIGENCE AND COMPUTING, INTL CONF ON CLOUD AND BIG DATA COMPUTING, INTL CONF ON CYBER SCIENCE AND TECHNOLOGY CONGRESS (DASC/PICOM/CBDCOM/CYBERSCITECH), P110, DOI 10.1109/DASC-PICom-CBDCom-CyberSciTech49142.2020.00032
   Tan R., 2020, P IEEE CVF C COMP VI, DOI [10.1109/CVPR42600.2020.01079, DOI 10.1109/CVPR42600.2020.01079]
   Tsoumakas G., 2007, International Journal of Data Warehousing and Mining, V3, P1, DOI [10.4018/jdwm.2007070101, DOI 10.4018/JDWM.2007070101]
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Wang L, 2022, IEEE T PATTERN ANAL, V44, P3048, DOI 10.1109/TPAMI.2021.3055564
   Yang JW, 2019, PROC CVPR IEEE, P7300, DOI 10.1109/CVPR.2019.00748
   Yu G, 2021, arXiv
   Zhou WJ, 2023, IEEE T IMAGE PROCESS, V32, P1329, DOI 10.1109/TIP.2023.3242775
NR 36
TC 0
Z9 0
U1 2
U2 2
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUL
PY 2024
VL 147
AR 105095
DI 10.1016/j.imavis.2024.105095
EA MAY 2024
PG 13
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA UH1H9
UT WOS:001247069800001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Attallah, O
AF Attallah, Omneya
TI Acute lymphocytic leukemia detection and subtype classification via
   extended wavelet pooling based-CNNs and statistical-texture features
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Acute lymphocytic leukemia (ALL); Feature fusion; Gray level
   cooccurrence matrix (GLCM); Local binary pattern (LBP); Handcrafted
   features; Transfer learning; ReleifF feature selection (FS)
ID FEATURE-SELECTION; BLOOD SMEAR
AB Acute lymphoblastic leukemia (ALL) is considered the most fatal form of leukemia (also known as blood cancer). It propagates quickly among adults and children and could lead to their death. Early detection of ALL and ALL subtypes is the key factor in selecting effective treatment types and improving survival rates. However, routine diagnostic approaches have several drawbacks. Computer-assisted diagnosis (CAD) is the perfect solution to avoid these challenges and achieve a fast and accurate diagnosis. Current CAD models require enhancement/ segmentation processing. Besides, they are either dependent on deep learning (DL) models or handcrafted features along with machine learning. Those CADs that employed DL approaches relied solely on spatial information during the training procedure. However, learning them with spectral temporal and temporal representations could improve performance. Furthermore, integrating deep features from DL models along with handcrafted features can increase the discrimination ability of attributes in medical image classification. This study aims to propose a novel CAD for ALL detection and subtype classification without pre-segmentation or enhancement steps. The proposed CAD extends the conventional DL models of convolutional neural networks by introducing an additional wavelet pooling, accompanied by a dense layer or a long-short-term memory (LSTM) layer, and then a SoftMax layer, acquiring spectral-temporal information along with temporal information. To further improve the framework's ability to discriminate, the introduced CAD then combines the wavelet-based deep features of every CNN with numerous handcrafted attributes. Afterward, a feature selection methodology is utilized to create a model with limited features and improved accuracy. The performance results show that the novel CAD is capable of achieving 100% ALL detection accuracy, as well as 100% ALL-subtype classification accuracy with just 88 and 146 features. Thus, this CAD can be employed to assist pathologists in the rapid and precise ALL identification and subcategories recognition.
C1 [Attallah, Omneya] Arab Acad Sci Technol & Maritime Transport, Coll Engn & Technol, Dept Elect & Commun Engn, Alexandria 21937, Egypt.
   [Attallah, Omneya] Arab Acad Sci Technol & Maritime Transport, Wearables Biosensing & Biosignal Proc Lab, Alexandria 21937, Egypt.
C3 Egyptian Knowledge Bank (EKB); Arab Academy for Science, Technology &
   Maritime Transport; Egyptian Knowledge Bank (EKB); Arab Academy for
   Science, Technology & Maritime Transport
RP Attallah, O (corresponding author), Arab Acad Sci Technol & Maritime Transport, Coll Engn & Technol, Dept Elect & Commun Engn, Alexandria 21937, Egypt.
EM o.attallah@aast.edu
CR Abhishek A, 2022, BIOMED SIGNAL PROCES, V72, DOI 10.1016/j.bspc.2021.103341
   Ahmed IA, 2023, DIAGNOSTICS, V13, DOI 10.3390/diagnostics13061026
   Al-Amleh EK, 2022, J CLIN MED, V11, DOI 10.3390/jcm11174954
   Alijamaat A., 2021, J. AI Data Mining, V9, P161
   Anwar SM, 2018, J MED SYST, V42, DOI 10.1007/s10916-018-1088-1
   Attallah O, 2023, EXPERT SYST APPL, V229, DOI 10.1016/j.eswa.2023.120624
   Attallah O, 2023, DIGIT HEALTH, V9, DOI 10.1177/20552076231180054
   Attallah O, 2023, APPL SCI-BASEL, V13, DOI 10.3390/app13031916
   Attallah O, 2023, DIAGNOSTICS, V13, DOI 10.3390/diagnostics13020171
   Attallah O, 2023, CHEMOMETR INTELL LAB, V233, DOI 10.1016/j.chemolab.2022.104750
   Attallah O, 2022, DIGIT HEALTH, V8, DOI 10.1177/20552076221124432
   Attallah O, 2022, APPL SOFT COMPUT, V128, DOI 10.1016/j.asoc.2022.109401
   Attallah O, 2022, BIOSENSORS-BASEL, V12, DOI 10.3390/bios12050299
   Attallah O, 2022, DIGIT HEALTH, V8, DOI 10.1177/20552076221092543
   Attallah O, 2022, LIFE-BASEL, V12, DOI 10.3390/life12020232
   Attallah O, 2022, COMPUT BIOL MED, V142, DOI 10.1016/j.compbiomed.2022.105210
   Attallah O, 2021, DIAGNOSTICS, V11, DOI 10.3390/diagnostics11112034
   Attallah O, 2021, CONTRAST MEDIA MOL I, V2021, DOI 10.1155/2021/7192016
   Attallah O, 2021, FRONT NEUROINFORM, V15, DOI 10.3389/fninf.2021.663592
   Attallah O, 2021, PEERJ COMPUT SCI, DOI 10.7717/peerj-cs.493
   Attallah O, 2021, DIAGNOSTICS, V11, DOI 10.3390/diagnostics11020359
   Atteia G, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22155520
   Baig R, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12136317
   Billah ME, 2022, APPL ARTIF INTELL, V36, DOI 10.1080/08839514.2021.2011688
   Billing R.J., 1980, Implicat. Normal Lymphoid Different., V56, P1120
   Bukhari M, 2022, MATH PROBL ENG, V2022, DOI 10.1155/2022/2801227
   Burger W., 2009, Principles of Digital Image Processing, V111
   Cai J, 2018, NEUROCOMPUTING, V300, P70, DOI 10.1016/j.neucom.2017.11.077
   Chand S, 2022, MULTIMED TOOLS APPL, V81, P37243, DOI 10.1007/s11042-022-13543-2
   Chelali M, 2021, COMPUT VIS IMAGE UND, V208, DOI 10.1016/j.cviu.2021.103221
   Das P.K., 2020, P 2020 IEEE HYDCON I, P1
   Das PK, 2022, IEEE ACCESS, V10, P81741, DOI 10.1109/ACCESS.2022.3196037
   Das PK, 2022, MEASUREMENT, V191, DOI 10.1016/j.measurement.2022.110762
   Das PK, 2021, EXPERT SYST APPL, V183, DOI 10.1016/j.eswa.2021.115311
   Dores GM, 2012, BLOOD, V119, P34, DOI 10.1182/blood-2011-04-347872
   Fathi E, 2020, P I MECH ENG H, V234, P1051, DOI 10.1177/0954411920938567
   Howard AG, 2017, Arxiv, DOI [arXiv:1704.04861, 10.48550/arXiv.1704.04861]
   Ghaderzadeh M, 2022, SCI PROGRAMMING-NETH, V2022, DOI 10.1155/2022/4801671
   Ghaderzadeh M, 2022, INT J INTELL SYST, V37, P5113, DOI 10.1002/int.22753
   Goceri Evgin, 2020, 2020 IEEE 4th International Conference on Image Processing, Applications and Systems (IPAS), P144, DOI 10.1109/IPAS50080.2020.9334937
   Goceri Evgin, 2020, 2020 IEEE 4th International Conference on Image Processing, Applications and Systems (IPAS), P138, DOI 10.1109/IPAS50080.2020.9334956
   Goceri Evgin, 2017, International Conferences on Computer Graphics, Visualization, Computer, Vision and image Processing 2017 and Big Data Analytics, Data Mining and Computational Intelligence 2017. Proceedings, P300
   Goceri E., 2018, Celal Bayar niversitesi Fen Bilimleri Dergisi, V14, P125, DOI DOI 10.18466/CBAYARFBE.384729
   Goceri E, 2020, 14 INT C COMP GRAPH, P1
   Goceri E., 2021, INT C COMP GRAPH VIS, P29, DOI DOI 10.33965/MCCSIS2021_202107L004
   GOCERI E, 2021, IZMIR KATIP CELEBI U, V6, P91
   Goceri E., 2018, P INT C ADV TECHN AN
   Goceri E., 2018, P INT C APPL AN MATH, P156
   Goceri E, 2024, J IMAGING INFORM MED, V37, P851, DOI 10.1007/s10278-023-00954-2
   Goceri E, 2024, EXPERT SYST APPL, V241, DOI 10.1016/j.eswa.2023.122672
   Goceri E, 2023, BIOMED SIGNAL PROCES, V85, DOI 10.1016/j.bspc.2023.104949
   Goceri E, 2023, INT J IMAG SYST TECH, V33, P1727, DOI 10.1002/ima.22890
   Goceri E, 2023, ARTIF INTELL REV, V56, P12561, DOI 10.1007/s10462-023-10453-z
   Goceri E, 2023, COMPUT BIOL MED, V152, DOI 10.1016/j.compbiomed.2022.106474
   Goceri E, 2021, 2021 44TH INTERNATIONAL CONFERENCE ON TELECOMMUNICATIONS AND SIGNAL PROCESSING (TSP), P48, DOI 10.1109/TSP52935.2021.9522605
   Göçeri E, 2020, INT CONF IMAG PROC, DOI 10.1109/ipta50016.2020.9286706
   Gupta R, 2022, MED ENG PHYS, V103, DOI 10.1016/j.medengphy.2022.103793
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jha KK, 2019, COMPUT METH PROG BIO, V179, DOI 10.1016/j.cmpb.2019.104987
   Labati RD, 2011, IEEE IMAGE PROC
   Laosai J, 2018, BIOMED SIGNAL PROCES, V44, P127, DOI 10.1016/j.bspc.2018.01.020
   Li XC, 2019, IEEE ACCESS, V7, P33771, DOI 10.1109/ACCESS.2019.2891975
   Liu D., 2019, Proceedings of the Journal of Physics: Conference Series, V1345
   Lu J, 2015, KNOWL-BASED SYST, V80, P14, DOI 10.1016/j.knosys.2015.01.010
   Matek C, 2019, NAT MACH INTELL, V1, P538, DOI 10.1038/s42256-019-0101-9
   Mishra S, 2019, BIOMED SIGNAL PROCES, V47, P303, DOI 10.1016/j.bspc.2018.08.012
   Mishra S, 2017, BIOMED SIGNAL PROCES, V33, P272, DOI 10.1016/j.bspc.2016.11.021
   Moshavash Z, 2018, J DIGIT IMAGING, V31, P702, DOI 10.1007/s10278-018-0074-y
   Mou LC, 2019, IEEE T GEOSCI REMOTE, V57, P924, DOI 10.1109/TGRS.2018.2863224
   Pansombut T, 2019, COMPUT INTEL NEUROSC, V2019, DOI 10.1155/2019/7519603
   Pietikainen M, 2011, COMPUT IMAGING VIS, V40, P1
   Raab D, 2023, NEURAL COMPUT APPL, V35, P10051, DOI 10.1007/s00521-022-07809-x
   Rastogi P, 2022, COMPUT BIOL MED, V142, DOI 10.1016/j.compbiomed.2022.105236
   Rezayi S, 2021, COMPUT INTEL NEUROSC, V2021, DOI 10.1155/2021/5478157
   Sallam NM, 2023, ALEX ENG J, V68, P39, DOI 10.1016/j.aej.2023.01.004
   Sallam NM, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app122110760
   Sampathila N, 2022, HEALTHCARE-BASEL, V10, DOI 10.3390/healthcare10101812
   Shafique S, 2018, TECHNOL CANCER RES T, V17, DOI 10.1177/1533033818802789
   Siegel RL, 2023, CA-CANCER J CLIN, V73, P17, DOI 10.3322/caac.21763
   Terwilliger T, 2017, BLOOD CANCER J, V7, DOI 10.1038/bcj.2017.53
   Thanh T. T. P., 2018, International Journal of Computer Theory and Engineering, V10, P54, DOI 10.7763/IJCTE.2018.V10.1198
   Tuncer T, 2020, CHEMOMETR INTELL LAB, V203, DOI 10.1016/j.chemolab.2020.104054
   Tusar MTHK, 2022, Arxiv, DOI arXiv:2208.08992
   Urbanowicz RJ, 2018, J BIOMED INFORM, V85, P189, DOI 10.1016/j.jbi.2018.07.014
   WHO Global Cancer Burden Growing, 2024, amidst Mounting Need for Services
   Zhang JP, 2018, IEEE J BIOMED HEALTH, V22, P1521, DOI 10.1109/JBHI.2017.2775662
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
NR 87
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUL
PY 2024
VL 147
AR 105064
DI 10.1016/j.imavis.2024.105064
EA MAY 2024
PG 19
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA TH7L7
UT WOS:001240440100001
DA 2024-08-05
ER

PT J
AU Cheng, X
   Deng, SY
   Yu, H
AF Cheng, Xu
   Deng, Shuya
   Yu, Hao
TI Exploring modality enhancement and compensation spaces for
   visible-infrared person re-identification
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Visible -infrared person re -identification; Modality enhancement;
   Modality compensation
AB Visible-infrared person re-identification (VI-ReID) is a challenging task in computer vision due to the substantial modality gaps between visible and infrared images. The currently existing approaches can improve performance by addressing cross-modality discrepancies, but they often fail to generate compensation features that fully utilize the unique information present in each modality. Additionally, these methods mainly focus on pixel-level fusion of images, disregarding the challenge of modality misalignment. To address these issues, we propose a novel visible-infrared person re-identification method that explores modality enhancement and compensation spaces to extract more discriminative modality information. Furthermore, we introduce a modality mutual guidance strategy incorporating identity information mutual learning loss and modality-guided alignment loss, which can effectively leverage learned identity-related feature to guide alignment between visible and infrared modalities. Extensive experiments on public datasets demonstrate the significant superiority of our proposed method over existing state-of-the-art approaches.
C1 [Cheng, Xu; Deng, Shuya; Yu, Hao] Nanjing Univ Informat Sci & Technol, Sch Comp Sci, Nanjing 210044, Peoples R China.
   [Cheng, Xu; Deng, Shuya; Yu, Hao] Nanjing Univ Informat Sci & Technol, Engn Res Ctr Digital Forens, Minist Educ, Nanjing 210044, Peoples R China.
C3 Nanjing University of Information Science & Technology; Nanjing
   University of Information Science & Technology
RP Deng, SY (corresponding author), Nanjing Univ Informat Sci & Technol, Sch Comp Sci, Nanjing 210044, Peoples R China.; Deng, SY (corresponding author), Nanjing Univ Informat Sci & Technol, Engn Res Ctr Digital Forens, Minist Educ, Nanjing 210044, Peoples R China.
EM xcheng@nuist.edu.cn; 20211249484@nuist.edu.cn; yuhao@nuist.edu.cn
FU National Natural Science Foundation of China [61802058, 61911530397];
   China Postdoctoral Science Foundation [2019M651650]; China Scholarship
   Council (CSC) [201908320175]
FX This research is funded in part by the National Natural Science
   Foundation of China (Grant No. 61802058, 61911530397) , in part by the
   China Postdoctoral Science Foundation (Grant No. 2019M651650) and in
   part by the China Scholarship Council (CSC) (Grant No. 201908320175) .
CR Chen CQ, 2022, IEEE T IMAGE PROCESS, V31, P2352, DOI 10.1109/TIP.2022.3141868
   Chen M., 2021, arXiv
   Chen Y, 2022, IMAGE VISION COMPUT, V128, DOI 10.1016/j.imavis.2022.104587
   Dai G., 2022, P AS C COMP VIS ACCV, P1142
   Dai PY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P677
   Fan X., 2022, Vis. Comput., P1
   Feng JW, 2023, PROC CVPR IEEE, P22752, DOI 10.1109/CVPR52729.2023.02179
   Feng ZX, 2020, IEEE T IMAGE PROCESS, V29, P579, DOI 10.1109/TIP.2019.2928126
   Gao YJ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5257, DOI 10.1145/3474085.3475643
   Hou QB, 2021, PROC CVPR IEEE, P13708, DOI 10.1109/CVPR46437.2021.01350
   Jambigi R., 2021, arXiv
   Kim M, 2023, PROC CVPR IEEE, P18621, DOI 10.1109/CVPR52729.2023.01786
   Kong J, 2021, IEEE SIGNAL PROC LET, V28, P2003, DOI 10.1109/LSP.2021.3115040
   Leng QM, 2020, IEEE T CIRC SYST VID, V30, P1092, DOI 10.1109/TCSVT.2019.2898940
   Li DG, 2020, AAAI CONF ARTIF INTE, V34, P4610
   Li XL, 2022, LECT NOTES COMPUT SC, V13686, P381, DOI 10.1007/978-3-031-19809-0_22
   Li YL, 2021, PROC CVPR IEEE, P2897, DOI 10.1109/CVPR46437.2021.00292
   Liu JN, 2022, IEEE T CIRC SYST VID, V32, P7226, DOI 10.1109/TCSVT.2022.3168999
   Lu H, 2023, AAAI CONF ARTIF INTE, P1835
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Mang Ye, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P229, DOI 10.1007/978-3-030-58520-4_14
   Ming ZQ, 2022, IMAGE VISION COMPUT, V119, DOI 10.1016/j.imavis.2022.104394
   Nguyen DT, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17030605
   Park H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12026, DOI 10.1109/ICCV48922.2021.01183
   Park H, 2020, AAAI CONF ARTIF INTE, V34, P11839
   Seokeon Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10254, DOI 10.1109/CVPR42600.2020.01027
   Somers V, 2023, IEEE WINT CONF APPL, P1613, DOI 10.1109/WACV56688.2023.00166
   Sun YF, 2020, PROC CVPR IEEE, P6397, DOI 10.1109/CVPR42600.2020.00643
   Wang GA, 2020, AAAI CONF ARTIF INTE, V34, P12144
   Wang GA, 2019, IEEE I CONF COMP VIS, P3622, DOI 10.1109/ICCV.2019.00372
   Wang GC, 2019, AAAI CONF ARTIF INTE, P8933
   Wang HC, 2022, PROC CVPR IEEE, P7287, DOI 10.1109/CVPR52688.2022.00715
   Wang ZX, 2019, PROC CVPR IEEE, P618, DOI 10.1109/CVPR.2019.00071
   Wei ZY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P225, DOI 10.1109/ICCV48922.2021.00029
   Wu AC, 2017, IEEE I CONF COMP VIS, P5390, DOI 10.1109/ICCV.2017.575
   Wu Q, 2021, PROC CVPR IEEE, P4328, DOI 10.1109/CVPR46437.2021.00431
   Xuan SY, 2021, PROC CVPR IEEE, P11921, DOI 10.1109/CVPR46437.2021.01175
   Yan Lu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13376, DOI 10.1109/CVPR42600.2020.01339
   Yang B, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P2843, DOI 10.1145/3503161.3548198
   Yang MX, 2022, PROC CVPR IEEE, P14288, DOI 10.1109/CVPR52688.2022.01391
   Ye M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13547, DOI 10.1109/ICCV48922.2021.01331
   Ye M, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1092
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Ye M, 2018, AAAI CONF ARTIF INTE, P7501
   Yi S, 2021, INFRARED PHYS TECHN, V119, DOI 10.1016/j.infrared.2021.103947
   Yu H., 2023, P IEEECVF INT C COMP, P11185
   Yu H, 2023, PROC CVPR IEEE, P3541, DOI 10.1109/CVPR52729.2023.00345
   Zhang P, 2021, IMAGE VISION COMPUT, V108, DOI 10.1016/j.imavis.2021.104118
   Zhang Q, 2022, PROC CVPR IEEE, P7339, DOI 10.1109/CVPR52688.2022.00720
   Zhang YY, 2022, LECT NOTES COMPUT SC, V13674, P462, DOI 10.1007/978-3-031-19781-9_27
   Zhang YK, 2023, PROC CVPR IEEE, P2153, DOI 10.1109/CVPR52729.2023.00214
   Zhang YK, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P788, DOI 10.1145/3474085.3475250
   Zhao JQ, 2023, IEEE T MULTIMEDIA, V25, P3668, DOI 10.1109/TMM.2022.3163847
   Zhao ZW, 2021, AAAI CONF ARTIF INTE, V35, P3520
   Zheng L, 2016, Arxiv, DOI arXiv:1610.02984
   Zheng L, 2017, PROC CVPR IEEE, P3346, DOI 10.1109/CVPR.2017.357
NR 56
TC 0
Z9 0
U1 7
U2 7
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUN
PY 2024
VL 146
AR 105040
DI 10.1016/j.imavis.2024.105040
EA MAY 2024
PG 11
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA TC0E7
UT WOS:001238933900001
DA 2024-08-05
ER

PT J
AU Luo, Q
   Shao, J
   Dang, WL
   Wang, C
   Cao, LB
   Zhang, T
AF Luo, Qian
   Shao, Jie
   Dang, Wanli
   Wang, Chao
   Cao, Libo
   Zhang, Tao
TI An efficient feature pyramid attention network for person
   re-identification
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Person re -identification; Convolutional neural network; Feature
   pyramid; Attention mechanism
AB For person re -identification, occlusion, appearance similarity and background clutter have always been challenges. In order to effectively address the challenges, we propose an efficient feature pyramid attention network (FPA-Net), which combines visual features from different levels to focus on both detail features and information. Specifically, we embed a pair of attention mechanisms that complement each other in the backbone network to focus on the discriminant features of person areas. In addition, we designed a novel feature pyramid structure, which propagates the feature information from the cross -level through the top feature to the bottom feature and from the bottom feature to the top feature to supplement the detail information of the feature. Finally, we integrate features form different scales through a lightweight transition block to generate more discriminant features. Our method performed experimental analysis on four datasets: Market -1501, DukeMTMC-ReID, CUHK03(Detected) and MSMT17. A large number of experimental results prove that the performance of the method is significantly better than the existing state-of-the-art methods.
C1 [Luo, Qian; Dang, Wanli; Wang, Chao; Cao, Libo; Zhang, Tao] Civil Aviat Adm China, Res Inst 2, Chengdu 610041, Peoples R China.
   [Luo, Qian; Shao, Jie] Xihua Univ, Chengdu 610039, Peoples R China.
   [Dang, Wanli; Wang, Chao; Cao, Libo; Zhang, Tao] Civil Aviat Elect Technol Co Ltd, Chengdu 611430, Peoples R China.
C3 Xihua University
RP Luo, Q (corresponding author), Civil Aviat Adm China, Res Inst 2, Chengdu 610041, Peoples R China.; Luo, Q; Shao, J (corresponding author), Xihua Univ, Chengdu 610039, Peoples R China.
EM luoqian@caacetc.com; shaojie@stu.xhu.edu.cn
FU NNSFC; CAAC [U2133211]
FX This document is the results of the research project funded by the NNSFC
   and CAAC (U2133211) .
CR Bolle RM, 2005, FOURTH IEEE WORKSHOP ON AUTOMATIC IDENTIFICATION ADVANCED TECHNOLOGIES, PROCEEDINGS, P15, DOI 10.1109/AUTOID.2005.48
   Chen A, 2022, BIOCYBERN BIOMED ENG, V42, P204, DOI 10.1016/j.bbe.2021.12.010
   Chen GY, 2021, IEEE T IMAGE PROCESS, V30, P7663, DOI 10.1109/TIP.2021.3107211
   Chen H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14940, DOI 10.1109/ICCV48922.2021.01469
   Chen HY, 2022, PATTERN RECOGN, V130, DOI 10.1016/j.patcog.2022.108827
   Chen HY, 2022, COMPUT BIOL MED, V143, DOI 10.1016/j.compbiomed.2022.105265
   Chen PX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11813, DOI 10.1109/ICCV48922.2021.01162
   Chen TL, 2019, IEEE I CONF COMP VIS, P8350, DOI 10.1109/ICCV.2019.00844
   Chen XM, 2021, IEEE T IMAGE PROCESS, V30, P1935, DOI 10.1109/TIP.2021.3049943
   Chen XS, 2020, PROC CVPR IEEE, P3297, DOI 10.1109/CVPR42600.2020.00336
   Chen YF, 2022, PATTERN RECOGN, V126, DOI 10.1016/j.patcog.2022.108567
   Chen YC, 2018, IEEE T PATTERN ANAL, V40, P392, DOI 10.1109/TPAMI.2017.2666805
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dou ZP, 2022, LECT NOTES COMPUT SC, V13674, P588, DOI 10.1007/978-3-031-19781-9_34
   Fan ZZ, 2023, COMPUT BIOL MED, V162, DOI 10.1016/j.compbiomed.2023.107070
   Gong YP, 2022, IEEE COMPUT SOC CONF, P4312, DOI 10.1109/CVPRW56347.2022.00477
   Gu XQ, 2022, PROC CVPR IEEE, P1050, DOI 10.1109/CVPR52688.2022.00113
   He ST, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14993, DOI 10.1109/ICCV48922.2021.01474
   Hou RB, 2022, IEEE T PATTERN ANAL, V44, P4894, DOI 10.1109/TPAMI.2021.3079910
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang MY, 2023, IEEE T IMAGE PROCESS, V32, P1568, DOI 10.1109/TIP.2023.3247159
   Kulwa F, 2022, ENVIRON SCI POLLUT R, V29, P51909, DOI 10.1007/s11356-022-18849-0
   Lai SQ, 2021, IEEE INT CONF COMP V, P4133, DOI 10.1109/ICCVW54120.2021.00461
   Li HJ, 2021, PROC CVPR IEEE, P6725, DOI 10.1109/CVPR46437.2021.00666
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Li XT, 2022, ARTIF INTELL REV, V55, P4809, DOI 10.1007/s10462-021-10121-0
   Li YL, 2021, PROC CVPR IEEE, P2897, DOI 10.1109/CVPR46437.2021.00292
   Li ZY, 2021, COMPUT VIS IMAGE UND, V205, DOI 10.1016/j.cviu.2021.103172
   Lian SC, 2021, IEEE T CIRC SYST VID, V31, P3140, DOI 10.1109/TCSVT.2020.3037179
   Liao SC, 2022, PROC CVPR IEEE, P7349, DOI 10.1109/CVPR52688.2022.00721
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Lingxiao He, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P357, DOI 10.1007/978-3-030-58604-1_22
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu WL, 2022, PATTERN RECOGN, V130, DOI [10.1016/j.patcog.2020.108829, 10.1016/j.patcog.2022.108829]
   Liu YH, 2021, IEEE T IMAGE PROCESS, V30, P2060, DOI 10.1109/TIP.2021.3050839
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190
   Martinel N, 2020, IEEE T IMAGE PROCESS, V29, P7306, DOI 10.1109/TIP.2020.3000904
   Nie QQ, 2023, COMPUT BIOL MED, V167, DOI 10.1016/j.compbiomed.2023.107620
   Pervaiz N, 2023, VISUAL COMPUT, V39, P4087, DOI 10.1007/s00371-022-02577-0
   Pu Nan, 2021, P IEEECVF C COMPUTER, P7901
   Rahaman MM, 2021, COMPUT BIOL MED, V136, DOI 10.1016/j.compbiomed.2021.104649
   Rahaman MM, 2020, J X-RAY SCI TECHNOL, V28, P821, DOI 10.3233/XST-200715
   Rao YM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1005, DOI 10.1109/ICCV48922.2021.00106
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shang Gao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11741, DOI 10.1109/CVPR42600.2020.01176
   Shizhen Zhao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P647, DOI 10.1007/978-3-030-58539-6_39
   Somers V, 2023, IEEE WINT CONF APPL, P1613, DOI 10.1109/WACV56688.2023.00166
   Sun J, 2021, PATTERN RECOGN, V116, DOI 10.1016/j.patcog.2021.107937
   Wang HC, 2022, PROC CVPR IEEE, P7287, DOI 10.1109/CVPR52688.2022.00715
   Wang K, 2021, IEEE T IMAGE PROCESS, V30, P3405, DOI 10.1109/TIP.2021.3060909
   Wang P., 2022, The Visual Computer, P1
   Wang PY, 2021, IEEE T IMAGE PROCESS, V30, P2908, DOI 10.1109/TIP.2021.3055952
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/ISQED48828.2020.9137057, 10.1109/isqed48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang T, 2022, AAAI CONF ARTIF INTE, P2540
   Wang ZK, 2022, PROC CVPR IEEE, P4744, DOI 10.1109/CVPR52688.2022.00471
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu D, 2021, IEEE T EM TOP COMP I, V5, P70, DOI 10.1109/TETCI.2020.3034606
   Xia JE, 2024, Arxiv, DOI arXiv:2303.10976
   Yan C., 2021, P IEEECVF INT C COMP, P10943
   Yang JR, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11865, DOI 10.1109/ICCV48922.2021.01167
   Yin J, 2020, INT J COMPUT VISION, V128, P1654, DOI 10.1007/s11263-019-01259-0
   Zahra A, 2023, PATTERN RECOGN, V142, DOI 10.1016/j.patcog.2023.109669
   Zhang AG, 2021, PROC CVPR IEEE, P598, DOI 10.1109/CVPR46437.2021.00066
   Zhang JH, 2023, ARTIF INTELL REV, V56, P1013, DOI 10.1007/s10462-022-10192-7
   Zhang JH, 2021, PATTERN RECOGN, V115, DOI 10.1016/j.patcog.2021.107885
   Zhang ZZ, 2020, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR42600.2020.00325
   Zhang ZZ, 2020, IEEE T IMAGE PROCESS, V29, P7104, DOI 10.1109/TIP.2020.2998931
   Zhang Z, 2021, PATTERN RECOGN, V120, DOI 10.1016/j.patcog.2021.108155
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zhong YJ, 2021, IEEE T IMAGE PROCESS, V30, P8384, DOI 10.1109/TIP.2021.3113183
   Zhou KY, 2022, IEEE T PATTERN ANAL, V44, P5056, DOI 10.1109/TPAMI.2021.3069237
   Zhou QQ, 2020, IEEE T IMAGE PROCESS, V29, P7578, DOI 10.1109/TIP.2020.3004267
   Zhu HW, 2022, PROC CVPR IEEE, P4682, DOI 10.1109/CVPR52688.2022.00465
NR 76
TC 1
Z9 1
U1 7
U2 7
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD MAY
PY 2024
VL 145
AR 104963
DI 10.1016/j.imavis.2024.104963
EA MAR 2024
PG 17
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA OO0U5
UT WOS:001208105700001
DA 2024-08-05
ER

PT J
AU Karácsony, T
   Jeni, LA
   de la Torre, F
   Cunha, JPS
AF Karacsony, Tamas
   Jeni, Laszlo Attila
   De la Torre, Fernando
   Cunha, Joao Paulo Silva
TI Deep learning methods for single camera based clinical in-bed movement
   action recognition
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Action recognition; 3D motion capture; Clinical in -bed monitoring;
   Diagnosis support; Seizure semiology; Epilepsy
ID HUMAN POSE ESTIMATION; 3D POSE; NEURAL-NETWORKS; EPILEPSY; SYSTEM
AB Many clinical applications involve in-bed patient activity monitoring, from intensive care and neuro-critical infirmary, to semiology-based epileptic seizure diagnosis support or sleep monitoring at home, which require accurate recognition of in-bed movement actions from video streams. The major challenges of clinical application arise from the domain gap between common in-the-lab and clinical scenery (e.g. viewpoint, occlusions, out-of-domain actions), the requirement of minimally intrusive monitoring to already existing clinical practices (e.g. non-contact monitoring), and the significantly limited amount of labeled clinical action data available. Focusing on one of the most demanding in-bed clinical scenarios - semiology-based epileptic seizure classification - this review explores the challenges of video-based clinical in-bed monitoring, reviews video-based action recognition trends, monocular 3D MoCap, and semiology-based automated seizure classification approaches. Moreover, provides a guideline to take full advantage of transfer learning for in-bed action recognition for quantified, evidence-based clinical diagnosis support. The review suggests that an approach based on 3D MoCap and skeleton-based action recognition, strongly relying on transfer learning, could be advantageous for these clinical in-bed action recognition problems. However, these still face several challenges, such as spatio-temporal stability, occlusion handling, and robustness before realizing the full potential of this technology for routine clinical usage.
C1 [Karacsony, Tamas; Cunha, Joao Paulo Silva] Inst Syst Engn & Comp Technol & Sci INESC TEC, Ctr Biomed Engn Res, Porto, Portugal.
   [Karacsony, Tamas; Cunha, Joao Paulo Silva] Univ Porto, Fac Engn FEUP, Porto, Portugal.
   [Karacsony, Tamas; Jeni, Laszlo Attila; De la Torre, Fernando] Carnegie Mellon Univ, Robotics Inst, Pittsburgh, PA 15213 USA.
C3 INESC TEC; Universidade do Porto; Carnegie Mellon University
RP Karácsony, T; Cunha, JPS (corresponding author), Inst Syst Engn & Comp Technol & Sci INESC TEC, Ctr Biomed Engn Res, Porto, Portugal.
EM tamas.karacsony@inesctec.pt; jcunha@ieee.org
FU Fundacao para a Ciencia e a Tecnologia under the scope of the CMU
   Portugal program [PRT/BD/152202/2021]; National Funds through the
   Portuguese funding agency, FCT-Fundacao para a Ciencia e a Tecnologia
   [LA/P/0063/2020]
FX This work was partially funded by Fundacao para a Ciencia e a Tecnologia
   under the scope of the CMU Portugal program (Ref PRT/BD/152202/2021) .
   This work is financed by National Funds through the Portuguese funding
   agency, FCT-Fundacao para a Ciencia e a Tecnologia, within project
   LA/P/0063/2020. DOI 10.5 4499/LA/P/0063/2020 |
   https://doi.org/10.54499/LA/P/0063/2020.
CR Achilles Felix, 2016, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016. 19th International Conference. Proceedings: LNCS 9900, P491, DOI 10.1007/978-3-319-46720-7_57
   Achilles F, 2018, COMP M BIO BIO E-IV, V6, P264, DOI 10.1080/21681163.2016.1141062
   Ahmedt-Aristizabal D, 2019, IEEE ENG MED BIO, P1625, DOI [10.1109/EMBC.2019.8857656, 10.1109/embc.2019.8857656]
   Ahmedt-Aristizabal D, 2019, SEIZURE-EUR J EPILEP, V65, P65, DOI 10.1016/j.seizure.2018.12.017
   Ahmedt-Aristizabal D, 2018, IEEE ENG MED BIO, P3578, DOI 10.1109/EMBC.2018.8513031
   Ahmedt-Aristizabal D, 2018, EPILEPSY BEHAV, V87, P46, DOI 10.1016/j.yebeh.2018.07.028
   Ahmedt-Aristizabal D, 2018, EPILEPSY BEHAV, V82, P17, DOI 10.1016/j.yebeh.2018.02.010
   Ailing Zeng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P507, DOI 10.1007/978-3-030-58568-6_30
   Belagiannis V, 2016, IEEE T PATTERN ANAL, V38, P1929, DOI 10.1109/TPAMI.2015.2509986
   Belagiannis V, 2015, LECT NOTES COMPUT SC, V8925, P742, DOI 10.1007/978-3-319-16178-5_52
   Blum DE, 1996, NEUROLOGY, V47, P260, DOI 10.1212/WNL.47.1.260
   Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34
   Bridgeman Lewis, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). Proceedings, P2487, DOI 10.1109/CVPRW.2019.00304
   Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339
   Cai YJ, 2019, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2019.00236
   Carmona J., 2023, 2023 IEEE 7 PORT M B
   Carreira J., 2018, arXiv, DOI DOI 10.48550/ARXIV.1808.01340
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chadha A, 2020, COMPUT VIS MEDIA, V6, P307, DOI 10.1007/s41095-020-0175-7
   Chan K.C.K., 2021, IEEE COMP SOC C COMP
   Chan KCK, 2022, PROC CVPR IEEE, P5952, DOI 10.1109/CVPR52688.2022.00587
   Chen CH, 2017, PROC CVPR IEEE, P5759, DOI 10.1109/CVPR.2017.610
   Chen HG, 2022, INFORM FUSION, V79, P124, DOI 10.1016/j.inffus.2021.09.005
   Chen K, 2018, IEEE J TRANSL ENG HE, V6, DOI 10.1109/JTEHM.2018.2875464
   Chen T, 2020, PR MACH LEARN RES, V119
   Chen YX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13339, DOI 10.1109/ICCV48922.2021.01311
   Cheng Y, 2020, AAAI CONF ARTIF INTE, V34, P10631
   Cheng Y, 2019, IEEE I CONF COMP VIS, P723, DOI 10.1109/ICCV.2019.00081
   Cheng ZY, 2019, LECT NOTES COMPUT SC, V11363, P605, DOI 10.1007/978-3-030-20893-6_38
   Choi H, 2021, PROC CVPR IEEE, P1964, DOI 10.1109/CVPR46437.2021.00200
   Choi Hongsuk, 2020, COMPUTER VISION ECCV
   Ci H, 2019, IEEE I CONF COMP VIS, P2262, DOI 10.1109/ICCV.2019.00235
   Cunha JPS, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0145669
   Dabral R, 2018, LECT NOTES COMPUT SC, V11213, P679, DOI 10.1007/978-3-030-01240-3_41
   Desmarais Y, 2021, COMPUT VIS IMAGE UND, V212, DOI 10.1016/j.cviu.2021.103275
   Dong JT, 2022, IEEE T PATTERN ANAL, V44, P6981, DOI 10.1109/TPAMI.2021.3098052
   Dong JT, 2019, PROC CVPR IEEE, P7784, DOI 10.1109/CVPR.2019.00798
   Duan H., 2022, arXiv, DOI DOI 10.48550/ARXIV.2205.09443
   Duan HD, 2022, PROC CVPR IEEE, P2959, DOI 10.1109/CVPR52688.2022.00298
   Ershadi-Nasab S, 2018, MULTIMED TOOLS APPL, V77, P15573, DOI 10.1007/s11042-017-5133-8
   Fürbass F, 2015, CLIN NEUROPHYSIOL, V126, P1124, DOI 10.1016/j.clinph.2014.09.023
   Ge SM, 2019, IEEE T IMAGE PROCESS, V28, P2051, DOI 10.1109/TIP.2018.2883743
   Georgakis Georgios, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P768, DOI 10.1007/978-3-030-58520-4_45
   Gu RS, 2021, INT C PATT RECOG, P8243, DOI 10.1109/ICPR48806.2021.9412107
   Guan P, 2009, IEEE I CONF COMP VIS, P1381, DOI 10.1109/iccv.2009.5459300
   Guan SY, 2021, PROC CVPR IEEE, P10467, DOI 10.1109/CVPR46437.2021.01033
   Hadsell R., 2006, Computer vision and pattern recognition, 2006 IEEE computer society conference on, V2, P1735, DOI DOI 10.1109/CVPR.2006.100
   Han F, 2017, COMPUT VIS IMAGE UND, V158, P85, DOI 10.1016/j.cviu.2017.01.011
   Haris M., 2021, TASK DRIVEN SUPER RE, P387, DOI 10.1007/978-3-030-92307-5_45
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Hoppe C, 2007, ARCH NEUROL-CHICAGO, V64, P1595, DOI 10.1001/archneur.64.11.1595
   Hossain MRI, 2018, LECT NOTES COMPUT SC, V11214, P69, DOI 10.1007/978-3-030-01249-6_5
   Hou JC, 2021, 2021 17TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS 2021), DOI 10.1109/AVSS52988.2021.9663770
   Hou JC, 2022, INT CONF ACOUST SPEE, P1151, DOI 10.1109/ICASSP43922.2022.9746325
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Iskakov K, 2019, IEEE I CONF COMP VIS, P7717, DOI 10.1109/ICCV.2019.00781
   Jahangiri E, 2017, IEEE INT CONF COMP V, P805, DOI 10.1109/ICCVW.2017.100
   Jheng-Wei Su, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7965, DOI 10.1109/CVPR42600.2020.00799
   Jiang L., 2021, arXiv
   Jingbo Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P764, DOI 10.1007/978-3-030-58601-0_45
   Joo H, 2019, IEEE T PATTERN ANAL, V41, P190, DOI 10.1109/TPAMI.2017.2782743
   Joo H, 2015, IEEE I CONF COMP VIS, P3334, DOI 10.1109/ICCV.2015.381
   Kadkhodamohammadi A, 2020, MACH VISION APPL, V32, DOI 10.1007/s00138-020-01120-2
   Kanazawa A, 2018, PROC CVPR IEEE, P7122, DOI 10.1109/CVPR.2018.00744
   Karacsony T., 2021, BHI 2021 2021 IEEE E, DOI [10.1109/BHI50953.2021.9508555, DOI 10.1109/BHI50953.2021.9508555]
   Karácsony T, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-23133-9
   Karácsony T, 2020, INT CONF ACOUST SPEE, P4117, DOI [10.1109/ICASSP40776.2020.9054649, 10.1109/icassp40776.2020.9054649]
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   Kenkun Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P318, DOI 10.1007/978-3-030-58607-2_19
   Kerling F, 2006, EPILEPSY BEHAV, V9, P281, DOI 10.1016/j.yebeh.2006.05.010
   Kocabas M, 2022, Arxiv, DOI arXiv:2110.00620
   Kocabas M, 2020, PROC CVPR IEEE, P5252, DOI 10.1109/CVPR42600.2020.00530
   Kolotouros N, 2019, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2019.00234
   Kondratyuk D, 2021, PROC CVPR IEEE, P16015, DOI 10.1109/CVPR46437.2021.01576
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Kumar M., 2021, INT C LEARN REPR
   Kundu JN, 2020, AAAI CONF ARTIF INTE, V34, P11312
   Kundu JN, 2020, PROC CVPR IEEE, P6151, DOI 10.1109/CVPR42600.2020.00619
   Kurada AV, 2019, SEIZURE-EUR J EPILEP, V66, P61, DOI 10.1016/j.seizure.2019.02.007
   Lassner C, 2017, PROC CVPR IEEE, P4704, DOI 10.1109/CVPR.2017.500
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lee K, 2018, LECT NOTES COMPUT SC, V11211, P123, DOI 10.1007/978-3-030-01234-2_8
   Li C, 2019, PROC CVPR IEEE, P9879, DOI 10.1109/CVPR.2019.01012
   Li JA, 2017, PROC CVPR IEEE, P1951, DOI 10.1109/CVPR.2017.211
   Li MS, 2022, IEEE T PATTERN ANAL, V44, P3316, DOI 10.1109/TPAMI.2021.3053765
   Li WH, 2023, IEEE T MULTIMEDIA, V25, P1282, DOI 10.1109/TMM.2022.3141231
   Li Y, 2021, IRBM, V42, P120, DOI 10.1016/j.irbm.2020.08.004
   Li Y., 2021, ICCV IEEE INT C COMP
   Li Z, 2019, IEEE I CONF COMP VIS, P2192, DOI 10.1109/ICCV.2019.00228
   Liang JY, 2022, Arxiv, DOI [arXiv:2201.12288, DOI 10.48550/ARXIV.2201.12288]
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Liang S, 2018, COMPUT VIS IMAGE UND, V176, P1, DOI 10.1016/j.cviu.2018.10.006
   Lin K., 2021, ICCV 2021
   Lin K, 2021, PROC CVPR IEEE, P1954, DOI 10.1109/CVPR46437.2021.00199
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P2684, DOI 10.1109/TPAMI.2019.2916873
   Liu SJ, 2023, IEEE T PATTERN ANAL, V45, P1106, DOI 10.1109/TPAMI.2022.3155712
   Liu SJ, 2019, LECT NOTES COMPUT SC, V11764, P236, DOI 10.1007/978-3-030-32239-7_27
   Liu T, 2022, INT J COMPUT VISION, V130, P111, DOI 10.1007/s11263-021-01529-w
   Liu W, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3524497
   Liu ZY, 2020, PROC CVPR IEEE, P140, DOI 10.1109/CVPR42600.2020.00022
   Mahmood N, 2019, IEEE I CONF COMP VIS, P5441, DOI 10.1109/ICCV.2019.00554
   Maia P, 2019, 2019 6TH IEEE PORTUGUESE MEETING IN BIOENGINEERING (ENBENG), DOI 10.1109/enbeng.2019.8692465
   Martinez J, 2017, IEEE I CONF COMP VIS, P2659, DOI 10.1109/ICCV.2017.288
   Mehta D, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392410
   Mehta D, 2017, INT CONF 3D VISION, P506, DOI 10.1109/3DV.2017.00064
   Moreno-Noguer F, 2017, PROC CVPR IEEE, P1561, DOI 10.1109/CVPR.2017.170
   Neumann L, 2019, LECT NOTES COMPUT SC, V11363, P558, DOI 10.1007/978-3-030-20893-6_35
   Nie BX, 2017, IEEE I CONF COMP VIS, P3467, DOI 10.1109/ICCV.2017.373
   Noachtar S, 2009, EPILEPSY BEHAV, V15, P66, DOI 10.1016/j.yebeh.2009.02.028
   Noachtar S, 2009, EPILEPSY BEHAV, V15, P2, DOI 10.1016/j.yebeh.2009.02.029
   Noh J, 2019, IEEE I CONF COMP VIS, P9724, DOI 10.1109/ICCV.2019.00982
   Oh S.J., 2019, 7 INT C LEARN REPR I
   Omran M, 2018, INT CONF 3D VISION, P484, DOI 10.1109/3DV.2018.00062
   Oord A., 2018, Advances in Neural Information Processing Systems
   Pandey K, 2022, Arxiv, DOI [arXiv:2201.00308, 10.48550/arXiv.2201.00308]
   Pareek P, 2021, ARTIF INTELL REV, V54, P2259, DOI 10.1007/s10462-020-09904-8
   Patel P, 2021, PROC CVPR IEEE, P13463, DOI 10.1109/CVPR46437.2021.01326
   Pavlakos G, 2019, PROC CVPR IEEE, P10967, DOI 10.1109/CVPR.2019.01123
   Pavlakos G, 2018, PROC CVPR IEEE, P7307, DOI 10.1109/CVPR.2018.00763
   Pavlakos G, 2018, PROC CVPR IEEE, P459, DOI 10.1109/CVPR.2018.00055
   Pavlakos G, 2017, PROC CVPR IEEE, P1263, DOI 10.1109/CVPR.2017.139
   Pavllo D, 2019, PROC CVPR IEEE, P7745, DOI 10.1109/CVPR.2019.00794
   Pérez-García F, 2021, LECT NOTES COMPUT SC, V12905, P334, DOI 10.1007/978-3-030-87240-3_32
   Pothula PK, 2022, ANN IEEE SYST CONF, DOI 10.1109/SysCon53536.2022.9773923
   Qammaz A, 2021, INT C PATT RECOG, P6904, DOI 10.1109/ICPR48806.2021.9411956
   Qiang Nie, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P102, DOI 10.1007/978-3-030-58529-7_7
   Reddy ND, 2021, PROC CVPR IEEE, P15185, DOI 10.1109/CVPR46437.2021.01494
   Reddy ND, 2018, PROC CVPR IEEE, P1906, DOI 10.1109/CVPR.2018.00204
   Ren WY, 2020, IEEE ACCESS, V8, P135628, DOI 10.1109/ACCESS.2020.3011697
   Rong Y, 2020, Arxiv, DOI arXiv:2008.08324
   Rosenow F, 2001, BRAIN, V124, P1683, DOI 10.1093/brain/124.9.1683
   Rozumnyi D, 2021, PROC CVPR IEEE, P3455, DOI 10.1109/CVPR46437.2021.00346
   Sarandi I, 2018, Arxiv, DOI arXiv:1808.09316
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shaham TR, 2019, IEEE I CONF COMP VIS, P4569, DOI 10.1109/ICCV.2019.00467
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Sharma S, 2019, IEEE I CONF COMP VIS, P2325, DOI 10.1109/ICCV.2019.00241
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Sigal L, 2010, INT J COMPUT VISION, V87, P4, DOI 10.1007/s11263-009-0273-6
   Smaira L., 2020, arXiv, DOI DOI 10.48550/ARXIV.2010.10864
   Song YF, 2023, IEEE T PATTERN ANAL, V45, P1474, DOI 10.1109/TPAMI.2022.3157033
   Soomro K, 2012, Arxiv, DOI arXiv:1212.0402
   Srivastav V, 2021, Arxiv, DOI arXiv:1808.08180
   Sun WJ, 2020, IEEE T IMAGE PROCESS, V29, P4027, DOI 10.1109/TIP.2020.2970248
   Sun ZH, 2023, IEEE T PATTERN ANAL, V45, P3200, DOI 10.1109/TPAMI.2022.3183112
   Tan, 2017, BRIT MACH VIS C, DOI [10.5244/c.31.15, DOI 10.5244/C.31.15]
   Tan W., 2018, COMPUTER VISION PATT, P4321
   Tekin B, 2017, IEEE I CONF COMP VIS, P3961, DOI 10.1109/ICCV.2017.425
   Tekin B, 2016, PROC CVPR IEEE, pCP8, DOI 10.1109/CVPR.2016.113
   Tekin Bugra, 2016, P BRIT MACH VIS C BM, DOI DOI 10.5244/C.30.130
   Trumble Matthew., 2017, BRIT MACHINE VISION, DOI [10.5244/C.31.14, DOI 10.5244/C.31.14]
   Tung HYF, 2017, ADV NEUR IN, V30
   Varol G, 2017, PROC CVPR IEEE, P4627, DOI 10.1109/CVPR.2017.492
   Vilas-Boas Maria do Carmo, 2016, IEEE Rev Biomed Eng, V9, P15, DOI 10.1109/RBME.2016.2543683
   von Marcard T, 2018, LECT NOTES COMPUT SC, V11214, P614, DOI 10.1007/978-3-030-01249-6_37
   Wang C., 2021, arXiv
   Wang J, 2014, PROC CVPR IEEE, P1386, DOI 10.1109/CVPR.2014.180
   Wang J, 2019, IEEE I CONF COMP VIS, P7770, DOI 10.1109/ICCV.2019.00786
   Wang M, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P978
   Wang PC, 2018, COMPUT VIS IMAGE UND, V171, P118, DOI 10.1016/j.cviu.2018.04.007
   Wang XH, 2018, IEEE T MULTIMEDIA, V20, P634, DOI 10.1109/TMM.2017.2749159
   Wang ZY, 2016, PROC CVPR IEEE, P4792, DOI 10.1109/CVPR.2016.518
   Wang ZH, 2021, IEEE T PATTERN ANAL, V43, P3365, DOI 10.1109/TPAMI.2020.2982166
   Wohlhart P, 2015, PROC CVPR IEEE, P3109, DOI 10.1109/CVPR.2015.7298930
   Xiang DL, 2019, PROC CVPR IEEE, P10957, DOI 10.1109/CVPR.2019.01122
   Xiangyu Xu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P284, DOI 10.1007/978-3-030-58545-7_17
   Xie K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11512, DOI 10.1109/ICCV48922.2021.01133
   Xu JW, 2020, PROC CVPR IEEE, P896, DOI 10.1109/CVPR42600.2020.00098
   Xu XY, 2022, IEEE T PATTERN ANAL, V44, P4490, DOI 10.1109/TPAMI.2021.3070002
   Yan S, 2022, PROC CVPR IEEE, P3323, DOI 10.1109/CVPR52688.2022.00333
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yin Y, 2020, Arxiv, DOI arXiv:2012.06735
   Yu JH, 2022, Arxiv, DOI arXiv:2205.01917
   Yu T, 2017, IEEE I CONF COMP VIS, P910, DOI 10.1109/ICCV.2017.104
   Yu Y., 2023, arXiv
   Zanfir A, 2018, PROC CVPR IEEE, P2148, DOI 10.1109/CVPR.2018.00229
   Zavala-Mondragon LA, 2020, J AMB INTEL HUM COMP, V11, P2369, DOI 10.1007/s12652-019-01259-5
   Zhang SW, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11323, DOI 10.1109/ICCV48922.2021.01115
   Zhang Z, 2020, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR42600.2020.00227
   Zhao L, 2019, PROC CVPR IEEE, P3420, DOI 10.1109/CVPR.2019.00354
   Zhe Wang, 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12536), P523, DOI 10.1007/978-3-030-66096-3_36
   Zheng C, 2022, Arxiv, DOI [arXiv:2012.13392, DOI 10.48550/ARXIV.2012.13392]
   Zhou K, 2022, IEEE T PATTERN ANAL, V44, P3000, DOI 10.1109/TPAMI.2021.3051173
   Zhou S., 2023, P IEEECVF INT C COMP, P10477
   Zhou XW, 2019, IEEE T PATTERN ANAL, V41, P901, DOI 10.1109/TPAMI.2018.2816031
   Zhou XY, 2016, LECT NOTES COMPUT SC, V9915, P186, DOI 10.1007/978-3-319-49409-8_17
NR 186
TC 0
Z9 0
U1 4
U2 4
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD MAR
PY 2024
VL 143
AR 104928
DI 10.1016/j.imavis.2024.104928
EA FEB 2024
PG 15
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA LB1C2
UT WOS:001184215800001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Deng, TH
   Sun, Y
AF Deng, Tenghao
   Sun, Yan
TI Recent advances in deterministic human motion prediction: A review
SO IMAGE AND VISION COMPUTING
LA English
DT Review
DE Survey; Human motion prediction; Deep learning
ID NEURAL-NETWORK; MODEL; REPRESENTATION; TRANSLATION; MECHANISMS
AB In recent years, the rapid advancement of deep learning and the advent of extensive human motion datasets have significantly enhanced the prominence of human motion prediction technology. This article presents an overview of prevalent model architectures within this field, critically examining their advantages and drawbacks. It methodically reviews recent research breakthroughs, offering in-depth analyses of significant works. Additionally, the paper provides a comprehensive survey of current methodologies, widely used datasets, and standard evaluation metrics in human motion prediction. In closing, we highlight some limitations in the field and propose potential research directions to aid its further development in human motion prediction.
C1 [Deng, Tenghao; Sun, Yan] Shanghai Univ, Sch Comp Engn & Sci, Shanghai, Peoples R China.
C3 Shanghai University
RP Sun, Y (corresponding author), Shanghai Univ, Sch Comp Engn & Sci, Shanghai, Peoples R China.
EM dtenghao@shu.edu.cn; yansun@shu.edu.cn
FU National Natural Science Foundation of China [62002215]; Shanghai
   Pujiang Program [20PJ1404400]
FX This work is funded by the National Natural Science Foundation of China
   (No. 62002215) , this work is also partly funded by Shanghai Pujiang
   Program (No. 20PJ1404400) .
CR Aksan E, 2021, INT CONF 3D VISION, P565, DOI 10.1109/3DV53792.2021.00066
   Alfaifi R., 2020, SN Comput. Sci, V1, P286, DOI [10.1007/s42979-020-00293-x, DOI 10.1007/S42979-020-00293-X]
   Bai SJ, 2018, Arxiv, DOI [arXiv:1803.01271, DOI 10.48550/ARXIV.1803.01271]
   Baldassarre F, 2021, BIOINFORMATICS, V37, P360, DOI 10.1093/bioinformatics/btaa714
   Bloom V., 2012, 2012 IEEE COMP SOC C, p7?12, DOI [DOI 10.1109/CVPRW.2012.6239175, 10.1109/CVPRW.2012.6239175]
   Bouazizi A., 2022, INT JOINT C ART INT, P791
   Brand M, 2000, COMP GRAPH, P183, DOI 10.1145/344779.344865
   Brauwers G, 2023, IEEE T KNOWL DATA EN, V35, P3279, DOI 10.1109/TKDE.2021.3126456
   Bütepage J, 2018, IEEE INT CONF ROBOT, P4563, DOI 10.1109/ICRA.2018.8460651
   Bütepage J, 2017, PROC CVPR IEEE, P1591, DOI 10.1109/CVPR.2017.173
   Caetano C, 2019, Arxiv, DOI arXiv:1907.13025
   Cao WM, 2022, NEUROCOMPUTING, V493, P106, DOI 10.1016/j.neucom.2022.04.047
   Chen HP, 2023, IEEE T CIRC SYST VID, V33, P4577, DOI 10.1109/TCSVT.2023.3284013
   Chen LJ, 2024, IEEE T CONSUM ELECTR, V70, P3318, DOI 10.1109/TCE.2023.3278466
   Chen M, 2020, PR MACH LEARN RES, V119
   Chiu HK, 2019, IEEE WINT CONF APPL, P1423, DOI 10.1109/WACV.2019.00156
   Cui Q., 2023, proceedings of the AAAI conference on, Artif. Intell, V37, P6166, DOI DOI 10.1609/AAAI.V37I5.25760
   Cui QJ, 2021, PROC CVPR IEEE, P4799, DOI 10.1109/CVPR46437.2021.00477
   Cui QJ, 2021, INFORM SCIENCES, V545, P427, DOI 10.1016/j.ins.2020.08.123
   Cui QJ, 2020, PROC CVPR IEEE, P6518, DOI 10.1109/CVPR42600.2020.00655
   Cui Y, 2021, PROC VLDB ENDOW, V15, P224, DOI 10.14778/3489496.3489503
   Dai J, 2023, PATTERN RECOGN, V143, DOI 10.1016/j.patcog.2023.109806
   Dang LW, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11447, DOI 10.1109/ICCV48922.2021.01127
   Deng CY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12180, DOI 10.1109/ICCV48922.2021.01198
   Ding R., 2023, Preprints, DOI [10.2139/ssrn.4671155, DOI 10.2139/SSRN.4671155]
   Ding Z, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21082882
   Dong M., 2022, IEEE T NEUR NET LEAR, P1, DOI [10.1109/TNNLS.2022.3166861, DOI 10.1109/TNNLS.2022.3166861]
   Duc Nguyen M., 2023, 2023 15 INT C KNOWLE, P1, DOI [10.1109/KSE59128.2023.10299418, DOI 10.1109/KSE59128.2023.10299418]
   Eltouny KA, 2023, Arxiv, DOI arXiv:2307.03610
   Fernando T, 2023, Arxiv, DOI arXiv:2305.11394
   Fifty C, 2021, ADV NEUR IN, V34
   Fragkiadaki K, 2015, IEEE I CONF COMP VIS, P4346, DOI 10.1109/ICCV.2015.494
   Fu JJ, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3277476
   Fujii R, 2021, IEEE ACCESS, V9, P56140, DOI 10.1109/ACCESS.2021.3072135
   Gal Y, 2016, PR MACH LEARN RES, V48
   Gao XH, 2023, PROC CVPR IEEE, P6451, DOI 10.1109/CVPR52729.2023.00624
   Gao Z, 2021, IEEE T IMAGE PROCESS, V30, P767, DOI 10.1109/TIP.2020.3038372
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Guo W, 2023, IEEE WINT CONF APPL, P4798, DOI 10.1109/WACV56688.2023.00479
   He ZQ, 2023, FRONT COMPUT NEUROSC, V17, DOI 10.3389/fncom.2023.1145209
   Holden D, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925975
   Hu Y, 2021, INT J DISTRIB SYST T, V12, P16, DOI 10.4018/IJDST.2021010102
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Jabbar A, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3463475
   Jain A, 2016, PROC CVPR IEEE, P5308, DOI 10.1109/CVPR.2016.573
   Jain DK, 2020, NEURAL COMPUT APPL, V32, P14579, DOI 10.1007/s00521-020-04941-4
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jiang J., 2023, 37 C NEURAL INFORM P
   Kiciroglu S, 2022, INT CONF 3D VISION, P12, DOI 10.1109/3DV57658.2022.00014
   Koppula HS, 2016, IEEE T PATTERN ANAL, V38, P14, DOI 10.1109/TPAMI.2015.2430335
   Koppula HS, 2013, IEEE INT C INT ROBOT, P2071, DOI 10.1109/IROS.2013.6696634
   Lan W, 2022, NEUROCOMPUTING, V469, P384, DOI 10.1016/j.neucom.2020.09.094
   Lasota P.A., 2017, Foundations and Trends® in Robotics, V5, P261, DOI [10.1561/2300000052, DOI 10.1561/2300000052]
   Lebailly T., 2020, Lecture Notes in Computer Science, DOI [DOI 10.1007/978-3-030-69532-3_39, 10.1007/978-3-030-69532-3_39]
   Lefkopoulos V, 2021, IEEE ROBOT AUTOM LET, V6, P80, DOI 10.1109/LRA.2020.3032079
   Lehrmann AM, 2014, PROC CVPR IEEE, P1314, DOI 10.1109/CVPR.2014.171
   Li B, 2021, IEEE T IMAGE PROCESS, V30, P2562, DOI 10.1109/TIP.2020.3038362
   Li C, 2018, PROC CVPR IEEE, P5226, DOI 10.1109/CVPR.2018.00548
   Li JC, 2023, Arxiv, DOI arXiv:2306.01075
   Li JK, 2023, NEURAL COMPUT APPL, V35, P9463, DOI 10.1007/s00521-023-08362-x
   Li M., 2021, arXiv
   Li MS, 2019, Arxiv, DOI arXiv:1910.02212
   Li MS, 2022, Arxiv, DOI arXiv:2208.00368
   Li MS, 2020, PROC CVPR IEEE, P211, DOI 10.1109/CVPR42600.2020.00029
   Li Q, 2021, IEEE INT CONF ROBOT, P3197, DOI 10.1109/ICRA48506.2021.9561540
   Li S, 2018, PROC CVPR IEEE, P5457, DOI 10.1109/CVPR.2018.00572
   Liebel L, 2018, Arxiv, DOI [arXiv:1805.06334, DOI 10.48550/ARXIV.1805.06334]
   Liu X., 2020, arXiv
   Liu XL, 2022, VISUAL COMPUT, V38, P2603, DOI 10.1007/s00371-021-02135-0
   Liu ZG, 2023, IEEE T PATTERN ANAL, V45, P681, DOI 10.1109/TPAMI.2021.3139918
   Liu ZG, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13279, DOI 10.1109/ICCV48922.2021.01305
   Liu ZG, 2019, PROC CVPR IEEE, P9996, DOI 10.1109/CVPR.2019.01024
   Lyu K, 2022, NEUROCOMPUTING, V489, P345, DOI 10.1016/j.neucom.2022.02.045
   Ma TZ, 2022, PROC CVPR IEEE, P6427, DOI 10.1109/CVPR52688.2022.00633
   Mahata SK, 2019, J INTELL SYST, V28, P447, DOI 10.1515/jisys-2018-0016
   Mahmood N, 2019, IEEE I CONF COMP VIS, P5441, DOI 10.1109/ICCV.2019.00554
   Maini Surita, 2018, Int. J. Innov. Eng. Technol, V10, P199
   Mandery C, 2015, PROCEEDINGS OF THE 17TH INTERNATIONAL CONFERENCE ON ADVANCED ROBOTICS (ICAR), P329, DOI 10.1109/ICAR.2015.7251476
   Mao W, 2021, Arxiv, DOI arXiv:2106.09300
   Mao W, 2020, Arxiv, DOI arXiv:1908.05436
   Marchellus M, 2022, IEEE ACCESS, V10, P35919, DOI 10.1109/ACCESS.2022.3163269
   Martinez J, 2017, PROC CVPR IEEE, P4674, DOI 10.1109/CVPR.2017.497
   Martinez-Gonzalez A, 2021, IEEE INT CONF COMP V, P2276, DOI 10.1109/ICCVW54120.2021.00257
   Mascaro EV, 2022, IEEE INT C INT ROBOT, P10674, DOI 10.1109/IROS47612.2022.9981877
   McAllister R, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4745
   Medina E., 2024, P IEEECVF WINTER C A, P3232
   Medjaouri O, 2022, IEEE COMPUT SOC CONF, P2539, DOI 10.1109/CVPRW56347.2022.00286
   Min SJ, 2021, KNOWL-BASED SYST, V214, DOI 10.1016/j.knosys.2021.106746
   Mishra SR, 2020, PATTERN RECOGN LETT, V135, P329, DOI 10.1016/j.patrec.2020.04.031
   Müller A, 2015, MECH SCI, V6, P137, DOI 10.5194/ms-6-137-2015
   Müller A, 2018, MULTIBODY SYST DYN, V43, P37, DOI 10.1007/s11044-017-9582-7
   Murshed MGS, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3469029
   Nargund AA, 2023, Arxiv, DOI arXiv:2303.06277
   Nasiri E, 2021, COMPUT BIOL MED, V137, DOI 10.1016/j.compbiomed.2021.104772
   Paden B, 2016, Arxiv, DOI arXiv:1604.07446
   Peng B, 2023, Arxiv, DOI arXiv:2305.13048
   Pujol-Perich David, 2021, SIGCOMM '21: Proceedings of the SIGCOMM '21 Poster and Demo Sessions, P71, DOI 10.1145/3472716.3472853
   Qu LC, 2021, NEUROCOMPUTING, V451, P290, DOI 10.1016/j.neucom.2021.03.054
   Ren HW, 2023, Arxiv, DOI arXiv:2304.04956
   Ren TX, 2020, IEEE ACCESS, V8, P186212, DOI 10.1109/ACCESS.2020.3030258
   Saadatnejad S, 2024, Arxiv, DOI arXiv:2304.06707
   Seidenari L, 2013, IEEE COMPUT SOC CONF, P479, DOI 10.1109/CVPRW.2013.77
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Shen ZR, 2021, IEEE WINT CONF APPL, P3530, DOI 10.1109/WACV48630.2021.00357
   Shi JY, 2024, IEEE T MULTIMEDIA, V26, P5194, DOI 10.1109/TMM.2023.3330075
   Shu XB, 2019, Arxiv, DOI arXiv:1909.13245
   Sigal L, 2010, INT J COMPUT VISION, V87, P4, DOI 10.1007/s11263-009-0273-6
   Sofianos T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11189, DOI 10.1109/ICCV48922.2021.01102
   Song YF, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1625, DOI 10.1145/3394171.3413802
   Sun JR, 2023, Arxiv, DOI arXiv:2305.04443
   Sun XN, 2022, Arxiv, DOI arXiv:2208.01302
   Tang J, 2023, IEEE T CIRC SYST VID, V33, P3689, DOI 10.1109/TCSVT.2023.3239322
   Tang J, 2022, NEUROCOMPUTING, V468, P245, DOI 10.1016/j.neucom.2021.10.011
   Tang YY, 2018, Arxiv, DOI arXiv:1805.02513
   Taylor Graham W, 2006, ADV NEURAL INFORM PR, V19
   Thukral R, 2022, P INT C REC TRENDS C, P827, DOI DOI 10.1007/978-981-16-7118-0_70
   Thukral R, 2019, 2019 2ND INTERNATIONAL CONFERENCE ON INTELLIGENT COMMUNICATION AND COMPUTATIONAL TECHNIQUES (ICCT), P161, DOI [10.1109/ICCT46177.2019.8969036, 10.1109/icct46177.2019.8969036]
   Thung KH, 2018, MULTIMED TOOLS APPL, V77, P29705, DOI 10.1007/s11042-018-6463-x
   Tu ZG, 2018, PATTERN RECOGN, V79, P32, DOI 10.1016/j.patcog.2018.01.020
   Ueda I, 2022, ARRAY-NY, V15, DOI 10.1016/j.array.2022.100212
   Urtasun R., 2006, 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06), V1, P238, DOI [DOI 10.1109/CVPR.2006.15, 10.1109/CVPR.2006.15]
   Usman M, 2022, FRONT COMPUT NEUROSC, V16, DOI 10.3389/fncom.2022.1051222
   Vaswani A, 2017, ADV NEUR IN, V30
   von Marcard T, 2018, LECT NOTES COMPUT SC, V11214, P614, DOI 10.1007/978-3-030-01249-6_37
   Wang HS, 2021, IEEE T IMAGE PROCESS, V30, P6096, DOI 10.1109/TIP.2021.3089380
   Wang Jack, 2005, ADV NEURAL INFORM PR
   Wang JM, 2008, IEEE T PATTERN ANAL, V30, P283, DOI 10.1109/TPAMI.2007.1167
   Wang JX, 2023, Arxiv, DOI arXiv:2308.01097
   Wang Q., 2022, ANN DATA SCI, V9, P187, DOI [10.1007/s40745-020-00253-5, DOI 10.1007/S40745-020-00253-5]
   Wang XS, 2023, Arxiv, DOI [arXiv:2312.11850, 10.48550/arXiv.2312.11850]
   Wang XS, 2023, Arxiv, DOI arXiv:2307.14006
   Wang XS, 2024, IEEE T IMAGE PROCESS, V33, P1, DOI [10.1109/TIP.2023.3334954, 10.1080/10803548.2024.2308453]
   Wang XS, 2023, Arxiv, DOI arXiv:2304.03532
   Wu HX, 2023, Arxiv, DOI arXiv:2210.02186
   Xia L., 2012, P IEEE COMP SOC C CO, P20, DOI DOI 10.1109/CVPRW.2012.6239233
   Xiao W, 2020, J INTELL FUZZY SYST, V39, P3633, DOI 10.3233/JIFS-191913
   Xiao YJ, 2019, AAAI CONF ARTIF INTE, P7322
   Xu CX, 2023, Arxiv, DOI arXiv:2308.08942
   Xu CX, 2023, Arxiv, DOI arXiv:2303.10876
   Xu Tian, 2023, IPMV '23: Proceedings of the 2023 5th International Conference on Image Processing and Machine Vision, P7, DOI 10.1145/3582177.3582179
   Yang F, 2023, SCI REP-UK, V13, DOI 10.1038/s41598-023-31053-5
   Yao JC, 2021, EURASIP J AUDIO SPEE, V2021, DOI 10.1186/s13636-021-00234-3
   Yu WH, 2022, PROC CVPR IEEE, P10809, DOI 10.1109/CVPR52688.2022.01055
   Yujun Cai, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P226, DOI 10.1007/978-3-030-58571-6_14
   Zand M, 2023, Arxiv, DOI arXiv:2308.16801
   Zhang SB, 2023, IEEE IMAGE PROC, P960, DOI 10.1109/ICIP49359.2023.10222273
   Zhang WY, 2023, PROCEEDINGS OF THE 31ST ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2023, P2856, DOI 10.1145/3581783.3612532
   Zhang WY, 2013, IEEE I CONF COMP VIS, P2248, DOI 10.1109/ICCV.2013.280
   Zhang Y., 2024, IEEE VTS VEH TECHNOL, P6164
   Zhao MY, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3579359
   Zhong CY, 2022, PROC CVPR IEEE, P6437, DOI 10.1109/CVPR52688.2022.00634
   Zhong JQ, 2023, PATTERN RECOGN, V138, DOI 10.1016/j.patcog.2023.109427
   Zhou F., 2023, Lecture Notes in Electrical Engineering, P378, DOI [10.1007/978-981-99-6187-0_38, DOI 10.1007/978-981-99-6187-0_38]
   Zhou T, 2022, PR MACH LEARN RES
NR 154
TC 0
Z9 0
U1 13
U2 13
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD MAR
PY 2024
VL 143
AR 104926
DI 10.1016/j.imavis.2024.104926
EA FEB 2024
PG 20
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA LB6M2
UT WOS:001184356600001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Liu, XR
   Qi, L
   Song, YX
   Wen, Q
AF Liu, Xinran
   Qi, Lin
   Song, Yuxuan
   Wen, Qi
TI Depth awakens: A depth-perceptual attention fusion network for RGB-D
   camouflaged object detection
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Camouflaged object detection; RGB-D; Convolutional neural networks;
   Feature fusion
AB Camouflaged object detection (COD) presents a persistent challenge in accurately identifying objects that seamlessly blend into their surroundings. However, most existing COD models overlook the fact that visual systems operate within a genuine 3D environment. The scene depth inherent in a single 2D image provides rich spatial clues that can assist in the detection of camouflaged objects. Therefore, we propose a novel depthperception attention fusion network that leverages the depth map as an auxiliary input to enhance the network's ability to perceive 3D information, which is typically challenging for the human eye to discern from 2D images. The network uses a trident-branch encoder to extract chromatic and depth information and their communications. Recognizing that certain regions of a depth map may not effectively highlight the camouflaged object, we introduce a depth-weighted cross-attention fusion module to dynamically adjust the fusion weights on depth and RGB feature maps. To keep the model simple without compromising effectiveness, we design a straightforward feature aggregation decoder that adaptively fuses the enhanced aggregated features. Experiments demonstrate the significant superiority of our proposed method over other states of the arts, which further validates the contribution of depth information in camouflaged object detection. The code will be available at https://github.com/xinran-liu00/DAF-Net.
C1 [Liu, Xinran; Qi, Lin; Song, Yuxuan; Wen, Qi] Ocean Univ China, Dept Comp Sci & Technol, Qingdao 266100, Peoples R China.
C3 Ocean University of China
RP Qi, L (corresponding author), Ocean Univ China, Dept Comp Sci & Technol, Qingdao 266100, Peoples R China.
EM lxr7766@stu.ouc.edu.cn; qilin@ouc.edu.cn; syxvision@stu.ouc.edu.cn;
   wenqi@ouc.edu.cn
FU National Natural Science Foundation of China [41927805]; Key R & D
   Program of Shandong Province, China [2020CXGC010704]
FX The authors declare the following financial interests/personal re-
   lationships which may be considered as potential competing interests:
   Lin Qi reports financial support was provided by National Natural
   Science Foundation of China (Grant No. 41927805) . Lin Qi reports
   financial support was provided by Key R & D Program of Shandong
   Province, China (NO. 2020CXGC010704) .
CR Bi HB, 2023, PATTERN RECOGN, V136, DOI 10.1016/j.patcog.2022.109194
   Birkl R, 2023, Arxiv, DOI arXiv:2307.14460
   Chen G, 2022, IEEE T CIRC SYST VID, V32, P6981, DOI 10.1109/TCSVT.2022.3178173
   Chen H, 2018, PROC CVPR IEEE, P3051, DOI 10.1109/CVPR.2018.00322
   Chen Q, 2024, IEEE T NEUR NET LEAR, V35, P4309, DOI 10.1109/TNNLS.2022.3202241
   Cheng X, 2017, COMPUT ELECTRON AGR, V141, P351, DOI 10.1016/j.compag.2017.08.005
   Ciptadi A, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.112
   Cong RM, 2022, IEEE T IMAGE PROCESS, V31, P6800, DOI 10.1109/TIP.2022.3216198
   Deng-Ping Fan, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P263, DOI 10.1007/978-3-030-59725-2_26
   Ding Y, 2019, J VIS COMMUN IMAGE R, V61, P1, DOI 10.1016/j.jvcir.2019.03.019
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Fan DP, 2022, IEEE T PATTERN ANAL, V44, P6024, DOI 10.1109/TPAMI.2021.3085766
   Fan DP, 2020, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR42600.2020.00285
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   Han JW, 2018, IEEE T CYBERNETICS, V48, P3171, DOI 10.1109/TCYB.2017.2761775
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang NAC, 2022, IEEE T MULTIMEDIA, V24, P1651, DOI 10.1109/TMM.2021.3069297
   Ji GP, 2023, MACH INTELL RES, V20, P92, DOI 10.1007/s11633-022-1365-9
   Ji W, 2021, PROC CVPR IEEE, P9466, DOI 10.1109/CVPR46437.2021.00935
   Jia Q, 2022, PROC CVPR IEEE, P4703, DOI 10.1109/CVPR52688.2022.00467
   Lang CY, 2012, LECT NOTES COMPUT SC, V7573, P101, DOI 10.1007/978-3-642-33709-3_8
   Li CY, 2021, IEEE T CYBERNETICS, V51, P88, DOI 10.1109/TCYB.2020.2969255
   Li GY, 2020, IEEE T IMAGE PROCESS, V29, P4873, DOI 10.1109/TIP.2020.2976689
   Li L, 2024, IEEE T PATTERN ANAL, V46, P479, DOI 10.1109/TPAMI.2023.3324807
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Liu JW, 2022, IEEE WINT CONF APPL, P2613, DOI 10.1109/WACV51458.2022.00267
   Liu N, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4702, DOI 10.1109/ICCV48922.2021.00468
   Liu N, 2022, IEEE T PATTERN ANAL, V44, P9026, DOI 10.1109/TPAMI.2021.3122139
   Liu ZY, 2019, NEUROCOMPUTING, V363, P46, DOI 10.1016/j.neucom.2019.07.012
   Lv YQ, 2023, IEEE T CIRC SYST VID, V33, P3462, DOI 10.1109/TCSVT.2023.3234578
   Lv YQ, 2021, PROC CVPR IEEE, P11586, DOI 10.1109/CVPR46437.2021.01142
   Mei H., 2023, SCIENTIA SINICA INFO
   Mei HY, 2021, PROC CVPR IEEE, P8768, DOI 10.1109/CVPR46437.2021.00866
   Mertan A, 2022, DIGIT SIGNAL PROCESS, V123, DOI 10.1016/j.dsp.2022.103441
   Peng HW, 2014, LECT NOTES COMPUT SC, V8691, P92, DOI 10.1007/978-3-319-10578-9_7
   Ranftl R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12159, DOI 10.1109/ICCV48922.2021.01196
   Ranftl R, 2022, IEEE T PATTERN ANAL, V44, P1623, DOI 10.1109/TPAMI.2020.3019967
   Ren JJ, 2023, IEEE T CIRC SYST VID, V33, P1157, DOI 10.1109/TCSVT.2021.3126591
   Song HK, 2017, IEEE T IMAGE PROCESS, V26, P4204, DOI 10.1109/TIP.2017.2711277
   Song MK, 2022, IEEE T IMAGE PROCESS, V31, P6124, DOI 10.1109/TIP.2022.3205747
   Sun YJ, 2022, Arxiv, DOI arXiv:2207.00794
   Tankus A, 1998, 1998 IEEE WORKSHOP ON VISUAL SURVEILLANCE, PROCEEDINGS, P42
   Le TN, 2019, COMPUT VIS IMAGE UND, V184, P45, DOI 10.1016/j.cviu.2019.04.006
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/ISQED48828.2020.9137057, 10.1109/isqed48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu Zongwei, 2023, 2023 IEEE/CVF International Conference on Computer Vision (ICCV), P1032, DOI 10.1109/ICCV51070.2023.00101
   Wu ZW, 2022, INT CONF 3D VISION, P403, DOI 10.1109/3DV57658.2022.00052
   Xiang MC., 2021, arXiv
   Yang F, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4126, DOI 10.1109/ICCV48922.2021.00411
   Yao CL, 2022, IMAGE VISION COMPUT, V117, DOI 10.1016/j.imavis.2021.104351
   Yin W, 2023, IEEE T PATTERN ANAL, V45, P6480, DOI 10.1109/TPAMI.2022.3209968
   Zeng NY, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3153997
   Zhai Q, 2021, PROC CVPR IEEE, P12992, DOI 10.1109/CVPR46437.2021.01280
   Zhang M, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P5323, DOI 10.1145/3503161.3548178
   Zhang WB, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P731, DOI 10.1145/3474085.3475240
   Zhao JX, 2019, PROC CVPR IEEE, P3922, DOI 10.1109/CVPR.2019.00405
   Zhong YJ, 2022, PROC CVPR IEEE, P4494, DOI 10.1109/CVPR52688.2022.00446
   Zhou T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4661, DOI 10.1109/ICCV48922.2021.00464
   Zhou T, 2021, COMPUT VIS MEDIA, V7, P37, DOI 10.1007/s41095-020-0199-z
   Zhou XF, 2020, IMAGE VISION COMPUT, V95, DOI 10.1016/j.imavis.2020.103888
   Zhu HW, 2022, AAAI CONF ARTIF INTE, P3608
   Zhu JC, 2021, AAAI CONF ARTIF INTE, V35, P3599
NR 62
TC 0
Z9 0
U1 4
U2 4
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD MAR
PY 2024
VL 143
AR 104924
DI 10.1016/j.imavis.2024.104924
EA FEB 2024
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA KM2H6
UT WOS:001180313000001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Wang, XY
   Yang, K
   Ding, Q
   Wang, R
   Sun, JH
AF Wang, Xiangyang
   Yang, Kun
   Ding, Qiang
   Wang, Rui
   Sun, Jinhua
TI TQRFormer: Tubelet query recollection transformer for action detection
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Spatio-temporal action detection; Transformer; Query recollection;
   Matching strategy; Long-term context
AB Spatial and temporal action detection aims to precisely locate actions while predicting their respective categories. The existing solution, TubeR (Zhao et al., 2022), is designed to directly detect action tubes in videos by recognizing and localizing actions using a unified representation. However, a potential challenge arises during the decoding stage, leading to a gradual decrease in the model's performance in action detection, specifically in terms of the confidence associated with detected actions. In this paper, we propose TQRFormer: Tubelet Query Recollection Transformer, enabling the subsequent decoder to obtain information from the previous stage. Specifically, we designed Query Recollection Attention to correct errors and output the synthesized results, effectively breaking the limitations of sequential decoding. During the training stage, TubeR (Zhao et al., 2022) generates a limited number of positive sample queries through a one-to-one matching strategy, potentially impacting the effectiveness of training with positive samples. To enhance the quantity of positive samples, we propose a stage matching approach that combines both one -to -many matching and one-to-one matching without additional queries. This approach serves to boost the overall number of positive samples for improved training outcomes. We also propose a more elegant classification head that contains the start and end frames of the small tubes information, eliminating the necessity for a separate action switch. The performance of TQRFormer is superior to previous state-of-the-art technologies on public action detection datasets, including AVA, UCF101 -24, JHMDB-21 and MultiSports. The code will available at https://github.com/ykyk000/TQRFormer.
C1 [Wang, Xiangyang; Yang, Kun; Wang, Rui] Shanghai Univ, Sch Commun & Informat Engn, Shanghai, Peoples R China.
   [Ding, Qiang; Sun, Jinhua] Fudan Univ, Natl Childrens Med Ctr, Dept Psychol Med, Childrens Hosp, Shanghai 201102, Peoples R China.
C3 Shanghai University; Fudan University
RP Wang, R (corresponding author), Shanghai Univ, Sch Commun & Informat Engn, Shanghai, Peoples R China.; Sun, JH (corresponding author), Fudan Univ, Natl Childrens Med Ctr, Dept Psychol Med, Childrens Hosp, Shanghai 201102, Peoples R China.
EM wangxiangyang@shu.edu.cn; yangkun@shu.edu.cn; rwang@shu.edu.cn;
   2005sunjinhua@163.com
FU National Natural Science Foundation of China (NSFC) [61771299];
   Important and Weak Key Discipline Construction Projects of Health System
   in Shanghai [2019ZB0203]; Natural Science Foundation of Shanghai
   [19ZR1406500, GWV-10.2-XD31]; Clinical Science and Technology Innovation
   Projects of Shanghai Shenkang hospital development center [SHDC12020126]
FX This work was supported in part by the National Natural Science
   Foundation of China (NSFC) under Grant 61771299, in part by grants from
   the Important and Weak Key Discipline Construction Projects of Health
   System in Shanghai in 2019: Psychosomatic Medicine (2019ZB0203) ,
   Natural Science Foundation of Shanghai (19ZR1406500) , Three year action
   plan from 2020 to 2022 for the construction of Shanghai public health
   system (GWV-10.2-XD31) , Clinical Science and Technology Innovation
   Projects of Shanghai Shenkang hospital development center (SHDC12020126)
   .
CR Alwando EHP, 2020, IEEE T CIRC SYST VID, V30, P104, DOI 10.1109/TCSVT.2018.2887283
   Cao Xipeng, 2022, P AAAI C ART INT
   Carion N., 2020, EUROPEAN C COMPUTER
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen FY, 2023, PROC CVPR IEEE, P23756, DOI 10.1109/CVPR52729.2023.02275
   Chen L, 2023, Arxiv, DOI [arXiv:2304.08451, DOI arXiv:2304.08451.v3]
   Chen SF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8158, DOI 10.1109/ICCV48922.2021.00807
   Dave A, 2019, IEEE INT CONF COMP V, P1493, DOI 10.1109/ICCVW.2019.00187
   Duarte K, 2018, ADV NEUR IN, V31
   Faure GJ, 2023, IEEE WINT CONF APPL, P3329, DOI 10.1109/WACV56688.2023.00334
   Feichtenhofer C, 2020, PROC CVPR IEEE, P200, DOI 10.1109/CVPR42600.2020.00028
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Girdhar Rohit, 2018, P IEEE CVF C COMP VI
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Gkioxari G, 2015, PROC CVPR IEEE, P759, DOI 10.1109/CVPR.2015.7298676
   Gu CH, 2018, PROC CVPR IEEE, P6047, DOI 10.1109/CVPR.2018.00633
   He JW, 2018, IEEE WINT CONF APPL, P343, DOI 10.1109/WACV.2018.00044
   Hou R, 2017, IEEE I CONF COMP VIS, P5823, DOI 10.1109/ICCV.2017.620
   Jain M, 2014, PROC CVPR IEEE, P740, DOI 10.1109/CVPR.2014.100
   Jhuang HH, 2013, IEEE I CONF COMP VIS, P3192, DOI 10.1109/ICCV.2013.396
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jia D, 2023, PROC CVPR IEEE, P19702, DOI 10.1109/CVPR52729.2023.01887
   Kalogeiton V, 2017, IEEE I CONF COMP VIS, P4415, DOI 10.1109/ICCV.2017.472
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   Köpüklü O, 2021, Arxiv, DOI arXiv:1911.06644
   Li D, 2018, LECT NOTES COMPUT SC, V11210, P306, DOI 10.1007/978-3-030-01231-1_19
   Li F, 2022, PROC CVPR IEEE, P13609, DOI 10.1109/CVPR52688.2022.01325
   Li YX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13516, DOI 10.1109/ICCV48922.2021.01328
   Li Yonggang, 2020, EUR C COMP VIS
   Li YX, 2020, AAAI CONF ARTIF INTE, V34, P11466
   Liu S., 2022, arXiv
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu Y, 2023, J VIS COMMUN IMAGE R, V95, DOI 10.1016/j.jvcir.2023.103879
   Loshchilov I, 2019, Arxiv, DOI [arXiv:1711.05101, 10.48550/arXiv.1711.05101, DOI 10.48550/ARXIV.1711.05101]
   Ma XR, 2021, IEEE IJCNN, DOI 10.1109/IJCNN52387.2021.9533300
   Meng DP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3631, DOI 10.1109/ICCV48922.2021.00363
   Ni JC, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3437, DOI 10.1145/3474085.3475503
   Ning Z., 2021, Technical report, Technical report
   Peng XJ, 2016, LECT NOTES COMPUT SC, V9908, P744, DOI 10.1007/978-3-319-46493-0_45
   Pramono RRA, 2019, IEEE I CONF COMP VIS, P61, DOI 10.1109/ICCV.2019.00015
   Saha S, 2017, IEEE I CONF COMP VIS, P4424, DOI 10.1109/ICCV.2017.473
   Saha Suman, 2016, BMVC
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh G, 2023, IEEE WINT CONF APPL, P5998, DOI 10.1109/WACV56688.2023.00595
   Singh G, 2019, LECT NOTES COMPUT SC, V11366, P420, DOI 10.1007/978-3-030-20876-9_27
   Singh G, 2017, IEEE I CONF COMP VIS, P3657, DOI 10.1109/ICCV.2017.393
   Song L, 2019, PROC CVPR IEEE, P11979, DOI 10.1109/CVPR.2019.01226
   Soomro K., 2012, CoRR, V2
   Su R, 2019, PROC CVPR IEEE, P12008, DOI 10.1109/CVPR.2019.01229
   Sui L, 2023, IEEE WINT CONF APPL, P5988, DOI 10.1109/WACV56688.2023.00594
   Sun C, 2018, LECT NOTES COMPUT SC, V11215, P335, DOI 10.1007/978-3-030-01252-6_20
   Tang J., 2020, EUR C COMP VIS ECCV
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Tomei M, 2021, COMPUT VIS IMAGE UND, V206, DOI 10.1016/j.cviu.2021.103187
   Ulutan O, 2020, IEEE WINT CONF APPL, P516
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang YM, 2022, AAAI CONF ARTIF INTE, P2567
   Weinzaepfel P, 2015, IEEE I CONF COMP VIS, P3164, DOI 10.1109/ICCV.2015.362
   Wu CY, 2019, PROC CVPR IEEE, P284, DOI 10.1109/CVPR.2019.00037
   Wu Jianchao, 2020, EUR C COMP VIS ECCV
   Wu T, 2023, PROC CVPR IEEE, P14720, DOI 10.1109/CVPR52729.2023.01414
   Xu MZ, 2021, ADV NEUR IN, V34
   Yang XT, 2019, PROC CVPR IEEE, P264, DOI 10.1109/CVPR.2019.00035
   Yang Zhenheng, 2017, BRIT MACH VIS C
   Yixuan Li, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P68, DOI 10.1007/978-3-030-58517-4_5
   Yu F, 2018, PROC CVPR IEEE, P2403, DOI 10.1109/CVPR.2018.00255
   Zhang S., 2020, P IEEE CVF C COMP VI, P9759
   Zhang YY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13557, DOI [10.1109/ICCV48922.2021.01332, 10.1109/iccv48922.2021.01332]
   Zhang YB, 2019, PROC CVPR IEEE, P9967, DOI 10.1109/CVPR.2019.01021
   Zhao JJ, 2022, PROC CVPR IEEE, P13588, DOI 10.1109/CVPR52688.2022.01323
   Zhao JJ, 2019, PROC CVPR IEEE, P9927, DOI 10.1109/CVPR.2019.01017
   Zheng YD, 2023, Arxiv, DOI arXiv:2304.11975
   Zhu XZ, 2021, Arxiv, DOI [arXiv:2010.04159, 10.48550/arXiv.2010.04159]
   Zolfaghari M, 2017, IEEE I CONF COMP VIS, P2923, DOI 10.1109/ICCV.2017.316
NR 75
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUL
PY 2024
VL 147
AR 105059
DI 10.1016/j.imavis.2024.105059
EA MAY 2024
PG 11
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA TQ0X3
UT WOS:001242618400001
DA 2024-08-05
ER

PT J
AU Karthik, K
   Mahadevappa, M
AF Karthik, Karri
   Mahadevappa, Manjunatha
TI Deep learning with adaptive convolutions for classification of retinal
   diseases via optical coherence tomography
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Convolution neural network (CNN); Retinal diseases; Adaptive
   convolutions; Optical coherence tomography (OCT); Image classification;
   Zeckendorf's theorem
ID NEURAL-NETWORKS; IMAGE CLASSIFICATION; SPECKLE; SEGMENTATION; NOISE;
   LAYER; STATE
AB Optical coherence tomography (OCT) uses interferometry to capture high-resolution cross-sectional images of the retina to diagnose retinal diseases. Convolutional neural networks (CNNs) have become essential for developing efficient computer-aided diagnostic algorithms, but noisy images can hinder their performance. This study introduces an innovative image preprocessing strategy that involves a new method of representing images to reduce image noise and a new adaptive convolution layer. The adaptive convolution layer aims to replace traditional convolution layers for OCT image classification by relying on local'Feature Content'. The proposed image representation is based on Zeckendorf's theorem, which states that every positive integer may be split into a unique sum of distinct, non-adjacent Fibonacci numbers. The proposed approach enables the generation of two separate images, known as the 'base' and 'fine,' where the 'base' image is the denoised image. We assessed our methodology by evaluating against ten filters, comprising a Low-pass filter, Gaussian filter, Wiener filter, Wavelet filter, Guided filter, Lee filter, Frost filter, Kuan filter, Detail Preserving Anisotropic Diffusion (DPAD) filter, and Non-Local Means (NLM) filter. In our study, we found that the proposed filter produced the most favorable results in five of the six no-reference parameters (Blur Percent (BP), Blind/Referenceless Image Spatial Quality Evaluator (BRISQUE), Naturalness Image Quality Evaluator (NIQE), Wavelet Variance (WAVV), Wavelet Variance Radial (WAVR)) used to assess the effectiveness of the proposed image enhancement technique. The OCT dataset utilized in the study was compiled by the University of California San Diego. The proposed adaptive convolution layer and its accompanying activation function were tested using seven OCT image classification CNN architectures. The test architectures comprise OctNET, NT-CNN, AOCT-NET, M-CNN, LightOCT, RetiNet, and DeepOCT. Experiments were conducted to assess the impact of the new preprocessing algorithm and the placement of the adaptive convolution layer as a substitute for the conventional convolution layer. Implementing the proposed approaches resulted in accuracy improvements ranging from 0.44% to 2.44% across architectures. Our findings highlight the efficacy of the proposed indirect noise reduction technique and a texture-sensitive adaptive convolution layer.
C1 [Karthik, Karri; Mahadevappa, Manjunatha] Indian Inst Technol Kharagpur, Sch Med Sci & Technol, Kharagpur, W Bengal, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Kharagpur
RP Mahadevappa, M (corresponding author), Indian Inst Technol Kharagpur, Sch Med Sci & Technol, Kharagpur, W Bengal, India.
EM mmaha2@smst.iitkgp.ac.in
OI Karthik, Karri/0000-0002-8369-1905
CR Ai Z, 2022, FRONT NEUROINFORM, V16, DOI 10.3389/fninf.2022.876927
   Aja-Fernández S, 2006, IEEE T IMAGE PROCESS, V15, P2694, DOI 10.1109/TIP.2006.877360
   Akman A., 2018, Optical coherence tomography in glaucoma: a practical guide, P7
   Al E. Hassan Syed, 2021, 2021 1st International Conference on Artificial Intelligence and Data Analytics (CAIDA), P206, DOI 10.1109/CAIDA51941.2021.9425161
   Almayyahi AA, 2020, INT J ADV COMPUT SC, V11, P511
   Alqudah AM, 2020, MED BIOL ENG COMPUT, V58, P41, DOI 10.1007/s11517-019-02066-y
   Altan G, 2022, ENG SCI TECHNOL, V34, DOI 10.1016/j.jestch.2021.101091
   Apostolopoulos S, 2017, INVEST OPHTH VIS SCI, V58
   Ara RK, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22134675
   Archibald R.G, 1935, Goldbach's Theorem
   Arefin R, 2021, IEEE INT CONF HEALT, P48, DOI [10.1109/ICHI52183.2021.00020, 10.1109/ichi52183.2021.00020]
   Arian R, 2023, SCI REP-UK, V13, DOI 10.1038/s41598-023-50164-7
   Arora K, 2018, Handbook of Research on Advanced Concepts in Real-time Image and Video Processing, P28, DOI 10.4018/978-1-5225-2848-7.ch002
   Asif S, 2022, INTERDISCIP SCI, V14, P906, DOI 10.1007/s12539-022-00533-z
   Avila BT, 2017, IEEE T INFORM THEORY, V63, P2357, DOI 10.1109/TIT.2017.2663433
   Baharlouei Z, 2023, SCI REP-UK, V13, DOI 10.1038/s41598-023-46200-1
   Barua PD, 2021, ENTROPY-SWITZ, V23, DOI 10.3390/e23121651
   Battisti F., 2006, 3 INT C COMP DEV COM
   Bhadra R, 2020, PROCEEDINGS OF 2020 IEEE APPLIED SIGNAL PROCESSING CONFERENCE (ASPCON 2020), P212, DOI 10.1109/ASPCON49795.2020.9276708
   Bhowmik A, 2019, COMM COM INF SC, V1000, P104, DOI 10.1007/978-3-030-20257-6_9
   Bozinovski S, 2020, INFORM-INT J COMPUT, V44, P291, DOI 10.31449/inf.v44i3.2828
   Brown J, 1964, Zeckendorf's Theorem and some Appucations
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Chai JY, 2021, MACH LEARN APPL, V6, DOI 10.1016/j.mlwa.2021.100134
   Chakrabarti S, 2014, INT J REMOTE SENS, V35, P1804, DOI 10.1080/01431161.2013.879346
   Cheung CY, 2019, ASIA-PAC J OPHTHALMO, V8, P158, DOI 10.22608/APO.201976
   Das V, 2021, IEEE SENS J, V21, P23256, DOI 10.1109/JSEN.2021.3108642
   De K, 2013, PROCEDIA ENGINEER, V64, P149, DOI 10.1016/j.proeng.2013.09.086
   DENG G, 1993, NUCLEAR SCIENCE SYMPOSIUM & MEDICAL IMAGING CONFERENCE, VOLS 1-3, P1615, DOI 10.1109/NSSMIC.1993.373563
   Dey Sandipan, 2007, 2007 3rd International Symposium on Information Assurance and Security, P101
   Di Salle G., 2023, Introduction to Artificial Intelligence, P151
   DONAGHEY R, 1977, J COMB THEORY A, V23, P291, DOI 10.1016/0097-3165(77)90020-6
   Drexler W, 2008, PROG RETIN EYE RES, V27, P45, DOI 10.1016/j.preteyeres.2007.07.005
   Elsayed M., 2018, Int. J. Simul. Syst. Sci. Technol., V19
   Emerson N.D, 2006, J. Integer Sequences, V9, P3
   Fang LY, 2019, J VIS COMMUN IMAGE R, V59, P327, DOI 10.1016/j.jvcir.2019.01.022
   FROST VS, 1982, IEEE T PATTERN ANAL, V4, P157, DOI 10.1109/TPAMI.1982.4767223
   Gonzales R.C, 2002, Digital image processing, V2
   Gour N, 2022, MULTIMED TOOLS APPL, V81, P41765, DOI 10.1007/s11042-022-13617-1
   Gowda SN, 2017, IEEE REGION 10 SYMP
   Haq M.A., 2023, Computer Systems Science and Engineering, V47, P2689, DOI DOI 10.32604/CSSE.2023.039904
   Haq MA, 2023, FRACTALS, V31, DOI 10.1142/S0218348X23401023
   Haq MA, 2022, COMPUT SYST SCI ENG, V42, P837, DOI 10.32604/csse.2022.023016
   Haq MA, 2022, CMC-COMPUT MATER CON, V71, P1769, DOI 10.32604/cmc.2022.018708
   Hassan B, 2021, BIOMED SIGNAL PROCES, V70, DOI 10.1016/j.bspc.2021.103030
   Hassan T, 2021, IEEE J BIOMED HEALTH, V25, P108, DOI 10.1109/JBHI.2020.2982914
   He JZ, 2023, SCI REP-UK, V13, DOI 10.1038/s41598-023-30853-z
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   He XX, 2020, NEUROCOMPUTING, V405, P37, DOI 10.1016/j.neucom.2020.04.044
   Hernandez-Matas C, 2019, ELS MIC SOC BOOK SER, P59, DOI 10.1016/B978-0-08-102816-2.00004-6
   Hoffman J.I. E., 2019, Basic Biostatistics for Medical and Biomedical Practitioners, P391, DOI [DOI 10.1016/B978-0-12-817084-7.00025-5, 10.1016/B978-0-12-817084-7.00025-5]
   HUANG D, 1991, SCIENCE, V254, P1178, DOI 10.1126/science.1957169
   Huang LF, 2019, IEEE SIGNAL PROC LET, V26, P1026, DOI 10.1109/LSP.2019.2917779
   Ibrahim MR, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10144716
   Kamran Sharif Amit, 2021, Deep Learning Applications. Advances in Intelligent Systems and Computing (AISC 1232), P25, DOI 10.1007/978-981-15-6759-9_2
   Kamran Sharif Amit, 2019, 2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA), P964, DOI 10.1109/ICMLA.2019.00165
   Kandel I, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10062021
   Karthik Karri, 2023, Current Directions in Biomedical Engineering, P547, DOI 10.1515/cdbme-2023-1137
   Karthik K, 2023, BIOMED SIGNAL PROCES, V79, DOI 10.1016/j.bspc.2022.104176
   Karthik K, 2019, TENCON IEEE REGION, P1746, DOI [10.1109/TENCON.2019.8929607, 10.1109/tencon.2019.8929607]
   Kayadibi I, 2023, INT J COMPUT INT SYS, V16, DOI 10.1007/s44196-023-00210-z
   Kermany Daniel, 2018, Mendeley Data, V2
   Kermany DS, 2018, CELL, V172, P1122, DOI 10.1016/j.cell.2018.02.010
   Kim J, 2021, 2021 IEEE CONFERENCE ON COMPUTATIONAL INTELLIGENCE IN BIOINFORMATICS AND COMPUTATIONAL BIOLOGY (CIBCB), P129, DOI 10.1109/CIBCB49929.2021.9562919
   Kim J, 2020, COMP MED SY, P532, DOI 10.1109/CBMS49503.2020.00106
   Kora P, 2022, BIOCYBERN BIOMED ENG, V42, P79, DOI 10.1016/j.bbe.2021.11.004
   Koshy T, 2008, Catalan Numbers with Application
   KUAN DT, 1985, IEEE T PATTERN ANAL, V7, P165, DOI 10.1109/TPAMI.1985.4767641
   Kumar BPS, 2022, J SUPERCOMPUT, V78, P18318, DOI 10.1007/s11227-022-04587-0
   Kumar K.K., 2023, Comput. Syst. Sci. Eng., V46
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LEE JS, 1986, OPT ENG, V25, P636, DOI 10.1117/12.7973877
   Lemaître G, 2016, J OPHTHALMOL, V2016, DOI 10.1155/2016/3298606
   Li AF, 2023, BMC OPHTHALMOL, V23, DOI 10.1186/s12886-023-02916-2
   Li F, 2019, BIOMED OPT EXPRESS, V10, P6204, DOI 10.1364/BOE.10.006204
   Li F, 2019, GRAEF ARCH CLIN EXP, V257, P495, DOI 10.1007/s00417-018-04224-8
   Li XC, 2019, IEEE ACCESS, V7, P33771, DOI 10.1109/ACCESS.2019.2891975
   Lim JS, 1990, 2 DIMENSIONAL SIGNAL
   Liu XM, 2022, BIOMED SIGNAL PROCES, V71, DOI 10.1016/j.bspc.2021.103087
   Lu W, 2018, TRANSL VIS SCI TECHN, V7, DOI 10.1167/tvst.7.6.41
   Ma ZQ, 2022, BIOSENSORS-BASEL, V12, DOI 10.3390/bios12070542
   Maini Surita, 2018, Int. J. Innov. Eng. Technol, V10, P199
   Mishra Sapna S., 2022, IEEE Transactions on Artificial Intelligence, V3, P625, DOI 10.1109/TAI.2021.3135797
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Mooney P., 2018, Retinal OCT Images (Optical Coherence Tomography)
   Morid MA, 2021, COMPUT BIOL MED, V128, DOI 10.1016/j.compbiomed.2020.104115
   Silva TOE, 2014, MATH COMPUT, V83, P2033
   Özcelik YB, 2023, FRACTAL FRACT, V7, DOI 10.3390/fractalfract7080598
   Ozcelik Yusuf Bahri, 2023, P CANK INT C SCI RES, P10
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Pertuz S, 2013, PATTERN RECOGN, V46, P1415, DOI 10.1016/j.patcog.2012.11.011
   Raghu M, 2019, Arxiv, DOI [arXiv:1902.07208, 10.48550/arXiv.1902.07208]
   Rajpurkar P, 2023, NEW ENGL J MED, V388, P1981, DOI 10.1056/NEJMra2301725
   Rajpurkar P, 2022, NAT MED, V28, P31, DOI 10.1038/s41591-021-01614-0
   Raju K. M. S., 2013, IOSR J. Comput. Eng. (IOSR-JCE), V15, P10, DOI https://doi.org/10.9790/0661-1541015
   Ramzan A, 2019, IET IMAGE PROCESS, V13, P409, DOI 10.1049/iet-ipr.2018.5396
   Rasti R, 2018, IEEE T MED IMAGING, V37, P1024, DOI 10.1109/TMI.2017.2780115
   Rawat W, 2017, NEURAL COMPUT, V29, P2352, DOI [10.1162/NECO_a_00990, 10.1162/neco_a_00990]
   Rong YB, 2019, IEEE J BIOMED HEALTH, V23, P253, DOI 10.1109/JBHI.2018.2795545
   Roy AG, 2017, BIOMED OPT EXPRESS, V8, P3627, DOI 10.1364/BOE.8.003627
   Sakata LM, 2009, CLIN EXP OPHTHALMOL, V37, P90, DOI 10.1111/j.1442-9071.2009.02015.x
   Salem Nema, 2020, 2020 Fourth International Conference on Multimedia Computing, Networking and Applications (MCNA), P66, DOI 10.1109/MCNA50957.2020.9264295
   Saracevic M, 2019, FUTURE GENER COMP SY, V100, P186, DOI 10.1016/j.future.2019.05.010
   Schmitt JM, 1999, J BIOMED OPT, V4, P95, DOI 10.1117/1.429925
   Schmitt JM, 1998, J OPT SOC AM A, V15, P2288, DOI 10.1364/JOSAA.15.002288
   Shin HC, 2016, IEEE T MED IMAGING, V35, P1285, DOI 10.1109/TMI.2016.2528162
   Singh Gurpreet., 2013, INT J COMPUTER APPL, V67, P33, DOI [DOI 10.5120/11507-7224, 10.5120/11507-7224]
   Srivastava A., 2017, Int. J. Latest Technol. Eng. Manag. Appl. Sci. (IJLTEMAS) VI, VVI
   Srivastava A, 2018, Handbook of Research on Advanced Concepts in Real-time Image and Video Processing, P281, DOI 10.4018/978-1-5225-2848-7.ch011
   Stanley RP., 2015, Catalan Numbers, DOI 10.1017/CBO9781139871495
   Sunija AP, 2021, COMPUT METH PROG BIO, V200, DOI 10.1016/j.cmpb.2020.105877
   Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P1299, DOI 10.1109/TMI.2016.2535302
   Ting DSW, 2019, BRIT J OPHTHALMOL, V103, P167, DOI 10.1136/bjophthalmol-2018-313173
   van Velthoven MEJ, 2007, PROG RETIN EYE RES, V26, P57, DOI 10.1016/j.preteyeres.2006.10.002
   Wang DP, 2019, IEEE PHOTONICS J, V11, DOI 10.1109/JPHOT.2019.2934484
   Wei W, 2023, SCI REP-UK, V13, DOI 10.1038/s41598-023-35414-y
   Weiss Karl, 2016, Journal of Big Data, V3, DOI 10.1186/s40537-016-0043-6
   Wen HJ, 2022, COMPUT METH PROG BIO, V220, DOI 10.1016/j.cmpb.2022.106832
   Yousef R, 2023, DIAGNOSTICS, V13, DOI 10.3390/diagnostics13091624
   Yu XJ, 2023, BIOMED OPT EXPRESS, V14, P2773, DOI 10.1364/BOE.481870
   Zhou YC, 2008, PROC SPIE, V6812, DOI 10.1117/12.766591
   Zou JC, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL 3, PROCEEDINGS, P965
NR 123
TC 0
Z9 0
U1 1
U2 1
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUN
PY 2024
VL 146
AR 105044
DI 10.1016/j.imavis.2024.105044
EA MAY 2024
PG 16
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA TH9S3
UT WOS:001240498700001
DA 2024-08-05
ER

PT J
AU Luo, PY
   Nie, J
   Xie, J
   Cao, JL
   Zhang, XH
AF Luo, Peiyun
   Nie, Jing
   Xie, Jin
   Cao, Jiale
   Zhang, Xiaohong
TI Localization-aware logit mimicking for object detection in adverse
   weather conditions
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Object detection; Adverse weather conditions; Knowledge distillation
AB Adverse weather conditions would decrease the image quality, leading to a sharp decline in detection accuracy. Most of the researches focus on object detection in fine weather conditions, rather than in adverse weather conditions. Recently, some methods attempt to reduce the gap between degraded images and clean images to improve the detection accuracy in adverse weather conditions. Specifically, these methods usually conduct image restoration and object detection in a sequential way or by joint learning. While these methods can improve detection accuracy to a certain extent, image restoration models may introduce noise or artifacts and increase computational burden, limiting the accuracy and efficiency of object detection in adverse weather conditions. In this paper, we propose a knowledge distillation-based method, Localization-aware Logit Mimicking (LaLM), to improve detection accuracy in adverse weather conditions by reducing the gap between degraded images and clean images at the prediction level, rather than the image level. Moreover, the localization quality is designed as the mimicking target to make the knowledge distillation more effective. Experiments conducted on three popular benchmarks (i.e., RTTS, ExDark, and RID) demonstrate that our LaLM can achieve the state-of-the-art detection accuracy and inference speed in foggy, rainy, and low-light conditions. Code is available at: https://github. com/VIPLab-CQU/LaLM.
C1 [Luo, Peiyun; Xie, Jin; Zhang, Xiaohong] Chongqing Univ, Sch Big Data & Software Engn, Chongqing 400044, Peoples R China.
   [Nie, Jing] Chongqing Univ, Sch Microelect & Commun Engn, Chongqing 400044, Peoples R China.
   [Cao, Jiale] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Cao, Jiale] Shanghai Artificial Intelligence Lab, Shanghai 200232, Peoples R China.
C3 Chongqing University; Chongqing University; Tianjin University
RP Xie, J (corresponding author), Chongqing Univ, Sch Big Data & Software Engn, Chongqing 400044, Peoples R China.; Nie, J (corresponding author), Chongqing Univ, Sch Microelect & Commun Engn, Chongqing 400044, Peoples R China.
EM luopeiyun@cqu.edu.cn; jingnie@cqu.edu.cn; xiejin@cqu.edu.cn;
   connor@tju.edu.cn; xhongz@cqu.edu.cn
RI Zhang, Xiaohong/A-3060-2015
FU National Key Research and Development Program of China [2022ZD0160404];
   Fundamental Research Funds for the Central Universities [2023CDJXY-036];
   National Natural Science Foundation of China [62206031, 62301092,
   62271346]; China Postdoctoral Science Foundation [2021M700613,
   2022M720581, 2023T160762]; Tianjin Natural Science Foundation
   [21JCQNJC00420]
FX <BOLD> This work was supported in part by National Key Research and
   Development Program of China (Grant No. 2022ZD0160404) , in part by the
   Fundamental Research Funds for the Central Universities (Grant No.
   </BOLD> 2023CDJXY-036) , in part by National Natural Science Foundation
   of China (Grant Nos. 62206031, 62301092, 62271346) , in part by China
   Postdoctoral Science Foundation (Grant Nos. 2021M700613, 2022M720581,
   2023T160762) , and in part by Tianjin Natural Science Foundation (Grant
   No. 21JCQNJC00420) .
CR Ahn S, 2019, PROC CVPR IEEE, P9155, DOI 10.1109/CVPR.2019.00938
   Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644
   Cao W., 2022, Adv Neural Inf Process Syst, V35, P15394
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen K, 2019, Arxiv, DOI arXiv:1906.07155
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dai JF, 2016, ADV NEUR IN, V29
   Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Hu XW, 2021, IEEE T IMAGE PROCESS, V30, P1759, DOI 10.1109/TIP.2020.3048625
   Hu XW, 2019, PROC CVPR IEEE, P8014, DOI 10.1109/CVPR.2019.00821
   Huang SC, 2021, IEEE T PATTERN ANAL, V43, P2623, DOI 10.1109/TPAMI.2020.2977911
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Li B., 2017, An all-in-one network for dehazing and beyond, V14, P1, DOI [10.1109/ICCV.2017.511, DOI 10.1109/ICCV.2017.511]
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li CY, 2022, IEEE T PATTERN ANAL, V44, P4225, DOI 10.1109/TPAMI.2021.3063604
   Li QQ, 2017, PROC CVPR IEEE, P7341, DOI 10.1109/CVPR.2017.776
   Li SY, 2019, PROC CVPR IEEE, P3833, DOI 10.1109/CVPR.2019.00396
   Li X., 2020, ADV NEURAL INF PROCE, V33, P21002
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu WY, 2022, AAAI CONF ARTIF INTE, P1792
   Liu XH, 2019, IEEE I CONF COMP VIS, P7313, DOI 10.1109/ICCV.2019.00741
   Loh YP, 2019, COMPUT VIS IMAGE UND, V178, P30, DOI 10.1016/j.cviu.2018.10.010
   Ma L, 2022, PROC CVPR IEEE, P5627, DOI 10.1109/CVPR52688.2022.00555
   Nie J, 2022, IEEE T CIRC SYST VID, V32, P3334, DOI 10.1109/TCSVT.2021.3105685
   Park W, 2019, PROC CVPR IEEE, P3962, DOI 10.1109/CVPR.2019.00409
   Pei YT, 2018, LECT NOTES COMPUT SC, V11214, P697, DOI 10.1007/978-3-030-01249-6_42
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Song YD, 2022, Arxiv, DOI arXiv:2204.03883
   Taeyoung Son, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P749, DOI 10.1007/978-3-030-58545-7_43
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Wang JF, 2021, PROC CVPR IEEE, P15844, DOI 10.1109/CVPR46437.2021.01559
   Wang T, 2019, PROC CVPR IEEE, P4928, DOI 10.1109/CVPR.2019.00507
   Wang WH, 2022, COMPUT VIS MEDIA, V8, P415, DOI 10.1007/s41095-022-0274-8
   Wu WH, 2022, PROC CVPR IEEE, P5891, DOI 10.1109/CVPR52688.2022.00581
   Xu XG, 2022, PROC CVPR IEEE, P17693, DOI 10.1109/CVPR52688.2022.01719
   Yang Z., 2022, arXiv
   Yang ZD, 2022, LECT NOTES COMPUT SC, V13671, P53, DOI 10.1007/978-3-031-20083-0_4
   Yang ZD, 2022, PROC CVPR IEEE, P4633, DOI 10.1109/CVPR52688.2022.00460
   Zhang Linfeng, 2021, INT C LEARN REPR
   Zhang Z., 2020, P AS C COMP VIS 2020
   Zhang Z, 2022, PROC CVPR IEEE, P1889, DOI 10.1109/CVPR52688.2022.00194
   Zhao BR, 2022, PROC CVPR IEEE, P11943, DOI 10.1109/CVPR52688.2022.01165
   Zheng ZH, 2022, PROC CVPR IEEE, P9397, DOI 10.1109/CVPR52688.2022.00919
   Zhu YC, 2023, PROC CVPR IEEE, P19723, DOI 10.1109/CVPR52729.2023.01889
NR 51
TC 0
Z9 0
U1 5
U2 5
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUN
PY 2024
VL 146
AR 105035
DI 10.1016/j.imavis.2024.105035
EA APR 2024
PG 10
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA SU6K9
UT WOS:001236998200001
DA 2024-08-05
ER

PT J
AU Umer, M
   Alarfaj, AA
   Alabdulqader, EA
   Alsubai, S
   Cascone, L
   Narducci, F
AF Umer, Muhammad
   Alarfaj, Aisha Ahmed
   Alabdulqader, Ebtisam Abdullah
   Alsubai, Shtwai
   Cascone, Lucia
   Narducci, Fabio
TI Enhancing fall prediction in the elderly people using LBP features and
   transfer learning model
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE NASNet; Fall detection; Local binary patterns; Healthcare
AB In an era where the detection and prevention of falls are crucial for the well-being of elderly and vulnerable individuals, achieving high accuracy in identifying such incidents is of utmost importance. This study introduces a modified NASNet, a novel transformer learning model designed for the classification of fall and not fall people based on local binary patterns (LBP) features. The modified NASNet model demonstrates high performance, achieving an accuracy, precision, recall, and F1 score of 99% with LBP features in differentiating fall from not fall. To assess the performance of NASNet, this research work conducted a comparative analysis with other prominent deep learning, and transfer learning models, as well as state-of-the-art machine learning frameworks. The findings indicate NASNet's enhanced performance in fall detection, highlighting its potential for application in real-time fall detection systems, contributing to a safer environment for individuals at risk. This research sets the stage for enhanced healthcare and assistance for vulnerable populations, addressing a critical concern in the healthcare sector.
C1 [Umer, Muhammad] Islamia Univ Bahawalpur, Dept Comp Sci & Informat Technol, Bahawalpur 63100, Pakistan.
   [Alarfaj, Aisha Ahmed] Princess Nourah Bint Abdulrahman Univ, Coll Comp & Informat Sci, Dept Informat Syst, POB 84428, Riyadh 11671, Saudi Arabia.
   [Alabdulqader, Ebtisam Abdullah] King Saud Univ, Coll Comp & Informat Sci, Dept Informat Technol, Riyadh, Saudi Arabia.
   [Alsubai, Shtwai] Prince Sattam Bin Abdulaziz Univ, Coll Comp Engn & Sci, Dept Comp Sci, POB 151, Al Kharj 11942, Saudi Arabia.
   [Cascone, Lucia; Narducci, Fabio] Univ Salerno, Dept Comp Sci, Fisciano, Italy.
C3 Islamia University of Bahawalpur; Princess Nourah bint Abdulrahman
   University; King Saud University; Prince Sattam Bin Abdulaziz
   University; University of Salerno
RP Umer, M (corresponding author), Islamia Univ Bahawalpur, Dept Comp Sci & Informat Technol, Bahawalpur 63100, Pakistan.
EM umersabir1996@gmail.com; Aiaalarfaj@pnu.edu.sa; eabdulqader@ksu.edu.sa;
   Sa.alsubai@psau.edu.sa; lcascone@unisa.it; fnarducci@unisa.it
RI Narducci, Fabio/R-5833-2017; AlArfaj, Aisha/HJI-8859-2023; Alsubai,
   Shtwai/ABW-9013-2022; Alabdulqader, Ebtisam/AAX-3879-2021
OI Narducci, Fabio/0000-0003-4879-7138; AlArfaj, Aisha/0000-0002-5078-2579;
   Alsubai, Shtwai/0000-0002-6584-7400; Alabdulqader,
   Ebtisam/0000-0002-8539-5560
FU Princess Nourah bint Abdulrahman University, Riyadh, Saudi Arabia
   [PNURSP2024R348]
FX Princess Nourah bint Abdulrahman University Researchers Supporting
   Project number (PNURSP2024R348), Princess Nourah bint Abdulrahman
   University, Riyadh, Saudi Arabia.
CR Adedoja A., 2019, 2019 INT C ADV BIG D, P1, DOI 10.1109/ICABCD.2019.8851029
   Agrawal DK, 2023, IEEE ACCESS, V11, P23119, DOI 10.1109/ACCESS.2023.3252886
   Aicha AN, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18051654
   Alam E, 2022, COMPUT BIOL MED, V146, DOI 10.1016/j.compbiomed.2022.105626
   Alturki N, 2023, CANCERS, V15, DOI 10.3390/cancers15061767
   Analytical Impact Team, 2021, Overview of the UK Population: January 2021,
   [Anonymous], 2018, FALLS
   Badgujar Sejal, 2020, 2020 11 INT C COMP C, P1
   Beswick AD, 2010, REV CLIN GERONTOL, V20, P128, DOI 10.1017/S0959259810000079
   Burns E, 2018, MMWR-MORBID MORTAL W, V67, P509, DOI 10.15585/mmwr.mm6718a1
   Cascone L, 2023, BIG DATA RES, V31, DOI 10.1016/j.bdr.2022.100360
   Chandak A, 2022, COMPUT INTEL NEUROSC, V2022, DOI 10.1155/2022/9626170
   Chu Y, 2023, IEEE ACCESS, V11, P83763, DOI 10.1109/ACCESS.2023.3300726
   Ferreira Lidia, 2022, Revista Gaucha de Enfermagem, V43
   Fleming J, 2008, BRIT MED J, V337, DOI 10.1136/bmj.a2227
   Florence CS, 2018, J AM GERIATR SOC, V66, P693, DOI 10.1111/jgs.15304
   Hearst MA, 1998, IEEE INTELL SYST APP, V13, P18, DOI 10.1109/5254.708428
   Hemmatpour M, 2017, P INT COMP SOFTW APP, P973, DOI 10.1109/COMPSAC.2017.189
   Hussain Faisal., 2019, An efficient machine learning-based elderly fall detection algorithm
   Juna A, 2022, WATER-SUI, V14, DOI 10.3390/w14172592
   Karagiannakos Sergios, 2022, Neural architecture search (nas): Basic principles and different approaches
   Kelsey JL, 2012, AM J PUBLIC HEALTH, V102, P2149, DOI 10.2105/AJPH.2012.300677
   Madni HA, 2023, WATER-SUI, V15, DOI 10.3390/w15030475
   Martinez Fredy, 2020, Int. J. Adv. Sci. Eng. Inf. Technol., V10, P662
   Mujahid M, 2022, DIAGNOSTICS, V12, DOI 10.3390/diagnostics12051280
   Narra M, 2022, IEEE ACCESS, V10, P98724, DOI 10.1109/ACCESS.2022.3206963
   Peng C, 2021, IEEE T NEUR NET LEAR, V32, P2595, DOI 10.1109/TNNLS.2020.3006877
   Rasche Andreas., 2010, The United Nations Global Compact: Achievements, Trends and Challenges
   Rosdi BA, 2011, SENSORS-BASEL, V11, P11357, DOI 10.3390/s111211357
   Rubenstein LZ, 2006, AGE AGEING, V35, P37, DOI 10.1093/ageing/afl084
   Salim F, 2023, ELECTRONICS-SWITZ, V12, DOI 10.3390/electronics12143132
   Shahzad A, 2019, IEEE T IND INFORM, V15, P35, DOI 10.1109/TII.2018.2839749
   UTTEJ KUMAR KANDAGATLA, dataset
   Waheed M, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21062006
   Wang ZY, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9224890
   Zhu J, 2009, STAT INTERFACE, V2, P349
   Zi X, 2023, ELECTRONICS-SWITZ, V12, DOI 10.3390/electronics12051259
   Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907
   Zulfiqar F, 2023, BIOMED SIGNAL PROCES, V84, DOI 10.1016/j.bspc.2023.104777
   Zurbuchen N, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21030938
NR 40
TC 1
Z9 1
U1 6
U2 6
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD MAY
PY 2024
VL 145
AR 104992
DI 10.1016/j.imavis.2024.104992
EA MAR 2024
PG 9
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA PK9M2
UT WOS:001214089900001
DA 2024-08-05
ER

PT J
AU Irene, S
   Prakash, AJ
   Uthariaraj, VR
AF Irene, S.
   Prakash, A. John
   Uthariaraj, V. Rhymend
TI Person search over security video surveillance systems using deep
   learning methods: A review
SO IMAGE AND VISION COMPUTING
LA English
DT Review
DE Person search; Person retrieval; Deep learning; Person re
   -identification; Text based person search; Feature representation
ID REIDENTIFICATION; ATTRIBUTE; NETWORK; ALIGNMENT
AB Person search has become one of the most critical and challenging applications in today's video surveillance systems. It helps in locating a person in surveillance videos, which is plausible only with advanced deep learning models, large scale datasets and high compute power GPUs. This survey features exhaustive analysis of deep learning based person search through image, textual and attributes based description. The image based person search is reviewed based on aspects such as region proposal consideration, feature representation, context information and compute complexity. The text based person search is reviewed based on the aspects of feature representation and alignment. The attribute based person search is reviewed based on the aspect of high level and low level feature representation. The paper summarizes more than 100 research works and provides future perspectives for enhancements with the objective of guiding and facilitating the development of better solutions in future. We believe that this exclusive review on deep learning based person search for video surveillance security systems will facilitate better systematization.
C1 [Irene, S.] Ctr Dev Adv Comp, Chennai, India.
   [Prakash, A. John; Uthariaraj, V. Rhymend] Anna Univ, Ramanujan Comp Ctr, Chennai, India.
C3 Centre for Development of Advanced Computing (C-DAC); Anna University;
   Anna University Chennai
RP Prakash, AJ (corresponding author), Anna Univ, Ramanujan Comp Ctr, Chennai, India.
EM johnprakash@annauniv.edu
OI A, John Prakash/0000-0002-3772-8201
FU Ministry of Electronics and Information Technology (MeitY) , Government
   of India
FX The first author acknowledges the support by Ministry of Electronics and
   Information Technology (MeitY) , Government of India and her mentor Mr.
   R. Pitchiah.
CR Aggarwal S, 2020, IEEE WINT CONF APPL, P2606, DOI [10.1109/wacv45572.2020.9093640, 10.1109/WACV45572.2020.9093640]
   Ahmed E, 2015, PROC CVPR IEEE, P3908, DOI 10.1109/CVPR.2015.7299016
   [Anonymous], 2019, Wider Person Search by Language Dataset
   Bahdanau D, 2016, Arxiv, DOI arXiv:1409.0473
   Bai Y, 2023, Arxiv, DOI arXiv:2305.13653
   Behera NKS, 2022, IMAGE VISION COMPUT, V122, DOI 10.1016/j.imavis.2022.104432
   Behera NKS, 2020, PATTERN RECOGN LETT, V138, P282, DOI 10.1016/j.patrec.2020.07.030
   Bewley A, 2016, IEEE IMAGE PROC, P3464, DOI 10.1109/ICIP.2016.7533003
   Bhardwaj R, 2022, PROCEEDINGS OF THE 19TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION (NSDI '22), P119
   Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339
   Cao JL, 2022, PROC CVPR IEEE, P9448, DOI 10.1109/CVPR52688.2022.00924
   Caron M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9630, DOI 10.1109/ICCV48922.2021.00951
   Chang X, 2022, ECCV WORKSHOP REAL W
   Chen D., 2020, IEEE C COMPUTER VISI
   Chen DP, 2018, LECT NOTES COMPUT SC, V11220, P56, DOI 10.1007/978-3-030-01270-0_4
   Chen D, 2018, LECT NOTES COMPUT SC, V11211, P764, DOI 10.1007/978-3-030-01234-2_45
   Chen JX, 2021, PROC CVPR IEEE, P8142, DOI 10.1109/CVPR46437.2021.00805
   Chen LQ, 2021, IEEE IJCNN, DOI 10.1109/IJCNN52387.2021.9533620
   Chen TL, 2018, IEEE WINT CONF APPL, P1879, DOI 10.1109/WACV.2018.00208
   Chen W, 2022, Arxiv, DOI arXiv:2101.11282
   Chen WH, 2023, PROC CVPR IEEE, P15050, DOI 10.1109/CVPR52729.2023.01445
   Chen XL, 2020, Arxiv, DOI arXiv:2003.04297
   Chen Y, 2022, IMAGE VISION COMPUT, V128, DOI 10.1016/j.imavis.2022.104587
   Chen YC, 2021, IEEE T IMAGE PROCESS, V30, P4057, DOI 10.1109/TIP.2021.3068825
   Chen YH, 2022, NEUROCOMPUTING, V494, P171, DOI 10.1016/j.neucom.2022.04.081
   Cheng Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11949, DOI 10.1109/CVPR42600.2020.01197
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Ci YZ, 2023, PROC CVPR IEEE, P17840, DOI 10.1109/CVPR52729.2023.01711
   Deng YB, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P789, DOI 10.1145/2647868.2654966
   Denman S., P 2012 INT C DIGITAL
   Denman S, 2015, PATTERN RECOGN LETT, V68, P306, DOI 10.1016/j.patrec.2015.06.015
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Ding ZF, 2021, Arxiv, DOI arXiv:2107.12666
   Dong Q, 2019, IEEE I CONF COMP VIS, P3651, DOI 10.1109/ICCV.2019.00375
   Dong WK, 2020, PROC CVPR IEEE, P2836, DOI 10.1109/CVPR42600.2020.00291
   Dong WK, 2020, PROC CVPR IEEE, P2582, DOI 10.1109/CVPR42600.2020.00266
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Dubey SR, 2022, IEEE T CIRC SYST VID, V32, P2687, DOI 10.1109/TCSVT.2021.3080920
   Farooq A, 2022, AAAI CONF ARTIF INTE, P4477
   Feng Deying, 2022, ICCAI '22: Proceedings of the 8th International Conference on Computing and Artificial Intelligence, P625, DOI 10.1145/3532213.3532309
   Frikha M, 2021, PROCEDIA COMPUT SCI, V192, P90, DOI 10.1016/j.procs.2021.08.010
   Frikha M, 2017, J ELECTRON IMAGING, V26, DOI 10.1117/1.JEI.26.5.051405
   Fu DP, 2021, PROC CVPR IEEE, P14745, DOI 10.1109/CVPR46437.2021.01451
   Galiyawala H, 2021, 2021 17TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS 2021), DOI 10.1109/AVSS52988.2021.9663775
   Galiyawala H, 2021, MULTIMED TOOLS APPL, V80, P27343, DOI 10.1007/s11042-021-10983-0
   Galiyawala H, 2019, IMAGE VISION COMPUT, V92, DOI 10.1016/j.imavis.2019.10.002
   Galiyawala H, 2018, 2018 15TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), P471
   Gao CY, 2021, Arxiv, DOI arXiv:2101.03036
   Gao G., 2022, Signal Processing, Image Communication, P101
   Gao LY, 2023, IEEE T CIRC SYST VID, V33, P7884, DOI 10.1109/TCSVT.2023.3273719
   Gao LY, 2021, PROCEEDINGS OF THE 2021 INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR '21), P118, DOI 10.1145/3460426.3463652
   Ge J., 2019, arXiv
   Ge J, 2023, MULTIMEDIA SYST, V29, P3081, DOI 10.1007/s00530-022-00914-w
   Gong YP, 2022, Arxiv, DOI arXiv:2101.08533
   Gou MR, 2017, IEEE COMPUT SOC CONF, P1425, DOI 10.1109/CVPRW.2017.185
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Hadsell R., 2006, Computer vision and pattern recognition, 2006 IEEE computer society conference on, V2, P1735, DOI DOI 10.1109/CVPR.2006.100
   Halstead M., 2018, 2018 IEEE INT C ADV
   Halstead M, 2014, INT C PATT RECOG, P4501, DOI 10.1109/ICPR.2014.770
   Han B.J., 2021, arXiv
   Han BJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P905, DOI 10.1109/ICCV48922.2021.00096
   Han C., 2021, P IEEECVF INT C COMP, P12006
   Han CC, 2023, IEEE T PATTERN ANAL, V45, P7319, DOI 10.1109/TPAMI.2022.3221079
   Han CC, 2021, AAAI CONF ARTIF INTE, V35, P1505
   Han CC, 2019, IEEE I CONF COMP VIS, P9813, DOI 10.1109/ICCV.2019.00991
   Han Xiao, 2021, BMVC
   He H., 2020, P IEEE CVF C COMP VI, P9729
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He LX, 2020, Arxiv, DOI arXiv:2006.02631
   He ST, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14993, DOI 10.1109/ICCV48922.2021.01474
   He ZW, 2019, LECT NOTES COMPUT SC, V11362, P349, DOI 10.1007/978-3-030-20890-5_23
   Herzog F, 2021, IEEE IMAGE PROC, P1129, DOI 10.1109/ICIP42928.2021.9506733
   Hinton G., 2015, NEURIPS DEEP LEARN R, P9
   Hu WY, 2023, MULTIMEDIA SYST, V29, P3105, DOI 10.1007/s00530-022-00929-3
   Huang WX, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3565886
   Iandola F, 2014, Arxiv, DOI arXiv:1404.1869
   Iodice S., 2020, BRIT MACHINE VISION
   Islam K, 2020, IMAGE VISION COMPUT, V101, DOI 10.1016/j.imavis.2020.103970
   Jaffe L, 2023, IEEE WINT CONF APPL, P1684, DOI 10.1109/WACV56688.2023.00173
   Jeong B., 2021, P IEEECVF INT C COMP, P12016
   Ji Z, 2020, IEEE INTERNET THINGS, V7, P11147, DOI 10.1109/JIOT.2020.2995148
   Jia C., 2021, ARXIV
   Jia J, 2021, Arxiv, DOI arXiv:2107.03576
   Jiang D, 2023, PROC CVPR IEEE, P2787, DOI 10.1109/CVPR52729.2023.00273
   Jing Y., 2020, P IEEECVF C COMPUTER
   Jing Y, 2020, AAAI CONF ARTIF INTE, V34, P11189
   Ke X, 2022, IEEE T CIRC SYST VID, V32, P7924, DOI 10.1109/TCSVT.2022.3188551
   Khan A, 2020, ARTIF INTELL REV, V53, P5455, DOI 10.1007/s10462-020-09825-6
   Khosla P., 2020, Supervised contrastive learning, V33, P18661
   Ktena Sofia Ira, 2017, Medical Image Computing and Computer Assisted Intervention  MICCAI 2017. 20th International Conference. Proceedings: LNCS 10433, P469, DOI 10.1007/978-3-319-66182-7_54
   Lan X, 2018, LECT NOTES COMPUT SC, V11205, P553, DOI 10.1007/978-3-030-01246-5_33
   Lavi B, 2020, Arxiv, DOI [arXiv:2005.00355, 10.48550/arXiv.2005.00355]
   Leng QM, 2020, IEEE T CIRC SYST VID, V30, P1092, DOI 10.1109/TCSVT.2019.2898940
   Li D, 2022, ACM T INFORM SYST, V40, DOI 10.1145/3480967
   Li DW, 2016, Arxiv, DOI arXiv:1603.07054
   Li DW, 2019, IEEE T IMAGE PROCESS, V28, P1575, DOI 10.1109/TIP.2018.2878349
   Li H, 2022, IEEE T CIRC SYST VID, V32, P1624, DOI 10.1109/TCSVT.2021.3073718
   Li JH, 2021, ADV NEUR IN, V34
   Li S., 2023, P AAAI C ARTIFICIAL, P1405
   Li SP, 2022, INT CONF ACOUST SPEE, P2724, DOI 10.1109/ICASSP43922.2022.9746846
   Li S, 2017, PROC CVPR IEEE, P5187, DOI 10.1109/CVPR.2017.551
   Li W, 2021, PATTERN RECOGN, V114, DOI 10.1016/j.patcog.2021.107862
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Li XQ, 2021, NEUROCOMPUTING, V452, P675, DOI 10.1016/j.neucom.2020.07.139
   Li Y, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20185279
   Li ZJ, 2021, AAAI CONF ARTIF INTE, V35, P2011
   Lin YT, 2019, PATTERN RECOGN, V95, P151, DOI 10.1016/j.patcog.2019.06.006
   Liu C, 2021, Arxiv, DOI arXiv:2112.00527
   Liu C, 2022, PATTERN RECOGN, V127, DOI 10.1016/j.patcog.2022.108654
   Liu C, 2021, NEUROCOMPUTING, V465, P184, DOI 10.1016/j.neucom.2021.08.136
   Liu F, 2023, Arxiv, DOI arXiv:2308.10658
   Liu H, 2017, IEEE I CONF COMP VIS, P493, DOI 10.1109/ICCV.2017.61
   Liu JW, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3450, DOI 10.1145/3394171.3413878
   Liu JW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P665, DOI 10.1145/3343031.3350991
   Liu M., 2022, Multimed. Tools Appl., P1
   Liu XH, 2017, IEEE I CONF COMP VIS, P350, DOI 10.1109/ICCV.2017.46
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu Z, 2022, PROC CVPR IEEE, P11966, DOI 10.1109/CVPR52688.2022.01167
   Loper E., 2002, arXiv
   Luo H., 2021, arXiv
   Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190
   Lv N, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P1960, DOI 10.1109/ICASSP39728.2021.9413460
   Ma JY, 2021, INT J COMPUT VISION, V129, DOI 10.1007/s11263-020-01359-2
   Manning CD, 2014, PROCEEDINGS OF 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: SYSTEM DEMONSTRATIONS, P55, DOI 10.3115/v1/p14-5010
   Mao JZ, 2023, IEEE T MULTIMEDIA, V25, P1592, DOI 10.1109/TMM.2023.3265159
   Martinho-Corbishley D, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON IDENTITY, SECURITY AND BEHAVIOR ANALYSIS (ISBA)
   Miao JX, 2019, IEEE I CONF COMP VIS, P542, DOI 10.1109/ICCV.2019.00063
   Mnih V, 2014, ADV NEUR IN, V27
   Moon H, 2001, PERCEPTION, V30, P303, DOI 10.1068/p2896
   Munjal B., 2019, arXiv
   Munjal B, 2019, PROC CVPR IEEE, P811, DOI 10.1109/CVPR.2019.00090
   Kipf TN, 2017, Arxiv, DOI arXiv:1609.02907
   Ni XY, 2021, EUR W VIS INF PROCES, DOI 10.1109/EUVIP50544.2021.9484010
   Ni XY, 2021, INT C PATT RECOG, P9601, DOI 10.1109/ICPR48806.2021.9412481
   Niu K, 2023, IEEE T IMAGE PROCESS, V32, P3429, DOI 10.1109/TIP.2023.3285426
   Niu K, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4032, DOI 10.1145/3394171.3413895
   Niu K, 2020, IEEE T IMAGE PROCESS, V29, P5542, DOI 10.1109/TIP.2020.2984883
   Pang YW, 2019, IEEE I CONF COMP VIS, P4966, DOI 10.1109/ICCV.2019.00507
   Parshwa Shah V.G., 2021, IEEE WINTER C APPL C
   Peng Y., 2023, ACM Transactions on Multimedia Computing, Communications and Applications
   Qin J., 2023, ICASSP 2023 2023 IEE, P1
   Qu LG, 2023, Arxiv, DOI arXiv:2304.12570
   Radford A, 2021, PR MACH LEARN RES, V139
   Redmon J., 2018, CoRR
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Schumann A, 2018, 2018 15TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), P465
   Shi YX, 2022, NEUROCOMPUTING, V486, P237, DOI 10.1016/j.neucom.2021.11.038
   Shi YX, 2021, IEEE T MULTIMEDIA, V23, P4376, DOI 10.1109/TMM.2020.3042068
   Shoitan Rasha, 2023, Alex. Eng. J., V63, P454
   Shree V, 2020, IEEE ROBOT AUTOM LET, V5, P1851, DOI 10.1109/LRA.2020.2969921
   Shu Xiujun, 2022, PROC EUROPEAN C COMP
   Siebers P, 2022, IEEE ACCESS, V10, P96492, DOI 10.1109/ACCESS.2022.3205719
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Somers V, 2023, IEEE WINT CONF APPL, P1613, DOI 10.1109/WACV56688.2023.00166
   Specker A, 2023, IEEE WINT CONF APPL, P981, DOI 10.1109/WACV56688.2023.00104
   Specker A, 2022, IEEE WINT CONF APPL, P570, DOI 10.1109/WACVW54805.2022.00063
   Stefan LD, 2020, 2020 13TH INTERNATIONAL CONFERENCE ON COMMUNICATIONS (COMM), P303, DOI [10.1109/comm48946.2020.9141958, 10.1109/COMM48946.2020.9141958]
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Sun M, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P3628, DOI 10.1145/3503161.3548274
   Sun XX, 2019, PROC CVPR IEEE, P608, DOI 10.1109/CVPR.2019.00070
   Sun Y, 2014, ADV NEUR IN, V27
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Sun YF, 2020, PROC CVPR IEEE, P6397, DOI 10.1109/CVPR42600.2020.00643
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Suo W, 2022, LECT NOTES COMPUT SC, V13695, P726, DOI 10.1007/978-3-031-19833-5_42
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tan FW, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12085, DOI 10.1109/ICCV48922.2021.01189
   Tan HL, 2020, IEEE ACCESS, V8, P63632, DOI 10.1109/ACCESS.2020.2984915
   Tang SX, 2023, PROC CVPR IEEE, P21970, DOI 10.1109/CVPR52729.2023.02104
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang CJ, 2022, NEURAL COMPUT APPL, V34, P5625, DOI 10.1007/s00521-021-06734-9
   Wang CJ, 2021, NEUROCOMPUTING, V463, P388, DOI 10.1016/j.neucom.2021.08.058
   Wang GC, 2019, AAAI CONF ARTIF INTE, P8933
   Wang GS, 2023, Arxiv, DOI arXiv:2303.04497
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang H., 2023, Image Vis. Comput.
   Wang JY, 2018, PROC CVPR IEEE, P2275, DOI 10.1109/CVPR.2018.00242
   Wang XD, 2023, IEEE T MULTIMEDIA, V25, P9597, DOI 10.1109/TMM.2023.3256092
   Wang XD, 2022, IEEE T CYBERNETICS, V52, P13293, DOI 10.1109/TCYB.2021.3130047
   Wang X, 2019, PROC CVPR IEEE, P5017, DOI 10.1109/CVPR.2019.00516
   Wang Z., 2020, Lecture Notes in Computer Science, P12357
   Wang Z, 2020, Arxiv, DOI arXiv:1905.10048
   Wei DL, 2023, Arxiv, DOI arXiv:2304.02278
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Wu D, 2019, NEUROCOMPUTING, V337, P354, DOI 10.1016/j.neucom.2019.01.079
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Xia X, 2022, Arxiv, DOI arXiv:2205.09579
   Xiang XZ, 2022, INT CONF ACOUST SPEE, P2729, DOI 10.1109/ICASSP43922.2022.9747425
   Xiao JM, 2019, PATTERN RECOGN, V87, P332, DOI 10.1016/j.patcog.2018.10.028
   Xiao T, 2017, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2017.360
   Xiao Tong, 2016, ARXIV160401850, V2, P2
   Yaguchi T, 2018, 2018 15TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), P459
   Yamaguchi M, 2017, Arxiv, DOI arXiv:1704.07945
   Yan S, 2023, IEEE Trans. Image Process.
   Yan YC, 2022, AAAI CONF ARTIF INTE, P3027
   Yan YC, 2021, PROC CVPR IEEE, P7686, DOI 10.1109/CVPR46437.2021.00760
   Yan YC, 2019, PROC CVPR IEEE, P2153, DOI 10.1109/CVPR.2019.00226
   Yang JY, 2022, PROC CVPR IEEE, P15650, DOI 10.1109/CVPR52688.2022.01522
   Yang SY, 2023, Arxiv, DOI arXiv:2306.02898
   Yang X, 2024, IEEE T MULTIMEDIA, V26, P2493, DOI 10.1109/TMM.2023.3297391
   Yang Z, 2019, IEEE I CONF COMP VIS, P9656, DOI 10.1109/ICCV.2019.00975
   Yao HT, 2021, IEEE T IMAGE PROCESS, V30, P685, DOI 10.1109/TIP.2020.3038347
   Yao R, 2020, PATTERN RECOGN, V104, DOI 10.1016/j.patcog.2020.107350
   Yao Y., 2020, P COMP VIS ECCV 2020, P775
   Yao Y, 2023, PROC CVPR IEEE, P15568, DOI 10.1109/CVPR52729.2023.01494
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Yu Jiahui, 2016, P 24 ACM INT C MULT, P516
   Yu X., 2019, P IEEECVF INT C COMP
   Yu-Tong Cao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P230, DOI 10.1007/978-3-030-58568-6_14
   Zang XH, 2021, IMAGE VISION COMPUT, V116, DOI 10.1016/j.imavis.2021.104330
   Zha ZJ, 2020, IEEE T MULTIMEDIA, V22, P1836, DOI 10.1109/TMM.2020.2972168
   Zhang P., 2021, MM 20 P 28 ACM INT C, P1, DOI [10.1145/3444685.3446314, DOI 10.1145/3444685.3446314]
   Zhang SZ, 2023, Arxiv, DOI arXiv:2109.12965
   Zhang TY, 2021, PROC CVPR IEEE, P11501, DOI 10.1109/CVPR46437.2021.01134
   Zhang XM, 2020, Arxiv, DOI arXiv:2012.07620
   Zhang Y., 2023, IEEE Transactions on Multimedia, DOI [10.48550/arXiv.2306.08792, DOI 10.48550/ARXIV.2306.08792]
   Zhang YQ, 2021, IEEE T CYBERNETICS, V51, P5093, DOI 10.1109/TCYB.2019.2916158
   Zhang Y, 2018, LECT NOTES COMPUT SC, V11205, P707, DOI 10.1007/978-3-030-01246-5_42
   Zhao R, 2013, IEEE I CONF COMP VIS, P2528, DOI 10.1109/ICCV.2013.314
   Zheng DY, 2020, SIGNAL PROCESS-IMAGE, V86, DOI 10.1016/j.image.2020.115876
   Zheng KC, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3441, DOI 10.1145/3394171.3413864
   Zheng L, 2016, Arxiv, DOI arXiv:1610.02984
   Zheng L, 2017, PROC CVPR IEEE, P3346, DOI 10.1109/CVPR.2017.357
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng P., 2021, Global-local context network for person search
   Zheng ZD, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3159171
   Zheng ZD, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3383184
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong X, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P4250, DOI 10.1109/ICASSP39728.2021.9415107
   Zhong YJ, 2020, PROC CVPR IEEE, P6826, DOI 10.1109/CVPR42600.2020.00686
   Zhou JF, 2023, KNOWL-BASED SYST, V262, DOI 10.1016/j.knosys.2023.110253
   Zhou KY, 2019, Arxiv, DOI arXiv:1910.10093
   Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380
   Zhou WA, 2017, Arxiv, DOI arXiv:1706.06064
   Zhou Y., 2022, IEEE Transactions on Multimedia, DOI [10.48550/arXiv.2105.01447, DOI 10.48550/ARXIV.2105.01447]
   Zhu AC, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P209, DOI 10.1145/3474085.3475369
   Zhu J, 2021, DISPLAYS, V69, DOI 10.1016/j.displa.2021.102039
   Zhu Z, 2020, VIEWPOINT AWARE LOSS
   Zhuo JX, 2018, IEEE INT CON MULTI
   Zuo JL, 2023, Arxiv, DOI arXiv:2305.08386
NR 244
TC 1
Z9 1
U1 12
U2 12
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD MAR
PY 2024
VL 143
AR 104930
DI 10.1016/j.imavis.2024.104930
EA FEB 2024
PG 30
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA ML6U8
UT WOS:001193824200001
DA 2024-08-05
ER

PT J
AU Hao, F
   Zhong, FJ
   Yu, H
   Hu, J
   Yang, Y
AF Hao, Feng
   Zhong, Fujin
   Yu, Hong
   Hu, Jun
   Yang, Yan
TI STAFFormer: Spatio-temporal adaptive fusion transformer for efficient 3D
   human pose estimation
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE 3D human pose estimation; Spatio -temporal transformer; Attention
   fusion; Kinematic coherence loss; Integrated pre -training
ID SPATIAL-TEMPORAL FUSION; NETWORKS; GRAPH
AB Existing two-stage methods for 3D Human Pose Estimation often use 2D poses as input, which are then lifted to obtain 3D representations. This typically involves frame-by-frame estimation, whether in 2D or 3D, resulting in high computational demands unsuitable for edge devices. Due to the continuity of human movement, the differences between adjacent frames can be minimal, a question arises: is frame-by-frame estimation necessary? Previous works demonstrated the feasibility of using Transformer-based models to estimate poses with sparse frames, focusing on either temporal or spatial dependencies but neglecting holistic spatio-temporal correlations. To address this, we introduce the Spatio-Temporal Adaptive Fusion Transformer (STAFFormer). First, STAFFormer recovers dense temporal frames from sparsely sampled ones obtained from a 2D pose estimator through the Temporal Dense Frame Recovery (TDFR) module. This significantly reduces the computational complexity. Second, STAFFormer employs an adaptive fusion attention mechanism, enhancing accuracy by attentively navigating both spatial and temporal dimensions through the Spatio-Temporal Adaptive Fusion (STAF) module. Furthermore, We introduce a kinematic coherence loss to adapt to subtle joint movements, improving pose estimation fidelity. Finally, we explore the possibility of integrating different pre-training strategies using extensive marker-based datasets. Experimental results on challenging datasets show our network achieves stateof-the-art performance with low computational complexity.
C1 [Hao, Feng; Zhong, Fujin; Yu, Hong; Hu, Jun; Yang, Yan] Chongqing Univ Telecommun & Posts, Chongqing Key Lab Computat Intelligence, Chongqing, Peoples R China.
RP Hao, F (corresponding author), Chongqing Univ Telecommun & Posts, Chongqing Key Lab Computat Intelligence, Chongqing, Peoples R China.
EM s210201032@stu.cqupt.edu.cn
FU National Natural Science Foundation of China [61876027]; Natural Science
   Foundation of Chongqing [cstc2019jcyj-cxttX0002, cstc2021ycjh-
   bgzxm0013]; Key cooperation project of Chongqing Municipal Education
   Commission [HZ2021008]
FX This work is supported by the National Natural Science Foundation of
   China (Grant No: 61876027), the Natural Science Foundation of Chongqing
   (Grant No: cstc2019jcyj-cxttX0002, cstc2021ycjh- bgzxm0013), the key
   cooperation project of Chongqing Municipal Education Commission (Grant
   No: HZ2021008).
CR Ailing Zeng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P507, DOI 10.1007/978-3-030-58568-6_30
   Bengio J., 2009, Proceedings of the 26th Annual International Conference on Machine Learning, P41
   Bridgeman Lewis, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). Proceedings, P2487, DOI 10.1109/CVPRW.2019.00304
   Cai J., 2023, ICASSP 2023 2023 IEE, P1
   Cai YJ, 2019, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2019.00236
   Chen CZ, 2017, IEEE T IMAGE PROCESS, V26, P3156, DOI 10.1109/TIP.2017.2670143
   Chen TL, 2022, IEEE T CIRC SYST VID, V32, P198, DOI 10.1109/TCSVT.2021.3057267
   Chen WM, 2020, SYMMETRY-BASEL, V12, DOI 10.3390/sym12050744
   Chen XP, 2019, PROC CVPR IEEE, P10887, DOI 10.1109/CVPR.2019.01115
   Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
   Cheng Y, 2021, AAAI CONF ARTIF INTE, V35, P1157
   Choi S, 2021, IEEE COMPUT SOC CONF, P2328, DOI 10.1109/CVPRW53098.2021.00265
   Ci H, 2019, IEEE I CONF COMP VIS, P2262, DOI 10.1109/ICCV.2019.00235
   Dai LH, 2023, IEEE T CIRC SYST VID, V33, P2342, DOI 10.1109/TCSVT.2022.3222906
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Diaz-Arias A, 2024, VISUAL COMPUT, V40, P2555, DOI 10.1007/s00371-023-02936-5
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Einfalt M, 2023, IEEE WINT CONF APPL, P2902, DOI 10.1109/WACV56688.2023.00292
   Fan Zhipeng, 2021, P IEEE CVF INT C COM, P11719
   Fang HS, 2018, AAAI CONF ARTIF INTE, P6821
   Gong KH, 2021, PROC CVPR IEEE, P8571, DOI 10.1109/CVPR46437.2021.00847
   Hassanin M., 2022, arXiv
   He KM, 2022, PROC CVPR IEEE, P15979, DOI 10.1109/CVPR52688.2022.01553
   Hendrycks D, 2020, Arxiv, DOI arXiv:1606.08415
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu WB, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P602, DOI 10.1145/3474085.3475219
   Hwang DH, 2020, IEEE WINT CONF APPL, P468, DOI [10.1109/wacv45572.2020.9093595, 10.1109/WACV45572.2020.9093595]
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Ji H., 2024, P IEEE CVF WINT C AP, P3314
   Jingbo Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P764, DOI 10.1007/978-3-030-58601-0_45
   Kingma D. P., 2014, arXiv
   Lee K, 2018, LECT NOTES COMPUT SC, V11211, P123, DOI 10.1007/978-3-030-01234-2_8
   Li MZ, 2021, AAAI CONF ARTIF INTE, V35, P4189
   Li SJ, 2015, LECT NOTES COMPUT SC, V9004, P332, DOI 10.1007/978-3-319-16808-1_23
   Li WH, 2023, IEEE T MULTIMEDIA, V25, P1282, DOI 10.1109/TMM.2022.3141231
   Li WH, 2023, PATTERN RECOGN, V141, DOI 10.1016/j.patcog.2023.109631
   Li WH, 2022, PROC CVPR IEEE, P13137, DOI 10.1109/CVPR52688.2022.01280
   Lin J., 2019, Trajectory space factorization for deep video-based 3d human pose estimation
   Liu RX, 2020, PROC CVPR IEEE, P5063, DOI 10.1109/CVPR42600.2020.00511
   Loper M., 2023, Seminal Graphics Papers: Pushing the Boundaries, V2, P851
   Mahmood N, 2019, IEEE I CONF COMP VIS, P5441, DOI 10.1109/ICCV.2019.00554
   Martinez J, 2017, IEEE I CONF COMP VIS, P2659, DOI 10.1109/ICCV.2017.288
   Mehta D, 2017, INT CONF 3D VISION, P506, DOI 10.1109/3DV.2017.00064
   Mehta D, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073596
   Nair V., 2010, Proceedings of the 27th International Conference on Machine Learning (ICML-10), P807
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Nie XC, 2019, IEEE I CONF COMP VIS, P6941, DOI 10.1109/ICCV.2019.00704
   Pavlakos G, 2017, PROC CVPR IEEE, P1263, DOI 10.1109/CVPR.2017.139
   Pavllo D, 2019, PROC CVPR IEEE, P7745, DOI 10.1109/CVPR.2019.00794
   Qian XY, 2023, Arxiv, DOI arXiv:2301.07322
   Qin Z., 2023, P IEEE CVF INT C COM, P8690
   Shan WK, 2022, LECT NOTES COMPUT SC, V13665, P461, DOI 10.1007/978-3-031-20065-6_27
   Shan WK, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3446, DOI 10.1145/3474085.3475504
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Tekin B, 2016, Arxiv, DOI arXiv:1605.05180
   Tolstikhin I, 2021, ADV NEUR IN, V34
   Tome D, 2018, INT CONF 3D VISION, P474, DOI 10.1109/3DV.2018.00061
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang X., 2023, IEEE Transactions on Circuits and Systems for Video Technology
   Willett NS, 2020, PROCEEDINGS OF THE 25TH INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES, IUI 2020, P88, DOI 10.1145/3377325.3377505
   Xu JW, 2020, PROC CVPR IEEE, P896, DOI 10.1109/CVPR42600.2020.00098
   Xu TH, 2021, PROC CVPR IEEE, P16100, DOI 10.1109/CVPR46437.2021.01584
   Yuexi Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P609, DOI 10.1007/978-3-030-58520-4_36
   Yuhui Yuan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P173, DOI 10.1007/978-3-030-58539-6_11
   Zhang HT, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3448978
   Zhang JL, 2022, PROC CVPR IEEE, P13222, DOI 10.1109/CVPR52688.2022.01288
   Zhao QT, 2023, PROC CVPR IEEE, P8877, DOI 10.1109/CVPR52729.2023.00857
   Zheng C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11636, DOI 10.1109/ICCV48922.2021.01145
   Zhu W., 2023, P IEEECVF INT C COMP, P15085
NR 69
TC 0
Z9 0
U1 1
U2 1
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD SEP
PY 2024
VL 149
AR 105142
DI 10.1016/j.imavis.2024.105142
EA JUL 2024
PG 13
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA XV0F1
UT WOS:001264326600001
DA 2024-08-05
ER

PT J
AU Tofighi, NJ
   Elfkir, MH
   Imamoglu, N
   Ozcinar, C
   Erdem, A
   Erdem, E
AF Tofighi, Nafiseh Jabbari
   Elfkir, Mohamed Hedi
   Imamoglu, Nevrez
   Ozcinar, Cagri
   Erdem, Aykut
   Erdem, Erkut
TI Omnidirectional image quality assessment with local-global vision
   transformers
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE 360-degree images; Image quality assessment; Vision transformers
ID NATURAL SCENE STATISTICS; SIMILARITY INDEX; PERCEPTION; DEVIATION;
   EFFICIENT
AB With the rising popularity of omnidirectional images (ODIs) in virtual reality applications, the need for specialized image quality assessment (IQA) methods becomes increasingly critical. Traditional IQA approaches, designed for rectilinear images, often fail to evaluate ODIs accurately due to their 360 -degree scene representation. Addressing this, we introduce the Local - Global Transformer for 360 -degree Image Quality Assessment (LGT360IQ). This novel framework features dual branches tailored to mimic top -down and bottom -up visual attention mechanisms, adapted for the spherical characteristics of ODIs. The local branch processes tangent viewports from salient regions within the equirectangular projection image, extracting detailed features for granular quality assessment. In parallel, the global branch utilizes a task -dependent token sampling strategy for holistic image feature processing and quality score prediction. This integrated approach combines local and global information, offering an effective IQA method for ODIs. Our extensive evaluation across three benchmark ODI datasets, CVIQ, OIQA, and ODI, demonstrates LGT360IQ superior performance and establishes its role in advancing the field of IQA for omnidirectional images.
C1 [Tofighi, Nafiseh Jabbari; Erdem, Aykut] Koc Univ, Dept Comp Engn, Istanbul, Turkiye.
   [Tofighi, Nafiseh Jabbari; Erdem, Aykut] Koc Univ Is Bank AI Ctr, Istanbul, Turkiye.
   [Elfkir, Mohamed Hedi; Erdem, Erkut] Hacettepe Univ, Dept Comp Engn, Ankara, Turkiye.
   [Imamoglu, Nevrez] Natl Inst Adv Ind Sci & Technol, Digital Architecture Res Ctr, Tokyo, Japan.
   [Ozcinar, Cagri] Msk Ai, London, England.
C3 Koc University; Hacettepe University; National Institute of Advanced
   Industrial Science & Technology (AIST)
RP Erdem, E (corresponding author), Hacettepe Univ, Dept Comp Engn, Ankara, Turkiye.
EM erkut@cs.hacettepe.edu.tr
FU KUIS AI Center Research Award; TUBITAK-1001 Program [120E501]; BAGEP
   Award of Science Academy
FX This work was supported in part by KUIS AI Center Research Award to N.
   Jabbari Tofighi, TUBITAK-1001 Program Award No. 120E501, and BAGEP 2021
   Award of the Science Academy to A. Erdem. During the preparation of this
   work the author (s) used GPT-4 in order to improve language and
   readability. After using this tool/service, the author (s) reviewed and
   edited the content as needed and take (s) full responsibility for the
   content of the publication.
CR Akhtar Z, 2017, IEEE ACCESS, V5, P21090, DOI 10.1109/ACCESS.2017.2750918
   [Anonymous], 2021, U.S
   Chen DW, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P92, DOI [10.1109/VR46266.2020.1581216087067, 10.1109/VR46266.2020.00-77]
   Chen SJ, 2018, IEEE INT CON MULTI
   Cheon M, 2021, IEEE COMPUT SOC CONF, P433, DOI 10.1109/CVPRW53098.2021.00054
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Coxeter H.S.M., 1989, Introduction to Geometry, V2nd
   Dahou Yasser, 2021, Pattern Recognition. ICPR International Workshops and Challenges. Proceedings. Lecture Notes in Computer Science (LNCS 12663), P305, DOI 10.1007/978-3-030-68796-0_22
   Ding KY, 2022, IEEE T PATTERN ANAL, V44, P2567, DOI 10.1109/TPAMI.2020.3045810
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Duan HY, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351786
   Eder Marc, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12423, DOI 10.1109/CVPR42600.2020.01244
   Fang YM, 2015, IEEE SIGNAL PROC LET, V22, P838, DOI 10.1109/LSP.2014.2372333
   Fu J, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P961, DOI 10.1145/3503161.3548337
   Gao XB, 2013, IEEE T NEUR NET LEAR, V24, P2013, DOI 10.1109/TNNLS.2013.2271356
   Golestaneh SA, 2022, IEEE WINT CONF APPL, P3989, DOI 10.1109/WACV51458.2022.00404
   Han K, 2023, IEEE T PATTERN ANAL, V45, P87, DOI 10.1109/TPAMI.2022.3152247
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ke JJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5128, DOI 10.1109/ICCV48922.2021.00510
   Kim HG, 2020, IEEE T CIRC SYST VID, V30, P917, DOI 10.1109/TCSVT.2019.2898732
   Lu Y, 2022, LECT NOTES COMPUT SC, V13431, P644, DOI 10.1007/978-3-031-16431-6_61
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Nafchi HZ, 2016, IEEE ACCESS, V4, P5579, DOI 10.1109/ACCESS.2016.2604042
   Pan ZQ, 2022, IEEE T IMAGE PROCESS, V31, P1613, DOI 10.1109/TIP.2022.3144892
   Reisenhofer R, 2018, SIGNAL PROCESS-IMAGE, V61, P33, DOI 10.1016/j.image.2017.11.001
   Ryoo M.S., 2021, 2021 ADV NEUR INF PR, P2672
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Sendjasni A, 2022, IEEE INT CONF MULTI, DOI 10.1109/ICMEW56448.2022.9859468
   Sendjasni A, 2021, IEEE IMAGE PROC, P1439, DOI 10.1109/ICIP42928.2021.9506044
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Sun W, 2018, IEEE INT WORKSH MULT
   Sun W, 2020, IEEE J-STSP, V14, P64, DOI 10.1109/JSTSP.2019.2955024
   Tian CZ, 2022, IEEE T CIRC SYST VID, V32, P6557, DOI 10.1109/TCSVT.2022.3172135
   Tofighi N., 2023, 2023 IEEE INT C AC S, P1, DOI DOI 10.1109/ICASSP49357.2023.10096750
   Vaswani A, 2023, Arxiv, DOI arXiv:1706.03762
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu T., 2023, ADV NEUR IN
   Xu JH, 2021, IEEE T CIRC SYST VID, V31, P1724, DOI 10.1109/TCSVT.2020.3015186
   Xu M, 2020, IEEE J-STSP, V14, P5, DOI 10.1109/JSTSP.2020.2966864
   Xu M, 2019, IEEE T CIRC SYST VID, V29, P3516, DOI [10.1109/TCSVT.2018.2886277, 10.1080/17445302.2018.1558727]
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Yang LD, 2022, IEEE T ROBOT, V38, P1531, DOI 10.1109/TRO.2021.3111788
   Yang SD, 2022, IEEE COMPUT SOC CONF, P1190, DOI 10.1109/CVPRW56347.2022.00126
   You JY, 2021, IEEE IMAGE PROC, P1389, DOI 10.1109/ICIP42928.2021.9506075
   Yu M, 2015, 2015 IEEE International Symposium on Mixed and Augmented Reality, P31, DOI 10.1109/ISMAR.2015.12
   Yule S., 2016, MPEG Joint Video Explor. Team, V116
   Yun H., 2022, 2022 EUR C COMP VIS
   Zakharchenko V, 2016, PROC SPIE, V9970, DOI 10.1117/12.2235885
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028
   Zhang L, 2012, IEEE IMAGE PROC, P1473, DOI 10.1109/ICIP.2012.6467149
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang WX, 2020, IEEE T CIRC SYST VID, V30, P36, DOI 10.1109/TCSVT.2018.2886771
   Zhou W, 2022, IEEE T CIRC SYST VID, V32, P1778, DOI 10.1109/TCSVT.2021.3081182
   Zhou YF, 2018, INT CONF SIGN PROCES, P54, DOI 10.1109/ICSP.2018.8652269
NR 56
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD AUG
PY 2024
VL 148
AR 105151
DI 10.1016/j.imavis.2024.105151
EA JUN 2024
PG 13
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA XO6F2
UT WOS:001262657000001
DA 2024-08-05
ER

PT J
AU Wang, XY
   Tian, YH
   Geng, FD
   Wang, R
AF Wang, Xiangyang
   Tian, Yuhui
   Geng, Fudi
   Wang, Rui
TI DFSTrack: Dual-stream fusion Siamese network for human pose tracking in
   videos
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Human pose tracking; Residual graph convolutional block; Dual -stream
   spatial -temporal fusion; transformer
AB Human pose tracking is a challenging task that involves estimating the human pose and tracking it across multiple frames in a video sequence. In recent years, deep learning -based methods have made significant progress in this field, achieving state-of-the-art performance. However, due to complex background and occlusion among people missed detection and incorrect association matching are still the challenging problems. To address these issues, we adopt a top -down framework to perform human pose tracking in the paper. We propose a human detection prediction recovery module (HDP module) to recover missed detection, and propose a dualstream fusion Siamese network for human matching (DFSTrack). Specifically, we design a residual graph convolutional block (RGCN block) for spatial position encoding of human keypoints, and use spatial self -attention and temporal cross -attention to design a dual -stream spatial -temporal fusion transformer (DST Transformer). The graph convolutional block and transformer are cascaded to simultaneously obtain information on the spatial and temporal positions of human keypoints, allowing the Siamese network to solve the erroneous human matching. Experimental results on the PoseTrack17 dataset, PoseTrack18 dataset and PoseTrack21 dataset demonstrate that our proposed method outperforms state-of-the-art methods on human pose tracking tasks. Our code and pretrained models are available at https://github.com/yhtian2023/DFSTrack.
C1 [Wang, Xiangyang; Tian, Yuhui; Geng, Fudi; Wang, Rui] Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
C3 Shanghai University
RP Wang, R (corresponding author), Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
EM wangxiangyang@shu.edu.cn; tyh_99@shu.edu.cn; gfdnaruto@shu.edu.cn;
   rwang@shu.edu.cn
FU National Natural Science Foundation of China (NSFC) [61771299]
FX This work was supported in part by the National Natural Science
   Foundation of China (NSFC) under Grant 61771299.
CR Andriluka M, 2018, PROC CVPR IEEE, P5167, DOI 10.1109/CVPR.2018.00542
   Bai Y, 2022, LECT NOTES COMPUT SC, V13664, P422, DOI 10.1007/978-3-031-19772-7_25
   Bao Q, 2021, IEEE T MULTIMEDIA, V23, P161, DOI 10.1109/TMM.2020.2980194
   Barra P., 2019, DEPENDABILITY SENSOR, V5, P180
   Bergmann P, 2019, IEEE I CONF COMP VIS, P941, DOI 10.1109/ICCV.2019.00103
   Bin YR, 2020, PATTERN RECOGN, V106, DOI 10.1016/j.patcog.2020.107410
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen J., 2021, TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation
   Cheng BW, 2020, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR42600.2020.00543
   Chunluan Zhou, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P680, DOI 10.1007/978-3-030-58542-6_41
   Doering A, 2022, PROC CVPR IEEE, P20931, DOI 10.1109/CVPR52688.2022.02029
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Duan HD, 2022, PROC CVPR IEEE, P2959, DOI 10.1109/CVPR52688.2022.00298
   Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256
   Feng RY, 2023, PROC CVPR IEEE, P17131, DOI 10.1109/CVPR52729.2023.01643
   Girdhar R, 2018, PROC CVPR IEEE, P350, DOI 10.1109/CVPR.2018.00044
   Huang JJ, 2020, PROC CVPR IEEE, P5699, DOI 10.1109/CVPR42600.2020.00574
   Hwang J, 2019, IEEE IJCNN
   Iqbal U, 2016, LECT NOTES COMPUT SC, V9914, P627, DOI 10.1007/978-3-319-48881-3_44
   Jian Wang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P492, DOI 10.1007/978-3-030-58621-8_29
   Jibin Gao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P222, DOI 10.1007/978-3-030-58577-8_14
   Jin S, 2019, PROC CVPR IEEE, P5657, DOI 10.1109/CVPR.2019.00581
   Kreiss S, 2019, PROC CVPR IEEE, P11969, DOI 10.1109/CVPR.2019.01225
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lu DN, 2022, IEEE T INTELL TRANSP, V23, P24854, DOI 10.1109/TITS.2022.3198836
   Manchen Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11085, DOI 10.1109/CVPR42600.2020.01110
   Mao W., 2021, arXiv
   Moon G, 2019, PROC CVPR IEEE, P7765, DOI 10.1109/CVPR.2019.00796
   Newell A, 2017, ADV NEUR IN, V30
   Ning GH, 2020, IEEE COMPUT SOC CONF, P4456, DOI 10.1109/CVPRW50498.2020.00525
   Ning GH, 2019, LECT NOTES COMPUT SC, V11130, P227, DOI 10.1007/978-3-030-11012-3_20
   Pan JH, 2019, IEEE I CONF COMP VIS, P6340, DOI 10.1109/ICCV.2019.00643
   Park C, 2022, PROC CVPR IEEE, P16928, DOI 10.1109/CVPR52688.2022.01644
   Pishchulin L, 2016, PROC CVPR IEEE, P4929, DOI 10.1109/CVPR.2016.533
   Qiu ZW, 2020, AAAI CONF ARTIF INTE, V34, P11924
   Raaj Y, 2019, PROC CVPR IEEE, P4615, DOI 10.1109/CVPR.2019.00475
   Redmon J., 2018, CoRR
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Snower M, 2020, PROC CVPR IEEE, P6737, DOI 10.1109/CVPR42600.2020.00677
   Su K, 2019, PROC CVPR IEEE, P5667, DOI 10.1109/CVPR.2019.00582
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Xie EZ, 2021, ADV NEUR IN, V34
   Xu YF, 2022, ADV NEUR IN
   Xue N, 2022, PROC CVPR IEEE, P13055, DOI 10.1109/CVPR52688.2022.01272
   Yang CY, 2020, PROC CVPR IEEE, P588, DOI 10.1109/CVPR42600.2020.00067
   Yang S, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11782, DOI 10.1109/ICCV48922.2021.01159
   Yang YD, 2021, PROC CVPR IEEE, P8070, DOI 10.1109/CVPR46437.2021.00798
   Yu D., 2018, P EUR C COMP VIS ECC, DOI [10.1007/978-3-030-11012-319, DOI 10.1007/978-3-030-11012-319]
   Yuan YH, 2021, Arxiv, DOI arXiv:2110.09408
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhu X., 2017, P IEEE CVF INT C COM, P4321
NR 55
TC 0
Z9 0
U1 3
U2 3
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD AUG
PY 2024
VL 148
AR 105117
DI 10.1016/j.imavis.2024.105117
EA JUN 2024
PG 11
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA XC9B6
UT WOS:001259592200001
DA 2024-08-05
ER

PT J
AU Yao, JF
   Wang, XG
   Ye, L
   Liu, WY
AF Yao, Jingfeng
   Wang, Xinggang
   Ye, Lang
   Liu, Wenyu
TI Matte anything: Interactive natural image matting with segment anything
   model
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Image matting; Image segmentation; Segment anything model
AB Natural image matting algorithms aim to predict the transparency map (alpha-matte) with the trimap guidance. However, the production of trimap often requires significant labor, which limits the widespread application of matting algorithms on a large scale. To address the issue, we propose Matte Anything model (MatAny), an interactive natural image matting model that could produce high-quality alpha-matte with various simple hints. The key insight of MatAny is to generate pseudo trimap automatically with contour and transparency prediction. In our work, we leverage vision foundation models to enhance the performance of natural image matting. Specifically, we use the segment anything model to predict high-quality contour with user interaction and an open-vocabulary detector to predict the transparency of any object. Subsequently, a pre-trained image matting model generates alpha mattes with pseudo trimaps. MatAny is the interactive matting algorithm with the most supported interaction methods and the best performance to date. It consists of orthogonal vision models without any additional training. We evaluate the performance of MatAny against several previous image matting algorithms. MatAny has 58.3% improvement on MSE and 40.6% improvement on SAD compared to the previous image matting methods with simple guidance, achieving new state-of-the-art (SOTA) performance. The source codes and pre-trained models are available at https://github.com/hustvl/Matte-Anything.
C1 [Yao, Jingfeng; Wang, Xinggang; Ye, Lang; Liu, Wenyu] Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China.
C3 Huazhong University of Science & Technology
RP Wang, XG (corresponding author), Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China.
EM xgwang@hust.edu.cn
FU Agricultural Science and Technology Independent Innovation Foundation of
   Jiangsu Province, China [CX (22) 2031]; Suzhou Jiaoshi Intelligent
   Technology Limited Company
FX This work was supported by the Agricultural Science and Technology
   Independent Innovation Foundation of Jiangsu Province, China [CX (22)
   2031] and Suzhou Jiaoshi Intelligent Technology Limited Company.
CR Boda J, 2018, PROCEEDINGS OF THE 2018 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATION AND SIGNAL PROCESSING (ICCSP), P765, DOI 10.1109/ICCSP.2018.8523834
   Brown T., 2020, ADV NEURAL INFORM PR, V33, P1877, DOI DOI 10.48550/ARXIV.2005.14165
   Cai HQ, 2022, LECT NOTES COMPUT SC, V13689, P253, DOI 10.1007/978-3-031-19818-2_15
   Caron M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9630, DOI 10.1109/ICCV48922.2021.00951
   Cen JZ, 2024, Arxiv, DOI arXiv:2304.12308
   Chen QF, 2013, IEEE T PATTERN ANAL, V35, P2175, DOI 10.1109/TPAMI.2013.18
   Chen Q, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P618, DOI 10.1145/3240508.3240610
   Cheng B, 2021, ADV NEUR IN, V34
   Cheng BW, 2022, PROC CVPR IEEE, P1280, DOI 10.1109/CVPR52688.2022.00135
   Cheng YM, 2023, Arxiv, DOI arXiv:2305.06558
   Ding HH, 2022, IEEE T IMAGE PROCESS, V31, P2421, DOI 10.1109/TIP.2022.3155958
   Forte M, 2020, Arxiv, DOI [arXiv:2003.07711, 10.48550/arXiv.2003.07711]
   Gao P, 2024, INT J COMPUT VISION, V132, P581, DOI 10.1007/s11263-023-01891-x
   Gu Xiuye, 2021, arXiv
   Gupta A, 2019, PROC CVPR IEEE, P5351, DOI 10.1109/CVPR.2019.00550
   Hou QQ, 2019, IEEE I CONF COMP VIS, P4129, DOI 10.1109/ICCV.2019.00423
   Hu YH, 2023, Arxiv, DOI arXiv:2312.05915
   Jain J, 2023, PROC CVPR IEEE, P2989, DOI 10.1109/CVPR52729.2023.00292
   Kaiming He, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2049, DOI 10.1109/CVPR.2011.5995495
   Kirillov A, 2023, Arxiv, DOI arXiv:2304.02643
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Li C., 2022, Advances in Neural Information Processing Systems, V35, P9287
   Li J., 2021, P 30 INT JOINT C ART, P800, DOI [DOI 10.24963/IJCAI.2021/111, 10.24963/ijcai.2021/111]
   Li JZZ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3501, DOI 10.1145/3474085.3475512
   Li JZZ, 2022, INT J COMPUT VISION, V130, P246, DOI 10.1007/s11263-021-01541-0
   Li YY, 2020, AAAI CONF ARTIF INTE, V34, P11450
   Li Y, 2023, Arxiv, DOI arXiv:2304.05653
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu QL, 2024, Arxiv, DOI arXiv:2304.01171
   Liu SL, 2023, Arxiv, DOI arXiv:2303.05499
   Liu YH, 2024, Arxiv, DOI arXiv:2304.05622
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lu H, 2019, IEEE I CONF COMP VIS, P3265, DOI 10.1109/ICCV.2019.00336
   Ma J, 2024, NAT COMMUN, V15, DOI 10.1038/s41467-024-44824-z
   Mohapatra S, 2023, Arxiv, DOI arXiv:2304.04738
   OpenAI R., 2023, Gpt-4 technical report
   Park G, 2022, PROC CVPR IEEE, P11686, DOI 10.1109/CVPR52688.2022.01140
   Qiao Y., 2020, P IEEECVF C COMPUTER, P13676
   Radford A, 2021, PR MACH LEARN RES, V139
   Rombach R., 2021, arXiv, DOI DOI 10.48550/ARXIV.2112.10752
   Seo J, 2024, Arxiv, DOI arXiv:2303.07937
   Shahrian E, 2013, PROC CVPR IEEE, P636, DOI 10.1109/CVPR.2013.88
   Song JM, 2022, Arxiv, DOI [arXiv:2010.02502, DOI 10.48550/ARXIV.2010.02502]
   Touvron H, 2023, Arxiv, DOI [arXiv:2302.13971, DOI 10.48550/ARXIV.2302.13971]
   Wei TY, 2021, PROC CVPR IEEE, P15369, DOI 10.1109/CVPR46437.2021.01512
   Xu N, 2017, PROC CVPR IEEE, P311, DOI 10.1109/CVPR.2017.41
   Yang DH, 2023, Arxiv, DOI arXiv:2205.08324
   Yang JY, 2023, Arxiv, DOI [arXiv:2304.11968, DOI 10.48550/ARXIV.2304.11968]
   Yao JF, 2024, INFORM FUSION, V103, DOI 10.1016/j.inffus.2023.102091
   Yao L., 2022, Advances in Neural Information Processing Systems, V35, P9125
   Yu QH, 2021, PROC CVPR IEEE, P1154, DOI 10.1109/CVPR46437.2021.00121
   Yu T, 2023, Arxiv, DOI arXiv:2304.06790
   Zareian A, 2021, PROC CVPR IEEE, P14388, DOI 10.1109/CVPR46437.2021.01416
   Zhang L, 2023, Arxiv, DOI [arXiv:2302.05543, 10.48550/ARXIV.2302.05543]
   Zhang YK, 2019, PROC CVPR IEEE, P7461, DOI 10.1109/CVPR.2019.00765
NR 55
TC 0
Z9 0
U1 3
U2 3
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUL
PY 2024
VL 147
AR 105067
DI 10.1016/j.imavis.2024.105067
EA MAY 2024
PG 10
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA TW3J5
UT WOS:001244254300001
DA 2024-08-05
ER

PT J
AU Kim, J
   Kim, J
   Hong, S
AF Kim, Jiyun
   Kim, Jooho
   Hong, Sungeun
TI G-TRACE: Grouped temporal recalibration for video object segmentation
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Semi -supervised video object segmentation; Memory attention;
   Hierarchical grouping
AB In Semi-supervised Video Object Segmentation (SVOS), there is a critical emphasis on enhancing the memory and readout mechanisms for frame matching, especially in relation to temporal dynamics. Current methods predominantly use 2D CNNs for encoding video frames, which unfortunately neglects the crucial aspect of addressing temporal variations in individual frames and their associated masks during the encoding process. One potential solution would be to implement temporal models such as 3D CNNs instead of 2D CNNs, but this significantly increases computational requirements, making it impractical for real-world SVOS applications. In this paper, we introduce the Grouped Temporal Recalibration with Attention for Convolutional Encoders (GTRACE), a novel plug-and-play module that is compatible with various existing SVOS frameworks. G-TRACE uses hierarchical memory-centric attention and integrates effortlessly with 2D CNNs, offering a novel approach to temporal modeling that operates orthogonally to traditional frame matching methods. Extensive evaluations on four widely-used benchmarks demonstrate that our method consistently delivers significant performance improvements over various baseline models.
C1 [Kim, Jiyun] Inha Univ, Dept Elect & Comp Engn, Incheon, South Korea.
   [Kim, Jooho; Hong, Sungeun] Sungkyunkwan Univ, Dept Immers Media Engn, Seoul, South Korea.
C3 Inha University; Sungkyunkwan University (SKKU)
RP Hong, S (corresponding author), Sungkyunkwan Univ, Dept Immers Media Engn, Seoul, South Korea.
EM noahyun1222@gmail.com; ppkjhkjh@skku.edu; csehong@skku.edu
FU National Research Foundation of Korea (NRF) - Korea Government (MSIT)
   [2022R1A4A1033549, RS -2023-00211348]; MSIT (Ministry of Science and
   ICT) , Korea, under the Graduate School of Metaverse Convergence support
   program [IITP-2024-RS-2023-00254129]
FX This work was partly supported by the National Research Foundation of
   Korea (NRF) grant funded by the Korea Government (MSIT) (No.
   2022R1A4A1033549, No. RS -2023-00211348) and the MSIT (Ministry of
   Science and ICT) , Korea, under the Graduate School of Metaverse
   Convergence support program (IITP-2024-RS-2023-00254129) supervised by
   the IITP (Institute for Information & Communications Technology Planning
   & Evaluation) .
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   Bao LC, 2018, PROC CVPR IEEE, P5977, DOI 10.1109/CVPR.2018.00626
   Bhat Goutam, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P777, DOI 10.1007/978-3-030-58536-5_46
   Burgess J., 2018, YouTube: online video and participatory culture
   Caelles S, 2017, PROC CVPR IEEE, P5320, DOI 10.1109/CVPR.2017.565
   Chen YD, 2022, SCI CHINA INFORM SCI, V65, DOI 10.1007/s11432-021-3396-7
   Cheng HK, 2021, ADV NEUR IN, V34
   Cheng HK, 2022, LECT NOTES COMPUT SC, V13688, P640, DOI 10.1007/978-3-031-19815-1_37
   Cheng HK, 2021, PROC CVPR IEEE, P5555, DOI 10.1109/CVPR46437.2021.00551
   Cheng JC, 2018, PROC CVPR IEEE, P7415, DOI 10.1109/CVPR.2018.00774
   Choi S, 2023, PROCEEDINGS OF THE 2023 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, ICMR 2023, P217, DOI 10.1145/3591106.3592235
   Duke B, 2021, PROC CVPR IEEE, P5908, DOI 10.1109/CVPR46437.2021.00585
   Gu YC, 2020, AAAI CONF ARTIF INTE, V34, P10869
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Guo PX, 2022, IEEE T IMAGE PROCESS, V31, P7063, DOI 10.1109/TIP.2022.3219230
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hong LY, 2023, IEEE I CONF COMP VIS, P13434, DOI 10.1109/ICCV51070.2023.01240
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu L, 2021, PROC CVPR IEEE, P4142, DOI 10.1109/CVPR46437.2021.00413
   Hu P, 2018, PROC CVPR IEEE, P1400, DOI 10.1109/CVPR.2018.00152
   Hu YT, 2018, LECT NOTES COMPUT SC, V11212, P56, DOI 10.1007/978-3-030-01237-3_4
   Jiang ZL, 2011, PROC CVPR IEEE, P1697, DOI 10.1109/CVPR.2011.5995354
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee SH, 2017, PROC CVPR IEEE, P5863, DOI 10.1109/CVPR.2017.621
   Li MX, 2022, PROC CVPR IEEE, P1322, DOI 10.1109/CVPR52688.2022.00139
   Li X, 2022, AAAI CONF ARTIF INTE, P1429
   Li XX, 2018, LECT NOTES COMPUT SC, V11207, P93, DOI 10.1007/978-3-030-01219-9_6
   Liang SX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8045, DOI 10.1109/ICCV48922.2021.00796
   Liang Y., 2020, P 34 INT C NEUR INF, V33, P3430
   Lin HJ, 2019, IEEE I CONF COMP VIS, P3948, DOI 10.1109/ICCV.2019.00405
   Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718
   Lin ZH, 2022, PROC CVPR IEEE, P1352, DOI 10.1109/CVPR52688.2022.00142
   Lu X., 2020, COMPUTER VISION ECCV
   Lu Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P490, DOI 10.1007/978-3-030-58568-6_29
   Luiten J, 2019, LECT NOTES COMPUT SC, V11364, P565, DOI 10.1007/978-3-030-20870-7_35
   Mao YY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9650, DOI 10.1109/ICCV48922.2021.00953
   Oh SW, 2019, IEEE I CONF COMP VIS, P9225, DOI 10.1109/iccv.2019.00932
   Park Jongchan, 2018, P BRIT MACH VIS C BM
   Park K, 2022, PROC CVPR IEEE, P1342, DOI 10.1109/CVPR52688.2022.00141
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Perazzi F, 2017, PROC CVPR IEEE, P3491, DOI 10.1109/CVPR.2017.372
   Pont-Tuset J, 2018, Arxiv, DOI arXiv:1704.00675
   Ren SC, 2021, PROC CVPR IEEE, P15450, DOI 10.1109/CVPR46437.2021.01520
   Robinson Andreas, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7404, DOI 10.1109/CVPR42600.2020.00743
   Seong H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12869, DOI 10.1109/ICCV48922.2021.01265
   Seong Hongje, 2020, EUR C COMP VIS, P629, DOI DOI 10.1007/978-3-030-58542-638
   Sultani W, 2018, PROC CVPR IEEE, P6479, DOI 10.1109/CVPR.2018.00678
   Sun M., 2020, P IEEECVF C COMPUTER, P10791
   Voigtlaender P, 2019, PROC CVPR IEEE, P9473, DOI 10.1109/CVPR.2019.00971
   Wang HC, 2021, PROC CVPR IEEE, P1296, DOI 10.1109/CVPR46437.2021.00135
   Wang TH, 2019, IEEE I CONF COMP VIS, P10490, DOI 10.1109/ICCV.2019.01059
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu QQ, 2023, IEEE I CONF COMP VIS, P13833, DOI 10.1109/ICCV51070.2023.01276
   Xi Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9381, DOI 10.1109/CVPR42600.2020.00940
   Xie HZ, 2021, PROC CVPR IEEE, P1286, DOI 10.1109/CVPR46437.2021.00134
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yang LJ, 2018, PROC CVPR IEEE, P6499, DOI 10.1109/CVPR.2018.00680
   Yang ZX, 2021, ADV NEUR IN, V34
   Yang ZX, 2022, IEEE T PATTERN ANAL, V44, P4701, DOI 10.1109/TPAMI.2021.3081597
   Ye LW, 2019, PROC CVPR IEEE, P10494, DOI 10.1109/CVPR.2019.01075
   Yu Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P735, DOI 10.1007/978-3-030-58607-2_43
   Yu Y, 2022, LECT NOTES COMPUT SC, V13689, P612, DOI 10.1007/978-3-031-19818-2_35
   Yuk Heo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P297, DOI 10.1007/978-3-030-58520-4_18
   Zhang Yang, 2022, Proceedings of the 11th International Conference on Computer Engineering and Networks. Lecture Notes in Electrical Engineering (808), P90, DOI 10.1007/978-981-16-6554-7_10
   Zhang Y, 2023, PROC CVPR IEEE, P2246, DOI 10.1109/CVPR52729.2023.00223
   Zongxin Yang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P332, DOI 10.1007/978-3-030-58558-7_20
NR 67
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUL
PY 2024
VL 147
AR 105050
DI 10.1016/j.imavis.2024.105050
EA MAY 2024
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA TP3G4
UT WOS:001242418500001
DA 2024-08-05
ER

PT J
AU Yazici, ZA
   Öksüz, I
   Ekenel, HK
AF Yazici, Ziya Ata
   Oksuz, Ilkay
   Ekenel, Hazim Kemal
TI GLIMS: Attention-guided lightweight multi-scale hybrid network for
   volumetric semantic segmentation
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Medical image segmentation; Convolutional neural network; Vision
   transformer; Multi -scale features; Attention -guidance
AB Convolutional Neural Networks (CNNs) have become widely adopted for medical image segmentation tasks, demonstrating promising performance. However, the inherent inductive biases in convolutional architectures limit their ability to model long-range dependencies and spatial correlations. While recent transformer-based architectures address these limitations by leveraging self-attention mechanisms to encode long-range dependencies and learn expressive representations, they often struggle to extract low-level features and are highly dependent on data availability. This motivated us for the development of GLIMS, a data-efficient attentionguided hybrid volumetric segmentation network. GLIMS utilizes Dilated Feature Aggregator Convolutional Blocks (DACB) to capture local-global feature correlations efficiently. Furthermore, the incorporated Swin Transformer-based bottleneck bridges the local and global features to improve the robustness of the model. Additionally, GLIMS employs an attention-guided segmentation approach through Channel and Spatial-Wise Attention Blocks (CSAB) to localize expressive features for fine-grained border segmentation. Quantitative and qualitative results on glioblastoma and multi-organ CT segmentation tasks demonstrate GLIMS' effectiveness in terms of complexity and accuracy. GLIMS demonstrated outstanding performance on BraTS2021 and BTCV datasets, surpassing the performance of Swin UNETR. Notably, GLIMS achieved this high performance with a significantly reduced number of trainable parameters. Specifically, GLIMS has 47.16 M trainable parameters and 72.30G FLOPs, while Swin UNETR has 61.98 M trainable parameters and 394.84G FLOPs. The code is publicly available at https://github.com/yaziciz/GLIMS.
C1 [Yazici, Ziya Ata; Oksuz, Ilkay; Ekenel, Hazim Kemal] Istanbul Tech Univ, Dept Comp Engn, Istanbul, Turkiye.
C3 Istanbul Technical University
RP Yazici, ZA (corresponding author), Istanbul Tech Univ, Dept Comp Engn, Istanbul, Turkiye.
EM yaziciz21@itu.edu.tr; oksuzilkay@itu.edu.tr; ekenel@itu.edu.tr
RI Yazıcı, Ziya Ata/KJL-8860-2024
OI Yazıcı, Ziya Ata/0000-0001-7051-833X
FU Istanbul Technical University; Department of Computer Engineering and
   Turkcell
FX This study has been partially funded by Istanbul Technical University,
   Department of Computer Engineering and Turkcell via a Research
   Scholarship grant provided to Ziya Ata Yaz & imath;c & imath;.
CR Awais M., 2023, arXiv, DOI 10.48550/arXiv.2307.13721
   Bakas S, 2017, SCI DATA, V4, DOI 10.1038/sdata.2017.117
   Cai SJ, 2020, QUANT IMAG MED SURG, V10, P1275, DOI 10.21037/qims-19-1090
   Cao Hu, 2023, Computer Vision - ECCV 2022 Workshops: Proceedings. Lecture Notes in Computer Science (13803), P205, DOI 10.1007/978-3-031-25066-8_9
   Cao Y, 2023, BIOMED SIGNAL PROCES, V80, DOI 10.1016/j.bspc.2022.104296
   Chen J., 2021, TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation
   Cicek Ozgun, 2016, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016. 19th International Conference. Proceedings: LNCS 9901, P424, DOI 10.1007/978-3-319-46723-8_49
   Diakogiannis FI, 2020, ISPRS J PHOTOGRAMM, V162, P94, DOI 10.1016/j.isprsjprs.2020.01.013
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Gao YH, 2021, LECT NOTES COMPUT SC, V12903, P61, DOI 10.1007/978-3-030-87199-4_6
   Gibson Eli, 2018, Zenodo
   Gu R, 2021, IEEE T MED IMAGING, V40, P699, DOI 10.1109/TMI.2020.3035253
   Hatamizadeh A, 2022, LECT NOTES COMPUT SC, V12962, P272, DOI 10.1007/978-3-031-08999-2_22
   Hatamizadeh A, 2022, IEEE WINT CONF APPL, P1748, DOI 10.1109/WACV51458.2022.00181
   He AL, 2023, IEEE T MED IMAGING, V42, P2763, DOI 10.1109/TMI.2023.3264513
   Heidari M, 2023, IEEE WINT CONF APPL, P6191, DOI 10.1109/WACV56688.2023.00614
   Hong DF, 2024, Arxiv, DOI [arXiv:2311.07113, DOI 10.48550/ARXIV.2311.07113]
   Huang HM, 2020, INT CONF ACOUST SPEE, P1055, DOI [10.1109/icassp40776.2020.9053405, 10.1109/ICASSP40776.2020.9053405]
   Isensee F, 2021, NAT METHODS, V18, P203, DOI 10.1038/s41592-020-01008-z
   Khan S, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3505244
   Kirillov A, 2023, IEEE I CONF COMP VIS, P3992, DOI 10.1109/ICCV51070.2023.00371
   Lin AL, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3178991
   Lin H., 2022, P 2022 IEEE INT C MU, P1, DOI DOI 10.1109/ICME52920.2022.9859720
   Liu WT, 2022, LECT NOTES COMPUT SC, V13435, P235, DOI 10.1007/978-3-031-16443-9_23
   Liu YX, 2024, Arxiv, DOI [arXiv:2402.17177, 10.48550/arXiv.2402.17177, DOI 10.48550/ARXIV.2402.17177]
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu Z, 2022, PROC CVPR IEEE, P11966, DOI 10.1109/CVPR52688.2022.01167
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lüddecke T, 2022, PROC CVPR IEEE, P7076, DOI 10.1109/CVPR52688.2022.00695
   Menze BH, 2015, IEEE T MED IMAGING, V34, P1993, DOI 10.1109/TMI.2014.2377694
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Minaee S, 2022, IEEE T PATTERN ANAL, V44, P3523, DOI 10.1109/TPAMI.2021.3059968
   Myronenko A, 2019, LECT NOTES COMPUT SC, V11384, P311, DOI 10.1007/978-3-030-11726-9_28
   Oktay O, 2018, Arxiv, DOI [arXiv:1804.03999, DOI 10.48550/ARXIV.1804.03999]
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Roy S, 2023, LECT NOTES COMPUT SC, V14223, P405, DOI 10.1007/978-3-031-43901-8_39
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Sinha A, 2021, IEEE J BIOMED HEALTH, V25, P121, DOI 10.1109/JBHI.2020.2986926
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Tragakis A, 2023, IEEE WINT CONF APPL, P3649, DOI 10.1109/WACV56688.2023.00365
   Valanarasu JMJ, 2021, LECT NOTES COMPUT SC, V12901, P36, DOI 10.1007/978-3-030-87193-2_4
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang C, 2019, ENTROPY-SWITZ, V21, DOI 10.3390/e21020168
   Wang RS, 2022, IET IMAGE PROCESS, V16, P1243, DOI 10.1049/ipr2.12419
   Wang W., 2021, INT C LEARN REPR
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wang WX, 2021, LECT NOTES COMPUT SC, V12901, P109, DOI 10.1007/978-3-030-87193-2_11
   Wang XL, 2023, Arxiv, DOI [arXiv:2304.03284, 10.48550/arXiv.2304.03284]
   Wang Y, 2018, LECT NOTES COMPUT SC, V11073, P523, DOI 10.1007/978-3-030-00937-3_60
   Wu HP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P22, DOI 10.1109/ICCV48922.2021.00009
   Yuan FN, 2023, PATTERN RECOGN, V136, DOI 10.1016/j.patcog.2022.109228
   Zhou H.-Y., 2021, arXiv
   Zhou SK, 2021, P IEEE, V109, P820, DOI 10.1109/JPROC.2021.3054390
   Zhou Zongwei, 2018, Deep Learn Med Image Anal Multimodal Learn Clin Decis Support (2018), V11045, P3, DOI [10.1007/978-3-030-00889-5_1, 10.1007/978-3-030-00689-1_1]
   Zou X., 2024, Adv Neural Inf Process Syst, V36
NR 55
TC 1
Z9 1
U1 4
U2 4
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUN
PY 2024
VL 146
AR 105055
DI 10.1016/j.imavis.2024.105055
EA MAY 2024
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA TP2M0
UT WOS:001242398100001
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Shome, N
   Kashyap, R
   Laskar, RH
AF Shome, Nirupam
   Kashyap, Richik
   Laskar, Rabul Hussain
TI Detection of tuberculosis using customized MobileNet and transfer
   learning from chest X-ray image
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Tuberculosis detection; Deep-learning; Transfer learning; CXR images
ID AUTOMATIC DETECTION
AB One of the most contagious diseases in the world, tuberculosis (TB) is brought on by the bacteria Mycobacterium tuberculosis . This hazardous scenario can cause life losses and requires expert doctors and several hours to detect the disease. Using the MobileNet transfer learning model, a computationally lightweight model has been proposed in this study. The optimal model for the diagnosis of tuberculosis has been determined after testing numerous variants on the base model with pre -trained weights. A computationally light transfer learning model is proposed to obtain the maximum overall accuracy of 98.66%. The improvement over the best existing model is quite significant. The transfer learning model (DenseNet) utilized in this existing model is based on a very complex convolutional neural network (CNN), and as a result, the model requires greater amounts of time. The performance of the other existing models is relatively less in comparison to our proposed model, and the methods have a number of other drawbacks. Our goal in this work is to create a more accurate model that requires less computational effort. When compared to previous models, our model has a very less number of trainable parameters, which causes the model to converge more quickly and predict more accurately. Our approach also has the benefit of being simply able to modify its weights when the system is further updated with new datasets. Additionally, because of its lightweight architecture, it can be installed on mobile devices as well as used in webbased applications with ease. To analyze and validate the proposed method, we have collected data from Kaggle and used MC, CHN and NIH datasets.
C1 [Shome, Nirupam; Kashyap, Richik] Assam Univ, Dept Elect & Commun Engn, Silchar, Assam, India.
   [Laskar, Rabul Hussain] Natl Inst Technol, Dept Elect & Commun Engn, Silchar, Assam, India.
C3 Assam University; National Institute of Technology (NIT System);
   National Institute of Technology Silchar
RP Shome, N (corresponding author), Assam Univ, Dept Elect & Commun Engn, Silchar, Assam, India.
EM nirupam.shome@aus.ac.in; rhlaskar@ece.nits.ac.in
CR Abbas A, 2020, IEEE ACCESS, V8, P74901, DOI 10.1109/ACCESS.2020.2989273
   Abbas A, 2018, PROCEEDINGS OF 2018 13TH INTERNATIONAL CONFERENCE ON COMPUTER ENGINEERING AND SYSTEMS (ICCES), P122, DOI 10.1109/ICCES.2018.8639200
   Ahsan M, 2019, 2019 IEEE INTERNATIONAL CONFERENCE ON ELECTRO INFORMATION TECHNOLOGY (EIT), P427, DOI [10.1109/eit.2019.8833768, 10.1109/EIT.2019.8833768]
   Chang RI, 2020, J SUPERCOMPUT, V76, P8641, DOI 10.1007/s11227-020-03152-x
   Chhikara P, 2020, ADV INTELL SYST, V1064, P155, DOI 10.1007/978-981-15-0339-9_13
   Das H., 2015, Int. J. Adv. Res. Comput. Sci. Manag. Stud., V3, P149
   Elshennawy NM, 2020, DIAGNOSTICS, V10, DOI 10.3390/diagnostics10090649
   Evalgelista L.G. C., 2018, AN 15 ENC NAC INT AR, P518, DOI 10.5753/eniac.2018.4444
   Faruk O, 2021, J HEALTHC ENG, V2021, DOI 10.1155/2021/1002799
   Forero MG, 2004, REAL-TIME IMAGING, V10, P251, DOI 10.1016/j.rti.2004.05.007
   Howard AG, 2017, Arxiv, DOI [arXiv:1704.04861, 10.48550/arXiv.1704.04861]
   Hernández A, 2019, LECT NOTES COMPUT SC, V11871, P145, DOI 10.1007/978-3-030-33607-3_17
   Hernández J, 2010, EUR J CLIN MICROBIOL, V29, P1435, DOI 10.1007/s10096-010-1023-y
   Hogeweg L, 2010, LECT NOTES COMPUT SC, V6363, P650
   Hooda R, 2017, IEEE I C SIGNAL IMAG, P497, DOI 10.1109/ICSIPA.2017.8120663
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li ZX, 2023, IMAGE VISION COMPUT, V140, DOI 10.1016/j.imavis.2023.104864
   Longmore J.M., 2010, OXFORD HDB CLIN MED
   Lopes UK, 2017, COMPUT BIOL MED, V89, P135, DOI 10.1016/j.compbiomed.2017.08.001
   MacNeil A, 2020, MMWR-MORBID MORTAL W, V69, P281, DOI 10.15585/mmwr.mm6911a2
   Magboub HM, 2010, INT CONF COMP SCI, P498, DOI 10.1109/ICCSIT.2010.5563769
   Melendez J., 2016, Sci. Rep., V6, P1
   Meraj S.S., 2019, International Journal of Engineering and Advanced Technology (IJEAT), t, V9, P2270, DOI DOI 10.35940/IJEAT.A2632.109119
   Nafisah SI, 2024, NEURAL COMPUT APPL, V36, P111, DOI 10.1007/s00521-022-07258-6
   Nguyen QH, 2019, 2019 26TH INTERNATIONAL CONFERENCE ON TELECOMMUNICATIONS (ICT), P381, DOI [10.1109/ICT.2019.8798798, 10.1109/ict.2019.8798798]
   Pasa F, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-42557-4
   Phillips M, 2010, TUBERCULOSIS, V90, P145, DOI 10.1016/j.tube.2010.01.003
   Rahman T, 2020, IEEE ACCESS, V8, P191586, DOI 10.1109/ACCESS.2020.3031384
   Razzak MI, 2018, L N COMPUT VIS BIOME, V26, P323, DOI 10.1007/978-3-319-65981-7_12
   Rohilla A., 2017, ICETETSM, V17, P136
   Shouno H., 2017, Brain Neural Netw., V24, P3
   Showkatian E, 2022, POL J RADIOL, V87, pE118, DOI 10.5114/pjr.2022.113435
   Singh Niharika, 2019, Innovations in Electronics and Communication Engineering. Proceedings of the 7th ICIECE 2018. Lecture Notes in Networks and Systems (LNNS 65), P43, DOI 10.1007/978-981-13-3765-9_5
   Tahir AM, 2022, COGN COMPUT, V14, P1752, DOI 10.1007/s12559-021-09955-1
   van Ginneken B, 2002, IEEE T MED IMAGING, V21, P139, DOI 10.1109/42.993132
   Yadav O, 2018, IEEE INT C BIOINFORM, P2368, DOI 10.1109/BIBM.2018.8621525
   Yang XW, 2023, ENG APPL ARTIF INTEL, V126, DOI 10.1016/j.engappai.2023.106928
   Zhang J, 2022, IMAGE VISION COMPUT, V124, DOI 10.1016/j.imavis.2022.104513
NR 39
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUL
PY 2024
VL 147
AR 105063
DI 10.1016/j.imavis.2024.105063
EA MAY 2024
PG 14
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA TF7K7
UT WOS:001239912000001
DA 2024-08-05
ER

PT J
AU Liu, QK
   Sang, HF
   Wang, JY
   Chen, WX
   Liu, YL
AF Liu, Quankai
   Sang, Haifeng
   Wang, Jinyu
   Chen, Wangxing
   Liu, Yulong
TI Non-probability sampling network based on anomaly pedestrian trajectory
   discrimination for pedestrian trajectory prediction
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Pedestrian trajectory prediction; Non -probability sampling network;
   Subtraction fusion network; Long -tail trajectory prediction; First
   -person view
AB Pedestrian trajectory prediction in first-person view is an important support for achieving fully automated driving in cities. However, existing pedestrian trajectory prediction methods still have significant shortcomings in terms of pedestrian trajectory diversity, dynamic scene constraints, and dependence on long-term trajectory prediction. We proposes a non-probability sampling network based on pedestrian trajectory anomaly recognition (ADsampler) to predict multiple possible future pedestrian trajectories. First, by incorporating pose and optical flow information, ADsampler models the multi-dimensional motion characteristics of pedestrians based on observed trajectory information and discriminates trajectory states. The sampling range in the Gaussian latent space is determined based on the recognition results. Next, velocity and yaw information of the car are introduced to model the car's motion state. A subtraction fusion network is employed to remove redundant image feature constraints in highly dynamic scenes. Finally, ADsampler utilizes a novel trajectory decoding network that combines the position encoding capability of GRU with the long-term dependency capturing ability of Transformer to decode and predict the fused features. we evaluate our model on crowded videos in the public datasets JAAD, PIE, ETH and UCY. Experiments demonstrate that the proposed method outperforms state-of-theart approaches in prediction accuracy.
C1 [Liu, Quankai; Sang, Haifeng; Wang, Jinyu; Chen, Wangxing; Liu, Yulong] Shenyang Univ Technol, Sch Informat Sci & Engn, Shenyang 110870, Liaoning, Peoples R China.
C3 Shenyang University of Technology
RP Sang, HF (corresponding author), Shenyang Univ Technol, Sch Informat Sci & Engn, Shenyang 110870, Liaoning, Peoples R China.
EM sanghaif@163.com
FU National Natural Science Founda- tion of China [62173078]; Natural
   Science Foundation of Liaoning Province [2022 -MS -268]
FX This study was supported by the National Natural Science Founda- tion of
   China (62173078) and the Natural Science Foundation of Liaoning Province
   (2022 -MS -268) .
CR Afzal S, 2023, ACM T INTERACT INTEL, V13, DOI 10.1145/3576935
   Alikadic A, 2022, LECT NOTES COMPUT SC, V13599, P179, DOI 10.1007/978-3-031-20716-7_14
   Bae I, 2022, PROC CVPR IEEE, P6467, DOI 10.1109/CVPR52688.2022.00637
   Bhattacharyya A, 2018, PROC CVPR IEEE, P4194, DOI 10.1109/CVPR.2018.00441
   Bouindour Samir, 2017, 8th International Conference on Imaging for Crime Detection and Prevention (ICDP 2017), P1
   Bouindour S, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9040757
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chen GY, 2023, PROC CVPR IEEE, P17874, DOI 10.1109/CVPR52729.2023.01714
   Chen K, 2023, IMAGE VISION COMPUT, V134, DOI 10.1016/j.imavis.2023.104671
   Cunjun Yu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P507, DOI 10.1007/978-3-030-58610-2_30
   Czech P, 2022, 2022 21ST IEEE INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS, ICMLA, P437, DOI 10.1109/ICMLA55696.2022.00070
   Dendorfer Patrick, 2021, Computer Vision - ACCV 2020. 15th Asian Conference on Computer Vision. Lecture Notes in Computer Science (LNCS 12623), P405, DOI 10.1007/978-3-030-69532-3_25
   Gu TP, 2022, PROC CVPR IEEE, P17092, DOI 10.1109/CVPR52688.2022.01660
   Gupta A, 2018, PROC CVPR IEEE, P2255, DOI 10.1109/CVPR.2018.00240
   Halawa M, 2022, LECT NOTES COMPUT SC, V13699, P143, DOI 10.1007/978-3-031-19842-7_9
   Harrou F., 2022, Road Traffic Modeling and Management
   Harrou F, 2020, IEEE INSTRU MEAS MAG, V23, P57, DOI 10.1109/mim.2020.9153576
   Hittawe MM, 2022, IEEE INTL CONF IND I, P107, DOI 10.1109/INDIN51773.2022.9976090
   Mangalam K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15213, DOI 10.1109/ICCV48922.2021.01495
   Mao WB, 2023, PROC CVPR IEEE, P5517, DOI 10.1109/CVPR52729.2023.00534
   Meng M., 2022, Advances in Neural Information Processing Systems
   Neumann L, 2021, PROC CVPR IEEE, P10199, DOI 10.1109/CVPR46437.2021.01007
   Rasouli A, 2019, IEEE I CONF COMP VIS, P6271, DOI 10.1109/ICCV.2019.00636
   Salzmann Tim, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P683, DOI 10.1007/978-3-030-58523-5_40
   Sang HF, 2024, MULTIMED TOOLS APPL, V83, P8533, DOI 10.1007/s11042-023-15989-4
   Sang HF, 2023, MEASUREMENT, V213, DOI 10.1016/j.measurement.2023.112675
   Shi LS, 2023, IEEE T PATTERN ANAL, V45, P11184, DOI 10.1109/TPAMI.2023.3268110
   Su ZX, 2022, 2022 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION (ICRA 2022), P2337, DOI 10.1109/ICRA46639.2022.9812226
   Wang CH, 2022, IEEE ROBOT AUTOM LET, V7, P2716, DOI 10.1109/LRA.2022.3145090
   Wang DF, 2023, IEEE T PATTERN ANAL, V45, P1070, DOI 10.1109/TPAMI.2022.3147639
   Wang J., 2023, Phys. Scr., V99
   Wang RP, 2021, IMAGE VISION COMPUT, V107, DOI 10.1016/j.imavis.2021.104110
   Wu K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10013, DOI 10.1109/ICCV48922.2021.00988
   Xu CX, 2023, PROC CVPR IEEE, P1410, DOI 10.1109/CVPR52729.2023.00142
   Xu HF, 2022, PROC CVPR IEEE, P8111, DOI 10.1109/CVPR52688.2022.00795
   Yang DF, 2022, IEEE T INTELL VEHICL, V7, P221, DOI 10.1109/TIV.2022.3162719
   Yao Y, 2021, IEEE ROBOT AUTOM LET, V6, P1463, DOI 10.1109/LRA.2021.3056339
NR 37
TC 0
Z9 0
U1 8
U2 8
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD MAR
PY 2024
VL 143
AR 104954
DI 10.1016/j.imavis.2024.104954
EA FEB 2024
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA TE9G8
UT WOS:001239697700001
DA 2024-08-05
ER

PT J
AU Gao, H
   Hu, CC
   Han, G
   Mao, JF
   Huang, W
   Guan, Q
AF Gao, Hua
   Hu, Chenchen
   Han, Guang
   Mao, Jiafa
   Huang, Wei
   Guan, Qiu
TI Point-level feature learning based on vision transformer for occluded
   person re-identification
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Occluded person re -identification; Feature learning; Vision
   transformer; Pose estimation
AB Person re -identification is challenging due to the presence of variations in pose and occlusion, which significantly impact the matching of visual features across different camera views and pose considerable difficulty for accurate person re -identification. This paper proposes a novel method for occluded person re -identification by introducing point -level feature learning based on vision transformers. Our approach utilizes a pose estimator to detect the keypoints of the human body and employs these points to locate intermediate features. These intermediate features of keypoints are input to a pose -based transformer branch to learn point -level features. Then, we design a part -based transformer branch to learn part -level features that capture visual features of different image parts, further enhancing the discriminative power of the learned features. Additionally, we employ a global branch to learn the global -level feature by treating the person's image as a single entity. Finally, we integrate point -level, part -level, and global -level features to represent a person's features. The experimental results on occluded and partial person re -identification datasets demonstrate the effectiveness of our proposed approach in improving reidentification. Our approach shows potential for improving person re -identification in scenarios with occlusion and pose variations.
C1 [Gao, Hua; Hu, Chenchen; Mao, Jiafa; Huang, Wei; Guan, Qiu] Zhejiang Univ Technol, Coll Comp Sci, Hangzhou 310014, Peoples R China.
   [Han, Guang] Nanjing Univ Posts & Telecommun, Sch Commun & Informat Engn, Nanjing 210023, Peoples R China.
C3 Zhejiang University of Technology; Nanjing University of Posts &
   Telecommunications
RP Mao, JF (corresponding author), Zhejiang Univ Technol, Coll Comp Sci, Hangzhou 310014, Peoples R China.
EM maojiafa@zjut.edu.cn
FU National Key R & D Program of China [2022YFE0198900]; Natural Science
   Foundation of China [62176237, 61871445]; Zhejiang Provincial Natural
   Science Foundation of China [LY21F020027]; Key Programs for Science and
   Technology Development of Zhejiang Prov- ince [2022C03113]
FX The work was supported by the National Key R & D Program of China (Grant
   No. 2022YFE0198900) , the Natural Science Foundation of China (Grant No.
   62176237 and 61871445) , the Zhejiang Provincial Natural Science
   Foundation of China (Grant No. LY21F020027) , and the Key Programs for
   Science and Technology Development of Zhejiang Prov- ince (Grant No.
   2022C03113) .
CR Ahmed E, 2015, PROC CVPR IEEE, P3908, DOI 10.1109/CVPR.2015.7299016
   Bedagkar-Gala A, 2014, IMAGE VISION COMPUT, V32, P270, DOI 10.1016/j.imavis.2014.02.001
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chen DP, 2016, PROC CVPR IEEE, P1268, DOI 10.1109/CVPR.2016.142
   Chen PX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11813, DOI 10.1109/ICCV48922.2021.01162
   Dosovitskiy A., 2021, ICLR
   Fan HJ, 2024, IEEE T IND INFORM, V20, P442, DOI 10.1109/TII.2023.3266372
   Gou M., 2016, P BRIT MACHINE VISIO
   Gray D, 2008, LECT NOTES COMPUT SC, V5302, P262, DOI 10.1007/978-3-540-88682-2_21
   He LX, 2019, IEEE I CONF COMP VIS, P8449, DOI 10.1109/ICCV.2019.00854
   He ST, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14993, DOI 10.1109/ICCV48922.2021.01474
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Li YL, 2021, PROC CVPR IEEE, P2897, DOI 10.1109/CVPR46437.2021.00292
   Lin X, 2023, COMPUT VIS IMAGE UND, V228, DOI 10.1016/j.cviu.2023.103623
   Lu YH, 2023, IMAGE VISION COMPUT, V131, DOI 10.1016/j.imavis.2023.104633
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2905, DOI 10.1109/TMM.2020.2965491
   Ma BP, 2014, IMAGE VISION COMPUT, V32, P379, DOI 10.1016/j.imavis.2014.04.002
   Ma ZX, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1487, DOI 10.1145/3474085.3475283
   Matsukawa T, 2016, PROC CVPR IEEE, P1363, DOI 10.1109/CVPR.2016.152
   Miao JX, 2019, IEEE I CONF COMP VIS, P542, DOI 10.1109/ICCV.2019.00063
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Shang Gao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11741, DOI 10.1109/CVPR42600.2020.01176
   Somers V, 2023, IEEE WINT CONF APPL, P1613, DOI 10.1109/WACV56688.2023.00166
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Varior RR, 2016, LECT NOTES COMPUT SC, V9911, P135, DOI 10.1007/978-3-319-46478-7_9
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang GA, 2020, PROC CVPR IEEE, P6448, DOI 10.1109/CVPR42600.2020.00648
   Wang SJ, 2023, IEEE T INF FOREN SEC, V18, P147, DOI 10.1109/TIFS.2022.3218449
   Wang T, 2022, AAAI CONF ARTIF INTE, P2540
   Wang ZK, 2022, PROC CVPR IEEE, P4744, DOI 10.1109/CVPR52688.2022.00471
   Xi JL, 2023, PATTERN RECOGN, V134, DOI 10.1016/j.patcog.2022.109068
   Xiang J, 2023, PATTERN RECOGN, V135, DOI 10.1016/j.patcog.2022.109151
   Yang J, 2022, NEURAL COMPUT APPL, V34, P8241, DOI 10.1007/s00521-022-06903-4
   Yang SR, 2023, IEEE T CIRC SYST VID, V33, P283, DOI 10.1109/TCSVT.2022.3199394
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Yi D, 2014, INT C PATT RECOG, P34, DOI 10.1109/ICPR.2014.16
   Zhai Y, 2021, IEEE IJCNN, DOI 10.1109/IJCNN52387.2021.9534442
   Zhao R, 2013, PROC CVPR IEEE, P3586, DOI 10.1109/CVPR.2013.460
   Zhao YB, 2022, NEURAL COMPUT APPL, V34, P17633, DOI 10.1007/s00521-022-07400-4
   Zheng F, 2019, PROC CVPR IEEE, P8506, DOI 10.1109/CVPR.2019.00871
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng WS, 2015, IEEE I CONF COMP VIS, P4678, DOI 10.1109/ICCV.2015.531
   Zheng WS, 2011, PROC CVPR IEEE, P649, DOI 10.1109/CVPR.2011.5995598
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhu YJ, 2023, J VIS COMMUN IMAGE R, V90, DOI 10.1016/j.jvcir.2022.103714
   Zhuo JX, 2018, IEEE INT CON MULTI
NR 47
TC 0
Z9 0
U1 11
U2 11
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD MAR
PY 2024
VL 143
AR 104929
DI 10.1016/j.imavis.2024.104929
EA FEB 2024
PG 10
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA LA2W2
UT WOS:001183997700001
OA hybrid
DA 2024-08-05
ER

PT J
AU Hambarde, K
   Proenca, H
AF Hambarde, Kailash
   Proenca, Hugo
TI Image-based human re-identification: Which covariates are actually (the
   most) important?
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Human re -identification; Performance covariates; Biometric menagerie
ID PERSON REIDENTIFICATION
AB Human re-identification (re-ID) is nowadays among the most popular topics in computer vision, due to the increasing importance given to safety/security in modern societies. Being expected to sun in totally uncontrolled data acquisition settings (e.g., visual surveillance) automated re-ID not only depends on various factors that may occur in non-controlled data acquisition settings, but - most importantly - performance varies with respect to different subject features (e.g., gender, height, ethnicity, clothing, and action being performed), which may result in highly biased and undesirable automata. While many efforts have been putted in increase the robustness of identification to uncontrolled settings, a systematic assessment of the actual variations in performance with respect to each subject feature remains to be done. Accordingly, the contributions of this paper are threefold: 1) we report the correlation between the performance of three state-of-the-art re-ID models and different subject features; 2) we discuss the most concerning features and report valuable insights about the roles of the various features in re-ID performance, which can be used to develop more effective and unbiased re-ID systems; and 3) we leverage the concept of biometric menagerie, in order to identify the groups of individuals that typically fall into the most common menagerie families (e.g., goats, lambs, and wolves). Our findings not only contribute to a better understanding of the factors affecting re-ID performance, but also may offer practical guidance for researchers and practitioners concerned on human re-identification development.
C1 [Hambarde, Kailash; Proenca, Hugo] Univ Beira Interior, IT, Inst Telecomunicacoes, P-6201001 Covilha, Portugal.
C3 Instituto de Telecomunicacoes; Universidade da Beira Interior
RP Hambarde, K (corresponding author), Univ Beira Interior, IT, Inst Telecomunicacoes, P-6201001 Covilha, Portugal.
EM kailas.srt@gmail.com
RI Proença, Hugo/F-9499-2010; Hambarde, Kailash Anandrao/JVP-3710-2024
OI Proença, Hugo/0000-0003-2551-8570; Hambarde, Kailash
   Anandrao/0000-0003-1012-2952
FU FCT/MCTES; EU [UIDB/50008/2020]
FX * This work is funded by FCT/MCTES through national funds and cofounded
   by EU funds under the project UIDB/50008/2020.
CR [Anonymous], 2013, Information and Media Technologies
   Bai K, 2022, PROCEDIA COMPUT SCI, V199, P276, DOI 10.1016/j.procs.2022.01.034
   Chang HY, 2023, ENTERP INF SYST-UK, V17, DOI 10.1080/17517575.2021.1941274
   Chang XB, 2018, PROC CVPR IEEE, P2109, DOI 10.1109/CVPR.2018.00225
   Chen CQ, 2023, PROC CVPR IEEE, P15128, DOI 10.1109/CVPR52729.2023.01452
   Chen DP, 2018, PROC CVPR IEEE, pCP1, DOI 10.1109/CVPR.2018.00128
   Chen H, 2021, PROC CVPR IEEE, P2004, DOI 10.1109/CVPR46437.2021.00204
   Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145
   Chen YC, 2019, AAAI CONF ARTIF INTE, P8215
   Deng YB, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P789, DOI 10.1145/2647868.2654966
   Ge YX, 2018, ADV NEUR IN, V31
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hirzer M, 2011, LECT NOTES COMPUT SC, V6688, P91, DOI 10.1007/978-3-642-21227-7_9
   Huang MY, 2023, IEEE T IMAGE PROCESS, V32, P1568, DOI 10.1109/TIP.2023.3247159
   Huang NC, 2022, PATTERN RECOGN, V128, DOI 10.1016/j.patcog.2022.108653
   Iscen Ahmet, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P286, DOI 10.1007/978-3-030-58607-2_17
   Jia M., 2023, P AAAI C ART INT, V37, P998
   Khamis S, 2015, LECT NOTES COMPUT SC, V8927, P134, DOI 10.1007/978-3-319-16199-0_10
   Kohli Puneet, 2020, Advances in Information and Communication. Proceedings of the 2019 Future of Information and Communication Conference (FICC). Lecture Notes in Networks and Systems (LNNS 69), P261, DOI 10.1007/978-3-030-12388-8_19
   Kumar SVA, 2021, IEEE T INF FOREN SEC, V16, P1696, DOI 10.1109/TIFS.2020.3040881
   Li JN, 2019, AAAI CONF ARTIF INTE, P8618
   Li SZ, 2020, PATTERN RECOGN, V97, DOI 10.1016/j.patcog.2019.107016
   Li YJ, 2019, IEEE I CONF COMP VIS, P8089, DOI 10.1109/ICCV.2019.00818
   Liao XY, 2019, LECT NOTES COMPUT SC, V11366, P620, DOI 10.1007/978-3-030-20876-9_39
   Lin YT, 2019, PATTERN RECOGN, V95, P151, DOI 10.1016/j.patcog.2019.06.006
   Liu HM, 2021, NEUROCOMPUTING, V423, P57, DOI 10.1016/j.neucom.2020.10.019
   Liu JW, 2021, PROC CVPR IEEE, P4368, DOI 10.1109/CVPR46437.2021.00435
   Luo CC, 2019, IEEE I CONF COMP VIS, P4975, DOI 10.1109/ICCV.2019.00508
   Ming ZQ, 2022, IMAGE VISION COMPUT, V119, DOI 10.1016/j.imavis.2022.104394
   Nguyen BX, 2021, IEEE COMPUT SOC CONF, P3487, DOI 10.1109/CVPRW53098.2021.00388
   Ning X, 2021, NEUROCOMPUTING, V453, P801, DOI 10.1016/j.neucom.2020.05.106
   Qian XL, 2017, IEEE I CONF COMP VIS, P5409, DOI 10.1109/ICCV.2017.577
   Schumann A, 2017, IEEE COMPUT SOC CONF, P1435, DOI 10.1109/CVPRW.2017.186
   Shen YT, 2018, LECT NOTES COMPUT SC, V11219, P508, DOI 10.1007/978-3-030-01267-0_30
   Shoukry N, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12062835
   Song CF, 2018, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2018.00129
   Su C, 2018, PATTERN RECOGN, V75, P77, DOI 10.1016/j.patcog.2017.07.005
   Su C, 2016, LECT NOTES COMPUT SC, V9906, P475, DOI 10.1007/978-3-319-46475-6_30
   Sun YF, 2019, PROC CVPR IEEE, P393, DOI 10.1109/CVPR.2019.00048
   Wang YC, 2018, PROC CVPR IEEE, P1470, DOI 10.1109/CVPR.2018.00159
   Wang Z, 2023, IET COMPUT VIS, V17, P977, DOI 10.1049/cvi2.12215
   Wu DD, 2022, HELIYON, V8, DOI 10.1016/j.heliyon.2022.e12086
   Wu YM, 2020, IEEE T IMAGE PROCESS, V29, P8821, DOI 10.1109/TIP.2020.3001693
   Xian YQ, 2023, IEEE WINT CONF APPL, P4778, DOI 10.1109/WACV56688.2023.00477
   Xu RY, 2023, J VIS COMMUN IMAGE R, V94, DOI 10.1016/j.jvcir.2023.103849
   Xu SM, 2022, KNOWL-BASED SYST, V252, DOI 10.1016/j.knosys.2022.109354
   Yager N, 2010, IEEE T PATTERN ANAL, V32, P220, DOI 10.1109/TPAMI.2008.291
   Yan YC, 2020, PROC CVPR IEEE, P2896, DOI 10.1109/CVPR42600.2020.00297
   Yang JR, 2020, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR42600.2020.00335
   Yang WJ, 2019, PROC CVPR IEEE, P1389, DOI 10.1109/CVPR.2019.00148
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Yukun Huang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14072, DOI 10.1109/CVPR42600.2020.01409
   Zhang YQ, 2024, IEEE T MULTIMEDIA, V26, P1089, DOI 10.1109/TMM.2023.3276167
   Zhang ZZ, 2020, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR42600.2020.00325
   Zhang Z, 2021, PATTERN RECOGN, V120, DOI 10.1016/j.patcog.2021.108155
   Zhou KY, 2019, Arxiv, DOI arXiv:1910.10093
NR 57
TC 0
Z9 0
U1 2
U2 2
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD MAR
PY 2024
VL 143
AR 104917
DI 10.1016/j.imavis.2024.104917
EA FEB 2024
PG 11
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA JW3Y6
UT WOS:001176171700001
OA hybrid
DA 2024-08-05
ER

PT J
AU Zhao, GZ
   Zhang, C
   Wang, XP
   Lin, BW
   Yan, FH
AF Zhao, Guangzhe
   Zhang, Chen
   Wang, Xueping
   Lin, Benwang
   Yan, Feihu
TI PMANet: Progressive multi-stage attention networks for skin disease
   classification
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Skin disease classification; Progressive networks; Attention mechanism
ID MODEL
AB Automated skin disease classification is crucial for the timely diagnosis of skin lesions. However, accurate skin disease classification presents a challenge, given the significant intra-class variation and inter-class similarity among different kinds of skin diseases. Previous studies have attempted to address this issue by identifying the most discriminative part of a lesion, but they tend to overlook the interactions between multi-scale features. In this paper, we propose a Progressive Multi-stage Attention Network (PMANet) to enhance the learning of multiscale discriminative features, so that the model can gradually localize from stable fine-grained to coarse-grained regions in order to improve the accuracy of disease classification. Specifically, we utilize a progressive multistage network to supervise feature and classification, thereby fostering multi-scale information and improving the model's ability to learn intra-class consistent information. Additionally, we propose an enhanced region proposal block that highlights key discriminative features and suppresses background noise of lesions, reinforcing the learning of inter-class discriminative features. Furthermore, we propose a multi-branch feature fusion block that effectively fuses multi-scale lesion features from different stages. Comprehensive experiments conducted on two datasets substantiate the effectiveness and superiority of the proposed method in accurately classifying skin disease.
C1 [Zhao, Guangzhe; Zhang, Chen; Wang, Xueping; Lin, Benwang; Yan, Feihu] Beijing Univ Civil Engn & Architecture, Sch Elect & Informat Engn, Beijing 100044, Peoples R China.
C3 Beijing University of Civil Engineering & Architecture
RP Wang, XP (corresponding author), Beijing Univ Civil Engn & Architecture, Sch Elect & Informat Engn, Beijing 100044, Peoples R China.
EM wangxueping@bucea.edu.cn
FU National Natural Science Foundation of China [62176018]; R & D Program
   of Beijing Municipal Education Commission [KM202410016010]
FX <B>Funding</B> This work was supported by both the National Natural
   Science Foundation of China (Grant No. 62176018) and R & D Program of
   Beijing Municipal Education Commission (KM202410016010) .
CR Ahn N, 2018, IEEE COMPUT SOC CONF, P904, DOI 10.1109/CVPRW.2018.00123
   Alenezi F, 2023, EXPERT SYST APPL, V213, DOI 10.1016/j.eswa.2022.119064
   Ayas S, 2023, NEURAL COMPUT APPL, V35, P6713, DOI 10.1007/s00521-022-08053-z
   Barata C, 2019, IEEE J BIOMED HEALTH, V23, P1096, DOI 10.1109/JBHI.2018.2845939
   Biasi LD, 2022, IEEE J BIOMED HEALTH, V26, P962, DOI 10.1109/JBHI.2021.3113609
   Cai G, 2023, VISUAL COMPUT, V39, P2781, DOI 10.1007/s00371-022-02492-4
   Clevert DA, 2016, Arxiv, DOI [arXiv:1511.07289, DOI 10.48550/ARXIV.1511.07289]
   Combalia M, 2019, arXiv
   Dahmani D, 2020, IMAGE VISION COMPUT, V99, DOI 10.1016/j.imavis.2020.103925
   Datta SK, 2021, LECT NOTES COMPUT SC, V12929, P13, DOI 10.1007/978-3-030-87444-5_2
   Esteva A, 2017, NATURE, V546, P686, DOI 10.1038/nature22985
   Frisinger A, 2023, BMC PRIM CARE, V24, DOI 10.1186/s12875-023-02024-6
   Furriel BCRS, 2024, FRONT MED-LAUSANNE, V10, DOI 10.3389/fmed.2023.1305954
   González-Díaz I, 2019, IEEE J BIOMED HEALTH, V23, P547, DOI 10.1109/JBHI.2018.2806962
   Grandini M, 2020, Arxiv, DOI [arXiv:2008.05756, 10.48550/arXiv.2008.05756, DOI 10.48550/ARXIV.2008.05756]
   Hasan MK, 2023, COMPUT BIOL MED, V155, DOI 10.1016/j.compbiomed.2023.106624
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He XZ, 2022, MED IMAGE ANAL, V77, DOI 10.1016/j.media.2022.102357
   Hu H, 2019, IEEE I CONF COMP VIS, P3463, DOI 10.1109/ICCV.2019.00356
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang HW, 2021, J DERMATOL, V48, P310, DOI 10.1111/1346-8138.15683
   Huang XP, 2023, IMAGE VISION COMPUT, V137, DOI 10.1016/j.imavis.2023.104742
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Iqbal I, 2021, COMPUT MED IMAG GRAP, V88, DOI 10.1016/j.compmedimag.2020.101843
   Jaderberg M., Advances in Neural Information Processing Systems, V28
   Jia X, 2017, Arxiv, DOI arXiv:1703.01053
   Karthik R, 2022, BIOMED SIGNAL PROCES, V73, DOI 10.1016/j.bspc.2021.103406
   Khan MA, 2019, 2019 INTERNATIONAL CONFERENCE ON COMPUTER AND INFORMATION SCIENCES (ICCIS), P63, DOI 10.1109/iccisci.2019.8716400
   Lenc K, 2015, PROC CVPR IEEE, P991, DOI 10.1109/CVPR.2015.7298701
   Liu QD, 2020, IEEE T MED IMAGING, V39, P3429, DOI 10.1109/TMI.2020.2995518
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Manjunath R., 2023, Multimed. Tools Appl., P1
   Maqsood S, 2023, NEURAL NETWORKS, V160, P238, DOI 10.1016/j.neunet.2023.01.022
   Menegola A, 2017, Arxiv, DOI arXiv:1703.04819
   Mohamed Ensaf Hussein, 2019, 2019 Ninth International Conference on Intelligent Computing and Information Systems (ICICIS), P180, DOI 10.1109/ICICIS46948.2019.9014823
   Nakai K, 2022, BIOMED SIGNAL PROCES, V78, DOI 10.1016/j.bspc.2022.103997
   Naqvi SAR, 2023, IEEE T BIO-MED ENG, V70, P628, DOI 10.1109/TBME.2022.3199094
   Pacheco AGC, 2021, IEEE J BIOMED HEALTH, V25, P3554, DOI 10.1109/JBHI.2021.3062002
   Pampena R, 2017, J AM ACAD DERMATOL, V77, P938, DOI 10.1016/j.jaad.2017.06.149
   Ramamurthy K, 2023, CONCURR COMP-PRACT E, V35, DOI 10.1002/cpe.7834
   Reis HC, 2022, MED BIOL ENG COMPUT, V60, P643, DOI 10.1007/s11517-021-02473-0
   Ruoyi Du, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P153, DOI 10.1007/978-3-030-58565-5_10
   Cohen TS, 2015, Arxiv, DOI arXiv:1412.7659
   Sarker MMK, 2022, LECT NOTES COMPUT SC, V13413, P651, DOI 10.1007/978-3-031-12053-4_48
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Song L, 2020, IEEE J BIOMED HEALTH, V24, P2912, DOI 10.1109/JBHI.2020.2973614
   Swift Amelia, 2020, Evid Based Nurs, V23, P2, DOI 10.1136/ebnurs-2019-103225
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tang P, 2020, IEEE J BIOMED HEALTH, V24, P2870, DOI 10.1109/JBHI.2020.2977013
   Tapia JE, 2022, IEEE T INF FOREN SEC, V17, P42, DOI 10.1109/TIFS.2021.3132582
   Tschandl P., 2018, The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions, V5, P1, DOI DOI 10.7910/DVN/DBW86T
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vidhyalakshmi AM, 2024, NEURAL COMPUT APPL, V36, P4311, DOI 10.1007/s00521-023-09011-z
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang LT, 2023, MED IMAGE ANAL, V85, DOI 10.1016/j.media.2023.102746
   Wang YF, 2018, IEEE COMPUT SOC CONF, P977, DOI 10.1109/CVPRW.2018.00131
   Wei ZH, 2022, BIOMED SIGNAL PROCES, V74, DOI 10.1016/j.bspc.2022.103549
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xie YT, 2020, IEEE T MED IMAGING, V39, P2482, DOI 10.1109/TMI.2020.2972964
   Xin C, 2022, COMPUT BIOL MED, V149, DOI 10.1016/j.compbiomed.2022.105939
   Xue X, 2021, INT C PATT RECOG, P9083, DOI 10.1109/ICPR48806.2021.9412042
   Yiming Zhang, 2021, 2021 IEEE 2nd International Conference on Big Data, Artificial Intelligence and Internet of Things Engineering (ICBAIE), P14, DOI 10.1109/ICBAIE52039.2021.9389983
   Yu LQ, 2017, IEEE T MED IMAGING, V36, P994, DOI 10.1109/TMI.2016.2642839
   Zhang JB, 2020, IEEE T KNOWL DATA EN, V32, P468, DOI [10.1109/TKDE.2019.2891537, 10.1109/TMI.2019.2893944]
NR 65
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD SEP
PY 2024
VL 149
AR 105166
DI 10.1016/j.imavis.2024.105166
EA JUL 2024
PG 11
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA YS4W0
UT WOS:001270469400001
DA 2024-08-05
ER

PT J
AU Zhao, ZK
   Wang, YC
   Zhang, N
   Zhang, YX
   Li, Z
   Chen, C
AF Zhao, Zhikang
   Wang, Yongcheng
   Zhang, Ning
   Zhang, Yuxi
   Li, Zheng
   Chen, Chi
TI A method of degradation mechanism-based unsupervised remote sensing
   image super-resolution
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Super-resolution; Remote sensing; Deep learning; Unsupervised learning;
   Degradation mechanism
AB Remote sensing image (RSI) super -resolution (SR) is an efficient and low-cost technique to achieve highresolution and high-quality reconstruction images. The quality of RSI SR reconstruction is affected by the prior information contained in the degradation model. Therefore, studying how to incorporate more RSI degradation prior into the degradation model is crucial. This article presents an approach to design the degradation model by extracting degradation factors from the perspective of remote sensing imaging mechanisms. It includes two aspects: simulating the atmospheric scattering effect through RGB channel weights downsampling and the comprehensive degradation effect of the remote sensing imaging platform through combined blurring. Furthermore, we proposed a high-performance RSI SR network based on degradation mechanism (RSN-DM), which includes a degrader D and a generator G , to employ remote sensing prior fully. We conducted experiments on the UC Merced Land-Use and WPU-RESIS45 datasets, demonstrating that our proposed method is effective. Our method achieves state -of -the -art (SOTA) performance in quantitative evaluation and visual quality. Finally, we apply the proposed degradation model to other networks to further validate the model's effectiveness. Therefore, the degradation model proposed in this paper can enhance the performance of remote sensing image super -resolution techniques in practical applications.
C1 [Zhao, Zhikang; Wang, Yongcheng; Zhang, Yuxi; Li, Zheng; Chen, Chi] Chinese Acad Sci, Changchun Inst Opt Fine Mech & Phys, Changchun 130033, Jilin, Peoples R China.
   [Zhang, Ning] Tsinghua Univ, Dept Elect Engn, Beijing 100049, Peoples R China.
C3 Chinese Academy of Sciences; Changchun Institute of Optics, Fine
   Mechanics & Physics, CAS; Tsinghua University
RP Wang, YC (corresponding author), Chinese Acad Sci, Changchun Inst Opt Fine Mech & Phys, Changchun 130033, Jilin, Peoples R China.
EM zhaozhikang20@mails.ucas.ac.cn; wangyc@ciomp.ac.cn;
   cdd_ningzhang@tsinghua.edu.cn; zhangyuxi18@mails.ucas.ac.cn;
   lizheng20@mails.ucas.ac.cn; chenchi21@mails.ucas.ac.cn
CR Anbarjafari G, 2010, ETRI J, V32, P390, DOI 10.4218/etrij.10.0109.0303
   Andrews L.C., 2005, Laser Beam Propagation Through Random Media, Vsecond
   [Anonymous], 2017, Recommendation ITU-R P.1411-9. Propagation data and prediction methods for the planning of short-range outdoor radiocommunication systems and radio local area networks in the frequency range 300 MHz to 100 GHz
   Bell-Kligler S, 2019, ADV NEUR IN, V32
   Cheng G, 2017, P IEEE, V105, P1865, DOI 10.1109/JPROC.2017.2675998
   Cheng Ma, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7766, DOI 10.1109/CVPR42600.2020.00779
   Chung MKY, 2023, KOREAN J REMOTE SENS, V39, P395, DOI 10.7780/kjrs.2023.39.4.2
   Dai D., 2016, 2016 IEEE WINT C APP, P1, DOI DOI 10.1109/WACV.2016.7477613
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1618, DOI 10.1109/TIP.2012.2235847
   Dong XY, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11232857
   Efrat N, 2013, IEEE I CONF COMP VIS, P2832, DOI 10.1109/ICCV.2013.352
   Egiazarian K, 2015, EUR SIGNAL PR CONF, P2849, DOI 10.1109/EUSIPCO.2015.7362905
   Elachi C., 2021, Basic Principles of Atmospheric Sensing and Radiative Transfer, P377, DOI [10.1002/9781119523048.ch8, DOI 10.1002/9781119523048.CH8]
   Gao G., 2022, arXiv
   He H, 2011, PROC CVPR IEEE, P449, DOI 10.1109/CVPR.2011.5995713
   Huang Yan, 2020, Advances in Neural Information Processing Systems, V33, P5632, DOI DOI 10.48550/ARXIV.2010.02631
   Ji XZ, 2020, IEEE COMPUT SOC CONF, P1914, DOI 10.1109/CVPRW50498.2020.00241
   Ken T., 1990, Filters for Common Resampling Tasks, P147, DOI DOI 10.1016/B978-0-08-050753-8.50042-5
   Kim KI, 2010, IEEE T PATTERN ANAL, V32, P1127, DOI 10.1109/TPAMI.2010.25
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lei S, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3136190
   Lei S, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3069889
   Li F, 2023, IEEE T MULTIMEDIA, V25, P2825, DOI 10.1109/TMM.2022.3152090
   Liang JY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4076, DOI 10.1109/ICCV48922.2021.00406
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Lin H, 2020, IET IMAGE PROCESS, V14, P4520, DOI 10.1049/iet-ipr.2020.1176
   Liu YQ, 2021, IEEE T CIRC SYST VID, V31, P829, DOI 10.1109/TCSVT.2020.2990623
   Michaeli T, 2014, LECT NOTES COMPUT SC, V8691, P783, DOI 10.1007/978-3-319-10578-9_51
   Musunuri YR, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10050555
   Pooja S, 2023, MULTIMED TOOLS APPL, V82, P24181, DOI 10.1007/s11042-023-14335-y
   [秦世引 QIN Shiyin], 2011, [科技导报, Science & Technology Review], V29, P26
   Riegler G, 2015, IEEE I CONF COMP VIS, P522, DOI 10.1109/ICCV.2015.67
   Romano Y, 2017, IEEE T COMPUT IMAG, V3, P110, DOI 10.1109/TCI.2016.2629284
   Salby M.L., 2012, PHYS ATMOSPHERE CLIM
   Seinfeld J. H., 2006, Atmospheric chemistry and physics: from air pollution to climate change
   Sheng P.X., 2013, Atmospheric Physics
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Shocher A, 2018, PROC CVPR IEEE, P3118, DOI 10.1109/CVPR.2018.00329
   Sun J, 2008, PROC CVPR IEEE, P2471, DOI 10.1109/CVPR.2008.4587659
   Wang HY, 2021, LECT NOTES COMPUT SC, V12901, P131, DOI 10.1007/978-3-030-87193-2_13
   Wang L, 2020, PROC CVPR IEEE, P3773, DOI 10.1109/CVPR42600.2020.00383
   Wang LG, 2021, PROC CVPR IEEE, P10576, DOI 10.1109/CVPR46437.2021.01044
   Wang W, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4298, DOI 10.1109/ICCV48922.2021.00428
   Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002
   Wang XT, 2021, IEEE INT CONF COMP V, P1905, DOI 10.1109/ICCVW54120.2021.00217
   Wang ZZ, 2005, IMAGE VISION COMPUT, V23, P393, DOI 10.1016/j.imavis.2004.11.001
   Wu HJ, 2023, IMAGE VISION COMPUT, V140, DOI 10.1016/j.imavis.2023.104857
   Wu W, 2011, IMAGE VISION COMPUT, V29, P394, DOI 10.1016/j.imavis.2011.02.001
   Yang CY, 2014, LECT NOTES COMPUT SC, V8692, P372, DOI 10.1007/978-3-319-10593-2_25
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yang Y., 2010, P 18 SIGSPATIAL INT, DOI [10.1145/1869790.1869829, DOI 10.1145/1869790.1869829]
   Zhang JZ, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14122895
   Zhang K., 2017, Proceedings of the IEEE 2017 Conference on Computer Vision and Pattern Recognition, DOI [DOI 10.1109/CVPR.2017.300, 10.1109/CVPR.2017.300]
   Zhang K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4771, DOI 10.1109/ICCV48922.2021.00475
   Zhang K, 2019, PROC CVPR IEEE, P1671, DOI 10.1109/CVPR.2019.00177
   Zhang K, 2018, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2018.00344
   Zhang N, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2020.3042460
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhao Y., 2013, Principles and Methods of Remote Sensing Application Analysis, V2, P19
NR 61
TC 0
Z9 0
U1 1
U2 1
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD AUG
PY 2024
VL 148
AR 105108
DI 10.1016/j.imavis.2024.105108
EA JUN 2024
PG 16
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA XA9C4
UT WOS:001259067800001
DA 2024-08-05
ER

PT J
AU Zhou, CL
   Zhang, W
   Lian, ZC
AF Zhou, Chenglin
   Zhang, Wei
   Lian, Zhichao
TI Enhancing consistency in virtual try-on: A novel diffusion-based
   approach
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Virtual try-on; Diffusion models
AB In new technology scenarios, virtual try-on aims to integrate clothing onto body images naturally, enhancing shopping experience by simulating true effect of clothes. As the image resolution increases, we expect to improve the consistency of the result, i.e., to ensure that various elements of the image are harmonized in terms of color, shading, style, and texture in order to achieve a natural visual effect. Many studies based on Generative Adversarial Networks(GANs) struggle with consistency. They encounter challenges in accurately depicting the fabric of target garments, as well as natural shadows and folds, and sometimes exhibit visual discontinuities or inconsistencies. So, we propose a new approach CSD-VTON based on latent diffusion model. Considering that traditional UNet's computational primitives struggle to capture complex transformation relationships at the pixel level, we address this issue by concatenating the warped cloth images generated by the warping module with the noise image. Additionally, cascade feature extraction module is introduced to extract in-store garment features, which ensures the preservation of texture and details in the target garments. Finally, we incorporate the skipconnection supplementary module to compensate for the reconstruction error. We conducted experiments using the DressCode and VITON-HD datasets to demonstrate the effectiveness and superiority of our approach.
C1 [Zhou, Chenglin; Zhang, Wei; Lian, Zhichao] Nanjing Univ Sci & Technol, Nanjing, Peoples R China.
C3 Nanjing University of Science & Technology
RP Lian, ZC (corresponding author), Nanjing Univ Sci & Technol, Nanjing, Peoples R China.
EM zhouchenglin@njust.edu.cn; zwplus_pro@njust.edu.cn;
   newlzcts@njust.edu.cn
FU National Key R & D Program of China [2021YFF0602104-2]
FX This work is supported by the National Key R & D Program of China
   (2021YFF0602104-2) .
CR Bai S, 2022, LECT NOTES COMPUT SC, V13675, P409, DOI 10.1007/978-3-031-19784-0_24
   Baldrati A., 2023, Multimodal Garment Designer: Human-Centric Latent Diffusion Models for Fashion Image Editing
   Binkowski M., 2018, ARXIV180101401
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Choi S, 2021, PROC CVPR IEEE, P14126, DOI 10.1109/CVPR46437.2021.01391
   Dong H, 2019, IEEE I CONF COMP VIS, P9025, DOI 10.1109/ICCV.2019.00912
   Ge YY, 2021, PROC CVPR IEEE, P8481, DOI 10.1109/CVPR46437.2021.00838
   GOODFELLOW I, 2014, ADV NEURAL INFORM PR, V27
   Güler RA, 2018, PROC CVPR IEEE, P7297, DOI 10.1109/CVPR.2018.00762
   Han XT, 2019, IEEE I CONF COMP VIS, P10470, DOI 10.1109/ICCV.2019.01057
   Han XT, 2018, PROC CVPR IEEE, P7543, DOI 10.1109/CVPR.2018.00787
   Han Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7847, DOI 10.1109/CVPR42600.2020.00787
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He S, 2022, PROC CVPR IEEE, P3460, DOI 10.1109/CVPR52688.2022.00346
   Heusel M., 2017, NeurIPS, P6629
   Ho J., 2020, Adv. Neural. Inf. Process. Syst, V33, P6840, DOI DOI 10.48550/ARXIV.2006.11239
   Hsieh CW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P275, DOI 10.1145/3343031.3351075
   Gulrajani I, 2017, ADV NEUR IN, V30
   Issenhuth T, 2019, Arxiv, DOI arXiv:1906.01347
   Jandial Surgan, 2020, 2020 IEEE Winter Conference on Applications of Computer Vision (WACV). Proceedings, P2171, DOI 10.1109/WACV45572.2020.9093458
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Lee HJ, 2019, IEEE INT CONF COMP V, P3129, DOI 10.1109/ICCVW.2019.00381
   Lee S, 2022, LECT NOTES COMPUT SC, V13677, P204, DOI 10.1007/978-3-031-19790-1_13
   Lu Z., 2024, P IEEECVF WINTER C A, P5374
   Minar M.R., 2020, CVPR WORKSH, V3, P10
   Morelli D, 2022, IEEE COMPUT SOC CONF, P2230, DOI 10.1109/CVPRW56347.2022.00243
   Nichol A, 2021, Glide: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models
   Pandey N, 2020, NEUROCOMPUTING, V414, P356, DOI 10.1016/j.neucom.2020.07.092
   Pernus M, 2023, Arxiv, DOI arXiv:2301.02110
   Radford A, 2021, PR MACH LEARN RES, V139
   Radford L., 2016, Unsupervised representation learning with deep convolutional generative adversarial networks, P1
   Raffiee AH, 2021, INT C PATT RECOG, P3923, DOI 10.1109/ICPR48806.2021.9412908
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Roy D, 2024, IEEE T EM TOP COMP I, V8, P1853, DOI 10.1109/TETCI.2024.3353080
   Saharia C., 2022, ADV NEURAL INF PROCE, V35, P36479
   Song J., 2021, INT C LEARN REPR
   Wang BC, 2018, LECT NOTES COMPUT SC, V11217, P607, DOI 10.1007/978-3-030-01261-8_36
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wolberg G., 1990, Digital Image Warping, V10662, P90720
   Xie Z., 2021, Towards Scalable Unpaired Virtual Try-on Via Patch-Routed Spatially-Adaptive GAN
   Xie Z, 2023, PROC CVPR IEEE, P23550, DOI 10.1109/CVPR52729.2023.02255
   Yu RY, 2019, IEEE I CONF COMP VIS, P10510, DOI 10.1109/ICCV.2019.01061
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
NR 43
TC 0
Z9 0
U1 1
U2 1
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD AUG
PY 2024
VL 148
AR 105097
DI 10.1016/j.imavis.2024.105097
EA JUN 2024
PG 9
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA WB6O1
UT WOS:001252449300001
DA 2024-08-05
ER

PT J
AU Cavallaro, A
   Perillo, F
   Romano, M
   Sebillo, M
   Vitiello, G
AF Cavallaro, Antonella
   Perillo, Francesca
   Romano, Marco
   Sebillo, Monica
   Vitiello, Giuliana
TI Social robot in service of the cognitive therapy of elderly people:
   Exploring robot acceptance in a real-world scenario
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Social robot; Elderly; Psycological test
ID TRAIL
AB Aging is a global demographic trend that is leading to an increase in the prevalence of cognitive disorders. Innovative healthcare solutions are needed to meet the growing demand for assistance. Robots equipped with advanced artificial intelligence, sensors and social interaction capabilities offer promising tools to address the challenges of cognitive decline in the elderly. The main objective of this study is to assess the acceptance of social robots in healthcare among elderly people. Specifically, we provide the social robot Furhat with the ability to greet patients and assess their cognitive function through neuropsychological tests. The experiment involves 26 elderly people interacting with the social robot Furhat in a clinical setting. A questionnaire is administered in order to carry out our results to understand the usability, perceived ease of use, perceived usefulness, intention to use and comfort in interaction. In addition, a group discussion highlights some opinions and feedbacks among the participants. Our results suggest that the integration of social robots into cognitive testing for the elderly has a positive outlook.
C1 [Cavallaro, Antonella; Perillo, Francesca; Sebillo, Monica; Vitiello, Giuliana] Univ Salerno, Dept Comp Sci, I-84084 Fisciano, SA, Italy.
   [Romano, Marco] Int Univ Rome UNINT, Dept Int Humanities & Social Sci, Rome, Italy.
C3 University of Salerno
RP Perillo, F (corresponding author), Univ Salerno, Dept Comp Sci, I-84084 Fisciano, SA, Italy.
EM fperillo@unisa.it
CR Abdi J, 2018, BMJ OPEN, V8, DOI 10.1136/bmjopen-2017-018815
   Ashendorf L, 2008, ARCH CLIN NEUROPSYCH, V23, P129, DOI 10.1016/j.acn.2007.11.005
   Battistoni P., 2023, Using artificial intelligence and companion robots to improve home healthcare for the elderly, P3, DOI [10.1007/978-3-031-48,041-61, DOI 10.1007/978-3-031-48,041-61]
   Battistoni P, 2023, MULTIMODAL TECHNOLOG, V7, DOI 10.3390/mti7030024
   Belleville S, 1996, NEUROPSYCHOLOGIA, V34, P195, DOI 10.1016/0028-3932(95)00097-6
   Belpaeme T, 2018, SCI ROBOT, V3, DOI 10.1126/scirobotics.aat5954
   Bisiacchi P., 2003, Esame Neuropsicologico Breve, una Batteria di Test per lo Screening Neuropsicologico
   Bowie CR, 2006, NAT PROTOC, V1, P2277, DOI 10.1038/nprot.2006.390
   Breazeal C., 2004, Designing sociable robots
   Breazeal C, 2016, SPRINGER HANDBOOK OF ROBOTICS, P1935
   Broekens Joost, 2009, Gerontechnology, V8, P94, DOI 10.4017/gt.2009.08.02.002.00
   Cantone AA, 2023, ELECTRONICS-SWITZ, V12, DOI 10.3390/electronics12183918
   Castillo Jose Carlos, 2014, Ambient Assisted Living and Daily Activities. 6th International Work-Conference, IWAAL 2014. Proceedings: LNCS 8868, P320, DOI 10.1007/978-3-319-13105-4_46
   Cavallo F, 2018, J MED INTERNET RES, V20, DOI 10.2196/jmir.9460
   Chan JYC, 2021, AGEING RES REV, V72, DOI 10.1016/j.arr.2021.101506
   Chater N, 2023, PHILOS T R SOC A, V381, DOI 10.1098/rsta.2022.0040
   Clarke V, 2013, PSYCHOLOGIST, V26, P120
   DAVIS FD, 1989, MANAGE SCI, V35, P982, DOI 10.1287/mnsc.35.8.982
   FOLSTEIN MF, 1975, J PSYCHIAT RES, V12, P189, DOI 10.1016/0022-3956(75)90026-6
   Fong T, 2003, ROBOT AUTON SYST, V42, P143, DOI 10.1016/S0921-8890(02)00372-X
   Hurtado LC, 2021, HEALTHCARE-BASEL, V9, DOI 10.3390/healthcare9081067
   Ihamäki P, 2024, INFORM SYST FRONT, V26, P25, DOI 10.1007/s10796-021-10175-z
   Iroju O., 2017, State of the art: a study of human-robot interaction in healthcare
   Lytridis C, 2019, INT CONF SOFTW, P403, DOI 10.23919/softcom.2019.8903630
   Ma BX, 2023, AGEING RES REV, V83, DOI 10.1016/j.arr.2022.101808
   Nasreddine ZS, 2005, J AM GERIATR SOC, V53, P695, DOI 10.1111/j.1532-5415.2005.53221.x
   Noble SM, 2023, J ACAD MARKET SCI, V51, P747, DOI 10.1007/s11747-023-00948-0
   Palumbo V., 2020, P 13 ACM INT C PERVA, DOI [10.1145/3389189.3393739, DOI 10.1145/3389189.3393739]
   Pu LH, 2019, GERONTOLOGIST, V59, pE37, DOI 10.1093/geront/gny046
   Ragno L, 2023, SENSORS-BASEL, V23, DOI 10.3390/s23156820
   REITAN R. M., 1958, PERCEPT MOT SKILLS, V8, P271
   Rossi S, 2020, ROBOTICS, V9, DOI 10.3390/robotics9020039
   Salichs E, 2018, LECT NOTES ARTIF INT, V10978, P344, DOI 10.1007/978-3-319-94580-4_35
   Salichs MA, 2020, INT J SOC ROBOT, V12, P1231, DOI 10.1007/s12369-020-00687-0
   Salichs MA, 2016, INT J SOC ROBOT, V8, P85, DOI 10.1007/s12369-015-0319-6
   Samuels P., 2015, Advice on reliability analysis with small samples, DOI DOI 10.13140/RG.2.1.1495.5364
   Sharkey A, 2011, IEEE ROBOT AUTOM MAG, V18, P32, DOI 10.1109/MRA.2010.940151
   Spoladore D., 2023, Collaborative Networks in Digitalization and Society 5.0, P510
   Thunberg S, 2022, PROCEEDINGS OF THE 10TH CONFERENCE ON HUMAN-AGENT INTERACTION, HAI 2022, P4, DOI 10.1145/3527188.3561924
   Thunberg S, 2021, ACMIEEE INT CONF HUM, P294, DOI 10.1145/3434074.3447179
   Wang RH, 2017, INT PSYCHOGERIATR, V29, P67, DOI 10.1017/S1041610216001435
   World Health Organization W., Aging and Health. URL: HYPERLINK.
NR 42
TC 0
Z9 0
U1 1
U2 1
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUL
PY 2024
VL 147
AR 105072
DI 10.1016/j.imavis.2024.105072
EA MAY 2024
PG 8
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA TQ5L0
UT WOS:001242737300001
DA 2024-08-05
ER

PT J
AU Sun, CW
   Zhang, Q
   Zhuang, CY
   Zhang, MQ
AF Sun, Chenwang
   Zhang, Qing
   Zhuang, Chenyu
   Zhang, Mingqian
TI BMFNet: Bifurcated multi-modal fusion network for RGB-D salient object
   detection
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE RGB-D salient object detection; Cross-modal fusion; Multi-modal
   integration; Multi-level aggregation
ID IMAGE
AB Although deep learning-based RGB-D salient object detection methods have achieved impressive results in the recent years, there are still some issues need to be addressed including multi-modal fusion and multi-level aggregation. In this paper, we propose a bifurcated multi-modal fusion network (BMFNet) to address these two issues cooperatively. First, we design a multi-modal feature interaction (MFI) module to fully capture the complementary information between the RGB and depth features by leveraging the channel attention and spatial attention. Second, unlike the widely used layer-by-layer progressive fusion, we adopt a bifurcated fusion strategy for all the multi-level unimodal and cross-modal features to effectively reduce the gaps between features at different levels. For the intra-group feature aggregation, a multi-modal feature fusion (MFF) module is designed to integrate the intra-group multi-modal features to produce a low-level/high-level saliency feature. For the inter-group aggregation, a multi-scale feature learning (MFL) module is introduced to exploit the contextual interactions between different scales to boost fusion performance. Experimental results on five public RGB-D datasets demonstrate the effectiveness and superiority of our proposed network. The code and prediction maps will be available at https://github.com/ZhangQing0329/BMFNet
C1 [Sun, Chenwang; Zhang, Qing; Zhuang, Chenyu] Shanghai Inst Technol, Sch Comp Sci & Informat Engn, Shanghai 201418, Peoples R China.
   [Zhang, Mingqian] Shanghai Inst Technol, Sch Mech Engn, Shanghai 201418, Peoples R China.
C3 Shanghai Institute of Technology; Shanghai Institute of Technology
RP Zhang, Q (corresponding author), Shanghai Inst Technol, Sch Comp Sci & Informat Engn, Shanghai 201418, Peoples R China.
EM zhangqing0329@gmail.com
FU Natural Science Foundation of Shanghai [21ZR1462600, 19ZR1455300]
FX This work is supported by Natural Science Foundation of Shanghai under
   Grant Nos. 21ZR1462600 and 19ZR1455300.
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Ao Luo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P346, DOI 10.1007/978-3-030-58610-2_21
   Chen CLZ, 2021, IEEE T IMAGE PROCESS, V30, P2350, DOI 10.1109/TIP.2021.3052069
   Chen H, 2018, PROC CVPR IEEE, P3051, DOI 10.1109/CVPR.2018.00322
   Chen Q, 2021, AAAI CONF ARTIF INTE, V35, P1063
   Chen TY, 2023, NEUROCOMPUTING, V522, P152, DOI 10.1016/j.neucom.2022.12.004
   Chen ZY, 2021, IEEE T IMAGE PROCESS, V30, P7012, DOI 10.1109/TIP.2020.3028289
   Chen ZY, 2020, AAAI CONF ARTIF INTE, V34, P10599
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Cheng Y, 2014, IEEE INT CON MULTI
   Cong RM, 2023, PROCEEDINGS OF THE 31ST ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2023, P406, DOI 10.1145/3581783.3611982
   Cong RM, 2016, IEEE SIGNAL PROC LET, V23, DOI 10.1109/LSP.2016.2557347
   Cong RM, 2022, IEEE T IMAGE PROCESS, V31, P6800, DOI 10.1109/TIP.2022.3216198
   Deng-Ping Fan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P275, DOI 10.1007/978-3-030-58610-2_17
   Desingh K, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.98
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698
   Fan DP, 2021, IEEE T NEUR NET LEAR, V32, P2075, DOI 10.1109/TNNLS.2020.2996406
   Fang CW, 2022, SCI CHINA INFORM SCI, V65, DOI 10.1007/s11432-021-3384-y
   Fu KR, 2020, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR42600.2020.00312
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   Gao W, 2022, IEEE T CIRC SYST VID, V32, P2091, DOI 10.1109/TCSVT.2021.3082939
   Gongyang Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P665, DOI 10.1007/978-3-030-58520-4_39
   Guo CL, 2010, IEEE T IMAGE PROCESS, V19, P185, DOI 10.1109/TIP.2009.2030969
   Hong S, 2015, PR MACH LEARN RES, V37, P597
   Hou QB, 2019, IEEE T PATTERN ANAL, V41, P815, DOI 10.1109/TPAMI.2018.2815688
   Jerripothula KR, 2016, IEEE T MULTIMEDIA, V18, P1896, DOI 10.1109/TMM.2016.2576283
   Ji W, 2022, IEEE T IMAGE PROCESS, V31, P2321, DOI 10.1109/TIP.2022.3154931
   Ji W, 2021, PROC CVPR IEEE, P9466, DOI 10.1109/CVPR46437.2021.00935
   Jiang QP, 2018, IEEE T MULTIMEDIA, V20, P2035, DOI 10.1109/TMM.2017.2763321
   Jin WD, 2021, IEEE T IMAGE PROCESS, V30, P3376, DOI 10.1109/TIP.2021.3060167
   Ju R, 2014, IEEE IMAGE PROC, P1115, DOI 10.1109/ICIP.2014.7025222
   Jun Wei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13022, DOI 10.1109/CVPR42600.2020.01304
   Lee M, 2022, LECT NOTES COMPUT SC, V13689, P630, DOI 10.1007/978-3-031-19818-2_36
   Li GY, 2021, IEEE T IMAGE PROCESS, V30, P3528, DOI 10.1109/TIP.2021.3062689
   Li H, 2023, PROC CVPR IEEE, P15485, DOI 10.1109/CVPR52729.2023.01486
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu JJ, 2023, IEEE T PATTERN ANAL, V45, P887, DOI 10.1109/TPAMI.2021.3140168
   Liu JJ, 2021, IEEE T IMAGE PROCESS, V30, P9030, DOI 10.1109/TIP.2021.3122093
   Liu N, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4702, DOI 10.1109/ICCV48922.2021.00468
   Liu NA, 2020, IEEE T IMAGE PROCESS, V29, P6438, DOI 10.1109/TIP.2020.2988568
   Liu N, 2022, IEEE T PATTERN ANAL, V44, P9026, DOI 10.1109/TPAMI.2021.3122139
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu ZY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4481, DOI 10.1145/3474085.3475601
   Liu ZY, 2022, IEEE T CIRC SYST VID, V32, P4486, DOI 10.1109/TCSVT.2021.3127149
   Ma MC, 2021, AAAI CONF ARTIF INTE, V35, P2311
   Miao Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P374, DOI 10.1007/978-3-030-58604-1_23
   Niu YZ, 2012, PROC CVPR IEEE, P454, DOI 10.1109/CVPR.2012.6247708
   Pang YW, 2023, IEEE T IMAGE PROCESS, V32, P892, DOI 10.1109/TIP.2023.3234702
   Peng HW, 2014, LECT NOTES COMPUT SC, V8691, P92, DOI 10.1007/978-3-319-10578-9_7
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Sun FM, 2024, IEEE T MULTIMEDIA, V26, P2249, DOI 10.1109/TMM.2023.3294003
   Sun P, 2021, PROC CVPR IEEE, P1407, DOI 10.1109/CVPR46437.2021.00146
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wei LS, 2023, INFORM SCIENCES, V626, P223, DOI 10.1016/j.ins.2023.01.032
   Wen HF, 2021, IEEE T IMAGE PROCESS, V30, P9179, DOI 10.1109/TIP.2021.3123548
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu YH, 2023, IEEE T PATTERN ANAL, V45, P12760, DOI 10.1109/TPAMI.2022.3202765
   Wu YH, 2022, IEEE T PATTERN ANAL, V44, P10261, DOI 10.1109/TPAMI.2021.3134684
   Xiaoqi Zhao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P646, DOI 10.1007/978-3-030-58542-6_39
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yang N, 2021, SIGNAL PROCESS-IMAGE, V94, DOI 10.1016/j.image.2021.116218
   Yang Y, 2022, IEEE T CIRC SYST VID, V32, P5346, DOI 10.1109/TCSVT.2022.3144852
   Yongri Piao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9057, DOI 10.1109/CVPR42600.2020.00908
   Youwei Pang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P235, DOI 10.1007/978-3-030-58595-2_15
   Youwei Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9410, DOI 10.1109/CVPR42600.2020.00943
   Zhang C, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2094, DOI 10.1145/3474085.3475364
   Zhang J., 2020, P IEEE CVF C COMP VI, P8582, DOI DOI 10.1109/CVPR42600.2020.00861
   Zhang J, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4318, DOI 10.1109/ICCV48922.2021.00430
   Zhang M., 2022, IEEE Trans. Multimed.
   Zhang W., 2021, P IEEE INT C MULT EX, P1
   Zhang WB, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P731, DOI 10.1145/3474085.3475240
   Zhang X., 2023, IEEE Trans. Circuits Syst. Video Technol., P1
   Zhang YT, 2016, IEEE T MULTIMEDIA, V18, P1604, DOI 10.1109/TMM.2016.2568138
   Zhao JX, 2019, PROC CVPR IEEE, P3922, DOI 10.1109/CVPR.2019.00405
   Zhao YF, 2021, IEEE T IMAGE PROCESS, V30, P7717, DOI 10.1109/TIP.2021.3108412
   Zhao ZR, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4967, DOI 10.1145/3474085.3475494
   Zhou H., 2020, P IEEE CVF C COMP VI, P9138, DOI 10.1109/CVPR42600.2020.00916
   Zhou JY, 2022, LECT NOTES COMPUT SC, V13689, P270, DOI 10.1007/978-3-031-19818-2_16
   Zhou T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4661, DOI 10.1109/ICCV48922.2021.00464
   Zhou WJ, 2022, IEEE T MULTIMEDIA, V24, P2192, DOI 10.1109/TMM.2021.3077767
   Zhuge MC, 2023, IEEE T PATTERN ANAL, V45, P3738, DOI 10.1109/TPAMI.2022.3179526
NR 82
TC 0
Z9 0
U1 10
U2 10
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUL
PY 2024
VL 147
AR 105048
DI 10.1016/j.imavis.2024.105048
EA MAY 2024
PG 15
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA TG6W8
UT WOS:001240162600001
DA 2024-08-05
ER

PT J
AU Yao, RH
   He, BB
   Zhang, YF
   Li, ZY
   Zhu, JY
   Lang, X
AF Yao, Ruihan
   He, Bingbing
   Zhang, Yufeng
   Li, Zhiyao
   Zhu, Jingying
   Lang, Xun
TI Optimal fusion of features from decomposed ultrasound RF data with
   adaptive weighted ensemble classifier to improve breast lesion
   classification
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Breast lesion classification; Features extraction; Feature selection;
   Radio-frequency ultrasonic signal; Adaptive weighted ensemble
ID TEXTURE FEATURES; SEGMENTATION; IDENTIFICATION; DIAGNOSIS; BENIGN;
   MUSCLE; CANCER; IMAGES; TUMORS
AB Early diagnosis plays a crucial role in successful treatment of breast tumors and reduced mortality. In this study, considering the complementary advantages between features and combined with the improvement of classifiers, fusion of optimal complexity and texture features from decomposed ultrasound radio frequency (RF) data is proposed to improve breast lesion classification performance. In this method, three complexity features and four texture features were extracted from the ring regions of interest for breast lesions in all decomposed RF subimages and their combinations. Selection techniques based on analysis of feature relevance, redundancy, and interaction (FRRI) were used to determine the optimal feature sets (FS-FRRI). Finally, three classifiers with the best performance (weighted k-nearest neighbor, bagged tree, Gaussian Naive Bayes (NB)) were selected based on FS-FRRI. The three classifiers were integrated using the bagging method, and each classifier was adaptively weighted according to the genetic algorithm during the integration to classify breast lesions. The proposed method was evaluated using the Open Access Series of Breast Ultrasonic Data, with 10-fold cross-validation. The experimental results demonstrated that optimal performance was obtained by the FS-FRRI-based adaptive weighted ensemble classifier, with an accuracy of 97%, a sensitivity of 99%, a specificity of 96%, and an area under the receiver operating curve value of 0.97. Fusion of optimal complexity and texture features from decomposed ultrasound RF data with an adaptive weighted ensemble classifier can help improve breast lesion classification performance, which has great potential to assist clinicians in accurate diagnosis of breast lesions.
C1 [Yao, Ruihan; He, Bingbing; Zhang, Yufeng; Zhu, Jingying; Lang, Xun] Yunnan Univ, Informat Sch, Dept Elect Engn, Kunming 650091, Yunnan, Peoples R China.
   [Li, Zhiyao] Kunming Med Univ, Affiliated Hosp 3, Kunming 650118, Yunnan, Peoples R China.
   [Li, Zhiyao] Kunming Med Univ, Yunnan Canc Hosp, Affiliated Hosp 3, Kunming, Peoples R China.
C3 Yunnan University; Kunming Medical University; Kunming Medical
   University
RP He, BB (corresponding author), Yunnan Univ, Informat Sch, Dept Elect Engn, Kunming 650091, Yunnan, Peoples R China.; Li, ZY (corresponding author), Kunming Med Univ, Yunnan Canc Hosp, Affiliated Hosp 3, Kunming, Peoples R China.
EM hebingbing@ynu.edu.cn; lizhiyao53@sina.com
FU National Natural Science Foundation of China [62261057, 62201495]; Key
   Project of Fundamental Research of Yunnan Province [202101AS070031];
   Open Project Program of Yunnan Key Lab- oratory of Intelligent Systems
   and Computing [ISC22Y09]
FX This work was supported by the National Natural Science Foundation of
   China (grant numbers 62261057 and 62201495) , the Key Project of
   Fundamental Research of Yunnan Province (grant number 202101AS070031) ,
   and the Open Project Program of Yunnan Key Lab- oratory of Intelligent
   Systems and Computing (grant number ISC22Y09) .
CR Alam SK, 2011, ULTRASONIC IMAGING, V33, P17, DOI 10.1177/016173461103300102
   Breiman Leo, 1996, Machine Learn, V24, DOI [10.1023/A:1018054314350,123-14, DOI 10.1023/A:1018054314350,123-14]
   Byra M, 2022, ULTRASONICS, V121, DOI 10.1016/j.ultras.2021.106682
   Byra M, 2018, BIOCYBERN BIOMED ENG, V38, P684, DOI 10.1016/j.bbe.2018.05.003
   Chen SN, 2019, IEEE ACCESS, V7, P61046, DOI 10.1109/ACCESS.2019.2915610
   Cloutier G, 2021, INSIGHTS IMAGING, V12, DOI 10.1186/s13244-021-01071-w
   Dong LJ, 2016, ROCK MECH ROCK ENG, V49, P183, DOI 10.1007/s00603-015-0733-y
   Donohue KD, 2001, ULTRASOUND MED BIOL, V27, P1505, DOI 10.1016/S0301-5629(01)00468-9
   Faust O, 2018, BIOCYBERN BIOMED ENG, V38, P275, DOI 10.1016/j.bbe.2018.01.001
   Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010
   Georgiou G, 2001, IEEE T ULTRASON FERR, V48, P364, DOI 10.1109/58.911719
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   Hsu SM, 2019, INT J COMPUT ASS RAD, V14, P623, DOI 10.1007/s11548-018-01908-8
   Huang SM, 2013, IEEE SIGNAL PROC LET, V20, P91, DOI 10.1109/LSP.2012.2230257
   HUGHES MS, 1992, ULTRASON, P1205, DOI 10.1109/ULTSYM.1992.275884
   Jalalian A, 2013, CLIN IMAG, V37, P420, DOI 10.1016/j.clinimag.2012.09.024
   Jarosik P, 2020, BIOCYBERN BIOMED ENG, V40, P977, DOI 10.1016/j.bbe.2020.04.002
   Kim S, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12104942
   Klimonda Z, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-44376-z
   Kozegar E, 2018, IEEE T MED IMAGING, V37, P918, DOI 10.1109/TMI.2017.2787685
   Kukker A, 2021, COMPUT ELECTR ENG, V92, DOI 10.1016/j.compeleceng.2021.107154
   Lang X, 2018, IEEE ACCESS, V6, P65521, DOI 10.1109/ACCESS.2018.2877150
   Liao WX, 2020, IEEE J BIOMED HEALTH, V24, P984, DOI 10.1109/JBHI.2019.2960821
   Liao ZR, 2021, ULTRASONICS, V114, DOI 10.1016/j.ultras.2021.106419
   Mamou J., 2013, Quantitative Ultrasound in Soft Tissues, DOI DOI 10.1007/978-94-007-6952-6
   Muhtadi S., 2023, Texture quantified from ultrasound Nakagami parametric images is diagnostically relevant for breast tumor characterization, V10, P1, DOI [10.1117/1.JMI.10.S2.S22410, DOI 10.1117/1.JMI.10.S2.S22410]
   Muhtadi S, 2022, COMPUT MATH METHOD M, V2022, DOI 10.1155/2022/1633858
   Mustra M, 2016, MED BIOL ENG COMPUT, V54, P1003, DOI 10.1007/s11517-015-1411-7
   Nasief HG, 2019, ULTRASOUND MED BIOL, V45, P1603, DOI 10.1016/j.ultrasmedbio.2019.02.025
   Nieniewski M, 2020, IMAGE ANAL STEREOL, V39, P129, DOI 10.5566/ias.2113
   Nizam NI, 2017, IEEE T ULTRASON FERR, V64, P1487, DOI 10.1109/TUFFC.2017.2735629
   Ouyang YL, 2019, DIAGNOSTICS, V9, DOI 10.3390/diagnostics9040182
   Pang T, 2021, COMPUT METH PROG BIO, V203, DOI 10.1016/j.cmpb.2021.106018
   Paris MT, 2021, ULTRASOUND MED BIOL, V47, P880, DOI 10.1016/j.ultrasmedbio.2020.12.012
   Piotrzkowska-Wróblewska H, 2017, MED PHYS, V44, P6105, DOI 10.1002/mp.12538
   Pizzo A., 2018, Adv. Eng. Comput., P131, DOI [10.25073/jaec.201822.193, DOI 10.25073/JAEC.201822.193]
   Raghavendra U, 2017, ULTRASONICS, V77, P110, DOI 10.1016/j.ultras.2017.02.003
   SHANNON CE, 1948, BELL SYST TECH J, V27, P379, DOI 10.1002/j.1538-7305.1948.tb01338.x
   Silva LEV, 2018, SIGNAL PROCESS, V147, P224, DOI 10.1016/j.sigpro.2018.02.004
   Steifer T, 2019, BIOMED SIGNAL PROCES, V51, P235, DOI 10.1016/j.bspc.2019.02.020
   Tsui PH, 2017, SCI REP-UK, V7, DOI 10.1038/srep41004
   Tsui PH, 2010, IEEE T MED IMAGING, V29, P513, DOI 10.1109/TMI.2009.2037147
   Wang LX, 2021, EXPERT SYST APPL, V183, DOI 10.1016/j.eswa.2021.115365
   Wei MW, 2020, COMPUT MATH METHOD M, V2020, DOI 10.1155/2020/5894010
   Yang MC, 2013, IEEE T MED IMAGING, V32, P2262, DOI 10.1109/TMI.2013.2279938
   Yao RH, 2022, BIOMED SIGNAL PROCES, V75, DOI 10.1016/j.bspc.2022.103559
   Yigit H, 2015, J EXP THEOR ARTIF IN, V27, P189, DOI 10.1080/0952813X.2014.924585
   Yu L, 2004, J MACH LEARN RES, V5, P1205
NR 48
TC 0
Z9 0
U1 1
U2 1
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUN
PY 2024
VL 146
AR 105045
DI 10.1016/j.imavis.2024.105045
EA MAY 2024
PG 11
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA SU0N5
UT WOS:001236842500001
DA 2024-08-05
ER

PT J
AU Xie, JW
   Liu, Z
   Li, GY
   Song, YJ
AF Xie, Jiawei
   Liu, Zhi
   Li, Gongyang
   Song, Yingjie
TI Audio-visual saliency prediction with multisensory perception and
   integration
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Audio-visual saliency prediction; Audio-visual fusion; Image saliency
   prediction; Self-supervised learning
ID VISUAL-ATTENTION; OBJECT DETECTION; DRIVEN; MODEL
AB Audio-visual saliency prediction (AVSP) is a task that aims to model human attention patterns in the perception of auditory and visual scenes. Given the challenges associated with perceiving and combining multi-modal saliency features from videos, this paper presents a multi-sensory framework for AVSP. This framework is designed to extract audio, motion and image saliency features and integrate them effectively, which can then serve as a general architecture for the AVSP task. To obtain multi-sensory information, we develop a three-stream encoder that extracts audio, motion and image saliency features. In particular, we utilize a pre-trained encoder with knowledge related to image saliency to extract saliency features for each frame. The image saliency features are then incorporated with motion features using a spatial attention module. For motion features, 3D convolutional neural networks (CNNs) like S3D are commonly used in AVSP models. However, these networks are unable to effectively capture the global motion relationship in videos. To tackle this problem, we incorporate Transformerand MLP-based motion encoders into the AVSP models. To learn joint audio-visual representations, an audiovisual fusion block is exploited to enhance the correlation between audio and visual motion features under the supervision of a cosine similarity loss in a self-supervised manner. Finally, a multi-stage decoder integrates audio, motion and image saliency features to generate the final saliency map. We evaluate our methods on six audio-visual eye-tracking datasets. Experimental results demonstrate that our method achieves compelling performance compared to the state-of-the-art methods. The source code is available at https://github. com/oraclefina/MSPI.
C1 [Xie, Jiawei; Liu, Zhi; Li, Gongyang; Song, Yingjie] Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
   [Liu, Zhi; Li, Gongyang] Shanghai Univ, Wenzhou Inst, Wenzhou 325000, Peoples R China.
C3 Shanghai University; Shanghai University
RP Liu, Z (corresponding author), Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.; Liu, Z (corresponding author), Shanghai Univ, Wenzhou Inst, Wenzhou 325000, Peoples R China.
EM liuzhisjtu@163.com
RI LIU, Zhi/D-4518-2012; Li, Gongyang/IXD-9078-2023
OI LIU, Zhi/0000-0002-8428-1131; 
FU National Natural Science Foundation of China [62171269]; China
   Postdoctoral Science Foundation [2022M722037]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62171269, and in part by the China
   Postdoctoral Science Foundation under Grant 2022M722037.
CR Afouras T., 2020, Lecture Notes in Computer Science, P208
   Afouras T, 2022, IEEE T PATTERN ANAL, V44, P8717, DOI 10.1109/TPAMI.2018.2889052
   Alwassel H., 2020, NEURIPS, V33, P9758
   Arandjelovic R, 2017, IEEE I CONF COMP VIS, P609, DOI 10.1109/ICCV.2017.73
   Aydemir B, 2023, PROC CVPR IEEE, P6461, DOI 10.1109/CVPR52729.2023.00625
   Aytar Y, 2016, ADV NEUR IN, V29
   Bruce N., 2005, Advances in Neural Information Processing Systems, P18
   Cerf M, 2009, J VISION, V9, DOI 10.1167/9.12.10
   Chang QY, 2021, Arxiv, DOI arXiv:2105.04213
   Chao F.-Y., 2020, IEEE INT CONF MULTI, P1, DOI [DOI 10.1109/icmew46912.2020.9105956, 10.1109/ICMEW46912.2020.9105956]
   Chen CLZ, 2023, IEEE T CIRC SYST VID, V33, P457, DOI 10.1109/TCSVT.2022.3203421
   Chen HL, 2020, INT CONF ACOUST SPEE, P721, DOI [10.1109/ICASSP40776.2020.9053174, 10.1109/icassp40776.2020.9053174]
   Chen J, 2021, PATTERN RECOGN, V109, DOI 10.1016/j.patcog.2020.107615
   Chen T, 2020, PR MACH LEARN RES, V119
   Chen XL, 2021, PROC CVPR IEEE, P15745, DOI 10.1109/CVPR46437.2021.01549
   Cheng Y, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3884, DOI 10.1145/3394171.3413869
   Cornia M, 2018, IEEE T IMAGE PROCESS, V27, P5142, DOI 10.1109/TIP.2018.2851672
   Cornia M, 2016, INT C PATT RECOG, P3488, DOI 10.1109/ICPR.2016.7900174
   Coutrot A, 2016, SPRINGER SER COG NEU, V10, P291, DOI 10.1007/978-1-4939-3435-5_16
   Coutrot A, 2014, IEEE IMAGE PROC, P1100, DOI 10.1109/ICIP.2014.7025219
   Coutrot A, 2014, J VISION, V14, DOI 10.1167/14.8.5
   Ding GQ, 2022, IMAGE VISION COMPUT, V120, DOI 10.1016/j.imavis.2022.104395
   Duan HZ, 2023, SIGNAL PROCESS-IMAGE, V115, DOI 10.1016/j.image.2023.116968
   Erdem E, 2013, J VISION, V13, DOI 10.1167/13.4.11
   Feichtenhofer C, 2020, PROC CVPR IEEE, P200, DOI 10.1109/CVPR42600.2020.00028
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Girdhar R, 2023, PROC CVPR IEEE, P15180, DOI 10.1109/CVPR52729.2023.01457
   Gong Y., 2022, 11 INT C LEARNING RE
   Grill J.B., 2020, P 34 INT C NEUR INF, V33, P21271, DOI [10.48550/arXiv.2006.07733, DOI 10.48550/ARXIV.2006.07733]
   Gygli M, 2014, LECT NOTES COMPUT SC, V8695, P505, DOI 10.1007/978-3-319-10584-0_33
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hershey S, 2017, INT CONF ACOUST SPEE, P131, DOI 10.1109/ICASSP.2017.7952132
   Huang P. -Y., 2022, Proceedings of NeurIPS, V35, P28708
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jain S, 2021, IEEE INT C INT ROBOT, P3520, DOI 10.1109/IROS51168.2021.9635989
   Jia S, 2020, IMAGE VISION COMPUT, V95, DOI 10.1016/j.imavis.2020.103887
   Jiang L, 2019, Arxiv, DOI arXiv:1709.06316
   Jiang L, 2018, LECT NOTES COMPUT SC, V11218, P625, DOI 10.1007/978-3-030-01264-9_37
   Jiang M, 2015, PROC CVPR IEEE, P1072, DOI 10.1109/CVPR.2015.7298710
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   Koutras P, 2015, SIGNAL PROCESS-IMAGE, V38, P15, DOI 10.1016/j.image.2015.08.004
   Kümmerer M, 2017, IEEE I CONF COMP VIS, P4799, DOI 10.1109/ICCV.2017.513
   Kummerer M, 2015, Arxiv, DOI [arXiv:1411.1045, 10.48550/arXiv.1411.1045]
   Kummerer T., 2017, J. Vis., V17, P1147, DOI [DOI 10.1167/17.10.1147, 10.1167/17.10.1147]
   Lai QX, 2020, IEEE T IMAGE PROCESS, V29, P1113, DOI 10.1109/TIP.2019.2936112
   Li GY, 2024, IEEE T CIRCUITS-II, V71, P2464, DOI 10.1109/TCSII.2023.3333436
   Li GY, 2023, IEEE T IMAGE PROCESS, V32, P5257, DOI 10.1109/TIP.2023.3314285
   Li GY, 2023, IEEE T CYBERNETICS, V53, P526, DOI 10.1109/TCYB.2022.3162945
   Li GY, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3131221
   Li KC, 2023, IEEE T PATTERN ANAL, V45, P12581, DOI 10.1109/TPAMI.2023.3282631
   Li YH, 2022, PROC CVPR IEEE, P4794, DOI 10.1109/CVPR52688.2022.00476
   Linardos A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12899, DOI 10.1109/ICCV48922.2021.01268
   Liu N, 2015, PROC CVPR IEEE, P362, DOI 10.1109/CVPR.2015.7298633
   Liu Z, 2022, PROC CVPR IEEE, P3192, DOI 10.1109/CVPR52688.2022.00320
   Liu Z, 2022, PROC CVPR IEEE, P11966, DOI 10.1109/CVPR52688.2022.01167
   Loshchilov I., 2018, INT C LEARN REPR
   Ma C, 2022, IEEE T CIRC SYST VID, V32, P6850, DOI 10.1109/TCSVT.2022.3172971
   Mathe S, 2015, IEEE T PATTERN ANAL, V37, P1408, DOI 10.1109/TPAMI.2014.2366154
   Min K, 2019, IEEE I CONF COMP VIS, P2394, DOI 10.1109/ICCV.2019.00248
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P3805, DOI 10.1109/TIP.2020.2966082
   Min XK, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/2996463
   Mital PK, 2011, COGN COMPUT, V3, P5, DOI 10.1007/s12559-010-9074-z
   Niizumi D, 2023, IEEE-ACM T AUDIO SPE, V31, P137, DOI 10.1109/TASLP.2022.3221007
   Ning HL, 2022, KNOWL-BASED SYST, V256, DOI 10.1016/j.knosys.2022.109675
   Owens A, 2018, LECT NOTES COMPUT SC, V11210, P639, DOI 10.1007/978-3-030-01231-1_39
   Pan JT, 2018, Arxiv, DOI arXiv:1701.01081
   Pan JT, 2016, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2016.71
   PERROTT DR, 1990, PERCEPT PSYCHOPHYS, V48, P214, DOI 10.3758/BF03211521
   Prashnani E, 2021, BRIT MACHINE VISION
   Tavakoli HR, 2020, Arxiv, DOI arXiv:1905.10693
   Recasens A, 2023, Arxiv, DOI arXiv:2301.09595
   Recasens A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1235, DOI 10.1109/ICCV48922.2021.00129
   Reddy N, 2020, IEEE INT C INT ROBOT, P10241, DOI 10.1109/IROS45743.2020.9341574
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Sarkar P., 2023, AAAI, V37, P9723
   Shvetsova N, 2022, PROC CVPR IEEE, P19988, DOI 10.1109/CVPR52688.2022.01939
   Song GH, 2013, J EYE MOVEMENT RES, V6
   Song YJ, 2023, IEEE T MULTIMEDIA, V25, P9263, DOI 10.1109/TMM.2023.3249481
   Souza LS, 2020, PATTERN RECOGN, V97, DOI 10.1016/j.patcog.2019.107028
   Tatler BW, 2011, J VISION, V11, DOI 10.1167/11.5.5
   Tenenbaum JB, 2000, NEURAL COMPUT, V12, P1247, DOI 10.1162/089976600300015349
   Torralba A, 2006, PSYCHOL REV, V113, P766, DOI 10.1037/0033-295X.113.4.766
   Tsiami A, 2020, PROC CVPR IEEE, P4765, DOI 10.1109/CVPR42600.2020.00482
   Tsiami A, 2019, SIGNAL PROCESS-IMAGE, V76, P186, DOI 10.1016/j.image.2019.05.001
   Vaswani A, 2017, ADV NEUR IN, V30
   Vig E, 2014, PROC CVPR IEEE, P2798, DOI 10.1109/CVPR.2014.358
   Vroomen J, 2000, J EXP PSYCHOL HUMAN, V26, P1583, DOI 10.1037/0096-1523.26.5.1583
   Wang GT, 2021, PROC CVPR IEEE, P15114, DOI 10.1109/CVPR46437.2021.01487
   Wang WG, 2020, IEEE T PATTERN ANAL, V42, P1913, DOI 10.1109/TPAMI.2019.2905607
   Wang WG, 2018, PROC CVPR IEEE, P4894, DOI 10.1109/CVPR.2018.00514
   Wang WG, 2018, PROC CVPR IEEE, P1711, DOI 10.1109/CVPR.2018.00184
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P2368, DOI 10.1109/TIP.2017.2787612
   Wang YH, 2021, IMAGE VISION COMPUT, V112, DOI 10.1016/j.imavis.2021.104216
   Wang ZQ, 2023, IEEE T MULTIMEDIA, V25, P1161, DOI 10.1109/TMM.2021.3139743
   Wang ZQ, 2021, IMAGE VISION COMPUT, V109, DOI 10.1016/j.imavis.2021.104149
   Wu XY, 2020, AAAI CONF ARTIF INTE, V34, P12410
   Xie JW, 2024, KNOWL-BASED SYST, V284, DOI 10.1016/j.knosys.2023.111279
   Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19
   Xiong JW, 2023, PROC CVPR IEEE, P6441, DOI 10.1109/CVPR52729.2023.00623
   Yang S., 2021, ADV NEUR IN, V34
   Yang S, 2020, IEEE T MULTIMEDIA, V22, P2163, DOI 10.1109/TMM.2019.2947352
   Youwei Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9410, DOI 10.1109/CVPR42600.2020.00943
   Zhang DJ, 2022, LECT NOTES COMPUT SC, V13695, P230, DOI 10.1007/978-3-031-19833-5_14
   Zhang YZ, 2023, IMAGE VISION COMPUT, V136, DOI 10.1016/j.imavis.2023.104744
   Zhou JX, 2022, LECT NOTES COMPUT SC, V13697, P386, DOI 10.1007/978-3-031-19836-6_22
   Zhou XF, 2023, IEEE T CIRC SYST VID, V33, P7696, DOI 10.1109/TCSVT.2023.3278410
   Zhu DD, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3576857
NR 108
TC 0
Z9 0
U1 7
U2 7
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD MAR
PY 2024
VL 143
AR 104955
DI 10.1016/j.imavis.2024.104955
EA MAR 2024
PG 14
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA NL3L5
UT WOS:001200569800001
DA 2024-08-05
ER

PT J
AU Yu, M
   Xu, SY
   Sun, H
   Zheng, YL
   Yang, W
AF Yu, Mei
   Xu, Shouyi
   Sun, Hang
   Zheng, Yuelin
   Yang, Wen
TI Hierarchical slice interaction and multi-layer cooperative decoding
   networks for remote sensing image dehazing
SO IMAGE AND VISION COMPUTING
LA English
DT Review
DE Remote sensing image dehazing; Hierarchical slice interaction;
   Multi-layer cooperative decoding; U-shaped neural network
ID REMOVAL; HAZE
AB Recently, U-shaped neural networks have gained widespread application in remote sensing image dehazing and achieved promising performance. However, most of the existing U-shaped dehazing networks neglect the global and local information interaction across layers during the encoding phase, which leads to incomplete utilization of the extracted features for image restoration. Moreover, in the process of image reconstruction, utilizing only the information from the terminal layers of the decoding phase for haze-free image restoration leads to a dilution of semantic information, resulting in color and texture deviations in the dehazed image. To address these issues, We propose a Hierarchical Slice Interaction and Multi-layer Cooperative Decoding Networks for Remote Sensing Image dehazing (HSMD-Net). Specifically, a hierarchical slice information interaction module (HSIIM) is proposed to introduce Intra-layer feature autocorrelation and Inter-layer feature cross-correlation to facilitate global and local information interaction across layers, thereby enhancing the encoding features representation capability and improving the network dehazing performance. Furthermore, a multi-layer cooperative decoding reconstruction module (MCDRM) is proposed to fully utilize feature information in each decoding layer, mitigate semantic information dilution, and improve the network capability to restore image colors and textures. Experimental results demonstrate that our HSMD-Net outperforms several state-of-the-art methods in dehazing on two publicly available datasets. The source code is available at https://github.com/xushouyi1/HSMD-Net.
C1 [Yu, Mei; Xu, Shouyi; Sun, Hang; Zheng, Yuelin; Yang, Wen] China Three Gorges Univ, Hubei Key Lab Intelligent Vis Based Monitoring Hyd, Yichang 443002, Peoples R China.
   [Yu, Mei; Xu, Shouyi; Sun, Hang; Zheng, Yuelin; Yang, Wen] China Three Gorges Univ, Coll Comp & Informat Technol, Yichang 443002, Peoples R China.
C3 China Three Gorges University; China Three Gorges University
RP Sun, H (corresponding author), China Three Gorges Univ, Hubei Key Lab Intelligent Vis Based Monitoring Hyd, Yichang 443002, Peoples R China.; Sun, H (corresponding author), China Three Gorges Univ, Coll Comp & Informat Technol, Yichang 443002, Peoples R China.
EM sunhang@ctgu.edu.cn
FU Natural Science Foundation of Hubei Province of China [2021CFB004]
FX The authors declare that they have no known competing financial
   interests or personal relationships that could have appeared to
   influence the work reported in this paper. This work was partially
   supported by the Natural Science Foundation of Hubei Province of China
   under Grant 2021CFB004.
CR Bai HR, 2022, IEEE T IMAGE PROCESS, V31, P1217, DOI 10.1109/TIP.2022.3140609
   Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Bie YX, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2022.3177257
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chen X, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2022.3167476
   Chen X, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2021.3072917
   Chi KC, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3285228
   Feng X, 2024, IEEE T CIRC SYST VID, V34, P168, DOI 10.1109/TCSVT.2023.3286405
   Gao T, 2020, IEEE J-STARS, V13, P2610, DOI 10.1109/JSTARS.2020.2998517
   Gu ZQ, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11243008
   He J, 2022, INT J APPL EARTH OBS, V109, DOI 10.1016/j.jag.2022.102773
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Hu XG, 2023, IMAGE VISION COMPUT, V139, DOI 10.1016/j.imavis.2023.104823
   Huang BH, 2020, IEEE WINT CONF APPL, P1795, DOI [10.1109/wacv45572.2020.9093471, 10.1109/WACV45572.2020.9093471]
   Huang JJ, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3294436
   Huang YF, 2023, IEEE GEOSCI REMOTE S, V20, DOI 10.1109/LGRS.2023.3309655
   Huang YF, 2021, IEEE IMAGE PROC, P3852, DOI 10.1109/ICIP42928.2021.9506603
   Ienco D, 2024, NEUROCOMPUTING, V567, DOI 10.1016/j.neucom.2023.127031
   Jiang B, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3261545
   Kulkarni A, 2023, IEEE WINT CONF APPL, P6294, DOI 10.1109/WACV56688.2023.00624
   Li BY, 2021, INT J COMPUT VISION, V129, P1754, DOI 10.1007/s11263-021-01431-5
   Li JY, 2019, IEEE GEOSCI REMOTE S, V16, P472, DOI 10.1109/LGRS.2018.2874084
   Li YF, 2021, IEEE GEOSCI REMOTE S, V18, P1751, DOI 10.1109/LGRS.2020.3006533
   Li YA, 2019, IEEE I CONF COMP VIS, P3275, DOI 10.1109/ICCV.2019.00337
   Li ZC, 2022, IEEE T PATTERN ANAL, V44, P9904, DOI 10.1109/TPAMI.2021.3132068
   Lian Z, 2021, IEEE T SYST MAN CY-S, V51, P6477, DOI [10.1109/TSMC.2019.2961143, 10.1109/LGRS.2020.3023805]
   Lin CY, 2023, IEEE T MULTIMEDIA, V25, P3089, DOI 10.1109/TMM.2022.3155937
   Ma XF, 2023, INT J APPL EARTH OBS, V119, DOI 10.1016/j.jag.2023.103317
   Ren WQ, 2020, INT J COMPUT VISION, V128, P240, DOI 10.1007/s11263-019-01235-8
   Shah RA, 2023, IMAGE VISION COMPUT, V139, DOI 10.1016/j.imavis.2023.104817
   Song TY, 2023, IEEE GEOSCI REMOTE S, V20, DOI 10.1109/LGRS.2023.3319832
   Song YD, 2023, IEEE T IMAGE PROCESS, V32, P1927, DOI 10.1109/TIP.2023.3256763
   Sun H, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3321307
   Tu ZZ, 2022, PROC CVPR IEEE, P5759, DOI 10.1109/CVPR52688.2022.00568
   Wan J, 2023, IEEE T IMAGE PROCESS, V32, P1966, DOI 10.1109/TIP.2023.3261749
   Wan J, 2023, IEEE T CYBERNETICS, V53, P3546, DOI 10.1109/TCYB.2021.3131569
   Wan J, 2022, IEEE T NEUR NET LEAR, V33, P2181, DOI 10.1109/TNNLS.2020.3044078
   Wang S, 2021, IEEE IMAGE PROC, P3822, DOI 10.1109/ICIP42928.2021.9506604
   Wen YB, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3325927
   Wu HY, 2021, PROC CVPR IEEE, P10546, DOI 10.1109/CVPR46437.2021.01041
   Xu M, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3321294
   Yi QS, 2022, IEEE T MULTIMEDIA, V24, P3114, DOI 10.1109/TMM.2021.3093724
   Yuan QQ, 2020, J HYDROL, V580, DOI 10.1016/j.jhydrol.2019.124351
   Zhang F, 2016, NEUROCOMPUTING, V187, P75, DOI 10.1016/j.neucom.2015.07.132
   Zhang LB, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3207832
   Zhang XQ, 2022, IEEE T CIRC SYST VID, V32, P510, DOI 10.1109/TCSVT.2021.3067062
   Zhao D, 2021, IEEE T CIRC SYST VID, V31, P3037, DOI 10.1109/TCSVT.2020.3036992
   Zheng LR, 2023, IEEE T MULTIMEDIA, V25, P6794, DOI 10.1109/TMM.2022.3214780
   Zheng Ruohui, 2023, IEEE Geosci. Remote Sens. Lett., V20, P1
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 50
TC 0
Z9 0
U1 1
U2 1
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD AUG
PY 2024
VL 148
AR 105129
DI 10.1016/j.imavis.2024.105129
EA JUN 2024
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA XA7A1
UT WOS:001259013400001
DA 2024-08-05
ER

PT J
AU Han, XF
   Liu, Z
   Nan, H
   Zhao, K
   Zhao, DJ
   Jin, XD
AF Han, Xuefei
   Liu, Zheng
   Nan, Hai
   Zhao, Kai
   Zhao, Dongjie
   Jin, Xiaodan
TI PW-NeRF: Progressive wavelet-mask guided neural radiance fields view
   synthesis
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Novel view synthesis; Neural radiance fields; Wavelet decomposition;
   Coarse-to-fine
ID SCENES
AB Neural Radiance Fields (NeRF) can achieve state -of -the -art new view results when given a sufficient number of training views. However, NeRF's rendering is based on minimizing photometric consistency loss, and during the optimization process, it can suffer from overfitting due to factors such as lighting and texture, resulting in poor geometric and color reconstruction. The less data there is in the overlapping areas of the images, the greater the impact on the results, such as insufficient data caused by occlusion relationships. In this paper, we observed through experiments that introducing low-frequency images into NeRF during training can quickly obtain approximate geometric structures, which can guide NeRF to achieve more stable view synthesis results. Therefore, we propose a progressive wavelet mask to assist in the training of neural radiance fields. By first constructing a gaussian pyramid for the training images and then applying wavelet decomposition to them, we can obtain a series of low-frequency region masks to guide the neural radiance field to focus on learning lowfrequency pixel regions and gradually introduce high-frequency lighting and texture changes. Our experiments on the LLFF and Blender datasets show that using a progressive wavelet mask in NeRF training can achieve more realistic generation effects with almost no additional computational overhead. At the same time, we also tested our method in sparse scenes, where it still performs well in avoiding overfitting.
C1 [Han, Xuefei; Liu, Zheng; Nan, Hai; Zhao, Kai; Zhao, Dongjie; Jin, Xiaodan] Chongqing Univ Technol, Dept Comp Sci & Engn, Chongqing 400054, Peoples R China.
C3 Chongqing University of Technology
RP Nan, H (corresponding author), Chongqing Univ Technol, Dept Comp Sci & Engn, Chongqing 400054, Peoples R China.
EM lebenito@stu.cqut.edu.cn; liuzheng@cqut.edu.cn; stillwater@cqut.edu.cn;
   zhaokk@stu.cqut.edu.cn; zhaodongjie1223@163.com;
   jxd19960925@stu.cqut.edu.cn
CR Barron J.T., 2023, ICCV
   Barron JT, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5835, DOI 10.1109/ICCV48922.2021.00580
   CARROLL JD, 1970, PSYCHOMETRIKA, V35, P283, DOI 10.1007/BF02310791
   Chan ER, 2022, PROC CVPR IEEE, P16102, DOI 10.1109/CVPR52688.2022.01565
   Chen AP, 2022, LECT NOTES COMPUT SC, V13692, P333, DOI 10.1007/978-3-031-19824-3_20
   Chen AP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14104, DOI 10.1109/ICCV48922.2021.01386
   Chen Z, 2023, IEEE T PATTERN ANAL, V45, P15694, DOI 10.1109/TPAMI.2023.3305295
   Deng KL, 2022, PROC CVPR IEEE, P12872, DOI 10.1109/CVPR52688.2022.01254
   Fridovich-Keil S, 2023, PROC CVPR IEEE, P12479, DOI 10.1109/CVPR52729.2023.01201
   Fridovich-Keil S, 2022, PROC CVPR IEEE, P5491, DOI 10.1109/CVPR52688.2022.00542
   Garbin SJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14326, DOI 10.1109/ICCV48922.2021.01408
   Guangcong Z., 2023, Technical Report
   Huang X, 2023, PROC CVPR IEEE, P97, DOI 10.1109/CVPR52729.2023.00018
   Huang X, 2022, PROC CVPR IEEE, P18377, DOI 10.1109/CVPR52688.2022.01785
   Jain A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5865, DOI 10.1109/ICCV48922.2021.00583
   Kerbl B, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592433
   Li ZQ, 2021, PROC CVPR IEEE, P6494, DOI 10.1109/CVPR46437.2021.00643
   Liu L., 2020, Advances in Neural Information Processing Systems, V33, P15651
   Ma HC, 2022, IEEE T PATTERN ANAL, V44, P1247, DOI 10.1109/TPAMI.2020.3026003
   Mildenhall B, 2022, PROC CVPR IEEE, P16169, DOI 10.1109/CVPR52688.2022.01571
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Mildenhall B, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322980
   Müller T, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530127
   Niemeyer M, 2022, PROC CVPR IEEE, P5470, DOI 10.1109/CVPR52688.2022.00540
   Reiser C, 2023, ACM T GRAPHIC, V42, DOI 10.1145/3592426
   Rho D, 2023, PROC CVPR IEEE, P20680, DOI 10.1109/CVPR52729.2023.01981
   Roessle B, 2022, PROC CVPR IEEE, P12882, DOI 10.1109/CVPR52688.2022.01255
   Schönberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445
   Sucar E, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6209, DOI 10.1109/ICCV48922.2021.00617
   Tancik M., 2020, ADV NEURAL INFORM PR, V33, P7537, DOI DOI 10.48550/ARXIV.2006.10739
   Tian CW, 2023, PATTERN RECOGN, V134, DOI 10.1016/j.patcog.2022.109050
   Verbin D, 2022, PROC CVPR IEEE, P5481, DOI 10.1109/CVPR52688.2022.00541
   Wang C, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P6445, DOI 10.1145/3503161.3547808
   Wang Peng, 2021, NeurIPS
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yang JW, 2023, PROC CVPR IEEE, P8254, DOI 10.1109/CVPR52729.2023.00798
   Yu A, 2021, PROC CVPR IEEE, P4576, DOI 10.1109/CVPR46437.2021.00455
   Yu YC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14094, DOI 10.1109/ICCV48922.2021.01385
   Yuan YJ, 2022, PROC CVPR IEEE, P18332, DOI 10.1109/CVPR52688.2022.01781
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhuang Y., 2023, SIGGRAPH AS 2023 C, P1
NR 41
TC 0
Z9 0
U1 4
U2 4
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUL
PY 2024
VL 147
AR 105073
DI 10.1016/j.imavis.2024.105073
EA MAY 2024
PG 10
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA UE1I4
UT WOS:001246289600001
DA 2024-08-05
ER

PT J
AU Latke, V
   Narawade, V
AF Latke, Vaishali
   Narawade, Vaibhav
TI Detection of dental periapical lesions using retinex based image
   enhancement and lightweight deep learning model
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Periapical lesions; Deep learning; Segmentation; Retinex; Image
   enhancement
ID BEAM COMPUTED-TOMOGRAPHY; RADIOGRAPHY; ACCURACY
AB Dental periapical lesions, commonly associated with inflammation around the tooth apex, pose a significant challenge in early diagnosis and treatment. This study introduces a novel approach for the detection of dental periapical lesions through the integration of Retinex-based image enhancement techniques and a lightweight deep learning model. The Retinex algorithm is employed to enhance the radiographic images, addressing issues related to inconsistent illumination and contrast. Subsequently, a tailored lightweight deep learning model is designed to efficiently extract relevant features from the enhanced images. Present methodology leverages a dataset of dental radiographs to train and evaluate the deep learning model, incorporating a diverse range of periapical lesion cases. The model is optimized for computational efficiency while maintaining high accuracy, making it suitable for deployment in resource-constrained environment. To enhance precision in lesion detection, the U-Net segmentation technique has been incorporated, providing a sophisticated approach to delineate and analyze specific areas of interest within the radiographic images. This addition further refines our diagnostic framework, contributing to the robustness of lesion identification. Experimental results demonstrate the effectiveness of the Retinex-based image enhancement in improving the visibility of periapical lesions. The lightweight deep learning model exhibits promising performance in accurately detecting and classifying dental periapical lesions, showcasing its potential for early and efficient diagnosis. The results obtained from the present model are compared with those from Convolutional Neural Network (CNN) as well as with diagnosis of expert practitioners and the model is observed to perform very well. The study contributes to the advancement of computer-aided diagnostic tools in dentistry, offering a scalable and accessible solution for the identification of dental periapical lesions through the fusion of image enhancement and lightweight deep learning techniques.
C1 [Latke, Vaishali; Narawade, Vaibhav] DY Patil Deemed Be Univ, Ramrao Adik Inst Technol, Dept Comp Engn, Nerul, India.
RP Latke, V (corresponding author), DY Patil Deemed Be Univ, Ramrao Adik Inst Technol, Dept Comp Engn, Nerul, India.
EM vai.lat.rt21@dypatil.edu; vaibhav.narawade@rait.ac.in
RI Narawade, Dr. Vaibhav/CAF-0402-2022
OI Narawade, Dr. Vaibhav/0000-0001-7427-730X
CR Bayrakdar IS, 2022, BIOMED RES INT-UK, V2022, DOI 10.1155/2022/7035367
   Bornstein MM, 2011, J ENDODONT, V37, P151, DOI 10.1016/j.joen.2010.11.014
   Cheon BW, 2023, APPL SCI-BASEL, V13, DOI 10.3390/app132011394
   Cotti E, 2022, INT ENDOD J, V55, P1085, DOI 10.1111/iej.13828
   Cotti Elisabetta, 2010, Dent Clin North Am, V54, P215, DOI 10.1016/j.cden.2009.12.007
   de Oro JECG, 2022, DIAGNOSTICS, V12, DOI 10.3390/diagnostics12071526
   Dutra KL, 2016, J ENDODONT, V42, P356, DOI 10.1016/j.joen.2015.12.015
   Estrela C, 2008, J ENDODONT, V34, P273, DOI 10.1016/j.joen.2007.11.023
   Foros P, 2021, CARIES RES, V55, P247, DOI 10.1159/000516084
   Freire RT, 2022, J DIGIT IMAGING, V35, P654, DOI 10.1007/s10278-022-00596-w
   Hussein HI, 2023, EXPERT SYST APPL, V223, DOI 10.1016/j.eswa.2023.119900
   Kazemipoor Maryam, 2019, Iran Endod J, V14, P259, DOI 10.22037/iej.v14i4.24188
   Latke V., 2023, Int. J. Intell. Syst. Appl. Eng., V11, P73
   Li CW, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21217049
   Low KMT, 2008, J ENDODONT, V34, P557, DOI 10.1016/j.joen.2008.02.022
   Meusburger T, 2023, J CLIN MED, V12, DOI 10.3390/jcm12062224
   Molander B, 1993, Dentomaxillofac Radiol, V22, P28
   ORSTAVIK D, 1986, Endodontics and Dental Traumatology, V2, P20
   Patel S, 2012, INT ENDOD J, V45, P711, DOI 10.1111/j.1365-2591.2012.02076.x
   Patel S, 2012, INT ENDOD J, V45, P702, DOI 10.1111/j.1365-2591.2011.01989.x
   Pauwels R, 2021, OR SURG OR MED OR PA, V131, P610, DOI 10.1016/j.oooo.2021.01.018
   Rahman ZU, 2004, J ELECTRON IMAGING, V13, P100, DOI 10.1117/1.1636183
   Saidi Anastasia, 2015, J Int Oral Health, V7, P15
   Sakhdari Shirin, 2016, J Dent (Tehran), V13, P77
   Sebring D, 2022, INT ENDOD J, V55, P6, DOI 10.1111/iej.13634
   Sebring D, 2021, ACTA ODONTOL SCAND, V79, P554, DOI 10.1080/00016357.2021.1910728
   Seker O, 2021, DENTOMAXILLOFAC RAD, V50, DOI 10.1259/dmfr.20210026
   Sorantin E, 2022, PEDIATR RADIOL, V52, P2074, DOI 10.1007/s00247-021-05177-7
   Stöger K, 2021, COMMUN ACM, V64, P34, DOI 10.1145/3458652
   Sun Y, 2022, FRONT BIOENG BIOTECH, V10, DOI 10.3389/fbioe.2022.865820
   Tibúrcio-Machado CS, 2021, INT ENDOD J, V54, P712, DOI 10.1111/iej.13467
   Wulk Annika, 2023, Int J Environ Res Public Health, V20, DOI 10.3390/ijerph20032619
NR 32
TC 0
Z9 0
U1 4
U2 4
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD JUN
PY 2024
VL 146
AR 105016
DI 10.1016/j.imavis.2024.105016
EA APR 2024
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA RD2B5
UT WOS:001225656500001
DA 2024-08-05
ER

PT J
AU Si, HY
   Wei, XY
AF Si, Hongying
   Wei, Xianyong
TI Feature extraction and representation learning of 3D point cloud data
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Deep learning; 3D data; Point cloud; Represent learning; Feature
   extraction
AB Three-dimensional point cloud data serves as a critical source of information in various real-world application domains, such as computer vision, robotics, geographic information systems, and medical image processing. Due to the discrete and unordered nature of point clouds, applying 2D image feature extractors directly to the extraction of 3D point cloud features is challenging. Therefore, we propose a novel variational feature component extraction method called PointFEA. This paper aims to research and propose a series of methods to enhance the feature extraction and representation learning of 3D point cloud data. Firstly, in terms of feature extraction, local neighborhood encoding is combined with the local latent representation of point clouds to obtain more correlated point cloud features. Secondly, in the domain of point cloud representation learning, the multi-scale representation learning method maps point cloud data into a high-dimensional space to better capture critical features and adapt to different granularities of point cloud data. Lastly, features of different dimensions are input into a cross-fusion transformer to obtain local attention coefficients. We validate our methods on commonly used point cloud datasets, and the experiments demonstrate the effectiveness of our approach, achieving accuracies of 94.8% on ModelNet40 and 89.1% on ScanObjectNN.
C1 [Si, Hongying] Shangqiu Normal Univ, Sch Math & Stat, Shangqiu 476000, Henan, Peoples R China.
   [Wei, Xianyong] Shangqiu Polytech, Coll Comp Engn, Shangqiu 476000, Henan, Peoples R China.
C3 Shangqiu Normal University
RP Si, HY (corresponding author), Shangqiu Normal Univ, Sch Math & Stat, Shangqiu 476000, Henan, Peoples R China.
EM sihongying@sqnu.edu.cn
FU Key Scientific Research Proiects of colleges and universities in Henan
   Province "Research on path optimization of mobile robot for intelligent
   navigation" [23A520059]
FX This work is supported in part by Key Scientific Research Proiects of
   colleges and universities in Henan Province "Research on path
   optimization of mobile robot for intelligent navigation" [No.23A520059].
   We thank all the anonymous reviewers who generously contributed their
   time and efforts. Their professional recommendations have greatly
   enhanced the quality of the manuscript.
CR Atzmon M, 2018, Arxiv, DOI arXiv:1803.10091
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Feng T, 2023, IEEE I CONF COMP VIS, P8249, DOI 10.1109/ICCV51070.2023.00761
   Goyal A, 2021, PR MACH LEARN RES, V139
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   He YQ, 2015, NEUROCOMPUTING, V151, P354, DOI 10.1016/j.neucom.2014.09.029
   Hui L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6078, DOI 10.1109/ICCV48922.2021.00604
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Klokov R, 2017, IEEE I CONF COMP VIS, P863, DOI 10.1109/ICCV.2017.99
   Li JX, 2018, PROC CVPR IEEE, P9397, DOI 10.1109/CVPR.2018.00979
   Li YG, 2018, ASIA PACIF MICROWAVE, P31, DOI 10.23919/APMC.2018.8617636
   Liang HX, 2022, LECT NOTES COMPUT SC, V13663, P156, DOI 10.1007/978-3-031-20062-5_10
   Lin ZH, 2020, PROC CVPR IEEE, P1797, DOI 10.1109/CVPR42600.2020.00187
   Liu YC, 2019, PROC CVPR IEEE, P8887, DOI 10.1109/CVPR.2019.00910
   Ma X, 2022, Arxiv, DOI arXiv:2202.07123
   Mao JG, 2019, IEEE I CONF COMP VIS, P1578, DOI 10.1109/ICCV.2019.00166
   Meng QH, 2022, IEEE T PATTERN ANAL, V44, P4454, DOI 10.1109/TPAMI.2021.3063611
   Kipf TN, 2017, Arxiv, DOI arXiv:1609.02907
   Pang Y, 2022, LECT NOTES COMPUT SC, V13662, P604, DOI 10.1007/978-3-031-20086-1_35
   Qi CR, 2017, ADV NEUR IN, V30
   Qian G. C., 2022, ADV NEURAL INFORM PR, V35, P23192, DOI [DOI 10.48550/ARXIV.2206.04670, https://doi.org/10.48550/arXiv.2206.04670]
   Qingyong Hu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11105, DOI 10.1109/CVPR42600.2020.01112
   Qiu S, 2022, IEEE T MULTIMEDIA, V24, P1943, DOI 10.1109/TMM.2021.3074240
   Qiu S, 2021, IEEE WINT CONF APPL, P3812, DOI 10.1109/WACV48630.2021.00386
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Uy MA, 2019, IEEE I CONF COMP VIS, P1588, DOI 10.1109/ICCV.2019.00167
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985
   Wu X., 2022, Adv. Neural Inf. Process. Syst., VVolume 35, P33330
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xie SN, 2018, PROC CVPR IEEE, P4606, DOI 10.1109/CVPR.2018.00484
   Xu JY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10448, DOI 10.1109/ICCV48922.2021.01030
   Xu MY, 2020, AAAI CONF ARTIF INTE, V34, P12500
   Xu MT, 2021, AAAI CONF ARTIF INTE, V35, P3056
   Xu YF, 2018, LECT NOTES COMPUT SC, V11212, P90, DOI 10.1007/978-3-030-01237-3_6
   Yan X, 2020, PROC CVPR IEEE, P5588, DOI 10.1109/CVPR42600.2020.00563
   Yi L, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980238
   Yin JB, 2022, LECT NOTES COMPUT SC, V13698, P727, DOI 10.1007/978-3-031-19839-7_42
   Yin JB, 2022, LECT NOTES COMPUT SC, V13699, P17, DOI 10.1007/978-3-031-19842-7_2
   Yu XM, 2022, PROC CVPR IEEE, P19291, DOI 10.1109/CVPR52688.2022.01871
   Zhang H., 2023, Deep learning-based 3d point cloud classification: a systematic survey and outlook, Patent No. Displays102456
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
   Zhao HS, 2019, PROC CVPR IEEE, P5550, DOI 10.1109/CVPR.2019.00571
   Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472
NR 45
TC 1
Z9 1
U1 34
U2 34
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD FEB
PY 2024
VL 142
AR 104890
DI 10.1016/j.imavis.2023.104890
EA JAN 2024
PG 10
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA GR7P0
UT WOS:001154468000001
DA 2024-08-05
ER

PT J
AU Yu, XH
   Tian, JJ
   Chen, ZP
   Meng, YZ
   Zhang, J
AF Yu, Xiaohui
   Tian, Jingjun
   Chen, Zhipeng
   Meng, Yizhen
   Zhang, Jun
TI Predictive breast cancer diagnosis using ensemble fuzzy model
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Breast cancer diagnosis; Ensemble; Deep learning; Fuzzy logic; Inception
   V3; Medical imaging
ID NETWORK
AB Breast cancer continues to be a major global health challenge, necessitating reliable diagnostic methods for early detection and improved patient outcomes. This study introduces a novel ensemble fuzzy model for predictive breast cancer diagnosis, integrating multiple deep-learning classifiers with fuzzy logic to enhance decisionmaking. Traditional diagnostic approaches often struggle with the complexity and heterogeneity of breast cancer data, which this new model addresses through an innovative ensemble technique. The ensemble model combines the strengths of Inception-V4, Inception-ResNet, and Inception V3/V4 + BN, with fuzzy logic for adaptive priority assignment based on confidence scores. The method employs a re-parameterized Gompertz function to assign fuzzy ranks to constituent classifiers, allowing flexible fusion strategies. The proposed model is evaluated on two benchmark breast cancer datasets: the Digital Database for Screening Mammography (DDSM) and the Breast Cancer Histopathological Image Classification (BACH) dataset. It achieves high performance across key metrics, including accuracy, precision, recall, and F1 score, consistently outperforming individual classifiers. On the DDSM dataset, the ensemble fuzzy model attains an accuracy of 0.97, a recall of 0.93, a precision of 0.95, and an F1 score of 0.96. Similarly, on the BACH dataset, the proposed method records an accuracy of 97.05%, a recall of 99.31%, a precision of 95.44%, and an F1 score of 97.37%, demonstrating its robust capability to identify positive instances and maintain a balanced performance. These results highlight the potential of the ensemble fuzzy model to improve breast cancer diagnosis, offering a reliable solution to the inherent challenges in this field.
C1 [Yu, Xiaohui; Tian, Jingjun; Chen, Zhipeng; Meng, Yizhen; Zhang, Jun] Tangshan Normal Univ, Dept Comp Sci, Tangshan 063000, Hebei, Peoples R China.
C3 Tangshan Normal University
RP Tian, JJ (corresponding author), Tangshan Normal Univ, Dept Comp Sci, Tangshan 063000, Hebei, Peoples R China.
EM 1208673379@qq.com
FU Science and Technology Plan Project of Tangshan Science and Technology
   Bureau Tangshan Foundation Innovation Team of Digital Media Security
   [21130212D]
FX This work was supported in part by Science and Technology Plan Project
   of Tangshan Science and Technology Bureau Tangshan Foundation Innovation
   Team of Digital Media Security under Grant 21130212D.
CR Abunasser Basem S, 2023, Asian Pac J Cancer Prev, V24, P531, DOI 10.31557/APJCP.2023.24.2.531
   Afrin H, 2023, CANCERS, V15, DOI 10.3390/cancers15123139
   Almutairi SM, 2023, INT J INTELL SYST, V2023, DOI 10.1155/2023/8509433
   Amrisha R.R., 2023, 2023 2 INT C ADV EL, P1
   Deb SD, 2023, BIOMED SIGNAL PROCES, V85, DOI 10.1016/j.bspc.2023.104871
   Ghose S, 2023, CANCERS, V15, DOI 10.3390/cancers15071922
   Koné I, 2018, LECT NOTES COMPUT SC, V10882, P796, DOI 10.1007/978-3-319-93000-8_90
   Kumari D, 2023, BIOMED SIGNAL PROCES, V86, DOI 10.1016/j.bspc.2023.105121
   Lee RS, 2017, SCI DATA, V4, DOI 10.1038/sdata.2017.177
   Malathi S., 2023, J. Pharm. Negative Results, V14
   Manimurugan S, 2024, AIN SHAMS ENG J, V15, DOI 10.1016/j.asej.2024.102734
   Mui-zzud-din, 2024, MULTIMED TOOLS APPL, V83, P9503, DOI 10.1007/s11042-023-15488-6
   Muthu Subathra L., 2023, J. Pharm. Negative Results, P60
   Pati Abhilash, 2023, Designs, DOI 10.3390/designs7030057
   Pati A, 2023, DIAGNOSTICS, V13, DOI 10.3390/diagnostics13132191
   Rani J, 2023, EXPERT SYST, V40, DOI 10.1111/exsy.13309
   Saednia K, 2023, MED PHYS, V50, P7852, DOI 10.1002/mp.16574
   Sarveshwaran V, 2023, EXPERT SYST, V40, DOI 10.1111/exsy.13335
   Shanthi D, 2023, SOFT COMPUT, DOI 10.1007/s00500-023-08245-2
   Singh Ritika, 2023, Comp. Intellig. Aid. Syst. Healthc. Domain, P347
   Sirjani N, 2023, PHYS MEDICA, V107, DOI 10.1016/j.ejmp.2023.102560
   Velliangiri S., 2021, J. Mobile Multimedia, P349, DOI [10.13052/jmm1550-4646.18210, DOI 10.13052/JMM1550-4646.18210]
   Wang CW, 2023, CANCERS, V15, DOI 10.3390/cancers15153991
   Xia XL, 2017, 2017 2ND INTERNATIONAL CONFERENCE ON IMAGE, VISION AND COMPUTING (ICIVC 2017), P783, DOI 10.1109/ICIVC.2017.7984661
   Zakareya S, 2023, DIAGNOSTICS, V13, DOI 10.3390/diagnostics13111944
NR 25
TC 0
Z9 0
U1 0
U2 0
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD AUG
PY 2024
VL 148
AR 105146
DI 10.1016/j.imavis.2024.105146
EA JUN 2024
PG 9
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA XO2P5
UT WOS:001262563300001
DA 2024-08-05
ER

PT J
AU Xia, HY
   Su, CH
   Song, SX
   Tan, YM
AF Xia, Haiying
   Su, Chunhai
   Song, Shuxiang
   Tan, Yumei
TI Dual-consistency constraints network for noisy facial expression
   recognition
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Facial expression recognition; Noisy label learning; Consistency
   constraints
AB Although existing facial expression recognition (FER) methods have achieved great success, their performance degrades significantly under noisy labels caused by low-quality images, ambiguous expressions, and subjective and incorrect labeling. Recent studies have shown that deep neural networks (DNNs) can easily overfit noisy labels, which poses a great challenge to FER task in real-world scenarios. To address this issue, we propose a novel Dual-consistency Constraints Network (DC-Net) to automatically suppress noisy samples during training. Specifically, we first propose a Class Activation Mapping (CAM) Attention Consistency (CAC), which makes the model focus on partially important feature information. As a result, we obtain more robust local feature representations and reduce excessive attention to noisy labels. Then, a Class Feature Consistency (CFC) is designed to encourage the model to focus on the global semantic information of the image. Finally, with the collaboration of the CAC and the CFC, DC-Net can learn robust local and global feature information to prevent the model from learning biased information with noisy labels. We conducted extensive experiments on three field datasets, including RAF-DB, AffectNet, and FERPlus2013. Experimental results show that DC-Net significantly outperforms state-of-the-art noisy labeling methods at different noise rates and generalizes well to other tasks with a large number of classes, such as CIFAR100 and Tiny- ImageNet.
C1 [Xia, Haiying; Su, Chunhai; Song, Shuxiang] Guangxi Normal Univ, Sch Elect & Informat Engn, Sch Integrated Circuits, Gulin, Peoples R China.
   [Tan, Yumei] Guangxi Normal Univ, Sch Comp Sci & Engn, Guilin, Peoples R China.
C3 Guangxi Normal University; Guangxi Normal University
RP Tan, YM (corresponding author), Guangxi Normal Univ, Sch Comp Sci & Engn, Guilin, Peoples R China.
EM xhy22@mailbox.gxnu.edu.cn; 774375855@qq.com;
   songshuxiang@mailbox.gxnu.edu.cn; tanyumei@stu.gxnu.edu.cn
FU National Natural Science Foundation of China [62366006, 62106054,
   62167001]; Guilin Science and Technology Development Project
   [20222C243986]
FX This work is supported by the National Natural Science Foundation of
   China (No.62366006, No.62106054, and No.62167001) , the Guilin Science
   and Technology Development Project (No.20222C243986) .
CR Arazo E, 2019, PR MACH LEARN RES, V97
   Arpit D, 2017, PR MACH LEARN RES, V70
   Barsoum E, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P279, DOI 10.1145/2993148.2993165
   Fan XY, 2020, IEEE IMAGE PROC, P903, DOI [10.1109/ICIP40778.2020.9190643, 10.1109/icip40778.2020.9190643]
   Farzaneh AH, 2021, IEEE WINT CONF APPL, P2401, DOI 10.1109/WACV48630.2021.00245
   Gera D, 2023, Arxiv, DOI [arXiv:2305.01884, DOI 10.48550/ARXIV.2305.01884]
   Gera D, 2021, IEEE INT CONF COMP V, P3578, DOI 10.1109/ICCVW54120.2021.00399
   Goodfellow Ian J., 2013, Neural Information Processing. 20th International Conference, ICONIP 2013. Proceedings: LNCS 8228, P117, DOI 10.1007/978-3-642-42051-1_16
   Gu Y, 2023, IEEE T CIRC SYST VID, V33, P2033, DOI 10.1109/TCSVT.2022.3220669
   Guo H, 2022, INT J COMPUT VISION, V130, P1088, DOI 10.1007/s11263-022-01591-y
   Guo H, 2019, PROC CVPR IEEE, P729, DOI 10.1109/CVPR.2019.00082
   Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6
   Han B., 2020, P 34 C NEUR INF PROC, P4
   Han B, 2018, ADV NEUR IN, V31
   Han JF, 2019, IEEE I CONF COMP VIS, P5137, DOI 10.1109/ICCV.2019.00524
   Haq MA, 2023, FRACTALS, V31, DOI 10.1142/S0218348X23401023
   Haq MA, 2023, ELECTRONICS-SWITZ, V12, DOI 10.3390/electronics12051159
   Haq MA, 2022, COMPUT SYST SCI ENG, V42, P1031, DOI 10.32604/csse.2022.023221
   Haq MA, 2022, COMPUT SYST SCI ENG, V42, P837, DOI 10.32604/csse.2022.023016
   Haq MA, 2022, CMC-COMPUT MATER CON, V71, P2363, DOI 10.32604/cmc.2022.023059
   Haq MA, 2022, CMC-COMPUT MATER CON, V71, P1769, DOI 10.32604/cmc.2022.018708
   Haq MA, 2022, CMC-COMPUT MATER CON, V71, P1403, DOI 10.32604/cmc.2022.021968
   Haq MA, 2022, CMC-COMPUT MATER CON, V71, P1729, DOI 10.32604/cmc.2022.020938
   Haq MA, 2021, J INDIAN SOC REMOTE, V49, P601, DOI 10.1007/s12524-020-01231-3
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hongxin Wei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13723, DOI 10.1109/CVPR42600.2020.01374
   Huang JC, 2019, IEEE I CONF COMP VIS, P3325, DOI 10.1109/ICCV.2019.00342
   Jiang J, 2023, IEEE T AFFECT COMPUT, V14, P2402, DOI 10.1109/TAFFC.2021.3131621
   Kim Y, 2019, IEEE I CONF COMP VIS, P101, DOI 10.1109/ICCV.2019.00019
   Kingma D., 2014, ICLR 2015 C
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Le N, 2023, IEEE WINT CONF APPL, P6077, DOI 10.1109/WACV56688.2023.00603
   Li HY, 2021, IEEE T IMAGE PROCESS, V30, P2016, DOI 10.1109/TIP.2021.3049955
   Li JN, 2020, Arxiv, DOI arXiv:2002.07394
   Li JN, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9465, DOI 10.1109/ICCV48922.2021.00935
   Li S, 2017, PROC CVPR IEEE, P2584, DOI 10.1109/CVPR.2017.277
   Li ZY, 2019, Arxiv, DOI arXiv:1910.07454
   Lu J, 2018, PR MACH LEARN RES, V80
   Ma F., 2023, IEEE Transactions on Affective Computing, V99, P1
   Malach E, 2017, ADV NEUR IN, V30
   Mollahosseini A, 2019, IEEE T AFFECT COMPUT, V10, P18, DOI 10.1109/TAFFC.2017.2740923
   Nguyen D. T., 2019, Self: Learning to filter noisy labels with self-ensembling
   Patrini G, 2017, PROC CVPR IEEE, P2233, DOI 10.1109/CVPR.2017.240
   Reed S., 2014, Training deep neural networks on noisy labels with bootstrapping, DOI DOI 10.48550/ARXIV.1412.6596
   Ren MY, 2018, PR MACH LEARN RES, V80
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Saleem R.M., 2023, IEEE Access
   She JH, 2021, PROC CVPR IEEE, P6244, DOI 10.1109/CVPR46437.2021.00618
   Shikai Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13981, DOI 10.1109/CVPR42600.2020.01400
   Sukhbaatar S, 2015, Arxiv, DOI [arXiv:1406.2080, 10.48550/arXiv.1406.2080]
   Thulasidasan S, 2019, Arxiv, DOI arXiv:1905.10964
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang K, 2020, PROC CVPR IEEE, P6896, DOI 10.1109/CVPR42600.2020.00693
   Wang K, 2020, IEEE T IMAGE PROCESS, V29, P4057, DOI 10.1109/TIP.2019.2956143
   Xie MK, 2022, IEEE T PATTERN ANAL, V44, P3676, DOI 10.1109/TPAMI.2021.3059290
   Xu Y., 2019, Adv. Neural Inf. Proces. Syst., V32
   Ye M, 2020, IEEE T INF FOREN SEC, V15, P2655, DOI 10.1109/TIFS.2020.2970590
   Yi K, 2019, PROC CVPR IEEE, P7010, DOI 10.1109/CVPR.2019.00718
   Zeng JB, 2018, LECT NOTES COMPUT SC, V11217, P227, DOI 10.1007/978-3-030-01261-8_14
   Zhang CY, 2021, COMMUN ACM, V64, P107, DOI 10.1145/3446776
   Zhang FF, 2022, IEEE T MULTIMEDIA, V24, P1800, DOI 10.1109/TMM.2021.3072786
   Zhang Y., 2021, Advances in Neural Information Processing Systems, V34, P17616
   Zhang YH, 2022, LECT NOTES COMPUT SC, V13686, P418, DOI 10.1007/978-3-031-19809-0_24
   Zhang ZL, 2018, ADV NEUR IN, V31
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
NR 65
TC 0
Z9 0
U1 1
U2 1
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD AUG
PY 2024
VL 148
AR 105141
DI 10.1016/j.imavis.2024.105141
EA JUN 2024
PG 10
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA XH7F6
UT WOS:001260850200001
DA 2024-08-05
ER

PT J
AU Wu, RX
   Liu, YL
   Wang, XG
   Yang, PL
AF Wu, Ruixu
   Liu, Yanli
   Wang, Xiaogang
   Yang, Peilin
TI Visual tracking based on spatiotemporal transformer and fusion sequences
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Visual tracking; Flatten transformer; Spatiotemporal; Sequence fusion
ID OBJECT TRACKING
AB Currently, Transformer-based visual tracking methods have exhibited impressive performance. However, despite their widespread adoption, they still have certain limitations. For example, the design of the Transformer framework is somewhat original and redundant, resulting in lower efficiency. In addition, their application methods lack time consideration, and there is a lack of spatiotemporal correlation between tracking video sequences and predicting coordinate sequences, making it difficult to effectively integrate, and the robustness of corresponding tracking templates is insufficient. To address these issues, we propose a new visual tracking method (STFS). Firstly, it introduces a novel Flatten Transformer architecture, which, in comparison to previous modules, offers enhanced efficiency and expressiveness. Secondly, it takes multi frame feature maps and bounding box coordinates as inputs, integrates spatiotemporal information through the spatiotemporal sequence attention module, and provides relevant sequences for historical trend prediction. Finally, it uses diffusion methods to construct tracking templates and improve stability. To verify the performance of the tracker, we conducted experiments on benchmark datasets including GOT-10 K, LaSOT, TrackingNet, VOT2020, OTB100, and UAV123. The results demonstrate that STFS has achieved competitive experimental results.
C1 [Wu, Ruixu; Liu, Yanli; Wang, Xiaogang; Yang, Peilin] Shanghai Dianji Univ, Sch Elect Informat, Shanghai 201306, Peoples R China.
C3 Shanghai Dianji University
RP Liu, YL (corresponding author), Shanghai Dianji Univ, Sch Elect Informat, Shanghai 201306, Peoples R China.
EM wuruixu@sdju.edu.cn; liuyl@sdju.edu.cn; wangxg@sdju.edu.cn;
   peilin_yang@sdju.edu.cn
FU Shanghai Science and Technology Program, China [23010501000]; Natural
   Science Foundation of Shanghai Science and Technology Innovation Action
   Plan of China [22ZR1425300]; Shanghai Educational Science Research
   Project, China [C2022056]; Humanities and Social Sciences of Ministry of
   Education Planning Fund, China [22YJAZHA145]; National Natural Science
   Foundation of China [61963017]
FX This work was supported in part by Shanghai Science and Technology
   Program, China, under Grant 23010501000; in part by the Natural Science
   Foundation of Shanghai Science and Technology Innovation Action Plan of
   China under Grant 22ZR1425300; in part by Shanghai Educational Science
   Research Project, China, under Grant C2022056; in part by Humanities and
   Social Sciences of Ministry of Education Planning Fund, China, under
   Grant 22YJAZHA145; in part by the National Natural Science Foundation of
   China under Grant 61963017.
CR Alismail H, 2016, INT CONF 3D VISION, P389, DOI 10.1109/3DV.2016.48
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bhat G, 2019, IEEE I CONF COMP VIS, P6181, DOI 10.1109/ICCV.2019.00628
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Borsuk V, 2022, LECT NOTES COMPUT SC, V13682, P644, DOI 10.1007/978-3-031-20047-2_37
   Bouchrika I, 2016, MULTIMED TOOLS APPL, V75, P1201, DOI 10.1007/s11042-014-2364-9
   Carion N., 2020, EUROPEAN C COMPUTER
   Chen BY, 2022, LECT NOTES COMPUT SC, V13682, P375, DOI 10.1007/978-3-031-20047-2_22
   Chen X, 2023, PROC CVPR IEEE, P14572, DOI 10.1109/CVPR52729.2023.01400
   Chen X, 2021, PROC CVPR IEEE, P8122, DOI 10.1109/CVPR46437.2021.00803
   Chen ZD, 2020, PROC CVPR IEEE, P6667, DOI 10.1109/CVPR42600.2020.00670
   Cui YT, 2022, PROC CVPR IEEE, P13598, DOI 10.1109/CVPR52688.2022.01324
   Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Du XF, 2015, INT J COMPUT ASS RAD, V10, P1915, DOI 10.1007/s11548-015-1243-9
   Fan H, 2019, PROC CVPR IEEE, P7944, DOI 10.1109/CVPR.2019.00814
   Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552
   Fu ZH, 2022, Arxiv, DOI arXiv:2205.03776
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1144, DOI [10.1109/ICCV.2017.129, 10.1109/ICCV.2017.128]
   Gao JY, 2019, PROC CVPR IEEE, P4644, DOI 10.1109/CVPR.2019.00478
   Gao SY, 2022, Arxiv, DOI arXiv:2207.09603
   Guo DY, 2020, PROC CVPR IEEE, P6268, DOI 10.1109/CVPR42600.2020.00630
   Han D., 2023, INT C COMP VIS ICCV
   He KM, 2022, PROC CVPR IEEE, P15979, DOI 10.1109/CVPR52688.2022.01553
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Ho J., 2020, Adv. Neural. Inf. Process. Syst, V33, P6840, DOI DOI 10.48550/ARXIV.2006.11239
   Huang LH, 2020, AAAI CONF ARTIF INTE, V34, P11037
   Huang LH, 2021, IEEE T PATTERN ANAL, V43, P1562, DOI 10.1109/TPAMI.2019.2957464
   Huang YQ, 2024, Arxiv, DOI arXiv:2403.19242
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li X, 2023, IEEE I CONF COMP VIS, P9940, DOI 10.1109/ICCV51070.2023.00915
   Lin L., 2022, Adv. Neural Inf. Process. Syst, V35, P16743
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu Y, 2020, Arxiv, DOI arXiv:2010.07605
   Loshchilov I., 2017, ARXIV
   Lukezic A, 2020, PROC CVPR IEEE, P7131, DOI 10.1109/CVPR42600.2020.00716
   Ma F, 2022, PROC CVPR IEEE, P8771, DOI 10.1109/CVPR52688.2022.00858
   Mayer C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13424, DOI 10.1109/ICCV48922.2021.01319
   Memarmoghadam A., 2021, The Eighth Visual Object Tracking VOT2020 Challenge Results
   Müller M, 2018, LECT NOTES COMPUT SC, V11205, P310, DOI 10.1007/978-3-030-01246-5_19
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Peebles W, 2023, IEEE I CONF COMP VIS, P4172, DOI 10.1109/ICCV51070.2023.00387
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Saribas H, 2022, NEUROCOMPUTING, V492, P150, DOI 10.1016/j.neucom.2022.04.043
   Smeulders AWM, 2014, IEEE T PATTERN ANAL, V36, P1442, DOI 10.1109/TPAMI.2013.230
   Song ZK, 2023, Arxiv, DOI arXiv:2301.10938
   Song ZK, 2022, PROC CVPR IEEE, P8781, DOI 10.1109/CVPR52688.2022.00859
   Tang F, 2022, PROC CVPR IEEE, P8731, DOI 10.1109/CVPR52688.2022.00854
   Tao R, 2016, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2016.158
   Teng Z, 2017, IEEE I CONF COMP VIS, P1153, DOI 10.1109/ICCV.2017.130
   Tokekar P, 2014, IEEE INT C INT ROBOT, P3067, DOI 10.1109/IROS.2014.6942986
   Voigtlaender P., 2020, INT C COMP VIS PATT
   Wang N, 2021, PROC CVPR IEEE, P1571, DOI 10.1109/CVPR46437.2021.00162
   Wang Q, 2019, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2019.00142
   Wu RX, 2022, KNOWL-BASED SYST, V256, DOI 10.1016/j.knosys.2022.109897
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Xu Y., 2020, C ART INT AAAI
   Yan B, 2021, PROC CVPR IEEE, P5285, DOI 10.1109/CVPR46437.2021.00525
   Yan B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10428, DOI 10.1109/ICCV48922.2021.01028
   Ye B., 2022, EUR C COMP VIS
   Zhang JP, 2023, NEUROCOMPUTING, V522, P73, DOI 10.1016/j.neucom.2022.11.093
   Zhang KH, 2014, LECT NOTES COMPUT SC, V8693, P127, DOI 10.1007/978-3-319-10602-1_9
   Zhang T., 2021, IEEE Trans. Circuits Syst. Video Technol.
   Zhang YC, 2018, ICMLC 2020: 2020 12TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND COMPUTING, P145, DOI 10.1145/3383972.3383975
   Zhao MJ, 2021, Arxiv, DOI [arXiv:2105.03817, DOI 10.48550/ARXIV.2105.03817]
   Zhao Y, 2024, Arxiv, DOI arXiv:2311.16567
   Zhao Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P455, DOI 10.1007/978-3-030-58610-2_27
   Zheng L., 2020, EUR C COMP VIS ECCV
   Zhu Z, 2018, PROC CVPR IEEE, P548, DOI 10.1109/CVPR.2018.00064
   Zhu Z, 2018, LECT NOTES COMPUT SC, V11213, P103, DOI 10.1007/978-3-030-01240-3_7
   Zuo WM, 2019, IEEE T PATTERN ANAL, V41, P1158, DOI 10.1109/TPAMI.2018.2829180
NR 74
TC 0
Z9 0
U1 6
U2 6
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD AUG
PY 2024
VL 148
AR 105107
DI 10.1016/j.imavis.2024.105107
EA JUN 2024
PG 14
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA UY4Z1
UT WOS:001251624400001
DA 2024-08-05
ER

PT J
AU Atrey, K
   Singh, BK
   Bodhey, NK
AF Atrey, Kushangi
   Singh, Bikesh Kumar
   Bodhey, Narendra Kuber
TI Integration of ultrasound and mammogram for multimodal classification of
   breast cancer using hybrid residual neural network and machine learning
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Breast cancer; Feature fusion; Multimodal classification; Transfer
   learning; Machine learning
AB Breast cancer (BC) is one of the topmost causes of mortality in women all over the world. Early detection and classification of the tumor allow proper treatment of patients and chances of survival. In this article, we propose a hybrid residual neural network (ResNet) and machine learning framework and integrate the features of both mammography (MG) and ultrasound (US) images to perform the multimodal classification of BC images as benign or malignant. The features are extracted automatically from the input images of each modality using the residual neural network from the average pooling layer. Next, the feature level fusion is carried out to obtain a feature vector by combining features of MG & US. Finally, the multimodal classification is performed using the support vector machine (SVM) as a classifier. Experiments are performed on a real-time dataset collected from patients who have undergone both MG and US examinations. The classification accuracy obtained for the multimodal approach with SVM is 99.22%, which is higher than unimodal systems. Results show that the proposed multimodal approach performs better in classifying breast tumors than unimodal mammogram and ultrasound systems.
C1 [Atrey, Kushangi; Singh, Bikesh Kumar] Natl Inst Technol Raipur, Dept Biomed Engn, Raipur 492010, Chhattisgarh, India.
   [Bodhey, Narendra Kuber] All India Inst Med Sci, Dept Radiodiag, Raipur 492099, Chhattisgarh, India.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Raipur; All India Institute of Medical Sciences (AIIMS)
   Raipur
RP Singh, BK (corresponding author), Natl Inst Technol Raipur, Dept Biomed Engn, Raipur 492010, Chhattisgarh, India.
EM katrey.phd2017.bme@nitrr.ac.in; bsingh.bme@nitrr.ac.in;
   nkbodhey@aiimsraipur.edu.in
RI Atrey, Dr. Kushangi/KHX-0353-2024
CR Abdar M, 2020, PATTERN RECOGN LETT, V132, P123, DOI 10.1016/j.patrec.2018.11.004
   Ali MD, 2023, DIAGNOSTICS, V13, DOI 10.3390/diagnostics13132242
   Amin MN, 2023, BIOMED SIGNAL PROCES, V85, DOI 10.1016/j.bspc.2023.104808
   Arya N, 2022, IEEE ACM T COMPUT BI, V19, P1032, DOI 10.1109/TCBB.2020.3018467
   Assari Z, 2022, BIOMED SIGNAL PROCES, V73, DOI 10.1016/j.bspc.2021.103453
   Atrey Kushangi, 2020, 2020 First International Conference on Power, Control and Computing Technologies (ICPC2T), P454, DOI 10.1109/ICPC2T48082.2020.9071501
   Atrey K, 2022, INT J IMAG SYST TECH, V32, P1084, DOI 10.1002/ima.22690
   Atrey K, 2019, BRAZ ARCH BIOL TECHN, V62, DOI 10.1590/1678-4324-2019180486
   Avci H, 2023, DIAGNOSTICS, V13, DOI 10.3390/diagnostics13030348
   Cruz-Ramos C, 2023, ENTROPY-SWITZ, V25, DOI 10.3390/e25070991
   Daoud MI, 2019, EXPERT SYST APPL, V121, P78, DOI 10.1016/j.eswa.2018.11.024
   Deb SD, 2023, BIOMED SIGNAL PROCES, V85, DOI 10.1016/j.bspc.2023.104871
   Devarakonda M., 2019, Int. J. Innov. Technol. Explor. Eng., V8, P555
   Huang RB, 2021, MED IMAGE ANAL, V72, DOI 10.1016/j.media.2021.102137
   Inan MSK, 2022, BIOMED SIGNAL PROCES, V75, DOI 10.1016/j.bspc.2022.103553
   Iqbal A, 2023, KNOWL-BASED SYST, V267, DOI 10.1016/j.knosys.2023.110393
   Jemal A, 2011, CA-CANCER J CLIN, V61, P134, DOI [10.3322/caac.20107, 10.3322/caac.21492, 10.3322/caac.20115]
   Khan S, 2019, PATTERN RECOGN LETT, V125, P1, DOI 10.1016/j.patrec.2019.03.022
   Kriti, 2019, BIOCYBERN BIOMED ENG, V39, P536, DOI 10.1016/j.bbe.2019.02.004
   Li HY, 2022, IEEE T MED IMAGING, V41, P3, DOI 10.1109/TMI.2021.3102622
   Mishra AK, 2021, EXPERT SYST, V38, DOI 10.1111/exsy.12713
   Mokni R, 2021, BIOMED SIGNAL PROCES, V69, DOI 10.1016/j.bspc.2021.102914
   Niu XX, 2012, PATTERN RECOGN, V45, P1318, DOI 10.1016/j.patcog.2011.09.021
   Othman NA, 2023, BIG DATA COGN COMPUT, V7, DOI 10.3390/bdcc7010050
   Qiao MY, 2022, IEEE J BIOMED HEALTH, V26, P3059, DOI 10.1109/JBHI.2022.3140236
   Rasool M, 2022, ENTROPY-SWITZ, V24, DOI 10.3390/e24060799
   Sahu A, 2024, BIOMED SIGNAL PROCES, V87, DOI 10.1016/j.bspc.2023.105377
   Shorten C, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0197-0
   Singh B.K., 2015, INT J COMPUTER APPL, V116
   Soulami KB, 2021, BIOMED SIGNAL PROCES, V66, DOI 10.1016/j.bspc.2021.102481
   Wang JX, 2023, MED IMAGE ANAL, V83, DOI 10.1016/j.media.2022.102687
   Yan F, 2023, EXPERT SYST APPL, V227, DOI 10.1016/j.eswa.2023.120282
   Yan R, 2021, BMC MED INFORM DECIS, V21, DOI 10.1186/s12911-020-01340-6
   Yang X, 2023, PATTERN RECOGN, V139, DOI 10.1016/j.patcog.2023.109526
   Yi A, 2021, RADIOLOGY, V298, P568, DOI 10.1148/radiol.2021203134
NR 35
TC 0
Z9 0
U1 3
U2 3
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD MAY
PY 2024
VL 145
AR 104987
DI 10.1016/j.imavis.2024.104987
EA MAR 2024
PG 9
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA QS4K6
UT WOS:001222844800001
DA 2024-08-05
ER

PT J
AU Shin, H
   Lee, BY
   Ku, BH
   Ko, HS
AF Shin, Hyunuk
   Lee, Bokyeung
   Ku, Bonhwa
   Ko, Hanseok
TI Noisy label facial expression recognition via face-specific label
   distribution learning
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Facial expression recognition (FER); Emotion recognition; Noisy label;
   Label distribution learning (LDL); Uncertainty; Ambiguity
ID EMOTION RECOGNITION
AB The uncertainty in Facial expression recognition (FER) data is caused by factors such as ambiguous facial expressions, low -resolution facial images, the subjectivity of the annotator or subject of expression, and compound expression. FER has been steadily evolving with the advent of deep learning, but the label inconsistency problem due to the uncertainty in large FER datasets is one of the challenges in FER. Noisy labels in the label inconsistency problem adversely affect emotion recognition results. In this paper, we propose a Face -Specific Label Distribution Learning (FSLDL) method, which encourages deep networks to predict the actual emotion distribution of the input itself, rather than the noisy single label. Under the assumption that the emotion distribution of a specific sample is similar to the emotion distribution of the augmented sample from a different viewpoint, we generate a new target label distribution using facial expression -specific augmented samples. In order to generate a sophisticated target label distribution, the importance weights of the augmented samples are extracted and multiplied by each predicted emotion distribution. In addition, we compensate for the lack of information in the target label distribution by calculating the uncertainty of the provided label, and use it for model training. Finally, we make a more robust model by adding a rank regularization loss function for the importance weights and a discriminative loss function for the feature vectors. Representative experiments demonstrated that FSLDL outperforms the state-of-the-art on FER datasets such as RAF-DB, AffectNet, and SFEW. In addition, we demonstrated the effectiveness of the proposed method through various noisy label injection experiments.
C1 [Shin, Hyunuk; Lee, Bokyeung; Ku, Bonhwa; Ko, Hanseok] Korea Univ, Sch Elect Engn, Seoul 02841, South Korea.
C3 Korea University
RP Ko, HS (corresponding author), Korea Univ, Sch Elect Engn, Seoul 02841, South Korea.
EM hsko@korea.ac.kr
FU National Research Foundation of Korea (NRF) - Korea Government (MSIT)
   [NRF-2023R1A2C2005916]
FX This work was supported by the National Research Foundation of Korea
   (NRF) grant funded by the Korea Government (MSIT) (NRF-2023R1A2C2005916)
   .
CR Benitez-Quiroz CF, 2016, PROC CVPR IEEE, P5562, DOI 10.1109/CVPR.2016.600
   Cai J, 2018, IEEE INT CONF AUTOMA, P302, DOI 10.1109/FG.2018.00051
   Chattopadhyay J., 2020, New Trends in Computational Vision and Bio-inspired Computing, P1181
   Cowie R, 2001, IEEE SIGNAL PROC MAG, V18, P32, DOI 10.1109/79.911197
   Darwin C., 1872, P374
   Dhall A, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS)
   EKMAN P, 1992, COGNITION EMOTION, V6, P169, DOI 10.1080/02699939208411068
   Farzaneh AH, 2021, IEEE WINT CONF APPL, P2401, DOI 10.1109/WACV48630.2021.00245
   Farzaneh AH, 2020, IEEE COMPUT SOC CONF, P1631, DOI 10.1109/CVPRW50498.2020.00211
   Gwantae Kim, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). Proceedings, P1862, DOI 10.1109/CVPRW50498.2020.00236
   Kahou SE, 2013, ICMI'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P543, DOI 10.1145/2522848.2531745
   Kim Y, 2019, Arxiv, DOI arXiv:1703.07140
   Le N., 2023, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, P6088
   Lee B., 2023, P IEEECVF C COMPUTER, P5680
   Lee S, 2021, IEEE ACCESS, V9, P94557, DOI 10.1109/ACCESS.2021.3092735
   Lee S, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20226688
   Li HY, 2022, PROC CVPR IEEE, P4156, DOI 10.1109/CVPR52688.2022.00413
   Li S, 2017, PROC CVPR IEEE, P2584, DOI 10.1109/CVPR.2017.277
   Li Y, 2019, IEEE T IMAGE PROCESS, V28, P2439, DOI 10.1109/TIP.2018.2886767
   Lukasik M., 2020, INT C MACHINE LEARNI, P6448
   Mollahosseini A, 2019, IEEE T AFFECT COMPUT, V10, P18, DOI 10.1109/TAFFC.2017.2740923
   Radford A, 2021, PR MACH LEARN RES, V139
   Shao JJ, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, DOI 10.1145/3503161.3547960
   She JH, 2021, PROC CVPR IEEE, P6244, DOI 10.1109/CVPR46437.2021.00618
   Shikai Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13981, DOI 10.1109/CVPR42600.2020.01400
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tang YC, 2015, Arxiv, DOI arXiv:1306.0239
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang C, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P238, DOI 10.1145/3343031.3350872
   Wang K, 2020, PROC CVPR IEEE, P6896, DOI 10.1109/CVPR42600.2020.00693
   Wang K, 2020, IEEE T IMAGE PROCESS, V29, P4057, DOI 10.1109/TIP.2019.2956143
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Yan H., 2022, 2022 IEEE INT C MULT, P1
   Yang HY, 2018, PROC CVPR IEEE, P2168, DOI 10.1109/CVPR.2018.00231
   Zeng D, 2022, PROC CVPR IEEE, P20259, DOI 10.1109/CVPR52688.2022.01965
   Zeng JB, 2018, LECT NOTES COMPUT SC, V11217, P227, DOI 10.1007/978-3-030-01261-8_14
   Zhang YH, 2021, ADV NEUR IN, V34
   Zhang YH, 2022, LECT NOTES COMPUT SC, V13686, P418, DOI 10.1007/978-3-031-19809-0_24
   Zhao ZQ, 2021, AAAI CONF ARTIF INTE, V35, P3510
NR 39
TC 1
Z9 1
U1 4
U2 4
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD MAR
PY 2024
VL 143
AR 104901
DI 10.1016/j.imavis.2024.104901
EA FEB 2024
PG 9
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA KK9G9
UT WOS:001179967400001
DA 2024-08-05
ER

PT J
AU Lu, CC
   Jiang, YB
   Fu, KR
   Zhao, QJ
   Yang, HY
AF Lu, Chengcheng
   Jiang, Yiben
   Fu, Keren
   Zhao, Qijun
   Yang, Hongyu
TI LSTPNet: Long short-term perception network for dynamic facial
   expression recognition in the wild
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Dynamic facial expression recognition; Long short-term perception;
   Temporal attention; Transformer
ID EMOTION RECOGNITION; FEATURES; IMAGE
AB In-the-wild dynamic facial expression recognition (DFER) is a very challenging task, and previous methods based on convolutional neural networks (CNNs), recurrent neural networks (RNNs), or Transformers emphasize the extraction of either short-term temporal information or long-term temporal information from facial video sequences. Different from existing methods, this paper proposes a long short-term perceptimon network (LSTPNet) for dynamic facial expression recognition, taking into account the joint perception of the above two temporal cues to benefit the DFER task. Specifically, we propose a long short-term temporal Transformer (LSTformer) which can perceive both long-term and short-term temporal information effectively. In addition, we introduce a temporal channel excitation (TCE) module extended from the previous notable efficient channel attention (ECA) module, in order to establish temporal attention for intermediate features within the backbone network, and obtain more temporally representative features. Experimental results on three benchmark datasets demonstrate the state-of-the-art performance of the proposed LSTPNet. The code will be available at https://github. com/LLFabiann/LSTPNet/.
C1 [Lu, Chengcheng; Jiang, Yiben; Fu, Keren; Zhao, Qijun; Yang, Hongyu] Sichuan Univ, Coll Comp Sci, Chengdu, Sichuan, Peoples R China.
C3 Sichuan University
RP Fu, KR (corresponding author), Sichuan Univ, Coll Comp Sci, Chengdu, Sichuan, Peoples R China.
EM fkrsuper@scu.edu.cn
RI Yang, Hongyu/JXM-2064-2024; Zhao, QiJun/KIH-9623-2024; yang,
   yun/IZE-1092-2023
OI Yang, Hongyu/0000-0002-5894-1693; 
FU NSFC [62176169]; Sichuan Science and Technology Projects [2022YFQ0056,
   2023ZHCG0007]
FX This work was supported in part by the NSFC under No. 62176169, and
   Sichuan Science and Technology Projects (2022YFQ0056, 2023ZHCG0007) .
CR Abbasnejad I, 2017, IEEE INT CONF COMP V, P1609, DOI 10.1109/ICCVW.2017.189
   Abdat F, 2011, UKSIM EURO SYMP COMP, P196, DOI 10.1109/EMS.2011.20
   Arnab A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6816, DOI 10.1109/ICCV48922.2021.00676
   Ayral T, 2021, IEEE WINT CONF APPL, P3028, DOI 10.1109/WACV48630.2021.00307
   Ba L.J., 2016, arXiv, DOI DOI 10.48550/ARXIV.1607.06450
   Bisogni C, 2023, IMAGE VISION COMPUT, V136, DOI 10.1016/j.imavis.2023.104724
   Cai YY, 2016, COMM COM INF SC, V663, P679, DOI 10.1007/978-981-10-3005-5_56
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen WC, 2023, IEEE T AFFECT COMPUT, V14, P800, DOI 10.1109/TAFFC.2020.3027340
   Cho K., 2014, P 2014 C EMP METH NA, DOI 10.3115/v1/D14-1179
   Christoph R., 2016, P ADV NEURAL INFORM
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng JK, 2020, PROC CVPR IEEE, P5202, DOI 10.1109/CVPR42600.2020.00525
   Dhall A, 2019, ICMI'19: PROCEEDINGS OF THE 2019 INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P546, DOI 10.1145/3340555.3355710
   Dhall A, 2013, ICMI'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P509, DOI 10.1145/2522848.2531739
   Dhall A, 2012, IEEE MULTIMEDIA, V19, P34, DOI 10.1109/MMUL.2012.26
   Dosovitskiy A., 2020, P INT C LEARN REPR R
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Fan Y, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P445, DOI 10.1145/2993148.2997632
   Fei ZX, 2020, NEUROCOMPUTING, V388, P212, DOI 10.1016/j.neucom.2020.01.034
   Gong YF, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13061104
   Guo JY, 2022, PROC CVPR IEEE, P12165, DOI 10.1109/CVPR52688.2022.01186
   Hachisuka S, 2011, LECT NOTES ARTIF INT, V6781, P135, DOI 10.1007/978-3-642-21741-8_16
   Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou QB, 2021, PROC CVPR IEEE, P13708, DOI 10.1109/CVPR46437.2021.01350
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jain DK, 2023, IMAGE VISION COMPUT, V133, DOI 10.1016/j.imavis.2023.104659
   Jeong J.-Y., 2023, arXiv
   Jiang XX, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2881, DOI 10.1145/3394171.3413620
   Jung H, 2015, IEEE I CONF COMP VIS, P2983, DOI 10.1109/ICCV.2015.341
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   Khalfallah J, 2015, PROCEDIA COMPUT SCI, V73, P274, DOI 10.1016/j.procs.2015.12.030
   Kim DH, 2019, IEEE T AFFECT COMPUT, V10, P223, DOI 10.1109/TAFFC.2017.2695999
   King DE, 2009, J MACH LEARN RES, V10, P1755
   Kingma J. L., 2015, INT C LEARNING REPRE INT C LEARNING REPRE, P1
   Klaser A., 2008, BMVC, P1
   Kollias D., 2019, P BRIT MACH VIS C, P1
   Kondratyuk D, 2021, PROC CVPR IEEE, P16015, DOI 10.1109/CVPR46437.2021.01576
   Kotsia I, 2007, IEEE T IMAGE PROCESS, V16, P172, DOI 10.1109/TIP.2006.884954
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuo CM, 2018, IEEE COMPUT SOC CONF, P2202, DOI 10.1109/CVPRW.2018.00286
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Li HY, 2022, PROC CVPR IEEE, P4156, DOI 10.1109/CVPR52688.2022.00413
   Li HY, 2022, IEEE T IMAGE PROCESS, V31, P4637, DOI 10.1109/TIP.2022.3186536
   Li Hanting, 2022, arXiv
   Li Hanting, 2023, P AAAI C ART INT, V37, P67, DOI DOI 10.1609/AAAI.V37I1.25077
   Li S, 2017, PROC CVPR IEEE, P2584, DOI 10.1109/CVPR.2017.277
   Li YH, 2022, PROC CVPR IEEE, P4794, DOI 10.1109/CVPR52688.2022.00476
   Liu MY, 2015, LECT NOTES COMPUT SC, V9006, P143, DOI 10.1007/978-3-319-16817-3_10
   Liu Z, 2022, PROC CVPR IEEE, P3192, DOI 10.1109/CVPR52688.2022.00320
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lo HC, 2001, SECOND INTERNATIONAL WORKSHOP ON DIGITAL AND COMPUTATIONAL VIDEO, PROCEEDINGS, P132, DOI 10.1109/DCV.2001.929952
   Lowe D. G., 1999, Computer vision, V2, P1150, DOI [10.1109/ICCV.1999.790410, DOI 10.1109/ICCV.1999.790410]
   Lu C., 2023, P CHIN C PATT REC CO, P172
   Ma F., 2023, P IEEE INT C AC SPEE, P1
   Ma FY, 2022, Arxiv, DOI arXiv:2205.04749
   Ma X, 2022, IMAGE VISION COMPUT, V127, DOI 10.1016/j.imavis.2022.104556
   Mehta S., 2021, P INT C LEARN REPR, P1
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Parkhi O., 2015, BMVC 2015
   Paszke A, 2019, ADV NEUR IN, V32
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Rosenblatt F., 1962, PRINCIPLES NEURODYNA
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Simonyan K, 2014, ADV NEUR IN, V27
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tomar S., 2006, Linux J, V2006, P10
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   Udayakumar N., 2016, Int. J. Sci. Res. Publ., V6, P613
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Vielzeuf Valentin, 2017, P 19 ACM INT C MULT, P569
   Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/ISQED48828.2020.9137057, 10.1109/isqed48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wang X., 2020, P EUR C COMP VIS, P449
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang Y, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, DOI 10.1145/3503161.3547865
   Wang Y, 2022, PROC CVPR IEEE, P20890, DOI 10.1109/CVPR52688.2022.02025
   Wen JT, 2023, INFORM FUSION, V91, P123, DOI 10.1016/j.inffus.2022.10.009
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xue FL, 2022, IEEE COMPUT SOC CONF, P2411, DOI 10.1109/CVPRW56347.2022.00269
   Yan S, 2022, PROC CVPR IEEE, P3323, DOI 10.1109/CVPR52688.2022.00333
   Yu MJ, 2020, PATTERN RECOGN LETT, V131, P166, DOI 10.1016/j.patrec.2020.01.016
   Zhang W, 2022, IEEE COMPUT SOC CONF, P2427, DOI 10.1109/CVPRW56347.2022.00271
   Zhang YH, 2022, LECT NOTES COMPUT SC, V13686, P418, DOI 10.1007/978-3-031-19809-0_24
   Zhang ZY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4086, DOI 10.1109/ICCV48922.2021.00407
   Zhao GY, 2011, IMAGE VISION COMPUT, V29, P607, DOI 10.1016/j.imavis.2011.07.002
   Zhao JF, 2018, VISUAL COMPUT, V34, P1461, DOI 10.1007/s00371-018-1477-y
   Zhao ZQ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1553, DOI 10.1145/3474085.3475292
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
NR 95
TC 0
Z9 0
U1 10
U2 10
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD FEB
PY 2024
VL 142
AR 104915
DI 10.1016/j.imavis.2024.104915
EA JAN 2024
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA JT2Y7
UT WOS:001175360700001
DA 2024-08-05
ER

PT J
AU Marikhu, R
   Dailey, MN
   Ekpanyapong, M
AF Marikhu, Ramesh
   Dailey, Matthew N.
   Ekpanyapong, Mongkol
TI Three dimensional tracking of rigid objects in motion using 2D optical
   flows
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Motion estimation; Optical flows; LM optimization
ID SEGMENTATION; MODEL
AB Detecting and tracking objects in image sequences is paramount for any video analytic. While object detectors have become increasingly robust, motion estimates based on the straightforward approach of running a detector and linking detections are prone to bounding box noise. An algorithm for the monocular estimation of the 3D motion of rigid objects is presented, combining an object detector, minimal camera calibration, and 2D optical flows observed on the image plane. The algorithm utilizes the 2D Delaunay triangulation over geometrically consistent optical flow tracks to identify regions common to the same object. The algorithm is evaluated on the special case of image sequences of vehicles on roads. Experiments on BrnoCompSpeed dataset show that speed estimates from both detector-based tracking (mean error of 1.62 km/h) and optical flow based tracking (mean error of 2.19 km/h) perform competitively on straight roads. An extensive empirical evaluation is also conducted on a new dataset containing synthetic and real world scenes that also include vehicle trajectories involving rotation. A naive baseline bounding box track based method obtained a mean error of 5.29 km/h for vehicle speed, but optical flow tracks performed significantly better with mean error of 1.65 km/h on the new dataset including rotation. With the new method, video analytics such as vehicle speed estimation and lane change detection can obtain precise information about the 3D trajectory of rigid objects in motion.
C1 [Marikhu, Ramesh; Dailey, Matthew N.; Ekpanyapong, Mongkol] Asian Inst Technol, Sch Engn & Technol, Pathum Thani 12120, Thailand.
C3 Asian Institute of Technology
RP Marikhu, R (corresponding author), Asian Inst Technol, Sch Engn & Technol, Pathum Thani 12120, Thailand.
EM marikhu@gmail.com; mdailey@ait.asia; mongkol@ait.asia
FU Asian Institute of Technology
FX Ramesh Marikhu was supported by a graduate fellowship from the Asian
   Institute of Technology.
CR Bewley A, 2016, IEEE IMAGE PROC, P3464, DOI 10.1109/ICIP.2016.7533003
   Beymer D, 1997, PROC CVPR IEEE, P495, DOI 10.1109/CVPR.1997.609371
   bmatraffic, 2023, CCTV for access to road map and real-time traffic information
   Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Brox T, 2010, LECT NOTES COMPUT SC, V6315, P282, DOI 10.1007/978-3-642-15555-0_21
   Carr P, 2012, LECT NOTES COMPUT SC, V7572, P864, DOI 10.1007/978-3-642-33718-5_62
   Cavor I., 2023, 2023 27 INT C INFORM, P1
   Cvijetic A., 2023, 2023 27 INT C INFORM, P1
   Djukanovic S., 2022, 2022 30 TELECOMMUNIC, P1
   Dubská M, 2015, IEEE T INTELL TRANSP, V16, P1162, DOI 10.1109/TITS.2014.2352854
   Llorca DF, 2021, IET INTELL TRANSP SY, V15, P987, DOI 10.1049/itr2.12079
   Hartley R, 2003, Multiple view geometry in computer vision, DOI [10.1016/S0143-8166(01)00145-2, DOI 10.1017/CBO9780511811685]
   He KM, 2018, Arxiv, DOI [arXiv:1703.06870, DOI 10.48550/ARXIV.1703.06870]
   Held D, 2016, LECT NOTES COMPUT SC, V9905, P749, DOI 10.1007/978-3-319-46448-0_45
   Hoiem D, 2008, INT J COMPUT VISION, V80, P3, DOI 10.1007/s11263-008-0137-5
   Jocher G., 2023, YOLO by Ultralytics
   Kalal Z, 2010, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2010.5540231
   Kanhere NK, 2008, IEEE T INTELL TRANSP, V9, P148, DOI 10.1109/TITS.2007.911357
   Kanhere NK, 2005, PROC CVPR IEEE, P1152
   Keuper M, 2020, IEEE T PATTERN ANAL, V42, P140, DOI 10.1109/TPAMI.2018.2876253
   Kim Z, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P524
   Kocur V, 2020, MACH VISION APPL, V31, DOI 10.1007/s00138-020-01117-x
   Krajewski R, 2018, IEEE INT C INTELL TR, P2118, DOI 10.1109/ITSC.2018.8569552
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lourakis MIA., 2004, LEVMAR LEVENBERG MAR
   Lucas Bruce D., 1981, Proceedings of the 7th international joint conference on Artificial intelligence, P674, DOI DOI 10.5555/1623264.1623280
   Luvizon DC, 2017, IEEE T INTELL TRANSP, V18, P1393, DOI 10.1109/TITS.2016.2606369
   Mahendran A., 2018, P EUROPEAN C COMPUTE
   Mingxing Tan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10778, DOI 10.1109/CVPR42600.2020.01079
   Moers T, 2022, IEEE INT VEH SYM, P958, DOI 10.1109/IV51971.2022.9827305
   Pazho A.D., 23 SCANDINAVIAN C SC, P50
   Redmon J, 2018, Arxiv, DOI arXiv:1804.02767
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Santana-Cedrés D, 2016, IMAGE PROCESS ON LIN, V6, P326, DOI 10.5201/ipol.2016.130
   Saunier N., 2006, 3 CANADIAN C COMPUTE
   SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794
   Sochor J, 2019, IEEE T INTELL TRANSP, V20, P1633, DOI 10.1109/TITS.2018.2825609
   Sochor J, 2017, COMPUT VIS IMAGE UND, V161, P87, DOI 10.1016/j.cviu.2017.05.015
   Terven J., 2023, Mach. Learn. Knowl. Extr, V5, P1680, DOI [10.3390/make5040083, DOI 10.3390/MAKE5040083]
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wen LY, 2020, COMPUT VIS IMAGE UND, V193, DOI 10.1016/j.cviu.2020.102907
   Wojke N, 2017, IEEE IMAGE PROC, P3645, DOI 10.1109/ICIP.2017.8296962
   Yu LJ, 2018, 2018 15TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), P372
   Zivkovic Z, 2006, PATTERN RECOGN LETT, V27, P773, DOI 10.1016/j.patrec.2005.11.005
   Zivkovic Z, 2004, INT C PATT RECOG, P28, DOI 10.1109/ICPR.2004.1333992
NR 45
TC 0
Z9 0
U1 2
U2 2
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD FEB
PY 2024
VL 142
AR 104913
DI 10.1016/j.imavis.2024.104913
EA JAN 2024
PG 17
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Engineering; Optics
GA JL3L7
UT WOS:001173281600001
OA hybrid
DA 2024-08-05
ER

EF