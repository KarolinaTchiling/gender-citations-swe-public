FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Liu, XL
   Yan, JB
   Huang, LP
   Fang, YM
   Wan, Z
   Liu, Y
AF Liu, Xuelin
   Yan, Jiebin
   Huang, Liping
   Fang, Yuming
   Wan, Zheng
   Liu, Yang
TI Perceptual Quality Assessment of Omnidirectional Images: A Benchmark and
   Computational Model
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Omnidirectional image; image quality assessment; non-uniform distortion;
   viewing condition
ID SALIENCY; VIDEO
AB Compared with traditional 2D images, omnidirectional images (also referred to as 360 degrees images) have more complicated perceptual characteristics due to the particularities of imaging and display. How humans perceive omnidirectional images in an immersive environment and form the immersive quality of experience are important problems. Thus, it is crucial to measure the quality of omnidirectional images under different viewing conditions, which suffer from realistic distortions. In this article, we build a large-scale subjective assessment database for omnidirectional images and carry out a comprehensive psychophysical experiment to study the relationships between different factors (viewing conditions and viewing behaviors) and the perceptual quality of omnidirectional images. In addition, we collect both subjective ratings and head movement data. A thorough analysis of the collected subjective data is also provided, where we make several interesting findings. Moreover, with the proposed database, we propose a novel transformer-based omnidirectional image quality assessment model. To be consistent with the human viewing process, viewing conditions and behaviors are naturally incorporated into the proposed model. Specifically, the proposed model mainly consists of three parts: viewport sequence generation, multi-scale feature extraction, and perceptual quality prediction. Extensive experimental results conducted on the proposed database demonstrate the effectiveness of the proposed method over existing image quality assessment methods.
C1 [Liu, Xuelin; Yan, Jiebin; Huang, Liping; Fang, Yuming; Wan, Zheng] Jiangxi Univ Finance & Econ, 665 YupingWest St, Nanchang 330032, Jiangxi, Peoples R China.
   [Liu, Yang] SANY Heavy Ind Co Ltd, Beijing 102200, Peoples R China.
C3 Jiangxi University of Finance & Economics; SANY
RP Fang, YM; Wan, Z (corresponding author), Jiangxi Univ Finance & Econ, 665 YupingWest St, Nanchang 330032, Jiangxi, Peoples R China.
EM xuelinliu-bill@foxmail.com; jiebinyan@foxmail.com; phuang19@foxmail.com;
   fa0001ng@e.ntu.edu.sg; wanzheng@jxufe.edu.cn; liuy2655@sany.com.cn
OI Liu, Xuelin/0000-0001-5380-8343
FU Natural Science Foundation of China [62311530101, 62132006, 61961021];
   Natural Science Foundation of Jiangxi Province of China [20223AEI91002,
   20232BAB202001, 20224BAB212012]; China Postdoctoral Science Foundation
   [2022M721417]; Postgraduate Innovation Special Fund of Jiangxi Province
   [YC2022-B155]
FX This work was supported in part by the Natural Science Foundation of
   China under Grants 62311530101, 62132006, and 61961021, in part by the
   Natural Science Foundation of Jiangxi Province of China (20223AEI91002,
   20232BAB202001, 20224BAB212012), in part by the Project funded by China
   Postdoctoral Science Foundation (2022M721417), and in part by the
   Postgraduate Innovation Special Fund of Jiangxi Province under Grant
   YC2022-B155.
CR [Anonymous], 2002, Methodology for the subjective assessment of the quality of television pictures
   Chen MX, 2020, IEEE J-STSP, V14, P89, DOI 10.1109/JSTSP.2019.2956408
   Chen SJ, 2018, IEEE INT CON MULTI
   Cho K., 2014, P 2014 C EMP METH NA, DOI 10.3115/v1/D14-1179
   Curcio Igor D. D., 2017, P SMPTE 2017 ANN TEC, P1
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding KY, 2022, IEEE T PATTERN ANAL, V44, P2567, DOI 10.1109/TPAMI.2020.3045810
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Duan HY, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351786
   Duh HBL, 2004, HUM FACTORS, V46, P142, DOI 10.1518/hfes.46.1.142.30384
   Fang YM, 2023, IEEE T IMAGE PROCESS, V32, P2693, DOI 10.1109/TIP.2023.3272480
   Fang YM, 2022, AAAI CONF ARTIF INTE, P580
   Fang YM, 2021, IEEE T CIRC SYST VID, V31, P3010, DOI 10.1109/TCSVT.2020.3035679
   Fang YM, 2019, IEEE T IMAGE PROCESS, V28, P5253, DOI 10.1109/TIP.2019.2916766
   Fang YM, 2018, IEEE T IMAGE PROCESS, V27, P1600, DOI 10.1109/TIP.2017.2781307
   Fröhlich P, 2012, INT WORK QUAL MULTIM, P242, DOI 10.1109/QoMEX.2012.6263851
   Golestaneh SA, 2022, IEEE WINT CONF APPL, P3989, DOI 10.1109/WACV51458.2022.00404
   Hancheng Zhu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14131, DOI 10.1109/CVPR42600.2020.01415
   Hasler D, 2003, P SOC PHOTO-OPT INS, V5007, P87, DOI 10.1117/12.477378
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang MK, 2018, IEEE T IMAGE PROCESS, V27, P6039, DOI 10.1109/TIP.2018.2865089
   Jiang H, 2021, IEEE T IMAGE PROCESS, V30, P2364, DOI 10.1109/TIP.2021.3052073
   Kim HG, 2020, IEEE T CIRC SYST VID, V30, P917, DOI 10.1109/TCSVT.2019.2898732
   Kingma D. P., 2014, arXiv
   Krasula Lukas, 2016, 2016 Eighth International Conference on Quality of Multimedia Experience (QoMEX), DOI 10.1109/QoMEX.2016.7498936
   Li DQ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P789, DOI 10.1145/3394171.3413804
   Li DQ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2351, DOI 10.1145/3343031.3351028
   Liu Y, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3549544
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Park J, 2013, IEEE T IMAGE PROCESS, V22, P610, DOI 10.1109/TIP.2012.2219551
   RECOMMENDATION ITU-T P, 1999, Subjective video quality assessment methods for multimedia applications
   Rossi S, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3381846
   Sendjasni A, 2021, IEEE IMAGE PROC, P3413, DOI 10.1109/ICIP42928.2021.9506192
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Sitzmann V, 2018, IEEE T VIS COMPUT GR, V24, P1633, DOI 10.1109/TVCG.2018.2793599
   Su SL, 2020, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR42600.2020.00372
   Sui XJ, 2023, PROC CVPR IEEE, P6989, DOI 10.1109/CVPR52729.2023.00675
   Sui XJ, 2022, IEEE T VIS COMPUT GR, V28, P3022, DOI 10.1109/TVCG.2021.3050888
   Sun W, 2020, IEEE J-STSP, V14, P64, DOI 10.1109/JSTSP.2019.2955024
   Sun W, 2017, IEEE IMAGE PROC, P3450, DOI 10.1109/ICIP.2017.8296923
   Sun YL, 2017, IEEE SIGNAL PROC LET, V24, P1408, DOI 10.1109/LSP.2017.2720693
   Upenik E, 2016, PICT COD SYMP
   Video Quality Experts Group, 2000, VQEG M OTT CAN MARCH
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Zhou, 2017, P SMPTE ANN TECHN C, P1
   Wu TH, 2023, Arxiv, DOI arXiv:2305.10983
   Xiao JX, 2012, PROC CVPR IEEE, P2695, DOI 10.1109/CVPR.2012.6247991
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xu JH, 2019, PICT COD SYMP, DOI 10.1109/pcs48520.2019.8954555
   Xu JH, 2021, IEEE T CIRC SYST VID, V31, P1724, DOI 10.1109/TCSVT.2020.3015186
   Xu JH, 2018, LECT NOTES COMPUT SC, V11164, P589, DOI 10.1007/978-3-030-00776-8_54
   Xu M, 2020, IEEE J-STSP, V14, P5, DOI 10.1109/JSTSP.2020.2966864
   Yan JB, 2022, IEEE T IMAGE PROCESS, V31, P3896, DOI 10.1109/TIP.2022.3177127
   Yan JB, 2021, INT J COMPUT VISION, V129, P1768, DOI 10.1007/s11263-021-01450-2
   Yan JB, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102888
   Yan JB, 2020, IEEE T IMAGE PROCESS, V29, P7443, DOI 10.1109/TIP.2020.3003218
   Yang L, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, DOI 10.1145/3503161.3547748
   Yang Li, 2021, P IEEE INT C MULT EX, P1
   Ye Yan, 2017, JVET-G1003: Algorithm description of projection format conversion and video quality metrics in 360lib version 4
   Yu M, 2015, 2015 IEEE International Symposium on Mixed and Augmented Reality, P31, DOI 10.1109/ISMAR.2015.12
   Zakharchenko V, 2016, PROC SPIE, V9970, DOI 10.1117/12.2235885
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang WX, 2021, IEEE T IMAGE PROCESS, V30, P3474, DOI 10.1109/TIP.2021.3061932
   Zhang WX, 2020, IEEE T CIRC SYST VID, V30, P36, DOI 10.1109/TCSVT.2018.2886771
   Zheng XL, 2020, IEEE ACCESS, V8, P31647, DOI 10.1109/ACCESS.2020.2972158
   Zhou W, 2022, IEEE T CIRC SYST VID, V32, P1778, DOI 10.1109/TCSVT.2021.3081182
   Zhou Y, 2022, IEEE T CIRC SYST VID, V32, P1767, DOI 10.1109/TCSVT.2021.3081162
   Zhou YF, 2018, INT CONF SIGN PROCES, P54, DOI 10.1109/ICSP.2018.8652269
NR 71
TC 0
Z9 0
U1 3
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2024
VL 20
IS 6
SI SI
AR 175
DI 10.1145/3640344
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OQ2T3
UT WOS:001208681800025
DA 2024-08-05
ER

PT J
AU Yang, SW
   Shen, LH
   Shuai, HH
   Feng, KT
AF Yang, Shih-Wei
   Shen, Li-Hsiang
   Shuai, Hong-Han
   Feng, Kai-Ten
TI CMAF: Cross-modal Augmentation via Fusion for Underwater Acoustic Image
   Recognition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Neural networks; sonar image; multi-modal fusion; class imbalance
ID TARGET STRENGTH; FISH
AB Underwater image recognition is crucial for underwater detection applications. Fish classification has been one of the emerging research areas in recent years. Existing image classification models usually classify data collected from terrestrial environments. However, existing image classification models trained with terrestrial data are unsuitable for underwater images, as identifying underwater data is challenging due to their incomplete and noisy features. To address this, we propose a cross-modal augmentation via fusion (CMAF) framework for acoustic-based fish image classification. Our approach involves separating the process into two branches: visual modality and sonar signal modality, where the latter provides a complementary character feature. We augment the visual modality, design an attention-based fusion module, and adopt a masking-based training strategy with a mask-based focal loss to improve the learning of local features and address the class imbalance problem. Our proposed method outperforms the state-of-the-art methods. Our source code is available at https://github.com/WilkinsYang/CMAF
C1 [Yang, Shih-Wei; Feng, Kai-Ten] Natl Yang Ming Chiao Tung Univ, Dept Elect & Elect Engn, 1001 Univ Rd, Hsinchu 300093, Taiwan.
   [Shen, Li-Hsiang] Natl Cent Univ, Dept Commun Engn, Taoyuan 32001, Taiwan.
C3 National Yang Ming Chiao Tung University; National Central University
RP Yang, SW; Feng, KT (corresponding author), Natl Yang Ming Chiao Tung Univ, Dept Elect & Elect Engn, 1001 Univ Rd, Hsinchu 300093, Taiwan.
EM wilkins503261.ee10@nycu.edu.tw; gp3xu4vu6@gmail.com;
   hhshuai@nycu.edu.tw; ktfeng@nycu.edu.tw
OI Shuai, Hong-Han/0000-0003-2216-077X; Shen, Li-Hsiang/0000-0002-6412-5457
FU National Science and Technology Council (NSTC) [112-2221-E-A49-059-MY3,
   110-2221-E-A49-041-MY3, 112-2218-E-A49-020, 112-2218-EA49-023,
   112-2917-I-564-014, 112UC2N006, 112UA10019]; National Defense Science
   and Technology Academic Collaborative Research Project; Higher Education
   Sprout Project of the National Yang Ming Chiao Tung University; Ministry
   of Education (MoE); Co-creation Platform of the Industry-Academia
   Innovation School, NYCU; National Key Fields Industry-University
   Cooperation and Skilled Personnel Training Act; MOE; Hon Hai Research
   Institute, Taipei, Taiwan
FX This work was supported in part by the National Science and Technology
   Council (NSTC) under Grants No. 112-2221-E-A49-059-MY3, No.
   110-2221-E-A49-041-MY3, No. 112-2218-E-A49-020, No. 112-2218-EA49-023,
   No. 112-2917-I-564-014, No. 112UC2N006, and No. 112UA10019; in part by
   STEM Project; in part by the National Defense Science and Technology
   Academic Collaborative Research Project; in part by the Higher Education
   Sprout Project of the National Yang Ming Chiao Tung University (NYCU)
   and Ministry of Education (MoE); in part by the Co-creation Platform of
   the Industry-Academia Innovation School, NYCU, under the framework of
   the National Key Fields Industry-University Cooperation and Skilled
   Personnel Training Act, from the MOE and industry partners in Taiwan;
   and in part by the Hon Hai Research Institute, Taipei, Taiwan.
CR Akkaynak D, 2017, PROC CVPR IEEE, P568, DOI 10.1109/CVPR.2017.68
   Bovcon B, 2022, IEEE T CYBERNETICS, V52, P12661, DOI 10.1109/TCYB.2021.3085856
   Buda M, 2018, NEURAL NETWORKS, V106, P249, DOI 10.1016/j.neunet.2018.07.011
   Cai PD, 2020, IEEE ROBOT AUTOM LET, V5, P4218, DOI 10.1109/LRA.2020.2994027
   Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953
   Chiamanusorn C, 2017, PROCEEDINGS OF THE 2017 INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY (ICIT 2017), P341, DOI 10.1145/3176653.3176671
   Dosovitskiy A., 2021, ICLR
   DUSHAW BD, 1993, J ACOUST SOC AM, V93, P255, DOI 10.1121/1.405660
   Frouzova J, 2005, FISH RES, V75, P86, DOI 10.1016/j.fishres.2005.04.011
   Goodman J., 2022, ACM/IMS Trans. Data Sci., V2, P4
   Guo M, 2023, NAT PROD RES, V37, P1411, DOI 10.1080/14786419.2021.2011271
   Guo WZ, 2019, IEEE ACCESS, V7, P63373, DOI 10.1109/ACCESS.2019.2916887
   Ha J., 2016, P ACM INT C UBIQUITO
   Han ZB, 2022, PROC CVPR IEEE, P20675, DOI 10.1109/CVPR52688.2022.02005
   Hartman KJ, 2005, T AM FISH SOC, V134, P375, DOI 10.1577/T04-052.1
   Hazen EL, 2003, ICES J MAR SCI, V60, P555, DOI 10.1016/S1054-3139(03)00053-5
   Hu Xinghang, 2022, HCMA '22: Proceedings of the 3rd International Workshop on Human-Centric Multimedia Analysis, P25, DOI 10.1145/3552458.3556449
   Huang C, 2016, PROC CVPR IEEE, P5375, DOI 10.1109/CVPR.2016.580
   Jian MW, 2021, SIGNAL PROCESS-IMAGE, V91, DOI 10.1016/j.image.2020.116088
   Jiang YF, 2021, IEEE GEOSCI REMOTE S, V18, P1505, DOI 10.1109/LGRS.2020.3005679
   Jin LL, 2019, IEEE ACCESS, V7, P125522, DOI 10.1109/ACCESS.2019.2939005
   Jose J. A., 2021, P IEEE INT C COMMUNI, V1, P1
   Kaya V, 2023, J AGR SCI-TARIM BILI, V29, P298, DOI 10.15832/ankutbd.1031130
   Krawczyk B, 2021, IEEE IJCNN, DOI 10.1109/IJCNN52387.2021.9533379
   Krawczyk B, 2020, IEEE T NEUR NET LEAR, V31, P2818, DOI 10.1109/TNNLS.2019.2913673
   Lee J. T., 2021, P INT C LEARNING REP
   Lee-Thorp J, 2022, Arxiv, DOI [arXiv:2105.03824, 10.48550/arXiv.2105.03824, DOI 10.48550/ARXIV.2105.03824]
   Li CY, 2020, IEEE T IMAGE PROCESS, V29, P4376, DOI 10.1109/TIP.2019.2955241
   Lilja J, 2000, AQUAT LIVING RESOUR, V13, P355, DOI 10.1016/S0990-7440(00)01072-X
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Liu B, 2020, IEEE T AERO ELEC SYS, V56, P4899, DOI 10.1109/TAES.2020.3003703
   Liu CL, 2020, IEEE T KNOWL DATA EN, V32, P1543, DOI 10.1109/TKDE.2019.2905559
   Liu Hong, 2021, ADV NEURAL INFORM PR, V34
   Liu RS, 2020, IEEE T CIRC SYST VID, V30, P4861, DOI 10.1109/TCSVT.2019.2963772
   Long X, 2018, AAAI CONF ARTIF INTE, P7202
   LOVE RH, 1977, J ACOUST SOC AM, V62, P1397, DOI 10.1121/1.381672
   Mahmood A, 2016, IEEE IMAGE PROC, P519, DOI 10.1109/ICIP.2016.7532411
   McCann E, 2018, SCI DATA, V5, DOI 10.1038/sdata.2018.190
   Mittal S, 2023, IEEE T NEUR NET LEAR, V34, P6968, DOI 10.1109/TNNLS.2022.3143887
   Mohammed R, 2020, INT CONF INFORM COMM, P243, DOI 10.1109/ICICS49469.2020.239556
   Morozs N, 2020, IEEE ACCESS, V8, P136151, DOI 10.1109/ACCESS.2020.3011620
   Moursund RA, 2003, ICES J MAR SCI, V60, P678, DOI 10.1016/S1054-3139(03)00036-5
   Nobis F, 2019, 2019 SYMPOSIUM ON SENSOR DATA FUSION: TRENDS, SOLUTIONS, APPLICATIONS (SDF 2019), DOI 10.1109/sdf.2019.8916629
   Owens A, 2018, LECT NOTES COMPUT SC, V11210, P639, DOI 10.1007/978-3-030-01231-1_39
   Piergiovanni AJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15415, DOI 10.1109/ICCV48922.2021.01515
   Polap D, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3147367
   Popel MH, 2018, INT CONF COMPUT INFO
   Praveen RG, 2022, IEEE COMPUT SOC CONF, P2485, DOI 10.1109/CVPRW56347.2022.00278
   Rusmadi R, 2020, IOP C SER EARTH ENV, V540, DOI 10.1088/1755-1315/540/1/012087
   Saripuddin M., 2022, P ACM INT C MACHINE, P151
   Seiffert C, 2008, PROC INT C TOOLS ART, P445, DOI 10.1109/ICTAI.2008.59
   Sharma P, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3511021
   Shvetsova N, 2022, PROC CVPR IEEE, P19988, DOI 10.1109/CVPR52688.2022.01939
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   SiYuan Jiang, 2021, 2021 OES China Ocean Acoustics (COA), P1016, DOI 10.1109/COA50123.2021.9519941
   Stojanovic M., 2007, SIGMOBILE Mob. Comput. Commun. Rev., V11, P34, DOI 10.1145/1347364.1347373
   Terayama K, 2019, AQUACULT ENG, V86, DOI 10.1016/j.aquaeng.2019.102000
   THORP WH, 1967, J ACOUST SOC AM, V42, P270, DOI 10.1121/1.1910566
   Tolstikhin I, 2021, ADV NEUR IN, V34
   Triguero I, 2015, IEEE C EVOL COMPUTAT, P715, DOI 10.1109/CEC.2015.7256961
   Tripathi MK, 2019, 2019 2ND INTERNATIONAL CONFERENCE ON INTELLIGENT COMMUNICATION AND COMPUTATIONAL TECHNIQUES (ICCT), P70, DOI [10.1109/icct46177.2019.8969015, 10.1109/ICCT46177.2019.8969015]
   Urick R., 1989, Ambient Noise in the Sea
   Urick R. J, 1983, Principles of Underwater Sound
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang CY, 2023, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR52729.2023.00721
   Wang W., 2020, P IEEE CVF C COMP VI, P12695, DOI DOI 10.1109/CVPR42600.2020.01271
   Wang Y, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3489520
   Wu JJ, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3506708
   Wu JJ, 2022, SIGNAL PROCESS-IMAGE, V109, DOI 10.1016/j.image.2022.116855
   Xie T., 2021, arXiv
   Xie YF, 2021, INT PSYCHOGERIATR, V33, P157, DOI [10.1017/S1041610220001398, 10.1109/TKDE.2020.2985965]
   Xu HM, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3893, DOI 10.1145/3394171.3413581
   Xu HP, 2021, 2021 4TH INTERNATIONAL CONFERENCE ON INTELLIGENT AUTONOMOUS SYSTEMS (ICOIAS 2021), P44, DOI 10.1109/ICoIAS53694.2021.00016
   Yang Yang, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P1612, DOI 10.1145/3503161.3548203
   Yu JS, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P6241, DOI 10.1145/3503161.3547869
   Yu SH, 2022, PROC CVPR IEEE, P70, DOI 10.1109/CVPR52688.2022.00017
   Yu Y, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3281746
   Zhang W, 2022, IEEE COMPUT SOC CONF, P2427, DOI 10.1109/CVPRW56347.2022.00271
   Zhao H., 2018, P EUROPEAN C COMPUTE
   Zhou JC, 2022, APPL INTELL, V52, P16435, DOI 10.1007/s10489-022-03275-z
   Zhou T, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3181417
NR 81
TC 0
Z9 0
U1 20
U2 20
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2024
VL 20
IS 5
AR 124
DI 10.1145/3636427
PG 25
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MF3R9
UT WOS:001192177900004
DA 2024-08-05
ER

PT J
AU Zhang, YB
   Lin, WG
   Xu, JF
AF Zhang, Yibo
   Lin, Weiguo
   Xu, Junfeng
TI Joint Audio-Visual Attention with Contrastive Learning for More General
   Deepfake Detection
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Deepfake detection; audio-visual attention; contrastive learning
AB With the continuous advancement of deepfake technology, there has been a surge in the creation of realistic fake videos. Unfortunately, the malicious utilization of deepfake poses a significant threat to societal morality and political security. Therefore, numerous researchers have proposed various deepfake detection methods. However, traditional deepfake approaches tend to focus on specific forgery features, such as artifacts or inconsistent actions, which can be vulnerable to specialized countermeasures. Recent studies show an intrinsic correlation between facial and audio cues, which can be exploited for deepfake detection. To address these challenges and enhance the robustness and generalization of deepfake detection algorithms, we propose a novel joint audio-visual deepfake detection model named AVA-CL, which is capable of detecting deepfakes in both audio and visual domains. Furthermore, exploiting the inherent correlation and consistency between audio and visual enhances the effectiveness of deepfake detection significantly. Through extensive experiments, we demonstrate that our proposed AVA-CL model outperforms many state-of-the-art (SOTA) methods with superior robustness and generalization capabilities. This research presents a promising approach for deepfake detection and reducing the harm caused by malicious use.
C1 [Zhang, Yibo; Lin, Weiguo; Xu, Junfeng] Commun Univ China, Beijing, Peoples R China.
   [Zhang, Yibo; Lin, Weiguo; Xu, Junfeng] Commun Univ China, Beijing 100024, Peoples R China.
C3 Communication University of China; Communication University of China
RP Xu, JF (corresponding author), Commun Univ China, Beijing 100024, Peoples R China.
EM ybzhang@cuc.edu.cn; Linwei@cuc.edu.cn; junfeng@cuc.edu.cn
OI Lin, Weiguo/0000-0001-9425-8560
FU National Key Research and Development Program [2022YFF0902401]
FX This work was supported by National Key Research and Development Program
   (2022YFF0902401).
CR Afchar D, 2018, IEEE INT WORKS INFOR
   Agarwal S, 2020, IEEE COMPUT SOC CONF, P2814, DOI 10.1109/CVPRW50498.2020.00338
   Cai ZX, 2022, Arxiv, DOI [arXiv:2211.06627, 10.48550/arXiv.2211.06627]
   Cai Zhixi, 2022, 2022 INT C DIG IM CO, P1, DOI [DOI 10.1109/DICTA56598.2022.10034605, 10.1109/DICTA56598.2022.10034605]
   Chan C, 2019, IEEE I CONF COMP VIS, P5932, DOI 10.1109/ICCV.2019.00603
   Cheng HRY, 2022, Arxiv, DOI arXiv:2203.02195
   Chugh K, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P439, DOI 10.1145/3394171.3413700
   Cozzolino D, 2022, Arxiv, DOI arXiv:2204.03083
   Dolhansky B, 2020, Arxiv, DOI arXiv:2006.07397
   Nguyen DK, 2018, PROC CVPR IEEE, P6087, DOI 10.1109/CVPR.2018.00637
   Esser P, 2021, PROC CVPR IEEE, P12868, DOI 10.1109/CVPR46437.2021.01268
   Feng C, 2023, PROC CVPR IEEE, P10491, DOI 10.1109/CVPR52729.2023.01011
   Grill J.B., 2020, P 34 INT C NEUR INF, V33, P21271, DOI [10.48550/arXiv.2006.07733, DOI 10.48550/ARXIV.2006.07733]
   Haliassos A, 2022, PROC CVPR IEEE, P14930, DOI 10.1109/CVPR52688.2022.01453
   Haliassos A, 2021, PROC CVPR IEEE, P5037, DOI 10.1109/CVPR46437.2021.00500
   Haq Abdulloh Salahul, 2020, Proceeding of the Electrical Engineering Computer Science and Informatics, V7, P78
   Hong FT, 2022, PROC CVPR IEEE, P3387, DOI 10.1109/CVPR52688.2022.00339
   Ilyas H, 2023, APPL SOFT COMPUT, V136, DOI 10.1016/j.asoc.2023.110124
   Khalid H, 2022, Arxiv, DOI arXiv:2108.05080
   Kingma D. P., 2014, arXiv
   Korshunov P, 2018, Arxiv, DOI arXiv:1812.08685
   Li G, 2023, IEEE T INF FOREN SEC, V18, P1095, DOI 10.1109/TIFS.2023.3235579
   Li LZ, 2020, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR42600.2020.00505
   Li YZ, 2019, Arxiv, DOI arXiv:1811.00656
   Li YZ, 2018, IEEE INT WORKS INFOR
   Mittal T, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2823, DOI 10.1145/3394171.3413570
   Morgado P, 2021, PROC CVPR IEEE, P12470, DOI 10.1109/CVPR46437.2021.01229
   Muda L, 2010, Arxiv, DOI [arXiv:1003.4083, 10.48550/arXiv.1003.4083]
   Nguyen HH, 2019, INT CONF ACOUST SPEE, P2307, DOI 10.1109/ICASSP.2019.8682602
   Prajwal KR, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P484, DOI 10.1145/3394171.3413532
   Rössler A, 2019, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2019.00009
   Sanderson C, 2009, LECT NOTES COMPUT SC, V5558, P199, DOI 10.1007/978-3-642-01793-3_21
   Shahzad SA, 2022, ASIAPAC SIGN INFO PR, P1885, DOI 10.23919/APSIPAASC55919.2022.9980296
   Shao R, 2023, PROC CVPR IEEE, P6904, DOI 10.1109/CVPR52729.2023.00667
   Shiohara K, 2022, PROC CVPR IEEE, P18699, DOI 10.1109/CVPR52688.2022.01816
   Siarohin A, 2020, Arxiv, DOI arXiv:2003.00196
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Soares BS, 2022, EXPERT SYST APPL, V201, DOI 10.1016/j.eswa.2022.117104
   Suwajanakorn S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073640
   Tan MX, 2019, PR MACH LEARN RES, V97
   van den Oord A, 2019, Arxiv, DOI [arXiv:1807.03748, DOI 10.48550/ARXIV.1807.03748]
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang J, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3779, DOI 10.1145/3474085.3478324
   Wang JK, 2022, PROCEEDINGS OF THE 2022 INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, ICMR 2022, P615, DOI 10.1145/3512527.3531415
   Wolf L, 2011, PROC CVPR IEEE, P529, DOI 10.1109/CVPR.2011.5995566
   Wu XT, 2021, IEEE T IMAGE PROCESS, V30, P8658, DOI 10.1109/TIP.2021.3112059
   Yang S., 2021, ADV NEUR IN, V34
   Yang WY, 2023, IEEE T INF FOREN SEC, V18, P2015, DOI 10.1109/TIFS.2023.3262148
   Yang X, 2019, INT CONF ACOUST SPEE, P8261, DOI 10.1109/ICASSP.2019.8683164
   Yuyang Qian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P86, DOI 10.1007/978-3-030-58610-2_6
   Zhao TC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15003, DOI 10.1109/ICCV48922.2021.01475
   Zheng YL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15024, DOI 10.1109/ICCV48922.2021.01477
   Zhou YP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14780, DOI 10.1109/ICCV48922.2021.01453
   Zhu BQ, 2022, Arxiv, DOI arXiv:2204.14057
NR 55
TC 0
Z9 0
U1 18
U2 18
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2024
VL 20
IS 5
AR 137
DI 10.1145/3625100
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MF3R9
UT WOS:001192177900017
OA Bronze
DA 2024-08-05
ER

PT J
AU Djenouri, Y
   Belhadi, A
   Srivastava, G
   Lin, JCW
AF Djenouri, Youcef
   Belhadi, Asma
   Srivastava, Gautam
   Lin, Jerry Chun-Wei
TI An Efficient and Accurate GPU-based Deep Learning Model for Multimedia
   Recommendation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Human computer interaction; XAI; deep learning; GPU; pattern
   recommendation; multimedia data
AB This article proposes the use of deep learning in human-computer interaction and presents a new explainable hybrid framework for recommending relevant hashtags on a set of orpheline tweets, which are tweets with hashtags. The approach starts by determining the set of batches used in the convolution neural network based on frequent pattern mining solutions. The convolutional neural network is then applied to the set of batches of tweets to learn the hashtags of the tweets. An optimization strategy has been proposed to accurately perform the learning process by reducing the number of frequent patterns. Moreover, eXplainable AI is introduced for hashtag recommendations by analyzing the user preferences and understanding the different weights of the deep learning model used in the learning process. This is performed by learning the hyper-parameters of the deep architecture using the genetic algorithm. GPU computing is also investigated to achieve high speed and enable the execution of the overall framework in real time. Extensive experimental analysis has been performed to show that our methodology is useful on different collections of tweets. The experimental results clearly show the efficiency of our proposed approach compared to baseline approaches in terms of both runtime and accuracy. Thus, the proposed solution achieves an accuracy of 90% when analyzing complex Wikipedia data while the other algorithms did not achieve 85% when processing the same amount of data.
C1 [Djenouri, Youcef] SINTEF, SINTEF Digital, Forskningsveien 1, N-0373 Oslo, Norway.
   [Belhadi, Asma] Prinsens Gate 7-9, N-0107 Oslo, Norway.
   [Srivastava, Gautam] Brandon Univ, Dept Math & Comp Sci, 270 18th St, Brandon, MB R7A 6A9, Canada.
   [Srivastava, Gautam] China Med Univ, Res Ctr Interneural Comp, 91 Xueshi Rd, Taichung 40402, Taiwan.
   [Lin, Jerry Chun-Wei] Western Norway Univ Appl Sci, Dept Comp Sci Elect Engn & Math Sci, Inndalsveien 28, N-5063 Bergen, Norway.
C3 SINTEF; Brandon University; China Medical University Taiwan; Western
   Norway University of Applied Sciences
RP Lin, JCW (corresponding author), Western Norway Univ Appl Sci, Dept Comp Sci Elect Engn & Math Sci, Inndalsveien 28, N-5063 Bergen, Norway.
EM Youcef.Djenouri@sintef.no; asma.belhadi@kristiania.no;
   SRIVASTAVAG@brandonu.ca; jerrylin@ieee.org
RI Srivastava, Gautam/N-5668-2019
OI Srivastava, Gautam/0000-0001-9851-4103
CR Agrawal R., 1993, SIGMOD Record, V22, P207, DOI 10.1145/170036.170072
   Ahmad K, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3199668
   Belhadi A, 2021, J SUPERCOMPUT, V77, P7253, DOI 10.1007/s11227-020-03582-7
   Belhadi A, 2020, IEEE ACCESS, V8, P79182, DOI 10.1109/ACCESS.2020.2990799
   Belhadi A, 2020, IEEE ACCESS, V8, P10569, DOI 10.1109/ACCESS.2020.2964682
   Brin S., 1997, SIGMOD Record, V26, P265, DOI [10.1145/253262.253325, 10.1145/253262.253327]
   Cao D, 2020, KNOWL-BASED SYST, V203, DOI 10.1016/j.knosys.2020.106114
   Chan GYY, 2019, IEEE T VIS COMPUT GR, V25, P321, DOI 10.1109/TVCG.2018.2864826
   Cheng W, 2021, ACM T STORAGE, V17, DOI 10.1145/3404190
   Djenouri Y, 2021, ACM T KNOWL DISCOV D, V15, DOI 10.1145/3425867
   Djenouri Y, 2017, INFORM SCIENCES, V420, P1, DOI 10.1016/j.ins.2017.08.043
   Djenouri Y, 2017, LECT NOTES ARTIF INT, V10235, P644, DOI 10.1007/978-3-319-57529-2_50
   Du GY, 2017, ICEIS: PROCEEDINGS OF THE 19TH INTERNATIONAL CONFERENCE ON ENTERPRISE INFORMATION SYSTEMS - VOL 1, P201, DOI 10.5220/0006274102010208
   Fiok K, 2021, EXPERT SYST APPL, V186, DOI 10.1016/j.eswa.2021.115771
   Godin F, 2013, PROCEEDINGS OF THE 22ND INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'13 COMPANION), P593
   Gong YY, 2018, NEUROCOMPUTING, V272, P170, DOI 10.1016/j.neucom.2017.06.056
   Gouda K, 2001, 2001 IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P163, DOI 10.1109/ICDM.2001.989514
   Gurcan F, 2021, INT J HUM-COMPUT INT, V37, P267, DOI 10.1080/10447318.2020.1819668
   Hosseini S, 2020, SOFT COMPUT, V24, P351, DOI 10.1007/s00500-019-03914-7
   Ivanovs M, 2021, PATTERN RECOGN LETT, V150, P228, DOI 10.1016/j.patrec.2021.06.030
   Ke Wang, 2002, Advances in Knowledge Discovery and Data Mining. 6th Pacific-Asia Conference, PAKDD 2002. Proceedings (Lecture Notes in Artificial Intelligence Vol.2336), P334
   Kim Y, 2022, SUSTAIN CITIES SOC, V79, DOI 10.1016/j.scs.2022.103677
   Kou FF, 2018, J COMPUT SCI TECH-CH, V33, P711, DOI 10.1007/s11390-018-1851-2
   Kumar N, 2021, KNOWL INF SYST, V63, P175, DOI 10.1007/s10115-020-01515-7
   Lei K, 2020, NEUROCOMPUTING, V391, P65, DOI 10.1016/j.neucom.2020.01.091
   Li MM, 2019, PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM '19), P509, DOI 10.1145/3357384.3357912
   Li Y, 2017, ACM T INTEL SYST TEC, V8, DOI 10.1145/2932192
   Li Y, 2017, CONCURR COMP-PRACT E, V29, DOI 10.1002/cpe.3904
   Lin JCW, 2019, IEEE INT CONF BIG DA, P2674, DOI 10.1109/BigData47090.2019.9005996
   Lin JCW, 2019, SOFT COMPUT, V23, P12779, DOI 10.1007/s00500-019-03829-3
   Lin YS, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3469288
   Liu H, 2021, NEUROCOMPUTING, V433, P310, DOI 10.1016/j.neucom.2020.09.068
   Liu J, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3456
   Liu YN, 2020, COMPLEXITY, V2020, DOI 10.1155/2020/9789431
   Ma RF, 2021, IEEE T KNOWL DATA EN, V33, P388, DOI 10.1109/TKDE.2019.2932406
   Makki R, 2018, ACM T KNOWL DISCOV D, V12, DOI 10.1145/3047010
   Ota K, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3092831
   Ramon Yanou, 2021, SSRN Electr. J.
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Sedhai S, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P831, DOI 10.1145/2600428.2609452
   Shi BC, 2018, IEEE T KNOWL DATA EN, V30, P43, DOI 10.1109/TKDE.2017.2754253
   Shi BC, 2016, PROCEEDINGS OF THE 25TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'16), P1191, DOI 10.1145/2872427.2882982
   Shorfuzzaman M, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3469841
   Tang XY, 2020, APPL INTELL, V50, P2449, DOI 10.1007/s10489-020-01635-1
   Wei YW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1446, DOI 10.1145/3343031.3350858
   Wu Y, 2018, NEUROCOMPUTING, V314, P479, DOI 10.1016/j.neucom.2018.07.011
   Yang Q, 2020, IEEE T COMPUT SOC SY, V7, P768, DOI 10.1109/TCSS.2020.2986778
   Zhang BB, 2018, APPL SCI-BASEL, V8, DOI 10.3390/app8050769
   Zhang SY, 2018, LECT NOTES COMPUT SC, V10827, P83, DOI 10.1007/978-3-319-91452-7_6
   Zhao F, 2016, FUTURE GENER COMP SY, V65, P196, DOI 10.1016/j.future.2015.10.012
NR 50
TC 1
Z9 1
U1 6
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2024
VL 20
IS 2
SI SI
AR 40
DI 10.1145/3524022
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W6GR4
UT WOS:001092595800010
DA 2024-08-05
ER

PT J
AU Qayyum, A
   Razzak, I
   Tanveer, M
   Mazher, M
AF Qayyum, Abdul
   Razzak, Imran
   Tanveer, M.
   Mazher, Moona
TI Spontaneous Facial Behavior Analysis Using Deep Transformer-based
   Framework for Child-computer Interaction
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Datasets; neural networks; gaze detection; text tagging
ID RECOGNITION; EMOTION
AB A fascinating challenge in robotics-human interaction is imitating the emotion recognition capability of humans to robots with the aim to make human-robotics interaction natural, genuine and intuitive. To achieve the natural interaction in affective robots, human-machine interfaces, and autonomous vehicles, understanding our attitudes and opinions is very important, and it provides a practical and feasible path to realize the connection between machine and human. Multimodal interface that includes voice along with facial expression can manifest a large range of nuanced emotions compared to purely textual interfaces and provide a great value to improve the intelligence level of effective communication. Interfaces that fail to manifest or ignore user emotions may significantly impact the performance and risk being perceived as cold, socially inept, untrustworthy, and incompetent. To equip a child well for life, we need to help our children identify their feelings, manage them well, and express their needs in healthy, respectful, and direct ways. Early identification of emotional deficits can help to prevent low social functioning in children. In this work, we analyzed the child's spontaneous behavior using multimodal facial expression and voice signal presenting multimodal transformer-based last feature fusion for facial behavior analysis in children to extract contextualized representations from RGB video sequence and Hematoxylin and eosin video sequence and then using these representations followed by pairwise concatenations of contextualized representations using cross-feature fusion technique to predict users emotions. To validate the performance of the proposed framework, we have performed experiments with the different pairwise concatenations of contextualized representations that showed significantly better performance than state-of-the-art method. Besides, we perform t-distributed stochastic neighbor embedding visualization to visualize the discriminative feature in lower dimension space and probability density estimation to visualize the prediction capability of our proposed model.
C1 [Qayyum, Abdul] Univ Bourgogne, Dept Comp Sci & Engn, POBox 1212, F-21078 Dijon, France.
   [Razzak, Imran] Univ New South Wales, Sch Comp Sci & Engn, 1 Thorvald Circle, Sydney, NSW 1466, Australia.
   [Tanveer, M.] Indian Inst Technol Indore, Dept Math, Indore 453552, India.
   [Mazher, Moona] Univ Rovira & Virgili, Dept Comp Engn & Math, Tarragona, Spain.
C3 Universite de Bourgogne; University of New South Wales Sydney; Indian
   Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Indore; Universitat Rovira i Virgili
RP Qayyum, A (corresponding author), Univ Bourgogne, Dept Comp Sci & Engn, POBox 1212, F-21078 Dijon, France.
EM qayyum@enib.fr; imran.razzak@unsw.edu.au; mtanveer@iiti.ac.in;
   moona.mazher@gmail.com
RI Razzak, Imran/AEW-5139-2022
OI Razzak, Imran/0000-0002-3930-6600
CR Albraikan Amani, 2018, 2018 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops), P836, DOI 10.1109/PERCOMW.2018.8480090
   Albraikan A, 2019, IEEE SENS J, V19, P8402, DOI 10.1109/JSEN.2018.2867221
   Albraikan A, 2018, IEEE ACCESS, V6, P78780, DOI 10.1109/ACCESS.2018.2885279
   Beale R, 2008, LECT NOTES COMPUT SC, V4868, P1, DOI 10.1007/978-3-540-85099-1_1
   Demisse GG, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3176649
   Gu H, 2019, NEUROSCI LETT, V703, P198, DOI 10.1016/j.neulet.2019.01.032
   Jiabei Zeng, 2018, Computer Vision - ECCV 2018. 15th European Conference. Proceedings: Lecture Notes in Computer Science (LNCS 11217), P227, DOI 10.1007/978-3-030-01261-8_14
   Khan RA, 2019, IMAGE VISION COMPUT, V83-84, P61, DOI 10.1016/j.imavis.2019.02.004
   Kim Y, 2015, ACM T MULTIM COMPUT, V12, DOI 10.1145/2808204
   Kucirkova Natalia, 2020, Int J Child Comput Interact, V26, P100203, DOI 10.1016/j.ijcci.2020.100203
   Liu SG, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3464382
   Mayor-Torres JM, 2022, Arxiv, DOI arXiv:2111.13208
   Mehdizadehfar V, 2020, BIOMED SIGNAL PROCES, V56, DOI 10.1016/j.bspc.2019.101721
   Mehmood RM, 2016, COMPUT ELECTR ENG, V53, P444, DOI 10.1016/j.compeleceng.2016.04.009
   Miao Y, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3311747
   Qayyum A, 2022, IMAGE VISION COMPUT, V119, DOI 10.1016/j.imavis.2022.104375
   Shan CF, 2009, IMAGE VISION COMPUT, V27, P803, DOI 10.1016/j.imavis.2008.08.005
   Voeffray S., 2011, Emot. Recog., P1
   Wang K, 2020, IEEE T IMAGE PROCESS, V29, P4057, DOI 10.1109/TIP.2019.2956143
   Wang XP, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3355397
   Yang HF, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3152118
   Yao AB, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P472, DOI 10.1145/2993148.2997639
   Yin GH, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3490686
   Zhang W, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3292059
   Zhang Zhaoxin, 2020, ACM Trans. Multim. Comput., Commun. Applic., V16, P1
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
   Zhi RC, 2011, IEEE T SYST MAN CY B, V41, P38, DOI 10.1109/TSMCB.2010.2044788
   Zhong L, 2012, PROC CVPR IEEE, P2562, DOI 10.1109/CVPR.2012.6247974
NR 28
TC 0
Z9 0
U1 7
U2 22
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2024
VL 20
IS 2
SI SI
AR 43
DI 10.1145/3539577
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W6GR4
UT WOS:001092595800013
DA 2024-08-05
ER

PT J
AU Gao, J
   Li, P
   Laghari, AA
   Srivastava, G
   Gadekallu, TR
   Abbas, S
   Zhang, JN
AF Gao, Jing
   Li, Peng
   Laghari, Asif Ali
   Srivastava, Gautam
   Gadekallu, Thippa Reddy
   Abbas, Sidra
   Zhang, Jianing
TI Incomplete Multiview Clustering via Semidiscrete Optimal Transport for
   Multimedia Data Mining in IoT
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Incomplete multiview data; data integrity; internet of things; optimal
   transport
AB With the wide deployment of the Internet of Things (IoT), large volumes of incomplete multiview data that violates data integrity is generated by various applications, which inevitably produces negative impacts on the quality of service of IoT systems. Incomplete multiview clustering (IMC), as an essential technique of data processing, has the potential for mining patterns of incomplete IoT data. However, previous methods utilize notion-strong distances that can only measure differences between distributions at the overlap of data manifolds in fusing complementary information of data for pattern mining. They may suffer from biased estimation and information loss in capturing intrinsic structures of incomplete multiview data. To address these challenges, a semidiscrete multiview optimal transport (SD-MOT) is defined for IMC, which utilizes distances with weak notions to capture intrinsic structures of incomplete multiview data. Specifically, IMC is recast as an equivalent optimal transport between continuous incomplete multiview data and discrete clustering centroids, to avoid the strict assumption on overlap between manifolds in pattern mining. Then, SD-MOT is instantiated as a deep incomplete contrastive clustering network to remedy biased estimation and information loss on intrinsic structures of incompletemultiviewdata. Afterwards, a variational solution to SD-MOT is derived to effectively train the network parameters for pattern mining. Finally, extensive experiments on four representative incomplete multiview datasets verify the superiority of SD-MOT in comparison with nine baseline methods.
C1 [Gao, Jing; Li, Peng; Zhang, Jianing] Dalian Univ Technol, Sch Software Technol, 321 Tuqiang St, Dalian 116620, Peoples R China.
   [Laghari, Asif Ali] Shenyang Normal Univ, Software Coll, 253 Huanghebei St, Shenyang 116620, Liaoning, Peoples R China.
   [Srivastava, Gautam] Brandon Univ, Dept Math & Comp Sci, 270-18th St, Brandon, MB R7A 6A9, Canada.
   [Srivastava, Gautam] Lebanese Amer Univ, Dept Comp Sci & Math, VFVH 64, Beirut 135053, Lebanon.
   [Srivastava, Gautam] China Med Univ, Res Ctr Interneural Comp, 2 Yude Rd, Taichung 40402, Taiwan.
   [Gadekallu, Thippa Reddy] Zhongda Grp, Zhongda Sq, Jiaxing 314312, Zhejiang, Peoples R China.
   [Gadekallu, Thippa Reddy] Lebanese Amer Univ, Dept Elect & Comp Engn, 4M8F 6QF, Byblos 135053, Lebanon.
   [Gadekallu, Thippa Reddy] Vellore Inst Technol, Sch Informat Technol & Engn, Tiruvalam Rd, Vellore 632014, Tamil Nadu, India.
   [Gadekallu, Thippa Reddy] Jiaxing Univ, Coll Informat Sci & Engn, 899 Guangqiong Rd, Jiaxing 314001, Zhejiang, Peoples R China.
   [Gadekallu, Thippa Reddy] Lovely Profess Univ, Div Res & Dev, GT Rd, Phagwara 144411, Punjab, India.
   [Abbas, Sidra] COMSATS Univ Islamabad, Dept Comp Sci, Line 12,Pk Rd, Islamabad, Pakistan.
C3 Dalian University of Technology; Shenyang Normal University; Brandon
   University; Lebanese American University; China Medical University
   Taiwan; Lebanese American University; Vellore Institute of Technology
   (VIT); VIT Vellore; Jiaxing University; Lovely Professional University;
   COMSATS University Islamabad (CUI)
RP Li, P (corresponding author), Dalian Univ Technol, Sch Software Technol, 321 Tuqiang St, Dalian 116620, Peoples R China.; Laghari, AA (corresponding author), Shenyang Normal Univ, Software Coll, 253 Huanghebei St, Shenyang 116620, Liaoning, Peoples R China.
EM gaojing@dlut.edu.cn; lipeng2015@mail.dlut.edu.cn;
   asiflaghari@synu.edu.cn; SrivastavaG@brandonu.ca;
   thippareddy.g@vit.ac.in; sidraabbas@ieee.org;
   zhang1234567893@mail.dlut.edu.cn
RI Gadekallu, Thippa Reddy/T-4254-2019; Laghari, Asif Ali/AAF-5893-2020
OI Gadekallu, Thippa Reddy/0000-0003-0097-801X; Laghari, Asif
   Ali/0000-0001-5831-5943; Zhang, Jianing/0000-0002-0695-2788; Srivastava,
   Gautam/0000-0001-9851-4103
FU National Natural Science Foundation of China [62002044, 62076047];
   Liaoning Provincial Science Public Welfare Research Fund-Soft Science
   Research Program [2023JH4/10700039]; Peak Climbing Plan" Project of
   Dalian Municipal Central Hospital [2023ZZ029, 2021ZZ006]
FX This work was partly supported by the National Natural Science
   Foundation of China under Grants No. 62002044 and No. 62076047, partly
   supported by Liaoning Provincial Science Public Welfare Research
   Fund-Soft Science Research Program under Grant No. 2023JH4/10700039, and
   partly supported by "Peak Climbing Plan" Project of Dalian Municipal
   Central Hospital under Grants No. 2023ZZ029 and No. 2021ZZ006.
CR Hameed MA, 2022, COMPUT INTEL NEUROSC, V2022, DOI 10.1155/2022/2247675
   Adhikari D, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3533381
   Ahmad K, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3306240
   Al-Garadi MA, 2020, IEEE COMMUN SURV TUT, V22, P1646, DOI 10.1109/COMST.2020.2988293
   Cai X, 2012, BIOINFORMATICS, V28, pI16, DOI 10.1093/bioinformatics/bts220
   Gao J, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3265699
   Hu ML, 2019, AAAI CONF ARTIF INTE, P3838
   Hu ML, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2262
   Ji X, 2019, IEEE I CONF COMP VIS, P9864, DOI 10.1109/ICCV.2019.00996
   Jiang YuG., 2011, Proceedings of the 1st ACM International Conference on Multimedia Retrieval, P1
   Li LS, 2023, IEEE T KNOWL DATA EN, V35, P589, DOI 10.1109/TKDE.2021.3082470
   Li MM, 2023, IEEE T CYBERNETICS, V53, P3479, DOI 10.1109/TCYB.2021.3126727
   Li P, 2023, IEEE T IND INFORM, V19, P693, DOI 10.1109/TII.2022.3197201
   Li SY, 2014, AAAI CONF ARTIF INTE, P1968
   Li XF, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P3412, DOI 10.1145/3503161.3548245
   Lin YJ, 2021, PROC CVPR IEEE, P11169, DOI 10.1109/CVPR46437.2021.01102
   Lindqvist U, 2017, COMMUN ACM, V60, P26, DOI 10.1145/3029589
   Liu C, 2023, IEEE T KNOWL DATA EN, V35, P9394, DOI 10.1109/TKDE.2023.3238416
   Liu XW, 2024, IEEE T PATTERN ANAL, V46, P1412, DOI 10.1109/TPAMI.2021.3116948
   Liu XW, 2021, IEEE T PATTERN ANAL, V43, P2634, DOI 10.1109/TPAMI.2020.2974828
   Liu XW, 2019, AAAI CONF ARTIF INTE, P4392
   Liu XW, 2020, IEEE T PATTERN ANAL, V42, P1191, DOI 10.1109/TPAMI.2019.2892416
   Peng X, 2019, PR MACH LEARN RES, V97
   Salim S, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3537899
   Tang HY, 2022, 39 INT C MACHINE LEA
   Venkatesh Rachakonda, 2023, 2023 7th International Conference on Trends in Electronics and Informatics (ICOEI), P444, DOI 10.1109/ICOEI56765.2023.10125626
   Wang QQ, 2021, IEEE T IMAGE PROCESS, V30, P305, DOI 10.1109/TIP.2020.3036717
   Wang SH, 2023, APPL INTELL, V53, P3687, DOI 10.1007/s10489-022-03735-6
   Wang Y, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3408317
   Wen J, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3230
   Wen J, 2019, AAAI CONF ARTIF INTE, P5393
   Xu J, 2022, AAAI CONF ARTIF INTE, P8761
   Xu J, 2022, PROC CVPR IEEE, P16030, DOI 10.1109/CVPR52688.2022.01558
   Xu N, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1794, DOI 10.1145/3240508.3240679
   Yang MM, 2022, IEEE T SERV COMPUT, V15, P2188, DOI 10.1109/TSC.2020.3039336
   Yang MM, 2022, IEEE T INF FOREN SEC, V17, P2524, DOI 10.1109/TIFS.2022.3189532
   Yang MX, 2023, IEEE T PATTERN ANAL, V45, P1055, DOI 10.1109/TPAMI.2022.3155499
   Zhang W, 2023, IEEE T FUZZY SYST, V31, P1445, DOI 10.1109/TFUZZ.2022.3203506
   Zhang Y, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2717, DOI 10.1145/3474085.3475204
   Zhao H., 2016, PROC INT JOINT C AR, P2392
   Zhao J, 2015, NEURAL COMPUT, V27, P1345, DOI 10.1162/NECO_a_00732
   Zhao L, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3241055
NR 42
TC 1
Z9 1
U1 2
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2024
VL 20
IS 6
SI SI
AR 158
DI 10.1145/3625548
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OQ2T3
UT WOS:001208681800008
DA 2024-08-05
ER

PT J
AU Qi, L
   Yang, HP
   Shi, YH
   Geng, X
AF Qi, Lei
   Yang, Hongpeng
   Shi, Yinghuan
   Geng, Xin
TI MultiMatch: Multi-task Learning for Semi-supervised Domain
   Generalization
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Multi-task learning; semi-supervised learning; domain generalization
AB Domain generalization (DG) aims at learning a model on source domains to well generalize on the unseen target domain. Although it has achieved great success, most of the existing methods require the label information for all training samples in source domains, which is time-consuming and expensive in the real-world application. In this article, we resort to solving the semi-supervised domain generalization (SSDG) task, where there are a fewlabel information in each source domain. To address the task, we first analyze the theory of multi-domain learning, which highlights that (1) mitigating the impact of domain gap and (2) exploiting all samples to train the model can effectively reduce the generalization error in each source domain so as to improve the quality of pseudo-labels. According to the analysis, we propose MultiMatch, i.e., extending FixMatch to the multi-task learning framework, producing the high-quality pseudo-label for SSDG. To be specific, we consider each training domain as a single task (i.e., local task) and combine all training domains together (i.e., global task) to train an extra task for the unseen test domain. In the multi-task framework, we utilize the independent batch normalization and classifier for each task, which can effectively alleviate the interference from different domains during pseudo-labeling. Also, most of the parameters in the framework are shared, which can be trained by all training samples sufficiently. Moreover, to further boost the pseudo-label accuracy and the model's generalization, we fuse the predictions from the global task and local task during training and testing, respectively. A series of experiments validate the effectiveness of the proposed method, and it outperforms the existing semi-supervised methods and the SSDG method on several benchmark DG datasets.
C1 [Qi, Lei; Geng, Xin] Southeast Univ, Sch Comp Sci & Engn, Nanjing, Peoples R China.
   [Qi, Lei; Geng, Xin] Southeast Univ, Minist Educ, Key Lab New Generat Artiicial Intelligence Techno, Nanjing, Peoples R China.
   [Yang, Hongpeng] Southeast Univ, Sch Cyber Sci & Engn, Nanjing, Peoples R China.
   [Shi, Yinghuan] Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China.
C3 Southeast University - China; Southeast University - China; Southeast
   University - China; Nanjing University
RP Geng, X (corresponding author), Southeast Univ, Sch Comp Sci & Engn, Nanjing, Peoples R China.; Geng, X (corresponding author), Southeast Univ, Minist Educ, Key Lab New Generat Artiicial Intelligence Techno, Nanjing, Peoples R China.
EM qilei@seu.edu.cn; hp_yang@seu.edu.cn; syh@nju.edu.cn; xgeng@seu.edu.cn
OI Qi, Lei/0000-0001-7091-0702
FU NSFC Program [62206052, 62125602, 62076063]; Jiangsu Natural Science
   Foundation Project [BK20210224]; Xplorer Prize
FX The work is supported by NSFC Program (Grants No. 62206052, 62125602,
   62076063), Jiangsu Natural Science Foundation Project (Grant No.
   BK20210224), and the Xplorer Prize.
CR Balaji Y, 2018, ADV NEUR IN, V31
   Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4
   Berthelot D., 2020, INT C LEARN REPR, P1
   Berthelot D, 2019, ADV NEUR IN, V32
   Carlucci FM, 2019, PROC CVPR IEEE, P2224, DOI 10.1109/CVPR.2019.00233
   Chang WG, 2019, PROC CVPR IEEE, P7346, DOI 10.1109/CVPR.2019.00753
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dou Q., 2019, ADV NEUR IN, P579
   Fu SC, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3412846
   Gong CY, 2021, PROC CVPR IEEE, P13678, DOI 10.1109/CVPR46437.2021.01347
   Gong R, 2019, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2019.00258
   Grandvalet Y, 2004, Proceedings of NIPS, V17
   Hastie T., 2009, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, V2, DOI DOI 10.1007/978-0-387-84858-7
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Isobe T, 2021, PROC CVPR IEEE, P8183, DOI 10.1109/CVPR46437.2021.00809
   Jeon S, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P22, DOI 10.1145/3474085.3475271
   Jin X, 2021, IEEE T MULTIMEDIA, V24, P3636, DOI 10.1109/TMM.2021.3104379
   Li D, 2019, IEEE I CONF COMP VIS, P1446, DOI 10.1109/ICCV.2019.00153
   Li D, 2018, AAAI CONF ARTIF INTE, P3490
   Li D, 2017, IEEE I CONF COMP VIS, P5543, DOI 10.1109/ICCV.2017.591
   Li HL, 2018, PROC CVPR IEEE, P5400, DOI 10.1109/CVPR.2018.00566
   Li JN, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9455, DOI 10.1109/ICCV48922.2021.00934
   Li Y, 2018, LECT NOTES COMPUT SC, V11219, P647, DOI 10.1007/978-3-030-01267-0_38
   Liu YJ, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3524136
   Liu YJ, 2023, IEEE T MULTIMEDIA, V25, P126, DOI 10.1109/TMM.2021.3121564
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Muandet K., 2013, INT C MACHINE LEARNI, P10
   Nam H, 2021, PROC CVPR IEEE, P8686, DOI 10.1109/CVPR46437.2021.00858
   Nassar I, 2021, PROC CVPR IEEE, P7237, DOI 10.1109/CVPR46437.2021.00716
   Oh Y, 2022, PROC CVPR IEEE, P9776, DOI 10.1109/CVPR52688.2022.00956
   Peng XC, 2019, IEEE I CONF COMP VIS, P1406, DOI 10.1109/ICCV.2019.00149
   Qi L, 2023, IEEE T MULTIMEDIA, V25, P4856, DOI 10.1109/TMM.2022.3183393
   Qi L, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3419439
   Rahman MM, 2020, PATTERN RECOGN, V100, DOI 10.1016/j.patcog.2019.107124
   Rahman MM, 2019, IEEE WINT CONF APPL, P579, DOI 10.1109/WACV.2019.00067
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Samuli L., 2017, INT C LEARN REPR ICL, V4, P6
   Sarawagi Sunita, 2018, INT C LEARN REPR
   Seonguk Seo, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P68, DOI 10.1007/978-3-030-58542-6_5
   Sohn K., 2020, ADV NEURAL INFORM PR, V33, P596, DOI [10.48550/arXiv.2001.07685, DOI 10.48550/ARXIV.2001.07685]
   Tarvainen A, 2017, ADV NEUR IN, V30
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572
   Wang Jindong, 2021, INT JOINT C ARTIFICI, P4627
   Wang XD, 2022, PROC CVPR IEEE, P14627, DOI 10.1109/CVPR52688.2022.01424
   Wang Y, 2022, IEEE T CIRC SYST VID, V32, P5495, DOI 10.1109/TCSVT.2022.3152615
   Wang YF, 2021, Arxiv, DOI arXiv:2109.05826
   Wang YY, 2021, FRONT COMPUT SCI-CHI, V15, DOI 10.1007/s11704-019-9115-z
   Wu KH, 2023, FRONT COMPUT SCI-CHI, V17, DOI 10.1007/s11704-022-2146-x
   Xu QW, 2021, PROC CVPR IEEE, P14378, DOI 10.1109/CVPR46437.2021.01415
   Xu YF, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3501800
   Yang F, 2022, PROC CVPR IEEE, P14401, DOI 10.1109/CVPR52688.2022.01402
   Yue XY, 2019, IEEE I CONF COMP VIS, P2100, DOI 10.1109/ICCV.2019.00219
   Zeyi Huang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P124, DOI 10.1007/978-3-030-58536-5_8
   Zhang B., 2021, Advances in Neural Information Processing Systems, V34, P18408, DOI DOI 10.48550/ARXIV.2110.08263
   Zhang J, 2022, PATTERN RECOGN, V122, DOI 10.1016/j.patcog.2021.108292
   Zhao J, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3506711
   Zhao S., 2020, ADV NEURAL INFORM PR
   Zhao Z, 2022, PROC CVPR IEEE, P9747, DOI 10.1109/CVPR52688.2022.00953
   Zheng MK, 2022, PROC CVPR IEEE, P14451, DOI 10.1109/CVPR52688.2022.01407
   Zhou KY, 2021, Arxiv, DOI arXiv:2106.00592
   Zhou KY, 2023, IEEE T PATTERN ANAL, V45, P4396, DOI 10.1109/TPAMI.2022.3195549
   Zhou KY, 2021, IEEE T IMAGE PROCESS, V30, P8008, DOI 10.1109/TIP.2021.3112012
   Zhou KY, 2020, AAAI CONF ARTIF INTE, V34, P13025
   Zhou ZQ, 2022, PROC CVPR IEEE, P20824, DOI 10.1109/CVPR52688.2022.02019
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 68
TC 0
Z9 0
U1 8
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2024
VL 20
IS 6
SI SI
AR 184
DI 10.1145/3648680
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OQ2T3
UT WOS:001208681800034
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Guo, W
   Quan, WZ
   Gao, JY
   Zhang, TZ
   Xu, CS
AF Guo, Wen
   Quan, Wuzhou
   Gao, Junyu
   Zhang, Tianzhu
   Xu, Changsheng
TI Feature Disentanglement Network: Multi-Object Tracking Needs More
   Differentiated Features
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Multiple object tracking; Feature disentanglement network; one-shot
   tracking; feature enhancement
AB To reduce computational redundancies, a common approach is to integrate detection and re-identification (Re-ID) into a single network in multi-object tracking (MOT), referred to as "tracking by detection." Most of the previous research has focused on resolving the conflict between the detection and Re-ID branches, considering it a simple coupling. In our work, we uncover that the entangled state between the detection and Re-ID tasks is much more complex than previous idea, resulting in a form of competition that degrades performance. To address the preceding issue, we propose a feature disentanglement network that deeply disentangles the intricately interwoven latent space of features and provides differentiated feature maps for each individual task. Furthermore, considering the demand for shallow semantic features in the feature re-ID branch, we also introduce a feature re-globalization module to enrich the shallow semantics. By integrating two distinct networks into a one-shot online MOT method, we develop a robust MOT tracker (named HDGTrack). We conduct extensive experiments on a number of benchmarks, and our experimental results demonstrate that our method significantly outperforms state-of-the-art MOT methods. Besides, HDGTrack is efficient and can run at 13.9 (MOT17) and 8.7 (MOT20) frames per second.
C1 [Guo, Wen; Quan, Wuzhou] Shandong Technol & Business Univ, Sch Informat & Elect Engn, 191 Binhaizhong Rd, Yantai 264005, Shandong, Peoples R China.
   [Gao, Junyu; Xu, Changsheng] Chinese Acad Sci CASIA, Inst Automat, State Key Lab Multimodal Artificial Intelligence, 95 Zhongguancun East Rd, Beijing, Peoples R China.
   [Zhang, Tianzhu] Univ Sci & Technol China, Sch Informat Sci & Technol, 443 Huangshan Rd, Hefei 230027, Peoples R China.
C3 Shandong Technology & Business University; Chinese Academy of Sciences;
   Institute of Automation, CAS; Chinese Academy of Sciences; University of
   Science & Technology of China, CAS
RP Guo, W (corresponding author), Shandong Technol & Business Univ, Sch Informat & Elect Engn, 191 Binhaizhong Rd, Yantai 264005, Shandong, Peoples R China.
EM wguo@sdtbu.edu.cn; wzquan@sdtbu.edu.cn; jy-gao@nlpr.ia.ac.cn;
   tzzhang@ustc.edu.cn; csxu@nlpr.ia.ac.cn
RI xu, cj/HJZ-3488-2023; Gao, Junyu/HDO-5516-2022; Zhang,
   Tianzhu/AGY-9389-2022
OI Zhang, Tianzhu/0000-0003-0764-6106; Quan, Wuzhou/0000-0002-5593-2054;
   zhang, tian zhu/0000-0003-1856-9564; Gao, Junyu/0000-0002-8105-5497; xu,
   chang sheng/0000-0001-8343-9665
FU National Natural Science Foundation of China [62072286, 61572296,
   61876100]
FX This work was partially supported by the National Natural Science
   Foundation of China under grants 62072286, 61572296, and 61876100.
CR Aharon N, 2022, Arxiv, DOI [arXiv:2206.14651, DOI 10.48550/ARXIV.2206.14651]
   Andriyenko A, 2011, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2011.5995311
   Bergmann P, 2019, IEEE I CONF COMP VIS, P941, DOI 10.1109/ICCV.2019.00103
   Bernardin K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/246309
   Bewley A, 2016, IEEE IMAGE PROC, P3464, DOI 10.1109/ICIP.2016.7533003
   Bochinski E, 2017, 2017 14TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS)
   Cao JK, 2023, Arxiv, DOI arXiv:2203.14360
   Dendorfer P., 2020, arXiv
   Dollár P, 2009, PROC CVPR IEEE, P304, DOI 10.1109/CVPRW.2009.5206631
   Du YH, 2023, IEEE T MULTIMEDIA, V25, P8725, DOI 10.1109/TMM.2023.3240881
   Ess A, 2008, PROC CVPR IEEE, P1857
   Fang K, 2018, IEEE WINT CONF APPL, P466, DOI 10.1109/WACV.2018.00057
   Feichtenhofer C, 2017, IEEE I CONF COMP VIS, P3057, DOI 10.1109/ICCV.2017.330
   Han SD, 2022, NEUROCOMPUTING, V476, P75, DOI 10.1016/j.neucom.2021.12.104
   Jin Y, 2023, IEEE T CIRC SYST VID, V33, P5117, DOI 10.1109/TCSVT.2023.3249162
   Jinlong Peng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P145, DOI 10.1007/978-3-030-58548-8_9
   Kendall A, 2018, PROC CVPR IEEE, P7482, DOI 10.1109/CVPR.2018.00781
   Liang C, 2022, IEEE T IMAGE PROCESS, V31, P3182, DOI 10.1109/TIP.2022.3165376
   Liang YM, 2018, IEEE IMAGE PROC, P2351, DOI 10.1109/ICIP.2018.8451739
   Lin Tsung-Yi, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Luiten J, 2021, INT J COMPUT VISION, V129, P548, DOI 10.1007/s11263-020-01375-2
   Luo WH, 2022, Arxiv, DOI [arXiv:1409.7618, 10.1016/j.artint.2020.103448]
   Meinhardt T, 2022, PROC CVPR IEEE, P8834, DOI 10.1109/CVPR52688.2022.00864
   Milan A, 2016, Arxiv, DOI arXiv:1603.00831
   Pang B, 2020, PROC CVPR IEEE, P6307, DOI 10.1109/CVPR42600.2020.00634
   Particke F, 2017, PROCEEDINGS OF THE 12TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (VISIGRAPP 2017), VOL 6, P509, DOI 10.5220/0006215705090514
   Particke F, 2017, 2017 SENSOR DATA FUSION: TRENDS, SOLUTIONS, APPLICATIONS (SDF)
   Redmon J., 2018, CoRR
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren J, 2017, PROC CVPR IEEE, P752, DOI 10.1109/CVPR.2017.87
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rezatofighi SH, 2015, IEEE I CONF COMP VIS, P3047, DOI 10.1109/ICCV.2015.349
   Sadeghian A, 2017, IEEE I CONF COMP VIS, P300, DOI 10.1109/ICCV.2017.41
   Shao S, 2018, Arxiv, DOI arXiv:1805.00123
   Wang HD, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3533253
   Wang YX, 2021, IEEE INT CONF ROBOT, P13708, DOI 10.1109/ICRA48506.2021.9561110
   Wojke N, 2017, IEEE IMAGE PROC, P3645, DOI 10.1109/ICIP.2017.8296962
   Xiao T, 2017, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2017.360
   Xu YH, 2023, IEEE T PATTERN ANAL, V45, P7820, DOI 10.1109/TPAMI.2022.3225078
   Yang F, 2016, PROC CVPR IEEE, P2129, DOI 10.1109/CVPR.2016.234
   Yu E, 2023, IEEE T MULTIMEDIA, V25, P2686, DOI 10.1109/TMM.2022.3150169
   Yu F, 2018, PROC CVPR IEEE, P2403, DOI 10.1109/CVPR.2018.00255
   Zhang SS, 2017, PROC CVPR IEEE, P4457, DOI 10.1109/CVPR.2017.474
   Zhang Y, 2020, IEEE INTERNET THINGS, V7, P7892, DOI 10.1109/JIOT.2020.2996609
   Zhang YF, 2021, INT J COMPUT VISION, V129, P3069, DOI 10.1007/s11263-021-01513-4
   Zheng L, 2017, PROC CVPR IEEE, P3346, DOI 10.1109/CVPR.2017.357
   Zhichao Lu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14656, DOI 10.1109/CVPR42600.2020.01468
   Zhongdao Wang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P107, DOI 10.1007/978-3-030-58621-8_7
   Zhou XY, 2019, Arxiv, DOI arXiv:1904.07850
   Zhou ZW, 2018, INT C PATT RECOG, P1809, DOI 10.1109/ICPR.2018.8545450
NR 51
TC 0
Z9 0
U1 4
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2024
VL 20
IS 3
AR 83
DI 10.1145/3626825
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6F8
UT WOS:001153381000023
DA 2024-08-05
ER

PT J
AU Sarma, S
   Sur, A
AF Sarma, Sandipan
   Sur, Arijit
TI DiRaC-I: Identifying Diverse and Rare Training Classes for Zero-Shot
   Learning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Zero-shot learning; domain rarity; active learning; image classification
ID DATABASE
AB Zero-Shot Learning (ZSL) is an extreme form of transfer learning that aims at learning from a few "seen classes" to have an understanding about the "unseen classes" in the wild. Given a dataset in ZSL research, most existing works use a predetermined, disjoint set of seen-unseen classes to evaluate their methods. These seen (training) classes might be sub-optimal for ZSL methods to appreciate the diversity and rarity of an object domain. Inspired by strategies like active learning, it is intuitive that intelligently selecting the training classes can improve ZSL performance. In this work, we propose a framework called Diverse and Rare Class Identifier (DiRaC-I) which, given an attribute-based dataset, can intelligently yield the most suitable "seen classes" for training ZSL models. DiRaC-I has two main goals - constructing a diversified set of seed classes, and using them to initialize a visual-semantic mining algorithm for acquiring the classes capturing both diversity and rarity in the object domain adequately. These classes can then be used as "seen classes" to train ZSL models for image classification. We simulate a real-world scenario where visual samples of novel object classes in the wild are available to neither DiRaC-I nor the ZSL models during training and conducted extensive experiments on two benchmark data sets for zero-shot image classification - CUB and SUN. Our results demonstrate DiRaC-I helps ZSL models to achieve significant classification accuracy improvements specifically, up to 8% for CUB and up to 5% for SUN dataset. Additionally, while recognizing classes exhibiting rare attributes we also observe a performance boost for ZSL models, which is up to 10% and 7% for CUB and SUN datasets, respectively.
C1 [Sarma, Sandipan; Sur, Arijit] Indian Inst Technol Guwahati, Dept Comp Sci & Engn, Multimedia Lab, Gauhati 781039, Assam, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Guwahati
RP Sarma, S (corresponding author), Indian Inst Technol Guwahati, Dept Comp Sci & Engn, Multimedia Lab, Gauhati 781039, Assam, India.
EM sandipan.sarma@iitg.ac.in; arijit@iitg.ac.in
RI Sarma, Sandipan/JQW-4525-2023; Sur, Arijit/AAB-4216-2020
OI Sarma, Sandipan/0000-0003-4619-3058; Sur, Arijit/0000-0002-9038-8138
FU  [BT/COE/34/SP28408/2018]
FX We acknowledge the Department of Biotechnology, Govt. of India for the
   financial support for the project BT/COE/34/SP28408/2018 (for computing
   resources).
CR Akata Z, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2487986
   Akata Z, 2015, PROC CVPR IEEE, P2927, DOI 10.1109/CVPR.2015.7298911
   [Anonymous], 2017, International Journal of Data Science and Analytics, DOI DOI 10.1007/S41060-017-0042-5
   Bansal A, 2018, LECT NOTES COMPUT SC, V11205, P397, DOI 10.1007/978-3-030-01246-5_24
   Bendale A, 2016, PROC CVPR IEEE, P1563, DOI 10.1109/CVPR.2016.173
   Changpinyo S, 2016, PROC CVPR IEEE, P5327, DOI 10.1109/CVPR.2016.575
   Chen BZ, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3457124
   Chen S., 2022, IEEE T PATTERN ANAL, V01, P1
   Chen SM, 2021, ADV NEUR IN, V34
   Chen SM, 2022, AAAI CONF ARTIF INTE, P330
   Chen SM, 2022, PROC CVPR IEEE, P7602, DOI 10.1109/CVPR52688.2022.00746
   Chen SM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P122, DOI 10.1109/ICCV48922.2021.00019
   Chen Shizhe, 2021, P IEEE CVF INT C COM, P13638
   Chou Y-Y, 2021, INT C LEARN REPR
   Dligach Dmitriy, 2011, P 49 ANN M ASS COMPU, P6
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Felix R, 2018, LECT NOTES COMPUT SC, V11210, P21, DOI 10.1007/978-3-030-01231-1_2
   Feng LJ, 2021, IEEE T NEUR NET LEAR, V32, P2506, DOI 10.1109/TNNLS.2020.3006322
   Frome A., 2013, Advances in neural information processing systems, V26
   Fu ZY, 2018, IEEE T PATTERN ANAL, V40, P2009, DOI 10.1109/TPAMI.2017.2737007
   Howard AG, 2017, Arxiv, DOI [arXiv:1704.04861, 10.48550/arXiv.1704.04861]
   Guo-Sen Xie, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P562, DOI 10.1007/978-3-030-58548-8_33
   Han ZY, 2021, PROC CVPR IEEE, P2371, DOI 10.1109/CVPR46437.2021.00240
   Hanneke Steve, 2014, Now Foundations and Trends
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Ishihara K., 2021, P IEEE CVF C COMP VI, P2902
   Jiang SQ, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3391624
   Kampffmeyer M, 2019, PROC CVPR IEEE, P11479, DOI 10.1109/CVPR.2019.01175
   Kaufman L., 2009, Finding Groups in Data: An Introduction to Cluster Analysis, Vvol 344
   Kennedy BRC, 2019, FRONT MAR SCI, V6, DOI 10.3389/fmars.2019.00480
   Kodirov E, 2017, PROC CVPR IEEE, P4447, DOI 10.1109/CVPR.2017.473
   Kunz C, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P3654, DOI 10.1109/IROS.2008.4651097
   Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140
   Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594
   Liu XB, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3465220
   Liu XF, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3441577
   Mikolov T., 2013, Advances in Neural Information Processing Systems, P3111
   MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748
   Mishra A, 2018, IEEE COMPUT SOC CONF, P2269, DOI 10.1109/CVPRW.2018.00294
   Narayan Sanath, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P479, DOI 10.1007/978-3-030-58542-6_29
   Norouzi M, 2014, Arxiv, DOI arXiv:1312.5650
   Patterson G, 2014, INT J COMPUT VISION, V108, P59, DOI 10.1007/s11263-013-0695-z
   Pennington J., 2014, GLOVE GLOBAL VECTORS, DOI DOI 10.3115/V1/D14-1162
   Radford A, 2021, PR MACH LEARN RES, V139
   Rahman MA, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3421725
   Rezaei M, 2014, PROC CVPR IEEE, P129, DOI 10.1109/CVPR.2014.24
   Rohrbach M, 2010, PROC CVPR IEEE, P910, DOI 10.1109/CVPR.2010.5540121
   Romera-Paredes B, 2015, PR MACH LEARN RES, V37, P2152
   ROUSSEEUW PJ, 1987, J COMPUT APPL MATH, V20, P53, DOI 10.1016/0377-0427(87)90125-7
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Skorokhodov I, 2021, Arxiv, DOI arXiv:2006.11328
   Socher R, 2013, P 2013 C EMP METH NA, P935
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tang CW, 2022, IEEE T NEUR NET LEAR, V33, P6749, DOI 10.1109/TNNLS.2021.3083367
   Tomanek K., 2009, P NAACL HLT 2009 WOR, P9
   van der Maaten L, 2014, J MACH LEARN RES, V15, P3221
   Vyas Maunil R., 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P70, DOI 10.1007/978-3-030-58577-8_5
   Wah C., 2011, Tech. Rep. CNS-TR-2011-001
   Wang J, 2021, IEEE INT CONF COMP V, P885, DOI 10.1109/ICCVW54120.2021.00104
   Wang QB, 2021, NEUROCOMPUTING, V435, P15, DOI 10.1016/j.neucom.2020.12.127
   WARD JH, 1963, J AM STAT ASSOC, V58, P236, DOI 10.2307/2282967
   Xian YQ, 2019, PROC CVPR IEEE, P10267, DOI 10.1109/CVPR.2019.01052
   Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581
   Xian YQ, 2019, IEEE T PATTERN ANAL, V41, P2251, DOI 10.1109/TPAMI.2018.2857768
   Xian YQ, 2016, PROC CVPR IEEE, P69, DOI 10.1109/CVPR.2016.15
   Xie Guo-Sen, 2022, IEEE Transactions on Neural Networks and Learning Systems
   Xie SH, 2016, CIKM'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P1889, DOI 10.1145/2983323.2983866
   Xu BR, 2022, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2022.3142181
   Xu XL, 2021, ACM T SENSOR NETWORK, V17, DOI 10.1145/3447032
   Yi K, 2022, LECT NOTES COMPUT SC, V13680, P116, DOI 10.1007/978-3-031-20044-1_7
   Zhang ZM, 2015, IEEE I CONF COMP VIS, P4166, DOI 10.1109/ICCV.2015.474
   Zhao XJ, 2022, AAAI CONF ARTIF INTE, P3454
NR 74
TC 1
Z9 1
U1 3
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2024
VL 20
IS 1
AR 3
DI 10.1145/3603147
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T8LL3
UT WOS:001080441800003
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Tan, MK
   Wen, ZQ
   Fang, LY
   Wu, Q
AF Tan, Mingkui
   Wen, Zhiquan
   Fang, Leyuan
   Wu, Qi
TI Transformer-Based Relational Inference Network for Complex Visual
   Relational Reasoning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Visual Relational Reasoning; complex referring expression comprehension;
   Gated Graph Neural Network
AB Visual Relational Reasoning is the basis of many vision-and-language based tasks (e.g., visual question answering and referring expression comprehension). In this article, we regard the complex referring expression comprehension (c-REF) task as the reasoning basis, in which c-REF seeks to localise a target object in an image guided by a complex query. Such queries often contain complex logic and thus impose two critical challenges for reasoning: (i) Comprehending the complex queries is difficult since these queries usually refer to multiple objects and their relationships; (ii) Reasoning among multiple objects guided by the queries and then localising the target correctly are non-trivial. To address the above challenges, we propose a Transformer-based Relational Inference Network (Trans-RINet). Specifically, to comprehend the queries, we mimic the language-comprehending mechanism of humans, and devise a language decomposition module to decompose the queries into four types, i.e., basic attributes, absolute location, visual relationship and relative location. We further devise four modules to address the corresponding information. In each module, we consider the intra-(i.e., between the objects) and inter-modality relationships(i.e., between the queries and objects) to improve the reasoning ability. Moreover, we construct a relational graph to represent the objects and their relationships, and devise a multi-step reasoning method to progressively understand the complex logic. Since each type of the queries is closely related, we let each module interact with each other before making a decision. Extensive experiments on the CLEVR-Ref+, Ref-Reasoning, and CLEVR-CoGenT datasets demonstrate the superior reasoning performance of our Trans-RINet.
C1 [Tan, Mingkui; Wen, Zhiquan] South China Univ Technol, Guangzhou, Peoples R China.
   [Fang, Leyuan] Hunan Univ, Changsha, Peoples R China.
   [Wu, Qi] Univ Adelaide, Adelaide, SA, Australia.
C3 South China University of Technology; Hunan University; University of
   Adelaide
RP Tan, MK (corresponding author), South China Univ Technol, Guangzhou, Peoples R China.
EM mingkuitan@scut.edu.cn; sewenzhiquan@mail.scut.edu.cn;
   fangleyuan@gmail.com; qi.wu01@adelaide.edu.au
RI Wu, Qi/ABD-6304-2021; Fang, Leyuan/G-1468-2011
OI Wu, Qi/0000-0003-3631-256X; Wen, Zhiquan/0000-0002-4969-0060; Fang,
   Leyuan/0000-0003-2351-4461
FU STI 2030-Major Projects [2022ZD0208900]; National Natural Science
   Foundation of China (NSFC) [62072190]; Key Realm R&D Program of
   Guangzhou [202007030007]; Program for Guangdong Introducing Innovative
   and Entrepreneurial Teams [2017ZT0-7X183]
FX This work was partially supported by STI 2030-Major Projects
   (2022ZD0208900), National Natural Science Foundation of China (NSFC)
   62072190, Key Realm R&D Program of Guangzhou 202007030007, Program for
   Guangdong Introducing Innovative and Entrepreneurial Teams
   2017ZT0-7X183.
CR Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387
   Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12
   Bajaj M, 2019, IEEE I CONF COMP VIS, P4280, DOI 10.1109/ICCV.2019.00438
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen Zhenfang, 2021, P INT C LEARN REPR I
   Cho K., 2014, ARXIV14061078, V1406, P1078, DOI DOI 10.3115/V1/D14-1179
   Cirik Volkan, 2018, NAACL, P781
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dosovitskiy A., 2021, PROC INT C LEARNING, DOI DOI 10.48550/ARXIV.2010.11929
   Guan WL, 2022, PROCEEDINGS OF THE 45TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '22), P482, DOI 10.1145/3477495.3532038
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He MG, 2023, IEEE T CIRC SYST VID, V33, P2990, DOI 10.1109/TCSVT.2022.3225549
   Hu R., 2020, P IEEE CVF C COMP VI, DOI DOI 10.1109/CVPR42600.2020.01001
   Hu RH, 2018, LECT NOTES COMPUT SC, V11211, P55, DOI 10.1007/978-3-030-01234-2_4
   Hu RH, 2019, IEEE I CONF COMP VIS, P10293, DOI 10.1109/ICCV.2019.01039
   Hu RH, 2017, PROC CVPR IEEE, P4418, DOI 10.1109/CVPR.2017.470
   Huang D, 2020, AAAI CONF ARTIF INTE, V34, P11021
   Huang HS, 2019, IEEE I CONF COMP VIS, P7403, DOI 10.1109/ICCV.2019.00750
   Hudson D. A., 2018, INT C LEARN REPR, P1
   Jiasen Lu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10434, DOI 10.1109/CVPR42600.2020.01045
   Jing Chenchen, 2022, P AAAI C ART INT AAA, V2
   Johnson J, 2017, IEEE I CONF COMP VIS, P3008, DOI 10.1109/ICCV.2017.325
   Kazemzadeh S., 2014, EMNLP, DOI DOI 10.3115/V1/D14-1086
   Ke LYM, 2019, PROC CVPR IEEE, P6734, DOI 10.1109/CVPR.2019.00690
   Kingma D. P., 2014, arXiv
   Li Y., 2016, ICLR, P1
   Liu RT, 2019, PROC CVPR IEEE, P4180, DOI 10.1109/CVPR.2019.00431
   Liu YB, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3498340
   Mao J., 2019, P INT C LEARN REPR
   Mao JH, 2016, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2016.9
   Nguyen K, 2019, PROC CVPR IEEE, P12519, DOI 10.1109/CVPR.2019.01281
   Pan YH, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3487042
   Paszke A, 2019, ADV NEUR IN, V32
   Pennington J., 2014, GLOVE GLOBAL VECTORS, DOI DOI 10.3115/V1/D14-1162
   Qi Yuankai, 2021, P IEEECVF INT C COMP, P1655, DOI DOI 10.1109/ICCV48922.2021.00168
   Qiao YY, 2021, IEEE T MULTIMEDIA, V23, P4426, DOI 10.1109/TMM.2020.3042066
   Raffel C, 2020, J MACH LEARN RES, V21
   Rohrbach A, 2016, LECT NOTES COMPUT SC, V9905, P817, DOI 10.1007/978-3-319-46448-0_49
   Romero A., 2018, INT C LEARN REPR, P2920, DOI DOI 10.48550/ARXIV.1710.10903
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Shi JX, 2019, PROC CVPR IEEE, P8368, DOI 10.1109/CVPR.2019.00857
   Sibei Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9949, DOI 10.1109/CVPR42600.2020.00997
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Jia, 2022, ACM Trans. Multimedia Comput. Commun. Appl. (ACM TOMMCCAP)
   Wang P, 2019, PROC CVPR IEEE, P1960, DOI 10.1109/CVPR.2019.00206
   Wen Z., 2021, Advances in neural information processing systems
   Wen Zhiquan, 2023, Findings of the Association for Computational Linguistics
   Wen Zhiquan, 2023, IEEE Transactions on Multimedia (TMM)
   Xin Wang, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P6622, DOI 10.1109/CVPR.2019.00679
   Xu GH, 2021, PROC CVPR IEEE, P12632, DOI 10.1109/CVPR46437.2021.01245
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yang SB, 2019, IEEE I CONF COMP VIS, P4643, DOI 10.1109/ICCV.2019.00474
   Yang SB, 2019, PROC CVPR IEEE, P4140, DOI 10.1109/CVPR.2019.00427
   Yen-Chun Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P104, DOI 10.1007/978-3-030-58577-8_7
   Yu DF, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3316767
   Yu LC, 2018, PROC CVPR IEEE, P1307, DOI 10.1109/CVPR.2018.00142
   Yu LC, 2017, PROC CVPR IEEE, P3521, DOI 10.1109/CVPR.2017.375
   Zeng RH, 2019, IEEE I CONF COMP VIS, P7093, DOI 10.1109/ICCV.2019.00719
   Zhenfang Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10083, DOI 10.1109/CVPR42600.2020.01010
   Zheng Y., 2020, PROC ASIAN C COMPUT, P137
   Zhu M., 2013, Long Papers, V1, P434
NR 62
TC 0
Z9 0
U1 13
U2 21
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2024
VL 20
IS 1
AR 10
DI 10.1145/3605781
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T8LL3
UT WOS:001080441800010
DA 2024-08-05
ER

PT J
AU Zhang, LP
   Chen, SK
   Lin, F
   Ren, W
   Choo, KKR
   Min, G
AF Zhang, Liping
   Chen, Shukai
   Lin, Fei
   Ren, Wei
   Choo, Kim-Kwang Raymond
   Min, Geyong
TI 1DIEN: Cross-session Electrocardiogram Authentication Using 1D
   Integrated EfficientNet
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE 1D Efficientnet; ECG; authentication
ID ECG AUTHENTICATION
AB The potential of using electrocardiogram (ECG), an important physiological signal for humans, as a new biometric trait has been demonstrated, and ongoing efforts have focused on utilizing deep learning (e.g., 2D neural networks) to improve authentication accuracy (with some efficiency tradeoffs). In most of the existing ECG-based authentication approaches, the ECG recordings for enrollment and testing are collected within short intervals (e.g., within an hour). However, since ECG biometrics change over time, this design may decrease authentication accuracy when ECG recordings are collected weeks or even months prior. In this article, we propose 1D Integrated EfficientNet (1DIEN) to achieve cross-session ECG authentication. We adopt 1D neural networks as a lightweight alternative to 2D neural networks, and a voting scheme is designed to reduce variance and improve general authentication performance. We use three public ECG databases (i.e., an inter-session database, a mixed-session database, and an intra-session database) to evaluate our proposed 1DIEN under different authentication scenarios. The experimental results show that our approach achieves satisfactory performance for ECG authentication at a 3-month interval and is suitable for practical applications.
C1 [Zhang, Liping; Chen, Shukai] China Univ Geosci, Sch Comp Sci, Wuhan, Peoples R China.
   [Chen, Shukai] Natl Univ Def Technol, Coll Comp Sci & Technol, Changsha, Peoples R China.
   [Lin, Fei] Wuhan Maritime Commun Res Inst, Wuhan, Peoples R China.
   [Ren, Wei] Chuzhou Univ, Anhui Engn Res Ctr Intelligent Percept & Elderly, Chuzhou, Peoples R China.
   [Ren, Wei] Anhui Univ Technol, Anhui Engn Res Ctr Intelligent Applicat & Secur I, Maanshan, Peoples R China.
   [Ren, Wei] China Univ Geosci, Sch Comp Sci, Wuhan, Peoples R China.
   [Choo, Kim-Kwang Raymond] Univ Texas San Antonio, Dept Informat Syst & Cyber Secur, San Antonio, TX USA.
   [Min, Geyong] Univ Exeter, Dept Comp Sci, Exeter, Devon, England.
C3 China University of Geosciences; National University of Defense
   Technology - China; Chuzhou University; Anhui University of Technology;
   China University of Geosciences; University of Texas System; University
   of Texas at San Antonio (UTSA); University of Exeter
RP Zhang, LP (corresponding author), China Univ Geosci, Sch Comp Sci, Wuhan, Peoples R China.
EM carolyn321@163.com; chenshukai@cug.edu.cn; 4419737@qq.com;
   weirencs@cug.edu.cn; Ray-mond.Choo@utsa.edu; g.min@exeter.ac.uk
OI Ren, Wei/0000-0001-8590-1737; Chen, Shukai/0000-0001-7903-0808
FU National Natural Science Foundation of China [62172303]; Open Research
   Project of the Hubei Key Laboratory of Intelligent GeoInformation
   Processing [KLIGIP-2019B09]; open Foundation of Anhui Engineering
   Research Center of Intelligent Perception and Elderly Care, Chuzhou
   University [2022OPA01]; Knowledge Innovation Program of Wuhan -Basic
   Research [2022010801010197]; Opening Project of Nanchang Innovation
   Institute, Peking University [NCII2022A02]; Foundation of Anhui
   Engineering Research Center for Intelligent Applications and Security of
   Industrial Internet, Anhui University of Technology, Ma'anshan [243032,
   IASII22-02]; Cloud Technology Endowed Professorship
FX The research was financially supported by the National Natural Science
   Foundation of China (No. 62172303), the Open Research Project of the
   Hubei Key Laboratory of Intelligent GeoInformation Processing (No.
   KLIGIP-2019B09), the open Foundation of Anhui Engineering Research
   Center of Intelligent Perception and Elderly Care, Chuzhou University
   (No. 2022OPA01), the Knowledge Innovation Program of Wuhan -Basic
   Research (No. 2022010801010197), the Opening Project of Nanchang
   Innovation Institute, Peking University (No. NCII2022A02), and the
   Foundation of Anhui Engineering Research Center for Intelligent
   Applications and Security of Industrial Internet, Anhui University of
   Technology, Ma'anshan, Anhui, 243032, China (IASII22-02). The work of
   K.-K.R. Choo was supported only by the Cloud Technology Endowed
   Professorship.
CR Abdeldayem Sara S., 2020, IEEE Transactions on Biometrics, Behavior, and Identity Science, V2, P1, DOI 10.1109/TBIOM.2019.2947434
   Abdeldayem SS, 2018, IEEE INT CONF BIG DA, P4984, DOI 10.1109/BigData.2018.8622619
   AMARI S, 1993, NEUROCOMPUTING, V5, P185, DOI 10.1016/0925-2312(93)90006-O
   Apple Inc, 2022, Take an ECG with the ECG app on Apple Watch. [EB/OL]
   Arteaga-Falconi JS, 2016, IEEE T INSTRUM MEAS, V65, P591, DOI 10.1109/TIM.2015.2503863
   Belo D, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20154078
   Bousseljot R., 1995, Nutzung der ekg-signaldatenbank cardiodat der ptb uber das internet
   da Silva HP, 2014, COMPUT METH PROG BIO, V113, P503, DOI 10.1016/j.cmpb.2013.11.017
   Luz EJD, 2018, IEEE T INF FOREN SEC, V13, P1258, DOI 10.1109/TIFS.2017.2784362
   Ellavarason E, 2021, ACM COMPUT SURV, V53, DOI 10.1145/3394713
   Goldberger AL, 2000, CIRCULATION, V101, pE215, DOI 10.1161/01.CIR.101.23.e215
   Hammad M, 2021, EXPERT SYST, V38, DOI 10.1111/exsy.12547
   Huang P, 2019, IEEE INTERNET THINGS, V6, P9200, DOI 10.1109/JIOT.2019.2929087
   Ihsanto E, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10093304
   Khan S, 2020, ACM COMPUT SURV, V53, DOI 10.1145/3400030
   Kim MG, 2019, IEEE T IND INFORM, V15, P5656, DOI 10.1109/TII.2019.2909730
   Kim SK, 2019, IEEE ACCESS, V7, P94858, DOI 10.1109/ACCESS.2019.2927079
   Komeili M, 2018, IEEE T INF FOREN SEC, V13, P1810, DOI 10.1109/TIFS.2018.2804890
   Labati RD, 2019, PATTERN RECOGN LETT, V126, P78, DOI 10.1016/j.patrec.2018.03.028
   Lugovaya T. S., 2005, "Biometric human identification based on electrocardiogram
   Odinaka I, 2012, IEEE T INF FOREN SEC, V7, P1812, DOI 10.1109/TIFS.2012.2215324
   PAN J, 1985, IEEE T BIO-MED ENG, V32, P230, DOI 10.1109/TBME.1985.325532
   PyTorch, 2022, From research to productioin
   Scikit-Learn, 2022, Machine Learning in Python
   Tan MX, 2019, PR MACH LEARN RES, V97
   Zhang Y, 2018, J NETW COMPUT APPL, V117, P10, DOI 10.1016/j.jnca.2018.05.007
   Zhao ZD, 2018, COMPUT BIOL MED, V102, P168, DOI 10.1016/j.compbiomed.2018.09.027
NR 27
TC 2
Z9 2
U1 15
U2 33
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2024
VL 20
IS 1
AR 17
DI 10.1145/3609800
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T8LL3
UT WOS:001080441800017
DA 2024-08-05
ER

PT J
AU Chen, H
   Yu, YL
   Dong, YH
   Lu, ZM
   Li, YM
   Zhang, ZL
AF Chen, Hao
   Yu, Yunlong
   Dong, Yonghan
   Lu, Zheming
   Li, Yingming
   Zhang, Zhongfei
TI Multi-Content Interaction Network for Few-Shot Segmentation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Few-shot semantic segmentation; multi-content interaction;
   adjacent-layer similarity
AB Few-Shot Segmentation (FSS) poses significant challenges due to limited support images and large intraclass appearance discrepancies. Most existing approaches focus on aligning the support-query correlations from the same layer of the frozen backbone while neglecting the bias between different tasks and different layers. In this article, we propose a Multi-Content Interaction Network (MCINet) to remedy these issues by fully exploiting and interacting with the different contextual information contained in distinct branches. Specifically, MCINet improves FSS from three perspectives: (1) boosting the query representations through incorporating the independent information from another learnable branch into the features from the frozen backbone, (2) enhancing the support-query correlations by exploiting both the same-layer and adjacent-layer features, and (3) refining the predicted results with a multi-scale mask prediction strategy. Experiments on three benchmarks demonstrate that our approach reaches state-of-the-art performances and outperforms the best competitors with many desirable advantages, especially on the challenging COCO dataset. Code will be released on GitHub (https://github.com/chenhao-zju/mcinet).
C1 [Chen, Hao; Yu, Yunlong; Lu, Zheming; Li, Yingming] Zhejiang Univ, 866 Yuhangtang Rd, Hangzhou 310027, Zhejiang, Peoples R China.
   [Dong, Yonghan] Huawei Technol Ltd, 3998Wuhe Ave, Shenzhen 518129, Guangdong, Peoples R China.
   [Zhang, Zhongfei] SUNY Binghamton, 4400 Vestal Pkwy East Binghamton, Binghamton, NY 13902 USA.
C3 Zhejiang University; Huawei Technologies; State University of New York
   (SUNY) System; State University of New York (SUNY) Binghamton
RP Yu, YL; Lu, ZM (corresponding author), Zhejiang Univ, 866 Yuhangtang Rd, Hangzhou 310027, Zhejiang, Peoples R China.
EM chen_hao_zju@zju.edu.cn; yuyunlong@zju.edu.cn; dongyonghan@huawei.com;
   zheminglu@zju.edu.cn; yingming@zju.edu.cn; zhongfei@cs.binghamton.edu
OI Lu, Zhe-Ming/0000-0003-1785-7847; Chen, Hao/0000-0002-5700-1202; Zhang,
   Zhongfei/0000-0001-5098-2506
FU Key R&D Program of Zhejiang Province, China [2023C01043, 2021C01119];
   NSFC [62002320, U19B2043]; Science and Technology Innovation 2025 Major
   Project of Ningbo, China [2023Z236]
FX This work was supported in part by the Key R&D Program of Zhejiang
   Province, China (2023C01043, 2021C01119), NSFC (62002320, U19B2043), and
   the Science and Technology Innovation 2025 Major Project of Ningbo,
   China (2023Z236).
CR Boudiaf M, 2021, PROC CVPR IEEE, P13974, DOI 10.1109/CVPR46437.2021.01376
   Chen H., 2024, P IEEE CVF WINT C AP, P978
   Chen H., 2023, IEEE Transactions on Circuits and Systems for Video Technology, P1
   Chen H, 2023, IEEE ACCESS, V11, P73521, DOI 10.1109/ACCESS.2023.3295893
   Chen L.-C., 2018, ECCV, P801, DOI [DOI 10.1007/978-3-030-01234-249, 10.1007/978-3-030-01234-2_49]
   Chen LC, 2016, PROC CVPR IEEE, P3640, DOI 10.1109/CVPR.2016.396
   Dong KQ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2271, DOI 10.1145/3474085.3475389
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fan Q, 2022, LECT NOTES COMPUT SC, V13679, P701, DOI 10.1007/978-3-031-19800-7_41
   Gu JQ, 2022, PROC CVPR IEEE, P12084, DOI 10.1109/CVPR52688.2022.01178
   Haochen Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P730, DOI 10.1007/978-3-030-58601-0_43
   He J, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3511917
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hong S, 2022, LECT NOTES COMPUT SC, V13689, P108, DOI 10.1007/978-3-031-19818-2_7
   Jiang SQ, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3391624
   Johnander J, 2022, LECT NOTES COMPUT SC, V13689, P217, DOI 10.1007/978-3-031-19818-2_13
   Kayabas A, 2023, IEEE WINT CONF APPL, P2558, DOI 10.1109/WACV56688.2023.00259
   Lang C., 2022, INT JOINT C ART INT, P1024
   Lang CB, 2022, PROC CVPR IEEE, P8047, DOI 10.1109/CVPR52688.2022.00789
   Li G, 2021, PROC CVPR IEEE, P8330, DOI 10.1109/CVPR46437.2021.00823
   Li HY, 2019, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2019.00009
   Li X, 2020, PROC CVPR IEEE, P2866, DOI 10.1109/CVPR42600.2020.00294
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu HF, 2023, IEEE T MULTIMEDIA, V25, P8580, DOI 10.1109/TMM.2023.3238521
   Liu J, 2022, PROC CVPR IEEE, P11543, DOI 10.1109/CVPR52688.2022.01126
   Liu LZ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1441, DOI 10.1145/3394171.3413915
   Liu WD, 2023, IEEE T MULTIMEDIA, V25, P5130, DOI 10.1109/TMM.2022.3187855
   Liu XF, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3441577
   Liu Y., 2020, COMPUTER VISION ECCV, P142, DOI DOI 10.1007/978-3-030-58545-79
   Liu YW, 2022, PROC CVPR IEEE, P11563, DOI 10.1109/CVPR52688.2022.01128
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu ZH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8721, DOI 10.1109/ICCV48922.2021.00862
   Lüddecke T, 2022, PROC CVPR IEEE, P7076, DOI 10.1109/CVPR52688.2022.00695
   Min J, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6921, DOI 10.1109/ICCV48922.2021.00686
   Munkhdalai T, 2018, PR MACH LEARN RES, V80
   Munkhdalai T, 2017, PR MACH LEARN RES, V70
   Peng BH, 2023, PROC CVPR IEEE, P23641, DOI 10.1109/CVPR52729.2023.02264
   Punn NS, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3376922
   Ravi S., 2017, INT C LEARN REPR
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shaban Amirreza, 2017, BMVC, DOI 10.5244/C.31.167
   Shi Guangchen, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P5547, DOI 10.1145/3503161.3548218
   Shi XY, 2022, LECT NOTES COMPUT SC, V13680, P151, DOI 10.1007/978-3-031-20044-1_9
   Sun B, 2021, PROC CVPR IEEE, P7348, DOI 10.1109/CVPR46437.2021.00727
   Sun QR, 2019, PROC CVPR IEEE, P403, DOI 10.1109/CVPR.2019.00049
   Sun Yanpeng, 2022, P 36 C NEUR INF PROC
   Tang YM, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3555314
   Tian ZT, 2022, IEEE T PATTERN ANAL, V44, P1050, DOI 10.1109/TPAMI.2020.3013717
   Wang KX, 2019, IEEE I CONF COMP VIS, P9196, DOI 10.1109/ICCV.2019.00929
   Wang XL, 2023, Arxiv, DOI [arXiv:2304.03284, 10.48550/arXiv.2304.03284]
   Wang XL, 2023, Arxiv, DOI arXiv:2212.02499
   Wang Y, 2022, LECT NOTES COMPUT SC, V13689, P36, DOI 10.1007/978-3-031-19818-2_3
   Wu AM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9547, DOI 10.1109/ICCV48922.2021.00943
   Xiao TT, 2018, LECT NOTES COMPUT SC, V11209, P432, DOI 10.1007/978-3-030-01228-1_26
   Xu SX, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3548459
   Yoon SW, 2019, PR MACH LEARN RES, V97
   Yuan Y, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3321512
   Zhang B, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P2135, DOI 10.1145/3503161.3547961
   Zhang GW, 2021, ADV NEUR IN, V34
   Zhang J.-W., 2022, ADV NEURAL INFORM PR, P6575
   Zhang J, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P2586, DOI 10.1145/3503161.3547835
   Zhang L, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P2002, DOI 10.1145/3503161.3548412
   Zhang XL, 2020, IEEE T CYBERNETICS, V50, P3855, DOI 10.1109/TCYB.2020.2992433
   Zhang Y., 2022, P 31 INT JOINT C ART, P1658
   Zhuge YZ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5344, DOI 10.1145/3474085.3475658
NR 66
TC 1
Z9 1
U1 5
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2024
VL 20
IS 6
SI SI
AR 177
DI 10.1145/3643850
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OQ2T3
UT WOS:001208681800027
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Nouma, SE
   Yavuz, AA
AF Nouma, Saif E.
   Yavuz, Attila A.
TI Trustworthy and Efficient Digital Twins in Post-Quantum Era with Hybrid
   Hardware-Assisted Signatures
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Multimedia authentication; digital twins; post-quantum security;
   cyber-security
ID SECURITY; AUTHENTICATION
AB Digital Twins (DT) virtually model cyber-physical objects via sensory inputs by simulating or monitoring their behavior. Therefore, DTs usually harbor vast quantities of Internet of Things (IoT) components (e.g., sensors) that gather, process, and offload sensitive information (e.g., healthcare) to the cloud. It is imperative to ensure the trustworthiness of such sensitive information with long-term and compromise-resilient security guarantees. Digital signatures provide scalable authentication and integrity with non-repudiation and are vital tools for DTs. Post-quantum cryptography (PQC) and forward-secure signatures are two fundamental tools to offer long-term security and breach resiliency. However, NIST-PQC signature standards are exorbitantly costly for embedded DT components and are infeasible when forward-security is also considered. Moreover, NIST-PQC signatures do not admit aggregation, which is a highly desirable feature to mitigate the heavy storage and transmission burden in DTs. Finally, NIST recommends hybrid PQ solutions to enable cryptographic agility and transitional security. Yet, there is a significant gap in the state of the art in the achievement of all these advanced features simultaneously. Therefore, there is a significant need for lightweight digital signatures that offer compromise resiliency and compactness while permitting transitional security into the PQ era for DTs.
   We create a series of highly lightweight digital signatures called Hardware-ASisted Efficient Signature (HASES) that meets the above requirements. The core of HASES is a hardware-assisted cryptographic commitment construct oracle (CCO) that permits verifiers to obtain expensive commitments without signer interaction. We created three HASES schemes: PQ-HASES is a forward-secure PQ signature, LA-HASES is an efficient aggregate Elliptic-Curve signature, and HY-HASES is a novel hybrid scheme that combines PQ-HASES and LA-HASES with novel strong nesting and sequential aggregation. HASES does not require a secure-hardware on the signer. We prove that HASES schemes are secure and implemented them on commodity hardware and and 8-bit AVR ATmega2560. Our experiments confirm that PQ-HASES and LA-HASES are two magnitudes of times more signer efficient than their PQ and conventional-secure counterparts, respectively. HY-HASES outperforms NIST PQC and conventional signature combinations, offering a standard-compliant transitional solution for emerging DTs. We open-source HASES schemes for public-testing and adaptation.
C1 [Nouma, Saif E.; Yavuz, Attila A.] Univ S Florida, 3720 Spectrum Blvd, Tampa, FL 33612 USA.
C3 State University System of Florida; University of South Florida
RP Nouma, SE (corresponding author), Univ S Florida, 3720 Spectrum Blvd, Tampa, FL 33612 USA.
EM saifeddinenouma@usf.edu; attilaayavuz@usf.edu
OI Yavuz, Attila A/0000-0002-8680-9307
FU Cisco Research Award [220159]; NSF CAREER Award [CNS-1917627]
FX This research is supported by the unrestricted gift from the Cisco
   Research Award (220159), and the NSF CAREER Award CNS-1917627.
CR Aloqaily M, 2023, IEEE CONSUM ELECTR M, V12, P47, DOI 10.1109/MCE.2022.3212570
   Anthoine G, 2021, PROCEEDINGS OF THE 30TH USENIX SECURITY SYMPOSIUM, P537
   Ateniese G., 2008, P 4 INT C SEC PRIV C, P1, DOI 10.1145/1460877.1460889
   Bagchi Prithwi, 2023, IEEE Internet of Things Magazine, P52, DOI 10.1109/IOTM.001.2100215
   Barker Elaine, 2018, NIST Special Publication, V800, p56C
   Behnia R, 2021, 37TH ANNUAL COMPUTER SECURITY APPLICATIONS CONFERENCE, ACSAC 2021, P119, DOI 10.1145/3485832.3488023
   Bernstein DJ, 2019, PROCEEDINGS OF THE 2019 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (CCS'19), P2129, DOI 10.1145/3319535.3363229
   Bernstein DJ, 2012, J CRYPTOGR ENG, V2, P77, DOI 10.1007/s13389-012-0027-1
   Bindel N, 2017, LECT NOTES COMPUT SC, V10346, P384, DOI 10.1007/978-3-319-59879-6_22
   Boldyreva A, 2007, CCS'07: PROCEEDINGS OF THE 14TH ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P276
   Boneh Dan, 2019, Topics in Cryptology - CT-RSA 2019. The Cryptographers Track at the RSA Conference 2019. Proceedings: Lecture Notes in Computer Science (LNCS 11405), P251, DOI 10.1007/978-3-030-12612-4_13
   Boneh D, 2004, J CRYPTOL, V17, P297, DOI 10.1007/s00145-004-0314-9
   Boudgoust Katharina, 2023, Cryptology ePrint Archive
   Chamola V, 2021, COMPUT COMMUN, V176, P99, DOI 10.1016/j.comcom.2021.05.019
   Chen X, 2022, IEEE ICC, P1239, DOI 10.1109/ICC45855.2022.9838582
   Chen YB, 2022, LECT NOTES COMPUT SC, V13555, P385, DOI 10.1007/978-3-031-17146-8_19
   Cooper D. A., 2020, NIST SPECIAL PUBLICA, V800, P208
   Costan V, 2016, PROCEEDINGS OF THE 25TH USENIX SECURITY SYMPOSIUM, P857
   Costello C., 2016, Tech. Rep.
   Crockett E., 2019, Cryptology ePrint Archive, Report 2019/858, P1
   Dobraunig C, 2021, J CRYPTOL, V34, DOI 10.1007/s00145-021-09398-9
   Drijvers M, 2020, PROCEEDINGS OF THE 29TH USENIX SECURITY SYMPOSIUM, P2093
   Ducas L., 2018, IACR Trans. Cryptogr. Hardw. Embed. Syst., V1, P238, DOI [10.13154/tches.v2018.i1.238-268, DOI 10.13154/TCHES.V2018.I1.238-268, 10.46586/tches.v2018.i1.238-268]
   El Saddik A, 2021, IEEE INSTRU MEAS MAG, V24, P36
   El-Hindi M, 2022, 18TH INTERNATIONAL WORKSHOP ON DATA MANAGEMENT ON NEW HARDWARE, DAMON 2022, DOI 10.1145/3533737.3535098
   Espitau T, 2017, CCS'17: PROCEEDINGS OF THE 2017 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1857, DOI 10.1145/3133956.3134028
   Fouque P.A., 2018, Submiss. NISTs-Post-Quantum Cryptogr. Stand. Process, V36, P1
   Glas Benjamin, 2012, ESCAR EMB SEC CARS C, P93
   Hlsing A., 2013, Security Engineering and Intelligence Informatics, VVolume 8128, P194, DOI DOI 10.1007/978-3-642-40588-414
   Joseph D, 2022, NATURE, V605, P237, DOI 10.1038/s41586-022-04623-2
   Katz J, 2020, Introduction to Modern Cryptography
   Kim Sam, 2020, PREPRINT
   Lang F, 2021, LECT NOTES COMPUT SC, V12918, P310, DOI 10.1007/978-3-030-86890-1_18
   Li T, 2021, IEEE INTERNET THINGS, V8, P8387, DOI 10.1109/JIOT.2020.3045451
   Malkin T, 2002, LECT NOTES COMPUT SC, V2332, P400
   Mao HZ, 2016, PROCEEDINGS OF THE 15TH ACM WORKSHOP ON HOT TOPICS IN NETWORKS (HOTNETS '16), P50, DOI 10.1145/3005745.3005750
   NIST, PQC Standardization Process: Announcing Four Candidates to be Standardized, Plus Fourth Round Candidates
   Nouma SE, 2023, IEEE ICC, P4540, DOI 10.1109/ICC45041.2023.10279236
   Nouma SE, 2023, PROCEEDINGS 8TH ACM/IEEE CONFERENCE ON INTERNET OF THINGS DESIGN AND IMPLEMENTATION, IOTDI 2023, P340, DOI 10.1145/3576842.3582376
   Ott D, 2019, Arxiv, DOI [arXiv:1909.07353, DOI 10.48550/ARXIV.1909.07353]
   Ouyang WY, 2021, IEEE IPCCC, DOI 10.1109/IPCCC51483.2021.9679452
   Ozmen MO, 2019, IEEE CONF COMM NETW, P55, DOI 10.1109/cns.2019.8802675
   Paul S, 2020, SECURITY CRYPTOLOGY, V12309, P295, DOI 10.1007/978-3-030-59013-0_15
   Pointcheval D, 1996, LECT NOTES COMPUT SC, V1070, P387
   Qassim Y, 2016, INT CONF COMPUT NETW, P684
   Reyzin L, 2002, LECT NOTES COMPUT SC, V2384, P144
   Seyitoglu EUA, 2020, IEEE CONF COMM NETW
   Shaw S, 2022, J INF SECUR APPL, V69, DOI 10.1016/j.jisa.2022.103275
   Shor PW, 1997, SIAM J COMPUT, V26, P1484, DOI 10.1137/S0036144598347011
   Silva RD, 2022, INT J INF COMPUT SEC, V17, P21, DOI 10.1504/IJICS.2022.121289
   Vallent TF, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21092900
   Wang C, 2013, IEEE T COMPUT, V62, P362, DOI 10.1109/TC.2011.245
   Wang D, 2018, IEEE T DEPEND SECURE, V15, P708, DOI 10.1109/TDSC.2016.2605087
   Wang D, 2015, IEEE T DEPEND SECURE, V12, P428, DOI 10.1109/TDSC.2014.2355850
   Wang QX, 2023, IEEE T DEPEND SECURE, V20, P193, DOI 10.1109/TDSC.2021.3129512
   Wei S., 2021, Comput. Methods Progr. Biomed. Update, V1, DOI DOI 10.1016/J.CMPBUP.2021.100014
   Yavuz A. A., 2022, arXiv
   Yavuz AA, 2022, 2022 IEEE 4TH INTERNATIONAL CONFERENCE ON TRUST, PRIVACY AND SECURITY IN INTELLIGENT SYSTEMS, AND APPLICATIONS, TPS-ISA, P29, DOI 10.1109/TPS-ISA56441.2022.00014
   Yavuz AA, 2012, ACM T INFORM SYST SE, V15, DOI 10.1145/2240276.2240280
   Yavuz AA, 2018, IEEE T DEPEND SECURE, V15, P69, DOI 10.1109/TDSC.2016.2530708
   Yavuz Attila Altay, 2023, US Patent App, Patent No. [18/188,749, 18188749]
   Zhu HJ, 2009, IEEE T VEH TECHNOL, V58, P4628, DOI 10.1109/TVT.2009.2020105
NR 62
TC 1
Z9 1
U1 3
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2024
VL 20
IS 6
SI SI
AR 156
DI 10.1145/3638250
PG 30
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OQ2T3
UT WOS:001208681800006
DA 2024-08-05
ER

PT J
AU Ye, J
   Dan, M
   Jiang, WC
AF Ye, Jin
   Dan, Meng
   Jiang, Wenchao
TI A Visual Sensitivity Aware ABR Algorithm for DASH via Deep Reinforcement
   Learning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE ABR; DASH; QoE; visual sensitivity; deep reinforcement learning
ID QUALITY ASSESSMENT; VIDEO; REGION
AB In order to cope with the fluctuation of network bandwidth and provide smooth video services, adaptive video streaming technology is proposed. In particular, the adaptive bitrate (ABR) algorithm is widely used in dynamic adaptive streaming over HTTP (DASH) to improve quality of experience (QoE). However, existing ABR algorithms still ignore the inherent visual sensitivity of human visual system (HVS). As the final receiver of video, HVS has different sensitivity to the quality distortion of different video content, and video content with high visual sensitivity needs to allocate more bitrate resources. Therefore, existing ABR algorithms still have limitations in reasonably allocating bitrate and maximizing QoE. To solve this problem, this paper designs an adaptive bitrate strategy from the perspective of user vision, studies the modeling of visual sensitivity, and proposes a visual sensitivity aware ABR algorithm. We extract a set of content features and attribute features from the video, and consider the simulation of HVS to establish a total masking effect model that reflects the visual sensitivity more accurately. Further, the network status, buffer occupancy, and visual sensitivity are comprehensively considered under a deep reinforcement learning framework to select the appropriate bitrate for maximizing QoE. We implement the proposed algorithm over a realistic trace-driven evaluation and compare its performance with several latest algorithms. Experimental results show that our algorithm can align ABR strategy with visual sensitivity to achieve better QoE in high visual sensitivity content, and improves the average perceptual video quality and overall user QoE by 18.3% and 22.8%, respectively. Additionally, we prove the feasibility of our algorithm through subjective evaluation in the real environment.
C1 [Ye, Jin; Dan, Meng] Guangxi Univ, Sch Comp & Elect Informat, Guangxi Key Lab Multimedia Commun & Network Techn, Nanning 530000, Peoples R China.
   [Jiang, Wenchao] Singapore Univ Technol & Design, Informat Syst Technol & Design, Singapore 487372, Singapore.
C3 Guangxi University; Singapore University of Technology & Design
RP Ye, J (corresponding author), Guangxi Univ, Sch Comp & Elect Informat, Guangxi Key Lab Multimedia Commun & Network Techn, Nanning 530000, Peoples R China.
EM yejin@gxu.edu.cn; 1913392006@st.gxu.edu.cn; wenchaojiang@sutd.edu.sg
FU Project of End to End Transmission Theory and Key Technologies Ensuring
   Deterministic Delay [62132022]; Key Project of Guangxi Science
   Technology [2021AB06002]; Research on Load Balancing Mechanism for
   Heterogeneous Traffic in Data Center Network [61872387]
FX We would like to acknowledge the support from the Project of End to End
   Transmission Theory and Key Technologies Ensuring Deterministic Delay
   (NO.62132022), the Research on Load Balancing Mechanism for
   Heterogeneous Traffic in Data Center Network (NO.61872387), and the Key
   Project of Guangxi Science & Technology (NO.2021AB06002).
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Akamai, 2016, About us
   [Anonymous], 2017, CISCO VISUAL NETWORK
   Barman N, 2018, PROCEEDINGS OF THE 23TH ACM WORKSHOP ON PACKET VIDEO (PV'18), P7, DOI 10.1145/3210424.3210434
   Bokani A, 2015, IEEE T MULTIMEDIA, V17, P2297, DOI 10.1109/TMM.2015.2494458
   Chen KX, 2020, IEEE T NEUR NET LEAR, V31, P1747, DOI 10.1109/TNNLS.2019.2927224
   Choi LK, 2018, SIGNAL PROCESS-IMAGE, V67, P182, DOI 10.1016/j.image.2018.06.009
   Chou CH, 1995, IEEE T CIRC SYST VID, V5, P467, DOI 10.1109/76.475889
   Cichy RM, 2014, NAT NEUROSCI, V17, P455, DOI 10.1038/nn.3635
   Ciubotaru B, 2014, IEEE T BROADCAST, V60, P50, DOI 10.1109/TBC.2013.2290238
   Dan Meng, 2021, 23 IEEE INT C HIGH P
   DASH Industry Form, 2016, Reference Client 2.4.0
   Federal Communications Commission, 2016, Raw Data-Measuring Broadband America 2016
   Gadaleta M, 2017, IEEE T COGN COMMUN, V3, P703, DOI 10.1109/TCCN.2017.2755007
   Gao GY, 2018, IEEE T MULTIMEDIA, V20, P3399, DOI 10.1109/TMM.2018.2838330
   Hu SH, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3328997
   Hu SH, 2017, IEEE INT CON MULTI, P493, DOI 10.1109/ICME.2017.8019541
   Hu SH, 2014, IEEE GLOB COMM CONF, P1336, DOI 10.1109/GLOCOM.2014.7036993
   Huang Q, 2017, IEEE DATA COMPR CONF, P42, DOI 10.1109/DCC.2017.17
   Huang TY, 2014, SIGCOMM'14: PROCEEDINGS OF THE 2014 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P187, DOI 10.1145/2619239.2626296
   Huang TC, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P429, DOI 10.1145/3343031.3351014
   Huang TC, 2019, IEEE INT CON MULTI, P1678, DOI 10.1109/ICME.2019.00289
   Jiang JC, 2014, IEEE ACM T NETWORK, V22, P326, DOI 10.1109/TNET.2013.2291681
   Jin L., 2016, ELECT IMAGING, V2016, P1
   Kim J, 2017, PROC CVPR IEEE, P1969, DOI 10.1109/CVPR.2017.213
   Kim W, 2020, IEEE T IMAGE PROCESS, V29, P4219, DOI 10.1109/TIP.2020.2968283
   Kingma D. P., 2014, arXiv
   Krüger N, 2013, IEEE T PATTERN ANAL, V35, P1847, DOI 10.1109/TPAMI.2012.272
   Lee TS, 2003, J OPT SOC AM A, V20, P1434, DOI 10.1364/JOSAA.20.001434
   Lin K, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3502723
   Liu HH, 2020, IEEE T IMAGE PROCESS, V29, P641, DOI 10.1109/TIP.2019.2933743
   Liu XW, 2020, IEEE T MULTIMEDIA, V22, P949, DOI 10.1109/TMM.2019.2934425
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Luo MN, 2018, IEEE T CYBERNETICS, V48, P648, DOI 10.1109/TCYB.2017.2647904
   Mao HZ, 2017, SIGCOMM '17: PROCEEDINGS OF THE 2017 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P197, DOI 10.1145/3098822.3098843
   Mnih V, 2016, PR MACH LEARN RES, V48
   Netflix, 2018, VMAF-Video Multi-Method Assessment Fusion
   Netravali R, 2015, P USENIX ANN TECH C, P417
   Ou YF, 2014, IEEE T IMAGE PROCESS, V23, P2473, DOI 10.1109/TIP.2014.2303636
   Pan Gao, 2022, IEEE Transactions on Multimedia, V24, P1, DOI 10.1109/TMM.2020.3044458
   Riiser H., 2013, P 4 ACM MULT SYST C, P114, DOI DOI 10.1145/2483977.2483991
   Roodaki H, 2016, IEEE T MULTIMEDIA, V18, P14, DOI 10.1109/TMM.2015.2500036
   Shen X., 2020, IEEE INT CONF MULTI, P1, DOI DOI 10.1109/icmew46912.2020.9105955
   Spiteri K, 2016, IEEE INFOCOM SER, DOI 10.1109/infocom.2016.7524428
   Stockhammer T., 2011, Proceedings of the second annual ACM conference on Multimedia systems, P133
   Sun Y, 2016, PROCEEDINGS OF THE 2016 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '16), P272, DOI 10.1145/2934872.2934898
   Wang HG, 2016, IEEE IMAGE PROC, P1509, DOI 10.1109/ICIP.2016.7532610
   Wang HQ, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P6747, DOI 10.1109/ICASSP.2018.8461571
   Wang HQ, 2017, J VIS COMMUN IMAGE R, V46, P292, DOI 10.1016/j.jvcir.2017.04.009
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Watson AB, 1997, P SOC PHOTO-OPT INS, V3016, P2, DOI 10.1117/12.274501
   Wijnants M, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2449, DOI 10.1145/3343031.3351045
   Wilk S, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2983636
   Yin XQ, 2015, SIGCOMM'15: PROCEEDINGS OF THE 2015 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P325, DOI 10.1145/2785956.2787486
   Zhang DL, 2020, IEEE T CYBERNETICS, V50, P3033, DOI 10.1109/TCYB.2019.2905157
   Zhang XF, 2020, IEEE T IMAGE PROCESS, V29, P3777, DOI 10.1109/TIP.2020.2965994
   Zhou C, 2016, IEEE T MULTIMEDIA, V18, P738, DOI 10.1109/TMM.2016.2522650
   Zhou WJ, 2022, IEEE T MULTIMEDIA, V24, P2192, DOI 10.1109/TMM.2021.3077767
NR 58
TC 0
Z9 0
U1 4
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2024
VL 20
IS 3
AR 77
DI 10.1145/3591108
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6F8
UT WOS:001153381000017
DA 2024-08-05
ER

PT J
AU Zhang, YQ
   Zhang, Y
   Wang, SF
   Liang, Y
   Yin, BC
AF Zhang, Yuqing
   Zhang, Yong
   Wang, Shaofan
   Liang, Yun
   Yin, Baocai
TI Semi-supervised Video Object Segmentation Via an Edge Attention Gated
   Graph Convolutional Network
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE semi-supervised video object segmentation; superpixel; spatio-temporal
   graph model; graph convolutional network
AB Video object segmentation (VOS) exhibits heavy occlusions, large deformation, and severe motion blur. While many remarkable convolutional neural networks are devoted to the VOS task, they often mis-identify background noise as the target or output coarse object boundaries, due to the failure of mining detail information and high-order correlations of pixels within the whole video. In this work, we propose an edge attention gated graph convolutional network (GCN) for VOS. The seed point initialization and graph construction stages construct a spatio-temporal graph of the video by exploring the spatial intra-frame correlation and the temporal inter-frame correlation of superpixels. The node classification stage identifies foreground superpixels by using an edge attention gated GCN which mines higher-order correlations between superpixels and propagates features among different nodes. The segmentation optimization stage optimizes the classification of foreground superpixels and reduces segmentation errors by using a global appearance model which captures the long-term stable feature of objects. In summary, the key contribution of our framework is twofold: (a) the spatio-temporal graph representation can propagate the seed points of the first frame to subsequent frames and facilitate our framework for the semi-supervised VOS task; and (b) the edge attention gated GCN can learn the importance of each node with respect to both the neighboring nodes and the whole task with a small number of layers. Experiments on Davis 2016 and Davis 2017 datasets show that our framework achieves the excellent performance with only small training samples (45 video sequences).
C1 [Zhang, Yuqing; Zhang, Yong; Wang, Shaofan; Yin, Baocai] Beijing Univ Technol, Beijing Inst Artificial Intelligence, Fac Informat Technol, Beijing Key Lab Multimedia & Intelligent Software, 100 Pingleyuan, Beijing 100124, Peoples R China.
   [Liang, Yun] South China Agr Univ, Guangzhou Key Lab Intelligent Agr, Coll Math & Informat, Guangzhou, Peoples R China.
C3 Beijing University of Technology; South China Agricultural University
RP Zhang, Y (corresponding author), Beijing Univ Technol, Beijing Inst Artificial Intelligence, Fac Informat Technol, Beijing Key Lab Multimedia & Intelligent Software, 100 Pingleyuan, Beijing 100124, Peoples R China.
EM yuqingz@emails.bjut.edu.cn; zhangyong2010@bjut.edu.cn;
   wangshaofan@bjut.edu.cn; sdliangyun@163.com; ybc@bjut.edu.cn
RI Zhang, Yong/AAW-8880-2021
OI Zhang, Yong/0000-0001-6650-6790; WANG, SHAOFAN/0000-0002-3045-624X
FU National Key R&D Program of China [2021ZD0111902]; National Natural
   Science Foundation of China [62072015, U21B2038, U19B2039, 61772209];
   Science and Technology Planning Project of Guangdong Province
   [2019A050510034]
FX The research is supported by National Key R&D Program of China (No.
   2021ZD0111902), National Natural Science Foundation of China (No.
   62072015, No. U21B2038, U19B2039, No. 61772209), Science and Technology
   Planning Project of Guangdong Province (Nos. 2019A050510034).
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Azimi F, 2020, Arxiv, DOI arXiv:2010.05069
   Bao LC, 2018, PROC CVPR IEEE, P5977, DOI 10.1109/CVPR.2018.00626
   Bresson X, 2018, Arxiv, DOI arXiv:1711.07553
   Caelles S, 2017, PROC CVPR IEEE, P5320, DOI 10.1109/CVPR.2017.565
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chen YD, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3321507
   Chen YD, 2019, IEEE T MULTIMEDIA, V21, P1934, DOI 10.1109/TMM.2018.2890361
   Cheng Ho Kei, 2021, Advances in Neural Information Processing Systems, V34, P11781
   Fengting Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13961, DOI 10.1109/CVPR42600.2020.01398
   Gui Y, 2020, IEEE T CIRC SYST VID, V30, P4781, DOI 10.1109/TCSVT.2019.2961267
   Hao CY, 2020, NEUROCOMPUTING, V401, P28, DOI 10.1016/j.neucom.2020.03.020
   Hu L, 2021, PROC CVPR IEEE, P4142, DOI 10.1109/CVPR46437.2021.00413
   Hu YT, 2018, LECT NOTES COMPUT SC, V11212, P56, DOI 10.1007/978-3-030-01237-3_4
   Hu YT, 2017, ADV NEUR IN, V30
   Huang X., 2020, P IEEE CVF C COMP VI, P8879
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Johnander J, 2019, PROC CVPR IEEE, P8945, DOI 10.1109/CVPR.2019.00916
   Kipf T.N., 2017, INT C LEARN REPR, P1
   Li LL, 2022, PROC CVPR IEEE, P8709, DOI 10.1109/CVPR52688.2022.00852
   Li YX, 2022, INT J COMPUT VISION, V130, P2408, DOI 10.1007/s11263-022-01655-z
   Liu DZ, 2021, AAAI CONF ARTIF INTE, V35, P2100
   Liu WD, 2021, IEEE T CIRC SYST VID, V31, P1607, DOI 10.1109/TCSVT.2020.3010293
   Liu ZH, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2021.3138486
   Lu XK, 2019, PROC CVPR IEEE, P3618, DOI 10.1109/CVPR.2019.00374
   Märki N, 2016, PROC CVPR IEEE, P743, DOI 10.1109/CVPR.2016.87
   Maninis KK, 2019, IEEE T PATTERN ANAL, V41, P1515, DOI 10.1109/TPAMI.2018.2838670
   Oh SW, 2018, PROC CVPR IEEE, P7376, DOI 10.1109/CVPR.2018.00770
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Perazzi F, 2015, IEEE I CONF COMP VIS, P3227, DOI 10.1109/ICCV.2015.369
   Pont-Tuset J, 2018, Arxiv, DOI arXiv:1704.00675
   Robinson Andreas, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7404, DOI 10.1109/CVPR42600.2020.00743
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Seong Hongje, 2020, EUR C COMP VIS, P629, DOI DOI 10.1007/978-3-030-58542-638
   Seoung Wug Oh, 2019, 2019 IEEE/CVF International Conference on Computer Vision (ICCV). Proceedings, P9225, DOI 10.1109/ICCV.2019.00932
   Sun M., 2020, P IEEECVF C COMPUTER, P10791
   Tan ZT, 2021, IEEE T CIRC SYST VID, V31, P175, DOI 10.1109/TCSVT.2020.2971641
   Tsai YH, 2016, PROC CVPR IEEE, P3899, DOI 10.1109/CVPR.2016.423
   Voigtlaender P, 2019, PROC CVPR IEEE, P9473, DOI 10.1109/CVPR.2019.00971
   Wang Q, 2019, PROC CVPR IEEE, P1328, DOI 10.1109/CVPR.2019.00142
   Wang T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10777, DOI 10.1109/ICCV48922.2021.01062
   Wang WG, 2019, IEEE I CONF COMP VIS, P9235, DOI 10.1109/ICCV.2019.00933
   Wang WG, 2021, IEEE T PATTERN ANAL, V43, P2413, DOI 10.1109/TPAMI.2020.2966453
   Wang WG, 2019, IEEE T PATTERN ANAL, V41, P985, DOI 10.1109/TPAMI.2018.2819173
   Wang WG, 2017, IEEE I CONF COMP VIS, P1680, DOI 10.1109/ICCV.2017.185
   Wang WG, 2017, IEEE T IMAGE PROCESS, V26, P5645, DOI 10.1109/TIP.2017.2745098
   Wang WG, 2018, IEEE T PATTERN ANAL, V40, P20, DOI 10.1109/TPAMI.2017.2662005
   Xiankai Lu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P661, DOI 10.1007/978-3-030-58580-8_39
   Xiankai Lu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8957, DOI 10.1109/CVPR42600.2020.00898
   Xiao HX, 2020, IEEE T PATTERN ANAL, V42, P1205, DOI 10.1109/TPAMI.2018.2890659
   Xie HZ, 2021, PROC CVPR IEEE, P1286, DOI 10.1109/CVPR46437.2021.00134
   Xu N, 2018, LECT NOTES COMPUT SC, V11209, P603, DOI 10.1007/978-3-030-01228-1_36
   Yang C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7157, DOI 10.1109/ICCV48922.2021.00709
   Yeo D, 2017, PROC CVPR IEEE, P511, DOI 10.1109/CVPR.2017.62
   Yu JG, 2012, OPT LETT, V37, P4994, DOI 10.1364/OL.37.004994
   Zhang BF, 2022, IEEE T PATTERN ANAL, V44, P8082, DOI 10.1109/TPAMI.2021.3083269
   Zhang KH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8761, DOI 10.1109/ICCV48922.2021.00866
   Zhao ZJ, 2021, PATTERN RECOGN, V120, DOI 10.1016/j.patcog.2021.108120
   Zhou TF, 2022, Arxiv, DOI arXiv:2107.01153
   Zhu L, 2021, PROC CVPR IEEE, P1225, DOI 10.1109/CVPR46437.2021.00128
   Zhu WC, 2022, IEEE T CIRC SYST VID, V32, P330, DOI 10.1109/TCSVT.2021.3060015
   Zhuo T, 2020, IEEE T IMAGE PROCESS, V29, P237, DOI 10.1109/TIP.2019.2930152
NR 62
TC 0
Z9 0
U1 9
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2024
VL 20
IS 1
AR 24
DI 10.1145/3611389
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T8LL3
UT WOS:001080441800024
DA 2024-08-05
ER

PT J
AU Gao, HR
   Su, YM
   Wang, FS
   Li, HJ
AF Gao, Haorao
   Su, Yiming
   Wang, Fasheng
   Li, Haojie
TI Heterogeneous Fusion and Integrity Learning Network for RGB-D Salient
   Object Detection
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Salient object detection; heterogeneous modality fusion; capsule
   network; integrity learning
AB While significant progress has been made in recent years in the field of salient object detection, there are still limitations in heterogeneous modality fusion and salient feature integrity learning. The former is primarily attributed to a paucity of attention from researchers to the fusion of cross-scale information between different modalities during processing multi-modal heterogeneous data, coupled with an absence of methods for adaptive control of their respective contributions. The latter constraint stems from the shortcomings in existing approaches concerning the prediction of salient region's integrity. To address these problems, we propose a Heterogeneous Fusion and Integrity Learning Network for RGB-D Salient Object Detection (HFIL-Net). In response to the first challenge, we design an Advanced Semantic Guidance Aggregation (ASGA) module, which utilizes three fusion blocks to achieve the aggregation of three types of information: within-scale cross-modal, within-modal cross-scale, and cross-modal cross-scale. In addition, we embed the local fusion factor matrices in the ASGA module and utilize the global fusion factor matrices in the Multi-modal Information Adaptive Fusion module to control the contributions adaptively from different perspectives during the fusion process. For the second issue, we introduce the Feature Integrity Learning and Refinement Module. It leverages the idea of "part-whole" relationships from capsule networks to learn feature integrity and further refine the learned features through attention mechanisms. Extensive experimental results demonstrate that our proposed HFIL-Net outperforms over 17 state-of-the-art detection methods in testing across seven challenging standard datasets. Codes and results are available on https://github.com/BojueGao/HFIL-Net.
C1 [Gao, Haorao; Su, Yiming; Wang, Fasheng] Dalian Minzu Univ, Sch Informat & Commun Engn, Dalian, Liaoning, Peoples R China.
   [Li, Haojie] Shandong Univ Sci & Tech, Sch Comp Sci & Engn, Qingdao, Shandong, Peoples R China.
C3 Dalian Minzu University; Shandong University of Science & Technology
RP Wang, FS (corresponding author), Dalian Minzu Univ, Sch Informat & Commun Engn, Dalian, Liaoning, Peoples R China.
EM gaohaoran0519@163.com; suyiming812@163.com; wangfasheng@dlnu.edu.cn;
   hjli@sdust.edu.cn
OI Su, Yiming/0009-0005-2773-9969
FU National Natural Science Foundation of China [61972068, 61932020]; Joint
   Funds of Liaoning Science and Technology Program (Key RD Plan)
   [2023JH2/101800032]; Liaoning Revitalization Talents Program
   [XLYC2007023]; Taishan Scholars Program of Shandong Province
   [tsqn202312188, tstp20221128]; Fundamental Research Funds for the
   Central Universities
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61972068 and 61932020, Joint Funds of Liaoning Science
   and Technology Program (Key R&D Plan) Grant 2023JH2/101800032, the
   Liaoning Revitalization Talents Program under Grant XLYC2007023, the
   Taishan Scholars Program of Shandong Province under Grant tsqn202312188
   and tstp20221128, and the Fundamental Research Funds for the Central
   Universities.
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Chen BA, 2024, ACM T MULTIM COMPUT, V20, DOI 10.1145/3597612
   Chen G, 2023, IEEE T CIRC SYST VID, V33, P1787, DOI 10.1109/TCSVT.2022.3215979
   Chen G, 2022, IEEE T CIRC SYST VID, V32, P6308, DOI 10.1109/TCSVT.2022.3166914
   Chen H, 2023, Arxiv, DOI arXiv:2302.08052
   Chen Q, 2024, IEEE T NEUR NET LEAR, V35, P4309, DOI 10.1109/TNNLS.2022.3202241
   Chen Q, 2021, PATTERN RECOGN, V112, DOI 10.1016/j.patcog.2020.107740
   Cheng MM, 2021, INT J COMPUT VISION, V129, P2622, DOI [10.1007/s11263-021-01490-8, 10.1109/ICCV.2017.487]
   Cheng XL, 2023, IEEE T MULTIMEDIA, V25, P4253, DOI 10.1109/TMM.2022.3172852
   Cheng Y, 2014, IEEE INT CON MULTI
   Chongyi Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P225, DOI 10.1007/978-3-030-58598-3_14
   Cong RM, 2023, IEEE T MULTIMEDIA, V25, P6971, DOI 10.1109/TMM.2022.3216476
   Cong RM, 2022, IEEE T IMAGE PROCESS, V31, P6800, DOI 10.1109/TIP.2022.3216198
   Deng-Ping Fan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P275, DOI 10.1007/978-3-030-58610-2_17
   Dong JX, 2022, AUTOMAT CONSTR, V143, DOI 10.1016/j.autcon.2022.104537
   Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698
   Fan DP, 2021, IEEE T NEUR NET LEAR, V32, P2075, DOI 10.1109/TNNLS.2020.2996406
   Gao W, 2022, IEEE T CIRC SYST VID, V32, P2091, DOI 10.1109/TCSVT.2021.3082939
   Hinton G. E., 2018, INT C LEARN REPR, P3856, DOI DOI 10.2514/1.562
   Hinton GE, 2011, LECT NOTES COMPUT SC, V6791, P44, DOI 10.1007/978-3-642-21735-7_6
   Ji W, 2022, IEEE T IMAGE PROCESS, V31, P2321, DOI 10.1109/TIP.2022.3154931
   Ju R, 2014, IEEE IMAGE PROC, P1115, DOI 10.1109/ICIP.2014.7025222
   LaLonde R., 2018, CAPSULES OBJECT SEGM
   Lee M, 2022, LECT NOTES COMPUT SC, V13689, P630, DOI 10.1007/978-3-031-19818-2_36
   Li CY, 2021, IEEE T CYBERNETICS, V51, P88, DOI 10.1109/TCYB.2020.2969255
   Li JJ, 2023, INT J COMPUT VISION, V131, P855, DOI 10.1007/s11263-022-01734-1
   Lin ZQ, 2022, NEURAL NETWORKS, V147, P25, DOI 10.1016/j.neunet.2021.12.003
   Liu N, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4702, DOI 10.1109/ICCV48922.2021.00468
   Liu Y, 2022, IEEE T PATTERN ANAL, V44, P3688, DOI 10.1109/TPAMI.2021.3053577
   Liu Y, 2019, IEEE I CONF COMP VIS, P1232, DOI 10.1109/ICCV.2019.00132
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu ZY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4481, DOI 10.1145/3474085.3475601
   Liu ZY, 2022, IEEE T CIRC SYST VID, V32, P4486, DOI 10.1109/TCSVT.2021.3127149
   Liu ZY, 2023, IEEE T IMAGE PROCESS, V32, P5423, DOI 10.1109/TIP.2023.3318953
   Ma MC, 2023, IEEE T IMAGE PROCESS, V32, P1026, DOI 10.1109/TIP.2022.3232209
   Margolin R, 2014, PROC CVPR IEEE, P248, DOI 10.1109/CVPR.2014.39
   Niu YZ, 2012, PROC CVPR IEEE, P454, DOI 10.1109/CVPR.2012.6247708
   Pang YW, 2023, IEEE T IMAGE PROCESS, V32, P892, DOI 10.1109/TIP.2023.3234702
   Peng HW, 2014, LECT NOTES COMPUT SC, V8691, P92, DOI 10.1007/978-3-319-10578-9_7
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Piao YR, 2019, IEEE I CONF COMP VIS, P7253, DOI 10.1109/ICCV.2019.00735
   Rajasegaran J, 2019, PROC CVPR IEEE, P10717, DOI 10.1109/CVPR.2019.01098
   Sabour S, 2017, ADV NEUR IN, V30
   Song KC, 2023, IEEE T INSTRUM MEAS, V72, DOI 10.1109/TIM.2023.3236346
   Sun FM, 2024, IEEE T MULTIMEDIA, V26, P2249, DOI 10.1109/TMM.2023.3294003
   Tu ZZ, 2021, IEEE T IMAGE PROCESS, V30, P5678, DOI 10.1109/TIP.2021.3087412
   Wang FS, 2023, EXPERT SYST APPL, V214, DOI 10.1016/j.eswa.2022.119047
   Wang FS, 2022, MULTIMED TOOLS APPL, V81, P27879, DOI 10.1007/s11042-022-12760-z
   Wang Fasheng, 2023, IEEE Trans. Artif. Intell., V2023, P1
   Wang FY, 2022, IEEE T IMAGE PROCESS, V31, P1285, DOI 10.1109/TIP.2022.3140606
   Wang J, 2022, IEEE T CIRC SYST VID, V32, P2949, DOI 10.1109/TCSVT.2021.3099120
   Wang RM, 2024, ACM T MULTIM COMPUT, V20, DOI 10.1145/3624747
   Wang WG, 2021, IEEE T PATTERN ANAL, V43, P2413, DOI 10.1109/TPAMI.2020.2966453
   Wang WG, 2019, IEEE T PATTERN ANAL, V41, P1531, DOI 10.1109/TPAMI.2018.2840724
   Wang YB, 2022, COMPUT J, V65, P1846, DOI 10.1093/comjnl/bxab026
   Wu YH, 2022, IEEE T PATTERN ANAL, V44, P10261, DOI 10.1109/TPAMI.2021.3134684
   Wu ZW, 2023, IEEE T IMAGE PROCESS, V32, P2160, DOI 10.1109/TIP.2023.3263111
   YangWang Yanqing, 2022, P AS C COMP VIS ACCV, P3672
   Yao SY, 2023, IEEE T IMAGE PROCESS, V32, P5340, DOI 10.1109/TIP.2023.3315511
   Zade AAT, 2022, COMPUT BIOL MED, V148, DOI 10.1016/j.compbiomed.2022.105917
   Zeng C, 2023, NEUROCOMPUTING, V559, DOI 10.1016/j.neucom.2023.126779
   Zeng Y, 2019, IEEE I CONF COMP VIS, P7222, DOI 10.1109/ICCV.2019.00732
   Zhang DW, 2019, INT J COMPUT VISION, V127, P363, DOI 10.1007/s11263-018-1112-4
   Zhang M, 2023, IEEE T MULTIMEDIA, V25, P5142, DOI 10.1109/TMM.2022.3187856
   Zhou H, 2023, IEEE T IMAGE PROCESS, V32, P2593, DOI 10.1109/TIP.2023.3270801
   Zhou WJ, 2022, IEEE T CIRC SYST VID, V32, P1224, DOI 10.1109/TCSVT.2021.3077058
   Zhu CB, 2019, IEEE INT CON MULTI, P199, DOI 10.1109/ICME.2019.00042
   Zhu CB, 2017, IEEE INT CONF COMP V, P3008, DOI 10.1109/ICCVW.2017.355
   Zhuge MC, 2023, IEEE T PATTERN ANAL, V45, P3738, DOI 10.1109/TPAMI.2022.3179526
NR 69
TC 1
Z9 1
U1 3
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2024
VL 20
IS 7
SI SI
AR 223
DI 10.1145/3656476
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL0Q6
UT WOS:001234494100038
OA Bronze
DA 2024-08-05
ER

PT J
AU Han, TT
   Zhou, Q
   Yu, J
   Yu, Z
   Zhang, JH
   Zhao, SC
AF Han, Tingting
   Zhou, Quan
   Yu, Jun
   Yu, Zhou
   Zhang, Jianhui
   Zhao, Sicheng
TI Effective Video Summarization by Extracting Parameter-Free Motion
   Attention
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Video summarization; parameter-free; motion attention; feature fusion;
   multi-head attention
AB Video summarization remains a challenging task despite increasing research efforts. Traditional methods focus solely on long-range temporalmodeling of video frames, overlooking important local motion information that cannot be captured by frame-level video representations. In this article, we propose the Parameter-free Motion AttentionModule (PMAM) to exploit the crucial motion clues potentially contained in adjacent video frames, using a multi-head attention architecture. The PMAM requires no additional training for model parameters, leading to an efficient and effective understanding of video dynamics. Moreover, we introduce the Multi-feature Motion Attention Network (MMAN), integrating the PMAM with local and global multi-head attention based on object-centric and scene-centric video representations. The synergistic combination of local motion information, extracted by the proposed PMAM, with long-range interactions modeled by the local and global multi-head attention mechanism, can significantly enhance the performance of video summarization. Extensive experimental results on the benchmark datasets, SumMe and TVSum, demonstrate that the proposed MMAN outperforms other state-of-the-art methods, resulting in remarkable performance gains.
C1 [Han, Tingting; Yu, Jun] Hangzhou Dianzi Univ, Coll Comp Sci, Hangzhou, Peoples R China.
   [Zhou, Quan; Yu, Zhou; Zhang, Jianhui] Hangzhou Dianzi Univ, Hangzhou, Zhejiang, Peoples R China.
   [Zhao, Sicheng] Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol, Beijing, Peoples R China.
C3 Hangzhou Dianzi University; Hangzhou Dianzi University; Tsinghua
   University
RP Yu, J (corresponding author), Hangzhou Dianzi Univ, Coll Comp Sci, Hangzhou, Peoples R China.
EM ttinghan@hdu.edu.cn; zhouq@hdu.edu.cn; yujun@hdu.edu.cn; yuz@hdu.edu.cn;
   jh_zhang@hdu.edu.cn; schzhao@tsinghua.edu.cn
OI Yu, Zhou/0000-0001-8407-1137
FU Zhejiang Provincial Natural Science Foundation of China [LR22F020001,
   LDT23F02025F02]; National Natural Science Foundation of China (NSFC)
   [62125201, 62020106007, 62072147]
FX This work was supported in part by the Zhejiang Provincial Natural
   Science Foundation of China under grant nos. LR22F020001 and
   LDT23F02025F02 and the National Natural Science Foundation of China
   (NSFC) under grant nos. 62125201, 62020106007 and 62072147.
CR Apostolidis E, 2021, IEEE INT SYM MULTIM, P226, DOI 10.1109/ISM52913.2021.00045
   Cai SJ, 2018, LECT NOTES COMPUT SC, V11218, P193, DOI 10.1007/978-3-030-01264-9_12
   Casas LL, 2019, LECT NOTES COMPUT SC, V11296, P67, DOI 10.1007/978-3-030-05716-9_6
   Chen Yiyan, 2019, P ACM MULTIMEDIA ASI, P1
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Elfeki M, 2019, IEEE WINT CONF APPL, P754, DOI 10.1109/WACV.2019.00085
   Elhamifar E, 2016, IEEE T PATTERN ANAL, V38, P2182, DOI 10.1109/TPAMI.2015.2511748
   Fajtl J, 2019, LECT NOTES COMPUT SC, V11367, P39, DOI 10.1007/978-3-030-21074-8_4
   de Avila SEF, 2011, PATTERN RECOGN LETT, V32, P56, DOI 10.1016/j.patrec.2010.08.004
   Fu H, 2021, INT C PATT RECOG, P446, DOI 10.1109/ICPR48806.2021.9412057
   Fu H, 2021, PATTERN RECOGN LETT, V143, P19, DOI 10.1016/j.patrec.2020.12.016
   Fu TJ, 2019, IEEE WINT CONF APPL, P1579, DOI 10.1109/WACV.2019.00173
   Gao JY, 2021, IEEE T MULTIMEDIA, V23, P3203, DOI 10.1109/TMM.2020.3021980
   Gao JY, 2021, IEEE T PATTERN ANAL, V43, P3476, DOI 10.1109/TPAMI.2020.2985708
   Gao Junyu, 2023, IEEE Transactions on Pattern Analysis and Machine Intelligence
   Guan GL, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2632267
   Gygli M, 2014, LECT NOTES COMPUT SC, V8695, P505, DOI 10.1007/978-3-319-10584-0_33
   Hadi Y., 2006, Applied Computing 2006. 21st Annual ACM Symposium on Applied Computing, P1400, DOI 10.1145/1141277.1141601
   Han TT, 2022, IMAGE VISION COMPUT, V126, DOI 10.1016/j.imavis.2022.104532
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang C, 2020, IEEE T CIRC SYST VID, V30, P577, DOI 10.1109/TCSVT.2019.2890899
   Huang JH, 2023, Arxiv, DOI arXiv:2311.12159
   Ji Z, 2021, IEEE T NEUR NET LEAR, V32, P1765, DOI 10.1109/TNNLS.2020.2991083
   Ji Z, 2020, IEEE T CIRC SYST VID, V30, P1709, DOI 10.1109/TCSVT.2019.2904996
   Jung Y, 2019, AAAI CONF ARTIF INTE, P8537
   Jungin Park, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P647, DOI 10.1007/978-3-030-58595-2_39
   Kanafani H, 2021, PROCEEDINGS OF THE 2021 INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR '21), P466, DOI 10.1145/3460426.3463597
   Li P, 2021, PATTERN RECOGN, V111, DOI 10.1016/j.patcog.2020.107677
   Li WX, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3485472
   Li ZT, 2021, IEEE WINT CONF APPL, P3238, DOI 10.1109/WACV48630.2021.00328
   Liang GQ, 2022, NEUROCOMPUTING, V467, P1, DOI 10.1016/j.neucom.2021.09.015
   Liu YT, 2019, IEEE IMAGE PROC, P3377, DOI [10.1109/ICIP.2019.8803639, 10.1109/icip.2019.8803639]
   Liu Yen-Ting, 2020, P AS C COMP VIS
   Mahasseni B, 2017, PROC CVPR IEEE, P2982, DOI 10.1109/CVPR.2017.318
   Minaidi Maria Nektaria, 2023, 2023 31st European Signal Processing Conference (EUSIPCO), P571, DOI 10.23919/EUSIPCO58844.2023.10289808
   Narasimhan Medhini, 2021, Advances in Neural Information Processing Systems, V34
   Panda R, 2017, IEEE I CONF COMP VIS, P3677, DOI 10.1109/ICCV.2017.395
   Rochan M, 2018, LECT NOTES COMPUT SC, V11216, P358, DOI 10.1007/978-3-030-01258-8_22
   Sheng-Hua Zhong, 2022, ACM Transactions on Multimedia Computing, Communications and Applications, V18, DOI 10.1145/3477538
   Song YL, 2015, PROC CVPR IEEE, P5179, DOI 10.1109/CVPR.2015.7299154
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tang H, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3571735
   Wang JY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4023, DOI 10.1145/3394171.3414064
   Wu GD, 2021, Arxiv, DOI arXiv:2109.02625
   Yao T, 2016, PROC CVPR IEEE, P982, DOI 10.1109/CVPR.2016.112
   Yu Z, 2023, PROC CVPR IEEE, P23191, DOI 10.1109/CVPR52729.2023.02221
   Yu Z, 2021, NEUROCOMPUTING, V445, P72, DOI 10.1016/j.neucom.2021.03.026
   Yuan L, 2020, IEEE T MULTIMEDIA, V22, P2711, DOI 10.1109/TMM.2019.2959451
   Yuan Y, 2019, IEEE ACCESS, V7, P64676, DOI 10.1109/ACCESS.2019.2916989
   Zhang GZ, 2023, PROC CVPR IEEE, P5682, DOI 10.1109/CVPR52729.2023.00550
   Zhang K, 2018, LECT NOTES COMPUT SC, V11212, P391, DOI 10.1007/978-3-030-01237-3_24
   Zhang K, 2016, LECT NOTES COMPUT SC, V9911, P766, DOI 10.1007/978-3-319-46478-7_47
   Zhao B, 2022, NEUROCOMPUTING, V468, P360, DOI 10.1016/j.neucom.2021.10.039
   Zhao B, 2022, IEEE T PATTERN ANAL, V44, P2793, DOI 10.1109/TPAMI.2021.3072117
   Zhao B, 2021, IEEE T IND ELECTRON, V68, P3629, DOI 10.1109/TIE.2020.2979573
   Zhao B, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P863, DOI 10.1145/3123266.3123328
   Zhao B, 2018, PROC CVPR IEEE, P7405, DOI 10.1109/CVPR.2018.00773
   Zhao B, 2014, PROC CVPR IEEE, P2513, DOI 10.1109/CVPR.2014.322
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
   Zhou KY, 2018, AAAI CONF ARTIF INTE, P7582
   Zhu WC, 2022, IEEE T IMAGE PROCESS, V31, P3017, DOI 10.1109/TIP.2022.3163855
   Zhu WC, 2022, PATTERN RECOGN, V122, DOI 10.1016/j.patcog.2021.108312
   Zhu WC, 2021, IEEE T IMAGE PROCESS, V30, P948, DOI 10.1109/TIP.2020.3039886
   Zhu XG, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3491228
NR 64
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2024
VL 20
IS 7
SI SI
AR 219
DI 10.1145/3654670
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL0Q6
UT WOS:001234494100034
OA Bronze
DA 2024-08-05
ER

PT J
AU Antil, A
   Dhiman, C
AF Antil, Aashania
   Dhiman, Chhavi
TI MF<SUP>2</SUP>ShrT: Multimodal Feature Fusion Using Shared Layered
   Transformer for Face Anti-spoofing
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Face anti-spoofing; presentation attack detection; multimodal; vision
   transformer
ID PRESENTATION ATTACK DETECTION
AB In recent times, Face Anti-spoofing (FAS) has gained significant attention in both academic and industrial domains. Although various convolutional neural network (CNN)-based solutions have emerged, multimodal approaches incorporating RGB, depth, and information retrieval (IR) have exhibited better performance than unimodal classifiers. The increasing veracity of modern presentation attack instruments results in a persistent need to enhance the performance of such models. Recently, self-attention-based vision transformers (ViT) have become a popular choice in this field. Their fundamental aspects formultimodal FAS have not been thoroughly explored yet. Therefore, we propose a novel framework for FAS called MF(2)ShrT, which is based on a pretrained vision transformer. The proposed framework uses overlap patches and parameter sharing in the ViT network, allowing it to utilize multiple modalities in a computationally efficient manner. Furthermore, to effectively fuse intermediate features from different encoders of each ViT, we explore a T-encoder-based hybrid feature block enabling the system to identify correlations and dependencies across different modalities. MF(2)ShrT outperforms conventional vision transformers and achieves state-of-the-art performance on benchmarks CASIA-SURF and WMCA, demonstrating the efficiency of transformer-based models for presentation attack detection PAD).
C1 [Antil, Aashania; Dhiman, Chhavi] Delhi Technol Univ, Dept Elect & Commun Engn, New Delhi, India.
C3 Delhi Technological University
RP Dhiman, C (corresponding author), Delhi Technol Univ, Dept Elect & Commun Engn, New Delhi, India.
EM aashiantil40@gmail.com; chhavi.dhiman@dtu.ac.in
OI Dhiman, Chhavi/0000-0002-3401-596X
CR Almeida WR, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0238058
   Anjos A., 2011, INT JOINT C BIOM IJC
   Antil A, 2023, MULTIMEDIA SYST, V29, P1361, DOI 10.1007/s00530-023-01060-7
   Atoum Y, 2017, 2017 IEEE INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB), P319, DOI 10.1109/BTAS.2017.8272713
   Brown TB, 2020, Arxiv, DOI [arXiv:2005.14165, 10.48550/arXiv.2005.14165, DOI 10.48550/ARXIV.2005.14165]
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Carion N, 2020, Arxiv, DOI [arXiv:2005.12872, 10.48550/ARXIV.2005.12872]
   Chen HN, 2020, IEEE T INF FOREN SEC, V15, P578, DOI 10.1109/TIFS.2019.2922241
   Chingovska I., 2012, BIOSIG P INT C BIOM
   Dosovitskiy A., 2021, ICLR
   Feng LT, 2016, J VIS COMMUN IMAGE R, V38, P451, DOI 10.1016/j.jvcir.2016.03.019
   George A., 2021, arXiv
   George A, 2021, 2021 INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB 2021), DOI 10.1109/IJCB52358.2021.9484333
   George A, 2021, IEEE T INF FOREN SEC, V16, P361, DOI 10.1109/TIFS.2020.3013214
   George A, 2020, IEEE T INF FOREN SEC, V15, P42, DOI 10.1109/TIFS.2019.2916652
   Georgescu AL, 2019, 2019 10TH INTERNATIONAL CONFERENCE ON SPEECH TECHNOLOGY AND HUMAN-COMPUTER DIALOGUE (SPED), DOI 10.1109/sped.2019.8906555
   Han K, 2023, IEEE T PATTERN ANAL, V45, P87, DOI 10.1109/TPAMI.2022.3152247
   He KM, 2015, Arxiv, DOI [arXiv:1512.03385, DOI 10.48550/ARXIV.1512.03385]
   Huang G, 2018, Arxiv, DOI [arXiv:1608.06993, 10.48550/arXiv.1608.06993]
   Li YD, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3432817
   Li Z, 2021, Arxiv, DOI arXiv:2110.09108
   Lin TY, 2018, Arxiv, DOI [arXiv:1708.02002, 10.1109/ICCV.2017.324, 10.48550/arXiv.1708.02002]
   Liu A., 2022, P 31 INT JOINT C ART
   Liu A., 2019, arXiv
   Liu AJ, 2023, Arxiv, DOI arXiv:2305.03277
   Liu AJ, 2022, IEEE T INF FOREN SEC, V17, P2497, DOI 10.1109/TIFS.2022.3188149
   Liu AJ, 2021, IEEE WINT CONF APPL, P1178, DOI 10.1109/WACV48630.2021.00122
   Liu AJ, 2021, IEEE T INF FOREN SEC, V16, P2759, DOI 10.1109/TIFS.2021.3065495
   Liu Q., 2021, P 2021 INT C PATT RE
   Liu WD, 2021, INT J ENERG RES, V45, P11002, DOI 10.1002/er.6584
   Liu YJ, 2018, PROC CVPR IEEE, P389, DOI 10.1109/CVPR.2018.00048
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Ming Z., arXiv
   Nair V., 2010, ICML, P807
   Nikisins O, 2019, INT CONF BIOMETR
   Parkin A, 2019, IEEE COMPUT SOC CONF, P1617, DOI 10.1109/CVPRW.2019.00204
   Ramachandra R, 2017, ACM COMPUT SURV, V50, DOI 10.1145/3038924
   Shen T, 2019, IEEE COMPUT SOC CONF, P1611, DOI 10.1109/CVPRW.2019.00203
   Shifeng Zhang, 2020, IEEE Transactions on Biometrics, Behavior, and Identity Science, V2, P182, DOI 10.1109/TBIOM.2020.2973001
   Takase S, 2022, Arxiv, DOI arXiv:2104.06022
   Tolstikhin I, 2021, ADV NEUR IN, V34
   Vaswani A, 2023, Arxiv, DOI arXiv:1706.03762
   Wang GQ, 2019, IEEE COMPUT SOC CONF, P1584, DOI 10.1109/CVPRW.2019.00200
   Wang WH, 2022, IEEE T INF FOREN SEC, V17, P2284, DOI 10.1109/TIFS.2022.3183398
   Wang Zhuo, 2022, IEEE Transactions on Biometrics, Behavior, and Identity Science, V4, P439, DOI 10.1109/TBIOM.2022.3184500
   Wang Z, 2022, IEEE T INF FOREN SEC, V17, P1254, DOI 10.1109/TIFS.2022.3158062
   Wang ZY, 2023, ACM T SENSOR NETWORK, V19, DOI 10.1145/3579093
   Yang Q, 2020, Arxiv, DOI arXiv:2004.11744
   Yu J, 2020, IEEE T CIRC SYST VID, V30, P4467, DOI 10.1109/TCSVT.2019.2947482
   Yu J, 2017, IEEE T CYBERNETICS, V47, P4014, DOI 10.1109/TCYB.2016.2591583
   Yu Z., 2021, IEEE Transactions on Biometrics, Behavior and Identity Science, V2021
   Yu Z., 2023, arXiv
   Yu ZT, 2020, Arxiv, DOI arXiv:2004.08388
   Yu ZT, 2021, Arxiv, DOI arXiv:2104.07419
   Yu ZT, 2020, PROC CVPR IEEE, P5294, DOI 10.1109/CVPR42600.2020.00534
   Zhang BH, 2007, IEEE T IMAGE PROCESS, V16, P57, DOI 10.1109/TIP.2006.884956
   Zhang P, 2019, IEEE COMPUT SOC CONF, P1574, DOI 10.1109/CVPRW.2019.00199
   Zhang SH, 2019, PROC CVPR IEEE, P889, DOI 10.1109/CVPR.2019.00098
   Zheng SX, 2021, Arxiv, DOI [arXiv:2012.15840, DOI 10.48550/ARXIV.2012.15840]
   Zhiwei Zhang, 2012, 2012 5th IAPR International Conference on Biometrics (ICB), P26, DOI 10.1109/ICB.2012.6199754
   Zhong YY, 2021, Arxiv, DOI arXiv:2103.14803
NR 61
TC 3
Z9 3
U1 5
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2024
VL 20
IS 6
SI SI
AR 172
DI 10.1145/3640817
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OQ2T3
UT WOS:001208681800022
DA 2024-08-05
ER

PT J
AU Zhang, SP
   Zhao, CQ
   Basu, A
AF Zhang, Shupei
   Zhao, Chenqiu
   Basu, Anup
TI Principal Component Approximation Network for Image Compression
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image compression; neural network; decomposition
AB In this work, we propose a novel principal component approximation network (PCANet) for image compression. The proposed network is based on the assumption that a set of images can be decomposed into several shared feature matrices, and an image can be reconstructed by the weighted sum of these matrices. The proposed PCANet is specifically devised to learn and approximate these feature matrices and weight vectors, which are used to encode images for compression. Unlike previous deep learning-based methods, a distinctive aspect of our approach is its consideration of network size in the bit-rate computation. Despite this inclusion, our proposed method yields promising results. Through extensive experiments conducted on standard datasets, we demonstrate the effectiveness of our approach in comparison to state-of-the-art techniques. To the best of our knowledge, this is the first machine learning approach that includes the size of networks during bitrate computation in image compression.
C1 [Zhang, Shupei; Zhao, Chenqiu; Basu, Anup] Univ Alberta, 116 St & 85 Ave, Edmonton, AB T6R 2E8, Canada.
C3 University of Alberta
RP Zhang, SP (corresponding author), Univ Alberta, 116 St & 85 Ave, Edmonton, AB T6R 2E8, Canada.
EM shupei2@ualberta.ca; chenqiu1@ualberta.ca; basu@ualberta.ca
CR Agustsson E., 2018, arXiv
   Agustsson E, 2017, ADV NEUR IN, V30
   Ahanonu E, 2018, IEEE DATA COMPR CONF, P395, DOI 10.1109/DCC.2018.00048
   AHMED N, 1974, IEEE T COMPUT, VC 23, P90, DOI 10.1109/T-C.1974.223784
   Amirpour H, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3471905
   Andrews H.C., 1968, Proc. Hawaii Int.Conf. System Sciences, P677
   Baig MH, 2017, ADV NEUR IN, V30
   Balle Johannes, 2016, P 5 INT C LEARNING R
   Balle Johannes, 2018, P 6 INT C LEARNING R
   Bjontegaard G., 2001, ITU SG16 Doc. VCEG-M33
   Chen T, 2020, INT CONF ACOUST SPEE, P2163, DOI [10.1109/icassp40776.2020.9053885, 10.1109/ICASSP40776.2020.9053885]
   Chen T, 2021, IEEE T IMAGE PROCESS, V30, P3179, DOI 10.1109/TIP.2021.3058615
   Cheng ZX, 2019, PROC CVPR IEEE, P10063, DOI 10.1109/CVPR.2019.01031
   Christopoulos C, 2000, IEEE T CONSUM ELECTR, V46, P1103, DOI 10.1109/30.920468
   Gao Ge, 2021, P IEEE CVF INT C COM, P14677
   Gregor K, 2016, ADV NEUR IN, V29
   Gregor K, 2015, PR MACH LEARN RES, V37, P1462
   He DL, 2022, PROC CVPR IEEE, P5708, DOI 10.1109/CVPR52688.2022.00563
   Hu YY, 2022, IEEE T PATTERN ANAL, V44, P4194, DOI 10.1109/TPAMI.2021.3065339
   HUFFMAN DA, 1952, P IRE, V40, P1098, DOI 10.1109/JRPROC.1952.273898
   Johnston N, 2018, PROC CVPR IEEE, P4385, DOI 10.1109/CVPR.2018.00461
   Kim JH, 2022, PROC CVPR IEEE, P5982, DOI 10.1109/CVPR52688.2022.00590
   Lee JH, 2022, PROC CVPR IEEE, P16092, DOI 10.1109/CVPR52688.2022.01564
   Lee Jooyoung, 2019, P 7 INT C LEARNING R
   Lei J., 2022, CVPR, P19669
   Li M, 2018, PROC CVPR IEEE, P3214, DOI 10.1109/CVPR.2018.00339
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Ma SW, 2020, IEEE T CIRC SYST VID, V30, P1683, DOI 10.1109/TCSVT.2019.2910119
   Mentzer F, 2018, PROC CVPR IEEE, P4394, DOI 10.1109/CVPR.2018.00462
   Minnen D, 2018, ADV NEUR IN, V31
   Muckley Matthew, 2021, Neural Compression
   Pate Y, 2021, IEEE WINT CONF APPL, P227, DOI 10.1109/WACV48630.2021.00027
   PRATT WK, 1969, P IEEE, V57, P58, DOI 10.1109/PROC.1969.6869
   Rhee H, 2022, PROC CVPR IEEE, P6023, DOI 10.1109/CVPR52688.2022.00594
   Rippel O, 2017, PR MACH LEARN RES, V70
   Shen JL, 2021, INFORM SCIENCES, V569, P469, DOI 10.1016/j.ins.2020.11.026
   Shen JL, 2015, PATTERN RECOGN, V48, P3227, DOI 10.1016/j.patcog.2015.02.027
   Song M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2360, DOI 10.1109/ICCV48922.2021.00238
   Theis Lucas, 2017, P 5 INT C LEARNING R
   Toderici George, 2017, P IEEE C COMPUTER VI
   WALLACE GK, 1992, IEEE T CONSUM ELECTR, V38, pR18, DOI 10.1109/30.125072
   Wang DZ, 2022, PROC CVPR IEEE, P17358, DOI 10.1109/CVPR52688.2022.01686
   Wang Y, 2014, IEEE COMPUT SOC CONF, P393, DOI 10.1109/CVPRW.2014.126
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   WITTEN IH, 1987, COMMUN ACM, V30, P520, DOI 10.1145/214762.214771
   Wödlinger M, 2022, PROC CVPR IEEE, P651, DOI 10.1109/CVPR52688.2022.00074
   Wu LR, 2020, IEEE WINT CONF APPL, P2323, DOI [10.1109/wacv45572.2020.9093387, 10.1109/WACV45572.2020.9093387]
   Zheng Huiming, 2023, P ACM INT C MULTIMED
   Zhengxue Cheng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7936, DOI 10.1109/CVPR42600.2020.00796
   Zhou Lei, 2018, CVPR WORKSHOPS, P2617
   Zhu XS, 2022, PROC CVPR IEEE, P17591, DOI 10.1109/CVPR52688.2022.01709
   Zou RJ, 2022, PROC CVPR IEEE, P17471, DOI 10.1109/CVPR52688.2022.01697
NR 53
TC 0
Z9 0
U1 5
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2024
VL 20
IS 5
AR 121
DI 10.1145/3637490
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MF3R9
UT WOS:001192177900001
DA 2024-08-05
ER

PT J
AU Zhang, ZC
   Sun, W
   Zhou, YJ
   Jia, J
   Zhang, ZC
   Liu, J
   Min, XK
   Zhai, GT
AF Zhang, Zicheng
   Sun, Wei
   Zhou, Yingjie
   Jia, Jun
   Zhang, Zhichao
   Liu, Jing
   Min, Xiongkuo
   Zhai, Guangtao
TI Subjective and Objective Quality Assessment for in-the-Wild Computer
   Graphics Images
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Computer graphics images; in-the-wild distortions; image quality
   assessment; no-reference
AB Computer graphics images (CGIs) are artificially generated by means of computer programs and are widely perceived under various scenarios, such as games, streaming media, etc. In practice, the quality of CGIs consistently suffers from poor rendering during production, inevitable compression artifacts during the transmission of multimedia applications, and low aesthetic quality resulting from poor composition and design. However, few works have been dedicated to dealing with the challenge of computer graphics image quality assessment (CGIQA). Most image quality assessment (IQA) metrics are developed for natural scene images (NSIs) and validated on databases consisting of NSIs with synthetic distortions, which are not suitable for in-the-wild CGIs. To bridge the gap between evaluating the quality of NSIs and CGIs, we construct a large-scale in-the-wild CGIQA database consisting of 6,000 CGIs (CGIQA-6k) and carry out the subjective experiment in a well-controlled laboratory environment to obtain the accurate perceptual ratings of the CGIs. Then, we propose an effective deep learning-based no-reference (NR) IQA model by utilizing both distortion and aesthetic quality representation. Experimental results show that the proposed method outperforms all other state-of-the-art NR IQA methods on the constructed CGIQA-6k database and other CGIQA-related databases. The database is released at https://github.com/zzc-1998/CGIQA6K.
C1 [Zhang, Zicheng; Sun, Wei; Zhou, Yingjie; Jia, Jun; Zhang, Zhichao; Min, Xiongkuo; Zhai, Guangtao] Shanghai Jiao Tong Univ, 800 Dongchuan Rd, Shanghai 200240, Peoples R China.
   [Liu, Jing] Tianjin Univ, 92 Weijin Rd, Tianjin 300372, Peoples R China.
C3 Shanghai Jiao Tong University; Tianjin University
RP Sun, W; Zhai, GT (corresponding author), Shanghai Jiao Tong Univ, 800 Dongchuan Rd, Shanghai 200240, Peoples R China.
EM zzc1998@sjtu.edu.cn; sunguwei@sjtu.edu.cn; zyj2000@sjtu.edu.cn;
   jiajun0302@sjtu.edu.cn; liquortect@sjtu.edu.cn; jliu_tju@tju.edu.cn;
   minxiongkuo@sjtu.edu.cn; zhaiguangtao@sjtu.edu.cn
RI ZHOU, YINGJIE/KLE-8614-2024; Zhai, Guangtao/X-5949-2019
OI Zhai, Guangtao/0000-0001-8165-9322; Zhang, Zhichao/0000-0003-1466-6383;
   Sun, Wei/0000-0001-8162-1949; Zhou, Yingjie/0009-0001-3915-8257; Zhang,
   Zicheng/0000-0002-7247-7938
CR Golestaneh SA, 2020, Arxiv, DOI arXiv:2006.03783
   [Anonymous], 2012, 3D movie making: stereoscopic digital cinema from script to screen
   Bai WM, 2021, IEEE T IMAGE PROCESS, V30, P8439, DOI 10.1109/TIP.2021.3114989
   Barman N, 2019, IEEE ACCESS, V7, P74511, DOI 10.1109/ACCESS.2019.2920477
   Barman Nabajeet, 2018, 2018 16 ANN WORKSHOP, P1
   Chen H., 2021, IEEE T MULTIMEDIA
   Chen KT, 2014, IEEE T MULTIMEDIA, V16, P480, DOI 10.1109/TMM.2013.2291532
   Cui CR, 2019, IEEE T MULTIMEDIA, V21, P1209, DOI 10.1109/TMM.2018.2875357
   Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Denisova A, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P145, DOI 10.1145/2702123.2702256
   Dhar S, 2011, PROC CVPR IEEE, P1657, DOI 10.1109/CVPR.2011.5995467
   Fan Y, 2022, IEEE INT WORKSH MULT, DOI 10.1109/MMSP55362.2022.9949359
   Gintere I, 2019, SABIED INTEGR IZGL, P346, DOI 10.17770/sie2019vol4.3674
   GOODFELLOW I, 2014, ADV NEURAL INFORM PR, V27
   Greenberg D. P., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P477, DOI 10.1145/258734.258914
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hosu V, 2020, IEEE T IMAGE PROCESS, V29, P4041, DOI 10.1109/TIP.2020.2967829
   Hosu V, 2017, INT WORK QUAL MULTIM
   Hou JW, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P816, DOI 10.1145/3394171.3413695
   Hu B, 2023, NEUROCOMPUTING, V547, DOI 10.1016/j.neucom.2023.126378
   Hu B, 2020, SIGNAL PROCESS-IMAGE, V85, DOI 10.1016/j.image.2020.115839
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jiang QP, 2022, IEEE T CIRC SYST VID, V32, P5959, DOI 10.1109/TCSVT.2022.3164918
   Jiang QP, 2022, IEEE T IMAGE PROCESS, V31, P2279, DOI 10.1109/TIP.2022.3154588
   Kao YY, 2017, IEEE T IMAGE PROCESS, V26, P1482, DOI 10.1109/TIP.2017.2651399
   Ke JJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5128, DOI 10.1109/ICCV48922.2021.00510
   Ke Y., 2006, CVPR, P419, DOI DOI 10.1109/CVPR.2006.303
   Kingma D. P., 2014, arXiv
   Kong S, 2016, LECT NOTES COMPUT SC, V9905, P662, DOI 10.1007/978-3-319-46448-0_40
   Laghari AA, 2019, MULTIAGENT GRID SYST, V15, P289, DOI 10.3233/MGS-190313
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Li DQ, 2019, IEEE T MULTIMEDIA, V21, P1221, DOI 10.1109/TMM.2018.2875354
   Li L., 2023, IEEE Trans. Circuits Syst. Video Technol.
   Li LD, 2016, IEEE T CYBERNETICS, V46, P39, DOI 10.1109/TCYB.2015.2392129
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Lin HH, 2019, INT WORK QUAL MULTIM
   Liu YX, 2023, IEEE T CIRC SYST VID, V33, P1043, DOI 10.1109/TCSVT.2022.3209007
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu Z, 2022, PROC CVPR IEEE, P11966, DOI 10.1109/CVPR52688.2022.01167
   Lu W, 2023, IEEE T BROADCAST, V69, P406, DOI 10.1109/TBC.2022.3221689
   Lu X, 2015, IEEE I CONF COMP VIS, P990, DOI 10.1109/ICCV.2015.119
   Ma S, 2017, PROC CVPR IEEE, P722, DOI 10.1109/CVPR.2017.84
   Mackay D., 2017, FANTASY ROLE PLAYING
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Min XK, 2018, IEEE T BROADCAST, V64, P508, DOI 10.1109/TBC.2018.2816783
   Min XK, 2017, IEEE T IMAGE PROCESS, V26, P5462, DOI 10.1109/TIP.2017.2735192
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Narvekar ND, 2011, IEEE T IMAGE PROCESS, V20, P2678, DOI 10.1109/TIP.2011.2131660
   Nishiyama M, 2011, PROC CVPR IEEE, P33, DOI 10.1109/CVPR.2011.5995539
   Park J, 2018, Arxiv, DOI [arXiv:1807.06514, 10.48550/arXiv.1807.06514]
   Patton RM, 2013, STUD ART EDUC, V55, P35, DOI 10.1080/00393541.2013.11518915
   Peng ZY, 2022, IEEE T CIRC SYST VID, V32, P3422, DOI 10.1109/TCSVT.2021.3112933
   Pharr M., 2016, Physically based rendering: From theory to implementation
   Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009
   RECOMMENDATION ITU-R BT, 2002, Methodology for the Subjective Assessment of the Quality of Television Pictures
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Sheng KK, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P879, DOI 10.1145/3240508.3240554
   Shu Y., 2021, IEEE Transactions on Multimedia
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song TS, 2022, IEEE T CIRC SYST VID, V32, P7592, DOI 10.1109/TCSVT.2022.3179744
   Su SL, 2020, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR42600.2020.00372
   Suiyi Ling, 2020, QoEVMA'20: Proceedings of the 1st Workshop on Quality of Experience (QoE) in Visual Multimedia Applications, P47, DOI 10.1145/3423328.3423501
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sun Wei, 2023, IEEE Journal of Selected Topics in Signal Processing, V2023
   Sun X., 2009, 17 ACM INTERNA C MUL, P541
   Talebi H, 2018, IEEE T IMAGE PROCESS, V27, P3998, DOI 10.1109/TIP.2018.2831899
   Voorhees Gerald A., 2012, Guns, Grenades, and Grunts: FirstPerson Shooter Games
   Wang SQ, 2016, IEEE J EM SEL TOP C, V6, P532, DOI 10.1109/JETCAS.2016.2598756
   Wang T, 2023, IEEE T GAMES, V15, P658, DOI 10.1109/TG.2022.3212201
   Wang T, 2021, IEEE I C VI COM I PR, DOI 10.1109/VCIP53242.2021.9675430
   Wen SG, 2022, INT CONF ACOUST SPEE, P1810, DOI 10.1109/ICASSP43922.2022.9746547
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu HN, 2023, Arxiv, DOI arXiv:2211.04894
   Xu JZ, 2016, IEEE T CIRC SYST VID, V26, P50, DOI 10.1109/TCSVT.2015.2478706
   Yang H, 2015, IEEE T IMAGE PROCESS, V24, P4408, DOI 10.1109/TIP.2015.2465145
   Yang S, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1383, DOI 10.1145/3343031.3350990
   Ying ZQ, 2021, PROC CVPR IEEE, P14014, DOI 10.1109/CVPR46437.2021.01380
   You JY, 2022, IEEE IMAGE PROC, P26, DOI 10.1109/ICIP46576.2022.9897789
   Yu XX, 2022, IEEE WINT CONF APPL, P74, DOI 10.1109/WACVW54805.2022.00013
   Zadtootaghaj S, 2020, MMSYS'20: PROCEEDINGS OF THE 2020 MULTIMEDIA SYSTEMS CONFERENCE, P213, DOI 10.1145/3339825.3391872
   Zhai GT, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2757-1
   Zhang LM, 2014, IEEE T IMAGE PROCESS, V23, DOI 10.1109/TIP.2014.2303650
   Zhang WX, 2020, IEEE T CIRC SYST VID, V30, P36, DOI 10.1109/TCSVT.2018.2886771
   Zhang XD, 2019, IEEE T MULTIMEDIA, V21, P2815, DOI 10.1109/TMM.2019.2911428
   Zhang ZC, 2022, Arxiv, DOI arXiv:2208.14085
   Zhang Zicheng, 2021, IEEE Transactions on Circuits and Systems for Video Technology
   Zhang Zicheng, 2023, P IEEE CVF C COMPUTE
   Zhang Zicheng, 2021, IEEE INT C MULTIMEDI, P1
   Zhang Zicheng, 2023, INT JOINT C ARTIFICI
NR 95
TC 1
Z9 1
U1 2
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2024
VL 20
IS 4
AR 96
DI 10.1145/3631357
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6G9
UT WOS:001153382100006
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Beuve, N
   Hamidouche, W
   Déforges, O
AF Beuve, Nicolas
   Hamidouche, Wassim
   Deforges, Olivier
TI Hierarchical Learning and Dummy Triplet Loss for Efficient Deepfake
   Detection
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Deepfake forensics; neural networks; metric learning
AB The advancement of generative models has made it easier to create highly realistic Deepfake videos. This accessibility has led to a surge in research on Deepfake detection to mitigate potential misuse. Typically, Deepfake detection models utilize binary backbones, even though the training dataset contains additional exploitable information, such as the Deepfake generation method employed for each video. However, recent findings suggest that inferring a binary class from a multi-class backbone yields superior performance compared to directly employing a binary backbone. Building upon this research, our article introduces two novel methods to infer a binary class from a multi-class backbone. The first method, named root dummies, leverages the dummy triplet loss, which employs fixed vectors (i.e., dummies) instead of mined positives and negatives in the triplet loss. By training the multi-class backbone with these dummies, we can easily infer a binary class during testing by adjusting the number of dummies (from six during training to two during inference). Through this approach, we achieve an accuracy improvement of 0.23% compared to the existing inference method, without requiring additional training. The second proposed method is transfer learning. It involves training a classifier, such as a support vector machine, to predict binary classes based on the image embeddings generated by the multi-class backbone. Although this method necessitates additional training, it further enhances the model's performance, resulting in an accuracy increase of 1.79%. In summary, our proposed methods improve the accuracy of Deepfake detection by simply modifying the number of classes during training, making them suitable for integration into a variety of existing Deepfake training pipelines. Additionally, to foster reproducible research, we have made the source code of our solution publicly available at https://github.com/beuve/DmyT.
C1 [Beuve, Nicolas; Hamidouche, Wassim; Deforges, Olivier] Univ Rennes, CNRS, INSA Rennes, IETR UMR 6164, 20 Av Buttes Coesmes, F-35700 Rennes, France.
   [Beuve, Nicolas; Hamidouche, Wassim; Deforges, Olivier] Univ Rennes, CNRS, INSA Rennes, IETR UMR 6164, 20 Av Buttes Coesmes, F-35000 Rennes, France.
   [Hamidouche, Wassim] Technol Innovat Inst, Masdar City, U Arab Emirates.
C3 Universite de Rennes; Centre National de la Recherche Scientifique
   (CNRS); CNRS - Institute for Engineering & Systems Sciences (INSIS);
   Institut National des Sciences Appliquees de Rennes; Centre National de
   la Recherche Scientifique (CNRS); CNRS - Institute for Engineering &
   Systems Sciences (INSIS); Universite de Rennes; Institut National des
   Sciences Appliquees de Rennes; Technology Innovation Institute
RP Beuve, N (corresponding author), Univ Rennes, CNRS, INSA Rennes, IETR UMR 6164, 20 Av Buttes Coesmes, F-35700 Rennes, France.; Beuve, N (corresponding author), Univ Rennes, CNRS, INSA Rennes, IETR UMR 6164, 20 Av Buttes Coesmes, F-35000 Rennes, France.
EM nicolas.beuve@insa-rennes.fr; wassim.hamidouche@tii.ae;
   olivier.deforges@insa-rennes.fr
CR Afchar D, 2018, IEEE INT WORKS INFOR
   Agarwal S, 2020, Arxiv, DOI arXiv:2004.14491
   Agarwal S, 2020, IEEE COMPUT SOC CONF, P2814, DOI 10.1109/CVPRW50498.2020.00338
   Agarwal S, 2021, IEEE COMPUT SOC CONF, P981, DOI 10.1109/CVPRW53098.2021.00109
   Beuve Nicolas, 2021, ADGD '21: Proceedings of the 1st Workshop on Synthetic Multimedia - Audiovisual Deepfake Generation and Detection, P17, DOI 10.1145/3476099.3484316
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Cozzolino D, 2019, P IEEE CVF C COMP VI, P130
   Cozzolino Davide, 2021, P 2021 INT C COMP VI
   Demir I, 2021, PROCEEDINGS ETRA 2021: ACM SYMPOSIUM ON EYE TRACKING RESEARCH AND APPLICATIONS, DOI 10.1145/3448017.3457387
   Deng JK, 2020, PROC CVPR IEEE, P5202, DOI 10.1109/CVPR42600.2020.00525
   Do TT, 2019, PROC CVPR IEEE, P10396, DOI 10.1109/CVPR.2019.01065
   Dolhansky B, 2020, Arxiv, DOI arXiv:2006.07397
   Dong XY, 2020, Arxiv, DOI arXiv:2012.03930
   Durall R, 2020, Arxiv, DOI arXiv:1911.00686
   Fernandes S, 2019, IEEE INT CONF COMP V, P1721, DOI 10.1109/ICCVW.2019.00213
   Howard AG, 2017, Arxiv, DOI [arXiv:1704.04861, 10.48550/arXiv.1704.04861]
   Giudice O, 2021, J IMAGING, V7, DOI 10.3390/jimaging7080128
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Google AI Blog, 2019, Contributing data to deepfake detection research
   Guarnera L, 2020, IEEE COMPUT SOC CONF, P2841, DOI 10.1109/CVPRW50498.2020.00341
   Nguyen HH, 2019, Arxiv, DOI arXiv:1910.12467
   Heo YJ, 2021, Arxiv, DOI arXiv:2104.01353
   Hernandez-Ortega J, 2020, Arxiv, DOI arXiv:2010.00400
   Jain A, 2021, IEEE INT WORKSH MULT, DOI 10.1109/MMSP53017.2021.9733468
   Jiang LM, 2020, PROC CVPR IEEE, P2886, DOI 10.1109/CVPR42600.2020.00296
   Jung T, 2020, IEEE ACCESS, V8, P83144, DOI 10.1109/ACCESS.2020.2988660
   Korshunov P, 2022, INT CONF ACOUST SPEE, P8972, DOI 10.1109/ICASSP43922.2022.9747628
   Li LZ, 2020, PROC CVPR IEEE, P5073, DOI 10.1109/CVPR42600.2020.00512
   [李艳歌 Li Yange], 2018, [高分子通报, Polymer Bulletin], P46
   Li YZ, 2020, PROC CVPR IEEE, P3204, DOI 10.1109/CVPR42600.2020.00327
   Li Yuezun, 2018, 2018 IEEE International Workshop on Information Forensics and Security (WIFS), P1, DOI DOI 10.1109/WIFS.2018.8630787
   Naruniec J, 2020, COMPUT GRAPH FORUM, V39, P173, DOI 10.1111/cgf.14062
   Nguyen HH, 2019, INT CONF ACOUST SPEE, P2307, DOI 10.1109/ICASSP.2019.8682602
   NVIDIA, 2020, NVIDIA Maxine
   Perov I, 2021, Arxiv, DOI [arXiv:2005.05535, DOI 10.48550/ARXIV.2005.05535]
   Rössler A, 2019, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2019.00009
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sundar SS, 2021, J COMPUT-MEDIAT COMM, V26, P301, DOI 10.1093/jcmc/zmab010
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tariq S, 2020, Arxiv, DOI arXiv:2009.07480
   Thies J, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323035
   Thies J, 2016, PROC CVPR IEEE, P2387, DOI 10.1109/CVPR.2016.262
   Wang JK, 2022, Arxiv, DOI arXiv:2104.09770
   Wang YH, 2020, IEEE INT CONF AUTOMA, P515, DOI 10.1109/FG47880.2020.00089
   Wang Yuhan, 2021, P 30 INT JOINT C ART, P1136, DOI DOI 10.24963/IJCAI.2021/157
   Wu CY, 2017, IEEE I CONF COMP VIS, P2859, DOI 10.1109/ICCV.2017.309
   Wu WN, 2018, LECT NOTES COMPUT SC, V11205, P622, DOI 10.1007/978-3-030-01246-5_37
   Yang X, 2019, INT CONF ACOUST SPEE, P8261, DOI 10.1109/ICASSP.2019.8683164
   Yuyang Qian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P86, DOI 10.1007/978-3-030-58610-2_6
   Zhang X, 2019, IEEE INT WORKS INFOR, DOI 10.1109/wifs47025.2019.9035107
   Zi BJ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2382, DOI 10.1145/3394171.3413769
NR 52
TC 0
Z9 0
U1 11
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2024
VL 20
IS 3
AR 89
DI 10.1145/3626101
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6F8
UT WOS:001153381000029
DA 2024-08-05
ER

PT J
AU Yang, JF
   Wang, ZY
   Huang, BJ
   Ai, JX
   Yang, YH
   Xiong, ZX
AF Yang, Jifan
   Wang, Zhongyuan
   Huang, Baojin
   Ai, Jiaxin
   Yang, Yuhong
   Xiong, Zixiang
TI Joint Distortion Restoration and Quality Feature Learning for
   No-reference Image Quality Assessment
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image quality assessment; no-reference; joint optimization; multi-task
   learning; hybrid loss
ID NETWORK
AB No-reference image quality assessment (NR-IQA) methods, inspired by the free energy principle, improve the accuracy of image quality prediction by simulating the human brain's repair process for distorted images. However, existing methods use separate optimization schemes for distortion restoration and quality prediction, which undermines the accurate mapping of feature representations to quality scores. To address this issue, we propose a joint restoration and quality feature learning NR-IQA (RQFL-IQA) method to jointly tackle distortion image restoration and quality prediction within a unified framework. To accurately establish the quality reconstruction relationship between distorted and restored images, a hybrid loss function based on pixel-wise and structure-wise representations is used to improve the restoration capability of the image restoration network. The proposed RQFL-IQA exploits rich labels, including restored images and quality scores, to enable the model to learn more discriminative features and establish a more accurate mapping from feature representation to quality scores. In addition, to avoid the impact of poor restoration on quality prediction, we propose amodule with a cleaning function to reweight the fusion of restored and primitive features to achieve more perceptual consistency in feature fusion. Experimental results on public IQA datasets show that the proposed RQFL-IQA is superior over existing methods.
C1 [Yang, Jifan; Wang, Zhongyuan; Huang, Baojin; Ai, Jiaxin; Yang, Yuhong] Wuhan Univ, Sch Comp Sci, Wuhan, Peoples R China.
   [Xiong, Zixiang] Texas A&M Univ, Dept Elect & Comp Engn, College Stn, TX USA.
C3 Wuhan University; Texas A&M University System; Texas A&M University
   College Station
RP Wang, ZY (corresponding author), Wuhan Univ, Sch Comp Sci, Wuhan, Peoples R China.
EM yangjifan_1996@163.com; wzy_hope@163.com; huangbao-jin@whu.edu.cn;
   julyai@whu.edu.cn; yangyuhong@whu.edu.cn; zx@ece.tamu.edu
OI Xiong, Zixiang/0000-0002-4714-3311; Wang, Zhongyuan/0000-0002-9796-488X
FU National Natural Science Foundation of China [62371350, 62071339,
   62171326]; Key R&D Program in Hubei Province [2022BAA079]
FX This research was funded by National Natural Science Foundation of China
   (62371350, 62071339, 62171326) and Key R&D Program in Hubei Province
   (2022BAA079).
CR Ahn S, 2021, IEEE COMPUT SOC CONF, P344, DOI 10.1109/CVPRW53098.2021.00044
   Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Chen JM, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P2080, DOI 10.1109/ICASSP39728.2021.9413489
   Chen LY, 2022, LECT NOTES COMPUT SC, V13667, P17, DOI 10.1007/978-3-031-20071-7_2
   Fang YM, 2020, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR42600.2020.00373
   Ghadiyaram D, 2016, IEEE T IMAGE PROCESS, V25, P372, DOI 10.1109/TIP.2015.2500021
   Golestaneh SA, 2022, IEEE WINT CONF APPL, P3989, DOI 10.1109/WACV51458.2022.00404
   Gu K, 2018, IEEE T NEUR NET LEAR, V29, P1301, DOI 10.1109/TNNLS.2017.2649101
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Hosu V, 2020, IEEE T IMAGE PROCESS, V29, P4041, DOI 10.1109/TIP.2020.2967829
   Huang BJ, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3569943
   Huang BJ, 2023, PATTERN RECOGN, V135, DOI 10.1016/j.patcog.2022.109142
   Huang BJ, 2023, IEEE T NEUR NET LEAR, V34, P10875, DOI 10.1109/TNNLS.2022.3171604
   Huang ZQ, 2021, IEEE T CIRC SYST VID, V31, P2808, DOI 10.1109/TCSVT.2020.3027001
   Kang L, 2015, IEEE IMAGE PROC, P2791, DOI 10.1109/ICIP.2015.7351311
   Ke JJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5128, DOI 10.1109/ICCV48922.2021.00510
   Kim J, 2017, PROC CVPR IEEE, P1969, DOI 10.1109/CVPR.2017.213
   Kim W, 2020, IEEE T IMAGE PROCESS, V29, P4219, DOI 10.1109/TIP.2020.2968283
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Li F, 2021, IEEE T CIRC SYST VID, V31, P4798, DOI 10.1109/TCSVT.2021.3055197
   Li QH, 2016, IEEE T MULTIMEDIA, V18, P2457, DOI 10.1109/TMM.2016.2601028
   Lin HH, 2020, Arxiv, DOI arXiv:2001.08113
   Lin HH, 2019, INT WORK QUAL MULTIM
   Lin KY, 2018, PROC CVPR IEEE, P732, DOI 10.1109/CVPR.2018.00083
   Lin WS, 2011, J VIS COMMUN IMAGE R, V22, P297, DOI 10.1016/j.jvcir.2011.01.005
   Liu XL, 2017, IEEE I CONF COMP VIS, P1040, DOI 10.1109/ICCV.2017.118
   Ma Q, 2022, IEEE T COMPUT IMAG, V8, P28, DOI 10.1109/TCI.2021.3136759
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Pan ZQ, 2022, IEEE T CIRC SYST VID, V32, P7518, DOI 10.1109/TCSVT.2022.3188991
   Pan ZQ, 2022, IEEE T IMAGE PROCESS, V31, P1613, DOI 10.1109/TIP.2022.3144892
   Paszke A, 2019, ADV NEUR IN, V32
   Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009
   Ren HY, 2018, AAAI CONF ARTIF INTE, P7308
   Seo S, 2021, IEEE T CIRC SYST VID, V31, P2602, DOI 10.1109/TCSVT.2020.3030895
   Sheikh Hamid R, 2005, LIVE IMAGE QUALITY A, DOI DOI 10.1109/CVPR.2015.7298594
   Su SL, 2020, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR42600.2020.00372
   Tan MX, 2019, PR MACH LEARN RES, V97
   Vaswani A, 2017, ADV NEUR IN, V30
   Wan ZL, 2020, IEEE T MULTIMEDIA, V22, P2024, DOI 10.1109/TMM.2019.2950533
   Wang Guangcheng, 2024, IEEE Transactions on Artificial Intelligence, V5, P2805, DOI 10.1109/TAI.2023.3324892
   Wang GC, 2022, INFORM SCIENCES, V611, P432, DOI 10.1016/j.ins.2022.08.064
   Wang GC, 2022, IEEE T CIRC SYST VID, V32, P1119, DOI 10.1109/TCSVT.2021.3074181
   Wang GC, 2020, IEEE T IMAGE PROCESS, V29, P1802, DOI 10.1109/TIP.2019.2945675
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZD, 2022, PROC CVPR IEEE, P17662, DOI 10.1109/CVPR52688.2022.01716
   Xu L, 2017, IEEE T CIRC SYST VID, V27, P1833, DOI 10.1109/TCSVT.2016.2543099
   Yang S, 2020, IEEE T MULTIMEDIA, V22, P2163, DOI 10.1109/TMM.2019.2947352
   Yang S, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1383, DOI 10.1145/3343031.3350990
   Yang SD, 2022, IEEE COMPUT SOC CONF, P1190, DOI 10.1109/CVPRW56347.2022.00126
   Yang Y, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3555355
   Ye P, 2012, PROC CVPR IEEE, P1098, DOI 10.1109/CVPR.2012.6247789
   Ye YT, 2021, PROC CVPR IEEE, P2053, DOI 10.1109/CVPR46437.2021.00209
   Yu MZ, 2022, IEEE T CIRC SYST VID, V32, P7559, DOI 10.1109/TCSVT.2022.3190273
   Zhang CF, 2022, IEEE T CIRC SYST VID, V32, P5011, DOI 10.1109/TCSVT.2022.3143321
   Zhang L, 2014, IEEE MULTIMEDIA, V21, P67, DOI 10.1109/MMUL.2014.50
NR 57
TC 0
Z9 0
U1 1
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2024
VL 20
IS 7
SI SI
AR 195
DI 10.1145/3649899
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL0Q6
UT WOS:001234494100010
DA 2024-08-05
ER

PT J
AU Yan, YY
   Xiang, GQ
   Jia, HZ
   Chen, J
   Huang, XF
   Xie, XD
AF Yan, Yunyao
   Xiang, Guoqing
   Jia, Huizhu
   Chen, Jie
   Huang, Xiaofeng
   Xie, Xiaodong
TI Two-Stage Perceptual Quality Oriented Rate Control Algorithm for HEVC
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Rate control; HEVC; perceptual distortion model; Lagrange multiplier;
   distortion dependency
ID OPTIMAL BIT ALLOCATION; LEVEL RATE CONTROL; FRAME-LEVEL; ADAPTIVE
   QUANTIZATION; OPTIMIZATION; SCHEME
AB As a practical technique in mainstream video coding applications, rate control dominates important to ensure compression quality with limited bitrates constraints. However, most rate control methods mainly focus on objective quality while ignoring the perceptual quality improvement for human eyes. In this paper, we propose a two-stage rate control algorithm to optimize the perceptual quality at the frame encoding stage and the coding tree unit (CTU) encoding stage for high efficiency video coding (HEVC), respectively. Firstly, for the frame encoding stage, with inter-frame distortion dependency consideration, a frame-level rate control method is presented by adjusting the frame-level Lagrange multiplier adaptively with a preprocessing method. Secondly, for the CTU encoding stage, we propose a saliency-based CTU-level perceptual quality rate control algorithm, which employs CTU-level saliency weight to adjust the perceptual rate-distortion (R-D) model. We conduct the CTU-level rate control by an optimized Lagrange multiplier and quantization parameter (QP) to achieve perceptual quality optimization. Extensive experimental results reveal that, compared with state-of-the-art rate control methods on HEVC, our algorithm achieves significant perceptual coding performance with improved subjective visual quality.
C1 [Yan, Yunyao; Chen, Jie] Peking Univ, Sch Elect & Comp Engn, Beijing, Peoples R China.
   [Xiang, Guoqing; Jia, Huizhu; Xie, Xiaodong] Peking Univ, Sch Comp Sci, Beijing, Peoples R China.
   [Huang, Xiaofeng] Hangzhou Dianzi Univ, Sch Commun Engn, Hangzhou, Peoples R China.
C3 Peking University; Peking University; Hangzhou Dianzi University
RP Xiang, GQ; Jia, HZ (corresponding author), Peking Univ, Sch Comp Sci, Beijing, Peoples R China.
EM yunyaoyan@pku.edu.cn; gqxiang@pku.edu.cn; hzjia@pku.edu.cn;
   chenj@pcl.ac.cn; xfhuang@hdu.edu.cn; donxie@pku.edu.cn
OI huang, xiaofeng/0000-0002-8479-6960
CR B. Series, 2012, RECOMMENDATION ITU R, P13
   Bossen F., 2013, JCTVCL1100, V12, P7
   Bross B, 2021, IEEE T CIRC SYST VID, V31, P3736, DOI 10.1109/TCSVT.2021.3101953
   Chen ZZ, 2019, IEEE T IMAGE PROCESS, V28, P4541, DOI 10.1109/TIP.2019.2911180
   Choi H.-J., 2012, SEMICONDUCTOR NANOST, P1, DOI DOI 10.1109/OCEANS-YEOSU.2012.6263424
   De Vito Fabio, 2005, P VLBV WORKSH
   Gao W, 2016, IEEE T MULTIMEDIA, V18, P988, DOI 10.1109/TMM.2016.2535254
   Gao W, 2016, IEEE T CIRC SYST VID, V26, P139, DOI 10.1109/TCSVT.2015.2444671
   Gong YC, 2019, IEEE T CIRC SYST VID, V29, P156, DOI 10.1109/TCSVT.2017.2769703
   Guo HW, 2020, IEEE T BROADCAST, V66, P113, DOI 10.1109/TBC.2019.2917402
   Guo HW, 2019, IEEE T BROADCAST, V65, P270, DOI 10.1109/TBC.2018.2847445
   Huang XF, 2023, IEEE T BROADCAST, V69, P422, DOI 10.1109/TBC.2023.3247953
   Ki S, 2018, IEEE T IMAGE PROCESS, V27, P3178, DOI 10.1109/TIP.2018.2818439
   Kim J, 2015, IEEE T CIRC SYST VID, V25, P1786, DOI 10.1109/TCSVT.2015.2389491
   Ku C., 2019, 2019 IEEE Visual Communications and Image Processing, P1
   Lee B, 2014, IEEE T CIRC SYST VID, V24, P465, DOI 10.1109/TCSVT.2013.2276880
   Li Bin, 2012, ITU-T SG16 Contribution, JCTVC-K0103, P1
   Li L, 2018, IEEE T CIRC SYST VID, V28, P130, DOI 10.1109/TCSVT.2016.2598672
   Li SX, 2017, IEEE T CIRC SYST VID, V27, P2409, DOI 10.1109/TCSVT.2016.2589878
   Lim W, 2020, SIGNAL IMAGE VIDEO P, V14, P887, DOI 10.1007/s11760-019-01620-3
   Lin JL, 2022, Arxiv, DOI arXiv:2205.03595
   Liu FY, 2021, IEEE T IMAGE PROCESS, V30, P4706, DOI 10.1109/TIP.2021.3072225
   Luo ZY, 2021, IEEE T IMAGE PROCESS, V30, P5109, DOI 10.1109/TIP.2021.3078622
   Mao YH, 2022, IEEE T CIRC SYST VID, V32, P2371, DOI 10.1109/TCSVT.2021.3093315
   Pan ZQ, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3380827
   Rassool R, 2017, IEEE INT SYM BROADB, P351
   Rehman A., 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P497, DOI 10.1109/ICME.2012.175
   ShiqiWang Abdul Rehman, 2015, 2015 IEEE 17 INT WOR, P1
   Si JJ, 2013, PICT COD SYMP, P89, DOI 10.1109/PCS.2013.6737690
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tourapis AM, 2001, P SOC PHOTO-OPT INS, V4310, P883
   Valizadeh S, 2021, MULTIMED TOOLS APPL, V80, P10235, DOI 10.1007/s11042-020-09442-z
   Wang G, 2018, MULTIMED TOOLS APPL, V77, P12777, DOI 10.1007/s11042-017-4914-4
   Wang H, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351023
   Wang P, 2019, MULTIMED TOOLS APPL, V78, P23733, DOI 10.1007/s11042-019-7680-7
   Wen JT, 2015, IEEE DATA COMPR CONF, P53, DOI 10.1109/DCC.2015.35
   Wu JJ, 2016, INT CONF ACOUST SPEE, P1581, DOI 10.1109/ICASSP.2016.7471943
   Xiang G., 2020, 2020 IEEE INT C CONS, P1
   Xiang GQ, 2022, IEEE T BROADCAST, V68, P69, DOI 10.1109/TBC.2021.3120916
   Xiang GQ, 2018, J VIS COMMUN IMAGE R, V50, P280, DOI 10.1016/j.jvcir.2017.11.011
   Xu L, 2016, IEEE T MULTIMEDIA, V18, P590, DOI 10.1109/TMM.2016.2525004
   Xu M, 2017, IEEE T IMAGE PROCESS, V26, P369, DOI 10.1109/TIP.2016.2628583
   Yan YY, 2020, J VIS COMMUN IMAGE R, V73, DOI 10.1016/j.jvcir.2020.102917
   Yeo CH, 2013, INT CONF ACOUST SPEE, P1690, DOI 10.1109/ICASSP.2013.6637940
   Yuan H, 2021, IEEE T CONSUM ELECTR, V67, P97, DOI 10.1109/TCE.2021.3065636
   Yuan H, 2015, IEEE T MULTIMEDIA, V17, P2134, DOI 10.1109/TMM.2015.2477682
   Yuan H, 2014, J REAL-TIME IMAGE PR, V9, P609, DOI 10.1007/s11554-011-0237-2
   Zhao ZM, 2021, MULTIMED TOOLS APPL, V80, P345, DOI 10.1007/s11042-020-09721-9
   Zhou ML, 2022, IEEE T BROADCAST, V68, P582, DOI 10.1109/TBC.2022.3147103
   Zhou ML, 2019, IEEE T MULTIMEDIA, V21, P1921, DOI 10.1109/TMM.2019.2895281
   Zhou ML, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3107616
NR 51
TC 1
Z9 1
U1 8
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2024
VL 20
IS 5
AR 140
DI 10.1145/3636510
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MF3R9
UT WOS:001192177900020
DA 2024-08-05
ER

PT J
AU Hu, YJ
   Dong, B
   Huang, KZ
   Ding, L
   Wang, W
   Huang, XW
   Wang, QF
AF Hu, Yijie
   Dong, Bin
   Huang, Kaizhu
   Ding, Lei
   Wang, Wei
   Huang, Xiaowei
   Wang, Qiu-Feng
TI Scene Text Recognition via Dual-path Network with Shape-driven Attention
   Alignment
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE OCR; scene text recognition; deformable attention; attention alignment;
   dual path network
AB Scene text recognition (STR), one typical sequence-to-sequence problem, has drawn much attention recently in multimedia applications. To guarantee good performance, it is essential for STR to obtain aligned character-wise features from the whole-image feature maps. While most present works adopt fully data-driven attention-based alignment, such practice ignores specific character geometric information. In this article, built upon a group of learnable geometric points, we propose a novel shape-driven attention alignment method that is able to obtain character-wise features. Concretely, we first design a corner detector to generate a shape map to guide the attention alignments explicitly, where a series of points can be learned to represent character-wise features flexibly. We then propose a dual-path network with a mutual learning and cooperating strategy that successfully combines CNN with a ViT-based model, leading to further accuracy improvement. We conduct extensive experiments to evaluate the proposed method on various scene text benchmarks, including six popular regular and irregular datasets, two more challenging datasets (i.e., WordArt and OST), and three Chinese datasets. Experimental results indicate that our method can achieve superior performance with a comparable model size against many state-of-the-art models.
C1 [Hu, Yijie; Wang, Wei; Wang, Qiu-Feng] Xian Jiaotong Liverpool Univ, Sch Adv Technol, Renai Rd, Suzhou 215000, Jiangsu, Peoples R China.
   [Dong, Bin; Ding, Lei] Ricoh Software Res Ctr Beijing Co Ltd, Xizhimenwai St, Beijing 100080, Peoples R China.
   [Huang, Kaizhu] Duke Kunshan Univ, Data Sci Res Ctr, Duke Ave, Kunshan 215316, Jiangsu, Peoples R China.
   [Huang, Xiaowei] Univ Liverpool, Dept Comp Sci, Lime St, Liverpool L69 3BX, Merseyside, England.
C3 Xi'an Jiaotong-Liverpool University; Duke Kunshan University; University
   of Liverpool
RP Wang, QF (corresponding author), Xian Jiaotong Liverpool Univ, Sch Adv Technol, Renai Rd, Suzhou 215000, Jiangsu, Peoples R China.
EM Yijie.Hu20@student.xjtlu.edu.cn; Bin.Dong@cn.ricoh.com;
   kaizhu.huang@dukekunshan.edu.cn; Lei.Ding@cn.ricoh.com;
   Wei.Wang03@xjtlu.edu.cn; Xiaowei.Huang@liverpool.ac.uk;
   Qiufeng.Wang@xjtlu.edu.cn
RI Huang, Kaizhu/O-4721-2014
OI Huang, Kaizhu/0000-0002-3034-9639; Hu, Yijie/0009-0006-5968-3389; Wang,
   Wei/0000-0002-0707-8076
FU National Natural Science Foundation of China [92370119, 62376113,
   62276258]; Jiangsu Science and Technology Programme (Natural Science
   Foundation of Jiangsu Province) [BE2020006-4]; European Union [956123];
   UK EPSRC [EP/T026995/1]
FX The work was partially supported by the following: National Natural
   Science Foundation of China under No. 92370119, No. 62376113, and No.
   62276258; Jiangsu Science and Technology Programme (Natural Science
   Foundation of Jiangsu Province) under No. BE2020006-4, European Union's
   Horizon 2020 research and innovation programme No. 956123, and UK EPSRC
   under projects [EP/T026995/1].
CR Bai F, 2018, PROC CVPR IEEE, P1508, DOI 10.1109/CVPR.2018.00163
   Bautista D, 2022, LECT NOTES COMPUT SC, V13688, P178, DOI 10.1007/978-3-031-19815-1_11
   Bhunia Ayan Kumar, 2021, P IEEE CVF INT C COM, P14940, DOI DOI 10.1109/ICCV48922.2021.01467
   Bian XB, 2022, AAAI CONF ARTIF INTE, P113
   Chen JY, 2021, PROC CVPR IEEE, P12021, DOI 10.1109/CVPR46437.2021.01185
   Cheng ZZ, 2017, IEEE I CONF COMP VIS, P5086, DOI 10.1109/ICCV.2017.543
   Da C, 2022, LECT NOTES COMPUT SC, V13688, P322, DOI 10.1007/978-3-031-19815-1_19
   Deli Yu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12110, DOI 10.1109/CVPR42600.2020.01213
   Dosovitskiy A., 2021, ICLR
   Du Y., 2022, P 31 INT JOINT C ART, P884
   Fang SC, 2021, PROC CVPR IEEE, P7094, DOI 10.1109/CVPR46437.2021.00702
   Ganaie MA, 2022, ENG APPL ARTIF INTEL, V115, DOI 10.1016/j.engappai.2022.105151
   He Y, 2022, AAAI CONF ARTIF INTE, P888
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Hu Yijie, 2022, P INT C NEUR INF PRO, P705
   Huang K., 2019, Deep Learning: Fundamentals, Theory and Applications, Vfirst, DOI DOI 10.1007/978-3-030-06073-2
   Iwamura M, 2018, Arxiv, DOI arXiv:1812.05219
   Jing Li, 2020, Neural Information Processing. 27th International Conference, ICONIP 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12533), P152, DOI 10.1007/978-3-030-63833-7_13
   Ju C, 2018, J APPL STAT, V45, P2800, DOI 10.1080/02664763.2018.1441383
   Lam L, 1997, IEEE T SYST MAN CY A, V27, P553, DOI 10.1109/3468.618255
   Lee CY, 2014, PROC CVPR IEEE, P4050, DOI 10.1109/CVPR.2014.516
   Li H, 2019, AAAI CONF ARTIF INTE, P8610
   Liu CL, 2011, PROC INT CONF DOC, P37, DOI 10.1109/ICDAR.2011.17
   Liu YL, 2024, Arxiv, DOI arXiv:2305.07895
   Lucas S. M., 2005, International Journal on Document Analysis and Recognition, V7, P105, DOI 10.1007/s10032-004-0134-3
   Luo CJ, 2019, PATTERN RECOGN, V90, P109, DOI 10.1016/j.patcog.2019.01.020
   Qian Z, 2022, PATTERN RECOGN, V131, DOI 10.1016/j.patcog.2022.108889
   Qiao Z, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2046, DOI 10.1145/3474085.3475238
   Radford A, 2021, PR MACH LEARN RES, V139
   Shi BG, 2019, IEEE T PATTERN ANAL, V41, P2035, DOI 10.1109/TPAMI.2018.2848939
   Shi BG, 2017, IEEE T PATTERN ANAL, V39, P2298, DOI 10.1109/TPAMI.2016.2646371
   SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tang JQ, 2022, PROC CVPR IEEE, P4553, DOI 10.1109/CVPR52688.2022.00452
   Wan Zhaoyi, 2020, P IEEECVF C COMPUTER, P11425
   Wang J., 2022, BMVC, P772
   Wang QF, 2012, IEEE T PATTERN ANAL, V34, P1469, DOI 10.1109/TPAMI.2011.264
   Wang T, 2012, INT C PATT RECOG, P3304
   Wang TW, 2020, AAAI CONF ARTIF INTE, V34, P12216
   Wang YX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14174, DOI 10.1109/ICCV48922.2021.01393
   Xie HT, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3231737
   Xie XD, 2022, LECT NOTES COMPUT SC, V13688, P303, DOI 10.1007/978-3-031-19815-1_18
   Yan CG, 2021, IEEE T PATTERN ANAL, V43, P1445, DOI 10.1109/TPAMI.2020.2975798
   Yan CG, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3472810
   Yan CG, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3468872
   Yan CG, 2022, IEEE T CIRC SYST VID, V32, P43, DOI 10.1109/TCSVT.2021.3067449
   Yan CG, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3404374
   Yan RJ, 2021, PROC CVPR IEEE, P284, DOI 10.1109/CVPR46437.2021.00035
   Yu HY, 2022, Arxiv, DOI arXiv:2112.15093
   Zhang XY, 2022, AAAI CONF ARTIF INTE, P3353
   Zhang Y, 2018, PROC CVPR IEEE, P4320, DOI 10.1109/CVPR.2018.00454
   Zhang Z, 2016, PROC CVPR IEEE, P4159, DOI 10.1109/CVPR.2016.451
   Zhao S, 2024, Arxiv, DOI arXiv:2305.14014
   Zhi Qiao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13525, DOI 10.1109/CVPR42600.2020.01354
   Zhu X., 2021, ICLR, P1, DOI DOI 10.48550/ARXIV.2010.04159
NR 55
TC 0
Z9 0
U1 7
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2024
VL 20
IS 4
AR 107
DI 10.1145/3633517
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6G9
UT WOS:001153382100017
DA 2024-08-05
ER

PT J
AU Li, M
   Zhang, W
   Hu, B
   Kang, JM
   Wang, YQ
   Lu, SF
AF Li, Mi
   Zhang, Wei
   Hu, Bin
   Kang, Jiaming
   Wang, Yuqi
   Lu, Shengfu
TI Automatic Assessment of Depression and Anxiety through Encoding
   Pupil-wave from HCI in VR Scenes
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Deep learning; Virtual reality (VR); Human computer interaction (HCI);
   pupil-wave; width-channel attention module
ID VIRTUAL-REALITY; DISORDER; RUMINATION; BURDEN
AB At present, there have beenmany studies on themethods of using the deep learning regression model to assess depression level based on behavioral signals (facial expression, speech, and language); however, the research on the assessment method of anxiety level using deep learning is absent. In this article, pupil-wave, a physiological signal collected by Human Computer Interaction (HCI) that can directly represent the emotional state, is developed to assess the level of depression and anxiety for the first time. In order to distinguish between different depression and anxiety levels, we use the HCI method to induce the participants' emotional experience through three virtual reality (VR) emotional scenes of joyful, sad, and calm, and construct two differential pupil-waves of joyful and sad with the calm pupil-wave as the baseline. Correspondingly, a dual-channel fusion depression and anxiety level assessment model is constructed using the improved multi-scale convolution module and our proposed width-channel attention module for one-dimensional signal processing. The test results show that the MAE/RMSE of the depression and anxiety level assessment method proposed in this article is 3.05/4.11 and 2.49/1.85, respectively, which has better assessment performance than other related research methods. This study provides an automatic assessment technique based on human computer interaction and virtual reality for mental health physical examination.
C1 [Li, Mi; Lu, Shengfu] Beijing Univ Technol, Engn Res Ctr Intelligent Percept & Autonomous Con, Beijing Int Collaborat Base Brain Informat & Wisd, Fac Informat Technol,Minist Educ,Engn Res Ctr Dig, Beijing, Peoples R China.
   [Zhang, Wei; Kang, Jiaming; Wang, Yuqi] Beijing Univ Technol, Fac Informat Technol, Beijing, Peoples R China.
   [Hu, Bin] Beijing Inst Technol, Inst Engn Med, Beijing, Peoples R China.
   [Hu, Bin] Lanzhou Univ, Sch Informat Sci & Engn, Gansu Prov Key Lab Wearable Comp, Lanzhou, Peoples R China.
C3 Beijing University of Technology; Beijing University of Technology;
   Beijing Institute of Technology; Lanzhou University
RP Hu, B (corresponding author), Beijing Inst Technol, Inst Engn Med, Beijing, Peoples R China.; Hu, B (corresponding author), Lanzhou Univ, Sch Informat Sci & Engn, Gansu Prov Key Lab Wearable Comp, Lanzhou, Peoples R China.
EM limi@bjut.edu.cn; ZhangW@emails.bjut.edu.cn; bh@bit.edu.cn;
   kangjiaming@emails.bjut.edu.cn; WangYuqi@emails.bjut.edu.cn;
   lusf@bjut.edu.cn
RI Hu, Bin/ACD-0145-2022; li, mi/KSM-9187-2024
OI Hu, Bin/0000-0003-3514-5413; Wang, Yuqi/0000-0002-4246-4626
FU National Key Research and Development Program of China [2019YFA0706200];
   National Natural Science Foundation of China [61602017, 61632014,
   61627808]; National Basic Research Programme of China [2014CB744600]
FX This work was supported in part by the National Key Research and
   Development Program of China (Grant No. 2019YFA0706200), in part by the
   National Natural Science Foundation of China (Grant No. 61602017, No.
   61632014, No. 61627808), and in part by the National Basic Research
   Programme of China (2014CB744600).
CR Al Jazaery M, 2021, IEEE T AFFECT COMPUT, V12, P262, DOI 10.1109/TAFFC.2018.2870884
   Alhanai T, 2018, INTERSPEECH, P1716, DOI 10.21437/Interspeech.2018-2522
   Baas JM, 2004, BIOL PSYCHIAT, V55, P1056, DOI 10.1016/j.biopsych.2004.02.024
   Bandanau D, 2016, INT CONF ACOUST SPEE, P4945, DOI 10.1109/ICASSP.2016.7472618
   Baños RM, 2008, CYBERPSYCHOL BEHAV, V11, P1, DOI 10.1089/cpb.2007.9936
   Baños RM, 2006, LECT NOTES COMPUT SC, V3962, P7
   Beck AT, 1996, J PERS ASSESS, V67, P588, DOI 10.1207/s15327752jpa6703_13
   Bradley MM, 2008, PSYCHOPHYSIOLOGY, V45, P602, DOI 10.1111/j.1469-8986.2008.00654.x
   Card S., 1983, The psychology of human-computer interaction
   Chatterjee M, 2014, INT CONF ACOUST SPEE
   Chen Junfen, 2015, Refining Width Parameter of Gaussian Kernel Function for Tuning Coherent Point Drift (CPD) Registration
   Christ M., 2017, arXiv
   Cummins N, 2015, SPEECH COMMUN, V71, P10, DOI 10.1016/j.specom.2015.03.004
   Daway Hazim G., 2018, International Journal of Electrical and Computer Engineering, V8, P3278
   de Melo W.C., 2019, IEEE INT CONF AUTOMA, P1, DOI DOI 10.1109/fg.2019.8756568
   Desmet Pieter, 2018, Funology, P111
   Dibeklioglu H, 2018, IEEE J BIOMED HEALTH, V22, P525, DOI 10.1109/JBHI.2017.2676878
   Doehrmann O, 2013, JAMA PSYCHIAT, V70, P87, DOI 10.1001/2013.jamapsychiatry.5
   Donaldson C, 2007, BEHAV RES THER, V45, P2664, DOI 10.1016/j.brat.2007.07.002
   Duchowski A. T., 2017, Eye tracking methodology: Theory and practice, DOI DOI 10.1007/978-3-319-57883-5
   Duta I.C., 2020, arXiv
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   Girard JM, 2015, CURR OPIN PSYCHOL, V4, P75, DOI 10.1016/j.copsyc.2014.12.010
   Gong Yuan, Combining Global and Local Convolutional 3D Networks for Detecting Depression from Facial Expressions
   Greenberg PE, 2015, J CLIN PSYCHIAT, V76, P155, DOI 10.4088/JCP.14m09298
   Haque A, 2018, Arxiv, DOI arXiv:1811.08592
   Hashim AT, 2016, 2016 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P183, DOI 10.1109/CW.2016.39
   He L, 2018, J BIOMED INFORM, V83, P103, DOI 10.1016/j.jbi.2018.05.007
   Hilbert K, 2017, BRAIN BEHAV, V7, DOI 10.1002/brb3.633
   Hou QB, 2021, Arxiv, DOI [arXiv:2103.02907, DOI 10.48550/ARXIV.2103.02907]
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Husain W, 2016, 2016 3RD INTERNATIONAL CONFERENCE ON COMPUTER AND INFORMATION SCIENCES (ICCOINS), P37, DOI 10.1109/ICCOINS.2016.7783185
   Jan A, 2018, IEEE T COGN DEV SYST, V10, P668, DOI 10.1109/TCDS.2017.2721552
   Jan F, 2018, MULTIMED TOOLS APPL, V77, P1041, DOI 10.1007/s11042-016-4334-x
   Katsis CD, 2011, BIOMED SIGNAL PROCES, V6, P261, DOI 10.1016/j.bspc.2010.12.001
   Kessler RC, 2017, EPIDEMIOL PSYCH SCI, V26, P22, DOI 10.1017/S2045796016000020
   Kiranyaz S, 2021, MECH SYST SIGNAL PR, V151, DOI 10.1016/j.ymssp.2020.107398
   Kiruthiga AR, 2017, 2017 FOURTH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING, COMMUNICATION AND NETWORKING (ICSCN)
   Krishna SG, 2015, IARJSET, V2, P20, DOI DOI 10.17148/IARJSET.2015.2305
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kroenke K, 2002, PSYCHIAT ANN, V32, P509, DOI 10.3928/0048-5713-20020901-06
   Kroenke K, 2009, J AFFECT DISORDERS, V114, P163, DOI 10.1016/j.jad.2008.06.026
   Lader M., 2015, Encyclopedia of Psychopharmacology, P699, DOI [DOI 10.1007/978-3-642-36172-2_317, 10.1007/978-3-642-36172-2_317]
   Li M, 2016, J INT MED RES, V44, P529, DOI 10.1177/0300060516639169
   Li Mi, 2016, Journal of Psychiatry, V19
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Madsen KE, 2016, COMPUT HUM BEHAV, V56, P142, DOI 10.1016/j.chb.2015.11.041
   Mathers C, 2008, GLOBAL BURDEN OF DISEASE: 2004 UPDATE, P1
   Mulckhuyse M, 2018, COGN AFFECT BEHAV NE, V18, P411, DOI 10.3758/s13415-018-0590-8
   Muzammel M, 2020, MACH LEARN APPL, V2, DOI 10.1016/j.mlwa.2020.100005
   Nolen-Hoeksema S, 2000, J ABNORM PSYCHOL, V109, P504, DOI 10.1037/0021-843X.109.3.504
   Pampouchidou A, 2019, IEEE T AFFECT COMPUT, V10, P445, DOI 10.1109/TAFFC.2017.2724035
   Ringeval F., 2017, Proceedings of the 7th Annual Workshop on Audio/Visual Emotion Challenge, P3, DOI DOI 10.1145/3133944.3133953
   Santini T, 2018, COMPUT VIS IMAGE UND, V170, P40, DOI 10.1016/j.cviu.2018.02.002
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Spitzer RL, 2006, ARCH INTERN MED, V166, P1092, DOI 10.1001/archinte.166.10.1092
   STEUER J, 1992, J COMMUN, V42, P73, DOI 10.1111/j.1460-2466.1992.tb00812.x
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Valstar M., 2014, P 4 INT WORKSH AUD V, P3
   Valstar M., 2013, P 3 ACM INT WORKSH A, P3
   Valstar M, 2016, PROCEEDINGS OF THE 6TH INTERNATIONAL WORKSHOP ON AUDIO/VISUAL EMOTION CHALLENGE (AVEC'16), P3, DOI 10.1145/2988257.2988258
   Vaswani A, 2017, ADV NEUR IN, V30
   Vos T, 2015, LANCET, V386, P743, DOI 10.1016/S0140-6736(15)60692-4
   Wang QL, 2020, Arxiv, DOI [arXiv:1910.03151, DOI 10.48550/ARXIV.1910.03151]
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Williamson JR, 2016, PROCEEDINGS OF THE 6TH INTERNATIONAL WORKSHOP ON AUDIO/VISUAL EMOTION CHALLENGE (AVEC'16), P11, DOI 10.1145/2988257.2988263
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Yang L., 2017, P 7 ANN WORKSH AUD V
   Yang L, 2021, IEEE T AFFECT COMPUT, V12, P239, DOI 10.1109/TAFFC.2018.2870398
   Yang L, 2016, PROCEEDINGS OF THE 6TH INTERNATIONAL WORKSHOP ON AUDIO/VISUAL EMOTION CHALLENGE (AVEC'16), P89, DOI 10.1145/2988257.2988269
   Yuan PC, 2020, Arxiv, DOI arXiv:2010.07621
   Zhao ZP, 2020, IEEE J-STSP, V14, P423, DOI 10.1109/JSTSP.2019.2955012
   Zhou XZ, 2020, IEEE T AFFECT COMPUT, V11, P542, DOI 10.1109/TAFFC.2018.2828819
   Zhu DJ, 1999, COMPUT METH PROG BIO, V59, P145, DOI 10.1016/S0169-2607(98)00105-9
   Zhu Y, 2018, IEEE T AFFECT COMPUT, V9, P578, DOI 10.1109/TAFFC.2017.2650899
NR 76
TC 38
Z9 38
U1 78
U2 96
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2024
VL 20
IS 2
SI SI
AR 42
DI 10.1145/3513263
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W6GR4
UT WOS:001092595800012
HC Y
HP N
DA 2024-08-05
ER

PT J
AU Li, JK
   Wang, YH
   Li, WX
AF Li, Jiankai
   Wang, Yunhong
   Li, Weixin
TI Zero-shot Scene Graph Generation via Triplet Calibration and Reduction
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Scene analysis and understanding; scene graph generation; compositional
   zero-shot learning
AB Scene Graph Generation (SGG) plays a pivotal role in downstream vision-language tasks. Existing SGG methods typically suffer frompoor compositional generalizations on unseen triplets. They are generally trained on incompletely annotated scene graphs that contain dominant triplets and tend to bias toward these seen triplets during inference. To address this issue, we propose a Triplet Calibration and Reduction (T-CAR) framework in this article. In our framework, a triplet calibration loss is first presented to regularize the representations of diverse triplets and to simultaneously excavate the unseen triplets in incompletely annotated training scene graphs. Moreover, the unseen space of scene graphs is usually several times larger than the seen space, since it contains a huge number of unrealistic compositions. Thus, we propose an unseen space reduction loss to shift the attention of excavation to reasonable unseen compositions to facilitate the model training. Finally, we propose a contextual encoder to improve the compositional generalizations of unseen triplets by explicitly modeling the relative spatial relations between subjects and objects. Extensive experiments show that our approach achieves consistent improvements for zero-shot SGG over state-of-the-art methods. The code is available at https://github.com/jkli1998/T-CAR.
C1 [Li, Jiankai; Wang, Yunhong; Li, Weixin] Beihang Univ, State Key Lab Software Dev Environm, Sch Comp Sci & Engn, Beijing 100191, Peoples R China.
   [Li, Jiankai; Wang, Yunhong; Li, Weixin] Shanghai Artificial Intelligence Lab, Beijing 100191, Peoples R China.
C3 Beihang University
RP Li, WX (corresponding author), Beihang Univ, State Key Lab Software Dev Environm, Sch Comp Sci & Engn, Beijing 100191, Peoples R China.; Li, WX (corresponding author), Shanghai Artificial Intelligence Lab, Beijing 100191, Peoples R China.
EM lijiankai@buaa.edu.cn; yhwang@buaa.edu.cn; weixinli@buaa.edu.cn
OI Li, Jiankai/0009-0008-8858-7980
FU National Key R&D Program of China [2022ZD0161901]; National Nature
   Science Foundation of China [62276018, U20B2069]
FX This work is sponsored by the National Key R&D Program of China
   (2022ZD0161901) and the National Nature Science Foundation of China (No.
   62276018, U20B2069).
CR Bekker J, 2020, MACH LEARN, V109, P719, DOI 10.1007/s10994-020-05877-5
   Ben-Younes H, 2019, AAAI CONF ARTIF INTE, P8102
   Chang XJ, 2023, IEEE T PATTERN ANAL, V45, P1, DOI 10.1109/TPAMI.2021.3137605
   Chen C, 2022, AAAI CONF ARTIF INTE, P212
   Chen YX, 2019, IEEE I CONF COMP VIS, P8647, DOI 10.1109/ICCV.2019.00874
   Dhamo H, 2020, PROC CVPR IEEE, P5212, DOI 10.1109/CVPR42600.2020.00526
   Dong Xingning, 2022, P IEEE CVF C COMP VI, P19427
   Goel A., 2022, PROC IEEE C COMPUT V, P15596
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Gu JX, 2019, IEEE I CONF COMP VIS, P10322, DOI 10.1109/ICCV.2019.01042
   Hamilton WL, 2017, ADV NEUR IN, V30
   Hung ZS, 2021, IEEE T PATTERN ANAL, V43, P3820, DOI 10.1109/TPAMI.2020.2992222
   Huynh D., 2020, P INT C NEUR INF PRO, V33, P19849
   Isola P, 2015, PROC CVPR IEEE, P1383, DOI 10.1109/CVPR.2015.7298744
   Karthik Shyamgopal, 2022, CVPR, P9336
   Kiryo R, 2017, ADV NEUR IN, V30
   Knyazev B., 2021, P IEEE CVF INT C COM, P15827
   Knyazev Boris, 2020, BMVC, P1
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Li L, 2022, PROC CVPR IEEE, P18847, DOI 10.1109/CVPR52688.2022.01830
   Li Q, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3489142
   Li RJ, 2021, PROC CVPR IEEE, P11104, DOI 10.1109/CVPR46437.2021.01096
   Li XY, 2022, PROC CVPR IEEE, P9316, DOI 10.1109/CVPR52688.2022.00911
   Liang KM, 2018, AAAI CONF ARTIF INTE, P7098
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Lin X., 2022, P IEEE CVF C COMP VI, P19457
   Lin Xin, 2022, P IEEE CVF C COMP VI, P19476
   Liu YB, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3498340
   Lu CW, 2016, LECT NOTES COMPUT SC, V9905, P852, DOI 10.1007/978-3-319-46448-0_51
   Lu YC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15911, DOI 10.1109/ICCV48922.2021.01563
   Lyu X., 2022, P IEEECVF C COMPUTER, P19467
   Mancini M, 2021, PROC CVPR IEEE, P5218, DOI 10.1109/CVPR46437.2021.00518
   Pennington J., 2014, P C EMP METH NAT LAN, P1532, DOI DOI 10.3115/V1/D14-1162
   Pourpanah F, 2023, IEEE T PATTERN ANAL, V45, P4051, DOI 10.1109/TPAMI.2022.3191696
   Radford A, 2021, PR MACH LEARN RES, V139
   Rao YB, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3514247
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sharifzadeh S, 2022, AAAI CONF ARTIF INTE, P2189
   Shi JX, 2019, PROC CVPR IEEE, P8368, DOI 10.1109/CVPR.2019.00857
   Shizhe Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9959, DOI 10.1109/CVPR42600.2020.00998
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Suhail M, 2021, PROC CVPR IEEE, P13931, DOI 10.1109/CVPR46437.2021.01372
   Tang KH, 2020, PROC CVPR IEEE, P3713, DOI 10.1109/CVPR42600.2020.00377
   Tang KH, 2019, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2019.00678
   Teng Y, 2022, PROC CVPR IEEE, P19415, DOI 10.1109/CVPR52688.2022.01883
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang WG, 2022, IEEE T PATTERN ANAL, V44, P3508, DOI 10.1109/TPAMI.2021.3055780
   Wang XH, 2023, IEEE T PATTERN ANAL, V45, P6605, DOI 10.1109/TPAMI.2020.3015894
   Wenbin Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P222, DOI 10.1007/978-3-030-58601-0_14
   Wu Hanjie, 2022, ACM Trans. Multimedia Comput. Commun. Appl., V18, P2
   Xu DF, 2017, PROC CVPR IEEE, P3097, DOI 10.1109/CVPR.2017.330
   Xu Xing, 2021, ACM Trans. Multimedia Comput. Commun. Appl., V17, p1s
   Yanagi R, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3485042
   Yiwu Zhong, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P211, DOI 10.1007/978-3-030-58568-6_13
   Yu Hongchuan, 2023, ACM Trans. Multimedia Comput. Commun. Appl., V19, P3
   Yuan J, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3394955
   Yutian Guo, 2020, ICMR '20: Proceedings of the 2020 International Conference on Multimedia Retrieval, P9, DOI 10.1145/3372278.3390709
   Zareian Alireza, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P642, DOI 10.1007/978-3-030-58592-1_38
   Zellers R, 2018, PROC CVPR IEEE, P5831, DOI 10.1109/CVPR.2018.00611
   Zhang HW, 2017, PROC CVPR IEEE, P3107, DOI 10.1109/CVPR.2017.331
   Zhang LL, 2023, IEEE T PATTERN ANAL, V45, P3848, DOI 10.1109/TPAMI.2022.3183586
   Zhang Y, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3542820
   Zhao Y., 2022, P IEEECVF C COMPUTER, P14461
   Zhong YW, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1803, DOI 10.1109/ICCV48922.2021.00184
   Zhou TF, 2022, IEEE T PATTERN ANAL, V44, P2827, DOI 10.1109/TPAMI.2021.3049156
NR 65
TC 0
Z9 0
U1 4
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2024
VL 20
IS 1
AR 5
DI 10.1145/3604284
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T8LL3
UT WOS:001080441800005
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Wang, J
   Shuai, HH
   Li, YH
   Cheng, WH
AF Wang, Jia
   Shuai, Hong-Han
   Li, Yung-Hui
   Cheng, Wen-Huang
TI Language-guided Residual Graph Attention Network and Data Augmentation
   for Visual Grounding
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Residual graph attention network; data augmentation; visual grounding
AB Visual grounding is an essential task in understanding the semantic relationship between the given text description and the target object in an image. Due to the innate complexity of language and the rich semantic context of the image, it is still a challenging problem to infer the underlying relationship and to perform reasoning between the objects in an image and the given expression. Although existing visual grounding methods have achieved promising progress, cross-modal mapping across different domains for the task is still not well handled, especially when the expressions are complex and long. To address the issue, we propose a language-guided residual graph attention network for visual grounding (LRGAT-VG), which enables us to apply deeper graph convolution layers with the assistance of residual connections between them. This allows us to better handle long and complex expressions than other graph-based methods. Furthermore, we perform a Language-guided Data Augmentation (LGDA), which is based on copy-paste operations on pairs of source and target images to increase the diversity of training data while maintaining the relationship between the objects in the image and the expression. With extensive experiments on three visual grounding benchmarks, including RefCOCO, RefCOCO+, and RefCOCOg, LRGAT-VG with LGDA achieves competitive performance with other state-of-the-art graph network-based referring expression approaches and demonstrates its effectiveness.
C1 [Wang, Jia; Shuai, Hong-Han] Natl Yang Ming Chiao Tung Univ, 1001 Univ Rd, Hsinchu 30010, Taiwan.
   [Li, Yung-Hui] Hon Hai Res Inst, 2 Ziyou St,Tucheng Dist 32 Jihu Rd, Taipei 114699, Taiwan.
   [Cheng, Wen-Huang] Natl Taiwan Univ, 1,Sec 4,Roosevelt Rd, Taipei, Taiwan.
C3 National Yang Ming Chiao Tung University; National Taiwan University
RP Cheng, WH (corresponding author), Natl Taiwan Univ, 1,Sec 4,Roosevelt Rd, Taipei, Taiwan.
EM vicky.ee08@nycu.edu.tw; hhshuai@nycu.edu.tw; yunghui.li@foxconn.com;
   wenhuang@csie.ntu.edu.tw
OI Cheng, Wen-Huang/0000-0002-4662-7875; Shuai,
   Hong-Han/0000-0003-2216-077X; Li, Yung-Hui/0000-0002-0475-3689
FU National Science and Technology Council of Taiwan
   [NSTC-109-2223-E-009-002-MY3, NSTC-111-2634-F-007-002]
FX This work was supported in part by the National Science and Technology
   Council of Taiwan under Grants No. NSTC-109-2223-E-009-002-MY3 and No.
   NSTC-111-2634-F-007-002.
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12
   Chang HC, 2021, IEEE INT C INT ROBOT, P452, DOI 10.1109/IROS51168.2021.9636510
   Chen DL, 2020, AAAI CONF ARTIF INTE, V34, P3438
   Chen K, 2017, IEEE I CONF COMP VIS, P824, DOI 10.1109/ICCV.2017.95
   Chen L, 2021, AAAI CONF ARTIF INTE, V35, P1036
   Chen Xinchi, 2015, P 2015 C EMP METH NA, P1197, DOI DOI 10.18653/V1/D15-1141
   Cui Yuhao, 2021, ROSITA: Enhancing Vision-and-Language Semantic Alignments via Crossand Intra-Modal Knowledge Integration, P797, DOI [10.1145/3474085.3475251, DOI 10.1145/3474085.3475251]
   Das Gollapalli S, 2014, AAAI CONF ARTIF INTE, P1629
   Deng J., 2021, Proceedings of the IEEE/CVF International Conference on Computer Vision, P1769
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dhiman C, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3441628
   Du Y., 2022, P IEEE INT C MULT EX, P1
   Dvornik N, 2018, LECT NOTES COMPUT SC, V11216, P375, DOI 10.1007/978-3-030-01258-8_23
   Fang HS, 2019, IEEE I CONF COMP VIS, P682, DOI 10.1109/ICCV.2019.00077
   Gan Z., 2020, Adv. Neural Inf .Process. Syst, V33, P6616
   Gen Luo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10031, DOI 10.1109/CVPR42600.2020.01005
   Ghiasi G, 2021, PROC CVPR IEEE, P2917, DOI 10.1109/CVPR46437.2021.00294
   He KM, 2019, IEEE I CONF COMP VIS, P4917, DOI 10.1109/ICCV.2019.00502
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   HELBING D, 1995, PHYS REV E, V51, P4282, DOI 10.1103/PhysRevE.51.4282
   Hu RH, 2017, IEEE I CONF COMP VIS, P804, DOI 10.1109/ICCV.2017.93
   Hu RH, 2017, PROC CVPR IEEE, P4418, DOI 10.1109/CVPR.2017.470
   Hu RH, 2016, PROC CVPR IEEE, P4555, DOI 10.1109/CVPR.2016.493
   Kamath A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1760, DOI 10.1109/ICCV48922.2021.00180
   Kingma D. P., 2014, arXiv
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Li GH, 2019, IEEE I CONF COMP VIS, P9266, DOI 10.1109/ICCV.2019.00936
   Li Kun, 2023, ACM Transactions on Multimedia Computing, Communications and Applications, V19, P1
   Li L, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3359753
   Li Q, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3489142
   Liao Yiyi, 2020, CVPR
   Liao Y, 2022, PROC CVPR IEEE, P20091, DOI 10.1109/CVPR52688.2022.01949
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu SY, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P730, DOI 10.1109/ACPR.2015.7486599
   Lo L, 2022, IEEE T MULTIMEDIA, V24, P4275, DOI 10.1109/TMM.2022.3197365
   Lung KY, 2021, PATTERN RECOGN LETT, V144, P82, DOI 10.1016/j.patrec.2021.01.011
   Luo HS, 2022, NEUROCOMPUTING, V508, P293, DOI 10.1016/j.neucom.2022.07.028
   Luo RT, 2017, PROC CVPR IEEE, P3125, DOI 10.1109/CVPR.2017.333
   Ma YJ, 2022, IEEE T MULTIMEDIA, V24, P261, DOI 10.1109/TMM.2021.3050059
   Mao JH, 2016, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2016.9
   Messina N, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3451390
   Nagaraja VK, 2016, LECT NOTES COMPUT SC, V9908, P792, DOI 10.1007/978-3-319-46493-0_48
   Ordonez Vicente, 2011, Adv. Neural Info. Process. Syst., V24
   Plummer BA, 2015, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2015.303
   Qian SS, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3451215
   Qin TX, 2020, IEEE INT CONF MOB DA, P210, DOI 10.1109/MDM48529.2020.00044
   Qu MX, 2022, LECT NOTES COMPUT SC, V13695, P546, DOI 10.1007/978-3-031-19833-5_32
   Radford A, 2021, PR MACH LEARN RES, V139
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rohrbach A, 2016, LECT NOTES COMPUT SC, V9905, P817, DOI 10.1007/978-3-319-46448-0_49
   Romero A., 2018, INT C LEARN REPR, P2920, DOI DOI 10.48550/ARXIV.1710.10903
   Sadhu A, 2019, IEEE I CONF COMP VIS, P4693, DOI 10.1109/ICCV.2019.00479
   Scaiella Antonio, 2019, Italian Journal of Computational Linguistics, V2, P49
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Sharma P, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2556
   Shen S., 2021, arXiv
   Sio CH, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1948, DOI 10.1145/3394171.3413611
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tsai TH, 2014, IEEE T IMAGE PROCESS, V23, P1047, DOI 10.1109/TIP.2014.2298982
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang CH, 2024, IEEE CONSUM ELECTR M, V13, P51, DOI 10.1109/MCE.2022.3181759
   Wang J, 2022, IEEE IMAGE PROC, P326, DOI 10.1109/ICIP46576.2022.9897564
   Wang L, 2016, PROC CVPR IEEE, P5005, DOI 10.1109/CVPR.2016.541
   Wang P, 2019, PROC CVPR IEEE, P1960, DOI 10.1109/CVPR.2019.00206
   Wang Tiantian, 2020, P 30 BRIT MACH VIS C
   Wang X, 2022, PATTERN RECOGN LETT, V160, P66, DOI 10.1016/j.patrec.2022.05.024
   Xie HX, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2871, DOI 10.1145/3394171.3414012
   Yang L, 2022, PROC CVPR IEEE, P9489, DOI 10.1109/CVPR52688.2022.00928
   Yang L, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3386725
   Yang S., 2020, P IEEE CVF C COMP VI, P9952
   Yang SB, 2021, IEEE T PATTERN ANAL, V43, P2765, DOI 10.1109/TPAMI.2020.2973983
   Yang SB, 2019, IEEE I CONF COMP VIS, P4643, DOI 10.1109/ICCV.2019.00474
   Yang SB, 2019, PROC CVPR IEEE, P4140, DOI 10.1109/CVPR.2019.00427
   Yang ZY, 2019, IEEE I CONF COMP VIS, P4682, DOI 10.1109/ICCV.2019.00478
   Yen-Chun Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P104, DOI 10.1007/978-3-030-58577-8_7
   Yu DF, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3316767
   Yu LC, 2018, PROC CVPR IEEE, P1307, DOI 10.1109/CVPR.2018.00142
   Yu LC, 2017, PROC CVPR IEEE, P3521, DOI 10.1109/CVPR.2017.375
   Yu LC, 2016, LECT NOTES COMPUT SC, V9906, P69, DOI 10.1007/978-3-319-46475-6_5
   Zhang HW, 2018, PROC CVPR IEEE, P4158, DOI 10.1109/CVPR.2018.00437
   Zhang Si, 2019, Comput Soc Netw, V6, P11, DOI 10.1186/s40649-019-0069-y
   Zhu C, 2022, AAAI CONF ARTIF INTE, P3598
   Zhuang BH, 2018, PROC CVPR IEEE, P4252, DOI 10.1109/CVPR.2018.00447
NR 84
TC 0
Z9 0
U1 6
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2024
VL 20
IS 1
AR 7
DI 10.1145/3604557
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T8LL3
UT WOS:001080441800007
DA 2024-08-05
ER

PT J
AU Zhao, XJ
   Xu, T
   Shen, QQ
   Liu, Y
   Chen, YY
   Su, JY
AF Zhao, Xiaojia
   Xu, Tingting
   Shen, Qiangqiang
   Liu, Youfa
   Chen, Yongyong
   Su, Jingyong
TI Double High-Order Correlation Preserved Robust Multi-View Ensemble
   Clustering
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Ensemble clustering; hypergraph learning; tensor representation;
   highorder correlation
ID LOW-RANK; ITEM RECOMMENDATION
AB Ensemble clustering (EC), utilizing multiple basic partitions (BPs) to yield a robust consensus clustering, has shown promising clustering performance. Nevertheless, most current algorithms suffer from two challenging hurdles: (1) a surge of EC-based methods only focus on pair-wise sample correlation while fully ignoring the high-order correlations of diverse views. (2) they deal directly with the co-association (CA) matrices generated from BPs, which are inevitably corrupted by noise and thus degrade the clustering performance. To address these issues, we propose a novel Double High-Order Correlation Preserved Robust Multi-View Ensemble Clustering (DC-RMEC) method, which preserves the high-order inter-view correlation and the high-order correlation of original data simultaneously. Specifically, DC-RMEC constructs a hypergraph from BPs to fuse high-level complementary information from different algorithms and incorporates multiple CA-based representations into a low-rank tensor to discover the high-order relevance underlying CA matrices, such that double high-order correlation of multi-view features could be dexterously uncovered. Moreover, a marginalized denoiser is invoked to gain robust view-specific CA matrices. Furthermore, we develop a unified framework to jointly optimize the representation tensor and the result matrix. An effective iterative optimization algorithm is designed to optimize our DC-RMEC model by resorting to the alternating direction method of multipliers. Extensive experiments on seven real-world multi-view datasets have demonstrated the superiority of DC-RMEC compared with several state-of-the-art multi-view ensemble clustering methods.
C1 [Zhao, Xiaojia; Xu, Tingting; Chen, Yongyong; Su, Jingyong] Harbin Inst Technol Shenzhen, Sch Comp Sci & Technol, Harbin 518055, Peoples R China.
   [Shen, Qiangqiang] Harbin Inst Technol Shenzhen, Sch Elect & Informat Engn, Harbin 518055, Peoples R China.
   [Liu, Youfa] Huazhong Agr Univ, Coll Informat, Wuhan 430070, Peoples R China.
C3 Harbin Institute of Technology; Harbin Institute of Technology; Huazhong
   Agricultural University
RP Chen, YY (corresponding author), Harbin Inst Technol Shenzhen, Sch Comp Sci & Technol, Harbin 518055, Peoples R China.
EM 21S151152@stu.hit.edu.cn; 21s151168@stu.hit.edu.cn;
   YongyongChen.cn@gmail.com; sujingyong@hit.edu.cn; 1120810623@hit.edu.cn;
   liuyoufa@mail.hzau.edu.cn
RI ; Chen, yongyong/P-3801-2016
OI Shen, Qiangqiang/0000-0002-3564-6042; Chen,
   yongyong/0000-0003-1970-1993; Su, Jingyong/0000-0003-3216-7027; Liu,
   Youfa/0000-0002-3540-5775
FU National Natural Science Foundation of China [62106063]; Guangdong
   Natural Science Foundation [2022A1515010819]; Shenzhen Science and
   Technology Program [RCBS20210609103708013]; Humanities and Social
   Sciences Foundation of the Ministry of Education of China [22YJC630129];
   Guangdong Provincial Key Laboratory of Novel Security Intelligence
   Technologies [2022B1212010005]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62106063, by the Guangdong Natural
   Science Foundation under Grant 2022A1515010819, by the Shenzhen Science
   and Technology Program under Grant RCBS20210609103708013, by Humanities
   and Social Sciences Foundation of the Ministry of Education of China
   under Grant 22YJC630129, and by Guangdong Provincial Key Laboratory of
   Novel Security Intelligence Technologies under Grant 2022B1212010005.
CR [Anonymous], 2011, P ADV NEURAL INFORM
   [Anonymous], 2006, 19 INT C NEURAL INFO
   Brbic M, 2018, PATTERN RECOGN, V73, P247, DOI 10.1016/j.patcog.2017.08.024
   CARROLL JD, 1970, PSYCHOMETRIKA, V35, P283, DOI 10.1007/BF02310791
   Chen CF, 2012, PROC CVPR IEEE, P2618, DOI 10.1109/CVPR.2012.6247981
   Chen M., 2012, P 29 INT C MACHINE L, P1627
   Chen YY, 2022, IEEE T CIRC SYST VID, V32, P92, DOI 10.1109/TCSVT.2021.3055625
   Cristofor D, 2002, J UNIVERS COMPUT SCI, V8, P153
   Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57
   Fang XZ, 2019, IEEE T NEUR NET LEAR, V30, P1133, DOI 10.1109/TNNLS.2018.2861839
   Fern X.Z., 2004, P 21 INT C MACH LEAR, DOI [DOI 10.1145/1015330.1015414, 10.1145/1015330.1015414]
   Francis J, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3399806
   Franek L, 2014, PATTERN RECOGN, V47, P833, DOI 10.1016/j.patcog.2013.08.019
   Fred ALN, 2005, IEEE T PATTERN ANAL, V27, P835, DOI 10.1109/TPAMI.2005.113
   Hong Jia, 2021, 2021 12th International Conference on Computing Communication and Networking Technologies (ICCCNT), P367, DOI 10.1109/CIS54983.2021.00083
   Huang D., 2023, IEEE Trans. Knowl. Data Eng.
   Huang D, 2021, IEEE T SYST MAN CY-S, V51, P508, DOI 10.1109/TSMC.2018.2876202
   Huang D, 2018, IEEE T CYBERNETICS, V48, P1460, DOI 10.1109/TCYB.2017.2702343
   Huang D, 2016, IEEE T KNOWL DATA EN, V28, P1312, DOI 10.1109/TKDE.2015.2503753
   Huang D, 2016, PATTERN RECOGN, V50, P131, DOI 10.1016/j.patcog.2015.08.015
   Iam-On N, 2011, IEEE T PATTERN ANAL, V33, P2396, DOI 10.1109/TPAMI.2011.84
   Kilmer ME, 2013, SIAM J MATRIX ANAL A, V34, P148, DOI 10.1137/110837711
   LADES M, 1993, IEEE T COMPUT, V42, P300, DOI 10.1109/12.210173
   Lauer F, 2009, IEEE I CONF COMP VIS, P678, DOI 10.1109/ICCV.2009.5459173
   Li Fei-Fei, 2004, P WORKSH GEN MOD BAS
   Li HM, 2020, IEEE DATA MINING, P1094, DOI 10.1109/ICDM50108.2020.00131
   Li Y., 2015, P 29 AAAI C ART INT
   Liu G., 2010, P INT C MACH LEARN, P663
   Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88
   Liu HF, 2018, ACM T KNOWL DISCOV D, V12, DOI 10.1145/3182384
   Liu HF, 2018, DATA MIN KNOWL DISC, V32, P385, DOI 10.1007/s10618-017-0539-5
   Liu HF, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1745, DOI 10.1145/2939672.2939813
   Liu HF, 2017, IEEE T KNOWL DATA EN, V29, P1129, DOI 10.1109/TKDE.2017.2650229
   Liu HF, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P715, DOI 10.1145/2783258.2783287
   Liu HF, 2015, IEEE DATA MINING, P877, DOI 10.1109/ICDM.2015.18
   Ng AY, 2002, ADV NEUR IN, V14, P849
   Nie F., 2016, IJCAI, P1881
   Rafailidis D, 2013, IEEE T SYST MAN CY-S, V43, P673, DOI 10.1109/TSMCA.2012.2208186
   Rao S, 2010, IEEE T PATTERN ANAL, V32, P1832, DOI 10.1109/TPAMI.2009.191
   Strehl A, 2002, EIGHTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-02)/FOURTEENTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-02), PROCEEDINGS, P93, DOI 10.1162/153244303321897735
   Sun MJ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3528, DOI 10.1145/3474085.3475516
   Symeonidis P, 2016, IEEE T SYST MAN CY-S, V46, P1240, DOI 10.1109/TSMC.2015.2482458
   Tao ZQ, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2843
   Tao ZQ, 2023, IEEE T NEUR NET LEAR, V34, P2670, DOI 10.1109/TNNLS.2021.3107354
   Tao ZQ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3562
   Tao ZQ, 2020, IEEE T NEUR NET LEAR, V31, P600, DOI 10.1109/TNNLS.2019.2906867
   Tao ZQ, 2019, ACM T KNOWL DISCOV D, V13, DOI 10.1145/3278606
   Tao ZQ, 2016, CIKM'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P367, DOI 10.1145/2983323.2983745
   TUCKER LR, 1966, PSYCHOMETRIKA, V31, P279, DOI 10.1007/BF02289464
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wang H, 2020, IEEE T KNOWL DATA EN, V32, P1116, DOI 10.1109/TKDE.2019.2903810
   Wang SW, 2022, IEEE T IMAGE PROCESS, V31, P556, DOI 10.1109/TIP.2021.3131941
   Wang XY, 2009, IEEE I CONF COMP VIS, P32, DOI 10.1109/iccv.2009.5459207
   Wang Y., 2016, P 25 INT JOINT C ART, P2153
   Wang Y, 2018, IEEE T NEUR NET LEAR, V29, P4833, DOI 10.1109/TNNLS.2017.2777489
   Wang Y, 2018, NEURAL NETWORKS, V103, P1, DOI 10.1016/j.neunet.2018.03.006
   Wang Y, 2016, KNOWL INF SYST, V46, P515, DOI 10.1007/s10115-015-0833-8
   Wu JJ, 2015, IEEE T KNOWL DATA EN, V27, P155, DOI 10.1109/TKDE.2014.2316512
   Wu JJ, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P877
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2149
   Xie Y, 2018, INT J COMPUT VISION, V126, P1157, DOI 10.1007/s11263-018-1086-2
   Yuan B, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3290047
   Zhang Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1569, DOI 10.1145/3343031.3351023
   Zhou P, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P4112
NR 64
TC 0
Z9 0
U1 8
U2 21
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2024
VL 20
IS 1
AR 21
DI 10.1145/3612923
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T8LL3
UT WOS:001080441800021
DA 2024-08-05
ER

PT J
AU Deng, MY
   Zhang, WY
   Zhao, J
   Wang, Z
   Zhou, ML
   Luo, J
   Chen, C
AF Deng, Mingyu
   Zhang, Wanyi
   Zhao, Jie
   Wang, Zhu
   Zhou, Mingliang
   Luo, Jun
   Chen, Chao
TI Make Partition Fit Task: A Novel Framework for Joint Learning of City
   Region Partition and Representation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Region partition; partition learning; representation learning;
   prediction task; multimodal big data
AB The proliferation of multimodal big data in cities provides unprecedented opportunities for modeling and forecasting urban problems, such as crime prediction and house price prediction, through data-driven approaches. A fundamental and critical issue in modeling and forecasting urban problems lies in identifying suitable spatial analysis units, also known as city region partition. Existing works rely on subjective domain knowledge for static partitions, which is general and universal for all tasks. In fact, different tasks may need different city region partitions. To address this issue, we propose JLPR, a task-oriented framework for Joint Learning of region Partition and Representation. To make partitions fit tasks, JLPR integrates the region partition into the representation model training and learns region partitions using the supervision signal from the downstream task. We evaluate the framework on two prediction tasks (i.e., crime prediction and housing price prediction) in Chicago. Experiments show that JLPR consistently outperforms state-of-the-art partitioning methods in both tasks, which achieves above 25% and 70% performance improvements in terms of mean absolute error for crime prediction and house price prediction tasks, respectively. Additionally, we meticulously undertake three visualization case studies, which yield profound and illuminating findings from diverse perspectives, demonstrating the remarkable effectiveness and superiority of our approach.
C1 [Deng, Mingyu; Zhang, Wanyi; Zhao, Jie; Zhou, Mingliang; Luo, Jun; Chen, Chao] Chongqing Univ, 174 Shazhengjie, Chongqing 400044, Peoples R China.
   [Wang, Zhu] Northwestern Polytech Univ, 127 West Youyi Rd, Xian 710072, Shaanxi, Peoples R China.
C3 Chongqing University; Northwestern Polytechnical University
RP Zhang, WY (corresponding author), Chongqing Univ, 174 Shazhengjie, Chongqing 400044, Peoples R China.
EM dmy@cqu.edu.cn; wanyi.zhang@cqu.edu.cn; csjiezhao@cqu.edu.cn;
   wangzhu@nwpu.edu.cn; mingliang.zhou@cqu.edu.cn; luoj@cqu.edu.cn;
   cschaochen@cqu.edu.cn
OI Zhao, Jie/0000-0003-0039-6220; Zhou, Mingliang/0000-0002-1874-3641
FU NSFC [62322601, 62172066]; Excellent Youth Foundation of Chongqing
   [CSTB2023NSCQJQX0025]; Fundamental Research Funds for the Central
   Universities [2023CDJXY-038]
FX This work was supported by the NSFC (no. 62322601 and 62172066), the
   Excellent Youth Foundation of Chongqing (no. CSTB2023NSCQJQX0025), and
   the Fundamental Research Funds for the Central Universities (no.
   2023CDJXY-038).
CR Arik SÖ, 2021, NPJ DIGIT MED, V4, DOI 10.1038/s41746-021-00511-7
   Bianchi FM, 2020, PR MACH LEARN RES, V119
   Butt UM, 2020, IEEE ACCESS, V8, P166553, DOI 10.1109/ACCESS.2020.3022808
   Castro PS, 2013, ACM COMPUT SURV, V46, DOI 10.1145/2543581.2543584
   Chen C, 2022, IEEE T BIG DATA, V8, P1550, DOI 10.1109/TBDATA.2021.3063048
   Chen C, 2016, IT PROF, V18, P14, DOI 10.1109/MITP.2016.2
   Deng MY, 2022, FRONT COMPUT SCI-CHI, V16, DOI 10.1007/s11704-020-0007-z
   Duval A, 2022, PROCEEDINGS OF THE 31ST ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, CIKM 2022, P426, DOI 10.1145/3511808.3557353
   Fu YJ, 2019, AAAI CONF ARTIF INTE, P906
   Fu YJ, 2014, IEEE DATA MINING, P120, DOI 10.1109/ICDM.2014.18
   Gebru T, 2017, P NATL ACAD SCI USA, V114, P13108, DOI 10.1073/pnas.1700035114
   Getis A, 2004, GEOGR ANAL, V36, P90, DOI 10.1111/j.1538-4632.2004.tb01127.x
   Hartigan J. A., 1979, Applied Statistics, V28, P100, DOI 10.2307/2346830
   Huang TY, 2021, Arxiv, DOI arXiv:2105.02489
   Jenkins P, 2019, PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM '19), P1993, DOI 10.1145/3357384.3358001
   Jiang ZH, 2020, FRONT COMPUT SCI-CHI, V14, DOI 10.1007/s11704-019-9034-z
   Kang Gaganjot Kaur, 2018, International Journal of Environmental Science and Development, V9, P8, DOI [DOI 10.18178/IJESD.2018.9.1.1066, 10.18178/ijesd.2018.9.1.1066]
   Kang YH, 2021, LAND USE POLICY, V111, DOI 10.1016/j.landusepol.2020.104919
   Kim K, 2020, IEEE T INTELL TRANSP, V21, P2002, DOI 10.1109/TITS.2019.2910548
   Kingma D. P., 2014, arXiv
   Li FX, 2022, PROCEEDINGS OF THE 31ST ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, CIKM 2022, P1084, DOI 10.1145/3511808.3557243
   Li Tong, 2023, ACM Transactions on Intelligent Systems and Technology, V14, P1
   Li YX, 2015, 23RD ACM SIGSPATIAL INTERNATIONAL CONFERENCE ON ADVANCES IN GEOGRAPHIC INFORMATION SYSTEMS (ACM SIGSPATIAL GIS 2015), DOI 10.1145/2820783.2820837
   Li YF, 2022, PROCEEDINGS OF THE 31ST ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, CIKM 2022, P1209, DOI 10.1145/3511808.3557458
   Liang YX, 2021, PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE 2021 (WWW 2021), P1833, DOI 10.1145/3442381.3449792
   Miller HJ, 2004, ANN ASSOC AM GEOGR, V94, P284, DOI 10.1111/j.1467-8306.2004.09402005.x
   Murtagh F, 2014, J CLASSIF, V31, P274, DOI 10.1007/s00357-014-9161-z
   Kipf TN, 2017, Arxiv, DOI arXiv:1609.02907
   Ng AY, 2002, ADV NEUR IN, V14, P849
   Peng Richard, 2015, The Conference on Learning Theory (COLT), P1423
   Piyadasun T, 2017, 2017 3RD INTERNATIONAL MORATUWA ENGINEERING RESEARCH CONFERENCE (MERCON), P431, DOI 10.1109/MERCon.2017.7980523
   Qin YR, 2024, ACM T INFORM SYST, V42, DOI 10.1145/3595632
   Qu Hao, 2022, IEEE Transactions on Knowledge and Data Engineering
   Schubert E, 2017, ACM T DATABASE SYST, V42, DOI 10.1145/3068335
   Sun JK, 2022, IEEE T KNOWL DATA EN, V34, P2348, DOI 10.1109/TKDE.2020.3008774
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tsitsulin A, 2022, Arxiv, DOI arXiv:2006.16904
   Wang HJ, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P3300, DOI 10.1145/3308558.3313704
   Wang HJ, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P237, DOI 10.1145/3132847.3133006
   Wang HJ, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P635, DOI 10.1145/2939672.2939736
   Wang X, 2020, IEEE T MOBILE COMPUT, V19, P2804, DOI 10.1109/TMC.2019.2934461
   Wang ZC, 2020, AAAI CONF ARTIF INTE, V34, P1013
   Wen CC, 2019, SCI TOTAL ENVIRON, V654, P1091, DOI 10.1016/j.scitotenv.2018.11.086
   Wu SB, 2022, Arxiv, DOI arXiv:2201.09760
   Xu Liming, 2023, ACM Transactions on Multimedia Computing, Communications and Applications, V19, P1
   Yadav A, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3517139
   Yao ZJ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3919
   Yin XY, 2022, IEEE T INTELL TRANSP, V23, P4927, DOI 10.1109/TITS.2021.3054840
   Ying R, 2018, ADV NEUR IN, V31
   Yuan J, 2012, Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, P186
   Yunpeng Zheng, 2015, Cloud Computing and Big Data. Second International Conference, CloudCom-Asia 2015. Revised Selected Papers: LNCS 9106, P165, DOI 10.1007/978-3-319-28430-9_13
   Zhang MY, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4431
   Zhang YC, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1700, DOI 10.1145/3292500.3330972
   Zhou ZH, 2022, FRONT COMPUT SCI-CHI, V16, DOI 10.1007/s11704-022-2900-0
NR 54
TC 0
Z9 0
U1 1
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2024
VL 20
IS 7
SI SI
AR 210
DI 10.1145/3652857
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL0Q6
UT WOS:001234494100025
DA 2024-08-05
ER

PT J
AU Lin, WY
   Zhang, YF
   Dai, WR
   Liu, HB
   See, J
   Xiong, HK
AF Lin, Weiyao
   Zhang, Yufeng
   Dai, Wenrui
   Liu, Huabin
   See, John
   Xiong, Hongkai
TI Scene Graph Lossless Compression with Adaptive Prediction for Objects
   and Relations
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Scene graph compression; image compression; lossless compression
ID IMAGE; ALGORITHM
AB The scene graph is a novel data structure describing objects and their pairwise relationship within image scenes. As the size of scene graphs in vision and multimedia applications increases, the need for lossless storage and transmission of such data becomes more critical. However, the compression of scene graphs is less studied because of the complicated data structures involved and complex distributions. Existing solutions usually involve general-purpose compressors or graph structure compression methods, which are weak at reducing the redundancy in scene graph data. This article introduces a novel lossless compression framework with adaptive predictors for the joint compression of objects and relations in scene graph data. The proposed framework comprises a unified prior extractor and specialized element predictors to adapt to different data elements. Furthermore, to exploit the context information within and between graph elements, Graph Context Convolution is proposed to support different graph context modeling schemes for different graph elements. Finally, an overarching framework incorporates the learned distribution model to predict numerical data under complicated conditional constraints. Experiments conducted on labeled or generated scene graphs demonstrate the effectiveness of the proposed framework for scene graph lossless compression.
C1 [Lin, Weiyao; Zhang, Yufeng; Liu, Huabin; Xiong, Hongkai] Shanghai Jiao Tong Univ, Dept Elect Engn, 800 Dongchuan Rd, Shanghai, Peoples R China.
   [Dai, Wenrui] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, 800 Dongchuan Rd, Shanghai, Peoples R China.
   [See, John] Heriot Watt Univ Malaysia, Sch Math & Comp Sci, 1 Jalan Venna P5-2,Precinct 5, Putrajaya 62200, Malaysia.
C3 Shanghai Jiao Tong University; Shanghai Jiao Tong University; Heriot
   Watt University
RP Lin, WY (corresponding author), Shanghai Jiao Tong Univ, Dept Elect Engn, 800 Dongchuan Rd, Shanghai, Peoples R China.
EM wylin@sjtu.edu.cn; worldlife@sjtu.edu.cn; daiwenrui@sjtu.edu.cn;
   huabinliu@sjtu.edu.cn; j.see@hw.ac.uk; xionghongkai@sjtu.edu.cn
RI ; See, John/C-8633-2013
OI Xiong, Hongkai/0000-0003-4552-0029; See, John/0000-0003-3005-4109
FU National Natural Science Foundation of China [U21B2013, 62325109]
FX The article is supported in part by the National Natural Science
   Foundation of China (No. U21B2013, 62325109).
CR Alakuijala J, 2019, ACM T INFORM SYST, V37, DOI 10.1145/3231935
   Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24
   [Anonymous], 2019, ADV NEUR IN
   Balle J., 2018, ICLR
   Balle J., 2017, P INT C LEARN REPR
   Besta M, 2019, Arxiv, DOI arXiv:1806.01799
   Brisaboa NR, 2009, LECT NOTES COMPUT SC, V5721, P18, DOI 10.1007/978-3-642-03784-9_3
   Chang XJ, 2023, IEEE T PATTERN ANAL, V45, P1, DOI 10.1109/TPAMI.2021.3137605
   Chen T, 2021, IEEE T IMAGE PROCESS, V30, P3179, DOI 10.1109/TIP.2021.3058615
   Cheng YH, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3499027
   CLEARY JG, 1984, IEEE T COMMUN, V32, P396, DOI 10.1109/TCOM.1984.1096090
   Collet Yann, 2021, Zstandard-Real-time compression algorithm
   Duda Jarek, 2013, arXiv
   Fu HS, 2024, Arxiv, DOI arXiv:2107.06463
   Fukui A., 2016, ARXIV160601847, P457, DOI DOI 10.18653/V1/D16-1044
   Gailly Jean-Loup, 2021, zlib Home Site
   Goyal M, 2020, IEEE DATA COMPR CONF, P372, DOI 10.1109/DCC47342.2020.00065
   Han X., 2021, arXiv
   He DL, 2021, PROC CVPR IEEE, P14766, DOI 10.1109/CVPR46437.2021.01453
   HUFFMAN DA, 1952, P IRE, V40, P1098, DOI 10.1109/JRPROC.1952.273898
   Jiang WT, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3460474
   Jingwei Ji, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10233, DOI 10.1109/CVPR42600.2020.01025
   Kingma F., 2019, INT C MACHINE LEARNI, P3408
   Kipf T.N., 2017, INT C LEARN REPR, P1
   Knoll Byron, 2022, CMIX
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Kuznetsova A, 2020, Arxiv, DOI arXiv:1811.00982
   Li M, 2020, IEEE T IMAGE PROCESS, V29, P5900, DOI 10.1109/TIP.2020.2985225
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin WY, 2023, INT J COMPUT VISION, V131, P2994, DOI 10.1007/s11263-023-01842-6
   Lin WY, 2020, SIGNAL PROCESS-IMAGE, V80, DOI 10.1016/j.image.2019.115659
   Liu HY, 2021, PROC CVPR IEEE, P11541, DOI 10.1109/CVPR46437.2021.01138
   Ma HC, 2022, IEEE T PATTERN ANAL, V44, P1247, DOI 10.1109/TPAMI.2020.3026003
   Ma HC, 2020, IEEE T MULTIMEDIA, V22, P1667, DOI 10.1109/TMM.2019.2957990
   Mahoney Matthew V., 2005, Adaptive weighing of context models for lossless data compression
   Marino K, 2021, PROC CVPR IEEE, P14106, DOI 10.1109/CVPR46437.2021.01389
   Mentzer F, 2019, PROC CVPR IEEE, P10621, DOI 10.1109/CVPR.2019.01088
   Minnen D., 2018, P ANN C NEUR INF PRO, P10794
   Kipf TN, 2016, Arxiv, DOI arXiv:1611.07308
   Pavlov Igor, 2021, LZMA SDK (Software Development Kit)
   Schlichtkrull M, 2018, LECT NOTES COMPUT SC, V10843, P593, DOI 10.1007/978-3-319-93417-4_38
   Schluter Natalie, 2015, SEM
   SHARIR M, 1981, COMPUT MATH APPL, V7, P67, DOI 10.1016/0898-1221(81)90008-0
   Suhail M, 2021, PROC CVPR IEEE, P13931, DOI 10.1109/CVPR46437.2021.01372
   Tang KH, 2020, PROC CVPR IEEE, P3713, DOI 10.1109/CVPR42600.2020.00377
   Vigna Sebastiano, 2004, P 13 INT C WORLD WID, P595
   Wang SJ, 2020, IEEE WINT CONF APPL, P1497, DOI 10.1109/WACV45572.2020.9093614
   Xu DF, 2017, PROC CVPR IEEE, P3097, DOI 10.1109/CVPR.2017.330
   Xu KYL, 2018, PR MACH LEARN RES, V80
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yoon S, 2021, AAAI CONF ARTIF INTE, V35, P10718
   Zhang X, 2021, IEEE T IMAGE PROCESS, V30, P963, DOI 10.1109/TIP.2020.3040074
   Zhang Y, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3514041
   Zhao J, 2024, ACM T MULTIM COMPUT, V20, DOI 10.1145/3580499
   Zhengxue Cheng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7936, DOI 10.1109/CVPR42600.2020.00796
   ZIV J, 1977, IEEE T INFORM THEORY, V23, P337, DOI 10.1109/TIT.1977.1055714
NR 56
TC 0
Z9 0
U1 2
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2024
VL 20
IS 7
SI SI
AR 196
DI 10.1145/3649503
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL0Q6
UT WOS:001234494100011
OA Bronze, Green Submitted
DA 2024-08-05
ER

PT J
AU Sheng, YF
   Tao, M
   Wang, J
   Bao, BK
AF Sheng, Yefei
   Tao, Ming
   Wang, Jie
   Bao, Bing-Kun
TI ISF-GAN: Imagine, Select, and Fuse with GPT-based Text Enrichment for
   Text-to-image Synthesis
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Text-to-image synthesis; adversarial learning; generative pre-trained
   model
ID MACHINE
AB Text-to-Image synthesis aims to generate an accurate and semantically consistent image from a given text description. However, it is difficult for existing generative methods to generate semantically complete images from a single piece of text. Some works try to expand the input text to multiple captions via retrieving similar descriptions of the input text from the training set but still fail to fill in missing image semantics. In this article, we propose a GAN-based approach to Imagine, Select, and Fuse for Text-to-image synthesis, named ISF-GAN. The proposed ISF-GAN contains Imagine Stage and Select and Fuse Stage to solve the above problems. First, the Imagine Stage proposes a text completion and enrichment module. This module guides a GPT-based model to enrich the text expression beyond the original dataset. Second, the Select and Fuse Stage selects qualified text descriptions and then introduces a cross-modal attentional mechanism to interact these different sentence embeddings with the image features at different scales. In short, our proposed model enriches the input text information for completing missing semantics and introduces a cross-modal attentional mechanism to maximize the utilization of enriched text information to generate semantically consistent images. Experimental results on CUB, Oxford-102, and CelebA-HQ datasets prove the effectiveness and superiority of the proposed network. Code is available at https://github.com/Feilingg/ISF-GAN
C1 [Sheng, Yefei; Tao, Ming; Wang, Jie] Nanjing Univ Posts & Telecommun, Coll Telecommun & Informat Engn, Nanjing, Peoples R China.
   [Bao, Bing-Kun] Nanjing Univ Posts & Telecommun, Sch Comp Sci, Nanjing, Peoples R China.
C3 Nanjing University of Posts & Telecommunications; Nanjing University of
   Posts & Telecommunications
RP Bao, BK (corresponding author), Nanjing Univ Posts & Telecommun, Sch Comp Sci, Nanjing, Peoples R China.
EM Ysheng990618@gmail.com; mingtao2000@126.com; jwang_c@njupt.edu.cn;
   bingkunbao@njupt.edu.cn
OI Tao, Ming/0000-0002-4662-7170; Bao, Bingkun/0000-0001-5956-831X
FU National Natural Science Foundation of China [62325206, 61936005,
   62206132]; Key Research and Development Program of Jiangsu Province
   [BE2023016-4]; Natural Science Research Start-up Foundation of
   Recruiting Talents of Nanjing University of Posts and Telecommunications
   [NY222113]; Postgraduate Research & Practice Innovation Program of
   Jiangsu Province [KYCX23_1023]
FX This work was supported by the National Natural Science Foundation of
   China under Grant Nos. 62325206, 61936005, 62206132, the Key Research
   and Development Program of Jiangsu Province under Grant BE2023016-4, the
   Natural Science Research Start-up Foundation of Recruiting Talents of
   Nanjing University of Posts and Telecommunications under Grant No.
   NY222113, and the Postgraduate Research & Practice Innovation Program of
   Jiangsu Province (KYCX23_1023).
CR Alexandr N., 2021, 5 COMPUT METHODSSYST, V2, P748, DOI [DOI 10.1007/978-3-030-90321-3_61, 10.1007/978-3-030-90321-3_61]
   Chang XJ, 2023, IEEE T PATTERN ANAL, V45, P1, DOI 10.1109/TPAMI.2021.3137605
   Cheng F., 2020, CVPR, P10911
   Cheng J, 2022, IEEE T CIRC SYST VID, V32, P5187, DOI 10.1109/TCSVT.2021.3136857
   Deng Zijun, 2023, ACMTransactions onMultimedia Computing, Communications and Applications, V19, P1
   Ding M., 2021, Advances in Neural Information Processing Systems (NeurIPS-21), V34, P19822
   Feng FX, 2022, IEEE T MULTIMEDIA, V24, P2112, DOI 10.1109/TMM.2021.3075997
   Gao LL, 2019, AAAI CONF ARTIF INTE, P8312
   Gu SY, 2022, PROC CVPR IEEE, P10686, DOI 10.1109/CVPR52688.2022.01043
   Gulcehre C, 2018, NEURAL COMPUT, V30, P857, DOI [10.1162/NECO_a_01060, 10.1162/neco_a_01060]
   Hensel M, 2017, ADV NEUR IN, V30
   Huang PD, 2023, 2023 IEEE 8TH INTERNATIONAL CONFERENCE ON BIG DATA ANALYTICS, ICBDA, P159, DOI 10.1109/ICBDA57405.2023.10104850
   Joseph KJ, 2019, IEEE WINT CONF APPL, P358, DOI 10.1109/WACV.2019.00044
   Karras T, 2021, ADV NEUR IN, V34
   Li Bowen, 2019, Adv. Neural Inf. Process. Syst., V32
   Li MJ, 2023, IEEE T PATTERN ANAL, V45, P3918, DOI 10.1109/TPAMI.2022.3181116
   Li RF, 2020, IEEE T MULTIMEDIA, V22, P3075, DOI 10.1109/TMM.2020.2972856
   Li WB, 2019, PROC CVPR IEEE, P12166, DOI 10.1109/CVPR.2019.01245
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu BC, 2021, AAAI CONF ARTIF INTE, V35, P2082
   Luo XD, 2022, KNOWL-BASED SYST, V255, DOI 10.1016/j.knosys.2022.109750
   Peters Matthew E., 2017, Semi-supervised sequence tagging with bidirectional language models, DOI DOI 10.48550/ARXIV.1705.00108
   Qiao TT, 2019, ADV NEUR IN, V32
   Qiao TT, 2019, PROC CVPR IEEE, P1505, DOI 10.1109/CVPR.2019.00160
   Radford A., 2018, Improving language understanding by generative pre-training, P850
   Radford A, 2021, PR MACH LEARN RES, V139
   Ramesh A., 2022, Hierarchical text-conditional image generation with clip latents, DOI 10.48550/arXiv.2204.06125
   Reed S, 2016, ADV NEUR IN, V29
   Reed S, 2016, PR MACH LEARN RES, V48
   Rei M, 2017, Arxiv, DOI arXiv:1704.07156
   Rodriguez JD, 2022, NAACL 2022: THE 2022 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES, P1213
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Salimans T, 2016, ADV NEUR IN, V29
   Sievers Bjarne, 2020, C LABS EV FOR
   Sun JX, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2290, DOI 10.1145/3474085.3475391
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tan HC, 2023, IEEE T MULTIMEDIA, V25, P8620, DOI 10.1109/TMM.2023.3238554
   Tan HC, 2019, IEEE I CONF COMP VIS, P10500, DOI 10.1109/ICCV.2019.01060
   Tao M, 2022, Arxiv, DOI arXiv:2008.05865
   Tewel Y, 2022, Arxiv, DOI arXiv:2207.11100
   Wah C., 2011, Tech. Rep. CNS-TR-2011-001
   Wen J, 2019, AAAI CONF ARTIF INTE, P5401
   Xia WH, 2021, PROC CVPR IEEE, P2256, DOI 10.1109/CVPR46437.2021.00229
   Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143
   Yan CX, 2022, IEEE T PATTERN ANAL, V44, P9733, DOI 10.1109/TPAMI.2021.3127346
   Yang YH, 2021, IEEE T IMAGE PROCESS, V30, P2798, DOI 10.1109/TIP.2021.3055062
   Yang ZY, 2022, AAAI CONF ARTIF INTE, P3081
   Yin GJ, 2019, PROC CVPR IEEE, P2322, DOI 10.1109/CVPR.2019.00243
   Yuan BW, 2024, IEEE T MULTIMEDIA, V26, P1255, DOI 10.1109/TMM.2023.3278992
   Yuan MK, 2020, IEEE T MULTIMEDIA, V22, P1955, DOI 10.1109/TMM.2019.2951463
   Zhang H, 2019, IEEE T PATTERN ANAL, V41, P1947, DOI 10.1109/TPAMI.2018.2856256
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhang LL, 2023, IEEE T PATTERN ANAL, V45, P3848, DOI 10.1109/TPAMI.2022.3183586
   Zhang Zhenxing, 2022, arXiv
   Zhang ZZ, 2018, PROC CVPR IEEE, P6199, DOI 10.1109/CVPR.2018.00649
   Zhou Y., 2021, arXiv
   Zhu MF, 2019, PROC CVPR IEEE, P5795, DOI 10.1109/CVPR.2019.00595
NR 57
TC 0
Z9 0
U1 4
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2024
VL 20
IS 7
SI SI
AR 222
DI 10.1145/3650033
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL0Q6
UT WOS:001234494100037
OA Bronze
DA 2024-08-05
ER

PT J
AU Lyu, J
   Wang, GM
   Hossain, MS
AF Lyu, Jun
   Wang, Guangming
   Hossain, M. Shamim
TI Iterative Temporal-spatial Transformer-based Cardiac T1 Mapping MRI
   Reconstruction
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Cardiac MRI reconstruction; multi-level; transformer; temporal
   information; T1 mapping
ID ACCELERATED DYNAMIC MRI
AB The precise reconstruction of acceleratedmagnetic resonance imaging (MRI) brings about notable advantages, such as enhanced diagnostic precision and decreased examination costs. In contrast, traditional cardiac MRI necessitates repetitive acquisitions across multiple heartbeats, resulting in prolonged acquisition times. Significant strides have been made in accelerating MRI through deep learning-based reconstruction methods. However, these existing methods encounter certain limitations: (1) The intricate nature of heart reconstruction involving multiple complex time-series data poses a challenge in exploring nonlinear dependencies between temporal contexts. (2) Existing research often overlooks weight sharing in iterative frameworks, impeding the effective capturing of non-local information and, consequently, limiting improvements in model performance. In order to improve cardiac MRI reconstruction, we propose a novel temporal-spatial transformer with a strategy in this study. Based on the multi-level encoder and decoder transformer architecture, we conduct multi-level spatiotemporal information feature aggregation over several adjacent views, that create nonlinear dependencies among features and efficiently learn important information among adjacent cardiac temporal frames. Additionally, in order to improve contextual awareness between neighboring views, we add cross-view attention for temporal information fusion. Furthermore, we introduce an iterative strategy for training weights during the reconstruction process, which improves feature fusion in critical locations and reduces the number of computations required to calculate global feature dependencies. Extensive experiments have demonstrated the substantial superiority of this procedure over the most advanced techniques, suggesting that it has broad potential for clinical use.
C1 [Lyu, Jun; Wang, Guangming] Yantai Univ, Sch Comp & Control Engn, Yantai 264005, Peoples R China.
   [Hossain, M. Shamim] King Saud Univ, Coll Comp & Informat Sci, Dept Software Engn, Riyadh 12373, Saudi Arabia.
C3 Yantai University; King Saud University
RP Hossain, MS (corresponding author), King Saud Univ, Coll Comp & Informat Sci, Dept Software Engn, Riyadh 12373, Saudi Arabia.
EM ljdream0710@pku.edu.cn; 202200358049@s.ytu.edu.cn; mshossain@ksu.edu.sa
OI Hossain, M. Shamim/0000-0001-5906-9422
FU King Saud University, Riyadh, Saudi Arabia [RSP2024R32]; National
   Natural Science Foundation of China [62371413]; Yantai Basic Research
   Key Project [2023JCYJ041]; Youth Innovation Science and Technology
   Support Program of Shandong Provincial [2023KJ239]
FX This work was supported by the Researchers Supporting Project number
   (RSP2024R32), King Saud University, Riyadh, Saudi Arabia. This work was
   supported in part by the National Natural Science Foundation of China
   under Grant 62371413, in part by Yantai Basic Research Key Project
   2023JCYJ041, and in part by the Youth Innovation Science and Technology
   Support Program of Shandong Provincial under Grant 2023KJ239.
CR Aggarwal HK, 2019, IEEE T MED IMAGING, V38, P394, DOI 10.1109/TMI.2018.2865356
   Ahmed AH, 2020, IEEE T MED IMAGING, V39, P3933, DOI 10.1109/TMI.2020.3008329
   Alamri A, 2010, IEEE T INSTRUM MEAS, V59, P2554, DOI 10.1109/TIM.2010.2057750
   Amin SU, 2019, FUTURE GENER COMP SY, V101, P542, DOI 10.1016/j.future.2019.06.027
   Arnab A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6816, DOI 10.1109/ICCV48922.2021.00676
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Cong YR, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16352, DOI 10.1109/ICCV48922.2021.01606
   Du JL, 2020, NEUROCOMPUTING, V392, P209, DOI 10.1016/j.neucom.2018.10.102
   El Saddik A, 2007, IEEE INSTRU MEAS MAG, V10, P10, DOI 10.1109/MIM.2007.339540
   El Saddik A, 2018, IEEE MULTIMEDIA, V25, P87, DOI 10.1109/MMUL.2018.023121167
   Feng CM, 2021, LECT NOTES COMPUT SC, V12906, P307, DOI 10.1007/978-3-030-87231-1_30
   Guo R, 2022, J CARDIOVASC MAGN R, V24, DOI 10.1186/s12968-021-00834-0
   Guo XD, 2021, PROC CVPR IEEE, P12613, DOI 10.1109/CVPR46437.2021.01243
   Hara K, 2017, IEEE INT CONF COMP V, P3154, DOI 10.1109/ICCVW.2017.373
   Ho JAT, 2019, Arxiv, DOI arXiv:1912.12180
   Hossain MS, 2019, MULTIMEDIA SYST, V25, P565, DOI 10.1007/s00530-017-0561-x
   Huang QY, 2019, I S BIOMED IMAGING, P1622, DOI [10.1109/isbi.2019.8759423, 10.1109/ISBI.2019.8759423]
   Jung H, 2007, PHYS MED BIOL, V52, P3201, DOI 10.1088/0031-9155/52/11/018
   Li GY, 2022, PROC CVPR IEEE, P20604, DOI 10.1109/CVPR52688.2022.01998
   Liang JY, 2022, Arxiv, DOI [arXiv:2201.12288, DOI 10.48550/ARXIV.2201.12288]
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lin J, 2022, PR MACH LEARN RES
   Lingala SG, 2011, IEEE T MED IMAGING, V30, P1042, DOI 10.1109/TMI.2010.2100850
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lv J, 2021, COMPUT BIOL MED, V134, DOI 10.1016/j.compbiomed.2021.104504
   Lv J, 2021, DIAGNOSTICS, V11, DOI 10.3390/diagnostics11010061
   Lv J, 2018, BRIT J RADIOL, V91, DOI 10.1259/bjr.20170813
   Lv J, 2018, BRIT J RADIOL, V91, DOI 10.1259/bjr.20170788
   Lyu J, 2023, MED IMAGE ANAL, V85, DOI 10.1016/j.media.2023.102760
   Lyu J, 2022, LECT NOTES COMPUT SC, V13436, P474, DOI 10.1007/978-3-031-16446-0_45
   Machidon AlinaL., 2021, arXiv
   Murugesan B, 2019, LECT NOTES COMPUT SC, V11905, P3, DOI 10.1007/978-3-030-33843-5_1
   Otazo R, 2015, MAGN RESON MED, V73, P1125, DOI 10.1002/mrm.25240
   Piergiovanni AJ, 2023, PROC CVPR IEEE, P2214, DOI 10.1109/CVPR52729.2023.00220
   Qin C, 2019, IEEE T MED IMAGING, V38, P280, DOI 10.1109/TMI.2018.2863670
   Ramanarayanan S, 2020, I S BIOMED IMAGING, P1069, DOI [10.1109/isbi45749.2020.9098491, 10.1109/ISBI45749.2020.9098491]
   Schelbert EB, 2016, RADIOLOGY, V278, P658, DOI 10.1148/radiol.2016141802
   Schlemper J, 2017, LECT NOTES COMPUT SC, V10265, P647, DOI 10.1007/978-3-319-59050-9_51
   Simonyan K, 2014, ADV NEUR IN, V27
   Taylor AJ, 2016, JACC-CARDIOVASC IMAG, V9, P67, DOI 10.1016/j.jcmg.2015.11.005
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang CY, 2023, Arxiv, DOI arXiv:2309.10836
   Wang Guangming, 2024, Statistical Atlases and Computa- tional Models of the Heart. Regular and CMRxRecon Challenge Papers (STACOM'23),, V14507, DOI [10.1007/978-3-031-52448, DOI 10.1007/978-3-031-52448]
   Wang XQ, 2023, MAGN RESON MED, V89, P1368, DOI 10.1002/mrm.29521
   Wang YQ, 2021, PROC CVPR IEEE, P8737, DOI 10.1109/CVPR46437.2021.00863
   Xing ZH, 2022, LECT NOTES COMPUT SC, V13435, P140, DOI 10.1007/978-3-031-16443-9_14
   Yan S, 2022, PROC CVPR IEEE, P3323, DOI 10.1109/CVPR52688.2022.00333
   Yu WH, 2022, PROC CVPR IEEE, P10809, DOI 10.1109/CVPR52688.2022.01055
NR 48
TC 0
Z9 0
U1 2
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2024
VL 20
IS 6
SI SI
AR 179
DI 10.1145/3643640
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OQ2T3
UT WOS:001208681800029
DA 2024-08-05
ER

PT J
AU Hsu, WY
   Lin, HW
AF Hsu, Wei-Yen
   Lin, Hsien-Wen
TI Context-detail-aware United Network for Single Image Deraining
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Single image deraining; context awareness; detail awareness; united
   network
ID MODEL
AB Images captured outdoors are often affected by rainy days, resulting in a severe deterioration in the visual quality of the captured images and a decrease in the performance of related applications. Therefore, single image deraining has attracted attention as a challenging research topic. Nowadays, there are two common deraining architectures in single image deraining. The first one is to restore the rain-free image by deducting rain streaks learned by the model from the rain image, but the background structure is easily mistaken for rain streaks and subtracted. The other one is to directly learn the clean background structure through the model using rain images, but it is difficult to completely remove the rain streaks due to the complexity of the information in images with rain. Therefore, current methods cannot balance rain streak removal and rain-free image background restoration in a single architecture and achieve good results. To address this issue, we propose a novel framework, namely, Context-Detail-Aware United Network (CDaUNet), which combines the above two architectures in this study. More specifically, we divide the restoration of the background structure of rain-free images and the learning of rain streaks into two independent sub-networks. The proposed Structure-Aware Rain Removal Network (SaRRN) is to learn the background structure in images to reconstruct clean rain-free images, whereas Detail-Aware Rain Streak Learning Network (DaRLN) is proposed to learn the details of rain streaks in images. Finally, we fuse the results generated by the two sub-networks through our designed Dual Architecture Fusion Network (DAFN) to reconstruct original rain images to effectively fuse the results of the two sub-networks. The experimental results show that CDaUNet achieves satisfactory performance in comparison with the state-of-the-art approaches included in rain streak removal and rain-free image structure restoration architectures on both synthetic and real image datasets, confirming the effectiveness of our method.
C1 [Hsu, Wei-Yen; Lin, Hsien-Wen] Natl Chung Cheng Univ, Dept Informat Management, Chiayi, Taiwan.
   [Hsu, Wei-Yen; Lin, Hsien-Wen] Natl Chung Cheng Univ, Adv Inst Mfg Hightech Innovat, Chiayi, Taiwan.
   [Hsu, Wei-Yen; Lin, Hsien-Wen] Natl Chung Cheng Univ, Ctr Innovat Res Aging Soc CIRAS, Chiayi, Taiwan.
   [Hsu, Wei-Yen; Lin, Hsien-Wen] Natl Chung Cheng Univ, Dept Informat Management, 168 Univ Rd, Chiayi 62102, Taiwan.
   [Hsu, Wei-Yen; Lin, Hsien-Wen] Natl Chung Cheng Univ, Adv Inst Mfg Hightech Innovat, 168 Univ Rd, Chiayi 62102, Taiwan.
   [Hsu, Wei-Yen; Lin, Hsien-Wen] Natl Chung Cheng Univ, Ctr Innovat Res Aging Soc CIRAS, 168 Univ Rd, Chiayi 62102, Taiwan.
C3 National Chung Cheng University; National Chung Cheng University;
   National Chung Cheng University; National Chung Cheng University;
   National Chung Cheng University; National Chung Cheng University
RP Hsu, WY (corresponding author), Natl Chung Cheng Univ, Dept Informat Management, 168 Univ Rd, Chiayi 62102, Taiwan.; Hsu, WY (corresponding author), Natl Chung Cheng Univ, Adv Inst Mfg Hightech Innovat, 168 Univ Rd, Chiayi 62102, Taiwan.; Hsu, WY (corresponding author), Natl Chung Cheng Univ, Ctr Innovat Res Aging Soc CIRAS, 168 Univ Rd, Chiayi 62102, Taiwan.
EM shenswy@gmail.com; shenswy@mis.ccu.edu.tw
FU National Science and Technology Council, Taiwan
   [NSTC110-2221-E-194-027-MY3, NSTC111-2410-H-194-038-MY3]
FX We gratefully acknowledge the funding agency, National Science and
   Technology Council, Taiwan, under grant Nos. NSTC110-2221-E-194-027-MY3
   and NSTC111-2410-H-194-038-MY3.
CR Ba YH, 2022, LECT NOTES COMPUT SC, V13667, P723, DOI 10.1007/978-3-031-20071-7_42
   Cai LW, 2019, IEEE IMAGE PROC, P2756, DOI [10.1109/icip.2019.8803308, 10.1109/ICIP.2019.8803308]
   Chang Y, 2024, IEEE T NEUR NET LEAR, V35, P8414, DOI 10.1109/TNNLS.2022.3227730
   Chen X, 2022, PROC CVPR IEEE, P2007, DOI 10.1109/CVPR52688.2022.00206
   Deng LJ, 2018, APPL MATH MODEL, V59, P662, DOI 10.1016/j.apm.2018.03.001
   Eigen D, 2013, IEEE I CONF COMP VIS, P633, DOI 10.1109/ICCV.2013.84
   Fu XY, 2021, INT J COMPUT VISION, V129, P1691, DOI 10.1007/s11263-020-01428-6
   Fu XY, 2020, IEEE T NEUR NET LEAR, V31, P1794, DOI 10.1109/TNNLS.2019.2926481
   Fu XY, 2017, PROC CVPR IEEE, P1715, DOI 10.1109/CVPR.2017.186
   Fu XY, 2017, IEEE T IMAGE PROCESS, V26, P2944, DOI 10.1109/TIP.2017.2691802
   Gu SH, 2017, IEEE I CONF COMP VIS, P1717, DOI 10.1109/ICCV.2017.189
   Haoyu Ma, 2022, ACM Transactions on Multimedia Computing, Communications and Applications, V18, DOI 10.1145/3477553
   Hsu WY, 2023, IEEE T PATTERN ANAL, V45, P15979, DOI 10.1109/TPAMI.2023.3307666
   Hsu WY, 2023, IEEE T INTELL TRANSP, V24, P12312, DOI 10.1109/TITS.2023.3287574
   Hsu WY, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3592613
   Hsu WY, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3289958
   Hsu WY, 2023, IEEE T NEUR SYS REH, V31, P1659, DOI 10.1109/TNSRE.2023.3255233
   Hsu WY, 2023, OPT EXPRESS, V31, P3606, DOI 10.1364/OE.479370
   Hsu WY, 2023, PATTERN RECOGN, V137, DOI 10.1016/j.patcog.2022.109294
   Hsu WY, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3204081
   Hsu WY, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3142061
   Hsu WY, 2021, IEEE T IMAGE PROCESS, V30, P1369, DOI 10.1109/TIP.2020.3044209
   Hsu WY, 2021, IEEE T IMAGE PROCESS, V30, P934, DOI 10.1109/TIP.2020.3039574
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huynh-Thu Q, 2008, ELECTRON LETT, V44, P800, DOI 10.1049/el:20080522
   Jin X, 2020, PATTERN RECOGN, V100, DOI 10.1016/j.patcog.2019.107143
   Kingma D. P., 2014, arXiv
   Kui Jiang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8343, DOI 10.1109/CVPR42600.2020.00837
   Li C, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3587468
   Li RT, 2019, PROC CVPR IEEE, P1633, DOI 10.1109/CVPR.2019.00173
   Li X, 2018, LECT NOTES COMPUT SC, V11211, P262, DOI 10.1007/978-3-030-01234-2_16
   Li Y., 2023, IEEECVF C COMPUTER V, P4198
   Li YZ, 2022, IEEE WINT CONF APPL, P3957, DOI 10.1109/WACV51458.2022.00401
   Li Y, 2016, PROC CVPR IEEE, P2736, DOI 10.1109/CVPR.2016.299
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lin CY, 2020, IEEE T IMAGE PROCESS, V29, P9250, DOI 10.1109/TIP.2020.3025402
   Nanba Y, 2022, IEEE COMPUT SOC CONF, P567, DOI 10.1109/CVPRW56347.2022.00072
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Sun ZY, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3478457
   Wang Q, 2023, IEEE INT CON MULTI, P2747, DOI 10.1109/ICME55011.2023.00467
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZD, 2022, PROC CVPR IEEE, P17662, DOI 10.1109/CVPR52688.2022.01716
   Wei Y., 2021, IEEE INT C MULTIMEDI, P1
   Wei YY, 2021, IEEE T IMAGE PROCESS, V30, P4788, DOI 10.1109/TIP.2021.3074804
   Wu W, 2023, IEEE T CIRC SYST VID, V33, P4675, DOI 10.1109/TCSVT.2023.3246953
   Yamamichi K, 2021, IEEE ACCESS, V9, P146948, DOI 10.1109/ACCESS.2021.3122450
   Yang WH, 2019, IEEE T IMAGE PROCESS, V28, P2948, DOI 10.1109/TIP.2019.2892685
   Yang WH, 2017, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2017.183
   Yasarla R, 2019, PROC CVPR IEEE, P8397, DOI 10.1109/CVPR.2019.00860
   Yuan Y, 2018, IEEE COMPUT SOC CONF, P814, DOI 10.1109/CVPRW.2018.00113
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zhang H, 2018, PROC CVPR IEEE, P695, DOI 10.1109/CVPR.2018.00079
   Zhou M, 2021, PROC CVPR IEEE, P4905, DOI 10.1109/CVPR46437.2021.00487
NR 53
TC 1
Z9 1
U1 7
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2024
VL 20
IS 5
AR 144
DI 10.1145/3639407
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MF3R9
UT WOS:001192177900024
DA 2024-08-05
ER

PT J
AU Jia, ZY
   Lu, Y
   Li, HQ
AF Jia, Zhaoyang
   Lu, Yan
   Li, Houqiang
TI Exploring Neighbor Correspondence Matching for Multiple-hypotheses Video
   Frame Synthesis
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Video frame synthesis; correspondence matching
AB Video frame synthesis, which consists of interpolation and extrapolation, is an essential video processing technique that can be applied to various scenarios. However, most existing methods cannot handle small objects or large motion well, especially in high-resolution videos such as 4K videos. To eliminate such limitations, we introduce a neighbor correspondence matching (NCM) algorithm for flow-based frame synthesis. Since the current frame is not available in video frame synthesis, NCM is performed in a current-frame-agnostic fashion to establish multi-scale correspondences in the spatial-temporal neighborhoods of each pixel. Based on the powerful motion representation capability of NCM, we propose a heterogeneous coarse-to-fine scheme for intermediate flow estimation. The coarse-scale and fine-scale modules are trained progressively, making NCM computationally efficient and robust to large motions. We further explore the mechanism of NCM and find that neighbor correspondence is powerful, since it provides multiple-hypotheses motion information for synthesis. Based on this analysis, we introduce a multiple-hypotheses estimation process for video frame extrapolation, resulting in a more robust framework, NCM-MH. Experimental results show that NCM and NCM-MHachieve 31.63 and 28.08 dB for interpolation and extrapolation on the most challenging X4K1000FPS benchmark, outperforming all the other state-of-the-art methods that use two reference frames as input.
C1 [Jia, Zhaoyang; Li, Houqiang] Univ Sci & Technol China, 443 Huangshan Rd, Hefei 230027, Anhui, Peoples R China.
   [Lu, Yan] Microsoft Res Asia, 5 Dan Ling St, Beijing 100080, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Microsoft; Microsoft Research Asia
RP Li, HQ (corresponding author), Univ Sci & Technol China, 443 Huangshan Rd, Hefei 230027, Anhui, Peoples R China.
EM jzy_ustc@mail.ustc.edu.cn; yanlu@microsoft.com; lihq@ustc.edu.cn
CR Bao WB, 2019, PROC CVPR IEEE, P3698, DOI 10.1109/CVPR.2019.00382
   Bao WB, 2021, IEEE T PATTERN ANAL, V43, P933, DOI 10.1109/TPAMI.2019.2941941
   Brooks T, 2019, PROC CVPR IEEE, P6833, DOI 10.1109/CVPR.2019.00700
   Cheng Ho Kei, 2021, Advances in Neural Information Processing Systems, V34, P11781
   Choi M, 2020, AAAI CONF ARTIF INTE, V34, P10663
   Flynn J, 2016, PROC CVPR IEEE, P5515, DOI 10.1109/CVPR.2016.595
   Hartley R, 2003, Multiple view geometry in computer vision, DOI [10.1016/S0143-8166(01)00145-2, DOI 10.1017/CBO9780511811685]
   Hu XT, 2023, PROC CVPR IEEE, P6121, DOI 10.1109/CVPR52729.2023.00593
   Huang ZW, 2022, LECT NOTES COMPUT SC, V13674, P624, DOI 10.1007/978-3-031-19781-9_36
   Jia Zhaoyang, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P5389, DOI 10.1145/3503161.3548163
   Jiang HZ, 2018, PROC CVPR IEEE, P9000, DOI 10.1109/CVPR.2018.00938
   Jiang SA, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9752, DOI 10.1109/ICCV48922.2021.00963
   Junheum Park, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P109, DOI 10.1007/978-3-030-58568-6_7
   Kalantari NK, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980251
   Lee H, 2020, PROC CVPR IEEE, P5315, DOI 10.1109/CVPR42600.2020.00536
   Li Z, 2023, PROC CVPR IEEE, P9801, DOI 10.1109/CVPR52729.2023.00945
   Liu JY, 2020, IEEE T MULTIMEDIA, V22, P2497, DOI 10.1109/TMM.2019.2961504
   Liu ZW, 2017, IEEE I CONF COMP VIS, P4473, DOI [10.1109/ICCV.2017.478, 10.1109/ICCVW.2017.361]
   Loshchilov I., 2017, ARXIV
   Lu G, 2019, PROC CVPR IEEE, P10998, DOI 10.1109/CVPR.2019.01126
   Lu LY, 2022, PROC CVPR IEEE, P3522, DOI 10.1109/CVPR52688.2022.00352
   Niklaus S, 2020, PROC CVPR IEEE, P5436, DOI 10.1109/CVPR42600.2020.00548
   Niklaus S, 2018, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2018.00183
   Niklaus S, 2017, IEEE I CONF COMP VIS, P261, DOI 10.1109/ICCV.2017.37
   Park Junheum, 2021, P IEEECVF INT C COMP, P14539
   Pourreza R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6660, DOI 10.1109/ICCV48922.2021.00661
   Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291
   Schönberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445
   Sheng XH, 2023, IEEE T MULTIMEDIA, V25, P7311, DOI 10.1109/TMM.2022.3220421
   Shi ZH, 2022, IEEE T MULTIMEDIA, V24, P426, DOI 10.1109/TMM.2021.3052419
   Sim H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14469, DOI 10.1109/ICCV48922.2021.01422
   Soomro K, 2012, Arxiv, DOI arXiv:1212.0402
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931
   Tak-Wai Hui, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P169, DOI 10.1007/978-3-030-58565-5_11
   Teed Zachary, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P402, DOI 10.1007/978-3-030-58536-5_24
   Wu Y, 2022, PROC CVPR IEEE, P17793, DOI 10.1109/CVPR52688.2022.01729
   Xiao Jing, 2022, IEEE Transactions on Multimedia, V2022, P1
   Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2
   Yang GS, 2019, ADV NEUR IN, V32
   Yang ZX, 2022, IEEE T PATTERN ANAL, V44, P4701, DOI 10.1109/TPAMI.2021.3081597
   Yihao Liu, 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12538), P41, DOI 10.1007/978-3-030-66823-5_3
   Zhang GZ, 2023, PROC CVPR IEEE, P5682, DOI 10.1109/CVPR52729.2023.00550
   Zhao SY, 2022, PROC CVPR IEEE, P17571, DOI 10.1109/CVPR52688.2022.01707
NR 44
TC 0
Z9 0
U1 6
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2024
VL 20
IS 4
AR 111
DI 10.1145/3633780
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6G9
UT WOS:001153382100021
DA 2024-08-05
ER

PT J
AU Fang, J
   Yu, YB
   Wang, ZY
   Ding, X
   Hu, RM
AF Fang, Jing
   Yu, Yinbo
   Wang, Zhongyuan
   Ding, Xin
   Hu, Ruimin
TI An Image Arbitrary-Scale Super-Resolution Network Using Frequency-domain
   Information
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Super-resolution; image frequency domain; arbitrary magnification; deep
   reinforcement learning
AB Image super-resolution (SR) is a technique to recover lost high-frequency information in low-resolution (LR) images. Since spatial-domain information has been widely exploited, there is a new trend to involve frequency-domain information in SR tasks. Besides, image SR is typically application-oriented and various computer vision tasks call for image arbitrary magnification. Therefore, in this article, we study image features in the frequency domain to design a novel image arbitrary-scale SR network. First, we statistically analyze LR-HR image pairs of several datasets under different scale factors and find that the high-frequency spectra of different images under different scale factors suffer from different degrees of degradation, but the valid low-frequency spectra tend to be retained within a certain distribution range. Then, based on this finding, we devise an adaptive scale-aware feature division mechanism using deep reinforcement learning, which can accurately and adaptively divide the frequency spectrum into the low-frequency part to be retained and the high-frequency one to be recovered. Finally, we design a scale-aware feature recovery module to capture and fuse multi-level features for reconstructing the high-frequency spectrum at arbitrary scale factors. Extensive experiments on public datasets show the superiority of our method compared with state-of-the-art methods.
C1 [Fang, Jing; Wang, Zhongyuan; Hu, Ruimin] Wuhan Univ, Sch Comp Sci, Wuhan 430072, Hubei, Peoples R China.
   [Yu, Yinbo] Northwestern Polytech Univ, Sch Cybersecur, Xian 710072, Shaanxi, Peoples R China.
   [Ding, Xin] NingboTech Univ, Sch Comp & Data Sci, Ningbo 315199, Zhejiang, Peoples R China.
C3 Wuhan University; Northwestern Polytechnical University; NingboTech
   University
RP Hu, RM (corresponding author), Wuhan Univ, Sch Comp Sci, Wuhan 430072, Hubei, Peoples R China.
EM jingfang@whu.edu.cn; yinboyu@nwpu.edu.cn; wzy_hope@163.com;
   XDing07@163.com; hrm@whu.edu.cn
OI Wang, Zhongyuan/0000-0002-9796-488X; Hu, Ruimin/0000-0002-5872-3872
FU National Natural Science Foundation of China (NSFC) [91738302, 62202387,
   62071339]
FX The research was supported in part by the National Natural Science
   Foundation of China (NSFC) under Grant 91738302, Grant 62202387, Grant
   62071339.
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Behjati P, 2021, IEEE WINT CONF APPL, P2693, DOI 10.1109/WACV48630.2021.00274
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Cai R., 2021, arXiv
   Chen R, 2022, IEEE T CIRC SYST VID, V32, P8368, DOI 10.1109/TCSVT.2022.3192099
   Chen YB, 2021, PROC CVPR IEEE, P8624, DOI 10.1109/CVPR46437.2021.00852
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Esmaeilzehi A, 2021, IEEE T COMPUT IMAG, V7, P409, DOI 10.1109/TCI.2021.3070522
   Fang J, 2022, CHINA COMMUN, V19, P234, DOI 10.23919/JCC.2022.08.017
   Fuoli D, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2340, DOI 10.1109/ICCV48922.2021.00236
   Guo TT, 2019, IEEE T IMAGE PROCESS, V28, P4685, DOI 10.1109/TIP.2019.2913500
   Guo TT, 2017, IEEE COMPUT SOC CONF, P1100, DOI 10.1109/CVPRW.2017.148
   Haoyu Ma, 2022, ACM Transactions on Multimedia Computing, Communications and Applications, V18, DOI 10.1145/3477553
   Haris M., 2021, TASK DRIVEN SUPER RE, P387, DOI 10.1007/978-3-030-92307-5_45
   Hong C., 2022, P IEEECVF WINTER C A, P2675
   Hu XC, 2019, PROC CVPR IEEE, P1575, DOI 10.1109/CVPR.2019.00167
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Khayam S. A., 2003, Michigan State Univ., V114, P31
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Lee J, 2022, PROC CVPR IEEE, P1928, DOI 10.1109/CVPR52688.2022.00197
   Lepcha DC, 2023, INFORM FUSION, V91, P230, DOI 10.1016/j.inffus.2022.10.007
   Li Junxuan, 2018, P 2018 INT JOINT C N, P1
   Li YC, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3414838
   Liang J, 2022, PROC CVPR IEEE, P5647, DOI 10.1109/CVPR52688.2022.00557
   Liu J, 2020, INT CONF ACOUST SPEE, P2033, DOI [10.1109/icassp40776.2020.9053245, 10.1109/ICASSP40776.2020.9053245]
   Liu LX, 2014, SIGNAL PROCESS-IMAGE, V29, P856, DOI 10.1016/j.image.2014.06.006
   Liu PJ, 2018, IEEE COMPUT SOC CONF, P886, DOI 10.1109/CVPRW.2018.00121
   Liu YQ, 2022, IEEE T CIRC SYST VID, V32, P4927, DOI 10.1109/TCSVT.2021.3138431
   Liu Yuqing, 2022, ACM Transactions onMultimedia Computing, Communications, and Applications
   Ma SW, 2020, IEEE T CIRC SYST VID, V30, P1683, DOI 10.1109/TCSVT.2019.2910119
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Mei YQ, 2021, PROC CVPR IEEE, P3516, DOI 10.1109/CVPR46437.2021.00352
   Mishra S, 2019, PROCEEDINGS OF MVA 2019 16TH INTERNATIONAL CONFERENCE ON MACHINE VISION APPLICATIONS (MVA), DOI 10.23919/mva.2019.8757890
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mnih V, 2016, PR MACH LEARN RES, V48
   Pan ZH, 2022, PROC CVPR IEEE, P17368, DOI 10.1109/CVPR52688.2022.01687
   Schulter S, 2015, PROC CVPR IEEE, P3791, DOI 10.1109/CVPR.2015.7299003
   Son S, 2021, PROC CVPR IEEE, P7778, DOI 10.1109/CVPR46437.2021.00769
   Sun L, 2020, IEEE T CIRC SYST VID, V30, P3829, DOI 10.1109/TCSVT.2019.2946723
   Wang LG, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4781, DOI 10.1109/ICCV48922.2021.00476
   Wang YF, 2019, IEEE T CIRC SYST VID, V29, P1259, DOI 10.1109/TCSVT.2018.2839879
   Wu HP, 2021, IEEE T CIRC SYST VID, V31, P512, DOI 10.1109/TCSVT.2020.2988895
   Xie J, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3120891
   Xin JW, 2022, IEEE T NEUR NET LEAR, V33, P707, DOI 10.1109/TNNLS.2020.3028688
   Xu RY, 2022, DISPLAYS, V74, DOI 10.1016/j.displa.2022.102220
   Xue SK, 2020, SIGNAL IMAGE VIDEO P, V14, P257, DOI 10.1007/s11760-019-01548-8
   Yang Bin-Cheng, 2022, ACM Transactions on Multimedia Computing, Communications and Applications
   Yang JY, 2022, IEEE COMPUT SOC CONF, P1785, DOI 10.1109/CVPRW56347.2022.00192
   Yang WM, 2019, IEEE T MULTIMEDIA, V21, P3106, DOI 10.1109/TMM.2019.2919431
   Yang X, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3578934
   You SR, 2023, IEEE T NEUR NET LEAR, V34, P8802, DOI 10.1109/TNNLS.2022.3153088
   Yu JH, 2018, Arxiv, DOI arXiv:1808.08718
   Yun JS, 2022, MATHEMATICS-BASEL, V10, DOI 10.3390/math10020275
   Zeng K, 2022, P IEEE CVF C COMP VI, P1103
   Zhang DY, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3398685
   Zhang Y., 2020, P 2020 IEEE 15 INT C, P1
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhao ZX, 2022, PROC CVPR IEEE, P5687, DOI 10.1109/CVPR52688.2022.00561
   Zhou YJ, 2021, INT J PROD RES, V59, P6564, DOI 10.1080/00207543.2020.1821117
   Zhu J, 2021, INT J NEURAL SYST, V31, DOI 10.1142/S0129065721500374
   Zou FH, 2020, NEURAL COMPUT APPL, V32, P14549, DOI 10.1007/s00521-020-04893-9
NR 61
TC 0
Z9 0
U1 10
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2024
VL 20
IS 3
AR 81
DI 10.1145/3616376
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6F8
UT WOS:001153381000021
DA 2024-08-05
ER

PT J
AU Luo, X
   Ju, W
   Gu, YY
   Qin, YF
   Yi, SY
   Wu, DQ
   Liu, LC
   Zhang, M
AF Luo, Xiao
   Ju, Wei
   Gu, Yiyang
   Qin, Yifang
   Yi, Siyu
   Wu, Daqing
   Liu, Luchen
   Zhang, Ming
TI Toward Effective Semi-supervised Node Classification with Hybrid
   Curriculum Pseudo-labeling
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Graph neural network; semi-supervised learning; curriculum learning
ID GRAPH CONVOLUTIONAL NETWORKS; NEURAL-NETWORKS; PROPAGATION
AB Semi-supervised node classification is a crucial challenge in relational data mining and has attracted increasing interest in research on graph neural networks (GNNs). However, previous approaches merely utilize labeled nodes to supervise the overall optimization, but fail to sufficiently explore the information of their underlying label distribution. Even worse, they often overlook the robustness of models, which may cause instability of network outputs to random perturbations. To address the aforementioned shortcomings, we develop a novel framework termed Hybrid Curriculum Pseudo-Labeling (HCPL) for efficient semi-supervised node classification. Technically, HCPL iteratively annotates unlabeled nodes by training a GNN model on the labeled samples and any previously pseudo-labeled samples, and repeatedly conducts this process. To improve the model robustness, we introduce a hybrid pseudo-labeling strategy that incorporates both prediction confidence and uncertainty under random perturbations, therefore mitigating the influence of erroneous pseudo-labels. Finally, we leverage the idea of curriculum learning to start from annotating easy samples, and gradually explore hard samples as the iteration grows. Extensive experiments on a number of benchmarks demonstrate that our HCPL beats various state-of-the-art baselines in diverse settings.
C1 [Luo, Xiao] Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90095 USA.
   [Ju, Wei; Gu, Yiyang; Qin, Yifang; Liu, Luchen; Zhang, Ming] Peking Univ, Sch Comp Sci, Beijing 100871, Peoples R China.
   [Yi, Siyu] Nankai Univ, Sch Stat & Data Sci, Tianjin 300071, Peoples R China.
   [Wu, Daqing] Peking Univ, Sch Math Sci, Beijing 100871, Peoples R China.
C3 University of California System; University of California Los Angeles;
   Peking University; Nankai University; Peking University
RP Ju, W; Zhang, M (corresponding author), Peking Univ, Sch Comp Sci, Beijing 100871, Peoples R China.
EM xiaoluo@cs.ucla.edu; juwei@pku.edu.cn; yiyanggu@pku.edu.cn;
   qinyifang@pku.edu.cn; siyuyi@mail.nankai.edu.cn; wudq@pku.edu.cn;
   liuluchen@pku.edu.cn; mzhang_cs@pku.edu.cn
RI Qin, Yifang/JMC-3669-2023; Luo, Xiao/IYS-9183-2023; Zhang,
   Ming/L-8193-2019
OI Qin, Yifang/0000-0002-7520-8039; Luo, Xiao/0000-0002-7987-3714; Ju,
   Wei/0000-0001-9657-951X; Gu, Yiyang/0000-0002-5915-4448; Zhang,
   Ming/0000-0002-9809-3430
FU National Natural Science Foundation of China under NSFC [62106008,
   62276002, 62306014]; China Postdoctoral Science Foundation [2023M730057]
FX This article is partially supported by the National Natural Science
   Foundation of China under NSFC Grants No. 62106008, No. 62276002, and
   62306014, as well as the China Postdoctoral Science Foundation under
   Grant No. 2023M730057.
CR Assran M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8423, DOI 10.1109/ICCV48922.2021.00833
   Bahdanau D, 2016, Arxiv, DOI arXiv:1409.0473
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Bengio J., 2009, Proceedings of the 26th Annual International Conference on Machine Learning, P41
   Berthelot D, 2019, ADV NEUR IN, V32
   Bojchevski A, 2018, Arxiv, DOI arXiv:1707.03815
   Cai L, 2022, IEEE T PATTERN ANAL, V44, P5103, DOI 10.1109/TPAMI.2021.3080635
   Cascante-Bonilla P., AAAI C ART INT, V2021, P1
   Chen XK, 2021, PROC CVPR IEEE, P2613, DOI 10.1109/CVPR46437.2021.00264
   Defferrard M, 2016, ADV NEUR IN, V29
   Donahue J, 2014, PR MACH LEARN RES, V32
   Dong Xiao, 2017, P WORKSHOP VISUAL AN, P25
   dos Santos FP, 2020, NEURAL NETWORKS, V132, P131, DOI 10.1016/j.neunet.2020.08.016
   Fu SC, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3412846
   Gong C, 2019, ACM T INTEL SYST TEC, V10, DOI 10.1145/3322122
   Gong C, 2016, AAAI CONF ARTIF INTE, P1610
   Gong C, 2017, IEEE T NEUR NET LEAR, V28, P1452, DOI 10.1109/TNNLS.2016.2514360
   Gong C, 2016, IEEE T IMAGE PROCESS, V25, P3249, DOI 10.1109/TIP.2016.2563981
   Gong C, 2015, PROC CVPR IEEE, P2531, DOI 10.1109/CVPR.2015.7298868
   Grandvalet Y., 2005, Advances in Neural Information Processing Systems, V367, P281
   Hamilton WL, 2017, ADV NEUR IN, V30
   Hassani K, 2020, PR MACH LEARN RES, V119
   Huang FH, 2022, INFORM SCIENCES, V594, P286, DOI 10.1016/j.ins.2022.02.031
   Jin YQ, 2022, AAAI CONF ARTIF INTE, P5746
   Ju W, 2023, AAAI CONF ARTIF INTE, P4391
   Ju W, 2023, Arxiv, DOI arXiv:2304.11688
   Ju W, 2024, Arxiv, DOI arXiv:2304.05055
   Kingma D. P., 2014, arXiv
   Kipf T.N., 2017, INT C LEARN REPR, P1
   Lee D.H., 2013, WORKSH CHALL REPR LE, P07
   Li HF, 2022, INFORM SCIENCES, V592, P50, DOI 10.1016/j.ins.2021.12.077
   Li Xiang, 2022, P MACHINE LEARNING R
   Liu M, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P338, DOI 10.1145/3394486.3403076
   Luan ST, 2021, Arxiv, DOI arXiv:2109.05641
   Luo Xiao, 2023, IEEE Transactions on Knowledge and Data Engineering
   Mei Qiaozhu, 2008, SIGIR
   Peng H, 2020, INFORM SCIENCES, V521, P277, DOI 10.1016/j.ins.2020.01.043
   Peng Z, 2020, WEB CONFERENCE 2020: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2020), P259, DOI 10.1145/3366423.3380112
   Qian SS, 2021, AAAI CONF ARTIF INTE, V35, P2440
   Qin Yifang, 2023, ACM Transactions on Information Systems
   Qing YY, 2021, NEURAL NETWORKS, V143, P303, DOI 10.1016/j.neunet.2021.06.007
   Ran HY, 2022, INFORM SCIENCES, V592, P402, DOI 10.1016/j.ins.2022.01.036
   Rizve M, 2021, Arxiv, DOI [arXiv:2101.06329, 10.48550/arXiv.2101.06329]
   Romero A., 2018, INT C LEARN REPR, P2920, DOI DOI 10.48550/ARXIV.1710.10903
   Sen P, 2008, AI MAG, V29, P93, DOI 10.1609/aimag.v29i3.2157
   Shchur O, 2019, Arxiv, DOI arXiv:1811.05868
   Shervashidze N, 2011, J MACH LEARN RES, V12, P2539
   Shi Feng, 2021, arXiv
   Shi ZX, 2008, PROC SPIE, V6944, DOI 10.1117/12.778687
   Sohn K., 2020, NEURIPS
   Tu EM, 2022, NEURAL NETWORKS, V146, P350, DOI 10.1016/j.neunet.2021.11.026
   van Engelen JE, 2020, MACH LEARN, V109, P373, DOI 10.1007/s10994-019-05855-6
   Velickovic Petar, 2019, INT C LEARN REPR
   Wan S, 2021, AAAI CONF ARTIF INTE, V35, P10049
   Wang F, 2021, KNOWL-BASED SYST, V225, DOI 10.1016/j.knosys.2021.107130
   Wang J, 2021, INFORM SCIENCES, V573, P171, DOI 10.1016/j.ins.2021.05.057
   Wang X, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1243, DOI 10.1145/3394486.3403177
   Wang Yiyu, 2022, AAAI
   Wu F, 2022, INFORM SCIENCES, V591, P142, DOI 10.1016/j.ins.2022.01.013
   Wu F, 2019, PR MACH LEARN RES, V97
   Wu M, 2022, IEEE TETCI, V6, P1079, DOI 10.1109/TETCI.2022.3156044
   Wu M, 2021, ACM T KNOWL DISCOV D, V15, DOI 10.1145/3450316
   Wu M, 2020, IEEE DATA MINING, P681, DOI 10.1109/ICDM50108.2020.00077
   Wu ZH, 2021, IEEE T NEUR NET LEAR, V32, P4, DOI 10.1109/TNNLS.2020.2978386
   Xiao Z, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0238915
   Xu K, 2019, PROC INT CONF PARAL, DOI 10.1145/3337821.3337923
   Yang LW, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P2141, DOI 10.1145/3404835.3463028
   Yang RC, 2022, PROCEEDINGS OF THE 28TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, KDD 2022, P2253, DOI 10.1145/3534678.3539277
   Yao KX, 2022, ARTIF INTELL-AMST, V307, DOI 10.1016/j.artint.2022.103708
   Yi SY, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3314451
   You Yuning, 34 C NEURAL INFORM P
   Yu E, 2019, IEEE T MULTIMEDIA, V21, P1276, DOI 10.1109/TMM.2018.2877127
   Zhan ZH, 2021, INT CONF MACH LEARN, P15, DOI [10.1145/3450569.3463560, 10.1109/ICMLC54886.2021.9737245, 10.1145/3502434.3502477]
   Zhang JR, 2022, INFORM SCIENCES, V593, P201, DOI 10.1016/j.ins.2022.01.076
   Zhang YH, 2020, PROC CVPR IEEE, P250, DOI 10.1109/CVPR42600.2020.00033
   Zhao MB, 2015, KNOWL-BASED SYST, V76, P148, DOI 10.1016/j.knosys.2014.12.014
   Zhou DY, 2004, ADV NEUR IN, V16, P321
   Zhu Yanqiao, 2021, ICLR WORKSHOP
   Zou HD, 2021, INFORM SCIENCES, V562, P385, DOI 10.1016/j.ins.2021.03.044
NR 79
TC 3
Z9 3
U1 4
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2024
VL 20
IS 3
AR 82
DI 10.1145/3626528
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6F8
UT WOS:001153381000022
OA hybrid
DA 2024-08-05
ER

PT J
AU Peng, YJ
   Wu, JL
   Xu, BQ
   Cao, CS
   Liu, X
   Sun, ZN
   He, ZQ
AF Peng, Yunjie
   Wu, Jinlin
   Xu, Boqiang
   Cao, Chunshui
   Liu, Xu
   Sun, Zhenan
   He, Zhiqiang
TI Deep Learning Based Occluded Person Re-Identification: A Survey
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Occluded person re-identification; partial person re-identification;
   literature survey; deep learning
ID NETWORK
AB Occluded person re-identification (Re-ID) focuses on addressing the occlusion problem when retrieving the person of interest across non-overlapping cameras. With the increasing demand for intelligent video surveillance and the application of person Re-ID technology, the real-world occlusion problem draws considerable interest from researchers. Although a large number of occluded person Re-ID methods have been proposed, there are few surveys that focus on occlusion. To fill this gap and help boost future research, this article provides a systematic survey of occluded person Re-ID. In this work, we review recent deep learning based occluded person Re-ID research. First, we summarize the main issues caused by occlusion as four groups: position misalignment, scale misalignment, noisy information, and missing information. Second, we categorize existing methods into six solution groups: matching, image transformation, multi-scale features, attention mechanism, auxiliary information, and contextual recovery. We also discuss the characteristics of each approach, as well as the issues they address. Furthermore, we present the performance comparison of recent occluded person Re-ID methods on four public datasets: Partial-ReID, Partial-iLIDS, Occluded-ReID, and Occluded-DukeMTMC. We conclude the study with thoughts on promising future research directions.
C1 [Peng, Yunjie; He, Zhiqiang] Beihang Univ, Sch Comp Sci & Technol, 37 XueYuan Rd, Beijing 100191, Peoples R China.
   [Wu, Jinlin; Xu, Boqiang] Chinese Acad Sci, Inst Automat, 95 ZhongGuanCun East Rd, Beijing 100190, Peoples R China.
   [Cao, Chunshui; Liu, Xu] Watrix Technol Ltd Co Ltd, 51 XueYuan Rd, Beijing 100191, Peoples R China.
   [Sun, Zhenan] Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Ctr Res Intelligent Percept & Comp, 95 ZhongGuanCun East Rd, Beijing 100190, Peoples R China.
C3 Beihang University; Chinese Academy of Sciences; Institute of
   Automation, CAS; Chinese Academy of Sciences; Institute of Automation,
   CAS
RP Peng, YJ (corresponding author), Beihang Univ, Sch Comp Sci & Technol, 37 XueYuan Rd, Beijing 100191, Peoples R China.
EM yunjiepeng@buaa.edu.cn; jinlin.wu@nlpr.ia.ac.cn;
   boqiang.xu@cripac.ia.ac.cn; chunshui.cao@watrix.ai; xu.liu@watrix.ai;
   znsun@nlpr.ia.ac.cn; zqhe1963@gmail.com
RI Wu, Jinlin/ABH-1288-2021
OI Wu, Jinlin/0000-0001-6805-1822; Peng, Yunjie/0000-0002-9275-0356; Liu,
   Xu/0000-0002-0401-1343; He, Zhiqiang/0000-0003-3103-1902
CR Ahmed E, 2015, PROC CVPR IEEE, P3908, DOI 10.1109/CVPR.2015.7299016
   Ainam JP, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3377352
   Bedagkar-Gala A, 2014, IMAGE VISION COMPUT, V32, P270, DOI 10.1016/j.imavis.2014.02.001
   Cai HL, 2019, IEEE COMPUT SOC CONF, P1555, DOI 10.1109/CVPRW.2019.00197
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chen PX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11813, DOI 10.1109/ICCV48922.2021.01162
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Ess A, 2008, PROC CVPR IEEE, P1857
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Gao LS, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3771, DOI 10.1145/3394171.3413833
   Gong K, 2017, PROC CVPR IEEE, P6757, DOI 10.1109/CVPR.2017.715
   Güler RA, 2018, PROC CVPR IEEE, P7297, DOI 10.1109/CVPR.2018.00762
   Han CC, 2020, IEEE IMAGE PROC, P226, DOI 10.1109/ICIP40778.2020.9191196
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He LX, 2019, IEEE I CONF COMP VIS, P8449, DOI 10.1109/ICCV.2019.00854
   He LX, 2018, PROC CVPR IEEE, P7073, DOI 10.1109/CVPR.2018.00739
   He ST, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14993, DOI 10.1109/ICCV48922.2021.01474
   He TY, 2021, PROC CVPR IEEE, P9101, DOI 10.1109/CVPR46437.2021.00899
   He YH, 2021, IEEE IMAGE PROC, P2373, DOI 10.1109/ICIP42928.2021.9506293
   Hou RB, 2022, IEEE T PATTERN ANAL, V44, P4894, DOI 10.1109/TPAMI.2021.3079910
   Hou RB, 2019, PROC CVPR IEEE, P7176, DOI 10.1109/CVPR.2019.00735
   Huang HJ, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102789
   Huang MY, 2023, IEEE T IMAGE PROCESS, V32, P1568, DOI 10.1109/TIP.2023.3247159
   Huo LJ, 2021, INT C PATT RECOG, P3652, DOI 10.1109/ICPR48806.2021.9412527
   Islam K, 2020, IMAGE VISION COMPUT, V101, DOI 10.1016/j.imavis.2020.103970
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jia MX, 2023, IEEE T MULTIMEDIA, V25, P1294, DOI 10.1109/TMM.2022.3141267
   Jia MX, 2021, AAAI CONF ARTIF INTE, V35, P1673
   Jiang JW, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2635
   Jiao Licheng, 2023, IEEE Transactions on Artificial Intelligence, P2, DOI 10.1109/TAI.2022.3194869
   Jin HY, 2022, IEEE T CIRC SYST VID, V32, P2170, DOI 10.1109/TCSVT.2021.3088446
   Kalayeh MM, 2018, PROC CVPR IEEE, P1062, DOI 10.1109/CVPR.2018.00117
   Karanam S, 2019, IEEE T PATTERN ANAL, V41, P523, DOI 10.1109/TPAMI.2018.2807450
   Kim J, 2017, IEEE IMAGE PROC, P3425, DOI 10.1109/ICIP.2017.8296918
   Kim M, 2022, INT CONF ACOUST SPEE, P2719, DOI 10.1109/ICASSP43922.2022.9746734
   Kreiss S, 2019, PROC CVPR IEEE, P11969, DOI 10.1109/CVPR.2019.01225
   Kuan Zhu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P346, DOI 10.1007/978-3-030-58580-8_21
   Lavi B, 2020, Arxiv, DOI [arXiv:2005.00355, 10.48550/arXiv.2005.00355]
   Lavi B, 2018, Arxiv, DOI arXiv:1807.05284
   Leng QM, 2020, IEEE T CIRC SYST VID, V30, P1092, DOI 10.1109/TCSVT.2019.2898940
   Li S, 2018, PROC CVPR IEEE, P369, DOI 10.1109/CVPR.2018.00046
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Li X, 2016, IEEE T IMAGE PROCESS, V25, P3919, DOI 10.1109/TIP.2016.2579306
   Li YL, 2021, PROC CVPR IEEE, P2897, DOI 10.1109/CVPR46437.2021.00292
   Liang XD, 2019, IEEE T PATTERN ANAL, V41, P871, DOI 10.1109/TPAMI.2018.2820063
   Lin CS, 2021, IEEE IMAGE PROC, P2299, DOI 10.1109/ICIP42928.2021.9506470
   Lin P., 2021, arXiv
   Lin X., 2021, P 30 INT JOINT C ART, P4500, DOI [10.24963/IJCAI.2021/613, DOI 10.24963/IJCAI.2021/613]
   Lingxiao He, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P357, DOI 10.1007/978-3-030-58604-1_22
   Liu JW, 2021, PROC CVPR IEEE, P4368, DOI 10.1109/CVPR46437.2021.00435
   Liu YH, 2019, AAAI CONF ARTIF INTE, P8786
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2905, DOI 10.1109/TMM.2020.2965491
   Mazzon R, 2012, PATTERN RECOGN LETT, V33, P1828, DOI 10.1016/j.patrec.2012.02.014
   Miao JX, 2022, IEEE T NEUR NET LEAR, V33, P4624, DOI 10.1109/TNNLS.2021.3059515
   Miao JX, 2019, IEEE I CONF COMP VIS, P542, DOI 10.1109/ICCV.2019.00063
   Ming ZQ, 2022, IMAGE VISION COMPUT, V119, DOI 10.1016/j.imavis.2022.104394
   Minghao Yin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P191, DOI 10.1007/978-3-030-58555-6_12
   Pan XG, 2018, LECT NOTES COMPUT SC, V11208, P484, DOI 10.1007/978-3-030-01225-0_29
   Quispe R, 2019, IMAGE VISION COMPUT, V92, DOI 10.1016/j.imavis.2019.07.009
   Shang Gao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11741, DOI 10.1109/CVPR42600.2020.01176
   Somers V, 2023, IEEE WINT CONF APPL, P1613, DOI 10.1109/WACV56688.2023.00166
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Sun YF, 2019, PROC CVPR IEEE, P393, DOI 10.1109/CVPR.2019.00048
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tan HC, 2023, IEEE T NEUR NET LEAR, V34, P8210, DOI 10.1109/TNNLS.2022.3144163
   Tan HC, 2022, IEEE T CIRC SYST VID, V32, P160, DOI 10.1109/TCSVT.2021.3061412
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang GA, 2020, PROC CVPR IEEE, P6448, DOI 10.1109/CVPR42600.2020.00648
   Wang GS, 2021, IEEE SIGNAL PROC LET, V28, P1155, DOI 10.1109/LSP.2021.3087079
   Wang HR, 2022, IEEE T NEUR NET LEAR, V33, P145, DOI 10.1109/TNNLS.2020.3027589
   Wang T, 2022, AAAI CONF ARTIF INTE, P2540
   Wang Z, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4973
   Wang ZK, 2022, PROC CVPR IEEE, P4744, DOI 10.1109/CVPR52688.2022.00471
   Wu D, 2019, NEUROCOMPUTING, V337, P354, DOI 10.1016/j.neucom.2019.01.079
   Wu Y, 2018, PROC CVPR IEEE, P5177, DOI 10.1109/CVPR.2018.00543
   Xiao T, 2017, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2017.360
   Xu BQ, 2022, IEEE T IMAGE PROCESS, V31, P4651, DOI 10.1109/TIP.2022.3186759
   Xu J, 2018, PROC CVPR IEEE, P2119, DOI 10.1109/CVPR.2018.00226
   Xu YJ, 2021, KNOWL-BASED SYST, V212, DOI 10.1016/j.knosys.2020.106554
   Yan C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11855, DOI 10.1109/ICCV48922.2021.01166
   Yan YC, 2020, PROC CVPR IEEE, P2896, DOI 10.1109/CVPR42600.2020.00297
   Yang JR, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11865, DOI 10.1109/ICCV48922.2021.01167
   Yang X, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3089249
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Yu T, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3243217
   Zhai Y, 2021, IEEE IJCNN, DOI 10.1109/IJCNN52387.2021.9534442
   Zhang L, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3473341
   Zhang ZZ, 2019, PROC CVPR IEEE, P667, DOI 10.1109/CVPR.2019.00076
   Zhao CR, 2021, IEEE T IMAGE PROCESS, V30, P4212, DOI 10.1109/TIP.2021.3070182
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zheng F, 2019, PROC CVPR IEEE, P8506, DOI 10.1109/CVPR.2019.00871
   Zheng KC, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4537, DOI 10.1145/3474085.3475610
   Zheng L, 2016, Arxiv, DOI arXiv:1610.02984
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng WS, 2015, IEEE I CONF COMP VIS, P4678, DOI 10.1109/ICCV.2015.531
   Zheng WS, 2011, PROC CVPR IEEE, P649, DOI 10.1109/CVPR.2011.5995598
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong YJ, 2020, PROC CVPR IEEE, P6826, DOI 10.1109/CVPR42600.2020.00686
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhou QQ, 2020, IEEE T IMAGE PROCESS, V29, P7578, DOI 10.1109/TIP.2020.3004267
   Zhou SR, 2020, PATTERN RECOGN LETT, V138, P617, DOI 10.1016/j.patrec.2020.09.009
   Zhou XY, 2019, Arxiv, DOI arXiv:1904.07850
   Zhuo JX, 2018, IEEE INT CON MULTI
NR 103
TC 1
Z9 1
U1 19
U2 19
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2024
VL 20
IS 3
AR 73
DI 10.1145/3610534
PG 27
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6F8
UT WOS:001153381000013
OA Green Submitted
DA 2024-08-05
ER

PT J
AU He, QL
   Zheng, ZJ
   Hu, HF
AF He, Qiaolin
   Zheng, Zhijie
   Hu, Haifeng
TI A Feature Map isWorth a Video Frame: Rethinking Convolutional Features
   for Visible-Infrared Person Re-identification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Visible-infrared person re-identification; multi-level feature
   aggregation; middle modality; modality gap
AB Visible-Infrared Person Re-identification (VI-ReID) aims to search for the identity of the same person across different spectra. The feature maps obtained from the convolutional layers are generally used for loss calculation in the later stages of the model in VI-ReID, but their role in the early and middle stages of the model remains unexplored. In this article, we propose a novel Rethinking Convolutional Features (ReCF) approach for VI-ReID. ReCF consists of two modules: Middle Feature Generation (MFG), which utilizes the featuremaps in the early stage to reduce significant modality gap, and Temporal Feature Aggregation (TFA), which uses the feature maps in the middle stage to aggregate multi-level features for enlarging the receptive field. MFG generates middle modality features in the form of a learnable convolution layer as a bridge between RGB and IR modalities, which is more flexible than using fixed-parameter grayscale images and yields a better middle modality to further reduce the modality gap. TFA first treats the convolution process as a video sequence, and the feature map of each convolution layer can be considered a worthwhile video frame. Based on this, we can obtain a multi-level receptive field and a temporal refinement. In addition, we introduce a color-unrelated loss and a modality-unrelated loss to constrain the modality features for providing a common feature representation space. Experimental results on the challenging VI-ReID datasets demonstrate that our proposed method achieves state-of-the-art performance.
C1 [He, Qiaolin; Zheng, Zhijie; Hu, Haifeng] Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510006, Peoples R China.
C3 Sun Yat Sen University
RP Hu, HF (corresponding author), Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510006, Peoples R China.
EM heqlin5@mail2.sysu.edu.cn; zhengzhj6@mail2.sysu.edu.cn;
   huhaif@mail.sysu.edu.cn
OI Zheng, Zhijie/0000-0002-5584-1785; He, Qiaolin/0009-0001-2204-8668; Hu,
   Haifeng/0000-0002-4884-323X
FU National Natural Science Foundation of China [62076262, 61673402,
   61273270, 60802069]; National Key Research and Development Program of
   China [2018YFB1601101, 2018YFB1601100]; Natural Science Foundation of
   Guangdong Province [2017A030311029]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62076262, and Grant 61673402, Grant
   61273270, and Grant 60802069; in part by the National Key Research and
   Development Program of China under Grant 2018YFB1601101 and Grant
   2018YFB1601100; in part by the Natural Science Foundation of Guangdong
   Province under Grant 2017A030311029.
CR Alehdaghi Mahdi, 2023, Computer Vision - ECCV 2022 Workshops: Proceedings. Lecture Notes in Computer Science (13805), P720, DOI 10.1007/978-3-031-25072-9_48
   Branson S, 2013, PROC CVPR IEEE, P1806, DOI 10.1109/CVPR.2013.236
   Chen DP, 2018, PROC CVPR IEEE, pCP1, DOI 10.1109/CVPR.2018.00128
   Chen YB, 2017, IEEE INT CONF COMP V, P2590, DOI 10.1109/ICCVW.2017.304
   Chung D, 2017, IEEE I CONF COMP VIS, P1992, DOI 10.1109/ICCV.2017.218
   Dai PY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P677
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng ZY, 2019, AAAI CONF ARTIF INTE, P8239
   Eom C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12016, DOI 10.1109/ICCV48922.2021.01182
   Feng ZX, 2020, IEEE T IMAGE PROCESS, V29, P579, DOI 10.1109/TIP.2019.2928126
   Gao JY, 2018, Arxiv, DOI arXiv:1805.02104
   Glorot X., 2011, P 14 INT C ART INT S, V15, P315, DOI DOI 10.1002/ECS2.1832
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Hao Y, 2019, AAAI CONF ARTIF INTE, P8385
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang NC, 2023, INFORM FUSION, V91, P396, DOI 10.1016/j.inffus.2022.10.024
   Huang P., 2019, CVPR WORKSH, P80
   Huang WX, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3565886
   Huang Z, 2022, IEEE IMAGE PROC, P2671, DOI 10.1109/ICIP46576.2022.9897492
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Kirillov A, 2019, PROC CVPR IEEE, P6392, DOI 10.1109/CVPR.2019.00656
   Kniaz VV, 2019, LECT NOTES COMPUT SC, V11134, P606, DOI 10.1007/978-3-030-11024-6_46
   Köstinger M, 2012, PROC CVPR IEEE, P2288, DOI 10.1109/CVPR.2012.6247939
   Li DG, 2020, AAAI CONF ARTIF INTE, V34, P4610
   Li HF, 2022, KNOWL-BASED SYST, V251, DOI 10.1016/j.knosys.2022.109315
   Li JN, 2019, AAAI CONF ARTIF INTE, P8618
   Li MX, 2018, LECT NOTES COMPUT SC, V11208, P772, DOI 10.1007/978-3-030-01225-0_45
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Li YY, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3412384
   Li YJ, 2019, IEEE I CONF COMP VIS, P8089, DOI 10.1109/ICCV.2019.00818
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Lin XY, 2022, PROC CVPR IEEE, P20941, DOI 10.1109/CVPR52688.2022.02030
   Ling YG, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P889, DOI 10.1145/3394171.3413821
   Liu HJ, 2020, NEUROCOMPUTING, V398, P11, DOI 10.1016/j.neucom.2020.01.089
   Liu JW, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3231741
   Liu YH, 2019, AAAI CONF ARTIF INTE, P8786
   Liu Z, 2022, KNOWL-BASED SYST, V255, DOI 10.1016/j.knosys.2022.109741
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Mang Ye, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P229, DOI 10.1007/978-3-030-58520-4_14
   McLaughlin N, 2016, PROC CVPR IEEE, P1325, DOI 10.1109/CVPR.2016.148
   Nguyen DT, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17030605
   Park H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12026, DOI 10.1109/ICCV48922.2021.01183
   Qi L, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3419439
   Qian XL, 2017, IEEE I CONF COMP VIS, P5409, DOI 10.1109/ICCV.2017.577
   Ruan WJ, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3402666
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shen C, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3309881
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Wang GA, 2020, AAAI CONF ARTIF INTE, V34, P12144
   Wang GA, 2019, IEEE I CONF COMP VIS, P3622, DOI 10.1109/ICCV.2019.00372
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang Kan, 2022, ACM Transactions on Multimedia Computing, Communications and Applications, V19
   Wang ZX, 2019, PROC CVPR IEEE, P618, DOI 10.1109/CVPR.2019.00071
   Wu AC, 2020, INT J COMPUT VISION, V128, P1765, DOI 10.1007/s11263-019-01290-1
   Wu AC, 2017, IEEE I CONF COMP VIS, P5390, DOI 10.1109/ICCV.2017.575
   Wu Y, 2018, PROC CVPR IEEE, P5177, DOI 10.1109/CVPR.2018.00543
   Xiang Wangmeng, 2020, P AS C COMP VIS
   Xie PY, 2022, IEEE T MULTIMEDIA, V24, P4250, DOI 10.1109/TMM.2022.3186177
   Xinqian Gu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P228, DOI 10.1007/978-3-030-58536-5_14
   Xu SJ, 2017, IEEE I CONF COMP VIS, P4743, DOI 10.1109/ICCV.2017.507
   Yair N, 2018, PROC CVPR IEEE, P3165, DOI 10.1109/CVPR.2018.00334
   Yan Lu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13376, DOI 10.1109/CVPR42600.2020.01339
   Yang MX, 2022, PROC CVPR IEEE, P14288, DOI 10.1109/CVPR52688.2022.01391
   Yang Y, 2014, LECT NOTES COMPUT SC, V8689, P536, DOI 10.1007/978-3-319-10590-1_35
   Yao HT, 2019, IEEE T IMAGE PROCESS, V28, P2860, DOI 10.1109/TIP.2019.2891888
   Ye M, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1092
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Ye M, 2021, IEEE T INF FOREN SEC, V16, P728, DOI 10.1109/TIFS.2020.3001665
   Ye M, 2020, PROC CVPR IEEE, P5456, DOI 10.1109/CVPR42600.2020.00550
   Ye M, 2020, IEEE T IMAGE PROCESS, V29, P9387, DOI 10.1109/TIP.2020.2998275
   Ye M, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P347, DOI 10.1145/3343031.3351043
   Ye M, 2020, IEEE T INF FOREN SEC, V15, P407, DOI 10.1109/TIFS.2019.2921454
   Ye M, 2018, AAAI CONF ARTIF INTE, P7501
   Yuan X, 2023, IEEE J-STSP, V17, P560, DOI 10.1109/JSTSP.2023.3250989
   Zhang L, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3473341
   Zhang Q, 2022, PROC CVPR IEEE, P7339, DOI 10.1109/CVPR52688.2022.00720
   Zhang W, 2018, IEEE T CIRC SYST VID, V28, P2768, DOI 10.1109/TCSVT.2017.2718188
   Zheng F, 2019, PROC CVPR IEEE, P8506, DOI 10.1109/CVPR.2019.00871
   Zheng L, 2016, LECT NOTES COMPUT SC, V9910, P868, DOI 10.1007/978-3-319-46466-4_52
   Zheng WS, 2013, IEEE T PATTERN ANAL, V35, P653, DOI 10.1109/TPAMI.2012.138
   Zheng ZD, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3159171
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhou Z, 2017, PROC CVPR IEEE, P6776, DOI 10.1109/CVPR.2017.717
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 87
TC 0
Z9 0
U1 2
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2024
VL 20
IS 2
SI SI
AR 59
DI 10.1145/3617375
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W6GR4
UT WOS:001092595800029
DA 2024-08-05
ER

PT J
AU Liang, XP
   Tang, ZJ
   Li, ZX
   Yu, MZ
   Zhang, HY
   Zhang, XQ
AF Liang, Xiaoping
   Tang, Zhenjun
   Li, Zhixin
   Yu, Mengzhu
   Zhang, Hanyun
   Zhang, Xianquan
TI Robust Hashing via Global and Local Invariant Features for Image Copy
   Detection
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Robust hashing; saliency map; invariant features; image copy detection;
   dimension reduction
ID RING PARTITION
AB Robust hashing is a powerful technique for processing large-scale images. Currently, many reported image hashing schemes do not perform well in balancing the performances of discrimination and robustness, and thus they cannot efficiently detect image copies, especially the image copies with multiple distortions. To address this, we exploit global and local invariant features to develop a novel robust hashing for image copy detection. A critical contribution is the global feature calculation by gray level co-occurrence moment learned from the saliency map determined by the phase spectrum of quaternion Fourier transform, which can significantly enhance discrimination without reducing robustness. Another essential contribution is the local invariant feature computation via Kernel Principal Component Analysis (KPCA) and vector distances. As KPCA can maintain the geometric relationships within image, the local invariant features learned with KPCA and vector distances can guarantee discrimination and compactness. Moreover, the global and local invariant features are encrypted to ensure security. Finally, the hash is produced via the ordinal measures of the encrypted features for making a short length of hash. Numerous experiments are conducted to show efficiency of our scheme. Compared with some well-known hashing schemes, our scheme demonstrates a preferable classification performance of discrimination and robustness. The experiments of detecting image copies with multiple distortions are tested and the results illustrate the effectiveness of our scheme.
C1 [Liang, Xiaoping; Tang, Zhenjun; Li, Zhixin; Yu, Mengzhu; Zhang, Hanyun; Zhang, Xianquan] Guangxi Normal Univ, Key Lab Educ Blockchain & Intelligent Technol, Minist Educ, Guilin 541004, Peoples R China.
   Guangxi Normal Univ, Guangxi Key Lab Multisource Informat Mining & Sec, Guilin, Peoples R China.
C3 Guangxi Normal University; Guangxi Normal University
RP Tang, ZJ (corresponding author), Guangxi Normal Univ, Key Lab Educ Blockchain & Intelligent Technol, Minist Educ, Guilin 541004, Peoples R China.
EM xpliang6@163.com; tangzj230@163.com; lizx@gxnu.edu.cn; 475452749@qq.com;
   1391912366@qq.com; zxq6622@163.com
OI Li, Zhixin/0000-0002-5313-6134; yu, mengzhu/0000-0002-5650-6065; Zhang,
   Xianquan/0000-0003-3359-117X; Liang, Xiaoping/0000-0001-5644-3642; Tang,
   Zhenjun/0000-0003-3664-1363
FU Guangxi Natural Science Foundation [2022GXNSFAA035506]; National Natural
   Science Foundation of China [62272111, 62062013, 61962008, 62276073];
   Guangxi "Bagui Scholar" Team for Innovation and Research, Guangxi Talent
   Highland Project of Big Data Intelligence and Application; Guangxi
   Collaborative Innovation Center of Multi-source Information Integration
   and Intelligent Processing
FX This work is partially supported by the Guangxi Natural Science
   Foundation (Grant No. 2022GXNSFAA035506), the National Natural Science
   Foundation of China (Grants No. 62272111, No. 62062013, No. 61962008,
   and No. 62276073), Guangxi "Bagui Scholar" Team for Innovation and
   Research, Guangxi Talent Highland Project of Big Data Intelligence and
   Application, and Guangxi Collaborative Innovation Center of Multi-source
   Information Integration and Intelligent Processing.
CR Abdullahi SM, 2020, IEEE T INF FOREN SEC, V15, P2587, DOI 10.1109/TIFS.2020.2971142
   Bhat DN, 1996, PROC CVPR IEEE, P351, DOI 10.1109/CVPR.1996.517096
   Biswas R, 2020, NEUROCOMPUTING, V383, P24, DOI 10.1016/j.neucom.2019.11.065
   Chen HZ, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3572777
   Davarzani R, 2016, MULTIMED TOOLS APPL, V75, P4639, DOI 10.1007/s11042-015-2496-6
   Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010
   Gao Q, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON MECHATRONICS AND AUTOMATION (ICMA), P247, DOI 10.1109/ICMA.2017.8015822
   Guo CL, 2008, PROC CVPR IEEE, P2908
   Guo CL, 2010, IEEE T IMAGE PROCESS, V19, P185, DOI 10.1109/TIP.2009.2030969
   Huang X, 2016, IEEE TRUST BIG, P14, DOI [10.1109/TrustCom.2016.39, 10.1109/TrustCom.2016.0040]
   Huang ZQ, 2023, IEEE T DEPEND SECURE, V20, P463, DOI 10.1109/TDSC.2021.3136163
   Huang ZQ, 2021, IEEE T CIRC SYST VID, V31, P2808, DOI 10.1109/TCSVT.2020.3027001
   Huang ZQ, 2021, IEEE T MULTIMEDIA, V23, P1516, DOI 10.1109/TMM.2020.2999188
   Huang ZQ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1389, DOI 10.1145/3240508.3240690
   Li YN, 2018, IEEE SIGNAL PROC LET, V25, P140, DOI 10.1109/LSP.2017.2777881
   Liang XP, 2022, IET IMAGE PROCESS, V16, P3225, DOI 10.1049/ipr2.12555
   Liang XP, 2023, IEEE T MULTIMEDIA, V25, P1085, DOI 10.1109/TMM.2021.3139217
   Liang XP, 2023, IEEE T KNOWL DATA EN, V35, P3765, DOI 10.1109/TKDE.2021.3131188
   Liang XP, 2021, MULTIMEDIA SYST, V27, P389, DOI 10.1007/s00530-020-00696-z
   Liu SG, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3355394
   Monga V, 2006, IEEE T IMAGE PROCESS, V15, P3452, DOI 10.1109/TIP.2006.881948
   Mukherjee P, 2015, IEEE IMAGE PROC, P1290, DOI 10.1109/ICIP.2015.7351008
   Ouyang JL, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2978572
   Parashar D, 2021, IEEE SIGNAL PROC LET, V28, P66, DOI 10.1109/LSP.2020.3045638
   Qin C, 2021, IEEE T CIRC SYST VID, V31, P4523, DOI 10.1109/TCSVT.2020.3047142
   Qin C, 2021, IEEE SIGNAL PROC LET, V28, P1893, DOI 10.1109/LSP.2021.3111820
   Qin C, 2018, INFORM SCIENCES, V423, P284, DOI 10.1016/j.ins.2017.09.060
   Rampun Andrik, 2013, P 6 INT C COMP VIS C, P1, DOI [10.1145/2466715.2466720, DOI 10.1145/2466715.2466720]
   Schaefer G, 2004, PROC SPIE, V5307, P472, DOI 10.1117/12.525375
   Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467
   Shen Q, 2020, SIGNAL PROCESS, V166, DOI 10.1016/j.sigpro.2019.107244
   Singh J, 2013, IEEE T GEOSCI REMOTE, V51, P5273, DOI 10.1109/TGRS.2012.2230892
   Singh SP, 2022, IEEE T DEPEND SECURE, V19, P2211, DOI 10.1109/TDSC.2021.3050435
   Sun ZN, 2009, IEEE T PATTERN ANAL, V31, P2211, DOI 10.1109/TPAMI.2008.240
   Swaminathan A, 2006, IEEE T INF FOREN SEC, V1, P215, DOI 10.1109/TIFS.2006.873601
   Tang ZJ, 2021, COMPUT J, V64, P1656, DOI 10.1093/comjnl/bxz127
   Tang ZJ, 2019, IEEE T KNOWL DATA EN, V31, P549, DOI 10.1109/TKDE.2018.2837745
   Tang ZJ, 2018, COMPUT J, V61, P1695, DOI 10.1093/comjnl/bxy047
   Tang ZJ, 2018, NEUROCOMPUTING, V308, P147, DOI 10.1016/j.neucom.2018.04.057
   Tang ZJ, 2017, SIGNAL PROCESS, V137, P240, DOI 10.1016/j.sigpro.2017.02.008
   Tang ZJ, 2016, COMPUT SECUR, V62, P133, DOI 10.1016/j.cose.2016.07.006
   Tang ZJ, 2016, IEEE T INF FOREN SEC, V11, P200, DOI 10.1109/TIFS.2015.2485163
   Tang ZJ, 2014, OPTIK, V125, P5102, DOI 10.1016/j.ijleo.2014.05.015
   Tang ZJ, 2014, IEEE T KNOWL DATA EN, V26, P711, DOI 10.1109/TKDE.2013.45
   WanJing Meng, 2010, 2010 Proceedings of 3rd International Congress on Image and Signal Processing (CISP 2010), P1832, DOI 10.1109/CISP.2010.5646813
   Xie XD, 2006, IEEE T IMAGE PROCESS, V15, P2481, DOI 10.1109/TIP.2006.877435
   Xiong C, 2021, SECUR COMMUN NETW, V2021, DOI 10.1155/2021/8297244
   Yan CP, 2016, SIGNAL PROCESS, V121, P1, DOI 10.1016/j.sigpro.2015.10.027
   Ye ZD, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3356338
   Yuan XR, 2021, IEEE ACCESS, V9, P49325, DOI 10.1109/ACCESS.2021.3069045
   Zhao Y, 2021, SECUR COMMUN NETW, V2021, DOI 10.1155/2021/3803481
   Zhao Y, 2013, IEEE T INF FOREN SEC, V8, P55, DOI 10.1109/TIFS.2012.2223680
NR 52
TC 3
Z9 3
U1 6
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2024
VL 20
IS 1
AR 2
DI 10.1145/3600234
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T8LL3
UT WOS:001080441800002
OA hybrid
DA 2024-08-05
ER

PT J
AU Cortés, C
   Viola, I
   Gutiérrez, J
   Jansen, J
   Subramanyam, S
   Alexiou, E
   Pérez, P
   García, N
   César, P
AF Cortes, Carlos
   Viola, Irene
   Gutierrez, Jesus
   Jansen, Jack
   Subramanyam, Shishir
   Alexiou, Evangelos
   Perez, Pablo
   Garcia, Narciso
   Cesar, Pablo
TI Delay Threshold for Social Interaction in Volumetric eXtended Reality
   Communication
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE eXtended reality; volumetric Social XR; delay
AB Immersive technologies like eXtended Reality (XR) are the next step in videoconferencing. In this context, understanding the effect of delay on communication is crucial. This article presents the first study on the impact of delay on collaborative tasks using a realistic Social XR system. Specifically, we design an experiment and evaluate the impact of end-to-end delays of 300, 600, 900, 1,200, and 1,500 ms on the execution of a standardized task involving the collaboration of two remote users that meet in a virtual space and construct block-based shapes. To measure the impact of the delay in this communication scenario, objective and subjective data were collected. As objective data, we measured the time required to execute the tasks and computed conversational characteristics by analyzing the recorded audio signals. As subjective data, a questionnaire was prepared and completed by every user to evaluate different factors such as overall quality, perception of delay, annoyance using the system, level of presence, cybersickness, and other subjective factors associated with social interaction. The results show a clear influence of the delay on the perceived quality and a significant negative effect as the delay increases. Specifically, the results indicate that the acceptable threshold for end-to-end delay should not exceed 900 ms. This article additionally provides guidelines for developing standardized XR tasks for assessing interaction in Social XR environments.
C1 [Cortes, Carlos; Gutierrez, Jesus; Garcia, Narciso] Univ Politecn Madrid, Madrid 28040, Spain.
   [Viola, Irene; Jansen, Jack; Subramanyam, Shishir; Cesar, Pablo] Ctr Wiskunde & Informat, Amsterdam, Netherlands.
   [Alexiou, Evangelos] Netherlands Org Appl Sci Res, The Hague, Netherlands.
   [Alexiou, Evangelos] Xiaomi Communicat Co Ltd, NL-2595 AM The Hague, Netherlands.
   [Perez, Pablo] Extended Real Labs, Madrid 28050, Spain.
   [Cesar, Pablo] Delft Univ Technol, Amsterdam, Netherlands.
C3 Universidad Politecnica de Madrid; Netherlands Organization Applied
   Science Research; Delft University of Technology
RP Cortés, C (corresponding author), Univ Politecn Madrid, Madrid 28040, Spain.
EM carlos.cs@upm.es; irene.viola@cwi.nl; jesus.gutierrez@upm.es;
   jack.jansen@cwi.nl; s.subramanyam@cwi.nl; alexiou@xiaomi.com;
   pablo.perez@nokia.com; narciso.garcia@upm.es; p.s.cesar@cwi.nl
OI Cortes, Carlos/0000-0001-8867-0559; Jansen, Jack/0000-0002-7006-2560;
   Gutierrez, Jesus/0000-0001-7878-4712; Alexiou,
   Evangelos/0000-0002-5561-9711; Cesar, Pablo/0000-0003-1752-6837
FU European Union [HORIZON-IA-101070250, HORIZON-IA-101070109]; MCIN/AEI of
   the Spanish Government [PID2020-115132RB]; Ministry of Digital
   Transformation of the Spanish Government [UNICO-5G I+D
   TSI063000-2021-79, RED.ES 2021/C005/00144164]; NextGenerationEU
   (Recovery, Transformation and Resilience Plan-PRTR)
FX This work was partially supported by projects HORIZON-IA-101070250
   (XRECO) and HORIZON-IA-101070109 (TRANSMIXR) funded by the European
   Union, by project PID2020-115132RB (SARAOS) funded by
   MCIN/AEI/10.13039/501100011033 of the Spanish Government, and by
   projects UNICO-5G I+D TSI063000-2021-79 (B5GEMiNI-AIUC) and RED.ES
   2021/C005/00144164 (COMODIA) funded by the Ministry of Digital
   Transformation of the Spanish Government and the NextGenerationEU
   (Recovery, Transformation and Resilience Plan-PRTR).
CR Abdallah M, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3212804
   Attig C, 2017, LECT NOTES ARTIF INT, V10276, P3, DOI 10.1007/978-3-319-58475-1_1
   Battle L, 2020, IEEE T VIS COMPUT GR, V26, P1246, DOI 10.1109/TVCG.2019.2934556
   Becher A, 2020, VIRTUAL REAL-LONDON, V24, P369, DOI 10.1007/s10055-019-00395-9
   Beck S, 2013, IEEE T VIS COMPUT GR, V19, P616, DOI 10.1109/TVCG.2013.33
   Berndtsson G., 2012, 2012 Proceedings of the 19th International Packet Video Workshop (PV 2012), P25, DOI 10.1109/PV.2012.6229740
   Brunnstrom K., 2013, Qualinet white paper on definitions of quality of experience
   Brunnström K, 2020, SIGNAL PROCESS-IMAGE, V89, DOI 10.1016/j.image.2020.116005
   Carballeira P, 2022, IEEE T MULTIMEDIA, V24, P2378, DOI 10.1109/TMM.2021.3079711
   Cassola F, 2021, 2021 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2021), P44, DOI 10.1109/VRW52623.2021.00014
   Cermak GW, 2005, INT J SPEECH TECHNOL, V8, P259, DOI 10.1007/s10772-006-6368-3
   Cortés C, 2022, IEEE IMAGE PROC, P3131, DOI 10.1109/ICIP46576.2022.9897983
   Cortés C, 2020, INT WORK QUAL MULTIM, DOI 10.1109/qomex48832.2020.9123114
   Cortés C, 2019, IEEE ICCE, P281, DOI [10.1109/icce-berlin47944.2019.8966170, 10.1109/ICCE-Berlin47944.2019.8966170]
   Garg S, 2022, EXTENDED ABSTRACTS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2022, DOI 10.1145/3491101.3519678
   George D., 2010, SPSS WINDOWS STEP ST, DOI DOI 10.4324/9781003205333
   Gunkel S, 2018, TVX 2018: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON INTERACTIVE EXPERIENCES FOR TV AND ONLINE VIDEO, P233, DOI 10.1145/3210825.3213566
   Gupta K, 2016, IEEE T VIS COMPUT GR, V22, P2413, DOI 10.1109/TVCG.2016.2593778
   Gutiérrez J, 2022, IEEE T MULTIMEDIA, V24, P3087, DOI 10.1109/TMM.2021.3093717
   Hennig-Thurau T, 2023, J ACAD MARKET SCI, V51, P889, DOI 10.1007/s11747-022-00908-0
   ITU, 1993, Rec. ITU-T P.56
   ITU, 2016, Rec. ITU-T P.1312
   ITU, 2023, Rec. ITU-R BT.500-15
   ITU, 2017, Rec. ITU-T P.1301
   ITU, 2016, Rec. ITU-T P.1305
   ITU, 2021, Rec. ITU-R BT.1359
   ITU, 2000, Rec. ITU-T P.920
   ITU, 2008, Rec. ITU-T P.910
   ITU, 2015, Rec. ITU-T G.107
   Kachach R, 2021, INT SYM MIX AUGMENT, P451, DOI 10.1109/ISMAR-Adjunct54149.2021.00104
   Lawrence L, 2018, PROCEEDINGS OF THE 30TH AUSTRALIAN COMPUTER-HUMAN INTERACTION CONFERENCE (OZCHI 2018), P453, DOI 10.1145/3292147.3292203
   LEWIS JR, 1989, PROC HUM FACT SOC AN, P1223
   Li J, 2021, INT SYM MIX AUGMENT, P284, DOI 10.1109/ISMAR52148.2021.00044
   Li Jie, 2023, Immersive Video Technologies, P609, DOI [10.1016/B978-0-32-391755-1.00028-6, DOI 10.1016/B978-0-32-391755-1.00028-6]
   Lifesize, 2019, Industrial Report
   Mekuria R., 2013, Proceedings of the 4th ACM Multimedia Systems Conference, MMSys '13, New York, NY, USA, P24, DOI DOI 10.1145/2483977.2483980
   OBS, OBS Studio
   Orts-Escolano S, 2016, UIST 2016: PROCEEDINGS OF THE 29TH ANNUAL SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P741, DOI 10.1145/2984511.2984517
   Pérez P, 2022, FRONT SIGNAL PROC-SW, V2, DOI 10.3389/frsip.2022.917684
   Pérez P, 2021, 2021 4TH IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND VIRTUAL REALITY (AIVR 2021), P179, DOI 10.1109/AIVR52153.2021.00040
   RITU, 2021, Rec. ITU-T G.1035
   Sánchez-Margallo JA, 2021, FRONT VIRTUAL REAL, V2, DOI 10.3389/frvir.2021.692641
   Schmitt M, 2018, IEEE T MULTIMEDIA, V20, P1781, DOI 10.1109/TMM.2017.2777466
   Schmitt Marwin, 2014, P 3 INT WORKSH SOC A, P13, DOI [10.1145/2661126.2661135, DOI 10.1145/2661126.2661135]
   Schoenenberg K., 2014, 2014 Sixth International Workshop on Quality of Multimedia Experience (QoMEX), P31, DOI 10.1109/QoMEX.2014.6982282
   Schoenenberg K, 2014, INT J HUM-COMPUT ST, V72, P477, DOI 10.1016/j.ijhcs.2014.02.004
   Seuren LM, 2021, J PRAGMATICS, V172, P63, DOI 10.1016/j.pragma.2020.11.005
   Skowronek J, 2022, IEEE ACCESS, V10, P63885, DOI 10.1109/ACCESS.2022.3176369
   Song JR, 2016, IEEE T MULTIMEDIA, V18, P444, DOI 10.1109/TMM.2016.2520090
   Tam Jennifer., 2012, ACM CHI '12 Extended Abstracts on Human Factors in Computing Systems (CHI EA '12), P2045, DOI DOI 10.1145/2212776.2223750
   Viola I, 2023, IEEE MULTIMEDIA, V30, P48, DOI 10.1109/MMUL.2023.3263943
   Viola Irene, 2023, Immersive Video Technologies, P425, DOI 10.1016/B978-0-32-391755-1.00021-3
   Wang P, 2020, INTERACT COMPUT, V32, P153, DOI 10.1093/iwcomp/iwaa012
   Wang Yue, 2021, 2021 2nd International Conference on Big Data and Informatization Education (ICBDIE), P688, DOI 10.1109/ICBDIE52740.2021.00162
   Zioulis N, 2016, IEEE IMAGE PROC, P365, DOI 10.1109/ICIP.2016.7532380
NR 55
TC 0
Z9 0
U1 1
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2024
VL 20
IS 7
SI SI
AR 206
DI 10.1145/3651164
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL0Q6
UT WOS:001234494100021
OA hybrid
DA 2024-08-05
ER

PT J
AU Wang, H
   Li, H
   Smahi, A
   Zhao, F
   Yao, Y
   Chan, CC
   Wang, SY
   Yang, WY
   Li, SYR
AF Wang, Han
   Li, Hui
   Smahi, Abla
   Zhao, Feng
   Yao, Yao
   Chan, Ching Chuen
   Wang, Shiyu
   Yang, Wenyuan
   Li, Shuo-Yen Robert
TI MIS: A Multi-Identifier Management and Resolution System in the
   Metaverse
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Metaverse; blockchain; Domain Name System; identifier management
AB The metaverse gradually evolves into a virtual world containing a series of interconnected sub-metaverses. Diverse digital resources, including identities, contents, services, and supporting data, are key components of the sub-metaverse. Therefore, a Domain Name System (DNS)-like system is necessary for efficient management and resolution. However, the legacy DNS was designed with security vulnerabilities and trust risks due to centralized issues. Blockchain is used to mitigate these concerns due to its decentralized features. Additionally, it supports identity management as a default feature, making it a natural fit for the metaverse. While there are several DNS alternatives based on the blockchain, they either manage only a single type of identifiers or isolate identities from other sorts of identifiers, making it difficult for sub-metaverses to coexist and connect with each other. This article proposes a Multi-Identifier management and resolution System (MIS) in the meta-verse, supporting the registration, resolution, and inter-translation functions. The basic MIS is portrayed as a four-tier architecture on a consortium blockchain due to its manageability, enhanced security, and efficiency properties. On-chain data is lightweight and compressed to save on storage while accelerating reading and writing operations. The resource data is encrypted based on the attributes of the sub-metaverse in the storage tier for privacy protection and access control. For users with decentralization priorities, a modification named EMIS is built on top of Ethereum. Finally, MIS is implemented on two testbeds and is available online as the open-source system. The first testbed consists of 4 physical servers located in the UK and Malaysia while the second is made up of 200 virtual machines (VMs) spread over 26 countries across all 5 continents on Google Cloud. Experiments indicate that MIS provides efficient reading and writing performance than the legacy DNS and other public blockchain-based workarounds including EMIS and Ethereum Name Service (ENS).
C1 [Wang, Han; Li, Hui; Smahi, Abla; Zhao, Feng; Yao, Yao] Peking Univ, Sch Elect & Comp Engn, Shenzhen 518055, Peoples R China.
   [Li, Hui] Foshan Univ, Sch Math & Big Data, Foshan 528000, Peoples R China.
   [Li, Hui] Foshan Saisichan Technol Co Ltd, Foshan 528000, Peoples R China.
   [Chan, Ching Chuen] Univ Hong Kong, Dept Elect & Elect Engn, Hong Kong 999077, Peoples R China.
   [Wang, Shiyu; Li, Shuo-Yen Robert] Univ Elect Sci Technol China, Chengdu 611731, Peoples R China.
   [Yang, Wenyuan] Sun Yat Sen Univ, Sch Cyber Sci & Technol, Shenzhen 518055, Peoples R China.
C3 Peking University; Foshan University; University of Hong Kong;
   University of Electronic Science & Technology of China; Sun Yat Sen
   University
RP Li, H (corresponding author), Peking Univ, Sch Elect & Comp Engn, Shenzhen 518055, Peoples R China.; Li, H (corresponding author), Foshan Univ, Sch Math & Big Data, Foshan 528000, Peoples R China.; Li, H (corresponding author), Foshan Saisichan Technol Co Ltd, Foshan 528000, Peoples R China.; Yang, WY (corresponding author), Sun Yat Sen Univ, Sch Cyber Sci & Technol, Shenzhen 518055, Peoples R China.
EM wanghan2017@pku.edu.cn; lih64@pkusz.edu.cn; smahi_abla@pku.edu.cn;
   2101212848@stu.pku.edu.cn; yaoyao@stu.pku.edu.cn; ccchan@eee.hku.hk;
   2021090919002@std.uestc.edu.cn; yangwy56@mail.sysu.edu.cn;
   bobli@uestc.edu.cn
RI Smahi, Abla/AAE-2229-2021
OI Smahi, Abla/0000-0003-4371-433X
FU National Keystone Research and Development Program of China
   [2017YFB0803204]; Foshan Innovation Team [2018IT100082]; Basic Research
   Enhancement Program of China [2021-JCJQ-JJ-0483, [2020]386, SZFGW
   [2019]261]; Guangdong Province Research and Development Key Program
   [2019B010137001]; Guangdong Province Basic Research [2022A1515010836];
   Shenzhen Research Programs [JCYJ20220531093206015,
   JCYJ20210324122013036, JCYJ20190808155607340]; Shenzhen Fundamental
   Research Program [GXWD20201231165807007-20200807164903001,
   2019ZTE03-01]; Huawei Funding
FX This work was supported by the National Keystone Research and
   Development Program of China [2017YFB0803204]; Foshan Innovation Team
   [2018IT100082]; Basic Research Enhancement Program of China
   [2021-JCJQ-JJ-0483]; China Environment for Network Innovation GJFGW
   [2020]386, SZFGW [2019]261; Guangdong Province Research and Development
   Key Program [2019B010137001]; Guangdong Province Basic Research
   [2022A1515010836]; Shenzhen Research Programs [JCYJ20220531093206015,
   JCYJ20210324122013036, JCYJ20190808155607340]; Shenzhen Fundamental
   Research Program [GXWD20201231165807007-20200807164903001]; ZTE Funding
   [2019ZTE03-01]; Huawei Funding [TC20201222002].
CR Ali M, 2016, PROCEEDINGS OF USENIX ATC '16: 2016 USENIX ANNUAL TECHNICAL CONFERENCE, P181
   Ali Muneeb, 2020, Stacks 2.0: Apps and Smart Contracts for Bitcoin
   [Anonymous], 2019, Delegated proof of stake (dpos)
   [Anonymous], 1998, Proceedings, V17
   [Anonymous], 2022, Size of the Bitcoin Blockchain from January 2009
   [Anonymous], 2022, Ens Documentation
   Bai Yongjie, 2021, P 2021 3 INT C BLOCK
   Bao ZJ, 2018, Arxiv, DOI arXiv:1806.02008
   Benshoof B, 2016, IEEE SYM PARA DISTR, P1279, DOI 10.1109/IPDPSW.2016.109
   Bethencourt J, 2007, P IEEE S SECUR PRIV, P321, DOI 10.1109/sp.2007.11
   Blaze M, 1998, LECT NOTES COMPUT SC, V1403, P127, DOI 10.1007/BFb0054122
   Boneh Dan, 2001, P INT C THEOR APPL C
   Bouras MA, 2021, FUTURE INTERNET, V13, DOI 10.3390/fi13020024
   Bruun A, 2019, LECT NOTES COMPUT SC, V11748, P431, DOI 10.1007/978-3-030-29387-1_24
   Castro M, 1999, USENIX ASSOCIATION PROCEEDINGS OF THE THIRD SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '99), P173, DOI 10.1145/571637.571640
   cryptowexels, Cryptovoxels-a user Owned Virtual World
   Cui H, 2016, COMPUT J, V59, P1220, DOI 10.1093/comjnl/bxw007
   Decentraland, WELCOME DECENTRALAND
   Dionisio JDN, 2013, ACM COMPUT SURV, V45, DOI 10.1145/2480741.2480751
   Dowling M, 2022, FINANC RES LETT, V44, DOI 10.1016/j.frl.2021.102097
   Duan HH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P153, DOI 10.1145/3474085.3479238
   Emulab, About us
   ethereum, About us
   Eyal I, 2016, 13TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION (NSDI '16), P45
   Eyal I, 2014, LECT NOTES COMPUT SC, V8437, P436, DOI 10.1007/978-3-662-45472-5_28
   Ge CP, 2022, IEEE T DEPEND SECURE, V19, P2864, DOI 10.1109/TDSC.2021.3065999
   Ge CP, 2022, IEEE T DEPEND SECURE, V19, P2907, DOI 10.1109/TDSC.2021.3076580
   Ge CP, 2021, IEEE T DEPEND SECURE, V18, P2787, DOI 10.1109/TDSC.2020.2963978
   Ge CP, 2021, IEEE T DEPEND SECURE, V18, P1214, DOI 10.1109/TDSC.2019.2899300
   Gilad Y, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P51, DOI 10.1145/3132747.3132757
   Gilbert S., 2002, SIGACT News, V33, P51, DOI 10.1145/564585.564601
   github, Mis-blockchain
   Goanta Catalina, 2020, Selling land in decentraland: The regime of non-fungible tokens on the ethereum blockchain under the digital content directive
   Handshake, about us
   He GB, 2020, FUTURE GENER COMP SY, V102, P912, DOI 10.1016/j.future.2019.09.037
   ipfs, Ipfs Powers the Distributed Web
   Jin T, 2017, INT CONF UBIQ FUTUR, P75
   Jovanovic A, 2022, ELECTRONICS-SWITZ, V11, DOI 10.3390/electronics11030317
   Kiayias A, 2017, LECT NOTES COMPUT SC, V10401, P357, DOI 10.1007/978-3-319-63688-7_12
   Kotla Ramakrishna, 2007, P 21 ACM SIGOPS S OP
   Lee L H., 2021, PREPRINT
   Lewko A, 2011, LECT NOTES COMPUT SC, V6632, P568, DOI 10.1007/978-3-642-20465-4_31
   Li CX, 2020, PROCEEDINGS OF THE 2020 USENIX ANNUAL TECHNICAL CONFERENCE, P515
   Li H, 2020, IEEE ACCESS, V8, P36569, DOI 10.1109/ACCESS.2020.2974327
   Li KJ, 2020, FRONT BLOCKCHAIN, V3, DOI 10.3389/fbloc.2020.00011
   Li KJ, 2017, 2017 19TH IEEE INTERNATIONAL CONFERENCE ON HIGH PERFORMANCE COMPUTING AND COMMUNICATIONS (HPCC) / 2017 15TH IEEE INTERNATIONAL CONFERENCE ON SMART CITY (SMARTCITY) / 2017 3RD IEEE INTERNATIONAL CONFERENCE ON DATA SCIENCE AND SYSTEMS (DSS), P466, DOI 10.1109/HPCC-SmartCity-DSS.2017.61
   Liang Xiaohui., 2010, CIPHERTEXT POLICY AT
   Liu Jingqiang, 2018, P 2018 IEEE 3 INT C, P18
   Nakamoto S., 2008, BITCOIN PEER TO PEER, P21260
   Nami, About us
   Perard D, 2018, IEEE 2018 INTERNATIONAL CONGRESS ON CYBERMATICS / 2018 IEEE CONFERENCES ON INTERNET OF THINGS, GREEN COMPUTING AND COMMUNICATIONS, CYBER, PHYSICAL AND SOCIAL COMPUTING, SMART DATA, BLOCKCHAIN, COMPUTER AND INFORMATION TECHNOLOGY, P1622, DOI 10.1109/Cybermatics_2018.2018.00271
   Popov S, 2017, THE TANGLE
   Qi XD, 2020, PROC INT CONF DATA, P1926, DOI 10.1109/ICDE48307.2020.00205
   Sahai A, 2005, LECT NOTES COMPUT SC, V3494, P457, DOI 10.1007/11426639_27
   Sanchez Joe, 2007, P SOC INF TECH TEACH
   sandbox, The Sandbox Game-user-Generated Crypto
   Shamir Adi, 1985, P ADV CRYPT P CRYPTO
   Shen YT, 2021, INTELL AUTOM SOFT CO, V27, P259, DOI 10.32604/iasc.2021.013704
   Slepak Greg, 2014, Dnschain + okturtles
   Sompolinsky Y, 2015, LECT NOTES COMPUT SC, V8975, P507, DOI 10.1007/978-3-662-47854-7_32
   Song Y., 2021, Interna-tional Journal of Advanced Science and Convergence, V3, P9
   speedtest, about us.
   Swartz Aaron, Squaring the Triangle: Secure, Decentralized, Human-readable Names
   Wang Wentong, 2019, P INT C ART INT SEC
   Wang XG, 2017, 2017 19TH IEEE INTERNATIONAL CONFERENCE ON HIGH PERFORMANCE COMPUTING AND COMMUNICATIONS (HPCC) / 2017 15TH IEEE INTERNATIONAL CONFERENCE ON SMART CITY (SMARTCITY) / 2017 3RD IEEE INTERNATIONAL CONFERENCE ON DATA SCIENCE AND SYSTEMS (DSS), P617, DOI 10.1109/HPCC-SmartCity-DSS.2017.83
   Wicker S. B., 1999, Reed-Solomon codes and their applications
   Yang HK, 2019, IEEE ACCESS, V7, P6262, DOI 10.1109/ACCESS.2018.2885037
   Yin MF, 2019, PROCEEDINGS OF THE 2019 ACM SYMPOSIUM ON PRINCIPLES OF DISTRIBUTED COMPUTING (PODC '19), P347, DOI 10.1145/3293611.3331591
   Yoon K, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON INTELLIGENT REALITY (ICIR 2021), P49, DOI 10.1109/ICIR51845.2021.00016
   Yoon W, 2019, 2019 IEEE INTERNATIONAL CONFERENCE ON BLOCKCHAIN AND CRYPTOCURRENCY (ICBC), P219, DOI [10.1109/BLOC.2019.8751464, 10.1109/bloc.2019.8751464]
   Yu P, 2019, WIRELESS PERS COMMUN, V106, P719, DOI 10.1007/s11277-019-06187-3
   Yu Z, 2020, IEEE ACCESS, V8, P13640, DOI 10.1109/ACCESS.2020.2966428
   Zhang JW, 2022, IEEE INTERNET THINGS, V9, P10446, DOI 10.1109/JIOT.2021.3122949
NR 73
TC 1
Z9 1
U1 4
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2024
VL 20
IS 7
SI SI
AR 191
DI 10.1145/3597641
PG 25
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL0Q6
UT WOS:001234494100006
DA 2024-08-05
ER

PT J
AU Zhang, YY
   Zhang, S
   Hui, M
AF Zhang, Ying Ying
   Zhang, Shuo
   Hui, Ming
TI Semantic-Consistency-guided Learning on Deep Features for Unsupervised
   Salient Object Detection
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Multi-graph fusion learning; consistency similarity matrics;
   semantic-guided consistent graphs; semantic-enhanced cellular automata;
   large margin semi-supervised classification
ID NETWORK
AB Unsupervised salient object detection is an important task in many real-world scenarios where pixel-wise label information is of scarce availability. Despite its significance, this problem remains rarely explored, with a few works that consider unsupervised salient object detection methods based on the fused graph from the sum fusion of multiple deep feature similarity matrices. However, these methods ignore the interrelation of the low-level feature similarity matrices and the high-level semantic similarity matrice, which degrades the quality of the fused graph. In this article, we propose a semantic-consistency-guided multi-graph fusion learning algorithm for unsupervised saliency detection, where the consistency and inconsistency between multiple low-level feature similarity matrices and the high-level semantic similarity matrice are explored to promote the robustness and quality of the fused graph. In the first stage, a semantic-consistency-guided multi-graph fusion learning method is proposed to exploit consistency and inconsistency of multiple low-level deep features and the high-level semantic feature. The semantic-consistency-guided similarity matrices are computed for preliminary saliency ranking. In the following saliency refinement stage, the semantic-enhanced similarity matrices are built by the cross diffusion to fuse the multiple low-level deep features and the high semantic deep feature. Based on the semantic-enhanced similarity matrices, the refinement saliency maps are calculated in a semantic-enhanced cellular automata manner. Furthermore, the final ensemble stage of the large margin semi-supervised classification views the preliminary ranking results and refinement results as features, adopts the large margin graphs for saliency ensemble. Extensive evaluations over four benchmark datasets show that the proposed unsupervised method performs favorably against the state-of-the-art approaches and is competitive with some supervised deep learning-based methods.
C1 [Zhang, Ying Ying; Hui, Ming] Nanyang Normal Univ, Sch Phys & Elect Engn, Henan Engn Res Ctr Radio Frequency Front End & An, 1638 Wo Long Rd, Nan Yang 473061, Henan, Peoples R China.
   [Zhang, Shuo] Beijing Jiaotong Univ, Sch Comp & Informat Technol, 3 Shang yuan cun, Beijing, Peoples R China.
C3 Nanyang Normal College; Beijing Jiaotong University
RP Zhang, YY (corresponding author), Nanyang Normal Univ, Sch Phys & Elect Engn, Henan Engn Res Ctr Radio Frequency Front End & An, 1638 Wo Long Rd, Nan Yang 473061, Henan, Peoples R China.
EM zyyzs226@126.com; zhangshuo@bjtu.edu.cn; huimingsn@163.com
OI zhang, shuo/0000-0003-4622-0669
FU National Science Foundation of China [61702289]; Henan Province
   University Science and Technology Innovation Talent Support Program
   [No21HASTIT032]; scientific and technological project in Henan Province
   of China [No212102310304]; Cultivating Fund Project of the National
   Science Foundation [2023PY009]
FX This work was supported by the National Science Foundation of
   China(61702289), the Henan Province University Science and Technology
   Innovation Talent Support Program (grant No21HASTIT032), scientific and
   technological project in Henan Province of China (grant No212102310304)
   and the Cultivating Fund Project of the National Science Foundation
   (2023PY009).
CR Bai S, 2019, IEEE T IMAGE PROCESS, V28, P88, DOI 10.1109/TIP.2018.2863028
   Borji A, 2015, IEEE T IMAGE PROCESS, V24, P5706, DOI 10.1109/TIP.2015.2487833
   Borji A, 2012, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2012.6247711
   Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401
   Duan LJ, 2011, PROC CVPR IEEE, P473, DOI 10.1109/CVPR.2011.5995676
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fang CW, 2022, SCI CHINA INFORM SCI, V65, DOI 10.1007/s11432-021-3384-y
   Gong C, 2015, PROC CVPR IEEE, P2531, DOI 10.1109/CVPR.2015.7298868
   Hou QB, 2019, IEEE T PATTERN ANAL, V41, P815, DOI 10.1109/TPAMI.2018.2815688
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Ji YZ, 2021, INFORM SCIENCES, V546, P835, DOI 10.1016/j.ins.2020.09.003
   Jun Wei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13022, DOI 10.1109/CVPR42600.2020.01304
   Li GB, 2016, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2016.58
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li S, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2440755
   Li YN, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1725, DOI 10.1145/2939672.2939842
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Liang YW, 2019, IEEE DATA MINING, P1204, DOI 10.1109/ICDM.2019.00148
   Liu JJ, 2023, IEEE T PATTERN ANAL, V45, P887, DOI 10.1109/TPAMI.2021.3140168
   Liu MY, 2011, PROC CVPR IEEE
   Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80
   Liu RS, 2014, PROC CVPR IEEE, P3866, DOI 10.1109/CVPR.2014.494
   Liu Y, 2021, IEEE T IMAGE PROCESS, V30, P3804, DOI 10.1109/TIP.2021.3065239
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu HC, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2524198
   Luo ZM, 2017, PROC CVPR IEEE, P6593, DOI 10.1109/CVPR.2017.698
   Nie FP, 2018, IEEE T IMAGE PROCESS, V27, P1501, DOI 10.1109/TIP.2017.2754939
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178
   Qin Y, 2018, INT J COMPUT VISION, V126, P751, DOI 10.1007/s11263-017-1062-2
   Qin Y, 2015, PROC CVPR IEEE, P110, DOI 10.1109/CVPR.2015.7298606
   Ren ZX, 2014, IEEE T CIRC SYST VID, V24, P769, DOI 10.1109/TCSVT.2013.2280096
   Sugano Y, 2010, PROC CVPR IEEE, P2667, DOI 10.1109/CVPR.2010.5539984
   Wang B, 2012, PROC CVPR IEEE, P2997, DOI 10.1109/CVPR.2012.6248029
   Wang JD, 2017, INT J COMPUT VISION, V123, P251, DOI 10.1007/s11263-016-0977-3
   Wang TT, 2017, IEEE I CONF COMP VIS, P4039, DOI 10.1109/ICCV.2017.433
   Wang WG, 2022, IEEE T PATTERN ANAL, V44, P3239, DOI 10.1109/TPAMI.2021.3051099
   Wang WG, 2019, IEEE T PATTERN ANAL, V41, P1531, DOI 10.1109/TPAMI.2018.2840724
   Wang WG, 2018, IEEE T PATTERN ANAL, V40, P20, DOI 10.1109/TPAMI.2017.2662005
   Wang Y, 2017, IEEE T NEUR NET LEAR, V28, P57, DOI 10.1109/TNNLS.2015.2498149
   Wei YC, 2012, LECT NOTES COMPUT SC, V7574, P29, DOI 10.1007/978-3-642-33712-3_3
   Yan PX, 2022, AAAI CONF ARTIF INTE, P3000
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Zeng Y, 2018, IEEE T IMAGE PROCESS, V27, P4545, DOI 10.1109/TIP.2018.2838761
   Zhan K, 2019, IEEE T IMAGE PROCESS, V28, P1261, DOI 10.1109/TIP.2018.2877335
   Zhan K, 2018, IEEE T CYBERNETICS, V48, P2887, DOI 10.1109/TCYB.2017.2751646
   Zhang DW, 2022, SCI CHINA INFORM SCI, V65, DOI 10.1007/s11432-020-3181-9
   Zhang DW, 2020, IEEE T PATTERN ANAL, V42, P1755, DOI 10.1109/TPAMI.2019.2900649
   Zhang Dingwen, 2021, C NEUR INF PROC SYST
   Zhang Jing, 2020, ECCV, V16, P349, DOI DOI 10.1007/978-3-030-58520-4_21
   Zhang LH, 2018, IEEE T IMAGE PROCESS, V27, P987, DOI 10.1109/TIP.2017.2766787
   Zhang PP, 2017, IEEE I CONF COMP VIS, P202, DOI 10.1109/ICCV.2017.31
   Zhang PP, 2017, IEEE I CONF COMP VIS, P212, DOI 10.1109/ICCV.2017.32
   Zhang XN, 2018, PROC CVPR IEEE, P714, DOI 10.1109/CVPR.2018.00081
   Zhang YY, 2021, NEURAL NETWORKS, V142, P351, DOI 10.1016/j.neunet.2021.04.028
   Zhang YY, 2020, IEEE T IMAGE PROCESS, V29, P1536, DOI 10.1109/TIP.2019.2942796
   Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731
   Zhou HJ, 2023, IEEE T CIRC SYST VID, V33, P743, DOI 10.1109/TCSVT.2022.3203595
   Zhou Y, 2019, IEEE T MULTIMEDIA, V21, P74, DOI 10.1109/TMM.2018.2845667
   Zhou ZH, 2016, FRONT COMPUT SCI-CHI, V10, P589, DOI 10.1007/s11704-016-6906-3
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
   Zhuge WZ, 2017, IEEE T KNOWL DATA EN, V29, P2347, DOI 10.1109/TKDE.2017.2725263
NR 62
TC 0
Z9 0
U1 3
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2024
VL 20
IS 6
SI SI
AR 174
DI 10.1145/3640816
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OQ2T3
UT WOS:001208681800024
DA 2024-08-05
ER

PT J
AU Liu, Y
   Cui, GF
   Luo, JH
   Chang, XJ
   Yao, L
AF Liu, Yao
   Cui, Gangfeng
   Luo, Jiahui
   Chang, Xiaojun
   Yao, Lina
TI Two-stream Multi-level Dynamic Point Transformer for Two-person
   Interaction Recognition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Two-person interaction recognition; point cloud-based method; frame
   sampling; two-stream multi-level feature aggregation; transformer
AB As a fundamental aspect of human life, two-person interactions contain meaningful information about people's activities, relationships, and social settings. Human action recognition serves as the foundation for many smart applications, with a strong focus on personal privacy. However, recognizing two-person interactions poses more challenges due to increased body occlusion and overlap compared to single-person actions. In this article, we propose a point cloud-based network named Two-stream Multi-level Dynamic Point Transformer for two-person interaction recognition. Our model addresses the challenge of recognizing two-person interactions by incorporating local-region spatial information, appearance information, and motion information. To achieve this, we introduce a designed frame selection method named Interval Frame Sampling (IFS), which efficiently samples frames from videos, capturing more discriminative information in a relatively short processing time. Subsequently, a frame features learning module and a two-stream multi-level feature aggregation module extract global and partial features from the sampled frames, effectively representing the local-region spatial information, appearance information, and motion information related to the interactions. Finally, we apply a transformer to perform self-attention on the learned features for the final classification. Extensive experiments are conducted on two large-scale datasets, the interaction subsets of NTU RGB+D 60 and NTU RGB+D 120. The results show that our network outperforms state-of-the-art approaches in most standard evaluation settings.
C1 [Liu, Yao; Cui, Gangfeng] Univ New South Wales, Sch Comp Sci & Engn, Sydney, Australia.
   [Luo, Jiahui] Univ Melbourne, Sch Comp & Informat Syst, Melbourne, Australia.
   [Chang, Xiaojun] Univ Technol Sydney, Fac Engn & Informat Technol, Sydney, Australia.
   [Yao, Lina] Univ New South Wales, Data 61, CSIRO, Sydney, Australia.
   [Yao, Lina] Univ New South Wales, Sch Comp Sci & Engn, Sydney, Australia.
   [Liu, Yao] Univ New South Wales, Sch Comp Sci & Engn, Sydney, NSW 2052, Australia.
   [Luo, Jiahui] Univ Melbourne, Sch Comp & Informat Syst, Melbourne, Australia.
   [Chang, Xiaojun] Univ Technol Sydney, Fac Engn & Informat Technol, Sydney, NSW 2007, Australia.
   [Yao, Lina] Univ New South Wales, Data 61, CSIRO, Sydney, NSW 2015, Australia.
   [Yao, Lina] Univ New South Wales, Sch Comp Sci & Engn, Sydney, NSW 2015, Australia.
C3 University of New South Wales Sydney; University of Melbourne;
   University of Technology Sydney; University of New South Wales Sydney;
   Commonwealth Scientific & Industrial Research Organisation (CSIRO);
   University of New South Wales Sydney; University of New South Wales
   Sydney; University of Melbourne; University of Technology Sydney;
   University of New South Wales Sydney; Commonwealth Scientific &
   Industrial Research Organisation (CSIRO); University of New South Wales
   Sydney
RP Liu, Y (corresponding author), Univ New South Wales, Sch Comp Sci & Engn, Sydney, NSW 2052, Australia.
EM yao.liu3@unsw.edu.au; g.cui@student.unsw.edu.au;
   jialuo7@student.unimelb.edu.au; xiaojun.chang@uts.edu.au;
   lina.yao@unsw.edu.au
RI ; Chang, Xiaojun/A-2055-2015
OI Liu, Yao/0000-0002-5271-0536; Chang, Xiaojun/0000-0002-7778-8807
CR Bertasius G, 2021, PR MACH LEARN RES, V139
   Bloom V, 2016, COMPUT VIS IMAGE UND, V144, P62, DOI 10.1016/j.cviu.2015.12.001
   Chao-Lung Yang, 2020, 2020 IEEE International Conference on Image Processing (ICIP), P2166, DOI 10.1109/ICIP40778.2020.9190680
   Chiu S.Y., 2021, CAIP 2021 P 1 INT C, P56
   Dhiman C, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3441628
   Dosovitskiy Alexey, 2020, 9 INT C LEARN REPR I
   Fan HQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6804, DOI 10.1109/ICCV48922.2021.00675
   Fan HH, 2021, PROC CVPR IEEE, P14199, DOI 10.1109/CVPR46437.2021.01398
   Gao Feng, 2022, P IEEE INT C MULT EX, P1, DOI DOI 10.1109/ICME52920.2022.9859618
   Harjanto F, 2016, SIGNAL PROCESS, V124, P220, DOI 10.1016/j.sigpro.2015.08.006
   Huang M, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3177757
   Huang ZW, 2017, AAAI CONF ARTIF INTE, P2036
   Ito Y, 2022, IEEE IMAGE PROC, P231, DOI 10.1109/ICIP46576.2022.9897250
   Khan S, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3505244
   Kingma D. P., 2014, arXiv
   Li MS, 2019, PROC CVPR IEEE, P3590, DOI 10.1109/CVPR.2019.00371
   Li X, 2024, Arxiv, DOI arXiv:2111.08492
   Lin ZH, 2020, PROC CVPR IEEE, P1797, DOI 10.1109/CVPR42600.2020.00187
   Liu BL, 2019, PATTERN RECOGN, V94, P1, DOI 10.1016/j.patcog.2019.05.020
   Liu J, 2018, IEEE T IMAGE PROCESS, V27, P1586, DOI 10.1109/TIP.2017.2785279
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P2684, DOI 10.1109/TPAMI.2019.2916873
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P1453, DOI 10.1109/TPAMI.2019.2898954
   Liu J, 2017, PROC CVPR IEEE, P3671, DOI 10.1109/CVPR.2017.391
   Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50
   Liu Y, 2022, INT J INTELL SYST, V37, P12283, DOI 10.1002/int.23087
   Nguyen XS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13359, DOI 10.1109/ICCV48922.2021.01313
   Nils H. Shane, 2016, Deep, Convolutional, and Recurrent Models for Human Activity Recognition using wearables, V61, P454
   Perez M, 2022, IEEE T MULTIMEDIA, V24, P366, DOI 10.1109/TMM.2021.3050642
   Qi C. R., 2017, ADV NEURAL INFORM PR, P5099, DOI DOI 10.1109/CVPR.2017.16
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Sargano AB, 2017, APPL SCI-BASEL, V7, DOI 10.3390/app7010110
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Shi L, 2020, IEEE T IMAGE PROCESS, V29, P9532, DOI 10.1109/TIP.2020.3028207
   Shi L, 2019, PROC CVPR IEEE, P12018, DOI 10.1109/CVPR.2019.01230
   Shi QHY, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3485665
   Stergiou A, 2019, COMPUT VIS IMAGE UND, V188, DOI 10.1016/j.cviu.2019.102799
   Tang YS, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3472722
   Trabelsi R., 2017, P WORKSH MULT UND SO, P47
   van Gemeren C, 2014, LECT NOTES COMPUT SC, V8749, P101, DOI 10.1007/978-3-319-11839-0_9
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang PC, 2018, COMPUT VIS IMAGE UND, V171, P118, DOI 10.1016/j.cviu.2018.04.007
   Wang Q, 2022, IEEE T CIRC SYST VID, V32, P3603, DOI 10.1109/TCSVT.2021.3112214
   Wang YC, 2020, PROC CVPR IEEE, P508, DOI 10.1109/CVPR42600.2020.00059
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu HM, 2018, IEEE T HUM-MACH SYST, V48, P304, DOI 10.1109/THMS.2017.2776211
   Xia L, 2015, IEEE WINT CONF APPL, P357, DOI 10.1109/WACV.2015.54
   Xu CY, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3450410
   Xu HT, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3538749
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yun K., 2012, 2012 IEEE COMP SOC C, DOI DOI 10.1109/CVPRW.2012.6239234
   Zhang J, 2023, ALGORITHMS, V16, DOI 10.3390/a16040190
NR 51
TC 0
Z9 0
U1 4
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2024
VL 20
IS 5
AR 145
DI 10.1145/3639470
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MF3R9
UT WOS:001192177900025
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Shi, D
   Zhu, L
   Li, JJ
   Dong, GH
   Zhang, HX
AF Shi, Dan
   Zhu, Lei
   Li, Jingjing
   Dong, Guohua
   Zhang, Huaxiang
TI Incomplete Cross-Modal Retrieval with Deep Correlation Transfer
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Incomplete cross-modal retrieval; adjacency semantic correlation;
   robustness; graph attention
AB Most cross-modal retrieval methods assume the multi-modal training data is complete and has a one-to-one correspondence. However, in the real world, multi-modal data generally suffers from missing modality information due to the uncertainty of data collection and storage processes, which limits the practical application of existing cross-modal retrieval methods. Although some solutions have been proposed to generate the missing modality data using a single pseudo sample, this may lead to incomplete semantic restoration and suboptimal retrieval results due to the limited semantic information it provides. To address this challenge, this article proposes an Incomplete Cross-Modal Retrieval with Deep Correlation Transfer (ICMR-DCT) method that can robustly model incomplete multi-modal data and dynamically capture the adjacency semantic correlation for cross-modal retrieval. Specifically, we construct intra-modal graph attention-based auto-encoder to learn modality-invariant representations by performing semantic reconstruction through intra-modality adjacency correlation mining. Then, we design dual cross-modal alignment constraints to project multi-modal representations into a common semantic space, thus bridging the heterogeneous modality gap and enhancing the discriminability of the common representation. We further introduce semantic preservation to enhance adjacency semantic information and achieve cross-modal semantic correlation. Moreover, we propose a nearest-neighbor weighting integration strategy with cross-modal correlation transfer to generate the missing modality data according to inter-modality mapping relations and adjacency correlations between each sample and its neighbors, which improves the robustness of our method against incomplete multi-modal training data. Extensive experiments on three widely tested benchmark datasets demonstrate the superior performance of our method in cross-modal retrieval tasks under both complete and incomplete retrieval scenarios. Our used datasets and source codes are available at https://github.com/shidan0122/DCT.git.
C1 [Shi, Dan; Zhu, Lei; Zhang, Huaxiang] Shandong Normal Univ, Jinan 250358, Peoples R China.
   [Li, Jingjing] Univ Elect Sci & Technol China, Chengdu 611731, Peoples R China.
   [Dong, Guohua] Inst Basic Med Sci, Beijing 100850, Peoples R China.
C3 Shandong Normal University; University of Electronic Science &
   Technology of China
RP Zhu, L (corresponding author), Shandong Normal Univ, Jinan 250358, Peoples R China.
EM shidan0122@163.com; leizhu0608@gmail.com; lijin117@yeah.net;
   dgh1991.learn@gmail.com; huaxzhang@hotmail.com
RI Li, Jingjing/T-6522-2019; Zhu, Lei/GQQ-1130-2022
OI Zhu, Lei/0000-0002-5348-7532; zhang, hua xiang/0000-0001-6259-7533;
   Dong, Guohua/0000-0003-1955-3804; Zhu, Lei/0000-0002-2993-7142
FU National Natural Science Foundation of China [62172263, 62002209];
   Natural Science Foundation of Shandong Province [ZR2020QF042,
   ZR2020YQ47, ZR2020QF111]; Taishan Scholar Foundation of Shandong
   Province [ts20190924]; CCF-Baidu Open Fund [CCF-BAIDU OF2022008]
FX This work was supported in part by the National Natural Science
   Foundation of China under grants 62172263 and 62002209, in part by the
   Natural Science Foundation of Shandong Province under grants
   ZR2020QF042, ZR2020YQ47, and ZR2020QF111, in part by the Taishan Scholar
   Foundation of Shandong Province under grant ts20190924, and in part by
   the CCF-Baidu Open Fund under grant CCF-BAIDU OF2022008.
CR Afyouni I, 2022, INFORM FUSION, V79, P279, DOI 10.1016/j.inffus.2021.10.013
   Andrienko G., 2013, Introduction, P1
   Bruna J., 2014, ABS13126203 CORR, P1, DOI DOI 10.48550/ARXIV.1312.6203
   Cao M., 2022, PROC 31 INT JOINT, P5410, DOI 10.24963/ijcai.2022/759
   Chen B, 2023, IEEE T PATTERN ANAL, V45, P1388, DOI 10.1109/TPAMI.2022.3165024
   Chen D, 2021, PROG PHOTOVOLTAICS, V29, P1180, DOI 10.1002/pip.3362
   Cheng YH, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3499027
   Chua TS, 2009, P ACM INT C IM VID R, P1
   Chung J, 2014, PREPRINT
   Pereira JC, 2014, IEEE T PATTERN ANAL, V36, P521, DOI 10.1109/TPAMI.2013.142
   Deng C, 2016, IEEE T MULTIMEDIA, V18, P208, DOI 10.1109/TMM.2015.2508146
   Feng FX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P7, DOI 10.1145/2647868.2654902
   Guo J, 2020, IEEE T IMAGE PROCESS, V29, P1344, DOI 10.1109/TIP.2019.2941858
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   Hou ZY, 2022, PROCEEDINGS OF THE 28TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, KDD 2022, P594, DOI 10.1145/3534678.3539321
   Hu P, 2021, PROC CVPR IEEE, P5399, DOI 10.1109/CVPR46437.2021.00536
   Hu ZK, 2019, ICMR'19: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P141, DOI 10.1145/3323873.3325041
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Jing MM, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3283, DOI 10.1145/3394171.3413676
   Li C, 2018, PROC CVPR IEEE, P4242, DOI 10.1109/CVPR.2018.00446
   Liu H, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1589, DOI 10.1145/3240508.3240684
   Lu X, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1129, DOI 10.1145/3343031.3350999
   Peng YX, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3284750
   Peng YX, 2018, IEEE T IMAGE PROCESS, V27, P5585, DOI 10.1109/TIP.2018.2852503
   Peng YX, 2018, IEEE T MULTIMEDIA, V20, P405, DOI 10.1109/TMM.2017.2742704
   Peng Yuxin, 2016, IJCAI, P3846
   Radford A, 2021, PR MACH LEARN RES, V139
   Rashtchian C., 2010, P NAACL HLT 2010 WOR, P139
   Rasiwasia N, 2014, JMLR WORKSH CONF PRO, V33, P823
   Rasiwasia Nikhil, 2010, P 18 ACM INT C MULT, P251
   Romero A., 2018, INT C LEARN REPR, P2920, DOI DOI 10.48550/ARXIV.1710.10903
   Semedo D, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P75, DOI 10.1145/3343031.3351030
   Sharma A, 2012, PROC CVPR IEEE, P2160, DOI 10.1109/CVPR.2012.6247923
   Shen XB, 2017, IEEE T CYBERNETICS, V47, P4275, DOI 10.1109/TCYB.2016.2606441
   Shraga R, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P1399, DOI 10.1145/3397271.3401120
   Do TT, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3314051
   Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326
   Wang JS, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P4300, DOI 10.1145/3503161.3548263
   Wang K., 2016, PREPRINT
   Wang QF, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3904
   Wang W, 2016, Proceedings of ICLR
   Wang ZC, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3340262
   Wu YL, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P825, DOI 10.1145/3240508.3240521
   Wu Y, 2017, PROC CVPR IEEE, P3984, DOI 10.1109/CVPR.2017.424
   Xu X, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P1419, DOI 10.1145/3397271.3401149
   Xu Y, 2023, IEEE T KNOWL DATA EN, V35, P741, DOI 10.1109/TKDE.2021.3079581
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yang EK, 2022, PROC CVPR IEEE, P7541, DOI 10.1109/CVPR52688.2022.00740
   Zeng Z., 2022, arXiv
   Zeng ZX, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5427, DOI 10.1145/3474085.3475670
   Zeng ZX, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1125, DOI 10.1145/3404835.3462867
   Zhang CY, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3412847
   Zhang W, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3279952
   Zhen LL, 2019, PROC CVPR IEEE, P10386, DOI 10.1109/CVPR.2019.01064
   Zhu L, 2024, IEEE T KNOWL DATA EN, V36, P239, DOI 10.1109/TKDE.2023.3282921
   Zhu L, 2020, IEEE T IMAGE PROCESS, V29, P4643, DOI 10.1109/TIP.2020.2974065
NR 56
TC 0
Z9 0
U1 7
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2024
VL 20
IS 5
AR 127
DI 10.1145/3637442
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MF3R9
UT WOS:001192177900007
DA 2024-08-05
ER

PT J
AU Liu, J
   Zhou, JT
   Wu, HW
   Sun, WW
   Tian, JY
AF Liu, Jun
   Zhou, Jiantao
   Wu, Haiwei
   Sun, Weiwei
   Tian, Jinyu
TI Generating Robust Adversarial Examples against Online Social Networks
   (OSNs)
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Adversarial examples; adversarial images; robustness; online social
   networks; deep neural networks
AB Online Social Networks (OSNs) have blossomed into prevailing transmission channels for images in the modern era. Adversarial examples (AEs) deliberately designed to mislead deep neural networks (DNNs) are found to be fragile against the inevitable lossy operations conducted by OSNs. As a result, the AEs would lose their attack capabilities after being transmitted over OSNs. In this work, we aim to design a new framework for generating robust AEs that can survive the OSN transmission; namely, the AEs before and after the OSN transmission both possess strong attack capabilities. To this end, we first propose a differentiable network termed SImulated OSN (SIO) to simulate the various operations conducted by an OSN. Specifically, the SIO network consists of two modules: (1) a differentiable JPEG layer for approximating the ubiquitous JPEG compression and (2) an encoder-decoder subnetwork for mimicking the remaining operations. Based upon the SIO network, we then formulate an optimization framework to generate robust AEs by enforcing model outputs with and without passing through the SIO to be both misled. Extensive experiments conducted over Facebook, WeChat and QQ demonstrate that our attack methods produce more robust AEs than existing approaches, especially under small distortion constraints; the performance gain in terms of Attack Success Rate (ASR) could be more than 60%. Furthermore, we build a public dataset containing more than 10,000 pairs of AEs processed by Facebook, WeChat or QQ, facilitating future research in the robust AEs generation. The dataset and code are available at https://github.com/csjunjun/RobustOSNAttack.git.
C1 [Liu, Jun; Zhou, Jiantao; Wu, Haiwei] Univ Macau, Fac Sci & Technol, Dept Comp & Informat Sci, State Key Lab Internet Things Smart City, Univ Ave, Taipa 999078, Macau, Peoples R China.
   [Sun, Weiwei] Alibaba Grp, 699 Wangshang Rd, Hangzhou 310052, Zhejiang, Peoples R China.
   [Tian, Jinyu] Macau Univ Sci & Technol, Sch Comp Sci & Engn, Fac Innovat Engn, Weilong Rd, Taipa 999078, Macau, Peoples R China.
C3 University of Macau; Alibaba Group; Macau University of Science &
   Technology
RP Zhou, JT (corresponding author), Univ Macau, Fac Sci & Technol, Dept Comp & Informat Sci, State Key Lab Internet Things Smart City, Univ Ave, Taipa 999078, Macau, Peoples R China.
EM yc07453@umac.mo; jtzhou@umac.mo; yc07912@umac.mo;
   sunweiwei.sww@alibaba-inc.com; jytian@must.edu.mo
OI Liu, Jun/0000-0003-1167-5727; Wu, Haiwei/0000-0001-8807-0254
CR [Anonymous], 2010, P ACM INT C MULT
   Athalye A, 2018, PR MACH LEARN RES, V80
   Brendel W., 2018, Decision-based adversarial attacks: Reliable attacks against black-box machine learning models
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Chatterjee B, 2019, 2019 16TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV 2019), P41, DOI 10.1109/CRV.2019.00014
   Chen Pin-Yu, 2017, P 10 ACM WORKSH ART, P15, DOI [DOI 10.1145/3128572.3140448, 10.1145/3128572.3140448]
   Chen XY, 2021, IEEE T CIRC SYST VID, V31, P715, DOI 10.1109/TCSVT.2020.2987465
   Cozzolino D, 2020, IEEE T INF FOREN SEC, V15, P144, DOI 10.1109/TIFS.2019.2916364
   Dong YP, 2018, PROC CVPR IEEE, P9185, DOI 10.1109/CVPR.2018.00957
   Endo K, 2021, IEEE T CIRC SYST VID, V31, P4046, DOI 10.1109/TCSVT.2020.3045659
   Eykholt K, 2018, PROC CVPR IEEE, P1625, DOI 10.1109/CVPR.2018.00175
   Goodfellow IJ, 2015, P 3 INT C LEARNING R
   Hu H, 2018, IEEE T CIRC SYST VID, V28, P759, DOI 10.1109/TCSVT.2016.2620152
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang QY, 2019, I S BIOMED IMAGING, P1622, DOI [10.1109/isbi.2019.8759423, 10.1109/ISBI.2019.8759423]
   Ishida T, 2018, ADV NEUR IN, V31
   Jia XJ, 2019, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2019.00624
   Kansal P, 2019, IEEE INT CONF COMP V, P3688, DOI 10.1109/ICCVW.2019.00456
   Kingma D. P., 2015, P INT C LEARN REPR, P1
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Liu LQ, 2016, IEEE T MULTIMEDIA, V18, P64, DOI 10.1109/TMM.2015.2500730
   Liu ZH, 2019, PROC CVPR IEEE, P860, DOI 10.1109/CVPR.2019.00095
   Luo B, 2018, AAAI CONF ARTIF INTE, P1652
   Luo JB, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1953, DOI 10.1145/3123266.3130143
   Ma BY, 2021, NEURAL COMPUT APPL, V33, P5793, DOI 10.1007/s00521-020-05358-9
   Madry A., 2018, INT C LEARN REPR, P1
   Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282
   Naseer M, 2020, PROC CVPR IEEE, P259, DOI 10.1109/CVPR42600.2020.00034
   Nathan S, 2019, IEEE COMPUT SOC CONF, P1181, DOI 10.1109/CVPRW.2019.00156
   Nie L., 2022, Learning from Multiple Social Networks
   Niu GL, 2014, IEEE T MULTIMEDIA, V16, P2025, DOI 10.1109/TMM.2014.2340133
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Qin XH, 2022, IEEE T CIRC SYST VID, V32, P5110, DOI 10.1109/TCSVT.2022.3148406
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rony J, 2019, PROC CVPR IEEE, P4317, DOI 10.1109/CVPR.2019.00445
   Roy AG, 2019, IEEE T MED IMAGING, V38, P540, DOI 10.1109/TMI.2018.2867261
   Roy AG, 2018, LECT NOTES COMPUT SC, V11070, P421, DOI 10.1007/978-3-030-00928-1_48
   Shi Mengte, 2021, P IEEE INT C MULT EX, P1
   Shin Richard, 2017, P NEUR INF PROC SYST, P1
   Song XM, 2016, ACM T INFORM SYST, V34, DOI 10.1145/2832907
   Sun W., 2016, P 24 ACM INT C MULT, P581, DOI DOI 10.1145/2964284.2967288
   Sun WW, 2021, IEEE T IMAGE PROCESS, V30, P6292, DOI 10.1109/TIP.2021.3093794
   Sun WW, 2021, IEEE T CIRC SYST VID, V31, P1208, DOI 10.1109/TCSVT.2020.2998476
   Sun W, 2023, IEEE T CIRC SYST VID, V33, P1069, DOI 10.1109/TCSVT.2022.3210010
   Szegedy C, 2014, Arxiv, DOI arXiv:1312.6199
   Tang J, 2018, IEEE T MULTIMEDIA, V20, P1008, DOI 10.1109/TMM.2017.2760627
   Trzcinski T, 2017, IEEE T MULTIMEDIA, V19, P2561, DOI 10.1109/TMM.2017.2695439
   Tsai T, 2020, AAAI CONF ARTIF INTE, V34, P954
   Wang RK, 2021, Arxiv, DOI arXiv:2108.07033
   Wu HW, 2022, IEEE T INF FOREN SEC, V17, P443, DOI 10.1109/TIFS.2022.3144878
   Xie CY, 2018, IEEE PHOTONICS J, V10, DOI 10.1109/JPHOT.2018.2809731
   Xing YZ, 2021, PROC CVPR IEEE, P6283, DOI 10.1109/CVPR46437.2021.00622
   Xu WL, 2018, 25TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2018), DOI 10.14722/ndss.2018.23198
   Yatsura Maksym, 2021, P INT C NEUR INF PRO, V34, P1
   You QZ, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1445, DOI 10.1145/2964284.2971475
   Zhang HW, 2021, IEEE T INF FOREN SEC, V16, P701, DOI 10.1109/TIFS.2020.3021899
   Zhang Jiaming, 2023, Information Sciences, V2023
   Zhang JW, 2023, IEEE T CIRC SYST VID, V33, P562, DOI 10.1109/TCSVT.2022.3207008
   Zhang Y, 2020, IEEE T CIRC SYST VID, V30, P2750, DOI 10.1109/TCSVT.2019.2923980
   Zhao Sicheng, 2016, P 24 ACM INT C MULT, P1385
   Zhibo Wang, 2020, Mobihoc '20: Proceedings of the Twenty-First International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing, P81, DOI 10.1145/3397166.3409141
   Zhu ZY, 2023, IEEE T CIRC SYST VID, V33, P3017, DOI 10.1109/TCSVT.2022.3224243
NR 62
TC 0
Z9 0
U1 8
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2024
VL 20
IS 4
AR 98
DI 10.1145/3632528
PG 26
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6G9
UT WOS:001153382100008
OA Green Submitted, Bronze
DA 2024-08-05
ER

PT J
AU Cheung, M
AF Cheung, Ming
TI Learning from the Past: Fast NAS for Tasks and Datasets
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Auto-ML; machine learning; e-commerce
ID CENTRALITY
AB Nowadays, with the advancement of technology, many retail companies require in-house data scientist teams to build machine learning tasks, such as user segmentation and item price prediction. These teams typically use a trial-and-error process to obtain a good model for a given dataset and machine learning task, which is time-consuming and requires expertise. However, the team may have built models for other tasks on different datasets. This article proposes a framework to obtain a model architecture using the previous solved machine learning tasks and datasets. By analyzing real datasets with over 70,000 images from 11 online retail e-commerce websites, it is demonstrated that the performance of a model is related to the similarity among datasets, models, and machine learning tasks. A framework is hence proposed to obtain the model using the similarities among them. It was proven that the model was 26.6% better in accuracy, and using only 20% of the runtime while comparing to an auto network architecture search library, Auto-Keras, in predicting the attributes of fashion images. To the best of our knowledge, this is the first article to obtain the best model based on the similarity among machine learning tasks, models, and datasets.
C1 [Cheung, Ming] Lane Crawford Joyce Grp, Beta Labs HK, 2 Heung Yip Rd, Hong Kong 43017, Peoples R China.
RP Cheung, M (corresponding author), Lane Crawford Joyce Grp, Beta Labs HK, 2 Heung Yip Rd, Hong Kong 43017, Peoples R China.
EM mingcheung@lcjgroup.com
CR Achille A, 2019, IEEE I CONF COMP VIS, P6439, DOI 10.1109/ICCV.2019.00653
   Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746
   Bisong E., 2019, Building Machine Learning and Deep Learning Models on Google Cloud Platform, DOI DOI 10.1007/978-1-4842-4470-8
   Bottou Leon, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P421, DOI 10.1007/978-3-642-35289-8_25
   Brock A., 2018, P INT C LEARN REPR
   Chen X, 2015, PROC CVPR IEEE, P2422, DOI 10.1109/CVPR.2015.7298856
   Cheung M, 2020, IEEE T MULTIMEDIA, V22, P407, DOI 10.1109/TMM.2019.2930043
   Cheung M, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3095077
   Cheung M, 2015, IEEE T MULTIMEDIA, V17, P1417, DOI 10.1109/TMM.2015.2460192
   Cheung Ming., 2022, P 4 WORLD S SOFTW EN
   Creswell A, 2018, IEEE SIGNAL PROC MAG, V35, P53, DOI 10.1109/MSP.2017.2765202
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Glance, 2012, P 21 INT C WORLD WID, DOI [10.1145/2187836.2187863, DOI 10.1145/2187836.2187863]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He X, 2021, KNOWL-BASED SYST, V212, DOI 10.1016/j.knosys.2020.106622
   Heidari M, 2020, INT CONF DAT MIN WOR, P480, DOI 10.1109/ICDMW51313.2020.00071
   Hong WJ, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2326
   Hu X, 2021, KNOWL INF SYST, V63, P2585, DOI 10.1007/s10115-021-01605-0
   Huang L, 2019, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2019.00473
   Hwangbo H, 2018, ELECTRON COMMER R A, V28, P94, DOI 10.1016/j.elerap.2018.01.012
   Jin HF, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1946, DOI 10.1145/3292500.3330648
   Jin XB, 2022, MATHEMATICS-BASEL, V10, DOI 10.3390/math10040610
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Landherr A, 2010, BUS INFORM SYST ENG+, V2, P371, DOI 10.1007/s12599-010-0127-3
   Li JA, 2017, PROC CVPR IEEE, P1951, DOI 10.1109/CVPR.2017.211
   Liang XL, 2021, Arxiv, DOI arXiv:2101.11896
   Liu HaiJing Liu HaiJing, 2018, The Proceedings of the Fifteenth Congress of China Sheep Industry Development Sponsored by the China Animal Husbandry Association in 2018, Henan, China, 10-11 October, 2018, P13
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Maestre R, 2019, ADV INTELL SYST COMP, V868, P120, DOI 10.1007/978-3-030-01054-6_8
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Moriya S, 2018, P INT COMP SOFTW APP, P153, DOI 10.1109/COMPSAC.2018.10220
   Pouyanfar S, 2019, ACM COMPUT SURV, V51, DOI 10.1145/3234150
   Shen ZQ, 2024, Arxiv, DOI [arXiv:1810.13306, DOI 10.48550/ARXIV.1810.13306]
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Snoek J., 2012, ADV NEURAL INFORM PR, DOI DOI 10.48550/ARXIV.1206.2944
   Sun Y, 2015, Arxiv, DOI arXiv:1502.00873
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   van Rijn JN, 2015, LECT NOTES COMPUT SC, V9385, P298, DOI 10.1007/978-3-319-24465-5_26
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang BC, 2020, Arxiv, DOI arXiv:2011.01507
   Wang KIK, 2022, IEEE T IND INFORM, V18, P4088, DOI 10.1109/TII.2021.3088057
   Weiss Karl, 2016, Journal of Big Data, V3, DOI 10.1186/s40537-016-0043-6
   Zhang JL, 2017, ADV INTEL SYS RES, V132, P300
   Zoph B., 2017, Neural architecture search with reinforcement learning
NR 44
TC 0
Z9 0
U1 5
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2024
VL 20
IS 3
AR 70
DI 10.1145/3618000
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6F8
UT WOS:001153381000010
DA 2024-08-05
ER

PT J
AU Lei, SC
   Gong, YJ
   Xiao, XL
   Zhou, YC
   Zhang, J
AF Lei, Si-Chao
   Gong, Yue-Jiao
   Xiao, Xiao-Lin
   Zhou, Yi-Cong
   Zhang, Jun
TI Boosting Diversity in Visual Search with Pareto Non-Dominated Re-Ranking
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Visual image search; re-ranking; Pareto optimality; image diversity
ID SOCIAL IMAGE RETRIEVAL; RESULT DIVERSIFICATION; RELEVANCE
AB The field of visual search has gained significant attention recently, particularly in the context of web search engines and e-commerce product search platforms. However, the abundance of web images presents a challenge for modern image retrieval systems, as they need to find both relevant and diverse images that maximize users' satisfaction. In response to this challenge, we propose a non-dominated visual diversity re-ranking (NDVDR) method based on the concept of Pareto optimality. To begin with, we employ a fast binary hashing method as a coarse-grained retrieval procedure. This allows us to efficiently obtain a subset of candidate images for subsequent re-ranking. Fed with this initial retrieved image results, the NDVDR performs a fine-grained re-ranking procedure for boosting both relevance and visual diversity among the top-ranked images. Recognizing the inherent conflict nature between the objectives of relevance and diversity, the re-ranking procedure is simulated as the analytical stage of a multi-criteria decision-making process, seeking the optimal tradeoff between the two conflicting objectives within the initial retrieved images. In particular, a non-dominated sorting mechanism is devised that produces Pareto non-dominated hierarchies among images based on the Pareto dominance relation. Additionally, two novel measures are introduced for the effective characterization of the relevance and diversity scores among different images. We conduct experiments on three popular real-world image datasets and compare our re-ranking method with several state-of-the-art image search re-ranking methods. The experimental results validate that our re-ranking approach guarantees retrieval accuracy while simultaneously boosting diversity among the top-ranked images.
C1 [Lei, Si-Chao; Gong, Yue-Jiao] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510000, Peoples R China.
   [Gong, Yue-Jiao] Univ Elect Sci & Technol China, Shanghai, Peoples R China.
   [Xiao, Xiao-Lin] South China Normal Univ, Sch Comp Sci, Guangzhou 51000, Peoples R China.
   [Zhou, Yi-Cong] Univ Macau, Dept Comp & Informat Sci, Taipa 999078, Macao, Peoples R China.
   [Zhang, Jun] Hanyang Univ ERICA, Ansan 15588, South Korea.
   [Zhang, Jun] Nankai Univ, Tianjin 30071, Peoples R China.
C3 South China University of Technology; University of Electronic Science &
   Technology of China; South China Normal University; University of Macau;
   Hanyang University; Nankai University
RP Gong, YJ (corresponding author), South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510000, Peoples R China.; Zhang, J (corresponding author), Hanyang Univ ERICA, Ansan 15588, South Korea.; Zhang, J (corresponding author), Nankai Univ, Tianjin 30071, Peoples R China.
EM cssclei@outlook.com; gongyuejiao@gmail.com; shellyxiaolin@gmail.com;
   yicongzhou@um.edu.mo; junzhang@ieee.org
RI Xiao, Xiangli/JQJ-2169-2023; Zhang, Jun/E-9359-2011; Zhou,
   Yicong/A-8017-2009
OI Xiao, Xiangli/0000-0002-3250-0603; Zhang, Jun/0000-0001-7835-9871; Lei,
   Sichao/0000-0001-8684-4092; Zhou, Yicong/0000-0002-4487-6384
FU National Natural Science Foundation of China [62276100]; Guangdong
   Natural Science Funds for Distinguished Young Scholars
   [2022B1515020049]; Guangdong Regional Joint Fund for Basic and Applied
   Research [2021B1515120078]; TCL Young Scholars Program; National
   Research Foundation of Korea [NRF2022H1D3A2A01093478]
FX This work was supported in part by the National Natural Science
   Foundation of China under grant 62276100, in part by the Guangdong
   Natural Science Funds for Distinguished Young Scholars under grant
   2022B1515020049, in part by the Guangdong Regional Joint Fund for Basic
   and Applied Research under grant 2021B1515120078, in part by the TCL
   Young Scholars Program, and in part by the National Research Foundation
   of Korea under grant NRF2022H1D3A2A01093478.
CR Benavent X, 2019, FUTURE GENER COMP SY, V100, P250, DOI 10.1016/j.future.2019.05.029
   Boato G, 2016, MULTIMED TOOLS APPL, V75, P5581, DOI 10.1007/s11042-015-2526-4
   Bogdan Boteanu, 2016, P 2016 MEDIAEVAL WOR
   Boteanu B, 2017, MULTIMED TOOLS APPL, V76, P11889, DOI 10.1007/s11042-016-3678-6
   Boteanu Bogdan, 2014, P 2014 MEDIAEVAL WOR
   Bouchakwa M, 2020, PROG ARTIF INTELL, V9, P1, DOI 10.1007/s13748-019-00195-x
   Bouhlel N, 2020, MULTIMED TOOLS APPL, V79, P30257, DOI 10.1007/s11042-020-09418-z
   Bouhlel N, 2020, INT J MULTIMED INF R, V9, P205, DOI 10.1007/s13735-019-00191-w
   Bouhlel N, 2017, LECT NOTES COMPUT SC, V10424, P279, DOI 10.1007/978-3-319-64689-3_23
   Calumby RT, 2017, NEUROCOMPUTING, V259, P159, DOI 10.1016/j.neucom.2016.08.129
   Castellanos Angel, 2016, P 2016 MEDIAEVAL C
   CENSOR Y, 1977, APPL MATH OPT, V4, P41, DOI 10.1007/BF01442131
   Chen CF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P347, DOI 10.1109/ICCV48922.2021.00041
   Chen Y, 2020, IEEE T IMAGE PROCESS, V29, P3596, DOI 10.1109/TIP.2020.2963952
   Chua TS, 2009, P ACM INT C IM VID R, P1
   Dosovitskiy A., 2021, ICLR
   Dang-Nguyen DT, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3103613
   Escalante Hugo Jair, 2013, P 2013 MEDIAEVAL WOR
   Fan HQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6804, DOI 10.1109/ICCV48922.2021.00675
   Goynuk Burak, 2020, Advances in Information Retrieval. 42nd European Conference on IR Research, ECIR 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12036), P158, DOI 10.1007/978-3-030-45442-5_20
   Griffin G., 2007, Caltech-256 object category dataset
   Hu HD, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P359, DOI 10.1145/3219819.3219843
   Ionescu B, 2021, IEEE T MULTIMEDIA, V23, P677, DOI 10.1109/TMM.2020.2986579
   Ionescu B, 2016, MULTIMED TOOLS APPL, V75, P1301, DOI 10.1007/s11042-014-2369-4
   Jing PG, 2018, NEUROCOMPUTING, V274, P50, DOI 10.1016/j.neucom.2016.05.085
   Karako C, 2018, UMAP'18: ADJUNCT PUBLICATION OF THE 26TH CONFERENCE ON USER MODELING, ADAPTATION AND PERSONALIZATION, P23, DOI 10.1145/3213586.3226206
   Koutaki G, 2018, IEEE T IMAGE PROCESS, V27, P5378, DOI 10.1109/TIP.2018.2855427
   Krizhevskyand Alex, 2009, Learning Multiple Layers of Features from Tiny Images
   Ksibi A, 2014, INT J MULTIMED INF R, V3, P29, DOI 10.1007/s13735-013-0045-5
   Lei Si-Chao, 2023, Multimed. Tools Appl., V82, P1
   Figueredo JSL, 2022, MULTIMED TOOLS APPL, V81, P42991, DOI 10.1007/s11042-022-13050-4
   Lin WC, 2019, IEEE ACCESS, V7, P147553, DOI 10.1109/ACCESS.2019.2942142
   Lu W, 2019, INFORM SCIENCES, V502, P59, DOI 10.1016/j.ins.2019.06.020
   Milbich T., 2020, ECCV, P590
   Mithun NC, 2020, IEEE T CIRC SYST VID, V30, P1147, DOI 10.1109/TCSVT.2019.2898899
   Ouyang JB, 2021, IEEE T MULTIMEDIA, V23, P3646, DOI 10.1109/TMM.2020.3029886
   Peng Liang, 2017, P 2017 CEUR WORKSHOP
   Qian XM, 2017, IEEE T IMAGE PROCESS, V26, P3734, DOI 10.1109/TIP.2017.2699623
   Radu Anca-Livia, 2014, MultiMedia Modeling. 20th Anniversary International Conference, MMM 2014. Proceedings: LNCS 8325, P25, DOI 10.1007/978-3-319-04114-8_3
   Rao V, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P189, DOI 10.1145/2911996.2911998
   Santos RLT, 2015, FOUND TRENDS INF RET, V9, P1, DOI 10.1561/1500000040
   Seddati Omar, 2017, P 2017 MEDIAEVAL WOR
   Shen X., 2021, P ADV NEUR INF PROC, V34, P25932
   Spyromitros-Xioufis E, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P323, DOI 10.1145/2671188.2749334
   Tang Y., 2021, ADV NEURAL INF PROCE, V34, P15316
   Tollari Sabrina, 2016, P 2016 MEDIAEVAL WOR
   Vandersmissen Baptist, 2014, P 2014 MEDIAEVAL WOR
   vanLeuken Reinier H., 2009, P 18 INT C WORLD WID, P341, DOI DOI 10.1145/1526709.1526756
   Wang L, 2020, IEEE T CIRC SYST VID, V30, P4929, DOI 10.1109/TCSVT.2019.2959875
   Wang M, 2010, IEEE T MULTIMEDIA, V12, P829, DOI 10.1109/TMM.2010.2055045
   Wu ZJ, 2019, ACM T INFORM SYST, V37, DOI 10.1145/3320118
   Yan Y, 2017, MULTIMEDIA SYST, V23, P41, DOI 10.1007/s00530-014-0419-4
   Yanagi R, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3485042
   Zeng YW, 2023, IEEE T NEUR NET LEAR, V34, P10528, DOI 10.1109/TNNLS.2022.3168431
   Zhang YH, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P993, DOI 10.1145/3219819.3219820
   Zhong Ji, 2011, Proceedings of the Sixth International Conference on Image and Graphics (ICIG 2011), P981, DOI 10.1109/ICIG.2011.113
NR 56
TC 0
Z9 0
U1 6
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2024
VL 20
IS 3
AR 79
DI 10.1145/3625296
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6F8
UT WOS:001153381000019
DA 2024-08-05
ER

PT J
AU Padhy, RP
   Sa, PK
   Narducci, F
   Bisogni, C
   Bakshi, S
AF Padhy, Ram Prasad
   Sa, Pankaj Kumar
   Narducci, Fabio
   Bisogni, Carmen
   Bakshi, Sambit
TI Monocular Vision-aided Depth Measurement from RGB Images for Autonomous
   UAV Navigation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Monocular vision; RGB-D vision; scene understanding; UAV; depth
   measurement; obstacle avoidance; autonomous navigation; SURF
ID OBSTACLE AVOIDANCE; AERIAL VEHICLES; GPS; RECONSTRUCTION; FLIGHT
AB Monocular vision-based 3D scene understanding has been an integral part of many machine vision applications. Always, the objective is to measure the depth using a single RGB camera, which is at par with the depth cameras. In this regard, monocular vision-guided autonomous navigation of robots is rapidly gaining popularity among the research community. We propose an effective monocular vision-assisted method to measure the depth of an Unmanned Aerial Vehicle (UAV) from an impending frontal obstacle. This is followed by collision-free navigation in unknown GPS-denied environments. Our approach deals upon the fundamental principle of perspective vision that the size of an object relative to its field of view(FoV) increases as the center of projection moves closer towards the object. Our contribution involves modeling the depth followed by its realization through scale-invariant SURF features. Noisy depth measurements arising due to external wind, or the turbulence in the UAV, are rectified by employing a constant velocity-based Kalman filter model. Necessary control commands are then designed based on the rectified depth value to avoid the obstacle before collision. Rigorous experiments with SURF scale-invariant features reveal an overall accuracy of 88.6% with varying obstacles, in both indoor and outdoor environments.
C1 [Padhy, Ram Prasad] Indian Inst Informat Technol Design & Mfg IIITDM, Kancheepuram, India.
   [Sa, Pankaj Kumar; Bakshi, Sambit] Natl Inst Technol Rourkela, Rourkela 769008, India.
   [Narducci, Fabio; Bisogni, Carmen] Univ Salerno, Fisciano, Italy.
C3 Indian Institute of Information Technology, Design & Manufacturing,
   Kancheepuram; National Institute of Technology (NIT System); National
   Institute of Technology Rourkela; University of Salerno
RP Padhy, RP (corresponding author), Indian Inst Informat Technol Design & Mfg IIITDM, Kancheepuram, India.
EM ramprasad.nitr@gmail.com; pankajksa@nitrkl.ac.in; fnarducci@unisa.it;
   cbisogni@unisa.it; sambitbaksi@gmail.com
RI Bakshi, Sambit/JDC-3355-2023; Narducci, Fabio/R-5833-2017; K,
   Pankaj/A-9362-2017
OI Bakshi, Sambit/0000-0002-6107-114X; Narducci, Fabio/0000-0003-4879-7138;
   Bisogni, Carmen/0000-0003-1358-006X
FU NITROAA; NVIDIA Corporation; Start-up Research Grant (SRG) - Science &
   Engineering Research Board (SERB), Department of Science and Technology
   (DST), Government of India [SRG/2021/002399]
FX This research work has been partially supported by: NITROAA with support
   of Lenovo P920 and Dell Inception 7820 workstations; NVIDIA Corporation
   with support of NVIDIA Titan V and Quadro RTX 8000 GPUs; Grant Number
   SRG/2021/002399, Start-up Research Grant (SRG), funded by Science &
   Engineering Research Board (SERB), Department of Science and Technology
   (DST), Government of India.
CR Abou Merhy B, 2008, IEEE T INSTRUM MEAS, V57, P2827, DOI 10.1109/TIM.2008.926048
   Achtelik Markus, 2011, IEEE International Conference on Robotics and Automation, P3056
   Bachrach A, 2011, J FIELD ROBOT, V28, P644, DOI 10.1002/rob.20400
   Ban XC, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3024011
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Chang XJ, 2017, IEEE T PATTERN ANAL, V39, P1617, DOI 10.1109/TPAMI.2016.2608901
   Chen MY, 2013, 2013 IEEE SYSTEMS AND INFORMATION ENGINEERING DESIGN SYMPOSIUM (SIEDS), P213, DOI 10.1109/SIEDS.2013.6549521
   Cui YJ, 2003, IEEE T ROBOTIC AUTOM, V19, P15, DOI 10.1109/TRA.2002.807557
   De Falco A, 2019, LECT NOTES COMPUT SC, V11751, P443, DOI 10.1007/978-3-030-30642-7_40
   Deng H, 2020, IEEE T INSTRUM MEAS, V69, P3139, DOI 10.1109/TIM.2019.2928615
   Efraim H, 2017, J INTELL ROBOT SYST, V87, P169, DOI 10.1007/s10846-017-0510-0
   Engel J, 2014, ROBOT AUTON SYST, V62, P1646, DOI 10.1016/j.robot.2014.03.012
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Foggia P, 2015, IEEE T CIRC SYST VID, V25, P1545, DOI 10.1109/TCSVT.2015.2392531
   Fontanelli D, 2014, IEEE T INSTRUM MEAS, V63, P826, DOI 10.1109/TIM.2013.2289091
   Gageik N, 2015, IEEE ACCESS, V3, P599, DOI 10.1109/ACCESS.2015.2432455
   Gao Z, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3377876
   Garvey S, 2020, IEEE T SYST MAN CY-S, V50, P3898, DOI 10.1109/TSMC.2018.2868751
   Grigorev A, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3360050
   Guo YL, 2015, IEEE T INSTRUM MEAS, V64, P683, DOI 10.1109/TIM.2014.2358131
   He GJ, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3024405
   He ZH, 2006, P AMER CONTR CONF, V1-12, P2166
   Hening S, 2017, AIAA INFORM SYSTEMS, P0448
   Lee T, 2021, DRONES-BASEL, V5, DOI 10.3390/drones5020052
   Lin HY, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3073687
   Lioulemes Alexandros., 2014, Proceedings of the 7th International Conference on PErvasive Technologies Related to Assistive Environments, page, P33
   Liu XB, 2018, IEEE T PATTERN ANAL, V40, P710, DOI 10.1109/TPAMI.2017.2689007
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Martínez-Carranza J, 2015, LECT NOTES ARTIF INT, V9414, P560, DOI 10.1007/978-3-319-27101-9_43
   Maurovic I, 2018, IEEE T SYST MAN CY-S, V48, P1321, DOI 10.1109/TSMC.2017.2668603
   Mistry D., 2017, GRD JOURNALSGLOBAL R, V2, P7
   Mori T, 2013, IEEE INT CONF ROBOT, P1750, DOI 10.1109/ICRA.2013.6630807
   Padhy RP, 2021, INT C PATT RECOG, P9423, DOI 10.1109/ICPR48806.2021.9412096
   Padhy RP, 2019, PATTERN RECOGN LETT, V127, P165, DOI 10.1016/j.patrec.2018.07.012
   Padhy RP, 2019, IEEE T SUST COMPUT, V4, P96, DOI 10.1109/TSUSC.2018.2810952
   Padhy RP, 2018, PROCEDIA COMPUT SCI, V133, P643, DOI 10.1016/j.procs.2018.07.099
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Sa I, 2013, IEEE ASME INT C ADV, P1355, DOI 10.1109/AIM.2013.6584283
   Saha S, 2014, 2014 IEEE INTERNATIONAL CONFERENCE ON AEROSPACE ELECTRONICS AND REMOTE SENSING TECHNOLOGY (ICARES), P189, DOI 10.1109/ICARES.2014.7024382
   Cruz GCS, 2012, J INTELL ROBOT SYST, V65, P203, DOI 10.1007/s10846-011-9587-z
   Sasongko RA, 2017, J INTELL ROBOT SYST, V88, P567, DOI 10.1007/s10846-017-0543-4
   Shirmohammadi S, 2014, IEEE INSTRU MEAS MAG, V17, P41, DOI 10.1109/MIM.2014.6825388
   Stubblebine Andrew, 2015, AIAA INFOTECH AEROSP, P2026
   Wang CQ, 2020, IEEE T INSTRUM MEAS, V69, P9853, DOI 10.1109/TIM.2020.3001816
   Wang CQ, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON INFORMATION AND AUTOMATION, P1674, DOI 10.1109/ICInfA.2015.7279555
   Yuan D, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3486678
NR 46
TC 3
Z9 3
U1 18
U2 29
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2024
VL 20
IS 2
SI SI
AR 37
DI 10.1145/3550485
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W6GR4
UT WOS:001092595800007
DA 2024-08-05
ER

PT J
AU Peng, F
   Qin, L
   Long, M
   Li, J
AF Peng, Fei
   Qin, Le
   Long, Min
   Li, Jin
TI Detection of Adversarial Facial Accessory Presentation Attacks Using
   Local Face Differential
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Face verification; presentation attacks; adversarial examples;
   presentation attack detection; differential
ID SHAPE
AB To counter adversarial facial accessory presentation attacks (PAs), a detection method based on local face differential is proposed in this article. It extracts the local face differential features from a suspected face image and a reference face image, and then adaptively fuses the differential features of different local face regions to detect adversarial facial accessory PAs. Meanwhile, the principle of the proposed method is explained by theoretically investigating the local facial differences between a bona fide presentation and an adversarial facial accessory PA when they are compared with a reference face image. To evaluate the proposed method, this article builds a database with different adversarial examples (AEs), presentation attack instruments (PAIs), illumination conditions, and cameras. The experimental results show that it can effectively distinguish between adversarial facial accessory PAs and bona fide presentations, and it has good generalization ability to unseen AEs, PAIs, illumination conditions, and cameras. Moreover, it outperforms the existing AE detection and presentation attack detection methods in detecting adversarial facial accessory PAs.
C1 [Peng, Fei; Qin, Le; Long, Min; Li, Jin] Guangzhou Univ, Guangzhou Higher Educ Mega Ctr, 230Wai Huan Xi Rd, Guangzhou 510006, Peoples R China.
C3 Guangzhou University
RP Qin, L; Long, M (corresponding author), Guangzhou Univ, Guangzhou Higher Educ Mega Ctr, 230Wai Huan Xi Rd, Guangzhou 510006, Peoples R China.
EM eepengf@gmail.com; ql@gzhu.edu.cn; caslongm@aliyun.com;
   jinli71@gmail.com
RI Peng, Fei/H-6951-2017
OI Peng, Fei/0000-0001-8053-4587
FU National Natural Science Foundation of China [62072055, 62302113,
   62372128]; Natural Science Foundation of Guangdong Province
   [2023A1515011575]
FX This work was supported in part by the National Natural Science
   Foundation of China (62072055, 62302113, 62372128) and the Natural
   Science Foundation of Guangdong Province under Grant 2023A1515011575.
CR Agarwal A, 2016, INT CONF BIOMETR THE
   Agarwal A, 2019, 2019 IEEE FIFTH INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM 2019), P373, DOI [10.1109/BigMM.2019.00018, 10.1109/BigMM.2019.00066]
   Arashloo SR, 2023, IEEE T INF FOREN SEC, V18, P1421, DOI 10.1109/TIFS.2023.3240841
   Benalcazar D, 2023, IEEE T INF FOREN SEC, V18, P1814, DOI 10.1109/TIFS.2023.3255585
   Birla L, 2023, IEEE T DEPEND SECURE, V20, P1927, DOI 10.1109/TDSC.2022.3168345
   Boulkenafet Z, 2016, IEEE T INF FOREN SEC, V11, P1818, DOI 10.1109/TIFS.2016.2555286
   Cai RZ, 2021, IEEE T INF FOREN SEC, V16, P937, DOI 10.1109/TIFS.2020.3026553
   Neto JBC, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3527158
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chen Changsheng, 2021, arXiv
   Deb D, 2021, IEEE T INF FOREN SEC, V16, P1143, DOI 10.1109/TIFS.2020.3029879
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Di Martino JM, 2021, IEEE T IMAGE PROCESS, V30, P1086, DOI 10.1109/TIP.2020.3042082
   Nguyen DL, 2020, IEEE COMPUT SOC CONF, P3548, DOI 10.1109/CVPRW50498.2020.00415
   Drozdowski Pawel, 2020, IEEE Transactions on Technology and Society, V1, P89, DOI 10.1109/TTS.2020.2992344
   Fang ML, 2022, PATTERN RECOGN, V123, DOI 10.1016/j.patcog.2021.108398
   George A, 2021, IEEE T INF FOREN SEC, V16, P361, DOI 10.1109/TIFS.2020.3013214
   Goswami G, 2019, INT J COMPUT VISION, V127, P719, DOI 10.1007/s11263-019-01160-w
   ICAO, 2015, Doc 9303 Machine Readable Travel Documents Seventh Edition-Part 9: Deployment of Biometric Identification and Electronic Storage of Data in MRTDs
   ISO/IEC JTC1 SC37 Biometrics, 2017, ISO/IEC 30107-3 Information Technology-Biometric presentation attack detection-Part 3: Testing and Reporting
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jia S, 2021, IEEE T CIRC SYST VID, V31, P4031, DOI 10.1109/TCSVT.2020.3044986
   Kaziakhmedov Edgar, 2019, 2019 International Multi-Conference on Engineering, Computer and Information Sciences (SIBIRCON). Proceedings, P0422, DOI 10.1109/SIBIRCON48586.2019.8958122
   King DE, 2009, J MACH LEARN RES, V10, P1755
   Komkov S, 2019, Arxiv, DOI arXiv:1908.08705
   Li HL, 2020, IEEE J-STSP, V14, P933, DOI 10.1109/JSTSP.2020.3001719
   Li HL, 2018, IEEE T INF FOREN SEC, V13, P1794, DOI 10.1109/TIFS.2018.2801312
   Li Y, 2018, IEEE T DEPEND SECURE, V15, P231, DOI 10.1109/TDSC.2016.2550459
   Li Y, 2015, CCS'15: PROCEEDINGS OF THE 22ND ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1558, DOI 10.1145/2810103.2813612
   Li YD, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3432817
   Liu AJ, 2022, IEEE T INF FOREN SEC, V17, P2497, DOI 10.1109/TIFS.2022.3188149
   Liu SQ, 2021, IEEE T INF FOREN SEC, V16, P2683, DOI 10.1109/TIFS.2021.3050060
   Liu W, 2017, ADV SOC SCI EDUC HUM, V99, P212
   Liu YJ, 2019, PROC CVPR IEEE, P4675, DOI 10.1109/CVPR.2019.00481
   Liu YJ, 2018, PROC CVPR IEEE, P389, DOI 10.1109/CVPR.2018.00048
   Majumdar P, 2019, IEEE COMPUT SOC CONF, P11, DOI 10.1109/CVPRW.2019.00008
   Parkhi O. M., 2015, P BRIT MACH VIS C, p41.1
   Peng F, 2020, J VIS COMMUN IMAGE R, V66, DOI 10.1016/j.jvcir.2019.102746
   Peng F, 2018, 2018 27TH INTERNATIONAL CONFERENCE ON COMPUTER COMMUNICATION AND NETWORKS (ICCCN)
   Peng F, 2018, MULTIMED TOOLS APPL, V77, P8883, DOI 10.1007/s11042-017-4780-0
   Pinto A, 2020, IEEE T INF FOREN SEC, V15, P3347, DOI 10.1109/TIFS.2020.2988168
   Qin L, 2022, ACM T PRIV SECUR, V25, DOI 10.1145/3491199
   Qin Le, 2020, IEEE Transactions on Biometrics, Behavior, and Identity Science
   Ramachandra R, 2017, ACM COMPUT SURV, V50, DOI 10.1145/3038924
   Rathgeb C, 2021, INT C PATT RECOG, P3443, DOI 10.1109/ICPR48806.2021.9413347
   Rathgeb C, 2020, IEEE ACCESS, V8, P224958, DOI 10.1109/ACCESS.2020.3044723
   Saha Sanjay, 2020, 2020 IEEE INT JOINT, P1
   Scherhag U, 2020, IEEE T INF FOREN SEC, V15, P3625, DOI 10.1109/TIFS.2020.2994750
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sharif M, 2019, ACM T PRIV SECUR, V22, DOI 10.1145/3317611
   Sharif M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1528, DOI 10.1145/2976749.2978392
   Shen M, 2021, IEEE T INF FOREN SEC, V16, P4063, DOI 10.1109/TIFS.2021.3102492
   Siddiqui TA, 2016, INT C PATT RECOG, P1035, DOI 10.1109/ICPR.2016.7899772
   Singh Maneet, 2019, IEEE Transactions on Biometrics, Behavior, and Identity Science, V1, P97, DOI 10.1109/TBIOM.2019.2903860
   Sun WY, 2020, IEEE T INF FOREN SEC, V15, P3181, DOI 10.1109/TIFS.2020.2985530
   Tang D, 2018, 25TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2018), DOI 10.14722/ndss.2018.23176
   Tian JY, 2021, AAAI CONF ARTIF INTE, V35, P9877
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang Dong, 2023, IEEE CVF C COMP VIS, P6379
   Wen D, 2015, IEEE T INF FOREN SEC, V10, P746, DOI 10.1109/TIFS.2015.2400395
   Wenger E, 2021, PROC CVPR IEEE, P6202, DOI 10.1109/CVPR46437.2021.00614
   Xu WY, 2022, PROCEEDINGS OF THE 2022 THE 28TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, ACM MOBICOM 2022, P310, DOI 10.1145/3495243.3560515
   Yu ZT, 2020, PROC CVPR IEEE, P5294, DOI 10.1109/CVPR42600.2020.00534
   Yu Zitong, 2021, INT JOINT C ART INT
   Zhang BW, 2020, COMPUT VIS IMAGE UND, V197, DOI 10.1016/j.cviu.2020.102988
   Zhang Y, 2023, PROCEEDINGS OF THE 2023 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, ICMR 2023, P144, DOI 10.1145/3591106.3592289
   Zheng Z, 2023, IEEE T INF FOREN SEC, V18, P1295, DOI 10.1109/TIFS.2022.3232957
   Zhou Z, 2018, Arxiv, DOI arXiv:1803.04683
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu X, 2021, IEEE T CIRC SYST VID, V31, P2039, DOI 10.1109/TCSVT.2019.2949868
NR 70
TC 0
Z9 0
U1 2
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2024
VL 20
IS 7
SI SI
AR 192
DI 10.1145/3643831
PG 28
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL0Q6
UT WOS:001234494100007
DA 2024-08-05
ER

PT J
AU Sun, BL
   Ye, XC
   Yan, TT
   Wang, ZH
   Li, HJ
   Wang, ZY
AF Sun, Baoli
   Ye, Xinchen
   Yan, Tiantian
   Wang, Zhihui
   Li, Haojie
   Wang, Zhiyong
TI Discriminative Segment Focus Network for Fine-grained Video Action
   Recognition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Fine-grained action recognition; discriminative segment; correlation
AB Fine-grained video action recognition aims at identifying minor and discriminative variations among fine categories of actions. While many recent action recognition methods have been proposed to better model spatio-temporal representations, how to model the interactions among discriminative atomic actions to effectively characterize inter-class and intra-class variations has been neglected, which is vital for understanding fine-grained actions. In this work, we devise a Discriminative Segment Focus Network (DSFNet) to mine the discriminability of segment correlations and localize discriminative action-relevant segments for fine-grained video action recognition. Firstly, we propose a hierarchic correlation reasoning (HCR) module which explicitly establishes correlations between different segments at multiple temporal scales and enhances each segment by exploiting the correlations with other segments. Secondly, a discriminative segment focus (DSF) module is devised to localize the most action-relevant segments fromthe enhanced representations of HCR by enforcing the consistency between the discriminability and the classification confidence of a given segment with a consistency constraint. Finally, these localized segment representations are combined with the global action representation of the whole video for boosting final recognition. Extensive experimental results on two fine-grained action recognition datasets, i.e., FineGym and Diving48, and two action recognition datasets, i.e., Kinetics400 and Something-Something, demonstrate the effectiveness of our approach compared with the state-of-the-art methods.
C1 [Sun, Baoli] Dalian Univ Technol, Dalian, Liaoning, Peoples R China.
   [Ye, Xinchen; Wang, Zhihui] Dalian Univ Technol, DUT RU Int Sch Informat Sci & Engn, Dalian, Liaoning, Peoples R China.
   [Yan, Tiantian] Dalian Univ, Natl & Local Joint Engn Lab Comp Aided Design, Dalian, Liaoning, Peoples R China.
   [Li, Haojie] Shandong Univ Sci & Technol, Coll Comp Sci & Engn, Qingdao, Shandong, Peoples R China.
   [Wang, Zhiyong] Univ Sydney, Sch Informat Technol, Sydney, NSW, Australia.
C3 Dalian University of Technology; Dalian University of Technology; Dalian
   University; Shandong University of Science & Technology; University of
   Sydney
RP Wang, ZH (corresponding author), Dalian Univ Technol, DUT RU Int Sch Informat Sci & Engn, Dalian, Liaoning, Peoples R China.
EM baoli@mail.dlut.edu.cn; yexch@dlut.edu.cn; yan_tiantian01@163.com;
   zhwang@dlut.edu.cn; hjli@sdust.edu.cn; zhiyong.wang@sydney.edu.au
OI Wang, Zhiyong/0000-0002-8043-0312; wang, zhihui/0000-0002-5011-9726;
   Sun, Baoli/0000-0002-2861-4288
CR Arnab A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6816, DOI 10.1109/ICCV48922.2021.00676
   Bertasius G, 2021, PR MACH LEARN RES, V139
   Bruna J., 2014, ABS13126203 CORR, P1, DOI DOI 10.48550/ARXIV.1312.6203
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Calagari K, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2890103
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen BA, 2024, ACM T MULTIM COMPUT, V20, DOI 10.1145/3597612
   Chen X, 2021, INT J COMPUT VISION, V129, P2846, DOI 10.1007/s11263-021-01486-4
   Defferrard M, 2016, ADV NEUR IN, V29
   Diba A, 2017, Arxiv, DOI [arXiv:1711.08200, DOI 10.48550/ARXIV.1711.08200]
   Diba A, 2018, LECT NOTES COMPUT SC, V11208, P299, DOI 10.1007/978-3-030-01225-0_18
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Gabeur Valentin, 2020, COMPUTER VISION ECCV, DOI [10.48550/arXiv.2007.10639, DOI 10.48550/ARXIV.2007.10639]
   Gao HY, 2022, IEEE T PATTERN ANAL, V44, P4948, DOI 10.1109/TPAMI.2021.3081010
   Gao Z, 2022, IEEE T NEUR NET LEAR, V33, P1147, DOI 10.1109/TNNLS.2020.3041018
   Geng P, 2023, IEEE T CIRC SYST VID, V33, P4754, DOI 10.1109/TCSVT.2023.3248782
   Girdhar R, 2019, PROC CVPR IEEE, P244, DOI 10.1109/CVPR.2019.00033
   Goyal R, 2017, IEEE I CONF COMP VIS, P5843, DOI 10.1109/ICCV.2017.622
   Hamilton WL, 2017, ADV NEUR IN, V30
   He DL, 2019, AAAI CONF ARTIF INTE, P8401
   Jiang BY, 2019, IEEE I CONF COMP VIS, P2000, DOI 10.1109/ICCV.2019.00209
   Jie ZQ, 2016, ADV NEUR IN, V29
   Kipf T.N., 2017, INT C LEARN REPR, P1
   Lei P, 2018, PROC CVPR IEEE, P6742, DOI 10.1109/CVPR.2018.00705
   Li Kunchang, 2023, P IEEE CVF ICCV, P5373
   Li T, 2022, LECT NOTES COMPUT SC, V13664, P386, DOI 10.1007/978-3-031-19772-7_23
   Li XP, 2019, AAAI CONF ARTIF INTE, P8658
   Li Y, 2020, PROC CVPR IEEE, P906, DOI 10.1109/CVPR42600.2020.00099
   Li YW, 2018, LECT NOTES COMPUT SC, V11210, P520, DOI 10.1007/978-3-030-01231-1_32
   Li ZY, 2018, COMPUT VIS IMAGE UND, V166, P41, DOI 10.1016/j.cviu.2017.10.011
   Liang S, 2024, ACM T MULTIM COMPUT, V20, DOI 10.1145/3617596
   Liang ZM, 2022, J VIS COMMUN IMAGE R, V89, DOI 10.1016/j.jvcir.2022.103667
   Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718
   Liu CB, 2020, AAAI CONF ARTIF INTE, V34, P11555
   Liu J, 2022, IEEE T NEUR NET LEAR, V33, P1609, DOI 10.1109/TNNLS.2020.3043002
   Liu TY, 2022, INFORM SCIENCES, V606, P864, DOI 10.1016/j.ins.2022.05.092
   Liu YX, 2023, IEEE T CIRC SYST VID, V33, P1043, DOI 10.1109/TCSVT.2022.3209007
   Luo CX, 2019, IEEE I CONF COMP VIS, P5511, DOI 10.1109/ICCV.2019.00561
   Ma YJ, 2024, PATTERN RECOGN, V145, DOI 10.1016/j.patcog.2023.109905
   Niepert M, 2016, PR MACH LEARN RES, V48
   Qiu S, 2023, INFORM SCIENCES, V633, P264, DOI 10.1016/j.ins.2023.03.058
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ringeval F, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3181711
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Schlichtkrull M, 2018, LECT NOTES COMPUT SC, V10843, P593, DOI 10.1007/978-3-319-93417-4_38
   Shao D, 2020, PROC CVPR IEEE, P2613, DOI 10.1109/CVPR42600.2020.00269
   Shao D, 2020, PROC CVPR IEEE, P727, DOI 10.1109/CVPR42600.2020.00081
   Song LY, 2021, INT J COMPUT VISION, V129, P681, DOI 10.1007/s11263-020-01397-w
   Stroud JC, 2020, IEEE WINT CONF APPL, P614, DOI 10.1109/wacv45572.2020.9093274
   Sun BL, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P4779, DOI 10.1145/3503161.3548046
   Sun C, 2019, IEEE I CONF COMP VIS, P7463, DOI 10.1109/ICCV.2019.00756
   Tian Y, 2022, INT J COMPUT VISION, V130, P2453, DOI 10.1007/s11263-022-01661-1
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   Vaswani A., 2017, Advances in neural information processing systems, P5998
   Wang HR, 2024, ACM T MULTIM COMPUT, V20, DOI 10.1145/3603253
   Wang LM, 2021, PROC CVPR IEEE, P1895, DOI 10.1109/CVPR46437.2021.00193
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang R, 2022, PROC CVPR IEEE, P14713, DOI 10.1109/CVPR52688.2022.01432
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang ZH, 2020, AAAI CONF ARTIF INTE, V34, P12289
   Wu CY, 2019, PROC CVPR IEEE, P284, DOI 10.1109/CVPR.2019.00037
   Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19
   Xu HT, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3538749
   Yan S, 2022, PROC CVPR IEEE, P3323, DOI 10.1109/CVPR52688.2022.00333
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yang CY, 2020, PROC CVPR IEEE, P588, DOI 10.1109/CVPR42600.2020.00067
   Yang Z, 2018, LECT NOTES COMPUT SC, V11218, P438, DOI 10.1007/978-3-030-01264-9_26
   Yin JB, 2023, IEEE T PATTERN ANAL, V45, P9822, DOI 10.1109/TPAMI.2021.3125981
   Zhang B, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3449359
   Zhang CH, 2021, PROC CVPR IEEE, P4484, DOI 10.1109/CVPR46437.2021.00446
   Zhang YY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13557, DOI [10.1109/ICCV48922.2021.01332, 10.1109/iccv48922.2021.01332]
   Zhang Z, 2021, PROC CVPR IEEE, P12131, DOI 10.1109/CVPR46437.2021.01196
   Zhou BL, 2018, LECT NOTES COMPUT SC, V11205, P831, DOI 10.1007/978-3-030-01246-5_49
NR 75
TC 0
Z9 0
U1 4
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2024
VL 20
IS 7
SI SI
AR 218
DI 10.1145/3654671
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL0Q6
UT WOS:001234494100033
OA Bronze
DA 2024-08-05
ER

PT J
AU Chen, YZ
   Zhou, JH
   Peng, YX
AF Chen, Yanzhe
   Zhou, Jiahuan
   Peng, Yuxin
TI SPIRIT: Style-guided Patch Interaction for Fashion Image Retrieval with
   Text Feedback
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Fashion image retrieval with text feedback; style modeling; multimodal
   fusion
AB Fashion image retrieval with text feedback aims to find the target image according to the reference image and the modification from the user. This is a challenging task, as it requires not only the synergistic understanding of both visual and textual modalities but also the ability to model a wide variety of styles that fashion images contain. Hence, the crucial aspect of addressing this problem lies in exploiting the abundant semantic information inherent in fashion images and correlating it with the textual description of style. Recognizing that style is generally situated at the local level, we explicitly define style as the commonalities and differences between local areas of fashion images. Building upon this, we propose a Style-guided Patch InteRaction approach for fashion Image retrieval with Text feedback (SPIRIT), which focuses on the decisive influence of local details of fashion images on their style. Three corresponding networks are designed pertinently. The Patch-level Style Commonality network is introduced to fully leverage the semantic information among patches and compute their average as the style commonality. Subsequently, the Patch-level Style Difference network employs a graph reasoning network to model the patch-level difference and filter out insignificant patches. By considering the above two networks, mutual information about style is obtained from the interaction between patches. Finally, the Visual Textual Fusion network is utilized to integrate visual features with rich semantic information and textual features. Experimental results on four benchmark datasets demonstrate that our proposed SPIRIT achieves state-of-the-art performance. Source code is available at https://github.com/PKU- ICST-MIPL/SPIRIT_TOMM2024.
C1 [Chen, Yanzhe; Zhou, Jiahuan; Peng, Yuxin] Peking Univ, Wangxuan Inst Comp Technol, Beijing 100871, Peoples R China.
   [Peng, Yuxin] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
C3 Peking University; Peng Cheng Laboratory
RP Peng, YX (corresponding author), Peking Univ, Wangxuan Inst Comp Technol, Beijing 100871, Peoples R China.; Peng, YX (corresponding author), Peng Cheng Lab, Shenzhen 518055, Peoples R China.
EM pengyuxin@pku.edu.cn
OI Zhou, Jiahuan/0000-0002-3301-747X
FU National Natural Science Foundation of China [62132001, 61925201]
FX This work was supported by the grants from the National Natural Science
   Foundation of China (62132001 and 61925201).
CR Abdal R, 2019, IEEE I CONF COMP VIS, P4431, DOI 10.1109/ICCV.2019.00453
   Anwaar MU, 2021, IEEE WINT CONF APPL, P1139, DOI 10.1109/WACV48630.2021.00118
   Baldrati A, 2022, PROC CVPR IEEE, P21434, DOI 10.1109/CVPR52688.2022.02080
   Baldrati A, 2022, IEEE COMPUT SOC CONF, P4955, DOI 10.1109/CVPRW56347.2022.00543
   Baldrati Alberto, 2023, P IEEE CVF INT C COM, P15338
   Berg TL, 2010, LECT NOTES COMPUT SC, V6311, P663, DOI 10.1007/978-3-642-15549-9_48
   Chandran P, 2021, PROC CVPR IEEE, P7968, DOI 10.1109/CVPR46437.2021.00788
   Chen Yanzhe, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P4939, DOI 10.1145/3581783.3612408
   Chen YB, 2020, PROC CVPR IEEE, P2998, DOI 10.1109/CVPR42600.2020.00307
   Chen YY, 2024, Arxiv, DOI arXiv:2211.07394
   Chiu TY, 2022, IEEE WINT CONF APPL, P2978, DOI 10.1109/WACV51458.2022.00303
   Couairon G, 2022, IEEE COMPUT SOC CONF, P4946, DOI 10.1109/CVPRW56347.2022.00542
   Delmas G, 2022, Arxiv, DOI arXiv:2203.08101
   Diao HW, 2021, AAAI CONF ARTIF INTE, V35, P1218
   Dodds E, 2020, Arxiv, DOI arXiv:2007.00145
   Fu YJ, 2023, IEEE T CIRC SYST VID, V33, P1335, DOI 10.1109/TCSVT.2022.3210602
   Ge YY, 2019, PROC CVPR IEEE, P5332, DOI 10.1109/CVPR.2019.00548
   Goenka S, 2022, PROC CVPR IEEE, P14085, DOI 10.1109/CVPR52688.2022.01371
   Gordo A, 2016, LECT NOTES COMPUT SC, V9910, P241, DOI 10.1007/978-3-319-46466-4_15
   Gu G, 2024, Arxiv, DOI arXiv:2303.11916
   Guo X., 2018, Advances in Neural Information Processing Systems. NeurIPS '18
   Han X, 2022, LECT NOTES COMPUT SC, V13695, P634, DOI 10.1007/978-3-031-19833-5_37
   Han XT, 2017, IEEE I CONF COMP VIS, P1472, DOI 10.1109/ICCV.2017.163
   Han YP, 2023, PROC CVPR IEEE, P15028, DOI 10.1109/CVPR52729.2023.01443
   Hosseinzadeh M, 2020, PROC CVPR IEEE, P3593, DOI 10.1109/CVPR42600.2020.00365
   Hou YX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12127, DOI 10.1109/ICCV48922.2021.01193
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Jandial S, 2021, Arxiv, DOI arXiv:2009.01485
   Jandial S, 2022, IEEE WINT CONF APPL, P597, DOI 10.1109/WACV51458.2022.00067
   Jin Y, 2023, PROC CVPR IEEE, P11060, DOI 10.1109/CVPR52729.2023.01064
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kim JH, 2016, ADV NEUR IN, V29
   Kim J, 2021, AAAI CONF ARTIF INTE, V35, P1771
   Kingma D. P., 2014, arXiv
   Kocasari U, 2022, IEEE WINT CONF APPL, P3441, DOI 10.1109/WACV51458.2022.00350
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Lee S, 2021, PROC CVPR IEEE, P802, DOI 10.1109/CVPR46437.2021.00086
   Li YH, 2023, PROC CVPR IEEE, P23390, DOI 10.1109/CVPR52729.2023.02240
   Li YT, 2017, ADV NEUR IN, V30
   Liu ZY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2105, DOI 10.1109/ICCV48922.2021.00213
   Ma Z, 2020, AAAI CONF ARTIF INTE, V34, P11741
   Patashnik O, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2065, DOI 10.1109/ICCV48922.2021.00209
   Perez E, 2018, AAAI CONF ARTIF INTE, P3942
   Radford A, 2021, PR MACH LEARN RES, V139
   Saito K, 2023, PROC CVPR IEEE, P19305, DOI 10.1109/CVPR52729.2023.01850
   Santoro A, 2017, ADV NEUR IN, V30
   Sharma R., 2019, arXiv
   Shin M, 2021, Arxiv, DOI arXiv:2104.03015
   Suhr A, 2019, Arxiv, DOI arXiv:1811.00491
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Vo N, 2019, PROC CVPR IEEE, P6432, DOI 10.1109/CVPR.2019.00660
   Wen Haokun, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P915, DOI 10.1145/3581783.3611817
   Wu H, 2021, PROC CVPR IEEE, P11302, DOI 10.1109/CVPR46437.2021.01115
   Xide Xia, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P327, DOI 10.1007/978-3-030-58598-3_20
   Xu YH, 2023, IEEE T MULTIMEDIA, V25, P8346, DOI 10.1109/TMM.2023.3235495
   Yu YJ, 2020, Arxiv, DOI arXiv:2003.12299
   Zhan XL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11762, DOI 10.1109/ICCV48922.2021.01157
   Zhang PC, 2021, PROC CVPR IEEE, P5575, DOI 10.1109/CVPR46437.2021.00553
   Zhang X, 2023, Arxiv, DOI arXiv:2306.02092
   Zheng XY, 2023, COMPANION OF THE WORLD WIDE WEB CONFERENCE, WWW 2023, P356, DOI 10.1145/3543873.3584627
   Zhu HG, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3584703
   Zhu PH, 2020, PROC CVPR IEEE, P5103, DOI 10.1109/CVPR42600.2020.00515
   Zhuge MC, 2021, PROC CVPR IEEE, P12642, DOI 10.1109/CVPR46437.2021.01246
NR 64
TC 0
Z9 0
U1 2
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2024
VL 20
IS 6
SI SI
AR 167
DI 10.1145/3640345
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OQ2T3
UT WOS:001208681800017
DA 2024-08-05
ER

PT J
AU Li, SS
   Xu, X
   Jiang, X
   Shen, FM
   Sun, Z
   Cichocki, A
AF Li, Shenshen
   Xu, Xing
   Jiang, Xun
   Shen, Fumin
   Sun, Zhe
   Cichocki, Andrzej
TI Cross-Modal Attention Preservation with Self-Contrastive Learning for
   Composed Query-Based Image Retrieval
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Composed query-based image retrieval; cross-modal retrieval; cross-level
   interaction; preserved and modified attentions
AB In this article, we study the challenging cross-modal image retrieval task, Composed Query-Based Image Retrieval (CQBIR), in which the query is not a single text query but a composed query, i.e., a reference image, and a modification text. Compared with the conventional cross-modal image-text retrieval task, the CQBIR is more challenging as it requires properly preserving and modifying the specific image region according to the multi-level semantic information learned from the multi-modal query. Most recent works focus on extracting preserved and modified information and compositing it into a unified representation. However, we observe that the preserved regions learned by the existing methods contain redundant modified information, inevitably degrading the overall retrieval performance. To this end, we propose a novel method termed Cross-Modal Attention Preservation (CMAP). Specifically, we first leverage the cross-level interaction to fully account for multi-granular semantic information, which aims to supplement the high-level semantics for effective image retrieval. Furthermore, different from conventional contrastive learning, our method introduces self-contrastive learning into learning preserved information, to prevent the model from confusing the attention for the preserved part with the modified part. Extensive experiments on three widely used CQBIR datasets, i.e., FashionIQ, Shoes, and Fashion200k, demonstrate that our proposed CMAP method significantly outperforms the current state-of-the-art methods on all the datasets. The anonymous implementation code of our CMAP method is available at https://github.com/CFM- MSG/Code_CMAP.
C1 [Li, Shenshen; Xu, Xing; Jiang, Xun; Shen, Fumin] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Xiyuan Ave, Chengdu 611731, Peoples R China.
   [Sun, Zhe] Juntendo Univ Hongo, Bunkyo Ku, Tokyo 1360075, Japan.
   [Cichocki, Andrzej] Polish Acad Sci, Syst Res Inst, 6 Newelska St, PL-01447 Warsaw, Poland.
   [Cichocki, Andrzej] Riken AIP, Tensor Learning Lab, 1-4-1 Nihonbashi,Chuo Ku, Tokyo 1030027, Japan.
C3 University of Electronic Science & Technology of China; Polish Academy
   of Sciences; Systems Research Institute of the Polish Academy of
   Sciences; RIKEN
RP Xu, X (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Xiyuan Ave, Chengdu 611731, Peoples R China.
EM lishen-shen727@gmail.com; xing.xu@uestc.edu.cn; xun_jiang@outlook.com;
   fumin.shen@gmail.com; z.sun.kc@juntendo.ac.jp; cichockiand@gmail.com
RI Zhao, Yuxin/HNS-3187-2023; Cichocki, Andrzej/A-1545-2015
OI Cichocki, Andrzej/0000-0002-8364-7226; Jiang, Xun/0000-0003-2209-651X;
   Li, Shenshen/0000-0002-6340-012X
FU National Natural Science Foundation of China [62222203, 61976049,
   62072080]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62222203, 61976049, and 62072080.
CR Anwaar MU, 2021, IEEE WINT CONF APPL, P1139, DOI 10.1109/WACV48630.2021.00118
   Baldrati A, 2022, PROC CVPR IEEE, P21434, DOI 10.1109/CVPR52688.2022.02080
   Berg TL, 2010, LECT NOTES COMPUT SC, V6311, P663, DOI 10.1007/978-3-642-15549-9_48
   Chaudhary C, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3375786
   Chen YB, 2020, PROC CVPR IEEE, P2998, DOI 10.1109/CVPR42600.2020.00307
   Chen YY, 2024, Arxiv, DOI arXiv:2211.07394
   Chen Zhiguo, 2023, P 31 ACM INT C MULT
   Cheng YH, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3499027
   Cho K., 2014, P 2014 C EMP METH NA, DOI 10.3115/v1/D14-1179
   Delmas Ginger, 2022, Computing Research Repository
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Goenka S, 2022, PROC CVPR IEEE, P14085, DOI 10.1109/CVPR52688.2022.01371
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Gu G, 2024, Arxiv, DOI arXiv:2303.11916
   Guo XX, 2018, ADV NEUR IN, V31
   Han Xiao, 2022, Computing Research Repository
   Han XT, 2017, IEEE I CONF COMP VIS, P1472, DOI 10.1109/ICCV.2017.163
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hosseinzadeh M, 2020, PROC CVPR IEEE, P3593, DOI 10.1109/CVPR42600.2020.00365
   Jandial S, 2022, IEEE WINT CONF APPL, P597, DOI 10.1109/WACV51458.2022.00067
   Kim J, 2021, AAAI CONF ARTIF INTE, V35, P1771
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Lee S, 2021, PROC CVPR IEEE, P802, DOI 10.1109/CVPR46437.2021.00086
   Levy M, 2023, Arxiv, DOI arXiv:2303.09429
   Li Shenshen, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P6292, DOI 10.1145/3581783.3612244
   Li SS, 2024, IEEE T CIRC SYST VID, V34, P2959, DOI 10.1109/TCSVT.2023.3306738
   Liu ZY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2105, DOI 10.1109/ICCV48922.2021.00213
   Loshchilov I., 2018, INT C LEARN REPR
   Park Sungho, 2022, Computing Research Repository
   Pennington J., 2014, GLOVE GLOBAL VECTORS, DOI DOI 10.3115/V1/D14-1162
   Qing Liu, 2019, 2019 IEEE/CVF International Conference on Computer Vision (ICCV). Proceedings, P3661, DOI 10.1109/ICCV.2019.00376
   Radford A, 2021, PR MACH LEARN RES, V139
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sarafianos N, 2019, IEEE I CONF COMP VIS, P5813, DOI 10.1109/ICCV.2019.00591
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   van den Oord A, 2019, Arxiv, DOI [arXiv:1807.03748, DOI 10.48550/ARXIV.1807.03748]
   Vo N, 2019, PROC CVPR IEEE, P6432, DOI 10.1109/CVPR.2019.00660
   Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326
   Wang WL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14000, DOI 10.1109/ICCV48922.2021.01376
   Wang XH, 2023, IEEE T MULTIMEDIA, V25, P6079, DOI 10.1109/TMM.2022.3204444
   Wang XH, 2021, PROC CVPR IEEE, P5075, DOI 10.1109/CVPR46437.2021.00504
   Wen HK, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1369, DOI 10.1145/3404835.3462967
   Wu H, 2021, PROC CVPR IEEE, P11302, DOI 10.1109/CVPR46437.2021.01115
   Xu X, 2022, IEEE T CYBERNETICS, V52, P3261, DOI 10.1109/TCYB.2020.3009004
   Yang Q, 2023, IEEE T IMAGE PROCESS, V32, P4543, DOI 10.1109/TIP.2023.3299791
   Yang S, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3572844
   Zhang FF, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3478642
   Zhang FF, 2022, IEEE T IMAGE PROCESS, V31, P1000, DOI 10.1109/TIP.2021.3138302
   Zhang GJ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5353, DOI 10.1145/3474085.3475659
   Zhu AC, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P209, DOI 10.1145/3474085.3475369
   Zhu HG, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3584703
NR 52
TC 0
Z9 0
U1 1
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2024
VL 20
IS 6
SI SI
AR 165
DI 10.1145/3639469
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OQ2T3
UT WOS:001208681800015
DA 2024-08-05
ER

PT J
AU Wang, XZ
   Liu, R
   Yang, X
   Zhang, Q
   Zhou, DS
AF Wang, Xizhong
   Liu, Rui
   Yang, Xin
   Zhang, Qiang
   Zhou, Dongsheng
TI MCFNet: Multi-Attentional Class Feature Augmentation Network for
   Real-Time Scene Parsing
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Computer vision; CNN; real-time semantic segmentation; attention
   mechanism
ID SEMANTIC SEGMENTATION
AB For real-time scene parsing tasks, capturing multi-scale semantic features and performing effective feature fusion is crucial. However, many existing solutions ignore stripe-shaped things like poles, traffic lights and are so computationally expensive that cannot meet the high real-time requirements. This article presents a novel model, the Multi-Attention Class Feature Augmentation Network (MCFNet) to address this challenge. MCFNet is designed to capture long-range dependencies across different scaleswith lowcomputational cost and to perform a weighted fusion of feature maps. It features the BAM (Strip Matrix Based Attention Module) for extracting strip objects in images. The BAM module replaces the conventional self-attention method using square matrices with strip matrices, which allows it to focus more on strip objects while reducing computation. Additionally, MCFNet has a parallel branch that focuses on global information based on self-attention to avoid wasting computation. The two branches are merged to enhance the performance of traditional self-attention modules. Experimental results on two mainstream datasets demonstrate the effectiveness of MCFNet. On the Camvid and Cityscapes test sets, MCFNet achieved 207.5 FPS/73.5% mIoU and 136.1 FPS/71.63% mIoU, respectively. The experiments show that MCFNet outperforms other models on the Camvid dataset and can significantly improve the performance of real-time scene parsing tasks.
C1 [Wang, Xizhong; Liu, Rui; Zhou, Dongsheng] Dalian Univ, Sch Software Engn, Natl & Local Joint Engn Lab Comp Aided Design, 10th Xuefu St, Dalian, Peoples R China.
   [Yang, Xin; Zhang, Qiang] Dalian Univ Technol, Sch Comp Sci, 2th Lingshui Rd, Dalian 116024, Peoples R China.
C3 Dalian University; Dalian University of Technology
RP Zhou, DS (corresponding author), Dalian Univ, Sch Software Engn, Natl & Local Joint Engn Lab Comp Aided Design, 10th Xuefu St, Dalian, Peoples R China.
EM wangxizhong@s.dlu.edu.cn; liurui@dlu.edu.cn; xinyang@dlut.edu.cn;
   zhangq@dlut.edu.cn; zhouds@dlu.edu.cn
OI Zhou, Dongsheng/0000-0003-3414-9623; Liu, Rui/0000-0002-5200-640X
FU Key Project of NSFC [U1908214]; Program for Innovative Research Team in
   University of Liaoning Province [LT2020015]; Support Plan for Key Field
   Innovation Team of Dalian [2021RT06]; Support Plan for Leading
   Innovation Team of Dalian University [XLJ202010]; Science and Technology
   Innovation Fund of Dalian [2020JJ25CY001]; 111 Project [D23006]
FX This work was supported by the Key Project of NSFC (Grant No. U1908214),
   the Program for Innovative Research Team in University of Liaoning
   Province (LT2020015), the Support Plan for Key Field Innovation Team of
   Dalian (2021RT06), the Support Plan for Leading Innovation Team of
   Dalian University (XLJ202010); the Science and Technology Innovation
   Fund of Dalian (Grant No. 2020JJ25CY001); the 111 Project (grant number
   D23006).
CR Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Brostow GJ, 2008, LECT NOTES COMPUT SC, V5302, P44, DOI 10.1007/978-3-540-88682-2_5
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Ding MY, 2020, AAAI CONF ARTIF INTE, V34, P10713
   Dong GS, 2021, IEEE T INTELL TRANSP, V22, P3258, DOI 10.1109/TITS.2020.2980426
   Dong Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P323, DOI 10.1007/978-3-030-58604-1_20
   Fan JQ, 2023, IEEE T INTELL VEHICL, V8, P756, DOI 10.1109/TIV.2022.3176860
   Fan MY, 2021, PROC CVPR IEEE, P9711, DOI 10.1109/CVPR46437.2021.00959
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Gao GW, 2023, IEEE T MULTIMEDIA, V25, P3273, DOI 10.1109/TMM.2022.3157995
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   Hao YY, 2021, IEEE INT CONF COMP V, P1551, DOI 10.1109/ICCVW54120.2021.00180
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou QB, 2020, PROC CVPR IEEE, P4002, DOI 10.1109/CVPR42600.2020.00406
   Hou YN, 2019, IEEE I CONF COMP VIS, P1013, DOI 10.1109/ICCV.2019.00110
   Hu P, 2021, IEEE ROBOT AUTOM LET, V6, P263, DOI 10.1109/LRA.2020.3039744
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Li G, 2020, IEEE ACCESS, V8, P27495, DOI 10.1109/ACCESS.2020.2971760
   Li HC, 2019, PROC CVPR IEEE, P9514, DOI 10.1109/CVPR.2019.00975
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Liu J, 2022, NEUROCOMPUTING, V474, P115, DOI 10.1016/j.neucom.2021.12.003
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Lo S.-Y., 2019, P ACM MULT AS, P1, DOI DOI 10.1145/3338533.3366558
   Neven D, 2018, IEEE INT VEH SYM, P286
   Pan XR, 2022, PROC CVPR IEEE, P805, DOI 10.1109/CVPR52688.2022.00089
   Ramachandran P, 2019, ADV NEUR IN, V32
   Romera E, 2018, IEEE T INTELL TRANSP, V19, P263, DOI 10.1109/TITS.2017.2750080
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Si H., 2020, P BRIT MACH VIS VIRT
   Siam M, 2018, IEEE COMPUT SOC CONF, P700, DOI 10.1109/CVPRW.2018.00101
   Singha T., 2022, 2022 INT C DIG IM CO, P1
   Singha T, 2023, PATTERN RECOGN, V140, DOI 10.1016/j.patcog.2023.109557
   Song Q, 2021, AAAI CONF ARTIF INTE, V35, P2567
   Sun X, 2022, IEEE T CIRC SYST VID, V32, P2937, DOI 10.1109/TCSVT.2021.3096814
   Tan ZT, 2021, IEEE T CIRC SYST VID, V31, P175, DOI 10.1109/TCSVT.2020.2971641
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang Y, 2019, IEEE IMAGE PROC, P1860, DOI [10.1109/icip.2019.8803154, 10.1109/ICIP.2019.8803154]
   Wang YQ, 2021, PROC CVPR IEEE, P8737, DOI 10.1109/CVPR46437.2021.00863
   Xiangtai Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P775, DOI 10.1007/978-3-030-58452-8_45
   Yi QM, 2023, NEURAL PROCESS LETT, V55, P6425, DOI 10.1007/s11063-023-11145-z
   Yifan Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P352, DOI 10.1007/978-3-030-58607-2_21
   Yu CQ, 2021, INT J COMPUT VISION, V129, P3051, DOI 10.1007/s11263-021-01515-2
   Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20
   Yuan YH, 2021, INT J COMPUT VISION, V129, P2375, DOI 10.1007/s11263-021-01465-9
   Zha HF, 2021, COMPUT ANIMAT VIRT W, V32, DOI 10.1002/cav.2022
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhang XL, 2022, APPL INTELL, V52, P564, DOI 10.1007/s10489-021-02437-9
   Zhang XL, 2022, NEURAL COMPUT APPL, V34, P3573, DOI 10.1007/s00521-022-06932-z
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11207, P418, DOI 10.1007/978-3-030-01219-9_25
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhou DQ, 2022, PR MACH LEARN RES
   Zhuang JF, 2021, IEEE T CIRC SYST VID, V31, P3128, DOI 10.1109/TCSVT.2020.3037234
NR 55
TC 0
Z9 0
U1 5
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2024
VL 20
IS 6
SI SI
AR 166
DI 10.1145/3639053
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OQ2T3
UT WOS:001208681800016
DA 2024-08-05
ER

PT J
AU Wang, LL
   Shi, YH
   Wang, J
   Chen, SJ
   Yin, BC
   Ling, N
AF Wang, Lilong
   Shi, Yunhui
   Wang, Jin
   Chen, Shujun
   Yin, Baocai
   Ling, Nam
TI Graph Based Cross-Channel Transform for Color Image Compression
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Graph fourier transform (GFT); image compression; dual graph;
   graph-based transform (GBT); discrete cosine transform; graph learning
ID PREDICTIVE TRANSFORM; WAVELET TRANSFORM; FOURIER-TRANSFORM;
   INTRA-PREDICTION; COMPONENT
AB Adaptive transform coding is gaining more and more attention for better mining of image content over fixed transforms such as discrete cosine transform (DCT). As a special case, graph transform learning establishes a novel paradigm for the graph-based transforms. However, there still exists a challenge for graph transform learning-based image codecs design on natural image compression, and graph representation cannot describe regular image samples well over graph-structured data. Therefore, in this article, we propose a cross-channel graph-based transform (CCGBT) for natural color image compression. We observe that neighboring pixels having similar intensities should have similar values in the chroma channels, which means that the prominent structure of the luminance channel is related to the contours of the chrominance channels. A collaborative design of the learned graphs and their corresponding distinctive transforms lies in the assumption that a sufficiently small block can be considered smooth, meanwhile, guaranteeing the compression of the luma and chroma signals at the cost of a small overhead for coding the description of the designed luma graph. In addition, a color image compression framework based on the CCGBT is designed for comparing DCT on the classic JPEG codec. The proposed method benefits from its flexible transform block design on arbitrary sizes to exploit image content better than the fixed transform. The experimental results show that the unified graph-based transform outperforms conventional DCT, while close to discrete wavelet transform on JPEG2000 at high bit-rates.
C1 [Wang, Lilong; Shi, Yunhui; Wang, Jin; Yin, Baocai] Beijing Univ Technol, Fac Informat Technol, Beijing 100124, Peoples R China.
   [Chen, Shujun] Beijing Univ Technol, Fac Mat & Mfg, Beijing 100124, Peoples R China.
   [Ling, Nam] Santa Clara Univ, Dept Comp Sci & Engn, Santa Clara, CA 95053 USA.
C3 Beijing University of Technology; Beijing University of Technology;
   Santa Clara University
RP Wang, J (corresponding author), Beijing Univ Technol, Fac Informat Technol, Beijing 100124, Peoples R China.
EM wangll@emails.bjut.edu.cn; syhzm@bjut.edu.cn; ijinwang@bjut.edu.cn;
   sjchen@bjut.edu.cn; ybc@bjut.edu.cn; nling@scu.edu
OI Wang, Jin/0000-0001-5437-3150
FU National Key R&D Program of China [2021ZD0111902]; National Natural
   Science Foundation of China [62372018, 62272016, U21B2038, 61976011,
   U1937207]
FX This work is funded by the National Key R&D Program of China
   (No.2021ZD0111902), the National Natural Science Foundation of China
   (62372018, 62272016, U21B2038, 61976011, U1937207).
CR Abiko Kaito, 2019, P 2019 IEEE VISUAL C, P1, DOI [10.1109/VCIP47243.2019.8966021, DOI 10.1109/VCIP47243.2019.8966021]
   Agustsson E, 2023, PROC CVPR IEEE, P22324, DOI 10.1109/CVPR52729.2023.02138
   AHMED N, 1974, IEEE T COMPUT, VC 23, P90, DOI 10.1109/T-C.1974.223784
   Bagheri S, 2022, IEEE IMAGE PROC, P3667, DOI 10.1109/ICIP46576.2022.9897653
   Ballé J, 2016, PICT COD SYMP, DOI 10.1109/pcs.2016.7906310
   Balle Johannes, 2017, P 5 INT C LEARNING R
   Boragolla R, 2022, IEEE DATA COMPR CONF, P442, DOI 10.1109/DCC52660.2022.00053
   Bross Benjamin, 2022, MHV '22: Proceedings of the 1st Conference on Mile-High Video, P124, DOI 10.1145/3510450.3517315
   Chang CL, 2007, IEEE T IMAGE PROCESS, V16, P1289, DOI 10.1109/TIP.2007.894242
   DEVORE RA, 1992, IEEE T INFORM THEORY, V38, P719, DOI 10.1109/18.119733
   Dong XW, 2019, IEEE SIGNAL PROC MAG, V36, P44, DOI 10.1109/MSP.2018.2887284
   DONY RD, 1995, IEEE T IMAGE PROCESS, V4, P1358, DOI 10.1109/83.465101
   Egilmez HE, 2021, IEEE OPEN J SIGNAL P, V2, P441, DOI 10.1109/OJSP.2021.3092257
   Egilmez HE, 2020, IEEE T IMAGE PROCESS, V29, P9330, DOI 10.1109/TIP.2020.3026627
   Egilmez HE, 2015, IEEE IMAGE PROC, P3992, DOI 10.1109/ICIP.2015.7351555
   Fracastoro G, 2020, IEEE T IMAGE PROCESS, V29, P419, DOI 10.1109/TIP.2019.2932853
   Fracastoro G, 2016, PICT COD SYMP
   Gnutti A, 2021, IEEE IMAGE PROC, P1594, DOI 10.1109/ICIP42928.2021.9506636
   Goyal VK, 2001, IEEE SIGNAL PROC MAG, V18, P9, DOI 10.1109/79.952802
   Han JN, 2012, IEEE T IMAGE PROCESS, V21, P1874, DOI 10.1109/TIP.2011.2169976
   Hu W, 2022, IEEE T MULTIMEDIA, V24, P3961, DOI 10.1109/TMM.2021.3111440
   Hu W, 2015, IEEE SIGNAL PROC LET, V22, P1913, DOI 10.1109/LSP.2015.2446683
   Hu W, 2015, IEEE T IMAGE PROCESS, V24, P419, DOI 10.1109/TIP.2014.2378055
   ITU-T, 2020, H.266: Versatile video coding
   Lee S, 2013, IEEE T IMAGE PROCESS, V22, P2627, DOI 10.1109/TIP.2013.2253486
   Li JR, 2022, IEEE COMPUT SOC CONF, P1780, DOI 10.1109/CVPRW56347.2022.00191
   Li JR, 2021, IEEE MULTIMEDIA, V28, P52, DOI 10.1109/MMUL.2020.3041008
   Li JR, 2020, IEEE INT CONF MULTI, DOI 10.1109/icmew46912.2020.9105965
   Li Y, 2018, IEEE IMAGE PROC, P1797, DOI 10.1109/ICIP.2018.8451396
   Liu D, 2020, ACM COMPUT SURV, V53, DOI 10.1145/3368405
   Liu JM, 2023, PROC CVPR IEEE, P14388, DOI 10.1109/CVPR52729.2023.01383
   Lopes D, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P4225, DOI 10.1109/ICASSP39728.2021.9413816
   Ma HC, 2022, IEEE T PATTERN ANAL, V44, P1247, DOI 10.1109/TPAMI.2020.3026003
   Ma SW, 2020, IEEE T CIRC SYST VID, V30, P1683, DOI 10.1109/TCSVT.2019.2910119
   Mishiba K, 2014, IEEE IMAGE PROC, P5551, DOI 10.1109/ICIP.2014.7026123
   Parker J, 2023, ANALYSING THE HISTORY OF BRITISH SOCIAL WELFARE, P82, DOI 10.1145/3551389
   Pavez E, 2015, 2015 PICTURE CODING SYMPOSIUM (PCS) WITH 2015 PACKET VIDEO WORKSHOP (PV), P199, DOI 10.1109/PCS.2015.7170075
   Peter P, 2014, IEEE IMAGE PROC, P4822, DOI 10.1109/ICIP.2014.7025977
   Shen GW, 2010, IEEE IMAGE PROC, P3393, DOI 10.1109/ICIP.2010.5652792
   Shuman DI, 2013, IEEE SIGNAL PROC MAG, V30, P83, DOI 10.1109/MSP.2012.2235192
   Skodras A, 2001, IEEE SIGNAL PROC MAG, V18, P36, DOI 10.1109/79.952804
   Sole J, 2012, IEEE T CIRC SYST VID, V22, P1765, DOI 10.1109/TCSVT.2012.2223055
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tian T, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3408320
   Uruma K, 2019, SIGNAL PROCESS-IMAGE, V74, P266, DOI 10.1016/j.image.2018.12.011
   Uruma K, 2017, IEEE IMAGE PROC, P3255, DOI 10.1109/ICIP.2017.8296884
   WALLACE GK, 1991, COMMUN ACM, V34, P30, DOI 10.1145/103085.103089
   Wang DZ, 2022, PROC CVPR IEEE, P17358, DOI 10.1109/CVPR52688.2022.01686
   Wang M, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3589963
   Wang MH, 2014, IEEE T MULTIMEDIA, V16, P933, DOI 10.1109/TMM.2014.2305579
   Wang YZ, 2013, IEEE IMAGE PROC, P1655, DOI 10.1109/ICIP.2013.6738341
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xiong ZX, 1999, IEEE T CIRC SYST VID, V9, P692, DOI 10.1109/76.780358
   Youvalari Ramin Ghaznavi, 2020, IEEE INT WORKSH MULT, P1, DOI [10.1109/MMSP48831.2020.9287167, DOI 10.1109/mmsp48831.2020.9287167]
   Zeng B, 2008, IEEE T CIRC SYST VID, V18, P305, DOI 10.1109/TCSVT.2008.918455
   Zhang C, 2013, IEEE SIGNAL PROC LET, V20, P106, DOI 10.1109/LSP.2012.2230165
   Zhang D, 2017, IEEE T IMAGE PROCESS, V26, P1679, DOI 10.1109/TIP.2017.2661399
   Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P3983, DOI 10.1109/TIP.2018.2830640
   Zhang XF, 2020, IEEE T IMAGE PROCESS, V29, P9292, DOI 10.1109/TIP.2020.3025203
   Zhang YH, 2022, IEEE T IMAGE PROCESS, V31, P1298, DOI 10.1109/TIP.2021.3137659
   Zhao X, 2021, IEEE T CIRC SYST VID, V31, P3878, DOI 10.1109/TCSVT.2021.3087706
   Zhao X, 2018, IEEE T IMAGE PROCESS, V27, P2514, DOI 10.1109/TIP.2018.2802202
   Zhu LW, 2021, IEEE T CIRC SYST VID, V31, P3168, DOI 10.1109/TCSVT.2020.3035356
   Zhu SY, 2019, IEEE T CIRC SYST VID, V29, P1559, DOI 10.1109/TCSVT.2019.2895840
   Zhu SYY, 2019, IEEE T CIRC SYST VID, V29, P1474, DOI 10.1109/TCSVT.2018.2841642
NR 65
TC 0
Z9 0
U1 7
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2024
VL 20
IS 4
AR 102
DI 10.1145/3631710
PG 25
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6G9
UT WOS:001153382100012
DA 2024-08-05
ER

PT J
AU Cheng, H
   Guo, YY
   Wang, TY
   Li, Q
   Chang, XJ
   Nie, LQ
AF Cheng, Harry
   Guo, Yangyang
   Wang, Tianyi
   Li, Qi
   Chang, Xiaojun
   Nie, Liqiang
TI Voice-Face Homogeneity Tells Deepfake
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Deepfake detection; cross-modal matching; voice; face
AB Detecting forgery videos is highly desirable due to the abuse of deepfake. Existing detection approaches contribute to exploring the specific artifacts in deepfake videos and fit well on certain data. However, the growing technique on these artifacts keeps challenging the robustness of traditional deepfake detectors. As a result, the development of these approaches has reached a blockage. In this article, we propose to perform deepfake detection from an unexplored voice-face matching view. Our approach is founded on two supporting points: first, there is a high degree of homogeneity between the voice and face of an individual (i.e., they are highly correlated), and second, deepfake videos often involve mismatched identities between the voice and face due to face-swapping techniques. To this end, we develop a voice-face matching method that measures the matching degree between these two modalities to identify deepfake videos. Nevertheless, training on specific deepfake datasets makes the model overfit certain traits of deepfake algorithms. We instead advocate a method that quickly adapts to untapped forgery, with a pre-training then fine-tuning paradigm. Specifically, we first pre-train the model on a generic audio-visual dataset, followed by the fine-tuning on downstream deepfake data. We conduct extensive experiments over three widely exploited deepfake datasets: DFDC, FakeAVCeleb, and DeepfakeTIMIT. Our method obtains significant performance gains as compared to other state-of-the-art competitors. For instance, ourmethod outperforms the baselines by nearly 2%, achieving an AUC of 86.11% on FakeAVCeleb. It is also worth noting that our method already achieves competitive results when fine-tuned on limited deepfake data.
C1 [Cheng, Harry; Li, Qi] Shandong Univ, Sch Comp Sci & Technol, Qingdao 266237, Peoples R China.
   [Guo, Yangyang] Natl Univ Singapore, Sch Comp, Singapore 119391, Singapore.
   [Wang, Tianyi] Univ Hong Kong, Dept Comp Sci, Hong Kong 999077, Peoples R China.
   [Chang, Xiaojun] Univ Technol Sydney, Fac Engn & Informat Technol, Sydney, NSW, Australia.
   [Nie, Liqiang] Harbin Inst Technol Shenzhen, Sch Comp Sci & Technol, Shenzhen 518055, Peoples R China.
C3 Shandong University; National University of Singapore; University of
   Hong Kong; University of Technology Sydney; Harbin Institute of
   Technology
RP Cheng, H (corresponding author), Shandong Univ, Sch Comp Sci & Technol, Qingdao 266237, Peoples R China.
EM xaCheng1996@gmail.com; guoyang.eric@gmail.com; terry.ai.wang@gmail.com;
   iliqi.nice@gmail.com; cxj273@gmail.com; nieliqiang@gmail.com
RI Wang, Tian-Yi/AAI-5727-2021; Chang, Xiaojun/A-2055-2015
OI Wang, Tian-Yi/0000-0001-6488-339X; Chang, Xiaojun/0000-0002-7778-8807;
   Cheng, Harry/0000-0001-7436-0162; Guo, Yangyang/0000-0001-8691-5372;
   Wang, Tianyi/0000-0003-2920-6099
CR Afchar D, 2018, IEEE INT WORKS INFOR
   Bitouk D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360638
   Blanz V, 2004, COMPUT GRAPH FORUM, V23, P669, DOI 10.1111/j.1467-8659.2004.00799.x
   Cai ZX, 2023, Arxiv, DOI arXiv:2204.06228
   Cheng HRY, 2023, Arxiv, DOI arXiv:2307.14866
   Cheng Harry, 2021, IEEE Transactions on Multimedia
   Cheng K, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P448, DOI 10.1145/3394171.3413710
   Chung JS, 2018, INTERSPEECH, P1086
   Dolhansky B, 2020, Arxiv, DOI arXiv:2006.07397
   Dosovitskiy A., 2021, ICLR
   Ge SM, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3536426
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Groh M, 2022, P NATL ACAD SCI USA, V119, DOI 10.1073/pnas.2110013119
   GUO Y, 2021, P INT JOINT C ART IN, P708
   Guo YY, 2022, IEEE T IMAGE PROCESS, V31, P227, DOI 10.1109/TIP.2021.3128322
   Guo Z., 2023, Expert Systems with Applications, V215
   Guo ZQ, 2023, COMPUT VIS IMAGE UND, V226, DOI 10.1016/j.cviu.2022.103587
   Guo ZQ, 2021, COMPUT VIS IMAGE UND, V204, DOI 10.1016/j.cviu.2021.103170
   Guo Zhiqing, 2023, IEEE Transactions on Multimedia
   Haliassos A, 2021, PROC CVPR IEEE, P5037, DOI 10.1109/CVPR46437.2021.00500
   Horiguchi S, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1011, DOI 10.1145/3240508.3240601
   Kamachi M, 2003, CURR BIOL, V13, P1709, DOI 10.1016/j.cub.2003.09.005
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Kemelmacher-Shlizerman I, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925871
   Khalid Hasam, 2021, ADGD '21: Proceedings of the 1st Workshop on Synthetic Multimedia - Audiovisual Deepfake Generation and Detection, P7, DOI 10.1145/3476099.3484315
   Khalid H., 2021, P 35 C NEURAL INFORM, P1
   Kim H, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201283
   Korshunov P, 2018, ARXIV
   Korshunov P, 2018, EUR SIGNAL PR CONF, P2375, DOI 10.23919/EUSIPCO.2018.8553270
   Koujan MR, 2020, IEEE INT CONF AUTOMA, P16, DOI 10.1109/FG47880.2020.00048
   Lewis JK, 2020, IEEE APP IMG PAT, DOI 10.1109/AIPR50011.2020.9425167
   Li LZ, 2020, PROC CVPR IEEE, P5073, DOI 10.1109/CVPR42600.2020.00512
   Li LZ, 2020, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR42600.2020.00505
   Li YC, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3414838
   [李艳歌 Li Yange], 2018, [高分子通报, Polymer Bulletin], P46
   Li YZ, 2020, PROC CVPR IEEE, P3204, DOI 10.1109/CVPR42600.2020.00327
   Liu HG, 2021, PROC CVPR IEEE, P772, DOI 10.1109/CVPR46437.2021.00083
   Liu XL, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3558004
   Loshchilov I., 2018, INT C LEARN REPR
   Mahmud B, 2024, ACM T MULTIM COMPUT, V20, DOI 10.1145/3542698
   Masi Iacopo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P667, DOI 10.1007/978-3-030-58571-6_39
   Matern F, 2019, IEEE WINT CONF APPL, P83, DOI 10.1109/WACVW.2019.00020
   Mittal T, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2823, DOI 10.1145/3394171.3413570
   Nagrani A, 2018, PROC CVPR IEEE, P8427, DOI 10.1109/CVPR.2018.00879
   Nagrani A, 2017, INTERSPEECH, P2616, DOI 10.21437/Interspeech.2017-950
   Nguyen HH, 2019, INT CONF ACOUST SPEE, P2307, DOI 10.1109/ICASSP.2019.8682602
   Nirkin Y, 2019, IEEE I CONF COMP VIS, P7183, DOI 10.1109/ICCV.2019.00728
   Oh TH, 2019, PROC CVPR IEEE, P7531, DOI 10.1109/CVPR.2019.00772
   Oord A.v.d., 2018, ARXIV
   Pala Pietro., 2019, ACM Transactions on Multimedia Computing, Communications, and Applications, V15, P1
   Prajwal KR, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P484, DOI 10.1145/3394171.3413532
   Rössler A, 2019, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2019.00009
   Sanderson C, 2009, LECT NOTES COMPUT SC, V5558, P199, DOI 10.1007/978-3-642-01793-3_21
   Shao HR, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3399659
   Shelke NA, 2022, MULTIMEDIA SYST, V28, P267, DOI 10.1007/s00530-021-00837-y
   Shelke NA, 2022, MULTIMED TOOLS APPL, V81, P22731, DOI 10.1007/s11042-021-10989-8
   Shelke NA, 2021, MULTIMED TOOLS APPL, V80, P6247, DOI 10.1007/s11042-020-09974-4
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tan MX, 2019, PR MACH LEARN RES, V97
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Yuhan, 2021, P 30 INT JOINT C ART, P1136, DOI DOI 10.24963/IJCAI.2021/157
   Wen PS, 2021, PROC CVPR IEEE, P16342, DOI 10.1109/CVPR46437.2021.01608
   Wu X, 2020, INT CONF ACOUST SPEE, P2952, DOI [10.1109/icassp40776.2020.9053969, 10.1109/ICASSP40776.2020.9053969]
   Xue H, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3571857
   Yan CG, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3472810
   Yan L, 2022, KNOWL-BASED SYST, V241, DOI 10.1016/j.knosys.2022.108223
   Yang X, 2019, INT CONF ACOUST SPEE, P8261, DOI 10.1109/ICASSP.2019.8683164
   Yao YX, 2022, IEEE T IMAGE PROCESS, V31, P2584, DOI 10.1109/TIP.2022.3157450
   Yu Y, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3499026
   Yuyang Qian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P86, DOI 10.1007/978-3-030-58610-2_6
   Zhao HQ, 2021, PROC CVPR IEEE, P2185, DOI 10.1109/CVPR46437.2021.00222
   Zhou YP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14780, DOI 10.1109/ICCV48922.2021.01453
NR 73
TC 4
Z9 5
U1 26
U2 26
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2024
VL 20
IS 3
AR 76
DI 10.1145/3625231
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6F8
UT WOS:001153381000016
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Xiong, Y
   Wang, JR
   Zhou, Z
AF Xiong, Yuan
   Wang, Jingru
   Zhou, Zhong
TI VirtualLoc: Large-scale Visual Localization Using Virtual Images
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Visual localization; virtual reality; image retrieval; rendering
AB Robust and accurate camera pose estimation is fundamental in computer vision. Learning-based regression approaches acquire six-degree-of-freedom camera parameters accurately from visual cues of an input image. However, most are trained on street-view and landmark datasets. These approaches can hardly be generalized to overlooking use cases, such as the calibration of the surveillance camera and unmanned aerial vehicle. Besides, reference images captured from the real world are rare and expensive, and their diversity is not guaranteed. In this article, we address the problem of using alternative virtual images for visual localization training. This work has the following principle contributions: First, we present a new challenging localization dataset containing six reconstructed large-scale three-dimensional scenes, 10,594 calibrated photographs with condition changes, and 300k virtual images with pixelwise labeled depth, relative surface normal, and semantic segmentation. Second, we present a flexible multi-feature fusion network trained on virtual image datasets for robust image retrieval. Third, we propose an end-to-end confidence map prediction network for feature filtering and pose estimation. We demonstrate that large-scale rendered virtual images are beneficial to visual localization. Using virtual images can solve the diversity problem of real images and leverage labeled multi-feature data for deep learning. Experimental results show that our method achieves remarkable performance surpassing state-of-the-art approaches. To foster research on improvement for visual localization using synthetic images, we release our benchmark at https://github.com/YuanXiong/contributions.
C1 [Xiong, Yuan; Wang, Jingru; Zhou, Zhong] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Zhou, Zhong] State Key Lab Virtual Real Technol & Syst, 37 Xueyuan Rd, Beijing 100191, Peoples R China.
   [Zhou, Zhong] Zhongguancun Lab, 37 Xueyuan Rd, Beijing 100191, Peoples R China.
C3 Beihang University; Zhongguancun Laboratory
RP Zhou, Z (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.; Zhou, Z (corresponding author), State Key Lab Virtual Real Technol & Syst, 37 Xueyuan Rd, Beijing 100191, Peoples R China.; Zhou, Z (corresponding author), Zhongguancun Lab, 37 Xueyuan Rd, Beijing 100191, Peoples R China.
EM xiongyuanxy@buaa.edu; jrwang999@163.com; zz@buaa.edu.cn
RI Wang, Jingru/HJH-1860-2023
OI Zhou, Zhong/0000-0002-5825-7517; Xiong, Yuan/0000-0002-7253-4998
FU Natural Science Foundation of China [62272018]
FX This work is supported by the Natural Science Foundation of China under
   Grant No. 62272018.
CR Abrardo A, 2013, ACM T SENSOR NETWORK, V9, DOI 10.1145/2489253.2489261
   Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI [10.1109/TPAMI.2017.2711011, 10.1109/CVPR.2016.572]
   Bansal A, 2014, IEEE INT VEH SYM, P800, DOI 10.1109/IVS.2014.6856605
   Barath D, 2022, PROC CVPR IEEE, P15723, DOI 10.1109/CVPR52688.2022.01529
   Brachmann E, 2022, IEEE T PATTERN ANAL, V44, P5847, DOI 10.1109/TPAMI.2021.3070754
   Brahmbhatt S, 2018, PROC CVPR IEEE, P2616, DOI 10.1109/CVPR.2018.00277
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen WF, 2016, ADV NEUR IN, V29
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Cramariuc A, 2021, IEEE INT C INT ROBOT, P1183, DOI 10.1109/IROS51168.2021.9636156
   Dai PW, 2022, IEEE T IMAGE PROCESS, V31, P4076, DOI 10.1109/TIP.2022.3167919
   Dubé R, 2020, INT J ROBOT RES, V39, P339, DOI 10.1177/0278364919863090
   Dusmanu M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6038, DOI 10.1109/ICCV48922.2021.00600
   Dusmanu M, 2019, PROC CVPR IEEE, P8084, DOI 10.1109/CVPR.2019.00828
   Gao Z, 2023, IEEE T KNOWL DATA EN, V35, P7541, DOI 10.1109/TKDE.2022.3187091
   Gao Z, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P6146, DOI 10.1145/3503161.3548100
   Germain H, 2020, Arxiv, DOI arXiv:2004.01673
   Gordo A, 2017, INT J COMPUT VISION, V124, P237, DOI 10.1007/s11263-017-1016-8
   Guan WL, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3424115
   Hausler S, 2021, PROC CVPR IEEE, P14136, DOI 10.1109/CVPR46437.2021.01392
   Huang XY, 2020, IEEE T PATTERN ANAL, V42, P2702, DOI 10.1109/TPAMI.2019.2926463
   Hui L, 2022, IEEE T IMAGE PROCESS, V31, P1258, DOI 10.1109/TIP.2021.3136714
   Jegou Herve, 2012, P EUROPEAN C COMPUTE
   Kendall A, 2015, IEEE I CONF COMP VIS, P2938, DOI 10.1109/ICCV.2015.336
   Larsson M, 2019, IEEE I CONF COMP VIS, P31, DOI 10.1109/ICCV.2019.00012
   Lianos KN, 2018, LECT NOTES COMPUT SC, V11208, P246, DOI 10.1007/978-3-030-01225-0_15
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Maddern W, 2017, INT J ROBOT RES, V36, P3, DOI 10.1177/0278364916679498
   Menze M, 2015, PROC CVPR IEEE, P3061, DOI 10.1109/CVPR.2015.7298925
   Moller Andreas, 2012, P 11 INT C MOBILE UB, P1
   Panek V, 2023, PROC CVPR IEEE, P13175, DOI 10.1109/CVPR52729.2023.01266
   Panek V, 2022, LECT NOTES COMPUT SC, V13682, P589, DOI 10.1007/978-3-031-20047-2_34
   Pietrantoni M, 2023, PROC CVPR IEEE, P15380, DOI 10.1109/CVPR52729.2023.01476
   Qi C. R., 2017, ADV NEURAL INFORM PR, P5099, DOI DOI 10.1109/CVPR.2017.16
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Revaud J, 2019, ADV NEUR IN, V32
   Sarlin PE, 2021, PROC CVPR IEEE, P3246, DOI 10.1109/CVPR46437.2021.00326
   Sarlin PE, 2020, PROC CVPR IEEE, P4937, DOI 10.1109/CVPR42600.2020.00499
   Sarlin PE, 2019, PROC CVPR IEEE, P12708, DOI 10.1109/CVPR.2019.01300
   Sattler T, 2019, PROC CVPR IEEE, P3297, DOI 10.1109/CVPR.2019.00342
   Sattler T, 2018, PROC CVPR IEEE, P8601, DOI 10.1109/CVPR.2018.00897
   Sattler T, 2017, PROC CVPR IEEE, P6175, DOI 10.1109/CVPR.2017.654
   Sattler T, 2017, IEEE T PATTERN ANAL, V39, P1744, DOI 10.1109/TPAMI.2016.2611662
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Stenborg E, 2018, IEEE INT CONF ROBOT, P6484, DOI 10.1109/ICRA.2018.8463150
   Svärm L, 2017, IEEE T PATTERN ANAL, V39, P1455, DOI 10.1109/TPAMI.2016.2598331
   Toft Carl, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P473, DOI 10.1007/978-3-030-58517-4_28
   Toft C, 2022, IEEE T PATTERN ANAL, V44, P2074, DOI 10.1109/TPAMI.2020.3032010
   Toft C, 2018, LECT NOTES COMPUT SC, V11206, P391, DOI 10.1007/978-3-030-01216-8_24
   Toft C, 2017, IEEE INT CONF COMP V, P650, DOI 10.1109/ICCVW.2017.83
   Torii A, 2015, PROC CVPR IEEE, P1808, DOI 10.1109/CVPR.2015.7298790
   Uy MA, 2018, PROC CVPR IEEE, P4470, DOI 10.1109/CVPR.2018.00470
   Ye T., 2022, IEEE Trans. Instrum. Meas., V71, P1, DOI DOI 10.1109/TIM.2022.3165838
   Young J, 2019, IEEE T VIS COMPUT GR, V25, P1908, DOI 10.1109/TVCG.2019.2898737
   Zhou HY, 2019, IEEE T PATTERN ANAL, V41, P3022, DOI 10.1109/TPAMI.2018.2871832
NR 55
TC 1
Z9 1
U1 5
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2024
VL 20
IS 3
AR 66
DI 10.1145/3622788
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6F8
UT WOS:001153381000006
DA 2024-08-05
ER

PT J
AU Guo, KH
   Chen, L
   Zhu, XY
   Kui, XY
   Zhang, J
   Shi, HY
AF Guo, Kehua
   Chen, Liang
   Zhu, Xiangyuan
   Kui, Xiaoyan
   Zhang, Jian
   Shi, Heyuan
TI Double-Layer Search and Adaptive Pooling Fusion for Reference-Based
   Image Super-Resolution
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Reference-based super-resolution; double-layer search; adaptive pooling
   fusion; structure reconstruction
AB Reference-based image super-resolution (RefSR) aims to reconstruct high-resolution (HR) images from low-resolution (LR) images by introducing HR reference images. The key step of RefSR is to transfer reference features to LR features. However, existing methods still lack an efficient transfer mechanism, resulting in blurry details in the generated image. In this article, we propose a double-layer search module and an adaptive pooling fusion module group for reference-based image super-resolution, called DLASR. Based on the research strategy, the double-layer search module can produce an accurate index map and score map. These two maps are used to filter out accurate reference features, which greatly increases the efficiency of feature transfer in the later stage. Through two continuous feature-enhancement steps, the adaptive pooling fusion module group can transfer more valuable reference features to the corresponding LR features. In addition, a structure reconstruction module is proposed to recover the geometric information of the images, which further improves the visual quality of the generated image. We conduct comparative experiments on a variety of datasets, and the results prove that DLASR achieves significant improvements over other state-of-the-art methods, in terms of quantitative accuracy and qualitative visual effect. The code is available at https: //github.com/clttyou/DLASR.
C1 [Guo, Kehua; Chen, Liang; Zhu, Xiangyuan; Kui, Xiaoyan; Zhang, Jian] Cent South Univ, Sch Comp Sci & Engn, 932 Lushannan Rd, Changsha 410083, Hunan Sheng, Peoples R China.
   [Shi, Heyuan] Tsinghua Univ, KLISS, BNRist, 30 Shuangqing Rd, Beijing 100084, Peoples R China.
C3 Central South University; Tsinghua University
RP Zhang, J (corresponding author), Cent South Univ, Sch Comp Sci & Engn, 932 Lushannan Rd, Changsha 410083, Hunan Sheng, Peoples R China.; Shi, HY (corresponding author), Tsinghua Univ, KLISS, BNRist, 30 Shuangqing Rd, Beijing 100084, Peoples R China.
EM guoke-hua@csu.edu.cn; chenliang1@csu.edu.cn; zhuxiangyuan@csu.edu.cn;
   xykui@csu.edu.cn; hey.shi@foxmail.com
RI li, lin/KEJ-1056-2024; Huang, Yong/KFA-1191-2024; zhang,
   lu/KGL-6144-2024; Liu, Yan/KFQ-1417-2024; Yan, Lu/KHW-7015-2024; li,
   Shang/KHU-3233-2024; chen, xian/KHW-2227-2024; liu, yang/KFA-8402-2024;
   li, yan/KFQ-3850-2024; wang, yue/KDO-9209-2024; Zhang,
   Youyou/KCY-0810-2024; Chen, Bowen/KFB-3986-2024; li,
   xiaomin/KCX-9845-2024; Cheng, Lin/KFQ-3111-2024; Lu, Yi/KEJ-2560-2024;
   Sun, Yang/KHY-5117-2024; Li, Yan/KFQ-9244-2024; Liu,
   Zhiyuan/KDP-2606-2024; li, yf/KHX-1148-2024; lu, Li/KBA-2603-2024; LI,
   yue/KHC-6771-2024; Li, Yang/KFB-5350-2024
OI Zhang, Jian/0000-0001-5418-0455; Zhu, Xiangyuan/0000-0002-1349-3399
FU Natural Science Foundation of China [62076255]; Hunan Provincial Science
   and Technology Plan Project [2020SK2059]; National Science Foundation of
   Hunan Province, China [2019JJ20025, 2019JJ40406]; National Social
   Science Fund of China [20ZD120]; Fundamental Research Funds for the
   Central Universities of the Central South University [2021zzts0751]
FX This work was supported in part by the Natural Science Foundation of
   China under Grant 62076255; in part by the Hunan Provincial Science and
   Technology Plan Project 2020SK2059; in part by the National Science
   Foundation of Hunan Province, China, under Grant 2019JJ20025 and Grant
   2019JJ40406; in part by the National Social Science Fund of China (No.
   20 & ZD120); in part by the Fundamental Research Funds for the Central
   Universities of the Central South University (2021zzts0751).
CR Gatys LA, 2015, Arxiv, DOI arXiv:1505.07376
   Aly HA, 2005, IEEE T IMAGE PROCESS, V14, P1647, DOI 10.1109/TIP.2005.851684
   Boominathan V, 2014, IEEE INT CONF COMPUT
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Farajzadeh N., 2014, SCI COOP INT WORKSH, P82
   Fattal R, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276496, 10.1145/1239451.1239546]
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Goodfellow J., 2014, arXiv, DOI DOI 10.48550/ARXIV.1406.2661
   Guo Y, 2020, PROC CVPR IEEE, P5406, DOI 10.1109/CVPR42600.2020.00545
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He ZL, 2019, IEEE T IMAGE PROCESS, V28, P5464, DOI 10.1109/TIP.2019.2916751
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Jin X, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3417333
   Kingma D. P., 2014, arXiv
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li MY, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3377874
   Li X, 2001, IEEE T IMAGE PROCESS, V10, P1521, DOI 10.1109/83.951537
   Li XG, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3282445
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Lin M, 2014, Arxiv, DOI arXiv:1312.4400
   Liu Q, 2020, REMOTE SENS LETT, V11, P156, DOI 10.1080/2150704X.2019.1693071
   Liu XB, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3465220
   Lowe D. G., 1999, Computer vision, V2, P1150, DOI [10.1109/ICCV.1999.790410, DOI 10.1109/ICCV.1999.790410]
   Lugmayr Andreas, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P715, DOI 10.1007/978-3-030-58558-7_42
   Ma XJ, 2021, PATTERN RECOGN, V110, DOI 10.1016/j.patcog.2020.107332
   Matsui Y, 2017, MULTIMED TOOLS APPL, V76, P21811, DOI 10.1007/s11042-016-4020-z
   Park SC, 2003, IEEE SIGNAL PROC MAG, V20, P21, DOI 10.1109/MSP.2003.1203207
   Sajjadi MSM, 2017, IEEE I CONF COMP VIS, P4501, DOI 10.1109/ICCV.2017.481
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun J, 2008, PROC CVPR IEEE, P2471, DOI 10.1109/CVPR.2008.4587659
   Sun J, 2011, IEEE T IMAGE PROCESS, V20, P1529, DOI 10.1109/TIP.2010.2095871
   Wang XT, 2019, LECT NOTES COMPUT SC, V11133, P63, DOI 10.1007/978-3-030-11021-5_5
   Yan Q, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2414877
   Yang FZ, 2020, PROC CVPR IEEE, P5790, DOI 10.1109/CVPR42600.2020.00583
   Yang WH, 2017, IEEE T IMAGE PROCESS, V26, P5895, DOI 10.1109/TIP.2017.2750403
   Yue HJ, 2013, IEEE T IMAGE PROCESS, V22, P4865, DOI 10.1109/TIP.2013.2279315
   Zeyde R., 2012, INT C CURV SURF, P711
   Zhang DY, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3398685
   Zhang L, 2006, IEEE T IMAGE PROCESS, V15, P2226, DOI 10.1109/TIP.2006.877407
   Zhang WL, 2019, IEEE I CONF COMP VIS, P3096, DOI 10.1109/ICCV.2019.00319
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhang ZF, 2019, PROC CVPR IEEE, P7974, DOI 10.1109/CVPR.2019.00817
   Zhao MD, 2018, IEEE T COMPUT IMAG, V4, P406, DOI 10.1109/TCI.2018.2838457
   Zheng HT, 2018, LECT NOTES COMPUT SC, V11210, P87, DOI 10.1007/978-3-030-01231-1_6
   Zheng Haitian, 2017, P BMVC
   Zhihang Zhong, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P191, DOI 10.1007/978-3-030-58539-6_12
   Zhu XY, 2022, IEEE T CIRC SYST VID, V32, P1273, DOI 10.1109/TCSVT.2021.3078436
   Zhu XY, 2022, IEEE T MULTIMEDIA, V24, P3074, DOI 10.1109/TMM.2021.3092571
   Zhu Y, 2015, PROC CVPR IEEE, P5417, DOI 10.1109/CVPR.2015.7299180
NR 55
TC 1
Z9 1
U1 11
U2 25
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2024
VL 20
IS 1
AR 15
DI 10.1145/3604937
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T8LL3
UT WOS:001080441800015
DA 2024-08-05
ER

PT J
AU Liu, QZ
   Huang, YC
   Jin, CL
   Zhou, XH
   Mao, Y
   Catal, C
   Cheng, L
AF Liu, Qingzhi
   Huang, Yuchen
   Jin, Chenglu
   Zhou, Xiaohan
   Mao, Ying
   Catal, Cagatay
   Cheng, Long
TI Privacy and Integrity Protection for IoT Multimodal Data Using Machine
   Learning and Blockchain
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Blockchain; machine learning; Internet of Things; multimodal data;
   privacy; integrity
ID NOISE; MODEL
AB With the wide application of Internet of Things (IoT) technology, large volumes of multimodal data are collected and analyzed for various diagnoses, analyses, and predictions to help in decision-making and management. However, the research on protecting data integrity and privacy is quite limited, while the lack of proper protection for sensitive data may have significant impacts on the benefits and gains of data owners. In this research, we propose a protection solution for data integrity and privacy. Specifically, our system protects data integrity through distributed systems and blockchain technology. Meanwhile, our system guarantees data privacy using differential privacy and Machine Learning (ML) techniques. Our system aims to maintain the usability of the data for further data analytical tasks of data users, while encrypting the data according to the requirements of data owners. We implement our solution with smart contracts, distributed file systems, and ML models. The experimental results show that our proposed solution can effectively encrypt source IoT data according to the requirements of data users while data integrity can be protected under the blockchain.
C1 [Liu, Qingzhi; Huang, Yuchen; Zhou, Xiaohan] Wageningen Univ & Res, Droevendaalsesteeg 4, NL-6708 PB Wageningen, Netherlands.
   [Jin, Chenglu] Ctr Wiskunde & Informat, Sci Pk 123, NL-1098 XG Amsterdam, Netherlands.
   [Mao, Ying] Fordham Univ, Lincoln Ctr, NY 10023 USA.
   [Catal, Cagatay] Qatar Univ, Doha, Qatar.
   [Cheng, Long] North China Elect Power Univ, Beijing, Peoples R China.
C3 Wageningen University & Research; Qatar University; North China Electric
   Power University
RP Cheng, L (corresponding author), North China Elect Power Univ, Beijing, Peoples R China.
EM qingzhi.liu@wur.nl; yuchen.huang@wur.nl; chenglu.jin@cwi.nl;
   xiaohan.zhou@wur.nl; ymao41@fordham.edu; ccatal@qu.edu.qa;
   lcheng@ncepu.edu.cn
RI Cheng, Long/ADJ-1122-2022; Liu, Qingzhi/HHS-9573-2022
OI Cheng, Long/0000-0003-1638-059X; Liu, Qingzhi/0000-0003-2621-9222; Mao,
   Ying/0000-0002-4484-4892; Catal, Cagatay/0000-0003-0959-2930
CR Abadi M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P308, DOI 10.1145/2976749.2978318
   Afsar MM, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3543846
   Akrami NE, 2023, IEEE ACCESS, V11, P78879, DOI 10.1109/ACCESS.2023.3298371
   Benet J., 2014, arXiv
   Boritz J. E., 2005, International Journal of Accounting Information Systems, V6, P260, DOI 10.1016/j.accinf.2005.07.001
   Cao YN, 2023, COMPUT STAND INTER, V85, DOI 10.1016/j.csi.2022.103708
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Dagar Rahul, 2018, 2018 International Conference on Inventive Research in Computing Applications (ICIRCA). Proceedings, P1052, DOI 10.1109/ICIRCA.2018.8597264
   Dai JB, 2023, COMPUT NETW, V225, DOI 10.1016/j.comnet.2023.109652
   Du JB, 2022, IEEE T NETW SCI ENG, V9, P33, DOI 10.1109/TNSE.2021.3068340
   Duan L, 2023, J SYST ARCHITECT, V140, DOI 10.1016/j.sysarc.2023.102897
   Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14
   El Ouadrhiri A, 2022, IEEE ACCESS, V10, P22359, DOI 10.1109/ACCESS.2022.3151670
   Elsherbiny O, 2022, COMPUT ELECTRON AGR, V203, DOI 10.1016/j.compag.2022.107453
   Gaetani Edoardo, 2017, IT C CYB ITASEC VEN
   Ge Y, 2020, NEURAL COMPUT APPL, V32, P16843, DOI 10.1007/s00521-018-03970-4
   Geng Q, 2016, IEEE T INFORM THEORY, V62, P925, DOI 10.1109/TIT.2015.2504967
   Ghosh A, 2020, J NETW COMPUT APPL, V163, DOI 10.1016/j.jnca.2020.102635
   Katzin D, 2020, BIOSYST ENG, V194, P61, DOI 10.1016/j.biosystemseng.2020.03.010
   Kushwaha SS, 2022, IEEE ACCESS, V10, P6605, DOI 10.1109/ACCESS.2021.3140091
   Lee J., 2012, P 18 ACM SIGKDD INT, P1041, DOI DOI 10.1145/2339530.2339695
   Li Ninghui, 2013, ACM SIGSAC C COMP CO, P889
   Li XY, 2024, IEEE INTERNET THINGS, V11, P3392, DOI 10.1109/JIOT.2023.3296460
   Liu JW, 2018, IEEE GLOBAL C COMMUN
   Liu QZ, 2022, IEEE T PARALL DISTR, V33, P1491, DOI 10.1109/TPDS.2021.3116863
   Liu QZ, 2021, IEEE NETWORK, V35, P112, DOI 10.1109/MNET.011.2000303
   Mnih V, 2013, Arxiv, DOI arXiv:1312.5602
   Mohamed ES, 2021, EGYPT J REMOTE SENS, V24, P971, DOI 10.1016/j.ejrs.2021.08.007
   Monrat AA, 2019, IEEE ACCESS, V7, P117134, DOI 10.1109/ACCESS.2019.2936094
   Rifi N, 2017, 2017 FOURTH INTERNATIONAL CONFERENCE ON ADVANCES IN BIOMEDICAL ENGINEERING (ICABME), P198
   Sarathy R, 2011, TRANS DATA PRIV, V4, P1
   Shan ZH, 2018, ACM COMPUT SURV, V51, DOI 10.1145/3158363
   Sweeney L, 2002, INT J UNCERTAIN FUZZ, V10, P557, DOI 10.1142/S0218488502001648
   Vujiic D., 2018, 2018 17 INT S INFOTE, P1, DOI DOI 10.1109/INFOTEH.2018.8345547
   Wang SP, 2018, IEEE ACCESS, V6, P38437, DOI 10.1109/ACCESS.2018.2851611
   Wen BD, 2023, INFORM SCIENCES, V645, DOI 10.1016/j.ins.2023.119322
   Wiseman L, 2019, NJAS-WAGEN J LIFE SC, V90-91, DOI 10.1016/j.njas.2019.04.007
   Yang CX, 2020, IEEE ACCESS, V8, P70604, DOI 10.1109/ACCESS.2020.2985762
   Zhang JB, 2016, 24TH ACM SIGSPATIAL INTERNATIONAL CONFERENCE ON ADVANCES IN GEOGRAPHIC INFORMATION SYSTEMS (ACM SIGSPATIAL GIS 2016), DOI 10.1145/2996913.2997016
   Zhao Y, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3490237
NR 40
TC 3
Z9 3
U1 2
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2024
VL 20
IS 6
SI SI
AR 153
DI 10.1145/3638769
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OQ2T3
UT WOS:001208681800003
DA 2024-08-05
ER

PT J
AU Floris, A
   Porcu, S
   Atzori, L
AF Floris, Alessandro
   Porcu, Simone
   Atzori, Luigi
TI Controlling Media Player with Hands: A Transformer Approach and a
   Quality of Experience Assessment
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Human-computer interface; hand gesture recognition; quality of
   experience; transformer neural network; media player
ID NEURAL-NETWORKS; RECOGNITION
AB In this article, we propose a Hand Gesture Recognition (HGR) system based on a novel deep transformer (DT) neural network for media player control. The extracted hand skeleton features are processed by separate transformers for each finger in isolation to better identify the finger characteristics to drive the following classification. The achieved HGR accuracy (0.853) outperforms state-of-the-art HGR approaches when tested on the popular NVIDIA dataset. Moreover, we conducted a subjective assessment involving 30 people to evaluate the Quality of Experience (QoE) provided by the proposed DT-HGR for controlling a media player application compared with two traditional input devices, i.e., mouse and keyboard. The assessment participants were asked to evaluate objective (accuracy) and subjective (physical fatigue, usability, pragmatic quality, and hedonic quality) measurements. We found that (i) the accuracy of DT-HGR is very high (91.67%), only slightly lower than that of traditional alternative interaction modalities; and that (ii) the perceived quality for DT-HGR in terms of satisfaction, comfort, and interactivity is very high, with an average Mean Opinion Score (MOS) value as high as 4.4, whereas the alternative approaches did not reach 3.8, which encourages a more pervasive adoption of the natural gesture interaction.
C1 [Floris, Alessandro; Porcu, Simone; Atzori, Luigi] Univ Cagliari, DIEE, Via Marengo 2, I-09123 Cagliari, Italy.
   [Floris, Alessandro; Porcu, Simone; Atzori, Luigi] Univ Cagliari, CNIT, Via Marengo 2, I-09123 Cagliari, Italy.
C3 University of Cagliari; University of Cagliari
RP Floris, A (corresponding author), Univ Cagliari, DIEE, Via Marengo 2, I-09123 Cagliari, Italy.; Floris, A (corresponding author), Univ Cagliari, CNIT, Via Marengo 2, I-09123 Cagliari, Italy.
EM alessandro.floris84@unica.it; simone.porcu@unica.it; l.atzori@unica.it
RI ; Floris, Alessandro/L-6707-2018
OI PORCU, SIMONE/0000-0003-0792-1200; Floris,
   Alessandro/0000-0002-8745-1327
FU European Union under the Italian National Recovery and Resilience Plan
   (NRRP) of NextGenerationEU, "Sustainable Mobility Center" (Centro
   Nazionale per la Mobilit Sostenibile [CNMS] [CN_00000023]; PON "Ricerca
   e Innovazione" 2014-2020 (PON R&I)"Azione IV.4 Dottorati e contratti di
   ricerca su tematiche dell'innovazione" [1062]
FX This work has been partially supported by the European Union under the
   Italian National Recovery and Resilience Plan (NRRP) of
   NextGenerationEU, "Sustainable Mobility Center" (Centro Nazionale per la
   Mobilit Sostenibile [CNMS], grant no. CN_00000023), and by the PON
   "Ricerca e Innovazione" 2014-2020 (PON R&I)"Azione IV.4 Dottorati e
   contratti di ricerca su tematiche dell'innovazione" with D.M. 1062 on
   10.08.2021.
CR Abavisani M, 2019, PROC CVPR IEEE, P1165, DOI 10.1109/CVPR.2019.00126
   Agarwal S, 2015, NAT CONF COMPUT VIS
   Agrawal A, 2013, 2013 IEEE CONFERENCE ON INFORMATION AND COMMUNICATION TECHNOLOGIES (ICT 2013), P1288
   Avola D, 2022, PATTERN RECOGN, V129, DOI 10.1016/j.patcog.2022.108762
   Avola D, 2019, IEEE T MULTIMEDIA, V21, P234, DOI 10.1109/TMM.2018.2856094
   Barclay Kayne, 2011, PROC 23 AUSTR COMPUT, P31, DOI [10.1145/2071536.2071540, DOI 10.1145/2071536.2071540]
   Bazarevsky Valentin, 2019, CVPR WORKSHOP COMPUT
   BORG GAV, 1982, MED SCI SPORT EXER, V14, P377, DOI 10.1249/00005768-198205000-00012
   Brooke J., 1996, SUS-a quick and dirty usability scale, DOI [DOI 10.1201/9781498710411-35, DOI 10.1201/9781498710411]
   Buchinger Shelley, 2010, P 18 ACM INT C MULTI, P699, DOI [10.1145/1873951.1874055, DOI 10.1145/1873951.1874055]
   D'Eusanio A, 2020, INT CONF 3D VISION, P623, DOI 10.1109/3DV50981.2020.00072
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Fronteddu G, 2022, COMPUT NETW, V205, DOI 10.1016/j.comnet.2022.108781
   Gao Q, 2022, IEEE SENS J, V22, P17421, DOI 10.1109/JSEN.2021.3059685
   Gupta V, 2019, INT CONF 3D VISION, P289, DOI 10.1109/3DV.2019.00040
   Husic JB, 2020, INT J NETW MANAG, V30, DOI 10.1002/nem.2083
   Iorga C, 2019, INT C ELECT COMPUT, DOI 10.1109/ecai46879.2019.9042173
   ITU, 2008, SUBJ VID QUAL ASS ME
   Joy E, 2018, PROCEEDINGS OF THE 2018 SECOND INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING AND CONTROL SYSTEMS (ICICCS), P56, DOI 10.1109/ICCONS.2018.8662901
   Köpüklü O, 2019, IEEE INT CONF AUTOMA, P407
   Laugwitz B, 2008, LECT NOTES COMPUT SC, V5298, P63, DOI 10.1007/978-3-540-89350-9_6
   LaViola Jr Joseph J., 2014, Multimodal Input for Perceptual User Interfaces, P285, DOI [10.1002/9781118706237.ch9, DOI 10.1002/9781118706237.CH9]
   Le Callet P., 2013, Qualinet White Paper on Definitions of Quality of Experience
   Li Y., 2019, Virtual Reality Intell. Hardw., V1, P84, DOI [10.3724/SP.J.2096-5796.2018.0006, DOI 10.3724/SP.J.2096-5796.2018.0006]
   Liu Shuying, 2022, 2022 IEEECVF C COMPU, P1
   Molchanov P, 2016, PROC CVPR IEEE, P4207, DOI 10.1109/CVPR.2016.456
   Nagalapuram Gayathri Devi, 2021, 2021 IEEE Mysore Sub Section International Conference (MysuruCon), P79, DOI 10.1109/MysuruCon52639.2021.9641567
   Núñez JC, 2018, PATTERN RECOGN, V76, P80, DOI 10.1016/j.patcog.2017.10.033
   Ohn-Bar E, 2014, IEEE T INTELL TRANSP, V15, P2368, DOI 10.1109/TITS.2014.2337331
   Paliwal M, 2013, 2013 INTERNATIONAL CONFERENCE ON ADVANCES IN TECHNOLOGY AND ENGINEERING (ICATE)
   Park SM, 2022, IEEE ACCESS, V10, P4209, DOI 10.1109/ACCESS.2021.3140175
   Peng C, 2018, 2018 IEEE GAMES, ENTERTAINMENT, MEDIA CONFERENCE (GEM), P453, DOI 10.1109/GEM.2018.8516520
   Pirker J., 2017, ACM CHI, P620, DOI 10.1007/978-3-319-58071-5
   Rautaray S., 2010, International Journal of Computer Applications, V10, P11
   Rautaray SS, 2015, ARTIF INTELL REV, V43, P1, DOI 10.1007/s10462-012-9356-9
   Simon T, 2017, PROC CVPR IEEE, P4645, DOI 10.1109/CVPR.2017.494
   Simonyan K, 2014, ADV NEUR IN, V27
   Trojaniello Diana, 2018, HEALTHINFO 2018 3 IN, P36
   Turunen M, 2009, INTERSPEECH 2009: 10TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2009, VOLS 1-5, P2551
   van Beurden M.H. P. H., 2012, GESTURE SIGN LANGUAG, P36
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang H, 2016, INT J COMPUT VISION, V119, P219, DOI 10.1007/s11263-015-0846-5
   Wechsung I, 2012, J MULTIMODAL USER IN, V6, P73, DOI 10.1007/s12193-011-0088-y
   Yang XD, 2018, PROC CVPR IEEE, P6469, DOI 10.1109/CVPR.2018.00677
   Yu ZT, 2021, IEEE T IMAGE PROCESS, V30, P5626, DOI 10.1109/TIP.2021.3087348
   Zhang Fan, 2020, CVPR WORKSHOP COMPUT
NR 46
TC 0
Z9 0
U1 3
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2024
VL 20
IS 5
AR 132
DI 10.1145/3638560
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MF3R9
UT WOS:001192177900012
OA hybrid, Green Published
DA 2024-08-05
ER

PT J
AU Yi, P
   Wang, ZY
   Luo, LA
   Jiang, K
   He, Z
   Jiang, JJ
   Lu, T
   Ma, JY
AF Yi, Peng
   Wang, Zhongyuan
   Luo, Laigan
   Jiang, Kui
   He, Zheng
   Jiang, Junjun
   Lu, Tao
   Ma, Jiayi
TI Omniscient Video Super-Resolution with Explicit-Implicit Alignment
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Convolutional Neural Network; Video Super-Resolution; Omniscient
   Framework; Explicit-Implicit Alignment
ID IMAGE SUPERRESOLUTION
AB When considering the temporal relationships, most previous video super-resolution (VSR) methods follow the iterative or recurrent framework. The iterative framework adopts neighboring low-resolution (LR) frames from a sliding window, while the recurrent framework utilizes the output generated in the previous SR procedure. The hybrid framework combines them but still cannot fully leverage the temporal relationships. Meanwhile, the existing methods are limited in the receptive field of the optical flow or lack semantic constrains on motion information. In this work, we propose an omniscient framework to fully explore the temporal relationships in the video, which encompasses both LR frames and SR outputs from the past, present, and future. The omniscient framework is more generic because the iterative, recurrent, and hybrid frameworks can be regarded as its special cases. Besides, when addressing the motion information, most previous VSR methods adopt the explicit motion estimation and compensation, while many recent methods turn to implicit alignment. In implicit alignment methods, because basic non-local means suffers from heavy computational costs, we improve it by capturing the non-local correlations in a relatively local manner to reduce the complexity. Moreover, we integrate the explicit and implicit methods into an explicit-implicit alignment module to better utilize motion information. We have conducted extensive experiments on public datasets, which show that our method is superior over the state-of-the-art methods in objective metrics, subjective visual quality, and complexity. In particular, on datasets of Vid4 and UDM10, our method improves PSNR by 0.19 dB, 0.49 dB against the most advanced method BasicVSR++, respectively.
C1 [Yi, Peng; Wang, Zhongyuan] Wuhan Univ, Sch Comp Sci, Wuhan, Peoples R China.
   [Luo, Laigan] Wuhan Univ, Elect Informat Sch, Wuhan, Peoples R China.
   [Jiang, Kui; He, Zheng] Wuhan Univ, Sch Comp Sci, Wuhan, Peoples R China.
   [Jiang, Junjun] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin, Peoples R China.
   [Lu, Tao] Wuhan Inst Technol, Sch Comp Sci & Engn, Wuhan, Peoples R China.
   [Ma, Jiayi] Wuhan Univ, Elect Informat Sch, Wuhan, Peoples R China.
   [Yi, Peng; Wang, Zhongyuan] Wuhan Univ, Sch Comp Sci, Wuhan 430072, Hubei, Peoples R China.
   [Luo, Laigan; Jiang, Junjun] Wuhan Univ, Elect Informat Sch, Wuhan 430072, Hubei, Peoples R China.
   [Jiang, Junjun] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150006, Heilongjiang, Peoples R China.
   [Lu, Tao] Wuhan Inst Technol, Sch Comp Sci & Engn, Wuhan 430205, Hubei, Peoples R China.
C3 Wuhan University; Wuhan University; Wuhan University; Harbin Institute
   of Technology; Wuhan Institute of Technology; Wuhan University; Wuhan
   University; Wuhan University; Harbin Institute of Technology; Wuhan
   Institute of Technology
RP Wang, ZY (corresponding author), Wuhan Univ, Sch Comp Sci, Wuhan 430072, Hubei, Peoples R China.
EM yipeng@whu.edu.cn; wzy_hope@163.com; luolaigan@whu.edu.cn;
   kuijiang@whu.edu.cn; hezheng@whu.edu.cn; junjun0595@163.com;
   lutxyl@gmail.com; jyma2010@gmail.com
RI Jiang, Junjun/L-7087-2019
OI Jiang, Junjun/0000-0002-5694-505X; Lu, Tao/0000-0001-8117-2012; Jiang,
   Kui/0000-0002-4055-7503
FU National Natural Science Foundation of China [62071339, 62371350,
   62072350, 62171328, U23B2009]; Key R&D Program of Hubei Province
   [2022BAA079]
FX This research was funded by the National Natural Science Foundation of
   China (62071339, 62371350, 62072350, 62171328, U23B2009) and Key R&D
   Program of Hubei Province (2022BAA079).
CR Bao WB, 2021, IEEE T PATTERN ANAL, V43, P933, DOI 10.1109/TPAMI.2019.2941941
   Belekos SP, 2010, IEEE T IMAGE PROCESS, V19, P1451, DOI 10.1109/TIP.2010.2042115
   Caballero J, 2017, PROC CVPR IEEE, P2848, DOI 10.1109/CVPR.2017.304
   Chan KCK, 2022, PROC CVPR IEEE, P5962, DOI 10.1109/CVPR52688.2022.00588
   Chan KCK, 2021, PROC CVPR IEEE, P4945, DOI 10.1109/CVPR46437.2021.00491
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Farrugia RA, 2020, IEEE T PATTERN ANAL, V42, P1162, DOI 10.1109/TPAMI.2019.2893666
   Fuoli D, 2019, IEEE INT CONF COMP V, P3476, DOI 10.1109/ICCVW.2019.00431
   Haris M, 2019, PROC CVPR IEEE, P3892, DOI 10.1109/CVPR.2019.00402
   Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179
   Hu MS, 2023, IEEE T PATTERN ANAL, V45, P13376, DOI 10.1109/TPAMI.2023.3293522
   Isobe Takashi, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P645, DOI 10.1007/978-3-030-58610-2_38
   Isobe Takashi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8005, DOI 10.1109/CVPR42600.2020.00803
   Jiang K, 2022, IEEE T NEUR NET LEAR, V33, P378, DOI 10.1109/TNNLS.2020.3027849
   Jiang K, 2020, IEEE T MULTIMEDIA, V22, P2734, DOI 10.1109/TMM.2019.2960586
   Jiang K, 2019, IEEE T GEOSCI REMOTE, V57, P5799, DOI 10.1109/TGRS.2019.2902431
   Jo Y, 2018, PROC CVPR IEEE, P3224, DOI 10.1109/CVPR.2018.00340
   Kappeler A, 2016, IEEE T COMPUT IMAG, V2, P109, DOI 10.1109/TCI.2016.2532323
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kingma D. P., 2014, arXiv
   Kwon Y, 2015, IEEE T PATTERN ANAL, V37, P1792, DOI 10.1109/TPAMI.2015.2389797
   Lai WS, 2019, IEEE T PATTERN ANAL, V41, P2599, DOI 10.1109/TPAMI.2018.2865304
   Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li DY, 2017, IEEE T COMPUT IMAG, V3, P749, DOI 10.1109/TCI.2017.2671360
   Liu C, 2014, IEEE T PATTERN ANAL, V36, P346, DOI 10.1109/TPAMI.2013.127
   Liu D, 2018, IEEE T IMAGE PROCESS, V27, P3432, DOI 10.1109/TIP.2018.2820807
   Liu D, 2017, IEEE I CONF COMP VIS, P2526, DOI 10.1109/ICCV.2017.274
   Nah S, 2019, IEEE COMPUT SOC CONF, P1996, DOI 10.1109/CVPRW.2019.00251
   Sajjadi MSM, 2018, PROC CVPR IEEE, P6626, DOI 10.1109/CVPR.2018.00693
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Shi XJ, 2015, ADV NEUR IN, V28
   Tao X, 2017, IEEE I CONF COMP VIS, P4482, DOI 10.1109/ICCV.2017.479
   Tian YP, 2020, PROC CVPR IEEE, P3357, DOI 10.1109/CVPR42600.2020.00342
   Tong T, 2017, IEEE I CONF COMP VIS, P4809, DOI 10.1109/ICCV.2017.514
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang XT, 2019, IEEE COMPUT SOC CONF, P1954, DOI 10.1109/CVPRW.2019.00247
   Wang XT, 2019, LECT NOTES COMPUT SC, V11133, P63, DOI 10.1007/978-3-030-11021-5_5
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZH, 2021, IEEE T PATTERN ANAL, V43, P3365, DOI 10.1109/TPAMI.2020.2982166
   Wang ZY, 2019, IEEE T IMAGE PROCESS, V28, P2530, DOI 10.1109/TIP.2018.2887017
   Xia B, 2023, PROC CVPR IEEE, P22638, DOI 10.1109/CVPR52729.2023.02168
   Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2
   Yan B, 2019, AAAI CONF ARTIF INTE, P5597
   Yi P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4409, DOI 10.1109/ICCV48922.2021.00439
   Yi P, 2022, IEEE T PATTERN ANAL, V44, P2264, DOI 10.1109/TPAMI.2020.3042298
   Yi P, 2019, IEEE I CONF COMP VIS, P3106, DOI 10.1109/ICCV.2019.00320
   Yi P, 2020, IEEE T CIRC SYST VID, V30, P2503, DOI 10.1109/TCSVT.2019.2925844
   Yu X, 2020, IEEE T PATTERN ANAL, V42, P2926, DOI 10.1109/TPAMI.2019.2916881
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhang ZD, 2019, IEEE T IMAGE PROCESS, V28, P1625, DOI 10.1109/TIP.2018.2877483
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
NR 55
TC 0
Z9 0
U1 4
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2024
VL 20
IS 5
AR 150
DI 10.1145/3640346
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MF3R9
UT WOS:001192177900030
DA 2024-08-05
ER

PT J
AU Guo, Q
   Zhang, Z
   Zhou, ML
   Yue, H
   Pu, HY
   Luo, J
AF Guo, Qiang
   Zhang, Zhi
   Zhou, Mingliang
   Yue, Hong
   Pu, Huayan
   Luo, Jun
TI Image Defogging Based on Regional Gradient Constrained Prior
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image defogging; transmission estimation prior; regional gradient
   constraint function
ID WEATHER
AB Foggy days limit the functionality of outdoor surveillance systems. However, it is still a challenge for existing methods to maintain the uniformity of defogging between image regions with a similar depth of field and large differences in appearance. To address above problem, this article proposes a regional gradient constrained prior (RGCP) for defogging that uses the piecewise smoothing characteristic of the scene structure to achieve accurate estimation and reliable constraint of the transmission. RGCP first derives that when adjacent similar pixels in the fog image are aggregated and spatially divided into regions, clusters of region pixels in RGB space conform to a chi-square distribution. The offset of the confidence boundary of the clusters can be regarded as the initial transmission of each region. RGCP further uses a gradient distribution to distinguish different regional appearances and formulate an interregional constraint function to constrain the overestimation of the transmission in the flat region, thereby maintaining the consistency between the estimated transmission map and the depth map. The experimental results demonstrate that the proposed method can achieve natural defogging performance in terms of various foggy conditions.
C1 [Guo, Qiang] Beihang Univ, Beijing, Peoples R China.
   [Zhang, Zhi] Civil Aviat Univ China, Tianjin, Peoples R China.
   [Zhou, Mingliang] Chongqing Univ, Sch Comp Sci, Chongqing 400044, Peoples R China.
   [Yue, Hong] CICT Connected & Intelligent Technol Co Ltd, Chongqing, Peoples R China.
   [Pu, Huayan; Luo, Jun] Chongqing Univ, Coll Mech Engn, State Key Lab Mech & Transmiss, Chongqing 400044, Peoples R China.
C3 Beihang University; Civil Aviation University of China; Chongqing
   University; Chongqing University
RP Zhou, ML (corresponding author), Chongqing Univ, Sch Comp Sci, Chongqing 400044, Peoples R China.; Pu, HY; Luo, J (corresponding author), Chongqing Univ, Coll Mech Engn, State Key Lab Mech & Transmiss, Chongqing 400044, Peoples R China.
EM gq2016@buaa.edu.cn; zhangz@cauc.edu.cn; mingliangzhou@cqu.edu.cn;
   yuehong3@126.com; phygood_2001@shu.edu.cn; luojun@cqu.edu.cn
RI Zhou, Mingliang/HPC-0298-2023
OI Zhou, Mingliang/0000-0002-1874-3641
FU National Natural Science Foundation of China [62176027]; Joint Equipment
   Pre Research and Key Fund Project of the Ministry of Education
   [8091B012207]; Human Resources and Social Security Bureau Project of
   Chongqing [cx2020073]; Guangdong Oppo Mobile Telecommunications
   Corporation Ltd. [H20221694]; Natural Science Foundation of Chongqing,
   China [cstc2020jcyj-zdxmX0014]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62176027, in part by the Joint Equipment
   Pre Research and Key Fund Project of the Ministry of Education under
   Grant 8091B012207, in part by the Human Resources and Social Security
   Bureau Project of Chongqing under Grant cx2020073, in part by the
   Guangdong Oppo Mobile Telecommunications Corporation Ltd., under Grant
   H20221694, in part by Natural Science Foundation of Chongqing, China,
   under Grant cstc2020jcyj-zdxmX0014.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Ancuti CO, 2019, IEEE IMAGE PROC, P1014, DOI [10.1109/ICIP.2019.8803046, 10.1109/icip.2019.8803046]
   Ancuti CO, 2018, IEEE COMPUT SOC CONF, P867, DOI 10.1109/CVPRW.2018.00119
   Baiju PS, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3465454
   Berman D, 2020, IEEE T PATTERN ANAL, V42, P720, DOI 10.1109/TPAMI.2018.2882478
   Bui TM, 2018, IEEE T IMAGE PROCESS, V27, P999, DOI 10.1109/TIP.2017.2771158
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chen BH, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2726947
   Chen WT, 2019, PROC CVPR IEEE, P11673, DOI 10.1109/CVPR.2019.01195
   Choi LK, 2015, IEEE T IMAGE PROCESS, V24, P3888, DOI 10.1109/TIP.2015.2456502
   Gao YY, 2019, IEEE T MULTIMEDIA, V21, P351, DOI 10.1109/TMM.2018.2856095
   Gibson KB, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-37
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Hautiere Nicolas, 2008, Image Analysis & Stereology, V27, P87, DOI 10.5566/ias.v27.p87-95
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Hide R., 1977, Phys. Bull., V28, P11
   Hu HM, 2020, IEEE T MULTIMEDIA, V22, P1485, DOI 10.1109/TMM.2019.2944260
   Hu HM, 2019, IEEE T IMAGE PROCESS, V28, P2882, DOI 10.1109/TIP.2019.2891901
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Ju MY, 2021, IEEE T IMAGE PROCESS, V30, P9043, DOI 10.1109/TIP.2021.3122088
   Ju MY, 2021, IEEE T IMAGE PROCESS, V30, P2180, DOI 10.1109/TIP.2021.3050643
   KLINKER GJ, 1990, INT J COMPUT VISION, V4, P7, DOI 10.1007/BF00137441
   Li BY, 2019, IEEE T IMAGE PROCESS, V28, P492, DOI 10.1109/TIP.2018.2867951
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li MD, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3341728
   Li RD, 2018, PROC CVPR IEEE, P8202, DOI 10.1109/CVPR.2018.00856
   Liu PJ, 2019, IEEE T IMAGE PROCESS, V28, P2212, DOI 10.1109/TIP.2018.2823424
   Liu XH, 2019, IEEE I CONF COMP VIS, P7313, DOI 10.1109/ICCV.2019.00741
   Luo WX, 2021, IEEE T PATTERN ANAL, V43, P1070, DOI 10.1109/TPAMI.2019.2944377
   Meng GF, 2013, IEEE I CONF COMP VIS, P617, DOI 10.1109/ICCV.2013.82
   Narasimhan SG, 2003, IEEE T PATTERN ANAL, V25, P713, DOI 10.1109/TPAMI.2003.1201821
   Qu YY, 2019, PROC CVPR IEEE, P8152, DOI 10.1109/CVPR.2019.00835
   Santra S, 2018, IEEE T IMAGE PROCESS, V27, P4598, DOI 10.1109/TIP.2018.2841198
   Shao YJ, 2020, PROC CVPR IEEE, P2805, DOI 10.1109/CVPR42600.2020.00288
   Storath M, 2015, INVERSE PROBL, V31, DOI 10.1088/0266-5611/31/2/025003
   Suen PH, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P454, DOI 10.1109/ICCV.2001.937660
   Sun ZY, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3478457
   Tan RT, 2008, PROC CVPR IEEE, P2347, DOI 10.1109/cvpr.2008.4587643
   Tang YX, 2018, IEEE T PATTERN ANAL, V40, P3045, DOI 10.1109/TPAMI.2017.2771779
   You Y, 2019, IEEE T IMAGE PROCESS, V28, P45, DOI 10.1109/TIP.2018.2857219
   Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 42
TC 0
Z9 0
U1 8
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2024
VL 20
IS 3
AR 64
DI 10.1145/3617834
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6F8
UT WOS:001153381000004
DA 2024-08-05
ER

PT J
AU Wen, WY
   Huang, MH
   Zhang, YS
   Fang, YM
   Zuo, YF
AF Wen, Wenying
   Huang, Minghui
   Zhang, Yushu
   Fang, Yuming
   Zuo, Yifan
TI Visual Security Index Combining CNN and Filter for Perceptually
   Encrypted Light Field Images
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Light field image; convolutional neural networks; gabor filter;
   epi-polar plane image
ID QUALITY ASSESSMENT; SIMILARITY
AB Visual security index (VSI) represents a quantitative index for the visual security evaluation of perceptually encrypted images. Recently, the research on visual security of encrypted light field (LF) images faces two challenges. One is that the existing perceptually encrypted image databases are often too small, which is easy to cause overfitting in convolutional neural network (CNN). The other is that existing VSImodels did not take a full account the intrinsic characteristics of the LF images and highly relied on handcrafted feature extraction. In this article, we construct a new database of perceptually encrypted LF images, called the PE-SLF, which is 2.6 times as big as the existing largest perceptual encrypted image database. Moreover, a novel visual security index (VSI) model is proposed by taking into full consideration the intrinsic spatial-angular characteristics of the LF images and the outstanding capabilities of CNN in feature extraction. First, we exploit CNN to detect the texture and structure features of encrypted sub-aperture images in the spatial domain. Second, we apply the Gabor filter to detect the Gabor feature over the epi-polar plane images in angular domain. Last, the spatial and angular similarity measurements are subsequently calculated for jointly yielding the final visual security score. Experimental results on the constructed PE-SLF demonstrate that the proposed VSI model is closer to the perception of HVS in visual security evaluation of encrypted LF images compared to other classical and state-of-the-art models.
C1 [Wen, Wenying; Huang, Minghui; Fang, Yuming; Zuo, Yifan] Jiangxi Univ Finance & Econ, Sch Informat Technol, Yuping Ave,Changbei Natl Econ & Technol Dev Zone, Nanchang, Jiangxi, Peoples R China.
   [Fang, Yuming] Nanjing Univ Aeronaut & Astronaut, Sch Comp Sci & Technol, 29 Gen Ave, Nanjing 211106, Jiangsu, Peoples R China.
C3 Jiangxi University of Finance & Economics; Nanjing University of
   Aeronautics & Astronautics
RP Fang, YM (corresponding author), Nanjing Univ Aeronaut & Astronaut, Sch Comp Sci & Technol, 29 Gen Ave, Nanjing 211106, Jiangsu, Peoples R China.
EM wenyingwen@sina.cn; hmh0423@163.com; leo.fangyuming@foxmail.com;
   yushu@nuaa.edu.cn; kenny0410@126.com
RI Zuo, Yifan/JVZ-3041-2024
OI Zuo, Yifan/0000-0003-4980-7211; zhang, yushu/0000-0001-8183-8435; Wen,
   wenying/0000-0002-3098-4640
FU Natural Science Foundation of China [62201233, 61961022]; Double
   Thousand Plan of Jiangxi Province [jxsq2023201118]; Outstanding Youth
   Fund Program of Jiangxi Province [20232ACB212004]; Natural Science
   Foundation of Jiangxi Province [20224BAB211002]
FX This work is partially supported by the Natural Science Foundation of
   China under Grants 62201233 and 61961022, the Double Thousand Plan of
   Jiangxi Province under Grant jxsq2023201118, the Outstanding Youth Fund
   Program of Jiangxi Province under Grant 20232ACB212004, and the Natural
   Science Foundation of Jiangxi Province under Grant 20224BAB211002.
CR Adhikarla VK, 2017, PROC CVPR IEEE, P3720, DOI 10.1109/CVPR.2017.396
   Autrusseau Florent, 2010, Journal of the American Chemical Society, V2010
   BOLLES RC, 1987, INT J COMPUT VISION, V1, P7, DOI 10.1007/BF00128525
   Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Chang JH, 2022, IEEE T IMAGE PROCESS, V31, P2809, DOI 10.1109/TIP.2022.3159477
   Chen CLZ, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3447393
   Cheraaqee P, 2022, IEEE T CIRC SYST VID, V32, P566, DOI 10.1109/TCSVT.2021.3067627
   Ding KY, 2022, IEEE T PATTERN ANAL, V44, P2567, DOI 10.1109/TPAMI.2020.3045810
   ElSayed A, 2015, 2015 IEEE LONG ISLAND SYSTEMS, APPLICATIONS AND TECHNOLOGY CONFERENCE (LISAT)
   Engel Dominik, 2007, Proceedings 2007 IEEE International Conference on Image Processing, ICIP 2007, pII
   Fang YM, 2018, 2018 IEEE FOURTH INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM)
   FIELD DJ, 1987, J OPT SOC AM A, V4, P2379, DOI 10.1364/JOSAA.4.002379
   Flusser J, 2016, IEEE T IMAGE PROCESS, V25, P790, DOI 10.1109/TIP.2015.2512108
   Gatys LA, 2015, ADV NEUR IN, V28
   Guo CE, 2007, COMPUT VIS IMAGE UND, V106, P5, DOI 10.1016/j.cviu.2005.09.004
   Guo SW, 2020, IEEE T INF FOREN SEC, V15, P1151, DOI 10.1109/TIFS.2019.2935415
   Hofbauer H, 2021, IEEE T MULTIMEDIA, V24, P3595, DOI 10.1109/TMM.2021.3103394
   Hofbauer H, 2016, SIGNAL PROCESS-IMAGE, V46, P60, DOI 10.1016/j.image.2016.05.001
   Huang HL, 2022, IEEE T IMAGE PROCESS, V31, P3765, DOI 10.1109/TIP.2022.3175619
   Jiang QP, 2022, IEEE T CIRC SYST VID, V32, P5959, DOI 10.1109/TCSVT.2022.3164918
   Kauba C., 2016, PROC ACM WORKSHOP IN, P175
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Liu AM, 2012, IEEE T IMAGE PROCESS, V21, P1500, DOI 10.1109/TIP.2011.2175935
   Mao YN, 2004, IEEE IMAGE PROC, P569
   Marr D., 1982, Vision. A computational investigation into the human representation and processing of visual information
   Meng CL, 2021, IEEE T MULTIMEDIA, V24, P3193, DOI 10.1109/TMM.2021.3096071
   Ni ZK, 2017, IEEE T IMAGE PROCESS, V26, P4818, DOI 10.1109/TIP.2017.2718185
   Ponomarenko N., 2009, Advances of Modern Radioelectronics, V10, P30
   Seo S, 2021, IEEE T CIRC SYST VID, V31, P2602, DOI 10.1109/TCSVT.2020.3030895
   Sheikh Hamid R, 2005, LIVE IMAGE QUALITY A, DOI DOI 10.1109/CVPR.2015.7298594
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh KN, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3498342
   Sun J, 2011, MULTIMED TOOLS APPL, V53, P75, DOI 10.1007/s11042-010-0491-5
   Sun W, 2017, PATTERN RECOGN, V61, P153, DOI 10.1016/j.patcog.2016.07.033
   Tian Y, 2021, IEEE T CIRC SYST VID, V31, P2046, DOI 10.1109/TCSVT.2020.2971256
   Tian Y, 2020, IEEE T IMAGE PROCESS, V29, P7945, DOI 10.1109/TIP.2020.3008856
   Tsiligkaridis T, 2013, IEEE T AUDIO SPEECH, V21, P2255, DOI 10.1109/TASL.2013.2271592
   Virtanen T, 2015, IEEE T IMAGE PROCESS, V24, P390, DOI 10.1109/TIP.2014.2378061
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435
   Wen WY, 2021, IEEE T CIRC SYST VID, V31, P2522, DOI 10.1109/TCSVT.2020.3026817
   Wu QB, 2020, IEEE T CIRC SYST VID, V30, P3883, DOI 10.1109/TCSVT.2020.2972566
   Xia ZH, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3492705
   Xiang T, 2007, CHAOS, V17, DOI 10.1063/1.2728112
   Xiang T, 2020, IEEE T CIRC SYST VID, V30, P4129, DOI 10.1109/TCSVT.2019.2955298
   Xiang T, 2016, IEEE T INF FOREN SEC, V11, P951, DOI 10.1109/TIFS.2016.2515503
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Yang Y, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3555355
   Yang Y, 2021, IEEE T CIRC SYST VID, V31, P3293, DOI 10.1109/TCSVT.2020.3036854
   Yao Y, 2009, INFORM-J COMPUT INFO, V33, P69
   Yue GH, 2019, IEEE T MULTIMEDIA, V21, P2184, DOI 10.1109/TMM.2019.2913315
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
NR 56
TC 0
Z9 0
U1 4
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2024
VL 20
IS 1
AR 25
DI 10.1145/3612924
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T8LL3
UT WOS:001080441800025
DA 2024-08-05
ER

PT J
AU Chen, Q
   Huang, TL
   Liu, QF
AF Chen, Qiong
   Huang, Tianlin
   Liu, Qingfa
TI SWRM: Similarity Window Reweighting and Margin for Long-Tailed
   Recognition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Long-tailed recognition; class re-balancing; reweighting; logit
   adjustment
AB Real-world data usually obeys a long-tailed distribution, where a few classes have higher number of samples compared to the other classes. Recent studies have been proposed to alleviate the extreme data imbalance from different perspectives. In this article, we experimentally find that due to the easily confusing visual features between some head- and tail classes, the cross-entropy model is prone to misclassify tail samples to similar head classes. Therefore, to alleviate the influence of the confusion on model performance and improve the classification of tail classes, we propose a Similarity Window Reweighting and Margin (SWRM) algorithm, where the SWRM consists of Similarity Window Reweighting (SWR) and Similarity Window Margin (SWM) algorithms. For the confusable head- and tail classes, SWR assigns larger weights to tail classes and smaller weights to head classes. Therefore, the model can enlarge the importance of tail classes and effectively improve their classification. Moreover, SWR considers the difference in label frequency and the impact of category similarity simultaneously, so that the weight coefficients are more reasonable and efficacious. SWM generates adaptive margins that are proportional to the ratio of the classifier's weight norm, thus promoting the learning of tail classifier with small weight norm. Our SWRM effectively eliminates the confusion between head- and tail classes and alleviates the misclassification issues. Extensive experiments on three long-tailed datasets, i.e., CIFAR100-LT, ImageNet-LT, and Places-LT, verify our proposed method's effectiveness and superiority over comparative methods.
C1 [Chen, Qiong; Huang, Tianlin; Liu, Qingfa] South China Univ Technol, Guangzhou Higher Educ Mega Ctr, Guangzhou 510006, Guangdong, Peoples R China.
   [Huang, Tianlin] Guangzhou Higher Educ Mega Ctr, Guangdong Prov Key Lab Artificial Intelligence M, Guangzhou 510006, Guangdong, Peoples R China.
C3 South China University of Technology
RP Chen, Q (corresponding author), South China Univ Technol, Guangzhou Higher Educ Mega Ctr, Guangzhou 510006, Guangdong, Peoples R China.
EM csqchen@scut.edu.cn; htanry310@gmail.com; lewqf@outlook.com
RI Huang, Tianlin/JFK-2625-2023
OI Huang, Tianlin/0000-0003-1513-1356; Chen, Qiong/0000-0002-8507-4804
FU Guangdong Provincial Key Laboratory of Artificial Intelligence in
   Medical Image Analysis and Application [2022B1212010011]; National
   Natural Science Foundation of China [62176095]
FX This work is supported by the Guangdong Provincial Key Laboratory of
   Artificial Intelligence in Medical Image Analysis and Application (No.
   2022B1212010011), and the National Natural Science Foundation of China
   (No. 62176095).
CR Alshammari S, 2022, PROC CVPR IEEE, P6887, DOI 10.1109/CVPR52688.2022.00677
   Boyan Zhou, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9716, DOI 10.1109/CVPR42600.2020.00974
   Cai JR, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P112, DOI 10.1109/ICCV48922.2021.00018
   Cao KD, 2019, ADV NEUR IN, V32
   Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953
   Chen Q, 2023, NEURAL NETWORKS, V168, P214, DOI 10.1016/j.neunet.2023.09.022
   Chen ZN, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3231742
   Cubuk ED, 2019, PROC CVPR IEEE, P113, DOI 10.1109/CVPR.2019.00020
   Cui Y, 2019, PROC CVPR IEEE, P9260, DOI 10.1109/CVPR.2019.00949
   DeVries T, 2017, Arxiv, DOI arXiv:1708.04552
   Drummond Chris, 2003, Workshop on learning from imbalanced datasets II, International Conference on Machine Learning, V11, P1
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hong Y, 2021, PROC CVPR IEEE, P6622, DOI 10.1109/CVPR46437.2021.00656
   Hsin-Ping Chou, 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12540), P95, DOI 10.1007/978-3-030-65414-6_9
   Jingru Tan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11659, DOI 10.1109/CVPR42600.2020.01168
   Jitkrittum W., 2022, arXiv
   Kang B., 2020, 8 INT C LEARN REPR I
   Li J, 2023, PROC CVPR IEEE, P24080, DOI 10.1109/CVPR52729.2023.02306
   Li J, 2022, PROC CVPR IEEE, P6939, DOI 10.1109/CVPR52688.2022.00682
   Li Mengke, 2022, P 2022 IEEE INT C MU, P1
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu XB, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3465220
   Liu ZW, 2019, PROC CVPR IEEE, P2532, DOI 10.1109/CVPR.2019.00264
   Menon A. K., 2021, P 9 INT C LEARN REPR
   Park S, 2022, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR52688.2022.00676
   Park S, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P715, DOI 10.1109/ICCV48922.2021.00077
   Ren J., 2020, ADV NEURAL INFORM PR, V33, P4175, DOI DOI 10.48550/ARXIV.2007.10740
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shu J, 2019, ADV NEUR IN, V32
   Sinha S, 2023, IEEE WINT CONF APPL, P6433, DOI 10.1109/WACV56688.2023.00638
   Sinha S, 2022, INT J COMPUT VISION, V130, P2517, DOI 10.1007/s11263-022-01643-3
   Smith L, 2022, Arxiv, DOI arXiv:2202.08978
   Snell J, 2017, ADV NEUR IN, V30
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Van Horn G, 2018, PROC CVPR IEEE, P8769, DOI 10.1109/CVPR.2018.00914
   Wang JF, 2021, PROC CVPR IEEE, P3783, DOI 10.1109/CVPR46437.2021.00378
   Wang JQ, 2021, PROC CVPR IEEE, P9690, DOI 10.1109/CVPR46437.2021.00957
   Wang P, 2021, PROC CVPR IEEE, P943, DOI 10.1109/CVPR46437.2021.00100
   Wang T, 2021, PROC CVPR IEEE, P3102, DOI 10.1109/CVPR46437.2021.00312
   Wang XD, 2022, Arxiv, DOI arXiv:2010.01809
   Wang YR, 2019, IEEE I CONF COMP VIS, P5016, DOI 10.1109/ICCV.2019.00512
   Wang YX, 2017, 31 ANN C NEURAL INFO, V30
   Yang Y, 2023, PROC CVPR IEEE, P7131, DOI 10.1109/CVPR52729.2023.00689
   Zhang RH, 2022, INT CONF ACOUST SPEE, P2355, DOI 10.1109/ICASSP43922.2022.9746738
   Zhong ZS, 2021, PROC CVPR IEEE, P16484, DOI 10.1109/CVPR46437.2021.01622
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
NR 47
TC 0
Z9 0
U1 1
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2024
VL 20
IS 6
SI SI
AR 181
DI 10.1145/3643816
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OQ2T3
UT WOS:001208681800031
DA 2024-08-05
ER

PT J
AU Zhang, TY
   Min, WQ
   Liu, T
   Jiang, SQ
   Rui, Y
AF Zhang, Tianyu
   Min, Weiqing
   Liu, Tao
   Jiang, Shuqiang
   Rui, Yong
TI Toward Egocentric Compositional Action Anticipation with Adaptive
   Semantic Debiasing
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Egocentric video understanding; compositional action anticipation;
   semantic bias; adaptive counterfactual analysis
ID NETWORK
AB Predicting the unknown from the first-person perspective is expected as a necessary step toward machine intelligence, which is essential for practical applications including autonomous driving and robotics. As a human-level task, egocentric action anticipation aims at predicting an unknown action seconds before it is performed from the first-person viewpoint. Egocentric actions are usually provided as verb-noun pairs; however, predicting the unknown action may be trapped in insufficient training data for all possible combinations. Therefore, it is crucial for intelligent systems to use limited known verb-noun pairs to predict new combinations of actions that have never appeared, which is known as compositional generalization. In this article, we are the first to explore the egocentric compositional action anticipation problem, which is more in line with real-world settings but neglected by existing studies. Whereas prediction results are prone to suffer from semantic bias considering the distinct difference between training and test distributions, we further introduce a general and flexible adaptive semantic debiasing framework that is compatible with different deep neural networks. To capture and mitigate semantic bias, we can imagine one counterfactual situation where no visual representations have been observed and only semantic patterns of observation are used to predict the next action. Instead of the traditional counterfactual analysis scheme that reduces semantic bias in a mindless way, we devise a novel counterfactual analysis scheme to adaptively amplify or penalize the effect of semantic experience by considering the discrepancy both among categories and among examples. We also demonstrate that the traditional counterfactual analysis scheme is a special case of the devised adaptive counterfactual analysis scheme. We conduct experiments on three large-scale egocentric video datasets. Experimental results verify the superiority and effectiveness of our proposed solution.
C1 [Zhang, Tianyu; Min, Weiqing; Liu, Tao; Jiang, Shuqiang] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, 6 Kexueyuan South Rd, Beijing, Peoples R China.
   [Zhang, Tianyu; Min, Weiqing; Liu, Tao; Jiang, Shuqiang] Univ Chinese Acad Sci, 80 Zhongguancun East Rd, Beijing, Peoples R China.
   [Rui, Yong] Lenovo Grp, 6 Shangdi West Rd, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Legend Holdings; Lenovo
RP Zhang, TY (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, 6 Kexueyuan South Rd, Beijing, Peoples R China.; Zhang, TY (corresponding author), Univ Chinese Acad Sci, 80 Zhongguancun East Rd, Beijing, Peoples R China.
EM tianyu.zhang@vipl.ict.ac.cn; weiqing-min@ict.ac.cn;
   tao.liu@vipl.ict.ac.cn; sqjiang@ict.ac.cn; yongrui@lenovo.com
OI Rui, Yong/0000-0002-9142-5914
FU National Key Research and Development Project of New Generation
   Artificial Intelligence of China [2018AAA0102500]
FX This work was supported by the National Key Research and Development
   Project of New Generation Artificial Intelligence of China under grant
   2018AAA0102500.
CR Betancourt A, 2015, IEEE T CIRC SYST VID, V25, P744, DOI 10.1109/TCSVT.2015.2409731
   Camporese G, 2021, INT C PATT RECOG, P3312, DOI 10.1109/ICPR48806.2021.9412660
   Chen GY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9804, DOI 10.1109/ICCV48922.2021.00968
   Chernozhukov V, 2013, ECONOMETRICA, V81, P2205, DOI 10.3982/ECTA10582
   Damen D, 2022, INT J COMPUT VISION, V130, P33, DOI 10.1007/s11263-021-01531-2
   Damen D, 2018, LECT NOTES COMPUT SC, V11208, P753, DOI 10.1007/978-3-030-01225-0_44
   De Geest R, 2018, IEEE WINT CONF APPL, P1549, DOI 10.1109/WACV.2018.00173
   Dessalene Eadom, 2021, IEEE Transactions on Pattern Analysis and Machine Intelligence
   Furnari A, 2022, INT C PATT RECOG, P1250, DOI 10.1109/ICPR56361.2022.9956090
   Furnari A, 2019, IEEE I CONF COMP VIS, P6261, DOI 10.1109/ICCV.2019.00635
   Furnari A, 2021, IEEE T PATTERN ANAL, V43, P4021, DOI 10.1109/TPAMI.2020.2992889
   Furnari Antonino, 2018, P EUROPEAN C COMPUTE, P1
   Gao J., 2017, P BRIT MACHINE VISIO
   Girdhar R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13485, DOI 10.1109/ICCV48922.2021.01325
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Hu XJ, 2022, NEUROCOMPUTING, V491, P395, DOI 10.1016/j.neucom.2022.03.069
   Huang Y, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P245, DOI 10.1145/3474085.3475327
   Huang Y, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3409332
   Hutchinson MS, 2021, IEEE ACCESS, V9, P134611, DOI 10.1109/ACCESS.2021.3115476
   Jain A, 2016, IEEE INT CONF ROBOT, P3118, DOI 10.1109/ICRA.2016.7487478
   Kahneman D., 2011, THINKING FAST SLOW
   King BG, 2008, ADMIN SCI QUART, V53, P395, DOI 10.2189/asqu.53.3.395
   KNUTH DE, 1992, AM MATH MON, V99, P403, DOI 10.2307/2325085
   Lake B. M., 2014, Ph. D. Dissertation
   Lake BM, 2017, BEHAV BRAIN SCI, V40, DOI 10.1017/S0140525X16001837
   Li Y, 2018, LECT NOTES COMPUT SC, V11209, P639, DOI 10.1007/978-3-030-01228-1_38
   Li Y, 2013, IEEE I CONF COMP VIS, P3216, DOI 10.1109/ICCV.2013.399
   Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718
   Liu SW, 2022, PROC CVPR IEEE, P3272, DOI 10.1109/CVPR52688.2022.00328
   Liu TS, 2022, PROC CVPR IEEE, P13894, DOI 10.1109/CVPR52688.2022.01353
   Liu Xiaohao, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P687, DOI 10.1145/3503161.3548404
   Luo Zhekun, 2022, P C N AM CHAPTER ASS, P559
   Ma L., 2022, IEEE Transactions on Circuits and Systems for Video Technology
   Ma SG, 2016, PROC CVPR IEEE, P1942, DOI 10.1109/CVPR.2016.214
   Materzynska J, 2020, PROC CVPR IEEE, P1046, DOI 10.1109/CVPR42600.2020.00113
   Miao Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P704, DOI 10.1007/978-3-030-58452-8_41
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P1, DOI DOI 10.48550/ARXIV.1310.4546
   Nagarajan T, 2020, PROC CVPR IEEE, P160, DOI 10.1109/CVPR42600.2020.00024
   Nakamura K, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4220, DOI 10.1145/3474085.3475557
   Niu YL, 2021, PROC CVPR IEEE, P12695, DOI 10.1109/CVPR46437.2021.01251
   Niu Yulei, 2021, ADV NEURAL INFORM PR, V34, P2
   Nunez-Marcos Adrian, 2020, Image Analysis and Recognition. 17th International Conference, ICIAR 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12131), P174, DOI 10.1007/978-3-030-50347-5_16
   Osman N, 2021, IEEE INT CONF COMP V, P3430, DOI 10.1109/ICCVW54120.2021.00383
   Pan YH, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3487042
   Pearl J, 2016, J CAUSAL INFERENCE, V4, DOI 10.1515/jci-2016-0021
   Qi Zhaobo, 2021, IEEE Transactions on Pattern Analysis and Machine Intelligence
   Qian C., 2021, PROC C ANN MEET ASS, P5434
   Quinn T, 2018, TLS-TIMES LIT SUPPL, P31
   Radevski Gorjan, 2021, P BRIT MACHINE VISIO, P1
   Rao YM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1005, DOI 10.1109/ICCV48922.2021.00106
   Ren S., 2016, Faster r-cnn: Towards real-time object detection with region proposal networks
   Richiardi L, 2013, INT J EPIDEMIOL, V42, P1511, DOI 10.1093/ije/dyt127
   Rodin I, 2022, LECT NOTES COMPUT SC, V13233, P337, DOI 10.1007/978-3-031-06433-3_29
   Rodin I, 2021, COMPUT VIS IMAGE UND, V211, DOI 10.1016/j.cviu.2021.103252
   Roy D, 2021, IEEE T IMAGE PROCESS, V30, P8116, DOI 10.1109/TIP.2021.3113114
   Sahu A, 2021, IEEE T IMAGE PROCESS, V30, P4330, DOI 10.1109/TIP.2021.3070732
   Sener Fadime, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P154, DOI 10.1007/978-3-030-58517-4_10
   Simonyan K, 2014, ADV NEUR IN, V27
   Sudhakaran Swathikiran, 2021, IEEE T PATTERN ANAL
   Sun PZ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3220, DOI 10.1145/3474085.3475472
   Sun T, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, DOI 10.1145/3503161.3548211
   Tang K., 2020, P NIPS, P1513
   Tang KH, 2020, PROC CVPR IEEE, P3713, DOI 10.1109/CVPR42600.2020.00377
   Thapar D, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2095, DOI 10.1145/3394171.3413654
   Tian B, 2022, AAAI CONF ARTIF INTE, P11376
   Vaswani A, 2017, ADV NEUR IN, V30
   Vondrick C, 2016, PROC CVPR IEEE, P98, DOI 10.1109/CVPR.2016.18
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang SL, 2022, IEEE INT C INT ROBOT, P10627, DOI 10.1109/IROS47612.2022.9981172
   Wang Xiaohan, 2020, IEEE T PATTERN ANAL
   Wu JF, 2022, PROCEEDINGS OF THE 45TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '22), P2308, DOI 10.1145/3477495.3531850
   Wu Y, 2021, IEEE T IMAGE PROCESS, V30, P1143, DOI 10.1109/TIP.2020.3040521
   Xu XY, 2022, PROC CVPR IEEE, P12724, DOI 10.1109/CVPR52688.2022.01240
   Yan R, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P3666, DOI 10.1145/3503161.3547862
   Zatsarynna O, 2021, IEEE COMPUT SOC CONF, P2249, DOI 10.1109/CVPRW53098.2021.00254
   Zhang T, 2021, PROC 30 INT JOINT C, P1316
   Zhang TY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P402, DOI 10.1145/3394171.3413964
   Zhang YC, 2017, IEEE WINT CONF APPL, P121, DOI 10.1109/WACV.2017.21
   Zheng Na, 2022, ACMTransactions onMultimedia Computing, Communications, and Applications
   Zhong ZY, 2023, IEEE WINT CONF APPL, P6057, DOI 10.1109/WACV56688.2023.00601
NR 80
TC 0
Z9 0
U1 2
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2024
VL 20
IS 5
AR 122
DI 10.1145/3633333
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MF3R9
UT WOS:001192177900002
OA hybrid
DA 2024-08-05
ER

PT J
AU Chen, Y
   Yao, R
   Zhou, Y
   Zhao, JQ
   Liu, B
   El Saddik, A
AF Chen, Ying
   Yao, Rui
   Zhou, Yong
   Zhao, Jiaqi
   Liu, Bing
   El Saddik, Abdulmotaleb
TI Black-box Attack against Self-supervised Video Object Segmentation
   Models with Contrastive Loss
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Black-box adversarial attack; self-supervised video object segmentation;
   contrastive loss; feature loss; pixel-level loss
AB Deep learning models have been proven to be susceptible to malicious adversarial attacks, which manipulate input images to deceive the model into making erroneous decisions. Consequently, the threat posed to these models serves as a poignant reminder of the necessity to focus on the model security of object segmentation algorithms based on deep learning. However, the current landscape of research on adversarial attacks primarily centers around static images, resulting in a dearth of studies on adversarial attacks targeting Video Object Segmentation (VOS) models. Given that a majority of self-supervised VOS models rely on affinity matrices to learn feature representations of video sequences and achieve robust pixel correspondence, our investigation has delved into the impact of adversarial attacks on self-supervised VOS models. In response, we propose an innovative black-box attack method incorporating contrastive loss. This method induces segmentation errors in the model through perturbations in the feature space and the application of a pixel-level loss function. Diverging from conventional gradient-based attack techniques, we adopt an iterative black-box attack strategy that incorporates contrastive loss across the current frame, any two consecutive frames, and multiple frames. Through extensive experimentation conducted on the DAVIS 2016 and DAVIS 2017 datasets using three self-supervised VOS models and one unsupervised VOS model, we unequivocally demonstrate the potent attack efficiency of the black-box approach. Remarkably, the J&F metric value experiences a significant decline of up to 50.08% post-attack.
C1 [Chen, Ying; Yao, Rui; Zhou, Yong; Zhao, Jiaqi; Liu, Bing] China Univ Min & Technol, Sch Comp Sci & Technol, 1 Daxue Rd, Xuzhou, Jiangsu, Peoples R China.
   [Chen, Ying; Yao, Rui; Zhou, Yong; Zhao, Jiaqi; Liu, Bing] Minist Educ Peoples Republ China, Engn Res Ctr Mine Digitizat, 1 Daxue Rd, Xuzhou, Jiangsu, Peoples R China.
   [El Saddik, Abdulmotaleb] Univ Ottawa, Sch Elect Engn & Comp Sci, Multimedia Commun Res Lab, 800 King Edward, Ottawa, ON K1N 6N5, Canada.
C3 China University of Mining & Technology; University of Ottawa
RP Yao, R (corresponding author), China Univ Min & Technol, Sch Comp Sci & Technol, 1 Daxue Rd, Xuzhou, Jiangsu, Peoples R China.; Yao, R (corresponding author), Minist Educ Peoples Republ China, Engn Res Ctr Mine Digitizat, 1 Daxue Rd, Xuzhou, Jiangsu, Peoples R China.
EM ts20170007a31tm@cumt.edu.cn; ruiyao@cumt.edu.cn; yzhou@cumt.edu.cn;
   jiaqizhao@cumt.edu.cn; liubing@cumt.edu.cn; elsaddik@uottawa.ca
RI ; /D-4159-2009
OI Yao, Rui/0000-0003-2734-915X; /0000-0002-7690-8547; Liu,
   Bing/0000-0002-2365-6606
FU National Natural Science Foundation of China [62172417, 62272461,
   62276266]; Xuzhou Key Research and Development Program [KC22287]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62172417, 62272461 and 62276266, and in
   part by the Xuzhou Key Research and Development Program under Grant
   KC22287.
CR Agrawal P, 2015, IEEE I CONF COMP VIS, P37, DOI 10.1109/ICCV.2015.13
   Caelles S, 2017, PROC CVPR IEEE, P5320, DOI 10.1109/CVPR.2017.565
   Chen SX, 2021, PROC CVPR IEEE, P9791, DOI 10.1109/CVPR46437.2021.00967
   Chen T, 2020, PR MACH LEARN RES, V119
   Chen ZD, 2023, IEEE T PATTERN ANAL, V45, P5158, DOI 10.1109/TPAMI.2022.3195759
   Chuang Gan, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P5589, DOI 10.1109/CVPR.2018.00586
   Denton E, 2017, ADV NEUR IN, V30
   Goodfellow IJ, 2015, P 3 INT C LEARNING R
   Han JW, 2018, PROC CVPR IEEE, P9080, DOI 10.1109/CVPR.2018.00946
   He H., 2020, P IEEE CVF C COMP VI, P9729
   Hjelm R. D., 2019, ICLR
   Hou WJ, 2022, NEUROCOMPUTING, V481, P270, DOI 10.1016/j.neucom.2022.01.066
   Huang PL, 2022, IEEE-CAA J AUTOMATIC, V9, P339, DOI 10.1109/JAS.2021.1004210
   Huang Q, 2019, IEEE I CONF COMP VIS, P4732, DOI 10.1109/ICCV.2019.00483
   Inkawhich N., 2020, Advances in Neural Information Process- ing Systems, V33, P20791
   Inkawhich N, 2019, PROC CVPR IEEE, P7059, DOI 10.1109/CVPR.2019.00723
   Jabri A., 2020, Advances in neural information processing systems
   Jiang HZ, 2018, LECT NOTES COMPUT SC, V11215, P20, DOI 10.1007/978-3-030-01252-6_2
   Kim D, 2019, AAAI CONF ARTIF INTE, P8545
   Kim Y, 2020, IEEE WINT CONF APPL, P2046, DOI 10.1109/WACV45572.2020.9093294
   Kingma D. P., 2014, arXiv
   Kini Jyoti, 2022, arXiv
   Kurakin A., 2018, Artificial Intelligence Safety and Security, P99, DOI DOI 10.1201/9781351251389-8
   Lai ZH, 2020, PROC CVPR IEEE, P6478, DOI 10.1109/CVPR42600.2020.00651
   Lee J, 2019, PROC CVPR IEEE, P2273, DOI 10.1109/CVPR.2019.00238
   Li HC, 2019, PROC CVPR IEEE, P9514, DOI 10.1109/CVPR.2019.00975
   Li SY, 2018, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2018.00683
   Li SY, 2018, LECT NOTES COMPUT SC, V11207, P215, DOI 10.1007/978-3-030-01219-9_13
   Li XB, 2019, ADV CIV ENG, V2019, DOI 10.1155/2019/2724370
   Lian Z, 2018, PROCEEDINGS OF THE JOINT WORKSHOP OF THE 4TH WORKSHOP ON AFFECTIVE SOCIAL MULTIMEDIA COMPUTING AND FIRST MULTI-MODAL AFFECTIVE COMPUTING OF LARGE-SCALE MULTIMEDIA DATA (ASMMC-MMAC'18), P21
   Lu XK, 2019, PROC CVPR IEEE, P3618, DOI 10.1109/CVPR.2019.00374
   Madry A., 2018, INT C LEARN REPR, P1
   Maninis KK, 2019, IEEE T PATTERN ANAL, V41, P1515, DOI 10.1109/TPAMI.2018.2838670
   Mingmin Zhen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P445, DOI 10.1007/978-3-030-58583-9_27
   Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282
   Paszke A, 2019, ADV NEUR IN, V32
   Pathak D, 2017, PROC CVPR IEEE, P6024, DOI 10.1109/CVPR.2017.638
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Pont-Tuset Jordi, 2017, 2017 DAVIS CHALL VID, P1
   Tian Y., 2020, Advances in neural information processing systems, V33, P6827, DOI DOI 10.5555/3495724.3496297
   van den Oord A, 2019, Arxiv, DOI [arXiv:1807.03748, DOI 10.48550/ARXIV.1807.03748]
   Voigtlaender P., 2017, 2017 DAVIS CHALL VID, V5, P1
   Vondrick C, 2018, LECT NOTES COMPUT SC, V11217, P402, DOI 10.1007/978-3-030-01261-8_24
   Wang F, 2021, PROC CVPR IEEE, P2495, DOI 10.1109/CVPR46437.2021.00252
   Wang N, 2021, AAAI CONF ARTIF INTE, V35, P10174
   Wang N, 2019, PROC CVPR IEEE, P1308, DOI 10.1109/CVPR.2019.00140
   Wang WG, 2019, IEEE I CONF COMP VIS, P9235, DOI 10.1109/ICCV.2019.00933
   Wang Z., 2021, Proceedings of the IEEE/CVF international conference on computer vision, P7639
   Wei XX, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P954
   Wiles O, 2018, LECT NOTES COMPUT SC, V11217, P690, DOI 10.1007/978-3-030-01261-8_41
   Wiles Olivia, 2018, BRIT MACH VIS C BMVC
   Xiao CW, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3905
   Xie CH, 2017, IEEE I CONF COMP VIS, P1378, DOI 10.1109/ICCV.2017.153
   Yang C., 2021, P IEEECVF INT C COMP, P7177
   Ye M, 2019, PROC CVPR IEEE, P6203, DOI 10.1109/CVPR.2019.00637
   Yonglong Tian, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P776, DOI 10.1007/978-3-030-58621-8_45
   Yu N, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6711, DOI 10.1109/ICCV48922.2021.00666
   Zhang DW, 2020, IEEE T PATTERN ANAL, V42, P475, DOI 10.1109/TPAMI.2018.2881114
   Zhou LW, 2018, PROC CVPR IEEE, P8739, DOI 10.1109/CVPR.2018.00911
   Zhou TF, 2020, AAAI CONF ARTIF INTE, V34, P13066
   Zhou W, 2018, LECT NOTES COMPUT SC, V11218, P471, DOI 10.1007/978-3-030-01264-9_28
   Zhu WJ, 2021, NEUROCOMPUTING, V455, P325, DOI 10.1016/j.neucom.2021.04.090
   Zhu XZ, 2017, IEEE I CONF COMP VIS, P408, DOI 10.1109/ICCV.2017.52
NR 63
TC 0
Z9 0
U1 3
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2024
VL 20
IS 2
SI SI
AR 57
DI 10.1145/3617502
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W6GR4
UT WOS:001092595800027
DA 2024-08-05
ER

PT J
AU Liu, LL
   Zhang, HJ
   Li, Q
   Ma, JH
   Zhang, Z
AF Liu, Linlin
   Zhang, Haijun
   Li, Qun
   Ma, Jianghong
   Zhang, Zhao
TI Collocated Clothing Synthesis with GANs Aided by Textual Information: A
   Multi-Modal Framework
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Multi-modal; clothes collocation; generative adversarial networks; image
   translation; fashion data
ID FASHION DESIGN; HUMAN-BODY; IMAGE; TRANSLATION
AB Synthesizing realistic images of fashion items which are compatible with given clothing images, as well as conditioning on multiple modalities, brings novel and exciting applications together with enormous economic potential. In this work, we propose a multi-modal collocation framework based on generative adversarial network (GAN) for synthesizing compatible clothing images. Given an input clothing item that consists of an image and a text description, our model works on synthesizing a clothing image which is compatible with the input clothing, as well as being guided by a given text description from the target domain. Specifically, a generator aims to synthesize realistic and collocated clothing images relying on image- and text-based latent representations learned from the source domain. An auxiliary text representation from the target domain is added for supervising the generation results. In addition, a multi-discriminator framework is carried out to determine compatibility between the generated clothing images and the input clothing images, as well as visual-semantic matching between the generated clothing images and the targeted textual information. Extensive quantitative and qualitative results demonstrate that our model substantially outperforms state-of-the-art methods in terms of authenticity, diversity, and visual-semantic similarity between image and text.
C1 [Liu, Linlin; Zhang, Haijun; Li, Qun; Ma, Jianghong] Harbin Inst Technol, Dept Comp Sci, 2199 Lishui Rd, Shenzhen, Guangdong, Peoples R China.
   [Zhang, Zhao] Hefei Univ Technol, Dept Comp Sci, 193 Tunxi Rd, Hefei 230000, Anhui, Peoples R China.
C3 Harbin Institute of Technology; Hefei University of Technology
RP Zhang, HJ (corresponding author), Harbin Inst Technol, Dept Comp Sci, 2199 Lishui Rd, Shenzhen, Guangdong, Peoples R China.
EM hjzhang@hit.edu.cn
RI Zhang, Haijun/N-8470-2015; Zhang, Zhao/B-5136-2010
OI Zhang, Zhao/0000-0002-5703-7969; Ma, Jianghong/0000-0002-0524-3584; LI,
   QUN/0009-0005-1610-5071
FU National Natural Science Foundation of China [61972112, 61832004,
   62202122]; Guangdong Basic and Applied Basic Research Foundation
   [2021B1515020088]; Shenzhen Science and Technology Program
   [JCYJ20210324131203009]; HITSZJ&A Joint Laboratory of Digital Design and
   Intelligent Fabrication [HITSZ-JA-2021A01]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant no. 61972112, no. 61832004 and no.
   62202122, the Guangdong Basic and Applied Basic Research Foundation
   under Grant no. 2021B1515020088, the Shenzhen Science and Technology
   Program under Grant no. JCYJ20210324131203009, and the HITSZJ&A Joint
   Laboratory of Digital Design and Intelligent Fabrication under Grant no.
   HITSZ-J&A-2021A01.
CR Bugeau A, 2014, IEEE T IMAGE PROCESS, V23, P298, DOI 10.1109/TIP.2013.2288929
   Chen L., 2020, P IEEECVF WINTER C A, P3241
   Cui YR, 2018, COMPUT GRAPH FORUM, V37, P109, DOI 10.1111/cgf.13552
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dong Haoye, 2020, P IEEE CVF C COMP VI, P8120
   Dong X, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P302, DOI 10.1145/3343031.3350905
   Duc Minh Vo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P290, DOI 10.1007/978-3-030-58604-1_18
   Gu XL, 2021, IEEE T MULTIMEDIA, V23, P2361, DOI 10.1109/TMM.2020.3009500
   Han XT, 2018, PROC CVPR IEEE, P7543, DOI 10.1109/CVPR.2018.00787
   Ho TT, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3396237
   Hsiao WL, 2018, PROC CVPR IEEE, P7161, DOI 10.1109/CVPR.2018.00748
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Kim T, 2017, PR MACH LEARN RES, V70
   Kingma DP, 2018, ADV NEUR IN, V31
   Lee HY, 2020, INT J COMPUT VISION, V128, P2402, DOI 10.1007/s11263-019-01284-z
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Li B., 2020, 2020 IEEE CVF C COMP, P7877
   Li B., 2020, Advances in Neural Information Processing Systems, P22020, DOI DOI 10.48550/ARXIV.2010.12136
   Li ZJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13799, DOI 10.1109/ICCV48922.2021.01356
   Lin CH, 2019, IEEE I CONF COMP VIS, P4511, DOI 10.1109/ICCV.2019.00461
   Lin YJ, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P1095, DOI 10.1145/3308558.3313614
   Liu JH, 2020, NEUROCOMPUTING, V414, P215, DOI 10.1016/j.neucom.2020.06.033
   Liu LJ, 2021, Arxiv, DOI arXiv:2001.04947
   Liu LL, 2020, IEEE T NEUR NET LEAR, V31, P3540, DOI 10.1109/TNNLS.2019.2944979
   Liu LL, 2019, NEUROCOMPUTING, V341, P156, DOI 10.1016/j.neucom.2019.03.011
   Liu Y, 2019, IEEE T MULTIMEDIA, V21, P2209, DOI 10.1109/TMM.2019.2897897
   Ma Shuang, 2019, ICLR, V2, P1
   Men YF, 2020, PROC CVPR IEEE, P5083, DOI 10.1109/CVPR42600.2020.00513
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Nasrollahi K, 2014, MACH VISION APPL, V25, P1423, DOI 10.1007/s00138-014-0623-4
   Nie LQ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1098, DOI 10.1145/3343031.3350923
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   Park T., 2020, EUR C COMP VIS, P319, DOI [DOI 10.1007/978-3-030-58545-719, 10.1007/978-3-030-58545-719, DOI 10.1007/978-3-030-58545-7_19]
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Radford A, 2016, Arxiv, DOI [arXiv:1511.06434, DOI 10.48550/ARXIV.1511.06434]
   Radford A, 2021, PR MACH LEARN RES, V139
   Ravichandran Siddarth, 2023, 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), P4585, DOI 10.1109/CVPR52729.2023.00445
   Reed S, 2016, PR MACH LEARN RES, V48
   Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530
   Sbai O, 2019, LECT NOTES COMPUT SC, V11131, P37, DOI 10.1007/978-3-030-11015-4_5
   Shen Y., 2020, P IEEECVF C COMPUTER, P9243
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sohn K, 2015, ADV NEUR IN, V28
   Tang H, 2019, IEEE IJCNN
   Voynov Andrey, 2023, ACM SIGGRAPH 2023 C, P1
   Wan CD, 2017, PROC CVPR IEEE, P1196, DOI 10.1109/CVPR.2017.132
   Wang BC, 2018, LECT NOTES COMPUT SC, V11217, P607, DOI 10.1007/978-3-030-01261-8_36
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang WR, 2017, Arxiv, DOI [arXiv:1610.03454, DOI 10.48550/ARXIV.1610.03454]
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wu MK, 2018, ADV NEUR IN, V31
   Wu ZH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P293, DOI 10.1145/3343031.3351083
   Xian WQ, 2018, PROC CVPR IEEE, P8456, DOI 10.1109/CVPR.2018.00882
   Xie Junyuan, 2012, Advances In. Neural Information. Processing Systems, P341, DOI DOI 10.5555/2999134.2999173
   Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143
   Yan H, 2023, IEEE T MULTIMEDIA, V25, P2323, DOI [10.1109/TCSS.2022.3161996, 10.1109/TMM.2022.3146010]
   Yang X, 2019, AAAI CONF ARTIF INTE, P403
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Zhang FF, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3478642
   Zhang HJ, 2020, IEEE MULTIMEDIA, V27, P58, DOI 10.1109/MMUL.2020.3014037
   Zhang HJ, 2020, NEUROCOMPUTING, V382, P148, DOI 10.1016/j.neucom.2019.11.085
   Zhang HJ, 2020, NEURAL COMPUT APPL, V32, P4519, DOI [10.1007/s00521-018-3691-y, 10.1007/s00521-018-3579-x]
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhang P, 2020, PROC CVPR IEEE, P5142, DOI 10.1109/CVPR42600.2020.00519
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu SZ, 2017, IEEE I CONF COMP VIS, P1689, DOI 10.1109/ICCV.2017.186
NR 68
TC 2
Z9 2
U1 10
U2 23
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2024
VL 20
IS 1
AR 26
DI 10.1145/3614097
PG 25
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T8LL3
UT WOS:001080441800026
DA 2024-08-05
ER

PT J
AU Gao, PL
   Yang, X
   Zhang, R
   Huang, KZ
AF Gao, Penglei
   Yang, Xi
   Zhang, Rui
   Huang, Kaizhu
TI Continuous Image Outpainting with Neural ODE
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image outpainting; transformer; u-shaped structure; neural ODE
ID EDGE
AB Generalised image outpainting is an important and active research topic in computer vision, which aims to extend appealing content all-side around a given image. Existing state-of-the-art outpainting methods often rely on discrete extrapolation to extend the feature map in the bottleneck. They thus suffer from content unsmoothness, especially in circumstances where the outlines of objects in the extrapolated regions are incoherent with the input sub-images. To mitigate this issue, we design a novel bottleneck with Neural ODEs to make continuous extrapolation in latent space, which could be a plug-in for many deep learning frameworks. Our ODE-based network continuously transforms the state and makes accurate predictions by learning the incremental relationship among latent points, leading to both smooth and structured feature representation. Experimental results on three real-world datasets both applied on transformer-based and CNN-based frameworks show that our methods could generate more realistic and coherent images against the state-of-the-art image outpainting approaches. Our code is available at https://github.com/PengleiGao/Continuous-Image-Outpainting-with-Neural-ODE.
C1 [Gao, Penglei] Univ Liverpool, Dept Comp Sci, Liverpool, Merseyside, England.
   [Gao, Penglei; Zhang, Rui] Xian Jiaotong Liverpool Univ, Dept Foundat Math, 111 Renai Rd, Suzhou 215123, Jiangsu, Peoples R China.
   [Yang, Xi] Xian Jiaotong Liverpool Univ, Dept Intelligent Sci, 111 Renai Rd, Suzhou 215123, Jiangsu, Peoples R China.
   [Huang, Kaizhu] Duke Kunshan Univ, Data Sci Res Ctr, 8 Duke Ave, Kunshan 215316, Jiangsu, Peoples R China.
C3 University of Liverpool; Xi'an Jiaotong-Liverpool University; Xi'an
   Jiaotong-Liverpool University; Duke Kunshan University
RP Zhang, R (corresponding author), Xian Jiaotong Liverpool Univ, Dept Foundat Math, 111 Renai Rd, Suzhou 215123, Jiangsu, Peoples R China.
EM P.Gao6@liverpool.ac.uk; xi.yang01@xjtlu.edu.cn;
   rui.zhang02@xjtlu.edu.cn; kaizhu.huang@dukekunshan.edu.cn
RI ; Huang, Kaizhu/O-4721-2014
OI gao, penglei/0000-0003-1935-5752; Huang, Kaizhu/0000-0002-3034-9639
FU National Natural Science Foundation of China [92370119, 62376113,
   62206225]; Jiangsu Science and Technology Programme (Natural Science
   Foundation of Jiangsu Province) [BE2020006-4]; Natural Science
   Foundation of the Jiangsu Higher Education Institutions of China
   [22KJB520039]
FX The work was partially supported by the following: National Natural
   Science Foundation of China under No. 92370119, No. 62376113, and No.
   62206225; Jiangsu Science and Technology Programme (Natural Science
   Foundation of Jiangsu Province) under No. BE2020006-4; Natural Science
   Foundation of the Jiangsu Higher Education Institutions of China under
   No. 22KJB520039.
CR Cao Hu, 2023, Computer Vision - ECCV 2022 Workshops: Proceedings. Lecture Notes in Computer Science (13803), P205, DOI 10.1007/978-3-031-25066-8_9
   Chang HW, 2022, PROC CVPR IEEE, P11305, DOI 10.1109/CVPR52688.2022.01103
   Chen R.T., 2018, P 32 INT C NEURAL IN, V31, P6572
   Cheng YC, 2022, PROC CVPR IEEE, P11421, DOI 10.1109/CVPR52688.2022.01114
   Lu CN, 2021, PROC CVPR IEEE, P843, DOI 10.1109/CVPR46437.2021.00090
   d'Ascoli S, 2021, PR MACH LEARN RES, V139, DOI 10.1088/1742-5468/ac9830
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Dosovitskiy Alexey, 2016, ADV ANN C NEUR INF P, V29
   Esser P, 2021, PROC CVPR IEEE, P12868, DOI 10.1109/CVPR46437.2021.01268
   Fang F, 2022, COGN COMPUT, V14, P2296, DOI 10.1007/s12559-022-10029-z
   Gao PL, 2023, NEURAL NETWORKS, V162, P1, DOI 10.1016/j.neunet.2023.02.021
   Graham B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12239, DOI 10.1109/ICCV48922.2021.01204
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He XY, 2019, PROC CVPR IEEE, P1732, DOI 10.1109/CVPR.2019.00183
   Hensel M, 2017, ADV NEUR IN, V30
   Lim JH, 2017, Arxiv, DOI [arXiv:1705.02894, 10.48550/arXiv.1705.02894, DOI 10.48550/ARXIV.1705.02894]
   Iqbal A, 2022, COGN COMPUT, V14, P1287, DOI 10.1007/s12559-022-10038-y
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Khrulkov V., 2021, P IEEE CVF INT C COM, P14428
   Kim K, 2021, IEEE WINT CONF APPL, P2121, DOI 10.1109/WACV48630.2021.00217
   Kingma D. P., 2014, arXiv
   Larsen ABL, 2016, PR MACH LEARN RES, V48
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lin AL, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3178991
   Lin H, 2021, IEEE COMPUT SOC CONF, P806, DOI 10.1109/CVPRW53098.2021.00090
   Liu Z, 2021, Arxiv, DOI arXiv:2103.14030
   Lu C., 2022, P ADV NEUR INF PROC, V35, P5775
   Ma Y, 2021, Arxiv, DOI arXiv:2110.09267
   Man X, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3503927
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Qian Z, 2020, PATTERN RECOGN, V107, DOI 10.1016/j.patcog.2020.107453
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Sabini M., 2018, arXiv
   Salimans T, 2016, ADV NEUR IN, V29
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tan WR, 2016, IEEE IMAGE PROC, P3703, DOI 10.1109/ICIP.2016.7533051
   Valle R, 2019, Arxiv, DOI arXiv:1912.11683
   Van Hoorick B, 2020, Arxiv, DOI [arXiv:1912.10960, 10.48550/arXiv.1912.10960]
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang Y, 2019, PROC CVPR IEEE, P1399, DOI 10.1109/CVPR.2019.00149
   Xu SX, 2021, IEEE T CIRC SYST VID, V31, P1308, DOI 10.1109/TCSVT.2020.3001267
   Yang C, 2017, PROC CVPR IEEE, P4076, DOI 10.1109/CVPR.2017.434
   Yang ZX, 2019, IEEE I CONF COMP VIS, P10560, DOI 10.1109/ICCV.2019.01066
   Yao K, 2022, LECT NOTES COMPUT SC, V13683, P153, DOI 10.1007/978-3-031-20050-2_10
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Yuan J, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3578518
   Yuan L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P538, DOI 10.1109/ICCV48922.2021.00060
   Yuren Cong, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P636, DOI 10.1007/978-3-030-58565-5_38
   Zhao ZW, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3491225
   Zhou DQ, 2021, Arxiv, DOI arXiv:2103.11886
NR 52
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2024
VL 20
IS 7
SI SI
AR 203
DI 10.1145/3648367
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL0Q6
UT WOS:001234494100018
DA 2024-08-05
ER

PT J
AU Shen, XL
   Zheng, ZD
   Yang, Y
AF Shen, Xiaolong
   Zheng, Zhedong
   Yang, Yi
TI StepNet: Spatial-temporal Part-aware Network for Isolated Sign Language
   Recognition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Sign language recognition; video analysis
ID MODEL
AB The goal of sign language recognition (SLR) is to help those who are hard of hearing or deaf overcome the communication barrier. Most existing approaches can be typically divided into two lines, i.e., Skeleton-based, and RGB-based methods, but both lines of methods have their limitations. Skeleton-based methods do not consider facial expressions, while RGB-based approaches usually ignore the fine-grained hand structure. To overcome both limitations, we propose a new framework called the Spatial-temporal Part-aware network (StepNet), based on RGB parts. As its name suggests, it is made up of two modules: Part-level Spatial Modeling and Part-level Temporal Modeling. Part-level Spatial Modeling, in particular, automatically captures the appearance-based properties, such as hands and faces, in the feature space without the use of any keypoint-level annotations. On the other hand, Part-level Temporal Modeling implicitly mines the long short-term context to capture the relevant attributes over time. Extensive experiments demonstrate that our StepNet, thanks to spatial-temporal modules, achieves competitive Top-1 Per-instance accuracy on three commonly used SLR benchmarks, i.e., 56.89% on WLASL, 77.2% on NMFs-CSL, and 77.1% on BOBSL. Additionally, the proposed method is compatible with the optical flow input and can produce superior performance if fused. For those who are hard of hearing, we hope that our work can act as a preliminary step.
C1 [Shen, Xiaolong; Yang, Yi] Zhejiang Univ, Hangzhou 310013, Zhejiang, Peoples R China.
   [Zheng, Zhedong] Univ Macau, Taipa 999078, Macao, Peoples R China.
C3 Zhejiang University; University of Macau
RP Shen, XL (corresponding author), Zhejiang Univ, Hangzhou 310013, Zhejiang, Peoples R China.
EM sxlongcs@zju.edu.com; zhedongzheng@um.edu.mo; yangyics@zju.edu.cn
RI Zheng, Zhedong/R-5314-2019
OI Zheng, Zhedong/0000-0002-2434-9050
FU National Natural Science Foundation of China [U2336212]; Fundamental
   Research Funds for the Central Universities [226-2022-00051]
FX This work was supported by the National Natural Science Foundation of
   China (U2336212) and the Fundamental Research Funds for the Central
   Universities (No. 226-2022-00051).
CR Albanie S, 2021, Arxiv, DOI arXiv:2111.03635
   Albanie Samuel, 2020, ECCV, P35, DOI DOI 10.1007/978-3-030-58621-8_3
   Bohácek M, 2022, IEEE WINT CONF APPL, P182, DOI 10.1109/WACVW54805.2022.00024
   Buehler P, 2009, PROC CVPR IEEE, P2953, DOI 10.1109/CVPRW.2009.5206523
   Camgoz Necati Cihan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10020, DOI 10.1109/CVPR42600.2020.01004
   Cao CQ, 2019, IEEE T CIRC SYST VID, V29, P3247, DOI 10.1109/TCSVT.2018.2879913
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Cooper H, 2012, J MACH LEARN RES, V13, P2205
   Cui RP, 2019, IEEE T MULTIMEDIA, V21, P1880, DOI 10.1109/TMM.2018.2889563
   Ding YH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3955, DOI 10.1109/ICCV48922.2021.00394
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Duan JL, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3131343
   Fan Hehe, 2022, INT C LEARN REPR ICL
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Gammulle H, 2021, IEEE T IMAGE PROCESS, V30, P7689, DOI 10.1109/TIP.2021.3108349
   Gao Z, 2022, IEEE T NEUR NET LEAR, V33, P1147, DOI 10.1109/TNNLS.2020.3041018
   Grobel K, 1997, IEEE SYS MAN CYBERN, P162, DOI 10.1109/ICSMC.1997.625742
   Guan QJ, 2020, PATTERN RECOGN LETT, V131, P38, DOI 10.1016/j.patrec.2019.11.040
   Guo D, 2020, IEEE T IMAGE PROCESS, V29, P1575, DOI 10.1109/TIP.2019.2941267
   Guo D, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3152121
   Hosain A, 2021, IEEE WINT CONF APPL, P3428, DOI 10.1109/WACV48630.2021.00347
   Hu HZ, 2023, IEEE T PATTERN ANAL, V45, P11221, DOI 10.1109/TPAMI.2023.3269220
   Hu HZ, 2021, AAAI CONF ARTIF INTE, V35, P1558
   Hu HZ, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3436754
   Hu Hezhen, 2021, ICCV, P11087
   Jiang S, 2021, arXiv
   Koller O, 2018, INT J COMPUT VISION, V126, P1311, DOI 10.1007/s11263-018-1121-3
   Koller O, 2015, COMPUT VIS IMAGE UND, V141, P108, DOI 10.1016/j.cviu.2015.09.013
   Koller Oscar, 2016, BMVC, DOI DOI 10.5244/C.30.136
   Li C, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P786
   Li DX, 2020, PROC CVPR IEEE, P6204, DOI 10.1109/CVPR42600.2020.00624
   Li DX, 2020, IEEE WINT CONF APPL, P1448, DOI [10.1109/wacv45572.2020.9093512, 10.1109/WACV45572.2020.9093512]
   Li Kexin, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P1485, DOI 10.1145/3581783.3611724
   Li Y, 2020, PROC CVPR IEEE, P906, DOI 10.1109/CVPR42600.2020.00099
   Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718
   Lin JL, 2022, IEEE T IMAGE PROCESS, V31, P3780, DOI 10.1109/TIP.2022.3175601
   Liu Haotian, 2023, NEURAL INFORM PROCES
   Liu R, 2022, FRONT COMPUT SCI-CHI, V16, DOI 10.1007/s11704-021-1248-1
   Liu X, 2021, Arxiv, DOI arXiv:2107.00285
   Liu ZY, 2020, AAAI CONF ARTIF INTE, V34, P11669
   Liu ZZ, 2024, IEEE T NEUR NET LEAR, V35, P311, DOI 10.1109/TNNLS.2022.3174031
   Liwicki Stephan, 2009, 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P50, DOI 10.1109/CVPR.2009.5204291
   Loshchilov I., 2017, PROC INT C LEARNING
   Loshchilov I, 2019, Arxiv, DOI [arXiv:1711.05101, 10.48550/arXiv.1711.05101, DOI 10.48550/ARXIV.1711.05101]
   Lu ML, 2019, IEEE T IMAGE PROCESS, V28, P3703, DOI 10.1109/TIP.2019.2901707
   Ma Fan, 2024, IEEE CVF COMP VIS PA
   Mercanoglu Sincan O, 2019, SIG PROCESS COMMUN, DOI 10.1109/siu.2019.8806467
   Miech A, 2019, IEEE I CONF COMP VIS, P2630, DOI 10.1109/ICCV.2019.00272
   Paszke A, 2019, ADV NEUR IN, V32
   Pigou L, 2018, INT J COMPUT VISION, V126, P430, DOI 10.1007/s11263-016-0957-7
   Qin ZY, 2024, IEEE T NEUR NET LEAR, V35, P4783, DOI 10.1109/TNNLS.2022.3201518
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Quan Ruijie, 2024, IEEE CVF COMP VIS PA
   Romero J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130883
   Selvaraj P., 2021, arXiv
   Shen XL, 2023, PROC CVPR IEEE, P8887, DOI 10.1109/CVPR52729.2023.00858
   Shi L, 2020, IEEE T IMAGE PROCESS, V29, P9532, DOI 10.1109/TIP.2020.3028207
   ShiguangWang Zhizhong, 2020, denseflow
   Simonyan K, 2014, ADV NEUR IN, V27
   Song SJ, 2017, AAAI CONF ARTIF INTE, P4263
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Suo YC, 2024, ACM T MULTIM COMPUT, V20, DOI 10.1145/3648368
   Tharwat A, 2015, ADV INTELL SYST, V334, P359, DOI 10.1007/978-3-319-13572-4_30
   Tomar S., 2006, Linux J, V2006, P10
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   Tunga A, 2021, IEEE WINT CONF APPL, P31, DOI 10.1109/WACVW52041.2021.00008
   Wang LC, 2014, IEEE T MULTIMEDIA, V16, P751, DOI 10.1109/TMM.2014.2298382
   Wang TY, 2022, IEEE T CIRC SYST VID, V32, P867, DOI 10.1109/TCSVT.2021.3061265
   Wikipedia Contributors, 2004, Wikipedia The Free Encyclopedia
   Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19
   Xue HY, 2017, IEEE T IMAGE PROCESS, V26, P5656, DOI 10.1109/TIP.2017.2746267
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yang H, 2020, IEEE T IMAGE PROCESS, V29, P5783, DOI 10.1109/TIP.2020.2984904
   Yang ZX, 2024, Arxiv, DOI arXiv:2401.08392
   Yang ZX, 2022, IEEE T PATTERN ANAL, V44, P4701, DOI 10.1109/TPAMI.2021.3081597
   Yasir F, 2015, 2015 IEEE 8TH INTERNATIONAL WORKSHOP ON COMPUTATIONAL INTELLIGENCE AND APPLICATIONS (IWCIA) PROCEEDINGS, P35, DOI 10.1109/IWCIA.2015.7449458
   Yu ZT, 2021, IEEE T IMAGE PROCESS, V30, P5626, DOI 10.1109/TIP.2021.3087348
   Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22
   Zhao WC, 2023, Arxiv, DOI arXiv:2302.05075
   Zhao ZQ, 2021, IEEE T IMAGE PROCESS, V30, P6544, DOI 10.1109/TIP.2021.3093397
   Zheng ZD, 2024, IEEE T NEUR NET LEAR, V35, P7534, DOI 10.1109/TNNLS.2022.3214834
   Zheng ZD, 2019, IEEE T CIRC SYST VID, V29, P3037, DOI 10.1109/TCSVT.2018.2873599
   Zhou H, 2022, IEEE T MULTIMEDIA, V24, P768, DOI 10.1109/TMM.2021.3059098
NR 83
TC 0
Z9 0
U1 1
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2024
VL 20
IS 7
SI SI
AR 226
DI 10.1145/3656046
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL0Q6
UT WOS:001234494100041
OA Bronze
DA 2024-08-05
ER

PT J
AU Yang, WY
   Wu, SC
   Fei, JW
   Zeng, XW
   Ding, YM
   Xia, ZH
AF Yang, Wenyuan
   Wu, Shaocong
   Fei, Jianwei
   Zeng, Xianwang
   Ding, Yuemin
   Xia, Zhihua
TI A Bitcoin-based Secure Outsourcing Scheme for Optimization Problem in
   Multimedia Internet of Things
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Secure outsourcing computation; optimization problem; Bitcoin;
   multi-media internet of things
ID PAYMENT
AB With the development of the Internet of Things (IoT) and cloud computing, various multimedia data such as audio, video, and images have experienced explosive growth, ushering in the era of big data. Large-scale computing tasks in the Multimedia Internet of Things (M-IoT), such as mathematical optimization problems, have begun to be outsourced from IoT devices with limited computing power to cloud servers for execution. However, outsourcing computation brings security concerns, because the behaviors of clouds are invisible to users. The leakage of privacy data in outsourced optimization problems leads to immeasurable losses. Themutual distrust between clouds and users causes that the correctness of the optimal decisions and the fairness of the payment activities are not guaranteed. Blockchain technology has the characteristic of immutability and has become a new security paradigm for eliminating multi-party trust concerns. In this article, we propose a Bitcoin-based secure outsourcing scheme to address the aforementioned security concerns. To prevent confidential data leakage, the proposed scheme designs a computable privacy-preserving method for the outsourced optimization problems. To judge the correctness of the optimal decision and reduce verification costs, the proposed scheme designs a low-cost two-layer verification mechanism based on dual theory and blockchain technology. Blockchain nodes reach a consensus on the problem solutions and trigger an automatic fair payment protocol-based Bitcoin. Security analysis and experimental results demonstrate that our scheme guarantees privacy, fairness, and computational efficiency.
C1 [Yang, Wenyuan] Sun Yat Sen Univ, 66 Gongchang Rd, Shenzhen 518107, Guangdong, Peoples R China.
   [Wu, Shaocong] Xi An Jiao Tong Univ, 28 Xianning West Rd, Xian 710049, Shaanxi, Peoples R China.
   [Fei, Jianwei] Nanjing Univ Informat Sci & Technol, 219 Ningliu Rd, Nanjing 210044, Jiangsu, Peoples R China.
   [Zeng, Xianwang] Univ Elect Sci & Technol China, 2006 Xiyuan Ave, Chengdu 611731, Sichuan, Peoples R China.
   [Ding, Yuemin] Univ Navarra Tecnun, Paseo Manuel Lardizabal 13, San Sebastian 20018, Guipuzcoa, Spain.
   [Xia, Zhihua] Jinan Univ, 601 Huangpu Ave, Guangzhou 510632, Guangdong, Peoples R China.
C3 Sun Yat Sen University; Xi'an Jiaotong University; Nanjing University of
   Information Science & Technology; University of Electronic Science &
   Technology of China; University of Navarra; Jinan University
RP Zeng, XW (corresponding author), Univ Elect Sci & Technol China, 2006 Xiyuan Ave, Chengdu 611731, Sichuan, Peoples R China.
EM yangwy56@mail.sysu.edu.cn; shaocong.wsc@stu.xjtu.edu.cn;
   fjw826244895@163.com; xwzzeng@gmail.com; yueminding@tecnun.es;
   xia_zhihua@163.com
RI Fei, Jianwei/HJB-2007-2022
OI Fei, Jianwei/0000-0002-1243-3909; Ding, Yuemin/0000-0001-7697-2197;
   Yang, Wenyuan/0000-0003-0046-5994
FU National Key Research and Development Program of China [2022YFB2703303];
   National Natural Science Foundation of China [62122032]; Aeronautical
   Science Foundation of China [2022Z0660M1001]
FX This work is supported by the National Key Research and Development
   Program of China (No. 2022YFB2703303), the National Natural Science
   Foundation of China (No. 62122032), and the Aeronautical Science
   Foundation of China (No. 2022Z0660M1001).
CR Chakraborty Partha Sarathi, 2022, P IEEE INT C BLOCKCH, P1
   Cheng Huajie, 2022, 2022 IEEE 8th International Conference on Computer and Communications (ICCC), P1344, DOI 10.1109/ICCC56324.2022.10065644
   Dwivedi AD, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19020326
   Fang LM, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3408322
   Gan QQ, 2018, SCI CHINA INFORM SCI, V61, DOI 10.1007/s11432-017-9410-9
   Garg N, 2023, CONSUM COMM NETWORK, DOI 10.1109/CCNC51644.2023.10059690
   Ge CP, 2024, IEEE T DEPEND SECURE, V21, P937, DOI 10.1109/TDSC.2023.3265932
   Gennaro R, 2010, LECT NOTES COMPUT SC, V6223, P465, DOI 10.1007/978-3-642-14623-7_25
   Gentry C., 2009, FULLY HOMOMORPHIC EN, P43
   Huang PQ, 2020, IEEE TETCI, V4, P324, DOI 10.1109/TETCI.2019.2939373
   Khan AA, 2022, IEEE ACCESS, V10, P122679, DOI 10.1109/ACCESS.2022.3223370
   Kumar Sumita, 2023, 2023 International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT), P976, DOI 10.1109/IDCIoT56793.2023.10053430
   Leiserson Charles Eric, 1994, Introduction to Algorithms, V3
   Liang HR, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3415151
   Liao YJ, 2018, COMPUT STAND INTER, V56, P101, DOI 10.1016/j.csi.2017.09.008
   Lin C, 2021, IEEE T INF FOREN SEC, V16, P3241, DOI 10.1109/TIFS.2021.3073818
   Lu X, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1129, DOI 10.1145/3343031.3350999
   Luenberger DG, 2008, INT SER OPER RES MAN, V116, P1
   Nauman A, 2020, IEEE ACCESS, V8, P8202, DOI 10.1109/ACCESS.2020.2964280
   Nie HX, 2014, INT CON ADV INFO NET, P591, DOI 10.1109/AINA.2014.147
   Paindavoine Marie., 2015, SAC, P25
   Pearson S, 2010, 2010 IEEE 2 INT C CL, P693, DOI DOI 10.1109/CLOUDCOM.2010.66
   Popovic Kresimir, 2010, 2010 33rd International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO), P344
   Qin Z, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2978574
   Rahman MS, 2022, J IND INF INTEGR, V30, DOI 10.1016/j.jii.2022.100408
   Ren Guangjie, 2019, P ACM MULT AS C, P1
   Sangaiah AK, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20020539
   Shah Mohsin, 2019, ACM Transactions on Multimedia Computing, Communications and Applications, V15, DOI 10.1145/3325194
   Shan ZH, 2018, ACM COMPUT SURV, V51, DOI 10.1145/3158363
   Tanwar VK, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3380743
   Ullah Z, 2022, IEEE ACCESS, V10, P36978, DOI 10.1109/ACCESS.2022.3164081
   Wang C, 2016, IEEE T COMPUT, V65, P216, DOI 10.1109/TC.2015.2417542
   Xiang T, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3531016
   Xiao MB, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P708, DOI 10.1145/3123266.3123339
   Xie JX, 2023, COMPUT ELECTRON AGR, V204, DOI 10.1016/j.compag.2022.107493
   Yang, 2021, ACM T SENSOR NETWORK, V17, P1
   Yang WY, 2021, IEEE T INF FOREN SEC, V16, P100, DOI 10.1109/TIFS.2020.3001728
   Yogita, 2020, Proceedings of the 3rd International Conference on Intelligent Sustainable Systems (ICISS 2020), P1284, DOI 10.1109/ICISS49785.2020.9315948
   Zhang YH, 2021, IEEE T SERV COMPUT, V14, P1152, DOI 10.1109/TSC.2018.2864191
   Zhao LC, 2021, IEEE T PARALL DISTR, V32, P2524, DOI 10.1109/TPDS.2021.3068195
   Zhi-kuanWang Liu, 2017, P INT C WIR COMM NET, P185
   Zou X, 2023, IEEE ACCESS, V11, P30118, DOI 10.1109/ACCESS.2023.3261560
NR 42
TC 1
Z9 1
U1 1
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2024
VL 20
IS 6
SI SI
AR 152
DI 10.1145/3637489
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OQ2T3
UT WOS:001208681800002
OA Bronze
DA 2024-08-05
ER

PT J
AU Li, JY
   Mao, ZD
   Li, H
   Chen, WD
   Zhang, YD
AF Li, Jingyu
   Mao, Zhendong
   Li, Hao
   Chen, Weidong
   Zhang, Yongdong
TI Exploring Visual Relationships via Transformer-based Graphs for Enhanced
   Image Captioning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image captioning; transformer; scene graph; topology graph
ID REPRESENTATION
AB Image captioning (IC), bringing vision to language, has drawn extensive attention. A crucial aspect of IC is the accurate depiction of visual relations among image objects. Visual relations encompass two primary facets: content relations and structural relations. Content relations, which comprise geometric positions content (i.e., distances and sizes) and semantic interactions content (i.e., actions and possessives), unveil the mutual correlations between objects. In contrast, structural relations pertain to the topological connectivity of object regions. Existing Transformer-based methods typically resort to geometric positions to enhance the visual relations, yet only using the shallow geometric content is unable to precisely cover actional content correlations and structural connection relations. In this article, we adopt a comprehensive perspective to examine the correlations between objects, incorporating both content relations (i.e., geometric and semantic relations) and structural relations, with the aim of generating plausible captions. To achieve this, first, we construct a geometric graph from bounding box features and a semantic graph from the scene graph parser to model the content relations. Innovatively, we construct a topology graph that amalgamates the sparsity characteristics of the geometric and semantic graphs, enabling the representation of image structural relations. Second, we propose a novel unified approach to enrich image relation representations by integrating semantic, geometric, and structural relations into self-attention. Finally, in the language decoding stage, we further leverage the semantic relation as prior knowledge to generate accurate words. Extensive experiments on MS-COCO dataset demonstrate the effectiveness of our model, with improvements of CIDEr from 128.6% to 136.6%. Codes have been released at https://github.com/CrossmodalGroup/ER-SAN/tree/main/VG-Cap.
C1 [Li, Jingyu; Mao, Zhendong; Li, Hao; Chen, Weidong; Zhang, Yongdong] Univ Sci & Technol China, Hefei 230022, Anhui, Peoples R China.
   [Mao, Zhendong; Zhang, Yongdong] Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei 230022, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Mao, ZD (corresponding author), Univ Sci & Technol China, Hefei 230022, Anhui, Peoples R China.; Mao, ZD (corresponding author), Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei 230022, Anhui, Peoples R China.
EM jingyuli@mail.ustc.edu.cn; zdmao@ustc.edu.cn; lihaohn@mail.ustc.edu.cn;
   chenweidong@ustc.edu.cn; zhyd73@ustc.edu.cn
OI Chen, Weidong/0000-0003-2774-2875
FU National Natural Science Foundation of China [62302474, 62222212,
   U19A2057]; Science Fund for Creative Research Groups [62121002]
FX This work is supported by the National Natural Science Foundation of
   China under Grant No. 62222212, the National Natural Science Foundation
   of China under Grant No. U19A2057, the Science Fund for Creative
   Research Groups under Grant No. 62121002, and the National Natural
   Science Foundation of China under Grant No. 62302474.
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24
   Banerjee S., 2005, ACL WORKSH INTR EXTR, P65
   Cao S, 2022, IEEE T CIRC SYST VID, V32, P7005, DOI 10.1109/TCSVT.2022.3178844
   Carion N., 2020, EUR C COMP VIS, P213
   Child R, 2019, Arxiv, DOI arXiv:1904.10509
   Cornia Marcella, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10575, DOI 10.1109/CVPR42600.2020.01059
   Fan ZH, 2021, Arxiv, DOI arXiv:2106.10936
   Fei ZC, 2022, AAAI CONF ARTIF INTE, P607
   Feng QY, 2020, IEEE T CIRC SYST VID, V30, P3413, DOI 10.1109/TCSVT.2020.2965966
   Guo LT, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P765, DOI 10.1145/3343031.3350943
   Herdade S, 2019, ADV NEUR IN, V32
   Huang FC, 2022, INT CONF ACOUST SPEE, P1945, DOI 10.1109/ICASSP43922.2022.9747820
   Huang L, 2019, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2019.00473
   Huang L, 2019, ADV NEUR IN, V32
   Ji JY, 2022, IEEE T IMAGE PROCESS, V31, P4321, DOI 10.1109/TIP.2022.3183434
   Ji JY, 2021, AAAI CONF ARTIF INTE, V35, P1655
   Jiang Huaizu, 2020, P IEEE CVF C COMP VI, P10267, DOI DOI 10.1109/CVPR42600.2020.01028
   Jiang WT, 2022, IEEE T CIRC SYST VID, V32, P7706, DOI 10.1109/TCSVT.2022.3181490
   Jiang WT, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3460474
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Li J., 2022, P INT JOINT C ART IN, P1056
   Li ZL, 2021, Arxiv, DOI arXiv:2012.15150
   Lin C.-Y., 2004, ANN M ASS COMP LING, P74
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu AA, 2024, IEEE T MULTIMEDIA, V26, P1639, DOI 10.1109/TMM.2023.3284594
   Liu AA, 2024, IEEE T KNOWL DATA EN, V36, P35, DOI 10.1109/TKDE.2023.3284032
   Liu AA, 2022, IEEE T CIRC SYST VID, V32, P3685, DOI 10.1109/TCSVT.2021.3107035
   Liu AA, 2017, IEEE T PATTERN ANAL, V39, P102, DOI 10.1109/TPAMI.2016.2537337
   Liu S., 2022, arXiv
   Liu XX, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3409388
   Longteng Guo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10324, DOI 10.1109/CVPR42600.2020.01034
   Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345
   Luo YP, 2021, Arxiv, DOI arXiv:2101.06462
   Kipf TN, 2017, Arxiv, DOI arXiv:1609.02907
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Ren S., 2016, Faster r-cnn: Towards real-time object detection with region proposal networks
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Shi Z, 2020, Arxiv, DOI arXiv:2006.11807
   Song ZL, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5056, DOI 10.1145/3474085.3475607
   Vaswani A, 2017, ADV NEUR IN, V30
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Wang XH, 2023, PROC CVPR IEEE, P19048, DOI 10.1109/CVPR52729.2023.01826
   Wang YH, 2022, IEEE T CIRC SYST VID, V32, P4417, DOI 10.1109/TCSVT.2021.3121062
   Wu H, 2022, ACM T SENSOR NETWORK, V18, DOI 10.1145/3470850
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xu Y., 2022, arXiv
   Yan CG, 2022, IEEE T CIRC SYST VID, V32, P43, DOI 10.1109/TCSVT.2021.3067449
   Yang X, 2019, IEEE I CONF COMP VIS, P4249, DOI 10.1109/ICCV.2019.00435
   Yang X, 2019, PROC CVPR IEEE, P10677, DOI 10.1109/CVPR.2019.01094
   Yang Y, 2021, FRONT INFORM TECH EL, V22, P1551, DOI 10.1631/FITEE.2100463
   Yao T, 2019, IEEE I CONF COMP VIS, P2621, DOI 10.1109/ICCV.2019.00271
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yingwei Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10968, DOI 10.1109/CVPR42600.2020.01098
   Yu J, 2020, IEEE T CIRC SYST VID, V30, P4467, DOI 10.1109/TCSVT.2019.2947482
   Yuan MQ, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3532627
   Zhang K, 2024, IEEE T CIRC SYST VID, V34, P2973, DOI 10.1109/TCSVT.2023.3307554
   Zhang K, 2023, IEEE T MULTIMEDIA, V25, P1320, DOI 10.1109/TMM.2022.3141603
   Zhang K, 2022, PROC CVPR IEEE, P15640, DOI 10.1109/CVPR52688.2022.01521
   Zhang XY, 2021, PROC CVPR IEEE, P15460, DOI 10.1109/CVPR46437.2021.01521
   Zhang ZS, 2020, AAAI CONF ARTIF INTE, V34, P9636
   Zhao Guangxiang, 2019, Explicit Sparse Transformer: Concentrated Attention through Explicit Selection
   Zhou TF, 2022, IEEE T PATTERN ANAL, V44, P2827, DOI 10.1109/TPAMI.2021.3049156
NR 64
TC 0
Z9 0
U1 6
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2024
VL 20
IS 5
AR 133
DI 10.1145/3638558
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MF3R9
UT WOS:001192177900013
DA 2024-08-05
ER

PT J
AU Qi, L
   Dong, P
   Xiong, T
   Xue, H
   Geng, X
AF Qi, Lei
   Dong, Peng
   Xiong, Tan
   Xue, Hui
   Geng, Xin
TI DoubleAUG: Single-domain Generalized Object Detector in Urban via Color
   Perturbation and Dual-style Memory
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Single-domain generalization; Object detection; DoubleAUG
AB Object detection in urban scenarios is crucial for autonomous driving in intelligent traffic systems. However, unlike conventional object detection tasks, urban-scene images vary greatly in style. For example, images taken on sunny days differ significantly from those taken on rainy days. Therefore, models trained on sunny-day images may not generalize well to rainy-day images. In this article, we aim to solve the single-domain generalizable object detection task in urban scenarios, meaning that a model trained on images from one weather condition should be able to perform well on images from any other weather conditions. To address this challenge, we propose a novel Double AUGmentation (DoubleAUG) method that includes image- and feature-level augmentation schemes. In the image-level augmentation, we consider the variation in color information across different weather conditions and propose a Color Perturbation (CP) method that randomly exchanges the RGB channels to generate various images. In the feature-level augmentation, we propose to utilize a Dual-Style Memory (DSM) to explore the diverse style information on the entire dataset, further enhancing the model's generalization capability. Extensive experiments demonstrate that our proposed method outperforms state-of-the-artmethods. Furthermore, ablation studies confirm the effectiveness of each module in our proposed method. Moreover, our method is plug-and-play and can be integrated into existing methods to further improve model performance.
C1 [Qi, Lei; Dong, Peng; Xiong, Tan; Xue, Hui; Geng, Xin] Southeast Univ, Minist Educ, Key Lab New Generat Artificial Intelligence Techn, Nanjing, Peoples R China.
C3 Southeast University - China
RP Geng, X (corresponding author), Southeast Univ, Minist Educ, Key Lab New Generat Artificial Intelligence Techn, Nanjing, Peoples R China.
EM qilei@seu.edu.cn; dongpeng@seu.edu.cn; xiongtan@seu.edu.cn;
   hxue@seu.edu.cn; xgeng@seu.edu.cn
OI Qi, Lei/0000-0001-7091-0702; Xue, Hui/0000-0002-5856-4445
FU NSFC Program [62206052, 62125602, 62076063]; Jiangsu Natural Science
   Foundation Project [BK20210224]
FX This work is supported by NSFC Program (Grant Nos. 62206052, 62125602,
   62076063) and Jiangsu Natural Science Foundation Project (Grant No.
   BK20210224).
CR Albuquerque I, 2021, Arxiv, DOI [arXiv:1911.00804, 10.48550/arXiv.1911.00804]
   Aversa R, 2020, DATA INTELLIGENCE, V2, P513, DOI 10.1162/dint_a_00062
   Chang-Dong Xu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11721, DOI 10.1109/CVPR42600.2020.01174
   Chen C., 2020, P IEEECVF C COMPUTER, P8869, DOI DOI 10.1109/CVPR42600.2020.00889
   Chen CQ, 2022, PROC CVPR IEEE, P7109, DOI 10.1109/CVPR52688.2022.00698
   Chen Y, 2018, PROC CVPR IEEE, P3339, DOI 10.1109/CVPR.2018.00352
   Cheng-Chun Hsu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P733, DOI 10.1007/978-3-030-58545-7_42
   Choi S, 2021, PROC CVPR IEEE, P11575, DOI 10.1109/CVPR46437.2021.01141
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fan XJ, 2021, PROC CVPR IEEE, P8204, DOI 10.1109/CVPR46437.2021.00811
   Fei B, 2023, PROC CVPR IEEE, P9935, DOI 10.1109/CVPR52729.2023.00958
   Fengchun Qiao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12553, DOI 10.1109/CVPR42600.2020.01257
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Harary S, 2022, PROC CVPR IEEE, P5270, DOI 10.1109/CVPR52688.2022.00521
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hsu HK, 2020, IEEE WINT CONF APPL, P738, DOI [10.1109/wacv45572.2020.9093358, 10.1109/WACV45572.2020.9093358]
   Hu QC, 2016, IEEE T INTELL TRANSP, V17, P1002, DOI 10.1109/TITS.2015.2496795
   Hu XW, 2019, PROC CVPR IEEE, P8014, DOI 10.1109/CVPR.2019.00821
   Huang L, 2019, PROC CVPR IEEE, P4869, DOI 10.1109/CVPR.2019.00501
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Jocher G., YoloV5
   Johnson-Roberson Matthew, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P746, DOI 10.1109/ICRA.2017.7989092
   Kaiyang Zhou, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P561, DOI 10.1007/978-3-030-58517-4_33
   Kang J, 2022, PROC CVPR IEEE, P7120, DOI 10.1109/CVPR52688.2022.00699
   Li D, 2018, AAAI CONF ARTIF INTE, P3490
   Li WY, 2022, PROC CVPR IEEE, P5281, DOI 10.1109/CVPR52688.2022.00522
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu Y., 2023, ACM Trans. Multimedia Comput. Commun. Appl., V19, P1
   Meng R, 2022, LECT NOTES COMPUT SC, V13694, P322, DOI 10.1007/978-3-031-19830-4_19
   Min S, 2022, LECT NOTES COMPUT SC, V13697, P37, DOI 10.1007/978-3-031-19836-6_3
   Minghao Xu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12352, DOI 10.1109/CVPR42600.2020.01237
   Pan XG, 2019, IEEE I CONF COMP VIS, P1863, DOI 10.1109/ICCV.2019.00195
   Pan XG, 2018, LECT NOTES COMPUT SC, V11208, P484, DOI 10.1007/978-3-030-01225-0_29
   Paszke A, 2019, ADV NEUR IN, V32
   Redmon J., 2018, CoRR
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Saito K, 2019, PROC CVPR IEEE, P6949, DOI 10.1109/CVPR.2019.00712
   Sakaridis C, 2018, INT J COMPUT VISION, V126, P973, DOI 10.1007/s11263-018-1072-8
   Shujun Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P159, DOI 10.1007/978-3-030-58545-7_10
   Teney D, 2022, PROC CVPR IEEE, P16740, DOI 10.1109/CVPR52688.2022.01626
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Volpi R., 2018, NeurIPS, V31, P5334
   Wan CQ, 2022, PROC CVPR IEEE, P4672, DOI 10.1109/CVPR52688.2022.00464
   Wang Jindong, 2021, INT JOINT C ARTIFICI, P4627
   Wang X, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9123, DOI 10.1109/ICCV48922.2021.00901
   Wang ZJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P814, DOI 10.1109/ICCV48922.2021.00087
   Wu AM, 2022, PROC CVPR IEEE, P837, DOI 10.1109/CVPR52688.2022.00092
   Wu A, 2022, IEEE T PATTERN ANAL, V44, P4178, DOI 10.1109/TPAMI.2021.3060446
   Wu JX, 2022, PROC CVPR IEEE, P5291, DOI 10.1109/CVPR52688.2022.00523
   Wu L, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3486251
   Wu Y., 2019, Detectron2
   Xu YF, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3501800
   Yang YZ, 2022, LECT NOTES COMPUT SC, V13680, P57, DOI 10.1007/978-3-031-20044-1_4
   Yao XF, 2022, PROC CVPR IEEE, P7087, DOI 10.1109/CVPR52688.2022.00696
   Zhang HL, 2022, PROC CVPR IEEE, P8014, DOI 10.1109/CVPR52688.2022.00786
   Zhang J, 2022, LECT NOTES COMPUT SC, V13687, P161, DOI 10.1007/978-3-031-19812-0_10
   Zhang XX, 2022, PROC CVPR IEEE, P4900, DOI 10.1109/CVPR52688.2022.00486
   Zhang YB, 2022, PROC CVPR IEEE, P8025, DOI 10.1109/CVPR52688.2022.00787
   Zhang YH, 2024, DATA INTELLIGENCE, V6, P183, DOI 10.1162/dint_a_00199
   Zhou K, 2021, INT C LEARNING REPRE
   Zhou KY, 2020, AAAI CONF ARTIF INTE, V34, P13025
NR 65
TC 0
Z9 0
U1 2
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2024
VL 20
IS 5
AR 126
DI 10.1145/3634683
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MF3R9
UT WOS:001192177900006
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Zhang, YZ
   Yu, Y
   Wang, MY
   Huang, M
   Hossain, MS
AF Zhang, Yazhou
   Yu, Yang
   Wang, Mengyao
   Huang, Min
   Hossain, M. Shamim
TI Self-Adaptive Representation Learning Model for Multi-Modal Sentiment
   and Sarcasm Joint Analysis
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Multi-modal sentiment analysis; sarcasm detection; representation
   learning; multi-task learning
AB Sentiment and sarcasm are intimate and complex, as sarcasm often deliberately elicits an emotional response in order to achieve its specific purpose. Current challenges in multi-modal sentiment and sarcasm joint detection mainly include multi-modal representation fusion and the modeling of the intrinsic relationship between sentiment and sarcasm. To address these challenges, we propose a single-input stream self-adaptive representation learning model (SRLM) for sentiment and sarcasm joint recognition. Specifically, we divide the image into blocks to learn its serialized features and fuse textual feature as input to the target model. Then, we introduce an adaptive representation learning network using a gated network approach for sarcasm and sentiment classification. In this framework, each task is equipped with its dedicated expert network responsible for learning task-specific information, while the shared expert knowledge is acquired and weighted through the gating network. Finally, comprehensive experiments conducted on two publicly available datasets, namely Memotion and MUStARD, demonstrate the effectiveness of the proposed model when compared to state-of-the-art baselines. The results reveal a notable improvement on the performance of sentiment and sarcasm tasks.
C1 [Zhang, Yazhou] Chongqing Univ, Minist Educ, Key Lab Dependable Serv Comp Cyber Phys Soc, Chongqing, Peoples R China.
   [Zhang, Yazhou] China Mobile Commun Grp Tianjin Co, Articial Intelligence Lab, Tianjin, Peoples R China.
   [Zhang, Yazhou] Zhengzhou Univ Light Ind, Zhengzhou, Peoples R China.
   [Yu, Yang; Wang, Mengyao; Huang, Min] Zhengzhou Univ Light Ind, Software Engn Coll, 136 Sci Ave, Zhengzhou 450048, Peoples R China.
   [Hossain, M. Shamim] King Saud Univ, Coll Comp & Informat Sci, Dept Software Engn, Riyadh 13272, Saudi Arabia.
C3 Chongqing University; Zhengzhou University of Light Industry; Zhengzhou
   University of Light Industry; King Saud University
RP Hossain, MS (corresponding author), King Saud Univ, Coll Comp & Informat Sci, Dept Software Engn, Riyadh 13272, Saudi Arabia.
EM yzzhang@zzuli.edu.cn; yuyang19980818@outlook.com;
   wangmengyao516@outlook.com; huangmin@zzuli.edu.cn; mshossain@ksu.edu.sa
RI Min, Huang/KLZ-0497-2024
OI Yu, Yang/0009-0000-4607-8661; Hossain, M. Shamim/0000-0001-5906-9422
FU King Saud University, Riyadh, Saudi Arabia [RSP2024R32]; Foundation of
   Key Laboratory of Dependable Service Computing in CyberPhysical-Society
   (Ministry of Education), Chongqing University [CPSDSC202103]; National
   Science Foundation of China [62006212, 61702462]; China Postdoctoral
   Science Foundation [2023M733907]
FX This work was supported by the Researchers Supporting Project number
   (RSP2024R32), King Saud University, Riyadh, Saudi Arabia. This is also
   supported by the Foundation of Key Laboratory of Dependable Service
   Computing in CyberPhysical-Society (Ministry of Education), Chongqing
   University (PJ.No: CPSDSC202103), National Science Foundation of China
   under grant No. 62006212, 61702462, Fellowship from the China
   Postdoctoral Science Foundation (2023M733907).
CR Akhtar MS, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P370
   Albraikan A, 2019, IEEE SENS J, V19, P8402, DOI 10.1109/JSEN.2018.2867221
   Albraikan A, 2018, IEEE ACCESS, V6, P78780, DOI 10.1109/ACCESS.2018.2885279
   Cai YT, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P2506
   Chaturvedi I, 2021, COGN COMPUT, V13, P96, DOI 10.1007/s12559-020-09807-4
   Chauhan DS, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P4351
   Chhavi Sharma, 2020, P 14 INT WORKSHOP SE
   Cramer J, 2019, INT CONF ACOUST SPEE, P3852, DOI 10.1109/ICASSP.2019.8682475
   Deng Y, 2023, PROCEEDINGS OF THE 61ST ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2023): LONG PAPERS, VOL 1, P12272
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Ding N, 2022, MULTIMED TOOLS APPL, V81, P8597, DOI 10.1007/s11042-022-12122-9
   Dosovitskiy A., 2021, ICLR
   El Saddik A., 2001, IEEE Multimedia, V8, P30, DOI 10.1109/93.939998
   Farias DIH, 2017, SENTIMENT ANALYSIS IN SOCIAL NETWORKS, P113, DOI 10.1016/B978-0-12-804412-4.00007-3
   Ghosal D, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P3454
   He JX, 2022, IEEE SIGNAL PROC LET, V29, P454, DOI 10.1109/LSP.2021.3139856
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hossain MS, 2019, INFORM FUSION, V49, P69, DOI 10.1016/j.inffus.2018.09.008
   Hossain MS, 2018, IEEE INTERNET THINGS, V5, P2399, DOI 10.1109/JIOT.2017.2772959
   Kim K, 2023, INFORM FUSION, V92, P37, DOI 10.1016/j.inffus.2022.11.022
   Kumar A, 2020, INT CONF ACOUST SPEE, P4477, DOI 10.1109/ICASSP40776.2020.9053012
   Liang B, 2022, PROCEEDINGS OF THE 60TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), VOL 1: (LONG PAPERS), P1767
   Liu H., 2022, P 2022 C EMP METH NA, P4995
   Liu Yi, 2022, 2022 IEEE Smartworld, Ubiquitous Intelligence & Computing, Scalable Computing & Communications, Digital Twin, Privacy Computing, Metaverse, Autonomous & Trusted Vehicles (SmartWorld/UIC/ScalCom/DigitalTwin/PriComp/Meta), P508, DOI 10.1109/SmartWorld-UIC-ATC-ScalCom-DigitalTwin-PriComp-Metaverse56740.2022.00090
   Liu Y, 2024, IEEE T NEUR NET LEAR, V35, P10006, DOI 10.1109/TNNLS.2023.3238337
   Lu Xinkai, 2022, 2022 IEEE Smartworld, Ubiquitous Intelligence & Computing, Scalable Computing & Communications, Digital Twin, Privacy Computing, Metaverse, Autonomous & Trusted Vehicles (SmartWorld/UIC/ScalCom/DigitalTwin/PriComp/Meta), P1455, DOI 10.1109/SmartWorld-UIC-ATC-ScalCom-DigitalTwin-PriComp-Metaverse56740.2022.00210
   Majumder N, 2019, IEEE INTELL SYST, V34, P38, DOI 10.1109/MIS.2019.2904691
   Moin A, 2023, J SUPERCOMPUT, V79, P9320, DOI 10.1007/s11227-022-05026-w
   Peng JJ, 2023, EXPERT SYST APPL, V221, DOI 10.1016/j.eswa.2023.119721
   Potamias RA, 2020, NEURAL COMPUT APPL, V32, P17309, DOI 10.1007/s00521-020-05102-3
   Pramanick S, 2022, IEEE WINT CONF APPL, P546, DOI 10.1109/WACV51458.2022.00062
   Qiao Y., 2023, P AAAI C ART INT, V37, P9507
   Riloff E., 2013, P 18 C EMP METH NAT, P704
   Schifanella R., 2016, P 24 ACM INT C MULT, P1136, DOI 10.1145/2964284.2964321
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tan Yue, 2023, FINDINGS ASS COMPUTA, P6506
   Tsai YHH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6558, DOI 10.18653/v1/p19-1656
   Vaswani A, 2017, ADV NEUR IN, V30
   Vitman O, 2023, EXPERT SYST APPL, V234, DOI 10.1016/j.eswa.2023.121068
   Vlad George-Alexandru, 2020, P 14 WORKSH SEM EV, P1208
   Wang XB, 2020, TWENTY-FIFTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXV), P19, DOI 10.1145/3373376.3378532
   Wei YW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1437, DOI 10.1145/3343031.3351034
   Wen CS, 2023, PROC CVPR IEEE, P2540, DOI 10.1109/CVPR52729.2023.00250
   Yang B, 2022, IEEE-ACM T AUDIO SPE, V30, P2015, DOI 10.1109/TASLP.2022.3178204
   Yin CY, 2021, PATTERN RECOGN IMAGE, V31, P103, DOI 10.1134/S105466182101017X
   Zadeh A., 2017, C EMP METH NAT LANG
   Zhang QG, 2023, APPL INTELL, V53, P16332, DOI 10.1007/s10489-022-03343-4
   Zhang Xiaoheng, 2023, P 61 ANN M ASS COMPU, V1, P13099
   Zhang YH, 2023, IONICS, V29, P3505, DOI 10.1007/s11581-023-05076-x
   [张乐 Zhang Yue], 2023, [人类学学报, Acta Anthropologica Sinica], V42, P1
   Zhao HJ, 2023, PATTERN RECOGN LETT, V168, P10, DOI 10.1016/j.patrec.2023.02.023
NR 51
TC 0
Z9 0
U1 7
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2024
VL 20
IS 5
AR 125
DI 10.1145/3635311
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MF3R9
UT WOS:001192177900005
DA 2024-08-05
ER

PT J
AU Liang, RJ
   Zhang, SC
   Zhang, WZ
   Zhang, GX
   Tang, JY
AF Liang, Rongjiao
   Zhang, Shichao
   Zhang, Wenzhen
   Zhang, Guixian
   Tang, Jinyun
TI Nonlocal Hybrid Network for Long-tailed Image Classification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Nonlocal module; balanced contrastive loss; logits adjustment;
   long-tailed classification
AB It is a significant issue to deal with long-tailed data when classifying images. A nonlocal hybrid network (NHN) that takes into account both feature learning and classifier learning is proposed. The NHN can capture the existence of dependencies between two locations that are far away from each other as well as alleviate the impact of long-tailed data on the model to some extent. The dependency relationship between distant pixels is obtained first through a nonlocal module to extract richer feature representations. Then, a learnable soft class center is proposed to balance the supervised contrastive loss and reduce the impact of long-tailed data on feature learning. For efficiency, a logit adjustment strategy is adopted to correct the bias caused by the different label distributions between the training and test sets and obtain a classifier that is more suitable for long-tailed data. Finally, extensive experiments are conducted on two benchmark datasets, the long-tailed CIFAR and the large-scale real-world iNaturalist 2018, both of which have imbalanced label distributions. The experimental results show that the proposed NHN model is efficient and promising.
C1 [Liang, Rongjiao; Zhang, Shichao; Zhang, Wenzhen; Zhang, Guixian; Tang, Jinyun] Guangxi Normal Univ, Guangxi Key Lab MultiSource Informat Min & Secur, Guilin 541004, Guangxi, Peoples R China.
C3 Guangxi Normal University
RP Liang, RJ (corresponding author), Guangxi Normal Univ, Guangxi Key Lab MultiSource Informat Min & Secur, Guilin 541004, Guangxi, Peoples R China.
EM lrj36906@outlook.com; zhangsc@mailbox.gxnu.edu.cn;
   jianjiu17@outlook.com; zgxcs@stu.gxnu.edu.cn; tangjy1101@outlook.com
RI liang, Rongjiao/KXR-5385-2024; Zhang, Guixian/JXO-0651-2024
OI liang, Rongjiao/0000-0002-8514-9263; Tang, Jinyun/0009-0000-1442-5980
FU Project of Guangxi Science and Technology [GuiKeAB23026040]; Research
   Fund of Guangxi Key Lab of Multi-source Information Mining Security
   [20-A-01-02]
FX This research is supported by the Project of Guangxi Science and
   Technology (grant no. GuiKeAB23026040) and the Research Fund of Guangxi
   Key Lab of Multi-source Information Mining & Security (grant no.
   20-A-01-02).
CR Boyan Zhou, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9716, DOI 10.1109/CVPR42600.2020.00974
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Buda M, 2018, NEURAL NETWORKS, V106, P249, DOI 10.1016/j.neunet.2018.07.011
   Cao KD, 2019, ADV NEUR IN, V32
   Cao Y, 2023, IEEE T PATTERN ANAL, V45, P6881, DOI 10.1109/TPAMI.2020.3047209
   Chen ZN, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3231742
   Cui JQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P695, DOI 10.1109/ICCV48922.2021.00075
   Cui Y, 2019, PROC CVPR IEEE, P9260, DOI 10.1109/CVPR.2019.00949
   Cui Y, 2018, PROC CVPR IEEE, P4109, DOI 10.1109/CVPR.2018.00432
   Deng ZY, 2021, PROC CVPR IEEE, P10498, DOI 10.1109/CVPR46437.2021.01036
   Goyal P, 2018, Arxiv, DOI arXiv:1706.02677
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hong Y, 2021, PROC CVPR IEEE, P6622, DOI 10.1109/CVPR46437.2021.00656
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang C, 2020, IEEE T PATTERN ANAL, V42, P2781, DOI 10.1109/TPAMI.2019.2914680
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Kang B., 2020, 8 INT C LEARN REPR I
   Kang Bingyi, 2019, arXiv
   Khan SH, 2018, IEEE T NEUR NET LEAR, V29, P3573, DOI 10.1109/TNNLS.2017.2732482
   Khosla P., 2020, Adv. Neural Inf. Process. Syst, P18661, DOI DOI 10.48550/ARXIV.2004.11362
   Krizhevsky A., 2012, Learning multiple layers of features from tiny images, DOI DOI 10.1145/3079856.3080246
   Liu XB, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3465220
   Liu ZW, 2019, PROC CVPR IEEE, P2532, DOI 10.1109/CVPR.2019.00264
   Mahajan D, 2018, LECT NOTES COMPUT SC, V11206, P185, DOI 10.1007/978-3-030-01216-8_12
   Menon AK, 2021, Arxiv, DOI arXiv:2007.07314
   More A, 2016, Arxiv, DOI arXiv:1608.06048
   Ren J., 2020, ADV NEURAL INFORM PR, V33, P4175, DOI DOI 10.48550/ARXIV.2007.10740
   ROUSSEEUW PJ, 1987, J COMPUT APPL MATH, V20, P53, DOI 10.1016/0377-0427(87)90125-7
   SHANNON CE, 1948, BELL SYST TECH J, V27, P379, DOI 10.1002/j.1538-7305.1948.tb01338.x
   Shu J, 2019, ADV NEUR IN, V32
   Tang K., 2020, P NIPS, P1513
   Tao Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P728, DOI 10.1007/978-3-030-58568-6_43
   Tong Wu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P162, DOI 10.1007/978-3-030-58548-8_10
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Wang JF, 2021, PROC CVPR IEEE, P3783, DOI 10.1109/CVPR46437.2021.00378
   Wang P, 2021, PROC CVPR IEEE, P943, DOI 10.1109/CVPR46437.2021.00100
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang YR, 2019, IEEE I CONF COMP VIS, P5016, DOI 10.1109/ICCV.2019.00512
   Wang Z, 2019, ACM T KNOWL DISCOV D, V13, DOI 10.1145/3314202
   Wu T, 2021, PROC CVPR IEEE, P8655, DOI 10.1109/CVPR46437.2021.00855
   Yang Y., 2020, Adv. Neural Inf. Process. Syst.
   Ye HJ, 2022, Arxiv, DOI arXiv:2001.01385
   Yin X, 2019, PROC CVPR IEEE, P5697, DOI 10.1109/CVPR.2019.00585
   Zang Y., 2021, P IEEECVF INT C COMP, P3457
   Zhang JJ, 2020, Arxiv, DOI arXiv:1912.04486
   Zhang SY, 2021, PROC CVPR IEEE, P2361, DOI 10.1109/CVPR46437.2021.00239
   Zhang Y, 2023, Arxiv, DOI [arXiv:2110.04596, DOI 10.48550/ARXIV.2110.04596]
   Zhong ZS, 2021, PROC CVPR IEEE, P16484, DOI 10.1109/CVPR46437.2021.01622
NR 48
TC 0
Z9 0
U1 20
U2 20
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2024
VL 20
IS 4
AR 108
DI 10.1145/3630256
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6G9
UT WOS:001153382100018
OA Bronze
DA 2024-08-05
ER

PT J
AU Huang, WL
   Chen, YQ
   Jiang, XL
   Zhang, T
   Chen, Q
AF Huang, Wuliang
   Chen, Yiqiang
   Jiang, Xinlong
   Zhang, Teng
   Chen, Qian
TI GJFusion: A Channel-Level Correlation Construction Method for Multimodal
   Physiological Signal Fusion
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Multimodal; physiological signal; graph neural network; emotion state
   recognition; ubiquitous computing
ID EMOTION RECOGNITION; EEG DATA
AB Physiological signal based ubiquitous computing has garnered significant attention. However, the heterogeneity among multimodal physiological signals poses a critical challenge to practical applications. To traverse this heterogeneity gap, recent studies have focused on establishing inter-modality correlations. Early works only consider coarse-level correlations between the embeddings of each modality. More recent graph-based approaches incorporate prior knowledge-based correlations, although they may not be entirely accurate. In this article, we propose the Graph Joint Fusion (GJFusion) network, which leverages channel-level inter-modality correlations based on a graph joint to mitigate the heterogeneous gap. Our proposed GJFusion first represents each modality as a graph, with each vertex corresponding to a signal channel, and the edges denoting their functional connectivity. We then join each modality by constructing inter-modality correlations for each salient channel using a sampling-based matching method. Discarded channels are transformed into a virtual vertex through a lightweight pooling operation. Subsequently, the fusion network integrates intra- and inter-modality features, enabling multimodal physiological signal fusion. To validate the effectiveness of our method, we select emotional state recognition as the downstream task and conduct comprehensive experiments on two benchmark datasets. The results demonstrate that our proposed GJFusion network surpasses the latest state-of-the-art methods, achieving relative accuracy improvements of 1.22% and 0.81% on the DEAP and MAHNOB-HCI datasets, respectively. Furthermore, visualization experiments of the salient brain regions reveal the presence of interpretable knowledge within the proposed GJFusion model.
C1 [Huang, Wuliang; Chen, Yiqiang; Jiang, Xinlong; Zhang, Teng; Chen, Qian] Chinese Acad Sci, Inst Comp Technol, 6 Kexueyuan South Rd Zhongguancun, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS
RP Chen, YQ; Jiang, XL (corresponding author), Chinese Acad Sci, Inst Comp Technol, 6 Kexueyuan South Rd Zhongguancun, Beijing 100190, Peoples R China.
EM huangwuliang19b@ict.ac.cn; yqchen@ict.ac.cn; jiangxinlong@ict.ac.cn;
   zhangteng19s@ict.ac.cn; chenqian20b@ict.ac.cn
RI Yin, Jing/KDO-6274-2024; li, li/KHE-5750-2024
OI Chen, Yiqiang/0000-0002-8407-0780; Zhang, Teng/0000-0003-1870-1051;
   Chen, Qian/0009-0002-2579-4380
FU National Key Research and Development Plan of China [2021YFC2501202];
   Beijing Municipal Science & Technology Commission [Z221100002722009];
   Hunan Provincial Natural Science Foundation of China [2023JJ70034];
   Youth Innovation Promotion Association CAS
FX This work was supported by the National Key Research and Development
   Plan of China (no. 2021YFC2501202), the Beijing Municipal Science &
   Technology Commission (no. Z221100002722009), the Hunan Provincial
   Natural Science Foundation of China (no. 2023JJ70034), and the Youth
   Innovation Promotion Association CAS.
CR [Anonymous], 2004, Affective Neuroscience: The Foundations of Human and Animal Emotions
   Ceballos R, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3152128
   Cheng J, 2021, IEEE J BIOMED HEALTH, V25, P453, DOI 10.1109/JBHI.2020.2995767
   Cui H, 2020, KNOWL-BASED SYST, V205, DOI 10.1016/j.knosys.2020.106243
   DAVIDSON RJ, 1982, SCIENCE, V218, P1235, DOI 10.1126/science.7146906
   Ding Y., 2022, IEEE Transactions on Affective Computing
   Gagliardi G, 2023, IEEE ACCESS, V11, P39544, DOI 10.1109/ACCESS.2023.3268233
   Goodkind M.S., 2011, Cognitive empathy following orbitofrontal cortex and dorsolateral prefrontal cortex damage
   Gou JP, 2023, IEEE T IND INFORM, V19, P7099, DOI 10.1109/TII.2022.3209672
   Hu JZ, 2021, NEUROCOMPUTING, V463, P177, DOI 10.1016/j.neucom.2021.08.018
   Jia ZY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1047, DOI 10.1145/3474085.3475583
   Jiang XL, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376374
   Jinxiang Liao, 2020, IOP Conference Series: Materials Science and Engineering, V782, DOI 10.1088/1757-899X/782/3/032005
   Kim JH, 2018, ADV NEUR IN, V31
   Kipf T.N., 2017, INT C LEARN REPR, P1
   Koelstra S, 2012, IEEE T AFFECT COMPUT, V3, P18, DOI 10.1109/T-AFFC.2011.15
   Li Jia Wen, 2022, IEEE Transactions on Cognitive and Developmental Systems, V15, P163
   Li QM, 2018, AAAI CONF ARTIF INTE, P3538
   Li XX, 2021, MED IMAGE ANAL, V74, DOI 10.1016/j.media.2021.102233
   LINDSLEY DB, 1950, FEELINGS EMOTIONS, P238
   Liu Chunxiao, 2020, CVPR, P10918, DOI DOI 10.1109/CVPR42600.2020.01093
   Liu W, 2016, LECT NOTES COMPUT SC, V9948, P521, DOI 10.1007/978-3-319-46672-9_58
   Ma JX, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P176, DOI 10.1145/3343031.3350871
   Maddison C. J., 2017, The concrete distribution: A continuous relaxation of discrete random variables, P1
   Messina N, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3451390
   Mittal T, 2020, AAAI CONF ARTIF INTE, V34, P1359
   Moctezuma LA, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-07517-5
   Nummenmaa L, 2014, P NATL ACAD SCI USA, V111, P646, DOI 10.1073/pnas.1321664111
   Padhmashree V, 2022, KNOWL-BASED SYST, V238, DOI 10.1016/j.knosys.2021.107867
   Piho L, 2020, IEEE T AFFECT COMPUT, V11, P722, DOI 10.1109/TAFFC.2018.2840973
   Priyasad D, 2022, KNOWL-BASED SYST, V250, DOI 10.1016/j.knosys.2022.109038
   RUSSELL JA, 1980, J PERS SOC PSYCHOL, V39, P1161, DOI 10.1037/h0077714
   Smith SM, 2011, NEUROIMAGE, V54, P875, DOI 10.1016/j.neuroimage.2010.08.063
   Soleymani M, 2012, IEEE T AFFECT COMPUT, V3, P42, DOI 10.1109/T-AFFC.2011.25
   Song TF, 2020, IEEE T AFFECT COMPUT, V11, P532, DOI 10.1109/TAFFC.2018.2817622
   Tang H, 2017, LECT NOTES COMPUT SC, V10637, P811, DOI 10.1007/978-3-319-70093-9_86
   Tao W, 2023, IEEE T AFFECT COMPUT, V14, P382, DOI 10.1109/TAFFC.2020.3025777
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang S, 2023, IEEE ACCESS, V11, P33061, DOI 10.1109/ACCESS.2023.3263670
   Wang XW, 2014, NEUROCOMPUTING, V129, P94, DOI 10.1016/j.neucom.2013.06.046
   Wu Yirui, 2023, IEEE/ACM Trans Comput Biol Bioinform, VPP, DOI 10.1109/TCBB.2023.3258455
   Xiang Li, 2021, 2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), P3642, DOI 10.1109/BIBM52615.2021.9669544
   Yang YL, 2018, IEEE IJCNN, P793
   Ye ZY, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, DOI 10.1145/3503161.3548258
   Yin YQ, 2021, APPL SOFT COMPUT, V100, DOI 10.1016/j.asoc.2020.106954
   Zhang JH, 2020, INFORM FUSION, V59, P103, DOI 10.1016/j.inffus.2020.01.011
   Zhang L, 2020, WEB CONFERENCE 2020: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2020), P3098, DOI 10.1145/3366423.3380083
   Zhang TY, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21010052
   Zhang T, 2022, IEEE T AFFECT COMPUT, V13, P379, DOI 10.1109/TAFFC.2019.2937768
   Zhang XW, 2022, IEEE T AFFECT COMPUT, V13, P958, DOI 10.1109/TAFFC.2020.2981440
   Zhang Y, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3542820
   Zhang Z, 2023, IEEE T AFFECT COMPUT, V14, P2048, DOI 10.1109/TAFFC.2022.3170369
   Zhao SC, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3233184
NR 53
TC 0
Z9 0
U1 30
U2 47
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2024
VL 20
IS 2
SI SI
AR 60
DI 10.1145/3617503
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W6GR4
UT WOS:001092595800030
OA hybrid
DA 2024-08-05
ER

PT J
AU Xiao, Y
   Liu, T
   Han, Y
   Liu, Y
   Wang, YT
AF Xiao, Yi
   Liu, Tong
   Han, Yu
   Liu, Yue
   Wang, Yongtian
TI Realtime Recognition of Dynamic Hand Gestures in Practical Applications
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Dynamic gesture recognition; activation delay; asymmetric gesture
   design; convolutional neural network; human-computer interaction
AB Dynamic hand gesture acting as a semaphoric gesture is a practical and intuitive mid-air gesture interface. Nowadays benefiting from the development of deep convolutional networks, the gesture recognition has already achieved a high accuracy, however, when performing a dynamic hand gesture such as gestures of direction commands, some unintentional actions are easily misrecognized due to the similarity of the hand poses. This hinders the application of dynamic hand gestures and cannot be solved by just improving the accuracy of the applied algorithm on public datasets, thus it is necessary to study such problems from the perspective of human-computer interaction. In this article, twomethods are proposed to avoid misrecognition by introducing activation delay and using asymmetric gesture design. First the temporal process of a dynamic hand gesture is decomposed and redefined, then a realtime dynamic hand gesture recognition system is built through a two-dimensional convolutional neural network. In order to investigate the influence of activation delay and asymmetric gesture design on system performance, a user study is conducted and experimental results show that the two proposedmethods can effectively avoid misrecognition. The two methods proposed in this article can provide valuable guidance for researchers when designing realtime recognition system in practical applications.
C1 [Xiao, Yi; Wang, Yongtian] Beijing Inst Technol, 5 South St, Beijing 100081, Peoples R China.
   [Liu, Tong; Han, Yu] Beijing Inst Technol, Beijing, Peoples R China.
   [Liu, Yue] Beijing Inst Technol, AICFVE Beijing Film Acad, 5 South St, Beijing, Peoples R China.
   [Liu, Yue] AICFVE Beijing Film Acad, 4 Xitucheng Rd, Beijing 100088, Peoples R China.
C3 Beijing Institute of Technology; Beijing Institute of Technology;
   Beijing Institute of Technology
RP Liu, Y (corresponding author), Beijing Inst Technol, AICFVE Beijing Film Acad, 5 South St, Beijing, Peoples R China.; Liu, Y (corresponding author), AICFVE Beijing Film Acad, 4 Xitucheng Rd, Beijing 100088, Peoples R China.
EM yixiao0202@gmail.com; lt_leonard@163.com; han.yu@outlook.com;
   liuyue@bit.edu.cn; wyt@bit.edu.cn
OI Wang, Yongtian/0000-0001-9422-0888
CR Bangor A, 2009, J USABILITY STUD, V4, P114
   Bozgeyikli E, 2021, 2021 IEEE VIRTUAL REALITY AND 3D USER INTERFACES (VR), P778, DOI 10.1109/VR50410.2021.00105
   Brooke J., 1996, SUS-a quick and dirty usability scale, DOI [DOI 10.1201/9781498710411-35, DOI 10.1201/9781498710411]
   Cabral Marcio C., 2005, P 2005 LATIN AM C HU, P100, DOI DOI 10.1145/1111360.1111370
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chan E, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P3403, DOI 10.1145/2858036.2858589
   Chang Chuo-Ling, 2020, P 2020 CVPR WORKSH C
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Huang S, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3152129
   Hutchinson MS, 2021, IEEE ACCESS, V9, P134611, DOI 10.1109/ACCESS.2021.3115476
   Izutov E., 2021, arXiv
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jiang BY, 2019, IEEE I CONF COMP VIS, P2000, DOI 10.1109/ICCV.2019.00209
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Köpüklü O, 2019, IEEE INT CONF AUTOMA, P407
   Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718
   Lv Z, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2645860
   Materzynska J, 2019, IEEE INT CONF COMP V, P2874, DOI 10.1109/ICCVW.2019.00349
   MORRELSAMUELS P, 1990, INT J MAN MACH STUD, V32, P581, DOI 10.1016/S0020-7373(05)80034-3
   Nielsen M, 2003, LECT NOTES ARTIF INT, V2915, P409
   Pavlovic VI, 1997, IEEE T PATTERN ANAL, V19, P677, DOI 10.1109/34.598226
   Qian C, 2014, PROC CVPR IEEE, P1106, DOI 10.1109/CVPR.2014.145
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Quader Niamul, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P35, DOI 10.1007/978-3-030-58577-8_3
   Quek F., 2002, ACM Transactions on Computer-Human Interaction, V9, P171, DOI 10.1145/568513.568514
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Sharma Adwait, 2019, Grasping Microgestures: Eliciting SingleHand Microgestures for Handheld Objects, P1, DOI [10.1145/3290605.3300632, DOI 10.1145/3290605.3300632]
   Simonyan K, 2014, ADV NEUR IN, V27
   Vatavu Radu-Daniel, 2021, UIST '21: The 34th Annual ACM Symposium on User Interface Software and Technology, P710, DOI 10.1145/3472749.3474780
   Wang LM, 2019, IEEE T PATTERN ANAL, V41, P2740, DOI 10.1109/TPAMI.2018.2868668
   Wang RY, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531369
   Wobbrock JO, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P1083
   Xiao YQ, 2020, UNIVERSAL ACCESS INF, V19, P433, DOI 10.1007/s10209-019-00647-0
   Zhang C, 2020, Arxiv, DOI arXiv:2008.03462
   Zhang YF, 2018, IEEE T MULTIMEDIA, V20, P1038, DOI 10.1109/TMM.2018.2808769
   Zhao L, 2020, 2020 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES (VR 2020), P239, DOI [10.1109/VR46266.2020.1581066900344, 10.1109/VR46266.2020.00-61]
   Zhou BL, 2018, LECT NOTES COMPUT SC, V11205, P831, DOI 10.1007/978-3-030-01246-5_49
NR 38
TC 1
Z9 1
U1 18
U2 30
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2024
VL 20
IS 2
SI SI
AR 50
DI 10.1145/3561822
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W6GR4
UT WOS:001092595800020
DA 2024-08-05
ER

PT J
AU Zhang, ZC
   Chen, ZD
   Xie, ZY
   Luo, X
   Xu, XS
AF Zhang, Zi-Chao
   Chen, Zhen-Duo
   Xie, Zhen-Yu
   Luo, Xin
   Xu, Xin-Shun
TI S3Mix: Same Category Same Semantics Mixing for Augmenting Fine-grained
   Images
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Data augmentation; fine-grained images; homology loss; Vision
   Transformer
AB Data augmentation is a common technique to improve the generalization performance of models for image classification. Although methods such as Mixup and CutMix that mix images randomly are indeed instrumental in general image classification, randomly swapping or masking regions is not friendly to fine-grained images, since the key to fine-grained image classification precisely lies in discriminative and informative regions, and it is unreasonable to generate labels solely consistent with the proportion of synthesis. Some erasing methods like Cutout even endanger fine-grained image classification because of erasing the discriminative regions by chance. In this article, we propose the Same Category Same Semantics Mixing method (S3Mix) corresponding to the characteristics of fine-grained images. Specifically, we limit the mixture to regions of the same category and semantics. The core of the method is two constraints. The exchange with the semantic region ensures the discrimination and semantics integrity of the generated image, and the exchange in the same class avoids the problem of unreasonable label generation. At the same time, we propose a homology loss to promote the semantic relationship between the generated positive image pairs. Experiments have been conducted on four fine-grained datasets, and the results show the proposed method is superior to the traditional image augmentation methods as well as some fine-grained data augmentation methods.
C1 [Zhang, Zi-Chao; Chen, Zhen-Duo; Xie, Zhen-Yu; Luo, Xin; Xu, Xin-Shun] Shandong Univ, Sch Software, Jinan 250101, Peoples R China.
C3 Shandong University
RP Chen, ZD (corresponding author), Shandong Univ, Sch Software, Jinan 250101, Peoples R China.
EM chenzd.sdu@gmail.com; xiezhenyu47@163.com; luoxin.lxin@gmail.com;
   xuxinshun@sdu.edu.cn
RI Xie, Zhenyu/AAA-5933-2021; Luo, Xin/HNR-3191-2023
OI Xie, Zhenyu/0000-0003-1704-4379; Luo, Xin/0000-0002-6901-5476; Zhang,
   Zi-Chao/0000-0003-1365-4401; Chen, Zhen-Duo/0000-0002-3481-4892
FU National Natural Science Foundation of China [62172256, 62202278,
   62202272]; Natural Science Foundation of Shandong Province [ZR2019ZD06,
   ZR2020QF036, ZR2021ZD15]; Young Scholars Program of Shandong University;
   Major Program of the National Natural Science Foundation of China
   [61991411]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62172256, 62202278, 62202272, in part by
   Natural Science Foundation of Shandong Province under Grant ZR2019ZD06,
   ZR2020QF036, ZR2021ZD15, in part by the Young Scholars Program of
   Shandong University, and in part by the Major Program of the National
   Natural Science Foundation of China under Grant 61991411.
CR Abnar S, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P4190
   Berg T, 2014, PROC CVPR IEEE, P2019, DOI 10.1109/CVPR.2014.259
   Choi J, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21248444
   Chou P.-Y., 2022, arXiv, DOI DOI 10.48550/ARXIV.2202.03822
   DeVries T, 2017, Arxiv, DOI arXiv:1708.04552
   Diao Q., 2022, arXiv, DOI 10.48550/arXiv.2203.02751
   Dosovitskiy A., 2021, PROC INT C LEARNING, DOI DOI 10.48550/ARXIV.2010.11929
   Gao Y, 2016, PROC CVPR IEEE, P317, DOI 10.1109/CVPR.2016.41
   Hammoudi Karim, 2022, arXiv
   He J, 2022, AAAI CONF ARTIF INTE, P852
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He XT, 2020, IEEE T CIRC SYST VID, V30, P520, DOI 10.1109/TCSVT.2019.2892802
   He XT, 2019, INT J COMPUT VISION, V127, P1235, DOI 10.1007/s11263-019-01176-2
   He XT, 2017, PROC CVPR IEEE, P7332, DOI 10.1109/CVPR.2017.775
   Hu YQ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4239, DOI 10.1145/3474085.3475561
   Huang SL, 2021, AAAI CONF ARTIF INTE, V35, P1628
   Huang SL, 2016, PROC CVPR IEEE, P1173, DOI 10.1109/CVPR.2016.132
   Khosla A., 2011, P IEEE C COMP VIS PA, V2
   Kong S, 2017, PROC CVPR IEEE, P7025, DOI 10.1109/CVPR.2017.743
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Lam M, 2017, PROC CVPR IEEE, P6497, DOI 10.1109/CVPR.2017.688
   Lin D, 2015, PROC CVPR IEEE, P1666, DOI 10.1109/CVPR.2015.7298775
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Liu XD, 2022, NEUROCOMPUTING, V492, P137, DOI 10.1016/j.neucom.2022.04.037
   Maji S, 2013, Arxiv, DOI arXiv:1306.5151
   Min SB, 2020, IEEE T IMAGE PROCESS, V29, P4996, DOI 10.1109/TIP.2020.2977457
   Neubeck A, 2006, INT C PATT RECOG, P850, DOI 10.1109/icpr.2006.479
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Song KT, 2020, IEEE T IMAGE PROCESS, V29, P7006, DOI 10.1109/TIP.2020.2996736
   Sun HF, 2022, IEEE T CIRC SYST VID, V32, P7177, DOI 10.1109/TCSVT.2022.3171972
   Sun HB, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P5853, DOI 10.1145/3503161.3548308
   Sun M, 2018, LECT NOTES COMPUT SC, V11220, P834, DOI 10.1007/978-3-030-01270-0_49
   Sun ZR, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10582, DOI 10.1109/ICCV48922.2021.01043
   Van Horn G, 2015, PROC CVPR IEEE, P595, DOI 10.1109/CVPR.2015.7298658
   Wah C., 2011, Tech. Rep. CNS-TR-2011-001
   Wang JF, 2021, 2021 3RD ASIA ENERGY AND ELECTRICAL ENGINEERING SYMPOSIUM (AEEES 2021), P170, DOI 10.1109/AEEES51875.2021.9403006
   Wei X, 2018, LECT NOTES COMPUT SC, V11207, P365, DOI 10.1007/978-3-030-01219-9_22
   Wei Yang, 2022, IEEE INT C MULT EXP, P1
   Xu HP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1043
   Xu JQ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5609, DOI 10.1145/3474085.3475705
   Yang Z, 2018, LECT NOTES COMPUT SC, V11218, P438, DOI 10.1007/978-3-030-01264-9_26
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Zhang CY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2372, DOI 10.1145/3394171.3414044
   Zhang CY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4063, DOI 10.1145/3474085.3475536
   Zhang H., 2018, INT C LEARNING REPRE
   Zhang LB, 2021, IEEE WINT CONF APPL, P3208, DOI 10.1109/WACV48630.2021.00325
   Zhang LB, 2019, IEEE I CONF COMP VIS, P8330, DOI 10.1109/ICCV.2019.00842
   Zheng HL, 2019, PROC CVPR IEEE, P5007, DOI 10.1109/CVPR.2019.00515
   Zheng HL, 2020, IEEE T IMAGE PROCESS, V29, P476, DOI 10.1109/TIP.2019.2921876
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zou DN, 2020, COMPUT VIS MEDIA, V6, P477, DOI 10.1007/s41095-020-0184-6
NR 51
TC 0
Z9 0
U1 7
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2024
VL 20
IS 1
AR 9
DI 10.1145/3605892
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T8LL3
UT WOS:001080441800009
DA 2024-08-05
ER

PT J
AU Li, F
   Wu, YX
   Li, AQ
   Bai, HH
   Cong, RM
   Zhao, Y
AF Li, Feng
   Wu, Yixuan
   Li, Anqi
   Bai, Huihui
   Cong, Runmin
   Zhao, Yao
TI Enhanced Video Super-Resolution Network towards Compressed Data
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Compressed video super-resolution; video quality enhancement;
   motion-excited temporal adaption; multi-frame SR network
ID CONVOLUTIONAL NETWORK
AB Video super-resolution (VSR) algorithms aim at recovering a temporally consistent high-resolution (HR) video from its corresponding low-resolution (LR) video sequence. Due to the limited bandwidth during video transmission, most available videos on the internet are compressed. Nevertheless, few existing algorithms consider the compression factor in practical applications. In this paper, we propose an enhanced VSR model towards compressed videos, termed as ECVSR, to simultaneously achieve compression artifacts reduction and SR reconstruction end-to-end. ECVSR contains a motion-excited temporal adaption network (METAN) and a multiframe SR network (SRNet). The METAN takes decoded LR video frames as input and models inter-frame correlations via bidirectional deformable alignment and motion-excited temporal adaption, where temporal differences are calculated as motion prior to excite the motion-sensitive regions of temporal features. In SRNet, cascaded recurrent multi-scale blocks (RMSB) are employed to learn deep spatio-temporal representations from adapted multi-frame features. Then, we build a reconstruction module for spatio-temporal information integration and HR frame reconstruction, which is followed by a detail refinement module for texture and visual quality enhancement. Extensive experimental results on compressed videos demonstrate the superiority of our method for compressed VSR. Code will be available at https://github.com/lifengcs/ECVSR.
C1 [Li, Feng] Hefei Univ Technol, 483 Danxia Rd, Hefei 230601, Anhui, Peoples R China.
   [Wu, Yixuan; Li, Anqi; Bai, Huihui; Zhao, Yao] Beijing Jiaotong Univ, 3 Shangyuancun, Beijing 100044, Peoples R China.
   [Cong, Runmin] Shandong Univ, 17923 Jingshi Rd, Jinan 250002, Shandong, Peoples R China.
C3 Hefei University of Technology; Beijing Jiaotong University; Shandong
   University
RP Bai, HH (corresponding author), Beijing Jiaotong Univ, 3 Shangyuancun, Beijing 100044, Peoples R China.
EM fengli@hfut.edu.cn; wuyixuan@bjtu.edu.cn; lianqi@bjtu.edu.cn;
   hhbai@bjtu.edu.cn; rmcong@sdu.edu.cn; yzhao@bjtu.edu.cn
OI Bai, Huihui/0000-0002-3879-8957; Li, Feng/0000-0001-9862-0432; Zhao,
   Yao/0000-0002-8581-9554
FU National Natural Science Foundation of China [62302141, 62331003,
   62120106009]; Beijing Natural Science Foundation [L223022]; Taishan
   Scholar Project of Shandong Province [tsqn202306079]
FX This work was supported in part by the National Natural Science
   Foundation of China (No. 62302141, 62331003, 62120106009), the Beijing
   Natural Science Foundation (L223022), the Taishan Scholar Project of
   Shandong Province under Grant tsqn202306079.
CR Aly HA, 2005, IEEE T IMAGE PROCESS, V14, P1647, DOI 10.1109/TIP.2005.851684
   [Anonymous], 2018, HM HEVC Reference Software
   Bossen F., 2013, JCTVCL1100
   Caballero J, 2017, PROC CVPR IEEE, P2848, DOI 10.1109/CVPR.2017.304
   Chen PL, 2021, IEEE T IMAGE PROCESS, V30, P7156, DOI 10.1109/TIP.2021.3101826
   Chu MY, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392457
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Demirel H, 2011, IEEE T GEOSCI REMOTE, V49, P1997, DOI 10.1109/TGRS.2010.2100401
   Deng JN, 2020, AAAI CONF ARTIF INTE, V34, P10696
   Ding Q, 2021, IEEE T IMAGE PROCESS, V30, P6459, DOI 10.1109/TIP.2021.3092949
   Dong C, 2015, IEEE I CONF COMP VIS, P576, DOI 10.1109/ICCV.2015.73
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Fang CW, 2020, IEEE T IMAGE PROCESS, V29, P3078, DOI 10.1109/TIP.2019.2955640
   Farsiu S, 2004, IEEE T IMAGE PROCESS, V13, P1327, DOI 10.1109/TIP.2004.834669
   Guan ZY, 2021, IEEE T PATTERN ANAL, V43, P949, DOI 10.1109/TPAMI.2019.2944806
   Haris M, 2019, PROC CVPR IEEE, P3892, DOI 10.1109/CVPR.2019.00402
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang Y, 2018, IEEE T PATTERN ANAL, V40, P1015, DOI 10.1109/TPAMI.2017.2701380
   Huang ZJ, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3569583
   Isobe Takashi, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P645, DOI 10.1007/978-3-030-58610-2_38
   Isobe Takashi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8005, DOI 10.1109/CVPR42600.2020.00803
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jo Y, 2018, PROC CVPR IEEE, P3224, DOI 10.1109/CVPR.2018.00340
   Kappeler A, 2016, IEEE T COMPUT IMAG, V2, P109, DOI 10.1109/TCI.2016.2532323
   Khani M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4501, DOI 10.1109/ICCV48922.2021.00448
   Kim J, 2016, PROC CVPR IEEE, P1646, DOI 10.1109/CVPR.2016.182
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kingma D. P., 2014, arXiv
   Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618
   Li F, 2023, IEEE T MULTIMEDIA, V25, P2825, DOI 10.1109/TMM.2022.3152090
   Li F, 2020, IEEE T IMAGE PROCESS, V29, P4474, DOI 10.1109/TIP.2020.2972118
   Li F, 2020, IEEE T CIRC SYST VID, V30, P1511, DOI 10.1109/TCSVT.2019.2906428
   Li S, 2019, PROC CVPR IEEE, P10514, DOI 10.1109/CVPR.2019.01077
   Li W., 2021, P EUR C COMP VIS, P335
   Li YC, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3414838
   Li YX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2523, DOI 10.1109/ICCV48922.2021.00254
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu C, 2014, IEEE T PATTERN ANAL, V36, P346, DOI 10.1109/TPAMI.2013.127
   Liu YQ, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3532864
   Lucas A, 2019, IEEE T IMAGE PROCESS, V28, P3312, DOI 10.1109/TIP.2019.2895768
   Luo JP, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1882, DOI 10.1145/3394171.3413587
   Ma ZY, 2015, PROC CVPR IEEE, P5224, DOI 10.1109/CVPR.2015.7299159
   Noh J, 2019, IEEE I CONF COMP VIS, P9724, DOI 10.1109/ICCV.2019.00982
   Ohm JR, 2012, IEEE T CIRC SYST VID, V22, P1669, DOI 10.1109/TCSVT.2012.2221192
   Protter M, 2009, IEEE T IMAGE PROCESS, V18, P36, DOI 10.1109/TIP.2008.2008067
   Sajjadi MSM, 2018, PROC CVPR IEEE, P6626, DOI 10.1109/CVPR.2018.00693
   Shen MM, 2011, IEEE T CIRC SYST VID, V21, P755, DOI 10.1109/TCSVT.2011.2130390
   Shi XJ, 2015, ADV NEUR IN, V28
   Song HH, 2021, IEEE T IMAGE PROCESS, V30, P2923, DOI 10.1109/TIP.2021.3056868
   Sun J, 2011, IEEE T IMAGE PROCESS, V20, P1529, DOI 10.1109/TIP.2010.2095871
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Tai YW, 2010, PROC CVPR IEEE, P2400, DOI 10.1109/CVPR.2010.5539933
   Takeda H, 2009, IEEE T IMAGE PROCESS, V18, P1958, DOI 10.1109/TIP.2009.2023703
   Tao X, 2017, IEEE I CONF COMP VIS, P4482, DOI 10.1109/ICCV.2017.479
   Tian YP, 2020, PROC CVPR IEEE, P3357, DOI 10.1109/CVPR42600.2020.00342
   Wang L, 2020, PROC CVPR IEEE, P3773, DOI 10.1109/CVPR42600.2020.00383
   Wang LG, 2020, IEEE T IMAGE PROCESS, V29, P4323, DOI 10.1109/TIP.2020.2967596
   Wang M, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3589963
   Wang TT, 2017, IEEE DATA COMPR CONF, P410, DOI 10.1109/DCC.2017.42
   Wang XT, 2019, IEEE COMPUT SOC CONF, P1954, DOI 10.1109/CVPRW.2019.00247
   Wang ZY, 2016, PROC CVPR IEEE, P2764, DOI 10.1109/CVPR.2016.302
   Wang ZX, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3569900
   Wu Y., 2022, IEEE Trans. Multimedia, DOI [10.1109/TMM.2022.3216115, DOI 10.1109/TMM.2022.3216115]
   Xie SN, 2018, Arxiv, DOI [arXiv:1712.04851, DOI 10.48550/ARXIV.1712.04851]
   Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2
   Yang R, 2018, PROC CVPR IEEE, P6664, DOI 10.1109/CVPR.2018.00697
   Yi P, 2019, IEEE I CONF COMP VIS, P3106, DOI 10.1109/ICCV.2019.00320
   Yi P, 2020, IEEE T CIRC SYST VID, V30, P2503, DOI 10.1109/TCSVT.2019.2925844
   Zhang DY, 2021, IEEE T CIRC SYST VID, V31, P3954, DOI 10.1109/TCSVT.2020.3044451
   Zhang DY, 2024, IEEE T CIRC SYST VID, V34, P2934, DOI 10.1109/TCSVT.2023.3307438
   Zhang HC, 2019, IEEE I CONF COMP VIS, P8798, DOI 10.1109/ICCV.2019.00889
   Zhang LP, 2010, SIGNAL PROCESS, V90, P848, DOI 10.1016/j.sigpro.2009.09.002
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
NR 74
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2024
VL 20
IS 7
SI SI
AR 202
DI 10.1145/3651309
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL0Q6
UT WOS:001234494100017
DA 2024-08-05
ER

PT J
AU Ren, YJ
   Lv, ZY
   Xiong, NN
   Wang, J
AF Ren, Yongjun
   Lv, Zhiying
   Xiong, Neal N.
   Wang, Jin
TI HCNCT: A Cross-chain Interaction Scheme for the Blockchain-based
   Metaverse
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Metaverse; blockchain; cross-chain; blockchain interoperability; Hash
   timelock contract; notary mechanism
ID STORAGE MECHANISM
AB As a new type of digital living space that blends virtual and reality, Metaverse combines many emerging technologies. It provides an immersive experience based on VR technology and stores and protects users' digital content and digital assets through blockchain technology. However, different virtual environments are often highly heterogeneous in terms of underlying architecture and software implementation technology, which leads to many challenges in scalability and interoperability for blockchains serving the Metaverse. Cross-chain technology is an essential technology to realize the scalability and interoperability of blockchain. However, the current cross-chain technologies all have their own merits and demerits, and there is no cross-chain solution that can be fully applied to any scenario. To this end, in the blockchain-based Metaverse, this article proposes a cross-chain transaction scheme based on improved hash timelock, HCNCT. By combining the notary mechanism, this scheme uses a group of notaries to supervise and participate in cross-chain trans-actions, effectively solving the problem that malicious users create a large number of time-out transactions to block the transaction channel, which exists in the traditional hash timelock method. Besides, this article uses the verifiable secret sharing method in the notary group, which can effectively prevent the centralization problem of the notary mechanism. Moreover, this article discusses the process of key processing, cross-chain transaction and transaction verification of the scheme, and designs the user credibility evaluation mechanism, which can effectively reduce the occurrence of malicious default of users. Compared with existing solutions, our solution has the advantage of effectively addressing time-out transaction attacks and centralization issues while guaranteeing security. The experiments also verify the effectiveness of the proposed scheme.
C1 [Ren, Yongjun; Lv, Zhiying] Nanjing Univ Informat Sci & Technol, Sch Comp, Engn Res Ctr Digital Forens, Minist Educ, 219Ningliu Rd, Nanjing 210044, Jiangsu, Peoples R China.
   [Xiong, Neal N.] Sul Ross State Univ, Dept Comp Sci & Math, Alpine, TX 79830 USA.
   [Wang, Jin] Changsha Univ Sci & Technol, Sch Comp & Commun Engn, 960,2nd Sec,Wanjiali RD, Changsha 410004, Hunan, Peoples R China.
C3 Nanjing University of Information Science & Technology; Texas State
   University System; Changsha University of Science & Technology
RP Wang, J (corresponding author), Changsha Univ Sci & Technol, Sch Comp & Commun Engn, 960,2nd Sec,Wanjiali RD, Changsha 410004, Hunan, Peoples R China.
EM renyj100@126.com; nslvzy@foxmail.com; xiongnaixue@gmail.com;
   jinwang@csust.edu.cn
RI xiong, naixue/M-4277-2019; /AAI-7009-2020
OI xiong, naixue/0000-0002-0394-4635; /0000-0001-5473-8738
FU National Natural Science Foundation of China [62072249, 62072056];
   National Science Foundation of Hunan Province [2020JJ2029]; Hunan
   Provincial Key Research and Development Program [2022GK2019]
FX This work was supported by National Natural Science Foundation of China
   (Grants No. 62072249 and No. 62072056), the National Science Foundation
   of Hunan Province (Grant No. 2020JJ2029), and by Hunan Provincial Key
   Research and Development Program (Grant No. 2022GK2019).
CR Belchior R, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3471140
   Chandramouli A, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3512344
   Dai B., 2020, P INT C BLOCKCH TRUS, P218
   Deng LP, 2018, LECT NOTES COMPUT SC, V10973, P144, DOI 10.1007/978-3-319-94340-4_12
   Duan HH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P153, DOI 10.1145/3474085.3479238
   Fang WW, 2018, IEEE T SYST MAN CY-S, V48, P522, DOI 10.1109/TSMC.2016.2606400
   Faraboschi P, 2022, COMPUTER, V55, P100, DOI 10.1109/MC.2022.3192702
   Ge CP, 2021, IEEE T DEPEND SECURE, V18, P2787, DOI 10.1109/TDSC.2020.2963978
   Ge CP, 2021, IEEE T DEPEND SECURE, V18, P1214, DOI 10.1109/TDSC.2019.2899300
   Han PC, 2023, J INF SECUR APPL, V74, DOI 10.1016/j.jisa.2023.103446
   Herlihy M, 2019, Arxiv, DOI arXiv:1905.09743
   Herlihy M, 2018, PODC'18: PROCEEDINGS OF THE 2018 ACM SYMPOSIUM ON PRINCIPLES OF DISTRIBUTED COMPUTING, P245, DOI 10.1145/3212734.3212736
   Huynh-The T, 2022, Arxiv, DOI [arXiv:2202.10336, DOI 10.48550/ARXIV.2202.10336]
   Joseph Poon, 2017, Plasma: Scalable Autonomous Smart Contracts
   Lee L H., 2021, PREPRINT
   Liang HR, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3415151
   Lin JH, 2022, CHAOS SOLITON FRACT, V164, DOI 10.1016/j.chaos.2022.112620
   [路爱同 Lu Aitong], 2019, [信息网络安全, Netinfo Security], P83
   Mitra Samrat, 2021, Proceedings of International Conference on Frontiers in Computing and Systems. COMSYS 2020. Advances in Intelligent Systems and Computing (AISC 1255), P625, DOI 10.1007/978-981-15-7834-2_58
   Mohanty SK, 2021, COMPUT SECUR, V106, DOI 10.1016/j.cose.2021.102291
   Ning Huansheng, 2021, arXiv, DOI [DOI 10.48550/ARXIV.2111.09673, 10.48550/arXiv.2111.09673]
   Palletone, 2018, Protocol for Abstract-level Ledger Ecosystem
   Gadekallu TR, 2022, Arxiv, DOI [arXiv:2203.09738, DOI 10.48550/ARXIV.2203.09738]
   Ren YJ, 2022, IEEE T INTELL TRANSP, V23, P1639, DOI 10.1109/TITS.2021.3100103
   Ren YJ, 2021, FUTURE GENER COMP SY, V115, P304, DOI 10.1016/j.future.2020.09.019
   Ren YJ, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20010207
   Ren YJ, 2019, MATH BIOSCI ENG, V16, P1874, DOI 10.3934/mbe.2019091
   Rohrer Elias, 2020, AFT '20: Proceedings of the 2nd ACM Conference on Advances in Financial Technologies, P214, DOI 10.1145/3419614.3423262
   Shi LC, 2020, CHINESE J ELECTRON, V29, P887, DOI 10.1049/cje.2020.08.004
   Thomas S, 2015, A protocol for interledger payments
   Tsabary Itay, 2021, 2021 IEEE Symposium on Security and Privacy (SP), P1230, DOI 10.1109/SP40001.2021.00080
   Wang J, 2022, COMPUT IND ENG, V164, DOI 10.1016/j.cie.2021.107903
   Wang J, 2020, J INTERNET TECHNOL, V21, P1681, DOI 10.3966/160792642020112106010
   Wang J, 2020, CMC-COMPUT MATER CON, V65, P2365, DOI 10.32604/cmc.2020.011567
   Wang J, 2020, J INTERNET TECHNOL, V21, P393, DOI 10.3966/160792642020032102008
   Wang J, 2020, CMC-COMPUT MATER CON, V62, P695, DOI 10.32604/cmc.2020.08674
   Wang J, 2019, INT J DISTRIB SENS N, V15, DOI 10.1177/1550147719839581
   Wang J, 2019, CMC-COMPUT MATER CON, V58, P711, DOI 10.32604/cmc.2019.05450
   Wang J, 2018, WIREL COMMUN MOB COM, DOI 10.1155/2018/9472075
   Wang J, 2018, CMC-COMPUT MATER CON, V56, P433, DOI 10.3970/cmc.2018.04132
   Wu M, 2015, SENSORS-BASEL, V15, P248, DOI 10.3390/s150100248
   Wu Q, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22197524
   Wu Q, 2023, COMPUT SYST SCI ENG, V44, P1633, DOI 10.32604/csse.2023.030646
   Xiong HL, 2020, IEEE ACCESS, V8, P108148, DOI 10.1109/ACCESS.2020.3001007
   Xiong NX, 2012, INT PARALL DISTRIB P, P668, DOI 10.1109/IPDPS.2012.126
   Yang QL, 2022, Arxiv, DOI arXiv:2201.03201
   Yanzhen Qu, 2012, 2012 41st International Conference on Parallel Processing (ICPP 2012), P520, DOI 10.1109/ICPP.2012.3
   Yu XF, 2023, FUTURE GENER COMP SY, V143, P392, DOI 10.1016/j.future.2023.02.011
   Zhang Q, 2018, IEEE T IND INFORM, V14, P2497, DOI 10.1109/TII.2017.2768998
NR 49
TC 6
Z9 6
U1 3
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2024
VL 20
IS 7
SI SI
AR 188
DI 10.1145/3594542
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL0Q6
UT WOS:001234494100003
DA 2024-08-05
ER

PT J
AU Huang, QB
   Li, PJ
   Huang, YJ
   Shuang, F
   Cai, Y
AF Huang, Qingbao
   Li, Pijian
   Huang, Youji
   Shuang, Feng
   Cai, Yi
TI Region-Focused Network for Dense Captioning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Dense captioning; interaction relation; region-focus; transformer
AB Dense captioning is a very critical but under-explored task, which aims to densely detect localized regions-of-interest (RoIs) and describe them with natural language in a given image. Although recent studies tried to fuse multi-scale features from different visual instances to generate more accurate descriptions, their methods still suffer from the lack of exploration of relation semantic information in images, leading to less informative descriptions. Furthermore, indiscriminately fusing all visual instance features will introduce redundant information, resulting in poor matching between descriptions and corresponding regions. In thiswork, we propose a Region-Focused Network (RFN) to address these issues. Specifically, to fully comprehend the images, we first extract the object-level features, and encode the interaction and position relations between objects to enhance the object representations. Then, to decrease the interference fromredundant information about the target region, we extract the most relevant information to the region. Finally, a region-based Transformer is employed to compose and align the previous mined information and generate the corresponding descriptions. Extensive experiments on Visual Genome V1.0 and V1.2 datasets showthat our RFNmodel outperforms the state-of-the-art methods, thus verifying its effectiveness. Our code is available at https://github.com/VILAN-Lab/DesCap.
C1 [Huang, Qingbao; Li, Pijian; Huang, Youji; Shuang, Feng] Guangxi Univ, Nanning 530004, Peoples R China.
   [Shuang, Feng] Guangxi Key Lab Intelligent Control & Maintenance, Nanning 530004, Peoples R China.
   [Cai, Yi] South China Univ Technol, Guangzhou 510006, Peoples R China.
   [Cai, Yi] Minist Educ, Key Lab Big Dat & Intelligent Robot SCUT, Guangzhou 510006, Peoples R China.
C3 Guangxi University; South China University of Technology
RP Huang, QB (corresponding author), Guangxi Univ, Nanning 530004, Peoples R China.
EM qbhuang@gxu.edu.cn; lscissan@gmail.com; yjhuang@st.gxu.edu.cn;
   fshuang@gxu.edu.cn; ycai@scut.edu.cn
OI Li, Pijian/0009-0005-1924-5248; Huang, Qingbao/0000-0001-7691-347X
FU National Natural Science Foundation of China [62076100]; Guangxi Natural
   Science Foundation [2022GXNSFAA035627]; Guangxi Scientific and
   Technological Bases and Talents Special Projects [guikeAD23026230,
   guikeAD23026213]; Bagui Scholar Program of Guangxi; Fundamental Research
   Funds for the Central Universities, SCUT [D2230080]; Innovation Project
   of Guangxi Graduate Education [JGY2023016]; Technology Planning Project
   of Guangdong Province [2020B0101100002]
FX This work was supported by National Natural Science Foundation of China
   (62276072), the Guangxi Natural Science Foundation (No.
   2022GXNSFAA035627), Guangxi Scientific and Technological Bases and
   Talents Special Projects (guikeAD23026230 and guikeAD23026213), the
   Bagui Scholar Program of Guangxi, and partly by National Natural Science
   Foundation of China (62076100), the Fundamental Research Funds for the
   Central Universities, SCUT (D2230080), Innovation Project of Guangxi
   Graduate Education (JGY2023016) and Technology Planning Project of
   Guangdong Province (2020B0101100002).
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen JW, 2022, IEEE WINT CONF APPL, P786, DOI 10.1109/WACV51458.2022.00086
   Cornia Marcella, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10575, DOI 10.1109/CVPR42600.2020.01059
   Dosovitskiy A., 2022, INT C LEARN REPR
   Duan HD, 2022, PROC CVPR IEEE, P2959, DOI 10.1109/CVPR52688.2022.00298
   Gao AQ, 2024, IEEE T CIRC SYST VID, V34, P2000, DOI 10.1109/TCSVT.2022.3202810
   Gordon J., 2013, P 2013 WORKSH AUT KN, P25, DOI [DOI 10.1145/2509558.2509563, 10.1145/2509558.2509563?casa_token=mE3LH0NgZXYAAAAA:jV9pdGKqpOSLdftVM3UudHk0sa9nhH_xUspKq9oeBYEnQ9FK-yDUCenVi9ofiqGHqSL0eNnqVIgKvA]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Herdade S, 2019, ADV NEUR IN, V32
   Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378
   Huang L, 2019, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2019.00473
   Huang Q., 2020, P 58 ANN M ASS COMP, P7166
   Huang QB, 2022, IEEE T MULTIMEDIA, V24, P2004, DOI 10.1109/TMM.2021.3074803
   Ji Z, 2022, IMAGE VISION COMPUT, V128, DOI 10.1016/j.imavis.2022.104585
   Jiang WT, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3460474
   Jiang WH, 2018, AAAI CONF ARTIF INTE, P6959
   Jiang XZ, 2020, AAAI CONF ARTIF INTE, V34, P11125
   Johnson J, 2016, PROC CVPR IEEE, P4565, DOI 10.1109/CVPR.2016.494
   Karpathy A, 2017, IEEE T PATTERN ANAL, V39, P664, DOI 10.1109/TPAMI.2016.2598339
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Li G, 2019, IEEE I CONF COMP VIS, P8927, DOI 10.1109/ICCV.2019.00902
   Li LJ, 2019, IEEE I CONF COMP VIS, P10312, DOI 10.1109/ICCV.2019.01041
   Li XY, 2019, AAAI CONF ARTIF INTE, P8650
   Li YK, 2018, LECT NOTES COMPUT SC, V11205, P346, DOI 10.1007/978-3-030-01246-5_21
   Liao ZM, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5074, DOI 10.1145/3474085.3475712
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu AA, 2022, IEEE T CIRC SYST VID, V32, P3685, DOI 10.1109/TCSVT.2021.3107035
   Liu AA, 2021, PATTERN RECOGN LETT, V145, P187, DOI 10.1016/j.patrec.2021.01.024
   Liu AA, 2019, MULTIMED TOOLS APPL, V78, P677, DOI 10.1007/s11042-017-5532-x
   Liu XX, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3409388
   Liu YB, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3498340
   Longteng Guo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10324, DOI 10.1109/CVPR42600.2020.01034
   Luo YP, 2021, AAAI CONF ARTIF INTE, V35, P2286
   Mazzia V, 2022, PATTERN RECOGN, V124, DOI 10.1016/j.patcog.2021.108487
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shao Z, 2023, IEEE T MULTIMEDIA, V25, P8753, DOI 10.1109/TMM.2023.3241517
   Shao Zhuang, 2022, IEEE Trans Neural Netw Learn Syst, VPP, DOI 10.1109/TNNLS.2022.3152990
   Song ZL, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5056, DOI 10.1145/3474085.3475607
   Vaswani A, 2017, ADV NEUR IN, V30
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang YH, 2022, IEEE T CIRC SYST VID, V32, P4417, DOI 10.1109/TCSVT.2021.3121062
   Wang ZW, 2020, Arxiv, DOI arXiv:2006.08322
   Wei HY, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3439734
   Wei XR, 2017, 2017 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)
   Wu JL, 2022, Arxiv, DOI arXiv:2212.00280
   Xian TT, 2022, NEURAL NETWORKS, V148, P129, DOI 10.1016/j.neunet.2022.01.011
   Xiao XY, 2019, PATTERN RECOGN, V90, P285, DOI 10.1016/j.patcog.2019.01.028
   Xie JY, 2022, IEEE T CIRC SYST VID, V32, P7547, DOI 10.1109/TCSVT.2022.3189242
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yan CG, 2021, IEEE T PATTERN ANAL, V43, P1445, DOI 10.1109/TPAMI.2020.2975798
   Yan CG, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3472810
   Yan CG, 2022, IEEE T CIRC SYST VID, V32, P43, DOI 10.1109/TCSVT.2021.3067449
   Yan K, 2021, 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1 (ACL-IJCNLP 2021), P2014
   Yanagi R, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3485042
   Yang L, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3386725
   Yang LJ, 2017, PROC CVPR IEEE, P1978, DOI 10.1109/CVPR.2017.214
   Yang X, 2019, PROC CVPR IEEE, P10677, DOI 10.1109/CVPR.2019.01094
   Yang XW, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P5398, DOI 10.1145/3503161.3548409
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yin GJ, 2019, PROC CVPR IEEE, P6234, DOI 10.1109/CVPR.2019.00640
   Yiwu Zhong, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P211, DOI 10.1007/978-3-030-58568-6_13
   Yu J, 2021, IEEE T IMAGE PROCESS, V30, P220, DOI 10.1109/TIP.2020.3034494
   Yuan J, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3394955
   Zellers R, 2018, PROC CVPR IEEE, P5831, DOI 10.1109/CVPR.2018.00611
   Zhang J, 2019, AAAI CONF ARTIF INTE, P9185
   Zhang SJ, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4235
   Zhao DX, 2020, NEUROCOMPUTING, V373, P98, DOI 10.1016/j.neucom.2019.09.055
   Ziqi Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13275, DOI 10.1109/CVPR42600.2020.01329
NR 69
TC 0
Z9 0
U1 3
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2024
VL 20
IS 6
SI SI
AR 183
DI 10.1145/3648370
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OQ2T3
UT WOS:001208681800033
DA 2024-08-05
ER

PT J
AU Liu, ZY
   Li, D
   Zhang, XY
   Zhang, Z
   Zhang, P
   Shan, CF
   Han, JG
AF Liu, Zhenyu
   Li, Da
   Zhang, Xinyu
   Zhang, Zhang
   Zhang, Peng
   Shan, Caifeng
   Han, Jungong
TI Pedestrian Attribute Recognition via Spatio-temporal Relationship
   Learning for Visual Surveillance
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Video-based pedestrian attribute recognition; spatio-temporal
   relationship learning; IoT security
AB Pedestrian attribute recognition (PAR) aims at predicting the visual attributes of a pedestrian image. PAR has been used as soft biometrics for visual surveillance and IoT security. Most of the current PAR methods are developed based on discrete images. However, it is challenging for the image-based method to handle the occlusion and action-related attributes in real-world applications. Recently, video-based PAR has attractedmuch attention in order to exploit the temporal cues in the video sequences for better PAR. Unfortunately, existing methods usually ignore the correlations among different attributes and the relations between attributes and spatio regions. To address this problem, we propose a novel method for video-based PAR by exploring the relationships among different attributes in both the spatio and temporal domains. More specifically, a spatio-temporal saliency module (STSM) is introduced to capture the key visual patterns from the video sequences, and a module for spatio-temporal attribute relationship learning (STARL) is proposed to mine the correlations among these patterns. Meanwhile, a large-scale benchmark for video-based PAR, RAP-Video, is built by extending the image-based dataset RAP-2, which contains 83,216 tracklets with 25 scenes. To the best of our knowledge, this is the largest dataset for video-based PAR. Extensive experiments are performed on the proposed benchmark as well as on MARS Attribute and DukeMTMC-Video Attribute. The superior performance demonstrates the effectiveness of the proposed method.
C1 [Liu, Zhenyu; Zhang, Xinyu; Zhang, Peng; Shan, Caifeng] Shandong Univ Sci & Technol, Qingdao, Shandong, Peoples R China.
   [Li, Da; Zhang, Zhang] Chinese Acad Sci CASIA, Inst Automat, Ctr Res Intelligent Percept & Comp CRIPAC, State Key Lab Multimodal Artificial Intelligence, Beijing, Peoples R China.
   [Zhang, Zhang] Univ Chinese Acad Sci UCAS, Sch Artificial Intelligence, Beijing, Peoples R China.
   [Shan, Caifeng] Nanjing Univ, Nanjing, Jiangsu, Peoples R China.
   [Han, Jungong] Univ Sheffield, Sheffield S1 4DP, S Yorkshire, England.
C3 Shandong University of Science & Technology; Chinese Academy of
   Sciences; Institute of Automation, CAS; Nanjing University; University
   of Sheffield
RP Shan, CF (corresponding author), Shandong Univ Sci & Technol, Qingdao, Shandong, Peoples R China.; Shan, CF (corresponding author), Nanjing Univ, Nanjing, Jiangsu, Peoples R China.
EM liuzhenyu@sdust.edu.cn; da.li@cripac.ia.ac.cn; zhangxinyu@sdust.edu.cn;
   zzhang@nlpr.ia.ac.cn; pengzhang_skd@sdust.edu.cn;
   caifeng.shan@gmail.com; jungonghan77@gmail.com
RI Shan, Caifeng/W-6178-2019; Zhang, Xinyu/KDP-0796-2024
OI Shan, Caifeng/0000-0002-2131-1671; Zhang, Xinyu/0000-0002-2031-9348;
   zhang, zhang/0000-0001-9425-3065; Zhang, Peng/0000-0001-6794-7352;
   Zhang, XinYu/0009-0002-8189-524X
FU Talent Introduction Program for Youth Innovation Teams of Shandong
   Province
FX This work is supported by the Talent Introduction Program for Youth
   Innovation Teams of Shandong Province.
CR Chai TR, 2022, IEEE T CIRC SYST VID, V32, P7951, DOI 10.1109/TCSVT.2022.3189027
   Chen XD, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11793, DOI 10.1109/ICCV48922.2021.01160
   Cheng XH, 2022, IEEE T CIRC SYST VID, V32, P6994, DOI 10.1109/TCSVT.2022.3178144
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Han K, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2456
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jia J, 2022, AAAI CONF ARTIF INTE, P1069
   Jin X, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3547144
   Lanchantin J, 2021, PROC CVPR IEEE, P16473, DOI 10.1109/CVPR46437.2021.01621
   Layne R, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.24
   Li DW, 2019, IEEE T IMAGE PROCESS, V28, P1575, DOI 10.1109/TIP.2018.2878349
   Li DW, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P111, DOI 10.1109/ACPR.2015.7486476
   Li QZ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P833
   Li QZ, 2020, IEEE T CIRC SYST VID, V30, P2167, DOI 10.1109/TCSVT.2019.2923444
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Lin YT, 2019, PATTERN RECOGN, V95, P151, DOI 10.1016/j.patcog.2019.06.006
   Liu PZ, 2018, Arxiv, DOI arXiv:1808.09102
   Liu Z, 2022, PROC CVPR IEEE, P11966, DOI 10.1109/CVPR52688.2022.01167
   Loshchilov I., 2018, INT C LEARN REPR
   Radford A, 2021, PR MACH LEARN RES, V139
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Ruibing Hou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P388, DOI 10.1007/978-3-030-58595-2_24
   Saquib Sarfraz M., 2017, BRIT MACH VIS C 2017
   Sarafianos N, 2018, LECT NOTES COMPUT SC, V11215, P708, DOI 10.1007/978-3-030-01252-6_42
   Shi QHY, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3485665
   Siadari TS, 2019, IEEE INT CONF COMP V, P1098, DOI 10.1109/ICCVW.2019.00140
   Tan ZC, 2020, AAAI CONF ARTIF INTE, V34, P12055
   Tan ZC, 2019, IEEE T IMAGE PROCESS, V28, P6126, DOI 10.1109/TIP.2019.2919199
   Tang CF, 2019, IEEE I CONF COMP VIS, P4996, DOI 10.1109/ICCV.2019.00510
   Tang ZY, 2023, IEEE T MULTIMEDIA, V25, P7917, DOI 10.1109/TMM.2022.3231103
   Wang X, 2022, PATTERN RECOGN, V121, DOI 10.1016/j.patcog.2021.108220
   Xiang SC, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3588441
   Xinqian Gu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P228, DOI 10.1007/978-3-030-58536-5_14
   Xu C, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3554739
   Xu HT, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3538749
   Yang WC, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21186163
   Yang Y, 2021, INT J COMPUT VISION, V129, P2731, DOI 10.1007/s11263-021-01499-z
   Zheng L, 2016, LECT NOTES COMPUT SC, V9910, P868, DOI 10.1007/978-3-319-46466-4_52
   Zhiyuan Chen, 2019, Pattern Recognition and Computer Vision. Second Chinese Conference, PRCV 2019. Proceedings. Lecture Notes in Computer Science (LNCS 11858), P209, DOI 10.1007/978-3-030-31723-2_18
   Zhu JQ, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P331, DOI 10.1109/ICCVW.2013.51
   Zhu J, 2023, Arxiv, DOI arXiv:2304.10091
NR 43
TC 1
Z9 1
U1 4
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2024
VL 20
IS 6
SI SI
AR 159
DI 10.1145/3632624
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OQ2T3
UT WOS:001208681800009
DA 2024-08-05
ER

PT J
AU Zuo, RL
   Mak, B
AF Zuo, Ronglai
   Mak, Brian
TI Improving Continuous Sign Language Recognition with Consistency
   Constraints and Signer Removal
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Continuous sign language recognition; auxiliary learning;
   signer-independent; feature disentanglement
ID ATTENTION
AB Deep-learning-based continuous sign language recognition (CSLR) models typically consist of a visual module, a sequential module, and an alignment module. However, the effectiveness of training such CSLR backbones is hindered by limited training samples, rendering the use of a single connectionist temporal classification loss insufficient. To address this limitation, we propose three auxiliary tasks to enhance CSLR backbones. First, we enhance the visual module, which is particularly sensitive to the challenges posed by limited training samples, from the perspective of consistency. Specifically, since sign languages primarily rely on signers' facial expressions and hand movements to convey information, we develop a keypoint-guided spatial attention module that directs the visual module to focus on informative regions, thereby ensuring spatial attention consistency. Furthermore, recognizing that the output features of both the visual and sequential modules represent the same sentence, we leverage this prior knowledge to better exploit the power of the backbone. We impose a sentence embedding consistency constraint between the visual and sequential modules, enhancing the representation power of both features. The resulting CSLR model, referred to as consistency-enhanced CSLR, demonstrates superior performance on signer-dependent datasets, where all signers appear during both training and testing. To enhance its robustness for the signer-independent setting, we propose a signer removal module based on feature disentanglement, effectively eliminating signer-specific information from the backbone. To validate the effectiveness of the proposed auxiliary tasks, we conduct extensive ablation studies. Notably, utilizing a transformer-based backbone, our model achieves state-of-the-art or competitive performance on five benchmarks, including PHOENIX-2014, PHOENIX-2014-T, PHOENIX-2014-SI, CSL, and CSL-Daily. Code and models are available at https://github.com/2000ZRL/LCSA_C2SLR_SRM.
C1 [Zuo, Ronglai; Mak, Brian] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
C3 Hong Kong University of Science & Technology
RP Zuo, RL (corresponding author), Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
EM rzuo@cse.ust.hk; mak@cse.ust.hk
OI Zuo, Ronglai/0000-0002-7184-5137
FU Research Grants Council of the Hong Kong Special Administrative Region,
   China [HKUST16200118]
FX The work described in this paper was supported by a grant from the
   Research Grants Council of the Hong Kong Special Administrative Region,
   China (Project No. HKUST16200118).
CR Adaloglou N, 2022, IEEE T MULTIMEDIA, V24, P1750, DOI 10.1109/TMM.2021.3070438
   Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471
   Ba L.J., 2016, arXiv, DOI DOI 10.48550/ARXIV.1607.06450
   Camgoz Necati Cihan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10020, DOI 10.1109/CVPR42600.2020.01004
   Camgoz NC, 2018, PROC CVPR IEEE, P7784, DOI 10.1109/CVPR.2018.00812
   Cao Y, 2019, IEEE INT CONF COMP V, P1971, DOI 10.1109/ICCVW.2019.00246
   Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   Carlsson Fredrik, 2020, ICLR
   Carreira J., 2018, arXiv, DOI DOI 10.48550/ARXIV.1808.01340
   Chen SX, 2019, AAAI CONF ARTIF INTE, P8191
   Chen Yongwei, 2022, NEURIPS
   Chen YT, 2022, PROC CVPR IEEE, P5110, DOI 10.1109/CVPR52688.2022.00506
   Cheng YH, 2022, AAAI CONF ARTIF INTE, P436
   Cui RP, 2019, IEEE T MULTIMEDIA, V21, P1880, DOI 10.1109/TMM.2018.2889563
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   DevonHjelm R., 2019, ICLR
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Ganin Y, 2016, J MACH LEARN RES, V17
   Gao TY, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P6894
   Goyal R, 2017, IEEE I CONF COMP VIS, P5843, DOI 10.1109/ICCV.2017.622
   Graves A., 2006, P 23 INT C MACH LEAR, P369, DOI [DOI 10.1145/1143844.1143891, 10.1145/1143844.1143891]
   Guo D, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P744
   Guo D, 2018, AAAI CONF ARTIF INTE, P6845
   Guo LM, 2023, PROC CVPR IEEE, P10771, DOI 10.1109/CVPR52729.2023.01037
   Hao AM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11283, DOI 10.1109/ICCV48922.2021.01111
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Hu HZ, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3436754
   Hu LY, 2023, PROC CVPR IEEE, P2529, DOI 10.1109/CVPR52729.2023.00249
   Huang J, 2018, AAAI CONF ARTIF INTE, P2257
   Huang ZZ, 2021, PROC CVPR IEEE, P7278, DOI 10.1109/CVPR46437.2021.00720
   Jiao PQ, 2023, IEEE I CONF COMP VIS, P20619, DOI 10.1109/ICCV51070.2023.01890
   Jin X, 2020, PROC CVPR IEEE, P3140, DOI 10.1109/CVPR42600.2020.00321
   Ka Leong Cheng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P697, DOI 10.1007/978-3-030-58586-0_41
   Kingma D. P., 2015, P INT C LEARN REPR, P1
   Koller O, 2020, Arxiv, DOI arXiv:2008.09918
   Koller O, 2020, IEEE T PATTERN ANAL, V42, P2306, DOI 10.1109/TPAMI.2019.2911077
   Koller O, 2017, PROC CVPR IEEE, P3416, DOI 10.1109/CVPR.2017.364
   Koller O, 2015, COMPUT VIS IMAGE UND, V141, P108, DOI 10.1016/j.cviu.2015.09.013
   Li XZ, 2020, AAAI CONF ARTIF INTE, V34, P11434
   Linsley Drew, 2018, ICLR
   Liu Y, 2018, INTERSPEECH, P2247, DOI 10.21437/Interspeech.2018-1226
   Liu Y, 2018, PROC CVPR IEEE, P2080, DOI 10.1109/CVPR.2018.00222
   Liu Y, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1239, DOI 10.1145/3343031.3350986
   Luong T., 2015, P 2015 C EMPIRICAL M, P1412, DOI DOI 10.18653/V1/D15-1166
   Min YC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11522, DOI 10.1109/ICCV48922.2021.01134
   Palangi H, 2016, IEEE-ACM T AUDIO SPE, V24, P694, DOI 10.1109/TASLP.2016.2520371
   Pang YW, 2019, IEEE I CONF COMP VIS, P4966, DOI 10.1109/ICCV.2019.00507
   Papadimitriou K, 2020, INTERSPEECH, P2752, DOI 10.21437/Interspeech.2020-2691
   Pironkov G, 2016, EUR SIGNAL PR CONF, P1911, DOI 10.1109/EUSIPCO.2016.7760581
   Pu JF, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1497, DOI 10.1145/3394171.3413931
   Pu JF, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P885
   Pu JF, 2019, PROC CVPR IEEE, P4160, DOI 10.1109/CVPR.2019.00429
   Reimers N, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P3982
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shaw P., 2018, Self-attention with relative position representations, P464
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Snyder D, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P5329
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tang SG, 2022, IEEE T MULTIMEDIA, V24, P4433, DOI 10.1109/TMM.2021.3117124
   Tian Xu, 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12540), P506, DOI 10.1007/978-3-030-65414-6_35
   van den Oord A, 2019, Arxiv, DOI [arXiv:1807.03748, DOI 10.48550/ARXIV.1807.03748]
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang JD, 2022, Arxiv, DOI [arXiv:2103.03097, DOI 10.48550/ARXIV.2103.03097]
   Wang S, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1483, DOI 10.1145/3240508.3240671
   Wei CC, 2021, IEEE T CIRC SYST VID, V31, P1138, DOI 10.1109/TCSVT.2020.2999384
   Wei FY, 2023, IEEE I CONF COMP VIS, P23555, DOI 10.1109/ICCV51070.2023.02158
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu Felix, 2018, ICLR
   Yang BS, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P4449
   Ye M, 2019, PROC CVPR IEEE, P6203, DOI 10.1109/CVPR.2019.00637
   Yin F, 2016, LECT NOTES COMPUT SC, V9911, P434, DOI 10.1007/978-3-319-46478-7_27
   Yu A, 2018, 2018 OPTICAL FIBER COMMUNICATIONS CONFERENCE AND EXPOSITION (OFC)
   Zhang Y, 2022, IEEE T KNOWL DATA EN, V34, P5586, DOI 10.1109/TKDE.2021.3070203
   Zhe Niu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P172, DOI 10.1007/978-3-030-58517-4_11
   Zheng JB, 2023, PROC CVPR IEEE, P23141, DOI 10.1109/CVPR52729.2023.02216
   Zhou H, 2021, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR46437.2021.00137
   Zhou H, 2020, AAAI CONF ARTIF INTE, V34, P13009
   Zhou H, 2019, IEEE INT CON MULTI, P1282, DOI 10.1109/ICME.2019.00223
   Zuo R, 2023, PROC CVPR IEEE, P14890, DOI 10.1109/CVPR52729.2023.01430
   Zuo RL, 2022, INTERSPEECH, P4810, DOI 10.21437/Interspeech.2022-164
   Zuo RL, 2022, PROC CVPR IEEE, P5121, DOI 10.1109/CVPR52688.2022.00507
NR 83
TC 1
Z9 1
U1 2
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2024
VL 20
IS 6
SI SI
AR 163
DI 10.1145/3640815
PG 25
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OQ2T3
UT WOS:001208681800013
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Hou, SJ
   Li, JC
   Min, WQ
   Hou, Q
   Zhao, YN
   Zheng, YJ
   Jiang, SQ
AF Hou, Sujuan
   Li, Jiacheng
   Min, Weiqing
   Hou, Qiang
   Zhao, Yanna
   Zheng, Yuanjie
   Jiang, Shuqiang
TI Deep Learning for Logo Detection: A Survey
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Logo detection; computer vision; deep learning; datasets
ID CONTEXT; NETWORK
AB Logo detection has gradually become a research hotspot in the field of computer vision and multimedia for its various applications, such as social media monitoring, intelligent transportation, and video advertising recommendation. Recent advances in this area are dominated by deep learning-based solutions, where many datasets, learning strategies, network architectures, and loss functions have been employed. This article reviews the advance in applying deep learning techniques to logo detection. First, we discuss a comprehensive account of public datasets designed to facilitate performance evaluation of logo detection algorithms, which tend to be more diverse, more challenging, and more reflective of real life. Next, we perform an in-depth analysis of the existing logo detection strategies and their strengths and weaknesses of each learning strategy. Subsequently, we summarize the applications of logo detection in various fields, from intelligent transportation and brand monitoring to copyright and trademark compliance. Finally, we analyze the potential challenges and present the future directions for the development of logo detection. This study aims better to inform readers about the current state of logo detection and encourage more researchers to get involved in logo detection.
C1 [Hou, Sujuan; Li, Jiacheng; Hou, Qiang; Zhao, Yanna; Zheng, Yuanjie] Shandong Normal Univ, Sch Informat Sci & Engn, 1 Daxue Rd, Shandong 250358, Peoples R China.
   [Min, Weiqing; Jiang, Shuqiang] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, 6 Kexueyuan South Rd, Beijing 100190, Peoples R China.
C3 Shandong Normal University; Chinese Academy of Sciences; Institute of
   Computing Technology, CAS
RP Hou, SJ (corresponding author), Shandong Normal Univ, Sch Informat Sci & Engn, 1 Daxue Rd, Shandong 250358, Peoples R China.
EM sujuanhou@sdnu.edu.cn; 2021317140@stu.sdnu.edu.cn; minweiqing@ict.ac.cn;
   2019309052@stu.sdnu.edu.cn; yannazhao@sdnu.edu.cn;
   zhengyuanjie@gmail.com; sqjiang@ict.ac.cn
RI zhang, zhang/KGK-5266-2024
OI Zheng, Yuanjie/0000-0002-5786-2491; zhao, yanna/0000-0001-7154-6949;
   hou, sujuan/0000-0002-6547-6048
FU National Nature Science Foundation of China [62072289, 61972378,
   U19B2040]; CAAI-Huawei MindSpore Open Fund
FX This work is supported by the National Nature Science Foundation of
   China (Grants No. 62072289, No. 61972378, and No. U19B2040) and
   CAAI-Huawei MindSpore Open Fund.
CR Alaei A, 2014, 2014 11TH IAPR INTERNATIONAL WORKSHOP ON DOCUMENT ANALYSIS SYSTEMS (DAS 2014), P324, DOI 10.1109/DAS.2014.79
   Bernabeu M, 2022, Arxiv, DOI arXiv:2205.05419
   Bianco S, 2017, NEUROCOMPUTING, V245, P23, DOI 10.1016/j.neucom.2017.03.051
   Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Boia Raluca, 2015, P BRIT MACHINE VISIO
   Hoi SCH, 2015, Arxiv, DOI arXiv:1511.02462
   Carion N., 2020, EUR C COMP VIS, P213
   Carvalho Pedro, 2021, Appl. Sci., V11, P74
   Chen H, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4789, DOI 10.1145/3474085.3479227
   Chen Q, 2021, PROC CVPR IEEE, P13034, DOI 10.1109/CVPR46437.2021.01284
   Chen Y, 2011, COMM COM INF SC, V259, P31
   Cheng ZQ, 2017, IEEE T MULTIMEDIA, V19, P1170, DOI 10.1109/TMM.2016.2647386
   Cheng ZQ, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1365, DOI 10.1145/2964284.2964326
   ChengdeWan Zhicheng Zhao, 2013, Visual Communications and Image Processing, P1
   Constantinopoulos C., 2011, P IEEE INT C MULTIME, P1
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Coustaty Mickael, 2013, P IAPR INT WORKSHOP
   Da P, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON INFORMATION AND AUTOMATION (ICIA), P1793, DOI 10.1109/ICInfA.2016.7832108
   Divvala SK, 2009, PROC CVPR IEEE, P1271, DOI 10.1109/CVPRW.2009.5206532
   Dixit UD, 2023, MULTIMED TOOLS APPL, V82, P863, DOI 10.1007/s11042-022-13300-5
   Eggert C, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR'17), P172, DOI 10.1145/3078971.3078990
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fehérvári I, 2019, IEEE WINT CONF APPL, P715, DOI 10.1109/WACV.2019.00081
   Gallego AJ, 2020, LECT NOTES COMPUT SC, V11867, P485, DOI 10.1007/978-3-030-31332-6_42
   Gao Y, 2016, IEEE T MULTIMEDIA, V18, P2115, DOI 10.1109/TMM.2016.2581483
   Ge Z, 2021, Arxiv, DOI arXiv:2107.08430
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gopinathan S., 2018, 2018 2nd International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC), P707, DOI 10.1109/I-SMAC.2018.8653762
   Guru DS, 2017, ADV INTELL SYST, V459, P555, DOI 10.1007/978-981-10-2104-6_50
   He JM, 2016, 2016 2ND IEEE INTERNATIONAL CONFERENCE ON COMPUTER AND COMMUNICATIONS (ICCC), P370, DOI 10.1109/CompComm.2016.7924725
   Hendrick, 2018, 2018 International Conference on Applied Information Technology and Innovation (ICAITI). Proceedings, P162, DOI 10.1109/ICAITI.2018.8686730
   Tong KH, 2022, Arxiv, DOI arXiv:2206.11462
   Hou Q, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4670, DOI 10.1145/3474085.3475289
   Hou SJ, 2017, COMPLEXITY, DOI 10.1155/2017/3169149
   Hu Changbo, 2020, P IEEE INT C MULTIME, P1
   Huang CL, 2006, IEEE T MULTIMEDIA, V8, P749, DOI 10.1109/TMM.2006.876289
   Jain RK, 2021, IEEE ICCE, DOI 10.1109/ICCE50685.2021.9427658
   Jia XJ, 2021, Arxiv, DOI arXiv:2108.00422
   Jiajun Liu, 2019, 2019 IEEE 4th International Conference on Image, Vision and Computing (ICIVC), P479, DOI 10.1109/ICIVC47709.2019.8981041
   Jiang XL, 2022, ELECTRONICS-SWITZ, V11, DOI 10.3390/electronics11203400
   Jin X, 2020, INT CONF ACOUST SPEE, P4387, DOI [10.1109/ICASSP40776.2020.9053990, 10.1109/icassp40776.2020.9053990]
   Joly A., 2009, P 17 ACM INT C MULTI, P581, DOI DOI 10.1145/1631272.1631361
   Kalantidis Yannis, 2011, P 1 ACM INT C MULTIM, P1
   Karimi Mohammad, 2019, 2019 4th International Conference on Pattern Recognition and Image Analysis (IPRIA), P216, DOI 10.1109/PRIA.2019.8786032
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kumar G, 2021, MULTIMED TOOLS APPL, V80, P4341, DOI 10.1007/s11042-020-09813-6
   Kuznetsov Andrey, 2020, Computer Vision and Graphics. International Conference, ICCVG 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12334), P87, DOI 10.1007/978-3-030-59006-2_8
   Le Viet Phuong, 2016, P CONFALRENCE RECHER
   Lei BY, 2012, 2012 IEEE INTERNATIONAL SYMPOSIUM ON MULTIMEDIA (ISM), P222, DOI 10.1109/ISM.2012.50
   Leng F, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4765, DOI 10.1145/3474085.3479201
   Li CG, 2022, IEEE WINT CONF APPL, P587, DOI 10.1109/WACV51458.2022.00066
   Li CY, 2022, Arxiv, DOI [arXiv:2209.02976, DOI 10.48550/ARXIV.2209.02976]
   Li X, 2024, ENVIRON DEV SUSTAIN, V26, P3639, DOI 10.1007/s10668-022-02851-0
   Li YY, 2017, 2017 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)
   Liao Y, 2017, IEEE I CONF COMP VIS, P4856, DOI 10.1109/ICCV.2017.519
   Lin Qian, 2018, Electron. Imag., V10
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Lu WL, 2021, NEUROCOMPUTING, V463, P623, DOI 10.1016/j.neucom.2021.08.030
   Meng Y, 2021, DISPLAYS, V70, DOI 10.1016/j.displa.2021.102090
   Iandola FN, 2015, Arxiv, DOI arXiv:1510.02131
   Naga Sujini G., 2022, Int. J. Res. Appl. Sci. Eng. Technol., V10, P3267
   Oliveira G, 2016, IEEE IJCNN, P985, DOI 10.1109/IJCNN.2016.7727305
   Orti O, 2019, LECT NOTES COMPUT SC, V11507, P125, DOI 10.1007/978-3-030-20518-8_11
   Ozay Nedret, 2009, 2009 17th European Signal Processing Conference (EUSIPCO 2009), P839
   Palecek K, 2021, 2021 44TH INTERNATIONAL CONFERENCE ON TELECOMMUNICATIONS AND SIGNAL PROCESSING (TSP), P357, DOI 10.1109/TSP52935.2021.9522636
   Pan C., 2013, IET International Conference on Smart and Sustainable City 2013 (ICSSC 2013), P123
   Patalappa Kiran Kumar Jakkur, 2021, Global Trans. Proc., V2, P421
   Pimkote Pichitchai, 2018, 2018 International Conference on Digital Arts, Media and Technology (ICDAMT), P135, DOI 10.1109/ICDAMT.2018.8376510
   Redmon J., 2018, CoRR
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Revaud J., 2012, P 20 ACM INT C MULT, P965
   Romberg Stefan, 2011, P 1 ACM INT C MULTIM, P1
   Sahbi H, 2013, IEEE T IMAGE PROCESS, V22, P1018, DOI 10.1109/TIP.2012.2226046
   Sahel S, 2021, ENG TECHNOL APPL SCI, V11, P6724, DOI 10.48084/etasr.3919
   Sathiaseelan Mukhil Azhagan Mallaiyan, 2021, P 47 INT S TESTING F, P12
   Sotheeswaran Sittampalam, 2014, 2014 9th International Conference on Industrial and Information Systems (ICIIS), P1, DOI 10.1109/ICIINFS.2014.7036486
   Su H, 2021, COMPUT VIS IMAGE UND, V204, DOI 10.1016/j.cviu.2020.103156
   Su H, 2017, IEEE INT CONF COMP V, P270, DOI 10.1109/ICCVW.2017.41
   Su H, 2017, IEEE WINT CONF APPL, P530, DOI 10.1109/WACV.2017.65
   Su Hang, 2022, SSRN Electronic J
   Su Hang, 2018, ARXIV180701964, P111
   Surwase S, 2023, EVOL INTELL, V16, P485, DOI 10.1007/s12065-021-00671-1
   Pham TA, 2011, PROC INT CONF DOC, P718, DOI 10.1109/ICDAR.2011.150
   Tüzkö A, 2018, PROCEEDINGS OF THE 13TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (VISIGRAPP 2018), VOL 5: VISAPP, P284, DOI 10.5220/0006614602840292
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Velazquez DA, 2021, IEEE ACCESS, V9, P106998, DOI 10.1109/ACCESS.2021.3101297
   Le VP, 2014, INT C PATT RECOG, P3056, DOI 10.1109/ICPR.2014.527
   Le VP, 2012, INT C PATT RECOG, P3484
   Le VP, 2013, PROC INT CONF DOC, P270, DOI 10.1109/ICDAR.2013.61
   Wang CY, 2023, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR52729.2023.00721
   Wang CY, 2021, Arxiv, DOI arXiv:2105.04206
   Wang J, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3466780
   Wang J, 2020, AAAI CONF ARTIF INTE, V34, P6194
   Wilms C, 2021, INT C PATT RECOG, P4996, DOI 10.1109/ICPR48806.2021.9412030
   Wilms C, 2019, LECT NOTES COMPUT SC, V11362, P678, DOI 10.1007/978-3-030-20890-5_43
   Xiang ZQ, 2016, INT CONF INFO SCI, P487, DOI 10.1109/ICIST.2016.7483463
   [徐佳宇 Xu Jiayu], 2018, [计算机辅助设计与图形学学报, Journal of Computer-Aided Design & Computer Graphics], V30, P1878
   Xu WP, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4784, DOI 10.1145/3474085.3479203
   Yang S, 2019, OPT LASER TECHNOL, V110, P196, DOI 10.1016/j.optlastec.2018.08.007
   Yang Shuo, 2020, P 2 EAI INT C ROBOTI, P75
   Yang Zhixiong, 2022, 2022 IEEE 4th International Conference on Power, Intelligent Computing and Systems (ICPICS), P350, DOI 10.1109/ICPICS55264.2022.9873791
   Ye QT, 2017, 2017 IEEE THIRD INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM 2017), P250, DOI 10.1109/BigMM.2017.39
   Yin KN, 2020, LECT NOTES COMPUT SC, V12384, P666, DOI 10.1007/978-3-030-59016-1_55
   Yohannes E, 2021, IEEE ACCESS, V9, P102623, DOI 10.1109/ACCESS.2021.3098713
   Yousaf W, 2021, J INTELL FUZZY SYST, V40, P3849, DOI 10.3233/JIFS-190660
   Yu Y, 2022, J ELECTRON IMAGING, V31, DOI 10.1117/1.JEI.31.6.063042
   Yu YT, 2021, IEEE T INTELL TRANSP, V22, P758, DOI 10.1109/TITS.2019.2956082
   Zhang BL, 2013, 2013 6TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING (CISP), VOLS 1-3, P176, DOI 10.1109/CISP.2013.6743981
   Zhang Dong Ming, 2017, P 10 INT C IMAGE SIG, P1
   [张广朋 Zhang Guangpeng], 2022, [软件学报, Journal of Software], V33, P3180
   Zhang JX, 2021, COMPUT ELECTR ENG, V90, DOI 10.1016/j.compeleceng.2021.107004
   Zhang JX, 2021, MOBILE NETW APPL, V26, P67, DOI 10.1007/s11036-020-01722-0
   Zhang YF, 2014, LECT NOTES COMPUT SC, V8485, P805, DOI 10.1007/978-3-319-08010-9_86
   Zhao ZY, 2022, LECT NOTES COMPUT SC, V13529, P624, DOI 10.1007/978-3-031-15919-0_52
   Zhe Li, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2716, DOI 10.1109/ICPR.2010.665
   Zhu G, 2007, PROC INT CONF DOC, P864
NR 119
TC 1
Z9 1
U1 9
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2024
VL 20
IS 3
AR 72
DI 10.1145/3611309
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6F8
UT WOS:001153381000012
OA Bronze, Green Submitted
DA 2024-08-05
ER

PT J
AU Xu, ZB
   Hu, HM
   Liu, L
   Zhang, DP
   Zhang, SF
   Tan, WM
AF Xu, Zhenbo
   Hu, Hai-Miao
   Liu, Liu
   Zhang, Dongping
   Zhang, Shifeng
   Tan, Wenming
TI Instance-Based Continual Learning: A Real-World Dataset and Baseline for
   Fresh Recognition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Online continual learning; fresh recognition
AB Real-time learning on real-world data streams with temporal relations is essential for intelligent agents. However, current online Continual Learning (CL) benchmarks adopt the mini-batch setting and are composed of temporally unrelated and disjoint tasks aswell as pre-set class boundaries. In this paper, we delve into a real-world CL scenario for fresh recognition where algorithms are required to recognize a huge variety of products to facilitate the checkout speed. Products mainly consists of packaged cereals, seasonal fruits, and vegetables from local farms or shipped from overseas. Since algorithms process instance streams consisting of sequential images, we name this real-world CL problem as Instance-Based Continual Learning (IBCL). Different from the current online CL setting, algorithms are required to perform instant testing and learning upon each incoming instance. Moreover, IBCL has no task boundaries or class boundaries and allows the evolution and the forgetting of old samples within each class. To promote the researches on real CL challenges, we propose the first real-world CL dataset coined the Continual Fresh Recognition (CFR) dataset, which consists of fresh recognition data streams (766K labelled images in total) collected from 30 supermarkets. Based on the CFR dataset, we extensively evaluate the performance of current online CL methods under various settings and find that current prominent online CL methods operate at high latency and demand significant memory consumption to cache old samples for replaying. Therefore, we make the first attempt to design an efficient and effective Instant Training-Free Learning (ITFL) framework for IBCL. ITFL consists of feature extractors trained in the metric learning manner and reformulates CL as a temporal classification problem among several most similar classes. Unlike current online CL methods that cache image samples (150KB per image) and rely on training to learn new knowledge, our framework only caches features (2KB per image) and is free of training in deployment. Extensive evaluations across three datasets demonstrate that our method achieves comparable recognition accuracy to current methods with lower latency and less resource consumption. Our codes and datasets will be publicly available at https://github.com/detectRecog/IBCL.
C1 [Xu, Zhenbo] Beihang Univ, Hangzhou Innovat Inst, Hangzhou, Peoples R China.
   [Hu, Hai-Miao] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Hangzhou, Peoples R China.
   [Liu, Liu] ShiFang Technol Inc, Hangzhou, Peoples R China.
   [Zhang, Dongping] China Jiliang Univ, Coll Informat Engn, Hangzhou, Peoples R China.
   [Zhang, Shifeng; Tan, Wenming] Hikvis Res Inst, 555 Qianmo Rd, Hangzhou, Peoples R China.
C3 Beihang University; Beihang University; China Jiliang University
RP Hu, HM (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Hangzhou, Peoples R China.
EM xuzhenbo@mail.ustc.edu.cn; hu@buaa.edu.cn; williuworld@163.com;
   06A0303103@cjlu.edu.cn; zhangshifeng@hikvision.com;
   tanwenming@hikvision.com
OI Xu, zhenbo/0000-0002-8948-1589; Zhang, Dongping/0000-0001-6743-8945; ,
   Di/0000-0001-8065-5901; tan, wenming/0000-0003-1338-4536
FU "Leading Goose" R&D Program of Zhejiang [2022C01082]; National Natural
   Science Foundation of China [62206012, 62122011, U21A20514]; China
   Postdoctoral Science Foundation [2021M700346]; Natural Science
   Foundation of Zhejiang province [Q23F020065]; Fundamental Research Funds
   for the Central Universities
FX This work was partially supported by the "Pioneer" and "Leading Goose"
   R&D Program of Zhejiang (Grant No. 2022C01082), the National Natural
   Science Foundation of China (No. 62206012, No. 62122011, U21A20514), and
   the China Postdoctoral Science Foundation (Grand No. 2021M700346), and
   the Natural Science Foundation of Zhejiang province (No. Q23F020065),
   and the Fundamental Research Funds for the Central Universities.
CR Rusu AA, 2016, Arxiv, DOI [arXiv:1606.04671, DOI 10.48550/ARXIV.1606.04671]
   Aljundi Rahaf, 2019, Advances in neural information processing systems, V32
   [Anonymous], 2016, I EMNLP 2016
   Bang J, 2022, PROC CVPR IEEE, P9265, DOI 10.1109/CVPR52688.2022.00906
   Caccia L, 2021, Arxiv, DOI arXiv:2106.09065
   Cai Q, 2018, PROC CVPR IEEE, P4080, DOI 10.1109/CVPR.2018.00429
   Caron M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9630, DOI 10.1109/ICCV48922.2021.00951
   Castro FM, 2018, LECT NOTES COMPUT SC, V11216, P241, DOI 10.1007/978-3-030-01258-8_15
   Cha Hyuntak, 2021, ICCV, P9516
   Chaudhry A., 2019, arXiv, DOI DOI 10.48550/ARXIV.1902.10486
   Chaudhry A, 2019, Arxiv, DOI [arXiv:1812.00420, DOI 10.48550/ARXIV.1812.00420]
   Chaudhry A, 2018, LECT NOTES COMPUT SC, V11215, P556, DOI 10.1007/978-3-030-01252-6_33
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Douillard A, 2022, PROC CVPR IEEE, P9275, DOI 10.1109/CVPR52688.2022.00907
   Fini Enrico, 2021, arXiv
   Gallardo J, 2021, Arxiv, DOI arXiv:2103.14010
   Graves A, 2014, Arxiv, DOI arXiv:1410.5401
   Gu Yanan, 2022, P IEEE CVF C COMP VI, P7442
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou Q, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4670, DOI 10.1145/3474085.3475289
   Jiang SQ, 2023, IEEE T PATTERN ANAL, V45, P229, DOI 10.1109/TPAMI.2022.3153611
   Jiang SQ, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3391624
   Karagkioules T, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3460819
   Kemker R, 2018, Arxiv, DOI arXiv:1711.10563
   Khosla P., 2020, Adv. Neural Inf. Process. Syst, P18661, DOI DOI 10.48550/ARXIV.2004.11362
   Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Lee KH, 2018, PROC CVPR IEEE, P5447, DOI 10.1109/CVPR.2018.00571
   Lee SC, 2020, Arxiv, DOI arXiv:2001.00689
   Li W, 2017, Arxiv, DOI arXiv:1708.02862
   Li YF, 2022, INT J COMPUT VISION, V130, P2205, DOI 10.1007/s11263-022-01639-z
   Li ZZ, 2018, IEEE T PATTERN ANAL, V40, P2935, DOI 10.1109/TPAMI.2017.2773081
   Lin Zhiqiu, 2021, 35 C NEUR INF PROC S
   Lomonaco V., 2017, P 1 ANN C ROB LEARN, P17
   Lopez-Paz D, 2017, ADV NEUR IN, V30
   Luo DZ, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3473342
   Madaan D., 2021, arXiv, DOI DOI 10.48550/ARXIV.2110.06976
   Mai ZD, 2022, NEUROCOMPUTING, V469, P28, DOI 10.1016/j.neucom.2021.10.021
   Mai Z, 2021, IEEE COMPUT SOC CONF, P3584, DOI 10.1109/CVPRW53098.2021.00398
   Mermillod M, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00504
   Min WQ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P393, DOI 10.1145/3394171.3414031
   Pang B, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3485061
   Parisi GI, 2018, FRONT NEUROROBOTICS, V12, DOI 10.3389/fnbot.2018.00078
   Parisi GI, 2017, NEURAL NETWORKS, V96, P137, DOI 10.1016/j.neunet.2017.09.001
   Pellegrini L, 2020, IEEE INT C INT ROBOT, P10203, DOI 10.1109/IROS45743.2020.9341460
   Prabhu Ameya, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P524, DOI 10.1007/978-3-030-58536-5_31
   Rebuffi SA, 2017, PROC CVPR IEEE, P5533, DOI 10.1109/CVPR.2017.587
   Serrà J, 2018, PR MACH LEARN RES, V80
   Shim DS, 2021, AAAI CONF ARTIF INTE, V35, P9630
   Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802
   Tiwari R, 2022, PROC CVPR IEEE, P99, DOI 10.1109/CVPR52688.2022.00020
   van de Ven G. M, 2019, Three scenarios for continual learning
   Vinyals O, 2016, 30 C NEURAL INFORM P, V29
   Wan Timmy S. T., 2022, P IEEECVF C COMPUTER, P16702
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wang Z, 2022, PROC CVPR IEEE, P171, DOI [10.1109/CVPR52688.2022.00027, 10.1109/ICAPC57304.2022.00039]
   Weston J., 2014, Eprint Arxiv, V2014
   Weston J. E., 2017, End-to-End Memory Networks
   Xu XM, 2023, ANIM BIOTECHNOL, V34, P253, DOI 10.1080/10495398.2021.1955699
   Xue MQ, 2022, PROC CVPR IEEE, P150, DOI 10.1109/CVPR52688.2022.00025
   Yan QS, 2022, PROC CVPR IEEE, P109, DOI 10.1109/CVPR52688.2022.00021
   Zenke F, 2017, PR MACH LEARN RES, V70
   Zeno C, 2019, Arxiv, DOI arXiv:1803.10123
NR 64
TC 0
Z9 0
U1 3
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2024
VL 20
IS 1
AR 1
DI 10.1145/3591209
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T8LL3
UT WOS:001080441800001
DA 2024-08-05
ER

PT J
AU Gu, XM
   Ou, LS
   Zeng, W
   Zhang, JN
   Wong, N
   Wang, Y
AF Gu, Xiangming
   Ou, Longshen
   Zeng, Wei
   Zhang, Jianan
   Wong, Nicholas
   Wang, Ye
TI Automatic Lyric Transcription and Automatic Music Transcription from
   Multimodal Singing
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Singing; automatic lyric transcription; automatic music transcription;
   multimodality; dataset; self-supervised-learning
ID POLYPHONIC MUSIC; PITCH ESTIMATION; SPEECH; ALIGNMENT
AB Automatic lyric transcription (ALT) refers to transcribing singing voices into lyrics, while automatic music transcription (AMT) refers to transcribing singing voices into note events, i.e., musical MIDI notes. Despite these two tasks having significant potential for practical application, they are still nascent. This is because the transcription of lyrics and note events solely from singing audio is notoriously difficult due to the presence of noise contamination, e.g., musical accompaniment, resulting in a degradation of both the intelligibility of sung lyrics and the recognizability of sung notes. To address this challenge, we propose a general framework for implementing multimodal ALT and AMT systems. Additionally, we curate the first multimodal singing dataset, comprising N20EMv1 and N20EMv2, which encompasses audio recordings and videos of lip movements, together with ground truth for lyrics and note events. For model construction, we propose adapting self-supervised learning models from the speech domain as acoustic encoders and visual encoders to alleviate the scarcity of labeled data. We also introduce a residual cross-attention mechanism to effectively integrate features from the audio and video modalities. Through extensive experiments, we demonstrate that our single-modal systems exhibit state-of-the-art performance on both ALT and AMT tasks. Subsequently, through single-modal experiments, we also explore the individual contributions of each modality to the multimodal system. Finally, we combine these and demonstrate the effectiveness of our proposed multimodal systems, particularly in terms of their noise robustness.
C1 [Gu, Xiangming; Ou, Longshen; Zeng, Wei; Zhang, Jianan; Wong, Nicholas; Wang, Ye] Natl Univ Singapore, 21 Lower Kent Ridge Rd, Singapore 119077, Singapore.
C3 National University of Singapore
RP Gu, XM (corresponding author), Natl Univ Singapore, 21 Lower Kent Ridge Rd, Singapore 119077, Singapore.
EM xiangming@u.nus.edu; oulongshen@u.nus.edu; w.zeng@u.nus.edu;
   e0950471@u.nus.edu; wong_nicholas@u.nus.edu; wangye@comp.nus.edu.sg
RI Zeng, Wei/KFS-6178-2024; Wang, Ye/KGL-6405-2024
OI Zeng, Wei/0000-0002-0953-5314; Wang, Ye/0000-0002-0123-1260
FU Ministry of Education in Singapore [MOE-T2EP20120-0012]
FX This project is funded by a research grant MOE-T2EP20120-0012 from the
   Ministry of Education in Singapore.
CR Afouras T, 2018, Arxiv, DOI arXiv:1809.00496
   Arroyo V, 2022, INT CONF ACOUST SPEE, P4603, DOI 10.1109/ICASSP43922.2022.9746239
   Baevski A, 2020, Advances in neural information processing systems, V33, P12449, DOI 10.5555/3495724.3496768
   Basak S, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P266, DOI 10.1109/ICASSP39728.2021.9415096
   Chen K, 2022, INT CONF ACOUST SPEE, P621, DOI 10.1109/ICASSP43922.2022.9747304
   Chorowski J, 2015, ADV NEUR IN, V28
   Chung JS, 2018, INTERSPEECH, P1086
   Dabike GR, 2019, INTERSPEECH, P579, DOI 10.21437/Interspeech.2019-2378
   Davies R, 2009, INT J LANG COMM DIS, V44, P164, DOI 10.1080/13682820801997189
   de Cheveigné A, 2002, J ACOUST SOC AM, V111, P1917, DOI 10.1121/1.1458024
   Defossez A., 2021, P ISMIR 2021 WORKSH
   Défossez A, 2021, Arxiv, DOI [arXiv:1911.13254, DOI 10.48550/ARXIV.1911.13254]
   Demirel E, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P586, DOI 10.1109/ICASSP39728.2021.9414395
   Demirel E, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207052
   Demirel Emir, 2021, P 22 INT SOC MUS INF, P151
   Deng Tengyu, 2022, 23 INT SOC MUS INF R, P633
   Duan ZY, 2013, ASIAPAC SIGN INFO PR
   Dzhambazov G., 2017, Knowledge-based probabilistic modeling for tracking lyrics in music audio signals
   Fu Zih-Sing, 2019, P 20 INT SOC MUS INF, P900
   Fujihara Hiromasa, 2008, P 9 INT SOC MUS INF, P281
   Gao XX, 2022, INT CONF ACOUST SPEE, P791, DOI 10.1109/ICASSP43922.2022.9747684
   Gao XX, 2022, IEEE-ACM T AUDIO SPE, V30, P2280, DOI 10.1109/TASLP.2022.3190742
   Gao Xiaoxue, 2023, 2023 IEEE INT C AC S, P1
   Gfeller B, 2020, IEEE-ACM T AUDIO SPE, V28, P1118, DOI 10.1109/TASLP.2020.2982285
   Gómez E, 2013, COMPUT MUSIC J, V37, P73, DOI 10.1162/COMJ_a_00180
   Gonzalez S, 2014, IEEE-ACM T AUDIO SPE, V22, P518, DOI 10.1109/TASLP.2013.2295918
   Gu Xiangming, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P8760, DOI 10.1145/3581783.3612272
   Gu XM, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P3328, DOI 10.1145/3503161.3548411
   Gupta C, 2020, INT CONF ACOUST SPEE, P496, DOI [10.1109/icassp40776.2020.9054567, 10.1109/ICASSP40776.2020.9054567]
   Hansen Jens Kofod, 2012, 9 SOUND MUS COMP C S, P494
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hosoya Toru, 2005, P 6 INT SOC MUS INF, P532
   Hsu Jui-Yang, 2021, INT SOC MUSIC INFORM, P293
   Hu XX, 2019, IEEE IMAGE PROC, P1440, DOI [10.1109/icip.2019.8803025, 10.1109/ICIP.2019.8803025]
   Huang F, 2013, IEEE T AUDIO SPEECH, V21, P97, DOI 10.1109/TASL.2012.2215589
   Huang RJ, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P2525, DOI 10.1145/3503161.3547854
   Huang Y., 2021, ADV NEURAL INFORM PR
   Kim JW, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P161, DOI 10.1109/ICASSP.2018.8461329
   Kingma D. P., 2014, arXiv
   Kruspe Anna M., 2015, P 16 INT SOC MUS INF, P336
   Kum S, 2022, INT CONF ACOUST SPEE, P796, DOI 10.1109/ICASSP43922.2022.9747147
   Kumar A, 2022, INT C LEARN REPR
   Li YZ, 2024, Arxiv, DOI arXiv:2306.00107
   Liu JL, 2022, AAAI CONF ARTIF INTE, P11020
   Ma PC, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P7613, DOI 10.1109/ICASSP39728.2021.9414567
   Mauch Matthias, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P659, DOI 10.1109/ICASSP.2014.6853678
   Mauch M, 2012, IEEE T AUDIO SPEECH, V20, P200, DOI 10.1109/TASL.2011.2159595
   Mauch Matthias, 2015, P INT C TECHN MUS NO, P23
   MCGURK H, 1976, NATURE, V264, P746, DOI 10.1038/264746a0
   McVicar Matt, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P3117, DOI 10.1109/ICASSP.2014.6854174
   MELTZOFF AN, 1977, SCIENCE, V198, P75, DOI 10.1126/science.198.4312.75
   Mesaros A, 2010, INT CONF ACOUST SPEE, P2146, DOI 10.1109/ICASSP.2010.5495585
   Meseguer-Brocal G, 2019, Arxiv, DOI arXiv:1906.10606
   Meseguer-Brocal Gabriel, 2020, Transactions of the International Society for Music Information Retrieval, V3, P55
   Miyato T, 2019, IEEE T PATTERN ANAL, V41, P1979, DOI 10.1109/TPAMI.2018.2858821
   Molina E., 2014, PROC 15 INT SOC MUSI, P567
   Muller Meinard, 2019, Dagstuhl Reports, V9
   Murad D, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1679, DOI 10.1145/3240508.3240691
   Nishikimi R, 2021, APSIPA TRANS SIGNAL, V10, DOI 10.1017/ATSIP.2021.4
   Ou Longshen, 2022, P 23 INT SOC MUSIC I, P891
   Pan XC, 2022, PROCEEDINGS OF THE 60TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), VOL 1: (LONG PAPERS), P4491
   Panayotov V, 2015, INT CONF ACOUST SPEE, P5206, DOI 10.1109/ICASSP.2015.7178964
   Qizhe Xie, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10684, DOI 10.1109/CVPR42600.2020.01070
   Raffel Colin, 2014, P 15 INT SOC MUS INF
   Roman Miguel A., 2018, ISMIR, P34
   Seichter D, 2021, IEEE INT CONF ROBOT, P13525, DOI 10.1109/ICRA48506.2021.9561675
   Sharma B, 2020, IEEE-ACM T AUDIO SPE, V28, P319, DOI 10.1109/TASLP.2019.2955253
   Shi BW, 2022, Arxiv, DOI arXiv:2201.01763
   Shi Bowen, 2022, INT C LEARN REPR
   Stoller D, 2019, INT CONF ACOUST SPEE, P181, DOI 10.1109/ICASSP.2019.8683470
   Su L, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P371, DOI 10.1109/ICASSP.2018.8462420
   Tam Cynthia, 2007, Occup Ther Int, V14, P99, DOI 10.1002/oti.227
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tao RJ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3927, DOI 10.1145/3474085.3475587
   Vaswani A., 2017, Advances in neural information processing systems, P5998
   Wang JY, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P276, DOI 10.1109/ICASSP39728.2021.9414601
   Wang Y, 2008, IEEE MULTIMEDIA, V15, P70, DOI 10.1109/MMUL.2008.49
   Watanabe S, 2017, IEEE J-STSP, V11, P1240, DOI 10.1109/JSTSP.2017.2763455
   Yang WM, 2023, IEEE T MULTIMEDIA, V25, P3881, DOI 10.1109/TMM.2022.3168132
   Zhang Chen, 2022, P 23 INT SOC MUS INF, P454
NR 80
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2024
VL 20
IS 7
SI SI
AR 209
DI 10.1145/3651310
PG 29
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL0Q6
UT WOS:001234494100024
OA hybrid
DA 2024-08-05
ER

PT J
AU Huo, YD
   Qin, QB
   Dai, JY
   Zhang, WF
   Huang, L
   Wang, CD
AF Huo, Yadong
   Qin Qibing
   Dai, Jiangyan
   Zhang, Wenfeng
   Huang, Lei
   Wang, Chengduan
TI Deep Neighborhood-aware Proxy Hashing with Uniform Distribution
   Constraint for Cross-modal Retrieval
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Cross-modal retrieval; deep hashing; neighborhood-aware proxy loss;
   uniform distribution constraint; discrimination loss
ID NETWORK
AB Cross-modal retrieval methods based on hashing have gained significant attention in both academic and industrial research. Deep learning techniques have played a crucial role in advancing supervised cross-modal hashing methods, leading to significant practical improvements. Despite these achievements, current deep cross-modal hashing still encounters some underexplored limitations. Specifically, most of the available deep hashing usually utilizes pair-wise or triplet-wise strategies to promote the separation of the inter-classes by calculating the relative similarities between samples, weakening the compactness of intra-class data from different modalities, which could generate ambiguous neighborhoods. In this article, the Deep Neighborhoodaware Proxy Hashing (DNPH) framework is proposed to learn a discriminative embedding spacewith the original neighborhood relation preserved. By introducing learnable shared category proxies, the neighborhoodaware proxy loss is proposed to project the heterogeneous data into a unified common embedding, in which the sample is pulled closer to the corresponding category proxy and is pushed away from other proxies, capturing small within-class scatter and big between-class scatter. To enhance the quality of the obtained binary codes, the uniform distribution constraint is developed tomake each hash bit independently obey the discrete uniform distribution. In addition, the discrimination loss is designed to preserve modality-specific semantic information of samples. Extensive experiments are performed on three benchmark datasets to prove that our proposed DNPH framework achieves comparable or even better performance compared with the state-of-the-art cross-modal retrieval applications. The corresponding code implementation of our DNPH framework is as follows: https://github.com/QinLab-WFU/OUR-DNPH.
C1 [Huo, Yadong] Qufu Normal Univ, 80 Yantai North Rd, Rizhao, Peoples R China.
   [Qin Qibing] Weifang Univ, 5147 Dongfeng East St, Weifang, Peoples R China.
   [Qin Qibing; Huang, Lei] Ocean Univ China, 238 Songling Rd, Qingdao 266100, Peoples R China.
   [Dai, Jiangyan; Wang, Chengduan] Weifang Univ, 5147 Dongfeng East St, Weifang 261061, Peoples R China.
   [Zhang, Wenfeng] Chongqing Normal Univ, 37 Univ Town Cent Rd, Chongqing, Peoples R China.
C3 Qufu Normal University; Weifang University; Ocean University of China;
   Weifang University; Chongqing Normal University
RP Qin, QB (corresponding author), Weifang Univ, 5147 Dongfeng East St, Weifang, Peoples R China.; Qin, QB (corresponding author), Ocean Univ China, 238 Songling Rd, Qingdao 266100, Peoples R China.
EM hyd199810@163.com; qinbing@wfu.edu.cn; daijy@wfu.edu.cn;
   itzhangwf@cqnu.edu.cn; huangl@ouc.edu.cn; 20111182@wfu.edu.cn
OI Qin, Qibing/0000-0001-7976-318X; Huo, Yadong/0009-0008-8805-8958; Huang,
   Lei/0000-0003-4087-3677; Zhang, Wengfeng/0000-0001-7459-2510
FU National Natural Science Foundation of China [62302338, 62006174];
   Shandong Provincial Natural Science Foundation [ZR2022QF046,
   ZR2023MF033]; Natural Science Foundation of Chongqing
   [CSTB2023NSCQ-MSX0407]; Science and Technology Research Program of
   Chongqing Municipal Education Commission [KJQN202200551]; Chongqing
   Normal University Foundation [21XLB026]
FX This work was supported in part by the National Natural Science
   Foundation of China (Grants No. 62302338 and No. 62006174), Shandong
   Provincial Natural Science Foundation (Grants No. ZR2022QF046 and No.
   ZR2023MF033), Natural Science Foundation of Chongqing (Grant No.
   CSTB2023NSCQ-MSX0407), Science and Technology Research Program of
   Chongqing Municipal Education Commission (Grant No. KJQN202200551), and
   Chongqing Normal University Foundation (Grant No. 21XLB026).
CR Arjovsky M, 2017, PR MACH LEARN RES, V70
   Cao Y, 2018, LECT NOTES COMPUT SC, V11205, P207, DOI 10.1007/978-3-030-01246-5_13
   Cao Y, 2018, PROC CVPR IEEE, P1287, DOI 10.1109/CVPR.2018.00140
   Chen DP, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P3291, DOI 10.1145/3503161.3548195
   Chen W, 2023, IEEE T PATTERN ANAL, V45, P7270, DOI 10.1109/TPAMI.2022.3218591
   Cong Bai, 2020, ICMR '20: Proceedings of the 2020 International Conference on Multimedia Retrieval, P525, DOI 10.1145/3372278.3390711
   Deng C, 2018, IEEE T IMAGE PROCESS, V27, P3893, DOI 10.1109/TIP.2018.2821921
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Doan KD, 2022, PROC CVPR IEEE, P9437, DOI 10.1109/CVPR52688.2022.00923
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Fu YJ, 2020, IEEE T IMAGE PROCESS, V29, P6535, DOI 10.1109/TIP.2020.2991510
   Gu W, 2019, ICMR'19: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P159, DOI 10.1145/3323873.3325045
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu P, 2023, IEEE T PATTERN ANAL, V45, P3877, DOI 10.1109/TPAMI.2022.3177356
   Huo YD, 2024, IEEE T CIRC SYST VID, V34, P576, DOI 10.1109/TCSVT.2023.3285266
   Gulrajani I, 2017, ADV NEUR IN, V30
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Kingma D. P., 2014, arXiv
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li C, 2018, PROC CVPR IEEE, P4242, DOI 10.1109/CVPR.2018.00446
   Li TY, 2022, AAAI CONF ARTIF INTE, P10275
   Li YQ, 2021, AAAI CONF ARTIF INTE, V35, P2002
   Liao L, 2023, IEEE T CIRC SYST VID, V33, P920, DOI 10.1109/TCSVT.2022.3203247
   Lin KY, 2020, AAAI CONF ARTIF INTE, V34, P11515
   Lu X, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1129, DOI 10.1145/3343031.3350999
   Lu X, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P715, DOI 10.1145/3331184.3331217
   Luo X, 2023, ACM T KNOWL DISCOV D, V17, DOI 10.1145/3532624
   Meng M, 2021, IEEE T IMAGE PROCESS, V30, P986, DOI 10.1109/TIP.2020.3038365
   Paszke A, 2019, ADV NEUR IN, V32
   Qin QB, 2024, IEEE T MULTIMEDIA, V26, P1881, DOI 10.1109/TMM.2023.3289765
   Qin QB, 2023, IEEE T CIRC SYST VID, V33, P7914, DOI 10.1109/TCSVT.2023.3281868
   Qin QB, 2021, IEEE T CIRC SYST VID, V31, P2852, DOI 10.1109/TCSVT.2020.3032402
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Sennrich R, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1715
   Sun CC, 2022, IEEE COMPUT SOC CONF, P4937, DOI 10.1109/CVPRW56347.2022.00541
   Sun L, 2023, PROCEEDINGS OF THE 2023 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, ICMR 2023, P499, DOI 10.1145/3591106.3592242
   Tu Junfeng, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P453, DOI 10.1145/3503161.3548187
   Tu RC, 2023, IEEE T KNOWL DATA EN, V35, P6798, DOI 10.1109/TKDE.2022.3187023
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Villani C., 2009, Optimal Transport, DOI [10.1007/978-3-540-71050-9, DOI 10.1007/978-3-540-71050-9]
   Wang Chia-Hui, 2023, P IEEE CVF C COMP VI, P6165
   Wang J, 2012, IEEE T PATTERN ANAL, V34, P2393, DOI 10.1109/TPAMI.2012.48
   Wang XZ, 2020, NEUROCOMPUTING, V400, P255, DOI 10.1016/j.neucom.2020.03.019
   Wang YX, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P871, DOI 10.1145/3394171.3413971
   Wei Yuxin, 2023, Cross-modal retrieval based on shared proxies, DOI [10.21203/rs.3.rs-2667484/v1, DOI 10.21203/RS.3.RS-2667484/V1]
   Wu F, 2021, IEEE IMAGE PROC, P2743, DOI 10.1109/ICIP42928.2021.9506623
   Wu JL, 2023, IEEE T IMAGE PROCESS, V32, P2215, DOI 10.1109/TIP.2023.3265261
   Xu CY, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P3173, DOI 10.1145/3503161.3548032
   Xu RQ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P982
   Ye ZD, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3356338
   Yu J, 2021, AAAI CONF ARTIF INTE, V35, P4626
   Zhan YW, 2022, PATTERN RECOGN, V122, DOI 10.1016/j.patcog.2021.108262
   Zhang CY, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3412847
   Zhang PF, 2022, IEEE T MULTIMEDIA, V24, P466, DOI 10.1109/TMM.2021.3053766
   Zhang Q., 2022, P INT JOINT C ART IN, P1651
   Zhang Z, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3532519
   Zhang Z, 2023, IEEE T KNOWL DATA EN, V35, P5091, DOI 10.1109/TKDE.2022.3144352
   Zheng CQ, 2023, EXPERT SYST APPL, V233, DOI 10.1016/j.eswa.2023.120913
   Zhong HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9204, DOI 10.1109/ICCV48922.2021.00909
   Zhu L, 2024, IEEE T KNOWL DATA EN, V36, P239, DOI 10.1109/TKDE.2023.3282921
   Zhu L, 2020, IEEE T IMAGE PROCESS, V29, P4643, DOI 10.1109/TIP.2020.2974065
   Zhu QN, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1943
   Zou XT, 2022, NEUROCOMPUTING, V467, P138, DOI 10.1016/j.neucom.2021.09.053
NR 63
TC 0
Z9 0
U1 4
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2024
VL 20
IS 6
SI SI
AR 169
DI 10.1145/3643639
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OQ2T3
UT WOS:001208681800019
DA 2024-08-05
ER

PT J
AU Suo, YC
   Zheng, ZD
   Wang, XH
   Zhang, B
   Yang, Y
AF Suo, Yucheng
   Zheng, Zhedong
   Wang, Xiaohan
   Zhang, Bang
   Yang, Yi
TI Jointly Harnessing Prior Structures and Temporal Consistency for Sign
   Language Video Generation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Sign language; motion transfer; video generation; jointly training
ID HUMAN POSE ESTIMATION
AB Sign language provides a way for differently-abled individuals to express their feelings and emotions. However, learning sign language can be challenging and time consuming. An alternative approach is to animate user photos using sign language videos of specific words, which can be achieved using existing image animation methods. However, the finger motions in the generated videos are often not ideal. To address this issue, we propose the Structure-aware Temporal Consistency Network (STCNet), which jointly optimizes the prior structure of humans with temporal consistency to produce sign language videos. We use a fine-grained skeleton detector to acquire knowledge of body structure and introduce both short- and long-term cycle loss to ensure the continuity of the generated video. The two losses and keypoint detector network are optimized in an end-to-end manner. Quantitative and qualitative evaluations on three widely used datasets, namely LSA64, Phoenix-2014T, and WLASL-2000, demonstrate the effectiveness of the proposed method. It is our hope that this work can contribute to future studies on sign language production.
C1 [Suo, Yucheng; Wang, Xiaohan; Yang, Yi] Zhejiang Univ, Coll Comp Sci & Technol, 38 Zheda Rd, Hangzhou 310027, Zhejiang, Peoples R China.
   [Zheng, Zhedong] Univ Macau, Fac Sci & Technol, Taipa Univ Blvd, Macau 999078, Peoples R China.
   [Zheng, Zhedong] Univ Macau, Inst Collaborat Innovat, Taipa Univ Blvd, Macau 999078, Peoples R China.
   [Zhang, Bang] Alibaba Grp, DAMO Acad, 969 Wenyi West Rd, Hangzhou 311121, Zhejiang, Peoples R China.
C3 Zhejiang University; University of Macau; University of Macau; Alibaba
   Group
RP Yang, Y (corresponding author), Zhejiang Univ, Coll Comp Sci & Technol, 38 Zheda Rd, Hangzhou 310027, Zhejiang, Peoples R China.
EM suoych@zju.edu.cn; zhedongzheng@um.edu.mo; xiaohan.wang@zju.edu.cn;
   zhangbang.zb@alibaba-inc.com; yangyics@zju.edu.cn
RI Zheng, Zhedong/R-5314-2019
OI Zheng, Zhedong/0000-0002-2434-9050; Suo, Yucheng/0000-0001-5358-6410
FU Major program of the National Natural Science Foundation of China
   [T2293723]; Natural Science Foundation of Zhejiang Province
   [DT23F020008]
FX This work was partially supported by the Major program of the National
   Natural Science Foundation of China (grant T2293723). This work was also
   supported in part by the Natural Science Foundation of Zhejiang Province
   (DT23F020008).
CR Albanie S, 2021, Arxiv, DOI arXiv:2111.03635
   Albanie Samuel, 2020, ECCV, P35, DOI DOI 10.1007/978-3-030-58621-8_3
   Andriluka M, 2018, PROC CVPR IEEE, P5167, DOI 10.1109/CVPR.2018.00542
   Artacho B, 2020, PROC CVPR IEEE, P7033, DOI 10.1109/CVPR42600.2020.00706
   Ben S, 2021, IEEE INT CONF AUTOMA
   BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792
   Camgoz NC, 2018, PROC CVPR IEEE, P7784, DOI 10.1109/CVPR.2018.00812
   Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chan C, 2019, IEEE I CONF COMP VIS, P5932, DOI 10.1109/ICCV.2019.00603
   Charles J, 2016, PROC CVPR IEEE, P3063, DOI 10.1109/CVPR.2016.334
   Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
   Chen YT, 2022, PROC CVPR IEEE, P5110, DOI 10.1109/CVPR52688.2022.00506
   Cherian A, 2014, PROC CVPR IEEE, pCP32, DOI 10.1109/CVPR.2014.302
   Dang Q, 2019, TSINGHUA SCI TECHNOL, V24, P663, DOI 10.26599/TST.2018.9010100
   Dantone M, 2013, PROC CVPR IEEE, P3041, DOI 10.1109/CVPR.2013.391
   Duarte A, 2021, PROC CVPR IEEE, P2734, DOI 10.1109/CVPR46437.2021.00276
   Efros AA, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P726
   Eichner M, 2012, INT J COMPUT VISION, V99, P190, DOI 10.1007/s11263-012-0524-9
   Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256
   Gao SC, 2023, PROC CVPR IEEE, P10021, DOI 10.1109/CVPR52729.2023.00966
   Gruber I, 2021, IEEE COMPUT SOC CONF, P3419, DOI 10.1109/CVPRW53098.2021.00381
   Guo D, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P744
   Han XT, 2018, PROC CVPR IEEE, P7543, DOI 10.1109/CVPR.2018.00787
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ho J., 2020, Adv. Neural. Inf. Process. Syst, V33, P6840, DOI DOI 10.48550/ARXIV.2006.11239
   Ho TT, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3396237
   Hu BW, 2022, IEEE T MULTIMEDIA, V24, P1233, DOI 10.1109/TMM.2022.3143712
   Hu HZ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11067, DOI 10.1109/ICCV48922.2021.01090
   Hu HZ, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3436754
   Huang SL, 2017, IEEE I CONF COMP VIS, P3047, DOI 10.1109/ICCV.2017.329
   Huang ZK, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P652
   Jiang S, 2021, arXiv
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karras J, 2023, Arxiv, DOI arXiv:2304.06025
   Kingma D. P., 2014, arXiv
   Kreiss S, 2022, IEEE T INTELL TRANSP, V23, P13498, DOI 10.1109/TITS.2021.3124981
   Kreiss S, 2019, PROC CVPR IEEE, P11969, DOI 10.1109/CVPR.2019.01225
   Krishna S, 2021, IEEE INT CONF COMP V, P2640, DOI 10.1109/ICCVW54120.2021.00298
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li D., 2020, Advances in Neural Information Processing Systems (NeurIPS), V33, P12034
   Li DX, 2020, IEEE WINT CONF APPL, P1448, DOI [10.1109/wacv45572.2020.9093512, 10.1109/WACV45572.2020.9093512]
   Li JF, 2019, PROC CVPR IEEE, P10855, DOI 10.1109/CVPR.2019.01112
   Li YL, 2020, PROC CVPR IEEE, P379, DOI [10.1109/ICEMME51517.2020.00080, 10.1109/CVPR42600.2020.00046]
   Liu PR, 2023, Arxiv, DOI arXiv:2110.04658
   Liu SG, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3571746
   Liu ZM, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3554738
   Murray JJ, 2018, HUM RIGHTS QUART, V40, P37, DOI 10.1353/hrq.2018.0001
   Newell A, 2017, ADV NEUR IN, V30
   Nichol A, 2022, Arxiv, DOI arXiv:2112.10741
   Nie BX, 2015, PROC CVPR IEEE, P1293, DOI 10.1109/CVPR.2015.7298734
   Papandreou G, 2018, LECT NOTES COMPUT SC, V11218, P282, DOI 10.1007/978-3-030-01264-9_17
   Paszke A, 2019, ADV NEUR IN, V32
   Pfister T, 2015, IEEE I CONF COMP VIS, P1913, DOI 10.1109/ICCV.2015.222
   Pishchulin L, 2016, PROC CVPR IEEE, P4929, DOI 10.1109/CVPR.2016.533
   Pu JF, 2019, PROC CVPR IEEE, P4160, DOI 10.1109/CVPR.2019.00429
   Qiu ZW, 2023, Arxiv, DOI arXiv:2306.17074
   Ronchetti F., 2016, Congr. Argentine/ Ciencias la Comput
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Saunders Ben, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P687, DOI 10.1007/978-3-030-58621-8_40
   Saunders B, 2022, PROC CVPR IEEE, P5131, DOI 10.1109/CVPR52688.2022.00508
   Saunders B, 2020, Arxiv, DOI arXiv:2011.09846
   Saunders B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1899, DOI 10.1109/ICCV48922.2021.00193
   Saunders Ben, 2020, P 31 BRIT MACH VIS V
   Siarohin A, 2019, ADV NEUR IN, V32
   Siarohin A, 2021, PROC CVPR IEEE, P13648, DOI 10.1109/CVPR46437.2021.01344
   Siarohin A, 2019, PROC CVPR IEEE, P2372, DOI 10.1109/CVPR.2019.00248
   Simon T, 2017, PROC CVPR IEEE, P4645, DOI 10.1109/CVPR.2017.494
   Simonyan K., 2015, P 3 INT C LEARN REPR, P1
   Sincan OM, 2021, IEEE COMPUT SOC CONF, P3467, DOI 10.1109/CVPRW53098.2021.00386
   Song JM, 2022, Arxiv, DOI [arXiv:2010.02502, DOI 10.48550/ARXIV.2010.02502]
   Song J, 2017, PROC CVPR IEEE, P5563, DOI 10.1109/CVPR.2017.590
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Tavella F, 2022, INT CONF ACOUST SPEE, P8452, DOI 10.1109/ICASSP43922.2022.9747212
   Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214
   Tulyakov S, 2018, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR.2018.00165
   Vadivel A., 2003, P INT C INF TECHN CI, P159
   Ventura L, 2021, Arxiv, DOI arXiv:2012.10941
   Wang T, 2024, Arxiv, DOI arXiv:2307.00040
   Wang XL, 2019, PROC CVPR IEEE, P2561, DOI 10.1109/CVPR.2019.00267
   Wang YH, 2020, PROC CVPR IEEE, P5263, DOI 10.1109/CVPR42600.2020.00531
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511
   Weinzaepfel P, 2013, IEEE I CONF COMP VIS, P1385, DOI 10.1109/ICCV.2013.175
   Wu XT, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3576858
   Xu CM, 2020, IEEE T IMAGE PROCESS, V29, P9060, DOI 10.1109/TIP.2020.3023853
   Yang CY, 2018, LECT NOTES COMPUT SC, V11214, P204, DOI 10.1007/978-3-030-01249-6_13
   Yang Shuyu, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P4492, DOI 10.1145/3581783.3611709
   Yoon JS, 2021, PROC CVPR IEEE, P15034, DOI 10.1109/CVPR46437.2021.01479
   Zauss D, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11037, DOI 10.1109/ICCV48922.2021.01087
   Zelinka J, 2020, IEEE WINT CONF APPL, P3384, DOI 10.1109/WACV45572.2020.9093516
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhao J, 2022, PROC CVPR IEEE, P3647, DOI 10.1109/CVPR52688.2022.00364
   Zheng C, 2024, ACM COMPUT SURV, V56, DOI 10.1145/3603618
   Zheng Zhedong, 2022, IEEE Transactions on Neural Networks and Learning Systems, P1
   Zhou H, 2020, AAAI CONF ARTIF INTE, V34, P13009
   Zhou YP, 2019, IEEE INT CONF COMP V, P1208, DOI 10.1109/ICCVW.2019.00153
NR 97
TC 1
Z9 1
U1 1
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2024
VL 20
IS 6
SI SI
AR 185
DI 10.1145/3648368
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OQ2T3
UT WOS:001208681800035
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Huang, JF
   Zhang, TJ
   Zhao, SJ
   Zhang, L
   Zhou, YC
AF Huang, Jiafeng
   Zhang, Tianjun
   Zhao, Shengjie
   Zhang, Lin
   Zhou, Yicong
TI An Underwater Organism Image Dataset and a Lightweight Module Designed
   for Object Detection Networks
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Benchmark dataset; object detection; lightweight module
ID FISH ABUNDANCE
AB Long-term monitoring and recognition of underwater organism objects are of great significance in marine ecology, fisheries science and many other disciplines. Traditional techniques in this field, including manual fishing-based ones and sonar-based ones, are usually flawed. Specifically, the method based on manual fishing is time-consuming and unsuitable for scientific researches, while the sonar-based one, has the defects of low acoustic image accuracy and large echo errors. In recent years, the rapid development of deep learning and its excellent performance in computer vision tasks make vision-based solutions feasible. However, the researches in this area are still relatively insufficient in mainly two aspects. First, to our knowledge, there is still a lack of large-scale datasets of underwater organism images with accurate annotations. Second, in consideration of the limitation on hardware resources of underwater devices, an underwater organism detection algorithm that is both accurate and lightweight enough to be able to infer in real time is still lacking. As an attempt to fill in the aforementioned research gaps to some extent, we established the Multiple Kinds of Underwater Organisms (MKUO) dataset with accurate bounding box annotations of taxonomic information, which consists of 10,043 annotated images, covering eighty-four underwater organism categories. Based on our benchmark dataset, we evaluated a series of existing object detection algorithms to obtain their accuracy and complexity indicators as the baseline for future reference. In addition, we also propose a novel lightweight module, namely Sparse Ghost Module, designed especially for object detection networks. By substituting the standard convolution with our proposed one, the network complexity can be significantly reduced and the inference speed can be greatly improved without obvious detection accuracy loss. To make our results reproducible, the dataset and the source code are available online at https://cslinzhang.github.io/MKUO-and-Sparse-Ghost-Module/.
C1 [Huang, Jiafeng; Zhang, Tianjun; Zhao, Shengjie; Zhang, Lin] Tongji Univ, Sch Soft ware Engn, Shanghai, Peoples R China.
   [Zhou, Yicong] Univ Macau, Dept Comp & Informat Sci, Macau, Peoples R China.
   [Huang, Jiafeng; Zhang, Tianjun; Zhao, Shengjie; Zhang, Lin] Tongji Univ, Sch Software Engn, 4800 CaoAn Rd, Shanghai 200092, Peoples R China.
   [Zhou, Yicong] Univ Macau, Dept Comp & Informat Sci, Taipa Univ Rd, Macau 999078, Peoples R China.
C3 Tongji University; University of Macau; Tongji University; University of
   Macau
RP Zhao, SJ; Zhang, L (corresponding author), Tongji Univ, Sch Software Engn, 4800 CaoAn Rd, Shanghai 200092, Peoples R China.
EM 2010195@tongji.edu.cn; 1911036@tongji.edu.cn; shengjiezhao@tongji.edu;
   cslinzhang@tongji.edu.cn; yicongzhou@um.edu.mo
RI Zhou, Yicong/A-8017-2009
OI Zhou, Yicong/0000-0002-4487-6384; Zhang, Lin/0000-0002-4360-5523; Huang,
   jiafeng/0009-0002-0451-1628; Zhao, Shengjie/0000-0002-4301-394X
FU National Natural Science Foundation of China [61936014, 62272343,
   61973235]; Shanghai Science and Technology Innovation Plan
   [20510760400]; Shuguang Program of Shanghai Education Development
   Foundation; Shanghai Municipal Education Commission [21SG23];
   Fundamental Research Funds for the Central Universities
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61936014, Grant 62272343, and Grant
   61973235; in part by the Shanghai Science and Technology Innovation Plan
   under Grant 20510760400; in part by the Shuguang Program of Shanghai
   Education Development Foundation and Shanghai Municipal Education
   Commission under Grant 21SG23; and in part by the Fundamental Research
   Funds for the Central Universities.
CR Anantharajah K, 2014, IEEE WINT CONF APPL, P309, DOI 10.1109/WACV.2014.6836084
   APPENZELLER AR, 1992, CAN J FISH AQUAT SCI, V49, P2179, DOI 10.1139/f92-240
   Australian Centre for Field Robotics, 2022, Tasmania Coral Point Count
   Beijbom O, 2016, SCI REP-UK, V6, DOI 10.1038/srep23166
   Beijbom O, 2012, PROC CVPR IEEE, P1170, DOI 10.1109/CVPR.2012.6247798
   Boom BJ, 2012, INT C PATT RECOG, P1542
   Cai KW, 2020, AQUACULT ENG, V91, DOI 10.1016/j.aquaeng.2020.102117
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Cutter G, 2015, 2015 IEEE WINTER APPLICATIONS AND COMPUTER VISION WORKSHOPS (WACVW), P57, DOI 10.1109/WACVW.2015.11
   Dawkins M, 2013, IEEE WORK APP COMP, P160, DOI 10.1109/WACV.2013.6475014
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Feng CJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3490, DOI 10.1109/ICCV48922.2021.00349
   Howard AG, 2017, Arxiv, DOI [arXiv:1704.04861, 10.48550/arXiv.1704.04861]
   Ge Z, 2021, Arxiv, DOI arXiv:2107.08430
   Han K, 2020, PROC CVPR IEEE, P1577, DOI 10.1109/CVPR42600.2020.00165
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hongkai Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P260, DOI 10.1007/978-3-030-58555-6_16
   JAdger J., 2015, BRIT MACHINE VISION
   Jiaqi Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P403, DOI 10.1007/978-3-030-58548-8_24
   Joly A., 2014, LNCS, P229, DOI DOI 10.1007/978-3-319-11382-120
   Joly A, 2015, LECT NOTES COMPUT SC, V9283, P462, DOI 10.1007/978-3-319-24027-5_46
   Kang Kim, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P355, DOI 10.1007/978-3-030-58595-2_22
   Kong T, 2020, IEEE T IMAGE PROCESS, V29, P7389, DOI 10.1109/TIP.2020.3002345
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li YS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P458, DOI 10.1109/ICCV48922.2021.00052
   Lin J., 2020, PROC ADV NEURAL IN, V33, P11711
   Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Linnaeus C., 1753, SPECIES PLANTARUM, VI, DOI DOI 10.5962/BHL.TITLE.669
   Liu W., 2017, INT C LEARNING REPRE, P1
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Lyu R., 2021, NanoDet-Plus
   Mahmood A, 2016, OCEANS 2016 MTS/IEEE MONTEREY, DOI 10.1109/OCEANS.2016.7761105
   MISUND OA, 1995, ICES J MAR SCI, V52, P11, DOI 10.1016/1054-3139(95)80011-5
   National Oceanic and Atmospheric Administration, 2021, How much of the ocean have we explored?
   Northeast Fisheries Science Center, 2022, Habitat mapping camera (HABCAM)
   OpenAI, 2020, GPT-3: Language Models are Few-Shot Learners
   Ovchinnikova K, 2021, ECOL INFORM, V62, DOI 10.1016/j.ecoinf.2021.101233
   Pedersen M., 2019, P IEEE CVF C COMP VI, P18
   Redmon J., 2018, CoRR
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Salman A, 2016, LIMNOL OCEANOGR-METH, V14, P570, DOI 10.1002/lom3.10113
   Siddiqui SA, 2018, ICES J MAR SCI, V75, P374, DOI 10.1093/icesjms/fsx109
   Sun PZ, 2021, PROC CVPR IEEE, P14449, DOI 10.1109/CVPR46437.2021.01422
   Ultralytics, 2021, YOLOv5
   Van Horn G, 2018, PROC CVPR IEEE, P8769, DOI 10.1109/CVPR.2018.00914
   Villon S, 2016, LECT NOTES COMPUT SC, V10016, P160, DOI 10.1007/978-3-319-48680-2_15
   Wang CY, 2020, IEEE COMPUT SOC CONF, P1571, DOI 10.1109/CVPRW50498.2020.00203
   Wulandari N., 2022, Journal RESTI (Rekayasa Sistem Dan Teknologi Informasi), V6, P252
   Yue Wu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10183, DOI 10.1109/CVPR42600.2020.01020
   Zhang H., 2023, INT C LEARNING REPRE, P1
   Zhang HY, 2021, PROC CVPR IEEE, P8510, DOI 10.1109/CVPR46437.2021.00841
   Zhang S., 2020, P IEEE CVF C COMP VI, P9759
   Zhang SL, 2023, PROC CVPR IEEE, P7329, DOI 10.1109/CVPR52729.2023.00708
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhuang PQ, 2021, IEEE T MULTIMEDIA, V23, P3603, DOI 10.1109/TMM.2020.3028482
   Zhuang PQ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1301, DOI 10.1145/3240508.3240616
   Zwolinski J, 2009, CAN J FISH AQUAT SCI, V66, P2081, DOI 10.1139/F09-138
NR 59
TC 0
Z9 0
U1 9
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2024
VL 20
IS 5
AR 147
DI 10.1145/3640465
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MF3R9
UT WOS:001192177900027
DA 2024-08-05
ER

PT J
AU Clement, N
   Schoen, A
   Boedihardjo, A
   Jenkins, A
AF Clement, Nathan
   Schoen, Alan
   Boedihardjo, Arnold
   Jenkins, Andrew
TI Synthetic Data and Hierarchical Object Detection in Overhead Imagery
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Deep learning; object detection; remote sensing; synthetic data;
   low-shot learning
AB The performance of neural network models is often limited by the availability of big datasets. To treat this problem, we survey and develop novel synthetic data generation and augmentation techniques for enhancing low/zero-sample learning in satellite imagery. In addition to extending synthetic data generation approaches, we propose a hierarchical detection architecture to improve the utility of synthetic training samples. We consider existing techniques for producing synthetic imagery-3D models and neural style transfer-as well as introducing our own adversarially trained reskinning network, the GAN-Reskinner, to blend 3D models. Additionally, we test the value of synthetic data in a two-stage, hierarchical detection/classification model of our own construction. To test the effectiveness of synthetic imagery, we employ it in the training of detection models and our two stage model, and evaluate the resulting models on real satellite images. All modalities of synthetic data are tested extensively on practical, geospatial analysis problems. Our experiments show that synthetic data developed using our approach can often enhance detection performance, particularly when combined with some real training images. When the only source of data is synthetic, our GAN-Reskinner often boosts performance over conventionally rendered 3D models and in all cases, the hierarchical model outperforms the baseline end-to-end detection architecture.
C1 [Clement, Nathan; Schoen, Alan; Boedihardjo, Arnold; Jenkins, Andrew] Maxar Technol, 2325 Dulles Corner Blvd,Suite 1000, Herndon, VA 20171 USA.
RP Clement, N (corresponding author), Maxar Technol, 2325 Dulles Corner Blvd,Suite 1000, Herndon, VA 20171 USA.
EM nathan.clement@maxar.com; alan.schoen@maxar.com;
   arnold.boedihardjo@maxar.com; andrew.jenkins@maxar.com
CR Borrego J, 2018, Arxiv, DOI arXiv:1807.09834
   Bowles C, 2018, Arxiv, DOI arXiv:1810.10863
   Frid-Adar M, 2018, I S BIOMED IMAGING, P289, DOI 10.1109/ISBI.2018.8363576
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Gidaris S, 2018, PROC CVPR IEEE, P4367, DOI 10.1109/CVPR.2018.00459
   Goodenough AA, 2017, IEEE J-STARS, V10, P4818, DOI 10.1109/JSTARS.2017.2758964
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Hana Sanghui, 2017, P AUT TARG REC 27 IN, V10202
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Howe J, 2019, PROC SPIE, V11139, DOI 10.1117/12.2529586
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Ju C, 2018, J APPL STAT, V45, P2800, DOI 10.1080/02664763.2018.1441383
   Karras T., 2018, INT C LEARNING REPRE
   Lakshminarayanan B, 2017, ADV NEUR IN, V30
   Lam D., 2018, arXiv
   Marcel S, 2010, P 18 ACM INT C MULTI, P1485, DOI DOI 10.1145/1873951.1874254
   Ouyang X, 2018, Arxiv, DOI arXiv:1804.02047
   Prakash A, 2019, IEEE INT CONF ROBOT, P7249, DOI [10.1109/icra.2019.8794443, 10.1109/ICRA.2019.8794443]
   Qi H, 2018, PROC CVPR IEEE, P5822, DOI 10.1109/CVPR.2018.00610
   Radford L., 2016, Unsupervised representation learning with deep convolutional generative adversarial networks, P1
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sankaranarayanan S, 2018, PROC CVPR IEEE, P3752, DOI 10.1109/CVPR.2018.00395
   Seo J, 2018, Arxiv, DOI arXiv:1806.03002
   Shrivastava A, 2017, PROC CVPR IEEE, P2242, DOI 10.1109/CVPR.2017.241
   Guibas JT, 2018, Arxiv, DOI arXiv:1709.01872
   Tobin J, 2017, IEEE INT C INT ROBOT, P23
   Tremblay J, 2018, IEEE COMPUT SOC CONF, P1082, DOI 10.1109/CVPRW.2018.00143
   Yosinski J, 2014, ADV NEUR IN, V27
NR 29
TC 1
Z9 1
U1 6
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2024
VL 20
IS 4
AR 117
DI 10.1145/3635309
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6G9
UT WOS:001153382100027
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Fu, FY
   Fang, SC
   Chen, WD
   Mao, ZD
AF Fu, Fengyi
   Fang, Shancheng
   Chen, Weidong
   Mao, Zhendong
TI Sentiment-Oriented Transformer-Based Variational Autoencoder Network for
   Live Video Commenting
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Automatic live video commenting; multi-modal learning; variational
   autoencoder; batch attention mechanism
AB Automatic live video commenting is getting increasing attention due to its significance in narration generation, topic explanation, etc. However, the diverse sentiment consideration of the generated comments is missing from current methods. Sentimental factors are critical in interactive commenting, and there has been lack of research so far. Thus, in this article, we propose a Sentiment-oriented Transformer-based Variational Autoencoder (So-TVAE) network, which consists of a sentiment-oriented diversity encoder module and a batch attention module, to achieve diverse video commenting with multiple sentiments and multiple semantics. Specifically, our sentiment-oriented diversity encoder elegantly combines a VAE and random mask mechanism to achieve semantic diversity under sentiment guidance, which is then fused with cross-modal features to generate live video comments. A batch attention module is also proposed in this article to alleviate the problem of missing sentimental samples, caused by the data imbalance that is common in live videos as the popularity of videos varies. Extensive experiments on Livebot and VideoIC datasets demonstrate that the proposed So-TVAE outperforms the state-of-the-art methods in terms of the quality and diversity of generated comments. Related code is available at https://github.com/fufy1024/So-TVAE.
C1 [Fu, Fengyi; Fang, Shancheng; Chen, Weidong; Mao, Zhendong] Univ Sci & Technol China, 100 Fuxing Rd, Hefei 230000, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Fang, SC (corresponding author), Univ Sci & Technol China, 100 Fuxing Rd, Hefei 230000, Anhui, Peoples R China.
EM ff142536f@mail.ustc.edu.cn; fangsc@ustc.edu.cn; chenweidong@ustc.edu.cn;
   zdmao@ustc.edu.cn
OI Chen, Weidong/0000-0003-2774-2875
FU National Science Fund for Excellent Young Scholars [62222212]; National
   Natural Science Foundation of China [62102384, 62302474]
FX This work is supported by the National Science Fund for Excellent Young
   Scholars under grant no. 62222212 and the National Natural Science
   Foundation of China under grant nos. 62102384 and 62302474.
CR Aafaq N, 2019, PROC CVPR IEEE, P12479, DOI 10.1109/CVPR.2019.01277
   Aafaq N, 2020, ACM COMPUT SURV, V52, DOI 10.1145/3355390
   Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Bai QC, 2019, IEEE ACCESS, V7, P53509, DOI 10.1109/ACCESS.2019.2909054
   Banerjee Satanjeev, 2007, METEOR: An automatic metric for MT evaluation with Long short-term memory improved correlation with human judgments, P65
   Bengio S, 2015, ADV NEUR IN, V28
   Bowman S. R., 2015, Computer Science, V2015
   Boxiao Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10867, DOI 10.1109/CVPR42600.2020.01088
   Cao S, 2022, IEEE T CIRC SYST VID, V32, P7005, DOI 10.1109/TCSVT.2022.3178844
   Chen JW, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3539225
   Chen KZ, 2021, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, EMNLP 2021, P4456
   Chen TL, 2018, LECT NOTES COMPUT SC, V11214, P527, DOI 10.1007/978-3-030-01249-6_32
   Chen WD, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3514250
   Chen X, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P315, DOI 10.1145/3077136.3080776
   Cheng Q, arXiv
   Collobert R, 2019, PR MACH LEARN RES, V97
   Cornia M, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3177745
   Cui Y, 2019, PROC CVPR IEEE, P9260, DOI 10.1109/CVPR.2019.00949
   [赵增顺 Zhao Zengshun], 2018, [小型微型计算机系统, Journal of Chinese Computer Systems], V39, P2602
   Devlin J, 2018, ARXIV
   Dong SS, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3550276
   Duan Chaoqun, 2020, Multimodal matching transformer for live commenting
   Farhadi A, 2010, LECT NOTES COMPUT SC, V6314, P15, DOI 10.1007/978-3-642-15561-1_2
   Gan C, 2017, PROC CVPR IEEE, P955, DOI 10.1109/CVPR.2017.108
   Gao JL, 2019, PROC CVPR IEEE, P6293, DOI 10.1109/CVPR.2019.00646
   Gong YC, 2014, LECT NOTES COMPUT SC, V8692, P529, DOI 10.1007/978-3-319-10593-2_35
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Guo LT, 2019, PROC CVPR IEEE, P4199, DOI 10.1109/CVPR.2019.00433
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hodosh M, 2013, J ARTIF INTELL RES, V47, P853, DOI 10.1613/jair.3994
   Hou RB, 2019, ADV NEUR IN, V32
   Hou Z, 2022, PROC CVPR IEEE, P7246, DOI 10.1109/CVPR52688.2022.00711
   Hu ZT, 2017, P MACHINE LEARNING R, V70
   Huang L, 2019, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2019.00473
   Jiang WT, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3460474
   Kim W, 2022, NAACL 2022: THE 2022 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES, P4118
   Kingma D. P., 2014, arXiv
   Kingma DP, 2014, ADV NEUR IN, V27
   Kiros R, 2014, PR MACH LEARN RES, V32, P595
   Kojima A, 2002, INT J COMPUT VISION, V50, P171, DOI 10.1023/A:1020346032608
   Krishnamoorthy N., 2013, Generating naturallanguage video descriptions using text-mined knowledge
   Kulkarni G, 2013, IEEE T PATTERN ANAL, V35, P2891, DOI 10.1109/TPAMI.2012.162
   Kumar S, 2004, HLT-NAACL 2004: HUMAN LANGUAGE TECHNOLOGY CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE MAIN CONFERENCE, P169
   Lachaux Marie-Anne, 2020, C EMPIRICAL METHODS
   Li CC, 2020, IEEE T MULTIMEDIA, V22, P1634, DOI 10.1109/TMM.2019.2946477
   Li S., 2011, Composing simple image descriptions using web-scale n-grams
   Li SM, 2022, AAAI CONF ARTIF INTE, P11002
   Li W, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P4843
   Li YH, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P757, DOI 10.1145/2964284.2973835
   Liao SC, 2022, PROC CVPR IEEE, P7349, DOI 10.1109/CVPR52688.2022.00721
   Lim J, 2022, PROC CVPR IEEE, P212, DOI 10.1109/CVPR52688.2022.00031
   Lin ZJ, 2019, INT CONF ACOUST SPEE, P7225, DOI 10.1109/ICASSP.2019.8682945
   Liu SQ, 2017, IEEE I CONF COMP VIS, P873, DOI 10.1109/ICCV.2017.100
   Liu XH, 2018, LECT NOTES COMPUT SC, V11219, P353, DOI 10.1007/978-3-030-01267-0_21
   Lu HM, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3422668
   Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345
   Lv GY, 2022, IEEE T BIG DATA, V8, P535, DOI 10.1109/TBDATA.2019.2950411
   Lv Guangyi, 2016, Reading the Videos: Temporal Labeling for Crowdsourced Time-Sync Videos Based on Semantic Embedding
   Ma SM, 2019, AAAI CONF ARTIF INTE, P6810
   Man X, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3503927
   Mathews A, 2016, AAAI CONF ARTIF INTE, P3574
   Naeem MF, 2021, PROC CVPR IEEE, P953, DOI 10.1109/CVPR46437.2021.00101
   Qi MS, 2020, IEEE T CIRC SYST VID, V30, P2617, DOI 10.1109/TCSVT.2019.2921655
   Qin LH, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 2, P151
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278
   Sabour S, 2022, AAAI CONF ARTIF INTE, P11229
   Shao Huajie, 2020, INT C MACHINE LEARNI
   Shen TX, 2019, PR MACH LEARN RES, V97
   Shi Wenxian, 2020, INT C MACHINE LEARNI, P8840
   Shi YY, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3546828
   Sun C, 2015, IEEE I CONF COMP VIS, P2596, DOI 10.1109/ICCV.2015.298
   Tan Ganchao, 2020, P 29 INT JOINT C ART
   Tian H, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P4067
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Venugopalan S., 2014, Computer Science, V2014
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wane XY, 2021, 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1 (ACL-IJCNLP 2021), P2364
   Wang BR, 2018, PROC CVPR IEEE, P7622, DOI 10.1109/CVPR.2018.00795
   Wang C, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3115432
   Wang L., 2017, ARXIV171107068
   Wang T, 2021, IEEE T CIRC SYST VID, V31, P1890, DOI 10.1109/TCSVT.2020.3014606
   Wang WY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2599, DOI 10.1145/3394171.3413890
   Wang Wenlin, 2019, CoRR
   Wang XD, 2019, IEEE T CIRC SYST VID, V29, P3454, DOI 10.1109/TCSVT.2018.2877694
   Wei HY, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3439734
   Wen TH, 2017, PR MACH LEARN RES, V70
   Wu Hao, 2021, INT C MULTIMODAL INT
   Wu L, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1431
   Xu J, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P537, DOI 10.1145/3123266.3123448
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yan HQ, 2021, 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (ACL-IJCNLP 2021), VOL 1, P3364
   Yang L, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3386725
   Yang PC, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P2680
   Yang WD, 2019, ACM T KNOWL DISCOV D, V13, DOI 10.1145/3363575
   Yang W, 2019, IEEE INT CON MULTI, P454, DOI 10.1109/ICME.2019.00085
   Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512
   Yu HY, 2018, PROC CVPR IEEE, P6006, DOI 10.1109/CVPR.2018.00629
   Yu J, 2020, IEEE T CIRC SYST VID, V30, P4467, DOI 10.1109/TCSVT.2019.2947482
   Yuan J, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3394955
   Yuan MQ, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3532627
   Zeng WH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019:): STUDENT RESEARCH WORKSHOP, P229
   Zeng ZH, 2021, LECT NOTES ARTIF INT, V12713, P690, DOI 10.1007/978-3-030-75765-6_55
   Zhang ZW, 2020, IEEE T CIRC SYST VID, V30, P3130, DOI 10.1109/TCSVT.2019.2936526
   Zhao Tiancheng., 2017, Association for Computational Linguistics
   Zhao Tiancheng, 2018, Association for Computational Linguistics
   Zhao WT, 2020, AAAI CONF ARTIF INTE, V34, P12984
   Zhao ZS, 2019, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-019-0465-0
   Zhihan Zhang, 2020, Natural Language Processing and Chinese Computing. 9th CCF International Conference, NLPCC 2020. Proceedings. Lecture Notes in Artificial Intelligence Subseries of Lecture Notes in Computer Science (LNAI 12431), P3, DOI 10.1007/978-3-030-60457-8_1
   Zhou CT, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P310, DOI 10.18653/v1/P17-1029
   Zhu QL, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P2636
   Zhu YM, 2018, ACM/SIGIR PROCEEDINGS 2018, P1097, DOI 10.1145/3209978.3210080
   Zhu YZ, 2018, PROC CVPR IEEE, P1004, DOI 10.1109/CVPR.2018.00111
   Ziqi Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13275, DOI 10.1109/CVPR42600.2020.01329
NR 117
TC 0
Z9 0
U1 8
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2024
VL 20
IS 4
AR 104
DI 10.1145/3633334
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6G9
UT WOS:001153382100014
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Shi, PA
   Hu, M
   Shi, XF
   Ren, FJ
AF Shi, Piao
   Hu, Min
   Shi, Xuefeng
   Ren, Fuji
TI Deep Modular Co-Attention Shifting Network for Multimodal Sentiment
   Analysis
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Multimodal sentiment analysis; co-attention; cross-modal modulation;
   fine-tuning; shifted representations; modality-mixing adaptation gate
AB Human Multimodal Sentiment Analysis (MSA) is an attractive research that studies sentiment expressed from multiple heterogeneous modalities. While transformer-based methods have achieved great success, designing an effective "co-attention" model to associate text modality with nonverbal modalities remains challenging. There are two main problems: 1) the dominant role of the text in modalities is underutilization, and 2) the interaction between modalities is not sufficiently explored. This paper proposes a deep modular Co-Attention Shifting Network (CoASN) for MSA. A Cross-modal Modulation Module based on Co-attention (CMMC) and an Advanced Modality-mixing Adaptation Gate (AMAG) are constructed. The CMMC consists of the Text-guided Co-Attention (TCA) and Interior Transformer Encoder (ITE) units to capture inter-modal features and intra-modal features. With text modality as the core, the CMMC module aims to guide and promote the expression of emotion in nonverbal modalities, and the nonverbal modalities increase the richness of the text-based multimodal sentiment information. In addition, the AMAG module is introduced to explore the dynamical correlations among all modalities. Particularly, this efficient module first captures the nonverbal shifted representations and then combines them to calculate the shifted word embedding representations for the final MSA tasks. Extensive experiments on two commonly used datasets, CMU-MOSI and CMU-MOSEI, demonstrate that our proposed method is superior to the state-of-the-art performance.
C1 [Shi, Piao; Hu, Min; Shi, Xuefeng] Hefei Univ Technol, Sch Comp Sci & Informat Engn, Natl Smart Eldercare Int Sci & Technol Cooperat B, Minist Educ,Anhui Prov Key Lab Affect Comp & Adv, 485 Danxia Rd, Hefei 230601, Anhui, Peoples R China.
   [Ren, Fuji] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, 2006,Xiyuan Ave,West Hi tech Zone, Chengdu 611731, Sichuan, Peoples R China.
C3 Hefei University of Technology; University of Electronic Science &
   Technology of China
RP Hu, M (corresponding author), Hefei Univ Technol, Sch Comp Sci & Informat Engn, Natl Smart Eldercare Int Sci & Technol Cooperat B, Minist Educ,Anhui Prov Key Lab Affect Comp & Adv, 485 Danxia Rd, Hefei 230601, Anhui, Peoples R China.; Ren, FJ (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, 2006,Xiyuan Ave,West Hi tech Zone, Chengdu 611731, Sichuan, Peoples R China.
EM shipiao@mail.hfut.edu.cn; jsjxhumin@hfut.edu.cn;
   2020010107@mail.hfut.edu.cn; renfuji@uestc.edu.cn
OI Shi, Piao/0000-0002-0783-5487
FU National Natural Science Foundation of China [62176084, 62176083];
   Fundamental Research Funds for the Central Universities of China
   [PA2021GDSK0092, PA2022GDSK0066]; Provincial Natural Science Research
   Project [KJ2020A0773]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62176084, and Grant 62176083, and in
   part by the Fundamental Research Funds for the Central Universities of
   China under Grant PA2021GDSK0092 and PA2022GDSK0066, and in part by the
   Provincial Natural Science Research Project KJ2020A0773.
CR Arjmand M, 2021, Arxiv, DOI arXiv:2109.05522
   Birjali M, 2021, KNOWL-BASED SYST, V226, DOI 10.1016/j.knosys.2021.107134
   Brown T., 2020, ADV NEURAL INFORM PR, V33, P1877, DOI DOI 10.48550/ARXIV.2005.14165
   Caliendo M, 2023, ENTREP THEORY PRACT, V47, P788, DOI 10.1177/10422587221102106
   Chen M., 2020, P 28 INT C COMP LING, P1067
   Chen M., 2017, P 19 ACM INT C MULT, P163, DOI [10.1145/3136755.3136801, DOI 10.1145/3136755.3136801]
   Cheng Hongju, 2023, IEEE Transactions on Affective Computing
   Clark K, 2020, Arxiv, DOI [arXiv:2003.10555, DOI 10.48550/ARXIV.2003.10555]
   Degottex G, 2014, INT CONF ACOUST SPEE, DOI 10.1109/ICASSP.2014.6853739
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Nguyen DK, 2018, PROC CVPR IEEE, P6087, DOI 10.1109/CVPR.2018.00637
   Fang LY, 2022, IEEE IJCNN, DOI 10.1109/IJCNN55064.2022.9892116
   Gandhi A, 2023, INFORM FUSION, V91, P424, DOI 10.1016/j.inffus.2022.09.025
   Guo JW, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P3394, DOI 10.1145/3503161.3548137
   Hasan MK, 2023, Arxiv, DOI arXiv:2303.15430
   Hazarika D, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1122, DOI 10.1145/3394171.3413678
   He JX, 2022, IEEE SIGNAL PROC LET, V29, P454, DOI 10.1109/LSP.2021.3139856
   He J, 2022, COMPUT INTEL NEUROSC, V2022, DOI 10.1155/2022/2105593
   Huang CQ, 2023, KNOWL-BASED SYST, V269, DOI 10.1016/j.knosys.2023.110502
   Huddar M.G., 2019, International Journal of Computer Sciences and Engineering, V7, P876
   iMotions, 2017, FACIAL EXPRESSION AN
   Joshi M, 2020, T ASSOC COMPUT LING, V8, P64, DOI 10.1162/tacl_a_00300
   Liang B, 2023, ACM T INFORM SYST, V41, DOI 10.1145/3529954
   Liu F, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P869
   Liu YH, 2019, Arxiv, DOI [arXiv:1907.11692, DOI 10.48550/ARXIV.1907.11692]
   Liu Z, 2018, Arxiv, DOI [arXiv:1806.00064, 10.48550/arXiv.1806.00064]
   Lu JS, 2016, ADV NEUR IN, V29
   Ma LY, 2022, Arxiv, DOI arXiv:2206.07981
   Mai SJ, 2023, IEEE T AFFECT COMPUT, V14, P2276, DOI 10.1109/TAFFC.2022.3172360
   MEHRABIAN A, 1967, J PERS SOC PSYCHOL, V6, P109, DOI 10.1037/h0024532
   MEHRABIAN A, 1967, J CONSULT PSYCHOL, V31, P248, DOI 10.1037/h0024648
   Morency L.-P., 2011, P 13 INT C MULT INT, P169, DOI [DOI 10.1145/2070481.2070509, 10.1145/2070481.2070509]
   Nam H, 2017, PROC CVPR IEEE, P2156, DOI 10.1109/CVPR.2017.232
   Park N., 2022, arXiv, DOI DOI 10.48550/ARXIV.2202.06709
   Qian F, 2022, INTERSPEECH, P1973, DOI 10.21437/Interspeech.2022-532
   Radford A., 2018, Improving language understanding by generative pre-training, P850
   Radford A., 2019, OpenAI blog, V1, P9
   Rahman W, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P2359, DOI 10.18653/v1/2020.acl-main.214
   Ren FJ, 2016, IEEE T HUM-MACH SYST, V46, P810, DOI 10.1109/THMS.2016.2599495
   Shi PA, 2022, J ELECTRON IMAGING, V31, DOI 10.1117/1.JEI.31.4.043056
   Shi Piao, 2022, Journal of Electronic Imaging, V2022
   Shi XF, 2022, SYMMETRY-BASEL, V14, DOI 10.3390/sym14081698
   Soleymani M, 2017, IMAGE VISION COMPUT, V65, P3, DOI 10.1016/j.imavis.2017.08.003
   Sun ZK, 2020, AAAI CONF ARTIF INTE, V34, P8992
   Tang JJ, 2023, IEEE T CIRC SYST VID, V33, P1966, DOI 10.1109/TCSVT.2022.3218018
   Tashu TM, 2021, J IMAGING, V7, DOI 10.3390/jimaging7080157
   Tsai YHH, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P1823, DOI 10.18653/v1/2020.emnlp-main.143
   Tsai YHH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6558, DOI 10.18653/v1/p19-1656
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang J, 2023, ACM T SENSOR NETWORK, V19, DOI 10.1145/3563776
   Wang YS, 2019, AAAI CONF ARTIF INTE, P7216
   Wei Han, 2021, ICMI '21: Proceedings of the 2021 International Conference on Multimodal Interaction, P6, DOI 10.1145/3462244.3479919
   Wu J, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3566126
   Xue XJ, 2023, IEEE T KNOWL DATA EN, V35, P5105, DOI 10.1109/TKDE.2022.3155290
   Yadav A, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3517139
   Yang Bo, 2022, IEEE/ACM Transactions on Audio, Speech, and Language Processing
   Yang KC, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P521, DOI 10.1145/3394171.3413690
   Yang ZL, 2019, ADV NEUR IN, V32
   Yu Z, 2018, IEEE T NEUR NET LEAR, V29, P5947, DOI 10.1109/TNNLS.2018.2817340
   Yu Z, 2019, PROC CVPR IEEE, P6274, DOI 10.1109/CVPR.2019.00644
   Zadeh A, 2017, Arxiv, DOI arXiv:1707.07250
   Zadeh A, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2236
   Zadeh A, 2018, AAAI CONF ARTIF INTE, P5634
   Zadeh A, 2016, IEEE INTELL SYST, V31, P82, DOI 10.1109/MIS.2016.94
   Zhang S, 2023, IEEE T EM TOP COMP I, V7, P200, DOI 10.1109/TETCI.2022.3224929
   Zhao JM, 2022, INT CONF ACOUST SPEE, P4703, DOI 10.1109/ICASSP43922.2022.9746910
   Zhu LA, 2023, INFORM FUSION, V95, P306, DOI 10.1016/j.inffus.2023.02.028
   Zou HQ, 2022, INT CONF ACOUST SPEE, P7367, DOI 10.1109/ICASSP43922.2022.9747095
   Zou Wenwen, 2022, 2022 IEEE INT C MULT, P1
NR 70
TC 1
Z9 1
U1 32
U2 32
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2024
VL 20
IS 4
AR 109
DI 10.1145/3634706
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6G9
UT WOS:001153382100019
DA 2024-08-05
ER

PT J
AU Han, N
   Zeng, YW
   Shi, CH
   Xiao, GY
   Chen, H
   Chen, JJ
AF Han, Ning
   Zeng, Yawen
   Shi, Chuhao
   Xiao, Guangyi
   Chen, Hao
   Chen, Jingjing
TI BiC-Net: Learning Efficient Spatio-temporal Relation for Text-Video
   Retrieval
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Text-video retrieval; spatio-temporal relation; bi-branch complementary
   network
ID IMAGE
AB The task of text-video retrieval aims to understand the correspondence between language and vision and has gained increasing attention in recent years. Recent works have demonstrated the superiority of local spatio-temporal relation learning with graph-based models. However, most existing graph-based models are handcrafted and depend heavily on expert knowledge and empirical feedback, which may be unable to mine the high-level fine-grained visual relations effectively. These limitations result in their inability to distinguish videos with the same visual components but different relations. To solve this problem, we propose a novel cross-modal retrieval framework, Bi-Branch Complementary Network (BiC-Net), which modifies Transformer architecture to effectively bridge text-video modalities in a complementary manner via combining local spatio-temporal relation and global temporal information. Specifically, local video representations are encoded using multiple Transformer blocks and additional residual blocks to learn fine-grained spatio-temporal relations and long-term temporal dependency, calling the module a Fine-grained Spatio-temporal Transformer (FST). Global video representations are encoded using a multi-layer Transformer block to learn global temporal features. Finally, we align the spatio-temporal relation and global temporal features with the text feature on two embedding spaces for cross-modal text-video retrieval. Extensive experiments are conducted on MSR-VTT, MSVD, and YouCook2 datasets. The results demonstrate the effectiveness of our proposed model. Our code is public at https://github.com/lionel-hing/BiC-Net.
C1 [Han, Ning] Xiangtan Univ, Sch Comp Sci, Xiangtan 411105, Peoples R China.
   [Zeng, Yawen] Bytedance AI Lab, 43 North Third Ring West Rd, Beijing 100098, Peoples R China.
   [Shi, Chuhao; Xiao, Guangyi; Chen, Hao] Hunan Univ, Coll Comp Sci & Elect Engn, 116 Lu Shan South Rd, Changsha 410082, Peoples R China.
   [Chen, Jingjing] Fudan Univ, Sch Comp Sci, 20 Handan Rd, Shanghai 200433, Peoples R China.
C3 Xiangtan University; Hunan University; Fudan University
RP Xiao, GY (corresponding author), Hunan Univ, Coll Comp Sci & Elect Engn, 116 Lu Shan South Rd, Changsha 410082, Peoples R China.
EM ninghan@xtu.edu.cn; yawenzeng11@gmail.com; sch8288@hnu.edu.cn;
   guangyi.xiao@gmail.com; chenhao@hnu.edu.cn; chenjingjing@fudan.edu.cn
RI Zhou, Xinyi/KGM-6689-2024; yi, cheng/KHC-5004-2024; li,
   fang/KDO-8841-2024; cheng, qian/KFB-6227-2024; Chen,
   Yukun/KGK-4521-2024; Wang, Yuhan/KGL-5855-2024; Wang,
   Yifan/KDO-8319-2024; Xiao, Guangyi/L-1618-2014
OI zeng, yawen/0000-0003-1908-1157; Xiao, Guangyi/0000-0003-0625-0804
FU National Natural Science Foundation of China [62272156]; Shanghai
   Pujiang Program [20PJ1401900]
FX This work is supported by the National Natural Science Foundation of
   China (grant no. 62272156) and the Shanghai Pujiang Program (grant no.
   20PJ1401900).
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI [10.1109/TPAMI.2017.2711011, 10.1109/CVPR.2016.572]
   Ba L.J., 2016, arXiv, DOI DOI 10.48550/ARXIV.1607.06450
   Bain M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1708, DOI 10.1109/ICCV48922.2021.00175
   Bertasius G, 2021, PR MACH LEARN RES, V139
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen David, 2011, ACL
   Chen JJ, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P32, DOI 10.1145/2964284.2964315
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dong JF, 2022, IEEE T PATTERN ANAL, V44, P4065, DOI 10.1109/TPAMI.2021.3059295
   Dong JF, 2019, PROC CVPR IEEE, P9338, DOI 10.1109/CVPR.2019.00957
   Dong JF, 2018, IEEE T MULTIMEDIA, V20, P3377, DOI 10.1109/TMM.2018.2832602
   Fang H., 2021, arXiv
   Feng ZR, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1005
   Gabeur Valentin, 2020, COMPUTER VISION ECCV, DOI [10.48550/arXiv.2007.10639, DOI 10.48550/ARXIV.2007.10639]
   Ging Simon, 2020, ADV NEURAL INFORM PR, P22605, DOI DOI 10.18653/V1/P19-1641
   Han N, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3826, DOI 10.1145/3474085.3475241
   Han N, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3483381
   Hao YB, 2022, PROC CVPR IEEE, P918, DOI 10.1109/CVPR52688.2022.00100
   Hendrycks D, 2020, Arxiv, DOI arXiv:1606.08415
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.2307/2333955
   Junbin Xiao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P447, DOI 10.1007/978-3-030-58539-6_27
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   Kingma D. P., 2014, arXiv
   Kipf T.N., 2017, INT C LEARN REPR, P1
   Lei J, 2021, PROC CVPR IEEE, P7327, DOI 10.1109/CVPR46437.2021.00725
   Li XR, 2021, IEEE T MULTIMEDIA, V23, P4351, DOI 10.1109/TMM.2020.3042067
   Liu S, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11895, DOI 10.1109/ICCV48922.2021.01170
   Liu XJ, 2023, IEEE T PATTERN ANAL, V45, P3003, DOI 10.1109/TPAMI.2022.3186410
   Liu Yang, 2019, ARXIV190713487
   Lu W, 2023, IEEE T MULTIMEDIA, V25, P77, DOI 10.1109/TMM.2021.3121567
   Luo HS, 2022, NEUROCOMPUTING, V508, P293, DOI 10.1016/j.neucom.2022.07.028
   Miech Antoine, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9876, DOI 10.1109/CVPR42600.2020.00990
   Miech A, 2020, Arxiv, DOI arXiv:1804.02516
   Miech A, 2019, IEEE I CONF COMP VIS, P2630, DOI 10.1109/ICCV.2019.00272
   Mithun NC, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P19, DOI 10.1145/3206025.3206064
   Patrick Mandela, 2021, INT C LEARNING REPRE, P1
   Perronnin F, 2007, PROC CVPR IEEE, P2272
   Qi MS, 2021, IEEE T IMAGE PROCESS, V30, P2989, DOI 10.1109/TIP.2020.3048680
   Qian XF, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P84, DOI 10.1145/3343031.3351058
   Radford A, 2021, PR MACH LEARN RES, V139
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rouditchenko A, 2021, INTERSPEECH, P1584, DOI 10.21437/Interspeech.2021-1312
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shizhe Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10635, DOI 10.1109/CVPR42600.2020.01065
   Song X, 2022, IEEE T MULTIMEDIA, V24, P2914, DOI 10.1109/TMM.2021.3090595
   Su ZX, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3127, DOI 10.1145/3394171.3413764
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Vaswani A, 2017, ADV NEUR IN, V30
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Wang H, 2023, IEEE T PATTERN ANAL, V45, P7711, DOI 10.1109/TPAMI.2022.3226328
   Wang H, 2021, PROC CVPR IEEE, P7022, DOI 10.1109/CVPR46437.2021.00695
   Wang H, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4116, DOI 10.1145/3394171.3413975
   Wang W, 2023, IEEE T MULTIMEDIA, V25, P2661, DOI 10.1109/TMM.2022.3149716
   Wang XH, 2021, PROC CVPR IEEE, P5075, DOI 10.1109/CVPR46437.2021.00504
   Wang XL, 2018, LECT NOTES COMPUT SC, V11209, P413, DOI 10.1007/978-3-030-01228-1_25
   Wray M, 2019, IEEE I CONF COMP VIS, P450, DOI 10.1109/ICCV.2019.00054
   Wu JC, 2019, PROC CVPR IEEE, P9956, DOI 10.1109/CVPR.2019.01020
   Wu P, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3518, DOI 10.1145/3474085.3475515
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Yang X, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P1339, DOI 10.1145/3397271.3401151
   Yang X, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3089249
   Yu Y, 2018, LECT NOTES COMPUT SC, V11211, P487, DOI 10.1007/978-3-030-01234-2_29
   Yutian Guo, 2020, ICMR '20: Proceedings of the 2020 International Conference on Multimedia Retrieval, P9, DOI 10.1145/3372278.3390709
   Zeng Yawen, 2023, SIGIR '23: Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, P2344, DOI 10.1145/3539618.3592054
   Zeng Y., 2023, PROC AAAI C ARTIF IN, P3376
   Zeng Yawen, 2023, IEEE Transactions on Multimedia, V2023, P1
   Zhang H, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P5773, DOI 10.1145/3503161.3547908
   Zhang H, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P917, DOI 10.1145/3474085.3475272
   Zhang YC, 2021, IEEE T MULTIMEDIA, V23, P2917, DOI 10.1109/TMM.2020.3019714
   Zhao R, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102913
   Zhou LW, 2018, AAAI CONF ARTIF INTE, P7590
   Zou Xiaofeng, 2020, IEEE Transactions on Industrial Informatics, V18, P448
NR 73
TC 0
Z9 0
U1 8
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2024
VL 20
IS 3
AR 86
DI 10.1145/3627103
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6F8
UT WOS:001153381000026
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Chen, XW
   Jiang, X
   Zhan, LS
   Guo, SH
   Ruan, QS
   Luo, GL
   Liao, MH
   Qin, YP
AF Chen, Xiaowei
   Jiang, Xiao
   Zhan, Lishuang
   Guo, Shihui
   Ruan, Qunsheng
   Luo, Guoliang
   Liao, Minghong
   Qin, Yipeng
TI Full-body Human Motion Reconstruction with Sparse Joint Tracking Using
   Flexible Sensors
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Flexible sensors; sparse signal processing; temporal convolutional
   network; median pooling
ID HUMAN POSE ESTIMATION; PERFORMANCE; NETWORK
AB Human motion tracking is a fundamental building block for various applications including computer animation, human-computer interaction, healthcare, and so on. To reduce the burden of wearing multiple sensors, human motion prediction from sparse sensor inputs has become a hot topic in human motion tracking. However, such predictions are non-trivial as (i) the widely adopted data-driven approaches can easily collapse to average poses, and (ii) the predicted motions contain unnatural jitters. In this work, we address the aforementioned issues by proposing a novel framework which can accurately predict the human joint moving angles from the signals of only four flexible sensors, thereby achieving the tracking of human joints in multidegrees of freedom. Specifically, we mitigate the collapse to average poses by implementing the model with a Bi-LSTM neural network that makes full use of short-time sequence information; we reduce jitters by adding a median pooling layer to the network, which smooths consecutive motions. Although being bio-compatible and ideal for improving the wearing experience, the flexible sensors are prone to aging which increases prediction errors. Observing that the aging of flexible sensors usually results in drifts of their resistance ranges, we further propose a novel dynamic calibration technique to rescale sensor ranges, which further improves the prediction accuracy. Experimental results show that our method achieves a low and stable tracking error of 4.51 degrees across different motion types with only four sensors.
C1 [Chen, Xiaowei; Jiang, Xiao; Zhan, Lishuang; Guo, Shihui; Ruan, Qunsheng; Liao, Minghong] Xiamen Univ, Sch Informat, Xiamen 361005, Fujian, Peoples R China.
   [Guo, Shihui] Xiamen Univ, Jiujiang Res Inst, Jiujiang 332105, Jiangxi, Peoples R China.
   [Luo, Guoliang] East China Jiao Tong Univ, Nanchang 330013, Jiangxi, Peoples R China.
   [Qin, Yipeng] Cardiff Univ, Sch Comp Sci & Informat, Cardiff, Wales.
C3 Xiamen University; Xiamen University; East China Jiaotong University;
   Cardiff University
RP Guo, SH (corresponding author), Xiamen Univ, Sch Informat, Xiamen 361005, Fujian, Peoples R China.; Guo, SH (corresponding author), Xiamen Univ, Jiujiang Res Inst, Jiujiang 332105, Jiangxi, Peoples R China.
EM guoshihui@xmu.edu.cn
OI Zhan, Lishuang/0000-0002-6008-1450; Jiang, Xiao/0000-0002-4525-435X;
   Chen, Xiaowei/0000-0003-0027-2420; Qin, Yipeng/0000-0002-1551-9126; ,
   ruan/0000-0002-6679-8655
FU National Natural Science Foundation of China [62072383, 61702433,
   62077039, 61962021]; Fundamental Research Funds for the Central
   Universities [20720210044, 20720190006]; Open Project Program of State
   Key Laboratory of Virtual Reality Technology and Systems, Beihang
   University [VRLAB2020B17]; Science and Technology Guiding Project of
   Fujian Province; Key Research Program of Jiangxi Province; Natural
   Science Foundation of Fujian Province of China [2021J011169,
   2020J01435]; Key Project of National Key RD Project [2017YFC1703303];
   Industry-University-Research Cooperation Project of Fujian Science and
   Technology Planning [2022H6012]; Xiamen University [2020C001]; China
   Scholarship Council (CSC) [202006310161]; Royal Society
   [IEC\NSFC\211022]; Industry-University-Research Cooperation Project of
   Ningde City [2020C001]
FX This work is supported by National Natural Science Foundation of China
   (62072383, 61702433, 62077039, 61962021), the Fundamental Research Funds
   for the Central Universities (20720210044, 20720190006), the Open
   Project Program of State Key Laboratory of Virtual Reality Technology
   and Systems, Beihang University (VRLAB2020B17), Science and Technology
   Guiding Project of Fujian Province, Key Research Program of Jiangxi
   Province, and Natural Science Foundation of Fujian Province of China
   (No. 2021J011169, No. 2020J01435), the Key Project of National Key R&D
   Project (No. 2017YFC1703303), Industry-University-Research Cooperation
   Project of Fujian Science and Technology Planning (No: 2022H6012),
   Industry-University-Research Cooperation Project of Ningde City and
   Xiamen University (No. 2020C001). Also thanks the China Scholarship
   Council (CSC) for providing financial support (program no.
   202006310161). This work is partially supported by Royal Society
   (IEC\NSFC\211022).
CR Andrews S., 2016, P 13 EUR C VIS MED P, P1
   Bouvier B, 2015, SENSORS-BASEL, V15, P18813, DOI 10.3390/s150818813
   Caeiro-Rodríguez M, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21082667
   Copaci D, 2017, APPL BIONICS BIOMECH, V2017, DOI 10.1155/2017/1605101
   Cucchiara R, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3476839
   FLASH T, 1985, J NEUROSCI, V5, P1688, DOI 10.1523/jneurosci.05-07-01688.1985
   Guidolin M, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON MECHATRONICS (ICM), DOI 10.1109/ICM46511.2021.9385684
   Guzov V, 2021, PROC CVPR IEEE, P4316, DOI 10.1109/CVPR46437.2021.00430
   Ha Sehoon, 2011, 2011 ACM SIGGRAPH EU, P129, DOI DOI 10.1145/2019406.2019424
   Hassan MM, 2019, INFORM FUSION, V51, P10, DOI 10.1016/j.inffus.2018.10.009
   Hooshiar A, 2022, IEEE-ASME T MECH, V27, P766, DOI 10.1109/TMECH.2021.3071295
   Huang YH, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275108
   Jiang YF, 2022, Arxiv, DOI [arXiv:2203.15720, DOI 10.48550/ARXIV.2203.15720]
   Katsuhara Y, 2019, UBICOMP/ISWC'19 ADJUNCT: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING AND PROCEEDINGS OF THE 2019 ACM INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS, P97, DOI 10.1145/3341162.3343776
   Kim D, 2019, IEEE-ASME T MECH, V24, P56, DOI 10.1109/TMECH.2018.2874647
   Kok M, 2018, Arxiv, DOI [arXiv:1704.06053, 10.48550/arXiv.1704.06053]
   LEE YH, 1985, IEEE T ACOUST SPEECH, V33, P672
   Li MY, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3377874
   Li Yanran, 2021, Ph. D. Dissertation.
   Marta G, 2020, IEEE T INSTRUM MEAS, V69, P1219, DOI 10.1109/TIM.2019.2911756
   Masdar A, 2013, BIOMED ENG INT CONF
   Matsumoto T, 2020, IEICE T INF SYST, VE103D, P1257, DOI 10.1587/transinf.2019MVP0007
   Mengüç Y, 2014, INT J ROBOT RES, V33, P1748, DOI 10.1177/0278364914543793
   Montoya MF, 2020, IEEE T NEUR SYS REH, V28, P740, DOI 10.1109/TNSRE.2020.2968869
   Mousas C, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17112589
   Park YL, 2012, APPL PHYS LETT, V101, DOI 10.1063/1.4767217
   Paszke A, 2019, ADV NEUR IN, V32
   Qiu S, 2022, INFORM FUSION, V80, P241, DOI 10.1016/j.inffus.2021.11.006
   Qiu S, 2022, INT J INTELL SYST, V37, P1646, DOI 10.1002/int.22689
   Rallis Ioannis, 2018, 2018 10 INT C VIRT W, P1
   Rietzler M, 2016, EICS'16: PROCEEDINGS OF THE 8TH ACM SIGCHI SYMPOSIUM ON ENGINEERING INTERACTIVE COMPUTING SYSTEMS, P73, DOI 10.1145/2933242.2933263
   Roetenberg D., 2009, Xsens Motion Technologies BV, Tech. Rep
   Santaera G, 2015, IEEE INT CONF ROBOT, P2728, DOI 10.1109/ICRA.2015.7139569
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Schwarz LA, 2009, LECT NOTES COMPUT SC, V5903, P159, DOI 10.1007/978-3-642-10470-1_14
   Tautges J, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1966394.1966397
   Vaswani A, 2017, ADV NEUR IN, V30
   von Marcard T, 2017, COMPUT GRAPH FORUM, V36, P349, DOI 10.1111/cgf.13131
   von Marcard T, 2018, LECT NOTES COMPUT SC, V11214, P614, DOI 10.1007/978-3-030-01249-6_37
   von Marcard T, 2016, IEEE T PATTERN ANAL, V38, P1533, DOI 10.1109/TPAMI.2016.2522398
   Wang W. W., 2011, 2011 IEEE INT S MED, P370, DOI [10.1109/MeMeA.2011.5966732, DOI 10.1109/MEMEA.2011.5966732]
   Wouda FJ, 2019, AAAI CONF ARTIF INTE, P10063
   Xia SH, 2017, J COMPUT SCI TECH-CH, V32, P536, DOI 10.1007/s11390-017-1742-y
   Yang D, 2021, COMPUT GRAPH FORUM, V40, P265, DOI 10.1111/cgf.142631
   Yi XY, 2022, PROC CVPR IEEE, P13157, DOI 10.1109/CVPR52688.2022.01282
   Yi XY, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459786
   You Q, 2020, CHIN CONTR CONF, P7481, DOI 10.23919/CCC50068.2020.9188413
   Yuan ZK, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1410, DOI 10.1145/3343031.3351079
   Zhang YH, 2016, IEEE-ASME T MECH, V21, P163, DOI 10.1109/TMECH.2015.2490118
   Zheng ZR, 2018, LECT NOTES COMPUT SC, V11213, P389, DOI 10.1007/978-3-030-01240-3_24
NR 50
TC 1
Z9 1
U1 15
U2 25
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2024
VL 20
IS 2
SI SI
AR 44
DI 10.1145/3564700
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W6GR4
UT WOS:001092595800014
OA Green Accepted
DA 2024-08-05
ER

PT J
AU Yaqoob, A
   Muntean, GM
AF Yaqoob, Abid
   Muntean, Gabriel-Miro
TI Advanced Predictive Tile Selection Using Dynamic Tiling for Prioritized
   360° Video VR Streaming
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE 360 degrees Video streaming; dynamic tiling; tiles selection; bitrate
   adaptation; QoE
ID SCHEME
AB The widespread availability of smart computing and display devices such as mobile phones, gaming consoles, laptops, and tethered/untethered head-mounted displays has fueled an increase in demand for omnidirectional (360 degrees) videos. 360 degrees video applications enable users to change their viewing angles while interacting with the video during playback. This allows users to have a more personalized and interactive viewing experience. Unfortunately, these applications require substantial network and computational resources that the conventional infrastructure and end devices cannot support. Recently proposed viewport adaptive fixed tiling solutions stream only relevant video tiles based on user interaction with the virtual reality (VR) space to use existing transmission resources more efficiently. However, achieving real-time accurate viewport extraction and transmission in response to both head movements and bandwidth dynamics can be challenging, which can impact the user's Quality of Experience (QoE). This article proposes innovative dynamic tiling-based adaptive 360 degrees video streaming solutions in order to achieve high viewer QoE. First, novel and easy-to-scale tiling layout selection methods are introduced, and the best tiling layouts are employed in each adaptation interval based on the prediction-assisted visual quality metric and the observed viewport divergence. Second, a novel proactive tile selection approach is presented, which adaptively extracts tiles for each selected tiling layout based on two low-complex viewport prediction mechanisms. Finally, a practical dynamic tile priorityoriented bitrate adaptation scheme is introduced, which uniformly distributes the bitrate budget among different tiles during 360 degrees video streaming. Extensive trace-driven experiments are conducted to evaluate the proposed solutions using head motion traces from 48 VR users for five 360 degrees videos with tiling layouts of 4 x 3, 6 x 4, and 8 x 6 and segment durations of 1s, 1.5s, and 2s. The experimental evaluations show that the dynamic video tiling solutions achieve up to 11.2% more viewport matches and an average improvement in QoE of 9.7% to 18% compared to state-of-the-art 360 degrees streaming approaches.
C1 [Yaqoob, Abid; Muntean, Gabriel-Miro] Dublin City Univ, Sch Elect Engn, Dublin D09 Y5N0, Ireland.
C3 Dublin City University
RP Yaqoob, A (corresponding author), Dublin City Univ, Sch Elect Engn, Dublin D09 Y5N0, Ireland.
EM abid.yaqoob2@mail.dcu.ie; gabriel.muntean@dcu.ie
OI Yaqoob, Abid/0000-0002-9541-4251
FU Science Foundation Ireland (SFI) [21/FFP-P/10244, 12/RC/2289_P2]
FX This work was supported by the Science Foundation Ireland (SFI) via the
   Frontiers Projects grant 21/FFP-P/10244 (FRADIS) and Research Centres
   grant 12/RC/2289_P2 (INSIGHT).
CR Anwar MS, 2020, ELECTRONICS-SWITZ, V9, DOI 10.3390/electronics9091530
   Azevedo RGD, 2020, IEEE T CIRC SYST VID, V30, P2524, DOI 10.1109/TCSVT.2019.2927344
   Bao YN, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA), P1161, DOI 10.1109/BigData.2016.7840720
   Ben Yahia M, 2018, IEEE INT SYM MULTIM, P89, DOI 10.1109/ISM.2018.00023
   Chen XL, 2021, CAAI T INTELL TECHNO, V6, P347, DOI 10.1049/cit2.12011
   Concolato C, 2018, IEEE T CIRC SYST VID, V28, P1981, DOI 10.1109/TCSVT.2017.2688491
   Corbillon X, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P199, DOI 10.1145/3083187.3083215
   Corbillon X, 2017, IEEE ICC, DOI 10.1109/ICC.2017.7996611
   Dong PP, 2023, IEEE NETWORK, V37, P26, DOI 10.1109/MNET.106.2100449
   Fernandes AS, 2016, 2016 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P201, DOI 10.1109/3DUI.2016.7460053
   Graf M, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P261, DOI 10.1145/3083187.3084016
   Guo CJ, 2019, IEEE WIREL COMMUN LE, V8, P145, DOI 10.1109/LWC.2018.2864151
   He Dongbiao, 2018, ACM SIGCOMM WORKSHOP
   Hosseini M, 2016, IEEE INT SYM MULTIM, P107, DOI [10.1109/ISM.2016.0028, 10.1109/ISM.2016.45]
   Hu XJ, 2019, MOB INF SYST, V2019, DOI 10.1155/2019/8487372
   ITU-T Recommendation, 2014, Methods for the Subjective Assessment of Video Quality, Audio Quality and Audiovisual Quality of Internet Video and Distribution Quality Television in Any Environment, P913
   Jiang XL, 2018, C LOCAL COMPUT NETW, P393, DOI 10.1109/LCN.2018.8638092
   Kattadige Chamara, 2021, arXiv
   Khiem N. Quang Minh, 2011, P 2 ANN ACM C MULT S, P211, DOI DOI 10.1145/1943552.1943581
   Le Feuvre J, 2016, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON MULTIMEDIA SYSTEMS (MMSYS'16), P329, DOI 10.1145/2910017.2910641
   Li WH, 2023, IEEE T MULTIMEDIA, V25, P5662, DOI 10.1109/TMM.2022.3198013
   Lo WC, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P211, DOI 10.1145/3083187.3083219
   Long Kaixuan, 2018, 2018 IEEE GLOB COMM, P1
   Lungaro P, 2018, IEEE T VIS COMPUT GR, V24, P1535, DOI 10.1109/TVCG.2018.2794119
   Mao HZ, 2017, SIGCOMM '17: PROCEEDINGS OF THE 2017 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P197, DOI 10.1145/3098822.3098843
   Mesfin G, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3303080
   Minh Khiem Ngo Quang., 2010, Proceedings of the first annual ACM SIGMM conference on Multimedia systems, P259, DOI DOI 10.1145/1730836.1730868
   Nasrabadi AT, 2020, NOSSDAV '20: PROCEEDINGS OF THE 2020 WORKSHOP ON NETWORK AND OPERATING SYSTEM SUPPORT FOR DIGITAL AUDIO AND VIDEO, P34, DOI 10.1145/3386290.3396934
   Nasrabadi AT, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1689, DOI 10.1145/3123266.3123414
   Nguyen DV, 2019, IEEE J EM SEL TOP C, V9, P29, DOI 10.1109/JETCAS.2019.2899488
   Nguyen DV, 2019, IEICE T INF SYST, VE102D, P48, DOI 10.1587/transinf.2018MUL0001
   Ordonez-Ante L, 2022, J NETW SYST MANAG, V30, DOI 10.1007/s10922-022-09649-5
   Ozcinar C, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON VISUAL COMMUNICATIONS AND IMAGE PROCESSING (IEEE VCIP)
   Ozcinar C, 2019, IEEE J EM SEL TOP C, V9, P217, DOI 10.1109/JETCAS.2019.2895096
   Petrangeli S, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P306, DOI 10.1145/3123266.3123453
   Qian F., 2016, P 5 WORKSH ALL THING, P1, DOI DOI 10.1145/2980055.2980056
   Qian F, 2018, MOBICOM'18: PROCEEDINGS OF THE 24TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P99, DOI 10.1145/3241539.3241565
   Rondon Miguel Fabian Romero, 2019, arXiv
   Sánchez Y, 2015, IEEE IMAGE PROC, P2244, DOI 10.1109/ICIP.2015.7351200
   Spiteri K, 2016, IEEE INFOCOM SER, DOI 10.1109/infocom.2016.7524428
   Tengfei Cao, 2019, ACM Transactions on Multimedia Computing, Communications and Applications, V15, DOI 10.1145/3328996
   Upenik Evgeniy, 2017, 2017 IEEE International Conference on Multimedia and Expo: Workshops (ICMEW), P73, DOI 10.1109/ICMEW.2017.8026231
   van der Hooft J, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3362101
   Wang H., 2014, PROC NETW OPERATING, P25
   Wei XK, 2022, IEEE T MOBILE COMPUT, V21, P3428, DOI 10.1109/TMC.2021.3058099
   Wei XK, 2021, INFORM SCIENCES, V569, P786, DOI 10.1016/j.ins.2021.05.012
   Wu CL, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P193, DOI 10.1145/3083187.3083210
   Xiao MB, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P708, DOI 10.1145/3123266.3123339
   Xie L, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P315, DOI 10.1145/3123266.3123291
   Yadav PK, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3724, DOI 10.1145/3394171.3413550
   Yadav PK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1130, DOI 10.1145/3123266.3123390
   Yaqoob A, 2022, IEEE ACCESS, V10, P29377, DOI 10.1109/ACCESS.2022.3157339
   Yaqoob A, 2021, IEEE T BROADCAST, V67, P746, DOI 10.1109/TBC.2021.3105022
   Yaqoob A, 2020, IEEE COMMUN SURV TUT, V22, P2801, DOI 10.1109/COMST.2020.3006999
   Yaqoob A, 2019, INT WIREL COMMUN, P643, DOI 10.1109/IWCMC.2019.8766648
   Yaqoob Abid, 2020, 2020 IEEE INT S BROA
   Yuan H, 2020, IEEE J-STSP, V14, P177, DOI 10.1109/JSTSP.2019.2957981
   Yuan ZH, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2661329
   Zare A., 2016, P 24 ACM INT C MULT, P601
   Zhang HD, 2023, IEEE T MULTIMEDIA, V25, P4225, DOI 10.1109/TMM.2022.3172550
   Zhang L, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4007, DOI 10.1145/3474085.3475590
   Zhang YH, 2022, IEEE SYMP COMP COMMU, DOI 10.1109/ISCC55528.2022.9913007
   Zhang YX, 2019, IEEE INFOCOM SER, P1252, DOI [10.1109/INFOCOM.2019.8737361, 10.1109/infocom.2019.8737361]
   Zhou C, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3209660
   Zou JN, 2020, IEEE J-STSP, V14, P161, DOI 10.1109/JSTSP.2019.2956716
NR 65
TC 0
Z9 0
U1 3
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2024
VL 20
IS 1
AR 6
DI 10.1145/3603146
PG 28
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T8LL3
UT WOS:001080441800006
OA Bronze
DA 2024-08-05
ER

PT J
AU Zhao, J
   Li, B
   Li, JH
   Xiong, RQ
   Lu, Y
AF Zhao, Jing
   Li, Bin
   Li, Jiahao
   Xiong, Ruiqin
   Lu, Yan
TI A Universal Optimization Framework for Learning-based Image Codec
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image compression; rate distortion optimization; universal optimization
   framework; machine learning
ID RATE-DISTORTION OPTIMIZATION; COMPRESSION
AB Recently, machine learning-based image compression has attracted increasing interests and is approaching the state-of-the-art compression ratio. But unlike traditional codec, it lacks a universal optimization method to seek efficient representation for different images. In this paper, we develop a plug-and-play optimization framework for seeking higher compression ratio, which can be flexibly applied to existing and potential future compression networks. To make the latent representation more efficient, we propose a novel latent optimization algorithm to adaptively remove the redundancy for each image. Additionally, inspired by the potential of side information for traditional codecs, we introduce side information into our framework, and integrate side information optimization with latent optimization to further enhance the compression ratio. In particular, with the joint side information and latent optimization, we can achieve fine rate control using only single model instead of training different models for different rate-distortion trade-offs, which significantly reduces the training and storage cost to support multiple bit rates. Experimental results demonstrate that our proposed framework can remarkably boost the machine learning-based compression ratio, achieving more than 10% additional bit rate saving on three different representative network structures. With the proposed optimization framework, we can achieve 7.6% bit rate saving against the latest traditional coding standard VVC on Kodak dataset, yielding the state-of-the-art compression ratio.
C1 [Zhao, Jing; Xiong, Ruiqin] Peking Univ, 5 Yiheyuan Rd, Beijing 100871, Peoples R China.
   [Li, Bin; Li, Jiahao; Lu, Yan] Microsoft Res Asia, 5 Dan Ling St, Beijing 100080, Peoples R China.
C3 Peking University; Microsoft Research Asia; Microsoft
RP Zhao, J (corresponding author), Peking Univ, 5 Yiheyuan Rd, Beijing 100871, Peoples R China.
EM jzhaopku@microsoft.com; libin@microsoft.com; li.jiahao@microsoft.com;
   rqxiong@pku.edu.cn; yanlu@microsoft.com
RI zhang, yueqi/JXM-4287-2024; Zhang, Can/JUU-9511-2023
OI Xiong, Ruiqin/0000-0001-9796-0478; Li, Jiahao/0000-0003-4559-0086; Zhao,
   Jing/0000-0002-8413-9979
CR Agustsson E, 2017, ADV NEUR IN, V30
   Agustsson E, 2019, IEEE I CONF COMP VIS, P221, DOI 10.1109/ICCV.2019.00031
   Balle J., 2018, ICLR
   Ballé J, 2016, PICT COD SYMP, DOI 10.1109/pcs.2016.7906310
   Balle Johannes, 2017, 5 INT C LEARNING REP
   Begaint J., 2020, arXiv
   Bjontegaard Gisle, 2001, VCEGM33
   Bossen Frank, 2020, JVET document, JVET-Q0003
   Cai CL, 2019, IEEE T CIRC SYST VID, V29, P3687, DOI 10.1109/TCSVT.2018.2880492
   Campos J., 2019, IEEE C COMP VIS PATT
   Chen T, 2021, IEEE T IMAGE PROCESS, V30, P3179, DOI 10.1109/TIP.2021.3058615
   Cheng ZZ, 2019, PROC CVPR IEEE, P5438, DOI 10.1109/CVPR.2019.00559
   Choi I, 2006, IEEE T CIRC SYST VID, V16, P1557, DOI 10.1109/TCSVT.2006.883506
   Choi Y, 2019, IEEE I CONF COMP VIS, P3146, DOI 10.1109/ICCV.2019.00324
   CLIC,, 2021, Workshop and Challenge on Learned Image Compression
   Cui Z, 2021, PROC CVPR IEEE, P10527, DOI 10.1109/CVPR46437.2021.01039
   Fu CM, 2012, IEEE T CIRC SYST VID, V22, P1755, DOI 10.1109/TCSVT.2012.2221529
   Fu T., 2021, 4th Challenge on Learned Image Compression
   Guo Lu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P456, DOI 10.1007/978-3-030-58536-5_27
   Guo ZY, 2022, IEEE T CIRC SYST VID, V32, P2329, DOI 10.1109/TCSVT.2021.3089491
   Guo ZY, 2020, IEEE COMPUT SOC CONF, P520, DOI 10.1109/CVPRW50498.2020.00066
   HM, 2021, HEVC Reference Software
   Hu YY, 2020, AAAI CONF ARTIF INTE, V34, P11013
   Hu YY, 2022, IEEE T PATTERN ANAL, V44, P4194, DOI 10.1109/TPAMI.2021.3065339
   Huang YH, 2010, IEEE T CIRC SYST VID, V20, P1614, DOI 10.1109/TCSVT.2010.2087472
   Johnston N, 2018, PROC CVPR IEEE, P4385, DOI 10.1109/CVPR.2018.00461
   Klopp J. P., 2018, P BMVC, P124
   Kodak, 2013, Kodak lossless true color image suite
   Lee H, 2016, IEEE T CIRC SYST VID, V26, P107, DOI 10.1109/TCSVT.2015.2450151
   Lee Jaehoon, 2018, INT C LEARN REPR
   Li M, 2018, PROC CVPR IEEE, P3214, DOI 10.1109/CVPR.2018.00339
   Liu Jerry, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P453, DOI 10.1007/978-3-030-58520-4_27
   Ma HC, 2022, IEEE T PATTERN ANAL, V44, P1247, DOI 10.1109/TPAMI.2020.3026003
   Ma SW, 2020, IEEE T CIRC SYST VID, V30, P1683, DOI 10.1109/TCSVT.2019.2910119
   Ma SW, 2005, IEEE T CIRC SYST VID, V15, P1533, DOI 10.1109/TCSVT.2005.857300
   Mentzer F, 2018, PROC CVPR IEEE, P4394, DOI 10.1109/CVPR.2018.00462
   Mentzer Fabian, 2020, Neural Information Processing Systems
   Minnen D, 2020, IEEE IMAGE PROC, P3339, DOI [10.1109/icip40778.2020.9190935, 10.1109/ICIP40778.2020.9190935]
   Minnen D, 2018, ADV NEUR IN, V31
   Mishra D, 2021, IEEE T CIRC SYST VID, V31, P1452, DOI 10.1109/TCSVT.2020.3010627
   Ohm J.R., 2018, PICT COD S
   OpenJPEG, 2000, JPEG2000 Reference Software
   Rabbani M, 2002, SIGNAL PROCESS-IMAGE, V17, P3, DOI 10.1016/S0923-5965(01)00024-8
   Rippel O, 2017, PR MACH LEARN RES, V70
   Song M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2360, DOI 10.1109/ICCV48922.2021.00238
   Sukthankar R., 2015, INT C LEARN REPR
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sullivan GJ, 1998, IEEE SIGNAL PROC MAG, V15, P74, DOI 10.1109/79.733497
   Theis Lucas, 2017, ARXIV170300395
   Toderici G, 2017, PROC CVPR IEEE, P5435, DOI 10.1109/CVPR.2017.577
   Tsai CY, 2013, IEEE J-STSP, V7, P934, DOI 10.1109/JSTSP.2013.2271974
   Tschannen M, 2018, ADV NEUR IN, V31
   VTM, 2021, VVC Reference Software
   WALLACE GK, 1991, COMMUN ACM, V34, P30, DOI 10.1145/103085.103089
   Wang YF, 2021, IEEE T CIRC SYST VID, V31, P1193, DOI 10.1109/TCSVT.2020.3000331
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2
   Yang EH, 2007, IEEE T IMAGE PROCESS, V16, P1774, DOI 10.1109/TIP.2007.896685
   Yang F, 2021, PROC CVPR IEEE, P4996, DOI 10.1109/CVPR46437.2021.00496
   Yang KF, 2015, SIGNAL PROCESS-IMAGE, V31, P10, DOI 10.1016/j.image.2014.11.005
   Zhang K, 2022, IEEE T PATTERN ANAL, V44, P6360, DOI 10.1109/TPAMI.2021.3088914
   Zhao J, 2021, IEEE COMPUT SOC CONF, P1880, DOI 10.1109/CVPRW53098.2021.00210
   Zhao X, 2012, IEEE T CIRC SYST VID, V22, P138, DOI 10.1109/TCSVT.2011.2158363
   Zhengxue Cheng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7936, DOI 10.1109/CVPR42600.2020.00796
NR 64
TC 2
Z9 2
U1 2
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2024
VL 20
IS 1
AR 16
DI 10.1145/3580499
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T8LL3
UT WOS:001080441800016
DA 2024-08-05
ER

PT J
AU Hu, XB
   Lin, YF
   Fan, HH
   Wang, S
   Wu, ZH
   Lv, K
AF Hu, Xiaobo
   Lin, Youfang
   Fan, Hehe
   Wang, Shuo
   Wu, Zhihao
   Lv, Kai
TI Building Category Graphs Representation with Spatial and Temporal
   Attention for Visual Navigation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Object visual navigation; relation graph; attention network;
   reinforcement learning
AB Given an object of interest, visual navigation aims to reach the object's location based on a sequence of partial observations. To this end, an agent needs to (1) acquire specific knowledge about the relations of object categories in the world during training and (2) locate the target object based on the pre-learned object category relations and its trajectory in the current unseen environment. In this article, we propose a Category Relation Graph (CRG) to learn the knowledge of object category layout relations and a Temporal-Spatial-Region attention (TSR) architecture to perceive the long-term spatial-temporal dependencies of objects, aiding navigation. We establish CRG to learn prior knowledge of object layout and deduce the positions of specific objects. Subsequently, we propose the TSR architecture to capture relationships among objects in temporal, spatial, and regions within observation trajectories. Specifically, we implement a Temporal attention module (T) to model the temporal structure of the observation sequence, implicitly encoding historical moving or trajectory information. Then, a Spatial attention module (S) uncovers the spatial context of the current observation objects based on CRG and past observations. Last, a Region attention module (R) shifts the attention to the target-relevant region. Leveraging the visual representation extracted by ourmethod, the agent accurately perceives the environment and easily learns a superior navigation policy. Experiments on AI2-THOR demonstrate that our CRG-TSR method significantly outperforms existing methods in both effectiveness and efficiency. The supplementary material includes the code and will be publicly available.
C1 [Hu, Xiaobo; Lin, Youfang; Wang, Shuo; Wu, Zhihao; Lv, Kai] Beijing Jiaotong Univ, Sch Comp & Informat Technol, Beijing Key Lab Traff Data Anal & Min, Beijing, Peoples R China.
   [Fan, Hehe] Zhejiang Univ, Coll Comp Sci & Technol, Zhejiang, Peoples R China.
C3 Beijing Jiaotong University; Zhejiang University
RP Lv, K (corresponding author), Beijing Jiaotong Univ, Sch Comp & Informat Technol, Beijing Key Lab Traff Data Anal & Min, Beijing, Peoples R China.
EM xiaobohu@bjtu.edu.cn; yflin@bjtu.edu.cn; crane.h.fan@gmail.com;
   shuo.wang@bjtu.edu.cn; zhwu@bjtu.edu.cn; lvkai@bjtu.edu.cn
OI Fan, Hehe/0000-0001-9572-2345; Lv, Kai/0000-0001-6533-5176
FU National Natural Science Foundation of China [62206013]; Aeronautical
   Science Foundation of China [202300010M5001]
FX This work was supported by the National Natural Science Foundation of
   China (Grant No. 62206013) and the Aeronautical Science Foundation of
   China (Grant No. 202300010M5001).
CR Anderson P, 2018, Arxiv, DOI arXiv:1807.06757
   Babaeizadeh Mohammad, 2017, INT C LEARN REPR OP
   Blosch Michael, 2010, 2010 IEEE International Conference on Robotics and Automation (ICRA 2010), P21, DOI 10.1109/ROBOT.2010.5509920
   Borenstein J., 1990, Proceedings 1990 IEEE International Conference on Robotics and Automation (Cat. No.90CH2876-1), P572, DOI 10.1109/ROBOT.1990.126042
   BORENSTEIN J, 1991, IEEE T ROBOTIC AUTOM, V7, P278, DOI 10.1109/70.88137
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chang A, 2017, INT CONF 3D VISION, P667, DOI 10.1109/3DV.2017.00081
   Chen JW, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3526024
   Chen K, 2019, ROBOTICS: SCIENCE AND SYSTEMS XV
   Chen XT, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P694
   Cummins M, 2007, IEEE INT CONF ROBOT, P2042, DOI 10.1109/ROBOT.2007.363622
   Dang RH, 2023, IEEE I CONF COMP VIS, P8216, DOI 10.1109/ICCV51070.2023.00758
   Dang RH, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P3617, DOI 10.1145/3503161.3547852
   Deitke M, 2020, PROC CVPR IEEE, P3161, DOI 10.1109/CVPR42600.2020.00323
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dissanayake MWMG, 2001, IEEE T ROBOTIC AUTOM, V17, P229, DOI 10.1109/70.938381
   Du Heming, 2021, INT C LEARN REPR OPE
   Fang K, 2019, PROC CVPR IEEE, P538, DOI 10.1109/CVPR.2019.00063
   Fukushima R, 2022, IEEE INT CONF ROBOT, P11288, DOI 10.1109/ICRA46639.2022.9812027
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Guo SN, 2022, IEEE T KNOWL DATA EN, V34, P5415, DOI 10.1109/TKDE.2021.3056502
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heming Du, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P19, DOI 10.1007/978-3-030-58571-6_2
   Hu JF, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3293814
   Hu XB, 2024, IEEE T CIRC SYST VID, V34, P1295, DOI 10.1109/TCSVT.2023.3291131
   Joulin A., 2017, P 15 C EUR CHAPT ASS, P427, DOI DOI 10.18653/V1/E17-2068
   Kidono K, 2002, ROBOT AUTON SYST, V40, P121, DOI 10.1016/S0921-8890(02)00237-3
   Kingma D. P., 2014, arXiv
   Kohl N, 2004, IEEE INT CONF ROBOT, P2619, DOI 10.1109/ROBOT.2004.1307456
   Kolve E, 2022, Arxiv, DOI arXiv:1712.05474
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Liang YQ, 2021, IEEE INT CONF ROBOT, P13194, DOI 10.1109/ICRA48506.2021.9560925
   Lin Y., 2023, arXiv
   Liu Yan., 2017, P INT C LEARNING REP, V1707, P01926
   Liu Y, 2022, IEEE T IMAGE PROCESS, V31, P1978, DOI 10.1109/TIP.2022.3147032
   Liu Y, 2022, IEEE T IMAGE PROCESS, V31, P1684, DOI 10.1109/TIP.2022.3142526
   Lv K, 2021, IEEE T MULTIMEDIA, V23, P4198, DOI 10.1109/TMM.2020.3038311
   Lv K, 2020, IEEE T IMAGE PROCESS, V29, P5163, DOI 10.1109/TIP.2020.2980130
   Lyu C, 2022, Arxiv, DOI arXiv:2212.07784
   Mayo B, 2021, PROC CVPR IEEE, P16893, DOI 10.1109/CVPR46437.2021.01662
   Meng M., 1993, IEEE Control Systems Magazine, V13, P30, DOI 10.1109/37.236323
   Meng XY, 2020, IEEE INT CONF ROBOT, P672, DOI [10.1109/ICRA40945.2020.9196644, 10.1109/icra40945.2020.9196644]
   Mirowski Piotr, 2017, INT C LEARN REPR OPE
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Ng AY, 2004, ADV NEUR IN, V16, P799
   ORIOLO G, 1995, IEEE INT CONF ROBOT, P2900, DOI 10.1109/ROBOT.1995.525695
   Padhy RP, 2024, ACM T MULTIM COMPUT, V20, DOI 10.1145/3550485
   Pathak D, 2018, IEEE COMPUT SOC CONF, P2131, DOI 10.1109/CVPRW.2018.00278
   Pennington J., 2014, P C EMP METH NAT LAN, P1532, DOI DOI 10.3115/V1/D14-1162
   Peters J, 2008, NEURAL NETWORKS, V21, P682, DOI 10.1016/j.neunet.2008.02.003
   Puig Xavier, 2021, INT C LEARN REPR OPE
   Ramakrishnan S. K., 2021, P NEUR INF PROC SYST
   Ramakrishnan SK, 2022, PROC CVPR IEEE, P18868, DOI 10.1109/CVPR52688.2022.01832
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Romero A., 2018, INT C LEARN REPR, P2920, DOI DOI 10.48550/ARXIV.1710.10903
   Savinov Nikolay, 2018, INT C LEARN REPR OPE
   Savva M, 2019, IEEE I CONF COMP VIS, P9338, DOI 10.1109/ICCV.2019.00943
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Thrun S, 1998, ARTIF INTELL, V99, P21, DOI 10.1016/S0004-3702(97)00078-7
   Vaswani A., 2017, Advances in neural information processing systems, P5998
   Wang S, 2023, IEEE T MULTIMEDIA, V25, P8920, DOI 10.1109/TMM.2023.3243618
   Wang Shuo, 2024, Association for the Advancement of Artificial Intelligence
   Wei H, 2018, IEEE T IMAGE PROCESS, V27, P3164, DOI 10.1109/TIP.2018.2818931
   Wortsman M, 2019, PROC CVPR IEEE, P3743, DOI 10.1109/CVPR.2019.00691
   Wu Y, 2019, IEEE I CONF COMP VIS, P2769, DOI 10.1109/ICCV.2019.00286
   Wu ZH, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1907
   Xia F, 2018, PROC CVPR IEEE, P9068, DOI 10.1109/CVPR.2018.00945
   Xie Wei, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P6785, DOI 10.1145/3581783.3612100
   Yang Wei, 2019, INT C LEARN REPR OPE
   Yu J, 2020, IEEE T NEUR NET LEAR, V31, P661, DOI 10.1109/TNNLS.2019.2908982
   Yu J, 2022, IEEE T PATTERN ANAL, V44, P563, DOI 10.1109/TPAMI.2019.2932058
   Zhang SX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15110, DOI 10.1109/ICCV48922.2021.01485
NR 72
TC 0
Z9 0
U1 3
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2024
VL 20
IS 7
SI SI
AR 217
DI 10.1145/3653714
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL0Q6
UT WOS:001234494100032
OA Bronze, Green Submitted
DA 2024-08-05
ER

PT J
AU Tan, JW
   Yang, PA
   Chen, L
   Wang, HX
AF Tan, Jiawei
   Yang, Pingan
   Chen, Lu
   Wang, Hongxing
TI Temporal Scene Montage for Self-Supervised Video Scene Boundary
   Detection
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Self-supervised learning; video scene boundary detection; video
   structuring; video understanding
ID SEGMENTATION
AB Once a video sequence is organized as basic shot units, it is of great interest to temporally link shots into semantic-compact scene segments to facilitate long video understanding. However, it still challenges existing video scene boundary detection methods to handle various visual semantics and complex shot relations in video scenes. We proposed a novel self-supervised learning method, Video Scene Montage for Boundary Detection (VSMBD), to extract rich shot semantics and learn shot relations using unlabeled videos. More specifically, we present Video Scene Montage (VSM) to synthesize reliable pseudo scene boundaries, which learns task-related semantic relations between shots in a self-supervised manner. To lay a solid foundation for modeling semantic relations between shots, we decouple visual semantics of shots into foreground and background. Instead of costly learning from scratch as in most previous self-supervised learning methods, we build our model upon large-scale pre-trained visual encoders to extract the foreground and background features. Experimental results demonstrate VSMBD trains a model with strong capability in capturing shot relations, surpassing previous methods by significant margins. The code is available at https://github.com/mini-mind/VSMBD.
C1 Sch Big Data & Software Engn, Chongqing, Peoples R China.
RP Wang, HX (corresponding author), Chongqing Univ, Minist Educ, Key Lab Dependable Serv Comp Cyber Phys Soc, Chongqing, Peoples R China.
EM jwtan@cqu.edu.cn; pnyang@cqu.edu.cn; alchen@cqu.edu.cn;
   ihxwang@cqu.edu.cn
OI Tan, Jiawei/0000-0003-3299-0785
FU National Natural Science Foundation of China [61976029]; Key Project of
   Chongqing Technology Innovation and Application Development
   [cstc2021jscx-gksbX0033]
FX This work is supported in part by the National Natural Science
   Foundation of China under Grant 61976029 and the Key Project of
   Chongqing Technology Innovation and Application Development under Grant
   cstc2021jscx-gksbX0033.
CR Ahsan U, 2019, IEEE WINT CONF APPL, P179, DOI 10.1109/WACV.2019.00025
   Anyi Rao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10143, DOI 10.1109/CVPR42600.2020.01016
   Baraldi L, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1199, DOI 10.1145/2733373.2806316
   BBC, 2006, Planet earth
   Benaim Sagie, 2020, P IEEE CVF C COMP VI, P9919, DOI DOI 10.1109/CVPR42600.2020.00994
   Castellanos Brandon, 2014, PySceneDetect
   Chasanis VT, 2009, IEEE T MULTIMEDIA, V11, P89, DOI 10.1109/TMM.2008.2008924
   Chen Lu, 2024, MultiMedia Modeling: 30th International Conference, MMM 2024, Proceedings. Lecture Notes in Computer Science (14556), P14, DOI 10.1007/978-3-031-53311-2_2
   Chen L, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P737
   Chen SX, 2023, PROC CVPR IEEE, P6535, DOI 10.1109/CVPR52729.2023.00632
   Chen SX, 2021, PROC CVPR IEEE, P9791, DOI 10.1109/CVPR46437.2021.00967
   Chen WD, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3514250
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dosovitskiy A., 2021, ICLR
   Feichtenhofer Christoph, 2022, NEURIPS
   Fernando B, 2017, PROC CVPR IEEE, P5729, DOI 10.1109/CVPR.2017.607
   Gaikwad B, 2021, IEEE INT CONF COMP V, P3198, DOI 10.1109/ICCVW54120.2021.00359
   Gehring J, 2017, PR MACH LEARN RES, V70
   Gu Albert, 2022, ICLR
   Han G., 2022, ACCV, P4027
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He T, 2019, PROC CVPR IEEE, P558, DOI 10.1109/CVPR.2019.00065
   Inoue M, 2018, LECT NOTES COMPUT SC, V11292, P164, DOI 10.1007/978-3-030-03520-4_16
   Islam MM, 2023, PROC CVPR IEEE, P18749, DOI 10.1109/CVPR52729.2023.01798
   Kender JR, 1998, PROC CVPR IEEE, P367, DOI 10.1109/CVPR.1998.698632
   Kim D, 2019, AAAI CONF ARTIF INTE, P8545
   Kingma D. P., 2014, arXiv
   Li CC, 2022, PROC CVPR IEEE, P13947, DOI 10.1109/CVPR52688.2022.01358
   Liu D, 2021, IEEE T CIRC SYST VID, V31, P3559, DOI 10.1109/TCSVT.2020.3042476
   Luo DZ, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3473342
   Na S, 2017, IEEE I CONF COMP VIS, P677, DOI 10.1109/ICCV.2017.80
   Ngo CW, 2005, IEEE T CIRC SYST VID, V15, P296, DOI 10.1109/TCSVT.2004.841694
   Qian R, 2021, PROC CVPR IEEE, P6960, DOI 10.1109/CVPR46437.2021.00689
   Qingqiu Huang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P709, DOI 10.1007/978-3-030-58548-8_41
   Rasheed Z, 2005, IEEE T MULTIMEDIA, V7, P1097, DOI 10.1109/TMM.2005.858392
   Rasheed Z, 2003, PROC CVPR IEEE, P343
   Rotman D, 2017, INT J SEMANT COMPUT, V11, P193, DOI 10.1142/S1793351X17400086
   Rui Y, 1999, MULTIMEDIA SYST, V7, P359, DOI 10.1007/s005300050138
   Chung JS, 2019, Arxiv, DOI arXiv:1906.10555
   Tan JW, 2024, ACM T MULTIM COMPUT, V20, DOI 10.1145/3630257
   Tan Jing, 2023, IEEE Trans Pattern Anal Mach Intell, V45, P12506, DOI 10.1109/TPAMI.2023.3283067
   Tapaswi M, 2014, PROC CVPR IEEE, P827, DOI 10.1109/CVPR.2014.111
   Tavenard R, 2020, J MACH LEARN RES, V21
   Truong BT, 2003, IEEE T CIRC SYST VID, V13, P5, DOI 10.1109/TCSVT.2002.808084
   Ul Haq I, 2021, INFORM FUSION, V76, P24, DOI 10.1016/j.inffus.2021.04.016
   Vaswani A., 2017, Advances in neural information processing systems, P5998
   Wang JH, 2003, VISUAL COMPUT, V19, P329, DOI 10.1007/s00371-002-0184-9
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wu HQ, 2022, PROC CVPR IEEE, P14001, DOI 10.1109/CVPR52688.2022.01363
   Xie RF, 2011, IEEE INT C BIO BIO W, P165, DOI 10.1109/BIBMW.2011.6112370
   Xu DJ, 2019, PROC CVPR IEEE, P10326, DOI 10.1109/CVPR.2019.01058
   Xu MM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7200, DOI 10.1109/ICCV48922.2021.00713
   Yang Haosen, 2022, arXiv
   Yang Yang, 2023, P AAAI C ARTIFICIAL, P3206
   Yao Y, 2020, PROC CVPR IEEE, P6547, DOI 10.1109/CVPR42600.2020.00658
   Yeung M, 1998, COMPUT VIS IMAGE UND, V71, P94, DOI 10.1006/cviu.1997.0628
   Yuan Y, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3595923
   Zhao Yanjun, 2007, CVPR
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
NR 60
TC 0
Z9 0
U1 1
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2024
VL 20
IS 7
SI SI
AR 215
DI 10.1145/3654669
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL0Q6
UT WOS:001234494100030
OA Bronze
DA 2024-08-05
ER

PT J
AU Jonker, S
   Jelstrup, M
   Meng, WZ
   Lampe, B
AF Jonker, Simon
   Jelstrup, Malthe
   Meng, Weizhi
   Lampe, Brooke
TI Detecting Post Editing of Multimedia Images using Transfer Learning and
   Fine Tuning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Multimedia data integrity; image forgery; fake news; post editing; fine
   tuning; transfer learning
AB In the domain of general image forgery detection, a myriad of different classification solutions have been developed to distinguish a "tampered" image from a "pristine" image. In this work, we aim to develop a new method to tackle the problem of binary image forgery detection. Our approach builds upon the extensive training that state-of-the-art image classification models have undergone on regular images from the ImageNet dataset, and transfers that knowledge to the image forgery detection space. By leveraging transfer learning and fine tuning, we can fit state-of-the-art image classification models to the forgery detection task. We train the models on a diverse and evenly distributed image forgery dataset. With five models-EfficientNetB0, VGG16, Xception, ResNet50V2, and NASNet-Large-we transferred and adapted pre-trained knowledge from ImageNet to the forgery detection task. Each model was fitted, fine-tuned, and evaluated according to a set of performance metrics. Our evaluation demonstrated the efficacy of large-scale image classification models-paired with transfer learning and fine tuning-at detecting image forgeries. When pitted against a previously unseen dataset, the best-performing model of EfficientNetB0 could achieve an accuracy rate of nearly 89.7%.
C1 [Jonker, Simon; Jelstrup, Malthe; Meng, Weizhi; Lampe, Brooke] Tech Univ Denmark, DTU Compute, Richard Petersens Plads, DK-2800 Lyngby, Denmark.
C3 Technical University of Denmark
RP Meng, WZ (corresponding author), Tech Univ Denmark, DTU Compute, Richard Petersens Plads, DK-2800 Lyngby, Denmark.
EM s184297@student.dtu.dk; s184291@student.dtu.dk; weme@dtu.dk; blam@dtu.dk
OI Kidmose, Brooke/0000-0002-6673-6163; Meng, Weizhi/0000-0003-4384-5786
CR Baviskar M., 2022, P 2022 INT C COMP CO, P1
   Benhamza H., 2021, 2021 INT C INF SYST, P1
   Chen ZW, 2022, CONCURR COMP-PRACT E, V34, DOI 10.1002/cpe.5787
   Chierchia G, 2014, IEEE T INF FOREN SEC, V9, P554, DOI 10.1109/TIFS.2014.2302078
   Choi HY, 2017, INT CONF SYST SIGNAL
   Cristin R., 2022, Lecture Notes in Electrical Engineering, V907, DOI [10.1007/978-981-19-4687-5_51, DOI 10.1007/978-981-19-4687-5_51]
   Das D, 2022, ANNU IEEE IND CONF, DOI 10.1109/INDICON56171.2022.10039789
   Ernawati M., 2022, P 2022 6 INT C INF C, P134
   Ganguly S, 2022, EXPERT SYST APPL, V210, DOI 10.1016/j.eswa.2022.118423
   Gupta PR, 2022, ADV INTELL SYST COMP, V1411, P141, DOI 10.1007/978-981-16-6887-6_13
   Hebbar N., 2022, P 2022 IEEE 3 GLOB C, P1
   Joshi R., 2022, P 2022 13 INT C COMP, P1
   Kaushik Manish Shankar, 2023, 2023 International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT), P860, DOI 10.1109/IDCIoT56793.2023.10053434
   Li K, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3152126
   Li WJ, 2022, IEEE COMMUN SURV TUT, V24, P280, DOI 10.1109/COMST.2021.3139052
   Li WJ, 2020, J NETW COMPUT APPL, V161, DOI 10.1016/j.jnca.2020.102631
   Li YM, 2019, IEEE T INF FOREN SEC, V14, P1307, DOI 10.1109/TIFS.2018.2876837
   Lin M, 2014, 2014 INTERNATIONAL CONFERENCE ON MEDICAL BIOMETRICS (ICMB 2014), P1, DOI 10.1109/ICMB.2014.8
   Majumder MTH, 2018, PROCEEDINGS OF 2018 5TH INTERNATIONAL CONFERENCE ON NETWORKING, SYSTEMS AND SECURITY (NSYSS), P102
   Matern F, 2020, IEEE T INF FOREN SEC, V15, P1303, DOI 10.1109/TIFS.2019.2935913
   Mayer O, 2018, IEEE T INF FOREN SEC, V13, P1762, DOI 10.1109/TIFS.2018.2799421
   Mazumdar A, 2019, IEEE IMAGE PROC, P116, DOI [10.1109/icip.2019.8802969, 10.1109/ICIP.2019.8802969]
   Meng WZ, 2021, INFORM FUSION, V70, P60, DOI 10.1016/j.inffus.2020.12.006
   Meng WH, 2020, INT J INF SECUR, V19, P279, DOI 10.1007/s10207-019-00462-x
   Meng WZ, 2017, IEEE T NETW SERV MAN, V14, P233, DOI 10.1109/TNSM.2017.2664893
   Nazli Mohammed N., 2017, 2017 8th International Conference on Information Technology (ICIT). Proceedings, P442, DOI 10.1109/ICITECH.2017.8080040
   ndtv, Latest News, Photos, Videos on Donald Trump Fake News
   Nirmalapriya G, 2023, COMPUT SECUR, V128, DOI 10.1016/j.cose.2023.103155
   Pham NT, 2023, IEEE ACCESS, V11, P11224, DOI 10.1109/ACCESS.2023.3241837
   Poisel R., 2011, Proceedings of the 2011 6th International Conference on IT Security Incident Management and IT Forensics (IMF 2011), P48, DOI 10.1109/IMF.2011.14
   Pradhan Shantanu, 2022, 2022 2nd International Conference on Innovative Practices in Technology and Management (ICIPTM), P697, DOI 10.1109/ICIPTM54933.2022.9754125
   Pun CM, 2015, IEEE T INF FOREN SEC, V10, P1705, DOI 10.1109/TIFS.2015.2423261
   Qian SS, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3451215
   Rani P. B. Shailaja, 2019, 2019 3rd International Conference on Electronics, Communication and Aerospace Technology (ICECA). Proceedings, P959, DOI 10.1109/ICECA.2019.8822064
   Rani R., 2021, P 2021 6 INT C IM IN, P533
   Shah TJ, 2020, INT CONF COMP COMMUN, P4, DOI 10.1109/iccci48352.2020.9104074
   Sharma K. R., 2023, P 19 INT C NETW SERV, P28
   Singh KN, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3561513
   Sun XG, 2022, IEEE GLOB COMM CONF, P2603, DOI 10.1109/GLOBECOM48099.2022.10001267
   Tan MX, 2019, PR MACH LEARN RES, V97
   Thuong Le-Tien, 2020, 2020 International Signal Processing, Communications and Engineering Management Conference (ISPCEM), P244, DOI 10.1109/ISPCEM52197.2020.00056
   Vinolin V, 2021, COMPUT J, V64, P1692, DOI 10.1093/comjnl/bxz148
   Wei WM, 2010, IEEE T INF FOREN SEC, V5, P507, DOI 10.1109/TIFS.2010.2051254
   Wilscy M., 2018, P 2018 INT C CIRC SY, P1
   Wu HW, 2022, IEEE T INF FOREN SEC, V17, P443, DOI 10.1109/TIFS.2022.3144878
   Yan CP, 2023, IEEE ACCESS, V11, P33313, DOI 10.1109/ACCESS.2023.3264014
NR 46
TC 1
Z9 1
U1 1
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2024
VL 20
IS 6
SI SI
AR 154
DI 10.1145/3633284
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OQ2T3
UT WOS:001208681800004
OA Green Published, Bronze
DA 2024-08-05
ER

PT J
AU Hu, HT
   Xie, LX
   Huo, XY
   Hong, RC
   Tian, Q
AF Hu, Hengtong
   Xie, Lingxi
   Huo, Xinyue
   Hong, Richang
   Tian, Qi
TI One-Bit Supervision for Image Classification: Problem, Solution, and
   Beyond
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE One-bit supervision; semi-supervised learning; active learning;
   self-supervised learning; unsupervised domain adaptation
AB This article presents one-bit supervision, a novel setting of learning with fewer labels, for image classification. Instead of the training model using the accurate label of each sample, our setting requires the model to interact with the system by predicting the class label of each sample and learn from the answer whether the guess is correct, which provides one bit (yes or no) of information. An intriguing property of the setting is that the burden of annotation largely is alleviated in comparison to offering the accurate label. There are two keys to one-bit supervision: (i) improving the guess accuracy and (ii) making good use of the incorrect guesses. To achieve these goals, we propose a multi-stage training paradigm and incorporate negative label suppression into an off-the-shelf semi-supervised learning algorithm. Theoretical analysis shows that one-bit annotation is more efficient than full-bit annotation in most cases and gives the conditions of combining our approach with active learning. Inspired by this, we further integrate the one-bit supervision framework into the self-supervised learning algorithm, which yields an even more efficient training schedule. Different from training from scratch, when self-supervised learning is used for initialization, both hard example mining and class balance are verified to be effective in boosting the learning performance. However, these two frameworks still need full-bit labels in the initial stage. To cast off this burden, we utilize unsupervised domain adaptation to train the initial model and conduct pure one-bit annotations on the target dataset. In multiple benchmarks, the learning efficiency of the proposed approach surpasses that using full-bit, semi-supervised supervision.
C1 [Hu, Hengtong; Hong, Richang] Hefei Univ Technol, Hefei 230000, Anhui, Peoples R China.
   [Xie, Lingxi; Tian, Qi] Huawei Inc, Shenzhen, Guangdong, Peoples R China.
   [Huo, Xinyue] Univ Sci & Technol China, Hefei, Peoples R China.
C3 Hefei University of Technology; Huawei Technologies; Chinese Academy of
   Sciences; University of Science & Technology of China, CAS
RP Hong, RC (corresponding author), Hefei Univ Technol, Hefei 230000, Anhui, Peoples R China.
EM huhengtong.hfut@gmail.com; 198808xc@gmail.com; xinyueh@mail.ustc.edu.cn;
   hongrc@hfut.edu.cn; qi1@huawei.com
OI Xinyue, Huo/0000-0003-1724-9438
FU National Key Research and Development Program of China [2019YFA0706200,
   2018AAA0102002]; National Natural Science Foundation of China [61932009]
FX This work was supported by the National Key Research and Development
   Program of China under grants 2019YFA0706200 and 2018AAA0102002, and in
   part by the National Natural Science Foundation of China under grant
   61932009.
CR Atlas Les E., 1990, Proc. of the 2nd International Conference on Neural Information Processing Systems, P566
   Berthelot D, 2019, ADV NEUR IN, V32
   Cascante-Bonilla P, 2020, Arxiv, DOI arXiv:2001.06001
   Chen C, 2020, AAAI CONF ARTIF INTE, V34, P3422
   Chen J., 2020, P 37 INT C MACHINE L, P1704
   Chen T, 2020, Arxiv, DOI arXiv:2006.10029
   Chen T, 2020, PR MACH LEARN RES, V119
   Chen XL, 2020, Arxiv, DOI arXiv:2003.04297
   Chia-Wen Kuo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P479, DOI 10.1007/978-3-030-58523-5_28
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fergus R., 2009, Advances in Neural Information Processing Systems, P522
   Florensa C., 2018, arXiv
   Freytag A, 2014, LECT NOTES COMPUT SC, V8692, P562, DOI 10.1007/978-3-319-10593-2_37
   Furlanello T, 2018, Arxiv, DOI arXiv:1805.04770
   Gal Y, 2017, PR MACH LEARN RES, V70
   Gidaris S, 2018, Arxiv, DOI arXiv:1803.07728
   Grandvalet Y, 2004, Proceedings of NIPS, V17
   Guillaumin M, 2010, PROC CVPR IEEE, P902, DOI 10.1109/CVPR.2010.5540120
   Han Tao, 2021, P AAAI C ART INT
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Houlsby N, 2011, Arxiv, DOI [arXiv:1112.5745, 10.48550/arXiv.1112.5745]
   Hu HT, 2020, Arxiv, DOI arXiv:2004.00280
   Hu HN, 2022, ANIM BIOTECHNOL, V33, P731, DOI 10.1080/10495398.2020.1830102
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Iscen A, 2019, PROC CVPR IEEE, P5065, DOI 10.1109/CVPR.2019.00521
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Kim Y, 2019, IEEE I CONF COMP VIS, P101, DOI 10.1109/ICCV.2019.00019
   Kirsch A, 2019, ADV NEUR IN, V32
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Laine S, 2017, Arxiv, DOI [arXiv:1610.02242, 10.48550/arXiv.1610.02242]
   Larsson G, 2017, PROC CVPR IEEE, P840, DOI 10.1109/CVPR.2017.96
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee D.H., 2013, WORKSH CHALL REPR LE, P07
   Lewis D. D., 1994, SIGIR '94. Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, P3
   Li S, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9082, DOI 10.1109/ICCV48922.2021.00897
   Luo Wenjie, 2013, ADV NEURAL INFORM PR, P728
   Malisiewicz Tomasz, 2009, Advances in Neural Information Processing Systems, P1222
   Mingfei Gao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P510, DOI 10.1007/978-3-030-58607-2_30
   Miyato T, 2019, IEEE T PATTERN ANAL, V41, P1979, DOI 10.1109/TPAMI.2018.2858821
   Noroozi M, 2018, PROC CVPR IEEE, P9359, DOI 10.1109/CVPR.2018.00975
   Noroozi M, 2017, IEEE I CONF COMP VIS, P5899, DOI 10.1109/ICCV.2017.628
   Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5
   Papadopoulos DP, 2017, PROC CVPR IEEE, P180, DOI 10.1109/CVPR.2017.27
   Papadopoulos DP, 2016, PROC CVPR IEEE, P854, DOI 10.1109/CVPR.2016.99
   Pathak D, 2017, PROC CVPR IEEE, P6024, DOI 10.1109/CVPR.2017.638
   Peng XC, 2019, IEEE I CONF COMP VIS, P1406, DOI 10.1109/ICCV.2019.00149
   Pinsler R., 2019, Advances in Neural Information Processing Systems, P6356
   Qiao SY, 2018, LECT NOTES COMPUT SC, V11219, P142, DOI 10.1007/978-3-030-01267-0_9
   Qizhe Xie, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10684, DOI 10.1109/CVPR42600.2020.01070
   Rasmus A, 2015, ADV NEUR IN, V28
   Ravi S., 2017, INT C LEARN REPR
   Rizve M, 2021, Arxiv, DOI [arXiv:2101.06329, 10.48550/arXiv.2101.06329]
   Romero A, 2015, Arxiv, DOI arXiv:1412.6550
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sener O, 2018, Arxiv, DOI [arXiv:1708.00489, 10.48550/arXiv.1708.00489, DOI 10.48550/ARXIV.1708.00489]
   Shi Weishi, 2019, Advances in Neural Information Processing Systems, P2282
   Sohn K, 2020, Arxiv, DOI arXiv:2001.07685
   Sun Y., 2020, PMLR, V119, P9229
   Sun Yu, 2019, arXiv
   Tarvainen A, 2017, ADV NEUR IN, V30
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   Wang KZ, 2017, IEEE T CIRC SYST VID, V27, P2591, DOI 10.1109/TCSVT.2016.2589879
   Xie QZ, 2020, Arxiv, DOI arXiv:1904.12848
   Xu Haohang, 2020, arXiv
   Xu N, 2016, PROC CVPR IEEE, P373, DOI 10.1109/CVPR.2016.47
   Yang CL, 2018, Arxiv, DOI arXiv:1805.05551
   Yoo D, 2019, PROC CVPR IEEE, P93, DOI 10.1109/CVPR.2019.00018
   Yu B, 2019, PROC CVPR IEEE, P10668, DOI 10.1109/CVPR.2019.01093
   Zagoruyko S, 2017, Arxiv, DOI arXiv:1605.07146
   Zellinger W, 2019, Arxiv, DOI arXiv:1702.08811
   Zhai XH, 2019, IEEE I CONF COMP VIS, P1476, DOI 10.1109/ICCV.2019.00156
   Zhang LH, 2020, PROC CVPR IEEE, P3911, DOI 10.1109/CVPR42600.2020.00397
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zhu ZT, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3609
NR 75
TC 0
Z9 0
U1 1
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2024
VL 20
IS 4
AR 113
DI 10.1145/3633779
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6G9
UT WOS:001153382100023
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Guo, JT
   Qi, L
   Shi, YH
   Gao, Y
AF Guo, Jintao
   Qi, Lei
   Shi, Yinghuan
   Gao, Yang
TI PLACE Dropout: A Progressive Layer-wise and Channel-wise Dropout for
   Domain Generalization
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Domain generalization; dropout regularization; overfitting problem;
   distribution shift
AB Domain generalization (DG) aims to learn a generic model from multiple observed source domains that generalizes well to arbitrary unseen target domains without further training. The major challenge in DG is that the model inevitably faces a severe overfitting issue due to the domain gap between source and target domains. To mitigate this problem, some dropout-based methods have been proposed to resist overfitting by discarding part of the representation of the intermediate layers. However, we observe that most of these methods only conduct the dropout operation in some specific layers, leading to an insufficient regularization effect on the model. We argue that applying dropout at multiple layers can produce stronger regularization effects, which could alleviate the overfitting problem on source domains more adequately than previous layer-specific dropout methods. In this article, we develop a novel layer-wise and channel-wise dropout for DG, which randomly selects one layer and then randomly selects its channels to conduct dropout. Particularly, the proposed method can generate a variety of data variants to better deal with the overfitting issue. We also provide theoretical analysis for our dropout method and prove that it can effectively reduce the generalization error bound. Besides, we leverage the progressive scheme to increase the dropout ratio with the training progress, which can gradually boost the difficulty of training the model to enhance its robustness. Extensive experiments on three standard benchmark datasets have demonstrated that our method outperforms several state-of-the-art DG methods. Our code is available at https://github.com/lingeringlight/PLACEdropout.
C1 [Guo, Jintao; Shi, Yinghuan; Gao, Yang] Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China.
   [Qi, Lei] Southeast Univ, Sch Comp Sci & Engn, Nanjing, Peoples R China.
   [Qi, Lei] Southeast Univ, Minist Educ, Key Lab NewGenerat Artificial Intelligence Techno, Nanjing, Peoples R China.
C3 Nanjing University; Southeast University - China; Southeast University -
   China
RP Guo, JT; Shi, YH (corresponding author), Nanjing Univ, State Key Lab Novel Software Technol, Nanjing, Peoples R China.; Qi, L (corresponding author), Southeast Univ, Sch Comp Sci & Engn, Nanjing, Peoples R China.; Qi, L (corresponding author), Southeast Univ, Minist Educ, Key Lab NewGenerat Artificial Intelligence Techno, Nanjing, Peoples R China.
EM guojintao@smail.nju.edu.cn; qilei.cs@gmail.com; syh@nju.edu.cn;
   gaoy@nju.edu.cn
RI GAO, Yang/HMO-8142-2023
OI Qi, Lei/0000-0001-7091-0702; Gao, Yang/0000-0002-2488-1813; Guo,
   Jintao/0000-0003-1101-4443
FU NSFC Program [62222604, 62206052, 62192783]; Jiangsu Natural Science
   Foundation Project [BK20210224]
FX The work is supported by NSFC Program (Grants No. 62222604, No.
   62206052, No. 62192783) and Jiangsu Natural Science Foundation Project
   (Grant No. BK20210224).
CR Balaji Y, 2018, ADV NEUR IN, V31
   Bengio J., 2009, Proceedings of the 26th Annual International Conference on Machine Learning, P41
   Borlino FC, 2021, INT C PATT RECOG, P9227, DOI 10.1109/ICPR48806.2021.9412735
   Carlucci FM, 2019, PROC CVPR IEEE, P2224, DOI 10.1109/CVPR.2019.00233
   Chen CQ, 2022, PROC CVPR IEEE, P7109, DOI 10.1109/CVPR52688.2022.00698
   Chen Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9144, DOI 10.1109/ICCV48922.2021.00903
   Cubuk ED, 2020, IEEE COMPUT SOC CONF, P3008, DOI 10.1109/CVPRW50498.2020.00359
   DInnocente Antonio, 2018, Proceedings of the GCPR
   Dou Qi, 2019, Proceedings of the Neur IPS
   Fan XJ, 2021, PROC CVPR IEEE, P8204, DOI 10.1109/CVPR46437.2021.00811
   Ghiasi Golnaz, 2018, Proceedings of the Neur IPS
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou QB, 2023, IEEE T PATTERN ANAL, V45, P1328, DOI 10.1109/TPAMI.2022.3145427
   Hou SH, 2019, AAAI CONF ARTIF INTE, P8425
   Huang JX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8968, DOI 10.1109/ICCV48922.2021.00886
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Huang Zeyi, 2020, P ECCV
   Kang J, 2022, PROC CVPR IEEE, P7120, DOI 10.1109/CVPR52688.2022.00699
   Li D, 2017, IEEE I CONF COMP VIS, P5543, DOI 10.1109/ICCV.2017.591
   Li P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8866, DOI 10.1109/ICCV48922.2021.00876
   Li Zekun, 2023, Proceedings of the ICCV
   Liu L, 2023, Arxiv, DOI arXiv:2306.05704
   Mahajan Divyat, 2021, Proceedings of the ICML
   Matsuura T, 2020, AAAI CONF ARTIF INTE, V34, P11749
   Meng R, 2022, LECT NOTES COMPUT SC, V13694, P322, DOI 10.1007/978-3-031-19830-4_19
   Morerio P, 2017, IEEE I CONF COMP VIS, P3564, DOI 10.1109/ICCV.2017.383
   Nagpal Shruti, 2020, Proceedings of the CVPRW
   Nam H, 2021, PROC CVPR IEEE, P8686, DOI 10.1109/CVPR46437.2021.00858
   Nuriel O, 2021, PROC CVPR IEEE, P9477, DOI 10.1109/CVPR46437.2021.00936
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Park S, 2017, LECT NOTES COMPUT SC, V10112, P189, DOI 10.1007/978-3-319-54184-6_12
   Qi Jiaxin, 2022, Proceedings of the ECCV
   Qi Lei, 2023, PR, V140
   Rao Y., 2021, P NEURIPS
   Schreckenberger C, 2019, LECT NOTES COMPUT SC, V11877, P113, DOI 10.1007/978-3-030-33246-4_7
   Seonguk Seo, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P68, DOI 10.1007/978-3-030-58542-6_5
   Shi Baifeng, 2020, Proceedings of the ICML
   Shi QHY, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3485665
   Shu Y, 2021, PROC CVPR IEEE, P9619, DOI 10.1109/CVPR46437.2021.00950
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Tompson J, 2015, PROC CVPR IEEE, P648, DOI 10.1109/CVPR.2015.7298664
   Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347
   Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572
   Volpi Riccardo, 2018, Proceedings of the Neur IPS
   Wager Stefan, 2013, Proceedings of the Neur IPS
   Wang JD, 2023, IEEE T KNOWL DATA EN, V35, P8052, DOI 10.1109/TKDE.2022.3178128
   Wang Shujun, 2020, Proceedings of the ECCV
   Wang Xiran, 2023, Proceedings of the ICCV
   Wang Y, 2022, IEEE T CIRC SYST VID, V32, P5495, DOI 10.1109/TCSVT.2022.3152615
   Wang ZJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P814, DOI 10.1109/ICCV48922.2021.00087
   Wei GQ, 2021, PROC CVPR IEEE, P16638, DOI 10.1109/CVPR46437.2021.01637
   Wu L, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3486251
   Xu QW, 2021, PROC CVPR IEEE, P14378, DOI 10.1109/CVPR46437.2021.01415
   Xu Yifan, 2022, TOMM, V18, P1
   Yang ZZ, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3446618
   Yao XF, 2022, PROC CVPR IEEE, P7087, DOI 10.1109/CVPR52688.2022.00696
   YufeiWang Haoliang Li, 2021, Proceedings of the ACM MM
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zeng YY, 2021, PATTERN RECOGN, V120, DOI 10.1016/j.patcog.2021.108117
   Zhang Dinghuai, 2021, Proceedings of the ICML
   Zhang J., 2022, P ECCV
   Zhang J, 2022, LECT NOTES COMPUT SC, V13687, P161, DOI 10.1007/978-3-031-19812-0_10
   Zhang Jian, 2023, P ICCV
   Zhang XX, 2021, PROC CVPR IEEE, P5368, DOI 10.1109/CVPR46437.2021.00533
   Zhang YB, 2022, PROC CVPR IEEE, P8025, DOI 10.1109/CVPR52688.2022.00787
   Zhou KY, 2022, Arxiv, DOI arXiv:2103.02503
   Zhou KY, 2020, AAAI CONF ARTIF INTE, V34, P13025
   Zhou Kaiyang, 2020, Proceedings of the ICLR
NR 68
TC 1
Z9 1
U1 12
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2024
VL 20
IS 3
AR 65
DI 10.1145/3624015
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6F8
UT WOS:001153381000005
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Li, XY
   Xu, HY
   Jiang, GY
   Yu, M
   Luo, T
   Zhang, XB
   Ying, HW
AF Li, Xinyue
   Xu, Haiyong
   Jiang, Gangyi
   Yu, Mei
   Luo, Ting
   Zhang, Xuebo
   Ying, Hongwei
TI Underwater Image Quality Assessment from Synthetic to Real-world:
   Dataset and Objective Method
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Underwater image quality assessment; underwater imaging; transmission
   map
ID ENHANCEMENT; STATISTICS; COLOR
AB The complicated underwater environment and lighting conditions lead to severe influence on the quality of underwater imaging, which tends to impair underwater exploration and research. To effectively evaluate the quality of underwater images, an underwater image quality assessment dataset is constructed from synthetic to real-world, and then a new objective underwater image assessment method based on the characteristics of the underwater imaging is proposed (UICQA). Specifically, to address the lack of a publicly available datasets and more accurately quantify the quality of underwater images, a subjective underwater image quality assessment dataset from synthetic to real-world underwater images, named USRD, is constructed. Considering that the transmission map can effectively reflect the characteristics of the underwater imaging, statistical features are effectively extracted from the transmission map for distinguishing underwater images of different quality. Further, considering that the transmission map negatively correlates with scene depth, a local-to-global transmission map weighted contrast feature is constructed. Additionally, the color features of human perception and texture features based on fractal dimensions are proposed. Finally, the experimental results show that the proposed UICQA method exhibits the highest correlation with ground truth scores compared to state-of-the-art UIQA methods.
C1 [Li, Xinyue; Xu, Haiyong] Ningbo Univ, Sch Math & Stat, Ningbo, Peoples R China.
   [Jiang, Gangyi; Yu, Mei; Luo, Ting] Ningbo Univ, Fac Informat Sci & Engn, Ningbo, Peoples R China.
   [Zhang, Xuebo] Northwest Normal Univ, Lanzhou, Peoples R China.
   [Ying, Hongwei] Ningbo Univ Technol, Ningbo, Peoples R China.
C3 Ningbo University; Ningbo University; Northwest Normal University -
   China; Ningbo University of Technology
RP Li, XY (corresponding author), Ningbo Univ, Sch Math & Stat, Ningbo, Peoples R China.
EM 773257986@qq.com; xuhaiyong@nbu.edu.cn; jianggangyi@nbu.edu.cn;
   yumei@nbu.edu.cn; luoting@nbu.edu.cn; xuebo_zhang@sina.cn;
   67395166@qq.com
RI Zhang, Xuebo/ABI-2669-2020; jiang, gang/KII-8233-2024; Li,
   Xinyue/KFA-7798-2024
OI Zhang, Xuebo/0000-0001-5164-754X; Xu, Haiyong/0000-0003-1590-6799; Luo,
   Ting/0000-0003-1762-148X; Yu, Mei/0000-0003-3583-1587
FU Natural Science Foundation of China [62171243, 61871247, 62071266,
   61931022, 61671412, 62271276, 61971247]; Zhejiang Natural Science
   Foundation of China [LY19F020009, LY21F010003, LY19F010002, LQ20F010002,
   LY21F010014, LY22F020020]; Scientific Research Fund of Zhejiang
   Provincial Education Department [Y202044368]; Natural Science Foundation
   of Ningbo, China [20221JCGY010527, 2022J066, 202003N4088]
FX This work was supported in part by the Natural Science Foundation of
   China (62171243, 61871247, 62071266, 61931022, 61671412, 62271276,
   61971247), in part by the Zhejiang Natural Science Foundation of China
   (LY19F020009, LY21F010003, LY19F010002, LQ20F010002, LY21F010014,
   LY22F020020), in part by Scientific Research Fund of Zhejiang Provincial
   Education Department (Y202044368), and in part by Natural Science
   Foundation of Ningbo, China (20221JCGY010527, 2022J066, 202003N4088).
CR Berman D, 2021, IEEE T PATTERN ANAL, V43, P2822, DOI 10.1109/TPAMI.2020.2977624
   Berman Dana, 2017, P BRIT MACHINE VISIO, V1
   Chiang JY, 2012, IEEE T IMAGE PROCESS, V21, P1756, DOI 10.1109/TIP.2011.2179666
   Drews P, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P825, DOI 10.1109/ICCVW.2013.113
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Galdran A, 2015, J VIS COMMUN IMAGE R, V26, P132, DOI 10.1016/j.jvcir.2014.11.006
   Ghadiyaram D, 2017, J VISION, V17, DOI 10.1167/17.1.32
   Gibson KB, 2012, IEEE T IMAGE PROCESS, V21, P662, DOI 10.1109/TIP.2011.2166968
   Guo PF, 2023, IEEE T MULTIMEDIA, V25, P5093, DOI 10.1109/TMM.2022.3187212
   Guo PF, 2022, IEEE T MULTIMEDIA, V24, P1980, DOI 10.1109/TMM.2021.3074825
   Haoran Xu, 2012, 2012 IEEE International Conference on Information Science and Technology, P663, DOI 10.1109/ICIST.2012.6221729
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Hou G., 2022, SSRN 4089412
   Hou GJ, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3578584
   Huang SC, 2015, IEEE T IND ELECTRON, V62, P2962, DOI 10.1109/TIE.2014.2364798
   Huang SC, 2014, IEEE T CIRC SYST VID, V24, P1814, DOI 10.1109/TCSVT.2014.2317854
   Huang SR, 2023, PROC CVPR IEEE, P18145, DOI 10.1109/CVPR52729.2023.01740
   ITU-R, 2012, ITU-R Recommendation BT.500-13
   Islam MJ, 2020, Arxiv, DOI arXiv:2002.01155
   Jiang QP, 2022, IEEE T CIRC SYST VID, V32, P5959, DOI 10.1109/TCSVT.2022.3164918
   Jiang XS, 2013, IEEE IMAGE PROC, P553, DOI 10.1109/ICIP.2013.6738114
   Lasmar NE, 2009, IEEE IMAGE PROC, P2281, DOI 10.1109/ICIP.2009.5414404
   Li CY, 2020, IEEE T IMAGE PROCESS, V29, P4376, DOI 10.1109/TIP.2019.2955241
   Li CY, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107038
   Li WX, 2022, SYMMETRY-BASEL, V14, DOI 10.3390/sym14030558
   Li Yuxuan, 2023, No-reference underwater image quality assessment based on quality-aware features
   Liu YP, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3550274
   Luo ZH, 2022, EXPERT SYST APPL, V191, DOI 10.1016/j.eswa.2021.116361
   Ma L, 2012, IEEE J-STSP, V6, P626, DOI 10.1109/JSTSP.2012.2211996
   Min XK, 2018, IEEE T MULTIMEDIA, V20, P2049, DOI 10.1109/TMM.2017.2788206
   Min XK, 2017, IEEE T IMAGE PROCESS, V26, P5462, DOI 10.1109/TIP.2017.2735192
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Panetta K, 2016, IEEE J OCEANIC ENG, V41, P541, DOI 10.1109/JOE.2015.2469915
   Panetta K, 2011, IEEE T SYST MAN CY B, V41, P460, DOI 10.1109/TSMCB.2010.2058847
   Peng YT, 2018, IEEE T IMAGE PROCESS, V27, P2856, DOI 10.1109/TIP.2018.2813092
   RUDERMAN DL, 1994, NETWORK-COMP NEURAL, V5, P517, DOI 10.1088/0954-898X/5/4/006
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P1427, DOI 10.1109/TIP.2010.2042111
   Sharma P, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3511021
   Uplavikar P. M., 2019, CVPR WORKSHOPS, P1
   Varga D, 2020, J IMAGING, V6, DOI 10.3390/jimaging6080075
   Wang Y, 2018, COMPUT ELECTR ENG, V70, P904, DOI 10.1016/j.compeleceng.2017.12.006
   Wang Y, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3489520
   Wang Y, 2017, IEEE IMAGE PROC, P1382, DOI 10.1109/ICIP.2017.8296508
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Yan CG, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3468872
   Yang M, 2015, IEEE T IMAGE PROCESS, V24, P6062, DOI 10.1109/TIP.2015.2491020
   Yang N, 2021, SIGNAL PROCESS-IMAGE, V94, DOI 10.1016/j.image.2021.116218
   Zhai GT, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2757-1
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhao XW, 2015, OCEAN ENG, V94, P163, DOI 10.1016/j.oceaneng.2014.11.036
   Zheng YN, 2022, IEEE T IMAGE PROCESS, V31, P5456, DOI 10.1109/TIP.2022.3196815
NR 53
TC 1
Z9 1
U1 16
U2 16
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2024
VL 20
IS 3
AR 71
DI 10.1145/3624983
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6F8
UT WOS:001153381000011
DA 2024-08-05
ER

PT J
AU Manzoor, MA
   Albarri, S
   Xian, ZT
   Meng, ZQ
   Nakov, P
   Liang, SS
AF Manzoor, Muhammad Arslan
   Albarri, Sarah
   Xian, Ziting
   Meng, Zaiqiao
   Nakov, Preslav
   Liang, Shangsong
TI Multimodality Representation Learning: A Survey on Evolution,
   Pretraining and Its Applications
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Multimodality; representation learning; pretrained models; multimodal
   methods; multimodal applications
ID LANGUAGE; ATTENTION; FUSION
AB Multimodality Representation Learning, as a technique of learning to embed information from different modalities and their correlations, has achieved remarkable success on a variety of applications, such as Visual Question Answering (VQA), Natural Language for Visual Reasoning (NLVR), and Vision Language Retrieval (VLR). Among these applications, cross-modal interaction and complementary information from different modalities are crucial for advanced models to perform any multimodal task, e.g., understand, recognize, retrieve, or generate optimally. Researchers have proposed diverse methods to address these tasks. The different variants of transformer-based architectures performed extraordinarily on multiple modalities. This survey presents the comprehensive literature on the evolution and enhancement of deep learning multimodal architectures to deal with textual, visual and audio features for diverse cross-modal and modern multimodal tasks. This study summarizes the (i) recent task-specific deep learning methodologies, (ii) the pretraining types and multimodal pretraining objectives, (iii) from state-of-the-art pretrained multimodal approaches to unifying architectures, and (iv) multimodal task categories and possible future improvements that can be devised for better multimodal learning. Moreover, we prepare a dataset section for new researchers that covers most of the benchmarks for pretraining and finetuning. Finally, major challenges, gaps, and potential research topics are explored. A constantly-updated paperlist related to our survey is maintained at https://github.com/marslanm/multimodality-representation-learning.
C1 [Manzoor, Muhammad Arslan; Albarri, Sarah; Nakov, Preslav; Liang, Shangsong] Mohamed bin Zayed Univ Artificial Intelligence, Bldg 1B,POB 7909, Abu Dhabi, U Arab Emirates.
   [Xian, Ziting] Sun Yat Sen Univ, Univ TownWaihuan Rd 132, Guangzhou 510006, Peoples R China.
   [Meng, Zaiqiao] Univ Glasgow, Glasgow G12 8QQ, Lanark, Scotland.
C3 Mohamed Bin Zayed University of Artificial Intelligence; Sun Yat Sen
   University; University of Glasgow
RP Liang, SS (corresponding author), Mohamed bin Zayed Univ Artificial Intelligence, Bldg 1B,POB 7909, Abu Dhabi, U Arab Emirates.
EM muhammad.arslan@mbzuai.ac.ae; sarah.albarri@mbzuai.ac.ae;
   xianzt@mail2.sysu.edu.cn; zaiqiao.meng@glasgow.ac.uk;
   Preslav.nakov@mbzuai.ac.ae; Shangsong.liang@mbzuai.ac.ae
RI Liang, Shangsong/AAB-5514-2021; Nakov, Preslav/D-2421-2017
OI Liang, Shangsong/0000-0003-1625-2168; Nakov, Preslav/0000-0002-3600-1510
CR Afouras T, 2022, IEEE T PATTERN ANAL, V44, P8717, DOI 10.1109/TPAMI.2018.2889052
   Afouras T, 2018, Arxiv, DOI arXiv:1809.00496
   Agrawal A, 2018, PROC CVPR IEEE, P4971, DOI 10.1109/CVPR.2018.00522
   Agrawal H, 2019, IEEE I CONF COMP VIS, P8947, DOI 10.1109/ICCV.2019.00904
   Akbari H, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P2516, DOI 10.1109/ICASSP.2018.8461856
   Akbari S, 2021, ADV NEUR IN, V34
   Alam F., 2018, P INT AAAI C WEB SOC, V12
   Algiriyage Nilani, 2022, SN Comput Sci, V3, P92, DOI 10.1007/s42979-021-00971-4
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Atrey PK, 2010, MULTIMEDIA SYST, V16, P345, DOI 10.1007/s00530-010-0182-0
   Baevski A, 2020, Advances in neural information processing systems, V33, P12449, DOI 10.5555/3495724.3496768
   Baevski A, 2022, PR MACH LEARN RES
   Bahdanau D, 2016, Arxiv, DOI arXiv:1409.0473
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Bao H., 2022, Advances in Neural Information Pro- cessing Systems, V35, P32897
   Barrón-Cedeño A, 2019, INFORM PROCESS MANAG, V56, P1849, DOI 10.1016/j.ipm.2019.03.005
   Bayoudh K, 2022, VISUAL COMPUT, V38, P2939, DOI 10.1007/s00371-021-02166-7
   Bender EM, 2020, P 58 ANN M ASS COMPU, P5185, DOI [10.18653/v1/2020.aclmain.463, DOI 10.18653/V1/2020.ACL-MAIN.463]
   Bisk Y, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P8718
   Caesar Holger, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11618, DOI 10.1109/CVPR42600.2020.01164
   Caglayan O, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4159
   Camacho-Collados J, 2018, Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, P40, DOI 10.18653/v1/W18-5406
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Carletta J, 2005, LECT NOTES COMPUT SC, V3869, P28
   Caselli T, 2021, WOAH 2021: THE 5TH WORKSHOP ON ONLINE ABUSE AND HARMS, P17
   Chandrasekaran G, 2021, WIRES DATA MIN KNOWL, V11, DOI 10.1002/widm.1415
   Chappuis Christel, 2021, EUROPEAN C MACHINE L
   Chen FL, 2023, MACH INTELL RES, V20, P38, DOI 10.1007/s11633-022-1369-5
   Chen GZ, 2022, AAAI CONF ARTIF INTE, P5530
   Chen Guanzheng, 2022, P C EMP METH NAT LAN, P2612, DOI DOI 10.18653/V1/2022.EMNLP-MAIN.168
   Chen K, 2017, PROC CVPR IEEE, P6203, DOI 10.1109/CVPR.2017.657
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Chen M., 2020, P 28 INT C COMP LING, P1067
   Chen Q, 2021, EXPERT SYST APPL, V177, DOI 10.1016/j.eswa.2021.114939
   Chen T, 2023, IEEE T CYBERNETICS, V53, P7749, DOI 10.1109/TCYB.2022.3197127
   Chen X, 2022, Arxiv, DOI arXiv:2209.06794
   Chi ZW, 2021, 2021 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL-HLT 2021), P3576
   Chiang W-L, 2023, Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality
   Chu XX, 2021, Arxiv, DOI arXiv:2102.10882
   Chung HW, 2022, Arxiv, DOI [arXiv:2210.11416, 10.48550/arXiv.2210.11416]
   Clark K., 2020, P INT C LEARNING REP
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Courville Aaron C., 2019, VISUALLY GROUNDED IN
   Da San Martino G, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5636
   Dai WL, 2023, Arxiv, DOI arXiv:2305.06500
   Desai K, 2021, PROC CVPR IEEE, P11157, DOI 10.1109/CVPR46437.2021.01101
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dimitrov D., 2021, arXiv preprint arXiv:2109.08013, V1, P6603
   Du Y., 2022, P 31 INT JOINT C ART
   Nguyen DK, 2018, PROC CVPR IEEE, P6087, DOI 10.1109/CVPR.2018.00637
   Ektefaie Y, 2023, NAT MACH INTELL, V5, P340, DOI 10.1038/s42256-023-00624-6
   Ephrat A, 2017, INT CONF ACOUST SPEE, P5095, DOI 10.1109/ICASSP.2017.7953127
   Evangelopoulos G, 2013, IEEE T MULTIMEDIA, V15, P1553, DOI 10.1109/TMM.2013.2267205
   Fallon M, 2013, INT J ROBOT RES, V32, P1695, DOI 10.1177/0278364913509035
   Fang JY, 2021, KDD '21: PROCEEDINGS OF THE 27TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P353, DOI 10.1145/3447548.3467327
   Fang ZY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1408, DOI 10.1109/ICCV48922.2021.00146
   Fei NY, 2022, NAT COMMUN, V13, DOI 10.1038/s41467-022-30761-2
   Fengda Zhu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10009, DOI 10.1109/CVPR42600.2020.01003
   Biten AF, 2019, PROC CVPR IEEE, P12458, DOI 10.1109/CVPR.2019.01275
   Gan Z, 2022, FOUND TRENDS COMPUT, V14, P163, DOI 10.1561/0600000105
   Gao D., 2020, P IEEECVF C COMPUTER, P12746, DOI 10.1109/CVPR42600.2020.01276
   Gao JL, 2022, IEEE ACM T COMPUT BI, V19, P699, DOI 10.1109/TCBB.2021.3083566
   Gao J, 2020, NEURAL COMPUT, V32, P829, DOI 10.1162/neco_a_01273
   Gao P, 2019, PROC CVPR IEEE, P6632, DOI 10.1109/CVPR.2019.00680
   Gardner M, 2018, NLP OPEN SOURCE SOFTWARE (NLP-OSS), P1
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Gu ZX, 2022, PROC CVPR IEEE, P4573, DOI 10.1109/CVPR52688.2022.00454
   Guo WZ, 2019, IEEE ACCESS, V7, P63373, DOI 10.1109/ACCESS.2019.2916887
   Gururangan S, 2020, ACL, P8342, DOI 10.18653/v1/2020.aclmain.740
   Habibian A, 2017, IEEE T PATTERN ANAL, V39, P2089, DOI 10.1109/TPAMI.2016.2627563
   Han Xintong, 2017, Fashion 200K Benchmark
   Li LH, 2019, Arxiv, DOI [arXiv:1908.03557, DOI 10.48550/ARXIV.1908.03557]
   Harte N, 2015, IEEE T MULTIMEDIA, V17, P603, DOI 10.1109/TMM.2015.2407694
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hodosh M, 2013, J ARTIF INTELL RES, V47, P853, DOI 10.1613/jair.3994
   Hsu WN, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P6533, DOI 10.1109/ICASSP39728.2021.9414460
   Hu LM, 2017, KNOWL-BASED SYST, V138, P105, DOI 10.1016/j.knosys.2017.09.039
   Hu RH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1419, DOI 10.1109/ICCV48922.2021.00147
   Hu RH, 2019, IEEE I CONF COMP VIS, P10293, DOI 10.1109/ICCV.2019.01039
   Huang ZC, 2020, Arxiv, DOI arXiv:2004.00849
   Park DH, 2017, Arxiv, DOI arXiv:1612.04757
   Im J, 2021, 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1 (ACL-IJCNLP 2021), P388
   Jabri A, 2016, LECT NOTES COMPUT SC, V9912, P727, DOI 10.1007/978-3-319-46484-8_44
   Jain A, 2021, Arxiv, DOI arXiv:2109.05125
   Jang Y, 2017, PROC CVPR IEEE, P1359, DOI 10.1109/CVPR.2017.149
   Jaume G, 2019, PROC INT CONF DOC, P1, DOI 10.1109/ICDARW.2019.10029
   Ji JY, 2021, AAAI CONF ARTIF INTE, V35, P1655
   Jia C, 2021, PR MACH LEARN RES, V139
   Jiang WT, 2021, IEEE ACCESS, V9, P69700, DOI 10.1109/ACCESS.2021.3067607
   Jiasen Lu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10434, DOI 10.1109/CVPR42600.2020.01045
   Jin ZW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P795, DOI 10.1145/3123266.3123454
   Johnson AEW, 2016, SCI DATA, V3, DOI 10.1038/sdata.2016.35
   Kafle K, 2017, IEEE I CONF COMP VIS, P1983, DOI 10.1109/ICCV.2017.217
   Kalyan KS, 2022, J BIOMED INFORM, V126, DOI 10.1016/j.jbi.2021.103982
   Khattar D, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P2915, DOI 10.1145/3308558.3313552
   Kim EHJ, 2016, J INF SCI, V42, P763, DOI 10.1177/0165551515608733
   Kim W, 2021, PR MACH LEARN RES, V139
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lan Zhenzhong, 2019, P INT C LEARNING REP
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee J, 2020, BIOINFORMATICS, V36, P1234, DOI 10.1093/bioinformatics/btz682
   Lewis M, 2019, P 58 ANN M ASS COMP, DOI [10.18653/v1/2020.acl-main.703, DOI 10.18653/V1/2020.ACL-MAIN.703]
   Li JH, 2021, ADV NEUR IN, V34
   Li JN, 2022, PR MACH LEARN RES
   Li JN, 2023, Arxiv, DOI arXiv:2301.12597
   Li LJ, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P2046
   Li Qing, 2021, arXiv
   Li X., 2020, Lecture Notes in Computer Science, P121, DOI DOI 10.1007/978-3-030-58577-8_8
   Li Y., 2016, P INT C LEARNING REP
   Li YL, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1912, DOI 10.1145/3474085.3475345
   Liang Paul Pu, 35 C NEURAL INFORM P
   Lienhart R, 1998, PROC SPIE, V3656, P290, DOI 10.1117/12.333848
   Lin JY, 2021, Arxiv, DOI arXiv:2003.13198
   Lin JY, 2021, KDD '21: PROCEEDINGS OF THE 27TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P3251, DOI 10.1145/3447548.3467206
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Ling SS, 2020, Arxiv, DOI arXiv:2012.06659
   Liu NY, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P1834
   Liu YH, 2019, Arxiv, DOI [arXiv:1907.11692, DOI 10.48550/ARXIV.1907.11692]
   Liu Yongfei, 2022, FINDINGS ASS COMPUTA, P1589
   Long S., 2022, IJCAI, P5530, DOI DOI 10.24963/IJCAI.2022/773
   Lu JS, 2019, ADV NEUR IN, V32
   Makino T, 2019, 2019 IEEE AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING WORKSHOP (ASRU 2019), P905, DOI [10.1109/asru46091.2019.9004036, 10.1109/ASRU46091.2019.9004036]
   Marino K, 2019, PROC CVPR IEEE, P3190, DOI 10.1109/CVPR.2019.00331
   MCGURK H, 1976, NATURE, V264, P746, DOI 10.1038/264746a0
   McKeown G, 2010, IEEE INT CON MULTI, P1079, DOI 10.1109/ICME.2010.5583006
   Michelsanti D, 2021, IEEE-ACM T AUDIO SPE, V29, P1368, DOI 10.1109/TASLP.2021.3066303
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P1, DOI DOI 10.48550/ARXIV.1310.4546
   Mogadala Aditya, 2021, Journal of Artificial Intelligence Research, P1183
   Ngiam J., 2011, P 28 INT C MACH LEAR, P689
   Ni MH, 2021, PROC CVPR IEEE, P3976, DOI 10.1109/CVPR46437.2021.00397
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Palaskar S, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6587
   Panda S, 2021, 2021 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL-HLT 2021), P88
   Ping Wei, 2018, P INT C LEARNING REP
   Plummer BA, 2015, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2015.303
   Prajwal K. R., 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13793, DOI 10.1109/CVPR42600.2020.01381
   Qiao YY, 2021, IEEE T MULTIMEDIA, V23, P4426, DOI 10.1109/TMM.2020.3042066
   Qin YJ, 2022, NAACL 2022: THE 2022 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES, P3921
   Qu LY, 2019, INTERSPEECH, P2768, DOI 10.21437/Interspeech.2019-1393
   Radford, 2018, OPENAI BLOG
   Radford A, 2021, PR MACH LEARN RES, V139
   Raffel C, 2020, J MACH LEARN RES, V21
   Rahate A, 2022, INFORM FUSION, V81, P203, DOI 10.1016/j.inffus.2021.12.003
   Rahman T, 2021, IEEE COMPUT SOC CONF, P1653, DOI 10.1109/CVPRW53098.2021.00181
   Ramachandram D, 2017, IEEE SIGNAL PROC MAG, V34, P96, DOI 10.1109/MSP.2017.2738401
   Rasiwasia Nikhil, 2010, P 18 ACM INT C MULT, P251
   Reed S, 2016, PR MACH LEARN RES, V48
   Rehr R, 2018, IEEE-ACM T AUDIO SPE, V26, P357, DOI 10.1109/TASLP.2017.2778151
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Sanabria Ramon, 2018, C NEURAL INFORM PROC
   Schuller B, 2011, LECT NOTES COMPUT SC, V6975, P415, DOI 10.1007/978-3-642-24571-8_53
   Sharma P, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2556
   Shen J, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P4779, DOI 10.1109/ICASSP.2018.8461368
   Shi Bowen, 2022, P INT C LEARNING REP
   Shi Xingjian, 35 C NEURAL INFORM P
   Silberman N, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS)
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh A, 2022, PROC CVPR IEEE, P15617, DOI 10.1109/CVPR52688.2022.01519
   Song DD, 2021, KNOWL-BASED SYST, V230, DOI 10.1016/j.knosys.2021.107408
   Stappen L, 2023, IEEE T AFFECT COMPUT, V14, P1334, DOI 10.1109/TAFFC.2021.3097002
   Su JS, 2021, INFORM SCIENCES, V554, P47, DOI 10.1016/j.ins.2020.11.024
   Su W., 2020, INT C LEARNING REPRE
   Subramanian Sanjay, 2019, Advances in neural information processing systems, V7
   Summaira J., 2021, arXiv
   Tan H, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5100
   Tan H, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P2066
   Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802
   Tsimpoukelli M., 2021, NEURIPS, V34, P200
   Valstar M., 2014, P 4 INT WORKSH AUD V, P3
   Vaswani A, 2017, ADV NEUR IN, V30
   Velickovic P, 2018, INT C LEARN REPR, DOI DOI 10.48550/ARXIV.1710.10903
   Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Vougioukas K, 2019, INTERSPEECH, P4125, DOI 10.21437/Interspeech.2019-1445
   Wada S, 2021, Arxiv, DOI arXiv:2005.07202
   Wah C., 2011, Tech. Rep. CNS-TR-2011-001
   Wang JZ, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12031093
   Wang P, 2022, 39 INT C MACHINE LEA
   Wang X, 2018, PROC CVPR IEEE, P4213, DOI 10.1109/CVPR.2018.00443
   Wang YA, 2023, Arxiv, DOI arXiv:2205.11501
   Wang YQ, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P849, DOI 10.1145/3219819.3219903
   Weng WH, 2019, Arxiv, DOI arXiv:1909.09248
   Wood B, 2021, GLOBALIZATION HEALTH, V17, DOI 10.1186/s12992-021-00667-7
   Wu J, 2017, ELECTRON LETT, V53, P1642, DOI 10.1049/el.2017.3159
   Wu Q, 2017, COMPUT VIS IMAGE UND, V163, P21, DOI 10.1016/j.cviu.2017.05.001
   Xiao KJ, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12042204
   Xu DJ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1645, DOI 10.1145/3123266.3123427
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143
   Xu Y, 2021, 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (ACL-IJCNLP 2021), VOL 1, P2579
   Yen-Chun Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P104, DOI 10.1007/978-3-030-58577-8_7
   Yin YJ, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P3025
   Yu F, 2021, AAAI CONF ARTIF INTE, V35, P3208
   Yu LC, 2019, PROC CVPR IEEE, P6302, DOI 10.1109/CVPR.2019.00647
   Yu TZ, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P3995
   Zadeh A, 2016, Arxiv, DOI arXiv:1606.06259
   Zadeh A, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2236
   Zellers R, 2019, PROC CVPR IEEE, P6713, DOI 10.1109/CVPR.2019.00688
   Zeng Y, 2023, Arxiv, DOI arXiv:2206.00621
   Zhan XL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11762, DOI 10.1109/ICCV48922.2021.01157
   Zhang PC, 2021, PROC CVPR IEEE, P5575, DOI 10.1109/CVPR46437.2021.00553
   Zhang X., 2021, P 2021 SIAM INT C DA, P585
   Zhang YH, 2020, IEEE INT CONF AUTOMA, P356, DOI 10.1109/FG47880.2020.00134
   Zhao Hang, 2018, P EUR C COMP VIS ECC, P570, DOI DOI 10.1109/CVPR.2018.00374
   Zheng Huang, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P1516, DOI 10.1109/ICDAR.2019.00244
   Zhou LW, 2020, AAAI CONF ARTIF INTE, V34, P13041
   Zhou MY, 2021, PROC CVPR IEEE, P4153, DOI 10.1109/CVPR46437.2021.00414
   Zhou XY, 2020, LECT NOTES ARTIF INT, V12085, P354, DOI 10.1007/978-3-030-47436-2_27
   Zhu Lingyu, 2021, 2021 9 EUROPEAN WORK, P1
   Zhu X, 2021, INT C LEARNING REPRE
   Zhu XR, 2024, IEEE T KNOWL DATA EN, V36, P715, DOI 10.1109/TKDE.2022.3224228
   Zhuge MC, 2021, PROC CVPR IEEE, P12642, DOI 10.1109/CVPR46437.2021.01246
NR 214
TC 0
Z9 0
U1 17
U2 17
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2024
VL 20
IS 3
AR 74
DI 10.1145/3617833
PG 34
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6F8
UT WOS:001153381000014
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Sun, SQ
   Huang, DL
   Tao, XM
   Pan, CK
   Liu, GY
   Chen, CW
AF Sun, Shiqi
   Huang, Danlan
   Tao, Xiaoming
   Pan, Chengkang
   Liu, Guangyi
   Chen, Changwen
TI Boosting Scene Graph Generation with Contextual Information
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Scene graph generation; Visual relationship detection
AB Scene graph generation (SGG) has been developed to detect objects and their relationships from the visual data and has attracted increasing attention in recent years. Existing works have focused on extracting object context for SGG. However, very few works have attempted to exploit implicit contextual correlations among relationships of the objects. Furthermore, most existing SGG schemes rely on high-level features to predict the predicates while overlooking the potential inherent association of low-level features with the object relationships. We present in this article a novel scheme to capture enhanced contextual information for both objects and relationships. We design a Dual-branch Context Analysis Transformer (DCAT) architecture to extract both object context and relationship context from the visual data with dual transformer branches and then effectively fuse both high-level and low-level features by an adaptive approach to facilitate relationship prediction. Specifically, we first conduct feature representation learning to enrich relation representations by the visual, spatial, and linguistic feature extractors. Next, two transformer branches are designed to leverage the modeling of global associative interaction and mine the hidden association among objects and relationships. Then, we devise a novel feature disentangling method to decouple contextualized high-level features with guidance from the visual semantics. Finally, we develop a refined attention module to perform low-level feature recalibration for the refinement of the final predicate prediction. Experiments on Visual Genome and Action Genome datasets demonstrate the effectiveness of DCAT for both image and video SGG settings. Moreover, we also test the quality of the generated image scene graphs to verify the generalizability on downstream tasks like sentence-to-graph retrieval and image retrieval.
C1 [Sun, Shiqi; Tao, Xiaoming] Tsinghua Univ, Beijing 100084, Peoples R China.
   [Huang, Danlan] Beijing Univ Posts & Telecommun, Beijing 100876, Peoples R China.
   [Pan, Chengkang; Liu, Guangyi] China Mobile Res Inst, Beijing 100033, Peoples R China.
   [Chen, Changwen] Hong Kong Polytech Univ, Kowloon, Hong Kong, Peoples R China.
C3 Tsinghua University; Beijing University of Posts & Telecommunications;
   China Mobile; Hong Kong Polytechnic University
RP Tao, XM (corresponding author), Tsinghua Univ, Beijing 100084, Peoples R China.
EM sunsq21@mails.tsinghua.edu.cn; huangdl@bupt.edu.cn;
   taoxm@tsinghua.edu.cn; panchengkang@chinamobile.com;
   liuguangyi@chinamobile.com; chang-wen.chen@polyu.edu.hk
OI Chen, Chang Wen/0000-0002-6720-234X; Sun, Shiqi/0000-0002-8483-0110
FU National Key R&D Program of China [2018YFB1800804]; National Natural
   Science Foundation of China [NSFC 61925105, 62171257, 62101307];
   Tsinghua University-China Mobile Communications Group Co., Ltd.; Tencent
   Foundation through the XPLORER PRIZE
FX This work was supported by the National Key R&D Program of China
   (2018YFB1800804), the National Natural Science Foundation of China (NSFC
   61925105, 62171257 and 62101307) and Tsinghua University-China Mobile
   Communications Group Co., Ltd. Joint Institute. This work was also
   supported by Tencent Foundation through the XPLORER PRIZE..
CR Bai YS, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1988
   Burnaev E, 2015, PROC SPIE, V9875, DOI 10.1117/12.2228523
   Chen L, 2019, IEEE I CONF COMP VIS, P4612, DOI 10.1109/ICCV.2019.00471
   Chen XL, 2017, IEEE I CONF COMP VIS, P4106, DOI 10.1109/ICCV.2017.440
   Chen Y, 2019, IEEE INT CON MULTI, P508, DOI 10.1109/ICME.2019.00094
   Cong YR, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16352, DOI 10.1109/ICCV48922.2021.01606
   Divvala SK, 2009, PROC CVPR IEEE, P1271, DOI 10.1109/CVPRW.2009.5206532
   Gkanatsios N, 2019, IEEE INT CONF COMP V, P1754, DOI 10.1109/ICCVW.2019.00218
   Gkanatsios N, 2019, IEEE IMAGE PROC, P1840, DOI [10.1109/ICIP.2019.8803106, 10.1109/icip.2019.8803106]
   Gu JX, 2019, IEEE I CONF COMP VIS, P10322, DOI 10.1109/ICCV.2019.01042
   Gu JX, 2019, PROC CVPR IEEE, P1969, DOI 10.1109/CVPR.2019.00207
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Herzig Roei, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P210, DOI 10.1007/978-3-030-58574-7_13
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Hung ZS, 2021, IEEE T PATTERN ANAL, V43, P3820, DOI 10.1109/TPAMI.2020.2992222
   Jingwei Ji, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10233, DOI 10.1109/CVPR42600.2020.01025
   Johnson J, 2018, PROC CVPR IEEE, P1219, DOI 10.1109/CVPR.2018.00133
   Johnson J, 2015, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2015.7298990
   Kaihua Tang, 2020, A Scene Graph Generation Codebase in PyTorch
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kim JH, 2018, ADV NEUR IN, V31
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Li H, 2018, ARXIV COMPUTER VISIO
   Li L., 2010, P NIPS, V23, P1378
   Li RJ, 2021, PROC CVPR IEEE, P11104, DOI 10.1109/CVPR46437.2021.01096
   Li YK, 2018, LECT NOTES COMPUT SC, V11205, P346, DOI 10.1007/978-3-030-01246-5_21
   Li YK, 2017, IEEE I CONF COMP VIS, P1270, DOI 10.1109/ICCV.2017.142
   Li YM, 2022, PROC CVPR IEEE, P13864, DOI 10.1109/CVPR52688.2022.01350
   Li Yingying, 2019, ADV NEUR IN, V32
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Lin X, 2020, PROC CVPR IEEE, P3743, DOI 10.1109/CVPR42600.2020.00380
   Liu Y, 2018, PROC CVPR IEEE, P6985, DOI 10.1109/CVPR.2018.00730
   Lu CW, 2016, LECT NOTES COMPUT SC, V9905, P852, DOI 10.1007/978-3-319-46448-0_51
   Lu YC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15911, DOI 10.1109/ICCV48922.2021.01563
   Lyu X., 2022, P IEEECVF C COMPUTER, P19467
   Maheshwari P, 2021, AAAI CONF ARTIF INTE, V35, P2328
   Marszalek M, 2009, PROC CVPR IEEE, P2921, DOI 10.1109/CVPRW.2009.5206557
   Oliva A, 2007, TRENDS COGN SCI, V11, P520, DOI 10.1016/j.tics.2007.09.009
   Pennington J., 2014, GLOVE GLOBAL VECTORS, DOI DOI 10.3115/V1/D14-1162
   Peter Young M. H., 2014, Transactions of the Association for Computational Linguistics, V2, P67
   Qin ZJ, 2023, IEEE WIREL COMMUN, V30, P18, DOI 10.1109/MWC.013.2200553
   Qin ZJ, 2022, Arxiv, DOI [arXiv:2201.01389, DOI 10.48550/ARXIV.2201.01389]
   Reimers N, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P3982
   Ren L, 2018, IEEE INT SYM MULTIM, P73, DOI 10.1109/ISM.2018.00021
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Schroeder B, 2020, IEEE COMPUT SOC CONF, P680, DOI 10.1109/CVPRW50498.2020.00097
   Schuster Sebastian, 2015, P 4 WORKSH VIS LANG, P70, DOI DOI 10.18653/V1/W15-2812
   Suhail M, 2021, PROC CVPR IEEE, P13931, DOI 10.1109/CVPR46437.2021.01372
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tang KH, 2020, PROC CVPR IEEE, P3713, DOI 10.1109/CVPR42600.2020.00377
   Tang KH, 2019, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2019.00678
   Teng Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13668, DOI 10.1109/ICCV48922.2021.01343
   Vaswani A, 2017, ADV NEUR IN, V30
   Vo MLH, 2019, CURR OPIN PSYCHOL, V29, P205, DOI 10.1016/j.copsyc.2019.03.009
   Wang JQ, 2021, PROC CVPR IEEE, P9690, DOI 10.1109/CVPR46437.2021.00957
   Wang P, 2018, IEEE T PATTERN ANAL, V40, P2413, DOI 10.1109/TPAMI.2017.2754246
   Wang SJ, 2020, IEEE WINT CONF APPL, P1497, DOI 10.1109/WACV45572.2020.9093614
   Wang WB, 2019, PROC CVPR IEEE, P8180, DOI 10.1109/CVPR.2019.00838
   Wolfe JM, 1998, CURR BIOL, V8, pR303, DOI 10.1016/S0960-9822(98)70192-7
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu Q, 2018, IEEE T PATTERN ANAL, V40, P1367, DOI 10.1109/TPAMI.2017.2708709
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xu DF, 2017, PROC CVPR IEEE, P3097, DOI 10.1109/CVPR.2017.330
   Xu L, 2022, LECT NOTES COMPUT SC, V13687, P374, DOI 10.1007/978-3-031-19812-0_22
   Yan ST, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P265, DOI 10.1145/3394171.3413722
   Yang JW, 2018, LECT NOTES COMPUT SC, V11205, P690, DOI 10.1007/978-3-030-01246-5_41
   Yang X, 2019, PROC CVPR IEEE, P10677, DOI 10.1109/CVPR.2019.01094
   Yao BP, 2010, PROC CVPR IEEE, P17, DOI 10.1109/CVPR.2010.5540235
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yoon S, 2021, AAAI CONF ARTIF INTE, V35, P10718
   Yu Jing, 2020, P 29 INT JOINT C ART, P1274
   Zareian Alireza, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P606, DOI 10.1007/978-3-030-58592-1_36
   Zellers R, 2018, PROC CVPR IEEE, P5831, DOI 10.1109/CVPR.2018.00611
   Zhang HW, 2017, PROC CVPR IEEE, P3107, DOI 10.1109/CVPR.2017.331
   Zhang J, 2019, PROC CVPR IEEE, P11527, DOI 10.1109/CVPR.2019.01180
   Zhang XY, 2021, PROC CVPR IEEE, P15460, DOI 10.1109/CVPR46437.2021.01521
   Zhang Y, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3514041
   Zhao QJ, 2019, AAAI CONF ARTIF INTE, P9259
   Zhong YW, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1803, DOI 10.1109/ICCV48922.2021.00184
NR 80
TC 0
Z9 0
U1 4
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2024
VL 20
IS 2
SI SI
AR 54
DI 10.1145/3615868
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W6GR4
UT WOS:001092595800024
OA hybrid
DA 2024-08-05
ER

PT J
AU Wu, SX
   Sang, JT
   Xu, KY
   Zheng, GH
   Xu, CS
AF Wu, Shangxi
   Sang, Jitao
   Xu, Kaiyan
   Zheng, Guanhua
   Xu, Changsheng
TI Adaptive Adversarial Logits Pairing
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Adversarial defense; adaptive; dropout
ID NEURAL-NETWORKS; ROBUSTNESS
AB Adversarial examples provide an opportunity as well as impose a challenge for understanding image classification systems. Based on the analysis of the adversarial training solution-Adversarial Logits Pairing (ALP), we observed in this work that: (1) The inference of adversarially robust model tends to rely on fewer high-contribution features compared with vulnerable ones. (2) The training target of ALP does not fit well to a noticeable part of samples, where the logits pairing loss is overemphasized and obstructs minimizing the classification loss. Motivated by these observations, we design an Adaptive Adversarial Logits Pairing (AALP) solution by modifying the training process and training target of ALP. Specifically, AALP consists of an adaptive feature optimization module with Guided Dropout to systematically pursue fewer high-contribution features, and an adaptive sample weighting module by setting sample-specific training weights to balance between logits pairing loss and classification loss. The proposed AALP solution demonstrates superior defense performance on multiple datasets with extensive experiments.
C1 [Wu, Shangxi; Sang, Jitao; Xu, Kaiyan] Beijing Jiaotong Univ, Beijing Key Lab Traff Data Anal & Min, Beijing, Peoples R China.
   [Sang, Jitao] Tianjin Normal Univ, Tianjin, Peoples R China.
   [Zheng, Guanhua] Univ Sci & Technol China, Beijing, Peoples R China.
   [Xu, Changsheng] Chinese Acad Sci, Inst Automat, Beijing, Peoples R China.
C3 Beijing Jiaotong University; Tianjin Normal University; Chinese Academy
   of Sciences; University of Science & Technology of China, CAS; Chinese
   Academy of Sciences; Institute of Automation, CAS
RP Wu, SX (corresponding author), Beijing Jiaotong Univ, Beijing Key Lab Traff Data Anal & Min, Beijing, Peoples R China.
EM wushangxi@bjtu.edu.cn; jtsang@bjtu.edu.cn; wscjxky@gmail.com;
   zhenggh@mail.ustc.edu.cn; csxu@nlpr.ia.ac.cn
OI Zheng, Guanhua/0000-0002-6878-971X
FU Fundamental Research Funds for the Central Universities [2023JBZY033];
   National Natural Science Foundation of China [61832002, 62172094];
   Beijing Natural Science Foundation [JQ20023]; CCF-Zhipu AI Large Model
   Fund
FX This work is supported by the Fundamental Research Funds for the Central
   Universities (Grant No. 2023JBZY033), the National Natural Science
   Foundation of China (Grants No. 61832002 and No. 62172094), the Beijing
   Natural Science Foundation (Grant No. JQ20023), and CCF-Zhipu AI Large
   Model Fund.
CR Amini S, 2020, IEEE T MULTIMEDIA, V22, P1889, DOI 10.1109/TMM.2020.2969784
   Athalye A, 2018, PR MACH LEARN RES, V80
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Carlini N, 2018, 2018 IEEE SYMPOSIUM ON SECURITY AND PRIVACY WORKSHOPS (SPW 2018), P1, DOI 10.1109/SPW.2018.00009
   Choe J, 2019, PROC CVPR IEEE, P2214, DOI 10.1109/CVPR.2019.00232
   Dhillon G., 2018, Stochastic activation pruning for robust adversarial defense, P1
   Du YL, 2019, IEEE T MULTIMEDIA, V21, P555, DOI 10.1109/TMM.2018.2887018
   Ebrahimi J, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 2, P31
   Feinman Reuben, 2017, ARXIV170300410
   Goodfellow IJ, 2015, P 3 INT C LEARNING R
   Guo Chuan, 2017, INT C LEARNING REPRE
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jin GQ, 2019, INT CONF ACOUST SPEE, P3842, DOI [10.1109/icassp.2019.8683044, 10.1109/ICASSP.2019.8683044]
   Kannan H., 2018, ARXIV180306373
   Kingma D. P., 2014, arXiv
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Kurakin A., 2018, Artificial Intelligence Safety and Security, P99, DOI DOI 10.1201/9781351251389-8
   LeCun Y, 1999, LECT NOTES COMPUT SC, V1681, P319, DOI 10.1007/3-540-46805-6_19
   Liao FZ, 2018, PROC CVPR IEEE, P1778, DOI 10.1109/CVPR.2018.00191
   Lin YT, 2019, IEEE ACCESS, V7, P99441, DOI 10.1109/ACCESS.2019.2930550
   Liu CH, 2019, Arxiv, DOI arXiv:1810.02424
   Liu YY, 2019, IEEE IMAGE PROC, P4644, DOI [10.1109/icip.2019.8803618, 10.1109/ICIP.2019.8803618]
   Madry A, 2019, Arxiv, DOI arXiv:1706.06083
   Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282
   Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Sharif M, 2019, Arxiv, DOI arXiv:1801.00349
   Song Y, 2018, Arxiv, DOI arXiv:1710.10766
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Su Z, 2019, IEEE T MULTIMEDIA, V21, P537, DOI 10.1109/TMM.2019.2899279
   Szegedy C, 2014, Arxiv, DOI arXiv:1312.6199
   TramŠr F, 2020, Arxiv, DOI arXiv:1705.07204
   Wang K, 2011, IEEE I CONF COMP VIS, P1457, DOI 10.1109/ICCV.2011.6126402
   Wang YL, 2020, IEEE T MULTIMEDIA, V22, P1796, DOI 10.1109/TMM.2019.2949872
   Xie CH, 2017, IEEE I CONF COMP VIS, P1378, DOI 10.1109/ICCV.2017.153
   Zhang HY, 2019, PR MACH LEARN RES, V97
NR 36
TC 0
Z9 0
U1 4
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2024
VL 20
IS 2
SI SI
AR 56
DI 10.1145/3616375
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W6GR4
UT WOS:001092595800026
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Yang, CN
   Wu, X
   Chung, MJ
AF Yang, Ching-Nung
   Wu, Xiaotian
   Chung, Min-Jung
TI Enhancement of Information Carrying and Decoding for Visual Cryptography
   with Error Correction
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Secret sharing; visual cryptography; error correction; carrying
   information; decoding
ID SHARING SCHEME
AB Recently, three visual cryptography schemes with t-error-correcting capability (VCSs-tEC) were introduced for preventing the shadows carrying additional information from being corrupted by noise interference. However, the concerns on VCS-tEC, such as the average amount of carrying information, decoding of information from shadows, and the demonstration way of a secret, should be considered and improved. In this article, two schemes, namely the (k, n) probabilistic VCS-tEC (PVCS-tEC) and the ( 2, n) deterministic VCS-tEC (DVCS-tEC), are proposed. The concept of probabilistic VCS is combined with the Bose-Chaudhuri-Hocquenghem code (BCH code) for designing the (k, n) PVCS-tEC. Furthermore, some constant-weight BCH codewords are adopted to build the ( 2, n)-DVCS-tEC. Comprehensive results and experiments are demonstrated to clarify the enhancement of information-carrying and decoding by the two proposed methods.
C1 [Yang, Ching-Nung; Chung, Min-Jung] Natl Dong Hwa Univ, Dept Comp Sci & Informat Engn, 1 Sec 2,Hsueh Rd,Shou Feng, Hualien 97401, Taiwan.
   [Wu, Xiaotian] Jinan Univ, Dept Comp Sci, 601 Huangpu West Ave, Guangzhou 510632, Peoples R China.
C3 National Dong Hwa University; Jinan University
RP Wu, X (corresponding author), Jinan Univ, Dept Comp Sci, 601 Huangpu West Ave, Guangzhou 510632, Peoples R China.
EM cnyang@gms.ndhu.edu.tw; wxt.sysu@gmail.com; 610921224@gms.ndhu.edu.tw
RI Yang, Ching-Nung/HKV-1639-2023
FU National Natural Science Foundation of China [61972179]; Guangdong Basic
   and Applied Basic Research Foundation [2020A1515011476]; National
   Science and Technology Council [112-2221-E-259-007-MY2]
FX This work was partially supported by National Natural Science Foundation
   of China (Grant No. 61972179), Guangdong Basic and Applied Basic
   Research Foundation (Grant No. 2020A1515011476), and National Science
   and Technology Council (Grant No. 112-2221-E-259-007-MY2).
CR BOSE B, 1982, IEEE T COMPUT, V31, P521, DOI 10.1109/TC.1982.1676034
   Chen B, 2022, IEEE T DEPEND SECURE, V19, P978, DOI 10.1109/TDSC.2020.3011923
   Chih-Ching Thien, 2002, Computers & Graphics, V26, P765, DOI 10.1016/S0097-8493(02)00131-0
   Cimato S, 2006, COMPUT J, V49, P97, DOI 10.1093/comjnl/bxh152
   Eisen PA, 2002, DESIGN CODE CRYPTOGR, V25, P15, DOI 10.1023/A:1012504516447
   Hou YC, 2003, PATTERN RECOGN, V36, P1619, DOI 10.1016/S0031-3203(02)00258-3
   Jia XX, 2022, INFORM SCIENCES, V595, P54, DOI 10.1016/j.ins.2022.02.016
   Jia XX, 2018, IEEE T CIRC SYST VID, V28, P1056, DOI 10.1109/TCSVT.2016.2631404
   Kasyap H, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3426474
   KUNDU S, 1990, IEEE T COMPUT, V39, P752, DOI 10.1109/12.53596
   Laih CS, 1996, IEEE T COMPUT, V45, P103, DOI 10.1109/12.481491
   Lathey A, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2656205
   Li P, 2018, SIGNAL PROCESS-IMAGE, V65, P210, DOI 10.1016/j.image.2018.04.002
   Li P, 2012, J VIS COMMUN IMAGE R, V23, P441, DOI 10.1016/j.jvcir.2012.01.003
   Lin SJ, 2007, PATTERN RECOGN, V40, P3652, DOI 10.1016/j.patcog.2007.04.001
   Liu YX, 2017, SIGNAL PROCESS-IMAGE, V58, P49, DOI 10.1016/j.image.2017.06.011
   Liu ZQ, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3508394
   Liu ZQ, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3418212
   Naor M., 1995, Advances in Cryptology - EUROCRYPT '94. Workshop on the Theory and Application of Cryptographic Techniques. Proceedings, P1, DOI 10.1007/BFb0053419
   PETERSON WW, 1960, IRE T INFORM THEOR, V6, P459, DOI 10.1109/TIT.1960.1057586
   Puteaux P, 2023, IEEE T CIRC SYST VID, V33, P3030, DOI 10.1109/TCSVT.2022.3225644
   SHAMIR A, 1979, COMMUN ACM, V22, P612, DOI 10.1145/359168.359176
   Shen G, 2017, DESIGN CODE CRYPTOGR, V85, P15, DOI 10.1007/s10623-016-0285-5
   Shyu SJ, 2015, IEEE T CIRC SYST VID, V25, P1557, DOI 10.1109/TCSVT.2015.2389372
   Verheul E. R., 1997, Designs, Codes and Cryptography, V11, P179, DOI 10.1023/A:1008280705142
   Weir J, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2344436.2344438
   Wu XT, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3517140
   Wu XT, 2019, J VIS COMMUN IMAGE R, V61, P74, DOI 10.1016/j.jvcir.2019.03.020
   Xia ZH, 2024, IEEE T COMPUT, V73, P340, DOI 10.1109/TC.2021.3073171
   Xiong LZ, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3512797
   Yan XH, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3419750
   Yan XH, 2020, IEEE T INF FOREN SEC, V15, P3848, DOI 10.1109/TIFS.2020.3001735
   Yan XH, 2017, DIGIT SIGNAL PROCESS, V71, P36, DOI 10.1016/j.dsp.2017.08.006
   Yang CN, 2021, IEEE T CIRC SYST VID, V31, P2465, DOI 10.1109/TCSVT.2020.3017126
   Yang CN, 2014, INFORM SCIENCES, V278, P141, DOI 10.1016/j.ins.2014.03.033
   Yang CN, 2014, IEEE T CIRC SYST VID, V24, P189, DOI 10.1109/TCSVT.2013.2276708
   Yang CN, 2004, PATTERN RECOGN LETT, V25, P481, DOI 10.1016/j.patrec.2003.12.011
NR 37
TC 2
Z9 2
U1 2
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2024
VL 20
IS 1
AR 23
DI 10.1145/3612927
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T8LL3
UT WOS:001080441800023
DA 2024-08-05
ER

PT J
AU Peng, JJ
   Song, PP
   Li, H
   Wang, HB
AF Peng, Jinjia
   Song Pengpeng
   Li, Hui
   Wang, Huibing
TI ReFID: Reciprocal Frequency-aware Generalizable Person Re-identification
   via Decomposition and Filtering
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Domain generalization; person re-identification; frequency domain
   learning
ID NETWORK
AB Domain generalization of person re-identification aims to conduct testing across domains that have not been previously encountered, without utilizing target domain data during the training stage. As the number of source domains increases, the relationships between training samples become more complex. This can lead to domain-invariant features that include certain instance-level spurious correlations, which can impact the model's ability to generalize further. To overcome this limitation, the Reciprocal Frequency-aware Generalizable Person Re-identification method is proposed in this article, which aims to utilize spectral feature correlation learning to transmit frequency component information and generate more discriminative hybrid features. A module called Bilateral Frequency Component-guided Attention is developed to help the network understand high-level semantic and texture information from various frequency features. Furthermore, to reduce the impact of noise from the frequency domain, this article proposes an innovative module called Fourier Noise Masquerade Filtering. This module enhances the portability of frequency domain components while simultaneously suppressing elements that do not contribute to generalization. Extensive experimental results on various datasets demonstrate that our method is effective and superior to the state-of-the-art methods.
C1 [Peng, Jinjia; Song Pengpeng; Li, Hui] Hebei Univ, Sch Cyber Secur & Comp, Baoding 071000, Hebei, Peoples R China.
   [Wang, Huibing] Dalian Maritime Univ, Coll Informat Sci & Technol, Dalian 116026, Liaoning, Peoples R China.
C3 Hebei University; Dalian Maritime University
RP Li, H (corresponding author), Hebei Univ, Sch Cyber Secur & Comp, Baoding 071000, Hebei, Peoples R China.; Wang, HB (corresponding author), Dalian Maritime Univ, Coll Informat Sci & Technol, Dalian 116026, Liaoning, Peoples R China.
EM pengjinjia@hbu.edu.cn; songpengpeng@stumail.hbu.edu.cn;
   lihui15794@hbu.edu.cn; huibing.wang@dlmu.edu.cn
FU Central Government Guides Local Science and Technology Development Fund
   Projects [236Z0301G]; Science Research Project of Hebei Education
   Department [QN2023186]; National Natural Science Foundation of China
   [62002041]; Dalian Science and Technology Bureau [2022JJ12GX019]
FX This work is supported by Central Government Guides Local Science and
   Technology Development Fund Projects (Grant No. 236Z0301G); Science
   Research Project of Hebei Education Department (Grant No. QN2023186);
   National Natural Science Foundation of China (Grant No. 62002041), and
   Dalian Science and Technology Bureau (Grant No. 2022JJ12GX019).
CR Bai Y, 2021, PROC CVPR IEEE, P2123, DOI 10.1109/CVPR46437.2021.00216
   Bu QW, 2023, IEEE I CONF COMP VIS, P4379, DOI 10.1109/ICCV51070.2023.00406
   Chang WG, 2019, PROC CVPR IEEE, P7346, DOI 10.1109/CVPR.2019.00753
   Chen Guangyao, 2021, P IEEECVF INT C COMP, P458
   Chen PX, 2021, AAAI CONF ARTIF INTE, V35, P1054
   Chen YB, 2019, IEEE I CONF COMP VIS, P232, DOI 10.1109/ICCV.2019.00032
   Cheng H, 2023, IEEE I CONF COMP VIS, P11780, DOI 10.1109/ICCV51070.2023.01085
   Choi S, 2021, PROC CVPR IEEE, P3424, DOI 10.1109/CVPR46437.2021.00343
   Dai YX, 2021, PROC CVPR IEEE, P16140, DOI 10.1109/CVPR46437.2021.01588
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dou ZP, 2023, IEEE I CONF COMP VIS, P15801, DOI 10.1109/ICCV51070.2023.01452
   Ge YX, 2024, IEEE T NEUR NET LEAR, V35, P258, DOI 10.1109/TNNLS.2022.3173489
   Gray D, 2008, LECT NOTES COMPUT SC, V5302, P262, DOI 10.1007/978-3-540-88682-2_21
   Guo J, 2018, Arxiv, DOI arXiv:1809.02256
   Han J, 2022, AAAI CONF ARTIF INTE, P790
   Han K, 2022, AAAI CONF ARTIF INTE, P817
   Haohan Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8681, DOI 10.1109/CVPR42600.2020.00871
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Hirzer M, 2011, LECT NOTES COMPUT SC, V6688, P91, DOI 10.1007/978-3-642-21227-7_9
   Huang JX, 2021, PROC CVPR IEEE, P6887, DOI 10.1109/CVPR46437.2021.00682
   Huang Y, 2019, IEEE I CONF COMP VIS, P9526, DOI 10.1109/ICCV.2019.00962
   Izmailov P., 2022, Advances in Neural Information Processing Systems, V35, P38516
   Jiang GQ, 2021, NEUROCOMPUTING, V427, P225, DOI 10.1016/j.neucom.2020.07.132
   Jiao BL, 2022, LECT NOTES COMPUT SC, V13674, P285, DOI 10.1007/978-3-031-19781-9_17
   Jin X, 2020, PROC CVPR IEEE, P3140, DOI 10.1109/CVPR42600.2020.00321
   Joo HT, 2019, IEEE CONF COMPU INTE
   Kirichenko Polina, 2022, arXiv
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Li W, 2013, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2013.461
   Lin CS, 2021, INT C PATT RECOG, P6758, DOI 10.1109/ICPR48806.2021.9413013
   Lin JL, 2022, IEEE T IMAGE PROCESS, V31, P3780, DOI 10.1109/TIP.2022.3175601
   Liu JW, 2022, AAAI CONF ARTIF INTE, P1729
   Liu JW, 2019, PROC CVPR IEEE, P7195, DOI 10.1109/CVPR.2019.00737
   Liu XB, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P547, DOI 10.1145/3394171.3413904
   Loy CC, 2010, INT J COMPUT VISION, V90, P106, DOI 10.1007/s11263-010-0347-5
   Ni H., 2023, IEEECVF INT CONFCOMP, P11280
   Shengcai Liao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P456, DOI 10.1007/978-3-030-58621-8_27
   Sun YF, 2021, IEEE T PATTERN ANAL, V43, P902, DOI 10.1109/TPAMI.2019.2938523
   Wang HB, 2024, IEEE T NEUR NET LEAR, V35, P10121, DOI 10.1109/TNNLS.2023.3239033
   Wang HB, 2021, IEEE T MULTIMEDIA, V23, P3828, DOI 10.1109/TMM.2020.3032023
   Wang HB, 2020, IEEE MULTIMEDIA, V27, P112, DOI 10.1109/MMUL.2020.2999464
   Wang HB, 2020, IEEE T VEH TECHNOL, V69, P10484, DOI 10.1109/TVT.2020.3009162
   Wang HB, 2016, NEUROCOMPUTING, V216, P286, DOI 10.1016/j.neucom.2016.07.044
   Wang T, 2022, AAAI CONF ARTIF INTE, P2540
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Wu YH, 2022, AAAI CONF ARTIF INTE, P2750
   Xiao T., 2016, arXiv
   Xu BQ, 2022, LECT NOTES COMPUT SC, V13674, P372, DOI 10.1007/978-3-031-19781-9_22
   Xu BQ, 2022, IEEE T IMAGE PROCESS, V31, P4651, DOI 10.1109/TIP.2022.3186759
   Xu K, 2020, PROC CVPR IEEE, P1737, DOI 10.1109/CVPR42600.2020.00181
   Xu QW, 2021, PROC CVPR IEEE, P14378, DOI 10.1109/CVPR46437.2021.01415
   Xu ZQJ, 2024, Arxiv, DOI arXiv:1901.06523
   Xu ZQJ, 2019, LECT NOTES COMPUT SC, V11953, P264, DOI 10.1007/978-3-030-36708-4_22
   Yang Y., 2020, 2020 IEEE C COMP VIS, P9008, DOI [10.1109/cvpr42600.2020.00903, DOI 10.1109/CVPR42600.2020.00903]
   Yang YC, 2020, PROC CVPR IEEE, P4084, DOI 10.1109/CVPR42600.2020.00414
   Yang Zou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P87, DOI 10.1007/978-3-030-58536-5_6
   Yunpeng Zhai, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9018, DOI 10.1109/CVPR42600.2020.00904
   Zhao YY, 2021, PROC CVPR IEEE, P6273, DOI 10.1109/CVPR46437.2021.00621
   Zheng KC, 2021, PROC CVPR IEEE, P5306, DOI 10.1109/CVPR46437.2021.00527
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng Wei-Shi, 2009, BRIT MACH VIS C, P1, DOI 10.5244/C.23.23
   Zheng ZD, 2024, IEEE T NEUR NET LEAR, V35, P7534, DOI 10.1109/TNNLS.2022.3214834
   Zheng ZD, 2021, IEEE T MULTIMEDIA, V23, P2683, DOI 10.1109/TMM.2020.3014488
   Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI 10.1109/CVPR.2019.00224
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2019, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2019.00069
   Zhu Hancheng, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P6794, DOI 10.1145/3581783.3611942
   Zhu HC, 2023, IEEE T MULTIMEDIA, V25, P179, DOI 10.1109/TMM.2021.3123468
   Zijie Zhuang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P140, DOI 10.1007/978-3-030-58610-2_9
NR 72
TC 1
Z9 1
U1 2
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2024
VL 20
IS 7
SI SI
AR 205
DI 10.1145/3643684
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL0Q6
UT WOS:001234494100020
DA 2024-08-05
ER

PT J
AU Cheng, YH
   Yan, YC
   Zhu, WH
   Pan, Y
   Pan, BW
   Yang, XK
AF Cheng, Yuhao
   Yan, Yichao
   Zhu, Wenhan
   Pan, Ye
   Pan, Bowen
   Yang, Xiaokang
TI Head3D: Complete 3D Head Generation via Tri-plane Feature Distillation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Head generation; neural radiance field; adversarial generative network;
   limited data
AB Head generation with diverse identities is an important task in computer vision and computer graphics, widely used in multimedia applications. However, current full-head generation methods require a large number of three-dimensional (3D) scans or multi-view images to train the model, resulting in expensive data acquisition costs. To address this issue, we propose Head3D, a method to generate full 3D heads with limited multi-view images. Specifically, our approach first extracts facial priors represented by tri-planes learned in EG3D, a 3D-aware generative model, and then proposes feature distillation to deliver the 3D frontal faces within complete heads without compromising head integrity. To mitigate the domain gap between the face and head models, we present a dual-discriminator to guide the frontal and back head generation. Our model achieves cost-efficient and diverse complete head generation with photo-realistic renderings and high-quality geometry representations. Extensive experiments demonstrate the effectiveness of our proposed Head3D, both qualitatively and quantitatively.
C1 [Cheng, Yuhao; Yan, Yichao; Zhu, Wenhan; Yang, Xiaokang] Shanghai Jiao Tong Univ, AI Inst, Key Lab Artificial Intelligence, MoE, 800 Dongchuan Rd, Shanghai, Shanghai, Peoples R China.
   [Pan, Ye] Shanghai Jiao Tong Univ, John Hopcroft Ctr Comp Sci, 800 Dongchuan Rd, Shanghai, Shanghai, Peoples R China.
   [Pan, Bowen] Alibaba Grp, 969 West Wen Yi Rd, Hangzhou, Zhejiang, Peoples R China.
C3 Shanghai Jiao Tong University; Shanghai Jiao Tong University; Alibaba
   Group
RP Yan, YC (corresponding author), Shanghai Jiao Tong Univ, AI Inst, Key Lab Artificial Intelligence, MoE, 800 Dongchuan Rd, Shanghai, Shanghai, Peoples R China.
EM chengyuhao@sjtu.edu.cn; yanyichao@sjtu.edu.cn; zhuwenhan823@sjtu.edu.cn;
   whitneypanye@sjtu.edu.cn; bowen.pbw@alibaba-inc.com; xkyang@sjtu.edu.cn
FU National Natural Science Foundation of China [62201342, 62101325,
   U19B2035]; Shanghai Municipal Science and Technology Major Project
   [2021SHZDZX0102]; CCF-Alibaba Innovative Research Fund For Young
   Scholars
FX This article was supported by the National Natural Science Foundation of
   China (grant nos. 62201342, 62101325, and U19B2035), the Shanghai
   Municipal Science and Technology Major Project (grant no.
   2021SHZDZX0102), and the CCF-Alibaba Innovative Research Fund For Young
   Scholars.
CR An SZ, 2023, PROC CVPR IEEE, P20950, DOI 10.1109/CVPR52729.2023.02007
   Bińkowski M, 2021, Arxiv, DOI [arXiv:1801.01401, 10.48550/arXiv.1801.0140132, 10.48550/arXiv.1801.01401]
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Booth J, 2017, PROC CVPR IEEE, P5464, DOI 10.1109/CVPR.2017.580
   Booth J, 2016, PROC CVPR IEEE, P5543, DOI 10.1109/CVPR.2016.598
   Burkov E, 2023, Arxiv, DOI arXiv:2209.04436
   Cao C, 2014, IEEE T VIS COMPUT GR, V20, P413, DOI 10.1109/TVCG.2013.249
   Chan ER, 2022, PROC CVPR IEEE, P16102, DOI 10.1109/CVPR52688.2022.01565
   Chan ER, 2021, PROC CVPR IEEE, P5795, DOI 10.1109/CVPR46437.2021.00574
   Chen AP, 2022, LECT NOTES COMPUT SC, V13692, P333, DOI 10.1007/978-3-031-19824-3_20
   Chen HT, 2020, AAAI CONF ARTIF INTE, V34, P3585
   Cho JH, 2019, IEEE I CONF COMP VIS, P4793, DOI 10.1109/ICCV.2019.00489
   Dai H, 2017, IEEE I CONF COMP VIS, P3104, DOI 10.1109/ICCV.2017.335
   DaoyeWang Prashanth Chandran, 2022, ACM SIGGRAPH 2022 Conference Proceedings, P1
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Deng Y, 2019, IEEE COMPUT SOC CONF, P285, DOI 10.1109/CVPRW.2019.00038
   Feng Y, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459936
   Gal R, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530164
   Genova K, 2020, PROC CVPR IEEE, P4856, DOI 10.1109/CVPR42600.2020.00491
   Gerig T, 2018, IEEE INT CONF AUTOMA, P75, DOI 10.1109/FG.2018.00021
   Giebenhain S, 2023, Arxiv, DOI arXiv:2212.02761
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Grassal PW, 2022, PROC CVPR IEEE, P18632, DOI 10.1109/CVPR52688.2022.01810
   Gu Jiatao, 2022, ICLR
   Guodong Xu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P588, DOI 10.1007/978-3-030-58545-7_34
   Hensel M, 2017, ADV NEUR IN, V30
   Hinton G., 2015, NEURIPS DEEP LEARN R, P9
   Ho TT, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3396237
   Hong Y, 2022, PROC CVPR IEEE, P20342, DOI 10.1109/CVPR52688.2022.01973
   Hou L, 2021, AAAI CONF ARTIF INTE, V35, P7746
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T., 2020, Advances in neural information processing systems, V33, P12104, DOI DOI 10.48550/ARXIV.2006.06676
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Kingma D. P., 2014, arXiv
   Li MY, 2020, PROC CVPR IEEE, P5283, DOI 10.1109/CVPR42600.2020.00533
   Li TY, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130813
   Li YC, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3414838
   Liu SG, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3571746
   Liu YC, 2021, PROC CVPR IEEE, P12151, DOI 10.1109/CVPR46437.2021.01198
   Meng Q, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6331, DOI 10.1109/ICCV48922.2021.00629
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Niemeyer M, 2021, PROC CVPR IEEE, P11448, DOI 10.1109/CVPR46437.2021.01129
   OrEl R, 2022, PROC CVPR IEEE, P13493, DOI 10.1109/CVPR52688.2022.01314
   Park W, 2019, PROC CVPR IEEE, P3962, DOI 10.1109/CVPR.2019.00409
   Patel A, 2009, PROC CVPR IEEE, P1327, DOI 10.1109/CVPRW.2009.5206522
   Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58
   Ploumpis S, 2021, IEEE T PATTERN ANAL, V43, P4142, DOI 10.1109/TPAMI.2020.2991150
   Ploumpis S, 2019, PROC CVPR IEEE, P10926, DOI 10.1109/CVPR.2019.01119
   Ramon E, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5600, DOI 10.1109/ICCV48922.2021.00557
   Romero A, 2015, Arxiv, DOI arXiv:1412.6550
   Schwarz Katja, 2020, Advances in Neural Information Processing Systems
   Shen YJ, 2022, IEEE T PATTERN ANAL, V44, P2004, DOI 10.1109/TPAMI.2020.3034267
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sitzmann V, 2020, Adv. Neural. Inf. Process. Syst, V33, P7462
   Skorokhodov Ivan, 2022, CoRR abs/2206.10535
   Sun JX, 2022, PROC CVPR IEEE, P7662, DOI 10.1109/CVPR52688.2022.00752
   TengfeiWang Bo Zhang, 2022, Rodin: A Generative Model for Sculpting 3D Digital Avatars Using Diffusion
   Tian Y., 2020, ICLR
   Tran L, 2019, PROC CVPR IEEE, P1126, DOI 10.1109/CVPR.2019.00122
   Wang Huiyu, 2020, ECCV
   Wang XP, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3355397
   Wu SJ, 2023, PROC CVPR IEEE, P437, DOI 10.1109/CVPR52729.2023.00050
   Wu Yi, 2023, Proceedings of the 4th International Conference on Advances in Civil and Ecological Engineering Research: ACEER2022. Lecture Notes in Civil Engineering (292), P327, DOI 10.1007/978-981-19-5783-3_26
   Xiang JF, 2023, IEEE I CONF COMP VIS, P2195, DOI 10.1109/ICCV51070.2023.00209
   Xu GD, 2022, Arxiv, DOI arXiv:2208.08840
   Xu YH, 2022, PROC CVPR IEEE, P18409, DOI 10.1109/CVPR52688.2022.01788
   Xue H, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3571857
   Xue Y, 2022, PROC CVPR IEEE, P18419, DOI 10.1109/CVPR52688.2022.01789
   Yenamandra T, 2021, PROC CVPR IEEE, P12798, DOI 10.1109/CVPR46437.2021.01261
   Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20
   Yu H, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3514248
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang XM, 2022, PROC CVPR IEEE, P18429, DOI 10.1109/CVPR52688.2022.01790
   Zhang YZ, 2020, APPL POWER ELECT CO, P658, DOI [10.1109/apec39645.2020.9124291, 10.1007/978-3-030-58529-7_39]
   Zhou Peng, 2021, CoRR abs/2110.09788
NR 75
TC 0
Z9 0
U1 1
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2024
VL 20
IS 6
SI SI
AR 176
DI 10.1145/3635717
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OQ2T3
UT WOS:001208681800026
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Dang, YJ
   Huang, CX
   Chen, P
   Zhao, DD
   Gao, N
   Liang, RH
   Huan, RH
AF Dang, Yuanjie
   Huang, Chunxia
   Chen, Peng
   Zhao, Dongdong
   Gao, Nan
   Liang, Ronghua
   Huan, Ruohong
TI Discriminative Action Snippet Propagation Network for Weakly Supervised
   Temporal Action Localization
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Temporal action localization; weakly supervised; contrastive learning;
   cross attention; pseudo labels; feature propagation
ID MODEL
AB Weakly supervised temporal action localization (WTAL) aims to classify and localize actions in untrimmed videos with only video-level labels. Recent studies have attempted to obtain more accurate temporal boundaries by exploiting latent action instances in ambiguous snippets or propagating representative action features. However, empirically handcrafted ambiguous snippet extraction and the imprecise alignment of representative snippet propagation lead to challenges in modeling the completeness of actions for these methods. In this article, we propose a Discriminative Action Snippet Propagation Network (DASP-Net) to accurately discover ambiguous snippets in videos and propagate discriminative instance-level features throughout the video for improving action completeness. Specifically, we introduce a novel discriminative feature propagation module for capturing the global contextual attention and propagating the action concept across the whole video by perceiving the discriminative action snippets with instance information from the same video. Simultaneously, we incorporate denoised pseudo-labels as supervision, where we correct the controversial prediction based on the feature space distribution during training, thereby alleviating false detection caused by noise background features. Furthermore, we design an ambiguous feature mining module, which maximizes the feature affinity information of action and background in ambiguous snippets to generate more accurate latent action and background snippets and learns more precise action instance boundaries through contrastive learning of action and background snippets. Extensive experiments show that DASP-Net achieves state-of-the-art results on THUMOS14 and ActivityNet1.2 datasets.
C1 [Dang, Yuanjie; Huang, Chunxia; Chen, Peng; Zhao, Dongdong; Gao, Nan; Liang, Ronghua; Huan, Ruohong] Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou 310000, Zhejiang, Peoples R China.
C3 Zhejiang University of Technology
RP Chen, P (corresponding author), Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou 310000, Zhejiang, Peoples R China.
EM dangyj@zjut.edu.cn; huangcx@zjut.edu.cn; chenpeng@zjut.edu.cn;
   zhaodd@zjut.edu.cn; gaonan@zjut.edu.cn; rhliang@zjut.edu.cn;
   huanrh@zjut.edu.cn
OI gao, nan/0000-0003-4545-7197; Dang, Yuanjie/0000-0002-8302-1338
FU Natural Science Foundation of China [62206250, 62036009, 62276237,
   62001418, 62371421]; Zhejiang Provincial Natural Science Foundation of
   China [LQ22F020007, LD24F020005, LDT23F0202, LDT23F02021F02]; Ten
   Thousand Talent Program of Zhejiang Province
FX This work was supported in part by the Natural Science Foundation of
   China under Grant 62206250, 62036009, 62276237, 62001418, 62371421, the
   Zhejiang Provincial Natural Science Foundation of China under Grant
   LQ22F020007, LD24F020005, LDT23F0202, LDT23F02021F02, and the Ten
   Thousand Talent Program of Zhejiang Province.
CR Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Chen T, 2020, PR MACH LEARN RES, V119
   Ciptadi A, 2014, LECT NOTES COMPUT SC, V8690, P695, DOI 10.1007/978-3-319-10605-2_45
   Dou P, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3567828
   Fan Ma, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P420, DOI 10.1007/978-3-030-58548-8_25
   Gao JY, 2022, PROC CVPR IEEE, P19967, DOI 10.1109/CVPR52688.2022.01937
   He B, 2022, PROC CVPR IEEE, P13915, DOI 10.1109/CVPR52688.2022.01355
   Hong FT, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1591, DOI 10.1145/3474085.3475298
   Huang LJ, 2022, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR52688.2022.00327
   Huang LJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7982, DOI 10.1109/ICCV48922.2021.00790
   Huang LJ, 2022, IEEE T PATTERN ANAL, V44, P5729, DOI 10.1109/TPAMI.2021.3076172
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Islam A, 2021, AAAI CONF ARTIF INTE, V35, P1637
   Islam A, 2020, IEEE WINT CONF APPL, P536, DOI 10.1109/WACV45572.2020.9093620
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Ji Y, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P853, DOI 10.1145/3474085.3475261
   Jiang Y.-G., 2014, THUMOS challenge: Action recognition with a large number of classes
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Kim S, 2022, IEEE IMAGE PROC, P2311, DOI 10.1109/ICIP46576.2022.9897871
   Lee P, 2021, AAAI CONF ARTIF INTE, V35, P1854
   Lee P, 2020, AAAI CONF ARTIF INTE, V34, P11320
   Lee YJ, 2012, PROC CVPR IEEE, P1346, DOI 10.1109/CVPR.2012.6247820
   Li JJ, 2022, PROC CVPR IEEE, P19882, DOI 10.1109/CVPR52688.2022.01929
   Li ZQ, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P5371, DOI 10.1145/3503161.3548300
   Lin H., 2022, P IEEE INT C MULT EX, P1
   Lin TW, 2018, LECT NOTES COMPUT SC, V11208, P3, DOI 10.1007/978-3-030-01225-0_1
   Liu C, 2021, AAAI CONF ARTIF INTE, V35, P8635
   Liu DC, 2019, PROC CVPR IEEE, P1298, DOI 10.1109/CVPR.2019.00139
   Liu ZM, 2021, AAAI CONF ARTIF INTE, V35, P4267
   Luo W, 2021, PROC CVPR IEEE, P9964, DOI 10.1109/CVPR46437.2021.00984
   Ma YF, 2005, IEEE T MULTIMEDIA, V7, P907, DOI 10.1109/TMM.2005.854410
   Narayan S, 2019, IEEE I CONF COMP VIS, P8678, DOI 10.1109/ICCV.2019.00877
   Pan T, 2021, PROC CVPR IEEE, P11200, DOI 10.1109/CVPR46437.2021.01105
   Paul S, 2018, LECT NOTES COMPUT SC, V11208, P588, DOI 10.1007/978-3-030-01225-0_35
   Nguyen P, 2018, PROC CVPR IEEE, P6752, DOI 10.1109/CVPR.2018.00706
   Nguyen PX, 2019, IEEE I CONF COMP VIS, P5501, DOI 10.1109/ICCV.2019.00560
   Qu SQ, 2021, Arxiv, DOI arXiv:2104.02967
   Ren H, 2023, PROC CVPR IEEE, P2394, DOI 10.1109/CVPR52729.2023.00237
   Shi BF, 2020, PROC CVPR IEEE, P1006, DOI 10.1109/CVPR42600.2020.00109
   Shi HC, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P3820, DOI 10.1145/3503161.3548077
   Shou Z, 2018, LECT NOTES COMPUT SC, V11220, P162, DOI 10.1007/978-3-030-01270-0_10
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Shu XB, 2023, IEEE T PATTERN ANAL, V45, P7559, DOI 10.1109/TPAMI.2022.3222871
   Singh Krishna Kumar, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P3544, DOI 10.1109/ICCV.2017.381
   Sultani W, 2018, PROC CVPR IEEE, P6479, DOI 10.1109/CVPR.2018.00678
   Tong He, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P255, DOI 10.1007/978-3-030-58577-8_16
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang Y, 2023, PROC CVPR IEEE, P18878, DOI 10.1109/CVPR52729.2023.01810
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Xie C, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3567827
   Xu BQ, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3247103
   Xu BQ, 2022, IEEE T IMAGE PROCESS, V31, P3852, DOI 10.1109/TIP.2022.3175605
   Xu YL, 2019, AAAI CONF ARTIF INTE, P9070
   Yan R, 2023, IEEE T PATTERN ANAL, V45, P10317, DOI 10.1109/TPAMI.2023.3261659
   Yang ZC, 2022, AAAI CONF ARTIF INTE, P3090
   Ye M, 2019, PROC CVPR IEEE, P6203, DOI 10.1109/CVPR.2019.00637
   Yuan Y, 2019, Arxiv, DOI arXiv:1905.08586
   Yuanhao Zhai, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P37, DOI 10.1007/978-3-030-58539-6_3
   Zeng RH, 2019, IEEE I CONF COMP VIS, P7093, DOI 10.1109/ICCV.2019.00719
   Zhang C, 2021, PROC CVPR IEEE, P16005, DOI 10.1109/CVPR46437.2021.01575
   Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317
   Zhong JX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P35, DOI 10.1145/3240508.3240511
   Zhou Dingfu, 2020, P AS C COMP VIS
   Zhou JX, 2023, IEEE WINT CONF APPL, P6017, DOI 10.1109/WACV56688.2023.00597
NR 64
TC 0
Z9 0
U1 3
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2024
VL 20
IS 6
SI SI
AR 180
DI 10.1145/3643815
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OQ2T3
UT WOS:001208681800030
DA 2024-08-05
ER

PT J
AU Jing, PG
   Liu, XY
   Zhang, LJ
   Li, Y
   Liu, Y
   Su, YT
AF Jing, Peiguang
   Liu, Xianyi
   Zhang, Lijuan
   Li, Yun
   Liu, Yu
   Su, Yuting
TI Multimodal Attentive Representation Learning for Micro-video Multi-label
   Classification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Micro-video; multimodal representations; multi-label; graph network
ID NEURAL-NETWORKS
AB As one of the representative types of user-generated contents (UGCs) in social platforms, micro-videos have been becoming popular in our daily life. Although micro-videos naturally exhibit multimodal features that are rich enough to support representation learning, the complex correlations across modalities render valuable information difficult to integrate. In this paper, we introduced a multimodal attentive representation network (MARNET) to learn complete and robust representations to benefit micro-video multi-label classification. To address the commonly missing modality issue, we presented a multimodal information aggregation mechanism module to integrate multimodal information, where latent common representations are obtained by modeling the complementarity and consistency in terms of visual-centered modality groupings instead of single modalities. For the label correlation issue, we designed an attentive graph neural network module to adaptively learn the correlation matrix and representations of labels for better compatibility with training data. In addition, a cross-modal multi-head attention module is developed to make the learned common representations label-aware for multi-label classification. Experiments conducted on two micro-video datasets demonstrate the superior performance of MARNET compared with state-of-the-art methods.
C1 [Jing, Peiguang; Liu, Xianyi; Zhang, Lijuan; Liu, Yu; Su, Yuting] Tianjin Univ, Weijin Rd, Tianjin 300072, Peoples R China.
   [Li, Yun] Guangxi Univ Finance & Econ, Nanning, Peoples R China.
C3 Tianjin University; Guangxi University of Finance & Economics
RP Li, Y (corresponding author), Guangxi Univ Finance & Econ, Nanning, Peoples R China.
EM pgjing@tju.edu.cn; goog@tju.edu.cn; ljzhang330@tju.edu.cn;
   liuyu@tju.edu.cn; liyun@guat.edu.cn; ytsu@tju.edu.cn
OI Liu, Yu/0000-0002-5949-6587; Jing, Peiguang/0000-0003-2648-7358
FU National Natural Science Foundation of China [62361002, 62371330];
   Guangxi Key Laboratory of Big Data in Finance and Economics; Doctor
   Start-up Funds [BS2021025]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62361002 and 62371330, in part by the
   Guangxi Key Laboratory of Big Data in Finance and Economics, and in part
   by Doctor Start-up Funds under Grant BS2021025.
CR Andrew R., 2013, INT C MACHINE LEARNI, P1247, DOI DOI 10.5555/3042817.3043076
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Boutell MR, 2004, PATTERN RECOGN, V37, P1757, DOI 10.1016/j.patcog.2004.03.009
   Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3
   Cai DS, 2022, IEEE T MULTIMEDIA, V24, P805, DOI 10.1109/TMM.2021.3059508
   Chatfield K, 2014, Arxiv, DOI arXiv:1405.3531
   Chen GB, 2017, IEEE IJCNN, P2377, DOI 10.1109/IJCNN.2017.7966144
   Chen JY, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P898, DOI 10.1145/2964284.2964314
   Chen S., 2020, CVPR, P10638
   Chen TS, 2019, IEEE I CONF COMP VIS, P522, DOI 10.1109/ICCV.2019.00061
   Chen XS, 2021, IEEE T MULTIMEDIA, V23, P484, DOI 10.1109/TMM.2020.2978618
   Chen ZM, 2019, PROC CVPR IEEE, P5172, DOI 10.1109/CVPR.2019.00532
   Cheng ZQ, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P3272, DOI 10.1145/3503161.3547943
   Cheng ZQ, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR'17), P292, DOI 10.1145/3078971.3079025
   Cheng ZQ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P90, DOI 10.1145/3240508.3240518
   Cheng ZQ, 2017, PROC CVPR IEEE, P4169, DOI 10.1109/CVPR.2017.444
   Cheng ZQ, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1365, DOI 10.1145/2964284.2964326
   Ding ZM, 2016, AAAI CONF ARTIF INTE, P1181
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Durand T, 2019, PROC CVPR IEEE, P647, DOI 10.1109/CVPR.2019.00074
   Fan WQ, 2020, PROCEEDINGS OF THE 2020 SIAM INTERNATIONAL CONFERENCE ON DATA MINING (SDM), P352, DOI 10.1137/1.9781611976236.40
   Frome A., 2013, Advances in neural information processing systems, V26
   Fürnkranz J, 2008, MACH LEARN, V73, P133, DOI 10.1007/s10994-008-5064-8
   Gao Y, 2022, IEEE T CIRC SYST VID, V32, P1552, DOI 10.1109/TCSVT.2021.3078560
   He JY, 2021, NEUROCOMPUTING, V444, P319, DOI 10.1016/j.neucom.2020.05.118
   Hershey S, 2017, INT CONF ACOUST SPEE, P131, DOI 10.1109/ICASSP.2017.7952132
   Huang W, 2023, INT J MACH LEARN CYB, V14, P2913, DOI 10.1007/s13042-023-01809-6
   Hui Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12652, DOI 10.1109/CVPR42600.2020.01267
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Jiang YG, 2018, IEEE T PATTERN ANAL, V40, P352, DOI 10.1109/TPAMI.2017.2670560
   Jiang ZH, 2021, IEEE T IMAGE PROCESS, V30, P5490, DOI 10.1109/TIP.2021.3083079
   Jin Ye, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P649, DOI 10.1007/978-3-030-58589-1_39
   Jing PG, 2018, IEEE T KNOWL DATA EN, V30, P1519, DOI 10.1109/TKDE.2017.2785784
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Li C, 2016, PR MACH LEARN RES, V48
   Li X, 2022, IEEE T PATTERN ANAL, V44, P5918, DOI 10.1109/TPAMI.2021.3086895
   Li Y, 2023, INFORM SCIENCES, V630, P356, DOI 10.1016/j.ins.2022.11.111
   Liu JZ, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P115, DOI 10.1145/3077136.3080834
   Liu M, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P970, DOI 10.1145/3123266.3123341
   Lu W, 2023, IEEE T MULTIMEDIA, V25, P77, DOI 10.1109/TMM.2021.3121567
   Lu W, 2023, IEEE SIGNAL PROC LET, V30, P60, DOI 10.1109/LSP.2023.3240889
   Lu X, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1129, DOI 10.1145/3343031.3350999
   Lyu GY, 2022, AAAI CONF ARTIF INTE, P7647
   Markatopoulou F, 2019, IEEE T CIRC SYST VID, V29, P1631, DOI 10.1109/TCSVT.2018.2848458
   Ngiam J., 2011, P 28 INT C MACH LEAR, P689
   Nie LQ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1192, DOI 10.1145/3123266.3123313
   Pancoast Stephanie, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P1370, DOI 10.1109/ICASSP.2014.6853821
   Peng ML, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2505
   Rajagopalan SS, 2016, LECT NOTES COMPUT SC, V9911, P338, DOI 10.1007/978-3-319-46478-7_21
   Read J, 2011, MACH LEARN, V85, P333, DOI 10.1007/s10994-011-5256-5
   Sadanand S, 2012, PROC CVPR IEEE, P1234, DOI 10.1109/CVPR.2012.6247806
   Scovanner P., 2007, Proceedings of the ACM International Conference on Multimedia, P357, DOI [DOI 10.1145/1291233.1291311, 10.1145/1291233.1291311]
   Srivastava N, 2014, J MACH LEARN RES, V15, P2949
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tran D, 2019, IEEE I CONF COMP VIS, P5551, DOI 10.1109/ICCV.2019.00565
   Tu SY, 2023, Arxiv, DOI [arXiv:2304.10465, 10.48550/arXiv.2304.10465]
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Veličkovic P, 2018, Arxiv, DOI arXiv:1710.10903
   Wang LC, 2020, AAAI CONF ARTIF INTE, V34, P6227
   Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059
   Wang L, 2016, PROC CVPR IEEE, P5005, DOI 10.1109/CVPR.2016.541
   Wang LY, 2020, IEEE T CIRC SYST VID, V30, P4876, DOI 10.1109/TCSVT.2019.2958871
   Wang Z, 2022, IEEE T CIRC SYST VID, V32, P1848, DOI 10.1109/TCSVT.2021.3083978
   Wei YW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1437, DOI 10.1145/3343031.3351034
   Wei YW, 2020, IEEE T IMAGE PROCESS, V29, P1, DOI 10.1109/TIP.2019.2923608
   Wu N, 2022, PR MACH LEARN RES
   Xie JY, 2020, WEB CONFERENCE 2020: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2020), P2542, DOI 10.1145/3366423.3380004
   Yang DK, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P1642, DOI 10.1145/3503161.3547754
   Yang XT, 2017, PROC CVPR IEEE, P5066, DOI 10.1109/CVPR.2017.538
   Yeh CK, 2017, AAAI CONF ARTIF INTE, P2838
   Zellinger W, 2019, Arxiv, DOI arXiv:1702.08811
   Zhang J, 2019, PATTERN RECOGN, V95, P136, DOI 10.1016/j.patcog.2019.06.003
   Zhang ML, 2006, IEEE T KNOWL DATA EN, V18, P1338, DOI 10.1109/TKDE.2006.162
   Zhang ML, 2014, IEEE T KNOWL DATA EN, V26, P1819, DOI 10.1109/TKDE.2013.39
   Zhang Y, 2012, Arxiv, DOI arXiv:1203.3536
   Zhao DW, 2023, IEEE T MULTIMEDIA, V25, P7235, DOI 10.1109/TMM.2022.3219650
   Zhou FT, 2022, IEEE T CIRC SYST VID, V32, P4513, DOI 10.1109/TCSVT.2021.3128054
   Zhu F, 2017, PROC CVPR IEEE, P2027, DOI 10.1109/CVPR.2017.219
   Zhu L, 2024, IEEE T KNOWL DATA EN, V36, P239, DOI 10.1109/TKDE.2023.3282921
   Zhu L, 2020, IEEE T IMAGE PROCESS, V29, P4643, DOI 10.1109/TIP.2020.2974065
   Zhu XY, 2023, INFORM SCIENCES, V623, P94, DOI 10.1016/j.ins.2022.12.022
   Zhu Y, 2018, IEEE T KNOWL DATA EN, V30, P1081, DOI 10.1109/TKDE.2017.2785795
NR 83
TC 0
Z9 0
U1 9
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2024
VL 20
IS 6
SI SI
AR 182
DI 10.1145/3643888
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OQ2T3
UT WOS:001208681800032
DA 2024-08-05
ER

PT J
AU Liu, QL
   Meng, QL
   Lv, XQ
   Li, ZL
   Yu, W
   Zhang, SP
AF Liu, Qinglin
   Meng, Quanling
   Lv, Xiaoqian
   Li, Zonglin
   Yu, Wei
   Zhang, Shengping
TI Human Selective Matting
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image matting; human matting; semantic segmentation
ID IMAGE; MODEL
AB Existing human matting methods are incapable of accurately estimating the alpha mattes of arbitrarily selected humans from a group photo. An alternative solution is to apply them to the corresponding cropped image patches. However, this option obtains an inaccurate alpha estimation due to the interference of the body parts of the neighboring humans. In addition, these methods are only trained on finely annotated synthetic data, which causes poor performance in real-world scenarios due to the domain shift. To address these problems, we propose human selective matting (HSMatt), which performs matting for arbitrarily selected humans from a group photo given only a simple bounding box as guidance. Specifically, we design a global-local context network to extract both local and global semantic context features. A human-aware trimap network is then proposed to generate human-aware trimaps for the selected humans, which adopts stacked bidirectional inference modules with intermediate supervision to progressively refine the estimated trimap. Finally, a partially supervised matting network is introduced to estimate the alpha matte, which uses a sample-varying loss to train the network on both the finely annotated synthetic data and coarsely annotated real-world data, resulting in high accuracy and good generalization. To evaluate the proposed HSMatt, we construct the first human selective matting dataset, named HSM-200K, which contains over 200,000 human images with instance-level alpha matte annotations. Experimental results demonstrate that the proposed HSMatt outperforms state-of-the-art methods.
C1 [Liu, Qinglin; Meng, Quanling; Lv, Xiaoqian; Li, Zonglin; Yu, Wei; Zhang, Shengping] Harbin Inst Technol, Wenhuaxi Rd, Weihai 264209, Shandong, Peoples R China.
C3 Harbin Institute of Technology
RP Meng, QL (corresponding author), Harbin Inst Technol, Wenhuaxi Rd, Weihai 264209, Shandong, Peoples R China.
EM qinglin.liu@outlook.com; quanling.meng@hit.edu.cn;
   xiaoqian.hit@gmail.com; zonglin.li@hit.edu.cn; 20b903014@stu.hit.edu.cn;
   s.zhang@hit.edu.cn
OI Liu, Qinglin/0000-0002-2408-3344; Yu, Wei/0000-0002-4805-3115
FU National Natural Science Foundation of China [62072141]
FX This work was supported by the National Natural Science Foundation of
   China (grant no. 62072141).
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Aksoy Y, 2017, PROC CVPR IEEE, P228, DOI 10.1109/CVPR.2017.32
   BERMAN A, 2000, Patent No. 6134346
   Borji A, 2012, PROC CVPR IEEE, P438, DOI 10.1109/CVPR.2012.6247706
   Cai SF, 2019, IEEE I CONF COMP VIS, P8818, DOI 10.1109/iccv.2019.00891
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen QF, 2013, IEEE T PATTERN ANAL, V35, P2175, DOI 10.1109/TPAMI.2013.18
   Chen Q, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P618, DOI 10.1145/3240508.3240610
   Chen Shifeng, 2007, ACM MM
   Chen T, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618470
   Chen YB, 2018, IEEE T IMAGE PROCESS, V27, P1526, DOI 10.1109/TIP.2017.2779274
   Cho D, 2017, IEEE T PATTERN ANAL, V39, P1504, DOI 10.1109/TPAMI.2016.2606397
   Chuang YY, 2001, PROC CVPR IEEE, P264
   Dai YT, 2022, PROC CVPR IEEE, P11697, DOI 10.1109/CVPR52688.2022.01141
   Ding HH, 2022, IEEE T IMAGE PROCESS, V31, P2421, DOI 10.1109/TIP.2022.3155958
   Ding HH, 2018, PROC CVPR IEEE, P2393, DOI 10.1109/CVPR.2018.00254
   Fang YX, 2021, Arxiv, DOI arXiv:2105.01928
   Forte M, 2020, Arxiv, DOI [arXiv:2003.07711, 10.48550/arXiv.2003.07711]
   French RM, 1999, TRENDS COGN SCI, V3, P128, DOI 10.1016/S1364-6613(99)01294-2
   Gastal ESL, 2010, COMPUT GRAPH FORUM, V29, P575, DOI 10.1111/j.1467-8659.2009.01627.x
   Gong K, 2018, LECT NOTES COMPUT SC, V11208, P805, DOI 10.1007/978-3-030-01225-0_47
   Gong ML, 2015, IEEE T IMAGE PROCESS, V24, P1356, DOI 10.1109/TIP.2015.2401516
   Grady L, 2005, PROCEEDINGS OF THE FIFTH IASTED INTERNATIONAL CONFERENCE ON VISUALIZATION, IMAGING, AND IMAGE PROCESSING, P423
   Gupta V, 2016, 2016 INTERNATIONAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING (ICONSIP)
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2010, PROC CVPR IEEE, P2165, DOI 10.1109/CVPR.2010.5539896
   Hsieh CL, 2013, ASIAPAC SIGN INFO PR
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jinlin Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8560, DOI 10.1109/CVPR42600.2020.00859
   Ke L, 2022, PROC CVPR IEEE, P4402, DOI 10.1109/CVPR52688.2022.00437
   Ke ZH, 2022, AAAI CONF ARTIF INTE, P1140
   Kingma D. P., 2014, arXiv
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Li JZ, 2023, Arxiv, DOI arXiv:2304.04672
   Li JZZ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3501, DOI 10.1145/3474085.3475512
   Li JZZ, 2022, INT J COMPUT VISION, V130, P246, DOI 10.1007/s11263-021-01541-0
   Li YY, 2020, AAAI CONF ARTIF INTE, V34, P11450
   Lin SC, 2021, PROC CVPR IEEE, P8758, DOI 10.1109/CVPR46437.2021.00865
   Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Liu QL, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P526, DOI 10.1145/3474085.3475203
   Liu YH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7535, DOI 10.1109/ICCV48922.2021.00746
   Lu H, 2022, IEEE T PATTERN ANAL, V44, P242, DOI 10.1109/TPAMI.2020.3004474
   Lutz S., 2018, ARXIV180710088, P259
   Ma Sihan, 2023, IJCV
   Park G, 2022, PROC CVPR IEEE, P11686, DOI 10.1109/CVPR52688.2022.01140
   Paszke A, 2019, ADV NEUR IN, V32
   Pexels Contributors, 2023, Pexels.com
   Qiao Y, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3540201
   Qin XB, 2020, PATTERN RECOGN, V106, DOI 10.1016/j.patcog.2020.107404
   Ruzon MA, 2000, PROC CVPR IEEE, P18, DOI 10.1109/CVPR.2000.855793
   Shahrian E, 2013, PROC CVPR IEEE, P636, DOI 10.1109/CVPR.2013.88
   Shen X, 2016, LECT NOTES COMPUT SC, V9905, P92, DOI 10.1007/978-3-319-46448-0_6
   Sun J, 2004, ACM T GRAPHIC, V23, P315, DOI 10.1145/1015706.1015721
   Sun YN, 2022, PROC CVPR IEEE, P2637, DOI 10.1109/CVPR52688.2022.00267
   Sun YN, 2021, PROC CVPR IEEE, P6971, DOI 10.1109/CVPR46437.2021.00690
   Sun YA, 2021, PROC CVPR IEEE, P11115, DOI 10.1109/CVPR46437.2021.01097
   Supervisely Contributors, 2023, Supervisely.com
   Tang H, 2019, IEEE IMAGE PROC, P4255, DOI [10.1109/icip.2019.8803682, 10.1109/ICIP.2019.8803682]
   Tang JW, 2019, PROC CVPR IEEE, P3050, DOI 10.1109/CVPR.2019.00317
   Tian Xin, 2020, BMVC
   Wang J, 2007, PROC CVPR IEEE, P281
   Wang L, 2012, INT J COMPUT VISION, V97, P104, DOI 10.1007/s11263-011-0471-x
   Wang WG, 2019, PROC CVPR IEEE, P5961, DOI 10.1109/CVPR.2019.00612
   Wang XL, 2022, IEEE T PATTERN ANAL, V44, P8587, DOI 10.1109/TPAMI.2021.3111116
   Wu YX, 2020, INT J COMPUT VISION, V128, P742, DOI [10.1007/s11263-019-01198-w, 10.1109/CSTIC.2018.8369274]
   Xu N, 2017, PROC CVPR IEEE, P311, DOI 10.1109/CVPR.2017.41
   Yang DH, 2023, Arxiv, DOI arXiv:2205.08324
   Yang X, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3408323
   Yu Qiao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13673, DOI 10.1109/CVPR42600.2020.01369
   Yu QH, 2021, PROC CVPR IEEE, P1154, DOI 10.1109/CVPR46437.2021.00121
   Yu ZJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7147, DOI 10.1109/ICCV48922.2021.00708
   Zhang YK, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5128, DOI 10.1145/3474085.3475623
   Zhang YK, 2019, PROC CVPR IEEE, P7461, DOI 10.1109/CVPR.2019.00765
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhao J, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P792, DOI 10.1145/3240508.3240509
   Zhu BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P297, DOI 10.1145/3123266.3123286
NR 78
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2024
VL 20
IS 6
SI SI
AR 164
DI 10.1145/3640017
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OQ2T3
UT WOS:001208681800014
DA 2024-08-05
ER

PT J
AU Dai, K
   Li, XT
   Lin, HW
   Jiang, Y
   Chen, XL
   Ye, YM
   Xian, D
AF Dai, Kuai
   Li, Xutao
   Lin, Huiwei
   Jiang, Yin
   Chen, Xunlai
   Ye, Yunming
   Xian, Di
TI TinyPredNet: A Lightweight Framework for Satellite Image Sequence
   Prediction
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Satellite image sequence prediction; deep learning; lightweight;
   interpretability
ID NETWORK
AB Satellite image sequence prediction aims to precisely infer future satellite image frames with historical observations, which is a significant and challenging dense prediction task. Though existing deep learning models deliver promising performance for satellite image sequence prediction, the methods suffer from quite expensive training costs, especially in training time and GPU memory demand, due to the inefficiently modeling for temporal variations. This issue seriously limits the lightweight application in satellites such as space-borne forecast models. In this article, we propose a lightweight prediction framework TinyPredNet for satellite image sequence prediction, in which a spatial encoder and decoder model the intra-frame appearance features and a temporal translator captures inter-frame motion patterns. To efficiently model the temporal evolution of satellite image sequences, we carefully design a multi-scale temporal-cascaded structure and a channel attention-gated structure in the temporal translator. Comprehensive experiments are conducted on FengYun4A (FY-4A) satellite dataset, which show that the proposed framework achieves very competitive performance with much lower computation cost compared to state-of-the-art methods. In addition, corresponding interpretability experiments are conducted to show how our designed structures work. We believe the proposed method can serve as a solid lightweight baseline for satellite image sequence prediction.
C1 [Dai, Kuai; Li, Xutao; Lin, Huiwei] Harbin Inst Technol, Dept Comp Sci, Harbin, Peoples R China.
   [Jiang, Yin; Chen, Xunlai] Shenzhen Meteorol Bur, Shenzhen, Peoples R China.
   [Ye, Yunming] Harbin Inst Technol, Dept Comp Sci, Harbin, Peoples R China.
   [Xian, Di] China Meteorol Adm, Natl Satellite Meteorol Ctr, Beijing, Peoples R China.
   [Dai, Kuai; Li, Xutao; Lin, Huiwei; Jiang, Yin] Harbin Inst Technol, Dept Comp Sci, Taoyuan St, Taoyuan 518055, Peoples R China.
   [Jiang, Yin; Chen, Xunlai] Shenzhen Meteorol Bur, Shenzhen Key Lab Severe Weather South China, Bamboolin Meteorl Rd, Shenzhen, Peoples R China.
   [Xian, Di] China Meteorol Adm, Natl Satellite Meteorol Ctr, Zhongguancun South St, Beijing 100081, Peoples R China.
C3 Harbin Institute of Technology; Harbin Institute of Technology; China
   Meteorological Administration; Harbin Institute of Technology; China
   Meteorological Administration
RP Li, XT (corresponding author), Harbin Inst Technol, Dept Comp Sci, Taoyuan St, Taoyuan 518055, Peoples R China.
EM daikuai_hit@163.com; lix-utao@hit.edu.cn; linhuiwei@stu.hit.edu.cn;
   jy_sz@163.com; chenxunlai@weather.sz.gov.cn; yeyunming@hit.edu.cn;
   xiandi@cma.gov.cn
RI Dai, Kuai/KMX-5469-2024; chen, xunlai/ACX-6238-2022
OI Dai, Kuai/0000-0002-9921-4249; chen, xunlai/0000-0001-6906-2654
FU Shenzhen Science and Technology Program [JCYJ20200109113014456,
   KCXFZ20211020163403005]; FengYun Application Pioneering Project
   [FY-APP-ZX-2022.0220]; Science and Technology Innovation Team Project of
   Guangdong Meteorological Bureau [GRMCTD202104]; Innovation and
   Development Project of China Meteorological Administration
   [CXFZ2022J002]; NSFC [62376072]
FX This work was supported by the Shenzhen Science and Technology Program
   under Grant JCYJ20200109113014456 and Grant KCXFZ20211020163403005, and
   FengYun Application Pioneering Project under Grant FY-APP-ZX-2022.0220,
   in part by Science and Technology Innovation Team Project of Guangdong
   Meteorological Bureau (GRMCTD202104) and Innovation and Development
   Project of China Meteorological Administration (CXFZ2022J002). This work
   was also in part supported by NSFC under Grant 62376072.
CR Aseeri AO, 2023, J COMPUT SCI-NETH, V68, DOI 10.1016/j.jocs.2023.101984
   Bai C, 2022, IEEE T CYBERNETICS, V52, P12538, DOI 10.1109/TCYB.2021.3080121
   Ballas N., 2016, P INT C LEARNING REP
   Benson V, 2024, Arxiv, DOI [arXiv:2303.16198, 10.48550/arXiv.2303.16198]
   Bouguet J.-Y., 2001, Intel Corporation, V5, P4, DOI DOI 10.1109/HPDC.2004.1323531
   Chang Z, 2022, PROC CVPR IEEE, P13926, DOI 10.1109/CVPR52688.2022.01356
   Chen JR, 2023, PROC CVPR IEEE, P12021, DOI 10.1109/CVPR52729.2023.01157
   Cho K., 2014, P 2014 C EMP METH NA, DOI 10.3115/v1/D14-1179
   Dai K, 2023, IEEE GEOSCI REMOTE S, V20, DOI 10.1109/LGRS.2023.3261317
   Dai KA, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3181279
   Dai Kuai, 2023, IEEE Transactions on Geoscience and Remote Sensing, V61, P1
   Espeholt Lasse, 2021, arXiv
   Howard AG, 2017, Arxiv, DOI [arXiv:1704.04861, 10.48550/arXiv.1704.04861]
   Gao ZY, 2022, PROC CVPR IEEE, P3160, DOI 10.1109/CVPR52688.2022.00317
   Geng YLA, 2020, IEEE DATA MINING, P1034, DOI 10.1109/ICDM50108.2020.00121
   Graves A, 2014, Arxiv, DOI [arXiv:1308.0850, DOI 10.48550/ARXIV.1308.0850]
   Guen V. L., 2020, P IEEE COMP SOC C CO, P11474, DOI 10.1109/CVPR42600.2020.01149
   Guibas John, 2022, P INT C LEARNING REP
   Ho JAT, 2019, Arxiv, DOI arXiv:1912.12180
   Huiyu Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P108, DOI 10.1007/978-3-030-58548-8_7
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Kim S, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P7333, DOI 10.1109/ICASSP39728.2021.9414784
   Lagerquist R, 2021, MON WEATHER REV, V149, P3897, DOI 10.1175/MWR-D-21-0096.1
   Lee JH, 2020, IEEE T GEOSCI REMOTE, V58, P2212, DOI 10.1109/TGRS.2019.2955538
   Lee S, 2021, PROC CVPR IEEE, P3053, DOI 10.1109/CVPR46437.2021.00307
   Lin ZH, 2020, AAAI CONF ARTIF INTE, V34, P11531
   Liu XY, 2023, PROC CVPR IEEE, P14420, DOI 10.1109/CVPR52729.2023.01386
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Min M, 2019, IEEE T GEOSCI REMOTE, V57, P2557, DOI 10.1109/TGRS.2018.2874950
   Nie J, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3558770
   Ning Shuliang, 2023, P AAAI C ARTIFICIAL
   Pathak J., ARXIV
   Ravuri S, 2021, NATURE, V597, P672, DOI 10.1038/s41586-021-03854-z
   Ren XM, 2021, EXPERT SYST APPL, V167, DOI 10.1016/j.eswa.2020.114363
   Requena-Mesa C, 2021, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW53098.2021.00124
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Schön C, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2979, DOI 10.1145/3292500.3330682
   Seo M, 2023, Arxiv, DOI arXiv:2303.07849
   Shi XJ, 2015, ADV NEUR IN, V28
   Shi XJ, 2017, ADV NEUR IN, V30
   Shukla BP, 2014, IEEE T GEOSCI REMOTE, V52, P4155, DOI 10.1109/TGRS.2013.2280094
   Shukla BP, 2011, IEEE GEOSCI REMOTE S, V8, P216, DOI 10.1109/LGRS.2010.2060311
   Su Jiahao, 2020, arXiv:2002.09131, V33, P13714
   Sun MZ, 2023, PROC CVPR IEEE, P18727, DOI 10.1109/CVPR52729.2023.01796
   Tan C, 2022, Arxiv, DOI arXiv:2211.12509
   Tan MX, 2019, PROC CVPR IEEE, P2815, DOI [arXiv:1807.11626, 10.1109/CVPR.2019.00293]
   Tang XCA, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3454009
   Teed Zachary, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P402, DOI 10.1007/978-3-030-58536-5_24
   Trebing K, 2021, PATTERN RECOGN LETT, V145, P178, DOI 10.1016/j.patrec.2021.01.036
   Wang CH, 2024, IEEE CONSUM ELECTR M, V13, P51, DOI 10.1109/MCE.2022.3181759
   Wang Y, 2018, P 35 INT C MACH LEAR, P5123, DOI 10.48550/arXiv.1804.06300
   Wang YB, 2019, PROC CVPR IEEE, P9146, DOI 10.1109/CVPR.2019.00937
   Wang YB, 2017, ADV NEUR IN, V30
   Wu BC, 2019, PROC CVPR IEEE, P10726, DOI 10.1109/CVPR.2019.01099
   Wu HX, 2021, PROC CVPR IEEE, P15430, DOI 10.1109/CVPR46437.2021.01518
   Xu HF, 2022, PROC CVPR IEEE, P8111, DOI 10.1109/CVPR52688.2022.00795
   Xu XL, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3401979
   Xu Z, 2019, IEEE ICC, DOI [10.1109/ICC.2019.8761462, 10.1109/icc.2019.8761462]
   Xu ZR, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2940
   Yan W., 2021, arXiv, DOI DOI 10.48550/ARXIV.2104.10157
   Yu W., 2020, P INT C LEARNING REP
   Alom MZ, 2018, Arxiv, DOI arXiv:1803.01164
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhang YC, 2023, NATURE, V619, P526, DOI 10.1038/s41586-023-06184-4
   Zhao DY, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3270204
   Zheng CY, 2024, ACM T MULTIM COMPUT, V20, DOI 10.1145/3603628
NR 68
TC 0
Z9 0
U1 3
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2024
VL 20
IS 5
AR 142
DI 10.1145/3638773
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MF3R9
UT WOS:001192177900022
DA 2024-08-05
ER

PT J
AU Liu, J
   Shang, L
   Su, YT
   Nie, WZ
   Wen, X
   Liu, AA
AF Liu, Jing
   Shang, Litao
   Su, Yuting
   Nie, Weizhi
   Wen, Xin
   Liu, Anan
TI Privacy-preserving Multi-source Cross-domain Recommendation Based on
   Knowledge Graph
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Cross-domain recommendation; collaborative filtering; privacy
   preserving; knowledge graph; implicit feedback
ID LEARNING NETWORK; USER
AB The cross-domain recommender systems aim to alleviate the data sparsity problem in the target domain by transferring knowledge from the auxiliary domain. However, existing works ignore the fact that the data sparsity problem may also exist in the single auxiliary domain, and sharing user behavior data is restricted by the privacy policy. In addition, their cross-domain models lack interpretability. To address these concerns, we propose a novel multi-source cross-domain model based on knowledge graph. Specifically, to avoid the insufficiency of single auxiliary domain, we construct a knowledge graph comprehensively leveraging items from multiple auxiliary domains. To avoid the leakage of user privacy when user information is transferred to multiple domains, we construct graph for information transfer between items to effectively avoid the propagation of users' private information between different domains. We implicitly integrate the user-item interaction by transferring the learned item embeddings. To improve the interpretability of cross-domain knowledge transfer, we propose a knowledge graph-based retrieval and fusion method to transfer knowledge derived from multiple auxiliary domains. An attention-based fusion network is designed to enhance the representation of the targeted user and items with the transferred item embedding. We perform extensive experiments on three real-world datasets, demonstrating that our model outperforms the states of the art.
C1 [Liu, Jing; Shang, Litao; Su, Yuting; Nie, Weizhi; Wen, Xin; Liu, Anan] Tianjin Univ, Tianjin, Peoples R China.
   [Liu, Jing; Shang, Litao; Su, Yuting; Nie, Weizhi; Wen, Xin; Liu, Anan] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
C3 Tianjin University; Tianjin University
RP Nie, WZ; Liu, AA (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM jliu_tju@tju.edu.cn; shanglitao@tju.edu.cn; ytsu@tju.edu.cn;
   weizhinie@tju.edu.cn; wenxin113@tju.edu.cn; anan0422@gmail.com
RI Wen, Xin/GRJ-9085-2022
OI Wen, Xin/0000-0003-3070-4482; nie, weizhi/0000-0002-0578-8138
FU National Key Research and Development Program of China [2021YFF0901603]
FX This work was supported by the National Key Research and Development
   Program of China (2021YFF0901603).
CR Alharbe N, 2023, EXPERT SYST APPL, V215, DOI 10.1016/j.eswa.2022.119380
   Berkovsky S, 2007, LECT NOTES ARTIF INT, V4511, P355
   Chen CC, 2022, PROCEEDINGS OF THE ACM WEB CONFERENCE 2022 (WWW'22), P1455, DOI 10.1145/3485447.3512192
   Chen JY, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P335, DOI 10.1145/3077136.3080797
   Chen X, 2023, ACM T INFORM SYST, V41, DOI 10.1145/3522762
   Cheng H.-T., 2016, DLRS RECSYS 2016, P7, DOI 10.1145/ 2988450.2988454
   Covington P, 2016, PROCEEDINGS OF THE 10TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'16), P191, DOI 10.1145/2959100.2959190
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Ding SH, 2022, PROCEEDINGS OF THE 28TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, KDD 2022, P305, DOI 10.1145/3534678.3539240
   Elkahky A, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P278, DOI 10.1145/2736277.2741667
   Gao C, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P491, DOI 10.1145/3308558.3313538
   Goldberg Y, 2014, Arxiv, DOI [arXiv:1402.3722, DOI 10.48550/ARXIV.1402.3722]
   Gong JB, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P79, DOI [10.1145/11221.27, 10.1145/3397271.3401057]
   He XN, 2018, IEEE T KNOWL DATA EN, V30, P2354, DOI 10.1109/TKDE.2018.2831682
   He XN, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P173, DOI 10.1145/3038912.3052569
   Hu GN, 2018, CIKM'18: PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P667, DOI 10.1145/3269206.3271684
   Huang QH, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2598779
   Kingma D. P., 2014, arXiv
   Li P, 2020, PROCEEDINGS OF THE 13TH INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM '20), P331, DOI 10.1145/3336191.3371793
   Li Y, 2020, J COMPUT SCI TECH-CH, V35, P794, DOI 10.1007/s11390-020-0314-8
   Liu Weiming, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P6243, DOI 10.1145/3581783.3611924
   Man T, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2464
   Meng L, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3460, DOI 10.1145/3394171.3413598
   Mirbakhsh N, 2015, ACM T KNOWL DISCOV D, V9, DOI 10.1145/2724720
   Mnih A, 2007, ADV NEURAL INF PROCE, V20
   Nie WZ, 2022, IEEE T CIRC SYST VID, V32, P992, DOI 10.1109/TCSVT.2021.3070969
   Nie Weizhi, 2023, IEEE Trans. Multimedia
   Perera D, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1780, DOI 10.1145/3123266.3123447
   Le Q, 2014, PR MACH LEARN RES, V32, P1188
   Rendle S, 2012, Arxiv, DOI [arXiv:1205.2618, 10.48550/arXiv.1205.2618]
   Sheng Gao, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference (ECML PKDD 2013). Proceedings: LNCS 8189, P161, DOI 10.1007/978-3-642-40991-2_11
   Shi C, 2019, IEEE T KNOWL DATA EN, V31, P357, DOI 10.1109/TKDE.2018.2833443
   Song YG, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3524618
   Tan SL, 2011, ACM T MULTIM COMPUT, V7, DOI 10.1145/2037676.2037679
   Wang HW, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P3307, DOI 10.1145/3308558.3313417
   Wang HW, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P1835, DOI 10.1145/3178876.3186175
   Wang X, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P950, DOI 10.1145/3292500.3330989
   Wang XD, 2019, SIGBIOMED WORKSHOP ON BIOMEDICAL NATURAL LANGUAGE PROCESSING (BIONLP 2019), P165, DOI 10.1145/3331184.3331267
   Wu CH, 2021, Arxiv, DOI arXiv:2102.04925
   Wu XD, 2003, IEEE T KNOWL DATA EN, V15, P353, DOI 10.1109/TKDE.2003.1185839
   Xue HJ, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3203
   Yan DC, 2022, DIGIT COMMUN NETW, V8, P552, DOI 10.1016/j.dcan.2022.04.034
   Yan M, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2957755
   Yu X., 2013, P 7 ACM C REC SYST, P347
   Yu X, 2021, INFORM PROCESS MANAG, V58, DOI 10.1016/j.ipm.2021.102691
   Yu X, 2022, IEEE J BIOMED HEALTH, V26, P1928, DOI 10.1109/JBHI.2021.3069629
   Yu X, 2019, PATTERN RECOGN, V94, P96, DOI 10.1016/j.patcog.2019.05.030
   Yu X, 2018, KNOWL-BASED SYST, V141, P80, DOI 10.1016/j.knosys.2017.11.010
   Yu Xu, 2023, IEEE Trans. Cons. Electr., V2023
   Zhang FZ, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P353, DOI 10.1145/2939672.2939673
   Zhang Q, 2019, IEEE T NEUR NET LEAR, V30, P1998, DOI 10.1109/TNNLS.2018.2875144
   Zhang YF, 2018, Arxiv, DOI arXiv:1803.06540
   Zhu F, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3001
   Zhu F, 2019, PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM '19), P1533, DOI 10.1145/3357384.3357992
   Zhu XF, 2020, ACM T KNOWL DISCOV D, V14, DOI 10.1145/3396238
   Zhu XF, 2019, IEEE T KNOWL DATA EN, V31, P2022, DOI 10.1109/TKDE.2018.2873378
   Zhu XF, 2016, IEEE T CYBERNETICS, V46, P450, DOI 10.1109/TCYB.2015.2403356
   Zhu XF, 2015, NEUROCOMPUTING, V169, P43, DOI 10.1016/j.neucom.2014.08.106
NR 58
TC 0
Z9 0
U1 12
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2024
VL 20
IS 5
AR 148
DI 10.1145/3639706
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MF3R9
UT WOS:001192177900028
DA 2024-08-05
ER

PT J
AU You, SS
   Yao, HT
   Bao, BK
   Xu, CS
AF You, Sisi
   Yao, Hantao
   Bao, Bing-Kun
   Xu, Changsheng
TI Multi-object Tracking with Spatial-Temporal Tracklet Association
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Multi-object tracking; spatial-temporal tracklet association;
   tracklet-guided spatial-temporal attention network; bi-tracklet
   spatial-temporal association
ID APPEARANCE
AB Recently, the tracking-by-detection methods have achieved excellent performance in Multi-Object Tracking (MOT), which focuses on obtaining a robust feature for each object and generating tracklets based on feature similarity. However, they are confronted with two issues: (1) unstable features in short-term occlusion and (2) insufficient matching in long-term occlusion. Specifically, the unstable feature is caused by the appearance variation under occlusion, and the association with the current unstable feature will lead to insufficient matching in long-term occlusion. To address the above issues, we propose a two-stage tracklet-level association method, Spatial-Temporal Tracklet Association (STTA), to effectively combine spatial-temporal context between feature extraction and data association. In the first stage, we propose the Tracklet-guided Spatial-Temporal Attention network (TSTA) to generate robust and stable features. Specifically, TSTA captures spatial-temporal context to obtain the most salient regions between the current and previous clips. In the second stage, we design the Bi-Tracklet Spatial-Temporal association (BTST) module to fully exploit the spatial-temporal context in data association. Specifically, we leverage BTST to merge different tracklets into long-term trajectories by jointly learning visual feature and spatial-temporal context and designing a bidirectional interpolation to recover the missed objects between matched tracklets. Extensive experiments of public and private detections on four benchmarks demonstrate the robustness of STTA. Furthermore, the proposed method is a model-agnostic method, which can be plugged and played with existing methods to boost their performance, e.g., obtain 11.0%, 10.1%, 2.9%, 3.2%, and 7.8% improvement on IDF1 in the MOT16 validation dataset for Tracktor, CenterTrack, Deepsort, JDE, and CTracker, respectively.
C1 [You, Sisi; Bao, Bing-Kun] Nanjing Univ Posts & Telecommun, 66 New Model Rd, Nanjing 210003, Jiangsu, Peoples R China.
   [You, Sisi] Tianjin Univ Technol, Nanjing, Peoples R China.
   [Yao, Hantao; Xu, Changsheng] Chinese Acad Sci CASIA, Inst Automat, State Key Lab Multimodal Artificial Intelligence, 95 Zhongguancun East Rd, Beijing 100190, Peoples R China.
C3 Nanjing University of Posts & Telecommunications; Tianjin University of
   Technology; Chinese Academy of Sciences; Institute of Automation, CAS
RP Xu, CS (corresponding author), Chinese Acad Sci CASIA, Inst Automat, State Key Lab Multimodal Artificial Intelligence, 95 Zhongguancun East Rd, Beijing 100190, Peoples R China.
EM ssyou@njupt.edu.cn; hantao.yao@nlpr.ia.ac.cn; bingkunbao@njupt.edu.cn;
   csxu@nlpr.ia.ac.cn
OI Yao, Hantao/0000-0001-8125-2864; xu, chang sheng/0000-0001-8343-9665;
   Bao, Bingkun/0000-0001-5956-831X; you, sisi/0009-0004-2173-1694
FU National Key Research and Development Program of China [2021ZD0112200];
   National Natural Science Foundation of China [62325206, 61936005,
   62301276, 62036012, 62376268]; Beijing Natural Science Foundation
   [L201001, 4222039]; Key Research and Development Program of Jiangsu
   Province [BE2023016-4]; Opening Foundation of Key Laboratory of Computer
   Vision and System, Ministry of Education, Tianjin University of
   Technology, China
FX This work was supported by the National Key Research and Development
   Program of China (2021ZD0112200), National Natural Science Foundation of
   China under Grants (No. 62325206, 61936005, 62301276, 62036012,
   62376268), Beijing Natural Science Foundation (L201001, 4222039), Key
   Research and Development Program of Jiangsu Province under Grant
   BE2023016-4, and the Opening Foundation of Key Laboratory of Computer
   Vision and System, Ministry of Education, Tianjin University of
   Technology, China.
CR An N, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3441656
   Babaee Maryam, 2018, arXiv
   Bae SH, 2014, PROC CVPR IEEE, P1218, DOI 10.1109/CVPR.2014.159
   Bai ST, 2022, PROC CVPR IEEE, P7329, DOI 10.1109/CVPR52688.2022.00719
   Bergmann P, 2019, IEEE I CONF COMP VIS, P941, DOI 10.1109/ICCV.2019.00103
   Bernardin K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/246309
   Brasó G, 2020, PROC CVPR IEEE, P6246, DOI 10.1109/CVPR42600.2020.00628
   Carion N., 2020, EUR C COMP VIS, P213
   Chen GY, 2019, IEEE T IMAGE PROCESS, V28, P4192, DOI 10.1109/TIP.2019.2908062
   Chen L, 2019, IEEE SIGNAL PROC LET, V26, P1613, DOI 10.1109/LSP.2019.2940922
   Cheng YM, 2023, Arxiv, DOI arXiv:2305.06558
   Chu Q, 2017, IEEE I CONF COMP VIS, P4846, DOI 10.1109/ICCV.2017.518
   Dai P, 2021, PROC CVPR IEEE, P2443, DOI 10.1109/CVPR46437.2021.00247
   Dendorfer P., 2020, arXiv
   Dollár P, 2014, IEEE T PATTERN ANAL, V36, P1532, DOI 10.1109/TPAMI.2014.2300479
   Dong XP, 2021, IEEE T PATTERN ANAL, V43, P1515, DOI 10.1109/TPAMI.2019.2956703
   Ess A, 2007, IEEE I CONF COMP VIS, P2065
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Fu Y, 2019, AAAI CONF ARTIF INTE, P8287
   Ge Z, 2021, Arxiv, DOI arXiv:2107.08430
   Guo HJ, 2022, PROC CVPR IEEE, P20020, DOI 10.1109/CVPR52688.2022.01942
   Guo S, 2021, PROC CVPR IEEE, P8132, DOI 10.1109/CVPR46437.2021.00804
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Hornakova A, 2020, PR MACH LEARN RES, V119
   Hu T, 2020, Arxiv, DOI arXiv:2003.02795
   Jinlong Peng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P145, DOI 10.1007/978-3-030-58548-8_9
   Kingma D. P., 2014, arXiv
   Kuhn HW, 2005, NAV RES LOG, V52, P7, DOI 10.1002/nav.20053
   Leal-Taix‚ L, 2015, Arxiv, DOI [arXiv:1504.01942, DOI 10.48550/ARXIV.1504.01942]
   Li R, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3595379
   Li S, 2018, PROC CVPR IEEE, P369, DOI 10.1109/CVPR.2018.00046
   Liu QK, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P530
   Luiten J, 2021, INT J COMPUT VISION, V129, P548, DOI 10.1007/s11263-020-01375-2
   McLaughlin N, 2016, PROC CVPR IEEE, P1325, DOI 10.1109/CVPR.2016.148
   Milan A, 2016, Arxiv, DOI arXiv:1603.00831
   Pang B, 2020, PROC CVPR IEEE, P6307, DOI 10.1109/CVPR42600.2020.00634
   Paszke A, 2019, ADV NEUR IN, V32
   Peng JL, 2020, PATTERN RECOGN, V107, DOI 10.1016/j.patcog.2020.107480
   Peng JL, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON VISUAL COMMUNICATIONS AND IMAGE PROCESSING (IEEE VCIP)
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Ruibing Hou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P388, DOI 10.1007/978-3-030-58595-2_24
   Saleh F, 2021, PROC CVPR IEEE, P14324, DOI 10.1109/CVPR46437.2021.01410
   Shen H, 2018, Arxiv, DOI arXiv:1808.01562
   Stadler D, 2022, IEEE WINT CONF APPL, P133, DOI 10.1109/WACVW54805.2022.00019
   Stadler D, 2021, PROC CVPR IEEE, P10953, DOI 10.1109/CVPR46437.2021.01081
   Subramaniam A, 2019, IEEE I CONF COMP VIS, P562, DOI 10.1109/ICCV.2019.00065
   Sun PZ, 2021, Arxiv, DOI arXiv:2012.15460
   Tokmakov P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10840, DOI 10.1109/ICCV48922.2021.01068
   Tseng A, 2022, PROC CVPR IEEE, P2201, DOI 10.1109/CVPR52688.2022.00225
   Wang GA, 2023, IEEE T MULTIMEDIA, V25, P1256, DOI 10.1109/TMM.2022.3140919
   Wang GA, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9856, DOI 10.1109/ICCV48922.2021.00973
   Wang GA, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P482, DOI 10.1145/3343031.3350853
   Wang HD, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3533253
   Wang Q, 2021, PROC CVPR IEEE, P3875, DOI 10.1109/CVPR46437.2021.00387
   Wang S, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13199, DOI 10.1109/ICCV48922.2021.01297
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang YX, 2021, IEEE INT CONF ROBOT, P13708, DOI 10.1109/ICRA48506.2021.9561110
   Wang Z., 2022, P IEEE CVF C COMP VI, P13096
   Wojke N, 2017, IEEE IMAGE PROC, P3645, DOI 10.1109/ICIP.2017.8296962
   Wu W, 2022, PROC CVPR IEEE, P7309, DOI 10.1109/CVPR52688.2022.00717
   Xiang J, 2021, IEEE T CIRC SYST VID, V31, P275, DOI 10.1109/TCSVT.2020.2975842
   Xingyi Zhou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P474, DOI 10.1007/978-3-030-58548-8_28
   Xu JR, 2019, IEEE I CONF COMP VIS, P3987, DOI 10.1109/ICCV.2019.00409
   Xu YH, 2023, IEEE T PATTERN ANAL, V45, P7820, DOI 10.1109/TPAMI.2022.3225078
   Yang F, 2021, IMAGE VISION COMPUT, V106, DOI 10.1016/j.imavis.2020.104091
   Yin JB, 2020, PROC CVPR IEEE, P6767, DOI 10.1109/CVPR42600.2020.00680
   Yu Qian, 2007, CVPR, P1
   Zeng FG, 2022, LECT NOTES COMPUT SC, V13687, P659, DOI 10.1007/978-3-031-19812-0_38
   Zhang L, 2021, IEEE T PATTERN ANAL, V43, P1460, DOI 10.1109/TPAMI.2020.2976969
   Zhang W, 2020, IEEE T IMAGE PROCESS, V29, P3365, DOI 10.1109/TIP.2019.2959653
   Zhang Y, 2020, IEEE T IMAGE PROCESS, V29, P6694, DOI 10.1109/TIP.2020.2993073
   Zhang YF, 2022, LECT NOTES COMPUT SC, V13682, P1, DOI 10.1007/978-3-031-20047-2_1
   Zhang YF, 2021, INT J COMPUT VISION, V129, P3069, DOI 10.1007/s11263-021-01513-4
   Zhao YR, 2019, PROC CVPR IEEE, P4908, DOI 10.1109/CVPR.2019.00505
   Zhongdao Wang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P107, DOI 10.1007/978-3-030-58621-8_7
   Zhou XY, 2022, PROC CVPR IEEE, P8761, DOI 10.1109/CVPR52688.2022.00857
   Zhu TY, 2023, IEEE T PATTERN ANAL, V45, P12783, DOI 10.1109/TPAMI.2022.3213073
NR 79
TC 0
Z9 0
U1 5
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2024
VL 20
IS 5
AR 129
DI 10.1145/3635155
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MF3R9
UT WOS:001192177900009
DA 2024-08-05
ER

PT J
AU Telili, A
   Fezza, SA
   Hamidouche, W
   Meftah, HFZB
AF Telili, Ahmed
   Fezza, Sid Ahmed
   Hamidouche, Wassim
   Meftah, Hanene F. Z. Brachemi
TI 2BiVQA: Double Bi-LSTM-based Video Quality Assessment of UGC Videos
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Blind video quality assessment; user-generated content; deep learning;
   Bi-LSTM; spatial pooling; temporal pooling
ID STATISTICS; CLASSIFICATION; PREDICTION; NETWORKS
AB Recently, with the growing popularity of mobile devices as well as video sharing platforms (e.g., YouTube, Facebook, TikTok, and Twitch), User-Generated Content (UGC) videos have become increasingly common and now account for a large portion of multimedia traffic on the internet. Unlike professionally generated videos produced by filmmakers and videographers, typically, UGC videos contain multiple authentic distortions, generally introduced during capture and processing by naive users. Quality prediction of UGC videos is of paramount importance to optimize and monitor their processing in hosting platforms, such as their coding, transcoding, and streaming. However, blind quality prediction of UGC is quite challenging, because the degradations of UGC videos are unknown and very diverse, in addition to the unavailability of pristine reference. Therefore, in this article, we propose an accurate and efficient Blind Video Quality Assessment (BVQA) model for UGC videos, which we name 2BiVQA for double Bi-LSTM Video Quality Assessment. 2BiVQA metric consists of three main blocks, including a pre-trained Convolutional Neural Network to extract discriminative features from image patches, which are then fed into two Recurrent Neural Networks for spatial and temporal pooling. Specifically, we use two Bi-directional Long Short-term Memory networks, the first is used to capture short-range dependencies between image patches, while the second allows capturing long-range dependencies between frames to account for the temporal memory effect. Experimental results on recent large-scale UGC VQA datasets show that 2BiVQA achieves high performance at lower computational cost than most state-of-the-art VQA models. The source code of our 2BiVQA metric is made publicly available at https://github.com/atelili/2BiVQA.
C1 [Telili, Ahmed; Hamidouche, Wassim] Univ Rennes, INSA Rennes, CNRS, IETR UMR 6164, 20 Ave Buttes Coesmes, F-35000 Rennes, France.
   [Fezza, Sid Ahmed; Meftah, Hanene F. Z. Brachemi] Natl Higher Sch Telecommun & ICT, BP 1518 El MNaouer,Route Es Senia, Oran 31000, Algeria.
C3 Centre National de la Recherche Scientifique (CNRS); CNRS - Institute
   for Engineering & Systems Sciences (INSIS); Institut National des
   Sciences Appliquees de Rennes; Universite de Rennes
RP Telili, A (corresponding author), Univ Rennes, INSA Rennes, CNRS, IETR UMR 6164, 20 Ave Buttes Coesmes, F-35000 Rennes, France.
EM atelili@insa-rennes.fr; sfezza@ensttic.dz; whamidouche@insa-rennes.fr;
   hbrachemi@inttic.dz
OI Brachemi Meftah, Hanene F. Z./0009-0008-1399-4506; Telili,
   Ahmed/0000-0002-2659-7840; Fezza, Sid Ahmed/0000-0001-6453-8588
CR Ahn S, 2018, IEEE IMAGE PROC, P619, DOI 10.1109/ICIP.2018.8451450
   Amer A, 2005, IEEE T CIRC SYST VID, V15, P113, DOI 10.1109/TCSVT.2004.837017
   Amirshahi SA, 2016, J IMAGING SCI TECHN, V60, DOI 10.2352/J.ImagingSci.Technol.2016.60.6.060410
   Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Cao YQ, 2023, IEEE T IMAGE PROCESS, V32, P3847, DOI 10.1109/TIP.2023.3290528
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Feng XJ, 2006, PROC SPIE, V6076, DOI 10.1117/12.645089
   Gao F, 2017, NEUROCOMPUTING, V257, P104, DOI 10.1016/j.neucom.2017.01.054
   Ghadiyaram D, 2017, J VISION, V17, DOI 10.1167/17.1.32
   Ghadiyaram D, 2016, IEEE T IMAGE PROCESS, V25, P372, DOI 10.1109/TIP.2015.2500021
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Götz-Hahn F, 2021, IEEE ACCESS, V9, P72139, DOI 10.1109/ACCESS.2021.3077642
   Graves A, 2005, NEURAL NETWORKS, V18, P602, DOI 10.1016/j.neunet.2005.06.042
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Graves A, 2013, 2013 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P273, DOI 10.1109/ASRU.2013.6707742
   Gu K, 2014, IEEE T BROADCAST, V60, P555, DOI 10.1109/TBC.2014.2344471
   HaoningWu Erli Zhang, 2023, P IEEE CVF INT C COM
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1998, INT J UNCERTAIN FUZZ, V6, P107, DOI 10.1142/S0218488598000094
   Hosu V, 2020, IEEE T IMAGE PROCESS, V29, P4041, DOI 10.1109/TIP.2020.2967829
   Hosu V, 2017, INT WORK QUAL MULTIM
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang XH, 2023, Arxiv, DOI arXiv:2303.13859
   ITU Recommendation BT, 2012, Int. Telecommun. Union, V6
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kancharla P, 2022, IEEE T IMAGE PROCESS, V31, P263, DOI 10.1109/TIP.2021.3130541
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Kim J, 2017, IEEE SIGNAL PROC MAG, V34, P130, DOI 10.1109/MSP.2017.2736018
   Kim W, 2018, LECT NOTES COMPUT SC, V11205, P224, DOI 10.1007/978-3-030-01246-5_14
   Kingma D. P., 2014, arXiv
   Korhonen J, 2019, IEEE T IMAGE PROCESS, V28, P5923, DOI 10.1109/TIP.2019.2923051
   Kundu D, 2017, IEEE T IMAGE PROCESS, V26, P2957, DOI 10.1109/TIP.2017.2685941
   Li DQ, 2021, INT J COMPUT VISION, V129, P1238, DOI 10.1007/s11263-020-01408-w
   Li DQ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2351, DOI 10.1145/3343031.3351028
   Li XL, 2016, IEEE T IMAGE PROCESS, V25, P3329, DOI 10.1109/TIP.2016.2568752
   Liu W, 2015, Arxiv, DOI [arXiv:1506.04579, 10.48550/arXiv.1506.04579]
   Liu WT, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P546, DOI 10.1145/3240508.3240643
   Liu YT, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3414837
   Lu YA, 2015, IEEE SIGNAL PROC LET, V22, P1811, DOI 10.1109/LSP.2015.2436908
   Manasa K, 2016, IEEE IMAGE PROC, P2400, DOI 10.1109/ICIP.2016.7532789
   Marziliano P, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P57, DOI 10.1109/ICIP.2002.1038902
   Min XK, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3470970
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P6054, DOI 10.1109/TIP.2020.2988148
   Min XK, 2018, IEEE T MULTIMEDIA, V20, P2049, DOI 10.1109/TMM.2017.2788206
   Min XK, 2018, IEEE T BROADCAST, V64, P508, DOI 10.1109/TBC.2018.2816783
   Min XK, 2017, IEEE T IMAGE PROCESS, V26, P5462, DOI 10.1109/TIP.2017.2735192
   Mitra S, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P1914, DOI 10.1145/3503161.3548064
   Mittal A, 2016, IEEE T IMAGE PROCESS, V25, P289, DOI 10.1109/TIP.2015.2502725
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Moorthy AK, 2010, IEEE SIGNAL PROC LET, V17, P513, DOI 10.1109/LSP.2010.2043888
   Ninassi A, 2009, IEEE J-STSP, V3, P253, DOI 10.1109/JSTSP.2009.2014806
   Norkin A, 2018, IEEE DATA COMPR CONF, P3, DOI 10.1109/DCC.2018.00008
   Park J, 2013, IEEE T IMAGE PROCESS, V22, P610, DOI 10.1109/TIP.2012.2219551
   Pei SC, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2440172
   Pinson M, 2003, P SOC PHOTO-OPT INS, V5150, P583, DOI 10.1117/12.509909
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   RUDERMAN DL, 1994, PHYS REV LETT, V73, P814, DOI 10.1103/PhysRevLett.73.814
   Saad MA, 2014, IEEE T IMAGE PROCESS, V23, P1352, DOI 10.1109/TIP.2014.2299154
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Saad MA, 2010, IEEE SIGNAL PROC LET, V17, P583, DOI 10.1109/LSP.2010.2045550
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Seshadrinathan K, 2011, INT CONF ACOUST SPEE, P1153
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P1427, DOI 10.1109/TIP.2010.2042111
   Shen WH, 2022, IEEE T BROADCAST, V68, P651, DOI 10.1109/TBC.2022.3164332
   Siami-Namini S, 2019, IEEE INT CONF BIG DA, P3285, DOI 10.1109/BigData47090.2019.9005997
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sinno Z, 2019, IEEE IMAGE PROC, P1750, DOI [10.1109/icip.2019.8803115, 10.1109/ICIP.2019.8803115]
   Sinno Z, 2019, IEEE T IMAGE PROCESS, V28, P612, DOI 10.1109/TIP.2018.2869673
   Sun W, 2021, IEEE INT CONF MULTI, DOI 10.1109/ICMEW53276.2021.9455999
   Sun Wei, 2023, IEEE J. Select. Top. Signal Process.
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tiotsop LF, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3464393
   Tu Z, 2021, IEEE OPEN J SIGNAL P, V2, P425, DOI 10.1109/OJSP.2021.3090333
   Tu ZZ, 2021, IEEE T IMAGE PROCESS, V30, P4449, DOI 10.1109/TIP.2021.3072221
   Tu ZZ, 2020, IEEE IMAGE PROC, P141, DOI 10.1109/ICIP40778.2020.9191169
   Tu ZZ, 2020, INT CONF ACOUST SPEE, P2712, DOI [10.1109/icassp40776.2020.9053634, 10.1109/ICASSP40776.2020.9053634]
   U Cisco, 2020, white paper
   Wang X, 2008, CISP 2008: FIRST INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, VOL 1, PROCEEDINGS, P467, DOI 10.1109/CISP.2008.371
   Wang YL, 2019, IEEE INT WORKSH MULT, DOI 10.1109/mmsp.2019.8901772
   Wang YL, 2016, IEEE IMAGE PROC, P2067, DOI 10.1109/ICIP.2016.7532722
   Wang Z, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P981, DOI 10.1109/ICIP.2000.899622
   Watson Andrew B., 1997, P EL IM SCI TECHN C
   Wei Sun, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P856, DOI 10.1145/3503161.3548329
   Wu HN, 2022, LECT NOTES COMPUT SC, V13666, P538, DOI 10.1007/978-3-031-20068-7_31
   Xu JH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1248, DOI 10.1145/3474085.3475486
   Xu JT, 2016, IEEE T IMAGE PROCESS, V25, P4444, DOI 10.1109/TIP.2016.2585880
   Xu JT, 2014, IEEE IMAGE PROC, P491, DOI 10.1109/ICIP.2014.7025098
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P4850, DOI 10.1109/TIP.2014.2355716
   Yang XH, 2020, NEUROCOMPUTING, V401, P209, DOI 10.1016/j.neucom.2020.03.072
   Yang XH, 2019, IEEE ACCESS, V7, P123788, DOI 10.1109/ACCESS.2019.2938900
   Ye P, 2012, PROC CVPR IEEE, P1098, DOI 10.1109/CVPR.2012.6247789
   Yi FW, 2021, IEEE IMAGE PROC, P1414, DOI 10.1109/ICIP42928.2021.9506420
   Ying Zhang, 2014, ACM Transactions on Multimedia Computing, Communications and Applications, V11, DOI 10.1145/2659520
   Ying ZQ, 2020, PROC CVPR IEEE, P3572, DOI 10.1109/CVPR42600.2020.00363
   You JY, 2019, IEEE IMAGE PROC, P2349, DOI [10.1109/ICIP.2019.8803395, 10.1109/icip.2019.8803395]
   Zhai GT, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2757-1
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang XY, 2016, IEEE T PATTERN ANAL, V38, P1943, DOI 10.1109/TPAMI.2015.2502579
   Zhang Y, 2014, SIGNAL PROCESS-IMAGE, V29, P725, DOI 10.1016/j.image.2014.05.004
   Zhang Y, 2013, J ELECTRON IMAGING, V22, DOI 10.1117/1.JEI.22.4.043025
   Zhang ZC, 2023, PROC CVPR IEEE, P1746, DOI 10.1109/CVPR52729.2023.00174
   Zhu Y, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3183512
NR 105
TC 2
Z9 2
U1 16
U2 16
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2024
VL 20
IS 4
AR 100
DI 10.1145/3632178
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6G9
UT WOS:001153382100010
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Yuan, H
   Gao, W
   Ma, SW
   Yan, YQ
AF Yuan, Hang
   Gao, Wei
   Ma, Siwei
   Yan, Yiqiang
TI Divide-and-conquer-based RDO-free CU Partitioning for 8K Video
   Compression
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE AVS3; 8K UHD; CU decision; RDO-free; hardware implementation
ID RATE-DISTORTION OPTIMIZATION; DECISION ALGORITHM; SIZE DECISION; HEVC;
   INTRA; ARCHITECTURE
AB 8K (7689 x 4320) ultra-high definition (UHD) videos are growing popular with the improvement of human visual experience demand. Therefore, the compression of 8K UHD videos has become a top priority in the third-generation audio video coding standard (AVS3). However, as an important part of the coding standard promotion, the real-time hardware implementation for AVS3-based 8K UHD video intra coding is severely hindered, especially in the coding unit (CU) partition stage. To break through the limitation, this article proposes a divide-and-conquer-based rate-distortion-optimization-free (RDO-free) CU partitioning algorithm for efficient hardware implementation. Aimed at the complex CU partition in AVS3, we separately design a lightweight optimization for original partitioning rules to improve division efficiency and a decision tree-based RDO-free CU decision framework to eliminate the latency caused by the waiting for rate-distortion cost calculation in RDO strategy. Afterward, a divide-and-conquer-based hardware-friendly gradient difference calculating approach is devised to accelerate the learning feature extracting speed. To ensure that the proposed algorithm is sufficient to support the real-time CU partition for 8K videos, we also develop a hardware architecture based on FPGA. Experimental results illustrate that the software coding performance of our algorithm is significantly ahead of the efficient implementation uAVS3e for AVS3 and the reference software HM-16.20 for High Efficiency Video Coding (HEVC), even though there is 9.96% loss on BD-Rate Y. Considering its importance for the hardware implementation of AVS3-based 8K real-time encoder, the coding loss is acceptable. Moreover, the hardware simulation results on VU440 FPGA with Vivado 2019 show that our algorithm can support 61.12 frames per second (fps) CU partition for 8K UHD videos with only 0.00%, 0.00%, 1.01%, and 7.78% consumption of BRAM_18K, DSP48E, FF, and LUT, respectively. Additionally, with dual-path parallelism, 122.24 fps also can be implemented with controllable resource utilization, which achieves the state-of-the-art performance.
C1 [Yuan, Hang; Gao, Wei] Peking Univ, Sch Elect & Comp Engn, Shenzhen 518055, Peoples R China.
   [Yuan, Hang] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
   [Ma, Siwei] Peking Univ, Sch Comp Sci, Beijing 100871, Peoples R China.
   [Yan, Yiqiang] Lenovo Grp, Beijing 100091, Peoples R China.
C3 Peking University; Peng Cheng Laboratory; Peking University; Legend
   Holdings; Lenovo
RP Gao, W (corresponding author), Peking Univ, Sch Elect & Comp Engn, Shenzhen 518055, Peoples R China.
EM hyuan@pku.edu.cn; gaowei262@pku.edu.cn; swma@pku.edu.cn;
   yanyq@lenovo.com
FU Natural Science Foundation of China [62271013, 62031013]; Guangdong
   Basic and Applied Basic Research Foundation [2019A1515012031]; Shenzhen
   Fundamental Research Program [GXWD20201231165807007-20200806163656003];
   Shenzhen Science and Technology Program [JCYJ20230807120808017];
   CCF-Lenovo Blue Ocean Research Fund [CCF-Lenovo OF 202208]
FX This work was supported by Natural Science Foundation of China
   (62271013, 62031013), Guangdong Basic and Applied Basic Research
   Foundation (2019A1515012031), Shenzhen Fundamental Research Program
   (GXWD20201231165807007-20200806163656003), and Shenzhen Science and
   Technology Program (JCYJ20230807120808017), and was sponsored by
   CCF-Lenovo Blue Ocean Research Fund (CCF-Lenovo OF 202208).
CR [Anonymous], 2022, HM-16.20
   Audio Video Coding Standard Workgroup, 2018, AVS Proposal N2617: AVS3-P2 common test conditions v5.0
   AVS3 Reference Software, 2022, HPM-10.0 software repository.
   Azgin H, 2019, 2019 22ND EUROMICRO CONFERENCE ON DIGITAL SYSTEM DESIGN (DSD), P194, DOI 10.1109/DSD.2019.00037
   BENTLEY JL, 1980, COMMUN ACM, V23, P214, DOI 10.1145/358841.358850
   Bjontegaard G., 2001, Calculation of Average PSNR Differences between RDcurves
   Bross B, 2021, IEEE T CIRC SYST VID, V31, P3736, DOI 10.1109/TCSVT.2021.3101953
   Cai Yangang, 2021, IEEE INT C MULT EXP, P1, DOI [10.1109/ICME51207.2021.9428331, DOI 10.1109/ICME51207.2021.9428331]
   Cai ZY, 2021, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS51556.2021.9401121
   Chen J, 2016, IEEE INT C MULT EXP, P1, DOI [10.1109/ICME.2016.7552970, DOI 10.1109/ICME.2016.7552970]
   Cui J, 2020, IEEE DATA COMPR CONF, P103, DOI 10.1109/DCC47342.2020.00018
   Duanmu F, 2016, IEEE J EM SEL TOP C, V6, P517, DOI 10.1109/JETCAS.2016.2597698
   Fan YB, 2020, IEEE T CIRC SYST VID, V30, P3289, DOI 10.1109/TCSVT.2019.2934752
   FFmpeg, 2023, about us
   Gao W, 2022, IEEE T IND INFORM, V18, P1594, DOI 10.1109/TII.2021.3079231
   Gao W, 2019, IEEE T BROADCAST, V65, P94, DOI 10.1109/TBC.2018.2865647
   Gao W, 2017, IEEE T IMAGE PROCESS, V26, P6074, DOI 10.1109/TIP.2017.2745099
   Gao W, 2016, IEEE T CIRC SYST VID, V26, P139, DOI 10.1109/TCSVT.2015.2444671
   Gao Wen, 2014, Advanced Video Coding Systems, P4
   Guo Y, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3507970
   ITU, 2008, SUBJ VID QUAL ASS ME
   Kammoun A, 2020, IEEE T CIRC SYST VID, V30, P4340, DOI 10.1109/TCSVT.2019.2954749
   Lei M, 2019, IEEE IMAGE PROC, P4120, DOI [10.1109/icip.2019.8803421, 10.1109/ICIP.2019.8803421]
   Li L, 2016, IEEE T MULTIMEDIA, V18, P2023, DOI 10.1109/TMM.2016.2595264
   Li TY, 2021, IEEE T IMAGE PROCESS, V30, P5377, DOI 10.1109/TIP.2021.3083447
   Li Y, 2021, IEEE T BROADCAST, V67, P710, DOI 10.1109/TBC.2021.3073556
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu XG, 2019, IEEE T CIRC SYST VID, V29, P144, DOI 10.1109/TCSVT.2017.2777903
   Liu ZY, 2016, IEEE T IMAGE PROCESS, V25, P5088, DOI 10.1109/TIP.2016.2601264
   Park SH, 2021, IEEE T MULTIMEDIA, V23, P4388, DOI 10.1109/TMM.2020.3042062
   Pastuszak G, 2016, IEEE T CIRC SYST VID, V26, P210, DOI 10.1109/TCSVT.2015.2428571
   Peking University and Peng Cheng Laboratory, 2022, PP8KLite Dataset.
   Saldanha M, 2022, IEEE T CIRC SYST VID, V32, P3947, DOI 10.1109/TCSVT.2021.3108671
   Shen LQ, 2013, IEEE T CONSUM ELECTR, V59, P207, DOI 10.1109/TCE.2013.6490261
   Sun HM, 2017, IEEE T MULTIMEDIA, V19, P2375, DOI 10.1109/TMM.2017.2700629
   Tan XY, 2010, IEEE T IMAGE PROCESS, V19, P1635, DOI 10.1109/TIP.2010.2042645
   Tao L., 2022, PROC 2022 IEEE INT C, P1, DOI [10.1109/ICME52920.2022.9859988, DOI 10.1109/ICME52920.2022.9859988]
   Wang D, 2015, IEEE T CYBERNETICS, V45, P1838, DOI 10.1109/TCYB.2014.2360924
   Wang M, 2019, PICT COD SYMP, DOI 10.1109/pcs48520.2019.8954510
   Wieckowski A, 2021, IEEE INT CONF MULTI, DOI 10.1109/ICMEW53276.2021.9455944
   Wu T, 2021, IEEE ACCESS, V9, P7540, DOI 10.1109/ACCESS.2021.3049148
   Xilinx, 2022, Ultrascale FPGA Product Selection Guide.
   Xiong J, 2015, IEEE T MULTIMEDIA, V17, P2147, DOI 10.1109/TMM.2015.2491018
   Xiong J, 2014, IEEE T MULTIMEDIA, V16, P2141, DOI 10.1109/TMM.2014.2356795
   Xiong J, 2014, IEEE T MULTIMEDIA, V16, P559, DOI 10.1109/TMM.2013.2291958
   Yang H, 2020, IEEE T CIRC SYST VID, V30, P1668, DOI 10.1109/TCSVT.2019.2904198
   Yang Zetao, 2023, IEEE Trans. Image Process.
   Yuan Hang, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P3085, DOI 10.1145/3503161.3548215
   Yuan H, 2023, PROCEEDINGS OF THE 31ST ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2023, P9660, DOI 10.1145/3581783.3613465
   Yuan Hang, 2021, IEEE INT C MULT EXP, P1, DOI [10.1109/ICME51207.2021.9428275, DOI 10.1109/ICME51207.2021.9428275]
   Zhang HC, 2021, IEEE DATA COMPR CONF, P382, DOI 10.1109/DCC50243.2021.00060
   Zhang J, 2018, IEEE T MULTIMEDIA, V20, P29, DOI 10.1109/TMM.2017.2723238
   Zhang J, 2019, PICT COD SYMP, DOI 10.1109/pcs48520.2019.8954503
   Zhang YZ, 2019, IEEE T CIRC SYST VID, V29, P3415, DOI 10.1109/TCSVT.2018.2878399
   Zhang Y, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3321510
   Zhang Y, 2015, IEEE T IMAGE PROCESS, V24, P2225, DOI 10.1109/TIP.2015.2417498
   Zhao D, 2019, IEEE INT SOC CONF, P176, DOI 10.1109/SOCC46988.2019.1570548652
   Zhao TS, 2016, IEEE T IMAGE PROCESS, V25, P2997, DOI 10.1109/TIP.2016.2556941
NR 58
TC 0
Z9 0
U1 13
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2024
VL 20
IS 4
AR 114
DI 10.1145/3634705
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6G9
UT WOS:001153382100024
DA 2024-08-05
ER

PT J
AU Qiao, SB
   Xiong, NN
   Gao, YB
   Fang, ZJ
   Yu, WJ
   Zhang, J
   Jiang, XY
AF Qiao, Shanbao
   Xiong, Neal N.
   Gao, Yongbin
   Fang, Zhijun
   Yu, Wenjun
   Zhang, Juan
   Jiang, Xiaoyan
TI Self-Supervised Learning of Depth and Ego-Motion for 3D Perception in
   Human Computer Interaction
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Autonomous driving; 3D perception; monocular depth and motion
   estimation; Self-supervised Learning; visual SLAM
AB 3D perception of depth and ego-motion is of vital importance in intelligent agent and Human Computer Interaction (HCI) tasks, such as robotics and autonomous driving. There are different kinds of sensors that can directly obtain 3D depth information. However, the commonly used Lidar sensor is expensive, and the effective range of RGB-D cameras is limited. In the field of computer vision, researchers have done a lot of work on 3D perception. While traditional geometric algorithms require a lot of manual features for depth estimation, Deep Learning methods have achieved great success in this field. In this work, we proposed a novel self-supervised method based on Vision Transformer (ViT) with Convolutional Neural Network (CNN) architecture, which is referred to as ViT-Depth. The image reconstruction losses computed by the estimated depth and motion between adjacent frames are treated as supervision signal to establish a self-supervised learning pipeline. This is an effective solution for tasks that need accurate and low-cost 3D perception, such as autonomous driving, robotic navigation, 3D reconstruction, and so on. Our method could leverage both the ability of CNN and Transformer to extract deep features and capture global contextual information. In addition, we propose a cross-frame loss that could constrain photometric error and scale consistency among multi-frames, which lead the training process to be more stable and improve the performance. Extensive experimental results on autonomous driving dataset demonstrate the proposed approach is competitive with the state-of-the-art depth and motion estimation methods.
C1 [Qiao, Shanbao; Gao, Yongbin; Fang, Zhijun; Yu, Wenjun; Zhang, Juan; Jiang, Xiaoyan] Shanghai Univ Engn Sci, 333 Longteng Rd, Shanghai 201600, Peoples R China.
   [Xiong, Neal N.] Sul Ross State Univ, East Highway 90, Alpine, TX 79832 USA.
C3 Shanghai University of Engineering Science; Texas State University
   System
RP Gao, YB (corresponding author), Shanghai Univ Engn Sci, 333 Longteng Rd, Shanghai 201600, Peoples R China.
EM 17864216432@163.com; xiongnaixue@gmail.com; gaoyongbin@sues.edu.cn;
   zjfang@sues.edu.cn; yuwenjun@sues.edu.cn; zhang-j@foxmail.com;
   xiaoyan.jiang@sues.edu.cn
RI xiong, naixue/M-4277-2019
OI xiong, naixue/0000-0002-0394-4635; YU, WENJUN/0009-0009-4880-9339; Qiao,
   Shanbao/0000-0002-9580-2641; Fang, Zhijun/0000-0001-8563-5678
FU National Natural Science Foundation of China [U2033218, 61831018,
   61802253]; Shanghai Local Capacity Enhancement project [21010501500];
   "Science and Technology Innovation Action Plan" of Shanghai Science and
   Technology Commission for social development project [21DZ1204900];
   Chenguang talented program of Shanghai [17CG59]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant U2033218, 61831018, 61802253, in part by
   Shanghai Local Capacity Enhancement project (No. 21010501500), in part
   by "Science and Technology Innovation Action Plan" of Shanghai Science
   and Technology Commission for social development project under Grant
   21DZ1204900, in part by Chenguang talented program of Shanghai (17CG59).
CR Bian JW, 2019, ADV NEUR IN, V32
   Bian Jiawang, 2021, IJCV
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Casser V, 2019, AAAI CONF ARTIF INTE, P8001
   Cipolletta Antonio, 2021, IEEE Internet of Things Journal, V99, P1
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Eigen D, 2014, ADV NEUR IN, V27
   Fu H, 2018, PROC CVPR IEEE, P2002, DOI 10.1109/CVPR.2018.00214
   Garg R, 2019, IEEE I CONF COMP VIS, P7627, DOI 10.1109/ICCV.2019.00772
   Garg R, 2016, LECT NOTES COMPUT SC, V9912, P740, DOI 10.1007/978-3-319-46484-8_45
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Godard C, 2019, IEEE I CONF COMP VIS, P3827, DOI 10.1109/ICCV.2019.00393
   Gordon A, 2019, IEEE I CONF COMP VIS, P8976, DOI 10.1109/ICCV.2019.00907
   Hartley R., 2000, Multiple view geometry in compute vision
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Kundu JN, 2018, PROC CVPR IEEE, P2656, DOI 10.1109/CVPR.2018.00281
   Kuznietsov Y, 2017, PROC CVPR IEEE, P2215, DOI 10.1109/CVPR.2017.238
   Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32
   Lam Huynh, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P581, DOI 10.1007/978-3-030-58574-7_35
   Li F, 2019, IEEE INTERNET THINGS, V6, P8870, DOI 10.1109/JIOT.2019.2924244
   Li HH, 2020, Arxiv, DOI arXiv:2010.16404
   Li J, 2017, IEEE I CONF COMP VIS, P3392, DOI 10.1109/ICCV.2017.365
   Li L, 2021, IEEE T IND INFORM, V17, P3920, DOI 10.1109/TII.2020.3011067
   Lin Huang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P17, DOI 10.1007/978-3-030-58595-2_2
   Liu FY, 2016, IEEE T PATTERN ANAL, V38, P2024, DOI 10.1109/TPAMI.2015.2505283
   Luo CX, 2020, IEEE T PATTERN ANAL, V42, P2624, DOI 10.1109/TPAMI.2019.2930258
   Mahjourian R, 2018, PROC CVPR IEEE, P5667, DOI 10.1109/CVPR.2018.00594
   Mayer N, 2018, INT J COMPUT VISION, V126, P942, DOI 10.1007/s11263-018-1082-6
   Mehta I, 2018, INT CONF 3D VISION, P314, DOI 10.1109/3DV.2018.00044
   Meng Y, 2019, PROC CVPR IEEE, P9802, DOI 10.1109/CVPR.2019.01004
   Miao YN, 2019, IEEE INTERNET THINGS, V6, P15, DOI 10.1109/JIOT.2018.2872435
   Ranjan A, 2019, PROC CVPR IEEE, P12232, DOI 10.1109/CVPR.2019.01252
   Shen TW, 2019, IEEE INT CONF ROBOT, P6359, DOI [10.1109/icra.2019.8793479, 10.1109/ICRA.2019.8793479]
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Tu XH, 2021, IEEE T IND INFORM, V17, P2821, DOI 10.1109/TII.2020.3020583
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang CY, 2018, PROC CVPR IEEE, P2022, DOI 10.1109/CVPR.2018.00216
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang Y., 2020, arXiv
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yang N, 2020, PROC CVPR IEEE, P1278, DOI 10.1109/CVPR42600.2020.00136
   Yang N, 2018, LECT NOTES COMPUT SC, V11212, P835, DOI 10.1007/978-3-030-01237-3_50
   Yang ZH, 2018, AAAI CONF ARTIF INTE, P7493
   Yin ZC, 2018, PROC CVPR IEEE, P1983, DOI 10.1109/CVPR.2018.00212
   Zhan HY, 2018, PROC CVPR IEEE, P340, DOI 10.1109/CVPR.2018.00043
   Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700
   Zou Yuliang, 2018, P 15 EUR C MUN GERM
NR 49
TC 0
Z9 0
U1 7
U2 22
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2024
VL 20
IS 2
SI SI
AR 45
DI 10.1145/3588571
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W6GR4
UT WOS:001092595800015
DA 2024-08-05
ER

PT J
AU Zhang, SX
   Wang, WM
   Li, HL
   Zhang, SY
AF Zhang, Shixiong
   Wang, Wenmin
   Li, Honglei
   Zhang, Shenyong
TI 10.E-detector: Asynchronous Spatio-temporal for Event-based Object
   Detection in Intelligent Transportation System
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Bio-inspired sensor; event-based vision; adaptable framework; moving
   object detection
ID CAMERA; FUSION
AB In intelligent transportation systems, various sensors, including radar and conventional frame cameras, are used to improve system robustness in various challenging scenarios. An event camera is a novel bio-inspired sensor that has attracted the interest of several researchers. It provides a form of neuromorphic vision to capture motion information asynchronously at high speeds. Thus, it possesses advantages for intelligent transportation systems that conventional frame cameras cannotmatch, such as high temporal resolution, high dynamic range, as well as sparse and minimal motion blur. Therefore, this study proposes an E-detector based on event cameras that asynchronously detect moving objects. The main innovation of our framework is that the spatiotemporal domain of the event camera can be adjusted according to different velocities and scenarios. It overcomes the inherent challenges that traditional cameras face when detecting moving objects in complex environments, such as high speed, complex lighting, and motion blur. Moreover, our approach adopts filter models and transfer learning to improve the performance of event-based object detection. Experiments have shown that our method can detect high-speed moving objects better than conventional cameras using state-of-the-art detection algorithms. Thus, our proposed approach is extremely competitive and extensible, as it can be extended to other scenarios concerning high-speed moving objects. The study findings are expected to unlock the potential of event cameras in intelligent transportation system applications.
C1 [Zhang, Shixiong; Wang, Wenmin; Li, Honglei; Zhang, Shenyong] Macau Univ Sci & Technol, Sch Comp Sci & Engn, A320,Ave Wai Long, Taipa, Macau, Peoples R China.
C3 Macau University of Science & Technology
RP Wang, WM (corresponding author), Macau Univ Sci & Technol, Sch Comp Sci & Engn, A320,Ave Wai Long, Taipa, Macau, Peoples R China.
EM zsxpascal@gmail.com; wmwang@must.edu.mo;
   2009853cia30001@student.must.edu.mo; 2009853GIA30007@student.must.edu.mo
RI Wang, Wenmin/W-3511-2019; Li, Honglei/GXF-8312-2022
OI Wang, Wenmin/0000-0003-2664-4413; Li, Honglei/0000-0003-0777-0799;
   Zhang, Shixiong/0000-0002-3816-839X
FU Science and Technology Development Fund (FDCT) of Macau [0016/2019/A1]
FX This work was supported by the Science and Technology Development Fund
   (FDCT) of Macau under Grant No. 0016/2019/A1.
CR Argyriou Andreas, 2006, Advances in neural information processing systems, V19
   Badue C, 2021, EXPERT SYST APPL, V165, DOI 10.1016/j.eswa.2020.113816
   Bardow P, 2016, PROC CVPR IEEE, P884, DOI 10.1109/CVPR.2016.102
   Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Brandli C, 2014, IEEE J SOLID-ST CIRC, V49, P2333, DOI 10.1109/JSSC.2014.2342715
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Carneiro J, 2013, NEURAL NETWORKS, V45, P27, DOI 10.1016/j.neunet.2013.03.006
   Chan V, 2007, IEEE T CIRCUITS-I, V54, P48, DOI 10.1109/TCSI.2006.887979
   Chen L, 2020, IEEE T IND ELECTRON, V67, P10600, DOI [10.1109/TIE.2019.2962413, 10.1109/TITS.2020.3004655]
   Chen L, 2017, IEEE T INTELL TRANSP, V18, P3093, DOI 10.1109/TITS.2017.2680538
   Chen SS, 2019, IEEE COMPUT SOC CONF, P1682, DOI 10.1109/CVPRW.2019.00214
   Cheng WS, 2019, IEEE COMPUT SOC CONF, P1666, DOI 10.1109/CVPRW.2019.00210
   Chin TJ, 2019, IEEE COMPUT SOC CONF, P1646, DOI 10.1109/CVPRW.2019.00208
   de Tournemire P, 2020, Arxiv, DOI arXiv:2001.08499
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667
   Fu C, 2017, arXiv
   Gallego G, 2022, IEEE T PATTERN ANAL, V44, P154, DOI 10.1109/TPAMI.2020.3008413
   Gallego G, 2019, PROC CVPR IEEE, P12272, DOI 10.1109/CVPR.2019.01256
   Gallego G, 2018, PROC CVPR IEEE, P3867, DOI 10.1109/CVPR.2018.00407
   Gallego G, 2018, IEEE T PATTERN ANAL, V40, P2402, DOI 10.1109/TPAMI.2017.2769655
   Gao Z, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3377876
   Gehrig M, 2020, IEEE INT CONF ROBOT, P4195, DOI [10.1109/ICRA40945.2020.9197133, 10.1109/icra40945.2020.9197133]
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Gou JP, 2024, ACM T MULTIM COMPUT, V20, DOI 10.1145/3568679
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Huang XM, 2022, IEEE T MULTIMEDIA, V24, P4458, DOI 10.1109/TMM.2021.3094356
   Jiang ZY, 2019, IEEE INT CONF ROBOT, P8332, DOI [10.1109/icra.2019.8793924, 10.1109/ICRA.2019.8793924]
   Kim H, 2016, LECT NOTES COMPUT SC, V9910, P349, DOI 10.1007/978-3-319-46466-4_21
   Kong T, 2020, IEEE T IMAGE PROCESS, V29, P7389, DOI 10.1109/TIP.2020.3002345
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee Y., 2020, CVPR, P13906
   Li HL, 2022, IEEE T CIRC SYST VID, V32, P4271, DOI 10.1109/TCSVT.2021.3130196
   Li JC, 2021, IEEE COMPUT SOC CONF, P2378, DOI 10.1109/CVPRW53098.2021.00270
   Li QQ, 2014, IEEE T VEH TECHNOL, V63, P540, DOI 10.1109/TVT.2013.2281199
   Lichtsteiner P, 2005, 2005 PHD RESEARCH IN MICROELECTRONICS AND ELECTRONICS, VOLS 1 AND 2, PROCEEDINGS, P406
   Lin CT, 2020, IEEE T VEH TECHNOL, V69, P1505, DOI 10.1109/TVT.2019.2961625
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Mitrolchin A, 2018, IEEE INT C INT ROBOT, P6895, DOI 10.1109/IROS.2018.8593805
   Mondal A, 2021, IEEE INT CONF COMP V, P876, DOI 10.1109/ICCVW54120.2021.00103
   Pan LY, 2019, PROC CVPR IEEE, P6813, DOI 10.1109/CVPR.2019.00698
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Perot E., 2020, Advances in Neural Information Processing Systems, V33, P16639, DOI 10.48550/arXiv.2009.13436
   Rebecq H, 2021, IEEE T PATTERN ANAL, V43, P1964, DOI 10.1109/TPAMI.2019.2963386
   Redmon J., 2018, CoRR
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rogers A, 2020, T ASSOC COMPUT LING, V8, P842, DOI 10.1162/tacl_a_00349
   Rozumnyi D., 2021, P IEEE CVF INT C COM, P3541
   Rozumnyi D, 2017, PROC CVPR IEEE, P4838, DOI 10.1109/CVPR.2017.514
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Scheerlinck C, 2019, LECT NOTES COMPUT SC, V11365, P308, DOI 10.1007/978-3-030-20873-8_20
   Stoffregen T, 2019, PROC CVPR IEEE, P12292, DOI 10.1109/CVPR.2019.01258
   Sultana M, 2021, IEEE T MULTIMEDIA, V23, P2005, DOI 10.1109/TMM.2020.3006419
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu Y, 2016, PROC CVPR IEEE, P5101, DOI 10.1109/CVPR.2016.551
   Xu X, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3414839
   Yan YC, 2021, PROC CVPR IEEE, P7686, DOI 10.1109/CVPR46437.2021.00760
   Yazdi M, 2018, COMPUT SCI REV, V28, P157, DOI 10.1016/j.cosrev.2018.03.001
   Yuanyi Zhong, 2020, 2020 IEEE Winter Conference on Applications of Computer Vision (WACV). Proceedings, P1275, DOI 10.1109/WACV45572.2020.9093498
   Zhang S, 2018, PROC CVPR IEEE, P4203, DOI 10.1109/CVPR.2018.00442
   Zhang SX, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22166090
   Zhang XS, 2022, IEEE T PATTERN ANAL, V44, P3096, DOI 10.1109/TPAMI.2021.3050494
   Zhang Y, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3542820
   Zhao J, 2022, BIOMIMETICS-BASEL, V7, DOI 10.3390/biomimetics7010031
   Zhao XM, 2020, IEEE SENS J, V20, P4901, DOI 10.1109/JSEN.2020.2966034
   Zhao Zhong-Qiu, 2019, IEEE Trans Neural Netw Learn Syst, V30, P3212, DOI 10.1109/TNNLS.2018.2876865
   Zhen PN, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3513133
   Zheng ZD, 2021, IEEE T MULTIMEDIA, V23, P2683, DOI 10.1109/TMM.2020.3014488
   Zhou DQ, 2021, Arxiv, DOI arXiv:2103.11886
   Zhou J, 2007, IEEE T VEH TECHNOL, V56, P51, DOI 10.1109/TVT.2006.883735
   Zhou XY, 2019, Arxiv, DOI arXiv:1904.07850
   Zhu CC, 2019, PROC CVPR IEEE, P840, DOI 10.1109/CVPR.2019.00093
   Zhu XZ, 2021, Arxiv, DOI [arXiv:2010.04159, 10.48550/arXiv.2010.04159]
NR 79
TC 1
Z9 1
U1 6
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2024
VL 20
IS 2
SI SI
AR 36
DI 10.1145/3584361
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W6GR4
UT WOS:001092595800006
DA 2024-08-05
ER

PT J
AU Qu, XF
   Liu, L
   Zhu, L
   Nie, LQ
   Zhang, HX
AF Qu, Xiaofeng
   Liu, Li
   Zhu, Lei
   Nie, Liqiang
   Zhang, Huaxiang
TI Instance-level Adversarial Source-free Domain Adaptive Person
   Re-identification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Person re-identification; source-free domain adaptation; adversarial
   attacks; mutual teaching
AB Domain adaption (DA) for person re-identification (ReID) has attained considerable progress by transferring knowledge from a source domain with labels to a target domain without labels. Nonetheless, most of the existing methods require access to source data, which raises privacy concerns. Source-free DA has recently emerged as a response to these privacy challenges, yet its direct application to open-set pedestrian re-identification tasks is hindered by the reliance on a shared category space in existing methods. Current source-free DA approaches for person ReID still encounter several obstacles, particularly the divergence-agnostic problem and the notable domain divergence due to the absent source data. In this article, we introduce an Instance-level Adversarial Mutual Teaching (IAMT) framework, which utilizes adversarial views to tackle the challenges mentioned above. Technically, we first elaborately develop a variance-based division (VBD) module to segregate the target data into instance-level subsets based on their similarity and dissimilarity to the source using the source-trained model, implicitly tackling the divergence-agnostic problem. To mitigate domain divergence, we additionally introduce a dynamic adversarial alignment (DAA) strategy, aiming to enhance the consistence of feature distribution across domains by employing adversarial instances from the target data to confuse the discriminators. Experiments reveal the superiority of the IAMT over state-of-the-art methods for DA person ReID tasks, while preserving the privacy of the source data.
C1 [Qu, Xiaofeng; Liu, Li; Zhang, Huaxiang] Shandong Normal Univ, Jinan 250358, Peoples R China.
   [Zhu, Lei] Tongji Univ, Shanghai 200092, Peoples R China.
   [Nie, Liqiang] Harbin Inst Technol Shenzhen, Shenzhen 518055, Peoples R China.
   [Zhang, Huaxiang] Aerosp Informat Technol Univ, Jinan 250200, Peoples R China.
C3 Shandong Normal University; Tongji University; Harbin Institute of
   Technology
RP Liu, L; Zhang, HX (corresponding author), Shandong Normal Univ, Jinan 250358, Peoples R China.; Zhang, HX (corresponding author), Aerosp Informat Technol Univ, Jinan 250200, Peoples R China.
EM quxf@163.com; liuli_790209@163.com; leizhu0608@gmail.com;
   nieliqiang@gmail.com; huaxzhang@hotmail.com
RI Zhu, Lei/GQQ-1130-2022
OI Zhu, Lei/0000-0002-5348-7532; Zhu, Lei/0000-0002-2993-7142; zhang, hua
   xiang/0000-0001-6259-7533
FU National Natural Science Foundation of China [62176144, 62076153,
   U1836216]; major fundamental research project of Shandong, China
   [ZR2019ZD03]; Taishan Scholar Project of Shandong, China [ts20190924];
   CCF-Baidu Open Fund [CCF-BAIDU OF2022008]
FX The work is partially supported by the National Natural Science
   Foundation of China (Nos. 62176144, 62076153, U1836216); the major
   fundamental research project of Shandong, China (No. ZR2019ZD03); and
   the Taishan Scholar Project of Shandong, China (No. ts20190924), and in
   part by CCF-Baidu Open Fund under Grant CCF-BAIDU OF2022008.
CR Agarwal P, 2021, Arxiv, DOI arXiv:2103.14577
   Bertocco GC, 2021, IEEE T INF FOREN SEC, V16, P4419, DOI 10.1109/TIFS.2021.3107157
   Chen F, 2023, PATTERN RECOGN, V138, DOI 10.1016/j.patcog.2023.109369
   Chen H, 2021, IEEE WINT CONF APPL, P1, DOI 10.1109/WACV48630.2021.00005
   Chen H, 2021, PROC CVPR IEEE, P2004, DOI 10.1109/CVPR46437.2021.00204
   Cho Y, 2022, PROC CVPR IEEE, P7298, DOI 10.1109/CVPR52688.2022.00716
   Chu Q., 2023, arXiv, DOI DOI 10.48550/ARXIV.2301.04265
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng WJ, 2018, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.2018.00110
   Ding YH, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3369393
   Domingos P, 2012, COMMUN ACM, V55, P78, DOI 10.1145/2347736.2347755
   Dongkai Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10978, DOI 10.1109/CVPR42600.2020.01099
   Fan HH, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3243316
   Fang Zhao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P526, DOI 10.1007/978-3-030-58621-8_31
   Fu DP, 2022, PROC CVPR IEEE, P2466, DOI 10.1109/CVPR52688.2022.00251
   Fu Y, 2019, IEEE I CONF COMP VIS, P6111, DOI 10.1109/ICCV.2019.00621
   Ge Y., 2020, Advances in neural information processing systems, V33, P11309
   Ge YX, 2020, Arxiv, DOI [arXiv:2001.01526, 10.48550/arXiv.2001.01526]
   Ge YX, 2024, IEEE T NEUR NET LEAR, V35, P258, DOI 10.1109/TNNLS.2022.3173489
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He T, 2022, AAAI CONF ARTIF INTE, P879
   Hou YZ, 2021, Arxiv, DOI arXiv:2008.07514
   Jia XM, 2022, IEEE T IMAGE PROCESS, V31, P4227, DOI 10.1109/TIP.2022.3183469
   Ke X, 2022, IEEE T CIRC SYST VID, V32, P7924, DOI 10.1109/TCSVT.2022.3188551
   Kim Y, 2021, Arxiv, DOI arXiv:2007.01524
   Kurmi VK, 2021, IEEE WINT CONF APPL, P615, DOI 10.1109/WACV48630.2021.00066
   Li Rui, 2020, P IEEE CVF C COMP VI, P9641
   Li YJ, 2019, IEEE I CONF COMP VIS, P7918, DOI 10.1109/ICCV.2019.00801
   Liang J., 2020, International Conference on Machine Learning, P6028
   Lin M, 2014, Arxiv, DOI arXiv:1312.4400
   Lin YT, 2020, PROC CVPR IEEE, P3387, DOI 10.1109/CVPR42600.2020.00345
   Lin YT, 2019, AAAI CONF ARTIF INTE, P8738
   Liu XB, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P547, DOI 10.1145/3394171.3413904
   Liu Y, 2021, PROC CVPR IEEE, P1215, DOI 10.1109/CVPR46437.2021.00127
   Liu YX, 2023, IEEE T CIRC SYST VID, V33, P326, DOI 10.1109/TCSVT.2022.3200671
   Luo CC, 2023, IEEE T PATTERN ANAL, V45, P1963, DOI 10.1109/TPAMI.2022.3167053
   Ning X, 2021, IEEE T CIRC SYST VID, V31, P3391, DOI 10.1109/TCSVT.2020.3043026
   Pang B, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3485061
   Paszke A, 2019, ADV NEUR IN, V32
   Peng Yunjie, 2023, ACM Trans. Multim. Comput., Commun., Appl., V20, P1
   Rami Hamza, 2022, P IEEECVF C COMPUTER, P3830
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Tao YS, 2023, IEEE T MULTIMEDIA, V25, P4586, DOI 10.1109/TMM.2022.3178599
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang JD, 2023, IEEE T KNOWL DATA EN, V35, P8052, DOI 10.1109/TKDE.2022.3178128
   Wang JY, 2018, PROC CVPR IEEE, P2275, DOI 10.1109/CVPR.2018.00242
   Wang ML, 2021, AAAI CONF ARTIF INTE, V35, P2764
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wu AC, 2023, IEEE T PATTERN ANAL, V45, P15512, DOI 10.1109/TPAMI.2023.3292936
   Wu GL, 2020, AAAI CONF ARTIF INTE, V34, P12362
   Xiang Suncheng, 2023, ACM Trans. Multim. Comput. Commun. Appl., V20, P1
   Xu X, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3532866
   Yan G, 2023, IEEE T CIRC SYST VID, V33, P4217, DOI 10.1109/TCSVT.2023.3241764
   Yang FX, 2020, AAAI CONF ARTIF INTE, V34, P12597
   Yang Zou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P87, DOI 10.1007/978-3-030-58536-5_6
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Yin JH, 2022, PATTERN RECOGN, V126, DOI 10.1016/j.patcog.2022.108568
   Zha LF, 2023, IEEE INT CON MULTI, P1547, DOI 10.1109/ICME55011.2023.00267
   Zhang GQ, 2022, IEEE T CIRC SYST VID, V32, P6766, DOI 10.1109/TCSVT.2022.3169422
   Zheng KC, 2021, PROC CVPR IEEE, P5306, DOI 10.1109/CVPR46437.2021.00527
   Zheng KC, 2021, AAAI CONF ARTIF INTE, V35, P3538
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI 10.1109/CVPR.2019.00224
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2018, LECT NOTES COMPUT SC, V11217, P176, DOI 10.1007/978-3-030-01261-8_11
   Zhong Z, 2021, IEEE T PATTERN ANAL, V43, P2723, DOI 10.1109/TPAMI.2020.2976933
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhong Z, 2019, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2019.00069
   Zhong Z, 2019, IEEE T IMAGE PROCESS, V28, P1176, DOI 10.1109/TIP.2018.2874313
   Zhou KY, 2023, IEEE T PATTERN ANAL, V45, P4396, DOI 10.1109/TPAMI.2022.3195549
NR 71
TC 0
Z9 0
U1 4
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2024
VL 20
IS 7
SI SI
AR 197
DI 10.1145/3649900
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL0Q6
UT WOS:001234494100012
DA 2024-08-05
ER

PT J
AU Zhang, ZC
   Sun, W
   Wu, HN
   Zhou, YJ
   Li, CY
   Chen, ZJ
   Min, XK
   Zhai, GT
   Lin, WS
AF Zhang, Zicheng
   Sun, Wei
   Wu, Haoning
   Zhou, Yingjie
   Li, Chunyi
   Chen, Zijian
   Min, Xiongkuo
   Zhai, Guangtao
   Lin, Weisi
TI GMS-3DQA: Projection-Based Grid Mini-patch Sampling for 3D ModelQuality
   Assessment
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE 3D model quality assessment; no-reference; projection-based; mini-patch;
   efficient
ID IMAGE QUALITY ASSESSMENT; POINT CLOUD QUALITY; VISUAL QUALITY; BIT
   ALLOCATION; MODEL; ERROR
AB Nowadays, most three-dimensional model quality assessment (3DQA) methods have been aimed at improving accuracy. However, little attention has been paid to the computational cost and inference time required for practical applications. Model-based 3DQA methods extract features directly from the 3D models, which are characterized by their high degree of complexity. As a result, many researchers are inclined towards utilizing projection-based 3DQA methods. Nevertheless, previous projection-based 3DQA methods directly extract features from multi-projections to ensure quality prediction accuracy, which calls for more resource consumption and inevitably leads to inefficiency. Thus, in this article, we address this challenge by proposing a no-reference (NR) projection-based Grid Mini-patch Sampling 3D Model Quality Assessment (GMS-3DQA) method. The projection images are rendered from six perpendicular viewpoints of the 3D model to cover sufficient quality information. To reduce redundancy and inference resources, we propose a multi-projection grid mini-patch sampling strategy (MP-GMS), which samples grid mini-patches from the multi-projections and forms the sampled grid mini-patches into one quality mini-patch map (QMM). The Swin-Transformer tiny backbone is then used to extract quality-aware features from the QMMs. The experimental results show that the proposed GMS-3DQA outperforms existing state-of-the-art NR-3DQA methods on the point cloud quality assessment databases for both accuracy and efficiency. The efficiency analysis reveals that the proposed GMS-3DQA requires far less computational resources and inference time than other 3DQA competitors. The code is available at https://github.com/zzc-1998/GMS-3DQA.
C1 [Zhang, Zicheng; Sun, Wei; Zhou, Yingjie; Li, Chunyi; Chen, Zijian; Min, Xiongkuo; Zhai, Guangtao] Shanghai Jiao Tong Univ, Inst Image Commun & Network Engn, 800 Dongchuan Rd, Shanghai 200240, Peoples R China.
   [Wu, Haoning] Nanyang Technol Univ, S Lab, 50 Nanyang Ave, Singapore 639798, Singapore.
   [Lin, Weisi] Nanyang Technol Univ, Sch Comp Sci & Engn, 50 Nanyang Ave, Singapore 639798, Singapore.
C3 Shanghai Jiao Tong University; Nanyang Technological University; Nanyang
   Technological University
RP Zhai, GT (corresponding author), Shanghai Jiao Tong Univ, Inst Image Commun & Network Engn, 800 Dongchuan Rd, Shanghai 200240, Peoples R China.
EM zzc1998@sjtu.edu.cn; sunguwei@sjtu.edu.cn; haoning001@e.ntu.edu.sg;
   zyj2000@sjtu.edu.cn; lcysyzxdxc@sjtu.edu.cn; zijian.chen@sjtu.edu.cn;
   minxiongkuo@sjtu.edu.cn; zhaiguangtao@sjtu.edu.cn; wslin@ntu.edu.sg
RI ZHOU, YINGJIE/KLE-8614-2024; Lin, Wei/D-3353-2012; li,
   chunyi/KIG-6117-2024; Zhai, Guangtao/X-5949-2019; Chen,
   Zijian/JFA-3575-2023; Lin, Weisi/A-3696-2011
OI Zhai, Guangtao/0000-0001-8165-9322; Chen, Zijian/0000-0002-8502-4110;
   Zhang, Zicheng/0000-0002-7247-7938; Zhou, Yingjie/0009-0001-3915-8257;
   Lin, Weisi/0000-0001-9866-1947; Sun, Wei/0000-0001-8162-1949
FU NSFC [62225112, 61831015]; Fundamental Research Funds for the Central
   Universities; National Key R&D Program of China [2021YFE0206700];
   Shanghai Municipal Science and Technology Major Project [2021SHZDZX0102]
FX This work was supported in part by NSFC (No. 62225112, No. 61831015),
   the Fundamental Research Funds for the Central Universities, National
   Key R&D Program of China 2021YFE0206700, and Shanghai Municipal Science
   and Technology Major Project (2021SHZDZX0102).
CR Abouelaziz I, 2020, PATTERN RECOGN, V100, DOI 10.1016/j.patcog.2019.107174
   Abouelaziz I, 2017, IEEE IMAGE PROC, P755, DOI 10.1109/ICIP.2017.8296382
   Abouelaziz I, 2016, LECT NOTES COMPUT SC, V9680, P369, DOI 10.1007/978-3-319-33618-3_37
   Alexiou E, 2020, IEEE INT CONF MULTI
   Alexiou E, 2018, IEEE INT CON MULTI
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fan Y, 2022, IEEE INT WORKSH MULT, DOI 10.1109/MMSP55362.2022.9949359
   Fang YM, 2022, AAAI CONF ARTIF INTE, P580
   Graf H., 2006, IEEE INT S VIRT ENV, P178
   Graziosi D, 2020, APSIPA TRANS SIGNAL, V9, DOI 10.1017/ATSIP.2020.12
   Gu J, 2019, AAAI CONF ARTIF INTE, P8336
   Gu K, 2015, IEEE T BROADCAST, V61, P520, DOI 10.1109/TBC.2015.2459851
   Guo JJ, 2017, ACM T APPL PERCEPT, V14, DOI 10.1145/2996296
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Javaheri A, 2020, INT WORK QUAL MULTIM, DOI 10.1109/qomex48832.2020.9123087
   Kingma D. P., 2015, P INT C LEARN REPR, P1
   Lavoué G, 2011, COMPUT GRAPH FORUM, V30, P1427, DOI 10.1111/j.1467-8659.2011.02017.x
   Li LD, 2016, IEEE T CYBERNETICS, V46, P39, DOI 10.1109/TCYB.2015.2392129
   Li L, 2021, IEEE T CIRC SYST VID, V31, P326, DOI 10.1109/TCSVT.2020.2966118
   Liu Q, 2021, IEEE T CIRC SYST VID, V31, P4645, DOI 10.1109/TCSVT.2021.3100282
   Liu Q, 2021, IEEE T MULTIMEDIA, V23, P3278, DOI 10.1109/TMM.2020.3023294
   Liu Q, 2021, IEEE T IMAGE PROCESS, V30, P6623, DOI 10.1109/TIP.2021.3096060
   Liu YP, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3550274
   Liu YT, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3414837
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Mekuria R, 2016, 16332 ISO IEC MPEG
   Mekuria R, 2017, IEEE T CIRC SYST VID, V27, P828, DOI 10.1109/TCSVT.2016.2543039
   Meynet G, 2020, INT WORK QUAL MULTIM, DOI 10.1109/qomex48832.2020.9123147
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Nehme Y, 2022, Arxiv, DOI arXiv:2202.02397
   Nehmé Y, 2021, IEEE T VIS COMPUT GR, V27, P2202, DOI 10.1109/TVCG.2020.3036153
   Schwarz S, 2019, IEEE J EM SEL TOP C, V9, P133, DOI 10.1109/JETCAS.2018.2885981
   Shan Ziyu, 2023, IEEE Transactions on Visualization and Computer Graphics
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Su HL, 2019, IEEE IMAGE PROC, P3182, DOI [10.1109/icip.2019.8803298, 10.1109/ICIP.2019.8803298]
   Tian DH, 2008, IEEE T CIRC SYST VID, V18, P23, DOI 10.1109/TCSVT.2007.903319
   Tian D, 2017, IEEE IMAGE PROC, P3460, DOI 10.1109/ICIP.2017.8296925
   Torlig EM, 2018, PROC SPIE, V10752, DOI 10.1117/12.2322741
   Tu RW, 2023, IEEE T EM TOP COMP I, V7, P462, DOI 10.1109/TETCI.2022.3201619
   Vása L, 2012, COMPUT GRAPH FORUM, V31, P1715, DOI 10.1111/j.1467-8659.2012.03176.x
   Video Quality Experts Group, 2000, VQEG M OTT CAN MARCH
   Wang K, 2012, COMPUT GRAPH-UK, V36, P808, DOI 10.1016/j.cag.2012.06.004
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei Sun, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P856, DOI 10.1145/3503161.3548329
   Wen SG, 2021, Arxiv, DOI arXiv:2111.07104
   Woo S., 2023, PREPRINT
   Wu HN, 2023, Arxiv, DOI arXiv:2211.04894
   Wu HN, 2022, LECT NOTES COMPUT SC, V13666, P538, DOI 10.1007/978-3-031-20068-7_31
   Wu Haoning, 2022, arXiv
   Xie Wuyuan, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P3250, DOI 10.1145/3581783.3611998
   Yang Q, 2022, PROC CVPR IEEE, P21147, DOI 10.1109/CVPR52688.2022.02050
   Yang Q, 2021, IEEE T MULTIMEDIA, V23, P3877, DOI 10.1109/TMM.2020.3033117
   Yang Qi, 2022, IEEE Trans Pattern Anal Mach Intell, V44, P3015, DOI 10.1109/TPAMI.2020.3047083
   Yang Q, 2020, IEEE T BROADCAST, V66, P310, DOI 10.1109/TBC.2019.2954063
   Zhai GT, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3457905
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang XY, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3532625
   Zhang Z., 2021, 2021 IEEE INT C MULT, P1, DOI DOI 10.1109/ICME51207.2021.9428312
   Zhang ZC, 2024, IEEE T BROADCAST, DOI 10.1109/TBC.2023.3345656
   Zhang ZC, 2024, ACM T MULTIM COMPUT, V20, DOI 10.1145/3631357
   Zhang ZC, 2023, PROC CVPR IEEE, P1746, DOI 10.1109/CVPR52729.2023.00174
   Zhang ZC, 2023, Arxiv, DOI arXiv:2307.02808
   Zhang ZC, 2022, IEEE T CIRC SYST VID, V32, P7618, DOI 10.1109/TCSVT.2022.3186894
   Zhang ZC, 2021, IEEE INT CONF MULTI, DOI 10.1109/ICMEW53276.2021.9455963
   Zhang Zicheng, 2023, IJCAI
   Zhang Zicheng, 2023, ICASSP 2023 2023 IEE, P1
   Zhang Zicheng, 2023, IEEE Transactions on Multimedia
   Zhou QY, 2018, Arxiv, DOI arXiv:1801.09847
   Zhou W., 2022, arXiv
   Zhou W, 2023, IEEE SIGNAL PROC LET, V30, P354, DOI 10.1109/LSP.2023.3264105
NR 71
TC 0
Z9 0
U1 3
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2024
VL 20
IS 6
SI SI
AR 178
DI 10.1145/3643817
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OQ2T3
UT WOS:001208681800028
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Wang, J
   Li, GQ
   Shi, J
   Xi, JW
AF Wang, Jie
   Li, Guoqiang
   Shi, Jie
   Xi, Jinwen
TI Weighted Guided Optional Fusion Network for RGB-T Salient Object
   Detection
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Salient object detection; RGB-T; transformer; modality contribution
   weights; cross-modal fusion
AB There is no doubt that the rational and effective use of visible and thermal infrared image data information to achieve cross-modal complementary fusion is the key to improving the performance of RGB-T salient object detection (SOD). A meticulous analysis of the RGB-T SOD data reveals that it mainly consists of three scenarios in which both modalities (RGB and T) have a significant foreground and only a single modality (RGB or T) is disturbed. However, existing methods are obsessed with pursuing more effective cross-modal fusion based on treating both modalities equally. Obviously, the subjective use of equivalence has two significant limitations. Firstly, it does not allow for practical discrimination of which modality makes the dominant contribution to performance. While both modalities may have visually significant foregrounds, differences in their imaging properties will result in distinct performance contributions. Secondly, in a specific acquisition scenario, a pair of images with two modalities will contribute differently to the final detection performance due to their varying sensitivity to the same background interference. Intelligibly, for the RGB-T saliency detection task, it would be more reasonable to generate exclusive weights for the two modalities and select specific fusion mechanisms based on different weight configurations to perform cross-modal complementary integration. Consequently, we propose a weighted guided optional fusion network (WGOFNet) for RGB-T SOD. Specifically, a feature refinement module is first used to perform an initial refinement of the extracted multilevel features. Subsequently, a weight generation module (WGM) will generate exclusive network performance contribution weights for each of the two modalities, and an optional fusion module (OFM) will rely on this weight to perform particular integration of cross-modal information. Simple cross-level fusion is finally utilized to obtain the final saliency prediction map. Comprehensive experiments on three publicly available benchmark datasets demonstrate the proposed WGOFNet achieves superior performance compared with the state-of-the-art RGB-T SOD methods. The source code is available at: https://github.com/WJ-CV/WGOFNet.
C1 [Wang, Jie] Tianjin Univ, Coll Intelligence & Comp, Tianjin, Peoples R China.
   [Li, Guoqiang; Xi, Jinwen] Chinese Acad Sci, Inst Sci & Dev, Beijing, Peoples R China.
   [Shi, Jie] North Automat Control Technol Inst, Taiyuan, Peoples R China.
   [Xi, Jinwen] Zhongguancun Lab, Beijing, Peoples R China.
C3 Tianjin University; Chinese Academy of Sciences; Zhongguancun Laboratory
RP Li, GQ; Xi, JW (corresponding author), Chinese Acad Sci, Inst Sci & Dev, Beijing, Peoples R China.
EM wangjiexy@tju.edu.cn; ligq@tongji.edu.cn; shijie_nwpu@163.com;
   xijw@zgclab.edu.cn
OI Xi, Jinwen/0000-0002-7504-3457; Wang, Jie/0000-0002-7266-8179
FU Scientific and Technological Innovation 2030 [22022ZD0116407]
FX Guoqiang Li makes outstanding contributions to the writing of this
   article and is a co-first author of this article. This work was
   supported by the Scientific and Technological Innovation 2030
   (22022ZD0116407).
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Bao YQ, 2021, J VIS COMMUN IMAGE R, V80, DOI 10.1016/j.jvcir.2021.103306
   Chen CLZ, 2021, IEEE T IMAGE PROCESS, V30, P3995, DOI 10.1109/TIP.2021.3068644
   Chen CLZ, 2021, IEEE T IMAGE PROCESS, V30, P2350, DOI 10.1109/TIP.2021.3052069
   Chen CLZ, 2020, IEEE T IMAGE PROCESS, V29, P4296, DOI 10.1109/TIP.2020.2968250
   Chen CLZ, 2020, IEEE T IMAGE PROCESS, V29, P1090, DOI 10.1109/TIP.2019.2934350
   Chen CZ, 2017, IEEE T IMAGE PROCESS, V26, P3156, DOI 10.1109/TIP.2017.2670143
   Chen G, 2022, IEEE T CIRC SYST VID, V32, P6308, DOI 10.1109/TCSVT.2022.3166914
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen Shaoxiang, 2020, P EUR C COMP VIS ECC, DOI DOI 10.1007/978-3-030-58598-3_31
   Chen ZY, 2021, IEEE T IMAGE PROCESS, V30, P7012, DOI 10.1109/TIP.2020.3028289
   Cheng MM, 2021, INT J COMPUT VISION, V129, P2622, DOI [10.1007/s11263-021-01490-8, 10.1109/ICCV.2017.487]
   Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414
   Cong RM, 2023, IEEE T MULTIMEDIA, V25, P6971, DOI 10.1109/TMM.2022.3216476
   Fan DP, 2018, Arxiv, DOI arXiv:1805.10421
   Feng G, 2022, PATTERN RECOGN, V128, DOI 10.1016/j.patcog.2022.108666
   Fu KR, 2020, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR42600.2020.00312
   Gao W, 2022, IEEE T CIRC SYST VID, V32, P2091, DOI 10.1109/TCSVT.2021.3082939
   Gao Y, 2015, IEEE T MULTIMEDIA, V17, P359, DOI 10.1109/TMM.2015.2389616
   Guo QL, 2021, IEEE SIGNAL PROC LET, V28, P1655, DOI 10.1109/LSP.2021.3102524
   Huang LM, 2022, IEEE T CIRC SYST VID, V32, P1366, DOI 10.1109/TCSVT.2021.3069812
   Huang LM, 2020, IEEE SIGNAL PROC LET, V27, P1585, DOI 10.1109/LSP.2020.3020735
   Huang NAC, 2022, IEEE T MULTIMEDIA, V24, P1651, DOI 10.1109/TMM.2021.3069297
   Huo FS, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3185323
   Huo FS, 2022, IEEE T CIRC SYST VID, V32, P3111, DOI 10.1109/TCSVT.2021.3102268
   Ji W, 2021, PROC CVPR IEEE, P9466, DOI 10.1109/CVPR46437.2021.00935
   Jia XY, 2021, IEEE INT CONF COMP V, P3489, DOI 10.1109/ICCVW54120.2021.00389
   Li GY, 2021, IEEE T IMAGE PROCESS, V30, P3528, DOI 10.1109/TIP.2021.3062689
   Liang YH, 2022, NEUROCOMPUTING, V490, P132, DOI 10.1016/j.neucom.2022.03.029
   Liao GB, 2022, IEEE T CIRC SYST VID, V32, P7646, DOI 10.1109/TCSVT.2022.3184840
   Liu JJ, 2023, IEEE T PATTERN ANAL, V45, P887, DOI 10.1109/TPAMI.2021.3140168
   Liu JY, 2022, PROC CVPR IEEE, P5792, DOI 10.1109/CVPR52688.2022.00571
   Liu N, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4702, DOI 10.1109/ICCV48922.2021.00468
   Liu ZY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4481, DOI 10.1145/3474085.3475601
   Liu ZY, 2022, IEEE T CIRC SYST VID, V32, P4486, DOI 10.1109/TCSVT.2021.3127149
   Margolin R, 2014, PROC CVPR IEEE, P248, DOI 10.1109/CVPR.2014.39
   Pang YW, 2023, IEEE T IMAGE PROCESS, V32, P892, DOI 10.1109/TIP.2023.3234702
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Shao L, 2006, PATTERN RECOGN, V39, P1932, DOI 10.1016/j.patcog.2006.04.010
   Song KC, 2023, IEEE T INSTRUM MEAS, V72, DOI 10.1109/TIM.2023.3236346
   Song KC, 2023, IEEE-ASME T MECH, V28, P1558, DOI 10.1109/TMECH.2022.3215909
   Sun F, 2022, IEEE SIGNAL PROC LET, V29, P1714, DOI 10.1109/LSP.2022.3194843
   Sun P, 2021, PROC CVPR IEEE, P1407, DOI 10.1109/CVPR46437.2021.00146
   Tu ZZ, 2023, IEEE T MULTIMEDIA, V25, P4163, DOI 10.1109/TMM.2022.3171688
   Tu ZZ, 2022, IEEE T IMAGE PROCESS, V31, P3752, DOI 10.1109/TIP.2022.3176540
   Tu ZZ, 2021, IEEE T IMAGE PROCESS, V30, P5678, DOI 10.1109/TIP.2021.3087412
   Tu ZZ, 2020, IEEE T MULTIMEDIA, V22, P160, DOI 10.1109/TMM.2019.2924578
   Tu ZZ, 2019, 2019 2ND IEEE CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2019), P141, DOI 10.1109/MIPR.2019.00032
   Wang G., 2018, P 13 C IM GRAPH TECH, P359
   Wang GT, 2021, PROC CVPR IEEE, P15114, DOI 10.1109/CVPR46437.2021.01487
   Wang J, 2022, ENG APPL ARTIF INTEL, V114, DOI 10.1016/j.engappai.2022.105162
   Wang J, 2022, IEEE T CIRC SYST VID, V32, P2949, DOI 10.1109/TCSVT.2021.3099120
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wen HF, 2021, IEEE T IMAGE PROCESS, V30, P9179, DOI 10.1109/TIP.2021.3123548
   Wu YH, 2022, IEEE T IMAGE PROCESS, V31, P3125, DOI 10.1109/TIP.2022.3164550
   Wu YH, 2022, IEEE T PATTERN ANAL, V44, P10261, DOI 10.1109/TPAMI.2021.3134684
   Yu NN, 2023, VISUAL COMPUT, V39, P1251, DOI 10.1007/s00371-022-02402-8
   Yu NN, 2022, IET IMAGE PROCESS, V16, P535, DOI 10.1049/ipr2.12369
   Zhang J, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4318, DOI 10.1109/ICCV48922.2021.00430
   Zhang PY, 2022, PROC CVPR IEEE, P8876, DOI 10.1109/CVPR52688.2022.00868
   Zhang PP, 2020, PATTERN RECOGN, V100, DOI 10.1016/j.patcog.2019.107130
   Zhang Q, 2021, IEEE T CIRC SYST VID, V31, P1804, DOI 10.1109/TCSVT.2020.3014663
   Zhang Q, 2020, IEEE T IMAGE PROCESS, V29, P3321, DOI 10.1109/TIP.2019.2959253
   Zhang Yahong Han Zihao, 2023, ACM INT C MULTIMEDIA
   Zhao JW, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1745, DOI 10.1145/3394171.3413855
   Zhou H.Y., 2023, IEEE Transactions on Image Processing
   Zhou T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4661, DOI 10.1109/ICCV48922.2021.00464
   Zhou T, 2021, COMPUT VIS MEDIA, V7, P37, DOI 10.1007/s41095-020-0199-z
   Zhou WJ, 2022, IEEE T EM TOP COMP I, V6, P957, DOI 10.1109/TETCI.2021.3118043
   Zhou WJ, 2022, IEEE T CIRC SYST VID, V32, P1224, DOI 10.1109/TCSVT.2021.3077058
NR 70
TC 0
Z9 0
U1 6
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2024
VL 20
IS 5
AR 136
DI 10.1145/3624984
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MF3R9
UT WOS:001192177900016
OA Bronze
DA 2024-08-05
ER

PT J
AU Ren, B
   Tang, H
   Meng, FY
   Ding, RW
   Torr, PHS
   Sebe, N
AF Ren, Bin
   Tang, Hao
   Meng, Fanyang
   Ding Runwei
   Torr, Philip H. S.
   Sebe, Nicu
TI Cloth Interactive Transformer for Virtual Try-On
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Virtual try-on; transformer; garment transfer; cross attention
ID IMAGE; NETWORKS
AB The 2D image-based virtual try-on has aroused increased interest from the multimedia and computer vision fields due to its enormous commercial value. Nevertheless, most existing image-based virtual try-on approaches directly combine the person-identity representation and the in-shop clothing items without taking their mutual correlations into consideration. Moreover, these methods are commonly established on pure convolutional neural networks (CNNs) architectures which are not simple to capture the long-range correlations among the input pixels. As a result, it generally results in inconsistent results. To alleviate these issues, in this article, we propose a novel two-stage cloth interactive transformer (CIT) method for the virtual try-on task. During the first stage, we design a CIT matching block, aiming at precisely capturing the long-range correlations between the cloth-agnostic person information and the in-shop cloth information. Consequently, it makes the warped in-shop clothing items look more natural in appearance. In the second stage, we put forth a CIT reasoning block for establishing global mutual interactive dependencies among person representation, the warped clothing item, and the corresponding warped cloth mask. The empirical results, based on mutual dependencies, demonstrate that the final try-on results are more realistic. Substantial empirical results on a public fashion dataset illustrate that the suggested CIT attains competitive virtual try-on performance.
C1 [Ren, Bin; Sebe, Nicu] Univ Trento, Via Sommarive 9, I-38123 Trento, Italy.
   [Ren, Bin] Univ Pisa, Via Sommarive 9, I-38123 Trento, Italy.
   [Tang, Hao] Swiss Fed Inst Technol, Sternwartstr 7, CH-8092 Zurich, Switzerland.
   [Meng, Fanyang; Ding Runwei] Peng Cheng Lab, 2 Xingke 1st St, Shenzhen 518000, Guangdong, Peoples R China.
   [Torr, Philip H. S.] Univ Oxford, Parks Rd, Oxford OX1 3PJ, England.
C3 University of Trento; University of Pisa; Swiss Federal Institutes of
   Technology Domain; ETH Zurich; Peng Cheng Laboratory; University of
   Oxford
RP Ren, B (corresponding author), Univ Trento, Via Sommarive 9, I-38123 Trento, Italy.; Ren, B (corresponding author), Univ Pisa, Via Sommarive 9, I-38123 Trento, Italy.
EM bin.ren@unitn.it; hao.tang@vision.ee.ethz.ch; mengfy@pcl.ac.cn;
   dingrw@pcl.ac.cn; philip.torr@eng.ox.ac.uk; niculae.sebe@unitn.it
RI Sebe, Niculae/KEC-2000-2024
OI Sebe, Niculae/0000-0002-6597-7248; Meng, Fanyang/0000-0001-5725-2178;
   Tang, Hao/0000-0002-2077-1246; Ren, Bin/0000-0002-9790-1504
FU National Ph.D. in Artificial Intelligence for Society Program of Italy;
   MUR PNRR project FAIR - NextGenerationEU [PE00000013]; EU H2020 AI4Media
   Project [951911]
FX This work was supported by the National Ph.D. in Artificial Intelligence
   for Society Program of Italy, the MUR PNRR project FAIR (PE00000013)
   funded by the NextGenerationEU and the EU H2020 AI4Media Project under
   Grant 951911.
CR Alldieck T, 2018, PROC CVPR IEEE, P8387, DOI 10.1109/CVPR.2018.00875
   Bai S, 2022, LECT NOTES COMPUT SC, V13675, P409, DOI 10.1007/978-3-031-19784-0_24
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792
   Brouet R, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185532
   Chen WZ, 2016, INT CONF 3D VISION, P479, DOI 10.1109/3DV.2016.58
   Choi S, 2021, PROC CVPR IEEE, P14126, DOI 10.1109/CVPR46437.2021.01391
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Chopra A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5413, DOI 10.1109/ICCV48922.2021.00538
   Cui Aiyu, 2021, P IEEECVF INT C COMP, P14638
   Devlin J, 2018, ARXIV
   Dong HY, 2019, IEEE I CONF COMP VIS, P1161, DOI 10.1109/ICCV.2019.00125
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Ehara Jun, 2006, 2006 IEEE/ACM International Symposium on Mixed and Augmented Reality, P139, DOI 10.1109/ISMAR.2006.297805
   Fele B, 2022, IEEE WINT CONF APPL, P2203, DOI 10.1109/WACV51458.2022.00226
   Fincato M, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3491226
   Fincato M, 2021, INT C PATT RECOG, P7669, DOI 10.1109/ICPR48806.2021.9412052
   Ge CJ, 2021, PROC CVPR IEEE, P16923, DOI 10.1109/CVPR46437.2021.01665
   Gu XL, 2021, IEEE T MULTIMEDIA, V23, P2361, DOI 10.1109/TMM.2020.3009500
   Guan P, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185531
   Gundogdu E, 2019, IEEE I CONF COMP VIS, P8738, DOI 10.1109/ICCV.2019.00883
   Han XT, 2018, PROC CVPR IEEE, P7543, DOI 10.1109/CVPR.2018.00787
   Han Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7847, DOI 10.1109/CVPR42600.2020.00787
   Hao Tang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P717, DOI 10.1007/978-3-030-58595-2_43
   He S, 2022, PROC CVPR IEEE, P3460, DOI 10.1109/CVPR52688.2022.00346
   Issenhuth Thibaut, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P619, DOI 10.1007/978-3-030-58565-5_37
   Jaccard P., 1912, NEW PHYTOL, V11, P37, DOI [DOI 10.1111/J.1469-8137.1912.TB05611.X, 10.1111/J.1469-8137.1912.TB05611.X]
   Jetchev N, 2017, IEEE INT CONF COMP V, P2287, DOI 10.1109/ICCVW.2017.269
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Khan S, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3505244
   Lähner Z, 2018, LECT NOTES COMPUT SC, V11208, P698, DOI 10.1007/978-3-030-01225-0_41
   Lee S, 2022, LECT NOTES COMPUT SC, V13677, P204, DOI 10.1007/978-3-031-19790-1_13
   Li KD, 2021, PROC CVPR IEEE, P15541, DOI 10.1109/CVPR46437.2021.01529
   Liu YH, 2022, Arxiv, DOI arXiv:2205.12551
   Liu Y, 2019, IEEE T MULTIMEDIA, V21, P2209, DOI 10.1109/TMM.2019.2897897
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Men YF, 2020, PROC CVPR IEEE, P5083, DOI 10.1109/CVPR42600.2020.00513
   Mikolajczyk K, 2002, LECT NOTES COMPUT SC, V2350, P128, DOI 10.1007/3-540-47969-4_9
   Minar M. R., 2020, P C COMP VIS PATT RE, V2
   Morelli D, 2022, LECT NOTES COMPUT SC, V13668, P345, DOI 10.1007/978-3-031-20074-8_20
   Ott M, 2019, NAACL HLT 2019: THE 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES: PROCEEDINGS OF THE DEMONSTRATIONS SESSION, P48
   Pons-Moll G, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073711
   Ren B., 2021, BRIT MACH VIS C, P1
   Ren B, 2023, PROC CVPR IEEE, P20382, DOI 10.1109/CVPR52729.2023.01952
   Rocco I, 2017, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2017.12
   Salimans T, 2016, ADV NEUR IN, V29
   Schlemper J, 2019, MED IMAGE ANAL, V53, P197, DOI 10.1016/j.media.2019.01.012
   Schmid C, 1997, IEEE T PATTERN ANAL, V19, P530, DOI 10.1109/34.589215
   Sekhavat YA, 2017, IEEE T MULTIMEDIA, V19, P1041, DOI 10.1109/TMM.2016.2639380
   Sekine Masahiro, 2014, Int. Conf. on 3D Body Scanning Technologies, P406
   Strubell E, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P5027
   Tang H., 2020, P BRIT MACH VIS C, P1
   Tolstikhin I, 2021, ADV NEUR IN, V34
   Tsai YHH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6558, DOI 10.18653/v1/p19-1656
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang BC, 2018, LECT NOTES COMPUT SC, V11217, P607, DOI 10.1007/978-3-030-01261-8_36
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xu J, 2021, IEEE T MULTIMEDIA, V23, P2222, DOI 10.1109/TMM.2021.3070972
   Yu RY, 2019, IEEE I CONF COMP VIS, P10510, DOI 10.1109/ICCV.2019.01061
   Yuan ML, 2013, IEEE T MULTIMEDIA, V15, P1958, DOI 10.1109/TMM.2013.2280560
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhu SZ, 2017, IEEE I CONF COMP VIS, P1689, DOI 10.1109/ICCV.2017.186
NR 63
TC 3
Z9 3
U1 6
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2024
VL 20
IS 4
AR 92
DI 10.1145/3617374
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6G9
UT WOS:001153382100002
OA Green Published, Green Submitted, hybrid
DA 2024-08-05
ER

PT J
AU Shi, YY
   Yang, SW
   Yang, WJ
   Shi, DX
   Li, XH
AF Shi, Yanyan
   Yang, Shaowu
   Yang, Wenjing
   Shi, Dianxi
   Li, Xuehui
TI Boosting Few-shot Object Detection with Discriminative Representation
   and Class Margin
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Deep learning; few-shot object detection; transfer learning
AB Classifying and accurately locating a visual category with few annotated training samples in computer vision has motivated the few-shot object detection technique, which exploits transfering the source-domain detection model to the target domain. Under this paradigm, however, such transferred source-domain detection model usually encounters difficulty in the classification of the target domain because of the low data diversity of novel training samples. To combat this, we present a simple yet effective few-shot detector, Transferable RCNN. To transfer general knowledge learned from data-abundant base classes to data-scarce novel classes, we propose aweight transfer strategy to promote model transferability and an attention-based feature enhancement mechanism to learn more robust object proposal feature representations. Further, we ensure strong discrimination by optimizing the contrastive objectives of feature maps via a supervised spatial contrastive loss. Meanwhile, we introduce an angle-guided additive margin classifier to augment instance-level inter-class difference and intra-class compactness, which is beneficial for improving the discriminative power of the few-shot classification head under a few supervisions. Our proposed framework outperforms the current works in various settings of PASCAL VOC and MSCOCO datasets; this demonstrates the effectiveness and generalization ability.
C1 [Shi, Yanyan; Yang, Shaowu; Yang, Wenjing] Natl Univ Def Technol, Coll Comp, Changsha, Hunan, Peoples R China.
   [Shi, Dianxi; Li, Xuehui] Natl Innovat Inst Def Technol, Changsha, Peoples R China.
C3 National University of Defense Technology - China
RP Shi, YY (corresponding author), Natl Univ Def Technol, Coll Comp, Changsha, Hunan, Peoples R China.; Li, XH (corresponding author), Natl Innovat Inst Def Technol, Changsha, Peoples R China.
EM yany_shi@163.com; shaowu.yang@nudt.edu.cn; wjyang1088@163.com;
   dxshi@nudt.edu.cn; doudouli007@163.com
RI Shi, Yaolin/JXN-8322-2024
OI Shi, Dianxi/0000-0002-8112-371X; Yang, Wenjing/0000-0002-6997-0406; shi,
   yanyan/0000-0002-8822-1558
FU Integrated Program of National Natural Science Foundation of China
   [91948303]
FX This work was supported by the Integrated Program of National Natural
   Science Foundation of China (No. 91948303).
CR Antonelli S, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3519022
   Aoxue Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12573, DOI 10.1109/CVPR42600.2020.01259
   Chen H, 2018, AAAI CONF ARTIF INTE, P2836
   Chen WY, 2020, Arxiv, DOI arXiv:1904.04232
   Chen XY, 2020, Arxiv, DOI arXiv:2007.12104
   Cheng H, 2018, LECT NOTES COMPUT SC, V11215, P181, DOI 10.1007/978-3-030-01252-6_11
   Chenkai Yu, 2020, arXiv
   Fan Q, 2020, PROC CVPR IEEE, P4012, DOI 10.1109/CVPR42600.2020.00407
   Fan ZB, 2021, PROC CVPR IEEE, P4525, DOI 10.1109/CVPR46437.2021.00450
   Finn C, 2017, PR MACH LEARN RES, V70
   Guo YH, 2019, PROC CVPR IEEE, P4800, DOI 10.1109/CVPR.2019.00494
   Han GX, 2017, LECT NOTES COMPUT SC, V10636, P14, DOI 10.1007/978-3-319-70090-8_2
   Hou SH, 2019, PROC CVPR IEEE, P831, DOI 10.1109/CVPR.2019.00092
   Ji RY, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3432861
   Jiaxi Wu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P456, DOI 10.1007/978-3-030-58517-4_27
   Kai Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13467, DOI 10.1109/CVPR42600.2020.01348
   Kang BY, 2019, IEEE I CONF COMP VIS, P8419, DOI 10.1109/ICCV.2019.00851
   Karlinsky L, 2019, PROC CVPR IEEE, P5192, DOI 10.1109/CVPR.2019.00534
   Khan S, 2019, PROC CVPR IEEE, P103, DOI 10.1109/CVPR.2019.00019
   Khosla P., 2020, Adv. Neural Inf. Process. Syst, P18661, DOI DOI 10.48550/ARXIV.2004.11362
   Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050
   Li YT, 2021, PROC CVPR IEEE, P15390, DOI 10.1109/CVPR46437.2021.01514
   Liu ST, 2021, Arxiv, DOI arXiv:2011.13677
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Luo XF, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3381086
   Nichol A, 2018, Arxiv, DOI arXiv:1803.02999
   Ouali Y, 2021, LECT NOTES ARTIF INT, V12975, P671, DOI 10.1007/978-3-030-86486-6_41
   Ravi S., 2017, INT C LEARN REPR
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shen DG, 2017, ANNU REV BIOMED ENG, V19, P221, DOI [10.1146/annurev-bioeng-071516-044442, 10.1146/annurev-bioeng-071516044442]
   Simon M, 2015, IEEE I CONF COMP VIS, P1143, DOI 10.1109/ICCV.2015.136
   Snell J, 2017, ADV NEUR IN, V30
   Sun B, 2021, PROC CVPR IEEE, P7348, DOI 10.1109/CVPR46437.2021.00727
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Vanschoren J, 2018, Arxiv, DOI arXiv:1810.03548
   Vinyals O., 2016, Advances in neural information processing systems, V29
   Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552
   Wang X, 2020, Arxiv, DOI arXiv:2003.06957
   Wang X, 2019, PROC CVPR IEEE, P1831, DOI 10.1109/CVPR.2019.00193
   Wang XL, 2021, PROC CVPR IEEE, P3023, DOI 10.1109/CVPR46437.2021.00304
   Wang YX, 2019, IEEE I CONF COMP VIS, P9924, DOI 10.1109/ICCV.2019.01002
   Xiao Y, 2023, IEEE T PATTERN ANAL, V45, P3090, DOI 10.1109/TPAMI.2022.3174072
   Yan XP, 2019, IEEE I CONF COMP VIS, P9576, DOI 10.1109/ICCV.2019.00967
   Yang DB, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3472393
   Yang Y., 2020, P ADV NEUR INF PROC, V33, P3521
   Yonglong Tian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P266, DOI 10.1007/978-3-030-58568-6_16
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
NR 48
TC 0
Z9 0
U1 10
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2024
VL 20
IS 3
AR 75
DI 10.1145/3608478
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6F8
UT WOS:001153381000015
DA 2024-08-05
ER

PT J
AU Wang, J
   Wang, X
   Zhao, GS
AF Wang, Jian
   Wang, Xiao
   Zhao, Guosheng
TI Task Recommendation via Heterogeneous Multi-modal Features and Decision
   Fusion in Mobile Crowdsensing
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Mobile crowd sensing; multi-modal information fusion; similarity
   network; user preference; task recommendation
AB In the decision-making process of the behavior of mobile crowdsensing, using a single view to learn a user's preference will lead to a mismatch between the user's wishes and the final task recommendation list, resulting in the low efficiency of the model recommendation. Aiming at the lack of perceptual representation and cognitive fusion of multimodal coupled information, a task recommendation method based on heterogeneous multimodal features and decision fusion is proposed. According to the content characteristics of multi-source data in the user's historical task set, several task-task similarity matrices are constructed to align feature dimensions and feature semantics. Using the improved similarity network fusion algorithm, networks composed of multiple content similarity matrices are effectively fused into a similarity network. Considering the influence of the time factor, the tasks that have had interest drift are filtered out from the set of tasks that the user has participated in. Finally, the updated similarity network is clustered to predict the current preference of the user for new tasks. Experimental results based on simulation and real datasets show that the proposed method can effectively improve the accuracy and efficiency of task assignments while improving user satisfaction.
C1 [Wang, Jian; Wang, Xiao] Harbin Univ Sci & Technol, Sch Comp Sci & Technol, 52 Xue Fu Rd, Harbin 150080, Peoples R China.
   [Zhao, Guosheng] Harbin Normal Univ, Coll Comp Sci & Informat Engn, 1 Shi Da Rd, Harbin 150025, Peoples R China.
C3 Harbin University of Science & Technology; Harbin Normal University
RP Wang, J (corresponding author), Harbin Univ Sci & Technol, Sch Comp Sci & Technol, 52 Xue Fu Rd, Harbin 150080, Peoples R China.
EM wangjianlydia@163.com; 1945659234@qq.com; zgswj@163.com
OI zhao, guosheng/0000-0002-0571-9247
FU National Natural Science Foundation of China [61403109, 61202458];
   Specialized Research Fund for the Doctoral Program of Higher Education
   of China [20112303120007]; Natural Science Foundation of Heilongjiang
   Province [LH2020F034]
FX This present research work was supported by the National Natural Science
   Foundation of China (61403109, 61202458), the Specialized Research Fund
   for the Doctoral Program of Higher Education of China (20112303120007)
   and the Natural Science Foundation of Heilongjiang Province
   (LH2020F034).
CR Ahmad Z, 2021, IEEE ACCESS, V9, P100615, DOI 10.1109/ACCESS.2021.3097614
   Chen JR, 2020, IEEE SYST J, V14, P244, DOI 10.1109/JSYST.2019.2900325
   Cheng SN, 2021, LECT NOTE NETW SYST, V275, P543, DOI 10.1007/978-3-030-80091-8_64
   Gao H, 2020, IEEE INTERNET THINGS, V7, P5302, DOI 10.1109/JIOT.2020.2976778
   Gaw N, 2022, IISE TRANS, V54, P1098, DOI 10.1080/24725854.2021.1987593
   Guan Weili, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P268, DOI 10.1145/3503161.3548020
   Guan WL, 2022, PROCEEDINGS OF THE 45TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '22), P482, DOI 10.1145/3477495.3532038
   Han JY, 2020, IEEE T NEUR NET LEAR, V31, P737, DOI 10.1109/TNNLS.2019.2909432
   Huang Y, 2022, IEEE T IND INFORM, V18, P2476, DOI 10.1109/TII.2021.3094527
   Jia Z., INT C INT TRANSP BIG, P213
   Jiang XP, 2014, SCI CHINA LIFE SCI, V57, P1115, DOI 10.1007/s11427-014-4735-x
   Li PG, 2021, IEEE T MULTIMEDIA, V24, P3455, DOI 10.1109/TMM.2021.3098988
   Li QM, 2021, IEEE T SIGNAL PROCES, V69, P1140, DOI 10.1109/TSP.2021.3054986
   Long H., 2019, J. Commun., V40, P42
   Ma C., 2021, J. Comput, V32, P197
   Malik A, 2021, IEEE T MOBILE COMPUT, V20, P2124, DOI 10.1109/TMC.2020.2975786
   Min Xiao, 2011, Proceedings of the 2011 International Conference on Business Management and Electronic Information (BMEI 20111), P520, DOI 10.1109/ICBMEI.2011.5920508
   Nguyen TN, 2022, ACM T INTERNET TECHN, V22, DOI 10.1145/3431502
   Peng YM, 2022, IEEE T EVOLUT COMPUT, V26, P886, DOI [10.1109/TEVC.2021.3117702, 10.1145/3474880.3474882]
   Sritrakool N, 2021, IEEE ACCESS, V9, P155491, DOI 10.1109/ACCESS.2021.3128769
   Truong QT, 2021, 15TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS 2021), P834, DOI 10.1145/3460231.3473324
   Wang CD, 2015, I S BIOMED IMAGING, P1340, DOI 10.1109/ISBI.2015.7164123
   Wang JJ, 2021, IEEE T SOFTWARE ENG, V47, P1259, DOI 10.1109/TSE.2019.2918520
   Wang PM, 2020, IEEE T SERV COMPUT, V13, P675, DOI 10.1109/TSC.2020.2964663
   Wang ZB, 2021, IEEE T MOBILE COMPUT, V20, P2080, DOI 10.1109/TMC.2020.2973990
   Wei YW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1437, DOI 10.1145/3343031.3351034
   Wu F, 2021, IEEE T MOBILE COMPUT, V20, P2961, DOI 10.1109/TMC.2020.2993963
   Wu YF, 2022, IEEE T IND INFORM, V18, P1210, DOI 10.1109/TII.2021.3071771
   Wu YX, 2022, IEEE T KNOWL DATA EN, V34, P1944, DOI 10.1109/TKDE.2020.3002531
   Xiong JB, 2021, T EMERG TELECOMMUN T, V32, DOI 10.1002/ett.4000
   Xu Y, 2023, IEEE T KNOWL DATA EN, V35, P741, DOI 10.1109/TKDE.2021.3079581
   Xue YX, 2022, IEEE SENS J, V22, P6793, DOI 10.1109/JSEN.2022.3148992
   Yang GS, 2021, IEEE T COMPUT SOC SY, V8, P1311, DOI 10.1109/TCSS.2021.3073031
   Yin B, 2021, IEEE T NETW SCI ENG, V8, P1542, DOI 10.1109/TNSE.2021.3064335
   Yucel F, 2020, COMPUT NETW, V172, DOI 10.1016/j.comnet.2020.107156
   Zagoruyko S, 2015, PROC CVPR IEEE, P4353, DOI 10.1109/CVPR.2015.7299064
   Zhang W, 2021, TSINGHUA SCI TECHNOL, V26, P869, DOI 10.26599/TST.2020.9010046
   Zhu L, 2023, IEEE T KNOWL DATA EN, V35, P6901, DOI 10.1109/TKDE.2022.3185093
NR 38
TC 0
Z9 0
U1 9
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2024
VL 20
IS 3
AR 78
DI 10.1145/3626239
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6F8
UT WOS:001153381000018
DA 2024-08-05
ER

PT J
AU Gong, W
   Zhang, Y
   Wang, W
   Cheng, P
   Gonzàlez, J
AF Gong, Wenjuan
   Zhang, Yue
   Wang, Wei
   Cheng, Peng
   Gonzalez, Jordi
TI Meta-MMFNet: Meta-learning-based Multi-model Fusion Network for
   Micro-expression Recognition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Feature fusion; model fusion; meta-learning; micro-expression
   recognition
AB Despite its wide applications in criminal investigations and clinical communications with patients suffering from autism, automatic micro-expression recognition remains a challenging problem because of the lack of training data and imbalanced classes problems. In this study, we proposed a meta-learning-based multi-model fusion network (Meta-MMFNet) to solve the existing problems. The proposed method is based on the metric-based meta-learning pipeline, which is specifically designed for few-shot learning and is suitable for model-level fusion. The frame difference and optical flow features were fused, deep features were extracted from the fused feature, and finally in themeta-learning-based framework, weighted summodel fusion method was applied for micro-expression classification. Meta-MMFNet achieved better results than state-of-the-art methods on four datasets. The code is available at https://github.com/wenjgong/meta-fusion- based- method.
C1 [Gong, Wenjuan; Zhang, Yue] China Univ Petr East China, 66 Changjiangxi Rd, Qingdao, Peoples R China.
   [Wang, Wei] Chinese Acad Sci, Inst Automat, 95 Zhongguancun East Rd, Beijing, Peoples R China.
   [Cheng, Peng] ASTAR, Inst High Performance Comp, 1 Fusionopolis Way,16-16 Connexis North Tower, Singapore, Singapore.
   [Gonzalez, Jordi] Autonomous Univ Barcelona, Comp Vis Ctr, Edificio O,Campus UAB Bellaterra Cerdanyola, Barcelona, Spain.
C3 China University of Petroleum; Chinese Academy of Sciences; Institute of
   Automation, CAS; Agency for Science Technology & Research (A*STAR);
   A*STAR - Institute of High Performance Computing (IHPC); Autonomous
   University of Barcelona; Centre de Visio per Computador (CVC)
RP Gong, W (corresponding author), China Univ Petr East China, 66 Changjiangxi Rd, Qingdao, Peoples R China.
EM wenjuangong@upc.edu.cn; z20070048@s.upc.edu.cn; wangwei@nlpr.ia.ac.cn;
   cheng_peng@ihpc.a-star.edu.sg; Jordi.Gonzalez@uab.cat
RI su, lin/KHC-5034-2024; zhang, yan/KHC-3163-2024; liu, qi/KHC-7509-2024;
   li, jing/KHC-8303-2024; guo, yi/KHC-4669-2024; Gonzàlez,
   Jordi/I-1812-2015; li, yf/KHX-1148-2024; CHENG, PENG/HNP-4206-2023
OI Gonzàlez, Jordi/0000-0001-8033-0306; Gong, Wenjuan/0000-0001-7805-3629
FU Spanish Ministry of Economy and Competitiveness (MINECO); European
   Regional Development Fund (ERDF)
   [PID2020-120311RB-I00/AEI/10.13039/501100011033]
FX Jordi Gonzalez acknowledges the support by the Spanish Ministry of
   Economy and Competitiveness (MINECO) and the European Regional
   Development Fund (ERDF) under Project No.
   PID2020-120311RB-I00/AEI/10.13039/501100011033.
CR Altae-Tran H, 2016, Arxiv, DOI arXiv:1611.03199
   Chen XZ, 2017, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2017.691
   Chen YB, 2021, Arxiv, DOI [arXiv:2003.04390, 10.48550/arXiv.2003.04390]
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Finn C, 2017, PR MACH LEARN RES, V70
   Gan YS, 2019, SIGNAL PROCESS-IMAGE, V74, P129, DOI 10.1016/j.image.2019.02.005
   Guan Jiazhi, 2021, FME'21: Proceedings of the 1st Workshop on Facial Micro-Expression: Advanced Techniques for Facial Expressions Generation and Spotting, P19, DOI 10.1145/3476100.3484461
   Han XH, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3477396
   Haoyu Ma, 2022, ACM Transactions on Multimedia Computing, Communications and Applications, V18, DOI 10.1145/3477553
   Happy SL, 2019, IEEE T AFFECT COMPUT, V10, P394, DOI 10.1109/TAFFC.2017.2723386
   Huang G, 2018, Arxiv, DOI [arXiv:1608.06993, 10.48550/arXiv.1608.06993]
   Huang XH, 2019, IEEE T AFFECT COMPUT, V10, P32, DOI 10.1109/TAFFC.2017.2713359
   Huang XH, 2016, NEUROCOMPUTING, V175, P564, DOI 10.1016/j.neucom.2015.10.096
   Khor HQ, 2018, IEEE INT CONF AUTOMA, P667, DOI 10.1109/FG.2018.00105
   Kim DH, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P382, DOI 10.1145/2964284.2967247
   Li J, 2019, PATTERN ANAL APPL, V22, P1331, DOI 10.1007/s10044-018-0757-5
   Li QY, 2019, MULTIMED TOOLS APPL, V78, P29307, DOI 10.1007/s11042-018-6857-9
   Li XB, 2013, IEEE INT CONF AUTOMA, DOI 10.1109/FG.2013.6553717
   Li YT, 2021, IEEE T IMAGE PROCESS, V30, P249, DOI 10.1109/TIP.2020.3035042
   Lin CH, 2018, INT CONF INFO SCI, P487, DOI 10.1109/ICIST.2018.8426088
   Liong ST, 2018, Arxiv, DOI arXiv:1606.01721
   Liong ST, 2018, SIGNAL PROCESS-IMAGE, V62, P82, DOI 10.1016/j.image.2017.11.006
   Liu H., 2019, P INT C LEARN REPR, P1
   Liu YS, 2018, IEEE GEOSCI REMOTE S, V15, P183, DOI 10.1109/LGRS.2017.2779469
   Liu YC, 2019, IEEE INT CONF AUTOMA, P631, DOI 10.1109/fg.2019.8756583
   Lu H, 2018, SIGNAL PROCESS-IMAGE, V67, P108, DOI 10.1016/j.image.2018.05.014
   Lu ZY, 2015, LECT NOTES COMPUT SC, V9009, P698, DOI 10.1007/978-3-319-16631-5_51
   Metz Luke, 2019, P 7 INT C LEARN REPR
   Miao Y, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3311747
   Morris MW, 2000, RES ORGAN BEHAV, V22, P1, DOI 10.1016/S0191-3085(00)22002-9
   Nhi Thi Thu Nguyen, 2021, ICMLSC'21: 2021 The 5th International Conference on Machine Learning and Soft Computing, P115, DOI 10.1145/3453800.3453821
   Niu MY, 2018, 2018 FIRST ASIAN CONFERENCE ON AFFECTIVE COMPUTING AND INTELLIGENT INTERACTION (ACII ASIA)
   OJALA T, 1994, INT C PATT RECOG, P582, DOI 10.1109/ICPR.1994.576366
   PAN H., 2021, P 1 WORKSH FAC MICR, P25, DOI [10.1145/3476100.3484463, DOI 10.1145/3476100.3484463]
   Peng M, 2019, INT CONF AFFECT, DOI [10.1109/ACII.2019.8925525, 10.1109/acii.2019.8925525]
   Peng M, 2018, IEEE INT CONF AUTOMA, P657, DOI 10.1109/FG.2018.00103
   Real E, 2019, AAAI CONF ARTIF INTE, P4780
   Santoro A, 2016, PR MACH LEARN RES, V48
   Snell J, 2017, ADV NEUR IN, V30
   Song BL, 2019, IEEE ACCESS, V7, P184537, DOI 10.1109/ACCESS.2019.2960629
   Sun DQ, 2014, INT J COMPUT VISION, V106, P115, DOI 10.1007/s11263-013-0644-x
   Takalkar MA, 2017, 2017 INTERNATIONAL CONFERENCE ON DIGITAL IMAGE COMPUTING - TECHNIQUES AND APPLICATIONS (DICTA), P688
   Thrun S, 1998, LEARNING TO LEARN, P3
   VanQuang N., 2019, IEEE INT CONF AUTOMA, P1, DOI [DOI 10.1109/fg.2019.8756544, 10.1109/FG.2019.8756544]
   Wang SF, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3391290
   Wang YD, 2015, LECT NOTES COMPUT SC, V9003, P525, DOI 10.1007/978-3-319-16865-4_34
   Wang ZT, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3462219
   Whitehill J, 2014, IEEE T AFFECT COMPUT, V5, P86, DOI 10.1109/TAFFC.2014.2316163
   Wu HY, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185561
   Xia Z., 2018, 2018 8 INT C IMAGE P, P1, DOI DOI 10.1109/IPTA.2018.8608119
   Xia ZQ, 2020, IEEE T MULTIMEDIA, V22, P626, DOI 10.1109/TMM.2019.2931351
   Xu F, 2017, IEEE T AFFECT COMPUT, V8, P254, DOI 10.1109/TAFFC.2016.2518162
   Yan WJ, 2013, IEEE INT CONF AUTOMA
   Yan WJ, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0086041
   Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22
   Zhai DM, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3472809
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
   Zhou L, 2021, Arxiv, DOI arXiv:2101.04838
   Zong Y, 2018, IEEE T MULTIMEDIA, V20, P3160, DOI 10.1109/TMM.2018.2820321
NR 59
TC 4
Z9 4
U1 28
U2 55
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2024
VL 20
IS 2
SI SI
AR 39
DI 10.1145/3539576
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W6GR4
UT WOS:001092595800009
DA 2024-08-05
ER

PT J
AU Qiu, HQ
   Li, HL
   Wu, QB
   Shi, HC
   Wang, LX
   Meng, FM
   Xu, LF
AF Qiu, Heqian
   Li, Hongliang
   Wu, Qingbo
   Shi, Hengcan
   Wang, Lanxiao
   Meng, Fanman
   Xu, Linfeng
TI Learning Offset Probability Distribution for Accurate Object Detection
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Object detection; distance-aware offset bin classification; offset
   probability distribution; expectation-based offset prediction;
   hierarchical focusing method
AB Object detection combines object classification and object localization problems. Current object detection methods heavily depend on regression networks to locate objects, which are optimized with various regression loss functions to predict offsets between candidate boxes and objects. However, these regression losses are difficult to assign the appropriate penalties for samples with large offset errors, resulting in suboptimal regression networks and inaccurate object offsets. In this article, we consider object location as offset bin classification problem, and propose a distance-aware offset bin classification network optimized with multiple binary cross entropy losses to learn various offset probability distribution, including single label distribution and distance-aware label distribution. On one hand, it provides gradient contributions for different samples based on the bounded probability instead of previous incalculable offset error. On the other hand, it explores the distance correlations between discrete offset bins to facilitate network learning. Specifically, we discretize the continuous offset into a number of bins, and predict the probability of each offset bin, in which the probability should be higher for the offset bin closer to the target offsets, and vice versa. Furthermore, we propose an expectation-based offset prediction and a hierarchical focusing method to improve the precision of prediction. We conduct extensive experiments to evaluate the effectiveness of our method. In addition, our method can be conveniently and flexibly inserted into existing object detection methods, which consistently achieves a large gain based on popular anchor-based and anchor-free methods on the PASCAL VOC, MS-COCO, KITTI, and CrowdHuman datasets. Code will be released at: https://github.com/QiuHeqian/DBC.
C1 [Qiu, Heqian; Li, Hongliang; Wu, Qingbo; Shi, Hengcan; Wang, Lanxiao; Meng, Fanman; Xu, Linfeng] Univ Elect Sci & Technol China, 2006 Xiyuan Ave, Chengdu 611731, Sichuan, Peoples R China.
C3 University of Electronic Science & Technology of China
RP Qiu, HQ; Li, HL; Wang, LX (corresponding author), Univ Elect Sci & Technol China, 2006 Xiyuan Ave, Chengdu 611731, Sichuan, Peoples R China.
EM hqqiu@std.uestc.edu.cn; hlli@uestc.edu.cn; qbwu@uestc.edu.cn;
   shihc@std.uestc.edu.cn; lanxiao.wang@std.uestc.edu.cn;
   fmmeng@uestc.edu.cn; lfxu@uestc.edu.cn
RI ; Wu, Qingbo/M-5065-2015; Xu, Linfeng/HME-1913-2023
OI Qiu, Heqian/0000-0002-0963-0311; Wu, Qingbo/0000-0003-2936-6340; Xu,
   Linfeng/0000-0002-9934-0958
FU Sichuan Province Innovative Talent Funding Project for Postdoctoral
   Fellows [BX202212]; National Natural Science Foundation of China
   [61831005]; China Postdoctoral Science Foundation [2023TQ0046,
   2023M740529]
FX This work was supported in part by Sichuan Province Innovative Talent
   Funding Project for Postdoctoral Fellows BX202212, National Natural
   Science Foundation of China under Grant 61831005, Project funded by
   China Postdoctoral Science Foundation 2023TQ0046 and 2023M740529.
CR Cai ZW, 2021, IEEE T PATTERN ANAL, V43, P1483, DOI 10.1109/TPAMI.2019.2956516
   Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644
   Cai ZW, 2016, LECT NOTES COMPUT SC, V9908, P354, DOI 10.1007/978-3-319-46493-0_22
   Carion N., 2020, EUR C COMP VIS, P213
   Chen K, 2019, Arxiv, DOI arXiv:1906.07155
   Chen K, 2019, PROC CVPR IEEE, P4969, DOI 10.1109/CVPR.2019.00511
   Chen SJ, 2020, IEEE SIGNAL PROC LET, V27, P1680, DOI 10.1109/LSP.2020.3025128
   Dai XY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2968, DOI 10.1109/ICCV48922.2021.00298
   Deng JJ, 2021, IEEE T MULTIMEDIA, V23, P846, DOI 10.1109/TMM.2020.2990070
   Dosovitskiy A., 2020, INT C LEARNING REPRE
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Gao P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3601, DOI 10.1109/ICCV48922.2021.00360
   Ge SM, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3536426
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Gidaris S, 2016, PROC CVPR IEEE, P789, DOI 10.1109/CVPR.2016.92
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Gong J., 2019, BMVC, P223
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He YH, 2019, PROC CVPR IEEE, P2883, DOI 10.1109/CVPR.2019.00300
   Huang LC, 2015, Arxiv, DOI arXiv:1509.04874
   Jiale Cao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11482, DOI 10.1109/CVPR42600.2020.01150
   Jiang BR, 2018, LECT NOTES COMPUT SC, V11218, P816, DOI 10.1007/978-3-030-01264-9_48
   Jiaqi Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P403, DOI 10.1007/978-3-030-58548-8_24
   Kong T, 2020, IEEE T IMAGE PROCESS, V29, P7389, DOI 10.1109/TIP.2020.3002345
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Li X., 2020, ADV NEURAL INF PROCE, V33, P21002
   Li ZX, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3426974
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu C., 2022, ACM T MULTIM COMPUT, V18, P1
   Liu J, 2021, PROC CVPR IEEE, P264, DOI 10.1109/CVPR46437.2021.00033
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lu X, 2019, PROC CVPR IEEE, P7355, DOI 10.1109/CVPR.2019.00754
   Meng DP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3631, DOI 10.1109/ICCV48922.2021.00363
   Najibi M, 2016, PROC CVPR IEEE, P2369, DOI 10.1109/CVPR.2016.260
   Pang JM, 2019, PROC CVPR IEEE, P821, DOI 10.1109/CVPR.2019.00091
   Qiu H., 2020, P IEEE C COMPUTER VI, P13188
   Redmon J., 2018, CoRR
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Rothe R, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P252, DOI 10.1109/ICCVW.2015.41
   Shao S, 2018, Arxiv, DOI arXiv:1805.00123
   Sun PZ, 2021, PROC CVPR IEEE, P14449, DOI 10.1109/CVPR46437.2021.01422
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tian Z, 2022, IEEE T PATTERN ANAL, V44, P1922, DOI 10.1109/TPAMI.2020.3032166
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Wang KY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3611, DOI 10.1109/ICCV48922.2021.00361
   Wang ZT, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3462219
   Wu S, 2022, IEEE T MULTIMEDIA, V24, P2058, DOI 10.1109/TMM.2021.3075323
   Xu CY, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3450410
   Xuangeng Chu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12211, DOI 10.1109/CVPR42600.2020.01223
   Yang DB, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3472393
   Yang Z, 2019, IEEE I CONF COMP VIS, P9656, DOI 10.1109/ICCV.2019.00975
   Yu Jiahui, 2016, P 24 ACM INT C MULT, P516
   Zeng DH, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3564608
   Zhang S., 2020, P IEEE CVF C COMP VI, P9759
   Zhou Q, 2021, PROC CVPR IEEE, P4079, DOI 10.1109/CVPR46437.2021.00407
   Zhou XY, 2019, Arxiv, DOI arXiv:1904.07850
   Zhou XY, 2019, PROC CVPR IEEE, P850, DOI 10.1109/CVPR.2019.00094
   Zhu CC, 2019, PROC CVPR IEEE, P840, DOI 10.1109/CVPR.2019.00093
   Zhu X., 2021, ICLR, P1, DOI DOI 10.48550/ARXIV.2010.04159
NR 66
TC 0
Z9 0
U1 6
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2024
VL 20
IS 5
AR 131
DI 10.1145/3637214
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MF3R9
UT WOS:001192177900011
DA 2024-08-05
ER

PT J
AU Huang, H
   Xiao, D
   Liang, J
AF Huang, Hui
   Xiao, Di
   Liang, Jia
TI Secure Low-complexity Compressive Sensing with Preconditioning Prior
   Regularization Reconstruction
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Compressive sensing; sparse circulant matrix; joint quantization and
   diffusion; preconditioning prior regularization reconstruction
ID PROJECTION ALGORITHM; MEASUREMENT MATRIX; ACQUISITION; EFFICIENT;
   RECOVERY; INTERNET; SYSTEM
AB Compressive sensing (CS), a breakthrough technology in image processing, provides a privacy-preserving layer and image reconstruction while performing sensing and recovery processes, respectively. Unfortunately, it still faces high-complexity, low-security, and low-quality reconstruction challenges during image processing. Therefore, this article presents a secure low-complexity CS scheme with preconditioning prior regularization reconstruction. More specifically, the original image is compressed by a low-complexity LFSR-based sparse circulant matrix to obtain measurements. It is worth noting that measurements achieve preliminary distribution equalization through the Tanh sequence to acquire processed measurements. Furthermore, the privacy-preserving edge processing for processed measurements can achieve high security. Finally, preconditioning prior regularization CS reconstruction is designed to improve reconstruction performance. Simulation results and analyses demonstrate that the proposed scheme can achieve low-complexity sampling, high security, and superior reconstruction performance.
C1 [Huang, Hui; Liang, Jia] Chongqing Univ Posts & Telecommun, 2 Chongwen Rd, Chongqing, Peoples R China.
   [Xiao, Di] Chongqing Univ, 174 Shazheng St, Chongqing, Peoples R China.
C3 Chongqing University of Posts & Telecommunications; Chongqing University
RP Xiao, D (corresponding author), Chongqing Univ, 174 Shazheng St, Chongqing, Peoples R China.
EM cqyyhuang@163.com; xiaodi_cqu@hotmail.com; jialiang@cqu.edu.cn
FU National Key R&D Program of China [2020YFB1805400]; National Natural
   Science Foundation of China [62072063]
FX Thework was supported by the National Key R&D Program of China (Grant
   No. 2020YFB1805400) and the National Natural Science Foundation of China
   (Grant No. 62072063).
CR Abdelwahab S, 2014, IEEE INTERNET THINGS, V1, P276, DOI 10.1109/JIOT.2014.2325071
   Bai H, 2015, IEEE T SIGNAL PROCES, V63, P1581, DOI 10.1109/TSP.2015.2399864
   Bandeira AS, 2013, IEEE T INFORM THEORY, V59, P3448, DOI 10.1109/TIT.2013.2248414
   Bianchi T, 2016, IEEE T INF FOREN SEC, V11, P313, DOI 10.1109/TIFS.2015.2493982
   Candès EJ, 2011, IEEE T INFORM THEORY, V57, P7235, DOI 10.1109/TIT.2011.2161794
   Chen G, 2014, SIGNAL PROCESS, V104, P15, DOI 10.1016/j.sigpro.2014.03.039
   Chen GR, 2004, CHAOS SOLITON FRACT, V21, P749, DOI 10.1016/j.chaos.2003.12.022
   Cho W, 2020, IEEE T INF FOREN SEC, V15, P1999, DOI 10.1109/TIFS.2019.2953383
   Chowdhury M. M. H., 2012, Int J Comput Sci Issues, V9, P327
   Ding X, 2017, IEEE T SIGNAL PROCES, V65, P3632, DOI 10.1109/TSP.2017.2699639
   Eldar Y C., 2012, COMPRESSED SENSING T, DOI [DOI 10.1017/CBO9780511794308.002, 10.1017/CBO9780511794308, DOI 10.1017/CBO9780511794308]
   Fang Y, 2012, SCI CHINA INFORM SCI, V55, P889, DOI 10.1007/s11432-012-4551-5
   Feng J, 2019, ACM T INTERNET TECHN, V19, DOI 10.1145/3230641
   Ghaffari A, 2009, INT CONF ACOUST SPEE, P3157, DOI 10.1109/ICASSP.2009.4960294
   Gupta M., 2012, International Journal of Engineering Research and Applications (IJERA), V2, P515
   He ZX, 2010, IEEE IMAGE PROC, P4301, DOI 10.1109/ICIP.2010.5651800
   Hu GQ, 2017, INFORM SCIENCES, V387, P132, DOI 10.1016/j.ins.2016.09.045
   Hua ZY, 2019, INFORM SCIENCES, V480, P403, DOI 10.1016/j.ins.2018.12.048
   Huang H, 2024, VISUAL COMPUT, V40, P2103, DOI 10.1007/s00371-023-02906-x
   Huang H, 2022, IEEE SIGNAL PROC LET, V29, P2452, DOI 10.1109/LSP.2022.3223287
   Huang H, 2018, SIGNAL PROCESS, V150, P183, DOI 10.1016/j.sigpro.2018.04.014
   Jiang QR, 2020, IEEE T MULTIMEDIA, V22, P594, DOI 10.1109/TMM.2019.2931400
   Kuldeep G, 2020, IEEE GLOB COMM CONF, DOI 10.1109/GLOBECOM42002.2020.9348093
   Li G, 2013, IEEE T SIGNAL PROCES, V61, P2887, DOI 10.1109/TSP.2013.2253776
   Li LX, 2020, IEEE T MULTIMEDIA, V22, P82, DOI 10.1109/TMM.2019.2923111
   Li YH, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2903717
   Mun S, 2009, IEEE IMAGE PROC, P3021, DOI 10.1109/ICIP.2009.5414429
   Rachlin Y, 2008, ANN ALLERTON CONF, P813, DOI 10.1109/ALLERTON.2008.4797641
   Ravelomanantsoa A, 2015, IEEE T INSTRUM MEAS, V64, P3405, DOI 10.1109/TIM.2015.2459471
   SHANNON CE, 1949, BELL SYST TECH J, V28, P656, DOI 10.1002/j.1538-7305.1949.tb00928.x
   Testa M., 2019, COMPRESSED SENSING P
   Wang MD, 2023, SIGNAL PROCESS, V204, DOI 10.1016/j.sigpro.2022.108823
   Wang MD, 2021, IEEE INTERNET THINGS, V8, P1662, DOI 10.1109/JIOT.2020.3015237
   Wu DP, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2978570
   Yan WJ, 2014, IEEE T INSTRUM MEAS, V63, P1073, DOI 10.1109/TIM.2014.2298271
   Ye XL, 2020, NONLINEAR DYNAM, V99, P1489, DOI 10.1007/s11071-019-05370-2
   You D, 2021, IEEE T IMAGE PROCESS, V30, P6066, DOI 10.1109/TIP.2021.3091834
   Zhang B, 2023, SIGNAL PROCESS, V210, DOI 10.1016/j.sigpro.2023.109055
   Zhang B, 2021, IEEE T MULTIMEDIA, V23, P2656, DOI 10.1109/TMM.2020.3014489
   Zhang B, 2021, IEEE DATA COMPR CONF, P283, DOI 10.1109/DCC50243.2021.00036
   Zhang J, 2020, IEEE J-STSP, V14, P765, DOI 10.1109/JSTSP.2020.2977507
   Zhang YS, 2021, IEEE T IND INFORM, V17, P3401, DOI 10.1109/TII.2020.3008914
   Zhang YS, 2020, IEEE T IND INFORM, V16, P7566, DOI 10.1109/TII.2019.2957404
   Zhang YS, 2020, IEEE T IND INFORM, V16, P6641, DOI 10.1109/TII.2020.2966511
   Zhang YS, 2017, IEEE INTERNET THINGS, V4, P1380, DOI 10.1109/JIOT.2017.2732357
NR 45
TC 0
Z9 0
U1 7
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2024
VL 20
IS 4
AR 116
DI 10.1145/3635308
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6G9
UT WOS:001153382100026
DA 2024-08-05
ER

PT J
AU Liu, DZ
   Qu, XY
   Dong, JF
   Zhou, P
   Xu, ZC
   Wang, HZ
   Di, X
   Lu, WN
   Cheng, Y
AF Liu, Daizong
   Qu, Xiaoye
   Dong, Jianfeng
   Zhou, Pan
   Xu, Zichuan
   Wang, Haozhao
   Di, Xing
   Lu, Weining
   Cheng, Yu
TI Transform-Equivariant Consistency Learning for Temporal Sentence
   Grounding
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Temporal sentence grounding; transformation; equivariant; consistency
   learning
ID LOCALIZATION; IMAGE
AB This paper addresses the temporal sentence grounding (TSG). Although existing methods have made decent achievements in this task, they not only severely rely on abundant video-query paired data for training, but also easily fail into the dataset distribution bias. To alleviate these limitations, we introduce a novel Equivariant Consistency Regulation Learning (ECRL) framework to learn more discriminative query-related frame-wise representations for each video, in a self-supervised manner. Our motivation comes from that the temporal boundary of the query-guided activity should be consistently predicted under various video-level transformations. Concretely, we first design a series of spatio-temporal augmentations on both foreground and background video segments to generate a set of synthetic video samples. In particular, we devise a self-refine module to enhance the completeness and smoothness of the augmented video. Then, we present a novel self-supervised consistency loss (SSCL) applied on the original and augmented videos to capture their invariant query-related semantic by minimizing the KL-divergence between the sequence similarity of two videos and a prior Gaussian distribution of timestamp distance. At last, a shared grounding head is introduced to predict the transform-equivariant query-guided segment boundaries for both the original and augmented videos. Extensive experiments on three challenging datasets (ActivityNet, TACoS, and Charades-STA) demonstrate both effectiveness and efficiency of our proposed ECRL framework.
C1 [Liu, Daizong] Peking Univ, Wangxuan Inst Comp Technol, Beijing, Peoples R China.
   [Qu, Xiaoye; Zhou, Pan; Wang, Haozhao] Huazhong Univ Sci & Technol, Sch Cyber Sci & Engn, Wuhan, Peoples R China.
   [Dong, Jianfeng] Zhejiang Gongshang Univ, Coll Comp Sci & Technol, Hangzhou, Peoples R China.
   [Xu, Zichuan] Dalian Univ Technol, Sch Software, Dalian, Peoples R China.
   [Di, Xing] Protagolabs, Vienna, VA USA.
   [Lu, Weining] Tsinghua Univ, Dept Automat, Beijing, Peoples R China.
   [Cheng, Yu] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Hong Kong, Peoples R China.
C3 Peking University; Huazhong University of Science & Technology; Zhejiang
   Gongshang University; Dalian University of Technology; Tsinghua
   University; Chinese University of Hong Kong
RP Zhou, P (corresponding author), Huazhong Univ Sci & Technol, Sch Cyber Sci & Engn, Wuhan, Peoples R China.
EM dongjf24@gmail.com; xiaoye@hust.edu.cn; dongjf24@gmail.com;
   panzhou@hust.edu.cn; z.xu@dlut.edu.cn; hz_wang@hust.edu.cn;
   xing.di@protagolabs.com; luwn@tsinghua.edu.cn; chengyu@cse.cuhk.edu.hk
RI chen, xian/KHW-2227-2024; li, yf/KHX-1148-2024; Xu, Zichuan/T-5498-2019;
   Yan, Lu/KHW-7015-2024
OI Xu, Zichuan/0000-0001-5438-1468; Wang, Haozhao/0000-0002-7591-5315; Lu,
   Weining/0000-0002-0927-1259; Dong, Jianfeng/0000-0001-5244-3274; liu,
   daizong/0000-0001-8179-4508; Qu, Xiaoye/0000-0002-4907-3978
CR Benaim Sagie, 2020, P IEEE CVF C COMP VI, P9919, DOI DOI 10.1109/CVPR42600.2020.00994
   Cao M, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P9810
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen JY, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P162
   Chen L, 2020, AAAI CONF ARTIF INTE, V34, P10551
   Chen SX, 2019, AAAI CONF ARTIF INTE, P8199
   Chen T, 2020, PR MACH LEARN RES, V119
   Chu WS, 2015, PROC CVPR IEEE, P3584, DOI 10.1109/CVPR.2015.7298981
   Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167
   Dong JF, 2022, IEEE T PATTERN ANAL, V44, P4065, DOI 10.1109/TPAMI.2021.3059295
   Dong JF, 2018, IEEE T MULTIMEDIA, V20, P3377, DOI 10.1109/TMM.2018.2832602
   Dong JF, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1082, DOI 10.1145/2964284.2984064
   Dong Jianfeng, 2022, arXiv
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Feichtenhofer C, 2021, PROC CVPR IEEE, P3298, DOI 10.1109/CVPR46437.2021.00331
   Gao JY, 2017, IEEE I CONF COMP VIS, P5277, DOI 10.1109/ICCV.2017.563
   Gao LL, 2019, AAAI CONF ARTIF INTE, P6391
   Ge RZ, 2019, IEEE WINT CONF APPL, P245, DOI 10.1109/WACV.2019.00032
   Gidaris S, 2018, ICLR, DOI DOI 10.1109/ICCV.2019.00156
   Goldberger J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P487
   Han TD, 2019, IEEE INT CONF COMP V, P1483, DOI 10.1109/ICCVW.2019.00186
   Hendricks LA, 2017, IEEE I CONF COMP VIS, P5804, DOI 10.1109/ICCV.2017.618
   Hu YP, 2021, IEEE T IMAGE PROCESS, V30, P5933, DOI 10.1109/TIP.2021.3090521
   Hu YP, 2021, IEEE T IMAGE PROCESS, V30, P4667, DOI 10.1109/TIP.2021.3073867
   Jiang WH, 2018, LECT NOTES COMPUT SC, V11206, P510, DOI 10.1007/978-3-030-01216-8_31
   Jonghwan Mun, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10807, DOI 10.1109/CVPR42600.2020.01082
   Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83
   Kuang Haofei, 2021, P IEEE CVF INT C COM, P3195
   Lan X, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3565573
   Lan XH, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3532626
   Li HX, 2024, Arxiv, DOI arXiv:2301.05997
   Lin TW, 2019, IEEE I CONF COMP VIS, P3888, DOI 10.1109/ICCV.2019.00399
   Lin TW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P988, DOI 10.1145/3123266.3123343
   Liu DZ, 2022, AAAI CONF ARTIF INTE, P1665
   Liu DZ, 2022, AAAI CONF ARTIF INTE, P1683
   Liu DZ, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P9302
   Liu DZ, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P9292
   Liu DZ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4070, DOI 10.1145/3394171.3414026
   Liu DZ, 2021, PROC CVPR IEEE, P11230, DOI 10.1109/CVPR46437.2021.01108
   Liu M, 2018, ACM/SIGIR PROCEEDINGS 2018, P15, DOI 10.1145/3209978.3210003
   Lu JS, 2016, ADV NEUR IN, V29
   Misra I, 2016, LECT NOTES COMPUT SC, V9905, P527, DOI 10.1007/978-3-319-46448-0_32
   Nan GS, 2021, PROC CVPR IEEE, P2764, DOI 10.1109/CVPR46437.2021.00279
   Noroozi M, 2017, IEEE I CONF COMP VIS, P5899, DOI 10.1109/ICCV.2017.628
   Otani Mayu, 2020, arXiv
   Pennington J., 2014, GLOVE GLOBAL VECTORS, DOI DOI 10.3115/V1/D14-1162
   Piergiovanni AJ, 2019, PR MACH LEARN RES, V97
   Pramono RRA, 2022, IEEE T MULTIMEDIA, V24, P625, DOI 10.1109/TMM.2021.3056892
   Qian R, 2021, PROC CVPR IEEE, P6960, DOI 10.1109/CVPR46437.2021.00689
   Qiao TT, 2018, AAAI CONF ARTIF INTE, P7300
   Regneri M., 2013, T ASSOC COMPUT LING, V1, P25, DOI DOI 10.1162/TACL_A_00207
   Rodriguez-Opazo C, 2020, IEEE WINT CONF APPL, P2453, DOI [10.1109/wacv45572.2020.9093328, 10.1109/WACV45572.2020.9093328]
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Shaoxiang Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P333, DOI 10.1007/978-3-030-58548-8_20
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Sigurdsson GA, 2016, LECT NOTES COMPUT SC, V9905, P510, DOI 10.1007/978-3-319-46448-0_31
   Song YL, 2015, PROC CVPR IEEE, P5179, DOI 10.1109/CVPR.2015.7299154
   Sun C, 2022, IEEE T MULTIMEDIA, V24, P274, DOI 10.1109/TMM.2021.3050067
   Thao Minh Le, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9969, DOI 10.1109/CVPR42600.2020.00999
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang JW, 2020, AAAI CONF ARTIF INTE, V34, P12168
   Wang ZZ, 2022, AAAI CONF ARTIF INTE, P2613
   Xiao SN, 2021, AAAI CONF ARTIF INTE, V35, P2986
   Xu HJ, 2019, AAAI CONF ARTIF INTE, P9062
   Xu HJ, 2019, IEEE T PATTERN ANAL, V41, P2319, DOI 10.1109/TPAMI.2019.2921539
   Xu M., 2020, P IEEE CVF C COMP VI, P10156
   Yan XM, 2023, ANIM BIOTECHNOL, V34, P1261, DOI 10.1080/10495398.2021.2019756
   Yang L, 2020, IEEE T IMAGE PROCESS, V29, P8535, DOI 10.1109/TIP.2020.3016486
   Yang X, 2022, IEEE T IMAGE PROCESS, V31, P1204, DOI 10.1109/TIP.2022.3140611
   Yang X, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P1339, DOI 10.1145/3397271.3401151
   Yao T, 2021, AAAI CONF ARTIF INTE, V35, P10656
   Yawen Zeng, 2022, ACM Transactions on Multimedia Computing, Communications and Applications, V18, DOI 10.1145/3478025
   Yuan J, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2534409
   Yuan YT, 2019, AAAI CONF ARTIF INTE, P9159
   Zeng R., 2020, P IEEECVF C COMPUTER, P10287
   Zhai YH, 2022, IEEE T MULTIMEDIA, V24, P1857, DOI 10.1109/TMM.2021.3073235
   Zhang D, 2019, PROC CVPR IEEE, P1247, DOI 10.1109/CVPR.2019.00134
   Zhang H., 2021, ARXIV
   Zhang Hao, 2020, ANN M ASS COMPUTATIO
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zhang SY, 2020, AAAI CONF ARTIF INTE, V34, P12870
   Zhang YQ, 2019, AAAI CONF ARTIF INTE, P9235
   Zhang Z, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P655, DOI 10.1145/3331184.3331235
   Zhang ZM, 2021, IEEE T IMAGE PROCESS, V30, P8265, DOI 10.1109/TIP.2021.3113791
   Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317
   Zheng Q, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3543857
   Zhu W, 2019, Advances in Neural Information Processing Systems, P534
NR 87
TC 0
Z9 0
U1 5
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2024
VL 20
IS 4
AR 106
DI 10.1145/3634749
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6G9
UT WOS:001153382100016
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Peng, YX
   Fu, C
   Cao, GX
   Song, W
   Chen, JX
   Sham, CW
AF Peng, Yuxiang
   Fu, Chong
   Cao, Guixing
   Song, Wei
   Chen, Junxin
   Sham, Chiu-Wing
TI JPEG-compatible Joint Image Compression and Encryption Algorithm with
   File Size Preservation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image encryption; JPEG-compatible; Chaotic system; File size
   preservation; Parallel encryption
ID CHAOTIC SYSTEM; PROTECTION; SECURITY; SCHEME
AB Joint image compression and encryption algorithms are intensively investigated due to their powerful capability of simultaneous image data compression and sensitive information protection. Unfortunately, most of the existing algorithms suffered from either poor compression efficiency or weak encryption strength, making them vulnerable to cryptanalysis. To address these limitations, we propose a chaos-based JPEG-compatible joint image compression and encryption algorithm. We separate the luminance and chrominance coefficients to preserve file size and encrypt the discrete cosine transform (DCT) coefficients in parallel. The proposed inter-block DC encryption strategy achieves high encryption intensity based on the permutation-substitution structure. In addition, we apply both inter- and intra-block permutations to AC coefficients and strengthen the encryption using an inter-block substitution for non-zero AC coefficients. The results of security and performance analyses demonstrate that the proposed algorithm offers robust encryption of image data while maintaining compression efficiency for real-time transmission.
C1 [Peng, Yuxiang; Fu, Chong; Song, Wei] Northeastern Univ, Sch Comp Sci & Engn, Shenyang 110819, Peoples R China.
   [Fu, Chong] Northeastern Univ, Minist Educ, Engn Res Ctr Secur Technol Complex Network Syst, Key Lab Intelligent Comp Med Image, Shenyang 110819, Peoples R China.
   [Cao, Guixing] Northeastern Univ, Sch Comp Sci & Engn, Shenyang 110819, Peoples R China.
   [Cao, Guixing] China Acad Space Technol, Inst Telecommun Satellite, Beijing 100094, Peoples R China.
   [Chen, Junxin] Dalian Univ Technol, Sch Software, Dalian 116621, Peoples R China.
   [Sham, Chiu-Wing] Univ Auckland, Sch Comp Sci, Auckland, New Zealand.
C3 Northeastern University - China; Northeastern University - China;
   Northeastern University - China; Dalian University of Technology;
   University of Auckland
RP Peng, YX (corresponding author), Northeastern Univ, Sch Comp Sci & Engn, Shenyang 110819, Peoples R China.
EM 2190122@stu.neu.edu.cn; fuchong@mail.neu.edu.cn; castcao13579@163.com;
   songwei@mail.neu.edu.cn; junxinchen@ieee.org; sham@auckland.ac.nz
RI Sham, Chiu-Wing/C-3819-2014; Peng, YuXiang/ADP-1617-2022; Chen,
   Junxin/ABC-1747-2020
OI Sham, Chiu-Wing/0000-0001-7007-6746; Chen, Junxin/0000-0003-4745-8361;
   Fu, Chong/0000-0002-4549-744X
FU National Natural Science Foundation of China [62171114, 62032013];
   Fundamental Research Funds for the Central Universities [N2324004-12,
   N2316010]
FX This research is supported by the National Natural Science Foundation of
   China (Nos. 62171114 and 62032013), and the Fundamental Research Funds
   for the Central Universities (Nos. N2324004-12 and N2316010).
CR Alvarez G, 2006, INT J BIFURCAT CHAOS, V16, P2129, DOI 10.1142/S0218127406015970
   Changgui Shi, 1998, Proceedings ACM Multimedia 98, P81
   Cheng H, 2016, J VIS COMMUN IMAGE R, V40, P111, DOI 10.1016/j.jvcir.2016.06.016
   Chuman T, 2019, IEEE T INF FOREN SEC, V14, P1515, DOI 10.1109/TIFS.2018.2881677
   Diao WR, 2018, J COMPUT SECUR, V26, P283, DOI 10.3233/JCS-16909
   Fangchao Wang, 2013, 2013 Fifth International Conference on Computational and Information Sciences (ICCIS 2013), P60, DOI 10.1109/ICCIS.2013.24
   Fridrich J, 1998, INT J BIFURCAT CHAOS, V8, P1259, DOI 10.1142/S021812749800098X
   Fu C, 2011, OPT COMMUN, V284, P5415, DOI 10.1016/j.optcom.2011.08.013
   He JH, 2018, IEEE T MULTIMEDIA, V20, P2645, DOI 10.1109/TMM.2018.2817065
   Hua ZY, 2019, INFORM SCIENCES, V480, P403, DOI 10.1016/j.ins.2018.12.048
   ITU, 1992, Information technology-Digital compression and coding of continuous-tone still images: Requirements and guidelines
   Jia QA, 2007, PHYS LETT A, V366, P217, DOI 10.1016/j.physleta.2007.02.024
   Jiang DH, 2021, IET IMAGE PROCESS, V15, P3698, DOI 10.1049/ipr2.12237
   Kurihara K, 2015, IEICE T FUND ELECTR, VE98A, P2238, DOI 10.1587/transfun.E98.A.2238
   Li PY, 2020, IET SIGNAL PROCESS, V14, P475, DOI 10.1049/iet-spr.2019.0276
   Li PY, 2019, ASIAPAC SIGN INFO PR, P1140, DOI 10.1109/APSIPAASC47483.2019.9023179
   Li PY, 2017, J VIS COMMUN IMAGE R, V44, P61, DOI 10.1016/j.jvcir.2017.01.021
   Li SS, 2016, KSII T INTERNET INF, V10, P1790, DOI 10.3837/tiis.2016.04.018
   Li WH, 2007, INT J COMPUT MATH, V84, P1367, DOI 10.1080/00207160701294376
   Li YP, 2017, OPT LASER ENG, V90, P238, DOI 10.1016/j.optlaseng.2016.10.020
   Lian SG, 2004, IEEE INFOR VIS, P217, DOI 10.1109/IV.2004.1320147
   Liang HH, 2019, J VIS COMMUN IMAGE R, V61, P149, DOI 10.1016/j.jvcir.2019.03.021
   [陆阳 Lu Yang], 2003, [计算机工程与应用, Computer Engineering and Application], V39, P130
   MAY RM, 1976, NATURE, V261, P459, DOI 10.1038/261459a0
   Ong SY, 2015, SIGNAL PROCESS-IMAGE, V31, P47, DOI 10.1016/j.image.2014.11.008
   Peiya Li, 2021, Advances in Intelligent Information Hiding and Multimedia Signal Processing. Proceedings of the 16th International Conference on IIHMSP in Conjunction with the 13th International Conference on FITAT. Smart Innovation, Systems and Technologies (SIST 211), P140, DOI 10.1007/978-981-33-6420-2_18
   Puchala D, 2020, IEEE DATA COMPR CONF, P313, DOI 10.1109/DCC47342.2020.00039
   Qin C, 2023, IEEE T MULTIMEDIA, V25, P2528, DOI 10.1109/TMM.2022.3148591
   [邱劲 Qiu Jing], 2012, [计算机科学, Computer Science], V39, P44
   Shimizu K, 2021, PICT COD SYMP, P166, DOI 10.1109/PCS50896.2021.9477508
   Shreyamsha Kumar BK, 2010, SIGNAL IMAGE VIDEO P, V4, P419, DOI 10.1007/s11760-009-0131-6
   Singh KN, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3561513
   Singh KN, 2023, ACM J DATA INF QUAL, V15, DOI 10.1145/3532783
   Singh KN, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3498342
   Socek D, 2007, MULTIMEDIA SYST, V13, P191, DOI 10.1007/s00530-007-0083-z
   Song W, 2023, MATH COMPUT SIMULAT, V204, P71, DOI 10.1016/j.matcom.2022.07.029
   Song W, 2022, SIGNAL PROCESS-IMAGE, V102, DOI 10.1016/j.image.2021.116628
   Song W, 2022, NEURAL COMPUT APPL, V34, P5743, DOI 10.1007/s00521-021-06725-w
   Wu CP, 2001, PROC SPIE, V4314, P128, DOI 10.1117/12.435392
   Wu CP, 2001, PROC SPIE, V4209, P284, DOI 10.1117/12.420829
   Xie Dahua, 2004, 2004 IEEE INT S CIRC, V5, pV
   Xu YY, 2014, J VIS COMMUN IMAGE R, V25, P805, DOI 10.1016/j.jvcir.2014.01.005
   Yuling Luo, 2012, 2012 5th International Workshop on Chaos-Fractals Theories and Applications (IWCFTA 2012), P191, DOI 10.1109/IWCFTA.2012.49
   Zhang DH, 2014, OPTIK, V125, P717, DOI 10.1016/j.ijleo.2013.07.069
   Zhang XP, 2014, IEEE T MULTIMEDIA, V16, P1327, DOI 10.1109/TMM.2014.2315974
   Zhang YQ, 2014, INFORM SCIENCES, V273, P329, DOI 10.1016/j.ins.2014.02.156
   Zhou Q, 2011, J VIS COMMUN IMAGE R, V22, P85, DOI 10.1016/j.jvcir.2010.10.007
NR 47
TC 4
Z9 4
U1 29
U2 29
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2024
VL 20
IS 4
AR 105
DI 10.1145/3633459
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6G9
UT WOS:001153382100015
OA Bronze
DA 2024-08-05
ER

PT J
AU Shi, HJ
   Wang, H
   Ma, RH
   Hua, Y
   Song, T
   Gao, HH
   Guan, HB
AF Shi, Hongjian
   Wang, Hao
   Ma, Ruhui
   Hua, Yang
   Song, Tao
   Gao, Honghao
   Guan, Haibing
TI Robust Searching-Based Gradient Collaborative Management in Intelligent
   Transportation System
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE All-reduce; communication scheduling; gradient aggregation; robustness;
   collaborative management
ID GO; NETWORKS; OPTIMIZATION; ALGORITHM; SHOGI; CHESS; GAME
AB With the rapid development of big data and the Internet of Things (IoT), traffic data from an Intelligent Transportation System (ITS) is becoming more and more accessible. To understand and simulate the traffic patterns from the traffic data, Multimedia Cognitive Computing (MCC) is an efficient and practical approach. Distributed Machine Learning (DML) has been the trend to provide sufficient computing resources and efficiency for MCC tasks to handle massive data and complex models. DML can speed up computation with those computing resources but introduces communication overhead. Gradient collaborative management or gradient aggregation in DML for MCC tasks is a critical task. An efficient managing algorithm of the communication schedules for gradient aggregation in ITS can improve the performance of MCC tasks. However, existing communication schedules typically rely on specific physical connection matrices, which have low robustness when a malfunction occurs. In this article, we propose Robust Searching-based Gradient Collaborative Management (RSGCM) in Intelligent Transportation System, a practical ring-based gradient managing algorithm for communication schedules across devices to deal with ITS malfunction. RSGCM provides solutions of communication schedules to various kinds of connection matrices with an acceptable amount of training time. Our experimental results have shown that RSGCM can deal with more varieties of connection matrices than existing state-of-the-art communication schedules. RSGCM also increases the robustness of ITS since it can restore the system's functionality in an acceptable time when device or connection breakdown happens.
C1 [Shi, Hongjian; Ma, Ruhui; Song, Tao; Guan, Haibing] Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, 800 Dongchuan Rd, Shanghai 200240, Peoples R China.
   [Wang, Hao] Louisiana State Univ, Div Comp Sci & Engn, Baton Rouge, LA USA.
   [Hua, Yang] Queens Univ Belfas, Sch Elect Elect Engn & Comp Sci, Belfast, North Ireland.
   [Gao, Honghao] Shanghai Univ, Sch Comp Engn & Sci, Shanghai, Peoples R China.
C3 Shanghai Jiao Tong University; Louisiana State University System;
   Louisiana State University; Shanghai University
RP Ma, RH (corresponding author), Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, 800 Dongchuan Rd, Shanghai 200240, Peoples R China.
EM shhjwu5@sjtu.edu.cn; haowang@lsu.edu; ruhuima@sjtu.edu.cn;
   Y.Hua@qub.ac.uk; songt333@sjtu.edu.cn; gaohonghao@shu.edu.cn;
   hbguan@sjtu.edu.cn
RI Shi, Hongjian/IZQ-1980-2023; Lin, Wei/KFQ-5381-2024; Gao,
   Honghao/AAX-4529-2020; chen, shuo bing/KHV-7129-2024; Wang,
   Hao/ABA-2658-2021
OI Shi, Hongjian/0000-0003-0743-7806; Gao, Honghao/0000-0001-6861-9684;
   Song, Tao/0000-0002-5965-3140; Wang, Hao/0000-0002-1444-2657
FU National NSF of China [61872234, 61732010]; Shanghai Key Laboratory of
   Scalable Computing and Systems, Innovative Research Foundation of Ship
   General Performance [25622114]; SJTU Library-Jiangsu Jiatu Future
   Library Smart Service Joint RD Center; Key Laboratory of PK System
   Technologies Research of Hainan
FX This work was supported in part by National NSF of China (NO. 61872234,
   61732010), Shanghai Key Laboratory of Scalable Computing and Systems,
   Innovative Research Foundation of Ship General Performance (NO.
   25622114), SJTU Library-Jiangsu Jiatu Future Library Smart Service Joint
   R&D Center and the Key Laboratory of PK System Technologies Research of
   Hainan.
CR Aloufi S, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3462777
   [Anonymous], 2019, PyTorch
   Ba Q, 2018, IEEE T CONTROL NETW, V5, P1479, DOI 10.1109/TCNS.2017.2724842
   Baidu, 2016, Baidu Allreduce
   Bao YX, 2020, IEEE INFOCOM SER, P626, DOI [10.1109/INFOCOM41043.2020.9155446, 10.1109/infocom41043.2020.9155446]
   Bao YX, 2018, IEEE INFOCOM SER, P495, DOI 10.1109/INFOCOM.2018.8486422
   Canny John., 2013, SDM, P785
   Castellano G, 2019, IEEE INFOCOM SER, P2548, DOI [10.1109/INFOCOM.2019.8737532, 10.1109/infocom.2019.8737532]
   Castelló A, 2021, CLUSTER COMPUT, V24, P3797, DOI 10.1007/s10586-021-03370-9
   Chen JM, 2017, Arxiv, DOI arXiv:1604.00981
   Cho M, 2019, IBM J RES DEV, V63, DOI 10.1147/JRD.2019.2947013
   Chujyo M, 2021, APPL NETW SCI, V6, DOI 10.1007/s41109-020-00343-6
   Cormen T. H., 2009, Introduction to Algorithms, V3rd
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Di Girolamo S, 2021, Arxiv, DOI arXiv:2010.03536
   Dorigo M., 2007, SCHOLARPEDIA, V2, P1461, DOI DOI 10.1201/9781420010749
   Dowsland KA., 2012, HDB NATURAL COMPUTIN, P1623, DOI [10.1007/978-3-540-92910-9_49, DOI 10.1007/978-3-540-92910-9_49]
   Gao Z, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3377876
   Google, 2021, ABOUT US
   Guo CX, 2008, ACM SIGCOMM COMP COM, V38, P75, DOI 10.1145/1402946.1402968
   Guo CX, 2009, ACM SIGCOMM COMP COM, V39, P63, DOI 10.1145/1594977.1592577
   Guo HX, 2021, PROCEEDINGS OF THE 2021 ACM SYMPOSIUM ON CLOUD COMPUTING (SOCC '21), P47, DOI 10.1145/3472883.3486990
   HOCKNEY RW, 1994, PARALLEL COMPUT, V20, P389, DOI 10.1016/0167-8191(94)90095-7
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jia XY, 2018, Arxiv, DOI arXiv:1807.11205
   Jiang Youhe, 2020, IEEE Access, V8
   Jocksch A, 2021, PARALLEL COMPUT, V107, DOI 10.1016/j.parco.2021.102812
   Jouppi NP, 2017, 44TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA 2017), P1, DOI 10.1145/3079856.3080246
   Kennedy R. E. J., 2010, Particle Swarm Optimization, P1942
   Lee J, 2020, DES AUT CON, DOI 10.1109/dac18072.2020.9218538
   Li A, 2020, IEEE T PARALL DISTR, V31, P94, DOI 10.1109/TPDS.2019.2928289
   Li M., 2014, 11 USENIX S OPERATIN, P583
   Lillicrap T.P., 2015, Continuous control with deep reinforcement learning
   Lin Y., 2017, arXiv
   Lin YX, 2017, 2017 IEEE 3RD INTERNATIONAL CONFERENCE ON BIG DATA SECURITY ON CLOUD (BIGDATASECURITY, IEEE 3RD INTERNATIONAL CONFERENCE ON HIGH PERFORMANCE AND SMART COMPUTING, (HPSC) AND 2ND IEEE INTERNATIONAL CONFERENCE ON INTELLIGENT DATA AND SECURITY (IDS), P167, DOI 10.1109/BigDataSecurity.2017.50
   Liu Y, 2021, IEEE T CIRCUITS-I, V68, P4194, DOI 10.1109/TCSI.2021.3098841
   Ma RH, 2015, IEEE T EMERG TOP COM, V3, P372, DOI 10.1109/TETC.2015.2445101
   Malak D, 2020, IEEE INFOCOM SER, P327, DOI [10.1109/infocom41043.2020.9155442, 10.1109/INFOCOM41043.2020.9155442]
   Message Passing Interface Forum, 2021, MESSAGE PASSING INTE
   Mikami H, 2019, Arxiv, DOI arXiv:1811.05233
   Mnih V, 2016, PR MACH LEARN RES, V48
   Nguyen TT, 2021, 21ST IEEE/ACM INTERNATIONAL SYMPOSIUM ON CLUSTER, CLOUD AND INTERNET COMPUTING (CCGRID 2021), P396, DOI 10.1109/CCGrid51090.2021.00049
   NVIDIA, 2022, CUDA Toolkit Documentation
   NVIDIA, 2020, NVIDIA Collective Communication Library (NCCL) Documentation
   Qi L, 2008, 2008 WORKSHOP ON POWER ELECTRONICS AND INTELLIGENT TRANSPORTATION SYSTEM, PROCEEDINGS, P529, DOI 10.1109/PEITS.2008.124
   Qi ZW, 2017, IEEE T CLOUD COMPUT, V5, P443, DOI 10.1109/TCC.2016.2535295
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Schrittwieser J, 2020, NATURE, V588, P604, DOI 10.1038/s41586-020-03051-4
   Schulman J, 2017, Arxiv, DOI [arXiv:1707.06347, DOI 10.48550/ARXIV.1707.06347]
   Selman B., 2006, Encyclopedia of cognitive science, V81, P82, DOI [10.1002/0470018860.s00015, DOI 10.1002/0470018860.S00015]
   Sergeev A, 2018, Arxiv, DOI arXiv:1802.05799
   ShaohuaWan Zan Gao, 2021, ACM Transactions on Multimedia Computing, Communications, and Applications, V17, P1
   Silver D, 2018, SCIENCE, V362, P1140, DOI 10.1126/science.aar6404
   Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sohn I, 2019, ICT EXPRESS, V5, P163, DOI [10.1016/j.icte.2018.10.001, 10.1016/j.icte.2018.09.001]
   Thakur R, 2005, INT J HIGH PERFORM C, V19, P49, DOI 10.1177/1094342005051521
   Nguyen TT, 2021, CONCURR COMP-PRACT E, V33, DOI 10.1002/cpe.5574
   Wang Guanhua, 2020, Proceedings of Machine Learning and Systems, V2, P172
   Wang H, 2019, IEEE INFOCOM SER, P1288, DOI [10.1109/INFOCOM.2019.8737391, 10.1109/infocom.2019.8737391]
   Wang ST, 2018, ADV NEUR IN, V31
   Wang S, 2021, IEEE ACM T NETWORK, V29, P2019, DOI 10.1109/TNET.2021.3075432
   Watcharapichat P, 2016, PROCEEDINGS OF THE SEVENTH ACM SYMPOSIUM ON CLOUD COMPUTING (SOCC 2016), P84, DOI 10.1145/2987550.2987586
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696
   Yang XH, 2021, IEEE T CIRCUITS-II, V68, P953, DOI 10.1109/TCSII.2020.3015877
   Ying CS, 2018, Arxiv, DOI arXiv:1811.06992
   Zhang JN, 2019, I C DES RELIABL COMM, P122, DOI [10.1109/drcn.2019.8713747, 10.1109/DRCN.2019.8713747]
   Zhang Jiaru, 2022, Improving Bayesian neural networks by adversarial sampling
   Zhang W, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3292059
   Zhang XX, 2020, IEEE INFOCOM SER, P139, DOI [10.1109/infocom41043.2020.9155448, 10.1109/INFOCOM41043.2020.9155448]
   Zhang Y, 2019, IEEE NETWORK, V33, P58, DOI 10.1109/MNET.2019.1800344
   Zheng Y, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3501404
   Zixian Cai, 2021, PPoPP '21: Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, P62, DOI 10.1145/3437801.3441620
   Zou SJ, 2019, INT CON DISTR COMP S, P57, DOI 10.1109/ICDCS.2019.00015
NR 75
TC 3
Z9 3
U1 12
U2 20
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2024
VL 20
IS 2
SI SI
AR 34
DI 10.1145/3549939
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W6GR4
UT WOS:001092595800004
DA 2024-08-05
ER

PT J
AU Tasaka, S
AF Tasaka, Shuji
TI Usefulness of QoS in Multidimensional QoE Prediction for
   Haptic-Audiovisual Communications
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Quality of Experience (QoE); Quality of Service (QoS); multidimensional
   QoE; QoE prediction; logistic regression; SEM; MIMIC; construct; latent
   variables; Bayesianmodeling; hapticaudiovisual interactive
   communications; MCMC; JAGS; R
AB This article investigates prediction of Quality of Experience (QoE) by comparing borrowing-from-neighbor situations and isolated ones. We demonstrate that joint utilization of multiple QoE measures enhances the accuracy of QoE prediction compared to that by a collection of individual QoE measures each regressed on Quality of Service (QoS) parameters, while the accuracy improvement with additional usage of QoS information in the former is limited. As an example of system that needs multidimensional QoE representation, the article gives haptic audiovisual interactive communications. We employ QoE and QoS data taken previously in an experiment, where 13 QoE measures (a five-point score each) and 12 QoS parameters (nonnegative continuous values each) are available at three average rates of load traffic. We build two kinds of Bayesianmodels for QoE prediction; one is a logistic regression model of a single QoE measure as the response variable and QoS parameters as predictors, which is a typical traditional method of discrete QoE prediction and isolated in a sense. The other is a structural equation model (SEM) with latent constructs (i.e., factors) of audiovisual quality, haptic quality, and user experience quality; the original SEM, which contains only QoE indicators of three constructs (audiovisual quality (AVQ), haptic quality (HQ), and user experience quality (UXQ)), was proposed in one of the author's previous studies. This article extends the SEM to accommodate QoS parameters. We develop two kinds of new SEMs with QoS parameters: One has three extended constructs referred to as eAVQ, eHQ, and eUXQ, each of which has both QoE and QoS indicators, and the other has separate constructs for QoE and QoS, which lead to totally six constructs (AVqoe, Hqoe, UXqoe, AVqos, Hqos, and UXqos). We performed Markov chain Monte Carlo simulation of the Bayesian models with the JAGS software in an R environment. For comparison of QoE prediction accuracy, we adopt the 10-fold cross validation method and in part widely applicable information criterion. We then found that the three-construct models outperform the logistic regression models with respect to all subjective QoE measures and that the two kinds of the models are comparable as for the objective measure. The six-construct model exhibits almost the same accuracy as that of the three-construct one unless the number of QoE measures (nqoe) in the model is small. When the number nqoe is small, single-construct models may be a better choice. We have thus learned that multiple QoE measures should be utilized jointly (i.e., borrowing from neighbor) in QoE prediction rather than resorting to QoS information only.
C1 [Tasaka, Shuji] Nagoya Ind Sci Res Inst, 1-13 Yotsuya Dori,Chikusa ku, Nagoya, Aichi 4640819, Japan.
RP Tasaka, S (corresponding author), Nagoya Ind Sci Res Inst, 1-13 Yotsuya Dori,Chikusa ku, Nagoya, Aichi 4640819, Japan.
EM tasaka@nisri.jp
FU JSPS KAKENHI (Japan Society for the Promotion of Science) [20K04495]
FX This work was supported by JSPS KAKENHI (Grant-In-Aid for Scientific
   Research of Japan Society for the Promotion of Science) grant number
   20K04495.
CR Aharonson Barak S., 2004, P DRUID SUMM C IND D
   Bi T, 2018, PROCEEDINGS OF THE 10TH ACM WORKSHOP ON IMMERSIVE MIXED AND VIRTUAL ENVIRONMENT SYSTEMS (MMVE'18), P1, DOI 10.1145/3210438.3210443
   Bollen K. A., 1989, STRUCTURAL EQUATIONS, DOI DOI 10.1002/9781118619179
   Congdon P., 2014, Applied Bayesian modelling
   Hamam A., 2010, Haptic Audio-Visual Environments and Games (HAVE), 2010 IEEE International Symposium on, P1
   Hamam A, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2540991
   Hamam A, 2013, IEEE T INSTRUM MEAS, V62, P3315, DOI 10.1109/TIM.2013.2272859
   Hamam A, 2013, MULTIMED TOOLS APPL, V67, P455, DOI 10.1007/s11042-012-0990-7
   Hossfeld T, 2020, PROCEEDINGS OF THE 2020 6TH IEEE CONFERENCE ON NETWORK SOFTWARIZATION (NETSOFT 2020): BRIDGING THE GAP BETWEEN AI AND NETWORK SOFTWARIZATION, P51, DOI 10.1109/NetSoft48620.2020.9165426
   Isomura Eiichi, 2013, IEICE Transactions on Communications (Japanese Edition), VJ96-B, P59
   Isomura E, 2013, 2013 IEEE CONSUMER COMMUNICATIONS AND NETWORKING CONFERENCE (CCNC), P196, DOI 10.1109/CCNC.2013.6488446
   Iwata K., 2010, Computers in Entertainment (CIE), V8, P1
   JAGS, Just Another Gibbs Sampler 4.3.1.
   James G, 2013, SPRINGER TEXTS STAT, V103, P1, DOI 10.1007/978-1-4614-7138-7_1
   Kougioumtzidis G, 2022, IEEE ACCESS, V10, P19507, DOI 10.1109/ACCESS.2022.3149592
   Levy R, 2016, CH CRC STAT SOC BEHA, P1, DOI 10.1201/9781315374604
   Lunn D., 2013, BUGS BOOK PRACTICAL
   Nunome T, 2020, IEICE T COMMUN, VE103B, P1107, DOI 10.1587/transcom.2019EBP3235
   Pearl J, 2016, J CAUSAL INFERENCE, V4, DOI 10.1515/jci-2016-0021
   Salisbury JK, 1997, IEEE COMPUT GRAPH, V17, P6, DOI 10.1109/MCG.1997.1626171
   Skorin-Kapov L, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3176648
   Song X., 2012, Basic and advanced Bayesian structural equation modeling: With applications in the medical and behavioral sciences
   Su Yu-Sung, 2022, Package 'R2jags': Using R to Run 'JAGS'.
   Tasaka S, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3473986
   Tasaka S, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3375922
   Tatematsu A., 2010, Communications Quality and Reliability (CQR), 2010 IEEE International Workshop Technical Committee on, P1, DOI DOI 10.1109/CQR.2010.5619913
   Watanabe S., 2018, MATH THEORY BAYESIAN
   Watanabe S, 2010, J MACH LEARN RES, V11, P3571
   Xu Xiao, 2017, P IEEE INT S HAPT AU, DOI [10.1109/HAVE.2017.8240352, DOI 10.1109/HAVE.2017.8240352]
   Yuan ZH, 2015, IEEE T MULTIMEDIA, V17, P104, DOI 10.1109/TMM.2014.2371240
NR 30
TC 0
Z9 0
U1 1
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2024
VL 20
IS 1
AR 22
DI 10.1145/3613246
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T8LL3
UT WOS:001080441800022
DA 2024-08-05
ER

PT J
AU Yang, YM
   Hu, WP
   Hu, HF
AF Yang, Yiming
   Hu, Weipeng
   Hu, Haifeng
TI Syncretic Space Learning Network for NIR-VIS Face Recognition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE NIR-VIS face recognition; syncretic space learning; adversarial learning
ID HALLUCINATION
AB To overcome the technical bottleneck of face recognition in low-light scenarios, Near-InfraRed and VISible (NIR-VIS) heterogeneous face recognition is proposed for matching well-lit VIS faces with poorly lit NIR faces. Current cross-modal synthesis methods visually convert the NIR modality to the VIS modality and then perform face matching in the VIS modality. However, using a heavyweight GAN network on unpaired NIR-VIS faces may lead to high synthesis difficulty, low inference efficiency, and other problems. To alleviate the above problems, we simultaneously synthesize NIR and VIS images into modality-independent syncretic images and propose a novel syncretic space learning (SSL) model to eliminate the modal gap. First, Syncretic Modality Generator (SMG) synthesizes NIR and VIS images into syncretic images using channel-level convolution with a shallow CNN. In particular, the discriminative structural information is well preserved and the face quality can be further improved with small modal variations in a self-supervised learning manner. Second, Modality-adversarial Syncretic space Learning (MSL) projects NIR and VIS images into the syncretic space by a syncretic-modality adversarial learning strategy with syncretic pattern guided objective, so the modal gap of NIR-VIS faces can be effectively reduced. Finally, the Syncretic Distribution Consistency (SDC) constructed by NIR-syncretic, syncretic-syncretic, and VIS-syncretic consistency can enhance the intra-class compactness and learn discriminative representations. Extensive experiments on three challenging datasets demonstrate the effectiveness of the SSL method.
C1 [Yang, Yiming; Hu, Haifeng] Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou, Peoples R China.
   [Hu, Weipeng] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore, Singapore.
C3 Sun Yat Sen University; Nanyang Technological University
RP Hu, HF (corresponding author), Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou, Peoples R China.
EM yangym53@mail2.sysu.edu.cn; weipeng.hu@ntu.edu.sg;
   huhaif@mail.sysu.edu.cn
RI Yang, Yiming/HMO-9785-2023; Weipeng, Hu/AAS-1819-2020
OI Weipeng, Hu/0000-0003-2886-7346; Hu, Haifeng/0000-0002-4884-323X; Yang,
   Yiming/0009-0005-7466-1041
FU National Natural Science Foundation of China [62076262, 61673402,
   61273270, 60802069]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62076262, Grant 61673402, Grant
   61273270, and Grant 60802069.
CR Chen J., 2020, INT S INT COMP APPL, P242, DOI DOI 10.1007/978-981-15-5577-0_18
   Chen J, 2009, PROC CVPR IEEE, P156, DOI 10.1109/CVPRW.2009.5206832
   Dai PY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P677
   Di X, 2018, INT CONF BIOMETR THE
   Dong Yi, 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163093
   Duan Boyan, 2020, P IEEE CVF C COMP VI, P7927, DOI 10.1109/CVPR42600.2020.00795
   Fu CY, 2023, IEEE T PATTERN ANAL, V45, P9135, DOI 10.1109/TPAMI.2022.3227180
   Fu CY, 2022, IEEE T PATTERN ANAL, V44, P2938, DOI 10.1109/TPAMI.2021.3052549
   Gong DH, 2017, IEEE T IMAGE PROCESS, V26, P2079, DOI 10.1109/TIP.2017.2651380
   Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6
   Hao X, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16383, DOI 10.1109/ICCV48922.2021.01609
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He R, 2019, IEEE T PATTERN ANAL, V41, P1761, DOI 10.1109/TPAMI.2018.2842770
   He R, 2020, IEEE T PATTERN ANAL, V42, P1025, DOI 10.1109/TPAMI.2019.2961900
   He R, 2017, AAAI CONF ARTIF INTE, P2000
   Hu WP, 2022, IEEE T INF FOREN SEC, V17, P1435, DOI 10.1109/TIFS.2022.3160612
   Hu WP, 2022, IEEE T CIRC SYST VID, V32, P3630, DOI 10.1109/TCSVT.2021.3105411
   Hu WP, 2022, IEEE T CIRC SYST VID, V32, P2411, DOI 10.1109/TCSVT.2021.3081514
   Hu WP, 2021, IEEE T MULTIMEDIA, V23, P145, DOI 10.1109/TMM.2020.2980201
   Hu WP, 2021, IEEE T INF FOREN SEC, V16, P70, DOI 10.1109/TIFS.2020.3005314
   Huang G. B., 2007, Technical report
   Kan MN, 2016, IEEE T PATTERN ANAL, V38, P188, DOI 10.1109/TPAMI.2015.2435740
   Klare BF, 2013, IEEE T PATTERN ANAL, V35, P1410, DOI 10.1109/TPAMI.2012.229
   Lei Z, 2009, PROC CVPR IEEE, P1123, DOI 10.1109/CVPRW.2009.5206860
   Li DG, 2020, AAAI CONF ARTIF INTE, V34, P4610
   Li SZ, 2013, IEEE COMPUT SOC CONF, P348, DOI 10.1109/CVPRW.2013.59
   Lin X., 2022, P IEEECVF C COMPUTER, P20973
   Liu DC, 2022, IEEE T NEUR NET LEAR, V33, P5611, DOI 10.1109/TNNLS.2021.3071119
   Liu DC, 2018, NEUROCOMPUTING, V302, P46, DOI 10.1016/j.neucom.2018.03.042
   Liu XX, 2016, INT CONF BIOMETR
   Liu X, 2017, FRONT COMPUT SCI-CHI, V11, P208, DOI 10.1007/s11704-016-6076-3
   Lu JW, 2018, IEEE T PATTERN ANAL, V40, P1979, DOI 10.1109/TPAMI.2017.2737538
   Luo MD, 2022, IEEE T INF FOREN SEC, V17, P2095, DOI 10.1109/TIFS.2022.3177960
   Mudunuri SP, 2019, IEEE T INF FOREN SEC, V14, P886, DOI 10.1109/TIFS.2018.2868173
   Panetta K, 2020, IEEE T PATTERN ANAL, V42, P509, DOI 10.1109/TPAMI.2018.2884458
   Pang M, 2022, IEEE T INF FOREN SEC, V17, P1544, DOI 10.1109/TIFS.2022.3164215
   Parkhi O. M., 2015, P BRIT MACH VIS C, p41.1
   Peng CL, 2021, IEEE T INF FOREN SEC, V16, P346, DOI 10.1109/TIFS.2020.3013209
   Peng CL, 2019, PATTERN RECOGN, V90, P161, DOI 10.1016/j.patcog.2019.01.041
   Reale C, 2016, IEEE COMPUT SOC CONF, P320, DOI 10.1109/CVPRW.2016.47
   Ren Y., 2021, P IEEE CVF INT C COM, P6793
   Saxena S, 2016, LECT NOTES COMPUT SC, V9915, P483, DOI 10.1007/978-3-319-49409-8_40
   Shao M, 2017, IEEE T NEUR NET LEAR, V28, P451, DOI 10.1109/TNNLS.2016.2517014
   Sifei Liu, 2012, 2012 5th IAPR International Conference on Biometrics (ICB), P79, DOI 10.1109/ICB.2012.6199762
   Song LX, 2018, AAAI CONF ARTIF INTE, P7355
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu AC, 2017, IEEE I CONF COMP VIS, P5390, DOI 10.1109/ICCV.2017.575
   Wu F, 2021, PATTERN RECOGN, V111, DOI 10.1016/j.patcog.2020.107632
   Wu HX, 2021, INT C PATT RECOG, P4206, DOI 10.1109/ICPR48806.2021.9412141
   Wu X, 2018, AAAI CONF ARTIF INTE, P1679
   Wu X, 2018, IEEE T INF FOREN SEC, V13, P2884, DOI 10.1109/TIFS.2018.2833032
   Xu Yingguo, 2020, IJCB, P1
   Yang SM, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4137, DOI 10.1145/3474085.3475546
   Yang Yiming, 2023, IEEE Trans. Circ. Syst. Vid. Technol., V2023
   Yang ZM, 2022, IEEE T INF FOREN SEC, V17, P1344, DOI 10.1109/TIFS.2022.3160595
   Yu AJ, 2021, INT J COMPUT VISION, V129, P1467, DOI 10.1007/s11263-021-01432-4
   Yu JC, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1018
   Zhang H, 2019, INT J COMPUT VISION, V127, P845, DOI 10.1007/s11263-019-01175-3
   Zhang KP, 2016, IEEE SIGNAL PROC LET, V23, P1499, DOI 10.1109/LSP.2016.2603342
   Zhang YK, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P788, DOI 10.1145/3474085.3475250
   Zhao J, 2022, NEUROCOMPUTING, V506, P355, DOI 10.1016/j.neucom.2022.07.084
   Zhao Suiyi, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P6220, DOI 10.1145/3503161.3548113
   Zhu JY, 2014, IEEE T INF FOREN SEC, V9, P501, DOI 10.1109/TIFS.2014.2299977
NR 63
TC 3
Z9 3
U1 5
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2024
VL 20
IS 1
AR 11
DI 10.1145/3607143
PG 25
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T8LL3
UT WOS:001080441800011
DA 2024-08-05
ER

PT J
AU Zou, C
   Wang, R
   Jin, C
   Zhang, S
   Wang, X
AF Zou, Cong
   Wang, Rui
   Jin, Cheng
   Zhang, Sanyi
   Wang, Xin
TI S<SUP>2</SUP>CL - Leaf Net : Recognizing Leaf Images Like Human
   Botanists
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Fine-grained image classification; few-shot learning; leaf recognition
ID CLASSIFICATION; PLANTS
AB Automatically classifying plant leaves is a challenging fine-grained classification task because of the diversity in leaf morphology, including size, texture, shape, and venation. Although powerful deep learning-based methods have achieved great improvement in leaf classification, these methods still require a large number of well-labeled samples for supervised training, which is difficult to get. In contrast, relying on the specific coarse-to-fine classification strategy, human botanists only require a small number of samples for accurate leaf recognition. Inspired by the classification strategy of human botanists, we propose a novel (SCL)-C-2 - Leaf Net, which exploits multi-granularity clues with a hierarchical attention mechanism and boosts the learning ability with the supervised sampling contrastive learning with limited training samples to classify plant leaves as human botanists do. Specifically, to fully explore and exploit the subtle details of the leaves, a novel sampling transformation mechanism is combined with the supervised contrastive learning to enhance the network's perception of details by amplifying the discriminative regions with a weighted sampling of different regions. Furthermore, we construct the hierarchical attention mechanism to produce attention maps of different granularity, which helps to discover details in leaves that are important for classification. Experiments are conducted on the open-access leaf datasets, including Flavia, Swedish, and LeafSnap, which prove the effectiveness of the proposed (SCL)-C-2 - Leaf Net
C1 [Zou, Cong; Wang, Rui] Chinese Acad Sci, Inst Informat Engn, State Key Lab Informat Secur, Beijing, Peoples R China.
   [Zou, Cong; Wang, Rui] Univ Chinese Acad Sci, Sch Cyber Secur, 19 Shucun Rd, Beijing, Peoples R China.
   [Jin, Cheng] Fudan Univ, Shanghai Key Lab Intelligent Informat Proc, Sch Comp Sci, 2005 Songhu Rd, Shanghai, Peoples R China.
   [Zhang, Sanyi] Chinese Acad Sci, Inst Informat Engn, State Key Lab Informat Secur, 19 Shucun Rd, Beijing, Peoples R China.
   [Wang, Xin] Tsinghua Univ, Dept Comp Sci & Technol, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Information Engineering, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Fudan University; Chinese Academy of Sciences; Tsinghua University
RP Wang, R (corresponding author), Chinese Acad Sci, Inst Informat Engn, State Key Lab Informat Secur, Beijing, Peoples R China.; Wang, R (corresponding author), Univ Chinese Acad Sci, Sch Cyber Secur, 19 Shucun Rd, Beijing, Peoples R China.
EM zoucong@iie.ac.cn; wangrui@iie.ac.cn; jc@fudan.edu.cn;
   zhangsanyi@iie.ac.cn; xin_wang@tsinghua.edu.cn
RI li, yf/KHX-1148-2024; zhang, yan/KHC-3163-2024; li, li/KHE-5750-2024;
   zhang, zheng/KHY-8870-2024; li, lan/KCJ-5061-2024; Zhang,
   Yi/KHW-2039-2024; zhang, yuanyuan/KHV-4459-2024; Lu, Xin/KHW-8570-2024;
   wang, nan/KHW-4897-2024; wang, rui/JAC-6240-2023; yang,
   le/KFB-5420-2024; Chen, Nuo/JZD-0344-2024; li, qing/KHU-6871-2024; Wang,
   Chen/JZE-6385-2024; yang, ying/KHW-9378-2024; Wang,
   Jiachen/KFT-0161-2024; lin, lin/KFB-9548-2024; jing, wang/KCZ-2144-2024;
   Wang, Fei/KEH-6292-2024; ZHANG, JING/KHY-1073-2024; li,
   jing/KHC-8303-2024; ZHOU, YUE/KCJ-8790-2024; wang, jin/KHD-7243-2024
OI Lu, Xin/0000-0001-9885-6031; Zou, Cong/0000-0002-1901-5363; , Rui
   Wang/0000-0002-4792-1945; Wang, Xin/0000-0002-0351-2939; Jin,
   Cheng/0000-0003-3063-1957
FU National Natural Science Foundation of China [U20B2066, 62176253]
FX This work is supported in part by the National Natural Science
   Foundation of China under Grant Nos. U20B2066 and 62176253.
CR Aakif A, 2015, BIOSYST ENG, V139, P66, DOI 10.1016/j.biosystemseng.2015.08.003
   Barbedo JGA, 2018, COMPUT ELECTRON AGR, V153, P46, DOI 10.1016/j.compag.2018.08.013
   Arun Priya C., 2012, Proceedings of the 2012 International Conference on Pattern Recognition, Informatics and Medical Engineering (PRIME), P428, DOI 10.1109/ICPRIME.2012.6208384
   Ash A., 1999, MANUAL LEAF ARCHITEC
   Barré P, 2017, ECOL INFORM, V40, P50, DOI 10.1016/j.ecoinf.2017.05.005
   Beikmohammadi A., 2020, arXiv
   Cai SJ, 2017, IEEE I CONF COMP VIS, P511, DOI 10.1109/ICCV.2017.63
   Chang DL, 2021, PROC CVPR IEEE, P11471, DOI 10.1109/CVPR46437.2021.01131
   Chen T, 2019, IEEE T IMAGE PROCESS, V28, P2389, DOI 10.1109/TIP.2018.2886758
   Coombes Allen J., 2014, The Book of Leaves: A Leaf-by-leaf Guide to Six Hundred of the World's Great Trees
   Cui Y, 2017, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR.2017.325
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding Y, 2019, IEEE I CONF COMP VIS, P6598, DOI 10.1109/ICCV.2019.00670
   Elhariri E, 2014, 2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER ENGINEERING & SYSTEMS (ICCES), P271, DOI 10.1109/ICCES.2014.7030971
   Ellis B., 2009, Manual of Leaf Architecture
   Gao Y, 2016, PROC CVPR IEEE, P317, DOI 10.1109/CVPR.2016.41
   Guan X, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5011, DOI 10.1145/3474085.3475184
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu J, 2018, IEEE SIGNAL PROC LET, V25, P853, DOI 10.1109/LSP.2018.2809688
   Hu YQ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4239, DOI 10.1145/3474085.3475561
   Hu YT, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3446208
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Isola P., 2020, arXiv, DOI DOI 10.48550/ARXIV.2004.11362
   Jaderberg M, 2015, ADV NEUR IN, V28
   Kolesnikov Alexander, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P491, DOI 10.1007/978-3-030-58558-7_29
   Kolivand H, 2019, ARAB J SCI ENG, V44, P3315, DOI 10.1007/s13369-018-3504-8
   Kong S, 2017, PROC CVPR IEEE, P7025, DOI 10.1109/CVPR.2017.743
   Krause J, 2015, PROC CVPR IEEE, P5546, DOI 10.1109/CVPR.2015.7299194
   Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3_36
   Lam M, 2017, PROC CVPR IEEE, P6497, DOI 10.1109/CVPR.2017.688
   Lee S, 2017, 24TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2017), DOI [10.14722/ndss.2017.23457, 10.1016/j.patcog.2017.05.015]
   Li GJ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5273, DOI 10.1145/3474085.3475646
   Li LL, 2015, J INTELL FUZZY SYST, V29, P1465, DOI 10.3233/IFS-151626
   Lin D, 2015, PROC CVPR IEEE, P1666, DOI 10.1109/CVPR.2015.7298775
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Liu Z, 2022, Arxiv, DOI [arXiv:2111.09883, 10.48550/arXiv.2111.09883, DOI 10.48550/ARXIV.2111.09883]
   Naresh YG, 2016, NEUROCOMPUTING, V173, P1789, DOI 10.1016/j.neucom.2015.08.090
   Nauta M, 2021, PROC CVPR IEEE, P14928, DOI 10.1109/CVPR46437.2021.01469
   Pearline SA, 2019, J INTELL FUZZY SYST, V36, P1997, DOI 10.3233/JIFS-169911
   Recasens A, 2018, LECT NOTES COMPUT SC, V11213, P52, DOI 10.1007/978-3-030-01240-3_4
   Ruoyi Du, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P153, DOI 10.1007/978-3-030-58565-5_10
   Saleem G, 2019, COMPUT ELECTRON AGR, V157, P270, DOI 10.1016/j.compag.2018.12.038
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh UP, 2019, IEEE ACCESS, V7, P43721, DOI 10.1109/ACCESS.2019.2907383
   Soderkvist O., 2001, COMPUTER VISION CLAS
   Sun M, 2018, LECT NOTES COMPUT SC, V11220, P834, DOI 10.1007/978-3-030-01270-0_49
   Thanh TKN, 2018, LECT NOTES ARTIF INT, V10751, P565, DOI 10.1007/978-3-319-75417-8_53
   Wang B, 2019, IEEE ACCESS, V7, P151754, DOI 10.1109/ACCESS.2019.2947510
   Wang YM, 2018, PROC CVPR IEEE, P4148, DOI 10.1109/CVPR.2018.00436
   Wang ZB, 2016, NEURAL COMPUT APPL, V27, P899, DOI 10.1007/s00521-015-1904-1
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120
   Yang S., 2021, arXiv
   Yang Z, 2018, LECT NOTES COMPUT SC, V11218, P438, DOI 10.1007/978-3-030-01264-9_26
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang H, 2016, PROC CVPR IEEE, P1143, DOI 10.1109/CVPR.2016.129
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54
   Zhang SW, 2011, KNOWL-BASED SYST, V24, P341, DOI 10.1016/j.knosys.2010.11.002
   Zhao C, 2015, PATTERN RECOGN, V48, P3203, DOI 10.1016/j.patcog.2015.04.004
   Zhao Yifan, 2022, CoRR abs/2212.13685
   Zheng HL, 2019, Arxiv, DOI arXiv:1911.03621
   Zheng HL, 2019, PROC CVPR IEEE, P5007, DOI 10.1109/CVPR.2019.00515
   Zheng HL, 2017, IEEE I CONF COMP VIS, P5219, DOI 10.1109/ICCV.2017.557
NR 63
TC 0
Z9 0
U1 3
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2024
VL 20
IS 1
AR 30
DI 10.1145/3615659
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T8LL3
UT WOS:001080441800030
OA Bronze
DA 2024-08-05
ER

PT J
AU Xie, TX
   Gai, KK
   Zhu, LH
   Wang, S
   Zhang, ZJ
AF Xie, Tianxiu
   Gai, Keke
   Zhu, Liehuang
   Wang, Shuo
   Zhang, Zijian
TI RAC-Chain: An Asynchronous Consensus-based Cross-chain Approach to
   Scalable Blockchain for Metaverse
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Relay chain; cross-chain technology; asynchronous consensus; blockchain
   scalability; blockchain interoperability
AB The metaverse, as an emerging technical term, conceptually aims to construct a virtual digital space that runs parallel to the physical world. Due to human behaviors and interactions being represented in the virtual world, security in themetaverse is a challenging issue inwhich the traditional centralized servicemodel is one of the threat sources. To conquer the obstacle caused by centralized computing, blockchain-based solutions are potential problem-solving methods. However, it is difficult for a single blockchain to support large-scale data and business services in the metaverse, due to the scalability restrictions. Moreover, multi-chain settings also encounter the interoperability issues. In this work, we propose a Relay chain and Asynchronous consensus-based Consortium blockchain cross-Chain model, which realizes message transmission and cross-chain transactions in multiple chains by adopting the relay chain and cross-chain gateways. All nodes of the application chains and the relay chain execute cross-chain transactions in sequence and reach a consensus on transactions at any transmission delay. Our experiment evaluations demonstrate that our approach performs well in atomicity, security, and functionality (cross-chain transactions), such that the performance of blockchain scalability in the metaverse can be improved, compared with the traditional relay chain schemes.
C1 [Xie, Tianxiu; Gai, Keke; Zhu, Liehuang; Wang, Shuo; Zhang, Zijian] Beijing Inst Technol, Rm 701,5,S. Zhongguancun Str, Beijing 100081, Peoples R China.
C3 Beijing Institute of Technology
RP Xie, TX; Gai, KK (corresponding author), Beijing Inst Technol, Rm 701,5,S. Zhongguancun Str, Beijing 100081, Peoples R China.
EM 3120215672@bit.edu.cn; gaikeke@bit.edu.cn; liehuangz@bit.edu.cn;
   3220215214@bit.edu.cn; zhangzijian@bit.edu.cn
RI Gai, Keke/M-4857-2017
OI Gai, Keke/0000-0001-6784-0221; Zhang, Zijian/0000-0002-6313-4407
FU National Key Research and Development Program of China [2021YFB2701300];
   National Natural Science Foundation of China [62232002]
FX This work is supported by the National Key Research and Development
   Program of China (Grant No. 2021YFB2701300) and National Natural Science
   Foundation of China (Grant No. 62232002).
CR Abraham I, 2020, P IEEE S SECUR PRIV, P106, DOI 10.1109/SP40000.2020.00044
   Amoussou-Guenou Y., 2018, C PRINC DISTR SYST O
   Ben-Or M., 1994, Proceedings of the Thirteenth Annual ACM Symposium on Principles of Distributed Computing, P183, DOI 10.1145/197917.198088
   Bessani A, 2020, I C DEPEND SYS NETWO, P424, DOI 10.1109/DSN48063.2020.00057
   Canetti R., 1993, Proceedings of the Twenty-Fifth Annual ACM Symposium on the Theory of Computing, P42, DOI 10.1145/167088.167105
   Cong Zhong, 2022, 2022 IEEE 2nd International Conference on Power, Electronics and Computer Applications (ICPECA), P422, DOI 10.1109/ICPECA53709.2022.9719075
   Dai B., 2020, P INT C BLOCKCH TRUS, P218
   Deng LP, 2018, LECT NOTES COMPUT SC, V10973, P144, DOI 10.1007/978-3-319-94340-4_12
   Duan HH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P153, DOI 10.1145/3474085.3479238
   Duan SS, 2018, PROCEEDINGS OF THE 2018 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (CCS'18), P2028, DOI 10.1145/3243734.3243812
   DWORK C, 1988, J ACM, V35, P288, DOI 10.1145/42282.42283
   FISCHER MJ, 1985, J ACM, V32, P374, DOI 10.1145/3149.214121
   Frauenthaler P, 2020, 2020 IEEE INTERNATIONAL CONFERENCE ON BLOCKCHAIN (BLOCKCHAIN 2020), P204, DOI 10.1109/Blockchain50366.2020.00032
   Gai KK, 2023, IEEE T SERV COMPUT, V16, P1673, DOI 10.1109/TSC.2022.3192166
   Gai KK, 2023, IEEE T COMPUT SOC SY, V10, P2201, DOI 10.1109/TCSS.2022.3226717
   Gai KK, 2022, ACM T SENSOR NETWORK, V18, DOI 10.1145/3526195
   Guo BY, 2020, CCS '20: PROCEEDINGS OF THE 2020 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P803, DOI 10.1145/3372297.3417262
   Miraz MH, 2019, Arxiv, DOI arXiv:1902.04471
   Herlihy M, 2018, PODC'18: PROCEEDINGS OF THE 2018 ACM SYMPOSIUM ON PRINCIPLES OF DISTRIBUTED COMPUTING, P245, DOI 10.1145/3212734.3212736
   Hope-Bailie A, 2016, PROCEEDINGS OF THE 25TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'16 COMPANION), P281, DOI 10.1145/2872518.2889307
   Ittai A., 2020, J. Environ. Sciences (China) English Ed.
   Jiang YM, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19092042
   Kannengiesser N., 2020, P 53 HAW INT C SYST
   Kong LX, 2016, IEEE SENS J, V16, P5826, DOI 10.1109/JSEN.2016.2568243
   Lee L H., 2021, PREPRINT
   Lind J, 2019, PROCEEDINGS OF THE TWENTY-SEVENTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '19), P63, DOI 10.1145/3341301.3359627
   Liu SY, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P485
   Liu ZT, 2019, PROCEEDINGS OF THE 2019 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (CCS'19), P549, DOI 10.1145/3319535.3355503
   Lys Leonard, 2020, CRYBLOCK 2020. Proceedings of the 3rd Workshop on Cryptocurrencies and Blockchains for Distributed Systems, P59, DOI 10.1145/3410699.3413799
   Malkhi D, 2019, PROCEEDINGS OF THE 2019 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (CCS'19), P1041, DOI 10.1145/3319535.3354225
   Miller A., 2016, P 23 C COMPUTER COMM, P31
   Momose A, 2021, CCS '21: PROCEEDINGS OF THE 2021 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1686, DOI 10.1145/3460120.3484554
   Ou W, 2022, COMPUT NETW, V218, DOI 10.1016/j.comnet.2022.109378
   Pass R, 2017, LECT NOTES COMPUT SC, V10211, P643, DOI 10.1007/978-3-319-56614-6_22
   Gadekallu TR, 2022, Arxiv, DOI [arXiv:2203.09738, DOI 10.48550/ARXIV.2203.09738]
   Ryskeldiev B, 2018, ACM INT CONF PR SER, DOI 10.1145/3174910.3174952
   Saad M, 2021, CCS '21: PROCEEDINGS OF THE 2021 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P988, DOI 10.1145/3460120.3484561
   Seres IA, 2020, SPR PROC BUS ECON, P1, DOI 10.1007/978-3-030-37110-4_1
   Singh A, 2020, J NETW COMPUT APPL, V149, DOI 10.1016/j.jnca.2019.102471
   Thyagarajan SA, 2022, P IEEE S SECUR PRIV, P1299, DOI [10.1109/SP46214.2022.00063, 10.1109/SP46214.2022.9833731]
   Vujicic D., 2018, P 17 INT S INF JAH I, P1
   Wang A., 2022, arXiv
   Wang JP, 2019, PROCEEDINGS OF THE 16TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P95
   Wang YT, 2022, Arxiv, DOI arXiv:2203.02662
   Worley C, 2018, IEEE 2018 INTERNATIONAL CONGRESS ON CYBERMATICS / 2018 IEEE CONFERENCES ON INTERNET OF THINGS, GREEN COMPUTING AND COMMUNICATIONS, CYBER, PHYSICAL AND SOCIAL COMPUTING, SMART DATA, BLOCKCHAIN, COMPUTER AND INFORMATION TECHNOLOGY, P1582, DOI 10.1109/Cybermatics_2018.2018.00265
   Yang D, 2020, 2020 2ND INTERNATIONAL CONFERENCE ON BLOCKCHAIN TECHNOLOGY (ICBCT 2020), P1, DOI 10.1145/3390566.3391665
   Yin MF, 2019, PROCEEDINGS OF THE 2019 ACM SYMPOSIUM ON PRINCIPLES OF DISTRIBUTED COMPUTING (PODC '19), P347, DOI 10.1145/3293611.3331591
   Yin ZY, 2022, IEEE T INF FOREN SEC, V17, P3465, DOI 10.1109/TIFS.2022.3209546
   Zhao Z, 2019, INFORM SCIENCES, V505, P352, DOI 10.1016/j.ins.2019.07.086
   Zie JY, 2019, LECT NOTES COMPUT SC, V11737, P219, DOI 10.1007/978-3-030-31500-9_14
NR 50
TC 0
Z9 0
U1 2
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2024
VL 20
IS 7
SI SI
AR 187
DI 10.1145/3586011
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL0Q6
UT WOS:001234494100002
DA 2024-08-05
ER

PT J
AU Zhao, WC
   Hu, HZ
   Zhou, WG
   Li, L
   Li, HQ
AF Zhao, Weichao
   Hu, Hezhen
   Zhou, Wengang
   Li, Li
   Li, Houqiang
TI Exploiting Spatial-Temporal Context for Interacting Hand Reconstruction
   on Monocular RGB Video
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Interacting hand; model-based 3D hand reconstrcution; temporal context
ID POSE ESTIMATION; REALTIME; NETWORK
AB Reconstructing interacting hands from monocular RGB data is a challenging task, as it involves many interfering factors, e.g., self- and mutual occlusion and similar textures. Previous works only leverage information from a single RGB image without modeling their physically plausible relation, which leads to inferior reconstruction results. In this work, we are dedicated to explicitly exploiting spatial-temporal information to achieve better interacting hand reconstruction. On the one hand, we leverage temporal context to complement insufficient information provided by the single frame and design a novel temporal framework with a temporal constraint for interacting hand motion smoothness. On the other hand, we further propose an interpenetration detection module to produce kinetically plausible interacting hands without physical collisions. Extensive experiments are performed to validate the effectiveness of our proposed framework, which achieves new state-of-the-art performance on public benchmarks.
C1 [Zhao, Weichao; Hu, Hezhen; Zhou, Wengang; Li, Li; Li, Houqiang] Univ Sci & Technol China, Hefei 230027, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Zhao, WC (corresponding author), Univ Sci & Technol China, Hefei 230027, Anhui, Peoples R China.
EM saruka@mail.ustc.edu.cn; alexhu@mail.ustc.edu.cn; zhwg@ustc.edu.cn;
   lil1@ustc.edu.cn; lihq@ustc.edu.cn
OI Hu, Hezhen/0000-0003-0327-1562
FU GPU cluster built by MCC Lab of Information Science and Technology
   Institution; Supercomputing Center of the USTC
FX This work was supported by the GPU cluster built by MCC Lab of
   Information Science and Technology Institution and the Supercomputing
   Center of the USTC.
CR Ballan L, 2012, LECT NOTES COMPUT SC, V7577, P640, DOI 10.1007/978-3-642-33783-3_46
   Boukhayma A, 2019, PROC CVPR IEEE, P10835, DOI 10.1109/CVPR.2019.01110
   Cai YJ, 2018, LECT NOTES COMPUT SC, V11210, P678, DOI 10.1007/978-3-030-01231-1_41
   Chen YJ, 2021, PROC CVPR IEEE, P10446, DOI 10.1109/CVPR46437.2021.01031
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Diffenderfer J, 2021, ADV NEUR IN, V34
   Fan HH, 2022, IEEE T CIRC SYST VID, V32, P275, DOI 10.1109/TCSVT.2021.3058688
   Fan ZC, 2021, INT CONF 3D VISION, P1, DOI 10.1109/3DV53792.2021.00011
   Grady P, 2021, PROC CVPR IEEE, P1471, DOI 10.1109/CVPR46437.2021.00152
   Guo HK, 2017, IEEE IMAGE PROC, P4512, DOI 10.1109/ICIP.2017.8297136
   Han SC, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392452
   Hasson Y, 2020, PROC CVPR IEEE, P568, DOI 10.1109/CVPR42600.2020.00065
   Hasson Y, 2019, PROC CVPR IEEE, P11799, DOI 10.1109/CVPR.2019.01208
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu HZ, 2023, IEEE T PATTERN ANAL, V45, P11221, DOI 10.1109/TPAMI.2023.3269220
   Hu HZ, 2021, AAAI CONF ARTIF INTE, V35, P1558
   Huang S, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3152129
   Huang WT, 2020, AAAI CONF ARTIF INTE, V34, P11061
   Iqbal U, 2018, LECT NOTES COMPUT SC, V11215, P125, DOI 10.1007/978-3-030-01252-6_8
   Jiao YY, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3519305
   Kanazawa A, 2019, PROC CVPR IEEE, P5597, DOI 10.1109/CVPR.2019.00576
   Kim DU, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11169, DOI 10.1109/ICCV48922.2021.01100
   Kingma D. P., 2014, arXiv
   Li MC, 2022, PROC CVPR IEEE, P2751, DOI 10.1109/CVPR52688.2022.00278
   Li MR, 2021, AAAI CONF ARTIF INTE, V35, P1921
   Liang H, 2015, IEEE T CIRC SYST VID, V25, P1125, DOI 10.1109/TCSVT.2014.2363750
   Lin FQ, 2021, IEEE WINT CONF APPL, P2372, DOI 10.1109/WACV48630.2021.00242
   Liu JB, 2021, Arxiv, DOI arXiv:2106.13391
   Meng H, 2022, LECT NOTES COMPUT SC, V13666, P380, DOI 10.1007/978-3-031-20068-7_22
   Moon G, 2018, PROC CVPR IEEE, P5079, DOI 10.1109/CVPR.2018.00533
   Moon Gyeongsik, 2020, EUR C COMPUT VIS
   Mueller F, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322958
   Narasimhaswamy S., 2020, C NEUR INF PROC SYST, P7841
   Paszke A, 2019, ADV NEUR IN, V32
   Qian C, 2014, PROC CVPR IEEE, P1106, DOI 10.1109/CVPR.2014.145
   Romero J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130883
   Romero J, 2010, IEEE INT CONF ROBOT, P458, DOI 10.1109/ROBOT.2010.5509753
   Sanchez-Riera J, 2018, IEEE T CIRC SYST VID, V28, P2289, DOI 10.1109/TCSVT.2017.2718622
   Shan Dandan, 2021, ADV NEURAL INF PROCE, P5898
   Sharp T, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P3633, DOI 10.1145/2702123.2702179
   Smith B, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417768
   Tagliasacchi A, 2015, COMPUT GRAPH FORUM, V34, P101, DOI 10.1111/cgf.12700
   Tang DH, 2015, IEEE I CONF COMP VIS, P3325, DOI 10.1109/ICCV.2015.380
   Tompson J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2629500
   Tzionas D, 2015, IEEE I CONF COMP VIS, P729, DOI 10.1109/ICCV.2015.90
   Vaswani A, 2017, ADV NEUR IN, V30
   Wan CD, 2019, PROC CVPR IEEE, P10845, DOI 10.1109/CVPR.2019.01111
   Wang JY, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417852
   Wang YG, 2019, IEEE T CIRC SYST VID, V29, P3258, DOI 10.1109/TCSVT.2018.2879980
   Wang YG, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462000
   Xiao Y, 2024, ACM T MULTIM COMPUT, V20, DOI 10.1145/3561822
   Xu L, 2021, IEEE T CIRC SYST VID, V31, P890, DOI 10.1109/TCSVT.2020.2991987
   Yang John, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P122, DOI 10.1007/978-3-030-58610-2_8
   Yang LX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11077, DOI 10.1109/ICCV48922.2021.01091
   Zhang BW, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11334, DOI 10.1109/ICCV48922.2021.01116
   Zhao Weichao, 2023, P AAAI C ARTIFICIAL, P3597
   Zhou YX, 2020, PROC CVPR IEEE, P5345, DOI 10.1109/CVPR42600.2020.00539
   Zimmermann C, 2017, IEEE I CONF COMP VIS, P4913, DOI 10.1109/ICCV.2017.525
NR 58
TC 0
Z9 0
U1 1
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2024
VL 20
IS 6
SI SI
AR 171
DI 10.1145/3639707
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OQ2T3
UT WOS:001208681800021
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Liu, Y
   Zhao, MB
   Zhang, Z
   Liu, YP
   Yan, SC
AF Liu, Yu
   Zhao, Mingbo
   Zhang, Zhao
   Liu, Yuping
   Yan, Shuicheng
TI Arbitrary Virtual Try-on Network: Characteristics Preservation and
   Tradeoff between Body and Clothing
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Deep learning; virtual try-on; generative adversarial networks;
   artificial intelligence in fashion
AB Deep learning based virtual try-on system has achieved some encouraging progress recently, but there still remain several big challenges that need to be solved, such as trying on arbitrary clothes of all types, trying on the clothes from one category to another and generating image-realistic results with few artifacts. To handle this issue, we in this article first collect a new dataset with all types of clothes, i.e., tops, bottoms, and whole clothes, each one hasmultiple categories with rich information of clothing characteristics such as patterns, logos, and other details. Based on this dataset, we then propose the Arbitrary Virtual Try-On Network (AVTON) that is utilized for all-type clothes, which can synthesize realistic try-on images by preserving and trading off characteristics of the target clothes and the reference person. Our approach includes three modules: (1) Limbs Prediction Module, which is utilized for predicting the human body parts by preserving the characteristics of the reference person. This is especially good for handling cross-category try-on task (e.g., long sleeves. short sleeves or long pants. skirts), where the exposed arms or legs with the skin colors and details can be reasonably predicted; (2) Improved Geometric Matching Module, which is designed to warp clothes according to the geometry of the target person. We improve the TPS based warping method with a compactly supported radial function (Wendland's.-function); (3) Trade-Off Fusion Module, which is to tradeoff the characteristics of the warped clothes and the reference person. This module is to make the generated try-on images look more natural and realistic based on a fine-tune symmetry of the network structure. Extensive simulations are conducted and our approach can achieve better performance compared with the state-of-the-art virtual try-on methods.
C1 [Liu, Yu; Zhao, Mingbo] Donghua Univ, Shanghai, Peoples R China.
   [Zhang, Zhao] Hefei Univ Technol, Hefei, Peoples R China.
   [Liu, Yuping] Fudan Univ, Shanghai, Peoples R China.
   [Yan, Shuicheng] Natl Univ Singapore, Singapore, Singapore.
C3 Donghua University; Hefei University of Technology; Fudan University;
   National University of Singapore
RP Liu, Y; Zhao, MB (corresponding author), Donghua Univ, Shanghai, Peoples R China.; Zhang, Z (corresponding author), Hefei Univ Technol, Hefei, Peoples R China.; Liu, YP (corresponding author), Fudan Univ, Shanghai, Peoples R China.
EM 2191408@mail.dhu.edu.cn; mzhao4@dhu.edu.cn; cszzhang@gmail.com;
   liuyp@fudan.edu.cn
RI Yan, Shuicheng/HCI-1431-2022; liu, yu/HRB-3273-2023; Zhang,
   Zhao/B-5136-2010
OI liu, yu/0000-0003-1304-0872; Zhang, Zhao/0000-0002-5703-7969
FU Graduate Student Innovation Fund of Donghua University
   [GSIF-DH-M-2021006]; National Natural Science Foundation of China
   [61971121, 62072151]; Anhui Provincial Natural Science Fund for the
   Distinguished Young Scholars [2008085J30]; Open Foundation of Yunnan Key
   Laboratory of Software Engineering [2023SE103]; CAAI-Huawei MindSpore
   Open Fund
FX This work is supported by Graduate Student Innovation Fund of Donghua
   University (GSIF-DH-M-2021006), supported by National Natural Science
   Foundation of China (61971121, 62072151), partially supported by Anhui
   Provincial Natural Science Fund for the Distinguished Young Scholars
   (2008085J30), Open Foundation of Yunnan Key Laboratory of Software
   Engineering (2023SE103), CCF-Baidu Open Fund and CAAI-Huawei MindSpore
   Open Fund.
CR Alldieck T, 2018, PROC CVPR IEEE, P8387, DOI 10.1109/CVPR.2018.00875
   Cui ZY, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P307, DOI 10.1145/3308558.3313444
   Delaunay B., 1934, Bulletin of Academy of Sciences of the USSR, V6, P793
   Dong HY, 2019, Arxiv, DOI arXiv:1906.00884
   Duchon J., 1977, Lecture notes in Mathematics, P85
   Feng ZL, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3326332
   Fornefett M., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P402, DOI 10.1109/CVPR.1999.786970
   Fornefett M, 2001, IMAGE VISION COMPUT, V19, P87, DOI 10.1016/S0262-8856(00)00057-3
   Ge YY, 2019, PROC CVPR IEEE, P5332, DOI 10.1109/CVPR.2019.00548
   Gong K, 2017, PROC CVPR IEEE, P6757, DOI 10.1109/CVPR.2017.715
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Guan P, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185531
   Güler RA, 2018, PROC CVPR IEEE, P7297, DOI 10.1109/CVPR.2018.00762
   Han XT, 2019, IEEE I CONF COMP VIS, P4480, DOI 10.1109/ICCV.2019.00458
   Han XT, 2018, PROC CVPR IEEE, P7543, DOI 10.1109/CVPR.2018.00787
   Han XT, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1078, DOI 10.1145/3123266.3123394
   Han Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7847, DOI 10.1109/CVPR42600.2020.00787
   Häubl G, 2000, MARKET SCI, V19, P4, DOI 10.1287/mksc.19.1.4.15178
   Hauswiesner S, 2013, IEEE T VIS COMPUT GR, V19, P1552, DOI 10.1109/TVCG.2013.67
   He RN, 2016, IEEE DATA MINING, P937, DOI [10.1109/ICDM.2016.65, 10.1109/ICDM.2016.0116]
   Hosseinzadeh M, 2020, PROC CVPR IEEE, P3593, DOI 10.1109/CVPR42600.2020.00365
   Hsiao WL, 2019, IEEE I CONF COMP VIS, P5046, DOI 10.1109/ICCV.2019.00515
   Hu BW, 2022, IEEE T MULTIMEDIA, V24, P1233, DOI 10.1109/TMM.2022.3143712
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Iwata T., 2011, P 22 INT JOINT C ART
   Jandial S, 2021, Arxiv, DOI arXiv:2009.01485
   Karras T, 2018, Arxiv, DOI [arXiv:1710.10196, DOI 10.48550/ARXIV.1710.10196]
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Li JS, 2018, Arxiv, DOI arXiv:1705.07206
   Li PK, 2022, IEEE T PATTERN ANAL, V44, P3260, DOI 10.1109/TPAMI.2020.3048039
   Li XR, 2021, PATTERN RECOGN LETT, V141, P68, DOI 10.1016/j.patrec.2020.12.001
   Li YC, 2017, IEEE T MULTIMEDIA, V19, P1946, DOI 10.1109/TMM.2017.2690144
   Liang XD, 2015, IEEE T PATTERN ANAL, V37, P2402, DOI 10.1109/TPAMI.2015.2408360
   Lin YL, 2020, PROC CVPR IEEE, P3308, DOI 10.1109/CVPR42600.2020.00337
   Liu JY, 2019, LECT NOTES COMPUT SC, V11131, P30, DOI 10.1007/978-3-030-11015-4_4
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Liu ZW, 2016, LECT NOTES COMPUT SC, V9906, P229, DOI 10.1007/978-3-319-46475-6_15
   Mir A, 2020, PROC CVPR IEEE, P7021, DOI 10.1109/CVPR42600.2020.00705
   Mo S., 2018, arXiv, DOI 10.48550/arxiv.1812.10889
   Neuberger A, 2020, PROC CVPR IEEE, P5183, DOI 10.1109/CVPR42600.2020.00523
   Pandey N, 2020, NEUROCOMPUTING, V414, P356, DOI 10.1016/j.neucom.2020.07.092
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Pons-Moll G, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073711
   Raj A, 2018, LECT NOTES COMPUT SC, V11216, P679, DOI 10.1007/978-3-030-01258-8_41
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Salimans T, 2016, ADV NEUR IN, V29
   Shih YS, 2018, AAAI CONF ARTIF INTE, P2403
   Veit A, 2015, IEEE I CONF COMP VIS, P4642, DOI 10.1109/ICCV.2015.527
   Wang BC, 2018, LECT NOTES COMPUT SC, V11217, P607, DOI 10.1007/978-3-030-01261-8_36
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang WG, 2019, IEEE I CONF COMP VIS, P5702, DOI 10.1109/ICCV.2019.00580
   Wang WG, 2018, PROC CVPR IEEE, P4271, DOI 10.1109/CVPR.2018.00449
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wendland H., 1995, Advances in Computational Mathematics, V4, P389, DOI 10.1007/BF02123482
   Wenguan Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8926, DOI 10.1109/CVPR42600.2020.00895
   Wu H, 2021, PROC CVPR IEEE, P11302, DOI 10.1109/CVPR46437.2021.01115
   Yang X, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3425636
   Yu RY, 2019, IEEE I CONF COMP VIS, P10510, DOI 10.1109/ICCV.2019.01061
   Zeng W, 2020, NEURAL COMPUT APPL, V32, P17587, DOI 10.1007/s00521-020-04928-1
   Zhang FF, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3478642
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang XL, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3519029
   Zhao MB, 2020, IEEE MULTIMEDIA, V27, P122, DOI 10.1109/MMUL.2020.3024221
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu SZ, 2017, IEEE I CONF COMP VIS, P1689, DOI 10.1109/ICCV.2017.186
NR 65
TC 0
Z9 0
U1 6
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2024
VL 20
IS 5
AR 123
DI 10.1145/3636426
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MF3R9
UT WOS:001192177900003
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Ma, ZY
   Wang, SW
   Luo, X
   Gu, ZH
   Chen, C
   Li, JX
   Hua, XS
   Lu, GM
AF Ma, Zeyu
   Wang, Siwei
   Luo, Xiao
   Gu, Zhonghui
   Chen, Chong
   Li, Jinxing
   Hua, Xian-Sheng
   Lu, Guangming
TI HARR: Learning Discriminative and High-Quality Hash Codes for Image
   Retrieval
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Deep unsupervised hashing; similarity learning; large-scale image
   retrieval
ID OF-ARRIVAL; DEEP
AB This article studies deep unsupervised hashing, which has attracted increasing attention in large-scale image retrieval. The majority of recent approaches usually reconstruct semantic similarity information, which then guides the hash code learning. However, they still fail to achieve satisfactory performance in reality for two reasons. On the one hand, without accurate supervised information, these methods usually fail to produce independent and robust hash codes with semantics information well preserved, which may hinder effective image retrieval. On the other hand, due to discrete constraints, how to effectively optimize the hashing network in an end-to-end manner with small quantization errors remains a problem. To address these difficulties, we propose a novel unsupervised hashing method called HARR to learn discriminative and high-quality hash codes. To comprehensively explore semantic similarity structure, HARR adopts the Winner-Take-All hash to model the similarity structure. Then similarity-preserving hash codes are learned under the reliable guidance of the reconstructed similarity structure. Additionally, we improve the quality of hash codes by a bit correlation reduction module, which forces the cross-correlation matrix between a batch of hash codes under different augmentations to approach the identity matrix. In this way, the generated hash bits are expected to be invariant to disturbances with minimal redundancy, which can be further interpreted as an instantiation of the information bottleneck principle. Finally, for effective hashing network training, we minimize the cosine distances between real-value network outputs and their binary codes for small quantization errors. Extensive experiments demonstrate the effectiveness of our proposed HARR.
C1 [Ma, Zeyu; Li, Jinxing; Lu, Guangming] Harbin Inst Technol, Sch Comp Sci & Technol, Shenzhen, Peoples R China.
   [Wang, Siwei] Hunan Univ, Coll Finance & Stat, Changsha, Peoples R China.
   [Luo, Xiao] Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90095 USA.
   [Gu, Zhonghui] Peking Univ, Peking Tsinghua Ctr Life Sci, Acad Adv Interdisciplinary Studies, Beijing, Peoples R China.
   [Chen, Chong; Hua, Xian-Sheng] Terminus Grp, Beijing 100027, Peoples R China.
C3 Harbin Institute of Technology; Hunan University; University of
   California System; University of California Los Angeles; Peking
   University
RP Lu, GM (corresponding author), Harbin Inst Technol, Sch Comp Sci & Technol, Shenzhen, Peoples R China.; Luo, X (corresponding author), Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90095 USA.
EM zeyu.ma@stu.hit.edu.cn; siweiwang@hnu.edu.cn; xiaoluo@cs.ucla.edu;
   guzhh20@stu.pku.edu.cn; chenchong.cz@gmail.com; lijinxing158@hit.edu.cn;
   huaxiansheng@gmail.com; luguangm@hit.edu.cn
RI Luo, Xiao/IYS-9183-2023
OI Luo, Xiao/0000-0002-7987-3714; Hua, Xian-Sheng/0000-0002-8232-5049; Ma,
   Zeyu/0000-0002-2553-0679; Lu, Guangming/0000-0003-1578-2634; Gu,
   Zhonghui/0000-0001-6142-4358
FU NSFC fund [62176077]; Shenzhen Key Technical Project [2022N001,
   2020N046]; Guangdong International Science and Technology Cooperation
   Project [20220505]; Shenzhen Fundamental Research Fund
   [JCYJ20210324132210025]; Guangdong Provincial Key Laboratory of Novel
   Security Intelligence Technologies [2022B1212010005]; National Natural
   Science Foundation of China [62272133, 61906162]; Shenzhen Colleges and
   Universities Stable Support Program [GXWD20220811170100001]; Shenzhen
   Science and Technology Program [RCBS20200714114910193]
FX This work was supported in part by the NSFC fund (62176077), in part by
   the Shenzhen Key Technical Project (2022N001 and 2020N046), in part by
   the Guangdong International Science and Technology Cooperation Project
   (20220505), in part by the Shenzhen Fundamental Research Fund
   (JCYJ20210324132210025), in part by the Guangdong Provincial Key
   Laboratory of Novel Security Intelligence Technologies
   (2022B1212010005), in part by the National Natural Science Foundation of
   China (grants 62272133 and 61906162), in part by the Shenzhen Colleges
   and Universities Stable Support Program (GXWD20220811170100001), and in
   part by the Shenzhen Science and Technology Program
   (RCBS20200714114910193).
CR Amjad RA, 2020, IEEE T PATTERN ANAL, V42, P2225, DOI 10.1109/TPAMI.2019.2909031
   Bai JL, 2020, IEEE T MULTIMEDIA, V22, P215, DOI 10.1109/TMM.2019.2922130
   Cai TT, 2010, ANN STAT, V38, P2118, DOI 10.1214/09-AOS752
   Cao ZJ, 2017, IEEE I CONF COMP VIS, P5609, DOI 10.1109/ICCV.2017.598
   Caron Mathilde, 2020, P C NEURAL INFORM PR
   Chen H, 2021, PROC CVPR IEEE, P2004, DOI 10.1109/CVPR46437.2021.00204
   Chen ZD, 2022, IEEE T PATTERN ANAL, V44, P13, DOI 10.1109/TPAMI.2020.3010201
   Chua TS, 2009, P ACM INT C IM VID R, P1
   Dai B, 2017, PR MACH LEARN RES, V70
   Deng C, 2020, IEEE T NEUR NET LEAR, V31, P2189, DOI 10.1109/TNNLS.2019.2929068
   Dizaji KG, 2018, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR.2018.00386
   Dong X, 2021, IEEE T CIRC SYST VID, V31, P3266, DOI 10.1109/TCSVT.2020.3035775
   Dubey SR, 2022, IEEE T CIRC SYST VID, V32, P2687, DOI 10.1109/TCSVT.2021.3080920
   Fan JQ, 2008, J ECONOMETRICS, V147, P186, DOI 10.1016/j.jeconom.2008.09.017
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Grill Jean-Bastien, 2021, P INT C LEARNING REP
   Gu HF, 2008, SIGNAL PROCESS, V88, P75, DOI 10.1016/j.sigpro.2007.07.013
   Gu YF, 2019, NEUROCOMPUTING, V368, P114, DOI 10.1016/j.neucom.2019.08.050
   Hadsell Raia., 2006, P IEEECVF C COMPUTER
   He H., 2020, P IEEE CVF C COMP VI, P9729
   Hoe JT, 2021, ADV NEUR IN, V34
   Hu QJ, 2021, PROC CVPR IEEE, P1074, DOI 10.1109/CVPR46437.2021.00113
   Huiskes Mark J., 2008, P 1 ACM INT C MULTIM
   Jang YK, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12065, DOI 10.1109/ICCV48922.2021.01187
   Jia XH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P590, DOI 10.1109/ICCV48922.2021.00065
   Khosla P., 2020, Adv. Neural Inf. Process. Syst, P18661, DOI DOI 10.48550/ARXIV.2004.11362
   Kikuchi S, 2006, IEEE ANTENN WIREL PR, V5, P35, DOI 10.1109/LAWP.2005.863610
   Kilimci Perihan, 2011, P INT C INFORM SOC
   Kralevska K, 2018, IEEE T BIG DATA, V4, P516, DOI 10.1109/TBDATA.2017.2749255
   Krizhevsky A, 2009, CIFAR-10 dataset
   Kulis B, 2009, IEEE I CONF COMP VIS, P2130, DOI 10.1109/ICCV.2009.5459466
   Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947
   Lawder JK, 2001, SIGMOD RECORD, V30, P19, DOI 10.1145/373626.373678
   Li JJ, 2023, IEEE T NEUR NET LEAR, V34, P623, DOI 10.1109/TNNLS.2021.3098767
   Li Wu-Jun, 2016, P AAAI C ARTIFICIAL
   Lin KV, 2016, PROC CVPR IEEE, P1183, DOI 10.1109/CVPR.2016.133
   Lin QH, 2022, AAAI CONF ARTIF INTE, P7488
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liong VE, 2015, PROC CVPR IEEE, P2475, DOI 10.1109/CVPR.2015.7298862
   Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227
   Luo X, 2023, ACM T KNOWL DISCOV D, V17, DOI 10.1145/3532624
   Luo X, 2022, IEEE SIGNAL PROC LET, V29, P602, DOI 10.1109/LSP.2022.3148674
   Luo Xiao, 2021, P IEEE INT C MULTIME
   Ma Zeyu, 2022, P 31 INT JOINT C ART, P1254
   ROCKAFELLAR RT, 1993, SIAM REV, V35, P183, DOI 10.1137/1035044
   RukaiWei Yu Liu, 2023, Pattern Recognition, V139
   Shan YH, 2020, INT J MACH LEARN CYB, V11, P1825, DOI 10.1007/s13042-020-01074-x
   Shen FM, 2018, IEEE T PATTERN ANAL, V40, P3034, DOI 10.1109/TPAMI.2018.2789887
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Shen Fumin, 2017, P ACM INT C MULTIMED
   Shen XB, 2022, IEEE T MULTIMEDIA, V24, P1116, DOI 10.1109/TMM.2021.3119868
   Shen YM, 2020, PROC CVPR IEEE, P2815, DOI 10.1109/CVPR42600.2020.00289
   Shu Zhang, 2015, IEEE Transactions on Big Data, V1, P84, DOI 10.1109/TBDATA.2015.2499191
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Su HB, 2020, Arxiv, DOI arXiv:2010.13357
   Do TT, 2020, IEEE T MULTIMEDIA, V22, P992, DOI 10.1109/TMM.2019.2935680
   Do TT, 2016, LECT NOTES COMPUT SC, V9909, P219, DOI 10.1007/978-3-319-46454-1_14
   Tishby N., 2000, arXiv, DOI DOI 10.48550/ARXIV.PHYSICS/0004057
   Tu RC, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3466
   Tu Rong-Cheng, 2023, IEEE Transactions on Multimedia
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang D, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P1409, DOI 10.1145/3397271.3401132
   Wang JD, 2018, IEEE T PATTERN ANAL, V40, P769, DOI 10.1109/TPAMI.2017.2699960
   Wang YB, 2021, IEEE T CIRC SYST VID, V31, P387, DOI 10.1109/TCSVT.2020.2974768
   Wang ZW, 2023, IEEE T PATTERN ANAL, V45, P1919, DOI 10.1109/TPAMI.2022.3161600
   Weiss Yair, 2009, P C NEURAL INFORM PR
   Wu DY, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3524021
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Xi Z, 2023, IEEE T CIRC SYST VID, V33, P3529, DOI 10.1109/TCSVT.2023.3234037
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2156
   XiaoqinWang Chen Chen, 2022, ACM Transactions on Multimedia Computing, Communications and Applications, V18, P1
   Xie EZ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8372, DOI 10.1109/ICCV48922.2021.00828
   Yagnik J, 2011, IEEE I CONF COMP VIS, P2431, DOI 10.1109/ICCV.2011.6126527
   Yang EK, 2019, PROC CVPR IEEE, P2941, DOI 10.1109/CVPR.2019.00306
   Yang EK, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1064
   Yang Yang, 2015, IEEE Transactions on Big Data, V1, P162, DOI 10.1109/TBDATA.2016.2516024
   Yu JG, 2022, Arxiv, DOI arXiv:2201.13322
   Zbontar J, 2021, PR MACH LEARN RES, V139
   Zeyu Ma, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P659, DOI 10.1145/3503161.3548403
   Zhai HJ, 2021, IEEE T CIRC SYST VID, V31, P742, DOI 10.1109/TCSVT.2020.2991171
   Zhao XY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10603, DOI 10.1109/ICCV48922.2021.01045
   Zhou W, 2021, IEEE T BIG DATA, V7, P759, DOI 10.1109/TBDATA.2017.2736547
NR 82
TC 0
Z9 0
U1 3
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2024
VL 20
IS 5
AR 134
DI 10.1145/3627162
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MF3R9
UT WOS:001192177900014
OA Bronze
DA 2024-08-05
ER

PT J
AU Li, MY
   Zhou, T
   Huang, Z
   Yang, J
   Yang, J
   Gong, C
AF Li, Mingyu
   Zhou, Tao
   Huang, Zhuo
   Yang, Jian
   Yang, Jie
   Gong, Chen
TI Dynamic Weighted Adversarial Learning for Semi-Supervised Classification
   under Intersectional Class Mismatch
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Semi-supervised learning; intersectional class mismatch; adversarial
   domain adaptation; dissimilarity maximization
AB Nowadays, class-mismatch problem has drawn intensive attention in Semi-Supervised Learning (SSL), where the classes of labeled data are assumed to be only a subset of the classes of unlabeled data. However, in a more realistic scenario, the labeled data and unlabeled data often share some common classes while they also have their individual classes, which leads to an "intersectional class-mismatch" problem. As a result, existing SSL methods are often confused by these individual classes and suffer from performance degradation. To address this problem, we propose a novel Dynamic Weighted Adversarial Learning (DWAL) framework to properly utilize unlabeled data for boosting the SSL performance. Specifically, to handle the influence of the individual classes in unlabeled data (i.e., Out-Of-Distribution classes), we propose an enhanced adversarial domain adaptation to dynamically assign weight for each unlabeled example from the perspectives of domain adaptation and a class-wise weighting mechanism, which consists of transferability score and prediction confidence value. Besides, to handle the influence of the individual classes in labeled data (i.e., private classes), we propose a dissimilarity maximization strategy to suppress the inaccurate correlations caused by the examples of individual classes within labeled data. Therefore, our DWAL can properly make use of unlabeled data to acquire an accurate SSL classifier under intersectional class-mismatch setting, and extensive experimental results on five public datasets demonstrate the effectiveness of the proposed model over other state-of-the-art SSL methods.
C1 [Li, Mingyu; Zhou, Tao; Yang, Jian; Gong, Chen] Nanjing Univ Sci & Technol, Xiaolingwei St 200, Nanjing 210094, Peoples R China.
   [Huang, Zhuo] Univ Sydney, Camperdown, NSW 2006, Australia.
   [Yang, Jie] Shanghai Jiao Tong Univ, Dongchuan Rd 800, Shanghai 200240, Peoples R China.
C3 Nanjing University of Science & Technology; University of Sydney;
   Shanghai Jiao Tong University
RP Zhou, T; Gong, C (corresponding author), Nanjing Univ Sci & Technol, Xiaolingwei St 200, Nanjing 210094, Peoples R China.
EM mingyuli.ai@gmail.com; taozhou.dreams@gmail.com;
   zhua0420@uni.sydney.edu.au; csjyang@njust.edu.cn; jieyang@sjtu.edu.cn;
   chen.gong@njust.edu.cn
RI Li, Ming-Yu/AAH-6879-2020
OI Li, Ming-Yu/0000-0003-4812-8604; Li, my/0000-0002-1375-1992; Gong,
   Chen/0000-0002-1154-6194; Zhou, Tao/0000-0002-3733-7286
FU NSF of China [62336003, 12371510, 62172228, 62376153]; NSF of Jiangsu
   Province [BZ2021013]; NSF for Distinguished Young Scholar of Jiangsu
   Province [BK20220080]; Fundamental Research Funds for the Central
   Universities [30920032202, 30921013114]; "111" Program [B13022];
   CAAI-Huawei MindSpore Open Fund
FX This research is supported by NSF of China (Nos: 62336003, 12371510,
   62172228, 62376153), NSF of Jiangsu Province (No: BZ2021013), NSF for
   Distinguished Young Scholar of Jiangsu Province (No: BK20220080), the
   Fundamental Research Funds for the Central Universities (Nos:
   30920032202, 30921013114), CAAI-Huawei MindSpore Open Fund, and "111"
   Program (No: B13022).
CR Bengio J., 2009, Proceedings of the 26th Annual International Conference on Machine Learning, P41
   Berthelot David, 2019, P INT C LEARNING REP
   Cao Kaidi, 2022, P INT C LEARNING REP
   Cao ZJ, 2018, LECT NOTES COMPUT SC, V11212, P139, DOI 10.1007/978-3-030-01237-3_9
   Cao ZJ, 2019, PROC CVPR IEEE, P2980, DOI 10.1109/CVPR.2019.00310
   Cascante-Bonilla P, 2021, AAAI CONF ARTIF INTE, V35, P6912
   Chapelle O., 2009, SEMISUPERVISED LEARN, V20, P542, DOI 10.1109/TNN.2009.2015974
   Chen YB, 2020, AAAI CONF ARTIF INTE, V34, P3569
   Chrabaszcz P, 2017, Arxiv, DOI arXiv:1707.08819
   Cui S., 2020, Advances in Neural Information Processing Systems, V33, P7571
   Cui SH, 2020, PROC CVPR IEEE, P3940, DOI 10.1109/CVPR42600.2020.00400
   Dai Zihang, 2017, P INT C NEURAL INFOR, V30
   Dong JH, 2019, ADV NEUR IN, V32
   Fu SC, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3412846
   Ganin Y, 2016, J MACH LEARN RES, V17
   Gong C, 2016, IEEE T IMAGE PROCESS, V25, P3249, DOI 10.1109/TIP.2016.2563981
   Gong C, 2015, IEEE T NEUR NET LEAR, V26, P2148, DOI 10.1109/TNNLS.2014.2376963
   Gong CY, 2021, PROC CVPR IEEE, P13678, DOI 10.1109/CVPR46437.2021.01347
   Goodfellow Ian, 2014, P 27 INT C NEURAL IN, V27
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Goodfellow IJ, 2015, P 3 INT C LEARNING R
   Grandvalet Y., 2005, Advances in Neural Information Processing Systems, V367, P281
   Guo LZ, 2020, PR MACH LEARN RES, V119
   Guo Lan-Zhe, 2022, ADV NEURAL INFORM PR
   Huang J., 2021, P IEEE INT C COMP VI, P8310
   Huang Z, 2023, IEEE T MULTIMEDIA, V25, P1844, DOI 10.1109/TMM.2022.3179895
   Huang Zhuo, 2023, P C NEURAL INFORM PR
   Huang Zhuo, 2023, P INT C LEARNING REP
   Joachims T, 1999, MACHINE LEARNING, PROCEEDINGS, P200
   Joachims T., 2003, P 20 INT C INT C MAC, P290, DOI DOI 10.1145/2612669.2612699
   Ke ZH, 2019, IEEE I CONF COMP VIS, P6727, DOI 10.1109/ICCV.2019.00683
   Khosla P., 2020, Adv. Neural Inf. Process. Syst, P18661, DOI DOI 10.48550/ARXIV.2004.11362
   Koltchinskii V, 2002, ANN STAT, V30, P1
   Kumar A, 2017, ADV NEUR IN, V30
   Laine S., 2016, P INT C LEARNING REP
   Lee D.H., 2013, WORKSH CHALL REPR LE, P07
   Li CX, 2017, ADV NEUR IN, V30
   Li Y, 2021, IEEE T MULTIMEDIA, V23, P1354, DOI 10.1109/TMM.2020.2997185
   Long MS, 2018, ADV NEUR IN, V31
   Miyato T, 2019, IEEE T PATTERN ANAL, V41, P1979, DOI 10.1109/TPAMI.2018.2858821
   Mowery Bernice D, 2011, Pediatr Nurs, V37, P320
   Nassar I, 2021, PROC CVPR IEEE, P7237, DOI 10.1109/CVPR46437.2021.00716
   Oh Y, 2022, PROC CVPR IEEE, P9776, DOI 10.1109/CVPR52688.2022.00956
   Oliver A., 2018, Advances in Neural Information Processing Systems, P3239
   Park S, 2018, AAAI CONF ARTIF INTE, P3917
   Qing Yu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P438, DOI 10.1007/978-3-030-58610-2_26
   Qizhe Xie, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10684, DOI 10.1109/CVPR42600.2020.01070
   Rizve M., 2021, P INT C LEARNING REP
   Saito D., 2021, Advances in Neural Information Processing Systems, P25956
   Sajjadi M, 2016, ADV NEUR IN, V29
   Sohn K., 2020, Advances in Neural Information Processing Systems, P596
   Suzuki T, 2020, AAAI CONF ARTIF INTE, V34, P5916
   Tarvainen Antti, 2017, P INT C LEARNING REP
   Wang WJ, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3572916
   Wang Y, 2023, PROC CVPR IEEE, P23849, DOI 10.1109/CVPR52729.2023.02284
   Wang YY, 2012, IEEE T NEUR NET LEAR, V23, P689, DOI 10.1109/TNNLS.2012.2186825
   Wu Xiaofu, 2020, Pattern Recognition, V121
   Xia YD, 2020, IEEE WINT CONF APPL, P3635, DOI [10.1109/wacv45572.2020.9093608, 10.1109/WACV45572.2020.9093608]
   Xiao JY, 2022, PROC CVPR IEEE, P11194, DOI 10.1109/CVPR52688.2022.01092
   Yang F., 2022, P IEEECVF C COMPUTER, P14421
   You KC, 2019, PROC CVPR IEEE, P2715, DOI 10.1109/CVPR.2019.00283
   Zhang Bowen, 2021, Proc. Adv. Neural Inf. Process. Syst., V34, P18408
   Zhang H., 2017, P INT C LEARNING REP
   Zhang YC, 2019, PR MACH LEARN RES, V97
   Zhang YH, 2023, IEEE T MULTIMEDIA, V25, P1749, DOI 10.1109/TMM.2022.3158069
   Zhao J, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3506711
   Zhou ZH, 2005, IEEE T KNOWL DATA EN, V17, P1529, DOI 10.1109/TKDE.2005.186
   Zhu BW, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5281, DOI 10.1145/3474085.3475649
   Zhu H, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P4456, DOI 10.1145/3503161.3548026
NR 69
TC 0
Z9 0
U1 6
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2024
VL 20
IS 4
AR 115
DI 10.1145/3635310
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6G9
UT WOS:001153382100025
DA 2024-08-05
ER

PT J
AU Khaleel, M
   Idris, A
   Tavanapong, W
   Pratt, JR
   Oh, J
   De Groen, PC
AF Khaleel, Mohammed
   Idris, Azeez
   Tavanapong, Wallapak
   Pratt, Jacob R.
   Oh, Junghwan
   De Groen, Piet C.
TI VisActive: Visual-concept-based Active Learning for Image Classification
   under Class Imbalance
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Deep active learning; deep neural network; class imbalance
AB Active learning methods recommend the most informative images from a large unlabeled dataset for manual labeling. These methods improve the performance of an image classifier while minimizing manual labeling efforts. We propose VisActive, a visual-concept-based active learning method for image classification under class imbalance. VisActive learns a visual concept, a generalized representation that holds the most important image characteristics for class prediction, and then recommends for each class four sets of unlabeled images with different visual concepts to increase the diversity and enlarge the training dataset. Experimental results on four datasets show that VisActive outperforms the state-of-the-art deep active learning methods.
C1 [Khaleel, Mohammed; Idris, Azeez; Tavanapong, Wallapak; Pratt, Jacob R.] Iowa State Univ, Dept Comp Sci, Ames, IA 50011 USA.
   [Oh, Junghwan] Univ North Texas, Dept Comp Sci & Engn, Denton, TX 76203 USA.
   [De Groen, Piet C.] Univ Minnesota, Dept Med, Minneapolis, MN 55455 USA.
C3 Iowa State University; University of North Texas System; University of
   North Texas Denton; University of Minnesota System; University of
   Minnesota Twin Cities
RP Khaleel, M (corresponding author), Iowa State Univ, Dept Comp Sci, Ames, IA 50011 USA.
EM mkhaleel@iastate.edu; aidris@iastate.edu; tavanapo@iastate.edu;
   jrpratt@iastate.edu; junghwan.oh@unt.edu; degroen@umn.edu
OI Pratt, Jacob/0000-0003-2415-0882
FU NIH [1R01DK106130-01A1]
FX This work is partially supported by NIH Grant No. 1R01DK106130-01A1.
CR Abadi M., 2016, Tech. rep
   [Anonymous], 2015, Gastrointest Endoscopy, V81, P31
   Balcan MF, 2009, J COMPUT SYST SCI, V75, P78, DOI 10.1016/j.jcss.2008.07.003
   Beluch WH, 2018, PROC CVPR IEEE, P9368, DOI 10.1109/CVPR.2018.00976
   Branco P, 2016, ACM COMPUT SURV, V49, DOI 10.1145/2907070
   Chen CF, 2019, ADV NEUR IN, V32
   Choi J, 2021, PROC CVPR IEEE, P6745, DOI 10.1109/CVPR46437.2021.00668
   Cooper Gregory S, 2005, Am J Med, V118, P1413, DOI 10.1016/j.amjmed.2005.06.019
   Du XF, 2019, IEEE IMAGE PROC, P1685, DOI [10.1109/icip.2019.8803135, 10.1109/ICIP.2019.8803135]
   Farahani RZ, 2009, CONTRIB MANAG SCI, P347, DOI 10.1007/978-3-7908-2151-2_15
   Freytag A, 2014, LECT NOTES COMPUT SC, V8692, P562, DOI 10.1007/978-3-319-10593-2_37
   Gal Y, 2015, ARXIV
   Gal Y, 2017, PR MACH LEARN RES, V70
   Ganganwar V., 2012, International Journal of Emerging Technology and Advanced Engineering, V2, P42, DOI DOI 10.46338/IJETAE0412_13
   github, Azeez Idris
   Griffin G., 2007, Caltech-256 object category dataset
   Gudovskiy Denis, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9038, DOI 10.1109/CVPR42600.2020.00906
   He HB, 2009, IEEE T KNOWL DATA EN, V21, P1263, DOI 10.1109/TKDE.2008.239
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Herman B, 2017, arXiv
   Huang Y, 2020, IEEE T INTELL TRANSP, V21, P79, DOI 10.1109/TITS.2018.2888698
   Johnson JM, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0192-5
   Joshi Ajay J., 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2372, DOI 10.1109/CVPRW.2009.5206627
   Khaleel M, 2022, J BIG DATA-GER, V9, DOI 10.1186/s40537-022-00583-6
   Khaleel M, 2021, COMP MED SY, P25, DOI 10.1109/CBMS52027.2021.00012
   Kim K, 2021, PROC CVPR IEEE, P8162, DOI 10.1109/CVPR46437.2021.00807
   Lee B, 2018, LECT NOTES COMPUT SC, V11071, P841, DOI 10.1007/978-3-030-00934-2_93
   Mannor S., 2005, P 22 INT C MACH LEAR, P561
   Nam JG, 2019, RADIOLOGY, V290, P218, DOI 10.1148/radiol.2018180237
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Pogorelov K, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P164, DOI 10.1145/3083187.3083212
   Raj RJS, 2020, IEEE ACCESS, V8, P58006, DOI 10.1109/ACCESS.2020.2981337
   Ren PZ, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3472291
   Sener Ozan, 2018, P INT C LEARNING REP
   Settles B., 2012, ACTIVE LEARNING, DOI DOI 10.2200/S00429ED1V01Y201207AIM018
   Sinha S, 2019, IEEE I CONF COMP VIS, P5971, DOI 10.1109/ICCV.2019.00607
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Tan CQ, 2018, LECT NOTES COMPUT SC, V11141, P270, DOI 10.1007/978-3-030-01424-7_27
   Tavanapong Wallapak, 2021, Endoscopy Dataset
   Ting K.M., 2011, Encyclopedia of Machine Learning, DOI [DOI 10.1007/978-0-387-30164-8_652, 10.1007/978-0-387-30164-8_652., 10.1007/978-0-387-30164-8_652]
   Wang KZ, 2017, IEEE T CIRC SYST VID, V27, P2591, DOI 10.1109/TCSVT.2016.2589879
   Wang W., 2020, Deep learning in healthcare: paradigms and applications, P33, DOI [10.1007/978-3-030-32606-7_3, DOI 10.1007/978-3-030-32606-7_3]
   Weiss Karl, 2016, Journal of Big Data, V3, DOI 10.1186/s40537-016-0043-6
   Wiegreffe S, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P11
   Witten IH, 2017, DATA MINING: PRACTICAL MACHINE LEARNING TOOLS AND TECHNIQUES, 4TH EDITION, pCP1
   Yehuda Ofer, 2022, P C NEURAL INFORM PR, P1
   Yi John Seon Keun, 2022, Computer Vision (ECCV'22)
   Yin QY, 2013, MATH PROBL ENG, V2013, DOI 10.1155/2013/761814
   Yoo D, 2019, PROC CVPR IEEE, P93, DOI 10.1109/CVPR.2019.00018
   Zhang C, 2018, IEEE DATA MINING, P1422, DOI 10.1109/ICDM.2018.00196
   Zhou ZW, 2017, PROC CVPR IEEE, P4761, DOI 10.1109/CVPR.2017.506
NR 52
TC 0
Z9 0
U1 11
U2 11
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2024
VL 20
IS 3
AR 84
DI 10.1145/3617999
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6F8
UT WOS:001153381000024
OA Bronze
DA 2024-08-05
ER

PT J
AU Gu, XL
   Huang, J
   Wong, YK
   Yu, J
   Fan, JP
   Peng, P
   Kankanhalli, MS
AF Gu, Xiaoling
   Huang, Jie
   Wong, Yongkang
   Yu, Jun
   Fan, Jianping
   Peng, Pai
   Kankanhalli, Mohan S.
TI PAINT: Photo-realistic Fashion Design Synthesis
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Generative adversarial network; fashion image synthesis; AI-assisted
   fashion design
AB In this article, we investigate a newproblem of generating a variety of multi-viewfashion designs conditioned on a human pose and texture examples of arbitrary sizes, which can replace the repetitive and low-level design work for fashion designers. To solve this challenging multi-modal image translation problem, we propose a novel Photo-reAlistic fashIon desigN synThesis (PAINT) framework, which decomposes the framework into three manageable stages. In the first stage, we employ a Layout Generative Network (LGN) to transform an input human pose into a series of person semantic layouts. In the second stage, we propose a Texture Synthesis Network (TSN) to synthesize textures on all transformed semantic layouts. Specifically, we design a novel attentive texture transfer mechanism for precisely expanding texture patches to the irregular clothing regions of the target fashion designs. In the third stage, we leverage an Appearance Flow Network (AFN) to generate the fashion design images of other viewpoints from a single-view observation by learning 2D multi-scale appearance flow fields. Experimental results demonstrate that our method is capable of generating diverse photo-realistic multi-view fashion design images with fine-grained appearance details conditioned on the provided multiple inputs. The source code and trained models are available at https://github.com/gxl-groups/PAINT.
C1 [Gu, Xiaoling; Huang, Jie; Yu, Jun; Fan, Jianping] Hangzhou Dianzi Univ, Key Lab Complex SystemsModeling & Simulat, Sch Comp Sci & Technol, Hangzhou, Peoples R China.
   [Wong, Yongkang; Kankanhalli, Mohan S.] Natl Univ Singapore, Singapore, Singapore.
   [Peng, Pai] Tencent Technol Co Ltd, YoutuLab, Shenzhen, Peoples R China.
C3 Hangzhou Dianzi University; National University of Singapore; Tencent
RP Fan, JP (corresponding author), Hangzhou Dianzi Univ, Key Lab Complex SystemsModeling & Simulat, Sch Comp Sci & Technol, Hangzhou, Peoples R China.
EM guxl@hdu.edu.cn; huangjiejie@hdu.edu.cn; yongkang.wong@nus.edu.sg;
   yujun@hdu.edu.cn; fanjianping@hdu.edu.cn; popeyepeng@tencent.com;
   mohan@comp.nus.edu.sg
RI Kankanhalli, Mohan/Q-9284-2019
OI Kankanhalli, Mohan/0000-0002-4846-2015; Wong,
   Yongkang/0000-0002-1239-4428
FU Zhejiang Provincial Natural Science Foundation of China [LY21F020019];
   National Science Foundation of China [62125201, 62020106007, U21B2040,
   61802100, 61972119]; National Research Foundation, Singapore under its
   Strategic Capability Research Centres Funding Initiative
FX This work was supported by the Zhejiang Provincial Natural Science
   Foundation of China (No. LY21F020019) and the National Science
   Foundation of China under Grants 62125201, 62020106007, U21B2040,
   61802100, and 61972119. This work is partially supported by the National
   Research Foundation, Singapore under its Strategic Capability Research
   Centres Funding Initiative.
CR AlBahar B, 2019, IEEE I CONF COMP VIS, P9015, DOI 10.1109/ICCV.2019.00911
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chen LC, 2016, PROC CVPR IEEE, P3640, DOI 10.1109/CVPR.2016.396
   Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
   Dubey A, 2020, Arxiv, DOI [arXiv:2007.04950, 10.48550/arXiv.2007.04950]
   Esser P, 2018, PROC CVPR IEEE, P8857, DOI 10.1109/CVPR.2018.00923
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Gong K, 2018, LECT NOTES COMPUT SC, V11208, P805, DOI 10.1007/978-3-030-01225-0_47
   Gu XL, 2020, INFORM PROCESS MANAG, V57, DOI 10.1016/j.ipm.2020.102276
   Han XT, 2018, PROC CVPR IEEE, P7543, DOI 10.1109/CVPR.2018.00787
   Haoye Dong, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8117, DOI 10.1109/CVPR42600.2020.00814
   Hensel M, 2017, ADV NEUR IN, V30
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Hui TW, 2018, PROC CVPR IEEE, P8981, DOI 10.1109/CVPR.2018.00936
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jiang SH, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3721
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kato N, 2019, PROCEEDINGS OF THE 10TH AUGMENTED HUMAN INTERNATIONAL CONFERENCE 2019 (AH2019), DOI 10.1145/3311823.3311863
   Li YN, 2019, PROC CVPR IEEE, P3688, DOI 10.1109/CVPR.2019.00381
   Li Z, 2021, NEUROCOMPUTING, V464, P130, DOI 10.1016/j.neucom.2021.08.085
   Li ZY, 2021, IEEE T MULTIMEDIA, V23, P2694, DOI 10.1109/TMM.2020.3015015
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu GL, 2018, LECT NOTES COMPUT SC, V11215, P89, DOI 10.1007/978-3-030-01252-6_6
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Ma LQ, 2017, ADV NEUR IN, V30
   Menglin Jia, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P316, DOI 10.1007/978-3-030-58452-8_19
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Salimans T, 2016, ADV NEUR IN, V29
   Sbai O, 2019, LECT NOTES COMPUT SC, V11131, P37, DOI 10.1007/978-3-030-11015-4_5
   Siarohin A, 2018, PROC CVPR IEEE, P3408, DOI 10.1109/CVPR.2018.00359
   Tao Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9260, DOI 10.1109/CVPR42600.2020.00928
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang WG, 2022, IEEE T PATTERN ANAL, V44, P3508, DOI 10.1109/TPAMI.2021.3055780
   Wang WG, 2019, IEEE I CONF COMP VIS, P5702, DOI 10.1109/ICCV.2019.00580
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wenguan Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8926, DOI 10.1109/CVPR42600.2020.00895
   Xian WQ, 2018, PROC CVPR IEEE, P8456, DOI 10.1109/CVPR.2018.00882
   Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143
   Yamaguchi K, 2012, PROC CVPR IEEE, P3570, DOI 10.1109/CVPR.2012.6248101
   Yang YH, 2021, IEEE T IMAGE PROCESS, V30, P2798, DOI 10.1109/TIP.2021.3055062
   Yu C, 2019, IEEE I CONF COMP VIS, P9045, DOI 10.1109/ICCV.2019.00914
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Zeng YH, 2019, PROC CVPR IEEE, P1486, DOI 10.1109/CVPR.2019.00158
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhou TF, 2022, IEEE T PATTERN ANAL, V44, P2827, DOI 10.1109/TPAMI.2021.3049156
   Zhou TH, 2016, LECT NOTES COMPUT SC, V9908, P286, DOI 10.1007/978-3-319-46493-0_18
   Zhu JY, 2017, ADV NEUR IN, V30
   Zhu SZ, 2017, IEEE I CONF COMP VIS, P1689, DOI 10.1109/ICCV.2017.186
   Zhu Z, 2019, PROC CVPR IEEE, P2342, DOI 10.1109/CVPR.2019.00245
NR 51
TC 3
Z9 3
U1 13
U2 22
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2024
VL 20
IS 2
SI SI
AR 48
DI 10.1145/3545610
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W6GR4
UT WOS:001092595800018
DA 2024-08-05
ER

PT J
AU Kang, Y
   Pu, B
   Kou, YQ
   Yang, Y
   Chen, JG
   Muhammad, K
   Yang, P
   Xu, LD
   Hijji, M
AF Kang, Yan
   Pu, Bin
   Kou, Yongqi
   Yang, Yun
   Chen, Jianguo
   Muhammad, Khan
   Yang, Po
   Xu, Lida
   Hijji, Mohammad
TI A Deep Graph Network with Multiple Similarity for User Clustering in
   Human-Computer Interaction
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Attributed graph clustering; cluster-friendly features; deep graph
   embedding; self-supervision module
AB User counterparts, such as user attributes in social networks or user interests, are the keys to more natural Human-Computer Interaction (HCI). In addition, users' attributes and social structures help us understand the complex interactions in HCI. Most previous studies have been based on supervised learning to improve the performance of HCI. However, in the real world, owing to signal malfunctions in user devices, large amounts of abnormal information, unlabeled data, and unsupervised approaches (e.g., the clustering method) based on mining user attributes are particularly crucial. This paper focuses on improving the clustering performance of users' attributes in HCI and proposes a deep graph embedding network with feature and structure similarity (called DGENFS) to cluster users' attributes in HCI applications based on feature and structure similarity. The DGENFS model consists of a Feature Graph Autoencoder (FGA) module, a Structure Graph Attention Network (SGAT) module, and a Dual Self-supervision (DSS) module. First, we design an attributed graph clustering method to divide users into clusters by making full use of their attributes. To take full advantage of the information of human feature space, a k-neighbor graph is generated as a feature graph based on the similarity between human features. Then, the FGA and SGAT modules are utilized to extract the representations of human features and topological space, respectively. Next, an attention mechanism is further developed to learn the importance weights of different representations to effectively integrate human features and social structures. Finally, to learn cluster-friendly features, the DSS module unifies and integrates the features learned from the FGA and SGAT modules. DSS explores the high-confidence cluster assignment as a soft label to guide the optimization of the entire network. Extensive experiments are conducted on five real-world data sets on user attribute clustering. The experimental results demonstrate that the proposed DGENFS model achieves the most advanced performance compared with nine competitive baselines.
C1 [Kang, Yan; Kou, Yongqi; Yang, Yun] Yunnan Univ, East Outer Ring Rd, Kunming 650091, Yunnan, Peoples R China.
   [Pu, Bin] Hunan Univ, Lushan Rd S, Changsha 410082, Hunan, Peoples R China.
   [Muhammad, Khan] Sungkyunkwan Univ, Visual Analyt Knowledge Lab VIS2KNOW Lab, Dept Appl Artificial Intelligence, Sch Convergence,Coll Comp & Informat, Seoul 03063, South Korea.
   [Yang, Po] Univ Sheffield, Sheffield, S Yorkshire, England.
   [Xu, Lida] Old Dominion Univ, Norfolk, VA 23529 USA.
   [Hijji, Mohammad] Univ Tabuk, Tabuk 47711, Saudi Arabia.
C3 Yunnan University; Hunan University; Sungkyunkwan University (SKKU);
   University of Sheffield; Old Dominion University; University of Tabuk
RP Yang, Y (corresponding author), Yunnan Univ, East Outer Ring Rd, Kunming 650091, Yunnan, Peoples R China.
EM kangyan@ynu.edu.cn; pubin@hnu.edu.cn; yongqikou@163.com;
   yunyang@ynu.edu.cn; chenjg33@mail.sysu.edu.cn; khan.muhammad@ieee.org;
   po.yang@sheffield.ac.uk; LXu@odu.edu; m.hijji@ut.edu.sa
RI Muhammad, Khan/L-9059-2016; Yang, Po/C-9936-2011; Chen,
   Jianguo/AFZ-9932-2022; Hijji, Mohammad/HMP-1048-2023
OI Muhammad, Khan/0000-0003-4055-7412; Yang, Po/0000-0002-8553-7127; Hijji,
   Mohammad/0000-0001-9279-401X; Yang, Yun/0000-0002-9893-3436; Muhammad,
   Khan/0000-0002-5302-1150
CR Adeyemi IR, 2016, INT CONF USER SCI, P51, DOI 10.1109/IUSER.2016.7857933
   Andre E, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2502433
   Bo DY, 2020, WEB CONFERENCE 2020: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2020), P1400, DOI 10.1145/3366423.3380214
   Cai HY, 2018, IEEE T KNOWL DATA EN, V30, P1616, DOI 10.1109/TKDE.2018.2807452
   Cao SS, 2016, AAAI CONF ARTIF INTE, P1145
   Cao Shaosheng, 2015, P 24 ACM INT C INF K, P891
   Cao Y, 2019, Arxiv, DOI arXiv:1904.04969
   CHAPANIS A., 1965, MAN MACHINE ENG
   Chen S., 2017, P 7 ANN WORKSH AUD V, P19, DOI 10.1145/3133944.3133949
   Duan MX, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3355542
   Nguyen DS, 2020, 2020 3RD INTERNATIONAL CONFERENCE ON INFORMATION AND COMPUTER TECHNOLOGIES (ICICT 2020), P230, DOI 10.1109/ICICT50521.2020.00042
   Fan Y, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P445, DOI 10.1145/2993148.2997632
   Fortunato S, 2010, PHYS REP, V486, P75, DOI 10.1016/j.physrep.2009.11.002
   Guo XF, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1753
   Hastings MB, 2006, PHYS REV E, V74, DOI 10.1103/PhysRevE.74.035102
   He XF, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1230812.1230816
   Li S, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3131344
   Li XL, 2020, IEEE WIREL COMMUN, V27, P116, DOI 10.1109/MWC.001.2000076
   Lv Z, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2645860
   Mencarini E, 2019, IEEE T HUM-MACH SYST, V49, P314, DOI 10.1109/THMS.2019.2919702
   Miandashti FJ, 2020, IEEE T HUM-MACH SYST, V50, P573, DOI 10.1109/THMS.2020.3017784
   Kipf TN, 2016, Arxiv, DOI arXiv:1611.07308
   Kipf TN, 2017, Arxiv, DOI arXiv:1609.02907
   Ng AY, 2002, ADV NEUR IN, V14, P849
   Nikolentzos G, 2017, AAAI CONF ARTIF INTE, P2429
   Norman D.A., 1986, USER CTR SYSTEM DESI
   Nylander Stina, 2014, HCI and sports, V2014, P115
   Pantic Maja, 2008, International Journal of Automomous and Adaptive Communications Systems, V1, P168, DOI 10.1504/IJAACS.2008.019799
   Peng X, 2018, IEEE T IMAGE PROCESS, V27, P5076, DOI 10.1109/TIP.2018.2848470
   Perozzi B, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P701, DOI 10.1145/2623330.2623732
   Picard R.W., 1997, Studies, V136, DOI DOI 10.1007/BF01238028
   Pimenta A, 2016, NEUROCOMPUTING, V172, P413, DOI 10.1016/j.neucom.2015.03.105
   Sang JT, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2502436
   Semsar A, 2017, MULTIMEDIA SYST, V23, P583, DOI 10.1007/s00530-016-0519-4
   Shen W, 2015, PROCEEDINGS OF 2015 4TH INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND NETWORK TECHNOLOGY (ICCSNT 2015), P374, DOI 10.1109/ICCSNT.2015.7490772
   Silva JM, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2457450.2457451
   Tang J, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P1067, DOI 10.1145/2736277.2741093
   Tian F, 2014, AAAI CONF ARTIF INTE, P1293
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Villanueva A, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2501643.2501647
   Wang C, 2019, Arxiv, DOI arXiv:1906.06532
   Wang C, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P889, DOI 10.1145/3132847.3132967
   Wang DX, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1225, DOI 10.1145/2939672.2939753
   Wang J, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1314303.1314306
   Wang X, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1243, DOI 10.1145/3394486.3403177
   Wu ZH, 2021, IEEE T NEUR NET LEAR, V32, P4, DOI 10.1109/TNNLS.2020.2978386
   Xie JY, 2016, PR MACH LEARN RES, V48
   Xu KYL, 2018, PR MACH LEARN RES, V80
   Xu Keyulu, 2018, PREPRINT, DOI DOI 10.48550/ARXIV.1810.00826
   Yang Y, 2019, IEEE T CYBERNETICS, V49, P1657, DOI 10.1109/TCYB.2018.2809562
   Yang Y, 2018, PATTERN RECOGN, V76, P391, DOI 10.1016/j.patcog.2017.11.022
   Yang Y, 2016, IEEE T NEUR NET LEAR, V27, P952, DOI 10.1109/TNNLS.2015.2430821
   Zhang Shengping, 2020, Introduction to the Special Issue on Multimodal Machine Learning for Human Behavior Analysis
   Zhang XT, 2021, NEURAL NETWORKS, V142, P388, DOI 10.1016/j.neunet.2021.05.026
   Zhang Z, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3155
   Zhou J, 2020, AI OPEN, V1, P57, DOI 10.1016/j.aiopen.2021.01.001
NR 56
TC 2
Z9 2
U1 3
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2024
VL 20
IS 2
SI SI
AR 46
DI 10.1145/3549954
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W6GR4
UT WOS:001092595800016
OA Green Accepted
DA 2024-08-05
ER

PT J
AU Weng, ZJ
   Wu, ZX
   Li, HD
   Chen, JJ
   Jiang, YG
AF Weng, Zejia
   Wu, Zuxuan
   Li, Hengduo
   Chen, Jingjing
   Jiang, Yu-Gang
TI HCMS: Hierarchical and Conditional Modality Selection for Efficient
   Video Recognition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Multimodal analysis; video recognition; efficient inference
AB Videos are multimodal in nature. Conventional video recognition pipelines typically fuse multimodal features for improved performance. However, this is not only computationally expensive but also neglects the fact that different videos rely on different modalities for predictions. This article introduces Hierarchical and Conditional Modality Selection (HCMS), a simple yet efficient multimodal learning framework for efficient video recognition. HCMS operates on a low-cost modality, i.e., audio clues, by default, and dynamically decides on-the-fly whether to use computationally expensive modalities, including appearance and motion clues, on a per-input basis. This is achieved by the collaboration of three LSTMs that are organized in a hierarchical manner. In particular, LSTMs that operate on high-cost modalities contain a gating module, which takes as inputs lower-level features and historical information to adaptively determine whether to activate its corresponding modality; otherwise, it simply reuses historical information. We conduct extensive experiments on two large-scale video benchmarks, FCVID and ActivityNet, and the results demonstrate the proposed approach can effectively explore multimodal information for improved classification performance while requiring much less computation.
C1 [Weng, Zejia; Wu, Zuxuan; Chen, Jingjing; Jiang, Yu-Gang] Fudan Univ, Shanghai Key Lab Intelligent Info Proc, Sch CS, Shanghai, Peoples R China.
   [Li, Hengduo] Univ Maryland, Dept Comp Sci, College Pk, MD USA.
C3 Fudan University; University System of Maryland; University of Maryland
   College Park
RP Wu, ZX (corresponding author), Fudan Univ, Shanghai Key Lab Intelligent Info Proc, Sch CS, Shanghai, Peoples R China.
EM zjweng20@fudan.edu.cn; zxwu@fudan.edu.cn; hdli@umd.edu;
   chenjingjing@fudan.edu.cn; ygj@fudan.edu.cn
RI Li, Hengduo/JWP-3348-2024; Wang, Yifan/KDO-8319-2024; yi,
   cheng/KHC-5004-2024; cheng, qian/KFB-6227-2024; Zhou,
   Xinyi/KGM-6689-2024; Wang, Yuhan/KGL-5855-2024; Chen,
   Yukun/KGK-4521-2024; li, fang/KDO-8841-2024
OI Li, Hengduo/0000-0001-5314-6853; Weng, Zejia/0000-0001-9706-6484
FU NSFC [62102092]; Shanghai Science and Technology Program [21JC1400600]
FX This project was supported by NSFC under Grant No. 62102092 and Shanghai
   Science and Technology Program (Project No. 21JC1400600).
CR Aloufi S, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3462777
   Arandjelovic R, 2017, IEEE I CONF COMP VIS, P609, DOI 10.1109/ICCV.2017.73
   Bolukbasi T., 2017, ICML
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Courtney PG, 2015, IEEE COMP SEMICON
   Feichtenhofer C, 2020, PROC CVPR IEEE, P200, DOI 10.1109/CVPR42600.2020.00028
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Howard AG, 2017, Arxiv, DOI [arXiv:1704.04861, 10.48550/arXiv.1704.04861]
   Gao RH, 2020, PROC CVPR IEEE, P10454, DOI 10.1109/CVPR42600.2020.01047
   Gao Z, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3377876
   Gemmeke JF, 2017, INT CONF ACOUST SPEE, P776, DOI 10.1109/ICASSP.2017.7952261
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou Zhijian., 2021, ACM MULTIMEDIA
   Jang E., 2017, ICLR (Poster)
   Jhuo IH, 2014, MACH VISION APPL, V25, P33, DOI 10.1007/s00138-013-0567-0
   Jiang YG, 2018, IEEE T PATTERN ANAL, V40, P352, DOI 10.1109/TPAMI.2017.2670560
   Jiang YG, 2018, IEEE T MULTIMEDIA, V20, P3137, DOI 10.1109/TMM.2018.2823900
   Jiang YG, 2015, IEEE T MULTIMEDIA, V17, P1174, DOI 10.1109/TMM.2015.2436813
   Kang SH, 2018, LECT NOTES COMPUT SC, V11218, P402, DOI 10.1007/978-3-030-01264-9_24
   Korbar B, 2019, IEEE I CONF COMP VIS, P6241, DOI 10.1109/ICCV.2019.00633
   Li H, 2019, IEEE I CONF COMP VIS, P1891, DOI 10.1109/ICCV.2019.00198
   Li ZY, 2018, COMPUT VIS IMAGE UND, V166, P41, DOI 10.1016/j.cviu.2017.10.011
   Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718
   Liu Chunhui, 2021, arXiv
   Long X, 2018, AAAI CONF ARTIF INTE, P7202
   Maddison C. J., 2017, The concrete distribution: A continuous relaxation of discrete random variables, P1
   Meng Yue, 2021, ICLR
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Owens A, 2018, LECT NOTES COMPUT SC, V11210, P639, DOI 10.1007/978-3-030-01231-1_39
   Pan Bowen, 2021, ICLR
   Plakal M., 2020, YAMNet
   Qi ZB, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3857, DOI 10.1145/3394171.3413954
   Qiu ZF, 2016, Arxiv, DOI arXiv:1611.09502
   Qiu ZF, 2017, PROC CVPR IEEE, P4085, DOI 10.1109/CVPR.2017.435
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Senocak Arda., 2019, IEEE Trans. Pattern Anal. Mach. Intell., V2019
   Simonyan K, 2014, ADV NEUR IN, V27
   Song SJ, 2020, IEEE T IMAGE PROCESS, V29, P3957, DOI 10.1109/TIP.2020.2967577
   Sutton RS, 2018, ADAPT COMPUT MACH LE, P1
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tian YP, 2018, LECT NOTES COMPUT SC, V11206, P252, DOI 10.1007/978-3-030-01216-8_16
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   Ullah A, 2019, IEEE T IND ELECTRON, V66, P9692, DOI 10.1109/TIE.2018.2881943
   Uzkent Burak, 2020, CVPR
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang JP, 2022, IEEE T MULTIMEDIA, V24, P2553, DOI 10.1109/TMM.2021.3087023
   Wang LM, 2019, IEEE T PATTERN ANAL, V41, P2740, DOI 10.1109/TPAMI.2018.2868668
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002
   Wei XY, 2011, IEEE T CIRC SYST VID, V21, P62, DOI 10.1109/TCSVT.2011.2105597
   Weng ZJ, 2022, Arxiv, DOI arXiv:2111.11067
   Wu WH, 2019, IEEE I CONF COMP VIS, P6231, DOI 10.1109/ICCV.2019.00632
   Wu ZX, 2022, IEEE T PATTERN ANAL, V44, P1699, DOI 10.1109/TPAMI.2020.3029425
   Wu ZX, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P791, DOI 10.1145/2964284.2964328
   Xiao FY, 2020, Arxiv, DOI arXiv:2001.08740
   Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19
   Xu BH, 2019, IEEE T IMAGE PROCESS, V28, P4941, DOI 10.1109/TIP.2019.2917283
   Yang H, 2020, IEEE T IMAGE PROCESS, V29, P5783, DOI 10.1109/TIP.2020.2984904
   Yang L, 2020, PROC CVPR IEEE, P2366, DOI 10.1109/CVPR42600.2020.00244
   Yang XD, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P978, DOI 10.1145/2964284.2964297
   Ye H, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P435, DOI 10.1145/2671188.2749406
   Yue Meng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P86, DOI 10.1007/978-3-030-58571-6_6
   Zhang BW, 2016, PROC CVPR IEEE, P2718, DOI 10.1109/CVPR.2016.297
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhang Y, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3542820
   Zhao RW, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3231739
   Zheng L, 2015, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2015.7298783
   Zheng Y, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3501404
   Zhou BL, 2018, LECT NOTES COMPUT SC, V11205, P831, DOI 10.1007/978-3-030-01246-5_49
   Zhou H, 2019, IEEE I CONF COMP VIS, P283, DOI 10.1109/ICCV.2019.00037
   Zhou YP, 2018, PROC CVPR IEEE, P3550, DOI 10.1109/CVPR.2018.00374
   Zhu C, 2018, LECT NOTES COMPUT SC, V11209, P139, DOI 10.1007/978-3-030-01228-1_9
   Zhu LC, 2020, AAAI CONF ARTIF INTE, V34, P13098
NR 76
TC 2
Z9 2
U1 0
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2024
VL 20
IS 2
SI SI
AR 35
DI 10.1145/3572776
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W6GR4
UT WOS:001092595800005
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Li, CH
   Li, ZZ
   Sun, J
   Zhang, Y
   Jiang, XP
   Zhang, F
AF Li, Chenghua
   Li, Zongze
   Sun, Jing
   Zhang, Yun
   Jiang, Xiaoping
   Zhang, Fan
TI DynamicWeighted Gradient Reversal Network for Visible-infrared Person
   Re-identification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Dynamic weight; gradient reversal; adversarial training; VI Re-ID
AB Due to intra-modality variations and cross-modality discrepancy, visible-infrared person re-identification (VI Re-ID) is an important and challenging task in intelligent video surveillance. The cross-modality discrepancy is mainly caused by the differences between visible images and infrared images, the inherent essence of which is heterogeneous. To alleviate this discrepancy, we propose a DynamicWeighted Gradient Reversal Network (DGRNet) to enhance the learning of discriminative common representations by confusing the modality discrimination. In the proposed DGRNet, we design the gradient reversal model guiding adversarial training between identity classifier and modality discriminator to reduce the modality discrepancy of the same person in different modalities. Furthermore, we propose an optimization training method, that is, designing dynamic weight of gradient reversal to achieve optimal adversarial training, and dynamic weight has the ability to dynamically and adaptively evaluate the significance of target loss term, without involving hyper-parameter tuning. Extensive experiments were conducted on two public VI Re-ID datasets, SYSU-MM01 and RegDB. The experimental results show that the proposed DGRNet outperforms state-of-the-art methods and demonstrate the effectiveness of the DGRNet to learn more discriminative common representations for VI Re-ID.
C1 [Li, Chenghua; Li, Zongze; Jiang, Xiaoping] South Cent Minzu Univ, 182 Minzu Rd, Wuhan, Peoples R China.
   [Sun, Jing; Zhang, Yun] Chinese Acad Sci, Shenzhen Inst Adv Technol, 1068 Xueyuan Rd, Shenzhen, Peoples R China.
   [Zhang, Fan] Shenzhen Beidou Appl Technol Res Inst, 1068 Xueyuan Rd, Shenzhen, Peoples R China.
C3 South Central Minzu University; Chinese Academy of Sciences; Shenzhen
   Institute of Advanced Technology, CAS
RP Sun, J (corresponding author), Chinese Acad Sci, Shenzhen Inst Adv Technol, 1068 Xueyuan Rd, Shenzhen, Peoples R China.
EM mdlich@mail.scuec.edu.cn; 2020110198@mail.scuec.edu.cn;
   sunjing528@163.com; yun.zhang@siat.ac.cn; jiangxp@mail.scuec.edu.cn;
   zhangfan@sibat.cn
RI Yang, YiChen/KEI-0140-2024; yang, liu/JXX-5043-2024; Li,
   Yuanxiang/KCX-8706-2024; zhang, wen/JXN-0191-2024; hu,
   xin/KHT-2406-2024; Zhang, Yansong/KHW-4097-2024; zhang,
   xiaoyu/KEJ-0657-2024; Li, Binxu/KDO-3273-2024; li, cheng/KCZ-0615-2024;
   zhou, chen/KHW-8121-2024; xiao, wei/KCK-6954-2024
FU National Key R&D Program of China [2020YFC1522900]; Guangdong Basic and
   Applied Basic Research Foundation [2021A1515011913]; funds of South
   Central Minzu University [CZT20001]
FX This work is supported by National Key R&D Program of China (No.
   2020YFC1522900) and Guangdong Basic and Applied Basic Research
   Foundation (Grant No. 2021A1515011913) and funds of South Central Minzu
   University (Grant No. CZT20001).
CR Bai X, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107036
   Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4
   Chen CQ, 2022, IEEE T IMAGE PROCESS, V31, P2352, DOI 10.1109/TIP.2022.3141868
   Chen YHS, 2021, PROC CVPR IEEE, P587, DOI 10.1109/CVPR46437.2021.00065
   Dai PY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P677
   Fan HH, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3243316
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Gao Guangwei, 2022, WWW J., P1
   Hao X, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16383, DOI 10.1109/ICCV48922.2021.01609
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Hu WP, 2021, IEEE T MULTIMEDIA, V23, P145, DOI 10.1109/TMM.2020.2980201
   Hu WP, 2021, IEEE T INF FOREN SEC, V16, P70, DOI 10.1109/TIFS.2020.3005314
   Huang NC, 2021, Arxiv, DOI arXiv:2104.11539
   Kansal K, 2020, IEEE T CIRC SYST VID, V30, P3422, DOI 10.1109/TCSVT.2019.2963721
   Kniaz VV, 2019, LECT NOTES COMPUT SC, V11134, P606, DOI 10.1007/978-3-030-11024-6_46
   Li DG, 2020, AAAI CONF ARTIF INTE, V34, P4610
   Li YY, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3412384
   Liang WQ, 2021, IEEE T IMAGE PROCESS, V30, P6392, DOI 10.1109/TIP.2021.3092578
   Liu HJ, 2021, IEEE T MULTIMEDIA, V23, P4414, DOI 10.1109/TMM.2020.3042080
   Liu HJ, 2020, NEUROCOMPUTING, V398, P11, DOI 10.1016/j.neucom.2020.01.089
   Liu JN, 2022, IEEE T CIRC SYST VID, V32, P7226, DOI 10.1109/TCSVT.2022.3168999
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2905, DOI 10.1109/TMM.2020.2965491
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190
   Luo MD, 2021, IEEE T INF FOREN SEC, V16, P5003, DOI 10.1109/TIFS.2021.3122072
   Mang Ye, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P229, DOI 10.1007/978-3-030-58520-4_14
   Moon H, 2001, PERCEPTION, V30, P303, DOI 10.1068/p2896
   Nguyen DT, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17030605
   Seokeon Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10254, DOI 10.1109/CVPR42600.2020.01027
   Sun YF, 2021, IEEE T PATTERN ANAL, V43, P902, DOI 10.1109/TPAMI.2019.2938523
   Tzeng E, 2014, Arxiv, DOI [arXiv:1412.3474, DOI 10.48550/ARXIV.1412.3474]
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang GA, 2020, AAAI CONF ARTIF INTE, V34, P12144
   Wang GA, 2019, IEEE I CONF COMP VIS, P3622, DOI 10.1109/ICCV.2019.00372
   Wang PY, 2021, IEEE T MULTIMEDIA, V23, P1474, DOI 10.1109/TMM.2020.2999180
   Wang ZX, 2019, PROC CVPR IEEE, P618, DOI 10.1109/CVPR.2019.00071
   Wei ZY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P225, DOI 10.1109/ICCV48922.2021.00029
   Wei ZY, 2022, IEEE T NEUR NET LEAR, V33, P4676, DOI 10.1109/TNNLS.2021.3059713
   Wu AC, 2020, INT J COMPUT VISION, V128, P1765, DOI 10.1007/s11263-019-01290-1
   Wu AC, 2017, IEEE I CONF COMP VIS, P5390, DOI 10.1109/ICCV.2017.575
   Wu BT, 2023, IEEE SIGNAL PROC LET, V30, P140, DOI 10.1109/LSP.2023.3244747
   Wu Q, 2021, PROC CVPR IEEE, P4328, DOI 10.1109/CVPR46437.2021.00431
   Xiao G., 2021, P IEEE INT S CIRC SY, P1
   Yan Lu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13376, DOI 10.1109/CVPR42600.2020.01339
   Yang Bin, 2023, PROC IEEE INT CONF A, P1
   Ye HR, 2021, IEEE T IMAGE PROCESS, V30, P1583, DOI 10.1109/TIP.2020.3045261
   Ye M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13547, DOI 10.1109/ICCV48922.2021.01331
   Ye M, 2022, IEEE T INF FOREN SEC, V17, P386, DOI 10.1109/TIFS.2021.3139224
   Ye M, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1092
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Ye M, 2021, IEEE T INF FOREN SEC, V16, P728, DOI 10.1109/TIFS.2020.3001665
   Ye M, 2020, IEEE T INF FOREN SEC, V15, P407, DOI 10.1109/TIFS.2019.2921454
   Ye M, 2018, AAAI CONF ARTIF INTE, P7501
   Zhang La, 2022, ACM Trans. Multimedia Comput. Commun. Appl., V18, P15
   Zhang La, 2022, ACM Trans. Multimedia Comput. Commun. Appl.
   Zhang P, 2020, IEEE T CIRC SYST VID, V30, P4554, DOI 10.1109/TCSVT.2019.2939564
   Zhang Q, 2022, PROC CVPR IEEE, P7339, DOI 10.1109/CVPR52688.2022.00720
   Zhang SZ, 2021, IEEE T IMAGE PROCESS, V30, P8861, DOI 10.1109/TIP.2021.3120881
   Zhao ZW, 2021, AAAI CONF ARTIF INTE, V35, P3520
   Zheng L, 2018, IEEE T PATTERN ANAL, V40, P1224, DOI 10.1109/TPAMI.2017.2709749
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zhong X, 2022, IEEE T CIRC SYST VID, V32, P1418, DOI 10.1109/TCSVT.2021.3072171
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
NR 64
TC 0
Z9 0
U1 4
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2024
VL 20
IS 1
AR 12
DI 10.1145/3607535
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T8LL3
UT WOS:001080441800012
DA 2024-08-05
ER

PT J
AU Wu, HS
   Wang, ZZ
   Li, YF
   Liu, XT
   Lee, TY
AF Wu, Huisi
   Wang, Zhaoze
   Li, Yifan
   Liu, Xueting
   Lee, Tong-Yee
TI Suitable and Style-Consistent Multi-Texture Recommendation for Cartoon
   Illustrations
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Texture recommendation; texture replacement; cartoon texture; texture
   style-consistency; texture synthesis
ID REPRESENTATION; IMAGE
AB Texture plays an important role in cartoon illustrations to display object materials and enrich visual experiences. Unfortunately, manually designing and drawing an appropriate texture is not easy even for proficient artists, let alone novice or amateur people. While there exist tons of textures on the Internet, it is not easy to pick an appropriate one using traditional text-based search engines. Although several texture pickers have been proposed, they still require the users to browse the textures by themselves, which is still labor-intensive and time-consuming. In this article, an automatic texture recommendation system is proposed for recommending multiple textures to replace a set of user-specified regions in a cartoon illustration with visually pleasant look. Two measurements, the suitability measurement and the style-consistency measurement, are proposed to make sure that the recommended textures are suitable for cartoon illustration and at the same time mutually consistent in style. The suitability is measured based on the synthesizability, cartoonity, and region fitness of textures. The style-consistency is predicted using a learning-based solution since it is subjective to judge whether two textures are consistent in style. An optimization problem is formulated and solved via the genetic algorithm. Our method is validated on various cartoon illustrations, and convincing results are obtained.
C1 [Wu, Huisi; Wang, Zhaoze; Li, Yifan; Liu, Xueting] Shenzhen Univ, Coll Comp Sci & Software Engn, 3688 Nanhai Rd, Shenzhen, Guangdong, Peoples R China.
   [Lee, Tong-Yee] Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, 1 Univ Rd, Tainan 70101, Taiwan.
C3 Shenzhen University; National Cheng Kung University
RP Wu, HS (corresponding author), Shenzhen Univ, Coll Comp Sci & Software Engn, 3688 Nanhai Rd, Shenzhen, Guangdong, Peoples R China.
EM hswu@szu.edu.cn; 2070276045@email.szu.edu.cn;
   1810272030@email.szu.edu.cn; xtliu@szu.edu.cn; tonylee@mail.ncku.edu.tw
OI Liu, Xueting/0000-0002-0868-5353; Wu, Huisi/0000-0002-0399-9089
FU National Natural Science Foundation of China [62273241, 62002232];
   National Science and Technology Council [110-2221-E-006-135-MY3,
   111-2221-E-006-112-MY3]
FX This work was supported in part by grants from the National Natural
   Science Foundation of China (62273241 and 62002232) and the National
   Science and Technology Council (110-2221-E-006-135-MY3 and
   111-2221-E-006-112-MY3), Taiwan.
CR Andrearczyk V, 2016, PATTERN RECOGN LETT, V84, P63, DOI 10.1016/j.patrec.2016.08.016
   Barla P, 2006, COMPUT GRAPH FORUM, V25, P663, DOI 10.1111/j.1467-8659.2006.00986.x
   Bu XY, 2019, PATTERN RECOGN, V91, P34, DOI 10.1016/j.patcog.2019.02.003
   Chen B, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3366371
   Cimpoi M, 2016, INT J COMPUT VISION, V118, P65, DOI 10.1007/s11263-015-0872-3
   Dai DX, 2014, PROC CVPR IEEE, P3027, DOI 10.1109/CVPR.2014.387
   Dong WM, 2016, IEEE T VIS COMPUT GR, V22, P1088, DOI 10.1109/TVCG.2015.2440255
   Fu CC, 2018, Arxiv, DOI arXiv:1802.02992
   Fu YF, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3429742
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ho TT, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3396237
   Huisi Wu, 2018, Computational Visual Media, V4, P173, DOI 10.1007/s41095-018-0106-z
   Ishibashi K, 2018, 2018 NICOGRAPH INTERNATIONAL (NICOINT 2018), P37, DOI 10.1109/NICOINT.2018.00015
   Kazi RubaiatHabib., 2012, Proceedings of the ACM Conference on Human Factors in Computing Systems, P1727
   Kopf J, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366159
   Kwatra V, 2003, ACM T GRAPHIC, V22, P277, DOI 10.1145/882262.882264
   Lai H, 2018, LECT NOTES COMPUT SC, V10736, P735, DOI 10.1007/978-3-319-77383-4_72
   Lazebnik S, 2005, IEEE T PATTERN ANAL, V27, P1265, DOI 10.1109/TPAMI.2005.151
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Le Thi-Ngoc-Hanh, 2023, ACM Transactions on Multimedia Computing, Communications, and Applications, V19, P1
   Li H, 2011, GRAPP 2011: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON COMPUTER GRAPHICS THEORY AND APPLICATIONS, P166
   Li W, 2019, IEEE T VIS COMPUT GR, V25, P2296, DOI 10.1109/TVCG.2018.2831220
   Liu Xueting, 2017, [Computational Visual Media, 计算可视媒体], V3, P61
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Pang WM, 2010, IUI 2010, P365
   Qu YG, 2006, ACM T GRAPHIC, V25, P1214, DOI 10.1145/1141911.1142017
   Qu YG, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409108
   Roy SK, 2020, ADV INTELL SYST COMP, V1024, P271, DOI 10.1007/978-981-32-9291-8_22
   S'ykora D., 2011, INT S NONPHOTOREALIS, P75
   Sendik O, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3015461
   Shamai G, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3337067
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tan MX, 2020, Arxiv, DOI arXiv:1905.11946
   Tian T, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3408320
   Tsubota K, 2019, IEEE INT SYM MULTIM, P212, DOI 10.1109/ISM46123.2019.00046
   Wu H, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3551891
   Wu HS, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON SIGNAL AND IMAGE PROCESSING (ICSIP), P277, DOI 10.1109/SIPROCESS.2016.7888267
   Yan CG, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3472810
   Yan H, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3567596
   Yang F, 2019, IEEE T IMAGE PROCESS, V28, P2502, DOI 10.1109/TIP.2018.2886807
   Yao CY, 2017, IEEE T VIS COMPUT GR, V23, P1070, DOI 10.1109/TVCG.2016.2525774
   Yao YQ, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3131345
   Yin Lulu, 2017, Advances in Multimedia Information Processing-PCM 2017. Lecture Notes in Computer Science, V736, P745
   Yu N, 2019, PROC CVPR IEEE, P12156, DOI 10.1109/CVPR.2019.01244
   Zhang H, 2017, PROC CVPR IEEE, P2896, DOI 10.1109/CVPR.2017.309
   Zhang WQ, 2022, PROC CVPR IEEE, P12073, DOI 10.1109/CVPR52688.2022.01177
   Zhang YS, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3571076
   Zhou ML, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3590965
   Zhu L, 2020, IEEE T VIS COMPUT GR, V26, P2471, DOI 10.1109/TVCG.2018.2889055
NR 49
TC 0
Z9 0
U1 2
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2024
VL 20
IS 7
SI SI
AR 220
DI 10.1145/3652518
PG 26
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL0Q6
UT WOS:001234494100035
DA 2024-08-05
ER

PT J
AU Yang, RY
   Liu, D
   Ma, SW
   Wu, F
   Gao, W
AF Yang, Runyu
   Liu, Dong
   Ma, Siwei
   Wu, Feng
   Gao, Wen
TI Perceptual Quality-Oriented Rate Allocation via Distillation from
   End-to-End Image Compression
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE AVS3; block-based coding; end-to-end image compression; H.266/VVC;
   perceptual quality; rate allocation
ID OPTIMIZATION
AB Mainstream image/video coding standards, exemplified by the state-of-the-art H.266/VVC, AVS3, and AV1, follow the block-based hybrid coding framework. Due to the block-based framework, encoders designed for these standards are easily optimized for peak signal-to-noise ratio (PSNR) but have difficulties optimizing for the metrics more aligned to perceptual quality, e.g., multi-scale structural similarity (MS-SSIM), since these metrics cannot be accurately evaluated at the small block level. We address this problem by leveraging inspiration from the end-to-end image compression built on deep networks, which is easily optimized through network training for any metric as long as the metric is differentiable. We compared the trained models using the same network structure but different metrics and observed that the models allocate rates in different ratios. We then propose a distillation method to obtain the rate allocation rule from end-to-end image compression models with different metrics and to utilize such a rule in the block-based encoders. We implement the proposedmethod on the VVC reference software-VTMand the AVS3 reference software-HPM, focusing on intraframe coding. Experimental results show that the proposed method on top of VTM achieves more than 10% BD-rate reduction than the anchor when evaluated withMS-SSIM or LPIPS, which leads to concrete perceptual quality improvement.
C1 [Yang, Runyu; Liu, Dong; Wu, Feng] Univ Sci & Technol China, 443 Huangshan Rd, Hefei 230027;, Anhui, Peoples R China.
   [Ma, Siwei; Gao, Wen] Peking Univ, 5 Yiheyuan Rd, Beijing 100871, Peoples R China.
   [Gao, Wen] Peng Cheng Lab, 2 Xingkeyi St, Shenzhen 518055, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Peking University; Peng Cheng Laboratory
RP Liu, D (corresponding author), Univ Sci & Technol China, 443 Huangshan Rd, Hefei 230027;, Anhui, Peoples R China.
EM yry@mail.ustc.edu.cn; dongeliu@ustc.edu.cn; swma@pku.edu.cn;
   fengwu@ustc.edu.cn; wgao@pku.edu.cn
OI Yang, Runyu/0009-0002-4233-0211
FU Natural Science Foundation of China [62021001]; Fundamental Research
   Funds for the Central Universities [WK3490000006]
FX This work was supported by the Natural Science Foundation of China under
   Grant 62021001 and by the Fundamental Research Funds for the Central
   Universities under Grant WK3490000006.
CR Ball‚ J, 2018, Arxiv, DOI arXiv:1802.01436
   Bosse S, 2019, IEEE IMAGE PROC, P126, DOI [10.1109/ICIP.2019.8802925, 10.1109/icip.2019.8802925]
   Bosse S, 2019, DIGIT SIGNAL PROCESS, V91, P54, DOI 10.1016/j.dsp.2018.12.005
   Bosse Sebastian, 2017, Technical Report JVET-H0047
   Bross B, 2021, P IEEE, V109, P1463, DOI 10.1109/JPROC.2020.3043399
   Chen T., 2019, arXiv
   Chen Y, 2018, PICT COD SYMP, P41, DOI 10.1109/PCS.2018.8456249
   Cheng ZX, 2019, IEEE IMAGE PROC, P719, DOI [10.1109/icip.2019.8803824, 10.1109/ICIP.2019.8803824]
   Fan L, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P423, DOI 10.1109/ICME.2004.1394219
   Gao W, 2016, IEEE T MULTIMEDIA, V18, P988, DOI 10.1109/TMM.2016.2535254
   Girod Bernd, 1993, P207
   Helmrich Christian, 2018, Technical Report JVET-K0206
   Jin ZP, 2020, NEUROCOMPUTING, V394, P158, DOI 10.1016/j.neucom.2018.12.090
   Kim Y, 2020, IEEE COMPUT SOC CONF, P565, DOI 10.1109/CVPRW50498.2020.00076
   Kingma D. P., 2014, arXiv
   Klopp JP, 2021, PROC CVPR IEEE, P16160, DOI 10.1109/CVPR46437.2021.01590
   Kumar Manoj, 2022, Trans. Mach. Learn. Res.
   Lee JY, 2020, Arxiv, DOI arXiv:1912.12817
   Li B, 2014, IEEE T IMAGE PROCESS, V23, P3841, DOI 10.1109/TIP.2014.2336550
   Li Y, 2021, IEEE T BROADCAST, V67, P500, DOI 10.1109/TBC.2021.3068871
   Li Ye, 2017, IEEE C VIS COMM IM P, P1
   Liu D, 2020, ACM COMPUT SURV, V53, DOI 10.1145/3368405
   Liu JY, 2020, IEEE T IMAGE PROCESS, V29, P7845, DOI 10.1109/TIP.2020.3007828
   Ma HC, 2023, IEEE T PATTERN ANAL, V45, P3648, DOI 10.1109/TPAMI.2022.3185316
   Ma HC, 2022, IEEE T PATTERN ANAL, V44, P1247, DOI 10.1109/TPAMI.2020.3026003
   Ma SW, 2015, IEEE SIGNAL PROC MAG, V32, P172, DOI 10.1109/MSP.2014.2371951
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149
   Toderici G, 2016, Arxiv, DOI [arXiv:1511.06085, DOI 10.48550/ARXIV.1511.06085]
   Vidal E, 2017, SIGNAL PROCESS-IMAGE, V52, P124, DOI 10.1016/j.image.2016.12.003
   Wang XT, 2019, LECT NOTES COMPUT SC, V11133, P63, DOI 10.1007/978-3-030-11021-5_5
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2009, IEEE SIGNAL PROC MAG, V26, P98, DOI 10.1109/MSP.2008.930649
   Wu JJ, 2019, FRONT COMPUT SCI-CHI, V13, P4, DOI 10.1007/s11704-016-6213-z
   Wu YJ, 2022, IEEE T CIRC SYST VID, V32, P3978, DOI 10.1109/TCSVT.2021.3119660
   Xu L, 2016, IEEE T MULTIMEDIA, V18, P590, DOI 10.1109/TMM.2016.2525004
   Yan Ning, 2020, Technical Report AVS-M5591
   Yang RY, 2021, IEEE IMAGE PROC, P3438, DOI 10.1109/ICIP42928.2021.9506765
   Zagoruyko S, 2017, Arxiv, DOI [arXiv:1612.03928, DOI 10.48550/ARXIV.1612.03928]
   Zeng HQ, 2016, MULTIMED TOOLS APPL, V75, P10383, DOI 10.1007/s11042-015-2997-3
   Zhang J, 2019, PICT COD SYMP, DOI 10.1109/pcs48520.2019.8954503
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang Y, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3571727
   Zhao X, 2020, PROC SPIE, V11510, DOI 10.1117/12.2570003
   Zhou ML, 2020, IEEE T IMAGE PROCESS, V29, P7603, DOI 10.1109/TIP.2020.3004714
   Zhu SP, 2020, IEEE T CIRC SYST VID, V30, P1946, DOI 10.1109/TCSVT.2019.2911396
NR 47
TC 0
Z9 0
U1 2
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2024
VL 20
IS 7
SI SI
AR 198
DI 10.1145/3650034
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL0Q6
UT WOS:001234494100013
DA 2024-08-05
ER

PT J
AU Hossain, MS
   Hao, YX
   Hu, L
   Liu, J
   Wei, G
   Min, C
AF Hossain, M. Shamim
   Hao, Yixue
   Hu, Long
   Liu, Jia
   Wei, Gang
   Min, Chen
TI Immersive Multimedia Service Caching in Edge Cloud with Renewable Energy
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Edge cloud; intelligent scheduling; renewable energy; multimedia service
   caching
ID COMPUTATION; PLACEMENT
AB Immersive service caching, based on the intelligent edge cloud, canmeet delay-sensitive service requirements. Although numerous service caching solutions for edge clouds have been designed, they have not been well explored. Moreover, to the best of our knowledge, there is no work to consider the immersive service caching scheme under the supply of renewable energy. In this article, we investigate the service caching problem under the renewable energy supply to minimize service latency while making full use of renewable energy. Specifically, we formulate the service caching and renewable energy harvesting problem, which considers the dynamic renewable energy, unknown service requests, and limited capacity of the edge cloud. To solve this problem, we propose an effective algorithm, called OSCRE. Our algorithm first uses Lyapunov optimization to convert the time-average problem into time-independence optimization and thus realizes optimal renewable energy harvesting. Then, it realizes the service caching scheme using data-driven combinatorial multi-armed bandit learning. The simulation results show that the OSCRE scheme can save service latency while making sufficient use of renewable energy.
C1 [Hossain, M. Shamim] King Saud Univ, Dept Software Engn, Coll Comp & Informat Sci, Riyadh 12372, Saudi Arabia.
   [Hao, Yixue; Hu, Long; Liu, Jia] Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan 430074, Peoples R China.
   [Wei, Gang] South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510640, Peoples R China.
   [Min, Chen] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510640, Peoples R China.
   [Min, Chen] South China Univ Technol, Pazhou Lab, Guangzhou 510640, Peoples R China.
C3 King Saud University; Huazhong University of Science & Technology; South
   China University of Technology; South China University of Technology;
   Pazhou Lab; South China University of Technology
RP Hossain, MS (corresponding author), King Saud Univ, Dept Software Engn, Coll Comp & Informat Sci, Riyadh 12372, Saudi Arabia.; Hao, YX (corresponding author), Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan 430074, Peoples R China.
EM mshossain@ksu.edu.sa; yixuehao@hust.edu.cn; hulong@hust.edu.cn;
   jialiu0330@hust.edu.cn; ecgwei@scut.edu.cn; minchen@ieee.org
OI Hossain, M. Shamim/0000-0001-5906-9422
FU King Saud University, Riyadh, Saudi Arabia [RSP2024R32]
FX This work was supported by the Researchers Supporting Project number
   (RSP2024R32), King Saud University, Riyadh, Saudi Arabia.
CR Alam KM, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2501643.2501649
   Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   Chen LX, 2018, IEEE T WIREL COMMUN, V17, P8388, DOI 10.1109/TWC.2018.2876823
   Dai PL, 2020, IEEE T VEH TECHNOL, V69, P7821, DOI 10.1109/TVT.2020.2991641
   Guo M, 2022, COMPUT NETW, V204, DOI 10.1016/j.comnet.2021.108678
   Hao YX, 2021, IEEE T IND INFORM, V17, P5552, DOI [10.1109/TII.2020.3041713, 10.1109/tii.2020.3041713]
   Hao YX, 2018, IEEE ACCESS, V6, P11365, DOI 10.1109/ACCESS.2018.2805798
   Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872
   Jiang YA, 2023, IEEE T VEH TECHNOL, V72, P1084, DOI 10.1109/TVT.2022.3204839
   Keighrey C, 2021, IEEE T MULTIMEDIA, V23, P333, DOI 10.1109/TMM.2020.2982046
   Khalid U, 2023, IEEE WIREL COMMUN, V30, P26, DOI 10.1109/MWC.002.2200613
   Ku ML, 2016, IEEE COMMUN SURV TUT, V18, P1384, DOI 10.1109/COMST.2015.2497324
   Ma X, 2020, IEEE INFOCOM SER, P2076, DOI [10.1109/infocom41043.2020.9155455, 10.1109/INFOCOM41043.2020.9155455]
   Miao YM, 2021, IEEE T NETW SCI ENG, V8, P625, DOI [10.1109/TNSE.2020.3047417, 10.1109/tnse.2020.3047417]
   Min MH, 2019, IEEE T VEH TECHNOL, V68, P1930, DOI 10.1109/TVT.2018.2890685
   Ozel O, 2011, IEEE J SEL AREA COMM, V29, P1732, DOI 10.1109/JSAC.2011.110921
   Poularakis K, 2020, IEEE ACM T NETWORK, V28, P1047, DOI 10.1109/TNET.2020.2980175
   Somuyiwa SO, 2018, IEEE J SEL AREA COMM, V36, P1331, DOI 10.1109/JSAC.2018.2844985
   Sun C, 2023, IEEE J SEL AREA COMM, V41, P690, DOI 10.1109/JSAC.2023.3235443
   Ulukus S, 2015, IEEE J SEL AREA COMM, V33, P360, DOI 10.1109/JSAC.2015.2391531
   Xia XY, 2021, IEEE T PARALL DISTR, V32, P281, DOI 10.1109/TPDS.2020.3016344
   Xiao H, 2021, IEEE GLOB COMM CONF, DOI 10.1109/GLOBECOM46510.2021.9685687
   Xu J, 2018, IEEE INFOCOM SER, P207, DOI 10.1109/INFOCOM.2018.8485977
   Xu J, 2017, IEEE T COGN COMMUN, V3, P361, DOI 10.1109/TCCN.2017.2725277
   Xu ZC, 2020, IEEE INFOCOM SER, P2066, DOI [10.1109/INFOCOM41043.2020.9155365, 10.1109/infocom41043.2020.9155365]
   Zang SZ, 2019, IEEE J SEL AREA COMM, V37, P1518, DOI 10.1109/JSAC.2019.2916451
   Zhang GL, 2018, IEEE T IND INFORM, V14, P4642, DOI 10.1109/TII.2018.2843365
   Zhang J, 2020, IEEE INTERNET THINGS, V7, P9303, DOI 10.1109/JIOT.2020.3000527
   Zhao FJ, 2021, IEEE T NETW SERV MAN, V18, P2154, DOI 10.1109/TNSM.2021.3069993
NR 29
TC 0
Z9 0
U1 1
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2024
VL 20
IS 6
SI SI
AR 173
DI 10.1145/3643818
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OQ2T3
UT WOS:001208681800023
DA 2024-08-05
ER

PT J
AU Ma, YN
   Zhao, CQ
   Huang, B
   Li, XD
   Basu, A
AF Ma, Yingnan
   Zhao, Chenqiu
   Huang, Bingran
   Li, Xudong
   Basu, Anup
TI RAST: Restorable Arbitrary Style Transfer
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Neural style transfer; multi-restorations; style difference
AB The objective of arbitrary style transfer is to apply a given artistic or photo-realistic style to a target image. Although current methods have shown some success in transferring style, arbitrary style transfer still has several issues, including content leakage. Embedding an artistic style can result in unintended changes to the image content. This article proposes an iterative framework called Restorable Arbitrary Style Transfer (RAST) to effectively ensure content preservation and mitigate potential alterations to the content information. RAST can transmit both content and style information through multi-restorations and balance the content-style tradeoff in stylized images using the image restoration accuracy. To ensure RAST's effectiveness, we introduce two novel loss functions: multi-restoration loss and style difference loss. We also propose a new quantitative evaluation method to assess content preservation and style embedding performance. Experimental results show that RAST outperforms state-of-the-art methods in generating stylized images that preserve content and embed style accurately.
C1 [Ma, Yingnan; Zhao, Chenqiu; Huang, Bingran; Li, Xudong; Basu, Anup] Univ Alberta, Edmonton, AB, Canada.
C3 University of Alberta
RP Ma, YN (corresponding author), Univ Alberta, 116 St 85 Ave, Edmonton, AB T6G 2R3, Canada.
EM ma4@ualberta.ca; zhao.chenqiu@ualberta.ca; bingran1@ualberta.ca;
   xudong9@ualberta.ca; basu@ualberta.ca
CR An J, 2021, PROC CVPR IEEE, P862, DOI 10.1109/CVPR46437.2021.00092
   Cetinic E, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3475799
   Chen HB, 2021, ADV NEUR IN, V34
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Deng YY, 2022, PROC CVPR IEEE, P11316, DOI 10.1109/CVPR52688.2022.01104
   Deng YY, 2021, AAAI CONF ARTIF INTE, V35, P1210
   Deng YY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2719, DOI 10.1145/3394171.3414015
   Dinh L, 2015, Arxiv, DOI [arXiv:1410.8516, 10.48550/arXiv.1410.8516]
   Dumoulin V., 2016, ARXIV PREPRINT ARXIV
   Elad M, 2017, IEEE T IMAGE PROCESS, V26, P2338, DOI 10.1109/TIP.2017.2678168
   Fiser J, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925948
   Frigo O, 2016, PROC CVPR IEEE, P553, DOI 10.1109/CVPR.2016.66
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Gooch B, 2004, ACM T GRAPHIC, V23, P27, DOI 10.1145/966131.966133
   Goodfellow I. J., 2014, ADV NEURAL INFORM PR
   Gu XL, 2024, ACM T MULTIM COMPUT, V20, DOI 10.1145/3545610
   Hensel M, 2017, ADV NEUR IN, V30
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   Hertzmann A., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P453, DOI 10.1145/280814.280951
   Ho J., 2019, PMLR, V97, P2722
   Hong K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14589, DOI 10.1109/ICCV48922.2021.01434
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Kang M., 2020, Advances in Neural Information Processing Systems, P21357
   Karayev S., 2014, P BRIT MACH VIS C, DOI [DOI 10.5244/C.28.122, 10.5244/c.28.122, 10.5244%2Fc.28.122]
   Kim T, 2017, PR MACH LEARN RES, V70
   Kingma D. P., 2014, arXiv
   Kingma Durk P., 2018, P 32 INT C NEUR INF
   Kotovenko D, 2019, IEEE I CONF COMP VIS, P4421, DOI 10.1109/ICCV.2019.00452
   Li C, 2016, PROC CVPR IEEE, P2479, DOI 10.1109/CVPR.2016.272
   Li XT, 2019, PROC CVPR IEEE, P3804, DOI 10.1109/CVPR.2019.00393
   Li Yijun, 2017, P 31 INT C NEUR INF
   Lin MX, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3450525
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu SH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6629, DOI 10.1109/ICCV48922.2021.00658
   Liu ZS, 2022, Arxiv, DOI arXiv:2202.13562
   Lu M, 2019, IEEE I CONF COMP VIS, P5951, DOI 10.1109/ICCV.2019.00605
   Luo X, 2022, Arxiv, DOI arXiv:2201.02233
   Ma YN, 2023, IEEE WINT CONF APPL, P331, DOI 10.1109/WACV56688.2023.00041
   Pang B, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3485061
   Park DY, 2019, PROC CVPR IEEE, P5873, DOI 10.1109/CVPR.2019.00603
   Park T., 2020, EUR C COMP VIS, P319, DOI [DOI 10.1007/978-3-030-58545-719, 10.1007/978-3-030-58545-719, DOI 10.1007/978-3-030-58545-7_19]
   Chen TQ, 2016, Arxiv, DOI arXiv:1612.04337
   Sanakoyeu A, 2018, LECT NOTES COMPUT SC, V11212, P715, DOI 10.1007/978-3-030-01237-3_43
   Seobin Park, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P754, DOI 10.1007/978-3-030-58583-9_45
   Sheng L, 2018, PROC CVPR IEEE, P8242, DOI 10.1109/CVPR.2018.00860
   Shih YC, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601137
   Siarohin A, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3311781
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Soh JW, 2020, PROC CVPR IEEE, P3513, DOI 10.1109/CVPR42600.2020.00357
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Ulyanov D, 2016, PR MACH LEARN RES, V48
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Q, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3506710
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Winnemöller H, 2006, ACM T GRAPHIC, V25, P1221, DOI 10.1145/1141911.1142018
   Wu HY, 2021, PROC CVPR IEEE, P10546, DOI 10.1109/CVPR46437.2021.01041
   Wu XL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14598, DOI 10.1109/ICCV48922.2021.01435
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang Y, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3542820
   Zhang Yuxin, 2022, ACMSIGGRAPH 2022 C P, P1
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 64
TC 0
Z9 0
U1 6
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2024
VL 20
IS 5
AR 143
DI 10.1145/3638770
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MF3R9
UT WOS:001192177900023
DA 2024-08-05
ER

PT J
AU Han, K
   Liu, Y
   Wei, RK
   Zhou, K
   Xu, JH
   Long, K
AF Han, Kai
   Liu, Yu
   Wei, Rukai
   Zhou, Ke
   Xu, Jinhui
   Long, Kun
TI Supervised Hierarchical Online Hashing for Cross-modal Retrieval
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Online hashing; cross-modal retrieval; label hierarchy
AB Online cross-modal hashing has gained attention for its adaptability in processing streaming data. However, existing methods only define the hard similarity between data using labels. This results in poor retrieval performance, as they fail to exploit the semantic structure information of labels and miss the high-quality hash codes guided by the hierarchical relevance between labels. In addition, they ignore the bit-flipping problem, which leads to sub-optimal cross-modal retrieval performance. To address these issues, we propose Supervised Hierarchical Online Hashing (SHOH) for cross-modal retrieval. Our approach acquires hierarchical similarity via cross-layer affiliation of labels and explores its application to online hashing. We design a hierarchical similarity learning method in the online learning framework, which includes virtual center learning and hierarchical similarity embedding. Labels with soft similarity bridge the label hierarchy and cross-modal hash embedding. Furthermore, we propose a Weighted Retrieval Strategy (WRS) to mitigate the impact caused by bit-flipping errors. Extensive experiments and verification on hierarchical and non-hierarchical datasets demonstrate that SHOH preserves accurate inter-class distances and achieves performance improvements compared to state-of-the-art methods. The source code is available at https://github.com/HUST-IDSM-AI/SHOH.
C1 [Han, Kai; Liu, Yu; Wei, Rukai; Zhou, Ke; Xu, Jinhui] Huazhong Univ Sci & Technol, Wuhan, Peoples R China.
   [Long, Kun] Huawei Technol Co Ltd, Shenzhen, Peoples R China.
C3 Huazhong University of Science & Technology; Huawei Technologies
RP Liu, Y (corresponding author), Huazhong Univ Sci & Technol, Wuhan, Peoples R China.
EM kylehan@hust.edu.cn; liu_yu@hust.edu.cn; weirukai@hust.edu.cn;
   zhke@hust.edu.cn; xjh@hust.edu.cn; longkun2@huawei.com
OI Wei, Rukai/0000-0003-1164-6360; Liu, Yu/0000-0002-1964-9278
FU National Natural Science Foundation of China [62232007, 61902135,
   61821003]; Natural Science Foundation of Hubei Province [2022CFB060]
FX This work is supported by the National Natural Science Foundation of
   China (Grant No. 62232007, No. 61902135, No. 61821003) and the Natural
   Science Foundation of Hubei Province (Grant No. 2022CFB060).
CR Bentaleb A, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3219752
   Cakir F, 2017, IEEE I CONF COMP VIS, P437, DOI 10.1109/ICCV.2017.55
   Chen XX, 2017, CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE (UAI2017)
   Chua TS, 2009, P ACM INT C IM VID R, P1
   Desrousseaux R, 2021, IEEE INT CONF BIG DA, P1595, DOI 10.1109/BigData52589.2021.9671716
   Ding CL, 2022, PROCEEDINGS OF THE 31ST ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, CIKM 2022, P375, DOI 10.1145/3511808.3557488
   Jati A, 2020, INT CONF ACOUST SPEE, P4497, DOI [10.1109/ICASSP40776.2020.9053766, 10.1109/icassp40776.2020.9053766]
   Jin S, 2021, AAAI CONF ARTIF INTE, V35, P1717
   Karagkioules T, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3460819
   Leng C, 2015, PROC CVPR IEEE, P2503, DOI 10.1109/CVPR.2015.7298865
   Li SY, 2021, PROC CVPR IEEE, P13544, DOI 10.1109/CVPR46437.2021.01334
   Lin Haitao, 2022, IEEE INT C MULT EXP, P1, DOI [10.1109/ICME52920.2022.9859620, DOI 10.1109/ICME52920.2022.9859620]
   Lin MB, 2022, IEEE T PATTERN ANAL, V44, P2453, DOI 10.1109/TPAMI.2020.3042193
   Lin MB, 2020, INT J COMPUT VISION, V128, P2279, DOI 10.1007/s11263-020-01332-z
   Liu SG, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3355394
   Liu X, 2023, IEEE T MULTIMEDIA, V25, P3811, DOI 10.1109/TMM.2022.3166668
   Liu X, 2021, IEEE T PATTERN ANAL, V43, P964, DOI 10.1109/TPAMI.2019.2940446
   Liu Y, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102819
   Lu X, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1129, DOI 10.1145/3343031.3350999
   MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748
   Odijk D, 2017, PROCEEDINGS OF THE ELEVENTH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'17), P348, DOI 10.1145/3109859.3109925
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Shen HT, 2021, IEEE T KNOWL DATA EN, V33, P3351, DOI [10.1109/TKDE.2020.2970050, 10.1109/TNNLS.2020.2995708]
   Shen L, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3356316
   Shu ZQ, 2023, APPL INTELL, V53, P14201, DOI 10.1007/s10489-022-04189-6
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song XM, 2018, ACM/SIGIR PROCEEDINGS 2018, P5, DOI 10.1145/3209978.3209996
   Sun CC, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P725, DOI 10.1145/3331184.3331229
   Wang D, 2018, AAAI CONF ARTIF INTE, P7388
   Wang D, 2023, IEEE T MULTIMEDIA, V25, P1217, DOI 10.1109/TMM.2022.3140656
   Wang D, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P1409, DOI 10.1145/3397271.3401132
   Wang S, 2022, IEEE T CIRC SYST VID, V32, P8022, DOI 10.1109/TCSVT.2022.3186714
   Wang XQ, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3558769
   Wang YX, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P871, DOI 10.1145/3394171.3413971
   Weng ZY, 2020, AAAI CONF ARTIF INTE, V34, P12354
   Wu DY, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3524021
   Wu XM, 2022, AAAI CONF ARTIF INTE, P4263
   Xie L, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3133
   Xie L, 2016, AAAI CONF ARTIF INTE, P294
   Xie YZ, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P955
   Yan C, 2019, NEUROCOMPUTING, V337, P58, DOI 10.1016/j.neucom.2019.01.040
   Yao T, 2019, PATTERN RECOGN, V89, P1, DOI 10.1016/j.patcog.2018.12.012
   Ye ZD, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3356338
   Yi Jinhan, 2021, IEEE INT C MULT EXP, P1, DOI [10.1109/ICME51207.2021.9428323, DOI 10.1109/ICME51207.2021.9428323]
   Yu E, 2022, NEUROCOMPUTING, V486, P215, DOI 10.1016/j.neucom.2021.11.035
   Yudong Chen, 2021, MM '21: Proceedings of the 29th ACM International Conference on Multimedia, P1921, DOI 10.1145/3474085.3475346
   Zhan YW, 2022, PATTERN RECOGN, V122, DOI 10.1016/j.patcog.2021.108262
   Zhan YW, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3386, DOI 10.1145/3394171.3413962
   Zhang DL, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3446774
   Zhang J, 2018, IEEE T MULTIMEDIA, V20, P2400, DOI 10.1109/TMM.2018.2804763
   Zhang Z, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3532519
   Zhu L, 2020, IEEE T IMAGE PROCESS, V29, P4643, DOI 10.1109/TIP.2020.2974065
   Zhuo YX, 2022, PROCEEDINGS OF THE 2022 INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, ICMR 2022, P158, DOI 10.1145/3512527.3531381
NR 53
TC 0
Z9 0
U1 15
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2024
VL 20
IS 4
AR 103
DI 10.1145/3632527
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6G9
UT WOS:001153382100013
OA Bronze
DA 2024-08-05
ER

PT J
AU Mahmud, B
   Hong, G
   Fong, B
AF Mahmud, Bahar
   Hong, Guan
   Fong, Bernard
TI A Study of Human-AI Symbiosis for Creative Work: Recent Developments and
   Future Directions in Deep Learning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Human-AI collaboration; human-AI teaming; artificial intelligence;
   collaborative concept development
ID DESIGN
AB Recent advances in Artificial Intelligence (AI), particularly deep learning, are having an enormous impact on our society today. Record numbers of jobs previously held by people have been automated, from manufacturing to transportation to customer services. The concerns of AI replacing humans by taking over people's jobs need to be urgently addressed. This article investigates some promising different directions of AI development: Instead of using AI to replace people, we should use AI to team up with people so that both can work better and smarter. Human-AI symbiosis refers to people and AI working together to jointly solve problems and perform specific tasks. The recent developments in deep learning models and frameworks have significantly improved the efficiency and performance of human and AI collaborations. In this article, some research work on human-AI collaborative environments has been extensively studied and analyzed to reveal the progress in this field. Although the teaming of humans and machines includes many complex tasks, the development has been very promising. One of the main goals in this field is to develop additional capabilities in machines capable of being successful teammates with a human partner. The correctness of the outcomes is often determined by the underlying technology and how performance and human satisfaction are measured through the collaborative nature of the system. We conclude that the teaming of humans and AI, particularly deep learning, has the advantage of combining the power of AI with the human domain expertise to improve performance and create value. Human-AI symbiosis could be a promising future direction for AI's continuing integration into the world.
C1 [Mahmud, Bahar; Hong, Guan] Western Michigan Univ, 1903 W Michigan Ave, Kalamazoo, MI 49008 USA.
   [Fong, Bernard] Providence Univ, Taichung 433, Taiwan.
C3 Western Michigan University; Providence University - Taiwan
RP Mahmud, B (corresponding author), Western Michigan Univ, 1903 W Michigan Ave, Kalamazoo, MI 49008 USA.
EM baharuddin.mahmud@wmich.edu; guanyue.hong@wmich.edu; bfong1@pu.edu.tw
OI Hong, Guan Yue/0000-0002-1775-9377; Mahmud, Bahar
   Uddin/0000-0002-9275-4170
CR Adiwardana D, 2020, Arxiv, DOI [arXiv:2001.09977, DOI 10.48550/ARXIV.2001.09977, 10.48550/arXiv.2001.09977]
   Akoury N, 2020, Arxiv, DOI arXiv:2010.01717
   Alldieck T, 2019, PROC CVPR IEEE, P1175, DOI 10.1109/CVPR.2019.00127
   Arjovsky M., 2017, arXiv, DOI DOI 10.48550/ARXIV.1701.04862
   Bau D., 2019, P INT C LEARN REPR I
   Bennett D, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1005020
   Bogo F, 2017, PROC CVPR IEEE, P5573, DOI 10.1109/CVPR.2017.591
   Burggraf Peter, 2020, Procedia CIRP, P1, DOI 10.1016/j.procir.2020.03.047
   Chandler S., 2020, Why deepfakes are a net positive for humanity
   Cheng ZX, 2020, IEEE T MULTIMEDIA, V22, P860, DOI 10.1109/TMM.2019.2938345
   Chopade P., 2018, CEUR WORKSHOP PROC, V2153, P48
   Cooke NJ, 2013, COGNITIVE SCI, V37, P255, DOI 10.1111/cogs.12009
   Cooney M., 2019, ICRA X ROB ART FOR
   Corbett-Davies S, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P797, DOI 10.1145/3097983.3098095
   Davis N, 2016, PROCEEDINGS OF THE 21ST INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES (IUI'16), P196, DOI 10.1145/2856767.2856795
   Deng B., 2020, LNCS, P612, DOI DOI 10.1007/978-3-030-58571-636
   DHaro L. F., 2015, Nat. Lang. Dialog Syst. Intell. Assist., V37, P233
   Dickson A., 2021, Deepfake porn is still a threat, particularly for K-Pop stars
   Fan JE, 2019, PROCEEDINGS OF THE 2019 ON CREATIVITY AND COGNITION - C&C 19, P556, DOI 10.1145/3325480.3326578
   Foley Joseph, 2021, 10 Deepfake Example That Terrified and Amused the Internet
   Gombolay M, 2017, INT J ROBOT RES, V36, P597, DOI 10.1177/0278364916688255
   Goodfellow I. J., 2014, P C WORKSH NEUR INF
   Gorecky D, 2014, IEEE INTL CONF IND I, P289, DOI 10.1109/INDIN.2014.6945523
   Groom V, 2007, INTERACT STUD, V8, P483
   Ha David, 2017, Draw Together with a Neural Network
   Heard J, 2019, ACM T HUM-ROBOT INTE, V8, DOI 10.1145/3314387
   Iyyer M., 2021, Feuding Families and Former Friends: Unsupervised Learning for Dynamic Fictional Relationships
   Jiang DD, 2021, IEEE T INTELL TRANSP, V22, P1868, DOI 10.1109/TITS.2020.3029015
   Karimi P., 2019, In, P17
   Kilbertus N, 2017, ADV NEUR IN, V30
   Lähner Z, 2018, LECT NOTES COMPUT SC, V11208, P698, DOI 10.1007/978-3-030-01225-0_41
   Lakkaraju H, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P275, DOI 10.1145/3097983.3098066
   Landrum AR, 2015, TRENDS COGN SCI, V19, P109, DOI 10.1016/j.tics.2014.12.007
   Lasota P.A., 2017, Foundations and Trends® in Robotics, V5, P261, DOI [10.1561/2300000052, DOI 10.1561/2300000052]
   Lee M.-L., 2021, P 2021 CHI C HUM FAC, P1, DOI [DOI 10.1145/3411764.3445472, 10.1145/3411764.3445472]
   Levy Steven, 2015, Inside Deep Dreams: How Google Made Its Computers Go Crazy
   Lin C.-Y., 2004, ANN M ASS COMP LING, P74
   Lv ZH, 2021, ACM T INTERNET TECHN, V21, DOI 10.1145/3406115
   Lv ZH, 2021, ACM T INTERNET TECHN, V21, DOI 10.1145/3414842
   Lv ZH, 2021, IEEE COMMUN MAG, V59, P126, DOI 10.1109/MCOM.001.2000945
   Ma QL, 2020, PROC CVPR IEEE, P6468, DOI 10.1109/CVPR42600.2020.00650
   Mahmud B. U., 2019, P INT C SUST TECHN I, P1, DOI [10.1109/STI47673.2019.9068093, DOI 10.1109/STI47673.2019.9068093]
   Mahmud B. U., 2022, arXiv
   Mahmud B. U., 2020, Int. J. Innov. Technol. Explor. Eng., V9, P369, DOI [10.35940/ijitee.H6437.069820, DOI 10.35940/IJITEE.H6437.069820]
   Mahmud BU, 2021, Arxiv, DOI [arXiv:2105.00192, DOI 10.48550/ARXIV.2105.00192]
   Mahmud Bahar Uddin, 2020, J. Comput. Aid. Parallel Program., V5, P1, DOI [10.5281/zenodo.3752498, DOI 10.5281/ZENODO.3752498]
   Majumder M. R., 2019, 22 INT C COMP INF TE, P1
   Matheson L, 2011, Doing Your Literature Review: Traditional and Systematic Techniques
   McNeese NJ, 2018, HUM FACTORS, V60, P262, DOI 10.1177/0018720817743223
   Mihajlovic M, 2021, PROC CVPR IEEE, P10456, DOI 10.1109/CVPR46437.2021.01032
   Mordvintsev Alexander, 2015, Google AI Blog: Inceptionism: Going Deeper into Neural Networks
   Neiva FW, 2016, INFORM SOFTWARE TECH, V72, P137, DOI 10.1016/j.infsof.2015.12.013
   Nguyen AT, 2018, UIST 2018: PROCEEDINGS OF THE 31ST ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P189, DOI 10.1145/3242587.3242666
   O'Reilly RC, 2014, COGNITIVE SCI, V38, P1229, DOI 10.1111/j.1551-6709.2011.01214.x
   Oh C, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174223
   openai.com, Better Language Models and Their Implications
   PAI Staff, 2019, Human-AI Collaboration Framework and Case Studies
   Plaisance Patrick L., 2019, Ethics and "Synthetic Media
   REES G, 2019, HERES DEEPFAKE TECHN
   Saito S, 2021, PROC CVPR IEEE, P2885, DOI 10.1109/CVPR46437.2021.00291
   Santos OC, 2014, SCI WORLD J, DOI 10.1155/2014/893525
   Schmidt J, 2019, NPJ COMPUT MATER, V5, DOI 10.1038/s41524-019-0221-0
   SCLAROFF S, 1991, COMP GRAPH, V25, P247, DOI 10.1145/127719.122745
   Seeber I, 2020, INFORM MANAGE-AMSTER, V57, DOI 10.1016/j.im.2019.103174
   Sennrich R, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1715
   So DR, 2019, PR MACH LEARN RES, V97
   Tanveer M, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3344998
   Tiwari Garvita, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P1, DOI 10.1007/978-3-030-58580-8_1
   Tiwari Garvita, 2021, ICCV, P11708
   Vincent James, 2019, New AI deepfake app creates nude images of women in seconds
   Wang D., 2019, PROC ACM HUMAN COMPU, V3, P1, DOI 10.1145/3359313
   Wang YQ, 2020, ACM COMPUT SURV, V53, DOI 10.1145/3386252
   Wei DN, 2020, Arxiv, DOI arXiv:2011.01381
   Weidt F., 2016, Systematic Literature Review in Computer Science-A Practical Guide, DOI [10.13140/RG.2.2.35453.87524, DOI 10.13140/RG.2.2.35453.87524]
   Williams D., 2016, The history of augmented reality (infografic)
   Wilson H. James, 2018, Humans and machines can enhance each other's strengths
   Xiang DL, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480545
   Yin WP, 2020, Arxiv, DOI arXiv:2007.09604
   Zhao TC, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P654, DOI 10.18653/v1/P17-1061
NR 79
TC 2
Z9 2
U1 36
U2 87
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2024
VL 20
IS 2
SI SI
AR 47
DI 10.1145/3542698
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W6GR4
UT WOS:001092595800017
DA 2024-08-05
ER

PT J
AU Cui, WX
   Wang, XT
   Fan, XP
   Liu, SH
   Gao, XW
   Zhao, DB
AF Cui, Wenxue
   Wang, Xingtao
   Fan, Xiaopeng
   Liu, Shaohui
   Gao, Xinwei
   Zhao, Debin
TI Deep Network for Image Compressed Sensing Coding Using Local Structural
   Sampling
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Compressed sensing (CS); compressed sensing coding; local structural
   sampling; convolutional neural network (CNN); third-party image codec
ID RECOVERY; ALGORITHM; FRAMEWORK
AB Existing image compressed sensing (CS) coding frameworks usually solve an inverse problem based on measurement coding and optimization-based image reconstruction, which still exist the following two challenges: (1) the widely used random sampling matrix, such as the Gaussian Random Matrix (GRM), usually leads to low measurement coding efficiency, and (2) the optimization-based reconstruction methods generally maintain a much higher computational complexity. In this article, we propose a new convolutional neural network based image CS coding framework using local structural sampling (dubbed CSCNet) that includes three functional modules: local structural sampling, measurement coding, and Laplacian pyramid reconstruction. In the proposed framework, instead of GRM, a new local structural sampling matrix is first developed, which is able to enhance the correlation between the measurements through a local perceptual sampling strategy. Besides, the designed local structural sampling matrix can be jointly optimized with the other functional modules during the training process. After sampling, the measurements with high correlations are produced, which are then coded into final bitstreams by the third-party image codec. Last, a Laplacian pyramid reconstruction network is proposed to efficiently recover the target image from the measurement domain to the image domain. Extensive experimental results demonstrate that the proposed scheme outperforms the existing state-of-the-art CS coding methods while maintaining fast computational speed.
C1 [Cui, Wenxue; Wang, Xingtao; Fan, Xiaopeng; Liu, Shaohui; Zhao, Debin] Harbin Inst Technol, Dept Comp Sci & Technol, West Dazhi St, Harbin 150001, Heilongjiang, Peoples R China.
   [Fan, Xiaopeng; Liu, Shaohui; Zhao, Debin] Peng Cheng Lab, Shenzhen, Guangdong, Peoples R China.
   [Gao, Xinwei] Tencent, Wechat Business Grp, Shenzhen, Guangdong, Peoples R China.
C3 Harbin Institute of Technology; Peng Cheng Laboratory; Tencent
RP Zhao, DB (corresponding author), Harbin Inst Technol, Dept Comp Sci & Technol, West Dazhi St, Harbin 150001, Heilongjiang, Peoples R China.; Zhao, DB (corresponding author), Peng Cheng Lab, Shenzhen, Guangdong, Peoples R China.
EM wxcui@hit.edu.cn; xtwang@hit.edu.cn; fxp@hit.edu.cn; shliu@hit.edu.cn;
   vitogao@tencent.com; dbzhao@hit.edu.cn
OI Cui, Wenxue/0000-0001-8656-0954
FU National Key R&D Program of China [2021YFF0900500]; National Natural
   Science Foundation of China (NSFC) [62302128, 62272128]
FX This work was supported in part by the National Key R&D Program of China
   (2021YFF0900500), and the National Natural Science Foundation of China
   (NSFC) under grants 62302128 and 62272128.
CR Agustsson E, 2019, IEEE I CONF COMP VIS, P221, DOI 10.1109/ICCV.2019.00031
   Candès EJ, 2008, IEEE SIGNAL PROC MAG, V25, P21, DOI 10.1109/MSP.2007.914731
   Chen B, 2022, IEEE T IMAGE PROCESS, V31, P5412, DOI 10.1109/TIP.2022.3195319
   Chen T, 2021, IEEE T IMAGE PROCESS, V30, P3179, DOI 10.1109/TIP.2021.3058615
   Chen Z, 2020, IEEE T CIRC SYST VID, V30, P1109, DOI 10.1109/TCSVT.2019.2898908
   Chen Z, 2018, IEEE T MULTIMEDIA, V20, P1610, DOI 10.1109/TMM.2017.2774004
   Cho W, 2020, IEEE T INF FOREN SEC, V15, P1999, DOI 10.1109/TIFS.2019.2953383
   Cui WX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1777, DOI 10.1145/3240508.3240706
   Daubechies I, 2004, COMMUN PUR APPL MATH, V57, P1413, DOI 10.1002/cpa.20042
   Dong WS, 2014, IEEE T IMAGE PROCESS, V23, P3618, DOI 10.1109/TIP.2014.2329449
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Duarte MF, 2008, IEEE SIGNAL PROC MAG, V25, P83, DOI 10.1109/MSP.2007.914730
   Ebrahim M, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2818712
   Everingham M., 2012, PASCAL VISUAL OBJECT
   Gan L, 2007, PROCEEDINGS OF THE 2007 15TH INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING, P403
   Gao XW, 2015, IEEE DATA COMPR CONF, P133, DOI 10.1109/DCC.2015.47
   Gao ZF, 2023, IEEE T MED IMAGING, V42, P1859, DOI 10.1109/TMI.2023.3240862
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   Huang G, 2013, IEEE IMAGE PROC, P2101, DOI 10.1109/ICIP.2013.6738433
   Jacques L, 2011, IEEE T INFORM THEORY, V57, P559, DOI 10.1109/TIT.2010.2093310
   Jia CM, 2019, IEEE J EM SEL TOP C, V9, P177, DOI 10.1109/JETCAS.2018.2886642
   Jiang F, 2018, IEEE T CIRC SYST VID, V28, P3007, DOI 10.1109/TCSVT.2017.2734838
   Dinh KQ, 2017, IEEE T CIRC SYST VID, V27, P2294, DOI 10.1109/TCSVT.2016.2587398
   Khanh Quoc Dinh, 2013, 2013 20th IEEE International Conference on Image Processing (ICIP), P10, DOI 10.1109/ICIP.2013.6738003
   Li ChunLong Li ChunLong, 2009, China Vegetables, P46
   Li M, 2021, IEEE T PATTERN ANAL, V43, P3446, DOI 10.1109/TPAMI.2020.2983926
   Li M, 2020, IEEE T IMAGE PROCESS, V29, P5900, DOI 10.1109/TIP.2020.2985225
   Li YH, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2903717
   Li Y, 2019, IEEE T IMAGE PROCESS, V28, P1092, DOI 10.1109/TIP.2018.2872876
   Li Y, 2018, IEEE T CIRC SYST VID, V28, P2316, DOI 10.1109/TCSVT.2017.2727682
   Lin JP, 2019, IEEE T CIRC SYST VID, V29, P3701, DOI 10.1109/TCSVT.2018.2884203
   Liu XM, 2016, IEEE T IMAGE PROCESS, V25, P2844, DOI 10.1109/TIP.2016.2554320
   Liu Zhi, 2022, IEEE Transactions on Instrumentation and Measurement, V72, P1
   Lustig M, 2008, IEEE SIGNAL PROC MAG, V25, P72, DOI 10.1109/MSP.2007.914728
   MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082
   Metzler CA, 2016, IEEE T INFORM THEORY, V62, P5117, DOI 10.1109/TIT.2016.2556683
   Mousavi A, 2015, ANN ALLERTON CONF, P1336, DOI 10.1109/ALLERTON.2015.7447163
   Mun S, 2012, EUR SIGNAL PR CONF, P1424
   Shi WZ, 2023, IEEE T INSTRUM MEAS, V72, DOI 10.1109/TIM.2023.3246523
   Shi WZ, 2021, IEEE T CIRC SYST VID, V31, P425, DOI 10.1109/TCSVT.2020.2978703
   Shi WZ, 2019, PROC CVPR IEEE, P12282, DOI 10.1109/CVPR.2019.01257
   Shi WZ, 2020, IEEE T IMAGE PROCESS, V29, P375, DOI 10.1109/TIP.2019.2928136
   Song JC, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4249, DOI 10.1145/3474085.3475562
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sun YB, 2020, IEEE T IMAGE PROCESS, V29, P9482, DOI 10.1109/TIP.2020.3023629
   Tang CQ, 2020, IEEE T INSTRUM MEAS, V69, P5287, DOI 10.1109/TIM.2019.2962562
   Tran Thuy T. T., 2020, IEEE INT WORKSH MULT, P1, DOI [10.1109/MMSP48831.2020.9287074, DOI 10.1109/mmsp48831.2020.9287074]
   Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108
   Wan RT, 2021, IEEE T MULTIMEDIA, V24, P3558, DOI 10.1109/TMM.2021.3102394
   Wang HK, 2023, IEEE T IMAGE PROCESS, V32, P2761, DOI 10.1109/TIP.2023.3274967
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Wright SJ, 2009, IEEE T SIGNAL PROCES, V57, P2479, DOI 10.1109/TSP.2009.2016892
   Wu DP, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2978570
   Xu HW, 2014, IEEE T INSTRUM MEAS, V63, P1253, DOI 10.1109/TIM.2013.2292359
   Xu K, 2018, LECT NOTES COMPUT SC, V11214, P491, DOI 10.1007/978-3-030-01249-6_30
   Yan WJ, 2014, IEEE T INSTRUM MEAS, V63, P1073, DOI 10.1109/TIM.2014.2298271
   Yang PH, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3447431
   Yang Z, 2013, IEEE T SIGNAL PROCES, V61, P2815, DOI 10.1109/TSP.2013.2256901
   Yao Hantao, 2017, P IEEE C COMP VIS PA
   You D, 2021, IEEE T IMAGE PROCESS, V30, P6066, DOI 10.1109/TIP.2021.3091834
   Yuan X, 2020, IEEE T MULTIMEDIA, V22, P2889, DOI 10.1109/TMM.2020.2967646
   Zhang J, 2020, IEEE J-STSP, V14, P765, DOI 10.1109/JSTSP.2020.2977507
   Zhang J, 2018, PROC CVPR IEEE, P1828, DOI 10.1109/CVPR.2018.00196
   Zhang J, 2013, IEEE IMAGE PROC, P1021, DOI 10.1109/ICIP.2013.6738211
   Zhang J, 2014, IEEE T IMAGE PROCESS, V23, P3336, DOI 10.1109/TIP.2014.2323127
   Zhang J, 2012, IEEE DATA COMPR CONF, P287, DOI 10.1109/DCC.2012.71
   Zhang ZH, 2021, IEEE T IMAGE PROCESS, V30, P1487, DOI 10.1109/TIP.2020.3044472
   Zhao C, 2016, IEEE DATA COMPR CONF, P161, DOI 10.1109/DCC.2016.104
   Zhao LJ, 2019, IEEE T CIRC SYST VID, V29, P2494, DOI 10.1109/TCSVT.2018.2867067
   Zhu SY, 2019, IEEE T CIRC SYST VID, V29, P1559, DOI 10.1109/TCSVT.2019.2895840
NR 70
TC 0
Z9 0
U1 5
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2024
VL 20
IS 7
SI SI
AR 212
DI 10.1145/3649441
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL0Q6
UT WOS:001234494100027
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Fu, HY
   Liu, J
   Yu, T
   Wang, X
   Ma, HD
AF Fu, Huiyuan
   Liu, Jin
   Yu, Ting
   Wang, Xin
   Ma, Huadong
TI Multi-Domain Image-to-Image Translation with Cross-Granularity
   Contrastive Learning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image-to-image translation; GAN; cross-granularity; contrastive
   learning; multi-domain
AB The objective of multi-domain image-to-image translation is to learn the mapping from a source domain to a target domain in multiple image domains while preserving the content representation of the source domain. Despite the importance and recent efforts, most previous studies disregard the large style discrepancy between images and instances in various domains, or fail to capture instance details and boundaries properly, resulting in poor translation results for rich scenes. To address these problems, we present an effective architecture for multi-domain image-to-image translation that only requires one generator. Specifically, we provide detailed procedures for capturing the features of instances throughout the learning process, as well as learning the relationship between the style of the global image and that of a local instance in the image by enforcing the cross-granularity consistency. In order to capture local details within the content space, we employ a dual contrastive learning strategy that operates at both the instance and patch levels. Extensive studies on different multi-domain image-to-image translation datasets reveal that our proposed method outperforms state-of-the-art approaches.
C1 [Fu, Huiyuan; Liu, Jin; Yu, Ting; Ma, Huadong] Beijing Univ Posts & Telecommun, State Key Lab Networking & Switching Technol, Beijing Key Lab Intelligent Telecommun Software &, Beijing, Beijing, Peoples R China.
   [Wang, Xin] SUNY Stony Brook, Dept Elect & Comp Engn, Stony Brook, NY USA.
C3 Beijing University of Posts & Telecommunications; State University of
   New York (SUNY) System; State University of New York (SUNY) Stony Brook
RP Ma, HD (corresponding author), Beijing Univ Posts & Telecommun, State Key Lab Networking & Switching Technol, Beijing Key Lab Intelligent Telecommun Software &, Beijing, Beijing, Peoples R China.
EM fhy@bupt.edu.cn; ljin@bupt.edu.cn; 2436904681@qq.com;
   x.wang@stonybrook.edu; mhd@bupt.edu.cn
OI Liu, Jin/0009-0008-5078-3302; Wang, Xin/0000-0001-8639-3818
FU NSFC [62272059]; National Key R&D Program of China [2023YFF0904800];
   Beijing Nova Program [20230484406]; Innovation Research Group Project of
   NSFC [61921003]; 111 Project [B18008]
FX This work is supported in part by the NSFC under No.62272059, the
   National Key R&D Program of China under No.2023YFF0904800, the Beijing
   Nova Program under No.20230484406, the Innovation Research Group Project
   of NSFC (61921003), and the 111 Project (B18008).
CR Arjovsky M, 2017, PR MACH LEARN RES, V70
   Bhattacharjee D, 2020, PROC CVPR IEEE, P4786, DOI 10.1109/CVPR42600.2020.00484
   Cai RC, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2060
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen T, 2020, PR MACH LEARN RES, V119
   Chen XL, 2021, PROC CVPR IEEE, P15745, DOI 10.1109/CVPR46437.2021.01549
   Chen YC, 2019, PROC CVPR IEEE, P2403, DOI 10.1109/CVPR.2019.00251
   Chen YS, 2018, PROC CVPR IEEE, P6306, DOI 10.1109/CVPR.2018.00660
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304
   Emami H, 2021, IEEE T MULTIMEDIA, V23, P391, DOI 10.1109/TMM.2020.2975961
   Fu HY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3099, DOI 10.1145/3394171.3413656
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Grill J-B., 2020, ADV NEURAL INFORM PR, V33, P21271, DOI DOI 10.48550/ARXIV.2006.07733
   Guo KH, 2024, ACM T MULTIM COMPUT, V20, DOI 10.1145/3604937
   Hensel M, 2017, ADV NEUR IN, V30
   Huang JL, 2022, IEEE T MULTIMEDIA, V24, P1435, DOI 10.1109/TMM.2021.3065230
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Gulrajani I, 2017, ADV NEUR IN, V30
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jeong S, 2021, PROC CVPR IEEE, P6554, DOI 10.1109/CVPR46437.2021.00649
   Jin YT, 2024, ACM T MULTIM COMPUT, V20, DOI 10.1145/3608952
   Kim K, 2022, PROC CVPR IEEE, P18218, DOI 10.1109/CVPR52688.2022.01770
   Kim T, 2017, PR MACH LEARN RES, V70
   Kingma D. P., 2014, arXiv
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Le Thi-Ngoc-Hanh, 2023, ACM Transactions on Multimedia Computing, Communications, and Applications, V19, P1
   Lee HY, 2020, INT J COMPUT VISION, V128, P2402, DOI 10.1007/s11263-019-01284-z
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Li C.-L., 2017, P ADV NEUR INF PROC
   Li JA, 2017, PROC CVPR IEEE, P1951, DOI 10.1109/CVPR.2017.211
   Liming Jiang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P206, DOI 10.1007/978-3-030-58580-8_13
   Liu K, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4664, DOI 10.1145/3394171.3416298
   Liu K, 2021, IEEE T CIRC SYST VID, V31, P647, DOI 10.1109/TCSVT.2020.2984569
   Liu K, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1490, DOI 10.1145/3343031.3350998
   Liu K, 2018, AAAI CONF ARTIF INTE, P7138
   Liu SH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6629, DOI 10.1109/ICCV48922.2021.00658
   Liu ZY, 2022, IEEE T CIRC SYST VID, V32, P7418, DOI 10.1109/TCSVT.2022.3188433
   Misra I, 2020, PROC CVPR IEEE, P6706, DOI 10.1109/CVPR42600.2020.00674
   Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5
   Pang YX, 2022, IEEE T MULTIMEDIA, V24, P3859, DOI 10.1109/TMM.2021.3109419
   Park T., 2020, EUR C COMP VIS, P319, DOI [DOI 10.1007/978-3-030-58545-719, 10.1007/978-3-030-58545-719, DOI 10.1007/978-3-030-58545-7_19]
   Pizzati F, 2021, PROC CVPR IEEE, P14283, DOI 10.1109/CVPR46437.2021.01406
   Radford L., 2016, Unsupervised representation learning with deep convolutional generative adversarial networks, P1
   Salimans T, 2016, ADV NEUR IN, V29
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Shen ZQ, 2019, PROC CVPR IEEE, P3678, DOI 10.1109/CVPR.2019.00380
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   van den Oord A, 2019, Arxiv, DOI [arXiv:1807.03748, DOI 10.48550/ARXIV.1807.03748]
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang XL, 2016, LECT NOTES COMPUT SC, V9908, P318, DOI 10.1007/978-3-319-46493-0_20
   Wang YX, 2021, IEEE T IMAGE PROCESS, V30, P670, DOI 10.1109/TIP.2020.3037528
   Xiao Jiayu, 2022, P IEEECVF C COMPUTER, P11204, DOI DOI 10.48550/ARXIV.2203.04121
   Xie SA, 2023, PROC CVPR IEEE, P10177, DOI 10.1109/CVPR52729.2023.00981
   Yang XW, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P374, DOI 10.1145/3240508.3240716
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Yu F, 2020, PROC CVPR IEEE, P2633, DOI 10.1109/CVPR42600.2020.00271
   Yunjey Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8185, DOI 10.1109/CVPR42600.2020.00821
   Zareapoor M, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3458280
   Zhang JQ, 2022, IEEE T CIRC SYST VID, V32, P1020, DOI 10.1109/TCSVT.2021.3071191
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang WL, 2019, IEEE I CONF COMP VIS, P3096, DOI 10.1109/ICCV.2019.00319
   Zhao YZ, 2023, IEEE T MULTIMEDIA, V25, P3017, DOI 10.1109/TMM.2022.3154600
   Zheng Jianwei, 2023, ACM Transactions on Multimedia Computing, Communications and Applications, V20, P1
   Zheng ZQ, 2022, IEEE T MULTIMEDIA, V24, P480, DOI 10.1109/TMM.2021.3053775
   Zhu JY, 2017, ADV NEUR IN, V30
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu JY, 2016, LECT NOTES COMPUT SC, V9909, P597, DOI 10.1007/978-3-319-46454-1_36
NR 68
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2024
VL 20
IS 7
SI SI
AR 228
DI 10.1145/3656048
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL0Q6
UT WOS:001234494100043
OA Bronze
DA 2024-08-05
ER

PT J
AU Li, ZY
   Shi, YX
   Ling, HF
   Chen, JZ
   Liu, BY
   Wang, RS
   Zhao, CX
AF Li, Zongyi
   Shi, Yuxuan
   Ling, Hefei
   Chen, Jiazhong
   Liu, Boyuan
   Wang, Runsheng
   Zhao, Chengxin
TI Viewpoint Disentangling and Generation for Unsupervised Object Re-ID
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Person re-identification; unsupervised; generation; disentanglement;
   viewpoint
ID INVARIANCE
AB Unsupervised object Re-ID aims to learn discriminative identity features from a fully unlabeled dataset to solve the open-class re-identification problem. Satisfying results have been achieved in existing unsupervised Re-ID methods, primarily trained with pseudo-labels created by feature clustering. However, the viewpoint variation of objects is the key challenge, introducing noisy labels in the clustering process. To address this problem, a novel viewpoint disentangling and generation framework (VDG) is proposed to learn viewpoint-invariant ID features, including a disentangling and generation module, as well as a contrastive learning module. First, we design an ID encoder to map the viewpoint and identity features into the latent space. Second, a generator is used to disentangle view features and synthesize images with different orientations. Especially, the well-trained encoder serves as a pre-trained feature extractor in the contrastive learning module. Third, a viewpoint-aware loss and a class-level loss are integrated to facilitate contrastive learning between original and novel views. The generation of novel view images and the application of viewpoint-aware contrastive loss mutually assist model learning viewpoint-invariant ID features. Extensive experiments on Market-1501, DukeMTMC, MSMT17, and VeRi-776 demonstrate the effectiveness of the proposed VDG framework, as well as its superiority over the existing state-of-the-art approaches. The VDG model also demonstrates high quality in the image generation tasks.
C1 [Li, Zongyi; Shi, Yuxuan; Ling, Hefei; Chen, Jiazhong; Liu, Boyuan; Wang, Runsheng; Zhao, Chengxin] Huazhong Univ Sci & Technol, Wuhan, Peoples R China.
C3 Huazhong University of Science & Technology
RP Shi, YX (corresponding author), Huazhong Univ Sci & Technol, Wuhan, Peoples R China.
EM zongyili@hust.edu.cn; shiyx@hust.edu.cn; lhefei@hust.edu.cn;
   jzchen@hust.edu.cn; leobryan@hust.edu.cn; wrsh@hust.edu.cn;
   chengxinzhao@hust.edu.cn
OI Shi, Yuxuan/0000-0001-7858-5369; Ling, Hefei/0000-0001-6797-7412; Zhao,
   Chengxin/0000-0001-8269-9799
FU Natural Science Foundation of China [61972169]; China Postdoctoral
   Science Foundation [2022M711251]; National key research and development
   program of China [2019QY(Y)0202, 2022YFB2601802]; Major Scientific and
   Technological Project of Hubei Province [2022BAA046, 2022BAA042];
   Research Programme on Applied Fundamentals and Frontier Technologies of
   Wuhan [2020010601012182]; Knowledge Innovation Program of Wuhan-Basic
   Research
FX This work was supported in part by the Natural Science Foundation of
   China under Grant 61972169, in part by China Postdoctoral Science
   Foundation 2022M711251, in part by the National key research and
   development program of China (2019QY(Y)0202, 2022YFB2601802), in part by
   the Major Scientific and Technological Project of Hubei Province
   (2022BAA046, 2022BAA042), in part by the Research Programme on Applied
   Fundamentals and Frontier Technologies of Wuhan (2020010601012182) and
   the Knowledge Innovation Program of Wuhan-Basic Research.
CR Abdal R, 2019, IEEE I CONF COMP VIS, P4431, DOI 10.1109/ICCV.2019.00453
   Chen H, 2023, IEEE T PATTERN ANAL, V45, P7494, DOI 10.1109/TPAMI.2022.3226866
   Chen H, 2021, PROC CVPR IEEE, P2004, DOI 10.1109/CVPR46437.2021.00204
   Chen YB, 2019, IEEE I CONF COMP VIS, P232, DOI 10.1109/ICCV.2019.00032
   Cho Y, 2022, PROC CVPR IEEE, P7298, DOI 10.1109/CVPR52688.2022.00716
   Chu RH, 2019, IEEE I CONF COMP VIS, P8281, DOI 10.1109/ICCV.2019.00837
   Dai YX, 2021, IEEE T IMAGE PROCESS, V30, P7815, DOI 10.1109/TIP.2021.3104169
   Deng WJ, 2018, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.2018.00110
   Ding YH, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3369393
   Dongkai Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10978, DOI 10.1109/CVPR42600.2020.01099
   Eom C, 2019, ADV NEUR IN, V32
   Fan HH, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3243316
   Fu Y, 2019, IEEE I CONF COMP VIS, P6111, DOI 10.1109/ICCV.2019.00621
   Ge Y., 2020, Advances in Neural Information Processing Systems, p11 309
   Ge YX, 2018, ADV NEUR IN, V31
   Ge YX, 2020, Arxiv, DOI [arXiv:2001.01526, 10.48550/arXiv.2001.01526]
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Gu XQ, 2022, PROC CVPR IEEE, P1050, DOI 10.1109/CVPR52688.2022.00113
   Han J, 2022, AAAI CONF ARTIF INTE, P790
   Han XM, 2023, IEEE T IMAGE PROCESS, V32, P29, DOI 10.1109/TIP.2022.3224325
   Harkonen E, 2020, C NEUR INF PROC SYST
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hensel M, 2017, ADV NEUR IN, V30
   Higgins I., 2017, INT C LEARN REPR
   Jiang SQ, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3391624
   Jianing Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P483, DOI 10.1007/978-3-030-58586-0_29
   Jiapeng Zhu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P592, DOI 10.1007/978-3-030-58520-4_35
   Kaiwei Zeng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13654, DOI 10.1109/CVPR42600.2020.01367
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Li YJ, 2019, IEEE I CONF COMP VIS, P7918, DOI 10.1109/ICCV.2019.00801
   Li ZY, 2022, AAAI CONF ARTIF INTE, P1527
   Lin YT, 2019, AAAI CONF ARTIF INTE, P8738
   Liu XC, 2016, LECT NOTES COMPUT SC, V9906, P869, DOI 10.1007/978-3-319-46475-6_53
   Lu ZF, 2023, IEEE T INTELL TRANSP, V24, P4333, DOI 10.1109/TITS.2022.3233565
   Mescheder L, 2018, PR MACH LEARN RES, V80
   Nitzan Y, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417826
   Pang B, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3485061
   Peng JJ, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P913
   Qian XL, 2018, LECT NOTES COMPUT SC, V11213, P661, DOI 10.1007/978-3-030-01240-3_40
   Richardson E, 2021, PROC CVPR IEEE, P2287, DOI 10.1109/CVPR46437.2021.00232
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Shen YJ, 2021, PROC CVPR IEEE, P1532, DOI 10.1109/CVPR46437.2021.00158
   Si TZ, 2023, IEEE T MULTIMEDIA, V25, P4323, DOI 10.1109/TMM.2022.3174414
   Simoudis E., 1996, KDD 96 P 2 INT C KNO, P226
   Singh KK, 2019, PROC CVPR IEEE, P6483, DOI 10.1109/CVPR.2019.00665
   Tov O, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459838
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang Huibing, 2021, P IEEE INT C MULTIME, P1
   Wang ML, 2022, IEEE T IMAGE PROCESS, V31, P6548, DOI 10.1109/TIP.2022.3213193
   Wang ML, 2021, AAAI CONF ARTIF INTE, V35, P2764
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wei R, 2023, IEEE T INTELL TRANSP, V24, P2935, DOI 10.1109/TITS.2022.3225025
   Xiao T, 2017, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2017.360
   Xuan SY, 2021, PROC CVPR IEEE, P11921, DOI 10.1109/CVPR46437.2021.01175
   Xuan Shiyu, 2022, IEEE Trans. Pattern Anal. Mach. Intell, V2022
   Yang F, 2021, IEEE T MULTIMEDIA, V23, P1681, DOI 10.1109/TMM.2020.3001522
   Yang Zou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P87, DOI 10.1007/978-3-030-58536-5_6
   Yunpeng Zhai, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9018, DOI 10.1109/CVPR42600.2020.00904
   Zhai Y., 2020, COMPUTER VISION ECCV, P594, DOI DOI 10.1007/978-3-030-58571-6_35
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang X, 2021, PROC CVPR IEEE, P3435, DOI 10.1109/CVPR46437.2021.00344
   Zhang XY, 2019, IEEE I CONF COMP VIS, P8221, DOI 10.1109/ICCV.2019.00831
   Zhang Y, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3542820
   Zhang YX, 2021, Arxiv, DOI arXiv:2010.09125
   Zhao ZW, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3491225
   Zheng AH, 2022, IEEE T INTELL TRANSP, V23, P11422, DOI 10.1109/TITS.2021.3103961
   Zheng Dingyuan, 2022, IEEE Trans. Circ. Syst. Vid. Technol, V2022
   Zheng KC, 2021, PROC CVPR IEEE, P5306, DOI 10.1109/CVPR46437.2021.00527
   Zheng KC, 2021, AAAI CONF ARTIF INTE, V35, P3538
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8351, DOI 10.1109/ICCV48922.2021.00826
   Zheng Y, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3501404
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2021, IEEE T PATTERN ANAL, V43, P2723, DOI 10.1109/TPAMI.2020.2976933
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhong Z, 2019, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2019.00069
   Zhong Z, 2019, IEEE T IMAGE PROCESS, V28, P1176, DOI 10.1109/TIP.2018.2874313
NR 79
TC 1
Z9 1
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2024
VL 20
IS 5
AR 141
DI 10.1145/3632959
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MF3R9
UT WOS:001192177900021
DA 2024-08-05
ER

PT J
AU Zeng, XH
   Wang, XY
   Xie, YC
AF Zeng, Xianhua
   Wang, Xinyu
   Xie, Yicai
TI Multiple Pseudo-Siamese Network with Supervised Contrast Learning for
   Medical Multi-modal Retrieval
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Pseudo-siamese network; supervised contrast learning; semantic
   consistency; multi-modal; medical image retrieval
AB Medical multi-modal retrieval aims to provide doctors with similar medical images from different modalities, which can greatly promote the efficiency and accuracy of clinical diagnosis. However, most existing medical retrieval methods hardly support the retrieval of multi-modal medical images, i.e., the number of modalities is greater than 2, and just convert retrieval to classification or clustering. It futilely breaks the gap between the visual information and the semantic information in different medical image modalities. To solve the problem, a Supervised Contrast Learning method based on a Multiple Pseudo-Siamese network (SCL-MPS) is proposed for multi-modal medical image retrieval. In order to make the samples with semantic similarity close neighbors on Riemann manifold, the multiple constraints based on semantic consistency and modal invariance are designed in different forward stages of SCL-MPS. We theoretically demonstrate the feasibility of the designed constraints. Finally, experiments on four benchmark datasets (ADNI1, ADNI2, ADNI3, and OASIS3) show that SCL-MPS achieves state-of-the-art performance compared to 15 retrieval methods. Especially, SCL-MPS achieves a 100% mAP score in medical cross-modal retrieval on ADNI1.
C1 [Zeng, Xianhua; Wang, Xinyu] Chongqing Univ Posts & Telecommun, Coll Comp Sci & Technol, 2 Chongwen Rd,Nanshan Ave, Chongqing 400065, Peoples R China.
   [Wang, Xinyu] Chongqing Mil Ind Grp Co LTD, Chongqing, Peoples R China.
   [Xie, Yicai] Gannan Normal Univ, Sch Math & Comp Sci, CHIShida South Rd, Ganzhou City 341000, Jiangxi, Peoples R China.
C3 Chongqing University of Posts & Telecommunications; Gannan Normal
   University
RP Zeng, XH (corresponding author), Chongqing Univ Posts & Telecommun, Coll Comp Sci & Technol, 2 Chongwen Rd,Nanshan Ave, Chongqing 400065, Peoples R China.
EM zengxh@cqupt.edu.cn; wangxinyu@cqcmi.com; 31585040@qq.com
OI Zeng, Xianhua/0000-0001-5892-2372
FU National Natural Science Foundation of China [62076044]; Chongqing
   Talent Plan Project [cstc2022ycjh-bgzxm0160]; Chongqing Graduate
   Research Innovation Project of China [CYS21307]; Doctoral Program of
   Chongqing University of Posts and Telecommunications [BYJS202013]
FX The authors are grateful to the anonymous editors and reviewers for
   their advice that improved this paper. This work was supported by The
   National Natural Science Foundation of China (Grant No. 62076044), The
   Chongqing Talent Plan Project (Grant No. cstc2022ycjh-bgzxm0160), The
   Chongqing Graduate Research Innovation Project of China (Grant No.
   CYS21307) and The Doctoral Program of Chongqing University of Posts and
   Telecommunications (Grant No. BYJS202013).
CR Andrew R., 2013, INT C MACHINE LEARNI, P1247, DOI DOI 10.5555/3042817.3043076
   Belkin M, 2002, ADV NEUR IN, V14, P585
   Cao Y, 2014, CANCER INFORM, V13, P125, DOI 10.4137/CIN.S14053
   Chen T, 2020, PR MACH LEARN RES, V119
   Chen XL, 2021, PROC CVPR IEEE, P15745, DOI 10.1109/CVPR46437.2021.01549
   Chen YX, 2020, NEUROCOMPUTING, V385, P111, DOI 10.1016/j.neucom.2019.12.078
   Cheng YH, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3499027
   Fan LX, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P825
   Fang JS, 2021, MED IMAGE ANAL, V69, DOI 10.1016/j.media.2021.101981
   Feng DD, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3580501
   Gao Y, 2015, LECT NOTES COMPUT SC, V9350, P86, DOI 10.1007/978-3-319-24571-3_11
   Hadsell R., 2006, Computer vision and pattern recognition, 2006 IEEE computer society conference on, V2, P1735, DOI DOI 10.1109/CVPR.2006.100
   Han K, 2023, IEEE T PATTERN ANAL, V45, P87, DOI 10.1109/TPAMI.2022.3152247
   Han N, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3483381
   Jack CR, 2008, J MAGN RESON IMAGING, V27, P685, DOI 10.1002/jmri.21049
   Khosla P., 2020, Adv. Neural Inf. Process. Syst, P18661, DOI DOI 10.48550/ARXIV.2004.11362
   Kim M, 2020, ADV NEUR IN, V33
   Kingma D. P., 2014, arXiv
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   LaMontagne PJ., 2019, medRxiv, V15, P2019
   Li Q, 2017, ADV NEUR IN, V30
   Li T, 2022, IEEE SIGNAL PROC LET, V29, P827, DOI 10.1109/LSP.2022.3157517
   Li WJ, 2016, Arxiv, DOI arXiv:1511.03855
   Litjens G, 2017, MED IMAGE ANAL, V42, P60, DOI 10.1016/j.media.2017.07.005
   Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227
   Ma XH, 2020, IEEE T MULTIMEDIA, V22, P3101, DOI 10.1109/TMM.2020.2969792
   Mo Yujie, 2023, INT C MACH LEARN
   Peng L, 2024, IEEE T NEUR NET LEAR, V35, P8609, DOI 10.1109/TNNLS.2022.3230979
   Peng Yuxin, 2016, IJCAI, P3846
   Qayyum A, 2017, NEUROCOMPUTING, V266, P8, DOI 10.1016/j.neucom.2017.05.025
   Safaei A, 2021, MED BIOL ENG COMPUT, V59, P1993, DOI 10.1007/s11517-021-02392-0
   Shen FM, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1522, DOI 10.1145/3123266.3123345
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vikram M, 2019, PROCEEDINGS OF THE 6TH ACM IKDD CODS AND 24TH COMAD, P44, DOI 10.1145/3297001.3297007
   Wang XY, 2023, SCI REP-UK, V13, DOI 10.1038/s41598-023-29320-6
   Wu DY, 2019, PROC CVPR IEEE, P9061, DOI 10.1109/CVPR.2019.00928
   Wu L, 2019, IEEE T IMAGE PROCESS, V28, P1602, DOI 10.1109/TIP.2018.2878970
   Xu LM, 2022, IEEE T IMAGE PROCESS, V31, P3371, DOI 10.1109/TIP.2022.3171081
   Yan CG, 2021, IEEE T PATTERN ANAL, V43, P1445, DOI 10.1109/TPAMI.2020.2975798
   Yanagi R, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3485042
   Yang EK, 2021, IEEE T MED IMAGING, V40, P503, DOI 10.1109/TMI.2020.3030752
   Yang J, 2004, IEEE T PATTERN ANAL, V26, P131, DOI 10.1109/TPAMI.2004.1261097
   Yang W, 2018, IEEE T MED IMAGING, V37, P977, DOI 10.1109/TMI.2018.2790962
   Yuan L, 2020, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR42600.2020.00315
   Zhang FF, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3478642
   Zhang Jiwei, 2023, ACM Transactions on Multimedia Computing, Communications and Applications, V19, P1
   Zhen LL, 2019, PROC CVPR IEEE, P10386, DOI 10.1109/CVPR.2019.01064
   Zheng F, 2018, IEEE T PATTERN ANAL, V40, P1059, DOI 10.1109/TPAMI.2016.2645565
   Zhou Y, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P4325, DOI 10.1109/ICASSP39728.2021.9414247
   Zhu Han, 2016, P AAAI C ART INT, V30
   Zhu H, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3567
NR 52
TC 0
Z9 0
U1 2
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2024
VL 20
IS 5
AR 128
DI 10.1145/3637441
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MF3R9
UT WOS:001192177900008
DA 2024-08-05
ER

PT J
AU Zhou, ML
   Zhao, XW
   Luo, FT
   Luo, J
   Pu, HY
   Xiang, T
AF Zhou, Mingliang
   Zhao, Xinwen
   Luo, Futing
   Luo, Jun
   Pu, Huayan
   Xiang, Tao
TI Robust RGB-T Tracking via Adaptive Modality Weight Correlation Filters
   and Cross-modality Learning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Correlation filters; RGBT tracking; adaptive modality weight;
   cross-modality learning
ID OBJECT TRACKING; NETWORK
AB RGBT tracking is gaining popularity due to its ability to provide effective tracking results in a variety of weather conditions. However, feature specificity and complementarity have not been fully used in existing models that directly fuse the correlation filtering response, which leads to poor tracker performance. In this article, we propose correlation filters with adaptive modality weight and cross-modality learning (AWCM) ability to solve multimodality tracking tasks. First, we use weighted activation to fuse thermal infrared and visible modalities, and the fusion modality is used as an auxiliary modality to suppress noise and increase the learning ability of shared modal features. Second, we design modal weights through average peak-to-correlation energy coefficients to improvemodel reliability. Third, we propose consistency in using the fusion modality as an intermediate variable for joint learning consistency, thereby increasing tracker robustness via interactive cross-modal learning. Finally, we use the alternating direction method of multipliers algorithm to produce a closed solution and conduct extensive experiments on the RGBT234, VOT-TIR2019, and GTOT tracking benchmark datasets to demonstrate the superior performance of the proposed AWCM against compared to existing tracking algorithms. The code developed in this study is available at the following website.(1)
C1 [Zhou, Mingliang; Zhao, Xinwen; Luo, Futing; Xiang, Tao] Chongqing Univ, Sch Comp Sci, Chongqing 400044, Peoples R China.
   [Luo, Jun; Pu, Huayan] Chongqing Univ, Coll Mech Engn, State Key Lab Mech & Transmiss, Chongqing 400044, Peoples R China.
C3 Chongqing University; Chongqing University
RP Luo, J (corresponding author), Chongqing Univ, Coll Mech Engn, State Key Lab Mech & Transmiss, Chongqing 400044, Peoples R China.
EM mingliangzhou@cqu.edu.cn; 202114131110@cqu.edu.cn; lft@cqu.edu.cn;
   luojun@cqu.edu.cn; Phygood_2001@shu.edu.cn; txiang@cqu.edu.cn
RI Zhou, Mingliang/HPC-0298-2023; Xiang, Tao/N-3706-2016
OI Xiang, Tao/0000-0002-9439-4623; Zhou, Mingliang/0000-0002-1874-3641
FU National Natural Science Foundation of China [62176027]; Chongqing
   Talent [CQYC20220511986]; Joint Equipment Pre Research and Key Fund
   Project of the Ministry of Education [8091B012207]; Natural Science
   Foundation of Chongqing, China [cstc2020jcyj-zdxmX0014]; Human Resources
   and Social Security Bureau Project of Chongqing [cx2020073]; Guangdong
   Oppo Mobile Telecommunications Corporation Ltd. [H20221694]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62176027; in part by the Chongqing
   Talent under Grant CQYC20220511986; in part by the Joint Equipment Pre
   Research and Key Fund Project of the Ministry of Education under Grant
   8091B012207; in part by the Natural Science Foundation of Chongqing,
   China under Grant cstc2020jcyj-zdxmX0014; in part by the Human Resources
   and Social Security Bureau Project of Chongqing under Grant cx2020073;
   and in part by the Guangdong Oppo Mobile Telecommunications Corporation
   Ltd., under Grant H20221694.
CR Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Chenglong Li, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P222, DOI 10.1007/978-3-030-58542-6_14
   Choi J, 2017, PROC CVPR IEEE, P4828, DOI 10.1109/CVPR.2017.513
   Chu BF, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3557896
   Dai KN, 2019, PROC CVPR IEEE, P4665, DOI 10.1109/CVPR.2019.00480
   Danelljan M., 2014, BRIT MACH VIS C NOTT, DOI DOI 10.5244/C.28.65
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   ECKSTEIN J, 1992, MATH PROGRAM, V55, P293, DOI 10.1007/BF01581204
   Feng MZ, 2022, KNOWL-BASED SYST, V249, DOI 10.1016/j.knosys.2022.108945
   Feng MZ, 2020, J VIS COMMUN IMAGE R, V72, DOI 10.1016/j.jvcir.2020.102881
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1144, DOI [10.1109/ICCV.2017.129, 10.1109/ICCV.2017.128]
   Gao Y., 2019, P IEEE CVF INT C COM
   Hare S, 2016, IEEE T PATTERN ANAL, V38, P2096, DOI 10.1109/TPAMI.2015.2509974
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Huang B, 2020, IEEE T MULTIMEDIA, V22, P2820, DOI 10.1109/TMM.2020.2965482
   Kang B, 2023, IEEE T NEUR NET LEAR, V34, P9900, DOI 10.1109/TNNLS.2022.3161969
   Kim HU, 2015, IEEE I CONF COMP VIS, P3011, DOI 10.1109/ICCV.2015.345
   Kristan M., 2019, P IEEE CVF INT C COM, P0
   Lan XY, 2019, IEEE T IND ELECTRON, V66, P9887, DOI 10.1109/TIE.2019.2898618
   Lan XY, 2020, PATTERN RECOGN LETT, V130, P12, DOI 10.1016/j.patrec.2018.10.002
   Lan XY, 2018, AAAI CONF ARTIF INTE, P7008
   Li CL, 2019, IEEE INT CONF COMP V, P2262, DOI 10.1109/ICCVW.2019.00279
   Li CL, 2019, PATTERN RECOGN, V96, DOI 10.1016/j.patcog.2019.106977
   Li CL, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1856, DOI 10.1145/3123266.3123289
   Li CL, 2018, LECT NOTES COMPUT SC, V11217, P831, DOI 10.1007/978-3-030-01261-8_49
   Li CL, 2016, IEEE T IMAGE PROCESS, V25, P5743, DOI 10.1109/TIP.2016.2614135
   Li F, 2018, PROC CVPR IEEE, P4904, DOI 10.1109/CVPR.2018.00515
   Li Y, 2015, LECT NOTES COMPUT SC, V8926, P254, DOI 10.1007/978-3-319-16181-5_18
   Liu HP, 2012, SCI CHINA INFORM SCI, V55, P590, DOI 10.1007/s11432-011-4536-9
   Liu Q, 2021, IEEE T MULTIMEDIA, V23, P2114, DOI 10.1109/TMM.2020.3008028
   Lu AD, 2021, IEEE T IMAGE PROCESS, V30, P5613, DOI 10.1109/TIP.2021.3087341
   Lukezic A, 2017, PROC CVPR IEEE, P4847, DOI 10.1109/CVPR.2017.515
   Ma B, 2015, IEEE T MULTIMEDIA, V17, P1818, DOI 10.1109/TMM.2015.2463221
   Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465
   OConaire Ciaran, 2006, P 14 INT C INFORM FU
   Ruan WJ, 2019, IEEE T MULTIMEDIA, V21, P1122, DOI 10.1109/TMM.2018.2872897
   Shen JB, 2022, IEEE T PATTERN ANAL, V44, P8896, DOI 10.1109/TPAMI.2021.3127492
   Shen JB, 2020, IEEE T CYBERNETICS, V50, P3068, DOI 10.1109/TCYB.2019.2936503
   Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531
   Wang MM, 2017, PROC CVPR IEEE, P4800, DOI 10.1109/CVPR.2017.510
   Wang N, 2018, PROC CVPR IEEE, P4844, DOI 10.1109/CVPR.2018.00509
   Wang Y, 2022, NEURAL COMPUT APPL, V34, P5757, DOI 10.1007/s00521-021-06704-1
   Wang YL, 2018, LECT NOTES COMPUT SC, V11259, P295, DOI 10.1007/978-3-030-03341-5_25
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Xia WD, 2022, NEUROCOMPUTING, V493, P327, DOI 10.1016/j.neucom.2022.04.017
   Xiao Y, 2022, AAAI CONF ARTIF INTE, P2831
   Xu Q, 2022, IEEE T MULTIMEDIA, V24, P567, DOI 10.1109/TMM.2021.3055362
   Xu TY, 2021, INT J COMPUT VISION, V129, P1359, DOI 10.1007/s11263-021-01435-1
   Xu TY, 2019, IEEE I CONF COMP VIS, P7949, DOI 10.1109/ICCV.2019.00804
   Yang K, 2022, IEEE T MULTIMEDIA, V24, P1956, DOI 10.1109/TMM.2021.3074239
   Yang Rui, 2019, P IEEE INT C IMAGE P
   Yao R, 2017, IEEE T MULTIMEDIA, V19, P772, DOI 10.1109/TMM.2016.2631727
   YiWu Erik Blasch, 2011, P 14 INT C INFORM FU
   Yuan D, 2022, NEUROCOMPUTING, V491, P44, DOI 10.1016/j.neucom.2022.03.055
   Yuan D, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3486678
   Yuan D, 2021, IEEE T IMAGE PROCESS, V30, P976, DOI 10.1109/TIP.2020.3037518
   Yuan D, 2020, J VIS COMMUN IMAGE R, V72, DOI 10.1016/j.jvcir.2020.102882
   Yuan D, 2020, KNOWL-BASED SYST, V195, DOI 10.1016/j.knosys.2020.105697
   Zhai SL, 2019, NEUROCOMPUTING, V334, P172, DOI 10.1016/j.neucom.2019.01.022
   Zhang JM, 2014, LECT NOTES COMPUT SC, V8694, P188, DOI 10.1007/978-3-319-10599-4_13
   Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472
   Zhao L, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21175800
   Zhong W, 2012, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2012.6247882
   Zhu XF, 2021, IEEE T CIRC SYST VID, V31, P557, DOI 10.1109/TCSVT.2020.2979480
   Zhu YB, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P465, DOI 10.1145/3343031.3350928
   Zuo WM, 2019, IEEE T PATTERN ANAL, V41, P1158, DOI 10.1109/TPAMI.2018.2829180
NR 70
TC 0
Z9 0
U1 20
U2 20
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2024
VL 20
IS 4
AR 95
DI 10.1145/3630100
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6G9
UT WOS:001153382100005
DA 2024-08-05
ER

PT J
AU Wang, Y
   Li, PZ
   Si, QY
   Zhang, HW
   Zang, WY
   Lin, Z
   Fu, P
AF Wang, Yan
   Li, Peize
   Si, Qingyi
   Zhang, Hanwen
   Zang, Wenyu
   Lin, Zheng
   Fu, Peng
TI Cross-modality Multiple Relations Learning for Knowledge-based Visual
   Question Answering
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Cross-modality relation; external knowledge; visual question answering
AB Knowledge-based visual question answering not only needs to answer the questions based on images but also incorporates external knowledge to study reasoning in the joint space of vision and language. To bridge the gap between visual content and semantic cues, it is important to capture the question-related and semantics-rich vision-language connections. Most existing solutions model simple intra-modality relation or represent cross-modality relation using a single vector, which makes it difficult to effectively model complex connections between visual features and question features. Thus, we propose a cross-modality multiple relations learning model, aiming to better enrich cross-modality representations and construct advanced multi-modality knowledge triplets. First, we design a simple yet effective method to generate multiple relations that represent the rich cross-modality relations. The various cross-modality relations link the textual question to the related visual objects. These multi-modality triplets efficiently align the visual objects and corresponding textual answers. Second, to encourage multiple relations to better align with different semantic relations, we further formulate a novel global-local loss. The global loss enables the visual objects and corresponding textual answers close to each other through cross-modality relations in the vision-language space, and the local loss better preserves semantic diversity among multiple relations. Experimental results on the Outside Knowledge VQA and Knowledge-Routed Visual Question Reasoning datasets demonstrate that our model outperforms the state-of-the-art methods.
C1 [Wang, Yan] Jilin Univ, Coll Comp Sci & Technol, Sch Artificial Intelligence, Changchun 130012, Peoples R China.
   [Wang, Yan] Jilin Univ, Coll Comp Sci & Technol, Minist Educ, Key Lab Symbol Comp & Knowledge Engn, Changchun 130012, Peoples R China.
   [Li, Peize] Jilin Univ, Sch Artificial Intelligence, Changchun 130012, Peoples R China.
   [Si, Qingyi; Zhang, Hanwen; Lin, Zheng; Fu, Peng] Chinese Acad Sci, Inst Informat Engn, Beijing 100049, Peoples R China.
   [Si, Qingyi; Zhang, Hanwen; Lin, Zheng; Fu, Peng] Univ Chinese Acad Sci, Sch Cyber Secur, Beijing 100049, Peoples R China.
   [Zang, Wenyu] China Elect Corp, Beijing 100846, Peoples R China.
C3 Jilin University; Jilin University; Jilin University; Chinese Academy of
   Sciences; Institute of Information Engineering, CAS; Chinese Academy of
   Sciences; University of Chinese Academy of Sciences, CAS; China
   Electronics Corporation
RP Lin, Z; Fu, P (corresponding author), Chinese Acad Sci, Inst Informat Engn, Beijing 100049, Peoples R China.; Lin, Z; Fu, P (corresponding author), Univ Chinese Acad Sci, Sch Cyber Secur, Beijing 100049, Peoples R China.
EM wy6868@jlu.edu.cn; lipz21@mails.jlu.edu.cn; siqingyi@iie.ac.cn;
   zhanghanwen@iie.ac.cn; wenyuzang@sina.com; linzheng@iie.ac.cn;
   fupeng@iie.ac.cn
RI Zhang, Hanwen/KLD-5748-2024
FU National Natural Science Foundation of China [62072212, 61906187,
   61976207, 61902394]; Development Project of Jilin Province of China
   [20220508125RC, 20230201065GX, 20200401083GX, 2020C003, 20200403172SF];
   National Key RD Program [2018YFC2001302]; Jilin Provincial Key
   Laboratory of Big Data Intelligent Cognition [20210504003GH]
FX This work is supported by the National Natural Science Foundation of
   China (Nos. 62072212, 61906187, 61976207, and 61902394), the Development
   Project of Jilin Province of China (Nos. 20220508125RC, 20230201065GX,
   20200401083GX, 2020C003, and 20200403172SF), the National Key R&D
   Program (No. 2018YFC2001302), and the Jilin Provincial Key Laboratory of
   Big Data Intelligent Cognition (No. 20210504003GH).
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   [Anonymous], 2017, WIKIPEDIA FREE E
   Auer S, 2007, LECT NOTES COMPUT SC, V4825, P722, DOI 10.1007/978-3-540-76298-0_52
   Ben-Younes H, 2019, AAAI CONF ARTIF INTE, P8102
   Ben-younes H, 2017, IEEE I CONF COMP VIS, P2631, DOI 10.1109/ICCV.2017.285
   Cadene R, 2019, PROC CVPR IEEE, P1989, DOI 10.1109/CVPR.2019.00209
   Cao QX, 2022, IEEE T NEUR NET LEAR, V33, P2758, DOI 10.1109/TNNLS.2020.3045034
   Ding Y, 2022, PROC CVPR IEEE, P5079, DOI 10.1109/CVPR52688.2022.00503
   Fan HH, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3390891
   Gardères F, 2020, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, EMNLP 2020, P489
   Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670
   hasPartKB, 2004, hasPartKB: A New Knowledge Base of hasPart Relations
   Jang E., 2017, ICLR (Poster)
   Jin WK, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3321505
   Kim JH, 2018, ADV NEUR IN, V31
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Li GH, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1227, DOI 10.1145/3394171.3413943
   Li Liunian Harold, 2020, P 58 ANN M ASS COMP, P5265, DOI DOI 10.18653/V1/2020.ACL-MAIN.469
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu H, 2004, BT TECHNOL J, V22, P211, DOI 10.1023/B:BTTJ.0000047600.45421.6d
   Liu YB, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3498340
   Loshchilov I., 2017, ARXIV
   Lu JS, 2019, ADV NEUR IN, V32
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Marino K, 2021, PROC CVPR IEEE, P14106, DOI 10.1109/CVPR46437.2021.01389
   Marino K, 2019, PROC CVPR IEEE, P3190, DOI 10.1109/CVPR.2019.00331
   Mithun NC, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1856, DOI 10.1145/3240508.3240712
   Pan YH, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3487042
   Paszke A, 2019, ADV NEUR IN, V32
   Perez E, 2018, AAAI CONF ARTIF INTE, P3942
   Qu C, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1753, DOI 10.1145/3404835.3462987
   Rebele T, 2016, LECT NOTES COMPUT SC, V9982, P177, DOI 10.1007/978-3-319-46547-0_19
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sergieh H. M., 2018, P 7 JOINT C LEX COMP, P225, DOI DOI 10.18653/V1/S18-2027
   Shevchenko Violetta, 2021, P 3RDWORKSHOP VISION
   Shu XB, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P35, DOI 10.1145/2733373.2806216
   Tan H, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5100
   Tandon N, 2014, AAAI CONF ARTIF INTE, P166
   Tang JH, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2998574
   Tian Y., 2020, Advances in neural information processing systems, V33, P6827, DOI DOI 10.5555/3495724.3496297
   Wang S, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3314577
   Wang WN, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2020.107248
   Wu JL, 2022, AAAI CONF ARTIF INTE, P2712
   Xu T, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3416493
   Yu DF, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3316767
   Yu J, 2020, PATTERN RECOGN, V108, DOI 10.1016/j.patcog.2020.107563
   Yu Z, 2018, IEEE T NEUR NET LEAR, V29, P5947, DOI 10.1109/TNNLS.2018.2817340
   Yu Z, 2019, PROC CVPR IEEE, P6274, DOI 10.1109/CVPR.2019.00644
   Zha ZJ, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3320061
   Zhang PC, 2021, PROC CVPR IEEE, P5575, DOI 10.1109/CVPR46437.2021.00553
   Zheng WB, 2021, KDD '21: PROCEEDINGS OF THE 27TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2360, DOI 10.1145/3447548.3467285
   Zheng WB, 2021, INFORM FUSION, V67, P14, DOI 10.1016/j.inffus.2020.10.007
   Zhu ZH, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1097
NR 53
TC 0
Z9 0
U1 17
U2 17
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2024
VL 20
IS 3
AR 63
DI 10.1145/3618301
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6F8
UT WOS:001153381000003
DA 2024-08-05
ER

PT J
AU Liu, J
   Zhou, JT
   Tian, JY
   Sun, WW
AF Liu, Jun
   Zhou, Jiantao
   Tian, Jinyu
   Sun, Weiwei
TI Recoverable Privacy-preserving Image Classification through Noise-like
   Adversarial Examples
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Privacy-preserving; image classification; encryption; deep neural
   networks
AB With the increasing prevalence of cloud computing platforms, ensuring data privacy during the cloud-based image-related services such as classification has become crucial. In this study, we propose a novel privacy-preserving image classification scheme that enables the direct application of classifiers trained in the plaintext domain to classify encrypted images without the need of retraining a dedicated classifier. Moreover, encrypted images can be decrypted back into their original form with high fidelity (recoverable) using a secret key. Specifically, our proposed scheme involves utilizing a feature extractor and an encoder to mask the plaintext image through a newly designed Noise-like Adversarial Example (NAE). Such an NAE not only introduces a noise-like visual appearance to the encrypted image but also compels the target classifier to predict the ciphertext as the same label as the original plaintext image. At the decoding phase, we adopt a Symmetric Residual Learning (SRL) framework for restoring the plaintext image with minimal degradation. Extensive experiments demonstrate that (1) the classification accuracy of the classifier trained in the plaintext domain remains the same in both the ciphertext and plaintext domains; (2) the encrypted images can be recovered into their original form with an average PSNR of up to 51+ dB for the SVHN dataset and 48+ dB for the VG-GFace2 dataset; (3) our system exhibits satisfactory generalization capability on the encryption, decryption, and classification tasks across datasets that are different from the training one; and (4) a high-level of security is achieved against three potential threat models. The code is available at https://github.com/csjunjun/RIC.git.
C1 [Liu, Jun; Zhou, Jiantao] Univ Macau, Fac Sci & Technol, Dept Comp & Informat Sci, State Key Lab Internet Things Smart City, Univ Ave, Taipa 999078, Macau, Peoples R China.
   [Tian, Jinyu] Macau Univ Sci & Technol, Fac Innovat Engn, Sch Comp Sci & Engn, Weilong Rd, Taipa 999078, Macau, Peoples R China.
   [Sun, Weiwei] Alibaba Grp, 699 Wangshang Rd, Hangzhou, Zhejiang, Peoples R China.
C3 University of Macau; Macau University of Science & Technology; Alibaba
   Group
RP Zhou, JT (corresponding author), Univ Macau, Fac Sci & Technol, Dept Comp & Informat Sci, State Key Lab Internet Things Smart City, Univ Ave, Taipa 999078, Macau, Peoples R China.
EM yc07453@umac.mo; jtzhou@umac.mo; jytian@must.edu.mo;
   sunweiwei.sww@alibaba-inc.com
OI Liu, Jun/0000-0003-1167-5727
FU Macau Science and Technology Development Fund [SKLIOTSC-2024-2026,
   0072/2020/AMJ, 0022/2022/A1, 0014/2022/AFJ]; Research Committee at
   University of Macau [MYRG202200152-FST, MYRG-GRG2023-00058-FST-UMDF];
   Natural Science Foundation of China [61971476]; Alibaba Group through
   Alibaba Innovative Research Program
FX This work was supported in part by Macau Science and Technology
   Development Fund under SKLIOTSC-2024-2026, 0072/2020/AMJ, 0022/2022/A1,
   and 0014/2022/AFJ; in part by Research Committee at University of Macau
   under MYRG202200152-FST and MYRG-GRG2023-00058-FST-UMDF; in part by
   Natural Science Foundation of China under 61971476; and in part by
   Alibaba Group through Alibaba Innovative Research Program.
CR Abadi M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P308, DOI 10.1145/2976749.2978318
   Al-Rubaie M, 2019, IEEE SECUR PRIV, V17, P49, DOI 10.1109/MSEC.2018.2888775
   An SHY, 2020, Arxiv, DOI [arXiv:2008.10400, DOI 10.48550/ARXIV.2008.10400]
   Bayatbabolghani F, 2018, PROCEEDINGS OF THE 2018 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (CCS'18), P2157, DOI 10.1145/3243734.3264419
   Bian S., 2020, P IEEE CVF C COMP VI, P9403
   Cai JR, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P112, DOI 10.1109/ICCV48922.2021.00018
   Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Chen Aaron, 2023, Pytorch-playground
   Chen JX, 2018, SIGNAL PROCESS, V142, P340, DOI 10.1016/j.sigpro.2017.07.034
   Chen ZN, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3231742
   Cheng KL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1971, DOI 10.1109/ICCV48922.2021.00200
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding XF, 2022, IEEE T KNOWL DATA EN, V34, P1967, DOI 10.1109/TKDE.2020.2997604
   Dong H, 2018, IEEE T INF FOREN SEC, V13, P662, DOI 10.1109/TIFS.2017.2763126
   Dong YP, 2018, PROC CVPR IEEE, P9185, DOI 10.1109/CVPR.2018.00957
   Dowlin N, 2016, PR MACH LEARN RES, V48
   Duan J, 2021, IEEE T SERV COMPUT, V14, P1940, DOI 10.1109/TSC.2019.2911282
   Dwork C, 2008, LECT NOTES COMPUT SC, V4978, P1, DOI 10.1007/978-3-540-79228-4_1
   Gentry C., 2009, FULLY HOMOMORPHIC EN, P43
   Goodfellow IJ, 2015, P 3 INT C LEARNING R
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang Yangsibo, 2020, ICML 20, P4507
   Hubara I, 2016, ADV NEUR IN, V29
   Ilyas Andrew, 2018, P MACHINE LEARNING R, V80
   Ji JZ, 2022, LECT NOTES COMPUT SC, V13672, P475, DOI 10.1007/978-3-031-19775-8_28
   Jing JP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4713, DOI 10.1109/ICCV48922.2021.00469
   Johnson NF, 1998, COMPUTER, V31, P26, DOI 10.1109/MC.1998.4655281
   Juvekar C, 2018, PROCEEDINGS OF THE 27TH USENIX SECURITY SYMPOSIUM, P1651
   Kasyap H, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3426474
   Kingma D. P., 2014, arXiv
   Krizhevsky A., 2012, Learning multiple layers of features from tiny images, DOI DOI 10.1145/3079856.3080246
   Liu J, 2017, CCS'17: PROCEEDINGS OF THE 2017 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P619, DOI 10.1145/3133956.3134056
   Liu ZY, 2023, IEEE T INF FOREN SEC, V18, P1839, DOI 10.1109/TIFS.2022.3163592
   Madry A., 2018, INT C LEARN REPR, P1
   Mi YX, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P6755, DOI 10.1145/3503161.3548303
   Mireshghallah F, 2021, PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE 2021 (WWW 2021), P669, DOI 10.1145/3442381.3449965
   Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282
   Nadimpalli AV, 2022, IEEE COMPUT SOC CONF, P91, DOI 10.1109/CVPRW56347.2022.00019
   Neshatavar R, 2022, PROC CVPR IEEE, P17562, DOI 10.1109/CVPR52688.2022.01706
   Netzer Y., 2011, P NIPS WORKSH DEEP L, P7
   Osia SA, 2020, IEEE INTERNET THINGS, V7, P4505, DOI 10.1109/JIOT.2020.2967734
   Osia SA, 2020, IEEE T KNOWL DATA EN, V32, P54, DOI 10.1109/TKDE.2018.2878698
   Pidhorskyi S., 2020, P IEEE CVF C COMP VI, P14092
   Pintelas P., 2006, GESTS International Transactions on Computer Science and Engineering, V30, P25
   Ribeiro M, 2015, 2015 IEEE 14TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA), P896, DOI 10.1109/ICMLA.2015.152
   Rivest R. L., 1978, FDN SECURE COMPUTATI, V4, P169, DOI DOI 10.4067/S0716-078X2003000400011
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Salim S, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3537899
   Sanyal A., 2018, In: 35th International Conference on Machine Learning, P4490
   Sarmadi A, 2024, IEEE T DEPEND SECURE, V21, P486, DOI 10.1109/TDSC.2023.3263507
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shamsabadi AS, 2020, IEEE T INF FOREN SEC, V15, P3819, DOI 10.1109/TIFS.2020.2988132
   Wang J, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2407, DOI 10.1145/3219819.3220106
   Wu Y, 2013, INFORM SCIENCES, V222, P323, DOI 10.1016/j.ins.2012.07.049
   Xu K, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3566125
   Yang Q, 2019, ACM T INTEL SYST TEC, V10, DOI 10.1145/3298981
   Yang ZZ, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3446618
   Yao Andrew C., 1982, P ANN S FDN COMP SCI
   Yuan J, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3578518
   Zhang C., 2020, Adv. Neural Inf. Process. Syst., V33, P10223
   Zhang HW, 2021, IEEE T INF FOREN SEC, V16, P701, DOI 10.1109/TIFS.2020.3021899
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang YL, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P19, DOI [10.1109/SP.2017.12, 10.1145/3132747.3132768]
   Zhao LC, 2021, IEEE T PARALL DISTR, V32, P2524, DOI 10.1109/TPDS.2021.3068195
   Zill Dennis G, 2009, Multivariable Calculus
NR 66
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2024
VL 20
IS 7
SI SI
AR 216
DI 10.1145/3653676
PG 27
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL0Q6
UT WOS:001234494100031
OA Bronze, Green Submitted
DA 2024-08-05
ER

PT J
AU Bisogni, C
   Cascone, L
   Nappi, M
   Pero, C
AF Bisogni, Carmen
   Cascone, Lucia
   Nappi, Michele
   Pero, Chiara
TI IoT-enabled Biometric Security: Enhancing Smart Car Safety with
   Depth-based Head Pose Estimation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Head pose estimation; depth images; driver attention; HPE; fractal
   encoding; landmark prediction; data fusion
ID OPTIMIZATION
AB Advanced Driver Assistance Systems (ADAS) are experiencing higher levels of automation, facilitated by the synergy among various sensors integrated within vehicles, thereby forming an Internet of Things (IoT) framework. Among these sensors, cameras have emerged as valuable tools for detecting driver fatigue and distraction. This study introduces HYDE-F, a Head Pose Estimation (HPE) system exclusively utilizing depth cameras. HYDE-F adeptly identifies critical driver head poses associated with risky conditions, thus enhancing the safety of IoT-enabled ADAS. The core of HYDE-F's innovation lies in its dual-process approach: it employs a fractal encoding technique and keypoint intensity analysis in parallel. These two processes are then fused using an optimization algorithm, enabling HYDE-F to blend the strengths of bothmethods for enhanced accuracy. Evaluations conducted on a specialized driving dataset, Pandora, demonstrate HYDE-F's competitive performance compared to existing methods, surpassing current techniques in terms of average Mean Absolute Error (MAE) by nearly 1 degrees. Moreover, case studies highlight the successful integration of HYDE-F with vehicle sensors. Additionally, HYDE-F exhibits robust generalization capabilities, as evidenced by experiments conducted on standard laboratory-based HPE datasets, i.e., Biwi and ICT-3DHP databases, achieving an average MAE of 4.9 degrees and 5 degrees, respectively.
C1 [Bisogni, Carmen; Cascone, Lucia; Nappi, Michele; Pero, Chiara] Univ Salerno, Via Giovanni Paolo 2,132, Salerno 84084, Italy.
C3 University of Salerno
RP Bisogni, C (corresponding author), Univ Salerno, Via Giovanni Paolo 2,132, Salerno 84084, Italy.
EM cbisogni@unisa.it; lcascone@unisa.it; mnappi@unisa.it; cpero@unisa.it
RI Cascone, Lucia/ABV-5620-2022; Pero, Chiara/HMD-7334-2023
OI Cascone, Lucia/0000-0002-9333-5699; Bisogni, Carmen/0000-0003-1358-006X;
   Nappi, Michele/0000-0002-2517-2867; Pero, Chiara/0000-0002-5517-2198
CR Aghaei AS, 2016, IEEE SIGNAL PROC MAG, V33, P35, DOI 10.1109/MSP.2016.2602379
   Alioua N., 2014, International journal of vehicular technology, V2014, P1, DOI [10.1155/2014/678786, DOI 10.1155/2014/678786]
   Alioua N, 2016, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-016-0103-z
   [Anonymous], 1991, Fractals for the Classroom: Strategic Activities Volume One, VOne
   Baltrusaitis T, 2012, PROC CVPR IEEE, P2610, DOI 10.1109/CVPR.2012.6247980
   Bisogni C, 2021, INT C PATT RECOG, P1725, DOI 10.1109/ICPR48806.2021.9413227
   Bisogni C, 2021, IEEE T IMAGE PROCESS, V30, P3192, DOI 10.1109/TIP.2021.3059409
   Borghi G, 2020, IEEE T PATTERN ANAL, V42, P596, DOI 10.1109/TPAMI.2018.2885472
   Borghi G, 2017, PROC CVPR IEEE, P5494, DOI 10.1109/CVPR.2017.583
   BOX MJ, 1965, COMPUT J, V8, P42, DOI 10.1093/comjnl/8.1.42
   Cai Q, 2010, LECT NOTES COMPUT SC, V6313, P229
   Camgöz NC, 2015, SIG PROCESS COMMUN, P1997, DOI 10.1109/SIU.2015.7130256
   Chen Q, 2019, SEC'19: PROCEEDINGS OF THE 4TH ACM/IEEE SYMPOSIUM ON EDGE COMPUTING, P88, DOI 10.1145/3318216.3363300
   El Khatib A, 2020, IEEE T INTELL TRANSP, V21, P4483, DOI 10.1109/TITS.2019.2940874
   Fanelli G, 2011, P JOINT PATT REC S
   Gao FC, 2012, COMPUT OPTIM APPL, V51, P259, DOI 10.1007/s10589-010-9329-3
   Gong X, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON IDENTITY, SECURITY AND BEHAVIOR ANALYSIS (ISBA)
   Hu TC, 2022, IEEE T INTELL TRANSP, V23, P8063, DOI 10.1109/TITS.2021.3075350
   Hu TC, 2020, IEEE INT VEH SYM, P1176, DOI [10.1109/iv47402.2020.9304592, 10.1109/IV47402.2020.9304592]
   Huo XQ, 2016, IEEE IJCNN, P897, DOI 10.1109/IJCNN.2016.7727294
   Jacquin A, 1989, FRACTAL THEORY ITERA
   Ju JP, 2022, INFRARED PHYS TECHN, V123, DOI 10.1016/j.infrared.2022.104146
   Kazemi V, 2014, PROC CVPR IEEE, P1867, DOI 10.1109/CVPR.2014.241
   Kolda TG, 2003, SIAM REV, V45, P385, DOI [10.1137/S003614450242889, 10.1137/S0036144502428893]
   Li CL, 2018, MULTIMED TOOLS APPL, V77, P14605, DOI 10.1007/s11042-017-5050-x
   Liu WH, 2019, FUTURE INTERNET, V11, DOI 10.3390/fi11050115
   Ma XT, 2021, J CIRCUIT SYST COMP, V30, DOI 10.1142/S0218126621501395
   Ma XT, 2020, IEEE IMAGE PROC, P2840, DOI [10.1109/icip40778.2020.9191082, 10.1109/ICIP40778.2020.9191082]
   Masanovic L, 2019, 2019 ZOOMING INNOVATION IN CONSUMER TECHNOLOGIES CONFERENCE (ZINC), P33, DOI [10.1109/zinc.2019.8769377, 10.1109/ZINC.2019.8769377]
   Parkinson J. M., 1972, Conference on numerical methods for non-linear optimization, P115
   Pietruch M, 2022, IEEE ACCESS, V10, P106110, DOI 10.1109/ACCESS.2022.3211942
   Rahman A, 2015, 2015 NATIONAL SOFTWARE ENGINEERING CONFERENCE (NSEC), P1, DOI 10.1109/NSEC.2015.7396336
   Ribeiro RF, 2019, IEEE INT CONF AUTOMA, P678
   Ross Arun, 2009, Springer eBooks, P611, DOI DOI 10.1007/978-0-387-73003-5_158
   Sarshar M, 2010, ANN IEEE SYST CONF, P529, DOI 10.1109/SYSTEMS.2010.5482319
   Shahbakhti M, 2022, IEEE J BIOMED HEALTH, V26, P1001, DOI 10.1109/JBHI.2021.3096984
   Sheng L, 2017, PROC CVPR IEEE, P4598, DOI 10.1109/CVPR.2017.489
   Soultana Abdelfettah, 2022, International Journal of Interactive Mobile Technologies, P160, DOI 10.3991/ijim.v16i16.33075
   Wang Jie, 2017, [Computational Visual Media, 计算可视媒体], V3, P229
   Xiao SH, 2020, INT CONF ACOUST SPEE, P1883, DOI [10.1109/ICASSP40776.2020.9053370, 10.1109/icassp40776.2020.9053370]
   Xing Y, 2018, IEEE T COMPUT SOC SY, V5, P95, DOI 10.1109/TCSS.2017.2766884
   Zhang F, 2017, 2017 INTERNATIONAL CONFERENCE ON MACHINE VISION AND INFORMATION TECHNOLOGY (CMVIT), P105, DOI 10.1109/CMVIT.2017.25
   Zhao ZP, 2020, COMPUT INTEL NEUROSC, V2020, DOI 10.1155/2020/9606908
NR 43
TC 1
Z9 1
U1 1
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2024
VL 20
IS 6
SI SI
AR 155
DI 10.1145/3639367
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OQ2T3
UT WOS:001208681800005
OA hybrid
DA 2024-08-05
ER

PT J
AU Liu, XY
   Hua, ZY
   Yi, SA
   Zhang, YS
   Zhou, YC
AF Liu, Xingyu
   Hua, Zhongyun
   Yi, Shuang
   Zhang, Yushu
   Zhou, Yicong
TI Bi-directional Block Encoding for Reversible Data Hiding over Encrypted
   Images
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image encoding; reversible data hiding; encrypted image
ID SECURITY
AB Reversible data hiding over encrypted images (RDH-EI) technology is a viable solution for privacy-preserving cloud storage, as it enables the reversible embedding of additional data into images while maintaining image confidentiality. Since the data hiders, e.g., cloud servers, are willing to embed as much data as possible for storage, management, or other processing purposes, a large embedding capacity is desirable in an RDH-EI scheme. In this article, we introduce a novel bi-directional block encoding (BDBE) method, which, for the first time, encodes the distances of values in a binary sequence from both ends. This approach allows for encoding images with smaller sizes compared to traditional and state-of-the-art encoding methods. Leveraging the BDBE technique, we propose a high-capacity RDH-EI scheme. In this scheme, the content owner initially predicts the image pixels and then employs BDBE to encode the prediction errors, creating space for data embedding. The resulting encoded data are subsequently encrypted using a secure stream cipher, such as the Advanced Encryption Standard, before being transmitted to a data hider. The data hider can embed confidential information within the encrypted image for the purposes of storage, management, or other processing. Upon receiving the data, an authorized receiver can accurately recover the original image and the embedded data without any loss. Experimental results demonstrate that our RDH-EI scheme achieves a significantly larger embedding capacity compared to several state-of-the-art schemes.
C1 [Liu, Xingyu] Harbin Inst Technol, Shenzhen, Peoples R China.
   [Yi, Shuang] Southwest Univ Polit Sci & Law, Chongqing, Peoples R China.
   Nanjing Univ Aeronaut & Astronaut, Nanjing, Peoples R China.
   [Zhou, Yicong] Univ Macau, Macau, Peoples R China.
   [Liu, Xingyu; Hua, Zhongyun] Harbin Inst Technol, Sch Comp Sci & Technol, Shenzhen 518055, Guangdong, Peoples R China.
   [Yi, Shuang] Southwest Univ Polit Sci & Law, Coll Criminal Invest, Chongqing Educ Comm, Engn Res Ctr Forens Sci, Chongqing 401120, Chongqing, Peoples R China.
   [Zhang, Yushu] Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing 211106, Peoples R China.
   [Zhou, Yicong] Univ Macau, Dept Comp & Informat Sci, Macau 999078, Peoples R China.
C3 Harbin Institute of Technology; Southwest University of Political
   Science & Law - China; Nanjing University of Aeronautics & Astronautics;
   University of Macau; Harbin Institute of Technology; Southwest
   University of Political Science & Law - China; Nanjing University of
   Aeronautics & Astronautics; University of Macau
RP Hua, ZY (corresponding author), Harbin Inst Technol, Sch Comp Sci & Technol, Shenzhen 518055, Guangdong, Peoples R China.
EM liuxyuh@gmail.com; huazyum@gmail.com; yishuang@swupl.edu.cn;
   yushu@nuaa.edu.cn; yicongzhou@um.edu.mo
RI Hua, Zhongyun/F-1887-2016; Zhou, Yicong/A-8017-2009
OI Zhou, Yicong/0000-0002-4487-6384; , Shuang/0000-0002-2186-7995
FU National Natural Science Foundation of China [62071142, 62002301];
   Guangdong Basic and Applied Basic Research Foundation [2021A1515011406];
   Science and Technology Research Program of Chongqing Municipal Education
   Commission [KJQN202200303, KJQN201900310]; Research Foundation of
   Southwest University of Political Science and Law [2018XZQN-31];
   Foundation for Science and Technology Innovation of Shenzhen
   [RCBS20210609103708014]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 62071142 and 62002301, the Guangdong Basic and
   Applied Basic Research Foundation under Grant 2021A1515011406, the
   Science and Technology Research Program of Chongqing Municipal Education
   Commission under Grants KJQN202200303 and KJQN201900310, the Research
   Foundation of Southwest University of Political Science and Law under
   Grant 2018XZQN-31, and the Foundation for Science and Technology
   Innovation of Shenzhen under Grant RCBS20210609103708014.
CR Benesty J, 2008, IEEE T AUDIO SPEECH, V16, P757, DOI 10.1109/TASL.2008.919072
   Bhardwaj R, 2020, PATTERN RECOGN LETT, V139, P60, DOI 10.1016/j.patrec.2018.01.014
   Chen F, 2021, IEEE T CIRC SYST VID, V31, P905, DOI 10.1109/TCSVT.2020.2992817
   Chen KM, 2019, J VIS COMMUN IMAGE R, V58, P334, DOI 10.1016/j.jvcir.2018.12.023
   Guan B, 2020, J VIS COMMUN IMAGE R, V66, DOI 10.1016/j.jvcir.2019.102744
   Ke Y, 2022, IEEE T CIRC SYST VID, V32, P2469, DOI 10.1109/TCSVT.2021.3081575
   Khelifi F, 2018, SIGNAL PROCESS, V143, P336, DOI 10.1016/j.sigpro.2017.09.020
   Kong L, 2021, IEEE T POWER DELIVER, V36, P1428, DOI 10.1109/TPWRD.2020.3008924
   Li Q, 2018, MULTIMED TOOLS APPL, V77, P30749, DOI 10.1007/s11042-018-6187-y
   Liu ZL, 2022, IEEE T DEPEND SECURE, V19, P1382, DOI 10.1109/TDSC.2020.3011838
   Ma KD, 2013, IEEE T INF FOREN SEC, V8, P553, DOI 10.1109/TIFS.2013.2248725
   Masud M, 2020, COMPUT COMMUN, V152, P215, DOI 10.1016/j.comcom.2020.01.050
   Mohammadi A, 2020, IEEE T CIRC SYST VID, V30, P2366, DOI 10.1109/TCSVT.2020.2990952
   Puech W, 2008, PROC SPIE, V6819, DOI 10.1117/12.766754
   Puteaux P, 2021, IEEE T MULTIMEDIA, V23, P636, DOI 10.1109/TMM.2020.2985537
   Qian ZX, 2016, IEEE T CIRC SYST VID, V26, P636, DOI 10.1109/TCSVT.2015.2418611
   Qiu YQ, 2022, IEEE T CIRC SYST VID, V32, P5874, DOI 10.1109/TCSVT.2022.3163905
   Qu LF, 2022, IEEE T MULTIMEDIA, V24, P2924, DOI 10.1109/TMM.2021.3090588
   Shen M, 2020, FUTURE GENER COMP SY, V109, P621, DOI 10.1016/j.future.2018.04.089
   Weng SW, 2021, J VIS COMMUN IMAGE R, V75, DOI 10.1016/j.jvcir.2020.102932
   WITTEN IH, 1987, COMMUN ACM, V30, P520, DOI 10.1145/214762.214771
   Wu HT, 2019, J VIS COMMUN IMAGE R, V62, P87, DOI 10.1016/j.jvcir.2019.04.015
   Wu XL, 1997, IEEE T COMMUN, V45, P437, DOI 10.1109/26.585919
   Wu ZB, 2021, IEEE T CYBERNETICS, V51, P3588, DOI 10.1109/TCYB.2020.3026673
   Xia ZH, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3492705
   Xia ZH, 2022, IEEE T SERV COMPUT, V15, P202, DOI 10.1109/TSC.2019.2927215
   Xiong LZ, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3512797
   Xu S, 2023, IEEE T DEPEND SECURE, V20, P4199, DOI 10.1109/TDSC.2022.3219843
   Yi S, 2019, IEEE T MULTIMEDIA, V21, P51, DOI 10.1109/TMM.2018.2844679
   Yi S, 2017, SIGNAL PROCESS, V133, P40, DOI 10.1016/j.sigpro.2016.10.017
   Yi S, 2015, 2015 IEEE CHINA SUMMIT & INTERNATIONAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING, P225, DOI 10.1109/ChinaSIP.2015.7230396
   Yin ZX, 2022, IEEE T DEPEND SECURE, V19, P992, DOI 10.1109/TDSC.2020.3019490
   Yin ZX, 2020, IEEE T MULTIMEDIA, V22, P874, DOI 10.1109/TMM.2019.2936314
   Zhang WM, 2014, SIGNAL PROCESS, V94, P118, DOI 10.1016/j.sigpro.2013.06.023
   Zhang XP, 2011, IEEE SIGNAL PROC LET, V18, P255, DOI 10.1109/LSP.2011.2114651
   Zhou JT, 2016, IEEE T CIRC SYST VID, V26, P441, DOI 10.1109/TCSVT.2015.2416591
NR 36
TC 0
Z9 0
U1 12
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2024
VL 20
IS 5
AR 149
DI 10.1145/3638771
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MF3R9
UT WOS:001192177900029
OA Bronze
DA 2024-08-05
ER

PT J
AU Zhou, S
   Guo, D
   Yang, X
   Dong, JF
   Wang, M
AF Zhou, Sheng
   Guo, Dan
   Yang, Xun
   Dong, Jianfeng
   Wang, Meng
TI Graph Pooling Inference Network for Text-based VQA
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Text-based visual question answering; graph inference; graph pooling
AB Effectively leveraging objects and optical character recognition (OCR) tokens to reason out pivotal scene text is critical for the challenging Text-based Visual Question Answering (TextVQA) task. Graph-based models can effectively capture the semantic relationship among visual entities (objects and tokens) and report remarkable performance in TextVQA. However, previous efforts usually leverage all visual entities and ignore the negative effect of superfluous entities. This article presents a Graph Pooling Inference Network (GPIN), which is an evolutionary graph learning method to purify the visual entities and capture the core semantics. It is observed that the dense distribution of reduplicative objects and the crowd of semantically dependent OCR tokens usually co-exist in the image. Motivated by this, GPIN adopts an adaptive node dropping strategy to dynamically downscale semantically closed nodes for graph evolution and update. To deepen the comprehension of scene text, GPIN is a dual-path hierarchical graph architecture that progressively aggregates the evolved object graph and the evolved token graph semantics into a graph vector that serves as visual cues to facilitate the answer reasoning. It can effectively eliminate object redundancy and enhance the association of semantically continuous tokens. Experiments conducted on TextVQA and ST-VQA datasets show that GPIN achieves promising performance compared with state-of-the-art methods.
C1 [Zhou, Sheng; Guo, Dan; Wang, Meng] HeFei Univ Technol, 485 Danxia Rd, Hefei 230601, Anhui, Peoples R China.
   [Yang, Xun] Univ Sci & Technol China, 96 JinZhai Rd, Hefei 230026, Anhui, Peoples R China.
   [Dong, Jianfeng] Zhejiang Gongshang Univ, 18,Xuecheng St,Xiasha Higher Educ Pk, Hangzhou 310018, Zhejiang, Peoples R China.
C3 Hefei University of Technology; Chinese Academy of Sciences; University
   of Science & Technology of China, CAS; Zhejiang Gongshang University
RP Guo, D; Wang, M (corresponding author), HeFei Univ Technol, 485 Danxia Rd, Hefei 230601, Anhui, Peoples R China.; Yang, X (corresponding author), Univ Sci & Technol China, 96 JinZhai Rd, Hefei 230026, Anhui, Peoples R China.
EM hzgn97@gmail.com; guodan@hfut.edu.cn; xyang21@ustc.edu.cn;
   dongjf24@gmail.com; eric.mengwang@gmail.com
OI YANG, Xun/0000-0003-0201-1638; Guo, Dan/0000-0003-2594-254X; Dong,
   Jianfeng/0000-0001-5244-3274
FU National Key Research and Development Program of China [2022YFB4500600];
   National Natural Science Foundation of China (NSFC) [62020106007,
   62272144, U20A20183, 72188101, 62272435, U22A2094]; Major Project of
   Anhui Province [202203a05020011]; University Synergy Innovation Program
   of Anhui Province [GXXT-2022-047]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2022YFB4500600; in part by the
   National Natural Science Foundation of China (NSFC) under Grant
   62020106007, Grant 62272144, Grant U20A20183, Grant 72188101, Grant
   62272435, and Grant U22A2094; in part by the Major Project of Anhui
   Province under Grant 202203a05020011; and in part by the University
   Synergy Innovation Program of Anhui Province under Grant GXXT-2022-047.
CR Almazán J, 2014, IEEE T PATTERN ANAL, V36, P2552, DOI 10.1109/TPAMI.2014.2339814
   Biten AF, 2022, PROC CVPR IEEE, P16527, DOI 10.1109/CVPR52688.2022.01605
   Biten AF, 2019, IEEE I CONF COMP VIS, P4290, DOI 10.1109/ICCV.2019.00439
   Bojanowski Piotr, 2017, T ASSOC COMPUT LING, V5, P135, DOI [10.48550/arXiv.1607.04606, DOI 10.1162/TACLA00051]
   Borisyuk F, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P71, DOI 10.1145/3219819.3219861
   Chen CF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P347, DOI 10.1109/ICCV48922.2021.00041
   Chen DL, 2020, AAAI CONF ARTIF INTE, V34, P3438
   Dan Guo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10052, DOI 10.1109/CVPR42600.2020.01007
   Dancette C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1554, DOI 10.1109/ICCV48922.2021.00160
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Ding Y, 2022, PROC CVPR IEEE, P5079, DOI 10.1109/CVPR52688.2022.00503
   Dong JF, 2022, IEEE T PATTERN ANAL, V44, P4065, DOI 10.1109/TPAMI.2021.3059295
   Fang Chengyang, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P4378, DOI 10.1145/3581783.3611753
   Gao CY, 2022, IEEE T PATTERN ANAL, V44, P9603, DOI 10.1109/TPAMI.2021.3132034
   Gao D., 2020, P IEEECVF C COMPUTER, P12746, DOI 10.1109/CVPR42600.2020.01276
   Gao F, 2022, PROC CVPR IEEE, P5057, DOI 10.1109/CVPR52688.2022.00501
   Guo D, 2022, IEEE T PATTERN ANAL, V44, P6056, DOI 10.1109/TPAMI.2021.3085755
   Gurari D, 2018, PROC CVPR IEEE, P3608, DOI 10.1109/CVPR.2018.00380
   Han Wei, 2020, ARXIV201002582, P3118, DOI DOI 10.18653/V1/2020.COLING-MAIN.278
   Han XZ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1564, DOI 10.1109/ICCV48922.2021.00161
   Hegde Shamanthak, 2023, P IEEE CVF C COMP VI, P5579
   Heo YJ, 2022, PROCEEDINGS OF THE 60TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), VOL 1: (LONG PAPERS), P373
   Hu J, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3505, DOI 10.1145/3394171.3413711
   Hu R., 2020, P IEEE CVF C COMP VI, DOI DOI 10.1109/CVPR42600.2020.01001
   Jing Wang, 2020, MM '20: Proceedings of the 28th ACM International Conference on Multimedia, P4337, DOI 10.1145/3394171.3413753
   JunWang Mingfei Gao, 2022, P BRIT MACH VIS C BM
   Kant Yash, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P715, DOI 10.1007/978-3-030-58545-7_41
   Karatzas D, 2015, PROC INT CONF DOC, P1156, DOI 10.1109/ICDAR.2015.7333942
   Karatzas D, 2013, PROC INT CONF DOC, P1484, DOI 10.1109/ICDAR.2013.221
   Krasin I., 2017, Openimages: A public dataset for large-scale multi-label and multi-class image classification, P18
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   LEVENSHT.VI, 1965, DOKL AKAD NAUK SSSR+, V163, P845
   Li Bingjia, 2022, P AS C COMP VIS ACCV, P4143
   Li H, 2023, IEEE T IMAGE PROCESS, V32, P3367, DOI 10.1109/TIP.2023.3276570
   Li JC, 2022, PROC CVPR IEEE, P3022, DOI 10.1109/CVPR52688.2022.00304
   Li XP, 2022, PATTERN RECOGN, V124, DOI 10.1016/j.patcog.2021.108455
   Li Yicong, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P3172, DOI 10.1145/3581783.3612577
   Liu F, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4060, DOI 10.1145/3394171.3413924
   Liu YL, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3052
   Long SB, 2022, PROC CVPR IEEE, P1039, DOI 10.1109/CVPR52688.2022.00112
   Lu XP, 2021, IEEE INT CONF COMP V, P2631, DOI 10.1109/ICCVW54120.2021.00297
   Mishra A, 2013, IEEE I CONF COMP VIS, P3040, DOI 10.1109/ICCV.2013.378
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sidorov O., 2020, EUR C COMP VIS, DOI DOI 10.1007/978-3-030-58536-544
   Singh A, 2019, PROC CVPR IEEE, P8309, DOI 10.1109/CVPR.2019.00851
   Song Peipei, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P589, DOI 10.1145/3581783.3611726
   Tu ZZ, 2022, LECT NOTES COMPUT SC, V13684, P459, DOI 10.1007/978-3-031-20053-3_27
   Veit A, 2016, Arxiv, DOI arXiv:1601.07140
   Wang YA, 2023, Arxiv, DOI arXiv:2205.11501
   Wu JR, 2022, PR MACH LEARN RES
   Yang Michael, 2021, P IEEE C COMP VIS PA
   Yang X, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1, DOI 10.1145/3404835.3462823
   Yang X, 2022, IEEE T IMAGE PROCESS, V31, P1204, DOI 10.1109/TIP.2022.3140611
   Yang X, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P1339, DOI 10.1145/3397271.3401151
   Yang ZY, 2021, PROC CVPR IEEE, P8747, DOI 10.1109/CVPR46437.2021.00864
   Zeng GY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P376, DOI 10.1145/3474085.3475606
   Zhang L, 2020, WEB CONFERENCE 2020: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2020), P3098, DOI 10.1145/3366423.3380083
   Zhang WQ, 2022, AAAI CONF ARTIF INTE, P3335
   Zhang XY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2519, DOI 10.1145/3474085.3475425
   Zheng Q, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3543857
   Zhou S, 2023, IEEE T IMAGE PROCESS, V32, P5060, DOI 10.1109/TIP.2023.3310332
   Zhu Q, 2021, AAAI CONF ARTIF INTE, V35, P3608
   Zhu Yongxin, 2023, P AAAI C ART INT AAA, P11479
NR 64
TC 3
Z9 3
U1 9
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2024
VL 20
IS 4
AR 112
DI 10.1145/3634918
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6G9
UT WOS:001153382100022
DA 2024-08-05
ER

PT J
AU Chen, BA
   Chen, ZL
   Hu, XW
   Xu, J
   Xie, HR
   Qin, J
   Wei, MQ
AF Chen, Baian
   Chen, Zhilei
   Hu, Xiaowei
   Xu, Jun
   Xie, Haoran
   Qin, Jing
   Wei, Mingqiang
TI Dynamic Message Propagation Network for RGB-D and Video Salient Object
   Detection
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE RGB-D salient object detection; dynamic message propagation; crossmodal
   learning; depth feature propagation
ID SEGMENTATION; FUSION
AB Exploiting long-range semantic contexts and geometric information is crucial to infer salient objects from RGB and depth features. However, existing methods mainly focus on excavating local features within fixed regions by continuously feeding forward networks. In this article, we introduce Dynamic Message Propagation (DMP) to dynamically learn context information within more flexible regions. We integrate DMP into a Siamese-based network to process the RGB image and depth map separately and design a multi-level feature fusion module to explore cross-level information between refined RGB and depth features. Extensive experiments show clear improvements of our method over 17 methods on six benchmark datasets for RGB-D salient object detection (SOD). Additionally, our method outperforms its competitors for the video SOD task. Code is available at https://github.com/chenbaian-cs/DMPNet.
C1 [Chen, Baian; Chen, Zhilei; Wei, Mingqiang] Nanjing Univ Aeronaut & Astronaut, Shenzhen Res Inst, Shenzhen, Guangdong, Peoples R China.
   [Hu, Xiaowei] Shanghai AI Lab, Shanghai, Peoples R China.
   [Xu, Jun] Nankai Univ, Tianjin, Peoples R China.
   [Xie, Haoran] Lingnan Univ, Hong Kong, Peoples R China.
   [Qin, Jing] Hong Kong Polytech Univ, Hong Kong, Peoples R China.
C3 Nanjing University of Aeronautics & Astronautics; Shanghai Artificial
   Intelligence Laboratory; Nankai University; Lingnan University; Hong
   Kong Polytechnic University
RP Wei, MQ (corresponding author), Nanjing Univ Aeronaut & Astronaut, Shenzhen Res Inst, Shenzhen, Guangdong, Peoples R China.; Xie, HR (corresponding author), Lingnan Univ, Hong Kong, Peoples R China.
EM sx2116068@nuaa.edu.cn; zhil_chen@outlook.com; huxiaowei@pjlab.org.cn;
   nankaimathxujun@gmail.com; hrxie2@gmail.com; harry.qin@polyu.edu.hk;
   mingqiang.wei@gmail.com
RI Xie, Haoran/AFS-3515-2022
OI Xie, Haoran/0000-0003-0965-3617
FU Shenzhen Science and Technology Program [JCYJ20220818103401003,
   JCYJ20220530172403007]; General Program of Natural Science Foundation of
   Guangdong Province [2022A1515010170]; Free Exploration of Basic Research
   Project, Local Science and Technology Development Fund Guided by the
   Central Government of China [2021Szvup060]; ResearchGrant entitled
   "Self-Supervised Learning for Medical Images" [871228]; Shenzhen
   University-Lingnan University Joint Research Programme of Lingnan
   University, Hong Kong [SZU-LU006/2122]
FX This work was supported by the Shenzhen Science and Technology Program
   (No. JCYJ20220818103401003, No. JCYJ20220530172403007), by the General
   Program of Natural Science Foundation of Guangdong Province (No.
   2022A1515010170), by the Free Exploration of Basic Research Project,
   Local Science and Technology Development Fund Guided by the Central
   Government of China (No. 2021Szvup060), and the ResearchGrant entitled
   "Self-Supervised Learning for Medical Images"' (No. 871228) and Shenzhen
   University-Lingnan University Joint Research Programme (SZU-LU006/2122)
   of Lingnan University, Hong Kong.
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Borji A, 2015, IEEE T IMAGE PROCESS, V24, P5706, DOI 10.1109/TIP.2015.2487833
   Chen CLZ, 2021, IEEE T IMAGE PROCESS, V30, P3995, DOI 10.1109/TIP.2021.3068644
   Chen CLZ, 2020, IEEE T IMAGE PROCESS, V29, P1090, DOI 10.1109/TIP.2019.2934350
   Chen H, 2019, IEEE T IMAGE PROCESS, V28, P2825, DOI 10.1109/TIP.2019.2891104
   Chen H, 2018, IEEE INT C INT ROBOT, P6821, DOI 10.1109/IROS.2018.8594373
   Chen YH, 2018, IEEE T IMAGE PROCESS, V27, P3345, DOI 10.1109/TIP.2018.2813165
   Chen ZY, 2021, IEEE T IMAGE PROCESS, V30, P7012, DOI 10.1109/TIP.2020.3028289
   Cheng MM, 2021, INT J COMPUT VISION, V129, P2622, DOI [10.1007/s11263-021-01490-8, 10.1109/ICCV.2017.487]
   Cheng Y, 2014, IEEE INT CON MULTI
   Chongyi Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P225, DOI 10.1007/978-3-030-58598-3_14
   Cong RM, 2022, IEEE T IMAGE PROCESS, V31, P6800, DOI 10.1109/TIP.2022.3216198
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Diao YY, 2020, INT GEOSCI REMOTE SE, P521, DOI 10.1109/IGARSS39084.2020.9324530
   Ding Y, 2019, J VIS COMMUN IMAGE R, V61, P1, DOI 10.1016/j.jvcir.2019.03.019
   Fan DP, 2022, IEEE T PATTERN ANAL, V44, P4339, DOI 10.1109/TPAMI.2021.3060412
   Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698
   Fan DP, 2021, IEEE T NEUR NET LEAR, V32, P2075, DOI 10.1109/TNNLS.2020.2996406
   Fan DP, 2019, PROC CVPR IEEE, P8546, DOI 10.1109/CVPR.2019.00875
   Feng D, 2016, PROC CVPR IEEE, P2343, DOI 10.1109/CVPR.2016.257
   Fu KR, 2020, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR42600.2020.00312
   Gilmer J, 2017, PR MACH LEARN RES, V70
   Gongyang Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P665, DOI 10.1007/978-3-030-58520-4_39
   Gu YC, 2020, AAAI CONF ARTIF INTE, V34, P10869
   Han JW, 2018, IEEE T CYBERNETICS, V48, P3171, DOI 10.1109/TCYB.2017.2761775
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Ji GP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4902, DOI 10.1109/ICCV48922.2021.00488
   Ji W, 2021, PROC CVPR IEEE, P9466, DOI 10.1109/CVPR46437.2021.00935
   Jin WD, 2021, IEEE T IMAGE PROCESS, V30, P3376, DOI 10.1109/TIP.2021.3060167
   Ju R, 2014, IEEE IMAGE PROC, P1115, DOI 10.1109/ICIP.2014.7025222
   Korshunov P, 2011, ACM T MULTIM COMPUT, V7, DOI 10.1145/2000486.2000488
   Lee M, 2022, LECT NOTES COMPUT SC, V13689, P630, DOI 10.1007/978-3-031-19818-2_36
   Li GY, 2020, IEEE T IMAGE PROCESS, V29, P4873, DOI 10.1109/TIP.2020.2976689
   Li GB, 2018, PROC CVPR IEEE, P3243, DOI 10.1109/CVPR.2018.00342
   Li J, 2018, IEEE T IMAGE PROCESS, V27, P349, DOI 10.1109/TIP.2017.2762594
   Li SY, 2018, LECT NOTES COMPUT SC, V11207, P215, DOI 10.1007/978-3-030-01219-9_13
   Liu ZY, 2022, IEEE T CIRC SYST VID, V32, P4486, DOI 10.1109/TCSVT.2021.3127149
   Miao Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P374, DOI 10.1007/978-3-030-58604-1_23
   Niu YZ, 2012, PROC CVPR IEEE, P454, DOI 10.1109/CVPR.2012.6247708
   Ochs P, 2014, IEEE T PATTERN ANAL, V36, P1187, DOI 10.1109/TPAMI.2013.242
   Peng HW, 2014, LECT NOTES COMPUT SC, V8691, P92, DOI 10.1007/978-3-319-10578-9_7
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Piao YR, 2019, IEEE I CONF COMP VIS, P7253, DOI 10.1109/ICCV.2019.00735
   Qi L, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3419439
   Qu LQ, 2017, IEEE T IMAGE PROCESS, V26, P2274, DOI 10.1109/TIP.2017.2682981
   Ren JQ, 2015, IEEE COMPUT SOC CONF, DOI 10.1109/CVPRW.2015.7301391
   Riegler M, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3079765
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song HK, 2017, IEEE T IMAGE PROCESS, V26, P4204, DOI 10.1109/TIP.2017.2711277
   Sun P, 2021, PROC CVPR IEEE, P1407, DOI 10.1109/CVPR46437.2021.00146
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tang Y, 2019, IEEE T CIRC SYST VID, V29, P1973, DOI 10.1109/TCSVT.2018.2859773
   Teed Z., 2020, EUR C COMP VIS, P402, DOI DOI 10.1007/978-3-030-58536-524
   Wang L, 2021, PROC CVPR IEEE, P454, DOI 10.1109/CVPR46437.2021.00052
   Wang NN, 2019, IEEE ACCESS, V7, P55277, DOI 10.1109/ACCESS.2019.2913107
   Wang XQ, 2022, IEEE T IMAGE PROCESS, V31, P1107, DOI 10.1109/TIP.2021.3139232
   Wei Ji, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P52, DOI 10.1007/978-3-030-58523-5_4
   Xiaoqi Zhao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P646, DOI 10.1007/978-3-030-58542-6_39
   Xu Xu, 2020, 2020 IEEE 5th International Conference on Image, Vision and Computing (ICIVC), P211, DOI 10.1109/ICIVC50857.2020.9177430
   Yan PX, 2019, IEEE I CONF COMP VIS, P7283, DOI 10.1109/ICCV.2019.00738
   Yang Y, 2022, IEEE T CIRC SYST VID, V32, P5346, DOI 10.1109/TCSVT.2022.3144852
   Youwei Pang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P235, DOI 10.1007/978-3-030-58595-2_15
   Zhai YJ, 2021, IEEE T IMAGE PROCESS, V30, P8727, DOI 10.1109/TIP.2021.3116793
   Zhang C, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2094, DOI 10.1145/3474085.3475364
   Zhang J., 2020, P IEEE CVF C COMP VI, P8582, DOI DOI 10.1109/CVPR42600.2020.00861
   Zhang L, 2020, PROC CVPR IEEE, P3723, DOI 10.1109/CVPR42600.2020.00378
   Zhang M, 2020, PROC CVPR IEEE, P3469, DOI 10.1109/CVPR42600.2020.00353
   Zhang WB, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P731, DOI 10.1145/3474085.3475240
   Zhang Z, 2021, IEEE T IMAGE PROCESS, V30, P1949, DOI 10.1109/TIP.2021.3049959
   Zhao J, 2020, IEEE T FUZZY SYST, V28, P2287, DOI 10.1109/TFUZZ.2019.2930492
   Zhao YF, 2021, IEEE T IMAGE PROCESS, V30, P7717, DOI 10.1109/TIP.2021.3108412
   Zhou JY, 2022, LECT NOTES COMPUT SC, V13689, P270, DOI 10.1007/978-3-031-19818-2_16
   Zhu JY, 2015, IEEE T PATTERN ANAL, V37, P862, DOI 10.1109/TPAMI.2014.2353617
   Zhu XG, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3491228
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
NR 78
TC 4
Z9 4
U1 12
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2024
VL 20
IS 1
AR 18
DI 10.1145/3597612
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T8LL3
UT WOS:001080441800018
DA 2024-08-05
ER

PT J
AU Jin, YT
   Wu, J
   Wang, WL
   Yan, YD
   Jiang, JW
   Zheng, JW
AF Jin, Yiting
   Wu, Jie
   Wang, Wanliang
   Yan, Yidong
   Jiang, Jiawei
   Zheng, Jianwei
TI Cascading Blend Network for Image Inpainting
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image inpainting; deep convolutional networks; large-proportion
   corrupted images; attention-based; multi-scale context blend
AB Image inpainting refers to filling in unknown regions with known knowledge, which is in full flourish accompanied by the popularity and prosperity of deep convolutional networks. Current inpainting methods have excelled in completing small-sized corruption or specifically masked images. However, for large-proportion corrupted images, most attention-based and structure-based approaches, though reported with state-of-the-art performance, fail to reconstruct high-quality results due to the short consideration of semantic relevance. To relieve the above problem, in this paper, we propose a novel image inpainting approach, namely cascading blend network (CBNet), to strengthen the capacity of feature representation. As a whole, we introduce an adjacent transfer attention (ATA) module in the decoder, which preserves contour structure reasonably from the deep layer and blends structure-texture information from the shadow layer. In a coarse to delicate manner, a multi-scale contextual blend (MCB) block is further designed to felicitously assemble the multi-stage feature information. In addition, to ensure a high qualified hybrid of the feature information, extra deep supervision is applied to the intermediate features through a cascaded loss. Qualitative and quantitative experiments on the Paris StreetView, CelebA, and Places2 datasets demonstrate the superior performance of our approach compared with most state-of-the-art algorithms.
C1 [Jin, Yiting; Wu, Jie; Wang, Wanliang; Yan, Yidong; Jiang, Jiawei; Zheng, Jianwei] Zhejiang Univ Technol, Coll Comp Sci & Technol, Liuhe Rd, Hangzhou 310023, Zhejiang, Peoples R China.
C3 Zhejiang University of Technology
RP Zheng, JW (corresponding author), Zhejiang Univ Technol, Coll Comp Sci & Technol, Liuhe Rd, Hangzhou 310023, Zhejiang, Peoples R China.
EM jytzjut@zjut.edu.cn; wuj@zjut.edu.cn; wangwanliang@zjut.edu.cn;
   164336437@qq.com; jjw@zjut.edu.cn; zjw@zjut.edu.cn
OI jiang, jiawei/0000-0002-9200-9189; Zheng, Jianwei/0000-0001-6017-0552;
   Wu, Jie/0000-0002-7211-2816
FU "Leading Goose" R&D Program of Zhejiang [2023C01241]; National Natural
   Science Foundation of China [62276232, 61873240]; Research Foundation of
   the Department of Education of Zhejiang Province [Y202145927]
FX This work was supported in part by the "Pioneer" and "Leading Goose" R&D
   Program of Zhejiang, under Grant 2023C01241, the National Natural
   Science Foundation of China (Nos. 62276232, 61873240), and the Research
   Foundation of the Department of Education of Zhejiang Province (No.
   Y202145927).
CR Afsari A, 2018, IEEE T ANTENN PROPAG, V66, P5521, DOI 10.1109/TAP.2018.2855642
   Antoniadis I, 2020, J COSMOL ASTROPART P, DOI 10.1088/1475-7516/2020/04/033
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Cai JY, 2022, IEEE COMPUT SOC CONF, P977, DOI 10.1109/CVPRW56347.2022.00111
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Doersch C, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185597
   Feng YC, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3168331
   Feng YC, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13214407
   Ge CJ, 2021, PROC CVPR IEEE, P16923, DOI 10.1109/CVPR46437.2021.01665
   Guo XF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14114, DOI 10.1109/ICCV48922.2021.01387
   Han XT, 2019, IEEE I CONF COMP VIS, P4480, DOI 10.1109/ICCV.2019.00458
   Haoyu Ma, 2022, ACM Transactions on Multimedia Computing, Communications and Applications, V18, DOI 10.1145/3477553
   Ho TT, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3396237
   Hongyu Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P725, DOI 10.1007/978-3-030-58536-5_43
   Hou GJ, 2019, NEUROCOMPUTING, V369, P106, DOI 10.1016/j.neucom.2019.08.041
   Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659
   Jiang K, 2022, Arxiv, DOI arXiv:2207.10455
   Jin YT, 2022, ELECTRONICS-SWITZ, V11, DOI 10.3390/electronics11223792
   Jo IS, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11020624
   Kui Jiang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8343, DOI 10.1109/CVPR42600.2020.00837
   Lan M, 2020, INFORM SCIENCES, V535, P156, DOI 10.1016/j.ins.2020.05.062
   Li JY, 2019, IEEE I CONF COMP VIS, P5961, DOI 10.1109/ICCV.2019.00606
   Liu GL, 2018, LECT NOTES COMPUT SC, V11215, P89, DOI 10.1007/978-3-030-01252-6_6
   Liu HY, 2019, IEEE I CONF COMP VIS, P4169, DOI 10.1109/ICCV.2019.00427
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Lu FH, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3524137
   Lugmayr A, 2022, PROC CVPR IEEE, P11451, DOI 10.1109/CVPR52688.2022.01117
   Muqi Huang, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P4674, DOI 10.1145/3503161.3548348
   Nazeri K, 2019, IEEE INT CONF COMP V, P3265, DOI 10.1109/ICCVW.2019.00408
   Ouyang H., 2021, P IEEE CVF INT C COM, P14579
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Peng JL, 2021, PROC CVPR IEEE, P10770, DOI 10.1109/CVPR46437.2021.01063
   Qiao Y, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3540201
   Qin J, 2021, COMPUT VIS IMAGE UND, V204, DOI 10.1016/j.cviu.2020.103155
   Ren YR, 2019, IEEE I CONF COMP VIS, P181, DOI 10.1109/ICCV.2019.00027
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Saharia C., 2022, ACM SIGGRAPH 2022 C, P1
   Shen L, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1861, DOI 10.1145/3343031.3350903
   Song LS, 2019, AAAI CONF ARTIF INTE, P2506
   Song YH, 2018, LECT NOTES COMPUT SC, V11206, P3, DOI [10.1109/APCAP.2017.8420330, 10.1007/978-3-030-01216-8_1]
   Suvorov Roman, 2022, P IEEECVF WINTER C A, P2149, DOI [10.48550/arXiv.2109.07161, DOI 10.48550/ARXIV.2109.07161]
   SuWang Chitwan Saharia, 2023, P IEEE CVF C COMP VI
   Uddin SMN, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20113204
   Uittenbogaard R, 2019, PROC CVPR IEEE, P10573, DOI 10.1109/CVPR.2019.01083
   Wang N, 2021, IEEE T IMAGE PROCESS, V30, P1784, DOI 10.1109/TIP.2020.3048629
   Wang N, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3748
   Wang TF, 2022, Arxiv, DOI arXiv:2205.12952
   Wang TF, 2021, PROC CVPR IEEE, P5116, DOI 10.1109/CVPR46437.2021.00508
   Wang Tengfei, 2021, PROC IEEECVF INT C C, P2001
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Xiang HY, 2023, PATTERN RECOGN, V134, DOI 10.1016/j.patcog.2022.109046
   Xiao Y, 2022, INT J APPL EARTH OBS, V108, DOI 10.1016/j.jag.2022.102731
   Xiao Y, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3107352
   Xie CH, 2019, IEEE I CONF COMP VIS, P8857, DOI 10.1109/ICCV.2019.00895
   Xiong W, 2019, PROC CVPR IEEE, P5833, DOI 10.1109/CVPR.2019.00599
   Xu HH, 2022, IEEE T CIRC SYST VID, V32, P538, DOI 10.1109/TCSVT.2021.3067022
   Xu Honghui, 2022, Applied Intelligence, P1
   Yan ZY, 2018, LECT NOTES COMPUT SC, V11218, P3, DOI 10.1007/978-3-030-01264-9_1
   Yang C, 2017, PROC CVPR IEEE, P4076, DOI 10.1109/CVPR.2017.434
   Yang J, 2020, AAAI CONF ARTIF INTE, V34, P12605
   Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Zeng YH, 2023, IEEE T VIS COMPUT GR, V29, P3266, DOI 10.1109/TVCG.2022.3156949
   Zeng YH, 2019, PROC CVPR IEEE, P1486, DOI 10.1109/CVPR.2019.00158
   Zheng CX, 2019, PROC CVPR IEEE, P1438, DOI 10.1109/CVPR.2019.00153
   Zheng JW, 2021, ENG APPL ARTIF INTEL, V106, DOI 10.1016/j.engappai.2021.104472
   Zheng JW, 2021, IEEE J-STARS, V14, P224, DOI 10.1109/JSTARS.2020.3042966
   Zheng JW, 2021, IEEE T GEOSCI REMOTE, V59, P522, DOI 10.1109/TGRS.2020.2995575
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
NR 69
TC 1
Z9 1
U1 9
U2 17
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2024
VL 20
IS 1
AR 14
DI 10.1145/3608952
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T8LL3
UT WOS:001080441800014
DA 2024-08-05
ER

PT J
AU Liu, H
   Liu, XL
   Tan, ZC
   Li, XL
   Zhao, Y
AF Liu, Huan
   Liu, Xiaolong
   Tan, Zichang
   Li, Xiaolong
   Zhao, Yao
TI PADVG: A Simple Baseline of Active Protection for Audio-Driven Video
   Generation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Active protection; audio-driven video generative models
AB Over the past few years, deep generative models have significantly evolved, enabling the synthesis of realistic content and also bringing security concerns of illegal misuse. Therefore, active protection for generative models has been proposed recently, aiming to generate samples with hidden messages for future identification while preserving the original generating performance. However, existing active protection methods are specifically designed for generative adversarial networks (GANs), restricted to handling unconditional image generation. We observe that they get limited identification performance and visual quality when handling audio-driven video generation conditioned on target audio and source input to drive video generation with consistent context, e.g., identity and movement, between frame sequences. To address this issue, we introduce a simple yet effective active Protection framework forAudio-Driven VideoGeneration, named PADVG. To be specific, we present a novel frame-shared embedding module in which messages to hide are first transformed into frame-shared message coefficients. Then, these coefficients are assembled with the intermediate feature maps of video generators at multiple feature levels to generate the embedded video frames. Besides, PADVG further considers two visual consistent losses: (i) intra-frame loss is utilized to keep the visual consistency with different hidden messages; (ii) inter-frame loss is used to preserve the visual consistency across different video frames. Moreover, we also propose an auxiliary denoising training strategy through perturbing the assembled features by learnable pixel-level noise to improve identification performance, while enhancing robustness against real-world disturbances. Extensive experiments demonstrate that our proposed PADVG for audio-driven video generation can effectively identify the generated videos and achieve high visual quality.
C1 [Liu, Huan; Liu, Xiaolong; Li, Xiaolong; Zhao, Yao] Beijing Jiaotong Univ, Inst Informat Sci, Beijing Key Lab Adv Informat Sci & Network Techno, 3 Shangyuancun Rd, Beijing 100044, Peoples R China.
   [Tan, Zichang] Baidu Res, Inst Deep Learning & Natl Engn Lab Deep Learning, 10 Shangdi 10th St, Beijing 100085, Peoples R China.
C3 Beijing Jiaotong University; Baidu
RP Zhao, Y (corresponding author), Beijing Jiaotong Univ, Inst Informat Sci, Beijing Key Lab Adv Informat Sci & Network Techno, 3 Shangyuancun Rd, Beijing 100044, Peoples R China.
EM liu.huan@bjtu.edu.cn; 18112008@bjtu.edu.cn; tanzichang@baidu.com;
   lixl@bjtu.edu.cn; yzhao@bjtu.edu.cn
OI Liu, Huan/0009-0000-6347-5060; Zhao, Yao/0000-0002-8581-9554
FU National Key R&D Program of China [2021ZD0112100]; National NSF of China
   [62120106009, 62261160653]
FX This work was supported in part by the National Key R&D Program of China
   (No.2021ZD0112100), National NSF of China (No.U1936212, No.62120106009,
   No.62261160653).
CR Afouras T, 2022, IEEE T PATTERN ANAL, V44, P8717, DOI 10.1109/TPAMI.2018.2889052
   Afouras T, 2018, Arxiv, DOI arXiv:1809.00496
   Bamatraf A., 2010, 2010 International Conference on Computer Applications and Industrial Electronics (ICCAIE), P155, DOI 10.1109/ICCAIE.2010.5735066
   Brundage M, 2018, Arxiv, DOI [arXiv:1802.07228, DOI 10.48550/ARXIV.1802.07228]
   Chen S, 2021, AAAI CONF ARTIF INTE, V35, P1081
   Chung JS, 2017, LECT NOTES COMPUT SC, V10112, P87, DOI 10.1007/978-3-319-54184-6_6
   Fernandez P, 2022, INT CONF ACOUST SPEE, P3054, DOI 10.1109/ICASSP43922.2022.9746058
   Filler T, 2010, PROC SPIE, V7541, DOI 10.1117/12.838002
   Ge SM, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3536426
   Gong Y., 2021, P INT JOINT C ART IN, P2439
   Goodfellow I. J., 2014, ADV NEURAL INFORM PR
   Guo YD, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5764, DOI 10.1109/ICCV48922.2021.00573
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hensel M, 2017, ADV NEUR IN, V30
   Hu Ziheng, 2021, P 30 INT JOINT C ART, P736, DOI DOI 10.24963/IJCAI.2021/102
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Ji Xinya, 2022, Proceedings of the SIGGRAPH.
   Jia ZY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P41, DOI 10.1145/3474085.3475324
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Kingma D. P., 2015, P INT C LEARN REPR, P1
   Langelaar GC, 2000, IEEE SIGNAL PROC MAG, V17, P20, DOI 10.1109/79.879337
   Li LZ, 2020, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR42600.2020.00505
   Li XY, 2021, PROC CVPR IEEE, P8635, DOI 10.1109/CVPR46437.2021.00853
   Liang D, 2019, AAAI CONF ARTIF INTE, P8698
   Liu Huan, 2023, arXiv
   Liu XL, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3558004
   Lu YX, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480484
   Mallya A., 2020, ECCV
   Masi Iacopo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P667, DOI 10.1007/978-3-030-58571-6_39
   Miao CT, 2023, IEEE T INF FOREN SEC, V18, P1039, DOI 10.1109/TIFS.2022.3233774
   Miao CT, 2022, IEEE T INF FOREN SEC, V17, P3008, DOI 10.1109/TIFS.2022.3198275
   Nagrani A, 2017, INTERSPEECH, P2616, DOI 10.21437/Interspeech.2017-950
   Ong DS, 2021, PROC CVPR IEEE, P3629, DOI 10.1109/CVPR46437.2021.00363
   Prajwal KR, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P484, DOI 10.1145/3394171.3413532
   Ren YR, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13739, DOI 10.1109/ICCV48922.2021.01350
   Salem A, 2020, PROCEEDINGS OF THE 29TH USENIX SECURITY SYMPOSIUM, P1291
   Sharp T., 2001, INT WORKSH INF HID, V2137, P13, DOI [10.1007/3-540-45496-9_2, DOI 10.1007/3-540-45496-92]
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tancik M, 2020, PROC CVPR IEEE, P2114, DOI 10.1109/CVPR42600.2020.00219
   Velazquez-Garcia L, 2022, SIGNAL PROCESS-IMAGE, V102, DOI 10.1016/j.image.2021.116593
   Wang RF, 2021, PROTEINS, V89, P558, DOI 10.1002/prot.26041
   Wang R, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3546, DOI 10.1145/3474085.3475518
   Wang TC, 2021, PROC CVPR IEEE, P10034, DOI 10.1109/CVPR46437.2021.00991
   Wang XT, 2019, LECT NOTES COMPUT SC, V11133, P63, DOI 10.1007/978-3-030-11021-5_5
   Yang Y, 2021, FRONT INFORM TECH EL, V22, P1551, DOI 10.1631/FITEE.2100463
   Yu N, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14428, DOI 10.1109/ICCV48922.2021.01418
   Yu Ning, 2022, Proceedings of the ICLR.
   Yu Y, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3499026
   Yuyang Qian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P86, DOI 10.1007/978-3-030-58610-2_6
   Zhang CX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3847, DOI 10.1109/ICCV48922.2021.00384
   Zhao HQ, 2021, PROC CVPR IEEE, P2185, DOI 10.1109/CVPR46437.2021.00222
   Zhou H, 2021, PROC CVPR IEEE, P4174, DOI 10.1109/CVPR46437.2021.00416
NR 53
TC 0
Z9 0
U1 1
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2024
VL 20
IS 6
SI SI
AR 168
DI 10.1145/3638556
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OQ2T3
UT WOS:001208681800018
DA 2024-08-05
ER

PT J
AU Roy, S
   Etemad, A
AF Roy, Shuvendu
   Etemad, Ali
TI Contrastive Learning of View-invariant Representations for Facial
   Expressions Recognition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Affective computing; contrastive learning; expression recognition
ID FIELD-BASED FACE; MULTIVIEW
AB Although there has been much progress in the area of facial expression recognition (FER), most existing methods suffer when presented with images that have been captured from viewing angles that are non-frontal and substantially different from those used in the training process. In this article, we propose ViewFX, a novel view-invariant FER framework based on contrastive learning, capable of accurately classifying facial expressions regardless of the input viewing angles during inference. ViewFX learns view-invariant features of expression using a proposed self-supervised contrastive loss, which brings together different views of the same subject with a particular expression in the embedding space. We also introduce a supervised contrastive loss to push the learned view-invariant features of each expression away from other expressions. Since facial expressions are often distinguished with very subtle differences in the learned feature space, we incorporate the Barlow twins loss to reduce the redundancy and correlations of the representations in the learned representations. The proposed method is a substantial extension of our previously proposed CL-MEx, which only had a self-supervised loss. We test the proposed framework on two public multi-view facial expression recognition datasets, KDEF and DDCF. The experiments demonstrate that our approach outperforms previous works in the area and sets a new state-of-the-art for both datasets while showing considerably less sensitivity to challenging angles and the number of output labels used for training. We also perform detailed sensitivity and ablation experiments to evaluate the impact of different components of our model as well as its sensitivity to different parameters.
C1 [Roy, Shuvendu; Etemad, Ali] Queens Univ, Dept ECE, Kingston, ON, Canada.
   [Roy, Shuvendu; Etemad, Ali] Queens Univ, Ingenu Labs Res Inst, Kingston, ON, Canada.
C3 Queens University - Canada; Queens University - Canada; Ingenuity Labs
   Research Institute
RP Roy, S (corresponding author), Queens Univ, Dept ECE, Kingston, ON, Canada.; Roy, S (corresponding author), Queens Univ, Ingenu Labs Res Inst, Kingston, ON, Canada.
EM shuvendu.roy@queensu.ca; ali.etemad@queensu.ca
OI Etemad, Ali/0000-0001-7128-0220; Roy, Shuvendu/0000-0002-7674-0238
FU BMO Bank of Montreal; Mitacs
FX We would like to thank BMO Bank of Montreal and Mitacs for funding this
   research.
CR Albu F, 2015, EDULEARN PROC, P3229
   Bardes A, 2022, Arxiv, DOI arXiv:2105.04906
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Cho Y, 2019, JMIR MENT HEALTH, V6, DOI 10.2196/10140
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Dalrymple KA, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0079131
   Hjelm RD, 2019, Arxiv, DOI arXiv:1808.06670
   Eleftheriadis S, 2015, IEEE T IMAGE PROCESS, V24, P189, DOI 10.1109/TIP.2014.2375634
   ESSA IA, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P360, DOI 10.1109/ICCV.1995.466916
   Fang B, 2023, IEEE ACCESS, V11, P45547, DOI 10.1109/ACCESS.2023.3274193
   Fard AP, 2022, IEEE ACCESS, V10, P26756, DOI 10.1109/ACCESS.2022.3156598
   Frosst N, 2019, PR MACH LEARN RES, V97
   Hariri W, 2017, ENG APPL ARTIF INTEL, V64, P25, DOI 10.1016/j.engappai.2017.05.009
   Hasani B, 2022, IEEE T AFFECT COMPUT, V13, P1023, DOI 10.1109/TAFFC.2020.2986440
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Henaff OJ, 2020, PR MACH LEARN RES, V119
   Hu Yuxiao, 2008, 8 IEEE INT C AUT FAC, P1
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jiang J, 2023, IEEE T AFFECT COMPUT, V14, P2402, DOI 10.1109/TAFFC.2021.3131621
   Jure Z., 2021, arXiv
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Khan RA, 2019, FRONT COMPUT SCI-CHI, V13, P183, DOI 10.1007/s11704-017-6114-9
   Khosla P., 2020, Adv. Neural Inf. Process. Syst, P18661, DOI DOI 10.48550/ARXIV.2004.11362
   Kolahdouzi M, 2021, IEEE INT CONF AUTOMA, DOI 10.1109/FG52635.2021.9666986
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Leng H, 2007, LECT NOTES COMPUT SC, V4566, P237
   Li HY, 2022, PROC CVPR IEEE, P4156, DOI 10.1109/CVPR52688.2022.00413
   Li HY, 2022, IEEE T IMAGE PROCESS, V31, P4637, DOI 10.1109/TIP.2022.3186536
   Li YJ, 2022, IEEE T CIRC SYST VID, V32, P3190, DOI 10.1109/TCSVT.2021.3103782
   Li Yinqi, 2022, Adv. Neural Inf. Process. Syst., V35, P18327
   Liu CJ, 2023, VISUAL COMPUT, V39, P2637, DOI 10.1007/s00371-022-02483-5
   Liu YY, 2019, Arxiv, DOI arXiv:1905.10059
   Liu YY, 2018, IEEE INT CONF AUTOMA, P458, DOI 10.1109/FG.2018.00074
   Lundqvist D., 1998, KAROLINSKA DIRECTED, DOI DOI 10.1037/T27732-000
   Mahesh VGV, 2021, IEEE ACCESS, V9, P52509, DOI 10.1109/ACCESS.2021.3069881
   Misra I, 2020, PROC CVPR IEEE, P6706, DOI 10.1109/CVPR42600.2020.00674
   Miyai A, 2023, IEEE WINT CONF APPL, P2808, DOI 10.1109/WACV56688.2023.00283
   Moore S, 2011, COMPUT VIS IMAGE UND, V115, P541, DOI 10.1016/j.cviu.2010.12.001
   Pourmirzaei M., 2021, arXiv
   Psaroudakis A, 2022, IEEE COMPUT SOC CONF, P2366, DOI 10.1109/CVPRW56347.2022.00264
   Vo QN, 2019, IEEE INT WORKSH MULT, DOI 10.1109/mmsp.2019.8901797
   Rao QY, 2015, INT CONF AFFECT, P630, DOI 10.1109/ACII.2015.7344635
   Roy Shuvendu, 2021, ICMI '21: Proceedings of the 2021 International Conference on Multimodal Interaction, P253, DOI 10.1145/3462244.3479955
   Roy S, 2021, INT CONF AFFECT, DOI 10.1109/ACII52823.2021.9597460
   Roy Shuvendu, 2023, INT C AFF COMP INT I
   Roy Shuvendu, 2023, IEEE INT C AC SPEECH, P1
   Salakhutdinov R, 2007, J MACHINE LEARNING R, V2, P412
   SAMAL A, 1992, PATTERN RECOGN, V25, P65, DOI 10.1016/0031-3203(92)90007-6
   Sanchez-Cortes Dairazalia, 2013, 12 INT C MOB UB MULT, P1
   Santra B, 2016, TENTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS AND IMAGE PROCESSING (ICVGIP 2016), DOI 10.1145/3009977.3010008
   Schoneveld L, 2021, PATTERN RECOGN LETT, V146, P1, DOI 10.1016/j.patrec.2021.03.007
   Sepas-Moghaddam A, 2021, PROC CVPR IEEE, P16535, DOI 10.1109/CVPR46437.2021.01627
   Sepas-Moghaddam A, 2020, INT CONF ACOUST SPEE, P3367, DOI [10.1109/ICASSP40776.2020.9053919, 10.1109/icassp40776.2020.9053919]
   Sepas-Moghaddam A, 2021, IEEE T IMAGE PROCESS, V30, P2627, DOI 10.1109/TIP.2021.3054476
   Sepas-Moghaddam A, 2021, IEEE T INF FOREN SEC, V16, P1365, DOI 10.1109/TIFS.2020.3036242
   Sepas-Moghaddam A, 2019, INT CONF AFFECT, DOI [10.1109/ACII.2019.8925445, 10.1109/acii.2019.8925445]
   Sermanet P, 2018, IEEE INT CONF ROBOT, P1134
   Shan CF, 2009, IMAGE VISION COMPUT, V27, P803, DOI 10.1016/j.imavis.2008.08.005
   Shan CF, 2005, IEEE IMAGE PROC, P2225
   Shao J, 2020, PATTERN RECOGN IMAGE, V30, P805, DOI 10.1134/S1054661820040197
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Siqueira H, 2020, AAAI CONF ARTIF INTE, V34, P5800
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Taheri S., 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P306, DOI 10.1109/FG.2011.5771415
   Tang Y, 2021, IEEE T IMAGE PROCESS, V30, P444, DOI 10.1109/TIP.2020.3037467
   Thrasher M, 2011, LECT NOTES COMPUT SC, V6974, P377, DOI 10.1007/978-3-642-24600-5_41
   Tokuno Shinichi, 2011, DEF SCI RES C EXP, P1
   Venkatesh YV, 2012, PATTERN RECOGN LETT, V33, P1785, DOI 10.1016/j.patrec.2012.05.015
   Vo TH, 2020, IEEE ACCESS, V8, P131988, DOI 10.1109/ACCESS.2020.3010018
   Wang K, 2020, IEEE T IMAGE PROCESS, V29, P4057, DOI 10.1109/TIP.2019.2956143
   Wang Lingfeng, 2021, IEEE CVF INT C COMP, P3603
   Wu ZR, 2018, LECT NOTES COMPUT SC, V11211, P712, DOI 10.1007/978-3-030-01234-2_42
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Xia Bin, 2021, IJCAI, P1186
   Xuan-Phung Huynh, 2016, International Conference on Information Science and Applications (ICISA) 2016. LNEE 376, P441, DOI 10.1007/978-981-10-0557-2_44
   Xue FL, 2022, IEEE COMPUT SOC CONF, P2411, DOI 10.1109/CVPRW56347.2022.00269
   YimingWang Hui Yu, 2016, ASIAN C COMPUT VIS, P375
   Yin LJ, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P211
   Yonglong Tian, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P776, DOI 10.1007/978-3-030-58621-8_45
   Zeng D, 2022, PROC CVPR IEEE, P20259, DOI 10.1109/CVPR52688.2022.01965
   Zhang FF, 2020, IEEE T IMAGE PROCESS, V29, P4445, DOI 10.1109/TIP.2020.2972114
   Zhang FF, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3176646
   Zheng M., 2021, Proceedings of the IEEE/CVF International Conference on Computer Vision, P10042
   Zhou YQ, 2017, IEEE IJCNN, P2031
NR 84
TC 0
Z9 0
U1 15
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2024
VL 20
IS 4
AR 97
DI 10.1145/3632960
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6G9
UT WOS:001153382100007
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Xiang, SC
   Qian, DH
   Gao, JS
   Zhang, ZR
   Liu, T
   Fu, YZ
AF Xiang, Suncheng
   Qian, Dahong
   Gao, Jingsheng
   Zhang, Zirui
   Liu, Ting
   Fu, Yuzhuo
TI Rethinking Person Re-Identification via Semantic-based Pretraining
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Person re-identification; synthetic data; efficient training
AB Pretraining is a dominant paradigm in computer vision. Generally, supervised ImageNet pretraining is commonly used to initialize the backbones of person re-identification (Re-ID) models. However, recent works show a surprising result that CNN-based pretraining on ImageNet has limited impacts on Re-ID system due to the large domain gap between ImageNet and person Re-ID data. To seek an alternative to traditional pretraining, here we investigate semantic-based pretraining as another method to utilize additional textual data against ImageNet pretraining. Specifically, we manually construct a diversified FineGPR-C caption dataset for the first time on person Re-ID events. Based on it, a pure semantic-based pretraining approach named VTBR is proposed to adopt dense captions to learn visual representations with fewer images. We train convolutional neural networks from scratch on the captions of FineGPR-C dataset, and then transfer them to downstream Re-ID tasks. Comprehensive experiments conducted on benchmark datasets show that our VTBR can achieve competitive performance compared with ImageNet pretraining-despite using up to 1.4x fewer images, revealing its potential in Re-ID pretraining. Our source code is also publicly available at https://github.com/JeremyXSC/VTBR.
C1 [Xiang, Suncheng; Qian, Dahong] Shanghai Jiao Tong Univ, Sch Biomed Engn, Shanghai 200240, Peoples R China.
   [Gao, Jingsheng; Zhang, Zirui; Liu, Ting; Fu, Yuzhuo] Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Shanghai 200240, Peoples R China.
C3 Shanghai Jiao Tong University; Shanghai Jiao Tong University
RP Xiang, SC (corresponding author), Shanghai Jiao Tong Univ, Sch Biomed Engn, Shanghai 200240, Peoples R China.
EM xiangsuncheng17@sjtu.edu.cn; dahong.qian@sjtu.edu.cn;
   gaojingsheng@sjtu.edu.cn; ziruizhang_1@163.com; louisa_liu@sjtu.edu.cn;
   yzfu@sjtu.edu.cn
RI Zhang, Zirui/JSK-2011-2023
OI Zhang, Zirui/0009-0000-5209-7906; Xiang, Suncheng/0000-0002-9141-6460
FU National Natural Science Foundation of China [62301315]; Startup Fund
   for Young Faculty at SJTU (SFYF at SJTU) [23X010501967]
FX This work was partially supported by the National Natural Science
   Foundation of China under Grant No. 62301315 and Startup Fund for Young
   Faculty at SJTU (SFYF at SJTU) under Grant No. 23X010501967.
CR Bai ZC, 2021, PROC CVPR IEEE, P12909, DOI 10.1109/CVPR46437.2021.01272
   Chen BH, 2019, IEEE I CONF COMP VIS, P371, DOI 10.1109/ICCV.2019.00046
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng WJ, 2018, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.2018.00110
   Desai K, 2021, PROC CVPR IEEE, P11157, DOI 10.1109/CVPR46437.2021.01101
   Farenzena M, 2010, PROC CVPR IEEE, P2360, DOI 10.1109/CVPR.2010.5539926
   Fu DP, 2021, PROC CVPR IEEE, P14745, DOI 10.1109/CVPR46437.2021.01451
   Fu Y, 2019, IEEE I CONF COMP VIS, P6111, DOI 10.1109/ICCV.2019.00621
   Ge YX, 2018, ADV NEUR IN, V31
   He KM, 2019, IEEE I CONF COMP VIS, P4917, DOI 10.1109/ICCV.2019.00502
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Jeong B., 2021, P IEEECVF INT C COMP, P12016
   Kaiwei Zeng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13654, DOI 10.1109/CVPR42600.2020.01367
   Kukunuri Rithwik, 2019, Lookahead optimizer: k steps forward, 1 step back
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin YT, 2019, PATTERN RECOGN, V95, P151, DOI 10.1016/j.patcog.2019.06.006
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   López-Cifuentes A, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2020.107256
   Luo H., 2021, arXiv
   Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190
   Muhammad MB, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9206626
   Ning X, 2021, IEEE T CIRC SYST VID, V31, P3391, DOI 10.1109/TCSVT.2020.3043026
   Ning X, 2021, NEUROCOMPUTING, V453, P801, DOI 10.1016/j.neucom.2020.05.106
   Pan XG, 2018, LECT NOTES COMPUT SC, V11208, P484, DOI 10.1007/978-3-030-01225-0_29
   Park H, 2020, AAAI CONF ARTIF INTE, V34, P11839
   Paszke A, 2019, ADV NEUR IN, V32
   Peng PX, 2016, PROC CVPR IEEE, P1306, DOI 10.1109/CVPR.2016.146
   Qian XL, 2018, LECT NOTES COMPUT SC, V11213, P661, DOI 10.1007/978-3-030-01240-3_40
   Radford A, 2021, PR MACH LEARN RES, V139
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Sun YF, 2020, PROC CVPR IEEE, P6397, DOI 10.1109/CVPR42600.2020.00643
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Sun YF, 2017, IEEE I CONF COMP VIS, P3820, DOI 10.1109/ICCV.2017.410
   Varior RR, 2016, LECT NOTES COMPUT SC, V9912, P791, DOI 10.1007/978-3-319-46484-8_48
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Y, 2018, PROC CVPR IEEE, P8042, DOI [10.1109/CVPR.2018.00839, 10.1109/CVPR.2018.00736]
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Xiang SC, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3588441
   Xiang SC, 2024, MACH LEARN, V113, P1921, DOI 10.1007/s10994-023-06352-7
   Xiang SC, 2022, IEEE COMPUT SOC CONF, P4730, DOI 10.1109/CVPRW56347.2022.00519
   Xiang SC, 2023, MACH LEARN, V112, P1923, DOI 10.1007/s10994-022-06184-x
   Xiang SC, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P3765, DOI 10.1109/ICASSP39728.2021.9413757
   Xiang SC, 2020, IET IMAGE PROCESS, V14, P3527, DOI 10.1049/iet-ipr.2020.0166
   Xiang SC, 2020, MULTIMED TOOLS APPL, V79, P32079, DOI 10.1007/s11042-020-09569-z
   Xiang SC, 2020, MULTIMED TOOLS APPL, V79, P19769, DOI 10.1007/s11042-020-08723-x
   Xiao QQ, 2017, Arxiv, DOI arXiv:1710.00478
   Yang ZZ, 2022, PROC CVPR IEEE, P14278, DOI 10.1109/CVPR52688.2022.01390
   Yi D, 2014, INT C PATT RECOG, P34, DOI 10.1109/ICPR.2014.16
   Zhang Z, 2018, ADV NEUR IN, V31
   Zhao R, 2014, PROC CVPR IEEE, P144, DOI 10.1109/CVPR.2014.26
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng M, 2019, PROC CVPR IEEE, P5728, DOI 10.1109/CVPR.2019.00588
   Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI 10.1109/CVPR.2019.00224
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
NR 58
TC 0
Z9 0
U1 5
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2024
VL 20
IS 3
AR 90
DI 10.1145/3628452
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6F8
UT WOS:001153381000030
OA Green Submitted, Bronze
DA 2024-08-05
ER

PT J
AU Zhao, RN
   Yang, LT
   Liu, DB
   Lu, WL
   Zhu, CL
   Ruan, YH
AF Zhao, Ruonan
   Yang, Laurence T.
   Liu, Debin
   Lu, Wanli
   Zhu, Chenlu
   Ruan, Yiheng
TI Tensor-Empowered LSTM for Communication-Efficient and Privacy-Enhanced
   Cognitive Federated Learning in Intelligent Transportation Systems
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Federated learning; tensor ring-block decomposition; homomorphic
   encryption; communication efficiency
AB Multimedia cognitive computing as a revolutionary emerging concept of artificial intelligence emulating the reasoning process like human brains can facilitate the evolution of intelligent transportation systems (ITS) to be smarter, safer, and more efficient. Massive multimedia traffic big data is an important prerequisite for the success of cognitive computing in ITS. However, traditional data-centralized artificial intelligence approaches often face the problems of data islands and data famine due to concerns about data privacy and security. To this end, we propose the concept of cognitive federated learning leveraging federated learning as the learning paradigm for cognitive computing, which solves the preceding concerns by sharing updated models rather than raw data. Nevertheless, the exchange of numerous model parameters not only generates significant communication overhead but also suffers from the risk of privacy leakage due to inference attacks. This article aims to design a novel lightweight and privacy-enhanced cognitive federated learning architecture to facilitate the development of ITS. First, a privacy-enhanced model protection scheme with homomorphic encryption as the underlying technology is proposed to simultaneously defend against the inference attacks launched by external malicious attackers, honest-but-curious cognitive platforms, and internal participants. Furthermore, a novel tensor ring-block decomposition and its corresponding deep computation model converting the weight tensor into a set of matrices and third-order core tensors are proposed, which could reduce the communication overhead and storage requirements without compromising model performance. Experimental results on real-world datasets show that the proposed approach performs well.
C1 [Zhao, Ruonan; Yang, Laurence T.; Lu, Wanli] Huazhong Univ Sci & Technol, Sch Cyber Sci & Engn, Hubei Engn Res Ctr Big Data Secur, Hubei Key Lab Distributed Syst Secur, Wuhan 430074, Peoples R China.
   [Yang, Laurence T.] Sch Comp Sci & Technol, Haikou 570228, Peoples R China.
   [Yang, Laurence T.] Hainan Univ, Haikou 570228, Peoples R China.
   [Yang, Laurence T.] St Francis Xavier Univ, Antigonish, NS, Canada.
   [Liu, Debin] Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, 1037 Guanshan Rd, Wuhan, Peoples R China.
   [Zhu, Chenlu] Hubei Chutian Expressway Digital Technol, Wuhan, Peoples R China.
   [Ruan, Yiheng] Hubei Chutian Intelligence Transportat Co Ltd, Wuhan, Peoples R China.
C3 Huazhong University of Science & Technology; Hainan University; Saint
   Francis Xavier University - Canada; Huazhong University of Science &
   Technology
RP Yang, LT (corresponding author), Huazhong Univ Sci & Technol, Sch Cyber Sci & Engn, Hubei Engn Res Ctr Big Data Secur, Hubei Key Lab Distributed Syst Secur, Wuhan 430074, Peoples R China.; Yang, LT (corresponding author), Sch Comp Sci & Technol, Haikou 570228, Peoples R China.; Yang, LT (corresponding author), Hainan Univ, Haikou 570228, Peoples R China.; Yang, LT (corresponding author), St Francis Xavier Univ, Antigonish, NS, Canada.
EM zhaoruonan@hust.edu.cn; ltyang@gmail.com; debinliuhust@gmail.com;
   lu_wl@hust.edu.cn; zhu_cl@mikimobile.com; 33159954@qq.com
RI Laurence T. Yang, FCAE/AAA-1898-2019; Zhang, Lijuan/KAM-0174-2024
OI Laurence T. Yang, FCAE/0000-0002-7986-4244; zhu,
   chenlu/0000-0003-2175-6221; Lu, Wanli/0000-0003-0098-791X; Liu,
   debin/0000-0001-5233-9637
FU National Natural Science Foundation of China [61932010]; National Key
   Research and Development Program of China [2019YFB1705903]; Artificial
   Intelligence and Intelligent Transportation Joint Technical Center of
   Huazhong University of Science and Technology (HUST); Hubei Chutian
   Intelligent Transportation Co., Ltd. under the project Intelligent
   Transportation Operation Monitoring Network and System [0231129029]
FX This work was supported in part by the National Natural Science
   Foundation of China (grant 61932010), the National Key Research and
   Development Program of China (grant 2019YFB1705903), and the Artificial
   Intelligence and Intelligent Transportation Joint Technical Center of
   Huazhong University of Science and Technology (HUST) and Hubei Chutian
   Intelligent Transportation Co., Ltd. under the project Intelligent
   Transportation Operation Monitoring Network and System (grant
   0231129029).
CR Boukerche A, 2020, COMPUT NETW, V182, DOI 10.1016/j.comnet.2020.107484
   Chen SC, 2003, IEEE T INTELL TRANSP, V4, P154, DOI 10.1109/TITS.2003.821290
   Dong Z, 2015, IEEE T INTELL TRANSP, V16, P2247, DOI 10.1109/TITS.2015.2402438
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Jia B, 2022, IEEE T IND INFORM, V18, P4049, DOI 10.1109/TII.2021.3085960
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lin K, 2023, IEEE T INTELL TRANSP, V24, P1088, DOI 10.1109/TITS.2022.3151754
   Liu DB, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3491223
   Ma J, 2021, Arxiv, DOI arXiv:2104.06824
   McMahan H.B., 2016, CoRR, DOI 10.48550/arXiv.1602.05629
   McMahan HB, 2017, PR MACH LEARN RES, V54, P1273
   Mounica B, 2020, P INT C EM TRENDS IN, P1, DOI [10.1109/icETITE47903.2020.PDFeXpID6335287, DOI 10.1109/ICETITE47903.2020.PDFEXPID6335287]
   Oseledets IV, 2011, SIAM J SCI COMPUT, V33, P2295, DOI 10.1137/090752286
   Paillier P, 1999, LECT NOTES COMPUT SC, V1592, P223
   Phong LT, 2018, IEEE T INF FOREN SEC, V13, P1333, DOI 10.1109/TIFS.2017.2787987
   Qu YY, 2021, IEEE T IND INFORM, V17, P2964, DOI 10.1109/TII.2020.3007817
   Raja G, 2018, INT CONF ADV COMPU, P146
   Sun Y, 2023, IEEE T INTELL TRANSP, V24, P1062, DOI 10.1109/TITS.2021.3129598
   TUCKER LR, 1966, PSYCHOMETRIKA, V31, P279, DOI 10.1007/BF02289464
   Wei K, 2020, IEEE T INF FOREN SEC, V15, P3454, DOI 10.1109/TIFS.2020.2988575
   Xu GW, 2022, IEEE T DEPEND SECURE, V19, P1364, DOI 10.1109/TDSC.2020.3005909
   Yuan XS, 2022, I C COMM SOFTW NET, P1, DOI 10.1109/ICCSN55126.2022.9817586
   Zhao B, 2021, IEEE T IND INFORM, V17, P6314, DOI 10.1109/TII.2021.3052183
   Zhao QB, 2016, Arxiv, DOI arXiv:1606.05535
   Zhu J, 2018, IEEE INTERNET THINGS, V5, P2375, DOI 10.1109/JIOT.2017.2759728
NR 25
TC 0
Z9 0
U1 7
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2024
VL 20
IS 2
SI SI
AR 33
DI 10.1145/3575661
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W6GR4
UT WOS:001092595800003
DA 2024-08-05
ER

PT J
AU Lou, XL
   Wu, TH
   Hu, HF
   Chen, D
AF Lou, Xulei
   Wu, Tinghui
   Hu, Haifeng
   Chen, Dihu
TI Self-Supervised Consistency Based on Joint Learning for Unsupervised
   Person Re-identification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Person re-identification; unsupervised domain adaptive; self-supervised;
   joint learning
ID ENHANCEMENT; NETWORK
AB Recently, unsupervised domain adaptive person re-identification (Re-ID) methods have been extensively studied thanks to not requiring annotations, and they have achieved excellent performance. Most of the existing methods aim to train the Re-ID model for learning a discriminative feature representation. However, they usually only consider training the model to learn a global feature of a pedestrian image, but neglecting the local feature, which restricts further improvement of model performance. To address this problem, two local branches are added to the networks, aiming to allow the model to focus on the local feature containing identity information. Furthermore, we propose a self-supervised consistency constraint to further improve robustness of the model. Specifically, the self-supervised consistency constraint uses the basic data augmentation operations without other auxiliary networks, which can improve performance of the model effectively. Then, a learnable memory matrix is designed to store the mapping vectors that maps person features into probability distributions. Finally, extensive experiments are conducted on multiple commonly used person Re-ID datasets to verify the effectiveness of the proposed generative adversarial networks fusing global and local features. Experimental results reveal that our method achieves results comparable to state-of-the-art methods.
C1 [Lou, Xulei; Wu, Tinghui; Hu, Haifeng; Chen, Dihu] Sun Yat Sen Univ, Sch Elect & Informat Technol, 132 Huandong Rd,Univ Town, Guangzhou, Guangdong, Peoples R China.
C3 Sun Yat Sen University
RP Chen, D (corresponding author), Sun Yat Sen Univ, Sch Elect & Informat Technol, 132 Huandong Rd,Univ Town, Guangzhou, Guangdong, Peoples R China.
EM louxlei@mail2.sysu.edu.cn; wuth8@mail2.sysu.edu.cn;
   huhaif@mail.sysu.edu.cn; stscdh@mail.sysu.edu.cn
OI chen, dihu/0000-0001-5432-8149; Hu, Haifeng/0000-0002-4884-323X; Wu,
   Tinghui/0009-0007-0144-6169
FU Science and Technology Program of Guangdong Province [2022B0701180001]
FX The work was supported by the Science and Technology Program of
   Guangdong Province under grant 2022B0701180001. Authors' address: X.
   Lou, T. Wu, H. Hu, and D. Chen (corresponding author), School of
   Electronics and Information Technology, Sun Yat-sen University, 132
   Huandong Road, University Town, Guangzhou, Guangdong, China; emails:
   {louxlei, wuth8}@mail2.sysu.edu.cn, {huhaif, stscdh}@mail.sysu.edu.cn.
CR Caron M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9630, DOI 10.1109/ICCV48922.2021.00951
   Chen H, 2021, PROC CVPR IEEE, P2004, DOI 10.1109/CVPR46437.2021.00204
   Chen T, 2020, Arxiv, DOI [arXiv:2002.05709, DOI 10.48550/ARXIV.2002.05709]
   Chen XL, 2020, Arxiv, DOI arXiv:2003.04297
   Chen XL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9620, DOI 10.1109/ICCV48922.2021.00950
   Cheng D, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P964
   Cho Y, 2022, PROC CVPR IEEE, P7298, DOI 10.1109/CVPR52688.2022.00716
   Dai Zuozhuo, 2021, P AS C COMP VIS
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding YH, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3369393
   Dongkai Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10978, DOI 10.1109/CVPR42600.2020.01099
   Eom Chanho, 2019, P 33 C NEUR INF PROC, P1
   Fu DP, 2021, PROC CVPR IEEE, P14745, DOI 10.1109/CVPR46437.2021.01451
   Fu Y, 2019, IEEE I CONF COMP VIS, P6111, DOI 10.1109/ICCV.2019.00621
   Ge Y., 2020, Advances in neural information processing systems, V33, P11309
   Ge YX, 2020, Arxiv, DOI [arXiv:2001.01526, 10.48550/arXiv.2001.01526]
   Ge Yixiao, 2018, P 32 C NEUR INF PROC, P1
   Grill J-B, 2020, PROC 34 INT C NEURAL
   Han Jian, 2021, P AAAI C ART INT
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Kaiwei Zeng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13654, DOI 10.1109/CVPR42600.2020.01367
   Li Q, 2022, PATTERN RECOGN, V125, DOI 10.1016/j.patcog.2022.108521
   Li W., 2017, Person re-identification by deep joint learning of multi-loss classification
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Li YY, 2022, IEEE T MULTIMEDIA, V24, P415, DOI 10.1109/TMM.2021.3052354
   Li YY, 2021, IEEE T IMAGE PROCESS, V30, P7952, DOI 10.1109/TIP.2021.3112039
   Li YJ, 2019, IEEE I CONF COMP VIS, P7918, DOI 10.1109/ICCV.2019.00801
   Lin P., 2021, arXiv
   Ling Sen, 2022, Digital TV and Wireless Multimedia Communications: 18th International Forum, IFTC 2021, Revised Selected Papers. Communications in Computer and Information Science (1560), P297, DOI 10.1007/978-981-19-2266-4_23
   Liu WH, 2018, AAAI CONF ARTIF INTE, P7162
   Liu WH, 2017, LECT NOTES ARTIF INT, V10534, P103, DOI 10.1007/978-3-319-71249-9_7
   Luo H., 2021, arXiv
   Ma AJ, 2013, IEEE I CONF COMP VIS, P3567, DOI 10.1109/ICCV.2013.443
   MacQueen J. B., 1967, Some Methods for Classification and Analysis of Multivariate Observations, P281, DOI DOI 10.1007/S11665-016-2173-6
   Ming Z., 2021, arXiv
   Pan XG, 2018, LECT NOTES COMPUT SC, V11208, P484, DOI 10.1007/978-3-030-01225-0_29
   Pang ZQ, 2022, IEEE T CIRC SYST VID, V32, P3164, DOI 10.1109/TCSVT.2021.3103753
   Peng PX, 2016, PROC CVPR IEEE, P1306, DOI 10.1109/CVPR.2016.146
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Simoudis E., 1996, KDD 96 P 2 INT C KNO, P226
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Sun ZZ, 2021, IEEE IMAGE PROC, P2363, DOI 10.1109/ICIP42928.2021.9506220
   Tang Q, 2021, IEEE IMAGE PROC, P1139, DOI 10.1109/ICIP42928.2021.9506109
   van den Oord A, 2019, Arxiv, DOI [arXiv:1807.03748, DOI 10.48550/ARXIV.1807.03748]
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/ISQED48828.2020.9137057, 10.1109/isqed48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Xi JL, 2022, NEUROCOMPUTING, V483, P116, DOI 10.1016/j.neucom.2022.01.013
   Yan CG, 2021, IEEE T PATTERN ANAL, V43, P1445, DOI 10.1109/TPAMI.2020.2975798
   Yan CG, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3472810
   Yan CG, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3468872
   Yan CG, 2022, IEEE T CIRC SYST VID, V32, P43, DOI 10.1109/TCSVT.2021.3067449
   Yan CG, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3404374
   Yang FX, 2021, PROC CVPR IEEE, P4853, DOI 10.1109/CVPR46437.2021.00482
   Yang QZ, 2019, PROC CVPR IEEE, P3628, DOI 10.1109/CVPR.2019.00375
   Yu YB, 2022, IEEE SIGNAL PROC LET, V29, P75, DOI 10.1109/LSP.2021.3125828
   Yunpeng Zhai, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9018, DOI 10.1109/CVPR42600.2020.00904
   Zhang MY, 2021, AAAI CONF ARTIF INTE, V35, P3360
   Zhang WF, 2020, NEUROCOMPUTING, V411, P20, DOI 10.1016/j.neucom.2020.05.094
   Zhang XY, 2022, PROC CVPR IEEE, P7359, DOI 10.1109/CVPR52688.2022.00722
   Zhang Y, 2018, PROC CVPR IEEE, P4320, DOI 10.1109/CVPR.2018.00454
   Zhang Y, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3542820
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zhihui Z., 2019, P 33 AAAI C ART INT, V27
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhong Z, 2018, PROC CVPR IEEE, pCP99, DOI 10.1109/CVPR.2018.00541
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu XD, 2021, NEUROCOMPUTING, V452, P78, DOI 10.1016/j.neucom.2021.04.120
   Zhu YH, 2020, NEUROCOMPUTING, V403, P88, DOI 10.1016/j.neucom.2020.04.088
   Zilong Ji, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P20, DOI 10.1007/978-3-030-58604-1_2
NR 69
TC 0
Z9 0
U1 9
U2 20
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2024
VL 20
IS 1
AR 27
DI 10.1145/3612926
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T8LL3
UT WOS:001080441800027
DA 2024-08-05
ER

PT J
AU Zhang, Y
   Ding, G
   Ding, D
   Ma, Z
   Li, Z
AF Zhang, Yichi
   Ding, Gongchun
   Ding, Dandan
   Ma, Zhan
   Li, Zhu
TI On Content-Aware Post-Processing: Adapting Statistically Learned Models
   to Dynamic Content
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE VVC; in-loop filtering; post-processing; transformer; Meta-learning
AB Learning-based post-processing methods generally produce neural models that are statistically optimal on their training datasets. These models, however, neglect intrinsic variations of local video content and may fail to process unseen content. To address this issue, this article proposes a content-aware approach for the post-processing of compressed videos. We develop a backbone network, called BackboneFormer, where a Fast Transformer using Separable Self-Attention, Spatial Attention, and Channel Attention is devised to support underlying feature embedding and aggregation. Furthermore, we introduce Meta-learning to strengthen BackboneFormer for better performance. Specifically, we propose Meta Post-Processing (Meta-PP) which leverages the Meta-learning framework to drive BackboneFormer to capture and analyze input video variations for spontaneous updating. Since the original frame is unavailable to the decoder, we devise a Compression Degradation Estimation model where a low-complexity neural model and classic operators are used collaboratively to estimate the compression distortion. The estimated distortion is then utilized to guide the BackboneFormer model for dynamic updating of weighting parameters. Experimental results demonstrate that the proposed BackboneFormer itself gains about 3.61% Bjontegaard delta bit-rate reduction over Versatile Video Coding in the post-processing task and "BackboneFormer + Meta-PP" attains 4.32%, costing only 50K and 61K parameters, respectively. The computational complexity of MACs is 49k/pixel and 50k/pixel, which represents only about 16% of state-of-the-art methods having similar coding gains.
C1 [Zhang, Yichi; Ding, Gongchun; Ding, Dandan] Hangzhou Normal Univ, Hangzhou, Peoples R China.
   [Ma, Zhan] Nanjing Univ, Nanjing, Peoples R China.
   [Li, Zhu] Univ Missouri Kansas City, Kansas City, MO USA.
C3 Hangzhou Normal University; Nanjing University; University of Missouri
   System; University of Missouri Kansas City
RP Ding, D (corresponding author), Hangzhou Normal Univ, Hangzhou, Peoples R China.
EM 1ch.zhang233@gmail.com; 2021112011023@stu.hznu.edu.cn;
   DandanDing@hznu.edu.cn; mazhan@nju.edu.cn; zhu.li@ieee.org
RI Ma, Zhan/HKW-2859-2023; Zhang, Yichi/IUN-3019-2023
OI Ma, Zhan/0000-0003-3686-4057; Zhang, Yichi/0009-0006-7136-4997
FU National Natural Science Foundation of China [62171174]; National
   Undergraduate Training Program for Innovation and Entrepreneurship
FX This research was supported by the National Natural Science Foundation
   of China (62171174) and National Undergraduate Training Program for
   Innovation and Entrepreneurship.
CR Bahat Y, 2017, IEEE I CONF COMP VIS, P3306, DOI 10.1109/ICCV.2017.356
   Bjontegaard G., 2001, Calculation of average PSNR differences between RD-Curves
   Chen HB, 2021, ADV NEUR IN, V34
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Chen LY, 2022, Arxiv, DOI arXiv:2204.04676
   Dai YY, 2017, LECT NOTES COMPUT SC, V10132, P28, DOI 10.1007/978-3-319-51811-4_3
   Ding DD, 2020, IEEE T CIRC SYST VID, V30, P1871, DOI 10.1109/TCSVT.2019.2935508
   Ding Dandan, 2023, IEEE Transactions on Circuits and Systems for Video Technology
   Dong C, 2015, IEEE I CONF COMP VIS, P576, DOI 10.1109/ICCV.2015.73
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Finn C, 2018, Arxiv, DOI arXiv:1710.11622
   Finn C, 2017, PR MACH LEARN RES, V70
   Fu CM, 2012, IEEE T CIRC SYST VID, V22, P1755, DOI 10.1109/TCSVT.2012.2221529
   Fu XY, 2019, IEEE I CONF COMP VIS, P2501, DOI 10.1109/ICCV.2019.00259
   Grant E, 2018, Arxiv, DOI arXiv:1801.08930
   Guan ZY, 2021, IEEE T PATTERN ANAL, V43, P949, DOI 10.1109/TPAMI.2019.2944806
   Guo MH, 2022, COMPUT VIS MEDIA, V8, P331, DOI 10.1007/s41095-022-0271-y
   Han K, 2022, Arxiv, DOI arXiv:2012.12556
   Haoyu Ma, 2022, ACM Transactions on Multimedia Computing, Communications and Applications, V18, DOI 10.1145/3477553
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu XC, 2019, PROC CVPR IEEE, P1575, DOI 10.1109/CVPR.2019.00167
   Huang ZJ, 2022, IEEE T CIRC SYST VID, V32, P2342, DOI 10.1109/TCSVT.2021.3089498
   Huang ZJ, 2021, IEEE T IMAGE PROCESS, V30, P5439, DOI 10.1109/TIP.2021.3084345
   Ignatov A, 2019, LECT NOTES COMPUT SC, V11133, P315, DOI 10.1007/978-3-030-11021-5_20
   Jia CM, 2019, IEEE T IMAGE PROCESS, V28, P3343, DOI 10.1109/TIP.2019.2896489
   Jia W, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3460820
   Karczewicz M, 2021, IEEE T CIRC SYST VID, V31, P3907, DOI 10.1109/TCSVT.2021.3072297
   Kingma D. P., 2014, arXiv
   Kong LY, 2020, IEEE IMAGE PROC, P3379, DOI [10.1109/icip40778.2020.9190807, 10.1109/ICIP40778.2020.9190807]
   Li Y, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3529107
   Lin K, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3502723
   Liu C, 2020, IEEE INT C MULTIMEDI
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Ma D, 2021, IEEE J-STSP, V15, P378, DOI 10.1109/JSTSP.2020.3043064
   Mehta S, 2022, Separable self-attention for mobile vision transformers
   Messina N, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3451390
   Mosseri I, 2013, IEEE INT CONF COMPUT
   Nichol A, 2018, Arxiv, DOI arXiv:1803.02999
   Norkin A, 2012, IEEE T CIRC SYST VID, V22, P1746, DOI 10.1109/TCSVT.2012.2223053
   Pan ZQ, 2020, IEEE T IMAGE PROCESS, V29, P5352, DOI 10.1109/TIP.2020.2982534
   Qunliang Xing, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P275, DOI 10.1007/978-3-030-58517-4_17
   Schwarz Heiko, 2014, Integrated Circuits and Systems, V39, P49
   Seobin Park, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P754, DOI 10.1007/978-3-030-58583-9_45
   Soh JW, 2020, PROC CVPR IEEE, P3513, DOI 10.1109/CVPR42600.2020.00357
   Tsai CY, 2013, IEEE J-STSP, V7, P934, DOI 10.1109/JSTSP.2013.2271974
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang DZ, 2021, IEEE T IMAGE PROCESS, V30, P4198, DOI 10.1109/TIP.2021.3068638
   Wang H., 2022, WG 05 MPEG Joint Video Coding Team(s) with ITU-T SG 16, JVET-AA0111
   Wang Jiahao, 2023, 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), P14443, DOI 10.1109/CVPR52729.2023.01388
   Wang S, 2022, INT CONF ACOUST SPEE, P1630, DOI 10.1109/ICASSP43922.2022.9746146
   Wang TF, 2021, PROC CVPR IEEE, P5116, DOI 10.1109/CVPR46437.2021.00508
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wang XT, 2021, IEEE INT CONF COMP V, P1905, DOI 10.1109/ICCVW54120.2021.00217
   Wang Z, 2021, IEEE DATA COMPR CONF, P23, DOI 10.1109/DCC50243.2021.00010
   Wang ZD, 2022, PROC CVPR IEEE, P17662, DOI 10.1109/CVPR52688.2022.01716
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xiao Jie, 2022, P 36 C NEUR INF PROC
   Xing YZ, 2021, PROC CVPR IEEE, P6283, DOI 10.1109/CVPR46437.2021.00622
   Yang FZ, 2020, PROC CVPR IEEE, P5790, DOI 10.1109/CVPR42600.2020.00583
   Yu WH, 2022, PROC CVPR IEEE, P10809, DOI 10.1109/CVPR52688.2022.01055
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
NR 61
TC 1
Z9 1
U1 6
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2024
VL 20
IS 1
AR 28
DI 10.1145/3612925
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T8LL3
UT WOS:001080441800028
DA 2024-08-05
ER

PT J
AU Lei, SC
   Gong, YJ
   Xiao, XL
   Zhou, YC
   Zhang, J
AF Lei, Si-Chao
   Gong, Yue-Jiao
   Xiao, Xiao-Lin
   Zhou, Yi-Cong
   Zhang, Jun
TI Tensorial Evolutionary Optimization for Natural Image Matting
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Natural image matting; tensorial evolutionary algorithm; heuristic
   optimization
AB Natural image matting has garnered increasing attention in various computer vision applications. The matting problem aims to find the optimal foreground/background (F/B) color pair for each unknown pixel and thus obtain an alpha matte indicating the opacity of the foreground object. This problem is typically modeled as a large-scale pixel pair combinatorial optimization (PPCO) problem. Heuristic optimization is widely employed to tackle the PPCO problem owing to its gradient-free property and promising search ability. However, traditional heuristic methods often encode F/B solutions to a one-dimensional (1D) representation and then evolve the solutions in a 1D manner. This 1D representation destroys the intrinsic two-dimensional (2D) structure of images, where the significant spatial correlations among pixels are ignored. Moreover, the 1D representation also brings operation inefficiency. To address the above issues, this article develops a spatial-aware tensorial evolutionary image matting (TEIM) method. Specifically, the matting problem is modeled as a 2D Spatial-PPCO (S-PPCO) problem, and a global tensorial evolutionary optimizer is proposed to tackle the S-PPCO problem. The entire population is represented as a whole by a third-order tensor, in which individuals are classified into two types: F and B individuals for denoting the 2D F/B solutions, respectively. The evolution process, consisting of three tensorial evolutionary operators, is implemented based on pure tensor computation for efficiently seeking F/B solutions. The local spatial smoothness of images is also integrated into the evaluation process for obtaining a high-quality alpha matte. Experimental results compared with state-of-the-art methods validate the effectiveness of TEIM.
C1 [Lei, Si-Chao; Gong, Yue-Jiao] South China Univ Technol, Sch Comp Sci & Technol, Univ Town Campus, Guangzhou 510000, Guangdong, Peoples R China.
   [Xiao, Xiao-Lin] South China Normal Univ, Sch Comp Sci, Guangzhou 510000, Guangdong, Peoples R China.
   [Zhou, Yi-Cong] Univ Macau, Dept Comp & Informat Sci, Macau 999078, Peoples R China.
   [Zhang, Jun] Hanyang Univ, Dept Elect & Elect Engn, Seoul 15588, South Korea.
C3 South China University of Technology; South China Normal University;
   University of Macau; Hanyang University
RP Gong, YJ (corresponding author), South China Univ Technol, Sch Comp Sci & Technol, Univ Town Campus, Guangzhou 510000, Guangdong, Peoples R China.; Xiao, XL (corresponding author), South China Normal Univ, Sch Comp Sci, Guangzhou 510000, Guangdong, Peoples R China.
EM cssclei@outlook.com; gongyuejiao@gmail.com; shellyxiaolin@gmail.com;
   yicongzhou@um.edu.mo; jun-zhang@ieee.org
RI Zhang, Jun/E-9359-2011; Xiao, Xiangli/JQJ-2169-2023; Zhou,
   Yicong/A-8017-2009
OI Zhang, Jun/0000-0001-7835-9871; Xiao, Xiangli/0000-0002-3250-0603; Zhou,
   Yicong/0000-0002-4487-6384
FU National Natural Science Foundation of China [62276100]; Guangdong
   Natural Science Funds for Distinguished Young Scholars
   [2022B1515020049]; Guangdong Regional Joint Funds for Basic and Applied
   Research [2021B1515120078]; TCL Young Scholars Program
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62276100, in part by the Guangdong
   Natural Science Funds for Distinguished Young Scholars under Grant
   2022B1515020049, in part by the Guangdong Regional Joint Funds for Basic
   and Applied Research under Grant 2021B1515120078, and in part by the TCL
   Young Scholars Program.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Aksoy Y, 2017, PROC CVPR IEEE, P228, DOI 10.1109/CVPR.2017.32
   Amin B, 2020, IET IMAGE PROCESS, V14, P1628, DOI 10.1049/iet-ipr.2019.0067
   [Anonymous], 2015, P GEN EV COMP C
   Cai ZQ, 2019, MEMET COMPUT, V11, P53, DOI 10.1007/s12293-018-0275-4
   Cai ZQ, 2017, SOFT COMPUT, V21, P4417, DOI 10.1007/s00500-016-2250-7
   Cao GY, 2016, I C VIRTUAL REALITY, P24, DOI 10.1109/ICVRV.2016.13
   Chen QF, 2013, IEEE T PATTERN ANAL, V35, P2175, DOI 10.1109/TPAMI.2013.18
   Chen X, 2017, IEEE ACCESS, V5, P27732, DOI 10.1109/ACCESS.2017.2773651
   Chuang YY, 2001, PROC CVPR IEEE, P264
   Dai YT, 2021, PROC CVPR IEEE, P6837, DOI 10.1109/CVPR46437.2021.00677
   Ding HH, 2022, IEEE T IMAGE PROCESS, V31, P2421, DOI 10.1109/TIP.2022.3155958
   [冯夫健 Feng Fujian], 2020, [中国科学. 信息科学, Scientia Sinica Informationis], V50, P424
   Feng XX, 2016, LECT NOTES COMPUT SC, V9906, P204, DOI 10.1007/978-3-319-46475-6_13
   Garcia Luis Pedro., 2014, Ann. Par. GPU Prog., V1, P1
   Gastal ESL, 2010, COMPUT GRAPH FORUM, V29, P575, DOI 10.1111/j.1467-8659.2009.01627.x
   Hebborn AK, 2017, PROCEEDINGS OF THE 2017 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR), P62, DOI 10.1109/ISMAR.2017.23
   Hou QQ, 2019, IEEE I CONF COMP VIS, P4129, DOI 10.1109/ICCV.2019.00423
   Huang H, 2019, SOFT COMPUT, V23, P4421, DOI 10.1007/s00500-018-3098-9
   Huang H, 2019, IEEE T IMAGE PROCESS, V28, P3739, DOI 10.1109/TIP.2019.2902830
   Johnson J, 2016, IEEE T IMAGE PROCESS, V25, P3032, DOI 10.1109/TIP.2016.2555705
   Johnson Jubin, 2014, P BRIT MACH VIS C BM, V1, P5
   Kaiming He, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2049, DOI 10.1109/CVPR.2011.5995495
   Karacan L, 2015, IEEE I CONF COMP VIS, P424, DOI 10.1109/ICCV.2015.56
   Kilmer ME, 2011, LINEAR ALGEBRA APPL, V435, P641, DOI 10.1016/j.laa.2010.09.020
   Lei Si-Chao, 2022, IEEE Trans. Artif. Intell., V2022
   Lepcha Dawa Chyophel, 2021, Int. J. Image Graph., V2021
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P1699, DOI 10.1109/TPAMI.2008.168
   Li JZ, 2023, Arxiv, DOI arXiv:2304.04672
   Li YY, 2020, AAAI CONF ARTIF INTE, V34, P11450
   Li ZQ, 2015, PROC CVPR IEEE, P1356, DOI 10.1109/CVPR.2015.7298741
   Liang YH, 2023, APPL SOFT COMPUT, V143, DOI 10.1016/j.asoc.2023.110407
   Liang YH, 2020, IEEE ACCESS, V8, P93487, DOI 10.1109/ACCESS.2020.2995207
   Liang YH, 2020, FRONT COMPUT SCI-CHI, V14, DOI 10.1007/s11704-019-8441-5
   Liang YH, 2019, IEEE T FUZZY SYST, V27, P1100, DOI 10.1109/TFUZZ.2019.2896533
   Liang YH, 2018, LECT NOTES COMPUT SC, V10955, P656, DOI 10.1007/978-3-319-95933-7_75
   Lin SC, 2022, IEEE WINT CONF APPL, P3132, DOI 10.1109/WACV51458.2022.00319
   Qin XF, 2019, IEEE ACCESS, V7, P41669, DOI 10.1109/ACCESS.2019.2907282
   Rhemann C, 2009, PROC CVPR IEEE, P1826, DOI 10.1109/CVPRW.2009.5206503
   Shahrian E, 2013, PROC CVPR IEEE, P636, DOI 10.1109/CVPR.2013.88
   Shahrian E, 2012, PROC CVPR IEEE, P718, DOI 10.1109/CVPR.2012.6247741
   Shuang Liu, 2021, 2021 IEEE International Conference on Computer Science, Electronic Information Engineering and Intelligent Control Technology (CEI), P421, DOI 10.1109/CEI52496.2021.9574550
   Sun J, 2004, ACM T GRAPHIC, V23, P315, DOI 10.1145/1015706.1015721
   Tang JW, 2019, PROC CVPR IEEE, P3050, DOI 10.1109/CVPR.2019.00317
   Varnousfaderani ES, 2013, IEEE T IMAGE PROCESS, V22, P4260, DOI 10.1109/TIP.2013.2271549
   Wang Jue, 2007, P IEEE C COMP VIS PA, P1
   Wei T., 2021, P IEEECVF C COMPUTER, P15374
   Xie Jun, 2022, P IEEE INT C MULT EX, P1
   Yan XM, 2018, INT J MACH LEARN CYB, V9, P621, DOI 10.1007/s13042-016-0584-1
   Yuan GJ, 2021, MULTIMED TOOLS APPL, V80, P6143, DOI 10.1007/s11042-020-09908-0
   Zhan ZH, 2022, IEEE T EM TOP COMP I, V6, P315, DOI 10.1109/TETCI.2020.3047410
   Zhu XY, 2018, MULTIMED TOOLS APPL, V77, P19089, DOI 10.1007/s11042-017-5357-7
NR 52
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2024
VL 20
IS 7
SI SI
AR 194
DI 10.1145/3649138
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL0Q6
UT WOS:001234494100009
DA 2024-08-05
ER

PT J
AU Li, F
   Chen, YX
   Liu, HY
   Zhao, ZX
   Yao, YZ
   Liao, X
AF Li, Fan
   Chen, Yanxiang
   Liu, Haiyang
   Zhao, Zuxing
   Yao, Yuanzhi
   Liao, Xin
TI Vocoder Detection of Spoofing Speech Based on GAN Fingerprints and
   Domain Generalization
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Speech forgery; vocoder; GAN fingerprint; domain generalization;
   curriculum learning
ID FEATURES
AB As an important part of the text-to-speech (TTS) system, vocoders convert acoustic features into speech waveforms. The difference in vocoders is key to producing different types of forged speech in the TTS system. With the rapid development of general adversarial networks (GANs), an increasing number of GAN vocoders have been proposed. Detectors often encounter vocoders of unknown types, which leads to a decline in the generalization performance of models. However, existing studies lack research on detection generalization based on GAN vocoders. To solve this problem, this study proposes vocoder detection of spoofed speech based on GAN fingerprints and domain generalization. The framework can widen the distance between real speech and forged speech in feature space, improving the detection model's performance. Specifically, we utilize a fingerprint extractor based on an autoencoder to extract GAN fingerprints from vocoders. We then weight them to the forged speech for subsequent classification to learn the forged speech features with high differentiation. Subsequently, domain generalization is used to further improve the generalization ability of the model for unseen forgery types. We achieve domain generalization using domain-adversarial learning and asymmetric triplet loss to learn a better generalized feature space in which real speech is compact and forged speech synthesized by different vocoders is dispersed. Finally, to optimize the training process, curriculum learning is used to dynamically adjust the contributions of the samples with different difficulties in the training process. Experimental results show that the proposed method achieves the most advanced detection results among four GAN vocoders. The code is available at https://github.com/multimedia-infomation-security/GAN-Vocoder-detection.
C1 [Li, Fan; Chen, Yanxiang; Liu, Haiyang; Zhao, Zuxing; Yao, Yuanzhi] Hefei Univ Technol, Hefei 230601, Anhui, Peoples R China.
   [Liao, Xin] Hunan Univ, Changsha 410082, Hunan, Peoples R China.
C3 Hefei University of Technology; Hunan University
RP Chen, YX (corresponding author), Hefei Univ Technol, Hefei 230601, Anhui, Peoples R China.
EM 2021111020@mail.hfut.edu.cn; chenyx@hfut.edu.cn; 1405357533@qq.com;
   2021171120@mail.hfut.edu.cn; yaoyz@hfut.edu.cn; xinliao@hnu.edu.cn
FU National Natural Science Foundation of China [61972127, 61972142,
   U22A2030]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 61972127, 61972142, U22A2030.
CR Castells T., 2020, Advances in Neural Information Processing Systems, P4308
   Chen ZX, 2017, INTERSPEECH, P102, DOI 10.21437/Interspeech.2017-1085
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Graves A, 2017, PR MACH LEARN RES, V70
   Hua G, 2021, IEEE SIGNAL PROC LET, V28, P1265, DOI 10.1109/LSP.2021.3089437
   Kong J., 2020, Advances in Neural Information Processing Systems, V33, P17022
   Kumar AK, 2021, INT J SPEECH TECHNOL, V24, P193, DOI 10.1007/s10772-020-09785-w
   Kumar K, 2019, ADV NEUR IN, V32
   Lavrentyeva G, 2019, Arxiv, DOI arXiv:1904.05576
   Lavrentyeva G, 2017, INTERSPEECH, P82, DOI 10.21437/Interspeech.2017-360
   Lei ZC, 2020, INTERSPEECH, P1116, DOI 10.21437/Interspeech.2020-2723
   Li H, 2018, Arxiv, DOI [arXiv:1806.09276, DOI 10.48550/ARXIV.1806.09276]
   Li HL, 2018, PROC CVPR IEEE, P5400, DOI 10.1109/CVPR.2018.00566
   Li X., 2021, arXiv
   Ma JC, 2022, PROCEEDINGS OF THE 31ST ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, CIKM 2022, P4309, DOI 10.1145/3511808.3557574
   Ma KJ, 2023, IEEE SIGNAL PROC LET, V30, P359, DOI 10.1109/LSP.2023.3262419
   Marra F, 2019, 2019 2ND IEEE CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2019), P506, DOI 10.1109/MIPR.2019.00103
   Mo YCA, 2022, INT CONF ACOUST SPEE, P6392, DOI 10.1109/ICASSP43922.2022.9746059
   Mustafa A, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P6034, DOI 10.1109/ICASSP39728.2021.9413605
   Patel TB, 2017, IEEE J-STSP, V11, P644, DOI 10.1109/JSTSP.2017.2682788
   Paul D, 2017, IEEE J-STSP, V11, P605, DOI 10.1109/JSTSP.2017.2684705
   Pengxiang Xu, 2021, ICCAI 2021: 2021 7th International Conference on Computing and Artificial Intelligence, P196, DOI 10.1145/3467707.3467736
   Shao R, 2019, PROC CVPR IEEE, P10015, DOI 10.1109/CVPR.2019.01026
   Sun Chengzhe, 2023, 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P904, DOI 10.1109/CVPRW59228.2023.00097
   Tak H, 2021, Arxiv, DOI arXiv:2107.12710
   Tak H, 2020, Arxiv, DOI [arXiv:2005.10393, DOI 10.48550/ARXIV.2005.10393, 10.48550/arXiv.2005.10393]
   Todisco M, 2018, INTERSPEECH, P77
   Todisco Massimiliano, 2016, ODYSSEY, V2016, P283, DOI DOI 10.21437/ODYSSEY.2016-41
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   Van den Oord A., 2016, CoRR, P125, DOI DOI 10.1109/ICASSP.2009.4960364
   Wang HM, 2022, Arxiv, DOI [arXiv:2212.13466, 10.48550/arXiv:2212.13466, DOI 10.48550/ARXIV:2212.13466]
   Wang WF, 2016, INTERSPEECH, P2243, DOI 10.21437/Interspeech.2016-134
   Wang X, 2021, Arxiv, DOI arXiv:2103.11326
   Wang YX, 2017, INTERSPEECH, P4006, DOI 10.21437/Interspeech.2017-1452
   Wei Han, 2006, 2006 IEEE International Symposium on Circuits and Systems (IEEE Cat. No. 06CH37717C)
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Yamamoto R, 2020, INT CONF ACOUST SPEE, P6199, DOI [10.1109/ICASSP40776.2020.9053795, 10.1109/icassp40776.2020.9053795]
   Yan Xinrui, 2022, DDAM '22: Proceedings of the 1st International Workshop on Deepfake Detection for Audio Multimedia, P61, DOI 10.1145/3552466.3556525
   Yu N, 2019, IEEE I CONF COMP VIS, P7555, DOI 10.1109/ICCV.2019.00765
   Zhang Y., 2021, P INTERSPEECH
   Zhang Y, 2021, IEEE SIGNAL PROC LET, V28, P937, DOI 10.1109/LSP.2021.3076358
NR 41
TC 1
Z9 1
U1 3
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2024
VL 20
IS 6
SI SI
AR 157
DI 10.1145/3630751
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OQ2T3
UT WOS:001208681800007
DA 2024-08-05
ER

PT J
AU Chen, HH
   Li, ZQ
   Wei, MQ
   Wang, J
AF Chen, Honghua
   Li, Zhiqi
   Wei, Mingqing
   Wang, Jun
TI Geometric and Learning-Based Mesh Denoising: A Comprehensive Survey
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Mesh denoising; deep learning; low-rank recovery; optimization method
ID DIFFUSION; REGULARIZATION; PHOTOGRAPHY; FLASH; NET
AB Mesh denoising is a fundamental problem in digital geometry processing. It seeks to remove surface noise while preserving surface intrinsic signals as accurately as possible. While traditional wisdom has been built upon specialized priors to smooth surfaces, learning-based approaches are making their debut with great success in generalization and automation. In this work, we provide a comprehensive review of the advances in mesh denoising, containing both traditional geometric approaches and recent learning-based methods. First, to familiarize readers with the denoising tasks, we summarize four common issues in mesh denoising. We then provide two categorizations of the existing denoising methods. Furthermore, three important categories, including optimization-, filter-, and data-driven-based techniques, are introduced and analyzed in detail, respectively. Both qualitative and quantitative comparisons are illustrated, to demonstrate the effectiveness of the state-of-the-art denoising methods. Finally, potential directions of future work are pointed out to solve the common problems of these approaches. A mesh denoising benchmark is also built in this work, and future researchers will easily and conveniently evaluate their methods with state-of-the-art approaches. To aid reproducibility, we release our datasets and used results at https://github.com/chenhonghua/Mesh-Denoiser.
C1 [Chen, Honghua; Wei, Mingqing; Wang, Jun] Nanjing Univ Aeronaut & Astronaut, 29 Yudao St, Nanjing 210016, Peoples R China.
   [Li, Zhiqi] Bournemouth Univ, Fac Media & Commun, Poole BH12 5BB, Dorset, England.
C3 Nanjing University of Aeronautics & Astronautics; Bournemouth University
RP Wang, J (corresponding author), Nanjing Univ Aeronaut & Astronaut, 29 Yudao St, Nanjing 210016, Peoples R China.
EM chenhonghuacn@gmail.com; nanjzq16@gmail.com; mingqiang.wei@gmail.com;
   wjun@nuaa.edu.cn
OI Honghua, Chen/0000-0001-7473-1146
FU National Natural Science Foundation of China [52275493, 92267201,
   62172218]
FX This work was supported by the National Natural Science Foundation of
   China (No. 52275493, No. 92267201, No. 62172218).
CR Afzal H, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3177756
   Armando M, 2022, IEEE T VIS COMPUT GR, V28, P2999, DOI 10.1109/TVCG.2020.3045490
   Arvanitis G, 2019, IEEE T VIS COMPUT GR, V25, P1513, DOI 10.1109/TVCG.2018.2802926
   Avron H, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1857907.1857911
   Bahirat K, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3209661
   Bajaj CL, 2003, ACM T GRAPHIC, V22, P4, DOI 10.1145/588272.588276
   Bian Z, 2011, COMPUT AIDED GEOM D, V28, P50, DOI 10.1016/j.cagd.2010.10.001
   Botsch J, 2022, IEEE ACCESS, V10, P38635, DOI 10.1109/ACCESS.2022.3164714
   Botsch M, 2010, Polygon Mesh Processing, DOI DOI 10.1201/B10688
   Bülow T, 2004, IEEE T PATTERN ANAL, V26, P1650, DOI 10.1109/TPAMI.2004.129
   Centin M, 2018, IEEE T VIS COMPUT GR, V24, P2380, DOI 10.1109/TVCG.2017.2731771
   Chen HH, 2022, INT J COMPUT VISION, V130, P615, DOI 10.1007/s11263-021-01564-7
   Chen HH, 2019, COMPUT AIDED DESIGN, V115, P122, DOI 10.1016/j.cad.2019.05.036
   Cheng X, 2014, COMPUT GRAPH-UK, V38, P150, DOI 10.1016/j.cag.2013.10.025
   Clarenz U, 2000, IEEE VISUAL, P397, DOI 10.1109/VISUAL.2000.885721
   Dai CF, 2020, Arxiv, DOI arXiv:2008.01358
   Desbrun M, 1999, COMP GRAPH, P317, DOI 10.1145/311535.311576
   Eisemann E, 2004, ACM T GRAPHIC, V23, P673, DOI 10.1145/1015706.1015778
   El Ouafdi AF, 2008, COMPUT GRAPH FORUM, V27, P1357, DOI 10.1111/j.1467-8659.2008.01275.x
   Fan HQ, 2010, IEEE T VIS COMPUT GR, V16, P312, DOI 10.1109/TVCG.2009.70
   Fleishman S, 2003, ACM T GRAPHIC, V22, P950, DOI 10.1145/882262.882368
   Gao ZH, 2013, GRAPH MODELS, V75, P23, DOI 10.1016/j.gmod.2012.10.007
   Gu SH, 2014, PROC CVPR IEEE, P2862, DOI 10.1109/CVPR.2014.366
   Guo MQ, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21020412
   Hattori S, 2022, LECT NOTES COMPUT SC, V13663, P363, DOI 10.1007/978-3-031-20062-5_21
   He L, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461965
   Hermosilla P, 2019, IEEE I CONF COMP VIS, P52, DOI 10.1109/ICCV.2019.00014
   Hildebrandt K, 2004, COMPUT GRAPH FORUM, V23, P391, DOI 10.1111/j.1467-8659.2004.00770.x
   Huang H, 2008, SIAM J SCI COMPUT, V31, P74, DOI 10.1137/060676684
   Ji Zhongping, 2006, International Journal of CAD/CAM, V6, P89
   Jones TR, 2003, ACM T GRAPHIC, V22, P943, DOI 10.1145/882262.882367
   Kim B, 2005, COMPUT GRAPH FORUM, V24, P295, DOI 10.1111/j.1467-8659.2005.00854.x
   Lee KW, 2005, INT C COMP AID DES C, P275
   Li T, 2019, GRAPH MODELS, V101, P17, DOI 10.1016/j.gmod.2018.12.002
   Li T, 2017, FRONT INFORM TECH EL, V18, P1828, DOI 10.1631/FITEE.1601229
   Li XZ, 2021, IEEE T VIS COMPUT GR, V27, P4060, DOI 10.1109/TVCG.2020.3001681
   Li XZ, 2018, COMPUT GRAPH FORUM, V37, P155, DOI 10.1111/cgf.13556
   Li ZQ, 2020, COMPUT AIDED DESIGN, V127, DOI 10.1016/j.cad.2020.102861
   Liu B, 2023, COMPUT AIDED DESIGN, V154, DOI 10.1016/j.cad.2022.103422
   Liu XG, 2002, GRAPH MODELS, V64, P169, DOI 10.1006/gmod.2002.0576
   Liu YW, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13112145
   Liu Z, 2022, IEEE T VIS COMPUT GR, V28, P4418, DOI 10.1109/TVCG.2021.3088118
   Liu Z, 2019, COMPUT AIDED GEOM D, V71, P190, DOI 10.1016/j.cagd.2019.04.013
   Liu Z, 2019, SIAM J SCI COMPUT, V41, pB1, DOI 10.1137/17M115743X
   Lu XQ, 2022, IEEE T VIS COMPUT GR, V28, P1835, DOI 10.1109/TVCG.2020.3026785
   Lu XQ, 2017, COMPUT AIDED GEOM D, V54, P49, DOI 10.1016/j.cagd.2017.02.011
   Lu XQ, 2017, OPT LASER ENG, V90, P186, DOI 10.1016/j.optlaseng.2016.09.003
   Lu XQ, 2016, IEEE T VIS COMPUT GR, V22, P1181, DOI 10.1109/TVCG.2015.2500222
   Nehab D, 2005, ACM T GRAPHIC, V24, P536, DOI 10.1145/1073204.1073226
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Nousias S, 2021, IEEE T IND INFORM, V17, P980, DOI 10.1109/TII.2020.3000491
   Ohtake Y., 2000, Proceedings Geometric Modeling and Processing 2000. Theory and Applications, P229, DOI 10.1109/GMAP.2000.838255
   Ohtake Y, 2001, COMPUT AIDED DESIGN, V33, P789, DOI 10.1016/S0010-4485(01)00095-1
   Pan W, 2020, COMPUT AIDED DESIGN, V121, DOI 10.1016/j.cad.2019.102807
   Petschnigg G, 2004, ACM T GRAPHIC, V23, P664, DOI 10.1145/1015706.1015777
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Rakotosaona MJ, 2020, COMPUT GRAPH FORUM, V39, P185, DOI 10.1111/cgf.13753
   RenjieWang Wenbo Zhao, 2017, P THE18TH PACIFICRIM, P920
   Roveri R, 2018, COMPUT GRAPH FORUM, V37, P87, DOI 10.1111/cgf.13344
   Shen YF, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3480168
   Shen YZ, 2004, IEEE T VIS COMPUT GR, V10, P252, DOI 10.1109/TVCG.2004.1272725
   Solomon J, 2014, Arxiv, DOI arXiv:1405.4734
   Su ZX, 2009, SMI 2009: IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P1, DOI 10.1109/SMI.2009.5170156
   Sun XF, 2007, IEEE T VIS COMPUT GR, V13, P925, DOI 10.1109/TVCG.2007.1065
   Sun YJ, 2015, COMPUT AIDED GEOM D, V35-36, P2, DOI 10.1016/j.cagd.2015.03.011
   Tang WM, 2023, COMPUT VIS IMAGE UND, V233, DOI 10.1016/j.cviu.2023.103710
   Tasdizen T, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P125, DOI 10.1109/VISUAL.2002.1183766
   Taubin G., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P351, DOI 10.1145/218380.218473
   Taubin Gabriel., 2001, Res. Rep. RC2213 IBM, V1, P4
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Tsuchie S, 2012, GRAPH MODELS, V74, P130, DOI 10.1016/j.gmod.2012.03.010
   Vollmer J, 1999, COMPUT GRAPH FORUM, V18, pC131, DOI 10.1111/1467-8659.00334
   Wang CCL, 2006, IEEE T VIS COMPUT GR, V12, P629, DOI 10.1109/TVCG.2006.60
   Wang J, 2019, COMPUT AIDED DESIGN, V114, P133, DOI 10.1016/j.cad.2019.05.027
   Wang J, 2012, COMPUT AIDED DESIGN, V44, P597, DOI 10.1016/j.cad.2012.03.001
   Wang J, 2012, J COMPUT SCI TECH-CH, V27, P163, DOI 10.1007/s11390-012-1214-3
   Wang J, 2011, GRAPH MODELS, V73, P127, DOI 10.1016/j.gmod.2011.01.002
   Wang PS, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980232
   Wang PS, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818068
   Wang RM, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2557449
   Wei M, 2019, COMPUT GRAPH FORUM, V38, P591, DOI 10.1111/cgf.13863
   Wei MQ, 2019, IEEE T VIS COMPUT GR, V25, P2910, DOI 10.1109/TVCG.2018.2865363
   Wei MQ, 2018, OPT LASER ENG, V103, P110, DOI 10.1016/j.optlaseng.2017.11.014
   Wei MQ, 2017, IEEE T AUTOM SCI ENG, V14, P931, DOI 10.1109/TASE.2016.2553449
   Wei MQ, 2015, IEEE T VIS COMPUT GR, V21, P43, DOI 10.1109/TVCG.2014.2326872
   Wei MQ, 2013, OPT LASER ENG, V51, P1223, DOI 10.1016/j.optlaseng.2013.04.018
   Wu XQ, 2015, COMPUT GRAPH FORUM, V34, P35, DOI 10.1111/cgf.12743
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Yadav SK, 2019, IEEE T VIS COMPUT GR, V25, P2304, DOI 10.1109/TVCG.2018.2828818
   Yadav SK, 2018, IEEE T VIS COMPUT GR, V24, P2366, DOI 10.1109/TVCG.2017.2740384
   Yagou H, 2003, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P28, DOI 10.1109/CGI.2003.1214444
   Yagou H, 2002, GEOMETRIC MODELING AND PROCESSING: THEORY AND APPLICATIONS, PROCEEDINGS, P124, DOI 10.1109/GMAP.2002.1027503
   Yoshizawa S, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P38
   Yu H, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3514248
   Yu JZ, 2014, OPT LASER ENG, V62, P57, DOI 10.1016/j.optlaseng.2014.05.002
   Yu LQ, 2018, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2018.00295
   Yu YZ, 2004, ACM T GRAPHIC, V23, P644, DOI 10.1145/1015706.1015774
   Zhang H, 2010, COMPUT GRAPH FORUM, V29, P1865, DOI 10.1111/j.1467-8659.2010.01655.x
   Zhang HY, 2015, IEEE T VIS COMPUT GR, V21, P873, DOI 10.1109/TVCG.2015.2398432
   Zhang JY, 2019, IEEE T VIS COMPUT GR, V25, P1774, DOI 10.1109/TVCG.2018.2816926
   Zhang WY, 2015, COMPUT GRAPH FORUM, V34, P23, DOI 10.1111/cgf.12742
   Zhang Y, 2007, IEEE T IMAGE PROCESS, V16, P1036, DOI 10.1109/TIP.2007.891787
   Zhang YK, 2022, COMPUT AIDED DESIGN, V144, DOI 10.1016/j.cad.2021.103154
   Zhao W., 2017, PACIFIC RIM C MULTIM, P645
   Zhao W., 2019, arXiv
   Zhao WB, 2021, IEEE T VIS COMPUT GR, V27, P1937, DOI 10.1109/TVCG.2019.2944357
   Zhao Y, 2018, COMPUT AIDED DESIGN, V101, P82, DOI 10.1016/j.cad.2018.04.001
   Zheng YY, 2011, IEEE T VIS COMPUT GR, V17, P1521, DOI 10.1109/TVCG.2010.264
   Zhong SS, 2018, COMPUT ANIMAT VIRT W, V29, DOI 10.1002/cav.1827
   Zhou L, 2022, GRAPH MODELS, V121, DOI 10.1016/j.gmod.2022.101140
   Zhu DK, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3061247
   Zhu L, 2013, COMPUT GRAPH FORUM, V32, P371, DOI 10.1111/cgf.12245
NR 112
TC 0
Z9 0
U1 1
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2024
VL 20
IS 3
AR 85
DI 10.1145/3625098
PG 28
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6F8
UT WOS:001153381000025
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Xu, J
   Liu, B
   Zhou, Y
   Liu, M
   Yao, R
   Shao, Z
AF Xu, Jing
   Liu, Bing
   Zhou, Yong
   Liu, Mingming
   Yao, Rui
   Shao, Zhiwen
TI Diverse Image Captioning via Conditional Variational Autoencoder and
   Dual Contrastive Learning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Diverse image captioning; variational autoencoder; contrastive learning;
   sequential latent space
AB Diverse image captioning has achieved substantial progress in recent years. However, the discriminability of generative models and the limitation of cross entropy loss are generally overlooked in the traditional diverse image captioning models, which seriously hurts both the diversity and accuracy of image captioning. In this article, aiming to improve diversity and accuracy simultaneously, we propose a novel Conditional Variational Autoencoder (DCL-CVAE) framework for diverse image captioning by seamlessly integrating sequential variational autoencoder with contrastive learning. In the encoding stage, we first build conditional variational autoencoders to separately learn the sequential latent spaces for a pair of captions. Then, we introduce contrastive learning in the sequential latent spaces to enhance the discriminability of latent representations for both image-caption pairs andmismatched pairs. In the decoding stage, we leverage the captions sampled from the pre-trained Long Short-Term Memory (LSTM), LSTM decoder as the negative examples and perform contrastive learning with the greedily sampled positive examples, which can restrain the generation of common words and phrases induced by the cross entropy loss. By virtue of dual constrastive learning, DCL-CVAE is capable of encouraging the discriminability and facilitating the diversity, while promoting the accuracy of the generated captions. Extensive experiments are conducted on the challenging MSCOCO dataset, showing that our proposed methods can achieve a better balance between accuracy and diversity compared to the state-of-the-art diverse image captioning models.
C1 [Xu, Jing; Liu, Bing; Zhou, Yong; Liu, Mingming; Yao, Rui; Shao, Zhiwen] China Univ Min & Technol, Sch Comp Sci & Technol, Engn Res Ctr Mine Digitizat, Minist Educ, Xuzhou 221116, Jiangsu, Peoples R China.
C3 China University of Mining & Technology
RP Liu, B (corresponding author), China Univ Min & Technol, Sch Comp Sci & Technol, Engn Res Ctr Mine Digitizat, Minist Educ, Xuzhou 221116, Jiangsu, Peoples R China.
EM xuj@cumt.edu.cn; liubing@cumt.edu.cn; yzhou@cumt.edu.cn;
   jsjzi_lmm@126.com; ruiyao@cumt.edu.cn; zhiwen_shao@cumt.edu.cn
OI Liu, Bing/0000-0002-2365-6606; Yao, Rui/0000-0003-2734-915X
FU National Natural Science Foundation of China [62276266, 61801198,
   62272461, 62172417, 62106268]
FX This work was supported by the National Natural Science Foundation of
   China (No. 62276266, 61801198, 62272461, 62172417, 62106268).
CR ahajan S., 2020, ARXIV201100966, P3613
   Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24
   Aneja J, 2019, IEEE I CONF COMP VIS, P4260, DOI 10.1109/ICCV.2019.00436
   Barnard K, 2003, J MACH LEARN RES, V3, P1107, DOI 10.1162/153244303322533214
   Chen X, 2015, PROC CVPR IEEE, P2422, DOI 10.1109/CVPR.2015.7298856
   Chung J, 2015, ADV NEUR IN, V28
   Cornia Marcella, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10575, DOI 10.1109/CVPR42600.2020.01059
   Dai B, 2017, ADV NEUR IN, V30
   Denkowski M., 2014, P WMT ACL, P376
   Deshpande A, 2019, PROC CVPR IEEE, P10687, DOI 10.1109/CVPR.2019.01095
   Devlin Jacob, 2015, abs/1505.04467
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Farhadi A, 2010, LECT NOTES COMPUT SC, V6314, P15, DOI 10.1007/978-3-642-15561-1_2
   Fraccaro Marco, 2016, P 30 INT C NEUR INF
   Freitag Markus, 2017, P 1 WORKSHOP NEURAL, P56, DOI [10.18653/v1/W17-3207, DOI 10.18653/V1/W17-3207]
   Goyal Anirudh, 2017, P 31 C NEUR INF PROC, P6697
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Huang L, 2019, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2019.00473
   Jang E., 2017, ICLR (Poster)
   Jia X, 2015, IEEE I CONF COMP VIS, P2407, DOI 10.1109/ICCV.2015.277
   Jingna Mao, 2015, 2015 IEEE Biomedical Circuits and Systems Conference (BioCAS), P1, DOI 10.1109/BioCAS.2015.7348279
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Ke L, 2019, IEEE I CONF COMP VIS, P8887, DOI 10.1109/ICCV.2019.00898
   Krause J, 2017, PROC CVPR IEEE, P3337, DOI 10.1109/CVPR.2017.356
   Kulkarni G, 2013, IEEE T PATTERN ANAL, V35, P2891, DOI 10.1109/TPAMI.2012.162
   Li Dianqi, 2018, P C NEUR INF PROC SY
   Lin C.-Y., 2004, ANN M ASS COMP LING, P74
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu XH, 2018, LECT NOTES COMPUT SC, V11219, P353, DOI 10.1007/978-3-030-01267-0_21
   Mahajan Shweta, 2020, P 8 INT C LEARN REPR
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Ren L, 2019, IEEE WINT CONF APPL, P263, DOI 10.1109/WACV.2019.00034
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ren Z, 2017, PROC CVPR IEEE, P1151, DOI 10.1109/CVPR.2017.128
   Shetty R, 2017, IEEE I CONF COMP VIS, P4155, DOI 10.1109/ICCV.2017.445
   SINHA NK, 1971, IEEE T SYST MAN CYB, VSMC1, P338, DOI 10.1109/TSMC.1971.4308316
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Vered G, 2019, IEEE I CONF COMP VIS, P8897, DOI 10.1109/ICCV.2019.00899
   Vijayakumar AK, 2018, AAAI CONF ARTIF INTE, P7371
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang JN, 2023, IEEE T PATTERN ANAL, V45, P2088, DOI 10.1109/TPAMI.2022.3159811
   Wang Liwei, 2017, ADV NEURAL INFORM PR, P5756
   Yang ZL, 2016, ADV NEUR IN, V29
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yingwei Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10968, DOI 10.1109/CVPR42600.2020.01098
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
NR 47
TC 1
Z9 1
U1 8
U2 20
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2024
VL 20
IS 1
AR 29
DI 10.1145/3614435
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T8LL3
UT WOS:001080441800029
DA 2024-08-05
ER

PT J
AU Gao, L
   Guo, Z
   Guan, L
AF Gao, Lei
   Guo, Zheng
   Guan, Ling
TI An Optimal Edge-weighted Graph Semantic Correlation Framework for
   Multi-view Feature Representation Learning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Multi-view feature representation; graph model; semantic correlation;
   data visualization; face recognition; emotion recognition; text-image
   recognition; object recognition
ID CANONICAL CORRELATION-ANALYSIS; PRESERVING PROJECTIONS; MODEL; SPARSE;
   CLASSIFICATION; RETRIEVAL
AB In this article, we present an optimal edge-weighted graph semantic correlation (EWGSC) framework for multi-view feature representation learning. Different from most existing multi-view representation methods, local structural information and global correlation in multi-view feature spaces are exploited jointly in the EWGSC framework, leading to a new and high-qualitymulti-view feature representation. Specifically, a novel edge-weighted graph model is first conceptualized and developed to preserve local structural information in each of the multi-view feature spaces. Then, the explored structural information is integrated with a semantic correlation algorithm, labeled multiple canonical correlation analysis (LMCCA), to form a powerful platform for effectively exploiting local and global relations across multi-view feature spaces jointly. We then theoretically verified the relation between the upper limit on the number of projected dimensions and the optimal solution to the multi-view feature representation problem. To validate the effectiveness and generality of the proposed framework, we conducted experiments on five datasets of different scales, including visual-based (University of California Irvine (UCI) iris database, Olivetti Research Lab (ORL) face database, and Caltech 256 database), text-image-based (Wiki database), and video-based (Ryerson Multimedia Lab (RML) audio-visual emotion database) examples. The experimental results show the superiority of the proposed framework on multi-view feature representation over state-of-the-art algorithms.
C1 [Gao, Lei; Guo, Zheng; Guan, Ling] Toronto Metropolitan Univ, 350 Victoria St, Toronto, ON M5B 2K3, Canada.
C3 Toronto Metropolitan University
RP Gao, L (corresponding author), Toronto Metropolitan Univ, 350 Victoria St, Toronto, ON M5B 2K3, Canada.
EM iegaolei@gmail.com; zjguo@ryerson.ca; lguan2007@hotmail.com
CR Begum N, 2022, MULTIMED TOOLS APPL, V81, P18521, DOI 10.1007/s11042-022-12238-y
   Bhaskar S, 2023, MULTIMED TOOLS APPL, V82, P5455, DOI 10.1007/s11042-022-12796-1
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Cao GQ, 2017, IEEE SIGNAL PROC LET, V24, P1537, DOI 10.1109/LSP.2017.2748392
   Cen Chen, 2022, IEEE Transactions on Circuits and Systems for Video Technology, V32, P240, DOI 10.1109/TCSVT.2021.3058098
   Chan TH, 2015, IEEE T IMAGE PROCESS, V24, P5017, DOI 10.1109/TIP.2015.2475625
   Chen J, 2019, IEEE T SIGNAL PROCES, V67, P2826, DOI 10.1109/TSP.2019.2910475
   Déniz O, 2011, PATTERN RECOGN LETT, V32, P1598, DOI 10.1016/j.patrec.2011.01.004
   Feng XX, 2020, NEUROCOMPUTING, V402, P283, DOI 10.1016/j.neucom.2020.03.062
   Feng yang, 2021, Journal of Electronic Science and Technology, P1, DOI 10.1016/j.jnlest.2021.100096
   Gao L, 2023, ACM T INTEL SYST TEC, V14, DOI 10.1145/3587253
   Gao L, 2022, IEEE T MULTIMEDIA, V24, P1503, DOI 10.1109/TMM.2021.3066118
   Gao L, 2021, J VIS COMMUN IMAGE R, V81, DOI 10.1016/j.jvcir.2021.103366
   Gao L, 2019, IEEE T MULTIMEDIA, V21, P375, DOI 10.1109/TMM.2018.2859590
   Gao L, 2018, IEEE T IMAGE PROCESS, V27, P1951, DOI 10.1109/TIP.2017.2765820
   Gao L, 2015, IEEE IMAGE PROC, P2710, DOI 10.1109/ICIP.2015.7351295
   Ge WF, 2017, PROC CVPR IEEE, P10, DOI 10.1109/CVPR.2017.9
   Ghalyan IF, 2023, PATTERN RECOGN, V139, DOI 10.1016/j.patcog.2023.109482
   Ghalyan IFJ, 2020, PATTERN RECOGN, V99, DOI 10.1016/j.patcog.2019.107094
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He R, 2015, IEEE T IMAGE PROCESS, V24, P5543, DOI 10.1109/TIP.2015.2466106
   He X., 2003, Adv. Neural Inf. Process. Syst, V16
   Hu HF, 2014, IEEE T CIRC SYST VID, V24, P617, DOI 10.1109/TCSVT.2013.2280098
   Kansizoglou I, 2022, IEEE T AFFECT COMPUT, V13, P756, DOI 10.1109/TAFFC.2019.2961089
   Khanh-Duy Nguyen, 2013, Neural Information Processing. 20th International Conference, ICONIP 2013. Proceedings: LNCS 8228, P433, DOI 10.1007/978-3-642-42051-1_54
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lan RS, 2017, IEEE INT CON MULTI, P1392, DOI 10.1109/ICME.2017.8019308
   Lestariningati SI, 2022, ELECTRONICS-SWITZ, V11, DOI 10.3390/electronics11172723
   Li D, 2023, INFORM FUSION, V91, P612, DOI 10.1016/j.inffus.2022.11.006
   Li XJ, 2022, ACM T KNOWL DISCOV D, V16, DOI 10.1145/3473912
   Li XH, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107049
   Li YO, 2009, IEEE T SIGNAL PROCES, V57, P3918, DOI 10.1109/TSP.2009.2021636
   Li YM, 2019, IEEE T KNOWL DATA EN, V31, P1863, DOI 10.1109/TKDE.2018.2872063
   Lin YJ, 2023, IEEE T PATTERN ANAL, V45, P4447, DOI 10.1109/TPAMI.2022.3197238
   Liong VE, 2017, IEEE T MULTIMEDIA, V19, P1234, DOI 10.1109/TMM.2016.2646180
   Liu BY, 2021, AAAI CONF ARTIF INTE, V35, P8627
   Liu CJ, 2002, IEEE T IMAGE PROCESS, V11, P467, DOI 10.1109/TIP.2002.999679
   Liu JH, 2021, IEEE T CIRC SYST VID, V31, P3242, DOI 10.1109/TCSVT.2020.3037661
   Liu QF, 2017, IEEE T NEUR NET LEAR, V28, P2010, DOI 10.1109/TNNLS.2016.2572204
   Luo W, 2018, IEEE T NEUR NET LEAR, V29, P3289, DOI 10.1109/TNNLS.2017.2712793
   Ma YX, 2019, INFORM FUSION, V46, P184, DOI 10.1016/j.inffus.2018.06.003
   Mahmood A, 2017, IEEE IMAGE PROC, P1597, DOI 10.1109/ICIP.2017.8296551
   Mahmood A, 2020, IMAGE VISION COMPUT, V93, DOI 10.1016/j.imavis.2019.09.002
   Manjunath BS, 1996, IEEE T PATTERN ANAL, V18, P837, DOI 10.1109/34.531803
   Melzer T, 2003, PATTERN RECOGN, V36, P1961, DOI 10.1016/S0031-3203(03)00058-X
   Meng M, 2020, IEEE T IMAGE PROCESS, V29, P186, DOI 10.1109/TIP.2019.2926774
   Pan H, 2018, J VIS COMMUN IMAGE R, V57, P12, DOI 10.1016/j.jvcir.2018.10.009
   Peng YX, 2018, IEEE T CIRC SYST VID, V28, P2372, DOI 10.1109/TCSVT.2017.2705068
   Rejeesh MR, 2019, MULTIMED TOOLS APPL, V78, P22691, DOI 10.1007/s11042-019-7577-5
   Relan D, 2019, MULTIMED TOOLS APPL, V78, P12783, DOI 10.1007/s11042-018-6474-7
   Rupnik J., 2010, C DAT MIN DAT WAR SI, P1
   Sapatinas T, 2005, J R STAT SOC A STAT, V168, P635, DOI 10.1111/j.1467-985X.2005.00368_10.x
   Seng KP, 2018, IEEE T AFFECT COMPUT, V9, P3, DOI 10.1109/TAFFC.2016.2588488
   Sharma A, 2011, PROC CVPR IEEE, P593, DOI 10.1109/CVPR.2011.5995350
   Singha A, 2020, MULTIMED TOOLS APPL, V79, P35069, DOI 10.1007/s11042-020-08892-9
   Slimi A, 2022, PROCEEDINGS OF THE 2022 INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, ICMR 2022, P435, DOI 10.1145/3512527.3531406
   Sun TK, 2007, INT C WAVEL ANAL PAT, P1283
   Tang H, 2021, IEEE T NEUR NET LEAR, V32, P2129, DOI 10.1109/TNNLS.2020.2997289
   Tie Y, 2013, IEEE T CIRC SYST VID, V23, P142, DOI 10.1109/TCSVT.2012.2203210
   Wan MH, 2019, NEURAL PROCESS LETT, V49, P951, DOI 10.1007/s11063-018-9840-6
   Wang C, 2016, MULTIMED TOOLS APPL, V75, P9255, DOI 10.1007/s11042-016-3380-8
   Wang R, 2023, PROC CVPR IEEE, P11590, DOI 10.1109/CVPR52729.2023.01115
   Wang ZM, 2023, IEEE T CIRC SYST VID, V33, P3899, DOI 10.1109/TCSVT.2023.3234993
   Wang Z, 2020, NEUROCOMPUTING, V388, P324, DOI 10.1016/j.neucom.2020.01.017
   Wei XQ, 2022, MULTIMED TOOLS APPL, V81, P44247, DOI 10.1007/s11042-022-13203-5
   Wen J, 2019, IEEE T CIRC SYST VID, V29, P390, DOI 10.1109/TCSVT.2018.2799214
   Xiong HY, 2022, ACM T KNOWL DISCOV D, V16, DOI 10.1145/3530836
   Xu J, 2019, PATTERN RECOGN, V88, P679, DOI 10.1016/j.patcog.2018.12.023
   Xu MX, 2020, IEEE T CYBERNETICS, V50, P4772, DOI 10.1109/TCYB.2019.2904753
   Xu Y, 2017, IEEE T NEUR NET LEAR, V28, P2233, DOI 10.1109/TNNLS.2016.2580572
   Yan XQ, 2019, IEEE ACCESS, V7, P36045, DOI 10.1109/ACCESS.2019.2904554
   Yang B, 2013, NEUROCOMPUTING, V120, P365, DOI 10.1016/j.neucom.2012.10.032
   Yang XJ, 2018, MULTIMED TOOLS APPL, V77, P3071, DOI 10.1007/s11042-017-5022-1
   Yang YM, 2019, IEEE T NEUR NET LEAR, V30, P3313, DOI 10.1109/TNNLS.2018.2890787
   You XG, 2019, PATTERN RECOGN, V92, P37, DOI 10.1016/j.patcog.2019.03.008
   Zhang CJ, 2018, IEEE T MULTIMEDIA, V20, P903, DOI 10.1109/TMM.2017.2759500
   Zhang CJ, 2018, IEEE T NEUR NET LEAR, V29, P3442, DOI 10.1109/TNNLS.2017.2728060
   Zhang DQ, 2014, AAAI CONF ARTIF INTE, P2177
   Zhang SQ, 2018, IEEE T MULTIMEDIA, V20, P1576, DOI 10.1109/TMM.2017.2766843
   Zhang WD, 2021, IEEE T IND INFORM, V17, P1562, DOI 10.1109/TII.2020.2983749
   Zhang WD, 2020, NEUROCOMPUTING, V414, P57, DOI 10.1016/j.neucom.2020.07.018
   Zhang YJ, 2022, APPL INTELL, V52, P3766, DOI 10.1007/s10489-021-02557-2
   Zhao J, 2017, INFORM FUSION, V38, P43, DOI 10.1016/j.inffus.2017.02.007
   Zheng CY, 2019, PATTERN RECOGN LETT, V117, P30, DOI 10.1016/j.patrec.2018.11.005
   Zheng QH, 2023, INFORM FUSION, V89, P198, DOI 10.1016/j.inffus.2022.08.014
   Zhong Y., 2020, P IEEE CVF C COMP VI, P13637
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
   Zhou J, 2020, AI OPEN, V1, P57, DOI 10.1016/j.aiopen.2021.01.001
   Zhu WW, 2020, IEEE T CIRC SYST VID, V30, P3740, DOI 10.1109/TCSVT.2019.2940647
NR 89
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2024
VL 20
IS 7
SI SI
AR 200
DI 10.1145/3649466
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL0Q6
UT WOS:001234494100015
DA 2024-08-05
ER

PT J
AU Garcia, R
   Cediel, A
   Teixido, M
   Gil, R
AF Garcia, Roberto
   Cediel, Ana
   Teixido, Merce
   Gil, Rosa
TI Semantics and Non-fungible Tokens for Copyright Management on the
   Metaverse and Beyond
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Metaverse; Non-Fungible Token; copyright; social media; blockchain;
   ontology
AB Recent initiatives related to the Metaverse focus on better visualization, like augmented or virtual reality, but also persistent digital objects. To guarantee real ownership of these digital objects, open systems based on public blockchains and Non-Fungible Tokens (NFTs) are emerging together with a nascent decentralized and open creator economy. To manage this emerging economy in a more organized way, and fight the so common NFT plagiarism, we propose CopyrightLY, a decentralized application for authorship and copyright management. It provides means to claim content authorship, including supporting evidence. Content and metadata are stored in decentralized storage and registered on the blockchain. A token is used to curate these claims, and potential complaints, by staking it on them. Staking is incentivized by the fact that the token is minted using a bonding curve. The tokenomics include the resolution of complaints and enabling the monetization of curated claims. Monetization is achieved through licensing NFTs with metadata enhanced by semantic technologies. Semantic data makes explicit the reuse conditions transferred with the token while keeping the connection to the underlying copyright claims to improve the trustability of the NFTs. Moreover, the semantic metadata is flexible enough to enable licensing not just in the real world. Licenses can refer to reuses in specific locations in a metaverse, thus facilitating the emergence of creative economies in them.
C1 [Garcia, Roberto; Teixido, Merce; Gil, Rosa] Univ Lleida, Comp Sci & Digital Design Dept, Jaume II,69, Lleida 25001, Spain.
   [Cediel, Ana] Univ Lleida, Dept Publ Law, Jaume II,73, Lleida 25001, Spain.
C3 Universitat de Lleida; Universitat de Lleida
RP Garcia, R (corresponding author), Univ Lleida, Comp Sci & Digital Design Dept, Jaume II,69, Lleida 25001, Spain.
EM roberto.garcia@udl.cat; anna.cs@udl.cat; merce.teixido@udl.cat;
   rosamaria.gil@udl.cat
RI Cediel, Ana/ABE-9711-2021; Gil, Rosa M./I-5452-2012; Garcia,
   Roberto/B-3388-2008
OI Cediel, Ana/0000-0002-8495-2505; Gil, Rosa M./0000-0001-6304-9635;
   Teixido Cairol, Merce/0000-0001-9124-1464; Garcia,
   Roberto/0000-0003-2207-9605
FU Project CopyrightLY -ONTOCHAIN, which has received funding from the
   European Union [957338, PID2020-117912RB-C22]; MCIN/AEI
FX This work was partially supported by project CopyrightLY -ONTOCHAIN,
   which has received funding from the European Union's Horizon 2020
   research and innovation programme under grant agreement 957338, and
   project "ANGRU: Applying kNowledge Graphs to research data ReUsability"
   with reference PID2020-117912RB-C22 and funded by
   MCIN/AEI/10.13039/501100011033.
CR Antonopoulos A.M., 2018, Mastering Ethereum: Building Smart Contracts and Dapps
   Aouidef Y, 2021, FRONT BLOCKCHAIN, V4, DOI 10.3389/fbloc.2021.564551
   Benet J., 2014, arXiv
   Brickley Dan, 2014, Announcing Schema.org Actions
   Cai W, 2018, IEEE ACCESS, V6, P53019, DOI 10.1109/ACCESS.2018.2870644
   Cetinic E, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3475799
   Clark Mitchell, 2021, NFTs, explained
   Dobre RA, 2020, INT SYM DES TECH ELE, P206, DOI [10.1109/SIITME50350.2020.9292296, 10.1109/siitme50350.2020.9292296]
   Dobrovnik M, 2018, LOGISTICS-BASEL, V2, DOI 10.3390/logistics2030018
   Duan HH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P153, DOI 10.1145/3474085.3479238
   DuCharme B., 2013, Learning SPARQL: Querying and Updating with SPARQL 1.1
   García R, 2021, LECT NOTES COMPUT SC, V13072, P199, DOI 10.1007/978-3-030-92916-9_18
   García R, 2010, INFORM SYST, V35, P483, DOI 10.1016/j.is.2008.12.001
   Guha RV, 2016, COMMUN ACM, V59, P44, DOI 10.1145/2844544
   Hartig O, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P1155, DOI 10.1145/3178876.3186014
   Hertzog Eyal, 2018, Bancor protocol. Continuous Liquidity for Cryptographic To-kens through their Smart Contracts
   Hitzler P., 2012, OWL 2 Web Ontology Language Primer, Vsecond
   Kaur Jaspreet, 2021, P EM TECHN DAT MIN I, V1286, P189, DOI [DOI 10.1007/978-981-15-9927-9_19, 10.1007/978-981-15-9927-, DOI 10.1007/978-981-15-9927]
   Kochovski P, 2019, FUTURE GENER COMP SY, V101, P747, DOI 10.1016/j.future.2019.07.030
   Kripa M., 2021, P INF COMM TECHN INT, V195, P451, DOI [DOI 10.1007/978-981-15-7078-0_43, 10.1007/978-981- 15-7078-0_43]
   Lessig, 1999, CODE OTHER LAWS CYBE
   Mengelkamp E, 2018, COMPUT SCI-RES DEV, V33, P207, DOI 10.1007/s00450-017-0360-9
   Mezei Peter, 2021, The Rise of Non-Fungible Tokens (NFTs) and the Role of Copyright Law-Part II
   Michalko Matej, 2020, How TikTok Used Blockchain to Defeat Copyright Infringement
   Natgunanathan I, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3492803
   Pfeffer Johannes, 2018, Technical Report 0.2
   Preukschat A., 2021, Self-sovereign identity
   Schmalfeld Jonathan, 2021, How copyright violations can crash your NFT party
   Shadbolt N, 2006, IEEE INTELL SYST, V21, P96, DOI 10.1109/MIS.2006.62
   Taleb NN, 2014, REV BEHAV ECON, V1, P115, DOI 10.1561/105.00000006
   Walport M., 2016, Report 19
   Wang YL, 2019, 2019 IEEE INTERNATIONAL CONFERENCE ON BLOCKCHAIN AND CRYPTOCURRENCY (ICBC), P188, DOI [10.1109/BLOC.2019.8751443, 10.1109/bloc.2019.8751443]
   World Intellectual Property Organization (WIPO), 2004, WIPO intellectual property handbook, V2nd
   Zargham M, 2020, 2020 IEEE INTERNATIONAL CONFERENCE ON BLOCKCHAIN AND CRYPTOCURRENCY (IEEE ICBC), DOI 10.1109/icbc48266.2020.9169474
   Zhou ZL, 2021, FUTURE GENER COMP SY, V124, P155, DOI 10.1016/j.future.2021.05.035
NR 35
TC 1
Z9 1
U1 1
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2024
VL 20
IS 7
SI SI
AR 186
DI 10.1145/3585387
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL0Q6
UT WOS:001234494100001
OA Bronze, Green Submitted
DA 2024-08-05
ER

PT J
AU Liang, XP
   Liu, WT
   Zhang, XQ
   Tang, ZJ
AF Liang, Xiaoping
   Liu, Wanting
   Zhang, Xianquan
   Tang, Zhenjun
TI Robust Image Hashing via CP Decomposition and DCT for Copy Detection
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE CP decomposition; tensor construction; image hashing; copy detection;
   dimensionality reduction
ID TRANSFORM; PATTERN; SCHEME
AB Copy detection is a key task of image copyright protection. This article proposes a robust image hashing algorithm by CP decomposition and discrete cosine transform (DCT) for copy detection. The first contribution is the third-order tensor construction with low-frequency coefficients in the DCT domain. Since the low-frequency DCT coefficients contain most of the image energy, they can reflect the basic visual content of the image and are less disturbed by noise. Hence, the third-order tensor construction with the low-frequency DCT coefficients can ensure robustness of our algorithm. Another contribution is the application of the CP decomposition to the third-order tensor for learning a short binary hash. As the factor matrices learned from the CP decomposition can preserve the topology of the original tensor, the binary hash derived from the factor matrices can reach good discrimination. Lots of experiments and comparisons are done to validate effectiveness and advantage of our algorithm. The results demonstrate that our algorithm has superior classification and copy detection performances than several baseline algorithms. In addition, our algorithm is also better than some baseline algorithms with regard to hash length and computational time.
C1 [Liang, Xiaoping; Liu, Wanting; Zhang, Xianquan; Tang, Zhenjun] Guangxi Normal Univ, Key Lab Educ Blockchain & Intelligent Technol, Minist Educ, Guilin 541004, Peoples R China.
   [Liang, Xiaoping; Liu, Wanting; Zhang, Xianquan; Tang, Zhenjun] Guangxi Normal Univ, Guangxi Key Lab Multi Source Informat Min & Secur, Guilin, Peoples R China.
C3 Guangxi Normal University; Guangxi Normal University
RP Tang, ZJ (corresponding author), Guangxi Normal Univ, Key Lab Educ Blockchain & Intelligent Technol, Minist Educ, Guilin 541004, Peoples R China.; Tang, ZJ (corresponding author), Guangxi Normal Univ, Guangxi Key Lab Multi Source Informat Min & Secur, Guilin, Peoples R China.
EM xpliang6@163.com; wtliu218@163.com; zxq6622@163.com; tangzj230@163.com
OI Tang, Zhenjun/0000-0003-3664-1363; Zhang, Xianquan/0000-0003-3359-117X
FU National Natural Science Foundation of China [62272111, 62302108,
   62062013, 62162006, 61962008]; Guangxi Natural Science Foundation
   [2022GXNSFAA035506]; Guangxi "Bagui Scholar" Team for Innovation and
   Research, Guangxi Talent Highland Project of Big Data Intelligence and
   Application; Guangxi Collaborative Innovation Center of Multi-source
   Information Integration and Intelligent Processing
FX This work is partially supported by the National Natural Science
   Foundation of China (62272111, 62302108, 62062013, 62162006, 61962008),
   the Guangxi Natural Science Foundation (2022GXNSFAA035506), Guangxi
   "Bagui Scholar" Team for Innovation and Research, Guangxi Talent
   Highland Project of Big Data Intelligence and Application, and Guangxi
   Collaborative Innovation Center of Multi-source Information Integration
   and Intelligent Processing.
CR CARROLL JD, 1970, PSYCHOMETRIKA, V35, P283, DOI 10.1007/BF02310791
   Chen HZ, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3572777
   Everingham M., 2012, PASCAL VISUAL OBJECT
   Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010
   Fridrich J., 2000, Proceedings International Conference on Information Technology: Coding and Computing (Cat. No.PR00540), P178, DOI 10.1109/ITCC.2000.844203
   Hitchcock F.L., 1927, Journal of Mathematics and Physics, V7, P39, DOI DOI 10.1002/SAPM19287139
   Hitchcock F.L., 1927, Journal of Mathematics and Physics, V6, DOI [DOI 10.1002/SAPM192761164, 10.1002/sapm192761164]
   Hosny KM, 2018, CIRC SYST SIGNAL PR, V37, P5441, DOI 10.1007/s00034-018-0822-8
   Huang X, 2016, IEEE TRUST BIG, P14, DOI [10.1109/TrustCom.2016.39, 10.1109/TrustCom.2016.0040]
   Huang ZQ, 2023, IEEE T DEPEND SECURE, V20, P463, DOI 10.1109/TDSC.2021.3136163
   Huang ZQ, 2021, IEEE T CIRC SYST VID, V31, P2808, DOI 10.1109/TCSVT.2020.3027001
   Huang ZQ, 2021, IEEE T MULTIMEDIA, V23, P1516, DOI 10.1109/TMM.2020.2999188
   Huang ZQ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1389, DOI 10.1145/3240508.3240690
   KROONENBERG PM, 1980, PSYCHOMETRIKA, V45, P69, DOI 10.1007/BF02293599
   Laradji IH, 2013, IEEE IMAGE PROC, P4402, DOI 10.1109/ICIP.2013.6738907
   Li YN, 2012, IEEE T IMAGE PROCESS, V21, P1963, DOI 10.1109/TIP.2011.2171698
   Liang XP, 2024, ACM T MULTIM COMPUT, V20, DOI 10.1145/3600234
   Liang XP, 2022, IET IMAGE PROCESS, V16, P3225, DOI 10.1049/ipr2.12555
   Liang XP, 2023, IEEE T MULTIMEDIA, V25, P1085, DOI 10.1109/TMM.2021.3139217
   Liang XP, 2023, IEEE T KNOWL DATA EN, V35, P3765, DOI 10.1109/TKDE.2021.3131188
   Liang XP, 2021, MULTIMEDIA SYST, V27, P389, DOI 10.1007/s00530-020-00696-z
   Liang Xiaoping, 2024, IEEE Transactions on Dependable and Secure Computing (2024), P1
   Liu SG, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3355394
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Nion D, 2008, SIGNAL PROCESS, V88, P749, DOI 10.1016/j.sigpro.2007.07.024
   Ouyang JL, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2978572
   Ouyang JL, 2015, DIGIT SIGNAL PROCESS, V41, P98, DOI 10.1016/j.dsp.2015.03.006
   Petitcolas FAP, 2000, IEEE SIGNAL PROC MAG, V17, P58, DOI 10.1109/79.879339
   Qin C, 2021, IEEE T CIRC SYST VID, V31, P4523, DOI 10.1109/TCSVT.2020.3047142
   Qin C, 2019, IEEE ACCESS, V7, P45460, DOI 10.1109/ACCESS.2019.2908029
   Qin C, 2018, INFORM SCIENCES, V423, P284, DOI 10.1016/j.ins.2017.09.060
   Qin C, 2018, SIGNAL PROCESS, V142, P194, DOI 10.1016/j.sigpro.2017.07.019
   Qin C, 2016, DISPLAYS, V45, P26, DOI 10.1016/j.displa.2016.09.003
   Shen Q, 2020, SIGNAL PROCESS, V166, DOI 10.1016/j.sigpro.2019.107244
   Tang ZJ, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3597434
   Tang ZJ, 2021, COMPUT J, V64, P1656, DOI 10.1093/comjnl/bxz127
   Tang ZJ, 2019, IEEE T KNOWL DATA EN, V31, P549, DOI 10.1109/TKDE.2018.2837745
   Tang ZJ, 2018, COMPUT J, V61, P1695, DOI 10.1093/comjnl/bxy047
   Tang ZJ, 2018, NEUROCOMPUTING, V308, P147, DOI 10.1016/j.neucom.2018.04.057
   Tang ZJ, 2017, SIGNAL PROCESS, V137, P240, DOI 10.1016/j.sigpro.2017.02.008
   Tang ZJ, 2016, IEEE T INF FOREN SEC, V11, P200, DOI 10.1109/TIFS.2015.2485163
   Tang ZJ, 2014, OPTIK, V125, P5102, DOI 10.1016/j.ijleo.2014.05.015
   Veganzones MA, 2016, IEEE T GEOSCI REMOTE, V54, P2577, DOI 10.1109/TGRS.2015.2503737
   Venkatesan R, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P664, DOI 10.1109/ICIP.2000.899541
   Wang JZ, 2001, IEEE T PATTERN ANAL, V23, P947, DOI 10.1109/34.955109
   Wang XF, 2015, IEEE T INF FOREN SEC, V10, P1336, DOI 10.1109/TIFS.2015.2407698
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xu Y, 2020, IEEE T GEOSCI REMOTE, V58, P348, DOI 10.1109/TGRS.2019.2936486
   Yan CP, 2016, IEEE T INF FOREN SEC, V11, P2664, DOI 10.1109/TIFS.2016.2594136
   Ye ZD, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3356338
   Zdunek R, 2019, SIGNAL PROCESS-IMAGE, V73, P37, DOI 10.1016/j.image.2018.11.001
   Zhao Y, 2021, SECUR COMMUN NETW, V2021, DOI 10.1155/2021/3803481
   Zhao Y, 2020, IEEE ACCESS, V8, P26041, DOI 10.1109/ACCESS.2020.2970757
   Zhao Y, 2013, IEEE T INF FOREN SEC, V8, P55, DOI 10.1109/TIFS.2012.2223680
NR 54
TC 0
Z9 0
U1 3
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2024
VL 20
IS 7
SI SI
AR 201
DI 10.1145/3650112
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL0Q6
UT WOS:001234494100016
OA hybrid
DA 2024-08-05
ER

PT J
AU Ruiz-Serra, J
   White, J
   Petrie, S
   Kameneva, T
   Mccarthy, C
AF Ruiz-Serra, Jaime
   White, Jack
   Petrie, Stephen
   Kameneva, Tatiana
   Mccarthy, Chris
TI Learning Scene Representations for Human-assistive Displays Using
   Self-attention Networks
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Vision processing; deep reinforcement learning; prosthetic vision;
   display algorithms
ID SIMULATED PROSTHETIC VISION; RECOGNITION
AB Video-see-through (VST) augmented reality (AR) is widely used to present novel augmentative visual experiences by processing video frames for viewers. Among VST AR systems, assistive vision displays aim to compensate for low vision or blindness, presenting enhanced visual information to support activities of daily living for the vision impaired/deprived. Despite progress, current assistive displays suffer from a visual information bottleneck, limiting their functional outcomes compared to healthy vision. This motivates the exploration of methods to selectively enhance and augment salient visual information. Traditionally, vision processing pipelines for assistive displays rely on hand-crafted, single-modality filters, lacking adaptability to time-varying and environment-dependent needs. This article proposes the use of Deep Reinforcement Learning (DRL) and Self-attention (SA) networks as a means to learn vision processing pipelines for assistive displays. SA networks selectively attend to task-relevant features, offering a more parameter-and compute-efficient approach to RL-based task learning. We assess the feasibility of using SA networks in a simulation-trained model to generate relevant representations of real-world states for navigation with prosthetic vision displays. We explore two prosthetic vision applications, vision-to-auditory encoding, and retinal prostheses, using simulated phosphene visualisations. This article introduces SA-px, a general-purpose vision processing pipeline using self-attention networks, and SA-phos, a display-specific formulation targeting low-resolution assistive displays. We present novel scene visualisations derived from SA image patches importance rankings to support mobility with prosthetic vision devices. To the best of our knowledge, this is the first application of self-attention networks to the task of learning vision processing pipelines for prosthetic vision or assistive displays.
C1 [Ruiz-Serra, Jaime; White, Jack; Petrie, Stephen; Kameneva, Tatiana; Mccarthy, Chris] Swinburne Univ Technol, Hawthorn, Vic, Australia.
   [Ruiz-Serra, Jaime] Univ Sydney, Darlington, Durham, England.
   [Kameneva, Tatiana] Univ Melbourne, Parkville, Vic, Australia.
C3 Swinburne University of Technology; University of Sydney; University of
   Melbourne
RP Ruiz-Serra, J (corresponding author), Swinburne Univ Technol, Hawthorn, Vic, Australia.; Ruiz-Serra, J (corresponding author), Univ Sydney, Darlington, Durham, England.
EM jruizserra@swin.edu.au; jack.white@au.ey.com; spetrie@swin.edu.au;
   tkameneva@swin.edu.au; cdmccarthy@swin.edu.au
OI Ruiz-Serra, Jaime/0000-0002-0220-3253; Kameneva,
   Tatiana/0000-0003-0081-8569
FU National Collaborative Research Infrastructure Strategy (NCRIS); SUT
FX OzSTAR is funded by SUT and the National Collaborative Research
   Infrastructure Strategy (NCRIS).
CR Agaian S.S., 2000, IASTED INT C SIGN PR
   Ayton LN, 2020, CLIN NEUROPHYSIOL, V131, P1383, DOI 10.1016/j.clinph.2019.11.029
   Barnes N, 2016, J NEURAL ENG, V13, DOI 10.1088/1741-2560/13/3/036013
   Barnes N, 2013, IEEE IMAGE PROC, P1532, DOI 10.1109/ICIP.2013.6738315
   Barnes NM, 2015, INVEST OPHTH VIS SCI, V56
   Barry MP, 2016, FRONT SYST NEUROSCI, V10, DOI [10.3389/fnsys.2016.00041, 10.3389/hsys.2015.00041]
   Beattie C., 2016, ARXIV161203801
   Bermudez-Cameo J, 2017, LECT NOTES COMPUT SC, V10255, P427, DOI 10.1007/978-3-319-58838-4_47
   Bhattarai Manish, 2020, 2020 19th IEEE International Conference on Machine Learning and Applications (ICMLA), P1224, DOI 10.1109/ICMLA51294.2020.00193
   Bloch E, 2019, THER ADV OPHTHALMOL, V11, DOI 10.1177/2515841418817501
   Brown D, 2011, PERCEPTION, V40, P1120, DOI 10.1068/p6952
   Chaudhari S, 2021, ACM T INTEL SYST TEC, V12, DOI 10.1145/3465055
   Coughlan JM, 2017, ADJUNCT PROCEEDINGS OF THE 2017 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR-ADJUNCT), P288, DOI 10.1109/ISMAR-Adjunct.2017.89
   Crossland MD, 2019, OPHTHAL PHYSL OPT, V39, P422, DOI 10.1111/opo.12646
   Danilov Yuri, 2005, Journal of Integrative Neuroscience, V4, P537, DOI 10.1142/S0219635205000914
   Ehrlich JR, 2017, AM J OPHTHALMOL, V176, P26, DOI 10.1016/j.ajo.2016.12.021
   Feng D, 2016, PROC CVPR IEEE, P2343, DOI 10.1109/CVPR.2016.257
   Gorjestani A., 2003, DRIVER ASSISTIVE SYS
   Hanassy Shlomi, 2013, Multisensory Research, V26, P116, DOI [10.1163/22134808-000s0084, DOI 10.1163/22134808-000S0084]
   Hansen N., 2016, ARXIV160400772, DOI DOI 10.1162/106365603321828970
   Horne L, 2016, COMPUT VIS IMAGE UND, V149, P113, DOI 10.1016/j.cviu.2016.02.015
   Horne L, 2015, IEEE ENG MED BIO, P3379, DOI 10.1109/EMBC.2015.7319117
   Irons JL, 2017, VISION RES, V137, P61, DOI 10.1016/j.visres.2017.06.002
   Jack White, 2023, P ANN INT C IEEE ENG
   Kolic M, 2021, INVEST OPHTH VIS SCI, V62
   Lang CY, 2012, LECT NOTES COMPUT SC, V7573, P101, DOI 10.1007/978-3-642-33709-3_8
   Lee YH, 2016, COMPUT VIS IMAGE UND, V149, P3, DOI 10.1016/j.cviu.2016.03.019
   Leo M, 2017, COMPUT VIS IMAGE UND, V154, P1, DOI 10.1016/j.cviu.2016.09.001
   Li CS, 2021, Arxiv, DOI arXiv:2108.03272
   Li Y, 2012, IEEE ENG MED BIO, P2961, DOI 10.1109/EMBC.2012.6346585
   Lieby P, 2011, IEEE ENG MED BIO, P8017, DOI 10.1109/IEMBS.2011.6091977
   Lou JX, 2022, Arxiv, DOI arXiv:2110.03593
   Luo YHL, 2016, PROG RETIN EYE RES, V50, P89, DOI 10.1016/j.preteyeres.2015.09.003
   Macé MJM, 2015, ARTIF ORGANS, V39, pE102, DOI 10.1111/aor.12476
   McCarthy C, 2015, J NEURAL ENG, V12, DOI 10.1088/1741-2560/12/1/016003
   McCarthy C, 2014, INT SYM MIX AUGMENT, P45, DOI 10.1109/ISMAR.2014.6948408
   McCarthy C, 2012, IEEE ENG MED BIO, P2780, DOI 10.1109/EMBC.2012.6346541
   MEIJER PBL, 1992, IEEE T BIO-MED ENG, V39, P112, DOI 10.1109/10.121642
   Mnih V, 2016, PR MACH LEARN RES, V48
   Moussallem L, 2023, INVEST OPHTH VIS SCI, V64
   Nowik K, 2020, J CLIN NEUROSCI, V78, P8, DOI 10.1016/j.jocn.2020.05.041
   Perez-Yus A, 2017, IEEE INT CONF COMP V, P1516, DOI 10.1109/ICCVW.2017.179
   Ramamonjisoa M, 2019, IEEE INT CONF COMP V, P2109, DOI 10.1109/ICCVW.2019.00266
   Sanchez-Garcia M, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0227677
   Sayed AM, 2020, AM J OPHTHALMOL, V210, P136, DOI 10.1016/j.ajo.2019.10.005
   Schmidhuber J, 1997, NEURAL NETWORKS, V10, P857, DOI 10.1016/S0893-6080(96)00127-X
   Simons DJ, 1999, PERCEPTION, V28, P1059, DOI 10.1068/p2952
   Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655
   Stingl K, 2013, P ROY SOC B-BIOL SCI, V280, DOI 10.1098/rspb.2013.0077
   Stronks HC, 2014, EXPERT REV MED DEVIC, V11, P23, DOI 10.1586/17434440.2014.862494
   Tang YJ, 2020, GECCO'20: PROCEEDINGS OF THE 2020 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P414, DOI 10.1145/3377930.3389847
   van Steveninck JD, 2022, J VISION, V22, DOI 10.1167/jov.22.2.20
   Vaswani A., 2017, Advances in neural information processing systems, P5998
   Verbancsics P, 2013, Arxiv, DOI arXiv:1312.5355
   Vergnieux V, 2017, ARTIF ORGANS, V41, P852, DOI 10.1111/aor.12868
   White J, 2019, IEEE ENG MED BIO, P2809, DOI [10.1109/embc.2019.8856541, 10.1109/EMBC.2019.8856541]
   White J, 2021, INT J OSTEOARCHAEOL, V31, P1030, DOI 10.1002/oa.3016
   Xia P, 2015, ARTIF ORGANS, V39, P1038, DOI 10.1111/aor.12504
NR 58
TC 0
Z9 0
U1 2
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2024
VL 20
IS 7
SI SI
AR 204
DI 10.1145/3650111
PG 26
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL0Q6
UT WOS:001234494100019
DA 2024-08-05
ER

PT J
AU Wang, RM
   Wang, FS
   Su, YM
   Sun, J
   Sun, FM
   Li, HJ
AF Wang, Ruimin
   Wang, Fasheng
   Su, Yiming
   Sun, Jing
   Sun, Fuming
   Li, Haojie
TI Attention-guided Multi-modality Interaction Network for RGB-D Salient
   Object Detection
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Salient object detection; boundary aware; multi-modality; depth
   enhancement
AB The past decade has witnessed great progress in RGB-D salient object detection (SOD). However, there are two bottlenecks that limit its further development. The first one is low-quality depth maps. Most existing methods directly use raw depth maps to perform detection, but low-quality depth images can bring negative impacts to the detection performance. Hence, it is not desirable to utilize depth maps indiscriminately. The other one is how to effectively predict salient maps with clear boundary and complete salient region. To address these problems, an Attention-Guided Multi-Modality Interaction Network (AMINet) is proposed. First, we propose a new quality enhancement strategy for unreliable depth images, named Depth EnhancementModule (DEM). With respect to the second issue, we propose Cross-Modality Attention Module (CMAM) to rapidly locate salient region. The Boundary-Aware Module (BAM) is designed to utilize high-level feature to guide the low-level feature generation in a top-down way to make up for the dilution of the boundary. To further improve the accuracy, we propose Atrous Refined Block (ARB) to adaptively compensate for the shortcoming of atrous convolution. By integrating these interactive modules, features from depth and RGB streams can be refined efficiently, which consequently boosts the detection performance. Experimental results demonstrate the proposed AMINet exceeds state-of-the-art (SOTA) methods on several public RGB-D datasets.
C1 [Wang, Ruimin] Dalian Minzu Univ, Sch Informat & Commun Engn, 18 Liaohexi Rd, Dalian 116600, Peoples R China.
   [Wang, Fasheng; Su, Yiming; Sun, Jing; Sun, Fuming] Dalian Minzu Univ, 18 Liaohexi Rd, Dalian 116600, Peoples R China.
   [Li, Haojie] Dalian Univ Technol, 321 Tuqiang Rd, Dalian, Peoples R China.
C3 Dalian Minzu University; Dalian Minzu University; Dalian University of
   Technology
RP Wang, FS (corresponding author), Dalian Minzu Univ, 18 Liaohexi Rd, Dalian 116600, Peoples R China.
EM wrm970226@163.com; wangfasheng@dlnu.edu.cn; suyiming812@163.com;
   jingsun@dlnu.edu.cn; hjli@dlut.edu.cn
RI Sun, Jing/JSK-0987-2023; Sun, Fuming/HGT-8610-2022
OI Sun, Jing/0000-0003-1389-1562; Sun, Fuming/0000-0003-3932-2712; Su,
   Yiming/0009-0005-2773-9969
FU National Natural Science Foundation of China [61972068, 61976042];
   LiaoNing Revitalization Talents Program [XLYC2007023]; Liaoning
   Baiqianwan Talents Program [2018B09]; Innovative Talents Program for
   Liaoning Universities [LR2019020]
FX This work was supported by National Natural Science Foundation of China
   (Grant Nos. 61972068, 61976042), LiaoNing Revitalization Talents Program
   (Grant No. XLYC2007023), Liaoning Baiqianwan Talents Program (Grant No.
   2018B09) and Innovative Talents Program for Liaoning Universities (Grant
   No. LR2019020).
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Borji A, 2015, IEEE T IMAGE PROCESS, V24, P5706, DOI 10.1109/TIP.2015.2487833
   Borji A, 2019, COMPUT VIS MEDIA, V5, P117, DOI 10.1007/s41095-019-0149-9
   Chen Baian, 2023, ACM Trans. Multim. Comput., Commun. Applic., V20
   Chen CLZ, 2021, IEEE T IMAGE PROCESS, V30, P2350, DOI 10.1109/TIP.2021.3052069
   Chen CLZ, 2020, IEEE T IMAGE PROCESS, V29, P4296, DOI 10.1109/TIP.2020.2968250
   Chen G, 2023, IEEE T CIRC SYST VID, V33, P1787, DOI 10.1109/TCSVT.2022.3215979
   Chen Q, 2021, PATTERN RECOGN, V112, DOI 10.1016/j.patcog.2020.107740
   Cheng MM, 2021, INT J COMPUT VISION, V129, P2622, DOI [10.1007/s11263-021-01490-8, 10.1109/ICCV.2017.487]
   Cheng Y, 2014, IEEE INT CON MULTI
   Deng-Ping Fan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P275, DOI 10.1007/978-3-030-58610-2_17
   Eigen D, 2014, ADV NEUR IN, V27
   Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304
   Fan DP, 2018, Arxiv, DOI arXiv:1805.10421
   Fan DP, 2021, IEEE T NEUR NET LEAR, V32, P2075, DOI 10.1109/TNNLS.2020.2996406
   Fang X, 2023, PATTERN RECOGN, V135, DOI 10.1016/j.patcog.2022.109139
   Fu H, 2018, PROC CVPR IEEE, P2002, DOI 10.1109/CVPR.2018.00214
   Fu KR, 2022, IEEE T PATTERN ANAL, V44, P5541, DOI 10.1109/TPAMI.2021.3073689
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   Gao W, 2022, IEEE T CIRC SYST VID, V32, P2091, DOI 10.1109/TCSVT.2021.3082939
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu XW, 2021, IEEE T CIRC SYST VID, V31, P1079, DOI 10.1109/TCSVT.2020.2995220
   Hui SX, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3570507
   Ji W, 2022, IEEE T IMAGE PROCESS, V31, P2321, DOI 10.1109/TIP.2022.3154931
   Ji W, 2021, PROC CVPR IEEE, P9466, DOI 10.1109/CVPR46437.2021.00935
   Jiang L, 2021, PROC CVPR IEEE, P16504, DOI 10.1109/CVPR46437.2021.01624
   Jin X, 2022, IEEE T CIRC SYST VID, V32, P7632, DOI 10.1109/TCSVT.2022.3180274
   Ju R, 2014, IEEE IMAGE PROC, P1115, DOI 10.1109/ICIP.2014.7025222
   Kingma D. P., 2014, arXiv
   Klein DA, 2011, IEEE I CONF COMP VIS, P2214, DOI 10.1109/ICCV.2011.6126499
   Li GY, 2021, IEEE T IMAGE PROCESS, V30, P3528, DOI 10.1109/TIP.2021.3062689
   Li NY, 2014, PROC CVPR IEEE, P2806, DOI 10.1109/CVPR.2014.359
   Liu HP, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P1270, DOI 10.1145/3503161.3548265
   Liu JJ, 2023, IEEE T PATTERN ANAL, V45, P887, DOI 10.1109/TPAMI.2021.3140168
   Liu N, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4702, DOI 10.1109/ICCV48922.2021.00468
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu ZY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4481, DOI 10.1145/3474085.3475601
   Liu ZY, 2022, IEEE T CIRC SYST VID, V32, P4486, DOI 10.1109/TCSVT.2021.3127149
   Margolin R, 2014, PROC CVPR IEEE, P248, DOI 10.1109/CVPR.2014.39
   Miao Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P374, DOI 10.1007/978-3-030-58604-1_23
   Mousavian A, 2016, INT CONF 3D VISION, P611, DOI 10.1109/3DV.2016.69
   Nian Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13753, DOI 10.1109/CVPR42600.2020.01377
   Niu YZ, 2012, PROC CVPR IEEE, P454, DOI 10.1109/CVPR.2012.6247708
   Peng HW, 2014, LECT NOTES COMPUT SC, V8691, P92, DOI 10.1007/978-3-319-10578-9_7
   Pereira, 2001, CONDITIONAL RANDOM F
   Piao YR, 2019, IEEE I CONF COMP VIS, P7253, DOI 10.1109/ICCV.2019.00735
   Qian B, 2023, PROC CVPR IEEE, P7960, DOI 10.1109/CVPR52729.2023.00769
   Qian B, 2022, LECT NOTES COMPUT SC, V13671, P449, DOI 10.1007/978-3-031-20083-0_27
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun P, 2021, PROC CVPR IEEE, P1407, DOI 10.1109/CVPR46437.2021.00146
   Wang FS, 2023, EXPERT SYST APPL, V214, DOI 10.1016/j.eswa.2022.119047
   Wang FY, 2022, IEEE T IMAGE PROCESS, V31, P1285, DOI 10.1109/TIP.2022.3140606
   Wang LS, 2021, IEEE T CIRC SYST VID, V31, P728, DOI 10.1109/TCSVT.2020.2988768
   Wang R, 2019, PROC CVPR IEEE, P5647, DOI 10.1109/CVPR.2019.00570
   Wang TT, 2018, PROC CVPR IEEE, P3127, DOI 10.1109/CVPR.2018.00330
   Wang WG, 2024, IEEE T PATTERN ANAL, V46, P1635, DOI 10.1109/TPAMI.2022.3168530
   Wang WG, 2020, IEEE T PATTERN ANAL, V42, P1913, DOI 10.1109/TPAMI.2019.2905607
   Wang WG, 2021, IEEE T PATTERN ANAL, V43, P220, DOI 10.1109/TPAMI.2019.2924417
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P38, DOI 10.1109/TIP.2017.2754941
   Wang WG, 2019, PROC CVPR IEEE, P1448, DOI 10.1109/CVPR.2019.00154
   Wang WG, 2019, PROC CVPR IEEE, P5961, DOI 10.1109/CVPR.2019.00612
   Wang YB, 2022, COMPUT J, V65, P1846, DOI 10.1093/comjnl/bxab026
   Wang Y, 2022, SCI CHINA INFORM SCI, V65, DOI 10.1007/s11432-021-3383-y
   Wang Y, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3408317
   Wang Yue, 2020, P ASIAN C COMPUTER V, P336
   Wei Ji, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P52, DOI 10.1007/978-3-030-58523-5_4
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu YH, 2022, IEEE T PATTERN ANAL, V44, P10261, DOI 10.1109/TPAMI.2021.3134684
   Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403
   Xiao HC, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3510004
   Xiaoqi Zhao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P646, DOI 10.1007/978-3-030-58542-6_39
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang Y, 2022, IEEE T CIRC SYST VID, V32, P5346, DOI 10.1109/TCSVT.2022.3144852
   Yin W, 2019, IEEE I CONF COMP VIS, P5683, DOI 10.1109/ICCV.2019.00578
   Yongri Piao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9057, DOI 10.1109/CVPR42600.2020.00908
   Youwei Pang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P235, DOI 10.1007/978-3-030-58595-2_15
   Zhang C, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2094, DOI 10.1145/3474085.3475364
   Zhang J., 2021, Advances in Neural Information Processing Systems, V34
   Zhang M, 2023, IEEE T MULTIMEDIA, V25, P5142, DOI 10.1109/TMM.2022.3187856
   Zhang M, 2020, PROC CVPR IEEE, P3469, DOI 10.1109/CVPR42600.2020.00353
   Zhang WB, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P731, DOI 10.1145/3474085.3475240
   Zhang X., 2021, P IEEE CVF C COMP VI, P13354
   Zhao JX, 2019, PROC CVPR IEEE, P3922, DOI 10.1109/CVPR.2019.00405
   Zhao JW, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1745, DOI 10.1145/3394171.3413855
   Zhao X., 2020, PROC 16 EUR C COMPU, P35, DOI 10.1007/ 978-3-030-58536-5_3
   Zhou JY, 2022, LECT NOTES COMPUT SC, V13689, P270, DOI 10.1007/978-3-031-19818-2_16
   Zhou T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4661, DOI 10.1109/ICCV48922.2021.00464
   Zhou T, 2021, COMPUT VIS MEDIA, V7, P37, DOI 10.1007/s41095-020-0199-z
NR 89
TC 3
Z9 3
U1 14
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2024
VL 20
IS 3
AR 68
DI 10.1145/3624747
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6F8
UT WOS:001153381000008
DA 2024-08-05
ER

PT J
AU Xie, YR
   Guan, L
AF Xie, Yurui
   Guan, Ling
TI Sparsity-guided Discriminative Feature Encoding for Robust Keypoint
   Detection
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Keypoint; feature representation; visual dictionary; sparse coding
ID SCALE; REPRESENTATION; ALGORITHM
AB Existing handcrafted keypoint detectors typically focus on designing specific local structures manually while ignoring whether they have enough flexibility to explore diverse visual patterns in an image. Despite the advancement of learning-based approaches in the past few years, most of them still rely on the availability of the outputs of handcrafted detectors as a part of training. In fact, such dependence limits their ability to discover various visual information. Recently, semi-handcrafted methods based on sparse coding have emerged as a promising paradigm to alleviate the above issue. However, the visual relationships between feature points have not been considered in the encoding stage, which may weaken the discriminative capability of feature representations for keypoint recognition. To tackle this problem, we propose a novel sparsity-guided discriminative feature representation (SDFR) method that attempts to explore the intrinsic correlations of keypoint candidates, thus ensuring the validity of characterizing distinctive and diverse structural information. Specifically, we first incorporate an affinity constraint into the feature representation objective, which jointly encodes all the patches in an image while highlighting the similarities and differences between them. Meanwhile, a smoother sparsity regularization with the Frobenius norm is leveraged to further preserve the similarity relationships of patch representations. Due to the differentiable property of this sparsity, SDFR is computationally feasible and effective for representing dense patches. Finally, we treat the SDFR model as multiple optimization sub-problems and introduce an iterative solver. During comprehensive evaluations on five challenging benchmarks, the proposed method achieves favorable performances compared with the state of the art in the literature.
C1 [Xie, Yurui; Guan, Ling] Ryerson Multimedia Res Lab, 350 Victoria St, Toronto, ON M5B 2K3, Canada.
   [Xie, Yurui] Chengdu Univ Informat Technol, Chengdu 610225, Peoples R China.
C3 Chengdu University of Information Technology
RP Xie, YR (corresponding author), Ryerson Multimedia Res Lab, 350 Victoria St, Toronto, ON M5B 2K3, Canada.; Xie, YR (corresponding author), Chengdu Univ Informat Technol, Chengdu 610225, Peoples R China.
EM gloriousxyr@163.com; lguan@ee.ryerson.ca
FU National Natural Science Foundation of China [61806028]; Fund of China
   Scholarship Council [201908510028]
FX This work is supported by the National Natural Science Foundation of
   China (No. 61806028), and the Fund of China Scholarship Council (No.
   201908510028).
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   Alcantarilla PF, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.13
   Alcantarilla PF, 2012, LECT NOTES COMPUT SC, V7577, P214, DOI 10.1007/978-3-642-33783-3_16
   Balntas V, 2017, PROC CVPR IEEE, P3852, DOI 10.1109/CVPR.2017.410
   Barroso-Laguna A, 2019, IEEE I CONF COMP VIS, P5835, DOI 10.1109/ICCV.2019.00593
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Dai HB, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3503464
   Davison AJ, 2007, IEEE T PATTERN ANAL, V29, P1052, DOI 10.1109/TPAMI.2007.1049
   Dusmanu M, 2019, PROC CVPR IEEE, P8084, DOI 10.1109/CVPR.2019.00828
   Förstner W, 2009, IEEE I CONF COMP VIS, P2256, DOI 10.1109/ICCV.2009.5459458
   Gao SH, 2010, PROC CVPR IEEE, P3555, DOI 10.1109/CVPR.2010.5539943
   Gleize P, 2023, Arxiv, DOI arXiv:2304.06194
   Harris C, 1988, A combined corner and edge detector, P147, DOI [10.5244/C.2.23.23.1-23.6, DOI 10.5244/C.2.23]
   Hauagge DC, 2012, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2012.6247677
   Hong Phouc T., 2021, ACM Transactions on Intelligent Systems and Technology, V12, P1
   Huang WX, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3565886
   Jiao YY, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3519305
   Kavukcuoglu K, 2009, PROC CVPR IEEE, P1605, DOI 10.1109/CVPRW.2009.5206545
   Ke Y, 2004, PROC CVPR IEEE, P506
   Lee H., 2007, ADV NEURAL INF PROCE, P801
   Lee J, 2022, PROC CVPR IEEE, P4837, DOI 10.1109/CVPR52688.2022.00480
   Lenc K., 2011, VLBenchmkars
   Lenc K, 2016, LECT NOTES COMPUT SC, V9915, P100, DOI 10.1007/978-3-319-49409-8_11
   Li L, 2022, IEEE T IMAGE PROCESS, V31, P2726, DOI 10.1109/TIP.2022.3158546
   Li L, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3359753
   Li ZM, 2020, IEEE T NEUR NET LEAR, V31, P786, DOI 10.1109/TNNLS.2019.2910146
   Liu XJ, 2023, IEEE T PATTERN ANAL, V45, P3003, DOI 10.1109/TPAMI.2022.3186410
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luo ZX, 2020, PROC CVPR IEEE, P6588, DOI 10.1109/CVPR42600.2020.00662
   Matas J, 2004, IMAGE VISION COMPUT, V22, P761, DOI 10.1016/j.imavis.2004.02.006
   McNally W, 2022, LECT NOTES COMPUT SC, V13666, P37, DOI 10.1007/978-3-031-20068-7_3
   Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Revaud J, 2019, ADV NEUR IN, V32
   Rosten E, 2006, LECT NOTES COMPUT SC, V3951, P430, DOI 10.1007/11744023_34
   Skretting K, 2011, PROC SPIE, V8138, DOI 10.1117/12.892684
   Sun YL, 2020, IEEE T NEUR NET LEAR, V31, P4303, DOI 10.1109/TNNLS.2019.2954545
   Thanh HP, 2020, IEEE T IMAGE PROCESS, V29, P747, DOI 10.1109/TIP.2019.2934891
   Tolstov G.P., 1976, FOURIER SERIES
   Triggs B, 2004, LECT NOTES COMPUT SC, V2034, P100
   Tropp JA, 2010, P IEEE, V98, P948, DOI 10.1109/JPROC.2010.2044010
   Truong P, 2019, IEEE I CONF COMP VIS, P10731, DOI 10.1109/ICCV.2019.01083
   Verdie Y, 2015, PROC CVPR IEEE, P5279, DOI 10.1109/CVPR.2015.7299165
   Wang H, 2023, IEEE T PATTERN ANAL, V45, P7711, DOI 10.1109/TPAMI.2022.3226328
   Xie YR, 2022, INT CONF ACOUST SPEE, P2490, DOI 10.1109/ICASSP43922.2022.9747017
   Xie YR, 2020, IEEE INT SYM MULTIM, P127, DOI 10.1109/ISM.2020.00029
   Xu LW, 2022, PROC CVPR IEEE, P9469, DOI 10.1109/CVPR52688.2022.00926
   Yang CW, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3472623
   Yi KM, 2016, LECT NOTES COMPUT SC, V9910, P467, DOI 10.1007/978-3-319-46466-4_28
   Zhang SB, 2021, PROC CVPR IEEE, P1065, DOI 10.1109/CVPR46437.2021.00112
   Zhang X, 2017, PROC CVPR IEEE, P4923, DOI 10.1109/CVPR.2017.523
   Zhang Z, 2021, IEEE T NEUR NET LEAR, V32, P947, DOI 10.1109/TNNLS.2020.2979748
   Zhang Z, 2018, IEEE T NEUR NET LEAR, V29, P3798, DOI 10.1109/TNNLS.2017.2740224
   Zhao XM, 2023, IEEE T MULTIMEDIA, V25, P3101, DOI 10.1109/TMM.2022.3155927
   Zitnick CL, 2011, IEEE I CONF COMP VIS, P359, DOI 10.1109/ICCV.2011.6126263
NR 56
TC 0
Z9 0
U1 9
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2024
VL 20
IS 3
AR 88
DI 10.1145/3628432
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6F8
UT WOS:001153381000028
DA 2024-08-05
ER

PT J
AU Gou, JP
   Sun, LY
   Yu, BS
   Wan, SH
   Tao, DC
AF Gou, Jianping
   Sun, Liyuan
   Yu, Baosheng
   Wan, Shaohua
   Tao, Dacheng
TI Hierarchical Multi-Attention Transfer for Knowledge Distillation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Model compression; knowledge distillation; hierarchical attention
   transfer
AB Knowledge distillation (KD) is a powerful and widely applicable technique for the compression of deep learning models. The main idea of knowledge distillation is to transfer knowledge from a large teacher model to a small student model, where the attention mechanism has been intensively explored in regard to its great flexibility for managing different teacher-student architectures. However, existing attention-based methods usually transfer similar attention knowledge from the intermediate layers of deep neural networks, leaving the hierarchical structure of deep representation learning poorly investigated for knowledge distillation. In this paper, we propose a hierarchical multi-attention transfer framework (HMAT), where different types of attention are utilized to transfer the knowledge at different levels of deep representation learning for knowledge distillation. Specifically, position-based and channel-based attention knowledge characterize the knowledge from low-level and high-level feature representations, respectively, and activation-based attention knowledge characterize the knowledge from both mid-level and high-level feature representations. Extensive experiments on three popular visual recognition tasks, image classification, image retrieval, and object detection, demonstrate that the proposed hierarchical multi-attention transfer or HMAT significantly outperforms recent state-of-the-art KD methods.
C1 [Gou, Jianping] Southwest Univ, Coll Comp & Informat Sci, Coll Software, Chongqing 400715, Peoples R China.
   [Gou, Jianping; Sun, Liyuan] Jiangsu Univ, Sch Comp Sci & Commun Engn, Zhenjiang 212013, Jiangsu, Peoples R China.
   [Yu, Baosheng; Tao, Dacheng] Univ Sydney, Fac Engn, Sch Comp Sci, Sydney, NSW 2008, Australia.
   [Wan, Shaohua] Univ Elect Sci & Technol China, Shenzhen Inst Adv Study, Shenzhen 518110, Peoples R China.
C3 Southwest University - China; Jiangsu University; University of Sydney;
   University of Electronic Science & Technology of China; Shenzhen
   Institute for Advanced Study, UESTC
RP Gou, JP (corresponding author), Southwest Univ, Coll Comp & Informat Sci, Coll Software, Chongqing 400715, Peoples R China.
EM cherish.gjp@gmail.com; 2211908031@stmail.ujs.edu.cn;
   baosheng.yu@sydney.edu.au; shaohua.wan@ieee.org;
   dacheng.tao@sydney.edu.au
RI Tao, Dacheng/A-5449-2012; Gou, Jianping/JQX-2453-2023; Wan,
   Shaohua/B-9243-2014
OI Tao, Dacheng/0000-0001-7225-5449; Gou, Jianping/0000-0003-1413-0693;
   Wan, Shaohua/0000-0001-7013-9081
FU National Natural Science Foundation of China [61976107]; Qing Lan
   Project of Colleges and Universities of Jiangsu Province in 2020;
   Australian Research Council Project [FL-170100117]
FX Theworkwas in part supported by National Natural Science Foundation of
   China (Grant No. 61976107), and in part by Qing Lan Project of Colleges
   and Universities of Jiangsu Province in 2020. Dr Baosheng Yu is
   partially supported by Australian Research Council Project with ID
   FL-170100117.
CR Adriana Romero, 2015, INT C LEARN REPR
   Ahn S, 2019, PROC CVPR IEEE, P9155, DOI 10.1109/CVPR.2019.00938
   An SM, 2022, IEEE T INTELL TRANSP, V23, P15256, DOI 10.1109/TITS.2021.3139001
   Anbang Yao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P294, DOI 10.1007/978-3-030-58555-6_18
   Bahdanau D, 2016, Arxiv, DOI arXiv:1409.0473
   Chen DF, 2021, AAAI CONF ARTIF INTE, V35, P7028
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Chorowski J, 2014, Arxiv, DOI [arXiv:1412.1602, DOI 10.48550/ARXIV.1412.1602]
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Everingham M., 2011, Tech. Rep, V8
   Everingham M., 2009, The PASCAL Visual Object Classes Challenge 2009 (VOC) Results
   Ge SM, 2020, IEEE T IMAGE PROCESS, V29, P6898, DOI 10.1109/TIP.2020.2995049
   Gou JP, 2021, INT J COMPUT VISION, V129, P1789, DOI 10.1007/s11263-021-01453-z
   Guodong Xu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P588, DOI 10.1007/978-3-030-58545-7_34
   Han S, 2015, ADV NEUR IN, V28
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heo B, 2019, IEEE I CONF COMP VIS, P1921, DOI 10.1109/ICCV.2019.00201
   Hinton G., 2015, NEURIPS DEEP LEARN R, P9
   Hou QB, 2021, PROC CVPR IEEE, P13708, DOI 10.1109/CVPR46437.2021.01350
   Huang Zehao, 2017, P IEEE CVF C COMP VI
   Huang ZH, 2022, INT J INTELL SYST, V37, P5902, DOI 10.1002/int.22819
   Huang ZH, 2022, IEEE T IMAGE PROCESS, V31, P1364, DOI 10.1109/TIP.2022.3141255
   Hubara I, 2016, ADV NEUR IN, V29
   Ji M, 2021, AAAI CONF ARTIF INTE, V35, P7945
   Kim JH, 2018, ADV NEUR IN, V31
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Krizhevsky A, 2009, CIFAR-10 dataset
   Kundu S, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P2225, DOI 10.1109/ICASSP39728.2021.9415117
   Li H, 2017, Arxiv, DOI arXiv:1608.08710
   Li J, 2020, IEEE T IMAGE PROCESS, V29, P1902, DOI 10.1109/TIP.2019.2946102
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Mahendran A, 2016, INT J COMPUT VISION, V120, P233, DOI 10.1007/s11263-016-0911-8
   Mitsuno K, 2021, INT C PATT RECOG, P7573, DOI 10.1109/ICPR48806.2021.9412760
   Peng BY, 2019, IEEE I CONF COMP VIS, P5006, DOI 10.1109/ICCV.2019.00511
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shu Changyong, 2020, arXiv
   Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434
   Tian Y., 2020, Contrastive representation distillation
   Wah C., 2011, The caltech-ucsd birds-200-2011 dataset
   Wang W., 2020, Advances in Neural Information Processing Systems, V33, P5776
   Yan Qu, 2020, Pattern Recognition and Computer Vision. Third Chinese Conference, PRCV 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12307), P249, DOI 10.1007/978-3-030-60636-7_21
   Yang ZD, 2022, PROC CVPR IEEE, P4633, DOI 10.1109/CVPR52688.2022.00460
   Yim J, 2017, PROC CVPR IEEE, P7130, DOI 10.1109/CVPR.2017.754
   You S, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1285, DOI 10.1145/3097983.3098135
   Yushuo Guan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P469, DOI 10.1007/978-3-030-58520-4_28
   Zagoruyko S., 2017, P INT C LEARN REPR, P1
   Zagoruyko S, 2017, Arxiv, DOI arXiv:1605.07146
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang KK, 2022, IEEE T CIRC SYST VID, V32, P2251, DOI 10.1109/TCSVT.2021.3090902
   Zhang Linfeng, 2020, P INT C LEARN REPR
   Zhao BR, 2022, PROC CVPR IEEE, P11943, DOI 10.1109/CVPR52688.2022.01165
   Zhao HR, 2022, IEEE T CYBERNETICS, V52, P2070, DOI 10.1109/TCYB.2020.3007506
NR 55
TC 19
Z9 19
U1 56
U2 73
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2024
VL 20
IS 2
SI SI
AR 51
DI 10.1145/3568679
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W6GR4
UT WOS:001092595800021
HC Y
HP N
DA 2024-08-05
ER

PT J
AU Gao, X
   Hu, W
   Qi, GJ
AF Gao, Xiang
   Hu, Wei
   Qi, Guo-Jun
TI Self-supervised Multi-view Learning via Auto-encoding 3D Transformations
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Self-supervised learning; multi-viewlearning; transformation equivariant
   representation
ID CONVOLUTIONAL NEURAL-NETWORKS
AB 3D object representation learning is a fundamental challenge in computer vision to infer about the 3D world. Recent advances in deep learning have shown their efficiency in 3D object recognition, among which view-based methods have performed best so far. However, feature learning of multiple views in existing methods is mostly performed in a supervised fashion, which often requires a large amount of data labels with high costs. In contrast, self-supervised learning aims to learnmulti-view feature representations without involving labeled data. To this end, we propose a novel self-supervised framework to learn Multi-View Transformation Equivariant Representations (MV-TER), exploring the equivariant transformations of a 3D object and its projected multiple views that we derive. Specifically, we perform a 3D transformation on a 3D object and obtain multiple views before and after the transformation via projection. Then, we train a representation encoding module to capture the intrinsic 3D object representation by decoding 3D transformation parameters from the fused feature representations of multiple views before and after the transformation. Experimental results demonstrate that the proposedMV-TER significantly outperforms the state-of-the-art view-based approaches in 3D object classification and retrieval tasks and show the generalization to real-world datasets. The code is available at https://github.com/gyshgx868/mvter.
C1 [Gao, Xiang; Hu, Wei] Peking Univ, Wangxuan Inst Comp Technol, 128 Zhongguancun North St, Beijing 100080, Peoples R China.
   [Qi, Guo-Jun] OPPO US Res Ctr, Seattle, WA 98006 USA.
C3 Peking University
RP Hu, W (corresponding author), Peking Univ, Wangxuan Inst Comp Technol, 128 Zhongguancun North St, Beijing 100080, Peoples R China.
EM gyshgx868@pku.edu.cn; forhuwei@pku.edu.cn; guojunq@gmail.com
RI Qi, Guo-Jun/AAH-8294-2019
FU National Key R&D Program of China [2021YFF0901502]; National Natural
   Science Foundation of China [61972009]
FX This work is supported in part by the National Key R&D Program of China
   under contract No. 2021YFF0901502, and in part by National Natural
   Science Foundation of China (61972009).
CR Achlioptas P, 2018, PR MACH LEARN RES, V80
   Atzmon M, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201301
   Bachman P, 2019, ADV NEUR IN, V32
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Chen T, 2020, PR MACH LEARN RES, V119
   Chen XL, 2020, Arxiv, DOI arXiv:2003.04297
   Cohen TS, 2016, PR MACH LEARN RES, V48
   Dieleman S, 2016, PR MACH LEARN RES, V48
   Dieleman S, 2015, MON NOT R ASTRON SOC, V450, P1441, DOI 10.1093/mnras/stv632
   Feng YF, 2018, PROC CVPR IEEE, P264, DOI 10.1109/CVPR.2018.00035
   Gadelha M, 2018, LECT NOTES COMPUT SC, V11211, P105, DOI 10.1007/978-3-030-01234-2_7
   Gao X, 2020, PROC CVPR IEEE, P7161, DOI 10.1109/CVPR42600.2020.00719
   Gao Z, 2022, IEEE T CIRC SYST VID, V32, P2264, DOI 10.1109/TCSVT.2021.3091581
   Gao Z, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3377876
   Gens R., 2014, Advances in neural information processing systems, V27, P2537
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Guo YL, 2021, IEEE T PATTERN ANAL, V43, P4338, DOI 10.1109/TPAMI.2020.3005434
   Han ZZ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3862, DOI 10.1145/3474085.3475172
   Han ZZ, 2019, IEEE I CONF COMP VIS, P10441, DOI 10.1109/ICCV.2019.01054
   Han ZZ, 2019, IEEE T IMAGE PROCESS, V28, P3986, DOI 10.1109/TIP.2019.2904460
   Han ZZ, 2019, IEEE T IMAGE PROCESS, V28, P658, DOI 10.1109/TIP.2018.2868426
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He XW, 2018, PROC CVPR IEEE, P1945, DOI 10.1109/CVPR.2018.00208
   Hinton GE, 2011, LECT NOTES COMPUT SC, V6791, P44, DOI 10.1007/978-3-642-21735-7_6
   Hjelm R. D., 2019, ICLR
   Hu QJ, 2021, PROC CVPR IEEE, P1074, DOI 10.1109/CVPR46437.2021.00113
   Jiang JW, 2019, AAAI CONF ARTIF INTE, P8513
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Kanezaki A, 2018, PROC CVPR IEEE, P5010, DOI 10.1109/CVPR.2018.00526
   Kivinen JJ, 2011, LECT NOTES COMPUT SC, V6791, P1, DOI 10.1007/978-3-642-21735-7_1
   Klokov R, 2017, IEEE I CONF COMP VIS, P863, DOI 10.1109/ICCV.2017.99
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lai K, 2011, IEEE INT CONF ROBOT, P1817
   Le T, 2018, PROC CVPR IEEE, P9204, DOI 10.1109/CVPR.2018.00959
   Leibe B, 2003, PROC CVPR IEEE, P409
   Lenc K, 2015, PROC CVPR IEEE, P991, DOI 10.1109/CVPR.2015.7298701
   Lenssen JE, 2018, ADV NEUR IN, V31
   Li WH, 2022, IEEE T CIRC SYST VID, V32, P3265, DOI 10.1109/TCSVT.2021.3099496
   Li YY, 2018, ADV NEUR IN, V31
   Liu SK, 2018, INT CONF 3D VISION, P542, DOI 10.1109/3DV.2018.00068
   Liu X., 2021, IEEE Transactions on Knowledge and Data Engineering
   Liu YC, 2019, PROC CVPR IEEE, P8887, DOI 10.1109/CVPR.2019.00910
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   Poursaeed O, 2020, INT CONF 3D VISION, P1018, DOI 10.1109/3DV50981.2020.00112
   Qi C. R., 2017, POINTNET DEEP HIERAR
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609
   Qi GJ, 2022, IEEE T PATTERN ANAL, V44, P2045, DOI 10.1109/TPAMI.2020.3029801
   Qi GJ, 2019, IEEE I CONF COMP VIS, P8129, DOI 10.1109/ICCV.2019.00822
   Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701
   Schmidt U, 2012, PROC CVPR IEEE, P2050, DOI 10.1109/CVPR.2012.6247909
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Skibbe H., 2013, Ph. D. Dissertation
   Sohn K., 2012, P 29 INT C INT C MAC, P1339
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Sun YB, 2020, IEEE WINT CONF APPL, P61, DOI [10.1109/wacv45572.2020.9093430, 10.1109/WACV45572.2020.9093430]
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Te GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P746, DOI 10.1145/3240508.3240621
   van den Oord A, 2016, PR MACH LEARN RES, V48
   Velickovic P., 2019, P INT C LEARN REPR
   Wang C, 2018, LECT NOTES COMPUT SC, V11208, P56, DOI 10.1007/978-3-030-01225-0_4
   Wang JY, 2020, PROC CVPR IEEE, P469, DOI 10.1109/CVPR42600.2020.00055
   Wang PS, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073608
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wei X, 2020, PROC CVPR IEEE, P1847, DOI 10.1109/CVPR42600.2020.00192
   Wu W., 2019, PROC CVPR IEEE, P9621, DOI DOI 10.1109/CVPR.2019.00985
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Chang AX, 2015, Arxiv, DOI [arXiv:1512.03012, DOI 10.48550/ARXIV.1512.03012]
   Xu Y, 2021, IEEE T IMAGE PROCESS, V30, P5299, DOI 10.1109/TIP.2021.3082310
   Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029
   Yang Z, 2019, IEEE I CONF COMP VIS, P7504, DOI 10.1109/ICCV.2019.00760
   Yu T, 2021, IEEE T IMAGE PROCESS, V30, P2168, DOI 10.1109/TIP.2021.3049968
   Yu T, 2018, PROC CVPR IEEE, P186, DOI 10.1109/CVPR.2018.00027
   Zhang LH, 2019, PROC CVPR IEEE, P2542, DOI 10.1109/CVPR.2019.00265
   Zhang YX, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P6279, DOI 10.1109/ICASSP.2018.8462291
   Zhao YH, 2019, PROC CVPR IEEE, P1009, DOI 10.1109/CVPR.2019.00110
NR 77
TC 1
Z9 1
U1 1
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2024
VL 20
IS 1
AR 19
DI 10.1145/3597613
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T8LL3
UT WOS:001080441800019
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Song, JJ
   Li, Z
   Min, WQ
   Jiang, SQ
AF Song, Jiajun
   Li, Zhuo
   Min, Weiqing
   Jiang, Shuqiang
TI Towards Food Image Retrieval via Generalization-Oriented Sampling and
   Loss Function Design
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Food computing; image retrieval; deep learning
AB Food computing has increasingly received widespread attention in the multimedia field. As a basic task of food computing, food image retrieval has wide applications, that is, food image retrieval can help users to find the desired food from a large number of food images. Besides, the retrieved information can be applied to establish a richer database for the subsequent food content-related recommendation. Food image retrieval aims to achieve better performance on novel categories. Thus, it is worth studying to transfer the embedding ability from the training set to the unseen test set, that is, the generalization of the model. Food is influenced by various factors, such as culture and geography, leading to great differences between domains, such as Asian food and western food. Therefore, it is challenging to study the generalization of the model in food image retrieval. In this article, we improve the classical metric learning framework and propose a generalization-oriented sampling strategy, which boosts the generalization of the model by maximizing the intra-class distance from a proportion of positive pairs to avoid the excessive distance compression in the embedding space. Considering that the existing optimization process is in an opposite direction to our proposed sampling strategy, we further propose an adaptive gradient assignment policy named gradient-adaptive optimization, which can alleviate the intra-class distance compression during optimization by assigning different gradients to different samples. Extensive evaluation on three popular food image datasets demonstrates the effectiveness of the proposed method. We also experiment on three popular general datasets to prove that solving the problem from the generalization can also improve the performance of general image retrieval. Code is available at https://github.com/Jiajun-ISIA/Generalization- oriented- Sampling- and- Loss.
C1 [Song, Jiajun; Li, Zhuo; Min, Weiqing; Jiang, Shuqiang] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, 6 Kexueyuan South Rd, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS
RP Song, JJ (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, 6 Kexueyuan South Rd, Beijing, Peoples R China.
EM jiajun.song@vipl.ict.ac.cn; li@vipl.ict.ac.cn; weiqingmin@ict.ac.cn;
   sqjiang@ict.ac.cn
OI li, zhuo/0009-0001-2550-5887
FU National Nature Science Foundation of China [61972378, 62125207,
   U1936203, U19B2040]; CAAI-Huawei MindSpore Open Fund
FX This work was supported in part by the National Nature Science
   Foundation of China (grant nos. 61972378, 62125207, U1936203, U19B2040)
   and CAAI-Huawei MindSpore Open Fund.
CR Barlacchi G., 2016, P 3 IT C COMP LING C, P46
   Bellet A, 2015, NEUROCOMPUTING, V151, P259, DOI 10.1016/j.neucom.2014.09.044
   Bossard L, 2014, LECT NOTES COMPUT SC, V8694, P446, DOI 10.1007/978-3-319-10599-4_29
   Brown Andrew, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P677, DOI 10.1007/978-3-030-58545-7_39
   Carvalho M, 2018, ACM/SIGIR PROCEEDINGS 2018, P35, DOI 10.1145/3209978.3210036
   Chang M, 2018, PROCEEDINGS OF THE 2018 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2018), DOI 10.1145/3173574.3174025
   Chaudhary C, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3375786
   Chen JJ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1020, DOI 10.1145/3240508.3240627
   Chen JJ, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P32, DOI 10.1145/2964284.2964315
   Ciocca G, 2017, LECT NOTES COMPUT SC, V10590, P426, DOI 10.1007/978-3-319-70742-6_41
   Cui Y, 2016, PROC CVPR IEEE, P1153, DOI 10.1109/CVPR.2016.130
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dosovitskiy A., 2021, PROC INT C LEARNING, DOI DOI 10.48550/ARXIV.2010.11929
   Farinella GM, 2016, COMPUT BIOL MED, V77, P23, DOI 10.1016/j.compbiomed.2016.07.006
   Gordo A, 2016, LECT NOTES COMPUT SC, V9910, P241, DOI 10.1007/978-3-319-46466-4_15
   Gu G, 2021, AAAI CONF ARTIF INTE, V35, P1460
   Hadsell R., 2006, Computer vision and pattern recognition, 2006 IEEE computer society conference on, V2, P1735, DOI DOI 10.1109/CVPR.2006.100
   Haoran Xie, 2010, Proceedings 2010 IEEE International Symposium on Multimedia (ISM 2010), P254, DOI 10.1109/ISM.2010.44
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Horiguchi S, 2018, IEEE T MULTIMEDIA, V20, P2836, DOI 10.1109/TMM.2018.2814339
   Hu JL, 2014, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2014.242
   Ji X, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1654, DOI 10.1145/3123266.3123429
   Jiang SQ, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3391624
   Kingma D., 2014, P INT C LEARN REPR, P1
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Li YY, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3412384
   Li Z, 2022, Arxiv, DOI arXiv:2102.04640
   Milbich T, 2022, IEEE T PATTERN ANAL, V44, P416, DOI 10.1109/TPAMI.2020.3009620
   Min WQ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P393, DOI 10.1145/3394171.3414031
   Min WQ, 2020, IEEE T MULTIMEDIA, V22, P2659, DOI 10.1109/TMM.2019.2958761
   Min WQ, 2019, ACM COMPUT SURV, V52, DOI 10.1145/3329168
   Min WQ, 2017, IEEE T MULTIMEDIA, V19, P1100, DOI 10.1109/TMM.2016.2639382
   Movshovitz-Attias Y, 2017, IEEE I CONF COMP VIS, P360, DOI 10.1109/ICCV.2017.47
   Ong EJ, 2017, Arxiv, DOI arXiv:1702.00338
   Pouladzadeh P, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3063592
   Qian Q, 2019, IEEE I CONF COMP VIS, P6459, DOI 10.1109/ICCV.2019.00655
   Roth K., 2020, PMLR, P8242
   Roth K., 2019, About us
   Salvador A, 2017, PROC CVPR IEEE, P3068, DOI 10.1109/CVPR.2017.327
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shimoda W, 2017, 2017 IEEE THIRD INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM 2017), P165, DOI 10.1109/BigMM.2017.73
   Sohn K, 2016, ADV NEUR IN, V29
   Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434
   Sun YF, 2020, PROC CVPR IEEE, P6397, DOI 10.1109/CVPR42600.2020.00643
   Tang ZM, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3501405
   Do TT, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3314051
   Tishby N, 2015, 2015 IEEE INFORMATION THEORY WORKSHOP (ITW)
   Ustinova E, 2016, ADV NEUR IN, V29
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Verma V, 2019, PR MACH LEARN RES, V97
   Wah C., 2011, Tech. Rep. CNS-TR-2011-001
   Wang H, 2022, IEEE T MULTIMEDIA, V24, P2515, DOI 10.1109/TMM.2021.3083109
   Wang Liping, 2008, Proc. 17th Int. Conf. World Wide Web (WWW'08), P979, DOI [DOI 10.1145/1367497.1367629, 10.1145/1367497.1367629]
   Wang WJ, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3418211
   Wang ZC, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3340262
   Wu CY, 2017, IEEE I CONF COMP VIS, P2859, DOI 10.1109/ICCV.2017.309
   Wu JJ, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3497746
   Xu QL, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3446209
   Yao XX, 2021, IEEE T MULTIMEDIA, V23, P1640, DOI 10.1109/TMM.2020.3001527
   Zhai Andrew, 2019, P BRIT MACH VIS C BM
   Zhang L, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3473341
   Zhang SL, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3505280
   Zhu B, 2022, IEEE T MULTIMEDIA, V24, P1175, DOI 10.1109/TMM.2021.3123474
NR 63
TC 0
Z9 0
U1 2
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2024
VL 20
IS 1
AR 13
DI 10.1145/3600095
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T8LL3
UT WOS:001080441800013
OA Bronze
DA 2024-08-05
ER

PT J
AU Chen, Z
   Zhao, J
   Yang, MY
   Zhou, WG
   Li, HQ
AF Chen, Zheng
   Zhao, Jian
   Yang, Mingyu
   Zhou, Wengang
   Li, Houqiang
TI Optimizing Camera Motion with MCTS and Target Motion Modeling in
   Multi-Target Active Object Tracking
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Active object tracking; reinforcement learning; target motion modeling;
   Monte Carlo tree search
ID NETWORKS; COVERAGE
AB In this work, we are dedicated to multi-target active object tracking (AOT), where the goal is to achieve continuous tracking of targets through real-time control of camera. This form of active camera control can be applied to unmanned aerial vehicles (UAV), intelligent robots, and sports events. Our work is conducted in an environment featuring multiple cameras and targets, where our goal is to maximize target coverage. Contrasting with previous research, our work introduces additional degrees of freedom for the cameras, allowing them not only to rotate but also to move along boundary lines. In addition, we model the motion of target to predict the future position of the target in environment. With target's future position, we use Monte Carlo Tree Search (MCTS) method to find the optimal action of camera. Since the action space is large, we propose to leverage the action selection from multi-agent reinforcement learning (MARL) network to prune the search tree of Monte Carlo Tree Search method, so as to find the optimal action more efficiently. We establish a multi-target 2D environment to simulate several sports games, and experimental results demonstrate that our method can effectively improve the target coverage. The code is available at: http://github.com/HopeChanger/ActiveObjectTracking.
C1 [Chen, Zheng; Zhao, Jian; Yang, Mingyu; Zhou, Wengang; Li, Houqiang] Univ Sci & Technol China, Hefei 230027, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Zhou, WG (corresponding author), Univ Sci & Technol China, Hefei 230027, Anhui, Peoples R China.
EM czczcz@mail.ustc.edu.cn; zj140@mail.ustc.edu.cn; ymy@mail.ustc.edu.cn;
   zhwg@ustc.edu.cn; lihq@ustc.edu.cn
OI Zhao, Jian/0000-0003-4895-990X
FU National Key R&D Program of China [2022ZD0119802]; National Natural
   Science Foundation of China [61836011]; GPU cluster built by MCC Lab of
   Information Science and Technology Institution of USTC
FX This work was supported in part by National Key R&D Program of China
   under contract 2022ZD0119802, and in part by National Natural Science
   Foundation of China under Contract 61836011. It was also supported by
   the GPU cluster built by MCC Lab of Information Science and Technology
   Institution of USTC.
CR Ai J, 2006, J COMB OPTIM, V11, P21, DOI 10.1007/s10878-006-5975-x
   An N, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3441656
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bhat G, 2019, IEEE I CONF COMP VIS, P6181, DOI 10.1109/ICCV.2019.00628
   Cai Y, 2007, PROCEEDINGS OF FUTURE GENERATION COMMUNICATION AND NETWORKING, MAIN CONFERENCE PAPERS, VOL 1, P273
   Çelik Y, 2017, 2017 INTERNATIONAL ARTIFICIAL INTELLIGENCE AND DATA PROCESSING SYMPOSIUM (IDAP)
   CHAN YT, 1979, IEEE T AERO ELEC SYS, V15, P237, DOI 10.1109/TAES.1979.308710
   Chen UR, 2008, 2008 IEEE ASIA-PACIFIC SERVICES COMPUTING CONFERENCE, VOLS 1-3, PROCEEDINGS, P174, DOI 10.1109/APSCC.2008.37
   CHENG YZ, 1995, IEEE T PATTERN ANAL, V17, P790, DOI 10.1109/34.400568
   Comaniciu D, 2000, THIRD IEEE INTERNATIONAL WORKSHOP ON VISUAL SURVEILLANCE, PROCEEDINGS, P11, DOI 10.1109/VS.2000.856853
   Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   DENZLER J, 1994, IEEE IMAGE PROC, P635, DOI 10.1109/ICIP.1994.413812
   Fang Z., 2022, arXiv
   Fischer T, 2023, IEEE T PATTERN ANAL, V45, P15380, DOI 10.1109/TPAMI.2023.3301975
   Guvensan MA, 2011, AD HOC NETW, V9, P1238, DOI 10.1016/j.adhoc.2011.02.003
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Hessel M, 2018, AAAI CONF ARTIF INTE, P3215
   Kempka M, 2016, IEEE CONF COMPU INTE
   Kim KK, 2005, 7TH INTERNATIONAL CONFERENCE ON ADVANCED COMMUNICATION TECHNOLOGY, VOLS 1 AND 2, PROCEEDINGS, P817
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li J, 2020, AAAI CONF ARTIF INTE, V34, P759
   Li R, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3595379
   Li W, 2018, COMPUT NETW, V133, P33, DOI 10.1016/j.comnet.2018.01.031
   Luo W., 2018, PMLR, P3286
   Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352
   Mnih V, 2013, Arxiv, DOI arXiv:1312.5602
   Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465
   Osais YE, 2010, MOBILE NETW APPL, V15, P216, DOI 10.1007/s11036-009-0179-0
   Padhy Ram Prasad, 2022, ACM Transactions on Multimedia Computing, Communications, and Applications, V20, P1
   Pan X., 2022, PROC INT C ADV NEUR, V35, P27862
   Qiu WC, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1221, DOI 10.1145/3123266.3129396
   Ramaswamy Arunselvan, 2022, IEEE Transactions on Artificial Intelligence, V3, P139, DOI 10.1109/TAI.2021.3111142
   Rosello P, 2018, PROCEEDINGS OF THE 17TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS (AAMAS' 18), P1397
   Samvelyan Mikayel, 2019, P INT C AUT AG MULT
   Singh A. J., 2020, P INT C AUT AG MULT, P1278
   Tampuu A, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0172395
   Torabi Behnam, 2020, P INT C AUT AG MULT, P1386
   Torkaman B., 2012, 2012 20th Iranian Conference on Electrical Engineering (ICEE 2012), P928, DOI 10.1109/IranianCEE.2012.6292486
   Vinyals O., 2019, ALPHASTAR MASTERING, V2, P20
   Xi M, 2022, IEEE T CIRC SYST VID, V32, P3697, DOI 10.1109/TCSVT.2021.3107153
   Xian-Xin Shao, 2022, IEEE Transactions on Artificial Intelligence, V3, P29, DOI 10.1109/TAI.2021.3103143
   Xu Jing, 2020, P INT C NEUR INF PRO
   Yorke-Smith N, 2020, P 19 INT C AUT AG MU, P816, DOI DOI 10.5555/3398761.3398858
   Yuan D, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3486678
   YuanfeiWang Fangwei Zhong, 2021, P INT C LEARN REPR I
   Zhang GL, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16122183
   Zhong F., 2021, PMLR, P12782
   Zhong F., 2018, P INT C LEARN REPR I
   Zhong FW, 2021, IEEE T PATTERN ANAL, V43, P1467, DOI 10.1109/TPAMI.2019.2952590
   Zhong Fangwei, 2023, P AAAI C ART INT AAA
NR 51
TC 0
Z9 0
U1 2
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2024
VL 20
IS 7
SI SI
AR 208
DI 10.1145/3648369
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL0Q6
UT WOS:001234494100023
DA 2024-08-05
ER

PT J
AU Gao, XJ
   Pang, Y
   Liu, YY
   Han, MK
   Yu, J
   Wang, W
   Chen, YX
AF Gao, Xinjian
   Pang, Ye
   Liu, Yuyu
   Han, Maokun
   Yu, Jun
   Wang, Wei
   Chen, Yuanxu
TI Multimodal Visual-Semantic Representations Learning for Scene Text
   Recognition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Scene text recognition; vision transformer; self-supervised learning
AB Scene Text Recognition (STR), the critical step in OCR systems, has attracted much attention in computer vision. Recent research on modeling textual semantics with Language Model (LM) has witnessed remarkable progress. However, LM only optimizes the joint probability of the estimated characters generated from the Vision Model (VM) in a single language modality, ignoring the visual-semantic relations in different modalities. Thus, LM-based methods can hardly generalize well to some challenging conditions, in which the text has weak or multiple semantics, arbitrary shape, and so on. To migrate the above issue, in this paper, we propose Multimodal Visual-Semantic Representations Learning for Text Recognition Network (MVSTRN) to reason and combine the multimodal visual-semantic information for accurate Scene Text Recognition. Specifically, our MVSTRN builds a bridge between vision and language through its unified architecture and has the ability to reason visual semantics by guiding the network to reconstruct the original image from the latent text representation, breaking the structural gap between vision and language. Finally, the tailored multimodal Fusion (MMF) module is motivated to combine the multimodal visual and textual semantics from VM and LM to make the final predictions. Extensive experiments demonstrate our MVSTRN achieves state-of-the-art performance on several benchmarks.
C1 [Gao, Xinjian; Yu, Jun] Univ Sci & Technol China USTC, 96 Jinzhai Rd, Hefei 230026, Peoples R China.
   [Pang, Ye; Liu, Yuyu; Han, Maokun; Wang, Wei; Chen, Yuanxu] Ping An Technol Co Ltd, 3 Xinyuan South Rd, Beijing 100016, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Yu, J (corresponding author), Univ Sci & Technol China USTC, 96 Jinzhai Rd, Hefei 230026, Peoples R China.; Wang, W (corresponding author), Ping An Technol Co Ltd, 3 Xinyuan South Rd, Beijing 100016, Peoples R China.
EM gxjhlj@mail.ustc.edu.cn; pangye165@pingan.com.cn; harryjun@ustc.edu.cn
FU Natural Science Foundation of China [62276242]; National Aviation
   Science Foundation [2022Z071078001]; CAAI-Huawei MindSpore Open Fund
   [CAAIXSJLJJ-2021-016B, CAAIXSJLJJ-2022-001A]; Anhui Province Key
   Research and Development Program [202104a05020007]; Dreams Foundation of
   Jianghuai Advance Technology Center [2023-ZM01Z001]; USTC-IAT
   Application Sci. & Tech. Achievement Cultivation Program [JL06521001Y];
   Sci. & Tech. Innovation Special Zone [20-163-14-LZ-001-004-01]
FX This work was supported by the Natural Science Foundation of China
   (62276242), National Aviation Science Foundation (2022Z071078001),
   CAAI-Huawei MindSpore Open Fund (CAAIXSJLJJ-2021-016B,
   CAAIXSJLJJ-2022-001A), Anhui Province Key Research and Development
   Program (202104a05020007), Dreams Foundation of Jianghuai Advance
   Technology Center (2023-ZM01Z001), USTC-IAT Application Sci. & Tech.
   Achievement Cultivation Program (JL06521001Y), Sci. & Tech. Innovation
   Special Zone (20-163-14-LZ-001-004-01).
CR Aberdam A, 2022, Arxiv, DOI arXiv:2205.03873
   Atienza R, 2021, LECT NOTES COMPUT SC, V12821, P319, DOI 10.1007/978-3-030-86549-8_21
   Baek J, 2019, IEEE I CONF COMP VIS, P4714, DOI 10.1109/ICCV.2019.00481
   Bao H., 2021, arXiv preprint arXiv:2106.08254, DOI DOI 10.48550/ARXIV.2106.08254
   Bhunia AK, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14920, DOI 10.1109/ICCV48922.2021.01467
   Brown T., 2020, ADV NEURAL INFORM PR, V33, P1877, DOI DOI 10.48550/ARXIV.2005.14165
   Chen X., 2021, P IEEECVF INT C COMP, P9640
   Cheng ZZ, 2017, IEEE I CONF COMP VIS, P5086, DOI 10.1109/ICCV.2017.543
   Chuhan Zhang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P51, DOI 10.1007/978-3-030-58517-4_4
   Deli Yu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12110, DOI 10.1109/CVPR42600.2020.01213
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Fang SC, 2021, PROC CVPR IEEE, P7094, DOI 10.1109/CVPR46437.2021.00702
   Feng W, 2019, IEEE I CONF COMP VIS, P9075, DOI 10.1109/ICCV.2019.00917
   Fu Zilong, 2022, ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)
   Gan C, 2017, IEEE I CONF COMP VIS, P1829, DOI 10.1109/ICCV.2017.201
   Gupta A, 2016, PROC CVPR IEEE, P2315, DOI 10.1109/CVPR.2016.254
   He KM, 2022, PROC CVPR IEEE, P15979, DOI 10.1109/CVPR52688.2022.01553
   He P, 2016, AAAI CONF ARTIF INTE, P3501
   Hui Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P751, DOI 10.1007/978-3-030-58586-0_44
   Jaderberg M, 2014, Arxiv, DOI arXiv:1406.2227
   Jaderberg M, 2016, INT J COMPUT VISION, V116, P1, DOI 10.1007/s11263-015-0823-z
   JianfengWang Xiaolin, 2017, Advances in Neural Information Processing Systems, V30
   Jiang H, 2021, LECT NOTES COMPUT SC, V12821, P287, DOI 10.1007/978-3-030-86549-8_19
   Karatzas D, 2015, PROC INT CONF DOC, P1156, DOI 10.1109/ICDAR.2015.7333942
   Karatzas D, 2013, PROC INT CONF DOC, P1484, DOI 10.1109/ICDAR.2013.221
   Lee J, 2020, IEEE COMPUT SOC CONF, P2326, DOI 10.1109/CVPRW50498.2020.00281
   Li H, 2019, AAAI CONF ARTIF INTE, P8610
   Li XP, 2022, PATTERN RECOGN, V124, DOI 10.1016/j.patcog.2021.108455
   Liao MH, 2019, AAAI CONF ARTIF INTE, P8714
   Liao MH, 2021, IEEE T PATTERN ANAL, V43, P532, DOI 10.1109/TPAMI.2019.2937086
   Linlin Chao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P460, DOI 10.1007/978-3-030-58604-1_28
   Liu ZD, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3356728
   Liu Zhandong, 2021, ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), V17, P1
   Mao JY, 2019, Arxiv, DOI arXiv:1904.12584
   Mishra A, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.127
   Qiao Z, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2046, DOI 10.1145/3474085.3475238
   Radford A., 2018, Improving language understanding by generative pre-training, P850
   Radford A., 2019, OpenAI blog, V1, P9
   Risnumawan A, 2014, EXPERT SYST APPL, V41, P8027, DOI 10.1016/j.eswa.2014.07.008
   Shi BG, 2019, IEEE T PATTERN ANAL, V41, P2035, DOI 10.1109/TPAMI.2018.2848939
   Shi BG, 2017, IEEE T PATTERN ANAL, V39, P2298, DOI 10.1109/TPAMI.2016.2646371
   Phan TQ, 2013, IEEE I CONF COMP VIS, P569, DOI 10.1109/ICCV.2013.76
   Wan ZY, 2020, AAAI CONF ARTIF INTE, V34, P12120
   Wang K, 2011, IEEE I CONF COMP VIS, P1457, DOI 10.1109/ICCV.2011.6126402
   Wang TW, 2020, AAAI CONF ARTIF INTE, V34, P12216
   Wang YX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14174, DOI 10.1109/ICCV48922.2021.01393
   Xie HT, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3231737
   Xie ZC, 2019, PROC CVPR IEEE, P6531, DOI 10.1109/CVPR.2019.00670
   Yan RJ, 2021, PROC CVPR IEEE, P284, DOI 10.1109/CVPR46437.2021.00035
   Yang L, 2020, NEUROCOMPUTING, V414, P67, DOI 10.1016/j.neucom.2020.07.010
   Yang X, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3280
   Yi KX, 2018, ADV NEUR IN, V31
   Yue X., 2020, LNCS, P135, DOI [DOI 10.1007/978-3, 10.1007/978-3-030-58529-7_9, DOI 10.1007/978-3-030-58529-7_9]
   Yuxin Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11750, DOI 10.1109/CVPR42600.2020.01177
   Zhan FN, 2019, PROC CVPR IEEE, P2054, DOI 10.1109/CVPR.2019.00216
   Zhang CW, 2021, Arxiv, DOI arXiv:2005.13117
   Zhi Qiao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13525, DOI 10.1109/CVPR42600.2020.01354
   Zhou JH, 2022, Arxiv, DOI arXiv:2111.07832
   Zhu B, 2015, ACM T MULTIM COMPUT, V12, DOI 10.1145/2808210
NR 60
TC 0
Z9 0
U1 6
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2024
VL 20
IS 7
SI SI
AR 193
DI 10.1145/3646551
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL0Q6
UT WOS:001234494100008
OA Bronze
DA 2024-08-05
ER

PT J
AU Zhang, QN
   Xiong, ZH
   Zhu, JM
   Gao, S
   Yang, WT
AF Zhang, Qinnan
   Xiong, Zehui
   Zhu, Jianming
   Gao, Sheng
   Yang, Wanting
TI A Privacy-preserving Auction Mechanism for Learning Model as an NFT in
   Blockchain-driven Metaverse
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Metaverse; blockchain; federated learning; NFT; auction mechanism;
   Stackelberg game; HMM
ID WIRELESS NETWORKS
AB The Metaverse, envisioned as the next-generation Internet, will be constructed via twining a practical world in a virtual form, wherein Meterverse service providers (MSPs) are required to collect massive data from Meterverse users (MUs). In this regard, a critical demand exists for MSPs to motivate MUs to contribute computing resources and data while preserving user privacy. Federated learning (FL), as a privacy-preserving collaborative machine learning paradigm, can support distributed intensive computation in the Metaverse. In this work, we first investigate minting the machine learning models into NFT with FL assistance (referred to as FL-NFT), such that MUs as stakeholders can control the ownership and share the economic value of user-generated content (UGC). Specifically, MUs are encouraged to establish a decentralized autonomous organization (i.e., MU-DAO) to aggregate local models and mint FL-NFT. MUs and MSPs optimize the strategies by formulating an imperfect information Stackelberg game to trade off the cost and benefit. We apply the backward induction to derive the equilibrium solution. Then, we construct a privacy-preserving multi-winner sealed-bid auction mechanism (PMS-AM), in which the Hidden Markov Model assists MSPs in choosing rational bidding strategies according to historical bids, and the double auction mechanism determines the winners and price of FL-NFT. Finally, the numerical results based on theoretical analysis and simulations demonstrate that the proposed PMS-AM can increase the quality of FL-NFT and achieve the economic properties of incentive mechanisms such as individual rationality and incentive compatibility.
C1 [Zhang, Qinnan; Zhu, Jianming; Gao, Sheng] Cent Univ Finance & Econ, 39 Coll South Rd, Beijing 100081, Peoples R China.
   [Zhang, Qinnan; Xiong, Zehui; Yang, Wanting] Singapore Univ Technol & Design, 8 Somapah Rd, Singapore 487372, Singapore.
   [Yang, Wanting] Jilin Univ, 2699 Qianjin St, Changchun, Jilin, Peoples R China.
C3 Central University of Finance & Economics; Singapore University of
   Technology & Design; Jilin University
RP Zhu, JM; Gao, S (corresponding author), Cent Univ Finance & Econ, 39 Coll South Rd, Beijing 100081, Peoples R China.; Xiong, ZH (corresponding author), Singapore Univ Technol & Design, 8 Somapah Rd, Singapore 487372, Singapore.
EM zhangqnp@163.com; zehui_xiong@sutd.edu.sg; zjm@cufe.edu.cn;
   sgao@cufe.edu.cn; yangwt18@mails.jlu.edu.cn
RI Xiong, Zehui/B-9792-2019; Gao, Sheng/ISS-8753-2023
OI Xiong, Zehui/0000-0002-4440-941X; Gao, Sheng/0000-0001-8118-411X; Zhang,
   Qinnan/0000-0001-9220-2694
FU National Research Foundation (NRF); Infocomm Media Development Authority
   under the Future Communications Research Development Programme (FCP);
   SUTD Grant [SRG-ISTD-2021-165]; SUTD-ZJU IDEA Grant; SUTD-ZJU [202102];
   Ministry of Education, Singapore, under its SUTD Kickstarter Initiative
   [SKI 20210204]; National Natural Science Foundation of China [62072487];
   Beijing Natural Science Foundation [M21036]; China Scholarship Council
   [202206490012]
FX The research is supported by the National Research Foundation (NRF) and
   Infocomm Media Development Authority under the Future Communications
   Research Development Programme (FCP). The research is also supported by
   the SUTD Grant No. SRG-ISTD-2021-165, the SUTD-ZJU IDEA Grant (No.
   SUTD-ZJU (VP) 202102), and the Ministry of Education, Singapore, under
   its SUTD Kickstarter Initiative (No. SKI 20210204). The research is also
   supported by National Natural Science Foundation of China (No.
   62072487), Beijing Natural Science Foundation (No. M21036), and the
   China Scholarship Council (No. 202206490012)
CR Acquisti A, 2015, SCIENCE, V347, P509, DOI 10.1126/science.aaa1465
   Al-Tous H, 2012, 6TH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING AND COMMUNICATION SYSTEMS (ICSPCS'2012)
   Asheralieva A, 2019, IEEE T VEH TECHNOL, V68, P10094, DOI 10.1109/TVT.2019.2934027
   Chen JM, 2017, Arxiv, DOI arXiv:1604.00981
   Dawei Chen, 2020, 2020 International Conference on Computing, Networking and Communications (ICNC), P767, DOI 10.1109/ICNC47757.2020.9049708
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Du JB, 2020, IEEE INTERNET THINGS, V7, P9517, DOI 10.1109/JIOT.2020.3003449
   Duan HH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P153, DOI 10.1145/3474085.3479238
   Dwork C, 2008, LECT NOTES COMPUT SC, V4978, P1, DOI 10.1007/978-3-540-79228-4_1
   Fan SZ, 2023, IEEE T NETW SCI ENG, V10, P1376, DOI 10.1109/TNSE.2022.3163791
   Gao S, 2020, IEEE T VEH TECHNOL, V69, P5784, DOI 10.1109/TVT.2020.2967099
   Gao S, 2019, CHINA COMMUN, V16, P111, DOI 10.23919/JCC.2019.12.008
   Hanna MG, 2018, ARCH PATHOL LAB MED, V142, P638, DOI 10.5858/arpa.2017-0189-OA
   Hee Kwon Chang, 2021, [Journal of the Chosun Natural Science, 조선자연과학논문집], V14, P21
   Huang C, 2023, IEEE T MOBILE COMPUT, V22, P2989, DOI 10.1109/TMC.2021.3131445
   Ian G., 2016, DEEP LEARNING
   Jeong H, 2022, INT J INNOV COMPUT I, V18, P221, DOI 10.24507/ijicic.18.01.221
   Jiang Y., 2021, arXiv
   Joshua J, 2017, INTERDISCIP LIT STUD, V19, P17
   Kang JW, 2019, IEEE INTERNET THINGS, V6, P10700, DOI 10.1109/JIOT.2019.2940820
   Kim Soo-Hyun, 2021, J. Korea Entertain. Industry Assoc., V15, P1
   Konečny J, 2017, Arxiv, DOI arXiv:1610.05492
   Krizhevsky Alex, 2010, Convolutional deep belief networks on cifar-10
   LeCun Y., 1998, The MNIST Database of Handwritten Digits
   Lee L H., 2021, PREPRINT
   Li AR, 2021, IEEE INFOCOM SER, DOI 10.1109/INFOCOM42981.2021.9488723
   Li BW, 2023, IEEE T PATTERN ANAL, V45, P4521, DOI 10.1109/TPAMI.2022.3195956
   Li X, 2020, Arxiv, DOI arXiv:1907.02189
   Lin X, 2023, IEEE T MOBILE COMPUT, V22, P2402, DOI 10.1109/TMC.2021.3122013
   Lin X, 2023, IEEE T MOBILE COMPUT, V22, P269, DOI 10.1109/TMC.2021.3074816
   Liu RX, 2020, LECT NOTES COMPUT SC, V12112, P485, DOI 10.1007/978-3-030-59410-7_33
   Lu YL, 2021, IEEE T IND INFORM, V17, P5098, DOI 10.1109/TII.2020.3017668
   Lu YL, 2021, IEEE INTERNET THINGS, V8, P2276, DOI 10.1109/JIOT.2020.3015772
   MCAFEE RP, 1992, J ECON THEORY, V56, P434, DOI 10.1016/0022-0531(92)90091-U
   McMahan HB, 2017, PR MACH LEARN RES, V54, P1273
   Mohammad U, 2020, Arxiv, DOI arXiv:1905.01656
   Paillier P, 1999, LECT NOTES COMPUT SC, V1592, P223
   PAL SK, 1992, IEEE T NEURAL NETWOR, V3, P683, DOI 10.1109/72.159058
   Palihawadana C, 2022, NEUROCOMPUTING, V483, P432, DOI 10.1016/j.neucom.2021.08.141
   Gadekallu TR, 2022, Arxiv, DOI [arXiv:2203.09738, DOI 10.48550/ARXIV.2203.09738]
   Shah-Mansouri H, 2017, IEEE T MOBILE COMPUT, V16, P2983, DOI 10.1109/TMC.2017.2688402
   Shokri R, 2015, CCS'15: PROCEEDINGS OF THE 22ND ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1310, DOI 10.1145/2810103.2813687
   Stiglitz Joseph E., 1999, Global public goods, DOI [DOI 10.1093/0195130529.003.0015, 10.1093/0195130529.003.0015]
   Suhail S, 2022, IEEE INTERNET COMPUT, V26, P58, DOI 10.1109/MIC.2021.3059320
   Sun W, 2022, IEEE INTERNET THINGS, V9, P5839, DOI 10.1109/JIOT.2021.3058213
   Tran NH, 2019, IEEE INFOCOM SER, P1387, DOI [10.1109/infocom.2019.8737464, 10.1109/INFOCOM.2019.8737464]
   Wang Q, 2019, IEEE ACM T NETWORK, V27, P848, DOI 10.1109/TNET.2019.2903879
   Wei W, 2018, IEEE T SERV COMPUT, V11, P78, DOI 10.1109/TSC.2016.2528246
   Wu YW, 2021, IEEE INTERNET THINGS, V8, P13789, DOI 10.1109/JIOT.2021.3079510
   Xiong ZH, 2020, IEEE T NETW SCI ENG, V7, P2363, DOI 10.1109/TNSE.2020.3016963
   Xiong ZH, 2020, IEEE INTERNET THINGS, V7, P5184, DOI 10.1109/JIOT.2020.2975804
   Xu Mengde, 2021, ARXIV
   Xu MR, 2023, IEEE COMMUN SURV TUT, V25, P656, DOI 10.1109/COMST.2022.3221119
   Yang QL, 2022, IEEE OPEN J COMP SOC, V3, P122, DOI 10.1109/OJCS.2022.3188249
   Yu SZ, 2003, IEEE SIGNAL PROC LET, V10, P11, DOI 10.1109/LSP.2002.806705
   Yu X., 2012, P INT C HLTH INF SCI, P4
   Zhang Meng, 2022, ACM SIGMETRICS Performance Evaluation Review, P115, DOI 10.1145/3547353.3526953
   Zhang Qinnan, 2023, P INT C MET COMP NET, P1
   Zhang XJ, 2022, Arxiv, DOI arXiv:2203.05816
   Zhang ZL, 2018, ADV NEUR IN, V31
NR 60
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2024
VL 20
IS 7
SI SI
AR 190
DI 10.1145/3599971
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL0Q6
UT WOS:001234494100005
OA Bronze
DA 2024-08-05
ER

PT J
AU Chen, CX
   Zhang, PY
AF Chen, Chengxin
   Zhang, Pengyuan
TI Modality-collaborative Transformer with Hybrid Feature Reconstruction
   for Robust Emotion Recognition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Multimodal emotion recognition; transformer; sequential data missing;
   hybrid feature reconstruction
ID FUSION
AB As a vital aspect of affective computing, Multimodal Emotion Recognition has been an active research area in the multimedia community. Despite recent progress, this field still confronts two major challenges in real-world applications: (1) improving the efficiency of constructing joint representations from unaligned multimodal features and (2) relieving the performance decline caused by random modality feature missing. In this article, we propose a unified framework, Modality-Collaborative Transformer with Hybrid Feature Reconstruction (MCT-HFR), to address these issues. The crucial component of MCT is a novel attention-based encoder that concurrently extracts and dynamically balances the intra- and inter-modality relations for all associated modalities. With additional modality-wise parameter sharing, a more compact representation can be encoded with less time and space complexity. To improve the robustness of MCT, we further introduce HFR, which consists of two modules: Local Feature Imagination (LFI) and Global Feature Alignment (GFA). During model training, LFI leverages complete features as supervisory signals to recover local missing features, while GFA is designed to reduce the global semantic gap between pairwise-complete and -incomplete representations. Experimental evaluations on two popular benchmark datasets demonstrate that our proposed method consistently outperforms advanced baselines in both complete and incomplete data scenarios.
C1 [Chen, Chengxin; Zhang, Pengyuan] Chinese Acad Sci, Inst Acoust, Key Lab Speech Acoust & Content Understanding, Beijing, Peoples R China.
   [Chen, Chengxin; Zhang, Pengyuan] Chinese Acad Sci, Inst Acoust, Key Lab Speech Acoust & Content Understanding, 21 North 4th Ring Rd, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Acoustics, CAS; Chinese
   Academy of Sciences
RP Chen, CX (corresponding author), Chinese Acad Sci, Inst Acoust, Key Lab Speech Acoust & Content Understanding, 21 North 4th Ring Rd, Beijing, Peoples R China.
EM chenchengxin@hccl.ioa.ac.cn; zhangpengyuan@hccl.ioa.ac.cn
OI chen, chengxin/0000-0003-4510-3313
FU National Key Research and Development Program of China [2021YFC3320103]
FX This research was partially supported by the National Key Research and
   Development Program of China (No. 2021YFC3320103).
CR Abdu SA, 2021, INFORM FUSION, V76, P204, DOI 10.1016/j.inffus.2021.06.003
   Ayata D, 2020, J MED BIOL ENG, V40, P149, DOI 10.1007/s40846-019-00505-7
   Ba L.J., 2016, arXiv, DOI DOI 10.48550/ARXIV.1607.06450
   Baevski A, 2020, Advances in neural information processing systems, V33, P12449, DOI 10.5555/3495724.3496768
   Baltrusaitis T, 2018, IEEE INT CONF AUTOMA, P59, DOI 10.1109/FG.2018.00019
   Bao Hangbo, 2021, P 9 INT C LEARNING R
   Binghua Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P431, DOI 10.1007/978-3-030-58586-0_26
   Bishop CM., 2006, PATTERN RECOGN
   Busso C, 2017, IEEE T AFFECT COMPUT, V8, P67, DOI 10.1109/TAFFC.2016.2515617
   Busso C, 2008, LANG RESOUR EVAL, V42, P335, DOI 10.1007/s10579-008-9076-6
   Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020
   Caron M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9630, DOI 10.1109/ICCV48922.2021.00951
   Chen M., 2017, P 19 ACM INT C MULT, P163, DOI [10.1145/3136755.3136801, DOI 10.1145/3136755.3136801]
   Chen SY, 2022, IEEE J-STSP, V16, P1505, DOI 10.1109/JSTSP.2022.3188113
   Chen T, 2020, PR MACH LEARN RES, V119
   Chen WD, 2022, INT CONF ACOUST SPEE, P6897, DOI 10.1109/ICASSP43922.2022.9746598
   Chiang D, 2022, PROCEEDINGS OF THE 60TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), VOL 1: (LONG PAPERS), P7654
   Chumachenko K, 2022, INT C PATT RECOG, P2822, DOI 10.1109/ICPR56361.2022.9956592
   Clark K, 2020, INFORM SYST RES, DOI 10.48550/arXiv.2003.10555
   DanHendrycks Mantas Mazeika, 2019, Adv. Neural Inf. Process. Syst., V32
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Du CD, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P108, DOI 10.1145/3240508.3240528
   Du MN, 2021, 2021 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL-HLT 2021), P915
   Georgiou E, 2021, INTERSPEECH, P2876, DOI 10.21437/Interspeech.2021-1739
   Georgiou E, 2019, INTERSPEECH, P1646, DOI 10.21437/Interspeech.2019-3243
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Goncalves L, 2022, IEEE T AFFECT COMPUT, V13, P2156, DOI 10.1109/TAFFC.2022.3216993
   Guo D, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3152121
   Guo JW, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P3394, DOI 10.1145/3503161.3548137
   Han Wei, 2022, arXiv
   Hazarika D, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1122, DOI 10.1145/3394171.3413678
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Pengcheng, 2020, P 8 INT C LEARNING R
   Hsu WN, 2021, IEEE-ACM T AUDIO SPE, V29, P3451, DOI 10.1109/TASLP.2021.3122291
   Huang J, 2020, INT CONF ACOUST SPEE, P3507, DOI [10.1109/icassp40776.2020.9053762, 10.1109/ICASSP40776.2020.9053762]
   Lian Z, 2023, IEEE T PATTERN ANAL, V45, P8419, DOI 10.1109/TPAMI.2023.3234553
   Liang PP, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P150
   Liang PP, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P1569
   Liu Z, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2247
   Loshchilov I, 2019, Arxiv, DOI [arXiv:1711.05101, 10.48550/arXiv.1711.05101, DOI 10.48550/ARXIV.1711.05101]
   Tran L, 2017, PROC CVPR IEEE, P4971, DOI 10.1109/CVPR.2017.528
   Lv F, 2021, PROC CVPR IEEE, P2554, DOI 10.1109/CVPR46437.2021.00258
   Ma MM, 2021, AAAI CONF ARTIF INTE, V35, P2302
   Mirsamadi S, 2017, INT CONF ACOUST SPEE, P2227, DOI 10.1109/ICASSP.2017.7952552
   Nojavanasghari B, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P284, DOI 10.1145/2993148.2993176
   Paszke A, 2019, ADV NEUR IN, V32
   Pérez-Rúa JM, 2019, PROC CVPR IEEE, P6959, DOI 10.1109/CVPR.2019.00713
   Peters ME, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P1499, DOI 10.5771/9783845286846
   Pham H, 2019, AAAI CONF ARTIF INTE, P6892
   Polignano M, 2021, EXPERT SYST APPL, V170, DOI 10.1016/j.eswa.2020.114382
   Poria S, 2017, INFORM FUSION, V37, P98, DOI 10.1016/j.inffus.2017.02.003
   Radford A, 2021, PR MACH LEARN RES, V139
   Rahman W, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P2359, DOI 10.18653/v1/2020.acl-main.214
   Rozgic V, 2012, ASIAPAC SIGN INFO PR
   Schneider S, 2019, INTERSPEECH, P3465, DOI 10.21437/Interspeech.2019-1873
   Sun LC, 2024, IEEE T AFFECT COMPUT, V15, P309, DOI 10.1109/TAFFC.2023.3274829
   Sun ZK, 2020, AAAI CONF ARTIF INTE, V34, P8992
   Tsai YHH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6558, DOI 10.18653/v1/p19-1656
   Tsai Yao-HungHubert, 2019, P 7 INT C LEARNING R
   Tseng SY, 2021, IEEE SIGNAL PROC LET, V28, P608, DOI 10.1109/LSP.2021.3065598
   van den Oord A, 2019, Arxiv, DOI [arXiv:1807.03748, DOI 10.48550/ARXIV.1807.03748]
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang YS, 2019, AAAI CONF ARTIF INTE, P7216
   Yang JW, 2022, PROC CVPR IEEE, P19141, DOI 10.1109/CVPR52688.2022.01857
   Young T, 2018, IEEE COMPUT INTELL M, V13, P55, DOI 10.1109/MCI.2018.2840738
   Yuan ZQ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4400, DOI 10.1145/3474085.3475585
   Zadeh A., 2017, C EMP METH NAT LANG
   Zadeh A, 2018, AAAI CONF ARTIF INTE, P5642
   Zadeh A, 2018, AAAI CONF ARTIF INTE, P5634
   Zadeh A, 2016, IEEE INTELL SYST, V31, P82, DOI 10.1109/MIS.2016.94
   Zellinger Werner, 2017, P 5 INT C LEARNING R
   Zeng JDA, 2022, PROCEEDINGS OF THE 45TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '22), P1545, DOI 10.1145/3477495.3532064
   Zhang Changqing, 2020, IEEE Trans. Pattern Anal. Mach. Intell., V44, P2402
   Zhang H, 2021, PROC CVPR IEEE, P833, DOI 10.1109/CVPR46437.2021.00089
   Zhang K, 2021, IEEE SIGNAL PROC LET, V28, P1898, DOI 10.1109/LSP.2021.3112314
   Zhao JM, 2021, 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (ACL-IJCNLP 2021), VOL 1, P2608
   Zhou H, 2018, AAAI CONF ARTIF INTE, P730
NR 78
TC 0
Z9 0
U1 7
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2024
VL 20
IS 5
AR 146
DI 10.1145/3640343
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MF3R9
UT WOS:001192177900026
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Tan, JW
   Wang, HX
   Yuan, JS
AF Tan, Jiawei
   Wang, Hongxing
   Yuan, Junsong
TI Characters Link Shots: Character Attention Network for Movie Scene
   Segmentation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Movie scene detection; video structuring; masked attention; video
   understanding
AB Movie scene segmentation aims to automatically segment a movie into multiple story units, i.e., scenes, each of which is a series of semantically coherent and time-continual shots. Previous methods have continued efforts on shot semantic association, but few take into account the impact of different semantics on foreground characters and background scenes in movie shots. In particular, the background scene in the shot can adversely affect scene boundary classification. Motivated by the fact that it is the characters who drive the plot development of a movie scene, we build a Character Attention Network (CANet) to detect movie scene boundaries in a character-centric fashion. To eliminate the background clutter, we extract multi-view character semantics for each shot in terms of human bodies and faces. Furthermore, we equip our CANet with two stages of character attention. The first is Masked Shot Attention (MSA) through selective self-attention over similar temporal contexts from multi-view character semantics to yield an enhanced omni-view shot representation, by which the CANet can better handle the variations of characters in pose and appearance. The second is Key Character Attention (KCA) through temporal-aware attention on character reappearances for Bidirectional Long Short-TermMemory (Bi-LSTM) feature association so that linking shots can be focused on those with recurring key characters. We encourage the proposed CANet in learning boundary-discriminative shot features. Specifically, we formulate a Boundary-Aware circle Loss (BAL) to push far apart CANet-features between adjacent scenes, which is also coupled with the cross-entropy loss to drive CANet-features sensitive to scene boundaries. Experimental results on the MovieNet-SSeg and OVSD datasets show that our method achieves superior performance in temporal scene segmentation compared with state-of-the-art methods.
C1 [Tan, Jiawei; Wang, Hongxing] Chongqing Univ, Key Lab Dependable Serv Comp Cyber Phys Soc, Minist Educ, 174 Shazhengjie, Chongqing 400044, Peoples R China.
   [Tan, Jiawei; Wang, Hongxing] Chongqing Univ, Sch Big Data & Software Engn, 174 Shazhengjie, Chongqing 400044, Peoples R China.
   [Yuan, Junsong] SUNY Buffalo, Dept Comp Sci & Engn, 12 Capen Hall, Buffalo, NY 14260 USA.
C3 Chongqing University; Chongqing University; State University of New York
   (SUNY) System; State University of New York (SUNY) Buffalo
RP Wang, HX (corresponding author), Chongqing Univ, Key Lab Dependable Serv Comp Cyber Phys Soc, Minist Educ, 174 Shazhengjie, Chongqing 400044, Peoples R China.; Wang, HX (corresponding author), Chongqing Univ, Sch Big Data & Software Engn, 174 Shazhengjie, Chongqing 400044, Peoples R China.
EM jwtan@cqu.edu.cn; ihxwang@cqu.edu.cn; jsyuan@buffalo.edu
RI Yuan, Junsong/A-5171-2011; Wang, Hongxing/F-4670-2011
OI Tan, Jiawei/0000-0003-3299-0785; Yuan, Junsong/0000-0002-7901-8793;
   Wang, Hongxing/0000-0001-7799-1023
FU National Natural Science Foundation of China [61976029]; Key Project of
   Chongqing Technology Innovation and Application Development
   [cstc2021jscx-gksbX0033]
FX This work is supported in part by the National Natural Science
   Foundation of China under grant no. 61976029 and the Key Project of
   Chongqing Technology Innovation and Application Development under grant
   no. cstc2021jscx-gksbX0033.
CR Anyi Rao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10143, DOI 10.1109/CVPR42600.2020.01016
   Bain Max, 2021, Computer Vision - ACCV 2020 15th Asian Conference on Computer Vision. Revised Selected Papers. Lecture Notes in Computer Science (LNCS 12626), P460, DOI 10.1007/978-3-030-69541-5_28
   Baraldi L, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1199, DOI 10.1145/2733373.2806316
   Beltagy Iz, 2020, arXiv
   Bertasius G, 2021, PR MACH LEARN RES, V139
   Blender Institute, 2007, ABOUT US
   Carion N., 2020, EUR C COMP VIS, P213
   Castellano B., 2018, Pyscenedetect: Intelligent scene cut detection and video splitting tool
   Chasanis VT, 2009, IEEE T MULTIMEDIA, V11, P89, DOI 10.1109/TMM.2008.2008924
   Chen SX, 2023, PROC CVPR IEEE, P6535, DOI 10.1109/CVPR52729.2023.00632
   Chen SX, 2021, PROC CVPR IEEE, P9791, DOI 10.1109/CVPR46437.2021.00967
   Chen X, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P5904, DOI 10.1109/ICASSP39728.2021.9413535
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dosovitskiy A., 2021, ICLR
   Fryzlewicz P, 2007, J AM STAT ASSOC, V102, P1318, DOI 10.1198/016214507000000860
   Fryzlewicz P, 2014, ANN STAT, V42, P2243, DOI 10.1214/14-AOS1245
   Giannetti Louis D., 1999, Understanding Movies, V1
   Graves A, 2005, NEURAL NETWORKS, V18, P602, DOI 10.1016/j.neunet.2005.06.042
   Guerrini F, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3103241
   Gulati A, 2020, INTERSPEECH, P5036, DOI 10.21437/Interspeech.2020-3015
   Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6
   Han G., 2022, ACCV, P4027
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang QQ, 2018, LECT NOTES COMPUT SC, V11217, P437, DOI 10.1007/978-3-030-01261-8_26
   Islam MM, 2023, PROC CVPR IEEE, P18749, DOI 10.1109/CVPR52729.2023.01798
   Jung B, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1230812.1230817
   Katz E., 2012, The Film Encyclopedia 7th Edition: The Complete Guide to Film and the Film Industry
   Kender JR, 1998, PROC CVPR IEEE, P367, DOI 10.1109/CVPR.1998.698632
   Kingma D. P., 2015, P INT C LEARN REPR, P1
   Li ZJ, 2021, AAAI CONF ARTIF INTE, V35, P2011
   Liu D, 2021, IEEE T CIRC SYST VID, V31, P3559, DOI 10.1109/TCSVT.2020.3042476
   Man X, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3503927
   Panda R, 2018, IEEE T CYBERNETICS, V48, P836, DOI 10.1109/TCYB.2017.2657692
   Pei Yingjiao, 2020, MMAsia, V40
   Potapov D, 2014, LECT NOTES COMPUT SC, V8694, P540, DOI 10.1007/978-3-319-10599-4_35
   Qingqiu Huang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P709, DOI 10.1007/978-3-030-58548-8_41
   Rasheed Z, 2005, IEEE T MULTIMEDIA, V7, P1097, DOI 10.1109/TMM.2005.858392
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rotman D, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P187, DOI 10.1145/3206025.3206055
   Rotman D, 2017, INT J SEMANT COMPUT, V11, P193, DOI 10.1142/S1793351X17400086
   Rotman D, 2016, IEEE INT SYM MULTIM, P275, DOI [10.1109/ISM.2016.0061, 10.1109/ISM.2016.117]
   Rotman Daniel, 2022, arXiv
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sheng-Hua Zhong, 2022, ACM Transactions on Multimedia Computing, Communications and Applications, V18, DOI 10.1145/3477538
   Sidiropoulos P, 2011, IEEE T CIRC SYST VID, V21, P1163, DOI 10.1109/TCSVT.2011.2138830
   Singhania Dipika, 2023, IEEE Trans. Pattern Anal. Mach. Intell., V2023, P1
   Sun YF, 2020, PROC CVPR IEEE, P6397, DOI 10.1109/CVPR42600.2020.00643
   Tan Jing, 2023, IEEE Trans Pattern Anal Mach Intell, V45, P12506, DOI 10.1109/TPAMI.2023.3283067
   Tapaswi M, 2014, PROC CVPR IEEE, P827, DOI 10.1109/CVPR.2014.111
   van den Oord A, 2019, Arxiv, DOI [arXiv:1807.03748, DOI 10.48550/ARXIV.1807.03748]
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Vicol P, 2018, PROC CVPR IEEE, P8581, DOI 10.1109/CVPR.2018.00895
   White S, 2005, SIAM PROC S, P274
   Wu HQ, 2022, PROC CVPR IEEE, P14001, DOI 10.1109/CVPR52688.2022.01363
   Xie RF, 2011, IEEE INT C BIO BIO W, P165, DOI 10.1109/BIBMW.2011.6112370
   Yang ZL, 2019, ADV NEUR IN, V32
   Zellers R, 2019, PROC CVPR IEEE, P6713, DOI 10.1109/CVPR.2019.00688
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhang KP, 2016, IEEE SIGNAL PROC LET, V23, P1499, DOI 10.1109/LSP.2016.2603342
   Zhang L, 2022, IEEE T PATTERN ANAL, V44, P456, DOI 10.1109/TPAMI.2020.3009758
   Zhou LW, 2018, AAAI CONF ARTIF INTE, P7590
   Zhu GY, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3538648
NR 64
TC 1
Z9 1
U1 8
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2024
VL 20
IS 4
AR 94
DI 10.1145/3630257
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6G9
UT WOS:001153382100004
DA 2024-08-05
ER

PT J
AU Zhang, J
   Guo, D
   Yang, X
   Song, PP
   Wang, M
AF Zhang, Jing
   Guo, Dan
   Yang, Xun
   Song, Peipei
   Wang, Meng
TI Visual-linguistic-stylistic Triple Reward for Cross-lingual Image
   Captioning
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Cross-lingual; image captioning; triple reward; semantic matching
AB Generating image captions in different languages is worth exploring and essential for non-native speakers. Nevertheless, collecting paired annotation for every language is time-consuming and impractical, particularly for minor languages. To this end, the cross-lingual image captioning task is proposed, which leverages existing image-source caption annotation data and wild unrelated target corpus to generate satisfactory caption in the target language. Current methods perform a two-step translation process of image-to-pivot (source) and pivot-to-target. The distinct two-step process comes with certain caption issues, such as the weak semantic alignment between the image and the generated caption and the generated caption's non-target language style. To address these issues, we propose an end-to-end reinforce learning framework with Visual-linguistic-stylistic Triple Reward named TriR. In TriR, we jointly consider the visual, linguistic, and stylistic alignments to generate factual, fluent, and natural caption in the target language. To be specific, the image-source caption annotation provides factual semantic guidance, whereas the unrelated target corpus guides the language style of generated caption. To achieve this, we construct a visual reward module to measure the cross-modal semantic embedding of image and target caption, a linguistic reward module to measure the cross-linguistic embedding of source and target captions, and a stylistic reward module to imitate the presentation style of target corpus. The TriR can be implemented with either classical CNN-LSTM or prevalent Transformer architecture. Extensive experiments are conducted with four cross-lingual settings, i.e., Chinese-to-English, English-to-Chinese, English-to-German, and English-to-French. Experimental results demonstrate the remarkable superiority of our method, and sufficient ablation experiments validate the beneficial impact of every reward.
C1 [Zhang, Jing; Wang, Meng] Hefei Univ Technol, Sch Comp Sci & Informat Engn, 420 Feicui Rd, Hefei 230601, Anhui, Peoples R China.
   [Guo, Dan] Hefei Univ Technol HFUT, Sch Comp Sci & Informat Engn, Sch Artificial Intelligence, Intelligent Interconnected Syst Lab Anhui Prov HF, 420 Feicui Rd, Hefei 230601, Anhui, Peoples R China.
   [Yang, Xun; Song, Peipei] Univ Sci & Technol China, Sch Informat Sci & Technol, 96,Jinzhai Rd, Hefei 230026, Anhui, Peoples R China.
C3 Hefei University of Technology; Hefei University of Technology; Chinese
   Academy of Sciences; University of Science & Technology of China, CAS
RP Wang, M (corresponding author), Hefei Univ Technol, Sch Comp Sci & Informat Engn, 420 Feicui Rd, Hefei 230601, Anhui, Peoples R China.; Guo, D (corresponding author), Hefei Univ Technol HFUT, Sch Comp Sci & Informat Engn, Sch Artificial Intelligence, Intelligent Interconnected Syst Lab Anhui Prov HF, 420 Feicui Rd, Hefei 230601, Anhui, Peoples R China.; Yang, X (corresponding author), Univ Sci & Technol China, Sch Informat Sci & Technol, 96,Jinzhai Rd, Hefei 230026, Anhui, Peoples R China.
EM hfutzhangjing@gmail.com; guodan@hfut.edu.cn; xyang21@ustc.edu.cn;
   beta.songpp@gmail.com; eric.mengwang@gmail.com
RI Song, Peipei/KEH-0796-2024; hu, guangchen/KEI-6324-2024
OI zhang, Jing/0009-0005-1590-5886; Guo, Dan/0000-0003-2594-254X; YANG,
   Xun/0000-0003-0201-1638
FU National Key R&D Program of China [2022YFB4500600]; National Natural
   Science Foundation of China [72188101, 62020106007, 62272144, U20A20183,
   62272435, U22A2094]; Major Project of Anhui Province [202203a05020011]
FX This work was supported by the National Key R&D Program of China
   (2022YFB4500600), the National Natural Science Foundation of China
   (72188101, 62020106007, 62272144, U20A20183, 62272435, and U22A2094),
   and the Major Project of Anhui Province (202203a05020011).
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   [Anonymous], 2016, P 1 C MACHINE TRANSL, DOI 10.18653/v1/W16-2346
   Ben HX, 2022, IEEE T MULTIMEDIA, V24, P904, DOI 10.1109/TMM.2021.3060948
   Calixto I., 2017, arXiv
   Calixto I, 2017, Arxiv, DOI arXiv:1702.01287
   Carion N., 2020, EUR C COMP VIS, P213
   Chen Y, 2018, AAAI CONF ARTIF INTE, P5086
   Denkowski M., 2014, P WMT ACL, P376
   Dong JF, 2022, IEEE T PATTERN ANAL, V44, P4065, DOI 10.1109/TPAMI.2021.3059295
   Elliott D, 2017, Arxiv, DOI arXiv:1705.04350
   Elliott D, 2016, Arxiv, DOI arXiv:1605.00459
   Feng Y, 2019, PROC CVPR IEEE, P4120, DOI 10.1109/CVPR.2019.00425
   Gao JH, 2022, AAAI CONF ARTIF INTE, P10654
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Gu JX, 2018, LECT NOTES COMPUT SC, V11205, P519, DOI 10.1007/978-3-030-01246-5_31
   Gu JX, 2019, IEEE I CONF COMP VIS, P10322, DOI 10.1109/ICCV.2019.01042
   Guo D, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P920
   Guo D, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1823, DOI 10.1145/3343031.3350881
   Guo D, 2018, AAAI CONF ARTIF INTE, P6845
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu Y, 2023, Arxiv, DOI arXiv:2211.09699
   Huang PY, 2020, Arxiv, DOI arXiv:2005.03119
   Ive J, 2019, Arxiv, DOI arXiv:1906.07701
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kingma D. P., 2014, arXiv
   Laina I, 2019, IEEE I CONF COMP VIS, P7413, DOI 10.1109/ICCV.2019.00751
   Lan WY, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1549, DOI 10.1145/3123266.3123366
   Li GZ, 2023, PROC CVPR IEEE, P10648, DOI 10.1109/CVPR52729.2023.01026
   Li Kunpeng, 2023, ACM Trans. Multim. Comput., Commun. Applic, V18, P1
   Li XR, 2019, IEEE T MULTIMEDIA, V21, P2347, DOI 10.1109/TMM.2019.2896494
   Li XR, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P271, DOI 10.1145/2911996.2912049
   Li YA, 2022, PROC CVPR IEEE, P17969, DOI 10.1109/CVPR52688.2022.01746
   Li Y, 2022, PROC CVPR IEEE, P5206, DOI 10.1109/CVPR52688.2022.00515
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu CY, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3218921
   Liu FL, 2019, IEEE DATA MINING, P439, DOI 10.1109/ICDM.2019.00054
   Liu XH, 2018, LECT NOTES COMPUT SC, V11219, P353, DOI 10.1007/978-3-030-01267-0_21
   Liu Y, 2017, IEEE I CONF COMP VIS, P4127, DOI 10.1109/ICCV.2017.442
   Long QY, 2021, Arxiv, DOI arXiv:2009.09654
   Malla S, 2023, IEEE WINT CONF APPL, P1043, DOI 10.1109/WACV56688.2023.00110
   Miyazaki T, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1780
   Nakayama H, 2017, MACH TRANSL, V31, P49, DOI 10.1007/s10590-017-9197-z
   Nie WZ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4510, DOI 10.1145/3474085.3475604
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Ranzato M, 2016, Arxiv, DOI [arXiv:1511.06732, 10.48550/arXiv.1511.06732]
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Shao Z, 2023, IEEE T MULTIMEDIA, V25, P8753, DOI 10.1109/TMM.2023.3241517
   Shizhe Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9959, DOI 10.1109/CVPR42600.2020.00998
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Song PP, 2023, IEEE T CYBERNETICS, V53, P4388, DOI 10.1109/TCYB.2022.3175012
   Song YQ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P784, DOI 10.1145/3343031.3350996
   Su YH, 2019, PROC CVPR IEEE, P10474, DOI 10.1109/CVPR.2019.01073
   Sutton RS, 2000, ADV NEUR IN, V12, P1057
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang S, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1483, DOI 10.1145/3240508.3240671
   Wu JH, 2019, IEEE INT CON MULTI, P1480, DOI 10.1109/ICME.2019.00256
   Wu SY, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P975
   Xu N, 2020, IEEE T MULTIMEDIA, V22, P1372, DOI 10.1109/TMM.2019.2941820
   Xu Z, 2022, IEEE T IMAGE PROCESS, V31, P5178, DOI 10.1109/TIP.2022.3191841
   Yang X, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1, DOI 10.1145/3404835.3462823
   Yang X, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1939, DOI 10.1145/3394171.3413610
   Yang X, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P1339, DOI 10.1145/3397271.3401151
   Ye Junjie, 2022, P 29 INT C COMPUTATI, P5098
   Zeng Y, 2023, Arxiv, DOI arXiv:2206.00621
   Zhang B, 2018, ICMLC 2020: 2020 12TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND COMPUTING, P426, DOI 10.1145/3383972.3384072
   Zhang MR, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3502092
   Zhou YAN, 2020, PROC CVPR IEEE, P4776, DOI 10.1109/CVPR42600.2020.00483
   Zhu PP, 2023, IEEE T MULTIMEDIA, V25, P6702, DOI 10.1109/TMM.2022.3214090
NR 72
TC 0
Z9 0
U1 4
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2024
VL 20
IS 4
AR 110
DI 10.1145/3634917
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6G9
UT WOS:001153382100020
DA 2024-08-05
ER

PT J
AU Feng, Y
   Hu, YJ
   Fang, PF
   Liu, S
   Yang, YH
   Chen, SY
AF Feng, Yuan
   Hu, Yaojun
   Fang, Pengfei
   Liu, Sheng
   Yang, Yanhong
   Chen, Shengyong
TI Asymmetric Dual-Decoder U-Net for Joint Rain and Haze Removal
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Joint rain and haze removal; asymmetric dual-decoder U-Net (ADU-Net);
   contamination residual; scene residual
ID IMAGE; NETWORK; STREAKS
AB This work studies the multi-weather restoration problem. In real-life scenarios, rain and haze, two often co-occurring common weather phenomena, can greatly degrade the clarity and quality of the scene images, leading to a performance drop in the visual applications, such as autonomous driving. However, jointly removing the rain and haze in scene images is ill-posed and challenging, where the existence of haze and rain and the change of atmosphere light, can both degrade the scene information. Current methods focus on the contamination removal part, thus ignoring the restoration of the scene information affected by the change of atmospheric light. We propose a novel deep neural network, named Asymmetric Dual-decoder U-Net (ADU-Net), to address the aforementioned challenge. The ADU-Net produces both the contamination residual and the scene residual to efficiently remove the contamination while preserving the fidelity of the scene information. Extensive experiments show our work outperforms the existing state-of-the-art methods by a considerable margin in both synthetic data and real-world data benchmarks, including RainCityscapes, BID Rain, and SPA-Data. For instance, we improve the state-of-the-art PSNR value by 2.26/4.57 on the RainCityscapes/SPA-Data, respectively. Codes will be made available freely to the research community.
C1 [Feng, Yuan] Zhejiang Univ Technol, Coll Sci, 288 Liuhe Rd, Hangzhou 310023, Zhejiang, Peoples R China.
   [Hu, Yaojun] Zhejiang Univ, Coll Comp Sci & Technol, 866 Yuhangtang Rd, Hangzhou 310058, Zhejiang, Peoples R China.
   [Fang, Pengfei] Southeast Univ, Sch Comp Sci & Engn, 2 Southeast Univ Rd, Nanjing 210096, Jiangsu, Peoples R China.
   [Liu, Sheng] Zhejiang Univ Technol, Coll Comp Sci, 288 Liuhe Rd, Hangzhou 310023, Zhejiang, Peoples R China.
   [Yang, Yanhong; Chen, Shengyong] Tianjin Univ Technol, Coll Comp Sci & Technol, 391 Binshui West Rd, Tianjin 300384, Peoples R China.
C3 Zhejiang University of Technology; Zhejiang University; Southeast
   University - China; Zhejiang University of Technology; Tianjin
   University of Technology
RP Fang, PF (corresponding author), Southeast Univ, Sch Comp Sci & Engn, 2 Southeast Univ Rd, Nanjing 210096, Jiangsu, Peoples R China.
EM fy@ieee.org; yaojunhu@zju.edu.cn; fangpengfei@seu.edu.cn;
   edliu@zjut.edu.cn; yyh_03@163.com; sy@ieee.org
OI Feng, Yuan/0000-0003-2563-8724
FU Natural Science Foundation of Zhejiang Province [LGG21F030011]; National
   Natural Science Foundation of Chinander [62306070]; Southeast University
   Start-Up Grant for New Faculty [4009002309]; Big Data Computing Center
   of Southeast University
FX This work is supported by the Natural Science Foundation of Zhejiang
   Province under Grant No. LGG21F030011 and the National Natural Science
   Foundation of Chinander Grant No. 61972355. Thiswork is also supported
   by the National Natural Science Foundation of Chinander Grant No.
   62306070 and by the Southeast University Start-Up Grant for New Faculty
   under Grant No. 4009002309. Furthermore, the work is also supported by
   the Big Data Computing Center of Southeast University.
CR Agarap A.F., 2018, arXiv, DOI 10.48550/arXiv.1803.08375
   Ancuti CO, 2020, IEEE COMPUT SOC CONF, P1798, DOI 10.1109/CVPRW50498.2020.00230
   Ba YH, 2022, LECT NOTES COMPUT SC, V13667, P723, DOI 10.1007/978-3-031-20071-7_42
   Blinn J. F., 1982, Computer Graphics, V16, DOI 10.1145/965145.801290
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chen CH, 2021, PROC CVPR IEEE, P7738, DOI 10.1109/CVPR46437.2021.00765
   Chen LC, 2017, Arxiv, DOI [arXiv:1706.05587, 10.48550/arXiv.1706.05587, DOI 10.48550/ARXIV.1706.05587]
   Chen L, 2019, IEEE T IMAGE PROCESS, V28, P4883, DOI 10.1109/TIP.2019.2913079
   Chen WT, 2022, PROC CVPR IEEE, P17632, DOI 10.1109/CVPR52688.2022.01713
   Chen WT, 2019, PROC CVPR IEEE, P11673, DOI 10.1109/CVPR.2019.01195
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dong H, 2020, PROC CVPR IEEE, P2154, DOI 10.1109/CVPR42600.2020.00223
   Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552
   Fan ZW, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1751, DOI 10.1145/3240508.3240694
   Fattal R, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2651362
   Fu XY, 2017, PROC CVPR IEEE, P1715, DOI 10.1109/CVPR.2017.186
   Garg K, 2007, INT J COMPUT VISION, V75, P3, DOI 10.1007/s11263-006-0028-6
   Han JL, 2022, LECT NOTES COMPUT SC, V13678, P218, DOI 10.1007/978-3-031-19797-0_13
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Hu XW, 2021, IEEE T IMAGE PROCESS, V30, P1759, DOI 10.1109/TIP.2020.3048625
   Hu XW, 2019, PROC CVPR IEEE, P8014, DOI 10.1109/CVPR.2019.00821
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Kang LW, 2012, IEEE T IMAGE PROCESS, V21, P1742, DOI 10.1109/TIP.2011.2179057
   Kim DH, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11062873
   Kulkarni A, 2022, IEEE T INTELL TRANSP, V23, P24488, DOI 10.1109/TITS.2022.3208372
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li G, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1056, DOI 10.1145/3240508.3240636
   Li RT, 2020, PROC CVPR IEEE, P3172, DOI 10.1109/CVPR42600.2020.00324
   Li X, 2018, LECT NOTES COMPUT SC, V11211, P262, DOI 10.1007/978-3-030-01234-2_16
   Li Y, 2016, PROC CVPR IEEE, P2736, DOI 10.1109/CVPR.2016.299
   Liang Chuping, 2021, Journal of Zhejiang University (Science Edition), P270, DOI 10.3785/j.issn.1008-9497.2021.03.002
   Liu X, 2019, PROC CVPR IEEE, P7000, DOI 10.1109/CVPR.2019.00717
   Liu YF, 2018, IEEE T IMAGE PROCESS, V27, P3064, DOI 10.1109/TIP.2018.2806202
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Luo Y, 2015, IEEE I CONF COMP VIS, P3397, DOI 10.1109/ICCV.2015.388
   Paszke A, 2019, ADV NEUR IN, V32
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Qu YY, 2019, PROC CVPR IEEE, P8152, DOI 10.1109/CVPR.2019.00835
   Ren DW, 2019, PROC CVPR IEEE, P3932, DOI 10.1109/CVPR.2019.00406
   Ren WQ, 2020, INT J COMPUT VISION, V128, P240, DOI 10.1007/s11263-019-01235-8
   Ren WQ, 2018, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2018.00343
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Sakaridis C, 2018, INT J COMPUT VISION, V126, P973, DOI 10.1007/s11263-018-1072-8
   Song YD, 2023, IEEE T IMAGE PROCESS, V32, P1927, DOI 10.1109/TIP.2023.3256763
   Sun ZY, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3478457
   Valanarasu JMJ, 2022, PROC CVPR IEEE, P2343, DOI 10.1109/CVPR52688.2022.00239
   Wang H, 2020, PROC CVPR IEEE, P3100, DOI 10.1109/CVPR42600.2020.00317
   Wang TY, 2019, PROC CVPR IEEE, P12262, DOI 10.1109/CVPR.2019.01255
   Wang Y, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3197546
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu HY, 2021, PROC CVPR IEEE, P10546, DOI 10.1109/CVPR46437.2021.01041
   Yang WH, 2017, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2017.183
   Yeh CH, 2020, IEEE T IMAGE PROCESS, V29, P3153, DOI 10.1109/TIP.2019.2957929
   Yu Y, 2022, PROC CVPR IEEE, P6003, DOI 10.1109/CVPR52688.2022.00592
   Zamir SW, 2021, PROC CVPR IEEE, P14816, DOI 10.1109/CVPR46437.2021.01458
   Zhang H, 2019, PROC CVPR IEEE, P548, DOI 10.1109/CVPR.2019.00064
   Zhang H, 2020, IEEE T CIRC SYST VID, V30, P3943, DOI 10.1109/TCSVT.2019.2920407
   Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337
   Zhang H, 2018, PROC CVPR IEEE, P695, DOI 10.1109/CVPR.2018.00079
   Zhu L, 2021, IEEE T CIRC SYST VID, V31, P2147, DOI 10.1109/TCSVT.2020.3022707
   Zhu L, 2017, IEEE I CONF COMP VIS, P2545, DOI 10.1109/ICCV.2017.276
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 63
TC 0
Z9 0
U1 14
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2024
VL 20
IS 3
AR 87
DI 10.1145/3628451
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6F8
UT WOS:001153381000027
OA hybrid, Green Submitted
DA 2024-08-05
ER

PT J
AU Deb, S
   Das, A
   Kar, N
AF Deb, Subhrajyoti
   Das, Abhilash
   Kar, Nirmalya
TI An Applied Image Cryptosystem on Moore's Automaton Operating on
   δ(<i>q<sub>k</sub></i>)/F<sub>2</sub>
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image encryption; Moore's Automaton; escalation; diffusion; security;
   cyclic group; encryption; decryption
ID ENCRYPTION; CIPHER; CHAOS; CRYPTANALYSIS
AB The volume of multimedia-based image data or video frames in Web 3.0 is constantly increasing, owing to the advancement of real-time data transmission. However, security vulnerabilities frequently impair the performance of real-time applications. Many researchers have recently proposed image encryption schemes based on a high-dimensional chaotic system due to properties such as ergodicity and initial state sensitivity. Nonetheless, most schemes have suffered from excessive computational complexity, low security, and the generation of cryptographically secure random numbers. To overcome these challenges, an efficient and highly secure cryptosystem is necessary for safe multimedia transmission in Web 3.0. This article proposes a novel work on the image cryptosystem based on the Escalation function with a one-time key-oriented Moore's Automaton over a finite field F-2. The Escalation function is a nonlinear scrambling technique for plaintext images that goes through the confusion phase and plays an essential role in row-column permutation. To make the algorithm more secure and robust in the diffusion phase, the proposed Moore's Automaton produced ciphertext images through a highly random key stream generated by the combination of a logistic map and cyclic group. Specifically, the proposed Moore's Automaton operates on delta(q(k))/F-2 to render random binary bits into unpredictable sequences to construct ciphertext images. Our new finding quickens the speed and provides adequate key space, and pixel distributions are more uniform, have high entropy value, and are secure against differential and statistical attacks.
C1 [Deb, Subhrajyoti] ICFAI Univ Tripura, Dept Comp Sci & Engn, Kamalghat 799210, Tripura, India.
   [Das, Abhilash] Indian Inst Technol, Dept Comp Sci & Engn, Jammu 181221, India.
   [Kar, Nirmalya] Natl Inst Technol Agartala, Dept Comp Sci & Engn, Agartala, Tripura, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) Jammu; National Institute of Technology (NIT System);
   National Institute of Technology Agartala
RP Deb, S (corresponding author), ICFAI Univ Tripura, Dept Comp Sci & Engn, Kamalghat 799210, Tripura, India.
EM subhrajyotideb1@gmail.com; dasgate77@gmail.com; nirmalya@ieee.org
RI Kar, Nirmalya/J-7946-2018
OI Kar, Nirmalya/0000-0002-7371-232X; Deb, Subhrajyoti/0000-0001-6939-0113
CR Abd-El-Atty B, 2021, OPT LASER ENG, V138, DOI 10.1016/j.optlaseng.2020.106403
   Alawida M, 2019, SIGNAL PROCESS, V164, P249, DOI 10.1016/j.sigpro.2019.06.013
   Alexan Wassim, 2021, 2021 International Conference on Microelectronics (ICM), P34, DOI 10.1109/ICM52667.2021.9664961
   Alexan W, 2022, SYMMETRY-BASEL, V14, DOI 10.3390/sym14030443
   Aparna H, 2021, J INF SECUR APPL, V63, DOI 10.1016/j.jisa.2021.102972
   Babaei A, 2020, OPTIK, V203, DOI 10.1016/j.ijleo.2019.164000
   Barker Elaine., 2018, Transitioning the use of cryptographic algorithms and key lengths
   Belazi A, 2022, J INF SECUR APPL, V66, DOI 10.1016/j.jisa.2022.103131
   Belazi A, 2016, SIGNAL PROCESS, V128, P155, DOI 10.1016/j.sigpro.2016.03.021
   Borghoff J, 2012, LECT NOTES COMPUT SC, V7658, P208, DOI 10.1007/978-3-642-34961-4_14
   Chai XL, 2022, INFORM SCIENCES, V604, P115, DOI 10.1016/j.ins.2022.05.008
   Chai XL, 2022, IEEE SIGNAL PROC LET, V29, P972, DOI 10.1109/LSP.2022.3163685
   Chai XL, 2021, NEURAL COMPUT APPL, V33, P10371, DOI 10.1007/s00521-021-05797-y
   Chen L, 2015, COMPUT BIOL MED, V65, P69, DOI 10.1016/j.compbiomed.2015.07.024
   Das A, 2022, J AMB INTEL HUM COMP, V13, P1021, DOI 10.1007/s12652-021-02943-1
   Deb S, 2022, INT J AD HOC UBIQ CO, V41, P118, DOI 10.1504/IJAHUC.2022.125428
   Deb S, 2022, OPTIK, V253, DOI 10.1016/j.ijleo.2021.168548
   Deb S, 2021, MULTIMED TOOLS APPL, V80, P19803, DOI 10.1007/s11042-020-10308-7
   Deb S, 2019, MULTIMED TOOLS APPL, V78, P34901, DOI 10.1007/s11042-019-08086-y
   Dong YH, 2022, INFORM SCIENCES, V593, P121, DOI 10.1016/j.ins.2022.01.031
   Abd El-Latif AA, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10121392
   Hosny KM, 2022, MULTIMED TOOLS APPL, V81, P505, DOI 10.1007/s11042-021-11384-z
   Hua ZY, 2017, INFORM SCIENCES, V396, P97, DOI 10.1016/j.ins.2017.02.036
   Jin J, 2012, OPT LASER ENG, V50, P1836, DOI 10.1016/j.optlaseng.2012.06.002
   Kang SW, 2022, MULTIMED TOOLS APPL, V81, P1209, DOI 10.1007/s11042-021-11424-8
   Kar N, 2018, ICT EXPRESS, V4, P6, DOI 10.1016/j.icte.2018.01.003
   Krishna Gandhi B., 2011, International Journal of Computer Applications, V6, P61
   Lai Q, 2023, IEEE T NEUR NET LEAR, V34, P7824, DOI 10.1109/TNNLS.2022.3146570
   Lathey A, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2656205
   Li CQ, 2013, INT J BIFURCAT CHAOS, V23, DOI 10.1142/S0218127413500752
   Li YH, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2903717
   Liu WH, 2016, OPT LASER ENG, V84, P26, DOI 10.1016/j.optlaseng.2016.03.019
   Ma YL, 2020, J INF SECUR APPL, V54, DOI 10.1016/j.jisa.2020.102566
   Del Rey AM, 2015, INT J MOD PHYS C, V26, DOI 10.1142/S0129183114500697
   Moore E.F., 1956, Automata Studies, P129
   Nestor T, 2022, SYMMETRY-BASEL, V14, DOI 10.3390/sym14020424
   Niyat AY, 2017, OPT LASER ENG, V90, P225, DOI [10.1016/j.optlaseng.2016.10:019, 10.1016/j.optlaseng.2016.10.019]
   Panwar K, 2018, J ELECTRON IMAGING, V27, DOI 10.1117/1.JEI.27.5.053037
   Paul A, 2022, APPL INTELL, V52, P10979, DOI 10.1007/s10489-021-03063-1
   Pavithran P, 2021, COMPUT SECUR, V104, DOI 10.1016/j.cose.2020.102160
   Ping P, 2022, MULTIMED TOOLS APPL, V81, P7323, DOI 10.1007/s11042-021-11799-8
   Praveenkumar P, 2018, MULTIMED TOOLS APPL, V77, P8393, DOI 10.1007/s11042-017-4741-7
   Roy S, 2021, J INF SECUR APPL, V61, DOI 10.1016/j.jisa.2021.102919
   Sankpal PR, 2014, 2014 FIFTH INTERNATIONAL CONFERENCE ON SIGNAL AND IMAGE PROCESSING (ICSIP 2014), P102, DOI 10.1109/ICSIP.2014.80
   Sen Teh J, 2020, J INF SECUR APPL, V50, DOI 10.1016/j.jisa.2019.102421
   Shahna K, 2020, APPL SOFT COMPUT, V90, DOI 10.1016/j.asoc.2020.106162
   Shivani S, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2935618
   Singh KN, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3498342
   Soreng AV, 2022, J VIS COMMUN IMAGE R, V84, DOI 10.1016/j.jvcir.2022.103466
   Tralic D, 2016, RADIOENGINEERING, V25, P548, DOI 10.13164/re.2016.0548
   Tu GY, 2013, OPTIK, V124, P5411, DOI 10.1016/j.ijleo.2013.03.113
   Wang CF, 2018, COMPLEXITY, DOI 10.1155/2018/9818520
   Wang Y, 2018, NEUROCOMPUTING, V275, P1318, DOI 10.1016/j.neucom.2017.09.068
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu Y., 2011, Journal of Selected Areas in Telecommunications (JSAT), V1, P31
   Yu F, 2021, INFORM SCIENCES, V554, P145, DOI 10.1016/j.ins.2020.12.037
   Zhang XP, 2014, NONLINEAR DYNAM, V75, P319, DOI 10.1007/s11071-013-1068-4
   Zia U, 2022, INT J INF SECUR, V21, P917, DOI 10.1007/s10207-022-00588-5
NR 58
TC 0
Z9 0
U1 1
U2 7
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2024
VL 20
IS 2
SI SI
AR 52
DI 10.1145/3614433
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W6GR4
UT WOS:001092595800022
DA 2024-08-05
ER

PT J
AU Loveleen, G
   Mohan, B
   Shikhar, BS
   Nz, J
   Shorfuzzaman, M
   Masud, M
AF Loveleen, Gaur
   Mohan, Bhandari
   Shikhar, Bhadwal Singh
   Nz, Jhanjhi
   Shorfuzzaman, Mohammad
   Masud, Mehedi
TI Explanation-Driven HCI Model to Examine the Mini-Mental State for
   Alzheimer's Disease
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Human computer interface; Explainable AI; deep learning; machine
   learning; Alzheimer's prediction; SHAP; LIME
AB Directing research on Alzheimer's disease toward only early prediction and accuracy cannot be considered a feasible approach toward tackling a ubiquitous degenerative disease today. Applying deep learning (DL), Explainable artificial intelligence, and advancing toward the human-computer interface (HCI) model can be a leap forward in medical research. This research aims to propose a robust explainable HCI model using SHAPley additive explanation, local interpretable model-agnostic explanations, and DL algorithms. The use of DL algorithms-logistic regression (80.87%), support vector machine (85.8%), k-nearest neighbor (87.24%), multilayer perceptron (91.94%), and decision tree (100%)-and explainability can help in exploring untapped avenues for research in medical sciences that can mold the future of HCI models. The presented model's results show improved prediction accuracy by incorporating a user-friendly computer interface into decision-making, implying a high significance level in the context of biomedical and clinical research.
C1 [Loveleen, Gaur; Shikhar, Bhadwal Singh] Amity Univ, Amity Int Business Sch, Sect 125, Noida 201305, Uttar Pradesh, India.
   [Mohan, Bhandari] Samriddhi Coll, Bhaktapur 44800, Bagmati, Nepal.
   [Nz, Jhanjhi] Taylors Univ, Sch Comp Sci, Sect 125, Subang Jaya 47500, Selangor, Malaysia.
   [Shorfuzzaman, Mohammad; Masud, Mehedi] Taif Univ, Dept Comp Sci, Coll Comput & Informat Technol, Mecca 21944, Saudi Arabia.
C3 Amity University Noida; Taylor's University; Taif University
RP Masud, M (corresponding author), Taif Univ, Dept Comp Sci, Coll Comput & Informat Technol, Mecca 21944, Saudi Arabia.
EM gaurloveleen@yahoo.com; mail2mohanbhandari@gmail.com;
   shikharbhadwal9@gmail.com; noorzaman.jhanjhi@taylors.edu.my;
   m.shorf@tu.edu.sa; mmasud@tu.edu.sa
RI Masud, Mehedi/AAZ-7022-2020; Jhanjhi, Prof Dr Noor Zaman/F-3051-2011;
   Bhandari, Mohan/KVC-3085-2024; Gaur, Loveleen/ABB-9664-2020
OI Masud, Mehedi/0000-0001-6019-7245; Jhanjhi, Prof Dr Noor
   Zaman/0000-0001-8116-4733; Gaur, Loveleen/0000-0002-0885-1550; Bhandari,
   Mohan/0000-0003-2551-3163
FU Taif University Researchers Supporting Project, Taif University, Taif,
   Saudi Arabia [TURSP-2020/79]
FX This work was supported by the Taif University Researchers Supporting
   Project number (TURSP-2020/79), Taif University, Taif, Saudi Arabia.
CR Ahmad Irfan, 2020, Mater. Today Proc, DOI [10.1016/j.matpr.2020.09.625, DOI 10.1016/J.MATPR.2020.09.625]
   Alhamid MF, 2016, IEEE T HUM-MACH SYST, V46, P615, DOI 10.1109/THMS.2015.2509965
   Antor MB, 2021, J HEALTHC ENG, V2021, DOI 10.1155/2021/9917919
   Carrasco E, 2008, LECT NOTES COMPUT SC, V5105, P38, DOI 10.1007/978-3-540-70540-6_5
   Essemlali Achraf, 2020, P 3 C MED IM DEEP LE
   Gao Y, 2019, COGN SYST RES, V56, P192, DOI 10.1016/j.cogsys.2018.12.006
   Holzinger A, 2021, INFORM FUSION, V71, P28, DOI 10.1016/j.inffus.2021.01.008
   Hossain MS, 2020, IEEE NETWORK, V34, P120, DOI 10.1109/MNET.011.2000064
   Hossain MS, 2020, IEEE NETWORK, V34, P126, DOI 10.1109/MNET.011.2000458
   Hossain MS, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3241056
   Hossain MS, 2019, INFORM FUSION, V49, P69, DOI 10.1016/j.inffus.2018.09.008
   Hossain MS, 2018, IEEE INTERNET THINGS, V5, P2399, DOI 10.1109/JIOT.2017.2772959
   Hossain MS, 2017, IEEE SYST J, V11, P118, DOI 10.1109/JSYST.2015.2470644
   Janghel RR, 2021, IRBM, V42, P258, DOI 10.1016/j.irbm.2020.06.006
   Jin MW, 2018, J NEUROSCI METH, V302, P35, DOI 10.1016/j.jneumeth.2018.02.014
   Kamal MS, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3107056
   Kautzky A, 2018, FRONT AGING NEUROSCI, V10, DOI 10.3389/fnagi.2018.00406
   Klymentiev Ruslan, 2018, Dementia Prediction w/ Tree-Based Models
   Kuang J, 2021, GERIATR GERONTOL INT, V21, P43, DOI 10.1111/ggi.14097
   Liu L, 2020, SIMUL MODEL PRACT TH, V99, DOI 10.1016/j.simpat.2019.102023
   Liu Y, 2021, IEEE INTERNET THINGS, V8, P6348, DOI 10.1109/JIOT.2020.3011726
   Lynch C., Alzheimers Dement, V2020, pe038255, DOI [10.1002/alz.038255, DOI 10.1002/ALZ.038255]
   Mandiliotis Dimitris, 2013, Universal Access in Human-Computer Interaction. User and Context Diversity. 7th International Conference, UAHCI 2013 Held as Part of HCI International 2013. Proceedings. LNCS 8010, P123, DOI 10.1007/978-3-642-39191-0_14
   Mehta RI, 2021, CURR OPIN NEUROL, V34, P237, DOI 10.1097/WCO.0000000000000912
   Mofrad RB, 2019, ALZH DEMENT-DADM, V11, P1, DOI 10.1016/j.dadm.2018.10.004
   Muhammad G, 2021, INFORM FUSION, V72, P80, DOI 10.1016/j.inffus.2021.02.013
   Muhammad G, 2021, IEEE J SEL AREA COMM, V39, P603, DOI 10.1109/JSAC.2020.3020654
   Mukhopadhyay S, 2021, J ALZHEIMERS DIS, V83, P1537, DOI 10.3233/JAD-215065
   Palmqvist S, 2021, NAT MED, V27, P1034, DOI 10.1038/s41591-021-01348-z
   Prince Master, 2019, HCI Domain., V5, P2
   Qian SS, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2659521
   Qiu Y, 2019, IEEE T MULTIMEDIA, V21, P1778, DOI 10.1109/TMM.2018.2883866
   Rahman MA, 2021, IEEE INTERNET THINGS, V8, P9603, DOI 10.1109/JIOT.2020.3013710
   Rahman MA, 2020, IEEE NETWORK, V34, P98, DOI 10.1109/MNET.011.2000353
   Shorfuzzaman M, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3469841
   Shorfuzzaman M, 2021, SUSTAIN CITIES SOC, V64, DOI 10.1016/j.scs.2020.102582
   Shorfuzzaman M, 2021, PATTERN RECOGN, V113, DOI 10.1016/j.patcog.2020.107700
   Smith MQRP, 2020, BEHAV ECOL SOCIOBIOL, V74, DOI 10.1007/s00265-020-02916-y
   Srivastava S, 2021, EUR J MED CHEM, V216, DOI 10.1016/j.ejmech.2021.113320
   Vásquez-Morales GR, 2019, IEEE ACCESS, V7, P152900, DOI 10.1109/ACCESS.2019.2948430
   Wang N, 2021, Arxiv, DOI arXiv:2006.14135
   Xiao RY, 2021, BIOMED SIGNAL PROCES, V66, DOI 10.1016/j.bspc.2020.102362
   Zhang W, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3279952
NR 43
TC 14
Z9 14
U1 29
U2 37
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2024
VL 20
IS 2
SI SI
AR 41
DI 10.1145/3527174
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W6GR4
UT WOS:001092595800011
HC Y
HP N
DA 2024-08-05
ER

PT J
AU Jeong, JB
   Lee, S
   Ryu, ES
AF Jeong, Jong-Beom
   Lee, Soonbin
   Ryu, Eun-Seok
TI DATRA-MIV: Decoder-adaptive Tiling and Rate Allocation for MPEG
   Immersive Video
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Spatial computing; six degrees of freedom; MPEG immersive video; frame
   packing
AB The emerging immersive video coding standard moving picture experts group (MPEG) immersive video (MIV), which is ongoing standardization by MPEG-Immersive (MPEG-I) group, enables six degrees of freedom in a virtual reality environment that represents both natural and computer-generated scenes using multi-view video compression. The MIV eliminates the redundancy between multi-view videos and merges the residuals into multiple pictures, called an atlas. Thus, bitstreams with encoded atlases are generated and corresponding number of decoders are needed, which is challenging for the lightweight device with a single decoder. This article proposes a decoder-adaptive tiling and rate allocation method for MIV to overcome the challenge. First, the proposed method divides atlases into subpictures considering two aspects: (i) subpicture bitstream extracting and merging into one bitstream to use a single decoder and (ii) separation of each source view from the atlases for rate allocation. Second, the atlases are encoded by versatile video coding (VVC), using an extractable subpicture to divide the atlases into subpictures. Third, each subpicture bitstream is extracted, and asymmetric quality allocation for each subpictures is conducted by considering the residuals in the subpicture. Fourth, mixed-quality subpictures were merged by using the proposed bitstream merger. Fifth, the merged bitstream is decoded by using a single decoder. Finally, the viewing area of the user is synthesized by using the reconstructed atlases. Experimental results with the VVC test model (VTM) show that the proposed method achieves a 21.37% Bjontegaard delta rate saving for immersive video peak signal-to-noise ratio and a 26.76% decoding runtime saving compared to the VTM anchor configuration. Moreover, it supports bitstreams for multiple decoders and single decoderwithout re-encoding, transcoding, or a substantial increase of the server-side storage.
C1 [Jeong, Jong-Beom; Ryu, Eun-Seok] Sungkyunkwan Univ SKKU, 25-2 Seonggyungwan Ro, Seoul 03063, South Korea.
   [Lee, Soonbin] Fraunhofer Heinrich Hertz Inst HHI, Salzufer 15-16, D-10587 Berlin, Germany.
C3 Sungkyunkwan University (SKKU)
RP Jeong, JB (corresponding author), Sungkyunkwan Univ SKKU, 25-2 Seonggyungwan Ro, Seoul 03063, South Korea.
EM uof4949@skku.edu; soonbin.lee@hhi.fraunhofer.de; esryu@skku.edu
OI Lee, Soonbin/0000-0002-8951-0335; Jeong, Jong-Beom/0000-0002-7356-5753;
   Ryu, Eun-Seok/0000-0003-4894-6105
FU Institute of Information & Communications Technology Planning &
   Evaluation (IITP) - Korea government (MSIT) [RS2023-00254129,
   RS-2023-00167169]
FX This work was supported by Institute of Information & Communications
   Technology Planning & Evaluation (IITP) grant funded by the Korea
   government (MSIT) (Grant No. RS-2023-00167169, Development of Moving
   Robot-based Immersive Video Acquisition and Processing System in
   Metaverse). This work was also supported by Institute of Information &
   Communications Technology Planning & Evaluation (IITP) grant funded by
   the Korea government (MSIT) (Grant No. RS2023-00254129, Graduate School
   of Metaverse Convergence (Sungkyunkwan University)).
CR [Anonymous], 2019, Standard ISO/IEC JTC1/SC29/WG11, MPEG/n18438
   [Anonymous], 2021, Standard ISO/IEC JTC1/SC29/WG3, MPEG/n0420
   BinWang Yule Sun, 2019, P 126 MPEG M ISO IEC
   Bross B, 2021, IEEE T CIRC SYST VID, V31, P3736, DOI 10.1109/TCSVT.2021.3101953
   Choi B., 2017, ISO/IEC, P23090
   Domanski Marek, 2019, P 126 MPEG M ISO IEC
   Dziembowski A, 2022, IEEE T CIRC SYST VID, V32, P7575, DOI 10.1109/TCSVT.2022.3179575
   Fleureau Julien, 2019, P 126 MPEG M ISO IEC
   Hannuksela MM, 2015, IEEE IMAGE PROC, P2154
   Hillmann Cornel., 2019, Unreal for Mobile and Standalone VR Create Professional VR Apps Without Coding, VFirst
   ISO/IEC JTC1/SC29/WG11, 2019, P 125 MPEG M ISO IEC
   Jeong JB, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3687, DOI 10.1145/3394171.3413712
   Jeong JongBeom, 2021, [Journal of Internet Computing and Services, 인터넷정보학회논문지], V22, P51
   Jung Joel, 2020, Standard ISO/IEC JTC1/SC29/WG4, MPEG/n0051
   Jung Joel, 2021, Standard ISO/IEC JTC1/SC29/WG4, MPEG/n0055
   Kondrad Lukasz, 2020, Standard ISO/IEC JTC1/SC29/WG11, MPEG/m54274
   Kroon Bart, 2019, P 126 MPEG M ISO IEC
   Lee Soonbin, 2020, Standard ISO/IEC JTC1/SC29/WG11, MPEG/m55014
   Lim Young-Kwon, 2005, Standard ISO/IEC JTC1/SC29/WG11, MPEG/n7504
   Saha A, 2023, J REAL-TIME IMAGE PR, V20, DOI 10.1007/s11554-023-01376-7
   Salahieh Basel, 2021, Standard ISO/IEC JTC1/SC29/WG4, MPEG/n0050
   Skupin Robert, 2018, P 122 MPEG M ISO IE
   Son J, 2018, PROCEEDINGS OF THE 28TH ACM WORKSHOP ON NETWORK AND OPERATING SYSTEMS SUPPORT FOR DIGITAL AUDIO AND VIDEO (NOSSDAV'18), P61, DOI 10.1145/3210445.3210455
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sun YL, 2017, IEEE SIGNAL PROC LET, V24, P1408, DOI 10.1109/LSP.2017.2720693
   Le TT, 2021, CMC-COMPUT MATER CON, V66, P2627, DOI 10.32604/cmc.2021.013399
   Thudor F., 2021, Standard ISO/IEC JTC1/SC29/WG4, MPEG/m56727
   Vadakital Vinod Kumar Malamal, 2019, P 126 MPEG M ISO IEC
   VVC Test Model (VTM) Repository, 2020, VTM 11.0
   Wang Bin, 2018, P 123 MPEG M ISO IEC
   Wieckowski A, 2021, IEEE INT CONF MULTI, DOI 10.1109/ICMEW53276.2021.9455944
   Wieckowski A, 2020, IEEE IMAGE PROC, P3124, DOI 10.1109/ICIP40778.2020.9191199
NR 32
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2024
VL 20
IS 7
SI SI
AR 207
DI 10.1145/3648371
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL0Q6
UT WOS:001234494100022
OA hybrid
DA 2024-08-05
ER

PT J
AU Zhang, H
   Liu, M
   Qi, Y
   Yang, N
   Hu, SB
   Nie, LQ
   Zhang, WY
AF Zhang, Hao
   Liu, Meng
   Qi, Yuan
   Yang, Ning
   Hu, Shunbo
   Nie, Liqiang
   Zhang, Wenyin
TI Efficient Brain Tumor Segmentation with Lightweight Separable Spatial
   Convolutional Network
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Brain Tumor Segmentation; Separable Spatial Convolutional Network
ID MEDICAL IMAGE SEGMENTATION; MODEL
AB Accurate and automated segmentation of lesions in brain MRI scans is crucial in diagnostics and treatment planning. Despite the significant achievements of existing approaches, they often require substantial computational resources and fail to fully exploit the synergy between low-level and high-level features. To address these challenges, we introduce the Separable Spatial Convolutional Network (SSCN), an innovative model that refines the U-Net architecture to achieve efficient brain tumor segmentation with minimal computational cost. SSCN integrates the PocketNet paradigm and replaces standard convolutions with depthwise separable convolutions, resulting in a significant reduction in parameters and computational load. Additionally, our feature complementary module enhances the interaction between features across the encoder-decoder structure, facilitating the integration of multi-scale features while maintaining low computational demands. The model also incorporates a separable spatial attention mechanism, enhancing its capability to discern spatial details. Empirical validations on standard datasets demonstrate the effectiveness of our proposed model, especially in segmenting small and medium-sized tumors, with only 0.27M parameters and 3.68 GFlops. Our code is available at https://github.com/zzpr/SSCN.
C1 [Zhang, Hao; Qi, Yuan; Hu, Shunbo; Zhang, Wenyin] Linyi Univ, Ind Ave, Linyi 276000, Shandong, Peoples R China.
   [Liu, Meng; Yang, Ning] Shandong Jianzhu Univ, 1000 Fengming Rd, Jinan 250101, Shandong, Peoples R China.
   [Nie, Liqiang] Harbin Inst Technol Shenzhen, Taoyuan St, Taoyuan 518055, Guangdong, Peoples R China.
C3 Linyi University; Shandong Jianzhu University; Harbin Institute of
   Technology
RP Zhang, H (corresponding author), Linyi Univ, Ind Ave, Linyi 276000, Shandong, Peoples R China.
EM haozhang14688@163.com; mengliu.sdu@gmail.com; qyuan1122@163.com;
   ningyang20@sdjzu.edu.cn; hushunbo@lyu.edu.cn; nieliqiang@gmail.com;
   zhangwenyin@lyu.edu.cn
OI Hu, Shunbo/0000-0002-1442-0976; Liu, Meng/0000-0002-1582-5764; Zhang,
   Wenyin/0000-0002-3595-3448
FU National Natural Science Foundation of China [62376140, U23A20315,
   62236003]; Science and Technology Innovation Program for Distinguished
   Young Scholars of Shandong Province Higher Education Institutions
   [2023KJ128]; Natural Science Foundation of Shandong Province
   [ZR202103010201]; Special Fund for distinguished professors of Shandong
   Jianzhu University
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants No. 62376140, No. U23A20315, and No.
   62236003; Science and Technology Innovation Program for Distinguished
   Young Scholars of Shandong Province Higher Education Institutions (Grant
   No. 2023KJ128); the Natural Science Foundation of Shandong Province
   (Grant No. ZR202103010201); and in part by the Special Fund for
   distinguished professors of Shandong Jianzhu University.
CR Behrad F, 2023, EXPERT SYST APPL, V213, DOI 10.1016/j.eswa.2022.118996
   Biswas R, 2020, IJST-T ELECTR ENG, V44, P505, DOI 10.1007/s40998-019-00213-7
   Celaya A, 2023, IEEE T MED IMAGING, V42, P1172, DOI 10.1109/TMI.2022.3224873
   Chen WD, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P4416, DOI 10.1145/3503161.3547761
   Chen Y, 2019, IEEE INT CON MULTI, P508, DOI 10.1109/ICME.2019.00094
   Feng SL, 2020, IEEE T MED IMAGING, V39, P3008, DOI 10.1109/TMI.2020.2983721
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang HM, 2020, INT CONF ACOUST SPEE, P1055, DOI [10.1109/icassp40776.2020.9053405, 10.1109/ICASSP40776.2020.9053405]
   Intisar Rizwan I. Haque, 2020, Informatics in Medicine Unlocked, V18, DOI 10.1016/j.imu.2020.100297
   Jiang ZY, 2020, LECT NOTES COMPUT SC, V11992, P231, DOI 10.1007/978-3-030-46640-4_22
   Kong XM, 2018, IFIP ADV INF COMM TE, V538, P346, DOI 10.1007/978-3-030-00828-4_35
   Kumar A, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3457187
   Lei T, 2023, Arxiv, DOI [arXiv:2306.03373, 10.48550/arXiv.2306.03373]
   Li D, 2019, IEEE IMAGE PROC, P1425, DOI [10.1109/ICIP.2019.8803101, 10.1109/icip.2019.8803101]
   Liu ZH, 2023, COMPLEX INTELL SYST, V9, P1001, DOI 10.1007/s40747-022-00815-5
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lopes AP, 2019, 2019 6TH IEEE PORTUGUESE MEETING IN BIOENGINEERING (ENBENG), DOI 10.1109/enbeng.2019.8692520
   Luo ZR, 2021, IEEE J BIOMED HEALTH, V25, P737, DOI 10.1109/JBHI.2020.2998146
   Menze BH, 2015, IEEE T MED IMAGING, V34, P1993, DOI 10.1109/TMI.2014.2377694
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Oktay O, 2018, Arxiv, DOI [arXiv:1804.03999, DOI 10.48550/ARXIV.1804.03999]
   Pereira S, 2016, IEEE T MED IMAGING, V35, P1240, DOI 10.1109/TMI.2016.2538465
   Punn NS, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3376922
   Qi Y, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app122311980
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Valanarasu JMJ, 2022, LECT NOTES COMPUT SC, V13435, P23, DOI 10.1007/978-3-031-16443-9_3
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang WX, 2021, LECT NOTES COMPUT SC, V12901, P109, DOI 10.1007/978-3-030-87193-2_11
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu Y, 2023, CMES-COMP MODEL ENG, V136, P2465, DOI 10.32604/cmes.2023.026189
   Xie YT, 2021, IEEE T MED IMAGING, V40, P286, DOI 10.1109/TMI.2020.3025308
   Xie YT, 2020, IEEE T MED IMAGING, V39, P2482, DOI 10.1109/TMI.2020.2972964
   Xu SX, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3548459
   Yan ZQ, 2019, IEEE J BIOMED HEALTH, V23, P1427, DOI 10.1109/JBHI.2018.2872813
   Yang L, 2021, NEUROCOMPUTING, V448, P168, DOI 10.1016/j.neucom.2021.03.085
   Yang ZZ, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3446618
   Yu LF, 2021, CMES-COMP MODEL ENG, V129, P805, DOI 10.32604/cmes.2021.017332
   Zeng XH, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3592614
   Zhang RF, 2023, J CLIN MED, V12, DOI 10.3390/jcm12020538
   Zhang ZX, 2018, IEEE GEOSCI REMOTE S, V15, P749, DOI 10.1109/LGRS.2018.2802944
   Zhou CH, 2020, IEEE T IMAGE PROCESS, V29, P4516, DOI 10.1109/TIP.2020.2973510
   Zhou ZW, 2018, LECT NOTES COMPUT SC, V11045, P3, DOI 10.1007/978-3-030-00889-5_1
NR 43
TC 0
Z9 0
U1 4
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2024
VL 20
IS 7
SI SI
AR 229
DI 10.1145/3653715
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL0Q6
UT WOS:001234494100044
OA Bronze
DA 2024-08-05
ER

PT J
AU Zhang, YC
   Ding, DD
   Ma, Z
   Li, Z
AF Zhang, Yichi
   Ding, Dandan
   Ma, Zhan
   Li, Zhu
TI A Reconfigurable Framework for Neural Network Based Video In-Loop
   Filtering
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE In-loop filter; reconfigurable; H.266/VVC; neural model; Transformer
ID SAMPLE ADAPTIVE OFFSET
AB This article proposes a reconfigurable framework for neural network based video in-loop filtering to guide large-scale models for content-aware processing. Specifically, the backbone neural model is decomposed into several convolutional groups and the encoder systematically traverses all candidate configurations combined by these groups to find the best one. The selected configuration index is then encapsulated as side information and passed to the decoder, enabling dynamic model reconfiguration during the decoding stage. The preceding reconfiguration process is only deployed in the inference stage on top of a pre-trained backbone model. Furthermore, we devise WMSPFormer, a wavelet multi-scale Poolformer, as the backbone network structure. WMSPFormer utilizes a wavelet-based multi-scale structure to losslessly decompose the input into multiple scales for spatial-spectral features aggregation. Moreover, it uses multi-scale pooling operations (MSPoolformer) instead of complicated matrix calculations to substitute the attention process. We also extend MSPoolformer to a large-scale version using more parameters, referred to as MSPoolformerExt. Extensive experiments demonstrate that the proposed WMSPFormer+Reconfig. and WMSPFormerExt+Reconfig. achieve a remarkable 7.13% and 7.92% BD-Rate reduction over the anchor H.266/VVC, outperforming most existing methods evaluated under the same training and testing conditions. In addition, the low-complexity nature of the WMSPFormer series makes it attractive for practical applications.
C1 [Zhang, Yichi; Ding, Dandan] Hangzhou Normal Univ, 2318 Yuhangtang Rd, Hangzhou 311121, Zhejiang, Peoples R China.
   [Ma, Zhan] Nanjing Univ, 163 Xianlin Ave, Nanjing 210023, Jiangsu, Peoples R China.
   [Li, Zhu] Univ Missouri Kansas City, 5000 Holmes St, Kansas City, MO 64110 USA.
C3 Hangzhou Normal University; Nanjing University; University of Missouri
   System; University of Missouri Kansas City
RP Ding, DD (corresponding author), Hangzhou Normal Univ, 2318 Yuhangtang Rd, Hangzhou 311121, Zhejiang, Peoples R China.
EM 1ch@acm.org; DandanDing@hznu.edu.cn; mazhan@nju.edu.cn; zhu.li@ieee.org
OI Zhang, Yichi/0009-0006-7136-4997
FU National Natural Science Foundation of China [62171174, U20A20184]
FX This work was supported by the National Natural Science Foundation of
   China (62171174 and U20A20184).
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Bjontegaard G., 2001, P ITU T VID COD EXP
   Bordes P, 2021, PICT COD SYMP, P11, DOI 10.1109/PCS50896.2021.9477457
   Bross B, 2021, IEEE T CIRC SYST VID, V31, P3736, DOI 10.1109/TCSVT.2021.3101953
   Bross B, 2021, P IEEE, V109, P1463, DOI 10.1109/JPROC.2020.3043399
   Cho SJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4621, DOI 10.1109/ICCV48922.2021.00460
   Cui BY, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P3548
   Dai YY, 2017, LECT NOTES COMPUT SC, V10132, P28, DOI 10.1007/978-3-319-51811-4_3
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Ding DD, 2023, IEEE T CIRC SYST VID, V33, P6057, DOI 10.1109/TCSVT.2023.3260266
   Ding DD, 2022, IEEE T IMAGE PROCESS, V31, P773, DOI 10.1109/TIP.2021.3134465
   Ding DD, 2021, SIGNAL PROCESS-IMAGE, V94, DOI 10.1016/j.image.2021.116201
   Ding DD, 2020, IEEE T CIRC SYST VID, V30, P1871, DOI 10.1109/TCSVT.2019.2935508
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Eadie Samuel, 2023, P M JOINT VID EXP TE
   Fu CM, 2012, IEEE T CIRC SYST VID, V22, P1755, DOI 10.1109/TCSVT.2012.2221529
   Haoyu Ma, 2022, ACM Transactions on Multimedia Computing, Communications and Applications, V18, DOI 10.1145/3477553
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang YW, 2020, IEEE T CIRC SYST VID, V30, P1311, DOI 10.1109/TCSVT.2019.2945048
   Huang ZJ, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3569583
   Huang ZJ, 2022, IEEE T CIRC SYST VID, V32, P2342, DOI 10.1109/TCSVT.2021.3089498
   Huang ZJ, 2021, IEEE T IMAGE PROCESS, V30, P5439, DOI 10.1109/TIP.2021.3084345
   Jaejun Yoo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8372, DOI 10.1109/CVPR42600.2020.00840
   Jia CM, 2019, IEEE T IMAGE PROCESS, V28, P3343, DOI 10.1109/TIP.2019.2896489
   Jia W, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3460820
   Jia W, 2020, IEEE IMAGE PROC, P3109, DOI [10.1109/ICIP40778.2020.9191106, 10.1109/icip40778.2020.9191106]
   Katharopoulos A, 2020, PR MACH LEARN RES, V119
   Kingma D. P., 2014, arXiv
   Kong LY, 2020, IEEE IMAGE PROC, P3379, DOI [10.1109/icip40778.2020.9190807, 10.1109/ICIP40778.2020.9190807]
   Lee-Thorp J, 2022, NAACL 2022: THE 2022 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES, P4296
   Li JR, 2022, IEEE DATA COMPR CONF, P462, DOI 10.1109/DCC52660.2022.00073
   Li TY, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2921877
   Li WX, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3579167
   Li Y., 2022, P M JOINT VID EXP TE
   Li Y, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3529107
   Li Yue, 2023, P M JOINT VID EXP TE
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lin K, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3502723
   Lin WY, 2020, IEEE T MULTIMEDIA, V22, P2749, DOI 10.1109/TMM.2019.2962310
   Liu C, 2020, IEEE INT C MULTIMEDI
   Liu C, 2022, IEEE T IMAGE PROCESS, V31, P3032, DOI 10.1109/TIP.2022.3152627
   Liu JY, 2020, IEEE T IMAGE PROCESS, V29, P7845, DOI 10.1109/TIP.2020.3007828
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Luo SJ, 2021, ADV NEUR IN
   Ma HC, 2020, IEEE I C VI COM I PR, P403, DOI 10.1109/vcip49819.2020.9301805
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   MALLAT SG, 1989, IEEE T PATTERN ANAL, V11, P674, DOI 10.1109/34.192463
   Norkin A, 2012, IEEE T CIRC SYST VID, V22, P1746, DOI 10.1109/TCSVT.2012.2223053
   Pan ZQ, 2020, IEEE T IMAGE PROCESS, V29, P5352, DOI 10.1109/TIP.2020.2982534
   Park N., 2022, P INT C LEARN REPR, P1
   Peng B, 2023, IEEE T CIRC SYST VID, V33, P1911, DOI 10.1109/TCSVT.2022.3213515
   Peng C, 2017, PROC CVPR IEEE, P1743, DOI 10.1109/CVPR.2017.189
   Qunliang Xing, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P275, DOI 10.1007/978-3-030-58517-4_17
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Sikora T, 1997, IEEE T CIRC SYST VID, V7, P19, DOI 10.1109/76.554415
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tolstikhin I, 2021, ADV NEUR IN, V34
   Tsai CY, 2013, IEEE J-STSP, V7, P934, DOI 10.1109/JSTSP.2013.2271974
   Tudor PN, 1995, ELECTRON COMMUN ENG, V7, P257, DOI 10.1049/ecej:19950606
   Vetro A, 2011, P IEEE, V99, P626, DOI 10.1109/JPROC.2010.2098830
   Wang DZ, 2021, IEEE T IMAGE PROCESS, V30, P4198, DOI 10.1109/TIP.2021.3068638
   Wang H., 2022, P M JOINT VID EXP TE
   Wang JH, 2023, PROC CVPR IEEE, P14443, DOI 10.1109/CVPR52729.2023.01388
   Wang Liqiang, 2022, P 2022 IEEE INT C MU, P1
   Wang S, 2022, INT CONF ACOUST SPEE, P1630, DOI 10.1109/ICASSP43922.2022.9746146
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wang ZD, 2022, PROC CVPR IEEE, P17662, DOI 10.1109/CVPR52688.2022.01716
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xiao Jie, 2022, P 36 C NEUR INF PROC
   Yu T, 2022, IEEE WINT CONF APPL, P3615, DOI 10.1109/WACV51458.2022.00367
   Yu WH, 2024, IEEE T PATTERN ANAL, V46, P896, DOI 10.1109/TPAMI.2023.3329173
   Yu WH, 2022, PROC CVPR IEEE, P10809, DOI 10.1109/CVPR52688.2022.01055
   Zhang H., 2022, P M JOINT VID EXP TE
   Zhang Y, 2024, ACM T MULTIM COMPUT, V20, DOI 10.1145/3612925
   Zhang YC, 2023, COMPUT ELECTR ENG, V106, DOI 10.1016/j.compeleceng.2023.108608
   Zhang YB, 2018, IEEE T IMAGE PROCESS, V27, P3827, DOI 10.1109/TIP.2018.2815841
   Zhou Chuan, 2023, P M JOINT VID EXP TE
NR 80
TC 0
Z9 0
U1 1
U2 1
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2024
VL 20
IS 6
SI SI
AR 162
DI 10.1145/3640467
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OQ2T3
UT WOS:001208681800012
DA 2024-08-05
ER

PT J
AU Liu, CP
   Yang, GL
   Zuo, WM
   Zang, TY
AF Liu, Chunpu
   Yang, Guanglei
   Zuo, Wangmeng
   Zang, Tianyi
TI DPDFormer: A Coarse-to-Fine Model for Monocular Depth Estimation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Monocular depth estimation; coarse-to-fine; depth post-discretization;
   DPDformer
AB Monocular depth estimation attracts great attention from computer vision researchers for its convenience in acquiring environment depth information. Recently classification-based MDE methods show its promising performance and begin to act as an essential role in many multi-view applications such as reconstruction and 3D object detection. However, existed classification-based MDE models usually apply fixed depth range discretization strategy across a whole scene. This fixed depth range discretization leads to the imbalance of discretization scale among different depth ranges, resulting in the inexact depth range localization. In this article, to alleviate the imbalanced depth range discretization problem in classification-based monocular depth estimation (MDE) method we follow the coarse-to-fine principle and propose a novel depth range discretization method called depth post-discretization (DPD). Based on a coarse depth anchor roughly indicating the depth range, the DPD generates the depth range discretization adaptively for every position. The depth range discretization with DPD is more fine-grained around the actual depth, which is beneficial for locating the depth range more precisely for each scene position. Besides, to better manage the prediction of the coarse depth anchor and depth probability distribution for calculating the final depth, we design a dual-decoder transformer-based network, i.e., DPDFormer, which is more compatible with our proposed DPD method. We evaluate DPDFormer on popular depth datasets NYU Depth V2 and KITTI. The experimental results prove the superior performance of our proposed method.
C1 [Liu, Chunpu; Yang, Guanglei; Zuo, Wangmeng; Zang, Tianyi] Harbin Inst Technol, Harbin, Peoples R China.
   [Liu, Chunpu; Yang, Guanglei; Zuo, Wangmeng; Zang, Tianyi] Harbin Inst Technol, Harbin 150001, Peoples R China.
C3 Harbin Institute of Technology; Harbin Institute of Technology
RP Zang, TY (corresponding author), Harbin Inst Technol, Harbin 150001, Peoples R China.
EM coll@hit.edu.cn; yangguanglei@hit.edu.cn; wmzuo@hit.edu.cn;
   tianyi.zang@hit.edu.cn
RI Zuo, Wangmeng/B-3701-2008; Yang, Guanglei/HME-0024-2023
OI Yang, Guanglei/0000-0002-5324-3642; Zuo, Wangmeng/0000-0002-3330-783X
FU Natural Science Foundation of China [62076082]; HIT Assistant Professor
   ResearchInitiation Program [AUGA5630115523]
FX This work has been supported by Natural Science Foundation of China
   (62076082) and HIT Assistant Professor ResearchInitiation Program
   (AUGA5630115523).
CR Bae G, 2022, PROC CVPR IEEE, P2832, DOI 10.1109/CVPR52688.2022.00286
   Bhat SF, 2022, LECT NOTES COMPUT SC, V13661, P480, DOI 10.1007/978-3-031-19769-7_28
   Bhat SF, 2021, PROC CVPR IEEE, P4008, DOI 10.1109/CVPR46437.2021.00400
   Cao YZH, 2018, IEEE T CIRC SYST VID, V28, P3174, DOI 10.1109/TCSVT.2017.2740321
   Chang JR, 2018, PROC CVPR IEEE, P5410, DOI 10.1109/CVPR.2018.00567
   Chen PY, 2019, PROC CVPR IEEE, P2619, DOI 10.1109/CVPR.2019.00273
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Eigen D, 2014, ADV NEUR IN, V27
   Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304
   Fu H, 2018, PROC CVPR IEEE, P2002, DOI 10.1109/CVPR.2018.00214
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Geiger A, 2011, LECT NOTES COMPUT SC, V6492, P25, DOI 10.1007/978-3-642-19315-6_3
   Godard C, 2019, IEEE I CONF COMP VIS, P3827, DOI 10.1109/ICCV.2019.00393
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hirschmüller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI [10.1109/TPAMI.2007.1166, 10.1109/TPAMl.2007.1166]
   Itoh H, 2021, INT J COMPUT ASS RAD, V16, P989, DOI 10.1007/s11548-021-02398-x
   Ji P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12767, DOI 10.1109/ICCV48922.2021.01255
   Jung H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12622, DOI 10.1109/ICCV48922.2021.01241
   Kingma D. P., 2014, arXiv
   Ladicky L, 2014, PROC CVPR IEEE, P89, DOI 10.1109/CVPR.2014.19
   Laidlow T, 2019, IEEE INT CONF ROBOT, P4068, DOI [10.1109/icra.2019.8793527, 10.1109/ICRA.2019.8793527]
   Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee J.H., 2019, arXiv
   Li RZ, 2023, IEEE T CIRC SYST VID, V33, P830, DOI 10.1109/TCSVT.2022.3207105
   Li Y, 2021, IEEE T IMAGE PROCESS, V30, P2288, DOI 10.1109/TIP.2021.3051761
   Liu C, 2023, Arxiv, DOI arXiv:2302.06556
   Liu FY, 2016, IEEE T PATTERN ANAL, V38, P2024, DOI 10.1109/TPAMI.2015.2505283
   Liu SP, 2022, IEEE INTERNET THINGS, V9, P16168, DOI 10.1109/JIOT.2022.3151374
   Liu YF, 2023, IEEE T PATTERN ANAL, V45, P7035, DOI 10.1109/TPAMI.2020.3001940
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lopez-Rodriguez A, 2023, INT J COMPUT VISION, V131, P752, DOI 10.1007/s11263-022-01718-1
   Paszke A, 2019, ADV NEUR IN, V32
   Patil V, 2022, PROC CVPR IEEE, P1600, DOI 10.1109/CVPR52688.2022.00166
   Ramamonjisoa M, 2019, IEEE INT CONF COMP V, P2109, DOI 10.1109/ICCVW.2019.00266
   Ranftl R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12159, DOI 10.1109/ICCV48922.2021.01196
   Reading C, 2021, PROC CVPR IEEE, P8551, DOI 10.1109/CVPR46437.2021.00845
   Saxena A., 2005, P ADV NEUR INF PROC, P1
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Song M, 2021, IEEE T CIRC SYST VID, V31, P4381, DOI 10.1109/TCSVT.2021.3049869
   Sun QY, 2022, IEEE T NEUR NET LEAR, V33, P2023, DOI 10.1109/TNNLS.2021.3100895
   Tateno K, 2017, PROC CVPR IEEE, P6565, DOI 10.1109/CVPR.2017.695
   Vankadari Madhu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P443, DOI 10.1007/978-3-030-58604-1_27
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang AJ, 2020, IEEE T IMAGE PROCESS, V29, P4130, DOI 10.1109/TIP.2020.2968751
   Wang Y, 2019, PROC CVPR IEEE, P8063, DOI 10.1109/CVPR.2019.00826
   Xu D, 2019, IEEE T PATTERN ANAL, V41, P1426, DOI 10.1109/TPAMI.2018.2839602
   Xu XF, 2021, IEEE T IMAGE PROCESS, V30, P8811, DOI 10.1109/TIP.2021.3120670
   Yang XB, 2020, IEEE T VIS COMPUT GR, V26, P3446, DOI 10.1109/TVCG.2020.3023634
   Yao Y, 2018, LECT NOTES COMPUT SC, V11212, P785, DOI 10.1007/978-3-030-01237-3_47
   Yao Y, 2019, PROC CVPR IEEE, P5520, DOI 10.1109/CVPR.2019.00567
   Yin W, 2019, IEEE I CONF COMP VIS, P5683, DOI 10.1109/ICCV.2019.00578
   You YR, 2020, Arxiv, DOI arXiv:1906.06310
   Yuan WH, 2022, PROC CVPR IEEE, P3906, DOI 10.1109/CVPR52688.2022.00389
   Zhao YH, 2020, PROC CVPR IEEE, P3327, DOI 10.1109/CVPR42600.2020.00339
NR 55
TC 0
Z9 0
U1 20
U2 20
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2024
VL 20
IS 5
AR 139
DI 10.1145/3638559
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MF3R9
UT WOS:001192177900019
DA 2024-08-05
ER

PT J
AU Zhang, CY
   Zhang, Y
   Li, B
   Piao, XL
   Yin, BC
AF Zhang, Chengyang
   Zhang, Yong
   Li, Bo
   Piao, Xinglin
   Yin, Baocai
TI CrowdGraph: Weakly supervised Crowd Counting via Pure Graph Neural
   Network
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Crowd counting; weakly supervised learning; graph neural network; uneven
   distribution of crowds
ID LOCALIZATION; SCALE
AB Most existing weakly supervised crowd counting methods utilize Convolutional Neural Networks (CNN) or Transformer to estimate the total number of individuals in an image. However, both CNN-based (grid-to-count paradigm) and Transformer-based (sequence-to-count paradigm) methods take images as inputs in a regular form. This approach treats all pixels equally but cannot address the uneven distribution problem within human crowds. This challenge would lead to a decline in the counting performance of the model. Compared with grid and sequence, the graph structure could better explore the relationship among features. In this article, we propose a new graph-based crowd counting method named CrowdGraph, which reinterprets the weakly supervised crowd counting problem from a graph-to-count perspective. In the proposed CrowdGraph, each image is constructed as a graph, and a graph-based network is designed to extract features at the graph level. CrowdGraph comprises three main components: a dynamic graph convolutional backbone, a multi-scale dilated graph convolution module, and a regression head. To the best of our knowledge, CrowdGraph is the first method that is completely formulated based on the Graph Neural Network (GNN) for the crowd counting task. Extensive experiments demonstrate that the proposed CrowdGraph outperforms pure CNN-based and pure Transformer-based weakly supervised methods comprehensively and achieves highly competitive counting performance.
C1 [Zhang, Chengyang; Zhang, Yong; Li, Bo; Piao, Xinglin; Yin, Baocai] Beijing Univ Technol, Fac Informat Technol, Beijing Key Lab Multimedia & Intelligent Software, Beijing Artiicial Intelligence Inst, Beijing 100124, Peoples R China.
C3 Beijing University of Technology
RP Zhang, Y (corresponding author), Beijing Univ Technol, Fac Informat Technol, Beijing Key Lab Multimedia & Intelligent Software, Beijing Artiicial Intelligence Inst, Beijing 100124, Peoples R China.
EM Cy_Zhang@emails.bjut.edu.cn; zhangyong2010@bjut.edu.cn;
   bo_li@emails.bjut.edu.cn; piaoxl@bjut.edu.cn; ybc@bjut.edu.cn
OI Zhang, Chengyang/0000-0003-3658-5779; Li, Bo/0000-0003-0608-1502
FU National Key R&D Program of China [2021ZD0111902]; National Natural
   Science Foundation of China [62072015, U21B2038, U19B2039, 61902053];
   Beijing Natural Science Foundation [4222021]
FX The research project is partially supported by the National Key R&D
   Program of China (No. 2021ZD0111902), National Natural Science
   Foundation of China (No.62072015, U21B2038, U19B2039, 61902053), and
   Beijing Natural Science Foundation (4222021).
CR Abousamra S, 2021, AAAI CONF ARTIF INTE, V35, P872
   Bai S, 2020, PROC CVPR IEEE, P4593, DOI 10.1109/CVPR42600.2020.00465
   Basalamah S, 2019, IEEE ACCESS, V7, P71576, DOI 10.1109/ACCESS.2019.2918650
   Cao XK, 2018, LECT NOTES COMPUT SC, V11209, P757, DOI 10.1007/978-3-030-01228-1_45
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen X, 2022, arXiv
   Chen XY, 2020, NEUROCOMPUTING, V407, P399, DOI 10.1016/j.neucom.2020.04.117
   Cheng J, 2021, IEEE T IMAGE PROCESS, V30, P2862, DOI 10.1109/TIP.2021.3055631
   Deng MF, 2024, VISUAL COMPUT, V40, P1053, DOI 10.1007/s00371-023-02831-z
   Deng SQ, 2023, IEEE T GEOSCI REMOTE, V61, DOI 10.1109/TGRS.2023.3244750
   Dhariwal P, 2021, ADV NEUR IN, V34
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Gao JY, 2022, NEUROCOMPUTING, V513, P94, DOI 10.1016/j.neucom.2022.09.113
   Gao JY, 2020, IEEE T CIRC SYST VID, V30, P3486, DOI 10.1109/TCSVT.2019.2919139
   Han K, 2022, Arxiv, DOI arXiv:2206.00272
   Hu Hanzhe, 2020, P EUROPEAN C COMPUTE, P1
   Hu Mingzhe, 2023, P MEDICAL IMAGING 20, V2468, P322
   Idrees H, 2018, LECT NOTES COMPUT SC, V11206, P544, DOI 10.1007/978-3-030-01216-8_33
   Idrees H, 2013, PROC CVPR IEEE, P2547, DOI 10.1109/CVPR.2013.329
   Khan SD, 2021, INT J COMPUT INT SYS, V14, DOI 10.1007/s44196-021-00016-x
   Khan SD, 2021, ARAB J SCI ENG, V46, P3051, DOI 10.1007/s13369-020-04990-w
   Khan SD, 2021, VISUAL COMPUT, V37, P2127, DOI 10.1007/s00371-020-01974-7
   Kong XY, 2020, INT CONF ACOUST SPEE, P2722, DOI [10.1109/ICASSP40776.2020.9054258, 10.1109/icassp40776.2020.9054258]
   Lei YJ, 2021, PATTERN RECOGN, V109, DOI 10.1016/j.patcog.2020.107616
   Li B, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3594670
   Li B, 2023, VISUAL COMPUT, V39, P2671, DOI 10.1007/s00371-022-02485-3
   Li B, 2021, PATTERN ANAL APPL, V24, P853, DOI 10.1007/s10044-021-00959-z
   Li Bo, 2024, Engineering Applications of Artificial Intelligence, V129
   Li GH, 2019, IEEE I CONF COMP VIS, P9266, DOI 10.1109/ICCV.2019.00936
   Li YH, 2018, PROC CVPR IEEE, P1091, DOI 10.1109/CVPR.2018.00120
   Liang DK, 2022, LECT NOTES COMPUT SC, V13661, P38, DOI 10.1007/978-3-031-19769-7_3
   Liang DK, 2023, IEEE T MULTIMEDIA, V25, P6040, DOI 10.1109/TMM.2022.3203870
   Liang DK, 2022, SCI CHINA INFORM SCI, V65, DOI 10.1007/s11432-021-3445-y
   Lin H, 2022, PROC CVPR IEEE, P19596, DOI 10.1109/CVPR52688.2022.01901
   Liu LB, 2019, IEEE I CONF COMP VIS, P1774, DOI 10.1109/ICCV.2019.00186
   Liu N, 2019, PROC CVPR IEEE, P3220, DOI 10.1109/CVPR.2019.00334
   Liu WZ, 2019, PROC CVPR IEEE, P5094, DOI 10.1109/CVPR.2019.00524
   Liu YB, 2022, IEEE T CIRC SYST VID, V32, P6821, DOI 10.1109/TCSVT.2022.3171235
   Luo A, 2020, AAAI CONF ARTIF INTE, V34, P11693
   Ma ZH, 2019, IEEE I CONF COMP VIS, P6141, DOI 10.1109/ICCV.2019.00624
   Miao ZZ, 2023, COMPUT VIS MEDIA, V9, P859, DOI 10.1007/s41095-022-0313-5
   Nguyen HD, 2021, AAAI CONF ARTIF INTE, V35, P9092
   QiWang Junyu Gao, 2020, IEEE Trans. Pattern Anal. Mach. Intell., V43, P2141
   Ranasinghe Y, 2024, Arxiv, DOI arXiv:2303.12790
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Sajid U, 2020, IEEE T CIRC SYST VID, V30, P3499, DOI 10.1109/TCSVT.2020.2978717
   Savner SS, 2023, J VIS COMMUN IMAGE R, V94, DOI 10.1016/j.jvcir.2023.103853
   Savner SS, 2022, Arxiv, DOI arXiv:2203.03768
   Shu WB, 2022, PROC CVPR IEEE, P19586, DOI 10.1109/CVPR52688.2022.01900
   Sindagi VA, 2017, 2017 14TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS)
   Sindagi VA, 2022, IEEE T PATTERN ANAL, V44, P2594, DOI 10.1109/TPAMI.2020.3035969
   Sindagi VA, 2019, IEEE I CONF COMP VIS, P1002, DOI 10.1109/ICCV.2019.00109
   Song QY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3345, DOI 10.1109/ICCV48922.2021.00335
   Song QY, 2021, AAAI CONF ARTIF INTE, V35, P2576
   Tian Y, 2021, Arxiv, DOI arXiv:2109.14483
   Valsesia D., 2019, INT C LEARN REPR ICL, P1
   Vaswani A, 2017, ADV NEUR IN, V30
   Wan J, 2021, PROC CVPR IEEE, P1974, DOI 10.1109/CVPR46437.2021.00201
   Wan J, 2019, PROC CVPR IEEE, P4031, DOI 10.1109/CVPR.2019.00416
   Wang Fusen, 2022, arXiv
   Wang L, 2020, PROCEEDINGS OF THE 2020 INTERNATIONAL CONFERENCE ON ARTIFICIAL LIFE AND ROBOTICS (ICAROB2020), P611
   Wang MJ, 2023, IEEE WINT CONF APPL, P167, DOI 10.1109/WACV56688.2023.00025
   Wang Q, 2019, PROC CVPR IEEE, P8190, DOI 10.1109/CVPR.2019.00839
   Wang WH, 2022, COMPUT VIS MEDIA, V8, P415, DOI 10.1007/s41095-022-0274-8
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wu Z, 2023, IEEE T CIRC SYST VID, V33, P228, DOI 10.1109/TCSVT.2022.3187194
   Xiong Zheng, 2022, arXiv
   Xu CF, 2022, INT J COMPUT VISION, V130, P405, DOI 10.1007/s11263-021-01542-z
   Yifan Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P1, DOI 10.1007/978-3-030-58598-3_1
   Zhai Q, 2023, IEEE T MULTIMEDIA, V25, P5813, DOI 10.1109/TMM.2022.3199555
   Zhang AR, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3356019
   Zhang CY, 2024, Arxiv, DOI arXiv:2311.16203
   Zhang CY, 2024, IEEE J BIOMED HEALTH, V28, P355, DOI 10.1109/JBHI.2023.3329542
   Zhang L, 2020, PROC CVPR IEEE, P3723, DOI 10.1109/CVPR42600.2020.00378
   Zhang YY, 2016, PROC CVPR IEEE, P589, DOI 10.1109/CVPR.2016.70
   Zhao GM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2743, DOI 10.1109/ICCV48922.2021.00276
   Zhao JW, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P163, DOI 10.1109/ICCV48922.2021.00023
   Zhao WD, 2022, IEEE T CIRC SYST VID, V32, P5399, DOI 10.1109/TCSVT.2022.3146459
NR 79
TC 0
Z9 0
U1 6
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2024
VL 20
IS 5
AR 135
DI 10.1145/3638774
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MF3R9
UT WOS:001192177900015
DA 2024-08-05
ER

PT J
AU Chen, HZ
   Duan, HH
   Abdallah, M
   Zhu, YF
   Wen, YG
   El Saddik, A
   Cai, W
AF Chen, Hongzhou
   Duan, Haihan
   Abdallah, Maha
   Zhu, Yufeng
   Wen, Yonggang
   El Saddik, Abdulmotaleb
   Cai, Wei
TI Web3 Metaverse: State-of-the-Art and Vision
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Metaverse; Web3; human-centered; definition and framework; survey
ID BLOCKCHAIN; FACEBOOK
AB The metaverse, as a rapidly evolving socio-technical phenomenon, exhibits significant potential across diverse domains by leveraging Web3 (a.k.a. Web 3.0) technologies such as blockchain, smart contracts, and non-fungible tokens (NFTs). This survey aims to provide a comprehensive overview of the Web3 metaverse from a human-centered perspective. We (i) systematically review the development of the metaverse over the past 30 years, highlighting the balanced contributions from its core components: Web3, immersive convergence, and crowd intelligence communities, (ii) define the metaverse that integrates the Web3 community as the Web3 metaverse and propose an analysis framework from the community, society, and human layers to describe the features, missions, and relationships for each community and their overlapping sections, (iii) survey the state-of-the-art of the Web3 metaverse from a human-centered perspective, namely, the identity, field, and behavior aspects, and (iv) provide supplementary technical reviews. To the best of our knowledge, this work represents the first systematic, interdisciplinary survey on the Web3 metaverse. Specifically, we commence by discussing the potential for establishing decentralized identities (DID) utilizing mechanisms such as profile picture (PFP) NFTs, domain name NFTs, and soulbound tokens (SBTs). Subsequently, we examine land, utility, and equipment NFTs within the Web3 metaverse, highlighting interoperable and full on-chain solutions for existing centralization challenges. Lastly, we spotlight current research and practices about individual, intragroup, and inter-group behaviors within the Web3 metaverse, such as Creative Commons Zero license (CC0) NFTs, decentralized education, decentralized science (DeSci), and decentralized autonomous organizations (DAO). Furthermore, we share our insights into several promising directions, encompassing three key socio-technical facets of Web3 metaverse development.
C1 [Chen, Hongzhou; Duan, Haihan; Cai, Wei] Chinese Univ Hong Kong, 2001 Longxiang Blvd, Shenzhen 518172, Guangdong, Peoples R China.
   [Chen, Hongzhou; Duan, Haihan; El Saddik, Abdulmotaleb] Mohamed bin Zayed Univ Artificial Intelligence, Abu Dhabi, U Arab Emirates.
   [Abdallah, Maha] Sorbonne Univ, CNRS, LIP6, 4 Pl Jussieu, F-75005 Paris, France.
   [Zhu, Yufeng] Meta Real Lab Res, 10301 Willows Rd NE, Redmond, WA 98052 USA.
   [Wen, Yonggang] Nanyang Technol Univ, 50 Nanyang Ave, Singapore 639798, Singapore.
   [El Saddik, Abdulmotaleb] Univ Ottawa, Multimedia Commun Res Lab MCRLab, 75 Laurier Ave E, Ottawa, ON K1N 6N6, Canada.
C3 The Chinese University of Hong Kong, Shenzhen; Mohamed Bin Zayed
   University of Artificial Intelligence; Centre National de la Recherche
   Scientifique (CNRS); Sorbonne Universite; Nanyang Technological
   University; University of Ottawa
RP Cai, W (corresponding author), Chinese Univ Hong Kong, 2001 Longxiang Blvd, Shenzhen 518172, Guangdong, Peoples R China.
EM hongzhouchen1@link.cuhk.edu.cn; haihanduan@link.cuhk.edu.cn;
   Maha.Abdallah@lip6.fr; yufengzhu@meta.com; ygwen@ntu.edu.sg;
   elsaddik@uottawa.ca; caiwei@cuhk.edu.cn
RI Zhu, Yufeng/I-4564-2019; Chen, Hongzhou/HGT-9253-2022; Wen,
   Yonggang/B-8848-2011; Wen, Yonggang/P-9406-2017; /D-4159-2009
OI Zhu, Yufeng/0000-0001-8431-7731; Chen, Hongzhou/0000-0002-8310-3202;
   Duan, Haihan/0000-0001-6438-3790; Wen, Yonggang/0000-0002-2751-5114;
   /0000-0002-7690-8547
FU Shenzhen Science and Technology Program [JCYJ20210324124205016]; Chinese
   University of Hong Kong, Shenzhen-White Matrix Joint Metaverse
   Laboratory
FX This work was supported in part by Shenzhen Science and Technology
   Program (Grant No. JCYJ20210324124205016) and in part by The Chinese
   University of Hong Kong, Shenzhen-White Matrix Joint Metaverse
   Laboratory.
CR Achiam OJ, 2023, Arxiv, DOI arXiv:2303.08774
   Adams E., 2004, Postmodernism and the three types of immersion
   Aliyev Hafiz, 2023, PREPRINT
   Aljamos Yusof Mahmoud, 2022, Journal of Contemporary Maqasid Studies, V1, P59
   Anders Lance, 2022, ERC-4907: Rental NFT, an Extension of EIP-721
   Ante L, 2022, INT J INNOV TECHNOL, V19, DOI 10.1142/S0219877022500080
   Avellaneda O., 2019, IEEE Commun. Stand. Mag., V3, P10, DOI [10.1109/MCOMSTD.2019.9031542, DOI 10.1109/MCOMSTD.2019.9031542]
   Azuma RT, 1997, PRESENCE-VIRTUAL AUG, V6, P355, DOI 10.1162/pres.1997.6.4.355
   Ball M., 2022, The Metaverse: And How It Will Revolutionize Everything
   Bambara J., 2018, BLOCKCHAIN PRACTICAL
   Bansal G, 2022, IEEE ACCESS, V10, P119914, DOI 10.1109/ACCESS.2022.3219845
   Bao H, 2022, J RISK FINANC MANAG, V15, DOI 10.3390/jrfm15050215
   Barnard ChesterI., 2003, Organization and Management: Selected Papers
   Baudrillard Jean., 1981, CRITIQUE POLITICAL E
   Baytas MA, 2022, EXTENDED ABSTRACTS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2022, DOI 10.1145/3491101.3519694
   Belk R, 2022, J BUS RES, V153, P198, DOI 10.1016/j.jbusres.2022.08.031
   Bjork S., 2005, The game design reader: A rules of play anthology, P410
   Bloomberg Intelligence, 2021, Metaverse may be $800 billion market, next tech platform.
   Blum M, 2019, Providing Sound Foundations for Cryptography: On the Work of Shafi Goldwasser and Silvio Micali, P329
   Bodkhe U, 2020, IEEE ACCESS, V8, P79764, DOI 10.1109/ACCESS.2020.2988579
   Borrajo F, 2010, DECIS SUPPORT SYST, V48, P498, DOI 10.1016/j.dss.2009.06.009
   Brabham D.C, 2008, Convergence, V14, P75, DOI DOI 10.1177/1354856507084420
   Burks Zach, 2020, ERC-2981: NFT Royalty Standard
   Burtis CA, 1996, CLIN CHEM, V42, P1735
   Buterin V, 2014, Ethereum: A next-generation smart contract and decentralized application platform
   Buterin Vitalik, 2022, Soulbound
   Buterin Vitalik, 2023, What do I think about biometric proof of personhood? (2023)
   Cai W, 2018, IEEE ACCESS, V6, P53019, DOI 10.1109/ACCESS.2018.2870644
   Cai Wei., 2023, P 2023 CHI C HUM FAC, P1
   Cao LB, 2022, IEEE INTELL SYST, V37, P6, DOI 10.1109/MIS.2022.3181504
   Caplan Bryan, 2018, The case against education
   Carter M, 2020, GAMES CULT, V15, P453, DOI 10.1177/1555412020913771
   Casale-brunet Simone, 2022, GoodIT 2022: Conference on Information Technology for Social Good, P283, DOI 10.1145/3524458.3547230
   Castro M, 1999, USENIX ASSOCIATION PROCEEDINGS OF THE THIRD SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDI '99), P173, DOI 10.1145/571637.571640
   Chaffer Tomer Jordi, 2022, J. Strateg. Innov. Sustain, V17, P1
   Chalmers D., 2022, Journal of Business Venturing Insights, V17, DOI DOI 10.1016/J.JBVI.2022.E00309
   Chang Jessica, 2022, ERC-6239: Semantic Soulbound Tokens
   Chaturvedi Aakanksha, 2022, BAYC accused of discrimination; faces flak on Twitter
   Chaum D. L., 1979, Computer Systems Established, Maintained and Trusted by Mutually Suspicious Groups
   Cheah ET, 2015, ECON LETT, V130, P32, DOI 10.1016/j.econlet.2015.02.029
   Chen H, 2021, ENERGY TECHNOL-GER, V9, DOI 10.1002/ente.202100045
   Chen Hongzhou, 2023, IEEE Transactions on Computational Social Systems, V2023, P1
   Chen Hongzhou, 2023, IEEE Transactions on Computational Social Systems, P1
   Chen Shuo, 2022, arXiv
   Chen Y, 2021, J MANAGE, V47, P1305, DOI 10.1177/0149206320916755
   Chen Zi-Hyo, 2022, BCP Business and Management
   Christodoulou K., 2022, Blockchains and the Token Economy: Theory and Practice, P139
   Clarke Roger, 2005, P ARS EL 2005 S HYBR, P2
   Coban Onder, 2021, Int. J. Inf. Secur. Sci, V10, P1
   Crowston K, 2012, ACM COMPUT SURV, V44, DOI 10.1145/2089125.2089127
   Darcy Sean, 2022, ERC-5570: Digital Receipt Non-Fungible Tokens
   Davenport Thomas H., 2001, Ubiquity, V2001, P1, DOI [10.1145/376625.376626, 10.1145/375348.376626, DOI 10.1145/375348.376626]
   Dawson A., 2022, Self-Gov. Manag. Econ., V10, P52
   De Filippi Primavera, 2016, First Monday, V21, DOI 10.5210/fm.v21i12.7113
   Decentraland, 2019, What makes a virtual world more than a game?
   DeFrancesco L, 2022, NAT BIOTECHNOL, V40, P1310, DOI 10.1038/s41587-022-01459-z
   Deloitte, 2022, Metaverse: The hype, possibilities, and beyond
   Diedrich H., 2016, ETHEREUM BLOCKCHAINS
   Dilger W, 1997, IEEE SYS MAN CYBERN, P351, DOI 10.1109/ICSMC.1997.625775
   Dincelli E, 2022, J STRATEGIC INF SYST, V31, DOI 10.1016/j.jsis.2022.101717
   Ding WW, 2022, IEEE T COMPUT SOC SY, V9, P1563, DOI 10.1109/TCSS.2022.3204745
   Dionisio JDN, 2013, ACM COMPUT SURV, V45, DOI 10.1145/2480741.2480751
   Dowling M, 2022, FINANC RES LETT, V44, DOI 10.1016/j.frl.2021.102097
   Duan Haihan, 2022, 2022 IEEE Conference on Games (CoG), P536, DOI 10.1109/CoG51982.2022.9893717
   Duan HH, 2023, IEEE COMMUN MAG, V61, P52, DOI 10.1109/MCOM.003.2200461
   Duan HH, 2022, IEEE INT WORKSH MULT, DOI 10.1109/MMSP55362.2022.9949108
   Duan HH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P153, DOI 10.1145/3474085.3479238
   Eduard LAspez i Fina, 2023, ERC-6997: ERC-721 with transaction validation step
   Ehrsam Fred, 2017, VR is a Killer App for Blockchains
   El Faqir Y, 2020, PROCEEDINGS OF THE 16TH INTERNATIONAL SYMPOSIUM ON OPEN COLLABORATION (OPENSYM), DOI 10.1145/3412569.3412579
   Entriken William, 2018, Technical Report
   ethereum.org, 2023, Ethereum Virtual Machine (EVM)
   Etzrodt Martin, 2018, Decentralizing science
   Exmundo Jex, 2023, Quantum: The Story Behind the World's First NFT
   Exmundo Jex, 2022, How Soulbound Tokens Could Change Society as We Know It
   Fan SZ, 2023, WORLD WIDE WEB, V26, P1181, DOI 10.1007/s11280-022-01077-4
   Fan Sizheng, 2023, P ACM CHI C HUM CHI
   Faqir-Rhazoui Y, 2021, EXTENDED ABSTRACTS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'21), DOI 10.1145/3411763.3451755
   Far Saeed Banaeian, 2022, Procedia Computer Science, P755, DOI 10.1016/j.procs.2022.11.238
   Filipcic S., 2022, 2022 45th Jubilee International Convention on Information, Communication and Electronic Technology (MIPRO)., P1278, DOI 10.23919/MIPRO55190.2022.9803324
   Fitri Afiq., 2022, Will DAOs survive the crypto winter?
   Franceschet M, 2021, LEONARDO, V54, P402, DOI 10.1162/leon_a_02003
   Freni P, 2022, BLOCKCHAIN-RES APPL, V3, DOI 10.1016/j.bcra.2022.100069
   Gentry C, 2009, ACM S THEORY COMPUT, P169, DOI 10.1145/1536414.1536440
   Gilbertson Scott, 2007, Slap in the Facebook: It's time for social networks to open up
   Gillpatrick Tom, 2022, Economics, V10, P105, DOI DOI 10.2478/EOIK-2022-0009
   Glaser F., 2014, Bitcoin-asset or currency? revealing users' hidden intentions
   GlenWeyl E, 2022, Decentralized society: Finding web3's soul
   Goanta Catalina, 2020, Disruptive Technology, Legal Innovation, and the Future of Real Estate, P139
   Griffin DeJuawn, 2023, Mercer Law Review, V74, P13
   Grimmelmann James, 2022, Copyright Vulnerabilities in NFTs
   Guardian, 2022, The Guardian
   Habashi Janette, 2017, Political Socialization of Youth: A Palestinian Case Study, P61
   Halpin Harry, 2020, Security Standardisation Research. 6th International Conference, SSR 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12529), P148, DOI 10.1007/978-3-030-64357-7_7
   Harkavy Elizabeth, 2022, 7 Essential Ingredients of a Metaverse
   Hayward Andrew, 2022, What is The Sandbox? The Ethereum NFT Metaverse Game
   Heun David, 2017, As China mulls ICOs, blockchain leaders turn to Russia
   Hong S, 2020, INT CON DISTR COMP S, P1269, DOI 10.1109/ICDCS47774.2020.00163
   Hudson-Smith A, 2022, URBAN PLAN, V7, P343, DOI 10.17645/up.v7i2.5193
   Huynh-The T, 2023, FUTURE GENER COMP SY, V143, P401, DOI 10.1016/j.future.2023.02.008
   Ilyushina N, 2022, J BRIT BLOCKCHAIN AS, V5, P50
   Jain Mayank, 2023, Revamping social networking using blockchain: Conceptual case-study of lens protocol
   Jakobsson M, 1999, INT FED INFO PROC, V23, P258
   Jarvenpaa SL, 1999, ORGAN SCI, V10, P791, DOI 10.1287/orsc.10.6.791
   Jentzsch C, 2016, CISC VIS NETW IND GL
   Jiang Y, 2022, BSCI'22: PROCEEDINGS OF THE FOURTH ACM INTERNATIONAL SYMPOSIUM ON BLOCKCHAIN AND SECURE CRITICAL INFRASTRUCTURE, P35, DOI 10.1145/3494106.3528675
   Jiang Yu, 2022, P 17 INT C FDN DIG G, P1
   Jiao J, 2020, P IEEE S SECUR PRIV, P1695, DOI 10.1109/SP40000.2020.00066
   John Gavin, 2022, ERC-5507: Refundable Tokens
   Kaneriya Jayana, 2020, Proceedings of the 3rd International Conference on Intelligent Sustainable Systems (ICISS 2020), P1150, DOI 10.1109/ICISS49785.2020.9315899
   Karl KA, 2022, SMALL GR RES, V53, P343, DOI 10.1177/10464964211015286
   Kent Charlotte, 2023, The Crypto Bubble's Burst. What Will Happen to the NFT Artworld?
   Kent Charlotte, 2023, Art needs curators-and so do NFTs
   Kirillov Andrew, 2022, 2022 IEEE International Conference on Big Data (Big Data), P3201, DOI 10.1109/BigData55660.2022.10020887
   Kitzler Stefan, 2021, ACM Transactions on the Web., V17, P1
   Kokoris-Kogias E, 2018, P IEEE S SECUR PRIV, P583, DOI 10.1109/SP.2018.000-5
   Korpal Gaurish, 2022, Decentralization and web3 technologies
   Kovacova M., 2022, Linguistic and Philosophical Investigations, V21, P57, DOI [10.22381/lpi2120224, DOI 10.22381/LPI2120224]
   Kral Pavol, 2022, Psychosociological Issues in Human Resource Management, V10, P92
   Kraus S, 2022, INT J ENTREP BEHAV R, V28, P52, DOI 10.1108/IJEBR-12-2021-0984
   Lahdesmaki Tuuli, 2022, Learning Cultural Literacy Through Creative Practices in Schools: Cultural and Multimodal Approaches to Meaning -Making, P31
   Lee Edward, 2023, Creators Take Control: How NFTs Revolutionize Art, Business, and Entertainment, P1
   Lee L H., 2021, PREPRINT
   Lenz Karl., 2022, P GOFFM HDB LEB WERK, P267
   Li W, 2017, FRONT INFORM TECH EL, V18, P15, DOI 10.1631/FITEE.1601859
   Li XF, 2019, 2019 IEEE 2ND INTERNATIONAL CONFERENCE ON INFORMATION AND COMPUTER TECHNOLOGIES (ICICT), P204, DOI [10.1109/INFOCT.2019.8711021, 10.1109/infoct.2019.8711021]
   Lin LH, 2023, PROCEEDINGS OF THE 2023 PROCEEDINGS OF THE 14TH ACM MULTIMEDIA SYSTEMS CONFERENCE, MMSYS 2023, P397, DOI 10.1145/3587819.3592549
   Lin ZH, 2023, Arxiv, DOI arXiv:2308.03165
   Lingwall J, 2019, UMKC L REV, V88, P285
   Littleton K., 2004, LEARNING COLLABORATE
   Lockyer Matt, 2018, ERC-998: Composable Non-Fungible Token
   Lumineau F, 2021, ORGAN SCI, V32, P500, DOI 10.1287/orsc.2020.1379
   Machinations, 2023, Game Design for Sustainable Web 3.0 Economies.
   Mackenzie S, 2022, CRIME MEDIA CULT, V18, P527, DOI 10.1177/17416590211039797
   Malinova Katya, 2018, Tokenomics: When tokens beat equity
   Marttila Jarno, 2022, ERC-5023: Shareable Non-Fungible Token
   Marwick AE, 2011, NEW MEDIA SOC, V13, P114, DOI 10.1177/1461444810365313
   McConaghy M, 2017, STRATEG CHANG, V26, P461, DOI 10.1002/jsc.2146
   McKinsey, 2022, Value creation in the metaverse.
   MERTON RK, 1968, SCIENCE, V159, P56, DOI 10.1126/science.159.3810.56
   Mileva Geri, 2023, NFTs and Their Role in the Creator Economy.
   Min T, 2019, IEEE CONF COMPU INTE
   Min T, 2022, CCF T PERVAS COMPUT, V4, P124, DOI 10.1007/s42486-022-00094-6
   Morewedge CK, 2021, J MARKETING, V85, P196, DOI 10.1177/0022242920957007
   Mozumder MAI, 2022, INT CONF ADV COMMUN, P256, DOI 10.23919/ICACT53585.2022.9728808
   Murray A, 2021, ACAD MANAGE PERSPECT, V35, P622, DOI 10.5465/amp.2018.0066
   Nakamoto S., 2008, BITCOIN PEER TO PEER, P21260
   Nalbant Kemal Gokhan, 2021, Journal of Metaverse, V1, P9
   Narin N.G., 2021, J. Metaverse, V1, P17
   Nation Jeremy, 2022, Lens Protocol stalls services due to transaction issues.
   National Institute of Standards and Technology, 2003, Announcing approval of federal information processing standard (FIPS) 180-2, secure hash standard; a revision of FIPS 180-1
   Patsakis C, 2020, IEEE ACCESS, V8, P118559, DOI 10.1109/ACCESS.2020.3004727
   Paul T, 2014, COMPUT NETW, V75, P437, DOI 10.1016/j.comnet.2014.10.005
   Pollock Darryn, 2018, Crypto Prizes On The Rise, Magical Marketing Or Another Scam? (2018)
   Polygon DoD team, 2023, Games Over Governance: Recentering DAOs on Coordination
   Popescu A-D., 2021, 5 INT C INN BUS EC M
   Qin H, 2021, J RETAIL CONSUM SERV, V58, DOI 10.1016/j.jretconser.2020.102337
   Radomski Witek, 2018, ERC- 1155: Multi Token Standard
   Ramesh A., 2022, Hierarchical text-conditional image generation with clip latents, DOI 10.48550/arXiv.2204.06125
   Richman-Abdou Kelly, 2019, What Is Curating? See Why More and More People Are Interested in Becoming Curators
   Rico Sara Vazquez, 2022, Banking and fintech in the customer experience era
   Rodrigo Pablo, 2021, DICG'21: Proceedings of the 2nd International Workshop on Distributed Infrastructure for Common Good, P5, DOI 10.1145/3493426.3493823
   Rolfe Alex, 2015, The fall and rise of Tokenization
   Rosenfeld M., 2012, Overview of colored coins, V41, P94
   Ryskeldiev B, 2018, ACM INT CONF PR SER, DOI 10.1145/3174910.3174952
   Schaar L, 2022, J BRIT BLOCKCHAIN AS, V5, P22, DOI 10.31585/jbba-5-1-(2)2022
   Schuemie MJ, 2001, CYBERPSYCHOL BEHAV, V4, P183, DOI 10.1089/109493101300117884
   Serada A, 2021, GAMES CULT, V16, P457, DOI 10.1177/1555412019898305
   SevenX Ventures, 2022, Diving into The Web3 Data Sector: Landscape, Layers, and the Future of User Data
   Siapera E, 2015, INFORM COMMUN SOC, V18, P1297, DOI 10.1080/1369118X.2015.1070188
   Skvorc Bruno, 2022, ERC-6059: Parent-Governed Nestable Non-Fungible Tokens
   Steiner Alfred'Dave, 2022, Bored apes and monkey selfies: Copyright and PFP NFTs
   Steinmetz F, 2021, TECHNOL FORECAST SOC, V173, DOI 10.1016/j.techfore.2021.121073
   Stephenson N., 2003, Snow crash: A novel
   Stone AllucqureRosanne., 1996, WAR DESIRE TECHNOLOG
   Sun J., arXiv
   Sun NG, 2022, SUSTAINABILITY-BASEL, V14, DOI 10.3390/su142114584
   Sun Xinyao, 2022, Smart Multimedia: Third International Conference, ICSM 2022, Revised Selected Papers. Lecture Notes in Computer Science (13497), P272, DOI 10.1007/978-3-031-22061-6_20
   Tajfel H., 1985, Differentiation Between Social Groups: Studies in the Social Psychology of Intergroup Relations, V2nd, P7, DOI [10.4324/9780203505984-16, DOI 10.4324/9780203505984-16]
   Tan Joshua, 2022, ERC-4824: Common Interfaces for DAOs
   Tejashwin U, 2023, P 2023 INT C ADV TEC, P1
   Telamon Ardavanis, 2022, Membership NFTs: Blockchain technology, opportunities, and implementation of utility based non-fungible-tokens
   Tencent and Accenture, 2022, Tencent Introduces 'Immersive Convergence' to Drive Connections Between Digital and Real Worlds
   The Sandbox, 2020, The Sandbox Game Maker-Creating Your First Game
   The Sandbox, 2020, What Is The Sandbox?
   Thompson Cam, 2023, Azuki 'Elementals' Mint Mishap Highlights the Fragile State of the NFT Market
   Thompson Cam, 2022, It's Lonely in the Metaverse: DappRadar Data Suggests Decentraland Has 38 'Daily Active' Users in $1.3B Ecosystem
   Tim DaubenschA.tz and Anders, 2022, ERC-5192: Minimal Soulbound NFTs
   Tisue Seth, 2004, INT C COMPL SYST, P16
   Tlili A, 2022, SMART LEARN ENVIRON, V9, DOI 10.1186/s40561-022-00205-x
   Truong VT, 2023, IEEE ACCESS, V11, P26258, DOI 10.1109/ACCESS.2023.3257029
   Tyler Alex, 2022, ERC-5058: Lockable Non-Fungible Tokens
   Vasan K, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-05146-6
   Voshmgir S., 2020, Token Economy: How the Web3 Reinvents the Internet, V2
   W3C Recommendation, 2022, Decentralized Identifiers (DIDs) v1.0
   Walport Mark, 2016, Distributed ledger technology: Beyond blockchain
   Wandmacher R., 2019, Cryptofinance and Mechanisms of Exchange: The Making of Virtual Currency, P113
   Wang AQ, 2022, PROCEEDINGS OF THE 2022 INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, ICMI 2022, P662, DOI 10.1145/3536221.3558176
   Wang H, 2023, IEEE INTERNET THINGS, V10, P14671, DOI 10.1109/JIOT.2023.3278329
   Wang PF, 2019, FINANC RES LETT, V31, P1, DOI 10.1016/j.frl.2019.04.031
   Wang Q, 2021, Arxiv, DOI [arXiv:2105.07447, DOI 10.48550/ARXIV.2105.07447]
   Wang Will, 2020, ERC-3525: Semi-Fungible Token
   Wang YT, 2023, IEEE COMMUN SURV TUT, V25, P319, DOI 10.1109/COMST.2022.3202047
   White Joshua T., 2022, The role of the media in speculative markets: Evidence from non-fungible tokens (NFTs)
   Williams Chris, 2022, Opinion: Of Course Yuga Labs Is Not "Web3
   Worldcoin, 2017, HUMANNESS IN THE AGE OF AI
   Xia PC, 2021, Arxiv, DOI arXiv:2104.05185
   Xia Pengcheng, 2022, P 22 ACM INT MEAS C
   Xu H, 2022, IEEE INT CONF COMM, P7, DOI [10.1109/ICCWorkshops53468.2022.9814538, 10.1109/ICCWORKSHOPS53468.2022.9814538]
   Xu JJ, 2021, ANN EPIDEMIOL, V55, P1, DOI 10.1016/j.annepidem.2020.11.002
   Yao Nanjun, 2022, IEEE Transactions on Computational Social Systems
   Yee N, 2006, CYBERPSYCHOL BEHAV, V9, P772, DOI 10.1089/cpb.2006.9.772
   Yeung K, 2019, MOD LAW REV, V82, P207, DOI 10.1111/1468-2230.12399
   Zhang Dan, 2022, The Metaverse: Opportunities and Challenges for Marketing in Web3
   Zhang LJ, 2022, LECT NOTES COMPUT SC, V12993, P102, DOI 10.1007/978-3-030-96068-1_8
   Zhao SY, 2008, COMPUT HUM BEHAV, V24, P1816, DOI 10.1016/j.chb.2008.02.012
   Zhao Wenbing, 2021, P 3 INT C BLOCKCH TE
   Zheng Shawn, 2023, ERC-6358: Cross-Chain Token States Synchronization
   Zhu QY, 2020, ACM COMPUT SURV, V52, DOI 10.1145/3359982
   Zvarikova K., 2022, REV CONT PHILOS, V21, P171, DOI [10.22381/RCP21202211, DOI 10.22381/RCP21202211]
   Zyda M, 2022, COMPUTER, V55, P124, DOI 10.1109/MC.2021.3130480
NR 221
TC 1
Z9 1
U1 42
U2 42
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2024
VL 20
IS 4
AR 101
DI 10.1145/3630258
PG 42
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6G9
UT WOS:001153382100011
DA 2024-08-05
ER

PT J
AU Feng, ZZ
   Xu, JM
   Ma, L
   Zhang, SL
AF Feng, Zhanzhou
   Xu, Jiaming
   Ma, Lei
   Zhang, Shiliang
TI Efficient Video Transformers via Spatial-temporal Token Merging for
   Action Recognition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Efficient video recognition; deep learning; transformer;
   spatial-temporal information
AB Transformer has exhibited promising performance in various video recognition tasks but brings a huge computational cost in modeling spatial-temporal cues. This work aims to boost the efficiency of existing video transformers for action recognition through eliminating redundancies in their tokens and efficiently learning motion cues of moving objects. We propose a lightweight and plug-and-play module, namely Spatial-temporal Token Merger (STTM), to merge the tokens belonging to the same object into a more compact representation. STTM first adaptively identifies crucial object clues underlying the video as meta tokens. Similarity scores between input tokens and meta tokens are hence computed and used to guide the fusion of similar tokens in both spatial and temporal domains, respectively. To compensate for motion cues lost in the merging procedure, we compute the linear aggregation of spatial-temporal positions of tokens as motion features. STTM hence outputs a compact set of tokens fusing both appearance and motion features of moving objects. This procedure substantially decreases the number of tokens that need to be processed by each Transformer block and boosts the efficiency. As a general module, STTM can be applied to different layers of various video Transformers. Extensive experiments on the action recognition datasets Kinectics-400 and SSv2 demonstrate its promising performance. For example, it reduces the computation complexity of ViT by 38% while maintaining a similar performance on Kinectics-400. It also brings 1.7% gains of top-1 accuracy on SSv2 under the same computational cost.
C1 [Feng, Zhanzhou; Ma, Lei; Zhang, Shiliang] Peking Univ, Sch Comp Sci, Natl Key Lab Multimedia Informat Proc, Beijing 100871, Peoples R China.
   [Xu, Jiaming] Peking Univ, Sch Elect Engn & Comp Sci, Beijing 100871, Peoples R China.
   [Ma, Lei] Peking Univ, Coll Future Technol, Natl Biomed Imaging Ctr, Beijing 100871, Peoples R China.
   [Ma, Lei] Beijing Acad Artificial Intelligence, Beijing 100871, Peoples R China.
C3 Peking University; Peking University; Peking University
RP Feng, ZZ (corresponding author), Peking Univ, Sch Comp Sci, Natl Key Lab Multimedia Informat Proc, Beijing 100871, Peoples R China.
EM fengzz@stu.pku.edu.cn; 2000012915@stu.pku.edu.cn; lei.ma@pku.edu.cn;
   slzhang.jdl@pku.edu.cn
RI Zhang, ShiLiang/AAA-4638-2020
OI Ma, Lei/0000-0001-6024-3854
FU National Key Research and Development Program of China [2018YFE0118400];
   Natural Science Foundation of China [U20B2052, 61936011]
FX This work is supported in part by The National Key Research and
   Development Program of China under Grant No. 2018YFE0118400, in part by
   Natural Science Foundation of China under Grant No. U20B2052, 61936011.
CR Abnar S, 2020, Arxiv, DOI [arXiv:2005.00928, 10.48550/arXiv.2005.00928, DOI 10.48550/ARXIV.2005.00928]
   Arnab A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6816, DOI 10.1109/ICCV48922.2021.00676
   Bertasius G, 2021, PR MACH LEARN RES, V139
   Bolya D, 2023, Arxiv, DOI arXiv:2210.09461
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen JW, 2022, IEEE WINT CONF APPL, P786, DOI 10.1109/WACV51458.2022.00086
   Chen YP, 2019, IEEE I CONF COMP VIS, P3434, DOI 10.1109/ICCV.2019.00353
   Chen Zhen, 2023, ACM Transactions on Multimedia Computing, Communications and Applications (TOMM'23), V19, P1
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Fan QF, 2019, ADV NEUR IN, V32
   Fan Quanfu, 2021, arXiv
   Feichtenhofer C, 2020, PROC CVPR IEEE, P200, DOI 10.1109/CVPR42600.2020.00028
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Feng Zhanzhou, 2023, 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), P10386, DOI 10.1109/CVPR52729.2023.01001
   Feng ZZ, 2023, IEEE T IMAGE PROCESS, V32, P4156, DOI 10.1109/TIP.2023.3293763
   Gowda SN, 2021, AAAI CONF ARTIF INTE, V35, P1451
   Goyal R, 2017, IEEE I CONF COMP VIS, P5843, DOI 10.1109/ICCV.2017.622
   Graham B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12239, DOI 10.1109/ICCV48922.2021.01204
   Han Tengda, 2022, arXiv
   He KM, 2022, PROC CVPR IEEE, P15979, DOI 10.1109/CVPR52688.2022.01553
   Huang H., 2023, P IEEE CVF C COMP VI, P22690
   Huang Ziyuan, 2022, INT C LEARNING REPRE
   Jiang BY, 2019, IEEE I CONF COMP VIS, P2000, DOI 10.1109/ICCV.2019.00209
   Jiang ZH, 2021, ADV NEUR IN, V34
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   Kondratyuk D, 2021, PROC CVPR IEEE, P16015, DOI 10.1109/CVPR46437.2021.01576
   Korbar B, 2019, IEEE I CONF COMP VIS, P6241, DOI 10.1109/ICCV.2019.00633
   Li WX, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3485472
   Li Y, 2020, PROC CVPR IEEE, P906, DOI 10.1109/CVPR42600.2020.00099
   Li Z., 2021, NEURIPS, P13165
   Liang Y, 2022, arXiv
   Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718
   Liu Y, 2024, Arxiv, DOI [arXiv:2106.03180, DOI 10.48550/ARXIV.2106.03180]
   Liu Z, 2022, PROC CVPR IEEE, P3192, DOI 10.1109/CVPR52688.2022.00320
   Long SF, 2023, PROC CVPR IEEE, P10334, DOI 10.1109/CVPR52729.2023.00996
   Loshchilov I, 2019, Arxiv, DOI [arXiv:1711.05101, 10.48550/arXiv.1711.05101, DOI 10.48550/ARXIV.1711.05101]
   Marin D, 2021, Arxiv, DOI arXiv:2110.03860
   Neimark D, 2021, IEEE INT CONF COMP V, P3156, DOI [arXiv:2102.00719, 10.1109/ICCVW54120.2021.00355]
   Park SH, 2022, LECT NOTES COMPUT SC, V13695, P160, DOI 10.1007/978-3-031-19833-5_10
   Patrick M, 2021, ADV NEUR IN, V34
   Rao YM, 2021, 35 C NEURAL INFORM P, V34
   Shu XB, 2022, IEEE T PATTERN ANAL, V44, P3300, DOI 10.1109/TPAMI.2021.3050918
   Shu XB, 2021, IEEE T PATTERN ANAL, V43, P1110, DOI 10.1109/TPAMI.2019.2942030
   Tang H, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3571735
   Tang JH, 2022, IEEE T PATTERN ANAL, V44, P636, DOI 10.1109/TPAMI.2019.2928540
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang JK, 2022, LECT NOTES COMPUT SC, V13695, P69, DOI 10.1007/978-3-031-19833-5_5
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wang XH, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3569584
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Weng ZJ, 2024, ACM T MULTIM COMPUT, V20, DOI 10.1145/3572776
   Wu CY, 2020, PROC CVPR IEEE, P150, DOI 10.1109/CVPR42600.2020.00023
   Wu CY, 2018, PROC CVPR IEEE, P6026, DOI 10.1109/CVPR.2018.00631
   Wu ZX, 2019, PROC CVPR IEEE, P1278, DOI 10.1109/CVPR.2019.00137
   Xie EZ, 2021, ADV NEUR IN, V34
   Xu BQ, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3247103
   Xu Binqian, 2023, arXiv
   Xu HT, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3538749
   Xu Ruihan, 2023, P IEEE CVF INT C COM, P5752
   Yang CHY, 2023, Arxiv, DOI arXiv:2212.06795
   Zeng W, 2022, PROC CVPR IEEE, P11091, DOI 10.1109/CVPR52688.2022.01082
   Zhang H, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P917, DOI 10.1145/3474085.3475272
   Zhang YY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13557, DOI [10.1109/ICCV48922.2021.01332, 10.1109/iccv48922.2021.01332]
   Zhao MY, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3579359
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhu DD, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3576857
   Zhu XZ, 2021, Arxiv, DOI [arXiv:2010.04159, 10.48550/arXiv.2010.04159]
   Zolfaghari M, 2018, LECT NOTES COMPUT SC, V11206, P713, DOI 10.1007/978-3-030-01216-8_43
NR 71
TC 0
Z9 0
U1 8
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2024
VL 20
IS 4
AR 120
DI 10.1145/3633781
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6G9
UT WOS:001153382100030
DA 2024-08-05
ER

PT J
AU Peng, M
   Shao, XH
   Shi, Y
   Zhou, XD
AF Peng, Min
   Shao, Xiaohu
   Shi, Yu
   Zhou, Xiangdong
TI Hierarchical Synergy-Enhanced Multimodal Relational Network for
   VideoQuestion Answering
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Video question answering; multimodal learning; attention mechanisms;
   multiscale semantics
ID VIDEO
AB Video question answering (VideoQA) is challenging as it requires reasoning about natural language and multi-modal interactive relations. Most existing methods apply attention mechanisms to extract interactions between the question and the video or to extract effective spatio-temporal relational representations. However, these methods neglect the implication of relations between intra- and inter-modal interactions for multimodal learning, and they fail to fully exploit the synergistic effect of multiscale semantics in answer reasoning. In this article, we propose a novel hierarchical synergy-enhanced multimodal relational network (HMRNet) to address these issues. Specifically, we devise (i) a compact and unified relation-oriented interaction module that explores the relation between intra- and inter-modal interactions to enable effective multimodal learning; and (ii) a hierarchical synergistic memory unit that leverages a memory-based interaction scheme to complement and fuse multimodal semantics at multiple scales to achieve synergistic enhancement of answer reasoning. With careful design of each component, our HMRNet has fewer parameters and is computationally efficient. Extensive experiments and qualitative analyses demonstrate that the HMRNet is superior to previous state-of-the-art methods on eight benchmark datasets. We also demonstrate the effectiveness of the different components of our method.
C1 [Peng, Min; Shi, Yu; Zhou, Xiangdong] Chinese Acad Sci, Chongqing Inst Green & Intelligent Technol, 266 Fangzheng Ave, Chongqing 400714, Peoples R China.
   [Peng, Min] Univ Chinese Acad Sci, Chongqing Sch, 266 Fangzheng Ave, Chongqing 400714, Peoples R China.
   [Shao, Xiaohu] Beijing IDRIVERPLUS Technol Co Ltd, 66 Xixiaokou Rd, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Chongqing Institute of Green & Intelligent
   Technology, CAS; Chinese Academy of Sciences; University of Chinese
   Academy of Sciences, CAS
RP Peng, M (corresponding author), Chinese Acad Sci, Chongqing Inst Green & Intelligent Technol, 266 Fangzheng Ave, Chongqing 400714, Peoples R China.; Peng, M (corresponding author), Univ Chinese Acad Sci, Chongqing Sch, 266 Fangzheng Ave, Chongqing 400714, Peoples R China.
EM pengmin@cigit.ac.cn; shaoxi-aohu@cigit.ac.cn; shiyu@cigit.ac.cn;
   zhouxiangdong@cigit.ac.cn
FU National Natural Science Foundation of China [62106247]
FX This work is funded by the National Natural Science Foundation of China
   (Grant No. 62106247).
CR Amrani E, 2021, AAAI CONF ARTIF INTE, V35, P6644
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Cho K., 2014, EMNLP, DOI 10.3115/v1/w14-4012
   Dang Long Hoang, 2021, P 30 INT JOINT C ART, P636, DOI DOI 10.24963/IJCAI.2021/88
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Fan CY, 2019, PROC CVPR IEEE, P1999, DOI 10.1109/CVPR.2019.00210
   Gao JY, 2018, PROC CVPR IEEE, P6576, DOI 10.1109/CVPR.2018.00688
   Gao LL, 2022, IEEE T IMAGE PROCESS, V31, P202, DOI 10.1109/TIP.2021.3120867
   Gao LL, 2021, PATTERN RECOGN, V120, DOI 10.1016/j.patcog.2021.108145
   Gao LL, 2020, NEUROCOMPUTING, V395, P222, DOI 10.1016/j.neucom.2018.06.096
   Gao LL, 2019, AAAI CONF ARTIF INTE, P6391
   Gu M, 2021, IEEE T IMAGE PROCESS, V30, P2758, DOI 10.1109/TIP.2021.3051756
   Guo ZY, 2021, IEEE T CIRC SYST VID, V31, P1697, DOI 10.1109/TCSVT.2020.3014775
   Guo ZC, 2021, ACL-IJCNLP 2021: THE 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 2, P973
   Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang D, 2020, AAAI CONF ARTIF INTE, V34, P11021
   Jang Y, 2019, INT J COMPUT VISION, V127, P1385, DOI 10.1007/s11263-019-01189-x
   Jang Y, 2017, PROC CVPR IEEE, P1359, DOI 10.1109/CVPR.2017.149
   Jiang JW, 2020, AAAI CONF ARTIF INTE, V34, P11101
   Jiang P, 2020, AAAI CONF ARTIF INTE, V34, P11109
   Jin WK, 2021, IEEE T IMAGE PROCESS, V30, P5477, DOI 10.1109/TIP.2021.3076556
   Jin WK, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3321505
   Kim KM, 2018, LECT NOTES COMPUT SC, V11219, P698, DOI 10.1007/978-3-030-01267-0_41
   Le TM, 2021, INT J COMPUT VISION, V129, P3027, DOI 10.1007/s11263-021-01514-3
   Lei J, 2021, PROC CVPR IEEE, P7327, DOI 10.1109/CVPR46437.2021.00725
   Lei J, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P1369
   Li XP, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1166, DOI 10.1145/3343031.3350971
   Li XP, 2019, AAAI CONF ARTIF INTE, P8658
   Li XR, 2022, MULTIMEDIA SYST, V28, P161, DOI 10.1007/s00530-021-00805-6
   Liang JW, 2018, PROC CVPR IEEE, P6135, DOI 10.1109/CVPR.2018.00642
   Lin TW, 2018, LECT NOTES COMPUT SC, V11208, P3, DOI 10.1007/978-3-030-01225-0_1
   Liu F, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1678, DOI 10.1109/ICCV48922.2021.00172
   Liu F, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4253, DOI 10.1145/3394171.3413649
   Liu Y, 2022, PATTERN RECOGN, V132, DOI 10.1016/j.patcog.2022.108959
   Long X, 2022, IEEE T PATTERN ANAL, V44, P2140, DOI 10.1109/TPAMI.2020.3029554
   Manning CD, 2014, PROCEEDINGS OF 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: SYSTEM DEMONSTRATIONS, P55, DOI 10.3115/v1/p14-5010
   Park J, 2021, PROC CVPR IEEE, P15521, DOI 10.1109/CVPR46437.2021.01527
   Peng Min, 2022, P 31 INT JOINT C ART, P1276, DOI [10.24963/ijcai.2022/178, DOI 10.24963/IJCAI.2022/178]
   Pennington J., 2014, GLOVE GLOBAL VECTORS, DOI DOI 10.3115/V1/D14-1162
   Ren MY, 2015, ADV NEUR IN, V28
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Seo A., 2021, P 59 ANN M ASS COMP, V1, P6167, DOI 10.18653/v1/2021.acllong.481
   Seo PH, 2021, PROC CVPR IEEE, P16872, DOI 10.1109/CVPR46437.2021.01660
   Thao Minh Le, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9969, DOI 10.1109/CVPR42600.2020.00999
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang B, 2018, AAAI CONF ARTIF INTE, P7380
   Wang JY, 2021, IEEE T MULTIMEDIA, V24, P3369, DOI 10.1109/TMM.2021.3097171
   Xiao JB, 2022, LECT NOTES COMPUT SC, V13696, P39, DOI 10.1007/978-3-031-20059-5_3
   Xiao JB, 2022, AAAI CONF ARTIF INTE, P2804
   Xiao JB, 2021, PROC CVPR IEEE, P9772, DOI 10.1109/CVPR46437.2021.00965
   Xiao S., 2022, PROC C EMPIR ICAL ME, P8188
   Xiao SN, 2021, AAAI CONF ARTIF INTE, V35, P2986
   Xiao SN, 2020, NEURAL PROCESS LETT, V52, P993, DOI 10.1007/s11063-019-10003-1
   Xu DJ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1645, DOI 10.1145/3123266.3123427
   Xu L, 2021, PROC CVPR IEEE, P9873, DOI 10.1109/CVPR46437.2021.00975
   Xue HW, 2022, PROC CVPR IEEE, P5026, DOI 10.1109/CVPR52688.2022.00498
   Xue HY, 2017, IEEE T IMAGE PROCESS, V26, P5656, DOI 10.1109/TIP.2017.2746267
   Yang A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1666, DOI 10.1109/ICCV48922.2021.00171
   Yang ZK, 2020, IEEE WINT CONF APPL, P1545, DOI [10.1109/WACV45572.2020.9093596, 10.1109/wacv45572.2020.9093596]
   Ye YN, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P829, DOI 10.1145/3077136.3080655
   Yu T, 2021, IEEE T CIRC SYST VID, V31, P931, DOI 10.1109/TCSVT.2020.2995959
   Yu T, 2020, IEEE T IMAGE PROCESS, V29, P1204, DOI 10.1109/TIP.2019.2940677
   Yu WJ, 2021, ADV NEUR IN, V34
   Yu Z, 2019, AAAI CONF ARTIF INTE, P9127
   Zadeh A, 2019, PROC CVPR IEEE, P8799, DOI 10.1109/CVPR.2019.00901
   Zellers R, 2021, Advances in Neural Information Processing Systems, V34
   Zeng PP, 2022, IEEE T IMAGE PROCESS, V31, P5936, DOI 10.1109/TIP.2022.3205212
   Zha ZJ, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3320061
   Zhang H, 2022, IEEE T PATTERN ANAL, V44, P4252, DOI 10.1109/TPAMI.2021.3060449
   Zhang HA, 2023, PATTERN RECOGN, V138, DOI 10.1016/j.patcog.2023.109339
   Zhang JP, 2022, IEEE T CIRC SYST VID, V32, P63, DOI 10.1109/TCSVT.2020.3048440
   Zhao Z, 2019, IEEE T IMAGE PROCESS, V28, P5939, DOI 10.1109/TIP.2019.2922062
   Zhuang YT, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3366710
NR 74
TC 0
Z9 0
U1 8
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2024
VL 20
IS 4
AR 91
DI 10.1145/3630101
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6G9
UT WOS:001153382100001
OA Bronze
DA 2024-08-05
ER

PT J
AU Wang, DW
   Yang, GB
   Guo, ZQ
   Chen, JY
AF Wang, Dewang
   Yang, Gaobo
   Guo, Zhiqing
   Chen, Jiyou
TI Enhancing Adversarial Embedding based Image Steganography via Clustering
   Modification Directions
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image steganography; adversarial embedding; convolutional neural
   network; steganalysis
ID STEGANALYSIS
AB Image steganography is a technique used to conceal secret information within cover images without being detected. However, the advent of convolutional neural networks (CNNs) has threatened the security of image steganography. Due to the inherent properties of adversarial examples, adding perturbations to stego images can mislead the CNN-based image steganalysis, but it also easily leads to some errors when extracting secret information. Recently, some adversarial embedding methods have been proposed for improving image steganography security. In this work, we aim at furthering enhance the security of adversarial embedding-based image steganography by exploiting the strong correlation between adjacent pixels. Specifically, we divide the cover image into four non-overlapping parts for four-stage information embedding. During the adversarial embedding process, we cluster the modification directions of adjacent pixels and select only those with relatively larger amplitudes of gradients and smaller embedding costs to update their original embedding costs. Experimental results demonstrate that our proposed method can effectively fool targeted steganalyzers and outperform state-of-the-art techniques under different scenarios.
C1 [Wang, Dewang; Yang, Gaobo; Guo, Zhiqing; Chen, Jiyou] Hunan Univ, Lushan Nan Rd, Changsha 410082, Peoples R China.
C3 Hunan University
RP Yang, GB (corresponding author), Hunan Univ, Lushan Nan Rd, Changsha 410082, Peoples R China.
EM dewang_wang@126.com; yanggaobo@hnu.edu.cn; guozhiqing@hnu.edu.cn;
   jiyouchen@hnu.edu.cn
OI Wang, Dewang/0000-0003-1860-3021; chen, jiyou/0000-0001-8883-9430; Yang,
   Gaobo/0000-0003-2734-659X
FU National Natural Science Foundation of China [61972143]
FX This work is supported in part by the National Natural Science
   Foundation of China (61972143).
CR Bas Patrick, 2011, Information Hiding. 13th International Conference, IH 2011. Revised Selected Papers, P59, DOI 10.1007/978-3-642-24178-9_5
   Bas P, 2017, Image database of BOWS-2
   Bender W, 1996, IBM SYST J, V35, P313, DOI 10.1147/sj.353.0313
   Bernard S, 2021, IEEE T INF FOREN SEC, V16, P812, DOI 10.1109/TIFS.2020.3021913
   Boroumand M, 2019, IEEE T INF FOREN SEC, V14, P1181, DOI 10.1109/TIFS.2018.2871749
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Denemark T, 2015, P 3 ACM WORKSH INF H, P5
   Denemark T, 2014, IEEE INT WORKS INFOR, P48, DOI 10.1109/WIFS.2014.7084302
   Deng XQ, 2019, IH&MMSEC '19: PROCEEDINGS OF THE ACM WORKSHOP ON INFORMATION HIDING AND MULTIMEDIA SECURITY, P230, DOI 10.1145/3335203.3335739
   Filler T, 2011, IEEE T INF FOREN SEC, V6, P920, DOI 10.1109/TIFS.2011.2134094
   Filler T, 2010, IEEE T INF FOREN SEC, V5, P705, DOI 10.1109/TIFS.2010.2077629
   Fridrich J, 2007, PROC SPIE, V6505, DOI 10.1117/12.697471
   Fridrich J, 2012, IEEE T INF FOREN SEC, V7, P868, DOI 10.1109/TIFS.2012.2190402
   Goodfellow I. J., 2014, ADV NEURAL INFORM PR
   Holub V, 2014, EURASIP J INF SECUR, DOI 10.1186/1687-417X-2014-1
   Holub V, 2012, IEEE INT WORKS INFOR, P234, DOI 10.1109/WIFS.2012.6412655
   Goodfellow IJ, 2015, Arxiv, DOI [arXiv:1412.6572, DOI 10.48550/ARXIV.1412.6572]
   Li B, 2014, IEEE IMAGE PROC, P4206, DOI 10.1109/ICIP.2014.7025854
   Li B, 2015, IEEE T INF FOREN SEC, V10, P1905, DOI 10.1109/TIFS.2015.2434600
   Liu ML, 2023, IEEE T DEPEND SECURE, V20, P2375, DOI 10.1109/TDSC.2022.3182041
   Liu ML, 2021, IEEE T INF FOREN SEC, V16, P4621, DOI 10.1109/TIFS.2021.3111748
   Ma S, 2019, MULTIMED TOOLS APPL, V78, P32503, DOI 10.1007/s11042-019-07994-3
   Madry A, 2019, Arxiv, DOI arXiv:1706.06083
   Papernot N, 2016, Arxiv, DOI [arXiv:1605.07277, 10.48550/arXiv.1605.07277]
   Papernot N, 2016, 1ST IEEE EUROPEAN SYMPOSIUM ON SECURITY AND PRIVACY, P372, DOI 10.1109/EuroSP.2016.36
   Pevny T, 2010, LECT NOTES COMPUT SC, V6387, P161, DOI 10.1007/978-3-642-16435-4_13
   Pevny T, 2008, LECT NOTES COMPUT SC, V5284, P251
   Sedighi V, 2016, IEEE T INF FOREN SEC, V11, P221, DOI 10.1109/TIFS.2015.2486744
   Shi HC, 2018, LECT NOTES COMPUT SC, V10735, P534, DOI 10.1007/978-3-319-77380-3_51
   Shi W., 2022, ACM T MULTIM COMPUT, V18, P1
   Tang WX, 2019, IEEE T INF FOREN SEC, V14, P2074, DOI 10.1109/TIFS.2019.2891237
   Tang WX, 2017, IEEE SIGNAL PROC LET, V24, P1547, DOI 10.1109/LSP.2017.2745572
   Volkhonskiy D, 2020, PROC SPIE, V11433, DOI 10.1117/12.2559429
   Xu GS, 2016, IEEE SIGNAL PROC LET, V23, P708, DOI 10.1109/LSP.2016.2548421
   Yang JH, 2020, IEEE T INF FOREN SEC, V15, P839, DOI 10.1109/TIFS.2019.2922229
   Ye J, 2017, IEEE T INF FOREN SEC, V12, P2545, DOI 10.1109/TIFS.2017.2710946
   Zhang YW, 2018, PROCEEDINGS OF THE 6TH ACM WORKSHOP ON INFORMATION HIDING AND MULTIMEDIA SECURITY (IH&MMSEC'18), P67, DOI 10.1145/3206004.3206012
NR 37
TC 0
Z9 0
U1 6
U2 20
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2024
VL 20
IS 1
AR 20
DI 10.1145/3603377
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T8LL3
UT WOS:001080441800020
DA 2024-08-05
ER

PT J
AU Zheng, CY
   Song, N
   Zhang, RY
   Huang, L
   Wei, ZQ
   Nie, J
AF Zheng, Chengyu
   Song, Ning
   Zhang, Ruoyu
   Huang, Lei
   Wei, Zhiqiang
   Nie, Jie
TI Scale-Semantic Joint Decoupling Network for Image-Text Retrieval in
   Remote Sensing
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Remote sensing; scale-semantic joint decoupling; image-text retrieval
AB Image-text retrieval in remote sensing aims to provide flexible information for data analysis and application. In recent years, state-of-the-art methods are dedicated to "scale decoupling" and "semantic decoupling" strategies to further enhance the capability of representation. However, these previous approaches focus on either the disentangling scale or semantics but ignore merging these two ideas in a union model, which extremely limits the performance of cross-modal retrieval models. To address these issues, we propose a novel Scale-Semantic Joint Decoupling Network (SSJDN) for remote sensing image-text retrieval. Specifically, we design the Bidirectional Scale Decoupling (BSD) module, which exploits Salience Extraction Map (SEM) and Salience Suppression Map (SSM) units to adaptively extract potential features and suppress cumbersome features at other scales in a bidirectional pattern to yield different scale clues. Besides, we design the Label-supervised Semantic Decoupling (LSD) module by leveraging the category semantic labels as prior knowledge to supervise images and texts probing significant semantic-related information. Finally, we design a Semantic-guided Triple Loss (STL), which adaptively generates a constant to adjust the loss function to improve the probability of matching the same semantic image and text and shorten the convergence time of the retrieval model. Our proposed SSJDN outperforms state-of-the-art approaches in numerical experiments conducted on four benchmark remote sensing datasets.
C1 [Zheng, Chengyu; Song, Ning; Zhang, Ruoyu; Huang, Lei; Wei, Zhiqiang; Nie, Jie] Ocean Univ China, Coll Informat Sci & Engn, Songling Rd 238, Qingdao 266005, Shandong, Peoples R China.
C3 Ocean University of China
RP Nie, J (corresponding author), Ocean Univ China, Coll Informat Sci & Engn, Songling Rd 238, Qingdao 266005, Shandong, Peoples R China.
EM zhengchengyu@stu.ouc.edu.cn; songning@stu.ouc.edu.cn;
   zhangruoyu@stu.ouc.edu.cn; huangl@ouc.edu.cn; weizhiqiang@ouc.edu.cn;
   niejie@ouc.edu.cn
RI Nie, Jie/ABG-9228-2021; wei, zhiqiang/M-8868-2013
OI Nie, Jie/0000-0003-4952-7666; Huang, Lei/0000-0003-4087-3677
FU National Natural Science Foundation of China [62072418]; Fundamental
   Research Funds for the Central Universities [202042008]; National Key
   Research and Development Program of China [2021YFF0704000]
FX This work was supported in part by the National Natural Science
   Foundation of China (62172376), the National Natural Science Foundation
   of China (62072418), the Fundamental Research Funds for the Central
   Universities (202042008) and the National Key Research and Development
   Program of China (2021YFF0704000).
CR Abdullah T, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12030405
   Bai C, 2022, IEEE J-STARS, V15, P7333, DOI 10.1109/JSTARS.2022.3202246
   Bai C, 2022, IEEE T CYBERNETICS, V52, P12538, DOI 10.1109/TCYB.2021.3080121
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen TS, 2019, IEEE I CONF COMP VIS, P522, DOI 10.1109/ICCV.2019.00061
   Cheng QM, 2021, IEEE J-STARS, V14, P4284, DOI 10.1109/JSTARS.2021.3070872
   Faghri F, 2018, Arxiv, DOI arXiv:1707.05612
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hoxha G, 2020, IEEE J-STARS, V13, P4462, DOI 10.1109/JSTARS.2020.3013818
   Hoxha G, 2020, 2020 MEDITERRANEAN AND MIDDLE-EAST GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (M2GARSS), P1, DOI [10.1109/M2GARSS47143.2020.9105191, 10.1109/m2garss47143.2020.9105191]
   Huang Y, 2018, PROC CVPR IEEE, P6163, DOI 10.1109/CVPR.2018.00645
   Kiros R, 2015, 29 ANN C NEURAL INFO, V28
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Li XL, 2021, IEEE T GEOSCI REMOTE, V59, P5246, DOI 10.1109/TGRS.2020.3010106
   Liu C, 2019, INT GEOSCI REMOTE SE, P4324, DOI [10.1109/igarss.2019.8900431, 10.1109/IGARSS.2019.8900431]
   Liu YS, 2020, IEEE T GEOSCI REMOTE, V58, P7872, DOI 10.1109/TGRS.2020.2984703
   Liu YS, 2020, IEEE J-STARS, V13, P1119, DOI 10.1109/JSTARS.2020.2981372
   Lu XQ, 2020, IEEE T GEOSCI REMOTE, V58, P1985, DOI 10.1109/TGRS.2019.2951636
   Lu XX, 2018, IEEE T GEOSCI REMOTE, V56, P2183, DOI 10.1109/TGRS.2017.2776321
   Lv YF, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2021.3131592
   Napoletano P, 2018, INT J REMOTE SENS, V39, P1343, DOI 10.1080/01431161.2017.1399472
   Nogueira K, 2018, IEEE GEOSCI REMOTE S, V15, P1446, DOI 10.1109/LGRS.2018.2845549
   Qu B, 2016, INT CONF COMP INFO, P124
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shi ZW, 2017, IEEE T GEOSCI REMOTE, V55, P3623, DOI 10.1109/TGRS.2017.2677464
   Sukhia KN, 2020, DIGIT SIGNAL PROCESS, V104, DOI 10.1016/j.dsp.2020.102765
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Wang BQ, 2019, IEEE GEOSCI REMOTE S, V16, P1274, DOI 10.1109/LGRS.2019.2893772
   Wang T, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P12, DOI 10.1145/3343031.3350875
   Wang ZH, 2019, IEEE I CONF COMP VIS, P5763, DOI 10.1109/ICCV.2019.00586
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xiong W, 2020, IEEE J-STARS, V13, P5284, DOI 10.1109/JSTARS.2020.3021390
   Yang Y., 2010, P 18 SIGSPATIAL INT, DOI [10.1145/1869790.1869829, DOI 10.1145/1869790.1869829]
   Yao FL, 2023, IEEE J-STARS, V16, P688, DOI 10.1109/JSTARS.2022.3226325
   Ye DJ, 2017, ISPRS INT J GEO-INF, V6, DOI 10.3390/ijgi6110364
   Yu Hongfeng, 2022, IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
   Yuan ZW, 2022, INT J RAIL TRANSP, V10, P216, DOI 10.1080/23248378.2021.1882890
   Yuan ZQ, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3163706
   Yuan ZQ, 2022, INT J APPL EARTH OBS, V115, DOI 10.1016/j.jag.2022.103071
   Yuan ZQ, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3078451
   Zhang F, 2015, IEEE T GEOSCI REMOTE, V53, P2175, DOI 10.1109/TGRS.2014.2357078
   Zhang K, 2022, PROC CVPR IEEE, P15640, DOI 10.1109/CVPR52688.2022.01521
   Zhang XR, 2017, INT GEOSCI REMOTE SE, P4798, DOI 10.1109/IGARSS.2017.8128075
   Zhang XT, 2019, INT GEOSCI REMOTE SE, P10039, DOI [10.1109/IGARSS.2019.8900503, 10.1109/igarss.2019.8900503]
   Zhang Z, 2023, IEEE T KNOWL DATA EN, V35, P5091, DOI 10.1109/TKDE.2022.3144352
   Zhang Z, 2021, IEEE T NEUR NET LEAR, V32, P4514, DOI 10.1109/TNNLS.2020.3018790
   Zhang Z, 2019, IEEE T IMAGE PROCESS, V28, P4803, DOI 10.1109/TIP.2019.2912290
   Zou C, 2018, PROC SPIE, V10806, DOI 10.1117/12.2503185
NR 48
TC 3
Z9 3
U1 5
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2024
VL 20
IS 1
AR 4
DI 10.1145/3603628
PG 20
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T8LL3
UT WOS:001080441800004
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Jiang, XR
   Yao, YZ
   Liu, S
   Shen, FM
   Nie, LQ
   Hua, XS
AF Jiang, Xiruo
   Yao, Yazhou
   Liu, Sheng
   Shen, Fumin
   Nie, Liqiang
   Hua, Xian-Sheng
TI Dual Dynamic Threshold Adjustment Strategy
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Deep metric learning; sample mining strategy; image retrieval
AB Loss functions and sample mining strategies are essential components in deep metric learning algorithms. However, the existing loss function or mining strategy often necessitates the incorporation of additional hyperparameters, notably the threshold, which defines whether the sample pair is informative. The threshold provides a stable numerical standard for determining whether to retain the pairs. It is a vital parameter to reduce the redundant sample pairs participating in training. Nonetheless, finding the optimal threshold can be a time-consuming endeavor, often requiring extensive grid searches. Because the threshold cannot be dynamically adjusted in the training stage, we should conduct plenty of repeated experiments to determine the threshold. Therefore, we introduce a novel approach for adjusting the thresholds associated with both the loss function and the samplemining strategy. We design a static Asymmetric SampleMining Strategy (ASMS) and its dynamic version, the Adaptive Tolerance ASMS (AT-ASMS), tailored for sample mining methods. ASMS utilizes differentiated thresholds to address the problems (too few positive pairs and too many redundant negative pairs) caused by only applying a single threshold to filter samples. The AT-ASMS can adaptively regulate the ratio of positive and negative pairs during training according to the ratio of the currently mined positive and negative pairs. This meta-learning-based threshold generation algorithm utilizes a single-step gradient descent to obtain new thresholds. We combine these two threshold adjustment algorithms to form the Dual Dynamic Threshold Adjustment Strategy (DDTAS). Experimental results show that our algorithm achieves competitive performance on the CUB200, Cars196, and SOP datasets. Our codes are available at https://github.com/NUST-Machine-Intelligence-Laboratory/DDTAS.
C1 [Jiang, Xiruo; Yao, Yazhou] Nanjing Univ Sci & Technol, Comp Sci & Engn, Nanjing, Jiangsu, Peoples R China.
   [Liu, Sheng] Beihang Univ, Sch Comp Sci & Engn, Beijing, Peoples R China.
   [Shen, Fumin] Univ Elect Sci & Technol China, Chengdu, Peoples R China.
   [Nie, Liqiang] Harbin Inst Technol Shenzhen, Sch Comp Sci & Technol, Shenzhen, Peoples R China.
   [Hua, Xian-Sheng] Terminus Grp, Beijing, Peoples R China.
C3 Nanjing University of Science & Technology; Beihang University;
   University of Electronic Science & Technology of China; Harbin Institute
   of Technology
RP Yao, YZ (corresponding author), Nanjing Univ Sci & Technol, Comp Sci & Engn, Nanjing, Jiangsu, Peoples R China.
EM jiangxiruo622@njust.edu.cn; yazhou.yao@njust.edu.cn;
   liu_sheng@buaa.edu.cn; fumin.shen@gmail.com; nieliqiang@gmail.com;
   huaxiansheng@gmail.com
OI Yao, Yazhou/0000-0002-0337-9410; Hua, Xian-Sheng/0000-0002-8232-5049
FU National Natural Science Foundation of China [62102182]; Fundamental
   Research Funds for the Central Universities [30923010303]
FX This work was supported by the National Natural Science Foundation of
   China (grant no. 62102182) and Fundamental Research Funds for the
   Central Universities (grant no. 30923010303).
CR Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339
   Chen HZ, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3572777
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Finn C, 2017, PR MACH LEARN RES, V70
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Hadsell R., 2006, INT C COMPUTER VISIO, P1735, DOI DOI 10.1109/CVPR.2006.100
   Harwood B, 2017, IEEE I CONF COMP VIS, P2840, DOI 10.1109/ICCV.2017.307
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He XW, 2018, PROC CVPR IEEE, P1945, DOI 10.1109/CVPR.2018.00208
   Hoi SCH, 2010, ACM T MULTIM COMPUT, V6, DOI 10.1145/1823746.1823752
   Hu JL, 2014, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2014.242
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Jiang W, 2021, IEEE T CIRC SYST VID, V31, P1091, DOI 10.1109/TCSVT.2020.2995754
   Jiang XR, 2024, IEEE T NEUR NET LEAR, V35, P5103, DOI 10.1109/TNNLS.2022.3202571
   Kim DH, 2021, PATTERN RECOGN, V110, DOI 10.1016/j.patcog.2020.107643
   Kim S, 2020, PROC CVPR IEEE, P3235, DOI 10.1109/CVPR42600.2020.00330
   Kim W, 2018, LECT NOTES COMPUT SC, V11205, P760, DOI 10.1007/978-3-030-01246-5_45
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Liang CH, 2021, PATTERN RECOGN LETT, V150, P49, DOI 10.1016/j.patrec.2021.06.027
   Liu DY, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3522714
   LOWE DG, 1995, NEURAL COMPUT, V7, P72, DOI 10.1162/neco.1995.7.1.72
   Mao JZ, 2023, IEEE T MULTIMEDIA, V25, P1592, DOI 10.1109/TMM.2023.3265159
   Movshovitz-Attias Y, 2017, IEEE I CONF COMP VIS, P360, DOI 10.1109/ICCV.2017.47
   Munkhdalai T, 2017, PR MACH LEARN RES, V70
   Ni JZ, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P1189, DOI 10.1145/3132847.3133022
   Nichol A, 2018, Arxiv, DOI arXiv:1803.02999
   Opitz M, 2020, IEEE T PATTERN ANAL, V42, P276, DOI 10.1109/TPAMI.2018.2848925
   Opitz M, 2017, IEEE I CONF COMP VIS, P5199, DOI 10.1109/ICCV.2017.555
   Qian Q, 2019, IEEE I CONF COMP VIS, P6459, DOI 10.1109/ICCV.2019.00655
   Ravi S., 2017, INT C LEARN REPR
   Ren MY, 2018, PR MACH LEARN RES, V80
   Rippel O, 2016, Arxiv, DOI arXiv:1511.05939
   Roth K, 2020, PROC CVPR IEEE, P6567, DOI 10.1109/CVPR42600.2020.00660
   Santoro A, 2016, PR MACH LEARN RES, V48
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sharma P, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3511021
   Sheng Mengmeng, 2024, P AAAI C ART INT, P1
   Snell J., 2017, Advances in Neural Information Processing Systems, V30, P4077
   Sohn K, 2016, ADV NEUR IN, V29
   Song HO, 2017, PROC CVPR IEEE, P2206, DOI 10.1109/CVPR.2017.237
   Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434
   Sun YF, 2020, PROC CVPR IEEE, P6397, DOI 10.1109/CVPR42600.2020.00643
   Sun ZR, 2022, PROC CVPR IEEE, P5301, DOI 10.1109/CVPR52688.2022.00524
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Tang Yin, 2023, IEEE Trans. Circuits Syst. Video Technol.
   Vasudeva Bhavya, 2021, P IEEE INT C COMP VI, P10634
   Verma VK, 2020, AAAI CONF ARTIF INTE, V34, P6062
   Vinyals O., 2016, Advances in neural information processing systems, V29
   Wah C., 2011, Tech. Rep. CNS-TR-2011-001
   Wang J, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P1760, DOI 10.1109/ICASSP39728.2021.9414668
   Wang X, 2020, PROC CVPR IEEE, P6387, DOI 10.1109/CVPR42600.2020.00642
   Wang X, 2019, PROC CVPR IEEE, P5017, DOI 10.1109/CVPR.2019.00516
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   Wu CY, 2017, IEEE I CONF COMP VIS, P2859, DOI 10.1109/ICCV.2017.309
   Xing EP, 2003, NIPS 15, V15, P505
   Yang X, 2019, IEEE T NEUR NET LEAR, V30, P2987, DOI [10.1109/TNNLS.2018.2861991, 10.1109/TNNLS.2018.2790479]
   Yao HT, 2019, IEEE T IMAGE PROCESS, V28, P2860, DOI 10.1109/TIP.2019.2891888
   Yi D, 2014, INT C PATT RECOG, P34, DOI 10.1109/ICPR.2014.16
   Yuan YH, 2017, IEEE I CONF COMP VIS, P814, DOI 10.1109/ICCV.2017.94
   Zhang DY, 2018, IEEE T CIRC SYST VID, V28, P2622, DOI 10.1109/TCSVT.2017.2723429
   Zheng WZ, 2020, PROC CVPR IEEE, P2957, DOI 10.1109/CVPR42600.2020.00303
   Zhong GQ, 2018, IEEE T CIRC SYST VID, V28, P2460, DOI 10.1109/TCSVT.2017.2726526
NR 65
TC 0
Z9 0
U1 2
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2024
VL 20
IS 7
SI SI
AR 224
DI 10.1145/3656047
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL0Q6
UT WOS:001234494100039
OA Bronze
DA 2024-08-05
ER

PT J
AU Zhang, PP
   Liu, M
   Song, XM
   Cao, D
   Gao, Z
   Nie, LQ
AF Zhang, Panpan
   Liu, Meng
   Song, Xuemeng
   Cao, Da
   Gao, Zan
   Nie, Liqiang
TI Universal Relocalizer forWeakly Supervised Referring Expression
   Grounding
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Weakly supervised referring expression grounding; universal relocalizer;
   refined grounding score
ID RECONSTRUCTION; NETWORK
AB This article introduces the Universal Relocalizer, a novel approach designed for weakly supervised referring expression grounding. Our method strives to pinpoint a target proposal that corresponds to a specific query, eliminating the need for region-level annotations during training. To bolster the localization precision and enrich the semantic understanding of the target proposal, we devise three key modules: the category module, the color module, and the spatial relationship module. The category and color modules assign respective category and color labels to region proposals, enabling the computation of category and color scores. Simultaneously, the spatial relationship module integrates spatial cues, yielding a spatial score for each proposal to enhance localization accuracy further. By adeptly amalgamating the category, color, and spatial scores, we derive a refined grounding score for every proposal. Comprehensive evaluations on the RefCOCO, RefCOCO+, and RefCOCOg datasets manifest the prowess of the Universal Relocalizer, showcasing its formidable performance across the board.
C1 [Zhang, Panpan] Shandong Univ, Qingdao, Shandong, Peoples R China.
   [Liu, Meng] Shandong Jianzhu Univ, Jinan, Shandong, Peoples R China.
   [Song, Xuemeng] Shandong Univ, Sch Comp Sci & Technol, Qingdao, Peoples R China.
   [Cao, Da] Hunan Univ, Coll Comp Sci & Elect Engn, Changsha, Hunan, Peoples R China.
   [Gao, Zan] Qilu Univ Technol, Jinan, Shandong, Peoples R China.
   [Nie, Liqiang] Harbin Inst Technol, Sch Comp Sci & Technol, Shenzhen, Peoples R China.
C3 Shandong University; Shandong Jianzhu University; Shandong University;
   Hunan University; Qilu University of Technology; Harbin Institute of
   Technology
RP Liu, M (corresponding author), Shandong Jianzhu Univ, Jinan, Shandong, Peoples R China.; Song, XM (corresponding author), Shandong Univ, Sch Comp Sci & Technol, Qingdao, Peoples R China.
EM 202135177@mail.sdu.edu.cn; mengliu.sdu@gmail.com; sxmustc@gmail.com;
   caoda0721@gmail.com; zangaonsh4522@gmail.com; nieliqiang@gmail.com
RI Zhang, Panpan/JNT-0772-2023
OI Zhang, Panpan/0000-0002-8211-5930; Liu, Meng/0000-0002-1582-5764; Cao,
   Da/0000-0002-2611-2559; , zan/0000-0003-2182-5741
FU National Natural Science Foundation of China [62376140, 62376137,
   U23A20315]; Shandong Provincial Natural Science Foundation [ZR2022YQ59];
   Science and Technology Innovation Program for Distinguished Young
   Scholars of Shandong Province Higher Education Institutions [2023KJ128];
   Special Fund for distinguished professors of Shandong Jianzhu University
FX This work was supported in part by the National Natural Science
   Foundation of China, No. 62376140, No. 62376137, and No. U23A20315; the
   Shandong Provincial Natural Science Foundation, No. ZR2022YQ59; the
   Science and Technology Innovation Program for Distinguished Young
   Scholars of Shandong Province Higher Education Institutions, No.
   2023KJ128, and in part by the Special Fund for distinguished professors
   of Shandong Jianzhu University.
CR Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Cheng Y, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5047, DOI 10.1145/3474085.3475677
   Cheng YH, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3499027
   Fang CW, 2023, IEEE T MED IMAGING, V42, P1720, DOI 10.1109/TMI.2023.3237183
   Fang ZW, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3282469
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Kazemzadeh S., 2014, EMNLP, DOI DOI 10.3115/V1/D14-1086
   Lao MR, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3370, DOI 10.1145/3474085.3475492
   Li Kun, 2023, ACM Transactions on Multimedia Computing, Communications and Applications, V19, P1
   Li LW, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5167, DOI 10.1145/3474085.3475629
   Liao Yiyi, 2020, CVPR
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu XH, 2019, PROC CVPR IEEE, P1950, DOI 10.1109/CVPR.2019.00205
   Liu XJ, 2023, IEEE T PATTERN ANAL, V45, P3003, DOI 10.1109/TPAMI.2022.3186410
   Liu XJ, 2019, IEEE I CONF COMP VIS, P2611, DOI 10.1109/ICCV.2019.00270
   Liu XJ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P539, DOI 10.1145/3343031.3351074
   Luo RT, 2017, PROC CVPR IEEE, P3125, DOI 10.1109/CVPR.2017.333
   Mao JH, 2016, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2016.9
   Nagaraja VK, 2016, LECT NOTES COMPUT SC, V9908, P792, DOI 10.1007/978-3-319-46493-0_48
   Pennington J., 2014, GLOVE GLOBAL VECTORS, DOI DOI 10.3115/V1/D14-1162
   Qiao YY, 2021, IEEE T MULTIMEDIA, V23, P4426, DOI 10.1109/TMM.2020.3042066
   Qu LG, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1047, DOI 10.1145/3394171.3413961
   Radford A, 2021, PR MACH LEARN RES, V139
   Redmon J, 2018, Arxiv, DOI arXiv:1804.02767
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Song XM, 2018, ACM/SIGIR PROCEEDINGS 2018, P5, DOI 10.1145/3209978.3209996
   Song XM, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P753, DOI 10.1145/3123266.3123314
   Subramanian S, 2022, PROCEEDINGS OF THE 60TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), VOL 1: (LONG PAPERS), P5198
   Sun MJ, 2023, IEEE T MULTIMEDIA, V25, P1611, DOI 10.1109/TMM.2021.3139467
   Sun MJ, 2021, IEEE T PATTERN ANAL, V43, P4189, DOI 10.1109/TPAMI.2021.3058684
   Suo W., 2021, P INT JOINT C ART IN, P1032
   Suo W, 2023, IEEE T IMAGE PROCESS, V32, P854, DOI 10.1109/TIP.2022.3227466
   Wang J, 2023, ACM T SENSOR NETWORK, V19, DOI 10.1145/3563776
   Wang ZC, 2022, AAAI CONF ARTIF INTE, P5914
   Xiao FY, 2017, PROC CVPR IEEE, P5253, DOI 10.1109/CVPR.2017.558
   Xue WH, 2024, CAAI T INTELL TECHNO, V9, P695, DOI 10.1049/cit2.12239
   Yang SB, 2019, IEEE I CONF COMP VIS, P4643, DOI 10.1109/ICCV.2019.00474
   Yang SB, 2019, PROC CVPR IEEE, P4140, DOI 10.1109/CVPR.2019.00427
   Yang S, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3572844
   Yang ZY, 2019, IEEE I CONF COMP VIS, P4682, DOI 10.1109/ICCV.2019.00478
   Yu DF, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3316767
   Yu LC, 2018, PROC CVPR IEEE, P1307, DOI 10.1109/CVPR.2018.00142
   Yu LC, 2016, LECT NOTES COMPUT SC, V9906, P69, DOI 10.1007/978-3-319-46475-6_5
   Zellers R, 2019, PROC CVPR IEEE, P6713, DOI 10.1109/CVPR.2019.00688
   Zhang C, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1258, DOI 10.1145/3343031.3351063
   Zhang DW, 2024, IEEE T NEUR NET LEAR, V35, P5395, DOI 10.1109/TNNLS.2022.3204337
   Zhang HT, 2022, AAAI CONF ARTIF INTE, P3262
   Zhang X, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1793, DOI 10.1145/3474085.3475328
   Zhang X, 2022, IEEE T MULTIMEDIA, V24, P2986, DOI 10.1109/TMM.2021.3091882
   Zhao F, 2018, PROC CVPR IEEE, P5696, DOI 10.1109/CVPR.2018.00597
NR 50
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2024
VL 20
IS 7
SI SI
AR 225
DI 10.1145/3656045
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL0Q6
UT WOS:001234494100040
OA Bronze
DA 2024-08-05
ER

PT J
AU Bian, J
   Li, XH
   Wang, T
   Wang, QZ
   Huang, J
   Liu, C
   Zhao, J
   Lu, FX
   Dou, DJ
   Xiong, HY
AF Bian, Jiang
   Li, Xuhong
   Wang, Tao
   Wang, Qingzhong
   Huang, Jun
   Liu, Chen
   Zhao, Jun
   Lu, Feixiang
   Dou, Dejing
   Xiong, Haoyi
TI P<SUP>2</SUP>ANet: A Large-Scale Benchmark for Dense Action Detection
   from Table Tennis Match Broadcasting Videos
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Datasets; annotation toolbox; video analysis; action recognition and
   localization; table tennis
AB While deep learning has been widely used for video analytics, such as video classification and action detection, dense action detection with fast-moving subjects from sports videos is still challenging. In this work, we release yet another sports video benchmark P(2)ANet for Ping Pong-Action detection, which consists of 2,721 video clips collected from the broadcasting videos of professional table tennis matches in World Table Tennis Championships and Olympiads. We work with a crew of table tennis professionals and referees on a specially designed annotation toolbox to obtain fine-grained action labels (in 14 classes) for every ping-pong action that appeared in the dataset, and formulate two sets of action detection problems-action localization and action recognition. We evaluate a number of commonly seen action recognition (e.g., TSM, TSN, Video SwinTransformer, and Slowfast) and action localization models (e.g., BSN, BSN++, BMN, TCANet), using P(2)ANet for both problems, under various settings. These models can only achieve 48% area under the AR-AN curve for localization and 82% top-one accuracy for recognition since the ping-pong actions are dense with fast-moving subjects but broadcasting videos are with only 25 FPS. The results confirm that P(2)ANet is still a challenging task and can be used as a special benchmark for dense action detection from videos. We invite readers to examine our dataset by visiting the following link: https://github.com/Fred1991/P2ANET.
C1 [Bian, Jiang; Li, Xuhong; Wang, Tao; Wang, Qingzhong; Huang, Jun; Liu, Chen; Zhao, Jun; Lu, Feixiang; Dou, Dejing; Xiong, Haoyi] Baidu Inc, 10 Shangdi 10th St, Beijing 100094, Peoples R China.
C3 Baidu
RP Xiong, HY (corresponding author), Baidu Inc, 10 Shangdi 10th St, Beijing 100094, Peoples R China.
EM jiangbian03@gmail.com; lixuhong@baidu.com; wangtao@bupt.edu.cn;
   qingzwang@outlook.com; huangjun12@baidu.com; ttjygbtj@gmail.com;
   zhaojun12@baidu.com; lufeixiang@baidu.com; doudejing@baidu.com;
   haoyi.xiong.fr@ieee.org
RI Li, Xuhong/GYD-3485-2022; XIONG, HAOYI/E-5079-2015
OI XIONG, HAOYI/0000-0002-5451-3253; Wang, Tao/0000-0003-4740-6932
CR Abu-El-Haija S., 2016, arXiv
   Arnab A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6816, DOI 10.1109/ICCV48922.2021.00676
   Bao HS, 2021, MICROPROCESS MICROSY, V81, DOI 10.1016/j.micpro.2020.103655
   Bertasius G, 2021, PR MACH LEARN RES, V139
   Bertasius G, 2017, IEEE I CONF COMP VIS, P2196, DOI 10.1109/ICCV.2017.239
   Bian J, 2022, IEEE INTERNET THINGS, V9, P8364, DOI 10.1109/JIOT.2022.3161050
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Cai DS, 2023, IEEE T MULTIMEDIA, V25, P2761, DOI 10.1109/TMM.2022.3151026
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chetoui M, 2020, J MED IMAGING, V7, DOI 10.1117/1.JMI.7.4.044503
   Cubuk ED, 2019, PROC CVPR IEEE, P113, DOI 10.1109/CVPR.2019.00020
   Dai XY, 2017, IEEE I CONF COMP VIS, P5727, DOI 10.1109/ICCV.2017.610
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deliege A, 2021, IEEE COMPUT SOC CONF, P4503, DOI 10.1109/CVPRW53098.2021.00508
   Dong JF, 2022, IEEE T PATTERN ANAL, V44, P4065, DOI 10.1109/TPAMI.2021.3059295
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Faulkner Hayden, 2017, P 2017 INT C DIGITAL, P1
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Gabeur Valentin, 2020, COMPUTER VISION ECCV, DOI [10.48550/arXiv.2007.10639, DOI 10.48550/ARXIV.2007.10639]
   Goyal R, 2017, IEEE I CONF COMP VIS, P5843, DOI 10.1109/ICCV.2017.622
   Gu CH, 2018, PROC CVPR IEEE, P6047, DOI 10.1109/CVPR.2018.00633
   Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685
   Ibrahim MS, 2016, PROC CVPR IEEE, P1971, DOI 10.1109/CVPR.2016.217
   Jiang Yudong, 2020, P 3 INTERNATIONALWOR, P1
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kimura M., 2021, Lecture Notes in Computer Science, P558
   Kondratyuk D, 2021, PROC CVPR IEEE, P16015, DOI 10.1109/CVPR46437.2021.01576
   Koshkina M, 2021, IEEE COMPUT SOC CONF, P4523, DOI 10.1109/CVPRW53098.2021.00510
   Kulkarni KM, 2021, IEEE COMPUT SOC CONF, P4571, DOI 10.1109/CVPRW53098.2021.00515
   Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718
   Lin TW, 2019, IEEE I CONF COMP VIS, P3888, DOI 10.1109/ICCV.2019.00399
   Lin TW, 2018, LECT NOTES COMPUT SC, V11208, P3, DOI 10.1007/978-3-030-01225-0_1
   Liu W, 2017, MULTIMED TOOLS APPL, V76, P24983, DOI 10.1007/s11042-017-5002-5
   Liu Y, 2020, Arxiv, DOI arXiv:1907.13487
   Liu Y, 2019, IEEE INT CONF MULTI, P651, DOI 10.1109/ICMEW.2019.00126
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Long X, 2018, PROC CVPR IEEE, P7834, DOI 10.1109/CVPR.2018.00817
   Ma X, 2023, ENG APPL ARTIF INTEL, V120, DOI 10.1016/j.engappai.2023.105936
   Martin Pierre-Etienne, 2021, MMSports'21: Proceedings of the 4th International Workshop on Multimedia Content Analysis in Sports, P35, DOI 10.1145/3475722.3482793
   Martin PE, 2018, INT WORK CONTENT MUL
   Nawhal M., 2021, arXiv
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Niebles JC, 2010, LECT NOTES COMPUT SC, V6312, P392, DOI 10.1007/978-3-642-15552-9_29
   Olympedia, 2012, Referee Biographical Information
   Qing ZW, 2021, PROC CVPR IEEE, P485, DOI 10.1109/CVPR46437.2021.00055
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shao D, 2020, PROC CVPR IEEE, P2613, DOI 10.1109/CVPR42600.2020.00269
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Simonyan K, 2014, ADV NEUR IN, V27
   Soomro K., 2014, Advances in Computer Vision and Pattern Recognition, V71, P181
   Sri-Iesaranusorn Panyawut, 2021, P 2021 13 INT C MOBI, P1
   Su HS, 2021, AAAI CONF ARTIF INTE, V35, P2602
   Thilakarathne H, 2022, MACH VISION APPL, V33, DOI 10.1007/s00138-022-01346-2
   Voeikov R, 2020, IEEE COMPUT SOC CONF, P3857, DOI 10.1109/CVPRW50498.2020.00450
   Wadsworth N, 2020, J SPORT PSYCHOL ACTI, V11, P73, DOI 10.1080/21520704.2018.1528324
   Wang B, 2019, KSII T INTERNET INF, V13, P5130
   Wang L, 2014, IEEE INT CONF VLSI
   Wang LM, 2023, PROC CVPR IEEE, P14549, DOI 10.1109/CVPR52729.2023.01398
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang SF, 2020, IEEE T MULTIMEDIA, V22, P1084, DOI 10.1109/TMM.2019.2934824
   Wang X, 2021, PROC CVPR IEEE, P1905, DOI 10.1109/CVPR46437.2021.00194
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wei C, 2022, PROC CVPR IEEE, P14648, DOI 10.1109/CVPR52688.2022.01426
   Wu CY, 2020, PROC CVPR IEEE, P150, DOI 10.1109/CVPR42600.2020.00023
   Wu F, 2023, IEEE T MULTIMEDIA, V25, P7943, DOI 10.1109/TMM.2022.3232034
   Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19
   Xu MM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7200, DOI 10.1109/ICCV48922.2021.00713
   Yan H, 2021, IEEE T KNOWL DATA EN, V33, P180, DOI 10.1109/TKDE.2019.2926078
   Yeung S, 2016, PROC CVPR IEEE, P2678, DOI 10.1109/CVPR.2016.293
   Yu Dianhai, 2019, Frontiers of Data and Domputing, V1, P105
   Yuan J, 2016, PROC CVPR IEEE, P3093, DOI 10.1109/CVPR.2016.337
   Zalluhoglu C, 2020, IMAGE VISION COMPUT, V94, DOI 10.1016/j.imavis.2020.103870
   Zhu Y, 2020, Arxiv, DOI arXiv:2012.06567
NR 73
TC 0
Z9 0
U1 13
U2 13
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2024
VL 20
IS 4
AR 118
DI 10.1145/3633516
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6G9
UT WOS:001153382100028
DA 2024-08-05
ER

PT J
AU Baldrati, A
   Bertini, M
   Uricchio, T
   Del Bimbo, A
AF Baldrati, Alberto
   Bertini, Marco
   Uricchio, Tiberio
   Del Bimbo, Alberto
TI Composed Image Retrieval using Contrastive Learning and Task-oriented
   CLIP-based Features
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Multimodal retrieval; combiner networks; vision language model
AB Given a query composed of a reference image and a relative caption, the Composed Image Retrieval goal is to retrieve images visually similar to the reference one that integrates the modifications expressed by the caption. Given that recent research has demonstrated the efficacy of large-scale vision and language pre-trained (VLP) models in various tasks, we rely on features from the OpenAI CLIP model to tackle the considered task. We initially perform a task-oriented fine-tuning of both CLIP encoders using the element-wise sum of visual and textual features. Then, in the second stage, we train a Combiner network that learns to combine the image-text features integrating the bimodal information and providing combined features used to perform the retrieval. We use contrastive learning in both stages of training. Starting from the bare CLIP features as a baseline, experimental results show that the task-oriented fine-tuning and the carefully crafted Combiner network are highly effective and outperform more complex state-of-the-art approaches on FashionIQ and CIRR, two popular and challenging datasets for composed image retrieval. Code and pre-trained models are available at https://github.com/ABaldrati/CLIP4Cir.
C1 [Baldrati, Alberto; Bertini, Marco; Del Bimbo, Alberto] Univ Firenze, Viale Morgagni 65, I-50124 Florence, Italy.
   [Baldrati, Alberto] Univ Pisa, Largo Bruno Pontecorvo 3, I-56127 Pisa, Italy.
   [Uricchio, Tiberio] Univ Macerata, Via Garibaldi 20, I-62100 Macerata, Italy.
C3 University of Florence; University of Pisa; University of Macerata
RP Baldrati, A (corresponding author), Univ Pisa, Largo Bruno Pontecorvo 3, I-56127 Pisa, Italy.
EM alberto.baldrati@unifi.it; marco.bertini@unifi.it;
   tiberio.uricchio@unimc.it; alberto.delbimbo@unifi.it
RI Bertini, Marco/ABF-2564-2020
OI Bertini, Marco/0000-0003-3010-855X; DEL BIMBO,
   ALBERTO/0000-0002-1052-8322; Bertini, Marco/0000-0002-1364-218X;
   URICCHIO, TIBERIO/0000-0003-1025-4541
FU European Commission under European Horizon 2020 Programme [101004545
   -ReInHerit]
FX This work was partially supported by the European Commission under
   European Horizon 2020 Programme, grant number 101004545 -ReInHerit.
CR Agarwal S, 2021, Arxiv, DOI arXiv:2108.02818
   Ahmad J, 2018, FUTURE GENER COMP SY, V81, P314, DOI 10.1016/j.future.2017.11.002
   Anwaar MU, 2021, IEEE WINT CONF APPL, P1139, DOI 10.1109/WACV48630.2021.00118
   Baldrati Alberto, 2022, FUTURE HERITAGE SCI, P140
   Banerjee I, 2018, J BIOMED INFORM, V84, P123, DOI 10.1016/j.jbi.2018.07.002
   Brown T. B., 2020, P 34 INT C NEURAL IN, P1
   Chen TQ, 2016, Arxiv, DOI arXiv:1604.06174
   Chen YB, 2020, PROC CVPR IEEE, P2998, DOI 10.1109/CVPR42600.2020.00307
   Cheng RZ, 2021, IEEE COMPUT SOC CONF, P3113, DOI 10.1109/CVPRW53098.2021.00348
   Chung JY, 2014, Arxiv, DOI [arXiv:1412.3555, DOI 10.48550/ARXIV.1412.3555]
   Companioni-Brito C, 2018, LECT NOTES COMPUT SC, V10882, P278, DOI 10.1007/978-3-319-93000-8_32
   Conde MV, 2021, IEEE COMPUT SOC CONF, P3951, DOI 10.1109/CVPRW53098.2021.00444
   Delmas Ginger, 2021, INT C LEARN REPR
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dodds E, 2020, Arxiv, DOI arXiv:2007.00145
   Dong Xiao, 2021, arXiv
   Fang H., 2021, arXiv
   Galatolo FA, 2021, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING AND VISION ENGINEERING (IMPROVE), P166, DOI 10.5220/0010503701660174
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Guo XX, 2018, ADV NEUR IN, V31
   Han X, 2022, LECT NOTES COMPUT SC, V13695, P634, DOI 10.1007/978-3-031-19833-5_37
   Han XT, 2017, IEEE I CONF COMP VIS, P1472, DOI 10.1109/ICCV.2017.163
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ionescu B, 2019, LECT NOTES COMPUT SC, V11696, P358, DOI 10.1007/978-3-030-28577-7_28
   Ionescu Bogdan, 2020, Advances in Information Retrieval, V12036, P533
   Jandial S, 2021, Arxiv, DOI arXiv:2009.01485
   Jandial S, 2022, IEEE WINT CONF APPL, P597, DOI 10.1109/WACV51458.2022.00067
   Jia C, 2021, PR MACH LEARN RES, V139
   Kim J, 2021, AAAI CONF ARTIF INTE, V35, P1771
   Kovashka A, 2015, INT J COMPUT VISION, V115, P185, DOI 10.1007/s11263-015-0814-0
   Lee S, 2021, PROC CVPR IEEE, P802, DOI 10.1109/CVPR46437.2021.00086
   Li ML, 2022, PROC CVPR IEEE, P16399, DOI 10.1109/CVPR52688.2022.01593
   Li X., 2020, Lecture Notes in Computer Science, P121, DOI DOI 10.1007/978-3-030-58577-8_8
   Li XQ, 2021, NEUROCOMPUTING, V452, P675, DOI 10.1016/j.neucom.2020.07.139
   Liang Victor Weixin, 2022, ADV NEURAL INFORM PR, V35, P17612
   Liu Z., 2021, P IEEE CVF INT C COM, P2125, DOI DOI 10.1109/1CCV48922.2021.00213
   Loshchilov I., 2018, INT C LEARN REPR
   Micikevicius P., 2018, C TRACK P
   Pennington J., 2014, GLOVE GLOBAL VECTORS, DOI DOI 10.3115/V1/D14-1162
   Radford A, 2021, PR MACH LEARN RES, V139
   Rui Y, 1998, IEEE T CIRC SYST VID, V8, P644, DOI 10.1109/76.718510
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Shin M, 2021, Arxiv, DOI arXiv:2104.03015
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Suhr A, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6418
   Vo N, 2019, PROC CVPR IEEE, P6432, DOI 10.1109/CVPR.2019.00660
   Wang ZC, 2022, Arxiv, DOI arXiv:2201.05729
   Wen HK, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1369, DOI 10.1145/3404835.3462967
   Wu H, 2021, PROC CVPR IEEE, P11302, DOI 10.1109/CVPR46437.2021.01115
   Yating Liu, 2021, MultiMedia Modeling. 27th International Conference, MMM 2021. Proceedings. Lecture Notes in Computer Science (LNCS 12572), P315, DOI 10.1007/978-3-030-67832-6_26
   Yu YJ, 2020, Arxiv, DOI arXiv:2003.12299
   Yuan YF, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P839, DOI 10.1145/3404835.3462881
   Zhan XL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11762, DOI 10.1109/ICCV48922.2021.01157
   Zhao B, 2017, PROC CVPR IEEE, P6156, DOI 10.1109/CVPR.2017.652
   Zheng L, 2018, IEEE T PATTERN ANAL, V40, P1224, DOI 10.1109/TPAMI.2017.2709749
NR 56
TC 1
Z9 1
U1 10
U2 10
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2024
VL 20
IS 3
AR 62
DI 10.1145/3617597
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6F8
UT WOS:001153381000002
OA Green Published, Green Submitted
DA 2024-08-05
ER

PT J
AU Rime, J
   Archer-Boyd, A
   Collins, T
AF Rime, Jemily
   Archer-Boyd, Alan
   Collins, Tom
TI How Will You Pod? Implications of Creators' Perspectives for Designing
   Innovative Podcasting Tools
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Podcast; audio production; production workflow; broadcasting; new media
ID PERSONALIZATION; EDUCATION; RADIO
AB While centred on the medium of audio, podcasts are often a multimedia concern, and one that has become hugely popular in recent years, though relatively little is known about the perspectives of podcast creators and their visions of innovation. This article details the results of an exploratory study conducted to enhance our understanding of potential innovation in the field of podcasting. Sixteen podcast creators were interviewed about their work and what they wanted from next-generation podcasts, in order to understand the requirements and expectations of tools that could be built to create new forms of audio-based programming. Through a combination of qualitative and quantitative analysis, we reveal novel findings, such as the duality between "listener-centric" and "creator-centric" innovations, to improve listener experience but also to unleash new creative possibilities in a streamlined production workflow. We shed light on what podcast creators envision as "next-generation podcasting," the archetypal podcast production workflow, and creators' expectations of podcasting tools. Combining these findings, we identify how the workflow could be modified to include new steps that will help to realise podcast creators' visions. This study crystalises on important information about podcasters-their behavior and perspectives on the future of the medium-which will allow further research, design, and development in the field to be founded upon empirical observations.
C1 [Rime, Jemily] Univ York, Sch Arts & Creat Technol, York, N Yorkshire, England.
   [Rime, Jemily; Archer-Boyd, Alan] BBC R&D, York, N Yorkshire, England.
   [Collins, Tom] Univ Miami, Frost Sch Music, Miami, FL USA.
   [Collins, Tom] Mus Artificial Intelligence Algorithms Inc, Miami, FL USA.
C3 University of York - UK; University of Miami
RP Rime, J (corresponding author), Univ York, Sch Arts & Creat Technol, York, N Yorkshire, England.; Rime, J (corresponding author), BBC R&D, York, N Yorkshire, England.
EM jir506@york.ac.uk; alan.archer-boyd@bbc.co.uk; tomthecollins@gmail.com
FU UK Arts and Humanities Research Council Creative Industries Clusters
   Programme [AH/S002839/1]; University of York; BBC
FX This research has been supported by the XR Stories project, partly
   funded by the UK Arts and Humanities Research Council Creative
   Industries Clusters Programme, grant reference AH/S002839/1, the
   University of York and the BBC.
CR Abras C., 2004, Encyclopedia of Human -Computer Interaction, P445
   Adams WC, 2015, HDB PRACTICAL PROGRA, P492, DOI DOI 10.1002/9781119171386.CH19
   Albarran AB, 2007, J RADIO AUDIO MEDIA, V14, P92, DOI 10.1080/10955040701583171
   [Anonymous], 2020, A Blind Legend
   [Anonymous], 2004, Understanding Digital Cinema: A Professional Handbook
   [Anonymous], 2022, The Infinite Dial 2022
   Apple Newsroom, 2021, Apple Podcasts Subscriptions and Channels are Now Available Worldwide
   Armstrong Mike, 2022, WHP 396, the Role of the Audience in Media
   Baddeley W.Hugh., 1970, The Technique of Documentary Film Production, V2nd
   Bailer Werner, 2011, AT Forum Medientechnik-Next Generation, New Ideas: Beitrage der Tagungen 2010 und 2011 an der Fachhochschule St. Polten, P174
   Bailey Brian P., 2000, Wiley Encyclopedia of Electrical and Electronics Engineering, P1
   Bartindale Tom., 2012, Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. CHI '12, P169
   Baume C, 2018, INT J HUM-COMPUT ST, V115, P67, DOI 10.1016/j.ijhcs.2018.03.006
   Baume Chris, 2018, Ph. D. Dissertation
   BBC, 2021, BBC World Service International Podcast Competition: Meet the Judges.
   BBC, 2020, Radio 3 in Immersive Sound.
   BBC Taster, 2020, Monster.
   BBC Taster, 2022, Spectrum Sounds.
   BBC Taster, 2023, Instagramification.
   Becker V., 2017, Lecture Notes in Computer Science, V10271, P3, DOI 10.1007/978-3-319-58071-5_1
   Bendel O, 2019, AI SOC, V34, P83, DOI 10.1007/s00146-017-0748-x
   Berry Richard., 2006, CONVERGENCE, V12, P143
   Beuscart Jean-Samuel, 2015, Terrains & Travaux, V26, P83
   Braun V., 2006, QUAL RES PSYCHOL, V3, P77, DOI 10.1191/1478088706qp063oa
   Broth M, 2008, J PRAGMATICS, V40, P904, DOI 10.1016/j.pragma.2007.10.010
   Burgess R. J., 2014, The History of Music Production
   Burns William, 2013, P 6 INT C PERVASIVE, P1
   Buzzsprout.com, 2022, How to Start a Podcast: Your Lightning Fast, No -Sweat, Guide for 2022
   Buzzsprout.com, 2022, How to Start a Podcast: Complete Step-by-Step Guide (2022).
   Castleberry A, 2018, CURR PHARM TEACH LEA, V10, P807, DOI 10.1016/j.cptl.2018.03.019
   Chaniotis IK, 2015, COMPUTING, V97, P1023, DOI 10.1007/s00607-014-0394-9
   Chen Liming, 2014, Machine Learning and Cybernetics (Communications in Computer and Information Science), P287
   Chenail RJ, 2011, QUAL REP, V16, P255
   Christel Michael G., 1992, Technical Report., DOI [10.21236/ADA258932, DOI 10.21236/ADA258932]
   CNET, 2011, The Complete History of Apple's iPod.
   Colonel JT, 2021, J ACOUST SOC AM, V150, P608, DOI 10.1121/10.0005622
   Devlin Patricia, 2022, It's Time to Tune In to Women Podcasters.
   Durrani M, 2015, AM ANTHROPOL, V117, P593, DOI 10.1111/aman.12302
   Dwornik Weronika., 2021, Adaptive Podcasting & Rabbit Holes Collective: Interview with Ian Forrester
   Ford M., 2016, Writing Interactive Fiction with Twine, V1st
   Fox M, 2023, SOC WORK EDUC, V42, P404, DOI 10.1080/02615479.2021.1972963
   Frary M, 2017, INDEX CENSORSHIP, V46, P24, DOI 10.1177/0306422017730789
   Frias-Martinez E, 2006, INT J INFORM MANAGE, V26, P234, DOI 10.1016/j.ijinfomgt.2006.02.006
   Furner J, 2020, J ASSOC INF SCI TECH, V71, pE33, DOI 10.1002/asi.24295
   Gao M, 2010, INFORM SYST FRONT, V12, P607, DOI 10.1007/s10796-009-9199-3
   Gillham B., 2000, Case Study Research Methods
   GrandViewResearch, 2021, Podcasting Market Size, Share, Industry Report, 2021-2028, P2021
   Guest G., 2012, APPL THEMATIC ANAL, DOI 10.4135/9781483384436
   Hancock D., 2018, Podcasting: New Aural Cultures and Digital Media, P81, DOI DOI 10.1007/978-3-319-90056-85
   Heck R, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1198302.1198306
   Higueras-Ruiz MJ, 2018, COMMUN SOC-SPAIN, V31, P91, DOI 10.15581/003.31.1.91-106
   Hyperradio Radio France, 2021, Son 3D.
   Kalpokas Ignas, 2021, Malleable, Digital, and Posthuman., P47
   Kay RH, 2012, COMPUT HUM BEHAV, V28, P820, DOI 10.1016/j.chb.2012.01.011
   Kazai G, 2003, Digital Media: Processing Multimedia Interactive Services, P487, DOI 10.1142/9789812704337_0089
   KELLOGG B., 2014, 11 USENIX S NETW SYS, P303
   Kong QQ, 2019, IEEE-ACM T AUDIO SPE, V27, P777, DOI 10.1109/TASLP.2019.2895254
   Kujala S, 2003, BEHAV INFORM TECHNOL, V22, P1, DOI [10.1080/01449290301782, 10.1080/0144929021000055530]
   Kumar A., 2016, P 24 ACM INT C MULTI, P1038
   Lee SH, 2013, IEEE ICCE, P173, DOI 10.1109/ICCE.2013.6486845
   Lewis Daniel J., 2023, Apple Podcasts Statistics.
   Lim CP, 2000, PROCEEDINGS OF THE FIRST INTERNATIONAL CONFERENCE ON WEB INFORMATION SYSTEMS ENGINEERING, VOL I, P419, DOI 10.1109/WISE.2000.882421
   Limbik Theatre, 2022, Ambisonic Stories.
   Lindgren M, 2023, JOURNAL PRACT, V17, P704, DOI 10.1080/17512786.2021.1943497
   Malham D.G., 1998, Organised sound, V3, P167
   Malterud K, 2012, SCAND J PUBLIC HEALT, V40, P795, DOI 10.1177/1403494812465030
   Mamer Bruce., 2013, Film Production Technique: Creating the Accomplished Image
   Markman KM, 2012, NEW MEDIA SOC, V14, P547, DOI 10.1177/1461444811420848
   Matthews Kerry, 2006, Research into podcasting technology including current and possible future uses, P88
   McClung S, 2010, J RADIO AUDIO MEDIA, V17, P82, DOI 10.1080/19376521003719391
   McHugh Siobhan., 2016, RADIO J INT STUDIES, V14, P65, DOI [10.1386/rjao.14.1.65_1, DOI 10.1386/RJAO.14.1.65_1]
   Murugesan S., 2001, Active Media Technology. 6th International Computer Science Conference, AMT 2002. Proceedings (Lecture Notes in Computer Science Vol.2252), P65
   Music Radar, 2022, 6 AI Powered Intelligent Plugins that Could Change the Way You Make Music
   Nelson DR., 2016, J SOCIAL MEDIA SOC, V5, P38
   NPR, 2022, Student Podcast Challenge.
   NPR, 2022, A Studio At Your Fingertips: 5 Apps Teachers Are Using To Make Student Podcasts
   ODonoghue Michael, 2008, Guidelines for podcast production and use in tertiary education, P2
   Pan YW, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3448981
   Pardoe Lawrence, 2020, Audio Engineering Society Convention
   Parham Bahadoran, 2018, Journal of the Audio Engineering Society
   Passamonti F, 2020, LANCET HAEMATOL, V7, pE737, DOI 10.1016/S2352-3026(20)30251-9
   Petitjean Virginie, 2008, Entrelacs. Cinema et Audiovisuel HS
   Pike C., 2019, Evaluating the Perceived Quality of Binaural Technology
   Podnews.net, 2022, 1 in 10 Brits Will Launch their OwnPodcast in 2022
   Prince BF, 2020, TEACH SOCIOL, V48, P269, DOI 10.1177/0092055X20959837
   Ragusa AT, 2009, INFORM COMMUN SOC, V12, P678, DOI 10.1080/13691180802471471
   Rime J, 2022, PROCEEDINGS OF THE 2022 ACM INTERNATIONAL CONFERENCE ON INTERACTIVE MEDIA EXPERIENCES, IMX 2022, P11, DOI 10.1145/3505284.3529977
   Rime J, 2022, CONVERGENCE-US, V28, P1260, DOI 10.1177/13548565221104444
   Robson M, 2021, PERSPECT BIOL MED, V64, P388
   Rousseau B., 2005, P 7 IEEE INT S MULTI
   Rousseau Boris, 2004, P 19 ANN C ACM S APP
   ROWE MW, 1992, PHILOSOPHY, V67, P467, DOI 10.1017/S0031819100040663
   Salminen J, 2022, PROCEEDINGS OF THE 2022 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI' 22), DOI 10.1145/3491102.3517589
   Sharon T, 2019, POP COMMUN, V17, P333, DOI 10.1080/15405702.2019.1610175
   Sherrill LA, 2022, JOURNAL PRACT, V16, P1473, DOI 10.1080/17512786.2020.1852884
   Sinh Huynh, 2018, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, V2, DOI 10.1145/3191745
   Spotify, 2022, Podcasts Homepage.
   Spotify Newsroom, 2021, Spotify Opens Doors for More Underrepresented Podcasters Through New Sound Up Programs
   Spotify Newsroom, 2020, Get to Know Your Favorite Podcasts Even Better With New Polls Feature
   Strickland BK, 2021, ECOL EVOL, V11, P3597, DOI 10.1002/ece3.7353
   Sullivan John, 2020, AoIR Selected Papers of Internet Research
   The Orpheus Project, 2017, The Mermaid's Tears
   The Simplecast Blog, 2019, What's the Difference Between Streams and Downloads?
   Tossell CC, 2012, BEHAV INFORM TECHNOL, V31, P995, DOI 10.1080/0144929X.2012.687773
   Trivedi A., 2018, IOSR J. Comput. Eng, V20, P36
   Ursu MF, 2020, PROCEEDINGS OF THE 2020 ACM INTERNATIONAL CONFERENCE ON INTERACTIVE MEDIA EXPERIENCES, IMX 2020, P127, DOI 10.1145/3391614.3393654
   Ursu MF, 2008, ACM T MULTIM COMPUT, V4, DOI 10.1145/1412196.1412198
   van der Aalst WMP, 2000, COMPUT SYST SCI ENG, V15, P267
   Vonderhaar James Keith, 1983, Production of a Radio Program Series.
   Wallick Scott Allan, 2003, Christopher Lydon Interviews: All the Lydon Interviews in One Download.
   Ward Lauren, 2020, IBC2020
   Ward Lauren A, 2017, ADJUNCT PUBLICATION
   Witt Stephen., 2015, MUSIC GOT FREE END I
   Yu Liang, 2023, IUI '23: Proceedings of the 28th International Conference on Intelligent User Interfaces, P142, DOI 10.1145/3581641.3584032
NR 114
TC 1
Z9 1
U1 2
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2024
VL 20
IS 3
AR 69
DI 10.1145/3625099
PG 25
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6F8
UT WOS:001153381000009
OA hybrid
DA 2024-08-05
ER

PT J
AU Wang, HR
   Wang, YJ
   Yu, BS
   Zhan, YB
   Yuan, CF
   Yang, WK
AF Wang, Haoran
   Wang, Yajie
   Yu, Baosheng
   Zhan, Yibing
   Yuan, Chunfeng
   Yang, Wankou
TI Attentional Composition Networks for Long-Tailed Human Action
   Recognition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Compositional learning; long tail; few-shot; zero-shot; action
   recognition
AB The problem of long-tailed visual recognition has been receiving increasing research attention. However, the long-tailed distribution problem remains underexplored for video-based visual recognition. To address this issue, in this article we propose a compositional learning based solution for video-based human action recognition. Our method, named Attentional Composition Networks (ACN), first learns verb-like and prepositionlike components, then shuffles these components to generate samples for the tail classes in the feature space to augment the data for the tail classes. Specifically, during training, we represent each action video by a graph that captures the spatial-temporal relations (edges) among detected human/object instances (nodes). Then, ACN utilizes the position information to decompose each action into a set of verb and preposition representations using the edge features in the graph. After that, the verb and preposition features from different videos are combined via an attention structure to synthesize feature representations for tail classes. This way, we can enrich the data for the tail classes and consequently improve the action recognition for these classes. To evaluate the compositional human action recognition, we further contribute a new human action recognition dataset, namely NEU-Interaction (NEU-I). Experimental results on both Something-Something V2 and the proposed NEU-I demonstrate the effectiveness of the proposed method for long-tailed, few-shot, and zero-shot problems in human action recognition. Source code and the NEU-I dataset are available at https://github.com/YajieW99/ACN.
C1 [Wang, Haoran; Wang, Yajie] Northeastern Univ, Coll Informat Sci & Engn, Shenyang 110819, Peoples R China.
   [Yu, Baosheng] Univ Sydney, Sch Comp Sci, Fac Engn, Darlington, NSW 2008, Australia.
   [Zhan, Yibing] JD Explore Acad, Beijing 100176, Peoples R China.
   [Yuan, Chunfeng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Yang, Wankou] Southeast Univ, Sch Automat, Nanjing, Peoples R China.
C3 Northeastern University - China; University of Sydney; Chinese Academy
   of Sciences; Institute of Automation, CAS; Southeast University - China
RP Wang, HR (corresponding author), Northeastern Univ, Coll Informat Sci & Engn, Shenyang 110819, Peoples R China.
EM wanghaoran@ise.neu.edu.cn; 2000859@stu.neu.edu.cn;
   baosheng.yu@sydney.edu.au; zhanyibing@jd.com; cfyuan@nlpr.ia.ac.cn;
   wkyang@seu.edu.cn
RI Xing, Junliang/HGE-9630-2022
OI Xing, Junliang/0000-0001-6801-0510; Yang, Wankou/0000-0002-6385-6776;
   yuan, chun feng/0000-0003-2219-4961
FU Major Science and Technology Innovation 2030 "New Generation Artificial
   Intelligence" key project [2021ZD0111700]; Fundamental Research Funds
   for the Central Universities of China [N2304012]; National Nature
   Science Foundation of China [61773117, 61972397, 62276061, 62002090]
FX This work was supported in part by the Major Science and Technology
   Innovation 2030 "New Generation Artificial Intelligence" key project
   (2021ZD0111700), the Fundamental Research Funds for the Central
   Universities of China (N2304012), and the National Nature Science
   Foundation of China (61773117, 61972397, 62276061, 62002090).
CR Alfassy A, 2019, PROC CVPR IEEE, P6541, DOI 10.1109/CVPR.2019.00671
   Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12
   Angtian Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12642, DOI 10.1109/CVPR42600.2020.01266
   Bertasius G, 2021, PR MACH LEARN RES, V139
   Buda M, 2018, NEURAL NETWORKS, V106, P249, DOI 10.1016/j.neunet.2018.07.011
   Byrd J, 2019, PR MACH LEARN RES, V97
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen Y, 2019, PROC CVPR IEEE, P5152, DOI 10.1109/CVPR.2019.00530
   Chen ZT, 2019, AAAI CONF ARTIF INTE, P3379
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Fang HS, 2021, AAAI CONF ARTIF INTE, V35, P1300
   Feichtenhofer C, 2017, PROC CVPR IEEE, P7445, DOI 10.1109/CVPR.2017.787
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Gkioxari G, 2018, PROC CVPR IEEE, P8359, DOI 10.1109/CVPR.2018.00872
   Gong LY, 2019, PROC CVPR IEEE, P9203, DOI 10.1109/CVPR.2019.00943
   Goyal R, 2017, IEEE I CONF COMP VIS, P5843, DOI 10.1109/ICCV.2017.622
   Gupta A, 2009, IEEE T PATTERN ANAL, V31, P1775, DOI 10.1109/TPAMI.2009.83
   He HB, 2009, IEEE T KNOWL DATA EN, V21, P1263, DOI 10.1109/TKDE.2008.239
   Hou Z, 2021, PROC CVPR IEEE, P495, DOI 10.1109/CVPR46437.2021.00056
   Hou Z, 2021, PROC CVPR IEEE, P14641, DOI 10.1109/CVPR46437.2021.01441
   Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378
   Hu RH, 2017, IEEE I CONF COMP VIS, P804, DOI 10.1109/ICCV.2017.93
   Huang C, 2016, PROC CVPR IEEE, P5375, DOI 10.1109/CVPR.2016.580
   Japkowicz N., 2002, Intelligent Data Analysis, V6, P429
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jiang XH, 2020, IEEE T CIRC SYST VID, V30, P2129, DOI 10.1109/TCSVT.2019.2914137
   Johnson J, 2015, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2015.7298990
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kato K, 2018, LECT NOTES COMPUT SC, V11218, P247, DOI 10.1007/978-3-030-01264-9_15
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   Kortylewski Adam, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8937, DOI 10.1109/CVPR42600.2020.00896
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Li Y, 2020, PROC CVPR IEEE, P906, DOI 10.1109/CVPR42600.2020.00099
   Li yl, 2020, NeurIPS, V33, P1
   Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718
   Liu YZ, 2022, IEEE T IMAGE PROCESS, V31, P4104, DOI 10.1109/TIP.2022.3180585
   Long FC, 2022, PROC CVPR IEEE, P3182, DOI 10.1109/CVPR52688.2022.00319
   Materzynska J, 2020, PROC CVPR IEEE, P1046, DOI 10.1109/CVPR42600.2020.00113
   Misra I, 2017, PROC CVPR IEEE, P1160, DOI 10.1109/CVPR.2017.129
   Nguyen TV, 2017, NEUROCOMPUTING, V260, P123, DOI 10.1016/j.neucom.2017.04.007
   Peng Chu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P694, DOI 10.1007/978-3-030-58526-6_41
   Qi SY, 2018, LECT NOTES COMPUT SC, V11213, P407, DOI 10.1007/978-3-030-01240-3_25
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Ramanathan V, 2015, PROC CVPR IEEE, P1100, DOI 10.1109/CVPR.2015.7298713
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Santra A, 2017, ADV GEOSPAT TECH, P1, DOI 10.4018/978-1-5225-1814-3
   Shen L, 2016, LECT NOTES COMPUT SC, V9911, P467, DOI 10.1007/978-3-319-46478-7_29
   Shi QHY, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3485665
   Simonyan K, 2014, ADV NEUR IN, V27
   Soomro K., 2012, CoRR, V2
   Truong TD, 2022, PROC CVPR IEEE, P19998, DOI 10.1109/CVPR52688.2022.01940
   Thatipelli A, 2022, PROC CVPR IEEE, P19926, DOI 10.1109/CVPR52688.2022.01933
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   Tulsiani S, 2017, PROC CVPR IEEE, P1466, DOI 10.1109/CVPR.2017.160
   Wang LM, 2021, PROC CVPR IEEE, P1895, DOI 10.1109/CVPR46437.2021.00193
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang P, 2017, IEEE T CIRC SYST VID, V27, P2613, DOI 10.1109/TCSVT.2016.2576761
   Wang XL, 2018, LECT NOTES COMPUT SC, V11209, P413, DOI 10.1007/978-3-030-01228-1_25
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang Y.-X., 2017, Advances in Neural Information Processing Systems, V30, P7032
   Xiao TT, 2019, IEEE I CONF COMP VIS, P3918, DOI 10.1109/ICCV.2019.00402
   Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19
   Xu CY, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3450410
   Xu HT, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3538749
   Yao BP, 2010, PROC CVPR IEEE, P17, DOI 10.1109/CVPR.2010.5540235
   Zhang FF, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3478642
   Zhi Hou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P584, DOI 10.1007/978-3-030-58555-6_35
   Zhou BL, 2018, LECT NOTES COMPUT SC, V11205, P831, DOI 10.1007/978-3-030-01246-5_49
   Zhou PH, 2019, IEEE I CONF COMP VIS, P843, DOI 10.1109/ICCV.2019.00093
   Zhou TF, 2020, PROC CVPR IEEE, P4262, DOI 10.1109/CVPR42600.2020.00432
NR 70
TC 1
Z9 1
U1 18
U2 24
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JAN
PY 2024
VL 20
IS 1
AR 8
DI 10.1145/3603253
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA T8LL3
UT WOS:001080441800008
DA 2024-08-05
ER

PT J
AU Li, YY
   Xiao, F
   Liang, W
   Gui, LQ
AF Li, Yunyi
   Xiao, Fu
   Liang, Wei
   Gui, Linqing
TI Multiply Complementary Priors for Image Compressive Sensing
   Reconstruction in Impulsive Noise
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Truncated-cauchy; compressive sensing; impulsive noise; nonlocal and
   deep priors; alternatively
ID SPARSE REPRESENTATION; ALGORITHM; REGULARIZATION; MINIMIZATION;
   EFFICIENT; RECOVERY; MODEL
AB Impulsive noise is always present in real-world image Compressive Sensing (CS) acquisition systems, where existing CS reconstruction performance may seriously deteriorate. In this article, we propose a robust CS formulation for image reconstruction to suppress outliers in the presence of impulsive noise. To address this issue, we consider a novel truncated-Cauchy loss function as the metric of residual error to elevate the reconstruction robustness. Specifically, we design a complementary priors model to incorporate nonconvex nonlocal low-rank prior and deep denoiser prior for high-accuracy image reconstruction. By means of the half-quadratic optimization theory and generalized soft-thresholding technique, we also develop an alternative optimization algorithm for solving the induced nonconvex optimization problem. Numerical simulations demonstrate the robustness and accuracy of the proposed robust CS method compared to some recent CS methods for image reconstruction in impulsive noise.
C1 [Li, Yunyi; Xiao, Fu; Gui, Linqing] Nanjing Univ Posts & Telecommun, Coll Comp, 9 Wenyuan Rd, Nanjing 210023, Jiangsu, Peoples R China.
   [Li, Yunyi; Liang, Wei] Hunan Univ Sci & Technol, Sch Comp Sci & Engn, Taoyuan Rd, Xiangtan 411201, Hunan, Peoples R China.
C3 Nanjing University of Posts & Telecommunications; Hunan University of
   Science & Technology
RP Xiao, F (corresponding author), Nanjing Univ Posts & Telecommun, Coll Comp, 9 Wenyuan Rd, Nanjing 210023, Jiangsu, Peoples R China.
EM yunyili@hnust.edu.cn; xiaof@njupt.edu.cn; wliang@hnust.edu.cn;
   guilinqing@163.com
FU National Key R&D Program of China [2022ZD0119004]; National Science Fund
   for Distinguished Young Scholars of China [62125203]; National Natural
   Science Foundation of China [62302163, 62272160]; Hunan Provincial
   Natural Science Foundation of China [2023JJ40296]; Project of
   Educational Commission of Hunan Province of China [21B0466]
FX This work was supported in part by the National Key R&D Program of China
   (Grant No. 2022ZD0119004), the National Science Fund for Distinguished
   Young Scholars of China (Grant No. 62125203), the National Natural
   Science Foundation of China (Grants No. 62302163 and No. 62272160), the
   Hunan Provincial Natural Science Foundation of China (Grant No.
   2023JJ40296), and the Project of Educational Commission of Hunan
   Province of China (Grant No. 21B0466).
CR Bai YN, 2020, SIGNAL PROCESS, V177, DOI 10.1016/j.sigpro.2020.107729
   Candès EJ, 2008, IEEE SIGNAL PROC MAG, V25, P21, DOI 10.1109/MSP.2007.914731
   Carrillo RE, 2013, IEEE T SIGNAL PROCES, V61, P4822, DOI 10.1109/TSP.2013.2274275
   Chen BJ, 2018, NEUROCOMPUTING, V275, P586, DOI 10.1016/j.neucom.2017.09.006
   Chen CLP, 2015, IEEE T IMAGE PROCESS, V24, P4014, DOI 10.1109/TIP.2015.2456432
   Chen ZY, 2021, IEEE J-STARS, V14, P2284, DOI 10.1109/JSTARS.2021.3053603
   Cheng ZH, 2023, IEEE T PATTERN ANAL, V45, P2264, DOI 10.1109/TPAMI.2022.3161934
   Dong WS, 2014, IEEE T IMAGE PROCESS, V23, P3618, DOI 10.1109/TIP.2014.2329449
   Feng L, 2019, INFORM SCIENCES, V477, P265, DOI 10.1016/j.ins.2018.10.050
   Gelvez T, 2021, IEEE T GEOSCI REMOTE, V59, P415, DOI 10.1109/TGRS.2020.2993541
   Geng TY, 2018, SIAM J IMAGING SCI, V11, P1878, DOI 10.1137/17M1154588
   Guan NY, 2019, IEEE T PATTERN ANAL, V41, P246, DOI 10.1109/TPAMI.2017.2777841
   Han XH, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3477396
   Han Y, 2018, IEEE T MED IMAGING, V37, P1418, DOI 10.1109/TMI.2018.2823768
   He YC, 2019, INFORM SCIENCES, V480, P381, DOI 10.1016/j.ins.2018.12.039
   Li YY, 2023, J FRANKLIN I, V360, P4172, DOI 10.1016/j.jfranklin.2023.01.041
   Li YY, 2020, SIGNAL PROCESS, V176, DOI 10.1016/j.sigpro.2020.107655
   Li YY, 2020, J FRANKLIN I, V357, P6370, DOI 10.1016/j.jfranklin.2020.03.032
   Lian QS, 2023, SIGNAL PROCESS, V202, DOI 10.1016/j.sigpro.2022.108737
   Liu Z, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3510373
   Ma SQ, 2008, PROC CVPR IEEE, P389
   MinghuaWang QiangWang, 2021, IEEE Trans. Geosci. Remote Sens., V59, P1
   Pan XG, 2022, IEEE T PATTERN ANAL, V44, P7474, DOI 10.1109/TPAMI.2021.3115428
   Pham DS, 2012, IEEE T IMAGE PROCESS, V21, P397, DOI 10.1109/TIP.2011.2162418
   Qin C, 2019, IEEE T MED IMAGING, V38, P280, DOI 10.1109/TMI.2018.2863670
   Ravishankar S, 2020, P IEEE, V108, P86, DOI 10.1109/JPROC.2019.2936204
   Sadrizadeh S, 2021, IEEE T CIRC SYST VID, V31, P38, DOI 10.1109/TCSVT.2020.2969563
   Shi BS, 2023, IEEE-CAA J AUTOMATIC, V10, P2136, DOI 10.1109/JAS.2023.123543
   Shi BS, 2023, IEEE T COMPUT IMAG, V9, P55, DOI 10.1109/TCI.2023.3241551
   Shi BS, 2019, SIGNAL PROCESS, V162, P83, DOI 10.1016/j.sigpro.2019.04.013
   Shi WZ, 2019, PROC CVPR IEEE, P12282, DOI 10.1109/CVPR.2019.01257
   Shi WZ, 2020, IEEE T IMAGE PROCESS, V29, P375, DOI 10.1109/TIP.2019.2928136
   Shi WZ, 2017, IEEE INT CON MULTI, P877, DOI 10.1109/ICME.2017.8019428
   Sun XN, 2019, IEEE SIGNAL PROC LET, V26, P1842, DOI 10.1109/LSP.2019.2952290
   Trevisi M, 2020, IEEE T CIRC SYST VID, V30, P387, DOI 10.1109/TCSVT.2019.2892178
   Wang LS, 2023, IEEE T PATTERN ANAL, V45, P9072, DOI 10.1109/TPAMI.2022.3225382
   Wang WH, 2015, IEEE J-STARS, V8, P2618, DOI 10.1109/JSTARS.2015.2401603
   Wang Y, 2022, SIGNAL PROCESS, V195, DOI 10.1016/j.sigpro.2022.108464
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZY, 2023, IEEE T CIRC SYST VID, V33, P1521, DOI 10.1109/TCSVT.2022.3214583
   Wen BH, 2020, IEEE SIGNAL PROC MAG, V37, P41, DOI 10.1109/MSP.2019.2951469
   Wen F, 2017, IEEE T COMPUT IMAG, V3, P566, DOI 10.1109/TCI.2017.2744626
   Wen F, 2017, IEEE T SIGNAL PROCES, V65, P105, DOI 10.1109/TSP.2016.2598316
   Wu ZL, 2023, INT J COMPUT VISION, V131, P1662, DOI 10.1007/s11263-023-01777-y
   Yang, 2021, ACM T SENSOR NETWORK, V17, P1
   Yang G, 2018, IEEE T MED IMAGING, V37, P1310, DOI 10.1109/TMI.2017.2785879
   Yang JF, 2011, SIAM J SCI COMPUT, V33, P250, DOI 10.1137/090777761
   Zha ZY, 2023, IEEE SIGNAL PROC MAG, V40, P32, DOI 10.1109/MSP.2022.3217936
   Zha ZY, 2021, IEEE T IMAGE PROCESS, V30, P5819, DOI 10.1109/TIP.2021.3086049
   Zha ZY, 2020, IEEE T IMAGE PROCESS, V29, P8960, DOI 10.1109/TIP.2020.3021291
   Zhang HM, 2022, IEEE T CYBERNETICS, V52, P3276, DOI 10.1109/TCYB.2020.3010960
   Zhang J, 2014, IEEE T IMAGE PROCESS, V23, P3336, DOI 10.1109/TIP.2014.2323127
   Zhang JW, 2018, PROC CVPR IEEE, P2521, DOI 10.1109/CVPR.2018.00267
   Zhang K, 2022, IEEE T PATTERN ANAL, V44, P6360, DOI 10.1109/TPAMI.2021.3088914
   Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891
   Zhang ML, 2019, IEEE T IMAGE PROCESS, V28, P868, DOI 10.1109/TIP.2018.2874284
   Zhuang Lihao, 2023, IEEE Trans. Circ. Syst. Video Technol., V14, P14
   Zuo WM, 2013, IEEE I CONF COMP VIS, P217, DOI 10.1109/ICCV.2013.34
NR 58
TC 0
Z9 0
U1 9
U2 9
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2024
VL 20
IS 6
SI SI
AR 170
DI 10.1145/3643032
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OQ2T3
UT WOS:001208681800020
DA 2024-08-05
ER

PT J
AU Yao, T
   Li, YR
   Li, Y
   Zhu, YY
   Wang, G
   Yue, J
AF Yao, Tao
   Li, Yiru
   Li, Ying
   Zhu, Yingying
   Wang, Gang
   Yue, Jun
TI Cross-modal Semantically Augmented Network for Image-text Matching
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image-text matching; cross-modal semantically augmented; adaptive
   word-type prediction model; relationship alignment
ID ATTENTION
AB Image-text matching plays an important role in solving the problem of cross-modal information processing. Since there are nonnegligible semantic differences between heterogenous pairwise data, a crucial challenge is how to learn a unified representation. Existing methods mainly rely on the alignment between regional image features and corresponding entity words. However, the regional features in the image are often more concerned with the foreground entity information, and the attribute information of the entities and the relational information are ignored. How to effectively integrate entity-attribute alignment and relationship alignment has not been fully studied. Therefore, we propose a Cross-Modal Semantically Augmented Network for Image-Text Matching (CMSAN), which combines the relationships between entities in the image with the semantics of relational words in the text. CMSAN (1) proposes an adaptive word-type prediction model that classifies the words into four types, i.e., entity word, attribute word, relation word, and unnecessary word. It can align different image features at multiple levels. CMSAN (2) designs a sophisticated relationship alignment module and an entity-attribute alignment module that maximizes the exploitation of the semantic information, which enables the model to have more discriminative power and further improves the matching accuracy.
C1 [Yao, Tao; Li, Yiru; Wang, Gang; Yue, Jun] Ludong Univ, Dept Informat & Elect Engn, Yantai, Peoples R China.
   [Yao, Tao] Southwest Jiaotong Univ, Yantai Res Inst New Generat Informat Technol, Yantai, Peoples R China.
   [Li, Ying] Nanjing Normal Univ, Sch Comp & Elect Informat, Nanjing, Peoples R China.
   [Li, Ying] Nanjing Normal Univ, Sch Artificial Intelligence, Nanjing, Peoples R China.
   [Zhu, Yingying] Shenzhen Univ, Sch Comp & Software, Shenzhen, Peoples R China.
C3 Ludong University; Southwest Jiaotong University; Nanjing Normal
   University; Nanjing Normal University; Shenzhen University
RP Li, YR (corresponding author), Ludong Univ, Dept Informat & Elect Engn, Yantai, Peoples R China.; Li, Y (corresponding author), Nanjing Normal Univ, Sch Comp & Elect Informat, Nanjing, Peoples R China.; Li, Y (corresponding author), Nanjing Normal Univ, Sch Artificial Intelligence, Nanjing, Peoples R China.
EM yaotao@ldu.edu.cn; lyr@hotmail.com; freeliying08@gmail.com;
   zhuyy@szu.edu.cn; gangwang1970@ldu.edu.cn; yuejuncn@sohu.com
OI Li, Ying/0000-0002-5695-4706; Yao, Tao/0000-0003-2660-1050
FU Natural Science Foundation of Shandong Province [ZR2023MF031]; National
   Natural Science Foundation of China [62102186]
FX This work is supported by the Natural Science Foundation of Shandong
   Province (Grant No. ZR2023MF031) and the National Natural Science
   Foundation of China (No. 62102186).
CR Chen JC, 2021, PROC CVPR IEEE, P15784, DOI 10.1109/CVPR46437.2021.01553
   Chen TL, 2020, AAAI CONF ARTIF INTE, V34, P10583
   Cho KYHY, 2014, Arxiv, DOI [arXiv:1406.1078, 10.48550/arXiv.1406.1078]
   Deng C, 2020, IEEE T IMAGE PROCESS, V29, P8892, DOI 10.1109/TIP.2020.3020383
   Deng C, 2019, IEEE T IMAGE PROCESS, V28, P4032, DOI 10.1109/TIP.2019.2903661
   Deng C, 2018, IEEE T IMAGE PROCESS, V27, P3893, DOI 10.1109/TIP.2018.2821921
   Diao HW, 2021, AAAI CONF ARTIF INTE, V35, P1218
   Ding GG, 2014, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2014.267
   Faghri F, 2018, Arxiv, DOI arXiv:1707.05612
   Ge XR, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5185, DOI 10.1145/3474085.3475634
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Gu JX, 2018, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR.2018.00750
   Haoran Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P18, DOI 10.1007/978-3-030-58586-0_2
   He XT, 2020, IEEE T CIRC SYST VID, V30, P520, DOI 10.1109/TCSVT.2019.2892802
   Hui Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12652, DOI 10.1109/CVPR42600.2020.01267
   Ji Z., 2021, arXiv
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Jiang SQ, 2014, MULTIMEDIA SYST, V20, P645, DOI 10.1007/s00530-012-0299-4
   Lan H, 2022, IEEE SIGNAL PROC LET, V29, P374, DOI 10.1109/LSP.2021.3135825
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Li CX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1, DOI 10.1145/3240508.3240547
   Li JT, 2022, AAAI CONF ARTIF INTE, P1323
   Li KP, 2019, IEEE I CONF COMP VIS, P4653, DOI 10.1109/ICCV.2019.00475
   Liu CX, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P3, DOI 10.1145/3343031.3350869
   Liu Chunxiao, 2020, CVPR, P10918, DOI DOI 10.1109/CVPR42600.2020.01093
   Liu Hong, 2016, IJCAI, P1767, DOI DOI 10.1109/TIP.2016.2564638
   Liu S, 2022, IEEE T PATTERN ANAL, V44, P4761, DOI 10.1109/TPAMI.2021.3079993
   Lu X, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1414, DOI 10.1145/3474085.3475598
   Lu X, 2021, IEEE T MULTIMEDIA, V23, P4541, DOI 10.1109/TMM.2020.3044473
   Lu X, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1129, DOI 10.1145/3343031.3350999
   Ma L, 2019, NEUROCOMPUTING, V345, P36, DOI 10.1016/j.neucom.2018.11.089
   Messina N, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3451390
   Min WQ, 2020, IEEE T MULTIMEDIA, V22, P3128, DOI 10.1109/TMM.2020.2974326
   Peng YX, 2020, IEEE T CIRC SYST VID, V30, P4368, DOI 10.1109/TCSVT.2019.2953692
   Peng YX, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3284750
   Rehman SU, 2019, NEUROCOMPUTING, V365, P171, DOI 10.1016/j.neucom.2019.06.084
   Rehman SU, 2018, ENTROPY-SWITZ, V20, DOI 10.3390/e20040290
   Rehman SU, 2017, AI COMMUN, V30, P311, DOI 10.3233/AIC-170739
   Rehman SU, 2016, 2016 IEEE INTERNATIONAL CONFERENCE OF ONLINE ANALYSIS AND COMPUTING SCIENCE (ICOACS), P139
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Song Y, 2019, PROC CVPR IEEE, P1979, DOI 10.1109/CVPR.2019.00208
   Su SP, 2019, IEEE I CONF COMP VIS, P3027, DOI 10.1109/ICCV.2019.00312
   Tian MX, 2023, IEEE T IMAGE PROCESS, V32, P471, DOI 10.1109/TIP.2022.3229631
   Wang D, 2020, PATTERN RECOGN, V107, DOI 10.1016/j.patcog.2020.107479
   Wang H, 2022, IEEE T PATTERN ANAL, V44, P9181, DOI 10.1109/TPAMI.2021.3123315
   Wang L, 2016, PROC CVPR IEEE, P5005, DOI 10.1109/CVPR.2016.541
   Wang SJ, 2020, IEEE WINT CONF APPL, P1497, DOI 10.1109/WACV45572.2020.9093614
   Wang T, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P12, DOI 10.1145/3343031.3350875
   Wang YX, 2021, IEEE T KNOWL DATA EN, V33, P3507, DOI 10.1109/TKDE.2020.2974825
   Wang YB, 2022, IEEE T CIRC SYST VID, V32, P4765, DOI 10.1109/TCSVT.2021.3136330
   Wen KY, 2021, IEEE T CIRC SYST VID, V31, P2866, DOI 10.1109/TCSVT.2020.3030656
   Wu JL, 2021, PATTERN RECOGN, V113, DOI 10.1016/j.patcog.2020.107813
   Wu J, 2022, IEEE T CIRC SYST VID, V32, P388, DOI 10.1109/TCSVT.2021.3060713
   Xie D, 2020, IEEE T IMAGE PROCESS, V29, P3626, DOI 10.1109/TIP.2020.2963957
   Xu X, 2020, IEEE T NEUR NET LEAR, V31, P5412, DOI 10.1109/TNNLS.2020.2967597
   Yan F, 2015, PROC CVPR IEEE, P3441, DOI 10.1109/CVPR.2015.7298966
   Yang SB, 2019, PROC CVPR IEEE, P4140, DOI 10.1109/CVPR.2019.00427
   Yao T, 2024, PATTERN RECOGN, V145, DOI 10.1016/j.patcog.2023.109934
   Yao T, 2023, IEEE T KNOWL DATA EN, V35, P1391, DOI 10.1109/TKDE.2021.3107489
   Yao T, 2020, NEUROCOMPUTING, V385, P358, DOI 10.1016/j.neucom.2019.12.086
   Yao T, 2020, IEEE T CYBERNETICS, V50, P4896, DOI 10.1109/TCYB.2019.2912644
   Yao T, 2019, PATTERN RECOGN, V89, P1, DOI 10.1016/j.patcog.2018.12.012
   Zaremba W, 2015, Arxiv, DOI [arXiv:1409.2329, DOI 10.48550/ARXIV.1409.2329]
   Zhang CY, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3412847
   Zhang K, 2023, IEEE T MULTIMEDIA, V25, P1320, DOI 10.1109/TMM.2022.3141603
   Zhang K, 2022, PROC CVPR IEEE, P15640, DOI 10.1109/CVPR52688.2022.01521
   Zhang Z, 2023, PATTERN RECOGN, V139, DOI 10.1016/j.patcog.2023.109462
   Zhang Z, 2023, IEEE T KNOWL DATA EN, V35, P5091, DOI 10.1109/TKDE.2022.3144352
   Zhang Z, 2019, IEEE T IMAGE PROCESS, V28, P4803, DOI 10.1109/TIP.2019.2912290
   Zhu L, 2024, IEEE T KNOWL DATA EN, V36, P239, DOI 10.1109/TKDE.2023.3282921
   Zhu L, 2022, ACM T INFORM SYST, V40, DOI 10.1145/3477180
NR 71
TC 1
Z9 1
U1 14
U2 14
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2024
VL 20
IS 4
AR 99
DI 10.1145/3631356
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6G9
UT WOS:001153382100009
DA 2024-08-05
ER

PT J
AU Shen, CJ
   Liu, ZJ
   Gao, X
   Feng, ZL
   Song, ML
AF Shen, Chengji
   Liu, Zhenjiang
   Gao, Xin
   Feng, Zunlei
   Song, Mingli
TI Self-Adaptive Clothing Mapping Based Virtual Try-on
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Virtual try-on; self-adaptive; clothing mapping; color difference
AB VTON (Virtual Try-ON), as an innovative visual application in e-commerce scenarios with great commercial value, has been widely studied in recent years. Due to its better robustness and realistic effect, deformation synthesize-based VTON has become the dominant approach in this field. Existing clothing deformation techniques optimize the mapping relations between the original clothing image and the ground truth (GT) image of the worn clothing. However, there are color differences between the original and GT clothing images caused by lighting, warping, and occlusion. The color differences may lead to misaligned clothing mapping by only minimizing the cost of pixel value difference. Another drawback is that taking the parsing prediction as GT will bring alignment remnant, rooting in the processing order of parsing and deformation. Aiming above two drawbacks, we put forward SAME-VTON (Self-Adaptive clothing Mapping basEd Virtual Try-ON) for achieving realistic virtual try-on results. The core of SAME-VTON is the self-adaptive clothing mapping technique, composed of two parts: a color-adaptive clothing mapping module and a parsing-adaptive prediction process. In the color-adaptive clothing mapping module, we map each pixel of the target clothing with a combination of multiple pixel values from the original clothing image, which considers both the position and color changes. Furthermore, different combination weights are learned to increase the diversity of color mapping. In the parsing-adaptive prediction process, the color-adaptive clothing mapping module is adopted to deform clothing first, then the human parsing result is predicted under the reference of the deformed clothing, which can avoid alignment remnant. Extensive experiments demonstrate that the proposed SAME-VTON with the self-adaptive clothing mapping technique can achieve optimal mapping in the case of large color differences and obtain superior results compared with existing deformation-synthesize-based VTON.
C1 [Shen, Chengji; Feng, Zunlei; Song, Mingli] Zhejiang Univ, Hangzhou, Zhejiang, Peoples R China.
   [Liu, Zhenjiang; Gao, Xin] Alibaba Grp, Hangzhou, Zhejiang, Peoples R China.
   [Feng, Zunlei; Song, Mingli] ZJU Bangsun Joint Res Ctr, Hangzhou, Zhejiang, Peoples R China.
C3 Zhejiang University; Alibaba Group
RP Feng, ZL (corresponding author), Zhejiang Univ, Hangzhou, Zhejiang, Peoples R China.; Feng, ZL (corresponding author), ZJU Bangsun Joint Res Ctr, Hangzhou, Zhejiang, Peoples R China.
EM chengji.shen@zju.edu.cn; stan.lzj@alibaba-inc.com;
   zimu.gx@alibaba-inc.com; zunleifeng@zju.edu.cn; brooksong@zju.edu.cn
RI zhang, zhang/KGK-5266-2024
OI Feng, Zunlei/0000-0001-8640-8434; Liu, Zhenjiang/0009-0007-3846-4977
FU National Natural Science Foundation of China [61976186, U20B2066];
   Ningbo Natural Science Foundation [2022J182]; Fundamental Research Funds
   for the Central Universities [2021FZZX001-23, 226-2023-00048]
FX This work is supported by National Natural Science Foundation of China
   (61976186, U20B2066), Ningbo Natural Science Foundation (2022J182), and
   the Fundamental Research Funds for the Central Universities
   (2021FZZX001-23, 226-2023-00048).
CR Boyi Jiang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P18, DOI 10.1007/978-3-030-58565-5_2
   Cheng WH, 2021, ACM COMPUT SURV, V54, DOI [10.1145/3447239, 10.1145/3552468.3554360]
   Choi S, 2021, PROC CVPR IEEE, P14126, DOI 10.1109/CVPR46437.2021.01391
   Chopra A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5413, DOI 10.1109/ICCV48922.2021.00538
   Cui Aiyu, 2021, P IEEECVF INT C COMP, P14638
   De Divitiis L, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3531017
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Diakogiannis FI, 2020, ISPRS J PHOTOGRAMM, V162, P94, DOI 10.1016/j.isprsjprs.2020.01.013
   Fele B, 2022, IEEE WINT CONF APPL, P2203, DOI 10.1109/WACV51458.2022.00226
   Feng ZL, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3326332
   Fincato M, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3491226
   Fincato M, 2021, INT C PATT RECOG, P7669, DOI 10.1109/ICPR48806.2021.9412052
   Gao X, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P563, DOI 10.1145/3474085.3475210
   Ge CJ, 2021, PROC CVPR IEEE, P16923, DOI 10.1109/CVPR46437.2021.01665
   Ge YY, 2021, PROC CVPR IEEE, P8481, DOI 10.1109/CVPR46437.2021.00838
   GOODFELLOW I, 2014, ADV NEURAL INFORM PR, V27
   Gu XL, 2024, ACM T MULTIM COMPUT, V20, DOI 10.1145/3545610
   Güler RA, 2018, PROC CVPR IEEE, P7297, DOI 10.1109/CVPR.2018.00762
   Han XT, 2018, Arxiv, DOI arXiv:1711.08447
   Han XT, 2019, IEEE I CONF COMP VIS, P10470, DOI 10.1109/ICCV.2019.01057
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heming Zhu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P512, DOI 10.1007/978-3-030-58452-8_30
   Hensel M, 2017, ADV NEUR IN, V30
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Hsieh CW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P275, DOI 10.1145/3343031.3351075
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Issenhuth Thibaut, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P619, DOI 10.1007/978-3-030-58565-5_37
   Jang E., 2017, ICLR (Poster)
   Jing YC, 2021, PROC CVPR IEEE, P15704, DOI 10.1109/CVPR46437.2021.01545
   Kingma D. P., 2015, P INT C LEARN REPR, P1
   Lewis K, 2021, Arxiv, DOI arXiv:2101.02285
   Lewis KM, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459884
   Li KD, 2021, PROC CVPR IEEE, P15541, DOI 10.1109/CVPR46437.2021.01529
   Li PK, 2022, IEEE T PATTERN ANAL, V44, P3260, DOI 10.1109/TPAMI.2020.3048039
   Liu LQ, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2659234
   Liu SH, 2023, PROC CVPR IEEE, P3759, DOI 10.1109/CVPR52729.2023.00366
   Ma QL, 2020, PROC CVPR IEEE, P6468, DOI 10.1109/CVPR42600.2020.00650
   Men YF, 2020, PROC CVPR IEEE, P5083, DOI 10.1109/CVPR42600.2020.00513
   Minar Matiur Rahman, 2020, P IEEE C COMP VIS PA
   Minar Matiur Rahman, 2020, P IEEE C COMP VIS PA, V4
   Mir A, 2020, PROC CVPR IEEE, P7021, DOI 10.1109/CVPR42600.2020.00705
   Morelli D, 2022, IEEE COMPUT SOC CONF, P2230, DOI 10.1109/CVPRW56347.2022.00243
   Parmar G, 2022, PROC CVPR IEEE, P11400, DOI 10.1109/CVPR52688.2022.01112
   Paszke A, 2019, ADV NEUR IN, V32
   Patel Chaitanya, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7363, DOI 10.1109/CVPR42600.2020.00739
   Rocco I, 2017, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2017.12
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Salimans T, 2016, Arxiv, DOI [arXiv:1606.03498, 10.48550/arXiv.1606.03498, DOI 10.48550/ARXIV.1606.03498]
   Santesteban I, 2021, PROC CVPR IEEE, P11758, DOI 10.1109/CVPR46437.2021.01159
   Schaefer S, 2006, ACM T GRAPHIC, V25, P533, DOI 10.1145/1141911.1141920
   Shen Yu, 2020, P EUR C COMP VIS
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song J, 2022, IEEE T IMAGE PROCESS, V31, P3359, DOI 10.1109/TIP.2022.3170728
   Sorkine O, 2007, Proc. Symposium on Geometry Processing, V4, P109, DOI [DOI 10.1145/1281991.1282006, 10.1145/1073204.1073323]
   Tiwari Garvita, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P1, DOI 10.1007/978-3-030-58580-8_1
   Wang BC, 2018, LECT NOTES COMPUT SC, V11217, P607, DOI 10.1007/978-3-030-01261-8_36
   Wang JH, 2020, Arxiv, DOI arXiv:1912.06324
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wood SN, 2003, J ROY STAT SOC B, V65, P95, DOI 10.1111/1467-9868.00374
   Xie ZY, 2021, ADV NEUR IN
   Xie ZY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3350, DOI 10.1145/3474085.3475490
   Xu C, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3554739
   Yang HT, 2020, PROC CVPR IEEE, P598, DOI 10.1109/CVPR42600.2020.00068
   Yang X., 2022, Adv. Neural Inf. Process. Syst., V35, P25739
   Yang X, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3425636
   Yu RY, 2019, IEEE I CONF COMP VIS, P10510, DOI 10.1109/ICCV.2019.01061
   Yurui Ren, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7687, DOI 10.1109/CVPR42600.2020.00771
   Zhao FW, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13219, DOI 10.1109/ICCV48922.2021.01299
NR 68
TC 0
Z9 0
U1 5
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2024
VL 20
IS 3
AR 61
DI 10.1145/3613453
PG 26
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6F8
UT WOS:001153381000001
DA 2024-08-05
ER

PT J
AU Zhang, YH
   Yao, T
   Qiu, ZF
   Mei, T
AF Zhang, Yiheng
   Yao, Ting
   Qiu, Zhaofan
   Mei, Tao
TI Explaining Cross-domain Recognition with Interpretable Deep Classifier
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Explainable; unsupervised domain adaptation; memory matching
AB The recent advances in deep learning predominantly construct models in their internal representations, and it is opaque to explain the rationale behind and decisions to human users. Such explainability is especially essential for domain adaptation, whose challenges require developing more adaptive models across different domains. In this article, we ask the question: How much does each sample in the source domain contribute to the network's prediction on the samples from the target domain? To address this, we devise a novel Interpretable Deep Classifier (IDC) that learns the nearest source samples of a target sample as evidence upon which the classifier makes the decision. Technically, IDC maintains a differentiable memory bank for each category, and the memory slot derives a form of key-value pair. The key records the features of discriminative source samples, and the value stores the corresponding properties, e.g., representative scores of the features for describing the category. IDC computes the loss between the output of IDC and the labels of source samples to back-propagate to adjust the representative scores and update the memory banks. Extensive experiments on Office-Home and VisDA-2017 datasets demonstrate that our IDC leads to a more explainable model with almost no accuracy degradation and effectively calibrates classification for optimum reject options. More remarkably, when taking IDC as a prior interpreter, capitalizing on 0.1% source training data selected by IDC still yields superior results than that uses full training set on VisDA-2017 for unsupervised domain adaptation.
C1 [Zhang, Yiheng; Yao, Ting; Qiu, Zhaofan; Mei, Tao] HiDream Inc, Beijing, Peoples R China.
RP Yao, T (corresponding author), HiDream Inc, Beijing, Peoples R China.
EM yihengzhang.chn@gmail.com; tingyao.ustc@gmail.com; zhaofanqiu@gmail.com;
   tmei@hidream.ai
RI Zhang, yicheng/HNC-5513-2023
OI Yao, Ting/0000-0001-7587-101X
CR Alvarez-Melis David, 2018, P C NEURAL INFORM PR
   Bargal Sarah Adel, 2018, P BRIT MACHINE VISIO
   Arrieta AB, 2020, INFORM FUSION, V58, P82, DOI 10.1016/j.inffus.2019.12.012
   Cai Q, 2019, PROC CVPR IEEE, P11449, DOI 10.1109/CVPR.2019.01172
   Cai Q, 2018, PROC CVPR IEEE, P4080, DOI 10.1109/CVPR.2018.00429
   Cao CS, 2015, IEEE I CONF COMP VIS, P2956, DOI 10.1109/ICCV.2015.338
   Chen CF, 2019, ADV NEUR IN, V32
   Chen Y, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3399, DOI 10.1145/3474085.3475496
   Chen Yang, 2021, GEN CHEMISTRYPRINCIP, P9164
   Core Mark G., 2006, P AAAI C ARTIFICIAL
   Craven M. W., 1996, Extracting Comprehensible Models from Trained Neural Networks
   Elliott A, 2021, PROC CVPR IEEE, P10688, DOI 10.1109/CVPR46437.2021.01055
   Feng ZL, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3326332
   Ganin Y, 2016, J MACH LEARN RES, V17
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Goodfellow I., 2014, P C NEURAL INFORM PR, P1
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Gunning D., 2017, Defense Advanced Research Projects Agency (DARPA), V2, P1
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou YZ, 2021, PROC CVPR IEEE, P13819, DOI 10.1109/CVPR46437.2021.01361
   Kaiser L, 2017, P INT C LEARNING REP
   Kang GL, 2019, PROC CVPR IEEE, P4888, DOI 10.1109/CVPR.2019.00503
   Kingma D. P., 2015, P INT C LEARN REPR, P1
   Li JJ, 2022, IEEE T KNOWL DATA EN, V34, P5770, DOI 10.1109/TKDE.2021.3060473
   Li JJ, 2022, IEEE T PATTERN ANAL, V44, P8196, DOI 10.1109/TPAMI.2021.3109287
   Li LZ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1026, DOI 10.1109/ICCV48922.2021.00108
   Li Oscar., 2017, P AAAI C ARTIFICIAL
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin YS, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3469288
   Long M., 2015, P INT C MACHINE LEAR
   Long MS, 2017, PR MACH LEARN RES, V70
   Long MS, 2018, ADV NEUR IN, V31
   Long Mingsheng, 2016, P C NEURAL INFORM PR
   Lu YW, 2022, IEEE T MULTIMEDIA, V24, P1871, DOI 10.1109/TMM.2021.3073258
   Miller Alexander, 2016, P C EMPIRICAL METHOD
   Na J, 2021, PROC CVPR IEEE, P1094, DOI 10.1109/CVPR46437.2021.00115
   Pan YW, 2019, PROC CVPR IEEE, P2234, DOI 10.1109/CVPR.2019.00234
   Pan YW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1789, DOI 10.1145/3123266.3127905
   Pan Yingwei, 2020, P IEEE C COMPUTER VI
   Paszke A., 2019, P C NEURAL INFORM PR
   Peng XC, 2017, Arxiv, DOI arXiv:1710.06924
   Petsiuk V., 2018, P BRIT MACH VIS C BM
   Pritzel A, 2017, PR MACH LEARN RES, V70
   Qiu ZF, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P225, DOI 10.1145/3077136.3080842
   Real E, 2017, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR.2017.789
   Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16
   Saito K, 2018, PROC CVPR IEEE, P3723, DOI 10.1109/CVPR.2018.00392
   Selvaraju RR, 2019, IEEE I CONF COMP VIS, P2591, DOI 10.1109/ICCV.2019.00268
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Sharma A, 2021, PROC CVPR IEEE, P5357, DOI 10.1109/CVPR46437.2021.00532
   Shuhao Cui, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12452, DOI 10.1109/CVPR42600.2020.01247
   Simonyan K., 2014, 13126034 ARXIV
   Springenberg J T, 2015, P INT C LEARNING REP
   Sukhbaatar Sainbayar, 2015, P C NEURAL INFORM PR
   Swartout W.R., 1981, Producing explanations and justifications of expert consulting programs
   Swartout W. R., 1993, Second Generation Expert Systems, P543, DOI [10.1007/978-3-642-77927-5_24, DOI 10.1007/978-3-642-77927-5]
   Thrun Sebastian., 1995, P C NEURAL INFORM PR
   Tzeng E, 2014, Arxiv, DOI [arXiv:1412.3474, DOI 10.48550/ARXIV.1412.3474]
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572
   Wang R, 2023, IEEE T MULTIMEDIA, V25, P1665, DOI 10.1109/TMM.2022.3146744
   Wang YL, 2020, IEEE T MULTIMEDIA, V22, P1796, DOI 10.1109/TMM.2019.2949872
   Weston Jason, 2014, P INT C LEARNING REP
   Wu L, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3486251
   Xia BH, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3474557
   Yan HL, 2020, IEEE T MULTIMEDIA, V22, P2420, DOI 10.1109/TMM.2019.2953375
   Yang X, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3425636
   Yao T, 2015, PROC CVPR IEEE, P2142, DOI 10.1109/CVPR.2015.7298826
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang JM, 2018, INT J COMPUT VISION, V126, P1084, DOI 10.1007/s11263-017-1059-x
   Zhang Jingyi, 2021, P IEEE C COMPUTER VI
   Zhang Y., 2020, P CVPR, P9621
   Zhang YH, 2018, PROC CVPR IEEE, P6810, DOI 10.1109/CVPR.2018.00712
   Zhang Yujia, 2019, P INT C MACHINE LEAR, P1
   Zhu LC, 2020, PROC CVPR IEEE, P4343, DOI 10.1109/CVPR42600.2020.00440
   Zunino A, 2021, IEEE COMPUT SOC CONF, P3227, DOI 10.1109/CVPRW53098.2021.00361
   Zunino A, 2021, INT J COMPUT VISION, V129, P1139, DOI 10.1007/s11263-020-01422-y
NR 77
TC 0
Z9 0
U1 3
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2024
VL 20
IS 3
AR 67
DI 10.1145/3623399
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6F8
UT WOS:001153381000007
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Chen, LZ
   Li, W
   Cui, XH
   Wang, ZY
   Berretti, S
   Wan, SH
AF Chen, Liangzhe
   Li, Wei
   Cui, Xiaohui
   Wang, Zhenyu
   Berretti, Stefano
   Wan, Shaohua
TI MS-GDA: Improving Heterogeneous Recipe Representation via Multinomial
   Sampling Graph Data Augmentation
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE recipe data augmentation; heterogeneous representation; graph neural
   network; network embedding
AB We study the problem of classifying different cooking styles, based on the recipe. The difficulty is that the same food ingredients, seasoning, and the very similar instructions result in different flavors, with different cooking styles. Existing methods have limitations: they mainly focus on homogeneous data (e.g., instruction or image), ignoring heterogeneous data (e.g., flavor compound or ingredient), which certainly hurts the classification performance. This is because collecting enough available heterogeneous data of a recipe is a non-trivial task. In this paper, we present a new heterogeneous data augmentation method to improve classification performance. Specifically, we first construct a heterogeneous recipe graph network to represent heterogeneous data, which includes four main-stream types of heterogeneous data: ingredient, flavor compound, image, and instruction. Then, we draw a sequence of augmented graphs for Semi-Supervised learning through multinomial sampling. The probability distribution of sampling depends on the Cosine distance between the nodes of graph. In this way, we name our approach as Multinomial Sampling Graph Data Augmentation (MS-GDA). Extensive experiments demonstrate that MS-GDA significantly outperforms SOTA baselines on cuisine classification and region prediction with the recipe benchmark dataset. Code is available at https://github.com/LiangzheChen/MS-GDA.
C1 [Chen, Liangzhe; Li, Wei] Jiangnan Univ, Minist Educ, Sch Artificial Intelligence & Comp Sci, 1800 Li Lake Ave, Wuxi 214122, Jiangsu, Peoples R China.
   [Chen, Liangzhe; Li, Wei] Jiangnan Univ, Minist Educ, Jiangsu Key Lab Media Design & Software Technol, 1800 Li Lake Ave, Wuxi 214122, Jiangsu, Peoples R China.
   [Chen, Liangzhe; Li, Wei] Jiangnan Univ, Engn Res Ctr Intelligent Technol Healthcare, Minist Educ, 1800 Li Lake Ave, Wuxi 214122, Jiangsu, Peoples R China.
   [Cui, Xiaohui] Wuhan Univ, Sch Cyber Sci & Engn, Luojiashan Rd, Wuhan 430072, Hubei, Peoples R China.
   [Wang, Zhenyu] Jiaxing Inst Future Food, Jiaxing 314000, Zhejiang, Peoples R China.
   [Berretti, Stefano] Univ Firenze, Informat Engn, I-50100 Florence, Italy.
   [Wan, Shaohua] Univ Elect Sci & Technol China, Shenzhen Inst Adv Study, Shenzhen 518055, Guangdong, Peoples R China.
C3 Jiangnan University; Jiangnan University; Jiangnan University; Wuhan
   University; University of Florence; University of Electronic Science &
   Technology of China; Shenzhen Institute for Advanced Study, UESTC
RP Li, W (corresponding author), Jiangnan Univ, Minist Educ, Sch Artificial Intelligence & Comp Sci, 1800 Li Lake Ave, Wuxi 214122, Jiangsu, Peoples R China.; Li, W (corresponding author), Jiangnan Univ, Minist Educ, Jiangsu Key Lab Media Design & Software Technol, 1800 Li Lake Ave, Wuxi 214122, Jiangsu, Peoples R China.; Li, W (corresponding author), Jiangnan Univ, Engn Res Ctr Intelligent Technol Healthcare, Minist Educ, 1800 Li Lake Ave, Wuxi 214122, Jiangsu, Peoples R China.; Wan, SH (corresponding author), Univ Elect Sci & Technol China, Shenzhen Inst Adv Study, Shenzhen 518055, Guangdong, Peoples R China.
EM lzchen@stu.jiangnan.edu.cn; cs_weili@jiangnan.edu.cn; xcui@whu.edu.cn;
   zhenyuwang@whu.edu.cn; stefano.berretti@unifi.it; shaohua.wan@ieee.org
RI Berretti, Stefano/U-9004-2019; Wan, Shaohua/B-9243-2014
OI Berretti, Stefano/0000-0003-1219-4386; Wan, Shaohua/0000-0001-7013-9081;
   cui, xiaohui/0000-0001-6079-009X
CR Abbar S, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P3197, DOI 10.1145/2702123.2702153
   Achananuparp P, 2018, DH '18: PROCEEDINGS OF THE 2018 INTERNATIONAL CONFERENCE ON DIGITAL HEALTH, P35, DOI 10.1145/3194658.3194663
   Adjeisah M, 2023, COMPUT SCI REV, V47, DOI 10.1016/j.cosrev.2022.100527
   Ahn YY, 2011, SCI REP-UK, V1, DOI 10.1038/srep00196
   Brody S, 2022, Arxiv, DOI arXiv:2105.14491
   Chen M, 2020, INFORM PROCESS MANAG, V57, DOI 10.1016/j.ipm.2019.05.012
   Chen Y., 2015, Master's thesis
   Deng LX, 2022, IEEE T MULTIMEDIA, V24, P2034, DOI 10.1109/TMM.2021.3075037
   Dong YX, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P135, DOI 10.1145/3097983.3098036
   Fey M, 2018, PROC CVPR IEEE, P869, DOI 10.1109/CVPR.2018.00097
   Gou JP, 2024, ACM T MULTIM COMPUT, V20, DOI 10.1145/3568679
   Hamilton WL, 2017, ADV NEUR IN, V30
   Haussmann S, 2019, LECT NOTES COMPUT SC, V11779, P146, DOI 10.1007/978-3-030-30796-7_10
   He Ziqiang, 2023, IEEE Transactions on Artificial Intelligence
   Ho D, 2019, PR MACH LEARN RES, V97
   Izadi MR, 2020, IEEE INT CONF BIG DA, P171, DOI 10.1109/BigData50022.2020.9378063
   Kong K., 2020, arXiv
   Li DY, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1719, DOI 10.1145/3394486.3403223
   Li W, 2023, IEEE T AUTOM SCI ENG, DOI [10.1142/S021812742330001X, 10.1109/TASE.2023.3309629]
   Li Y, 2023, IEEE T KNOWL DATA EN, V35, P4880, DOI 10.1109/TKDE.2022.3146270
   Liang HZ, 2021, IEEE T MULTIMEDIA, V23, P3551, DOI 10.1109/TMM.2020.3028478
   Liu Songtao, 2022, P MACHINE LEARNING R
   Luan ST, 2021, Arxiv, DOI arXiv:2109.05641
   Luo Y, 2021, Arxiv, DOI arXiv:2106.08541
   Luo Y, 2022, MATHEMATICS-BASEL, V10, DOI 10.3390/math10081262
   Ma J, 2020, ADV NEURAL INFORM PR
   Ma JQ, 2022, WSDM'22: PROCEEDINGS OF THE FIFTEENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P675, DOI 10.1145/3488560.3498497
   Marin J, 2021, IEEE T PATTERN ANAL, V43, P187, DOI 10.1109/TPAMI.2019.2927476
   Park D, 2019, arXiv
   Park DJ, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-87171-5
   Perozzi B, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P701, DOI 10.1145/2623330.2623732
   Qian SS, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3451215
   Rokicki Markus, 2018, 12 INT AAAI C WEB SO
   Rong Y, 2020, Arxiv, DOI [arXiv:1907.10903, DOI 10.48550/ARXIV.1907.10903]
   Sarigun A., 2022, 1 LEARN GRAPHS C
   Schlichtkrull M, 2018, LECT NOTES COMPUT SC, V10843, P593, DOI 10.1007/978-3-319-93417-4_38
   Simas T., 2017, Front. ICT, V4, DOI [10.3389/fict.2017.00014, DOI 10.3389/FICT.2017.00014]
   Tian YJ, 2021, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT, CIKM 2021, P1824, DOI 10.1145/3459637.3482468
   Verma V, 2021, AAAI CONF ARTIF INTE, V35, P10024
   Wang HW, 2020, Arxiv, DOI arXiv:2002.06755
   Wu Tian, 2023, ACM Transactions on Multimedia Computing, Communications and Applications
   Zhang Y, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3542820
   Zhao T, 2020, Arxiv, DOI arXiv:2006.06830
NR 43
TC 0
Z9 0
U1 3
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2024
VL 20
IS 7
SI SI
AR 199
DI 10.1145/3648620
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL0Q6
UT WOS:001234494100014
DA 2024-08-05
ER

PT J
AU Han, XQ
   Han, B
   Li, JR
   Song, CX
AF Han, Xueqiang
   Han, Biao
   Li, Jinrong
   Song, Congxi
TI Multi-agent DRL-based Multipath Scheduling for Video Streaming with QUIC
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Multipath QUIC; adaptive packet scheduling; multi-agent deep
   reinforcement learning; video streaming
AB The popularization of video streaming brings challenges in satisfying diverse Quality of Service (QoS) requirements. The multipath extension of the Quick UDP Internet Connection (QUIC) protocol, also called MPQUIC, has the potential to improve video streaming performance with multiple simultaneously transmitting paths. The multipath scheduler of MPQUIC determines how to distribute the packets onto different paths. However, while applying current multipath schedulers into MPQUIC, our experimental results show that they fail to adapt to various receive buffer sizes of different devices and comprehensive QoS requirements of video streaming. These problems are especially severe under heterogeneous and dynamic network environments. To tackle these problems, we propose MARS, a Multi-agent deep Reinforcement learning (MADRL)-based Multipath QUIC Scheduler, which is able to promptly adapt to dynamic network environments. It exploits the MADRL method to learn a neural network for each path and generate scheduling policy. Besides, it introduces a novel multi-objective reward function that takes out-of-order queue size and different QoS metrics into consideration to realize adaptive scheduling optimization. We implement MARS in an MPQUIC prototype and deploy in Dynamic Adaptive Streaming over HTTP system. Then, we compare it with the state-of-the-art multipath schedulers in both emulated and real-world networks. Experimental results show that MARS outperforms the other schedulers with better adaptive capability regarding the receive buffer sizes and QoS.
C1 [Han, Xueqiang; Han, Biao; Li, Jinrong; Song, Congxi] Natl Univ Def Technol, 109 Deya Rd, Changsha 410073, Peoples R China.
C3 National University of Defense Technology - China
RP Han, B (corresponding author), Natl Univ Def Technol, 109 Deya Rd, Changsha 410073, Peoples R China.
EM hxq205732204@163.com; nudtbill@nudt.edu.cn; lijinrong15@nudt.edu.cn;
   songcongxi17@nudt.edu.cn
OI Han, Xueqiang/0009-0001-1437-763X; Song, Congxi/0000-0002-7672-0915
FU National Natural Science Foundation of China [62102430, 2020RC3027];
   Training Program for Excellent Young Innovators of Changsha [kq2206001]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant No. 62102430, in part by theHunan Young
   Talents under Grant No. 2020RC3027, and in part by the Training Program
   for Excellent Young Innovators of Changsha under Grant No. kq2206001.
CR [Anonymous], 2012, P INT MEAS C
   Bouacida N, 2019, IEEE T MOBILE COMPUT, V18, P1885, DOI 10.1109/TMC.2018.2868670
   De Coninck Q, 2017, CONEXT'17: PROCEEDINGS OF THE 2017 THE 13TH INTERNATIONAL CONFERENCE ON EMERGING NETWORKING EXPERIMENTS AND TECHNOLOGIES, P160, DOI 10.1145/3143361.3143370
   Ferlin S, 2016, 2016 IFIP NETWORKING CONFERENCE (IFIP NETWORKING) AND WORKSHOPS, P431, DOI 10.1109/IFIPNetworking.2016.7497206
   Ferlin-Oliveira S, 2014, 2014 IEEE 22ND INTERNATIONAL SYMPOSIUM OF QUALITY OF SERVICE (IWQOS), P123, DOI 10.1109/IWQoS.2014.6914310
   Ford A., 2013, RFC 6824
   Han B, 2016, PROCEEDINGS OF THE 12TH INTERNATIONAL CONFERENCE ON EMERGING NETWORKING EXPERIMENTS AND TECHNOLOGIES (CONEXT'16), P129, DOI 10.1145/2999572.2999606
   Han Xueqiang, 2023, P IEEE ACM 31 INT S, P01, DOI [10.1109/IWQoS57198.2023.10188744, DOI 10.1109/IWQOS57198.2023.10188744]
   Handigol N., 2012, CoNEXT, P253
   He B, 2021, IEEE T NETW SERV MAN, V18, P4770, DOI 10.1109/TNSM.2021.3093302
   Hemminger S., 2005, Linux conf au, P18
   Iyengar J., 2021, RFC 9000, DOI [10.17487/RFC9000, DOI 10.17487/RFC9000]
   Ji XL, 2022, INT WORKSH QUAL SERV, DOI 10.1109/IWQoS54832.2022.9812886
   Jiang JC, 2014, IEEE ACM T NETWORK, V22, P326, DOI 10.1109/TNET.2013.2291681
   Khalili R, 2013, IEEE ACM T NETWORK, V21, P1651, DOI 10.1109/TNET.2013.2274462
   Kuhn N, 2014, IEEE ICC, P1222, DOI 10.1109/ICC.2014.6883488
   Lillicrap T.P., 2015, Continuous control with deep reinforcement learning
   Lim YS, 2017, CONEXT'17: PROCEEDINGS OF THE 2017 THE 13TH INTERNATIONAL CONFERENCE ON EMERGING NETWORKING EXPERIMENTS AND TECHNOLOGIES, P147, DOI 10.1145/3143361.3143376
   Lowe R, 2017, ADV NEUR IN, V30
   Meng Zili, 2022, SIGCOMM '22: Proceedings of the ACM SIGCOMM 2022 Conference, P193, DOI 10.1145/3544216.3544225
   Paasch C., 2014, P 2014 ACM SIGCOMM W, P27, DOI DOI 10.1145/2630088.2631977
   Raiciu C., 2012, P 9 USENIX S NETW SY
   Raiciu C, 2011, ACM SIGCOMM COMP COM, V41, P266, DOI 10.1145/2043164.2018467
   Shi H, 2018, PROCEEDINGS OF THE 2018 USENIX ANNUAL TECHNICAL CONFERENCE, P719
   Song CX, 2024, IEEE NETWORK, V38, P202, DOI 10.1109/MNET.2023.3321521
   Spiteri K, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3336497
   Stockhammer T., 2011, Proceedings of the second annual ACM conference on Multimedia systems, P133
   Sutton RS, 2018, ADAPT COMPUT MACH LE, P1
   Taraghi B, 2022, PROCEEDINGS OF THE 13TH ACM MULTIMEDIA SYSTEMS CONFERENCE, MMSYS 2022, P216, DOI 10.1145/3524273.3532889
   Wu HJ, 2020, IEEE J SEL AREA COMM, V38, P2295, DOI 10.1109/JSAC.2020.3000365
   X. Sandvine, 2023, 2023 global internet phenomena report
   Xing YT, 2021, IEEE T WIREL COMMUN, V20, P7230, DOI 10.1109/TWC.2021.3081498
   Zhang H, 2019, IEEE INFOCOM SER, P1648, DOI [10.1109/infocom.2019.8737649, 10.1109/INFOCOM.2019.8737649]
   Zhang L, 2022, IEEE ACM T NETWORK, V30, P2274, DOI 10.1109/TNET.2022.3167713
   Zheng ZL, 2021, SIGCOMM '21: PROCEEDINGS OF THE 2021 ACM SIGCOMM 2021 CONFERENCE, P418, DOI 10.1145/3452296.3472893
NR 35
TC 0
Z9 0
U1 2
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2024
VL 20
IS 7
SI SI
AR 211
DI 10.1145/3649139
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL0Q6
UT WOS:001234494100026
DA 2024-08-05
ER

PT J
AU Fang, WH
   Xie, JY
   Liu, HF
   Chen, JL
   Cai, Y
AF Fang, Wenhao
   Xie, Jiayuan
   Liu, Hongfei
   Chen, Jiali
   Cai, Yi
TI Diverse Visual Question Generation Based on Multiple Objects Selection
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Multimodal; visual question generation; mixture of experts
AB Visual question generation task aims at generating high-quality questions about a given image. To make this tak applicable to various scenarios, e.g., the growing demand for exams, it is important to generate diverse questions. The existing methods for this task control diverse question generation based on different question types, e.g., "what" and "when." Although different question types lead to description diversity, they cannot guarantee semantic diversity when asking the same objects. Research in the field of psychology shows that humans pay attention to different objects in an image based on their preferences, which is beneficial to constructing semantically diverse questions. According to the research, we propose a multi-selector visual question generation (MS-VQG) model that aims to focus on different objects to generate diverse questions. Specifically, our MS-VQG model employs multiple selectors to imitate different humans to select different objects in a given image. Based on these different selected objects, our MS-VQG model can generate diverse questions corresponding to each selector. Extensive experiments on two datasets show that our proposed model outperforms the baselines in generating diverse questions.
C1 [Fang, Wenhao; Xie, Jiayuan; Liu, Hongfei; Chen, Jiali; Cai, Yi] South China Univ Technol, Sch Software Engn, Guangzhou, Peoples R China.
   [Fang, Wenhao; Xie, Jiayuan; Liu, Hongfei; Chen, Jiali; Cai, Yi] SCUT, Key Lab Big Data & Intelligent Robot, MOE China, Guangzhou, Peoples R China.
   [Cai, Yi] Peng Cheng Lab, Shenzhen, Peoples R China.
C3 South China University of Technology; South China University of
   Technology; Peng Cheng Laboratory
RP Cai, Y (corresponding author), South China Univ Technol, Sch Software Engn, Guangzhou, Peoples R China.; Cai, Y (corresponding author), SCUT, Key Lab Big Data & Intelligent Robot, MOE China, Guangzhou, Peoples R China.; Cai, Y (corresponding author), Peng Cheng Lab, Shenzhen, Peoples R China.
EM sewenhao-fang@mail.scut.edu.cn; sexiejiayuan@mail.scut.edu.cn;
   201830340384@mail.scut.edu.cn; segarychen@mail.scut.edu.cn;
   ycai@scut.edu.cn
RI Fang, Wenhao/C-3842-2015
OI Fang, Wenhao/0000-0002-2449-3749
FU National Natural Science Foundation of China [62076100, 62072188];
   Fundamental Research Funds for the Central Universities, SCUT
   [x2rjD2230080]; Science and Technology Planning Project of Guangdong
   Province [2020B0101100002]; CAAI-Huawei MindSpore Open FundCCF-Zhipu AI
   Large Model Fund
FX This work was supported by the National Natural Science Foundation of
   China (62076100, 62072188), Fundamental Research Funds for the Central
   Universities, SCUT (x2rjD2230080), the Science and Technology Planning
   Project of Guangdong Province (2020B0101100002), and then CAAI-Huawei
   MindSpore Open FundCCF-Zhipu AI Large Model Fund.
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Aneja J, 2019, IEEE I CONF COMP VIS, P4260, DOI 10.1109/ICCV.2019.00436
   [Anonymous], 2014, P INT C LEARN REPR I
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Bird Steven., 2006, P COLING ACL 2006 IN, P69, DOI 10.3115/1225403.1225421
   Chen F., 2019, ADV NEUR IN, P1929
   Chen Jiali, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P5132, DOI 10.1145/3581783.3612536
   Cho J, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P3121
   Cho K., 2014, P 2014 C EMP METH NA, DOI 10.3115/v1/D14-1179
   Fan ZH, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4048
   Fang ZW, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3282469
   Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Huang Q., 2020, P 58 ANN M ASS COMP, P7166
   Jacobs RA, 1991, NEURAL COMPUT, V3, P79, DOI 10.1162/neco.1991.3.1.79
   James William., 1890, The Principles of Psychology, VVol. 1, DOI DOI 10.1037/10538-000
   Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215
   Keogh B.K., 1981, FUTURE PSYCHOL SCH P, V10, P278
   Kingma D. P., 2014, arXiv
   Krishna R, 2019, PROC CVPR IEEE, P2008, DOI 10.1109/CVPR.2019.00211
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Kunichika Hidenobu, 2004, P IEEE INT C CONS EL
   Lee S, 2016, ADV NEUR IN, V29
   Li J., 2023, P MACHINE LEARNING R, P19730
   Li LJ, 2019, IEEE I CONF COMP VIS, P10312, DOI 10.1109/ICCV.2019.01041
   Li Q, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3300938
   Li YK, 2018, PROC CVPR IEEE, P6116, DOI 10.1109/CVPR.2018.00640
   Lin C.-Y., 2004, ANN M ASS COMP LING, P74
   Liu LX, 2019, IEEE I CONF COMP VIS, P4239, DOI 10.1109/ICCV.2019.00434
   Liu YB, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3498340
   Mahajan Shweta, 2020, P ANN C NEUR INF PRO
   Malinowski M, 2014, ADV NEUR IN, V27
   Mostafazadeh N, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1802
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Pennington J., 2014, GLOVE GLOBAL VECTORS, DOI DOI 10.3115/V1/D14-1162
   Ren MY, 2015, ADV NEUR IN, V28
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Simonyan K., 2014, C TRACK P
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Vijayakumar AK, 2018, AAAI CONF ARTIF INTE, P7371
   Wu Bo, 2021, P ANN C NEUR INF PRO
   Xie JY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4546, DOI 10.1145/3474085.3476969
   Xie JY, 2022, IEEE-ACM T AUDIO SPE, V30, P280, DOI 10.1109/TASLP.2021.3138706
   Xu JJ, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P3940
   Xu X, 2021, IEEE T NEUR NET LEAR, V32, P1654, DOI 10.1109/TNNLS.2020.2986029
   Yu DF, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3316767
   Zhang Tianyi, 2020, P ICLR
   Zhang YH, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P2205
NR 49
TC 0
Z9 0
U1 3
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2024
VL 20
IS 6
SI SI
AR 161
DI 10.1145/3640014
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OQ2T3
UT WOS:001208681800011
DA 2024-08-05
ER

PT J
AU Jha, M
   Bhandari, AK
AF Jha, Manvi
   Bhandari, Ashish Kumar
TI NSDIE: Noise Suppressing Dark Image Enhancement Using Multiscale Retinex
   and Low-Rank Minimization
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Image enhancement; nighttime scenes; multiscale retinex model; camera
   response function
ID HISTOGRAM EQUALIZATION
AB It is inevitable for dark images to have crucial information obscured by low-light conditions, which are worsened by the presence of noise in these images. This work introduces a groundbreaking solution, Noise-Suppressing Dark Image Enhancement for Web Apps (NSDIE), to address the challenging task of enhancing low-light images marred by noise. The proposed work utilizes a low-rank model with simultaneous enhancement of reflectance and illumination components to improve the nighttime scenes while also eradicating the present noise of the image. The reflectance component is further processed using a multiscale retinex model to compensate for the possible color distortions while the illumination component is enhanced using the camera response model to ensure the genuineness of the scene. The proposed work is also tested for a standalone application and is presented to the user through a web portal to aid the concerns of dark image enhancement in the daily life of the user. Rigorous quantitative and qualitative analyses assert NSDIE's superiority over existing techniques, establishing its pivotal role in addressing the critical concern of dark image enhancement.
C1 [Jha, Manvi; Bhandari, Ashish Kumar] Natl Inst Technol Patna, Dept Elect & Commun Engn, Patna 800005, Bihar, India.
C3 National Institute of Technology (NIT System); National Institute of
   Technology Patna
RP Jha, M (corresponding author), Natl Inst Technol Patna, Dept Elect & Commun Engn, Patna 800005, Bihar, India.
EM manvij.ug19.ec@nitp.ac.in; bhandari.iiitj@gmail.com
OI Bhandari, Ashish Kumar/0000-0001-9842-8125; Jha,
   Manvi/0000-0001-5613-7039
CR Agrawal SC, 2022, IEEE T CIRC SYST VID, V32, P593, DOI 10.1109/TCSVT.2021.3068625
   Arici T, 2009, IEEE T IMAGE PROCESS, V18, P1921, DOI 10.1109/TIP.2009.2021548
   Bhandari AK, 2020, IEEE T FUZZY SYST, V28, P2009, DOI 10.1109/TFUZZ.2019.2930028
   Cai JR, 2018, IEEE T IMAGE PROCESS, V27, P2049, DOI 10.1109/TIP.2018.2794218
   Channappayya SS, 2008, IEEE T IMAGE PROCESS, V17, P857, DOI 10.1109/TIP.2008.921328
   Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304
   Fu XY, 2016, SIGNAL PROCESS, V129, P82, DOI 10.1016/j.sigpro.2016.05.031
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Hao SJ, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3498341
   Hao SJ, 2020, IEEE T MULTIMEDIA, V22, P3025, DOI 10.1109/TMM.2020.2969790
   Jha M, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3165303
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P451, DOI 10.1109/83.557356
   Kong XY, 2021, IEEE SIGNAL PROC LET, V28, P1540, DOI 10.1109/LSP.2021.3096160
   Kumar M, 2022, IEEE T CONSUM ELECTR, V68, P401, DOI 10.1109/TCE.2022.3209791
   Li MD, 2018, IEEE T IMAGE PROCESS, V27, P2828, DOI 10.1109/TIP.2018.2810539
   Loh YP, 2019, COMPUT VIS IMAGE UND, V178, P30, DOI 10.1016/j.cviu.2018.10.010
   Majeed SH, 2021, IEEE ACCESS, V9, P6402, DOI 10.1109/ACCESS.2020.3048148
   Mann S, 2000, IEEE T IMAGE PROCESS, V9, P1389, DOI 10.1109/83.855434
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Pizer S. M., 1990, Proceedings of the First Conference on Visualization in Biomedical Computing (Cat. No.90TH0311-1), P337, DOI 10.1109/VBC.1990.109340
   Ren XT, 2020, IEEE T IMAGE PROCESS, V29, P5862, DOI 10.1109/TIP.2020.2984098
   Ren YR, 2019, IEEE T CIRC SYST VID, V29, P968, DOI 10.1109/TCSVT.2018.2828141
   Singh N, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3096266
   Srinivas K, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3597303
   Tanaka M, 2019, I SYMP CONSUM ELECTR, DOI DOI 10.1109/icce.2019.8662059
   Wang Q, 2007, IEEE T CONSUM ELECTR, V53, P757, DOI 10.1109/TCE.2007.381756
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wang Y, 1999, IEEE T CONSUM ELECTR, V45, P68, DOI 10.1109/30.754419
   Wang YF, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2922106
   Wei C, 2018, Arxiv, DOI arXiv:1808.04560
   Wu XM, 2021, IEEE T CIRC SYST VID, V31, P863, DOI 10.1109/TCSVT.2020.2991437
   Xu X, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3414839
   Yan J, 2019, Arxiv, DOI arXiv:1904.08879
   Yang WH, 2021, IEEE T IMAGE PROCESS, V30, P2072, DOI 10.1109/TIP.2021.3050850
   Ying ZQ, 2017, Arxiv, DOI arXiv:1711.00591
   Yue GH, 2019, IEEE T INSTRUM MEAS, V68, P2733, DOI 10.1109/TIM.2018.2868555
   Zhai GT, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3457905
   Zhou ML, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3590965
NR 39
TC 0
Z9 0
U1 5
U2 5
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUN
PY 2024
VL 20
IS 6
SI SI
AR 160
DI 10.1145/3638772
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OQ2T3
UT WOS:001208681800010
DA 2024-08-05
ER

PT J
AU Wang, DP
   Xu, RF
   Cheng, LL
   Wang, ZW
AF Wang, Depei
   Xu, Ruifeng
   Cheng, Lianglun
   Wang, Zhuowei
TI Knowledge-integrated Multi-modal Movie Turning Point Identification
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Knowledge enhance; multimodal representation; text tagging
AB The rapid development of artificial intelligence provides rich technologies and tools for the automated understanding of literary works. As a comprehensive carrier of storylines, movies are natural multimodal data sources that provide sufficient data foundations, and how to fully leverage the benefits of data remains a sustainable research hotspot. In addition, the efficient representation of multi-source data also poses new challenges for information fusion technology. Therefore, we propose a knowledge-enhanced turning points identification (KTPi) method for multimodal scene recognition. First, the BiLSTM method is used to encode scene text and integrate contextual information into scene representations to complete text sequence modeling. Then, the graph structure is used to model all scenes, which strengthens long-range semantic dependencies between scenes and enhances scene representations using graph convolution network. After, the self-supervised method is used to obtain the optimal number of neighboring nodes in sparse graph. Next, actor and verb knowledge involved in the scene text are added to the multimodal data to enhance the diversity of scene feature expressions. Finally, the teacher-student network strategy is used to train the KTPi model. Experimental results show that KTPi outperforms baseline methods in scene role recognition tasks, and ablation experiments show that incorporating knowledge into multimodal model can improve its performance.
C1 [Wang, Depei] Guangdong Southern Planning Designing Inst Teleco, Shenzhen, Peoples R China.
   [Wang, Depei; Xu, Ruifeng] Harbin Inst Technol Shenzhen, Harbin, Peoples R China.
   [Xu, Ruifeng] Peng Cheng Lab, Shenzhen, Peoples R China.
   [Cheng, Lianglun; Wang, Zhuowei] Guangdong Univ Technol, Guangzhou, Peoples R China.
   [Wang, Depei] Guangdong Southern Planning Designing Inst Teleco, Yuehai St, Shenzhen 518063, Guangdong, Peoples R China.
   [Wang, Depei; Xu, Ruifeng] Harbin Inst Technol Shenzhen, Taoyuan St, Taoyuan 518055, Guangdong, Peoples R China.
   [Xu, Ruifeng] Peng Cheng Lab, Xingke 1 St, Shenzhen 518000, Guangdong, Peoples R China.
   [Cheng, Lianglun; Wang, Zhuowei] Guangdong Univ Technol, Xiaoguwei St, Guangzhou 510006, Guangdong, Peoples R China.
C3 Harbin Institute of Technology; Peng Cheng Laboratory; Guangdong
   University of Technology; Harbin Institute of Technology; Peng Cheng
   Laboratory; Guangdong University of Technology
RP Xu, RF (corresponding author), Harbin Inst Technol Shenzhen, Taoyuan St, Taoyuan 518055, Guangdong, Peoples R China.; Xu, RF (corresponding author), Peng Cheng Lab, Xingke 1 St, Shenzhen 518000, Guangdong, Peoples R China.; Wang, ZW (corresponding author), Guangdong Univ Technol, Xiaoguwei St, Guangzhou 510006, Guangdong, Peoples R China.
EM depeiwang@qq.com; xuruifeng@hit.edu.cn; llcheng@gdut.edu.cn;
   wangzhuowei0710@163.com
OI Wang, Zhuowei/0000-0001-6479-5154; Xu, Ruifeng/0000-0002-4009-5679
FU Guangdong Provincial Key Laboratory of Cyber-Physical Systems
   [2020B1212060069]; National Natural Science Foundation of China
   [62176076]; Shenzhen Foundational Research Funding
   [JCYJ20220818102415032]; Major Key Project of PCL [PCL2023A09]
FX This work was supported in part by Guangdong Provincial Key Laboratory
   of Cyber-Physical Systems under Grant 2020B1212060069, National Natural
   Science Foundation of China 62176076, Shenzhen Foundational Research
   Funding JCYJ20220818102415032, and the Major Key Project of PCL
   PCL2023A09.
CR Akbari H, 2021, ADV NEUR IN
   [Anonymous], 2017, Storytelling Made Easy: Persuade and Transform Your Audiences, Buyers, and Clients-Simply, Quickly, and Profitably
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Cer D, 2018, Arxiv, DOI arXiv:1803.11175
   Chen BL, 2022, INT J APPL EARTH OBS, V108, DOI 10.1016/j.jag.2022.102762
   Cui HY, 2022, INFORM SCIENCES, V611, P18, DOI 10.1016/j.ins.2022.07.186
   Feng DD, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3580501
   Feng F, 2022, NEUROCOMPUTING, V500, P1052, DOI 10.1016/j.neucom.2022.05.098
   Gemmeke JF, 2017, INT CONF ACOUST SPEE, P776, DOI 10.1109/ICASSP.2017.7952261
   Gorinski Philip, 2015, P 2015 C N AM CHAPT, P1066
   Guo L, 2024, IEEE T NEUR NET LEAR, V35, P4002, DOI 10.1109/TNNLS.2022.3201533
   Jun TJ, 2021, EXPERT SYST APPL, V182, DOI 10.1016/j.eswa.2021.115211
   Kawintiranon K, 2021, 2021 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL-HLT 2021), P4725
   Kumar KS, 2022, CONCURR COMP-PRACT E, V34, DOI 10.1002/cpe.7374
   Lee M., 2021, P 3 WORKSHOP NARRATI, P56, DOI [10.18653/v1/2021.nuse-1.6, DOI 10.18653/V1/2021.NUSE-1.6]
   Li XY, 2023, ENTERP INF SYST-UK, V17, DOI 10.1080/17517575.2022.2037160
   Lin DW, 2022, INFORM PROCESS MANAG, V59, DOI 10.1016/j.ipm.2021.102788
   Liu H, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3583690
   Liu YY, 2022, COMPUT SPEECH LANG, V71, DOI 10.1016/j.csl.2021.101268
   Maddison C. J., 2017, The concrete distribution: A continuous relaxation of discrete random variables, P1
   MYERS CS, 1981, AT&T TECH J, V60, P1389, DOI 10.1002/j.1538-7305.1981.tb00272.x
   Papalampidi P, 2020, Arxiv, DOI arXiv:2012.07536
   Papalampidi P, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P1920
   Papalampidi P, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P1707
   Peters ME, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P43
   Quan RJ, 2021, IEEE T IMAGE PROCESS, V30, P3229, DOI 10.1109/TIP.2021.3058599
   Salur MU, 2022, NEURAL COMPUT APPL, V34, P18391, DOI 10.1007/s00521-022-07451-7
   Sun Y, 2019, Arxiv, DOI arXiv:1904.09223
   Tapaswi M, 2016, PROC CVPR IEEE, P4631, DOI 10.1109/CVPR.2016.501
   Wang HL, 2020, IEEE SIGNAL PROC LET, V27, P1560, DOI 10.1109/LSP.2020.3019702
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xing YR, 2021, 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1 (ACL-IJCNLP 2021), P525
   Yang Y, 2021, FRONT INFORM TECH EL, V22, P1551, DOI 10.1631/FITEE.2100463
   Yu F, 2021, Arxiv, DOI [arXiv:2006.16934, 10.1609/aaai.v35i4.16431]
   Zheng H, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6236
   Zhu L, 2023, J INTELL INF SYST, V60, P97, DOI 10.1007/s10844-022-00729-1
   Zhu WW, 2020, IEEE T CIRC SYST VID, V30, P3740, DOI 10.1109/TCSVT.2019.2940647
NR 37
TC 0
Z9 0
U1 4
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2024
VL 20
IS 5
AR 138
DI 10.1145/3638557
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MF3R9
UT WOS:001192177900018
DA 2024-08-05
ER

PT J
AU Yang, JF
   Wang, ZY
   Wang, GC
   Huang, BJ
   Yang, YH
   Tu, WP
AF Yang, Jifan
   Wang, Zhongyuan
   Wang, Guangcheng
   Huang, Baojin
   Yang, Yuhong
   Tu, Weiping
TI Auxiliary Information Guided Self-attention for Image Quality Assessment
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Transformer; image quality assessment; multi-task learning;
   full-reference; no-reference
ID SIMILARITY INDEX; EFFICIENT
AB Image quality assessment (IQA) is an important problem in computer vision with many applications. We propose a transformer-based multi-task learning framework for the IQA task. Two subtasks: constructing an auxiliary information error map and completing image quality prediction, are jointly optimized using a shared feature extractor. We use visual transformers (ViT) as a feature extractor for feature extraction and guide ViT to focus on image quality-related features by building auxiliary information error map subtask. In particular, we propose a fusion network that includes a channel focus module. Unlike the fusion methods commonly used in previous IQA methods, we use the fusion network, including the channel attention module, to fuse the auxiliary information error map features with the image features, which facilitates the model to mine the image quality features for more accurate image quality assessment. And by jointly optimizing the two subtasks, ViT focuses more on extracting image quality features and building a more precise mapping from feature representation to quality score. With slight adjustments to the model, our approach can be used in both no-reference (NR) and full-reference (FR) IQA environments. We evaluate the proposed method in multiple IQA databases, showing better performance than state-of-the-art FR and NR IQA methods.
C1 [Yang, Jifan; Wang, Zhongyuan; Huang, Baojin; Yang, Yuhong; Tu, Weiping] Wuhan Univ, Sch Comp, Wuhan, Hubei, Peoples R China.
   [Wang, Guangcheng] Nantong Univ, Sch Transportat & Civil Engn, Nantong, Jiangsu, Peoples R China.
C3 Wuhan University; Nantong University
RP Wang, ZY (corresponding author), Wuhan Univ, Sch Comp, Wuhan, Hubei, Peoples R China.
EM yangjifan_1996@163.com; wzy_hope@163.com; wangguangcheng0428@163.com;
   huangbaojin@whu.edu.cn; yangyuhong@whu.edu.cn; tuweiping@whu.edu.cn
OI Huang, Baojin/0000-0002-4882-5787; Yang, Jifan/0000-0003-1334-1827;
   Wang, Zhongyuan/0000-0002-9796-488X; Wang,
   Guangcheng/0000-0001-8277-797X; Yang, Yuhong/0000-0003-3001-7957
FU National Natural Science Foundation of China [62371350, 62071339,
   62171326]; Guangdong-Macau Joint Laboratory for Advanced and Intelligent
   Computing [2020B1212030003]; Guangdong High-Level Innovation Research
   Institute [2019B0909005]
FX This research was funded by National Natural Science Foundation of China
   (Grants No. 62371350, No. 62071339, No. 62171326), Guangdong-Macau Joint
   Laboratory for Advanced and Intelligent Computing (Grant No.
   2020B1212030003), and Guangdong High-Level Innovation Research Institute
   (Grant No. 2019B0909005).
CR Ahn S, 2021, IEEE COMPUT SOC CONF, P344, DOI 10.1109/CVPRW53098.2021.00044
   Aydin TO, 2008, PROC SPIE, V6806, DOI 10.1117/12.765095
   Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Chandler DM, 2007, IEEE T IMAGE PROCESS, V16, P2284, DOI 10.1109/TIP.2007.901820
   Chen CLZ, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3447393
   Chen JM, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P2080, DOI 10.1109/ICASSP39728.2021.9413489
   Chen WL, 2021, IEEE T MULTIMEDIA, V23, P1008, DOI 10.1109/TMM.2020.2991546
   Chubarau A, 2021, Arxiv, DOI arXiv:2110.01655
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Di Claudio ED, 2018, IEEE T IMAGE PROCESS, V27, P179, DOI 10.1109/TIP.2017.2757139
   Dosovitskiy A., 2021, ICLR
   Duanmu ZF, 2021, ANNU REV VIS SCI, V7, P437, DOI 10.1146/annurev-vision-100419-120301
   Fang YM, 2020, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR42600.2020.00373
   Gao F, 2017, NEUROCOMPUTING, V257, P104, DOI 10.1016/j.neucom.2017.01.054
   Ghadiyaram D, 2016, IEEE T IMAGE PROCESS, V25, P372, DOI 10.1109/TIP.2015.2500021
   Golestaneh SA, 2022, IEEE WINT CONF APPL, P3989, DOI 10.1109/WACV51458.2022.00404
   Gu Jinjin, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P633, DOI 10.1007/978-3-030-58621-8_37
   Guan JW, 2017, IEEE T MULTIMEDIA, V19, P2505, DOI 10.1109/TMM.2017.2703148
   Hosu V, 2020, IEEE T IMAGE PROCESS, V29, P4041, DOI 10.1109/TIP.2020.2967829
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang BJ, 2023, PATTERN RECOGN, V135, DOI 10.1016/j.patcog.2022.109142
   Huang BJ, 2023, IEEE T NEUR NET LEAR, V34, P10875, DOI 10.1109/TNNLS.2022.3171604
   Huang Baojin, 2022, ACM Trans. Multimedia Comput. Commun. Appl., V19, P3
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Ke JJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5128, DOI 10.1109/ICCV48922.2021.00510
   Kim J, 2017, PROC CVPR IEEE, P1969, DOI 10.1109/CVPR.2017.213
   LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9, DOI 10.1007/3-540-49430-8_2
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lin WS, 2011, J VIS COMMUN IMAGE R, V22, P297, DOI 10.1016/j.jvcir.2011.01.005
   Liu YT, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3414837
   Loshchilov I., 2017, ARXIV
   Mantiuk R, 2005, PROC SPIE, V5666, P204, DOI 10.1117/12.586757
   Mantiuk R, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964935
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Nafchi HZ, 2016, IEEE ACCESS, V4, P5579, DOI 10.1109/ACCESS.2016.2604042
   Paszke A, 2019, ADV NEUR IN, V32
   Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009
   Prashnani E, 2018, PROC CVPR IEEE, P1808, DOI 10.1109/CVPR.2018.00194
   Prechelt L, 1998, LECT NOTES COMPUT SC, V1524, P55
   Reisenhofer R, 2018, SIGNAL PROCESS-IMAGE, V61, P33, DOI 10.1016/j.image.2017.11.001
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Seo S, 2021, IEEE T CIRC SYST VID, V31, P2602, DOI 10.1109/TCSVT.2020.3030895
   Sheikh Hamid R, 2005, LIVE IMAGE QUALITY A, DOI DOI 10.1109/CVPR.2015.7298594
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Sheikh HR, 2005, IEEE T IMAGE PROCESS, V14, P2117, DOI 10.1109/TIP.2005.859389
   Wang GC, 2022, IEEE T CIRC SYST VID, V32, P1119, DOI 10.1109/TCSVT.2021.3074181
   Wang GC, 2020, IEEE T IMAGE PROCESS, V29, P1802, DOI 10.1109/TIP.2019.2945675
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu JJ, 2020, IEEE T IMAGE PROCESS, V29, P7414, DOI 10.1109/TIP.2020.3002478
   Wu JJ, 2014, IEEE SIGNAL PROC LET, V21, P437, DOI 10.1109/LSP.2014.2304714
   Yang H, 2015, IEEE T IMAGE PROCESS, V24, P4408, DOI 10.1109/TIP.2015.2465145
   Yang JF, 2023, PATTERN RECOGN, V140, DOI 10.1016/j.patcog.2023.109552
   Yang S, 2020, IEEE T MULTIMEDIA, V22, P2163, DOI 10.1109/TMM.2019.2947352
   Yang S, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1383, DOI 10.1145/3343031.3350990
   Yang Y, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3555355
   Ye P, 2012, PROC CVPR IEEE, P1098, DOI 10.1109/CVPR.2012.6247789
   Ying ZQ, 2020, PROC CVPR IEEE, P3572, DOI 10.1109/CVPR42600.2020.00363
   You JY, 2021, IEEE IMAGE PROC, P1389, DOI 10.1109/ICIP42928.2021.9506075
   Zhang L, 2014, IEEE MULTIMEDIA, V21, P67, DOI 10.1109/MMUL.2014.50
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang W, 2016, IEEE T NEUR NET LEAR, V27, P1266, DOI 10.1109/TNNLS.2015.2461603
   Zhu WH, 2019, IEEE T MULTIMEDIA, V21, P2334, DOI 10.1109/TMM.2019.2902484
NR 66
TC 0
Z9 0
U1 18
U2 18
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2024
VL 20
IS 4
AR 119
DI 10.1145/3635716
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6G9
UT WOS:001153382100029
DA 2024-08-05
ER

PT J
AU Zhang, HJ
   Li, P
   Liu, XB
   Yang, XF
   An, L
AF Zhang, Huijie
   Li, Pu
   Liu, Xiaobai
   Yang, Xianfeng
   An, Li
TI An Iterative Semi-supervised Approach with Pixel-wise Contrastive Loss
   for Road Extraction in Aerial Images
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Deep learning; semi-supervised learning; contrastive loss; iterative
   labeling; road extraction
AB Extracting roads in aerial images has numerous applications in artificial intelligence and multimedia computing, including traffic pattern analysis and parking space planning. Learning deep neural networks, though very successful, demand vast amounts of high-quality annotations, of which acquisition is time-consuming and expensive. In this work, we propose a semi-supervised approach for image-based road extraction in which only a small set of labeled images are available for training to address this challenge. We design a pixel-wise contrastive loss to self-supervise the network training to utilize the large corpus of unlabeled images. The key idea is to identify pairs of overlapping image regions (positive) or non-overlapping image regions (negative) and encourage the network to make similar outputs for positive pairs or dissimilar outputs for negative pairs. We also develop a negative sampling strategy to filter false-negative samples during the process. An iterative procedure is introduced to apply the network over raw images to generate pseudo-labels, filter and select high-quality labels with the proposed contrastive loss, and retrain the network with the enlarged training dataset. We repeat these iterative steps until convergence. We validate the effectiveness of the proposed methods by performing extensive experiments on the public SpaceNet3 and DeepGlobe Road datasets. Results show that our proposed method achieves state-of-the-art results on public image segmentation benchmarks and significantly outperforms other semi-supervised methods.
C1 [Zhang, Huijie; Li, Pu; Liu, Xiaobai; An, Li] San Diego State Univ, 5500 Campanile Dr, San Diego, CA 92182 USA.
   [Zhang, Huijie] Univ Calif Santa Barbara, Santa Barbara, CA 93106 USA.
   [Yang, Xianfeng] Univ Maryland, Civil & Environm Engn, College Pk, MD 20742 USA.
C3 California State University System; San Diego State University;
   University of California System; University of California Santa Barbara;
   University System of Maryland; University of Maryland College Park
RP Zhang, HJ (corresponding author), San Diego State Univ, 5500 Campanile Dr, San Diego, CA 92182 USA.; Zhang, HJ (corresponding author), Univ Calif Santa Barbara, Santa Barbara, CA 93106 USA.
EM hzhang9970@sdsu.edu; pli5270@sdsu.edu; xiaobai.liu@sdsu.edu;
   xtyang@umd.edu; lan@sdsu.edu
RI ZHANG, HUIJIE/CAF-7887-2022
FU National Science Foundation [2106965]
FX This work was supported by the National Science Foundation (grant no.
   2106965).
CR Abdollahi A, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12091444
   Alizadehsani R, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3462635
   Alonso I, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8199, DOI 10.1109/ICCV48922.2021.00811
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Bastani F, 2018, PROC CVPR IEEE, P4720, DOI 10.1109/CVPR.2018.00496
   Batra A, 2019, PROC CVPR IEEE, P10377, DOI 10.1109/CVPR.2019.01063
   Bengio J., 2009, Proceedings of the 26th Annual International Conference on Machine Learning, P41
   Cascante-Bonilla P, 2020, Arxiv, DOI arXiv:2001.06001
   Chaitanya K, 2023, MED IMAGE ANAL, V87, DOI 10.1016/j.media.2023.102792
   Chen T, 2020, PR MACH LEARN RES, V119
   Chen XK, 2021, PROC CVPR IEEE, P2613, DOI 10.1109/CVPR46437.2021.00264
   Chen XL, 2020, Arxiv, DOI arXiv:2003.04297
   Cheng GL, 2017, IEEE T GEOSCI REMOTE, V55, P3322, DOI 10.1109/TGRS.2017.2669341
   Dai JF, 2015, IEEE I CONF COMP VIS, P1635, DOI 10.1109/ICCV.2015.191
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Demir I, 2018, IEEE COMPUT SOC CONF, P172, DOI 10.1109/CVPRW.2018.00031
   DeVries T, 2017, Arxiv, DOI arXiv:1708.04552
   Feng Z., 2020, arXiv preprint arXiv:2004.08514 1(2), V1, P5
   French Geoff, 2020, arXiv
   French Geoffrey, 2019, arXiv preprint arXiv:1906.01916 2, 4, P5
   Homayounfar N, 2019, IEEE I CONF COMP VIS, P2911, DOI 10.1109/ICCV.2019.00300
   Huang ZL, 2022, IEEE T PATTERN ANAL, V44, P550, DOI 10.1109/TPAMI.2021.3062772
   Hung WC, 2018, Arxiv, DOI arXiv:1802.07934
   Ibrahim M.S., 2020, IEEE C COMPUT VIS PA, P12715
   Iscen A, 2019, PROC CVPR IEEE, P5065, DOI 10.1109/CVPR.2019.00521
   Jean N, 2019, AAAI CONF ARTIF INTE, P3967
   Jensen JR, 1999, PHOTOGRAMM ENG REM S, V65, P611
   Jeong J, 2019, ADV NEUR IN, V32
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Kang J, 2021, IEEE T GEOSCI REMOTE, V59, P2598, DOI 10.1109/TGRS.2020.3007029
   Kim J., 2020, arXiv
   Lai X, 2021, PROC CVPR IEEE, P1205, DOI 10.1109/CVPR46437.2021.00126
   Li LRH, 2020, IEEE T IMAGE PROCESS, V29, P2766, DOI 10.1109/TIP.2019.2952690
   Li P, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4491, DOI 10.1145/3474085.3475602
   Li XZ, 2019, ADV NEUR IN, V32
   Liang-Chieh Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P695, DOI 10.1007/978-3-030-58545-7_40
   Liu RY, 2020, IEEE GEOSCI REMOTE S, V17, P894, DOI 10.1109/LGRS.2019.2931928
   Liu Xiaoming, 2022, ACM Trans. Multimedia Comput., Commun., Appl., V18, P1
   Lu XY, 2019, IEEE T GEOSCI REMOTE, V57, P9362, DOI 10.1109/TGRS.2019.2926397
   Lv Z, 2017, IEEE GEOSCI REMOTE S, V14, P1238, DOI 10.1109/LGRS.2017.2704120
   Maboudi M, 2018, ISPRS J PHOTOGRAMM, V138, P151, DOI 10.1016/j.isprsjprs.2017.11.014
   Máttyus G, 2017, IEEE I CONF COMP VIS, P3458, DOI 10.1109/ICCV.2017.372
   Miao ZL, 2015, IEEE J-STARS, V8, P4853, DOI 10.1109/JSTARS.2015.2443552
   Mittal S, 2021, IEEE T PATTERN ANAL, V43, P1369, DOI 10.1109/TPAMI.2019.2960224
   Mosinska A, 2018, PROC CVPR IEEE, P3136, DOI 10.1109/CVPR.2018.00331
   Ouali Y., 2020, P IEEE CVF C COMP VI, P12674, DOI [10.1109/cvpr42600.2020.01269, DOI 10.48550/ARXIV.2003.09005]
   Pan H, 2019, IEEE GEOSCI REMOTE S, V16, P201, DOI 10.1109/LGRS.2018.2870488
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   SCUDDER HJ, 1965, IEEE T INFORM THEORY, V11, P363, DOI 10.1109/tit.1965.1053799
   Singh Suriya, 2018, BMVC, V1
   Sohn K, 2020, Arxiv, DOI [arXiv:2005.04757, 10.48550/arXiv.2005.04757]
   Songtao He, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P51, DOI 10.1007/978-3-030-58586-0_4
   Souly N, 2017, IEEE I CONF COMP VIS, P5689, DOI 10.1109/ICCV.2017.606
   Sun ZY, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3478457
   Van Etten A, 2019, Arxiv, DOI arXiv:1807.01232
   Wang P, 2021, PROC CVPR IEEE, P943, DOI 10.1109/CVPR46437.2021.00100
   Wang WG, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7283, DOI 10.1109/ICCV48922.2021.00721
   Wei YC, 2018, PROC CVPR IEEE, P7268, DOI 10.1109/CVPR.2018.00759
   Wu HY, 2021, PROC CVPR IEEE, P10546, DOI 10.1109/CVPR46437.2021.01041
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Xiao JS, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3419842
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1007/s11263-017-1004-z, 10.1109/ICCV.2015.164]
   Xu MD, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3040, DOI 10.1109/ICCV48922.2021.00305
   Xu XX, 2016, IEEE T PATTERN ANAL, V38, P1113, DOI 10.1109/TPAMI.2015.2476813
   Yang X, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3089249
   Yong-Qiang Tan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8907, DOI 10.1109/CVPR42600.2020.00893
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Yalniz IZ, 2019, Arxiv, DOI [arXiv:1905.00546, DOI 10.48550/ARXIV.1905.00546]
   Zhang BW, 2021, 35 C NEURAL INFORM P, V34
   Zhang Y, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11091017
   Zhang ZP, 2019, IEEE ACM T COMPUT BI, V16, P407, DOI 10.1109/TCBB.2017.2704587
   Zhao XY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10603, DOI 10.1109/ICCV48922.2021.01045
   Zhong YY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7253, DOI 10.1109/ICCV48922.2021.00718
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhou YN, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7016, DOI 10.1109/ICCV48922.2021.00695
   Zhu X.G., 2009, Synthesis Lectures on Artificial Intelligence and Machine Learning, DOI 10.2200/S00196ED1V01Y200906AIM006
NR 76
TC 0
Z9 0
U1 8
U2 8
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAR
PY 2024
VL 20
IS 3
AR 80
DI 10.1145/3606374
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6F8
UT WOS:001153381000020
DA 2024-08-05
ER

PT J
AU Dai, QF
   Wong, YK
   Sun, GF
   Wang, YW
   Zhou, Z
   Kankanhalli, MS
   Li, XD
   Geng, WD
AF Dai, Qingfeng
   Wong, Yongkang
   Sun, Guofei
   Wang, Yanwei
   Zhou, Zhou
   Kankanhalli, Mohan S.
   Li, Xiangdong
   Geng, Weidong
TI Unsupervised Domain Adaptation by Causal Learning for Biometric
   Signal-based HCI
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Deep learning; domain adaptation; causal learning; human-computer
   interface
ID RECOGNITION; EXTRACTION; PCA; EMG
AB Biometric signal based human-computer interface (HCI) has attracted increasing attention due to its wide application in healthcare, entertainment, neurocomputing, and so on. In recent years, deep learning-based approaches have made great progress on biometric signal processing. However, the state-of-the-art (SOTA) approaches still suffer from model degradation across subjects or sessions. In this work, we propose a novel unsupervised domain adaptation approach for biometric signal-based HCI via causal representation learning. Specifically, three kinds of interventions on biometric signals (i.e., subjects, sessions, and trials) can be selected to generalize deep models across the selected intervention. In the proposed approach, a generative model is trained for producing intervened features that are subsequently used for learning transferable and causal relations with three modes. Experiments on the EEG-based emotion recognition task and sEMG-based gesture recognition task are conducted to confirm the superiority of our approach. An improvement of +0.21% on the task of inter-subject EEG-based emotion recognition is achieved using our approach. Besides, on the task of inter-session sEMG-based gesture recognition, our approach achieves improvements of +1.47%, +3.36%, +1.71%, and +1.01% on sEMG datasets including CSL-HDEMG, CapgMyo DB-b, 3DC, and Ninapro DB6, respectively. The proposed approach also works on the task of inter-trial sEMG-based gesture recognition and an average improvement of +0.66% on Ninapro databases is achieved. These experimental results show the superiority of the proposed approach compared with the SOTA unsupervised domain adaptation methods on HCIs based on biometric signal.
C1 [Dai, Qingfeng; Sun, Guofei; Wang, Yanwei; Zhou, Zhou] Zhejiang Univ, Coll Comp Sci & Technol, State Key Lab CAD& CG, Zheda Rd 38, Hangzhou 310027, Zhejiang, Peoples R China.
   [Wong, Yongkang] Natl Univ Singapore, Sch Comp, 3 Res Link,I4-0 Bldg, Singapore 117602, Singapore.
   [Kankanhalli, Mohan S.] Natl Univ Singapore, Sch Comp, 11 Comp Dr,AS6,117416 Bldg, Singapore, Singapore.
   [Li, Xiangdong] Zhejiang Univ, Coll Comp Sci & Technol, Zheda Rd 38, Hangzhou 310027, Zhejiang, Peoples R China.
   [Geng, Weidong] Zhejiang Lab, Kechuang Ave, Hangzhou 311121, Zhejiang, Peoples R China.
C3 Zhejiang University; National University of Singapore; National
   University of Singapore; Zhejiang University; Zhejiang Laboratory
RP Li, XD (corresponding author), Zhejiang Univ, Coll Comp Sci & Technol, Zheda Rd 38, Hangzhou 310027, Zhejiang, Peoples R China.; Geng, WD (corresponding author), Zhejiang Lab, Kechuang Ave, Hangzhou 311121, Zhejiang, Peoples R China.
EM qfdai@zju.edu.cn; wong@nus.edu.sg; guofeisun@zju.edu.cn;
   wang_yw@zju.edu.cn; zwithz@zju.edu.cn; mohan@comp.nus.edu.sg;
   axli@zju.edu.cn; gengwd@zju.edu.cn
RI Kankanhalli, Mohan/Q-9284-2019
OI Kankanhalli, Mohan/0000-0002-4846-2015; Dai,
   Qingfeng/0000-0003-2850-0683; Wong, Yongkang/0000-0002-1239-4428
FU National Natural Science Foundation of China [61972346, 92148205];
   Science and Technology Planning Project of Zhejiang, China [2022C03103]
FX This work is supported by the National Natural Science Foundation of
   China (No. 61972346, No. 92148205) and Science and Technology Planning
   Project of Zhejiang, China (No. 2022C03103).
CR Rusu AA, 2016, Arxiv, DOI [arXiv:1606.04671, DOI 10.48550/ARXIV.1606.04671]
   Ajakan H, 2015, Arxiv, DOI arXiv:1412.4446
   Amma C, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P929, DOI 10.1145/2702123.2702501
   Arjovsky M, 2020, Arxiv, DOI [arXiv:1907.02893, DOI 10.48550/ARXIV.1907.02893]
   Atzori M, 2014, SCI DATA, V1, DOI 10.1038/sdata.2014.53
   Belakhdar I, 2016, 2016 2ND INTERNATIONAL CONFERENCE ON ADVANCED TECHNOLOGIES FOR SIGNAL AND IMAGE PROCESSING (ATSIP), P443, DOI 10.1109/ATSIP.2016.7523132
   Chalupka K, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P181
   Chen H, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.778488
   Chowdhury A., 2013, ICORD 13, P411, DOI [DOI 10.1007/978-81-322-1050-4_33, 10.1007/978-81-322-1050-4_33]
   Côté-Allard U, 2020, IEEE ACCESS, V8, P177941, DOI 10.1109/ACCESS.2020.3027497
   Côté-Allard U, 2019, IEEE T NEUR SYS REH, V27, P760, DOI 10.1109/TNSRE.2019.2896269
   Côté-Allard U, 2017, IEEE SYS MAN CYBERN, P1663, DOI 10.1109/SMC.2017.8122854
   Johansson FD, 2018, Arxiv, DOI arXiv:1802.08598
   Doswald A., 2014, XIII Mediterranean Conference on Medical and Biological Engineering and Computing 2013, P758
   Du Y, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17030458
   Duan F, 2016, IEEE T IND ELECTRON, V63, P1923, DOI 10.1109/TIE.2015.2497212
   Farina D, 2014, IEEE T NEUR SYS REH, V22, P797, DOI 10.1109/TNSRE.2014.2305111
   Gao XZ, 2020, ARTIF INTELL MED, V102, DOI 10.1016/j.artmed.2019.101711
   Geng WD, 2016, SCI REP-UK, V6, DOI 10.1038/srep36571
   Gu XT, 2021, IEEE ACM T COMPUT BI, V18, P1645, DOI 10.1109/TCBB.2021.3052811
   He H, 2020, IEEE T NEUR SYS REH, V28, P1091, DOI 10.1109/TNSRE.2020.2980299
   Hu XH, 2021, IEEE SENS J, V21, P20596, DOI 10.1109/JSEN.2021.3098120
   Hu Y, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0206049
   HUDGINS B, 1993, IEEE T BIO-MED ENG, V40, P82, DOI 10.1109/10.204774
   Jayaram V, 2016, IEEE COMPUT INTELL M, V11, P20, DOI 10.1109/MCI.2015.2501545
   Khushaba RN, 2017, IEEE T NEUR SYS REH, V25, P1821, DOI 10.1109/TNSRE.2017.2687520
   Kiatpanichagij K, 2009, BIOMED SIGNAL PROCES, V4, P127, DOI 10.1016/j.bspc.2009.02.004
   Krasoulis A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00891
   Krasoulis A, 2017, J NEUROENG REHABIL, V14, DOI 10.1186/s12984-017-0284-4
   Krishna G, 2020, Arxiv, DOI arXiv:2001.00501
   Kumar SK, 2017, Arxiv, DOI [arXiv:1704.08863, DOI 10.48550/ARXIV.1704.08863]
   Lawhern VJ, 2018, J NEURAL ENG, V15, DOI 10.1088/1741-2552/aace8c
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Li H, 2018, LECT NOTES COMPUT SC, V11305, P403, DOI 10.1007/978-3-030-04221-9_36
   Li JP, 2020, IEEE T COGN DEV SYST, V12, P344, DOI 10.1109/TCDS.2019.2949306
   Li Yanghao, 2017, 5 INT C LEARN REPR I
   Lopez-Paz D, 2017, PROC CVPR IEEE, P58, DOI 10.1109/CVPR.2017.14
   Luo Y, 2018, LECT NOTES COMPUT SC, V11305, P275, DOI 10.1007/978-3-030-04221-9_25
   Magliacane Sara, 2018, ADV NEURAL INFORM PR, P10879
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   McMahan T, 2015, ENTERTAIN COMPUT, V7, P1, DOI 10.1016/j.entcom.2015.03.001
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Palermo F, 2017, INT C REHAB ROBOT, P1154, DOI 10.1109/ICORR.2017.8009405
   Peng Y, 2020, APPL SOFT COMPUT, V97, DOI 10.1016/j.asoc.2020.106756
   Peters J, 2017, ADAPT COMPUT MACH LE
   Peters J, 2016, J ROY STAT SOC B, V78, P947, DOI 10.1111/rssb.12167
   Phinyomark A., 2018, Big Data and Cognitive Computing, V2, P21, DOI [DOI 10.3390/BDCC2030021, 10.3390/bdcc2030021]
   Phinyomark A, 2012, EXPERT SYST APPL, V39, P7420, DOI 10.1016/j.eswa.2012.01.102
   Pizzolato S, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0186132
   Qin ZY, 2019, IEEE SIGNAL PROC LET, V26, P637, DOI 10.1109/LSP.2019.2903334
   Rahimian E, 2021, IEEE T NEUR SYS REH, V29, P1004, DOI 10.1109/TNSRE.2021.3077413
   Rahimian E, 2020, INT CONF ACOUST SPEE, P1304, DOI [10.1109/ICASSP40776.2020.9054586, 10.1109/icassp40776.2020.9054586]
   Rayatdoost S, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P3955, DOI 10.1109/ICASSP39728.2021.9414496
   Riyad Mouad, 2020, Image and Signal Processing. 9th International Conference, ICISP 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12119), P103, DOI 10.1007/978-3-030-51935-3_11
   Salama ES, 2018, INT J ADV COMPUT SC, V9, P329
   Scheme E, 2011, J REHABIL RES DEV, V48, P643, DOI 10.1682/JRRD.2010.09.0177
   Schmidt C, 2018, INT J NEURAL SYST, V28, DOI 10.1142/S0129065717500514
   Schneider T, 2020, 2020 IEEE INTERNATIONAL CONFERENCE ON SMART COMPUTING (SMARTCOMP), P284, DOI 10.1109/SMARTCOMP50058.2020.00065
   Shalit U, 2017, PR MACH LEARN RES, V70
   She HT, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19204457
   Shoeibi A, 2021, EXPERT SYST APPL, V163, DOI 10.1016/j.eswa.2020.113788
   Siswoyo A, 2017, EMITTER, V5, P170, DOI 10.24003/emitter.v5i1.165
   Subasi A, 2010, EXPERT SYST APPL, V37, P8659, DOI 10.1016/j.eswa.2010.06.065
   Suhaimi NS, 2020, COMPUT INTEL NEUROSC, V2020, DOI 10.1155/2020/8875426
   Sun JY, 2021, 2021 IEEE 3RD GLOBAL CONFERENCE ON LIFE SCIENCES AND TECHNOLOGIES (IEEE LIFETECH 2021), P92, DOI [10.1109/LifeTech52111.2021.9391844, 10.1109/LIFETECH52111.2021.9391844]
   Tang XL, 2020, ENTROPY-SWITZ, V22, DOI 10.3390/e22010096
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   Vaswani A, 2017, ADV NEUR IN, V30
   Volpi R, 2018, PROC CVPR IEEE, P5495, DOI 10.1109/CVPR.2018.00576
   Wang C, 2020, BIOMED SIGNAL PROCES, V59, DOI 10.1016/j.bspc.2019.101774
   Wang P, 2018, IEEE T NEUR SYS REH, V26, P2086, DOI 10.1109/TNSRE.2018.2876129
   Wang Wenjuan, 2021, P INT C INT AUT SOFT, P413
   Wei WT, 2019, IEEE T BIO-MED ENG, V66, P2964, DOI 10.1109/TBME.2019.2899222
   Yao LY, 2021, ACM T KNOWL DISCOV D, V15, DOI 10.1145/3444944
   Yosinski J, 2014, ADV NEUR IN, V27
   Zhang Xiheng, 2021, P IEEE CVF INT C COM, P11270
   Zhao LM, 2021, AAAI CONF ARTIF INTE, V35, P863
   Zhao XQ, 2019, IEEE T NEUR SYS REH, V27, P2164, DOI 10.1109/TNSRE.2019.2938295
   Zheng WL, 2019, IEEE T CYBERNETICS, V49, P1110, DOI 10.1109/TCYB.2018.2797176
   Zheng WL, 2015, IEEE T AUTON MENT DE, V7, P162, DOI 10.1109/TAMD.2015.2431497
NR 80
TC 1
Z9 1
U1 12
U2 29
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2024
VL 20
IS 2
SI SI
AR 49
DI 10.1145/3583885
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W6GR4
UT WOS:001092595800019
DA 2024-08-05
ER

PT J
AU Chen, SM
   Xu, R
   Xu, J
   Xin, SQ
   Tu, CH
   Yang, CL
   Lu, L
AF Chen, Shuangmin
   Xu, Rui
   Xu, Jian
   Xin, Shiqing
   Tu, Changhe
   Yang, Chenglei
   Lu, Lin
TI QuickCSGModeling: Quick CSG Operations Based on Fusing Signed Distance
   Fields for VR Modeling
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Virtual reality; SDF fusion; boolean operation; modeling
AB The latest advancements in Virtual Reality (VR) enable the creation of 3Dmodels within a holographic immersive simulation environment. In this article, we create QuickCSGModeling, a user-friendly mid-air interactive modeling system. We first prepare a dataset consisting of diverse components and precompute the discrete signed distance function (SDF) for each component. During the modeling phase, users can freely design complicated shapes with a pair of VR controllers. Based on the discrete SDF representation, any CSG-like operation (union, intersection, and subtraction) can be performed voxel-wisely. Also, we maintain a single dynamic SDF for the whole scene, whose zero-level set surface exactly encodes the most recent constructed shape. Both SDF fusion and surface extraction are implemented via GPU for a smooth user experience. A total of 34 volunteers were asked to create their favorite models using QuickCSGModeling. With a simple training, most of them can create a fascinating shape or even a descriptive scene quickly. We also discuss how to extend our system to create articulated models with hinges, where an adaptive cube subdivision has to be enforced to improve the reconstruction accuracy around the hinge part, followed by a Dual Contouring-based surface extraction.(1)
C1 [Chen, Shuangmin] Qingdao Univ Sci & Technol, Qingdao 266061, Shandong, Peoples R China.
   [Xu, Rui; Xin, Shiqing; Tu, Changhe; Lu, Lin] Shandong Univ, Qingdao 266237, Shandong, Peoples R China.
   [Xu, Jian] Chinese Acad Sci, Ningbo Inst Mat Technol & Engn, Ningbo 315201, Zhejiang, Peoples R China.
   [Yang, Chenglei] Shandong Univ, Jinan 250199, Shandong, Peoples R China.
C3 Qingdao University of Science & Technology; Shandong University; Chinese
   Academy of Sciences; Ningbo Institute of Materials Technology and
   Engineering, CAS; Shandong University
RP Chen, SM (corresponding author), Qingdao Univ Sci & Technol, Qingdao 266061, Shandong, Peoples R China.
EM csmqq@163.com; xrvitd@163.com; xujian@nimte.ac.cn;
   xinshiqing@sdu.edu.cn; chtu@sdu.edu.cn; chl_yang@sdu.edu.cn;
   llu@sdu.edu.cn
RI ; Xu, Rui/GWM-7132-2022
OI Lu, Lin/0000-0001-5881-892X; Xin, Shiqing/0000-0001-8452-8723; Xu,
   Rui/0000-0001-8273-1808
FU National Key R&D Program of China [2022YFB3303200]; National Natural
   Science Foundation of China [62002190, 62272277]; Natural Science
   Foundation of Shandong Province [ZR2020MF036]
FX The authors would like to thank the anonymous reviewers for their
   valuable comments and suggestions. This work is supported by National
   Key R&D Program of China (2022YFB3303200), National Natural Science
   Foundation of China (62002190, 62272277), and Natural Science Foundation
   of Shandong Province (ZR2020MF036).
CR Arora R, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3459090
   Arora R, 2020, CHI'20: EXTENDED ABSTRACTS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3334480.3375028
   Bærentzen A, 2019, 25TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2019), DOI 10.1145/3359996.3364257
   Beever L, 2020, IEEE CONF COMPU INTE, P136, DOI 10.1109/CoG47356.2020.9231769
   Cherry E, 2014, ACM T COMPUT-HUM INT, V21, DOI 10.1145/2617588
   de Araújo BR, 2015, ACM COMPUT SURV, V47, DOI 10.1145/2732197
   De Leon JDO, 2016, PROCEEDINGS OF THE 2016 IEEE REGION 10 CONFERENCE (TENCON), P3708, DOI 10.1109/TENCON.2016.7848751
   Drey T, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376628
   Du XY, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459870
   Duan ZX, 2020, IEEE ACCESS, V8, P48376, DOI 10.1109/ACCESS.2020.2979571
   Evans A., 2015, Advances in Real-Time Rendering in Games
   Ferreira J, 2021, 2021 IEEE CONFERENCE ON VIRTUAL REALITY AND 3D USER INTERFACES ABSTRACTS AND WORKSHOPS (VRW 2021), P30, DOI 10.1109/VRW52623.2021.00012
   Fraser Duncan, 2023, Baroque Software
   Gottschalk S., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P171, DOI 10.1145/237170.237244
   Guo HX, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3550454.3555502
   Igarashi T, 1999, COMP GRAPH, P409, DOI 10.1145/311535.311602
   Jackson B, 2016, IEEE T VIS COMPUT GR, V22, P1442, DOI 10.1109/TVCG.2016.2518099
   Jerald J, 2013, 2013 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P197
   Jerald Jason, 2016, P ACM SIGGRAPH 2016, DOI [10.1145/2897826.2927320, DOI 10.1145/2897826.2927320]
   Kobbelt LP, 2001, COMP GRAPH, P57, DOI 10.1145/383259.383265
   Larsen Eric., 1999, Fast proximity queries with swept sphere volumes
   Lewiner T., 2003, Journal of Graphics Tools, V8, P1, DOI 10.1080/10867651.2003.10487582
   Liu YJ, 2011, IEEE COMPUT GRAPH, V31, P49, DOI 10.1109/MCG.2009.147
   Lopes Pedro, 2011, P 8 EUR S SKETCH BAS, P15
   Lorensen H. E., 1987, Proc. SIGGRAPH, V21, P163, DOI 10.1145/37401.37422
   McInerney T, 1999, IEEE T MED IMAGING, V18, P840, DOI 10.1109/42.811261
   Mendes D, 2017, IEEE SYMP 3D USER, P154, DOI 10.1109/3DUI.2017.7893332
   Newman TS, 2006, COMPUT GRAPH-UK, V30, P854, DOI 10.1016/j.cag.2006.07.021
   Osher S., 2002, Applied Mathematical Sciences
   Osher S., 2003, Level Set Methods and Dynamic Implicit Surfaces
   PASKO A, 1995, VISUAL COMPUT, V11, P429, DOI 10.1007/BF02464333
   Peng MQ, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201297
   Schaefer S, 2007, IEEE T VIS COMPUT GR, V13, P610, DOI 10.1109/TVCG.2007.1012
   Schmidt R., 2007, ACM SIGGRAPH 2007 courses, P43
   The CGAL Project, 2023, CGAL User and Reference Manual
   Wang J, 2013, 2013 IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI), P195
   Wang YH, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366184
   Wyvill B, 1999, COMPUT GRAPH FORUM, V18, P149, DOI 10.1111/1467-8659.00365
   Yu E, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445158
NR 39
TC 0
Z9 0
U1 2
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2024
VL 20
IS 7
SI SI
AR 189
DI 10.1145/3599729
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL0Q6
UT WOS:001234494100004
DA 2024-08-05
ER

PT J
AU Liu, SZ
   Lin, WY
   Chen, YH
   Zhang, YF
   Dai, WR
   See, J
   Xiong, HK
AF Liu, Shizhan
   Lin, Weiyao
   Chen, Yihang
   Zhang, Yufeng
   Dai, Wenrui
   See, John
   Xiong, Hong-Kai
TI A Unified Framework for Jointly Compressing Visual and Semantic Data
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Joint visual and semantic data compression; visual semantic data;
   multimedia processing; unified compression framework
ID BIT ALLOCATION
AB The rapid advancement of multimedia and imaging technologies has resulted in increasingly diverse visual and semantic data. A large range of applications such as remote-assisted driving requires the amalgamated storage and transmission of various visual and semantic data. However, existing works suffer from the limitation of insufficiently exploiting the redundancy between different types of data. In this article, we propose a unified framework to jointly compress a diverse spectrum of visual and semantic data, including images, point clouds, segmentation maps, object attributes, and relations. We develop a unifying process that embeds the representations of these data into a joint embedding graph according to their categories, which enables flexible handling of joint compression tasks for various visual and semantic data. To fully leverage the redundancy between different data types, we further introduce an embedding-based adaptive joint encoding process and a Semantic Adaptation Module to efficiently encode diverse data based on the learned embeddings in the joint embedding graph. Experiments on the Cityscapes, MSCOCO, and KITTI datasets demonstrate the superiority of our framework, highlighting promising steps toward scalable multimedia processing.
C1 [Liu, Shizhan; Lin, Weiyao; Chen, Yihang; Zhang, Yufeng; Xiong, Hong-Kai] Shanghai Jiao Tong Univ, Dept Elect Engn, 800 Dongchuan Rd, Shanghai, Peoples R China.
   [Dai, Wenrui] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, 800 Dongchuan Rd, Shanghai, Peoples R China.
   [See, John] Heriot Watt Univ Malaysia, Sch Math & Comp Sci, 1 Jalan Venna P5-2,Precinct 5, Putrajaya 62200, Malaysia.
C3 Shanghai Jiao Tong University; Shanghai Jiao Tong University; Heriot
   Watt University
RP Lin, WY (corresponding author), Shanghai Jiao Tong Univ, Dept Elect Engn, 800 Dongchuan Rd, Shanghai, Peoples R China.
EM shanluzuode@sjtu.edu.cn; wylin@sjtu.edu.cn; yhchen.ee@sjtu.edu.cn;
   worldlife@sjtu.edu.cn; daiwenrui@sjtu.edu.cn; j.see@hw.ac.uk;
   xionghongkai@sjtu.edu.cn
RI See, John/C-8633-2013
OI See, John/0000-0003-3005-4109; Xiong, Hongkai/0000-0003-4552-0029
FU National Natural Science Foundation of China [62325109, U21B2013,
   61971277]; Ministry of Higher Education (MOHE) Malaysia FRGS Scheme
   [FRGS/1/2022/ICT02/HWUM/02/1]
FX The article is supported in part by the National Natural Science
   Foundation of China (Grants No. 62325109, No. U21B2013, and No.
   61971277) and Ministry of Higher Education (MOHE) Malaysia FRGS Scheme
   (Grant No. FRGS/1/2022/ICT02/HWUM/02/1).
CR Akbari M, 2019, INT CONF ACOUST SPEE, P2042, DOI [10.1109/ICASSP.2019.8683541, 10.1109/icassp.2019.8683541]
   Alvar SR, 2020, INT CONF ACOUST SPEE, P4342, DOI [10.1109/ICASSP40776.2020.9054770, 10.1109/icassp40776.2020.9054770]
   Ball‚ J, 2017, Arxiv, DOI [arXiv:1611.01704, DOI 10.48550/ARXIV.1611.01704]
   Ball‚ J, 2018, Arxiv, DOI arXiv:1802.01436
   Bellard F., 2017, BPG Image Format
   Bjontegaard G., 2001, Calculation of average PSNR differences between RD-Curves
   Chang J., 2021, P IEEE INT C MULT EX, P1
   Chang JH, 2022, IEEE T IMAGE PROCESS, V31, P2809, DOI 10.1109/TIP.2022.3159477
   Chang JH, 2019, IEEE IMAGE PROC, P694, DOI [10.1109/ICIP.2019.8803805, 10.1109/icip.2019.8803805]
   Chang Jianhui., 2023, Int. J. Comput. Vision, V131, P1
   Chen Z, 2020, IEEE IMAGE PROC, P3094, DOI 10.1109/ICIP40778.2020.9190843
   Chen Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2414, DOI 10.1145/3343031.3350849
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Fu CY, 2022, AAAI CONF ARTIF INTE, P625
   Gailly J.-l., 2004, ZLIB COMPRESSION LIB
   Gao W, 2021, IEEE T CIRC SYST VID, V31, P4147, DOI 10.1109/TCSVT.2021.3104305
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Google, 2017, Draco 3D Graphics Compression
   Guarda AFR, 2019, PICT COD SYMP
   He DL, 2022, PROC CVPR IEEE, P5708, DOI 10.1109/CVPR52688.2022.00563
   Hoang TM, 2020, IEEE COMPUT SOC CONF, P619, DOI 10.1109/CVPRW50498.2020.00088
   Hu YZ, 2020, IEEE I C VI COM I PR, P475, DOI 10.1109/vcip49819.2020.9301807
   Li ZY, 2021, IEEE ACCESS, V9, P142782, DOI 10.1109/ACCESS.2021.3120720
   Lin M, 2014, Arxiv, DOI arXiv:1312.4400
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin WY, 2020, IEEE MULTIMEDIA, V27, P12, DOI 10.1109/MMUL.2020.2990863
   Liu HJ, 2019, Arxiv, DOI [arXiv:1904.09757, 10.48550/arXiv.1904.09757, DOI 10.48550/ARXIV.1904.09757]
   Liu K, 2021, INT J COMPUT VISION, V129, P2605, DOI 10.1007/s11263-021-01491-7
   Mentzer F, 2019, PROC CVPR IEEE, P10621, DOI 10.1109/CVPR.2019.01088
   Minnen D, 2018, ADV NEUR IN, V31
   MPEGGroup, 2021, Mpeg g-pcc tmc13
   Nan Du, 2020, CIPAE 2020: Proceedings of the 2020 International Conference on Computers, Information Processing and Advanced Education, P115, DOI 10.1145/3419635.3419668
   Ohm J.R., 2018, PICT COD S
   Parekar P. M., 2014, Int. J. Comput. Sci. Info. Technol., V5, P1
   Parker J, 2023, ANALYSING THE HISTORY OF BRITISH SOCIAL WELFARE, P82, DOI 10.1145/3551389
   Peng YX, 2024, ACM T MULTIM COMPUT, V20, DOI 10.1145/3633459
   Singh S, 2020, IEEE IMAGE PROC, P3349, DOI 10.1109/ICIP40778.2020.9190860
   Sneyers J, 2016, IEEE IMAGE PROC, P66, DOI 10.1109/ICIP.2016.7532320
   Song JK, 2020, INT J COMPUT VISION, V128, P2243, DOI 10.1007/s11263-020-01305-2
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Suzuki S, 2020, IEEE IMAGE PROC, P3099, DOI 10.1109/ICIP40778.2020.9190933
   Tang KH, 2020, PROC CVPR IEEE, P3713, DOI 10.1109/CVPR42600.2020.00377
   WALLACE GK, 1991, COMMUN ACM, V34, P30, DOI 10.1145/103085.103089
   Wang JQ, 2019, Arxiv, DOI [arXiv:1909.12037, 10.48550/ARXIV.1909.12037]
   Wu Y., 2019, Detectron2
   Yang Shuyu., 2023, P ACM MULT C
   Yuan Hang., 2023, ACM Trans. Multimedia Comput. Commun. Appl., V20, P4
   Zhang J, 2021, IEEE INTERNET THINGS, V8, P7789, DOI 10.1109/JIOT.2020.3039359
   Zhang LM, 2023, IEEE I CONF COMP VIS, P3813, DOI 10.1109/ICCV51070.2023.00355
   Zhang Q, 2021, INT J COMPUT VISION, V129, P2889, DOI 10.1007/s11263-021-01505-4
   Zhang T, 2020, IEEE INTERNET THINGS, V7, P11347, DOI 10.1109/JIOT.2020.3028766
   Zhang YF, 2023, Arxiv, DOI arXiv:2304.13359
   Zhang Z., 2021, P IEEE INT C MULT EX, P1
   Zhengxue Cheng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7936, DOI 10.1109/CVPR42600.2020.00796
NR 55
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2024
VL 20
IS 7
SI SI
AR 221
DI 10.1145/3654800
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL0Q6
UT WOS:001234494100036
OA Bronze
DA 2024-08-05
ER

PT J
AU Liu, WX
   Cai, JX
   Li, Q
   Liao, CY
   Cao, JJ
   He, SF
   Yu, YL
AF Liu, Wenxi
   Cai, Jiaxin
   Li, Qi
   Liao, Chenyang
   Cao, Jingjing
   He, Shengfeng
   Yu, Yuanlong
TI Learning Nighttime Semantic Segmentation the Hard Way
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Nighttime semantic segmentation; dual-branch; hard class; pretext task;
   fusion refinement scheme
AB Nighttime semantic segmentation is an important but challenging research problem for autonomous driving. The major challenges lie in the small objects or regions from the under-/over-exposed areas or suffer from motion blur caused by the camera deployed on moving vehicles. To resolve this, we propose a novel hard-classaware module that bridges the main network for full-class segmentation and the hard-class network for segmenting aforementioned hard-class objects. In specific, it exploits the shared focus of hard-class objects from the dual-stream network, enabling the contextual information flow to guide the model to concentrate on the pixels that are hard to classify. In the end, the estimated hard-class segmentation results will be utilized to infer the final results via an adaptive probabilistic fusion refinement scheme. Moreover, to overcome over-smoothing and noise caused by extreme exposures, our model is modulated by a carefully crafted pretext task of constructing an exposure-aware semantic gradient map, which guides the model to faithfully perceive the structural and semantic information of hard-class objects while mitigating the negative impact of noises and uneven exposures. In experiments, we demonstrate that our unique network design leads to superior segmentation performance over existing methods, featuring the strong ability of perceiving hard-class objects under adverse conditions.
C1 [Liu, Wenxi; Cai, Jiaxin; Li, Qi; Liao, Chenyang; Yu, Yuanlong] Fuzhou Univ, Coll Comp & Data Sci, 2 Xue Yuan Rd, Fuzhou 350108, Fujian, Peoples R China.
   [Cao, Jingjing] Wuhan Univ Technol, Sch Transportat & Logist Engn, 122 Luoshi Rd, Wuhan, Hubei, Peoples R China.
   [He, Shengfeng] Singapore Management Univ, Sch Comp & Informat Syst, Adm Bldg,81 Victoria St, Singapore 188065, Singapore.
C3 Fuzhou University; Wuhan University of Technology; Singapore Management
   University
RP Yu, YL (corresponding author), Fuzhou Univ, Coll Comp & Data Sci, 2 Xue Yuan Rd, Fuzhou 350108, Fujian, Peoples R China.
EM wenxi.liu@hotmail.com; 850839565@qq.com; 541261403@qq.com;
   222000213@fzu.edu.cn; bettycao@whut.edu.cn; shengfenghe7@gmail.com;
   yu.yuanlong@fzu.edu.cn
RI He, Shengfeng/E-5682-2016
OI He, Shengfeng/0000-0002-3802-4644; , Qi Li/0009-0000-7890-9220; Liao,
   Chenyang/0009-0002-7470-4193; CAO, Jingjing/0000-0002-3483-6100
FU National Natural Science Foundation of China [6720110, U21A20471,
   U21A20472]
FX This work was supported by the National Natural Science Foundation of
   China (grants 6720110, U21A20471, and U21A20472).
CR Afifi M, 2021, PROC CVPR IEEE, P9153, DOI 10.1109/CVPR46437.2021.00904
   Bao H., 2021, arXiv preprint arXiv:2106.08254, DOI DOI 10.48550/ARXIV.2106.08254
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen T, 2020, PR MACH LEARN RES, V119
   Chen T, 2019, PROC CVPR IEEE, P12146, DOI 10.1109/CVPR.2019.01243
   Chen YZ, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3460940
   Chen Z, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3596496
   Dai DX, 2018, IEEE INT C INTELL TR, P3819, DOI 10.1109/ITSC.2018.8569387
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Deng XQ, 2022, PROC CVPR IEEE, P16917, DOI 10.1109/CVPR52688.2022.01643
   Devlin J, 2019, Arxiv, DOI arXiv:1810.04805
   Ding HH, 2018, PROC CVPR IEEE, P2393, DOI 10.1109/CVPR.2018.00254
   Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167
   Dosovitskiy A., 2021, ICLR
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Gao H, 2022, PROC CVPR IEEE, P9903, DOI 10.1109/CVPR52688.2022.00968
   Gidaris S, 2018, Arxiv, DOI arXiv:1803.07728
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   Han K, 2021, ADV NEUR IN
   He KM, 2022, PROC CVPR IEEE, P15979, DOI 10.1109/CVPR52688.2022.01553
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou QB, 2020, PROC CVPR IEEE, P4002, DOI 10.1109/CVPR42600.2020.00406
   Hoyer L, 2022, LECT NOTES COMPUT SC, V13690, P372, DOI 10.1007/978-3-031-20056-4_22
   Hoyer L, 2022, PROC CVPR IEEE, P9914, DOI 10.1109/CVPR52688.2022.00969
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Ismail Mohammad Khalil, 2022, AL-Rafidain Journal of Computer Sciences and Mathematics, V16, P59
   Kolesnikov A, 2019, PROC CVPR IEEE, P1920, DOI 10.1109/CVPR.2019.00202
   LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108
   Larsson G, 2016, LECT NOTES COMPUT SC, V9908, P577, DOI 10.1007/978-3-319-46493-0_35
   Li Q, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7232, DOI 10.1109/ICCV48922.2021.00716
   Li X, 2019, IEEE I CONF COMP VIS, P9166, DOI 10.1109/ICCV.2019.00926
   Li Y., 2021, arXiv
   Lin HN, 2014, OPTIK, V125, P7143, DOI 10.1016/j.ijleo.2014.07.118
   Liu S., 2017, NIPS, P1520
   Liu WY, 2023, IEEE T CIRC SYST VID, V33, P5855, DOI 10.1109/TCSVT.2023.3260240
   Liu YW, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3425605
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Mikolov T, 2013, Arxiv, DOI [arXiv:1301.3781, DOI 10.48550/ARXIV.1301.3781]
   Minghao Yin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P191, DOI 10.1007/978-3-030-58555-6_12
   Nie J, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3558770
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Punn NS, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3376922
   Radford, 2018, OPENAI BLOG
   Radford A., 2019, OpenAI blog, V1, P9
   Romera E, 2019, IEEE INT VEH SYM, P1312, DOI [10.1109/ivs.2019.8813888, 10.1109/IVS.2019.8813888]
   Sakaridis C, 2022, IEEE T PATTERN ANAL, V44, P3139, DOI 10.1109/TPAMI.2020.3045882
   Sakaridis C, 2019, IEEE I CONF COMP VIS, P7373, DOI 10.1109/ICCV.2019.00747
   Sun L, 2019, PROC SPIE, V11169, DOI 10.1117/12.2532477
   Tan X, 2021, IEEE T IMAGE PROCESS, V30, P9085, DOI 10.1109/TIP.2021.3122004
   Vu TH, 2019, PROC CVPR IEEE, P2512, DOI 10.1109/CVPR.2019.00262
   Wang Jingdong, 2019, P CVPR
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu XY, 2021, PROC CVPR IEEE, P15764, DOI 10.1109/CVPR46437.2021.01551
   Wu XY, 2023, IEEE T PATTERN ANAL, V45, P58, DOI 10.1109/TPAMI.2021.3138829
   Xiao JS, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3419842
   Xiao TT, 2018, LECT NOTES COMPUT SC, V11209, P432, DOI 10.1007/978-3-030-01228-1_26
   Xie E., 2021, ADV NEURAL INF PROCE, V34, P12077
   Xie ZF, 2023, IEEE T IMAGE PROCESS, V32, P2386, DOI 10.1109/TIP.2023.3267044
   Xu Q, 2021, IEEE INT CONF COMP V, P2962, DOI 10.1109/ICCVW54120.2021.00331
   Yu F, 2020, PROC CVPR IEEE, P2633, DOI 10.1109/CVPR42600.2020.00271
   Yuan Y, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3321512
   Yuan YH, 2021, Arxiv, DOI arXiv:1809.00916
   Zhang H, 2019, PROC CVPR IEEE, P548, DOI 10.1109/CVPR.2019.00064
   Zhang H, 2018, PROC CVPR IEEE, P7151, DOI 10.1109/CVPR.2018.00747
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11213, P270, DOI 10.1007/978-3-030-01240-3_17
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhu ZF, 2021, IEEE T CIRC SYST VID, V31, P492, DOI 10.1109/TCSVT.2020.2987874
NR 74
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2024
VL 20
IS 7
SI SI
AR 213
DI 10.1145/3650032
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL0Q6
UT WOS:001234494100028
DA 2024-08-05
ER

PT J
AU Roy, K
AF Roy, Kankana
TI Multimodal Score Fusion with Sparse Low-rank Bilinear Pooling for
   Egocentric Hand Action Recognition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Bilinear score pooling; egocentric hand action recognition; RGB-D
   videos; sparse; low rank; CNN; RNN
ID NEURAL-NETWORKS
AB With the advent of egocentric cameras, there are new challenges where traditional computer vision is not sufficient to handle this kind of video. Moreover, egocentric cameras often offer multiple modalities that need to be modeled jointly to exploit complimentary information. In this article, we propose a sparse low-rank bilinear score pooling approach for egocentric hand action recognition from RGB-D videos. It consists of five blocks: a baseline CNN to encode RGB and depth information for producing classification probabilities; a novel bilinear score pooling block to generate a score matrix; a sparse low-rank matrix recovery block to reduce redundant features, which is common in bilinear pooling; a one-layer CNN for frame-level classification; and an RNN for video-level classification. We proposed to fuse classification probabilities instead of traditional CNN features from RGB and depth modality, involving an effective yet simple sparse low-rank bilinear score pooling to produce a fused RGB-D score matrix. To demonstrate the efficacy of our method, we perform extensive experiments over two large-scale hand action datasets, namely, THU-READ and FPHA, and two smaller datasets, GUN-71 and HAD. We observe that the proposed method outperforms state-of-the-art methods and achieves accuracies of 78.55% and 96.87% over the THU-READ dataset in cross-subject and cross-group settings, respectively. Further, we achieved accuracies of 91.59% and 43.87% over the FPHA and Gun-71 datasets, respectively.
C1 [Roy, Kankana] Karolinska Inst, Dept Oncol Pathol, S-17177 Stockholm, Sweden.
   [Roy, Kankana] Indian Inst Technol Kharagpur, Dept Comp Sci & Engn, Kharagpur 721302, West Bengal, India.
C3 Karolinska Institutet; Indian Institute of Technology System (IIT
   System); Indian Institute of Technology (IIT) - Kharagpur
RP Roy, K (corresponding author), Karolinska Inst, Dept Oncol Pathol, S-17177 Stockholm, Sweden.; Roy, K (corresponding author), Indian Inst Technol Kharagpur, Dept Comp Sci & Engn, Kharagpur 721302, West Bengal, India.
EM kankana.kankana.roy@gmail.com
OI Roy, Kankana/0000-0003-1296-7748
CR Andrew R., 2013, INT C MACHINE LEARNI, P1247, DOI DOI 10.5555/3042817.3043076
   Arnab A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6816, DOI 10.1109/ICCV48922.2021.00676
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Cheng J, 2022, IEEE T CIRC SYST VID, V32, P1498, DOI 10.1109/TCSVT.2021.3076165
   Dalal N, 2006, LECT NOTES COMPUT SC, V3952, P428, DOI 10.1007/11744047_33
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Engin M, 2018, LECT NOTES COMPUT SC, V11206, P629, DOI 10.1007/978-3-030-01216-8_38
   Fathi A, 2011, PROC CVPR IEEE
   Fathi A, 2011, IEEE I CONF COMP VIS, P407, DOI 10.1109/ICCV.2011.6126269
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Gao Y, 2016, PROC CVPR IEEE, P317, DOI 10.1109/CVPR.2016.41
   Garcia-Hernando G, 2018, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2018.00050
   Garcia-Hernando G, 2017, PROC CVPR IEEE, P407, DOI 10.1109/CVPR.2017.51
   Grauman K, 2022, PROC CVPR IEEE, P18973, DOI 10.1109/CVPR52688.2022.01842
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu JF, 2015, PROC CVPR IEEE, P5344, DOI 10.1109/CVPR.2015.7299172
   Ionescu C, 2015, IEEE I CONF COMP VIS, P2965, DOI 10.1109/ICCV.2015.339
   Kar P., 2012, Artificial intelligence and statistics, P583
   Kazakos E, 2019, IEEE I CONF COMP VIS, P5491, DOI 10.1109/ICCV.2019.00559
   Kong Y, 2015, PROC CVPR IEEE, P1054, DOI 10.1109/CVPR.2015.7298708
   Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756
   Li PH, 2017, IEEE I CONF COMP VIS, P2089, DOI 10.1109/ICCV.2017.228
   Li XY, 2022, IEEE T COGN DEV SYST, V14, P246, DOI 10.1109/TCDS.2020.3048883
   Li Y, 2015, PROC CVPR IEEE, P287, DOI 10.1109/CVPR.2015.7298625
   Lin TY, 2017, Arxiv, DOI arXiv:1707.06772
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Liu JB, 2020, PROC CVPR IEEE, P5750, DOI 10.1109/CVPR42600.2020.00579
   Ma MH, 2016, PROC CVPR IEEE, P1894, DOI 10.1109/CVPR.2016.209
   Min SB, 2020, IEEE T IMAGE PROCESS, V29, P4996, DOI 10.1109/TIP.2020.2977457
   Moghimi M, 2014, IEEE COMPUT SOC CONF, P611, DOI 10.1109/CVPRW.2014.94
   Ohn-Bar E, 2014, IEEE T INTELL TRANSP, V15, P2368, DOI 10.1109/TITS.2014.2337331
   Oreifej O, 2013, PROC CVPR IEEE, P716, DOI 10.1109/CVPR.2013.98
   Paszke A, 2019, ADV NEUR IN, V32
   Rahmani H, 2016, PROC CVPR IEEE, P1506, DOI 10.1109/CVPR.2016.167
   Ren LL, 2017, PATTERN RECOGN, V72, P446, DOI 10.1016/j.patcog.2017.06.037
   Rogez G, 2015, IEEE I CONF COMP VIS, P3889, DOI 10.1109/ICCV.2015.443
   Shahroudy A, 2018, IEEE T PATTERN ANAL, V40, P1045, DOI 10.1109/TPAMI.2017.2691321
   Shi QHY, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3485665
   Simonyan K, 2014, ADV NEUR IN, V27
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sudhakaran S, 2018, Arxiv, DOI arXiv:1807.11794
   Sudhakaran S, 2019, PROC CVPR IEEE, P9946, DOI 10.1109/CVPR.2019.01019
   Tan M, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3492221
   Tang YS, 2019, IEEE T CIRC SYST VID, V29, P3001, DOI 10.1109/TCSVT.2018.2875441
   Tang YS, 2017, IEEE IMAGE PROC, P3410, DOI 10.1109/ICIP.2017.8296915
   Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82
   Wang H., 2009, BMVC, P124
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang QL, 2017, PROC CVPR IEEE, P6507, DOI 10.1109/CVPR.2017.689
   Xu C, 2017, PATTERN RECOGN, V72, P494, DOI 10.1016/j.patcog.2017.08.009
   Xu HT, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3538749
   Ye J, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3038917
   Zaki HFM, 2017, PROC CVPR IEEE, P1619, DOI 10.1109/CVPR.2017.176
   Zanfir M, 2013, IEEE I CONF COMP VIS, P2752, DOI 10.1109/ICCV.2013.342
   Zhang XK, 2016, PROC CVPR IEEE, P4498, DOI 10.1109/CVPR.2016.487
   Zheng N, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3544493
   Zhou YZ, 2018, PROC CVPR IEEE, P449, DOI 10.1109/CVPR.2018.00054
NR 58
TC 0
Z9 0
U1 0
U2 0
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2024
VL 20
IS 7
SI SI
AR 227
DI 10.1145/3656044
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL0Q6
UT WOS:001234494100042
OA Bronze
DA 2024-08-05
ER

PT J
AU Yu, XY
   Wu, KJ
   Yang, Y
   Liu, Q
AF Yu, Xiaoya
   Wu, Kejun
   Yang, You
   Liu, Qiong
TI WaRENet: A Novel UrbanWaterlogging Risk Evaluation Network
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Urban waterlogging; waterlogging risk evaluation; object detection
ID FLOOD DETECTION
AB In this article, we propose a novel urban waterlogging risk evaluation network (WaRENet) to evaluate the risk of waterlogging. TheWaRENet distinguishes whether an urban image involves waterlogging by classification module, and estimates the waterlogging risk levels by multi-class reference objects detection module (MCROD). First, in the waterlogging scene classification, ResNet combined with Se-block is used to identify the waterlogging scene, and lightweight gradient-weighted class activation mapping (Grad-CAM) is also integrated to roughly locate overall waterlogging areas with low computational burden. Second, in the MCROD module, we detect reference objects, e.g., cars and persons in waterlogging scenes. The positional relationship between water depths and reference objects serves as risk indicators for accurately evaluating waterlogging risk. Specifically, we incorporate switchable atrous convolution (SAC) into YOLOv5 to solve occlusions and varying scales problems in complex waterlogging scenes. Moreover, we construct a large-scale urban waterlogging dataset called UrbanWaterloggingRiskDataset (UWRDataset) with 6,351 images for waterlogging scene classification and 3,217 images for reference objects detection. Experimental results on the dataset show that ourWaRENet outperforms all comparison methods. The waterlogging scene classification module achieves accuracy of 95.99%. The MCROD module obtains mAP of 54.9%, while maintaining a high processing speed of 70.04 fps.
C1 [Yu, Xiaoya; Wu, Kejun; Yang, You; Liu, Qiong] Huazhong Univ Sci & Technol, 1037 Luoyu Rd, Wuhan, Hubei, Peoples R China.
C3 Huazhong University of Science & Technology
RP Liu, Q (corresponding author), Huazhong Univ Sci & Technol, 1037 Luoyu Rd, Wuhan, Hubei, Peoples R China.
EM yara_yu@hust.edu.cn; wukejun_hust@163.com; yangyou@hust.edu.cn;
   q.liu@hust.edu.cn
FU National Key Research and Development Program of China [2020YFB2103501];
   Key Research and Development Program of Hubei Province [2023BAB021];
   Fundamental Research Program of HUST [2023BR023]
FX This work was supported in part by the National Key Research and
   Development Program of China (2020YFB2103501), in part by the Key
   Research and Development Program of Hubei Province (2023BAB021), in part
   by the Fundamental Research Program of HUST (2023BR023).
CR Ahmad K, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3306240
   An P, 2024, IEEE T MULTIMEDIA, V26, P2795, DOI 10.1109/TMM.2023.3304054
   An Pei, 2024, IEEE Transactions on Multimedia, V2024, P1
   Barz B, 2019, Arxiv, DOI arXiv:1908.03361
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chaudhary P., 2019, ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci, VIV-2-W5, P5, DOI [10.5194/isprs-annals-IV-2-W5-5-2019, DOI 10.5194/ISPRS-ANNALS-IV-2-W5-5-2019]
   Chen XL, 2020, Arxiv, DOI arXiv:2003.04297
   Chen YZ, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3460940
   de Vitry MM, 2019, HYDROL EARTH SYST SC, V23, P4621, DOI 10.5194/hess-23-4621-2019
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Feng Y, 2020, ISPRS J PHOTOGRAMM, V169, P301, DOI 10.1016/j.isprsjprs.2020.09.011
   Filonenko A, 2015, IEEE IND ELEC, P4082, DOI 10.1109/IECON.2015.7392736
   Geetha M, 2017, 2017 INTERNATIONAL CONFERENCE ON COMMUNICATION AND SIGNAL PROCESSING (ICCSP), P603, DOI 10.1109/ICCSP.2017.8286429
   Guo ZS, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14071752
   Gupta Kushagra, Studies, V13, P8
   Gupta R., 2019, arXiv, DOI 10.48550/arXiv.1911.09296
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang J, 2020, SUSTAINABILITY-BASEL, V12, DOI 10.3390/su12052149
   Karanjit R, 2023, DATA BRIEF, V48, DOI 10.1016/j.dib.2023.109164
   Borges PVK, 2008, IEEE IMAGE PROC, P13, DOI 10.1109/ICIP.2008.4711679
   Li JR, 2023, MEASUREMENT, V216, DOI 10.1016/j.measurement.2023.112891
   Liang YQ, 2023, ENVIRON MODELL SOFTW, V160, DOI 10.1016/j.envsoft.2022.105586
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lo SW, 2021, IEEE ACCESS, V9, P127185, DOI 10.1109/ACCESS.2021.3111623
   Lo SW, 2015, SENSORS-BASEL, V15, P20006, DOI 10.3390/s150820006
   Mahmud Bahar Uddin, 2022, ACM Transactions on Multimidia Computing Communications and Applications
   Muhadi NA, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11209691
   Ning H, 2020, ISPRS INT J GEO-INF, V9, DOI 10.3390/ijgi9020104
   Noppitak Sangdaow, 2020, ICISS 2020: Proceedings of the 2020 The 3rd International Conference on Information Science and System, P61, DOI 10.1145/3388176.3388184
   Pally RJ, 2022, ENVIRON MODELL SOFTW, V148, DOI 10.1016/j.envsoft.2021.105285
   Pan J, 2018, IEEE ACCESS, V6, P73561, DOI 10.1109/ACCESS.2018.2883702
   Park S, 2021, J COMPUT CIVIL ENG, V35, DOI 10.1061/(ASCE)CP.1943-5487.0000956
   Qianyu Zhang, 2019, 2019 16th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology (ECTI-CON). Proceedings, P927, DOI 10.1109/ECTI-CON47248.2019.8955273
   Qiao SY, 2021, PROC CVPR IEEE, P10208, DOI 10.1109/CVPR46437.2021.01008
   Rahnemoonfar M, 2021, IEEE ACCESS, V9, P89644, DOI 10.1109/ACCESS.2021.3090981
   Rambour C., 2020, Sen12-Flood: A SAR and Multispectral Dataset for Flood Detection
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shi Yifeng, 2022, CCRIS'22: Proceedings of the 2022 3rd International Conference on Control, Robotics and Intelligent System, P93, DOI 10.1145/3562007.3562025
   Vandaele Remy, 2021, Pattern Recognition. 42nd DAGM German Conference, DAGM GCPR 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12544), P232, DOI 10.1007/978-3-030-71278-5_17
   Weber Ethan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P331, DOI 10.1007/978-3-030-58529-7_20
   Wu JH, 2015, AER ADV ENG RES, V2, P268
   Wu KJ, 2024, OPT LETT, V49, P562, DOI 10.1364/OL.505496
   Wu KJ, 2024, IEEE T MULTIMEDIA, V26, P2993, DOI 10.1109/TMM.2023.3306072
   Wu KJ, 2023, IEEE T MULTIMEDIA, V25, P3975, DOI 10.1109/TMM.2022.3169055
   Xie J, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3584362
   Yang Kaixin, 2022, Complexity, V2022
   Yaul FM, 2012, J MICROELECTROMECH S, V21, P897, DOI 10.1109/JMEMS.2012.2190714
   Yuan QQ, 2020, REMOTE SENS ENVIRON, V241, DOI 10.1016/j.rse.2020.111716
   Zaffaroni M., 2020, International Journal of Information Systems for Crisis Response and Management, P24
   Zhou XY, 2019, Arxiv, DOI arXiv:1904.07850
NR 52
TC 0
Z9 0
U1 6
U2 6
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD JUL
PY 2024
VL 20
IS 7
SI SI
AR 214
DI 10.1145/3651163
PG 28
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SL0Q6
UT WOS:001234494100029
DA 2024-08-05
ER

PT J
AU Bingöl, G
   Porcu, S
   Floris, A
   Atzori, L
AF Bingol, Gulnaziye
   Porcu, Simone
   Floris, Alessandro
   Atzori, Luigi
TI QoE Estimation of WebRTC-based Audio-visual Conversations from Facial
   and Speech Features
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Quality of Experience; WebRTC; Facial Expressions; Speech; Machine
   Learning; Data Fusion
ID RECOGNITION; EXPRESSION
AB The utilization of user's facial- and speech-related features for the estimation of the Quality of Experience (QoE) of multimedia services is still underinvestigated despite its potential. Currently, only the use of either facial or speech features individually has been proposed, and relevant limited experiments have been performed. To advance in this respect, in this study, we focused on WebRTC-based videoconferencing, where it is often possible to capture both the facial expressions and vocal speech characteristics of the users. First, we performed thorough statistical analysis to identify the most significant facial- and speech-related features for QoE estimation, which we extracted from the participants' audio-video data collected during a subjective assessment. Second, we trained individual QoE estimation machine learning-based models on the separated facial and speech datasets. Finally, we employed data fusion techniques to combine the facial and speech datasets into a single dataset to enhance the QoE estimation performance due to the integrated knowledge provided by the fusion of facial and speech features. The obtained results demonstrate that the data fusion technique based on the Improved Centered Kernel Alignment (ICKA) allows for reaching a mean QoE estimation accuracy of 0.93, whereas the values of 0.78 and 0.86 are reached when using only facial or speech features, respectively.
C1 [Bingol, Gulnaziye; Porcu, Simone; Floris, Alessandro; Atzori, Luigi] Univ Cagliari, DIEE, Cagliari, Italy.
   [Bingol, Gulnaziye; Porcu, Simone; Floris, Alessandro; Atzori, Luigi] Univ Cagliari, CNIT, I-09123 Cagliari, Italy.
C3 University of Cagliari; University of Cagliari
RP Bingöl, G (corresponding author), Univ Cagliari, DIEE, Cagliari, Italy.; Bingöl, G (corresponding author), Univ Cagliari, CNIT, I-09123 Cagliari, Italy.
EM gulnaziye.bingol@unica.it; simone.porcu@unica.it;
   alessandro.floris84@unica.it; l.atzori@unica.it
RI Bingol, Gulnaziye/HNJ-4932-2023; Floris, Alessandro/L-6707-2018
OI Bingol, Gulnaziye/0009-0005-8959-112X; Floris,
   Alessandro/0000-0002-8745-1327; PORCU, SIMONE/0000-0003-0792-1200
FU European Union under the Italian National Recovery and Resilience Plan
   (NRRP) of NextGenerationEU, "Sustainable Mobility Center" Centro
   Nazionale per la Mobilit Sostenibile, CNMS [CN_00000023]; PON "Ricerca e
   Innovazione" 2014-2020 (PON R&I) "Azione IV.4 Dottorati e contratti di
   ricerca su tematiche dell" innovazione" [1062]
FX This work has been partially supported by the European Union under the
   Italian National Recovery and Resilience Plan (NRRP) of
   NextGenerationEU, "Sustainable Mobility Center" Centro Nazionale per la
   Mobilit Sostenibile, CNMS, CN_00000023, and by the PON "Ricerca e
   Innovazione" 2014-2020 (PON R&I) "Azione IV.4 Dottorati e contratti di
   ricerca su tematiche dell" innovazione" with D.M. 1062 on 10.08.2021.
CR Afshari S, 2016, MULTIMED TOOLS APPL, V75, P903, DOI 10.1007/s11042-014-2331-5
   Amour L., 2018, INT S PROGR SYST ISP, P1
   [Anonymous], 1996, Recommendation ITU-T P.800
   Baltrusaitis T, 2015, IEEE INT CONF AUTOMA
   Baltrusaitis T, 2018, IEEE INT CONF AUTOMA, P59, DOI 10.1109/FG.2018.00019
   Barakovic Sabina, 2017, Qual. User Exper., V6
   Bhattacharya A, 2012, HUM-CENT COMPUT INFO, V2, DOI 10.1186/2192-1962-2-7
   Bingöl G, 2022, 2022 16TH INTERNATIONAL CONFERENCE ON SIGNAL-IMAGE TECHNOLOGY & INTERNET-BASED SYSTEMS, SITIS, P577, DOI 10.1109/SITIS57111.2022.00092
   Bingol Gulnaziye, 2022, 14 INT C QUAL MULT E, P1
   Carlucci G, 2016, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON MULTIMEDIA SYSTEMS (MMSYS'16), P133, DOI 10.1145/2910017.2910605
   Casas P, 2016, IEEE T NETW SERV MAN, V13, P181, DOI 10.1109/TNSM.2016.2537645
   Charonyktakis P, 2016, IEEE T MOBILE COMPUT, V15, P1443, DOI 10.1109/TMC.2015.2461216
   Chen YJ, 2015, IEEE COMMUN SURV TUT, V17, P1126, DOI 10.1109/COMST.2014.2363139
   Chiariotti F, 2021, COMPUT COMMUN, V177, P133, DOI 10.1016/j.comcom.2021.06.029
   Chu F, 2005, STUD FUZZ SOFT COMP, V177, P343
   De Moor K, 2017, INT WORK QUAL MULTIM
   Egan D, 2016, 2016 EIGHTH INTERNATIONAL CONFERENCE ON QUALITY OF MULTIMEDIA EXPERIENCE (QOMEX)
   EKMAN P, 1971, J PERS SOC PSYCHOL, V17, P124, DOI 10.1037/h0030377
   Ekman P., 1978, Facial action coding system: A technique for the measurement of facial movement
   Engelke U, 2017, IEEE J-STSP, V11, P6, DOI 10.1109/JSTSP.2016.2609843
   Eyben F, 2010, P 18 INT C MULT 2010, DOI DOI 10.1145/1873951.1874246
   Eyben Florian, 2013, P 21 ACM INT C MULT, P835, DOI DOI 10.1145/2502081.2502224
   García B, 2020, ELECTRONICS-SWITZ, V9, DOI 10.3390/electronics9030462
   Gupta V, 2019, CLIN DERMATOL, V37, P430, DOI 10.1016/j.clindermatol.2019.07.010
   He HB, 2008, IEEE IJCNN, P1322, DOI 10.1109/IJCNN.2008.4633969
   ITU, 2015, Recommendation ITU-T G.107
   Jelassi S, 2012, IEEE COMMUN SURV TUT, V14, P491, DOI 10.1109/SURV.2011.120811.00063
   Khalil RA, 2019, IEEE ACCESS, V7, P117327, DOI 10.1109/ACCESS.2019.2936124
   Khan N, 2016, EURASIP J WIREL COMM, DOI 10.1186/s13638-016-0584-6
   Koolagudi SG, 2012, INT J SPEECH TECHNOL, V15, P99, DOI 10.1007/s10772-011-9125-1
   Kumar S, 2019, IEEE T NETW SERV MAN, V16, P1113, DOI 10.1109/TNSM.2019.2926720
   Kuriakose Teneema, 2023, Curr Top Microbiol Immunol, V442, P65, DOI 10.1007/82_2019_189
   Le Callet P., 2013, Qualinet White Paper on Definitions of Quality of Experience
   Li S, 2022, IEEE T AFFECT COMPUT, V13, P1195, DOI 10.1109/TAFFC.2020.2981446
   Liang WK, 2022, NEUROCOMPUTING, V492, P382, DOI 10.1016/j.neucom.2022.03.062
   Liu XW, 2020, IEEE T MULTIMEDIA, V22, P949, DOI 10.1109/TMM.2019.2934425
   McFee B., 2015, P 14 PYTH SCI C, V8, P18
   Moor K. D., 2014, SPIE, V9014, P204
   Mustafa MB, 2018, INT J SPEECH TECHNOL, V21, P137, DOI 10.1007/s10772-018-9493-x
   Porcu S, 2020, IEEE T NETW SERV MAN, V17, P2702, DOI 10.1109/TNSM.2020.3018303
   Porcu Simone, 2022, Qual. User Exper., V7, P1
   Rössler J, 2021, FUTURE INTERNET, V13, DOI 10.3390/fi13050126
   Sainburg T, 2020, PLOS COMPUT BIOL, V16, DOI 10.1371/journal.pcbi.1008228
   Sainburg Tim, 2019, Zenodo
   Sandvine, 2023, 2023 Global Internet Phenomena Report
   Schmitt M, 2018, IEEE T MULTIMEDIA, V20, P1781, DOI 10.1109/TMM.2017.2777466
   Schuller B, 2016, INTERSPEECH, P2001, DOI 10.21437/Interspeech.2016-129
   Scott MJ, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P481, DOI 10.1145/2733373.2806254
   Seufert M, 2015, IEEE COMMUN SURV TUT, V17, P469, DOI 10.1109/COMST.2014.2360940
   Skorin-Kapov L, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3176648
   Tiotsop Lohic Fotio, 2022, 14 INT C QUAL MULT E, P1
   Tsiaras C, 2015, 2015 IFIP NETWORKING CONFERENCE (IFIP NETWORKING)
   Tsolkas D, 2017, J NETW COMPUT APPL, V77, P1, DOI 10.1016/j.jnca.2016.10.016
   Vlahovic S, 2022, J MULTIMODAL USER IN, V16, P257, DOI 10.1007/s12193-022-00388-0
   Vucic D., 2016, 5 ISCA DEGA WORKSH P, P59
   Vucic D, 2020, IEEE ACCESS, V8, P107669, DOI 10.1109/ACCESS.2020.3000467
   Vucic D, 2019, LECT NOTES COMPUT SC, V11295, P459, DOI 10.1007/978-3-030-05710-7_38
   Wang Chaowei, 2023, 20 ACM C EMB NETW SE, P305
   Weninger F, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00292
   Wood E, 2015, IEEE I CONF COMP VIS, P3756, DOI 10.1109/ICCV.2015.428
   Yan H, 2021, IEEE T NETW SERV MAN, V18, P1789, DOI 10.1109/TNSM.2020.3043482
NR 61
TC 1
Z9 1
U1 2
U2 2
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD MAY
PY 2024
VL 20
IS 5
AR 130
DI 10.1145/3638251
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MF3R9
UT WOS:001192177900010
OA Green Published, hybrid
DA 2024-08-05
ER

PT J
AU Nie, XS
   Shi, Y
   Meng, ZY
   Huang, J
   Guan, WL
   Yin, YL
AF Nie, Xiushan
   Shi, Yang
   Meng, Ziyu
   Huang, Jin
   Guan, Weili
   Yin, Yilong
TI Complex Scenario Image Retrieval via Deep Similarity-aware Hashing
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Hashing; similarity-aware hashing; complex scenarios; difficult samples
AB When performing hashing-based image retrieval, it is difficult to learn discriminative hash codes especially for the multi-label, zero-shot and fine-grained settings. This is due to the fact that the similarities vary, even within the same category, under the conditions of complex scenario settings. To address this problem, this study develops a deep similarity-aware hashing method for complex scenario image retrieval (DEPISH). DEPISH more focuses on the samples that are difficult to distinguish from other images (i.e., "difficult samples"), such as images that contain multiple semantics. It dynamically divides attention among samples according to their difficulty levels with a margin weighting strategy. Furthermore, by adding special terms in the model, DEPISH is capable of avoiding the inconsistency between the hash code representation and true similarity among negative samples. In addition, unlike the existing methods that use a pre-defined similarity matrix with fixed values, the DEPISH adopts an adaptive similarity matrix, which accurately captures the various similarities among all samples. The results of our experiment on multiple benchmark datasets containing complex scenarios (i.e., multi-label, zero-shot, and fine-grained datasets) verify the effectiveness of this method.
C1 [Nie, Xiushan; Meng, Ziyu] Shandong Jianzhu Univ, 1000 Fengming Rd, Jinan, Shandong, Peoples R China.
   [Nie, Xiushan] Shandong Yunhai Guochuang Cloud Comp Equipment In, 1000 Fengming Rd, Jinan, Shandong, Peoples R China.
   [Shi, Yang; Huang, Jin; Yin, Yilong] Shandong Univ, 1500 Shunhua Rd, Jinan, Shandong, Peoples R China.
   [Meng, Ziyu] Monash Univ, Clayton Campus,Rono Hills, Melbourne, Vic, Australia.
C3 Shandong Jianzhu University; Shandong University; Monash University
RP Yin, YL (corresponding author), Shandong Univ, 1500 Shunhua Rd, Jinan, Shandong, Peoples R China.
EM niexsh@hotmail.com; shiyang@mail.sdu.edu.cn; darccccy@163.com;
   jinhuang.mla@gmail.com; weili.guan@monash.edu; ylyin@sdu.edu.cn
OI Shi, Yang/0000-0003-2515-1588
FU Shandong Provincial Natural Science Foundation for Distinguished Young
   Scholars [ZR2021JQ26]; National Natural Science Foundation of China
   [62176141, 62176139]; Major Basic Research Project of Natural Science
   Foundation of Shandong Province [ZR2021ZD15]; Taishan Scholar Project of
   Shandong Province [tsqn202103088]
FX This work was supported in part by the Shandong Provincial Natural
   Science Foundation for Distinguished Young Scholars(ZR2021JQ26),
   National Natural Science Foundation of China (62176141, 62176139), Major
   Basic Research Project of Natural Science Foundation of Shandong
   Province (ZR2021ZD15), and Taishan Scholar Project of Shandong Province
   (tsqn202103088).
CR Andoni A, 2006, ANN IEEE SYMP FOUND, P459
   Anwaar Muhammad Umer, 2021, ICMI '21: Proceedings of the 2021 International Conference on Multimodal Interaction, P34, DOI 10.1145/3462244.3479904
   Cao Y, 2018, PROC CVPR IEEE, P1229, DOI 10.1109/CVPR.2018.00134
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Chen Y, 2020, IEEE T IMAGE PROCESS, V29, P3596, DOI 10.1109/TIP.2020.2963952
   Chen Z, 2023, IEEE T MULTIMEDIA, V25, P5374, DOI 10.1109/TMM.2022.3190678
   Cheng MM, 2020, ACM T INFORM SYST, V38, DOI 10.1145/3389547
   Chua Tat-Seng, 2009, P C IMAGE VIDEO RETR
   Cui H, 2020, IEEE T IMAGE PROCESS, V29, P1271, DOI 10.1109/TIP.2019.2940693
   Deng C, 2018, IEEE T IMAGE PROCESS, V27, P3893, DOI 10.1109/TIP.2018.2821921
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Gui J, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1485, DOI 10.1145/3219819.3219955
   Guo YC, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1767
   Huang CQ, 2018, IEEE T IMAGE PROCESS, V27, P4490, DOI 10.1109/TIP.2018.2839522
   Jiang QY, 2018, AAAI CONF ARTIF INTE, P3342
   Jiang QY, 2018, IEEE T IMAGE PROCESS, V27, P5996, DOI 10.1109/TIP.2018.2864894
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Jin L, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P916, DOI 10.1145/3394171.3414022
   Khosla Bangpeng Yao Aditya, 2011, P C COMPUTER VISION, P1
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947
   Li C, 2018, PROC CVPR IEEE, P4242, DOI 10.1109/CVPR.2018.00446
   Li Q, 2017, ADV NEUR IN, V30
   Li Wu-Jun, 2016, P INT JOINT C ART IN, P1711
   Lin QH, 2021, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT, CIKM 2021, P1028, DOI 10.1145/3459637.3482247
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin ZJ, 2015, PROC CVPR IEEE, P3864, DOI 10.1109/CVPR.2015.7299011
   Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227
   Liu Wei, 2011, P INT C MACHINE LEAR
   Liu X, 2021, IEEE T PATTERN ANAL, V43, P964, DOI 10.1109/TPAMI.2019.2940446
   Liu XB, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3031
   Liu XB, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1561, DOI 10.1145/3343031.3351091
   Lu JW, 2021, IEEE T IMAGE PROCESS, V30, P332, DOI 10.1109/TIP.2020.3036735
   Luo X, 2018, ACM/SIGIR PROCEEDINGS 2018, P735, DOI 10.1145/3209978.3210035
   Peng YX, 2020, IEEE T MULTIMEDIA, V22, P2061, DOI 10.1109/TMM.2019.2951462
   Roy A, 2020, CIKM '20: PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT, P1315, DOI 10.1145/3340531.3411995
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Shen FM, 2015, IEEE T IMAGE PROCESS, V24, P1839, DOI 10.1109/TIP.2015.2405340
   Shi Y, 2023, IEEE T KNOWL DATA EN, V35, P5426, DOI 10.1109/TKDE.2022.3150790
   Shi Y, 2022, IEEE T IMAGE PROCESS, V31, P2755, DOI 10.1109/TIP.2022.3158092
   Shi Yang, 2021, P ACM MULTIMEDIA ASI
   Shi Yang, 2021, P ACM MULTIMEDIA ASI, P1
   Sun YF, 2020, PROC CVPR IEEE, P6397, DOI 10.1109/CVPR42600.2020.00643
   Tian JL, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5473, DOI 10.1145/3474085.3475676
   Tu RC, 2021, PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE 2021 (WWW 2021), P2869, DOI 10.1145/3442381.3449825
   Wah C., 2011, Tech. Rep. CNS-TR-2011-001
   Wang WW, 2021, INFORM SCIENCES, V547, P622, DOI 10.1016/j.ins.2020.08.092
   Weida Cao, 2021, VSIP 2021: 2021 3rd International Conference on Video, Signal and Image Processing, P37, DOI 10.1145/3503961.3503968
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2156
   Xian YQ, 2019, IEEE T PATTERN ANAL, V41, P2251, DOI 10.1109/TPAMI.2018.2857768
   Xiang XG, 2022, IEEE T IMAGE PROCESS, V31, P314, DOI 10.1109/TIP.2021.3131042
   Xu YH, 2017, IEEE INT CON MULTI, P133, DOI 10.1109/ICME.2017.8019425
   Xue Xuetong, 2023, ASIAN C MACHINE LEAR, P1197
   YairWeiss Antonio Torralba, 2008, P INT C NEURAL INFOR, P1753
   Yan C, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1535, DOI 10.1145/3343031.3350927
   Yang Y, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1286, DOI 10.1145/2964284.2964319
   Yang Yue-Hua, 2015, Journal of Software, V26, P1675, DOI 10.13328/j.cnki.jos.004622
   Yao T., 2016, IJCAI, P3931
   Zhang DQ, 2014, AAAI CONF ARTIF INTE, P2177
   Zhang HF, 2019, PATTERN RECOGN LETT, V117, P201, DOI 10.1016/j.patrec.2018.04.011
   Zhang PF, 2022, IEEE T MULTIMEDIA, V24, P466, DOI 10.1109/TMM.2021.3053766
   Zhao CR, 2021, IEEE T IMAGE PROCESS, V30, P7776, DOI 10.1109/TIP.2021.3109508
   Zhao Shuguang, 2021, P ACM MULTIMEDIA ASI, P1
   Zhu L, 2023, ACM T INFORM SYST, V41, DOI 10.1145/3559758
   Zhu L, 2022, ACM T INFORM SYST, V40, DOI 10.1145/3477180
   Zhu XF, 2013, ACM T INFORM SYST, V31, DOI 10.1145/2457465.2457469
NR 68
TC 0
Z9 0
U1 3
U2 3
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD APR
PY 2024
VL 20
IS 4
AR 93
DI 10.1145/3624016
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GN6G9
UT WOS:001153382100003
DA 2024-08-05
ER

PT J
AU Liang, S
   Ma, WT
   Xie, C
AF Liang, Shuang
   Ma, Wentao
   Xie, Chi
TI Relation with Free Objects for Action Recognition
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Action recognition; relation; object detection
AB Relevant objects are widely used for aiding human action recognition in still images. Such objects are founded by a dedicated and pre-trained object detector in all previous methods. Such methods have two drawbacks. First, training an object detector requires intensive data annotation. This is costly and sometimes unaffordable in practice. Second, the relation between objects and humans are not fully taken into account in training.
   This work proposes a systematic approach to address the two problems. We propose two novel network modules. The first is an object extraction module that automatically finds relevant objects for action recognition, without requiring annotations. Thus, it is free. The second is a human-object relation module that models the pairwise relation between humans and objects, and enhances their features. Both modules are trained in the action recognition network, end-to-end.
   Comprehensive experiments and ablation studies on three datasets for action recognition in still images demonstrate the effectiveness of the proposed approach. Our method yields state-of-the-art results. Specifically, on the HICO dataset, it achieves 44.9% mAP, which is 12% relative improvement over the previous best result. In addition, this work makes an observational contribution that it is no longer necessary to rely on a pre-trained object detector for this task. Relevant objects can be found via end-to-end learning with only action labels. This is encouraging for action recognition in the wild. Models and code will be released.
C1 [Liang, Shuang; Ma, Wentao; Xie, Chi] Tongji Univ, 4800 Caoan Rd, Shanghai 201800, Peoples R China.
C3 Tongji University
RP Liang, S (corresponding author), Tongji Univ, 4800 Caoan Rd, Shanghai 201800, Peoples R China.
EM shuangliang@tongji.edu.cn; wentao.ma@tongji.edu.cn; chixie@tongji.edu.cn
RI Xie, Chi/B-5612-2014; Wang, Jin/KAM-5595-2024
OI Ma, Wentao/0000-0002-6845-5672; Xie, Chi/0000-0002-5808-1742
FU National Natural Science Foundation of China [62076183, 61936014,
   61976159]; Natural Science Foundation of Shanghai [20ZR1473500];
   Shanghai Science and Technology Innovation Action Project [20511100700,
   22511105300]; Shanghai Municipal Science and Technology Major Project
   [2021SHZDZX0100]; Fundamental Research Funds for the Central
   Universities
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62076183, 61936014 and 61976159, in part
   by the Natural Science Foundation of Shanghai under Grant 20ZR1473500,
   in part by the Shanghai Science and Technology Innovation Action Project
   under Grant 20511100700 and 22511105300, in part by the Shanghai
   Municipal Science and Technology Major Project under Grant
   2021SHZDZX0100, and in part by the Fundamental Research Funds for the
   Central Universities.
CR Abu-Bakar SAR, 2019, IET IMAGE PROCESS, V13, P2381, DOI 10.1049/iet-ipr.2019.0350
   Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471
   Ashrafi SS, 2023, MULTIMED TOOLS APPL, V82, P25945, DOI 10.1007/s11042-023-14350-z
   Chao YW, 2018, IEEE WINT CONF APPL, P381, DOI 10.1109/WACV.2018.00048
   Chao YW, 2015, IEEE I CONF COMP VIS, P1017, DOI 10.1109/ICCV.2015.122
   Chen Gao, 2018, P BRIT MACH VIS C
   Chen TQ, 2015, Arxiv, DOI arXiv:1512.01274
   Dehkordi HA, 2022, KNOWL-BASED SYST, V250, DOI 10.1016/j.knosys.2022.109091
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fang HS, 2018, LECT NOTES COMPUT SC, V11214, P52, DOI 10.1007/978-3-030-01249-6_4
   Girdhar R., 2017, P C ADV NEURAL INFOR, P34
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gkioxari G, 2018, PROC CVPR IEEE, P8359, DOI 10.1109/CVPR.2018.00872
   Gkioxari G, 2015, IEEE I CONF COMP VIS, P1080, DOI 10.1109/ICCV.2015.129
   Gupta S, 2015, Arxiv, DOI arXiv:1505.04474
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378
   Hu HZ, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3422360
   Kong Y, 2022, INT J COMPUT VISION, V130, P1366, DOI 10.1007/s11263-022-01594-9
   Li YL, 2019, PROC CVPR IEEE, P3580, DOI 10.1109/CVPR.2019.00370
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu L, 2019, LECT NOTES COMPUT SC, V11365, P152, DOI 10.1007/978-3-030-20873-8_10
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Mallya A, 2016, LECT NOTES COMPUT SC, V9905, P414, DOI 10.1007/978-3-319-46448-0_25
   Mi SY, 2022, APPL INTELL, V52, P6760, DOI 10.1007/s10489-021-02760-1
   Redmon J., 2018, CoRR
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shi QHY, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3485665
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun C, 2018, LECT NOTES COMPUT SC, V11215, P335, DOI 10.1007/978-3-030-01252-6_20
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wu W, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P2450, DOI 10.1109/ICASSP39728.2021.9414302
   Xie C, 2023, PROC CVPR IEEE, P15275, DOI 10.1109/CVPR52729.2023.01466
   Xie C, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3567827
   Xu BJ, 2020, IEEE T MULTIMEDIA, V22, P1423, DOI 10.1109/TMM.2019.2943753
   Xu HT, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3538749
   Yan SY, 2018, IEEE T COGN DEV SYST, V10, P1116, DOI 10.1109/TCDS.2017.2783944
   Yao BP, 2011, IEEE I CONF COMP VIS, P1331, DOI 10.1109/ICCV.2011.6126386
   Zhang JX, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3321511
   Zhao ZC, 2017, IEEE I CONF COMP VIS, P3411, DOI 10.1109/ICCV.2017.367
   Zhao ZC, 2016, PATTERN RECOGN LETT, V84, P134, DOI 10.1016/j.patrec.2016.08.020
   Zheng YP, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3362161
   Zhuang BH, 2018, AAAI CONF ARTIF INTE, P7631
NR 47
TC 1
Z9 1
U1 5
U2 15
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2024
VL 20
IS 2
SI SI
AR 58
DI 10.1145/3617596
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W6GR4
UT WOS:001092595800028
DA 2024-08-05
ER

PT J
AU You, SS
   Zuo, YK
   Yao, HT
   Xu, CS
AF You, Sisi
   Zuo, Yukun
   Yao, Hantao
   Xu, Changsheng
TI Incremental Audio-Visual Fusion for Person Recognition in Earthquake
   Scene
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Cross-modal audio-visual fusion; incremental learning; person
   recognition; elastic weight consolidation; feature replay
AB Earthquakes have a profound impact on social harmony and property, resulting in damage to buildings and infrastructure. Effective earthquake rescue efforts require rapid and accurate determination of whether any survivors are trapped in the rubble of collapsed buildings. While deep learning algorithms can enhance the speed of rescue operations using single-modal data (either visual or audio), they are confronted with two primary challenges: insufficient information provided by single-modal data and catastrophic forgetting. In particular, the complexity of earthquake scenes means that single-modal features may not provide adequate information. Additionally, catastrophic forgetting occurs when the model loses the information learned in a previous task after training on subsequent tasks, due to non-stationary data distributions in changing earthquake scenes. To address these challenges, we propose an innovative approach that utilizes an incremental audio-visual fusion model for person recognition in earthquake rescue scenarios. Firstly, we leverage a cross-modal hybrid attention network to capture discriminative temporal context embedding, which uses self-attention and cross-modal attention mechanisms to combine multi-modality information, enhancing the accuracy and reliability of person recognition. Secondly, an incremental learning model is proposed to overcome catastrophic forgetting, which includes elastic weight consolidation and feature replay modules. Specifically, the elastic weight consolidation module slows down learning on certain weights based on their importance to previously learned tasks. The feature replay module reviews the learned knowledge by reusing the features conserved from the previous task, thus preventing catastrophic forgetting in dynamic environments. To validate the proposed algorithm, we collected the Audio-Visual Earthquake Person Recognition (AVEPR) dataset from earthquake films and real scenes. Furthermore, the proposed method gets 85.41% accuracy while learning the 10th new task, which demonstrates the effectiveness of the proposed method and highlights its potential to significantly improve earthquake rescue efforts.
C1 [You, Sisi] Nanjing Univ Posts & Telecommun, Sch Commun & Informat Engn, Nanjing 210003, Jiangsu, Peoples R China.
   [Zuo, Yukun] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, 96 JinZhai Rd, Hefei 230026, Anhui, Peoples R China.
   [Yao, Hantao; Xu, Changsheng] Chinese Acad Sci CASIA, Inst Automat, State Key Lab Multimodal Artificial Intelligence, Beijing 100190, Peoples R China.
   [Yao, Hantao; Xu, Changsheng] Univ Chinese Acad Sci UCAS, Sch Artificial Intelligence, Beijing 101408, Peoples R China.
C3 Nanjing University of Posts & Telecommunications; Chinese Academy of
   Sciences; University of Science & Technology of China, CAS; Chinese
   Academy of Sciences; Institute of Automation, CAS
RP Xu, CS (corresponding author), Chinese Acad Sci CASIA, Inst Automat, State Key Lab Multimodal Artificial Intelligence, Beijing 100190, Peoples R China.; Xu, CS (corresponding author), Univ Chinese Acad Sci UCAS, Sch Artificial Intelligence, Beijing 101408, Peoples R China.
EM ssyou@njupt.edu.cn; zykpy@mail.ustc.edu.cn; hantao.yao@nlpr.ia.ac.cn;
   csxu@nlpr.ia.ac.cn
RI xu, cj/HJZ-3488-2023
OI Yao, Hantao/0000-0001-8125-2864; Zuo, Yukun/0000-0002-2883-8572; xu,
   chang sheng/0000-0001-8343-9665; you, sisi/0009-0004-2173-1694
FU National Key Research and Development Program of China [2018AAA0102205];
   National Natural Science Foundation of China [61936005,
   62036012,61721004, U21B2044]; Beijing Natural Science Foundation
   [4222039]; Opening Foundation of Key Laboratory of Computer Vision and
   System, Ministry of Education, Tianjin University of Technology, China
FX This work was supported by National Key Research and Development Program
   of China (2018AAA0102205), National Natural Science Foundation of China
   (61936005, 62036012,61721004, U21B2044), Beijing Natural Science
   Foundation (4222039), and the Opening Foundation of Key Laboratory of
   Computer Vision and System, Ministry of Education, Tianjin University of
   Technology, China.
CR Rusu AA, 2016, Arxiv, DOI [arXiv:1606.04671, DOI 10.48550/ARXIV.1606.04671]
   Aljundi R, 2018, LECT NOTES COMPUT SC, V11207, P144, DOI 10.1007/978-3-030-01219-9_9
   Aytar Y, 2016, ADV NEUR IN, V29
   Bialobrzeski R, 2019, INTERSPEECH, P1028, DOI 10.21437/Interspeech.2019-2676
   Chung J, 2014, PREPRINT
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gomez-Alanis A, 2019, INTERSPEECH, P1068, DOI 10.21437/Interspeech.2019-2212
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Han S, 2015, ADV NEUR IN, V28
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hershey S, 2017, INT CONF ACOUST SPEE, P131, DOI 10.1109/ICASSP.2017.7952132
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Kamra N, 2018, Arxiv, DOI arXiv:1710.10368
   Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Le Cornu T, 2017, IEEE-ACM T AUDIO SPE, V25, P1447, DOI 10.1109/TASLP.2017.2716178
   Lee Sang-Woo, 2017, P 31 INT C NEUR INF
   Li ZZ, 2018, IEEE T PATTERN ANAL, V40, P2935, DOI 10.1109/TPAMI.2017.2773081
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu XL, 2018, INT C PATT RECOG, P2262, DOI 10.1109/ICPR.2018.8545895
   Ngiam J., 2011, P 28 INT C MACH LEAR, P689
   Kingma DP, 2014, Arxiv, DOI arXiv:1312.6114
   Parekh S., 2018, P IEEE C COMP VIS PA, P2518
   Petridis S, 2017, INT CONF ACOUST SPEE, P2592, DOI 10.1109/ICASSP.2017.7952625
   Rajasegaran Jathushan, 2019, P 33 INT C NEUR INF
   Rebuffi SA, 2017, PROC CVPR IEEE, P5533, DOI 10.1109/CVPR.2017.587
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Serra D., 2018, International Conference on Machine Learning, P4548
   Shen GY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P493, DOI 10.1145/3394171.3413909
   Shin Hanul, 2017, P 31 INT C NEUR INF
   Sriskandaraja K, 2018, INTERSPEECH, P671, DOI 10.21437/Interspeech.2018-1819
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   Trigeorgis G, 2016, INT CONF ACOUST SPEE, P5200, DOI 10.1109/ICASSP.2016.7472669
   Wand M, 2016, INT CONF ACOUST SPEE, P6115, DOI 10.1109/ICASSP.2016.7472852
   Wang XL, 2018, PROC CVPR IEEE, P7774, DOI 10.1109/CVPR.2018.00811
   Wang XS, 2021, IEEE-ACM T AUDIO SPE, V29, P850, DOI 10.1109/TASLP.2021.3053391
   Wu X, 2018, IEEE T INF FOREN SEC, V13, P2884, DOI 10.1109/TIFS.2018.2833032
   Wu Y, 2018, Arxiv, DOI arXiv:1802.00853
   Xia Y, 2022, PROC CVPR IEEE, P19957, DOI 10.1109/CVPR52688.2022.01936
   Xu H, 2006, ACM T MULTIM COMPUT, V2, P44, DOI 10.1145/1126004.1126007
   Yapeng Tian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P436, DOI 10.1007/978-3-030-58580-8_26
   Yoon J, 2018, Arxiv, DOI [arXiv:1708.01547, 10.48550/arXiv.1708.01547, DOI 10.48550/ARXIV.1708.01547]
   Zeng GX, 2019, NAT MACH INTELL, V1, P364, DOI 10.1038/s42256-019-0080-x
   Zhang SF, 2018, LECT NOTES COMPUT SC, V11207, P657, DOI 10.1007/978-3-030-01219-9_39
   Zhao H, 2019, IEEE I CONF COMP VIS, P1735, DOI 10.1109/ICCV.2019.00182
   Zhao Hang, 2018, P EUR C COMP VIS ECC, P570, DOI DOI 10.1109/CVPR.2018.00374
   Zhou XY, 2019, Arxiv, DOI arXiv:1904.07850
NR 52
TC 1
Z9 1
U1 2
U2 4
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2024
VL 20
IS 2
SI SI
AR 53
DI 10.1145/3614434
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W6GR4
UT WOS:001092595800023
DA 2024-08-05
ER

PT J
AU Zheng, JW
   Liu, Y
   Feng, YC
   Xu, HH
   Zhang, MY
AF Zheng, Jianwei
   Liu, Yu
   Feng, Yuchao
   Xu, Honghui
   Zhang, Meiyu
TI Contrastive Attention-guided Multi-level Feature Registration for
   Reference-based Super-resolution
SO ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS
LA English
DT Article
DE Reference-based super-resolution; contrastive attention; feature
   warping; spatial registration
AB Given low-quality input and assisted by referential images, reference-based super-resolution (RefSR) strives to enlarge the spatial size with the guarantee of realistic textures, for which sophisticated feature-matching strategies are naturally demanded. However, the miserable transformation gap between inputs and references, e.g., texture rotation and scaling within patches, often yields distorted textures and terrible ghosting artifacts, which seriously hampers the visual senses and their further investigation. To circumvent this challenge, we propose a contrastive attention-guided multi-level feature registration for RefSR, explicitly tapping the potential of interacting between inputs and references. Specifically, we develop a multi-level feature warping scheme, involving patch-level coarse feature swapping and pixel-level deformable alignment, to model generalized spatial transformation correspondences steered by contrastive attention. Notably, a spatial registration module is embedded for further calibration against the potential misalignment issue and inter-feature distribution difference. In addition, aiming at suppressing the impacts of irrelevant or superfluous information on cross-scale features, we incorporate a multi-residual feature fusion module to strive for visually plausible textures. Experimental results on four publicly available datasets demonstrate that our method outperforms most state-of-the-art approaches in terms of both efficiency and perceptual effectiveness.
C1 [Zheng, Jianwei; Liu, Yu; Feng, Yuchao; Xu, Honghui; Zhang, Meiyu] Zhejiang Univ Technol, Coll Comp Sci, Hangzhou 310014, Peoples R China.
C3 Zhejiang University of Technology
RP Zhang, MY (corresponding author), Zhejiang Univ Technol, Coll Comp Sci, Hangzhou 310014, Peoples R China.
EM zjw@zjut.edu.cn; luiszjut@gmail.com; fyc@zjut.edu.cn; xhh@zjut.edu.cn;
   zmy@zjut.edu.cn
OI honghui, xu/0000-0002-6213-2979; Fung, Yuchao/0000-0001-6097-3806;
   Zheng, Jianwei/0000-0001-6017-0552
FU "Pioneer" and "Leading Goose" R&D Program of Zhejiang [2023C01241];
   National Natural Science Foundation of China [62073293, 62276232]
FX This work was supported in part by the "Pioneer" and "Leading Goose" R&D
   Program of Zhejiang under Grant 2023C01241, and the National Natural
   Science Foundation of China under Grant Nos. 62073293 and 62276232.
CR Afzal H, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3177756
   Ben Niu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P191, DOI 10.1007/978-3-030-58610-2_12
   Blau Y, 2019, LECT NOTES COMPUT SC, V11133, P334, DOI 10.1007/978-3-030-11021-5_21
   Blau Y, 2018, PROC CVPR IEEE, P6228, DOI 10.1109/CVPR.2018.00652
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Feng YC, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3168331
   Gyumin Shim, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8422, DOI 10.1109/CVPR42600.2020.00845
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Jiang WT, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3460474
   Jolicoeur-Martineau A., 2018, INT C LEARN REPR
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li YC, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3414838
   Li Z, 2022, IEEE T IMAGE PROCESS, V31, P2647, DOI 10.1109/TIP.2022.3160072
   Lian CY, 2022, NEUROCOMPUTING, V500, P799, DOI 10.1016/j.neucom.2022.05.113
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu Zhiming, 2022, ACM Trans. Multimedia Comput. Commun. Appl., V19, P1
   Lu LY, 2021, PROC CVPR IEEE, P6364, DOI 10.1109/CVPR46437.2021.00630
   Matsui Y, 2017, MULTIMED TOOLS APPL, V76, P21811, DOI 10.1007/s11042-016-4020-z
   Mei YQ, 2021, PROC CVPR IEEE, P3516, DOI 10.1109/CVPR46437.2021.00352
   Sun Libin, 2012, P IEEE INT C COMP PH
   Sun WJ, 2022, IEEE T IMAGE PROCESS, V31, P1490, DOI 10.1109/TIP.2022.3142999
   Tian YP, 2020, PROC CVPR IEEE, P3357, DOI 10.1109/CVPR42600.2020.00342
   Tong T, 2017, IEEE I CONF COMP VIS, P4809, DOI 10.1109/ICCV.2017.514
   Wang Tengfei, 2021, PROC IEEECVF INT C C, P2001
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang XT, 2019, LECT NOTES COMPUT SC, V11133, P63, DOI 10.1007/978-3-030-11021-5_5
   Wanli Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P52, DOI 10.1007/978-3-030-58520-4_4
   Xu HH, 2023, SIGNAL PROCESS, V206, DOI 10.1016/j.sigpro.2022.108888
   Xu HH, 2022, IEEE T CIRC SYST VID, V32, P538, DOI 10.1109/TCSVT.2021.3067022
   Xu HH, 2021, IEEE J-STARS, V14, P8823, DOI 10.1109/JSTARS.2021.3108233
   Yang Bin-Cheng, 2022, ACM Trans. Multimedia Comput. Commun. Appl., V19, P1
   Yang FZ, 2020, PROC CVPR IEEE, P5790, DOI 10.1109/CVPR42600.2020.00583
   Yuan Y, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3321512
   Zhang K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4771, DOI 10.1109/ICCV48922.2021.00475
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang SP, 2021, PROC CVPR IEEE, P12001, DOI 10.1109/CVPR46437.2021.01183
   Zhang WL, 2019, IEEE I CONF COMP VIS, P3096, DOI 10.1109/ICCV.2019.00319
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang ZF, 2019, PROC CVPR IEEE, P7974, DOI 10.1109/CVPR.2019.00817
   Zheng HT, 2018, LECT NOTES COMPUT SC, V11210, P87, DOI 10.1007/978-3-030-01231-1_6
   Zhu Z, 2019, IEEE I CONF COMP VIS, P593, DOI 10.1109/ICCV.2019.00068
NR 41
TC 0
Z9 0
U1 6
U2 12
PU ASSOC COMPUTING MACHINERY
PI NEW YORK
PA 1601 Broadway, 10th Floor, NEW YORK, NY USA
SN 1551-6857
EI 1551-6865
J9 ACM T MULTIM COMPUT
JI ACM Trans. Multimed. Comput. Commun. Appl.
PD FEB
PY 2024
VL 20
IS 2
SI SI
AR 55
DI 10.1145/3616495
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA W6GR4
UT WOS:001092595800025
DA 2024-08-05
ER

EF